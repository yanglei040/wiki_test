{
    "hands_on_practices": [
        {
            "introduction": "The best way to truly grasp a numerical algorithm is to perform its steps by hand on a small, manageable system. This first practice invites you to do just that with the BiCGSTAB method. By applying the algorithm to a simple $2 \\times 2$ non-symmetric system , you will trace the evolution of the residual, search directions, and update scalars step-by-step. This exercise demystifies the method's mechanics and provides concrete insight into how it converges to the exact solution in a finite number of iterations in exact arithmetic.",
            "id": "2374444",
            "problem": "You will apply the BiConjugate Gradient Stabilized (BiCGSTAB) method, spelled out as BiConjugate Gradient Stabilized (BiCGSTAB), to a nonsymmetric linear system to explicitly illustrate that, in exact arithmetic and with a standard choice of parameters, it requires two iterations to converge starting from the zero initial guess. Consider the linear system with\n- matrix $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$,\n- right-hand side $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$,\n- initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$,\n- identity preconditioner, and\n- shadow residual $\\hat{r} = r_0$, where $r_0 = b - A x_0$.\n\nAll inner products are the standard Euclidean ones. Work in exact arithmetic (that is, do not round intermediate quantities). Perform the BiCGSTAB iterations step-by-step, starting from the core definitions of residuals, search directions, and scalar coefficients that define the BiCGSTAB recurrence, and show that:\n1. After the first iteration, the residual is nonzero.\n2. After the second iteration, the method reaches the exact solution (residual equal to the zero vector).\n\nExplicitly compute and report all intermediate vectors and scalars used by BiCGSTAB up to convergence to substantiate these claims. Finally, provide the exact value of the first smoothing parameter $\\omega_1$ produced during the first BiCGSTAB iteration for this system. Express your final answer as an exact rational number. Do not round.",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- **Method:** BiConjugate Gradient Stabilized (BiCGSTAB)\n- **Linear System:** $A x = b$\n- **Matrix:** $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$\n- **Right-hand side:** $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- **Initial guess:** $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n- **Preconditioner:** Identity matrix ($M=I$)\n- **Shadow residual:** $\\hat{r}_0 = r_0$, where $r_0 = b - A x_0$\n- **Inner product:** Standard Euclidean inner product, denoted as $(u, v) = u^T v$.\n- **Arithmetic:** Exact arithmetic is required.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is a direct and standard application of the BiCGSTAB algorithm, a well-established method in numerical linear algebra for solving nonsymmetric linear systems. It is scientifically sound.\n- **Well-Posedness:** The matrix $A$ has determinant $\\det(A) = (1)(4) - (2)(3) = 4 - 6 = -2 \\neq 0$. Thus, $A$ is invertible, and the linear system $Ax=b$ has a unique solution. The BiCGSTAB algorithm is a well-defined procedure. For a system of size $N \\times N$, Krylov subspace methods like BiCGSTAB are guaranteed to find the exact solution in at most $N$ iterations in exact arithmetic. For this $2 \\times 2$ system, convergence in exactly $2$ iterations is a plausible and verifiable claim.\n- **Objectivity:** The problem is stated using precise mathematical language, free of any subjectivity or ambiguity.\n\n**Verdict:** The problem is valid as it is scientifically grounded, well-posed, and objective. It is a formalizable problem within the field of computational engineering. We may proceed to the solution.\n\nThe solution will be constructed by applying the BiCGSTAB algorithm step-by-step. The algorithm is as follows:\n\n**Initialization:**\n1. Given an initial guess $x_0$.\n2. Compute the initial residual $r_0 = b - Ax_0$.\n3. Choose a shadow residual vector $\\hat{r}_0$, such that $(\\hat{r}_0, r_0) \\neq 0$. Here, we are given $\\hat{r}_0 = r_0$.\n4. Set initial parameters for the recurrence: $\\rho_0 = 1$, $\\alpha_0 = 1$, $\\omega_0 = 1$.\n5. Set initial search directions: $p_0 = \\mathbf{0}$, $v_0 = \\mathbf{0}$.\n\n**Main Loop (for $k = 1, 2, \\dots$):**\n1. $\\rho_k = (\\hat{r}_0, r_{k-1})$\n2. $\\beta_k = \\frac{\\rho_k}{\\rho_{k-1}} \\frac{\\alpha_{k-1}}{\\omega_{k-1}}$\n3. $p_k = r_{k-1} + \\beta_k (p_{k-1} - \\omega_{k-1} v_{k-1})$\n4. $v_k = A p_k$\n5. $\\alpha_k = \\frac{\\rho_k}{(\\hat{r}_0, v_k)}$\n6. $s_k = r_{k-1} - \\alpha_k v_k$\n7. $t_k = A s_k$\n8. $\\omega_k = \\frac{(t_k, s_k)}{(t_k, t_k)}$\n9. $x_k = x_{k-1} + \\alpha_k p_k + \\omega_k s_k$\n10. $r_k = s_k - \\omega_k t_k$\n\nWe will now apply this algorithm to the given system.\n\n**Initialization ($k=0$):**\n- The system is defined by $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$ and $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n- The initial guess is $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n- The initial residual is $r_0 = b - Ax_0 = b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n- The shadow residual is $\\hat{r}_0 = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n- The initial parameters are set to $\\rho_0 = 1$, $\\alpha_0 = 1$, $\\omega_0 = 1$.\n- The initial direction vectors are $p_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and $v_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\n**Iteration 1 ($k=1$):**\n1. $\\rho_1 = (\\hat{r}_0, r_0) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$.\n2. $\\beta_1 = \\frac{\\rho_1}{\\rho_0} \\frac{\\alpha_0}{\\omega_0} = \\frac{1}{1} \\frac{1}{1} = 1$.\n3. $p_1 = r_0 + \\beta_1(p_0 - \\omega_0 v_0) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + 1 \\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - 1 \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\right) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n4. $v_1 = A p_1 = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}$.\n5. The denominator for $\\alpha_1$ is $(\\hat{r}_0, v_1) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = 1$. Thus, $\\alpha_1 = \\frac{\\rho_1}{(\\hat{r}_0, v_1)} = \\frac{1}{1} = 1$.\n6. $s_1 = r_0 - \\alpha_1 v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - 1 \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix}$.\n7. $t_1 = A s_1 = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} -6 \\\\ -12 \\end{pmatrix}$.\n8. For $\\omega_1$, we compute the inner products:\n   $(t_1, s_1) = \\begin{pmatrix} -6 & -12 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = (0)(-6) + (-3)(-12) = 36$.\n   $(t_1, t_1) = \\begin{pmatrix} -6 & -12 \\end{pmatrix} \\begin{pmatrix} -6 \\\\ -12 \\end{pmatrix} = (-6)^2 + (-12)^2 = 36 + 144 = 180$.\n   So, $\\omega_1 = \\frac{(t_1, s_1)}{(t_1, t_1)} = \\frac{36}{180} = \\frac{1}{5}$.\n9. $x_1 = x_0 + \\alpha_1 p_1 + \\omega_1 s_1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + 1 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\frac{1}{5} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -\\frac{3}{5} \\end{pmatrix}$.\n10. $r_1 = s_1 - \\omega_1 t_1 = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} -6 \\\\ -12 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\begin{pmatrix} -\\frac{6}{5} \\\\ -\\frac{12}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix}$.\n\nAfter the first iteration, the residual is $r_1 = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix}$, which is not the zero vector. This substantiates the first claim.\n\n**Iteration 2 ($k=2$):**\n1. $\\rho_2 = (\\hat{r}_0, r_1) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} = \\frac{6}{5}$.\n2. $\\beta_2 = \\frac{\\rho_2}{\\rho_1} \\frac{\\alpha_1}{\\omega_1} = \\frac{6/5}{1} \\frac{1}{1/5} = \\frac{6}{5} \\cdot 5 = 6$.\n3. $p_2 = r_1 + \\beta_2(p_1 - \\omega_1 v_1) = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} + 6 \\left( \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} \\right) = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} + 6 \\begin{pmatrix} \\frac{4}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} + \\frac{24}{5} \\\\ -\\frac{3}{5} - \\frac{18}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{30}{5} \\\\ -\\frac{21}{5} \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ -\\frac{21}{5} \\end{pmatrix}$.\n4. $v_2 = A p_2 = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ -\\frac{21}{5} \\end{pmatrix} = \\begin{pmatrix} 6 - \\frac{42}{5} \\\\ 18 - \\frac{84}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{30-42}{5} \\\\ \\frac{90-84}{5} \\end{pmatrix} = \\begin{pmatrix} -\\frac{12}{5} \\\\ \\frac{6}{5} \\end{pmatrix}$.\n5. The denominator for $\\alpha_2$ is $(\\hat{r}_0, v_2) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} -\\frac{12}{5} \\\\ \\frac{6}{5} \\end{pmatrix} = -\\frac{12}{5}$. Thus, $\\alpha_2 = \\frac{\\rho_2}{(\\hat{r}_0, v_2)} = \\frac{6/5}{-12/5} = -\\frac{1}{2}$.\n6. $s_2 = r_1 - \\alpha_2 v_2 = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} - \\left(-\\frac{1}{2}\\right) \\begin{pmatrix} -\\frac{12}{5} \\\\ \\frac{6}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} - \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n7. Since $s_2 = \\mathbf{0}$, we have $t_2 = A s_2 = A \\mathbf{0} = \\mathbf{0}$.\n8. The formula for $\\omega_2$ becomes $\\frac{(t_2, s_2)}{(t_2, t_2)} = \\frac{0}{0}$. This is a so-called \"lucky breakdown\", which indicates that the exact solution will be found at this step. We can set $\\omega_2 = 0$.\n9. $x_2 = x_1 + \\alpha_2 p_2 + \\omega_2 s_2 = \\begin{pmatrix} 1 \\\\ -\\frac{3}{5} \\end{pmatrix} + \\left(-\\frac{1}{2}\\right) \\begin{pmatrix} 6 \\\\ -\\frac{21}{5} \\end{pmatrix} + 0 \\cdot \\mathbf{0} = \\begin{pmatrix} 1 - 3 \\\\ -\\frac{3}{5} + \\frac{21}{10} \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -\\frac{6}{10} + \\frac{21}{10} \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ \\frac{15}{10} \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ \\frac{3}{2} \\end{pmatrix}$.\n10. $r_2 = s_2 - \\omega_2 t_2 = \\mathbf{0} - 0 \\cdot \\mathbf{0} = \\mathbf{0}$.\n\nThe residual $r_2$ is the zero vector, which means the algorithm has converged to the exact solution. This substantiates the second claim. To verify, we check the solution:\n$A x_2 = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} 1(-2) + 2(\\frac{3}{2}) \\\\ 3(-2) + 4(\\frac{3}{2}) \\end{pmatrix} = \\begin{pmatrix} -2 + 3 \\\\ -6 + 6 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = b$.\nThe solution is correct.\n\nThe problem asks for the value of the first smoothing parameter, $\\omega_1$. As calculated in the first iteration, this value is $\\omega_1 = \\frac{1}{5}$.",
            "answer": "$$\n\\boxed{\\frac{1}{5}}\n$$"
        },
        {
            "introduction": "Now that you are familiar with the mechanics of BiCGSTAB, we can explore its primary motivation: robustness. The \"STAB\" in BiCGSTAB stands for \"stabilized,\" which addresses the numerical breakdowns and erratic convergence that can plague its predecessor, the Biconjugate Gradient (BiCG) method. This exercise  guides you through a carefully chosen scenario where BiCG fails due to a loss of bi-orthogonality, but BiCGSTAB successfully proceeds. This comparative analysis is crucial for understanding why BiCGSTAB is often a more reliable tool for solving non-symmetric linear systems.",
            "id": "2376326",
            "problem": "You are asked to demonstrate, from first principles of Krylov subspace iterative methods, how breakdowns in the Bi-Conjugate Gradient (BiCG) method can occur while the Bi-Conjugate Gradient Stabilized (BiCGSTAB) method still converges on the same linear system. Consider the linear system $A x = b$ with\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & 0 & 1\\\\\n1 & 3 & 0\\\\\n0 & 0 & 4\n\\end{pmatrix}, \n\\qquad\nb \\;=\\; \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix},\n\\qquad\nx_0 \\;=\\; \\begin{pmatrix} 0\\\\ 0\\\\ 0 \\end{pmatrix}.\n$$\nFor Bi-Conjugate Gradient (BiCG), set the initial residual $r_0 = b - A x_0$ and the initial shadow residual $\\tilde{r}_0 = r_0$. Using only the definitions of the first update step in BiCG (residuals and shadow residuals defined by single-step Krylov projections with a biorthogonality pairing), explicitly compute $r_1$ and $\\tilde{r}_1$ and verify that $r_1 \\neq 0$, $\\tilde{r}_1 \\neq 0$, and $r_1^{T}\\tilde{r}_1 = 0$, which implies BiCG breakdown at the next step.\n\nNext, apply the Bi-Conjugate Gradient Stabilized (BiCGSTAB) method (Bi-Conjugate Gradient Stabilized) to the same system with the same initial guess $x_0$, the same initial residual $r_0$, and a fixed shadow residual $\\hat{r}$ chosen as\n$$\n\\hat{r} \\;=\\; r_0 + \\begin{pmatrix} 0\\\\ 1\\\\ 0 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 1\\\\ 1\\\\ 0 \\end{pmatrix}.\n$$\nUsing the standard definitions of the BiCGSTAB recurrence (scalar coefficients defined by inner products of $r_{k-1}$, $\\hat{r}$, and Krylov images under $A$), carry out the first BiCGSTAB iteration to compute the smoothing parameter $\\omega_1$.\n\nYour task is to provide the value of the scalar $\\omega_1$. Express your final answer exactly as a reduced fraction. No rounding is required. The final answer must be a single number with no units.",
            "solution": "The posed problem is scientifically grounded, well-posed, and objective. It provides a specific, verifiable numerical example to demonstrate a known phenomenon in computational physics: the breakdown of the Bi-Conjugate Gradient (BiCG) method and the robustness of the Bi-Conjugate Gradient Stabilized (BiCGSTAB) method under certain conditions. All necessary data and definitions are provided, and the task is a direct computational one. The problem is therefore valid.\n\nThe linear system under consideration is $A x = b$, with the givens:\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & 0 & 1\\\\\n1 & 3 & 0\\\\\n0 & 0 & 4\n\\end{pmatrix}, \n\\qquad\nb \\;=\\; \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix},\n\\qquad\nx_0 \\;=\\; \\begin{pmatrix} 0\\\\ 0\\\\ 0 \\end{pmatrix}.\n$$\n\nFirst, we analyze the BiCG method. The initial residual is $r_0 = b - A x_0 = b - 0 = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix}$.\nThe problem specifies setting the initial shadow residual $\\tilde{r}_0 = r_0$. Thus, $\\tilde{r}_0 = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix}$.\nThe BiCG algorithm sets the initial search directions as $p_0 = r_0$ and $\\tilde{p}_0 = \\tilde{r}_0$.\nThe scalar $\\alpha_k$ is computed at each step $k$ as $\\alpha_k = \\frac{\\tilde{r}_k^T r_k}{\\tilde{p}_k^T A p_k}$.\nFor the first step ($k=0$):\nThe numerator is $\\tilde{r}_0^T r_0 = r_0^T r_0 = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} = 1$.\nTo compute the denominator, we first find $A p_0$:\n$A p_0 = A r_0 = \\begin{pmatrix} 2 & 0 & 1\\\\ 1 & 3 & 0\\\\ 0 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2\\\\ 1\\\\ 0 \\end{pmatrix}$.\nThe denominator is then $\\tilde{p}_0^T A p_0 = \\tilde{r}_0^T (A r_0) = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2\\\\ 1\\\\ 0 \\end{pmatrix} = 2$.\nThus, $\\alpha_0 = \\frac{1}{2}$.\n\nThe updated residuals $r_1$ and $\\tilde{r}_1$ are computed as follows:\n$r_1 = r_0 - \\alpha_0 A p_0 = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 2\\\\ 1\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1\\\\ 1/2\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0\\\\ -1/2\\\\ 0 \\end{pmatrix}$.\n$\\tilde{r}_1 = \\tilde{r}_0 - \\alpha_0 A^T \\tilde{p}_0$. The transpose of $A$ is $A^T = \\begin{pmatrix} 2 & 1 & 0\\\\ 0 & 3 & 0\\\\ 1 & 0 & 4 \\end{pmatrix}$.\n$A^T \\tilde{p}_0 = A^T \\tilde{r}_0 = \\begin{pmatrix} 2 & 1 & 0\\\\ 0 & 3 & 0\\\\ 1 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2\\\\ 0\\\\ 1 \\end{pmatrix}$.\n$\\tilde{r}_1 = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 2\\\\ 0\\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1\\\\ 0\\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 0\\\\ 0\\\\ -1/2 \\end{pmatrix}$.\n\nWe verify the conditions stated in the problem:\n1. $r_1 = \\begin{pmatrix} 0\\\\ -1/2\\\\ 0 \\end{pmatrix} \\neq 0$.\n2. $\\tilde{r}_1 = \\begin{pmatrix} 0\\\\ 0\\\\ -1/2 \\end{pmatrix} \\neq 0$.\n3. $r_1^T \\tilde{r}_1 = \\begin{pmatrix} 0 & -1/2 & 0 \\end{pmatrix} \\begin{pmatrix} 0\\\\ 0\\\\ -1/2 \\end{pmatrix} = (0)(0) + (-1/2)(0) + (0)(-1/2) = 0$.\n\nThe next step in the BiCG algorithm would be to compute $\\beta_0 = \\frac{\\tilde{r}_1^T r_1}{\\tilde{r}_0^T r_0} = \\frac{0}{1} = 0$.\nThis leads to new search directions $p_1 = r_1 + \\beta_0 p_0 = r_1$ and $\\tilde{p}_1 = \\tilde{r}_1 + \\beta_0 \\tilde{p}_0 = \\tilde{r}_1$.\nThen, the next scalar $\\alpha_1$ is computed as $\\alpha_1 = \\frac{\\tilde{r}_1^T r_1}{\\tilde{p}_1^T A p_1} = \\frac{0}{\\tilde{r}_1^T A r_1}$.\nThe denominator term is $\\tilde{r}_1^T A r_1 = \\begin{pmatrix} 0 & 0 & -1/2 \\end{pmatrix} \\begin{pmatrix} 2 & 0 & 1\\\\ 1 & 3 & 0\\\\ 0 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 0\\\\ -1/2\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & -2 \\end{pmatrix} \\begin{pmatrix} 0\\\\ -1/2\\\\ 0 \\end{pmatrix} = 0$.\nSince both the numerator and denominator for $\\alpha_1$ are zero, this constitutes a serious breakdown of the BiCG algorithm.\n\nNext, we apply the BiCGSTAB method. The initial residual is $r_0 = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix}$. The fixed shadow residual is given as $\\hat{r} = \\begin{pmatrix} 1\\\\ 1\\\\ 0 \\end{pmatrix}$.\nThe standard algorithm starts with initial values $\\rho_0=1$, $\\alpha_0=1$, $\\omega_0=1$, $p_0=0$, and $v_0=0$. We perform the first iteration ($k=1$).\n\n1. Compute $\\rho_1 = \\hat{r}^T r_0$:\n$\\rho_1 = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} = 1$.\n\n2. Compute $\\beta_1 = \\frac{\\rho_1}{\\rho_0} \\frac{\\alpha_0}{\\omega_0}$:\n$\\beta_1 = \\frac{1}{1} \\cdot \\frac{1}{1} = 1$.\n\n3. Compute $p_1 = r_0 + \\beta_1 (p_0 - \\omega_0 v_0)$:\n$p_1 = r_0 + 1 (0 - 1 \\cdot 0) = r_0 = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix}$.\n\n4. Compute $v_1 = A p_1$:\n$v_1 = A r_0 = \\begin{pmatrix} 2 & 0 & 1\\\\ 1 & 3 & 0\\\\ 0 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2\\\\ 1\\\\ 0 \\end{pmatrix}$.\n\n5. Compute $\\alpha_1 = \\frac{\\rho_1}{\\hat{r}^T v_1}$:\n$\\hat{r}^T v_1 = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2\\\\ 1\\\\ 0 \\end{pmatrix} = (1)(2) + (1)(1) + (0)(0) = 3$.\n$\\alpha_1 = \\frac{1}{3}$.\n\n6. Compute the temporary residual $s_1 = r_0 - \\alpha_1 v_1$:\n$s_1 = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 2\\\\ 1\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 - 2/3\\\\ 0 - 1/3\\\\ 0 - 0 \\end{pmatrix} = \\begin{pmatrix} 1/3\\\\ -1/3\\\\ 0 \\end{pmatrix}$.\n\n7. Compute $t_1 = A s_1$:\n$t_1 = \\begin{pmatrix} 2 & 0 & 1\\\\ 1 & 3 & 0\\\\ 0 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1/3\\\\ -1/3\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2(1/3) + 0(-1/3) + 1(0) \\\\ 1(1/3) + 3(-1/3) + 0(0) \\\\ 0(1/3) + 0(-1/3) + 4(0) \\end{pmatrix} = \\begin{pmatrix} 2/3\\\\ 1/3 - 1\\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2/3\\\\ -2/3\\\\ 0 \\end{pmatrix}$.\n\n8. Compute the smoothing parameter $\\omega_1 = \\frac{t_1^T s_1}{t_1^T t_1}$:\nThe numerator is $t_1^T s_1 = \\begin{pmatrix} 2/3 & -2/3 & 0 \\end{pmatrix} \\begin{pmatrix} 1/3\\\\ -1/3\\\\ 0 \\end{pmatrix} = (\\frac{2}{3})(\\frac{1}{3}) + (\\frac{-2}{3})(\\frac{-1}{3}) = \\frac{2}{9} + \\frac{2}{9} = \\frac{4}{9}$.\nThe denominator is $t_1^T t_1 = \\begin{pmatrix} 2/3 & -2/3 & 0 \\end{pmatrix} \\begin{pmatrix} 2/3\\\\ -2/3\\\\ 0 \\end{pmatrix} = (\\frac{2}{3})^2 + (\\frac{-2}{3})^2 = \\frac{4}{9} + \\frac{4}{9} = \\frac{8}{9}$.\n$\\omega_1 = \\frac{4/9}{8/9} = \\frac{4}{8} = \\frac{1}{2}$.\n\nThe value of the smoothing parameter $\\omega_1$ is $\\frac{1}{2}$. This demonstrates that the BiCGSTAB algorithm, with a suitable choice of $\\hat{r}$, does not break down and proceeds with the computation.",
            "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$"
        },
        {
            "introduction": "The ultimate test of understanding an algorithm is transitioning from theory to a functional implementation. This final practice challenges you to write your own BiCGSTAB solver and, more importantly, to think like a computational scientist by diagnosing a subtle but common implementation bug . The task involves creating a \"faulty\" version of the code and then devising a numerical test to specifically detect this error. This hands-on coding and debugging exercise builds a deeper appreciation for the mathematical invariants that ensure an algorithm's correctness and equips you with the critical skills needed to verify your own scientific software.",
            "id": "2376330",
            "problem": "You are asked to implement and analyze the Biconjugate Gradient Stabilized (BiCGSTAB) method for solving linear systems in computational physics. The central task is to deliberately implement a specific, common bug in BiCGSTAB and devise a numerical test that reliably detects this bug from first principles.\n\nStarting point and fundamental base:\n- Consider a linear system $A x = b$ over $\\mathbb{R}^{n}$, with $A \\in \\mathbb{R}^{n \\times n}$ nonsingular.\n- Krylov subspace methods construct approximate solutions in nested subspaces generated by successive powers of $A$ acting on the initial residual $r_0 = b - A x_0$.\n- The BiCGSTAB method is a Krylov subspace method designed for nonsymmetric $A$ that uses bi-orthogonality with a fixed shadow residual $\\tilde{r}$. A key invariant is that $\\tilde{r}$ is held fixed (often chosen as $\\tilde{r} = r_0$), and inner products of the form $\\rho_k = \\langle \\tilde{r}, r_{k-1} \\rangle$ are used to maintain a consistent bi-orthogonal recurrence for the search directions.\n\nDefinition of the specific bug:\n- In correct BiCGSTAB, the shadow residual $\\tilde{r}$ is fixed as $\\tilde{r} = r_0$ and the scalars $\\rho_k$ are formed as $\\rho_k = \\langle \\tilde{r}, r_{k-1} \\rangle$ at iteration index $k \\ge 1$.\n- A common bug is to silently update the shadow residual each iteration, effectively using $\\tilde{r}_k = r_k$ and then forming $\\rho_k$ from $\\langle \\tilde{r}_k, r_k \\rangle = \\langle r_k, r_k \\rangle$ instead of the intended $\\langle \\tilde{r}_{k-1}, r_{k-1} \\rangle$. This conflates indexing and breaks the bi-orthogonality the method depends on.\n\nYour tasks:\n- Implement a correct BiCGSTAB solver for real matrices using a fixed shadow residual $\\tilde{r} = r_0$, and a \"faulty\" BiCGSTAB variant that commits the bug by overwriting the shadow residual each iteration so that $\\rho_k = \\langle r_k, r_k \\rangle$ is effectively used.\n- From first principles, devise a numerical detection test for this specific bug, based on the logical consequences of using a fixed $\\tilde{r}$ versus an updated $\\tilde{r}_k$. The test must not rely on external references; it must use quantities computable during iterations. A robust signature is that for a fixed $\\tilde{r}$ one has $\\rho_2 = \\langle \\tilde{r}, r_1 \\rangle = \\langle r_0, r_1 \\rangle$, whereas the faulty variant produces $\\rho_2^{\\mathrm{faulty}} = \\langle r_1, r_1 \\rangle$. For generic problems where $r_1$ is not colinear with $r_0$, these two values differ markedly, and the faulty variant typically also shows inferior convergence within a fixed iteration budget.\n\nProgram requirements:\n- Your program must construct and solve the following four linear systems with the same initial guess $x_0 = 0$ and right-hand side $b = A \\mathbf{1}$, where $\\mathbf{1}$ is the vector of all ones:\n  1. Case $1$ (nonsymmetric, diagonally dominant): $n = 20$, $A$ is tridiagonal with main diagonal entries $4$, subdiagonal entries $-1$, superdiagonal entries $-1$, plus an added strictly upper bidiagonal with entries $0.05$ (to make $A$ nonsymmetric but still diagonally dominant).\n  2. Case $2$ (symmetric positive definite): $n = 20$, $A$ is tridiagonal with main diagonal entries $2$ and both sub/superdiagonals entries $-1$ (the standard $1$D Poisson stencil).\n  3. Case $3$ (dense, well-conditioned shift): $n = 16$, construct a dense matrix $M$ with reproducible entries, then set $A = M + 5 I$, where $I$ is the identity, making $A$ strictly diagonally dominant.\n  4. Case $4$ (nonsymmetric, nonnormal tridiagonal): $n = 20$, $A$ is tridiagonal with main diagonal entries $3$, subdiagonal entries $0.5$, and superdiagonal entries $1.5$.\n\n- For each case:\n  - Run the correct BiCGSTAB with tolerance $\\varepsilon = 10^{-10}$ on the relative residual $\\|r_k\\|_2 / \\|b\\|_2$ and a maximum of $200$ iterations.\n  - Run the faulty BiCGSTAB with the same parameters.\n  - Compute a boolean decision that declares the bug detected if and only if both of the following hold:\n    - The correct solver achieves $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$ within the iteration limit, and the faulty solver fails to do so within the same iteration limit, or the second inner-product signature detects the mismatch, namely $|\\rho_2^{\\mathrm{faulty}} - \\rho_2^{\\mathrm{correct}}|$ is larger than a relative threshold $\\tau = 10^{-8}$ when scaled by $|\\rho_2^{\\mathrm{correct}}| + 1$.\n    - This test is computed purely from numerical quantities produced by the two runs (residual norms and the first few $\\rho_k$ values).\n- The final output must be a single line containing a list of four booleans corresponding to cases $1$ through $4$, for example: \"[True,False,True,True]\".\n\nAngle units are not applicable. There are no physical units in this problem. All reported tolerances must be treated as dimensionless real numbers. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each result is a boolean corresponding to the detection outcome for its test case.",
            "solution": "The problem requires the implementation and analysis of the Biconjugate Gradient Stabilized (BiCGSTAB) algorithm, a pivotal iterative solver for nonsymmetric linear systems in computational physics. The task is to implement both a correct version of the algorithm and a version containing a specific, common implementation bug. Subsequently, a numerical test must be devised and implemented to detect this bug from first principles.\n\nThe analysis begins with the validation of the problem statement.\n\n**Problem Validation**\n\n**Step 1: Extracted Givens**\n- **Linear System**: Solve $A x = b$ for $x \\in \\mathbb{R}^{n}$, where $A \\in \\mathbb{R}^{n \\times n}$ is nonsingular.\n- **Initial Conditions**: Initial guess $x_0 = 0$; initial residual $r_0 = b - A x_0 = b$.\n- **Correct BiCGSTAB**: Uses a fixed shadow residual $\\tilde{r} = r_0$. The scalar $\\rho_k$ is computed as $\\rho_k = \\langle \\tilde{r}, r_{k-1} \\rangle$ for iteration $k \\ge 1$.\n- **Faulty BiCGSTAB**: Erroneously updates the shadow residual, leading to an effective computation of $\\rho_k$ as $\\rho_k = \\langle r_{k-1}, r_{k-1} \\rangle$. For $k=2$, this means $\\rho_2^{\\mathrm{faulty}} = \\langle r_1, r_1 \\rangle$ instead of the correct $\\rho_2^{\\mathrm{correct}} = \\langle r_0, r_1 \\rangle$.\n- **Solver Parameters**: Tolerance $\\varepsilon = 10^{-10}$ on the relative residual norm $\\|r_k\\|_2 / \\|b\\|_2$, and a maximum of $200$ iterations.\n- **Bug Detection Criteria**: A bug is detected if `(correct solver converges AND faulty solver fails to converge) OR (the numerical signature mismatches)`.\n    - Convergence signature: The correct implementation converges within the iteration limit while the faulty one does not.\n    - Numerical signature: $|\\rho_2^{\\mathrm{faulty}} - \\rho_2^{\\mathrm{correct}}| > \\tau \\cdot (|\\rho_2^{\\mathrm{correct}}| + 1)$, with threshold $\\tau = 10^{-8}$.\n- **Test Cases**:\n    1.  **Case 1**: $n = 20$. Nonsymmetric, diagonally dominant tridiagonal matrix. Main diagonal entries are $4$, subdiagonal are $-1$, superdiagonal are $-1$, with an added perturbation of $0.05$ on the superdiagonal.\n    2.  **Case 2**: $n = 20$. Symmetric positive definite (SPD) tridiagonal matrix (1D Poisson stencil). Main diagonal is $2$, off-diagonals are $-1$.\n    3.  **Case 3**: $n = 16$. Dense, well-conditioned matrix $A = M + 5I$, where $M$ has reproducible entries.\n    4.  **Case 4**: $n = 20$. Nonsymmetric, nonnormal tridiagonal matrix. Main diagonal is $3$, subdiagonal is $0.5$, superdiagonal is $1.5$.\n    For all cases, $b = A\\mathbf{1}$, where $\\mathbf{1}$ is the vector of all ones.\n- **Output**: A single-line list of four booleans indicating the detection result for each case.\n\n**Step 2: Validation Verdict**\nThe problem is scientifically grounded, well-posed, and objective. It concerns a standard algorithm from numerical linear algebra (BiCGSTAB) and a plausible implementation error. All parameters and test cases are specified with sufficient clarity to permit a unique and reproducible solution. The term \"reproducible entries\" for Case 3 is an implicit directive to use a deterministic function, a standard practice in numerical experiments. The detection logic is unambiguous. The problem is therefore deemed **valid**.\n\n**Principle-Based Solution Design**\n\nThe BiCGSTAB algorithm generates a sequence of approximate solutions $x_k$ to the linear system $A x = b$. Its key feature, which distinguishes it from methods like the Conjugate Gradient (CG), is its applicability to nonsymmetric matrices. This is achieved by generating two sequences of vectors, $p_k$ and $q_k$, which are made bi-orthogonal. BiCGSTAB improves upon the Biconjugate Gradient (BiCG) method by smoothing its often erratic convergence behavior.\n\n**The Correct BiCGSTAB Algorithm** ($k = 1, 2, ...$)\n1.  Initialize: $r_0 = b - A x_0$; choose a shadow residual $\\tilde{r}$ (typically $\\tilde{r} = r_0$); set $p_0=v_0=0, \\rho_0=\\alpha_0=\\omega_0=1$.\n2.  Compute $\\rho_k = \\langle \\tilde{r}, r_{k-1} \\rangle$. Breakdown occurs if $\\rho_k=0$.\n3.  Update search direction: $\\beta_k = (\\rho_k/\\rho_{k-1})(\\alpha_{k-1}/\\omega_{k-1})$; $p_k = r_{k-1} + \\beta_k(p_{k-1} - \\omega_{k-1}v_{k-1})$. For $k=1$, $\\beta_1=0$ and $p_1=r_0$.\n4.  $v_k = A p_k$.\n5.  Compute step length: $\\alpha_k = \\rho_k / \\langle \\tilde{r}, v_k \\rangle$. Breakdown occurs if the denominator is zero.\n6.  First update: $s_k = r_{k-1} - \\alpha_k v_k$; $x_{k-1/2} = x_{k-1} + \\alpha_k p_k$.\n7.  Stabilization step: $t_k = A s_k$.\n8.  Compute stabilization parameter: $\\omega_k = \\langle t_k, s_k \\rangle / \\langle t_k, t_k \\rangle$.\n9.  Second update: $x_k = x_{k-1/2} + \\omega_k s_k = x_{k-1} + \\alpha_k p_k + \\omega_k s_k$.\n10. Update residual: $r_k = s_k - \\omega_k t_k$. Check for convergence on $\\|r_k\\|_2$.\n\nThe bi-orthogonality depends on the consistent use of the *fixed* shadow residual $\\tilde{r}$ in the computation of both $\\rho_k$ and $\\alpha_k$.\n\n**The Defined Bug and Its Consequence**\nThe specified bug involves incorrectly computing $\\rho_k$. Instead of the bi-orthogonal inner product $\\rho_k = \\langle \\tilde{r}, r_{k-1} \\rangle$, the faulty implementation computes an inner product based on the current residual only: $\\rho_k^{\\mathrm{faulty}} = \\langle r_{k-1}, r_{k-1} \\rangle$.\n\nThis error breaks the theoretical foundation of the algorithm. The recurrence relation for the search directions $p_k$ is no longer guaranteed to maintain bi-orthogonality, which can lead to degraded or failed convergence.\n\n**The Detection Mechanism**\nThe detection strategy is twofold, combining a performance-based check with a direct numerical signature test.\n1.  **Convergence Test**: For many problems, particularly nonsymmetric ones, the faulty algorithm will converge much more slowly than the correct one, or fail entirely. The test checks if the correct solver succeeds while the buggy one fails within the given iteration budget. This is a practical, but not universal, indicator.\n2.  **Signature Test**: A more direct and robust test arises from the definition of $\\rho_k$. We inspect the value of $\\rho_2$:\n    - Correct: $\\rho_2^{\\mathrm{correct}} = \\langle \\tilde{r}, r_1 \\rangle = \\langle r_0, r_1 \\rangle$.\n    - Faulty: $\\rho_2^{\\mathrm{faulty}} = \\langle r_1, r_1 \\rangle = \\|r_1\\|_2^2$.\n    Geometrically, $\\langle r_0, r_1 \\rangle$ is the projection of $r_1$ onto $r_0$ (scaled), while $\\|r_1\\|_2^2$ is the squared magnitude of $r_1$. Unless $r_1$ is collinear with $r_0$ (which is not true in general), these two values will differ. The test $|\\rho_2^{\\mathrm{faulty}} - \\rho_2^{\\mathrm{correct}}| > \\tau (|\\rho_2^{\\mathrm{correct}}| + 1)$ provides a quantitative measure of this discrepancy. For an SPD matrix (Case 2), the faulty algorithm's computation of $\\rho_k$ resembles that of the Conjugate Gradient method, and it may perform well. In such cases, the signature test becomes essential for detection, as the convergence behavior might not be a reliable indicator of the bug.\n\nThe implementation will proceed by creating a single BiCGSTAB function with a flag to switch between the correct and faulty computation of $\\rho_k$. The four specified test cases will be constructed, and both versions of the solver will be run. The detection logic will then be applied to the results of these runs to produce the final boolean output for each case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef bicgstab(A, b, x0, tol, max_iter, is_faulty):\n    \"\"\"\n    Implementation of the Biconjugate Gradient Stabilized (BiCGSTAB) method.\n    \n    This function can run a correct version or a version with a specific bug\n    related to the computation of the rho scalar.\n    \n    Args:\n        A (np.ndarray): The coefficient matrix of the linear system.\n        b (np.ndarray): The right-hand side vector of the linear system.\n        x0 (np.ndarray): The initial guess for the solution.\n        tol (float): The convergence tolerance for the relative residual.\n        max_iter (int): The maximum number of iterations.\n        is_faulty (bool): If True, a bug in the rho calculation is introduced.\n        \n    Returns:\n        A dictionary containing the convergence status ('converged'), the number\n        of iterations performed ('iters'), and the first two rho values computed\n        ('rhos').\n    \"\"\"\n    n = len(b)\n    x = x0.copy()\n    r = b - A @ x\n    \n    # In the correct algorithm, r_tilde is fixed as the initial residual r0.\n    # The faulty algorithm does not use a fixed r_tilde correctly.\n    r_tilde = r.copy() \n\n    rho_prev = 1.0\n    alpha = 1.0\n    omega = 1.0\n    p = np.zeros(n)\n    v = np.zeros(n)\n\n    rhos = []\n\n    b_norm = np.linalg.norm(b)\n    if b_norm == 0.0:\n        b_norm = 1.0\n    \n    if np.linalg.norm(r) / b_norm < tol:\n        return {\"converged\": True, \"iters\": 0, \"rhos\": []}\n\n    for k in range(1, max_iter + 1):\n        if is_faulty:\n            # Buggy version: rho_k = <r_{k-1}, r_{k-1}>\n            # At the start of iteration k, the variable 'r' holds r_{k-1}.\n            rho_curr = np.dot(r, r)\n        else:\n            # Correct version: rho_k = <r_tilde_0, r_{k-1}>\n            rho_curr = np.dot(r_tilde, r)\n\n        # Store the first two rho values for the detection signature\n        if k <= 2:\n            rhos.append(rho_curr)\n\n        # Breakdown condition 1: stagnation or exact solution\n        if abs(rho_curr) < np.finfo(float).eps:\n            return {\"converged\": False, \"iters\": k, \"rhos\": rhos}\n\n        if k == 1:\n            p = r\n        else:\n            # A safe division for beta\n            if abs(rho_prev) < np.finfo(float).eps or abs(omega) < np.finfo(float).eps:\n                return {\"converged\": False, \"iters\": k, \"rhos\": rhos}\n            beta = (rho_curr / rho_prev) * (alpha / omega)\n            p = r + beta * (p - omega * v)\n\n        v = A @ p\n\n        r_tilde_dot_v = np.dot(r_tilde, v)\n        \n        # Breakdown condition 2\n        if abs(r_tilde_dot_v) < np.finfo(float).eps:\n            return {\"converged\": False, \"iters\": k, \"rhos\": rhos}\n        \n        alpha = rho_curr / r_tilde_dot_v\n        \n        s = r - alpha * v\n        \n        t = A @ s\n\n        t_dot_t = np.dot(t, t)\n\n        if abs(t_dot_t) < np.finfo(float).eps:\n            omega = 0.0\n        else:\n            omega = np.dot(t, s) / t_dot_t\n\n        x += alpha * p + omega * s\n        r = s - omega * t\n\n        rho_prev = rho_curr\n\n        if np.linalg.norm(r) / b_norm < tol:\n            return {\"converged\": True, \"iters\": k, \"rhos\": rhos}\n\n    return {\"converged\": False, \"iters\": max_iter, \"rhos\": rhos}\n\n\ndef construct_matrices():\n    \"\"\"\n    Constructs the four test case matrices as specified in the problem.\n    \"\"\"\n    # Case 1: nonsymmetric, diagonally dominant\n    n1 = 20\n    diag1 = np.full(n1, 4.0)\n    sub1 = np.full(n1 - 1, -1.0)\n    sup1 = np.full(n1 - 1, -1.0 + 0.05)\n    A1 = np.diag(diag1) + np.diag(sub1, k=-1) + np.diag(sup1, k=1)\n\n    # Case 2: symmetric positive definite (1D Poisson)\n    n2 = 20\n    diag2 = np.full(n2, 2.0)\n    offdiag2 = np.full(n2 - 1, -1.0)\n    A2 = np.diag(diag2) + np.diag(offdiag2, k=-1) + np.diag(offdiag2, k=1)\n\n    # Case 3: dense, well-conditioned shift\n    n3 = 16\n    i, j = np.ogrid[:n3, :n3]\n    M3 = np.sin(i * 0.5 + j * 0.3) + np.cos(i * j)\n    A3 = M3 + 5.0 * np.eye(n3)\n\n    # Case 4: nonsymmetric, nonnormal tridiagonal\n    n4 = 20\n    diag4 = np.full(n4, 3.0)\n    sub4 = np.full(n4 - 1, 0.5)\n    sup4 = np.full(n4 - 1, 1.5)\n    A4 = np.diag(diag4) + np.diag(sub4, k=-1) + np.diag(sup4, k=1)\n    \n    return [\n        {'A': A1, 'n': n1},\n        {'A': A2, 'n': n2},\n        {'A': A3, 'n': n3},\n        {'A': A4, 'n': n4},\n    ]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = construct_matrices()\n    \n    results = []\n    \n    tol = 1e-10\n    max_iter = 200\n    tau = 1e-8\n\n    for case_data in test_cases:\n        A = case_data['A']\n        n = case_data['n']\n        \n        b = A @ np.ones(n)\n        x0 = np.zeros(n)\n        \n        correct_run = bicgstab(A, b, x0, tol, max_iter, is_faulty=False)\n        faulty_run = bicgstab(A, b, x0, tol, max_iter, is_faulty=True)\n        \n        correct_converged = correct_run['converged']\n        faulty_converged = faulty_run['converged']\n\n        convergence_failure_detected = correct_converged and not faulty_converged\n        \n        signature_mismatch = False\n        if len(correct_run['rhos']) >= 2 and len(faulty_run['rhos']) >= 2:\n            rho2_correct = correct_run['rhos'][1]\n            rho2_faulty = faulty_run['rhos'][1]\n            \n            if abs(rho2_faulty - rho2_correct) > tau * (abs(rho2_correct) + 1.0):\n                signature_mismatch = True\n\n        bug_detected = convergence_failure_detected or signature_mismatch\n        results.append(bug_detected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}