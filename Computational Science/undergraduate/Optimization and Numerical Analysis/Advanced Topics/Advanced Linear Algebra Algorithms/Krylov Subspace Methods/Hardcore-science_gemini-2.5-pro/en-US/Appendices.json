{
    "hands_on_practices": [
        {
            "introduction": "Before diving into complex algorithms, it's crucial to understand the foundational structure they operate on: the Krylov subspace. The dimension of this subspace, spanned by successive applications of the matrix $A$ to a starting vector $b$, dictates the maximum number of iterations an ideal Krylov method needs. This practice will have you directly compute the vectors that form a Krylov subspace and determine its dimension, giving you a concrete feel for the space at the heart of these methods .",
            "id": "2183348",
            "problem": "Consider the real matrix $A$ and the real column vector $b$ given by:\n$$A = \\begin{pmatrix} 1  1  0 \\\\ 1  2  1 \\\\ 0  1  1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 4 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\nThe Krylov subspace of order $m$ generated by $A$ and $b$, denoted as $\\mathcal{K}_m(A, b)$, is the linear subspace spanned by the vectors $\\{b, Ab, A^2b, \\dots, A^{m-1}b\\}$. Determine the dimension of the Krylov subspace $\\mathcal{K}_3(A, b)$.",
            "solution": "The Krylov subspace $\\mathcal{K}_{3}(A,b)$ is the span of $\\{b, Ab, A^{2}b\\}$. To determine its dimension, we compute these vectors and check their linear independence.\nGiven\n$$\nA=\\begin{pmatrix}110\\\\121\\\\011\\end{pmatrix},\\quad b=\\begin{pmatrix}4\\\\1\\\\0\\end{pmatrix},\n$$\nwe compute\n$$\nAb=A\\begin{pmatrix}4\\\\1\\\\0\\end{pmatrix}=\\begin{pmatrix}1\\cdot 4+1\\cdot 1+0\\cdot 0\\\\1\\cdot 4+2\\cdot 1+1\\cdot 0\\\\0\\cdot 4+1\\cdot 1+1\\cdot 0\\end{pmatrix}=\\begin{pmatrix}5\\\\6\\\\1\\end{pmatrix}.\n$$\nNext,\n$$\nA^{2}b=A(Ab)=A\\begin{pmatrix}5\\\\6\\\\1\\end{pmatrix}=\\begin{pmatrix}1\\cdot 5+1\\cdot 6+0\\cdot 1\\\\1\\cdot 5+2\\cdot 6+1\\cdot 1\\\\0\\cdot 5+1\\cdot 6+1\\cdot 1\\end{pmatrix}=\\begin{pmatrix}11\\\\18\\\\7\\end{pmatrix}.\n$$\nForm the matrix with these vectors as columns:\n$$\nK=\\begin{pmatrix}4511\\\\1618\\\\017\\end{pmatrix}.\n$$\nThe vectors are linearly independent if and only if $\\det(K)\\neq 0$. Compute\n$$\n\\det(K)=4\\begin{vmatrix}618\\\\17\\end{vmatrix}-5\\begin{vmatrix}118\\\\07\\end{vmatrix}+11\\begin{vmatrix}16\\\\01\\end{vmatrix}\n=4(42-18)-5(7)+11(1)=4(24)-35+11=96-35+11=72\\neq 0.\n$$\nTherefore, $\\{b, Ab, A^{2}b\\}$ is linearly independent, and the dimension of $\\mathcal{K}_{3}(A,b)$ is $3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "For the important class of symmetric matrices, the Lanczos algorithm provides a remarkably efficient way to build an orthonormal basis for the Krylov subspace. This process doesn't just produce a basis; it simultaneously generates a small, symmetric tridiagonal matrix $T$ that represents the action of the original large matrix $A$ on the subspace. This exercise walks you through the first iteration of the Lanczos algorithm, allowing you to compute the very first entries of this simplified tridiagonal matrix .",
            "id": "2183327",
            "problem": "The Lanczos algorithm is an iterative method that transforms a symmetric matrix $A$ into a symmetric tridiagonal matrix $T$. The elements of $T$ consist of diagonal entries $\\alpha_j$ and off-diagonal entries $\\beta_j$. The algorithm starts with an initial vector $b$ and generates a sequence of orthonormal vectors $q_j$ known as Lanczos vectors.\n\nLet the symmetric matrix $A$ and the starting vector $b$ be defined as:\n$$A = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n\nPerform the first iteration of the Lanczos algorithm to find the first diagonal element, $\\alpha_1$, and the first off-diagonal element, $\\beta_1$, of the corresponding tridiagonal matrix $T$. The process begins with the normalization of $b$ to obtain the first Lanczos vector, $q_1$.\n\nReport your answer as an ordered pair for $(\\alpha_1, \\beta_1)$.",
            "solution": "The Lanczos algorithm generates a sequence of orthonormal vectors $q_j$ and a symmetric tridiagonal matrix $T$. We are asked to find the first entries of this matrix, $\\alpha_1$ and $\\beta_1$.\n\n**Step 1: Initialization**\n\nThe first step is to normalize the starting vector $b$ to obtain the first Lanczos vector $q_1$.\nThe given starting vector is $b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nWe calculate its Euclidean norm, $\\|b\\|_2$:\n$$\\|b\\|_2 = \\sqrt{1^2 + 1^2} = \\sqrt{2}$$\nNow, we find $q_1$ by dividing $b$ by its norm:\n$$q_1 = \\frac{b}{\\|b\\|_2} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n\n**Step 2: First Iteration (j=1)**\n\nThe core of the Lanczos iteration involves computing the elements $\\alpha_j$ and $\\beta_j$. For the first iteration ($j=1$), the algorithm proceeds as follows.\n\nFirst, we compute the vector $v$ by applying the matrix $A$ to $q_1$:\n$$v = A q_1 = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2(1) + 1(1) \\\\ 1(1) + 3(1) \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$$\n\nNext, we compute the first diagonal element, $\\alpha_1$. This is found by taking the inner product of $v$ with $q_1$:\n$$\\alpha_1 = q_1^T v$$\nSubstituting the expressions for $q_1$ and $v$:\n$$\\alpha_1 = \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  1 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} \\right) = \\frac{1}{(\\sqrt{2})^2} (1 \\cdot 3 + 1 \\cdot 4) = \\frac{1}{2}(7) = \\frac{7}{2}$$\n\nAfter finding $\\alpha_1$, we compute the unnormalized residual vector, $r_1$, which will be used to find $\\beta_1$ and $q_2$. This vector is made orthogonal to $q_1$ by subtracting the projection of $v$ onto $q_1$:\n$$r_1 = v - \\alpha_1 q_1$$\nSubstituting the known values:\n$$r_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} - \\frac{7}{2} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right)$$\n$$r_1 = \\frac{1}{\\sqrt{2}} \\left[ \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} - \\frac{7}{2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right] = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3 - \\frac{7}{2} \\\\ 4 - \\frac{7}{2} \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}$$\n\nFinally, the first off-diagonal element, $\\beta_1$, is the Euclidean norm of this residual vector $r_1$:\n$$\\beta_1 = \\|r_1\\|_2$$\n$$\\beta_1 = \\left\\| \\frac{1}{\\sqrt{2}} \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\right\\|_2 = \\frac{1}{\\sqrt{2}} \\left\\| \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\right\\|_2 = \\frac{1}{\\sqrt{2}} \\sqrt{\\left(-\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2}$$\n$$\\beta_1 = \\frac{1}{\\sqrt{2}} \\sqrt{\\frac{1}{4} + \\frac{1}{4}} = \\frac{1}{\\sqrt{2}} \\sqrt{\\frac{2}{4}} = \\frac{1}{\\sqrt{2}} \\sqrt{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}} \\frac{1}{\\sqrt{2}} = \\frac{1}{2}$$\n\nThe first iteration is now complete. The requested values are $\\alpha_1 = \\frac{7}{2}$ and $\\beta_1 = \\frac{1}{2}$.",
            "answer": "$$\\boxed{(\\frac{7}{2}, \\frac{1}{2})}$$"
        },
        {
            "introduction": "When dealing with non-symmetric systems, the Arnoldi process generalizes the Lanczos iteration, but choosing the 'best' solution within the resulting Krylov subspace becomes more nuanced. This exercise compares two cornerstone methods: the Full Orthogonalization Method (FOM) and the Generalized Minimal Residual Method (GMRES), which use different criteria to define the optimal solution. By calculating the final residual for both, you will directly observe the key property of GMRES that guarantees it finds the approximation with the smallest possible residual norm at each step .",
            "id": "2183323",
            "problem": "Consider the linear system $Ax=b$, where\n$$ A = \\begin{pmatrix} 1  1  0 \\\\ 0  1  1 \\\\ 1  0  1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}. $$\nWe want to find an approximate solution using two-step ($m=2$) Krylov subspace methods starting with an initial guess $x_0 = \\vec{0}$. The two methods are the Full Orthogonalization Method (FOM) and the Generalized Minimal Residual Method (GMRES).\n\nBoth methods start by constructing an orthonormal basis $\\{v_1, v_2\\}$ for the Krylov subspace $\\mathcal{K}_2(A, r_0)$, where $r_0 = b - Ax_0$, using the Arnoldi process. This process yields the relation $AV_2 = V_3 \\tilde{H}_2$, where $V_k = [v_1, \\dots, v_k]$ is the matrix with the basis vectors as columns, and $\\tilde{H}_2$ is a $3 \\times 2$ upper Hessenberg matrix. Let $H_2$ be the $2 \\times 2$ matrix obtained by removing the last row of $\\tilde{H}_2$.\n\nThe approximate solution after two steps is given by $x_2 = x_0 + V_2 y_2$, where $y_2$ is a vector in $\\mathbb{R}^2$.\n- For FOM(2), the vector $y_2^{\\text{FOM}}$ is found by solving the system $H_2 y = \\|r_0\\|_2 e_1$, where $e_1 = [1, 0]^T$.\n- For GMRES(2), the vector $y_2^{\\text{GMRES}}$ is found by solving the least-squares problem $\\min_{y \\in \\mathbb{R}^2} \\| \\tilde{H}_2 y - \\|r_0\\|_2 e_1 \\|_2$, where $e_1 = [1, 0, 0]^T$.\n\nThe final residual vectors are $r_2^{\\text{FOM}} = b - Ax_2^{\\text{FOM}}$ and $r_2^{\\text{GMRES}} = b - Ax_2^{\\text{GMRES}}$.\n\nCalculate the 2-norms of the final residual vectors for both methods, $\\|r_2^{\\text{FOM}}\\|_2$ and $\\|r_2^{\\text{GMRES}}\\|_2$. Express your answer as a pair of exact analytical values $(\\|r_2^{\\text{FOM}}\\|_2, \\|r_2^{\\text{GMRES}}\\|_2)$.",
            "solution": "We have the linear system $Ax=b$ with\n$$\nA=\\begin{pmatrix}110\\\\011\\\\101\\end{pmatrix},\\quad b=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix},\n$$\ninitial guess $x_{0}=\\vec{0}$, and $r_{0}=b-Ax_{0}=b$. Hence $\\beta=\\|r_{0}\\|_{2}=1$ and $v_{1}=r_{0}/\\beta=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}$.\n\nWe run two steps of the Arnoldi process to build an orthonormal basis $\\{v_{1},v_{2}\\}$ of $\\mathcal{K}_{2}(A,r_{0})$ and obtain the relation $AV_{2}=V_{3}\\tilde{H}_{2}$ with $V_{k}=[v_{1},\\dots,v_{k}]$.\n\nStep 1:\n$$\nw=A v_{1}=A\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}=\\begin{pmatrix}1\\\\0\\\\1\\end{pmatrix},\\quad h_{1,1}=v_{1}^{T}w=1,\n$$\n$$\nw:=w-h_{1,1}v_{1}=\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix},\\quad h_{2,1}=\\|w\\|_{2}=1,\\quad v_{2}=w/h_{2,1}=\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}.\n$$\n\nStep 2:\n$$\nw=A v_{2}=A\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}=\\begin{pmatrix}0\\\\1\\\\1\\end{pmatrix},\\quad h_{1,2}=v_{1}^{T}w=0,\\quad w:=w-h_{1,2}v_{1}=\\begin{pmatrix}0\\\\1\\\\1\\end{pmatrix},\n$$\n$$\nh_{2,2}=v_{2}^{T}w=1,\\quad w:=w-h_{2,2}v_{2}=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix},\\quad h_{3,2}=\\|w\\|_{2}=1,\\quad v_{3}=w/h_{3,2}=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}.\n$$\n\nTherefore,\n$$\n\\tilde{H}_{2}=\\begin{pmatrix}h_{1,1}h_{1,2}\\\\ h_{2,1}h_{2,2}\\\\ 0h_{3,2}\\end{pmatrix}\n=\\begin{pmatrix}10\\\\ 11\\\\ 01\\end{pmatrix},\\qquad\nH_{2}=\\begin{pmatrix}10\\\\ 11\\end{pmatrix}.\n$$\n\nThe two-step approximations have the form $x_{2}=x_{0}+V_{2}y$ with $y\\in\\mathbb{R}^{2}$.\n\nFor FOM(2), $y^{\\text{FOM}}$ solves $H_{2}y=\\beta e_{1}$ with $e_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$:\n$$\n\\begin{pmatrix}10\\\\11\\end{pmatrix}\\begin{pmatrix}y_{1}\\\\y_{2}\\end{pmatrix}=\\begin{pmatrix}1\\\\0\\end{pmatrix}\n\\;\\Rightarrow\\; y_{1}=1,\\; y_{2}=-1.\n$$\nHence $x_{2}^{\\text{FOM}}=V_{2}y^{\\text{FOM}}=v_{1}\\cdot 1+v_{2}\\cdot(-1)=\\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix}$, and the residual is\n$$\nr_{2}^{\\text{FOM}}=b-Ax_{2}^{\\text{FOM}}=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}-A\\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix}\n=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}-\\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix}\n=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix},\n$$\nso $\\|r_{2}^{\\text{FOM}}\\|_{2}=1$.\n\nFor GMRES(2), $y^{\\text{GMRES}}$ minimizes $\\|\\tilde{H}_{2}y-\\beta e_{1}\\|_{2}$ with $e_{1}=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}$. Let $y=\\begin{pmatrix}y_{1}\\\\y_{2}\\end{pmatrix}$. Then\n$$\n\\tilde{H}_{2}y-\\beta e_{1}=\\begin{pmatrix}y_{1}-1\\\\ y_{1}+y_{2}\\\\ y_{2}\\end{pmatrix},\n$$\nand the squared norm to minimize is\n$$\nf(y_{1},y_{2})=(y_{1}-1)^{2}+(y_{1}+y_{2})^{2}+y_{2}^{2}.\n$$\nSetting the gradient to zero,\n$$\n\\frac{\\partial f}{\\partial y_{1}}=2(y_{1}-1)+2(y_{1}+y_{2})=0\\;\\Rightarrow\\;2y_{1}+y_{2}-1=0,\n$$\n$$\n\\frac{\\partial f}{\\partial y_{2}}=2(y_{1}+y_{2})+2y_{2}=0\\;\\Rightarrow\\;y_{1}+2y_{2}=0.\n$$\nSolving, $y_{2}=-\\frac{1}{3}$ and $y_{1}=\\frac{2}{3}$. The minimized residual norm is\n$$\n\\|\\tilde{H}_{2}y^{\\text{GMRES}}-\\beta e_{1}\\|_{2}\n=\\left\\|\\begin{pmatrix}\\frac{2}{3}-1\\\\ \\frac{2}{3}-\\frac{1}{3}\\\\ -\\frac{1}{3}\\end{pmatrix}\\right\\|_{2}\n=\\left\\|\\begin{pmatrix}-\\frac{1}{3}\\\\ \\frac{1}{3}\\\\ -\\frac{1}{3}\\end{pmatrix}\\right\\|_{2}\n=\\sqrt{\\frac{1}{9}+\\frac{1}{9}+\\frac{1}{9}}=\\sqrt{\\frac{1}{3}}.\n$$\nBecause $r_{2}^{\\text{GMRES}}=V_{3}(\\beta e_{1}-\\tilde{H}_{2}y^{\\text{GMRES}})$ and $V_{3}$ is orthonormal, we have $\\|r_{2}^{\\text{GMRES}}\\|_{2}=\\|\\beta e_{1} - \\tilde{H}_{2}y^{\\text{GMRES}}\\|_{2}=\\sqrt{\\frac{1}{3}}$.\n\nTherefore, the requested pair of residual norms is $(\\|r_{2}^{\\text{FOM}}\\|_{2},\\|r_{2}^{\\text{GMRES}}\\|_{2})=\\left(1,\\sqrt{\\frac{1}{3}}\\right)$.",
            "answer": "$$\\boxed{(1, \\sqrt{\\frac{1}{3}})}$$"
        }
    ]
}