{
    "hands_on_practices": [
        {
            "introduction": "In many practical applications, linear systems arise from combining physical principles or data of different scales, leading to matrices with wildly varying row magnitudes. This exercise  demonstrates a simple yet effective preconditioning technique called row equilibration, which is a form of left preconditioning. By scaling the rows of the matrix to have a uniform norm, you will see how we can mitigate issues caused by poor scaling, a crucial first step in preparing a system for an iterative solver.",
            "id": "2194457",
            "problem": "Consider the linear system of equations $Ax = b$, where the matrix $A$ and vector $b$ are given by:\n$$\nA = \\begin{pmatrix} 2 & -1 & 0 \\\\ 1000 & 2000 & -1000 \\\\ 0 & -1 & 2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nIt is observed that the convergence of many iterative solvers can be improved by transforming the system into an equivalent one, $A'x = b'$, where the rows of the new matrix $A'$ have a more uniform \"size\". This transformation is achieved using an invertible matrix $P$ such that $A' = PA$ and $b' = Pb$.\n\nYour task is to find a diagonal matrix $P$ of the form\n$$\nP = \\begin{pmatrix} p_1 & 0 & 0 \\\\ 0 & p_2 & 0 \\\\ 0 & 0 & p_3 \\end{pmatrix}\n$$\nthat performs row equilibration on $A$. Specifically, determine the values of $p_1, p_2,$ and $p_3$ such that the infinity norm of each row vector in the resulting matrix $A'$ is equal to 1. The infinity norm of a vector $v = (v_1, v_2, \\dots, v_n)$ is defined as $\\|v\\|_{\\infty} = \\max_{i} |v_i|$. For simplicity, you may assume the diagonal entries of $P$ are positive.\n\nExpress your final answer as the 3x3 matrix $P$.",
            "solution": "We seek a diagonal scaling matrix $P=\\operatorname{diag}(p_{1},p_{2},p_{3})$ such that $A' = PA$ has each row with infinity norm equal to $1$. Let $a_{i}^{T}$ denote the $i$-th row of $A$. Then the $i$-th row of $A'$ is $p_{i}a_{i}^{T}$. Since $p_{i} > 0$, the infinity norm scales as\n$$\n\\|p_{i}a_{i}^{T}\\|_{\\infty} = p_{i}\\|a_{i}^{T}\\|_{\\infty}.\n$$\nImposing $\\|p_{i}a_{i}^{T}\\|_{\\infty} = 1$ gives\n$$\np_{i} = \\frac{1}{\\|a_{i}^{T}\\|_{\\infty}} \\quad \\text{for } i=1,2,3.\n$$\nCompute the row infinity norms of $A$:\n- Row $1$: $a_{1}^{T}=(2,-1,0)$, so $\\|a_{1}^{T}\\|_{\\infty}=\\max\\{|2|,|{-1}|,|0|\\}=2$, hence $p_{1}=\\frac{1}{2}$.\n- Row $2$: $a_{2}^{T}=(1000,2000,-1000)$, so $\\|a_{2}^{T}\\|_{\\infty}=\\max\\{1000,2000,1000\\}=2000$, hence $p_{2}=\\frac{1}{2000}$.\n- Row $3$: $a_{3}^{T}=(0,-1,2)$, so $\\|a_{3}^{T}\\|_{\\infty}=\\max\\{|0|,|{-1}|,|2|\\}=2$, hence $p_{3}=\\frac{1}{2}$.\n\nTherefore,\n$$\nP=\\begin{pmatrix}\n\\frac{1}{2} & 0 & 0 \\\\\n0 & \\frac{1}{2000} & 0 \\\\\n0 & 0 & \\frac{1}{2}\n\\end{pmatrix}.\n$$\nThis choice ensures each row of $A' = PA$ has infinity norm equal to $1$.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2} & 0 & 0 \\\\ 0 & \\frac{1}{2000} & 0 \\\\ 0 & 0 & \\frac{1}{2}\\end{pmatrix}}$$"
        },
        {
            "introduction": "A preconditioned iterative method transforms the original system $Ax=b$ into a new one, but our ultimate goal is still to solve for the original variables. This raises a crucial question: how do we measure convergence? This practice  explores the important distinction between the true residual, $r_k = b - Ax_k$, and the preconditioned residual, $\\hat{r}_k$, demonstrating that a small preconditioned residual does not necessarily imply a small true residual, a vital consideration for robust stopping criteria.",
            "id": "2194449",
            "problem": "In numerical linear algebra, preconditioning is a technique used to improve the convergence properties of iterative methods for solving a linear system $Ax=b$. Consider a system where\n$$ A = \\begin{pmatrix} 5 & 0 \\\\ 0 & 50 \\end{pmatrix} \\quad \\text{and} \\quad b = \\begin{pmatrix} 5 \\\\ 50 \\end{pmatrix}. $$\nAn iterative method with left preconditioning is used to find the solution. The preconditioner $M$ is chosen to be the Jacobi preconditioner, which is the diagonal matrix containing the diagonal entries of $A$. After a certain number of iterations, the method yields an approximate solution\n$$ x_k = \\begin{pmatrix} 0.8 \\\\ 0.9 \\end{pmatrix}. $$\nThe original residual is defined as $r_k = b - Ax_k$, and the preconditioned residual is $\\hat{r}_k = M^{-1}(b - Ax_k)$. The stopping criterion for the iterative method is based on the norm of the preconditioned residual, $\\|\\hat{r}_k\\|_2$, being small. However, this does not guarantee that the norm of the original residual, $\\|r_k\\|_2$, is also small.\n\nTo illustrate this, calculate the ratio $\\frac{\\|r_k\\|_2}{\\|\\hat{r}_k\\|_2}$. The notation $\\|\\cdot\\|_2$ denotes the standard Euclidean norm (or $\\ell_2$-norm) of a vector $v = (v_1, v_2, ..., v_n)$, defined as $\\|v\\|_2 = \\sqrt{\\sum_{i=1}^n v_i^2}$. Express your answer as a single symbolic expression in its simplest form.",
            "solution": "We are given $A=\\begin{pmatrix}5&0\\\\0&50\\end{pmatrix}$, $b=\\begin{pmatrix}5\\\\50\\end{pmatrix}$, $x_{k}=\\begin{pmatrix}0.8\\\\0.9\\end{pmatrix}$, and the Jacobi preconditioner $M=\\operatorname{diag}(A)=\\begin{pmatrix}5&0\\\\0&50\\end{pmatrix}$. The original residual is $r_{k}=b-Ax_{k}$ and the preconditioned residual is $\\hat{r}_{k}=M^{-1}(b-Ax_{k})$.\n\nFirst compute the residual:\n$$\nr_{k}=b-Ax_{k}\n=\\begin{pmatrix}5\\\\50\\end{pmatrix}-\\begin{pmatrix}5&0\\\\0&50\\end{pmatrix}\\begin{pmatrix}0.8\\\\0.9\\end{pmatrix}\n=\\begin{pmatrix}5-5\\cdot 0.8\\\\50-50\\cdot 0.9\\end{pmatrix}\n=\\begin{pmatrix}1\\\\5\\end{pmatrix}.\n$$\n\nCompute $M^{-1}$ and the preconditioned residual:\n$$\nM^{-1}=\\begin{pmatrix}\\frac{1}{5}&0\\\\0&\\frac{1}{50}\\end{pmatrix},\n\\quad\n\\hat{r}_{k}=M^{-1}r_{k}\n=\\begin{pmatrix}\\frac{1}{5}&0\\\\0&\\frac{1}{50}\\end{pmatrix}\\begin{pmatrix}1\\\\5\\end{pmatrix}\n=\\begin{pmatrix}\\frac{1}{5}\\\\\\frac{1}{10}\\end{pmatrix}.\n$$\n\nCompute the Euclidean norms:\n$$\n\\|r_{k}\\|_{2}=\\sqrt{1^{2}+5^{2}}=\\sqrt{26},\\qquad\n\\|\\hat{r}_{k}\\|_{2}=\\sqrt{\\left(\\frac{1}{5}\\right)^{2}+\\left(\\frac{1}{10}\\right)^{2}}\n=\\sqrt{\\frac{1}{25}+\\frac{1}{100}}\n=\\sqrt{\\frac{5}{100}}\n=\\frac{\\sqrt{5}}{10}.\n$$\n\nForm the ratio and simplify:\n$$\n\\frac{\\|r_{k}\\|_{2}}{\\|\\hat{r}_{k}\\|_{2}}\n=\\frac{\\sqrt{26}}{\\frac{\\sqrt{5}}{10}}\n=10\\cdot\\frac{\\sqrt{26}}{\\sqrt{5}}\n=10\\cdot\\frac{\\sqrt{130}}{5}\n=2\\sqrt{130}.\n$$",
            "answer": "$$\\boxed{2\\sqrt{130}}$$"
        },
        {
            "introduction": "Simple diagonal preconditioners are effective for some problems, but many systems from computational science possess a more complex structure. This challenge problem  introduces a powerful strategy for matrices that are a low-rank perturbation of a simpler matrix, a common scenario in discretized differential equations. You will use the Sherman-Morrison-Woodbury formula not to find the exact inverse, but as a blueprint for constructing a highly effective preconditioner for a problem modeling quantum transport.",
            "id": "2194480",
            "problem": "In the numerical solution of certain differential equations, such as those modeling quantum transport with localized potentials, one encounters large, sparse linear systems of the form $Ax=b$. Often, the system matrix $A$ can be expressed as a modification of a simpler, structured matrix $S$. A powerful technique for solving such systems iteratively is to use a preconditioner that approximates $A^{-1}$.\n\nConsider a linear system on $\\mathbb{C}^N$ where the matrix $A$ is given by $A = S + uv^H$, with $S = T + i\\gamma I$. Here, $T$ is the $N \\times N$ real symmetric matrix representing the discrete one-dimensional Laplacian with Dirichlet boundary conditions, having entries $T_{jj} = 2$ for $j=1,...,N$ and $T_{j, j+1} = T_{j+1, j} = -1$ for $j=1,...,N-1$. $I$ is the $N \\times N$ identity matrix, $\\gamma$ is a positive real constant, and $i = \\sqrt{-1}$. The term $uv^H$ represents a non-local, rank-one perturbation, where $u,v \\in \\mathbb{C}^N$.\n\nA sophisticated preconditioner $P$ for $A$ can be constructed based on the Sherman-Morrison-Woodbury formula. The exact inverse of $A$ is given by $A^{-1} = (S+uv^H)^{-1} = S^{-1} - S^{-1}u(1+v^H S^{-1}u)^{-1}v^H S^{-1}$. A preconditioner $P$ is formed by replacing the expensive-to-compute $S^{-1}$ with the action of an approximate inverse, $M_S^{-1}$, where $M_S$ is a good preconditioner for $S$. The action of the resulting preconditioner $P^{-1}$ on a vector $y$ is thus defined as:\n$$P^{-1}y = M_S^{-1}y - M_S^{-1}u(1+v^H M_S^{-1}u)^{-1}v^H M_S^{-1}y$$\n\nFor this problem, let the parameters be $N=5$, $\\gamma=2$, and the rank-one update be specified by $u=e_1$ and $v=\\bar{c}e_N$, where $c=1-2i$, and $e_j$ is the $j$-th standard basis vector. We choose a simple but effective preconditioner for $S = T+i\\gamma I$, namely $M_S = T$. We are interested in performing one step of a preconditioned iterative solver, which requires computing the effect of the preconditioner on the right-hand side vector $b$.\n\nYour task is to calculate the vector $z = P^{-1}b$, where the right-hand side vector is given by $b = \\gamma(N+1)e_1$.\n\nPresent your final answer as a column vector whose components are complex numbers in the form $a+bi$.",
            "solution": "We must compute $z = P^{-1}b$ using the preconditioner action\n$$P^{-1}y = M_{S}^{-1}y - M_{S}^{-1}u\\left(1 + v^{H}M_{S}^{-1}u\\right)^{-1}v^{H}M_{S}^{-1}y.$$\nGiven $M_{S} = T$, we have $M_{S}^{-1} = T^{-1}$. The data are $N=5$, $\\gamma=2$, $u = e_{1}$, $v = \\overline{c}\\,e_{5}$ with $c = 1 - 2i$ so that $v^{H} = c\\,e_{5}^{T}$, and $b = \\gamma(N+1)e_{1} = 12 e_{1}$.\n\nWe need $T^{-1}$. For the $5\\times 5$ tridiagonal $T$ with $2$ on the diagonal and $-1$ on the first off-diagonals, the inverse is known explicitly:\n$$\\left(T^{-1}\\right)_{ij} = \\begin{cases}\n\\dfrac{i(6-j)}{6}, & i \\leq j, \\\\\n\\dfrac{j(6-i)}{6}, & i \\geq j,\n\\end{cases}$$\nso the first column is $T^{-1}e_{1} = \\left[\\dfrac{5}{6}, \\dfrac{4}{6}, \\dfrac{3}{6}, \\dfrac{2}{6}, \\dfrac{1}{6}\\right]^{T}$.\n\nFirst compute $w := T^{-1}b = 12\\,T^{-1}e_{1} = [10,\\,8,\\,6,\\,4,\\,2]^{T}$.\n\nNext compute $x := T^{-1}u = T^{-1}e_{1} = \\left[\\dfrac{5}{6}, \\dfrac{4}{6}, \\dfrac{3}{6}, \\dfrac{2}{6}, \\dfrac{1}{6}\\right]^{T}$.\n\nCompute the scalar\n$$d := 1 + v^{H}T^{-1}u = 1 + c\\,e_{5}^{T}x = 1 + (1 - 2i)\\cdot \\dfrac{1}{6} = \\dfrac{7}{6} - \\dfrac{1}{3}i = \\dfrac{7 - 2i}{6},$$\nso\n$$d^{-1} = \\dfrac{6}{7 - 2i} = \\dfrac{6(7 + 2i)}{49 + 4} = \\dfrac{42 + 12i}{53}.$$\n\nCompute\n$$t := v^{H}T^{-1}b = c\\,e_{5}^{T}w = (1 - 2i)\\cdot 2 = 2 - 4i.$$\n\nThus\n$$d^{-1}t = \\dfrac{42 + 12i}{53}\\,(2 - 4i) = \\dfrac{132 - 144i}{53}.$$\n\nFinally,\n$$z = w - x\\,(d^{-1}t).$$\nWriting $x = \\left[\\dfrac{5}{6}, \\dfrac{4}{6}, \\dfrac{3}{6}, \\dfrac{2}{6}, \\dfrac{1}{6}\\right]^{T}$ and $d^{-1}t = \\dfrac{132}{53} - \\dfrac{144}{53}i$, it is convenient to use $\\dfrac{1}{6}(d^{-1}t) = \\dfrac{22}{53} - \\dfrac{24}{53}i$. With $p = [5,4,3,2,1]^{T}$, we have componentwise\n$$z_{i} = w_{i} - \\dfrac{p_{i}}{6}(d^{-1}t) = \\left(w_{i} - \\dfrac{22p_{i}}{53}\\right) + \\dfrac{24p_{i}}{53}i.$$\nEvaluating for $i=1,\\dots,5$ yields\n$$z = \\begin{pmatrix}\n\\dfrac{420}{53} + \\dfrac{120}{53} i \\\\\n\\dfrac{336}{53} + \\dfrac{96}{53} i \\\\\n\\dfrac{252}{53} + \\dfrac{72}{53} i \\\\\n\\dfrac{168}{53} + \\dfrac{48}{53} i \\\\\n\\dfrac{84}{53} + \\dfrac{24}{53} i\n\\end{pmatrix}.$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{420}{53} + \\frac{120}{53} i \\\\ \\frac{336}{53} + \\frac{96}{53} i \\\\ \\frac{252}{53} + \\frac{72}{53} i \\\\ \\frac{168}{53} + \\frac{48}{53} i \\\\ \\frac{84}{53} + \\frac{24}{53} i\\end{pmatrix}}$$"
        }
    ]
}