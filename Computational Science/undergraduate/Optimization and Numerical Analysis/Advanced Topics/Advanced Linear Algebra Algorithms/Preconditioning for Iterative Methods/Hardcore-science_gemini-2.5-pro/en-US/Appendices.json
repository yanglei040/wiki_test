{
    "hands_on_practices": [
        {
            "introduction": "Matrices with rows of vastly different scales can pose significant challenges for iterative solvers. A simple yet powerful preconditioning strategy is to \"equilibrate\" the system by scaling each row so they have a similar magnitude. This exercise guides you through this fundamental technique, known as diagonal scaling, which often serves as an effective and inexpensive first step in improving a system's properties for iterative solution. ",
            "id": "2194457",
            "problem": "Consider the linear system of equations $Ax = b$, where the matrix $A$ and vector $b$ are given by:\n$$\nA = \\begin{pmatrix} 2 & -1 & 0 \\\\ 1000 & 2000 & -1000 \\\\ 0 & -1 & 2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nIt is observed that the convergence of many iterative solvers can be improved by transforming the system into an equivalent one, $A'x = b'$, where the rows of the new matrix $A'$ have a more uniform \"size\". This transformation is achieved using an invertible matrix $P$ such that $A' = PA$ and $b' = Pb$.\n\nYour task is to find a diagonal matrix $P$ of the form\n$$\nP = \\begin{pmatrix} p_1 & 0 & 0 \\\\ 0 & p_2 & 0 \\\\ 0 & 0 & p_3 \\end{pmatrix}\n$$\nthat performs row equilibration on $A$. Specifically, determine the values of $p_1, p_2,$ and $p_3$ such that the infinity norm of each row vector in the resulting matrix $A'$ is equal to 1. The infinity norm of a vector $v = (v_1, v_2, \\dots, v_n)$ is defined as $\\|v\\|_{\\infty} = \\max_{i} |v_i|$. For simplicity, you may assume the diagonal entries of $P$ are positive.\n\nExpress your final answer as the 3x3 matrix $P$.",
            "solution": "We seek a diagonal scaling matrix $P=\\operatorname{diag}(p_{1},p_{2},p_{3})$ such that $A' = PA$ has each row with infinity norm equal to $1$. Let $a_{i}^{T}$ denote the $i$-th row of $A$. Then the $i$-th row of $A'$ is $p_{i}a_{i}^{T}$. Since $p_{i} > 0$, the infinity norm scales as\n$$\n\\|p_{i}a_{i}^{T}\\|_{\\infty} = p_{i}\\|a_{i}^{T}\\|_{\\infty}.\n$$\nImposing $\\|p_{i}a_{i}^{T}\\|_{\\infty} = 1$ gives\n$$\np_{i} = \\frac{1}{\\|a_{i}^{T}\\|_{\\infty}} \\quad \\text{for } i=1,2,3.\n$$\nCompute the row infinity norms of $A$:\n- Row $1$: $a_{1}^{T}=(2,-1,0)$, so $\\|a_{1}^{T}\\|_{\\infty}=\\max\\{|2|,|{-1}|,|0|\\}=2$, hence $p_{1}=\\frac{1}{2}$.\n- Row $2$: $a_{2}^{T}=(1000,2000,-1000)$, so $\\|a_{2}^{T}\\|_{\\infty}=\\max\\{1000,2000,1000\\}=2000$, hence $p_{2}=\\frac{1}{2000}$.\n- Row $3$: $a_{3}^{T}=(0,-1,2)$, so $\\|a_{3}^{T}\\|_{\\infty}=\\max\\{|0|,|{-1}|,|2|\\}=2$, hence $p_{3}=\\frac{1}{2}$.\n\nTherefore,\n$$\nP=\\begin{pmatrix}\n\\frac{1}{2} & 0 & 0 \\\\\n0 & \\frac{1}{2000} & 0 \\\\\n0 & 0 & \\frac{1}{2}\n\\end{pmatrix}.\n$$\nThis choice ensures each row of $A' = PA$ has infinity norm equal to $1$.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2} & 0 & 0 \\\\ 0 & \\frac{1}{2000} & 0 \\\\ 0 & 0 & \\frac{1}{2}\\end{pmatrix}}$$"
        },
        {
            "introduction": "The ultimate goal of preconditioning is to transform a difficult linear system into an easy one. The ideal preconditioned matrix would have a condition number $\\kappa$ of exactly 1, allowing many iterative methods to converge in a single iteration. This practice problem provides a thought experiment to construct such a \"perfect\" diagonal preconditioner for a simple $2 \\times 2$ system, offering a clear view of the theoretical target that practical preconditioners aim to approximate. ",
            "id": "2194479",
            "problem": "In numerical linear algebra, iterative methods are often used to find approximate solutions to large systems of linear equations of the form $A\\mathbf{x} = \\mathbf{b}$. The efficiency of many such methods is highly dependent on the properties of the coefficient matrix $A$, particularly its condition number, $\\kappa(A)$. To improve convergence, a technique called preconditioning is used. The original system is transformed into an equivalent one, $M\\mathbf{x} = \\mathbf{c}$, where $M = P^{-1}A$ and $\\mathbf{c} = P^{-1}\\mathbf{b}$. The matrix $P$ is called a preconditioner, and it is chosen to make the condition number of the preconditioned matrix $M$, $\\kappa(M)$, as close to 1 as possible, while ensuring that $P^{-1}$ is easy to compute.\n\nConsider the matrix $A$ given by:\n$$A = \\begin{pmatrix} 1 & 2 \\\\ -4 & 2 \\end{pmatrix}$$\nWe are looking for a diagonal preconditioner $P$ of the form:\n$$P = \\begin{pmatrix} p_1 & 0 \\\\ 0 & p_2 \\end{pmatrix}$$\nwhere $p_1$ and $p_2$ are positive real numbers.\n\nFind the value of the ratio $p_2/p_1$ such that the condition number of the preconditioned matrix $P^{-1}A$ with respect to the matrix 2-norm, $\\kappa_2(P^{-1}A)$, is exactly 1.",
            "solution": "We want $\\kappa_{2}(P^{-1}A)=1$. For the matrix 2-norm, $\\kappa_{2}(M)=1$ if and only if there exists $\\alpha>0$ such that $M^{T}M=\\alpha I$. Let $M=P^{-1}A$ with $P=\\mathrm{diag}(p_{1},p_{2})$ and $p_{1},p_{2}>0$. Then\n$$\nM^{T}M=A^{T}P^{-T}P^{-1}A=A^{T}P^{-2}A,\n$$\nsince $P$ is diagonal with positive entries. Write $D=P^{-2}=\\mathrm{diag}(d_{1},d_{2})$ where $d_{1}=p_{1}^{-2}$ and $d_{2}=p_{2}^{-2}$. We seek $d_{1},d_{2}>0$ and $\\alpha>0$ such that\n$$\nA^{T}DA=\\alpha I.\n$$\nWith $A=\\begin{pmatrix}1&2\\\\ -4&2\\end{pmatrix}$, first compute\n$$\nDA=\\begin{pmatrix}d_{1}&2d_{1}\\\\ -4d_{2}&2d_{2}\\end{pmatrix},\n$$\nand then\n$$\nA^{T}DA=\\begin{pmatrix}1&-4\\\\ 2&2\\end{pmatrix}\\begin{pmatrix}d_{1}&2d_{1}\\\\ -4d_{2}&2d_{2}\\end{pmatrix}\n=\\begin{pmatrix}d_{1}+16d_{2}&2d_{1}-8d_{2}\\\\ 2d_{1}-8d_{2}&4d_{1}+4d_{2}\\end{pmatrix}.\n$$\nFor this to equal $\\alpha I$, the off-diagonal entries must vanish and the diagonal entries must be equal. The off-diagonal condition gives\n$$\n2d_{1}-8d_{2}=0 \\quad \\Longrightarrow \\quad d_{1}=4d_{2}.\n$$\nWith this relation, the diagonals match automatically:\n$$\nd_{1}+16d_{2}=4d_{2}+16d_{2}=20d_{2}, \\quad 4d_{1}+4d_{2}=16d_{2}+4d_{2}=20d_{2}.\n$$\nTherefore $D$ must satisfy $d_{1}=4d_{2}$. In terms of $p_{1},p_{2}$, this is\n$$\np_{1}^{-2}=4p_{2}^{-2} \\quad \\Longrightarrow \\quad \\frac{p_{1}^{-2}}{p_{2}^{-2}}=4 \\quad \\Longrightarrow \\quad \\frac{p_{2}^{2}}{p_{1}^{2}}=4 \\quad \\Longrightarrow \\quad \\frac{p_{2}}{p_{1}}=2,\n$$\nusing $p_{1},p_{2}>0$. Hence $\\kappa_{2}(P^{-1}A)=1$ precisely when $p_{2}/p_{1}=2$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "Applying a preconditioner changes the system we are solving, and it's critical to understand how this affects our metrics for success, such as the stopping criteria for an iterative method. When using left preconditioning, a common approach is to monitor the norm of the preconditioned residual. This exercise serves as a crucial cautionary tale, demonstrating with a concrete example how a small preconditioned residual can be misleading and may not imply that the true residual of the original system is also small. ",
            "id": "2194449",
            "problem": "In numerical linear algebra, preconditioning is a technique used to improve the convergence properties of iterative methods for solving a linear system $Ax=b$. Consider a system where\n$$ A = \\begin{pmatrix} 5 & 0 \\\\ 0 & 50 \\end{pmatrix} \\quad \\text{and} \\quad b = \\begin{pmatrix} 5 \\\\ 50 \\end{pmatrix}. $$\nAn iterative method with left preconditioning is used to find the solution. The preconditioner $M$ is chosen to be the Jacobi preconditioner, which is the diagonal matrix containing the diagonal entries of $A$. After a certain number of iterations, the method yields an approximate solution\n$$ x_k = \\begin{pmatrix} 0.8 \\\\ 0.9 \\end{pmatrix}. $$\nThe original residual is defined as $r_k = b - Ax_k$, and the preconditioned residual is $\\hat{r}_k = M^{-1}(b - Ax_k)$. The stopping criterion for the iterative method is based on the norm of the preconditioned residual, $\\|\\hat{r}_k\\|_2$, being small. However, this does not guarantee that the norm of the original residual, $\\|r_k\\|_2$, is also small.\n\nTo illustrate this, calculate the ratio $\\frac{\\|r_k\\|_2}{\\|\\hat{r}_k\\|_2}$. The notation $\\|\\cdot\\|_2$ denotes the standard Euclidean norm (or $\\ell_2$-norm) of a vector $v = (v_1, v_2, ..., v_n)$, defined as $\\|v\\|_2 = \\sqrt{\\sum_{i=1}^n v_i^2}$. Express your answer as a single symbolic expression in its simplest form.",
            "solution": "We are given $A=\\begin{pmatrix}5&0\\\\0&50\\end{pmatrix}$, $b=\\begin{pmatrix}5\\\\50\\end{pmatrix}$, $x_{k}=\\begin{pmatrix}0.8\\\\0.9\\end{pmatrix}$, and the Jacobi preconditioner $M=\\operatorname{diag}(A)=\\begin{pmatrix}5&0\\\\0&50\\end{pmatrix}$. The original residual is $r_{k}=b-Ax_{k}$ and the preconditioned residual is $\\hat{r}_{k}=M^{-1}(b-Ax_{k})$.\n\nFirst compute the residual:\n$$\nr_{k}=b-Ax_{k}\n=\\begin{pmatrix}5\\\\50\\end{pmatrix}-\\begin{pmatrix}5&0\\\\0&50\\end{pmatrix}\\begin{pmatrix}0.8\\\\0.9\\end{pmatrix}\n=\\begin{pmatrix}5-5\\cdot 0.8\\\\50-50\\cdot 0.9\\end{pmatrix}\n=\\begin{pmatrix}1\\\\5\\end{pmatrix}.\n$$\n\nCompute $M^{-1}$ and the preconditioned residual:\n$$\nM^{-1}=\\begin{pmatrix}\\frac{1}{5}&0\\\\0&\\frac{1}{50}\\end{pmatrix},\n\\quad\n\\hat{r}_{k}=M^{-1}r_{k}\n=\\begin{pmatrix}\\frac{1}{5}&0\\\\0&\\frac{1}{50}\\end{pmatrix}\\begin{pmatrix}1\\\\5\\end{pmatrix}\n=\\begin{pmatrix}\\frac{1}{5}\\\\\\frac{1}{10}\\end{pmatrix}.\n$$\n\nCompute the Euclidean norms:\n$$\n\\|r_{k}\\|_{2}=\\sqrt{1^{2}+5^{2}}=\\sqrt{26},\\qquad\n\\|\\hat{r}_{k}\\|_{2}=\\sqrt{\\left(\\frac{1}{5}\\right)^{2}+\\left(\\frac{1}{10}\\right)^{2}}\n=\\sqrt{\\frac{1}{25}+\\frac{1}{100}}\n=\\sqrt{\\frac{5}{100}}\n=\\frac{\\sqrt{5}}{10}.\n$$\n\nForm the ratio and simplify:\n$$\n\\frac{\\|r_{k}\\|_{2}}{\\|\\hat{r}_{k}\\|_{2}}\n=\\frac{\\sqrt{26}}{\\frac{\\sqrt{5}}{10}}\n=10\\cdot\\frac{\\sqrt{26}}{\\sqrt{5}}\n=10\\cdot\\frac{\\sqrt{130}}{5}\n=2\\sqrt{130}.\n$$",
            "answer": "$$\\boxed{2\\sqrt{130}}$$"
        }
    ]
}