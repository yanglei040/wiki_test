{
    "hands_on_practices": [
        {
            "introduction": "Newton's method provides a powerful, iterative approach for approximating the roots of non-linear systems, extending the single-variable concept to multiple dimensions. This first exercise  walks you through the fundamental mechanics of a single iteration. By calculating the Jacobian matrix and solving a local linear system for the update step, you will gain hands-on proficiency with the core engine of this essential numerical algorithm.",
            "id": "2207863",
            "problem": "Consider the following system of two non-linear equations:\n$$\n\\begin{cases}\n2x^2 + y = 11 \\\\\nx + 2y^2 = 10\n\\end{cases}\n$$\nAn approximate solution to this system is sought using Newton's method for systems. Starting with the initial guess $(x_0, y_0) = (3, 1)$, perform a single iteration to find the next approximation $(x_1, y_1)$.\n\nFind the coordinates of $(x_1, y_1)$. Express each coordinate as a rational number in its simplest form.",
            "solution": "Define the vector function $\\mathbf{F}(x,y)$ and its Jacobian matrix $J(x,y)$ by\n$$\n\\mathbf{F}(x,y)=\\begin{pmatrix} 2x^{2}+y-11 \\\\ x+2y^{2}-10 \\end{pmatrix}, \n\\quad\nJ(x,y)=\\begin{pmatrix} \\frac{\\partial}{\\partial x}(2x^{2}+y-11)  \\frac{\\partial}{\\partial y}(2x^{2}+y-11) \\\\ \\frac{\\partial}{\\partial x}(x+2y^{2}-10)  \\frac{\\partial}{\\partial y}(x+2y^{2}-10) \\end{pmatrix}\n=\\begin{pmatrix} 4x  1 \\\\ 1  4y \\end{pmatrix}.\n$$\nNewton’s method for systems computes the update $\\mathbf{s}=(s_{x},s_{y})^{T}$ by solving\n$$\nJ(x_{0},y_{0})\\,\\mathbf{s}=-\\mathbf{F}(x_{0},y_{0}),\n$$\nand then sets $(x_{1},y_{1})=(x_{0},y_{0})+\\mathbf{s}$.\n\nAt $(x_{0},y_{0})=(3,1)$, evaluate\n$$\n\\mathbf{F}(3,1)=\\begin{pmatrix} 2\\cdot 3^{2}+1-11 \\\\ 3+2\\cdot 1^{2}-10 \\end{pmatrix}\n=\\begin{pmatrix} 8 \\\\ -5 \\end{pmatrix},\n\\quad\nJ(3,1)=\\begin{pmatrix} 4\\cdot 3  1 \\\\ 1  4\\cdot 1 \\end{pmatrix}\n=\\begin{pmatrix} 12  1 \\\\ 1  4 \\end{pmatrix}.\n$$\nSolve for $\\mathbf{s}$ in\n$$\n\\begin{pmatrix} 12  1 \\\\ 1  4 \\end{pmatrix}\\begin{pmatrix} s_{x} \\\\ s_{y} \\end{pmatrix}\n=-\\begin{pmatrix} 8 \\\\ -5 \\end{pmatrix}\n=\\begin{pmatrix} -8 \\\\ 5 \\end{pmatrix},\n$$\nwhich is the linear system\n$$\n\\begin{cases}\n12s_{x}+s_{y}=-8, \\\\\ns_{x}+4s_{y}=5.\n\\end{cases}\n$$\nFrom $s_{x}=5-4s_{y}$ and substitution into the first equation,\n$$\n12(5-4s_{y})+s_{y}=-8\n\\;\\Rightarrow\\;\n60-48s_{y}+s_{y}=-8\n\\;\\Rightarrow\\;\n-47s_{y}=-68\n\\;\\Rightarrow\\;\ns_{y}=\\frac{68}{47}.\n$$\nThen\n$$\ns_{x}=5-4\\cdot \\frac{68}{47}\n=\\frac{235}{47}-\\frac{272}{47}\n=-\\frac{37}{47}.\n$$\nUpdate the approximation:\n$$\nx_{1}=x_{0}+s_{x}=3-\\frac{37}{47}=\\frac{141}{47}-\\frac{37}{47}=\\frac{104}{47}, \n\\quad\ny_{1}=y_{0}+s_{y}=1+\\frac{68}{47}=\\frac{47}{47}+\\frac{68}{47}=\\frac{115}{47}.\n$$\nThus, the next Newton iterate is $\\left(\\frac{104}{47}, \\frac{115}{47}\\right)$, with both coordinates in simplest rational form.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{104}{47}  \\frac{115}{47}\\end{pmatrix}}$$"
        },
        {
            "introduction": "After performing iterations, a crucial question arises: how good is our approximate solution? This practice  introduces the concept of the residual vector, which directly measures how well a given point satisfies the system of equations. Calculating the Euclidean norm of this vector provides a single, quantitative measure of error, a technique widely used to define stopping criteria in iterative solvers.",
            "id": "2207890",
            "problem": "In a robotics manufacturing facility, the paths of two Automated Guided Vehicles (AGVs) are modeled on a dimensionless 2D Cartesian coordinate system. For a new routing protocol to be validated, a simulation must confirm the intersection points of the vehicle paths. The path of the first AGV is described by the equation $x \\cos(y) - y^2 + 1.5 = 0$, and the path of the second AGV is described by $x^2 y - \\exp(x) + 2 = 0$. The angle $y$ in the cosine function is expressed in radians.\n\nA control systems engineer has proposed an approximate intersection point at $(x^*, y^*) = (1.2, 1.4)$. To assess how well this point satisfies the system of equations, calculate the Euclidean norm (L2 norm) of the residual vector at this proposed point.\n\nRound your final answer to four significant figures.",
            "solution": "We are given two equations:\n$$f_{1}(x,y)=x\\cos(y)-y^{2}+1.5,$$\n$$f_{2}(x,y)=x^{2}y-\\exp(x)+2.$$\nAt the proposed point $(x^{*},y^{*})=(1.2,1.4)$, the residual vector is\n$$\\mathbf{r}=\\begin{pmatrix}f_{1}(1.2,1.4) \\\\ f_{2}(1.2,1.4)\\end{pmatrix}.$$\nCompute each component:\n$$f_{1}(1.2,1.4)=1.2\\cos(1.4)-1.4^{2}+1.5.$$\nUsing $\\cos(1.4)\\approx 0.1699671429$,\n$$f_{1}(1.2,1.4)\\approx 1.2\\cdot 0.1699671429-1.96+1.5=0.2039605715-0.46=-0.2560394285.$$\nNext,\n$$f_{2}(1.2,1.4)=1.2^{2}\\cdot 1.4-\\exp(1.2)+2=1.44\\cdot 1.4-\\exp(1.2)+2=2.016-\\exp(1.2)+2.$$\nUsing $\\exp(1.2)\\approx 3.3201169227$,\n$$f_{2}(1.2,1.4)\\approx 2.016-3.3201169227+2=0.6958830773.$$\nThus,\n$$\\mathbf{r}\\approx\\begin{pmatrix}-0.2560394285 \\\\ 0.6958830773\\end{pmatrix}.$$\nThe Euclidean norm (L2 norm) is\n$$\\|\\mathbf{r}\\|_{2}=\\sqrt{(-0.2560394285)^{2}+(0.6958830773)^{2}}\\approx\\sqrt{0.06555618895+0.48425325722}=\\sqrt{0.54980944617}.$$\nTherefore,\n$$\\|\\mathbf{r}\\|_{2}\\approx 0.741491\\ldots,$$\nwhich rounded to four significant figures is $0.7415$.",
            "answer": "$$\\boxed{0.7415}$$"
        },
        {
            "introduction": "A true master of a numerical method understands not only how it works, but also why it might fail. This final practice  shifts our focus from computation to conceptual analysis, asking you to diagnose a common problem in the application of Newton's method. Investigating the properties of the Jacobian matrix for certain initial guesses will deepen your understanding of the method's stability and the critical importance of a well-conditioned linear system at each step.",
            "id": "2207871",
            "problem": "An engineer is tasked with finding a numerical solution to a system of non-linear equations modeling a steady-state physical system. The equations are given by:\n$$f_1(x, y) = x^2 - y^2 - 4 = 0$$\n$$f_2(x, y) = xy - 3 = 0$$\nThe engineer decides to use Newton's method for systems. The iterative formula for this method is given by $\\mathbf{x}_{k+1} = \\mathbf{x}_k - [J(\\mathbf{x}_k)]^{-1} \\mathbf{F}(\\mathbf{x}_k)$, where $\\mathbf{x} = (x, y)^T$, $\\mathbf{F} = (f_1, f_2)^T$, and $J(\\mathbf{x})$ is the Jacobian matrix of $\\mathbf{F}$.\n\nAfter some trials, the engineer observes that choosing an initial guess $\\mathbf{x}_0 = (x_0, y_0)$ that lies on either the x-axis (where $y_0 = 0$) or the y-axis (where $x_0 = 0$) is a poor strategy for this specific system. Which of the following statements provides the most accurate and fundamental reason for this poor performance?\n\nA. For any initial guess on an axis, the first iteration of Newton's method produces a point that is also on an axis, preventing the algorithm from ever moving towards a solution, as the solutions do not lie on an axis.\n\nB. For any initial guess on an axis, the function vector $\\mathbf{F}(\\mathbf{x}_0)$ is a zero vector, which incorrectly signals to the algorithm that a solution has been found.\n\nC. The Jacobian matrix is singular for any point on the x-axis or y-axis, causing the method to fail immediately due to an inability to compute the matrix inverse.\n\nD. The point $(0,0)$ is the only location where the Jacobian is singular. An initial guess on an axis that is close to the origin leads to a nearly-singular Jacobian, causing the subsequent iteration to produce a point extremely far from the true solution, indicating poor convergence behavior.\n\nE. The system of equations has no real solutions, so Newton's method will fail to converge regardless of the initial guess.",
            "solution": "We are given the system\n$$\nf_{1}(x,y)=x^{2}-y^{2}-4,\\qquad f_{2}(x,y)=xy-3,\n$$\nwith vector function $\\mathbf{F}(x,y)=(f_{1}(x,y),f_{2}(x,y))^{T}$ and Jacobian\n$$\nJ(x,y)=\\begin{pmatrix}\n\\frac{\\partial f_{1}}{\\partial x}  \\frac{\\partial f_{1}}{\\partial y}\\\\[4pt]\n\\frac{\\partial f_{2}}{\\partial x}  \\frac{\\partial f_{2}}{\\partial y}\n\\end{pmatrix}\n=\\begin{pmatrix}\n2x  -2y \\\\\ny  x\n\\end{pmatrix}.\n$$\nNewton’s method updates $\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-J(\\mathbf{x}_{k})^{-1}\\mathbf{F}(\\mathbf{x}_{k})$, which requires $J$ to be invertible at the iterate.\n\nFirst, compute the determinant of the Jacobian:\n$$\n\\det J(x,y)=(2x)(x)-(-2y)(y)=2x^{2}+2y^{2}=2(x^{2}+y^{2}).\n$$\nThus $J(x,y)$ is singular if and only if $(x,y)=(0,0)$. In particular, $J$ is not singular at generic points on the axes (except at the origin). This immediately shows option C is false.\n\nNext, evaluate $\\mathbf{F}$ on the axes. On the $x$-axis with $y=0$,\n$$\n\\mathbf{F}(x,0)=\\bigl(x^{2}-4,\\,-3\\bigr),\n$$\nwhich is never the zero vector. On the $y$-axis with $x=0$,\n$$\n\\mathbf{F}(0,y)=\\bigl(-y^{2}-4,\\,-3\\bigr),\n$$\nwhich is also never the zero vector. Hence option B is false.\n\nNow check whether Newton’s method remains on an axis after one iteration. Use the Newton step $\\mathbf{s}$ defined by $J\\mathbf{s}=-\\mathbf{F}$. On the $x$-axis ($y=0$), we have\n$$\nJ(x,0)=\\begin{pmatrix}2x  0\\\\ 0  x\\end{pmatrix},\\qquad \\mathbf{F}(x,0)=\\begin{pmatrix}x^{2}-4\\\\ -3\\end{pmatrix}.\n$$\nThen\n$$\n\\mathbf{s}=-J^{-1}\\mathbf{F}=-\\begin{pmatrix}\\frac{1}{2x}  0\\\\[2pt] 0  \\frac{1}{x}\\end{pmatrix}\\begin{pmatrix}x^{2}-4 \\\\ -3\\end{pmatrix}\n=\\begin{pmatrix}-\\frac{x^{2}-4}{2x}\\\\[4pt]\\frac{3}{x}\\end{pmatrix},\n$$\nso the next iterate is\n$$\nx_{1}=x-\\frac{x^{2}-4}{2x}=\\frac{x}{2}+\\frac{2}{x},\\qquad y_{1}=0+\\frac{3}{x}=\\frac{3}{x}.\n$$\nSince $y_{1}=\\frac{3}{x}\\neq 0$ for any finite $x$, the iterate immediately leaves the axis. A similar calculation on the $y$-axis ($x=0$) solves\n$$\n\\begin{pmatrix}0  -2y\\\\ y  0\\end{pmatrix}\\begin{pmatrix}s_{x}\\\\ s_{y}\\end{pmatrix}=-\\begin{pmatrix}-y^{2}-4\\\\ -3\\end{pmatrix}=\\begin{pmatrix}y^{2}+4\\\\ 3\\end{pmatrix},\n$$\nwhich yields $s_{x}=\\frac{3}{y}$ and $s_{y}=-\\frac{y^{2}+4}{2y}$, so\n$$\nx_{1}=0+\\frac{3}{y}=\\frac{3}{y},\\qquad y_{1}=y-\\frac{y^{2}+4}{2y}=\\frac{y}{2}-\\frac{2}{y}.\n$$\nAgain, the iterate leaves the axis immediately. Therefore option A is false.\n\nTo rule out option E, we check for real solutions. From $xy=3$ we have $y=\\frac{3}{x}$, and substituting into $x^{2}-y^{2}=4$ gives\n$$\nx^{2}-\\frac{9}{x^{2}}=4\\;\\;\\Longrightarrow\\;\\; x^{4}-4x^{2}-9=0.\n$$\nLet $t=x^{2}$. Then $t^{2}-4t-9=0$, so $t=2\\pm\\sqrt{13}$. The admissible root is $t=2+\\sqrt{13}0$, hence\n$$\nx=\\pm\\sqrt{2+\\sqrt{13}},\\qquad y=\\frac{3}{x}=\\pm\\frac{3}{\\sqrt{2+\\sqrt{13}}},\n$$\nwhich shows there are two real solutions. Therefore option E is false.\n\nIt remains to identify the fundamental reason axes are a poor choice for initial guesses. Since\n$$\n\\det J(x,y)=2(x^{2}+y^{2}),\n$$\nthe Jacobian is singular at $(0,0)$ and becomes ill-conditioned when $(x,y)$ is close to the origin. The explicit inverse is\n$$\nJ(x,y)^{-1}=\\frac{1}{2(x^{2}+y^{2})}\\begin{pmatrix}x  2y\\\\ -y  2x\\end{pmatrix},\n$$\nwhose entries scale like $\\frac{1}{x^{2}+y^{2}}$ times linear functions of $x$ and $y$. Near the origin this produces large Newton steps. On the axes in particular, the Newton updates derived above contain terms like $\\frac{3}{x}$ or $\\frac{3}{y}$, which become very large in magnitude when the initial guess is on an axis and close to the origin. This ill-conditioning explains the observed poor performance and is precisely captured by option D.\n\nTherefore, the most accurate and fundamental reason is that the Jacobian is singular at the origin and nearly singular for axis-aligned initial guesses near the origin, leading to instability and poor convergence.",
            "answer": "$$\\boxed{D}$$"
        }
    ]
}