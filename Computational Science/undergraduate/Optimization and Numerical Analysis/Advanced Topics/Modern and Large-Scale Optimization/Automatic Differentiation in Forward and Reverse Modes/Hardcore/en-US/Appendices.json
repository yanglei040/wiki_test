{
    "hands_on_practices": [
        {
            "introduction": "Forward mode automatic differentiation can seem abstract, but it has an elegant and concrete implementation using dual numbers. This exercise allows you to practice this method by hand, demonstrating how evaluating a function at $x_0 + \\epsilon$ lets you compute both the function's value and its derivative in a single pass. This hands-on calculation solidifies the core principle of carrying both a value and its derivative forward through computations .",
            "id": "2154638",
            "problem": "In the field of computational science, automatic differentiation is a powerful technique for numerically evaluating the derivative of a function. The forward mode of automatic differentiation can be implemented using dual numbers. A dual number has the form $z = a + b\\epsilon$, where $a$ and $b$ are real numbers and $\\epsilon$ is a non-real number with the property that $\\epsilon^2 = 0$. Arithmetic with dual numbers is defined naturally; for example, $(a+b\\epsilon)(c+d\\epsilon) = ac + (ad+bc)\\epsilon + bd\\epsilon^2 = ac + (ad+bc)\\epsilon$.\n\nA key property is that if we evaluate a polynomial function $f(x)$ at the dual number $x = x_0 + 1\\epsilon$, the result is a dual number $f(x_0) + f'(x_0)\\epsilon$. This allows for the simultaneous computation of the function's value and its derivative's value at the point $x_0$.\n\nConsider a simple signal processing model where a filter's response is described by the polynomial function $f(x) = 2x^3 - 5x^2 + 3x + 7$. An engineer wants to determine both the filter's gain, $f(x_0)$, and its sensitivity to change, $f'(x_0)$, at the input signal frequency $x_0 = 4$.\n\nBy evaluating $f(4 + 1\\epsilon)$, the engineer obtains a result of the form $A + B\\epsilon$. What is the pair of values $(A, B)$?\n\nA. $(67, 59)$\n\nB. $(59, 67)$\n\nC. $(67, 56)$\n\nD. $(67, 96)$\n\nE. $(128, 96)$",
            "solution": "We use the dual-number property: for $x=x_{0}+\\epsilon$ with $\\epsilon^{2}=0$, evaluating a polynomial gives\n$$\nf(x_{0}+\\epsilon)=f(x_{0})+f'(x_{0})\\epsilon.\n$$\nGiven $f(x)=2x^{3}-5x^{2}+3x+7$ and $x_{0}=4$, we can verify by direct expansion with $\\epsilon^{2}=0$:\n$$\n(4+\\epsilon)^{2}=16+8\\epsilon,\\quad (4+\\epsilon)^{3}=64+48\\epsilon.\n$$\nThen\n$$\nf(4+\\epsilon)=2(64+48\\epsilon)-5(16+8\\epsilon)+3(4+\\epsilon)+7.\n$$\nSeparate constant and $\\epsilon$ parts:\n- Constant part:\n$$\n2\\cdot 64-5\\cdot 16+3\\cdot 4+7=128-80+12+7=67.\n$$\n- $\\epsilon$ coefficient:\n$$\n2\\cdot 48-5\\cdot 8+3\\cdot 1=96-40+3=59.\n$$\nHence\n$$\nf(4+\\epsilon)=67+59\\epsilon,\n$$\nso $(A,B)=(67,59)$, which corresponds to option A.\n\nFor confirmation via derivatives, compute\n$$\nf'(x)=6x^{2}-10x+3,\\quad f'(4)=6\\cdot 16-10\\cdot 4+3=96-40+3=59,\n$$\nconsistent with the coefficient of $\\epsilon$ found above.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Reverse mode automatic differentiation, the engine behind modern machine learning, works by propagating derivatives backward from the output. In this practice, you will manually trace these \"adjoints\" through a simple computational graph, applying the chain rule at each node. This step-by-step process demystifies backpropagation and builds a strong intuition for how gradients are efficiently computed for functions with many inputs .",
            "id": "2154653",
            "problem": "In the field of machine learning, automatic differentiation is a crucial technique for training complex models. It allows for the efficient and exact computation of gradients. Consider a function $f(x_1, x_2)$ whose value is computed through a sequence of elementary operations, forming a computational graph. The sequence is defined as follows:\n\n1.  $v_1 = x_1 + 5$\n2.  $v_2 = v_1 \\cdot x_2$\n3.  $v_3 = \\frac{1}{v_1}$\n4.  $v_4 = \\exp(v_2)$\n5.  $f = v_3 + v_4$\n\nYour task is to compute the gradient of the function $f$ with respect to its inputs $x_1$ and $x_2$. Using the principles of reverse-mode automatic differentiation (also known as backpropagation), calculate the exact values of the partial derivatives $\\frac{\\partial f}{\\partial x_1}$ and $\\frac{\\partial f}{\\partial x_2}$ at the point $(x_1, x_2) = (-4, 0)$.\n\nPresent your answers for the pair $\\left(\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}\\right)$ in a row matrix. The final answer must be an analytic expression, not a numerical approximation.",
            "solution": "We are given the computational sequence:\n$$\nv_{1}=x_{1}+5,\\quad v_{2}=v_{1}x_{2},\\quad v_{3}=\\frac{1}{v_{1}},\\quad v_{4}=\\exp(v_{2}),\\quad f=v_{3}+v_{4}.\n$$\nWe compute the gradient at $(x_{1},x_{2})=(-4,0)$ using reverse-mode automatic differentiation, which applies the chain rule by propagating adjoints $\\bar{u}=\\frac{\\partial f}{\\partial u}$ backward through the graph.\n\nForward evaluation at $(x_{1},x_{2})=(-4,0)$:\n$$\nv_{1}=(-4)+5=1,\\quad v_{2}=v_{1}x_{2}=1\\cdot 0=0,\\quad v_{3}=\\frac{1}{v_{1}}=\\frac{1}{1}=1,\\quad v_{4}=\\exp(v_{2})=\\exp(0)=1,\n$$\nso\n$$\nf=v_{3}+v_{4}=1+1=2.\n$$\n\nBackward propagation (chain rule):\n1. Seed the output adjoint:\n$$\n\\bar{f}=1.\n$$\nSince $f=v_{3}+v_{4}$,\n$$\n\\frac{\\partial f}{\\partial v_{3}}=1,\\quad \\frac{\\partial f}{\\partial v_{4}}=1 \\;\\Rightarrow\\; \\bar{v}_{3}=\\bar{f}\\cdot 1=1,\\quad \\bar{v}_{4}=\\bar{f}\\cdot 1=1.\n$$\n\n2. For $v_{4}=\\exp(v_{2})$, we have $\\frac{\\partial v_{4}}{\\partial v_{2}}=\\exp(v_{2})=v_{4}$, hence\n$$\n\\bar{v}_{2}=\\bar{v}_{4}\\cdot \\frac{\\partial v_{4}}{\\partial v_{2}}=\\bar{v}_{4}\\cdot \\exp(v_{2})=1\\cdot 1=1.\n$$\n\n3. For $v_{3}=\\frac{1}{v_{1}}$, we have $\\frac{\\partial v_{3}}{\\partial v_{1}}=-\\frac{1}{v_{1}^{2}}$, hence\n$$\n\\bar{v}_{1}\\mathrel{+}= \\bar{v}_{3}\\cdot \\left(-\\frac{1}{v_{1}^{2}}\\right)=1\\cdot \\left(-\\frac{1}{1^{2}}\\right)=-1.\n$$\n\n4. For $v_{2}=v_{1}x_{2}$, we have $\\frac{\\partial v_{2}}{\\partial v_{1}}=x_{2}$ and $\\frac{\\partial v_{2}}{\\partial x_{2}}=v_{1}$, hence\n$$\n\\bar{v}_{1}\\mathrel{+}= \\bar{v}_{2}\\cdot x_{2}=1\\cdot 0=0,\\quad \\bar{x}_{2}\\mathrel{+}= \\bar{v}_{2}\\cdot v_{1}=1\\cdot 1=1.\n$$\nThus $\\bar{v}_{1}=-1+0=-1$.\n\n5. For $v_{1}=x_{1}+5$, we have $\\frac{\\partial v_{1}}{\\partial x_{1}}=1$, hence\n$$\n\\bar{x}_{1}=\\bar{v}_{1}\\cdot 1=-1.\n$$\n\nTherefore, the gradient at $(-4,0)$ is\n$$\n\\left(\\frac{\\partial f}{\\partial x_{1}},\\frac{\\partial f}{\\partial x_{2}}\\right)=(-1,1).\n$$\nAs a row matrix, this is\n$$\n\\begin{pmatrix}\n-1 & 1\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}-1 & 1\\end{pmatrix}}$$"
        },
        {
            "introduction": "Why choose one mode of automatic differentiation over the other? The answer often lies in computational efficiency, especially when dealing with functions of many variables. This problem asks you to analyze and compare the total number of arithmetic operations required by forward and reverse modes to compute a gradient . By deriving the cost for each mode, you will gain a clear, quantitative understanding of why reverse mode is the preferred choice for large-scale optimization problems.",
            "id": "2154645",
            "problem": "Automatic Differentiation (AD) is a family of techniques for numerically evaluating the derivatives of a function specified by a computer program. AD has two primary modes: forward mode and reverse mode. Consider a function $f: \\mathbb{R}^n \\to \\mathbb{R}$ defined as the product of its inputs:\n$$f(x_1, x_2, \\dots, x_n) = \\prod_{i=1}^{n} x_i$$\nTo analyze the computational cost of finding the gradient $\\nabla f$, we can represent the function's evaluation as a sequence of elementary operations, forming a computational graph. Let's define this sequence as follows:\n- Start with an initial value $v_0 = 1$.\n- For $i = 1, 2, \\dots, n$, compute the intermediate variables $v_i = v_{i-1} \\cdot x_i$.\n- The final result is $y = v_n$.\n\nWe are interested in the total number of arithmetic operations (defined as a single multiplication or a single addition) required to compute the full gradient vector $\\nabla f = \\left(\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n}\\right)$ using both modes.\n\n**Forward Mode AD:**\nThe gradient is computed by making $n$ passes through the computational graph, one for each input variable $x_j$. A single pass computes one partial derivative $\\frac{\\partial f}{\\partial x_j}$. The total cost consists of:\n1.  A single \"primal pass\" to compute and store the values of all variables $v_i$.\n2.  $n$ \"tangent passes\". In each pass $j$, the derivatives (or \"tangents\") $\\dot{v}_i = \\frac{\\partial v_i}{\\partial x_j}$ are propagated forward. The propagation rule for a multiplication step $v_i = v_{i-1} \\cdot x_i$ is $\\dot{v}_i = \\dot{v}_{i-1} \\cdot x_i + v_{i-1} \\cdot \\dot{x}_i$. This calculation for $\\dot{v}_i$ requires two multiplications and one addition.\n\n**Reverse Mode AD:**\nThe entire gradient is computed in a single reverse pass after the primal pass. The total cost consists of:\n1.  A single \"primal pass\" to compute and store the values of all variables $v_i$ and $x_i$.\n2.  A single \"adjoint pass\" that propagates the adjoints $\\bar{v}_i = \\frac{\\partial f}{\\partial v_i}$ and $\\bar{x}_i = \\frac{\\partial f}{\\partial x_i}$ backward. For a multiplication step $v_i = v_{i-1} \\cdot x_i$, the adjoints are updated via the rules: $\\bar{v}_{i-1} \\mathrel{+}= \\bar{v}_i \\cdot x_i$ and $\\bar{x}_i \\mathrel{+}= \\bar{v}_i \\cdot v_{i-1}$. Each of these update rules costs one multiplication and one addition.\n\nLet $N_{\\text{fwd}}$ be the total number of arithmetic operations (multiplications and additions) required to compute the entire gradient using forward mode, and $N_{\\text{rev}}$ be the corresponding total for reverse mode. Determine the ratio $R = \\frac{N_{\\text{fwd}}}{N_{\\text{rev}}}$ as an expression in terms of $n$.",
            "solution": "We model the evaluation as the sequence $v_{0}=1$, $v_{i}=v_{i-1}\\cdot x_{i}$ for $i=1,\\dots,n$, and $y=v_{n}$. Arithmetic operations are multiplications and additions.\n\nForward mode AD:\n- Primal pass computes all $v_{i}$ with $n$ multiplications, so the primal cost is $n$.\n- For each tangent pass with seed in $x_{j}$, the propagation rule at step $i$ is\n$$\n\\dot{v}_{i}=\\dot{v}_{i-1}\\cdot x_{i}+v_{i-1}\\cdot \\dot{x}_{i},\n$$\nwhich requires two multiplications and one addition. There are $n$ such steps per pass, so each pass costs $3n$ operations. There are $n$ passes for $j=1,\\dots,n$, hence total tangent cost $3n^{2}$. Therefore\n$$\nN_{\\text{fwd}}=n+3n^{2}.\n$$\n\nReverse mode AD:\n- Primal pass cost is the same as above: $n$ multiplications, so cost $n$.\n- In the adjoint pass, for each multiplication node $v_{i}=v_{i-1}\\cdot x_{i}$, we perform\n$$\n\\bar{v}_{i-1}\\mathrel{+}=\\bar{v}_{i}\\cdot x_{i},\\quad \\bar{x}_{i}\\mathrel{+}=\\bar{v}_{i}\\cdot v_{i-1},\n$$\nwith each update costing one multiplication and one addition. Thus each $i$ costs $2$ multiplications plus $2$ additions, i.e., $4$ operations. Over $n$ steps, the adjoint pass costs $4n$. Therefore\n$$\nN_{\\text{rev}}=n+4n=5n.\n$$\n\nThe ratio is\n$$\nR=\\frac{N_{\\text{fwd}}}{N_{\\text{rev}}}=\\frac{n+3n^{2}}{5n}=\\frac{3n+1}{5}.\n$$",
            "answer": "$$\\boxed{\\frac{3n+1}{5}}$$"
        }
    ]
}