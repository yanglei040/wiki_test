{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we will start with the forward mode of automatic differentiation. This first practice illustrates the core principle using the elegant concept of dual numbers. By representing numbers in the form $a + b\\epsilon$ with the property $\\epsilon^2 = 0$, we can simultaneously compute a function's value and its derivative in a single forward pass, providing a concrete algebraic foundation for understanding how AD works .",
            "id": "2154638",
            "problem": "In the field of computational science, automatic differentiation is a powerful technique for numerically evaluating the derivative of a function. The forward mode of automatic differentiation can be implemented using dual numbers. A dual number has the form $z = a + b\\epsilon$, where $a$ and $b$ are real numbers and $\\epsilon$ is a non-real number with the property that $\\epsilon^2 = 0$. Arithmetic with dual numbers is defined naturally; for example, $(a+b\\epsilon)(c+d\\epsilon) = ac + (ad+bc)\\epsilon + bd\\epsilon^2 = ac + (ad+bc)\\epsilon$.\n\nA key property is that if we evaluate a polynomial function $f(x)$ at the dual number $x = x_0 + 1\\epsilon$, the result is a dual number $f(x_0) + f'(x_0)\\epsilon$. This allows for the simultaneous computation of the function's value and its derivative's value at the point $x_0$.\n\nConsider a simple signal processing model where a filter's response is described by the polynomial function $f(x) = 2x^3 - 5x^2 + 3x + 7$. An engineer wants to determine both the filter's gain, $f(x_0)$, and its sensitivity to change, $f'(x_0)$, at the input signal frequency $x_0 = 4$.\n\nBy evaluating $f(4 + 1\\epsilon)$, the engineer obtains a result of the form $A + B\\epsilon$. What is the pair of values $(A, B)$?\n\nA. $(67, 59)$\n\nB. $(59, 67)$\n\nC. $(67, 56)$\n\nD. $(67, 96)$\n\nE. $(128, 96)$",
            "solution": "We use the dual-number property: for $x=x_0+\\epsilon$ with $\\epsilon^2=0$, evaluating a polynomial gives\n$$\nf(x_0+\\epsilon)=f(x_0)+f'(x_0)\\epsilon.\n$$\nGiven $f(x)=2x^3-5x^2+3x+7$ and $x_0=4$, we can verify by direct expansion with $\\epsilon^2=0$:\n$$\n(4+\\epsilon)^2=16+8\\epsilon,\\quad (4+\\epsilon)^3=64+48\\epsilon.\n$$\nThen\n$$\nf(4+\\epsilon)=2(64+48\\epsilon)-5(16+8\\epsilon)+3(4+\\epsilon)+7.\n$$\nSeparate constant and $\\epsilon$ parts:\n- Constant part:\n$$\n2\\cdot 64-5\\cdot 16+3\\cdot 4+7=128-80+12+7=67.\n$$\n- $\\epsilon$ coefficient:\n$$\n2\\cdot 48-5\\cdot 8+3\\cdot 1=96-40+3=59.\n$$\nHence\n$$\nf(4+\\epsilon)=67+59\\epsilon,\n$$\nso $(A,B)=(67,59)$, which corresponds to option A.\n\nFor confirmation via derivatives, compute\n$$\nf'(x)=6x^2-10x+3,\\quad f'(4)=6\\cdot 16-10\\cdot 4+3=96-40+3=59,\n$$\nconsistent with the coefficient of $\\epsilon$ found above.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Next, we turn our attention to the reverse mode of automatic differentiation, a technique also known as backpropagation, which is the cornerstone of modern machine learning. In this exercise, you will manually trace the flow of derivatives backward through a computational graph, starting from the final output and accumulating gradients at each node. This practice is essential for building a deep intuition for how neural networks and other complex models are trained efficiently .",
            "id": "2154653",
            "problem": "In the field of machine learning, automatic differentiation is a crucial technique for training complex models. It allows for the efficient and exact computation of gradients. Consider a function $f(x_1, x_2)$ whose value is computed through a sequence of elementary operations, forming a computational graph. The sequence is defined as follows:\n\n1.  $v_1 = x_1 + 5$\n2.  $v_2 = v_1 \\cdot x_2$\n3.  $v_3 = \\frac{1}{v_1}$\n4.  $v_4 = \\exp(v_2)$\n5.  $f = v_3 + v_4$\n\nYour task is to compute the gradient of the function $f$ with respect to its inputs $x_1$ and $x_2$. Using the principles of reverse-mode automatic differentiation (also known as backpropagation), calculate the exact values of the partial derivatives $\\frac{\\partial f}{\\partial x_1}$ and $\\frac{\\partial f}{\\partial x_2}$ at the point $(x_1, x_2) = (-4, 0)$.\n\nPresent your answers for the pair $\\left(\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}\\right)$ in a row matrix. The final answer must be an analytic expression, not a numerical approximation.",
            "solution": "We are given the computational sequence:\n$$\nv_1=x_1+5,\\quad v_2=v_1x_2,\\quad v_3=\\frac{1}{v_1},\\quad v_4=\\exp(v_2),\\quad f=v_3+v_4.\n$$\nWe compute the gradient at $(x_1,x_2)=(-4,0)$ using reverse-mode automatic differentiation, which applies the chain rule by propagating adjoints $\\bar{u}=\\frac{\\partial f}{\\partial u}$ backward through the graph.\n\nForward evaluation at $(x_1,x_2)=(-4,0)$:\n$$\nv_1=(-4)+5=1,\\quad v_2=v_1x_2=1\\cdot 0=0,\\quad v_3=\\frac{1}{v_1}=\\frac{1}{1}=1,\\quad v_4=\\exp(v_2)=\\exp(0)=1,\n$$\nso\n$$\nf=v_3+v_4=1+1=2.\n$$\n\nBackward propagation (chain rule):\n1. Seed the output adjoint:\n$$\n\\bar{f}=1.\n$$\nSince $f=v_3+v_4$,\n$$\n\\frac{\\partial f}{\\partial v_3}=1,\\quad \\frac{\\partial f}{\\partial v_4}=1 \\;\\Rightarrow\\; \\bar{v}_3=\\bar{f}\\cdot 1=1,\\quad \\bar{v}_4=\\bar{f}\\cdot 1=1.\n$$\n\n2. For $v_4=\\exp(v_2)$, we have $\\frac{\\partial v_4}{\\partial v_2}=\\exp(v_2)=v_4$, hence\n$$\n\\bar{v}_2=\\bar{v}_4\\cdot \\frac{\\partial v_4}{\\partial v_2}=\\bar{v}_4\\cdot \\exp(v_2)=1\\cdot 1=1.\n$$\n\n3. For $v_3=\\frac{1}{v_1}$, we have $\\frac{\\partial v_3}{\\partial v_1}=-\\frac{1}{v_1^2}$, hence\n$$\n\\bar{v}_1\\mathrel{+}= \\bar{v}_3\\cdot \\left(-\\frac{1}{v_1^2}\\right)=1\\cdot \\left(-\\frac{1}{1^2}\\right)=-1.\n$$\n\n4. For $v_2=v_1x_2$, we have $\\frac{\\partial v_2}{\\partial v_1}=x_2$ and $\\frac{\\partial v_2}{\\partial x_2}=v_1$, hence\n$$\n\\bar{v}_1\\mathrel{+}= \\bar{v}_2\\cdot x_2=1\\cdot 0=0,\\quad \\bar{x}_2\\mathrel{+}= \\bar{v}_2\\cdot v_1=1\\cdot 1=1.\n$$\nThus $\\bar{v}_1=-1+0=-1$.\n\n5. For $v_1=x_1+5$, we have $\\frac{\\partial v_1}{\\partial x_1}=1$, hence\n$$\n\\bar{x}_1=\\bar{v}_1\\cdot 1=-1.\n$$\n\nTherefore, the gradient at $(-4,0)$ is\n$$\n\\left(\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2}\\right)=(-1,1).\n$$\nAs a row matrix, this is\n$$\n\\begin{pmatrix}\n-1 & 1\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}-1 & 1\\end{pmatrix}}$$"
        },
        {
            "introduction": "Having practiced both forward and reverse modes, it is crucial to understand when to use each. This final practice guides you through a comparative analysis of their computational efficiency for a common scenario: a function with many inputs and a single output, such as $f: \\mathbb{R}^n \\to \\mathbb{R}$. By counting the fundamental arithmetic operations, you will discover the significant performance advantage of reverse mode in this context, revealing why it has become the default choice for large-scale optimization problems .",
            "id": "2154645",
            "problem": "Automatic Differentiation (AD) is a family of techniques for numerically evaluating the derivatives of a function specified by a computer program. AD has two primary modes: forward mode and reverse mode. Consider a function $f: \\mathbb{R}^n \\to \\mathbb{R}$ defined as the product of its inputs:\n$$f(x_1, x_2, \\dots, x_n) = \\prod_{i=1}^{n} x_i$$\nTo analyze the computational cost of finding the gradient $\\nabla f$, we can represent the function's evaluation as a sequence of elementary operations, forming a computational graph. Let's define this sequence as follows:\n- Start with an initial value $v_0 = 1$.\n- For $i = 1, 2, \\dots, n$, compute the intermediate variables $v_i = v_{i-1} \\cdot x_i$.\n- The final result is $y = v_n$.\n\nWe are interested in the total number of arithmetic operations (defined as a single multiplication or a single addition) required to compute the full gradient vector $\\nabla f = \\left(\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n}\\right)$ using both modes.\n\n**Forward Mode AD:**\nThe gradient is computed by making $n$ passes through the computational graph, one for each input variable $x_j$. A single pass computes one partial derivative $\\frac{\\partial f}{\\partial x_j}$. The total cost consists of:\n1.  A single \"primal pass\" to compute and store the values of all variables $v_i$.\n2.  $n$ \"tangent passes\". In each pass $j$, the derivatives (or \"tangents\") $\\dot{v}_i = \\frac{\\partial v_i}{\\partial x_j}$ are propagated forward. The propagation rule for a multiplication step $v_i = v_{i-1} \\cdot x_i$ is $\\dot{v}_i = \\dot{v}_{i-1} \\cdot x_i + v_{i-1} \\cdot \\dot{x}_i$. This calculation for $\\dot{v}_i$ requires two multiplications and one addition.\n\n**Reverse Mode AD:**\nThe entire gradient is computed in a single reverse pass after the primal pass. The total cost consists of:\n1.  A single \"primal pass\" to compute and store the values of all variables $v_i$ and $x_i$.\n2.  A single \"adjoint pass\" that propagates the adjoints $\\bar{v}_i = \\frac{\\partial f}{\\partial v_i}$ and $\\bar{x}_i = \\frac{\\partial f}{\\partial x_i}$ backward. For a multiplication step $v_i = v_{i-1} \\cdot x_i$, the adjoints are updated via the rules: $\\bar{v}_{i-1} \\mathrel{+}= \\bar{v}_i \\cdot x_i$ and $\\bar{x}_i \\mathrel{+}= \\bar{v}_i \\cdot v_{i-1}$. Each of these update rules costs one multiplication and one addition.\n\nLet $N_{\\text{fwd}}$ be the total number of arithmetic operations (multiplications and additions) required to compute the entire gradient using forward mode, and $N_{\\text{rev}}$ be the corresponding total for reverse mode. Determine the ratio $R = \\frac{N_{\\text{fwd}}}{N_{\\text{rev}}}$ as an expression in terms of $n$.",
            "solution": "We model the evaluation as the sequence $v_0=1$, $v_i=v_{i-1}\\cdot x_i$ for $i=1,\\dots,n$, and $y=v_n$. Arithmetic operations are multiplications and additions.\n\nForward mode AD:\n- Primal pass computes all $v_i$ with $n$ multiplications, so the primal cost is $n$.\n- For each tangent pass with seed in $x_j$, the propagation rule at step $i$ is\n$$\n\\dot{v}_i=\\dot{v}_{i-1}\\cdot x_i+v_{i-1}\\cdot \\dot{x}_i,\n$$\nwhich requires two multiplications and one addition. There are $n$ such steps per pass, so each pass costs $3n$ operations. There are $n$ passes for $j=1,\\dots,n$, hence total tangent cost $3n^2$. Therefore\n$$\nN_{\\text{fwd}}=n+3n^2.\n$$\n\nReverse mode AD:\n- Primal pass cost is the same as above: $n$ multiplications, so cost $n$.\n- In the adjoint pass, for each multiplication node $v_i=v_{i-1}\\cdot x_i$, we perform\n$$\n\\bar{v}_{i-1}\\mathrel{+}=\\bar{v}_i\\cdot x_i,\\quad \\bar{x}_i\\mathrel{+}=\\bar{v}_i\\cdot v_{i-1},\n$$\nwith each update costing one multiplication and one addition. Thus each $i$ costs $2$ multiplications plus $2$ additions, i.e., $4$ operations. Over $n$ steps, the adjoint pass costs $4n$. Therefore\n$$\nN_{\\text{rev}}=n+4n=5n.\n$$\n\nThe ratio is\n$$\nR=\\frac{N_{\\text{fwd}}}{N_{\\text{rev}}}=\\frac{n+3n^2}{5n}=\\frac{3n+1}{5}.\n$$",
            "answer": "$$\\boxed{\\frac{3n+1}{5}}$$"
        }
    ]
}