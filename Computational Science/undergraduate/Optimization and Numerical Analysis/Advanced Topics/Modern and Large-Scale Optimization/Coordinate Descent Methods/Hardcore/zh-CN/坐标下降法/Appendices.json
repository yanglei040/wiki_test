{
    "hands_on_practices": [
        {
            "introduction": "第一个练习旨在让您直接上手体验坐标下降算法。通过在一个简单的二元函数上手动执行一个完整的周期，您将确切地看到该方法如何沿每个坐标轴进行迭代最小化。这项实践对于建立对算法核心过程的扎实直觉至关重要。",
            "id": "2164480",
            "problem": "考虑二元函数 $f(x,y) = 2x^2 + y^2 + xy - 6x - 5y$。我们采用一个优化程序，使用坐标下降算法来寻找该函数的最小值。该过程从初始点 $(x_0, y_0) = (0, 0)$ 开始。\n\n要求您执行整整一个周期的坐标下降。周期内的更新顺序如下：首先，关于 $x$ 坐标最小化函数，然后，使用新找到的 $x$ 坐标，关于 $y$ 坐标最小化函数。\n\n确定这单个周期后点的坐标 $(x_1, y_1)$。您的答案应包含两个坐标值，表示为精确分数。",
            "solution": "我们对二次函数 $f(x,y) = 2x^{2} + y^{2} + xy - 6x - 5y$ 执行一个完整的坐标下降周期，从点 $(x_{0}, y_{0}) = (0, 0)$ 开始，首先更新 $x$，然后更新 $y$。对于一个固定的坐标，最小化一个可微函数关于该坐标的方法是将其对应的偏导数设为零；由于 $f$ 在每个坐标上都是二次且严格凸的（由正的二阶偏导数验证），这会得到该坐标上的唯一最小化点。\n\n首先，固定 $y = y_{0} = 0$，对 $x$ 进行最小化。计算关于 $x$ 的偏导数：\n$$\n\\frac{\\partial f}{\\partial x} = 4x + y - 6.\n$$\n在 $y=0$ 时将其设为零：\n$$\n4x + 0 - 6 = 0 \\quad \\Longrightarrow \\quad 4x = 6 \\quad \\Longrightarrow \\quad x = \\frac{3}{2}.\n$$\n由于 $\\frac{\\partial^{2} f}{\\partial x^{2}} = 4  0$，因此对于固定的 $y=0$，这是 $x$ 的唯一最小化点。因此，在 $x$ 更新后，我们得到 $x_{1} = \\frac{3}{2}$，而 $y$ 保持为 $0$。\n\n接下来，固定 $x = x_{1} = \\frac{3}{2}$，对 $y$ 进行最小化。计算关于 $y$ 的偏导数：\n$$\n\\frac{\\partial f}{\\partial y} = 2y + x - 5.\n$$\n在 $x=\\frac{3}{2}$ 时将其设为零：\n$$\n2y + \\frac{3}{2} - 5 = 0 \\quad \\Longrightarrow \\quad 2y - \\frac{7}{2} = 0 \\quad \\Longrightarrow \\quad 2y = \\frac{7}{2} \\quad \\Longrightarrow \\quad y = \\frac{7}{4}.\n$$\n由于 $\\frac{\\partial^{2} f}{\\partial y^{2}} = 2  0$，因此对于固定的 $x=\\frac{3}{2}$，这是 $y$ 的唯一最小化点。因此，经过一个完整的周期后，更新后的点是 $\\left(x_{1}, y_{1}\\right) = \\left(\\frac{3}{2}, \\frac{7}{4}\\right)$。",
            "answer": "$$\\boxed{\\left(\\frac{3}{2}, \\frac{7}{4}\\right)}$$"
        },
        {
            "introduction": "为了真正理解坐标下降法的独特特性，将其与其他优化技术进行比较会很有帮助。本练习要求您从同一起点计算坐标下降法和广泛使用的梯度下降法的第一步。通过对比它们各自的更新路径，您将更清晰地理解它们不同的搜索策略和几何行为。",
            "id": "2164428",
            "problem": "考虑最小化函数 $f(x, y) = 2x^2 + y^2 - xy + x - 4y$ 的优化问题。我们将探讨两种不同的迭代优化算法的第一步，两者都从初始点 $(x_0, y_0) = (2, 3)$ 开始。\n\n1.  **坐标下降法 (CD)：** 执行一轮完整的坐标下降，从更新 $x$ 坐标开始。这包括两个连续的步骤。首先，将 $y$ 固定在其初始值 $y_0 = 3$，找到使函数最小化的 $x$ 的值；令其为更新后的 $x$ 坐标。其次，使用这个新的 $x$ 坐标，找到使函数最小化的 $y$ 的值。设这一轮之后得到的点为 $(x_{CD}, y_{CD})$。\n\n2.  **梯度下降法 (GD)：** 执行一步梯度下降，固定学习率为 $\\alpha = 0.1$。点 $\\mathbf{x}_k$ 的更新由规则 $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)$ 给出。设得到的点为 $(x_{GD}, y_{GD})$。\n\n确定这两点的坐标。将您的答案表示为一个单行矩阵，其中包含按顺序排列的四个值 $x_{CD}, y_{CD}, x_{GD}, y_{GD}$。这些值应以精确的整数或分数形式给出。",
            "solution": "我们最小化二次函数 $f(x,y)=2x^{2}+y^{2}-xy+x-4y$。\n\n对于坐标下降法，我们从 $(x_{0},y_{0})=(2,3)$ 开始，首先在 $y$ 固定为 $y_{0}=3$ 的情况下对 $x$ 进行最小化。单变量函数为\n$$\nf(x,3)=2x^{2}+3^{2}-x\\cdot 3+x-4\\cdot 3=2x^{2}-2x-3.\n$$\n对 $x$ 求导并令其为零：\n$$\n\\frac{\\partial}{\\partial x}f(x,3)=4x-2=0 \\quad\\Longrightarrow\\quad x=\\frac{1}{2},\n$$\n并且由于 $\\frac{\\partial^{2}}{\\partial x^{2}}f(x,3)=40$，所以这是最小值点。接下来，在 $x=\\frac{1}{2}$ 固定的情况下，对 $y$ 进行最小化：\n$$\nf\\!\\left(\\frac{1}{2},y\\right)=2\\left(\\frac{1}{2}\\right)^{2}+y^{2}-\\frac{1}{2}y+\\frac{1}{2}-4y=y^{2}-\\frac{9}{2}y+1.\n$$\n对 $y$ 求导并令其为零：\n$$\n\\frac{\\partial}{\\partial y}f\\!\\left(\\frac{1}{2},y\\right)=2y-\\frac{9}{2}=0 \\quad\\Longrightarrow\\quad y=\\frac{9}{4},\n$$\n而 $\\frac{\\partial^{2}}{\\partial y^{2}}f=20$ 证实了这是一个最小值点。因此 $(x_{CD},y_{CD})=\\left(\\frac{1}{2},\\frac{9}{4}\\right)$。\n\n对于学习率为 $\\alpha=\\frac{1}{10}$ 的一步梯度下降，使用更新规则 $\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\alpha\\nabla f(\\mathbf{x}_{k})$。梯度是\n$$\n\\nabla f(x,y)=\\bigl(4x-y+1,\\;2y-x-4\\bigr).\n$$\n在点 $(2,3)$ 处，\n$$\n\\nabla f(2,3)=\\bigl(4\\cdot 2-3+1,\\;2\\cdot 3-2-4\\bigr)=(6,0).\n$$\n因此\n$$\n(x_{GD},y_{GD})=(2,3)-\\frac{1}{10}(6,0)=\\left(2-\\frac{6}{10},\\,3\\right)=\\left(\\frac{7}{5},\\,3\\right).\n$$\n\n因此，按顺序 $x_{CD}, y_{CD}, x_{GD}, y_{GD}$ 排列的所要求的行矩阵是 $\\begin{pmatrix}\\frac{1}{2}  \\frac{9}{4}  \\frac{7}{5}  3\\end{pmatrix}$。",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2}  \\frac{9}{4}  \\frac{7}{5}  3\\end{pmatrix}}$$"
        },
        {
            "introduction": "一个算法的可靠性取决于保证其成功的条件。这个问题将我们的重点从计算转移到坐标下降法的理论基础。您将分析一个函数，对于该函数，作为算法核心的坐标级最小化步骤可能没有明确定义，这突显了在应用该方法之前验证关键假设的重要性。",
            "id": "2164433",
            "problem": "坐标下降算法是一种优化方法，它通过沿每个坐标轴依次最小化的方式来寻找多变量函数的局部最小值。\n\n为了使该算法是良定义的，并使其极限点为驻点，通常要求目标函数满足某些条件。\n\n考虑函数 $f(x, y) = \\exp(x^2 - y^2)$，其中 $(x,y) \\in \\mathbb{R}^2$。\n\n我们将针对坐标下降法成功应用于连续可微函数 $g: \\mathbb{R}^2 \\to \\mathbb{R}$ 的两个常见充分条件来分析此函数：\n\n**条件 I：** 函数 $g$ 在其定义域上是凸函数。\n\n**条件 II：** 对于定义域中的任意点 $(x_k, y_k)$，一维最小化问题 $\\min_{x} g(x, y_k)$ 和 $\\min_{y} g(x_k, y)$ 各自都有良定义且唯一的解。\n\n下列哪个陈述正确描述了函数 $f(x, y)$ 与这些条件的关系？\n\nA. $f(x, y)$ 同时满足条件 I 和条件 II。\n\nB. $f(x, y)$ 满足条件 I 但不满足条件 II。\n\nC. $f(x, y)$ 满足条件 II 但不满足条件 I。\n\nD. $f(x, y)$ 既不满足条件 I 也不满足条件 II。",
            "solution": "我们根据所述条件来分析 $f(x,y)=\\exp(x^{2}-y^{2})$。\n\n对于条件 I (凸性)：由于 $f$ 是二阶连续可微的，其在 $\\mathbb{R}^{2}$ 上为凸函数的一个标准的充分必要条件是其 Hessian 矩阵处处是半正定的。计算梯度和 Hessian 矩阵。令 $u=x^{2}-y^{2}$，则 $f=\\exp(u)$。于是\n$$\n\\frac{\\partial f}{\\partial x}=2x\\,\\exp(u), \\qquad \\frac{\\partial f}{\\partial y}=-2y\\,\\exp(u).\n$$\n再次求导，\n$$\n\\frac{\\partial^{2} f}{\\partial x^{2}}=\\exp(u)\\left(2+4x^{2}\\right), \\qquad \\frac{\\partial^{2} f}{\\partial y^{2}}=\\exp(u)\\left(-2+4y^{2}\\right), \\qquad \\frac{\\partial^{2} f}{\\partial x\\,\\partial y}=-4xy\\,\\exp(u).\n$$\n因此 Hessian 矩阵为\n$$\n\\nabla^{2} f(x,y)=\\exp(x^{2}-y^{2})\\begin{pmatrix}2+4x^{2}  -4xy \\\\ -4xy  -2+4y^{2}\\end{pmatrix}.\n$$\n在点 $(x,y)=(0,0)$ 处，该矩阵变为\n$$\n\\nabla^{2} f(0,0)=\\begin{pmatrix}2  0 \\\\ 0  -2\\end{pmatrix},\n$$\n该矩阵的特征值为 $2$ 和 $-2$，因此是不定的，而不是半正定的。所以 $f$ 在 $\\mathbb{R}^{2}$ 上不是凸函数，故条件 I 不成立。\n\n对于条件 II (对于任意 $(x_{k},y_{k})$，一维最小化子存在且唯一)：固定 $y_{k}\\in\\mathbb{R}$ 并考虑 $\\min_{x} f(x,y_{k})$。由于\n$$\nf(x,y_{k})=\\exp(-y_{k}^{2})\\,\\exp(x^{2}),\n$$\n对 $x$ 进行最小化等价于最小化 $\\exp(x^{2})$。计算关于 $x$ 的导数：\n$$\n\\frac{d}{dx}f(x,y_{k})=2x\\,\\exp(x^{2}-y_{k}^{2}),\n$$\n该导数当且仅当 $x=0$ 时为零。其二阶导数为\n$$\n\\frac{d^{2}}{dx^{2}}f(x,y_{k})=\\exp(x^{2}-y_{k}^{2})\\left(2+4x^{2}\\right)0 \\quad \\text{for all } x,\n$$\n所以 $x=0$ 是一个严格全局最小化子。因此，对于每个固定的 $y_{k}$，$\\min_{x} f(x,y_{k})$ 都有一个良定义且唯一的解 $x^{\\ast}=0$。\n\n现在固定 $x_{k}\\in\\mathbb{R}$ 并考虑 $\\min_{y} f(x_{k},y)$。我们有\n$$\nf(x_{k},y)=\\exp(x_{k}^{2})\\,\\exp(-y^{2}),\n$$\n所以对 $y$ 进行最小化等同于最小化 $\\exp(-y^{2})$。关于 $y$ 的导数是\n$$\n\\frac{d}{dy}f(x_{k},y)=-2y\\,\\exp(x_{k}^{2}-y^{2}),\n$$\n该导数在 $y=0$ 处为零。二阶导数是\n$$\n\\frac{d^{2}}{dy^{2}}f(x_{k},y)=\\exp(x_{k}^{2}-y^{2})\\left(-2+4y^{2}\\right),\n$$\n因此在 $y=0$ 处，二阶导数等于 $-2\\,\\exp(x_{k}^{2})  0$，表明这是一个局部最大值。此外，$\\lim_{|y|\\to\\infty} f(x_{k},y)=0$，所以 $f(x_{k},y)$ 关于 $y$ 的下确界是 $0$，但对于任何有限的 $y$都无法取到这个值。因此，一维最小化问题 $\\min_{y} f(x_{k},y)$ 没有良定义的解（最小化子不存在），故条件 II 不成立。\n\n由于 $f$ 不是凸函数（条件 I 不成立），且关于 $y$ 的子问题没有可达到的最小化子（条件 II 不成立），因此正确的选项是 $f$ 两个条件都不满足。",
            "answer": "$$\\boxed{D}$$"
        }
    ]
}