{
    "hands_on_practices": [
        {
            "introduction": "The subgradient method is a powerful tool for nonsmooth optimization, but it behaves differently from the familiar gradient descent method. A key difference is that a subgradient step is not guaranteed to decrease the function's value. This exercise illustrates this fundamental, non-descent property using a simple one-dimensional problem, highlighting how the choice of step size can cause the iterates to \"overshoot\" the minimum. ",
            "id": "2207140",
            "problem": "Consider the problem of minimizing the nonsmooth convex function $f(x) = |x|$ using the subgradient method. The iterative update rule for the subgradient method is given by $x_{k+1} = x_k - \\alpha_k g_k$, where $x_k$ is the iterate at step $k$, $\\alpha_k$ is the step size, and $g_k$ is a subgradient of the function $f$ at the point $x_k$. For the function $f(x)=|x|$, a valid subgradient at any point $x \\neq 0$ is given by $g = \\text{sgn}(x)$, where $\\text{sgn}(x)$ is the sign function which returns $1$ for $x0$ and $-1$ for $x0$.\n\nSuppose we start the iteration at the point $x_0 = 5$ and use a constant step size $\\alpha_k = \\alpha = 10$ for all steps.\n\nCalculate the value of the next iterate, $x_1$.",
            "solution": "We are minimizing the convex nonsmooth function $f(x)=|x|$ using the subgradient method with update rule\n$$\nx_{k+1} = x_{k} - \\alpha_{k} g_{k},\n$$\nwhere $g_{k} \\in \\partial f(x_{k})$ is a subgradient of $f$ at $x_{k}$. For $f(x)=|x|$ and any $x \\neq 0$, a valid subgradient is $g=\\text{sgn}(x)$, where $\\text{sgn}(x)=1$ if $x0$ and $\\text{sgn}(x)=-1$ if $x0$.\n\nGiven $x_{0}=5$ and constant step size $\\alpha_{k}=\\alpha=10$, we compute the subgradient at $x_{0}$:\n$$\ng_{0}=\\text{sgn}(x_{0})=\\text{sgn}(5)=1.\n$$\nApplying the update rule,\n$$\nx_{1}=x_{0}-\\alpha g_{0}=5-10\\cdot 1=-5.\n$$\nThus, the next iterate is $x_{1}=-5$.",
            "answer": "$$\\boxed{-5}$$"
        },
        {
            "introduction": "For a nonsmooth function, the concept of a gradient is replaced by the subdifferential, which is a set of vectors called subgradients. At points where the function is not differentiable, this set can contain more than one element, giving us a choice of direction. This hands-on problem explores the practical implications of this non-uniqueness by showing how selecting different valid subgradients at the same point can lead to different subsequent iterates. ",
            "id": "2207146",
            "problem": "Consider the problem of minimizing the $L_1$-norm function $f(x_1, x_2) = |x_1| + |x_2|$ in $\\mathbb{R}^2$ using the subgradient method. The iterative update rule for the subgradient method is given by:\n$$x_{k+1} = x_k - \\alpha_k g_k$$\nwhere $x_k$ is the current iterate, $\\alpha_k  0$ is the step size, and $g_k$ is a subgradient of the function $f$ at $x_k$.\n\nThe set of all subgradients of the $L_1$-norm, known as the subdifferential $\\partial f(x)$, is defined as follows: A vector $g = (g_1, g_2)$ is a subgradient of $f(x_1, x_2) = |x_1| + |x_2|$ at $x = (x_1, x_2)$ if its components satisfy:\n$$ g_i = \\begin{cases} \\operatorname{sign}(x_i)  \\text{if } x_i \\neq 0 \\\\ v_i  \\text{if } x_i = 0 \\end{cases} $$\nwhere $\\operatorname{sign}(\\cdot)$ is the sign function, and for any component $i$ where $x_i = 0$, $v_i$ can be any value in the interval $[-1, 1]$.\n\nSuppose we start at the point $x_0 = (1, 0)$ and use a fixed step size of $\\alpha_0 = 0.5$. Since the second component of $x_0$ is zero, the subgradient at this point is not unique.\n\nLet's consider two distinct valid subgradients at $x_0$.\nFirst, choose the subgradient $g^{(a)}$ where the component corresponding to the zero coordinate of $x_0$ is set to its minimum possible value from its allowed interval.\nSecond, choose the subgradient $g^{(b)}$ where the component corresponding to the zero coordinate of $x_0$ is set to its maximum possible value from its allowed interval.\n\nCalculate the next two iterates, $x_1^{(a)}$ and $x_1^{(b)}$, resulting from applying one step of the subgradient method with subgradients $g^{(a)}$ and $g^{(b)}$, respectively.\n\nYour final answer should be a row matrix containing the four coordinates of these two points in the order $(x_{1}^{(a)}, x_{2}^{(a)}, x_{1}^{(b)}, x_{2}^{(b)})$.",
            "solution": "We minimize $f(x_{1},x_{2})=|x_{1}|+|x_{2}|$ with subgradient method $x_{k+1}=x_{k}-\\alpha_{k}g_{k}$, where $g_{k}\\in\\partial f(x_{k})$. The subdifferential is given componentwise by $g_{i}=\\operatorname{sign}(x_{i})$ if $x_{i}\\neq 0$ and $g_{i}\\in[-1,1]$ if $x_{i}=0$.\n\nAt $x_{0}=(1,0)$, the subgradient components satisfy $g_{1}=\\operatorname{sign}(1)=1$ and $g_{2}\\in[-1,1]$. With step size $\\alpha_{0}=\\frac{1}{2}$:\n- Choose $g^{(a)}=(1,-1)$ by taking the minimum permissible value for the second component.\n- Choose $g^{(b)}=(1,1)$ by taking the maximum permissible value for the second component.\n\nApply the update:\n$$x_{1}^{(a)}=x_{0}-\\alpha_{0}g^{(a)}=(1,0)-\\frac{1}{2}(1,-1)=\\left(1-\\frac{1}{2},\\,0-(-\\frac{1}{2})\\right)=\\left(\\frac{1}{2},\\,\\frac{1}{2}\\right),$$\n$$x_{1}^{(b)}=x_{0}-\\alpha_{0}g^{(b)}=(1,0)-\\frac{1}{2}(1,1)=\\left(1-\\frac{1}{2},\\,0-\\frac{1}{2}\\right)=\\left(\\frac{1}{2},\\,-\\frac{1}{2}\\right).$$\n\nThus, in the order $(x_{1}^{(a)}, x_{2}^{(a)}, x_{1}^{(b)}, x_{2}^{(b)})$, the row matrix is $\\left(\\frac{1}{2},\\,\\frac{1}{2},\\,\\frac{1}{2},\\,-\\frac{1}{2}\\right)$.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2}  \\frac{1}{2}  \\frac{1}{2}  -\\frac{1}{2}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Many optimization problems in science and engineering involve constraints on the decision variables. The projected subgradient method extends the standard algorithm to handle such constrained problems by \"projecting\" the updated point back onto the feasible set after each step. This exercise provides a clear introduction to this technique, combining a standard subgradient update with an intuitive projection onto a simple interval. ",
            "id": "2207183",
            "problem": "Consider the constrained optimization problem of minimizing the function $f(x) = |x - 10|$ for $x$ in the feasible set $C = [0, 5]$. We will apply one iteration of the projected subgradient method to find an updated estimate for the minimum.\n\nThe iterative update rule for the projected subgradient method is given by:\n$$x_{k+1} = P_C(x_k - \\alpha_k g_k)$$\nwhere $x_k$ is the current point, $g_k$ is a subgradient of the objective function $f$ at $x_k$, $\\alpha_k$ is the step size, and $P_C(y)$ is the Euclidean projection of a point $y$ onto the feasible set $C$. For a closed interval $[a, b]$, the projection of a point $y$ onto this interval is the point in $[a,b]$ that is closest to $y$.\n\nGiven the initial point $x_0 = 4$ and a constant step size of $\\alpha = 3$, calculate the value of the next iterate, $x_1$.",
            "solution": "The problem asks for one iteration of the projected subgradient method to minimize $f(x) = |x-10|$ on the set $C = [0, 5]$, starting from $x_0=4$ with a step size $\\alpha=3$. The update rule is $x_1 = P_C(x_0 - \\alpha g_0)$, where $g_0$ is a subgradient of $f$ at $x_0$, and $P_C$ is the projection onto $C$.\n\n**Step 1: Find a subgradient of $f(x)$ at $x_0=4$.**\nThe objective function is $f(x) = |x - 10|$. The subdifferential of a function $h(x) = |x-c|$ at a point $x$ is given by:\n$$\n\\partial h(x) =\n\\begin{cases}\n\\{1\\}  \\text{if } x  c \\\\\n\\{-1\\}  \\text{if } x  c \\\\\n[-1, 1]  \\text{if } x = c\n\\end{cases}\n$$\nIn our case, $c=10$ and we are evaluating the subgradient at $x_0 = 4$. Since $4  10$, the function $f(x)$ is differentiable at this point, and its subdifferential contains only one element, which is the derivative. The function for $x10$ is $f(x) = -(x-10) = 10-x$. The derivative is $f'(x) = -1$.\nSo, a subgradient $g_0$ of $f$ at $x_0=4$ is $g_0 = -1$.\n\n**Step 2: Perform the unconstrained update step.**\nLet's first compute the point before projection, which we denote as $y_1$:\n$$y_1 = x_0 - \\alpha g_0$$\nWe are given $x_0 = 4$, $\\alpha = 3$, and we found $g_0 = -1$. Substituting these values:\n$$y_1 = 4 - 3(-1) = 4 + 3 = 7$$\n\n**Step 3: Project the point $y_1$ onto the feasible set $C$.**\nThe final step is to find the next iterate $x_1$ by projecting the intermediate point $y_1=7$ onto the feasible set $C = [0, 5]$. The projection $P_C(y_1)$ is the point in the interval $[0, 5]$ that is closest to $y_1=7$.\nThe point $y_1 = 7$ lies to the right of the interval $[0, 5]$. The point in the interval closest to 7 is the right endpoint, which is 5.\n\nFormally, the projection of a point $y$ onto a closed interval $[a,b]$ is given by the formula:\n$$P_{[a,b]}(y) = \\max(a, \\min(b, y))$$\nApplying this formula with $a=0$, $b=5$, and $y=7$:\n$$x_1 = P_{[0,5]}(7) = \\max(0, \\min(5, 7))$$\nFirst, we evaluate the inner part: $\\min(5, 7) = 5$.\nThen, we evaluate the outer part: $\\max(0, 5) = 5$.\nTherefore, the next iterate is $x_1 = 5$.",
            "answer": "$$\\boxed{5}$$"
        }
    ]
}