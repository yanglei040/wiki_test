{
    "hands_on_practices": [
        {
            "introduction": "为了真正掌握动量法，让我们从基础开始。第一个练习将引导你使用经典动量法，在一个简单的二次函数上进行分步计算。通过手动计算最初的几步，你将对速度项 $v_t$ 如何累积梯度信息并影响优化路径建立起扎实的直觉。",
            "id": "2187765",
            "problem": "一个优化算法被用来最小化一个成本函数 $f(x, y)$，该函数依赖于两个参数 $x$ 和 $y$。成本函数由下式给出：\n$$f(x, y) = x^2 + 2y^2$$\n\n所选的算法是经典动量法。在步骤 $t$ 时，参数向量 $w_t = (x_t, y_t)$ 和速度向量 $v_t$ 的更新规则如下：\n1. $v_t = \\gamma v_{t-1} + \\eta \\nabla f(w_{t-1})$\n2. $w_t = w_{t-1} - v_t$\n\n其中 $\\nabla f(w_{t-1})$ 是成本函数在先前参数向量 $w_{t-1}$ 处计算的梯度。\n\n优化从初始参数向量 $w_0 = (x_0, y_0) = (4, 2)$ 开始。初始速度向量为 $v_0 = (0, 0)$。算法的超参数设置为学习率 $\\eta = 0.1$ 和动量参数 $\\gamma = 0.9$。\n\n你的任务是计算经过两次完整更新步骤后的参数向量 $w_2 = (x_2, y_2)$。将你的答案表示为一个具有两个分量的行向量。",
            "solution": "成本函数为 $f(x,y)=x^{2}+2y^{2}$，所以其梯度为\n$$\n\\nabla f(x,y)=\\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ [4pt] \\frac{\\partial f}{\\partial y} \\end{pmatrix}\n=\\begin{pmatrix} 2x \\\\ [4pt] 4y \\end{pmatrix}.\n$$\n动量更新公式为 $v_{t}=\\gamma v_{t-1}+\\eta\\,\\nabla f(w_{t-1})$ 和 $w_{t}=w_{t-1}-v_{t}$，其中 $w_{0}=\\begin{pmatrix}4\\\\ [2pt]2\\end{pmatrix}$，$v_{0}=\\begin{pmatrix}0\\\\ [2pt]0\\end{pmatrix}$，$\\eta=\\frac{1}{10}$，以及 $\\gamma=\\frac{9}{10}$。\n\n第一步 $t=1$：\n$$\n\\nabla f(w_{0})=\\begin{pmatrix}2\\cdot 4\\\\ [2pt]4\\cdot 2\\end{pmatrix}\n=\\begin{pmatrix}8\\\\ [2pt]8\\end{pmatrix},\\qquad\nv_{1}=\\frac{9}{10}\\begin{pmatrix}0\\\\ [2pt]0\\end{pmatrix}+\\frac{1}{10}\\begin{pmatrix}8\\\\ [2pt]8\\end{pmatrix}\n=\\begin{pmatrix}\\frac{4}{5}\\\\ [2pt]\\frac{4}{5}\\end{pmatrix},\n$$\n$$\nw_{1}=w_{0}-v_{1}=\\begin{pmatrix}4\\\\ [2pt]2\\end{pmatrix}-\\begin{pmatrix}\\frac{4}{5}\\\\ [2pt]\\frac{4}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{16}{5}\\\\ [2pt]\\frac{6}{5}\\end{pmatrix}.\n$$\n\n第二步 $t=2$：\n$$\n\\nabla f(w_{1})=\\begin{pmatrix}2\\cdot \\frac{16}{5}\\\\ [2pt]4\\cdot \\frac{6}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{32}{5}\\\\ [2pt]\\frac{24}{5}\\end{pmatrix},\n$$\n$$\nv_{2}=\\frac{9}{10}\\begin{pmatrix}\\frac{4}{5}\\\\ [2pt]\\frac{4}{5}\\end{pmatrix}+\\frac{1}{10}\\begin{pmatrix}\\frac{32}{5}\\\\ [2pt]\\frac{24}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{36}{50}\\\\ [2pt]\\frac{36}{50}\\end{pmatrix}+\\begin{pmatrix}\\frac{32}{50}\\\\ [2pt]\\frac{24}{50}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{68}{50}\\\\ [2pt]\\frac{60}{50}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{34}{25}\\\\ [2pt]\\frac{6}{5}\\end{pmatrix},\n$$\n$$\nw_{2}=w_{1}-v_{2}=\\begin{pmatrix}\\frac{16}{5}\\\\ [2pt]\\frac{6}{5}\\end{pmatrix}-\\begin{pmatrix}\\frac{34}{25}\\\\ [2pt]\\frac{6}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{80}{25}-\\frac{34}{25}\\\\ [2pt]0\\end{pmatrix}\n=\\begin{pmatrix}\\frac{46}{25}\\\\ [2pt]0\\end{pmatrix}.\n$$\n因此，经过两次完整更新后，参数向量是行向量 $\\begin{pmatrix}\\frac{46}{25} & 0\\end{pmatrix}$。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{46}{25} & 0 \\end{pmatrix}}$$"
        },
        {
            "introduction": "在经典动量的基础上，Nesterov 加速梯度（NAG）引入了一个微妙但强大的改变：它在一个“前瞻”点计算梯度。这个练习从计算转向概念理解，要求你分析一段代码，并根据其结构识别算法。这种实践可以提升你将算法的数学公式与其具体实现联系起来的能力。",
            "id": "2187801",
            "problem": "在机器学习领域，迭代优化算法通过更新参数向量 $w$ 来最小化损失函数 $L(w)$。两种流行的基于动量的方法是经典动量法（Classical Momentum）和 Nesterov 加速梯度（NAG）。\n\n动量的核心思想是加速在持续下降方向上的移动，并抑制振荡。这是通过在更新规则中加入一个“速度”项 $v$ 来实现的。更新依赖于学习率 $\\eta$ 和动量系数 $\\gamma$。\n\n这两种方法之间的关键区别在于评估损失函数梯度的*位置*。\n- **经典动量法（Classical Momentum）**：在当前参数位置 $w_t$ 处计算梯度。\n- **Nesterov 加速梯度（NAG）**：首先，它沿着先前速度的方向进行一次“前瞻”步骤。然后，它在这个前瞻位置计算梯度，以进行更明智的修正。\n\n考虑以下用于优化算法单个更新步骤的伪代码。函数 `compute_gradient_at(point)` 计算损失函数 $L$ 在给定 `point` 处的梯度。\n\n```\nfunction update_step(w, v, eta, gamma):\n    // w: current parameter vector\n    // v: current velocity vector\n    // eta: learning rate\n    // gamma: momentum coefficient\n\n    // Step 1: Calculate a temporary, projected position\n    w_projected = w - gamma * v\n\n    // Step 2: Compute the gradient at the projected position\n    grad = compute_gradient_at(w_projected)\n\n    // Step 3: Update the velocity vector\n    v_new = gamma * v + eta * grad\n\n    // Step 4: Update the parameter vector\n    w_new = w - v_new\n\n    // Return the updated parameter and velocity vectors\n    return w_new, v_new\n```\n\n该伪代码实现了哪种优化算法？\n\nA. 简单动量梯度下降法（经典动量法）\n\nB. Nesterov 加速梯度（NAG）\n\nC. 简单梯度下降法\n\nD. Adagrad\n\nE. RMSprop",
            "solution": "我们得到了一个更新方案，其参数为 $w$ 和速度 $v$，学习率为 $\\eta$，动量系数为 $\\gamma$。该伪代码执行以下步骤：\n1) 投影位置：\n$$w_{\\text{proj}}=w-\\gamma v.$$\n2) 在投影位置的梯度：\n$$g=\\nabla L(w_{\\text{proj}})=\\nabla L(w-\\gamma v).$$\n3) 速度更新：\n$$v_{\\text{new}}=\\gamma v+\\eta g.$$\n4) 参数更新：\n$$w_{\\text{new}}=w-v_{\\text{new}}=w-(\\gamma v+\\eta g).$$\n\n经典动量法使用\n$$v_{t+1}=\\gamma v_{t}+\\eta \\nabla L(w_{t}),\\qquad w_{t+1}=w_{t}-v_{t+1},$$\n也就是说，梯度是在当前点 $w_{t}$ 处评估的。\n\nNesterov 加速梯度首先计算一个前瞻点\n$$\\tilde{w}_{t}=w_{t}-\\gamma v_{t},$$\n然后在此处评估梯度，\n$$g_{t}=\\nabla L(\\tilde{w}_{t})=\\nabla L(w_{t}-\\gamma v_{t}),$$\n并更新\n$$v_{t+1}=\\gamma v_{t}+\\eta g_{t},\\qquad w_{t+1}=w_{t}-v_{t+1}.$$\n\n将这些公式与伪代码进行比较，我们发现它与 Nesterov 的过程完全一致：梯度是在前瞻位置 $w-\\gamma v$ 计算的，然后应用标准的动量式速度和参数更新。因此，该伪代码实现的算法是 Nesterov 加速梯度。\n\n它不是简单梯度下降法，因为存在一个速度项；它不是经典动量法，因为梯度不是在 $w$ 处评估的；它也不是 Adagrad 或 RMSprop，因为没有使用累积平方梯度进行逐参数的自适应缩放。",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "为什么动量法在复杂的优化环境中如此有效？这个问题探讨了一个关键优势：穿越鞍点的能力——在鞍点处梯度为零，但它并非最小值点。通过在一个初始“推动”下从一个驻点开始，你将看到动量项如何使算法逃离标准梯度下降会停滞的区域。",
            "id": "2187802",
            "problem": "对一个由 $f(x,y) = \\frac{1}{2}(\\alpha x^2 - \\gamma y^2)$ 给出的二维二次函数执行优化过程，其中 $\\alpha$ 和 $\\gamma$ 是正常数。所使用的优化算法是重球法，它是经典动量法的一种变体。第 $k$ 步的位置向量记为 $\\mathbf{p}_k = (x_k, y_k)^T$，它根据前两步的位置 $\\mathbf{p}_{k-1}$ 和 $\\mathbf{p}_{k-2}$，以及以下对 $k \\geq 0$ 成立的迭代公式进行更新：\n$$\n\\mathbf{p}_{k+1} = \\mathbf{p}_k - \\eta \\nabla f(\\mathbf{p}_k) + \\beta (\\mathbf{p}_k - \\mathbf{p}_{k-1})\n$$\n其中 $\\eta$ 是学习率，$\\beta$ 是动量参数，两者均为正常数。\n\n该过程在驻点 $\\mathbf{p}_0 = (0, 0)^T$ 处初始化。为了计算第一步，通过将虚拟步 $k=-1$ 处的点定义为 $\\mathbf{p}_{-1} = (-\\delta_x, -\\delta_y)^T$ 来提供一个非零的历史记录，其中 $\\delta_x$ 和 $\\delta_y$ 是代表初始扰动的小正常数。\n\n你的任务是确定经过两次完整更新步骤后的位置向量 $\\mathbf{p}_2 = (x_2, y_2)^T$。请将你的答案表示为行向量 $(x_2, y_2)$，并用 $\\alpha, \\gamma, \\eta, \\beta, \\delta_x,$ 和 $\\delta_y$ 来表达。",
            "solution": "给定二次函数 $f(x,y) = \\frac{1}{2}(\\alpha x^{2} - \\gamma y^{2})$，其中 $\\alpha>0$ 且 $\\gamma>0$。其梯度通过对每个坐标求偏导数计算得出：\n$$\n\\frac{\\partial f}{\\partial x} = \\alpha x, \\quad \\frac{\\partial f}{\\partial y} = -\\gamma y,\n$$\n所以\n$$\n\\nabla f(x,y) = \\begin{pmatrix} \\alpha x \\\\ -\\gamma y \\end{pmatrix}.\n$$\n重球法的更新公式为\n$$\n\\mathbf{p}_{k+1} = \\mathbf{p}_{k} - \\eta \\nabla f(\\mathbf{p}_{k}) + \\beta(\\mathbf{p}_{k} - \\mathbf{p}_{k-1}),\n$$\n初始条件为 $\\mathbf{p}_{0} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ 和 $\\mathbf{p}_{-1} = \\begin{pmatrix} -\\delta_{x} \\\\ -\\delta_{y} \\end{pmatrix}$。\n\n第一步 ($k=0$)：由于 $\\nabla f(\\mathbf{p}_{0}) = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$，\n$$\n\\mathbf{p}_{1} = \\mathbf{p}_{0} - \\eta \\nabla f(\\mathbf{p}_{0}) + \\beta(\\mathbf{p}_{0} - \\mathbf{p}_{-1})\n= \\beta\\left(\\mathbf{p}_{0} - \\mathbf{p}_{-1}\\right)\n= \\beta \\begin{pmatrix} \\delta_{x} \\\\ \\delta_{y} \\end{pmatrix},\n$$\n所以 $x_{1} = \\beta \\delta_{x}$ 且 $y_{1} = \\beta \\delta_{y}$。\n\n第二步 ($k=1$)：计算在 $\\mathbf{p}_{1}$ 处的梯度：\n$$\n\\nabla f(\\mathbf{p}_{1}) = \\begin{pmatrix} \\alpha x_{1} \\\\ -\\gamma y_{1} \\end{pmatrix}\n= \\begin{pmatrix} \\alpha \\beta \\delta_{x} \\\\ -\\gamma \\beta \\delta_{y} \\end{pmatrix}.\n$$\n使用 $\\mathbf{p}_{1} - \\mathbf{p}_{0} = \\mathbf{p}_{1}$，更新可得\n$$\n\\mathbf{p}_{2} = \\mathbf{p}_{1} - \\eta \\nabla f(\\mathbf{p}_{1}) + \\beta(\\mathbf{p}_{1} - \\mathbf{p}_{0})\n= (1+\\beta)\\mathbf{p}_{1} - \\eta \\nabla f(\\mathbf{p}_{1}).\n$$\n按分量计算，\n$$\nx_{2} = (1+\\beta)x_{1} - \\eta \\alpha x_{1} = (1+\\beta - \\eta \\alpha)\\,x_{1} = \\beta(1+\\beta - \\eta \\alpha)\\,\\delta_{x},\n$$\n$$\ny_{2} = (1+\\beta)y_{1} - \\eta(-\\gamma y_{1}) = (1+\\beta + \\eta \\gamma)\\,y_{1} = \\beta(1+\\beta + \\eta \\gamma)\\,\\delta_{y}.\n$$\n因此，\n$$\n(x_{2}, y_{2}) = \\left(\\beta(1+\\beta - \\eta \\alpha)\\delta_{x},\\; \\beta(1+\\beta + \\eta \\gamma)\\delta_{y}\\right).\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\beta\\left(1+\\beta-\\eta \\alpha\\right)\\delta_{x} & \\beta\\left(1+\\beta+\\eta \\gamma\\right)\\delta_{y}\\end{pmatrix}}$$"
        }
    ]
}