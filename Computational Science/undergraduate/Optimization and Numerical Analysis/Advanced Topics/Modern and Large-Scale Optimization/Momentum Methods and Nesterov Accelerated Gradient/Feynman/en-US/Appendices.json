{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp how momentum accelerates optimization, we must start with the fundamentals. This first exercise will guide you through the step-by-step application of the classical momentum method on a simple quadratic function. By manually computing the first few iterations, you will build a concrete understanding of how the velocity term $v_t$ accumulates past gradients and influences the parameter update $w_t$, setting the stage for more complex scenarios .",
            "id": "2187765",
            "problem": "An optimization algorithm is used to minimize a cost function $f(x, y)$, which depends on two parameters $x$ and $y$. The cost function is given by:\n$$f(x, y) = x^2 + 2y^2$$\n\nThe algorithm of choice is the classical momentum method. The update rules for the parameter vector $w_t = (x_t, y_t)$ and the velocity vector $v_t$ at step $t$ are given by:\n1. $v_t = \\gamma v_{t-1} + \\eta \\nabla f(w_{t-1})$\n2. $w_t = w_{t-1} - v_t$\n\nwhere $\\nabla f(w_{t-1})$ is the gradient of the cost function evaluated at the previous parameter vector $w_{t-1}$.\n\nThe optimization starts at the initial parameter vector $w_0 = (x_0, y_0) = (4, 2)$. The initial velocity vector is $v_0 = (0, 0)$. The algorithm's hyperparameters are set to a learning rate $\\eta = 0.1$ and a momentum parameter $\\gamma = 0.9$.\n\nYour task is to compute the parameter vector $w_2 = (x_2, y_2)$ after two full update steps. Express your answer as a row vector with two components.",
            "solution": "The cost function is $f(x,y)=x^{2}+2y^{2}$, so its gradient is\n$$\n\\nabla f(x,y)=\\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix}\n=\\begin{pmatrix} 2x \\\\ 4y \\end{pmatrix}.\n$$\nThe momentum updates are $v_{t}=\\gamma v_{t-1}+\\eta\\,\\nabla f(w_{t-1})$ and $w_{t}=w_{t-1}-v_{t}$ with $w_{0}=\\begin{pmatrix}4\\\\2\\end{pmatrix}$, $v_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$, $\\eta=\\frac{1}{10}$, and $\\gamma=\\frac{9}{10}$.\n\nFirst step $t=1$:\n$$\n\\nabla f(w_{0})=\\begin{pmatrix}2\\cdot 4\\\\4\\cdot 2\\end{pmatrix}\n=\\begin{pmatrix}8\\\\8\\end{pmatrix},\\qquad\nv_{1}=\\frac{9}{10}\\begin{pmatrix}0\\\\0\\end{pmatrix}+\\frac{1}{10}\\begin{pmatrix}8\\\\8\\end{pmatrix}\n=\\begin{pmatrix}\\frac{4}{5}\\\\\\frac{4}{5}\\end{pmatrix},\n$$\n$$\nw_{1}=w_{0}-v_{1}=\\begin{pmatrix}4\\\\2\\end{pmatrix}-\\begin{pmatrix}\\frac{4}{5}\\\\\\frac{4}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{16}{5}\\\\\\frac{6}{5}\\end{pmatrix}.\n$$\n\nSecond step $t=2$:\n$$\n\\nabla f(w_{1})=\\begin{pmatrix}2\\cdot \\frac{16}{5}\\\\4\\cdot \\frac{6}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{32}{5}\\\\\\frac{24}{5}\\end{pmatrix},\n$$\n$$\nv_{2}=\\frac{9}{10}\\begin{pmatrix}\\frac{4}{5}\\\\\\frac{4}{5}\\end{pmatrix}+\\frac{1}{10}\\begin{pmatrix}\\frac{32}{5}\\\\\\frac{24}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{36}{50}\\\\\\frac{36}{50}\\end{pmatrix}+\\begin{pmatrix}\\frac{32}{50}\\\\\\frac{24}{50}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{68}{50}\\\\\\frac{60}{50}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{34}{25}\\\\\\frac{6}{5}\\end{pmatrix},\n$$\n$$\nw_{2}=w_{1}-v_{2}=\\begin{pmatrix}\\frac{16}{5}\\\\\\frac{6}{5}\\end{pmatrix}-\\begin{pmatrix}\\frac{34}{25}\\\\\\frac{6}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{80}{25}-\\frac{34}{25}\\\\0\\end{pmatrix}\n=\\begin{pmatrix}\\frac{46}{25}\\\\0\\end{pmatrix}.\n$$\nThus, after two full updates, the parameter vector is the row vector $\\begin{pmatrix}\\frac{46}{25}  0\\end{pmatrix}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{46}{25}  0 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Nesterov Accelerated Gradient (NAG) refines the momentum concept by introducing a \"smarter\" update. Instead of calculating the gradient at the current position, NAG \"looks ahead\" in the direction of momentum before making a correction. This practice hones in on this crucial difference by having you calculate the precise look-ahead point, providing insight into why NAG can often navigate complex loss landscapes more efficiently than the classical method .",
            "id": "2187811",
            "problem": "An engineer is applying the Nesterov Accelerated Gradient (NAG) algorithm to minimize a one-dimensional objective function. The function is given by $f(x) = 2x^2$. The algorithm iteratively updates a position parameter $x$ and a velocity term $v$ according to the following set of rules, starting from an initial position $x_0$ and an initial velocity $v_0=0$:\n$$ v_{t} = \\gamma v_{t-1} + \\eta \\nabla f(x_{t-1} - \\gamma v_{t-1}) $$\n$$ x_{t} = x_{t-1} - v_{t} $$\nIn these equations, a subscript denotes the iteration number, so $t=1, 2, 3, \\ldots$. The constant $\\gamma$ is the momentum parameter, $\\eta$ is the learning rate, and $\\nabla f$ is the gradient of the function $f(x)$. The evaluation of the gradient occurs at the \"look-ahead\" point, given by the term $x_{t-1} - \\gamma v_{t-1}$.\n\nFor an initial position of $x_0 = 10$, a momentum parameter of $\\gamma = 0.9$, and a learning rate of $\\eta = 0.1$, calculate the numerical value of the look-ahead point that is used during the second iteration of the algorithm (i.e., for $t=2$).",
            "solution": "We are given the Nesterov Accelerated Gradient updates:\n$$v_{t}=\\gamma v_{t-1}+\\eta \\nabla f\\!\\left(x_{t-1}-\\gamma v_{t-1}\\right), \\quad x_{t}=x_{t-1}-v_{t}.$$\nThe objective is $f(x)=2x^{2}$, so its gradient is\n$$\\nabla f(x)=\\frac{d}{dx}(2x^{2})=4x.$$\n\nGiven $x_{0}=10$, $v_{0}=0$, $\\gamma=0.9$, and $\\eta=0.1$, first compute the look-ahead point for $t=1$:\n$$x_{0}-\\gamma v_{0}=10-0.9\\cdot 0=10.$$\nThen update the velocity at $t=1$:\n$$v_{1}=\\gamma v_{0}+\\eta \\nabla f(10)=0.9\\cdot 0+0.1\\cdot 4\\cdot 10=4.$$\nUpdate the position:\n$$x_{1}=x_{0}-v_{1}=10-4=6.$$\n\nFor the second iteration ($t=2$), the look-ahead point is\n$$x_{1}-\\gamma v_{1}=6-0.9\\cdot 4=6-3.6=2.4.$$\nTherefore, the numerical value of the look-ahead point used during the second iteration is $2.4$.",
            "answer": "$$\\boxed{2.4}$$"
        },
        {
            "introduction": "Translating mathematical theory into functional code is a vital skill for any practitioner in machine learning or numerical optimization. This final practice shifts our focus from calculation to conceptual analysis, presenting you with a block of pseudocode. Your task is to analyze its structure to determine which momentum algorithm it implements, solidifying your understanding of the subtle but critical difference in their algorithmic flow .",
            "id": "2187801",
            "problem": "In the field of machine learning, iterative optimization algorithms are used to minimize a loss function $L(w)$ by updating a parameter vector $w$. Two popular momentum-based methods are Classical Momentum and Nesterov Accelerated Gradient (NAG).\n\nThe core idea of momentum is to accelerate movement in persistent directions of descent and to dampen oscillations. This is achieved by adding a \"velocity\" term, $v$, to the update rule. The update depends on a learning rate, $\\eta$, and a momentum coefficient, $\\gamma$.\n\nThe key difference between the two methods lies in *where* the gradient of the loss function is evaluated.\n- **Classical Momentum**: Computes the gradient at the current parameter position, $w_t$.\n- **Nesterov Accelerated Gradient (NAG)**: First, it makes a \"look-ahead\" step in the direction of the previous velocity. It then computes the gradient at this look-ahead position to make a more informed correction.\n\nConsider the following pseudocode for a single update step of an optimization algorithm. The function `compute_gradient_at(point)` calculates the gradient of the loss function $L$ at the given `point`.\n\n```\nfunction update_step(w, v, eta, gamma):\n    // w: current parameter vector\n    // v: current velocity vector\n    // eta: learning rate\n    // gamma: momentum coefficient\n\n    // Step 1: Calculate a temporary, projected position\n    w_projected = w - gamma * v\n\n    // Step 2: Compute the gradient at the projected position\n    grad = compute_gradient_at(w_projected)\n\n    // Step 3: Update the velocity vector\n    v_new = gamma * v + eta * grad\n\n    // Step 4: Update the parameter vector\n    w_new = w - v_new\n\n    // Return the updated parameter and velocity vectors\n    return w_new, v_new\n```\n\nWhich optimization algorithm does this pseudocode implement?\n\nA. Simple Gradient Descent with Momentum (Classical Momentum)\n\nB. Nesterov Accelerated Gradient (NAG)\n\nC. Simple Gradient Descent\n\nD. Adagrad\n\nE. RMSprop",
            "solution": "We are given an update scheme with parameters $w$ and velocity $v$, learning rate $\\eta$, and momentum coefficient $\\gamma$. The pseudocode performs the following steps:\n1) Projected position:\n$$w_{\\text{proj}}=w-\\gamma v.$$\n2) Gradient at the projected position:\n$$g=\\nabla L(w_{\\text{proj}})=\\nabla L(w-\\gamma v).$$\n3) Velocity update:\n$$v_{\\text{new}}=\\gamma v+\\eta g.$$\n4) Parameter update:\n$$w_{\\text{new}}=w-v_{\\text{new}}=w-(\\gamma v+\\eta g).$$\n\nClassical Momentum uses\n$$v_{t+1}=\\gamma v_{t}+\\eta \\nabla L(w_{t}),\\qquad w_{t+1}=w_{t}-v_{t+1},$$\nthat is, the gradient is evaluated at the current point $w_{t}$.\n\nNesterov Accelerated Gradient first computes a look-ahead point\n$$\\tilde{w}_{t}=w_{t}-\\gamma v_{t},$$\nthen evaluates the gradient there,\n$$g_{t}=\\nabla L(\\tilde{w}_{t})=\\nabla L(w_{t}-\\gamma v_{t}),$$\nand updates\n$$v_{t+1}=\\gamma v_{t}+\\eta g_{t},\\qquad w_{t+1}=w_{t}-v_{t+1}.$$\n\nComparing these formulas with the pseudocode, we see exact agreement with the Nesterov procedure: the gradient is computed at the look-ahead position $w-\\gamma v$, then the standard momentum-style velocity and parameter updates are applied. Therefore, the algorithm implemented by the pseudocode is Nesterov Accelerated Gradient.\n\nIt is not Simple Gradient Descent because there is a velocity term; it is not Classical Momentum because the gradient is not evaluated at $w$; and it is neither Adagrad nor RMSprop because there is no per-parameter adaptive scaling using accumulated squared gradients.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}