{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp how momentum methods accelerate optimization, it's best to start with a concrete calculation. This first exercise  will guide you through the fundamental mechanics of the classical momentum algorithm. By manually computing the first few update steps for a simple quadratic function, you will build a solid intuition for how the velocity term accumulates past gradient information to steer the optimization path more effectively than simple gradient descent.",
            "id": "2187765",
            "problem": "An optimization algorithm is used to minimize a cost function $f(x, y)$, which depends on two parameters $x$ and $y$. The cost function is given by:\n$$f(x, y) = x^2 + 2y^2$$\n\nThe algorithm of choice is the classical momentum method. The update rules for the parameter vector $w_t = (x_t, y_t)$ and the velocity vector $v_t$ at step $t$ are given by:\n1. $v_t = \\gamma v_{t-1} + \\eta \\nabla f(w_{t-1})$\n2. $w_t = w_{t-1} - v_t$\n\nwhere $\\nabla f(w_{t-1})$ is the gradient of the cost function evaluated at the previous parameter vector $w_{t-1}$.\n\nThe optimization starts at the initial parameter vector $w_0 = (x_0, y_0) = (4, 2)$. The initial velocity vector is $v_0 = (0, 0)$. The algorithm's hyperparameters are set to a learning rate $\\eta = 0.1$ and a momentum parameter $\\gamma = 0.9$.\n\nYour task is to compute the parameter vector $w_2 = (x_2, y_2)$ after two full update steps. Express your answer as a row vector with two components.",
            "solution": "The cost function is $f(x,y)=x^{2}+2y^{2}$, so its gradient is\n$$\n\\nabla f(x,y)=\\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix}\n=\\begin{pmatrix} 2x \\\\ 4y \\end{pmatrix}.\n$$\nThe momentum updates are $v_{t}=\\gamma v_{t-1}+\\eta\\,\\nabla f(w_{t-1})$ and $w_{t}=w_{t-1}-v_{t}$ with $w_{0}=\\begin{pmatrix}4\\\\2\\end{pmatrix}$, $v_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$, $\\eta=\\frac{1}{10}$, and $\\gamma=\\frac{9}{10}$.\n\nFirst step $t=1$:\n$$\n\\nabla f(w_{0})=\\begin{pmatrix}2\\cdot 4\\\\4\\cdot 2\\end{pmatrix}\n=\\begin{pmatrix}8\\\\8\\end{pmatrix},\\qquad\nv_{1}=\\frac{9}{10}\\begin{pmatrix}0\\\\0\\end{pmatrix}+\\frac{1}{10}\\begin{pmatrix}8\\\\8\\end{pmatrix}\n=\\begin{pmatrix}\\frac{4}{5}\\\\\\frac{4}{5}\\end{pmatrix},\n$$\n$$\nw_{1}=w_{0}-v_{1}=\\begin{pmatrix}4\\\\2\\end{pmatrix}-\\begin{pmatrix}\\frac{4}{5}\\\\\\frac{4}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{16}{5}\\\\\\frac{6}{5}\\end{pmatrix}.\n$$\n\nSecond step $t=2$:\n$$\n\\nabla f(w_{1})=\\begin{pmatrix}2\\cdot \\frac{16}{5}\\\\4\\cdot \\frac{6}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{32}{5}\\\\\\frac{24}{5}\\end{pmatrix},\n$$\n$$\nv_{2}=\\frac{9}{10}\\begin{pmatrix}\\frac{4}{5}\\\\\\frac{4}{5}\\end{pmatrix}+\\frac{1}{10}\\begin{pmatrix}\\frac{32}{5}\\\\\\frac{24}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{36}{50}\\\\\\frac{36}{50}\\end{pmatrix}+\\begin{pmatrix}\\frac{32}{50}\\\\\\frac{24}{50}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{68}{50}\\\\\\frac{60}{50}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{34}{25}\\\\\\frac{6}{5}\\end{pmatrix},\n$$\n$$\nw_{2}=w_{1}-v_{2}=\\begin{pmatrix}\\frac{16}{5}\\\\\\frac{6}{5}\\end{pmatrix}-\\begin{pmatrix}\\frac{34}{25}\\\\\\frac{6}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{80}{25}-\\frac{34}{25}\\\\0\\end{pmatrix}\n=\\begin{pmatrix}\\frac{46}{25}\\\\0\\end{pmatrix}.\n$$\nThus, after two full updates, the parameter vector is the row vector $\\begin{pmatrix}\\frac{46}{25}  0\\end{pmatrix}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{46}{25}  0 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Building upon classical momentum, the Nesterov Accelerated Gradient (NAG) introduces a clever modification that often leads to faster convergence. The key insight is to calculate the gradient not at the current position, but at a \"look-ahead\" point projected in the direction of the current velocity. This practice  focuses directly on this core concept, asking you to calculate the precise location of this look-ahead point, thereby revealing the \"smarter\" correction that NAG applies at each step.",
            "id": "2187811",
            "problem": "An engineer is applying the Nesterov Accelerated Gradient (NAG) algorithm to minimize a one-dimensional objective function. The function is given by $f(x) = 2x^2$. The algorithm iteratively updates a position parameter $x$ and a velocity term $v$ according to the following set of rules, starting from an initial position $x_0$ and an initial velocity $v_0=0$:\n$$ v_{t} = \\gamma v_{t-1} + \\eta \\nabla f(x_{t-1} - \\gamma v_{t-1}) $$\n$$ x_{t} = x_{t-1} - v_{t} $$\nIn these equations, a subscript denotes the iteration number, so $t=1, 2, 3, \\ldots$. The constant $\\gamma$ is the momentum parameter, $\\eta$ is the learning rate, and $\\nabla f$ is the gradient of the function $f(x)$. The evaluation of the gradient occurs at the \"look-ahead\" point, given by the term $x_{t-1} - \\gamma v_{t-1}$.\n\nFor an initial position of $x_0 = 10$, a momentum parameter of $\\gamma = 0.9$, and a learning rate of $\\eta = 0.1$, calculate the numerical value of the look-ahead point that is used during the second iteration of the algorithm (i.e., for $t=2$).",
            "solution": "We are given the Nesterov Accelerated Gradient updates:\n$$v_{t}=\\gamma v_{t-1}+\\eta \\nabla f\\!\\left(x_{t-1}-\\gamma v_{t-1}\\right), \\quad x_{t}=x_{t-1}-v_{t}.$$\nThe objective is $f(x)=2x^{2}$, so its gradient is\n$$\\nabla f(x)=\\frac{d}{dx}(2x^{2})=4x.$$\n\nGiven $x_{0}=10$, $v_{0}=0$, $\\gamma=0.9$, and $\\eta=0.1$, first compute the look-ahead point for $t=1$:\n$$x_{0}-\\gamma v_{0}=10-0.9\\cdot 0=10.$$\nThen update the velocity at $t=1$:\n$$v_{1}=\\gamma v_{0}+\\eta \\nabla f(10)=0.9\\cdot 0+0.1\\cdot 4\\cdot 10=4.$$\nUpdate the position:\n$$x_{1}=x_{0}-v_{1}=10-4=6.$$\n\nFor the second iteration ($t=2$), the look-ahead point is\n$$x_{1}-\\gamma v_{1}=6-0.9\\cdot 4=6-3.6=2.4.$$\nTherefore, the numerical value of the look-ahead point used during the second iteration is $2.4$.",
            "answer": "$$\\boxed{2.4}$$"
        },
        {
            "introduction": "While momentum methods can significantly speed up convergence, they are not a magic bullet. The choice of hyperparameters, such as the learning rate and momentum coefficient, is critical and can dramatically affect performance and stability. This exercise  presents a powerful, albeit hypothetical, case study where an improperly tuned momentum method diverges, while standard gradient descent still manages to converge. Working through this problem provides a crucial hands-on lesson on the potential for instability and the importance of careful hyperparameter selection.",
            "id": "2187798",
            "problem": "In the field of numerical optimization, different algorithms are used to find the minimum of a function. Consider the simple one-dimensional convex objective function $f(x) = \\frac{1}{2} C x^2$, where $C=1.0$. An analyst is comparing the behavior of two iterative optimization algorithms, starting from the initial position $x_0 = 10.0$.\n\nAlgorithm A is the Standard Gradient Descent (GD) method. The position is updated at each step $k$ according to the rule:\n$$x_{k+1} = x_k - \\eta_{A} \\nabla f(x_k)$$\nThe analyst uses a learning rate of $\\eta_A = 1.5$.\n\nAlgorithm B is the Momentum method. This method introduces a \"velocity\" term $v$ that accumulates past gradients. Starting with an initial velocity $v_0 = 0$, the updates are given by:\n$$v_{k+1} = \\beta v_k + \\eta_{B} \\nabla f(x_k)$$\n$$x_{k+1} = x_k - v_{k+1}$$\nFor this algorithm, the analyst uses a learning rate of $\\eta_B = 0.8$ and a momentum parameter of $\\beta = 0.8$.\n\nLet $x_5^{(A)}$ be the position of the iterate after 5 steps using Algorithm A, and $x_5^{(B)}$ be the position after 5 steps using Algorithm B. Calculate the ratio $R = x_5^{(B)} / x_5^{(A)}$. Report your final answer for $R$ rounded to three significant figures.",
            "solution": "The problem asks for the ratio of the final positions, $R = x_5^{(B)} / x_5^{(A)}$, after 5 iterations of two different optimization algorithms on the function $f(x) = \\frac{1}{2} C x^2$ with $C=1.0$. The gradient of the function is $\\nabla f(x) = \\frac{d}{dx}f(x) = C x = x$. The initial position is $x_0 = 10.0$ for both algorithms.\n\n**Part 1: Calculation for Algorithm A (Standard Gradient Descent)**\n\nAlgorithm A follows the update rule $x_{k+1} = x_k - \\eta_{A} \\nabla f(x_k)$.\nWith $\\nabla f(x_k) = x_k$ and $\\eta_A = 1.5$, the rule becomes:\n$x_{k+1} = x_k - 1.5 x_k = (1 - 1.5) x_k = -0.5 x_k$.\n\nWe start with $x_0 = 10.0$ and iterate 5 times:\n- Step 1: $x_1 = -0.5 \\times x_0 = -0.5 \\times 10.0 = -5.0$.\n- Step 2: $x_2 = -0.5 \\times x_1 = -0.5 \\times (-5.0) = 2.5$.\n- Step 3: $x_3 = -0.5 \\times x_2 = -0.5 \\times 2.5 = -1.25$.\n- Step 4: $x_4 = -0.5 \\times x_3 = -0.5 \\times (-1.25) = 0.625$.\n- Step 5: $x_5^{(A)} = -0.5 \\times x_4 = -0.5 \\times 0.625 = -0.3125$.\n\nThis algorithm appears to be converging (oscillating, but the magnitude decreases).\n\n**Part 2: Calculation for Algorithm B (Momentum)**\n\nAlgorithm B follows the update rules:\n$v_{k+1} = \\beta v_k + \\eta_{B} \\nabla f(x_k)$\n$x_{k+1} = x_k - v_{k+1}$\nWith $\\nabla f(x_k) = x_k$, $\\eta_B = 0.8$, and $\\beta = 0.8$, the rules are:\n$v_{k+1} = 0.8 v_k + 0.8 x_k$\n$x_{k+1} = x_k - v_{k+1}$\n\nWe start with $x_0 = 10.0$ and $v_0 = 0$ and iterate 5 times:\n\n- **Step 1 (k=0 to k=1):**\n  $v_1 = 0.8 v_0 + 0.8 x_0 = 0.8(0) + 0.8(10.0) = 8.0$.\n  $x_1 = x_0 - v_1 = 10.0 - 8.0 = 2.0$.\n\n- **Step 2 (k=1 to k=2):**\n  $v_2 = 0.8 v_1 + 0.8 x_1 = 0.8(8.0) + 0.8(2.0) = 6.4 + 1.6 = 8.0$.\n  $x_2 = x_1 - v_2 = 2.0 - 8.0 = -6.0$.\n\n- **Step 3 (k=2 to k=3):**\n  $v_3 = 0.8 v_2 + 0.8 x_2 = 0.8(8.0) + 0.8(-6.0) = 6.4 - 4.8 = 1.6$.\n  $x_3 = x_2 - v_3 = -6.0 - 1.6 = -7.6$.\n\n- **Step 4 (k=3 to k=4):**\n  $v_4 = 0.8 v_3 + 0.8 x_3 = 0.8(1.6) + 0.8(-7.6) = 1.28 - 6.08 = -4.8$.\n  $x_4 = x_3 - v_4 = -7.6 - (-4.8) = -2.8$.\n\n- **Step 5 (k=4 to k=5):**\n  $v_5 = 0.8 v_4 + 0.8 x_4 = 0.8(-4.8) + 0.8(-2.8) = -3.84 - 2.24 = -6.08$.\n  $x_5^{(B)} = x_4 - v_5 = -2.8 - (-6.08) = 3.28$.\n\nDespite the chosen parameters, which might seem reasonable, this algorithm is exhibiting signs of divergence as the magnitude of the position begins to increase after an initial decrease ($|x_0|=10, |x_1|=2, |x_2|=6, |x_3|=7.6, |x_4|=2.8, |x_5|=3.28$).\n\n**Part 3: Final Calculation**\n\nWe need to compute the ratio $R = x_5^{(B)} / x_5^{(A)}$.\nUsing the calculated values:\n$R = \\frac{3.28}{-0.3125} = -10.496$.\n\nThe problem asks for the answer rounded to three significant figures.\n$R \\approx -10.5$.",
            "answer": "$$\\boxed{-10.5}$$"
        }
    ]
}