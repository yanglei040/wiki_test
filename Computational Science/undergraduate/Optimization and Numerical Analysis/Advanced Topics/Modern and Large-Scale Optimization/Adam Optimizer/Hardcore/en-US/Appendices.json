{
    "hands_on_practices": [
        {
            "introduction": "To master the Adam optimizer, we begin with its foundational components: the first and second moment estimates. These are essentially the optimizer's \"memory,\" tracking the running average of the gradient and its squared value. This first exercise  isolates this core mechanism, allowing you to practice the fundamental update equations for the moment vectors, $m_t$ and $v_t$, which are the building blocks for the entire algorithm.",
            "id": "2152288",
            "problem": "In the field of machine learning, optimizers are algorithms used to adjust the parameters of a model to minimize a loss function. The Adam (Adaptive Moment Estimation) optimizer is a popular choice that computes adaptive learning rates for each parameter from an estimate of the first and second moments of the gradients.\n\nThe update rules for the first moment estimate (a moving average of the gradient, $m$) and the second moment estimate (a moving average of the squared gradient, $v$) at a time step $t$ are given by:\n$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\nwhere $g_t$ is the gradient of the loss function with respect to the model parameters at step $t$, and the square operation $g_t^2$ is performed element-wise. The hyperparameters $\\beta_1$ and $\\beta_2$ are decay rates for the moving averages.\n\nConsider a simple two-parameter model. At the beginning of the optimization process ($t=1$), the initial moment estimates are initialized to zero vectors, i.e., $m_0 = [0, 0]^T$ and $v_0 = [0, 0]^T$. The hyperparameters are set to their commonly used values: $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$. For the first training step, the computed gradient of the loss function is $g_1 = [2.0, -4.0]^T$.\n\nCalculate the updated first moment vector $m_1 = [m_{1,1}, m_{1,2}]^T$ and the updated second moment vector $v_1 = [v_{1,1}, v_{1,2}]^T$. Your final answer should be a row matrix containing the four numerical components in the specific order $[m_{1,1}, m_{1,2}, v_{1,1}, v_{1,2}]$.",
            "solution": "We use the Adam moment update equations at step $t=1$:\n$$m_{1}=\\beta_{1}m_{0}+(1-\\beta_{1})g_{1}, \\quad v_{1}=\\beta_{2}v_{0}+(1-\\beta_{2})g_{1}^{2},$$\nwith $m_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, $v_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, $\\beta_{1}=0.9$, $\\beta_{2}=0.999$, and $g_{1}=\\begin{bmatrix}2.0 \\\\ -4.0\\end{bmatrix}$. The square $g_{1}^{2}$ is taken element-wise.\n\nFirst moment:\nSince $m_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, the term $\\beta_{1}m_{0}$ is $\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$. Thus,\n$$m_{1}=(1-\\beta_{1})g_{1}=0.1\\begin{bmatrix}2.0 \\\\ -4.0\\end{bmatrix}=\\begin{bmatrix}0.2 \\\\ -0.4\\end{bmatrix}.$$\nTherefore, $m_{1,1}=0.2$ and $m_{1,2}=-0.4$.\n\nSecond moment:\nCompute the element-wise square $g_{1}^{2}$:\n$$g_{1}^{2}=\\begin{bmatrix}(2.0)^{2} \\\\ (-4.0)^{2}\\end{bmatrix}=\\begin{bmatrix}4.0 \\\\ 16.0\\end{bmatrix}.$$\nSince $v_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$, the term $\\beta_{2}v_{0}$ is $\\begin{bmatrix}0 \\\\ 0\\endbmatrix}$. Thus,\n$$v_{1}=(1-\\beta_{2})g_{1}^{2}=0.001\\begin{bmatrix}4.0 \\\\ 16.0\\end{bmatrix}=\\begin{bmatrix}0.004 \\\\ 0.016\\end{bmatrix}.$$\nTherefore, $v_{1,1}=0.004$ and $v_{1,2}=0.016$.\n\nThe requested row matrix in the order $[m_{1,1}, m_{1,2}, v_{1,1}, v_{1,2}]$ is $\\begin{pmatrix}0.2  -0.4  0.004  0.016\\end{pmatrix}$.",
            "answer": "$$\\boxed{\\begin{pmatrix}0.2  -0.4  0.004  0.016\\end{pmatrix}}$$"
        },
        {
            "introduction": "After computing the raw moment estimates, the next stage in the Adam algorithm involves correcting for initialization bias and performing the parameter update. This practice  walks you through a complete, single step of the optimization process on a simple quadratic function. By working through the bias correction and the final update rule, you will see how Adam combines its gradient \"memory\" with a learning rate to intelligently navigate the loss landscape.",
            "id": "2152250",
            "problem": "In the field of numerical optimization, the Adam algorithm is a widely used method for finding the minimum of a function. Consider the one-dimensional cost function $f(x) = 5x^2$. We wish to find the value of $x$ that minimizes this function, starting from an initial guess of $x_0 = 2$.\n\nCalculate the value of the parameter after one update step, denoted as $x_1$, using the Adam algorithm. The algorithm is configured with the following hyperparameters:\n- Learning rate, $\\alpha = 0.1$\n- Exponential decay rate for the first moment estimate, $\\beta_1 = 0.9$\n- Exponential decay rate for the second moment estimate, $\\beta_2 = 0.999$\n- A small constant for numerical stability, $\\epsilon = 10^{-8}$\n\nThe initial first and second moment estimates, $m_0$ and $v_0$, are both initialized to zero.\n\nRound your final answer to five significant figures.",
            "solution": "We minimize $f(x)=5x^{2}$ using Adam starting from $x_{0}=2$. The gradient is $\\nabla f(x)=10x$. At the first step ($t=1$), the gradient evaluated at $x_{0}$ is\n$$\ng_{1}=\\nabla f(x_{0})=10x_{0}=10\\cdot 2=20.\n$$\nAdam moment updates are\n$$\nm_{1}=\\beta_{1}m_{0}+(1-\\beta_{1})g_{1}, \\quad v_{1}=\\beta_{2}v_{0}+(1-\\beta_{2})g_{1}^{2}.\n$$\nWith $m_{0}=0$ and $v_{0}=0$, $\\beta_{1}=0.9$, and $\\beta_{2}=0.999$,\n$$\nm_{1}=0.9\\cdot 0+(1-0.9)\\cdot 20=2, \\quad v_{1}=0.999\\cdot 0+(1-0.999)\\cdot 20^{2}=0.001\\cdot 400=0.4.\n$$\nBias-corrected estimates are\n$$\n\\hat{m}_{1}=\\frac{m_{1}}{1-\\beta_{1}^{1}}=\\frac{2}{1-0.9}=\\frac{2}{0.1}=20, \\quad \\hat{v}_{1}=\\frac{v_{1}}{1-\\beta_{2}^{1}}=\\frac{0.4}{1-0.999}=\\frac{0.4}{0.001}=400.\n$$\nThe Adam update is\n$$\nx_{1}=x_{0}-\\alpha\\frac{\\hat{m}_{1}}{\\sqrt{\\hat{v}_{1}}+\\epsilon}=2-0.1\\cdot \\frac{20}{\\sqrt{400}+10^{-8}}=2-0.1\\cdot \\frac{20}{20+10^{-8}}=2-\\frac{2}{20+10^{-8}}.\n$$\nEvaluating the fraction,\n$$\n\\frac{2}{20+10^{-8}} \\approx 0.1-5\\times 10^{-11},\n$$\nso\n$$\nx_{1}\\approx 2-\\left(0.1-5\\times 10^{-11}\\right)=1.90000000005.\n$$\nRounded to five significant figures, this is $1.9000$.",
            "answer": "$$\\boxed{1.9000}$$"
        },
        {
            "introduction": "The true power of an optimizer is revealed not in a single step, but in its dynamic behavior over time. This exercise  presents a fascinating scenario where the optimizer's path crosses a point of non-differentiability, causing an abrupt sign change in the gradient. By analyzing how the momentum and adaptive learning rate components react, you will gain a deeper intuition for why Adam is so robust and how its \"memory\" helps it navigate complex and challenging optimization problems.",
            "id": "2152240",
            "problem": "An optimization algorithm known as Adam (short for Adaptive Moment Estimation) is being used to find the minimum of the one-dimensional objective function $f(x) = |x|$. The gradient of this function for any non-zero $x$ is given by $g(x) = \\text{sign}(x)$.\n\nThe Adam update rule proceeds in discrete time steps $t=1, 2, 3, \\ldots$. At each step, given the current parameter value $x_{t-1}$, the algorithm updates its estimates of the first moment (mean) $m_t$ and the second moment (uncentered variance) $v_t$ of the gradients. It then uses these bias-corrected estimates to update the parameter $x_t$. The full set of update equations are:\n1.  Compute the gradient: $g_t = \\nabla f(x_{t-1})$\n2.  Update first moment estimate: $m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$\n3.  Update second moment estimate: $v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$\n4.  Compute bias-corrected first moment: $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$\n5.  Compute bias-corrected second moment: $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$\n6.  Update parameter: $x_t = x_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$\n\nThe algorithm's hyperparameters are the learning rate $\\alpha$, the exponential decay rates for the moment estimates $\\beta_1$ and $\\beta_2$, and a small stabilization constant $\\epsilon$ to prevent division by zero. All hyperparameters are positive real numbers, with $\\beta_1, \\beta_2 \\in (0, 1)$.\n\nConsider the optimization process starting at time $t=0$ with an initial parameter value $x_0 = \\delta$, where $\\delta$ is a small positive constant. The initial moment estimates are $m_0 = 0$ and $v_0 = 0$. The value of $\\delta$ is chosen to be sufficiently small such that the first update step results in $x_1  0$, causing the iterate to cross the origin.\n\nYour task is to determine the value of the parameter at the next step, $x_2$. Express your answer as a single closed-form analytic expression in terms of $\\delta, \\alpha, \\beta_1, \\beta_2$, and $\\epsilon$.",
            "solution": "We use the Adam updates exactly as stated. For the one-dimensional function $f(x)=|x|$ with $x_{t-1}\\neq 0$, the gradient is $g_{t}=\\text{sign}(x_{t-1})$ and $g_{t}^{2}=1$.\n\nAt $t=1$, with $x_{0}=\\delta0$, we have $g_{1}=\\text{sign}(x_{0})=1$. Using $m_{0}=0$ and $v_{0}=0$,\n$$\nm_{1}=\\beta_{1} m_{0}+(1-\\beta_{1})g_{1}=(1-\\beta_{1}),\n$$\n$$\nv_{1}=\\beta_{2} v_{0}+(1-\\beta_{2}) g_{1}^{2}=(1-\\beta_{2}).\n$$\nBias corrections give\n$$\n\\hat{m}_{1}=\\frac{m_{1}}{1-\\beta_{1}^{1}}=\\frac{1-\\beta_{1}}{1-\\beta_{1}}=1,\\qquad\n\\hat{v}_{1}=\\frac{v_{1}}{1-\\beta_{2}^{1}}=\\frac{1-\\beta_{2}}{1-\\beta_{2}}=1.\n$$\nThus the parameter update is\n$$\nx_{1}=x_{0}-\\alpha \\frac{\\hat{m}_{1}}{\\sqrt{\\hat{v}_{1}}+\\epsilon}\n=\\delta-\\alpha\\frac{1}{1+\\epsilon}.\n$$\nBy assumption, $\\delta$ is small enough that $x_{1}0$, so $g_{2}=\\text{sign}(x_{1})=-1$.\n\nAt $t=2$, compute the moments:\n$$\nm_{2}=\\beta_{1} m_{1}+(1-\\beta_{1}) g_{2}=\\beta_{1}(1-\\beta_{1})-(1-\\beta_{1})\n=(1-\\beta_{1})(\\beta_{1}-1)=-(1-\\beta_{1})^{2},\n$$\n$$\nv_{2}=\\beta_{2} v_{1}+(1-\\beta_{2}) g_{2}^{2}\n=\\beta_{2}(1-\\beta_{2})+(1-\\beta_{2})=(1-\\beta_{2})(1+\\beta_{2})=1-\\beta_{2}^{2}.\n$$\nBias corrections yield\n$$\n\\hat{m}_{2}=\\frac{m_{2}}{1-\\beta_{1}^{2}}\n=\\frac{-(1-\\beta_{1})^{2}}{(1-\\beta_{1})(1+\\beta_{1})}\n=-\\frac{1-\\beta_{1}}{1+\\beta_{1}},\n\\qquad\n\\hat{v}_{2}=\\frac{v_{2}}{1-\\beta_{2}^{2}}=1.\n$$\nTherefore,\n$$\nx_{2}=x_{1}-\\alpha \\frac{\\hat{m}_{2}}{\\sqrt{\\hat{v}_{2}}+\\epsilon}\n=x_{1}-\\alpha \\frac{-\\frac{1-\\beta_{1}}{1+\\beta_{1}}}{1+\\epsilon}\n=x_{1}+\\alpha \\frac{1-\\beta_{1}}{(1+\\beta_{1})(1+\\epsilon)}.\n$$\nSubstituting $x_{1}=\\delta-\\alpha/(1+\\epsilon)$,\n$$\nx_{2}=\\delta-\\frac{\\alpha}{1+\\epsilon}+\\frac{\\alpha(1-\\beta_{1})}{(1+\\beta_{1})(1+\\epsilon)}\n=\\delta+\\frac{\\alpha}{1+\\epsilon}\\left(-1+\\frac{1-\\beta_{1}}{1+\\beta_{1}}\\right)\n=\\delta-\\frac{2\\alpha \\beta_{1}}{(1+\\beta_{1})(1+\\epsilon)}.\n$$\nThis is the required closed-form expression, which notably does not depend on $\\beta_{2}$ up to the second step because $\\hat{v}_{1}=\\hat{v}_{2}=1$ given $g_{1}^{2}=g_{2}^{2}=1$.",
            "answer": "$$\\boxed{\\delta-\\frac{2\\alpha \\beta_{1}}{(1+\\beta_{1})(1+\\epsilon)}}$$"
        }
    ]
}