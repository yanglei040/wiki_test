{
    "hands_on_practices": [
        {
            "introduction": "共轭梯度法 (CG) 的核心在于其迭代过程。这个练习将带你从头到尾完整地走一遍CG方法的第一轮迭代。通过计算梯度、确定初始搜索方向、求解最优步长并更新位置，你将掌握CG算法的基本操作流程。",
            "id": "2211319",
            "problem": "在数值优化中，共轭梯度法是一种用于寻找函数局部最小值的迭代算法。对于二次函数，它可以在有限步内找到精确的最小值。\n\n考虑一个包含两个变量 $x_1$ 和 $x_2$ 的二次目标函数 $f(x)$，其中 $x = [x_1, x_2]^T$：\n$$\nf(x_1, x_2) = (x_1 - 2)^2 + 2(x_2 - 1)^2\n$$\n从初始点 $x_0 = [0, 0]^T$ 开始，执行一次完整的共轭梯度算法迭代。这包括计算初始搜索方向、找到最优步长以及更新位置以找到下一个点 $x_1$。\n\n确定点 $x_1$ 的坐标。将你的答案 $x_1$ 表示为一个具有精确分数分量的列向量。",
            "solution": "我们使用共轭梯度法，从 $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始，最小化二次函数 $f(x) = (x_{1}-2)^{2} + 2(x_{2}-1)^{2}$。其梯度和黑塞矩阵为\n$$\n\\nabla f(x) = \\begin{pmatrix} 2(x_{1}-2) \\\\ 4(x_{2}-1) \\end{pmatrix}, \\quad H = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix}.\n$$\n在 $x_{0}$ 处，梯度为\n$$\ng_{0} = \\nabla f(x_{0}) = \\begin{pmatrix} -4 \\\\ -4 \\end{pmatrix}.\n$$\n共轭梯度法的初始搜索方向是最速下降方向\n$$\nd_{0} = -g_{0} = \\begin{pmatrix} 4 \\\\ 4 \\end{pmatrix}.\n$$\n沿着 $d_0$ 进行精确线搜索，定义 $\\phi(\\alpha) = f(x_{0} + \\alpha d_{0})$。对于一个具有常数黑塞矩阵的二次函数，其导数为\n$$\n\\phi'(\\alpha) = d_{0}^{T}\\left(g_{0} + \\alpha H d_{0}\\right).\n$$\n令 $\\phi'(\\alpha) = 0$ 可得\n$$\nd_{0}^{T} g_{0} + \\alpha_{0}\\, d_{0}^{T} H d_{0} = 0 \\quad \\Longrightarrow \\quad \\alpha_{0} = -\\frac{d_{0}^{T} g_{0}}{d_{0}^{T} H d_{0}}.\n$$\n计算所需的量：\n$$\nd_{0}^{T} g_{0} = \\begin{pmatrix} 4 & 4 \\end{pmatrix} \\begin{pmatrix} -4 \\\\ -4 \\end{pmatrix} = -32, \\quad H d_{0} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 16 \\end{pmatrix},\n$$\n$$\nd_{0}^{T} H d_{0} = \\begin{pmatrix} 4 & 4 \\end{pmatrix} \\begin{pmatrix} 8 \\\\ 16 \\end{pmatrix} = 96.\n$$\n因此，\n$$\n\\alpha_{0} = -\\frac{-32}{96} = \\frac{1}{3}.\n$$\n更新以得到 $x_{1}$：\n$$\nx_{1} = x_{0} + \\alpha_{0} d_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{3} \\begin{pmatrix} 4 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{3} \\\\ \\frac{4}{3} \\end{pmatrix}.\n$$\n因此，经过一次迭代后，下一个点是如上所示的具有精确分数分量的列向量。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{4}{3} \\\\ \\frac{4}{3} \\end{pmatrix}}$$"
        },
        {
            "introduction": "共轭梯度法之所以如此高效，关键在于它能生成一组相对于Hessian矩阵 $A$ “共轭”的搜索方向。本练习旨在揭示这一核心特性。你将计算头两个搜索方向，并亲手验证它们满足 $A$-共轭属性，从而深刻理解“共轭”一词的含义及其在算法中的作用。",
            "id": "2211315",
            "problem": "考虑一个无约束优化问题，即最小化一个二维二次函数 $f(\\mathbf{x})$，其中 $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$。该函数定义为 $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T A \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}$，其中对称正定矩阵 $A$ 和向量 $\\mathbf{b}$ 由下式给出：\n$$\nA = \\begin{pmatrix} 5 & 2 \\\\ 2 & 1 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n我们将使用共轭梯度 (CG) 方法的 Fletcher-Reeves 变体来寻找该函数的最小值。该函数的梯度为 $\\nabla f(\\mathbf{x}) = A\\mathbf{x} - \\mathbf{b}$。从点 $\\mathbf{x}_k$ 开始，CG 方法的迭代步骤如下：\n\n1.  令 $g_k = \\nabla f(\\mathbf{x}_k)$ 为当前点的梯度。\n2.  搜索方向 $\\mathbf{p}_k$ 由以下规则确定：\n    -   对于第一次迭代 ($k=0$)：$\\mathbf{p}_0 = -g_0$。\n    -   对于后续迭代 ($k > 0$)：$\\mathbf{p}_k = -g_k + \\beta_k \\mathbf{p}_{k-1}$，其中 $\\beta_k = \\frac{g_k^T g_k}{g_{k-1}^T g_{k-1}}$。\n3.  步长 $\\alpha_k$ 计算为 $\\alpha_k = \\frac{g_k^T g_k}{\\mathbf{p}_k^T A \\mathbf{p}_k}$。\n4.  位置更新为 $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$。\n5.  下一次迭代的梯度使用高效公式 $g_{k+1} = g_k + \\alpha_k A \\mathbf{p}_k$ 进行更新。\n\n如果量 $\\mathbf{u}^T A \\mathbf{v}$ 等于零，则称两个向量 $\\mathbf{u}$ 和 $\\mathbf{v}$ 是关于矩阵 $A$ 共轭的。CG 方法的一个关键特性是它生成一系列相互共轭的搜索方向。\n\n你的任务是应用 CG 方法，从点 $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始，生成前两个搜索方向 $\\mathbf{p}_0$ 和 $\\mathbf{p}_1$。然后，计算表达式 $\\mathbf{p}_0^T A \\mathbf{p}_1$ 的值。",
            "solution": "我们最小化 $f(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^{T}A\\mathbf{x}-\\mathbf{b}^{T}\\mathbf{x}$，其中 $A=\\begin{pmatrix}5&2\\\\2&1\\end{pmatrix}$，$\\mathbf{b}=\\begin{pmatrix}1\\\\1\\end{pmatrix}$，从 $\\mathbf{x}_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$ 开始。梯度为 $\\nabla f(\\mathbf{x})=A\\mathbf{x}-\\mathbf{b}$。\n\n计算初始梯度：\n$$\n\\mathbf{g}_{0}=\\nabla f(\\mathbf{x}_{0})=A\\mathbf{x}_{0}-\\mathbf{b}=\\begin{pmatrix}0\\\\0\\end{pmatrix}-\\begin{pmatrix}1\\\\1\\end{pmatrix}=\\begin{pmatrix}-1\\\\-1\\end{pmatrix}.\n$$\n第一个搜索方向：\n$$\n\\mathbf{p}_{0}=-\\mathbf{g}_{0}=\\begin{pmatrix}1\\\\1\\end{pmatrix}.\n$$\n步长 $\\alpha_{0}$：\n$$\n\\alpha_{0}=\\frac{\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}}{\\mathbf{p}_{0}^{T}A\\mathbf{p}_{0}},\\quad \\mathbf{g}_{0}^{T}\\mathbf{g}_{0}=(-1)^{2}+(-1)^{2}=2,\n$$\n$$\nA\\mathbf{p}_{0}=\\begin{pmatrix}5&2\\\\2&1\\end{pmatrix}\\begin{pmatrix}1\\\\1\\end{pmatrix}=\\begin{pmatrix}7\\\\3\\end{pmatrix},\\quad \\mathbf{p}_{0}^{T}A\\mathbf{p}_{0}=\\begin{pmatrix}1&1\\end{pmatrix}\\begin{pmatrix}7\\\\3\\end{pmatrix}=10,\n$$\n$$\n\\alpha_{0}=\\frac{2}{10}=\\frac{1}{5}.\n$$\n更新位置和梯度：\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}+\\alpha_{0}\\mathbf{p}_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}+\\frac{1}{5}\\begin{pmatrix}1\\\\1\\end{pmatrix}=\\begin{pmatrix}\\frac{1}{5}\\\\\\frac{1}{5}\\end{pmatrix},\n$$\n$$\n\\mathbf{g}_{1}=\\mathbf{g}_{0}+\\alpha_{0}A\\mathbf{p}_{0}=\\begin{pmatrix}-1\\\\-1\\end{pmatrix}+\\frac{1}{5}\\begin{pmatrix}7\\\\3\\end{pmatrix}=\\begin{pmatrix}\\frac{2}{5}\\\\-\\frac{2}{5}\\end{pmatrix}.\n$$\n计算 $\\beta_{1}$ 和第二个搜索方向 $\\mathbf{p}_{1}$：\n$$\n\\beta_{1}=\\frac{\\mathbf{g}_{1}^{T}\\mathbf{g}_{1}}{\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}}=\\frac{\\left(\\frac{2}{5}\\right)^{2}+\\left(-\\frac{2}{5}\\right)^{2}}{2}=\\frac{\\frac{8}{25}}{2}=\\frac{4}{25},\n$$\n$$\n\\mathbf{p}_{1}=-\\mathbf{g}_{1}+\\beta_{1}\\mathbf{p}_{0}=-\\begin{pmatrix}\\frac{2}{5}\\\\-\\frac{2}{5}\\end{pmatrix}+\\frac{4}{25}\\begin{pmatrix}1\\\\1\\end{pmatrix}=\\begin{pmatrix}-\\frac{6}{25}\\\\\\frac{14}{25}\\end{pmatrix}.\n$$\n最后，计算 $\\mathbf{p}_{0}^{T}A\\mathbf{p}_{1}$：\n$$\nA\\mathbf{p}_{1}=\\begin{pmatrix}5&2\\\\2&1\\end{pmatrix}\\begin{pmatrix}-\\frac{6}{25}\\\\\\frac{14}{25}\\end{pmatrix}=\\begin{pmatrix}-\\frac{2}{25}\\\\\\frac{2}{25}\\end{pmatrix},\n$$\n$$\n\\mathbf{p}_{0}^{T}A\\mathbf{p}_{1}=\\begin{pmatrix}1&1\\end{pmatrix}\\begin{pmatrix}-\\frac{2}{25}\\\\\\frac{2}{25}\\end{pmatrix}=0.\n$$\n这证实了前两个 CG 搜索方向是 $A$-共轭的。",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "算法的性能通常与其处理问题的“几何形状”密切相关。本练习通过一个特殊情况——目标函数的等高线是完美圆形（即Hessian矩阵为 $c\\mathbf{I}$ 形式）——来探索CG方法的理想表现。通过这个思想实验，你将发现为何CG方法在一步之内就能找到精确解，并理解它与最速下降法之间的内在联系。",
            "id": "2211314",
            "problem": "考虑多元二次函数 $f(\\mathbf{x})$ 的最小化问题，其定义为：\n$$f(\\mathbf{x}) = \\frac{1}{2} c \\mathbf{x}^T \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}$$\n其中 $\\mathbf{x}$ 和 $\\mathbf{b}$ 是 $\\mathbb{R}^n$ 中的列向量（对于某个整数 $n \\ge 1$），而 $c$ 是一个正实值常数。\n\n从一个 $f$ 的梯度为非零的任意初始点 $\\mathbf{x}_0$ 开始，执行一次共轭梯度（CG）法的迭代来找到下一个点 $\\mathbf{x}_1$。\n\n确定 $\\mathbf{x}_1$ 的表达式。请用常数 $c$ 以及向量 $\\mathbf{b}$ 和 $\\mathbf{x}_0$ 来表示你的答案。",
            "solution": "我们考虑二次函数 $f(\\mathbf{x})=\\frac{1}{2}c\\,\\mathbf{x}^{T}\\mathbf{x}-\\mathbf{b}^{T}\\mathbf{x}$，其中 $c>0$。其梯度和黑塞矩阵为\n$$\n\\nabla f(\\mathbf{x})=c\\,\\mathbf{x}-\\mathbf{b}, \\qquad \\nabla^{2}f(\\mathbf{x})=c\\,\\mathbf{I},\n$$\n因此最小化 $f$ 等价于求解线性系统\n$$\n\\mathbf{A}\\mathbf{x}=\\mathbf{b}, \\quad \\text{其中} \\quad \\mathbf{A}=c\\,\\mathbf{I},\n$$\n该系统是对称正定的。\n\n在共轭梯度法中，从梯度非零的 $\\mathbf{x}_{0}$ 开始，将第一次迭代的残差和搜索方向定义为\n$$\n\\mathbf{r}_{0}=\\mathbf{b}-\\mathbf{A}\\mathbf{x}_{0}=\\mathbf{b}-c\\,\\mathbf{x}_{0}, \\qquad \\mathbf{p}_{0}=\\mathbf{r}_{0}.\n$$\n步长 $\\alpha_{0}$ 由下式给出\n$$\n\\alpha_{0}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{\\mathbf{p}_{0}^{T}\\mathbf{A}\\mathbf{p}_{0}}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{\\mathbf{r}_{0}^{T}(c\\,\\mathbf{I})\\mathbf{r}_{0}}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{c\\,\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}=\\frac{1}{c}.\n$$\n因此，经过一次 CG 迭代后更新的点为\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}+\\alpha_{0}\\mathbf{p}_{0}=\\mathbf{x}_{0}+\\frac{1}{c}\\left(\\mathbf{b}-c\\,\\mathbf{x}_{0}\\right)=\\frac{1}{c}\\,\\mathbf{b}.\n$$\n因此，由于 $\\mathbf{A}=c\\,\\mathbf{I}$ 只有一个特征值，单次 CG 步就能达到精确的最小值点。",
            "answer": "$$\\boxed{\\frac{1}{c}\\,\\mathbf{b}}$$"
        }
    ]
}