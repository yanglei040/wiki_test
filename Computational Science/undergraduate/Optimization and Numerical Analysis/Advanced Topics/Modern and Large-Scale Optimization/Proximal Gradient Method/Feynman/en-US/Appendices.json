{
    "hands_on_practices": [
        {
            "introduction": "Before applying the full proximal gradient method, it's essential to master its core component: the proximal operator. This operator generalizes the concept of projection and provides a systematic way to handle the non-smooth or constrained part of an optimization problem. This first exercise provides foundational practice by asking you to derive the closed-form solution for the proximal operator of a simple quadratic function, helping you build a solid understanding of its definition and mechanics. ",
            "id": "2195112",
            "problem": "In the field of numerical optimization, the proximal operator is a fundamental tool used in algorithms designed to solve non-differentiable or constrained problems. For a given scalar function $g(x)$ and a positive scaling parameter $\\lambda > 0$, the proximal operator of $\\lambda g$ applied to a point $v$ is defined as the value of $x$ that minimizes a composite objective function.\n\nThe definition is formally given by:\n$$\n\\text{prox}_{\\lambda g}(v) = \\arg\\min_{x \\in \\mathbb{R}} \\left( g(x) + \\frac{1}{2\\lambda} (x-v)^2 \\right)\n$$\nYour task is to find the proximal operator for a general quadratic function. Consider the function $g(x) = \\frac{1}{2}ax^2 + bx$, where $a$ and $b$ are real-valued constants and $a > 0$.\n\nDerive a closed-form analytic expression for $\\text{prox}_{\\lambda g}(v)$ in terms of the parameters $a$, $b$, $v$, and $\\lambda$.",
            "solution": "We are asked to compute $\\text{prox}_{\\lambda g}(v)$ for $g(x)=\\frac{1}{2}a x^{2}+b x$ with $a>0$ and $\\lambda>0$. By definition,\n$$\n\\text{prox}_{\\lambda g}(v)=\\arg\\min_{x\\in\\mathbb{R}}\\left(\\frac{1}{2}a x^{2}+b x+\\frac{1}{2\\lambda}(x-v)^{2}\\right).\n$$\nDefine the objective\n$$\nJ(x)=\\frac{1}{2}a x^{2}+b x+\\frac{1}{2\\lambda}(x-v)^{2}.\n$$\nSince $a>0$ and $\\lambda>0$, the function $J$ is strictly convex because its second derivative is\n$$\nJ''(x)=a+\\frac{1}{\\lambda}>0,\n$$\nso it has a unique minimizer characterized by the first-order optimality condition $J'(x)=0$. Compute the derivative:\n$$\nJ'(x)=a x+b+\\frac{1}{\\lambda}(x-v)=(a+\\frac{1}{\\lambda})x+b-\\frac{v}{\\lambda}.\n$$\nSet $J'(x)=0$ and solve for $x$:\n$$\n(a+\\frac{1}{\\lambda})x+b-\\frac{v}{\\lambda}=0\n\\;\\;\\Longrightarrow\\;\\;\nx=\\frac{\\frac{v}{\\lambda}-b}{a+\\frac{1}{\\lambda}}.\n$$\nMultiplying numerator and denominator by $\\lambda$ gives\n$$\nx=\\frac{v-b\\lambda}{1+a\\lambda}.\n$$\nTherefore,\n$$\n\\text{prox}_{\\lambda g}(v)=\\frac{v-b\\lambda}{1+a\\lambda}.\n$$",
            "answer": "$$\\boxed{\\frac{v-b\\lambda}{1+a\\lambda}}$$"
        },
        {
            "introduction": "With a grasp of the proximal operator, we can now see how it functions within a single iteration of the proximal gradient algorithm. Each step of the method involves a powerful two-part process: a standard gradient descent update on the smooth part of the objective, followed by a \"proximal correction\" that enforces the structure imposed by the non-smooth part. This hands-on problem allows you to execute one complete iteration for a simple constrained problem, giving you a tangible feel for the interplay between the gradient and proximal steps. ",
            "id": "2195110",
            "problem": "Consider the optimization problem of finding a point $x = (x_1, x_2) \\in \\mathbb{R}^2$ that minimizes a function $F(x)$ subject to non-negativity constraints on its components, i.e., $x_1 \\ge 0$ and $x_2 \\ge 0$. The function to be minimized is the squared Euclidean distance from $x$ to a target point $a$, given by $F(x) = \\frac{1}{2}\\|x - a\\|_2^2$.\n\nThis problem can be cast into the standard form for proximal algorithms, $\\min_{x} f(x) + g(x)$, by defining the smooth part as $f(x) = \\frac{1}{2}\\|x - a\\|_2^2$ and the non-smooth part $g(x)$ as the indicator function for the non-negative orthant. The indicator function $g(x)$ is zero if $x_1 \\ge 0$ and $x_2 \\ge 0$, and infinity otherwise.\n\nYou are tasked with applying the proximal gradient method to solve this problem. The iterative update rule for the proximal gradient method is given by:\n$$ x_{k+1} = \\text{prox}_{\\gamma g}(x_k - \\gamma \\nabla f(x_k)) $$\nwhere $\\gamma$ is the step size and $\\text{prox}_{\\gamma g}$ is the proximal operator associated with the function $g$.\n\nGiven the target point $a = (5, -4)$, the initial point $x_0 = (1, 1)$, and a step size of $\\gamma = 0.2$, compute the next iterate, $x_1$. Express your answer as a row vector $(x_{1,1}, x_{1,2})$, where $x_{1,1}$ and $x_{1,2}$ are the components of the vector $x_1$.",
            "solution": "We are minimizing $F(x)=\\frac{1}{2}\\|x-a\\|_{2}^{2}$ over the non-negative orthant. In the proximal gradient decomposition, set $f(x)=\\frac{1}{2}\\|x-a\\|_{2}^{2}$ and $g(x)=\\iota_{\\mathbb{R}_{+}^{2}}(x)$, the indicator of the feasible set $\\mathbb{R}_{+}^{2}=\\{x\\in\\mathbb{R}^{2}:x_{1}\\ge 0,\\ x_{2}\\ge 0\\}$.\n\nThe gradient of $f$ is given by\n$$\n\\nabla f(x)=x-a.\n$$\nThe proximal operator of $\\gamma g$ at a point $z$ is the Euclidean projection onto $\\mathbb{R}_{+}^{2}$:\n$$\n\\text{prox}_{\\gamma g}(z)=\\operatorname*{arg\\,min}_{y\\in\\mathbb{R}^{2}}\\left\\{\\iota_{\\mathbb{R}_{+}^{2}}(y)+\\frac{1}{2\\gamma}\\|y-z\\|_{2}^{2}\\right\\}=P_{\\mathbb{R}_{+}^{2}}(z),\n$$\nwhich is the componentwise truncation at zero:\n$$\nP_{\\mathbb{R}_{+}^{2}}(z)=\\big(\\max\\{z_{1},0\\},\\ \\max\\{z_{2},0\\}\\big).\n$$\n\nWith $a=(5,-4)$, $x_{0}=(1,1)$, and $\\gamma=0.2$, compute the gradient at $x_{0}$:\n$$\n\\nabla f(x_{0})=x_{0}-a=(1,1)-(5,-4)=(-4,5).\n$$\nPerform the gradient step:\n$$\nx_{0}-\\gamma \\nabla f(x_{0})=(1,1)-0.2\\,(-4,5)=(1+0.8,\\ 1-1)=\\left(\\frac{9}{5},\\ 0\\right).\n$$\nApply the proximal map, i.e., the projection onto $\\mathbb{R}_{+}^{2}$:\n$$\nx_{1}=\\text{prox}_{\\gamma g}\\big(x_{0}-\\gamma \\nabla f(x_{0})\\big)=P_{\\mathbb{R}_{+}^{2}}\\left(\\frac{9}{5},0\\right)=\\left(\\frac{9}{5},0\\right),\n$$\nsince both components are already non-negative.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{9}{5} & 0 \\end{pmatrix}}$$"
        },
        {
            "introduction": "A complete algorithm is more than just a single step; it is an iterative process that must eventually terminate. In any practical implementation, we need a reliable stopping criterion to halt the algorithm when the iterates are no longer changing in a meaningful way. This final exercise brings everything together by having you simulate multiple iterations of the proximal gradient method on the classic LASSO problem, where you will apply the algorithm until a common and practical convergence tolerance is met. ",
            "id": "2195138",
            "problem": "Consider the optimization problem known as LASSO (Least Absolute Shrinkage and Selection Operator), which aims to find a vector $x \\in \\mathbb{R}^n$ that minimizes the objective function $F(x) = f(x) + g(x)$. The function is composed of a smooth part, $f(x) = \\frac{1}{2} \\|Ax - b\\|_2^2$, and a non-smooth regularization part, $g(x) = \\lambda \\|x\\|_1$, where $\\lambda > 0$ is a regularization parameter.\n\nThis problem can be solved using the proximal gradient method, which generates a sequence of iterates $\\{x_k\\}$ according to the update rule:\n$$x_{k+1} = \\text{prox}_{\\alpha g}(x_k - \\alpha \\nabla f(x_k))$$\nwhere $\\alpha > 0$ is a constant step size. The proximal operator for $g(x) = \\lambda \\|x\\|_1$ is the soft-thresholding operator, $\\text{prox}_{\\alpha g}(v) = S_{\\alpha\\lambda}(v)$, which acts component-wise on a vector $v$ as:\n$$[S_{c}(v)]_i = \\text{sign}(v_i) \\max(|v_i| - c, 0)$$\nfor a positive constant $c$.\n\nYou are tasked to simulate this algorithm for a specific configuration. Let the vector be $x \\in \\mathbb{R}^2$. The system parameters are given by:\n$$A = \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 6 \\\\ 2 \\end{pmatrix}, \\quad \\lambda = 4$$\nThe algorithm starts from the initial point $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and uses a step size of $\\alpha = 0.25$.\n\nA common practical stopping criterion is to terminate the algorithm when the change between successive iterates is sufficiently small. The algorithm is said to have converged and terminates after completing iteration $k+1$ if the condition $\\|x_{k+1} - x_k\\|_2 < \\epsilon$ is met for the first time, where $\\epsilon$ is a predefined tolerance. For this problem, use a tolerance of $\\epsilon = 1.0$.\n\nDetermine the total number of iterations performed before the algorithm terminates.",
            "solution": "The problem asks for the number of iterations the proximal gradient method takes to converge based on a given stopping criterion. The objective function is $F(x) = \\frac{1}{2} \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1$.\n\nFirst, we need to find the gradient of the smooth part, $f(x) = \\frac{1}{2} \\|Ax - b\\|_2^2$.\nThe function $f(x)$ can be written as $f(x) = \\frac{1}{2} (Ax - b)^T (Ax - b) = \\frac{1}{2} (x^T A^T - b^T)(Ax - b) = \\frac{1}{2} (x^T A^T A x - x^T A^T b - b^T A x + b^T b)$.\nThe gradient with respect to $x$ is $\\nabla f(x) = \\frac{1}{2} (2 A^T A x - A^T b - (b^T A)^T) = A^T A x - A^T b = A^T(Ax - b)$.\n\nLet's compute the matrix $A^T A$:\n$$A^T A = \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 1 \\cdot 1 & 1 \\cdot 1 + 1 \\cdot (-1) \\\\ 1 \\cdot 1 + (-1) \\cdot 1 & 1 \\cdot 1 + (-1) \\cdot (-1) \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix} = 2I$$\nwhere $I$ is the $2 \\times 2$ identity matrix.\nLet's also compute the vector $A^T b$:\n$$A^T b = \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 6 + 1 \\cdot 2 \\\\ 1 \\cdot 6 - 1 \\cdot 2 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}$$\nSo, the gradient is $\\nabla f(x) = 2Ix - A^T b = 2x - \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}$.\n\nThe proximal gradient iteration is $x_{k+1} = \\text{prox}_{\\alpha g}(x_k - \\alpha \\nabla f(x_k))$.\nThe argument of the proximal operator is $v_k = x_k - \\alpha \\nabla f(x_k) = x_k - \\alpha \\left(2x_k - \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}\\right) = (1 - 2\\alpha)x_k + \\alpha \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}$.\nThe update is $x_{k+1} = S_{\\alpha\\lambda}(v_k)$.\n\nWe are given the parameters $\\alpha = 0.25$ and $\\lambda = 4$.\nThe constant for the soft-thresholding operator is $c = \\alpha \\lambda = 0.25 \\times 4 = 1$.\nThe expression for $v_k$ becomes:\n$v_k = (1 - 2(0.25))x_k + 0.25 \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix} = 0.5 x_k + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$.\nThe iteration rule is thus:\n$x_{k+1} = S_1\\left(0.5 x_k + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\right)$.\n\nWe start with $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and the stopping tolerance is $\\epsilon = 1.0$.\n\n**Iteration 1 (k=0):**\nWe start with $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nFirst, calculate $v_0$:\n$$v_0 = 0.5 x_0 + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 0.5 \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\nNext, apply the soft-thresholding operator to find $x_1$:\n$$x_1 = S_1(v_0) = \\begin{pmatrix} S_1(2) \\\\ S_1(1) \\end{pmatrix} = \\begin{pmatrix} \\text{sign}(2)\\max(|2|-1,0) \\\\ \\text{sign}(1)\\max(|1|-1,0) \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot \\max(1,0) \\\\ 1 \\cdot \\max(0,0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\nNow, check the stopping criterion: $\\|x_1 - x_0\\|_2 < \\epsilon$.\n$$\\|x_1 - x_0\\|_2 = \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\sqrt{1^2 + 0^2} = 1$$\nThe condition is $1 < 1.0$, which is false. The algorithm does not terminate. We proceed to the next iteration.\n\n**Iteration 2 (k=1):**\nWe start with $x_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nFirst, calculate $v_1$:\n$$v_1 = 0.5 x_1 + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 0.5 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2.5 \\\\ 1 \\end{pmatrix}$$\nNext, apply the soft-thresholding operator to find $x_2$:\n$$x_2 = S_1(v_1) = \\begin{pmatrix} S_1(2.5) \\\\ S_1(1) \\end{pmatrix} = \\begin{pmatrix} \\text{sign}(2.5)\\max(|2.5|-1,0) \\\\ \\text{sign}(1)\\max(|1|-1,0) \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot \\max(1.5,0) \\\\ 1 \\cdot \\max(0,0) \\end{pmatrix} = \\begin{pmatrix} 1.5 \\\\ 0 \\end{pmatrix}$$\nNow, check the stopping criterion: $\\|x_2 - x_1\\|_2 < \\epsilon$.\n$$\\|x_2 - x_1\\|_2 = \\left\\| \\begin{pmatrix} 1.5 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\left\\| \\begin{pmatrix} 0.5 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\sqrt{0.5^2 + 0^2} = 0.5$$\nThe condition is $0.5 < 1.0$, which is true. The algorithm terminates.\n\nThe first iteration (k=0) produced $x_1$. The second iteration (k=1) produced $x_2$. The stopping condition was met after the second iteration was completed. Therefore, the total number of iterations performed is 2.",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}