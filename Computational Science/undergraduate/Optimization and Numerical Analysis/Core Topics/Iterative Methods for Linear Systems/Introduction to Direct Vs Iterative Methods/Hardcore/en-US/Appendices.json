{
    "hands_on_practices": [
        {
            "introduction": "To truly understand how iterative methods work, it's best to start by performing the calculations yourself. This exercise walks you through two iterations of the Gauss-Seidel method, a fundamental algorithm for solving linear systems. By working through the steps, you will gain a hands-on feel for how the solution approximation is refined and appreciate the core feature of this method: using newly updated information as soon as it becomes available .",
            "id": "2180024",
            "problem": "Consider the system of linear equations given by $A\\mathbf{x} = \\mathbf{b}$, where the vector $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ is unknown, and the matrix $A$ and vector $\\mathbf{b}$ are defined as:\n$$\nA = \\begin{pmatrix} 5  -2 \\\\ -1  3 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 7 \\\\ 4 \\end{pmatrix}\n$$\nThe Gauss-Seidel method is an iterative algorithm used to find an approximate solution to this system. For a general $2 \\times 2$ system, the update rules for the components of the iterate $\\mathbf{x}^{(k+1)}$ based on the previous iterate $\\mathbf{x}^{(k)}$ are:\n$$\nx_1^{(k+1)} = \\frac{1}{a_{11}} \\left(b_1 - a_{12}x_2^{(k)}\\right)\n$$\n$$\nx_2^{(k+1)} = \\frac{1}{a_{22}} \\left(b_2 - a_{21}x_1^{(k+1)}\\right)\n$$\nNote that the most recently computed value, $x_1^{(k+1)}$, is immediately used to compute $x_2^{(k+1)}$ within the same iteration.\n\nStarting with the initial guess $\\mathbf{x}^{(0)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, compute the approximation $\\mathbf{x}^{(2)}$ after two full iterations of the Gauss-Seidel method. Express your final answer as a column vector where each component is an exact fraction in its simplest form.",
            "solution": "The problem asks for the second iterate, $\\mathbf{x}^{(2)}$, of the Gauss-Seidel method for a given linear system $A\\mathbf{x} = \\mathbf{b}$ and an initial guess $\\mathbf{x}^{(0)}$.\n\nThe system is defined by:\n$$\nA = \\begin{pmatrix} 5  -2 \\\\ -1  3 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 7 \\\\ 4 \\end{pmatrix}\n$$\nThis corresponds to the following two linear equations:\n1. $5x_1 - 2x_2 = 7$\n2. $-x_1 + 3x_2 = 4$\n\nThe initial guess is $\\mathbf{x}^{(0)} = \\begin{pmatrix} x_1^{(0)} \\\\ x_2^{(0)} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nThe Gauss-Seidel iteration formulas specific to this system are derived from the general formulas provided:\nFor the first component, $x_1^{(k+1)}$:\n$$\nx_1^{(k+1)} = \\frac{1}{5} \\left(7 - (-2)x_2^{(k)}\\right) = \\frac{1}{5} \\left(7 + 2x_2^{(k)}\\right)\n$$\nFor the second component, $x_2^{(k+1)}$:\n$$\nx_2^{(k+1)} = \\frac{1}{3} \\left(4 - (-1)x_1^{(k+1)}\\right) = \\frac{1}{3} \\left(4 + x_1^{(k+1)}\\right)\n$$\n\n**Iteration 1 (k=0):**\nWe compute $\\mathbf{x}^{(1)} = \\begin{pmatrix} x_1^{(1)} \\\\ x_2^{(1)} \\end{pmatrix}$ using the components of $\\mathbf{x}^{(0)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nFirst, we calculate $x_1^{(1)}$ using $x_2^{(0)}=1$:\n$$\nx_1^{(1)} = \\frac{1}{5} \\left(7 + 2x_2^{(0)}\\right) = \\frac{1}{5} \\left(7 + 2(1)\\right) = \\frac{9}{5}\n$$\nNext, we calculate $x_2^{(1)}$ using the newly computed value $x_1^{(1)} = \\frac{9}{5}$.\n$$\nx_2^{(1)} = \\frac{1}{3} \\left(4 + x_1^{(1)}\\right) = \\frac{1}{3} \\left(4 + \\frac{9}{5}\\right) = \\frac{1}{3} \\left(\\frac{20}{5} + \\frac{9}{5}\\right) = \\frac{1}{3} \\left(\\frac{29}{5}\\right) = \\frac{29}{15}\n$$\nSo, after the first iteration, the approximation is $\\mathbf{x}^{(1)} = \\begin{pmatrix} 9/5 \\\\ 29/15 \\end{pmatrix}$.\n\n**Iteration 2 (k=1):**\nNow we compute $\\mathbf{x}^{(2)} = \\begin{pmatrix} x_1^{(2)} \\\\ x_2^{(2)} \\end{pmatrix}$ using the components of $\\mathbf{x}^{(1)} = \\begin{pmatrix} 9/5 \\\\ 29/15 \\end{pmatrix}$.\n\nFirst, we calculate $x_1^{(2)}$ using $x_2^{(1)} = \\frac{29}{15}$:\n$$\nx_1^{(2)} = \\frac{1}{5} \\left(7 + 2x_2^{(1)}\\right) = \\frac{1}{5} \\left(7 + 2\\left(\\frac{29}{15}\\right)\\right) = \\frac{1}{5} \\left(7 + \\frac{58}{15}\\right)\n$$\nTo add the terms in the parenthesis, we find a common denominator:\n$$\nx_1^{(2)} = \\frac{1}{5} \\left(\\frac{7 \\times 15}{15} + \\frac{58}{15}\\right) = \\frac{1}{5} \\left(\\frac{105 + 58}{15}\\right) = \\frac{1}{5} \\left(\\frac{163}{15}\\right) = \\frac{163}{75}\n$$\nNext, we calculate $x_2^{(2)}$ using the newly computed value $x_1^{(2)} = \\frac{163}{75}$:\n$$\nx_2^{(2)} = \\frac{1}{3} \\left(4 + x_1^{(2)}\\right) = \\frac{1}{3} \\left(4 + \\frac{163}{75}\\right)\n$$\nAgain, we find a common denominator:\n$$\nx_2^{(2)} = \\frac{1}{3} \\left(\\frac{4 \\times 75}{75} + \\frac{163}{75}\\right) = \\frac{1}{3} \\left(\\frac{300 + 163}{75}\\right) = \\frac{1}{3} \\left(\\frac{463}{75}\\right) = \\frac{463}{225}\n$$\nBoth fractions $\\frac{163}{75}$ and $\\frac{463}{225}$ are in their simplest form since 163 and 463 are prime numbers and do not divide 75 or 225.\n\nThus, the approximation after two full iterations is $\\mathbf{x}^{(2)} = \\begin{pmatrix} 163/75 \\\\ 463/225 \\end{pmatrix}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{163}{75} \\\\ \\frac{463}{225} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Once we know how to perform an iteration, the next logical question is about efficiency: how quickly does the method approach the true solution? The speed of convergence is a critical factor in choosing an iterative algorithm. This practice problem allows you to compare two methods with different linear convergence rates, helping you to translate the abstract concept of an error constant into a concrete number of required iterations .",
            "id": "2180012",
            "problem": "In the field of orbital mechanics, engineers often use iterative numerical methods to solve complex, non-linear equations for satellite positioning. Consider one such problem where two different algorithms, Method A and Method B, are proposed to find a solution. Both methods are known to exhibit linear convergence near the true solution, meaning the error at a given iteration is proportional to the error from the previous iteration.\n\nFor Method A, the error bound is described by the inequality $|e_{k+1}| \\le C_A |e_k|$, where $C_A = \\exp(-\\alpha)$ is the asymptotic error constant. Here, $\\alpha$ is a dimensionless parameter related to the algorithm's damping strategy.\n\nFor Method B, the error bound is given by $|e_{k+1}| \\le C_B |e_k|$, with an error constant $C_B = \\frac{1}{1 + \\beta^2}$. Here, $\\beta$ is a dimensionless parameter representing the stiffness of the system of equations being solved.\n\nAssume a particular scenario where the parameters are $\\alpha = 0.25$ and $\\beta = 3$. Both methods start with the same initial guess, which has an error magnitude of $|e_0| = 0.8$. The goal is to reach a tolerance where the error magnitude is less than or equal to $\\epsilon = 1.0 \\times 10^{-7}$.\n\nCalculate how many more full iterations the slower iterative method requires to reach the desired tolerance compared to the faster method. The number of iterations must be an integer.",
            "solution": "Given linear convergence with bound $|e_{k+1}| \\le C|e_{k}|$ and $0C1$, by induction we have the $k$-step bound\n$$\n|e_{k}| \\le C^{k}|e_{0}|.\n$$\nTo achieve $|e_{k}| \\le \\epsilon$, it suffices that\n$$\nC^{k}|e_{0}| \\le \\epsilon \\quad \\Longleftrightarrow \\quad k \\ge \\frac{\\ln(\\epsilon/|e_{0}|)}{\\ln C}.\n$$\nThe minimal integer number of iterations is $k_{\\min}=\\lceil \\frac{\\ln(\\epsilon/|e_{0}|)}{\\ln C} \\rceil$.\n\nMethod A: $C_{A}=\\exp(-\\alpha)$ with $\\alpha=0.25=\\frac{1}{4}$. Then $\\ln C_{A}=\\ln(\\exp(-\\tfrac{1}{4}))=-\\tfrac{1}{4}$, so\n$$\nk_{A} \\ge \\frac{\\ln(\\epsilon/|e_{0}|)}{-\\tfrac{1}{4}} = -4\\,\\ln\\!\\left(\\frac{\\epsilon}{|e_{0}|}\\right) = 4\\,\\ln\\!\\left(\\frac{|e_{0}|}{\\epsilon}\\right).\n$$\nWith $|e_{0}|=0.8$ and $\\epsilon=1.0 \\times 10^{-7}$,\n$$\n\\frac{|e_{0}|}{\\epsilon}=\\frac{0.8}{1.0 \\times 10^{-7}}=0.8 \\times 10^{7}=8 \\times 10^{6},\n$$\nhence\n$$\nk_{A} \\ge 4\\,\\ln(8 \\times 10^{6})=4\\,[\\ln 8 + 6 \\ln 10].\n$$\nNumerically, $\\ln 8 \\approx 2.079441542$ and $\\ln 10 \\approx 2.302585093$, so\n$$\nk_{A} \\ge 4\\,(2.079441542 + 6 \\times 2.302585093) \\approx 4 \\times 15.894952100 \\approx 63.579808400,\n$$\nand therefore $k_{A}=\\lceil 63.579808400 \\rceil=64$.\n\nMethod B: $C_{B}=\\frac{1}{1+\\beta^{2}}$ with $\\beta=3$, so $C_{B}=\\frac{1}{10}=10^{-1}$ and $\\ln C_{B}=-\\ln 10$. Then\n$$\nk_{B} \\ge \\frac{\\ln(\\epsilon/|e_{0}|)}{-\\ln 10} = \\frac{\\ln(|e_{0}|/\\epsilon)}{\\ln 10} = \\log_{10}\\!\\left(\\frac{|e_{0}|}{\\epsilon}\\right) = \\log_{10}(8 \\times 10^{6}) = 6 + \\log_{10} 8.\n$$\nSince $\\log_{10} 8 \\approx 0.903089987$, we get\n$$\nk_{B} \\ge 6.903089987 \\quad \\Longrightarrow \\quad k_{B}=\\lceil 6.903089987 \\rceil=7.\n$$\n\nMethod B is faster because $C_{B}C_{A}$. The slower method (Method A) therefore requires $k_{A}-k_{B}=64-7=57$ more full iterations to reach the tolerance.",
            "answer": "$$\\boxed{57}$$"
        },
        {
            "introduction": "The choice between a direct and an iterative method is not just about speed; it's also about reliability and the type of answer you can expect. This is especially true for singular systems, which do not have a unique solution. This problem presents a head-to-head comparison, asking you to predict the outcome of using Gaussian elimination versus the Jacobi method on a system with infinitely many solutions, thereby highlighting their fundamentally different behaviors in such scenarios .",
            "id": "2180013",
            "problem": "Consider the following system of linear equations $A\\mathbf{x} = \\mathbf{b}$:\n$$\n\\begin{align*}\n2x_1 - x_2 + 3x_3 = 4 \\\\\n-4x_1 + 2x_2 - 6x_3 = -8 \\\\\nx_1 + x_2 + x_3 = 6\n\\end{align*}\n$$\nTwo different numerical methods are proposed to solve this system.\nMethod 1: A direct method, specifically Gaussian elimination, which systematically transforms the augmented matrix $[A|\\mathbf{b}]$ into an upper triangular form to solve for the variables.\nMethod 2: A stationary iterative method, specifically the Jacobi method, which generates a sequence of approximations $\\mathbf{x}^{(k)}$ starting from an initial guess $\\mathbf{x}^{(0)} = [0, 0, 0]^T$.\n\nWhich of the following statements most accurately describes the expected outcome when applying these two methods to the given linear system?\n\nA. Both methods will fail. Gaussian elimination will halt due to a division-by-zero error, and the Jacobi method will produce a sequence of vectors that diverges to infinity.\n\nB. Gaussian elimination will correctly identify that the system has a unique solution. The Jacobi method will also converge to this unique solution.\n\nC. Gaussian elimination will terminate by revealing that the system is inconsistent and has no solution. The Jacobi method will fail to converge.\n\nD. Both methods will successfully find a solution. Gaussian elimination will provide the general form of the infinite solutions, and the Jacobi method will converge to one particular solution from this infinite set.\n\nE. Gaussian elimination will terminate by revealing that the system has infinitely many solutions. The Jacobi method will fail to converge to a specific, unique solution vector.",
            "solution": "Write the system as $A\\mathbf{x}=\\mathbf{b}$ with\n$$\nA=\\begin{pmatrix}\n2  -1  3\\\\\n-4  2  -6\\\\\n1  1  1\n\\end{pmatrix},\\quad\n\\mathbf{b}=\\begin{pmatrix}\n4\\\\\n-8\\\\\n6\n\\end{pmatrix}.\n$$\nObserve that the second equation is exactly $-2$ times the first equation:\n$$\n-2\\,(2x_{1}-x_{2}+3x_{3})=-4x_{1}+2x_{2}-6x_{3}=-8,\n$$\nso the second row is linearly dependent on the first, and the right-hand sides satisfy the same dependence, hence no inconsistency arises from the first two equations. The third equation is not a scalar multiple of the first, so it is independent. Thus $\\operatorname{rank}(A)=2$ and $\\operatorname{rank}([A|\\mathbf{b}])=23$, implying there are infinitely many solutions. Performing Gaussian elimination confirms this and yields one free variable. For instance, solving\n$$\n\\begin{cases}\n2x_{1}-x_{2}+3x_{3}=4,\\\\\nx_{1}+x_{2}+x_{3}=6\n\\end{cases}\n$$\nfor $x_{1},x_{2}$ in terms of the free parameter $x_{3}=t$ gives\n$3x_{1}+4x_{3}=10 \\Rightarrow x_{1}=\\frac{10-4t}{3}$, and $x_{2}=6-x_{1}-x_{3}=6-\\frac{10-4t}{3}-t=\\frac{8+t}{3}$,\nso Gaussian elimination will produce the general solution set with infinitely many solutions. Therefore, the direct method succeeds and does not report inconsistency or uniqueness.\n\nFor the Jacobi method, write the Jacobi iteration in the standard splitting $A=D+L+U$, where\n$D=\\operatorname{diag}(2,2,1)$, and $L+U=A-D=\\begin{pmatrix}\n0  -1  3\\\\\n-4  0  -6\\\\\n1  1  0\n\\end{pmatrix}$.\nThe Jacobi iteration is\n$$\n\\mathbf{x}^{(k+1)}=D^{-1}\\big(\\mathbf{b}-(L+U)\\mathbf{x}^{(k)}\\big)\n= D^{-1}\\mathbf{b} - D^{-1}(L+U)\\mathbf{x}^{(k)}\\equiv \\mathbf{c}+T\\,\\mathbf{x}^{(k)},\n$$\nwith iteration matrix\n$$\nT=-D^{-1}(L+U)=-\\begin{pmatrix}\n\\frac{1}{2}  0  0\\\\\n0  \\frac{1}{2}  0\\\\\n0  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n0  -1  3\\\\\n-4  0  -6\\\\\n1  1  0\n\\end{pmatrix}\n=\\begin{pmatrix}\n0  \\frac{1}{2}  -\\frac{3}{2}\\\\\n2  0  3\\\\\n-1  -1  0\n\\end{pmatrix}.\n$$\nConvergence of the Jacobi method requires the spectral radius $\\rho(T)1$. Compute the characteristic polynomial of $T$:\n$$\n\\det(T-\\lambda I)=\\det\\begin{pmatrix}\n-\\lambda  \\frac{1}{2}  -\\frac{3}{2}\\\\\n2  -\\lambda  3\\\\\n-1  -1  -\\lambda\n\\end{pmatrix}\n=-\\lambda^{3}-\\frac{1}{2}\\lambda+\\frac{3}{2},\n$$\nso the eigenvalues satisfy\n$2\\lambda^{3}+\\lambda-3=0$.\nOne root is $\\lambda=1$ since $2\\cdot 1+1-3=0$. Factoring gives\n$(\\lambda-1)(2\\lambda^{2}+2\\lambda+3)$,\nand the remaining eigenvalues are\n$$\n\\lambda=\\frac{-2\\pm \\sqrt{-20}}{4}=\\frac{-1\\pm i\\sqrt{5}}{2},\n$$\nwhose modulus is\n$$\n\\left|\\frac{-1\\pm i\\sqrt{5}}{2}\\right|=\\sqrt{\\frac{1}{4}+\\frac{5}{4}}=\\sqrt{\\frac{3}{2}}>1.\n$$\nTherefore\n$$\n\\rho(T)=\\max\\left\\{1,\\sqrt{\\frac{3}{2}}\\right\\}=\\sqrt{\\frac{3}{2}}>1,\n$$\nso the Jacobi iteration does not converge (from the given initial guess $\\mathbf{x}^{(0)}=\\mathbf{0}$ or in general) to a solution.\n\nCombining these facts: Gaussian elimination will show the system has infinitely many solutions, while the Jacobi method will fail to converge to a specific, unique solution vector. The most accurate choice is E.",
            "answer": "$$\\boxed{E}$$"
        }
    ]
}