{
    "hands_on_practices": [
        {
            "introduction": "The fundamental theorem for iterative methods states that convergence is guaranteed if and only if the spectral radius $\\rho(T)$ of the iteration matrix $T$ is strictly less than 1. To move from this abstract principle to a concrete calculation, this first exercise asks you to derive the explicit convergence condition for the Jacobi method on a general $2 \\times 2$ system . By directly computing the eigenvalues of the Jacobi iteration matrix, you will see how the condition $\\rho(T_J) \\lt 1$ translates into a simple relationship between the matrix elements.",
            "id": "2163209",
            "problem": "Consider a general non-singular linear system of two equations with two unknowns, represented by the matrix equation $A\\mathbf{x} = \\mathbf{b}$, where the matrix $A$ is given by:\n$$\nA = \\begin{pmatrix} a_{11}  a_{12} \\\\ a_{21}  a_{22} \\end{pmatrix}\n$$\nIt is assumed that the diagonal elements, $a_{11}$ and $a_{22}$, are non-zero, allowing for the application of iterative methods like the Jacobi method.\n\nThe Jacobi method is an iterative algorithm for determining a numerical solution of a system of linear equations. A necessary and sufficient condition for a stationary iterative method to converge for any initial guess is that the absolute value of a specific expression, derived from the elements of matrix $A$, must be strictly less than 1.\n\nFor the Jacobi method to be guaranteed to converge for this 2x2 system, the absolute value of which of the following expressions must be strictly less than 1?\n\nA. $\\frac{a_{12}a_{21}}{a_{11}a_{22}}$\n\nB. $\\frac{a_{11}a_{22}}{a_{12}a_{21}}$\n\nC. $\\frac{a_{12}}{a_{11}} + \\frac{a_{21}}{a_{22}}$\n\nD. $\\frac{a_{11}+a_{22}}{a_{12}+a_{21}}$\n\nE. $a_{11} a_{22} - a_{12} a_{21}$",
            "solution": "We write the linear system as $A\\mathbf{x}=\\mathbf{b}$ with\n$$\nA=\\begin{pmatrix} a_{11}  a_{12} \\\\ a_{21}  a_{22} \\end{pmatrix},\n$$\nand assume $a_{11}\\neq 0$ and $a_{22}\\neq 0$ so that the diagonal matrix $D=\\operatorname{diag}(a_{11},a_{22})$ is invertible. Using the standard splitting $A=D+L+U$, where\n$$\nD=\\begin{pmatrix} a_{11}  0 \\\\ 0  a_{22} \\end{pmatrix},\\quad\nL=\\begin{pmatrix} 0  0 \\\\ a_{21}  0 \\end{pmatrix},\\quad\nU=\\begin{pmatrix} 0  a_{12} \\\\ 0  0 \\end{pmatrix},\n$$\nthe Jacobi iteration is\n$$\n\\mathbf{x}^{(k+1)}=-D^{-1}(L+U)\\mathbf{x}^{(k)} + D^{-1}\\mathbf{b}.\n$$\nThus the iteration matrix is\n$$\nT_{J}=-D^{-1}(L+U).\n$$\nFor the given $2\\times 2$ system,\n$$\nT_{J}\n=-\\begin{pmatrix} \\frac{1}{a_{11}}  0 \\\\ 0  \\frac{1}{a_{22}} \\end{pmatrix}\n\\begin{pmatrix} 0  a_{12} \\\\ a_{21}  0 \\end{pmatrix}\n=\n\\begin{pmatrix} 0  -\\frac{a_{12}}{a_{11}} \\\\ -\\frac{a_{21}}{a_{22}}  0 \\end{pmatrix}.\n$$\nA necessary and sufficient condition for convergence of a stationary iterative method for every initial guess is that the spectral radius of its iteration matrix is strictly less than $1$, i.e.,\n$$\n\\rho(T_{J})1.\n$$\nTo compute $\\rho(T_{J})$, we find the eigenvalues $\\lambda$ from the characteristic equation:\n$$\n\\det(\\lambda I - T_{J})=\\det\\begin{pmatrix} \\lambda  \\frac{a_{12}}{a_{11}} \\\\ \\frac{a_{21}}{a_{22}}  \\lambda \\end{pmatrix}\n=\\lambda^{2}-\\frac{a_{12}a_{21}}{a_{11}a_{22}}=0.\n$$\nHence the eigenvalues are\n$$\n\\lambda=\\pm\\sqrt{\\frac{a_{12}a_{21}}{a_{11}a_{22}}}.\n$$\nTherefore, the spectral radius is\n$$\n\\rho(T_{J})=\\max\\{|\\lambda|\\}=\\sqrt{\\left|\\frac{a_{12}a_{21}}{a_{11}a_{22}}\\right|}.\n$$\nThe convergence condition $\\rho(T_{J})1$ is thus equivalent to\n$$\n\\left|\\frac{a_{12}a_{21}}{a_{11}a_{22}}\\right|1.\n$$\nAmong the given options, this is the absolute value of the expression in option A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "While calculating the spectral radius provides a definitive test for convergence, it can be computationally expensive. A far more practical approach is to check for a sufficient condition, such as strict diagonal dominance (SDD), which guarantees convergence for both Jacobi and Gauss-Seidel methods. This next practice explores the powerful idea that even if a matrix is not initially SDD, it can sometimes be rearranged into one . Mastering this technique of row permutation is a valuable problem-solving skill for preparing a linear system for iterative solution.",
            "id": "2163189",
            "problem": "An $n \\times n$ matrix $A$ is defined as being Strictly Diagonally Dominant (SDD) if, for every row, the absolute value of the diagonal entry is strictly greater than the sum of the absolute values of all other non-diagonal entries in that row. Mathematically, for each $i \\in \\{1, 2, ..., n\\}$, the condition $|a_{ii}|  \\sum_{j \\neq i} |a_{ij}|$ must hold.\n\nConsider the following 3x3 matrix $M$:\n$$\nM = \\begin{pmatrix} 1  -2  9 \\\\ 7  -3  2 \\\\ 1  8  -4 \\end{pmatrix}\n$$\nBy performing only row swaps, is it possible to rearrange the matrix $M$ into a new matrix $P$ that is strictly diagonally dominant?\n\nSelect the correct statement from the options below.\n\nA. The original matrix $M$ is already strictly diagonally dominant.\n\nB. It is impossible to make the matrix strictly diagonally dominant through any permutation of its rows.\n\nC. The matrix $P = \\begin{pmatrix} 7  -3  2 \\\\ 1  8  -4 \\\\ 1  -2  9 \\end{pmatrix}$ is a possible strictly diagonally dominant rearrangement.\n\nD. The matrix $P = \\begin{pmatrix} 1  8  -4 \\\\ 1  -2  9 \\\\ 7  -3  2 \\end{pmatrix}$ is a possible strictly diagonally dominant rearrangement.\n\nE. The matrix $P = \\begin{pmatrix} 1  -2  9 \\\\ 7  -3  2 \\\\ 1  8  -4 \\end{pmatrix}$ is strictly diagonally dominant, but this is the same as the original matrix.",
            "solution": "We use the definition: a matrix is strictly diagonally dominant (SDD) if for every row $i$, the diagonal entry satisfies $|a_{ii}|  \\sum_{j \\neq i} |a_{ij}|$.\n\nFirst, test the given matrix $M$ row by row to check option A and E:\n$$\nM=\\begin{pmatrix}\n1  -2  9 \\\\\n7  -3  2 \\\\\n1  8  -4\n\\end{pmatrix}.\n$$\n- Row $1$: diagonal $a_{11}=1$, off-diagonal absolute sum $|{-2}|+|9|=2+9=11$. Check $|1|11$, which is false.\n- Row $2$: diagonal $a_{22}=-3$, off-diagonal sum $|7|+|2|=7+2=9$. Check $|{-3}|=39$, which is false.\n- Row $3$: diagonal $a_{33}=-4$, off-diagonal sum $|1|+|8|=1+8=9$. Check $|{-4}|=49$, which is false.\n\nThus $M$ is not SDD, so options A and E are false.\n\nNext, determine whether a row permutation can make an SDD matrix. A row swap places a chosen row in position $i$, making its column-$i$ entry the diagonal. For a given row $r=(r_{1},r_{2},r_{3})$, it can occupy position $i$ only if $|r_{i}|  |r_{j}| + |r_{k}|$ for the other two columns $j,k$.\n\nCompute, for each row, which column can serve as a dominating diagonal:\n- Row $1$ is $(1,-2,9)$ with absolute values $(1,2,9)$:\n  - Column $1$: check $|1|2+9$, i.e., $111$, false.\n  - Column $2$: check $|{-2}|=21+9=10$, false.\n  - Column $3$: check $|9|=91+2=3$, true. So row $1$ can be placed at position $3$.\n\n- Row $2$ is $(7,-3,2)$ with absolute values $(7,3,2)$:\n  - Column $1$: check $|7|=73+2=5$, true. So row $2$ can be placed at position $1$.\n  - Column $2$: check $|{-3}|=37+2=9$, false.\n  - Column $3$: check $|2|=27+3=10$, false.\n\n- Row $3$ is $(1,8,-4)$ with absolute values $(1,8,4)$:\n  - Column $1$: check $|1|=18+4=12$, false.\n  - Column $2$: check $|8|=81+4=5$, true. So row $3$ can be placed at position $2$.\n  - Column $3$: check $|{-4}|=41+8=9$, false.\n\nWe can therefore assign:\n- Position $1$ (column $1$): row $2$.\n- Position $2$ (column $2$): row $3$.\n- Position $3$ (column $3$): row $1$.\n\nThis yields the permuted matrix\n$$\nP=\\begin{pmatrix}\n7  -3  2 \\\\\n1  8  -4 \\\\\n1  -2  9\n\\end{pmatrix},\n$$\nwhich matches option C. Verify SDD explicitly:\n- Row $1$: diagonal $7$, off-diagonal sum $|{-3}|+|2|=3+2=5$, and $75$ holds.\n- Row $2$: diagonal $8$, off-diagonal sum $|1|+|{-4}|=1+4=5$, and $85$ holds.\n- Row $3$: diagonal $9$, off-diagonal sum $|1|+|{-2}|=1+2=3$, and $93$ holds.\n\nThus $P$ is SDD, so it is possible to obtain an SDD rearrangement by row swaps. Therefore, option B is false, option D is false (its first row would have $|1|8+4$ which fails), and the correct statement is option C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "A common question when learning iterative methods is: which is better, Jacobi or Gauss-Seidel? While Gauss-Seidel often converges faster by using the most up-to-date information, its convergence is not guaranteed even if the Jacobi method converges. This final challenge asks you to identify a classic counterexample: a $3 \\times 3$ system where the Jacobi method converges but the Gauss-Seidel method diverges . This exercise powerfully illustrates that the relationship between the two methods is more complex than it first appears and underscores the importance of analyzing each method's specific iteration matrix.",
            "id": "2163207",
            "problem": "In the numerical analysis of linear systems of the form $A\\mathbf{x} = \\mathbf{b}$, iterative methods are often employed. Consider a matrix $A$ that is decomposed into its diagonal part $D$, its strictly lower triangular part $L$, and its strictly upper triangular part $U$, such that $A = D + L + U$.\n\nTwo fundamental iterative schemes, the Jacobi method (Method J) and the Gauss-Seidel method (Method GS), are defined by the following update rules for the solution vector $\\mathbf{x}$ at iteration $k$:\n\n- **Method J:** $D\\mathbf{x}^{(k+1)} = -(L+U)\\mathbf{x}^{(k)} + \\mathbf{b}$\n- **Method GS:** $(D+L)\\mathbf{x}^{(k+1)} = -U\\mathbf{x}^{(k)} + \\mathbf{b}$\n\nFor these methods to converge to the unique solution for any initial guess $\\mathbf{x}^{(0)}$, the spectral radius of their respective iteration matrices ($T_J$ and $T_{GS}$) must be strictly less than 1. The iteration matrices are given by $T_J = -D^{-1}(L+U)$ and $T_{GS} = -(D+L)^{-1}U$.\n\nA well-known result states that for any $2 \\times 2$ system where the matrix $A$ has a non-zero diagonal, the convergence of Method J implies the convergence of Method GS. This property, however, does not hold for higher dimensions. Your task is to identify a counterexample in three dimensions.\n\nFor which of the following $3 \\times 3$ matrices $A$ does Method J converge, while Method GS diverges?\n\nA. $A = \\begin{pmatrix} 4  1  -1 \\\\ -1  3  1 \\\\ 2  1  5 \\end{pmatrix}$\n\nB. $A = \\begin{pmatrix} 1  2  -2 \\\\ 1  1  1 \\\\ 2  2  1 \\end{pmatrix}$\n\nC. $A = \\begin{pmatrix} 1  0  1 \\\\ -1  1  0 \\\\ 1  1  1 \\end{pmatrix}$\n\nD. $A = \\begin{pmatrix} 1  1  1 \\\\ 2  1  2 \\\\ 1  3  1 \\end{pmatrix}$",
            "solution": "To solve this problem, we must analyze each matrix option to determine the convergence properties of the Jacobi method (Method J) and the Gauss-Seidel method (Method GS). A method converges if the spectral radius, $\\rho(T)$, of its iteration matrix $T$ is less than 1. We use the standard decomposition $A = D + L + U$.\n\n**Option A: $A = \\begin{pmatrix} 4  1  -1 \\\\ -1  3  1 \\\\ 2  1  5 \\end{pmatrix}$**\nThis matrix is strictly diagonally dominant because for each row, the absolute value of the diagonal element is greater than the sum of the absolute values of the off-diagonal elements.\n- Row 1: $|4|  |1| + |-1| = 2$ (True)\n- Row 2: $|3|  |-1| + |1| = 2$ (True)\n- Row 3: $|5|  |2| + |1| = 3$ (True)\nIf a matrix is strictly diagonally dominant, both the Jacobi and Gauss-Seidel methods are guaranteed to converge. Thus, this is not the required counterexample.\n\n**Option B: $A = \\begin{pmatrix} 1  2  -2 \\\\ 1  1  1 \\\\ 2  2  1 \\end{pmatrix}$**\nWe decompose $A$ into $D$, $L$, and $U$:\n$D = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} = I$\n$L = \\begin{pmatrix} 0  0  0 \\\\ 1  0  0 \\\\ 2  2  0 \\end{pmatrix}$\n$U = \\begin{pmatrix} 0  2  -2 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix}$\n\n*Method J analysis:*\nThe iteration matrix is $T_J = -D^{-1}(L+U) = -(L+U) = \\begin{pmatrix} 0  -2  2 \\\\ -1  0  -1 \\\\ -2  -2  0 \\end{pmatrix}$.\nTo find the eigenvalues, we solve $\\det(T_J - \\lambda I) = 0$:\n$$ \\det \\begin{pmatrix} -\\lambda  -2  2 \\\\ -1  -\\lambda  -1 \\\\ -2  -2  -\\lambda \\end{pmatrix} = -\\lambda(\\lambda^2 - 2) - (-2)(\\lambda - 2) + 2(2 - 2\\lambda) = -\\lambda^3 + 2\\lambda + 2\\lambda - 4 + 4 - 4\\lambda = -\\lambda^3 $$\nThe characteristic equation is $-\\lambda^3 = 0$, which gives $\\lambda_1 = \\lambda_2 = \\lambda_3 = 0$.\nThe spectral radius is $\\rho(T_J) = \\max\\{|0|\\} = 0$. Since $\\rho(T_J)  1$, Method J converges (in fact, it converges in one iteration).\n\n*Method GS analysis:*\nThe iteration matrix is $T_{GS} = -(D+L)^{-1}U$. First, we compute $(D+L)^{-1}$.\n$D+L = \\begin{pmatrix} 1  0  0 \\\\ 1  1  0 \\\\ 2  2  1 \\end{pmatrix}$. Using forward substitution to find the inverse, we get $(D+L)^{-1} = \\begin{pmatrix} 1  0  0 \\\\ -1  1  0 \\\\ 0  -2  1 \\end{pmatrix}$.\nNow we compute $T_{GS}$:\n$$ T_{GS} = -\\begin{pmatrix} 1  0  0 \\\\ -1  1  0 \\\\ 0  -2  1 \\end{pmatrix} \\begin{pmatrix} 0  2  -2 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix} = -\\begin{pmatrix} 0  2  -2 \\\\ 0  -2  3 \\\\ 0  0  -2 \\end{pmatrix} = \\begin{pmatrix} 0  -2  2 \\\\ 0  2  -3 \\\\ 0  0  2 \\end{pmatrix} $$\nThe eigenvalues of this upper triangular matrix are its diagonal entries: $\\lambda_1 = 0, \\lambda_2 = 2, \\lambda_3 = 2$.\nThe spectral radius is $\\rho(T_{GS}) = \\max\\{|0|, |2|, |2|\\} = 2$.\nSince $\\rho(T_{GS})  1$, Method GS diverges.\nThis matrix is a counterexample where Jacobi converges and Gauss-Seidel diverges.\n\n**Option C: $A = \\begin{pmatrix} 1  0  1 \\\\ -1  1  0 \\\\ 1  1  1 \\end{pmatrix}$**\n$T_J = -D^{-1}(L+U) = - \\begin{pmatrix} 0  0  1 \\\\ -1  0  0 \\\\ 1  1  0 \\end{pmatrix} = \\begin{pmatrix} 0  0  -1 \\\\ 1  0  0 \\\\ -1  -1  0 \\end{pmatrix}$.\n$\\det(T_J - \\lambda I) = \\det\\begin{pmatrix} -\\lambda  0  -1 \\\\ 1  -\\lambda  0 \\\\ -1  -1  -\\lambda \\end{pmatrix} = -\\lambda(\\lambda^2) - 1(-1) = -\\lambda^3 + 1$.\nThe characteristic equation $\\lambda^3 = 1$ gives eigenvalues that are the cubic roots of unity, all with absolute value 1. Thus, $\\rho(T_J) = 1$. Convergence requires a spectral radius strictly less than 1, so Method J does not converge.\n\n**Option D: $A = \\begin{pmatrix} 1  1  1 \\\\ 2  1  2 \\\\ 1  3  1 \\end{pmatrix}$**\n$T_J = -D^{-1}(L+U) = - \\begin{pmatrix} 0  1  1 \\\\ 2  0  2 \\\\ 1  3  0 \\end{pmatrix}$.\n$\\det(T_J - \\lambda I) = \\det\\begin{pmatrix} -\\lambda  -1  -1 \\\\ -2  -\\lambda  -2 \\\\ -1  -3  -\\lambda \\end{pmatrix} = -\\lambda(\\lambda^2 - 6) + 1(2\\lambda - 2) -1(6-\\lambda) = -\\lambda^3 + 6\\lambda + 2\\lambda - 2 - 6 + \\lambda = -\\lambda^3 + 9\\lambda - 8 = 0$.\nLet $f(\\lambda) = \\lambda^3 - 9\\lambda + 8$. Testing integer roots of 8, we find $f(1)=1-9+8=0$, so $\\lambda=1$ is a root.\nThe eigenvalues are approximately $1, 2.54, -3.54$.\nThe spectral radius is $\\rho(T_J) \\approx 3.54 > 1$. Method J diverges.\n\nConclusion: Only matrix B satisfies the condition that Method J converges and Method GS diverges.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}