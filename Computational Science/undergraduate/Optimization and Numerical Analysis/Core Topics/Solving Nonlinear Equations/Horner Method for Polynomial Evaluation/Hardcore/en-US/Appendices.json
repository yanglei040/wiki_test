{
    "hands_on_practices": [
        {
            "introduction": "The best way to understand an algorithm is to execute it by hand. This first practice focuses on the core mechanics of Horner's method, asking you to trace the sequence of intermediate values generated during the evaluation of a simple polynomial. Completing this exercise will solidify your understanding of the recurrence relation that makes Horner's method so efficient. ",
            "id": "2177814",
            "problem": "A polynomial of degree $n$ is given by the general form $P(x) = \\sum_{i=0}^{n} a_i x^i = a_n x^n + a_{n-1} x^{n-1} + \\dots + a_1 x + a_0$. A computationally efficient method for evaluating this polynomial at a specific point $x=c$ involves generating a sequence of intermediate values, often referred to as Horner's method. This sequence, denoted by $y_0, y_1, \\dots, y_n$, is constructed using the following recurrence relation:\n- $y_0 = a_n$\n- $y_k = y_{k-1} \\cdot c + a_{n-k}$ for $k = 1, 2, \\dots, n$.\n\nThe final term of this sequence, $y_n$, is the value of the polynomial at $c$, i.e., $y_n = P(c)$.\n\nFor the polynomial $P(x) = 4x^3 - 2x^2 + 5x - 1$, determine the complete sequence of values $(y_0, y_1, y_2, y_3)$ that are generated by this method when evaluating $P(x)$ at the point $x=2$.",
            "solution": "We are given $P(x) = 4x^{3} - 2x^{2} + 5x - 1$ with degree $n=3$. Thus the coefficients are $a_{3} = 4$, $a_{2} = -2$, $a_{1} = 5$, and $a_{0} = -1$. We evaluate at $c=2$ using Horner’s recurrence:\n$$\ny_{0} = a_{3} = 4,\n$$\nand for $k=1,2,3$,\n$$\ny_{k} = y_{k-1}\\cdot c + a_{3-k}.\n$$\n\nCompute step by step:\n$$\ny_{1} = y_{0}\\cdot 2 + a_{2} = 4\\cdot 2 + (-2) = 8 - 2 = 6,\n$$\n$$\ny_{2} = y_{1}\\cdot 2 + a_{1} = 6\\cdot 2 + 5 = 12 + 5 = 17,\n$$\n$$\ny_{3} = y_{2}\\cdot 2 + a_{0} = 17\\cdot 2 + (-1) = 34 - 1 = 33.\n$$\n\nTherefore, the sequence is $(y_{0}, y_{1}, y_{2}, y_{3}) = (4, 6, 17, 33)$, and $y_{3} = P(2) = 33$.",
            "answer": "$$\\boxed{(4, 6, 17, 33)}$$"
        },
        {
            "introduction": "While Horner's method is elegant in theory, its behavior in the real world of finite-precision computers can be complex. This exercise simulates polynomial evaluation on a machine with limited precision to reveal a potential pitfall known as catastrophic cancellation. By carefully tracking the results of each arithmetic operation, you will gain a hands-on appreciation for why numerical stability is as important as algorithmic efficiency. ",
            "id": "2177809",
            "problem": "In numerical analysis, the choice of algorithm can dramatically affect the accuracy of the result, especially in finite-precision arithmetic. Consider the evaluation of a polynomial $P(x)$ given by:\n$$P(x) = a_4 x^4 + a_3 x^3 + a_2 x^2 + a_1 x + a_0$$\nwhere the coefficients are:\n$a_4 = 1.000000$\n$a_3 = -4.001000$\n$a_2 = 6.001500$\n$a_1 = -4.003000$\n$a_0 = 1.001001$\n\nYour task is to evaluate this polynomial at the point $x = 1.000150$ using Horner's method. You must simulate the computation on a machine with a floating-point arithmetic system that has a precision of 7 significant figures. Specifically, after each elementary arithmetic operation (addition, subtraction, and multiplication), you must round the intermediate result to 7 significant figures before using it in the next operation.\n\nCalculate the value of $P(1.000150)$ following this procedure. Express your final answer as a number rounded to four significant figures.",
            "solution": "We evaluate $P(x)$ at $x=1.000150$ via Horner’s method with rounding to 7 significant figures after each elementary operation:\n$$P(x)=((((a_{4}x)+a_{3})x+a_{2})x+a_{1})x+a_{0}.$$\n\nGiven $a_{4}=1.000000$, $a_{3}=-4.001000$, $a_{2}=6.001500$, $a_{1}=-4.003000$, $a_{0}=1.001001$, and $x=1.000150$.\n\nStart with $b_{4}=a_{4}=1.000000$.\n\n1) Multiply and round:\n$$t_{1}=\\operatorname{round}_{7}(b_{4}x)=\\operatorname{round}_{7}(1.000000\\cdot 1.000150)=1.000150.$$\n\n2) Add and round:\n$$b_{3}=\\operatorname{round}_{7}(t_{1}+a_{3})=\\operatorname{round}_{7}(1.000150-4.001000)=-3.000850.$$\n\n3) Multiply and round:\n$$t_{2}=\\operatorname{round}_{7}(b_{3}x)=\\operatorname{round}_{7}((-3.000850)\\cdot 1.000150)=\\operatorname{round}_{7}(-3.0013001275)=-3.001300.$$\n\n4) Add and round:\n$$b_{2}=\\operatorname{round}_{7}(t_{2}+a_{2})=\\operatorname{round}_{7}((-3.001300)+6.001500)=3.000200.$$\n\n5) Multiply and round:\n$$t_{3}=\\operatorname{round}_{7}(b_{2}x)=\\operatornameoperatorname{round}_{7}(3.000200\\cdot 1.000150)=\\operatorname{round}_{7}(3.00065003)=3.000650.$$\n\n6) Add and round:\n$$b_{1}=\\operatorname{round}_{7}(t_{3}+a_{1})=\\operatorname{round}_{7}(3.000650-4.003000)=-1.002350.$$\n\n7) Multiply and round:\n$$t_{4}=\\operatorname{round}_{7}(b_{1}x)=\\operatorname{round}_{7}((-1.002350)\\cdot 1.000150)=\\operatorname{round}_{7}(-1.0025003525)=-1.002500.$$\n\n8) Add and round:\n$$b_{0}=\\operatorname{round}_{7}(t_{4}+a_{0})=\\operatorname{round}_{7}((-1.002500)+1.001001)=-0.001499000.$$\n\nThus, under the specified rounding-after-each-operation procedure, $P(1.000150)=-0.001499000$. Rounding this result to four significant figures gives $-0.001499$.",
            "answer": "$$\\boxed{-0.001499}$$"
        },
        {
            "introduction": "An algorithm's efficiency isn't just about minimizing the total number of operations; its structure is crucial for performance on modern hardware. This problem challenges you to compare the inherently sequential Horner's method with a 'divide and conquer' algorithm designed for parallel execution. By analyzing the computational steps for both methods, you will explore the trade-offs between different algorithmic designs in the context of parallel computing. ",
            "id": "2177838",
            "problem": "In numerical analysis, evaluating a polynomial $P(x) = \\sum_{i=0}^{N} a_i x^i$ is a fundamental operation. While Horner's method provides an efficient sequential algorithm, its structure is difficult to parallelize. Consider an alternative approach for evaluation on a modern multi-core processor.\n\nLet $P(x)$ be a polynomial with $n$ coefficients, $\\{a_0, a_1, \\dots, a_{n-1}\\}$, such that $P(x) = \\sum_{i=0}^{n-1} a_i x^i$. For this problem, assume $n$ is a power of two, i.e., $n = 2^m$ for some positive integer $m \\ge 1$.\n\nWe will compare two methods for evaluating $P(x)$ based on a simplified model of computation where both a single floating-point multiplication and a single floating-point addition each take one \"computational step\".\n\n1.  **Horner's Method**: This standard sequential method evaluates the polynomial as $P(x) = a_0 + x(a_1 + x(a_2 + \\dots + x(a_{n-1}))\\dots)$. Since it is sequential, the total number of computational steps, $T_{Horner}$, is the sum of all operations performed on a single core.\n\n2.  **Recursive Parallel Splitting (RPS)**: This divide-and-conquer algorithm works as follows. The polynomial is split based on even and odd indexed coefficients:\n    $P(x) = \\left(\\sum_{k=0}^{n/2-1} a_{2k} x^{2k}\\right) + \\left(\\sum_{k=0}^{n/2-1} a_{2k+1} x^{2k+1}\\right)$\n    This can be rewritten as:\n    $P(x) = P_{even}(x^2) + x \\cdot P_{odd}(x^2)$,\n    where $P_{even}(y) = \\sum_{k=0}^{n/2-1} a_{2k} y^k$ and $P_{odd}(y) = \\sum_{k=0}^{n/2-1} a_{2k+1} y^k$.\n    Both $P_{even}$ and $P_{odd}$ are polynomials with $n/2$ coefficients. The RPS method computes $P_{even}(x^2)$ and $P_{odd}(x^2)$ recursively and in parallel on separate processor cores. We assume an ideal parallel environment with a sufficient number of cores to exploit all parallelism at each recursive step, and that communication overhead between cores is zero. The total time for the RPS method, $T_{RPS}$, is the number of steps in the longest chain of dependent operations.\n\nDetermine the ratio $\\mathcal{R}(n) = \\frac{T_{RPS}}{T_{Horner}}$ as a function of the number of coefficients, $n$.",
            "solution": "The problem asks for the ratio of the computational time required by the Recursive Parallel Splitting (RPS) method to that of Horner's method for evaluating a polynomial with $n$ terms, where $n=2^m$ and $m \\ge 1$. A computational step is defined as one addition or one multiplication.\n\n**Step 1: Analyze Horner's Method**\n\nHorner's method evaluates a polynomial $P(x) = \\sum_{i=0}^{n-1} a_i x^i$ of degree $n-1$ using the nested form:\n$P(x) = ((\\dots(a_{n-1}x + a_{n-2})x + \\dots + a_1)x + a_0)$.\n\nLet's count the operations. Starting from the innermost parenthesis, we have one multiplication ($a_{n-1}x$) and one addition ($+ a_{n-2}$). This process is repeated for each coefficient from $a_{n-2}$ down to $a_0$.\n- To compute $a_{n-1}x + a_{n-2}$, we perform 1 multiplication and 1 addition.\n- To multiply the result by $x$ and add $a_{n-3}$, we perform another 1 multiplication and 1 addition.\n- This continues until the final addition of $a_0$.\n\nFor a polynomial with $n$ coefficients ($a_0, \\dots, a_{n-1}$), there are $n-1$ such stages. Thus, there are a total of $n-1$ multiplications and $n-1$ additions.\nSince Horner's method is inherently sequential, the total number of computational steps is the sum of the operations.\n$T_{Horner}(n) = (n-1) \\text{ multiplications} + (n-1) \\text{ additions} = 2(n-1)$ steps.\n\n**Step 2: Analyze the Recursive Parallel Splitting (RPS) Method**\n\nThe RPS method is based on the recurrence $P(x) = P_{even}(x^2) + x \\cdot P_{odd}(x^2)$.\nLet $T_{RPS}(n)$ be the number of parallel computational steps to evaluate a polynomial with $n$ coefficients.\n\nThe evaluation proceeds as follows:\n1.  Compute the value $y = x^2$. This requires 1 multiplication. This step is necessary to provide the argument for the subproblems.\n2.  Evaluate $P_{even}(y)$ and $P_{odd}(y)$ recursively. Since the processor has sufficient cores, these two evaluations can happen in parallel. Both $P_{even}$ and $P_{odd}$ are polynomials with $n/2$ coefficients. The time taken for this stage is the time it takes to evaluate one of them, which is $T_{RPS}(n/2)$.\n3.  Combine the results. Once the values $V_{even} = P_{even}(y)$ and $V_{odd} = P_{odd}(y)$ are available, we must compute $P(x) = V_{even} + x \\cdot V_{odd}$. This involves one multiplication ($x \\cdot V_{odd}$) followed by one addition. These are sequential, so they take $1 + 1 = 2$ steps.\n\nThe operations in these three stages are dependent. The computation of $y=x^2$ must finish before the recursive calls can start. The recursive calls must finish before the final combination can start. Therefore, the total time is the sum of the times for each stage.\nThis gives the recurrence relation for $T_{RPS}(n)$:\n$T_{RPS}(n) = (\\text{Time for } x^2) + (\\text{Time for subproblems}) + (\\text{Time for combination})$\n$T_{RPS}(n) = 1 + T_{RPS}(n/2) + 2 = T_{RPS}(n/2) + 3$.\n\nThis recurrence is valid for $n > 2$. We need to establish a base case.\nLet's consider the smallest possible value of $n$ for the recursion, which is $n=2$ (since $m \\ge 1$).\nFor $n=2$, the polynomial is $P(x) = a_0 + a_1x$.\nThe RPS method would split this into $P_{even}(y) = a_0$ and $P_{odd}(y) = a_1$. These are polynomials with $n=1$ coefficient.\nThe time to evaluate a 1-coefficient polynomial, $P(x) = a_0$, is $T_{RPS}(1) = 0$ as no operations are needed.\nSo for $n=2$, the \"recursive\" step takes $T_{RPS}(1)=0$ time. The combination step is $P(x) = P_{even} + x \\cdot P_{odd} = a_0 + x \\cdot a_1$. This requires one multiplication and one addition, taking 2 steps. Note that the $y=x^2$ computation is not needed for this base case as the subproblems are constants.\nSo, the base case for our recurrence is $T_{RPS}(2) = 2$.\n\nNow we solve the recurrence relation $T_{RPS}(n) = T_{RPS}(n/2) + 3$ for $n = 2^m$ with $m > 1$, using the base case $T_{RPS}(2) = 2$.\nLet $n = 2^m$. The recurrence becomes $T_{RPS}(2^m) = T_{RPS}(2^{m-1}) + 3$ for $m > 1$.\nWe can unroll this recurrence:\n$T_{RPS}(2^m) = T_{RPS}(2^{m-1}) + 3$\n$= (T_{RPS}(2^{m-2}) + 3) + 3 = T_{RPS}(2^{m-2}) + 2 \\cdot 3$\n$= \\dots$\n$= T_{RPS}(2^1) + (m-1) \\cdot 3$\n\nSubstituting the base case $T_{RPS}(2^1) = T_{RPS}(2) = 2$:\n$T_{RPS}(2^m) = 2 + 3(m-1) = 2 + 3m - 3 = 3m - 1$.\n\nSince $n = 2^m$, we have $m = \\log_2(n)$. Substituting this back gives the closed form for $T_{RPS}(n)$:\n$T_{RPS}(n) = 3\\log_2(n) - 1$.\nThis formula is valid for $m \\ge 1$, which means $n \\ge 2$.\nLet's check for $m=1$ ($n=2$): $T_{RPS}(2) = 3\\log_2(2) - 1 = 3(1) - 1 = 2$. This matches our base case.\n\n**Step 3: Calculate the Ratio**\n\nWe have the expressions for the computational steps for both methods:\n- $T_{Horner}(n) = 2(n-1)$\n- $T_{RPS}(n) = 3\\log_2(n) - 1$\n\nThe desired ratio $\\mathcal{R}(n)$ is:\n$\\mathcal{R}(n) = \\frac{T_{RPS}(n)}{T_{Horner}(n)} = \\frac{3\\log_2(n) - 1}{2(n-1)}$.",
            "answer": "$$\\boxed{\\frac{3\\log_{2}(n) - 1}{2(n-1)}}$$"
        }
    ]
}