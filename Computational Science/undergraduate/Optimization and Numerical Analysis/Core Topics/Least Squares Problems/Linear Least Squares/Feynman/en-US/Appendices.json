{
    "hands_on_practices": [
        {
            "introduction": "This problem provides a foundational exercise in linear regression, one of the most common applications of the least squares method. By working through the process of fitting a straight line, $y = mx + c$, to a small set of data points, you will gain hands-on experience with setting up and solving the normal equations. This practice is essential for understanding how we can find the \"best-fit\" model for experimental data .",
            "id": "2142967",
            "problem": "A materials science student is investigating the elastic properties of a newly synthesized polymer fiber. In an experiment, the student applies a varying amount of tensile force and measures the resulting elongation of the fiber. The force is measured in a normalized, dimensionless unit, denoted by $x$, and the corresponding elongation is measured in another dimensionless unit, denoted by $y$. The student collects three data points $(x, y)$: $(0, 1)$, $(1, 3)$, and $(2, 2)$.\n\nTo model the material's behavior in this range, the student wishes to fit a linear model of the form $y = mx + c$ to the data. The parameters $m$ (the effective stiffness) and $c$ (the initial elongation at zero force) are to be determined using the method of least squares, which finds the line that minimizes the sum of the squared vertical distances from the data points to the line.\n\nDetermine the slope $m$ and the y-intercept $c$ of this best-fit line. Provide the values of $m$ and $c$, in that order, as your final answer. Express your results as exact fractions.",
            "solution": "We model the data by $y = mx + c$ and determine $m$ and $c$ by least squares, minimizing $S(m,c) = \\sum_{i=1}^{n}\\left(y_{i} - (m x_{i} + c)\\right)^{2}$. The normal equations obtained from $\\partial S/\\partial m = 0$ and $\\partial S/\\partial c = 0$ are\n$$\nm \\sum_{i=1}^{n} x_{i}^{2} + c \\sum_{i=1}^{n} x_{i} = \\sum_{i=1}^{n} x_{i} y_{i}, \\qquad\nm \\sum_{i=1}^{n} x_{i} + c\\, n = \\sum_{i=1}^{n} y_{i}.\n$$\nFor the data $(x,y) = (0,1),(1,3),(2,2)$, we have $n=3$, $\\sum x_{i} = 0+1+2 = 3$, $\\sum y_{i} = 1+3+2 = 6$, $\\sum x_{i}^{2} = 0^{2}+1^{2}+2^{2} = 5$, and $\\sum x_{i} y_{i} = 0\\cdot 1 + 1\\cdot 3 + 2\\cdot 2 = 7$. Substituting these into the normal equations gives\n$$\n5m + 3c = 7, \\qquad 3m + 3c = 6.\n$$\nSubtracting the second equation from the first yields $2m = 1$, hence $m = \\frac{1}{2}$. Substituting into $3m + 3c = 6$ gives $3\\left(\\frac{1}{2}\\right) + 3c = 6$, so $3c = \\frac{9}{2}$ and $c = \\frac{3}{2}$. These are exact fractions as required.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2} & \\frac{3}{2}\\end{pmatrix}}$$"
        },
        {
            "introduction": "After finding the best-fit parameters for a model, a crucial next step is to evaluate how well the model actually represents the data. This exercise shifts our focus from finding the solution to quantifying the error of that solution. You will calculate the least squares error, defined as the Euclidean norm of the residual vector, which provides a single, powerful measure of the discrepancy between the model's predictions and the actual observations .",
            "id": "2185338",
            "problem": "An engineering student is analyzing the motion of an object under a non-standard force field. The student records the object's position, $p$, at several different time points, $t$. The measurements, given as $(t, p)$ pairs, are (1 s, 3 m), (2 s, 10 m), and (3 s, 20 m).\n\nThe student proposes a theoretical model for the position as a function of time: $p(t) = c_{1}t + c_{2}t^2$, where $c_1$ and $c_2$ are unknown constants. To find the best fit for this model, the student sets up an overdetermined linear system $A\\mathbf{x} = \\mathbf{b}$ where $\\mathbf{x} = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$. Using the method of least squares, the student determines that the optimal coefficients are given by the vector $\\hat{\\mathbf{x}} = \\begin{pmatrix} 53/38 \\\\ 67/38 \\end{pmatrix}$.\n\nYour task is to calculate the least squares error for this model, which is defined as the Euclidean norm of the residual vector, $\\| \\mathbf{b} - A\\hat{\\mathbf{x}} \\|_2$. Express your final answer in meters, rounded to four significant figures.",
            "solution": "We model $p(t)=c_{1}t+c_{2}t^{2}$ with data $(t,p)$ at $t=1,2,3$. The design matrix and observation vector are\n$$\nA=\\begin{pmatrix}\n1 & 1 \\\\\n2 & 4 \\\\\n3 & 9\n\\end{pmatrix},\\quad\n\\mathbf{b}=\\begin{pmatrix}\n3 \\\\ 10 \\\\ 20\n\\end{pmatrix}.\n$$\nWith the least squares solution $\\hat{\\mathbf{x}}=\\begin{pmatrix} \\frac{53}{38} \\\\ \\frac{67}{38} \\end{pmatrix}$, the residual is $\\mathbf{r}=\\mathbf{b}-A\\hat{\\mathbf{x}}$. Compute $A\\hat{\\mathbf{x}}$ entrywise:\n$$\n(A\\hat{\\mathbf{x}})_{1}=1\\cdot \\frac{53}{38}+1\\cdot \\frac{67}{38}=\\frac{120}{38}=\\frac{60}{19},\\quad\n(A\\hat{\\mathbf{x}})_{2}=2\\cdot \\frac{53}{38}+4\\cdot \\frac{67}{38}=\\frac{374}{38}=\\frac{187}{19},\n$$\n$$\n(A\\hat{\\mathbf{x}})_{3}=3\\cdot \\frac{53}{38}+9\\cdot \\frac{67}{38}=\\frac{762}{38}=\\frac{381}{19}.\n$$\nThus\n$$\n\\mathbf{r}=\\begin{pmatrix}\n3-\\frac{60}{19} \\\\\n10-\\frac{187}{19} \\\\\n20-\\frac{381}{19}\n\\end{pmatrix}\n=\\begin{pmatrix}\n-\\frac{3}{19} \\\\\n\\frac{3}{19} \\\\\n-\\frac{1}{19}\n\\end{pmatrix}.\n$$\nThe least squares error is the Euclidean norm\n$$\n\\|\\mathbf{r}\\|_{2}=\\sqrt{\\left(-\\frac{3}{19}\\right)^{2}+\\left(\\frac{3}{19}\\right)^{2}+\\left(-\\frac{1}{19}\\right)^{2}}\n=\\frac{1}{19}\\sqrt{9+9+1}=\\frac{\\sqrt{19}}{19}=\\frac{1}{\\sqrt{19}}.\n$$\nConverting to a decimal and rounding to four significant figures gives\n$$\n\\|\\mathbf{b}-A\\hat{\\mathbf{x}}\\|_{2}=\\frac{1}{\\sqrt{19}}\\approx 0.2294.\n$$\nThis value is in meters.",
            "answer": "$$\\boxed{0.2294}$$"
        },
        {
            "introduction": "Linear least squares is a remarkably versatile tool that extends far beyond simple curve fitting. This advanced problem demonstrates its application in digital signal processing, a core area of modern engineering. You will design a digital filter to remove a specific frequency (like the $60 \\ \\mathrm{Hz}$ \"hum\" from electrical mains) from a signal, illustrating how the method can be used to decompose signals and isolate or remove unwanted components .",
            "id": "2409659",
            "problem": "You are given a uniformly sampled discrete-time signal that may contain a strong sinusoidal interference at $60$ $\\mathrm{Hz}$. For a set of sample times $\\{t_k\\}_{k=0}^{N-1}$, where $t_k = k / f_s$ with sampling rate $f_s$ in $\\mathrm{Hz}$ and $N$ the number of samples, model the interference as\n$$\ny_k \\approx c_1 \\sin(\\omega t_k) + c_2 \\cos(\\omega t_k),\n$$\nwhere $\\omega = 2\\pi \\cdot 60$ $\\mathrm{rad/s}$ and the angles of the trigonometric functions are in radians. For each test case below, determine the coefficients $c_1$ and $c_2$ that minimize\n$$\n\\sum_{k=0}^{N-1} \\left(y_k - c_1 \\sin(\\omega t_k) - c_2 \\cos(\\omega t_k)\\right)^2,\n$$\nthen subtract the fitted component from the original signal to form the residual sequence\n$$\nr_k = y_k - \\left(c_1 \\sin(\\omega t_k) + c_2 \\cos(\\omega t_k)\\right).\n$$\nFor each test case, compute the root-mean-square (RMS) value of the residual,\n$$\n\\mathrm{RMS} = \\sqrt{\\frac{1}{N}\\sum_{k=0}^{N-1} r_k^2}.\n$$\n\nUse the following test suite, where all amplitudes are dimensionless, time is in $\\mathrm{s}$, frequency is in $\\mathrm{Hz}$, and angular frequency is in $\\mathrm{rad/s}$:\n\n- Test case $1$ (general case with interfering content):\n  - $f_s = 8000$ $\\mathrm{Hz}$, $N = 4000$. For $k = 0,1,\\dots,N-1$, $t_k = k/f_s$ and\n    $$\n    y_k = 0.7 \\sin(\\omega t_k + 1.0) + 0.2 \\sin(2\\pi \\cdot 5 \\, t_k) + 0.1 \\sin(2\\pi \\cdot 120 \\, t_k).\n    $$\n- Test case $2$ (no $60$ $\\mathrm{Hz}$ component present):\n  - $f_s = 44100$ $\\mathrm{Hz}$, $N = 2205$. For $k = 0,1,\\dots,N-1$, $t_k = k/f_s$ and\n    $$\n    y_k = 0.3 \\cos(2\\pi \\cdot 440 \\, t_k) + 0.05 \\sin(2\\pi \\cdot 1000 \\, t_k).\n    $$\n- Test case $3$ (boundary case with minimal data and pure hum):\n  - $f_s = 1000$ $\\mathrm{Hz}$, $N = 2$. For $k = 0,1$, $t_k = k/f_s$ and\n    $$\n    y_k = 1.5 \\sin(\\omega t_k + 0.3).\n    $$\n\nYour program must compute, for each test case, the RMS of the residual after subtracting the best-fitting $c_1 \\sin(\\omega t) + c_2 \\cos(\\omega t)$ component defined above. The final output must be a single line containing the three RMS values, each rounded to six decimal places, as a comma-separated list enclosed in square brackets. For example,\n$$\n[\\text{rms}_1,\\text{rms}_2,\\text{rms}_3].\n$$",
            "solution": "We formalize the problem in linear algebraic terms. For a single test case, define the column vectors\n$$\n\\mathbf{s} = \\begin{bmatrix} \\sin(\\omega t_0) \\\\ \\sin(\\omega t_1) \\\\ \\vdots \\\\ \\sin(\\omega t_{N-1}) \\end{bmatrix}, \\quad\n\\mathbf{c} = \\begin{bmatrix} \\cos(\\omega t_0) \\\\ \\cos(\\omega t_1) \\\\ \\vdots \\\\ \\cos(\\omega t_{N-1}) \\end{bmatrix}, \\quad\n\\mathbf{y} = \\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_{N-1} \\end{bmatrix}.\n$$\nConstruct the design matrix\n$$\n\\mathbf{A} = \\begin{bmatrix} \\mathbf{s} & \\mathbf{c} \\end{bmatrix} \\in \\mathbb{R}^{N \\times 2},\n$$\nand let the parameter vector be\n$$\n\\boldsymbol{\\theta} = \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix}.\n$$\nThe model is $\\mathbf{y} \\approx \\mathbf{A}\\boldsymbol{\\theta}$. We seek $\\boldsymbol{\\theta}$ that minimizes the sum of squared residuals\n$$\nJ(\\boldsymbol{\\theta}) = \\|\\mathbf{y} - \\mathbf{A}\\boldsymbol{\\theta}\\|_2^2 = (\\mathbf{y} - \\mathbf{A}\\boldsymbol{\\theta})^\\top (\\mathbf{y} - \\mathbf{A}\\boldsymbol{\\theta}).\n$$\nSetting the gradient with respect to $\\boldsymbol{\\theta}$ equal to zero yields the normal equations\n$$\n\\nabla_{\\boldsymbol{\\theta}} J = -2 \\mathbf{A}^\\top \\mathbf{y} + 2 \\mathbf{A}^\\top \\mathbf{A} \\boldsymbol{\\theta} = \\mathbf{0}\n\\;\\;\\Rightarrow\\;\\;\n\\mathbf{A}^\\top \\mathbf{A} \\, \\boldsymbol{\\theta} = \\mathbf{A}^\\top \\mathbf{y}.\n$$\nWhen $\\mathbf{A}$ has full column rank, $\\mathbf{A}^\\top \\mathbf{A}$ is symmetric positive definite and invertible, and the unique minimizer is\n$$\n\\boldsymbol{\\theta}^\\star = (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{y}.\n$$\nThe fitted interference is $\\widehat{\\mathbf{y}} = \\mathbf{A}\\boldsymbol{\\theta}^\\star$, and the residual is $\\mathbf{r} = \\mathbf{y} - \\widehat{\\mathbf{y}}$. The root-mean-square (RMS) of the residual is\n$$\n\\mathrm{RMS} = \\sqrt{\\frac{1}{N} \\sum_{k=0}^{N-1} r_k^2} = \\sqrt{\\frac{1}{N} \\mathbf{r}^\\top \\mathbf{r}}.\n$$\n\nApplication to the test suite:\n- In test case $1$, the signal contains a $60$ $\\mathrm{Hz}$ component with amplitude $0.7$ and phase $1.0$ $\\mathrm{rad}$, plus additional components at $5$ $\\mathrm{Hz}$ and $120$ $\\mathrm{Hz}$. The least-squares fit will extract the $60$ $\\mathrm{Hz}$ portion, leaving mainly the $5$ $\\mathrm{Hz}$ and $120$ $\\mathrm{Hz}$ content in the residual. The computed RMS will reflect those remaining components.\n- In test case $2$, the signal has no $60$ $\\mathrm{Hz}$ content, so the optimal coefficients should be near zero, and the residual RMS will be nearly equal to the RMS of the original signal (up to small numerical projection effects over the finite window).\n- In test case $3$, with $N=2$ and a pure $60$ $\\mathrm{Hz}$ sinusoid present, the two columns of $\\mathbf{A}$ are linearly independent for the specified sampling times, so the two-parameter model can match the two samples exactly, yielding a residual of zero and hence an RMS of zero (within numerical precision).\n\nAlgorithmic steps for each case:\n1. Form $t_k = k/f_s$ for $k = 0,1,\\dots,N-1$ and compute $\\omega = 2\\pi \\cdot 60$ $\\mathrm{rad/s}$.\n2. Build $\\mathbf{A}$ with columns $\\sin(\\omega t_k)$ and $\\cos(\\omega t_k)$.\n3. Compute $\\boldsymbol{\\theta}^\\star$ by solving $\\mathbf{A}^\\top \\mathbf{A} \\, \\boldsymbol{\\theta} = \\mathbf{A}^\\top \\mathbf{y}$.\n4. Compute the residual $\\mathbf{r} = \\mathbf{y} - \\mathbf{A}\\boldsymbol{\\theta}^\\star$ and its $\\mathrm{RMS}$.\n5. Round each $\\mathrm{RMS}$ to six decimal places and output as a single list.\n\nThis procedure derives directly from the definition of the least-squares minimizer via the normal equations and the definition of the root-mean-square of a discrete sequence.",
            "answer": "```python\nimport numpy as np\n\ndef compute_rms_residual(fs, N, y_func):\n    \"\"\"\n    Compute the RMS of the residual after fitting c1*sin(omega*t) + c2*cos(omega*t)\n    at 60 Hz from the given signal y(t).\n    \"\"\"\n    t = np.arange(N, dtype=np.float64) / float(fs)\n    omega = 2.0 * np.pi * 60.0  # rad/s\n    # Design matrix with columns [sin(omega t), cos(omega t)]\n    A = np.column_stack((np.sin(omega * t), np.cos(omega * t)))\n    y = y_func(t).astype(np.float64)\n\n    # Solve normal equations (A^T A) theta = A^T y for theta = [c1, c2]\n    ATA = A.T @ A\n    ATy = A.T @ y\n    # Use solve; in the unlikely event of singularity, fall back to least squares\n    try:\n        theta = np.linalg.solve(ATA, ATy)\n    except np.linalg.LinAlgError:\n        theta, *_ = np.linalg.lstsq(A, y, rcond=None)\n\n    fitted = A @ theta\n    residual = y - fitted\n    rms = float(np.sqrt(np.mean(residual ** 2)))\n    return rms\n\ndef test_case_1():\n    # fs = 8000 Hz, N = 4000\n    fs = 8000\n    N = 4000\n    hum_omega = 2.0 * np.pi * 60.0\n    def y_func(t):\n        return (0.7 * np.sin(hum_omega * t + 1.0)\n                + 0.2 * np.sin(2.0 * np.pi * 5.0 * t)\n                + 0.1 * np.sin(2.0 * np.pi * 120.0 * t))\n    return compute_rms_residual(fs, N, y_func)\n\ndef test_case_2():\n    # fs = 44100 Hz, N = 2205\n    fs = 44100\n    N = 2205\n    def y_func(t):\n        return (0.3 * np.cos(2.0 * np.pi * 440.0 * t)\n                + 0.05 * np.sin(2.0 * np.pi * 1000.0 * t))\n    return compute_rms_residual(fs, N, y_func)\n\ndef test_case_3():\n    # fs = 1000 Hz, N = 2\n    fs = 1000\n    N = 2\n    hum_omega = 2.0 * np.pi * 60.0\n    def y_func(t):\n        return 1.5 * np.sin(hum_omega * t + 0.3)\n    return compute_rms_residual(fs, N, y_func)\n\ndef solve():\n    # Define and run the test suite\n    rms_values = [\n        test_case_1(),\n        test_case_2(),\n        test_case_3(),\n    ]\n    # Round to six decimals and format as required\n    formatted = [f\"{v:.6f}\" for v in rms_values]\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}