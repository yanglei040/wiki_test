{
    "hands_on_practices": [
        {
            "introduction": "The most classic application of linear least squares is finding the \"line of best fit\" for a set of data points. This foundational exercise grounds the method in a familiar context, guiding you through the process of setting up and solving the normal equations for a simple linear model, $y = mx + c$. Mastering this core skill is the first step toward applying least squares to more complex problems in science and engineering.",
            "id": "2142967",
            "problem": "A materials science student is investigating the elastic properties of a newly synthesized polymer fiber. In an experiment, the student applies a varying amount of tensile force and measures the resulting elongation of the fiber. The force is measured in a normalized, dimensionless unit, denoted by $x$, and the corresponding elongation is measured in another dimensionless unit, denoted by $y$. The student collects three data points $(x, y)$: $(0, 1)$, $(1, 3)$, and $(2, 2)$.\n\nTo model the material's behavior in this range, the student wishes to fit a linear model of the form $y = mx + c$ to the data. The parameters $m$ (the effective stiffness) and $c$ (the initial elongation at zero force) are to be determined using the method of least squares, which finds the line that minimizes the sum of the squared vertical distances from the data points to the line.\n\nDetermine the slope $m$ and the y-intercept $c$ of this best-fit line. Provide the values of $m$ and $c$, in that order, as your final answer. Express your results as exact fractions.",
            "solution": "We model the data by $y = mx + c$ and determine $m$ and $c$ by least squares, minimizing $S(m,c) = \\sum_{i=1}^{n}\\left(y_{i} - (m x_{i} + c)\\right)^{2}$. The normal equations obtained from $\\partial S/\\partial m = 0$ and $\\partial S/\\partial c = 0$ are\n$$\nm \\sum_{i=1}^{n} x_{i}^{2} + c \\sum_{i=1}^{n} x_{i} = \\sum_{i=1}^{n} x_{i} y_{i}, \\qquad\nm \\sum_{i=1}^{n} x_{i} + c\\, n = \\sum_{i=1}^{n} y_{i}.\n$$\nFor the data $(x,y) = (0,1),(1,3),(2,2)$, we have $n=3$, $\\sum x_{i} = 0+1+2 = 3$, $\\sum y_{i} = 1+3+2 = 6$, $\\sum x_{i}^{2} = 0^{2}+1^{2}+2^{2} = 5$, and $\\sum x_{i} y_{i} = 0\\cdot 1 + 1\\cdot 3 + 2\\cdot 2 = 7$. Substituting these into the normal equations gives\n$$\n5m + 3c = 7, \\qquad 3m + 3c = 6.\n$$\nSubtracting the second equation from the first yields $2m = 1$, hence $m = \\frac{1}{2}$. Substituting into $3m + 3c = 6$ gives $3\\left(\\frac{1}{2}\\right) + 3c = 6$, so $3c = \\frac{9}{2}$ and $c = \\frac{3}{2}$. These are exact fractions as required.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2} & \\frac{3}{2}\\end{pmatrix}}$$"
        },
        {
            "introduction": "The power of the least squares method lies in its ability to find the \"best\" approximate solution to an overdetermined system. But what happens if an exact solution already exists? This thought-provoking exercise explores this special case, revealing that the least-squares solution gracefully reduces to the exact solution, resulting in zero residual error. This practice deepens your understanding of the geometric nature of least squares as an orthogonal projection.",
            "id": "2409652",
            "problem": "Consider the overdetermined linear system with matrix $A \\in \\mathbb{R}^{3 \\times 2}$ and vector $b \\in \\mathbb{R}^{3}$ given by\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n3 \\\\\n1 \\\\\n-2\n\\end{pmatrix}.\n$$\nLet $x \\in \\mathbb{R}^{2}$ be the vector that minimizes the squared Euclidean norm of the residual, that is, minimizes $\\|A x - b\\|_{2}^{2}$. Determine the least-squares minimizer $x_{\\mathrm{LS}} \\in \\mathbb{R}^{2}$ and compute the Euclidean norm of the residual $r = b - A x_{\\mathrm{LS}}$, namely $\\|r\\|_{2}$. Provide exact values (no rounding). For your final response, report the triple $(x_{1}, x_{2}, \\|r\\|_{2})$ in a single row as a $1 \\times 3$ matrix.",
            "solution": "The problem is first subjected to a validation process.\n\nStep 1: Extract Givens.\nThe matrix $A \\in \\mathbb{R}^{3 \\times 2}$ is given by:\n$$ A = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix} $$\nThe vector $b \\in \\mathbb{R}^{3}$ is given by:\n$$ b = \\begin{pmatrix} 3 \\\\ 1 \\\\ -2 \\end{pmatrix} $$\nThe objective is to find the vector $x \\in \\mathbb{R}^{2}$ that minimizes the squared Euclidean norm of the residual, $\\|A x - b\\|_{2}^{2}$. This vector is denoted $x_{\\mathrm{LS}}$. The residual is $r = b - A x_{\\mathrm{LS}}$. The task is to determine $x_{\\mathrm{LS}} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ and compute the norm of the residual, $\\|r\\|_{2}$. The final result is to be reported as the triple $(x_{1}, x_{2}, \\|r\\|_{2})$.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is a standard exercise in linear least squares, a fundamental topic in computational engineering and linear algebra. The provided matrix $A$ has two columns, $\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $\\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$. These vectors are linearly independent, which ensures that the matrix $A$ has full column rank. Consequently, the matrix $A^T A$ is invertible, guaranteeing a unique solution $x_{\\mathrm{LS}}$ to the least-squares problem. The problem is self-contained, mathematically consistent, well-posed, and an objective question within the specified scientific domain. It does not violate any principles of scientific grounding or logic.\n\nStep 3: Verdict and Action.\nThe problem is deemed valid. A complete solution will now be derived.\n\nThe linear least-squares problem seeks to find a vector $x_{\\mathrm{LS}} \\in \\mathbb{R}^{2}$ that minimizes the objective function $J(x) = \\|Ax - b\\|_{2}^{2}$. A necessary condition for the minimum is that the gradient of $J(x)$ with respect to $x$ is zero. This condition leads to the set of linear equations known as the normal equations:\n$$ A^T A x_{\\mathrm{LS}} = A^T b $$\nFirst, we compute the matrix $A^T A$. The transpose of $A$ is:\n$$ A^T = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} $$\nThen, the product $A^T A$ is:\n$$ A^T A = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (1)(1) + (0)(0) & (1)(0) + (1)(1) + (0)(1) \\\\ (0)(1) + (1)(1) + (1)(0) & (0)(0) + (1)(1) + (1)(1) \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} $$\nNext, we compute the vector $A^T b$:\n$$ A^T b = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} (1)(3) + (1)(1) + (0)(-2) \\\\ (0)(3) + (1)(1) + (1)(-2) \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ -1 \\end{pmatrix} $$\nThe normal equations become:\n$$ \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ -1 \\end{pmatrix} $$\nThis corresponds to the system of two linear equations:\n$$ 2x_1 + x_2 = 4 $$\n$$ x_1 + 2x_2 = -1 $$\nFrom the first equation, we express $x_2$ in terms of $x_1$: $x_2 = 4 - 2x_1$. Substituting this into the second equation gives:\n$$ x_1 + 2(4 - 2x_1) = -1 $$\n$$ x_1 + 8 - 4x_1 = -1 $$\n$$ -3x_1 = -9 $$\n$$ x_1 = 3 $$\nNow, we substitute the value of $x_1$ back to find $x_2$:\n$$ x_2 = 4 - 2(3) = 4 - 6 = -2 $$\nThus, the least-squares solution is:\n$$ x_{\\mathrm{LS}} = \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix} $$\nThe next step is to compute the residual vector $r = b - A x_{\\mathrm{LS}}$. First, we calculate the product $A x_{\\mathrm{LS}}$:\n$$ A x_{\\mathrm{LS}} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} (1)(3) + (0)(-2) \\\\ (1)(3) + (1)(-2) \\\\ (0)(3) + (1)(-2) \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\\\ -2 \\end{pmatrix} $$\nThe residual vector is then:\n$$ r = b - A x_{\\mathrm{LS}} = \\begin{pmatrix} 3 \\\\ 1 \\\\ -2 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThis indicates that the vector $b$ lies in the column space of $A$, and the system $Ax = b$ had an exact solution, which the least-squares method has found. The Euclidean norm of the residual vector $r$ is:\n$$ \\|r\\|_{2} = \\sqrt{0^2 + 0^2 + 0^2} = 0 $$\nThe required triple $(x_1, x_2, \\|r\\|_{2})$ is therefore $(3, -2, 0)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 3 & -2 & 0 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Let's put theory into practice with a real-world application from signal processing. This problem challenges you to design a notch filter to remove unwanted $60$ $\\mathrm{Hz}$ electrical \"hum\" from a signal. By modeling the interference using a basis of sine and cosine functions, you will use the least squares method to estimate its amplitude and phase, and then subtract it from the original signal, demonstrating the versatility of this powerful technique.",
            "id": "2409659",
            "problem": "You are given a uniformly sampled discrete-time signal that may contain a strong sinusoidal interference at $60$ $\\mathrm{Hz}$. For a set of sample times $\\{t_k\\}_{k=0}^{N-1}$, where $t_k = k / f_s$ with sampling rate $f_s$ in $\\mathrm{Hz}$ and $N$ the number of samples, model the interference as\n$$\ny_k \\approx c_1 \\sin(\\omega t_k) + c_2 \\cos(\\omega t_k),\n$$\nwhere $\\omega = 2\\pi \\cdot 60$ $\\mathrm{rad/s}$ and the angles of the trigonometric functions are in radians. For each test case below, determine the coefficients $c_1$ and $c_2$ that minimize\n$$\n\\sum_{k=0}^{N-1} \\left(y_k - c_1 \\sin(\\omega t_k) - c_2 \\cos(\\omega t_k)\\right)^2,\n$$\nthen subtract the fitted component from the original signal to form the residual sequence\n$$\nr_k = y_k - \\left(c_1 \\sin(\\omega t_k) + c_2 \\cos(\\omega t_k)\\right).\n$$\nFor each test case, compute the root-mean-square (RMS) value of the residual,\n$$\n\\mathrm{RMS} = \\sqrt{\\frac{1}{N}\\sum_{k=0}^{N-1} r_k^2}.\n$$\n\nUse the following test suite, where all amplitudes are dimensionless, time is in $\\mathrm{s}$, frequency is in $\\mathrm{Hz}$, and angular frequency is in $\\mathrm{rad/s}$:\n\n- Test case $1$ (general case with interfering content):\n  - $f_s = 8000$ $\\mathrm{Hz}$, $N = 4000$. For $k = 0,1,\\dots,N-1$, $t_k = k/f_s$ and\n    $$\n    y_k = 0.7 \\sin(\\omega t_k + 1.0) + 0.2 \\sin(2\\pi \\cdot 5 \\, t_k) + 0.1 \\sin(2\\pi \\cdot 120 \\, t_k).\n    $$\n- Test case $2$ (no $60$ $\\mathrm{Hz}$ component present):\n  - $f_s = 44100$ $\\mathrm{Hz}$, $N = 2205$. For $k = 0,1,\\dots,N-1$, $t_k = k/f_s$ and\n    $$\n    y_k = 0.3 \\cos(2\\pi \\cdot 440 \\, t_k) + 0.05 \\sin(2\\pi \\cdot 1000 \\, t_k).\n    $$\n- Test case $3$ (boundary case with minimal data and pure hum):\n  - $f_s = 1000$ $\\mathrm{Hz}$, $N = 2$. For $k = 0,1$, $t_k = k/f_s$ and\n    $$\n    y_k = 1.5 \\sin(\\omega t_k + 0.3).\n    $$\n\nYour program must compute, for each test case, the RMS of the residual after subtracting the best-fitting $c_1 \\sin(\\omega t) + c_2 \\cos(\\omega t)$ component defined above. The final output must be a single line containing the three RMS values, each rounded to six decimal places, as a comma-separated list enclosed in square brackets. For example, `[rms_1,rms_2,rms_3]`.",
            "solution": "We formalize the problem in linear algebraic terms. For a single test case, define the column vectors\n$$\n\\mathbf{s} = \\begin{bmatrix} \\sin(\\omega t_0) \\\\ \\sin(\\omega t_1) \\\\ \\vdots \\\\ \\sin(\\omega t_{N-1}) \\end{bmatrix}, \\quad\n\\mathbf{c} = \\begin{bmatrix} \\cos(\\omega t_0) \\\\ \\cos(\\omega t_1) \\\\ \\vdots \\\\ \\cos(\\omega t_{N-1}) \\end{bmatrix}, \\quad\n\\mathbf{y} = \\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_{N-1} \\end{bmatrix}.\n$$\nConstruct the design matrix\n$$\n\\mathbf{A} = \\begin{bmatrix} \\mathbf{s} & \\mathbf{c} \\end{bmatrix} \\in \\mathbb{R}^{N \\times 2},\n$$\nand let the parameter vector be\n$$\n\\boldsymbol{\\theta} = \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix}.\n$$\nThe model is $\\mathbf{y} \\approx \\mathbf{A}\\boldsymbol{\\theta}$. We seek $\\boldsymbol{\\theta}$ that minimizes the sum of squared residuals\n$$\nJ(\\boldsymbol{\\theta}) = \\|\\mathbf{y} - \\mathbf{A}\\boldsymbol{\\theta}\\|_2^2 = (\\mathbf{y} - \\mathbf{A}\\boldsymbol{\\theta})^\\top (\\mathbf{y} - \\mathbf{A}\\boldsymbol{\\theta}).\n$$\nSetting the gradient with respect to $\\boldsymbol{\\theta}$ equal to zero yields the normal equations\n$$\n\\nabla_{\\boldsymbol{\\theta}} J = -2 \\mathbf{A}^\\top \\mathbf{y} + 2 \\mathbf{A}^\\top \\mathbf{A} \\boldsymbol{\\theta} = \\mathbf{0}\n\\;\\;\\Rightarrow\\;\\;\n\\mathbf{A}^\\top \\mathbf{A} \\, \\boldsymbol{\\theta} = \\mathbf{A}^\\top \\mathbf{y}.\n$$\nWhen $\\mathbf{A}$ has full column rank, $\\mathbf{A}^\\top \\mathbf{A}$ is symmetric positive definite and invertible, and the unique minimizer is\n$$\n\\boldsymbol{\\theta}^\\star = (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{y}.\n$$\nThe fitted interference is $\\widehat{\\mathbf{y}} = \\mathbf{A}\\boldsymbol{\\theta}^\\star$, and the residual is $\\mathbf{r} = \\mathbf{y} - \\widehat{\\mathbf{y}}$. The root-mean-square (RMS) of the residual is\n$$\n\\mathrm{RMS} = \\sqrt{\\frac{1}{N} \\sum_{k=0}^{N-1} r_k^2} = \\sqrt{\\frac{1}{N} \\mathbf{r}^\\top \\mathbf{r}}.\n$$\n\nApplication to the test suite:\n- In test case $1$, the signal contains a $60$ $\\mathrm{Hz}$ component with amplitude $0.7$ and phase $1.0$ $\\mathrm{rad}$, plus additional components at $5$ $\\mathrm{Hz}$ and $120$ $\\mathrm{Hz}$. The least-squares fit will extract the $60$ $\\mathrm{Hz}$ portion, leaving mainly the $5$ $\\mathrm{Hz}$ and $120$ $\\mathrm{Hz}$ content in the residual. The computed RMS will reflect those remaining components.\n- In test case $2$, the signal has no $60$ $\\mathrm{Hz}$ content, so the optimal coefficients should be near zero, and the residual RMS will be nearly equal to the RMS of the original signal (up to small numerical projection effects over the finite window).\n- In test case $3$, with $N=2$ and a pure $60$ $\\mathrm{Hz}$ sinusoid present, the two columns of $\\mathbf{A}$ are linearly independent for the specified sampling times, so the two-parameter model can match the two samples exactly, yielding a residual of zero and hence an RMS of zero (within numerical precision).\n\nAlgorithmic steps for each case:\n1. Form $t_k = k/f_s$ for $k = 0,1,\\dots,N-1$ and compute $\\omega = 2\\pi \\cdot 60$ $\\mathrm{rad/s}$.\n2. Build $\\mathbf{A}$ with columns $\\sin(\\omega t_k)$ and $\\cos(\\omega t_k)$.\n3. Compute $\\boldsymbol{\\theta}^\\star$ by solving $\\mathbf{A}^\\top \\mathbf{A} \\, \\boldsymbol{\\theta} = \\mathbf{A}^\\top \\mathbf{y}$.\n4. Compute the residual $\\mathbf{r} = \\mathbf{y} - \\mathbf{A}\\boldsymbol{\\theta}^\\star$ and its $\\mathrm{RMS}$.\n5. Round each $\\mathrm{RMS}$ to six decimal places and output as a single list.\n\nThis procedure derives directly from the definition of the least-squares minimizer via the normal equations and the definition of the root-mean-square of a discrete sequence.",
            "answer": "```python\nimport numpy as np\n\ndef compute_rms_residual(fs, N, y_func):\n    \"\"\"\n    Compute the RMS of the residual after fitting c1*sin(omega*t) + c2*cos(omega*t)\n    at 60 Hz from the given signal y(t).\n    \"\"\"\n    t = np.arange(N, dtype=np.float64) / float(fs)\n    omega = 2.0 * np.pi * 60.0  # rad/s\n    # Design matrix with columns [sin(omega t), cos(omega t)]\n    A = np.column_stack((np.sin(omega * t), np.cos(omega * t)))\n    y = y_func(t).astype(np.float64)\n\n    # Solve normal equations (A^T A) theta = A^T y for theta = [c1, c2]\n    ATA = A.T @ A\n    ATy = A.T @ y\n    # Use solve; in the unlikely event of singularity, fall back to least squares\n    try:\n        theta = np.linalg.solve(ATA, ATy)\n    except np.linalg.LinAlgError:\n        theta, *_ = np.linalg.lstsq(A, y, rcond=None)\n\n    fitted = A @ theta\n    residual = y - fitted\n    rms = float(np.sqrt(np.mean(residual ** 2)))\n    return rms\n\ndef test_case_1():\n    # fs = 8000 Hz, N = 4000\n    fs = 8000\n    N = 4000\n    hum_omega = 2.0 * np.pi * 60.0\n    def y_func(t):\n        return (0.7 * np.sin(hum_omega * t + 1.0)\n                + 0.2 * np.sin(2.0 * np.pi * 5.0 * t)\n                + 0.1 * np.sin(2.0 * np.pi * 120.0 * t))\n    return compute_rms_residual(fs, N, y_func)\n\ndef test_case_2():\n    # fs = 44100 Hz, N = 2205\n    fs = 44100\n    N = 2205\n    def y_func(t):\n        return (0.3 * np.cos(2.0 * np.pi * 440.0 * t)\n                + 0.05 * np.sin(2.0 * np.pi * 1000.0 * t))\n    return compute_rms_residual(fs, N, y_func)\n\ndef test_case_3():\n    # fs = 1000 Hz, N = 2\n    fs = 1000\n    N = 2\n    hum_omega = 2.0 * np.pi * 60.0\n    def y_func(t):\n        return 1.5 * np.sin(hum_omega * t + 0.3)\n    return compute_rms_residual(fs, N, y_func)\n\ndef solve():\n    # Define and run the test suite\n    rms_values = [\n        test_case_1(),\n        test_case_2(),\n        test_case_3(),\n    ]\n    # Round to six decimals and format as required\n    formatted = [f\"{v:.6f}\" for v in rms_values]\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}