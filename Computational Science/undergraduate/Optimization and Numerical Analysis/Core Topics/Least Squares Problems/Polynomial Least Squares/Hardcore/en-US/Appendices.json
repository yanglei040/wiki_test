{
    "hands_on_practices": [
        {
            "introduction": "This first practice grounds the theory of least squares in a tangible physical scenario. By analyzing hypothetical data from a spring experiment, you will derive the normal equations from first principles to find the best-fit line, a fundamental skill in experimental data analysis . This exercise provides a concrete application of linear regression, showing how to translate a physical law into a mathematical model and determine its parameters from noisy measurements.",
            "id": "2194103",
            "problem": "In a physics laboratory experiment to verify Hooke's Law, a student measures the displacement of a spring under various applied forces. The student models the relationship between the applied force, $F$, and the resulting displacement, $x$, using a linear equation $F(x) = kx + F_0$, where $k$ is the spring constant and $F_0$ is a potential force offset due to pre-loading or measurement-zeroing error.\n\nThe following five data points $(x_i, F_i)$ were collected, with displacement $x$ in meters (m) and force $F$ in Newtons (N):\n$$ (0.10, 16.0), \\ (0.20, 30.0), \\ (0.30, 46.0), \\ (0.40, 61.5), \\ (0.50, 74.0) $$\n\nUsing the method of least squares, determine the best-fit values for the spring constant $k$ and the force offset $F_0$. Report the value for the spring constant $k$ in N/m and the force offset $F_0$ in N. Round both of your final answers to three significant figures.",
            "solution": "The problem asks for the best-fit line of the form $F(x) = kx + F_0$ for a given set of five data points $(x_i, F_i)$. The method of least squares minimizes the sum of the squared residuals, $S$, which is the sum of the squared differences between the observed force values $F_i$ and the values predicted by the linear model, $F(x_i)$.\n\nThe function to minimize is:\n$$ S(k, F_0) = \\sum_{i=1}^{5} (F_i - (kx_i + F_0))^2 $$\n\nTo find the values of $k$ and $F_0$ that minimize $S$, we take the partial derivatives of $S$ with respect to $k$ and $F_0$ and set them to zero.\n\nThe partial derivative with respect to $k$ is:\n$$ \\frac{\\partial S}{\\partial k} = \\sum_{i=1}^{5} 2(F_i - kx_i - F_0)(-x_i) = -2 \\left( \\sum_{i=1}^{5} x_i F_i - k \\sum_{i=1}^{5} x_i^2 - F_0 \\sum_{i=1}^{5} x_i \\right) = 0 $$\nThis simplifies to the first normal equation:\n$$ k \\left( \\sum_{i=1}^{5} x_i^2 \\right) + F_0 \\left( \\sum_{i=1}^{5} x_i \\right) = \\sum_{i=1}^{5} x_i F_i $$\n\nThe partial derivative with respect to $F_0$ is:\n$$ \\frac{\\partial S}{\\partial F_0} = \\sum_{i=1}^{5} 2(F_i - kx_i - F_0)(-1) = -2 \\left( \\sum_{i=1}^{5} F_i - k \\sum_{i=1}^{5} x_i - F_0 \\sum_{i=1}^{5} 1 \\right) = 0 $$\nThis simplifies to the second normal equation, where $n=5$ is the number of data points:\n$$ k \\left( \\sum_{i=1}^{5} x_i \\right) + n F_0 = \\sum_{i=1}^{5} F_i $$\n\nNow, we calculate the required sums from the given data:\n$(x_1, F_1) = (0.10, 16.0)$, $(x_2, F_2) = (0.20, 30.0)$, $(x_3, F_3) = (0.30, 46.0)$, $(x_4, F_4) = (0.40, 61.5)$, $(x_5, F_5) = (0.50, 74.0)$.\n\n$$ \\sum_{i=1}^{5} x_i = 0.10 + 0.20 + 0.30 + 0.40 + 0.50 = 1.50 $$\n$$ \\sum_{i=1}^{5} F_i = 16.0 + 30.0 + 46.0 + 61.5 + 74.0 = 227.5 $$\n$$ \\sum_{i=1}^{5} x_i^2 = (0.10)^2 + (0.20)^2 + (0.30)^2 + (0.40)^2 + (0.50)^2 = 0.01 + 0.04 + 0.09 + 0.16 + 0.25 = 0.55 $$\n$$ \\sum_{i=1}^{5} x_i F_i = (0.10)(16.0) + (0.20)(30.0) + (0.30)(46.0) + (0.40)(61.5) + (0.50)(74.0) $$\n$$ \\sum_{i=1}^{5} x_i F_i = 1.6 + 6.0 + 13.8 + 24.6 + 37.0 = 83.0 $$\n\nThe system of normal equations in matrix form is:\n$$ \\begin{pmatrix} \\sum x_i^2  \\sum x_i \\\\ \\sum x_i  n \\end{pmatrix} \\begin{pmatrix} k \\\\ F_0 \\end{pmatrix} = \\begin{pmatrix} \\sum x_i F_i \\\\ \\sum F_i \\end{pmatrix} $$\n\nSubstituting the calculated values:\n$$ \\begin{pmatrix} 0.55  1.50 \\\\ 1.50  5 \\end{pmatrix} \\begin{pmatrix} k \\\\ F_0 \\end{pmatrix} = \\begin{pmatrix} 83.0 \\\\ 227.5 \\end{pmatrix} $$\n\nWe can solve this 2x2 system for $k$ and $F_0$. We'll use the matrix inversion method. Let the matrix be $M$.\n$$ M = \\begin{pmatrix} 0.55  1.50 \\\\ 1.50  5 \\end{pmatrix} $$\nThe determinant of $M$ is:\n$$ \\det(M) = (0.55)(5) - (1.50)(1.50) = 2.75 - 2.25 = 0.50 $$\nThe inverse of $M$ is:\n$$ M^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} 5  -1.50 \\\\ -1.50  0.55 \\end{pmatrix} = \\frac{1}{0.50} \\begin{pmatrix} 5  -1.50 \\\\ -1.50  0.55 \\end{pmatrix} = \\begin{pmatrix} 10  -3 \\\\ -3  1.1 \\end{pmatrix} $$\n\nNow, solve for the vector of coefficients:\n$$ \\begin{pmatrix} k \\\\ F_0 \\end{pmatrix} = M^{-1} \\begin{pmatrix} 83.0 \\\\ 227.5 \\end{pmatrix} = \\begin{pmatrix} 10  -3 \\\\ -3  1.1 \\end{pmatrix} \\begin{pmatrix} 83.0 \\\\ 227.5 \\end{pmatrix} $$\n\nCalculate $k$:\n$$ k = (10)(83.0) - (3)(227.5) = 830 - 682.5 = 147.5 $$\n\nCalculate $F_0$:\n$$ F_0 = (-3)(83.0) + (1.1)(227.5) = -249 + 250.25 = 1.25 $$\n\nSo, the spring constant is $k = 147.5$ N/m and the force offset is $F_0 = 1.25$ N.\n\nThe problem requires rounding both values to three significant figures.\n$k = 147.5$ rounds to $148$.\n$F_0 = 1.25$ is already at three significant figures.\n\nThe final answers are $k = 148$ N/m and $F_0 = 1.25$ N.",
            "answer": "$$\\boxed{\\begin{pmatrix} 148  1.25 \\end{pmatrix}}$$"
        },
        {
            "introduction": "What happens when our chosen model doesn't match the underlying pattern in the data? This exercise explores this crucial question by tasking you with fitting a quadratic polynomial to data generated by a periodic function . This practice highlights the concept of model mismatch and demonstrates how the calculated sum of squared residuals serves as a quantitative measure of how well the chosen model class can capture the data's structure.",
            "id": "2194121",
            "problem": "An engineer is analyzing sensor data from a simple harmonic oscillator. Over a specific interval, the measured displacement $y$ at time $x$ is recorded. The five data points $(x_i, y_i)$ collected are: $(0, 0)$, $(0.5, 1)$, $(1, 0)$, $(1.5, -1)$, and $(2, 0)$. In an attempt to create a simple empirical model, the engineer decides to fit a quadratic polynomial of the form $P(x) = c_0 + c_1 x + c_2 x^2$ to this data using the method of least squares. Determine the minimum possible value for the sum of squared residuals, defined as $S = \\sum_{i=1}^{5} (y_i - P(x_i))^2$. Round your final answer to three significant figures.",
            "solution": "We are asked to fit a polynomial $P(x) = c_0 + c_1 x + c_2 x^2$ to the data $(x_i, y_i)$ for $i=1, \\dots, 5$ using least squares. The normal equations are given by $(A^T A) \\mathbf{c} = A^T \\mathbf{y}$, which in terms of sums become:\n$$\n\\begin{aligned}\nS_0 c_0 + S_1 c_1 + S_2 c_2 = T_0 \\\\\nS_1 c_0 + S_2 c_1 + S_3 c_2 = T_1 \\\\\nS_2 c_0 + S_3 c_1 + S_4 c_2 = T_2\n\\end{aligned}\n$$\nwhere $S_k = \\sum_{i=1}^{5} x_i^k$ and $T_k = \\sum_{i=1}^{5} x_i^k y_i$. For the data $x \\in \\{0, 0.5, 1, 1.5, 2\\}$ and $y \\in \\{0, 1, 0, -1, 0\\}$, we compute the sums:\n*   $S_0 = 5$\n*   $S_1 = 0 + 0.5 + 1 + 1.5 + 2 = 5$\n*   $S_2 = 0 + 0.25 + 1 + 2.25 + 4 = 7.5 = \\frac{15}{2}$\n*   $S_3 = 0 + 0.125 + 1 + 3.375 + 8 = 12.5 = \\frac{25}{2}$\n*   $S_4 = 0 + 0.0625 + 1 + 5.0625 + 16 = 22.125 = \\frac{177}{8}$\n*   $T_0 = 0 + 1 + 0 - 1 + 0 = 0$\n*   $T_1 = 0(0) + 0.5(1) + 1(0) + 1.5(-1) + 2(0) = -1$\n*   $T_2 = 0^2(0) + 0.5^2(1) + 1^2(0) + 1.5^2(-1) + 2^2(0) = 0.25 - 2.25 = -2$\n\nThe system of equations is:\n$$\n\\begin{bmatrix}\n5  5  \\frac{15}{2}\\\\\n5  \\frac{15}{2}  \\frac{25}{2}\\\\\n\\frac{15}{2}  \\frac{25}{2}  \\frac{177}{8}\n\\end{bmatrix}\n\\begin{bmatrix}c_{0}\\\\ c_{1}\\\\ c_{2}\\end{bmatrix}\n=\n\\begin{bmatrix}0\\\\ -1\\\\ -2\\end{bmatrix}\n$$\nSolving this system yields $c_2=0$, $c_1=-\\frac{2}{5}$, and $c_0=\\frac{2}{5}$. Therefore, the least-squares fit is the linear polynomial $P(x) = \\frac{2}{5} - \\frac{2}{5}x$.\n\nThe minimum sum of squared residuals is $S_{\\min} = \\sum_{i=1}^{5} (y_i - P(x_i))^2$. We evaluate the residuals $r_i = y_i - P(x_i)$ at the five data points:\n*   For $x=0$: $r = 0 - \\frac{2}{5} = -\\frac{2}{5}$, so $r^2 = \\frac{4}{25}$.\n*   For $x=0.5$: $r = 1 - (\\frac{2}{5} - \\frac{1}{5}) = 1 - \\frac{1}{5} = \\frac{4}{5}$, so $r^2 = \\frac{16}{25}$.\n*   For $x=1$: $r = 0 - (\\frac{2}{5} - \\frac{2}{5}) = 0$, so $r^2 = 0$.\n*   For $x=1.5$: $r = -1 - (\\frac{2}{5} - \\frac{3}{5}) = -1 - (-\\frac{1}{5}) = -\\frac{4}{5}$, so $r^2 = \\frac{16}{25}$.\n*   For $x=2$: $r = 0 - (\\frac{2}{5} - \\frac{4}{5}) = 0 - (-\\frac{2}{5}) = \\frac{2}{5}$, so $r^2 = \\frac{4}{25}$.\n\nSumming the squared residuals:\n$$\nS_{\\min} = \\frac{4}{25} + \\frac{16}{25} + 0 + \\frac{16}{25} + \\frac{4}{25} = \\frac{40}{25} = \\frac{8}{5} = 1.6\n$$\nRounded to three significant figures, the result is $1.60$.",
            "answer": "$$\\boxed{1.60}$$"
        },
        {
            "introduction": "Fitting data is one thing, but building a model that makes accurate predictions is another. This final practice introduces a core technique from modern data science: using a validation set to select the optimal model complexity and prevent the common pitfall of overfitting . By comparing models of different degrees, you will learn a systematic process for choosing a polynomial that generalizes well to new, unseen data.",
            "id": "2194119",
            "problem": "An engineering team is characterizing a new passive electronic component whose resistance is a function of an external control parameter. To build a predictive model for this behavior, they have collected a set of experimental data points, where $x$ is the value of the control parameter (in arbitrary units) and $y$ is the measured resistance (in Ohms).\n\nThe team wishes to model the relationship using a polynomial function $P_d(x) = \\sum_{j=0}^{d} c_j x^j$, where $d$ is the degree of the polynomial. To avoid overfitting, they have split their data into a training set and a validation set. The model coefficients $c_j$ for a given degree $d$ are to be determined by performing a least-squares fit on the training data. The optimal degree $d$ is then selected as the one that minimizes the prediction error on the separate validation data.\n\nThe error is quantified using the Root Mean Square Error (RMSE), defined as $E_{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$, where $y_i$ are the true measured values and $\\hat{y}_i$ are the values predicted by the model for $n$ data points.\n\nThe collected data is as follows:\n- **Training Set**: $(0, 5.6)$, $(1, 3.1)$, $(2, 1.4)$, $(4, 1.6)$, $(5, 2.9)$, $(6, 5.7)$\n- **Validation Set**: $(0.5, 4.2)$, $(2.5, 1.0)$, $(3.5, 1.2)$, $(5.5, 4.0)$\n\nYour task is to determine the optimal polynomial degree $d$ from the set of candidates $\\{1, 2, 3\\}$ that best models the component's behavior by minimizing the RMSE on the validation set.\n\nWhich of the following is the optimal polynomial degree?\nA. $d=1$\n\nB. $d=2$\n\nC. $d=3$\n\nD. All three degrees result in validation RMSE values that are equal to within two significant figures.",
            "solution": "We model $y$ by a polynomial $P_d(x) = \\sum_{j=0}^{d} c_j x^j$ fit by least squares on the training set, and we select $d \\in \\{1, 2, 3\\}$ that minimizes the validation RMSE. Because the square root function is strictly increasing, minimizing RMSE is equivalent to minimizing the Mean Squared Error (MSE), which is the average of squared residuals on the validation set.\n\nThe training data consists of 6 points, and the validation data consists of 4 points. All calculations are performed using exact rational arithmetic to avoid floating-point errors.\n\n**Degree $d=1$ (linear):**\nWe fit $P_1(x) = c_0 + c_1 x$. The coefficients are found using the normal equations for linear regression. For the 6 training points, we calculate the necessary sums: $\\sum x_i = 18$, $\\sum y_i = 20.3$, $\\sum x_i^2 = 82$, $\\sum x_i y_i = 61$.\nSolving for the coefficients gives $c_1 = \\frac{1}{280}$ and $c_0 = \\frac{2833}{840}$.\nSo, $P_1(x) = \\frac{2833}{840} + \\frac{1}{280}x$.\nWe then calculate the sum of squared residuals on the 4 validation points. The resulting MSE is:\n$$ \\text{MSE}_1 \\approx 2.874 $$\n\n**Degree $d=2$ (quadratic):**\nWe fit $P_2(x) = c_0 + c_1 x + c_2 x^2$. The $3 \\times 3$ system of normal equations is solved using the training data. This yields the coefficients:\n$c_2 = \\frac{1019}{1960}$, $c_1 = -\\frac{6107}{1960}$, and $c_0 = \\frac{5513}{980}$.\nThe fitted polynomial is $P_2(x) = \\frac{5513}{980} - \\frac{6107}{1960}x + \\frac{1019}{1960}x^2$.\nEvaluating this model on the validation set gives an MSE of:\n$$ \\text{MSE}_2 \\approx 0.0165 $$\n\n**Degree $d=3$ (cubic):**\nWe fit $P_3(x) = c_0 + c_1 x + c_2 x^2 + c_3 x^3$. The $4 \\times 4$ system of normal equations for the training data is solved. The coefficients are:\n$c_3 = \\frac{1}{360}$, $c_2 = \\frac{97}{196}$, $c_1 = -\\frac{53983}{17640}$, and $c_0 = \\frac{1649}{294}$.\nThe fitted polynomial is $P_3(x) = \\frac{1649}{294} - \\frac{53983}{17640}x + \\frac{97}{196}x^2 + \\frac{1}{360}x^3$.\nEvaluating this model on the validation set gives an MSE of:\n$$ \\text{MSE}_3 \\approx 0.0169 $$\n\n**Comparison:**\nWe compare the validation MSE values for each degree:\n*   $\\text{MSE}_1 \\approx 2.874$\n*   $\\text{MSE}_2 \\approx 0.0165$\n*   $\\text{MSE}_3 \\approx 0.0169$\n\nThe lowest validation MSE is achieved with the degree-2 polynomial. Since minimizing MSE is equivalent to minimizing RMSE, the optimal polynomial degree is $d=2$.\n\nConsequently, the optimal degree is $d=2$, corresponding to option B.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}