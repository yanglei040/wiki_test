{
    "hands_on_practices": [
        {
            "introduction": "To effectively use the Hessian matrix, one must first be fluent in its calculation. This initial practice focuses on a fundamental function in optimization and machine learning: the squared Euclidean norm, $f(x) = \\frac{1}{2}\\|x\\|_2^2$. Calculating the Hessian for this function provides a baseline understanding of curvature and is an excellent warm-up for more complex analyses .",
            "id": "2198466",
            "problem": "In the context of numerical optimization and machine learning, the analysis of a function's local curvature is crucial for designing efficient algorithms like Newton's method. This curvature is captured by the Hessian matrix.\n\nConsider a function $f: \\mathbb{R}^n \\to \\mathbb{R}$ which represents a simplified regularization term in a model. This function is defined as the squared Euclidean norm of a vector $x \\in \\mathbb{R}^n$, scaled by a constant factor:\n$$f(x) = \\frac{1}{2}\\|x\\|_2^2$$\nwhere $\\|x\\|_2 = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2}$ is the standard Euclidean norm.\n\nThe Hessian matrix of $f$, denoted by $\\nabla^2 f(x)$, is an $n \\times n$ matrix whose entry at the $j$-th row and $k$-th column is given by the second-order partial derivative $\\frac{\\partial^2 f}{\\partial x_j \\partial x_k}$.\n\nCalculate the Hessian matrix of the function $f(x)$ for an arbitrary vector $x \\in \\mathbb{R}^n$. Express your final answer in terms of the $n \\times n$ identity matrix, denoted by $I_n$.",
            "solution": "We start from the definition of the function as a sum of squares:\n$$\nf(x) = \\frac{1}{2}\\|x\\|_{2}^{2} = \\frac{1}{2}\\sum_{i=1}^{n} x_{i}^{2}.\n$$\nTo compute the Hessian, we first find the gradient. For each coordinate $x_{j}$, the partial derivative is\n$$\n\\frac{\\partial f}{\\partial x_{j}} = \\frac{1}{2}\\cdot 2 x_{j} = x_{j}.\n$$\nCollecting these components, the gradient is\n$$\n\\nabla f(x) = \\begin{pmatrix} x_{1} \\\\ \\vdots \\\\ x_{n} \\end{pmatrix} = x.\n$$\nNext, we compute the second-order partial derivatives. For the Hessian entry at row $j$ and column $k$,\n$$\n\\frac{\\partial^{2} f}{\\partial x_{j}\\,\\partial x_{k}} = \\frac{\\partial}{\\partial x_{k}}\\left(\\frac{\\partial f}{\\partial x_{j}}\\right) = \\frac{\\partial}{\\partial x_{k}}(x_{j}) = \n\\begin{cases}\n1, & \\text{if } j = k, \\\\\n0, & \\text{if } j \\neq k,\n\\end{cases}\n$$\nwhich can be written as $\\delta_{jk}$, the Kronecker delta. Therefore, the Hessian matrix has $1$ on the diagonal and $0$ off-diagonal, which is exactly the $n \\times n$ identity matrix. Hence,\n$$\n\\nabla^{2} f(x) = I_{n}.\n$$\nThis matrix is constant and does not depend on $x$.",
            "answer": "$$\\boxed{I_{n}}$$"
        },
        {
            "introduction": "The true power of the Hessian lies in its ability to reveal the local geometry of a function's surface, acting as a \"convexity detector\". This exercise challenges you to apply this concept by identifying regions where a function is not locally convex, which can pose difficulties for optimization algorithms. By analyzing the Hessian's definiteness, you will delineate the boundary of a non-convex region and solve a related geometric problem, linking abstract matrix properties to tangible spatial characteristics .",
            "id": "2198489",
            "problem": "In numerical optimization, the local geometry of an objective function's surface is crucial for the performance of minimization algorithms. A function is considered 'locally convex' at a point if, in a small neighborhood around that point, its surface is uniformly shaped like a bowl opening upwards. Regions where a function is not locally convex (for example, regions with a saddle-like shape) can pose significant challenges for optimization routines trying to find a global minimum.\n\nThe local convexity of a sufficiently smooth multivariable function can be determined by analyzing its second partial derivatives at a given point.\n\nConsider a potential energy surface described by the function $f(x,y) = x^4 + y^4 - 4xy + 1$. The set of all points $(x,y)$ where this function is *not* locally convex forms a specific region in the $xy$-plane.\n\nDetermine the minimum distance from the origin $(0,0)$ to any point on the boundary of this non-convex region. Express your answer as a single symbolic expression.",
            "solution": "To determine local convexity, use the Hessian criterion: for a twice continuously differentiable function of two variables, the function is locally convex at a point if and only if the Hessian matrix at that point is positive semidefinite. For a symmetric $2 \\times 2$ Hessian $H = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}$, positive semidefiniteness requires the leading principal minors to satisfy $a \\geq 0$ and $\\det(H) \\geq 0$.\n\nGiven $f(x,y) = x^{4} + y^{4} - 4xy + 1$, compute second derivatives:\n$$\nf_{xx} = 12x^{2}, \\quad f_{yy} = 12y^{2}, \\quad f_{xy} = -4.\n$$\nThus the Hessian is\n$$\nH(x,y) = \\begin{pmatrix} 12x^{2} & -4 \\\\ -4 & 12y^{2} \\end{pmatrix}.\n$$\nHere $12x^{2} \\geq 0$ always. The determinant is\n$$\n\\det(H) = (12x^{2})(12y^{2}) - (-4)^{2} = 144x^{2}y^{2} - 16 = 16\\left(9x^{2}y^{2} - 1\\right).\n$$\nTherefore, the region where the function is not locally convex is where $\\det(H) < 0$, i.e.,\n$$\n9x^{2}y^{2} - 1 < 0 \\quad \\Longleftrightarrow \\quad |xy| < \\frac{1}{3}.\n$$\nThe boundary of this non-convex region is given by\n$$\n9x^{2}y^{2} - 1 = 0 \\quad \\Longleftrightarrow \\quad |xy| = \\frac{1}{3}.\n$$\n\nWe now minimize the Euclidean distance from the origin to this boundary. This is equivalent to minimizing $x^{2} + y^{2}$ subject to the constraint $xy = s$, where $s = \\pm \\frac{1}{3}$. Use Lagrange multipliers. Define $S = x^{2} + y^{2}$ and $g(x,y) = xy - s = 0$. The stationarity conditions are\n$$\n\\frac{\\partial}{\\partial x}\\left(x^{2} + y^{2} - \\lambda(xy - s)\\right) = 2x - \\lambda y = 0, \\quad \\frac{\\partial}{\\partial y}\\left(x^{2} + y^{2} - \\lambda(xy - s)\\right) = 2y - \\lambda x = 0,\n$$\ntogether with $xy = s$. From $2x = \\lambda y$ and $2y = \\lambda x$, eliminating $\\lambda$ yields $x^{2} = y^{2}$, so $|x| = |y|$. Combining with $xy = s$, we have:\n- If $s = \\frac{1}{3}$, then $y = x$ and $x^{2} = \\frac{1}{3}$.\n- If $s = -\\frac{1}{3}$, then $y = -x$ and $x^{2} = \\frac{1}{3}$.\nIn both cases,\n$$\nx^{2} + y^{2} = 2 \\cdot \\frac{1}{3} = \\frac{2}{3}.\n$$\nHence the minimal squared distance is $\\frac{2}{3}$, and the minimal distance from the origin to the boundary is\n$$\n\\sqrt{\\frac{2}{3}}.\n$$\nThis is achieved at points $\\left(\\pm \\frac{1}{\\sqrt{3}}, \\pm \\frac{1}{\\sqrt{3}}\\right)$ for $xy = \\frac{1}{3}$ and $\\left(\\pm \\frac{1}{\\sqrt{3}}, \\mp \\frac{1}{\\sqrt{3}}\\right)$ for $xy = -\\frac{1}{3}$.",
            "answer": "$$\\boxed{\\sqrt{\\frac{2}{3}}}$$"
        },
        {
            "introduction": "This final practice bridges theory and application, demonstrating how the Hessian's structure directly governs the performance of optimization algorithms. You will analyze the convergence of the gradient descent method on a model quadratic problem, a cornerstone of optimization analysis. By deriving the convergence rate, you will establish a quantitative link between the Hessian's condition number, $\\kappa$, and the algorithm's speed, offering deep insight into why some problems are inherently harder to solve than others .",
            "id": "2198464",
            "problem": "Consider the problem of minimizing the strongly convex quadratic function $f(x) = \\frac{1}{2}x^T Q x - b^T x$, where $x, b \\in \\mathbb{R}^n$ and $Q$ is an $n \\times n$ symmetric positive definite matrix. The minimum of this function is achieved at a unique point $x^*$.\n\nWe apply the gradient descent method to find this minimum, using the iterative update rule $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$, where $k$ is the iteration number and $\\alpha$ is a constant step size. Let $\\lambda_{\\min}$ and $\\lambda_{\\max}$ be the smallest and largest eigenvalues of the matrix $Q$, respectively. For this problem, we use the specific fixed step size $\\alpha = \\frac{2}{\\lambda_{\\min} + \\lambda_{\\max}}$.\n\nThe Euclidean norm of the error, $\\|x_k - x^*\\|_2$, is known to decrease at each step. The convergence is linear, meaning there exists a constant $\\rho \\in [0, 1)$ such that $\\|x_{k+1} - x^*\\|_2 \\le \\rho \\|x_k - x^*\\|_2$ for all $k \\ge 0$.\n\nYour task is to determine the tightest possible value for this convergence factor $\\rho$. Express your final answer solely in terms of the condition number $\\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$ of the matrix $Q$.",
            "solution": "The objective function is $f(x) = \\frac{1}{2}x^T Q x - b^T x$. Its gradient is given by $\\nabla f(x) = Qx - b$.\nThe optimal solution $x^*$ is found by setting the gradient to zero: $\\nabla f(x^*) = Qx^* - b = 0$, which implies $b = Qx^*$.\n\nThe gradient descent update rule is $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$.\nSubstituting the expression for the gradient, we get:\n$x_{k+1} = x_k - \\alpha (Qx_k - b)$.\n\nLet the error at iteration $k$ be defined as $e_k = x_k - x^*$. We want to find a recursive relation for the error.\n$e_{k+1} = x_{k+1} - x^* = (x_k - \\alpha (Qx_k - b)) - x^*$.\nSubstitute $b = Qx^*$:\n$e_{k+1} = x_k - \\alpha (Qx_k - Qx^*) - x^* = (x_k - x^*) - \\alpha Q(x_k - x^*)$.\nThis gives the error update rule:\n$e_{k+1} = (I - \\alpha Q)e_k$.\n\nTo analyze the convergence in the Euclidean norm, we take the 2-norm of both sides:\n$\\|e_{k+1}\\|_2 = \\|(I - \\alpha Q)e_k\\|_2$.\nUsing the property of matrix norms, $\\|Ax\\|_2 \\le \\|A\\|_2 \\|x\\|_2$, where $\\|A\\|_2$ is the spectral norm of $A$, we get:\n$\\|e_{k+1}\\|_2 \\le \\|I - \\alpha Q\\|_2 \\|e_k\\|_2$.\nThe convergence factor $\\rho$ is therefore the spectral norm of the iteration matrix $G = I - \\alpha Q$.\n$\\rho = \\|I - \\alpha Q\\|_2$.\n\nSince $Q$ is a symmetric matrix, the matrix $G = I - \\alpha Q$ is also symmetric. The spectral norm of a symmetric matrix is the maximum of the absolute values of its eigenvalues.\nLet $\\lambda_i$ for $i=1, \\dots, n$ be the eigenvalues of $Q$. Since $Q$ is positive definite, $0 < \\lambda_{\\min} \\le \\lambda_i \\le \\lambda_{\\max}$ for all $i$.\nThe eigenvalues of $G = I - \\alpha Q$ are $\\mu_i = 1 - \\alpha \\lambda_i$.\nSo, the convergence factor is:\n$\\rho = \\|G\\|_2 = \\max_{i} |\\mu_i| = \\max_{i} |1 - \\alpha \\lambda_i|$.\n\nThe function $g(\\lambda) = |1 - \\alpha \\lambda|$ is maximized over the interval $[\\lambda_{\\min}, \\lambda_{\\max}]$ at one of the endpoints. Thus,\n$\\rho = \\max(|1 - \\alpha \\lambda_{\\min}|, |1 - \\alpha \\lambda_{\\max}|)$.\n\nThe problem specifies the step size $\\alpha = \\frac{2}{\\lambda_{\\min} + \\lambda_{\\max}}$. We substitute this value into the expression for $\\rho$.\nLet's evaluate the two terms inside the maximum function.\nFirst term:\n$1 - \\alpha \\lambda_{\\min} = 1 - \\frac{2\\lambda_{\\min}}{\\lambda_{\\min} + \\lambda_{\\max}} = \\frac{(\\lambda_{\\min} + \\lambda_{\\max}) - 2\\lambda_{\\min}}{\\lambda_{\\min} + \\lambda_{\\max}} = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}}$.\nSecond term:\n$1 - \\alpha \\lambda_{\\max} = 1 - \\frac{2\\lambda_{\\max}}{\\lambda_{\\min} + \\lambda_{\\max}} = \\frac{(\\lambda_{\\min} + \\lambda_{\\max}) - 2\\lambda_{\\max}}{\\lambda_{\\min} + \\lambda_{\\max}} = \\frac{\\lambda_{\\min} - \\lambda_{\\max}}{\\lambda_{\\min} + \\lambda_{\\max}} = - \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}}$.\n\nThe absolute values of these two terms are equal:\n$|1 - \\alpha \\lambda_{\\min}| = |1 - \\alpha \\lambda_{\\max}| = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}}$.\nTherefore, the convergence factor is:\n$\\rho = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}}$.\n\nFinally, we need to express this in terms of the condition number $\\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$. To do this, we divide both the numerator and the denominator of the expression for $\\rho$ by $\\lambda_{\\min}$ (which is strictly positive):\n$\\rho = \\frac{\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} - \\frac{\\lambda_{\\min}}{\\lambda_{\\min}}}{\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} + \\frac{\\lambda_{\\min}}{\\lambda_{\\min}}} = \\frac{\\kappa - 1}{\\kappa + 1}$.\n\nThis is the tightest possible value for the convergence factor $\\rho$ for the given step size.",
            "answer": "$$\\boxed{\\frac{\\kappa - 1}{\\kappa + 1}}$$"
        }
    ]
}