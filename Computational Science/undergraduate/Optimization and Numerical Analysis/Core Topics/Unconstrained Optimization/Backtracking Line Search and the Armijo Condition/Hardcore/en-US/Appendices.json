{
    "hands_on_practices": [
        {
            "introduction": "The foundation of the backtracking line search is the Armijo condition for sufficient decrease. Before implementing the full iterative algorithm, it is crucial to be able to verify this condition for a given step size. This first exercise provides a straightforward scenario to practice the core calculation, ensuring you can correctly apply the inequality that governs the entire process. ",
            "id": "2154929",
            "problem": "In the field of robotics, an engineer is fine-tuning a parameter $x$ that controls the stability of a walking gait. The instability of the gait is quantified by a cost function $J(x) = 5(x-100)^2$, where $x$ is a dimensionless parameter. The goal is to find the value of $x$ that minimizes $J(x)$. The optimization process starts at an initial guess of $x_k = 101$.\n\nA gradient descent algorithm is employed. At $x_k=101$, the algorithm computes a search direction $p_k = -1$. To determine how far to step in this direction, a backtracking line search is used with an initial trial step size of $\\alpha = 0.1$. A step is considered acceptable if it satisfies the Armijo condition for sufficient decrease:\n$$J(x_k + \\alpha p_k) \\le J(x_k) + c_1 \\alpha \\nabla J(x_k) \\cdot p_k$$\nThe constant for this criterion is set to $c_1 = 0.5$.\n\nBased on these values, which of the following statements is correct?\n\nA. The Armijo condition is not satisfied, so the step size $\\alpha$ must be decreased.\n\nB. The Armijo condition is satisfied, and the new cost function value is $J(x_{k+1}) = 4.05$.\n\nC. The Armijo condition is satisfied, and the new cost function value is $J(x_{k+1}) = 4.50$.\n\nD. The Armijo condition is not satisfied, but the cost function value still decreases.\n\nE. The search direction $p_k=-1$ is not a descent direction at $x_k=101$.",
            "solution": "We are given the cost function $J(x) = 5(x-100)^2$, the current point $x_k = 101$, the search direction $p_k = -1$, the initial trial step size $\\alpha = 0.1$, and the Armijo condition\n$$\nJ(x_k + \\alpha p_k) \\le J(x_k) + c_1\\alpha \\nabla J(x_k) \\cdot p_k,\n$$\nwith $c_1 = 0.5$.\n\nFirst, compute the gradient of $J(x)$. Since $J(x) = 5(x-100)^2$, by differentiation we have\n$$\n\\nabla J(x) = \\frac{d}{dx} \\left[5(x-100)^2\\right] = 10(x-100).\n$$\nAt $x_k = 101$, this gives\n$$\n\\nabla J(x_k) = 10(101 - 100) = 10.\n$$\nCheck that $p_k$ is a descent direction:\n$$\n\\nabla J(x_k) \\cdot p_k = 10 \\cdot (-1) = -10  0,\n$$\nso $p_k$ is a descent direction, ruling out option E.\n\nEvaluate the left-hand side of the Armijo condition:\nThe new point is $x_k + \\alpha p_k = 101 + 0.1(-1) = 100.9$. The function value at this point is\n$$\nJ(x_k + \\alpha p_k) = J(100.9) = 5(100.9 - 100)^2 = 5(0.9)^2 = 5 \\cdot 0.81 = 4.05.\n$$\n\nEvaluate the right-hand side of the Armijo condition:\nThe current function value is $J(x_k) = 5(101 - 100)^2 = 5(1)^2 = 5$. The right-hand side is\n$$\nJ(x_k) + c_1\\alpha \\nabla J(x_k) \\cdot p_k = 5 + (0.5)(0.1)(-10) = 5 - 0.5 = 4.5.\n$$\n\nCompare both sides:\n$$\n4.05 \\le 4.5\n$$\nThe inequality holds. Therefore, the Armijo condition is satisfied with the trial step size $\\alpha = 0.1$, and the new cost function value is $J(x_{k+1}) = 4.05$.\n\nThis confirms that option B is correct. Options A and D are false because the Armijo condition is satisfied; option C gives an incorrect new cost value; and option E is false because $p_k$ is a descent direction.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "A single check of the Armijo condition is just one part of the process. The real power of backtracking comes from its iterative nature of reducing the step size until the condition is met. This next practice guides you through a complete backtracking search, starting with an initial guess for the step size and systematically reducing it until an acceptable step is found, demonstrating the full algorithm in action. ",
            "id": "2154887",
            "problem": "In the field of numerical optimization, the method of steepest descent is an iterative algorithm used to find a local minimum of a function. At each iteration $k$, the next point $x_{k+1}$ is determined from the current point $x_k$ by moving a certain distance along the direction of the negative gradient. The update rule is given by $x_{k+1} = x_k + \\alpha_k p_k$, where $p_k$ is the search direction and $\\alpha_k > 0$ is the step size.\n\nA common strategy to determine an appropriate step size $\\alpha_k$ is the backtracking line search. This procedure starts with an initial guess for the step size, $\\bar{\\alpha}$, and iteratively reduces it until a sufficient decrease in the function value is achieved. This is formalized by the Armijo condition.\n\nConsider the task of minimizing the function $f(x, y) = x^2 + 4y^2$. We are at the point $x_k = (3, 1)$ and wish to find the next iterate using the steepest descent direction. The search direction $p_k$ is therefore given by $p_k = -\\nabla f(x_k)$.\n\nTo find the step size $\\alpha_k$, you will perform a backtracking line search with the following parameters: an initial step size $\\bar{\\alpha} = 1$, a contraction factor $\\rho = 0.5$, and a sufficient decrease constant $c_1 = 0.25$. The process is as follows:\n1. Start with $\\alpha = \\bar{\\alpha}$.\n2. Check if the Armijo condition is satisfied: $f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^T p_k$.\n3. If the condition is met, the accepted step size is $\\alpha_k = \\alpha$.\n4. If the condition is not met, update the step size by setting $\\alpha \\leftarrow \\rho \\alpha$ and repeat from step 2.\n\nDetermine the accepted step size $\\alpha_k$ resulting from this procedure.",
            "solution": "We are minimizing the function $f(x,y)=x^2+4y^2$. Its gradient is $\\nabla f(x,y)=(2x,8y)$. We are at the point $x_k=(3,1)$.\n\nThe gradient at this point is $\\nabla f(x_k)=(6,8)$, so the steepest descent direction is $p_k=-\\nabla f(x_k)=(-6,-8)$.\n\nThe Armijo condition with parameters $c_1=0.25$ and step size $\\alpha>0$ is:\n$$\nf(x_k+\\alpha p_k) \\le f(x_k) + c_1\\alpha\\nabla f(x_k)^T p_k\n$$\nFirst, we compute the terms on the right-hand side:\n$f(x_k)=3^2+4(1)^2=13$.\n$\\nabla f(x_k)^T p_k=(6,8)\\cdot(-6,-8)=-36-64=-100$.\nThe right-hand side of the inequality is $13 + 0.25\\alpha(-100) = 13-25\\alpha$.\n\nNext, we compute the term on the left-hand side. The new point is $x_k+\\alpha p_k=(3-6\\alpha, 1-8\\alpha)$. The function value is:\n$$\nf(x_k+\\alpha p_k)=(3-6\\alpha)^2+4(1-8\\alpha)^2 = (9-36\\alpha+36\\alpha^2)+4(1-16\\alpha+64\\alpha^2) = 13-100\\alpha+292\\alpha^2\n$$\nThe Armijo inequality becomes:\n$$\n13-100\\alpha+292\\alpha^2 \\le 13-25\\alpha\n$$\nThis simplifies to:\n$$\n292\\alpha^2 \\le 75\\alpha\n$$\nFor $\\alpha>0$, this is equivalent to:\n$$\n\\alpha \\le \\frac{75}{292} \\approx 0.2568\n$$\nNow we perform the backtracking line search, starting with $\\bar{\\alpha}=1$ and using $\\rho=0.5$. We test trial values of $\\alpha$ sequentially:\n- **Try $\\alpha=1$**: Since $1 > \\frac{75}{292}$, the condition is not satisfied. We reduce the step size: $\\alpha \\leftarrow 0.5 \\times 1 = 0.5$.\n- **Try $\\alpha=0.5$**: Since $0.5 > \\frac{75}{292}$, the condition is not satisfied. We reduce the step size: $\\alpha \\leftarrow 0.5 \\times 0.5 = 0.25$.\n- **Try $\\alpha=0.25$**: Since $0.25 = \\frac{1}{4}$ and $\\frac{1}{4} \\le \\frac{75}{292}$ (because $292 \\le 4 \\times 75 = 300$), the Armijo condition is satisfied.\n\nTo double-check:\nFor $\\alpha=0.25$, the left-hand side is $f(x_k+\\frac{1}{4}p_k)=13-100(\\frac{1}{4})+292(\\frac{1}{4})^2 = 13-25+\\frac{292}{16} = -12+18.25 = 6.25$.\nThe right-hand side is $13-25(\\frac{1}{4}) = 13-6.25=6.75$.\nThe inequality $6.25 \\le 6.75$ is true.\n\nTherefore, the first value in the sequence to be accepted is $\\alpha_k=0.25$.",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        },
        {
            "introduction": "The parameters used in optimization algorithms, like $c_1$ in the Armijo condition, are not chosen arbitrarily; they have deep theoretical implications for convergence. This final exercise shifts our focus from computation to analysis. By examining the Armijo condition for the important case of a strongly convex quadratic function, you will uncover a fundamental property related to the choice of $c_1$ and its relationship with the exact line search minimum. ",
            "id": "2154908",
            "problem": "In the field of numerical optimization, a backtracking line search is a common method for choosing a step size $\\alpha$ that provides sufficient decrease in an objective function. Consider the task of minimizing a general strongly convex quadratic function of the form $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T Q \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}$, where $\\mathbf{x} \\in \\mathbb{R}^n$, $Q$ is a symmetric positive definite $n \\times n$ matrix, and $\\mathbf{b} \\in \\mathbb{R}^n$.\n\nWe are at a point $\\mathbf{x}_k$ which is not the minimizer, and we use the steepest descent direction for our search, defined as $\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$. A step size $\\alpha > 0$ is considered acceptable if it satisfies the Armijo condition for sufficient decrease:\n$$f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^T \\mathbf{p}_k$$\nwhere the parameter $c_1$ is a constant in the interval $(0, 1)$.\n\nLet $\\alpha^*$ be the unique step size that corresponds to the exact minimum of the function $f$ along the search direction $\\mathbf{p}_k$. This exact step size $\\alpha^*$ is guaranteed to be accepted by the Armijo condition (i.e., satisfy the inequality) for any valid choice of $Q$ and $\\mathbf{b}$ (provided $\\mathbf{x}_k$ is not the true minimizer) if and only if the parameter $c_1$ is less than or equal to a certain threshold value, which we will call $C_{max}$.\n\nDetermine the value of this threshold $C_{max}$.",
            "solution": "Let $g_k = \\nabla f(\\mathbf{x}_k) = Q\\mathbf{x}_k - \\mathbf{b}$ and choose the steepest descent direction $\\mathbf{p}_k = -g_k$. Consider the univariate function along the line, $\\phi(\\alpha) = f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$. For a quadratic function, the Taylor expansion is exact:\n$$\n\\phi(\\alpha) = f(\\mathbf{x}_k) + \\alpha g_k^T\\mathbf{p}_k + \\frac{1}{2}\\alpha^2\\mathbf{p}_k^T Q \\mathbf{p}_k\n$$\nTo find the exact minimizer $\\alpha^*$ along the line, we differentiate with respect to $\\alpha$ and set the result to zero:\n$$\n\\phi'(\\alpha) = g_k^T\\mathbf{p}_k + \\alpha \\mathbf{p}_k^T Q \\mathbf{p}_k\n$$\nSetting $\\phi'(\\alpha^*) = 0$ gives:\n$$\n\\alpha^* = -\\frac{g_k^T\\mathbf{p}_k}{\\mathbf{p}_k^T Q \\mathbf{p}_k}\n$$\nSince $\\mathbf{p}_k = -g_k$, we have $g_k^T\\mathbf{p}_k = -\\|g_k\\|^2  0$ and $\\mathbf{p}_k^T Q \\mathbf{p}_k = g_k^T Q g_k > 0$ (as $Q$ is positive definite and we assume $g_k \\ne \\mathbf{0}$). Thus, $\\alpha^* > 0$.\n\nThe function value at this minimum is:\n$$\n\\phi(\\alpha^*) = f(\\mathbf{x}_k) + \\alpha^* g_k^T\\mathbf{p}_k + \\frac{1}{2}(\\alpha^*)^2 \\mathbf{p}_k^T Q \\mathbf{p}_k = f(\\mathbf{x}_k) + \\alpha^* g_k^T\\mathbf{p}_k + \\frac{1}{2}\\alpha^*\\left(-\\frac{g_k^T\\mathbf{p}_k}{\\mathbf{p}_k^T Q \\mathbf{p}_k}\\right) \\mathbf{p}_k^T Q \\mathbf{p}_k = f(\\mathbf{x}_k) + \\frac{1}{2}\\alpha^* g_k^T\\mathbf{p}_k\n$$\nThe Armijo condition at $\\alpha = \\alpha^*$ requires:\n$$\n\\phi(\\alpha^*) \\le f(\\mathbf{x}_k) + c_1\\alpha^* g_k^T\\mathbf{p}_k\n$$\nSubstituting our expression for $\\phi(\\alpha^*)$:\n$$\nf(\\mathbf{x}_k) + \\frac{1}{2}\\alpha^* g_k^T\\mathbf{p}_k \\le f(\\mathbf{x}_k) + c_1\\alpha^* g_k^T\\mathbf{p}_k\n$$\nWe can cancel $f(\\mathbf{x}_k)$ from both sides. Since we are at a point that is not the minimizer, $\\alpha^* > 0$ and $g_k^T\\mathbf{p}_k  0$. Thus, we can divide by the negative quantity $\\alpha^* g_k^T\\mathbf{p}_k$, which reverses the inequality:\n$$\n\\frac{1}{2} \\ge c_1\n$$\nThis shows that the exact step size $\\alpha^*$ satisfies the Armijo condition if and only if $c_1 \\le \\frac{1}{2}$. This condition is independent of the specific quadratic function (i.e., independent of $Q$ and $\\mathbf{b}$), as long as it is strongly convex and we are not at the minimum.\n\nTherefore, the maximum threshold value for $c_1$ is $C_{\\max} = \\frac{1}{2}$.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        }
    ]
}