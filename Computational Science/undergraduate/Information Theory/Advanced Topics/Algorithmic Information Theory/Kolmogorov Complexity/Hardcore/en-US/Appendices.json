{
    "hands_on_practices": [
        {
            "introduction": "Before diving into complex calculations, it's crucial to grasp a foundational and somewhat counter-intuitive principle of algorithmic information theory: most strings are not compressible. This exercise demonstrates, through a straightforward counting argument, that the set of \"simple\" or compressible strings is vastly smaller than the set of all possible strings. By quantifying this, you will solidify your understanding of why random strings are the norm, and why their Kolmogorov complexity is approximately equal to their length .",
            "id": "1635770",
            "problem": "In the theoretical study of data compression, we analyze the properties of binary strings. Consider the set of all possible binary strings of length $n=128$. The Kolmogorov complexity of a string $s$, denoted $K(s)$, is defined as the length in bits of the shortest computer program that can generate $s$ as output and then halt. This value represents the ultimate limit of lossless compression for that string.\n\nWe can define a string $s$ as \"compressible\" if its Kolmogorov complexity is smaller than its length. For this problem, we will define a string $s$ of length $n=128$ as \"highly compressible\" if its complexity $K(s)$ satisfies the inequality $K(s) \\leq 118$.\n\nBased on the founding principle that for any given integer length $k$, there can be at most $2^k$ distinct programs of that length, calculate the minimum possible fraction of binary strings of length 128 that are *not* highly compressible. Express your answer as a decimal rounded to five significant figures.",
            "solution": "The goal is to find the minimum fraction of binary strings of length $n=128$ that are not \"highly compressible\". A string $s$ is defined as highly compressible if its Kolmogorov complexity $K(s) \\leq 118$. Therefore, a string is not highly compressible if $K(s)  118$.\n\nFirst, let's determine the total number of distinct binary strings of length $n=128$. Since each of the 128 positions in the string can be either a 0 or a 1, the total number of possible strings is $2^{128}$.\n\nNext, we need to find an upper bound on the number of strings that are highly compressible. A string $s$ is highly compressible if $K(s) \\leq 118$. This means there exists a program of length $k \\leq 118$ bits that generates $s$. We can count the total number of such programs.\n\nA program is itself a binary string. The number of possible programs of a specific length $k$ is $2^k$. To find the total number of programs with a length less than or equal to 118, we sum the number of programs for each possible length from $k=0$ to $k=118$.\nLet $N_{programs}$ be the total number of programs with length $k \\leq 118$.\n$$ N_{programs} = \\sum_{k=0}^{118} 2^k $$\nThis is a geometric series. The sum of a geometric series $\\sum_{i=0}^{m} r^i$ is given by the formula $\\frac{r^{m+1}-1}{r-1}$. For our case, $r=2$ and $m=118$.\n$$ N_{programs} = \\frac{2^{118+1}-1}{2-1} = 2^{119} - 1 $$\nSo, there are at most $2^{119} - 1$ distinct programs with a length of 118 bits or less.\n\nEach of these short programs can generate at most one unique output string. Therefore, the number of strings that can be generated by these programs is also at most $2^{119} - 1$. This gives us an upper bound on the number of highly compressible strings. Let $N_{comp}$ be the number of highly compressible strings.\n$$ N_{comp} \\leq 2^{119} - 1 $$\nThe maximum possible number of highly compressible strings is $2^{119} - 1$.\n\nThe problem asks for the minimum fraction of strings that are *not* highly compressible. Let $N_{not\\_comp}$ be the number of strings that are not highly compressible.\n$$ N_{not\\_comp} = (\\text{Total number of strings}) - N_{comp} $$\nTo find the minimum possible value of $N_{not\\_comp}$, we must subtract the maximum possible value of $N_{comp}$.\n$$ N_{not\\_comp, min} = 2^{128} - (2^{119} - 1) $$\nThe minimum fraction, $f_{min}$, is this minimum number divided by the total number of strings.\n$$ f_{min} = \\frac{N_{not\\_comp, min}}{2^{128}} = \\frac{2^{128} - (2^{119} - 1)}{2^{128}} $$\nWe can split the fraction:\n$$ f_{min} = \\frac{2^{128}}{2^{128}} - \\frac{2^{119}}{2^{128}} + \\frac{1}{2^{128}} $$\n$$ f_{min} = 1 - 2^{119-128} + 2^{-128} $$\n$$ f_{min} = 1 - 2^{-9} + 2^{-128} $$\nNow we evaluate this expression numerically.\n$$ 2^9 = 512 $$\n$$ 2^{-9} = \\frac{1}{512} = 0.001953125 $$\nThe term $2^{-128}$ is extremely small (approximately $2.9 \\times 10^{-39}$) and will not affect the first several significant figures of our result. So we have:\n$$ f_{min} \\approx 1 - 0.001953125 = 0.998046875 $$\nFinally, we need to round this result to five significant figures. The first five significant figures are 9, 9, 8, 0, and 4. The sixth digit is 6, which is 5 or greater, so we round up the fifth significant figure (4) to 5.\n$$ f_{min} \\approx 0.99805 $$\nThis result shows that an overwhelming majority of strings cannot be significantly compressed, as their shortest description is nearly as long as the strings themselves.",
            "answer": "$$\\boxed{0.99805}$$"
        },
        {
            "introduction": "While most strings are algorithmically random, many objects we encounter in science and mathematics are defined by deep, underlying structures. This exercise explores the famous Sierpinski triangle, a fractal that appears visually complex yet is generated by a very simple recursive rule. By estimating the Kolmogorov complexity of its bitmap representation, you will see how the complexity of a highly structured object is not related to its size ($4^n$ pixels), but rather to the much smaller amount of information needed to specify its generative parameter, $n$ .",
            "id": "1635762",
            "problem": "Consider a sequence of bitmapped images, $I_n$, defined on a square grid of size $2^n \\times 2^n$ pixels for $n \\ge 0$. Each pixel is either black (represented by the bit '1') or white (represented by the bit '0').\n\nThe sequence of images is defined by the following recursive rule:\n- The base case, $I_0$, is a $1 \\times 1$ grid containing a single black pixel.\n- For $n  0$, the $n$-th image, $I_n$, is constructed by dividing its $2^n \\times 2^n$ grid into four non-overlapping quadrants of size $2^{n-1} \\times 2^{n-1}$. The top-left, bottom-left, and bottom-right quadrants are each filled with a copy of the image $I_{n-1}$. The top-right quadrant is filled entirely with white pixels.\n\nLet $s_n$ be the binary string of length $4^n$ obtained by concatenating the rows of the bitmap for $I_n$ in order, starting from the top row and reading each row from left to right.\n\nThe Kolmogorov complexity of a string $x$, denoted $K(x)$, is the length of the shortest program for a universal Turing machine that outputs $x$ and then halts. Estimate the Kolmogorov complexity $K(s_n)$ for large $n$. Your answer should be the dominant term in a rigorous upper bound for $K(s_n)$. Express your answer as an analytic function of $n$.",
            "solution": "The Kolmogorov complexity $K(s_n)$ of the string $s_n$ is upper-bounded by the length of any computer program that generates $s_n$, plus a constant. That is, if $P$ is a program that takes an input $n$ and outputs the string $s_n$, then $K(s_n) \\le |P| + |d(n)| + C$, where $|P|$ is the length of the program's description, $|d(n)|$ is the length of the self-delimiting description of the input $n$, and $C$ is a constant that depends on the choice of the universal Turing machine. To find an upper bound for $K(s_n)$, we can devise a compact algorithm to generate the string and then determine its description length.\n\nThe most compact way to describe the string $s_n$ is not by simulating the recursive construction directly, but by finding a direct condition for a pixel at a given coordinate to be black or white. Let's consider a pixel at coordinate $(i, j)$ in the $2^n \\times 2^n$ grid, where $i$ is the row index and $j$ is the column index, with $0 \\le i, j  2^n$. We can represent these coordinates as $n$-bit binary numbers:\n$i = (i_{n-1}i_{n-2}...i_0)_2$\n$j = (j_{n-1}j_{n-2}...j_0)_2$\n\nThe recursive construction rule can be translated into a rule based on these bit representations. At the first level of recursion (for $n0$), the grid is divided into four quadrants based on the most significant bits of the coordinates, $(i_{n-1}, j_{n-1})$.\n- $(i_{n-1}, j_{n-1}) = (0, 0)$ corresponds to the top-left quadrant.\n- $(i_{n-1}, j_{n-1}) = (0, 1)$ corresponds to the top-right quadrant.\n- $(i_{n-1}, j_{n-1}) = (1, 0)$ corresponds to the bottom-left quadrant.\n- $(i_{n-1}, j_{n-1}) = (1, 1)$ corresponds to the bottom-right quadrant.\n\nAccording to the problem, the top-right quadrant is white. This means any pixel with $(i_{n-1}, j_{n-1}) = (0, 1)$ is white. For the other three quadrants, the color of the pixel $(i, j)$ depends on the color of the pixel at the corresponding location within the $I_{n-1}$ sub-image. This corresponds to examining the next pair of bits, $(i_{n-2}, j_{n-2})$.\n\nThis logic applies recursively. A pixel $(i, j)$ belongs to a white region if, at any level of the recursion, its corresponding sub-pixel coordinates fall into the top-right quadrant. The bits that determine the quadrant at level $k$ (where $k=n-1$ is the first level, down to $k=0$ for the last) are $(i_k, j_k)$. The pixel is white if the pair of bits $(i_k, j_k)$ is equal to $(0, 1)$ for any $k \\in \\{0, 1, ..., n-1\\}$.\n\nTherefore, a pixel at coordinate $(i, j)$ is black if and only if for all $k \\in \\{0, 1, ..., n-1\\}$, the condition $(i_k, j_k) \\neq (0, 1)$ is met. This provides a simple and direct test for the color of any pixel.\n\nWe can now describe an algorithm to generate the string $s_n$ based on this condition:\n1.  Initialize an empty string `output`.\n2.  Loop for the row index `i` from $0$ to $2^n - 1$.\n3.  Loop for the column index `j` from $0$ to $2^n - 1$.\n4.  Set a flag `is_black` to `true`.\n5.  Loop for the bit index `k` from $0$ to $n-1$.\n6.  Extract the $k$-th bit of `i` ($i_k$) and the $k$-th bit of `j` ($j_k$).\n7.  If $i_k=0$ and $j_k=1$, set `is_black` to `false` and break the inner loop (over `k`).\n8.  After the inner loop, if `is_black` is `true`, append '1' to `output`. Otherwise, append '0'.\n9.  After the loops complete, `output` is the string $s_n$.\n\nNow we estimate the length of a program that implements this algorithm. The program's description must contain the logic itself and the value of the input parameter $n$.\n- The logic of the algorithm (the loops, the bitwise checks, the append operations) is fixed and independent of the value of $n$. Its description length is a constant, let's call it $C_P$.\n- The input parameter $n$ must be provided to the program. In the theory of Kolmogorov complexity, for an input to be unambiguously parsed by a universal machine, it must be encoded in a self-delimiting format. A common way to do this is to prefix the binary representation of $n$ with its own length. The length of the binary representation of $n$ is $\\lfloor\\log_2 n\\rfloor + 1$. For large $n$, this is approximately $\\log_2 n$. The length of the representation of this length is in turn approximately $\\log_2(\\log_2 n)$. So, a self-delimiting code for $n$ has a length of $|d(n)| = \\log_2 n + O(\\log_2 \\log_2 n)$.\n\nThe total length of the program description is the sum of the constant part and the part that depends on $n$.\nLength $\\approx C_P + (\\log_2 n + O(\\log_2 \\log_2 n)) + C$\nCombining the constants, we get:\n$K(s_n) \\le \\log_2 n + O(\\log_2 \\log_2 n) + C'$\n\nFor large $n$, the term that grows fastest and thus dominates the expression is $\\log_2 n$. Therefore, the dominant term in the estimate for the Kolmogorov complexity of $s_n$ is $\\log_2 n$.",
            "answer": "$$\\boxed{\\log_{2}(n)}$$"
        },
        {
            "introduction": "Kolmogorov complexity is not just a static measure; it behaves predictably under computational transformations. This practice explores one of the simplest forms of data redundancy: duplicating a string to create a new string $xx$. By determining the tightest upper bound on the complexity $K(xx)$ in relation to $K(x)$, you will engage with a core property of algorithmic complexity: a simple, computable operation adds only a constant amount to the complexity, regardless of the complexity of the original string .",
            "id": "1429018",
            "problem": "In the field of algorithmic information theory, a subfield of theoretical computer science, we quantify the complexity of an object by the length of its shortest possible description. For a finite binary string $x$, its Kolmogorov complexity, denoted $K(x)$, is defined with respect to a fixed universal Turing Machine, $U$. It is the length (in bits) of the shortest program $p$ that, when provided as input to $U$, causes $U$ to output the string $x$ and then halt. Formally, $K(x) = \\min\\{|p| : U(p)=x\\}$. While the exact value of $K(x)$ depends on the chosen universal machine $U$, key relationships in complexity theory, such as upper and lower bounds, hold up to an additive constant.\n\nConsider a data processing task where a binary string $x$ is concatenated with itself to form a new string $xx$. This is a simple model for data redundancy. We are interested in the complexity of this new, longer string.\n\nWhich of the following inequalities represents the tightest general upper bound on the Kolmogorov complexity of the string $xx$ for any binary string $x$? In these expressions, $c$ represents a positive constant whose value depends only on the choice of the universal Turing Machine $U$ and not on the string $x$ or its properties.\n\nA. $K(xx) \\le K(x) + c$\n\nB. $K(xx) \\le 2K(x) + c$\n\nC. $K(xx) \\le K(x)^2 + c$\n\nD. $K(xx) \\le K(x) + c \\log_2(|x|)$\n\nE. $K(xx) \\le |x| + c$",
            "solution": "Let $U$ be a fixed universal Turing machine and $K(\\cdot)$ the associated (prefix-free) Kolmogorov complexity. For any total computable function $f$, there exists a constant $c_{f}$ (depending only on $f$ and $U$) such that\n$$\nK(f(x)) \\leq K(x) + c_{f}.\n$$\nDerivation of this standard upper bound: let $p$ be a shortest $U$-program for $x$, so $|p| = K(x)$ and $U(p)=x$. Fix a program $r_{f}$ of constant length $|r_{f}|=c_{f}$ that, given a description of some program $p$, simulates $U(p)$ to obtain $x$, then computes and outputs $f(x)$. Consider the concatenated code $r_{f}p$ (well-formed because we use prefix-free descriptions), which has length $|r_{f}p| = |r_{f}| + |p| = c_{f} + K(x)$ and causes $U$ to output $f(x)$. Therefore\n$$\nK(f(x)) \\leq |r_{f}p| = K(x) + c_{f}.\n$$\n\nApply this with $f(y)=yy$ (duplicate the string). Since $f$ is total computable, there exists a constant $c$ such that\n$$\nK(xx) \\leq K(x) + c.\n$$\n\nThis bound is essentially tight up to an additive constant: define $g(z)$ to be the function that returns the first half of $z$. Then $g$ is total computable and $g(xx)=x$, so by the same reasoning there exists $c'$ with\n$$\nK(x) \\leq K(xx) + c'.\n$$\nHence $K(xx) \\geq K(x) - c'$, which combined with the upper bound shows $K(xx) = K(x) \\pm O(1)$ in general. Among the given options, this makes option A the tightest general upper bound; options B, C, D, and E are looser since they exceed $K(x)+c$ by, respectively, an additional $K(x)$ term, a quadratic term, a growing $\\log_{2}(|x|)$ term, or a replacement of $K(x)$ by the generally larger $|x|$.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}