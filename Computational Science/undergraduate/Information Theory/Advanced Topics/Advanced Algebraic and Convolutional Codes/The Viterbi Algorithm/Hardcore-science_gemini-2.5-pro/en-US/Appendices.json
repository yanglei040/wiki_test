{
    "hands_on_practices": [
        {
            "introduction": "To truly master the Viterbi algorithm, it is essential to work through its core mechanics by hand. This exercise guides you through the \"forward pass\" of the algorithm, where you will build the computational trellis step-by-step. By calculating the path probability metric $\\delta_t(j)$ and storing the backpointer $\\psi_t(j)$ for a given Hidden Markov Model, you will gain a concrete understanding of how the algorithm efficiently prunes suboptimal paths at each stage .",
            "id": "1664342",
            "problem": "A system monitoring atmospheric particles is modeled as a Hidden Markov Model (HMM). The system can be in one of three hidden states, $S_1, S_2, S_3$, corresponding to low, medium, and high pollution levels, respectively. At each time step, the system transitions to a new state (or stays in the same state) and emits an observation, which can be either \"Clear\" (C) or \"Hazy\" (H).\n\nThe parameters of the HMM are as follows:\n\n1.  **Initial State Probabilities** ($\\pi$): The probability distribution of the state at time $t=1$.\n    $$ \\pi = P(q_1=S_i) = \\begin{pmatrix} 0.7 & 0.2 & 0.1 \\end{pmatrix} $$\n    where $\\pi_i$ is the probability of starting in state $S_i$.\n\n2.  **State Transition Probabilities** ($A$): The probability of transitioning from state $S_i$ to state $S_j$.\n    $$ A = \\{a_{ij}\\} = P(q_t=S_j | q_{t-1}=S_i) = \\begin{pmatrix} 0.6 & 0.3 & 0.1 \\\\ 0.2 & 0.5 & 0.3 \\\\ 0.1 & 0.2 & 0.7 \\end{pmatrix} $$\n\n3.  **Observation Emission Probabilities** ($B$): The probability of observing a particular emission given the current state.\n    $$ b_j(O_k) = P(O_k | q_t=S_j) $$\n    The values are given by:\n    -   In state $S_1$: $P(\\text{C} | S_1) = 0.8$, $P(\\text{H} | S_1) = 0.2$\n    -   In state $S_2$: $P(\\text{C} | S_2) = 0.5$, $P(\\text{H} | S_2) = 0.5$\n    -   In state $S_3$: $P(\\text{C} | S_3) = 0.1$, $P(\\text{H} | S_3) = 0.9$\n\nA sequence of three observations is recorded: $O = (\\text{C}, \\text{H}, \\text{H})$.\n\nTo find the most likely sequence of hidden states that produced these observations, one computes the Viterbi path variables. Let $\\delta_t(j)$ be the maximum probability of any single path ending in state $S_j$ at time $t$, given the first $t$ observations. Let $\\psi_t(j)$ be the state at time $t-1$ that is part of this most likely path.\n\nYour task is to calculate the values for $\\delta_3(j)$ and $\\psi_3(j)$ for all states $j \\in \\{1, 2, 3\\}$. The states are indexed by 1, 2, and 3. The backpointers $\\psi_t(j)$ should also be represented by these state indices.\n\nPresent your answer as a single row matrix with six entries in the format $(\\delta_3(1), \\delta_3(2), \\delta_3(3), \\psi_3(1), \\psi_3(2), \\psi_3(3))$. Round the values of $\\delta_3(j)$ to four significant figures.",
            "solution": "We use the Viterbi recursion. For observation sequence $O=(\\text{C},\\text{H},\\text{H})$, define $\\delta_{t}(j)=\\max_{q_{1:t-1}}P(q_{1:t-1},q_{t}=S_{j},O_{1:t})$ and $\\psi_{t}(j)=\\arg\\max_{i}\\{\\delta_{t-1}(i)a_{ij}\\}$. The recursions are:\n$$\n\\delta_{1}(j)=\\pi_{j}b_{j}(O_{1}),\\quad \\psi_{1}(j)\\text{ undefined},\n$$\n$$\n\\delta_{t}(j)=\\left[\\max_{i}\\delta_{t-1}(i)a_{ij}\\right]b_{j}(O_{t}),\\quad \\psi_{t}(j)=\\arg\\max_{i}\\{\\delta_{t-1}(i)a_{ij}\\}.\n$$\n\nInitialization at $t=1$ with $O_{1}=\\text{C}$:\n$$\n\\delta_{1}(1)=0.7\\cdot 0.8=0.56,\\quad \\delta_{1}(2)=0.2\\cdot 0.5=0.1,\\quad \\delta_{1}(3)=0.1\\cdot 0.1=0.01.\n$$\n\nInduction to $t=2$ with $O_{2}=\\text{H}$:\n- For $j=1$:\n$$\n\\max_{i}\\{\\delta_{1}(i)a_{i1}\\}=\\max\\{0.56\\cdot 0.6,\\,0.1\\cdot 0.2,\\,0.01\\cdot 0.1\\}=0.336,\n$$\n$$\n\\delta_{2}(1)=0.336\\cdot 0.2=0.0672,\\quad \\psi_{2}(1)=1.\n$$\n- For $j=2$:\n$$\n\\max_{i}\\{\\delta_{1}(i)a_{i2}\\}=\\max\\{0.56\\cdot 0.3,\\,0.1\\cdot 0.5,\\,0.01\\cdot 0.2\\}=0.168,\n$$\n$$\n\\delta_{2}(2)=0.168\\cdot 0.5=0.084,\\quad \\psi_{2}(2)=1.\n$$\n- For $j=3$:\n$$\n\\max_{i}\\{\\delta_{1}(i)a_{i3}\\}=\\max\\{0.56\\cdot 0.1,\\,0.1\\cdot 0.3,\\,0.01\\cdot 0.7\\}=0.056,\n$$\n$$\n\\delta_{2}(3)=0.056\\cdot 0.9=0.0504,\\quad \\psi_{2}(3)=1.\n$$\n\nInduction to $t=3$ with $O_{3}=\\text{H}$:\n- For $j=1$:\n$$\n\\max_{i}\\{\\delta_{2}(i)a_{i1}\\}=\\max\\{0.0672\\cdot 0.6,\\,0.084\\cdot 0.2,\\,0.0504\\cdot 0.1\\}=0.04032,\n$$\n$$\n\\delta_{3}(1)=0.04032\\cdot 0.2=0.008064,\\quad \\psi_{3}(1)=1.\n$$\n- For $j=2$:\n$$\n\\max_{i}\\{\\delta_{2}(i)a_{i2}\\}=\\max\\{0.0672\\cdot 0.3,\\,0.084\\cdot 0.5,\\,0.0504\\cdot 0.2\\}=0.042,\n$$\n$$\n\\delta_{3}(2)=0.042\\cdot 0.5=0.021,\\quad \\psi_{3}(2)=2.\n$$\n- For $j=3$:\n$$\n\\max_{i}\\{\\delta_{2}(i)a_{i3}\\}=\\max\\{0.0672\\cdot 0.1,\\,0.084\\cdot 0.3,\\,0.0504\\cdot 0.7\\}=0.03528,\n$$\n$$\n\\delta_{3}(3)=0.03528\\cdot 0.9=0.031752,\\quad \\psi_{3}(3)=3.\n$$\n\nRounding $\\delta_{3}(j)$ to four significant figures gives:\n$$\n\\delta_{3}(1)=0.008064,\\quad \\delta_{3}(2)=0.02100,\\quad \\delta_{3}(3)=0.03175.\n$$\n\nTherefore, the requested row matrix is $(\\delta_{3}(1),\\delta_{3}(2),\\delta_{3}(3),\\psi_{3}(1),\\psi_{3}(2),\\psi_{3}(3))$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 0.008064 & 0.02100 & 0.03175 & 1 & 2 & 3 \\end{pmatrix}}$$"
        },
        {
            "introduction": "The Viterbi algorithm involves two main stages: a forward pass to compute path probabilities and a traceback to reconstruct the single best path. This practice isolates the second stage, the traceback procedure . Given a completed backpointer matrix from the forward pass, you will learn how to efficiently retrace the steps from the most probable final state back to the beginning to reveal the optimal hidden state sequence.",
            "id": "863125",
            "problem": "In the context of a Hidden Markov Model (HMM), the Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states, called the Viterbi path, that results in a sequence of observed events.\n\nConsider an HMM with $K=4$ hidden states, indexed by the set $\\{1, 2, 3, 4\\}$, and an observation sequence of length $T=6$. The forward pass of the Viterbi algorithm has already been completed. This pass computes, among other things, a backpointer matrix, which we will denote here with elements $\\psi_t(j)$ for time steps $t \\in \\{2, ..., T\\}$ and states $j \\in \\{1, ..., K\\}$. The value of $\\psi_t(j)$ is the index of the state at time $t-1$ that is on the most likely path to state $j$ at time $t$.\n\nThe computed backpointer values are given in the table below:\n\n| Current State ($j$) | $\\psi_2(j)$ | $\\psi_3(j)$ | $\\psi_4(j)$ | $\\psi_5(j)$ | $\\psi_6(j)$ |\n|:---:|:---:|:---:|:---:|:---:|:---:|\n| 1 | 1 | 3 | 4 | 1 | 2 |\n| 2 | 1 | 1 | 2 | 3 | 4 |\n| 3 | 2 | 2 | 1 | 3 | 3 |\n| 4 | 3 | 2 | 3 | 4 | 4 |\n\nFurthermore, the algorithm determined that the most probable hidden state at the final time step, $T=6$, is state 3. Let $Q^* = (q^*_1, q^*_2, q^*_3, q^*_4, q^*_5, q^*_6)$ be the most likely sequence of hidden states. We are given $q^*_6 = 3$.\n\nUsing the provided backpointer table and the final state, perform the traceback step of the Viterbi algorithm to determine the index of the most probable hidden state at the initial time step, $t=1$. Derive the value of $q^*_1$.",
            "solution": "We use the traceback recursion for the Viterbi path:\n$$\nq^*_t = \\psi_{t+1}\\bigl(q^*_{t+1}\\bigr).\n$$\nGiven $q^*_6=3$, compute backwards:\n$$\nq^*_5 = \\psi_6(3) = 3,\n$$\n$$\nq^*_4 = \\psi_5(3) = 3,\n$$\n$$\nq^*_3 = \\psi_4(3) = 1,\n$$\n$$\nq^*_2 = \\psi_3(1) = 3,\n$$\n$$\nq^*_1 = \\psi_2(3) = 2.\n$$\nThus,\n$$\nq^*_1 = 2.\n$$",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "The real power of an algorithm is revealed when we adapt it to solve new kinds of problems. This advanced exercise challenges you to think beyond finding the *most* probable path and instead find the *least* probable one, a common task in anomaly detection . By transforming the problem's probabilities, you can use the standard Viterbi machinery to solve this seemingly opposite optimization problem, highlighting the algorithm's fundamental flexibility.",
            "id": "1664317",
            "problem": "In the field of network security, a Hidden Markov Model (HMM) is often used to model the \"normal\" behavior of a system, such as a server's sequence of CPU load states. Anomaly detection can then be framed as identifying sequences of operations that are extremely unlikely under this model of normal behavior. Your task is to develop a method for finding the *least probable* sequence of hidden states that could have generated a given observation sequence.\n\nConsider an HMM defined by a set of hidden states $S = \\{S_1, S_2\\}$, a set of observation symbols $O = \\{O_1, O_2, O_3\\}$, and the following parameters:\n\n1.  **Initial State Probabilities ($\\pi$):** The probability of the system starting in a particular state.\n    $P(x_1 = S_1) = 0.6$\n    $P(x_1 = S_2) = 0.4$\n\n2.  **State Transition Probability Matrix ($A$):** The probability of transitioning from state $S_i$ to state $S_j$, denoted $P(x_t = S_j | x_{t-1} = S_i)$.\n    $$\n    A = \\begin{pmatrix} 0.7 & 0.3 \\\\ 0.5 & 0.5 \\end{pmatrix}\n    $$\n    where $A_{ij}$ is the probability of transitioning from state $S_i$ to $S_j$.\n\n3.  **Observation Emission Probability Matrix ($B$):** The probability of observing symbol $O_k$ given the system is in state $S_j$, denoted $P(y_t = O_k | x_t = S_j)$.\n    $$\n    B = \\begin{pmatrix} 0.2 & 0.5 & 0.3 \\\\ 0.6 & 0.1 & 0.3 \\end{pmatrix}\n    $$\n    where $B_{jk}$ is the probability of observing $O_k$ from state $S_j$.\n\nGiven the observation sequence $Y = (O_1, O_3, O_2)$, find the hidden state sequence $X = (x_1, x_2, x_3)$ that minimizes the joint probability $P(X, Y)$. You must use a systematic method based on dynamic programming; brute-force enumeration of all possible paths is not permitted. Express your answer as a sequence of state indices, where $S_1$ corresponds to index 1 and $S_2$ corresponds to index 2.",
            "solution": "The problem asks for the hidden state sequence $X = (x_1, x_2, x_3)$ that minimizes the joint probability $P(X, Y)$ for a given observation sequence $Y = (y_1, y_2, y_3)$. The standard Viterbi algorithm is designed to find the sequence $X$ that *maximizes* this probability. We can, however, adapt the problem to use the standard Viterbi algorithm by performing a transformation.\n\nThe joint probability of a state sequence $X=(x_1, \\dots, x_T)$ and an observation sequence $Y=(y_1, \\dots, y_T)$ is given by:\n$$P(X, Y) = \\pi_{x_1} B_{x_1}(y_1) \\prod_{t=2}^{T} A_{x_{t-1}, x_t} B_{x_t}(y_t)$$\nwhere $\\pi_{x_1}$ is the initial probability of state $x_1$, $A_{ij}$ is the transition probability from state $i$ to $j$, and $B_j(y_t)$ is the emission probability of observation $y_t$ from state $j$.\n\nWe want to find $\\arg\\min_{X} P(X, Y)$. Since all probabilities are positive, minimizing $P(X, Y)$ is equivalent to maximizing its reciprocal, $1/P(X, Y)$.\n$$ \\frac{1}{P(X, Y)} = \\frac{1}{\\pi_{x_1} B_{x_1}(y_1) \\prod_{t=2}^{T} A_{x_{t-1}, x_t} B_{x_t}(y_t)} = \\left(\\frac{1}{\\pi_{x_1}}\\right) \\left(\\frac{1}{B_{x_1}(y_1)}\\right) \\prod_{t=2}^{T} \\left(\\frac{1}{A_{x_{t-1}, x_t}}\\right) \\left(\\frac{1}{B_{x_t}(y_t)}\\right) $$\nThis expression has the same multiplicative structure as the original probability. We can therefore define a new set of \"pseudo-probabilities\" (or costs) which are the reciprocals of the original probabilities:\n-   Initial costs: $\\pi'_i = 1/\\pi_i$\n-   Transition costs: $A'_{ij} = 1/A_{ij}$\n-   Emission costs: $B'_{j}(k) = 1/B_{j}(k)$\n\nNow, the problem is transformed into finding the state sequence $X$ that *maximizes* the product of these costs. This is a problem that can be solved directly by the Viterbi algorithm using these new cost values.\n\nLet's define the parameters for our \"inverse\" problem:\n-   States: $S_1, S_2$\n-   Observation Sequence: $Y = (O_1, O_3, O_2)$\n-   Initial costs: $\\pi' = [1/0.6, 1/0.4] \\approx [1.667, 2.5]$\n-   Transition costs: $A' = \\begin{pmatrix} 1/0.7 & 1/0.3 \\\\ 1/0.5 & 1/0.5 \\end{pmatrix} \\approx \\begin{pmatrix} 1.429 & 3.333 \\\\ 2.0 & 2.0 \\end{pmatrix}$\n-   Emission costs $B'$ for the given sequence $Y=(O_1, O_3, O_2)$:\n    -   $t=1 (O_1)$: $B'_{S_1}(O_1) = 1/0.2 = 5$, $B'_{S_2}(O_1) = 1/0.6 \\approx 1.667$\n    -   $t=2 (O_3)$: $B'_{S_1}(O_3) = 1/0.3 \\approx 3.333$, $B'_{S_2}(O_3) = 1/0.3 \\approx 3.333$\n    -   $t=3 (O_2)$: $B'_{S_1}(O_2) = 1/0.5 = 2$, $B'_{S_2}(O_2) = 1/0.1 = 10$\n\nWe now apply the Viterbi algorithm. Let $\\delta_t(i)$ be the maximum cost of any path ending in state $S_i$ at time $t$, and let $\\psi_t(i)$ be the back-pointer to the previous state on that path.\n\n**Step 1: Initialization (t=1)**\nThe observation is $y_1 = O_1$.\n$\\delta_1(S_1) = \\pi'_{S_1} \\times B'_{S_1}(O_1) = \\frac{1}{0.6} \\times \\frac{1}{0.2} = \\frac{1}{0.12} \\approx 8.333$\n$\\delta_1(S_2) = \\pi'_{S_2} \\times B'_{S_2}(O_1) = \\frac{1}{0.4} \\times \\frac{1}{0.6} = \\frac{1}{0.24} \\approx 4.167$\n$\\psi_1(S_1) = \\text{null}$, $\\psi_1(S_2) = \\text{null}$\n\n**Step 2: Recursion (t=2)**\nThe observation is $y_2 = O_3$.\nFor state $S_1$:\n$\\delta_2(S_1) = \\max \\left( \\delta_1(S_1) \\times A'_{S_1,S_1}, \\delta_1(S_2) \\times A'_{S_2,S_1} \\right) \\times B'_{S_1}(O_3)$\n$\\delta_2(S_1) = \\max \\left( \\frac{1}{0.12} \\times \\frac{1}{0.7}, \\frac{1}{0.24} \\times \\frac{1}{0.5} \\right) \\times \\frac{1}{0.3} = \\max \\left( \\frac{1}{0.084}, \\frac{1}{0.12} \\right) \\times \\frac{1}{0.3}$\n$\\delta_2(S_1) = \\max(11.905, 8.333) \\times 3.333 = 11.905 \\times 3.333 = \\frac{1}{0.084 \\times 0.3} = \\frac{1}{0.0252} \\approx 39.683$\nThe maximum came from state $S_1$, so $\\psi_2(S_1) = S_1$.\n\nFor state $S_2$:\n$\\delta_2(S_2) = \\max \\left( \\delta_1(S_1) \\times A'_{S_1,S_2}, \\delta_1(S_2) \\times A'_{S_2,S_2} \\right) \\times B'_{S_2}(O_3)$\n$\\delta_2(S_2) = \\max \\left( \\frac{1}{0.12} \\times \\frac{1}{0.3}, \\frac{1}{0.24} \\times \\frac{1}{0.5} \\right) \\times \\frac{1}{0.3} = \\max \\left( \\frac{1}{0.036}, \\frac{1}{0.12} \\right) \\times \\frac{1}{0.3}$\n$\\delta_2(S_2) = \\max(27.778, 8.333) \\times 3.333 = 27.778 \\times 3.333 = \\frac{1}{0.036 \\times 0.3} = \\frac{1}{0.0108} \\approx 92.593$\nThe maximum came from state $S_1$, so $\\psi_2(S_2) = S_1$.\n\n**Step 3: Recursion (t=3)**\nThe observation is $y_3 = O_2$.\nFor state $S_1$:\n$\\delta_3(S_1) = \\max \\left( \\delta_2(S_1) \\times A'_{S_1,S_1}, \\delta_2(S_2) \\times A'_{S_2,S_1} \\right) \\times B'_{S_1}(O_2)$\n$\\delta_3(S_1) = \\max \\left( \\frac{1}{0.0252} \\times \\frac{1}{0.7}, \\frac{1}{0.0108} \\times \\frac{1}{0.5} \\right) \\times \\frac{1}{0.5} = \\max \\left( \\frac{1}{0.01764}, \\frac{1}{0.0054} \\right) \\times 2$\n$\\delta_3(S_1) = \\max(56.69, 185.185) \\times 2 = 185.185 \\times 2 = \\frac{1}{0.0054 \\times 0.5} = \\frac{1}{0.0027} \\approx 370.37$\nThe maximum came from state $S_2$, so $\\psi_3(S_1) = S_2$.\n\nFor state $S_2$:\n$\\delta_3(S_2) = \\max \\left( \\delta_2(S_1) \\times A'_{S_1,S_2}, \\delta_2(S_2) \\times A'_{S_2,S_2} \\right) \\times B'_{S_2}(O_2)$\n$\\delta_3(S_2) = \\max \\left( \\frac{1}{0.0252} \\times \\frac{1}{0.3}, \\frac{1}{0.0108} \\times \\frac{1}{0.5} \\right) \\times \\frac{1}{0.1} = \\max \\left( \\frac{1}{0.00756}, \\frac{1}{0.0054} \\right) \\times 10$\n$\\delta_3(S_2) = \\max(132.275, 185.185) \\times 10 = 185.185 \\times 10 = \\frac{1}{0.0054 \\times 0.1} = \\frac{1}{0.00054} \\approx 1851.85$\nThe maximum came from state $S_2$, so $\\psi_3(S_2) = S_2$.\n\n**Step 4: Termination and Backtracking**\nThe highest final cost is $\\delta_3(S_2) \\approx 1851.85$.\nThus, the final state of the least likely path is $x_3 = S_2$.\n\nWe now backtrack to find the complete path:\n-   $x_3 = S_2$\n-   $x_2 = \\psi_3(x_3) = \\psi_3(S_2) = S_2$\n-   $x_1 = \\psi_2(x_2) = \\psi_2(S_2) = S_1$\n\nThe least likely state sequence is $(S_1, S_2, S_2)$.\nExpressed in terms of state indices (where $S_1=1, S_2=2$), the sequence is $(1, 2, 2)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 1 & 2 & 2 \\end{pmatrix}}$$"
        }
    ]
}