{
    "hands_on_practices": [
        {
            "introduction": "Before diving into the complexities of joint source-channel coding, we must first understand the theoretical foundation that governs all reliable communication: the source-channel separation principle. This principle states that reliable communication is possible if and only if the channel's capacity is greater than or equal to the source's information rate. This exercise  provides a concrete calculation of this fundamental limit, asking you to determine the minimum channel capacity required to transmit data from a simple probabilistic source, thereby grounding the abstract concept of information rate in a practical scenario.",
            "id": "1635306",
            "problem": "A deep space probe, the \"Odyssey-III,\" is tasked with monitoring energetic particle events in the Kuiper Belt. Its primary particle detector analyzes each event and classifies it into one of eight distinct, predefined energy categories. Based on pre-launch simulations and existing astrophysical models, it is assumed that an event is equally likely to fall into any of these eight categories. The instrument is calibrated to generate and send exactly one such classification to its communication subsystem every second.\n\nTo ensure the integrity of this scientific data, the communication link back to Earth must be able to transmit this stream of classifications reliably. According to the foundational principles of information theory, what is the absolute minimum channel capacity required for this specific task?\n\nExpress your final answer as a single number in units of bits per second.",
            "solution": "The problem asks for the minimum channel capacity, $C_{min}$, required for the reliable transmission of information from a source. The Source-Channel Coding Theorem, a cornerstone of information theory, states that reliable communication is possible if and only if the source's information rate, denoted by $R$, is less than or equal to the channel capacity, $C$. That is, the condition for reliable communication is $R \\le C$.\n\nTherefore, the minimum channel capacity required is equal to the information rate of the source:\n$$C_{min} = R$$\n\nThe information rate $R$ of a source is the product of its symbol rate $r_s$ and the average information content per symbol, which is the source entropy $H(S)$.\n$$R = r_s \\times H(S)$$\n\nFirst, we need to calculate the entropy of the source, $H(S)$. The source is the particle detector, which produces symbols (classifications) from a set of $M=8$ possible outcomes. The problem states that these outcomes are equiprobable.\n\nThe entropy of a discrete random variable $S$ with $M$ outcomes is given by the formula:\n$$H(S) = -\\sum_{i=1}^{M} P(s_i) \\log_2(P(s_i))$$\nwhere $P(s_i)$ is the probability of the $i$-th outcome.\n\nSince all eight categories are equally likely, the probability of any single category is:\n$$P(s_i) = \\frac{1}{M} = \\frac{1}{8}$$\nFor an equiprobable source, the entropy formula simplifies to:\n$$H(S) = \\log_2(M)$$\nSubstituting $M=8$ into the simplified formula, we get:\n$$H(S) = \\log_2(8)$$\nSince $2^3 = 8$, the entropy is:\n$$H(S) = 3 \\text{ bits per symbol}$$\n\nNext, we determine the symbol rate, $r_s$. The problem states that the instrument generates one classification per second. Thus, the symbol rate is:\n$$r_s = 1 \\text{ symbol/second}$$\n\nNow we can calculate the information rate $R$:\n$$R = r_s \\times H(S) = (1 \\text{ symbol/second}) \\times (3 \\text{ bits/symbol}) = 3 \\text{ bits/second}$$\n\nFinally, based on the Source-Channel Coding Theorem, the minimum channel capacity for reliable transmission is equal to this information rate.\n$$C_{min} = R = 3 \\text{ bits/second}$$\n\nThe minimum required channel capacity is 3 bits per second. The problem asks for the answer as a single number in these units.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "The separation principle is a powerful theoretical result, but its practical implementation requires care. A naive approach, such as directly transmitting the output of a variable-length source code (like a Huffman code) over a noisy channel, can lead to disastrous results. This practice problem  vividly illustrates the phenomenon of catastrophic error propagation, where a single bit flip can cause the decoder to lose synchronization, corrupting the entire subsequent message. Understanding this failure mode is crucial for appreciating why more robust, integrated coding strategies are often necessary.",
            "id": "1635279",
            "problem": "An engineer is developing a data compression scheme for a sensor that outputs one of four possible symbols: $\\{S_1, S_2, S_3, S_4\\}$. The observed probabilities of these symbols are $P(S_1) = 0.5$, $P(S_2) = 0.25$, $P(S_3) = 0.125$, and $P(S_4) = 0.125$. To minimize the average data rate, an optimal prefix-free binary code is generated. The resulting codebook is as follows:\n- $C(S_1) = 0$\n- $C(S_2) = 10$\n- $C(S_3) = 110$\n- $C(S_4) = 111$\n\nA specific sequence of symbols, $S_2, S_1, S_4, S_3, S_1$, is encoded into a single binary stream for transmission. During transmission over a noisy channel, modeled as a Binary Symmetric Channel (BSC), a single bit error occurs, flipping the 4th bit of the concatenated binary stream.\n\nA receiver on the other end attempts to decode the corrupted bitstream. The decoder operates by reading bits from the start of the stream until a sequence of bits matches a valid codeword in the codebook. Once a match is found, the corresponding symbol is recorded, and the process repeats on the remainder of the bitstream. This continues until all bits have been consumed.\n\nWhich of the following represents the sequence of symbols decoded by the receiver from the corrupted bitstream?\n\nA. $S_2, S_1, S_4, S_3, S_1$\n\nB. $S_2, S_1, S_1, S_4, S_2, S_1$\n\nC. $S_2, S_1, S_1, S_4, S_2$\n\nD. $S_2, S_1, S_1, S_1, S_1, S_2$\n\nE. The stream is undecodable.",
            "solution": "The codebook is prefix-free with mappings $C(S_{1})=0$, $C(S_{2})=10$, $C(S_{3})=110$, $C(S_{4})=111$. Encoding the original sequence $S_{2},S_{1},S_{4},S_{3},S_{1}$ yields the concatenation\n$$\nC(S_{2})\\,C(S_{1})\\,C(S_{4})\\,C(S_{3})\\,C(S_{1})=10\\,0\\,111\\,110\\,0,\n$$\nwhich is the bitstring $1001111100$. Indexing bits as $b_{1}b_{2}\\dots b_{10}=1001111100$ gives $b_{4}=1$. A single bit flip at the fourth bit over the BSC produces the corrupted stream $b_{1}b_{2}b_{3}b_{4}'b_{5}\\dots b_{10}=1000111100$ with $b_{4}'=1-b_{4}=0$.\n\nDecoding proceeds greedily from the start, matching the shortest prefix that is a valid codeword and consuming it:\n- The first two bits are $10$, which match $C(S_{2})$, so the first decoded symbol is $S_{2}$. The remaining bits are $00111100$.\n- The next bit is $0$, which matches $C(S_{1})$, so decode $S_{1}$. The remaining bits are $0111100$.\n- The next bit is $0$ again, which matches $C(S_{1})$, so decode another $S_{1}$. The remaining bits are $111100$.\n- The next three bits are $111$, which match $C(S_{4})$, so decode $S_{4}$. The remaining bits are $100$.\n- The next two bits are $10$, which match $C(S_{2})$, so decode $S_{2}$. The remaining bit is $0$.\n- The final bit is $0$, which matches $C(S_{1})$, so decode $S_{1}$.\n\nThus the receiver decodes the sequence $S_{2}, S_{1}, S_{1}, S_{4}, S_{2}, S_{1}$, which corresponds to option B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Having seen the potential pitfalls of a naively separated system, we now explore a core concept in joint source-channel coding: optimizing the system by considering the source and channel characteristics together. This problem  moves beyond simple error correction and into the realm of distortion minimization through intelligent design. You will tackle an \"index assignment\" task, where you must strategically map source symbols to channel codewords to minimize overall error, demonstrating a powerful form of unequal error protection (UEP) by matching the most likely symbols to the most reliable transmission patterns offered by an asymmetric channel.",
            "id": "1635292",
            "problem": "An autonomous environmental sensor monitors water quality and reports one of three possible states: 'Normal' (N), 'Warning' (W), or 'Critical' (C). Based on historical data, the probabilities of these states are $P(N) = 0.70$, $P(W) = 0.20$, and $P(C) = 0.10$. To transmit its finding, the sensor first maps each state to a unique binary codeword from the set of available codewords $C = \\{00, 01, 10\\}$.\n\nThe encoded bits are then transmitted sequentially over a memoryless Binary Asymmetric Channel (BAC). The channel is characterized by the following bit-flip probabilities: the probability of a transmitted '0' being received as a '1' is $p_{1|0} = 0.05$, and the probability of a transmitted '1' being received as a '0' is $p_{0|1} = 0.20$.\n\nThe performance of this communication system is evaluated by the average end-to-end distortion. The distortion for a single transmission is defined as the Hamming distance between the transmitted codeword and the received 2-bit word (i.e., the number of bit positions in which they differ). Your task is to find the optimal assignment of codewords from the set $C$ to the source states {N, W, C} that minimizes this average distortion.\n\nCalculate the value of this minimum possible average distortion. Round your final answer to three significant figures.",
            "solution": "Let the source states be $S \\in \\{N, W, C\\}$ with probabilities $P(N)=0.70$, $P(W)=0.20$, and $P(C)=0.10$. Each state is mapped to a length-$2$ binary codeword from $C=\\{00,01,10\\}$. Transmission occurs over a memoryless Binary Asymmetric Channel with bit-flip probabilities $p_{1|0}=0.05$ and $p_{0|1}=0.20$. The single-transmission distortion is the Hamming distance between the transmitted and received $2$-bit words.\n\nFor a single bit $b \\in \\{0,1\\}$ sent through the BAC, the expected Hamming contribution is the flip probability:\n$$\n\\mathbb{E}[d_{b} \\mid b=0] = p_{1|0}, \\quad \\mathbb{E}[d_{b} \\mid b=1] = p_{0|1}.\n$$\nBy memorylessness and additivity of Hamming distance across bit positions, for a length-$2$ codeword $b_1b_2$,\n$$\n\\mathbb{E}[d \\mid b_{1}b_{2}] = \\sum_{i=1}^{2} \\mathbb{E}[d_{b_{i}} \\mid b_{i}] .\n$$\nTherefore, for the available codewords,\n$$\n\\mathbb{E}[d \\mid 00] = 2 p_{1|0}, \\quad \\mathbb{E}[d \\mid 01] = \\mathbb{E}[d \\mid 10] = p_{1|0} + p_{0|1}.\n$$\nDefine $d_{00} := 2 p_{1|0}$ and $d_{01} = d_{10} := p_{1|0} + p_{0|1}$. The average distortion for an assignment $c(\\cdot)$ of states to codewords is\n$$\nD = \\sum_{s \\in \\{N,W,C\\}} P(s) \\, \\mathbb{E}[d \\mid c(s)].\n$$\nSince $d_{00}  d_{01}$, to minimize $D$ one should assign the most probable state to $00$. Hence, the optimal mapping is $c(N)=00$ and $c(W), c(C) \\in \\{01,10\\}$ in any order. The minimum average distortion is\n$$\nD_{\\min} = P(N) \\cdot 2 p_{1|0} + \\left(P(W)+P(C)\\right) \\cdot (p_{1|0}+p_{0|1}).\n$$\nSubstituting the given values,\n$$\nD_{\\min} = 0.70 \\cdot 2 \\cdot 0.05 + 0.30 \\cdot (0.05 + 0.20) = 0.07 + 0.075 = 0.145.\n$$\nRounded to three significant figures, the minimum possible average distortion is $0.145$.",
            "answer": "$$\\boxed{0.145}$$"
        }
    ]
}