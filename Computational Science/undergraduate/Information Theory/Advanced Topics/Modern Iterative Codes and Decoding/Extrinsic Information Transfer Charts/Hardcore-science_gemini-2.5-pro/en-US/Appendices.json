{
    "hands_on_practices": [
        {
            "introduction": "The Binary Erasure Channel (BEC) provides an ideal simplified setting to understand the core mechanics of information transfer in iterative systems. Because information in a BEC is either perfectly received or completely lost, we can establish a direct and simple relationship between mutual information and erasure probability. This exercise  builds intuition by having you calculate how a decoder combines a priori knowledge with channel evidence in this straightforward scenario.",
            "id": "1623746",
            "problem": "Consider a component decoder for a rate-1/3 repetition code, which is a simple code where a single information bit $U$ is encoded into a three-bit codeword $(U, U, U)$. This decoder is designed to be used within a larger iterative decoding system.\n\nThe input to this decoder consists of a priori information about the single information bit, $U$. This a priori information can be modeled as the output of a virtual Binary Erasure Channel (BEC), where a transmitted bit is either received correctly or erased. The erasure probability of this virtual channel is $p_A$, and its corresponding mutual information is denoted by $I_A$.\n\nThe three coded bits of the codeword are transmitted over three independent and identical BECs, each with an erasure probability of $\\epsilon$.\n\nThe relationship between the mutual information $I$ and the erasure probability $p$ for any BEC is given by the formula $I = 1 - p$.\n\nThe decoder's function is to compute an extrinsic information output. In this specific context, the extrinsic information about the bit $U$ is defined as the information derived from combining the a priori information with the information from **two** of the three channel outputs. The resulting mutual information of this extrinsic output is denoted $I_E$.\n\nGiven a channel erasure probability $\\epsilon = 0.6$ and an a priori mutual information $I_A = 0.3$, calculate the corresponding extrinsic mutual information, $I_E$. Provide the exact numerical value of your answer.",
            "solution": "We model the a priori information about $U$ as the output of a virtual BEC with erasure probability $p_{A}$ and mutual information $I_{A}$. For any BEC, the relationship between mutual information $I$ and erasure probability $p$ is\n$$\nI=1-p.\n$$\nThus, for the a priori source,\n$$\np_{A}=1-I_{A}.\n$$\nThe three transmitted bits pass through independent BECs with erasure probability $\\epsilon$. The extrinsic information is formed by combining the a priori information with two of the three channel outputs. Since all sources are independent and the BEC has no errors (only erasures), the extrinsic output is erased if and only if all contributing sources are erased. Therefore, the extrinsic erasure probability is\n$$\np_{E}=p_{A}\\,\\epsilon^{2}.\n$$\nThe mutual information of the extrinsic output is then\n$$\nI_{E}=1-p_{E}=1-p_{A}\\,\\epsilon^{2}.\n$$\nSubstituting the given values $\\epsilon=0.6$ and $I_{A}=0.3$ gives\n$$\np_{A}=1-I_{A}=1-0.3=0.7,\n$$\n$$\np_{E}=0.7\\times(0.6)^{2}=0.7\\times 0.36=0.252,\n$$\nand hence\n$$\nI_{E}=1-0.252=0.748.\n$$",
            "answer": "$$\\boxed{0.748}$$"
        },
        {
            "introduction": "While the BEC is instructive, most real-world channels introduce noise rather than erasures, a scenario best modeled by the Additive White Gaussian Noise (AWGN) channel. This requires a more sophisticated approach where information is quantified using Log-Likelihood Ratios (LLRs), which are approximated as Gaussian random variables. This practice  will guide you through the essential technique of using the special J-function to map between the mutual information ($I$) and the variance ($\\sigma^2$) of these LLRs, a cornerstone of EXIT analysis for practical systems.",
            "id": "1623748",
            "problem": "In the analysis of iterative decoders for error-correcting codes, Extrinsic Information Transfer (EXIT) charts are a powerful tool for predicting performance. This problem explores the fundamental calculation behind these charts in a simplified scenario.\n\nConsider a single processing node in a message-passing system. This node represents a binary variable $x$ which takes values in $\\{-1, +1\\}$ with equal probability. The node receives two independent input messages in the form of Log-Likelihood Ratios (LLRs).\n\n1.  The first input is an \"a priori\" LLR, denoted as $L_A$. The mutual information between the variable $x$ and $L_A$ is given as $I_A$. In the EXIT chart model, it is assumed that when the true value of the variable is $x$, $L_A$ is a random a variable drawn from a Gaussian distribution with mean $x \\cdot \\sigma_A^2/2$ and variance $\\sigma_A^2$. The mutual information $I_A = I(x; L_A)$ is related to the standard deviation $\\sigma_A$ through a function $I = J(\\sigma)$.\n\n2.  The second input is a \"channel\" LLR, denoted as $L_{ch}$. This LLR is derived from an observation $y = x + n$ from a Binary-Input Additive White Gaussian Noise (BIAWGN) channel, where the noise $n$ is a zero-mean Gaussian random variable with variance $\\sigma_N^2$. The LLR is given by $L_{ch} = \\frac{2y}{\\sigma_N^2}$.\n\nThe node combines these two inputs to produce an extrinsic output LLR, $L_E$, according to the rule $L_E = L_A + L_{ch}$.\n\nYour task is to calculate the output extrinsic mutual information, $I_E = I(x; L_E)$, given the following:\n- The input a priori mutual information is $I_A = 0.5$ bits.\n- The channel noise variance is $\\sigma_N^2 = 1.0$.\n\nYou may use linear interpolation on the data provided in the tables below. The relationship between mutual information $I$ and the corresponding Gaussian LLR standard deviation $\\sigma$ is given by the functions $I = J(\\sigma)$ and $\\sigma = J^{-1}(I)$.\n\nTable 1: Values for the inverse J-function, $\\sigma = J^{-1}(I)$.\n| $I$ (bits) | $\\sigma = J^{-1}(I)$ |\n|:----------:|:--------------------:|\n| 0.40       | 1.428                |\n| 0.45       | 1.529                |\n| 0.50       | 1.636                |\n| 0.55       | 1.749                |\n| 0.60       | 1.874                |\n\nTable 2: Values for the J-function, $I = J(\\sigma)$.\n| $\\sigma$ | $I = J(\\sigma)$ (bits) |\n|:--------:|:----------------------:|\n| 2.50     | 0.8138                 |\n| 2.55     | 0.8240                 |\n| 2.60     | 0.8336                 |\n| 2.65     | 0.8428                 |\n| 2.70     | 0.8514                 |\n\nCalculate the output extrinsic mutual information, $I_E$, in bits. Round your final answer to three significant figures.",
            "solution": "We model consistent Gaussian LLRs as follows: if an LLR $L$ conditioned on $x \\in \\{-1,+1\\}$ satisfies $L \\mid x \\sim \\mathcal{N}\\!\\left(x \\frac{\\sigma^{2}}{2},\\, \\sigma^{2}\\right)$, then the mutual information is $I(x;L)=J(\\sigma)$. Sums of independent consistent Gaussian LLRs remain consistent with variances adding.\n\n1) A priori input:\nGiven $I_A=I(x;L_A)$, the consistency model gives $\\sigma_A = J^{-1}(I_A)$. From Table 1 at $I_A=0.50$,\n$$\n\\sigma_A = 1.636.\n$$\n\n2) Channel input:\nThe channel output is $y=x+n$ with $n \\sim \\mathcal{N}(0,\\sigma_N^{2})$, and the LLR is $L_{ch}=\\frac{2y}{\\sigma_N^{2}}=\\frac{2x}{\\sigma_N^{2}}+\\frac{2n}{\\sigma_N^{2}}$. Therefore,\n$$\nL_{ch}\\mid x \\sim \\mathcal{N}\\!\\left(x \\frac{2}{\\sigma_N^{2}},\\, \\frac{4}{\\sigma_N^{2}}\\right).\n$$\nThis is consistent Gaussian with\n$$\n\\sigma_{ch}^{2}=\\frac{4}{\\sigma_N^{2}}, \\quad \\sigma_{ch}=\\frac{2}{\\sigma_N}.\n$$\nFor $\\sigma_N^{2}=1.0$, we have $\\sigma_N=1$ and hence\n$$\n\\sigma_{ch}=2.\n$$\n\n3) Output LLR and its mutual information:\nSince $L_E=L_A+L_{ch}$ is a sum of independent consistent Gaussian LLRs, it is also consistent with\n$$\n\\sigma_{E}^{2}=\\sigma_{A}^{2}+\\sigma_{ch}^{2}.\n$$\nThus\n$$\n\\sigma_E=\\sqrt{(1.636)^{2}+(2)^{2}}=\\sqrt{2.676496+4}=\\sqrt{6.676496}\\approx 2.5839.\n$$\nHence the output mutual information is\n$$\nI_E=J(\\sigma_E).\n$$\nWe evaluate $J(\\sigma)$ by linear interpolation from Table 2. Between $\\sigma_{1}=2.55$ with $I_{1}=0.8240$ and $\\sigma_{2}=2.60$ with $I_{2}=0.8336$, for $\\sigma_{E}\\approx 2.5839$,\n$$\nI_E \\approx I_{1} + \\left(I_{2}-I_{1}\\right)\\frac{\\sigma_E-\\sigma_{1}}{\\sigma_{2}-\\sigma_{1}}\n= 0.8240 + (0.8336-0.8240)\\frac{2.5839-2.55}{0.05}.\n$$\nCompute the fraction and increment:\n$$\n\\frac{2.5839-2.55}{0.05}=0.678,\\quad (0.8336-0.8240)\\times 0.678=0.0065088,\n$$\nso\n$$\nI_E \\approx 0.8240 + 0.0065088 = 0.8305088 \\approx 0.831 \\text{ bits (to three significant figures).}\n$$",
            "answer": "$$\\boxed{0.831}$$"
        },
        {
            "introduction": "The true power of EXIT charts lies not just in calculating a single transfer point, but in predicting the dynamic behavior of the entire iterative decoding process. By visualizing the exchange of information as a trajectory between two decoder curves, we can forecast whether the decoder will converge to an error-free state. This problem  challenges you to interpret an EXIT chart to diagnose a common performance limitation in turbo codes, the \"error floor,\" which occurs when the decoding trajectory becomes trapped before reaching perfect information.",
            "id": "1623726",
            "problem": "An engineer is designing a digital communication system that uses a parallel concatenated code, commonly known as a turbo code. The system employs an iterative decoding process where two constituent soft-in/soft-out decoders exchange information. To analyze the convergence behavior of this decoder, the engineer uses an Extrinsic Information Transfer (EXIT) chart.\n\nAn EXIT chart plots the transfer characteristics of the decoders. For a given component decoder, it shows the mutual information between a coded bit and the decoder's extrinsic output, denoted $I_E$, as a function of the mutual information between that bit and the a priori input, denoted $I_A$. The chart contains two curves: one for the first component decoder, $T_1$, and one for the second, $T_2$. The decoding process is visualized as a trajectory between these two curves, which are plotted on the same axes (with the axes for $T_2$ appropriately inverted).\n\nFor the iterative process to decode successfully and achieve a vanishingly small error rate, the decoding trajectory must be able to reach the point $(I_A, I_E) = (1, 1)$, which represents perfect knowledge. This requires an open \"tunnel\" between the two curves leading to this point.\n\nThe engineer simulates the system and plots the EXIT chart for her chosen component codes. She observes that even at a very high Signal-to-Noise Ratio (SNR), the two EXIT curves intersect at a point $(I_A^*, I_E^*)$ where both $I_A^*$ and $I_E^*$ are noticeably less than 1. This means no open tunnel to the (1,1) point is formed.\n\nWhat is the most significant and direct implication of this observation for the performance of the overall turbo code?\n\nA. The code must have a very high coding rate, approaching the channel capacity.\n\nB. The iterative decoding process is guaranteed to converge to a state with zero errors.\n\nC. The code's performance will be limited by an error floor at high SNR values.\n\nD. The two component codes are perfectly matched for iterative decoding at all SNR values.\n\nE. The communication channel must be a channel with memory, such as a fading channel.",
            "solution": "An EXIT chart characterizes the soft-in/soft-out transfer of each component decoder via functions $T_{1}$ and $T_{2}$ that map a priori mutual information to extrinsic mutual information. Let $\\gamma$ denote the SNR parameter of the channel. For constituent decoder $i \\in \\{1,2\\}$, its transfer curve is\n$$\nI_{E,i} = T_{i}(I_{A,i}; \\gamma),\n$$\nwhere $I_{A,i}$ is the mutual information between the coded bit and the a priori input to decoder $i$, and $I_{E,i}$ is the mutual information between the coded bit and the extrinsic output.\n\nIn iterative decoding for a parallel concatenated (turbo) code, the iteration at step $l$ can be expressed in terms of mutual informations as\n$$\nI_{E,1}^{(l)} = T_{1}(I_{A,1}^{(l)}; \\gamma), \\quad I_{A,2}^{(l)} = I_{E,1}^{(l)},\n$$\n$$\nI_{E,2}^{(l)} = T_{2}(I_{A,2}^{(l)}; \\gamma), \\quad I_{A,1}^{(l+1)} = I_{E,2}^{(l)},\n$$\nwith initialization $I_{A,1}^{(0)} \\approx 0$. On an EXIT chart, the $T_{2}$ curve is plotted with axes swapped so that the iterative trajectory alternates between the two curves. Successful decoding with vanishing error probability requires an open tunnel from the starting point near $(I_{A},I_{E}) \\approx (0, T_{1}(0;\\gamma))$ to the point $(1,1)$. This ensures that the sequence $\\{I_{A,1}^{(l)}\\}$ (and equivalently $\\{I_{E,i}^{(l)}\\}$) can monotonically increase to $1$.\n\nIf, even at very high $\\gamma$, the two EXIT curves intersect at $(I_{A}^{*}, I_{E}^{*})$ with $I_{A}^{*}  1$ and $I_{E}^{*}  1$, the tunnel to $(1,1)$ is closed. The intersection corresponds to a fixed point of the iteration:\n$$\nI_{E,1}^{*} = T_{1}(I_{A,1}^{*}; \\gamma), \\quad I_{A,2}^{*} = I_{E,1}^{*},\n$$\n$$\nI_{E,2}^{*} = T_{2}(I_{A,2}^{*}; \\gamma), \\quad I_{A,1}^{*} = I_{E,2}^{*},\n$$\nwhich, when plotted with the axes for $T_{2}$ inverted, appears as the intersection of the two curves. As $l \\to \\infty$, the iterative process is trapped and converges to this suboptimal fixed point, not to $(1,1)$.\n\nThe performance implication follows from information-theoretic bounds. For a binary, equiprobable coded bit $X \\in \\{0,1\\}$ and any decoder output statistic $L$ (e.g., a posteriori LLR derived from the exchanged extrinsic information), the mutual information satisfies\n$$\nI(X;L) = 1 - H(X \\mid L),\n$$\nso $I(X;L)  1$ implies $H(X \\mid L)  0$. By Fanoâ€™s inequality,\n$$\nH(X \\mid L) \\leq H_{b}(P_{b}),\n$$\nwhere $H_{b}(\\cdot)$ is the binary entropy function and $P_{b}$ is the bit error probability under optimal decision on $L$. Hence, if the iteration converges to $I(X;L) = I^{*}  1$, then $H(X \\mid L) = 1 - I^{*}  0$ and\n$$\nP_{b} \\geq H_{b}^{-1}\\!\\left(1 - I^{*}\\right)  0.\n$$\nTherefore, even at high SNR, the decoder cannot drive the error probability to zero and will exhibit a nonzero residual error probability, i.e., an error floor.\n\nEvaluating the choices:\n- A is not implied by an EXIT-curve intersection; coding rate is unrelated to the presence of an open tunnel at a given SNR.\n- B is contradicted; the closed tunnel prevents convergence to $(1,1)$.\n- C follows directly; a persistent intersection below $(1,1)$ at high SNR implies a residual error probability, i.e., an error floor.\n- D is false; perfect matching would yield an open tunnel to $(1,1)$ over the relevant SNR range.\n- E is not implied; the EXIT observation pertains to decoder transfer characteristics, not necessarily to channel memory.\n\nThus, the most significant and direct implication is an error floor at high SNR values.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}