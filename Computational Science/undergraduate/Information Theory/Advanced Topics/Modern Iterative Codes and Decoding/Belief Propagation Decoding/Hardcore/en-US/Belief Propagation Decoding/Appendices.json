{
    "hands_on_practices": [
        {
            "introduction": "Before the belief propagation algorithm can begin its iterative process, we must first establish the initial evidence from the communication channel for each received bit. This is accomplished by calculating the Log-Likelihood Ratio (LLR), which serves as the fundamental currency of information exchanged within the decoder. This first exercise  will guide you through calculating this crucial initial LLR, translating a raw channel observation into a meaningful measure of belief.",
            "id": "1603925",
            "problem": "A simple error correction scheme is the (3,1) repetition code, where a single data bit is encoded into a three-bit codeword by repeating the data bit three times. To transmit data bit '0', the codeword '000' is sent, and to transmit data bit '1', the codeword '111' is sent.\n\nThese codeword bits are transmitted sequentially over a noisy Binary Symmetric Channel (BSC). A BSC is a channel model where each transmitted bit has a probability $p$ of being flipped to the opposite value (a '0' becomes a '1' or a '1' becomes a '0'), and a probability $1-p$ of being transmitted correctly. For this particular communication system, the crossover probability is given as $p = 0.1$.\n\nSuppose a codeword is being transmitted, and the first of the three received bits is observed to be '1'.\n\nModern decoding algorithms, such as belief propagation, compute an initial reliability value for each received bit. This value, representing the channel evidence, is defined as the natural logarithm of the ratio of two conditional probabilities: the probability of receiving a '1' for that bit position, given the original data bit was '0', divided by the probability of receiving a '1' for that bit position, given the original data bit was '1'.\n\nCalculate this reliability value based on the first received bit. Round your final answer to four significant figures.",
            "solution": "Over a Binary Symmetric Channel with crossover probability $p$, the conditional probabilities for a single transmitted bit are:\n$$P(\\text{receive }1 \\mid \\text{transmit }0) = p, \\quad P(\\text{receive }1 \\mid \\text{transmit }1) = 1-p.$$\nIn the $(3,1)$ repetition code, each transmitted bit equals the original data bit, so the channel evidence (log-likelihood ratio) for a received $1$ is defined as\n$$L = \\ln\\!\\left(\\frac{P(\\text{receive }1 \\mid \\text{data }0)}{P(\\text{receive }1 \\mid \\text{data }1)}\\right) = \\ln\\!\\left(\\frac{p}{1-p}\\right).$$\nWith $p=0.1$, this becomes\n$$L = \\ln\\!\\left(\\frac{0.1}{0.9}\\right) = \\ln\\!\\left(\\frac{1}{9}\\right) = -\\ln(9) \\approx -2.197224576.$$\nRounding to four significant figures gives $-2.197$.",
            "answer": "$$\\boxed{-2.197}$$"
        },
        {
            "introduction": "With the initial LLRs established, the belief propagation algorithm commences its core task: iteratively refining beliefs by passing messages along the edges of the factor graph. This practice  focuses on a single, fundamental step in this process: the calculation of a message from a check node to a variable node. By performing this calculation, you will see firsthand how the parity-check constraints of the code are used to propagate information and influence the beliefs of neighboring bits.",
            "id": "1603932",
            "problem": "In the field of digital communications, belief propagation is a powerful message-passing algorithm used for decoding error-correcting codes. Consider a simple linear block code defined by a factor graph with three variable nodes, representing the codeword bits $(c_1, c_2, c_3)$, and two check nodes, $f_A$ and $f_B$. The check node $f_A$ is connected to $c_1$ and $c_2$ and enforces the parity check constraint $c_1 \\oplus c_2 = 0$ (where $\\oplus$ denotes addition modulo 2). The check node $f_B$ is connected to $c_2$ and $c_3$ and enforces $c_2 \\oplus c_3 = 0$. This configuration is equivalent to a [3,1] repetition code.\n\nA codeword from this code is transmitted over a noisy channel. At the receiver, the initial beliefs about the transmitted bits are captured by the channel Log-Likelihood Ratios (LLRs). The LLR for a bit $c$ is defined as $L(c) = \\ln \\frac{P(c=0 | \\text{observation})}{P(c=1 | \\text{observation})}$. The measured channel LLRs for the three bits are:\n$L_{ch}(c_1) = +0.5$\n$L_{ch}(c_2) = -1.2$\n$L_{ch}(c_3) = -0.8$\n\nYour task is to calculate the very first message sent from the check node $f_A$ to the variable node $c_1$ during the first iteration of the LLR-based belief propagation algorithm. A message in this algorithm is also an LLR value.\n\nExpress your answer as a single real number, rounded to three significant figures.",
            "solution": "We work in the LLR domain. At the first iteration, variable-to-check messages are initialized by the channel LLRs, so for any variable node $c_{i}$ and adjacent check $f$,\n$$\nL_{c_{i} \\to f}^{(1)} = L_{ch}(c_{i}).\n$$\nFor a parity-check node $f$ enforcing $c_{1} \\oplus c_{2} = 0$, the LLR message from $f$ to $c_{1}$ is given by the standard check-node update rule\n$$\nL_{f \\to c_{1}}^{(1)} = 2 \\arctanh\\!\\left(\\prod_{v \\in \\mathcal{N}(f)\\setminus c_{1}} \\tanh\\!\\left(\\frac{L_{v \\to f}^{(1)}}{2}\\right)\\right).\n$$\nFor $f_{A}$ with neighbors $\\{c_{1}, c_{2}\\}$, this reduces to\n$$\nL_{f_{A} \\to c_{1}}^{(1)} = 2 \\arctanh\\!\\left(\\tanh\\!\\left(\\frac{L_{c_{2} \\to f_{A}}^{(1)}}{2}\\right)\\right).\n$$\nUsing the identity $\\arctanh(\\tanh(x)) = x$ for all real $x$, we obtain\n$$\nL_{f_{A} \\to c_{1}}^{(1)} = L_{c_{2} \\to f_{A}}^{(1)}.\n$$\nAt the first iteration, $L_{c_{2} \\to f_{A}}^{(1)} = L_{ch}(c_{2}) = -1.2$. Therefore,\n$$\nL_{f_{A} \\to c_{1}}^{(1)} = -1.2.\n$$\nRounded to three significant figures, the required message is $-1.20$.",
            "answer": "$$\\boxed{-1.20}$$"
        },
        {
            "introduction": "After multiple iterations of message passing, the belief propagation decoder provides a final posterior LLR for each bit, representing its best estimate of the transmitted codeword. Sometimes, the hard decision based on these LLRs is not a valid codeword, but the 'soft' information remains invaluable. This final exercise  demonstrates how to interpret these final posterior LLRs, using their magnitudes to rank the reliability of each decoded bit and pinpoint the most likely locations of error.",
            "id": "1603921",
            "problem": "A linear block code is defined by the following parity-check matrix $H$ over the binary field $\\mathbb{F}_2$:\n$$\nH = \\begin{pmatrix}\n1 & 0 & 1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 & 1 & 0 \\\\\n1 & 1 & 0 & 0 & 0 & 1\n\\end{pmatrix}\n$$\nA codeword from this code is transmitted over a noisy channel. A belief propagation (BP) decoder is used to recover the transmitted codeword. After a set number of iterations, the decoder outputs a vector of final posterior Log-Likelihood Ratios (LLRs) for the six coded bits $(c_1, c_2, c_3, c_4, c_5, c_6)$. The LLR for a bit $c_i$ is defined as $L(c_i) = \\ln(P(c_i=0|\\text{observation})/P(c_i=1|\\text{observation}))$.\n\nThe final posterior LLR vector is:\n$$\n\\mathbf{L}_{\\text{post}} = [4.1, -0.3, 0.7, -2.5, -0.5, 1.8]\n$$\nA hard decision is made on each bit, where a positive LLR maps to a bit value of 0 and a negative LLR maps to a bit value of 1. The resulting hard-decision vector is not a valid codeword of the code defined by $H$.\n\nYour task is to identify the bit positions that are most likely to be in error in this hard-decision vector. Provide a list of the three most probable error locations, identified by their bit indices (from 1 to 6). The list should be ordered from the most likely error position to the third most likely.",
            "solution": "We are given the parity-check matrix $H$ and the final posterior LLRs $\\mathbf{L}_{\\text{post}} = [4.1, -0.3, 0.7, -2.5, -0.5, 1.8]$. A hard decision maps $L(c_{i})>0$ to $\\hat{c}_{i}=0$ and $L(c_{i})<0$ to $\\hat{c}_{i}=1$. Applying this to each component yields\n$$\n\\hat{\\mathbf{c}} = [0,\\,1,\\,0,\\,1,\\,1,\\,0].\n$$\nTo verify that $\\hat{\\mathbf{c}}$ is not a valid codeword, compute the syndrome $\\mathbf{s}=H\\hat{\\mathbf{c}}^{T}$ over $\\mathbb{F}_{2}$. Using the rows of $H$:\n- First check: $c_{1}+c_{3}+c_{4}=0+0+1=1$.\n- Second check: $c_{2}+c_{3}+c_{5}=1+0+1=0$.\n- Third check: $c_{1}+c_{2}+c_{6}=0+1+0=1$.\nThus\n$$\n\\mathbf{s}=\\begin{pmatrix}1\\\\0\\\\1\\end{pmatrix}\\neq\\mathbf{0},\n$$\nso $\\hat{\\mathbf{c}}$ is not a valid codeword.\n\nThe posterior LLR for bit $i$ is $L_{i}=\\ln\\!\\big(P(c_{i}=0|\\text{obs})/P(c_{i}=1|\\text{obs})\\big)$. Given the hard decision $\\hat{c}_{i}$, the posterior probability that this hard decision is wrong equals\n$$\np_{e,i}=\\begin{cases}\n\\displaystyle \\frac{1}{1+\\exp(L_{i})}, & L_{i}>0,\\\n$$1ex]\n\\displaystyle \\frac{1}{1+\\exp(-L_{i})}, & L_{i}<0,\n\\end{cases}\n$$\nwhich can be written compactly as\n$$\np_{e,i}=\\frac{1}{1+\\exp(|L_{i}|)}.\n$$\nTherefore, $p_{e,i}$ is a strictly decreasing function of $|L_{i}|$. The three most probable error locations are thus the three indices with the smallest $|L_{i}|$.\n\nCompute the magnitudes:\n$$\n|L_{1}|=4.1,\\quad |L_{2}|=0.3,\\quad |L_{3}|=0.7,\\quad |L_{4}|=2.5,\\quad |L_{5}|=0.5,\\quad |L_{6}|=1.8.\n$$\nOrdering these from smallest to largest gives indices $2$ $(0.3)$, $5$ $(0.5)$, and $3$ $(0.7)$ as the three smallest magnitudes. Hence, the three most likely error positions, from most to third most likely, are bits $2$, $5$, and $3$.",
            "answer": "$$\\boxed{\\begin{pmatrix}2 & 5 & 3\\end{pmatrix}}$$"
        }
    ]
}