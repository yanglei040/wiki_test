{
    "hands_on_practices": [
        {
            "introduction": "Understanding polar codes begins with their elegant, recursive construction. This first exercise guides you through the process of building the generator matrix $G_N$ for a small code, a fundamental step based on the Kronecker product of a simple $2 \\times 2$ kernel. By applying this matrix to an information vector, you will perform the core encoding operation, transforming source bits into a protected codeword. ",
            "id": "1646922",
            "problem": "Polar codes are a class of error-correcting codes with an explicit construction. The generator matrix $G_N$ for a code of length $N=2^n$ is found by taking the $n$-th Kronecker power of a base matrix $F$. This is expressed as $G_N = F^{\\otimes n}$, where the base matrix is $F = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}$.\n\nThe Kronecker product, $A \\otimes B$, of an $m_1 \\times n_1$ matrix $A$ and an $m_2 \\times n_2$ matrix $B$ is the $(m_1 m_2) \\times (n_1 n_2)$ block matrix given by:\n$$A \\otimes B = \\begin{pmatrix} a_{11}B & \\cdots & a_{1,n_1}B \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{m_1,1}B & \\cdots & a_{m_1,n_1}B \\end{pmatrix}$$\nFor example, $F^{\\otimes 2} = F \\otimes F$.\n\nThe encoding of an information bit vector $u = (u_1, u_2, \\dots, u_N)$ is performed by the matrix multiplication $x = u G_N$, where $x$ is the resulting codeword. All arithmetic operations are performed over the Galois Field GF(2), where addition corresponds to the XOR operation (i.e., $1+1=0$) and multiplication is standard. The Hamming weight of a vector is defined as the number of its non-zero elements.\n\nConsider a polar code of length $N=4$. Given an information vector $u = (1, 0, 1, 1)$, calculate the Hamming weight of the resulting codeword $x$.",
            "solution": "We are given a polar code with length $N=4$, so $N=2^{n}$ implies $n=2$. The generator matrix is $G_{4} = F^{\\otimes 2}$ with $F = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}$. Using the Kronecker product definition,\n$$\nG_{4} \\equiv F^{\\otimes 2} = F \\otimes F = \\begin{pmatrix} 1\\cdot F & 0\\cdot F \\\\ 1\\cdot F & 1\\cdot F \\end{pmatrix}\n= \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 1 & 1 & 1\n\\end{pmatrix}.\n$$\nEncoding over $\\mathrm{GF}(2)$ is $x = u G_{4}$, where $u = (1,0,1,1)$. As a row-vector times a matrix, this equals the modulo-$2$ sum of those rows of $G_{4}$ where $u_{i}=1$, namely rows $1$, $3$, and $4$:\n$$\nx = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix}\n\\oplus \\begin{pmatrix} 1 & 0 & 1 & 0 \\end{pmatrix}\n\\oplus \\begin{pmatrix} 1 & 1 & 1 & 1 \\end{pmatrix}.\n$$\nFirst,\n$$\n\\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix} \\oplus \\begin{pmatrix} 1 & 0 & 1 & 0 \\end{pmatrix}\n= \\begin{pmatrix} 0 & 0 & 1 & 0 \\end{pmatrix},\n$$\nthen\n$$\n\\begin{pmatrix} 0 & 0 & 1 & 0 \\end{pmatrix} \\oplus \\begin{pmatrix} 1 & 1 & 1 & 1 \\end{pmatrix}\n= \\begin{pmatrix} 1 & 1 & 0 & 1 \\end{pmatrix}.\n$$\nThus $x = (1,1,0,1)$. The Hamming weight is the number of nonzero components of $x$, which is $3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "The power of polar codes lies in a phenomenon called channel polarization, where a set of identical communication channels are transformed into synthetic channels of either extremely high or extremely low quality. This exercise allows you to quantify this effect by tracking the Bhattacharyya parameter, a measure of channel reliability, through the recursive construction process. By calculating these values, you will gain a concrete understanding of why some bit-channels are chosen to carry information while others are 'frozen'. ",
            "id": "1646915",
            "problem": "In the theory of polar codes, a communication channel's reliability is often characterized by its Bhattacharyya parameter. For a binary-input channel $W$, this parameter is denoted $Z(W)$. A common channel model is the Binary Erasure Channel, abbreviated as BEC, which is defined by its erasure probability $\\epsilon$. For a BEC, its Bhattacharyya parameter is simply equal to its erasure probability, i.e., $Z(W) = \\epsilon$.\n\nPolar codes are built by recursively creating a set of $N$ synthetic channels from a base channel $W$, where $N$ must be a power of two. The process for $N=2$ involves creating two new channels, $W_2^{(1)}$ and $W_2^{(2)}$, from two independent copies of $W$. The Bhattacharyya parameters for these synthetic channels are determined by the following recursive formulas:\n$$Z(W_2^{(1)}) = 2Z(W) - (Z(W))^2$$\n$$Z(W_2^{(2)}) = (Z(W))^2$$\nThis process is then repeated. To construct the four synthetic channels for $N=4$, we take $W_2^{(1)}$ and $W_2^{(2)}$ as new base channels. Applying the transformation to $W_2^{(1)}$ generates $W_4^{(1)}$ and $W_4^{(2)}$. Similarly, applying the transformation to $W_2^{(2)}$ generates $W_4^{(3)}$ and $W_4^{(4)}$.\n\nAssume the initial base channel $W$ is a BEC with an erasure probability of $\\epsilon = 0.5$. Your task is to compute the Bhattacharyya parameters for the four synthetic channels at $N=4$: $Z(W_4^{(1)})$, $Z(W_4^{(2)})$, $Z(W_4^{(3)})$, and $Z(W_4^{(4)})$.\n\nExpress your result as a single row matrix containing the four numerical values in the specified order, with each value written as an exact fraction.",
            "solution": "We are given a Binary Erasure Channel (BEC) with erasure probability $\\epsilon = \\frac{1}{2}$. For a BEC, the Bhattacharyya parameter equals the erasure probability, so\n$$\nZ(W) = \\epsilon = \\frac{1}{2}.\n$$\nFor $N=2$, the two synthetic channels $W_{2}^{(1)}$ and $W_{2}^{(2)}$ have Bhattacharyya parameters given by the recursive relations\n$$\nZ(W_{2}^{(1)}) = 2Z(W) - (Z(W))^{2}, \\qquad Z(W_{2}^{(2)}) = (Z(W))^{2}.\n$$\nSubstituting $Z(W) = \\frac{1}{2}$,\n$$\nZ(W_{2}^{(1)}) = 2\\left(\\frac{1}{2}\\right) - \\left(\\frac{1}{2}\\right)^{2} = 1 - \\frac{1}{4} = \\frac{3}{4},\n$$\n$$\nZ(W_{2}^{(2)}) = \\left(\\frac{1}{2}\\right)^{2} = \\frac{1}{4}.\n$$\n\nFor $N=4$, we apply the same transformation to each of $W_{2}^{(1)}$ and $W_{2}^{(2)}$. For $W_{2}^{(1)}$ with $Z(W_{2}^{(1)}) = \\frac{3}{4}$, we obtain $W_{4}^{(1)}$ and $W_{4}^{(2)}$:\n$$\nZ(W_{4}^{(1)}) = 2Z(W_{2}^{(1)}) - (Z(W_{2}^{(1)}))^{2} = 2\\left(\\frac{3}{4}\\right) - \\left(\\frac{3}{4}\\right)^{2} = \\frac{3}{2} - \\frac{9}{16} = \\frac{24}{16} - \\frac{9}{16} = \\frac{15}{16},\n$$\n$$\nZ(W_{4}^{(2)}) = (Z(W_{2}^{(1)}))^{2} = \\left(\\frac{3}{4}\\right)^{2} = \\frac{9}{16}.\n$$\n\nFor $W_{2}^{(2)}$ with $Z(W_{2}^{(2)}) = \\frac{1}{4}$, we obtain $W_{4}^{(3)}$ and $W_{4}^{(4)}$:\n$$\nZ(W_{4}^{(3)}) = 2Z(W_{2}^{(2)}) - (Z(W_{2}^{(2)}))^{2} = 2\\left(\\frac{1}{4}\\right) - \\left(\\frac{1}{4}\\right)^{2} = \\frac{1}{2} - \\frac{1}{16} = \\frac{8}{16} - \\frac{1}{16} = \\frac{7}{16},\n$$\n$$\nZ(W_{4}^{(4)}) = (Z(W_{2}^{(2)}))^{2} = \\left(\\frac{1}{4}\\right)^{2} = \\frac{1}{16}.\n$$\n\nTherefore, in the order $\\left(Z(W_{4}^{(1)}), Z(W_{4}^{(2)}), Z(W_{4}^{(3)}), Z(W_{4}^{(4)})\\right)$, the row matrix of exact fractions is\n$$\n\\begin{pmatrix}\n\\frac{15}{16} & \\frac{9}{16} & \\frac{7}{16} & \\frac{1}{16}\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{15}{16} & \\frac{9}{16} & \\frac{7}{16} & \\frac{1}{16}\\end{pmatrix}}$$"
        },
        {
            "introduction": "This final practice ties together the core concepts of encoding, channel polarization, and decoding into a complete end-to-end analysis. You will examine a simple polar code operating over a Binary Symmetric Channel (BSC), where one bit is 'frozen' to improve the reliability of the other. By working through the successive cancellation decoding logic and calculating the exact probability of a decoding error, you will see firsthand how these theoretical principles translate into tangible improvements in communication reliability. ",
            "id": "1646943",
            "problem": "A simple rate-1/2 polar code of length $N=2$ is designed to transmit a single information bit over a Binary Symmetric Channel (BSC). The BSC is a memoryless channel that flips each transmitted bit with a crossover probability $p$. The encoding for this polar code is defined by the transformation $\\mathbf{x} = \\mathbf{u} G_2$, where $\\mathbf{u} = (u_1, u_2)$ are the source bits, $\\mathbf{x} = (x_1, x_2)$ are the channel input bits, and the generator matrix is $G_2 = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}$. All additions are performed modulo 2 (i.e., the XOR operation).\n\nFor this coding scheme, the bit-channel synthesized for $u_1$ is deemed less reliable and is \"frozen\" by fixing its value to $u_1 = 0$. The information bit is assigned to $u_2$, which corresponds to a more reliable synthesized channel.\n\nSuppose we want to transmit the information bit $u_2 = 1$. The resulting codeword is sent over a BSC with a crossover probability of $p=0.1$. At the receiver, a successive cancellation decoder operates. It first sets the estimate $\\hat{u}_1$ to the known frozen value, 0. Then, it uses the channel outputs $(y_1, y_2)$ and the value of $\\hat{u}_1$ to produce an estimate $\\hat{u}_2$ by applying a maximum-likelihood decision rule.\n\nCalculate the exact probability that the decoder makes an error in estimating the information bit, i.e., find $P(\\hat{u}_2 \\neq u_2)$. Express your final answer as a numerical value rounded to four significant figures.",
            "solution": "The problem asks for the probability of error in decoding the information bit $u_2$ for a length-2 polar code. We will solve this by following the encoding and decoding steps precisely.\n\nFirst, we determine the transmitted codeword. The source bits are given by the frozen bit $u_1 = 0$ and the information bit $u_2 = 1$, so $\\mathbf{u} = (0, 1)$. The encoding is given by $\\mathbf{x} = \\mathbf{u} G_2$:\n$$ \\mathbf{x} = (x_1, x_2) = (u_1, u_2) \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix} = (u_1 \\oplus u_2, u_2) $$\nwhere $\\oplus$ denotes addition modulo 2. Substituting the values of $u_1$ and $u_2$:\n$$ x_1 = 0 \\oplus 1 = 1 $$\n$$ x_2 = 1 $$\nSo, the transmitted codeword is $\\mathbf{x} = (1, 1)$. This codeword is sent over a BSC with crossover probability $p=0.1$. The received vector is $\\mathbf{y} = (y_1, y_2)$.\n\nNext, we analyze the successive cancellation (SC) decoder. The decoder first estimates $u_1$. Since $u_1$ is a frozen bit with a known value of 0, the decoder sets its estimate $\\hat{u}_1 = 0$. There is no uncertainty or chance of error in this step.\n\nThen, the decoder estimates $u_2$ using the received vector $\\mathbf{y} = (y_1, y_2)$ and its knowledge that $u_1=0$ (or more formally, its estimate $\\hat{u}_1=0$). The decoder makes a maximum-likelihood decision. It compares the probability of receiving $\\mathbf{y}$ if $u_2=0$ versus if $u_2=1$, given that $u_1=0$.\nThe decoder decides $\\hat{u}_2=0$ if $P(y_1, y_2|u_1=0, u_2=0) \\ge P(y_1, y_2|u_1=0, u_2=1)$.\nThe decoder decides $\\hat{u}_2=1$ if $P(y_1, y_2|u_1=0, u_2=0) < P(y_1, y_2|u_1=0, u_2=1)$.\n\nSince the channel is memoryless, the joint probability is the product of individual probabilities:\n$$ P(y_1, y_2|u_1, u_2) = P(y_1|x_1) P(y_2|x_2) = P(y_1|u_1 \\oplus u_2) P(y_2|u_2) $$\nLet's evaluate the two probabilities for the decoder's comparison, given $u_1=0$:\nIf $u_2=0$: $x_1=0\\oplus0=0, x_2=0$. The probability is $P(y_1|x_1=0)P(y_2|x_2=0)$.\nIf $u_2=1$: $x_1=0\\oplus1=1, x_2=1$. The probability is $P(y_1|x_1=1)P(y_2|x_2=1)$.\n\nThe decision rule can be expressed using a likelihood ratio (LR). Let $L(y) = \\frac{P(y|x=0)}{P(y|x=1)}$. The decoder decides $\\hat{u}_2=0$ if the LR is greater than or equal to 1:\n$$ \\frac{P(y_1, y_2|u_1=0, u_2=0)}{P(y_1, y_2|u_1=0, u_2=1)} = \\frac{P(y_1|x_1=0)P(y_2|x_2=0)}{P(y_1|x_1=1)P(y_2|x_2=1)} = L(y_1) L(y_2) \\ge 1 $$\nAn error occurs if the decoder decides $\\hat{u}_2=0$ when we actually transmitted $u_2=1$. This happens when $L(y_1) L(y_2) \\ge 1$.\n\nFor a BSC with crossover probability $p$, we have $P(y=b|x=a) = 1-p$ if $a=b$ and $p$ if $a \\neq b$. The likelihoods are:\n$L(y=0) = \\frac{P(y=0|x=0)}{P(y=0|x=1)} = \\frac{1-p}{p}$\n$L(y=1) = \\frac{P(y=1|x=0)}{P(y=1|x=1)} = \\frac{p}{1-p}$\nGiven $p=0.1$, we have $1-p=0.9$. So, $L(0) = \\frac{0.9}{0.1} = 9$ and $L(1) = \\frac{0.1}{0.9} = \\frac{1}{9}$.\n\nNow we check the condition $L(y_1)L(y_2) \\ge 1$ for the four possible received vectors $\\mathbf{y}=(y_1, y_2)$:\n1. $\\mathbf{y}=(0,0)$: $L(0)L(0) = 9 \\times 9 = 81 \\ge 1$. This leads to an error ($\\hat{u}_2=0$).\n2. $\\mathbf{y}=(0,1)$: $L(0)L(1) = 9 \\times \\frac{1}{9} = 1 \\ge 1$. This leads to an error ($\\hat{u}_2=0$).\n3. $\\mathbf{y}=(1,0)$: $L(1)L(0) = \\frac{1}{9} \\times 9 = 1 \\ge 1$. This leads to an error ($\\hat{u}_2=0$).\n4. $\\mathbf{y}=(1,1)$: $L(1)L(1) = \\frac{1}{9} \\times \\frac{1}{9} = \\frac{1}{81} < 1$. This leads to a correct decision ($\\hat{u}_2=1$).\n\nAn error occurs if the received vector is $(0,0)$, $(0,1)$, or $(1,0)$. The total probability of error is the sum of the probabilities of these three events, given that the transmitted codeword was $\\mathbf{x}=(1,1)$.\nThe BSC acts on each bit independently.\n$P(\\text{error}) = P(\\mathbf{y}=(0,0)|\\mathbf{x}=(1,1)) + P(\\mathbf{y}=(0,1)|\\mathbf{x}=(1,1)) + P(\\mathbf{y}=(1,0)|\\mathbf{x}=(1,1))$\n\n1. $P(\\mathbf{y}=(0,0)|\\mathbf{x}=(1,1)) = P(y_1=0|x_1=1) P(y_2=0|x_2=1) = p \\cdot p = p^2$. (Both bits flipped).\n2. $P(\\mathbf{y}=(0,1)|\\mathbf{x}=(1,1)) = P(y_1=0|x_1=1) P(y_2=1|x_2=1) = p \\cdot (1-p)$. (First bit flipped).\n3. $P(\\mathbf{y}=(1,0)|\\mathbf{x}=(1,1)) = P(y_1=1|x_1=1) P(y_2=0|x_2=1) = (1-p) \\cdot p$. (Second bit flipped).\n\nThe total probability of error is the sum:\n$P(\\text{error}) = p^2 + p(1-p) + (1-p)p = p^2 + p - p^2 + p - p^2 = 2p - p^2$.\n\nNow, we substitute the given value $p=0.1$:\n$P(\\text{error}) = 2(0.1) - (0.1)^2 = 0.2 - 0.01 = 0.19$.\n\nThe problem asks for the answer rounded to four significant figures.\n$0.1900$.",
            "answer": "$$\\boxed{0.1900}$$"
        }
    ]
}