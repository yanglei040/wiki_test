{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the concepts of information theory, we must move from abstract definitions to concrete calculations. This first exercise provides foundational practice in computing Fisher information, a key measure of the precision with which we can estimate a parameter. By working with the exponential distribution—a model commonly used to describe phenomena like radioactive decay or waiting times—you will build essential skills for quantifying the information contained within your data. ",
            "id": "1653754",
            "problem": "In particle physics, the time until a specific type of unstable particle decays can be modeled as a random variable. A common model for this decay time is the exponential distribution. The Probability Density Function (PDF) for a single decay time $X$ is given by\n$$ f(x; \\lambda) = \\lambda \\exp(-\\lambda x) $$\nfor $x \\ge 0$, where $\\lambda > 0$ is the decay rate parameter. A larger $\\lambda$ implies the particle decays more quickly on average.\n\nAn experiment is conducted to estimate the decay rate $\\lambda$ of a newly discovered particle. In this experiment, a physicist observes a set of $n$ independent and identically distributed (i.i.d.) decay times, denoted as $X_1, X_2, \\ldots, X_n$.\n\nThe Fisher information provides a way to measure the amount of information that an observable random variable carries about an unknown parameter of a distribution that models it. For a set of $n$ i.i.d. observations, the Fisher information $I_n(\\lambda)$ quantifies the precision with which the parameter $\\lambda$ can be estimated from the data.\n\nCalculate the Fisher information, $I_n(\\lambda)$, for the set of $n$ decay time measurements with respect to the decay rate parameter $\\lambda$. Express your answer in terms of $n$ and $\\lambda$.",
            "solution": "We model each decay time $X_{i}$ as i.i.d. with density $f(x;\\lambda)=\\lambda \\exp(-\\lambda x)$ for $x\\ge 0$, where $\\lambda>0$. For observations $x_{1},\\ldots,x_{n}$, the likelihood is\n$$\nL(\\lambda;x_{1:n})=\\prod_{i=1}^{n}\\lambda \\exp(-\\lambda x_{i})=\\lambda^{n}\\exp\\!\\Big(-\\lambda\\sum_{i=1}^{n}x_{i}\\Big).\n$$\nThe log-likelihood is\n$$\n\\ell(\\lambda)=\\ln L(\\lambda;x_{1:n})=\\sum_{i=1}^{n}\\big(\\ln \\lambda-\\lambda x_{i}\\big)=n\\ln \\lambda-\\lambda\\sum_{i=1}^{n}x_{i}.\n$$\nThe score function (first derivative) is\n$$\n\\frac{\\partial \\ell(\\lambda)}{\\partial \\lambda}=\\frac{n}{\\lambda}-\\sum_{i=1}^{n}x_{i},\n$$\nand the second derivative is\n$$\n\\frac{\\partial^{2} \\ell(\\lambda)}{\\partial \\lambda^{2}}=-\\frac{n}{\\lambda^{2}}.\n$$\nBy the definition of Fisher information for i.i.d. samples under standard regularity conditions,\n$$\nI_{n}(\\lambda)=-\\mathbb{E}\\!\\left[\\frac{\\partial^{2} \\ell(\\lambda)}{\\partial \\lambda^{2}}\\right].\n$$\nSince $\\partial^{2}\\ell/\\partial \\lambda^{2}=-n/\\lambda^{2}$ does not depend on the data, its expectation equals itself, yielding\n$$\nI_{n}(\\lambda)=-\\Big(-\\frac{n}{\\lambda^{2}}\\Big)=\\frac{n}{\\lambda^{2}}.\n$$\nEquivalently, the Fisher information for one observation is $I_{1}(\\lambda)=\\frac{1}{\\lambda^{2}}$, and additivity over $n$ i.i.d. observations gives $I_{n}(\\lambda)=n I_{1}(\\lambda)=\\frac{n}{\\lambda^{2}}$.",
            "answer": "$$\\boxed{\\frac{n}{\\lambda^{2}}}$$"
        },
        {
            "introduction": "Having practiced the calculation of Fisher information, we now explore its direct relationship with entropy in a simple discrete system. This problem examines a Bernoulli process, the building block of digital information, to investigate the connection between uncertainty (entropy) and estimability (Fisher information). By finding the parameter value that maximizes a system's entropy, you will discover a fundamental and intuitive trade-off: the state of maximum uncertainty is precisely where an observation provides the least information about the underlying parameter. ",
            "id": "1653764",
            "problem": "Consider a simple binary system, such as a memory bit or a digital signal, which can be in one of two states: 'on' (represented by 1) or 'off' (represented by 0). Let the random variable $X$ describe the state of the system, following a Bernoulli distribution with parameter $p$. That is, the probability of the system being 'on' is $P(X=1) = p$, and the probability of it being 'off' is $P(X=0) = 1-p$, for $p \\in (0, 1)$.\n\nThe uncertainty of the system's state is measured by the Shannon entropy, given by the function $H(p) = -p \\ln(p) - (1-p) \\ln(1-p)$, where $\\ln$ denotes the natural logarithm.\n\nThe Fisher information, which quantifies how much information an observation of $X$ provides about the parameter $p$, is defined for a Bernoulli distribution as $I(p) = \\frac{1}{p(1-p)}$.\n\nYour task is to analyze the point of maximum uncertainty. First, find the value of $p$ that maximizes the entropy $H(p)$. Second, evaluate the Fisher information $I(p)$ at this specific value of $p$.\n\nProvide two numbers as your final answer: the value of $p$ that maximizes the entropy, and the corresponding value of the Fisher information. Express both values as exact fractions or integers.",
            "solution": "We are given the Bernoulli entropy $H(p) = -p \\ln(p) - (1-p) \\ln(1-p)$ for $p \\in (0,1)$ and the Fisher information $I(p) = \\frac{1}{p(1-p)}$. To find the value of $p$ that maximizes $H(p)$, we differentiate with respect to $p$ and set the derivative to zero.\n\nCompute the first derivative using standard differentiation rules:\n$$\n\\frac{d}{dp}\\big[-p \\ln p\\big] = -(\\ln p + 1), \\quad \\frac{d}{dp}\\big[-(1-p)\\ln(1-p)\\big] = \\ln(1-p) + 1.\n$$\nTherefore,\n$$\nH'(p) = -(\\ln p + 1) + (\\ln(1-p) + 1) = \\ln(1-p) - \\ln p = \\ln\\!\\left(\\frac{1-p}{p}\\right).\n$$\nSet $H'(p) = 0$ to find critical points:\n$$\n\\ln\\!\\left(\\frac{1-p}{p}\\right) = 0 \\quad \\Longrightarrow \\quad \\frac{1-p}{p} = 1 \\quad \\Longrightarrow \\quad 1 - p = p \\quad \\Longrightarrow \\quad p = \\frac{1}{2}.\n$$\nTo verify that this critical point is a maximum, compute the second derivative:\n$$\nH''(p) = \\frac{d}{dp}\\big[\\ln(1-p) - \\ln p\\big] = -\\frac{1}{1-p} - \\frac{1}{p} = -\\left(\\frac{1}{1-p} + \\frac{1}{p}\\right).\n$$\nFor $p \\in (0,1)$, both terms inside the parentheses are positive, so $H''(p) < 0$, confirming that $p = \\frac{1}{2}$ is a point of maximum entropy.\n\nNext, evaluate the Fisher information at this $p$:\n$$\nI\\!\\left(\\frac{1}{2}\\right) = \\frac{1}{\\left(\\frac{1}{2}\\right)\\left(1 - \\frac{1}{2}\\right)} = \\frac{1}{\\frac{1}{4}} = 4.\n$$\nThus, the value of $p$ that maximizes the entropy is $\\frac{1}{2}$, and the corresponding Fisher information is $4$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2} & 4 \\end{pmatrix}}$$"
        },
        {
            "introduction": "In our final practice, we turn to the cornerstone of statistics and signal processing: the Gaussian distribution. This problem uncovers a remarkably elegant and profound connection between Fisher information and differential entropy, one that is not immediately obvious. You will calculate a specific product involving the Fisher information $I(\\mu)$ and the entropy power $\\exp(2h(X))$, revealing a universal constant that is independent of the distribution's variance $\\sigma^2$. This surprising result, an instance of the de Bruijn identity, illustrates a deep structural property linking estimation precision and statistical uncertainty. ",
            "id": "1653753",
            "problem": "A measurement of a physical quantity is modeled by a random variable $X$ that follows a Gaussian (normal) distribution. The probability density function (PDF) for this distribution is given by:\n$$p(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\nHere, $\\mu$ is the mean of the distribution, representing the true value of the physical quantity, and $\\sigma^2$ is the variance, representing the measurement uncertainty. The variance $\\sigma^2$ is a known positive constant, while the mean $\\mu$ is the parameter we wish to gain information about.\n\nFor such a distribution, two important quantities from information theory are the Fisher information and the differential entropy.\n\nThe Fisher information, $I(\\theta)$, quantifies the amount of information that an observable random variable $X$ carries about an unknown parameter $\\theta$ of a distribution that models $X$. For a parameter $\\theta$, it is defined as the expectation of the squared score:\n$$I(\\theta) = \\mathbb{E}\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\ln p(x; \\theta)\\right)^2\\right]$$\nwhere the expectation $\\mathbb{E}[\\cdot]$ is taken with respect to the distribution $p(x; \\theta)$.\n\nThe differential entropy, $h(X)$, is a measure of the average uncertainty of a continuous random variable. It is defined as:\n$$h(X) = -\\int_{-\\infty}^{\\infty} p(x) \\ln p(x) \\, dx$$\nwhich can also be expressed using the expectation operator as $h(X) = -\\mathbb{E}[\\ln p(X)]$.\n\nYour task is to calculate the value of the product $I(\\mu) \\cdot \\exp(2h(X))$, where $I(\\mu)$ is the Fisher information of the Gaussian distribution with respect to its mean $\\mu$, and $h(X)$ is its differential entropy. The final result should be a symbolic expression in terms of fundamental mathematical constants.",
            "solution": "We start with the Gaussian density\n$$\np(x;\\mu,\\sigma^{2})=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right),\n$$\nwhere $\\sigma^{2}$ is known and $\\mu$ is the parameter of interest.\n\nFisher information for $\\mu$:\nThe log-density is\n$$\n\\ln p(x;\\mu,\\sigma^{2})=-\\frac{1}{2}\\ln(2\\pi\\sigma^{2})-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}.\n$$\nThe score with respect to $\\mu$ is\n$$\n\\frac{\\partial}{\\partial \\mu}\\ln p(x;\\mu,\\sigma^{2})=\\frac{x-\\mu}{\\sigma^{2}}.\n$$\nTherefore, using $\\mathbb{E}[(X-\\mu)^{2}]=\\sigma^{2}$,\n$$\nI(\\mu)=\\mathbb{E}\\!\\left[\\left(\\frac{\\partial}{\\partial \\mu}\\ln p(X;\\mu,\\sigma^{2})\\right)^{2}\\right]\n=\\mathbb{E}\\!\\left[\\left(\\frac{X-\\mu}{\\sigma^{2}}\\right)^{2}\\right]\n=\\frac{1}{\\sigma^{4}}\\mathbb{E}[(X-\\mu)^{2}]=\\frac{1}{\\sigma^{2}}.\n$$\n\nDifferential entropy:\nBy definition,\n$$\nh(X)=-\\mathbb{E}[\\ln p(X;\\mu,\\sigma^{2})]\n=\\mathbb{E}\\!\\left[\\frac{1}{2}\\ln(2\\pi\\sigma^{2})+\\frac{(X-\\mu)^{2}}{2\\sigma^{2}}\\right]\n=\\frac{1}{2}\\ln(2\\pi\\sigma^{2})+\\frac{1}{2},\n$$\nwhere we used $\\mathbb{E}[(X-\\mu)^{2}]=\\sigma^{2}$. Equivalently,\n$$\nh(X)=\\frac{1}{2}\\ln\\!\\big(2\\pi\\,\\exp(1)\\,\\sigma^{2}\\big).\n$$\nHence,\n$$\n\\exp\\big(2h(X)\\big)=\\exp\\!\\left(\\ln\\!\\big(2\\pi\\,\\exp(1)\\,\\sigma^{2}\\big)\\right)=2\\pi\\,\\exp(1)\\,\\sigma^{2}.\n$$\n\nProduct:\n$$\nI(\\mu)\\cdot \\exp\\big(2h(X)\\big)=\\frac{1}{\\sigma^{2}}\\cdot\\big(2\\pi\\,\\exp(1)\\,\\sigma^{2}\\big)=2\\pi\\,\\exp(1).\n$$\nThis final expression depends only on fundamental constants.",
            "answer": "$$\\boxed{2 \\pi \\exp(1)}$$"
        }
    ]
}