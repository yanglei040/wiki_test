{
    "hands_on_practices": [
        {
            "introduction": "To begin our exploration of Pinsker's inequality, we will start with the most fundamental case: comparing two simple Bernoulli distributions. This exercise provides a concrete opportunity to apply the definitions of Kullback-Leibler (KL) divergence and Total Variation (TV) distance. By calculating both sides of the inequality for a specific numerical example, you will build a foundational intuition for how these two important measures of statistical distance relate to one another .",
            "id": "1646398",
            "problem": "In a digital manufacturing process, a component is designed to be in one of two states, labeled 0 and 1. The ideal manufacturing specification dictates that the probability of a component being in state 1 is exactly $p$. This can be modeled by a Bernoulli distribution, let's call it $P$, with a probability mass function $P(1) = p$ and $P(0) = 1-p$.\n\nAfter a period of operation, a quality control check reveals that the process has drifted. The components now follow a different Bernoulli distribution, $Q$, with a probability mass function $Q(1) = q$ and $Q(0) = 1-q$.\n\nTo quantify the statistical difference between the ideal distribution $P$ and the actual distribution $Q$, two important metrics from information theory are used:\n\n1.  The Total Variation Distance, defined as $TV(P, Q) = \\frac{1}{2} \\sum_{x \\in \\{0, 1\\}} |P(x) - Q(x)|$.\n2.  The Kullback-Leibler (KL) Divergence, defined as $D_{KL}(P || Q) = \\sum_{x \\in \\{0, 1\\}} P(x) \\ln\\left(\\frac{P(x)}{Q(x)}\\right)$.\n\nThese two measures are related by Pinsker's inequality, which states that $D_{KL}(P || Q) \\ge 2[TV(P, Q)]^2$. The term $2[TV(P, Q)]^2$ acts as a theoretical lower bound for the KL divergence.\n\nSuppose the ideal specification is $p=0.5$, but the actual observed process has drifted to $q=0.8$. Calculate the ratio of the actual KL divergence to the theoretical lower bound given by Pinsker's inequality. That is, compute the value of the ratio $R = \\frac{D_{KL}(P || Q)}{2[TV(P, Q)]^2}$.\n\nRound your final answer to three significant figures.",
            "solution": "The total variation distance between two Bernoulli distributions with parameters $p$ and $q$ is\n$$\nTV(P,Q)=\\frac{1}{2}\\left(|p-q|+|(1-p)-(1-q)|\\right)=\\frac{1}{2}\\left(|p-q|+|q-p|\\right)=|p-q|.\n$$\nFor $p=\\frac{1}{2}$ and $q=\\frac{4}{5}$,\n$$\nTV(P,Q)=\\left|\\frac{1}{2}-\\frac{4}{5}\\right|=\\left|\\frac{5-8}{10}\\right|=\\frac{3}{10}.\n$$\nPinsker's lower bound is\n$$\n2[TV(P,Q)]^{2}=2\\left(\\frac{3}{10}\\right)^{2}=2\\cdot\\frac{9}{100}=\\frac{9}{50}.\n$$\nThe Kullback-Leibler divergence for Bernoulli distributions is\n$$\nD_{KL}(P\\|Q)=p\\ln\\!\\left(\\frac{p}{q}\\right)+(1-p)\\ln\\!\\left(\\frac{1-p}{1-q}\\right).\n$$\nSubstituting $p=\\frac{1}{2}$ and $q=\\frac{4}{5}$ gives\n$$\nD_{KL}(P\\|Q)=\\frac{1}{2}\\ln\\!\\left(\\frac{\\frac{1}{2}}{\\frac{4}{5}}\\right)+\\frac{1}{2}\\ln\\!\\left(\\frac{\\frac{1}{2}}{\\frac{1}{5}}\\right)=\\frac{1}{2}\\left[\\ln\\!\\left(\\frac{5}{8}\\right)+\\ln\\!\\left(\\frac{5}{2}\\right)\\right]=\\frac{1}{2}\\ln\\!\\left(\\frac{25}{16}\\right)=\\ln\\!\\left(\\frac{5}{4}\\right).\n$$\nTherefore, the ratio is\n$$\nR=\\frac{D_{KL}(P\\|Q)}{2[TV(P,Q)]^{2}}=\\frac{\\ln\\!\\left(\\frac{5}{4}\\right)}{\\frac{9}{50}}=\\frac{50}{9}\\ln\\!\\left(\\frac{5}{4}\\right)\\approx 1.239686\\ldots\n$$\nRounded to three significant figures, $R=1.24$.",
            "answer": "$$\\boxed{1.24}$$"
        },
        {
            "introduction": "While Pinsker's inequality provides a guaranteed lower bound for KL divergence, it is not always a tight one. This next practice explores a scenario where the bound is \"loose,\" meaning it is not very informative about the actual value of the total variation distance. By working through this example , you will gain a more nuanced understanding of the inequality's practical utility and see how certain distributions can lead to a large divergence.",
            "id": "1646412",
            "problem": "In information theory, we often need to quantify the \"difference\" between two probability distributions. Two common measures are the total variation distance and the Kullback-Leibler (KL) divergence.\n\nConsider two probability distributions, $P$ and $Q$, defined over the same discrete sample space $\\mathcal{X} = \\{x_1, x_2, x_3\\}$. The probabilities are given as:\n- $P(x_1) = 0.90$, $P(x_2) = 0.05$, $P(x_3) = 0.05$\n- $Q(x_1) = 0.01$, $Q(x_2) = 0.495$, $Q(x_3) = 0.495$\n\nThe total variation distance between $P$ and $Q$ is defined as:\n$$TV(P, Q) = \\frac{1}{2} \\sum_{i=1}^{3} |P(x_i) - Q(x_i)|$$\nThe Kullback-Leibler (KL) divergence, which measures how one probability distribution diverges from a second, expected probability distribution, is defined as:\n$$D_{KL}(P || Q) = \\sum_{i=1}^{3} P(x_i) \\ln\\left(\\frac{P(x_i)}{Q(x_i)}\\right)$$\nwhere $\\ln$ denotes the natural logarithm.\n\nPinsker's inequality provides an upper bound on the total variation distance in terms of the KL divergence:\n$$TV(P, Q) \\leq \\sqrt{\\frac{1}{2} D_{KL}(P || Q)}$$\n\nThe term on the right-hand side, $\\sqrt{\\frac{1}{2} D_{KL}(P || Q)}$, serves as an upper bound for $TV(P, Q)$. For the given distributions $P$ and $Q$, calculate the numerical value of this upper bound. Round your final answer to three significant figures.",
            "solution": "We use the given definitions. First compute the KL divergence:\n$$\nD_{KL}(P\\|Q)=\\sum_{i=1}^{3} P(x_i)\\ln\\left(\\frac{P(x_i)}{Q(x_i)}\\right)\n=0.90\\ln\\left(\\frac{0.90}{0.01}\\right)+0.05\\ln\\left(\\frac{0.05}{0.495}\\right)+0.05\\ln\\left(\\frac{0.05}{0.495}\\right).\n$$\nSimplify the ratios:\n$$\n\\frac{0.90}{0.01}=90,\\qquad \\frac{0.05}{0.495}=\\frac{10}{99}.\n$$\nHence,\n$$\nD_{KL}(P\\|Q)=0.90\\ln(90)+0.10\\ln\\left(\\frac{10}{99}\\right).\n$$\nEvaluate the logarithms numerically:\n$$\n\\ln(90)=4.499809670330265\\ldots,\\qquad \\ln\\left(\\frac{10}{99}\\right)=-2.292534757140543\\ldots,\n$$\nso\n$$\nD_{KL}(P\\|Q)\\approx 0.90\\times 4.499809670330265+0.10\\times(-2.292534757140543)\n=4.049828703297239-0.229253475714054\n=3.820575227583185.\n$$\nPinskerâ€™s inequality gives the upper bound\n$$\n\\sqrt{\\frac{1}{2}D_{KL}(P\\|Q)}=\\sqrt{\\frac{3.820575227583185}{2}}=\\sqrt{1.910287613791592}\\approx 1.38213155\\ldots\n$$\nRounded to three significant figures, the upper bound is $1.38$.",
            "answer": "$$\\boxed{1.38}$$"
        },
        {
            "introduction": "Our final practice takes a more theoretical perspective by using the tools of calculus to examine the relationship between KL divergence and squared TV distance. By analyzing how these two metrics behave as one distribution is infinitesimally perturbed from another, you will uncover why a quadratic relationship emerges . This exercise provides a deeper insight into the local structure of information-theoretic distances and reveals the mathematical underpinnings of Pinsker's inequality.",
            "id": "1646421",
            "problem": "In information theory and statistics, various metrics are used to quantify the \"difference\" between two probability distributions. Two of the most fundamental are the Kullback-Leibler divergence and the Total Variation distance. This problem explores the local relationship between these two measures for the family of Bernoulli distributions.\n\nLet a Bernoulli distribution, denoted `Bern(p)`, for a binary random variable $X \\in \\{0, 1\\}$ be defined by the probability mass function $P(X=1) = p$ and $P(X=0) = 1-p$, for a parameter $p \\in (0, 1)$.\n\nThe Kullback-Leibler (KL) divergence from a discrete probability distribution $Q$ to a distribution $P$ over a sample space $\\Omega$ is defined as:\n$$ D_{KL}(P \\| Q) = \\sum_{x \\in \\Omega} P(x) \\ln\\left(\\frac{P(x)}{Q(x)}\\right) $$\nThe Total Variation (TV) distance between two discrete probability distributions $P$ and $Q$ over a sample space $\\Omega$ is defined as:\n$$ TV(P, Q) = \\frac{1}{2} \\sum_{x \\in \\Omega} |P(x) - Q(x)| $$\n\nConsider a fixed reference distribution $P_0 = \\text{Bern}(p_0)$ and a variable distribution $P_p = \\text{Bern}(p)$. We define two functions of $p$:\n1.  $g(p) = D_{KL}(P_0 \\| P_p)$\n2.  $f(p) = \\left( TV(P_0, P_p) \\right)^2$\n\nBoth functions measure the discrepancy between the two distributions and are equal to zero when $p = p_0$. We are interested in comparing how fast these two functions grow as $p$ starts to deviate from $p_0$.\n\nFor a reference distribution with parameter $p_0 = 0.25$, calculate the value of the following limit:\n$$ L = \\lim_{p \\to p_0} \\frac{g(p)}{f(p)} $$\nProvide the value of $L$ as a real number. Round your final answer to four significant figures.",
            "solution": "We work with $P_{0}=\\text{Bern}(p_{0})$ and $P_{p}=\\text{Bern}(p)$, where $p_{0}=0.25$. By definition,\n$$\ng(p)=D_{KL}(P_{0}\\|P_{p})=p_{0}\\ln\\!\\left(\\frac{p_{0}}{p}\\right)+(1-p_{0})\\ln\\!\\left(\\frac{1-p_{0}}{1-p}\\right).\n$$\nFor Bernoulli distributions, the Total Variation distance is\n$$\nTV(P_0, P_p)=\\frac{1}{2}\\sum_{x\\in\\{0,1\\}}|P_{0}(x)-P_{p}(x)|=\\frac{1}{2}\\left(|p_{0}-p|+|(1-p_{0})-(1-p)|\\right)=|p-p_{0}|,\n$$\nso\n$$\nf(p)=\\left(TV(P_0, P_p)\\right)^{2}=(p-p_{0})^{2}.\n$$\n\nWe expand $g(p)$ around $p=p_{0}$. Differentiate:\n$$\ng'(p)=\\frac{d}{dp}\\left[-p_{0}\\ln p-(1-p_{0})\\ln(1-p)\\right]=-\\,\\frac{p_{0}}{p}+\\frac{1-p_{0}}{1-p},\n$$\nso\n$$\ng'(p_{0})=-\\,\\frac{p_{0}}{p_{0}}+\\frac{1-p_{0}}{1-p_{0}}=-1+1=0.\n$$\nSecond derivative:\n$$\ng''(p)=\\frac{p_{0}}{p^{2}}+\\frac{1-p_{0}}{(1-p)^{2}},\n$$\nhence\n$$\ng''(p_{0})=\\frac{p_{0}}{p_{0}^{2}}+\\frac{1-p_{0}}{(1-p_{0})^{2}}=\\frac{1}{p_{0}}+\\frac{1}{1-p_{0}}=\\frac{1}{p_{0}(1-p_{0})}.\n$$\nBy the Taylor expansion with $g'(p_{0})=0$, we have\n$$\ng(p)=\\frac{1}{2}g''(p_{0})(p-p_{0})^{2}+o\\!\\left((p-p_{0})^{2}\\right).\n$$\nTherefore,\n$$\nL=\\lim_{p\\to p_{0}}\\frac{g(p)}{f(p)}=\\lim_{p\\to p_{0}}\\frac{\\frac{1}{2}g''(p_{0})(p-p_{0})^{2}+o\\!\\left((p-p_{0})^{2}\\right)}{(p-p_{0})^{2}}=\\frac{1}{2}g''(p_{0})=\\frac{1}{2p_{0}(1-p_{0})}.\n$$\nWith $p_{0}=0.25$, we have $p_{0}(1-p_{0})=\\frac{1}{4}\\cdot\\frac{3}{4}=\\frac{3}{16}$, so\n$$\nL=\\frac{1}{2\\cdot\\frac{3}{16}}=\\frac{1}{\\frac{3}{8}}=\\frac{8}{3}\\approx 2.666666\\ldots\n$$\nRounded to four significant figures, $L=2.667$.",
            "answer": "$$\\boxed{2.667}$$"
        }
    ]
}