{
    "hands_on_practices": [
        {
            "introduction": "The method of types groups all sequences with the same empirical distribution into a single set called a type class. But what does a typical sequence *within* such a class look like? This first exercise moves beyond simple counting to explore the structural properties of these sequences, asking you to calculate the expected number of alternating runs, which provides insight into the randomness of sequences constrained to a specific type. ",
            "id": "1641277",
            "problem": "Consider a binary alphabet $\\mathcal{X} = \\{0, 1\\}$. The *type* of a sequence $x^n = (x_1, x_2, \\dots, x_n)$ is its empirical probability distribution, defined by $P_{x^n}(a) = \\frac{1}{n} N(a|x^n)$ for $a \\in \\mathcal{X}$, where $N(a|x^n)$ is the number of times the symbol $a$ appears in the sequence $x^n$.\n\nLet $P$ be a specific probability distribution on $\\mathcal{X}$ given by $P(1) = p$ and $P(0) = 1-p$, where $0 \\leq p \\leq 1$. The *type class* of $P$, denoted as $T_P^{(n)}$, is the set of all binary sequences of length $n$ whose type is $P$. For a type class to be well-defined and non-empty, we assume that $np$ is an integer.\n\nAn *alternating run* in a sequence is a contiguous subsequence of identical symbols. For instance, the sequence `11101100` consists of four alternating runs: `111`, `0`, `11`, and `00`.\n\nSuppose a sequence $X^n$ is chosen uniformly at random from the type class $T_P^{(n)}$. Determine the expected number of alternating runs in the sequence $X^n$. Express your final answer as a closed-form analytic expression in terms of $n$ and $p$.",
            "solution": "Let $k = np$ denote the number of ones in the type class (assumed integer), so there are $n-k$ zeros. For any binary sequence $x^{n}$, the number of alternating runs can be written as\n$$\nR(x^{n}) = 1 + \\sum_{i=1}^{n-1} \\mathbf{1}\\{x_{i} \\neq x_{i+1}\\}.\n$$\nTaking expectation with respect to the uniform distribution over the type class $T_{P}^{(n)}$ and using linearity of expectation gives\n$$\n\\mathbb{E}[R(X^{n})] = 1 + \\sum_{i=1}^{n-1} \\Pr\\{X_{i} \\neq X_{i+1}\\}.\n$$\nBy symmetry of the uniform distribution over permutations of the multiset with $k$ ones and $n-k$ zeros, $\\Pr\\{X_{i} \\neq X_{i+1}\\}$ is the same for all $i$, so denote this probability by $q$. Then\n$$\n\\mathbb{E}[R(X^{n})] = 1 + (n-1)q.\n$$\nTo compute $q$, note that the adjacent pair $(X_{i},X_{i+1})$ has the same law as two draws without replacement from an urn with $k$ ones and $n-k$ zeros. Hence\n$$\n\\Pr\\{X_{i} \\neq X_{i+1}\\} = \\Pr\\{01\\} + \\Pr\\{10\\}\n= \\frac{n-k}{n}\\cdot\\frac{k}{n-1} + \\frac{k}{n}\\cdot\\frac{n-k}{n-1}\n= \\frac{2k(n-k)}{n(n-1)}.\n$$\nTherefore,\n$$\n\\mathbb{E}[R(X^{n})] = 1 + (n-1)\\cdot \\frac{2k(n-k)}{n(n-1)} = 1 + \\frac{2k(n-k)}{n}.\n$$\nSubstituting $k = np$ yields\n$$\n\\mathbb{E}[R(X^{n})] = 1 + \\frac{2(np)\\big(n - np\\big)}{n} = 1 + 2n p(1 - p).\n$$\nThis expression is valid for all $0 \\leq p \\leq 1$ with $np$ an integer, and it satisfies the boundary cases $p=0$ or $p=1$, where the sequence is constant and the expected number of runs is $1$.",
            "answer": "$$\\boxed{1 + 2 n p (1 - p)}$$"
        },
        {
            "introduction": "Having explored the properties within a single type class, we now consider the entire space of possible types. In many practical applications, like data compression, we can't store information about every possible sequence; instead, we can represent a sequence by its nearest \"codebook\" type. This problem challenges you to design an efficient codebook by determining the minimum number of representative types needed to cover all possibilities within a given distortion, a core concept in vector quantization and universal source coding. ",
            "id": "1641276",
            "problem": "Consider a system designed for universal lossy compression of binary data. The system operates on blocks of length $n=1200$. The compression scheme is based on the method of types. For any binary sequence $x^n = (x_1, x_2, \\dots, x_n)$, its empirical probability distribution, known as its 'type', is given by $P_{x^n} = (\\frac{N_0}{n}, \\frac{N_1}{n})$, where $N_0$ and $N_1$ are the number of 0s and 1s in the sequence, respectively. A type is uniquely determined by the proportion of 1s, denoted $\\pi = N_1/n$.\n\nThe system uses a fixed codebook of $M$ reconstruction types, $\\mathcal{C} = \\{\\hat{\\pi}_1, \\hat{\\pi}_2, \\dots, \\hat{\\pi}_M\\}$. Each reconstruction type $\\hat{\\pi}_j$ must also correspond to a valid binary sequence of length $n$, meaning each $\\hat{\\pi}_j$ must be of the form $k/n$ for some integer $k \\in \\{0, 1, \\dots, n\\}$.\n\nThe distortion between a source type $\\pi_s$ and a reconstruction type $\\pi_r$ is defined as the absolute difference of their proportions of 1s: $d(\\pi_s, \\pi_r) = |\\pi_s - \\pi_r|$.\n\nYour task is to determine the minimum number of reconstruction types, $M$, required in the codebook to ensure that for any possible source sequence $x^n$, there is at least one reconstruction type $\\hat{\\pi}_j \\in \\mathcal{C}$ such that the distortion is within a maximum tolerance $\\delta = 0.03$. In other words, for any possible source type $\\pi$, there must exist some $\\hat{\\pi}_j \\in \\mathcal{C}$ satisfying $d(\\pi, \\hat{\\pi}_j) \\le \\delta$.\n\nThe final answer must be a single integer.",
            "solution": "Let the set of all possible source types be $\\{\\pi_{k} : \\pi_{k} = k/n,\\ k \\in \\{0,1,\\dots,n\\}\\}$; there are $n+1$ such types. A reconstruction type is also of the form $\\hat{\\pi} = k_{r}/n$ with $k_{r} \\in \\{0,1,\\dots,n\\}$.\n\nFor a source type $\\pi_{s} = k_{s}/n$ and a reconstruction type $\\pi_{r} = k_{r}/n$, the distortion is\n$$\nd(\\pi_{s},\\pi_{r}) = \\left|\\frac{k_{s}}{n} - \\frac{k_{r}}{n}\\right| = \\frac{|k_{s} - k_{r}|}{n}.\n$$\nThe requirement $d(\\pi_{s},\\pi_{r}) \\le \\delta$ is equivalent to\n$$\n|k_{s} - k_{r}| \\le n\\delta.\n$$\nSince $k_{s}$ and $k_{r}$ are integers, define $t \\triangleq \\lfloor n\\delta \\rfloor$. Then any codebook center $k_{r}$ covers all source indices $k_{s} \\in [k_{r} - t,\\, k_{r} + t] \\cap \\{0,1,\\dots,n\\}$, which is at most $2t+1$ indices. Therefore, to cover all $n+1$ indices, any codebook must have\n$$\nM \\ge \\left\\lceil \\frac{n+1}{2t+1} \\right\\rceil.\n$$\n\nAchievability with this bound is obtained by placing centers greedily. Choose centers at\n$$\nk_{j} = t + (j-1)(2t+1)\\quad \\text{for } j=1,2,\\dots,J-1,\n$$\nas long as $k_{j} \\le n - t$, and finally place the last center at $k_{J} = n - t$. This covers consecutive blocks of size $2t+1$ and, if needed, a final overlapping block up to $n$, yielding $J = \\left\\lceil \\frac{n+1}{2t+1} \\right\\rceil$ centers.\n\nNow substitute the given parameters $n=1200$ and $\\delta = 0.03$. Then\n$$\nn\\delta = 1200 \\times 0.03 = 36 \\quad \\Rightarrow \\quad t = \\lfloor 36 \\rfloor = 36,\\quad 2t+1 = 73,\\quad n+1 = 1201.\n$$\nHence\n$$\nM_{\\min} = \\left\\lceil \\frac{1201}{73} \\right\\rceil = \\lceil 16.452... \\rceil = 17.\n$$\nThis is achievable, for example, by centers at $k_{j} = 36 + (j-1)\\cdot 73$ for $j=1,\\dots,16$ and $k_{17} = 1164$, which together cover all $k \\in \\{0,1,\\dots,1200\\}$ within index distance $36$. The calculation is $\\lceil 1201/73 \\rceil = 17$.",
            "answer": "$$\\boxed{17}$$"
        },
        {
            "introduction": "The empirical distribution is not just a descriptive summary of a sequence; it's a powerful tool for statistical inference. When a sequence is generated by an unknown source, its type can provide crucial evidence about the source's identity. This exercise puts you in the role of an analyst tasked with deciding which of two possible sources generated a long data sequence, using only its empirical distribution to make a statistically optimal decision. ",
            "id": "1641267",
            "problem": "A deep space probe transmits data as a sequence of binary digits from the alphabet $\\mathcal{X} = \\{0, 1\\}$. The transmitter is known to be operating in one of two distinct modes, Mode 1 or Mode 2. In either mode, the digits are generated by a memoryless source.\n\n- In Mode 1, the source is a Bernoulli process that generates a '1' with probability $p_1$.\n- In Mode 2, the source is a Bernoulli process that generates a '1' with probability $p_2$.\n\nIt is given that $0 < p_2 < p_1 < 1$.\n\nAn analyst receives a very long sequence of digits, $x^n = (x_1, x_2, \\dots, x_n)$, and knows it was generated entirely by one of the two modes. To determine the origin of the sequence, the analyst computes its empirical distribution, which is fully characterized by $\\hat{p}$, the observed frequency of the symbol '1' in the sequence.\n\nThe decision of which source is the more likely origin for the sequence leads to a simple threshold rule. There exists a critical threshold value, $T$, such that if the observed frequency $\\hat{p}$ is greater than $T$, Mode 1 is the more likely source, whereas if $\\hat{p}$ is less than $T$, Mode 2 is more likely.\n\nDetermine the analytical expression for this threshold $T$ as a function of $p_1$ and $p_2$. Use the natural logarithm in your final expression.",
            "solution": "We model the problem as a binary hypothesis test with hypotheses $H_{1}: p = p_{1}$ and $H_{2}: p = p_{2}$, where a length-$n$ sequence $x^{n}$ with $k$ ones (so that $\\hat{p} = k/n$) has likelihood under $H_{i}$ given by $L_{i} = p_{i}^{k} (1 - p_{i})^{n - k}$ for $i \\in \\{1, 2\\}$. The sequence is more likely under Mode 1 than Mode 2 if and only if $L_{1} > L_{2}$, equivalently if and only if the log-likelihood ratio is positive:\n$$\n\\ln\\left(\\frac{L_{1}}{L_{2}}\\right) \\;=\\; k \\ln\\left(\\frac{p_{1}}{p_{2}}\\right) + (n - k) \\ln\\left(\\frac{1 - p_{1}}{1 - p_{2}}\\right).\n$$\nSubstituting $k = n \\hat{p}$ and dividing by $n > 0$ yields the inequality\n$$\n\\hat{p} \\ln\\left(\\frac{p_{1}}{p_{2}}\\right) + (1 - \\hat{p}) \\ln\\left(\\frac{1 - p_{1}}{1 - p_{2}}\\right) > 0.\n$$\nThis is linear in $\\hat{p}$, so the decision reduces to comparing $\\hat{p}$ to the threshold $T$ obtained by setting the left-hand side to zero:\n$$\n\\hat{p} \\left[ \\ln\\left(\\frac{p_{1}}{p_{2}}\\right) - \\ln\\left(\\frac{1 - p_{1}}{1 - p_{2}}\\right) \\right] = - \\ln\\left(\\frac{1 - p_{1}}{1 - p_{2}}\\right).\n$$\nRearranging and solving for the threshold $T=\\hat{p}$:\n$$\nT \\;=\\; \\frac{ - \\ln\\left(\\frac{1 - p_{1}}{1 - p_{2}}\\right) }{ \\ln\\left(\\frac{p_{1}}{p_{2}}\\right) - \\ln\\left(\\frac{1 - p_{1}}{1 - p_{2}}\\right) } \\;=\\; \\frac{ \\ln\\left(\\frac{1 - p_{2}}{1 - p_{1}}\\right) }{ \\ln\\left(\\frac{ p_{1} (1 - p_{2}) }{ p_{2} (1 - p_{1}) }\\right) }.\n$$\nGiven $0 < p_{2} < p_{1} < 1$, the denominator is positive, ensuring a unique threshold. The rule “choose Mode 1 if $\\hat{p} > T$ and Mode 2 if $\\hat{p} < T$” follows because the coefficient of $\\hat{p}$ in the log-likelihood ratio inequality is positive.",
            "answer": "$$\\boxed{\\frac{\\ln\\left(\\frac{1 - p_{2}}{1 - p_{1}}\\right)}{\\ln\\left(\\frac{p_{1}(1 - p_{2})}{p_{2}(1 - p_{1})}\\right)}}$$"
        }
    ]
}