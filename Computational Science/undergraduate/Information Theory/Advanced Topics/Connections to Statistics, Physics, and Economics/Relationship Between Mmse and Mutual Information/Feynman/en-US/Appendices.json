{
    "hands_on_practices": [
        {
            "introduction": "Before diving into complex scenarios, it is often insightful to test a powerful relationship with the simplest case imaginable. This exercise examines the transmission of a non-random, constant signal over a noisy channel. By exploring this degenerate case , you will verify how the fundamental I-MMSE relationship behaves, providing a solid conceptual anchor for more complex problems.",
            "id": "1654365",
            "problem": "Consider a simple communication system designed to transmit a constant DC voltage level. The input signal to the channel is a fixed, non-random value $X = c$, where $c$ is a real constant. The channel is an Additive White Gaussian Noise (AWGN) channel, so the received signal $Y$ is given by $Y = X + N$. The noise $N$ is a Gaussian random variable with a mean of zero and a variance of $\\sigma^2$, and it is statistically independent of the input $X$.\n\nTwo key performance metrics for such a system are:\n1.  The Minimum Mean Square Error, $\\text{mmse}(X|Y)$, which represents the lowest possible expected squared error in estimating the input signal $X$ after observing the output $Y$. It is formally defined as $\\text{mmse}(X|Y) = E[(X - E[X|Y])^2]$.\n2.  The rate of change of the mutual information $I(X;Y)$ with respect to the noise variance. The mutual information measures the statistical dependency between $X$ and $Y$.\n\nYour task is to determine the value of the MMSE, $\\text{mmse}(X|Y)$, and the value of the derivative $\\frac{d}{d(\\sigma^2)}I(X;Y)$.\n\nWhich of the following pairs correctly identifies these two quantities?\n\nA. ($\\text{mmse}(X|Y) = 0$, $\\frac{d}{d(\\sigma^2)}I(X;Y) = 0$)\n\nB. ($\\text{mmse}(X|Y) = c^2$, $\\frac{d}{d(\\sigma^2)}I(X;Y) = \\frac{1}{2\\sigma^2}$)\n\nC. ($\\text{mmse}(X|Y) = \\sigma^2$, $\\frac{d}{d(\\sigma^2)}I(X;Y) = 0$)\n\nD. ($\\text{mmse}(X|Y) = 0$, $\\frac{d}{d(\\sigma^2)}I(X;Y) = -\\frac{1}{2\\sigma^2}$)\n\nE. ($\\text{mmse}(X|Y) = \\frac{c^2 \\sigma^2}{c^2+\\sigma^2}$, $\\frac{d}{d(\\sigma^2)}I(X;Y) = \\frac{c^2}{2(c^2+\\sigma^2)^2}$)",
            "solution": "This problem asks for two quantities related to a constant signal $X=c$ transmitted over an AWGN channel: the Minimum Mean Square Error (MMSE) and the derivative of the mutual information with respect to the noise variance $\\sigma^2$.\n\n**Part 1: Calculation of the Minimum Mean Square Error (MMSE)**\n\nThe MMSE of estimating $X$ given $Y$ is defined as $\\text{mmse}(X|Y) = E[(X - \\hat{X}_{\\text{MMSE}}(Y))^2]$, where the MMSE estimator is the conditional expectation $\\hat{X}_{\\text{MMSE}}(Y) = E[X|Y]$.\n\nIn this specific problem, the input signal $X$ is a constant value, $X=c$. We need to find the conditional expectation of this constant value given the observation $Y$.\nThe expectation of a constant is just the constant itself, regardless of any conditioning. Therefore,\n$$\n\\hat{X}_{\\text{MMSE}}(Y) = E[X|Y] = E[c|Y] = c\n$$\nThis result is intuitive: if we know for a fact that the transmitted signal was $c$, the best possible estimate for it is $c$, no matter what noisy value $Y$ we receive.\n\nNow we substitute this estimator back into the definition of the MMSE:\n$$\n\\text{mmse}(X|Y) = E[(X - \\hat{X}_{\\text{MMSE}}(Y))^2] = E[(c - c)^2] = E[0^2] = 0\n$$\nSo, the Minimum Mean Square Error is 0.\n\n**Part 2: Calculation of the derivative of the mutual information**\n\nThe mutual information $I(X; Y)$ for continuous random variables can be expressed in terms of differential entropies as:\n$$\nI(X; Y) = h(Y) - h(Y|X)\n$$\nLet's calculate each term.\n\nFirst, we find the conditional differential entropy $h(Y|X)$.\nGiven a specific value of the input $X=x$, the output is $Y = x + N$. Since $N$ is a Gaussian random variable with mean 0 and variance $\\sigma^2$, $Y$ conditioned on $X=x$ is a Gaussian random variable with mean $x$ and variance $\\sigma^2$.\nThe differential entropy of a Gaussian random variable with variance $V$ is given by the formula $h = \\frac{1}{2}\\ln(2\\pi e V)$.\nSo, for a given $X=x$, the conditional entropy of $Y$ is:\n$$\nh(Y|X=x) = \\frac{1}{2}\\ln(2\\pi e \\sigma^2)\n$$\nSince this expression does not depend on the specific value $x$, the overall conditional entropy $h(Y|X)$, which is the expectation of $h(Y|X=x)$ over all possible values of $X$, is simply:\n$$\nh(Y|X) = E_X[h(Y|X=x)] = \\frac{1}{2}\\ln(2\\pi e \\sigma^2)\n$$\n\nNext, we find the differential entropy of the output, $h(Y)$.\nThe input signal is always $X=c$. Therefore, the output signal is always $Y = c + N$.\nSince $N$ is a Gaussian random variable with mean 0 and variance $\\sigma^2$, $Y$ is a Gaussian random variable with mean $c$ and variance $\\sigma^2$.\nUsing the same formula for the entropy of a Gaussian, the entropy of $Y$ is:\n$$\nh(Y) = \\frac{1}{2}\\ln(2\\pi e \\sigma^2)\n$$\n\nNow we can compute the mutual information:\n$$\nI(X; Y) = h(Y) - h(Y|X) = \\frac{1}{2}\\ln(2\\pi e \\sigma^2) - \\frac{1}{2}\\ln(2\\pi e \\sigma^2) = 0\n$$\nThis result is also intuitive. Since the input $X$ is a known constant, there is no uncertainty about it to begin with. Observing the noisy output $Y$ cannot provide any \"information\" about $X$ that we do not already possess. Hence, the mutual information is zero.\n\nFinally, we need to find the derivative of the mutual information with respect to the noise variance $\\sigma^2$. Since $I(X; Y) = 0$ (a constant), its derivative with respect to any variable is zero.\n$$\n\\frac{d}{d(\\sigma^2)}I(X;Y) = \\frac{d}{d(\\sigma^2)}(0) = 0\n$$\n\n**Conclusion**\n\nWe have found that $\\text{mmse}(X|Y) = 0$ and $\\frac{d}{d(\\sigma^2)}I(X;Y) = 0$. Comparing this result with the given options, we find that option A matches our calculations.\nThis specific case is a simple illustration of de Bruijn's identity, which states $\\frac{d}{d\\alpha} I(X; X+\\sqrt{\\alpha}Z) = \\frac{1}{2} \\text{mmse}(X|X+\\sqrt{\\alpha}Z)$, where $Z \\sim \\mathcal{N}(0,1)$ and $\\alpha = \\sigma^2$. Our results show that $0 = \\frac{1}{2} \\times 0$, satisfying the identity.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Understanding a communication channel's behavior at the limit of zero Signal-to-Noise Ratio (SNR) is critical for system design. This practice explores what happens when noise completely dominates the signal, rendering the observation uninformative. You will discover that in this scenario , the best possible estimate relies only on the signal's prior statistics, connecting the Minimum Mean-Square Error (MMSE) directly to the signal's variance.",
            "id": "1654355",
            "problem": "In digital communication, understanding channel performance at very low Signal-to-Noise Ratios (SNRs) is crucial for designing robust systems. For an Additive White Gaussian Noise (AWGN) channel, the slope of the mutual information curve with respect to SNR near zero is determined by a fundamental quantity: the Minimum Mean Square Error (MMSE) evaluated at zero SNR.\n\nConsider a communication system that transmits a signal $X$. The signal is a random variable that takes values from the set $\\{-A, 0, A\\}$, where $A=5.0$. The probability distribution for $X$ is given by $P(X = A) = 0.25$, $P(X = -A) = 0.25$, and $P(X=0)=0.5$.\n\nThe signal is transmitted over an AWGN channel, and the received signal is $Y = X + Z$. Here, $Z$ is a zero-mean Gaussian noise variable with variance $\\sigma^2$, independent of $X$. The SNR is defined as the ratio of the average signal power to the noise power, $\\text{SNR} = \\frac{E[X^2]}{\\sigma^2}$.\n\nThe MMSE is the minimum possible average squared error of any estimator of $X$ based on the observation $Y$, and is given by $\\text{mmse} = E[(X - E[X|Y])^2]$.\n\nCalculate the value of the MMSE in the limit as the SNR approaches zero.",
            "solution": "The problem requires us to compute the Minimum Mean Square Error (MMSE) in the limit of zero Signal-to-Noise Ratio (SNR). The MMSE in estimating a random variable $X$ from an observation $Y$ is given by the expression $\\text{mmse} = E[(X - \\hat{X})^2]$, where the optimal estimator is the conditional mean, $\\hat{X} = E[X|Y]$.\n\nThe SNR is defined as $\\text{SNR} = \\frac{E[X^2]}{\\sigma^2}$, where $E[X^2]$ is the average power of the signal $X$ and $\\sigma^2$ is the variance (power) of the noise $Z$.\n\nFirst, we compute the average power of the signal $X$. The signal $X$ can take values from $\\{-5.0, 0, 5.0\\}$ with probabilities $P(X=-5.0) = 0.25$, $P(X=0) = 0.5$, and $P(X=5.0) = 0.25$.\nThe average signal power, $E[X^2]$, is calculated as:\n$$ E[X^2] = \\sum_{x} x^2 P(X=x) $$\n$$ E[X^2] = (-5.0)^2 P(X=-5.0) + (0)^2 P(X=0) + (5.0)^2 P(X=5.0) $$\n$$ E[X^2] = (25.0)(0.25) + (0)(0.5) + (25.0)(0.25) = 6.25 + 0 + 6.25 = 12.5 $$\nThe average signal power $E[X^2]$ is a finite, positive constant.\n\nThe problem asks for the MMSE in the limit as $\\text{SNR} \\to 0$. We examine the implication of this limit on the noise power $\\sigma^2$:\n$$ \\lim_{\\text{SNR} \\to 0} \\frac{E[X^2]}{\\sigma^2} = \\lim_{\\text{SNR} \\to 0} \\frac{12.5}{\\sigma^2} = 0 $$\nFor this limit to hold, the denominator must approach infinity, so $\\sigma^2 \\to \\infty$. A zero-SNR condition corresponds to infinite noise power.\n\nThe received signal is $Y = X + Z$. In the limit of infinite noise power ($\\sigma^2 \\to \\infty$), the noise term $Z$ completely overwhelms the finite signal term $X$. Consequently, the observed signal $Y$ contains no information about the transmitted signal $X$. In this limit, $X$ and $Y$ become statistically independent.\n\nThe MMSE estimator is the conditional expectation $\\hat{X} = E[X|Y]$. When the observation $Y$ is independent of the signal $X$, the conditional expectation provides no more information than the unconditional expectation. Therefore, the estimator becomes:\n$$ \\lim_{\\sigma^2 \\to \\infty} E[X|Y] = E[X] $$\n\nWe now calculate the unconditional mean of the signal, $E[X]$:\n$$ E[X] = \\sum_{x} x P(X=x) $$\n$$ E[X] = (-5.0) P(X=-5.0) + (0) P(X=0) + (5.0) P(X=5.0) $$\n$$ E[X] = (-5.0)(0.25) + (0)(0.5) + (5.0)(0.25) = -1.25 + 0 + 1.25 = 0 $$\nThus, in the zero-SNR limit, the best estimate of $X$ is its mean value, which is $0$, irrespective of the value of $Y$.\n\nNow, we can find the MMSE in this limit by substituting the limiting estimator into the definition of MMSE:\n$$ \\text{mmse}(\\text{SNR}=0) = \\lim_{\\text{SNR} \\to 0} E[(X - E[X|Y])^2] $$\nBy moving the limit inside the expectation (which is permissible here), we get:\n$$ \\text{mmse}(\\text{SNR}=0) = E\\left[\\left(X - \\lim_{\\text{SNR} \\to 0} E[X|Y]\\right)^2\\right] = E[(X - E[X])^2] $$\nThis final expression is the definition of the variance of $X$, denoted as $\\text{Var}(X)$.\n\nThe variance is calculated using the formula $\\text{Var}(X) = E[X^2] - (E[X])^2$. Using the values we have already computed:\n$$ E[X^2] = 12.5 $$\n$$ E[X] = 0 $$\nThe variance is:\n$$ \\text{Var}(X) = 12.5 - (0)^2 = 12.5 $$\n\nTherefore, the MMSE in the limit as the SNR approaches zero is equal to the variance of the signal $X$, which is 12.5.",
            "answer": "$$\\boxed{12.5}$$"
        },
        {
            "introduction": "Having examined the boundary conditions, we now apply the I-MMSE relationship in its full integral form to a cornerstone of digital communications: the binary signal. This hands-on exercise  guides you through the process of first deriving the MMSE as a function of the noise variance and then using this function to construct the integral for mutual information. It is a powerful demonstration of how the accumulated estimation error across all SNRs quantifies the total information shared between the transmitter and receiver.",
            "id": "1654322",
            "problem": "Consider a simple digital communication system where a binary signal $X$ is drawn from the set $\\{-1, 1\\}$, with $P(X=1) = P(X=-1) = 1/2$. The signal is transmitted through an Additive White Gaussian Noise (AWGN) channel, resulting in a received signal is $Y = X + Z$, where the noise $Z$ is a random variable following a normal distribution with mean 0 and variance $\\sigma^2$, i.e., $Z \\sim \\mathcal{N}(0, \\sigma^2)$. The noise $Z$ is independent of the signal $X$.\n\nA fundamental result in information theory, known as the I-MMSE relation, connects the mutual information between the input and output of this channel to the Minimum Mean Square Error (MMSE) of estimating the input. This relationship is established by considering an auxiliary channel $Y_t = X + \\sqrt{t}W$, where $W \\sim \\mathcal{N}(0, 1)$ is standard Gaussian noise and $t \\ge 0$ is a parameter. Note that the original channel corresponds to the case where $t = \\sigma^2$. The mutual information for the original channel is then given by the integral:\n$$ I(X; Y) = \\frac{1}{2} \\int_0^{\\sigma^2} \\text{mmse}(X|Y_t) dt $$\nHere, $\\text{mmse}(X|Y_t) = E[(X - E[X|Y_t])^2]$ is the MMSE associated with estimating the signal $X$ from the observation $Y_t$.\n\nYour task is to derive a complete and explicit integral expression for the mutual information $I(X; Y)$ as a function of the noise variance $\\sigma^2$. The final expression will be in the form of a nested integral.",
            "solution": "We parameterize the auxiliary Gaussian channel as $Y_{t}=X+\\sqrt{t}W$ with $W\\sim\\mathcal{N}(0,1)$, independent of $X\\in\\{-1,1\\}$, $P(X=\\pm 1)=\\frac{1}{2}$. By the I-MMSE relation, the mutual information in nats for the original channel (with $t=\\sigma^{2}$) is\n$$\nI(X;Y)=\\frac{1}{2}\\int_{0}^{\\sigma^{2}}\\text{mmse}(X|Y_{t})\\,dt,\n$$\nwhere $\\text{mmse}(X|Y_{t})=E\\big[(X-E[X|Y_{t}])^{2}\\big]$.\n\nWe first compute the posterior mean $m_{t}(y)\\triangleq E[X|Y_{t}=y]$ using Bayesâ€™ rule. The conditional densities are\n$$\nf_{Y_{t}|X}(y|x)=\\frac{1}{\\sqrt{2\\pi t}}\\exp\\!\\left(-\\frac{(y-x)^{2}}{2t}\\right),\\quad x\\in\\{-1,1\\}.\n$$\nWith $P(X=\\pm 1)=\\frac{1}{2}$, the posterior probabilities are\n$$\nP(X=1|y)=\\frac{f_{Y_{t}|X}(y|1)}{f_{Y_{t}|X}(y|1)+f_{Y_{t}|X}(y|-1)},\\quad\nP(X=-1|y)=\\frac{f_{Y_{t}|X}(y|-1)}{f_{Y_{t}|X}(y|1)+f_{Y_{t}|X}(y|-1)}.\n$$\nHence the posterior mean is\n$$\nm_{t}(y)=P(X=1|y)-P(X=-1|y)=\\frac{f_{Y_{t}|X}(y|1)-f_{Y_{t}|X}(y|-1)}{f_{Y_{t}|X}(y|1)+f_{Y_{t}|X}(y|-1)}.\n$$\nFactor out $\\exp\\!\\left(-\\frac{y^{2}+1}{2t}\\right)$ to obtain\n$$\n\\frac{\\exp\\!\\left(\\frac{y}{t}\\right)-\\exp\\!\\left(-\\frac{y}{t}\\right)}{\\exp\\!\\left(\\frac{y}{t}\\right)+\\exp\\!\\left(-\\frac{y}{t}\\right)}=\\tanh\\!\\left(\\frac{y}{t}\\right),\n$$\nso\n$$\nm_{t}(y)=\\tanh\\!\\left(\\frac{y}{t}\\right).\n$$\n\nNext, the MMSE can be written using the orthogonality principle. Let $m_{t}(Y_{t})=E[X|Y_{t}]$. Then\n$$\n\\text{mmse}(X|Y_{t})=E\\big[(X-m_{t}(Y_{t}))^{2}\\big]=E[X^{2}]-2E[X\\,m_{t}(Y_{t})]+E[m_{t}(Y_{t})^{2}].\n$$\nSince $m_{t}(Y_{t})=E[X|Y_{t}]$, we have $E[X\\,m_{t}(Y_{t})]=E\\big[E[X|Y_{t}]\\,m_{t}(Y_{t})\\big]=E[m_{t}(Y_{t})^{2}]$. Also $E[X^{2}]=1$ because $X\\in\\{-1,1\\}$. Therefore\n$$\n\\text{mmse}(X|Y_{t})=1-E\\big[m_{t}(Y_{t})^{2}\\big]=1-E\\!\\left[\\tanh^{2}\\!\\left(\\frac{Y_{t}}{t}\\right)\\right].\n$$\nThe marginal density of $Y_{t}$ is the equally weighted Gaussian mixture\n$$\np_{Y_{t}}(y)=\\frac{1}{2}\\frac{1}{\\sqrt{2\\pi t}}\\exp\\!\\left(-\\frac{(y-1)^{2}}{2t}\\right)+\\frac{1}{2}\\frac{1}{\\sqrt{2\\pi t}}\\exp\\!\\left(-\\frac{(y+1)^{2}}{2t}\\right).\n$$\nThus\n$$\nE\\!\\left[\\tanh^{2}\\!\\left(\\frac{Y_{t}}{t}\\right)\\right]=\\int_{-\\infty}^{\\infty}\\tanh^{2}\\!\\left(\\frac{y}{t}\\right)\\,p_{Y_{t}}(y)\\,dy\n$$\nand\n$$\n\\text{mmse}(X|Y_{t})=1-\\int_{-\\infty}^{\\infty}\\tanh^{2}\\!\\left(\\frac{y}{t}\\right)\\,\\frac{1}{2\\sqrt{2\\pi t}}\\left[\\exp\\!\\left(-\\frac{(y-1)^{2}}{2t}\\right)+\\exp\\!\\left(-\\frac{(y+1)^{2}}{2t}\\right)\\right]dy.\n$$\n\nFinally, substitute this into the I-MMSE integral to obtain the desired explicit nested integral expression as a function of $\\sigma^{2}$:\n$$\nI(X;Y)=\\frac{1}{2}\\int_{0}^{\\sigma^{2}}\\left[1-\\int_{-\\infty}^{\\infty}\\tanh^{2}\\!\\left(\\frac{y}{t}\\right)\\,\\frac{1}{2\\sqrt{2\\pi t}}\\left(\\exp\\!\\left(-\\frac{(y-1)^{2}}{2t}\\right)+\\exp\\!\\left(-\\frac{(y+1)^{2}}{2t}\\right)\\right)dy\\right]dt.\n$$\nEquivalently, using $Y_{t}=X+\\sqrt{t}W$ and symmetry, one may write the inner expectation as a single Gaussian integral over $W\\sim\\mathcal{N}(0,1)$:\n$$\nI(X;Y)=\\frac{1}{2}\\int_{0}^{\\sigma^{2}}\\left[1-\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}\\tanh^{2}\\!\\left(\\frac{1+\\sqrt{t}\\,w}{t}\\right)\\exp\\!\\left(-\\frac{w^{2}}{2}\\right)dw\\right]dt.\n$$\nBoth forms are completely explicit nested integral representations for the mutual information in nats.",
            "answer": "$$\\boxed{\\frac{1}{2}\\int_{0}^{\\sigma^{2}}\\left[1-\\int_{-\\infty}^{\\infty}\\tanh^{2}\\!\\left(\\frac{y}{t}\\right)\\,\\frac{1}{2\\sqrt{2\\pi t}}\\left(\\exp\\!\\left(-\\frac{(y-1)^{2}}{2t}\\right)+\\exp\\!\\left(-\\frac{(y+1)^{2}}{2t}\\right)\\right)dy\\right]dt}$$"
        }
    ]
}