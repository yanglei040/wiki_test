{
    "hands_on_practices": [
        {
            "introduction": "To truly understand a concept like the Jensen-Shannon Divergence, there is no substitute for direct calculation. This first practice provides a concrete scenario involving the outputs of two machine learning models. By applying the JSD formula step-by-step , you will see how it quantifies the difference between two simple probability distributions and gain confidence in the mechanics of the calculation.",
            "id": "1634129",
            "problem": "In the field of machine learning, it is often necessary to quantify the similarity between two probability distributions. One such measure is the Jensen-Shannon Divergence (JSD).\n\nConsider two competing image classification models, Model A and Model B. Both models are tasked with classifying an image into one of three possible categories: 'Cat', 'Dog', or 'Bird'. For a specific input image, Model A outputs a probability distribution $P$ over the categories, and Model B outputs a probability distribution $Q$. The distributions are given as:\n$$P = [p_1, p_2, p_3] = [0.8, 0.1, 0.1]$$\n$$Q = [q_1, q_2, q_3] = [0.1, 0.8, 0.1]$$\nwhere the elements correspond to the probabilities of 'Cat', 'Dog', and 'Bird', respectively.\n\nThe Jensen-Shannon Divergence between $P$ and $Q$ is defined using the Kullback-Leibler Divergence (KLD). For two discrete probability distributions $X = [x_i]$ and $Y = [y_i]$, the KLD is given by:\n$$D_{KL}(X || Y) = \\sum_{i} x_i \\log_{2}\\left(\\frac{x_i}{y_i}\\right)$$\nThe JSD is then defined as a symmetrized version of the KLD:\n$$JSD(P || Q) = \\frac{1}{2} D_{KL}(P || M) + \\frac{1}{2} D_{KL}(Q || M)$$\nwhere $M$ is the mixture distribution defined as $M = \\frac{1}{2}(P+Q)$. The logarithm is taken base 2, so the resulting divergence is measured in bits.\n\nCalculate the Jensen-Shannon Divergence, $JSD(P || Q)$, for the given distributions. Express your answer in bits, rounded to four significant figures.",
            "solution": "We are given $P=[0.8,0.1,0.1]$ and $Q=[0.1,0.8,0.1]$. The Jensen-Shannon Divergence is defined by\n$$\nJSD(P||Q)=\\frac{1}{2}D_{KL}(P||M)+\\frac{1}{2}D_{KL}(Q||M),\n$$\nwhere the mixture distribution is $M=\\frac{1}{2}(P+Q)$. Compute $M$ elementwise:\n$$\nM=\\frac{1}{2}([0.8+0.1,\\,0.1+0.8,\\,0.1+0.1])=[0.45,\\,0.45,\\,0.1].\n$$\nUsing the definition of Kullback-Leibler divergence with base-2 logarithm,\n$$\nD_{KL}(X||Y)=\\sum_{i}x_{i}\\log_{2}\\!\\left(\\frac{x_{i}}{y_{i}}\\right),\n$$\nwe compute $D_{KL}(P||M)$:\n$$\nD_{KL}(P||M)=0.8\\log_{2}\\!\\left(\\frac{0.8}{0.45}\\right)+0.1\\log_{2}\\!\\left(\\frac{0.1}{0.45}\\right)+0.1\\log_{2}\\!\\left(\\frac{0.1}{0.1}\\right).\n$$\nNote $\\frac{0.8}{0.45}=\\frac{16}{9}$, $\\frac{0.1}{0.45}=\\frac{2}{9}$, and $\\frac{0.1}{0.1}=1$.\nSince $\\log_{2}(1)=0$, this simplifies to\n$$\nD_{KL}(P||M)=0.8\\log_{2}\\!\\left(\\frac{16}{9}\\right)+0.1\\log_{2}\\!\\left(\\frac{2}{9}\\right).\n$$\nBecause distributions $P$ and $Q$ are symmetric with respect to their first two elements, and the mixture distribution $M$ is also symmetric in its first two elements, we find that $D_{KL}(P||M) = D_{KL}(Q||M)$. Therefore,\n$$\nJSD(P||Q)=D_{KL}(P||M)=0.8\\log_{2}\\!\\left(\\frac{16}{9}\\right)+0.1\\log_{2}\\!\\left(\\frac{2}{9}\\right).\n$$\nSimplify each logarithm using $\\log_{2}\\!\\left(\\frac{16}{9}\\right)=\\log_{2}(16)-\\log_{2}(9)=4-2\\log_{2}(3)$ and $\\log_{2}\\!\\left(\\frac{2}{9}\\right)=\\log_{2}(2)-\\log_{2}(9)=1-2\\log_{2}(3)$. Then\n$$\nJSD(P||Q)=0.8\\left(4-2\\log_{2}(3)\\right)+0.1\\left(1-2\\log_{2}(3)\\right)=3.3-1.8\\log_{2}(3).\n$$\nFor a numerical value in bits, use $\\log_{2}(3)\\approx 1.5849625007$:\n$$\nJSD(P||Q)\\approx 3.3-1.8\\times 1.5849625007\\approx 3.3-2.8529325013\\approx 0.4470674987.\n$$\nRounding to four significant figures gives $0.4471$ bits.",
            "answer": "$$\\boxed{0.4471}$$"
        },
        {
            "introduction": "Moving beyond specific numerical values, this exercise challenges you to think more generally about the nature of distributions. You will derive a closed-form expression for the JSD between a uniform distribution (representing maximum uncertainty) and a deterministic distribution (representing complete certainty). This analytical approach  reveals how the divergence depends on the size of the possibility space, offering a more abstract understanding of the JSD's properties.",
            "id": "1634132",
            "problem": "Consider a system that transmits messages using a discrete alphabet $\\mathcal{X}$ containing $N$ distinct symbols, where $N$ is an integer such that $N \\ge 2$. We are interested in comparing two different probability distributions, $P$ and $Q$, over this alphabet.\n\nThe first distribution, $P$, is a uniform distribution, where every symbol has an equal probability of occurrence.\n\nThe second distribution, $Q$, is a deterministic distribution, where one specific symbol is transmitted with certainty (probability 1) and all other $N-1$ symbols have a probability of 0.\n\nThe dissimilarity between these two distributions can be quantified using the Jensen-Shannon Divergence (JSD). The JSD is a symmetrized and smoothed version of the Kullback-Leibler (KL) divergence. For any two discrete probability distributions $A$ and $B$ over the same alphabet $\\mathcal{X} = \\{x_1, x_2, \\dots, x_N\\}$, the KL divergence is defined as:\n$$D_{KL}(A || B) = \\sum_{i=1}^{N} A(x_i) \\ln \\left(\\frac{A(x_i)}{B(x_i)}\\right)$$\nHere, $\\ln$ denotes the natural logarithm, and by convention, the term in the sum is taken to be zero if $A(x_i) = 0$.\n\nThe JSD between $P$ and $Q$ is then given by:\n$$\\text{JSD}(P || Q) = \\frac{1}{2} D_{KL}(P || M) + \\frac{1}{2} D_{KL}(Q || M)$$\nwhere $M$ is the mixture distribution defined as $M = \\frac{1}{2}(P+Q)$.\n\nDerive a closed-form analytic expression for the Jensen-Shannon Divergence, $\\text{JSD}(P || Q)$, in terms of the alphabet size $N$.",
            "solution": "Let the alphabet be $\\mathcal{X}=\\{x_{1},\\dots,x_{N}\\}$ with $N\\ge 2$. The uniform distribution $P$ assigns $P(x_{i})=\\frac{1}{N}$ for all $i$, and the deterministic distribution $Q$ assigns $Q(x_{1})=1$ and $Q(x_{i})=0$ for $i\\ge 2$.\n\nThe mixture distribution is $M=\\frac{1}{2}(P+Q)$, so\n$$\nM(x_{1})=\\frac{1}{2}\\left(\\frac{1}{N}+1\\right)=\\frac{N+1}{2N},\\qquad\nM(x_{i})=\\frac{1}{2}\\left(\\frac{1}{N}+0\\right)=\\frac{1}{2N}\\quad\\text{for }i=2,\\dots,N.\n$$\n\nCompute $D_{KL}(P || M)$ using $D_{KL}(A || B)=\\sum_{i}A(x_{i})\\ln\\!\\left(\\frac{A(x_{i})}{B(x_{i})}\\right)$:\n$$\nD_{KL}(P || M)=\\sum_{i=1}^{N}\\frac{1}{N}\\ln\\!\\left(\\frac{\\frac{1}{N}}{M(x_{i})}\\right)\n=\\frac{1}{N}\\ln\\!\\left(\\frac{\\frac{1}{N}}{\\frac{N+1}{2N}}\\right)+\\sum_{i=2}^{N}\\frac{1}{N}\\ln\\!\\left(\\frac{\\frac{1}{N}}{\\frac{1}{2N}}\\right).\n$$\nThis gives\n$$\nD_{KL}(P || M)=\\frac{1}{N}\\ln\\!\\left(\\frac{2}{N+1}\\right)+(N-1)\\cdot\\frac{1}{N}\\ln 2\n=\\left(\\frac{1}{N}+\\frac{N-1}{N}\\right)\\ln 2-\\frac{1}{N}\\ln(N+1)\n=\\ln 2-\\frac{1}{N}\\ln(N+1).\n$$\n\nCompute $D_{KL}(Q || M)$; only $x_{1}$ contributes since $Q(x_{1})=1$:\n$$\nD_{KL}(Q || M)=\\ln\\!\\left(\\frac{1}{M(x_{1})}\\right)=-\\ln\\!\\left(\\frac{N+1}{2N}\\right)=\\ln\\!\\left(\\frac{2N}{N+1}\\right)=\\ln 2+\\ln N-\\ln(N+1).\n$$\n\nThe Jensen-Shannon divergence is\n$$\n\\text{JSD}(P || Q)=\\frac{1}{2}\\left[D_{KL}(P || M)+D_{KL}(Q || M)\\right]\n=\\frac{1}{2}\\left[\\ln 2-\\frac{1}{N}\\ln(N+1)+\\ln 2+\\ln N-\\ln(N+1)\\right].\n$$\nTherefore,\n$$\n\\text{JSD}(P || Q)=\\ln 2+\\frac{1}{2}\\ln N-\\frac{1}{2}\\left(1+\\frac{1}{N}\\right)\\ln(N+1)\n=\\ln 2+\\frac{1}{2}\\ln N-\\frac{N+1}{2N}\\ln(N+1).\n$$\nThis is a closed-form expression in terms of $N$.",
            "answer": "$$\\boxed{\\ln 2+\\frac{1}{2}\\ln N-\\frac{N+1}{2N}\\ln(N+1)}$$"
        },
        {
            "introduction": "What does it mean for two distributions to be 'as different as possible' in the eyes of the JSD? This practice delves into that question by asking you to find the conditions that maximize the divergence between two symmetrically opposed Bernoulli distributions . Solving this optimization problem will build a powerful intuition for what the JSD truly measures: the distinguishability of the underlying processes that generate the data.",
            "id": "1634107",
            "problem": "In information theory, a fundamental measure of the dissimilarity between two probability distributions is the Jensen-Shannon Divergence (JSD). The JSD is built upon the Kullback-Leibler (KL) divergence.\n\nFor two discrete probability distributions $P=\\{P(x)\\}$ and $Q=\\{Q(x)\\}$ defined over the same set of outcomes $\\mathcal{X}$, the Kullback-Leibler (KL) divergence from $Q$ to $P$ is given by:\n$$D_{KL}(P || Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\ln\\left(\\frac{P(x)}{Q(x)}\\right)$$\nThe base of the logarithm is the natural number $e$.\n\nThe Jensen-Shannon Divergence (JSD) is a symmetrized and smoothed version of the KL divergence, defined as:\n$$JSD(P || Q) = \\frac{1}{2} D_{KL}(P || M) + \\frac{1}{2} D_{KL}(Q || M)$$\nwhere $M$ is the mixture distribution defined by $M(x) = \\frac{1}{2}(P(x) + Q(x))$ for all $x \\in \\mathcal{X}$.\n\nConsider a scenario involving two biased coins. The behavior of each coin toss can be modeled by a Bernoulli distribution, $\\text{Ber}(\\theta)$, which describes an experiment with two outcomes, labeled $1$ (for heads) and $0$ (for tails). The probability of getting heads is $P(\\text{outcome}=1) = \\theta$, and the probability of getting tails is $P(\\text{outcome}=0) = 1-\\theta$.\n\nLet the first coin's behavior be described by the distribution $P \\sim \\text{Ber}(p)$, and the second coin's behavior be described by the distribution $Q \\sim \\text{Ber}(1-p)$, for some parameter $p$. The domain for $p$ is the closed interval $[0, 1]$.\n\nFind all values of $p$ in the interval $[0, 1]$ that maximize the Jensen-Shannon Divergence, $JSD(P || Q)$.",
            "solution": "Let $P \\sim \\text{Ber}(p)$ and $Q \\sim \\text{Ber}(1-p)$ on outcomes $\\{0,1\\}$, so $P(1)=p$, $P(0)=1-p$, $Q(1)=1-p$, and $Q(0)=p$. The mixture distribution $M$ is\n$$\nM(x)=\\frac{1}{2}\\big(P(x)+Q(x)\\big), \\quad x \\in \\{0,1\\}.\n$$\nHence\n$$\nM(1)=\\frac{1}{2}\\big(p+(1-p)\\big)=\\frac{1}{2}, \\qquad M(0)=\\frac{1}{2}\\big((1-p)+p\\big)=\\frac{1}{2}.\n$$\nThus $M$ is the fair Bernoulli with parameter $\\frac{1}{2}$, independently of $p$.\n\nCompute $D_{KL}(P||M)$:\n$$\nD_{KL}(P||M)=\\sum_{x \\in \\{0,1\\}} P(x)\\ln\\!\\left(\\frac{P(x)}{M(x)}\\right)\n= p \\ln\\!\\left(\\frac{p}{1/2}\\right) + (1-p)\\ln\\!\\left(\\frac{1-p}{1/2}\\right).\n$$\nThis simplifies to\n$$\nD_{KL}(P||M)=p \\ln(2p) + (1-p)\\ln\\big(2(1-p)\\big)\n= \\ln 2 + p \\ln p + (1-p)\\ln(1-p).\n$$\nBy symmetry, the same calculation for $Q$ gives\n$$\nD_{KL}(Q||M)=\\ln 2 + (1-p)\\ln(1-p) + p \\ln p.\n$$\nTherefore,\n$$\nJSD(P||Q)=\\frac{1}{2}D_{KL}(P||M)+\\frac{1}{2}D_{KL}(Q||M)\n= \\ln 2 + p \\ln p + (1-p)\\ln(1-p),\n$$\nwhere, by continuity, the convention $0\\ln 0=0$ is used at $p=0$ and $p=1$.\n\nTo maximize $JSD(P||Q)$ over $p \\in [0,1]$, differentiate with respect to $p$:\n$$\n\\frac{d}{dp}\\,JSD(P||Q)=\\frac{d}{dp}\\big(\\ln 2 + p \\ln p + (1-p)\\ln(1-p)\\big)\n= \\ln p + 1 - \\big(\\ln(1-p) + 1\\big)\n= \\ln\\!\\left(\\frac{p}{1-p}\\right).\n$$\nCritical points satisfy\n$$\n\\ln\\!\\left(\\frac{p}{1-p}\\right)=0 \\quad \\Longleftrightarrow \\quad \\frac{p}{1-p}=1 \\quad \\Longleftrightarrow \\quad p=\\frac{1}{2}.\n$$\nThe second derivative is\n$$\n\\frac{d^{2}}{dp^{2}}\\,JSD(P||Q)=\\frac{1}{p}+\\frac{1}{1-p}=\\frac{1}{p(1-p)} > 0 \\quad \\text{for } p \\in (0,1),\n$$\nso $p=\\frac{1}{2}$ is a strict minimum. Hence the maximum over the closed interval $[0,1]$ occurs at the endpoints $p=0$ and $p=1$ (where $JSD=\\ln 2$).",
            "answer": "$$\\boxed{0, 1}$$"
        }
    ]
}