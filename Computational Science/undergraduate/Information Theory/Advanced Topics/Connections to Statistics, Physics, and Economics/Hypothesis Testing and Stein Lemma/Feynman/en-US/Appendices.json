{
    "hands_on_practices": [
        {
            "introduction": "At the heart of hypothesis testing lies the need to quantify how \"different\" one statistical model is from another. The Kullback-Leibler (KL) divergence provides a fundamental measure of this difference, serving as the error exponent in Stein's Lemma. This first exercise provides essential practice in calculating the KL divergence for a basic but important case: distinguishing between two Bernoulli processes, which model a wide range of binary-outcome scenarios. ",
            "id": "1630521",
            "problem": "In a factory, a machine manufactures a specific type of electronic component. Each component is tested and classified as either functional (state 1) or defective (state 0). The machine's performance can be modeled by a Bernoulli distribution.\n\nThere are two hypotheses regarding the machine's operational status:\n- Hypothesis $H_0$: The machine is operating under normal conditions. The probability of producing a functional component is $p_0 = 1/3$. Let this distribution be $P_0$.\n- Hypothesis $H_1$: The machine requires maintenance. The probability of producing a functional component is $p_1 = 2/3$. Let this distribution be $P_1$.\n\nTo quantify the statistical distinguishability of the \"normal\" state from the \"maintenance required\" state, an engineer decides to compute the Kullback-Leibler (KL) divergence, also known as relative entropy.\n\nCalculate the KL divergence $D(P_0 || P_1)$. Provide your answer as a numerical value rounded to three significant figures.",
            "solution": "We model the machine output as a Bernoulli random variable. The Kullback-Leibler divergence from $P_{0}$ with parameter $p_{0}$ to $P_{1}$ with parameter $p_{1}$ for Bernoulli distributions is\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\;=\\; p_{0}\\,\\ln\\!\\left(\\frac{p_{0}}{p_{1}}\\right) \\;+\\; \\left(1-p_{0}\\right)\\,\\ln\\!\\left(\\frac{1-p_{0}}{1-p_{1}}\\right).\n$$\nSubstitute $p_{0}=\\frac{1}{3}$ and $p_{1}=\\frac{2}{3}$:\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\;=\\; \\frac{1}{3}\\,\\ln\\!\\left(\\frac{\\frac{1}{3}}{\\frac{2}{3}}\\right) \\;+\\; \\frac{2}{3}\\,\\ln\\!\\left(\\frac{\\frac{2}{3}}{\\frac{1}{3}}\\right).\n$$\nSimplify the ratios:\n$$\n\\frac{\\frac{1}{3}}{\\frac{2}{3}} \\;=\\; \\frac{1}{2}, \\qquad \\frac{\\frac{2}{3}}{\\frac{1}{3}} \\;=\\; 2,\n$$\nso\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\;=\\; \\frac{1}{3}\\,\\ln\\!\\left(\\frac{1}{2}\\right) \\;+\\; \\frac{2}{3}\\,\\ln(2).\n$$\nUse $\\ln\\!\\left(\\frac{1}{2}\\right)=-\\ln(2)$:\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\;=\\; -\\frac{1}{3}\\,\\ln(2) \\;+\\; \\frac{2}{3}\\,\\ln(2) \\;=\\; \\frac{1}{3}\\,\\ln(2).\n$$\nNumerically, using $\\ln(2)\\approx 0.693147$, we obtain\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\approx \\frac{1}{3}\\times 0.693147 \\approx 0.231049,\n$$\nwhich rounded to three significant figures is $0.231$.",
            "answer": "$$\\boxed{0.231}$$"
        },
        {
            "introduction": "Stein's Lemma is not just a theoretical curiosity; it provides a powerful, practical tool for experimental design. By relating the error exponent to the rate at which our certainty increases with more data, it helps us answer the critical question: \"How many samples do I need?\". This practice problem demonstrates how to apply the asymptotic approximation from Stein's Lemma to determine the minimum number of observations required to achieve a desired level of confidence in your conclusion. ",
            "id": "1630537",
            "problem": "A data scientist is conducting a binary hypothesis test to distinguish between two sources of data, modeled as sequences of independent and identically distributed Bernoulli random variables. The null hypothesis, $H_0$, posits that the data comes from a source described by a Bernoulli distribution with parameter $p_0 = 1/2$. The alternative hypothesis, $H_1$, posits the source is a Bernoulli distribution with parameter $p_1 \\neq p_0$.\n\nFor a hypothesis test on a sequence of $n$ samples, we define the Type I error probability as $\\alpha_n = P(\\text{decide } H_1 | H_0 \\text{ is true})$ and the Type II error probability as $\\beta_n = P(\\text{decide } H_0 | H_1 \\text{ is true})$. Stein's Lemma states that for a fixed Type I error probability constraint $\\alpha_n \\leq \\epsilon$ where $\\epsilon \\in (0, 1/2)$, the minimum achievable Type II error probability, $\\beta_n^*$, decays exponentially with the number of samples $n$. The rate of this decay is given by the Kullback-Leibler divergence between the two distributions.\n\nThe data scientist is using a testing procedure where the optimal error exponent, calculated using the natural logarithm, has been found to be $C = 0.0872$. The company's protocol requires fixing the Type I error probability at $\\epsilon = 0.05$. The goal is to determine the sample size needed to achieve a very low Type II error probability.\n\nUsing the large-sample approximation derived from Stein's Lemma, calculate the minimum integer number of samples, $n$, required to ensure that the Type II error probability is at most $\\beta^* = 1 \\times 10^{-5}$.",
            "solution": "The problem asks for the minimum number of samples, $n$, required to achieve a specific Type II error probability, $\\beta^*$, given the optimal error exponent, $C$, for a binary hypothesis test.\n\nStein's Lemma provides the asymptotic behavior of the minimum Type II error probability, $\\beta_n^*$, for a fixed Type I error probability $\\epsilon$. For a large number of samples $n$, the relationship is given by:\n$$ \\beta_n^* \\approx \\exp(-n \\cdot D(P_1 || P_0)) $$\nwhere $D(P_1 || P_0)$ is the Kullback-Leibler (KL) divergence between the probability distribution under hypothesis $H_1$ (denoted $P_1$) and the distribution under hypothesis $H_0$ (denoted $P_0$). This KL divergence is the optimal error exponent for the Type II error.\n\nThe problem states that the optimal error exponent is $C = 0.0872$. Therefore, we have:\n$$ C = D(P_1 || P_0) = 0.0872 $$\nIt is important to note that this result, according to Stein's Lemma, holds for any fixed Type I error constraint $\\epsilon \\in (0, 1/2)$ in the large $n$ limit. Therefore, the specific value $\\epsilon = 0.05$ is part of the problem's setup but is not required for the calculation of the sample size $n$ based on this approximation.\n\nWe are given the target for the Type II error probability, $\\beta^* = 1 \\times 10^{-5}$. We need to find the minimum integer $n$ such that $\\beta_n^* \\leq \\beta^*$. Using the approximation, we set:\n$$ \\beta^* = \\exp(-nC) $$\nNow, we solve this equation for $n$:\n$$ \\ln(\\beta^*) = \\ln(\\exp(-nC)) $$\n$$ \\ln(\\beta^*) = -nC $$\n$$ n = -\\frac{\\ln(\\beta^*)}{C} $$\nThis can also be written as:\n$$ n = \\frac{\\ln(1/\\beta^*)}{C} $$\n\nNow, we substitute the given numerical values into this expression:\n- $C = 0.0872$\n- $\\beta^* = 1 \\times 10^{-5}$\n\nSo,\n$$ n = \\frac{\\ln(1/(1 \\times 10^{-5}))}{0.0872} $$\n$$ n = \\frac{\\ln(10^5)}{0.0872} $$\nUsing the property of logarithms $\\ln(x^y) = y \\ln(x)$, we get:\n$$ n = \\frac{5 \\ln(10)}{0.0872} $$\nUsing the approximate value for the natural logarithm of 10, $\\ln(10) \\approx 2.302585$:\n$$ n \\approx \\frac{5 \\times 2.302585}{0.0872} $$\n$$ n \\approx \\frac{11.512925}{0.0872} $$\n$$ n \\approx 132.028956 $$\n\nThe number of samples, $n$, must be an integer. The problem asks for the minimum integer number of samples to ensure the Type II error is *at most* $\\beta^*$. The relationship $\\beta_n^* \\approx \\exp(-nC)$ shows that $\\beta_n^*$ is a decreasing function of $n$. Therefore, to ensure that the error probability is less than or equal to the target $\\beta^*$, we must round the calculated value of $n$ up to the next whole number. This mathematical operation is known as the ceiling function.\n\n$$ n_{\\text{min}} = \\lceil 132.028956 \\rceil = 133 $$\nThus, a minimum of 133 samples are required to meet the specified error probability requirements.",
            "answer": "$$\\boxed{133}$$"
        },
        {
            "introduction": "What happens when our measurement process discards crucial information? This final exercise presents a thought-provoking scenario where two physically distinct processes become statistically identical after a certain data processing step. It highlights a critical principle: the ability to distinguish between hypotheses depends entirely on the information present in the *observed* data. This problem forces a deeper consideration of how data acquisition and processing choices can fundamentally limit the power of any subsequent statistical test, a concept beautifully illustrated when the optimal error exponent becomes zero. ",
            "id": "1630530",
            "problem": "An engineer is designing a remote monitoring system for a sensitive physical experiment. The system generates a sequence of independent and identically distributed (i.i.d.) random measurements $X_1, X_2, \\ldots, X_n$. The statistical properties of these measurements depend on whether the system is in a nominal state ($H_0$) or a perturbed state ($H_1$).\n\nUnder hypothesis $H_0$, the measurements follow a normal distribution with a mean of zero and a variance of $\\sigma_0^2$.\nUnder hypothesis $H_1$, the measurements follow a normal distribution with a mean of zero and a variance of $\\sigma_1^2$.\nIt is known that the variances are different and positive, with $\\sigma_1^2  \\sigma_0^2  0$.\n\nDue to extreme data transmission constraints, the system cannot send the raw measurements $X_i$. Instead, it processes each measurement locally and transmits only its sign. Let $Y_i = \\text{sign}(X_i)$ be the transmitted value, where the sign function is defined as:\n$$\n\\text{sign}(x) = \\begin{cases} \n1  \\text{if } x \\ge 0 \\\\\n-1  \\text{if } x  0 \n\\end{cases}\n$$\nA central controller must decide between $H_0$ and $H_1$ based on the received sequence of signs $Y_1, Y_2, \\ldots, Y_n$. For a sufficiently large number of observations $n$, the minimum achievable probability of a type II error, $\\beta_n^*$, for a fixed upper bound on the probability of a type I error, can be approximated as $\\beta_n^* \\approx \\exp(-nE)$. This value $E$ is known as the optimal error exponent.\n\nCalculate the value of this optimal error exponent $E$.",
            "solution": "The problem asks for the optimal error exponent, $E$, for a hypothesis test based on a sequence of observed signs $Y_1, Y_2, \\ldots, Y_n$. According to Stein's Lemma, the optimal error exponent for distinguishing between two hypotheses is given by the Kullback-Leibler (KL) divergence between the probability distributions of the observed data under each hypothesis. In this case, the decision is based on the sequence of signs $Y_i$, not the original measurements $X_i$. Therefore, we must first determine the probability distributions of the random variable $Y_i$ under both hypotheses, $H_0$ and $H_1$.\n\nLet $Q_0$ be the probability distribution of $Y_i$ when the underlying measurement $X_i$ is drawn from the distribution under $H_0$, and let $Q_1$ be the distribution of $Y_i$ when $X_i$ is from the distribution under $H_1$. The random variable $Y_i$ can take on two values: $1$ and $-1$.\n\nFirst, let's determine the distribution $Q_0$. Under hypothesis $H_0$, the measurement $X_i$ is drawn from a normal distribution $\\mathcal{N}(0, \\sigma_0^2)$. The probability mass function (PMF) of $Y_i$ is given by:\n$$Q_0(Y_i=1) = P(X_i \\ge 0 | H_0)$$\n$$Q_0(Y_i=-1) = P(X_i  0 | H_0)$$\nA normal distribution with a mean of zero is symmetric about the origin. The probability that a random variable drawn from such a distribution is positive is equal to the probability that it is negative. Since the distribution is continuous, $P(X_i=0)=0$, so $P(X_i \\ge 0) = P(X_i  0)$. Due to symmetry:\n$$P(X_i \\ge 0 | H_0) = \\frac{1}{2}$$\n$$P(X_i  0 | H_0) = \\frac{1}{2}$$\nThus, under $H_0$, the distribution $Q_0$ for $Y_i$ is a Bernoulli distribution where $Y_i$ takes values $1$ and $-1$ with equal probability:\n$$Q_0(1) = \\frac{1}{2}, \\quad Q_0(-1) = \\frac{1}{2}$$\n\nNext, we determine the distribution $Q_1$. Under hypothesis $H_1$, the measurement $X_i$ is drawn from a normal distribution $\\mathcal{N}(0, \\sigma_1^2)$. The PMF of $Y_i$ is:\n$$Q_1(Y_i=1) = P(X_i \\ge 0 | H_1)$$\n$$Q_1(Y_i=-1) = P(X_i  0 | H_1)$$\nSimilar to the case for $H_0$, the distribution $\\mathcal{N}(0, \\sigma_1^2)$ is also a zero-mean normal distribution and is therefore symmetric about the origin. The variance $\\sigma_1^2$ affects the spread of the distribution but not its symmetry. Consequently:\n$$P(X_i \\ge 0 | H_1) = \\frac{1}{2}$$\n$$P(X_i  0 | H_1) = \\frac{1}{2}$$\nSo, under $H_1$, the distribution $Q_1$ for $Y_i$ is also a Bernoulli distribution:\n$$Q_1(1) = \\frac{1}{2}, \\quad Q_1(-1) = \\frac{1}{2}$$\n\nWe see that the probability distributions of the observed sign variable $Y_i$ are identical under both hypotheses: $Q_0 = Q_1$.\n\nThe optimal error exponent $E$ is given by the KL-divergence $D(Q_0 || Q_1)$. For discrete distributions, the KL-divergence is defined as:\n$$E = D(Q_0 || Q_1) = \\sum_{y \\in \\{-1, 1\\}} Q_0(y) \\ln\\left(\\frac{Q_0(y)}{Q_1(y)}\\right)$$\nSince $Q_0(y) = Q_1(y)$ for both possible outcomes $y=1$ and $y=-1$, the ratio inside the logarithm is always 1:\n$$\\frac{Q_0(1)}{Q_1(1)} = \\frac{1/2}{1/2} = 1$$\n$$\\frac{Q_0(-1)}{Q_1(-1)} = \\frac{1/2}{1/2} = 1$$\nThe natural logarithm of 1 is 0, i.e., $\\ln(1)=0$. Substituting this into the KL-divergence formula:\n$$E = Q_0(1) \\ln(1) + Q_0(-1) \\ln(1) = \\left(\\frac{1}{2}\\right) \\cdot 0 + \\left(\\frac{1}{2}\\right) \\cdot 0 = 0$$\n\nThe optimal error exponent is 0. This result signifies that the processing step of taking the sign of the zero-mean Gaussian measurement completely destroys the information contained in the variance. The resulting binary data has the same statistical distribution regardless of whether the system is in state $H_0$ or $H_1$. Therefore, it is impossible to distinguish between the two hypotheses based on the sequence of signs, and the probability of error does not decrease exponentially with the number of samples.",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}