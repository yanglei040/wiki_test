{
    "hands_on_practices": [
        {
            "introduction": "The first step in understanding the limits of hypothesis testing is to quantify the difference between competing hypotheses. The Kullback-Leibler (KL) divergence provides a powerful, information-theoretic measure of how one probability distribution is different from a second. This practice exercise  will guide you through a fundamental calculation of the KL divergence for a common scenario involving Bernoulli distributions, laying the groundwork for applying Stein's Lemma.",
            "id": "1630521",
            "problem": "In a factory, a machine manufactures a specific type of electronic component. Each component is tested and classified as either functional (state 1) or defective (state 0). The machine's performance can be modeled by a Bernoulli distribution.\n\nThere are two hypotheses regarding the machine's operational status:\n- Hypothesis $H_0$: The machine is operating under normal conditions. The probability of producing a functional component is $p_0 = 1/3$. Let this distribution be $P_0$.\n- Hypothesis $H_1$: The machine requires maintenance. The probability of producing a functional component is $p_1 = 2/3$. Let this distribution be $P_1$.\n\nTo quantify the statistical distinguishability of the \"normal\" state from the \"maintenance required\" state, an engineer decides to compute the Kullback-Leibler (KL) divergence, also known as relative entropy.\n\nCalculate the KL divergence $D(P_0 || P_1)$. Provide your answer as a numerical value rounded to three significant figures.",
            "solution": "We model the machine output as a Bernoulli random variable. The Kullback-Leibler divergence from $P_{0}$ with parameter $p_{0}$ to $P_{1}$ with parameter $p_{1}$ for Bernoulli distributions is\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\;=\\; p_{0}\\,\\ln\\!\\left(\\frac{p_{0}}{p_{1}}\\right) \\;+\\; \\left(1-p_{0}\\right)\\,\\ln\\!\\left(\\frac{1-p_{0}}{1-p_{1}}\\right).\n$$\nSubstitute $p_{0}=\\frac{1}{3}$ and $p_{1}=\\frac{2}{3}$:\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\;=\\; \\frac{1}{3}\\,\\ln\\!\\left(\\frac{\\frac{1}{3}}{\\frac{2}{3}}\\right) \\;+\\; \\frac{2}{3}\\,\\ln\\!\\left(\\frac{\\frac{2}{3}}{\\frac{1}{3}}\\right).\n$$\nSimplify the ratios:\n$$\n\\frac{\\frac{1}{3}}{\\frac{2}{3}} \\;=\\; \\frac{1}{2}, \\qquad \\frac{\\frac{2}{3}}{\\frac{1}{3}} \\;=\\; 2,\n$$\nso\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\;=\\; \\frac{1}{3}\\,\\ln\\!\\left(\\frac{1}{2}\\right) \\;+\\; \\frac{2}{3}\\,\\ln(2).\n$$\nUse $\\ln\\!\\left(\\frac{1}{2}\\right)=-\\ln(2)$:\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\;=\\; -\\frac{1}{3}\\,\\ln(2) \\;+\\; \\frac{2}{3}\\,\\ln(2) \\;=\\; \\frac{1}{3}\\,\\ln(2).\n$$\nNumerically, using $\\ln(2)\\approx 0.693147$, we obtain\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\approx \\frac{1}{3}\\times 0.693147 \\approx 0.231049,\n$$\nwhich rounded to three significant figures is $0.231$.",
            "answer": "$$\\boxed{0.231}$$"
        },
        {
            "introduction": "Stein's Lemma offers a profound connection between the abstract KL divergence and the practical performance of a hypothesis test. It tells us that the probability of making a mistake (a Type II error) decreases exponentially as we collect more data, with the rate of decay given by the KL divergence. This exercise  puts this principle into practice, showing you how to estimate the minimum number of samples needed to achieve a desired level of confidence in your conclusion.",
            "id": "1630537",
            "problem": "A data scientist is conducting a binary hypothesis test to distinguish between two sources of data, modeled as sequences of independent and identically distributed Bernoulli random variables. The null hypothesis, $H_0$, posits that the data comes from a source described by a Bernoulli distribution with parameter $p_0 = 1/2$. The alternative hypothesis, $H_1$, posits the source is a Bernoulli distribution with parameter $p_1 \\neq p_0$.\n\nFor a hypothesis test on a sequence of $n$ samples, we define the Type I error probability as $\\alpha_n = P(\\text{decide } H_1 | H_0 \\text{ is true})$ and the Type II error probability as $\\beta_n = P(\\text{decide } H_0 | H_1 \\text{ is true})$. Stein's Lemma states that for a fixed Type I error probability constraint $\\alpha_n \\leq \\epsilon$ where $\\epsilon \\in (0, 1)$, the minimum achievable Type II error probability, $\\beta_n^*$, decays exponentially with the number of samples $n$. The rate of this decay is given by the Kullback-Leibler divergence between the two distributions.\n\nThe data scientist is using a testing procedure where the optimal error exponent, calculated using the natural logarithm, has been found to be $C = 0.0872$. The company's protocol requires fixing the Type I error probability at $\\epsilon = 0.05$. The goal is to determine the sample size needed to achieve a very low Type II error probability.\n\nUsing the large-sample approximation derived from Stein's Lemma, calculate the minimum integer number of samples, $n$, required to ensure that the Type II error probability is at most $\\beta^* = 1 \\times 10^{-5}$.",
            "solution": "The problem asks for the minimum number of samples, $n$, required to achieve a specific Type II error probability, $\\beta^*$, given the optimal error exponent, $C$, for a binary hypothesis test.\n\nStein's Lemma provides the asymptotic behavior of the minimum Type II error probability, $\\beta_n^*$, for a fixed Type I error probability $\\epsilon$. For a large number of samples $n$, the relationship is given by:\n$$ \\beta_n^* \\approx \\exp(-n \\cdot D(P_0 || P_1)) $$\nwhere $D(P_0 || P_1)$ is the Kullback-Leibler (KL) divergence of the probability distribution under hypothesis $H_0$ (denoted $P_0$) from the distribution under hypothesis $H_1$ (denoted $P_1$). This KL divergence is the optimal error exponent for the Type II error.\n\nThe problem states that the optimal error exponent is $C = 0.0872$. Therefore, we have:\n$$ C = D(P_0 || P_1) = 0.0872 $$\nIt is important to note that this result, according to Stein's Lemma, holds for any fixed Type I error constraint $\\epsilon \\in (0, 1)$ in the large $n$ limit. Therefore, the specific value $\\epsilon = 0.05$ is part of the problem's setup but is not required for the calculation of the sample size $n$ based on this approximation.\n\nWe are given the target for the Type II error probability, $\\beta^* = 1 \\times 10^{-5}$. We need to find the minimum integer $n$ such that $\\beta_n^* \\leq \\beta^*$. Using the approximation, we set:\n$$ \\beta^* = \\exp(-nC) $$\nNow, we solve this equation for $n$:\n$$ \\ln(\\beta^*) = \\ln(\\exp(-nC)) $$\n$$ \\ln(\\beta^*) = -nC $$\n$$ n = -\\frac{\\ln(\\beta^*)}{C} $$\nThis can also be written as:\n$$ n = \\frac{\\ln(1/\\beta^*)}{C} $$\n\nNow, we substitute the given numerical values into this expression:\n- $C = 0.0872$\n- $\\beta^* = 1 \\times 10^{-5}$\n\nSo,\n$$ n = \\frac{\\ln(1/(1 \\times 10^{-5}))}{0.0872} $$\n$$ n = \\frac{\\ln(10^5)}{0.0872} $$\nUsing the property of logarithms $\\ln(x^y) = y \\ln(x)$, we get:\n$$ n = \\frac{5 \\ln(10)}{0.0872} $$\nUsing the approximate value for the natural logarithm of 10, $\\ln(10) \\approx 2.302585$:\n$$ n \\approx \\frac{5 \\times 2.302585}{0.0872} $$\n$$ n \\approx \\frac{11.512925}{0.0872} $$\n$$ n \\approx 132.028956 $$\n\nThe number of samples, $n$, must be an integer. The problem asks for the minimum integer number of samples to ensure the Type II error is *at most* $\\beta^*$. The relationship $\\beta_n^* \\approx \\exp(-nC)$ shows that $\\beta_n^*$ is a decreasing function of $n$. Therefore, to ensure that the error probability is less than or equal to the target $\\beta^*$, we must round the calculated value of $n$ up to the next whole number. This mathematical operation is known as the ceiling function.\n\n$$ n_{\\text{min}} = \\lceil 132.028956 \\rceil = 133 $$\nThus, a minimum of 133 samples are required to meet the specified error probability requirements.",
            "answer": "$$\\boxed{133}$$"
        },
        {
            "introduction": "To truly master the connection between model distinguishability and error rates, it is helpful to reverse the problem. Instead of starting with two models and finding the error exponent, what can we infer about the models if we are given a specific error exponent? This thought-provoking exercise  challenges you to work backward, using the relationship from Stein's Lemma to determine the underlying model parameters that correspond to a given level of asymptotic performance.",
            "id": "1630535",
            "problem": "Consider a binary hypothesis test designed to distinguish between two models for a sequence of $n$ independent and identically distributed binary outcomes, $X_1, \\dots, X_n$, where each outcome can be either 0 or 1.\n\nThe null hypothesis, $H_0$, models the outcomes as arising from a fair coin, i.e., a Bernoulli distribution with a success probability (P(X=1)) of $1/2$.\n\nThe alternative hypothesis, $H_1$, models the outcomes as arising from a biased coin, i.e., a Bernoulli distribution with a success probability of $p$, where $p$ is a constant such that $p \\in [0, 1]$ and $p \\neq 1/2$.\n\nWe are interested in a decision rule that, in the asymptotic limit of a large number of samples $n$, keeps the Type II error probability (incorrectly accepting $H_0$) below an arbitrarily small constant $\\epsilon > 0$, while minimizing the Type I error probability, $\\alpha_n$. The optimal exponential decay rate of this minimum Type I error probability, $\\alpha_n^*$, is given by the error exponent, defined as $\\lim_{n \\to \\infty} \\left(-\\frac{1}{n} \\log_2 \\alpha_n^*\\right)$. This exponent quantifies how quickly the probability of a Type I error vanishes as the number of samples increases, with the logarithm taken to base 2, giving units of bits.\n\nIf this error exponent is experimentally determined to be exactly 1 bit, which of the following options correctly identifies all possible values of the parameter $p$?\n\nA. $p=0$ only\nB. $p=1$ only\nC. $p=0$ and $p=1$\nD. $p=1/4$ and $p=3/4$\nE. The problem is ill-posed as no such value of $p$ exists.",
            "solution": "We consider i.i.d. binary samples under $H_{0}$: $\\text{Bernoulli}(1/2)$ and under $H_{1}$: $\\text{Bernoulli}(p)$ with fixed $p \\in [0,1]$, $p \\neq 1/2$. In the reverse Neyman-Pearson setting, with Type II error constrained below any fixed $\\epsilon > 0$, the Chernoff-Stein lemma states that the optimal Type I error exponent satisfies\n$$\n\\lim_{n \\to \\infty} \\left(-\\frac{1}{n} \\ln \\alpha_{n}^{*}\\right) \\;=\\; D(P_{1}\\|P_{0}),\n$$\nwhere $D(P_{1}\\|P_{0})$ is the Kullback-Leibler divergence with natural logarithms. The problem defines the exponent using $\\log_{2}$, so by the change of base,\n$$\n\\lim_{n \\to \\infty} \\left(-\\frac{1}{n} \\log_{2} \\alpha_{n}^{*}\\right) \\;=\\; \\frac{1}{\\ln 2}\\, D(P_{1}\\|P_{0}).\n$$\nThus, an observed exponent of exactly $1$ bit implies\n$$\n\\frac{1}{\\ln 2}\\, D\\big(\\text{Bernoulli}(p)\\,\\|\\,\\text{Bernoulli}(1/2)\\big) \\;=\\; 1,\n$$\nor equivalently\n$$\nD\\big(\\text{Bernoulli}(p)\\,\\|\\,\\text{Bernoulli}(1/2)\\big) \\;=\\; \\ln 2.\n$$\n\nCompute the divergence:\n$$\nD\\big(\\text{Bernoulli}(p)\\,\\|\\,\\text{Bernoulli}(1/2)\\big)\n= p \\ln\\!\\left(\\frac{p}{1/2}\\right) + (1-p)\\ln\\!\\left(\\frac{1-p}{1/2}\\right)\n= p \\ln p + (1-p)\\ln(1-p) - \\ln\\!\\left(\\frac{1}{2}\\right).\n$$\nUsing $\\ln(1/2) = -\\ln 2$, this simplifies to\n$$\nD\\big(\\text{Bernoulli}(p)\\,\\|\\,\\text{Bernoulli}(1/2)\\big)\n= p \\ln p + (1-p)\\ln(1-p) + \\ln 2.\n$$\nIntroduce the Bernoulli entropy in nats, $h(p) = -p \\ln p - (1-p)\\ln(1-p)$. Then\n$$\nD\\big(\\text{Bernoulli}(p)\\,\\|\\,\\text{Bernoulli}(1/2)\\big)\n= \\ln 2 - h(p).\n$$\nSetting this equal to $\\ln 2$ yields\n$$\n\\ln 2 - h(p) = \\ln 2 \\;\\;\\Longrightarrow\\;\\; h(p) = 0.\n$$\nFor a Bernoulli distribution, $h(p) = 0$ if and only if $p \\in \\{0, 1\\}$, since entropy is zero exactly when the distribution is degenerate.\n\nTherefore, the only values of $p$ that yield an error exponent of exactly $1$ bit are $p=0$ and $p=1$, corresponding to option C.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}