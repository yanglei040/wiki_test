{
    "hands_on_practices": [
        {
            "introduction": "To begin, let's practice identifying the structure of an exponential family within a familiar context. Many common probability distributions, like the Normal, Bernoulli, and Poisson, can be expressed in this powerful general form. This exercise challenges you to take the geometric distribution and algebraically rearrange it into the canonical exponential family representation, identifying its fundamental components . Mastering this skill is crucial for recognizing the shared mathematical structure that unifies a wide array of statistical models.",
            "id": "1623491",
            "problem": "In designing a reliable data transmission protocol, an engineer models the process of sending a data packet. Each transmission attempt is an independent Bernoulli trial with a probability of success $p$, where $0 < p < 1$. Let the random variable $X$ denote the number of failed attempts before the first successful transmission occurs. The probability mass function for $X$ is given by the geometric distribution:\n$$P(X=x) = (1-p)^x p, \\quad \\text{for } x \\in \\{0, 1, 2, \\dots\\}$$\nAny probability distribution that can be written in the form\n$$f(x|\\theta) = h(x) \\exp(\\eta(\\theta) T(x) - A(\\eta(\\theta)))$$\nis a member of the exponential family. For the canonical form, we use the natural parameter $\\eta = \\eta(\\theta)$ and write the distribution as:\n$$f(x|\\eta) = h(x) \\exp(\\eta T(x) - A(\\eta))$$\nwhere $h(x)$ is the base measure, $T(x)$ is the sufficient statistic, $\\eta$ is the natural parameter, and $A(\\eta)$ is the log-partition function (or cumulant-generating function).\n\nYour task is to represent the given geometric distribution in this canonical exponential family form. Which of the following correctly identifies the triplet of components $(h(x), T(x), A(\\eta))$, where $\\eta = \\ln(1-p)$?\n\nA. $(h(x)=1, \\quad T(x)=x, \\quad A(\\eta)=-\\ln(1-e^\\eta))$\n\nB. $(h(x)=1, \\quad T(x)=x, \\quad A(\\eta)=\\eta - \\ln(1-e^\\eta))$\n\nC. $(h(x)=p, \\quad T(x)=x, \\quad A(\\eta)=0)$\n\nD. $(h(x)=1, \\quad T(x)=x, \\quad A(\\eta)=\\ln(1-e^\\eta))$\n\nE. $(h(x)=1, \\quad T(x)=1, \\quad A(\\eta)=-\\ln(1-e^\\eta))$",
            "solution": "We start with the geometric pmf for the number of failures before the first success:\n$$\nP(X=x) = (1-p)^{x} p,\\quad x \\in \\{0,1,2,\\dots\\}.\n$$\nLet $q = 1-p$, so $0<q<1$. The problem specifies the natural parameter $\\eta = \\ln(1-p)$, hence $\\eta = \\ln q$ and $q = \\exp(\\eta)$ with $\\eta < 0$.\n\nExpress the pmf in terms of $\\eta$:\n$$\nP(X=x) = (1-q) q^{x} = \\bigl(1-\\exp(\\eta)\\bigr)\\exp(\\eta x).\n$$\nTo write this in canonical exponential family form,\n$$\nf(x \\mid \\eta) = h(x)\\exp\\bigl(\\eta T(x) - A(\\eta)\\bigr),\n$$\nwe can choose $h(x)=1$ and $T(x)=x$. Then normalization determines $A(\\eta)$. Enforce\n$$\n\\sum_{x=0}^{\\infty} f(x \\mid \\eta) = \\sum_{x=0}^{\\infty} \\exp\\bigl(\\eta x - A(\\eta)\\bigr) = \\exp\\bigl(-A(\\eta)\\bigr)\\sum_{x=0}^{\\infty} \\exp(\\eta x) = 1.\n$$\nFor $\\eta<0$, the geometric series sums to\n$$\n\\sum_{x=0}^{\\infty} \\exp(\\eta x) = \\frac{1}{1-\\exp(\\eta)}.\n$$\nThus,\n$$\n\\exp\\bigl(-A(\\eta)\\bigr)\\cdot \\frac{1}{1-\\exp(\\eta)} = 1\n\\quad\\Longrightarrow\\quad\n\\exp\\bigl(-A(\\eta)\\bigr) = 1 - \\exp(\\eta)\n\\quad\\Longrightarrow\\quad\n-A(\\eta) = \\ln\\bigl(1-\\exp(\\eta)\\bigr),\n$$\nso\n$$\nA(\\eta) = -\\ln\\bigl(1-\\exp(\\eta)\\bigr).\n$$\nTherefore, the canonical form is obtained with\n$$\nh(x)=1,\\quad T(x)=x,\\quad A(\\eta)=-\\ln\\bigl(1-\\exp(\\eta)\\bigr),\n$$\nwhich corresponds to option A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Now, we will explore the deep connection between exponential families and the principle of maximum entropy. This principle states that, given certain constraints (like a known average value), the most objective probability distribution is the one that maximizes Shannon entropy. In this problem, you will apply this principle to a physical system with discrete energy levels to find the most likely probability distribution of its states . This practice demonstrates how exponential family distributions, such as the Boltzmann distribution in physics, arise naturally from a fundamental principle of inference.",
            "id": "1623482",
            "problem": "A physical system is known to possess three discrete energy levels, $E_1=\\epsilon_0, E_2=2\\epsilon_0, E_3=3\\epsilon_0$, where $\\epsilon_0$ is a positive energy constant. Through a series of measurements on a large ensemble of identical systems prepared under the same conditions, the average energy is determined to be $\\langle E \\rangle = \\frac{7}{3}\\epsilon_0$. Based on the principle of maximum entropy, the system's state distribution $\\{p_1, p_2, p_3\\}$ is assumed to be the one with the highest possible entropy that is consistent with this observed average energy. Calculate the probability $p_2$ of finding the system in the second energy level, $E_2$. Express your answer as a single closed-form analytic expression.",
            "solution": "The problem asks for the probability distribution $\\{p_1, p_2, p_3\\}$ that maximizes the Shannon entropy, subject to certain constraints. The Shannon entropy is given by $H(p_1, p_2, p_3) = - \\sum_{i=1}^{3} p_i \\ln(p_i)$.\n\nThe constraints on the probabilities are:\n1. Normalization: The sum of all probabilities must be 1.\n$$g_1(p_1, p_2, p_3) = \\sum_{i=1}^{3} p_i - 1 = p_1 + p_2 + p_3 - 1 = 0$$\n2. Average energy: The expectation value of the energy must match the observed value.\n$$g_2(p_1, p_2, p_3) = \\sum_{i=1}^{3} p_i E_i - \\langle E \\rangle = p_1 E_1 + p_2 E_2 + p_3 E_3 - \\langle E \\rangle = 0$$\n\nWe use the method of Lagrange multipliers to solve this constrained optimization problem. The Lagrangian $\\mathcal{L}$ is constructed as:\n$$\\mathcal{L}(p_1, p_2, p_3, \\lambda_1, \\lambda_2) = H - \\lambda_1 g_1 - \\lambda_2 g_2$$\n$$\\mathcal{L} = -\\sum_{i=1}^{3} p_i \\ln(p_i) - \\lambda_1 (p_1+p_2+p_3-1) - \\lambda_2 (p_1 E_1 + p_2 E_2 + p_3 E_3 - \\langle E \\rangle)$$\nTo find the maximum, we take the partial derivative of $\\mathcal{L}$ with respect to each $p_i$ and set it to zero.\n$$\\frac{\\partial \\mathcal{L}}{\\partial p_i} = -(\\ln(p_i) + 1) - \\lambda_1 - \\lambda_2 E_i = 0$$\nSolving for $p_i$:\n$$\\ln(p_i) = -1 - \\lambda_1 - \\lambda_2 E_i$$\n$$p_i = \\exp(-1 - \\lambda_1 - \\lambda_2 E_i) = \\exp(-1-\\lambda_1) \\exp(-\\lambda_2 E_i)$$\nLet's define a new constant $\\beta = \\lambda_2$. The term $\\exp(-1-\\lambda_1)$ is a constant that can be determined by the normalization constraint. Let's call it $1/Z$. The probability distribution then takes the form of a Boltzmann distribution, which is a member of the exponential family:\n$$p_i = \\frac{1}{Z} \\exp(-\\beta E_i)$$\nThe normalization constant $Z$, also known as the partition function, is found by summing the probabilities:\n$$Z = \\sum_{i=1}^{3} \\exp(-\\beta E_i)$$\n\nNow, we use the given values: $E_1=\\epsilon_0, E_2=2\\epsilon_0, E_3=3\\epsilon_0$, and $\\langle E \\rangle = \\frac{7}{3}\\epsilon_0$. The average energy constraint is:\n$$\\langle E \\rangle = \\sum_{i=1}^{3} p_i E_i = \\frac{1}{Z} \\sum_{i=1}^{3} E_i \\exp(-\\beta E_i) = \\frac{7}{3}\\epsilon_0$$\nSubstituting the expressions for $E_i$:\n$$\\frac{\\epsilon_0 \\exp(-\\beta \\epsilon_0) + 2\\epsilon_0 \\exp(-2\\beta \\epsilon_0) + 3\\epsilon_0 \\exp(-3\\beta \\epsilon_0)}{\\exp(-\\beta \\epsilon_0) + \\exp(-2\\beta \\epsilon_0) + \\exp(-3\\beta \\epsilon_0)} = \\frac{7}{3}\\epsilon_0$$\nThe constant $\\epsilon_0$ cancels from both sides. Let's introduce a variable $x = \\exp(-\\beta \\epsilon_0)$. The equation becomes:\n$$\\frac{x + 2x^2 + 3x^3}{x + x^2 + x^3} = \\frac{7}{3}$$\nSince $x$ cannot be zero (as probabilities would be undefined), we can factor out $x$ from the numerator and denominator:\n$$\\frac{1 + 2x + 3x^2}{1 + x + x^2} = \\frac{7}{3}$$\nNow we solve for $x$:\n$$3(1 + 2x + 3x^2) = 7(1 + x + x^2)$$\n$$3 + 6x + 9x^2 = 7 + 7x + 7x^2$$\n$$2x^2 - x - 4 = 0$$\nThis is a quadratic equation for $x$. Using the quadratic formula $x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$:\n$$x = \\frac{1 \\pm \\sqrt{(-1)^2 - 4(2)(-4)}}{2(2)} = \\frac{1 \\pm \\sqrt{1 + 32}}{4} = \\frac{1 \\pm \\sqrt{33}}{4}$$\nSince $x = \\exp(-\\beta \\epsilon_0)$ and $\\epsilon_0>0$ is a physical energy, $\\beta$ must be real, which means $x$ must be a positive real number. Therefore, we take the positive root:\n$$x = \\frac{1 + \\sqrt{33}}{4}$$\nThe problem asks for the probability $p_2$. We express $p_2$ in terms of $x$:\n$$p_2 = \\frac{\\exp(-2\\beta\\epsilon_0)}{Z} = \\frac{\\exp(-2\\beta\\epsilon_0)}{\\exp(-\\beta\\epsilon_0) + \\exp(-2\\beta\\epsilon_0) + \\exp(-3\\beta\\epsilon_0)}$$\n$$p_2 = \\frac{x^2}{x + x^2 + x^3} = \\frac{x}{1 + x + x^2}$$\nFrom the quadratic equation $2x^2 - x - 4 = 0$, we have $x^2 = \\frac{x+4}{2}$. Let's substitute this into the denominator of the expression for $p_2$:\n$$1 + x + x^2 = 1 + x + \\frac{x+4}{2} = \\frac{2 + 2x + x + 4}{2} = \\frac{3x+6}{2}$$\nNow substitute this back into the expression for $p_2$:\n$$p_2 = \\frac{x}{\\frac{3x+6}{2}} = \\frac{2x}{3x+6} = \\frac{2x}{3(x+2)}$$\nFinally, we substitute the value of $x = \\frac{1+\\sqrt{33}}{4}$:\n$$p_2 = \\frac{2(\\frac{1+\\sqrt{33}}{4})}{3(\\frac{1+\\sqrt{33}}{4} + 2)} = \\frac{\\frac{1+\\sqrt{33}}{2}}{3(\\frac{1+\\sqrt{33}+8}{4})} = \\frac{1+\\sqrt{33}}{2} \\cdot \\frac{4}{3(9+\\sqrt{33})} = \\frac{2(1+\\sqrt{33})}{3(9+\\sqrt{33})}$$\nTo simplify, we rationalize the denominator by multiplying the numerator and denominator by the conjugate of $(9+\\sqrt{33})$, which is $(9-\\sqrt{33})$:\n$$p_2 = \\frac{2(1+\\sqrt{33})(9-\\sqrt{33})}{3(9+\\sqrt{33})(9-\\sqrt{33})} = \\frac{2(9 - \\sqrt{33} + 9\\sqrt{33} - 33)}{3(9^2 - (\\sqrt{33})^2)} = \\frac{2(8\\sqrt{33} - 24)}{3(81 - 33)}$$\n$$p_2 = \\frac{2 \\cdot 8(\\sqrt{33} - 3)}{3(48)} = \\frac{16(\\sqrt{33} - 3)}{144}$$\nSimplifying the fraction $\\frac{16}{144} = \\frac{1}{9}$:\n$$p_2 = \\frac{\\sqrt{33} - 3}{9}$$",
            "answer": "$$\\boxed{\\frac{\\sqrt{33}-3}{9}}$$"
        },
        {
            "introduction": "The utility of the exponential family framework extends beyond simply categorizing distributions; it provides a powerful computational machinery. A key feature is the log-partition function, $A(\\boldsymbol{\\eta})$, which not only normalizes the distribution but also acts as a generator for its moments. This final practice invites you to work with a custom-defined, two-parameter exponential family and use a derivative of its log-partition function to efficiently calculate an expected value . This exercise highlights the elegance of the framework, allowing you to compute statistical properties that would otherwise require more cumbersome calculations.",
            "id": "1623484",
            "problem": "Consider a discrete random variable $X$ whose domain is the set of integers $\\{0, 1, 2\\}$. The probability mass function (PMF) of $X$ is described by a two-parameter exponential family of the form:\n$$p(x | \\eta_1, \\eta_2) = \\exp(\\eta_1 T_1(x) + \\eta_2 T_2(x) - A(\\eta_1, \\eta_2))$$\nwhere $\\eta_1$ and $\\eta_2$ are the natural parameters, and $A(\\eta_1, \\eta_2)$ is the log-partition function that normalizes the distribution. The base measure is uniform, i.e., $h(x) = 1$ for all $x$ in the domain.\n\nThe sufficient statistics for this family are defined as:\n$T_1(x) = x$\n$T_2(x) = |x-1|$\n\nFor a specific instance of this distribution, the natural parameters are given as $\\eta_1 = 1.0$ and $\\eta_2 = -1.0$. Calculate the expected value of the random variable, $E[X]$.\n\nRound your final answer to four significant figures.",
            "solution": "The exponential family is given by\n$$p(x \\mid \\eta_{1},\\eta_{2})=\\exp\\!\\left(\\eta_{1}T_{1}(x)+\\eta_{2}T_{2}(x)-A(\\eta_{1},\\eta_{2})\\right),$$\nwith base measure $h(x)=1$, sufficient statistics $T_{1}(x)=x$, $T_{2}(x)=|x-1|$, and log-partition function\n$$A(\\eta_{1},\\eta_{2})=\\ln\\!\\left(\\sum_{x\\in\\{0,1,2\\}}\\exp\\!\\left(\\eta_{1}x+\\eta_{2}|x-1|\\right)\\right).$$\nA standard property of exponential families is\n$$E[T_{1}(X)]=\\frac{\\partial A}{\\partial \\eta_{1}},$$\nso $E[X]=\\frac{\\partial A}{\\partial \\eta_{1}}$.\n\nFor $\\eta_{1}=1$ and $\\eta_{2}=-1$, compute the unnormalized weights\n$$w(x)=\\exp\\!\\left(\\eta_{1}x+\\eta_{2}|x-1|\\right).$$\nFor $x=0,1,2$:\n$$w(0)=\\exp(-1),\\quad w(1)=\\exp(1),\\quad w(2)=\\exp(1).$$\nThus the partition function is\n$$Z=\\sum_{x}w(x)=\\exp(-1)+2\\exp(1),\\quad A=\\ln Z.$$\nThen\n$$E[X]=\\frac{\\partial A}{\\partial \\eta_{1}}=\\frac{1}{Z}\\sum_{x}x\\,w(x)=\\frac{0\\cdot \\exp(-1)+1\\cdot \\exp(1)+2\\cdot \\exp(1)}{\\exp(-1)+2\\exp(1)}=\\frac{3\\exp(1)}{\\exp(-1)+2\\exp(1)}=\\frac{3\\exp(2)}{1+2\\exp(2)}.$$\nNumerically,\n$$E[X]\\approx \\frac{3\\exp(2)}{1+2\\exp(2)}\\approx 1.404931605,$$\nwhich rounded to four significant figures is $1.405$.",
            "answer": "$$\\boxed{1.405}$$"
        }
    ]
}