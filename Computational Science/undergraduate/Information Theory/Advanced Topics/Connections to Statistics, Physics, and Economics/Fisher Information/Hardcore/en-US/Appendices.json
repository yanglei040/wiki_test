{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering Fisher information is to apply its definition to a standard probability distribution. This exercise  provides foundational practice by asking you to calculate the information about the rate parameter of a Gamma distribution, a model frequently used in fields like statistical physics. Successfully completing this problem will build your core computational skills and your intuition for how information relates to a distribution's parameters.",
            "id": "1624973",
            "problem": "In statistical physics, the time intervals between quantum events, such as the decay of radioactive particles, are often modeled using probability distributions. Consider a scenario where a physicist is modeling the waiting time, $T$, until a specific number of decay events, $\\alpha$, have occurred. This waiting time is described by a random variable $T$ following a Gamma distribution.\n\nThe probability density function (PDF) for the Gamma distribution is given by:\n$$f(t; \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} t^{\\alpha-1} \\exp(-\\beta t)$$\nfor $t > 0$, and $f(t; \\alpha, \\beta) = 0$ otherwise. Here, $\\Gamma(\\alpha)$ is the Gamma function.\n\nIn this model, the shape parameter $\\alpha$ is a known positive integer representing the number of events being waited for. The rate parameter $\\beta > 0$ is an unknown physical constant related to the average decay rate of the source, which the physicist wishes to estimate. The Fisher information, $I(\\beta)$, quantifies the amount of information that a single observation of the waiting time $T$ carries about the unknown parameter $\\beta$.\n\nCalculate the Fisher information $I(\\beta)$ for the rate parameter $\\beta$, based on a single observation from this Gamma distribution. Express your answer as a symbolic expression in terms of $\\alpha$ and $\\beta$.",
            "solution": "We have a single observation $T$ from the Gamma distribution with known shape $\\alpha>0$ and unknown rate $\\beta>0$, with density\n$$\nf(t;\\alpha,\\beta)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\,t^{\\alpha-1}\\exp(-\\beta t),\\quad t>0.\n$$\nThe log-likelihood for a single observation is\n$$\n\\ell(\\beta; t)=\\ln f(t;\\alpha,\\beta)=\\alpha\\ln\\beta-\\ln\\Gamma(\\alpha)+(\\alpha-1)\\ln t-\\beta t,\n$$\nwhere $\\Gamma(\\alpha)$ and $t$ are constants with respect to $\\beta$ because $\\alpha$ is known and $t$ is observed.\n\nThe score function is the first derivative with respect to $\\beta$:\n$$\n\\frac{\\partial \\ell}{\\partial \\beta}=\\frac{\\alpha}{\\beta}-t.\n$$\nThe second derivative is\n$$\n\\frac{\\partial^{2}\\ell}{\\partial \\beta^{2}}=-\\frac{\\alpha}{\\beta^{2}}.\n$$\nBy the definition of Fisher information for a single observation (under standard regularity conditions, which hold here since the support does not depend on $\\beta$ and the derivatives exist),\n$$\nI(\\beta)=-\\mathbb{E}\\!\\left[\\frac{\\partial^{2}\\ell}{\\partial \\beta^{2}}\\right]=-\\mathbb{E}\\!\\left[-\\frac{\\alpha}{\\beta^{2}}\\right]=\\frac{\\alpha}{\\beta^{2}}.\n$$\nThus, the Fisher information in one observation about $\\beta$ is $\\alpha/\\beta^{2}$.",
            "answer": "$$\\boxed{\\frac{\\alpha}{\\beta^{2}}}$$"
        },
        {
            "introduction": "In many real-world applications, raw data is processed or summarized before we can analyze it. This practice  presents a fascinating hypothetical scenario where a sensor only reports the parity (odd or even) of a particle count. By calculating the Fisher information from this single binary output, you will explore the important concept of information loss and understand how the nature of an observation fundamentally determines how much we can learn about an unknown parameter.",
            "id": "1624981",
            "problem": "A specialized sensor system is designed to detect particles emitted from a source. The number of particles, $K$, detected within a fixed time interval is a random variable that follows a Poisson distribution with a mean rate $\\lambda$. The probability mass function for this distribution is given by:\n$$P(K=k | \\lambda) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!}, \\quad \\text{for } k \\in \\{0, 1, 2, \\ldots\\}$$\nDue to hardware limitations, the sensor does not report the exact count $K$. Instead, it only outputs a single binary value, $Y$, which indicates the parity of the count. If the number of detected particles $K$ is even (0, 2, 4, ...), the output is $Y=0$. If $K$ is odd (1, 3, 5, ...), the output is $Y=1$.\n\nAssuming you have a single measurement of the binary output $Y$, determine the Fisher information, $I(\\lambda)$, for the parameter $\\lambda$. Express your answer as a closed-form analytic expression in terms of $\\lambda$.",
            "solution": "Let $K \\sim \\text{Poisson}(\\lambda)$ and $Y=\\mathbf{1}\\{K \\text{ is odd}\\}$. We first compute $P(Y=0 \\mid \\lambda)$ and $P(Y=1 \\mid \\lambda)$. Define\n$$\nS \\equiv \\mathbb{E}[(-1)^{K}] = \\sum_{k=0}^{\\infty} (-1)^{k} \\frac{\\lambda^{k} \\exp(-\\lambda)}{k!} = \\exp(-\\lambda) \\sum_{k=0}^{\\infty} \\frac{(-\\lambda)^{k}}{k!} = \\exp(-\\lambda)\\exp(-\\lambda) = \\exp(-2\\lambda).\n$$\nSince $(-1)^{K}=1$ when $K$ is even and $(-1)^{K}=-1$ when $K$ is odd, we have\n$$\nS = P(K \\text{ even}) - P(K \\text{ odd}) = P(Y=0) - P(Y=1),\n$$\nand also $P(Y=0) + P(Y=1) = 1$. Solving gives\n$$\nP(Y=0 \\mid \\lambda) = \\frac{1 + \\exp(-2\\lambda)}{2}, \\quad P(Y=1 \\mid \\lambda) = \\frac{1 - \\exp(-2\\lambda)}{2}.\n$$\nThus $Y \\sim \\text{Bernoulli}(p(\\lambda))$ with\n$$\np(\\lambda) = \\frac{1 - \\exp(-2\\lambda)}{2}.\n$$\nFor a single Bernoulli observation with success probability $p(\\lambda)$, the log-likelihood for $y \\in \\{0,1\\}$ is\n$$\n\\ell(\\lambda; y) = y \\ln p(\\lambda) + (1 - y) \\ln(1 - p(\\lambda)).\n$$\nDifferentiating,\n$$\n\\frac{\\partial \\ell}{\\partial \\lambda} = y \\frac{p'(\\lambda)}{p(\\lambda)} - (1 - y) \\frac{p'(\\lambda)}{1 - p(\\lambda)} = p'(\\lambda) \\left( \\frac{y}{p(\\lambda)} - \\frac{1 - y}{1 - p(\\lambda)} \\right).\n$$\nThe Fisher information is\n$$\nI(\\lambda) = \\mathbb{E}\\!\\left[ \\left( \\frac{\\partial \\ell}{\\partial \\lambda} \\right)^{2} \\right]\n= \\left(p'(\\lambda)\\right)^{2} \\, \\mathbb{E}\\!\\left[ \\left( \\frac{y}{p(\\lambda)} - \\frac{1 - y}{1 - p(\\lambda)} \\right)^{2} \\right].\n$$\nSince $P(Y=1)=p(\\lambda)$ and $P(Y=0)=1-p(\\lambda)$, the inner expectation evaluates to\n$$\np(\\lambda) \\left( \\frac{1}{p(\\lambda)} \\right)^{2} + \\left(1 - p(\\lambda)\\right) \\left( \\frac{1}{1 - p(\\lambda)} \\right)^{2}\n= \\frac{1}{p(\\lambda)} + \\frac{1}{1 - p(\\lambda)} = \\frac{1}{p(\\lambda)\\left(1 - p(\\lambda)\\right)}.\n$$\nHence the Fisher information simplifies to\n$$\nI(\\lambda) = \\frac{\\left(p'(\\lambda)\\right)^{2}}{p(\\lambda)\\left(1 - p(\\lambda)\\right)}.\n$$\nWith $p(\\lambda) = \\frac{1 - \\exp(-2\\lambda)}{2}$, we have\n$$\np'(\\lambda) = \\exp(-2\\lambda),\n$$\nand\n$$\np(\\lambda)\\left(1 - p(\\lambda)\\right) = \\frac{1 - \\exp(-2\\lambda)}{2} \\cdot \\frac{1 + \\exp(-2\\lambda)}{2} = \\frac{1 - \\exp(-4\\lambda)}{4}.\n$$\nTherefore,\n$$\nI(\\lambda) = \\frac{\\exp(-4\\lambda)}{\\frac{1 - \\exp(-4\\lambda)}{4}} = \\frac{4 \\exp(-4\\lambda)}{1 - \\exp(-4\\lambda)} = \\frac{4}{\\exp(4\\lambda) - 1}.\n$$\nThis is the Fisher information for a single binary parity observation $Y$.",
            "answer": "$$\\boxed{\\frac{4}{\\exp(4\\lambda)-1}}$$"
        },
        {
            "introduction": "Most realistic models involve more than one unknown parameter. This exercise  extends the concept of Fisher information to the multi-parameter case by introducing the Fisher Information Matrix. You will compute this matrix for a Gamma distribution with both its shape and scale parameters unknown, learning how this powerful tool quantifies not only the information about each parameter individually but also the correlation in their estimation.",
            "id": "1914846",
            "problem": "In statistical modeling, the Gamma distribution is a versatile two-parameter family of continuous probability distributions. Consider a single random variable $X$ drawn from a Gamma distribution with an unknown shape parameter $\\alpha > 0$ and an unknown scale parameter $\\theta > 0$. The probability density function (PDF) is given by:\n$$f(x; \\alpha, \\theta) = \\frac{1}{\\Gamma(\\alpha) \\theta^{\\alpha}} x^{\\alpha-1} \\exp\\left(-\\frac{x}{\\theta}\\right), \\quad \\text{for } x > 0$$\nwhere $\\Gamma(\\alpha)$ is the standard Gamma function.\n\nThe efficiency of estimators for the parameters $(\\alpha, \\theta)$ is related to the Fisher information matrix. The element in the $i$-th row and $j$-th column of the Fisher information matrix $I(\\boldsymbol{\\eta})$ for a parameter vector $\\boldsymbol{\\eta} = (\\eta_1, \\eta_2, \\dots, \\eta_k)$ is defined as $I_{ij}(\\boldsymbol{\\eta}) = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\eta_i \\partial \\eta_j}\\right]$, where $\\ell$ is the log-likelihood function for a single observation.\n\nFor this problem, you will need the definition of the trigamma function, which is the second derivative of the log-gamma function: $\\psi'(\\alpha) = \\frac{d^2}{d\\alpha^2}\\ln\\Gamma(\\alpha)$.\n\nYour task is to determine the complete $2 \\times 2$ Fisher information matrix, $I(\\alpha, \\theta)$, for the parameter vector $(\\alpha, \\theta)$ based on a single observation $X$ from this Gamma distribution. Express your answer as a matrix whose entries are functions of $\\alpha$, $\\theta$, and potentially the trigamma function $\\psi'(\\alpha)$.",
            "solution": "The problem asks for the Fisher information matrix for the parameters $(\\alpha, \\theta)$ of a Gamma distribution based on a single observation $X$. The parameter vector is $\\boldsymbol{\\eta} = (\\alpha, \\theta)$. The Fisher information matrix $I(\\alpha, \\theta)$ is a $2 \\times 2$ matrix with elements given by:\n$$I_{ij} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\eta_i \\partial \\eta_j}\\right]$$\nwhere $\\ell = \\ell(\\alpha, \\theta | x)$ is the log-likelihood function.\n\nFirst, we write down the log-likelihood function for a single observation $x$ from the Gamma distribution.\nThe PDF is $f(x; \\alpha, \\theta) = \\frac{1}{\\Gamma(\\alpha) \\theta^{\\alpha}} x^{\\alpha-1} \\exp\\left(-\\frac{x}{\\theta}\\right)$.\nTaking the natural logarithm, we get the log-likelihood:\n$$\\ell(\\alpha, \\theta | x) = \\ln\\left( \\frac{1}{\\Gamma(\\alpha) \\theta^{\\alpha}} x^{\\alpha-1} \\exp\\left(-\\frac{x}{\\theta}\\right) \\right)$$\n$$\\ell(\\alpha, \\theta | x) = -\\ln(\\Gamma(\\alpha)) - \\alpha \\ln(\\theta) + (\\alpha-1)\\ln(x) - \\frac{x}{\\theta}$$\n\nNext, we compute the first-order partial derivatives of $\\ell$ with respect to $\\alpha$ and $\\theta$.\nWe use the definition of the digamma function, $\\psi(\\alpha) = \\frac{d}{d\\alpha}\\ln\\Gamma(\\alpha) = \\frac{\\Gamma'(\\alpha)}{\\Gamma(\\alpha)}$.\n$$\\frac{\\partial \\ell}{\\partial \\alpha} = -\\frac{\\Gamma'(\\alpha)}{\\Gamma(\\alpha)} - \\ln(\\theta) + \\ln(x) = -\\psi(\\alpha) - \\ln(\\theta) + \\ln(x)$$\n$$\\frac{\\partial \\ell}{\\partial \\theta} = -\\frac{\\alpha}{\\theta} + \\frac{x}{\\theta^2}$$\n\nNow, we compute the second-order partial derivatives.\nThe derivative of the digamma function is the trigamma function, $\\psi'(\\alpha) = \\frac{d}{d\\alpha}\\psi(\\alpha)$.\n$$\\frac{\\partial^2 \\ell}{\\partial \\alpha^2} = -\\psi'(\\alpha)$$\n$$\\frac{\\partial^2 \\ell}{\\partial \\theta^2} = \\frac{\\alpha}{\\theta^2} - \\frac{2x}{\\theta^3}$$\nAnd the mixed partial derivative:\n$$\\frac{\\partial^2 \\ell}{\\partial \\alpha \\partial \\theta} = \\frac{\\partial}{\\partial \\alpha} \\left( -\\frac{\\alpha}{\\theta} + \\frac{x}{\\theta^2} \\right) = -\\frac{1}{\\theta}$$\nBy Clairaut's theorem, $\\frac{\\partial^2 \\ell}{\\partial \\theta \\partial \\alpha} = \\frac{\\partial^2 \\ell}{\\partial \\alpha \\partial \\theta} = -\\frac{1}{\\theta}$.\n\nThe next step is to find the expectation of the negative of these second derivatives. The expectation is taken with respect to the distribution of $X$, which is Gamma($\\alpha, \\theta$). A key property of the Gamma($\\alpha, \\theta$) distribution is that its expected value is $E[X] = \\alpha\\theta$.\n\nLet's compute the elements of the Fisher information matrix $I(\\alpha, \\theta) = \\begin{pmatrix} I_{11} & I_{12} \\\\ I_{21} & I_{22} \\end{pmatrix}$.\n\nFor $I_{11}$ (related to $\\alpha, \\alpha$):\n$$I_{11} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\alpha^2}\\right] = -E[-\\psi'(\\alpha)]$$\nSince $\\psi'(\\alpha)$ is a function of $\\alpha$ only and does not depend on $x$, its expectation is just the function itself.\n$$I_{11} = \\psi'(\\alpha)$$\n\nFor $I_{22}$ (related to $\\theta, \\theta$):\n$$I_{22} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right] = -E\\left[\\frac{\\alpha}{\\theta^2} - \\frac{2X}{\\theta^3}\\right]$$\nUsing the linearity of expectation:\n$$I_{22} = -\\left(E\\left[\\frac{\\alpha}{\\theta^2}\\right] - E\\left[\\frac{2X}{\\theta^3}\\right]\\right) = -\\left(\\frac{\\alpha}{\\theta^2} - \\frac{2E[X]}{\\theta^3}\\right)$$\nSubstitute $E[X] = \\alpha\\theta$:\n$$I_{22} = -\\left(\\frac{\\alpha}{\\theta^2} - \\frac{2(\\alpha\\theta)}{\\theta^3}\\right) = -\\left(\\frac{\\alpha}{\\theta^2} - \\frac{2\\alpha}{\\theta^2}\\right) = -\\left(-\\frac{\\alpha}{\\theta^2}\\right) = \\frac{\\alpha}{\\theta^2}$$\n\nFor the off-diagonal elements $I_{12}$ and $I_{21}$ (related to $\\alpha, \\theta$):\n$$I_{12} = I_{21} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\alpha \\partial \\theta}\\right] = -E\\left[-\\frac{1}{\\theta}\\right]$$\nThe term $-\\frac{1}{\\theta}$ is a constant with respect to $x$, so its expectation is the constant itself.\n$$I_{12} = I_{21} = \\frac{1}{\\theta}$$\n\nFinally, we assemble the Fisher information matrix:\n$$I(\\alpha, \\theta) = \\begin{pmatrix} I_{11} & I_{12} \\\\ I_{21} & I_{22} \\end{pmatrix} = \\begin{pmatrix} \\psi'(\\alpha) & \\frac{1}{\\theta} \\\\ \\frac{1}{\\theta} & \\frac{\\alpha}{\\theta^2} \\end{pmatrix}$$",
            "answer": "$$\\boxed{\\begin{pmatrix} \\psi'(\\alpha) & \\frac{1}{\\theta} \\\\ \\frac{1}{\\theta} & \\frac{\\alpha}{\\theta^2} \\end{pmatrix}}$$"
        }
    ]
}