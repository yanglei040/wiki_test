{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides an excellent entry point into applying the Principle of Maximum Entropy to systems with multiple components. By modeling two simple binary sensors with a known degree of correlation, you will learn how to construct the most unbiased joint probability distribution possible from limited information . This practice is fundamental for applications in fields like signal processing and machine learning, where we often need to model complex systems with only partial knowledge of their dependencies.",
            "id": "1640107",
            "problem": "An engineering team is modeling a system with two binary sensors, $S_1$ and $S_2$. The output of each sensor is a random variable, denoted by $X$ and $Y$ respectively, where each can take a value from the set $\\{0, 1\\}$. From extensive testing, the team has established only one reliable statistical property of the system's joint behavior: the sensors agree on their output 60% of the time. This is formally stated as the constraint $P(X=Y) = p_c$, where $p_c = 0.6$. No other information regarding correlations or biases in the joint distribution $P(X=x, Y=y)$ is available.\n\nTo create a predictive model that avoids introducing any assumptions not supported by evidence, the team must choose the joint distribution that is maximally non-committal, subject to the known constraint. Based on this principle, what is the probability that the first sensor reports a 1 and the second sensor reports a 0, i.e., what is the value of $P(X=1, Y=0)$?",
            "solution": "Let the joint probabilities be $p_{00} = P(X=0,Y=0)$, $p_{01} = P(X=0,Y=1)$, $p_{10} = P(X=1,Y=0)$, and $p_{11} = P(X=1,Y=1)$. The given constraint is\n$$\np_{00} + p_{11} = p_{c}, \\quad p_{01} + p_{10} = 1 - p_{c},\n$$\nand the normalization is implied by these two equalities.\n\nTo choose the maximally non-committal distribution subject to the constraint, we maximize the Shannon entropy\n$$\nH = -\\sum_{i,j \\in \\{0,1\\}} p_{ij} \\ln p_{ij}\n$$\nsubject to the linear constraints above. Introduce Lagrange multipliers $\\lambda_{1}$ and $\\lambda_{2}$ for the two constraints and consider\n$$\n\\mathcal{L} = -\\sum_{i,j} p_{ij} \\ln p_{ij} + \\lambda_{1}(p_{00} + p_{11} - p_{c}) + \\lambda_{2}(p_{01} + p_{10} - (1 - p_{c})).\n$$\nSetting partial derivatives to zero gives the stationarity conditions\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_{00}} = -\\left(1 + \\ln p_{00}\\right) + \\lambda_{1} = 0, \\quad\n\\frac{\\partial \\mathcal{L}}{\\partial p_{11}} = -\\left(1 + \\ln p_{11}\\right) + \\lambda_{1} = 0,\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_{01}} = -\\left(1 + \\ln p_{01}\\right) + \\lambda_{2} = 0, \\quad\n\\frac{\\partial \\mathcal{L}}{\\partial p_{10}} = -\\left(1 + \\ln p_{10}\\right) + \\lambda_{2} = 0.\n$$\nThese yield\n$$\n\\ln p_{00} = \\lambda_{1} - 1, \\quad \\ln p_{11} = \\lambda_{1} - 1 \\;\\;\\Rightarrow\\;\\; p_{00} = p_{11},\n$$\n$$\n\\ln p_{01} = \\lambda_{2} - 1, \\quad \\ln p_{10} = \\lambda_{2} - 1 \\;\\;\\Rightarrow\\;\\; p_{01} = p_{10}.\n$$\nApplying the constraints gives\n$$\np_{00} = p_{11} = \\frac{p_{c}}{2}, \\quad p_{01} = p_{10} = \\frac{1 - p_{c}}{2}.\n$$\nTherefore,\n$$\nP(X=1, Y=0) = p_{10} = \\frac{1 - p_{c}}{2}.\n$$\nSubstituting $p_{c} = 0.6$,\n$$\nP(X=1, Y=0) = \\frac{1 - 0.6}{2} = 0.2.\n$$",
            "answer": "$$\\boxed{0.2}$$"
        },
        {
            "introduction": "Building on the basics, this problem demonstrates how to incorporate multiple statistical constraints, a common scenario in physical modeling. You will determine the underlying jump probabilities for a quasiparticle's random walk, given macroscopic observations of its average displacement and mean squared displacement . This hands-on practice highlights how the maximum entropy framework allows us to derive a microscopic statistical model from just a few key physical averages, such as the system's \"drift\" (mean) and \"energy\" (variance).",
            "id": "1640135",
            "problem": "A physicist is modeling the one-dimensional diffusion of a special type of quasiparticle through a crystalline lattice. At each time step, the quasiparticle can jump from its current site to a neighboring site. Due to the structure of the lattice, the only possible displacements, measured in units of a fundamental lattice constant, are $\\Delta x \\in \\{-2, -1, 1, 2\\}$.\n\nFrom experimental observations, two macroscopic properties of the diffusion process are known:\n1. The process is unbiased, meaning the average displacement over many steps is zero.\n2. The mean squared displacement per step, a measure related to the effective kinetic energy of the quasiparticle, is exactly $3.0$ (in units of the squared lattice constant).\n\nTo construct a statistical model for this process, the physicist adopts the principle of maximum entropy. This principle posits that, given certain constraints, the most appropriate probability distribution to model a system is the one that maximizes the Shannon information entropy.\n\nAssuming this principle holds, calculate the probability that the quasiparticle makes a jump of displacement $+2$. Express your answer as a decimal rounded to three significant figures.",
            "solution": "Let the possible displacements be $x \\in \\{-2,-1,1,2\\}$ with probabilities $p_{x}$. The constraints are:\n1) Normalization: $\\sum_{x} p_{x} = 1$.\n2) Unbiased: $\\sum_{x} x\\, p_{x} = 0$.\n3) Mean squared displacement: $\\sum_{x} x^{2} p_{x} = 3$.\n\nMaximize the Shannon entropy $H = -\\sum_{x} p_{x} \\ln p_{x}$ subject to these constraints using Lagrange multipliers. Define\n$$\n\\mathcal{L} = -\\sum_{x} p_{x} \\ln p_{x} - \\alpha\\left(\\sum_{x} p_{x} - 1\\right) - \\beta\\left(\\sum_{x} x p_{x}\\right) - \\gamma\\left(\\sum_{x} x^{2} p_{x} - 3\\right).\n$$\nStationarity with respect to $p_{x}$ gives\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_{x}} = -\\left(1+\\ln p_{x}\\right) - \\alpha - \\beta x - \\gamma x^{2} = 0,\n$$\nso\n$$\n\\ln p_{x} = -1 - \\alpha - \\beta x - \\gamma x^{2} \\quad \\Rightarrow \\quad p_{x} = Z^{-1} \\exp\\!\\left(-\\beta x - \\gamma x^{2}\\right),\n$$\nwhere $Z = \\exp(1+\\alpha)$ ensures normalization.\n\nBecause the support $\\{-2,-1,1,2\\}$ and the constraints are invariant under $x \\mapsto -x$, the unique entropy maximizer inherits this symmetry, implying $p_{x} = p_{-x}$. In the exponential-family form this requires $\\beta = 0$. Hence\n$$\np_{x} = Z^{-1} \\exp\\!\\left(-\\gamma x^{2}\\right),\n$$\nso $p_{1} = p_{-1} = a$ and $p_{2} = p_{-2} = b$ for some $a,b$. The constraints reduce to\n$$\n2a + 2b = 1, \\qquad 2a + 8b = 3.\n$$\nSolving, from $a = \\frac{1}{2} - b$ and substituting into the second equation,\n$$\n2\\left(\\frac{1}{2} - b\\right) + 8b = 3 \\;\\Rightarrow\\; 1 - 2b + 8b = 3 \\;\\Rightarrow\\; 6b = 2 \\;\\Rightarrow\\; b = \\frac{1}{3}, \\quad a = \\frac{1}{6}.\n$$\nTherefore, the probability of a jump of displacement $+2$ is $p_{2} = b = \\frac{1}{3}$, which as a decimal rounded to three significant figures is $0.333$.",
            "answer": "$$\\boxed{0.333}$$"
        },
        {
            "introduction": "This final exercise transitions our focus from discrete probabilities to continuous probability distributions, a crucial step for modeling analog systems and physical quantities. You will be tasked with finding the probability density function for a sensor's output, constrained to a specific interval and with a known average value . This problem illustrates the power of maximizing differential entropy and reveals why the exponential family of distributions appears so frequently as the \"most uncertain\" or \"least biased\" model in science and engineering.",
            "id": "1640109",
            "problem": "An electrical engineer is characterizing a novel analog sensor. The sensor's output is a voltage that, after being processed and normalized, can be modeled as a continuous random variable $V$. This normalized voltage is always confined to the interval $[0, 1]$. After extensive testing, the engineer finds that the long-term average value of this normalized voltage is precisely $1/3$. Beyond this, no other information about the sensor's internal workings or noise sources is available. To construct a probabilistic model for the sensor's output, the engineer adopts the principle of maximum entropy, which prescribes selecting the probability distribution that is maximally non-committal, subject to the known constraints.\n\nLet the resulting maximum entropy probability density function (PDF) be denoted by $p(v)$. Your task is to calculate the value of this function at the lower bound of the interval, i.e., find $p(0)$. Report your answer as a real number rounded to three significant figures.",
            "solution": "The problem asks for the probability density function (PDF) $p(v)$ on the interval $[0, 1]$ that maximizes the differential entropy $h(V) = -\\int_0^1 p(v) \\ln(p(v)) dv$, subject to certain constraints.\n\nThe constraints are:\n1.  Normalization of the PDF: $\\int_0^1 p(v) dv = 1$.\n2.  Known expected value (mean): $E[V] = \\int_0^1 v p(v) dv = \\frac{1}{3}$.\n\nThis is a constrained optimization problem in the space of functions. We can solve it using the method of Lagrange multipliers for functionals. We define the Lagrangian functional $L$ as:\n$$L[p] = -\\int_0^1 p(v) \\ln p(v) dv - \\lambda_0 \\left(\\int_0^1 p(v) dv - 1\\right) - \\lambda_1 \\left(\\int_0^1 v p(v) dv - \\frac{1}{3}\\right)$$\nHere, $\\lambda_0$ and $\\lambda_1$ are Lagrange multipliers. To maximize the entropy, we take the functional derivative of $L$ with respect to $p(v)$ and set it to zero.\n$$\\frac{\\delta L}{\\delta p(v)} = 0$$\n$$-(\\ln p(v) + 1) - \\lambda_0 - \\lambda_1 v = 0$$\nSolving for $\\ln p(v)$:\n$$\\ln p(v) = -1 - \\lambda_0 - \\lambda_1 v$$\nExponentiating both sides gives the form of the maximum entropy PDF:\n$$p(v) = \\exp(-1 - \\lambda_0 - \\lambda_1 v) = \\exp(-1 - \\lambda_0) \\exp(-\\lambda_1 v)$$\nFor notational convenience, let's define $C = \\exp(-1 - \\lambda_0)$ and $\\lambda = \\lambda_1$. The form of the PDF is then:\n$$p(v) = C \\exp(-\\lambda v)$$\nNow, we must use the constraints to determine the constants $C$ and $\\lambda$.\n\nFirst, we apply the normalization constraint:\n$$\\int_0^1 C \\exp(-\\lambda v) dv = 1$$\n$$C \\left[ -\\frac{1}{\\lambda} \\exp(-\\lambda v) \\right]_0^1 = 1$$\n$$C \\left( -\\frac{1}{\\lambda} \\exp(-\\lambda) - \\left(-\\frac{1}{\\lambda} \\exp(0)\\right) \\right) = 1$$\n$$C \\left( \\frac{1 - \\exp(-\\lambda)}{\\lambda} \\right) = 1$$\nThis gives us an expression for $C$ in terms of $\\lambda$:\n$$C = \\frac{\\lambda}{1 - \\exp(-\\lambda)}$$\n\nNext, we apply the constraint on the expected value:\n$$E[V] = \\int_0^1 v p(v) dv = \\frac{1}{3}$$\n$$C \\int_0^1 v \\exp(-\\lambda v) dv = \\frac{1}{3}$$\nWe evaluate the integral using integration by parts, $\\int u dv' = uv' - \\int u'v'$. Let $u=v$ and $dv'=\\exp(-\\lambda v)dv$. Then $du=dv$ and $v'=-\\frac{1}{\\lambda}\\exp(-\\lambda v)$.\n$$\\begin{align*} \\int_0^1 v \\exp(-\\lambda v) dv &= \\left[ v \\left(-\\frac{1}{\\lambda}\\exp(-\\lambda v)\\right) \\right]_0^1 - \\int_0^1 \\left(-\\frac{1}{\\lambda}\\exp(-\\lambda v)\\right) dv \\\\ &= \\left(-\\frac{1}{\\lambda}\\exp(-\\lambda) - 0\\right) + \\frac{1}{\\lambda} \\int_0^1 \\exp(-\\lambda v) dv \\\\ &= -\\frac{1}{\\lambda}\\exp(-\\lambda) + \\frac{1}{\\lambda} \\left[ -\\frac{1}{\\lambda}\\exp(-\\lambda v) \\right]_0^1 \\\\ &= -\\frac{1}{\\lambda}\\exp(-\\lambda) - \\frac{1}{\\lambda^2}(\\exp(-\\lambda) - 1) \\\\ &= \\frac{1}{\\lambda^2} - \\frac{1}{\\lambda}\\exp(-\\lambda) - \\frac{1}{\\lambda^2}\\exp(-\\lambda) \\\\ &= \\frac{1 - (\\lambda+1)\\exp(-\\lambda)}{\\lambda^2}\\end{align*}$$\nNow, substitute this result back into the mean value equation:\n$$C \\left( \\frac{1 - (\\lambda+1)\\exp(-\\lambda)}{\\lambda^2} \\right) = \\frac{1}{3}$$\nSubstitute the expression for $C$:\n$$\\left(\\frac{\\lambda}{1 - \\exp(-\\lambda)}\\right) \\left( \\frac{1 - (\\lambda+1)\\exp(-\\lambda)}{\\lambda^2} \\right) = \\frac{1}{3}$$\n$$\\frac{1 - (\\lambda+1)\\exp(-\\lambda)}{\\lambda(1 - \\exp(-\\lambda))} = \\frac{1}{3}$$\nThis expression can be simplified:\n$$\\frac{(1 - \\exp(-\\lambda)) - \\lambda\\exp(-\\lambda)}{\\lambda(1 - \\exp(-\\lambda))} = \\frac{1}{\\lambda} - \\frac{\\exp(-\\lambda)}{1 - \\exp(-\\lambda)} = \\frac{1}{3}$$\nThis is a transcendental equation for $\\lambda$. It cannot be solved for $\\lambda$ using elementary functions. We must find the value of $\\lambda$ numerically. Let $f(\\lambda) = \\frac{1}{\\lambda} - \\frac{\\exp(-\\lambda)}{1 - \\exp(-\\lambda)} - \\frac{1}{3}$. We need to find the root of $f(\\lambda) = 0$. Using a numerical solver (like Newton's method or a graphing calculator's root-finding feature), we find the positive real root to be:\n$$\\lambda \\approx 2.14835$$\nThe problem asks for the value of $p(0)$. From our general form, $p(v) = C \\exp(-\\lambda v)$, this is:\n$$p(0) = C \\exp(-\\lambda \\cdot 0) = C$$\nWe have the expression for $C$ in terms of $\\lambda$:\n$$p(0) = C = \\frac{\\lambda}{1 - \\exp(-\\lambda)}$$\nSubstituting the numerical value of $\\lambda$:\n$$p(0) = \\frac{2.14835}{1 - \\exp(-2.14835)} = \\frac{2.14835}{1 - 0.11667...} = \\frac{2.14835}{0.88332...} \\approx 2.43207$$\nRounding the final answer to three significant figures, we get 2.43.",
            "answer": "$$\\boxed{2.43}$$"
        }
    ]
}