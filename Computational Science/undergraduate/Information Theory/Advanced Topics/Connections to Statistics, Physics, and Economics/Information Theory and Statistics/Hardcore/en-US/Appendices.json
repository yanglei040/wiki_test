{
    "hands_on_practices": [
        {
            "introduction": "To begin our exploration of information theory, we start with its most fundamental concept: Shannon entropy. This exercise provides a concrete calculation of entropy for a discrete random variable, which serves as the bedrock for quantifying uncertainty. By working through this problem , you will practice deriving the probability distribution of a new variable from an existing one and then apply the core formula for entropy.",
            "id": "1631970",
            "problem": "A discrete random variable $X$ is defined on the set of outcomes $\\{0, 1, 2, 3, 4, 5, 6, 7\\}$. The probability mass function for $X$ is uniform over this set. A second random variable, $Y$, is derived from $X$ through the transformation $Y = X \\pmod 4$.\n\nCalculate the Shannon entropy of the random variable $Y$. Express your answer as a closed-form analytic expression in units of bits.",
            "solution": "Let $X$ be uniformly distributed on $\\{0,1,2,3,4,5,6,7\\}$, so for every $x \\in \\{0,1,2,3,4,5,6,7\\}$,\n$$\n\\Pr(X=x)=\\frac{1}{8}.\n$$\nDefine $Y = X \\bmod 4$. Then $Y$ takes values in $\\{0,1,2,3\\}$. For any $y \\in \\{0,1,2,3\\}$,\n$$\n\\Pr(Y=y)=\\Pr(X \\in \\{y, y+4\\})=\\Pr(X=y)+\\Pr(X=y+4)=\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4}.\n$$\nThus $Y$ is uniform on $\\{0,1,2,3\\}$.\n\nThe Shannon entropy of $Y$ in bits is\n$$\nH(Y)=-\\sum_{y=0}^{3} \\Pr(Y=y)\\,\\log_{2}\\bigl(\\Pr(Y=y)\\bigr)\n= -4 \\cdot \\frac{1}{4}\\, \\log_{2}\\!\\left(\\frac{1}{4}\\right)\n= - \\log_{2}\\!\\left(\\frac{1}{4}\\right).\n$$\nUsing $\\log_{2}\\!\\left(\\frac{1}{4}\\right)=\\log_{2}\\!\\left(2^{-2}\\right)=-2$, we obtain\n$$\nH(Y)=2.\n$$\nThe entropy is therefore $2$ bits.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "While Shannon entropy is defined for discrete events, many real-world phenomena are continuous. This practice problem  introduces differential entropy, the analogue of Shannon entropy for continuous random variables. You will calculate the entropy of an exponentially distributed variable, a model frequently used in fields like reliability engineering to describe the lifetime of components.",
            "id": "1631980",
            "problem": "In reliability engineering, the lifetime of certain electronic components is often modeled using a continuous probability distribution. Consider a component whose lifetime, represented by the random variable $T$ (in hours), follows an exponential distribution. The Probability Density Function (PDF) for $T$ is given by:\n$$\nf(t) = \\begin{cases} \\lambda \\exp(-\\lambda t) & \\text{for } t \\ge 0 \\\\ 0 & \\text{for } t < 0 \\end{cases}\n$$\nwhere $\\lambda > 0$ is the constant rate parameter, representing the failure rate of the component.\n\nIn information theory, the uncertainty associated with a continuous random variable is quantified by its differential entropy. For a random variable $X$ with PDF $f(x)$, the differential entropy $h(X)$ is defined as:\n$$\nh(X) = -\\int_{-\\infty}^{\\infty} f(x) \\ln(f(x)) \\, dx\n$$\nUsing these definitions, determine the differential entropy, $h(T)$, of the component's lifetime. Express your answer as a closed-form analytic expression in terms of the rate parameter $\\lambda$.",
            "solution": "The differential entropy of a continuous random variable with PDF $f(x)$ is defined by\n$$\nh(X)=-\\int_{-\\infty}^{\\infty} f(x)\\ln\\big(f(x)\\big)\\,dx.\n$$\nFor the exponential lifetime $T$ with rate parameter $\\lambda>0$, the PDF is $f(t)=\\lambda\\exp(-\\lambda t)$ for $t\\ge 0$ and $f(t)=0$ for $t<0$. Therefore, the entropy integral reduces to\n$$\nh(T)=-\\int_{0}^{\\infty} \\lambda \\exp(-\\lambda t)\\,\\ln\\big(\\lambda \\exp(-\\lambda t)\\big)\\,dt.\n$$\nUsing $\\ln\\big(\\lambda \\exp(-\\lambda t)\\big)=\\ln(\\lambda)-\\lambda t$, we obtain\n$$\nh(T)=-\\int_{0}^{\\infty} \\lambda \\exp(-\\lambda t)\\big(\\ln(\\lambda)-\\lambda t\\big)\\,dt\n= -\\left[\\ln(\\lambda)\\int_{0}^{\\infty}\\lambda \\exp(-\\lambda t)\\,dt-\\int_{0}^{\\infty}\\lambda^{2} t \\exp(-\\lambda t)\\,dt\\right].\n$$\nFor the first integral, use the substitution $u=\\lambda t$, $dt=du/\\lambda$, giving\n$$\n\\int_{0}^{\\infty}\\lambda \\exp(-\\lambda t)\\,dt=\\int_{0}^{\\infty}\\exp(-u)\\,du=\\left[-\\exp(-u)\\right]_{0}^{\\infty}=1.\n$$\nFor the second integral, use the same substitution $u=\\lambda t$, $dt=du/\\lambda$:\n$$\n\\int_{0}^{\\infty}\\lambda^{2} t \\exp(-\\lambda t)\\,dt=\\int_{0}^{\\infty}u \\exp(-u)\\,du.\n$$\nEvaluate this by integration by parts with $a=u$, $db=\\exp(-u)\\,du$ so that $da=du$ and $b=-\\exp(-u)$:\n$$\n\\int_{0}^{\\infty}u \\exp(-u)\\,du=\\left[-u\\exp(-u)\\right]_{0}^{\\infty}+\\int_{0}^{\\infty}\\exp(-u)\\,du=0+1=1.\n$$\nSubstituting these results back yields\n$$\nh(T)=-\\big(\\ln(\\lambda)\\cdot 1-1\\big)=1-\\ln(\\lambda).\n$$\nThus, the differential entropy of the exponential lifetime $T$ is $1-\\ln(\\lambda)$.",
            "answer": "$$\\boxed{1-\\ln(\\lambda)}$$"
        },
        {
            "introduction": "Beyond simply measuring the uncertainty of a distribution, information theory provides powerful tools for statistical inference. This exercise introduces Fisher Information, a crucial concept that quantifies how much information a set of observations provides about an unknown parameter . By analyzing a simple communication system, you will learn how to calculate this quantity, which lies at the heart of modern estimation theory and experimental design.",
            "id": "1632005",
            "problem": "In a digital communication system, a sequence of $n$ identical bits is transmitted through a noisy channel to establish a baseline for its reliability. Each transmitted bit is a '1'. Due to noise, each bit reception is an independent random event. The probability of correctly receiving a '1' is denoted by $p$, where $0 < p < 1$. Correspondingly, the probability of an error (receiving a '0') is $1-p$. The sequence of $n$ received bits can thus be modeled as a set of $n$ independent Bernoulli trials.\n\nThe Fisher Information is a fundamental concept in information theory and statistics that quantifies the amount of information a random variable carries about an unknown parameter upon which its probability depends. For this communication system, the parameter of interest is the success probability $p$.\n\nDetermine the Fisher Information, $I(p)$, for the parameter $p$ based on the sequence of $n$ received bits. Express your answer as a single closed-form analytic expression in terms of $n$ and $p$.",
            "solution": "The problem asks for the Fisher Information $I(p)$ for the success probability parameter $p$ of $n$ independent Bernoulli trials.\n\nLet the sequence of received bits be represented by the random variables $X_1, X_2, \\ldots, X_n$. Each $X_i$ is a Bernoulli random variable where $X_i=1$ if the bit is received correctly (success) and $X_i=0$ if it is received incorrectly (failure). The probability mass function (PMF) for a single trial $X_i$ is given by:\n$$P(X_i=x | p) = p^x (1-p)^{1-x} \\quad \\text{for } x \\in \\{0, 1\\}$$\n\nSince the trials are independent, the joint probability mass function, or the likelihood function $L(p)$ for a specific observed sequence $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$, is the product of the individual PMFs:\n$$L(p | \\mathbf{x}) = \\prod_{i=1}^{n} P(X_i=x_i | p) = \\prod_{i=1}^{n} p^{x_i} (1-p)^{1-x_i}$$\nThis can be simplified by combining the exponents:\n$$L(p | \\mathbf{x}) = p^{\\sum_{i=1}^{n} x_i} (1-p)^{\\sum_{i=1}^{n} (1-x_i)} = p^{\\sum x_i} (1-p)^{n - \\sum x_i}$$\nLet $k = \\sum_{i=1}^{n} x_i$ be the total number of successes (correctly received bits) in the sequence of $n$ trials. The likelihood function becomes:\n$$L(p | k) = p^k (1-p)^{n-k}$$\n\nThe Fisher Information is typically calculated using the log-likelihood function, denoted by $\\ell(p)$. The log-likelihood is the natural logarithm of the likelihood function:\n$$\\ell(p | k) = \\ln(L(p | k)) = \\ln(p^k (1-p)^{n-k}) = k \\ln(p) + (n-k) \\ln(1-p)$$\n\nOne definition of the Fisher Information $I(p)$ is the negative of the expected value of the second derivative of the log-likelihood function with respect to the parameter $p$:\n$$I(p) = -E\\left[\\frac{\\partial^2 \\ell(p|k)}{\\partial p^2}\\right]$$\n\nFirst, we find the first derivative of the log-likelihood with respect to $p$. This is known as the score function.\n$$\\frac{\\partial \\ell}{\\partial p} = \\frac{\\partial}{\\partial p} [k \\ln(p) + (n-k) \\ln(1-p)] = \\frac{k}{p} - \\frac{n-k}{1-p}$$\n\nNext, we find the second derivative of the log-likelihood with respect to $p$:\n$$\\frac{\\partial^2 \\ell}{\\partial p^2} = \\frac{\\partial}{\\partial p} \\left[\\frac{k}{p} - \\frac{n-k}{1-p}\\right] = -\\frac{k}{p^2} - \\frac{n-k}{(1-p)^2}(-1) = -\\frac{k}{p^2} - \\frac{n-k}{(1-p)^2}$$\n\nNow, we compute the expected value of this second derivative. The random variable here is $k$, the number of successes. Since $k$ is the sum of $n$ independent and identically distributed Bernoulli trials with success probability $p$, $k$ follows a binomial distribution, $k \\sim \\text{Bin}(n, p)$. The expected value of $k$ is $E[k] = np$.\n\nWe substitute this into the expression for $I(p)$:\n$$I(p) = -E\\left[-\\frac{k}{p^2} - \\frac{n-k}{(1-p)^2}\\right] = E\\left[\\frac{k}{p^2} + \\frac{n-k}{(1-p)^2}\\right]$$\nBy linearity of expectation:\n$$I(p) = \\frac{E[k]}{p^2} + \\frac{E[n-k]}{(1-p)^2}$$\nWe know $E[k] = np$. Also, $E[n-k] = E[n] - E[k] = n - np = n(1-p)$.\nSubstituting these expected values:\n$$I(p) = \\frac{np}{p^2} + \\frac{n(1-p)}{(1-p)^2}$$\nSimplifying the terms gives:\n$$I(p) = \\frac{n}{p} + \\frac{n}{1-p}$$\nCombining the terms over a common denominator:\n$$I(p) = n \\left(\\frac{1-p+p}{p(1-p)}\\right) = \\frac{n}{p(1-p)}$$\n\nThus, the Fisher Information for the parameter $p$ from $n$ Bernoulli trials is $\\frac{n}{p(1-p)}$. This result represents the total information from all $n$ trials, which is simply $n$ times the Fisher Information for a single trial, $I_1(p) = \\frac{1}{p(1-p)}$.",
            "answer": "$$\\boxed{\\frac{n}{p(1-p)}}$$"
        }
    ]
}