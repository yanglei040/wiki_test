{
    "hands_on_practices": [
        {
            "introduction": "To begin our practical exploration of cross-entropy, we will start with a foundational scenario. This exercise provides a clear and simple context—a fair coin flip—to introduce the core calculation of cross-entropy. By quantifying the inefficiency of a biased model's predictions, you will gain a concrete understanding of how cross-entropy measures the divergence between a predicted probability distribution, $Q$, and a true one, $P$ .",
            "id": "1615167",
            "problem": "A data scientist is evaluating a predictive model for a binary random process. The true underlying process is known to be perfectly random, equivalent to a fair coin flip with the set of outcomes {Outcome A, Outcome B}. Let this true probability distribution be denoted by $P$.\n\nThe predictive model, however, has developed a bias. It predicts the outcomes according to a different probability distribution, denoted by $Q$. The model assigns a probability of 0.9 to 'Outcome A' and a probability of 0.1 to 'Outcome B'.\n\nCalculate the cross-entropy $H(P, Q)$ between the true distribution $P$ and the model's distribution $Q$. Express your answer in bits, rounded to four significant figures.",
            "solution": "We are asked for the cross-entropy in bits between the true distribution $P$ of a fair binary process and the model distribution $Q$ with $Q(\\text{A})=0.9$ and $Q(\\text{B})=0.1$.\n\nBy definition, for a discrete distribution on outcomes $x$ with true probabilities $P(x)$ and model probabilities $Q(x)$, the cross-entropy in bits is\n$$\nH(P,Q) \\equiv - \\sum_{x} P(x)\\,\\log_{2}\\big(Q(x)\\big).\n$$\nFor a fair process, $P(\\text{A})=\\frac{1}{2}$ and $P(\\text{B})=\\frac{1}{2}$. Therefore,\n$$\nH(P,Q) = -\\left[\\frac{1}{2}\\log_{2}(0.9) + \\frac{1}{2}\\log_{2}(0.1)\\right]\n= -\\frac{1}{2}\\left[\\log_{2}(0.9)+\\log_{2}(0.1)\\right]\n= -\\frac{1}{2}\\log_{2}\\big(0.9\\times 0.1\\big)\n= -\\frac{1}{2}\\log_{2}(0.09).\n$$\nEvaluating numerically (in bits),\n$$\n\\log_{2}(0.9) \\approx -0.152003093445049,\\quad \\log_{2}(0.1) \\approx -3.321928094887362,\n$$\nso\n$$\nH(P,Q) \\approx -\\frac{1}{2}\\big(-0.152003093445049-3.321928094887362\\big)\n= -\\frac{1}{2}\\big(-3.473931188332411\\big)\n= 1.736965594166206.\n$$\nRounding to four significant figures gives $1.737$ bits.",
            "answer": "$$\\boxed{1.737}$$"
        },
        {
            "introduction": "Building on the basic calculation, we now examine a critical edge case that reveals a profound property of cross-entropy. This problem explores what happens when a predictive model is not just inaccurate, but unjustifiably certain, assigning zero probability to an event that is actually possible. Understanding why this leads to infinite cross-entropy is an essential lesson in building robust machine learning models that avoid making absolute predictions .",
            "id": "1615201",
            "problem": "A data scientist is evaluating two computational models for predicting the weather in a specific city. The set of possible weather outcomes is {Sunny, Cloudy, Rainy}. Based on extensive historical data, the true probability distribution, $P$, for these outcomes is known:\n- $P(\\text{Sunny}) = 0.5$\n- $P(\\text{Cloudy}) = 0.3$\n- $P(\\text{Rainy}) = 0.2$\n\nThe first model, Model A, is a probabilistic forecaster that provides its own probability distribution, $Q_A$, for the next day's weather:\n- $Q_A(\\text{Sunny}) = 0.7$\n- $Q_A(\\text{Cloudy}) = 0.2$\n- $Q_A(\\text{Rainy}) = 0.1$\n\nThe second model, Model B, is a simple, deterministic model that always predicts a \"Sunny\" day. Its predicted probability distribution, $Q_B$, is therefore:\n- $Q_B(\\text{Sunny}) = 1$\n- $Q_B(\\text{Cloudy}) = 0$\n- $Q_B(\\text{Rainy}) = 0$\n\nTo evaluate how well each model's predictions align with the true distribution, the data scientist computes the cross-entropy, $H(P, Q)$, measured in bits. The formula for cross-entropy is given by $H(P, Q) = -\\sum_{x} P(x) \\log_2(Q(x))$, where the sum is over all possible outcomes $x$.\n\nDetermine the cross-entropy for Model A relative to the true distribution, $H(P, Q_A)$, and the cross-entropy for Model B relative to the true distribution, $H(P, Q_B)$. Select the option that correctly reports both values.\n\nA. $H(P, Q_A) = 1.485$; $H(P, Q_B) = 0$\n\nB. $H(P, Q_A) = 1.122$; $H(P, Q_B)$ is undefined\n\nC. $H(P, Q_A) = 1.618$; $H(P, Q_B)$ is infinite\n\nD. $H(P, Q_A) = 1.618$; $H(P, Q_B) = 0$\n\nE. $H(P, Q_A) = 1.280$; $H(P, Q_B)$ is infinite",
            "solution": "We use the cross-entropy definition in bits:\n$$H(P,Q)=-\\sum_{x}P(x)\\log_{2}\\bigl(Q(x)\\bigr).$$\n\nFor Model A,\n$$H(P,Q_{A})=-\\Bigl[0.5\\,\\log_{2}(0.7)+0.3\\,\\log_{2}(0.2)+0.2\\,\\log_{2}(0.1)\\Bigr].$$\nUsing the change of base $\\log_{2}(x)=\\frac{\\ln x}{\\ln 2}$ to obtain numerical values:\n$$\\log_{2}(0.7)\\approx-0.5145731728,\\quad \\log_{2}(0.2)=-2.3219280949,\\quad \\log_{2}(0.1)=-3.3219280949.$$\nThus,\n$$0.5\\,\\log_{2}(0.7)\\approx-0.2572865864,\\quad 0.3\\,\\log_{2}(0.2)\\approx-0.6965784285,\\quad 0.2\\,\\log_{2}(0.1)\\approx-0.6643856190,$$\nso the sum inside the brackets is approximately $-1.6182506339$, and therefore\n$$H(P,Q_{A})\\approx 1.6182506339\\ \\text{bits}\\approx 1.618.$$\n\nFor Model B,\n$$H(P,Q_{B})=-\\Bigl[0.5\\,\\log_{2}(1)+0.3\\,\\log_{2}(0)+0.2\\,\\log_{2}(0)\\Bigr].$$\nSince $\\log_{2}(1)=0$ and $\\log_{2}(0)=-\\infty$, with positive weights $0.3$ and $0.2$, the cross-entropy diverges:\n$$H(P,Q_{B})=+\\infty.$$\n\nTherefore, the correct option is the one reporting $H(P,Q_{A})=1.618$ and $H(P,Q_{B})$ is infinite, which is option C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Having learned to calculate and interpret cross-entropy, we now shift our focus to one of its most powerful applications: model optimization. This practice moves beyond mere evaluation to demonstrate how cross-entropy serves as a loss function to actively improve a model. By finding the parameter $\\theta$ that minimizes the cross-entropy, you will be performing a fundamental task in machine learning—tuning a model to best approximate the true data distribution .",
            "id": "1615207",
            "problem": "In the field of high-energy physics, machine learning models are often used to classify events recorded by detectors. Consider a simplified scenario where a particle collider experiment produces events that fall into one of three distinct categories: Category 1, Category 2, or Category 3. Based on a well-established theoretical framework, the true probability distribution, $P$, for an event to belong to each category is given by $P = (p_1, p_2, p_3) = (1/2, 1/3, 1/6)$.\n\nA data scientist proposes a new, computationally inexpensive model, $Q_\\theta$, to approximate this true distribution. The model's probabilities depend on a single tunable parameter $\\theta$ and are given by $Q_\\theta = (q_1(\\theta), q_2(\\theta), q_3(\\theta)) = (\\theta, 1 - 2\\theta, \\theta)$. For $Q_\\theta$ to represent a valid probability distribution, the parameter $\\theta$ must be in the range $[0, 1/2]$.\n\nTo find the best version of this model, the scientist decides to tune $\\theta$ by minimizing the cross-entropy, $H(P, Q_\\theta)$, between the true distribution $P$ and the model distribution $Q_\\theta$. The cross-entropy is a measure of the average number of bits needed to encode events from $P$ when using a code optimized for the distribution $Q_\\theta$.\n\nDetermine the exact value of the parameter $\\theta$ that minimizes the cross-entropy $H(P, Q_\\theta)$. Express your answer as a fraction.",
            "solution": "The cross-entropy in bits is defined by\n$$H_{2}(P,Q_{\\theta})=-\\sum_{i=1}^{3}p_{i}\\log_{2}q_{i}.$$\nSince $\\log_{2}x=\\ln x/\\ln 2$, minimizing $H_{2}(P,Q_{\\theta})$ over $\\theta$ is equivalent to minimizing\n$$F(\\theta)=-\\sum_{i=1}^{3}p_{i}\\ln q_{i}.$$\nWith $P=(\\frac{1}{2},\\frac{1}{3},\\frac{1}{6})$ and $Q_{\\theta}=(\\theta,1-2\\theta,\\theta)$ for $\\theta\\in[0,\\frac{1}{2}]$, we have\n$$F(\\theta)=-\\left(\\frac{1}{2}\\ln\\theta+\\frac{1}{3}\\ln(1-2\\theta)+\\frac{1}{6}\\ln\\theta\\right)=-\\left(\\frac{2}{3}\\ln\\theta+\\frac{1}{3}\\ln(1-2\\theta)\\right).$$\nDifferentiate on the open interval $(0,\\frac{1}{2})$:\n$$F'(\\theta)=-\\left(\\frac{2}{3}\\frac{1}{\\theta}+\\frac{1}{3}\\cdot\\frac{-2}{1-2\\theta}\\right)=-\\frac{2}{3}\\left(\\frac{1}{\\theta}-\\frac{1}{1-2\\theta}\\right).$$\nSet $F'(\\theta)=0$ to find stationary points:\n$$\\frac{1}{\\theta}-\\frac{1}{1-2\\theta}=0\\;\\;\\Longrightarrow\\;\\;\\frac{1}{\\theta}=\\frac{1}{1-2\\theta}\\;\\;\\Longrightarrow\\;\\;1-2\\theta=\\theta\\;\\;\\Longrightarrow\\;\\;\\theta=\\frac{1}{3}.$$\nCompute the second derivative to confirm a minimum:\n$$F''(\\theta)=-\\frac{2}{3}\\left(-\\frac{1}{\\theta^{2}}-\\frac{2}{(1-2\\theta)^{2}}\\right)=\\frac{2}{3}\\left(\\frac{1}{\\theta^{2}}+\\frac{2}{(1-2\\theta)^{2}}\\right)>0,$$\nso $F$ is strictly convex on $(0,\\frac{1}{2})$ and the stationary point is the unique global minimizer. The endpoints $\\theta=0$ and $\\theta=\\frac{1}{2}$ are invalid for minimization since $F(\\theta)\\to+\\infty$ as $\\theta\\to 0^{+}$ or $\\theta\\to(\\frac{1}{2})^{-}$ due to $\\ln 0$. Therefore, the minimizer in $[0,\\frac{1}{2}]$ is\n$$\\theta^{\\ast}=\\frac{1}{3}.$$",
            "answer": "$$\\boxed{\\frac{1}{3}}$$"
        }
    ]
}