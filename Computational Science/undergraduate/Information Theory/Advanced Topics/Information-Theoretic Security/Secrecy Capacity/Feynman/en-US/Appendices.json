{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we begin with a classic wiretap channel model. This exercise  presents a scenario where the legitimate channel is perfect, while the eavesdropper's channel is a symmetric noisy channel. This problem allows you to directly apply the fundamental definition of secrecy capacity, $C_s = \\max [I(X;Y) - I(X;Z)]$, and practice calculating the capacities of both the main and eavesdropper's channels to find the solution.",
            "id": "1656645",
            "problem": "A secure communication system is designed to transmit confidential information. The transmitter, Alice, sends a symbol $X$ from a ternary alphabet $\\mathcal{X} = \\{0, 1, 2\\}$ to a legitimate receiver, Bob, and an eavesdropper, Eve.\n\nThe main channel between Alice and Bob is perfect; any symbol transmitted by Alice is received by Bob without error. That is, if Alice sends symbol $X$, Bob receives symbol $Y = X$.\n\nThe eavesdropper's channel between Alice and Eve is a noisy, symmetric channel. If Alice sends symbol $X$, Eve observes a symbol $Z$ from the same alphabet $\\{0, 1, 2\\}$. The probability that Eve receives the correct symbol is $P(Z=X|X) = 1-\\alpha$. If an error occurs, the two possible incorrect symbols are equally likely. The channel parameter is given as $\\alpha = 0.3$.\n\nAssuming information is measured in bits, a single use of the channel corresponds to the transmission of one symbol. Calculate the secrecy capacity of this wiretap channel configuration. Express your answer in bits per channel use, rounded to four significant figures.",
            "solution": "The secrecy capacity $C_S$ of a wiretap channel is defined as the maximum achievable rate of secret communication. For a discrete memoryless wiretap channel, it is given by the optimization problem:\n$$C_S = \\max_{p(X)} [I(X;Y) - I(X;Z)]$$\nwhere $X$ is the random variable for the transmitted symbol, $Y$ is for Bob's received symbol, and $Z$ is for Eve's received symbol. The maximization is performed over all possible probability distributions $p(X)$ for the input symbol $X$.\n\nFirst, let's analyze the mutual information $I(X;Y)$ for the main channel (Alice to Bob). The mutual information is defined as:\n$$I(X;Y) = H(Y) - H(Y|X)$$\nwhere $H(Y)$ is the entropy of the output at Bob and $H(Y|X)$ is the conditional entropy. Since the channel is perfect, $Y=X$. This means there is no uncertainty about $Y$ given $X$, so $H(Y|X) = 0$. Consequently, $Y$ has the same distribution as $X$, which means $H(Y) = H(X)$. Therefore, the mutual information for the main channel is:\n$$I(X;Y) = H(X)$$\n\nNext, let's analyze the mutual information $I(X;Z)$ for the eavesdropper's channel (Alice to Eve). This is given by:\n$$I(X;Z) = H(Z) - H(Z|X)$$\nLet's first compute the conditional entropy $H(Z|X)$.\n$$H(Z|X) = \\sum_{x \\in \\{0,1,2\\}} p(X=x) H(Z|X=x)$$\nwhere $H(Z|X=x)$ is the entropy of the eavesdropper's output given that Alice transmitted symbol $x$.\nThe problem states that the probability of correct reception by Eve is $P(Z=x|X=x) = 1-\\alpha$, and the two incorrect symbols are equally likely. This means for any $x \\in \\{0,1,2\\}$ and $z \\neq x$, the error probability is $P(Z=z|X=x) = \\frac{\\alpha}{2}$.\nLet's calculate $H(Z|X=x)$ for any specific $x$, for example $x=0$:\n$$H(Z|X=0) = -\\sum_{z \\in \\{0,1,2\\}} P(Z=z|X=0) \\log_2 P(Z=z|X=0)$$\n$$H(Z|X=0) = - \\left[ P(0|0)\\log_2 P(0|0) + P(1|0)\\log_2 P(1|0) + P(2|0)\\log_2 P(2|0) \\right]$$\n$$H(Z|X=0) = - \\left[ (1-\\alpha)\\log_2(1-\\alpha) + \\frac{\\alpha}{2}\\log_2\\left(\\frac{\\alpha}{2}\\right) + \\frac{\\alpha}{2}\\log_2\\left(\\frac{\\alpha}{2}\\right) \\right]$$\n$$H(Z|X=0) = - \\left[ (1-\\alpha)\\log_2(1-\\alpha) + \\alpha\\log_2\\left(\\frac{\\alpha}{2}\\right) \\right]$$\nUsing the logarithm property $\\log(a/b) = \\log(a) - \\log(b)$:\n$$H(Z|X=0) = - \\left[ (1-\\alpha)\\log_2(1-\\alpha) + \\alpha\\log_2(\\alpha) - \\alpha\\log_2(2) \\right]$$\n$$H(Z|X=0) = -((1-\\alpha)\\log_2(1-\\alpha) + \\alpha\\log_2(\\alpha)) + \\alpha$$\nThe term in the parentheses is the definition of the binary entropy function, $h_b(\\alpha)$. So, $H(Z|X=x) = h_b(\\alpha) + \\alpha$. Because the channel is symmetric, this value is the same for all input symbols $x$.\nTherefore, the conditional entropy $H(Z|X)$ is:\n$$H(Z|X) = \\sum_{x} p(X=x) (h_b(\\alpha) + \\alpha) = (h_b(\\alpha) + \\alpha) \\sum_{x} p(X=x) = h_b(\\alpha) + \\alpha$$\nThis quantity is independent of the input distribution $p(X)$.\n\nNow we can write the expression for the secrecy capacity as:\n$$C_S = \\max_{p(X)} [H(X) - I(X;Z)]$$\nA crucial observation is that both terms in the original expression, $I(X;Y) = H(X)$ and $I(X;Z)$, are maximized by the same input distribution. $H(X)$ is maximized when the input distribution is uniform, i.e., $p(X=x) = 1/3$ for $x \\in \\{0, 1, 2\\}$. The eavesdropper's channel is a symmetric channel, and the capacity of a symmetric channel is achieved when the input distribution is uniform.\nTherefore, the maximization of the difference is the difference of the maxima:\n$$C_S = \\left(\\max_{p(X)} I(X;Y)\\right) - \\left(\\max_{p(X)} I(X;Z)\\right) = C_B - C_E$$\nwhere $C_B$ and $C_E$ are the capacities of the main and eavesdropper channels, respectively.\n\nThe capacity of the main channel is $C_B = \\max_{p(X)} H(X) = \\log_2(3)$, achieved with a uniform input.\n\nThe capacity of the symmetric eavesdropper channel is $C_E = \\log_2(|\\mathcal{Z}|) - H(\\text{row of transition matrix})$, where $|\\mathcal{Z}|=3$ is the size of the output alphabet and $H(\\text{row})$ is the entropy of any row of the transition matrix, which we found to be $h_b(\\alpha) + \\alpha$.\n$$C_E = \\log_2(3) - (h_b(\\alpha) + \\alpha)$$\n\nSubstituting these capacities back into the expression for $C_S$:\n$$C_S = C_B - C_E = \\log_2(3) - [\\log_2(3) - (h_b(\\alpha) + \\alpha)]$$\n$$C_S = h_b(\\alpha) + \\alpha$$\n\nNow, we substitute the given value $\\alpha=0.3$:\n$$C_S = h_b(0.3) + 0.3 = -[0.3\\log_2(0.3) + (1-0.3)\\log_2(1-0.3)] + 0.3$$\n$$C_S = -[0.3\\log_2(0.3) + 0.7\\log_2(0.7)] + 0.3$$\nWe use the base change formula $\\log_2(x) = \\frac{\\ln(x)}{\\ln(2)}$.\n$\\ln(2) \\approx 0.693147$\n$\\ln(0.3) \\approx -1.20397$\n$\\ln(0.7) \\approx -0.356675$\n\n$\\log_2(0.3) = \\frac{-1.20397}{0.693147} \\approx -1.73697$\n$\\log_2(0.7) = \\frac{-0.356675}{0.693147} \\approx -0.514573$\n\nNow, we calculate $h_b(0.3)$:\n$$h_b(0.3) \\approx -[0.3 \\times (-1.73697) + 0.7 \\times (-0.514573)]$$\n$$h_b(0.3) \\approx -[-0.521091 - 0.360201] = -[-0.881292] = 0.881292$$\n\nFinally, we calculate the secrecy capacity:\n$$C_S = h_b(0.3) + 0.3 \\approx 0.881292 + 0.3 = 1.181292$$\nThe problem asks to round the final answer to four significant figures.\n$$C_S \\approx 1.181$$\nThe secrecy capacity is 1.181 bits per channel use.",
            "answer": "$$\\boxed{1.181}$$"
        },
        {
            "introduction": "Secrecy is not always a matter of signal strength; it can also be achieved by cleverly encoding information to create ambiguity for the eavesdropper. This problem  explores a fascinating scenario where the eavesdropper can only observe the parity of transmitted bit pairs, a deterministic but information-losing process. By working through this exercise, you will learn to reframe the secrecy capacity problem as maximizing the eavesdropper's confusion, or equivocation $H(X|Z)$, and see the necessity of analyzing information in blocks rather than symbol by symbol.",
            "id": "1656681",
            "problem": "An information theorist, Alice, wishes to send a secret message to her colleague, Bob. The communication system is modeled as a discrete memoryless channel where Alice can choose the probability distribution of her input symbols.\n\n1.  Alice's transmitted signal is a sequence of binary digits (bits), $X_1, X_2, \\ldots$.\n2.  Bob's receiver is perfect. His received signal, $Y$, is identical to Alice's transmitted signal, $X$.\n3.  An eavesdropper, Eve, has tapped the communication line. However, her equipment is limited. For any pair of consecutive bits $(X_i, X_{i+1})$ transmitted by Alice, Eve can only observe their parity, $Z_i = X_i \\oplus X_{i+1}$, where $\\oplus$ denotes addition modulo 2 (the Exclusive OR operation).\n\nThe secrecy capacity of this system is the maximum rate of secret communication achievable. Due to the structure of Eve's channel, which creates dependencies, the capacity must be analyzed by considering blocks of input bits. Consider a transmission block of two bits, $X = (X_1, X_2)$. Bob receives the block $Y = (X_1, X_2)$ perfectly, while Eve receives a single bit $Z = X_1 \\oplus X_2$.\n\nThe secrecy capacity, $C_s$, for this block is given by the formula:\n$$C_s = \\max_{p(X_1, X_2)} \\left[ I(X;Y) - I(X;Z) \\right]$$\nwhere $I(A;B)$ is the mutual information between random variables $A$ and $B$, and the maximization is performed over all possible joint probability distributions $p(X_1, X_2)$ for the input block.\n\nDetermine the secrecy capacity of this system. Express your answer in units of bits per single channel use (i.e., per transmitted bit).",
            "solution": "We consider the two-bit block input $X=(X_{1},X_{2})$ taking values in $\\{00,01,10,11\\}$, Bob’s output $Y=X$, and Eve’s output $Z=X_{1}\\oplus X_{2}$, a deterministic function of $X$. The secrecy rate per two-bit block is\n$$\nC_{s,\\text{block}}=\\max_{p(X)}\\left[I(X;Y)-I(X;Z)\\right].\n$$\nBecause $Y$ is a noiseless copy of $X$, we have $I(X;Y)=H(X)$. Because $Z$ is a deterministic function of $X$, we have $I(X;Z)=H(Z)$. Therefore\n$$\nI(X;Y)-I(X;Z)=H(X)-H(Z).\n$$\nUsing the chain rule for entropy with $Z$ a function of $X$, we have $H(X,Z)=H(X)$ and also $H(X,Z)=H(Z)+H(X|Z)$, hence\n$$\nH(X)=H(Z)+H(X|Z)\\quad\\Rightarrow\\quad H(X)-H(Z)=H(X|Z).\n$$\nThus the optimization reduces to\n$$\nC_{s,\\text{block}}=\\max_{p(X)}H(X|Z).\n$$\nFor each value $z\\in\\{0,1\\}$, $Z=z$ restricts $X$ to a set of exactly two possibilities: if $z=0$, then $X\\in\\{00,11\\}$; if $z=1$, then $X\\in\\{01,10\\}$. Hence, for each $z$,\n$$\nH(X|Z=z)\\leq \\log_{2}2=1,\n$$\nwith equality if and only if the two possibilities within that parity class are equiprobable. Therefore,\n$$\nH(X|Z)=\\sum_{z\\in\\{0,1\\}}P(Z=z)\\,H(X|Z=z)\\leq P(Z=0)\\cdot 1+P(Z=1)\\cdot 1=1,\n$$\nso the maximum per two-bit block is at most $1$. This upper bound is achievable by choosing any distribution that is symmetric within each parity class, for example\n$$\np(00)=\\frac{a}{2},\\quad p(11)=\\frac{a}{2},\\quad p(01)=\\frac{1-a}{2},\\quad p(10)=\\frac{1-a}{2},\\quad a\\in[0,1],\n$$\nfor which $H(X|Z=0)=1$ and $H(X|Z=1)=1$, hence $H(X|Z)=1$. Therefore,\n$$\nC_{s,\\text{block}}=1.\n$$\nSince this is the secrecy rate per two-bit block, the secrecy capacity per single channel use (per transmitted bit) is\n$$\nC_{s}=\\frac{C_{s,\\text{block}}}{2}=\\frac{1}{2}.\n$$",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "Moving from discrete models to the more practical domain of continuous channels, this exercise  delves into the Gaussian wiretap channel, a cornerstone of modern communication theory. It presents the counter-intuitive but powerful concept that using a 'sub-optimal' signal for data transmission, like BPSK, can actually provide greater security than an 'ideal' Gaussian signal. This problem highlights a crucial principle of physical layer security: maximizing the *difference* in information rates is a more nuanced goal than simply maximizing the legitimate user's rate.",
            "id": "1656703",
            "problem": "A secure communication system is designed to operate over a Gaussian wiretap channel. A transmitter sends a signal with a fixed average power $P$ to a legitimate receiver, Bob. An eavesdropper, Eve, also intercepts the signal. The channel to Bob is an Additive White Gaussian Noise (AWGN) channel with noise power $\\sigma_B^2$, and the channel to Eve is an independent AWGN channel with noise power $\\sigma_E^2$.\n\nThe secrecy capacity, $C_s$, for a given input signal distribution is defined as the difference between the mutual information of the main channel (transmitter-to-Bob) and the mutual information of the eavesdropper's channel (transmitter-to-Eve), i.e., $C_s = I_B - I_E$. We assume the main channel is superior to the eavesdropper's channel.\n\nThe system operates in a low-power, wideband regime where the Signal-to-Noise Ratio (SNR), defined as $\\text{SNR} = P/\\sigma^2$, is much less than 1. In this regime, information theory provides the following highly accurate approximations for the mutual information, $I(\\text{SNR})$, in units of bits per transmission:\n-   For an ideal Gaussian input signal: $I_{\\text{Gauss}}(\\text{SNR}) \\approx \\frac{1}{\\ln 2} \\left( \\frac{1}{2} \\text{SNR} - \\frac{1}{4} \\text{SNR}^2 \\right)$\n-   For a Binary Phase-Shift Keying (BPSK) input signal, where the signal takes values from $\\{-\\sqrt{P}, +\\sqrt{P}\\}$ with equal probability: $I_{\\text{BPSK}}(\\text{SNR}) \\approx \\frac{1}{\\ln 2} \\left( \\frac{1}{2} \\text{SNR} - \\frac{1}{8} \\text{SNR}^2 \\right)$\n\nGiven the system parameters $P = 0.015$, $\\sigma_B^2 = 0.50$, and $\\sigma_E^2 = 0.75$, calculate the difference in secrecy capacity between using a Gaussian input and a BPSK input, $\\Delta C_s = C_{s, \\text{Gauss}} - C_{s, \\text{BPSK}}$.\n\nExpress your answer in bits per transmission, rounded to three significant figures.",
            "solution": "We define the secrecy capacity for a given input as $C_{s}=I_{B}-I_{E}$, where the mutual informations are approximated in the low-SNR regime as\n$$\nI_{\\text{Gauss}}(\\text{SNR}) \\approx \\frac{1}{\\ln 2}\\left(\\frac{1}{2}\\text{SNR}-\\frac{1}{4}\\text{SNR}^{2}\\right),\\quad\nI_{\\text{BPSK}}(\\text{SNR}) \\approx \\frac{1}{\\ln 2}\\left(\\frac{1}{2}\\text{SNR}-\\frac{1}{8}\\text{SNR}^{2}\\right).\n$$\nThe difference in secrecy capacity between Gaussian and BPSK inputs is\n$$\n\\Delta C_{s}=C_{s,\\text{Gauss}}-C_{s,\\text{BPSK}}=\\big(I_{\\text{Gauss}}(\\text{SNR}_{B})-I_{\\text{BPSK}}(\\text{SNR}_{B})\\big)-\\big(I_{\\text{Gauss}}(\\text{SNR}_{E})-I_{\\text{BPSK}}(\\text{SNR}_{E})\\big).\n$$\nFor any SNR,\n$$\nI_{\\text{Gauss}}(\\text{SNR})-I_{\\text{BPSK}}(\\text{SNR})=\\frac{1}{\\ln 2}\\left(-\\frac{1}{4}\\text{SNR}^{2}+\\frac{1}{8}\\text{SNR}^{2}\\right)=-\\frac{1}{8\\ln 2}\\text{SNR}^{2}.\n$$\nTherefore,\n$$\n\\Delta C_{s}=-\\frac{1}{8\\ln 2}\\left(\\text{SNR}_{B}^{2}-\\text{SNR}_{E}^{2}\\right).\n$$\nWith $P=0.015$, $\\sigma_{B}^{2}=0.50$, and $\\sigma_{E}^{2}=0.75$, the SNRs are\n$$\n\\text{SNR}_{B}=\\frac{P}{\\sigma_{B}^{2}}=\\frac{0.015}{0.50}=0.03,\\qquad\n\\text{SNR}_{E}=\\frac{P}{\\sigma_{E}^{2}}=\\frac{0.015}{0.75}=0.02.\n$$\nHence,\n$$\n\\text{SNR}_{B}^{2}-\\text{SNR}_{E}^{2}=0.03^{2}-0.02^{2}=0.0009-0.0004=0.0005.\n$$\nThus,\n$$\n\\Delta C_{s}=-\\frac{1}{8\\ln 2}\\times 0.0005.\n$$\nEvaluating numerically and rounding to three significant figures,\n$$\n\\Delta C_{s}\\approx -9.02\\times 10^{-5}\\ \\text{bits per transmission}.\n$$",
            "answer": "$$\\boxed{-9.02 \\times 10^{-5}}$$"
        }
    ]
}