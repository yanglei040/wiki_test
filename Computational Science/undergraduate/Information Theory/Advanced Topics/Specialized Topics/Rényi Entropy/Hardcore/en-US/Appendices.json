{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of Rényi entropy, we begin with a foundational exercise. This problem invites you to explore the relationship between a probability distribution's shape and its collision entropy, $H_2(X)$. By determining the distribution that maximizes this entropy for a simple three-symbol source, you will gain hands-on experience with the principle that entropy is maximized as a distribution becomes more uniform. ",
            "id": "1655420",
            "problem": "In information theory, the Rényi entropy of order $\\alpha$ is a generalization of the standard Shannon entropy. For a discrete random variable $X$ that can take $n$ possible values with probabilities $\\{p_1, p_2, \\dots, p_n\\}$, the Rényi entropy is defined as:\n$$H_\\alpha(X) = \\frac{1}{1-\\alpha} \\ln \\left( \\sum_{i=1}^n p_i^\\alpha \\right)$$\nfor any real number $\\alpha \\ge 0$ and $\\alpha \\ne 1$. A particularly useful case is the collision entropy, which corresponds to $\\alpha=2$.\n\nConsider a discrete information source that emits one of three symbols, $\\{s_1, s_2, s_3\\}$. The probabilities of these symbols being emitted are $P(X=s_1) = p$, $P(X=s_2) = p$, and $P(X=s_3) = 1-2p$, where $p$ is a real parameter. To ensure that these probabilities are valid (i.e., non-negative and sum to one), the parameter $p$ is constrained to the interval $[0, 1/2]$.\n\nFind the specific value of $p$ within this interval that maximizes the collision entropy, $H_2(X)$, of the information source.",
            "solution": "For order two, the Rényi (collision) entropy is obtained from the definition\n$$H_{2}(X)=\\frac{1}{1-2}\\ln\\!\\left(\\sum_{i=1}^{n}p_{i}^{2}\\right)=-\\ln\\!\\left(\\sum_{i=1}^{n}p_{i}^{2}\\right).$$\nWith probabilities $\\{p,p,1-2p\\}$, the sum of squares is\n$$S(p)=p^{2}+p^{2}+(1-2p)^{2}=2p^{2}+1-4p+4p^{2}=6p^{2}-4p+1.$$\nSince $S(p)>0$ on $[0,\\,\\frac{1}{2}]$, maximizing $H_{2}(X)=-\\ln(S(p))$ is equivalent to minimizing $S(p)$ on $[0,\\,\\frac{1}{2}]$ because $-\\ln(\\cdot)$ is strictly decreasing on $(0,\\infty)$. The function $S(p)$ is a convex quadratic with\n$$S'(p)=12p-4,\\qquad S''(p)=12>0.$$\nSetting the first derivative to zero gives the unique critical point\n$$12p-4=0\\;\\;\\Longrightarrow\\;\\;p=\\frac{1}{3},$$\nwhich lies in $[0,\\,\\frac{1}{2}]$. By convexity, this point is the global minimizer of $S(p)$ on the interval, hence it maximizes $H_{2}(X)$.",
            "answer": "$$\\boxed{\\frac{1}{3}}$$"
        },
        {
            "introduction": "Rényi entropy is not limited to discrete alphabets; it is a powerful tool for analyzing continuous systems as well. This practice problem takes you into the realm of quantum chaos, where the statistical properties of chaotic systems are described using Random Matrix Theory. You will calculate the Rényi entropy for the Porter-Thomas distribution, which characterizes eigenvector components in such systems, illustrating how information-theoretic measures can quantify complexity and delocalization in physics. ",
            "id": "868932",
            "problem": "In the study of quantum chaos, Random Matrix Theory (RMT) provides a powerful framework for describing the statistical properties of quantum systems whose classical counterparts are chaotic. For systems with time-reversal symmetry, the Hamiltonian can be modeled by a random matrix from the Gaussian Orthogonal Ensemble (GOE). A key prediction of RMT concerns the statistical distribution of the components of the eigenvectors.\n\nLet $\\vec{\\psi}$ be an eigenvector of a large $N \\times N$ GOE matrix, normalized such that $\\sum_{n=1}^N |\\psi_n|^2 = 1$. The components $\\psi_n$ are real-valued. The variable $y = N |\\psi_n|^2$, which represents the normalized intensity of the eigenvector at a specific basis state $n$, follows the Porter-Thomas distribution. The probability density function (PDF) for $y$ is given by:\n$$\nP(y) = \\frac{1}{\\sqrt{2\\pi y}} e^{-y/2}, \\quad \\text{for } y \\ge 0\n$$\nThis distribution is a chi-squared distribution with one degree of freedom ($\\chi_1^2$), and its mean is normalized to $\\langle y \\rangle = \\int_0^\\infty y P(y) dy = 1$.\n\nTo quantify the information content and delocalization of such distributions, one can use measures like the Rényi entropy. For a continuous probability distribution $P(y)$, the Rényi entropy of order $q$ is defined as:\n$$\nS_q = \\frac{1}{1-q} \\ln \\left( \\int_0^\\infty [P(y)]^q dy \\right)\n$$\nwhere $\\ln$ denotes the natural logarithm.\n\nCalculate the Rényi entropy of order $q=1/2$, denoted $S_{1/2}$, for the Porter-Thomas distribution.",
            "solution": "We wish to compute the Rényi entropy of order $q=\\frac12$\n\n$$\nS_{1/2} \\;=\\;\\frac{1}{1-\\tfrac12}\\,\\ln\\!\\Biggl(\\int_{0}^{\\infty}[P(y)]^{1/2}dy\\Biggr)\n\\;=\\;2\\,\\ln I,\n$$\n\nwhere\n\n$$\nP(y)=\\frac{1}{\\sqrt{2\\pi\\,y}}\\;e^{-y/2}, \n\\qquad\n[P(y)]^{1/2}=(2\\pi)^{-1/4}\\,y^{-1/4}\\,e^{-y/4}.\n$$\n\nThus\n\n$$\nI=\\int_{0}^{\\infty}[P(y)]^{1/2}dy\n=(2\\pi)^{-1/4}\\int_{0}^{\\infty}y^{-1/4}e^{-y/4}dy.\n$$\n\nMake the substitution $y=4t$, $dy=4\\,dt$:\n\n$$\n\\int_{0}^{\\infty}y^{-1/4}e^{-y/4}dy\n=\\int_{0}^{\\infty}(4t)^{-1/4}e^{-t}\\,4\\,dt\n=4^{3/4}\\,\\Gamma\\bigl(\\tfrac34\\bigr)\n=2^{3/2}\\,\\Gamma\\bigl(\\tfrac34\\bigr).\n$$\n\nHence\n\n$$\nI=(2\\pi)^{-1/4}\\,2^{3/2}\\,\\Gamma\\bigl(\\tfrac34\\bigr)\n=2^{5/4}\\,\\pi^{-1/4}\\,\\Gamma\\bigl(\\tfrac34\\bigr).\n$$\n\nTherefore\n\n$$\nS_{1/2}=2\\ln I\n=2\\Bigl(\\tfrac54\\ln2-\\tfrac14\\ln\\pi+\\ln\\Gamma(\\tfrac34)\\Bigr)\n=\\frac52\\ln2-\\frac12\\ln\\pi+2\\ln\\Gamma\\!\\bigl(\\tfrac34\\bigr).\n$$",
            "answer": "$$\\boxed{\\frac52\\ln2 - \\frac12\\ln\\pi + 2\\ln\\Gamma\\!\\bigl(\\tfrac34\\bigr)}$$"
        },
        {
            "introduction": "While theoretical formulas provide a clean definition of entropy, in any practical application—from data science to experimental physics—we must estimate these quantities from a finite set of observations. This exercise addresses the crucial question of how reliably we can estimate collision entropy using the straightforward 'plug-in' method. By investigating the bias of this estimator, you will confront a fundamental challenge in statistical inference and appreciate the subtleties of moving from theoretical concepts to data-driven analysis. ",
            "id": "1655435",
            "problem": "In the field of data science, a common task is to quantify the unpredictability of a sequence of events. Consider a system where a user makes a choice from a set of $k$ distinct items. Let the discrete random variable $X$ represent the user's choice, with an alphabet $\\mathcal{X} = \\{x_1, x_2, \\ldots, x_k\\}$. The probability of choosing item $x_i$ is given by the probability mass function $p(x_i)$, where $\\sum_{i=1}^k p(x_i) = 1$.\n\nA measure of the unpredictability of the user's choice is the Rényi entropy of order 2, also known as the collision entropy, defined as:\n$$H_2(X) = -\\ln\\left(\\sum_{i=1}^{k} p(x_i)^2\\right)$$\n\nIn practice, the true probabilities $p(x_i)$ are unknown and must be estimated from data. Suppose we observe $N$ independent and identically distributed (i.i.d.) samples of the user's choices. Let $N_i$ be the number of times item $x_i$ is observed in the $N$ samples, so that $\\sum_{i=1}^k N_i = N$. The empirical probability of choosing $x_i$ is then estimated as $\\hat{p}(x_i) = N_i/N$.\n\nA straightforward way to estimate the entropy is the \"plug-in\" estimator, where the true probabilities in the entropy formula are replaced by their empirical estimates:\n$$\\hat{H}_2(X) = -\\ln\\left(\\sum_{i=1}^{k} \\hat{p}(x_i)^2\\right)$$\n\nThe bias of this estimator is defined as $\\text{Bias}(\\hat{H}_2) = E[\\hat{H}_2(X)] - H_2(X)$, where the expectation $E[\\cdot]$ is taken over all possible sets of $N$ samples from the true distribution.\n\nAssuming the true distribution is not deterministic (i.e., no single $p(x_i)$ is equal to 1) and the number of samples is finite and $N \\ge 2$, what is the sign of the bias of the plug-in estimator $\\hat{H}_2(X)$?\n\nA. The bias is always positive.\n\nB. The bias is always negative.\n\nC. The bias is always zero.\n\nD. The sign of the bias depends on the specific probability distribution $p(x_i)$.\n\nE. The sign of the bias depends on the number of samples $N$.",
            "solution": "Let $S \\equiv \\sum_{i=1}^{k} p(x_{i})^{2}$ and $\\hat{S} \\equiv \\sum_{i=1}^{k} \\hat{p}(x_{i})^{2} = \\sum_{i=1}^{k} \\left(\\frac{N_{i}}{N}\\right)^{2}$. Then $H_{2}(X) = -\\ln S$, and $\\hat{H}_{2}(X) = -\\ln \\hat{S}$.\nWe first compute the mean of $\\hat{S}$. Using $E[N_{i}] = N p(x_{i})$ and $\\operatorname{Var}(N_{i}) = N p(x_{i})\\left(1 - p(x_{i})\\right)$ for the multinomial counts,\n$$\nE[N_{i}^{2}] = \\operatorname{Var}(N_{i}) + \\left(E[N_{i}]\\right)^{2} = N p(x_{i})(1 - p(x_{i})) + N^{2} p(x_{i})^{2}.\n$$\nTherefore,\n$$\nE[\\hat{S}] = \\sum_{i=1}^{k} \\frac{E[N_{i}^{2}]}{N^{2}}\n= \\sum_{i=1}^{k} \\left( p(x_{i})^{2} + \\frac{p(x_{i})(1 - p(x_{i}))}{N} \\right)\n= S + \\frac{1 - S}{N}.\n$$\nUnder the stated assumption (non-deterministic distribution), $S \\in (0,1)$, and for any finite $N \\ge 2$ we have $E[\\hat{S}] - S = \\frac{1 - S}{N} > 0$. Thus, the plug-in estimator of the collision probability $\\hat{S}$ is upward biased.\n\nBecause the entropy estimator is $\\hat{H}_{2} = -\\ln \\hat{S}$ and $-\\ln(\\cdot)$ is decreasing and convex on $(0,\\infty)$, an upward bias in $\\hat{S}$ intuitively induces a downward bias in $\\hat{H}_{2}$. We now make this precise in two complementary ways.\n\n1) Exact evaluation for $N=2$: In this case, two i.i.d. draws either collide (same item) or not. With probability $S$ both samples are the same, yielding $\\hat{S} = 1$; with probability $1 - S$ they are different, yielding $\\hat{S} = \\frac{1}{2}$. Hence\n$$\nE[\\hat{H}_{2}] = E[-\\ln \\hat{S}] = S \\cdot 0 + (1 - S)\\ln 2 = (1 - S)\\ln 2.\n$$\nTherefore the bias is\n$$\n\\text{Bias}(\\hat{H}_{2}) = E[\\hat{H}_{2}] - H_{2} = (1 - S)\\ln 2 + \\ln S.\n$$\nDefine $b(S) \\equiv (1 - S)\\ln 2 + \\ln S$. Then\n$$\n\\frac{d}{dS} b(S) = -\\ln 2 + \\frac{1}{S} > 0 \\quad \\text{for } S \\in (0,1],\n$$\nand $b(1) = 0$. Hence $b(S)  0$ for all $S \\in (0,1)$. This shows the bias is strictly negative for all non-deterministic distributions when $N=2$.\n\n2) First-order expansion for general $N$: Let $g(x) = -\\ln x$, so $g'(x) = -\\frac{1}{x}$ and $g''(x) = \\frac{1}{x^{2}}$. Write $S_{2} \\equiv \\sum_{i} p(x_{i})^{2} = S$, $S_{3} \\equiv \\sum_{i} p(x_{i})^{3}$, and expand $E[g(\\hat{S})]$ about $S$:\n$$\nE[g(\\hat{S})] \\approx g(S) + g'(S)\\big(E[\\hat{S}] - S\\big) + \\frac{1}{2} g''(S)\\operatorname{Var}(\\hat{S}).\n$$\nWe already have $E[\\hat{S}] - S = \\frac{1 - S}{N}$. A standard multinomial calculation yields the leading term of the variance,\n$$\n\\operatorname{Var}(\\hat{S}) = \\frac{4}{N}\\big(S_{3} - S^{2}\\big) + O\\!\\left(\\frac{1}{N^{2}}\\right),\n$$\nand by the Cauchy-Schwarz inequality, $S_{3} \\ge S^{2}$, with equality only for a uniform distribution on its support. Thus,\n$$\n\\text{Bias}(\\hat{H}_{2}) \\approx g'(S)\\frac{1 - S}{N} + \\frac{1}{2} g''(S)\\frac{4}{N}\\big(S_{3} - S^{2}\\big)\n= \\frac{1}{N}\\left( -\\frac{1 - S}{S} + \\frac{2\\,(S_{3} - S^{2})}{S^{2}} \\right).\n$$\nThe bracket simplifies to\n$$\n\\frac{2 S_{3} - S^{2} - S}{S^{2}}.\n$$\nWe claim $2 S_{3} - S^{2} - S \\le 0$ for all probability vectors with strict inequality unless the distribution is deterministic. Indeed, for each $x \\in [0,1]$, $x^{2}(x - 1)^{2} \\ge 0$, which expands to $x^4 - 2x^3 + x^2 \\ge 0$, or $2x^3 \\le x^4 + x^2$. Summing over all probabilities $p(x_i)$ gives $2S_3 \\le \\sum_i p(x_i)^4 + S$. Since $\\sum_i p(x_i)^4 \\le (\\sum_i p(x_i)^2)^2 = S^2$, it follows that $2S_3 \\le S^2 + S$, which rearranges to $2S_3 - S^2 - S \\le 0$. Therefore the leading term of the bias is strictly negative for any non-deterministic distribution, and higher-order terms in $1/N$ do not alter the sign for finite $N$.\n\nCombining the exact $N=2$ case (strictly negative bias for all non-deterministic distributions) with the general first-order analysis (strictly negative leading term for any $N \\ge 2$ and any non-deterministic distribution), we conclude that for any finite $N \\ge 2$ and any non-deterministic true distribution, the plug-in estimator $\\hat{H}_{2}(X)$ is negatively biased.\n\nThus the correct choice is that the bias is always negative.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}