{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering a new concept is to apply its definition directly. This exercise provides a straightforward scenario to calculate collision entropy from a given probability distribution, grounding the abstract formula in a concrete example from biophysics . By working through this foundational problem, you will solidify your understanding of how to translate a set of probabilities into a single measure of uncertainty.",
            "id": "1611469",
            "problem": "A biophysics research group develops a simplified statistical model for the behavior of a single ion channel in a cell membrane. At any given moment, the channel is assumed to be in one of three distinct conformational states: 'Open' (allowing ions to pass), 'Closed' (blocking ion flow), or 'Inactivated' (a different type of non-conducting state). Based on extensive experimental data, the probability of finding the channel in the 'Open' state is $0.6$, in the 'Closed' state is $0.3$, and in the 'Inactivated' state is $0.1$.\n\nFor a discrete random variable $X$ with outcomes $\\{x_1, x_2, \\dots, x_n\\}$ and a probability mass function $P(X=x_i) = p_i$, a quantity known as the collision probability, $P_{coll}(X)$, is defined as the probability that two independent samples drawn from this distribution are identical. Mathematically, it is given by $P_{coll}(X) = \\sum_{i=1}^{n} p_i^2$. The Rényi entropy of order 2, often denoted $H_2(X)$, is then defined as $H_2(X) = -\\ln(P_{coll}(X))$, where $\\ln$ is the natural logarithm.\n\nCalculate the Rényi entropy of order 2, $H_2(X)$, for the described ion channel model. Provide your final answer as a numerical value rounded to three significant figures.",
            "solution": "We are given a discrete random variable with three outcomes corresponding to the ion channel states, with probabilities $p_{\\text{Open}}=0.6$, $p_{\\text{Closed}}=0.3$, and $p_{\\text{Inactivated}}=0.1$. The collision probability is defined as the probability that two independent samples match, which is the sum of squared probabilities:\n$$\nP_{\\text{coll}}(X)=\\sum_{i} p_{i}^{2}=(0.6)^{2}+(0.3)^{2}+(0.1)^{2}.\n$$\nCompute each term:\n$$\n(0.6)^{2}=0.36,\\quad (0.3)^{2}=0.09,\\quad (0.1)^{2}=0.01,\n$$\nhence\n$$\nP_{\\text{coll}}(X)=0.36+0.09+0.01=0.46.\n$$\nThe Rényi entropy of order $2$ is defined as\n$$\nH_{2}(X)=-\\ln\\!\\big(P_{\\text{coll}}(X)\\big)=-\\ln(0.46).\n$$\nEvaluating the natural logarithm,\n$$\n-\\ln(0.46)\\approx 0.776528789.\n$$\nRounded to three significant figures, this yields\n$$\nH_{2}(X)\\approx 0.777.\n$$",
            "answer": "$$\\boxed{0.777}$$"
        },
        {
            "introduction": "In many real-world systems, we don't observe raw data but rather a function of it. This practice explores how such transformations affect a system's randomness, requiring you to first determine the new probability distribution before calculating its entropy . This two-step process is a common and essential skill in data analysis and information theory.",
            "id": "1611463",
            "problem": "Let $X$ be a discrete random variable representing the outcome of a single roll of a fair 8-sided die. The possible outcomes for $X$ are the integers from 1 to 8, inclusive, with each outcome being equally likely.\n\nA new random variable $Y$ is derived from the outcome of $X$ using the operation $Y = X \\pmod 3$, which gives the remainder when $X$ is divided by 3.\n\nThe collision entropy (also known as the Rényi entropy of order 2) for a discrete random variable $Z$ with a set of possible outcomes $\\{z_i\\}$ and corresponding probabilities $P(Z=z_i)$ is given by the formula:\n$$H_2(Z) = -\\ln \\left( \\sum_{i} [P(Z=z_i)]^2 \\right)$$\n\nCalculate the collision entropy of the random variable $Y$. Express your answer as a closed-form analytic expression in units of nats.",
            "solution": "We begin with the uniform distribution of $X$ over $\\{1,2,\\ldots,8\\}$, so $P(X=x)=\\frac{1}{8}$ for each $x$. The derived variable is $Y=X \\bmod 3$, whose support is $\\{0,1,2\\}$.\n\nCount the occurrences for each remainder:\n- $Y=0$ occurs when $X \\in \\{3,6\\}$, so $P(Y=0)=\\frac{2}{8}=\\frac{1}{4}$.\n- $Y=1$ occurs when $X \\in \\{1,4,7\\}$, so $P(Y=1)=\\frac{3}{8}$.\n- $Y=2$ occurs when $X \\in \\{2,5,8\\}$, so $P(Y=2)=\\frac{3}{8}$.\n\nBy the definition of collision entropy,\n$$\nH_{2}(Y)=-\\ln\\!\\left(\\sum_{y \\in \\{0,1,2\\}} \\left[P(Y=y)\\right]^{2}\\right).\n$$\nCompute the sum of squared probabilities:\n$$\n\\sum_{y} \\left[P(Y=y)\\right]^{2}=\\left(\\frac{1}{4}\\right)^{2}+\\left(\\frac{3}{8}\\right)^{2}+\\left(\\frac{3}{8}\\right)^{2}\n=\\frac{1}{16}+2\\cdot\\frac{9}{64}=\\frac{1}{16}+\\frac{18}{64}=\\frac{4}{64}+\\frac{18}{64}=\\frac{22}{64}=\\frac{11}{32}.\n$$\nTherefore,\n$$\nH_{2}(Y)=-\\ln\\!\\left(\\frac{11}{32}\\right)=\\ln\\!\\left(\\frac{32}{11}\\right).\n$$",
            "answer": "$$\\boxed{\\ln\\left(\\frac{32}{11}\\right)}$$"
        },
        {
            "introduction": "Having practiced calculation, we now turn to a more conceptual question about the fundamental properties of entropy. This problem asks you to demonstrate that if one variable is a deterministic function of another, no new uncertainty is introduced into the joint system . Mastering this proof will provide a deeper insight into the meaning of joint entropy and the nature of information itself.",
            "id": "1611486",
            "problem": "Consider two discrete random variables, $X$ and $Y$, with finite alphabets $\\mathcal{X}$ and $\\mathcal{Y}$ respectively. The random variable $X$ has a probability mass function $P(X=x)$ for $x \\in \\mathcal{X}$. The random variable $Y$ is a deterministic function of $X$, meaning that for each $x \\in \\mathcal{X}$, there is a unique $y \\in \\mathcal{Y}$ such that if $X=x$, then $Y=y$. We can write this relationship as $Y=f(X)$.\n\nThe collision entropy (or Rényi entropy of order 2) for a discrete random variable $Z$ with probability mass function $P(Z=z)$ is defined as:\n$$H_2(Z) = -\\ln \\left( \\sum_{z} [P(Z=z)]^2 \\right)$$\nSimilarly, the joint collision entropy of $X$ and $Y$ is defined using their joint probability mass function $P(X=x, Y=y)$:\n$$H_2(X,Y) = -\\ln \\left( \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} [P(X=x, Y=y)]^2 \\right)$$\n\nGiven this setup, express the joint collision entropy $H_2(X,Y)$ in terms of the collision entropy of $X$, denoted as $H_2(X)$. Your final answer should be a simple expression involving only $H_2(X)$.",
            "solution": "We are given discrete random variables $X$ and $Y$ with finite alphabets $\\mathcal{X}$ and $\\mathcal{Y}$, and $Y$ is a deterministic function of $X$, written as $Y=f(X)$. By the definition of a deterministic mapping, for each $x \\in \\mathcal{X}$, we have\n$$\nP(Y=f(x)\\mid X=x)=1 \\quad \\text{and} \\quad P(Y=y\\mid X=x)=0 \\text{ for } y \\neq f(x).\n$$\nUsing the definition of joint probabilities via conditioning, this implies\n$$\nP(X=x, Y=y) = P(X=x) P(Y=y \\mid X=x) = \n\\begin{cases}\nP(X=x), & \\text{if } y=f(x),\\\\\n0, & \\text{otherwise.}\n\\end{cases}\n$$\nThe joint collision entropy is defined as\n$$\nH_{2}(X,Y) = -\\ln \\left( \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} \\left[P(X=x, Y=y)\\right]^{2} \\right).\n$$\nSubstituting the structure of $P(X=x, Y=y)$ from the deterministic relation,\n$$\n\\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} \\left[P(X=x, Y=y)\\right]^{2}\n= \\sum_{x \\in \\mathcal{X}} \\left[P(X=x, Y=f(x))\\right]^{2} + \\sum_{x \\in \\mathcal{X}} \\sum_{y \\neq f(x)} 0\n= \\sum_{x \\in \\mathcal{X}} \\left[P(X=x)\\right]^{2}.\n$$\nTherefore,\n$$\nH_{2}(X,Y) = -\\ln \\left( \\sum_{x \\in \\mathcal{X}} \\left[P(X=x)\\right]^{2} \\right).\n$$\nBut the right-hand side is exactly the collision entropy of $X$, by definition:\n$$\nH_{2}(X) = -\\ln \\left( \\sum_{x \\in \\mathcal{X}} \\left[P(X=x)\\right]^{2} \\right).\n$$\nHence,\n$$\nH_{2}(X,Y) = H_{2}(X).\n$$",
            "answer": "$$\\boxed{H_{2}(X)}$$"
        }
    ]
}