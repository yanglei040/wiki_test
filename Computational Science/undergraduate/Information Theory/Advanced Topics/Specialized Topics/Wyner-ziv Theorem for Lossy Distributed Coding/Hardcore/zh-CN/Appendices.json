{
    "hands_on_practices": [
        {
            "introduction": "在任何分布式信源编码问题中，理解信源 $X$ 与边信息 $Y$ 之间的统计相关性是首要任务。本练习旨在巩固一项基本技能：根据给定的联合概率分布计算条件概率 。这是量化边信息 $Y$ 中包含多少关于 $X$ 的信息，并最终确定所需压缩码率的关键第一步。",
            "id": "1668845",
            "problem": "在一个分布式传感器网络中，两个同址二进制传感器 $X$ 和 $Y$ 用于监测同一环境状态。传感器输出是相关的，它们的联合行为由一个概率质量函数 (PMF) $p(x, y)$ 描述，其中 $x, y \\in \\{0, 1\\}$。已知的联合 PMF 如下：\n$p(X=0, Y=0) = \\frac{3}{8}$\n$p(X=0, Y=1) = \\frac{1}{8}$\n$p(X=1, Y=0) = \\frac{1}{8}$\n$p(X=1, Y=1) = \\frac{3}{8}$\n\n在设计 Wyner-Ziv 压缩方案的背景下，传感器读数 $X$ 的编码需要在不知道 $Y$ 的情况下进行，而解码器可以访问 $Y$ 作为边信息。确定可达压缩率的一个基本先决条件是刻画 $X$ 对 $Y$ 的统计依赖性。\n\n确定构成给定 $Y$ 时 $X$ 的条件概率分布的四个值。请将您的答案表示为一个单行矩阵，其中包含四个分数，顺序为 $[p(X=0|Y=0), p(X=1|Y=0), p(X=0|Y=1), p(X=1|Y=1)]$。",
            "solution": "我们已知联合 PMF 的值：\n$$\np(X=0, Y=0) = \\frac{3}{8}, \\quad p(X=0, Y=1) = \\frac{1}{8}, \\quad p(X=1, Y=0) = \\frac{1}{8}, \\quad p(X=1, Y=1) = \\frac{3}{8}.\n$$\n为了计算给定 $Y$ 时 $X$ 的条件概率，我们使用条件概率的定义：\n$$\np(X=x | Y=y) = \\frac{p(X=x, Y=y)}{p_{Y}(y)},\n$$\n其中 $Y$ 的边缘分布通过对 $x$ 求和得到：\n$$\np_{Y}(y) = \\sum_{x \\in \\{0,1\\}} p(X=x, Y=y).\n$$\n首先，计算 $Y$ 的边缘分布：\n$$\np_{Y}(0) = p(0,0) + p(1,0) = \\frac{3}{8} + \\frac{1}{8} = \\frac{4}{8} = \\frac{1}{2},\n$$\n$$\np_{Y}(1) = p(0,1) + p(1,1) = \\frac{1}{8} + \\frac{3}{8} = \\frac{4}{8} = \\frac{1}{2}.\n$$\n现在计算条件概率：\n对于 $Y=0$：\n$$\np(X=0 | Y=0) = \\frac{p(0,0)}{p_{Y}(0)} = \\frac{\\frac{3}{8}}{\\frac{1}{2}} = \\frac{3}{4},\n$$\n$$\np(X=1 | Y=0) = \\frac{p(1,0)}{p_{Y}(0)} = \\frac{\\frac{1}{8}}{\\frac{1}{2}} = \\frac{1}{4}.\n$$\n对于 $Y=1$：\n$$\np(X=0 | Y=1) = \\frac{p(0,1)}{p_{Y}(1)} = \\frac{\\frac{1}{8}}{\\frac{1}{2}} = \\frac{1}{4},\n$$\n$$\np(X=1 | Y=1) = \\frac{p(1,1)}{p_{Y}(1)} = \\frac{\\frac{3}{8}}{\\frac{1}{2}} = \\frac{3}{4}.\n$$\n因此，按顺序 $[p(X=0 | Y=0), p(X=1 | Y=0), p(X=0 | Y=1), p(X=1 | Y=1)]$ 排列所求的行矩阵为\n$$\n\\begin{pmatrix}\n\\frac{3}{4}  \\frac{1}{4}  \\frac{1}{4}  \\frac{3}{4}\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3}{4}  \\frac{1}{4}  \\frac{1}{4}  \\frac{3}{4}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Wyner-Ziv 理论的一个重要特例是无损压缩情景，此时允许的失真为零。该情景由 Slepian-Wolf 定理描述，它揭示了在解码端可获得边信息的情况下，无失真地重构信源所需的最小码率 。通过本练习，我们将计算条件熵 $H(X|Y)$，从而具体理解在分布式系统中实现完美重构的理论数据率下限。",
            "id": "1668812",
            "problem": "在一个远程环境监测系统中，一个主传感器测量一个系统的状态，该状态用随机变量 $X$ 表示，它可以取三个值之一：$X \\in \\{0, 1, 2\\}$。观测到这些状态以相等的概率出现。一个次级的、可靠性较低的传感器提供相关的旁路信息，用随机变量 $Y$ 表示，它也取值于 $\\{0, 1, 2\\}$。来自次级传感器的观测数据在中央数据中心可用。\n\n两个传感器之间的相关性由以下条件概率来表征：当主传感器测量到状态 $x$ 时，次级传感器也测量到 $x$ 的概率是 $P(Y=x | X=x) = 0.8$。当主传感器测量到状态 $x$ 时，次级传感器测量到任何其他特定状态 $y \\neq x$ 的概率对于所有其他状态都是相同的，即对于 $y \\neq x$ 有 $P(Y=y | X=x) = 0.1$。\n\n主传感器必须对其测量序列进行编码，并将其传输到中央数据中心。次级传感器的数据对主传感器的编码器是不可用的。根据分布式信源编码的原理（特别是Slepian-Wolf定理，它是无损压缩的Wyner-Ziv理论的一个特例），为了让中央数据中心能够无损地完美重构状态序列 $X$，主传感器的传输所需的理论最小平均数据速率是多少？\n\n以比特/测量为单位表示您的答案，并四舍五入到四位有效数字。",
            "solution": "我们被要求求解在解码器端有相关旁路信息 $Y$ 可用时，无损重构 $X$ 所需的理论最小平均数据速率。根据Slepian-Wolf定理，仅观测 $X$ 的编码器可达到的最小速率是条件熵\n$$\nR_{\\min}=H(X|Y).\n$$\n\n给定 $X \\in \\{0,1,2\\}$，对所有 $x$ 有 $P(X=x)=\\frac{1}{3}$，以及一个由下式指定的对称信道 $P(Y=y|X=x)$\n$$\nP(Y=x|X=x)=0.8,\\quad P(Y=y|X=x)=0.1\\ \\text{for }y\\neq x,\n$$\n我们首先计算 $Y$ 的边缘概率：\n$$\nP(Y=y)=\\sum_{x}P(Y=y|X=x)P(X=x)=\\frac{1}{3}\\left(0.8+0.1+0.1\\right)=\\frac{1}{3},\n$$\n因此 $Y$ 也是均匀分布的。\n\n使用贝叶斯法则和对称性，对于任何 $y$ 我们有\n$$\nP(X=y|Y=y)=\\frac{P(Y=y|X=y)P(X=y)}{P(Y=y)}=\\frac{0.8\\cdot \\frac{1}{3}}{\\frac{1}{3}}=0.8,\n$$\n且对于 $x\\neq y$,\n$$\nP(X=x|Y=y)=\\frac{P(Y=y|X=x)P(X=x)}{P(Y=y)}=\\frac{0.1\\cdot \\frac{1}{3}}{\\frac{1}{3}}=0.1.\n$$\n因此，后验分布 $P(X|Y=y)$ 对每个 $y$ 都是相同的，等于 $\\{0.8,0.1,0.1\\}$ 的一种排列。因此\n$$\nH(X|Y)=\\sum_{y}P(Y=y)\\,H\\big(P(X|Y=y)\\big)=H\\big(0.8,0.1,0.1\\big).\n$$\n以比特为单位计算熵，\n$$\nH(X|Y)=-\\left[0.8\\log_{2}(0.8)+0.1\\log_{2}(0.1)+0.1\\log_{2}(0.1)\\right]\n=-0.8\\log_{2}(0.8)-0.2\\log_{2}(0.1).\n$$\n使用 $\\log_{2}(0.8)\\approx -0.3219280949$ 和 $\\log_{2}(0.1)\\approx -3.3219280949$，我们得到\n$$\nH(X|Y)\\approx -[0.8\\times (-0.3219280949)+0.2\\times (-3.3219280949)]\\approx 0.2575424759 + 0.664385619 \\approx 0.9219280949\\ \\text{bits/measurement}.\n$$\n四舍五入到四位有效数字得到 $0.9219$ 比特/测量。\n\n因此，所需的理论最小平均数据速率为 $H(X|Y)\\approx 0.9219$ 比特/测量。",
            "answer": "$$\\boxed{0.9219}$$"
        },
        {
            "introduction": "在探索了无损压缩的极限后，我们转向率失真曲线的另一个极端：当编码码率为零时会发生什么？本练习探讨了这种零码率情景，此时解码器必须完全依赖边信息 $Y$ 来估计 $X$ 。通过将 Wyner-Ziv 理论与经典的贝叶斯估计联系起来，我们计算出最小可达失真 $D_{\\min}$，它为我们提供了一个衡量边信息价值的基准性能。",
            "id": "1668798",
            "problem": "一个二进制数据源产生一个随机变量 $X \\in \\{0, 1\\}$，其概率分布已知为 $P(X=1) = p_X$。该信源需要被编码以进行传输，但解码器可以访问到相关的边信息 $Y$。该边信息由信源的带噪版本生成，具体来说，是通过将 $X$ 传入一个交叉概率为 $\\epsilon$ 的二元对称信道 (BSC) 产生的。解码器不从编码器接收任何信息（零速率编码场景），并且必须仅基于观测到的边信息 $Y$ 来生成对信源符号 $X$ 的估计 $\\hat{X}$。\n\n估计器的性能由平均错误概率来衡量，这对应于平均汉明失真 $E[d(X, \\hat{X})]$，其中当 $x \\ne \\hat{x}$ 时 $d(x, \\hat{x})=1$，当 $x = \\hat{x}$ 时 $d(x, \\hat{x})=0$。一个最优解码器将实现一个估计规则 $\\hat{x}(y)$，以最小化此平均错误概率。这个最小可实现错误被记为 $D_{\\min}$。\n\n给定信源概率 $p_X = 0.800$ 和信道的交叉概率 $\\epsilon = 0.300$，计算 $D_{\\min}$ 的值。将你的答案表示为保留三位有效数字的小数。",
            "solution": "问题要求在仅使用相关边信息 $Y$ 来估计二进制信源 $X$ 时的最小平均错误概率 $D_{\\min}$。这个场景对应于贝叶斯估计，其目标是找到一个能够最小化平均失真的估计器 $\\hat{x}(y)$。对于汉明失真，平均失真就是错误概率，即 $D = P(X \\ne \\hat{X})$。\n\n一个能最小化错误概率的最优估计器是最大后验 (MAP) 估计器。该估计器选择在给定观测值 $y$ 的情况下具有最高后验概率的符号 $x$：\n$$ \\hat{x}(y) = \\arg\\max_{x \\in \\{0,1\\}} P(X=x|Y=y) $$\n总的最小错误概率 $D_{\\min}$ 是对于每个可能的观测值 $y$ 的错误概率之和，并按该观测值发生的概率加权：\n$$ D_{\\min} = P(Y=0)P(\\text{error}|Y=0) + P(Y=1)P(\\text{error}|Y=1) $$\n在给定观测值 $y$ 的情况下，发生错误的概率是 MAP 规则*未*选择的那个符号的后验概率。也就是两个后验概率中较小的那一个。\n$$ P(\\text{error}|Y=y) = \\min_{x \\in \\{0,1\\}} P(X=x|Y=y) $$\n将此代入 $D_{\\min}$ 的表达式中：\n$$ D_{\\min} = P(Y=0) \\left( \\min_{x \\in \\{0,1\\}} P(X=x|Y=0) \\right) + P(Y=1) \\left( \\min_{x \\in \\{0,1\\}} P(X=x|Y=1) \\right) $$\n使用恒等式 $P(A)P(B|A) = P(A,B)$，我们可以通过将边际概率 $P(Y=y)$ 移入最小化算子内部来简化表达式：\n$$ D_{\\min} = \\min_{x \\in \\{0,1\\}} P(X=x, Y=0) + \\min_{x \\in \\{0,1\\}} P(X=x, Y=1) $$\n这个简洁的公式使我们能够通过计算四个联合概率 $P(X=x, Y=y)$ 来计算 $D_{\\min}$。\n\n问题提供了以下参数：\n- 信源概率：$P(X=1) = p_X = 0.800$。这意味着 $P(X=0) = 1 - p_X = 0.200$。\n- 信道交叉概率：$\\epsilon = 0.300$。这是一个比特被翻转的概率，即 $P(Y \\neq x|X=x)$。正确传输的概率是 $1-\\epsilon = 0.700$。\n\n现在，我们使用公式 $P(X=x, Y=y) = P(Y=y|X=x)P(X=x)$ 来计算这四个联合概率：\n1.  $P(X=0, Y=0) = P(Y=0|X=0)P(X=0) = (1-\\epsilon)(1-p_X) = (0.700)(0.200) = 0.140$。\n2.  $P(X=1, Y=0) = P(Y=0|X=1)P(X=1) = \\epsilon \\cdot p_X = (0.300)(0.800) = 0.240$。\n3.  $P(X=0, Y=1) = P(Y=1|X=0)P(X=0) = \\epsilon(1-p_X) = (0.300)(0.200) = 0.060$。\n4.  $P(X=1, Y=1) = P(Y=1|X=1)P(X=1) = (1-\\epsilon)p_X = (0.700)(0.800) = 0.560$。\n\n我们可以将这些值代入我们关于 $D_{\\min}$ 的表达式中：\n$$ D_{\\min} = \\min(P(X=0, Y=0), P(X=1, Y=0)) + \\min(P(X=0, Y=1), P(X=1, Y=1)) $$\n$$ D_{\\min} = \\min(0.140, 0.240) + \\min(0.060, 0.560) $$\n从每对中取最小值：\n$$ D_{\\min} = 0.140 + 0.060 = 0.200 $$\n\n为了获得更深入的理解，让我们检查一下显式的 MAP 决策规则：\n- 如果观测到 $Y=0$：解码器比较 $P(X=0|Y=0)$ 和 $P(X=1|Y=0)$。由于 $P(X=1, Y=0)=0.240 > P(X=0, Y=0)=0.140$，解码器断定发送 $X=1$ 的可能性更大。因此，$\\hat{x}(0) = 1$。\n- 如果观测到 $Y=1$：解码器比较 $P(X=0|Y=1)$ 和 $P(X=1|Y=1)$。由于 $P(X=1, Y=1)=0.560 > P(X=0, Y=1)=0.060$，解码器断定 $\\hat{x}(1) = 1$。\n\n在两种情况下，最优决策都是猜测 $\\hat{X}=1$，而不管观测到的边信息 $Y$ 是什么。发生这种情况是因为信源高度偏向于 $X=1$（一个强的先验），并且信道噪声足够大，以至于观测到 $Y=0$ 所提供的证据不足以克服这个先验信念。因此，最优策略是总是猜测最可能的信源符号。\n该策略的错误概率是猜测错误的概率，即 $P(X \\ne 1) = P(X=0) = 1 - p_X = 1 - 0.800 = 0.200$。这证实了我们计算得出的结果。\n\n最终答案，保留三位有效数字，是 0.200。",
            "answer": "$$\\boxed{0.200}$$"
        }
    ]
}