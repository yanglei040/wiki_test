{
    "hands_on_practices": [
        {
            "introduction": "Before we can leverage the Wyner-Ziv theorem to design an efficient compression scheme, we must first have a precise mathematical description of the statistical relationship between the source, $X$, and the side information, $Y$. The most fundamental way to do this is by determining the conditional probability distribution, $p(x|y)$. This exercise  provides hands-on practice with this essential first step, calculating these key probabilities from a given joint distribution.",
            "id": "1668845",
            "problem": "In a distributed sensor network, two co-located binary sensors, $X$ and $Y$, are used to monitor the same environmental state. The sensor outputs are correlated, and their joint behavior is described by a Probability Mass Function (PMF), $p(x, y)$, where $x, y \\in \\{0, 1\\}$. The known joint PMF is given by:\n$p(X=0, Y=0) = \\frac{3}{8}$\n$p(X=0, Y=1) = \\frac{1}{8}$\n$p(X=1, Y=0) = \\frac{1}{8}$\n$p(X=1, Y=1) = \\frac{3}{8}$\n\nIn the context of designing a Wyner-Ziv compression scheme, the sensor reading $X$ is to be encoded without knowledge of $Y$, while the decoder has access to $Y$ as side information. A fundamental prerequisite for determining the achievable compression rate is to characterize the statistical dependency of $X$ on $Y$.\n\nDetermine the four values that constitute the conditional probability distributions of $X$ given $Y$. Present your answer as a single row matrix of four fractions in the specific order $[p(X=0|Y=0), p(X=1|Y=0), p(X=0|Y=1), p(X=1|Y=1)]$.",
            "solution": "We are given the joint PMF values:\n$$\np(X=0, Y=0) = \\frac{3}{8}, \\quad p(X=0, Y=1) = \\frac{1}{8}, \\quad p(X=1, Y=0) = \\frac{1}{8}, \\quad p(X=1, Y=1) = \\frac{3}{8}.\n$$\nTo compute the conditional probabilities of $X$ given $Y$, we use the definition of conditional probability:\n$$\np(X=x \\mid Y=y) = \\frac{p(X=x, Y=y)}{p_{Y}(y)},\n$$\nwhere the marginal distribution of $Y$ is obtained by summing over $x$:\n$$\np_{Y}(y) = \\sum_{x \\in \\{0,1\\}} p(X=x, Y=y).\n$$\nFirst, compute the marginals of $Y$:\n$$\np_{Y}(0) = p(0,0) + p(1,0) = \\frac{3}{8} + \\frac{1}{8} = \\frac{1}{2},\n$$\n$$\np_{Y}(1) = p(0,1) + p(1,1) = \\frac{1}{8} + \\frac{3}{8} = \\frac{1}{2}.\n$$\nNow compute the conditionals:\nFor $Y=0$:\n$$\np(X=0 \\mid Y=0) = \\frac{p(0,0)}{p_{Y}(0)} = \\frac{\\frac{3}{8}}{\\frac{1}{2}} = \\frac{3}{4},\n$$\n$$\np(X=1 \\mid Y=0) = \\frac{p(1,0)}{p_{Y}(0)} = \\frac{\\frac{1}{8}}{\\frac{1}{2}} = \\frac{1}{4}.\n$$\nFor $Y=1$:\n$$\np(X=0 \\mid Y=1) = \\frac{p(0,1)}{p_{Y}(1)} = \\frac{\\frac{1}{8}}{\\frac{1}{2}} = \\frac{1}{4},\n$$\n$$\np(X=1 \\mid Y=1) = \\frac{p(1,1)}{p_{Y}(1)} = \\frac{\\frac{3}{8}}{\\frac{1}{2}} = \\frac{3}{4}.\n$$\nTherefore, the requested row matrix in the order $[p(X=0 \\mid Y=0), p(X=1 \\mid Y=0), p(X=0 \\mid Y=1), p(X=1 \\mid Y=1)]$ is\n$$\n\\begin{pmatrix}\n\\frac{3}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{3}{4}\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{3}{4}\\end{pmatrix}}$$"
        },
        {
            "introduction": "A crucial question in any compression problem is establishing a performance baseline: what is the best possible outcome if we use no resources at all? In the context of Wyner-Ziv coding, this corresponds to a zero-rate code, where the encoder sends no information and the decoder must rely solely on the side information $Y$. This problem  challenges you to calculate the minimum achievable distortion, $D_{\\min}$, under these conditions, which is found by constructing an optimal Bayesian estimator.",
            "id": "1668798",
            "problem": "A binary data source produces a random variable $X \\in \\{0, 1\\}$ with a known probability distribution $P(X=1) = p_X$. This source is to be encoded for transmission, but the decoder will have access to correlated side information, $Y$. The side information is generated by a noisy version of the source, specifically by passing $X$ through a Binary Symmetric Channel (BSC) with a crossover probability of $\\epsilon$. The decoder receives no information from the encoder (a zero-rate coding scenario) and must produce an estimate $\\hat{X}$ of the source symbol $X$ based solely on the observed side information $Y$.\n\nThe performance of the estimator is measured by the average probability of error, which corresponds to the average Hamming distortion $E[d(X, \\hat{X})]$, where $d(x, \\hat{x})=1$ if $x \\ne \\hat{x}$ and $d(x, \\hat{x})=0$ if $x = \\hat{x}$. An optimal decoder will implement an estimation rule $\\hat{x}(y)$ that minimizes this average error probability. This minimum achievable error is denoted as $D_{\\min}$.\n\nGiven the source probability $p_X = 0.800$ and the channel's crossover probability $\\epsilon = 0.300$, calculate the value of $D_{\\min}$. Express your answer as a decimal rounded to three significant figures.",
            "solution": "The problem asks for the minimum average error probability, $D_{\\min}$, when estimating a binary source $X$ using only the correlated side information $Y$. This scenario corresponds to Bayesian estimation, where the goal is to find an estimator $\\hat{x}(y)$ that minimizes the average distortion. For Hamming distortion, the average distortion is the probability of error, $D = P(X \\ne \\hat{X})$.\n\nAn optimal estimator, which minimizes the probability of error, is the Maximum A Posteriori (MAP) estimator. This estimator chooses the symbol $x$ that has the highest posterior probability given the observation $y$:\n$$ \\hat{x}(y) = \\arg\\max_{x \\in \\{0,1\\}} P(X=x|Y=y) $$\nThe total minimum error probability, $D_{\\min}$, is the sum of the error probabilities for each possible observation $y$, weighted by the probability of that observation occurring:\n$$ D_{\\min} = P(Y=0)P(\\text{error}|Y=0) + P(Y=1)P(\\text{error}|Y=1) $$\nThe probability of making an error, given an observation $y$, is the posterior probability of the symbol that was *not* chosen by the MAP rule. This is the smaller of the two posterior probabilities.\n$$ P(\\text{error}|Y=y) = \\min_{x \\in \\{0,1\\}} P(X=x|Y=y) $$\nSubstituting this into the expression for $D_{\\min}$:\n$$ D_{\\min} = P(Y=0) \\left( \\min_{x \\in \\{0,1\\}} P(X=x|Y=0) \\right) + P(Y=1) \\left( \\min_{x \\in \\{0,1\\}} P(X=x|Y=1) \\right) $$\nUsing the identity $P(A,B) = P(A)P(B|A)$, we can simplify the expression by bringing the marginal probability $P(Y=y)$ inside the minimum operator:\n$$ D_{\\min} = \\min_{x \\in \\{0,1\\}} P(X=x, Y=0) + \\min_{x \\in \\{0,1\\}} P(X=x, Y=1) $$\nThis elegant formula allows us to compute $D_{\\min}$ by calculating the four joint probabilities $P(X=x, Y=y)$.\n\nThe problem provides the following parameters:\n- Source probability: $P(X=1) = p_X = 0.800$. This implies $P(X=0) = 1 - p_X = 0.200$.\n- Channel crossover probability: $\\epsilon = 0.300$. This is the probability that a bit is flipped, i.e., $P(Y \\neq X|X)$. The probability of correct transmission is $1-\\epsilon = 0.700$.\n\nNow, we compute the four joint probabilities using the formula $P(X=x, Y=y) = P(Y=y|X=x)P(X=x)$:\n1.  $P(X=0, Y=0) = P(Y=0|X=0)P(X=0) = (1-\\epsilon)(1-p_X) = (0.700)(0.200) = 0.140$.\n2.  $P(X=1, Y=0) = P(Y=0|X=1)P(X=1) = \\epsilon \\cdot p_X = (0.300)(0.800) = 0.240$.\n3.  $P(X=0, Y=1) = P(Y=1|X=0)P(X=0) = \\epsilon(1-p_X) = (0.300)(0.200) = 0.060$.\n4.  $P(X=1, Y=1) = P(Y=1|X=1)P(X=1) = (1-\\epsilon)p_X = (0.700)(0.800) = 0.560$.\n\nWe can substitute these values into our expression for $D_{\\min}$:\n$$ D_{\\min} = \\min(P(X=0, Y=0), P(X=1, Y=0)) + \\min(P(X=0, Y=1), P(X=1, Y=1)) $$\n$$ D_{\\min} = \\min(0.140, 0.240) + \\min(0.060, 0.560) $$\nTaking the minimum from each pair:\n$$ D_{\\min} = 0.140 + 0.060 = 0.200 $$\n\nTo gain more insight, let's examine the explicit MAP decision rule:\n- If $Y=0$ is observed: The decoder compares $P(X=0, Y=0)=0.140$ with $P(X=1, Y=0)=0.240$. Since $0.240 > 0.140$, the decoder concludes that it is more probable that $X=1$ was sent. Thus, $\\hat{x}(0) = 1$.\n- If $Y=1$ is observed: The decoder compares $P(X=0, Y=1)=0.060$ with $P(X=1, Y=1)=0.560$. Since $0.560 > 0.060$, the decoder concludes $\\hat{x}(1) = 1$.\n\nIn both cases, the optimal decision is to guess $\\hat{X}=1$, regardless of the observed side information $Y$. This happens because the source is highly biased towards $X=1$ (a strong prior), and the channel is sufficiently noisy that the evidence from observing $Y=0$ is not strong enough to overcome this prior belief. The optimal strategy is thus to always guess the most likely source symbol.\nThe error probability of this strategy is the probability that the guess is wrong, which is $P(X \\ne 1) = P(X=0) = 1 - p_X = 1 - 0.800 = 0.200$. This confirms the result from our calculation.\n\nThe final answer, rounded to three significant figures, is 0.200.",
            "answer": "$$\\boxed{0.200}$$"
        },
        {
            "introduction": "The Wyner-Ziv theorem provides a framework for lossy compression, but it gracefully includes the special case of lossless (zero distortion) coding. This scenario is famously described by the Slepian-Wolf theorem, which states that the minimum rate required to perfectly reconstruct $X$ given $Y$ is the conditional entropy $H(X|Y)$. This practice problem  allows you to apply this principle, solidifying the connection between correlation and the information rate needed for perfect recovery.",
            "id": "1668812",
            "problem": "In a remote environmental monitoring system, a primary sensor measures the state of a system, denoted by the random variable $X$, which can take one of three values: $X \\in \\{0, 1, 2\\}$. These states are observed to occur with equal probability. A secondary, less reliable sensor provides correlated side information, denoted by the random variable $Y$, which also takes values in $\\{0, 1, 2\\}$. The observations from the secondary sensor are available at a central data hub.\n\nThe correlation between the two sensors is characterized by the following conditional probabilities: when the primary sensor measures state $x$, the probability that the secondary sensor also measures $x$ is $P(Y=x | X=x) = 0.8$. When the primary sensor measures state $x$, the probability that the secondary sensor measures any other specific state $y \\neq x$ is the same for all other states, i.e., $P(Y=y | X=x) = 0.1$ for $y \\neq x$.\n\nThe primary sensor must encode its sequence of measurements and transmit them to the central hub. The secondary sensor's data is not available to the primary sensor's encoder. According to the principles of distributed source coding (specifically, the Slepian-Wolf theorem, which is a special case of Wyner-Ziv theory for lossless compression), what is the theoretical minimum average data rate required for the primary sensor's transmission to allow the central hub to perfectly reconstruct the sequence of states $X$ without any loss?\n\nExpress your answer in bits per measurement, rounded to four significant figures.",
            "solution": "We are asked for the theoretical minimum average data rate for lossless reconstruction of $X$ at the decoder when correlated side information $Y$ is available only at the decoder. By the Slepian-Wolf theorem, the minimum achievable rate for the encoder observing only $X$ is the conditional entropy\n$$\nR_{\\min}=H(X|Y).\n$$\n\nGiven $X \\in \\{0,1,2\\}$ with $P(X=x)=\\frac{1}{3}$ for all $x$, and a symmetric channel $P(Y=y|X=x)$ specified by\n$$\nP(Y=x|X=x)=0.8,\\quad P(Y=y|X=x)=0.1\\ \\text{for }y\\neq x,\n$$\nwe first compute the marginal of $Y$:\n$$\nP(Y=y)=\\sum_{x}P(Y=y|X=x)P(X=x)=\\frac{1}{3}\\left(0.8+0.1+0.1\\right)=\\frac{1}{3},\n$$\nso $Y$ is also uniform.\n\nUsing Bayesâ€™ rule and the symmetry, for any $y$ we have\n$$\nP(X=y|Y=y)=\\frac{P(Y=y|X=y)P(X=y)}{P(Y=y)}=\\frac{0.8\\cdot \\frac{1}{3}}{\\frac{1}{3}}=0.8,\n$$\nand for $x\\neq y$,\n$$\nP(X=x|Y=y)=\\frac{P(Y=y|X=x)P(X=x)}{P(Y=y)}=\\frac{0.1\\cdot \\frac{1}{3}}{\\frac{1}{3}}=0.1.\n$$\nTherefore, the posterior distribution $P(X|Y=y)$ is the same for every $y$ and equals $\\{0.8,0.1,0.1\\}$. Hence\n$$\nH(X|Y)=\\sum_{y}P(Y=y)\\,H\\big(P(X|Y=y)\\big)=H\\big(0.8,0.1,0.1\\big).\n$$\nEvaluating the entropy in bits,\n$$\nH(X|Y)=-\\left[0.8\\log_{2}(0.8)+0.1\\log_{2}(0.1)+0.1\\log_{2}(0.1)\\right]\n=-0.8\\log_{2}(0.8)-0.2\\log_{2}(0.1).\n$$\nUsing $\\log_{2}(0.8)\\approx -0.3219280949$ and $\\log_{2}(0.1)\\approx -3.3219280949$, we get\n$$\nH(X|Y)\\approx 0.8\\times 0.3219280949+0.2\\times 3.3219280949\\approx 0.9219280949\\ \\text{bits/measurement}.\n$$\nRounding to four significant figures gives $0.9219$ bits per measurement.\n\nTherefore, the theoretical minimum average data rate required is $H(X|Y)\\approx 0.9219$ bits per measurement.",
            "answer": "$$\\boxed{0.9219}$$"
        }
    ]
}