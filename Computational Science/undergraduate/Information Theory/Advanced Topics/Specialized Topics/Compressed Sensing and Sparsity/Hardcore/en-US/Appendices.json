{
    "hands_on_practices": [
        {
            "introduction": "This first exercise takes you to the very heart of the compressed sensing data acquisition process. You will apply the fundamental linear model $y = Ax$ to see how a small number of measurements can be generated from a high-dimensional, yet sparse, signal. This calculation  is the essential first step in understanding the entire compressed sensing pipeline, from signal to measurement.",
            "id": "1612169",
            "problem": "In the field of compressed sensing, a high-dimensional sparse signal can be reconstructed from a small number of linear measurements. The fundamental process of acquiring these measurements is described by the linear model $y = Ax$, where $x \\in \\mathbb{R}^N$ is the original signal, $A$ is an $M \\times N$ matrix known as the sensing matrix (with $M \\ll N$), and $y \\in \\mathbb{R}^M$ is the resulting measurement vector.\n\nConsider a sparse signal $x \\in \\mathbb{R}^{16}$ that is known to be \"3-sparse,\" meaning it has at most three non-zero elements. At a specific time, the non-zero elements of this signal are located at the 2nd, 7th, and 12th positions (using 1-based indexing), with corresponding values of $3$, $-2$, and $5$, respectively. All other components of $x$ are zero.\n\nThe signal $x$ is measured using a $4 \\times 16$ sensing matrix $A$ given by:\n$$\nA = \\begin{pmatrix}\n1  2  0  1  -1  0  3  1  0  1  2  -2  1  0  -1  1 \\\\\n-1  1  1  0  2  1  -1  0  1  2  0  1  0  3  1  0 \\\\\n0  1  -2  1  0  1  1  -1  2  0  1  1  -1  2  0  1 \\\\\n2  -1  1  0  1  -2  0  2  -1  1  0  -1  2  1  1  -1\n\\end{pmatrix}\n$$\nCalculate the measurement vector $y \\in \\mathbb{R}^4$. Express your answer as a $1 \\times 4$ row matrix.",
            "solution": "We are given a linear measurement model $y = Ax$ with $x \\in \\mathbb{R}^{16}$ being $3$-sparse. The nonzero entries are at indices $2$, $7$, and $12$ (1-based), with values $x_{2} = 3$, $x_{7} = -2$, and $x_{12} = 5$, and all other entries are zero. Therefore,\n$$\nx = \\begin{pmatrix}\n0 \\\\ 3 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ -2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 5 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\nBy linearity of matrix-vector multiplication, $y = Ax$ can be expressed as a linear combination of the columns of $A$. Denote the $j$-th column of $A$ by $a_{j}$. Then\n$$\ny = \\sum_{j=1}^{16} x_{j} a_{j} = 3 a_{2} - 2 a_{7} + 5 a_{12}.\n$$\nFrom the given matrix $A$, we read the relevant columns:\n$$\na_{2} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ -1 \\end{pmatrix}, \\quad\na_{7} = \\begin{pmatrix} 3 \\\\ -1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad\na_{12} = \\begin{pmatrix} -2 \\\\ 1 \\\\ 1 \\\\ -1 \\end{pmatrix}.\n$$\nCompute each scaled column:\n$$\n3 a_{2} = \\begin{pmatrix} 6 \\\\ 3 \\\\ 3 \\\\ -3 \\end{pmatrix}, \\quad\n-2 a_{7} = \\begin{pmatrix} -6 \\\\ 2 \\\\ -2 \\\\ 0 \\end{pmatrix}, \\quad\n5 a_{12} = \\begin{pmatrix} -10 \\\\ 5 \\\\ 5 \\\\ -5 \\end{pmatrix}.\n$$\nAdd these vectors componentwise to obtain $y$:\n$$\ny = \\begin{pmatrix} 6 \\\\ 3 \\\\ 3 \\\\ -3 \\end{pmatrix}\n+ \\begin{pmatrix} -6 \\\\ 2 \\\\ -2 \\\\ 0 \\end{pmatrix}\n+ \\begin{pmatrix} -10 \\\\ 5 \\\\ 5 \\\\ -5 \\end{pmatrix}\n= \\begin{pmatrix} -10 \\\\ 10 \\\\ 6 \\\\ -8 \\end{pmatrix}.\n$$\nExpressed as a $1 \\times 4$ row matrix, the measurement vector is\n$$\n\\begin{pmatrix} -10  10  6  -8 \\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}-10  10  6  -8\\end{pmatrix}}$$"
        },
        {
            "introduction": "Having seen how measurements are generated, we now turn to the inverse problem: signal recovery. This exercise introduces the core principle of compressed sensing, which involves solving an underdetermined system of equations by finding the solution with the minimum $L_1$-norm. You will see firsthand how this optimization promotes sparsity, allowing us to pinpoint a specific, meaningful signal from an infinite number of possibilities .",
            "id": "1612160",
            "problem": "In the field of signal processing, the principle of compressed sensing allows for the reconstruction of a signal from a limited number of measurements, under the assumption that the signal is sparse. A signal is considered sparse if a large number of its components are zero. This reconstruction often involves solving an underdetermined system of linear equations by finding the solution with the minimum $L_1$-norm, a process that promotes sparsity.\n\nConsider a signal represented by a vector $x = (x_1, x_2, x_3)$ in a three-dimensional space $\\mathbb{R}^3$. A measurement process has subjected the signal to the following two linear constraints:\n$$x_1 + x_2 + 2x_3 = 1$$\n$$x_1 - x_2 = \\frac{1}{2}$$\nThe $L_1$-norm of the vector $x$, denoted as $\\|x\\|_1$, is defined by the sum of the absolute values of its components:\n$$\\|x\\|_1 = |x_1| + |x_2| + |x_3|$$\nDetermine the signal vector $(x_1, x_2, x_3)$ that satisfies both measurement constraints and has the minimum possible $L_1$-norm. Express your answer as a row vector of three components.",
            "solution": "We are given the linear constraints\n$$x_{1}+x_{2}+2x_{3}=1,\\qquad x_{1}-x_{2}=\\frac{1}{2}.$$\nSolve these equations to parameterize the solution set. From $x_{1}-x_{2}=\\frac{1}{2}$, we have $x_{1}=x_{2}+\\frac{1}{2}$. Substitute into $x_{1}+x_{2}+2x_{3}=1$ to get\n$$(x_{2}+\\frac{1}{2})+x_{2}+2x_{3}=1 \\;\\;\\Rightarrow\\;\\; 2x_{2}+2x_{3}=\\frac{1}{2} \\;\\;\\Rightarrow\\;\\; x_{2}+x_{3}=\\frac{1}{4}.$$\nHence $x_{2}=\\frac{1}{4}-x_{3}$ and\n$$x_{1}=x_{2}+\\frac{1}{2}=\\frac{1}{4}-x_{3}+\\frac{1}{2}=\\frac{3}{4}-x_{3}.$$\nLet $t=x_{3}$. Then all feasible solutions are\n$$(x_{1},x_{2},x_{3})=\\left(\\frac{3}{4}-t,\\;\\frac{1}{4}-t,\\;t\\right).$$\nThe objective to minimize is the $L_1$-norm\n$$\\|x\\|_{1}=|x_{1}|+|x_{2}|+|x_{3}|=\\left|\\frac{3}{4}-t\\right|+\\left|\\frac{1}{4}-t\\right|+|t|.$$\nObserve that\n$$\\left|\\frac{3}{4}-t\\right|+\\left|\\frac{1}{4}-t\\right|+|t|=\\big|t-\\frac{3}{4}\\big|+\\big|t-\\frac{1}{4}\\big|+|t-0|,$$\nwhich is a sum of absolute deviations from the points $\\{0,\\frac{1}{4},\\frac{3}{4}\\}$. A standard property of the function $g(t)=\\sum_{i}|t-a_{i}|$ is that it is convex and minimized at any median of the set $\\{a_{i}\\}$. With three points, the median is unique and equals $\\frac{1}{4}$. Therefore the minimizing parameter is\n$$t^{\\ast}=\\frac{1}{4}.$$\nSubstituting $t^{\\ast}$ back gives\n$$x_{1}=\\frac{3}{4}-\\frac{1}{4}=\\frac{1}{2},\\qquad x_{2}=\\frac{1}{4}-\\frac{1}{4}=0,\\qquad x_{3}=\\frac{1}{4}.$$\nThis vector satisfies both constraints and has minimum $L_1$-norm. For completeness, the minimum norm value is $|\\,\\frac{1}{2}\\,|+|0|+|\\,\\frac{1}{4}\\,|=\\frac{3}{4}$, confirming optimality.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2}  0  \\frac{1}{4}\\end{pmatrix}}$$"
        },
        {
            "introduction": "While $L_1$-minimization is powerful, it is not foolproof. This final practice delves into the theoretical limits of compressed sensing by asking you to construct a scenario where recovery fails. By demonstrating a violation of the conditions needed for successful recovery, such as the Null Space Property, you will gain a more nuanced understanding of why properties of the sensing matrix $A$ are so crucial for guaranteeing that the sparsest solution is found .",
            "id": "1612125",
            "problem": "In the field of signal processing, Compressed Sensing (CS) provides a framework for recovering a sparse signal from a small number of linear measurements. Consider a scenario where a signal $z \\in \\mathbb{R}^3$ is measured via the linear transformation $y = Az$, where the measurement matrix $A$ is given by\n$$A = \\begin{pmatrix} 1  0  0.5 \\\\ 0  1  0.5 \\end{pmatrix}.$$\nThe standard CS recovery algorithm attempts to find the original signal by solving the $L_1$-norm minimization problem:\n$$ \\min_{z' \\in \\mathbb{R}^3} \\|z'\\|_1 \\quad \\text{subject to} \\quad Az' = y $$\nwhere $\\|z'\\|_1 = |z'_1| + |z'_2| + |z'_3|$. Successful recovery means that the original sparse signal is the unique solution to this problem.\n\nYour task is to construct a specific counterexample demonstrating the failure of this recovery method for the given matrix $A$. To do this, you must find two distinct vectors, $x$ and $\\hat{x}$, that satisfy the following conditions:\n1. The original signal $x$ is non-zero and 2-sparse (it has exactly two non-zero entries).\n2. The non-zero entries of $x$ must be the smallest possible positive integers that allow for a failure scenario.\n3. The alternative signal $\\hat{x}$ produces the same measurements as $x$, i.e., $A\\hat{x} = Ax$.\n4. The $L_1$-norm of the alternative signal is no larger than that of the original signal: $\\|\\hat{x}\\|_1 \\le \\|x\\|_1$.\n5. Among all possible vectors $\\hat{x}$ that satisfy the above conditions for your chosen $x$, you must select the one that is the sparsest (i.e., has the maximum number of zero entries).\n\nDetermine this specific sparsest alternative signal, $\\hat{x}$. Provide your answer as a column vector.",
            "solution": "The problem asks us to find a pair of signals, a 2-sparse signal $x$ and another signal $\\hat{x}$, which demonstrates the failure of $L_1$-minimization for recovery with the given matrix $A$. A failure occurs if we can find a signal $\\hat{x} \\ne x$ such that $A\\hat{x} = Ax$ and $\\|\\hat{x}\\|_1 \\le \\|x\\|_1$.\n\nFirst, let's analyze the condition $A\\hat{x} = Ax$. This is equivalent to $A(\\hat{x} - x) = 0$. This means the difference vector, $\\hat{x} - x$, must lie in the null space of $A$, denoted $N(A)$. Let's find the null space of $A$. A vector $h = [h_1, h_2, h_3]^T$ is in $N(A)$ if $Ah=0$.\n$$ \\begin{pmatrix} 1  0  0.5 \\\\ 0  1  0.5 \\end{pmatrix} \\begin{pmatrix} h_1 \\\\ h_2 \\\\ h_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\nThis gives the system of equations:\n$h_1 + 0.5 h_3 = 0 \\implies h_1 = -0.5 h_3$\n$h_2 + 0.5 h_3 = 0 \\implies h_2 = -0.5 h_3$\nThe null space consists of all vectors of the form $[-0.5t, -0.5t, t]^T$ for any scalar $t$. To work with integers, let's choose a basis vector for the null space by setting $t=-2$, giving $h = [1, 1, -2]^T$. Any alternative solution $\\hat{x}$ must be of the form $\\hat{x} = x - \\alpha h$ for some non-zero scalar $\\alpha$.\n\nThe failure of $L_1$-recovery is linked to the Null Space Property (NSP). The NSP of order $k$ states that for any non-zero $h \\in N(A)$ and any index set $S$ with $|S| \\le k$, we must have $\\|h_S\\|_1  \\|h_{S^c}\\|_1$. If this property is violated, a $k$-sparse signal might not be recoverable. Here, we are interested in 2-sparse signals, so $k=2$. Let's check the NSP for our null space vector $h = [1, 1, -2]^T$. We test it for sets $S$ with size $|S|=2$.\nLet $S = \\{1, 2\\}$. Then $h_S = [1, 1, 0]^T$ and $h_{S^c} = [0, 0, -2]^T$.\n$\\|h_S\\|_1 = |1| + |1| = 2$.\n$\\|h_{S^c}\\|_1 = |-2| = 2$.\nHere, $\\|h_S\\|_1 = \\|h_{S^c}\\|_1$, which violates the strict inequality required by the NSP. This suggests that we should construct our 2-sparse signal $x$ to have its support on $S=\\{1, 2\\}$.\n\nThe problem states that $x$ is 2-sparse with support on $S=\\{1, 2\\}$, so $x = [x_1, x_2, 0]^T$. It also states that its non-zero components must be the smallest possible positive integers. This leads us to choose $x_1=1$ and $x_2=1$.\nSo, our specific 2-sparse signal is $x = [1, 1, 0]^T$.\nThe $L_1$-norm of this signal is $\\|x\\|_1 = |1| + |1| + 0 = 2$.\n\nNow we search for an alternative signal $\\hat{x} \\neq x$. From our earlier work, any such signal must be of the form $\\hat{x}(\\alpha) = x - \\alpha h$ for some non-zero scalar $\\alpha$, where $h=[1, 1, -2]^T$.\n$\\hat{x}(\\alpha) = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\alpha \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1-\\alpha \\\\ 1-\\alpha \\\\ 2\\alpha \\end{pmatrix}$.\nThis family of vectors $\\hat{x}(\\alpha)$ satisfies $A\\hat{x}(\\alpha) = Ax$ for any $\\alpha$. We also need to satisfy the condition $\\|\\hat{x}\\|_1 \\le \\|x\\|_1 = 2$.\n$\\|\\hat{x}(\\alpha)\\|_1 = |1-\\alpha| + |1-\\alpha| + |2\\alpha| = 2|1-\\alpha| + 2|\\alpha|$.\n\nLet's analyze this norm as a function of $\\alpha$:\n- If $\\alpha  0$: $\\|\\hat{x}(\\alpha)\\|_1 = 2(1-\\alpha) + 2(-\\alpha) = 2 - 4\\alpha  2$.\n- If $0 \\le \\alpha \\le 1$: $\\|\\hat{x}(\\alpha)\\|_1 = 2(1-\\alpha) + 2(\\alpha) = 2 - 2\\alpha + 2\\alpha = 2$.\n- If $\\alpha  1$: $\\|\\hat{x}(\\alpha)\\|_1 = 2(\\alpha-1) + 2(\\alpha) = 4\\alpha - 2  2$.\n\nThe condition $\\|\\hat{x}\\|_1 \\le \\|x\\|_1 = 2$ is satisfied for any $\\alpha \\in [0, 1]$. Since we need $\\hat{x} \\neq x$, we must have $\\alpha \\neq 0$. So any $\\alpha \\in (0, 1]$ gives a valid counterexample.\n\nThe final condition is to find the sparsest $\\hat{x}$ in this family.\nThe vector is $\\hat{x}(\\alpha) = [1-\\alpha, 1-\\alpha, 2\\alpha]^T$.\n- For $\\alpha \\in (0, 1)$, none of the components are zero, so $\\hat{x}$ is 3-sparse (dense).\n- For the boundary case $\\alpha=1$, the vector becomes $\\hat{x}(1) = [1-1, 1-1, 2(1)]^T = [0, 0, 2]^T$. This vector is 1-sparse.\n\nThe sparsest possible $\\hat{x}$ is obtained at $\\alpha=1$, resulting in $\\hat{x} = [0, 0, 2]^T$.\nThis vector is different from $x=[1,1,0]^T$. Let's verify all conditions.\n1. $x=[1,1,0]^T$ is 2-sparse with smallest positive integer components.\n2. $\\hat{x}=[0,0,2]^T \\ne x$.\n3. $Ax = \\begin{pmatrix} 1  0  0.5 \\\\ 0  1  0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n   $A\\hat{x} = \\begin{pmatrix} 1  0  0.5 \\\\ 0  1  0.5 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. So $Ax = A\\hat{x}$.\n4. $\\|x\\|_1=2$. $\\|\\hat{x}\\|_1=|2|=2$. So $\\|\\hat{x}\\|_1 \\le \\|x\\|_1$.\n5. $\\hat{x}$ is 1-sparse, which is the sparsest possible alternative we found.\n\nTherefore, the signal $\\hat{x} = [0, 0, 2]^T$ meets all the criteria.",
            "answer": "$$\\boxed{\\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\end{pmatrix}}$$"
        }
    ]
}