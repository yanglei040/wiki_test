{
    "hands_on_practices": [
        {
            "introduction": "本练习旨在奠定信息压缩理论的基础。我们将为一个简单的三符号信源计算两个核心量：香农熵 $H(X)$ 和霍夫曼编码的平均长度 $G(X)$。通过这个直接的计算，您可以亲身体验理论下界与实际最优编码长度之间的关系，并理解为何 $G(X)$ 通常大于或等于 $H(X)$。",
            "id": "1653979",
            "problem": "一个离散无记忆信源由随机变量 $X$ 描述，它可以取三个可能符号 $\\{S_1, S_2, S_3\\}$ 中的一个。这些符号的概率由一个实数 $p$ 参数化，使得 $0  p  1/2$。概率由 $P(X=S_1) = p$，$P(X=S_2) = p$ 和 $P(X=S_3) = 1-2p$ 给出。\n\n对于这个信源，可以定义信息论中的两个重要量：\n1.  香农熵 $H(X)$，它是表示来自该信源的一个符号所需的平均比特数的理论下界。其计算公式为 $H(X) = -\\sum_{i=1}^{3} P(X=S_i) \\log_{2}(P(X=S_i))$。\n2.  为该信源设计的二进制霍夫曼编码的平均码长 $G(X)$。霍夫曼编码提供了一种最优的前缀无关编码，其平均长度由 $G(X) = \\sum_{i=1}^{3} P(X=S_i) l_i$ 给出，其中 $l_i$ 是分配给符号 $S_i$ 的二进制码字的长度。\n\n确定在信源符号等概率的特定情况下比率 $\\frac{G(X)}{H(X)}$ 的精确值，这种情况对应于 $p$ 的一个特定值。将您的答案表示为一个封闭形式的解析表达式。",
            "solution": "对于等概率符号，我们有 $P(X=S_{1})=P(X=S_{2})=P(X=S_{3})$，结合 $P(X=S_{1})=P(X=S_{2})=p$ 和 $P(X=S_{3})=1-2p$，可以推断出 $p=\\frac{1}{3}$。\n\n香农熵（以比特为单位）是\n$$\nH(X)=-\\sum_{i=1}^{3} P(X=S_{i}) \\log_{2}\\big(P(X=S_{i})\\big)\n=-3 \\cdot \\frac{1}{3} \\log_{2}\\!\\left(\\frac{1}{3}\\right)\n=-\\log_{2}\\!\\left(\\frac{1}{3}\\right)\n=\\log_{2}(3).\n$$\n\n对于具有三个等概率符号的二进制霍夫曼编码，最优编码首先合并两个概率相等的符号，得到码长 $\\{1,2,2\\}$。这满足克拉夫特不等式 $2^{-1}+2\\cdot 2^{-2}=1$，因此它是一个有效的最优前缀无关二进制编码。平均码长是\n$$\nG(X)=\\sum_{i=1}^{3} P(X=S_{i})\\,l_{i}\n=\\frac{1}{3}\\cdot 1+\\frac{1}{3}\\cdot 2+\\frac{1}{3}\\cdot 2\n=\\frac{5}{3}.\n$$\n\n因此，该比率为\n$$\n\\frac{G(X)}{H(X)}=\\frac{\\frac{5}{3}}{\\log_{2}(3)}=\\frac{5}{3\\log_{2}(3)}.\n$$",
            "answer": "$$\\boxed{\\frac{5}{3\\log_{2}(3)}}$$"
        },
        {
            "introduction": "在了解了 $G(X)$ 和 $H(X)$ 的基本关系后，我们来深入探讨它们之间的差异——编码冗余。这个练习将通过一个二元信源的例子，揭示即使是对于霍夫曼这样的“最优”编码，冗余也可能存在。通过分析，您将理解冗余的根源并非编码算法本身的缺陷，而是信源符号概率分布的固有属性所致。",
            "id": "1653958",
            "problem": "考虑一个产生符号集 $\\{S_1, S_2\\}$ 中符号的二进制信息源。发出符号 $S_1$ 的概率为 $p$，发出符号 $S_2$ 的概率为 $1-p$，其中 $0  p  1$。\n\n该信源数据压缩的基本极限由其香non熵 $H$ 给出，定义为：\n$$H(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$$\n熵的单位是比特/符号。\n\n该信源的最优前缀码（例如霍夫曼编码）的平均码长，记为 $G$，单位为比特/符号。编码的低效率由其冗余度 $\\rho$ 来衡量，定义为平均码长与熵之间的差值：\n$$\\rho(p) = G(p) - H(p)$$\n\n您的任务是分析该二进制信源最优编码的冗余度。确定在 $p \\leq 0.5$ 的约束下，概率 $p$ 的值，使得冗余度 $\\rho(p)$ 恰好是其在定义域 $0  p  1$ 上的最大可能值的一半。\n\n以下哪个值最接近正确的 $p$ 值？\n\nA. 0.028\n\nB. 0.081\n\nC. 0.110\n\nD. 0.250\n\nE. 0.395",
            "solution": "求解过程包括四个主要步骤：确定平均码长 $G(p)$，找到最大冗余度 $\\rho_{max}$，为所需条件建立方程，以及测试给定选项以找到答案。\n\n第 1 步：确定平均码长 $G(p)$。\n对于一个有两个符号 $S_1$ 和 $S_2$ 的二进制信源，最优前缀码（如霍夫曼编码）将为每个符号分配一个比特。例如，编码可以是 $\\{S_1 \\to 0, S_2 \\to 1\\}$。$S_1$ 的码长为 $l_1=1$，$S_2$ 的码长为 $l_2=1$。平均码长 $G(p)$ 是这些长度根据其概率的加权平均值：\n$$G(p) = p \\cdot l_1 + (1-p) \\cdot l_2 = p \\cdot 1 + (1-p) \\cdot 1 = p + 1 - p = 1$$\n因此，对于该二进制信源的最优编码，其平均码长是一个常数，对于任何 $p \\in (0, 1)$ 都等于 1 比特/符号。\n\n第 2 步：确定最大可能冗余度 $\\rho_{max}$。\n冗余度由 $\\rho(p) = G(p) - H(p)$ 给出。代入 $G(p)=1$ 的值，我们得到：\n$$\\rho(p) = 1 - H(p) = 1 - [-p \\log_2(p) - (1-p) \\log_2(1-p)]$$\n为了最大化冗余度 $\\rho(p)$，我们必须最小化熵 $H(p)$。二进制熵函数 $H(p)$ 定义在 $p \\in (0, 1)$上。它是一个凹函数，在 $p=0.5$ 时达到最大值 1，在其定义域的边界处达到最小值。我们来考察当 $p$ 趋近于 0 时的极限：\n$$\\lim_{p \\to 0^+} H(p) = \\lim_{p \\to 0^+} [-p \\log_2(p) - (1-p) \\log_2(1-p)]$$\n我们使用标准极限 $\\lim_{x \\to 0^+} x \\log_b(x) = 0$。第二项变为 $\\lim_{p \\to 0^+} -(1-p) \\log_2(1-p) = -(1) \\log_2(1) = 0$。\n因此，$\\lim_{p \\to 0^+} H(p) = 0$。根据对称性，$\\lim_{p \\to 1^-} H(p) = 0$。\n熵的最小值为 0。因此，最大冗余度为：\n$$\\rho_{max} = G - H_{min} = 1 - 0 = 1 \\text{ 比特/符号}$$\n\n第 3 步：建立方程。\n问题要求的是当冗余度是其最大值的一半时，$p$ 的值（$p \\le 0.5$）。\n$$\\rho(p) = \\frac{1}{2} \\rho_{max} = \\frac{1}{2} \\cdot 1 = 0.5$$\n代入 $\\rho(p) = 1 - H(p)$，我们得到：\n$$1 - H(p) = 0.5$$\n$$H(p) = 0.5$$\n这导出了以下超越方程：\n$$-p \\log_2(p) - (1-p) \\log_2(1-p) = 0.5$$\n\n第 4 步：测试给定选项。\n我们必须找出哪个给定的 $p$ 值产生的熵 $H(p)$最接近 0.5。\n\nA. 对于 $p = 0.028$：\n$H(0.028) = -0.028 \\log_2(0.028) - 0.972 \\log_2(0.972) \\approx -0.028(-5.159) - 0.972(-0.041) \\approx 0.1444 + 0.0398 \\approx 0.184$\n\nB. 对于 $p = 0.081$：\n$H(0.081) = -0.081 \\log_2(0.081) - 0.919 \\log_2(0.919) \\approx -0.081(-3.626) - 0.919(-0.122) \\approx 0.2937 + 0.1121 \\approx 0.406$\n\nC. 对于 $p = 0.110$：\n$H(0.110) = -0.110 \\log_2(0.110) - 0.890 \\log_2(0.890) \\approx -0.110(-3.185) - 0.890(-0.167) \\approx 0.3504 + 0.1486 \\approx 0.499$\n\nD. 对于 $p = 0.250$：\n$H(0.250) = -0.250 \\log_2(0.250) - 0.750 \\log_2(0.750) = -0.25(-2) - 0.75(\\log_2(3)-2) \\approx 0.5 - 0.75(1.585 - 2) \\approx 0.5 - 0.75(-0.415) \\approx 0.5 + 0.3113 \\approx 0.811$\n\nE. 对于 $p = 0.395$：\n$H(0.395) = -0.395 \\log_2(0.395) - 0.605 \\log_2(0.605) \\approx -0.395(-1.340) - 0.605(-0.724) \\approx 0.5293 + 0.4380 \\approx 0.967$\n\n将计算出的熵值与目标值 0.5 进行比较：\n- 对于 $p=0.028$，$H \\approx 0.184$\n- 对于 $p=0.081$，$H \\approx 0.406$\n- 对于 $p=0.110$，$H \\approx 0.499$\n- 对于 $p=0.250$，$H \\approx 0.811$\n- 对于 $p=0.395$，$H \\approx 0.967$\n\n$H(0.110)$ 的值最接近 0.5。因此，$p=0.110$ 是最佳选择。",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "此前的练习都聚焦于单个信源。现在，我们将视野拓展到处理多个相互关联的信源。本练习将引导您量化通过联合编码（即将多个信源视为一个整体进行编码）相对于独立编码所能获得的压缩收益。这个收益的大小直接关系到信源之间的互信息，它揭示了利用信源相关性来设计更高效压缩系统的重要性。",
            "id": "1654016",
            "problem": "在一个数据压缩系统中，我们分析两个相关的二进制信源 $X$ 和 $Y$，它们从字母表 $\\{0, 1\\}$ 中产生符号。这些信源的联合行为由联合概率质量函数 $p(x, y) = P(X=x, Y=y)$ 描述。具体的概率如下：\n$p(0, 0) = \\frac{1}{8}$\n$p(0, 1) = \\frac{3}{8}$\n$p(1, 0) = \\frac{3}{8}$\n$p(1, 1) = \\frac{1}{8}$\n\n在此背景下，信源 $S$ 的二进制编码的理论最小平均长度是其香农熵，我们记为 $G(S)$。这个理想的编码长度以比特为单位，并使用公式 $G(S) = H(S) = -\\sum_{i} p_i \\log_2(p_i)$ 计算，其中 $\\{p_i\\}$ 是来自信源 $S$ 的符号的概率分布。\n\n我们可以分别对信源 $X$ 和 $Y$进行编码，得到的总理想平均编码长度为 $G(X) + G(Y)$。或者，我们可以为联合信源 $(X, Y)$ 设计一个单一编码，其理想平均长度为 $G(X,Y)$。差值 $\\Delta G = (G(X) + G(Y)) - G(X,Y)$ 表示通过使用联合编码方案所消除的冗余。它量化了考虑信源之间统计依赖关系所带来的好处。\n\n计算这个冗余值 $\\Delta G$。您的答案应以比特为单位，并四舍五入到四位有效数字。",
            "solution": "给定一个二进制对 $(X,Y)$，其联合概率质量函数（pmf）为 $p(0,0)=\\frac{1}{8}$，$p(0,1)=\\frac{3}{8}$，$p(1,0)=\\frac{3}{8}$，$p(1,1)=\\frac{1}{8}$。冗余为 $\\Delta G = G(X)+G(Y)-G(X,Y)$，其中 $G(\\cdot)=H(\\cdot)$，单位为比特，并且 $\\Delta G$ 等于互信息 $I(X;Y)$。\n\n首先，计算边缘概率：\n$$\nP(X=0)=p(0,0)+p(0,1)=\\frac{1}{8}+\\frac{3}{8}=\\frac{1}{2},\\quad P(X=1)=\\frac{1}{2},\n$$\n$$\nP(Y=0)=p(0,0)+p(1,0)=\\frac{1}{8}+\\frac{3}{8}=\\frac{1}{2},\\quad P(Y=1)=\\frac{1}{2}.\n$$\n因此，$X$ 和 $Y$ 都是参数为 $\\frac{1}{2}$ 的伯努利分布，所以\n$$\nH(X)=H(Y)=-\\sum_{x\\in\\{0,1\\}} P(X=x)\\log_{2} P(X=x) = -2\\cdot \\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)=1.\n$$\n\n接下来，计算联合熵：\n$$\nH(X,Y)=-\\sum_{x,y\\in\\{0,1\\}} p(x,y)\\log_{2} p(x,y)\n= -\\Bigg[2\\cdot \\frac{1}{8}\\log_{2}\\!\\left(\\frac{1}{8}\\right) + 2\\cdot \\frac{3}{8}\\log_{2}\\!\\left(\\frac{3}{8}\\right)\\Bigg].\n$$\n使用 $\\log_{2}\\!\\left(\\frac{1}{8}\\right)=-3$ 和 $\\log_{2}\\!\\left(\\frac{3}{8}\\right)=\\log_{2}(3)-3$ 来简化：\n$$\nH(X,Y) = -2\\left[\\frac{1}{8}(-3) + \\frac{3}{8}\\left(\\log_{2} 3 - 3\\right)\\right]\n= -2\\left[-\\frac{3}{8} + \\frac{3}{8}\\log_{2} 3 - \\frac{9}{8}\\right]\n= -2\\left[\\frac{3}{8}\\log_{2} 3 - \\frac{12}{8}\\right].\n$$\n因此，\n$$\nH(X,Y) = -\\frac{3}{4}\\log_{2} 3 + 3.\n$$\n该公式有误，应为\n$$\nH(X,Y) = 3 - \\frac{3}{4}\\log_{2} 3.\n$$\n\n所以，\n$$\n\\Delta G = H(X)+H(Y)-H(X,Y) = 2 - \\left(3 - \\frac{3}{4}\\log_{2} 3\\right) = \\frac{3}{4}\\log_{2} 3 - 1.\n$$\n数值上，使用 $\\log_{2} 3 \\approx 1.5849625007$，我们得到\n$$\n\\Delta G \\approx \\frac{3}{4}\\cdot 1.5849625007 - 1 \\approx 0.1887218755 \\text{ 比特}.\n$$\n四舍五入到四位有效数字，结果是 $0.1887$ 比特。",
            "answer": "$$\\boxed{0.1887}$$"
        }
    ]
}