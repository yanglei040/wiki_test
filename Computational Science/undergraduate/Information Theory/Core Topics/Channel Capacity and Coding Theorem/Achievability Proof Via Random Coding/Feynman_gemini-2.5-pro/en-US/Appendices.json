{
    "hands_on_practices": [
        {
            "introduction": "The foundation of Shannon's achievability proof is the concept of a randomly generated codebook. Before analyzing its performance over a channel, it is essential to understand the statistical properties of the codebook itself. This first exercise provides a concrete starting point by asking you to calculate the expected number of a specific, simple codeword within a randomly generated ensemble, giving you a feel for the probabilistic nature of its construction. ",
            "id": "1601638",
            "problem": "In the analysis of channel coding, the random coding argument is a powerful tool used to prove the existence of good codes. Consider the construction of a random codebook, $\\mathcal{C}$, for a binary channel. The codebook contains $M = 2^{nR}$ unique codewords, where $n$ is the blocklength (the length of each codeword in bits) and $R$ is the code rate in bits per channel use.\n\nEach of the $M$ codewords in the codebook is generated independently. Specifically, for each codeword, its $n$ bits are generated as a sequence of independent and identically distributed (i.i.d.) random variables. Each bit is drawn from a Bernoulli distribution where the probability of the bit being a '1' is $p$ and the probability of it being a '0' is $1-p$.\n\nCalculate the expected number of codewords that are the all-zero sequence (i.e., a sequence of $n$ zeros) in this randomly generated codebook. Your answer should be a closed-form analytic expression in terms of $n$, $R$, and $p$.",
            "solution": "Let $M=2^{nR}$ be the number of codewords. For each codeword, define the indicator random variable $I_m$ that equals $1$ if the $m$th codeword is the all-zero sequence and $0$ otherwise, for $m \\in \\{1,\\dots,M\\}$. The total number of all-zero codewords in the codebook is\n$$\nN_0=\\sum_{m=1}^{M} I_m.\n$$\nWe compute $\\mathbb{E}[N_0]$ using linearity of expectation:\n$$\n\\mathbb{E}[N_0]=\\sum_{m=1}^{M} \\mathbb{E}[I_m]=\\sum_{m=1}^{M} \\Pr\\{I_m=1\\}.\n$$\nEach codeword consists of $n$ i.i.d. bits, with each bit equal to $0$ with probability $1-p$. By independence across the $n$ bits, the probability that a given codeword is the all-zero sequence is\n$$\n\\Pr\\{I_m=1\\}=(1-p)^{n}.\n$$\nTherefore,\n$$\n\\mathbb{E}[N_0]=M(1-p)^{n}=2^{nR}(1-p)^{n}.\n$$\nThis is a closed-form expression in terms of $n$, $R$, and $p$.",
            "answer": "$$\\boxed{2^{nR}(1-p)^{n}}$$"
        },
        {
            "introduction": "Having established the properties of a random codebook, the next step is to analyze its performance during transmission. This problem places our random code in a practical scenario, sending it over a Binary Erasure Channel (BEC), a fundamental model in information theory. By calculating the expected number of incorrect codewords that could cause a decoding error, you will directly engage with the core logic used in the achievability proof to show that the probability of error can be made arbitrarily small. ",
            "id": "1601672",
            "problem": "Consider a random codebook $\\mathcal{C}$ designed for communication over a Binary Erasure Channel (BEC). The codebook consists of $M$ codewords, say $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_M\\}$, each of length $n$. Each bit of each codeword is generated independently from a Bernoulli($1/2$) distribution, meaning the probability of a bit being 0 is $1/2$, and the probability of it being 1 is also $1/2$. The generation of each codeword is independent of all other codewords.\n\nSuppose codeword $\\mathbf{x}_1$ is transmitted, and the received sequence is $\\mathbf{y}$. Due to the nature of the BEC, this sequence $\\mathbf{y}$ has exactly $k$ components that are erasures and $n-k$ components that are identical to the corresponding bits in $\\mathbf{x}_1$.\n\nAn incorrect codeword, $\\mathbf{x}_i$ where $i \\neq 1$, is said to be 'consistent' with the received sequence $\\mathbf{y}$ if its bits match the non-erased bits of $\\mathbf{y}$. That is, for any position $j$ where the received bit $y_j$ is not an erasure, the $j$-th bit of $\\mathbf{x}_i$ must be equal to $y_j$.\n\nLet $N$ be the random variable for the number of incorrect codewords in the codebook that are consistent with $\\mathbf{y}$. Calculate the expected value of $N$, denoted $\\mathbb{E}[N]$, over the ensemble of all possible randomly generated codebooks.\n\nExpress your final answer as a closed-form analytic expression in terms of $M$, $n$, and $k$.",
            "solution": "Define the indicator variable for each incorrect codeword $\\mathbf{x}_i$, $i \\in \\{2,3,\\ldots,M\\}$, as\n$$\nI_i=\\begin{cases}\n1, & \\text{if }\\mathbf{x}_i\\text{ is consistent with }\\mathbf{y},\\\\\n0, & \\text{otherwise.}\n\\end{cases}\n$$\nBy definition, the total number of incorrect consistent codewords is\n$$\nN=\\sum_{i=2}^{M} I_i.\n$$\nUsing linearity of expectation,\n$$\n\\mathbb{E}[N]=\\sum_{i=2}^{M} \\mathbb{E}[I_i]=\\sum_{i=2}^{M} \\Pr\\{I_i=1\\}.\n$$\nCondition on the received sequence $\\mathbf{y}$ that has exactly $k$ erasures and $n-k$ non-erased positions that match $\\mathbf{x}_1$. Let $S$ be the set of non-erased positions, so $|S|=n-k$, and for each $j \\in S$, $y_j=x_{1,j}$ is fixed. Each codeword $\\mathbf{x}_i$ is generated independently with i.i.d. Bernoulli($1/2$) bits, independent of $\\mathbf{y}$ and $\\mathbf{x}_1$. Therefore, for a fixed $i \\neq 1$,\n$$\n\\Pr\\{I_i=1\\}=\\Pr\\{\\mathbf{x}_i\\text{ matches }\\mathbf{y}\\text{ on all }j \\in S\\}\n=\\prod_{j \\in S} \\Pr\\{x_{ij}=y_j\\}\n=\\left(\\frac{1}{2}\\right)^{|S|}=2^{-(n-k)}.\n$$\nThis probability is the same for each $i \\in \\{2,\\ldots,M\\}$, hence\n$$\n\\mathbb{E}[N]=\\sum_{i=2}^{M} 2^{-(n-k)}=(M-1)\\,2^{-(n-k)}.\n$$",
            "answer": "$$\\boxed{(M-1)\\,2^{-(n-k)}}$$"
        },
        {
            "introduction": "Why is the random coding approach so effective? The answer lies in the Asymptotic Equipartition Property (AEP), which states that for long blocklengths, nearly all source outputs belong to a small \"typical set.\" This final practice problem provides a powerful illustration by exploring a flawed coding strategy that deliberately avoids typical sequences. By demonstrating why a codebook built from these non-typical sequences fails for a biased source, you will gain a deeper appreciation for the profound insight behind Shannon's probabilistic method. ",
            "id": "1601642",
            "problem": "An engineer is designing a fixed-length source coding scheme for a binary memoryless source. The source generates bits, where the probability of generating a '1' is $p$ and the probability of generating a '0' is $1-p$. The value of $p$ is known to be a constant other than $1/2$.\n\nThe engineer misunderstands the Asymptotic Equipartition Property (AEP) and hypothesizes that the most important sequences to encode are those with the highest empirical entropy. For a binary sequence of length $n$, the empirical entropy is maximized when the number of '1's is equal to the number of '0's. Based on this flawed reasoning, for a given even integer block length $n$, the engineer constructs a codebook, $\\mathcal{C}_n$, that consists exclusively of all possible binary sequences of length $n$ containing exactly $n/2$ ones.\n\nThe coding scheme is considered a \"success\" for a given source output sequence if that sequence is a member of the codebook $\\mathcal{C}_n$, and a \"failure\" otherwise. Find an approximate analytical expression for the probability of success, $P_n(\\text{success})$, for large even $n$.\n\nFor your calculation, use the following large-$n$ approximation for the binomial coefficient, which is derived from Stirling's formula:\n$$ \\binom{n}{n\\alpha} \\approx \\frac{1}{\\sqrt{2\\pi n \\alpha(1-\\alpha)}} 2^{n H(\\alpha)} $$\nwhere $H(\\alpha) = -\\alpha \\log_2(\\alpha) - (1-\\alpha) \\log_2(1-\\alpha)$ is the binary entropy function.\n\nYour final answer should be an expression in terms of $n$ and $p$.",
            "solution": "The problem asks for the probability that a randomly generated sequence of length $n$ from a binary memoryless source falls into a specific set of sequences $\\mathcal{C}_n$. This set $\\mathcal{C}_n$ contains all binary sequences of length $n$ with exactly $n/2$ ones.\n\nLet $X^n = (X_1, X_2, \\dots, X_n)$ be a sequence of $n$ independent and identically distributed random variables, where each $X_i$ follows a Bernoulli distribution with $P(X_i=1) = p$ and $P(X_i=0) = 1-p$.\n\nFirst, let's determine the probability of any single specific sequence belonging to the codebook $\\mathcal{C}_n$. A sequence in $\\mathcal{C}_n$ has exactly $k=n/2$ ones and $n-k = n/2$ zeros. Since the source is memoryless, the probability of any such specific sequence is:\n$$ P(\\text{specific sequence with } n/2 \\text{ ones}) = p^{n/2} (1-p)^{n/2} = (p(1-p))^{n/2} $$\n\nNext, we need to find the number of such sequences in the codebook, which is the size of the set $|\\mathcal{C}_n|$. This is a combinatorial problem: we need to find the number of ways to place $n/2$ ones in $n$ available positions. This is given by the binomial coefficient:\n$$ |\\mathcal{C}_n| = \\binom{n}{n/2} $$\n\nThe total probability of success, $P_n(\\text{success})$, is the sum of the probabilities of all mutually exclusive sequences in the codebook. Since each sequence in $\\mathcal{C}_n$ has the same probability, we can find the total probability by multiplying the number of sequences by the probability of a single sequence:\n$$ P_n(\\text{success}) = |\\mathcal{C}_n| \\times P(\\text{one sequence in } \\mathcal{C}_n) $$\n$$ P_n(\\text{success}) = \\binom{n}{n/2} (p(1-p))^{n/2} $$\n\nNow, we use the provided large-$n$ approximation for the binomial coefficient. In our case, the number of ones is $k=n/2$, which corresponds to setting $\\alpha = k/n = (n/2)/n = 1/2$ in the given formula.\n$$ \\binom{n}{n/2} \\approx \\frac{1}{\\sqrt{2\\pi n (\\frac{1}{2})(1-\\frac{1}{2})}} 2^{n H(1/2)} $$\n\nLet's evaluate the terms in this approximation. The term in the square root is:\n$$ 2\\pi n (\\frac{1}{2})(\\frac{1}{2}) = \\frac{\\pi n}{2} $$\nThe binary entropy function $H(\\alpha)$ for $\\alpha=1/2$ is:\n$$ H(1/2) = -\\frac{1}{2}\\log_2\\left(\\frac{1}{2}\\right) - \\left(1-\\frac{1}{2}\\right)\\log_2\\left(1-\\frac{1}{2}\\right) = -\\frac{1}{2}(-1) - \\frac{1}{2}(-1) = \\frac{1}{2} + \\frac{1}{2} = 1 $$\nSubstituting these back into the approximation for the binomial coefficient:\n$$ \\binom{n}{n/2} \\approx \\frac{1}{\\sqrt{\\frac{\\pi n}{2}}} 2^{n \\cdot 1} = \\sqrt{\\frac{2}{\\pi n}} 2^n $$\n\nFinally, we substitute this approximate expression for $\\binom{n}{n/2}$ into our formula for $P_n(\\text{success})$:\n$$ P_n(\\text{success}) \\approx \\left(\\sqrt{\\frac{2}{\\pi n}} 2^n\\right) (p(1-p))^{n/2} $$\nWe can combine the terms with exponent $n$:\n$$ (p(1-p))^{n/2} = (\\sqrt{p(1-p)})^n $$\nSo, the expression becomes:\n$$ P_n(\\text{success}) \\approx \\sqrt{\\frac{2}{\\pi n}} \\cdot 2^n \\cdot (\\sqrt{p(1-p)})^n $$\n$$ P_n(\\text{success}) \\approx \\sqrt{\\frac{2}{\\pi n}} (2\\sqrt{p(1-p)})^n $$\nThis is the final approximate expression for the probability of success. Since the problem states $p \\neq 1/2$, the arithmetic mean-geometric mean inequality implies $\\sqrt{p(1-p)} < \\frac{p+(1-p)}{2} = \\frac{1}{2}$, which means $2\\sqrt{p(1-p)} < 1$. Therefore, the probability of success decays exponentially to zero as $n \\to \\infty$, demonstrating why this codebook design is flawed.",
            "answer": "$$\\boxed{\\sqrt{\\frac{2}{\\pi n}} \\left(2\\sqrt{p(1-p)}\\right)^n}$$"
        }
    ]
}