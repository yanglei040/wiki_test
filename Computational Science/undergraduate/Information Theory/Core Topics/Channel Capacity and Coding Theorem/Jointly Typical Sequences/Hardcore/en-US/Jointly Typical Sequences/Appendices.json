{
    "hands_on_practices": [
        {
            "introduction": "This first exercise serves as a fundamental check of the definition of joint typicality. By working through a small, concrete example, you will directly apply the three conditions involving marginal and joint entropies, transforming an abstract definition into a practical calculation. This practice builds a solid foundation for understanding what it means for a pair of sequences to be \"typical\" .",
            "id": "1635549",
            "problem": "In a simplified model of a wireless sensor network, a sensor node observes two correlated binary events, represented by random variables $X$ and $Y$, where $X, Y \\in \\{0, 1\\}$. The events are generated independently and identically distributed (i.i.d.) at each time step according to a fixed joint probability mass function (PMF), $p(x, y)$. The joint PMF is given by:\n$p(0, 0) = \\frac{1}{2}$\n$p(0, 1) = \\frac{1}{4}$\n$p(1, 0) = \\frac{1}{8}$\n$p(1, 1) = \\frac{1}{8}$\n\nA pair of sequences of length $n$, $(x^n, y^n) = ((x_1, \\dots, x_n), (y_1, \\dots, y_n))$, is said to be $\\epsilon$-jointly typical with respect to $p(x,y)$ if the following three conditions, based on the Shannon entropies $H(X)$, $H(Y)$, and $H(X,Y)$, are all satisfied (using logarithm base 2):\n1. $|-\\frac{1}{n} \\log_{2} p(x^n) - H(X)| \\le \\epsilon$\n2. $|-\\frac{1}{n} \\log_{2} p(y^n) - H(Y)| \\le \\epsilon$\n3. $|-\\frac{1}{n} \\log_{2} p(x^n, y^n) - H(X,Y)| \\le \\epsilon$\n\nHere, $p(x^n) = \\prod_{i=1}^{n} p(x_i)$, $p(y^n) = \\prod_{i=1}^{n} p(y_i)$, and $p(x^n, y^n) = \\prod_{i=1}^{n} p(x_i, y_i)$.\n\nConsider the specific pair of sequences of length $n=4$:\n$x^4 = (0, 1, 0, 0)$\n$y^4 = (0, 0, 1, 0)$\n\nGiven $\\epsilon = 0.3$, determine if this specific pair of sequences is $\\epsilon$-jointly typical for the given PMF. Select the most accurate statement from the options below.\n\nA. Yes, the sequence pair is jointly typical.\n\nB. No, the sequence pair is not jointly typical because the condition based on the marginal entropy $H(X)$ is not satisfied.\n\nC. No, the sequence pair is not jointly typical because the condition based on the marginal entropy $H(Y)$ is not satisfied.\n\nD. No, the sequence pair is not jointly typical because the condition based on the joint entropy $H(X,Y)$ is not satisfied.\n\nE. No, the sequence pair is not jointly typical because more than one of the required conditions are not satisfied.",
            "solution": "We are given the joint PMF\n$$\np(0,0)=\\frac{1}{2},\\quad p(0,1)=\\frac{1}{4},\\quad p(1,0)=\\frac{1}{8},\\quad p(1,1)=\\frac{1}{8},\n$$\nwhich yields marginals\n$$\np_{X}(0)=\\frac{3}{4},\\quad p_{X}(1)=\\frac{1}{4},\\quad p_{Y}(0)=\\frac{5}{8},\\quad p_{Y}(1)=\\frac{3}{8}.\n$$\nThe entropies (base 2) are\n$$\nH(X)=-\\left(\\frac{3}{4}\\log_{2}\\frac{3}{4}+\\frac{1}{4}\\log_{2}\\frac{1}{4}\\right),\\quad\nH(Y)=-\\left(\\frac{5}{8}\\log_{2}\\frac{5}{8}+\\frac{3}{8}\\log_{2}\\frac{3}{8}\\right),\n$$\n$$\nH(X,Y)=-\\left(\\frac{1}{2}\\log_{2}\\frac{1}{2}+\\frac{1}{4}\\log_{2}\\frac{1}{4}+\\frac{1}{8}\\log_{2}\\frac{1}{8}+\\frac{1}{8}\\log_{2}\\frac{1}{8}\\right).\n$$\nSince $\\log_{2}\\frac{1}{2}=-1$, $\\log_{2}\\frac{1}{4}=-2$, $\\log_{2}\\frac{1}{8}=-3$, we have\n$$\nH(X,Y)=\\frac{1}{2}\\cdot 1+\\frac{1}{4}\\cdot 2+\\frac{1}{8}\\cdot 3+\\frac{1}{8}\\cdot 3=\\frac{7}{4}.\n$$\n\nNow evaluate the three typicality conditions for $x^{4}=(0,1,0,0)$ and $y^{4}=(0,0,1,0)$ with $\\epsilon=0.3$.\n\n1) Marginal-$X$ condition:\nThe sequence $x^4$ has an empirical distribution that matches the source distribution $p_X$. Thus, its empirical entropy is exactly $H(X)$.\n$p(x^{4})=p_{X}(0)^{3}p_{X}(1)^{1}=\\left(\\frac{3}{4}\\right)^{3}\\left(\\frac{1}{4}\\right)$,\nso\n$-\\frac{1}{4}\\log_{2}p(x^{4})=-\\left(\\frac{3}{4}\\log_{2}\\frac{3}{4}+\\frac{1}{4}\\log_{2}\\frac{1}{4}\\right)=H(X)$.\nThus\n$\\left|-\\frac{1}{4}\\log_{2}p(x^{4})-H(X)\\right|=0\\le\\epsilon$.\n\n2) Marginal-$Y$ condition:\n$p(y^{4})=p_{Y}(0)^{3}p_{Y}(1)^{1}=\\left(\\frac{5}{8}\\right)^{3}\\left(\\frac{3}{8}\\right)$,\nso\n$-\\frac{1}{4}\\log_{2}p(y^{4})=-\\left(\\frac{3}{4}\\log_{2}\\frac{5}{8}+\\frac{1}{4}\\log_{2}\\frac{3}{8}\\right)$.\nThe deviation from $H(Y)$ is\n$$\n\\left|-\\frac{1}{4}\\log_{2}p(y^{4})-H(Y)\\right|\n=\\left|-\\left(\\frac{3}{4}a+\\frac{1}{4}b\\right)+\\left(\\frac{5}{8}a+\\frac{3}{8}b\\right)\\right|\n=\\left|\\frac{1}{8}a-\\frac{1}{8}b\\right|\n=\\frac{1}{8}\\left|\\log_{2}\\frac{5}{8}-\\log_{2}\\frac{3}{8}\\right|\n=\\frac{1}{8}\\log_{2}\\frac{5}{3},\n$$\nwhere $a=\\log_{2}\\frac{5}{8}$ and $b=\\log_{2}\\frac{3}{8}$. Since $\\frac{5}{3}2$, we have $\\log_{2}\\frac{5}{3}1$, hence\n$\\frac{1}{8}\\log_{2}\\frac{5}{3}\\frac{1}{8}=0.1250.3=\\epsilon$.\nThus the $Y$-marginal condition is satisfied.\n\n3) Joint condition:\nThe sequence of pairs is $((0,0), (1,0), (0,1), (0,0))$. The empirical joint distribution matches the source distribution $p(x,y)$, so its empirical joint entropy is exactly $H(X,Y)$.\n$p(x^{4},y^{4})=p(0,0)p(1,0)p(0,1)p(0,0)=\\left(\\frac{1}{2}\\right)\\left(\\frac{1}{8}\\right)\\left(\\frac{1}{4}\\right)\\left(\\frac{1}{2}\\right)=\\frac{1}{128}$,\nso\n$-\\frac{1}{4}\\log_{2}p(x^{4},y^{4})=-\\frac{1}{4}\\log_{2}\\frac{1}{128}=\\frac{7}{4}=H(X,Y)$,\nand hence\n$\\left|-\\frac{1}{4}\\log_{2}p(x^{4},y^{4})-H(X,Y)\\right|=0\\le\\epsilon$.\n\nAll three conditions are satisfied with $\\epsilon=0.3$, so the sequence pair is $\\epsilon$-jointly typical. The most accurate choice is A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Moving beyond a simple check, this problem illustrates one of the most powerful applications of joint typicality: characterizing noisy communication channels. You will discover how the concept helps quantify the number of possible output sequences that could result from a single typical input sequence after passing through a noisy channel. This exercise reveals the deep connection between conditional entropy and the \"fan-out\" effect of a channel, a key insight for understanding the limits of reliable communication .",
            "id": "1635559",
            "problem": "A deep-space probe sends a binary data stream to Earth. The data can be modeled as a sequence $X^n = (X_1, \\dots, X_n)$ of independent and identically distributed random variables from a Bernoulli source $X$ with $P(X=1) = p$ and $P(X=0)=1-p$. The transmission occurs over a noisy channel, which can be modeled as a memoryless Binary Symmetric Channel (BSC) with a crossover probability $\\alpha$, where $0  \\alpha  1$. The received sequence is $Y^n = (Y_1, \\dots, Y_n)$. This channel model implies that each received bit $Y_i$ is related to the transmitted bit $X_i$ by $Y_i = X_i \\oplus Z_i$, where $\\oplus$ denotes addition modulo 2, and $Z_i$ represents a noise bit from a Bernoulli process $Z$ with $P(Z=1) = \\alpha$. The noise process $Z$ is independent of the source process $X$.\n\nAccording to the Asymptotic Equipartition Property (AEP), for a sufficiently small $\\epsilon > 0$ and a very large sequence length $n$, the size of the set of typical source sequences, denoted by $|A_\\epsilon^{(n)}(X)|$, is well-approximated by $2^{n H(X)}$, where $H(X)$ is the Shannon entropy of the source. Similarly, the size of the set of jointly typical sequences, denoted by $|A_\\epsilon^{(n)}(X,Y)|$, is approximated by $2^{n H(X,Y)}$, where $H(X,Y)$ is the joint entropy.\n\nDetermine the approximate number of distinct received sequences $Y^n$ that can be considered jointly typical with any *single given* typical source sequence $X^n$. This quantity corresponds to the ratio $\\frac{|A_\\epsilon^{(n)}(X,Y)|}{|A_\\epsilon^{(n)}(X)|}$ in the large $n$ limit. Provide a closed-form analytic expression for this ratio. Express your answer in terms of $n$, the crossover probability $\\alpha$, and the binary entropy function $H(\\cdot)$, which is defined as $H(q) = -q \\log_2(q) - (1-q) \\log_2(1-q)$ for $q \\in (0,1)$.",
            "solution": "We model the i.i.d. Bernoulli source $X$ with $P(X=1)=p$ and the memoryless BSC with crossover probability $\\alpha$, so $Y=X\\oplus Z$, where $Z$ is independent of $X$ and $P(Z=1)=\\alpha$. For a small $\\epsilon>0$ and large $n$, the Asymptotic Equipartition Property implies the approximations\n$$\n|A_{\\epsilon}^{(n)}(X)| \\approx 2^{n H(X)}, \\quad |A_{\\epsilon}^{(n)}(X,Y)| \\approx 2^{n H(X,Y)}.\n$$\nThe number of distinct received sequences $Y^{n}$ that are jointly typical with any single given typical source sequence $X^{n}$ is well-approximated by the conditional typical set size, which is the ratio\n$$\n\\frac{|A_{\\epsilon}^{(n)}(X,Y)|}{|A_{\\epsilon}^{(n)}(X)|} \\approx 2^{n\\left[H(X,Y)-H(X)\\right]} = 2^{n H(Y|X)}.\n$$\nUsing the channel model $Y=X\\oplus Z$ with $Z$ independent of $X$, we evaluate $H(Y|X)$:\n$$\nH(Y|X) = \\sum_{x\\in\\{0,1\\}} P(X=x)\\, H(Y|X=x).\n$$\nFor each $x\\in\\{0,1\\}$, $Y|X=x$ is a Bernoulli random variable with $P(Y\\neq x)=\\alpha$ and $P(Y=x)=1-\\alpha$, so\n$$\nH(Y|X=x) = H(\\alpha),\n$$\nwhere $H(\\alpha)=-\\alpha \\log_{2}(\\alpha)-(1-\\alpha)\\log_{2}(1-\\alpha)$ is the binary entropy function. Hence\n$$\nH(Y|X) = H(\\alpha).\n$$\nTherefore, the desired ratio, which equals the approximate number of jointly typical $Y^{n}$ sequences per given typical $X^{n}$, is\n$$\n2^{n H(Y|X)} = 2^{n H(\\alpha)}.\n$$\nThis expression depends only on $n$ and the crossover probability $\\alpha$, as required.",
            "answer": "$$\\boxed{2^{n H(\\alpha)}}$$"
        },
        {
            "introduction": "This final practice challenges you to explore the \"robustness\" of the jointly typical set and the precision of the Asymptotic Equipartition Property (AEP). Starting with a sequence pair that perfectly matches the source statistics, you will investigate how many symbol-pair swaps can be tolerated before the sequence pair is no longer considered typical. This quantitative analysis provides a deeper appreciation for the strict statistical constraints that define the typical set and demonstrates how different types of deviations affect marginal and joint typicality conditions .",
            "id": "1635561",
            "problem": "Consider a discrete memoryless source that generates pairs of symbols $(X,Y)$ where $X, Y \\in \\{0, 1\\}$. The joint probability mass function $p(x,y)$ is given by the following table:\n\n| $p(x,y)$ | $y=0$ | $y=1$ |\n| :---: | :---: | :---: |\n| **x=0** | 0.5 | 0.2 |\n| **x=1** | 0.1 | 0.2 |\n\nA sequence pair $(x^n, y^n)$ of length $n=100$ is generated by this source. Let the number of occurrences of the pair $(i,j)$ in the sequence be denoted by $N(i,j)$. The specific sequence pair generated is known to be perfectly representative of the source statistics, meaning that the counts of each symbol pair are exactly $N(i,j) = n \\cdot p(i,j)$ for all $i,j \\in \\{0,1\\}$.\n\nA sequence pair is defined as being in the jointly typical set $A_{\\epsilon}^{(n)}(X,Y)$ for a given tolerance $\\epsilon > 0$ if all three of the following conditions are met (using logarithms in base 2):\n1.  $\\left| -\\frac{1}{n} \\log_2 p(x^n) - H(X) \\right| \\leq \\epsilon$\n2.  $\\left| -\\frac{1}{n} \\log_2 p(y^n) - H(Y) \\right| \\leq \\epsilon$\n3.  $\\left| -\\frac{1}{n} \\log_2 p(x^n, y^n) - H(X,Y) \\right| \\leq \\epsilon$\nwhere $p(x^n) = \\prod_{i=1}^{n} p_X(x_i)$, $p(y^n) = \\prod_{i=1}^{n} p_Y(y_i)$, $p(x^n, y^n) = \\prod_{i=1}^{n} p(x_i, y_i)$, and $H(X)$, $H(Y)$, and $H(X,Y)$ are the Shannon entropies of the respective distributions.\n\nA new sequence pair $(x'^n, y'^n)$ is created by modifying $(x^n, y^n)$. Specifically, $m$ instances of the symbol pair $(0,1)$ in the original sequence are swapped to become $(1,0)$. That is, for $m$ distinct indices $k$ where $(x_k, y_k) = (0,1)$, the new pair becomes $(x'_k, y'_k) = (1,0)$. All other symbols in the sequence remain unchanged.\n\nGiven a tolerance $\\epsilon = 0.04$, determine the maximum integer number of swaps, $m_{max}$, that can be performed such that the resulting sequence pair $(x'^n, y'^n)$ remains in the jointly typical set $A_{\\epsilon}^{(n)}(X,Y)$.",
            "solution": "The source has joint pmf\n$$p(0,0)=0.5,\\quad p(0,1)=0.2,\\quad p(1,0)=0.1,\\quad p(1,1)=0.2,$$\nso the source marginals are\n$$p_{X}(0)=0.7,\\quad p_{X}(1)=0.3,\\qquad p_{Y}(0)=0.6,\\quad p_{Y}(1)=0.4.$$\nThe initial sequence has length $n=100$ with exact counts $N(i,j)=100\\,p(i,j)$.\n\nAfter swapping $m$ instances of $(0,1)$ to $(1,0)$, the new empirical joint type is\n$$q'_{00}=0.5,\\quad q'_{01}=0.2-\\frac{m}{100},\\quad q'_{10}=0.1+\\frac{m}{100},\\quad q'_{11}=0.2,$$\nso the empirical marginals become\n$$\\hat{p}'_{X}(0)=0.7-\\frac{m}{100},\\quad \\hat{p}'_{X}(1)=0.3+\\frac{m}{100},$$\n$$\\hat{p}'_{Y}(0)=0.6+\\frac{m}{100},\\quad \\hat{p}'_{Y}(1)=0.4-\\frac{m}{100}.$$\n\nFor any sequence, the three typicality criteria are equivalent to checking the deviation between the empirical entropy (based on the sequence's type) and the true entropy. Since the original sequence satisfies each with equality, it suffices to track the deviations induced by $m$ swaps.\n\n1) Joint condition. Let $L_{ij}:=-\\log_{2}p(i,j)$. The deviation from joint entropy is:\n$$ \\Delta_{\\text{joint}} = \\sum_{i,j}(q'_{ij}-p(i,j))L_{ij} $$\nOnly terms for $(0,1)$ and $(1,0)$ are non-zero:\n$$ \\Delta_{\\text{joint}}=\\left(-\\frac{m}{100}\\right)L_{01}+\\left(\\frac{m}{100}\\right)L_{10}=\\frac{m}{100}\\big(L_{10}-L_{01}\\big)=\\frac{m}{100}\\log_{2}\\frac{p(0,1)}{p(1,0)}=\\frac{m}{100}\\log_{2}2=\\frac{m}{100}.$$\nThus the joint criterion requires $|\\Delta_{\\text{joint}}| = \\frac{m}{100}\\leq \\epsilon \\Longrightarrow m\\leq 100\\,\\epsilon=4.$\n\n2) $X$-marginal condition. Define $L_{X0}:=-\\log_{2}p_X(0)$ and $L_{X1}:=-\\log_{2}p_X(1)$. The deviation from marginal entropy $H(X)$ is:\n$$\\Delta_{X}=(\\hat{p}'_{X}(0)-p_X(0))L_{X0}+(\\hat{p}'_{X}(1)-p_X(1))L_{X1}$$\nUsing $\\hat{p}'_{X}(0)-p_X(0)=-\\frac{m}{100}$ and $\\hat{p}'_{X}(1)-p_X(1)=\\frac{m}{100}$,\n$$\\Delta_{X}=\\frac{m}{100}\\big(L_{X1}-L_{X0}\\big)=\\frac{m}{100}\\log_{2}\\frac{p_X(0)}{p_X(1)}=\\frac{m}{100}\\log_{2}\\!\\left(\\frac{0.7}{0.3}\\right)=\\frac{m}{100}\\log_{2}\\!\\left(\\frac{7}{3}\\right).$$\nThus the $X$-criterion is $|\\Delta_{X}|=\\frac{m}{100}\\log_{2}\\!\\left(\\frac{7}{3}\\right)\\leq \\epsilon \\Longrightarrow m\\leq \\frac{100\\,\\epsilon}{\\log_{2}(7/3)}=\\frac{4}{\\log_{2}(7/3)} \\approx \\frac{4}{1.222} \\approx 3.27$.\nSo, we must have $m \\le 3$.\n\n3) $Y$-marginal condition. Define $L_{Y0}:=-\\log_{2}p_Y(0)$ and $L_{Y1}:=-\\log_{2}p_Y(1)$. The deviation from marginal entropy $H(Y)$ is:\n$$\\Delta_{Y}=\\left(\\hat{p}'_{Y}(0)-p_Y(0)\\right)L_{Y0}+\\left(\\hat{p}'_{Y}(1)-p_Y(1)\\right)L_{Y1}=\\frac{m}{100}\\big(L_{Y0}-L_{Y1}\\big)=\\frac{m}{100}\\log_{2}\\frac{p_Y(1)}{p_Y(0)}=\\frac{m}{100}\\log_{2}\\!\\left(\\frac{0.4}{0.6}\\right).$$\nThus $|\\Delta_{Y}|=\\frac{m}{100}\\left|\\log_{2}\\!\\left(\\frac{2}{3}\\right)\\right|=\\frac{m}{100}\\log_{2}\\!\\left(\\frac{3}{2}\\right)\\leq \\epsilon \\Longrightarrow m\\leq \\frac{100\\,\\epsilon}{\\log_{2}(3/2)}=\\frac{4}{\\log_{2}(3/2)} \\approx \\frac{4}{0.585} \\approx 6.84$.\nSo, we must have $m \\le 6$.\n\nCollecting constraints $m \\le 4$ (from joint), $m \\le 3$ (from X-marginal), and $m \\le 6$ (from Y-marginal), the most restrictive condition is $m \\le 3$. The number of available $(0,1)$ pairs is $N(0,1)=20$, so this is a feasible number of swaps.\nTherefore the maximum integer $m$ such that all three typicality conditions remain satisfied is $m_{\\text{max}}=3$.",
            "answer": "$$\\boxed{3}$$"
        }
    ]
}