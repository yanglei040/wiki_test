{
    "hands_on_practices": [
        {
            "introduction": "The Blahut-Arimoto algorithm provides a powerful iterative method for numerically approximating a channel's capacity. To truly understand how this algorithm works, it's best to start by performing the calculations by hand. This first exercise guides you through a single, complete iteration of the algorithm, from an initial input distribution $p_0(x)$ to the updated distribution $p_1(x)$. Completing this practice will solidify your understanding of the core mechanics and the interplay between the channel matrix, the input distribution, and the update rules .",
            "id": "1605109",
            "problem": "Consider a discrete memoryless channel with a binary input alphabet $\\mathcal{X} = \\{x_1, x_2\\}$ and a binary output alphabet $\\mathcal{Y} = \\{y_1, y_2\\}$. The behavior of the channel is described by the matrix of conditional probabilities $P(y_j|x_i)$, where the rows correspond to the inputs $x_i$ and the columns to the outputs $y_j$. The specific channel matrix is given by:\n$$\nP(y|x) = \\begin{pmatrix} P(y_1|x_1) & P(y_2|x_1) \\\\ P(y_1|x_2) & P(y_2|x_2) \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}\n$$\nThe Blahut-Arimoto algorithm is an iterative procedure for computing the channel capacity and the input probability distribution $p(x)$ that achieves it. A single iteration of this algorithm updates an initial probability distribution $p_k(x)$ to a new distribution $p_{k+1}(x)$. The update consists of the following steps:\n1.  Compute the reverse conditional probability distribution $q_k(x|y)$ from the current input distribution $p_k(x)$ and the channel matrix $P(y|x)$:\n    $$\n    q_k(x|y) = \\frac{p_k(x) P(y|x)}{\\sum_{x' \\in \\mathcal{X}} p_k(x') P(y|x')}\n    $$\n2.  Compute the new, un-normalized weights $c_k(x)$ for each input symbol:\n    $$\n    c_k(x) = \\prod_{y \\in \\mathcal{Y}} [q_k(x|y)]^{P(y|x)}\n    $$\n    For this calculation, use the convention that $0^0 = 1$.\n3.  Normalize the weights to obtain the updated input distribution $p_{k+1}(x)$:\n    $$\n    p_{k+1}(x) = \\frac{c_k(x)}{\\sum_{x' \\in \\mathcal{X}} c_k(x')}\n    $$\nYour task is to perform one full iteration of this algorithm. Starting with a uniform input distribution $p_0(x) = (p_0(x_1), p_0(x_2)) = (\\frac{1}{2}, \\frac{1}{2})$, calculate the updated distribution $p_1(x) = (p_1(x_1), p_1(x_2))$.\n\nProvide your final answer as an exact analytic expression in the form of a row matrix.",
            "solution": "The goal is to compute the probability distribution $p_1(x) = (p_1(x_1), p_1(x_2))$ after one iteration of the Blahut-Arimoto algorithm, starting from a uniform distribution $p_0(x_1) = 1/2$, $p_0(x_2) = 1/2$. The channel matrix is given by:\n$$\nP(y|x) = \\begin{pmatrix} 1 & 0 \\\\ \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}\n$$\n\nThe first step is to calculate the output probability distribution $p_0(y)$ corresponding to the initial input distribution $p_0(x)$.\n$$\np_0(y_1) = \\sum_{i=1}^{2} p_0(x_i) P(y_1|x_i) = p_0(x_1)P(y_1|x_1) + p_0(x_2)P(y_1|x_2)\n$$\n$$\np_0(y_1) = \\left(\\frac{1}{2}\\right)(1) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{2} + \\frac{1}{4} = \\frac{3}{4}\n$$\n$$\np_0(y_2) = \\sum_{i=1}^{2} p_0(x_i) P(y_2|x_i) = p_0(x_1)P(y_2|x_1) + p_0(x_2)P(y_2|x_2)\n$$\n$$\np_0(y_2) = \\left(\\frac{1}{2}\\right)(0) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = 0 + \\frac{1}{4} = \\frac{1}{4}\n$$\nAs a check, $p_0(y_1) + p_0(y_2) = 3/4 + 1/4 = 1$.\n\nNext, we compute the reverse conditional probabilities $q_0(x|y)$.\nFor $y=y_1$:\n$$\nq_0(x_1|y_1) = \\frac{p_0(x_1)P(y_1|x_1)}{p_0(y_1)} = \\frac{(1/2)(1)}{3/4} = \\frac{1/2}{3/4} = \\frac{2}{3}\n$$\n$$\nq_0(x_2|y_1) = \\frac{p_0(x_2)P(y_1|x_2)}{p_0(y_1)} = \\frac{(1/2)(1/2)}{3/4} = \\frac{1/4}{3/4} = \\frac{1}{3}\n$$\nCheck: $q_0(x_1|y_1) + q_0(x_2|y_1) = 2/3 + 1/3 = 1$.\n\nFor $y=y_2$:\n$$\nq_0(x_1|y_2) = \\frac{p_0(x_1)P(y_2|x_1)}{p_0(y_2)} = \\frac{(1/2)(0)}{1/4} = 0\n$$\n$$\nq_0(x_2|y_2) = \\frac{p_0(x_2)P(y_2|x_2)}{p_0(y_2)} = \\frac{(1/2)(1/2)}{1/4} = \\frac{1/4}{1/4} = 1\n$$\nCheck: $q_0(x_1|y_2) + q_0(x_2|y_2) = 0 + 1 = 1$.\n\nNow, we compute the un-normalized weights $c_0(x)$.\nFor $x=x_1$:\n$$\nc_0(x_1) = [q_0(x_1|y_1)]^{P(y_1|x_1)} \\cdot [q_0(x_1|y_2)]^{P(y_2|x_1)} = \\left(\\frac{2}{3}\\right)^1 \\cdot (0)^0\n$$\nUsing the specified convention $0^0=1$:\n$$\nc_0(x_1) = \\frac{2}{3} \\cdot 1 = \\frac{2}{3}\n$$\n\nFor $x=x_2$:\n$$\nc_0(x_2) = [q_0(x_2|y_1)]^{P(y_1|x_2)} \\cdot [q_0(x_2|y_2)]^{P(y_2|x_2)} = \\left(\\frac{1}{3}\\right)^{1/2} \\cdot (1)^{1/2}\n$$\n$$\nc_0(x_2) = \\sqrt{\\frac{1}{3}} \\cdot 1 = \\frac{1}{\\sqrt{3}}\n$$\n\nFinally, we find the new distribution $p_1(x)$ by normalizing these weights. The normalization constant is the sum of the weights:\n$$\nZ = c_0(x_1) + c_0(x_2) = \\frac{2}{3} + \\frac{1}{\\sqrt{3}} = \\frac{2}{3} + \\frac{\\sqrt{3}}{3} = \\frac{2+\\sqrt{3}}{3}\n$$\nNow we find each component of $p_1(x)$:\n$$\np_1(x_1) = \\frac{c_0(x_1)}{Z} = \\frac{2/3}{(2+\\sqrt{3})/3} = \\frac{2}{2+\\sqrt{3}}\n$$\nTo simplify, we rationalize the denominator:\n$$\np_1(x_1) = \\frac{2}{2+\\sqrt{3}} \\cdot \\frac{2-\\sqrt{3}}{2-\\sqrt{3}} = \\frac{2(2-\\sqrt{3})}{2^2 - (\\sqrt{3})^2} = \\frac{4-2\\sqrt{3}}{4-3} = 4 - 2\\sqrt{3}\n$$\n$$\np_1(x_2) = \\frac{c_0(x_2)}{Z} = \\frac{1/\\sqrt{3}}{(2+\\sqrt{3})/3} = \\frac{1}{\\sqrt{3}} \\cdot \\frac{3}{2+\\sqrt{3}} = \\frac{\\sqrt{3}}{2+\\sqrt{3}}\n$$\nRationalizing the denominator:\n$$\np_1(x_2) = \\frac{\\sqrt{3}}{2+\\sqrt{3}} \\cdot \\frac{2-\\sqrt{3}}{2-\\sqrt{3}} = \\frac{\\sqrt{3}(2-\\sqrt{3})}{4-3} = \\frac{2\\sqrt{3}-3}{1} = 2\\sqrt{3} - 3\n$$\nAs a final check, let's sum the probabilities:\n$$\np_1(x_1) + p_1(x_2) = (4 - 2\\sqrt{3}) + (2\\sqrt{3} - 3) = 4 - 3 = 1\n$$\nThe updated distribution is $p_1(x) = (4 - 2\\sqrt{3}, 2\\sqrt{3} - 3)$.\nIn the required row matrix format, this is $\\pmatrix{4 - 2\\sqrt{3} & 2\\sqrt{3} - 3}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 4 - 2\\sqrt{3} & 2\\sqrt{3} - 3 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Beyond brute-force calculation, deeper insights into the Blahut-Arimoto algorithm come from analyzing its behavior under specific conditions. This practice problem presents a channel where two different input symbols, $x_1$ and $x_2$, are statistically indistinguishable to the receiver. By examining the update equations, you can uncover a key property of the algorithm concerning such symmetries without needing to perform multiple iterations. This exercise demonstrates how structural properties of the channel matrix $P(y|x)$ can impose invariants on the evolution of the probability distribution $p_k(x)$ .",
            "id": "1605140",
            "problem": "An engineer is tasked with characterizing a discrete memoryless channel. The channel has an input alphabet $\\mathcal{X} = \\{x_1, x_2, x_3, x_4\\}$ and an output alphabet $\\mathcal{Y} = \\{y_1, y_2, y_3\\}$. The channel's behavior is described by the conditional probability matrix $P(y_j|x_i)$, where the entry in the $i$-th row and $j$-th column represents the probability of receiving output $y_j$ given that input $x_i$ was sent. The matrix is given by:\n$$\nP(y|x) = \\begin{pmatrix}\n0.5 & 0.2 & 0.3 \\\\\n0.5 & 0.2 & 0.3 \\\\\n0.1 & 0.8 & 0.1 \\\\\n0.4 & 0.3 & 0.3\n\\end{pmatrix}\n$$\nTo find the channel capacity, the engineer employs the Blahut-Arimoto algorithm. The algorithm iteratively refines an initial input probability distribution $p(x)$ to converge towards the distribution that achieves capacity. The iterative process is defined as follows:\n\n1.  Start with an initial probability distribution $p_0(x)$ where $p_0(x) > 0$ for all $x \\in \\mathcal{X}$.\n2.  For each iteration $k = 0, 1, 2, \\dots$, compute a set of auxiliary values $c_k(x)$ for each input symbol $x \\in \\mathcal{X}$ using the formula:\n    $$\n    c_k(x) = \\exp\\left(\\sum_{y \\in \\mathcal{Y}} P(y|x) \\ln\\left( \\frac{P(y|x)}{q_k(y)} \\right)\\right)\n    $$\n    where $q_k(y) = \\sum_{x' \\in \\mathcal{X}} p_k(x') P(y|x')$.\n3.  Update the input probability distribution for the next iteration, $p_{k+1}(x)$, using the formula:\n    $$\n    p_{k+1}(x) = \\frac{p_k(x) c_k(x)}{\\sum_{x'' \\in \\mathcal{X}} p_k(x'') c_k(x'')}\n    $$\n\nThe engineer starts the algorithm with the initial distribution $p_0(x_1) = 0.1$, $p_0(x_2) = 0.4$, $p_0(x_3) = 0.3$, and $p_0(x_4) = 0.2$.\n\nAfter running the algorithm for several steps, the engineer wants to analyze the evolution of the input probabilities. Calculate the value of the ratio $p_5(x_1) / p_5(x_2)$ at the end of the 5th iteration. Provide your answer as a real number rounded to three significant figures.",
            "solution": "We are given a discrete memoryless channel with input symbols $x_{1},x_{2},x_{3},x_{4}$ and rows of the conditional probability matrix satisfying $P(y|x_{1})=P(y|x_{2})=[0.5,0.2,0.3]$. The Blahut-Arimoto iteration uses\n$$\nq_{k}(y)=\\sum_{x'} p_{k}(x')P(y|x'),\\quad\nc_{k}(x)=\\exp\\left(\\sum_{y}P(y|x)\\ln\\left(\\frac{P(y|x)}{q_{k}(y)}\\right)\\right),\n$$\nand\n$$\np_{k+1}(x)=\\frac{p_{k}(x)c_{k}(x)}{\\sum_{x''}p_{k}(x'')c_{k}(x'')}.\n$$\nWe seek the ratio $p_{5}(x_{1})/p_{5}(x_{2})$. Define the ratio at iteration $k$ as $r_{k}=\\frac{p_{k}(x_{1})}{p_{k}(x_{2})}$. Using the update,\n$$\n\\frac{p_{k+1}(x_{1})}{p_{k+1}(x_{2})}=\\frac{p_{k}(x_{1})c_{k}(x_{1})}{p_{k}(x_{2})c_{k}(x_{2})}=r_{k}\\cdot\\frac{c_{k}(x_{1})}{c_{k}(x_{2})}.\n$$\nTo evaluate $\\frac{c_{k}(x_{1})}{c_{k}(x_{2})}$, note that for all $y\\in\\mathcal{Y}$ we have $P(y|x_{1})=P(y|x_{2})$. Since $q_{k}(y)$ depends only on $p_{k}$ and the channel and not on which $x$ is being evaluated inside $c_{k}(x)$, it follows that\n$$\nc_{k}(x_{1})=\\exp\\left(\\sum_{y}P(y|x_{1})\\ln\\left(\\frac{P(y|x_{1})}{q_{k}(y)}\\right)\\right)\n=\\exp\\left(\\sum_{y}P(y|x_{2})\\ln\\left(\\frac{P(y|x_{2})}{q_{k}(y)}\\right)\\right)=c_{k}(x_{2}).\n$$\nTherefore $\\frac{c_{k}(x_{1})}{c_{k}(x_{2})}=1$ and the ratio is invariant across iterations:\n$$\nr_{k+1}=r_{k}\\quad\\text{for all }k.\n$$\nWith the initial distribution $p_{0}(x_{1})=0.1$ and $p_{0}(x_{2})=0.4$, we have\n$$\nr_{0}=\\frac{p_{0}(x_{1})}{p_{0}(x_{2})}=\\frac{0.1}{0.4}=\\frac{1}{4}.\n$$\nHence for all $k$, and in particular for $k=5$,\n$$\n\\frac{p_{5}(x_{1})}{p_{5}(x_{2})}=r_{5}=r_{0}=\\frac{1}{4}=0.25.\n$$\nRounded to three significant figures, the requested ratio is $0.250$.",
            "answer": "$$\\boxed{0.250}$$"
        },
        {
            "introduction": "The convergence of the Blahut-Arimoto algorithm depends critically on its starting point. This final practice explores a crucial boundary condition: what happens if an input symbol is assigned a zero probability in the initial distribution? By analyzing the multiplicative nature of the update rule, you can determine if the algorithm can \"rediscover\" this input or if it is permanently excluded from consideration. This thought experiment reveals important aspects about the algorithm's search for the optimal distribution and the need for an initial distribution with full support .",
            "id": "1605115",
            "problem": "Consider a Discrete Memoryless Channel (DMC) with an input alphabet $\\mathcal{X} = \\{x_1, x_2, x_3\\}$ and an output alphabet $\\mathcal{Y} = \\{y_1, y_2\\}$. The channel is characterized by the following conditional probability matrix $P(Y|X)$, where the entry in row $i$ and column $j$ is $p(y_j|x_i)$:\n$$\nP(Y|X) = \n\\begin{pmatrix} \n0.8 & 0.2 \\\\\n0.5 & 0.5 \\\\\n0.1 & 0.9 \n\\end{pmatrix}\n$$\nThe Blahut-Arimoto algorithm is an iterative procedure used to find the input probability distribution $p(x)$ that achieves channel capacity. Let $p_k(x_i)$ denote the probability of input symbol $x_i$ at iteration $k$. The algorithm updates the probabilities according to the following rule:\n$$\np_{k+1}(x_i) = \\frac{p_k(x_i) \\exp\\left( \\sum_{j=1}^{2} p(y_j|x_i) \\ln \\frac{p(y_j|x_i)}{p_k(y_j)} \\right)}{Z_k}\n$$\nwhere $p_k(y_j) = \\sum_{i=1}^{3} p(y_j|x_i) p_k(x_i)$ is the probability of output symbol $y_j$ at iteration $k$, and $Z_k$ is a normalization factor that ensures $\\sum_{i=1}^{3} p_{k+1}(x_i) = 1$.\n\nSuppose the algorithm is initialized at $k=0$ with the input distribution $p_0(x) = (p_0(x_1), p_0(x_2), p_0(x_3)) = (0, 0.5, 0.5)$. Which of the following statements accurately describes the value of the probability mass $p_k(x_1)$ for any iteration $k \\geq 1$?\n\nA. $p_k(x_1)$ will remain at 0 for all $k \\geq 1$.\n\nB. $p_k(x_1)$ will become non-zero after the first iteration and will converge towards the optimal probability for $x_1$.\n\nC. The value of $p_k(x_1)$ depends on the specific non-zero values in the channel matrix and cannot be determined without running the full algorithm.\n\nD. The algorithm will encounter a division-by-zero error on the first iteration because $p_0(y_j)$ will be zero for some $j$.\n\nE. $p_k(x_1)$ will increase to a small positive value in the first iteration and then decrease back to zero in the second iteration.",
            "solution": "The Blahut-Arimoto update for the input distribution is\n$$\np_{k+1}(x_{i})=\\frac{p_{k}(x_{i})\\exp\\left(\\sum_{j=1}^{2}p(y_{j}\\mid x_{i})\\ln\\frac{p(y_{j}\\mid x_{i})}{p_{k}(y_{j})}\\right)}{Z_{k}},\n$$\nwith\n$$\np_{k}(y_{j})=\\sum_{i=1}^{3}p(y_{j}\\mid x_{i})p_{k}(x_{i}),\\quad Z_{k}=\\sum_{i=1}^{3}p_{k}(x_{i})\\exp\\left(\\sum_{j=1}^{2}p(y_{j}\\mid x_{i})\\ln\\frac{p(y_{j}\\mid x_{i})}{p_{k}(y_{j})}\\right).\n$$\nThe initialization is $p_{0}(x_{1})=0$, $p_{0}(x_{2})=\\frac{1}{2}$, $p_{0}(x_{3})=\\frac{1}{2}$, and the channel matrix entries are $p(y_{1}\\mid x_{1})=\\frac{4}{5}$, $p(y_{2}\\mid x_{1})=\\frac{1}{5}$, $p(y_{1}\\mid x_{2})=\\frac{1}{2}$, $p(y_{2}\\mid x_{2})=\\frac{1}{2}$, $p(y_{1}\\mid x_{3})=\\frac{1}{10}$, $p(y_{2}\\mid x_{3})=\\frac{9}{10}$. First compute the output distribution at $k=0$:\n$$\np_{0}(y_{1})=\\sum_{i=1}^{3}p(y_{1}\\mid x_{i})p_{0}(x_{i})=\\frac{1}{2}\\cdot\\frac{1}{2}+\\frac{1}{2}\\cdot\\frac{1}{10}=\\frac{1}{4}+\\frac{1}{20}=\\frac{3}{10},\n$$\n$$\np_{0}(y_{2})=\\sum_{i=1}^{3}p(y_{2}\\mid x_{i})p_{0}(x_{i})=\\frac{1}{2}\\cdot\\frac{1}{2}+\\frac{1}{2}\\cdot\\frac{9}{10}=\\frac{1}{4}+\\frac{9}{20}=\\frac{7}{10}.\n$$\nBoth $p_{0}(y_{1})$ and $p_{0}(y_{2})$ are strictly positive, so all logarithms in the update are finite. Then, using the update for $i=1$ at $k=0$,\n$$\np_{1}(x_{1})=\\frac{p_{0}(x_{1})\\exp\\left(\\sum_{j=1}^{2}p(y_{j}\\mid x_{1})\\ln\\frac{p(y_{j}\\mid x_{1})}{p_{0}(y_{j})}\\right)}{Z_{0}}=\\frac{0\\cdot\\exp(\\cdot)}{Z_{0}}=0,\n$$\nwhere $Z_{0}>0$ because at least $i=2,3$ contribute strictly positive terms. This shows $p_{1}(x_{1})=0$.\n\nWe now prove by induction that $p_{k}(x_{1})=0$ for all $k\\geq 1$. The base case $k=1$ holds as shown. Assume $p_{k}(x_{1})=0$ and $p_{k}(x_{2})>0$, $p_{k}(x_{3})>0$ (the latter two hold because the update multiplies positive $p_{k-1}(x_{i})$ by positive exponentials and renormalizes). Then for each output symbol,\n$$\np_{k}(y_{j})=\\sum_{i=1}^{3}p(y_{j}\\mid x_{i})p_{k}(x_{i})=\\sum_{i=2}^{3}p(y_{j}\\mid x_{i})p_{k}(x_{i})>0,\n$$\nbecause $p(y_{j}\\mid x_{2})$ and $p(y_{j}\\mid x_{3})$ are strictly positive and $p_{k}(x_{2}),p_{k}(x_{3})>0$. Therefore the exponent for $i=1$ is finite and\n$$\np_{k+1}(x_{1})=\\frac{p_{k}(x_{1})\\exp\\left(\\sum_{j=1}^{2}p(y_{j}\\mid x_{1})\\ln\\frac{p(y_{j}\\mid x_{1})}{p_{k}(y_{j})}\\right)}{Z_{k}}=\\frac{0\\cdot\\exp(\\cdot)}{Z_{k}}=0,\n$$\nwith $Z_{k}>0$ for the same reason as before. Hence, by induction, $p_{k}(x_{1})=0$ for all $k\\geq 1$.\n\nThis establishes that the zero probability on $x_{1}$ is preserved by the Blahut-Arimoto multiplicative update. There is no division-by-zero at the first iteration because $p_{0}(y_{1})=\\frac{3}{10}$ and $p_{0}(y_{2})=\\frac{7}{10}$ are strictly positive. Therefore, among the options, the accurate statement is that $p_{k}(x_{1})$ remains zero for all iterations.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}