{
    "hands_on_practices": [
        {
            "introduction": "Our first exercise explores an idealized but highly instructive scenario: a channel where the possible sets of output symbols for each distinct input are mutually exclusive. This 'Partitioned Output Channel' represents a case with zero ambiguity; observing an output symbol perfectly reveals which input symbol was transmitted. By analyzing this channel , we build a strong intuition for the fundamental connection between a channel's structure and its capacity to resolve uncertainty about the input, revealing that capacity is directly related to the number of choices at the input.",
            "id": "1618511",
            "problem": "A novel communication system, termed a \"Partitioned Output Channel,\" is designed for a specialized application. The system has a finite set of input symbols, $\\mathcal{X} = \\{x_1, x_2, \\ldots, x_K\\}$, where $K$ is the total number of distinct input symbols. The set of output symbols is denoted by $\\mathcal{Y}$.\n\nA defining characteristic of this channel is its output structure. The output alphabet $\\mathcal{Y}$ is partitioned into $K$ disjoint, non-empty subsets, $\\mathcal{Y}_1, \\mathcal{Y}_2, \\ldots, \\mathcal{Y}_K$, such that $\\mathcal{Y} = \\bigcup_{i=1}^{K} \\mathcal{Y}_i$ and $\\mathcal{Y}_i \\cap \\mathcal{Y}_j = \\emptyset$ for all $i \\neq j$.\n\nThe channel's behavior is dictated by the following rule: when an input symbol $x_i$ is transmitted, the resulting output symbol $y$ is guaranteed to belong to the corresponding subset $\\mathcal{Y}_i$. Formally, the channel's conditional probability distribution $p(y|x_i)$ satisfies $p(y|x_i) = 0$ if $y \\notin \\mathcal{Y}_i$. For any given input $x_i$, the sum of probabilities over its associated output subset is one, i.e., $\\sum_{y \\in \\mathcal{Y}_i} p(y|x_i) = 1$. The specific probabilities $p(y|x_i)$ for $y \\in \\mathcal{Y}_i$ are non-zero but are not specified further.\n\nAssuming this system is modeled as a discrete memoryless channel, determine its channel capacity, $C$. Express your answer as a symbolic expression in terms of the number of input symbols, $K$. The logarithm in your final expression must be base 2.",
            "solution": "Let $X$ be the channel input taking values in $\\mathcal{X}=\\{x_{1},\\ldots,x_{K}\\}$ and $Y$ the output taking values in $\\mathcal{Y}=\\bigcup_{i=1}^{K}\\mathcal{Y}_{i}$ with $\\mathcal{Y}_{i}\\cap\\mathcal{Y}_{j}=\\emptyset$ for $i\\neq j$. The capacity of a discrete memoryless channel is\n$$\nC=\\max_{p(x)} I(X;Y),\n$$\nwhere\n$$\nI(X;Y)=H(X)-H(X|Y).\n$$\nBy the partitioned-output property, for each $y\\in\\mathcal{Y}$ there exists a unique index $i$ such that $y\\in\\mathcal{Y}_{i}$. The channel law satisfies $p(y|x_{j})=0$ for all $j\\neq i$, while $p(y|x_{i})0$. Hence, for any prior $p(x)$,\n$$\np(x_{i}|y)=\\frac{p(y|x_{i})p(x_{i})}{\\sum_{j=1}^{K}p(y|x_{j})p(x_{j})}=\\frac{p(y|x_{i})p(x_{i})}{p(y|x_{i})p(x_{i})}=1,\n$$\nand $p(x_{j}|y)=0$ for $j\\neq i$. Therefore $H(X|Y=y)=0$ for every $y$, which implies\n$$\nH(X|Y)=\\sum_{y}p(y)H(X|Y=y)=0.\n$$\nConsequently,\n$$\nI(X;Y)=H(X).\n$$\nMaximizing mutual information reduces to maximizing the input entropy over $K$ symbols. The entropy bound gives\n$$\nH(X)\\leq \\log_{2}(K),\n$$\nwith equality achieved by the uniform input distribution $p(x_{i})=\\frac{1}{K}$ for all $i$. Hence,\n$$\nC=\\max_{p(x)}I(X;Y)=\\max_{p(x)}H(X)=\\log_{2}(K).\n$$",
            "answer": "$$\\boxed{\\log_{2}(K)}$$"
        },
        {
            "introduction": "In most real-world channels, output sets overlap, introducing noise and uncertainty. However, we can still influence the channel's output by carefully choosing our input statistics. This practice  offers a powerful geometric perspective, showing that the set of all achievable output distributions is a convex combination of the channel's conditional probability rows. We will use this insight to tackle a practical engineering task: adjusting the input probabilities to make the resulting output distribution as close as possible to a desired target, demonstrating a key aspect of channel control beyond simply maximizing capacity.",
            "id": "1618451",
            "problem": "Consider an engineer designing a communication system around a specific Discrete Memoryless Channel (DMC). The channel has a binary input alphabet, $X \\in \\{0, 1\\}$, and a ternary output alphabet, $Y \\in \\{a, b, c\\}$. The channel's behavior is characterized by the following conditional probabilities:\n\n- For input $X=0$:\n  $P(Y=a|X=0) = 0.6$\n  $P(Y=b|X=0) = 0.3$\n  $P(Y=c|X=0) = 0.1$\n\n- For input $X=1$:\n  $P(Y=a|X=1) = 0.1$\n  $P(Y=b|X=1) = 0.2$\n  $P(Y=c|X=1) = 0.7$\n\nThe engineer can control the statistical properties of the source by choosing the input probability distribution, which is defined by $P(X=1) = \\alpha$ and $P(X=0) = 1-\\alpha$, where $\\alpha$ is a parameter that can be set to any value in the range $[0, 1]$.\n\nThe goal is to select an input distribution parameter $\\alpha$ that makes the resulting output distribution $P(Y)$ as close as possible to a target uniform distribution, where $Q(Y=a) = Q(Y=b) = Q(Y=c) = 1/3$. The \"closeness\" is measured by minimizing the squared Euclidean distance between the achieved output probability vector and the target probability vector. Let the output probability vector be $\\mathbf{p} = (P(Y=a), P(Y=b), P(Y=c))$ and the target vector be $\\mathbf{q} = (1/3, 1/3, 1/3)$. The objective is to find the value of $\\alpha$ that minimizes the quantity $D^2 = \\sum_{y \\in \\{a,b,c\\}} (P(Y=y) - Q(Y=y))^2$.\n\nDetermine the optimal value of $\\alpha$ that achieves this goal. Round your final answer to four significant figures.",
            "solution": "We denote the input distribution by $P(X=1)=\\alpha$ and $P(X=0)=1-\\alpha$. The output distribution is a convex combination of the channel output conditionals:\n$$\nP(Y=y)= (1-\\alpha)P(Y=y\\mid X=0)+\\alpha P(Y=y\\mid X=1).\n$$\nUsing the given channel, in fractional form,\n$$\nP(Y=a\\mid X=0)=\\frac{3}{5},\\quad P(Y=b\\mid X=0)=\\frac{3}{10},\\quad P(Y=c\\mid X=0)=\\frac{1}{10},\n$$\n$$\nP(Y=a\\mid X=1)=\\frac{1}{10},\\quad P(Y=b\\mid X=1)=\\frac{1}{5},\\quad P(Y=c\\mid X=1)=\\frac{7}{10}.\n$$\nThus,\n$$\nP(Y=a)=\\frac{3}{5}-\\frac{1}{2}\\alpha,\\quad P(Y=b)=\\frac{3}{10}-\\frac{1}{10}\\alpha,\\quad P(Y=c)=\\frac{1}{10}+\\frac{3}{5}\\alpha.\n$$\nLet $\\mathbf{q}=(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3})$. The squared Euclidean distance is\n$$\nD^{2}(\\alpha)=\\sum_{y\\in\\{a,b,c\\}}\\left(P(Y=y)-\\frac{1}{3}\\right)^{2}.\n$$\nCompute the deviations from $\\frac{1}{3}$:\n$$\nP(Y=a)-\\frac{1}{3}=\\frac{3}{5}-\\frac{1}{3}-\\frac{1}{2}\\alpha=\\frac{4}{15}-\\frac{1}{2}\\alpha,\n$$\n$$\nP(Y=b)-\\frac{1}{3}=\\frac{3}{10}-\\frac{1}{3}-\\frac{1}{10}\\alpha=-\\frac{1}{30}-\\frac{1}{10}\\alpha,\n$$\n$$\nP(Y=c)-\\frac{1}{3}=\\frac{1}{10}-\\frac{1}{3}+\\frac{3}{5}\\alpha=-\\frac{7}{30}+\\frac{3}{5}\\alpha.\n$$\nTherefore,\n$$\nD^{2}(\\alpha)=\\left(\\frac{4}{15}-\\frac{1}{2}\\alpha\\right)^{2}+\\left(-\\frac{1}{30}-\\frac{1}{10}\\alpha\\right)^{2}+\\left(-\\frac{7}{30}+\\frac{3}{5}\\alpha\\right)^{2}.\n$$\nExpand this quadratic in $\\alpha$:\n$$\n\\left(\\frac{4}{15}-\\frac{1}{2}\\alpha\\right)^{2}=\\frac{16}{225}-\\frac{4}{15}\\alpha+\\frac{1}{4}\\alpha^{2},\n$$\n$$\n\\left(-\\frac{1}{30}-\\frac{1}{10}\\alpha\\right)^{2}=\\frac{1}{900}+\\frac{1}{150}\\alpha+\\frac{1}{100}\\alpha^{2},\n$$\n$$\n\\left(-\\frac{7}{30}+\\frac{3}{5}\\alpha\\right)^{2}=\\frac{49}{900}-\\frac{42}{150}\\alpha+\\frac{9}{25}\\alpha^{2}.\n$$\nSumming constants, linear, and quadratic terms gives\n$$\nD^{2}(\\alpha)=\\frac{19}{150}-\\frac{27}{50}\\alpha+\\frac{31}{50}\\alpha^{2}.\n$$\nSince this is a convex quadratic in $\\alpha$, the minimizer is obtained by setting the derivative to zero:\n$$\n\\frac{d}{d\\alpha}D^{2}(\\alpha)=2\\cdot\\frac{31}{50}\\alpha-\\frac{27}{50}=0 \\quad\\Rightarrow\\quad \\alpha=\\frac{27}{62}.\n$$\nThis value lies in $[0,1]$, so it is the global minimizer. Converting to a decimal and rounding to four significant figures yields $0.4355$.",
            "answer": "$$\\boxed{0.4355}$$"
        },
        {
            "introduction": "Having explored channel structure and output shaping, we now address the central goal of maximizing the rate of reliable communication. This involves finding the channel capacity, $C$, by optimizing the input distribution to maximize the mutual information, $I(X;Y)$. In this final practice , we will calculate the capacity for a common model of an asymmetric channel. This exercise will guide you through the complete analytical process, from setting up the mutual information expression to using calculus to find both the optimal input strategy and the ultimate information-theoretic speed limit of the channel, illustrating that the best input distribution is not always a uniform one.",
            "id": "1618455",
            "problem": "Consider a simplified model for an optical communication system that transmits binary data, '0's and '1's, as pulses of light. The system is designed such that a '1' is represented by a high-intensity pulse and a '0' by a low-intensity pulse. The detector is perfectly reliable at identifying high-intensity pulses, so when a '1' is sent, it is always received correctly as a '1'. However, the low-intensity pulse representing a '0' can be affected by stray photons or thermal noise in the detector. This results in a non-zero probability, denoted by $p$, that a sent '0' is incorrectly registered as a '1'. A '0' is received correctly with probability $1-p$. We assume the channel is memoryless, meaning the outcome of each bit transmission is independent of all others, and that the noise parameter $p$ is a constant such that $0  p  1$.\n\nLet the random variable $X$ represent the binary input to this channel and $Y$ represent the binary output. Your task is to determine two quantities: the channel capacity, $C$, which is the maximum possible rate of information transmission through this channel, and the specific input probability distribution that achieves this capacity. Let this optimal distribution be defined by the probability of sending a '0', denoted by $w_0 = P(X=0)$.\n\nProvide two analytic expressions in terms of the error probability $p$ and the binary entropy function, which is defined as $H_b(x) = -x \\log_2(x) - (1-x) \\log_2(1-x)$. The first expression should be for the capacity $C$, in bits per channel use, and the second for the optimal input probability $w_0$.",
            "solution": "Define the binary-input, binary-output channel by the conditional probabilities: for $X \\in \\{0,1\\}$ and $Y \\in \\{0,1\\}$,\n$$\nP(Y=1 \\mid X=1)=1, \\quad P(Y=0 \\mid X=1)=0, \\quad P(Y=1 \\mid X=0)=p, \\quad P(Y=0 \\mid X=0)=1-p,\n$$\nwith $0p1$. Let $w_{0}=P(X=0)=q$ and $w_{1}=P(X=1)=1-q$. The output distribution is\n$$\nP(Y=1)=p q+(1-q)=1-q(1-p), \\quad P(Y=0)=(1-p) q.\n$$\nHence the output entropy is\n$$\nH(Y)=H_{b}(P(Y=1))=H_{b}(1-q(1-p))=H_{b}(q(1-p)),\n$$\nusing $H_{b}(x)=H_{b}(1-x)$. The conditional entropy is\n$$\nH(Y \\mid X)=P(X=0) H(Y \\mid X=0)+P(X=1) H(Y \\mid X=1)=q\\,H_{b}(p)+ (1-q)\\cdot 0=q\\,H_{b}(p).\n$$\nTherefore the mutual information as a function of $q$ is\n$$\nI(X;Y)=H_{b}\\big((1-p) q\\big)-q\\,H_{b}(p).\n$$\nTo find the capacity, maximize $I(X;Y)$ over $q \\in [0,1]$. Differentiate with respect to $q$; using $\\frac{d}{du}H_{b}(u)=\\log_{2}\\!\\left(\\frac{1-u}{u}\\right)$ and the chain rule gives\n$$\n\\frac{d}{dq} I(X;Y)=(1-p)\\,\\log_{2}\\!\\left(\\frac{1-(1-p) q}{(1-p) q}\\right)-H_{b}(p).\n$$\nSetting the derivative to zero yields\n$$\n(1-p)\\,\\log_{2}\\!\\left(\\frac{1-(1-p) q^{\\star}}{(1-p) q^{\\star}}\\right)=H_{b}(p).\n$$\nLet $r^{\\star}=(1-p) q^{\\star}$. Then\n$$\n\\log_{2}\\!\\left(\\frac{1-r^{\\star}}{r^{\\star}}\\right)=\\frac{H_{b}(p)}{1-p}\n\\quad\\Longrightarrow\\quad\n\\frac{1-r^{\\star}}{r^{\\star}}=2^{\\frac{H_{b}(p)}{1-p}}\n\\quad\\Longrightarrow\\quad\nr^{\\star}=\\frac{1}{1+2^{\\frac{H_{b}(p)}{1-p}}}.\n$$\nThus the capacity-achieving input probability of sending a zero is\n$$\nw_{0}^{\\star}=q^{\\star}=\\frac{r^{\\star}}{1-p}=\\frac{1}{(1-p)\\left(1+2^{\\frac{H_{b}(p)}{1-p}}\\right)}.\n$$\nConcavity follows from $\\frac{d^{2}}{du^{2}}H_{b}(u)=-\\frac{1}{\\ln 2}\\left(\\frac{1}{u}+\\frac{1}{1-u}\\right)0$ and the linear change of variables $u=(1-p) q$, so this stationary point gives the unique global maximum.\n\nEvaluate the capacity at $q^{\\star}$:\n$$\nC=I(X;Y)\\big|_{q=q^{\\star}}=H_{b}(r^{\\star})-\\frac{r^{\\star}}{1-p}\\,H_{b}(p).\n$$\nLet $a=\\frac{H_{b}(p)}{1-p}$ and $A=2^{a}$. With $r^{\\star}=\\frac{1}{1+A}$, compute\n$$\nH_{b}(r^{\\star})=-\\frac{1}{1+A}\\log_{2}\\!\\frac{1}{1+A}-\\frac{A}{1+A}\\log_{2}\\!\\frac{A}{1+A}\n=\\log_{2}(1+A)-\\frac{A}{1+A}\\log_{2}A.\n$$\nSince $\\log_{2}A=a$, we have\n$$\nC=\\left[\\log_{2}(1+A)-\\frac{A}{1+A}a\\right]-\\frac{1}{1-p}\\cdot\\frac{1}{1+A}H_{b}(p)\n=\\log_{2}(1+A)-a.\n$$\nReturning to $a=\\frac{H_{b}(p)}{1-p}$ and $A=2^{a}$ gives the closed form\n$$\nC=\\log_{2}\\!\\left(1+2^{\\frac{H_{b}(p)}{1-p}}\\right)-\\frac{H_{b}(p)}{1-p}.\n$$\nThese expressions provide the channel capacity in bits per channel use and the capacity-achieving input probability of sending a zero.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\log_{2}\\!\\left(1+2^{\\frac{H_{b}(p)}{1-p}}\\right)-\\frac{H_{b}(p)}{1-p}  \\frac{1}{(1-p)\\left(1+2^{\\frac{H_{b}(p)}{1-p}}\\right)}\\end{pmatrix}}$$"
        }
    ]
}