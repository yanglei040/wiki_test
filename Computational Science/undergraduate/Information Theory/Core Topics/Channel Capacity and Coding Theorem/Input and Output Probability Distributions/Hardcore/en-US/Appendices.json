{
    "hands_on_practices": [
        {
            "introduction": "Understanding how a communication channel transforms an input signal is a foundational skill in information theory. This first exercise provides practice in the most fundamental calculation: determining the output probability distribution for a given channel and specific input statistics. By working through this problem , you will see how the law of total probability serves as the essential tool for predicting a system's output.",
            "id": "1632599",
            "problem": "A simplified model for a volatile digital memory bit considers its state to be either 0 or 1. Let the true state of the bit be a random variable $X \\in \\{0, 1\\}$, and the measured state of the bit be a random variable $Y \\in \\{0, 1\\}$. Due to thermal noise, the measurement process is imperfect. The probability of measuring a 1 when the true state is 0 is 0.1. The probability of measuring a 0 when the true state is 1 is 0.3.\n\nTwo different algorithms are run on the hardware that uses this memory bit.\n- Algorithm A stores data such that the prior probability distribution for the bit's true state is $P_A(X=0) = 0.2$ and $P_A(X=1) = 0.8$.\n- Algorithm B stores data such that the prior probability distribution for the bit's true state is $P_B(X=0) = 0.8$ and $P_B(X=1) = 0.2$.\n\nLet the resulting output probability distributions for the measured state be $P_A(Y)$ and $P_B(Y)$, respectively. These distributions are represented as pairs $(P(Y=0), P(Y=1))$. Which of the following options correctly gives the pair of output distributions $(P_A(Y), P_B(Y))$?\n\nA. $(P_A(Y), P_B(Y)) = ((0.42, 0.58), (0.78, 0.22))$\n\nB. $(P_A(Y), P_B(Y)) = ((0.78, 0.22), (0.42, 0.58))$\n\nC. $(P_A(Y), P_B(Y)) = ((0.74, 0.26), (0.86, 0.14))$\n\nD. $(P_A(Y), P_B(Y)) = ((0.58, 0.42), (0.22, 0.78))$",
            "solution": "We model the measurement noise with the conditional probabilities: $P(Y=1 \\mid X=0)=0.1$ and $P(Y=0 \\mid X=1)=0.3$. By complementarity, $P(Y=0 \\mid X=0)=1-0.1=0.9$ and $P(Y=1 \\mid X=1)=1-0.3=0.7$.\n\nUsing the law of total probability, for any prior on $X$,\n$$\nP(Y=0)=P(Y=0 \\mid X=0)P(X=0)+P(Y=0 \\mid X=1)P(X=1),\n$$\n$$\nP(Y=1)=P(Y=1 \\mid X=0)P(X=0)+P(Y=1 \\mid X=1)P(X=1)=1-P(Y=0).\n$$\n\nAlgorithm A has $P_{A}(X=0)=0.2$ and $P_{A}(X=1)=0.8$. Therefore,\n$$\nP_{A}(Y=0)=0.9 \\cdot 0.2+0.3 \\cdot 0.8=0.18+0.24=0.42,\n$$\n$$\nP_{A}(Y=1)=1-0.42=0.58.\n$$\nHence $P_{A}(Y)=(0.42, 0.58)$.\n\nAlgorithm B has $P_{B}(X=0)=0.8$ and $P_{B}(X=1)=0.2$. Therefore,\n$$\nP_{B}(Y=0)=0.9 \\cdot 0.8+0.3 \\cdot 0.2=0.72+0.06=0.78,\n$$\n$$\nP_{B}(Y=1)=1-0.78=0.22.\n$$\nHence $P_{B}(Y)=(0.78, 0.22)$.\n\nComparing with the options, this matches option A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "While predicting outputs is crucial, engineers often face the inverse problem: characterizing an unknown channel. This practice simulates a realistic scenario where you must deduce the channel's transition probabilities, $P(Y|X)$, based on experimental input-output data. This exercise  shifts your perspective from simple calculation to system identification, a key skill in diagnostics and engineering.",
            "id": "1632593",
            "problem": "An engineer is characterizing a newly developed digital communication channel. The channel is known to be a memoryless binary channel, meaning its input alphabet is $\\mathcal{X} = \\{0, 1\\}$ and its output alphabet is $\\mathcal{Y} = \\{0, 1\\}$. The behavior of the channel is described by a transition probability matrix $C$, where the element in row $i$ and column $j$ (0-indexed) is $C_{ij} = P(Y=i|X=j)$. Thus, the first column corresponds to input $X=0$ and the second to input $X=1$, while the first row corresponds to output $Y=0$ and the second to output $Y=1$.\n\nThe engineer performs two experiments to determine this matrix.\n\nIn the first experiment, a uniformly random input signal is used. The input probability distribution is given by the vector $[P(X=0), P(X=1)] = [0.5, 0.5]$. The resulting output probability distribution is measured to be $[P(Y=0), P(Y=1)] = [0.5, 0.5]$.\n\nIn the second experiment, a deterministic input signal is used, where the input is always '0'. This corresponds to an input probability distribution of $[P(X=0), P(X=1)] = [1, 0]$. The measured output distribution for this case is $[P(Y=0), P(Y=1)] = [0.8, 0.2]$.\n\nBased on the results of these two experiments, determine the $2 \\times 2$ channel transition probability matrix $C$.",
            "solution": "Let the channel transition matrix be $C=\\begin{pmatrix}C_{00}  C_{01} \\\\ C_{10}  C_{11}\\end{pmatrix}$, where $C_{ij}=P(Y=i\\mid X=j)$. For each input $j\\in\\{0,1\\}$, probabilities must sum to one:\n$$\nC_{00}+C_{10}=1,\\quad C_{01}+C_{11}=1.\n$$\nFrom the second experiment with deterministic input $X=0$, the output distribution equals the first column of $C$:\n$$\n\\begin{pmatrix}P(Y=0) \\\\ P(Y=1)\\end{pmatrix}=\\begin{pmatrix}0.8 \\\\ 0.2\\end{pmatrix}=\\begin{pmatrix}C_{00} \\\\ C_{10}\\end{pmatrix}.\n$$\nHence $C_{00}=0.8$ and $C_{10}=0.2$.\n\nFrom the first experiment with $P(X=0)=P(X=1)=\\frac{1}{2}$ and measured $P(Y=0)=\\frac{1}{2}$, use the law of total probability:\n$$\nP(Y=0)=\\sum_{j=0}^{1}P(Y=0\\mid X=j)P(X=j)=\\tfrac{1}{2}C_{00}+\\tfrac{1}{2}C_{01}=\\tfrac{1}{2}.\n$$\nThus\n$$\nC_{00}+C_{01}=1 \\implies C_{01}=1-C_{00}=1-0.8=0.2.\n$$\nBy column normalization for $j=1$,\n$$\nC_{01}+C_{11}=1 \\implies C_{11}=1-C_{01}=1-0.2=0.8.\n$$\nTherefore,\n$$\nC=\\begin{pmatrix}0.8  0.2 \\\\ 0.2  0.8\\end{pmatrix}.\n$$\nA quick check with the uniform input gives $P(Y=0)=\\tfrac{1}{2}(0.8+0.2)=\\tfrac{1}{2}$ and $P(Y=1)=\\tfrac{1}{2}(0.2+0.8)=\\tfrac{1}{2}$, consistent with the measurements.",
            "answer": "$$\\boxed{\\begin{pmatrix}0.8  0.2 \\\\ 0.2  0.8\\end{pmatrix}}$$"
        },
        {
            "introduction": "Moving beyond analysis, the ultimate goal is often designâ€”controlling a system to achieve a desired outcome. This advanced problem challenges you to work backward, finding an input distribution that produces a specific target output, all while satisfying certain constraints. This introduces you to the powerful concept of constrained optimization , where you must navigate a space of possible solutions to find one that meets your design goals.",
            "id": "1632614",
            "problem": "An engineer is testing a noisy communication channel with a three-symbol input alphabet, $X = \\{x_1, x_2, x_3\\}$, and a three-symbol output alphabet, $Y = \\{y_1, y_2, y_3\\}$. The channel's characteristics are described by the set of conditional probabilities $P(y_j|x_i)$, which is the probability of receiving symbol $y_j$ given that symbol $x_i$ was sent. These probabilities are empirically determined to be:\n\n*   For input $x_1$: $P(y_1|x_1)=0.7$, $P(y_2|x_1)=0.2$, $P(y_3|x_1)=0.1$.\n*   For input $x_2$: $P(y_1|x_2)=0.1$, $P(y_2|x_2)=0.6$, $P(y_3|x_2)=0.3$.\n*   For input $x_3$: $P(y_1|x_3)=0.22$, $P(y_2|x_3)=0.52$, $P(y_3|x_3)=0.26$.\n\nThe engineer wants to find an input probability distribution, $P(X) = (P(X=x_1), P(X=x_2), P(X=x_3))$, that results in a specific target output distribution, $P(Y) = (P(Y=y_1), P(Y=y_2), P(Y=y_3))$, where $P(Y=y_1)=0.4$, $P(Y=y_2)=0.4$, and $P(Y=y_3)=0.2$.\n\nAmong all valid input distributions that produce this target output distribution, what is the maximum possible value for the probability of the first input symbol, $P(X=x_1)$? Your answer should be a single real number, expressed as a fraction or a decimal.",
            "solution": "Let the input probability distribution be denoted by the row vector $\\mathbf{p} = [p_1, p_2, p_3]$, where $p_i = P(X=x_i)$. Let the output probability distribution be the row vector $\\mathbf{r} = [r_1, r_2, r_3]$, where $r_j = P(Y=y_j)$. The channel is described by the transition matrix $Q$, where $Q_{ij} = P(y_j|x_i)$.\n\nThe given values for the channel matrix $Q$ are:\n$$\nQ = \\begin{pmatrix}\n0.7  0.2  0.1 \\\\\n0.1  0.6  0.3 \\\\\n0.22  0.52  0.26\n\\end{pmatrix}\n$$\nThe target output distribution is $\\mathbf{r} = [0.4, 0.4, 0.2]$.\n\nThe relationship between the input distribution, output distribution, and the channel matrix is given by the law of total probability: $P(y_j) = \\sum_{i=1}^{3} P(y_j|x_i) P(x_i)$. In matrix form, this is $\\mathbf{p} Q = \\mathbf{r}$.\n\nWriting this out component-wise gives a system of linear equations:\n1.  $p_1 P(y_1|x_1) + p_2 P(y_1|x_2) + p_3 P(y_1|x_3) = r_1 \\implies 0.7 p_1 + 0.1 p_2 + 0.22 p_3 = 0.4$\n2.  $p_1 P(y_2|x_1) + p_2 P(y_2|x_2) + p_3 P(y_2|x_3) = r_2 \\implies 0.2 p_1 + 0.6 p_2 + 0.52 p_3 = 0.4$\n3.  $p_1 P(y_3|x_1) + p_2 P(y_3|x_2) + p_3 P(y_3|x_3) = r_3 \\implies 0.1 p_1 + 0.3 p_2 + 0.26 p_3 = 0.2$\n\nAdditionally, since $\\mathbf{p}$ is a probability distribution, its components must sum to 1:\n4.  $p_1 + p_2 + p_3 = 1$\n\nAnd each component must be non-negative:\n5.  $p_1 \\ge 0$, $p_2 \\ge 0$, $p_3 \\ge 0$\n\nWe have a system of equations for $p_1, p_2, p_3$. Notice that the rows of the channel matrix $Q$ are linearly dependent. Specifically, row 3 is a linear combination of rows 1 and 2: $0.2 \\times \\text{row}_1 + 0.8 \\times \\text{row}_2 = [0.14, 0.04, 0.02] + [0.08, 0.48, 0.24] = [0.22, 0.52, 0.26] = \\text{row}_3$. This implies that the system of equations (1), (2), (3) is redundant. We only need to use two of the first three equations along with equation (4).\n\nLet's use equations (1), (2), and (4). From (4), we can express $p_3$ as $p_3 = 1 - p_1 - p_2$. Substitute this into equation (1):\n$0.7 p_1 + 0.1 p_2 + 0.22 (1 - p_1 - p_2) = 0.4$\n$0.7 p_1 + 0.1 p_2 + 0.22 - 0.22 p_1 - 0.22 p_2 = 0.4$\n$0.48 p_1 - 0.12 p_2 = 0.18$\nDividing by $0.12$, we get:\n$4 p_1 - p_2 = 1.5 \\implies p_2 = 4 p_1 - 1.5$\n\nNow, substitute $p_3 = 1 - p_1 - p_2$ into equation (2):\n$0.2 p_1 + 0.6 p_2 + 0.52 (1 - p_1 - p_2) = 0.4$\n$0.2 p_1 + 0.6 p_2 + 0.52 - 0.52 p_1 - 0.52 p_2 = 0.4$\n$-0.32 p_1 + 0.08 p_2 = -0.12$\nDividing by $0.08$, we get:\n$-4 p_1 + p_2 = -1.5 \\implies p_2 = 4 p_1 - 1.5$\n\nBoth equations yield the same relationship between $p_1$ and $p_2$, confirming the redundancy. We now have a family of solutions where the probabilities are parameterized by $p_1$:\n$p_2 = 4 p_1 - 1.5$\nTo find $p_3$, we substitute this back into the expression for $p_3$:\n$p_3 = 1 - p_1 - p_2 = 1 - p_1 - (4 p_1 - 1.5) = 1 - 5p_1 + 1.5 = 2.5 - 5p_1$\n\nNow we must apply the non-negativity constraints from (5):\na) $p_1 \\ge 0$\nb) $p_2 = 4 p_1 - 1.5 \\ge 0 \\implies 4 p_1 \\ge 1.5 \\implies p_1 \\ge \\frac{1.5}{4} = 0.375$\nc) $p_3 = 2.5 - 5 p_1 \\ge 0 \\implies 2.5 \\ge 5 p_1 \\implies p_1 \\le \\frac{2.5}{5} = 0.5$\n\nCombining these constraints, the set of all possible values for $p_1$ is the interval $[0.375, 0.5]$.\nThe problem asks for the maximum possible value of $p_1 = P(X=x_1)$. Looking at the valid range for $p_1$, the maximum value is $0.5$.\n\nAs a check, if $p_1 = 0.5$:\n$p_2 = 4(0.5) - 1.5 = 2 - 1.5 = 0.5$\n$p_3 = 2.5 - 5(0.5) = 2.5 - 2.5 = 0$\nThe input distribution would be $\\mathbf{p} = [0.5, 0.5, 0]$. This is a valid probability distribution as all components are non-negative and they sum to 1.\nThe resulting output is $\\mathbf{p}Q = [0.5, 0.5, 0] \\begin{pmatrix} 0.7  0.2  0.1 \\\\ 0.1  0.6  0.3 \\\\ 0.22  0.52  0.26 \\end{pmatrix} = [0.5(0.7)+0.5(0.1), 0.5(0.2)+0.5(0.6), 0.5(0.1)+0.5(0.3)] = [0.35+0.05, 0.1+0.3, 0.05+0.15] = [0.4, 0.4, 0.2]$, which matches the target output distribution $\\mathbf{r}$.\n\nThe maximum value for $P(X=x_1)$ is $0.5$.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        }
    ]
}