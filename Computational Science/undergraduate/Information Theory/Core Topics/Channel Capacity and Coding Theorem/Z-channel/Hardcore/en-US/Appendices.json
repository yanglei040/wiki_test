{
    "hands_on_practices": [
        {
            "introduction": "To begin our practical exploration of the Z-channel, we must first master its fundamental mechanics. This initial problem focuses on calculating an output probability based on a given input distribution and the channel's defining crossover probability. By applying the law of total probability, you will build a foundational understanding of how the channel's asymmetry influences the received signal, a crucial first step before tackling more complex concepts like capacity or decoding .",
            "id": "1669151",
            "problem": "A rudimentary digital sensor is designed to monitor a binary state, which can be either 'off' (represented by $0$) or 'on' (represented by $1$). The sensor's transmission system can be modeled as a Z-channel. A key characteristic of this channel is its asymmetry: if the sensor sends a bit $1$, it is always received correctly as $1$. However, if the sensor sends a bit $0$, there is a crossover probability $p$ that it will be incorrectly received as a $1$. Consequently, a sent $0$ is correctly received as a $0$ with probability $1-p$.\n\nThe system being monitored is more frequently in the 'on' state than 'off'. The source distribution for the input bit $X$ is given by $P(X=0) = \\frac{1}{3}$. The channel is known to have a crossover probability of $p = \\frac{1}{6}$.\n\nLet $Y$ represent the received bit at the output. Calculate the total probability of receiving a $0$ at the output, $P(Y=0)$. Express your answer as a common fraction.",
            "solution": "Let $X$ be the random variable representing the input bit sent by the sensor, and $Y$ be the random variable for the bit received at the output. The problem asks for the probability $P(Y=0)$.\n\nWe can calculate this using the law of total probability, which allows us to find the probability of an event by considering all possible scenarios for the input. In this binary case, the input $X$ can be either 0 or 1. The law of total probability is expressed as:\n$$P(Y=0) = P(Y=0|X=0)P(X=0) + P(Y=0|X=1)P(X=1)$$\n\nFirst, we identify the probabilities given in the problem statement.\nThe probability of the source sending a $0$ is:\n$$P(X=0) = \\frac{1}{3}$$\n\nSince the input is binary, the probability of the source sending a $1$ is:\n$$P(X=1) = 1 - P(X=0) = 1 - \\frac{1}{3} = \\frac{2}{3}$$\n\nNext, we need the conditional probabilities (the channel transition probabilities) which describe the behavior of the Z-channel.\nThe problem states that if a $0$ is sent, it is correctly received as a $0$ with probability $1-p$. This corresponds to the conditional probability:\n$$P(Y=0|X=0) = 1-p$$\nGiven the crossover probability $p = \\frac{1}{6}$, we have:\n$$P(Y=0|X=0) = 1 - \\frac{1}{6} = \\frac{5}{6}$$\n\nThe problem also states that if a $1$ is sent, it is always received correctly as a $1$. This means that a sent $1$ is never received as a $0$. This gives us the other required conditional probability:\n$$P(Y=0|X=1) = 0$$\n\nNow we substitute all these values back into the law of total probability equation:\n$$P(Y=0) = P(Y=0|X=0)P(X=0) + P(Y=0|X=1)P(X=1)$$\n$$P(Y=0) = \\left(\\frac{5}{6}\\right) \\left(\\frac{1}{3}\\right) + (0) \\left(\\frac{2}{3}\\right)$$\n\nPerforming the multiplication:\n$$P(Y=0) = \\frac{5 \\times 1}{6 \\times 3} + 0$$\n$$P(Y=0) = \\frac{5}{18}$$\n\nThus, the total probability of receiving a $0$ at the output is $\\frac{5}{18}$.",
            "answer": "$$\\boxed{\\frac{5}{18}}$$"
        },
        {
            "introduction": "Having understood the forward behavior of the Z-channel, we now turn to the inverse problem: inferring the input from a given output. This exercise introduces the Maximum A Posteriori (MAP) decoder, a powerful and intuitive strategy for making the best possible guess about the transmitted signal. You will determine the MAP decision rule for a specific Z-channel and calculate the average probability of error, a critical metric for evaluating the reliability of any communication system .",
            "id": "1669144",
            "problem": "A simplified model for a certain type of optical data storage involves reading binary bits, $0$ or $1$. Due to imperfections in the reading mechanism, errors can occur. The process is modeled as a Z-channel. For this channel, a stored $0$ is always read correctly as $0$. However, a stored $1$ is sometimes misread as a $0$. The probability that a stored $1$ is misread as a $0$ is $p$. A $0$ is never misread as a $1$.\n\nAssume the data was stored with $0$s and $1$s occurring with equal probability. A Maximum A Posteriori (MAP) decoder is employed to infer the original stored bit based on the bit that was read. The MAP decoder's strategy is to choose the stored bit that has the highest probability of having been the original, given the bit that was read.\n\nGiven that the error probability for reading a $1$ is $p = 1/3$, calculate the average probability of error for this decoding process. Express your answer as a fraction.",
            "solution": "Let the stored bit be $X \\in \\{0,1\\}$ with $P(X=0)=P(X=1)=\\frac{1}{2}$, and the read bit be $Y \\in \\{0,1\\}$. The Z-channel is defined by\n$$\nP(Y=0 \\mid X=0)=1,\\quad P(Y=1 \\mid X=0)=0,\\quad P(Y=0 \\mid X=1)=p,\\quad P(Y=1 \\mid X=1)=1-p.\n$$\nA MAP decoder chooses $\\hat{x}(y)=\\arg\\max_{x\\in\\{0,1\\}} P(X=x \\mid Y=y)$. By Bayes' rule,\n$$\nP(X=x \\mid Y=y) = \\frac{P(Y=y \\mid X=x)P(X=x)}{P(Y=y)} \\propto P(Y=y \\mid X=x)P(X=x).\n$$\nWith equal priors $P(X=0)=P(X=1)=\\frac{1}{2}$, the MAP rule reduces to maximum likelihood: choose the $x$ that maximizes $P(Y=y \\mid X=x)$.\n\nDecision for $y=1$: compare $P(Y=1 \\mid X=0)=0$ and $P(Y=1 \\mid X=1)=1-p$. Since $1-p0$ for $p1$, choose $\\hat{x}(1)=1$.\n\nDecision for $y=0$: compare $P(Y=0 \\mid X=0)=1$ and $P(Y=0 \\mid X=1)=p$. Since $1 \\geq p$, choose $\\hat{x}(0)=0$.\n\nThus the decoder is $\\hat{x}(Y)=Y$. The average probability of error is\n$$\nP_{e}=\\sum_{x\\in\\{0,1\\}}\\sum_{y\\in\\{0,1\\}} P(X=x)P(Y=y \\mid X=x)\\,\\mathbf{1}\\{\\hat{x}(y)\\neq x\\}.\n$$\nOnly the case $x=1$, $y=0$ contributes, because $\\hat{x}(0)=0\\neq 1$ and all other pairs are correct. Therefore,\n$$\nP_{e}=P(X=1)P(Y=0 \\mid X=1)=\\frac{1}{2}\\,p.\n$$\nSubstituting $p=\\frac{1}{3}$ gives\n$$\nP_{e}=\\frac{1}{2}\\cdot \\frac{1}{3}=\\frac{1}{6}.\n$$",
            "answer": "$$\\boxed{\\frac{1}{6}}$$"
        },
        {
            "introduction": "The ultimate goal of channel analysis is often to determine its capacity, the maximum rate of reliable communication. This is achieved by finding the optimal input probability distribution that maximizes the mutual information between input and output. This problem provides a hands-on look at the Arimoto-Blahut algorithm, a numerical method for finding this capacity-achieving distribution. By performing a single iteration, you will gain concrete insight into how this optimization process works to push a communication system toward its theoretical limit .",
            "id": "1669124",
            "problem": "A simple optical communication link is designed to transmit binary data. An input bit $X=0$ is encoded as 'no light pulse', and an input bit $X=1$ is encoded as a 'light pulse'. The receiver's detector for 'no light pulse' is perfect, so an input $X=0$ is always correctly identified as an output $Y=0$. However, the light pulse detector is imperfect. When a light pulse is sent (input $X=1$), it is sometimes missed and incorrectly registered as 'no light pulse' (output $Y=0$) with a crossover probability $p$, where $0  p  1$. This communication system is a model for a Z-channel.\n\nTo optimize the channel for maximum information throughput, we can use the Arimoto-Blahut algorithm, which iteratively refines an input probability distribution $P(X)$ to approach the one that achieves channel capacity. The update rule for an iteration from step $k$ to $k+1$ is given by:\n$$\nP_{k+1}(x) = \\frac{P_k(x) \\exp\\left( \\sum_{y \\in \\mathcal{Y}} P(y|x) \\ln \\frac{P(y|x)}{P_k(y)} \\right)}{Z_{k+1}}\n$$\nwhere $P_k(y) = \\sum_{x' \\in \\mathcal{X}} P_k(x') P(y|x')$ is the output distribution at step $k$, and $Z_{k+1}$ is a normalization constant. The alphabets are $\\mathcal{X} = \\{0, 1\\}$ and $\\mathcal{Y} = \\{0, 1\\}$.\n\nAssume the initial guess for the input distribution is uniform, given by $P_0(X=0) = 1/2$ and $P_0(X=1) = 1/2$. Perform a single iteration of the Arimoto-Blahut algorithm to find the updated input distribution, $P_1(X)$.\n\nExpress your answer as a row vector $(P_1(X=0), P_1(X=1))$ in terms of the crossover probability $p$.",
            "solution": "The problem asks for the updated input distribution $P_1(X)$ after one iteration of the Arimoto-Blahut algorithm, starting from a uniform distribution $P_0(X)$, for a Z-channel with crossover probability $p$.\n\nFirst, let's establish the channel transition probability matrix $P(y|x)$.\nFor the Z-channel described:\n- If $X=0$, then $Y=0$ with certainty. So, $P(Y=0|X=0) = 1$ and $P(Y=1|X=0) = 0$.\n- If $X=1$, it is mistaken for $Y=0$ with probability $p$. So, $P(Y=0|X=1) = p$ and $P(Y=1|X=1) = 1-p$.\n\nThe transition matrix is:\n$$\nP(Y|X) = \\begin{pmatrix} P(Y=0|X=0)  P(Y=1|X=0) \\\\ P(Y=0|X=1)  P(Y=1|X=1) \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ p  1-p \\end{pmatrix}\n$$\n\nThe initial input distribution is uniform: $P_0(X=0) = 1/2$ and $P_0(X=1) = 1/2$.\n\nThe Arimoto-Blahut iteration consists of several steps:\n\n**Step 1: Calculate the initial output distribution $P_0(y)$.**\nThe output distribution $P_0(y)$ is calculated using the law of total probability: $P_0(y) = \\sum_{x \\in \\mathcal{X}} P_0(x) P(y|x)$.\nFor $y=0$:\n$$\nP_0(Y=0) = P_0(X=0)P(Y=0|X=0) + P_0(X=1)P(Y=0|X=1) = \\left(\\frac{1}{2}\\right)(1) + \\left(\\frac{1}{2}\\right)(p) = \\frac{1+p}{2}\n$$\nFor $y=1$:\n$$\nP_0(Y=1) = P_0(X=0)P(Y=1|X=0) + P_0(X=1)P(Y=1|X=1) = \\left(\\frac{1}{2}\\right)(0) + \\left(\\frac{1}{2}\\right)(1-p) = \\frac{1-p}{2}\n$$\nAs a check, $P_0(Y=0) + P_0(Y=1) = \\frac{1+p}{2} + \\frac{1-p}{2} = \\frac{2}{2} = 1$.\n\n**Step 2: Calculate the un-normalized update terms.**\nThe update rule is $P_{1}(x) \\propto P_0(x) c_x$, where $c_x = \\exp\\left( \\sum_{y} P(y|x) \\ln \\frac{P(y|x)}{P_0(y)} \\right)$. We need to calculate $c_0$ for $x=0$ and $c_1$ for $x=1$.\n\nFor $x=0$:\nThe term in the exponent is the Kullback-Leibler divergence $D(P(Y|X=0) \\| P_0(Y))$.\n$$\n\\ln(c_0) = \\sum_{y \\in \\{0,1\\}} P(y|0) \\ln \\frac{P(y|0)}{P_0(y)} = P(Y=0|X=0) \\ln \\frac{P(Y=0|X=0)}{P_0(Y=0)} + P(Y=1|X=0) \\ln \\frac{P(Y=1|X=0)}{P_0(Y=1)}\n$$\nUsing the convention that $0 \\ln 0 = 0$:\n$$\n\\ln(c_0) = (1) \\ln\\left(\\frac{1}{(1+p)/2}\\right) + (0) \\ln\\left(\\frac{0}{(1-p)/2}\\right) = \\ln\\left(\\frac{2}{1+p}\\right)\n$$\nThus, $c_0 = \\exp\\left(\\ln\\left(\\frac{2}{1+p}\\right)\\right) = \\frac{2}{1+p}$.\n\nFor $x=1$:\nThe term in the exponent is $D(P(Y|X=1) \\| P_0(Y))$.\n$$\n\\ln(c_1) = \\sum_{y \\in \\{0,1\\}} P(y|1) \\ln \\frac{P(y|1)}{P_0(y)} = P(Y=0|X=1) \\ln \\frac{P(Y=0|X=1)}{P_0(Y=0)} + P(Y=1|X=1) \\ln \\frac{P(Y=1|X=1)}{P_0(Y=1)}\n$$\n$$\n\\ln(c_1) = p \\ln\\left(\\frac{p}{(1+p)/2}\\right) + (1-p) \\ln\\left(\\frac{1-p}{(1-p)/2}\\right) = p \\ln\\left(\\frac{2p}{1+p}\\right) + (1-p) \\ln(2)\n$$\nUsing logarithm properties:\n$$\n\\ln(c_1) = p(\\ln(2p) - \\ln(1+p)) + (1-p)\\ln(2) = p(\\ln(2) + \\ln(p) - \\ln(1+p)) + \\ln(2) - p\\ln(2)\n$$\n$$\n\\ln(c_1) = p\\ln(2) + p\\ln(p) - p\\ln(1+p) + \\ln(2) - p\\ln(2) = \\ln(2) + p\\ln(p) - p\\ln(1+p)\n$$\n$$\n\\ln(c_1) = \\ln(2) + p\\ln\\left(\\frac{p}{1+p}\\right) = \\ln(2) + \\ln\\left(\\left(\\frac{p}{1+p}\\right)^p\\right) = \\ln\\left(2 \\left(\\frac{p}{1+p}\\right)^p\\right)\n$$\nThus, $c_1 = 2 \\left(\\frac{p}{1+p}\\right)^p$.\n\n**Step 3: Calculate the updated, un-normalized probabilities.**\nLet $P_1'(x) = P_0(x) c_x$.\nFor $x=0$:\n$$\nP_1'(0) = P_0(0) c_0 = \\frac{1}{2} \\cdot \\frac{2}{1+p} = \\frac{1}{1+p}\n$$\nFor $x=1$:\n$$\nP_1'(1) = P_0(1) c_1 = \\frac{1}{2} \\cdot 2 \\left(\\frac{p}{1+p}\\right)^p = \\left(\\frac{p}{1+p}\\right)^p\n$$\n\n**Step 4: Normalize to find the final distribution $P_1(x)$.**\nThe normalization constant is $Z_1 = P_1'(0) + P_1'(1)$.\n$$\nZ_1 = \\frac{1}{1+p} + \\left(\\frac{p}{1+p}\\right)^p = \\frac{1}{1+p} + \\frac{p^p}{(1+p)^p}\n$$\nNow we find the final probabilities $P_1(x) = P_1'(x) / Z_1$.\n$$\nP_1(X=0) = \\frac{P_1'(0)}{Z_1} = \\frac{\\frac{1}{1+p}}{\\frac{1}{1+p} + \\frac{p^p}{(1+p)^p}}\n$$\nTo simplify, multiply the numerator and the denominator by $(1+p)$:\n$$\nP_1(X=0) = \\frac{1}{1 + (1+p) \\frac{p^p}{(1+p)^p}} = \\frac{1}{1 + p^p (1+p)^{1-p}}\n$$\nFor $P_1(X=1)$:\n$$\nP_1(X=1) = \\frac{P_1'(1)}{Z_1} = \\frac{\\left(\\frac{p}{1+p}\\right)^p}{\\frac{1}{1+p} + \\frac{p^p}{(1+p)^p}}\n$$\nSimilarly, multiply the numerator and the denominator by $(1+p)$:\n$$\nP_1(X=1) = \\frac{(1+p) \\left(\\frac{p}{1+p}\\right)^p}{1 + (1+p) \\frac{p^p}{(1+p)^p}} = \\frac{(1+p) \\frac{p^p}{(1+p)^p}}{1 + p^p (1+p)^{1-p}} = \\frac{p^p (1+p)^{1-p}}{1 + p^p (1+p)^{1-p}}\n$$\n\nThe updated input distribution is the vector $(P_1(X=0), P_1(X=1))$.\n$$\nP_1(X) = \\left( \\frac{1}{1 + p^p (1+p)^{1-p}}, \\frac{p^p (1+p)^{1-p}}{1 + p^p (1+p)^{1-p}} \\right)\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{1 + p^{p} (1+p)^{1-p}}  \\frac{p^{p} (1+p)^{1-p}}{1 + p^{p} (1+p)^{1-p}} \\end{pmatrix}}$$"
        }
    ]
}