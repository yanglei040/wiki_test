{
    "hands_on_practices": [
        {
            "introduction": "The true test of understanding an algorithm is applying it from start to finish. This first practice problem challenges you to do just that by constructing an optimal prefix code for a given source distribution. By working through the Huffman procedure and calculating the resulting expected codeword length, you will solidify your grasp of the fundamental mechanics of this elegant compression technique. ",
            "id": "1623278",
            "problem": "A deep-space probe is designed to monitor and classify five distinct types of cosmic ray events, labeled A, B, C, D, and E. The probe operates autonomously and, due to power limitations for its transmitter, must encode the classification of each detected event into a sequence of binary digits (bits) using the most efficient scheme possible. From extensive prior observations, the long-term probabilities for the detection of each event type within a standard observation window have been determined. Events A and B each occur with a probability of $1/3$. Events C, D, and E each occur with a probability of $1/9$.\n\nAssuming the goal is to minimize the average number of bits transmitted per event, what is this minimum possible average length? Express your answer as a single fraction.",
            "solution": "We model the problem as constructing a binary prefix code to minimize the expected codeword length. Let the symbol probabilities be $p_{A}=p_{B}=\\frac{1}{3}$ and $p_{C}=p_{D}=p_{E}=\\frac{1}{9}$. For optimality among binary prefix codes, we apply Huffman coding.\n\nStep-by-step Huffman merges using the smallest probabilities:\n1. Merge two $\\frac{1}{9}$ symbols to form a node of weight $\\frac{2}{9}$. This increases the codeword lengths of those two symbols by $1$.\n2. Merge $\\frac{2}{9}$ with the remaining $\\frac{1}{9}$ to form a node of weight $\\frac{3}{9}=\\frac{1}{3}$. This increases the codeword lengths of the three $\\frac{1}{9}$ symbols by $1$ (cumulative so far: the first two have increased by $2$, the last by $1$).\n3. Now the multiset of weights is $\\left\\{\\frac{1}{3},\\frac{1}{3},\\frac{1}{3}\\right\\}$. Merge any two $\\frac{1}{3}$ to get a node of weight $\\frac{2}{3}$. This increases the codeword lengths of those two symbols by $1$.\n4. Merge $\\frac{2}{3}$ with the remaining $\\frac{1}{3}$ to form the root. This increases the codeword lengths of all three symbols under these nodes by $1$.\n\nTracing depths (codeword lengths):\n- One possible resulting set of lengths is $\\{l_A, l_B, l_C, l_D, l_E\\} = \\{2, 2, 2, 3, 3\\}$. For example, symbols with probability $1/3$ and one symbol with probability $1/9$ get length 2, and the other two symbols with probability $1/9$ get length 3.\n\nThus the expected codeword length is\n$$\nL=\\frac{1}{3}\\cdot l_{A}+\\frac{1}{3}\\cdot l_{B}+\\frac{1}{9}\\cdot l_{C}+\\frac{1}{9}\\cdot l_{D}+\\frac{1}{9}\\cdot l_{E}\n= \\frac{1}{3}\\cdot 2+\\frac{1}{3}\\cdot 2+\\frac{1}{9}\\cdot 3+\\frac{1}{9}\\cdot 3+\\frac{1}{9}\\cdot 2.\n$$\nCompute:\n$$\nL=\\frac{2}{3}+\\frac{2}{3}+\\frac{3}{9}+\\frac{3}{9}+\\frac{2}{9}=\\frac{4}{3}+\\frac{8}{9}=\\frac{12}{9}+\\frac{8}{9}=\\frac{20}{9}.\n$$\nBy the optimality of Huffman coding, this is the minimum possible average number of bits per event. As a consistency check, the source entropy in bits is\n$$\nH=-2\\cdot \\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)-3\\cdot \\frac{1}{9}\\log_{2}\\!\\left(\\frac{1}{9}\\right)=\\frac{4}{3}\\log_{2}(3),\n$$\nand indeed $H \\leq \\frac{20}{9}$, consistent with the coding bound.",
            "answer": "$$\\boxed{\\frac{20}{9}}$$"
        },
        {
            "introduction": "A key insight into Huffman coding is that while the algorithm is guaranteed to produce an optimal code, the coding tree itself is not always unique. This exercise explores this concept by presenting a scenario with ties in probabilities, which allows for multiple valid merging choices. By calculating the expected length, you will demonstrate for yourself that different valid Huffman trees can exist for the same source, yet all achieve the identical, optimal average codeword length. ",
            "id": "1644345",
            "problem": "A discrete memoryless source emits symbols from an alphabet $\\mathcal{S} = \\{A, B, C, D, E\\}$ with the following probabilities:\n- $P(A) = \\frac{2}{5}$\n- $P(B) = \\frac{1}{5}$\n- $P(C) = \\frac{1}{5}$\n- $P(D) = \\frac{1}{10}$\n- $P(E) = \\frac{1}{10}$\n\nA variable-length, prefix-free binary code is to be constructed for this source using the Huffman coding algorithm. The algorithm iteratively merges the two symbols (or intermediate merged symbols) with the lowest probabilities. Note that in cases of ties in probabilities, different choices for merging can lead to the construction of different, yet equally optimal, coding trees.\n\nCalculate the optimal expected codeword length for this source. Express your answer as a simplified fraction.",
            "solution": "We are given a discrete memoryless source with alphabet $\\mathcal{S}=\\{A,B,C,D,E\\}$ and probabilities $P(A)=\\frac{2}{5}$, $P(B)=\\frac{1}{5}$, $P(C)=\\frac{1}{5}$, $P(D)=\\frac{1}{10}$, $P(E)=\\frac{1}{10}$. By the Huffman coding algorithm, at each step we merge the two least probable symbols (or merged nodes). Merging increases the codeword length of all leaves under the merged node by $1$. The expected codeword length $L$ of the resulting prefix-free code equals the sum of the combined probabilities at each merge step.\n\nStep-by-step merges (one optimal tie-breaking choice):\n- Merge $D$ and $E$: $\\frac{1}{10}+\\frac{1}{10}=\\frac{1}{5}$.\n- Now the multiset of weights is $\\left\\{\\frac{2}{5},\\frac{1}{5},\\frac{1}{5},\\frac{1}{5}\\right\\}$. Merge two of the $\\frac{1}{5}$ nodes (e.g., $B$ and $C$): $\\frac{1}{5}+\\frac{1}{5}=\\frac{2}{5}$.\n- Now weights are $\\left\\{\\frac{2}{5},\\frac{2}{5},\\frac{1}{5}\\right\\}$. Merge the remaining $\\frac{1}{5}$ node (for $D,E$) with one of the $\\frac{2}{5}$ nodes (e.g., for $A$): $\\frac{1}{5}+\\frac{2}{5}=\\frac{3}{5}$.\n- Now weights are $\\left\\{\\frac{2}{5},\\frac{3}{5}\\right\\}$. Merge to obtain $1$.\n\nTherefore, the optimal expected codeword length is the sum of the combined probabilities at these merges:\n$$\nL=\\frac{1}{5}+\\frac{2}{5}+\\frac{3}{5}+1=\\frac{11}{5}.\n$$\nFor verification, a valid Huffman code for this source can have codeword lengths $\\{l_A, l_B, l_C, l_D, l_E\\} = \\{2, 2, 2, 3, 3\\}$. The expected length for this code is:\n$$\nL = \\frac{2}{5}(2) + \\frac{1}{5}(2) + \\frac{1}{5}(2) + \\frac{1}{10}(3) + \\frac{1}{10}(3) = \\frac{4}{5} + \\frac{2}{5} + \\frac{2}{5} + \\frac{3}{10} + \\frac{3}{10} = \\frac{8}{5} + \\frac{6}{10} = \\frac{16+6}{10} = \\frac{22}{10} = \\frac{11}{5}.\n$$\nThis is minimal by Huffman optimality; different tie-breaks yield the same expected length.",
            "answer": "$$\\boxed{\\frac{11}{5}}$$"
        },
        {
            "introduction": "The optimality of Huffman codes hinges on a simple but powerful greedy choice: always merge the two least probable symbols. What happens if we violate this rule? This practice problem provides a direct, hands-on way to explore the consequences by comparing a correctly generated Huffman code with one produced by a faulty algorithm. Quantifying the loss in efficiency will give you a concrete appreciation for why the specific greedy strategy of the Huffman algorithm is essential for achieving optimality. ",
            "id": "1644338",
            "problem": "An engineer is designing a data compression scheme for a communication system that transmits data from a simple four-symbol source. The source alphabet is $\\mathcal{A} = \\{A, B, C, D\\}$, and the symbols are generated independently with the following probabilities: $P(A) = 0.4$, $P(B) = 0.3$, $P(C) = 0.16$, and $P(D) = 0.14$.\n\nThe engineer intends to use the optimal Huffman coding algorithm to create a binary prefix code, which we will call Code H. However, during the implementation, a bug is introduced. This bug alters the first step of the Huffman algorithm. Instead of combining the two symbols with the lowest probabilities, the faulty algorithm (which generates a code we'll call Code M) combines the symbol with the *second-lowest probability* and the symbol with the *third-lowest probability*. After this incorrect first step, the rest of the faulty algorithm proceeds correctly by always combining the two nodes with the lowest current probabilities until a single root node is formed.\n\nYour task is to quantify the inefficiency introduced by this bug. Calculate the difference between the average codeword length of the mistaken code, $L_M$, and the average codeword length of the correct Huffman code, $L_H$.\n\nExpress the value of $L_M - L_H$ in bits/symbol, rounded to three significant figures.",
            "solution": "We are given a memoryless source with alphabet $\\mathcal{A}=\\{A,B,C,D\\}$ and probabilities $P(A)=0.4$, $P(B)=0.3$, $P(C)=0.16$, $P(D)=0.14$. The average codeword length for a code with lengths $\\{l(A),l(B),l(C),l(D)\\}$ is\n$$\nL=\\sum_{x\\in\\mathcal{A}} P(x)\\,l(x).\n$$\n\nCorrect Huffman code (Code H):\n- Start by combining the two smallest probabilities: $0.14$ and $0.16$ combine to form a node of weight $0.30$.\n- Now the multiset of weights is $\\{0.30,0.30,0.40\\}$. Combine the two smallest: the two $0.30$ nodes combine to weight $0.60$.\n- Finally combine $0.40$ and $0.60$ to form the root.\n\nThe resulting codeword lengths are:\n- $l(A)=1$ (it joins only at the last merge),\n- $l(B)=2$ (it is merged at the second step and then at the root),\n- $l(C)=3$, $l(D)=3$ (they are merged first, then at the second step, then at the root).\n\nThus,\n$$\nL_{H}=0.4\\cdot 1+0.3\\cdot 2+0.16\\cdot 3+0.14\\cdot 3=0.4+0.6+0.48+0.42=1.9.\n$$\n\nFaulty code (Code M):\n- The bug combines the second-lowest and third-lowest probabilities first: $0.16$ and $0.30$ combine to form $0.46$.\n- Now the multiset is $\\{0.14,0.40,0.46\\}$. Proceeding correctly, combine the two smallest: $0.14$ and $0.40$ to form $0.54$.\n- Finally combine $0.46$ and $0.54$ to form the root.\n\nThe resulting codeword lengths are:\n- $l(B)=2$ and $l(C)=2$ (merged in the first step and at the root),\n- $l(D)=2$ and $l(A)=2$ (merged in the second step and at the root).\n\nThus,\n$$\nL_{M}=0.4\\cdot 2+0.3\\cdot 2+0.16\\cdot 2+0.14\\cdot 2=2.0.\n$$\n\nTherefore, the inefficiency introduced is\n$$\nL_{M}-L_{H}=2.0-1.9=0.1,\n$$\nwhich, rounded to three significant figures, is $0.100$ bits per symbol.",
            "answer": "$$\\boxed{0.100}$$"
        }
    ]
}