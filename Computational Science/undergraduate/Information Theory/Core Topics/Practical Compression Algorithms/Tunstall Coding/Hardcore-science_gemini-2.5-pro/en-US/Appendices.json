{
    "hands_on_practices": [
        {
            "introduction": "The foundation of Tunstall coding lies in its unique method of parsing a data stream. To master this technique, we begin with the core skill: constructing the dictionary of source phrases. This first practice exercise  guides you through the iterative process of building a Tunstall tree for a simple binary source, where you will repeatedly expand the most probable sequence until you reach a target dictionary size.",
            "id": "1665390",
            "problem": "A data compression system is being designed for a Binary Memoryless Source (BMS). A BMS is a source that emits symbols from a fixed alphabet, where each emission is independent of the previous ones. The alphabet for this source is $\\mathcal{X} = \\{0, 1\\}$, with a known probability distribution for the symbols: the probability of emitting a '0' is $P(0) = 0.8$, and the probability of emitting a '1' is $P(1) = 0.2$.\n\nThe design employs Tunstall coding, a method that parses the source data stream into a sequence of variable-length phrases. Each of these phrases belongs to a predefined dictionary and is then mapped to a fixed-length binary codeword.\n\nYour task is to determine the set of source phrases that constitute the Tunstall dictionary for this source, given that the target size of the dictionary is $M=4$. Which of the following options correctly represents this dictionary?\n\nA. $\\{0, 1, 00, 01\\}$\n\nB. $\\{00, 01, 10, 11\\}$\n\nC. $\\{000, 001, 01, 1\\}$\n\nD. $\\{1, 10, 11, 00\\}$\n\nE. $\\{0, 10, 110, 111\\}$",
            "solution": "For a Binary Memoryless Source with alphabet $\\mathcal{X}=\\{0,1\\}$ and probabilities $P(0)=0.8$ and $P(1)=0.2$, Tunstall coding constructs a parsing tree by repeatedly expanding the current most probable leaf until the number of leaves equals the target dictionary size $M$. Each expansion replaces a leaf phrase $w$ of probability $P(w)$ with its two children $w0$ and $w1$ having probabilities $P(w0)=P(w)P(0)$ and $P(w1)=P(w)P(1)$, respectively. The dictionary is the set of leaves when the tree reaches $M$ leaves.\n\nStart with the root (empty phrase) of probability $1$. Expand the root into its children:\n$$\\{0,1\\},\\quad P(0)=0.8,\\; P(1)=0.2.$$\nThere are $2$ leaves. The most probable leaf is $0$ with probability $0.8$. Expand $0$:\n$$0 \\to \\{00,01\\},\\quad P(00)=0.8\\cdot 0.8=0.64,\\; P(01)=0.8\\cdot 0.2=0.16.$$\nThe leaves are now $\\{00,01,1\\}$ with probabilities $0.64,0.16,0.2$, respectively. The most probable leaf is $00$ with probability $0.64$. Expand $00$:\n$$00 \\to \\{000,001\\},\\quad P(000)=0.64\\cdot 0.8=0.512,\\; P(001)=0.64\\cdot 0.2=0.128.$$\nThe leaves are now $\\{000,001,01,1\\}$, which is $M=4$ leaves, so we stop. The Tunstall dictionary is therefore $\\{000,001,01,1\\}$, which matches option C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Once a Tunstall dictionary is constructed, we need a way to measure its effectiveness. A key performance metric is the expected length of the source phrases in the dictionary. Longer average phrases generally lead to better compression, as more source symbols are represented by a single fixed-length output code. This exercise  builds on the previous one by challenging you to not only build the dictionary but also to calculate this crucial expected length.",
            "id": "1665354",
            "problem": "Consider a binary memoryless source that generates symbols from the alphabet $\\mathcal{S} = \\{0, 1\\}$. The probability of generating a '0' is $p=0.75$, and the probability of generating a '1' is consequently $1-p=0.25$. To compress the output of this source, a variable-to-fixed length code is constructed using the Tunstall algorithm. The algorithm begins with a dictionary containing the initial source symbols. It then iteratively expands the most probable sequence in the dictionary by appending each source symbol to it, adding the new longer sequences to the dictionary and removing the one that was expanded. This process is repeated until the dictionary contains exactly $M=5$ unique source sequences (codewords).\n\nWhat is the expected length of a source sequence in the final dictionary of $M=5$ codewords? Express your answer as a single numerical value.",
            "solution": "Let the source have $p=\\Pr\\{0\\}=0.75$ and $1-p=\\Pr\\{1\\}=0.25$. The Tunstall algorithm starts with the dictionary $\\{0,1\\}$ and, at each step, expands the most probable sequence by appending each source symbol.\n\nStart: dictionary $\\{0,1\\}$ with probabilities $p$ and $1-p$, respectively.\n\nIteration 1 (expand the most probable sequence $0$):\n- Replace $0$ by $00$ and $01$ with probabilities $p^{2}$ and $p(1-p)$.\n- Dictionary: $\\{1,01,00\\}$ with probabilities $\\{1-p,\\;p(1-p),\\;p^{2}\\}$.\n\nIteration 2 (expand the most probable sequence $00$):\n- Replace $00$ by $000$ and $001$ with probabilities $p^{3}$ and $p^{2}(1-p)$.\n- Dictionary: $\\{1,01,000,001\\}$ with probabilities $\\{1-p,\\;p(1-p),\\;p^{3},\\;p^{2}(1-p)\\}$.\n\nIteration 3 (expand the most probable sequence $000$):\n- Replace $000$ by $0000$ and $0001$ with probabilities $p^{4}$ and $p^{3}(1-p)$.\n- Final dictionary of $M=5$ sequences: $\\{1,01,001,0000,0001\\}$ with probabilities\n$\\{1-p,\\;p(1-p),\\;p^{2}(1-p),\\;p^{4},\\;p^{3}(1-p)\\}$ and lengths $\\{1,2,3,4,4\\}$.\n\nThe expected source-sequence length is\n$$\n\\mathbb{E}[L]\n=1\\cdot(1-p)+2\\cdot p(1-p)+3\\cdot p^{2}(1-p)+4\\cdot p^{4}+4\\cdot p^{3}(1-p).\n$$\nSubstitute $p=\\frac{3}{4}$:\n$$\n\\mathbb{E}[L]\n=1\\cdot\\frac{1}{4}+2\\cdot\\frac{3}{16}+3\\cdot\\frac{9}{64}+4\\cdot\\frac{81}{256}+4\\cdot\\frac{27}{256}\n=\\frac{64+96+108+324+108}{256}\n=\\frac{700}{256}\n=\\frac{175}{64}.\n$$\nHence, the expected length is $\\frac{175}{64}$.",
            "answer": "$$\\boxed{\\frac{175}{64}}$$"
        },
        {
            "introduction": "Real-world data sources are not limited to binary alphabets. This practice  expands our scope to a ternary source, demonstrating how the Tunstall algorithm adapts when the source alphabet size, $K$, is greater than two. It also introduces an important practical detail: since the dictionary size grows in steps of $K-1$, the exact target size may not be achievable. This problem illustrates how to handle such cases by continuing until the dictionary size is at least the target value.",
            "id": "1665400",
            "problem": "Consider a memoryless data source that emits symbols from the ternary alphabet $\\mathcal{A} = \\{S_0, S_1, S_2\\}$. The symbols are generated independently and with a uniform probability distribution, i.e., $P(S_0) = P(S_1) = P(S_2) = 1/3$.\n\nTo compress the data from this source, a variable-length-to-fixed-length encoding scheme based on Tunstall coding is to be designed. The process involves constructing a dictionary of variable-length source sequences by building a tree. Starting from a root node, the leaf node with the highest probability is iteratively expanded by appending each symbol from the source alphabet. This process is continued until the total number of leaf nodes, which constitute the dictionary of codewords, is at least $M=6$.\n\nCalculate the expected length, $L$, of a codeword in this final dictionary. Provide your answer as an exact fraction in its simplest form.",
            "solution": "The problem asks for the expected length of codewords generated by the Tunstall algorithm for a specific source and a target dictionary size.\n\nThe source is a ternary alphabet $\\mathcal{A} = \\{S_0, S_1, S_2\\}$ with uniform probabilities $P(S_i) = 1/3$ for $i \\in \\{0, 1, 2\\}$. The size of the alphabet is $K=3$.\n\nThe Tunstall algorithm builds a tree by iteratively expanding the leaf node with the highest probability. Each expansion of a leaf node replaces it with $K$ new leaf nodes, increasing the total number of leaves by $K-1$. For our ternary source, each expansion adds $3-1=2$ leaves to the tree.\n\nThe algorithm terminates when the number of leaves (the dictionary size) is at least $M=6$.\n\nLet's trace the construction of the tree step by step.\n\n**Initial State (Step 0):**\nThe tree starts with a single root node, which can be thought of as representing the empty string. The number of leaves is 1.\n\n**Step 1:**\nWe expand the root node. This creates three new leaf nodes corresponding to the source symbols: $S_0, S_1, S_2$.\n- The dictionary is $\\{S_0, S_1, S_2\\}$.\n- The number of leaves is 3.\n- The probability of each leaf is $1/3$.\n- Since the number of leaves ($3$) is less than the target size ($6$), we must continue.\n\n**Step 2:**\nWe must expand the leaf node with the highest probability. Currently, all three leaves ($S_0, S_1, S_2$) have the same maximum probability of $1/3$. By convention, we can choose any one to expand; let's choose $S_0$.\n- We replace the leaf $S_0$ with three new leaves: $S_0S_0, S_0S_1, S_0S_2$.\n- The probability of these new leaves is $P(S_0) \\times P(S_i) = (1/3) \\times (1/3) = 1/9$.\n- The current set of leaves (the dictionary) is now $\\{S_1, S_2, S_0S_0, S_0S_1, S_0S_2\\}$.\n- The number of leaves is $3 - 1 + 3 = 5$.\n- The probabilities of the leaves are $\\{1/3, 1/3, 1/9, 1/9, 1/9\\}$.\n- Since the number of leaves ($5$) is still less than the target size ($6$), we must continue.\n\n**Step 3:**\nWe again find the leaf with the highest probability. The leaves $S_1$ and $S_2$ both have probability $1/3$, which is greater than the $1/9$ of the other leaves. Let's choose to expand $S_1$.\n- We replace the leaf $S_1$ with three new leaves: $S_1S_0, S_1S_1, S_1S_2$.\n- The probability of these new leaves is $P(S_1) \\times P(S_i) = (1/3) \\times (1/3) = 1/9$.\n- The final set of leaves, which forms our dictionary, is $\\{S_2, S_0S_0, S_0S_1, S_0S_2, S_1S_0, S_1S_1, S_1S_2\\}$.\n- The number of leaves is now $5 - 1 + 3 = 7$.\n- Since the number of leaves ($7$) is greater than or equal to the target size ($6$), the algorithm terminates.\n\n**Final Dictionary and Properties:**\nThe final dictionary contains 7 codewords:\n- One codeword of length 1: $S_2$. Its probability is $P(S_2) = 1/3$.\n- Six codewords of length 2: $S_0S_0, S_0S_1, S_0S_2, S_1S_0, S_1S_1, S_1S_2$. Each of these has a probability of $1/9$.\n\nLet's verify the sum of probabilities of the dictionary words:\n$\\sum P(\\text{codeword}) = P(S_2) + 6 \\times P(S_0S_0) = \\frac{1}{3} + 6 \\times \\frac{1}{9} = \\frac{3}{9} + \\frac{6}{9} = \\frac{9}{9} = 1$. The probabilities sum to 1, as expected.\n\n**Calculating the Expected Length:**\nThe expected length $L$ of a codeword is the sum of the products of each codeword's length and its probability:\n$L = \\sum_{c \\in \\text{Dictionary}} P(c) \\cdot \\text{length}(c)$\n\n$L = \\left( P(S_2) \\cdot \\text{length}(S_2) \\right) + \\sum_{i=0}^{2} \\left( P(S_0S_i) \\cdot \\text{length}(S_0S_i) \\right) + \\sum_{i=0}^{2} \\left( P(S_1S_i) \\cdot \\text{length}(S_1S_i) \\right)$\n\n$L = \\left(\\frac{1}{3} \\cdot 1\\right) + 6 \\cdot \\left(\\frac{1}{9} \\cdot 2\\right)$\n\n$L = \\frac{1}{3} + \\frac{12}{9}$\n\nTo add these fractions, we find a common denominator, which is 9.\n$L = \\frac{3}{9} + \\frac{12}{9} = \\frac{15}{9}$\n\nFinally, we simplify the fraction by dividing the numerator and denominator by their greatest common divisor, which is 3.\n$L = \\frac{15 \\div 3}{9 \\div 3} = \\frac{5}{3}$",
            "answer": "$$\\boxed{\\frac{5}{3}}$$"
        }
    ]
}