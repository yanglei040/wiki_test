{
    "hands_on_practices": [
        {
            "introduction": "The foundation of Tunstall coding lies in its unique method of parsing a data stream. Before we can assign fixed-length codes, we must first build a \"dictionary\" of variable-length source phrases. This first practice will guide you through the core mechanical step of the Tunstall algorithm: the iterative construction of this dictionary by always expanding the most probable sequence in your collection until a target size is reached . Mastering this tree-building process is the essential first step toward understanding how Tunstall coding achieves compression.",
            "id": "1665390",
            "problem": "A data compression system is being designed for a Binary Memoryless Source (BMS). A BMS is a source that emits symbols from a fixed alphabet, where each emission is independent of the previous ones. The alphabet for this source is $\\mathcal{X} = \\{0, 1\\}$, with a known probability distribution for the symbols: the probability of emitting a '0' is $P(0) = 0.8$, and the probability of emitting a '1' is $P(1) = 0.2$.\n\nThe design employs Tunstall coding, a method that parses the source data stream into a sequence of variable-length phrases. Each of these phrases belongs to a predefined dictionary and is then mapped to a fixed-length binary codeword.\n\nYour task is to determine the set of source phrases that constitute the Tunstall dictionary for this source, given that the target size of the dictionary is $M=4$. Which of the following options correctly represents this dictionary?\n\nA. $\\{'0', '1', '00', '01'\\}$\n\nB. $\\{'00', '01', '10', '11'\\}$\n\nC. $\\{'000', '001', '01', '1'\\}$\n\nD. $\\{'1', '10', '11', '00'\\}$\n\nE. $\\{'0', '10', '110', '111'\\}$",
            "solution": "For a Binary Memoryless Source with alphabet $\\mathcal{X}=\\{0,1\\}$ and probabilities $P('0')=0.8$ and $P('1')=0.2$, Tunstall coding constructs a parsing tree by repeatedly expanding the current most probable leaf until the number of leaves equals the target dictionary size $M$. Each expansion replaces a leaf phrase 'w' of probability $P('w')$ with its two children 'w0' and 'w1' having probabilities $P('w0')=P('w') \\times P('0')$ and $P('w1')=P('w') \\times P('1')$, respectively. The dictionary is the set of leaves when the tree reaches $M$ leaves.\n\nThe initial dictionary consists of the source symbols, $\\{'0', '1'\\}$. The probabilities are $P('0')=0.8$ and $P('1')=0.2$. There are 2 leaves.\n\nThe most probable leaf is '0' (probability 0.8). We expand it, so the new leaves are $\\{'00', '01'\\}$ with probabilities $P('00') = 0.8 \\times 0.8 = 0.64$ and $P('01') = 0.8 \\times 0.2 = 0.16$. The set of leaves is now $\\{'1', '01', '00'\\}$, with 3 leaves.\n\nThe most probable leaf is now '00' (probability 0.64). We expand it, so the new leaves are $\\{'000', '001'\\}$ with probabilities $P('000')=0.64 \\times 0.8 = 0.512$ and $P('001') = 0.64 \\times 0.2 = 0.128$.\n\nThe set of leaves is now $\\{'1', '01', '000', '001'\\}$. This is $M=4$ leaves, so we stop. The Tunstall dictionary is therefore $\\{'000', '001', '01', '1'\\}$, which matches option C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Once you can construct a Tunstall dictionary, the next logical step is to evaluate its effectiveness. A key performance metric is the expected length of a source sequence, denoted as $E[L]$. This value tells us, on average, how many source symbols are grouped into a single dictionary entry. This exercise asks you to calculate this value, which requires you to first build the dictionary and then compute a weighted average based on the probabilities of each phrase . A higher expected length is generally desirable, as it means more source symbols are represented by a single fixed-length output code, leading to better compression.",
            "id": "1665354",
            "problem": "Consider a binary memoryless source that generates symbols from the alphabet $\\mathcal{S} = \\{0, 1\\}$. The probability of generating a '0' is $p=0.75$, and the probability of generating a '1' is consequently $1-p=0.25$. To compress the output of this source, a variable-to-fixed length code is constructed using the Tunstall algorithm. The algorithm begins with a dictionary containing the initial source symbols. It then iteratively expands the most probable sequence in the dictionary by appending each source symbol to it, adding the new longer sequences to the dictionary and removing the one that was expanded. This process is repeated until the dictionary contains exactly $M=5$ unique source sequences (codewords).\n\nWhat is the expected length of a source sequence in the final dictionary of $M=5$ codewords? Express your answer as a single numerical value.",
            "solution": "Let the source have $p=P(\\{0\\})=0.75$ and $1-p=P(\\{1\\})=0.25$. The Tunstall algorithm starts with the dictionary $\\{'0','1'\\}$ and, at each step, expands the most probable sequence by appending each source symbol.\n\nStart: dictionary $\\{'0', '1'\\}$ with probabilities $p$ and $1-p$, respectively.\n\nIteration 1 (expand the most probable sequence '0'):\n- Replace '0' by '00' and '01' with probabilities $p^{2}$ and $p(1-p)$.\n- Dictionary: $\\{'1', '01', '00'\\}$ with probabilities $\\{1-p,\\;p(1-p),\\;p^{2}\\}$.\n\nIteration 2 (expand the most probable sequence '00'):\n- Replace '00' by '000' and '001' with probabilities $p^{3}$ and $p^{2}(1-p)$.\n- Dictionary: $\\{'1', '01', '000', '001'\\}$ with probabilities $\\{1-p,\\;p(1-p),\\;p^{3},\\;p^{2}(1-p)\\}$.\n\nIteration 3 (expand the most probable sequence '000'):\n- Replace '000' by '0000' and '0001' with probabilities $p^{4}$ and $p^{3}(1-p)$.\n- Final dictionary of $M=5$ sequences: $\\{'1', '01', '001', '0000', '0001'\\}$ with probabilities\n$\\{1-p,\\;p(1-p),\\;p^{2}(1-p),\\;p^{4},\\;p^{3}(1-p)\\}$ and lengths $\\{1,2,3,4,4\\}$.\n\nThe expected source-sequence length is\n$$\n\\mathbb{E}[L] = 1 \\times (1-p) + 2 \\times p(1-p) + 3 \\times p^{2}(1-p) + 4 \\times p^{4} + 4 \\times p^{3}(1-p).\n$$\nSubstitute $p=\\frac{3}{4}$:\n$$\n\\begin{align*}\n\\mathbb{E}[L] &= 1 \\times \\frac{1}{4} + 2 \\times \\frac{3}{16} + 3 \\times \\frac{9}{64} + 4 \\times \\frac{81}{256} + 4 \\times \\frac{27}{256} \\\\\n&= \\frac{64}{256} + \\frac{96}{256} + \\frac{108}{256} + \\frac{324}{256} + \\frac{108}{256} \\\\\n&= \\frac{64+96+108+324+108}{256} \\\\\n&= \\frac{700}{256} = \\frac{175}{64}.\n\\end{align*}\n$$\nHence, the expected length is $\\frac{175}{64}$.",
            "answer": "$$\\boxed{\\frac{175}{64}}$$"
        },
        {
            "introduction": "Theoretical algorithms often have nuances when applied to practical engineering problems. This final practice moves beyond simple execution of the Tunstall algorithm to explore a crucial design trade-off. While the algorithm provides a way to generate an optimal dictionary for a specific size $M$, sometimes a better overall compression *rate* can be achieved by deliberately overshooting this target size and adjusting the output code length accordingly . By comparing two different design schemes, you will learn to evaluate the true measure of compression efficiency, the rate $R = \\frac{L_{out}}{E[L_{in}]}$, and make an informed decision that optimizes the entire system, not just one component.",
            "id": "1665395",
            "problem": "An engineer is designing a variable-length to fixed-length compression system for a discrete memoryless source. The source has an alphabet of three symbols, $\\mathcal{X} = \\{S_0, S_1, S_2\\}$, with probabilities of occurrence given by $P(S_0) = 0.90$, $P(S_1) = 0.05$, and $P(S_2) = 0.05$.\n\nThe design requires a dictionary of source words which are then mapped to fixed-length binary codes. The engineer considers two different approaches to generating this dictionary, both based on the Tunstall coding algorithm, for a target dictionary size of $M=4$.\n\nThe first approach, Scheme A, is a naive implementation. It generates a Tunstall dictionary by performing $k$ splits, where $k$ is chosen to produce the largest possible dictionary size $N_A$ such that $N_A \\le M$. The resulting $N_A$ source words are then mapped to a set of $N_A$ binary codes of length $\\lceil \\log_2 M \\rceil$ bits.\n\nThe second approach, Scheme B, is more flexible. It recognizes that the constraint on $M$ might be suboptimal. It constructs a Tunstall dictionary for the *next* achievable dictionary size, $N_B > M$, which results from performing one more split than in Scheme A. The resulting $N_B$ source words are mapped to binary codes of length $\\lceil \\log_2 N_B \\rceil$ bits.\n\nYour task is to determine which scheme provides better compression (i.e., a lower rate in bits per source symbol). Calculate the compression rate for the more efficient scheme. The compression rate $R$ is defined as $R = \\frac{L_{out}}{E[L_{in}]}$, where $L_{out}$ is the length of the fixed-length output binary code and $E[L_{in}]$ is the expected number of source symbols per dictionary word.\n\nExpress your final answer for the rate of the better scheme, rounded to four significant figures.",
            "solution": "Let the source alphabet be $\\mathcal{X}=\\{S_{0},S_{1},S_{2}\\}$ with probabilities $P(S_{0})=0.90$, $P(S_{1})=0.05$, and $P(S_{2})=0.05$. A Tunstall dictionary grown by splitting increases the number of leaves by $|\\mathcal{X}|-1=2$ per split. Thus the achievable dictionary sizes are $N=1+2k$ for integer $k\\geq 0$.\n\nScheme A: With $M=4$, the largest achievable $N\\leq M$ is $N_{A}=3$ (one split, $k=1$). After one split of the root, the dictionary is $\\{S_{0},S_{1},S_{2}\\}$ with phrase probabilities $\\{0.90,0.05,0.05\\}$ and all phrases have length $1$. Therefore,\n$$\nE[L_{\\text{in},A}]=\\sum_{i=0}^{2}P(S_{i})\\times 1=1,\n$$\nand the fixed output length is\n$$\nL_{\\text{out},A}=\\lceil \\log_{2} M\\rceil=\\lceil \\log_{2} 4\\rceil=2.\n$$\nHence the rate is\n$$\nR_{A}=\\frac{L_{\\text{out},A}}{E[L_{\\text{in},A}]}=\\frac{2}{1}=2.\n$$\n\nScheme B: The next achievable size is $N_{B}=N_{A}+2=5$ (two splits, $k=2$). Starting from $\\{S_{0},S_{1},S_{2}\\}$, split the most probable leaf $S_{0}$, yielding leaves\n$$\n\\{S_{0}S_{0},\\,S_{0}S_{1},\\,S_{0}S_{2},\\,S_{1},\\,S_{2}\\}\n$$\nwith probabilities\n$$\n\\{0.9\\times 0.9,\\,0.9\\times 0.05,\\,0.9\\times 0.05,\\,0.05,\\,0.05\\}=\\{0.81,\\,0.045,\\,0.045,\\,0.05,\\,0.05\\},\n$$\nand lengths $\\{2,2,2,1,1\\}$. The expected input phrase length is\n$$\nE[L_{\\text{in},B}]=2\\times(0.81+0.045+0.045)+1\\times(0.05+0.05)=2\\times 0.90+0.10=1.9.\n$$\nThe fixed output length is\n$$\nL_{\\text{out},B}=\\lceil \\log_{2} N_{B}\\rceil=\\lceil \\log_{2} 5\\rceil=3,\n$$\nso the rate is\n$$\nR_{B}=\\frac{L_{\\text{out},B}}{E[L_{\\text{in},B}]}=\\frac{3}{1.9}=\\frac{30}{19}\\approx 1.578947\\ldots.\n$$\n\nComparison:\n$$\nR_{A}=2,\\quad R_{B}=\\frac{30}{19}\\approx 1.578947\\ldots2.\n$$\nThus Scheme B provides better compression. Rounding to four significant figures gives $1.579$.",
            "answer": "$$\\boxed{1.579}$$"
        }
    ]
}