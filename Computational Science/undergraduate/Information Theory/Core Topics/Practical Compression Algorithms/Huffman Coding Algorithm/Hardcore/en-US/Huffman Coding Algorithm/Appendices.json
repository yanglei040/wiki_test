{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the power of Huffman coding, we begin with a foundational exercise. This practice walks you through the complete process, from analyzing a raw data sample to determine symbol probabilities, to applying the Huffman algorithm to find the optimal average codeword length. This mirrors the real-world task of designing an efficient compression scheme for a given data source .",
            "id": "1630312",
            "problem": "A deep-space probe uses a simplified communication protocol for transmitting its scientific data. The protocol is based on a source alphabet consisting of five distinct symbols: $\\mathcal{S} = \\{A, B, C, D, E\\}$. To design an efficient data compression scheme, a mission analyst studies a representative sample of telemetry data containing a sequence of 12 symbols:\n$$A, B, A, C, A, B, D, A, E, C, B, D$$\nAssuming this sample is statistically representative of the source's output, determine the theoretical minimum average number of binary digits (bits) required to encode one symbol using an optimal symbol-by-symbol, variable-length binary encoding scheme.\n\nExpress your answer as an exact fraction.",
            "solution": "We first compute empirical probabilities from the representative sample of length 12:\n- Counts: $A:4$, $B:3$, $C:2$, $D:2$, $E:1$.\n- Probabilities: $p_{A}=\\frac{4}{12}=\\frac{1}{3}$, $p_{B}=\\frac{3}{12}=\\frac{1}{4}$, $p_{C}=\\frac{2}{12}=\\frac{1}{6}$, $p_{D}=\\frac{2}{12}=\\frac{1}{6}$, $p_{E}=\\frac{1}{12}$.\n\nFor an optimal symbol-by-symbol variable-length binary code, the minimum average code length is achieved by a Huffman code. Apply the Huffman algorithm to the probabilities:\n1. Sort ascending: $\\frac{1}{12}, \\frac{1}{6}, \\frac{1}{6}, \\frac{1}{4}, \\frac{1}{3}$.\n2. Combine the two smallest: $\\frac{1}{12}+\\frac{1}{6}=\\frac{1}{4}$, yielding multiset $\\left\\{\\frac{1}{6}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{3}\\right\\}$.\n3. Combine the two smallest: $\\frac{1}{6}+\\frac{1}{4}=\\frac{5}{12}$, yielding $\\left\\{\\frac{1}{4}, \\frac{1}{3}, \\frac{5}{12}\\right\\}$.\n4. Combine the two smallest: $\\frac{1}{4}+\\frac{1}{3}=\\frac{7}{12}$, yielding $\\left\\{\\frac{5}{12}, \\frac{7}{12}\\right\\}$.\n5. Combine: $\\frac{5}{12}+\\frac{7}{12}=1$.\n\nTracing back the tree gives codeword lengths (in bits): $l_{A}=2$, $l_{B}=2$, $l_{C}=3$, $l_{D}=2$, $l_{E}=3$. The resulting minimum average length is\n$$\n\\bar{L}\n= \\sum_{s \\in \\{A,B,C,D,E\\}} p_{s}\\,l_{s}\n= \\frac{1}{3}\\cdot 2 + \\frac{1}{4}\\cdot 2 + \\frac{1}{6}\\cdot 3 + \\frac{1}{6}\\cdot 2 + \\frac{1}{12}\\cdot 3\n= \\frac{8+6+6+4+3}{12}\n= \\frac{27}{12}\n= \\frac{9}{4}.\n$$\nTherefore, the theoretical minimum average number of bits per symbol for an optimal symbol-by-symbol prefix code is $\\frac{9}{4}$.",
            "answer": "$$\\boxed{\\frac{9}{4}}$$"
        },
        {
            "introduction": "Having mastered the basic mechanics, let's explore an important structural property of Huffman codes. This problem  presents a scenario with one highly probable symbol, a situation common in many real-world datasets where $p > 0.5$. By applying the algorithm, you will uncover a simple and elegant rule that governs the encoding of such dominant symbols, reinforcing the greedy nature of the algorithm.",
            "id": "1630300",
            "problem": "A deep-space probe is analyzing the atmospheric composition of a newly discovered exoplanet. The probe transmits data as a stream of symbols, where each symbol represents a specific molecule detected. To conserve power, the probe uses an optimal prefix-free binary encoding scheme, specifically the Huffman coding algorithm, based on the measured relative frequencies of the molecules.\n\nThe onboard spectrometer has determined the following probabilities for the five most common molecules in the atmosphere:\n- Water vapor (H₂O): $0.53$\n- Dinitrogen (N₂): $0.21$\n- Methane (CH₄): $0.11$\n- Carbon Dioxide (CO₂): $0.09$\n- Argon (Ar): $0.06$\n\nThe sum of these probabilities is $1.00$. All other molecules occur with negligible probability. Based on this probability distribution, determine the length of the binary codeword assigned to the symbol for Water vapor (H₂O).",
            "solution": "We use the Huffman coding algorithm: at each step, combine the two least probable symbols into a new node whose probability is their sum; repeat until one node remains. In the resulting tree, the codeword length of a symbol equals its depth (the number of edges from the root to its leaf).\n\nList the given probabilities in ascending order: $0.06, 0.09, 0.11, 0.21, 0.53$.\n\nCombine the two smallest at each step:\n$$0.06+0.09=0.15,$$\nso the multiset becomes $0.11, 0.15, 0.21, 0.53$.\n\nNext,\n$$0.11+0.15=0.26,$$\nso the multiset becomes $0.21, 0.26, 0.53$.\n\nNext,\n$$0.21+0.26=0.47,$$\nso the multiset becomes $0.47, 0.53$.\n\nFinally,\n$$0.47+0.53=1.00,$$\nyielding the root.\n\nThe symbol with probability $0.53$ is only merged at the final step, so it attaches directly to the root and has depth $1$. Therefore, the length of its binary codeword is $1$. As a consistency check, the full set of lengths produced by these merges is $1, 2, 3, 4, 4$, which satisfies the Kraft equality:\n$$2^{-1}+2^{-2}+2^{-3}+2^{-4}+2^{-4}=1.$$\nThus, the optimal prefix-free code assigns the most probable symbol a codeword of length $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "The Huffman algorithm guarantees an optimal average length, but is the resulting code unique? This exercise  delves into this question by examining what happens when ties occur during the merging process. You will construct two distinct, equally optimal Huffman codes for the same source and analyze how a second-order property—the variance of the codeword length—can differ, providing deeper insight into the 'shape' of different optimal codes.",
            "id": "1630317",
            "problem": "A discrete memoryless source emits symbols from the alphabet $\\mathcal{S} = \\{S_1, S_2, S_3, S_4, S_5, S_6\\}$. The probabilities of these symbols are given by the set $P = \\{0.3, 0.2, 0.2, 0.1, 0.1, 0.1\\}$, where $p(S_1)=0.3$, $p(S_2)=0.2$, $p(S_3)=0.2$, and $p(S_4)=p(S_5)=p(S_6)=0.1$.\n\nThe Huffman coding algorithm is used to generate an optimal prefix-free binary code for this source. The execution of the algorithm involves iteratively merging the two nodes (symbols or groups of symbols) with the lowest probabilities. When ties in probabilities occur, different choices can be made, potentially leading to different valid Huffman trees. While all resulting codes are optimal and share the same minimum average codeword length, the distribution of individual codeword lengths can differ.\n\nYour task is to determine the maximum possible variance of the codeword length, $L$, for an optimal code generated for this source. The variance is defined as $\\text{Var}(L) = \\sum_{i=1}^{6} p(S_i) (l_i - \\bar{L})^2 = E[L^2] - (E[L])^2$, where $l_i$ is the length of the codeword for symbol $S_i$, and $\\bar{L} = E[L]$ is the average codeword length.\n\nExpress your answer as a single decimal number, rounded to three significant figures.",
            "solution": "Let the source probabilities be $p_{1}=0.3$, $p_{2}=0.2$, $p_{3}=0.2$, $p_{4}=0.1$, $p_{5}=0.1$, $p_{6}=0.1$. The Huffman merge sequence by weights is fixed:\n- Merge two $0.1$ to $0.2$.\n- Merge $0.1$ with $0.2$ to $0.3$.\n- Merge $0.2$ with $0.2$ to $0.4$.\n- Merge $0.3$ with $0.3$ to $0.6$.\n- Merge $0.6$ with $0.4$ to $1$.\n\nTies allow two structurally different optimal assignments of codeword lengths.\n\nCase I (do not merge the initial $0.1+0.1$ pair with the remaining $0.1$ at the next step): At step 2, merge the remaining $0.1$ with a singleton $0.2$. The resulting lengths are\n- $l_{1}=2$,\n- among $p=0.2$, one gets $l=2$ and one gets $l=3$,\n- all three $p=0.1$ get $l=3$.\nThus the length distribution is $L=2$ with total probability $0.5$ and $L=3$ with total probability $0.5$. Hence\n$$\nE[L]=2\\cdot 0.5+3\\cdot 0.5=2.5,\\quad\nE[L^{2}]=4\\cdot 0.5+9\\cdot 0.5=6.5,\n$$\nso\n$$\n\\operatorname{Var}(L)=E[L^{2}]-(E[L])^{2}=6.5-(2.5)^{2}=0.25.\n$$\n\nCase II (merge the initial $0.1+0.1$ pair with the remaining $0.1$ at the next step): At step 2, merge the remaining $0.1$ with the $0.2$ formed by two $0.1$’s. The resulting lengths are\n- $l_{1}=2$,\n- both $p=0.2$ symbols get $l=2$,\n- one $p=0.1$ symbol gets $l=3$,\n- the other two $p=0.1$ symbols get $l=4$.\nThus the length distribution is $L=2$ with probability $0.7$, $L=3$ with probability $0.1$, and $L=4$ with probability $0.2$. Hence\n$$\nE[L]=2\\cdot 0.7+3\\cdot 0.1+4\\cdot 0.2=2.5,\n$$\n$$\nE[L^{2}]=4\\cdot 0.7+9\\cdot 0.1+16\\cdot 0.2=6.9,\n$$\nso\n$$\n\\operatorname{Var}(L)=6.9-(2.5)^{2}=6.9-6.25=0.65.\n$$\n\nBoth cases are optimal with the same minimal average length $E[L]=2.5$. The variance is maximized in Case II. Therefore, the maximum possible variance is $0.65$, which to three significant figures is $0.650$.",
            "answer": "$$\\boxed{0.650}$$"
        }
    ]
}