{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering the concept of typical sets is to apply the formal definition to a concrete example. This practice  provides a foundational exercise where a sequence's observed symbol frequencies perfectly match the underlying probabilities of the source. By working through the calculation, you will verify how the mathematical condition for typicality confirms that such a sequence is indeed a quintessential member of the typical set, as its per-symbol surprisal exactly equals the source entropy.",
            "id": "1666266",
            "problem": "A memoryless binary source generates symbols from the alphabet $\\mathcal{X} = \\{0, 1\\}$. The probability of emitting a '1' is $p=0.2$. The entropy of this source is denoted by $H(X)$. An engineer observes the specific 10-symbol sequence $x^{10} = 1000010000$.\n\nThe set of weakly $\\epsilon$-typical sequences of length $n$, as defined by the Asymptotic Equipartition Property (AEP), is the set $A_{\\epsilon}^{(n)}$ of all sequences $y^n$ whose probability $P(y^n)$ satisfies the condition:\n$$ \\left| -\\frac{1}{n} \\log_2 P(y^n) - H(X) \\right| \\leq \\epsilon $$\nFor this memoryless binary source, the probability of a sequence $y^n$ with $k$ ones and $n-k$ zeros is given by $P(y^n) = p^k (1-p)^{n-k}$.\n\nFor comparison, one can also calculate the empirical entropy of the sequence, $H(y^n)$, which is the entropy of a distribution matching the symbol frequencies within $y^n$.\n\nGiven $\\epsilon = 0.1$, which of the following statements is true for the observed sequence $x^{10}$?\n\nA. The value of $-\\frac{1}{10}\\log_2 P(x^{10})$ is greater than $H(X)$, and the sequence is in $A_{0.1}^{(10)}$.\n\nB. The value of $-\\frac{1}{10}\\log_2 P(x^{10})$ is less than $H(X)$, and the sequence is in $A_{0.1}^{(10)}$.\n\nC. The value of $-\\frac{1}{10}\\log_2 P(x^{10})$ is equal to $H(X)$, and the sequence is in $A_{0.1}^{(10)}$.\n\nD. The value of $-\\frac{1}{10}\\log_2 P(x^{10})$ is greater than $H(X)$, and the sequence is not in $A_{0.1}^{(10)}$.\n\nE. The value of $-\\frac{1}{10}\\log_2 P(x^{10})$ is less than $H(X)$, and the sequence is not in $A_{0.1}^{(10)}$.",
            "solution": "The source is i.i.d. Bernoulli with parameter $p=0.2$. Its entropy is\n$$\nH(X)=-p\\log_{2}p-(1-p)\\log_{2}(1-p).\n$$\nFor a length-$n$ sequence $y^{n}$ with $k$ ones, the probability is $P(y^{n})=p^{k}(1-p)^{n-k}$, so\n$$\n-\\frac{1}{n}\\log_{2}P(y^{n})=-\\frac{k}{n}\\log_{2}p-\\left(1-\\frac{k}{n}\\right)\\log_{2}(1-p).\n$$\nFor the observed sequence $x^{10}=1000010000$, the number of ones is $k=2$, hence $\\frac{k}{n}=\\frac{2}{10}=0.2=p$. Substituting $\\frac{k}{n}=p$ into the expression gives\n$$\n-\\frac{1}{10}\\log_{2}P(x^{10})=-p\\log_{2}p-(1-p)\\log_{2}(1-p)=H(X).\n$$\nTherefore,\n$$\n\\left|-\\frac{1}{10}\\log_{2}P(x^{10})-H(X)\\right|=0\\leq \\epsilon \\quad \\text{for} \\quad \\epsilon=0.1,\n$$\nso the sequence is in $A_{0.1}^{(10)}$, and the value equals $H(X)$. This corresponds to option C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "It's a common trap to assume that the single most probable sequence from a source must also be \"typical.\" This exercise  is designed to challenge that intuition and reveal a deeper insight from the Asymptotic Equipartition Property (AEP). You will investigate a highly biased source and discover that its most likely outcome is, surprisingly, not a member of the typical set, forcing a crucial distinction between high probability and statistical representativeness.",
            "id": "1666210",
            "problem": "Consider a discrete memoryless source that produces a sequence of binary outcomes, which we will call \"Heads\" (H) and \"Tails\" (T). This source is analogous to flipping a biased coin. The probability of producing a \"Heads\" is $P(X=\\text{H}) = 0.9$, and the probability of producing a \"Tails\" is $P(X=\\text{T}) = 0.1$.\n\nA sequence $x^n = (x_1, x_2, \\dots, x_n)$ of length $n$ is said to be a member of the typical set $A_{\\epsilon}^{(n)}$ if its empirical entropy is close to the true entropy of the source. Specifically, the condition for membership is:\n$$ \\left| -\\frac{1}{n} \\log_2 P(x^n) - H(X) \\right| \\le \\epsilon $$\nwhere $H(X)$ is the Shannon entropy of the source $X$, $P(x^n)$ is the probability of the specific sequence $x^n$, and all logarithms are base-2.\n\nFor a sequence of length $n=10$ generated by this source and a given parameter $\\epsilon = 0.1$, determine which of the following statements is correct.\n\nA. The most probable sequence is HHHHHHHHHH, and it belongs to the typical set $A_{0.1}^{(10)}$.\n\nB. The most probable sequence is HHHHHHHHHH, and it does not belong to the typical set $A_{0.1}^{(10)}$.\n\nC. The most probable sequence is one with exactly nine Heads and one Tail (e.g., HHHHHHHHHT), and it belongs to the typical set $A_{0.1}^{(10)}$.\n\nD. The most probable sequence is one with exactly nine Heads and one Tail (e.g., HHHHHHHHHT), and it does not belong to the typical set $A_{0.1}^{(10)}$.\n\nE. The most probable sequence is one with exactly eight Heads and two Tails, and it belongs to the typical set $A_{0.1}^{(10)}$.",
            "solution": "Let the source be Bernoulli with $p \\triangleq P(X=\\text{H})=0.9$ and $q \\triangleq P(X=\\text{T})=1-p=0.1$. For any length-$10$ sequence with $k$ Heads, its probability is $P = p^{k} q^{10-k}$. The ratio between probabilities of sequences with $k+1$ and $k$ Heads is\n$$\n\\frac{p^{k+1} q^{9-k}}{p^{k} q^{10-k}}=\\frac{p}{q}.\n$$\nSince $\\frac{p}{q}=\\frac{0.9}{0.1}=91$, the probability increases with $k$, so the most probable single sequence is the one with $k=10$, namely HHHHHHHHHH.\n\nNow check whether HHHHHHHHHH is in $A_{0.1}^{(10)}$. For this sequence,\n$$\n-\\frac{1}{10}\\log_{2} P(\\text{HHHHHHHHHH})=-\\frac{1}{10}\\log_{2}\\left(p^{10}\\right)=-\\log_{2} p.\n$$\nThe source entropy is\n$$\nH(X)=-p\\log_{2} p - q\\log_{2} q.\n$$\nThus the typicality deviation is\n$$\n\\left|-\\log_{2} p - H(X)\\right|=\\left|-\\log_{2} p + p\\log_{2} p + q\\log_{2} q\\right|\n=\\left|(p-1)\\log_{2} p + q\\log_{2} q\\right|\n$$\n$$\n=\\left|-(1-p)\\log_{2} p + (1-p)\\log_{2} q\\right|\n=(1-p)\\left|\\log_{2}\\!\\left(\\frac{q}{p}\\right)\\right|.\n$$\nWith $p=0.9$ and $q=0.1$, this becomes\n$$\n(1-p)\\left|\\log_{2}\\!\\left(\\frac{q}{p}\\right)\\right|=0.1\\left|\\log_{2}\\!\\left(\\frac{1}{9}\\right)\\right|=0.1\\log_{2}(9).\n$$\nSince $92$, we have $\\log_{2}(9)\\log_{2}(2)=1$, hence $0.1\\log_{2}(9)0.1=\\epsilon$. Therefore HHHHHHHHHH does not satisfy the typicality criterion for $\\epsilon=0.1$.\n\nCombining the two conclusions: the most probable sequence is HHHHHHHHHH, and it does not belong to $A_{0.1}^{(10)}$. Hence, statement B is correct.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "The power of typical sets extends beyond classifying individual sequences; it allows us to characterize the source itself, which is fundamental to data compression. This practice  reverses the usual direction of inquiry: instead of using source entropy to analyze the typical set, you will use the size of the typical set to estimate the source's entropy. This approach demonstrates how knowing the volume of \"statistically significant\" outputs gives us a direct measure of the source's inherent randomness.",
            "id": "1666262",
            "problem": "A team of computer scientists is developing an optimal compression algorithm for a data stream generated by a specific type of sensor. This sensor's output can be modeled as a stationary binary memoryless source, which we denote as $X$. The source produces long sequences of bits (0s and 1s). The team analyzes sequences of length $n = 100$ bits. Using statistical analysis, they determine that the number of \"typical\" sequences, which occur with high probability and are the primary target for their compression scheme, is approximately $1.2 \\times 10^{15}$. For the purposes of this model, the size of this typical set, $|A_{\\epsilon}^{(n)}|$, is directly related to the source entropy.\n\nGiven the necessary values $\\ln(1.2 \\times 10^{15}) \\approx 34.72$ and $\\ln(2) \\approx 0.693$, estimate the entropy $H(X)$ of the binary source. Calculate your answer in bits per symbol and round it to three significant figures.",
            "solution": "We model the stationary binary memoryless source by the Asymptotic Equipartition Property, which implies that the typical set size scales as\n$$\n|A_{\\epsilon}^{(n)}| \\approx 2^{n H(X)}.\n$$\nTaking natural logarithms on both sides gives\n$$\n\\ln |A_{\\epsilon}^{(n)}| \\approx n H(X) \\ln 2.\n$$\nSolving for $H(X)$,\n$$\nH(X) \\approx \\frac{\\ln |A_{\\epsilon}^{(n)}|}{n \\ln 2}.\n$$\nSubstituting the given values $n=100$, $\\ln(1.2 \\times 10^{15}) \\approx 34.72$, and $\\ln(2) \\approx 0.693$,\n$$\nH(X) \\approx \\frac{34.72}{100 \\times 0.693} = \\frac{34.72}{69.3} \\approx 0.501.\n$$\nThus, the entropy is approximately $0.501$ bits per symbol (to three significant figures).",
            "answer": "$$\\boxed{0.501}$$"
        }
    ]
}