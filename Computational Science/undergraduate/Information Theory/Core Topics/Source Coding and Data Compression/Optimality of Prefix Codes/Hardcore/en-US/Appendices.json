{
    "hands_on_practices": [
        {
            "introduction": "The foundation of creating an optimal prefix code lies in mastering Huffman's algorithm. This first exercise provides a direct application of the algorithm to a common scenario, allowing you to calculate the minimum possible average codeword length for a given source. By working through this problem , you will solidify your understanding of the step-by-step merging process that guarantees optimality in binary coding.",
            "id": "1623278",
            "problem": "A deep-space probe is designed to monitor and classify five distinct types of cosmic ray events, labeled A, B, C, D, and E. The probe operates autonomously and, due to power limitations for its transmitter, must encode the classification of each detected event into a sequence of binary digits (bits) using the most efficient scheme possible. From extensive prior observations, the long-term probabilities for the detection of each event type within a standard observation window have been determined. Events A and B each occur with a probability of $1/3$. Events C, D, and E each occur with a probability of $1/9$.\n\nAssuming the goal is to minimize the average number of bits transmitted per event, what is this minimum possible average length? Express your answer as a single fraction.",
            "solution": "We model the problem as constructing a binary prefix code to minimize the expected codeword length. Let the symbol probabilities be $p_{A}=p_{B}=\\frac{1}{3}$ and $p_{C}=p_{D}=p_{E}=\\frac{1}{9}$. For optimality among binary prefix codes, we apply Huffman coding.\n\nStep-by-step Huffman merges using the smallest probabilities:\n1. Merge two $\\frac{1}{9}$ symbols to form a node of weight $\\frac{2}{9}$. This increases the codeword lengths of those two symbols by $1$.\n2. Merge $\\frac{2}{9}$ with the remaining $\\frac{1}{9}$ to form a node of weight $\\frac{3}{9}=\\frac{1}{3}$. This increases the codeword lengths of the three $\\frac{1}{9}$ symbols by $1$ (cumulative so far: the first two have increased by $2$, the last by $1$).\n3. Now the multiset of weights is $\\left\\{\\frac{1}{3},\\frac{1}{3},\\frac{1}{3}\\right\\}$. Merge any two $\\frac{1}{3}$ to get a node of weight $\\frac{2}{3}$. This increases the codeword lengths of those two symbols by $1$.\n4. Merge $\\frac{2}{3}$ with the remaining $\\frac{1}{3}$ to form the root. This increases the codeword lengths of all three symbols under these nodes by $1$.\n\nTracing depths (codeword lengths):\n- Symbols $A$ and $B$ (both with probability $\\frac{1}{3}$) are merged together in step 3 and then merged with the other $\\frac{1}{3}$ node in step 4, so $l_{A}=l_{B}=2$.\n- The symbol among $\\{C,D,E\\}$ that did not participate in the first merge (call it $E$) is merged in steps 2 and 4, so $l_{E}=2$.\n- The two symbols that were merged first (call them $C$ and $D$) are merged in steps 1, 2, and 4, so $l_{C}=l_{D}=3$.\n\nThus the expected codeword length is\n$$\nL=\\frac{1}{3}\\cdot l_{A}+\\frac{1}{3}\\cdot l_{B}+\\frac{1}{9}\\cdot l_{C}+\\frac{1}{9}\\cdot l_{D}+\\frac{1}{9}\\cdot l_{E}\n= \\frac{1}{3}\\cdot 2+\\frac{1}{3}\\cdot 2+\\frac{1}{9}\\cdot 3+\\frac{1}{9}\\cdot 3+\\frac{1}{9}\\cdot 2.\n$$\nCompute:\n$$\nL=\\frac{2}{3}+\\frac{2}{3}+\\frac{3}{9}+\\frac{3}{9}+\\frac{2}{9}=\\frac{4}{3}+\\frac{8}{9}=\\frac{12}{9}+\\frac{8}{9}=\\frac{20}{9}.\n$$\nBy the optimality of Huffman coding, this is the minimum possible average number of bits per event. As a consistency check, the source entropy in bits is\n$$\nH=-2\\cdot \\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)-3\\cdot \\frac{1}{9}\\log_{2}\\!\\left(\\frac{1}{9}\\right)=\\frac{4}{3}\\log_{2}(3),\n$$\nand indeed $H \\leq \\frac{20}{9}$, consistent with the coding bound.",
            "answer": "$$\\boxed{\\frac{20}{9}}$$"
        },
        {
            "introduction": "While binary codes are ubiquitous, the principles of optimal coding are more general and apply to any alphabet size. This exercise challenges you to adapt your knowledge to a non-binary system by constructing an optimal *ternary* code. This practice  demonstrates the versatility of the Huffman coding framework and introduces the necessary modification for D-ary alphabets.",
            "id": "1644609",
            "problem": "A research team is developing a new data compression scheme for a digital system that uses ternary logic, meaning it processes information using three distinct states (0, 1, and 2), often called \"trits\". The data source for this system produces symbols from a five-symbol alphabet, $\\mathcal{S} = \\{s_1, s_2, s_3, s_4, s_5\\}$. The probabilities of occurrence for these symbols are given as $P(s_1) = 0.30$, $P(s_2) = 0.25$, $P(s_3) = 0.20$, $P(s_4) = 0.15$, and $P(s_5) = 0.10$.\n\nTo ensure efficient and error-free transmission, the compression scheme must use a uniquely decodable ternary code that is also instantaneous (i.e., no codeword is a prefix of another). Your task is to determine the minimum possible average codeword length for such a code, which represents the theoretical limit of compression for this source under these constraints.\n\nExpress your answer for the average length in units of trits per symbol, rounded to three significant figures.",
            "solution": "The problem asks for the minimum average length of an instantaneous (prefix-free) and uniquely decodable code for a given source. This is achieved by constructing a Huffman code. Since the code alphabet is ternary ($\\{0, 1, 2\\}$), we must use the generalized Huffman algorithm for a $D$-ary code, where $D=3$.\n\nThe goal is to find the average codeword length, $\\bar{L} = \\sum_{i=1}^{N} p_i l_i$, where $p_i$ is the probability of the $i$-th symbol and $l_i$ is its codeword length.\n\nFirst, we list the symbols and their probabilities:\n$s_1: 0.30$\n$s_2: 0.25$\n$s_3: 0.20$\n$s_4: 0.15$\n$s_5: 0.10$\n\nThe generalized Huffman algorithm requires combining $D$ symbols at each step. This process is guaranteed to terminate in a single root node if the initial number of symbols, $N$, satisfies the condition $(N-1) \\pmod{D-1} = 0$. If not, dummy symbols with zero probability must be added until the condition is met.\n\nIn this problem, we have $N=5$ symbols and a ternary code, so $D=3$. We check the condition:\n$(N-1) \\pmod{D-1} = (5-1) \\pmod{3-1} = 4 \\pmod 2 = 0$.\nThe condition is satisfied, so no dummy symbols are needed. We can proceed by combining the $D=3$ least probable symbols at each step.\n\n**Step 1: First Reduction**\nWe identify the three symbols with the lowest probabilities: $s_5(0.10)$, $s_4(0.15)$, and $s_3(0.20)$. We combine them into a single new node. The probability of this new node is the sum of the individual probabilities:\n$p_{345} = 0.10 + 0.15 + 0.20 = 0.45$.\n\nOur list of symbols (and nodes) to be coded is now reduced from five to three. The new list, sorted by probability, is:\n$s_2: 0.25$\n$s_1: 0.30$\nNode $(s_3, s_4, s_5): 0.45$\n\n**Step 2: Second Reduction**\nWe now have three items in our list. We combine all of them, as $D=3$:\n$p_{12345} = 0.25 + 0.30 + 0.45 = 1.00$.\nThis creates the root of the Huffman tree, and the construction process is complete.\n\n**Step 3: Determine Codeword Lengths**\nThe length of each symbol's codeword is determined by its depth in the tree we just constructed.\n- In the second reduction step, we combined three nodes: $s_2$, $s_1$, and the node $(s_3, s_4, s_5)$. These three form the first level of branches from the root. Therefore, the symbols $s_1$ and $s_2$ have codewords of length $l_1=1$ and $l_2=1$.\n- The node $(s_3, s_4, s_5)$ is also at depth 1. The symbols $s_3, s_4, s_5$ are children of this node, so they are one level deeper. Their depth relative to the root is $1+1=2$.\n- Thus, the codeword lengths for these symbols are $l_3=2$, $l_4=2$, and $l_5=2$.\n\nSummary of codeword lengths:\n- $l(s_1)$ for $p=0.30$ is 1.\n- $l(s_2)$ for $p=0.25$ is 1.\n- $l(s_3)$ for $p=0.20$ is 2.\n- $l(s_4)$ for $p=0.15$ is 2.\n- $l(s_5)$ for $p=0.10$ is 2.\n\n**Step 4: Calculate the Average Codeword Length**\nThe average length $\\bar{L}$ is the sum of each symbol's probability multiplied by its codeword length:\n$$ \\bar{L} = \\sum_{i=1}^{5} P(s_i) l_i $$\n$$ \\bar{L} = (0.30 \\times 1) + (0.25 \\times 1) + (0.20 \\times 2) + (0.15 \\times 2) + (0.10 \\times 2) $$\n$$ \\bar{L} = 0.30 + 0.25 + 0.40 + 0.30 + 0.20 $$\n$$ \\bar{L} = 1.45 $$\n\nThe average codeword length is $1.45$ trits per symbol. The problem asks for the answer to be rounded to three significant figures. The value $1.45$ already has three significant figures.",
            "answer": "$$\\boxed{1.45}$$"
        },
        {
            "introduction": "A deep understanding of a scientific principle involves knowing its limits and dispelling common misconceptions. This final practice poses a conjecture: do distinct symbol probabilities guarantee distinct codeword lengths in an optimal code? By searching for a counterexample in this problem , you will test this hypothesis and gain a more nuanced insight into the relationship between probability distributions and the structure of optimal codes.",
            "id": "1644600",
            "problem": "An information theory student is investigating the properties of optimal prefix codes for a source that generates symbols from the alphabet $\\mathcal{S} = \\{s_1, s_2, s_3, s_4, s_5\\}$. The student makes the following conjecture:\n\n\"If the probabilities of all five symbols in the source distribution are distinct, then the lengths of the five corresponding codewords in an optimal prefix code must also be distinct.\"\n\nYour task is to evaluate this claim. Which of the following probability distributions $P = (p(s_1), p(s_2), p(s_3), p(s_4), p(s_5))$ provides a valid counterexample to the student's conjecture? A valid counterexample must satisfy the premise of the conjecture (all symbol probabilities are distinct) and falsify the conclusion (at least two codeword lengths are the same).\n\nA) $P_A = (0.40, 0.20, 0.20, 0.10, 0.10)$\n\nB) $P_B = (0.33, 0.24, 0.19, 0.12, 0.12)$\n\nC) $P_C = (0.40, 0.30, 0.15, 0.11, 0.04)$\n\nD) $P_D = (0.50, 0.25, 0.125, 0.0625, 0.0625)$\n\nE) The conjecture is correct; none of the given distributions serve as a counterexample.",
            "solution": "To find a counterexample to the student's conjecture, we must find a probability distribution that meets two conditions:\n1.  **Satisfy the premise:** All symbol probabilities must be distinct.\n2.  **Falsify the conclusion:** When an optimal prefix code is constructed for this distribution, at least two of the codewords must have the same length.\n\nAn optimal prefix code can be constructed using Huffman's algorithm. The set of codeword lengths produced by Huffman's algorithm is unique for a given probability distribution. Let's analyze each option.\n\n**Analyze Option A: $P_A = (0.40, 0.20, 0.20, 0.10, 0.10)$**\nThe probabilities in this set are not all distinct. For example, $p(s_2) = p(s_3) = 0.20$ and $p(s_4) = p(s_5) = 0.10$. Since this distribution does not satisfy the premise of the conjecture (\"If the probabilities... are distinct\"), it cannot serve as a counterexample.\n\n**Analyze Option B: $P_B = (0.33, 0.24, 0.19, 0.12, 0.12)$**\nThe probabilities in this set are not all distinct, as $p(s_4) = p(s_5) = 0.12$. This distribution does not satisfy the premise of the conjecture, so it cannot be a counterexample.\n\n**Analyze Option D: $P_D = (0.50, 0.25, 0.125, 0.0625, 0.0625)$**\nThe probabilities in this set are not all distinct, as $p(s_4) = p(s_5) = 0.0625$. This distribution also fails to satisfy the premise of the conjecture and cannot be a counterexample.\n\n**Analyze Option C: $P_C = (0.40, 0.30, 0.15, 0.11, 0.04)$**\nThe probabilities in this set are $0.40, 0.30, 0.15, 0.11, 0.04$. All five values are distinct. This distribution satisfies the premise of the conjecture. Now, we must check if it falsifies the conclusion by constructing the Huffman code and examining the codeword lengths.\n\nWe apply Huffman's algorithm. We start with the sorted probabilities:\nInitial list: $\\{0.40, 0.30, 0.15, 0.11, 0.04\\}$\n\nStep 1: Combine the two smallest probabilities, $0.04$ and $0.11$. Their sum is $0.15$.\nThe list of probabilities becomes: $\\{0.40, 0.30, 0.15, 0.15\\}$\n\nStep 2: Combine the two smallest probabilities, which are now $0.15$ and $0.15$. Their sum is $0.30$.\nThe list of probabilities becomes: $\\{0.40, 0.30, 0.30\\}$\n\nStep 3: Combine the two smallest probabilities, $0.30$ and $0.30$. Their sum is $0.60$.\nThe list of probabilities becomes: $\\{0.60, 0.40\\}$\n\nStep 4: Combine the final two probabilities, $0.60$ and $0.40$. Their sum is $1.00$. The algorithm terminates.\n\nNow, we can trace back to build the tree and find the codeword lengths. Let's assign binary digits '0' and '1' at each merge.\n- At Step 4, $0.60$ might be '0' and $0.40$ might be '1'.\n  - The symbol with probability $0.40$ gets codeword '1'. Its length is $l_1 = 1$.\n- The node for $0.60$ came from combining $0.30$ and $0.30$ (Step 3). Let's assign '00' to one $0.30$ and '01' to the other $0.30$.\n  - The original symbol with probability $0.30$ gets codeword '01' (or '00'). Its length is $l_2 = 2$.\n- The other $0.30$ node came from combining $0.15$ and $0.15$ (Step 2). Let's assign '000' and '001'.\n  - The original symbol with probability $0.15$ gets codeword '000' (or '001'). Its length is $l_3 = 3$.\n- The other $0.15$ node came from combining $0.11$ and $0.04$ (Step 1). Let's assign '0010' and '0011'.\n  - The symbol with probability $0.11$ gets codeword '0010'. Its length is $l_4 = 4$.\n  - The symbol with probability $0.04$ gets codeword '0011'. Its length is $l_5 = 4$.\n\nThe resulting set of codeword lengths for the probabilities $(0.40, 0.30, 0.15, 0.11, 0.04)$ is $\\{1, 2, 3, 4, 4\\}$.\nSince two codewords have length 4, the codeword lengths are not all distinct.\nTherefore, the distribution $P_C$ satisfies the premise (distinct probabilities) and falsifies the conclusion (the codeword lengths are not distinct). This makes $P_C$ a valid counterexample.\n\n**Analyze Option E: The conjecture is correct...**\nSince we have found a valid counterexample in Option C, the conjecture is proven to be false, and this option is incorrect.\n\nThe only distribution that serves as a valid counterexample is $P_C$.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}