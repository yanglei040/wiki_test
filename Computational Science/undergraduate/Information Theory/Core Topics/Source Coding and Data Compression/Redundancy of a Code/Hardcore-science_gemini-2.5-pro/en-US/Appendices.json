{
    "hands_on_practices": [
        {
            "introduction": "To begin, let's practice the fundamental calculation of redundancy. This exercise uses the intuitive scenario of a traffic light to demonstrate how a simple fixed-length code, while easy to implement, introduces inefficiency when the source symbols (the light's states) have different probabilities of occurring. By calculating the difference between the code's average length, $L$, and the source's entropy, $H$, you will quantify this inefficiency, which is the redundancy $R = L - H$ .",
            "id": "1652803",
            "problem": "A sensor in a smart city's traffic management system is designed to monitor a single traffic light. At any given moment, the light can be in one of three states: Red, Yellow, or Green. Based on extensive historical traffic data, the probability of the light being Red is $p_R = 0.5$, the probability of it being Yellow is $p_Y = 0.1$, and the probability of it being Green is $p_G = 0.4$. To transmit the state of the light, the system employs a fixed-length binary encoding scheme. Since there are three distinct states, each state is assigned a unique 2-bit codeword.\n\nCalculate the redundancy of this encoding scheme. Express your answer in bits per symbol, rounded to three significant figures.",
            "solution": "Redundancy in a fixed-length source code is the difference between the average codeword length and the source entropy (in bits per symbol). The entropy of a discrete source with probabilities $\\{p_{i}\\}$ is\n$$\nH = -\\sum_{i} p_{i} \\log_{2}(p_{i}).\n$$\nWith $p_{R} = 0.5$, $p_{Y} = 0.1$, and $p_{G} = 0.4$, the entropy is\n$$\nH = -\\left[0.5 \\log_{2}(0.5) + 0.1 \\log_{2}(0.1) + 0.4 \\log_{2}(0.4)\\right].\n$$\nUsing $\\log_{2}(0.5) = -1$, and evaluating the others numerically,\n$$\nH = -\\left[0.5(-1) + 0.1 \\log_{2}(0.1) + 0.4 \\log_{2}(0.4)\\right]\n= 0.5 - 0.1 \\log_{2}(0.1) - 0.4 \\log_{2}(0.4).\n$$\nWith $\\log_{2}(0.1) \\approx -3.321928094$ and $\\log_{2}(0.4) \\approx -1.321928095$,\n$$\nH \\approx 0.5 + 0.332192809 + 0.528771238 = 1.360964047 \\text{ bits/symbol}.\n$$\nA fixed-length 2-bit code has average length\n$$\nL = 2 \\text{ bits/symbol}.\n$$\nTherefore, the redundancy is\n$$\nR = L - H \\approx 2 - 1.360964047 = 0.639035953 \\text{ bits/symbol}.\n$$\nRounded to three significant figures, the redundancy is $0.639$ bits per symbol.",
            "answer": "$$\\boxed{0.639}$$"
        },
        {
            "introduction": "A code's performance is intrinsically linked to the statistical properties of the source it encodes. This next problem explores what happens when there is a mismatch between the source statistics a code was designed for and the actual statistics it encounters, a common issue in real-world systems due to faults or changing conditions. You will calculate the redundancy for a system where the sensor's behavior has changed, highlighting how a once-optimal code can become highly inefficient .",
            "id": "1652808",
            "problem": "A digital monitoring system is designed to transmit the state of a remote sensor. The sensor can be in one of 8 distinct states, which we can label as $s_1, s_2, \\dots, s_8$. The communication protocol uses a fixed-length binary code to represent these states. This code was designed under the initial assumption that all 8 states occur with equal probability.\n\nAfter deployment, a persistent partial failure in the sensor's circuitry alters its behavior. It is observed that the sensor now only ever reports state $s_1$ with a probability of $p_1 = 3/4$, or state $s_2$ with a probability of $p_2 = 1/4$. The other six states ($s_3$ through $s_8$) are never reported.\n\nGiven that the original fixed-length code is still in use, calculate the redundancy of this coding scheme with respect to the new, faulty probability distribution of the sensor.\n\nExpress your answer as a closed-form analytic expression in units of bits per symbol. Your expression may use logarithms, which should be specified in base 2.",
            "solution": "A fixed-length binary code for an alphabet of size 8 requires exactly $\\lceil \\log_{2}(8) \\rceil = 3$ bits per symbol. Therefore, the average codeword length remains\n$$\nL = 3 \\text{ bits/symbol}.\n$$\nWith the faulty sensor, the source distribution is $p_{1} = \\frac{3}{4}$, $p_{2} = \\frac{1}{4}$, and $p_{3}=\\cdots=p_{8}=0$. The Shannon entropy (in bits per symbol) is\n$$\nH = -\\sum_{i=1}^{8} p_{i} \\log_{2}(p_{i}) = -\\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{4}\\right) - \\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right),\n$$\nsince the zero-probability terms contribute $0$. Simplifying,\n$$\nH = -\\frac{3}{4}\\bigl(\\log_{2} 3 - \\log_{2} 4\\bigr) - \\frac{1}{4}\\bigl(\\log_{2} 1 - \\log_{2} 4\\bigr)\n= -\\frac{3}{4}\\log_{2} 3 + \\frac{3}{2} + \\frac{1}{2}\n= 2 - \\frac{3}{4}\\log_{2} 3.\n$$\nRedundancy is defined as the excess average code length over the source entropy:\n$$\nR = L - H = 3 - \\left(2 - \\frac{3}{4}\\log_{2} 3\\right) = 1 + \\frac{3}{4}\\log_{2} 3 \\text{ bits/symbol}.\n$$",
            "answer": "$$\\boxed{1+\\frac{3}{4}\\log_{2} 3}$$"
        },
        {
            "introduction": "Having learned to calculate and analyze redundancy, we now shift our perspective from analysis to optimization. For a given simple code, what source characteristics would make it maximally efficient? This final exercise asks you to find the source probability that minimizes redundancy, revealing the deep connection between coding efficiency and the principle of maximum entropy .",
            "id": "1652849",
            "problem": "Consider a simple digital sensor that monitors an environmental condition. The sensor has two possible output states, which we will denote as '0' and '1'. This stream of outputs can be modeled as a Discrete Memoryless Source (DMS). For this source, the probability of emitting a '0' is $p$, and the probability of emitting a '1' is $1-p$, where $p$ is a value strictly between 0 and 1.\nThe output from this sensor is directly transmitted using a simple binary code where the source symbol '0' is encoded as the codeword '0' and the source symbol '1' is encoded as the codeword '1'.\n\nThe redundancy of a code for a source is a measure of its inefficiency. It is defined as the difference between the average codeword length per source symbol and the entropy of the source. For this problem, the entropy should be calculated in bits (using base-2 logarithm).\n\nDetermine the exact value of the probability $p$ that minimizes the redundancy of this encoding scheme.",
            "solution": "A binary discrete memoryless source with symbol probabilities $P(0)=p$ and $P(1)=1-p$ is encoded with a fixed-length binary code where each symbol maps to one bit. The average codeword length per source symbol is therefore\n$$\nL=1.\n$$\nThe source entropy in bits is\n$$\nH(p)=-p\\log_{2}(p)-(1-p)\\log_{2}(1-p).\n$$\nThe redundancy is defined as\n$$\nR(p)=L-H(p)=1+\\;p\\log_{2}(p)+(1-p)\\log_{2}(1-p).\n$$\nMinimizing $R(p)$ is equivalent to maximizing $H(p)$. Differentiate $H(p)$ with respect to $p$:\n$$\n\\frac{dH}{dp}=-\\left[\\log_{2}(p)+\\frac{1}{\\ln 2}\\right]+\\left[\\log_{2}(1-p)+\\frac{1}{\\ln 2}\\right]\n=\\log_{2}(1-p)-\\log_{2}(p)=\\log_{2}\\!\\left(\\frac{1-p}{p}\\right).\n$$\nSet the derivative to zero to find critical points:\n$$\n\\log_{2}\\!\\left(\\frac{1-p}{p}\\right)=0 \\;\\;\\Longrightarrow\\;\\; \\frac{1-p}{p}=1 \\;\\;\\Longrightarrow\\;\\; p=\\frac{1}{2}.\n$$\nCheck the second derivative to confirm a maximum of $H(p)$ (and thus a minimum of $R(p)$):\n$$\n\\frac{d^{2}H}{dp^{2}}=-\\frac{1}{(1-p)\\ln 2}-\\frac{1}{p\\ln 2}0 \\quad \\text{for } p\\in(0,1),\n$$\nso $p=\\frac{1}{2}$ maximizes $H(p)$ and therefore minimizes $R(p)$. Hence, the redundancy is minimized at $p=\\frac{1}{2}$.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        }
    ]
}