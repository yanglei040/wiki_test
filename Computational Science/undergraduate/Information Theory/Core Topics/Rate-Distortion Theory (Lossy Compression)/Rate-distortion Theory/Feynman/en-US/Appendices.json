{
    "hands_on_practices": [
        {
            "introduction": "To understand the trade-off between rate and distortion, a natural starting point is to establish the function's boundaries. This exercise explores the scenario of zero-rate compression, where no budget is available to transmit information about the source. By calculating the minimum possible distortion when the reconstruction must be a single constant value, you will determine the maximum distortion $D_{\\max}$ for a Gaussian source, which is equivalent to the source's own variance $\\sigma^2$ . This value serves as a crucial benchmark, as any practical compression system must achieve a lower distortion to be considered effective.",
            "id": "1652592",
            "problem": "In the field of data compression, rate-distortion theory provides the fundamental limits on the trade-off between the compression rate and the fidelity of the reconstructed data.\n\nConsider a memoryless continuous source that generates independent and identically distributed random variables $X$. Each variable follows a Gaussian (normal) distribution with a mean of zero and a variance of $\\sigma^2$, denoted as $X \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nA simple compression system is designed to represent this source. The performance of this system is evaluated using the squared-error distortion measure, where the average distortion $D$ is given by the expected value of the squared difference between the original signal and its reconstruction: $D = E[(X - \\hat{X})^2]$.\n\nDue to extreme bandwidth constraints, the system is designed to operate at a rate of zero bits per source symbol. This \"zero-rate\" encoding means that no information about the specific value of any given a sample $x$ can be transmitted. Consequently, the decoder must produce the same constant estimate, let's call it $\\hat{x}_c$, for every source symbol it is asked to reconstruct.\n\nTo achieve the best possible performance under this severe constraint, the constant estimate $\\hat{x}_c$ must be chosen to minimize the average distortion $D$.\n\nCalculate this minimum achievable average distortion for the zero-rate system. Express your final answer as an analytic expression in terms of the source parameter $\\sigma$.",
            "solution": "The problem asks for the minimum average distortion for a zero-rate encoder, which uses a constant reconstruction value $\\hat{x}_c$ for a source $X \\sim \\mathcal{N}(0, \\sigma^2)$. The distortion measure is the mean squared error, $D = E[(X - \\hat{x}_c)^2]$. Our goal is to find the optimal constant $\\hat{x}_c$ that minimizes this distortion, and then calculate the value of this minimum distortion.\n\nFirst, let's express the average distortion $D$ as a function of the constant estimate $\\hat{x}_c$.\n$$D(\\hat{x}_c) = E[(X - \\hat{x}_c)^2]$$\nWe can expand the squared term inside the expectation:\n$$D(\\hat{x}_c) = E[X^2 - 2X\\hat{x}_c + \\hat{x}_c^2]$$\nUsing the linearity of the expectation operator, we can write:\n$$D(\\hat{x}_c) = E[X^2] - E[2X\\hat{x}_c] + E[\\hat{x}_c^2]$$\nSince $\\hat{x}_c$ is a constant, we can pull it out of the expectations:\n$$D(\\hat{x}_c) = E[X^2] - 2\\hat{x}_c E[X] + \\hat{x}_c^2$$\nTo find the value of $\\hat{x}_c$ that minimizes $D$, we can treat $D$ as a function of $\\hat{x}_c$ and find its minimum by taking the derivative with respect to $\\hat{x}_c$ and setting it to zero.\n$$\\frac{dD(\\hat{x}_c)}{d\\hat{x}_c} = \\frac{d}{d\\hat{x}_c} (E[X^2] - 2\\hat{x}_c E[X] + \\hat{x}_c^2)$$\nThe term $E[X^2]$ is a constant with respect to $\\hat{x}_c$, so its derivative is zero.\n$$\\frac{dD(\\hat{x}_c)}{d\\hat{x}_c} = 0 - 2E[X] + 2\\hat{x}_c$$\nSetting the derivative to zero to find the minimum:\n$$-2E[X] + 2\\hat{x}_c = 0$$\n$$\\hat{x}_c = E[X]$$\nThis shows that the optimal constant estimate that minimizes the mean squared error is the mean (expected value) of the source random variable.\n\nFor the given source, $X \\sim \\mathcal{N}(0, \\sigma^2)$, the mean is $E[X] = 0$. Therefore, the optimal constant reconstruction is $\\hat{x}_c = 0$.\n\nNow we substitute this optimal value back into the distortion formula to find the minimum achievable distortion:\n$$D_{min} = E[(X - 0)^2] = E[X^2]$$\nWe need to find the value of $E[X^2]$. We can use the definition of variance:\n$$\\text{Var}(X) = E[X^2] - (E[X])^2$$\nRearranging this to solve for $E[X^2]$:\n$$E[X^2] = \\text{Var}(X) + (E[X])^2$$\nFor our source, we are given that the variance is $\\text{Var}(X) = \\sigma^2$ and the mean is $E[X] = 0$.\nSubstituting these values:\n$$E[X^2] = \\sigma^2 + 0^2 = \\sigma^2$$\nThus, the minimum achievable average distortion for the zero-rate system is $\\sigma^2$. This is the distortion obtained by always guessing the mean of the source, and it represents the inherent uncertainty or variance of the source itself. No amount of compression can result in an average distortion greater than this value, as this level of distortion can be achieved without transmitting any information.",
            "answer": "$$\\boxed{\\sigma^{2}}$$"
        },
        {
            "introduction": "After establishing the maximum distortion at zero rate, we now examine a different extreme: a source with zero uncertainty. This practice considers a deterministic source, one that always produces the same value, making it perfectly predictable. By applying the fundamental definition of mutual information, $I(X; \\hat{X})$, you will reveal why the rate-distortion function $R(D)$ is identically zero for such a source . This result powerfully illustrates that a compression rate is only necessary when there is randomness or \"information\" to encode.",
            "id": "1652578",
            "problem": "In the field of data compression, rate-distortion theory provides the theoretical limits for lossy compression. Consider a scenario involving a faulty environmental sensor. This sensor is supposed to monitor a stable chemical process, but it has become stuck, consistently outputting a single, constant real value, $c$. This system can be modeled as a discrete memoryless source $X$ with a source alphabet $\\mathcal{X} = \\{c\\}$ and a corresponding probability mass function $p(X=c) = 1$.\n\nWe want to compress the signal from this sensor. The quality of the compressed-and-reconstructed signal, denoted by $\\hat{X}$, is assessed using a non-negative distortion measure, $d(x, \\hat{x})$. This measure has the fundamental property that a perfect reconstruction yields zero distortion, i.e., $d(c, c) = 0$.\n\nThe rate-distortion function, $R(D)$, defines the minimum achievable data rate (in bits per symbol) for a given maximum tolerable average distortion, $D$. It is formally given by:\n$$R(D) = \\min_{p(\\hat{x}|x) : \\mathbb{E}[d(X, \\hat{X})] \\le D} I(X; \\hat{X})$$\nwhere $I(X; \\hat{X})$ is the mutual information between the source $X$ and the reconstruction $\\hat{X}$. The minimization is performed over all conditional probability distributions $p(\\hat{x}|x)$ that satisfy the distortion constraint.\n\nFor this \"stuck\" sensor, determine the rate-distortion function $R(D)$ for any non-negative distortion level $D \\ge 0$. Select the option that correctly describes this function.\n\nA. $R(D) = 0$ for all $D \\geq 0$.\n\nB. $R(D) = 1$ for $D = 0$ and $R(D) = 0$ for $D > 0$.\n\nC. $R(D)$ is a strictly decreasing positive function for $D \\geq 0$.\n\nD. The function cannot be determined without knowing the specific form of the distortion measure $d(x, \\hat{x})$ for $\\hat{x} \\neq c$.",
            "solution": "We model the source as $X=c$ with $p(X=c)=1$. For any conditional distribution $p(\\hat{x}|x)$, the joint distribution is $p(x,\\hat{x})=\\mathbf{1}\\{x=c\\}\\,p(\\hat{x}|c)$ and the marginal of $\\hat{X}$ is $p(\\hat{x})=p(\\hat{x}|c)$.\n\nUsing the definition of mutual information,\n$$\nI(X;\\hat{X})=\\sum_{x,\\hat{x}} p(x,\\hat{x}) \\ln \\frac{p(x,\\hat{x})}{p(x)p(\\hat{x})}.\n$$\nSince only $x=c$ has positive probability,\n$$\nI(X;\\hat{X})=\\sum_{\\hat{x}} p(\\hat{x}|c)\\,\\ln \\frac{p(\\hat{x}|c)}{p(X=c)\\,p(\\hat{x})}\n=\\sum_{\\hat{x}} p(\\hat{x}|c)\\,\\ln \\frac{p(\\hat{x}|c)}{1\\cdot p(\\hat{x})}.\n$$\nBut $p(\\hat{x})=p(\\hat{x}|c)$, so the ratio inside the logarithm is $1$, hence $I(X;\\hat{X})=0$ for any choice of $p(\\hat{x}|x)$.\n\nThe distortion constraint is\n$$\n\\mathbb{E}[d(X,\\hat{X})]=\\mathbb{E}[d(c,\\hat{X})].\n$$\nChoosing $\\hat{X}=c$ almost surely (i.e., $p(\\hat{x}|c)=\\mathbf{1}\\{\\hat{x}=c\\}$) yields\n$$\n\\mathbb{E}[d(c,\\hat{X})]=d(c,c)=0\\le D\n$$\nfor every $D\\ge 0$, so the feasible set is nonempty for all $D\\ge 0$.\n\nThe rate-distortion function is\n$$\nR(D)=\\min_{p(\\hat{x}|x):\\,\\mathbb{E}[d(X,\\hat{X})]\\le D} I(X;\\hat{X}).\n$$\nSince $I(X;\\hat{X})=0$ for any $p(\\hat{x}|x)$ and mutual information is nonnegative, the minimum over the feasible set is exactly $0$ for all $D\\ge 0$.\n\nTherefore, the correct description is $R(D)=0$ for all $D\\ge 0$, which corresponds to option A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "While understanding the conceptual limits of $R(D)$ is crucial, it is equally important to learn how this function is computed for a general discrete source. This exercise provides a hands-on walk-through of the Blahut-Arimoto algorithm, an elegant iterative procedure that numerically finds the optimal trade-off between rate and distortion. By executing a single iteration of the algorithm, you will gain concrete experience with the mechanics of optimizing a \"test channel\" to find a point on the theoretical rate-distortion curve .",
            "id": "1652561",
            "problem": "Consider a data compression system designed for a discrete memoryless source $X$. The source has an alphabet $\\mathcal{X} = \\{0, 1\\}$ and follows a Bernoulli distribution with $P(X=1) = p$ and $P(X=0) = 1-p$. The compressed data is represented using a reproduction alphabet $\\mathcal{\\hat{X}} = \\{0, 1, 2\\}$. The quality of the reproduction is measured by a distortion matrix $d(x, \\hat{x})$, given by:\n$$\nd = \\begin{pmatrix} d(0,0) & d(0,1) & d(0,2) \\\\ d(1,0) & d(1,1) & d(1,2) \\end{pmatrix} = \\begin{pmatrix} 0 & \\delta_1 & \\delta_2 \\\\ \\delta_3 & 0 & \\delta_4 \\end{pmatrix}\n$$\nwhere $\\delta_1, \\delta_2, \\delta_3, \\delta_4$ are known positive constants.\n\nTo find the optimal trade-off between compression rate and distortion, the Blahut-Arimoto algorithm is employed. For a fixed slope parameter $s < 0$, this algorithm iteratively refines a conditional probability distribution (a \"test channel\") $q(\\hat{x}|x)$ and a reproduction probability distribution $q(\\hat{x})$. The update rules for an iteration, from step $k$ to $k+1$, are as follows:\n\n1.  Update the test channel:\n    $$q_{k+1}(\\hat{x}|x) = \\frac{q_k(\\hat{x}) \\exp(s \\cdot d(x, \\hat{x}))}{\\sum_{\\hat{x}' \\in \\mathcal{\\hat{X}}} q_k(\\hat{x}') \\exp(s \\cdot d(x, \\hat{x}'))}$$\n\n2.  Update the reproduction distribution:\n    $$q_{k+1}(\\hat{x}) = \\sum_{x \\in \\mathcal{X}} P(x) q_{k+1}(\\hat{x}|x)$$\n\nThe algorithm is initialized at step $k=0$ with a uniform reproduction distribution, i.e., $q_0(\\hat{x}) = 1/3$ for all $\\hat{x} \\in \\mathcal{\\hat{X}}$.\n\nYour task is to perform one full iteration of the algorithm. Determine the explicit analytical expression for the updated reproduction probability $q_1(\\hat{x}=0)$. Express your answer as a single closed-form analytic expression in terms of $p, s, \\delta_1, \\delta_2, \\delta_3,$ and $\\delta_4$.",
            "solution": "We follow the Blahut-Arimoto update rules exactly as given. The initial reproduction distribution is uniform, so for all $\\hat{x} \\in \\{0,1,2\\}$ we have $q_{0}(\\hat{x}) = \\frac{1}{3}$.\n\nFirst, compute the denominators for the test-channel update for each source symbol $x$:\nFor $x=0$, using $d(0,0)=0$, $d(0,1)=\\delta_{1}$, $d(0,2)=\\delta_{2}$,\n$$\nD_{0}=\\sum_{\\hat{x}\\in\\{0,1,2\\}} q_{0}(\\hat{x}) \\exp\\!\\big(s\\, d(0,\\hat{x})\\big)\n= \\frac{1}{3}\\big( \\exp(0) + \\exp(s \\delta_{1}) + \\exp(s \\delta_{2}) \\big)\n= \\frac{1}{3}\\big( 1 + \\exp(s \\delta_{1}) + \\exp(s \\delta_{2}) \\big).\n$$\nFor $x=1$, using $d(1,0)=\\delta_{3}$, $d(1,1)=0$, $d(1,2)=\\delta_{4}$,\n$$\nD_{1}=\\sum_{\\hat{x}\\in\\{0,1,2\\}} q_{0}(\\hat{x}) \\exp\\!\\big(s\\, d(1,\\hat{x})\\big)\n= \\frac{1}{3}\\big( \\exp(s \\delta_{3}) + \\exp(0) + \\exp(s \\delta_{4}) \\big)\n= \\frac{1}{3}\\big( \\exp(s \\delta_{3}) + 1 + \\exp(s \\delta_{4}) \\big).\n$$\n\nNow update the test channel for $\\hat{x}=0$:\nFor $x=0$,\n$$\nq_{1}(0 \\mid 0) = \\frac{q_{0}(0)\\exp\\!\\big(s\\, d(0,0)\\big)}{D_{0}}\n= \\frac{\\frac{1}{3}\\exp(0)}{\\frac{1}{3}\\big(1 + \\exp(s \\delta_{1}) + \\exp(s \\delta_{2})\\big)}\n= \\frac{1}{1 + \\exp(s \\delta_{1}) + \\exp(s \\delta_{2})}.\n$$\nFor $x=1$,\n$$\nq_{1}(0 \\mid 1) = \\frac{q_{0}(0)\\exp\\!\\big(s\\, d(1,0)\\big)}{D_{1}}\n= \\frac{\\frac{1}{3}\\exp(s \\delta_{3})}{\\frac{1}{3}\\big(\\exp(s \\delta_{3}) + 1 + \\exp(s \\delta_{4})\\big)}\n= \\frac{\\exp(s \\delta_{3})}{\\exp(s \\delta_{3}) + 1 + \\exp(s \\delta_{4})}.\n$$\n\nFinally, update the reproduction probability using the source distribution $P(X=1)=p$, $P(X=0)=1-p$:\n$$\nq_{1}(\\hat{x}=0) = \\sum_{x\\in\\{0,1\\}} P(x)\\, q_{1}(0 \\mid x)\n= (1-p)\\, \\frac{1}{1 + \\exp(s \\delta_{1}) + \\exp(s \\delta_{2})}\n+ p\\, \\frac{\\exp(s \\delta_{3})}{\\exp(s \\delta_{3}) + 1 + \\exp(s \\delta_{4})}.\n$$\nThis is the desired closed-form expression for the updated reproduction probability of symbol $0$ after one full iteration.",
            "answer": "$$\\boxed{(1-p)\\,\\frac{1}{1+\\exp(s\\delta_{1})+\\exp(s\\delta_{2})}\\;+\\;p\\,\\frac{\\exp(s\\delta_{3})}{\\exp(s\\delta_{3})+1+\\exp(s\\delta_{4})}}$$"
        }
    ]
}