{
    "hands_on_practices": [
        {
            "introduction": "We begin our hands-on exploration of rate-distortion theory with the most fundamental scenario: a memoryless binary source where each symbol is equally likely. This situation, representing maximum uncertainty or entropy, serves as a crucial benchmark. By calculating the minimum data rate required for a given level of distortion, we can clearly see the direct trade-off between compression and fidelity in its simplest form .",
            "id": "1606598",
            "problem": "A digital sensor monitors a simple binary phenomenon, producing a sequence of independent and identically distributed symbols, '0' and '1'. The probability of the sensor outputting a '1' is given as $p = \\frac{1}{2}$. This stream of data needs to be compressed for efficient storage. The compression scheme is lossy, meaning the reconstructed data may not be a perfect replica of the original. The distortion between an original symbol and its reconstructed counterpart is quantified as follows: the distortion is 0 if the symbols are the same, and 1 if they are different.\n\nYour task is to calculate the minimum theoretical data rate, in bits per symbol, required to encode this source if the average distortion per symbol is not allowed to exceed $D = 0.15$.\n\nProvide your final answer rounded to three significant figures.",
            "solution": "We have a memoryless binary source with $P(X=1)=\\frac{1}{2}$ and Hamming distortion measure $d(x,\\hat{x})=\\mathbf{1}\\{x\\neq \\hat{x}\\}$. For a Bernoulli$\\left(\\frac{1}{2}\\right)$ source with Hamming distortion, the rate-distortion function in bits per symbol is\n$$\nR(D)=1-H_b(D), \\quad 0\\leq D\\leq \\frac{1}{2},\n$$\nwhere the binary entropy function is\n$$\nH_b(D)=-D\\log_{2}D-(1-D)\\log_{2}(1-D).\n$$\nSince $D=0.15\\in[0,\\frac{1}{2}]$, this formula applies. Substituting $D=0.15$,\n$$\nH_b(0.15)=-0.15\\log_{2}(0.15)-0.85\\log_{2}(0.85).\n$$\nUsing $\\log_{2}x=\\frac{\\ln x}{\\ln 2}$, we compute\n$$\n\\log_{2}(0.15)\\approx -2.736965594,\\quad \\log_{2}(0.85)\\approx -0.234465254,\n$$\nthus\n$$\nH_b(0.15)\\approx -0.15(-2.736965594)-0.85(-0.234465254)\\approx 0.609840305.\n$$\nTherefore,\n$$\nR(0.15)=1-H_b(0.15)\\approx 1-0.609840305\\approx 0.390159695.\n$$\nRounded to three significant figures, the minimum rate is $0.390$ bits per symbol.",
            "answer": "$$\\boxed{0.390}$$"
        },
        {
            "introduction": "Building upon the symmetric case, we now consider a more realistic scenario where the source data exhibits a statistical bias. When one symbol is more probable than another, the source has inherent redundancy, which can be exploited for more efficient compression. This practice demonstrates how to apply the general rate-distortion formula, which accounts for the source's own entropy, to determine the compression limit for a non-uniform Bernoulli source .",
            "id": "1606619",
            "problem": "A discrete memoryless source produces symbols from the binary alphabet $\\mathcal{X} = \\{0, 1\\}$. The source statistics are such that the symbol '0' is seven times more likely to be generated than the symbol '1'. The output of this source is to be compressed and transmitted. The quality of the reconstruction at the receiver is evaluated using a Hamming distortion measure, where the distortion $d(x, \\hat{x})$ between a source symbol $x$ and its reconstruction $\\hat{x}$ is 1 if $x \\neq \\hat{x}$, and 0 if $x = \\hat{x}$.\n\nDetermine the rate-distortion function $R(D)$ for this source at an average distortion level of $D = 0.1$. The rate-distortion function gives the minimum number of bits per symbol required to represent the source while maintaining an average distortion no greater than $D$.\n\nExpress your final answer for the rate in units of bits per symbol, rounded to three significant figures.",
            "solution": "Let the source be Bernoulli with $P(X=1)=p$ and $P(X=0)=1-p$. The statement “0 is seven times more likely than 1” implies\n$$\nP(X=0)=7P(X=1), \\quad P(X=0)+P(X=1)=1 \\;\\Rightarrow\\; P(X=1)=p=\\frac{1}{8},\\; P(X=0)=\\frac{7}{8}.\n$$\nFor a binary memoryless source under Hamming distortion, the rate-distortion function is\n$$\nR(D)=\\begin{cases}\nH_{b}(p)-H_{b}(D),  0\\leq D\\leq \\min\\{p,1-p\\},\\\\\n0,  D\\geq \\min\\{p,1-p\\},\n\\end{cases}\n$$\nwhere $H_{b}(x)=-x\\log_{2}x-(1-x)\\log_{2}(1-x)$ is the binary entropy in bits. Here $\\min\\{p,1-p\\}=\\min\\{\\frac{1}{8},\\frac{7}{8}\\}=\\frac{1}{8}=0.125$, and the target distortion is $D=0.10.125$, so\n$$\nR(D)=H_{b}\\!\\left(\\frac{1}{8}\\right)-H_{b}(0.1).\n$$\nCompute each term:\n$$\nH_{b}\\!\\left(\\frac{1}{8}\\right)=-\\frac{1}{8}\\log_{2}\\!\\left(\\frac{1}{8}\\right)-\\frac{7}{8}\\log_{2}\\!\\left(\\frac{7}{8}\\right)\n=\\frac{1}{8}\\cdot 3-\\frac{7}{8}\\left(\\log_{2}7-3\\right)\\approx 0.543564443199596,\n$$\nusing $\\log_{2}\\!\\left(\\frac{1}{8}\\right)=-3$ and $\\log_{2}\\!\\left(\\frac{7}{8}\\right)=\\log_{2}7-3$. Also,\n$$\nH_{b}(0.1)=-0.1\\log_{2}(0.1)-0.9\\log_{2}(0.9)\\approx 0.468995593589281.\n$$\nTherefore,\n$$\nR(0.1)=H_{b}\\!\\left(\\frac{1}{8}\\right)-H_{b}(0.1)\\approx 0.543564443199596-0.468995593589281\\approx 0.074568849610315.\n$$\nRounding to three significant figures gives $R(0.1)\\approx 0.0746$ bits per symbol.",
            "answer": "$$\\boxed{0.0746}$$"
        },
        {
            "introduction": "This final practice exercise challenges you to synthesize concepts from both probability and information theory. We will analyze a new source that is derived from the combination of two independent data streams through a logical XOR operation. Before you can determine the rate-distortion function, you must first derive the statistical properties of this new composite source, providing a comprehensive test of your problem-solving skills .",
            "id": "1606664",
            "problem": "Two independent, memoryless binary data streams, represented by sequences of random variables $\\{X_{1,i}\\}$ and $\\{X_{2,i}\\}$, are generated by two separate processes. For each time step $i$, the random variable $X_{1,i}$ follows a Bernoulli distribution with $P(X_{1,i}=1) = p = 1/3$, and the random variable $X_{2,i}$ follows a Bernoulli distribution with $P(X_{2,i}=1) = q = 1/4$.\n\nA new data stream $\\{Y_i\\}$ is created by performing a bitwise exclusive OR (XOR) operation on the two source streams, such that $Y_i = X_{1,i} \\oplus X_{2,i}$ for each time step $i$. This new stream $\\{Y_i\\}$ constitutes a memoryless binary source.\n\nWe wish to compress this new source $Y$ and transmit it. The quality of the reconstructed signal $\\hat{Y}$ is evaluated using the Hamming distortion measure, defined as $d(y, \\hat{y}) = 0$ if $y = \\hat{y}$ and $d(y, \\hat{y}) = 1$ if $y \\neq \\hat{y}$. The rate-distortion function, $R(D)$, quantifies the minimum number of bits per symbol required to represent the source $Y$ such that the expected distortion $E[d(Y, \\hat{Y})]$ is no more than a given distortion level $D$.\n\nDetermine the rate-distortion function $R(D)$ for the source $Y$. The final answer should be given as an expression in terms of $D$. Use the base-2 logarithm in your calculations.",
            "solution": "We first determine the distribution of the XOR source. Since $X_{1,i}$ and $X_{2,i}$ are independent with $P(X_{1,i}=1)=p=\\frac{1}{3}$ and $P(X_{2,i}=1)=q=\\frac{1}{4}$, the XOR output is $Y_{i}=X_{1,i}\\oplus X_{2,i}$ with\n$$\nP(Y_{i}=1)=P(X_{1,i}\\neq X_{2,i})=p(1-q)+(1-p)q=p+q-2pq.\n$$\nSubstituting $p=\\frac{1}{3}$ and $q=\\frac{1}{4}$ gives\n$$\nP(Y=1)=\\frac{1}{3}+\\frac{1}{4}-2\\cdot\\frac{1}{3}\\cdot\\frac{1}{4}=\\frac{4}{12}+\\frac{3}{12}-\\frac{2}{12}=\\frac{5}{12},\n$$\nso $Y$ is a Bernoulli source with parameter $r=\\frac{5}{12}$, and $P(Y=0)=1-r=\\frac{7}{12}$.\n\nFor a binary memoryless source $Y\\sim\\text{Bernoulli}(r)$ under Hamming distortion, the rate-distortion function is well known and can be derived as follows. The Shannon lower bound for discrete memoryless sources with Hamming distortion is tight for binary sources and is achieved by a binary symmetric test channel with crossover probability $D$. Specifically, consider a test channel $P_{\\hat{Y}|Y}$ that is a binary symmetric channel with crossover probability $D$, so that $P(\\hat{Y}\\neq Y)=D$. Then the mutual information between $Y$ and $\\hat{Y}$ is\n$$\nI(Y;\\hat{Y})=H(\\hat{Y})-H(\\hat{Y}|Y)=H(\\hat{Y})-H_{b}(D),\n$$\nwhere $H_{b}(x)=-x\\log_{2}x-(1-x)\\log_{2}(1-x)$ is the binary entropy function. With a symmetric test channel applied to a Bernoulli$(r)$ input, $\\hat{Y}$ is Bernoulli with parameter $r\\star D=r(1-D)+(1-r)D=r+D-2rD$. When $D\\leq \\min\\{r,1-r\\}$ and $r\\leq \\frac{1}{2}$, the maximizing output entropy is $H(\\hat{Y})=H_{b}(r)$, and the minimum mutual information over all test channels meeting the distortion constraint $E[d(Y,\\hat{Y})]\\leq D$ is attained by the binary symmetric channel with crossover $D$, yielding\n$$\nR(D)=\\min_{P_{\\hat{Y}|Y}: E[d(Y,\\hat{Y})]\\leq D} I(Y;\\hat{Y})=H_{b}(r)-H_{b}(D), \\quad 0\\leq D\\leq \\min\\{r,1-r\\}.\n$$\nFor larger distortions, guessing the more likely symbol yields distortion $\\min\\{r,1-r\\}$ with zero rate, so $R(D)=0$ for $D\\geq \\min\\{r,1-r\\}$. In our case $r=\\frac{5}{12}\\frac{1}{2}$, hence $\\min\\{r,1-r\\}=r=\\frac{5}{12}$. Therefore,\n$$\nR(D)=H_{b}\\!\\left(\\frac{5}{12}\\right)-H_{b}(D), \\quad 0\\leq D\\leq \\frac{5}{12},\n$$\nand\n$$\nR(D)=0, \\quad D\\geq \\frac{5}{12}.\n$$\nWriting $H_{b}$ explicitly using base-$2$ logarithms,\n$$\nH_{b}\\!\\left(\\frac{5}{12}\\right)=-\\frac{5}{12}\\log_{2}\\!\\left(\\frac{5}{12}\\right)-\\frac{7}{12}\\log_{2}\\!\\left(\\frac{7}{12}\\right), \\quad\nH_{b}(D)=-D\\log_{2}(D)-(1-D)\\log_{2}(1-D).\n$$\nCombining these yields the desired piecewise expression for $R(D)$ in terms of $D$.",
            "answer": "$$\\boxed{R(D)=\\begin{cases}-\\dfrac{5}{12}\\log_{2}\\!\\left(\\dfrac{5}{12}\\right)-\\dfrac{7}{12}\\log_{2}\\!\\left(\\dfrac{7}{12}\\right)+D\\log_{2}(D)+(1-D)\\log_{2}(1-D),  0\\leq D\\leq \\dfrac{5}{12},\\\\[6pt] 0,  D\\geq \\dfrac{5}{12}.\\end{cases}}$$"
        }
    ]
}