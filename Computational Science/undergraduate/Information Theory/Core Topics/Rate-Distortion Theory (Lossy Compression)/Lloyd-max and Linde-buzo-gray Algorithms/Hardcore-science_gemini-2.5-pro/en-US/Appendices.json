{
    "hands_on_practices": [
        {
            "introduction": "An optimal scalar quantizer must satisfy two fundamental conditions. This first practice isolates one of them: the nearest neighbor condition. We explore how to determine the optimal decision boundaries when the reconstruction levels are already fixed, a crucial step in minimizing the mean squared error. This exercise  reveals that for the squared error distortion metric, the ideal boundary is simply the midpoint between the two reconstruction levels.",
            "id": "1637665",
            "problem": "An analog sensor produces a voltage signal, which can be modeled as a continuous random variable $X$ with a uniform probability distribution over the interval $[-3, 3]$. To digitize this signal for processing, a simple 1-bit scalar quantizer is employed. This quantizer maps the input voltage to one of two predefined reconstruction levels. Due to hardware constraints, the reconstruction levels are asymmetrically set to $y_1 = -2.5$ and $y_2 = 1.8$.\n\nAssuming the design goal is to minimize the mean squared error (MSE) distortion, determine the optimal decision boundary $b_1$ that separates the two quantization regions. The decision rule is as follows: if the input signal $x$ is less than or equal to $b_1$, it is mapped to $y_1$; otherwise, it is mapped to $y_2$. The domain of the random variable $X$ is the interval $[-3, 3]$, so the two quantization regions are $[-3, b_1]$ and $(b_1, 3]$.\n\nExpress your answer as a single real number.",
            "solution": "The problem asks for the optimal decision boundary $b_1$ for a 1-bit quantizer that minimizes the mean squared error (MSE). The quantizer maps an input signal $x$ to one of two reconstruction levels, $y_1 = -2.5$ or $y_2 = 1.8$. The input signal $X$ is a random variable uniformly distributed on $[-3, 3]$.\n\nThe probability density function (PDF) of the uniform distribution on the interval $[a, b]$ is given by $p(x) = \\frac{1}{b-a}$ for $x \\in [a, b]$ and $p(x)=0$ otherwise. For our case, with the interval $[-3, 3]$, the PDF is $p(x) = \\frac{1}{3 - (-3)} = \\frac{1}{6}$ for $x \\in [-3, 3]$.\n\nThe quantizer function $Q(x)$ is defined by the decision boundary $b_1$:\n$$\nQ(x) = \\begin{cases} y_1  \\text{if } x \\le b_1 \\\\ y_2  \\text{if } x  b_1 \\end{cases}\n$$\nThe MSE distortion, $D$, is the expected value of the squared error between the input signal $X$ and its quantized representation $Q(X)$. It is calculated by integrating the squared error over the entire distribution:\n$$\nD = E[(X - Q(X))^2] = \\int_{-3}^{3} (x - Q(x))^2 p(x) dx\n$$\nWe can split this integral into the two quantization regions defined by the boundary $b_1$: the region $[-3, b_1]$ where $Q(x) = y_1$, and the region $(b_1, 3]$ where $Q(x) = y_2$.\n$$\nD(b_1) = \\int_{-3}^{b_1} (x - y_1)^2 p(x) dx + \\int_{b_1}^{3} (x - y_2)^2 p(x) dx\n$$\nTo find the optimal decision boundary $b_1$ that minimizes the distortion $D$, we need to find the value of $b_1$ for which the derivative of $D$ with respect to $b_1$ is zero. We use the Leibniz integral rule for differentiation under the integral sign:\n$$\n\\frac{d}{dc} \\int_{a(c)}^{b(c)} f(x, c) dx = f(b(c), c) \\frac{db}{dc} - f(a(c), c) \\frac{da}{dc} + \\int_{a(c)}^{b(c)} \\frac{\\partial f}{\\partial c} dx\n$$\nIn our case, the integrand does not depend on $b_1$, so the partial derivative term is zero. Applying the rule, we get:\n$$\n\\frac{dD}{db_1} = \\left[ (b_1 - y_1)^2 p(b_1) \\right] - \\left[ (b_1 - y_2)^2 p(b_1) \\right]\n$$\nSetting the derivative to zero to find the minimum:\n$$\n\\frac{dD}{db_1} = p(b_1) \\left[ (b_1 - y_1)^2 - (b_1 - y_2)^2 \\right] = 0\n$$\nThe optimal boundary $b_1$ must lie within the interval $(-3, 3)$, where $p(b_1) = 1/6 \\neq 0$. Therefore, we can divide by $p(b_1)$:\n$$\n(b_1 - y_1)^2 - (b_1 - y_2)^2 = 0\n$$\n$$\n(b_1 - y_1)^2 = (b_1 - y_2)^2\n$$\nTaking the square root of both sides gives two possibilities:\n1. $b_1 - y_1 = b_1 - y_2$, which simplifies to $y_1 = y_2$. This is not possible since the reconstruction levels $y_1 = -2.5$ and $y_2 = 1.8$ are distinct.\n2. $b_1 - y_1 = -(b_1 - y_2)$, which simplifies to $b_1 - y_1 = y_2 - b_1$.\n\nWe solve the second equation for $b_1$:\n$$\n2b_1 = y_1 + y_2\n$$\n$$\nb_1 = \\frac{y_1 + y_2}{2}\n$$\nThis result is the well-known nearest neighbor condition for optimal quantization: the decision boundary between two reconstruction levels should be their midpoint.\n\nNow, we substitute the given values for $y_1$ and $y_2$ into this formula:\n$$\ny_1 = -2.5\n$$\n$$\ny_2 = 1.8\n$$\n$$\nb_1 = \\frac{-2.5 + 1.8}{2} = \\frac{-0.7}{2} = -0.35\n$$\nThe optimal decision boundary is $b_1 = -0.35$. This value is within the source's range of $[-3, 3]$, confirming it is a valid boundary.",
            "answer": "$$\\boxed{-0.35}$$"
        },
        {
            "introduction": "The Lloyd-Max algorithm is an iterative process that repeatedly applies the two optimality conditions to refine a quantizer. This practice  walks you through one complete cycle of the algorithm for a simple discrete data source. By first partitioning the data according to the nearest neighbor rule and then updating the reconstruction levels to be the centroids of these new partitions, you will gain a concrete understanding of how the algorithm converges towards an optimal solution.",
            "id": "1637671",
            "problem": "A data acquisition system is designed to measure a physical quantity that, after sampling and some initial processing, produces values from a discrete set of levels. The set of possible output levels is given by the alphabet $\\mathcal{X} = \\{1, 3, 4, 8\\}$. Due to the nature of the underlying physical process, each of these four levels is observed with equal probability.\n\nTo compress this data for transmission, a simple 1-bit scalar quantizer is to be implemented. This means all source values will be mapped to one of two reconstruction levels. The Lloyd-Max algorithm is chosen to optimize the placement of these two reconstruction levels to minimize the mean squared quantization error.\n\nThe optimization process begins with an initial guess for the two reconstruction levels: $y_1^{(0)} = 2.5$ and $y_2^{(0)} = 7.0$.\n\nYour task is to perform one full iteration of the Lloyd-Max algorithm. A full iteration consists of two steps: first, partitioning the source alphabet based on the nearest reconstruction level, and second, updating the reconstruction levels to be the centroids of the new partitions.\n\nCalculate the new pair of reconstruction levels, $(y_1^{(1)}, y_2^{(1)})$. Express your answer as a row matrix containing the two values in increasing order, using exact fractions if necessary.",
            "solution": "The source alphabet is $\\mathcal{X}=\\{1,3,4,8\\}$ with equal probabilities $p(x)=\\frac{1}{4}$ for each $x \\in \\mathcal{X}$. For mean squared error, the Lloyd-Max partition between two reconstruction levels is the midpoint of the current levels. With $y_{1}^{(0)}=\\frac{5}{2}$ and $y_{2}^{(0)}=7$, the decision boundary is\n$$b=\\frac{y_{1}^{(0)}+y_{2}^{(0)}}{2}=\\frac{\\frac{5}{2}+7}{2}=\\frac{\\frac{5}{2}+\\frac{14}{2}}{2}=\\frac{\\frac{19}{2}}{2}=\\frac{19}{4}.$$\n\nThus the partition induced by nearest-neighbor assignment is $S_{1}=\\{1,3,4\\}$ and $S_{2}=\\{8\\}$.\n\nThe centroid update for discrete equiprobable points is\n$$y_{k}^{(1)}=\\frac{\\sum_{x \\in S_{k}} x\\,p(x)}{\\sum_{x \\in S_{k}} p(x)}.$$\n\nFor $S_{1}$,\n$$y_{1}^{(1)}=\\frac{(1+3+4)\\cdot \\frac{1}{4}}{3 \\cdot \\frac{1}{4}}=\\frac{8}{3},$$\n\nand for $S_{2}$,\n$$y_{2}^{(1)}=\\frac{8 \\cdot \\frac{1}{4}}{1 \\cdot \\frac{1}{4}}=8.$$\n\nArranged in increasing order, the updated reconstruction levels are $\\frac{8}{3}$ and $8$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{8}{3}  8 \\end{pmatrix}}$$"
        },
        {
            "introduction": "The principles of optimal scalar quantization extend elegantly to multiple dimensions in the form of vector quantization (VQ), with the Linde-Buzo-Gray (LBG) algorithm being the most famous example. This exercise  moves our analysis into a 2D space, demonstrating how to partition vector data and update codebook vectors. Furthermore, it highlights a critical practical issue in VQ known as the \"empty cell problem,\" where a suboptimal initial codebook can cause some representative vectors to become unused, impacting the efficiency of the quantizer.",
            "id": "1637697",
            "problem": "A crucial algorithm in signal processing and data compression is the Linde-Buzo-Gray (LBG) algorithm for vector quantization. It operates by iteratively refining a set of representative points, called a codebook, to best match a given training dataset.\n\nA single iteration of the LBG algorithm involves two key steps:\n1.  **Partitioning:** Given a codebook $C = \\{c_1, c_2, ..., c_M\\}$, the training data space is partitioned into $M$ cells, $\\{R_1, R_2, ..., R_M\\}$. Each training vector $x$ from the dataset is assigned to the cell $R_i$ if the codeword $c_i$ is the closest codeword to $x$. Closeness is determined using the standard squared Euclidean distance. In case of a tie in distances, the vector is assigned to the cell corresponding to the codeword with the smallest index $i$.\n2.  **Updating:** A new codebook is formed by computing the centroid (component-wise arithmetic mean) of the training vectors in each cell. If a cell $R_i$ contains no training vectors (i.e., it is an empty cell), the corresponding codeword $c_i$ is discarded and is not part of the new codebook.\n\nConsider a training dataset $X$ in a 2D plane, composed of the following four vectors:\n$x_1 = (a, b)$, $x_2 = (a, -b)$, $x_3 = (-a, b)$, and $x_4 = (-a, -b)$.\n\nThe initial codebook for the LBG algorithm is $C_0 = \\{c_1, c_2, c_3\\}$, defined as:\n$c_1 = (-L, 0)$, $c_2 = (0, 0)$, and $c_3 = (L, 0)$.\n\nFor this problem, use the specific parameter values $a=5$, $b=2$, and $L=4$.\n\nCalculate the complete set of codewords in the new codebook, $C_1$, after one full iteration of the LBG algorithm. Let the new codewords be $c'_1, c'_2, ..., c'_{K}$. Order these codewords lexicographically (sort first by the x-component, then by the y-component). Present your final answer as a single row matrix containing the components of these ordered codewords, in the format $\\begin{pmatrix} x'_{1}  y'_{1}  x'_{2}  y'_{2}  \\dots \\end{pmatrix}$.",
            "solution": "We apply one iteration of the Linde-Buzo-Gray algorithm consisting of a partitioning step followed by an updating step. The squared Euclidean distance between a data vector $x=(x_{1},x_{2})$ and a codeword $c=(c_{1},c_{2})$ is\n$$d^{2}(x,c)=\\|x-c\\|^{2}=(x_{1}-c_{1})^{2}+(x_{2}-c_{2})^{2}.$$\nWith $a=5$, $b=2$, and $L=4$, the training set is\n$$x_{1}=(5,2),\\quad x_{2}=(5,-2),\\quad x_{3}=(-5,2),\\quad x_{4}=(-5,-2),$$\nand the initial codebook is\n$$c_{1}=(-4,0),\\quad c_{2}=(0,0),\\quad c_{3}=(4,0).$$\n\nPartitioning step. Compute distances of each $x_{j}$ to each $c_{i}$:\n- For $x_{1}=(5,2)$:\n$$d^{2}(x_{1},c_{1})=(5-(-4))^{2}+(2-0)^{2}=9^{2}+2^{2}=81+4=85,$$\n$$d^{2}(x_{1},c_{2})=(5-0)^{2}+(2-0)^{2}=5^{2}+2^{2}=25+4=29,$$\n$$d^{2}(x_{1},c_{3})=(5-4)^{2}+(2-0)^{2}=1^{2}+2^{2}=1+4=5.$$\nThus $x_{1}$ is assigned to $R_{3}$.\n\n- For $x_{2}=(5,-2)$:\n$$d^{2}(x_{2},c_{1})=(5-(-4))^{2}+(-2-0)^{2}=9^{2}+(-2)^{2}=81+4=85,$$\n$$d^{2}(x_{2},c_{2})=(5-0)^{2}+(-2-0)^{2}=5^{2}+(-2)^{2}=25+4=29,$$\n$$d^{2}(x_{2},c_{3})=(5-4)^{2}+(-2-0)^{2}=1^{2}+(-2)^{2}=1+4=5.$$\nThus $x_{2}$ is assigned to $R_{3}$.\n\n- For $x_{3}=(-5,2)$:\n$$d^{2}(x_{3},c_{1})=(-5-(-4))^{2}+(2-0)^{2}=(-1)^{2}+2^{2}=1+4=5,$$\n$$d^{2}(x_{3},c_{2})=(-5-0)^{2}+(2-0)^{2}=(-5)^{2}+2^{2}=25+4=29,$$\n$$d^{2}(x_{3},c_{3})=(-5-4)^{2}+(2-0)^{2}=(-9)^{2}+2^{2}=81+4=85.$$\nThus $x_{3}$ is assigned to $R_{1}$.\n\n- For $x_{4}=(-5,-2)$:\n$$d^{2}(x_{4},c_{1})=(-5-(-4))^{2}+(-2-0)^{2}=(-1)^{2}+(-2)^{2}=1+4=5,$$\n$$d^{2}(x_{4},c_{2})=(-5-0)^{2}+(-2-0)^{2}=(-5)^{2}+(-2)^{2}=25+4=29,$$\n$$d^{2}(x_{4},c_{3})=(-5-4)^{2}+(-2-0)^{2}=(-9)^{2}+(-2)^{2}=81+4=85.$$\nThus $x_{4}$ is assigned to $R_{1}$.\n\nTherefore, the nonempty cells are $R_{1}=\\{x_{3},x_{4}\\}$ and $R_{3}=\\{x_{1},x_{2}\\}$, while $R_{2}=\\varnothing$.\n\nUpdating step. For each nonempty cell $R_{i}$, compute the centroid (component-wise mean) to form the updated codeword $c_{i}'$; discard the empty cell $R_{2}$:\n- For $R_{1}$:\n$$c_{1}'=\\frac{1}{2}\\bigl((-5,2)+(-5,-2)\\bigr)=\\left(\\frac{-5+(-5)}{2},\\,\\frac{2+(-2)}{2}\\right)=(-5,0).$$\n- For $R_{3}$:\n$$c_{3}'=\\frac{1}{2}\\bigl((5,2)+(5,-2)\\bigr)=\\left(\\frac{5+5}{2},\\,\\frac{2+(-2)}{2}\\right)=(5,0).$$\n\nThe new codebook $C_{1}$ thus contains the two codewords $(-5,0)$ and $(5,0)$. Ordering lexicographically by the $x$-component and then the $y$-component gives $(-5,0)$, $(5,0)$. The required row matrix is\n$$\\begin{pmatrix}\n-5  0  5  0\n\\end{pmatrix}.$$",
            "answer": "$$\\boxed{\\begin{pmatrix}-5  0  5  0\\end{pmatrix}}$$"
        }
    ]
}