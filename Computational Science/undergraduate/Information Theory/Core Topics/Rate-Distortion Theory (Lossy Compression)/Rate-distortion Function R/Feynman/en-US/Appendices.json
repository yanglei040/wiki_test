{
    "hands_on_practices": [
        {
            "introduction": "Let's begin with the most fundamental example in rate-distortion theory: compressing a binary source with the possibility of bit-flip errors. This problem asks you to derive the rate-distortion function for a Bernoulli($p$) source under Hamming distortion, a scenario that models countless real-world communication channels. Solving this will reveal the classic and elegant relationship between rate, distortion, and entropy, $R(D) = H(p) - H(D)$, which forms the bedrock of lossy compression theory. ",
            "id": "1652137",
            "problem": "Consider a discrete memoryless source that generates binary symbols, $X$, from the alphabet $\\mathcal{X} = \\{0, 1\\}$. The probability of generating a '1' is $P(X=1) = p$, and the probability of generating a '0' is $P(X=0) = 1-p$. For simplicity, assume $0 < p \\le 1/2$. We wish to compress the output of this source and represent it with symbols, $\\hat{X}$, from the same alphabet $\\hat{\\mathcal{X}} = \\{0, 1\\}$.\n\nThe quality of the compression is measured by the single-letter Hamming distortion, defined as $d(x, \\hat{x}) = 0$ if $x = \\hat{x}$ and $d(x, \\hat{x}) = 1$ if $x \\neq \\hat{x}$. The average distortion is denoted by $D = E[d(X, \\hat{X})]$.\n\nThe rate-distortion function, $R(D)$, gives the minimum achievable rate (in bits per symbol) for a given maximum average distortion $D$. Find the expression for the rate-distortion function $R(D)$ for this source in the region where the rate is non-zero. Your final answer should be expressed in terms of the binary entropy function, $H(y) = -y \\log_2(y) - (1-y)\\log_2(1-y)$, and may involve the parameters $p$ and $D$.",
            "solution": "The rate-distortion function $R(D)$ is defined as the minimum possible mutual information $I(X; \\hat{X})$ between the source $X$ and the reconstruction $\\hat{X}$ over all joint distributions $p(x, \\hat{x})$ such that the average distortion $E[d(X, \\hat{X})]$ does not exceed $D$.\n$$R(D) = \\min_{p(\\hat{x}|x) : E[d(X,\\hat{X})] \\le D} I(X; \\hat{X})$$\nThe mutual information can be expressed in terms of entropy as $I(X; \\hat{X}) = H(X) - H(X|\\hat{X})$. For the given Bernoulli source, the entropy $H(X)$ is constant and equal to $H(p)$. Therefore, minimizing $I(X; \\hat{X})$ is equivalent to maximizing the conditional entropy $H(X|\\hat{X})$.\n\n$$R(D) = H(p) - \\max_{p(\\hat{x}|x) : E[d(X,\\hat{X})] \\le D} H(X|\\hat{X})$$\n\nWe can find an upper bound for $H(X|\\hat{X})$ using Fano's inequality. The Hamming distortion $d(x, \\hat{x})$ is 1 if $x \\neq \\hat{x}$ and 0 otherwise. Thus, the average distortion is the probability of error: $D = E[d(X, \\hat{X})] = P(X \\neq \\hat{X})$.\n\nFano's inequality states that for any two random variables $X$ and $\\hat{X}$ with the same alphabet $\\mathcal{X}$,\n$$H(X|\\hat{X}) \\le H(P(X \\neq \\hat{X})) + P(X \\neq \\hat{X}) \\log_2(|\\mathcal{X}|-1)$$\nFor our binary source, $|\\mathcal{X}|=2$, so $\\log_2(|\\mathcal{X}|-1) = \\log_2(1) = 0$. Using $P(X \\neq \\hat{X}) \\le D$, we get:\n$$H(X|\\hat{X}) \\le H(D)$$\nThis provides a lower bound on the rate-distortion function:\n$$R(D) \\ge H(p) - H(D)$$\nThis bound is achievable if we can find a test channel (i.e., a conditional distribution $p(\\hat{x}|x)$) that simultaneously satisfies the distortion constraint $E[d(X, \\hat{X})] = D$ and achieves the entropy bound $H(X|\\hat{X}) = H(D)$.\n\nLet's construct such a channel. The condition $H(X|\\hat{X}) = H(D)$ is met if the conditional distribution of $X$ given $\\hat{X}$ is a simple binary distribution with probability $D$. Let's define a *reverse* test channel where the crossover probability is $D$:\n$$p(X=1|\\hat{X}=0) = D \\quad \\text{and} \\quad p(X=0|\\hat{X}=1) = D$$\nThis implies $p(X=0|\\hat{X}=0) = 1-D$ and $p(X=1|\\hat{X}=1) = 1-D$.\nThe conditional entropy $H(X|\\hat{X}=\\hat{x})$ is thus $H(D)$ for both $\\hat{x}=0$ and $\\hat{x}=1$. Consequently, the total conditional entropy is $H(X|\\hat{X}) = \\sum_{\\hat{x}} p(\\hat{x}) H(X|\\hat{X}=\\hat{x}) = H(D)$.\n\nNow, we need to define the output distribution $p(\\hat{x})$ such that the source distribution $p(x)$ is recovered and the distortion constraint is met. Let $p(\\hat{X}=1) = q$. Then $p(\\hat{X}=0) = 1-q$. The marginal probability $P(X=1)$ can be calculated as:\n$$P(X=1) = \\sum_{\\hat{x}} P(\\hat{X}=\\hat{x}) P(X=1|\\hat{X}=\\hat{x})$$\n$$p = (1-q) P(X=1|\\hat{X}=0) + q P(X=1|\\hat{X}=1)$$\n$$p = (1-q)D + q(1-D) = D - qD + q - qD = q(1-2D) + D$$\nSolving for $q$:\n$$q = \\frac{p-D}{1-2D}$$\nFor $q$ to be a valid probability, we need $0 \\le q \\le 1$.\nGiven $p \\le 1/2$, the condition $D \\le p < 1/2$ ensures that $1-2D > 0$.\nThe requirement $q \\ge 0$ implies $p-D \\ge 0$, so $D \\le p$.\nThe requirement $q \\le 1$ implies $p-D \\le 1-2D$, so $D \\le 1-p$.\nSince we are given $p \\le 1/2$, we have $p \\le 1-p$. Therefore, the stricter condition is $D \\le p$. So, this construction is valid for $0 \\le D \\le p$.\n\nFinally, let's verify the average distortion for this channel:\n$$E[d(X, \\hat{X})] = P(X \\neq \\hat{X}) = \\sum_{\\hat{x}} P(\\hat{X}=\\hat{x}) P(X \\neq \\hat{x} | \\hat{X}=\\hat{x})$$\n$$E[d(X, \\hat{X})] = P(\\hat{X}=0) P(X=1|\\hat{X}=0) + P(\\hat{X}=1) P(X=0|\\hat{X}=1)$$\n$$E[d(X, \\hat{X})] = (1-q)D + qD = D$$\nThe distortion constraint is satisfied with equality.\n\nSince we have constructed a channel that produces distortion $D$ and has a rate $I(X;\\hat{X}) = H(p) - H(D)$ for any $D$ in the range $0 \\le D \\le p$, the lower bound is achievable. For $D > p$, we can achieve zero rate. For example, by always setting $\\hat{X}=0$ (since 0 is the more probable symbol), the distortion is $D = P(X=1) = p$ and the rate is $I(X; \\text{const}) = 0$. Since $R(D)$ is a non-increasing function, $R(D)=0$ for all $D \\ge p$.\n\nThe expression for the rate-distortion function in the non-trivial region ($0 \\le D \\le p$) is therefore:\n$$R(D) = H(p) - H(D)$$",
            "answer": "$$\\boxed{H(p) - H(D)}$$"
        },
        {
            "introduction": "Building on the binary case, we now explore a source with multiple output levels, a situation common in digital signal processing. This exercise challenges you to calculate the rate-distortion function for a discrete uniform source using a squared-error distortion measure, which is sensitive to the magnitude of errors. The solution involves a powerful geometric approach: finding the lower convex envelope of achievable rate-distortion points, a technique central to practical quantizer design. ",
            "id": "1652148",
            "problem": "A discrete memoryless source emits symbols from the alphabet $\\mathcal{X} = \\{1, 2, 3, 4\\}$. Each symbol is generated independently and with equal probability. We wish to compress this source and represent it with a reproduction alphabet $\\hat{\\mathcal{X}}$, which can consist of any real numbers. The quality of the reproduction is measured by the squared-error distortion, defined as $d(x, \\hat{x}) = (x - \\hat{x})^2$ for a source symbol $x \\in \\mathcal{X}$ and its reproduction $\\hat{x} \\in \\hat{\\mathcal{X}}$.\n\nDetermine the rate-distortion function $R(D)$ for this source, which gives the minimum achievable rate in bits per symbol for a given maximum average distortion $D$. Express your answer as a piecewise function of $D$.",
            "solution": "Let $X$ be uniform on $\\mathcal{X}=\\{1,2,3,4\\}$, with $p(x)=\\frac{1}{4}$. The rate-distortion function under squared-error distortion is\n$$\nR(D)=\\inf_{p(\\hat{x}|x):\\,\\mathbb{E}[(X-\\hat{X})^{2}]\\le D} I(X;\\hat{X}),\n$$\nmeasured in bits per symbol (so all logarithms are base $2$).\n\nKey facts used:\n- For squared-error distortion with unconstrained reproduction alphabet, for any deterministic partition (quantizer) of the source alphabet into clusters, the optimal reproduction value for a cluster is its conditional mean, and the clusterâ€™s contribution to the total mean-squared error is its within-cluster variance.\n- The set of achievable $(D,R)$ pairs is convex by time-sharing. Therefore, $R(D)$ is the lower convex envelope of the achievable points from deterministic mappings.\n\nCompute the canonical deterministic mappings (clusterings), their distortions, and rates.\n\n1) One-cluster mapping (constant reproduction).\n- Reproduction $\\hat{x}=\\mathbb{E}[X]=\\frac{1+2+3+4}{4}=\\frac{5}{2}$.\n- Distortion $D=\\mathrm{Var}(X)=\\mathbb{E}[X^{2}]-(\\mathbb{E}[X])^{2}=\\frac{30}{4}-\\left(\\frac{5}{2}\\right)^{2}=\\frac{15}{2}-\\frac{25}{4}=\\frac{5}{4}$.\n- Since $\\hat{X}$ is constant, $I(X;\\hat{X})=0$. Hence point $\\left(D,R\\right)=\\left(\\frac{5}{4},0\\right)$.\n\n2) Two-cluster mapping.\n- The optimal $2$-means partition groups adjacent points: $\\{1,2\\}$ and $\\{3,4\\}$, with centroids $\\frac{3}{2}$ and $\\frac{7}{2}$ respectively.\n- For a two-point adjacent cluster $\\{x,x+1\\}$, the centroid is $x+\\frac{1}{2}$ and each point has squared deviation $\\left(\\frac{1}{2}\\right)^{2}=\\frac{1}{4}$. Thus cluster distortion per occurrence is $\\frac{1}{4}$. With cluster probability $\\frac{2}{4}$, the contribution to overall $D$ is $\\frac{2}{4}\\cdot\\frac{1}{4}=\\frac{1}{8}$ per cluster. With two such clusters, total\n$$\nD=\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4}.\n$$\n- The cluster probabilities are $\\frac{1}{2}$ and $\\frac{1}{2}$, so $H(\\hat{X})=1$ and since the mapping is deterministic, $I(X;\\hat{X})=H(\\hat{X})=1$. Hence point $\\left(D,R\\right)=\\left(\\frac{1}{4},1\\right)$.\n- Any other two-cluster partition (e.g., $\\{1,2,3\\}$ vs. $\\{4\\}$ or nonadjacent pairings) yields larger distortion, so $\\left(\\frac{1}{4},1\\right)$ is the optimal two-cluster point.\n\n3) Three-cluster mapping.\n- The optimal partition merges one adjacent pair and leaves the other two as singletons, e.g., $\\{1,2\\}$, $\\{3\\}$, $\\{4\\}$ (or any symmetric variant). The merged pair contributes $\\frac{1}{8}$ to $D$ as above; singletons contribute $0$. Hence\n$$\nD=\\frac{1}{8}.\n$$\n- Cluster probabilities are $\\frac{1}{2},\\frac{1}{4},\\frac{1}{4}$, so\n$$\nI(X;\\hat{X})=H(\\hat{X})=-\\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right)-2\\cdot\\frac{1}{4}\\log_{2}\\left(\\frac{1}{4}\\right)=\\frac{1}{2}+1=\\frac{3}{2}.\n$$\nHence point $\\left(D,R\\right)=\\left(\\frac{1}{8},\\frac{3}{2}\\right)$.\n\n4) Four-cluster mapping (lossless reproduction).\n- Identity mapping with centroids at the source points gives $D=0$ and $R=H(X)=\\log_{2}4=2$. Hence point $\\left(D,R\\right)=\\left(0,2\\right)$.\n\nConvexification by time-sharing:\n- The achievable points include $\\left(\\frac{5}{4},0\\right)$, $\\left(\\frac{1}{4},1\\right)$, $\\left(\\frac{1}{8},\\frac{3}{2}\\right)$, and $\\left(0,2\\right)$, and by time-sharing, all points on line segments joining them.\n- Compute slopes between these points:\n  - Between $\\left(0,2\\right)$ and $\\left(\\frac{1}{4},1\\right)$: slope is $\\frac{1-2}{\\frac{1}{4}-0}=-4$; the intermediate point $\\left(\\frac{1}{8},\\frac{3}{2}\\right)$ lies exactly on this line, so it does not create a new segment.\n  - Between $\\left(\\frac{1}{4},1\\right)$ and $\\left(\\frac{5}{4},0\\right)$: slope is $\\frac{0-1}{\\frac{5}{4}-\\frac{1}{4}}=-1$.\n\nThus the lower convex envelope is piecewise linear with two segments:\n- For $0\\le D\\le\\frac{1}{4}$, the line through $\\left(0,2\\right)$ and $\\left(\\frac{1}{4},1\\right)$:\n$$\nR(D)=2-4D.\n$$\n- For $\\frac{1}{4}\\le D\\le\\frac{5}{4}$, the line through $\\left(\\frac{1}{4},1\\right)$ and $\\left(\\frac{5}{4},0\\right)$:\n$$\nR(D)=\\frac{5}{4}-D.\n$$\n- For $D\\ge\\frac{5}{4}$, one can use a constant-reproduction code and achieve $R(D)=0$.\n\nTherefore, in bits per symbol,\n$$\nR(D)=\n\\begin{cases}\n2-4D, & 0\\le D\\le \\frac{1}{4},\\\\\n\\frac{5}{4}-D, & \\frac{1}{4}\\le D\\le \\frac{5}{4},\\\\\n0, & D\\ge \\frac{5}{4}.\n\\end{cases}\n$$",
            "answer": "$$\\boxed{R(D)=\\begin{cases}\n2-4D, & 0\\le D\\le \\frac{1}{4},\\\\\n\\frac{5}{4}-D, & \\frac{1}{4}\\le D\\le \\frac{5}{4},\\\\\n0, & D\\ge \\frac{5}{4}.\n\\end{cases}}$$"
        },
        {
            "introduction": "Our exploration now moves from the discrete to the continuous domain, modeling signals like analog voltage. This practice problem involves a continuous, uniformly distributed source and an absolute-error distortion measure, which penalizes errors linearly. You will tackle this by applying the Shannon Lower Bound, a versatile tool for finding the rate-distortion function for continuous sources that provides deep insight into the efficiency of analog-to-digital conversion. ",
            "id": "1652130",
            "problem": "An analog-to-digital converter (ADC) is designed to process a continuous voltage signal. This signal is modeled as a random variable $X$ that is uniformly distributed over the interval $[-A, A]$, where $A$ is the maximum amplitude of the signal. The ADC represents the continuous signal $X$ with a discrete value $\\hat{X}$, which introduces a quantization error. The performance of this ADC is evaluated based on the absolute error distortion measure, defined as $d(x, \\hat{x}) = |x - \\hat{x}|$.\n\nAccording to rate-distortion theory, there is a minimum theoretical data rate, known as the rate-distortion function $R(D)$, required to represent the source signal $X$ with an average distortion that does not exceed a specified level $D$. Your goal is to find this function.\n\nDetermine the rate-distortion function $R(D)$ for the given source and distortion measure. Express your answer as an analytic function of the parameters $A$ and $D$. The result should be valid for the range of distortions $D$ for which a non-zero data rate is required. The rate should be expressed in units of nats per symbol.",
            "solution": "The source $X$ is uniform on the interval $[-A, A]$, so its density is $f_{X}(x) = \\frac{1}{2A}$ for $|x| \\leq A$ and $0$ otherwise, and its differential entropy is\n$$\nh(X) = \\ln(2A).\n$$\nWe measure distortion by $d(x,\\hat{x}) = |x-\\hat{x}|$ and seek the rate-distortion function\n$$\nR(D) = \\inf_{p(\\hat{x}|x):\\,\\mathbb{E}[|X-\\hat{X}|]\\leq D} I(X;\\hat{X}).\n$$\nFirst, we identify the largest distortion that can be achieved with zero rate. With zero rate, the reproduction $\\hat{X}$ must be a constant $c$, and the average distortion is minimized at a median of $X$. Since $X$ is symmetric and uniform on $[-A,A]$, the median is $c = 0$, and\n$$\n\\inf_{c}\\,\\mathbb{E}|X-c| = \\mathbb{E}|X| = \\int_{-A}^{A} \\frac{1}{2A} |x| \\, dx = \\frac{A}{2}.\n$$\nHence a non-zero rate is required precisely when $0 < D < \\frac{A}{2}$.\n\nNext, we use the Shannon lower bound (SLB) for difference distortion measures. For $d(x,\\hat{x}) = |x-\\hat{x}|$, the SLB states\n$$\nR(D) \\geq h(X) - \\sup_{f_{Z}:\\,\\mathbb{E}|Z|\\leq D} h(Z).\n$$\nWe maximize $h(Z)$ subject to $\\int f_{Z}(z)\\,dz = 1$ and $\\int |z| f_{Z}(z)\\,dz \\leq D$. Introducing Lagrange multipliers and taking the variational derivative yields the Gibbs-form maximizer\n$$\nf_{Z}(z) = \\frac{s}{2}\\exp(-s|z|), \\quad s > 0,\n$$\nwhich is the Laplace density with parameter $s$. Its expected absolute value is $\\mathbb{E}|Z| = \\frac{1}{s}$, so the constraint $\\mathbb{E}|Z| \\leq D$ is met with equality by choosing $s = \\frac{1}{D}$. The corresponding differential entropy is\n$$\nh(Z) = -\\int f_{Z}(z)\\ln f_{Z}(z)\\,dz = 1 + \\ln\\!\\left(\\frac{2}{s}\\right) = 1 + \\ln(2D).\n$$\nTherefore, the Shannon lower bound evaluates to\n$$\nR(D) \\geq h(X) - h(Z) = \\ln(2A) - \\bigl[1 + \\ln(2D)\\bigr] = \\ln\\!\\left(\\frac{A}{D}\\right) - 1.\n$$\nFor absolute-error distortion and this symmetric source, this lower bound is achieved by the optimal test channel in the Gibbs family, so the bound is tight in the positive-rate regime. Consequently, for the range of distortions requiring a non-zero rate, namely $0 < D < \\frac{A}{2}$, the rate-distortion function in nats per symbol is\n$$\nR(D) = \\ln\\!\\left(\\frac{A}{D}\\right) - 1.\n$$",
            "answer": "$$\\boxed{\\ln\\!\\left(\\frac{A}{D}\\right)-1}$$"
        }
    ]
}