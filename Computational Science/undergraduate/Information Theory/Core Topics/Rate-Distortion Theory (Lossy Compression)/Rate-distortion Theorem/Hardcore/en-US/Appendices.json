{
    "hands_on_practices": [
        {
            "introduction": "We begin our exploration of rate-distortion theory by examining the most fundamental scenario: compression at a zero rate. This thought experiment  helps establish a baseline for performance by calculating the unavoidable distortion when we represent every source symbol with a single, constant value. Understanding this foundational case is crucial for appreciating the benefits gained by investing even a small amount of data rate into the compression process.",
            "id": "1652370",
            "problem": "Consider a discrete memoryless source, represented by a random variable $X$, which generates symbols from the alphabet $\\mathcal{A} = \\{0, 1, 2, 3\\}$. The source has a uniform probability distribution, meaning each symbol in $\\mathcal{A}$ is equally likely to be generated. A very basic lossy compression scheme is employed where every symbol $x$ from the source is represented by a single, constant value $\\hat{x} = 1.5$. This type of encoding corresponds to a transmission rate of zero, as the representation does not depend on the input symbol.\n\nThe performance of this compression is evaluated using the squared-error distortion measure, given by $d(x, \\hat{x}) = (x - \\hat{x})^2$. Calculate the expected distortion, $D = E[d(X, \\hat{x})]$, for this system. Express your answer as a decimal.",
            "solution": "The random variable $X$ is uniform on $\\mathcal{A}=\\{0,1,2,3\\}$, so $p(x)=\\frac{1}{4}$ for each $x\\in\\mathcal{A}$. The reconstruction is the constant $\\hat{x}=1.5=\\frac{3}{2}$ and the distortion measure is $d(x,\\hat{x})=(x-\\hat{x})^{2}$. The expected distortion is\n$$\nD=E[d(X,\\hat{x})]=\\sum_{x\\in\\mathcal{A}} p(x)\\,(x-\\hat{x})^{2}=\\sum_{x\\in\\{0,1,2,3\\}} \\frac{1}{4}\\left(x-\\frac{3}{2}\\right)^{2}.\n$$\nEvaluating each term,\n$$\n\\left(0-\\frac{3}{2}\\right)^{2}=\\frac{9}{4},\\quad \\left(1-\\frac{3}{2}\\right)^{2}=\\frac{1}{4},\\quad \\left(2-\\frac{3}{2}\\right)^{2}=\\frac{1}{4},\\quad \\left(3-\\frac{3}{2}\\right)^{2}=\\frac{9}{4}.\n$$\nThus,\n$$\nD=\\frac{1}{4}\\left(\\frac{9}{4}+\\frac{1}{4}+\\frac{1}{4}+\\frac{9}{4}\\right)=\\frac{1}{4}\\cdot\\frac{20}{4}=\\frac{20}{16}=\\frac{5}{4}=1.25.\n$$\nTherefore, the expected distortion is $1.25$.",
            "answer": "$$\\boxed{1.25}$$"
        },
        {
            "introduction": "Having established a baseline distortion at zero rate, we now explore the fundamental trade-off at the heart of the rate-distortion theorem. This practice problem  challenges you to find the minimum possible distortion when you are allowed a non-zero data rateâ€”in this case, one bit per symbol. By working through this, you will engage in a simple optimization to design the best possible 2-level quantizer, gaining a concrete understanding of how investing rate reduces distortion.",
            "id": "1652375",
            "problem": "A simplified digital environmental sensor is designed to measure temperature. The sensor's output is a discrete random variable, $X$, which can take one of the four integer values from the alphabet $\\mathcal{X} = \\{1, 2, 3, 4\\}$. Long-term observations have shown that each of these four values is equally likely to occur.\n\nTo conserve power, the sensor's readings must be compressed before being transmitted over a wireless channel. The channel can support a maximum data rate of $R = 1$ bit per sensor reading. The performance of the compression system is evaluated by the average squared-error distortion, defined as $D = E[(X - \\hat{X})^2]$, where $\\hat{X}$ is the reconstructed value after transmission and decompression.\n\nAssuming an optimal compression and decompression scheme is used, what is the theoretical minimum average squared-error distortion $D$ that can be achieved given the channel's rate constraint? Provide your answer as an exact fraction.",
            "solution": "The problem asks for the minimum achievable average squared-error distortion $D$ for a given source and a rate constraint $R$. This is a classic problem in rate-distortion theory.\n\nThe source variable $X$ takes values from the set $\\mathcal{X} = \\{1, 2, 3, 4\\}$ with equal probability. Therefore, $P(X=x) = \\frac{1}{4}$ for any $x \\in \\mathcal{X}$. The distortion measure is the squared error, $d(x, \\hat{x}) = (x - \\hat{x})^2$. The rate constraint is $R = 1$ bit per symbol.\n\nThe rate $R$ limits the number of distinct values the reconstructed signal $\\hat{X}$ can take. If the reconstruction alphabet $\\hat{\\mathcal{X}}$ has size $|\\hat{\\mathcal{X}}| = M$, then the minimum rate required to represent these values is the entropy of the output, $H(\\hat{X})$. To transmit this information, we need $R \\ge H(\\hat{X})$. The simplest form of encoding uses a fixed-length code, where the number of levels $M$ must satisfy $M \\le 2^R$. In our case, with $R = 1$, we have $M \\le 2^1 = 2$. This means we can use at most two distinct reconstruction levels, let's call them $\\hat{x}_1$ and $\\hat{x}_2$.\n\nThe problem is thus reduced to finding the optimal 2-level quantizer for the source $X$ that minimizes the average squared-error distortion. An optimal quantizer partitions the source alphabet into disjoint sets, and maps all symbols in a given set to a single reconstruction value. For the squared-error distortion measure, the optimal reconstruction value for any set of source symbols is their conditional expectation (i.e., their mean).\n\nWe need to partition the source alphabet $\\mathcal{X} = \\{1, 2, 3, 4\\}$ into two non-empty subsets, $S_1$ and $S_2$. There are three distinct ways to do this (up to relabeling of the sets):\n1.  $S_1 = \\{1\\}$, $S_2 = \\{2, 3, 4\\}$\n2.  $S_1 = \\{1, 2\\}$, $S_2 = \\{3, 4\\}$\n3.  $S_1 = \\{1, 2, 3\\}$, $S_2 = \\{4\\}$\n\nWe will calculate the minimum average distortion for each case.\n\n**Case 1: Partition $S_1 = \\{1\\}$ and $S_2 = \\{2, 3, 4\\}$**\nThe optimal reconstruction level for $S_1$ is the mean of its elements:\n$\\hat{x}_1 = E[X | X \\in S_1] = 1$.\nThe optimal reconstruction level for $S_2$ is:\n$\\hat{x}_2 = E[X | X \\in S_2] = \\frac{2+3+4}{3} = 3$.\nThe average distortion $D_1$ is calculated by averaging the squared errors for each source symbol:\n$$D_1 = \\sum_{x \\in \\mathcal{X}} P(X=x) d(x, \\hat{x}(x))$$\nwhere $\\hat{x}(x) = \\hat{x}_1$ if $x \\in S_1$ and $\\hat{x}(x) = \\hat{x}_2$ if $x \\in S_2$.\n$$D_1 = P(1)(1-1)^2 + P(2)(2-3)^2 + P(3)(3-3)^2 + P(4)(4-3)^2$$\n$$D_1 = \\frac{1}{4}(0)^2 + \\frac{1}{4}(-1)^2 + \\frac{1}{4}(0)^2 + \\frac{1}{4}(1)^2 = \\frac{1}{4}(0 + 1 + 0 + 1) = \\frac{2}{4} = \\frac{1}{2}$$\n\n**Case 2: Partition $S_1 = \\{1, 2\\}$ and $S_2 = \\{3, 4\\}$**\nThe optimal reconstruction level for $S_1$:\n$\\hat{x}_1 = E[X | X \\in S_1] = \\frac{1+2}{2} = 1.5$.\nThe optimal reconstruction level for $S_2$:\n$\\hat{x}_2 = E[X | X \\in S_2] = \\frac{3+4}{2} = 3.5$.\nThe average distortion $D_2$ is:\n$$D_2 = P(1)(1-1.5)^2 + P(2)(2-1.5)^2 + P(3)(3-3.5)^2 + P(4)(4-3.5)^2$$\n$$D_2 = \\frac{1}{4}(-0.5)^2 + \\frac{1}{4}(0.5)^2 + \\frac{1}{4}(-0.5)^2 + \\frac{1}{4}(0.5)^2$$\n$$D_2 = \\frac{1}{4}(0.25) + \\frac{1}{4}(0.25) + \\frac{1}{4}(0.25) + \\frac{1}{4}(0.25) = \\frac{1}{4}(4 \\times 0.25) = \\frac{1}{4}(1) = \\frac{1}{4}$$\n\n**Case 3: Partition $S_1 = \\{1, 2, 3\\}$ and $S_2 = \\{4\\}$**\nBy symmetry with Case 1, we expect the same distortion, but we will compute it for completeness.\nThe optimal reconstruction level for $S_1$:\n$\\hat{x}_1 = E[X | X \\in S_1] = \\frac{1+2+3}{3} = 2$.\nThe optimal reconstruction level for $S_2$:\n$\\hat{x}_2 = E[X | X \\in S_2] = 4$.\nThe average distortion $D_3$ is:\n$$D_3 = P(1)(1-2)^2 + P(2)(2-2)^2 + P(3)(3-2)^2 + P(4)(4-4)^2$$\n$$D_3 = \\frac{1}{4}(-1)^2 + \\frac{1}{4}(0)^2 + \\frac{1}{4}(1)^2 + \\frac{1}{4}(0)^2 = \\frac{1}{4}(1 + 0 + 1 + 0) = \\frac{2}{4} = \\frac{1}{2}$$\n\nComparing the three cases, the minimum achievable distortion is $D_{\\min} = \\min(D_1, D_2, D_3) = \\min(\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{2}) = \\frac{1}{4}$.\n\nFinally, we must verify that this optimal quantizer does not require a rate greater than $R=1$. For the optimal partition (Case 2), the reconstruction levels are $\\hat{x}_1 = 1.5$ and $\\hat{x}_2 = 3.5$. The probabilities of these levels being used are:\n$P(\\hat{X} = \\hat{x}_1) = P(X \\in S_1) = P(X=1) + P(X=2) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\n$P(\\hat{X} = \\hat{x}_2) = P(X \\in S_2) = P(X=3) + P(X=4) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\nThe entropy of the output distribution is the minimum rate required to encode it:\n$H(\\hat{X}) = -\\sum_i P(\\hat{X}=\\hat{x}_i)\\log_2 P(\\hat{X}=\\hat{x}_i) = -(\\frac{1}{2}\\log_2\\frac{1}{2} + \\frac{1}{2}\\log_2\\frac{1}{2}) = - \\log_2\\frac{1}{2} = 1$ bit per symbol.\nSince the required rate $H(\\hat{X})=1$ is equal to the available channel rate $R=1$, this minimum distortion is achievable.\n\nTherefore, the theoretical minimum average squared-error distortion is $\\frac{1}{4}$.",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        },
        {
            "introduction": "Our first two exercises explored optimal compression for a known source. In practice, however, a compression system designed for one type of data may be used on another. This problem  simulates this realistic scenario, asking you to analyze the performance of an optimal binary compressor when the statistics of the source it receives are different from what it was designed for. This practice illuminates the crucial distinction between the theoretical rate-distortion function and the operational performance of a fixed, potentially sub-optimal, coding scheme.",
            "id": "1652583",
            "problem": "In the field of lossy data compression, the rate-distortion function describes the fundamental trade-off between the compression rate and the fidelity of the reconstructed data. Consider a memoryless binary source producing symbols from the alphabet $\\mathcal{X} = \\{0, 1\\}$. The distortion between a source symbol $x$ and its reconstruction $\\hat{x}$ is measured by the Hamming distortion, defined as $d(x, \\hat{x}) = 0$ if $x = \\hat{x}$ and $d(x, \\hat{x}) = 1$ if $x \\neq \\hat{x}$. The binary entropy function is given by $H_b(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$.\n\nA compression system is initially designed for a symmetric binary source $X$ where the symbols are equiprobable, i.e., $P(X=1) = 1/2$. The system is constructed to be optimal in the rate-distortion sense, achieving a specific average distortion of $D = E[d(X, \\hat{X})] = 0.1$. For a Bernoulli(1/2) source with Hamming distortion, it is a known result from rate-distortion theory that the optimal compression scheme for a target distortion $D$ (where $0 \\leq D \\leq 1/2$) is realized by a stochastic mapping equivalent to passing the source through a Binary Symmetric Channel (BSC) with a crossover probability $p_c = D$.\n\nThis exact same compression system, which was optimal for the source $X$, is now repurposed to compress a different memoryless binary source, $Y$. This new source is asymmetric, with a probability of producing a '1' given by $P(Y=1) = 1/4$.\n\nCalculate the new average distortion $D' = E[d(Y, \\hat{Y})]$ and the new operational rate $R' = I(Y; \\hat{Y})$ that result from applying this pre-existing compression system to the new source $Y$. Your answer should be a pair of numerical values $(D', R')$. Express the rate $R'$ in bits per symbol. Round both numerical values in your final answer to four significant figures.",
            "solution": "The problem asks us to evaluate the performance (distortion and rate) of a fixed compression scheme when applied to a source different from the one it was optimized for.\n\nFirst, we identify the properties of the fixed compression scheme. The problem states that the scheme was designed to be rate-distortion optimal for a Bernoulli(1/2) source with a target Hamming distortion of $D = 0.1$. It is also given that for this setup, the optimal scheme is equivalent to a Binary Symmetric Channel (BSC) with a crossover probability $p_c$ equal to the target distortion $D$.\nTherefore, the compression system is a BSC with crossover probability $p_c = 0.1$.\nThis channel is defined by the conditional probabilities:\n$P(\\hat{x}=1|x=0) = p_c = 0.1$\n$P(\\hat{x}=0|x=0) = 1 - p_c = 0.9$\n$P(\\hat{x}=0|x=1) = p_c = 0.1$\n$P(\\hat{x}=1|x=1) = 1 - p_c = 0.9$\nIn general, $P(\\hat{x} \\neq x | x) = p_c$ for any $x \\in \\{0, 1\\}$.\n\nNext, we apply this fixed BSC to the new source $Y$, which is a Bernoulli source with $P(Y=1) = p = 1/4 = 0.25$. Consequently, $P(Y=0) = 1-p = 3/4 = 0.75$. We denote the output of the channel as $\\hat{Y}$.\n\nLet's calculate the new average distortion, $D'$. The distortion is the expected value of the Hamming distortion function, which is equivalent to the probability of error $P(Y \\neq \\hat{Y})$.\nUsing the law of total probability:\n$$D' = P(Y \\neq \\hat{Y}) = \\sum_{y \\in \\{0,1\\}} P(Y=y) P(Y \\neq \\hat{Y} | Y=y)$$\nThe term $P(Y \\neq \\hat{Y} | Y=y)$ is the crossover probability of the channel, which is $p_c$.\n$$D' = P(Y=0) \\cdot p_c + P(Y=1) \\cdot p_c$$\n$$D' = (P(Y=0) + P(Y=1)) \\cdot p_c = 1 \\cdot p_c = p_c$$\nSubstituting the value of $p_c$:\n$$D' = 0.1$$\nRounding to four significant figures, we get $D' = 0.1000$.\n\nNow, let's calculate the new operational rate, $R'$. The rate of a lossy compression scheme is given by the mutual information between the source and its reconstruction, $R' = I(Y; \\hat{Y})$.\nWe can calculate the mutual information using the formula $I(Y; \\hat{Y}) = H(\\hat{Y}) - H(\\hat{Y}|Y)$.\n\nFirst, we find the conditional entropy $H(\\hat{Y}|Y)$:\n$$H(\\hat{Y}|Y) = \\sum_{y \\in \\{0,1\\}} P(Y=y) H(\\hat{Y}|Y=y)$$\nFor a fixed input $y$ to a BSC with crossover $p_c$, the output $\\hat{Y}$ is a Bernoulli random variable with parameter $p_c$ (if we consider the event of a flip). The entropy of this conditional distribution is the binary entropy of the crossover probability, $H_b(p_c)$.\n$$H(\\hat{Y}|Y=y) = H_b(p_c) \\quad \\text{for } y \\in \\{0,1\\}$$\nTherefore,\n$$H(\\hat{Y}|Y) = P(Y=0)H_b(p_c) + P(Y=1)H_b(p_c) = H_b(p_c)$$\nUsing $p_c = 0.1$:\n$$H(\\hat{Y}|Y) = H_b(0.1) = -0.1 \\log_2(0.1) - (1-0.1) \\log_2(0.9) \\approx 0.4689956 \\text{ bits/symbol}$$\n\nNext, we find the entropy of the output, $H(\\hat{Y})$. To do this, we need the probability distribution of $\\hat{Y}$. Let $q = P(\\hat{Y}=1)$.\nUsing the law of total probability:\n$$q = P(\\hat{Y}=1) = P(\\hat{Y}=1|Y=0)P(Y=0) + P(\\hat{Y}=1|Y=1)P(Y=1)$$\nWe have:\n$P(\\hat{Y}=1|Y=0) = p_c = 0.1$\n$P(Y=0) = 0.75$\n$P(\\hat{Y}=1|Y=1) = 1-p_c = 0.9$\n$P(Y=1) = 0.25$\n$$q = (0.1)(0.75) + (0.9)(0.25) = 0.075 + 0.225 = 0.3$$\nSo, the output $\\hat{Y}$ is a Bernoulli(0.3) random variable. Its entropy is:\n$$H(\\hat{Y}) = H_b(0.3) = -0.3 \\log_2(0.3) - (1-0.3) \\log_2(0.7) \\approx 0.8812909 \\text{ bits/symbol}$$\n\nFinally, we can calculate the rate $R'$:\n$$R' = H(\\hat{Y}) - H(\\hat{Y}|Y) \\approx 0.8812909 - 0.4689956$$\n$$R' \\approx 0.4122953 \\text{ bits/symbol}$$\nRounding to four significant figures, we get $R' = 0.4123$ bits/symbol.\n\nThe resulting pair $(D', R')$ is $(0.1000, 0.4123)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 0.1000  0.4123 \\end{pmatrix}}$$"
        }
    ]
}