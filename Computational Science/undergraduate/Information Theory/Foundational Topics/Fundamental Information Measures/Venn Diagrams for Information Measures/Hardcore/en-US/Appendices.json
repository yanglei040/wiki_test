{
    "hands_on_practices": [
        {
            "introduction": "To build a strong intuition for information measures, it is often helpful to start with extreme scenarios. This exercise explores the case of perfect dependence, where one random variable is an identical copy of another. By analyzing this simplified situation, you can clearly see how the concepts of joint entropy and mutual information behave, providing a foundational understanding before moving to more complex, partial dependencies .",
            "id": "1667600",
            "problem": "In information theory, various measures quantify the uncertainty and shared information between random variables. These measures are often conceptually represented using Venn-like diagrams, where sets represent random variables and their areas correspond to entropy values.\n\nLet $X$ and $Y$ be two discrete random variables. The key quantities are:\n-   **Entropy $H(X)$**: The area of the circle corresponding to $X$, representing its total uncertainty.\n-   **Joint Entropy $H(X,Y)$**: The total area of the union of the two circles, representing the total uncertainty of the pair $(X,Y)$.\n-   **Mutual Information $I(X;Y)$**: The area of the intersection of the two circles, representing the information shared between $X$ and $Y$.\n-   **Conditional Entropy $H(X|Y)$**: The area of the part of the $X$ circle that does not overlap with the $Y$ circle, representing the uncertainty remaining in $X$ after $Y$ is known.\n\nConsider a specific scenario where the random variable $Y$ is an identical copy of the random variable $X$. This means that for any given outcome, the value of $Y$ is always the same as the value of $X$. Assume that $X$ is not a deterministic constant, so its entropy $H(X)$ is strictly positive. Let $H_0 = H(X)$.\n\nWhich of the following statements provides the most accurate and complete description of the relationships between the information measures for this scenario?\n\nA. $I(X;Y) = 0$ and $H(X,Y) = 2H_0$.\n\nB. $H(X,Y) = H_0$ and $I(X;Y) = H_0$.\n\nC. $H(X,Y) > H_0$ and $I(X;Y)  H_0$.\n\nD. $H(X,Y) = H_0$ and $I(X;Y)  H_0$.\n\nE. $H(X,Y) = 0$.",
            "solution": "The problem asks us to determine the relationship between the fundamental information measures for two random variables, $X$ and $Y$, under the condition that $Y$ is an identical copy of $X$. This means $Y=X$. We assume $H(X) = H_0 > 0$.\n\nFirst, let's analyze the direct consequences of the condition $Y=X$.\nSince $Y$ is an identical copy of $X$, they must have the same probability distribution. The entropy of a random variable is determined solely by its probability distribution. Therefore, the entropy of $Y$ must be equal to the entropy of $X$.\n$$H(Y) = H(X) = H_0$$\n\nNext, we evaluate the conditional entropies. The conditional entropy $H(Y|X)$ quantifies the average uncertainty remaining about $Y$ when the value of $X$ is known.\nGiven the condition $Y=X$, if we know the value of $X$, we know the value of $Y$ with perfect certainty. There is no remaining uncertainty. Thus, the conditional entropy $H(Y|X)$ must be zero.\n$$H(Y|X) = 0$$\nBy a symmetrical argument, if we know the value of $Y$, we also know the value of $X$ with perfect certainty. Therefore, the conditional entropy $H(X|Y)$ is also zero.\n$$H(X|Y) = 0$$\n\nNow, we can use the fundamental identities of information theory to find the joint entropy $H(X,Y)$ and the mutual information $I(X;Y)$.\n\nThe chain rule for entropy relates joint and conditional entropies:\n$$H(X,Y) = H(X) + H(Y|X)$$\nSubstituting the values we have found:\n$$H(X,Y) = H_0 + 0 = H_0$$\nSo, the joint entropy of the pair $(X,Y)$ is equal to the entropy of $X$ alone. This makes intuitive sense: since $Y$ is just a copy of $X$, the pair $(X,Y)$ contains no more information or uncertainty than $X$ by itself. In terms of the Venn diagram, this means the total area of the union of the two circles is equal to the area of the circle for $X$.\n\nNext, let's find the mutual information $I(X;Y)$. Mutual information can be defined as:\n$$I(X;Y) = H(X) - H(X|Y)$$\nSubstituting the values we have found:\n$$I(X;Y) = H_0 - 0 = H_0$$\nThe mutual information is also equal to the entropy of $X$. This also makes sense: since $X$ and $Y$ are identical, all the information in $X$ is shared with $Y$. In the Venn diagram, this means the intersection area is equal to the area of the circle for $X$.\n\nLet's summarize our findings:\n1. $H(X) = H_0$\n2. $H(Y) = H_0$\n3. $H(X|Y) = 0$\n4. $H(Y|X) = 0$\n5. $H(X,Y) = H_0$\n6. $I(X;Y) = H_0$\n\nThis set of relations corresponds to a Venn diagram where the two circles, representing $X$ and $Y$, are of equal size and perfectly overlap.\n\nNow we evaluate the given options:\nA. $I(X;Y) = 0$ and $H(X,Y) = 2H_0$. This describes two independent random variables. Our variables are perfectly dependent, so this is incorrect.\nB. $H(X,Y) = H_0$ and $I(X;Y) = H_0$. This perfectly matches our derived results for joint entropy and mutual information. This is the correct option.\nC. $H(X,Y) > H_0$ and $I(X;Y)  H_0$. This describes the general case of two variables that are correlated but not perfectly determined by each other (i.e., their circles partially overlap). This is incorrect.\nD. $H(X,Y) = H_0$ and $I(X;Y)  H_0$. This would imply $H(X,Y) = H(X)$ but $I(X;Y)  H(X)$. From $I(X;Y) = H(X) - H(X|Y)$, this would mean $H(X|Y) > 0$, contradicting our finding that $H(X|Y)=0$. This case corresponds to one circle being a proper subset of another, which would mean $H(Y)  H(X)$, but we know $H(X)=H(Y)$. This option is incorrect.\nE. $H(X,Y) = 0$. This would only be true if $H(X)=0$, which means $X$ is a deterministic constant. The problem statement specifies that $H(X)>0$. This is incorrect.\n\nThus, the only option that accurately describes the scenario is B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "As we move from simple two-variable systems to more realistic scenarios involving multiple interacting components, our visual model must also evolve. This practice extends the Venn diagram analogy to three variables, allowing for a more nuanced decomposition of information. By representing entropies as combinations of distinct, atomic regions, you can visually prove one of the fundamental properties of information: that conditioning on more variables can never increase uncertainty, a principle elegantly expressed as $H(X|Y) \\ge H(X|Y,Z)$ .",
            "id": "1667606",
            "problem": "Consider the information-theoretic quantities associated with three discrete random variables, $X$, $Y$, and $Z$. A common visualization tool is a Venn diagram where the area of each circle represents the entropy of a variable (e.g., area of circle X corresponds to $H(X)$), and overlapping areas represent mutual information. The diagram is partitioned into seven disjoint, atomic regions. Let the non-negative measures of information content corresponding to the areas of these regions be denoted by lowercase letters as follows:\n\n-   $a$: Information content unique to $X$.\n-   $b$: Information content unique to $Y$.\n-   $c$: Information content unique to $Z$.\n-   $d$: Information content shared between $X$ and $Y$ only.\n-   $e$: Information content shared between $X$ and $Z$ only.\n-   $f$: Information content shared between $Y$ and $Z$ only.\n-   $g$: Information content shared among all three variables, $X$, $Y$, and $Z$.\n\nBased on this model, the conditional entropy $H(X|Y)$ is the measure of the part of the $X$ circle that does not overlap with the $Y$ circle. Similarly, the conditional entropy $H(X|Y,Z)$ is the measure of the part of the $X$ circle that does not overlap with the union of the $Y$ and $Z$ circles.\n\nWhich of the following statements correctly identifies the expressions for $H(X|Y)$ and $H(X|Y,Z)$ in terms of the atomic regions and the resulting relationship between them?\n\nA. $H(X|Y) = a+d$ and $H(X|Y,Z) = a$. The relationship between them cannot be determined without knowing the specific probability distributions.\n\nB. $H(X|Y) = a+e$ and $H(X|Y,Z) = a+g$. This implies $H(X|Y) \\le H(X|Y,Z)$.\n\nC. $H(X|Y) = a+e$ and $H(X|Y,Z) = a$. This implies $H(X|Y) \\ge H(X|Y,Z)$.\n\nD. $H(X|Y) = a$ and $H(X|Y,Z) = a+e$. This implies $H(X|Y) \\le H(X|Y,Z)$.\n\nE. $H(X|Y) = a+d+e+g$ and $H(X|Y,Z) = a+d+e$. This implies $H(X|Y) \\ge H(X|Y,Z)$.",
            "solution": "By construction of the information diagram, each atomic region is non-negative and\n$$\nH(X)=a+d+e+g,\\quad I(X;Y)=d+g,\\quad I(X;Z)=e+g.\n$$\nThe conditional entropy definition gives\n$$\nH(X|Y)=H(X)-I(X;Y).\n$$\nSubstituting the atomic-region expressions,\n$$\nH(X|Y)=(a+d+e+g)-(d+g)=a+e.\n$$\nFor conditioning on both $Y$ and $Z$, we use that $I(X;Y,Z)$ equals the overlap of the $X$ circle with the union of the $Y$ and $Z$ circles, i.e., $d+e+g$. Hence\n$$\nH(X|Y,Z)=H(X)-I(X;Y,Z)=(a+d+e+g)-(d+e+g)=a.\n$$\nTherefore,\n$$\nH(X|Y)-H(X|Y,Z)=(a+e)-a=e\\ge 0,\n$$\nwhich implies $H(X|Y)\\ge H(X|Y,Z)$.\n\nThese expressions and the inequality correspond to option C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "The true power of information theory lies in its ability to quantify and solve real-world problems. This final practice moves from abstract diagrams and identities to a concrete engineering application, where you must analyze data from multiple sensors. You will apply the relationships between joint and conditional entropies to calculate a meaningful quantity—the reduction in uncertainty gained from an additional data source—using provided numerical data, thus bridging the gap between theory and practical analysis .",
            "id": "1667605",
            "problem": "An engineer is designing the sensor fusion system for a new autonomous drone. The system integrates data from three main sources, which are modeled as discrete random variables: a high-resolution camera stream ($X$), a Light Detection and Ranging (LIDAR) point cloud ($Y$), and an Inertial Measurement Unit (IMU) ($Z$). To optimize the data processing pipeline, the engineer performs an information-theoretic analysis of the sensor data. The analysis yields the following entropy values, all measured in bits:\n\n-   The entropy of the joint stream from the camera and IMU, $H(X,Z)$, is $5.20$ bits.\n-   The entropy of the joint stream from the LIDAR and IMU, $H(Y,Z)$, is $4.80$ bits.\n-   The entropy of the IMU stream alone, $H(Z)$, is $3.50$ bits.\n-   The entropy of the joint stream from all three sensors, $H(X,Y,Z)$, is $6.10$ bits.\n\nYour task is to calculate the reduction in uncertainty about the camera data $X$ when the LIDAR data $Y$ is also observed, given that the IMU data $Z$ is already known. This quantity is formally expressed as the difference in conditional entropies: $H(X|Z) - H(X|Y,Z)$. Express your final answer in bits, rounded to two significant figures.",
            "solution": "We are asked to compute the reduction in uncertainty about $X$ when $Y$ is observed, given $Z$, which is $H(X|Z)-H(X|Y,Z)$. Using the definitions of conditional entropy in terms of joint entropies, we have\n$$\nH(X|Z)=H(X,Z)-H(Z),\n$$\nand\n$$\nH(X|Y,Z)=H(X,Y,Z)-H(Y,Z).\n$$\nTherefore,\n$$\nH(X|Z)-H(X|Y,Z)=\\left[H(X,Z)-H(Z)\\right]-\\left[H(X,Y,Z)-H(Y,Z)\\right].\n$$\nRearranging terms gives\n$$\nH(X|Z)-H(X|Y,Z)=H(X,Z)+H(Y,Z)-H(Z)-H(X,Y,Z),\n$$\nwhich equals the conditional mutual information $I(X;Y|Z)$. Substituting the given numerical values,\n$$\nH(X|Z)-H(X|Y,Z)=5.20+4.80-3.50-6.10.\n$$\nCompute step by step:\n$$\n5.20+4.80=10.00,\\quad 3.50+6.10=9.60,\n$$\nso\n$$\nH(X|Z)-H(X|Y,Z)=10.00-9.60=0.40.\n$$\nRounded to two significant figures, the result is $0.40$ bits.",
            "answer": "$$\\boxed{0.40}$$"
        }
    ]
}