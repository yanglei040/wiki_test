{
    "hands_on_practices": [
        {
            "introduction": "为了巩固信息度量的基本概念，我们从一个极限情况开始：两个随机变量完全相同。这个练习将帮助你直观地理解当变量间存在完全依赖时，联合熵和互信息会发生什么变化。通过这个思想实验 ，你可以验证维恩图表示法是否与信息论的基本恒等式保持一致，从而为理解更复杂的关系打下坚实基础。",
            "id": "1667600",
            "problem": "在信息论中，有多种度量用于量化随机变量之间的不确定性和共享信息。这些度量通常使用类Venn图进行概念上的表示，其中集合代表随机变量，其面积对应于熵值。\n\n设 $X$ 和 $Y$ 是两个离散随机变量。关键的量有：\n-   **熵 $H(X)$**：对应于 $X$ 的圆的面积，表示其总不确定性。\n-   **联合熵 $H(X,Y)$**：两个圆并集的总面积，表示随机变量对 $(X,Y)$ 的总不确定性。\n-   **互信息 $I(X;Y)$**：两个圆交集的面积，表示 $X$ 和 $Y$ 之间共享的信息。\n-   **条件熵 $H(X|Y)$**：$X$ 圆中不与 $Y$ 圆重叠部分的面积，表示在已知 $Y$ 的情况下 $X$ 中剩余的不确定性。\n\n考虑一个特定场景，其中随机变量 $Y$ 是随机变量 $X$ 的一个完全相同的副本。这意味着对于任何给定的结果，$Y$ 的值总是与 $X$ 的值相同。假设 $X$ 不是一个确定性常数，因此其熵 $H(X)$ 严格为正。令 $H_0 = H(X)$。\n\n以下哪个陈述为该场景下的信息度量关系提供了最准确和完整的描述？\n\nA. $I(X;Y) = 0$ 且 $H(X,Y) = 2H_0$。\n\nB. $H(X,Y) = H_0$ 且 $I(X;Y) = H_0$。\n\nC. $H(X,Y) > H_0$ 且 $I(X;Y)  H_0$。\n\nD. $H(X,Y) = H_0$ 且 $I(X;Y)  H_0$。\n\nE. $H(X,Y) = 0$。",
            "solution": "问题要求我们确定在随机变量 $Y$ 是 $X$ 的一个完全相同副本的条件下，两个随机变量 $X$ 和 $Y$ 的基本信息度量之间的关系。这意味着 $Y=X$。我们假设 $H(X) = H_0  0$。\n\n首先，让我们分析条件 $Y=X$ 的直接推论。\n由于 $Y$ 是 $X$ 的一个完全相同副本，它们必须具有相同的概率分布。随机变量的熵仅由其概率分布决定。因此，$Y$ 的熵必须等于 $X$ 的熵。\n$$H(Y) = H(X) = H_0$$\n\n接下来，我们评估条件熵。条件熵 $H(Y|X)$ 量化了在已知 $X$ 的值时，关于 $Y$ 剩余的平均不确定性。\n在 $Y=X$ 的条件下，如果我们知道 $X$ 的值，我们就能完全确定地知道 $Y$ 的值。不存在剩余的不确定性。因此，条件熵 $H(Y|X)$ 必须为零。\n$$H(Y|X) = 0$$\n通过对称的论证，如果我们知道 $Y$ 的值，我们也能完全确定地知道 $X$ 的值。因此，条件熵 $H(X|Y)$ 也为零。\n$$H(X|Y) = 0$$\n\n现在，我们可以使用信息论的基本恒等式来求联合熵 $H(X,Y)$ 和互信息 $I(X;Y)$。\n\n熵的链式法则关联了联合熵和条件熵：\n$$H(X,Y) = H(X) + H(Y|X)$$\n代入我们已经求得的值：\n$$H(X,Y) = H_0 + 0 = H_0$$\n所以，随机变量对 $(X,Y)$ 的联合熵等于单独 $X$ 的熵。这在直觉上是合理的：因为 $Y$ 只是 $X$ 的一个副本，所以随机变量对 $(X,Y)$ 包含的信息或不确定性并不比单独的 $X$ 更多。在Venn图的语境下，这意味着两个圆并集的总面积等于 $X$ 圆的面积。\n\n接下来，让我们求解互信息 $I(X;Y)$。互信息可以定义为：\n$$I(X;Y) = H(X) - H(X|Y)$$\n代入我们已经求得的值：\n$$I(X;Y) = H_0 - 0 = H_0$$\n互信息也等于 $X$ 的熵。这也合理：因为 $X$ 和 $Y$ 是相同的，所以 $X$ 中的所有信息都与 $Y$ 共享。在Venn图中，这意味着交集面积等于 $X$ 圆的面积。\n\n让我们总结一下我们的发现：\n1. $H(X) = H_0$\n2. $H(Y) = H_0$\n3. $H(X|Y) = 0$\n4. $H(Y|X) = 0$\n5. $H(X,Y) = H_0$\n6. $I(X;Y) = H_0$\n\n这组关系对应于一个Venn图，其中代表 $X$ 和 $Y$ 的两个圆大小相等且完全重叠。\n\n现在我们来评估给定的选项：\nA. $I(X;Y) = 0$ 且 $H(X,Y) = 2H_0$。这描述了两个独立的随机变量。我们的变量是完全相关的，所以这是不正确的。\nB. $H(X,Y) = H_0$ 且 $I(X;Y) = H_0$。这与我们推导出的联合熵和互信息的结果完全匹配。这是正确选项。\nC. $H(X,Y) > H_0$ 且 $I(X;Y)  H_0$。这描述了两个变量相关但不能完全相互确定的一般情况（即，它们的圆部分重叠）。这是不正确的。\nD. $H(X,Y) = H_0$ 且 $I(X;Y)  H_0$。这将意味着 $H(X,Y) = H(X)$ 但 $I(X;Y)  H(X)$。根据 $I(X;Y) = H(X) - H(X|Y)$，这意味着 $H(X|Y) > 0$，这与我们发现的 $H(X|Y)=0$ 相矛盾。这种情况对应于一个圆是另一个圆的真子集，这意味着 $H(Y)  H(X)$，但我们知道 $H(X)=H(Y)$。此选项不正确。\nE. $H(X,Y) = 0$。这仅在 $H(X)=0$ 时成立，这意味着 $X$ 是一个确定性常数。题目陈述中指明 $H(X)0$。这是不正确的。\n\n因此，唯一准确描述该场景的选项是B。",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "在掌握了双变量系统后，我们自然地过渡到更复杂的三变量场景。这个练习  将信息分解为七个“原子”区域，让你能够可视化地证明一个核心不等式。这不仅加深了你对条件熵的理解，也揭示了信息论中的一个深刻原理：获取更多信息（即增加条件）不会增加不确定性。",
            "id": "1667606",
            "problem": "考虑与三个离散随机变量 $X$、$Y$ 和 $Z$ 相关的信息论量。一种常见的可视化工具是维恩图，其中每个圆的面积代表一个变量的熵（例如，圆X的面积对应于 $H(X)$），重叠区域代表互信息。该图被划分为七个不相交的原子区域。设对应于这些区域面积的信息内容的非负度量用小写字母表示如下：\n\n-   $a$：$X$ 独有的信息内容。\n-   $b$：$Y$ 独有的信息内容。\n-   $c$：$Z$ 独有的信息内容。\n-   $d$：仅由 $X$ 和 $Y$ 共享的信息内容。\n-   $e$：仅由 $X$ 和 $Z$ 共享的信息内容。\n-   $f$：仅由 $Y$ 和 $Z$ 共享的信息内容。\n-   $g$：由所有三个变量 $X$、$Y$ 和 $Z$ 共享的信息内容。\n\n基于此模型，条件熵 $H(X|Y)$ 是 $X$ 圆中不与 $Y$ 圆重叠部分的度量。类似地，条件熵 $H(X|Y,Z)$ 是 $X$ 圆中不与 $Y$ 圆和 $Z$ 圆的并集重叠部分的度量。\n\n以下哪个陈述正确地指出了用原子区域表示的 $H(X|Y)$ 和 $H(X|Y,Z)$ 的表达式，以及它们之间的最终关系？\n\nA. $H(X|Y) = a+d$ 且 $H(X|Y,Z) = a$。在不知道具体概率分布的情况下，无法确定它们之间的关系。\n\nB. $H(X|Y) = a+e$ 且 $H(X|Y,Z) = a+g$。这意味着 $H(X|Y) \\le H(X|Y,Z)$。\n\nC. $H(X|Y) = a+e$ 且 $H(X|Y,Z) = a$。这意味着 $H(X|Y) \\ge H(X|Y,Z)$。\n\nD. $H(X|Y) = a$ 且 $H(X|Y,Z) = a+e$。这意味着 $H(X|Y) \\le H(X|Y,Z)$。\n\nE. $H(X|Y) = a+d+e+g$ 且 $H(X|Y,Z) = a+d+e$。这意味着 $H(X|Y) \\ge H(X|Y,Z)$。",
            "solution": "根据信息图的构造，每个原子区域都是非负的，并且\n$$\nH(X)=a+d+e+g,\\quad I(X;Y)=d+g,\\quad I(X;Z)=e+g.\n$$\n条件熵的定义给出\n$$\nH(X|Y)=H(X)-I(X;Y).\n$$\n代入原子区域的表达式，\n$$\nH(X|Y)=(a+d+e+g)-(d+g)=a+e.\n$$\n对于以 $Y$ 和 $Z$ 为条件的情况，我们使用 $I(X;Y,Z)$ 等于 $X$ 圆与 $Y$ 圆和 $Z$ 圆并集的重叠部分，即 $d+e+g$。因此\n$$\nH(X|Y,Z)=H(X)-I(X;Y,Z)=(a+d+e+g)-(d+e+g)=a.\n$$\n因此，\n$$\nH(X|Y)-H(X|Y,Z)=(a+e)-a=e\\ge 0,\n$$\n这意味着 $H(X|Y)\\ge H(X|Y,Z)$。\n\n这些表达式和不等式对应于选项 C。",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "理论的最终目的是应用于实践。这个练习  将我们从抽象的图表带入一个具体的工程问题，即无人机的传感器数据融合。你需要运用之前建立的视觉直觉和相关的熵恒等式来解决一个定量问题，计算出不同信息源之间的条件互信息，体会这些概念在实际系统分析中的力量。",
            "id": "1667605",
            "problem": "一位工程师正在为一架新型自动驾驶无人机设计传感器融合系统。该系统整合了来自三个主要来源的数据，这些数据被建模为离散随机变量：高分辨率摄像头数据流（$X$）、激光雷达（LIDAR）点云（$Y$）和惯性测量单元（IMU）（$Z$）。为了优化数据处理流程，该工程师对传感器数据进行了信息论分析。分析得出了以下熵值，均以比特为单位：\n\n-   来自摄像头和IMU的联合数据流的熵，$H(X,Z)$，为 $5.20$ 比特。\n-   来自LIDAR和IMU的联合数据流的熵，$H(Y,Z)$，为 $4.80$ 比特。\n-   仅IMU数据流的熵，$H(Z)$，为 $3.50$ 比特。\n-   来自所有三个传感器的联合数据流的熵，$H(X,Y,Z)$，为 $6.10$ 比特。\n\n您的任务是，在IMU数据$Z$已知的情况下，计算当同时观测到LIDAR数据$Y$时，关于摄像头数据$X$的不确定性的减少量。这个量在形式上表示为条件熵之差：$H(X|Z) - H(X|Y,Z)$。请用比特表示您的最终答案，并四舍五入到两位有效数字。",
            "solution": "我们需要计算在给定$Z$的情况下，当观测到$Y$时关于$X$的不确定性的减少量，即 $H(X|Z)-H(X|Y,Z)$。根据用联合熵表示的条件熵的定义，我们有\n$$\nH(X|Z)=H(X,Z)-H(Z),\n$$\n和\n$$\nH(X|Y,Z)=H(X,Y,Z)-H(Y,Z).\n$$\n因此，\n$$\nH(X|Z)-H(X|Y,Z)=\\left[H(X,Z)-H(Z)\\right]-\\left[H(X,Y,Z)-H(Y,Z)\\right].\n$$\n重新整理各项可得\n$$\nH(X|Z)-H(X|Y,Z)=H(X,Z)+H(Y,Z)-H(Z)-H(X,Y,Z),\n$$\n这等于条件互信息 $I(X;Y|Z)$。代入给定的数值，\n$$\nH(X|Z)-H(X|Y,Z)=5.20+4.80-3.50-6.10.\n$$\n逐步计算：\n$$\n5.20+4.80=10.00,\\quad 3.50+6.10=9.60,\n$$\n所以\n$$\nH(X|Z)-H(X|Y,Z)=10.00-9.60=0.40.\n$$\n四舍五入到两位有效数字，结果为 $0.40$ 比特。",
            "answer": "$$\\boxed{0.40}$$"
        }
    ]
}