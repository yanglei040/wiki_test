{
    "hands_on_practices": [
        {
            "introduction": "We begin by exploring an intuitive, foundational scenario. This practice examines the case of perfect dependence, where a random variable $Y$ is an identical copy of $X$, to solidify the meanings of joint entropy $H(X,Y)$ and mutual information $I(X;Y)$. By analyzing this extreme case, the visual representation of two completely overlapping circles becomes a clear anchor for understanding how these diagrams map to fundamental concepts. ",
            "id": "1667600",
            "problem": "In information theory, various measures quantify the uncertainty and shared information between random variables. These measures are often conceptually represented using Venn-like diagrams, where sets represent random variables and their areas correspond to entropy values.\n\nLet $X$ and $Y$ be two discrete random variables. The key quantities are:\n-   **Entropy $H(X)$**: The area of the circle corresponding to $X$, representing its total uncertainty.\n-   **Joint Entropy $H(X,Y)$**: The total area of the union of the two circles, representing the total uncertainty of the pair $(X,Y)$.\n-   **Mutual Information $I(X;Y)$**: The area of the intersection of the two circles, representing the information shared between $X$ and $Y$.\n-   **Conditional Entropy $H(X|Y)$**: The area of the part of the $X$ circle that does not overlap with the $Y$ circle, representing the uncertainty remaining in $X$ after $Y$ is known.\n\nConsider a specific scenario where the random variable $Y$ is an identical copy of the random variable $X$. This means that for any given outcome, the value of $Y$ is always the same as the value of $X$. Assume that $X$ is not a deterministic constant, so its entropy $H(X)$ is strictly positive. Let $H_0 = H(X)$.\n\nWhich of the following statements provides the most accurate and complete description of the relationships between the information measures for this scenario?\n\nA. $I(X;Y) = 0$ and $H(X,Y) = 2H_0$.\n\nB. $H(X,Y) = H_0$ and $I(X;Y) = H_0$.\n\nC. $H(X,Y) > H_0$ and $I(X;Y) < H_0$.\n\nD. $H(X,Y) = H_0$ and $I(X;Y) < H_0$.\n\nE. $H(X,Y) = 0$.",
            "solution": "The problem asks us to determine the relationship between the fundamental information measures for two random variables, $X$ and $Y$, under the condition that $Y$ is an identical copy of $X$. This means $Y=X$. We assume $H(X) = H_0 > 0$.\n\nFirst, let's analyze the direct consequences of the condition $Y=X$.\nSince $Y$ is an identical copy of $X$, they must have the same probability distribution. The entropy of a random variable is determined solely by its probability distribution. Therefore, the entropy of $Y$ must be equal to the entropy of $X$.\n$$H(Y) = H(X) = H_0$$\n\nNext, we evaluate the conditional entropies. The conditional entropy $H(Y|X)$ quantifies the average uncertainty remaining about $Y$ when the value of $X$ is known.\nGiven the condition $Y=X$, if we know the value of $X$, we know the value of $Y$ with perfect certainty. There is no remaining uncertainty. Thus, the conditional entropy $H(Y|X)$ must be zero.\n$$H(Y|X) = 0$$\nBy a symmetrical argument, if we know the value of $Y$, we also know the value of $X$ with perfect certainty. Therefore, the conditional entropy $H(X|Y)$ is also zero.\n$$H(X|Y) = 0$$\n\nNow, we can use the fundamental identities of information theory to find the joint entropy $H(X,Y)$ and the mutual information $I(X;Y)$.\n\nThe chain rule for entropy relates joint and conditional entropies:\n$$H(X,Y) = H(X) + H(Y|X)$$\nSubstituting the values we have found:\n$$H(X,Y) = H_0 + 0 = H_0$$\nSo, the joint entropy of the pair $(X,Y)$ is equal to the entropy of $X$ alone. This makes intuitive sense: since $Y$ is just a copy of $X$, the pair $(X,Y)$ contains no more information or uncertainty than $X$ by itself. In terms of the Venn diagram, this means the total area of the union of the two circles is equal to the area of the circle for $X$.\n\nNext, let's find the mutual information $I(X;Y)$. Mutual information can be defined as:\n$$I(X;Y) = H(X) - H(X|Y)$$\nSubstituting the values we have found:\n$$I(X;Y) = H_0 - 0 = H_0$$\nThe mutual information is also equal to the entropy of $X$. This also makes sense: since $X$ and $Y$ are identical, all the information in $X$ is shared with $Y$. In the Venn diagram, this means the intersection area is equal to the area of the circle for $X$.\n\nLet's summarize our findings:\n1. $H(X) = H_0$\n2. $H(Y) = H_0$\n3. $H(X|Y) = 0$\n4. $H(Y|X) = 0$\n5. $H(X,Y) = H_0$\n6. $I(X;Y) = H_0$\n\nThis set of relations corresponds to a Venn diagram where the two circles, representing $X$ and $Y$, are of equal size and perfectly overlap.\n\nNow we evaluate the given options:\nA. $I(X;Y) = 0$ and $H(X,Y) = 2H_0$. This describes two independent random variables. Our variables are perfectly dependent, so this is incorrect.\nB. $H(X,Y) = H_0$ and $I(X;Y) = H_0$. This perfectly matches our derived results for joint entropy and mutual information. This is the correct option.\nC. $H(X,Y) > H_0$ and $I(X;Y) < H_0$. This describes the general case of two variables that are correlated but not perfectly determined by each other (i.e., their circles partially overlap). This is incorrect.\nD. $H(X,Y) = H_0$ and $I(X;Y) < H_0$. This would imply $H(X,Y) = H(X)$ but $I(X;Y) < H(X)$. From $I(X;Y) = H(X) - H(X|Y)$, this would mean $H(X|Y) > 0$, contradicting our finding that $H(X|Y)=0$. This case corresponds to one circle being a proper subset of another, which would mean $H(Y) < H(X)$, but we know $H(X)=H(Y)$. This option is incorrect.\nE. $H(X,Y) = 0$. This would only be true if $H(X)=0$, which means $X$ is a deterministic constant. The problem statement specifies that $H(X)>0$. This is incorrect.\n\nThus, the only option that accurately describes the scenario is B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "To move from specific cases to a general framework, we must formalize the connection between diagram areas and information quantities. This exercise introduces the concept of \"atomic regions\" as the basic building blocks of a two-variable information diagram. By expressing standard measures like $H(X)$ and $H(X,Y)$ in terms of these disjoint components, you will develop a systematic method for translating between diagrams and algebraic formulas. ",
            "id": "1667611",
            "problem": "In the study of information theory, a helpful analogy for understanding the relationships between different entropy measures involves using areas in a diagram. Consider two discrete random variables, $X$ and $Y$. The uncertainty associated with them can be modeled using three disjoint regions with non-negative information content, which we will denote as $C_X$, $C_{XY}$, and $C_Y$.\n\n- $C_X$ represents the information that is unique to $X$ (i.e., not shared with $Y$).\n- $C_{XY}$ represents the information that is common to both $X$ and $Y$.\n- $C_Y$ represents the information that is unique to $Y$ (i.e., not shared with $X$).\n\nUsing this content-based analogy, any standard information measure involving $X$ and $Y$ can be expressed as a sum of one or more of these information content values. For example, the entropy of $X$, denoted $H(X)$, represents the total information content of $X$. Per the analogy, it is given by the sum of the content unique to $X$ and the content shared with $Y$.\n\nBased on this model, which of the following expressions correctly represents the quantity $H(X,Y) - H(Y)$? Here, $H(X,Y)$ is the joint entropy of $X$ and $Y$, and $H(Y)$ is the entropy of $Y$.\n\nA) $C_X$\n\nB) $C_Y$\n\nC) $C_{XY}$\n\nD) $C_X + C_{XY}$\n\nE) $C_Y + C_{XY}$\n\nF) $C_X + C_Y$",
            "solution": "We model the entropies using the three non-negative disjoint contents: $C_{X}$ (unique to $X$), $C_{XY}$ (shared by $X$ and $Y$), and $C_{Y}$ (unique to $Y$). By the content analogy:\n$$H(X)=C_{X}+C_{XY},\\quad H(Y)=C_{Y}+C_{XY},\\quad H(X,Y)=C_{X}+C_{XY}+C_{Y}.$$\nWe need $H(X,Y)-H(Y)$. Substituting the expressions above gives\n$$H(X,Y)-H(Y)=(C_{X}+C_{XY}+C_{Y})-(C_{Y}+C_{XY})=C_{X}.$$\nEquivalently, using the identity $H(X|Y) = H(X,Y) - H(Y)$, this quantity equals the uncertainty in $X$ not explained by $Y$, which corresponds exactly to the unique content $C_{X}$. Therefore the correct option is $C_{X}$, i.e., A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "With a solid foundation in two-variable diagrams, we can extend this powerful visual tool to analyze more complex systems. This practice applies the atomic region model to a three-variable system involving $X$, $Y$, and $Z$, guiding you to visually confirm a key property of information: \"conditioning reduces entropy.\" By mapping conditional entropies like $H(X|Y)$ and $H(X|Y,Z)$ to their corresponding regions, you can intuitively prove that adding knowledge (conditioning on $Z$) cannot increase the uncertainty in $X$ given $Y$. ",
            "id": "1667606",
            "problem": "Consider the information-theoretic quantities associated with three discrete random variables, $X$, $Y$, and $Z$. A common visualization tool is a Venn diagram where the area of each circle represents the entropy of a variable (e.g., area of circle X corresponds to $H(X)$), and overlapping areas represent mutual information. The diagram is partitioned into seven disjoint, atomic regions. Let the non-negative measures of information content corresponding to the areas of these regions be denoted by lowercase letters as follows:\n\n-   $a$: Information content unique to $X$.\n-   $b$: Information content unique to $Y$.\n-   $c$: Information content unique to $Z$.\n-   $d$: Information content shared between $X$ and $Y$ only.\n-   $e$: Information content shared between $X$ and $Z$ only.\n-   $f$: Information content shared between $Y$ and $Z$ only.\n-   $g$: Information content shared among all three variables, $X$, $Y$, and $Z$.\n\nBased on this model, the conditional entropy $H(X|Y)$ is the measure of the part of the $X$ circle that does not overlap with the $Y$ circle. Similarly, the conditional entropy $H(X|Y,Z)$ is the measure of the part of the $X$ circle that does not overlap with the union of the $Y$ and $Z$ circles.\n\nWhich of the following statements correctly identifies the expressions for $H(X|Y)$ and $H(X|Y,Z)$ in terms of the atomic regions and the resulting relationship between them?\n\nA. $H(X|Y) = a+d$ and $H(X|Y,Z) = a$. The relationship between them cannot be determined without knowing the specific probability distributions.\n\nB. $H(X|Y) = a+e$ and $H(X|Y,Z) = a+g$. This implies $H(X|Y) \\le H(X|Y,Z)$.\n\nC. $H(X|Y) = a+e$ and $H(X|Y,Z) = a$. This implies $H(X|Y) \\ge H(X|Y,Z)$.\n\nD. $H(X|Y) = a$ and $H(X|Y,Z) = a+e$. This implies $H(X|Y) \\le H(X|Y,Z)$.\n\nE. $H(X|Y) = a+d+e+g$ and $H(X|Y,Z) = a+d+e$. This implies $H(X|Y) \\ge H(X|Y,Z)$.",
            "solution": "By construction of the information diagram, each atomic region is non-negative and\n$$\nH(X)=a+d+e+g,\\quad I(X;Y)=d+g,\\quad I(X;Z)=e+g.\n$$\nThe conditional entropy definition gives\n$$\nH(X|Y)=H(X)-I(X;Y).\n$$\nSubstituting the atomic-region expressions,\n$$\nH(X|Y)=(a+d+e+g)-(d+g)=a+e.\n$$\nFor conditioning on both $Y$ and $Z$, we use that $I(X;Y,Z)$ equals the overlap of the $X$ circle with the union of the $Y$ and $Z$ circles, i.e., $d+e+g$. Hence\n$$\nH(X|Y,Z)=H(X)-I(X;Y,Z)=(a+d+e+g)-(d+e+g)=a.\n$$\nTherefore,\n$$\nH(X|Y)-H(X|Y,Z)=(a+e)-a=e\\ge 0,\n$$\nwhich implies $H(X|Y) \\ge H(X|Y,Z)$.\n\nThese expressions and the inequality correspond to option C.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}