{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering the concept of entropy is learning to compute it directly from a probability distribution. This exercise provides a foundational workout in applying the Shannon entropy formula. By calculating the entropy of a data source whose symbol probabilities follow a pattern inspired by statistical mechanics, you will practice the core mechanics of the calculation while also appreciating the deep connections between information theory and physics. ",
            "id": "1620545",
            "problem": "A simplified model for a data source, such as a sensor monitoring a physical system, generates symbols from a set of four distinct types, labeled $\\{S_0, S_1, S_2, S_3\\}$. The probability $P(S_i)$ of generating symbol $S_i$ is governed by a statistical mechanical principle, where each symbol type corresponds to a discrete energy level $E_i$. The probability distribution is given by:\n$$ P(S_i) = \\frac{1}{Z} \\exp(-\\alpha E_i) $$\nwhere $\\alpha$ is a positive constant related to the system's properties and $Z$ is the partition function, defined as $Z = \\sum_{j=0}^{3} \\exp(-\\alpha E_j)$.\nThe energy levels for the four symbol types have been determined to be:\n- $E_0 = 0$\n- $E_1 = \\frac{\\ln(2)}{\\alpha}$\n- $E_2 = \\frac{2\\ln(2)}{\\alpha}$\n- $E_3 = \\frac{2\\ln(2)}{\\alpha}$\n\nCalculate the Shannon entropy $H$ of this data source. The entropy measures the average uncertainty or information content per symbol. Express your answer in units of bits. For the entropy calculation, the base of the logarithm must be 2.",
            "solution": "We are given a Boltzmann-type distribution over four symbols with energies $E_{i}$ and probabilities $P(S_{i}) = Z^{-1}\\exp(-\\alpha E_{i})$, where the partition function is $Z = \\sum_{j=0}^{3} \\exp(-\\alpha E_{j})$. With $E_{0} = 0$, $E_{1} = \\frac{\\ln(2)}{\\alpha}$, $E_{2} = \\frac{2\\ln(2)}{\\alpha}$, and $E_{3} = \\frac{2\\ln(2)}{\\alpha}$, compute the unnormalized weights $w_{i} = \\exp(-\\alpha E_{i})$:\n$$\nw_{0} = \\exp(0) = 1,\\quad\nw_{1} = \\exp\\!\\left(-\\alpha\\cdot\\frac{\\ln(2)}{\\alpha}\\right) = \\exp(-\\ln 2) = 2^{-1},\n$$\n$$\nw_{2} = \\exp\\!\\left(-\\alpha\\cdot\\frac{2\\ln(2)}{\\alpha}\\right) = \\exp(-2\\ln 2) = 2^{-2},\\quad\nw_{3} = 2^{-2}.\n$$\nThe partition function is\n$$\nZ = w_{0} + w_{1} + w_{2} + w_{3} = 1 + 2^{-1} + 2^{-2} + 2^{-2} = 1 + 2^{-1} + 2\\cdot 2^{-2} = 1 + 2^{-1} + 2^{-1} = 2.\n$$\nHence the probabilities are\n$$\nP(S_{0}) = \\frac{1}{2},\\quad P(S_{1}) = \\frac{2^{-1}}{2} = \\frac{1}{4},\\quad P(S_{2}) = \\frac{2^{-2}}{2} = \\frac{1}{8},\\quad P(S_{3}) = \\frac{1}{8}.\n$$\nThe Shannon entropy in bits is\n$$\nH = -\\sum_{i=0}^{3} P(S_{i})\\,\\log_{2} P(S_{i}) = -\\left[\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right) + \\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right) + \\frac{1}{8}\\log_{2}\\!\\left(\\frac{1}{8}\\right) + \\frac{1}{8}\\log_{2}\\!\\left(\\frac{1}{8}\\right)\\right].\n$$\nUsing $\\log_{2}(1/2) = -1$, $\\log_{2}(1/4) = -2$, and $\\log_{2}(1/8) = -3$, we obtain\n$$\nH = \\frac{1}{2}\\cdot 1 + \\frac{1}{4}\\cdot 2 + 2\\cdot\\frac{1}{8}\\cdot 3 = \\frac{1}{2} + \\frac{1}{2} + \\frac{3}{4} = \\frac{7}{4}.\n$$\nThus the entropy is $\\frac{7}{4}$ bits.",
            "answer": "$$\\boxed{\\frac{7}{4}}$$"
        },
        {
            "introduction": "Entropy is a dynamic measure of uncertainty that changes as our knowledge or the system itself changes. This next practice explores what happens to entropy when we lose the ability to distinguish between different outcomes. By starting with a system of maximum uncertainty and then merging states, you will see firsthand how a loss of detail leads to a quantifiable reduction in entropy, deepening your intuition for what the entropy value truly represents. ",
            "id": "1620519",
            "problem": "A simplified diagnostic system for a wireless communication network monitors transmission packets and classifies them into one of four mutually exclusive states: {S1, S2, S3, S4}. Initially, under normal operating conditions, all four states are observed to occur with equal probability.\n\nA system upgrade is performed to simplify the monitoring process. After the upgrade, the diagnostic tool no longer distinguishes between states S3 and S4. Instead, any occurrence of either S3 or S4 is reported as a single, combined state, which we will call S-prime (S'). The probabilities of states S1 and S2 remain unchanged, and the probability of the new state S' is the sum of the original probabilities of S3 and S4.\n\nThe uncertainty of the system's output is quantified by the Shannon entropy, $H$, defined as $H(X) = -\\sum_{i=1}^{n} p(x_i) \\log_2(p(x_i))$, where $p(x_i)$ is the probability of the $i$-th outcome.\n\nCalculate the entropy of the probability distribution for the states reported by the upgraded system. Express your answer in bits, rounded to four significant figures.",
            "solution": "Let the set of initial states be $X = \\{S1, S2, S3, S4\\}$. The problem states that these four states occur with equal probability. Since the sum of probabilities must be 1, the probability of each state is:\n$P(S1) = P(S2) = P(S3) = P(S4) = \\frac{1}{4}$.\n\nAfter the system upgrade, the states S3 and S4 are merged into a new state S'. The new set of observable states is $X' = \\{S1, S2, S'\\}$. We need to determine the probability distribution for this new set of states.\n\nThe probabilities of S1 and S2 are unchanged:\n$P_{new}(S1) = P(S1) = \\frac{1}{4}$\n$P_{new}(S2) = P(S2) = \\frac{1}{4}$\n\nThe probability of the new state S' is the sum of the probabilities of the original states that were merged to form it:\n$P_{new}(S') = P(S3) + P(S4) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$\n\nWe can check that the new probabilities sum to 1:\n$P_{new}(S1) + P_{new}(S2) + P_{new}(S') = \\frac{1}{4} + \\frac{1}{4} + \\frac{1}{2} = \\frac{2}{4} + \\frac{1}{2} = \\frac{1}{2} + \\frac{1}{2} = 1$.\nSo, the new probability distribution is $p_{new} = \\{\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{2}\\}$.\n\nNow, we calculate the entropy of this new distribution using the Shannon entropy formula, $H = -\\sum_{i} p_i \\log_2(p_i)$.\nThe sum will have three terms, one for each state in $X'$.\n\n$$H_{new} = - \\left[ P_{new}(S1) \\log_2(P_{new}(S1)) + P_{new}(S2) \\log_2(P_{new}(S2)) + P_{new}(S') \\log_2(P_{new}(S')) \\right]$$\n\nSubstitute the probability values into the formula:\n$$H_{new} = - \\left[ \\frac{1}{4} \\log_2\\left(\\frac{1}{4}\\right) + \\frac{1}{4} \\log_2\\left(\\frac{1}{4}\\right) + \\frac{1}{2} \\log_2\\left(\\frac{1}{2}\\right) \\right]$$\n\nTo simplify, we evaluate the logarithm terms:\n$\\log_2\\left(\\frac{1}{4}\\right) = \\log_2(2^{-2}) = -2$\n$\\log_2\\left(\\frac{1}{2}\\right) = \\log_2(2^{-1}) = -1$\n\nNow, substitute these values back into the entropy equation:\n$$H_{new} = - \\left[ \\frac{1}{4}(-2) + \\frac{1}{4}(-2) + \\frac{1}{2}(-1) \\right]$$\n$$H_{new} = - \\left[ -\\frac{2}{4} - \\frac{2}{4} - \\frac{1}{2} \\right]$$\n$$H_{new} = - \\left[ -\\frac{1}{2} - \\frac{1}{2} - \\frac{1}{2} \\right]$$\n$$H_{new} = - \\left[ -\\frac{3}{2} \\right]$$\n$$H_{new} = \\frac{3}{2} = 1.5$$\n\nThe problem asks for the answer in bits, rounded to four significant figures.\nThe calculated value is exactly 1.5. To express this with four significant figures, we write it as 1.500.",
            "answer": "$$\\boxed{1.500}$$"
        },
        {
            "introduction": "Beyond measuring the total uncertainty of a system, entropy is also a powerful tool for quantifying the uncertainty that *remains* after partial information is revealed. This scenario delves into the practical application of conditional entropy in the context of cryptography and information security. By analyzing an information leak, you will learn how to calculate the precise amount of uncertainty an attacker has eliminated, thereby measuring the remaining security of a secret key. ",
            "id": "1620533",
            "problem": "A fault-tolerant memory system is designed to store an 8-bit cryptographic key, denoted by the bit vector $K = (K_1, K_2, \\dots, K_8)$. Initially, the key is chosen uniformly at random from all possible $2^8$ bit strings. The memory system has a built-in error-checking mechanism that, due to a design flaw, inadvertently leaks some information to a potential attacker. This attacker does not learn any of the key bits directly, but is able to determine the parity of the first four bits and the parity of the last four bits. Specifically, the attacker learns the values of two quantities: $P_1 = K_1 \\oplus K_2 \\oplus K_3 \\oplus K_4$ and $P_2 = K_5 \\oplus K_6 \\oplus K_7 \\oplus K_8$, where $\\oplus$ denotes the exclusive OR (XOR) operation.\n\nGiven this information leakage, calculate the average remaining entropy of the secret key $K$. Express your answer in bits. The entropy should be calculated using the base-2 logarithm.",
            "solution": "Let $K=(K_{1},\\dots,K_{8})$ be uniformly distributed over $\\{0,1\\}^{8}$, and let $P_{1}=K_{1}\\oplus K_{2}\\oplus K_{3}\\oplus K_{4}$ and $P_{2}=K_{5}\\oplus K_{6}\\oplus K_{7}\\oplus K_{8}$. The attacker learns $(P_{1},P_{2})$, so the average remaining uncertainty about $K$ is the conditional entropy $H(K\\,|\\,P_{1},P_{2})$ with base-$2$ logarithms.\n\nUse the identity for deterministic leakage: since $(P_{1},P_{2})$ is a deterministic function of $K$, we have $H(P_{1},P_{2}\\,|\\,K)=0$, hence the mutual information satisfies $I(K;(P_{1},P_{2}))=H(P_{1},P_{2})$. Therefore,\n$$\nH(K\\,|\\,P_{1},P_{2}) \\;=\\; H(K)-I(K;(P_{1},P_{2})) \\;=\\; H(K)-H(P_{1},P_{2}).\n$$\nBecause $K$ is uniform over $2^{8}$ values, \n$$\nH(K)=\\log_{2}(2^{8})=8.\n$$\nNext, $P_{1}$ depends only on $(K_{1},\\dots,K_{4})$ and $P_{2}$ only on $(K_{5},\\dots,K_{8})$. Under the uniform distribution, these halves are independent and each parity is $\\mathrm{Bernoulli}\\!\\left(\\frac{1}{2}\\right)$. Thus\n$$\nH(P_{1})=-\\sum_{p\\in\\{0,1\\}}\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)=1,\\qquad H(P_{2})=1,\n$$\nand independence gives\n$$\nH(P_{1},P_{2})=H(P_{1})+H(P_{2})=2.\n$$\nTherefore,\n$$\nH(K\\,|\\,P_{1},P_{2})=8-2=6.\n$$\n\nAs a counting cross-check: fixing $(P_{1},P_{2})$ imposes one linear constraint on each group of four bits, leaving $4-1=3$ degrees of freedom per group, hence $2^{3}\\cdot 2^{3}=2^{6}$ possible keys are consistent with any given $(P_{1},P_{2})$. A uniform posterior over $2^{6}$ possibilities has entropy $\\log_{2}(2^{6})=6$, agreeing with the result above.",
            "answer": "$$\\boxed{6}$$"
        }
    ]
}