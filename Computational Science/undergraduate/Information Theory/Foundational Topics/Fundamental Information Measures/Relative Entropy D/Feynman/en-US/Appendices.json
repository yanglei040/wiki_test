{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering relative entropy is to get comfortable with the fundamental computation. This exercise provides a foundational workout by asking you to calculate the Kullback-Leibler (KL) divergence between two binomial distributions, which are essential models for sequences of independent trials. By working through this problem (), you will see how the divergence between distributions of sequences relates back to the divergence for a single event, building a core skill for comparing statistical models.",
            "id": "1654970",
            "problem": "In the field of statistical modeling, it is often necessary to quantify the difference between two probability distributions. One common measure is the Kullback-Leibler (KL) divergence, also known as relative entropy. For two discrete probability distributions $P(x)$ and $Q(x)$ defined over the same sample space $\\mathcal{X}$, the KL divergence of $Q$ from $P$ is given by:\n$$D_{KL}(P || Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\ln\\left(\\frac{P(x)}{Q(x)}\\right)$$\nwhere $\\ln$ denotes the natural logarithm.\n\nConsider two independent manufacturing processes creating a specific type of semiconductor.\n- Process 1 is modeled by a random variable $K_1$ that follows a binomial distribution with parameters $(n, p_1)$. This represents the number of non-defective semiconductors in a batch of size $n$, where $p_1$ is the probability of any single semiconductor being non-defective.\n- Process 2 is modeled by a random variable $K_2$ that also follows a binomial distribution, but with parameters $(n, p_2)$. Here, $p_2$ is the probability of a single semiconductor being non-defective.\n\nThe probability mass function (PMF) for a binomial distribution $B(n, p)$ is given by $P(K=k) = \\binom{n}{k} p^k (1-p)^{n-k}$ for $k \\in \\{0, 1, \\dots, n\\}$.\n\nLet $P_1$ denote the binomial distribution for Process 1 and $P_2$ denote the distribution for Process 2. Assuming $p_1, p_2 \\in (0,1)$, find a closed-form analytic expression for the Kullback-Leibler divergence $D_{KL}(P_1 || P_2)$ in terms of $n$, $p_1$, and $p_2$.",
            "solution": "We denote the PMFs of $K_{1} \\sim \\mathrm{Bin}(n,p_{1})$ and $K_{2} \\sim \\mathrm{Bin}(n,p_{2})$ by\n$$\nP_{1}(k)=\\binom{n}{k}p_{1}^{k}(1-p_{1})^{n-k}, \\quad\nP_{2}(k)=\\binom{n}{k}p_{2}^{k}(1-p_{2})^{n-k},\n$$\nfor $k \\in \\{0,1,\\dots,n\\}$. By definition, the Kullback-Leibler divergence of $P_{2}$ from $P_{1}$ is\n$$\nD_{KL}(P_{1}\\,\\|\\,P_{2})=\\sum_{k=0}^{n}P_{1}(k)\\,\\ln\\!\\left(\\frac{P_{1}(k)}{P_{2}(k)}\\right).\n$$\nCompute the likelihood ratio inside the logarithm:\n$$\n\\frac{P_{1}(k)}{P_{2}(k)}=\\frac{\\binom{n}{k}p_{1}^{k}(1-p_{1})^{n-k}}{\\binom{n}{k}p_{2}^{k}(1-p_{2})^{n-k}}\n=\\left(\\frac{p_{1}}{p_{2}}\\right)^{k}\\left(\\frac{1-p_{1}}{1-p_{2}}\\right)^{n-k}.\n$$\nTaking the logarithm yields\n$$\n\\ln\\!\\left(\\frac{P_{1}(k)}{P_{2}(k)}\\right)\n= k\\,\\ln\\!\\left(\\frac{p_{1}}{p_{2}}\\right) + (n-k)\\,\\ln\\!\\left(\\frac{1-p_{1}}{1-p_{2}}\\right).\n$$\nSubstitute back into the definition of $D_{KL}$:\n$$\nD_{KL}(P_{1}\\,\\|\\,P_{2})\n=\\sum_{k=0}^{n}P_{1}(k)\\left[ k\\,\\ln\\!\\left(\\frac{p_{1}}{p_{2}}\\right) + (n-k)\\,\\ln\\!\\left(\\frac{1-p_{1}}{1-p_{2}}\\right)\\right].\n$$\nSince the logarithmic factors do not depend on $k$, we can factor them out and recognize the sums as expectations under $K \\sim \\mathrm{Bin}(n,p_{1})$:\n$$\nD_{KL}(P_{1}\\,\\|\\,P_{2})\n=\\ln\\!\\left(\\frac{p_{1}}{p_{2}}\\right)\\sum_{k=0}^{n}k\\,P_{1}(k)\n+\\ln\\!\\left(\\frac{1-p_{1}}{1-p_{2}}\\right)\\sum_{k=0}^{n}(n-k)\\,P_{1}(k).\n$$\nUsing $\\sum_{k=0}^{n}k\\,P_{1}(k)=\\mathbb{E}_{P_{1}}[K]=n p_{1}$ and $\\sum_{k=0}^{n}(n-k)\\,P_{1}(k)=n-\\mathbb{E}_{P_{1}}[K]=n(1-p_{1})$, we obtain\n$$\nD_{KL}(P_{1}\\,\\|\\,P_{2})\n= n p_{1}\\,\\ln\\!\\left(\\frac{p_{1}}{p_{2}}\\right)\n+ n(1-p_{1})\\,\\ln\\!\\left(\\frac{1-p_{1}}{1-p_{2}}\\right).\n$$\nEquivalently, this is $n$ times the KL divergence between $\\mathrm{Bernoulli}(p_{1})$ and $\\mathrm{Bernoulli}(p_{2})$, and it is finite for $p_{1},p_{2}\\in(0,1)$.",
            "answer": "$$\\boxed{n\\left[p_{1}\\ln\\!\\left(\\frac{p_{1}}{p_{2}}\\right)+(1-p_{1})\\ln\\!\\left(\\frac{1-p_{1}}{1-p_{2}}\\right)\\right]}$$"
        },
        {
            "introduction": "While the definition of KL divergence is straightforward, its properties for multi-variable systems can be subtle and even counter-intuitive. This thought experiment () challenges the simple assumption that the divergence of a joint distribution is the sum of its marginals. By analyzing a carefully constructed scenario, you will apply the chain rule for relative entropy, a powerful identity that clarifies how dependencies between variables impact the overall divergence.",
            "id": "1655003",
            "problem": "Consider two binary random variables, $X$ and $Y$, each taking values in the set $\\{0, 1\\}$. We define two distinct joint probability distributions over these variables, denoted by $p(x,y)$ and $q(x,y)$.\n\nThese distributions are constructed from the following components:\n\nFirst, the marginal distributions for the random variable $X$ are given as:\n- For the first model, the distribution is $p(x)$, with $p(X=0) = \\frac{1}{2}$.\n- For the second model, the distribution is $q(x)$, with $q(X=0) = \\frac{1}{4}$.\n\nSecond, the conditional probability distribution of $Y$ given $X$ is identical for both models. Let's denote this common conditional distribution by $f(y|x)$, so that $p(y|x) = f(y|x)$ and $q(y|x) = f(y|x)$. The specific conditional probabilities are:\n- $f(Y=0 | X=0) = \\frac{3}{4}$\n- $f(Y=0 | X=1) = \\frac{1}{4}$\n\nThe full joint distributions are then formed by the product rule of probability: $p(x,y) = p(x)f(y|x)$ and $q(x,y) = q(x)f(y|x)$.\n\nThe Kullback-Leibler (KL) divergence, also known as relative entropy, between two discrete probability distributions $u(z)$ and $v(z)$ defined over the same alphabet is given by $D(u||v) = \\sum_z u(z) \\ln\\frac{u(z)}{v(z)}$, where the sum is over all possible outcomes $z$ and $\\ln$ denotes the natural logarithm.\n\nYour task is to compute the value of the quantity $\\Delta$, defined as:\n$$ \\Delta = D(p(x,y)||q(x,y)) - D(p(x)||q(x)) - D(p(y)||q(y)) $$\nExpress your answer as a single closed-form analytic expression.",
            "solution": "The problem asks for the computation of the quantity $\\Delta = D(p(x,y)||q(x,y)) - D(p(x)||q(x)) - D(p(y)||q(y))$.\n\nThe most direct way to evaluate this is to use the chain rule for Kullback-Leibler (KL) divergence, which states:\n$$ D(p(x,y)||q(x,y)) = D(p(x)||q(x)) + D(p(y|x)||q(y|x)) $$\nHere, $D(p(y|x)||q(y|x))$ is the conditional relative entropy, defined as $\\sum_{x} p(x) D(p(y|X=x)||q(y|X=x))$.\n\nSubstituting the chain rule into the expression for $\\Delta$:\n$$ \\Delta = \\left( D(p(x)||q(x)) + D(p(y|x)||q(y|x)) \\right) - D(p(x)||q(x)) - D(p(y)||q(y)) $$\nThe term $D(p(x)||q(x))$ cancels out, simplifying the expression to:\n$$ \\Delta = D(p(y|x)||q(y|x)) - D(p(y)||q(y)) $$\n\nNow, we evaluate each of the two remaining terms.\n\nFirst, let's compute $D(p(y|x)||q(y|x))$. The definition is:\n$$ D(p(y|x)||q(y|x)) = \\sum_{x \\in \\{0,1\\}} p(x) \\sum_{y \\in \\{0,1\\}} p(y|x) \\ln\\frac{p(y|x)}{q(y|x)} $$\nAccording to the problem statement, the conditional distributions are identical, i.e., $p(y|x) = f(y|x)$ and $q(y|x) = f(y|x)$. Therefore, the ratio inside the logarithm is $\\frac{p(y|x)}{q(y|x)} = 1$ for all $x, y$ (where the distributions are defined). The natural logarithm of 1 is 0.\nThus, every term in the summation is zero, which means:\n$$ D(p(y|x)||q(y|x)) = 0 $$\nThe expression for $\\Delta$ simplifies further to:\n$$ \\Delta = - D(p(y)||q(y)) $$\n\nNext, we need to compute $D(p(y)||q(y))$. To do this, we must first find the marginal distributions $p(y)$ and $q(y)$ using the law of total probability.\n\nThe marginal distribution $p(y)$ is:\n$$ p(y) = \\sum_{x \\in \\{0,1\\}} p(x) p(y|x) = \\sum_{x \\in \\{0,1\\}} p(x) f(y|x) $$\nWe are given $p(X=0) = 1/2$, so $p(X=1) = 1-1/2 = 1/2$. We are also given $f(Y=0|X=0) = 3/4$ and $f(Y=0|X=1) = 1/4$. This implies $f(Y=1|X=0) = 1-3/4 = 1/4$ and $f(Y=1|X=1) = 1-1/4 = 3/4$.\nFor $y=0$:\n$p(Y=0) = p(X=0)f(Y=0|X=0) + p(X=1)f(Y=0|X=1) = \\left(\\frac{1}{2}\\right)\\left(\\frac{3}{4}\\right) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{4}\\right) = \\frac{3}{8} + \\frac{1}{8} = \\frac{4}{8} = \\frac{1}{2}$.\nSince $Y$ is a binary variable, $p(Y=1) = 1 - p(Y=0) = 1 - 1/2 = 1/2$.\n\nThe marginal distribution $q(y)$ is:\n$$ q(y) = \\sum_{x \\in \\{0,1\\}} q(x) q(y|x) = \\sum_{x \\in \\{0,1\\}} q(x) f(y|x) $$\nWe are given $q(X=0) = 1/4$, so $q(X=1) = 1-1/4 = 3/4$.\nFor $y=0$:\n$q(Y=0) = q(X=0)f(Y=0|X=0) + q(X=1)f(Y=0|X=1) = \\left(\\frac{1}{4}\\right)\\left(\\frac{3}{4}\\right) + \\left(\\frac{3}{4}\\right)\\left(\\frac{1}{4}\\right) = \\frac{3}{16} + \\frac{3}{16} = \\frac{6}{16} = \\frac{3}{8}$.\nThis implies $q(Y=1) = 1 - q(Y=0) = 1 - 3/8 = 5/8$.\n\nNow we can compute $D(p(y)||q(y))$:\n$$ D(p(y)||q(y)) = \\sum_{y \\in \\{0,1\\}} p(y) \\ln\\frac{p(y)}{q(y)} $$\n$$ D(p(y)||q(y)) = p(Y=0) \\ln\\frac{p(Y=0)}{q(Y=0)} + p(Y=1) \\ln\\frac{p(Y=1)}{q(Y=1)} $$\n$$ D(p(y)||q(y)) = \\left(\\frac{1}{2}\\right) \\ln\\frac{1/2}{3/8} + \\left(\\frac{1}{2}\\right) \\ln\\frac{1/2}{5/8} $$\n$$ D(p(y)||q(y)) = \\frac{1}{2} \\ln\\left(\\frac{8}{6}\\right) + \\frac{1}{2} \\ln\\left(\\frac{8}{10}\\right) = \\frac{1}{2} \\ln\\left(\\frac{4}{3}\\right) + \\frac{1}{2} \\ln\\left(\\frac{4}{5}\\right) $$\nUsing the property $\\ln(a) + \\ln(b) = \\ln(ab)$:\n$$ D(p(y)||q(y)) = \\frac{1}{2} \\ln\\left(\\frac{4}{3} \\cdot \\frac{4}{5}\\right) = \\frac{1}{2} \\ln\\left(\\frac{16}{15}\\right) $$\n\nFinally, we find $\\Delta$:\n$$ \\Delta = - D(p(y)||q(y)) = - \\frac{1}{2} \\ln\\left(\\frac{16}{15}\\right) $$\nUsing the property $-\\ln(a) = \\ln(1/a)$:\n$$ \\Delta = \\frac{1}{2} \\ln\\left(\\frac{15}{16}\\right) $$\nThis negative result demonstrates that the quantity $D(p(x,y)||q(x,y))$ is not always greater than or equal to the sum of the KL divergences of its marginals, $D(p(x)||q(x)) + D(p(y)||q(y))$.",
            "answer": "$$\\boxed{\\frac{1}{2}\\ln\\left(\\frac{15}{16}\\right)}$$"
        },
        {
            "introduction": "Relative entropy is more than just a metric; it's a cornerstone of modern statistical modeling and optimization. This practice problem () showcases one of its most powerful applications: finding a new probability distribution that incorporates new information while staying as close as possible to a prior belief. This method, known as the principle of minimum discrimination information, is a fundamental technique for updating models in fields ranging from machine learning to statistical physics.",
            "id": "1655009",
            "problem": "A cloud computing platform allocates its resources among four distinct classes of computational tasks, labeled 1, 2, 3, and 4. A historical analysis has established a baseline resource allocation strategy represented by a prior probability distribution $Q = (q_1, q_2, q_3, q_4)$, where $q_i$ is the fraction of resources allocated to task class $i$. The prior distribution is given by $Q = (0.1, 0.2, 0.3, 0.4)$.\n\nEach task class has an associated average power consumption, given by a function $f(i)$. The values are $f(1) = 1.0$, $f(2) = 2.0$, $f(3) = 4.0$, and $f(4) = 5.0$, in normalized power units.\n\nTo meet new energy efficiency targets, the platform needs to adopt a new allocation strategy $P = (p_1, p_2, p_3, p_4)$ that adjusts the average power consumption to a new target value of $C=3.9$ power units. To ensure a smooth transition and minimize disruption, the new strategy $P$ must be as close as possible to the prior strategy $Q$. The \"closeness\" is measured by the Kullback-Leibler (KL) divergence, also known as relative entropy, defined as $D_{KL}(P||Q) = \\sum_{i=1}^4 p_i \\ln\\left(\\frac{p_i}{q_i}\\right)$.\n\nYour task is to find the new allocation strategy $P$ that minimizes $D_{KL}(P||Q)$ subject to the constraints that it is a valid probability distribution and that the expected power consumption is equal to the target value $C$.\n\nCalculate the probability $p_4$ for the fourth task class under this new optimal allocation strategy. Round your final answer to three significant figures.",
            "solution": "We minimize the relative entropy $D_{KL}(P||Q) = \\sum_{i=1}^{4} p_{i} \\ln\\left(\\frac{p_{i}}{q_{i}}\\right)$ subject to $\\sum_{i=1}^{4} p_{i} = 1$ and $\\sum_{i=1}^{4} p_{i} f(i) = C$ with $C=3.9$. Introduce Lagrange multipliers $\\alpha$ and $\\beta$ and form the Lagrangian\n$$\n\\mathcal{L} = \\sum_{i=1}^{4} p_{i} \\ln\\left(\\frac{p_{i}}{q_{i}}\\right) + \\alpha \\left(\\sum_{i=1}^{4} p_{i} - 1\\right) + \\beta \\left(\\sum_{i=1}^{4} p_{i} f(i) - C\\right).\n$$\nStationarity with respect to $p_{i}$ gives\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_{i}} = \\ln\\left(\\frac{p_{i}}{q_{i}}\\right) + 1 + \\alpha + \\beta f(i) = 0,\n$$\nso\n$$\np_{i} = q_{i} \\exp\\left(-1 - \\alpha - \\beta f(i)\\right).\n$$\nLet $\\eta = -\\beta$ and define the normalizer\n$$\nZ(\\eta) = \\sum_{j=1}^{4} q_{j} \\exp\\left(\\eta f(j)\\right).\n$$\nThen\n$$\np_{i} = \\frac{q_{i} \\exp\\left(\\eta f(i)\\right)}{Z(\\eta)}, \\quad Z(\\eta) = \\sum_{j=1}^{4} q_{j} \\exp\\left(\\eta f(j)\\right).\n$$\nThe expectation constraint becomes\n$$\n\\sum_{i=1}^{4} p_{i} f(i) = \\frac{1}{Z(\\eta)} \\sum_{i=1}^{4} q_{i} f(i) \\exp\\left(\\eta f(i)\\right) = C.\n$$\nWith $Q=(0.1,0.2,0.3,0.4)$ and $f(1)=1$, $f(2)=2$, $f(3)=4$, $f(4)=5$, we have\n$$\nZ(\\eta) = 0.1 \\exp(\\eta) + 0.2 \\exp(2\\eta) + 0.3 \\exp(4\\eta) + 0.4 \\exp(5\\eta),\n$$\n$$\n\\sum_{i=1}^{4} q_{i} f(i) \\exp\\left(\\eta f(i)\\right) = 0.1 \\cdot 1 \\cdot \\exp(\\eta) + 0.2 \\cdot 2 \\cdot \\exp(2\\eta) + 0.3 \\cdot 4 \\cdot \\exp(4\\eta) + 0.4 \\cdot 5 \\cdot \\exp(5\\eta).\n$$\nSet this ratio equal to $C=3.9$:\n$$\n\\frac{0.1 \\exp(\\eta) + 0.4 \\exp(2\\eta) + 1.2 \\exp(4\\eta) + 2.0 \\exp(5\\eta)}{0.1 \\exp(\\eta) + 0.2 \\exp(2\\eta) + 0.3 \\exp(4\\eta) + 0.4 \\exp(5\\eta)} = 3.9.\n$$\nLet $t = \\exp(\\eta) > 0$. Then the equation becomes\n$$\n\\frac{0.1 t + 0.4 t^{2} + 1.2 t^{4} + 2.0 t^{5}}{0.1 t + 0.2 t^{2} + 0.3 t^{4} + 0.4 t^{5}} = 3.9.\n$$\nCross-multiplying and simplifying yields\n$$\n0.44 t^5 + 0.03 t^4 - 0.38 t^2 - 0.29 t = 0,\n$$\nequivalently, for $t>0$,\n$$\n0.44\\, t^{4} + 0.03\\, t^{3} - 0.38\\, t - 0.29 = 0.\n$$\nSolving numerically for $t>1$ (since the target $C=3.9$ exceeds the prior mean $3.7$) gives $t \\approx 1.11132$.\n\nWith this $t$, compute the normalizer and $p_{4}$. First compute powers:\n$$\nt \\approx 1.11132,\\quad t^{2} \\approx 1.2350321424,\\quad t^{4} \\approx 1.5253043928,\\quad t^{5} \\approx 1.6951012778.\n$$\nThen\n$$\nZ = 0.1 t + 0.2 t^{2} + 0.3 t^{4} + 0.4 t^{5} \\approx 0.111132 + 0.24700642848 + 0.45759131784 + 0.67804051112 \\approx 1.49377025741.\n$$\nTherefore\n$$\np_{4} = \\frac{0.4 t^{5}}{Z} \\approx \\frac{0.67804051112}{1.49377025741} \\approx 0.453912,\n$$\nwhich, rounded to three significant figures, is $0.454$.",
            "answer": "$$\\boxed{0.454}$$"
        }
    ]
}