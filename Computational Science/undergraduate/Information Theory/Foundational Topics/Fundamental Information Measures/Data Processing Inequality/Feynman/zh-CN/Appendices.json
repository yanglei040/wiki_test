{
    "hands_on_practices": [
        {
            "introduction": "在理论学习之后，通过实践来巩固概念是至关重要的。这个练习提供了一个数据处理不等式的经典场景。我们假设一个信号连续通过两个独立的噪声信道，你的任务是确定原始信号与中间信号及最终信号之间互信息的关系。这个练习旨在验证一个核心直觉：对数据进行处理（即通过第二个信道）不会增加我们对原始数据源的了解。",
            "id": "1613405",
            "problem": "一个二元信号 $X$ 来自一个分布，其中 $P(X=0) = \\alpha$ 且 $P(X=1) = 1-\\alpha$，并且 $0 < \\alpha < 1$。该信号通过两个级联的独立噪声信道发送。第一个信道是二元对称信道 (Binary Symmetric Channel, BSC)，它以 $p_1$ 的交叉概率翻转传输的比特，产生一个中间信号 $Y$。这个信号 $Y$ 接着立即作为输入送入第二个独立的交叉概率为 $p_2$ 的 BSC，产生最终信号 $Z$。假设信道既不完美也不完全随机，因此它们的交叉概率在范围 $0 < p_1 < 1$ 和 $0 < p_2 < 1$ 内。\n\n令 $I(X; Y)$ 表示信源 $X$ 与中间信号 $Y$ 之间的互信息，令 $I(X; Z)$ 表示信源 $X$ 与最终信号 $Z$ 之间的互信息。所有互信息值都以比特为单位计算。在给定条件下，对于所描述的系统，以下哪种关系是普遍成立的？\n\nA. $I(X; Z) > I(X; Y)$\n\nB. $I(X; Z) = I(X; Y)$\n\nC. $I(X; Z) < I(X; Y)$\n\nD. $I(X; Z) \\le I(X; Y)$\n\nE. $I(X; Z) \\ge I(X; Y)$",
            "solution": "问题描述了一个系统，其中信号 $X$ 经过处理生成 $Y$，然后 $Y$ 再经过进一步处理生成 $Z$。关键的洞察在于识别三个随机变量 $X$、$Y$ 和 $Z$ 之间的关系。\n\n第一个信道的输出 $Y$ 仅依赖于其输入 $X$。第二个信道的输出 $Z$ 仅依赖于其输入 $Y$。这意味着如果我们知道了中间信号 $Y$，那么最终信号 $Z$ 就条件独立于原始信号 $X$。这种结构被称为马尔可夫链 (Markov chain)，表示为 $X \\to Y \\to Z$。这种链的联合概率分布可以分解为 $p(x, y, z) = p(x) p(y|x) p(z|y)$。\n\n我们可以使用互信息的链式法则来分析 $I(X;Y)$ 和 $I(X;Z)$ 之间的关系。$X$ 与 $(Y, Z)$ 对之间的互信息可以用两种方式展开：\n\n1.  $I(X; Y, Z) = I(X; Y) + I(X; Z|Y)$\n2.  $I(X; Y, Z) = I(X; Z) + I(X; Y|Z)$\n\n根据马尔可夫链 $X \\to Y \\to Z$ 的定义，我们知道在给定 $Y$ 的条件下，$X$ 和 $Z$ 是条件独立的。这个性质意味着条件互信息 $I(X; Z|Y)$ 为零。\n$$I(X; Z|Y) = 0$$\n\n将此结果代入第一个展开式得到：\n$$I(X; Y, Z) = I(X; Y) + 0 = I(X; Y)$$\n\n现在我们可以令 $I(X; Y, Z)$ 的两个表达式相等：\n$$I(X; Y) = I(X; Z) + I(X; Y|Z)$$\n\n互信息的一个基本性质是它总是非负的。这也适用于条件互信息。因此：\n$$I(X; Y|Z) \\ge 0$$\n\n这一项 $I(X; Y|Z)$ 表示即使在已经知道 $Z$ 的情况下，通过知道 $X$ 所能消除的关于 $Y$ 的剩余不确定性。直观上，这是在第二个信道中“丢失”的信息。\n\n将我们的方程与非负性结合起来，我们得到：\n$$I(X; Y) \\ge I(X; Z)$$\n或等价地，\n$$I(X; Z) \\le I(X; Y)$$\n\n这个结果是信息论中一个著名的定理，称为数据处理不等式 (Data Processing Inequality)。它指出，对数据进行后处理（在本例中，是将 $Y$ 通过第二个 BSC 得到 $Z$）不能增加与原始信源 $X$ 相关的互信息。\n\n让我们基于这个不等式分析给定的选项：\n- 选项 A ($I(X; Z) > I(X; Y)$) 和选项 E ($I(X; Z) \\ge I(X; Y)$) 是不正确的，因为它们与数据处理不等式相矛盾（除非等号成立）。\n- 选项 B ($I(X; Z) = I(X; Y)$) 并不总是成立。等号成立当且仅当 $I(X; Y|Z) = 0$。这意味着可以从 $Z$ 完美地确定 $Y$。对于一个交叉概率为 $p_2 \\in (0, 1)$ 的 BSC，从 $Y$ 到 $Z$ 的映射不是一一对应的，因此通常情况下无法从 $Z$ 中无不确定性地恢复 $Y$。例如，如果 $p_1=0.1, p_2=0.1, \\alpha=0.5$，直接计算会显示一个严格的不等式。\n- 选项 C ($I(X; Z) < I(X; Y)$) 也不是总是成立。虽然它在大多数非平凡情况下成立，但等号是有可能出现的。例如，如果第一个信道使得 $I(X; Y) = 0$（例如，如果 $p_1=0.5$ 且 $\\alpha=0.5$），那么由于 $Z$ 是 $Y$ 的进一步处理版本，我们也必定有 $I(X; Z) = 0$。在这种情况下，$I(X; Z) = I(X; Y) = 0$。\n- 选项 D ($I(X; Z) \\le I(X; Y)$) 是唯一一个在指定范围内对任意选择的 $\\alpha$、$p_1$ 和 $p_2$ 都普遍成立的关系，因为它正确地捕捉了数据处理不等式，包括在特定边界情况下等号成立的可能性。",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "从离散信道模型转向更贴近现实的连续信号系统，我们可以更深入地理解信息损失。这个练习将我们带入一个高斯噪声环境。通过计算信号在经过每个噪声阶段后的互信息，你将不仅仅是确认不等式的成立，而是能够量化信息是如何在处理过程中被削弱的。这项实践将抽象的“信息损失”概念转化为你可以直接计算和比较的具体数值。",
            "id": "1613384",
            "problem": "考虑一个通信系统中的简化信号级联模型。一个初始连续信号，由随机变量 $X$ 建模，是一个均值为零、方差为 $\\sigma_X^2$ 的高斯随机变量。该信号通过第一个噪声信道，该信道叠加了独立的、均值为零、方差为 $\\sigma_{N_1}^2$ 的高斯噪声 $N_1$。得到的信号是 $Y = X + N_1$。然后，该信号 $Y$ 被输入到第二个信道，该信道又叠加了独立的、均值为零、方差为 $\\sigma_{N_2}^2$ 的高斯噪声 $N_2$。最终的输出信号是 $Z = Y + N_2$。随机变量 $X$、$N_1$ 和 $N_2$ 都是相互独立的。假设所有方差 $\\sigma_X^2$、$\\sigma_{N_1}^2$ 和 $\\sigma_{N_2}^2$ 都严格为正。\n\n所有的信息论量都以奈特（nats）为单位进行度量，这意味着所有的对数都是自然对数（$\\ln$）。令 $I(X;Y)$ 为原始信号 $X$ 与第一级处理后的信号 $Y$ 之间的互信息。令 $I(X;Z)$ 为原始信号 $X$ 与最终信号 $Z$ 之间的互信息。\n\n以下哪个陈述正确描述了 $I(X;Y)$ 和 $I(X;Z)$ 之间的关系？\n\nA. $I(X;Y) < I(X;Z)$\n\nB. $I(X;Y) > I(X;Z)$\n\nC. $I(X;Y) = I(X;Z)$\n\nD. 在不知道方差的具体数值的情况下，无法确定它们之间的关系。\n\nE. $I(X;Y)$ 和 $I(X;Z)$ 均为无穷大。",
            "solution": "给定 $X \\sim \\mathcal{N}(0,\\sigma_{X}^{2})$，$N_{1} \\sim \\mathcal{N}(0,\\sigma_{N_{1}}^{2})$，$N_{2} \\sim \\mathcal{N}(0,\\sigma_{N_{2}}^{2})$，它们都相互独立且方差严格为正。信道模型为 $Y = X + N_{1}$ 和 $Z = Y + N_{2} = X + (N_{1}+N_{2})$。因为 $N_{1}$ 和 $N_{2}$ 是独立的高斯变量，所以 $N_{1}+N_{2} \\sim \\mathcal{N}(0,\\sigma_{N_{1}}^{2}+\\sigma_{N_{2}}^{2})$ 并且与 $X$ 独立。\n\n我们使用公式 $I(U;V) = h(V) - h(V|U)$ 以及均值为零的高斯变量 $W \\sim \\mathcal{N}(0,\\sigma^{2})$ 的微分熵公式 $h(W) = \\frac{1}{2} \\ln\\!\\big(2\\pi e\\,\\sigma^{2}\\big)$ 来计算互信息。\n\n第一级：\n- $Y = X + N_{1}$ 是高斯变量，其方差为 $\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2}$，因此\n$$\nh(Y) = \\frac{1}{2} \\ln\\!\\big(2\\pi e\\,(\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2})\\big).\n$$\n- 给定 $X$ 时，$Y|X = X + N_{1}$ 的分布与 $N_{1}$ 相同，因此\n$$\nh(Y|X) = h(N_{1}) = \\frac{1}{2} \\ln\\!\\big(2\\pi e\\,\\sigma_{N_{1}}^{2}\\big).\n$$\n因此\n$$\nI(X;Y) = h(Y) - h(Y|X) = \\frac{1}{2} \\ln\\!\\left(\\frac{\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2}}{\\sigma_{N_{1}}^{2}}\\right) = \\frac{1}{2} \\ln\\!\\left(1 + \\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2}}\\right).\n$$\n\n第二级：\n- $Z = X + (N_{1}+N_{2})$ 是高斯变量，其方差为 $\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}$，因此\n$$\nh(Z) = \\frac{1}{2} \\ln\\!\\big(2\\pi e\\,(\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2})\\big).\n$$\n- 给定 $X$ 时，$Z|X$ 的分布与 $N_{1}+N_{2}$ 相同，因此\n$$\nh(Z|X) = h(N_{1}+N_{2}) = \\frac{1}{2} \\ln\\!\\big(2\\pi e\\,(\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2})\\big).\n$$\n因此\n$$\nI(X;Z) = h(Z) - h(Z|X) = \\frac{1}{2} \\ln\\!\\left(\\frac{\\sigma_{X}^{2} + \\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}}{\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}}\\right) = \\frac{1}{2} \\ln\\!\\left(1 + \\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}}\\right).\n$$\n\n比较：\n由于 $\\sigma_{N_{2}}^{2} > 0$，我们有 $\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2} > \\sigma_{N_{1}}^{2}$，这意味着\n$$\n\\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}} < \\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2}}.\n$$\n因为 $\\ln$ 函数是严格单调递增的，\n$$\n\\frac{1}{2} \\ln\\!\\left(1 + \\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2} + \\sigma_{N_{2}}^{2}}\\right) < \\frac{1}{2} \\ln\\!\\left(1 + \\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2}}\\right),\n$$\n所以 $I(X;Z) < I(X;Y)$。\n\n这也与马尔可夫链 $X \\to Y \\to Z$ 的数据处理不等式相符，该不等式保证了 $I(X;Z) \\leq I(X;Y)$；此处严格不等式成立，因为在第二级中加入了额外的独立高斯噪声，并且所有方差都严格为正。\n\n因此，正确选项是 B。",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "数据处理不等式保证了信息不会增加，但信息会损失多少呢？这个练习提出了一个更具挑战性的问题：我们能否设计一个处理步骤，使其完全抹除前一阶段获得的所有信息？你将构建一个系统，其中第一阶段成功传递了信息（$I(X;Y) > 0$），但第二阶段的处理却导致最终信息完全丢失（$I(X;Z) = 0$）。这个有趣的例子突显了数据处理的潜在破坏力，并加深了对信息得以保持或丢失条件的理解。",
            "id": "1613411",
            "problem": "考虑一个简化的两级数字通信系统模型。一个信源产生二进制信号 $X$，其取值可为 0 或 1。该信源的先验概率分布是对称的，即 $P(X=0) = P(X=1) = 1/2$。\n\n信号 $X$ 首先通过一个噪声信道传输到一个中间节点，产生信号 $Y \\in \\{0, 1\\}$。该信道是一个交叉概率为 $\\epsilon = 1/3$ 的二进制对称信道 (Binary Symmetric Channel)。这意味着比特翻转的概率为 $P(Y \\neq x | X = x) = 1/3$。\n\n信号 $Y$ 随后被中间节点处理并重传到最终目的地，产生信号 $Z \\in \\{+, -\\}$。这第二级也是一个噪声信道，由以下条件概率决定：\n- $P(Z = + | Y = 0) = 1/4$\n- $P(Z = + | Y = 1) = 1/4$\n\n整个过程构成一个马尔可夫链 $X \\rightarrow Y \\rightarrow Z$。\n\n计算原始信号与中间信号之间的互信息 $I(X;Y)$，以及原始信号与最终信号之间的互信息 $I(X;Z)$。将互信息的答案以比特为单位表示。您的最终答案应该是一个行矩阵，按顺序包含 $I(X;Y)$ 和 $I(X;Z)$ 的值。",
            "solution": "给定一个马尔可夫链 $X \\rightarrow Y \\rightarrow Z$，其中 $X \\in \\{0,1\\}$，$P(X=0)=P(X=1)=\\frac{1}{2}$，第一级是一个交叉概率为 $\\epsilon=\\frac{1}{3}$ 的二进制对称信道(BSC)，第二级满足 $P(Z=+ \\mid Y=0)=P(Z=+ \\mid Y=1)=\\frac{1}{4}$。\n\n首先，计算 $I(X;Y)$。根据定义，$I(X;Y)=H(Y)-H(Y \\mid X)$。\n\n对于交叉概率为 $\\epsilon=\\frac{1}{3}$ 且输入对称的BSC，给定 $X=x$ 时 $Y$ 的条件分布为 $P(Y=x \\mid X=x)=\\frac{2}{3}$ 和 $P(Y \\neq x \\mid X=x)=\\frac{1}{3}$。$Y$ 的边缘分布为\n$$\nP(Y=0)=P(Y=0 \\mid X=0)P(X=0)+P(Y=0 \\mid X=1)P(X=1)=\\frac{2}{3}\\cdot \\frac{1}{2}+\\frac{1}{3}\\cdot \\frac{1}{2}=\\frac{1}{2},\n$$\n同理可得 $P(Y=1)=\\frac{1}{2}$。因此\n$$\nH(Y)=-\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)-\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)=1.\n$$\n条件熵为\n$$\nH(Y \\mid X)=\\sum_{x \\in \\{0,1\\}}P(X=x)\\,H(Y \\mid X=x)=H\\!\\left(\\tfrac{2}{3},\\tfrac{1}{3}\\right)\n= -\\frac{2}{3}\\log_{2}\\!\\left(\\frac{2}{3}\\right)-\\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right).\n$$\n因此\n$$\nI(X;Y)=H(Y)-H(Y \\mid X)=1+\\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)+\\frac{2}{3}\\log_{2}\\!\\left(\\frac{2}{3}\\right).\n$$\n\n接下来，计算 $I(X;Z)$。给定 $P(Z=+ \\mid Y=0)=P(Z=+ \\mid Y=1)=\\frac{1}{4}$，所以 $P(Z=- \\mid Y=0)=P(Z=- \\mid Y=1)=\\frac{3}{4}$。因此，$Z$ 的分布不依赖于 $Y$，这意味着 $Z$ 与 $Y$ 相互独立。因为 $X \\rightarrow Y \\rightarrow Z$ 是一个马尔可夫链，所以 $Z$ 也与 $X$ 相互独立，因此\n$$\nI(X;Z)=0.\n$$\n等价地，可以验证 $P(Z=+)=\\sum_{y}P(Z=+ \\mid Y=y)P(Y=y)=\\frac{1}{4}$ 并且 $H(Z \\mid X)=H(Z)$，这也得出 $I(X;Z)=H(Z)-H(Z \\mid X)=0$。\n\n因此，以比特为单位的互信息为 $I(X;Y)=1+\\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)+\\frac{2}{3}\\log_{2}\\!\\left(\\frac{2}{3}\\right)$ 和 $I(X;Z)=0$。",
            "answer": "$$\\boxed{\\begin{pmatrix}1+\\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)+\\frac{2}{3}\\log_{2}\\!\\left(\\frac{2}{3}\\right) & 0\\end{pmatrix}}$$"
        }
    ]
}