{
    "hands_on_practices": [
        {
            "introduction": "Let's begin with a foundational exercise that connects directly to the definition of mutual information. In this scenario, we'll analyze a simple signal processing step where a raw signal $X$ is transformed into a new signal $Y$ via the function $Y=X^2$. By calculating the mutual information $I(X;Y)$, you'll gain hands-on experience with the core formulas and develop intuition for how information is quantified and how it can be lost through processing .",
            "id": "1649998",
            "problem": "A simple digital sensor produces a raw output signal, represented by the discrete random variable $X$. The signal $X$ can take one of three values, $\\{-1, 0, 1\\}$, with equal probability. This signal is then fed into a signal processing unit that computes a new variable $Y$ according to the function $Y = X^2$. This transformation effectively makes the system insensitive to the sign of the raw signal.\n\nYour task is to calculate the mutual information $I(X;Y)$ between the raw signal $X$ and the processed signal $Y$. This quantity measures the information (in an information-theoretic sense) that the processed signal $Y$ provides about the original signal $X$.\n\nExpress your final answer in bits as a closed-form analytic expression. Your calculations and final answer should use base 2 logarithms (e.g., $\\log_{2}(...)$).",
            "solution": "The mutual information $I(X;Y)$ can be calculated using the formula $I(X;Y) = H(X) - H(X|Y)$, where $H(X)$ is the entropy of the random variable $X$ and $H(X|Y)$ is the conditional entropy of $X$ given $Y$. We will calculate each term separately. All logarithms are base 2, as required for an answer in bits.\n\nFirst, let's calculate the entropy of the source signal $X$, denoted by $H(X)$. The random variable $X$ is uniformly distributed over the set $\\{-1, 0, 1\\}$. Therefore, the probabilities of its outcomes are:\n$P(X=-1) = \\frac{1}{3}$\n$P(X=0) = \\frac{1}{3}$\n$P(X=1) = \\frac{1}{3}$\n\nThe entropy $H(X)$ is given by the formula $H(X) = -\\sum_{x} P(x) \\log_{2}(P(x))$.\n$$H(X) = - \\left( P(X=-1)\\log_{2}(P(X=-1)) + P(X=0)\\log_{2}(P(X=0)) + P(X=1)\\log_{2}(P(X=1)) \\right)$$\n$$H(X) = - \\left( \\frac{1}{3}\\log_{2}\\left(\\frac{1}{3}\\right) + \\frac{1}{3}\\log_{2}\\left(\\frac{1}{3}\\right) + \\frac{1}{3}\\log_{2}\\left(\\frac{1}{3}\\right) \\right)$$\n$$H(X) = -3 \\times \\frac{1}{3} \\log_{2}\\left(\\frac{1}{3}\\right) = -\\log_{2}\\left(\\frac{1}{3}\\right) = \\log_{2}(3)$$\n\nNext, we calculate the conditional entropy $H(X|Y)$. This is defined as $H(X|Y) = \\sum_{y} P(y)H(X|Y=y)$. To do this, we first need to find the probability distribution of $Y$. The possible values for $Y=X^2$ are:\nIf $X=-1$, then $Y = (-1)^2 = 1$.\nIf $X=0$, then $Y = (0)^2 = 0$.\nIf $X=1$, then $Y = (1)^2 = 1$.\n\nSo, the set of possible values for $Y$ is $\\{0, 1\\}$. Now we find their probabilities:\nThe event $Y=0$ occurs only if $X=0$. So, $P(Y=0) = P(X=0) = \\frac{1}{3}$.\nThe event $Y=1$ occurs if $X=-1$ or $X=1$. So, $P(Y=1) = P(X=-1) + P(X=1) = \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3}$.\n\nNow we can calculate the conditional entropies for each value of $y$:\nCase 1: $Y=0$.\nGiven $Y=0$, we know for certain that $X$ must be 0. There is no uncertainty left. The conditional probability distribution $P(X|Y=0)$ is $P(X=0|Y=0)=1$ and $P(X=x|Y=0)=0$ for $x \\neq 0$.\nThe conditional entropy is $H(X|Y=0) = - \\sum_x P(x|Y=0) \\log_{2}(P(x|Y=0)) = - (1 \\cdot \\log_{2}(1)) = 0$.\n\nCase 2: $Y=1$.\nGiven $Y=1$, we know that $X$ could be either $-1$ or $1$. We need the conditional probabilities:\n$P(X=-1|Y=1) = \\frac{P(X=-1, Y=1)}{P(Y=1)} = \\frac{P(X=-1)}{P(Y=1)} = \\frac{1/3}{2/3} = \\frac{1}{2}$.\n$P(X=1|Y=1) = \\frac{P(X=1, Y=1)}{P(Y=1)} = \\frac{P(X=1)}{P(Y=1)} = \\frac{1/3}{2/3} = \\frac{1}{2}$.\nThe conditional entropy is:\n$$H(X|Y=1) = - \\left( P(X=-1|Y=1)\\log_{2}(P(X=-1|Y=1)) + P(X=1|Y=1)\\log_{2}(P(X=1|Y=1)) \\right)$$\n$$H(X|Y=1) = - \\left( \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right) + \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right) \\right) = - \\log_{2}\\left(\\frac{1}{2}\\right) = \\log_{2}(2) = 1$$\n\nNow we can compute the total conditional entropy $H(X|Y)$:\n$$H(X|Y) = P(Y=0)H(X|Y=0) + P(Y=1)H(X|Y=1)$$\n$$H(X|Y) = \\left(\\frac{1}{3}\\right) \\cdot (0) + \\left(\\frac{2}{3}\\right) \\cdot (1) = \\frac{2}{3}$$\n\nFinally, we calculate the mutual information $I(X;Y)$:\n$$I(X;Y) = H(X) - H(X|Y) = \\log_{2}(3) - \\frac{2}{3}$$\nThis expression represents the amount of information, in bits, that observing $Y$ provides about $X$.",
            "answer": "$$\\boxed{\\log_{2}(3) - \\frac{2}{3}}$$"
        },
        {
            "introduction": "Building on our first practice, this problem explores a more intricate dependency by examining two characteristics—primality and parity—derived from a single die roll. Calculating the mutual information $I(Y;Z)$ requires you to first determine the joint probability distribution of these two variables, which are linked only by their common origin. This exercise demonstrates how to analyze the statistical relationship between variables that are not directly linked but share a common cause, a frequent scenario in data analysis .",
            "id": "1650034",
            "problem": "Consider a random experiment involving a single roll of a fair six-sided die. Let the outcome of the roll be the random variable $X$, whose sample space is $\\Omega = \\{1, 2, 3, 4, 5, 6\\}$, with each outcome having a probability of $1/6$.\n\nTwo binary random variables, $Y$ and $Z$, are derived from the outcome $X$.\nThe variable $Y$ is defined based on whether $X$ is a prime number. Specifically, $Y=0$ if $X$ is prime (i.e., $X \\in \\{2, 3, 5\\}$), and $Y=1$ otherwise (i.e., $X \\in \\{1, 4, 6\\}$).\nThe variable $Z$ is defined based on the parity of $X$. Specifically, $Z=0$ if $X$ is an even number (i.e., $X \\in \\{2, 4, 6\\}$), and $Z=1$ if $X$ is an odd number (i.e., $X \\in \\{1, 3, 5\\}$).\n\nCalculate the mutual information $I(Y;Z)$ between the random variables $Y$ and $Z$. Express your answer as a closed-form analytic expression using base-2 logarithms (e.g., $\\log_{2}(3)$).",
            "solution": "The sample space is $\\Omega=\\{1,2,3,4,5,6\\}$ with $P(X=x)=\\frac{1}{6}$ for each $x\\in\\Omega$. By definition, $Y=0$ if $X\\in\\{2,3,5\\}$ and $Y=1$ if $X\\in\\{1,4,6\\}$; $Z=0$ if $X\\in\\{2,4,6\\}$ and $Z=1$ if $X\\in\\{1,3,5\\}$. Thus the joint values $(Y,Z)$ for each $X$ are:\n$X=1\\Rightarrow(Y,Z)=(1,1)$, $X=2\\Rightarrow(0,0)$, $X=3\\Rightarrow(0,1)$, $X=4\\Rightarrow(1,0)$, $X=5\\Rightarrow(0,1)$, $X=6\\Rightarrow(1,0)$.\nTherefore,\n$$\nP(Y=0,Z=0)=\\frac{1}{6},\\quad P(Y=0,Z=1)=\\frac{1}{3},\\quad P(Y=1,Z=0)=\\frac{1}{3},\\quad P(Y=1,Z=1)=\\frac{1}{6}.\n$$\nThe marginals are $P(Y=0)=\\frac{1}{2}$, $P(Y=1)=\\frac{1}{2}$ and $P(Z=0)=\\frac{1}{2}$, $P(Z=1)=\\frac{1}{2}$. The mutual information is\n$$\nI(Y;Z)=\\sum_{y\\in\\{0,1\\}}\\sum_{z\\in\\{0,1\\}} P(y,z)\\,\\log_{2}\\!\\left(\\frac{P(y,z)}{P(y)P(z)}\\right).\n$$\nSince $P(y)P(z)=\\frac{1}{4}$ for all $(y,z)$, we evaluate termwise:\n$$\nI(Y;Z)=2\\cdot\\frac{1}{6}\\log_{2}\\!\\left(\\frac{\\frac{1}{6}}{\\frac{1}{4}}\\right)+2\\cdot\\frac{1}{3}\\log_{2}\\!\\left(\\frac{\\frac{1}{3}}{\\frac{1}{4}}\\right)\n=\\frac{1}{3}\\log_{2}\\!\\left(\\frac{2}{3}\\right)+\\frac{2}{3}\\log_{2}\\!\\left(\\frac{4}{3}\\right).\n$$\nUsing $\\log_{2}\\!\\left(\\frac{2}{3}\\right)=1-\\log_{2}(3)$ and $\\log_{2}\\!\\left(\\frac{4}{3}\\right)=2-\\log_{2}(3)$, we obtain\n$$\nI(Y;Z)=\\frac{1}{3}\\bigl(1-\\log_{2}(3)\\bigr)+\\frac{2}{3}\\bigl(2-\\log_{2}(3)\\bigr)\n=\\frac{5}{3}-\\log_{2}(3).\n$$\nThis is the closed-form expression in bits.",
            "answer": "$$\\boxed{\\frac{5}{3}-\\log_{2}(3)}$$"
        },
        {
            "introduction": "This final practice advances our study to the realm of continuous variables, a scenario common in engineering and physics, by modeling a signal passing through a noisy two-stage pipeline. We will investigate a key principle, the Data Processing Inequality, in a practical context. Your task is to apply the formula for mutual information in Gaussian channels to determine the precise conditions under which a specific amount of information is lost, linking abstract theory to concrete system performance metrics like noise variance .",
            "id": "1650010",
            "problem": "Consider a simplified model for a two-stage signal processing pipeline. An input signal, represented by a random variable $X$, is transmitted through the first stage. During this process, it is corrupted by additive noise, modeled by a random variable $N_1$. The output of the first stage is $Y = X + N_1$. This intermediate signal $Y$ is then fed into a second stage, where it is further corrupted by another independent additive noise source, $N_2$, resulting in the final output signal $Z = Y + N_2$.\n\nThe random variables $X$, $N_1$, and $N_2$ are all mutually independent, zero-mean Gaussian random variables with variances $\\sigma_X^2$, $\\sigma_{N_1}^2$, and $\\sigma_{N_2}^2$, respectively. To ensure a meaningful analysis, assume the input signal is not deterministic ($\\sigma_X^2 > 0$) and the first stage is noisy ($\\sigma_{N_1}^2 > 0$).\n\nThe quality of the signal at the end of the first stage is characterized by the Signal-to-Noise Ratio (SNR), defined as $S_{in} = \\sigma_X^2 / \\sigma_{N_1}^2$. For this particular system, it is measured that $S_{in} = 15$.\n\nAccording to the Data Processing Inequality, information about the original signal $X$ can only be lost as the signal propagates through the cascade, meaning the mutual information $I(X;Z)$ is less than or equal to $I(X;Y)$. Your task is to find the specific condition under which the information retained at the final output is exactly 75% of the information available at the intermediate stage.\n\nCalculate the required ratio of the noise variances, $R = \\sigma_{N_2}^2 / \\sigma_{N_1}^2$, for which the relation $I(X; Z) = 0.75 \\cdot I(X; Y)$ holds.",
            "solution": "Because $X$, $N_{1}$, and $N_{2}$ are mutually independent zero-mean Gaussians, $Y=X+N_{1}$ and $Z=X+N_{1}+N_{2}$ are Gaussian. The mutual information for an additive Gaussian channel with Gaussian input is obtained from differential entropies:\n$$\nI(X;Y)=h(Y)-h(Y|X)=h(Y)-h(N_{1}).\n$$\nSince $Y\\sim\\mathcal{N}\\!\\left(0,\\sigma_{X}^{2}+\\sigma_{N_{1}}^{2}\\right)$ and $N_{1}\\sim\\mathcal{N}\\!\\left(0,\\sigma_{N_{1}}^{2}\\right)$, using $h(\\mathcal{N}(0,\\sigma^{2}))=\\tfrac{1}{2}\\ln\\!\\left(2\\pi e\\,\\sigma^{2}\\right)$ gives\n$$\nI(X;Y)=\\frac{1}{2}\\ln\\!\\left(\\frac{\\sigma_{X}^{2}+\\sigma_{N_{1}}^{2}}{\\sigma_{N_{1}}^{2}}\\right)=\\frac{1}{2}\\ln\\!\\left(1+\\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2}}\\right).\n$$\nDefine $S_{in}=\\sigma_{X}^{2}/\\sigma_{N_{1}}^{2}$. Then\n$$\nI(X;Y)=\\frac{1}{2}\\ln(1+S_{in}).\n$$\n\nFor the cascade output $Z=X+N_{1}+N_{2}$, the effective noise is $N_{1}+N_{2}$ with variance $\\sigma_{N_{1}}^{2}+\\sigma_{N_{2}}^{2}$, independent of $X$. Thus\n$$\nI(X;Z)=\\frac{1}{2}\\ln\\!\\left(1+\\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2}+\\sigma_{N_{2}}^{2}}\\right).\n$$\nIntroduce $R=\\sigma_{N_{2}}^{2}/\\sigma_{N_{1}}^{2}$ so that $\\sigma_{N_{1}}^{2}+\\sigma_{N_{2}}^{2}=\\sigma_{N_{1}}^{2}(1+R)$ and\n$$\nI(X;Z)=\\frac{1}{2}\\ln\\!\\left(1+\\frac{S_{in}}{1+R}\\right).\n$$\n\nImpose the condition $I(X;Z)=0.75\\,I(X;Y)$:\n$$\n\\frac{1}{2}\\ln\\!\\left(1+\\frac{S_{in}}{1+R}\\right)=0.75\\cdot\\frac{1}{2}\\ln(1+S_{in}).\n$$\nMultiply by $2$ and exponentiate:\n$$\n\\ln\\!\\left(1+\\frac{S_{in}}{1+R}\\right)=0.75\\,\\ln(1+S_{in})\n\\quad\\Longrightarrow\\quad\n1+\\frac{S_{in}}{1+R}=(1+S_{in})^{0.75}.\n$$\nSolve for $R$:\n$$\n\\frac{S_{in}}{1+R}=(1+S_{in})^{0.75}-1\n\\;\\Longrightarrow\\;\n1+R=\\frac{S_{in}}{(1+S_{in})^{0.75}-1}\n\\;\\Longrightarrow\\;\nR=\\frac{S_{in}}{(1+S_{in})^{0.75}-1}-1.\n$$\n\nWith the given $S_{in}=15$, compute\n$$\nR=\\frac{15}{16^{0.75}-1}-1.\n$$\nNote that $16^{0.75}=16^{3/4}=(2^{4})^{3/4}=2^{3}=8$, so\n$$\nR=\\frac{15}{8-1}-1=\\frac{15}{7}-1=\\frac{8}{7}.\n$$",
            "answer": "$$\\boxed{\\frac{8}{7}}$$"
        }
    ]
}