{
    "hands_on_practices": [
        {
            "introduction": "理解互信息的最佳方式之一就是亲手计算它。这个问题提供了一个简单而具体的场景：一个信号经过一个平方函数处理。你的任务是精确地量化处理后保留了多少关于原始信号的信息。这个练习将巩固你对熵、条件熵和互信息等基本定义的理解，并让你直观地感受信息是如何在处理过程中发生变化的 。",
            "id": "1649998",
            "problem": "一个简单的数字传感器产生一个原始输出信号，由离散随机变量 $X$ 表示。信号 $X$ 可以等概率地取三个值之一，即 $\\{-1, 0, 1\\}$。该信号随后被输入一个信号处理单元，该单元根据函数 $Y = X^2$ 计算出一个新变量 $Y$。这种变换实际上使系统对原始信号的符号不敏感。\n\n你的任务是计算原始信号 $X$ 和处理后信号 $Y$ 之间的互信息 $I(X;Y)$。这个量衡量了处理后的信号 $Y$ 所提供的关于原始信号 $X$ 的信息（在信息论意义上）。\n\n将你的最终答案以比特为单位，表示为一个封闭形式的解析表达式。你的计算和最终答案应使用以2为底的对数（例如，$\\log_{2}(...)$）。",
            "solution": "互信息 $I(X;Y)$ 可以使用公式 $I(X;Y) = H(X) - H(X|Y)$ 计算，其中 $H(X)$ 是随机变量 $X$ 的熵，$H(X|Y)$ 是在给定 $Y$ 的情况下 $X$ 的条件熵。我们将分别计算每一项。所有对数都以2为底，这是以比特为单位作答的要求。\n\n首先，我们计算源信号 $X$ 的熵，记为 $H(X)$。随机变量 $X$ 在集合 $\\{-1, 0, 1\\}$ 上均匀分布。因此，其结果的概率为：\n$P(X=-1) = \\frac{1}{3}$\n$P(X=0) = \\frac{1}{3}$\n$P(X=1) = \\frac{1}{3}$\n\n熵 $H(X)$ 由公式 $H(X) = -\\sum_{x} P(x) \\log_{2}(P(x))$ 给出。\n$$H(X) = - \\left( P(X=-1)\\log_{2}(P(X=-1)) + P(X=0)\\log_{2}(P(X=0)) + P(X=1)\\log_{2}(P(X=1)) \\right)$$\n$$H(X) = - \\left( \\frac{1}{3}\\log_{2}\\left(\\frac{1}{3}\\right) + \\frac{1}{3}\\log_{2}\\left(\\frac{1}{3}\\right) + \\frac{1}{3}\\log_{2}\\left(\\frac{1}{3}\\right) \\right)$$\n$$H(X) = -3 \\times \\frac{1}{3} \\log_{2}\\left(\\frac{1}{3}\\right) = -\\log_{2}\\left(\\frac{1}{3}\\right) = \\log_{2}(3)$$\n\n接下来，我们计算条件熵 $H(X|Y)$。其定义为 $H(X|Y) = \\sum_{y} P(y)H(X|Y=y)$。为此，我们首先需要找到 $Y$ 的概率分布。$Y=X^2$ 的可能值为：\n如果 $X=-1$，则 $Y = (-1)^2 = 1$。\n如果 $X=0$，则 $Y = (0)^2 = 0$。\n如果 $X=1$，则 $Y = (1)^2 = 1$。\n\n所以，$Y$ 的可能值集合是 $\\{0, 1\\}$。现在我们来求它们的概率：\n事件 $Y=0$ 仅在 $X=0$ 时发生。所以，$P(Y=0) = P(X=0) = \\frac{1}{3}$。\n事件 $Y=1$ 在 $X=-1$ 或 $X=1$ 时发生。所以，$P(Y=1) = P(X=-1) + P(X=1) = \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3}$。\n\n现在我们可以计算对于每个 $y$ 值的条件熵：\n情况1：$Y=0$。\n给定 $Y=0$，我们可以确定 $X$ 必须是 0。没有任何不确定性。条件概率分布 $P(X|Y=0)$ 为 $P(X=0|Y=0)=1$，且对于 $x \\neq 0$ 有 $P(X=x|Y=0)=0$。\n条件熵为 $H(X|Y=0) = - \\sum_x P(x|Y=0) \\log_{2}(P(x|Y=0)) = - (1 \\cdot \\log_{2}(1)) = 0$。\n\n情况2：$Y=1$。\n给定 $Y=1$，我们知道 $X$ 可能是 $-1$ 或 $1$。我们需要条件概率：\n$P(X=-1|Y=1) = \\frac{P(X=-1, Y=1)}{P(Y=1)} = \\frac{P(X=-1)}{P(Y=1)} = \\frac{1/3}{2/3} = \\frac{1}{2}$。\n$P(X=1|Y=1) = \\frac{P(X=1, Y=1)}{P(Y=1)} = \\frac{P(X=1)}{P(Y=1)} = \\frac{1/3}{2/3} = \\frac{1}{2}$。\n条件熵为：\n$$H(X|Y=1) = - \\left( P(X=-1|Y=1)\\log_{2}(P(X=-1|Y=1)) + P(X=1|Y=1)\\log_{2}(P(X=1|Y=1)) \\right)$$\n$$H(X|Y=1) = - \\left( \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right) + \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right) \\right) = - \\log_{2}\\left(\\frac{1}{2}\\right) = \\log_{2}(2) = 1$$\n\n现在我们可以计算总条件熵 $H(X|Y)$：\n$$H(X|Y) = P(Y=0)H(X|Y=0) + P(Y=1)H(X|Y=1)$$\n$$H(X|Y) = \\left(\\frac{1}{3}\\right) \\cdot (0) + \\left(\\frac{2}{3}\\right) \\cdot (1) = \\frac{2}{3}$$\n\n最后，我们计算互信息 $I(X;Y)$：\n$$I(X;Y) = H(X) - H(X|Y) = \\log_{2}(3) - \\frac{2}{3}$$\n这个表达式代表了观测 $Y$ 所提供的关于 $X$ 的信息量（以比特为单位）。",
            "answer": "$$\\boxed{\\log_{2}(3) - \\frac{2}{3}}$$"
        },
        {
            "introduction": "在掌握了直接计算之后，我们可以探索不同信息度量之间存在的深刻联系。这个问题展示了熵的链式法则是如何作为一条基本定律，将系统中多个变量的不确定性联系起来的。通过运用这一定律，我们可以从已知的信息量推导出未知的量，这体现了信息论中一种更高效、更具洞察力的问题解决方法 。",
            "id": "1650000",
            "problem": "一位信息理论家正在分析一个简化的天气预报模型。该模型涉及三个离散随机变量：\n- $Z$：表示大气压力类别，可以是“高”或“低”。\n- $Y$：表示温度类别，可以是“暖”或“冷”。\n- $X$：表示模型对第二天天气的预测，可以是“晴”或“雨”。\n\n这位理论家使用香农熵来量化这些变量之间的关系，香农熵衡量随机变量的平均不确定性。所有熵值均以比特（bit）为单位计算，这对应于其定义中使用了以2为底的对数。\n\n在分析了一个大型数据集后，这位理论家确定了以下条件熵：\n1. 仅给定压力 $Z$ 时，预测 $X$ 的不确定性为 $H(X|Z) = 0.875$ 比特。\n2. 仅给定压力 $Z$ 时，温度 $Y$ 的不确定性为 $H(Y|Z) = 0.941$ 比特。\n3. 同时给定预测 $X$ 和压力 $Z$ 时，温度 $Y$ 的不确定性为 $H(Y|X,Z) = 0.726$ 比特。\n\n使用这些信息，计算同时给定温度 $Y$ 和压力 $Z$ 时，预测 $X$ 的不确定性，记为 $H(X|Y,Z)$。请用比特作答，并四舍五入到三位有效数字。",
            "solution": "我们使用熵的条件链式法则。对于任意随机变量 $X$、$Y$ 和 $Z$，链式法则给出\n$$\nH(X,Y|Z)=H(X|Z)+H(Y|X,Z)=H(Y|Z)+H(X|Y,Z).\n$$\n令两种展开式相等，并求解 $H(X|Y,Z)$，我们得到\n$$\nH(X|Y,Z)=H(X|Z)+H(Y|X,Z)-H(Y|Z).\n$$\n代入给定值，\n$$\nH(X|Y,Z)=0.875+0.726-0.941=0.660 \\text{ bits}.\n$$\n四舍五入到三位有效数字，结果是 $0.660$ 比特。",
            "answer": "$$\\boxed{0.660}$$"
        },
        {
            "introduction": "现在，让我们将所学知识应用到一个更接近实际且涉及连续信号的信号处理场景中。这个问题通过一个高斯噪声信道模型来探讨“数据处理不等式”这一重要原理，它揭示了信息在处理过程中只会丢失而不会增加 。通过处理在通信理论中至关重要的高斯变量，你将确定在何种条件下会发生特定数量的信息损失，从而将抽象的理论与具体的工程设计问题联系起来。",
            "id": "1650010",
            "problem": "考虑一个简化的两级信号处理流水线模型。一个输入信号，由随机变量 $X$ 表示，通过第一级传输。在此过程中，它受到加性噪声的干扰，该噪声由随机变量 $N_1$ 建模。第一级的输出是 $Y = X + N_1$。这个中间信号 $Y$ 接着被送入第二级，在第二级中它被另一个独立的加性噪声源 $N_2$ 进一步干扰，得到最终的输出信号 $Z = Y + N_2$。\n\n随机变量 $X$、$N_1$ 和 $N_2$ 都是相互独立的、零均值的高斯随机变量，其方差分别为 $\\sigma_X^2$、$\\sigma_{N_1}^2$ 和 $\\sigma_{N_2}^2$。为确保分析有意义，假设输入信号不是确定性的 ($\\sigma_X^2 > 0$)，并且第一级是有噪声的 ($\\sigma_{N_1}^2 > 0$)。\n\n第一级末端信号的质量由信噪比（SNR）来表征，定义为 $S_{in} = \\sigma_X^2 / \\sigma_{N_1}^2$。对于这个特定系统，测得 $S_{in} = 15$。\n\n根据数据处理不等式，关于原始信号 $X$ 的信息在信号通过级联系统传播时只会丢失，这意味着互信息 $I(X;Z)$ 小于或等于 $I(X;Y)$。您的任务是找到一个特定条件，在该条件下，最终输出端保留的信息恰好是中间阶段可用信息的 75%。\n\n计算所需的噪声方差之比 $R = \\sigma_{N_2}^2 / \\sigma_{N_1}^2$，使得关系式 $I(X; Z) = 0.75 \\cdot I(X; Y)$ 成立。",
            "solution": "因为 $X$、$N_{1}$ 和 $N_{2}$ 是相互独立的零均值高斯变量，所以 $Y=X+N_{1}$ 和 $Z=X+N_{1}+N_{2}$ 也是高斯变量。对于高斯输入的加性高斯信道，其互信息可以通过微分熵得到：\n$$\nI(X;Y)=h(Y)-h(N_{1}).\n$$\n由于 $Y\\sim\\mathcal{N}\\!\\left(0,\\sigma_{X}^{2}+\\sigma_{N_{1}}^{2}\\right)$ 且 $N_{1}\\sim\\mathcal{N}\\!\\left(0,\\sigma_{N_{1}}^{2}\\right)$，使用 $h(\\mathcal{N}(0,\\sigma^{2}))=\\tfrac{1}{2}\\ln\\!\\left(2\\pi e\\,\\sigma^{2}\\right)$ 可得\n$$\nI(X;Y)=\\frac{1}{2}\\ln\\!\\left(\\frac{\\sigma_{X}^{2}+\\sigma_{N_{1}}^{2}}{\\sigma_{N_{1}}^{2}}\\right)=\\frac{1}{2}\\ln\\!\\left(1+\\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2}}\\right).\n$$\n定义 $S_{in}=\\sigma_{X}^{2}/\\sigma_{N_{1}}^{2}$。那么\n$$\nI(X;Y)=\\frac{1}{2}\\ln(1+S_{in}).\n$$\n\n对于级联输出 $Z=X+N_{1}+N_{2}$，有效噪声是 $N_{1}+N_{2}$，其方差为 $\\sigma_{N_{1}}^{2}+\\sigma_{N_{2}}^{2}$，且独立于 $X$。因此\n$$\nI(X;Z)=\\frac{1}{2}\\ln\\!\\left(1+\\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2}+\\sigma_{N_{2}}^{2}}\\right).\n$$\n引入 $R=\\sigma_{N_{2}}^{2}/\\sigma_{N_{1}}^{2}$，则 $\\sigma_{N_{1}}^{2}+\\sigma_{N_{2}}^{2}=\\sigma_{N_{1}}^{2}(1+R)$ 并且\n$$\nI(X;Z)=\\frac{1}{2}\\ln\\!\\left(1+\\frac{S_{in}}{1+R}\\right).\n$$\n\n施加条件 $I(X;Z)=0.75\\,I(X;Y)$：\n$$\n\\frac{1}{2}\\ln\\!\\left(1+\\frac{S_{in}}{1+R}\\right)=0.75\\cdot\\frac{1}{2}\\ln(1+S_{in}).\n$$\n两边乘以 $2$ 并取指数：\n$$\n\\ln\\!\\left(1+\\frac{S_{in}}{1+R}\\right)=0.75\\,\\ln(1+S_{in})\n\\quad\\Longrightarrow\\quad\n1+\\frac{S_{in}}{1+R}=(1+S_{in})^{0.75}.\n$$\n求解 $R$：\n$$\n\\frac{S_{in}}{1+R}=(1+S_{in})^{0.75}-1\n\\;\\Longrightarrow\\;\n1+R=\\frac{S_{in}}{(1+S_{in})^{0.75}-1}\n\\;\\Longrightarrow\\;\nR=\\frac{S_{in}}{(1+S_{in})^{0.75}-1}-1.\n$$\n\n给定 $S_{in}=15$，计算\n$$\nR=\\frac{15}{16^{0.75}-1}-1.\n$$\n注意到 $16^{0.75}=16^{3/4}=(2^{4})^{3/4}=2^{3}=8$，所以\n$$\nR=\\frac{15}{8-1}-1=\\frac{15}{7}-1=\\frac{8}{7}.\n$$",
            "answer": "$$\\boxed{\\frac{8}{7}}$$"
        }
    ]
}