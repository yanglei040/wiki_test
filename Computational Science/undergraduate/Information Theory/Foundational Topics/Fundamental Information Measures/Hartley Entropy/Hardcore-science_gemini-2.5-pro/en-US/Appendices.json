{
    "hands_on_practices": [
        {
            "introduction": "The first step in applying the Hartley entropy formula is often the most critical: correctly identifying the number of possible outcomes. This exercise provides practice in this fundamental skill, set in the practical context of a security protocol. By analyzing the system's rules, you will determine the size of the outcome space, $N$, from a set of specific constraints before calculating the associated information content .",
            "id": "1629232",
            "problem": "A researcher is designing a basic security protocol for a network of environmental sensors. The protocol, named 'Protocol Gamma,' generates a temporary access key for each data transmission. The key is a single integer selected uniformly at random from a specific set of allowed values. According to the design specifications for Protocol Gamma, the integer key must be chosen from the range of 1 to 20, inclusive. Furthermore, a hardware constraint requires that the chosen integer must be a multiple of 4.\n\nAs part of the system analysis, you are asked to quantify the information content associated with the selection of a single key. Calculate the Hartley entropy (also known as max-entropy) for the process of generating a single key under Protocol Gamma. The Hartley entropy is a measure of uncertainty for a set of possible outcomes, and for this problem, it should be determined using a base-2 logarithm to be expressed in units of bits. Provide the final answer as a closed-form analytic expression.",
            "solution": "The problem asks for the Hartley entropy of a set of possible security keys. The Hartley entropy, $H_0$, for a set of $N$ equally likely outcomes is given by the formula:\n$$H_0 = \\log_{b}(N)$$\nThe problem specifies that the entropy should be expressed in bits, which implies the use of a base-2 logarithm ($b=2$). Therefore, the formula we will use is:\n$$H_0 = \\log_{2}(N)$$\nwhere $N$ is the total number of possible keys.\n\nOur first step is to determine the set of all possible keys and find its size, $N$. The problem states that a key is an integer that satisfies two conditions:\n1. It must be in the range [1, 20], inclusive.\n2. It must be a multiple of 4.\n\nLet's list all integers in the range from 1 to 20 that are multiples of 4. A number $k$ is a multiple of 4 if $k = 4m$ for some integer $m$.\nWe need to find integers $k$ such that $1 \\le k \\le 20$ and $k=4m$.\n\n- For $m=1$, $k = 4 \\times 1 = 4$. This is in the range.\n- For $m=2$, $k = 4 \\times 2 = 8$. This is in the range.\n- For $m=3$, $k = 4 \\times 3 = 12$. This is in the range.\n- For $m=4$, $k = 4 \\times 4 = 16$. This is in the range.\n- For $m=5$, $k = 4 \\times 5 = 20$. This is in the range.\n- For $m=6$, $k = 4 \\times 6 = 24$. This is outside the range $[1, 20]$.\n\nThus, the set of all possible keys, $S$, is:\n$$S = \\{4, 8, 12, 16, 20\\}$$\n\nThe number of possible outcomes, $N$, is the number of elements in the set $S$. By counting the elements, we find:\n$$N = |S| = 5$$\n\nNow we can calculate the Hartley entropy by substituting $N=5$ into the formula:\n$$H_0 = \\log_{2}(5)$$\n\nThe problem asks for a closed-form analytic expression. The expression $\\log_{2}(5)$ is a complete and exact analytic representation of the Hartley entropy in bits.",
            "answer": "$$\\boxed{\\log_{2}(5)}$$"
        },
        {
            "introduction": "Information can be thought of as the resolution of uncertainty. This practice demonstrates this principle by showing how adding constraints to a system reduces its entropy, a measure of that uncertainty. By comparing the entropy of an unrestricted set of choices to a restricted one, you will quantify the \"information\" gained by knowing those constraints . This exercise highlights how entropy changes as our knowledge of a system improves.",
            "id": "1629265",
            "problem": "In the development of a chess-playing Artificial Intelligence (AI), the process of selecting a square on the board is analyzed using information theory. The Hartley entropy, which quantifies the information required to specify an outcome from a set of $N$ equally likely possibilities, is used for this analysis.\n\nConsider a standard $8 \\times 8$ chessboard.\nFirst, analyze the initial version of the AI, which can select any single square on the board with equal probability.\nNext, analyze a refined version where the AI's choice is restricted. In this version, the AI can only select a square that is not on the perimeter of the board (i.e., it cannot select a square in the first or last row, nor in the first or last column).\n\nCalculate the reduction in the Hartley entropy, in bits, that results from this refinement. Express your answer as a single closed-form analytic expression.",
            "solution": "Hartley entropy quantifies the information required to specify an outcome from a set of $N$ equally likely possibilities. When measured in bits, the Hartley entropy is given by\n$$\nH = \\log_{2}(N).\n$$\nFor a standard $8 \\times 8$ chessboard, the number of squares is\n$$\nN_{1} = 8 \\times 8 = 64,\n$$\nso the initial AI has entropy\n$$\nH_{1} = \\log_{2}(64).\n$$\nIn the refined version, the AI cannot choose squares on the perimeter. The interior squares form a $(8-2) \\times (8-2)$ grid, so\n$$\nN_{2} = 6 \\times 6 = 36,\n$$\nand the refined entropy is\n$$\nH_{2} = \\log_{2}(36).\n$$\nThe reduction in Hartley entropy is the difference:\n$$\n\\Delta H = H_{1} - H_{2} = \\log_{2}(64) - \\log_{2}(36) = \\log_{2}\\!\\left(\\frac{64}{36}\\right) = \\log_{2}\\!\\left(\\frac{16}{9}\\right).\n$$\nThis is a single closed-form analytic expression in bits.",
            "answer": "$$\\boxed{\\log_{2}\\!\\left(\\frac{16}{9}\\right)}$$"
        },
        {
            "introduction": "The method used to generate outcomes—for example, whether items can be repeated—has a significant impact on the total number of possibilities. This problem explores the crucial difference between sampling with replacement and sampling without replacement, a common consideration in fields like cryptography and statistics. You will calculate and compare the Hartley entropy for these two distinct scenarios to understand how the generation method fundamentally affects the information content of the resulting set of possibilities .",
            "id": "1629258",
            "problem": "A team of cryptographers is evaluating two algorithms for generating 3-character-long access codes. The character pool for these codes consists of 8 distinct, non-alphanumeric symbols.\n\nThe first algorithm, \"Method R,\" generates an ordered sequence of 3 characters where symbols can be repeated. This is analogous to sampling with replacement.\n\nThe second algorithm, \"Method U,\" generates an ordered sequence of 3 characters where each symbol in the sequence must be unique. This is analogous to sampling without replacement.\n\nThe uncertainty associated with a set of possible outcomes is measured by its Hartley entropy, defined as $H_0 = \\log_2(N)$, where $N$ is the total number of distinct outcomes. The unit for this entropy is bits.\n\nCalculate the absolute difference in Hartley entropy, in bits, between the set of all possible codes generated by Method R and the set of all possible codes generated by Method U. Present your answer as a single, simplified, closed-form analytic expression.",
            "solution": "Let $H_R$ be the Hartley entropy for the set of codes generated by Method R, and $H_U$ be the Hartley entropy for the set of codes generated by Method U. We are asked to find the absolute difference, $|\\Delta H| = |H_R - H_U|$.\n\nThe Hartley entropy is given by the formula $H_0 = \\log_2(N)$, where $N$ is the number of possible outcomes.\n\n**Step 1: Calculate the number of outcomes for Method R.**\nMethod R generates an ordered 3-character code from a pool of 8 symbols with replacement. For each of the 3 positions in the code, there are 8 possible choices.\nThe total number of distinct codes, $N_R$, is the product of the number of choices for each position:\n$$N_R = 8 \\times 8 \\times 8 = 8^3 = 512$$\n\n**Step 2: Calculate the Hartley entropy for Method R.**\nUsing the formula for Hartley entropy:\n$$H_R = \\log_2(N_R) = \\log_2(512) = \\log_2(2^9)$$\nUsing the logarithm property $\\log_a(b^c) = c \\log_a(b)$:\n$$H_R = 9 \\log_2(2) = 9 \\times 1 = 9 \\text{ bits}$$\n\n**Step 3: Calculate the number of outcomes for Method U.**\nMethod U generates an ordered 3-character code from a pool of 8 symbols without replacement. This is a permutation problem. The number of ways to choose an ordered sequence of $k=3$ items from a set of $n=8$ items without replacement is given by the permutation formula $P(n, k) = \\frac{n!}{(n-k)!}$.\n$$N_U = P(8, 3) = \\frac{8!}{(8-3)!} = \\frac{8!}{5!} = 8 \\times 7 \\times 6$$\n$$N_U = 336$$\n\n**Step 4: Calculate the Hartley entropy for Method U.**\nUsing the formula for Hartley entropy:\n$$H_U = \\log_2(N_U) = \\log_2(336)$$\n\n**Step 5: Calculate the absolute difference in entropy.**\nThe absolute difference is $|\\Delta H| = |H_R - H_U|$. Since $N_R = 512 > N_U = 336$, it follows that $H_R > H_U$. Therefore, the absolute difference is simply $H_R - H_U$.\n$$\\Delta H = H_R - H_U = 9 - \\log_2(336)$$\nTo express this as a single term, we can use the property of logarithms that $\\log_a(b) - \\log_a(c) = \\log_a\\left(\\frac{b}{c}\\right)$. We first write $9$ as a logarithm with base 2: $9 = 9 \\log_2(2) = \\log_2(2^9) = \\log_2(512)$.\n$$\\Delta H = \\log_2(512) - \\log_2(336) = \\log_2\\left(\\frac{512}{336}\\right)$$\nNow, we simplify the fraction:\n$$\\frac{512}{336} = \\frac{256}{168} = \\frac{128}{84} = \\frac{64}{42} = \\frac{32}{21}$$\nTherefore, the absolute difference in entropy is:\n$$\\Delta H = \\log_2\\left(\\frac{32}{21}\\right)$$\nThis is the final, simplified, closed-form analytic expression.",
            "answer": "$$\\boxed{\\log_{2}\\left(\\frac{32}{21}\\right)}$$"
        }
    ]
}