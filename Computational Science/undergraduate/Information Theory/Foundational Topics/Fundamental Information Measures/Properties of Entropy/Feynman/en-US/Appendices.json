{
    "hands_on_practices": [
        {
            "introduction": "Conditional entropy, such as $H(Y|X)$, quantifies the average uncertainty remaining in a variable $Y$ once the value of a variable $X$ is revealed. A natural but incorrect assumption is that this relationship is symmetric, meaning $H(Y|X) = H(X|Y)$. This exercise  provides a definitive counterexample using a simple biological model to solidify your understanding of this crucial and often misunderstood property of entropy.",
            "id": "1649381",
            "problem": "In a simplified model of a cellular signaling pathway, a signaling protein $X$ can exist in one of two states: 'active' ($A$) or 'inactive' ($B$). The state of this protein influences a downstream cellular response $Y$, which can result in one of three distinct outcomes: 'growth' (1), 'stasis' (2), or 'apoptosis' (3).\n\nThe joint probability distribution, $p(x, y)$, for the state of protein $X$ and the cellular response $Y$ has been determined experimentally and is given by the following table:\n\n|            | $Y=1$ (growth) | $Y=2$ (stasis) | $Y=3$ (apoptosis) |\n| :--------: | :------------: | :------------: | :---------------: |\n| $X=A$ (active) |     $1/2$      |       0        |         0         |\n| $X=B$ (inactive) |       0        |     $1/4$      |       $1/4$       |\n\nThe uncertainty of a random variable $Z$ is quantified by the Shannon entropy, $H(Z) = -\\sum_{z} p(z) \\log_2 p(z)$, where the logarithm is base 2 and the resulting unit is bits. The conditional entropy of a variable $U$ given another variable $V$, denoted $H(U|V)$, measures the remaining uncertainty in $U$ when $V$ is known.\n\nCalculate the conditional entropy of the cellular response given the protein state, $H(Y|X)$, and the conditional entropy of the protein state given the cellular response, $H(X|Y)$. Express your answer as the ordered pair $(H(Y|X), H(X|Y))$, with both values in units of bits.",
            "solution": "We use the definitions of Shannon entropy and conditional entropy. For any random variables $U$ and $V$, the conditional entropy can be written as\n$$\nH(U|V)=\\sum_{v} p(v)\\,H(U|V=v), \\quad \\text{where} \\quad H(U|V=v)=-\\sum_{u} p(u|v)\\log_{2} p(u|v).\n$$\nFrom the joint distribution, the marginals of $X$ are\n$$\np(X=A)=\\frac{1}{2}+0+0=\\frac{1}{2}, \\quad p(X=B)=0+\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}.\n$$\nThe conditional distributions $p(Y|X)$ are:\n- For $X=A$: $p(Y=1|X=A)=\\frac{(1/2)}{(1/2)}=1$, $p(Y=2|X=A)=0$, $p(Y=3|X=A)=0$, hence\n$$\nH(Y|X=A)=-\\left[1\\cdot \\log_{2} 1+0+0\\right]=0.\n$$\n- For $X=B$: $p(Y=1|X=B)=0$, $p(Y=2|X=B)=\\frac{(1/4)}{(1/2)}=\\frac{1}{2}$, $p(Y=3|X=B)=\\frac{(1/4)}{(1/2)}=\\frac{1}{2}$, hence\n$$\nH(Y|X=B)=-\\left[\\frac{1}{2}\\log_{2}\\frac{1}{2}+\\frac{1}{2}\\log_{2}\\frac{1}{2}\\right]\n=-\\left[\\frac{1}{2}(-1)+\\frac{1}{2}(-1)\\right]=1.\n$$\nTherefore,\n$$\nH(Y|X)=p(X=A)H(Y|X=A)+p(X=B)H(Y|X=B)=\\frac{1}{2}\\cdot 0+\\frac{1}{2}\\cdot 1=\\frac{1}{2}.\n$$\n\nNext, compute $H(X|Y)$. The marginals of $Y$ are\n$$\np(Y=1)=\\frac{1}{2}, \\quad p(Y=2)=\\frac{1}{4}, \\quad p(Y=3)=\\frac{1}{4}.\n$$\nThe conditional distributions $p(X|Y)$ are:\n- For $Y=1$: only $(X=A,Y=1)$ has nonzero probability, so $p(X=A|Y=1)=1$, $p(X=B|Y=1)=0$, hence $H(X|Y=1)=0$.\n- For $Y=2$: only $(X=B,Y=2)$ has nonzero probability, so $p(X=B|Y=2)=1$, hence $H(X|Y=2)=0$.\n- For $Y=3$: only $(X=B,Y=3)$ has nonzero probability, so $p(X=B|Y=3)=1$, hence $H(X|Y=3)=0$.\n\nThus,\n$$\nH(X|Y)=\\sum_{y} p(y)H(X|Y=y)=\\frac{1}{2}\\cdot 0+\\frac{1}{4}\\cdot 0+\\frac{1}{4}\\cdot 0=0.\n$$\n\nTherefore, the ordered pair is $\\left(H(Y|X),\\,H(X|Y)\\right)=\\left(\\frac{1}{2},\\,0\\right)$ in bits.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2} & 0\\end{pmatrix}}$$"
        },
        {
            "introduction": "Now that we understand how conditional entropy works, let's investigate an extreme case: what does it mean for the conditional entropy $H(Y|X)$ to be zero? This scenario implies that once we know $X$, all uncertainty about $Y$ vanishes. This practice problem  challenges you to work backward from this condition to determine the underlying structure of the system, revealing the powerful link between zero conditional entropy and a deterministic functional relationship.",
            "id": "1649394",
            "problem": "In a simplified model for a quantum computing architecture, the states of two correlated qubits, Qubit A and Qubit B, are represented by binary random variables $X$ and $Y$, respectively. Both $X$ and $Y$ can take values from the set $\\{0, 1\\}$. An analysis of the system's behavior over many trials has established the following marginal probabilities:\n1. The probability of Qubit A being in state 0 is $P(X=0) = \\frac{3}{5}$.\n2. The probability of Qubit B being in state 0 is $P(Y=0) = \\frac{2}{5}$.\n\nA key theoretical property of this specific architecture is that the joint entropy of the two-qubit system, $H(X,Y)$, is exactly equal to the marginal entropy of Qubit A, $H(X)$. In this context, all entropies are calculated using the base-2 logarithm.\n\nGiven this information, determine the complete joint probability distribution $P(X=x, Y=y)$. Express your answer as the set of a row matrix of four probabilities $(P(X=0, Y=0), P(X=0, Y=1), P(X=1, Y=0), P(X=1, Y=1))$.",
            "solution": "Let $p_{xy}:=P(X=x,Y=y)$ for $x,y\\in\\{0,1\\}$. The given marginals imply\n$$\np_{00}+p_{01}=P(X=0)=\\frac{3}{5},\\qquad p_{10}+p_{11}=P(X=1)=\\frac{2}{5},\n$$\n$$\np_{00}+p_{10}=P(Y=0)=\\frac{2}{5},\\qquad p_{01}+p_{11}=P(Y=1)=\\frac{3}{5},\n$$\nand $p_{00}+p_{01}+p_{10}+p_{11}=1$.\n\nSolve the linear system by eliminating variables. From $p_{00}+p_{01}=\\frac{3}{5}$ and $p_{00}+p_{10}=\\frac{2}{5}$, set $p_{00}=a$. Then\n$$\np_{01}=\\frac{3}{5}-a,\\qquad p_{10}=\\frac{2}{5}-a,\n$$\nand using the total sum constraint,\n$$\np_{11}=1-\\bigl(a+\\bigl(\\tfrac{3}{5}-a\\bigr)+\\bigl(\\tfrac{2}{5}-a\\bigr)\\bigr)=a.\n$$\nNonnegativity requires $0\\leq a\\leq \\frac{2}{5}$.\n\nNext use the entropy condition. By the chain rule for entropy (base-2),\n$$\nH(X,Y)=H(X)+H(Y\\mid X).\n$$\nGiven $H(X,Y)=H(X)$, it follows that $H(Y\\mid X)=0$. For a discrete random variable, $H(Y\\mid X)=0$ if and only if $Y$ is a deterministic function of $X$ almost surely. Equivalently, for each $x$ with $P(X=x)>0$, the conditional distribution $P(Y=\\cdot\\mid X=x)$ is degenerate.\n\nCompute the conditionals in terms of $a$:\n$$\nP(Y=0\\mid X=0)=\\frac{p_{00}}{P(X=0)}=\\frac{a}{3/5}=\\frac{5a}{3},\\qquad P(Y=1\\mid X=0)=1-\\frac{5a}{3}.\n$$\nDegeneracy for $X=0$ requires $\\frac{5a}{3}\\in\\{0,1\\}$, hence $a\\in\\{0,\\frac{3}{5}\\}$. Since $0\\leq a\\leq \\frac{2}{5}$, we must have $a=0$.\n\nCheck degeneracy for $X=1$ as well:\n$$\nP(Y=0\\mid X=1)=\\frac{p_{10}}{P(X=1)}=\\frac{\\frac{2}{5}-a}{2/5}=1-\\frac{5a}{2},\\qquad P(Y=1\\mid X=1)=\\frac{5a}{2}.\n$$\nWith $a=0$, this is also degenerate. Therefore $a=0$ is the unique solution consistent with the entropy constraint.\n\nSubstitute $a=0$ to obtain\n$$\np_{00}=0,\\quad p_{01}=\\frac{3}{5},\\quad p_{10}=\\frac{2}{5},\\quad p_{11}=0.\n$$\nThis corresponds to $Y=1-X$ almost surely, which also gives $P(Y=0)=P(X=1)=\\frac{2}{5}$ as required, and ensures $H(Y\\mid X)=0$, hence $H(X,Y)=H(X)$.",
            "answer": "$$\\boxed{\\begin{pmatrix}0 & \\frac{3}{5} & \\frac{2}{5} & 0\\end{pmatrix}}$$"
        },
        {
            "introduction": "The properties of entropy are not just theoretical curiosities; they are foundational principles in system design, from communication to cryptography. This exercise  places you in the role of an engineer tasked with making a system as predictable as possible by minimizing its joint entropy, $H(X,Y)$. By adjusting the level of correlation between two components, you will apply your understanding of entropy to solve a practical optimization problem and uncover the conditions that lead to minimal uncertainty.",
            "id": "1649373",
            "problem": "In the design of a cryptographic hardware module, two coupled processes generate a stream of binary random variables, denoted as $X$ and $Y$. These variables can take values in $\\{0, 1\\}$. For the system to function correctly, the marginal distributions of these variables must be uniform, meaning that the probability of observing a '1' is the same as observing a '0' for each variable individually. Specifically, the design enforces $P(X=1) = 1/2$ and $P(Y=1) = 1/2$.\n\nThe lead designer has one degree of freedom to tune the system: the conditional probability $p = P(Y=1|X=1)$. This parameter controls the correlation between the two binary streams. The goal is to configure the system to be as predictable as possible, which in the language of information theory means minimizing the total uncertainty, or joint entropy, $H(X,Y)$.\n\nAssuming entropy is calculated in bits (using logarithm base 2), determine all possible values of the parameter $p$ that will minimize the joint entropy $H(X,Y)$.",
            "solution": "Let $X,Y\\in\\{0,1\\}$ with $P(X=1)=\\frac{1}{2}$ and $P(Y=1)=\\frac{1}{2}$. Let $p=P(Y=1\\mid X=1)$. Denote $q=P(Y=1\\mid X=0)$. The marginal constraint on $Y$ gives, by the law of total probability,\n$$\nP(Y=1)=P(Y=1\\mid X=1)P(X=1)+P(Y=1\\mid X=0)P(X=0)=\\tfrac{1}{2}p+\\tfrac{1}{2}q=\\tfrac{1}{2},\n$$\nhence $p+q=1$ and thus $q=1-p$. The joint probabilities are\n$$\nP(X=1,Y=1)=\\tfrac{1}{2}p,\\quad P(X=1,Y=0)=\\tfrac{1}{2}(1-p),\\quad P(X=0,Y=1)=\\tfrac{1}{2}(1-p),\\quad P(X=0,Y=0)=\\tfrac{1}{2}p,\n$$\nso $0\\leq p\\leq 1$.\n\nThe joint entropy in bits is\n$$\nH(X,Y)=-\\sum_{x,y}P(x,y)\\log_{2}P(x,y)\n= -2\\cdot\\tfrac{1}{2}p\\log_{2}\\!\\big(\\tfrac{1}{2}p\\big)\\;-\\;2\\cdot\\tfrac{1}{2}(1-p)\\log_{2}\\!\\big(\\tfrac{1}{2}(1-p)\\big).\n$$\nUsing $\\log_{2}\\!\\big(\\tfrac{a}{2}\\big)=\\log_{2}a-1$, this simplifies to\n$$\nH(X,Y)=-p\\big(\\log_{2}p-1\\big)-(1-p)\\big(\\log_{2}(1-p)-1\\big)\n= -p\\log_{2}p-(1-p)\\log_{2}(1-p)+1.\n$$\nDefine the binary entropy function $h_{2}(p)=-p\\log_{2}p-(1-p)\\log_{2}(1-p)$. Then\n$$\nH(X,Y)=1+h_{2}(p).\n$$\nTo minimize $H(X,Y)$ over $p\\in[0,1]$, it suffices to minimize $h_{2}(p)$. Differentiating,\n$$\nh_{2}'(p)=-\\log_{2}p+\\log_{2}(1-p),\\qquad\nh_{2}''(p)=-\\frac{1}{\\ln 2}\\Big(\\frac{1}{p}+\\frac{1}{1-p}\\Big)<0\\quad\\text{for }p\\in(0,1),\n$$\nso $h_{2}$ is concave with its unique stationary point at $h_{2}'(p)=0\\Rightarrow p=1/2$, which is the maximum. Therefore the minimum of $h_{2}(p)$ on $[0,1]$ occurs at the endpoints $p=0$ and $p=1$, where $h_{2}(0)=h_{2}(1)=0$. Consequently,\n$$\n\\min_{p\\in[0,1]}H(X,Y)=1+h_{2}(p)\\quad\\text{is attained at }p\\in\\{0,1\\}.\n$$\nAt $p=1$, we have $Y=X$ almost surely; at $p=0$, we have $Y=1-X$ almost surely. Both satisfy the marginal constraints and yield the minimum joint entropy.",
            "answer": "$$\\boxed{\\begin{pmatrix}0 & 1\\end{pmatrix}}$$"
        }
    ]
}