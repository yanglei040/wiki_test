{
    "hands_on_practices": [
        {
            "introduction": "The chain rule for mutual information provides a powerful way to decompose the information that a set of variables provides about another. Much like how the chain rule in calculus breaks down complex derivatives, this rule allows us to understand information contributions sequentially. This first exercise  will challenge you to apply the fundamental definition of the chain rule, $I(X, Y; Z) = I(X; Z) + I(Y; Z|X)$, to a relatable scenario, ensuring you grasp how to correctly partition the total information from multiple sources.",
            "id": "1608881",
            "problem": "In an introductory physics course, a professor is analyzing how students' performance on exams relates to their final course outcome. The professor models the situation using three discrete random variables: $M$ represents the score on the midterm exam, $F$ represents the score on the final exam, and $G$ represents the final letter grade assigned for the course.\n\nThe professor is interested in quantifying the total amount of information that the collection of both exam scores, midterm and final, provides about the student's final grade. This quantity is represented by the mutual information $I(M, F; G)$.\n\nWhich of the following expressions is always equivalent to $I(M, F; G)$ based on the fundamental properties of information theory?\n\nA. $I(M; G) + I(F; G | M)$\n\nB. $I(M; G) + I(F; G)$\n\nC. $H(M) + H(F) - H(G)$\n\nD. $I(M; F) + I(G; M | F)$\n\nE. $H(G) - H(G | M) - H(G | F)$",
            "solution": "Let $I(X;Y)$ denote mutual information, $I(X;Y|Z)$ conditional mutual information, and $H(\\cdot)$ entropy. The fundamental definitions are:\n$$\nI(X;Y) = H(Y) - H(Y|X) = H(X) - H(X|Y),\n$$\n$$\nI(X;Y|Z) = H(Y|Z) - H(Y|X,Z) = H(X|Z) - H(X|Y,Z),\n$$\nand for joint variables,\n$$\nI(X,Y;Z) = H(Z) - H(Z|X,Y) = H(X,Y) - H(X,Y|Z).\n$$\nUsing these, derive the chain rule for mutual information:\n$$\nI(X,Y;Z) = H(Z) - H(Z|X,Y) = \\bigl[H(Z) - H(Z|X)\\bigr] + \\bigl[H(Z|X) - H(Z|X,Y)\\bigr] = I(X;Z) + I(Y;Z|X).\n$$\nApplying this with $X=M$, $Y=F$, and $Z=G$ gives\n$$\nI(M,F;G) = I(M;G) + I(F;G|M),\n$$\nwhich establishes option A as always correct.\n\nNow evaluate the other options:\n\nB. Compute $I(M;G) + I(F;G)$:\n$$\nI(M;G) + I(F;G) = \\bigl[H(G) - H(G|M)\\bigr] + \\bigl[H(G) - H(G|F)\\bigr] = 2H(G) - H(G|M) - H(G|F).\n$$\nCompare with\n$$\nI(M,F;G) = H(G) - H(G|M,F).\n$$\nIn general $2H(G) - H(G|M) - H(G|F) \\neq H(G) - H(G|M,F)$, so B is not always equal to $I(M,F;G)$.\n\nC. $H(M) + H(F) - H(G)$ depends only on marginal entropies and in general does not equal $I(M,F;G) = H(G) - H(G|M,F)$, which depends on the joint distribution. Thus C is not generally equal.\n\nD. Use symmetry of conditional mutual information $I(G;M|F) = I(M;G|F)$ and the chain rule:\n$$\nI(M;F) + I(G;M|F) = I(M;F) + I(M;G|F) = I(M;F,G) = I(M;(F,G)).\n$$\nThis equals $I(M;(F,G))$, not $I((M,F);G)$ in general, so D is not always equal.\n\nE. Rewrite\n$$\nH(G) - H(G|M) - H(G|F) = I(M;G) - H(G|F),\n$$\nwhich does not generally equal $H(G) - H(G|M,F) = I(M,F;G)$. Hence E is not always equal.\n\nTherefore, the only expression that is always equivalent to $I(M,F;G)$ is option A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "To truly understand information theory, it is essential to move from abstract formulas to concrete calculations. This practice problem  grounds the concept of mutual information in a simple, tangible scenario: a card game. By calculating the information that two revealed cards provide about a third, you will build a strong intuition for how knowledge reduces uncertainty, which is the very essence of mutual information. As you work through the solution, consider how the information from the first card, $I(C_1; C_3)$, and the additional information from the second card, $I(C_2; C_3 | C_1)$, combine to give the total.",
            "id": "1608889",
            "problem": "Consider a minimalist card game played with a special deck containing $N=20$ unique cards. The deck is perfectly shuffled, and three cards, denoted by the random variables $C_1$, $C_2$, and $C_3$, are drawn sequentially without replacement. An observer sees the first two cards, $(C_1, C_2)$, but the third card, $C_3$, remains unknown.\n\nCalculate the mutual information between the pair of observed cards $(C_1, C_2)$ and the unknown third card $C_3$. All logarithms are base 2. Express your answer in bits, rounded to four significant figures.",
            "solution": "Let $X=(C_{1},C_{2})$ and $Y=C_{3}$. The mutual information is defined as\n$$\nI(X;Y)=H(Y)-H(Y|X),\n$$\nwith all logarithms base $2$.\n\nSince the deck is perfectly shuffled, $C_{3}$ is uniformly distributed over the $N=20$ cards, so\n$$\nH(C_{3})=\\log_{2}(20).\n$$\nGiven $(C_{1},C_{2})$, the third card must be one of the remaining $20-2=18$ cards, and by symmetry it is uniform over these, hence\n$$\nH(C_{3}\\mid C_{1},C_{2})=\\log_{2}(18).\n$$\nTherefore,\n$$\nI\\big((C_{1},C_{2});C_{3}\\big)=\\log_{2}(20)-\\log_{2}(18)=\\log_{2}\\!\\left(\\frac{20}{18}\\right)=\\log_{2}\\!\\left(\\frac{10}{9}\\right).\n$$\nUsing the change of base formula $\\log_{2}(a)=\\frac{\\ln(a)}{\\ln(2)}$ and rounding to four significant figures,\n$$\n\\log_{2}\\!\\left(\\frac{10}{9}\\right)\\approx 0.1520 \\text{ bits}.\n$$",
            "answer": "$$\\boxed{0.1520}$$"
        },
        {
            "introduction": "The chain rule is not merely a definitional identity; it is a crucial analytical tool for understanding information flow in complex systems. In this problem , we explore a common scenario in communications engineering: a signal passing through a cascade of noisy channels. You will use the chain rule, in conjunction with the properties of Markov chains, to quantify how much information is preserved or lost at different stages. This exercise demonstrates the power of these principles to analyze and reason about the propagation of information in practical signal processing applications.",
            "id": "1612841",
            "problem": "In a digital communication system, a binary information source produces a random bit $X$, which is equally likely to be 0 or 1. This bit is transmitted through a noisy communication channel, which can be modeled as a Binary Symmetric Channel (BSC), to produce an output bit $Y$. The crossover probability of this channel, which is the probability that the output bit differs from the input bit, is denoted by $\\alpha$, where $0 < \\alpha < 0.5$.\n\nThe received bit $Y$ is not observed directly. Instead, it is immediately processed and re-transmitted through a second, independent BSC to produce a final observed bit $Z$. This second channel has a crossover probability of $\\beta$, where $0 < \\beta < 0.5$. The entire process can be summarized as the chain $X \\to Y \\to Z$.\n\nYour task is to calculate the conditional mutual information $I(X;Y|Z)$, which quantifies the amount of additional information that the original bit $X$ and the intermediate bit $Y$ provide about each other, given that the final bit $Z$ is known.\n\nProvide your final answer as an analytical expression in terms of $\\alpha$, $\\beta$, and the binary entropy function $H_b(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$.",
            "solution": "Let $X \\to Y \\to Z$ be the Markov chain with $X \\in \\{0,1\\}$ uniform, $Y$ obtained from $X$ through a BSC with crossover probability $\\alpha$, and $Z$ obtained from $Y$ through an independent BSC with crossover probability $\\beta$. We are to compute $I(X;Y|Z)$.\n\nBy the chain rule for mutual information,\n$$\nI(X;Y,Z) = I(X;Z) + I(X;Y|Z) = I(X;Y) + I(X;Z|Y).\n$$\nFor the Markov chain $X \\to Y \\to Z$, we have conditional independence $X \\perp Z \\mid Y$, hence\n$$\nI(X;Z|Y)=0.\n$$\nTherefore,\n$$\nI(X;Y|Z) = I(X;Y) - I(X;Z).\n$$\n\nWe now compute $I(X;Y)$ and $I(X;Z)$.\n\nFirst, for the BSC$(\\alpha)$ with uniform input, the output $Y$ is uniform:\n$$\nP(Y=0) = P(X=0)P(Y=0|X=0) + P(X=1)P(Y=0|X=1) = \\frac{1}{2}(1-\\alpha) + \\frac{1}{2}\\alpha = \\frac{1}{2},\n$$\nso $H(Y)=1$. The conditional entropy is $H(Y|X)=H_{b}(\\alpha)$. Thus\n$$\nI(X;Y) = H(Y) - H(Y|X) = 1 - H_{b}(\\alpha).\n$$\n\nNext, the cascade of two BSCs is itself a BSC with effective crossover probability\n$$\n\\gamma = P(Z \\neq X) = \\alpha(1-\\beta) + (1-\\alpha)\\beta = \\alpha + \\beta - 2\\alpha\\beta.\n$$\nWith $X$ uniform, $Z$ is uniform, so $H(Z)=1$ and $H(Z|X)=H_{b}(\\gamma)$, giving\n$$\nI(X;Z) = H(Z) - H(Z|X) = 1 - H_{b}(\\gamma) = 1 - H_{b}(\\alpha + \\beta - 2\\alpha\\beta).\n$$\n\nCombining,\n$$\nI(X;Y|Z) = \\bigl[1 - H_{b}(\\alpha)\\bigr] - \\bigl[1 - H_{b}(\\alpha + \\beta - 2\\alpha\\beta)\\bigr] = H_{b}(\\alpha + \\beta - 2\\alpha\\beta) - H_{b}(\\alpha).\n$$\nThis is the desired analytical expression in terms of $\\alpha$, $\\beta$, and $H_{b}(\\cdot)$.",
            "answer": "$$\\boxed{H_{b}\\!\\left(\\alpha+\\beta-2\\alpha\\beta\\right)-H_{b}(\\alpha)}$$"
        }
    ]
}