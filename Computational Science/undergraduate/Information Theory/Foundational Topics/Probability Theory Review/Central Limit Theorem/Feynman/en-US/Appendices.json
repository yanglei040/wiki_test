{
    "hands_on_practices": [
        {
            "introduction": "The Central Limit Theorem (CLT) is a cornerstone of probability, providing a powerful tool for approximating the distribution of a sum of independent random variables. This exercise offers a direct application of the theorem to a classic scenario involving Poisson-distributed variables, common in modeling event counts. By working through this problem, you will practice the core mechanics of using the CLT to calculate probabilities that would otherwise be computationally challenging .",
            "id": "480135",
            "problem": "Let $Y_1, Y_2, \\dots$ be a sequence of independent and identically distributed random variables, each following a Poisson distribution with parameter $\\lambda = 1$. Define the partial sum $S_n = \\sum_{k=1}^n Y_k$ for $n \\in \\mathbb{N}$. Using the central limit theorem, approximate the probability $P(90 \\leq S_{100} \\leq 110)$. Provide the numerical value of the approximation rounded to 10 significant digits.",
            "solution": "1. For i.i.d. $Y_k\\sim\\mathrm{Poisson}(1)$, the sum $S_{100}=\\sum_{k=1}^{100}Y_k$ has mean and variance\n$$\\mu=\\mathbb{E}[S_{100}]=100\\cdot1=100,\\quad\\sigma^2=\\mathrm{Var}(S_{100})=100\\cdot1=100.$$\n2. By the central limit theorem, for $Z\\sim N(0,1)$,\n$$P(90\\le S_{100}\\le110)\\approx P\\Bigl(\\frac{90-\\mu}{\\sigma}\\le Z\\le\\frac{110-\\mu}{\\sigma}\\Bigr).$$\n3. Compute the standardized bounds:\n$$\\frac{90-100}{10}=-1,\\quad\\frac{110-100}{10}=1.$$\n4. Thus\n$$P(90\\le S_{100}\\le110)\\approx \\Phi(1)-\\Phi(-1)=2\\Phi(1)-1.$$\n5. Using $\\Phi(1)\\approx0.8413447461$ gives\n$$2\\cdot0.8413447461-1=0.6826894921370859\\approx0.6826894921\\quad(\\text{10 significant digits}).$$",
            "answer": "$$\\boxed{0.6826894921}$$"
        },
        {
            "introduction": "When approximating a discrete probability distribution with a continuous one like the normal distribution, a subtle but important refinement is needed for accuracy. This practice introduces the continuity correction, a technique that adjusts the boundaries of the interval to better align the discrete and continuous models. This hypothetical problem provides an excellent opportunity to master this crucial step, enhancing the precision of your CLT-based approximations .",
            "id": "852436",
            "problem": "A discrete random variable $X$ can take values from the set $S = \\{-2, -1, 0, 1, 3\\}$. Its probability mass function (PMF) is given by $P(X=k) = C(k^2+1)$ for $k \\in S$, where $C$ is a normalization constant.\n\nLet $X_1, X_2, \\dots, X_n$ be a sequence of $n$ independent and identically distributed (i.i.d.) random variables, each with the same distribution as $X$. Consider their sum, $S_n = \\sum_{i=1}^n X_i$.\n\nFor $n=470$, using the Central Limit Theorem with continuity correction, calculate the probability $P(S_{470} > 540)$.\n\nYou are given that for a standard normal random variable $Z \\sim N(0,1)$, the value of its cumulative distribution function (CDF) at $z=1.5$ is $\\Phi(1.5) \\approx 0.9331927987$.",
            "solution": "1. Normalization constant:\n$$\\sum_{k\\in\\{-2,-1,0,1,3\\}}(k^2+1)=5+2+1+2+10=20,\\quad C=\\frac1{20}.$$\n2. Mean:\n$$\\mu=E[X]=C\\sum k\\,(k^2+1)\n=\\frac1{20}(-2\\cdot5-1\\cdot2+0\\cdot1+1\\cdot2+3\\cdot10)\n=\\frac{20}{20}=1.$$\n3. Second moment:\n$$E[X^2]=C\\sum k^2\\,(k^2+1)\n=\\frac1{20}(20+2+0+2+90)=\\frac{114}{20}=5.7.$$\nVariance:\n$$\\sigma^2=E[X^2]-\\mu^2=5.7-1^2=4.7.$$\n4. Sum $S_{470}$: mean $470\\mu=470$, variance $470\\sigma^2=470\\cdot4.7=2209$, so $\\sigma_{S}=47$.\n5. Continuity correction:\n$$P(S_{470}>540)\\approx P\\bigl(S_{470}>540.5\\bigr)\n= P\\!\\Bigl(Z>\\frac{540.5-470}{47}\\Bigr)\n= P(Z>1.5)=1-\\Phi(1.5)\\approx1-0.9331927987=0.0668072013.$$",
            "answer": "$$\\boxed{0.0668072013}$$"
        },
        {
            "introduction": "The utility of the Central Limit Theorem extends far beyond simply analyzing sums; it is a foundational tool for statistical inference. This exercise demonstrates how the CLT, combined with the Delta Method, allows us to determine the properties of parameter estimators. You will derive the asymptotic variance for a method-of-moments estimator, gaining insight into how we evaluate the quality of statistical methods used to infer model parameters from data .",
            "id": "852426",
            "problem": "A physical process is modeled by a sequence of independent and identically distributed (i.i.d.) random variables $X_1, X_2, \\dots, X_n$, representing measurements from an experiment. Each measurement $X_i$ is assumed to follow a uniform distribution on the interval $[0, \\theta]$, where $\\theta > 0$ is an unknown parameter of the model.\n\nTo estimate $\\theta$, the method of moments is employed. This method consists of two steps:\n1. Expressing the population mean $\\mu = E[X]$ as a function of the parameter, i.e., $\\mu = h(\\theta)$.\n2. Equating the population mean to the sample mean $\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i$, and solving for the parameter. The resulting solution is the method-of-moments estimator, $\\hat{\\theta}_{MoM}$.\n\nThe quality of this estimator for large sample sizes ($n \\to \\infty$) is often characterized by its asymptotic variance. The asymptotic variance of an estimator $\\hat{\\theta}$ is defined as the variance of the limiting normal distribution of the scaled and centered statistic $\\sqrt{n}(\\hat{\\theta} - \\theta)$. By the Central Limit Theorem and its extensions (like the Delta Method), this limiting distribution is typically normal.\n\nDerive the asymptotic variance of the method-of-moments estimator, $\\hat{\\theta}_{MoM}$, for the parameter $\\theta$.",
            "solution": "1. For $X_i\\sim \\mathrm{Uniform}(0,\\theta)$, we have\n$$\\mu=E[X_i]=\\frac{\\theta}{2},\\qquad \\Var(X_i)=\\frac{\\theta^2}{12}.$$\n2. The method-of-moments estimator solves $\\bar X_n=\\mu=\\theta/2$, giving\n$$\\hat\\theta_{MoM}=2\\bar X_n.$$\n3. By the Central Limit Theorem,\n$$\\sqrt{n}(\\bar X_n-\\tfrac{\\theta}{2})\\xrightarrow{d}N\\bigl(0,\\tfrac{\\theta^2}{12}\\bigr).$$\n4. Apply the Delta Method with $g(x)=2x,\\;g'(\\mu)=2$:\n$$\\sqrt{n}(\\hat\\theta_{MoM}-\\theta)\n=\\sqrt{n}\\bigl(g(\\bar X_n)-g(\\tfrac{\\theta}{2})\\bigr)\n\\xrightarrow{d}N\\bigl(0,\\,(g'(\\tfrac{\\theta}{2}))^2\\cdot\\tfrac{\\theta^2}{12}\\bigr).$$\n5. Hence the asymptotic variance is\n$$4\\cdot\\frac{\\theta^2}{12}=\\frac{\\theta^2}{3}.$$",
            "answer": "$$\\boxed{\\frac{\\theta^2}{3}}$$"
        }
    ]
}