{
    "hands_on_practices": [
        {
            "introduction": "The Weak Law of Large Numbers (WLLN) provides a foundational guarantee that a sample average will approach the true mean, but a natural follow-up question is: how quickly? This practice explores how we can use Chebyshev's inequality to translate this convergence into a practical tool for experimental design. By working through a quality control scenario , you will learn how to calculate the minimum sample size required to ensure that an estimate is within a certain tolerance with a specified probability.",
            "id": "1967293",
            "problem": "An automated precision-cutting machine operates in cycles. In each cycle, the machine's cutting head makes a small positional adjustment. Let the adjustment in the $i$-th cycle be represented by a random variable $X_i$. These adjustments are independent and identically distributed. The machine is calibrated so that each adjustment is either a positive displacement of $a$ or a negative displacement of $-a$ with equal probability.\n\nThe long-term stability of the machine depends on the average adjustment over many cycles being close to zero. A quality control standard requires that after $n$ cycles, the probability that the magnitude of the average adjustment, $\\frac{1}{n}\\sum_{i=1}^{n} X_i$, is greater than or equal to a certain tolerance $\\epsilon$ must be no more than a given value $\\delta$.\n\nGiven the following parameters:\n- Adjustment step size: $a = 0.5$ units.\n- Tolerance for the average adjustment: $\\epsilon = 0.02$ units.\n- Maximum allowable probability of exceeding tolerance: $\\delta = 0.01$.\n\nUsing a common probability inequality to establish a sufficient condition, determine the minimum number of cycles, $n$, required to meet this quality control standard. The final answer must be a single integer.",
            "solution": "Let $\\{X_{i}\\}_{i=1}^{n}$ be independent and identically distributed with $P(X_{i}=a)=P(X_{i}=-a)=\\frac{1}{2}$. Then\n$$\nE[X_{i}]=\\frac{a+(-a)}{2}=0,\\quad E[X_{i}^{2}]=\\frac{a^{2}+(-a)^{2}}{2}=a^{2},\n$$\nso\n$$\n\\text{Var}(X_{i})=E[X_{i}^{2}]-(E[X_{i}])^{2}=a^{2}.\n$$\nLet $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. By independence,\n$$\n\\text{Var}(\\bar{X}_{n})=\\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right)=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\text{Var}(X_{i})=\\frac{na^2}{n^2}=\\frac{a^{2}}{n}.\n$$\nBy Chebyshevâ€™s inequality, for any $\\epsilon0$,\n$$\nP\\left(\\left|\\bar{X}_{n}-E[\\bar{X}_{n}]\\right|\\geq \\epsilon\\right)\\leq \\frac{\\text{Var}(\\bar{X}_{n})}{\\epsilon^{2}}=\\frac{a^{2}}{n\\epsilon^{2}}.\n$$\nA sufficient condition to ensure $P(|\\bar{X}_{n}|\\geq \\epsilon)\\leq \\delta$ is therefore\n$$\n\\frac{a^{2}}{n\\epsilon^{2}}\\leq \\delta \\quad\\Longleftrightarrow\\quad n\\geq \\frac{a^{2}}{\\delta\\epsilon^{2}}.\n$$\nSubstituting $a=0.5$, $\\epsilon=0.02$, and $\\delta=0.01$ gives\n$$\nn\\geq \\frac{0.5^{2}}{0.01 \\cdot 0.02^{2}}=\\frac{0.25}{0.01 \\cdot 0.0004}=\\frac{0.25}{0.000004}=62500.\n$$\nSince $n$ must be an integer, the minimum number of cycles is $62500$.",
            "answer": "$$\\boxed{62500}$$"
        },
        {
            "introduction": "The power of the WLLN extends beyond the average of simple random variables; it also applies to the average of functions of those variables, a principle of immense utility in statistics. This exercise  demonstrates this concept by examining the convergence of the sample second moment, a quantity crucial for estimating variance. This practice reinforces the fundamental relationship between a variable's mean, its variance, and its second moment, showing how the WLLN underpins the estimation of more complex statistical properties.",
            "id": "1967327",
            "problem": "Consider a sequence of random variables $X_1, X_2, \\dots, X_n$ that are independent and identically distributed (i.i.d.). Each random variable $X_i$ in this sequence has a known finite mean $E[X_i] = \\mu$ and a known finite, positive variance $\\text{Var}(X_i) = \\sigma^2$.\n\nWe define the sample second moment about the origin for this sequence as:\n$$M_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i^2$$\nThis quantity is of interest in various fields, for instance, in physics where it might relate to the average energy of a system of particles.\n\nDetermine the value to which $M_n$ converges in probability as the sample size $n$ approaches infinity. Express your answer as a closed-form analytic expression in terms of $\\mu$ and $\\sigma$.",
            "solution": "The problem asks for the value to which the sample second moment, $M_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i^2$, converges in probability. This is a direct application of the Weak Law of Large Numbers (WLLN).\n\nThe WLLN states that for a sequence of independent and identically distributed (i.i.d.) random variables $Y_1, Y_2, \\dots$ with a finite expected value $E[Y_i] = \\mu_Y$, their sample mean $\\bar{Y}_n = \\frac{1}{n} \\sum_{i=1}^{n} Y_i$ converges in probability to $\\mu_Y$. We can write this as $\\bar{Y}_n \\xrightarrow{p} \\mu_Y$ as $n \\to \\infty$.\n\nTo apply the WLLN to our problem, let's define a new sequence of random variables $Y_i = X_i^2$. The quantity $M_n$ can then be rewritten as the sample mean of this new sequence:\n$$M_n = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}_n$$\n\nNow, we must check if the conditions for the WLLN are met for the sequence $Y_i$.\n1.  **I.I.D. Condition:** The problem states that the random variables $X_1, X_2, \\dots$ are i.i.d. Since each $Y_i$ is a function of the corresponding $X_i$ (specifically $Y_i = X_i^2$), and the function is the same for all $i$, the sequence of random variables $Y_1, Y_2, \\dots$ is also independent and identically distributed.\n\n2.  **Finite Mean Condition:** The WLLN requires that the expected value of $Y_i$, denoted $E[Y_i]$, is finite. Let's calculate this expectation.\n$$E[Y_i] = E[X_i^2]$$\nWe can relate $E[X_i^2]$ to the given mean and variance of $X_i$. The definition of variance is:\n$$\\text{Var}(X_i) = E[X_i^2] - (E[X_i])^2$$\nWe are given that $\\text{Var}(X_i) = \\sigma^2$ and $E[X_i] = \\mu$. Substituting these values into the variance formula gives:\n$$\\sigma^2 = E[X_i^2] - \\mu^2$$\nSolving for $E[X_i^2]$, we get:\n$$E[X_i^2] = \\mu^2 + \\sigma^2$$\nSince $\\mu$ and $\\sigma^2$ are given as finite, the expectation $E[Y_i] = \\mu^2 + \\sigma^2$ is also finite. Let's denote this common mean of the $Y_i$ sequence as $\\mu_Y = \\mu^2 + \\sigma^2$.\n\nSince both conditions for the WLLN are satisfied for the sequence $Y_i = X_i^2$, we can conclude that their sample mean, $M_n$, converges in probability to their true mean, $\\mu_Y$.\n$$M_n \\xrightarrow{p} E[Y_i] = \\mu^2 + \\sigma^2$$\n\nThus, the value to which $M_n$ converges in probability is $\\mu^2 + \\sigma^2$.",
            "answer": "$$\\boxed{\\mu^{2} + \\sigma^{2}}$$"
        },
        {
            "introduction": "The assumption that random variables are independent is critical for the sample average to converge to a constant mean. This advanced problem  challenges you to analyze a scenario with systemic correlation, where a common random factor influences all measurements. Discovering what the sample average converges to in this case reveals a profound distinction between averaging out independent noise and the persistence of shared, systemic effects, a common feature in fields from finance to environmental science.",
            "id": "1668567",
            "problem": "A large-scale environmental sensing network is deployed to monitor a certain physical quantity, $T$. The value of $T$ is not a fixed constant but fluctuates over time due to complex environmental dynamics. For the purpose of analysis, $T$ is modeled as a random variable with a well-defined but unknown mean $E[T] = \\mu$ and a finite, non-zero variance $\\text{Var}(T) = \\sigma_T^2$.\n\nThe network consists of $n$ identical sensors. The reading of the $i$-th sensor, denoted by $X_i$, is a sum of the true quantity $T$ and an independent internal noise term $\\epsilon_i$. Thus, $X_i = T + \\epsilon_i$. The noise terms $\\epsilon_1, \\epsilon_2, \\dots, \\epsilon_n$ are modeled as independent and identically distributed (i.i.d.) random variables, each with an expected value of $E[\\epsilon_i] = 0$ and a finite variance $\\text{Var}(\\epsilon_i) = \\sigma_\\epsilon^2$. Furthermore, the noise terms are independent of the physical quantity $T$.\n\nTo estimate the mean quantity $\\mu$, an engineer computes the sample average of all sensor readings:\n$$ \\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i $$\nWhile it is often assumed that such an average converges to the true mean $\\mu$ for large $n$, the common influence of $T$ on all sensors introduces a systemic correlation. Your task is to determine the limit of the sample average $\\bar{X}_n$ as the number of sensors $n$ approaches infinity. Find the value $L$ to which $\\bar{X}_n$ converges in probability, meaning that for any arbitrary small positive number $\\delta$, the probability $P(|\\bar{X}_n - L| \\geq \\delta)$ approaches zero as $n \\to \\infty$.\n\nExpress your answer for $L$ as an analytic expression in terms of the variables defined.",
            "solution": "We start from the given measurement model for each sensor,\n$$\nX_{i} = T + \\epsilon_{i},\n$$\nwhere $T$ is a random variable with $E[T] = \\mu$ and $\\text{Var}(T) = \\sigma_{T}^{2}$, and the $\\epsilon_{i}$ are i.i.d. with $E[\\epsilon_{i}] = 0$ and $\\text{Var}(\\epsilon_{i}) = \\sigma_{\\epsilon}^{2}$, and are independent of $T$.\n\nThe sample average over $n$ sensors is\n$$\n\\bar{X}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}.\n$$\nSubstituting the model into the average gives\n$$\n\\bar{X}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} \\left(T + \\epsilon_{i}\\right) = T + \\frac{1}{n}\\sum_{i=1}^{n} \\epsilon_{i}.\n$$\nDefine the noise average\n$$\n\\bar{\\epsilon}_{n} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} \\epsilon_{i}.\n$$\nThen\n$$\n\\bar{X}_{n} = T + \\bar{\\epsilon}_{n}.\n$$\n\nBy the Strong Law of Large Numbers (or the Weak Law, which suffices for convergence in probability), since the $\\epsilon_{i}$ are i.i.d. with finite mean $E[\\epsilon_{i}] = 0$, we have\n$$\n\\bar{\\epsilon}_{n} \\xrightarrow{p} 0 \\quad \\text{as } n \\to \\infty.\n$$\nTherefore, for any $\\delta  0$,\n$$\nP\\left(\\left|\\bar{X}_{n} - T\\right| \\geq \\delta\\right) \n= P\\left(\\left|T + \\bar{\\epsilon}_{n} - T\\right| \\geq \\delta\\right) \n= P\\left(\\left|\\bar{\\epsilon}_{n}\\right| \\geq \\delta\\right) \\to 0 \\quad \\text{as } n \\to \\infty.\n$$\nThis directly proves that\n$$\n\\bar{X}_{n} \\xrightarrow{p} T.\n$$\nEquivalently, the limit in probability $L$ is the random variable $T$ (not the constant $\\mu$), because the common term $T$ does not average out across sensors whereas the independent noises do. For completeness, note that $E[\\bar{X}_{n}] = \\mu$ for all $n$, and $\\text{Var}(\\bar{X}_{n}) = \\sigma_{T}^{2} + \\sigma_{\\epsilon}^{2}/n \\to \\sigma_{T}^{2}$, consistent with convergence in probability to $T$ rather than to a constant.",
            "answer": "$$\\boxed{T}$$"
        }
    ]
}