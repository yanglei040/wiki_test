{
    "hands_on_practices": [
        {
            "introduction": "The Strong Law of Large Numbers (SLLN) provides a powerful link between theoretical probability and real-world observation. This first exercise demonstrates the law in its most direct form, calculating the long-term average for a sequence of independent and identically distributed (i.i.d.) random variables . By working through this example, you will solidify your understanding of how the sample mean almost surely converges to the expected value of the underlying distribution.",
            "id": "1460774",
            "problem": "A research team is studying the output of a novel signal processor. Each time the processor runs, it generates a normalized value, which can be modeled as a random variable. A sequence of these values, $X_1, X_2, X_3, \\dots$, is recorded. These values are considered to be independent and identically distributed (i.i.d.) random variables. The theoretical model for the probability distribution of any single value $X_i$ is described by the probability density function (PDF):\n$$\nf(x) = \\begin{cases} 3x^2  \\text{for } 0 \\le x \\le 1 \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nThe long-term average of these measurements after $n$ trials is given by the sample mean $S_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$. As the number of trials $n$ grows infinitely large, the value of $S_n$ will converge almost surely to a specific constant.\n\nDetermine the value of this constant. Express your answer as a fraction in simplest form.",
            "solution": "We are given independent and identically distributed random variables $X_{1},X_{2},\\dots$ with common density\n$$\nf(x)=\\begin{cases}\n3x^{2},  0\\le x\\le 1,\\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\nFirst, verify that $f$ is a valid probability density by checking normalization:\n$$\n\\int_{-\\infty}^{\\infty} f(x)\\,dx=\\int_{0}^{1}3x^{2}\\,dx=3\\left[\\frac{x^{3}}{3}\\right]_{0}^{1}=1.\n$$\nBy the Strong Law of Large Numbers, since the $X_{i}$ are i.i.d. with finite mean $E[|X_{1}|]\\infty$ (which holds because $0\\le X_{1}\\le 1$ almost surely), the sample mean\n$$\nS_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\n$$\nconverges almost surely to $E[X_{1}]$ as $n\\to\\infty$.\n\nCompute the expectation:\n$$\nE[X_{1}]=\\int_{-\\infty}^{\\infty} x f(x)\\,dx=\\int_{0}^{1} x\\cdot 3x^{2}\\,dx=3\\int_{0}^{1} x^{3}\\,dx=3\\left[\\frac{x^{4}}{4}\\right]_{0}^{1}=\\frac{3}{4}.\n$$\nTherefore,\n$$\n\\lim_{n\\to\\infty} S_{n}=\\frac{3}{4}\\quad \\text{almost surely}.\n$$",
            "answer": "$$\\boxed{\\frac{3}{4}}$$"
        },
        {
            "introduction": "In many scientific and engineering applications, we are interested not in the raw data itself, but in a function of that data. This practice problem explores how the Strong Law of Large Numbers extends to such cases . You will see that by creating a new sequence of transformed variables, the SLLN can be readily applied, showcasing the law's versatility in data analysis.",
            "id": "1957052",
            "problem": "A research team is studying the energy emissions from a newly discovered type of microscopic organism. The energy emitted by a single organism during a short observation period is a random variable. The team conducts a series of independent experiments, with each experiment measuring the energy emission, $X_i$, from a new, randomly selected organism. These measurements, $X_1, X_2, X_3, \\dots$, can be modeled as a sequence of independent and identically distributed (i.i.d.) positive random variables. It is known from theoretical models that the expected energy emission is finite, i.e., $E[X_i] = \\mu$ for some finite constant $\\mu > 0$.\n\nFor a data processing task, an analyst is not interested in the energy itself, but in a related quantity, $Y_i = \\sqrt{X_i}$. The analyst computes the sample average of these transformed quantities over $n$ experiments:\n$$S_n = \\frac{1}{n}\\sum_{i=1}^n Y_i = \\frac{1}{n}\\sum_{i=1}^n \\sqrt{X_i}$$\nAs the number of experiments $n$ grows infinitely large, what value does the sample average $S_n$ converge to almost surely? Express your answer in terms of the random variable $X_1$.",
            "solution": "The problem asks for the almost sure limit of the sample average $S_n = \\frac{1}{n}\\sum_{i=1}^n \\sqrt{X_i}$ as $n \\to \\infty$.\n\nLet's define a new sequence of random variables $Y_i = \\sqrt{X_i}$ for $i=1, 2, 3, \\dots$. Since the random variables $X_1, X_2, \\dots$ are independent and identically distributed (i.i.d.), the random variables $Y_1 = \\sqrt{X_1}, Y_2 = \\sqrt{X_2}, \\dots$ are also independent and identically distributed. This is because any function of i.i.d. random variables results in a new sequence of i.i.d. random variables.\n\nThe sample average can be rewritten in terms of the $Y_i$ variables:\n$$S_n = \\frac{1}{n}\\sum_{i=1}^n Y_i$$\nThis is the sample mean of the i.i.d. sequence $Y_i$.\n\nWe can apply the Strong Law of Large Numbers (SLLN) to the sequence $\\{Y_i\\}$. The SLLN states that if $Y_1, Y_2, \\dots$ is a sequence of i.i.d. random variables with a finite expected value, i.e., $E[|Y_1|]  \\infty$, then the sample mean converges almost surely to the true mean:\n$$\\frac{1}{n}\\sum_{i=1}^n Y_i \\xrightarrow{\\text{a.s.}} E[Y_1] \\quad \\text{as } n \\to \\infty$$\n\nTo use the SLLN, we must verify that $E[|Y_1|]$ is finite.\nWe have $Y_1 = \\sqrt{X_1}$. The problem states that the $X_i$ are positive random variables, so $X_1 > 0$. This implies that $\\sqrt{X_1}$ is a real and positive value, so $|Y_1| = |\\sqrt{X_1}| = \\sqrt{X_1}$. We need to check if $E[\\sqrt{X_1}]$ is finite.\n\nWe can use Jensen's inequality to relate $E[\\sqrt{X_1}]$ to the known finite mean $E[X_1] = \\mu$. Jensen's inequality for a concave function $g(x)$ and a random variable $X$ states that $E[g(X)] \\le g(E[X])$.\n\nThe function $g(x) = \\sqrt{x}$ is a concave function for $x > 0$. This can be shown by checking its second derivative:\n$g'(x) = \\frac{1}{2}x^{-1/2}$\n$g''(x) = -\\frac{1}{4}x^{-3/2}$\nSince $x > 0$, we have $g''(x)  0$, which confirms that $g(x) = \\sqrt{x}$ is strictly concave.\n\nApplying Jensen's inequality with $g(x) = \\sqrt{x}$ and the random variable $X_1$, we get:\n$$E[\\sqrt{X_1}] \\le \\sqrt{E[X_1]}$$\nThe problem states that $E[X_1] = \\mu$, and $\\mu$ is a finite positive constant. Therefore,\n$$E[\\sqrt{X_1}] \\le \\sqrt{\\mu}$$\nSince $\\mu$ is finite, $\\sqrt{\\mu}$ is also finite. This shows that $E[\\sqrt{X_1}]$ is finite.\nBecause $Y_1 = \\sqrt{X_1}$ is a positive random variable, we have $E[|Y_1|] = E[Y_1] = E[\\sqrt{X_1}]$, which is finite.\n\nThe condition for the SLLN is satisfied for the sequence $\\{Y_i\\}$. Therefore, we can conclude that the sample average $S_n$ converges almost surely to the expectation of $Y_1$:\n$$\\lim_{n \\to \\infty} S_n = \\lim_{n \\to \\infty} \\frac{1}{n}\\sum_{i=1}^n Y_i \\xrightarrow{\\text{a.s.}} E[Y_1]$$\nSubstituting back $Y_1 = \\sqrt{X_1}$, we find the limit:\n$$\\lim_{n \\to \\infty} S_n \\xrightarrow{\\text{a.s.}} E[\\sqrt{X_1}]$$\nThe question asks for the value to which $S_n$ converges almost surely. This value is $E[\\sqrt{X_1}]$.",
            "answer": "$$\\boxed{E[\\sqrt{X_1}]}$$"
        },
        {
            "introduction": "While the classic SLLN is stated for i.i.d. random variables, its principles can be cleverly extended to certain types of dependent sequences. This problem challenges you to analyze a sequence where each term depends on its predecessor, a common structure in time-series data . By decomposing the sequence, you will learn a powerful technique for applying the logic of the SLLN even when the i.i.d. assumption is not strictly met.",
            "id": "1460782",
            "problem": "Consider a sequence of independent and identically distributed (i.i.d.) random variables $\\{X_n\\}_{n=1}^{\\infty}$ defined on a common probability space. These variables are characterized by a mean of $E[X_n] = 0$ and a second moment of $E[X_n^2] = 1$ for all $n \\ge 1$.\n\nWe form a new sequence of sample averages defined as $A_n = \\frac{1}{n} \\sum_{i=1}^{n-1} X_i X_{i+1}$ for $n \\ge 2$.\n\nDetermine the value to which the sequence $A_n$ converges almost surely as $n$ approaches infinity.",
            "solution": "Define $Y_{i} = X_{i}X_{i+1}$ for $i \\ge 1$. Then $A_{n} = \\frac{1}{n} \\sum_{i=1}^{n-1} Y_{i}$ for $n \\ge 2$. By independence of the $X_{i}$ and identical distribution, each $Y_{i}$ has the same distribution, with\n$$\nE[Y_{i}] = E[X_{i}X_{i+1}] = E[X_{i}]\\,E[X_{i+1}] = 0,\n$$\nand\n$$\nE[Y_{i}^{2}] = E[X_{i}^{2}X_{i+1}^{2}] = E[X_{i}^{2}]\\,E[X_{i+1}^{2}] = 1.\n$$\nHence $E[|Y_{i}|] \\le \\sqrt{E[Y_{i}^{2}]} = 1$ by Cauchyâ€“Schwarz, so the $Y_{i}$ are integrable.\n\nNext, observe the dependence structure: the family $\\{Y_{i}\\}_{i \\ge 1}$ is $1$-dependent, and more specifically the subsequences $\\{Y_{2j-1}\\}_{j \\ge 1}$ and $\\{Y_{2j}\\}_{j \\ge 1}$ consist of independent random variables because they depend on disjoint sets of the independent $X_{i}$. Moreover, each subsequence is identically distributed with mean $0$ and finite second moment. By the Kolmogorov strong law of large numbers applied separately to each subsequence,\n$$\n\\frac{1}{k} \\sum_{j=1}^{k} Y_{2j-1} \\to 0 \\quad \\text{a.s.}, \n\\qquad\n\\frac{1}{k} \\sum_{j=1}^{k} Y_{2j} \\to 0 \\quad \\text{a.s.}\n$$\n\nFor each $n \\ge 2$, let $N_{o}(n)$ be the number of odd indices in $\\{1,\\dots,n-1\\}$ and $N_{e}(n)$ the number of even indices in $\\{1,\\dots,n-1\\}$. Then $N_{o}(n) + N_{e}(n) = n-1$, with $N_{o}(n) = \\lceil (n-1)/2 \\rceil$ and $N_{e}(n) = \\lfloor (n-1)/2 \\rfloor$, so $N_{o}(n)/n \\to \\frac{1}{2}$ and $N_{e}(n)/n \\to \\frac{1}{2}$ deterministically as $n \\to \\infty$. Decompose\n$$\nA_{n} \n= \\frac{1}{n} \\sum_{i=1}^{n-1} Y_{i}\n= \\frac{N_{o}(n)}{n} \\left( \\frac{1}{N_{o}(n)} \\sum_{j=1}^{N_{o}(n)} Y_{2j-1} \\right)\n+ \\frac{N_{e}(n)}{n} \\left( \\frac{1}{N_{e}(n)} \\sum_{j=1}^{N_{e}(n)} Y_{2j} \\right).\n$$\nAs $n \\to \\infty$, we have $N_{o}(n) \\to \\infty$ and $N_{e}(n) \\to \\infty$, so by the almost sure limits of the subsequence averages and the deterministic limits of the weights, it follows that $A_{n} \\to 0$ almost surely.\n\nTherefore, the sequence $A_{n}$ converges almost surely to $0$.",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}