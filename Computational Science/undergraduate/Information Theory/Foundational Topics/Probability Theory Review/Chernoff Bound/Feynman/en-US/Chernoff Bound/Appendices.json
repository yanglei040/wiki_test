{
    "hands_on_practices": [
        {
            "introduction": "Understanding the probability of rare events is crucial in many engineering disciplines. This first practice places us in the role of a cybersecurity engineer evaluating a firewall's false alarm rate. By comparing the Chernoff bound with more elementary inequalities like those of Markov and Chebyshev, you will gain a hands-on appreciation for why exponential bounds are indispensable for obtaining meaningful estimates on the tails of a distribution.",
            "id": "1610102",
            "problem": "A cybersecurity firm is evaluating the false-alarm rate of its new network firewall. During a test, a stream of $N = 20,000$ packets, all known to be benign, is sent through the system. The firewall's analysis algorithm has a known false positive probability: any individual benign packet is incorrectly flagged as malicious with a probability of $p = 0.1$.\n\nThe firewall employs an automated lockdown mechanism. If the total number of packets flagged as malicious in the stream exceeds a threshold of $k=2,500$, the entire network is locked down. Let $X$ be the random variable representing the total number of flagged packets.\n\nThe engineers need to find a rigorous upper bound on the probability of a false alarm, $P(X \\ge 2,500)$. They have computed several possible upper bounds using different concentration inequalities. Which of the following values represents the tightest valid upper bound that can be derived using standard concentration inequalities (such as Markov's, Chebyshev's, or Chernoff's inequality)?\n\nA. $8.00 \\times 10^{-1}$\n\nB. $7.20 \\times 10^{-3}$\n\nC. $6.01 \\times 10^{-26}$\n\nD. $2.14 \\times 10^{-56}$",
            "solution": "Let $X \\sim \\mathrm{Binomial}(n,p)$ with $n=20000$ and $p=0.1$. Then\n$$\n\\mu = \\mathbb{E}[X] = np = 2000, \\quad \\mathrm{Var}(X) = np(1-p) = 1800.\n$$\nWe seek an upper bound for $P(X \\ge 2500)$, i.e., for the upper tail at $k=2500$. We can write $k = \\mu+500$. Define the relative deviation\n$$\n\\delta = \\frac{k-\\mu}{\\mu} = \\frac{500}{2000} = \\frac{1}{4}.\n$$\n\nMarkov's inequality for nonnegative $X$ gives\n$$\nP(X \\ge 2500) \\le \\frac{\\mathbb{E}[X]}{2500} = \\frac{2000}{2500} = \\frac{4}{5} = 8.00 \\times 10^{-1}.\n$$\n\nChebyshev's inequality gives\n$$\nP(|X-\\mu| \\ge 500) \\le \\frac{\\mathrm{Var}(X)}{500^{2}} = \\frac{1800}{250000} = 7.20 \\times 10^{-3}.\n$$\n\nA standard Chernoff (multiplicative) bound for sums of independent Bernoulli variables states that for $\\delta>0$,\n$$\nP(X \\ge (1+\\delta)\\mu) \\le \\left(\\frac{\\exp(\\delta)}{(1+\\delta)^{1+\\delta}}\\right)^{\\mu}\n= \\exp\\left(-\\mu\\left[(1+\\delta)\\ln(1+\\delta)-\\delta\\right]\\right).\n$$\nWith $\\mu=2000$ and $\\delta=\\frac{1}{4}$,\n$$\n(1+\\delta)\\ln(1+\\delta)-\\delta = \\frac{5}{4}\\ln\\left(\\frac{5}{4}\\right) - \\frac{1}{4},\n$$\nso\n$$\nP(X \\ge 2500) \\le \\exp\\left(-2000\\left[\\frac{5}{4}\\ln\\left(\\frac{5}{4}\\right) - \\frac{1}{4}\\right]\\right).\n$$\nNumerically,\n$$\n\\frac{5}{4}\\ln\\left(\\frac{5}{4}\\right) - \\frac{1}{4} \\approx 0.028929439,\\quad\n2000 \\times 0.028929439 \\approx 57.858878,\n$$\nhence\n$$\nP(X \\ge 2500) \\lesssim \\exp(-57.858878) \\approx 7.45 \\times 10^{-26}.\n$$\n\nComparing the three standard bounds:\n- Markov: $8.00 \\times 10^{-1}$ (option A),\n- Chebyshev: $7.20 \\times 10^{-3}$ (option B),\n- Chernoff: on the order of $10^{-26}$, consistent with option C $6.01 \\times 10^{-26}$ and far tighter than option D $2.14 \\times 10^{-56}$.\n\nTherefore, among the listed values, the tightest valid upper bound obtainable via standard concentration inequalities is the Chernoff-type bound corresponding to option C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Deviations from an expected value can be concerning whether they are surprisingly high or surprisingly low. This exercise shifts our focus from an upper tail to a lower tail event within a quality control context. We will apply the Chernoff bound to calculate the probability of a batch of products passing inspection by \"getting lucky\"â€”that is, having a sample defect rate that is misleadingly better than the true underlying process quality.",
            "id": "1610148",
            "problem": "A state-of-the-art semiconductor fabrication plant produces advanced processors for use in high-performance computing. Due to the extreme complexity of the manufacturing process, there is a non-zero probability of defects. The established baseline for the process indicates that any single processor has a probability $p = 0.05$ of containing a critical flaw, independent of other processors. A standard production run consists of a batch of $n=10,000$ processors.\n\nAs part of the quality assurance protocol, a batch is approved for shipping if the observed fraction of defective processors within that batch is at most $0.04$. The company wants to quantify the risk of a statistically fortunate batch (one with a deceptively low number of observed defects) passing inspection, despite the underlying process quality.\n\nCalculate a numerical upper bound for the probability that a randomly selected batch passes this quality inspection. To do this, you should apply a standard multiplicative form of the Chernoff bound for the lower tail of a sum of independent random variables. Express your final answer as a number rounded to three significant figures.",
            "solution": "Let $X_{i}$ be the indicator that processor $i$ is defective, with $X_{i} \\sim \\text{Bernoulli}(p)$, $p=0.05$, independently for $i=1,\\dots,n$, where $n=10000$. Let $X=\\sum_{i=1}^{n} X_{i}$ be the total number of defective processors in the batch. Then the mean is $\\mu=\\mathbb{E}[X]=np=10000 \\cdot 0.05=500$.\n\nThe batch passes if the observed defect fraction is at most $0.04$, i.e., if $X \\leq 0.04n=400$. This is a lower-tail event relative to the mean $\\mu=500$. Define $\\delta \\in [0,1]$ by $(1-\\delta)\\mu=400$. Since $\\mu=0.05n$, we compute\n$$\n\\delta \\;=\\; 1 - \\frac{400}{\\mu} \\;=\\; 1 - \\frac{0.04n}{0.05n} \\;=\\; 1 - \\frac{4}{5} \\;=\\; \\frac{1}{5}.\n$$\n\nBy the standard multiplicative Chernoff bound for the lower tail of a sum of independent Bernoulli random variables, for $0 \\leq \\delta \\leq 1$,\n$$\n\\mathbb{P}\\!\\left(X \\leq (1-\\delta)\\mu\\right) \\;\\leq\\; \\exp\\!\\left(-\\frac{\\mu \\delta^{2}}{2}\\right).\n$$\nSubstituting $\\mu=500$ and $\\delta=\\frac{1}{5}$,\n$$\n\\mathbb{P}(X \\leq 400) \\;\\leq\\; \\exp\\!\\left(-\\frac{500 \\cdot (1/5)^{2}}{2}\\right)\n\\;=\\; \\exp\\!\\left(-\\frac{500 \\cdot \\frac{1}{25}}{2}\\right)\n\\;=\\; \\exp(-10).\n$$\n\nNumerically, $\\exp(-10) \\approx 4.539992976 \\times 10^{-5}$, which rounded to three significant figures is $4.54 \\times 10^{-5}$.",
            "answer": "$$\\boxed{4.54 \\times 10^{-5}}$$"
        },
        {
            "introduction": "The Chernoff bound is more than just a calculation tool; it reveals a deep connection between probability theory and information theory. This problem presents a classic hypothesis testing scenario: deciding if a system is operating normally or is in a faulty state based on observed data. In deriving the tightest possible Chernoff bound for this task, you will discover its fundamental relationship with the Kullback-Leibler (KL) divergence, a core concept for measuring the \"distance\" between probability distributions.",
            "id": "1610163",
            "problem": "An automated quality control system for a manufacturing process performs a series of $n$ independent tests on each product. The outcome of each test is binary, either a success (0) or a failure (1). Let the outcomes be denoted by the random variables $X_1, X_2, \\dots, X_n$, which are assumed to be independent and identically distributed Bernoulli random variables with a parameter $p$, representing the true failure probability.\n\nThe system is designed to distinguish between two states of the manufacturing line:\n1.  Normal operation (Hypothesis $H_0$): The failure probability is $p_0$.\n2.  Faulty operation (Hypothesis $H_1$): The failure probability is $p_1$, with $p_1 > p_0$.\n\nA decision is made based on the observed sample mean of failures, $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$. If $\\bar{X}$ is greater than or equal to a certain threshold $\\alpha$, the system flags the operation as faulty. Otherwise, it is considered normal. This event, flagging a normal operation as faulty, is termed a \"false alarm\".\n\nGiven the following parameters:\n-   Number of tests: $n = 1000$\n-   Failure probability under normal operation: $p_0 = 0.10$\n-   Decision threshold: $\\alpha = 0.15$\n\nCalculate a tight exponential upper bound for the probability of a false alarm. Round your final answer to three significant figures.",
            "solution": "The problem asks for an upper bound on the probability of a false alarm. A false alarm occurs when the true state is normal operation ($H_0$, with failure probability $p_0$), but the system decides the operation is faulty. This decision is made if the sample mean of failures $\\bar{X}$ is greater than or equal to the threshold $\\alpha$. We want to find an upper bound for $P(\\bar{X} \\ge \\alpha)$ under the assumption that the data $X_i$ are generated from a Bernoulli($p_0$) distribution.\n\nLet $S_n = \\sum_{i=1}^n X_i$ be the total number of failures. The condition $\\bar{X} \\ge \\alpha$ is equivalent to $\\frac{S_n}{n} \\ge \\alpha$, or $S_n \\ge n\\alpha$. We are looking for an upper bound on $P(S_n \\ge n\\alpha)$.\n\nWe use the Chernoff bound method. For any parameter $s > 0$, we can write:\n$$P(S_n \\ge n\\alpha) = P(\\exp(sS_n) \\ge \\exp(sn\\alpha)) \\le \\frac{E[\\exp(sS_n)]}{\\exp(sn\\alpha)}$$\nSince the $X_i$ are i.i.d., $E[\\exp(sS_n)] = (E[\\exp(sX)])^n$. The moment generating function (MGF) of a single Bernoulli($p_0$) random variable is:\n$$M_X(s) = E[\\exp(sX)] = (1-p_0) \\cdot \\exp(s \\cdot 0) + p_0 \\cdot \\exp(s \\cdot 1) = 1 - p_0 + p_0 \\exp(s)$$\nSubstituting this back, we get:\n$$P(S_n \\ge n\\alpha) \\le \\frac{(1 - p_0 + p_0 \\exp(s))^n}{\\exp(sn\\alpha)} = \\left(\\frac{1 - p_0 + p_0 \\exp(s)}{\\exp(s\\alpha)}\\right)^n$$\nTo obtain the tightest bound, we must find the value of $s$ that minimizes the base of the exponent, $g(s) = (1 - p_0 + p_0 \\exp(s))\\exp(-s\\alpha)$. We minimize $g(s)$ by taking its derivative with respect to $s$ and setting it to zero.\n$$\\frac{dg}{ds} = (p_0\\exp(s))\\exp(-s\\alpha) + (1 - p_0 + p_0\\exp(s))(-\\alpha\\exp(-s\\alpha)) = 0$$\nSince $\\exp(-s\\alpha) > 0$, we can divide by it and rearrange the terms to solve for $\\exp(s)$:\n$$p_0\\exp(s) - \\alpha(1 - p_0 + p_0\\exp(s)) = 0$$\n$$p_0\\exp(s) - \\alpha p_0\\exp(s) = \\alpha(1 - p_0)$$\n$$\\exp(s)p_0(1 - \\alpha) = \\alpha(1 - p_0)$$\n$$\\exp(s) = \\frac{\\alpha(1-p_0)}{p_0(1-\\alpha)}$$\nThe optimal $s$ is $s^* = \\ln\\left(\\frac{\\alpha(1-p_0)}{p_0(1-\\alpha)}\\right)$.\n\nPlugging this optimal $s^*$ back into the bound gives the tightest exponential bound of this form, which is $\\exp(-n D(\\alpha || p_0))$, where $D(\\alpha || p_0)$ is the Kullback-Leibler (KL) divergence between Bernoulli distributions with parameters $\\alpha$ and $p_0$. The formula for the KL divergence is:\n$$D(\\alpha || p_0) = \\alpha \\ln\\left(\\frac{\\alpha}{p_0}\\right) + (1-\\alpha) \\ln\\left(\\frac{1-\\alpha}{1-p_0}\\right)$$\nSo, the tightest upper bound is $P(\\bar{X} \\ge \\alpha) \\le \\exp(-n D(\\alpha || p_0))$.\n\nNow, we substitute the given numerical values: $n=1000$, $p_0=0.10$, and $\\alpha=0.15$.\nFirst, calculate the KL divergence, $D(0.15 || 0.10)$:\n$$D(0.15 || 0.10) = 0.15 \\ln\\left(\\frac{0.15}{0.10}\\right) + (1-0.15) \\ln\\left(\\frac{1-0.15}{1-0.10}\\right)$$\n$$D(0.15 || 0.10) = 0.15 \\ln(1.5) + 0.85 \\ln\\left(\\frac{0.85}{0.90}\\right)$$\nUsing a calculator for the natural logarithms:\n$\\ln(1.5) \\approx 0.405465$\n$\\ln(0.85/0.90) = \\ln(17/18) \\approx -0.057158$\n$$D(0.15 || 0.10) \\approx 0.15 \\times 0.405465 + 0.85 \\times (-0.057158)$$\n$$D(0.15 || 0.10) \\approx 0.060820 - 0.048584 \\approx 0.012236$$\nNow, we calculate the final bound:\n$$\\text{Bound} = \\exp(-n D(\\alpha || p_0)) = \\exp(-1000 \\times 0.012236) = \\exp(-12.236)$$\n$$\\exp(-12.236) \\approx 4.854 \\times 10^{-6}$$\nRounding to three significant figures, we get $4.86 \\times 10^{-6}$.",
            "answer": "$$\\boxed{4.86 \\times 10^{-6}}$$"
        }
    ]
}