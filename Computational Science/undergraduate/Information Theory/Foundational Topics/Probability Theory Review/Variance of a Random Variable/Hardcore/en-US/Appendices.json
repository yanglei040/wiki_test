{
    "hands_on_practices": [
        {
            "introduction": "Understanding the variance of a dataset is a foundational skill, but data rarely stays in its raw form. This first exercise  explores how variance behaves when we apply a linear transformationâ€”a common operation for normalizing or scaling data. You will practice the two key steps: first, calculating the variance from the raw moments of a variable using the formula $\\text{Var}(X) = E[X^2] - (E[X])^2$, and second, determining the variance of the newly transformed variable.",
            "id": "1409797",
            "problem": "A data scientist is analyzing the output of a machine learning model designed to predict customer engagement. The model generates a raw, uncalibrated numerical score for each customer profile, represented by a random variable $X$. After analyzing a large sample of scores, the data scientist determines that the expected value (mean) of these scores is $E[X] = 2$, and the expected value of the square of the scores is $E[X^2] = 13$.\n\nTo make these scores more interpretable and align them with a business-defined scale, a linear transformation is applied to create a new score, $Y$. The transformation is given by the equation $Y = \\frac{1}{2}X + 8$.\n\nCalculate the variance of the transformed scores, $\\text{Var}(Y)$. Your answer should be a numerical value.",
            "solution": "The goal is to compute the variance of the transformed random variable $Y$, which is defined as $Y = \\frac{1}{2}X + 8$. We are given the first two moments of the original random variable $X$: $E[X] = 2$ and $E[X^2] = 13$.\n\nFirst, we need to calculate the variance of the original random variable, $X$. The variance of a random variable is defined by the formula:\n$$ \\text{Var}(X) = E[X^2] - (E[X])^2 $$\nSubstituting the given values into this formula:\n$$ \\text{Var}(X) = 13 - (2)^2 = 13 - 4 = 9 $$\n\nNext, we use the properties of variance for a linear transformation. For a random variable $X$ and constants $a$ and $b$, the variance of the transformed variable $Y = aX + b$ is given by:\n$$ \\text{Var}(Y) = \\text{Var}(aX + b) = a^2 \\text{Var}(X) $$\nNote that the additive constant $b$ does not affect the variance, as variance is a measure of spread, and adding a constant shifts the distribution without changing its spread.\n\nIn our problem, the transformation is $Y = \\frac{1}{2}X + 8$. By comparing this to the general form $Y = aX + b$, we can identify the constants as $a = \\frac{1}{2}$ and $b = 8$.\n\nNow, we can apply the variance property using our value for $a$ and the previously calculated $\\text{Var}(X)$:\n$$ \\text{Var}(Y) = \\left(\\frac{1}{2}\\right)^2 \\text{Var}(X) $$\n$$ \\text{Var}(Y) = \\frac{1}{4} \\times 9 $$\n$$ \\text{Var}(Y) = \\frac{9}{4} $$\n\nFinally, we express the result as a numerical value in decimal form:\n$$ \\text{Var}(Y) = 2.25 $$",
            "answer": "$$\\boxed{2.25}$$"
        },
        {
            "introduction": "How does variability accumulate? This practice  presents a thought-provoking scenario comparing two ways of processing a signal: amplifying a single source versus adding two independent sources. This problem will challenge your intuition and solidify your understanding of how variance scales, $\\text{Var}(aX) = a^2\\text{Var}(X)$, versus how it sums for independent random variables, $\\text{Var}(X_A + X_B) = \\text{Var}(X_A) + \\text{Var}(X_B)$. This is a distinction with profound practical consequences in signal processing and systems analysis.",
            "id": "1409813",
            "problem": "In a signal processing application, the noise associated with a measurement is modeled as a random variable $X$. This noise has an expected value $E[X]=0$ and a known, non-zero variance $\\text{Var}(X) = \\sigma^2$. Two different techniques are proposed to process the signal, which in turn affect the resulting noise.\n\n- **Technique 1:** A single measurement is performed, and the resulting value is digitally multiplied by 2. The new noise variable is $Y_1 = 2X$.\n- **Technique 2:** Two independent measurements are performed. The noises from these measurements are random variables $X_A$ and $X_B$, which are independent and identically distributed, both following the same distribution as $X$. The results are then added together. The new noise variable is $Y_2 = X_A + X_B$.\n\nWhich of the following statements correctly compares the variance of the resulting noise from the two techniques?\n\nA. The variance of $Y_1$ is greater than the variance of $Y_2$.\n\nB. The variance of $Y_2$ is greater than the variance of $Y_1$.\n\nC. The variances of $Y_1$ and $Y_2$ are equal.\n\nD. The comparison depends on the specific probability distribution of $X$ (e.g., Normal, Uniform).\n\nE. The comparison cannot be made without the numerical value of $\\sigma^2$.",
            "solution": "We are given a random variable $X$ with $E[X]=0$ and $\\text{Var}(X)=\\sigma^{2}$, where $\\sigma^{2}>0$. Two techniques create new noise variables $Y_{1}$ and $Y_{2}$, and we compare their variances.\n\nFirst, use the scaling property of variance: for any constant $a$ and any random variable with finite variance,\n$$\n\\text{Var}(aX)=E\\big[(aX-E[aX])^{2}\\big]=E\\big[(a(X-E[X]))^{2}\\big]=a^{2}E\\big[(X-E[X])^{2}\\big]=a^{2}\\text{Var}(X).\n$$\nApplying this to Technique 1 with $Y_{1}=2X$,\n$$\n\\text{Var}(Y_{1})=\\text{Var}(2X)=4\\text{Var}(X)=4\\sigma^{2}.\n$$\n\nNext, consider Technique 2 with $Y_{2}=X_{A}+X_{B}$, where $X_{A}$ and $X_{B}$ are independent and identically distributed as $X$. Using the variance of a sum,\n$$\n\\text{Var}(Y_{2})=\\text{Var}(X_{A}+X_{B})=\\text{Var}(X_{A})+\\text{Var}(X_{B})+2\\text{Cov}(X_{A},X_{B}).\n$$\nIndependence implies $\\text{Cov}(X_{A},X_{B})=0$, and since both have variance $\\sigma^{2}$,\n$$\n\\text{Var}(Y_{2})=\\sigma^{2}+\\sigma^{2}=2\\sigma^{2}.\n$$\n\nComparing the two,\n$$\n\\text{Var}(Y_{1})=4\\sigma^{2} \\quad \\text{and} \\quad \\text{Var}(Y_{2})=2\\sigma^{2},\n$$\nand because $\\sigma^{2}>0$, it follows that $4\\sigma^{2}>2\\sigma^{2}$. Therefore, the variance of $Y_{1}$ is greater than the variance of $Y_{2}$, which corresponds to option A. This comparison does not depend on the specific distribution of $X$, only on the variance and independence, and does not require the numerical value of $\\sigma^{2}$.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "In information theory, we often analyze processes that unfold over time, such as waiting for a specific signal to arrive. This exercise  models such a scenario using the geometric distribution, which describes the 'waiting time' for the first success in a series of independent trials. By calculating the variance of this waiting time, you will learn how to quantify the uncertainty or unpredictability inherent in memoryless information sources, a key concept in characterizing communication channels and data streams.",
            "id": "1667144",
            "problem": "Consider a memoryless binary source that generates a sequence of bits. The probability of generating a '1' is $p$, and the probability of generating a '0' is $1-p$, with $0 < p < 1$. The generation of each bit is an independent event. Let the random variable $N$ represent the number of bits that must be observed from the source until the first '1' appears. For example, if the sequence is '001...', then $N=3$. Determine the variance of $N$, denoted as $\\text{Var}(N)$, as a function of $p$.",
            "solution": "The source emits independent Bernoulli trials with success probability $p$ for symbol $1$. Let $N$ be the trial index of the first success. Then $N$ follows the geometric distribution on $\\{1,2,3,\\ldots\\}$ with probability mass function\n$$\n\\Pr(N=n)= (1-p)^{n-1}p,\\quad n=1,2,\\ldots\n$$\nLet $r=1-p$ so that $0<r<1$. We will compute $E[N]$ and $E[N^{2}]$ using series identities derived from the geometric series. Start with\n$$\n\\sum_{n=0}^{\\infty} r^{n}=\\frac{1}{1-r}.\n$$\nDifferentiate both sides with respect to $r$ to obtain\n$$\n\\sum_{n=1}^{\\infty} n r^{\\,n-1}=\\frac{1}{(1-r)^{2}}.\n$$\nDifferentiate once more:\n$$\n\\sum_{n=2}^{\\infty} n(n-1) r^{\\,n-2}=\\frac{2}{(1-r)^{3}}.\n$$\nMultiplying the last equation by $r$ and adding the first derivative identity gives\n$$\n\\sum_{n=2}^{\\infty} n(n-1) r^{\\,n-1}=\\frac{2r}{(1-r)^{3}},\\quad \\sum_{n=1}^{\\infty} n r^{\\,n-1}=\\frac{1}{(1-r)^{2}}.\n$$\nNote that $n^{2}=n(n-1)+n$, hence\n$$\n\\sum_{n=1}^{\\infty} n^{2} r^{\\,n-1}=\\sum_{n=1}^{\\infty} n(n-1) r^{\\,n-1}+\\sum_{n=1}^{\\infty} n r^{\\,n-1}=\\frac{2r}{(1-r)^{3}}+\\frac{1}{(1-r)^{2}}=\\frac{1+r}{(1-r)^{3}}.\n$$\n\nNow compute the moments of $N$:\n$$\nE[N]=\\sum_{n=1}^{\\infty} n\\,(1-p)^{n-1}p=p\\sum_{n=1}^{\\infty} n r^{\\,n-1}=p\\cdot \\frac{1}{(1-r)^{2}}=\\frac{p}{p^{2}}=\\frac{1}{p}.\n$$\nSimilarly,\n$$\nE[N^{2}]=\\sum_{n=1}^{\\infty} n^{2}\\,(1-p)^{n-1}p=p\\sum_{n=1}^{\\infty} n^{2} r^{\\,n-1}=p\\cdot \\frac{1+r}{(1-r)^{3}}=p\\cdot \\frac{1+(1-p)}{p^{3}}=\\frac{2-p}{p^{2}}.\n$$\nTherefore, the variance is\n$$\n\\mathrm{Var}(N)=E[N^{2}]-\\bigl(E[N]\\bigr)^{2}=\\frac{2-p}{p^{2}}-\\left(\\frac{1}{p}\\right)^{2}=\\frac{1-p}{p^{2}}.\n$$",
            "answer": "$$\\boxed{\\frac{1-p}{p^{2}}}$$"
        }
    ]
}