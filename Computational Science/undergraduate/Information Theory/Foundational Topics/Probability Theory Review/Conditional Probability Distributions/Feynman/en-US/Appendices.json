{
    "hands_on_practices": [
        {
            "introduction": "Understanding how information about one random variable changes our knowledge of another is the essence of conditional probability. This foundational exercise grounds the abstract definition of a conditional distribution in a concrete, practical scenario involving quality control. By working with a discrete joint probability mass function (PMF) presented in a simple table, you will practice the fundamental skill of calculating the conditional PMF $p_{X|Y}(x|y)$, effectively isolating a slice of the full probability space once a specific condition is known .",
            "id": "1906145",
            "problem": "In a semiconductor fabrication plant, a quality control process inspects batches of processors for defects. Let the discrete random variable $X$ represent the number of major defects, and the discrete random variable $Y$ represent the number of minor defects found in a randomly selected batch. The joint probability mass function (PMF), $p_{X,Y}(x,y)$, for these two variables is given by the following table:\n\n| $p_{X,Y}(x,y)$ | $y=0$ | $y=1$ | $y=2$ | $y=3$ |\n|:--------------:|:-----:|:-----:|:-----:|:-----:|\n|     **$x=0$**    | 0.10  | 0.15  | 0.08  | 0.05  |\n|     **$x=1$**    | 0.08  | 0.20  | 0.12  | 0.02  |\n|     **$x=2$**    | 0.05  | 0.07  | 0.06  | 0.02  |\n\nAn inspector finds that a particular batch contains exactly one minor defect. Given this information, what is the conditional PMF of the number of major defects, $X$?\n\nProvide the values of the conditional PMF $p_{X|Y}(x|1)$ for $x=0, 1, 2$ as a row matrix, with entries corresponding to increasing values of $x$. Express your answers as exact fractions.",
            "solution": "The goal is to find the conditional probability mass function (PMF) of the random variable $X$ given that the random variable $Y$ has taken the value $1$. This conditional PMF is denoted by $p_{X|Y}(x|1)$.\n\nThe definition of the conditional PMF of $X$ given $Y=y$ is:\n$$p_{X|Y}(x|y) = \\frac{p_{X,Y}(x,y)}{p_Y(y)}$$\nwhere $p_{X,Y}(x,y)$ is the joint PMF of $X$ and $Y$, and $p_Y(y)$ is the marginal PMF of $Y$. The marginal PMF $p_Y(y)$ is found by summing the joint PMF over all possible values of $X$:\n$$p_Y(y) = \\sum_{\\text{all } x} p_{X,Y}(x,y)$$\n\nIn this problem, we are interested in the case where $y=1$. First, we must calculate the marginal probability $p_Y(1)$. We do this by summing the probabilities in the column corresponding to $y=1$ in the given joint PMF table. The possible values for $X$ are $0, 1, 2$.\n\n$$p_Y(1) = p_{X,Y}(0,1) + p_{X,Y}(1,1) + p_{X,Y}(2,1)$$\n\nSubstituting the values from the table:\n$$p_Y(1) = 0.15 + 0.20 + 0.07 = 0.42$$\n\nNow that we have the marginal probability $p_Y(1)$, we can find the conditional probabilities for each possible value of $X$ (i.e., $x=0, 1, 2$) given $Y=1$.\n\nFor $x=0$:\n$$p_{X|Y}(0|1) = \\frac{p_{X,Y}(0,1)}{p_Y(1)} = \\frac{0.15}{0.42}$$\nTo express this as an exact fraction, we can write:\n$$p_{X|Y}(0|1) = \\frac{15}{42} = \\frac{3 \\times 5}{3 \\times 14} = \\frac{5}{14}$$\n\nFor $x=1$:\n$$p_{X|Y}(1|1) = \\frac{p_{X,Y}(1,1)}{p_Y(1)} = \\frac{0.20}{0.42}$$\nAs an exact fraction:\n$$p_{X|Y}(1|1) = \\frac{20}{42} = \\frac{2 \\times 10}{2 \\times 21} = \\frac{10}{21}$$\n\nFor $x=2$:\n$$p_{X|Y}(2|1) = \\frac{p_{X,Y}(2,1)}{p_Y(1)} = \\frac{0.07}{0.42}$$\nAs an exact fraction:\n$$p_{X|Y}(2|1) = \\frac{7}{42} = \\frac{7 \\times 1}{7 \\times 6} = \\frac{1}{6}$$\n\nAs a check, the sum of these conditional probabilities should be 1:\n$$\\sum_{x} p_{X|Y}(x|1) = \\frac{5}{14} + \\frac{10}{21} + \\frac{1}{6}$$\nTo sum these fractions, we find a common denominator, which is 42.\n$$\\frac{5 \\times 3}{14 \\times 3} + \\frac{10 \\times 2}{21 \\times 2} + \\frac{1 \\times 7}{6 \\times 7} = \\frac{15}{42} + \\frac{20}{42} + \\frac{7}{42} = \\frac{15+20+7}{42} = \\frac{42}{42} = 1$$\nThe probabilities correctly sum to 1.\n\nThe conditional PMF for $X$ given $Y=1$ is therefore:\n$p_{X|Y}(0|1) = \\frac{5}{14}$\n$p_{X|Y}(1|1) = \\frac{10}{21}$\n$p_{X|Y}(2|1) = \\frac{1}{6}$\n\nThe problem asks for these values to be presented as a row matrix in increasing order of $x$. The final result is a matrix with the three calculated fractions as its elements.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{5}{14} & \\frac{10}{21} & \\frac{1}{6} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Moving from discrete to continuous variables, we can uncover fascinating properties that have profound implications for real-world modeling. This problem introduces the \"memoryless\" property, a hallmark of the exponential distribution and a cornerstone of reliability theory and queuing systems. By calculating the conditional distribution of a component's remaining lifetime, given it has already survived for some time, you will discover a counter-intuitive yet powerful result that simplifies the analysis of many stochastic processes .",
            "id": "1906142",
            "problem": "A deep-space probe is powered by a critical non-rechargeable power source. The lifetime of this power source, denoted by the random variable $T$ (in years), is modeled by an exponential distribution with a constant rate parameter $\\lambda > 0$. The probability density function (PDF) for $T$ is given by $f_T(t) = \\lambda \\exp(-\\lambda t)$ for $t \\ge 0$, and $f_T(t) = 0$ for $t < 0$.\n\nAfter the probe has been successfully operating for a duration of $t_0$ years, mission control wants to analyze the reliability for the remainder of the mission. Let $Y = T - t_0$ represent the additional lifetime of the power source, given that it has already survived past time $t_0$.\n\nDetermine the probability density function of the random variable $Y$ for $y \\ge 0$. Express your answer as a function of $y$ and $\\lambda$.",
            "solution": "We are given that $T$ has an exponential distribution with rate parameter $\\lambda>0$, so its probability density function is $f_{T}(t)=\\lambda \\exp(-\\lambda t)$ for $t\\ge 0$ and $f_{T}(t)=0$ for $t<0$. The additional lifetime after time $t_{0}$ is defined as $Y=T-t_{0}$, conditional on the event $\\{T>t_{0}\\}$.\n\nTo find the density of $Y$, we use the conditional density of $T$ given survival past $t_{0}$ and then apply the shift transformation. For $y\\ge 0$,\n$$\nf_{Y}(y)=f_{T\\mid T>t_{0}}(t_{0}+y)=\\frac{f_{T}(t_{0}+y)}{\\mathbb{P}(T>t_{0})}.\n$$\nWe compute the survival probability:\n$$\n\\mathbb{P}(T>t_{0})=\\int_{t_{0}}^{\\infty}\\lambda \\exp(-\\lambda t)\\,dt=\\exp(-\\lambda t_{0}).\n$$\nSubstituting $f_{T}(t_{0}+y)=\\lambda \\exp\\!\\big(-\\lambda(t_{0}+y)\\big)$ and dividing by $\\exp(-\\lambda t_{0})$ gives\n$$\nf_{Y}(y)=\\frac{\\lambda \\exp\\!\\big(-\\lambda(t_{0}+y)\\big)}{\\exp(-\\lambda t_{0})}=\\lambda \\exp(-\\lambda y),\\quad y\\ge 0.\n$$\nFor completeness, $f_{Y}(y)=0$ for $y<0$. This demonstrates the memoryless property: $Y$ is exponential with the same rate $\\lambda$ and does not depend on $t_{0}$.",
            "answer": "$$\\boxed{\\lambda \\exp(-\\lambda y)}$$"
        },
        {
            "introduction": "Conditional probability finds one of its most powerful applications in the framework of Bayesian inference, which provides a mathematical model for learning from experience. This advanced exercise demonstrates how to update our beliefs about an unknown quantity by incorporating new, noisy measurements from multiple sources, a process known as sensor fusion. You will derive the posterior distribution that synthesizes prior knowledge with observed data, illustrating how systems in fields from robotics to finance use conditional probability to combine information and make optimal estimates .",
            "id": "1613129",
            "problem": "A materials scientist is characterizing a newly developed photovoltaic cell. The true, but unknown, open-circuit voltage of the cell is represented by the random variable $X$. Based on theoretical models and results from early prototypes, the scientist's prior belief about $X$ is modeled by a Gaussian probability density function (PDF) with a mean $\\mu_X$ and a variance $\\sigma_X^2$.\n\nTo obtain a more precise estimate, the scientist performs two measurements of the voltage using two different high-precision multimeters. The measurement process for each multimeter is corrupted by additive, zero-mean Gaussian noise. The noise sources for the two multimeters are independent of each other and of the true voltage $X$.\n\n- Multimeter 1 yields a measurement $Y_1$. The measurement error is a Gaussian random variable with variance $\\sigma_1^2$.\n- Multimeter 2 yields a measurement $Y_2$. The measurement error is a Gaussian random variable with variance $\\sigma_2^2$.\n\nThe relationship between the measurements and the true voltage is given by $Y_i = X + N_i$ for $i \\in \\{1, 2\\}$, where $N_i$ represents the noise term for the $i$-th multimeter. Given the independence of the noise, the joint conditional PDF of the measurements given the true voltage is $p(y_1, y_2|x) = p(y_1|x)p(y_2|x)$.\n\nAfter obtaining the specific measurement values $y_1$ and $y_2$, the scientist updates their knowledge about $X$ by computing the posterior PDF, $p(x|y_1, y_2)$. It is known that this posterior distribution is also a Gaussian. Your task is to determine the mean, $\\mu_{\\text{post}}$, and the variance, $\\sigma_{\\text{post}}^2$, of this posterior distribution.\n\nProvide two separate symbolic expressions, one for $\\mu_{\\text{post}}$ and one for $\\sigma_{\\text{post}}^2$. The first expression should be for the mean and the second for the variance. Both expressions should be in terms of the given parameters: $\\mu_X, \\sigma_X^2, y_1, \\sigma_1^2, y_2$, and $\\sigma_2^2$.",
            "solution": "The problem requires us to find the parameters of the posterior distribution $p(x|y_1, y_2)$. We can use Bayes' theorem for continuous random variables. The posterior PDF is proportional to the product of the likelihood and the prior PDF:\n$$p(x|y_1, y_2) = \\frac{p(y_1, y_2|x)p(x)}{p(y_1, y_2)}$$\nThe denominator $p(y_1, y_2)$ is a normalization constant that does not depend on $x$. Therefore, we can work with proportionality:\n$$p(x|y_1, y_2) \\propto p(y_1, y_2|x)p(x)$$\nThe problem states that the measurements are conditionally independent given $x$, so $p(y_1, y_2|x) = p(y_1|x)p(y_2|x)$. This gives:\n$$p(x|y_1, y_2) \\propto p(y_1|x) p(y_2|x) p(x)$$\nNow, we write out the explicit forms for each of the three Gaussian PDFs on the right-hand side.\nThe prior distribution for $X$ is Gaussian with mean $\\mu_X$ and variance $\\sigma_X^2$:\n$$p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_X^2}} \\exp\\left(-\\frac{(x - \\mu_X)^2}{2\\sigma_X^2}\\right)$$\nThe likelihood for the first measurement $Y_1$ is $p(y_1|x)$. Since $Y_1 = X + N_1$ where $N_1 \\sim \\mathcal{N}(0, \\sigma_1^2)$, the distribution of $Y_1$ conditional on $X=x$ is a Gaussian with mean $x$ and variance $\\sigma_1^2$:\n$$p(y_1|x) = \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} \\exp\\left(-\\frac{(y_1 - x)^2}{2\\sigma_1^2}\\right)$$\nSimilarly, the likelihood for the second measurement $Y_2$ has mean $x$ and variance $\\sigma_2^2$:\n$$p(y_2|x) = \\frac{1}{\\sqrt{2\\pi\\sigma_2^2}} \\exp\\left(-\\frac{(y_2 - x)^2}{2\\sigma_2^2}\\right)$$\nSubstituting these into the proportionality relation, and ignoring the constant multiplicative factors:\n$$p(x|y_1, y_2) \\propto \\exp\\left(-\\frac{(y_1 - x)^2}{2\\sigma_1^2}\\right) \\exp\\left(-\\frac{(y_2 - x)^2}{2\\sigma_2^2}\\right) \\exp\\left(-\\frac{(x - \\mu_X)^2}{2\\sigma_X^2}\\right)$$\nWe can combine the exponents:\n$$p(x|y_1, y_2) \\propto \\exp\\left[ -\\frac{1}{2} \\left( \\frac{(x - y_1)^2}{\\sigma_1^2} + \\frac{(x - y_2)^2}{\\sigma_2^2} + \\frac{(x - \\mu_X)^2}{\\sigma_X^2} \\right) \\right]$$\nThe expression inside the parentheses is a quadratic in $x$. Since the posterior is known to be Gaussian, its PDF must have the form $p(x|y_1, y_2) \\propto \\exp\\left( -\\frac{(x - \\mu_{\\text{post}})^2}{2\\sigma_{\\text{post}}^2} \\right)$. To find $\\mu_{\\text{post}}$ and $\\sigma_{\\text{post}}^2$, we expand the exponent and match the coefficients of $x^2$ and $x$. Let $E$ be the term in the large parentheses:\n$$E = \\frac{x^2 - 2xy_1 + y_1^2}{\\sigma_1^2} + \\frac{x^2 - 2xy_2 + y_2^2}{\\sigma_2^2} + \\frac{x^2 - 2x\\mu_X + \\mu_X^2}{\\sigma_X^2}$$\nCollecting terms with $x^2$ and $x$:\n$$E = x^2 \\left( \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2} + \\frac{1}{\\sigma_X^2} \\right) - 2x \\left( \\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2} + \\frac{\\mu_X}{\\sigma_X^2} \\right) + (\\text{terms not involving } x)$$\nThe general form of the exponent of a Gaussian with mean $\\mu$ and variance $\\sigma^2$ is $\\frac{(x - \\mu)^2}{\\sigma^2} = \\frac{x^2 - 2x\\mu + \\mu^2}{\\sigma^2} = x^2\\left(\\frac{1}{\\sigma^2}\\right) - 2x\\left(\\frac{\\mu}{\\sigma^2}\\right) + \\frac{\\mu^2}{\\sigma^2}$.\nBy comparing the coefficient of $x^2$ in our expression for $E$ with the general form, we identify the inverse of the posterior variance:\n$$\\frac{1}{\\sigma_{\\text{post}}^2} = \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2} + \\frac{1}{\\sigma_X^2}$$\nSolving for $\\sigma_{\\text{post}}^2$, we get:\n$$\\sigma_{\\text{post}}^2 = \\left( \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2} + \\frac{1}{\\sigma_X^2} \\right)^{-1}$$\nNext, we compare the coefficient of the linear term in $x$. From the general form this is $-2\\frac{\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^2}$, and from our derived exponent it is $-2\\left( \\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2} + \\frac{\\mu_X}{\\sigma_X^2} \\right)$.\nEquating these gives:\n$$\\frac{\\mu_{\\text{post}}}{\\sigma_{\\text{post}}^2} = \\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2} + \\frac{\\mu_X}{\\sigma_X^2}$$\nNow we solve for the posterior mean, $\\mu_{\\text{post}}$:\n$$\\mu_{\\text{post}} = \\sigma_{\\text{post}}^2 \\left( \\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2} + \\frac{\\mu_X}{\\sigma_X^2} \\right)$$\nSubstituting the expressions for $\\sigma_{\\text{post}}^2$ and $1/\\sigma_{\\text{post}}^2$:\n$$\\mu_{\\text{post}} = \\frac{\\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2} + \\frac{\\mu_X}{\\sigma_X^2}}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2} + \\frac{1}{\\sigma_X^2}}$$\nThese are the final expressions for the posterior mean and variance. The posterior mean is a weighted average of the prior mean and the two measurements, where each term is weighted by its precision (the inverse of its variance). The posterior precision is the sum of the individual precisions.",
            "answer": "$$\n\\boxed{\\frac{\\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2} + \\frac{\\mu_X}{\\sigma_X^2}}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2} + \\frac{1}{\\sigma_X^2}}}\n$$\n\n$$\n\\boxed{\\left( \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2} + \\frac{1}{\\sigma_X^2} \\right)^{-1}}\n$$"
        }
    ]
}