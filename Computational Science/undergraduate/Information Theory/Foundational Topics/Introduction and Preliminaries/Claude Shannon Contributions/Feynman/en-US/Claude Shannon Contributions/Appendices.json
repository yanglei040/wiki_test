{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of Claude Shannon's information theory is the quantification of information itself. He introduced the concept of entropy, a powerful measure of the uncertainty or unpredictability associated with a random variable. This first exercise  provides a direct application of this fundamental idea, asking you to calculate the entropy of a simplified weather model. By working through this problem, you will gain hands-on experience in applying the Shannon entropy formula, which is the theoretical foundation for all modern data compression algorithms.",
            "id": "1659073",
            "problem": "A simplified meteorological model describes the weather for the next day as a discrete random variable $W$ with four possible outcomes: Sunny, Cloudy, Rainy, or Snowy. The model is treated as a discrete memoryless source. Based on extensive historical data for a particular city, the probabilities of these outcomes are determined to be:\n- $P(W=\\text{Sunny}) = 0.60$\n- $P(W=\\text{Cloudy}) = 0.25$\n- $P(W=\\text{Rainy}) = 0.10$\n- $P(W=\\text{Snowy}) = 0.05$\nCalculate the Shannon entropy of this weather source. Express your answer in bits, rounded to four significant figures.",
            "solution": "For a discrete memoryless source with outcomes having probabilities $\\{p_i\\}$, the Shannon entropy in bits is defined by\n$$\nH(W)=-\\sum_{i} p_i\\log_2(p_i).\n$$\nSubstituting the given probabilities,\n$$\nH(W)=-\\big(0.60\\log_2(0.60)+0.25\\log_2(0.25)+0.10\\log_2(0.10)+0.05\\log_2(0.05)\\big).\n$$\nUsing the change-of-base formula $\\log_2(x)=\\frac{\\ln(x)}{\\ln(2)}$, we evaluate each term:\n$$\n\\log_2(0.60)\\approx -0.7369655942 \\;\\Rightarrow\\; -0.60\\log_2(0.60)\\approx 0.4421793565,\n$$\n$$\n\\log_2(0.25)=-2 \\;\\Rightarrow\\; -0.25\\log_2(0.25)=0.5000000000,\n$$\n$$\n\\log_2(0.10)\\approx -3.3219280949 \\;\\Rightarrow\\; -0.10\\log_2(0.10)\\approx 0.3321928095,\n$$\n$$\n\\log_2(0.05)\\approx -4.3219280949 \\;\\Rightarrow\\; -0.05\\log_2(0.05)\\approx 0.2160964047.\n$$\nSumming these contributions gives\n$$\nH(W)\\approx 0.4421793565+0.5000000000+0.3321928095+0.2160964047=1.4904685707 \\text{ bits}.\n$$\nRounding to four significant figures yields $1.490$ bits.",
            "answer": "$$\\boxed{1.490}$$"
        },
        {
            "introduction": "Having learned how to measure information, the next logical step is to understand the limits of transmitting it reliably over a noisy medium. This is where Shannon's groundbreaking concept of channel capacity comes into play. This exercise  models a single, volatile bit of computer memory as a communication channel through time, specifically as a Binary Symmetric Channel (BSC). Calculating the capacity of this channel will help you grasp how Shannon's abstract theorems provide a practical upper bound on the rate of reliable computation and data storage in the presence of noise.",
            "id": "1610553",
            "problem": "Consider a simplified model for a volatile computational memory system, structured as a one-dimensional tape of cells. Each cell can store a single bit, either a 0 or a 1. The memory is subject to noise. At each discrete time step of a computational clock, the bit stored in any given cell has a constant probability $p$ of spontaneously flipping to its opposite state (i.e., 0 becomes 1, or 1 becomes 0), where $0 \\le p \\le 1$. This flipping event is independent for each cell and for each time step.\n\nWe can analyze this memory system from an information-theoretic perspective by modeling a single cell as a communication channel through time. The input to this channel is the state of the cell at a time $t$, and the output is the state of that same cell at the next time step, $t+1$.\n\nDetermine the Shannon capacity of this single-cell memory channel. Your answer should be an analytic expression in terms of the flip probability $p$. All logarithms used must be base 2. Provide the capacity in units of bits per cell per time step.",
            "solution": "Let $X \\in \\{0,1\\}$ denote the state of a given cell at time $t$ and $Y \\in \\{0,1\\}$ the state of that same cell at time $t+1$. By the problemâ€™s dynamics, the transition law is\n$$\n\\Pr(Y=X)=1-p,\\quad \\Pr(Y\\neq X)=p,\n$$\nso this is a binary symmetric channel (BSC) with crossover probability $p$ used once per time step.\n\nLet the input distribution be $\\Pr(X=1)=q$ and $\\Pr(X=0)=1-q$. The output distribution is\n$$\n\\Pr(Y=1)=\\Pr(Y=1|X=1)\\Pr(X=1)+\\Pr(Y=1|X=0)\\Pr(X=0)\n= (1-p)q + p(1-q) = p + q(1-2p).\n$$\nLet $r=\\Pr(Y=1)=p+q(1-2p)$. The mutual information between $X$ and $Y$ for a fixed input distribution is\n$$\nI(X;Y)=H(Y)-H(Y|X).\n$$\nBecause the channel is a BSC with crossover probability $p$, the conditional entropy is\n$$\nH(Y|X)=H_2(p)=-p\\log_2(p)-(1-p)\\log_2(1-p).\n$$\nThe output entropy is\n$$\nH(Y)=H_2(r)=-r\\log_2(r)-(1-r)\\log_2(1-r).\n$$\nTherefore,\n$$\nI(X;Y)=H_2(r)-H_2(p)=H_2\\big(p+q(1-2p)\\big)-H_2(p).\n$$\nThe Shannon capacity per channel use (per cell per time step) is the maximum of $I(X;Y)$ over the input distribution $q \\in [0,1]$. Since the second term $H_2(p)$ is independent of $q$, we must maximize $H_2(r)$ with respect to $q$ (equivalently with respect to $r$). The binary entropy $H_2(r)$ is concave in $r$ and is maximized at $r=\\frac{1}{2}$. To verify, compute\n$$\n\\frac{d}{dr}H_2(r)=-\\log_2(r) - \\frac{1}{\\ln 2} + \\log_2(1-r) + \\frac{1}{\\ln 2}\n=\\log_2\\!\\left(\\frac{1-r}{r}\\right),\n$$\nwhich vanishes if and only if $r=\\frac{1}{2}$; the second derivative is negative, so this is a maximum. Imposing $r=\\frac{1}{2}$ yields\n$$\np+q(1-2p)=\\frac{1}{2}\\quad\\Longrightarrow\\quad q^\\star=\\frac{\\frac{1}{2}-p}{1-2p}.\n$$\nFor $p\\neq \\frac{1}{2}$ this simplifies to $q^\\star=\\frac{1}{2}$; for $p=\\frac{1}{2}$ one has $r=\\frac{1}{2}$ for any $q$, and the capacity will be zero as shown below.\n\nAt the maximizing input, $H(Y)=H_2(\\frac{1}{2})=1$, so the channel capacity is\n$$\nC=\\max_{q} I(X;Y)=1 - H_2(p).\n$$\nWriting $H_2(p)$ explicitly with base-2 logarithms gives\n$$\nC = 1 + p\\log_2(p) + (1-p)\\log_2(1-p),\n$$\nwith the usual convention that $0\\log_2(0)=0$. This capacity is in bits per cell per time step, equals $1$ at $p=0$ or $p=1$, and equals $0$ at $p=\\frac{1}{2}$.",
            "answer": "$$\\boxed{1 + p\\log_2(p) + (1-p)\\log_2(1-p)}$$"
        },
        {
            "introduction": "While the Binary Symmetric Channel is a vital pedagogical tool, many real-world communication systems, from Wi-Fi to deep-space probes, are better modeled by continuous channels like the Additive White Gaussian Noise (AWGN) channel. This advanced problem  challenges you to determine the capacity of an AWGN channel with a realistic complication: the noise power is not constant but increases with the signal power $P$. This scenario forces you to move beyond simply applying the Shannon-Hartley formula and instead use calculus to find the optimal transmission power that maximizes the data rate. This exercise exemplifies how information theory guides practical engineering decisions in optimizing communication system design.",
            "id": "1610570",
            "problem": "A deep-space communication system is designed to transmit data over vast interstellar distances. The communication channel is modeled as an Additive White Gaussian Noise (AWGN) channel with a fixed bandwidth $W$. A unique characteristic of this channel is that the high-power radio signals transmitted by the probe interact with the tenuous interstellar plasma, generating a secondary noise component. This results in a total noise power, $\\sigma_N^2$, that depends on the transmitted signal power, $P$. The relationship is well-approximated by the model:\n\n$$ \\sigma_N^2(P) = N_0 + \\beta P^2 $$\n\nHere, $N_0$ is the constant background noise power (from cosmic microwave background and receiver electronics), and $\\beta$ is a positive dimensionless coefficient that quantifies the strength of the signal-plasma interaction. The transmitter on the space probe is subject to an average power constraint, such that the utilized signal power $P$ cannot exceed a maximum value $P_{\\text{max}}$.\n\nAssuming the use of an optimal encoding scheme that approaches the Shannon limit, determine a single, closed-form analytic expression for the capacity $C$ of this communication channel in terms of $W$, $N_0$, $\\beta$, and $P_{\\text{max}}$.",
            "solution": "The continuous-time AWGN channel of bandwidth $W$ with total (in-band) noise power $N$ and average signal power $P$ has Shannon capacity\n$$\nC(P) = W \\log_2\\!\\left(1 + \\frac{P}{N}\\right),\n$$\nassuming optimal coding. In this problem, the total noise power depends on the chosen transmit power $P$ as\n$$\n\\sigma_N^2(P) = N_0 + \\beta P^2,\n$$\nwith $P$ constrained by $0 \\leq P \\leq P_{\\text{max}}$. Substituting $N = \\sigma_N^2(P)$ gives the capacity as a function of $P$:\n$$\nC(P) = W \\log_2\\!\\left(1 + \\frac{P}{N_0 + \\beta P^2}\\right).\n$$\n\nTo satisfy the power constraint and maximize capacity, we optimize over $P \\in [0, P_{\\text{max}}]$. Define the signal-to-noise ratio\n$$\n\\mathrm{SNR}(P) = \\frac{P}{N_0 + \\beta P^2}.\n$$\nThen\n$$\nC(P) = \\frac{W}{\\ln 2}\\,\\ln\\!\\bigl(1 + \\mathrm{SNR}(P)\\bigr).\n$$\nSince $\\ln(1+x)$ is strictly increasing, maximizing $C(P)$ is equivalent to maximizing $\\mathrm{SNR}(P)$ over $P \\in [0, P_{\\text{max}}]$. Differentiate $\\mathrm{SNR}(P)$:\n$$\n\\frac{d}{dP}\\mathrm{SNR}(P) = \\frac{N_0 - \\beta P^2}{\\bigl(N_0 + \\beta P^2\\bigr)^2}.\n$$\nSetting the derivative to zero yields the unique interior critical point\n$$\nN_0 - \\beta P^2 = 0 \\quad \\Rightarrow \\quad P_0 = \\sqrt{\\frac{N_0}{\\beta}}.\n$$\nFor $P  P_0$, the derivative is positive; for $P > P_0$, it is negative. Hence $P_0$ is the global maximizer of $\\mathrm{SNR}(P)$ over $P \\geq 0$. Enforcing the constraint $P \\leq P_{\\text{max}}$ gives the optimal transmit power\n$$\nP^\\star = \\min\\!\\left(P_{\\text{max}}, \\sqrt{\\frac{N_0}{\\beta}}\\right).\n$$\nSubstituting $P^\\star$ back into the capacity expression gives the channel capacity under the power-dependent noise model:\n$$\nC = W \\log_2\\!\\left(1 + \\frac{P^\\star}{N_0 + \\beta (P^\\star)^2}\\right), \\text{ where } P^\\star = \\min\\!\\left(P_{\\text{max}}, \\sqrt{\\frac{N_0}{\\beta}}\\right).\n$$\nThis is a single, closed-form analytic expression in terms of $W$, $N_0$, $\\beta$, and $P_{\\text{max}}$.",
            "answer": "$$\\boxed{W \\log_2\\!\\left(1 + \\frac{\\min\\!\\left(P_{\\text{max}},\\, \\sqrt{N_0/\\beta}\\right)}{N_0 + \\beta\\,\\min\\!\\left(P_{\\text{max}},\\, \\sqrt{N_0/\\beta}\\right)^2}\\right)}$$"
        }
    ]
}