{
    "hands_on_practices": [
        {
            "introduction": "The ability to convert between different units of information—bits, nats, and hartleys—is a fundamental skill in information theory. This first exercise provides practice in this core competency by asking you to translate a measurement from the decimal-based hartley system to the binary-based bit system, which is the standard in computing and digital communications . Mastering this conversion is essential for comparing and integrating data from systems that use different logarithmic bases.",
            "id": "1666613",
            "problem": "In the field of information theory, the amount of information, $I$, gained from observing an event with probability $p$ is defined as $I = -\\log_{b}(p)$. The choice of the base of the logarithm, $b$, determines the unit of information. The standard unit is the 'bit', which corresponds to a base of $b=2$.\n\nAn interplanetary probe is analyzing signals from a newly discovered celestial phenomenon. The probe's internal systems, designed with a decimal-based architecture, measure information content using the 'hartley' unit, which corresponds to a base of $b=10$. After a period of observation, the probe calculates that the mutual information between two correlated signal patterns is $2.5$ hartleys. For this measurement to be compared with standard scientific data on Earth, it must be converted into bits.\n\nCalculate the value of this mutual information in bits. Express your final answer in bits, rounded to three significant figures.",
            "solution": "The information content with base $b$ is $I_{b}(p) = -\\log_{b}(p)$. For the same event probability $p$, the values in bases $10$ and $2$ are related by the change-of-base formula:\n$$\n\\log_{2}(p) = \\frac{\\log_{10}(p)}{\\log_{10}(2)} \\quad \\Rightarrow \\quad -\\log_{2}(p) = -\\frac{\\log_{10}(p)}{\\log_{10}(2)}.\n$$\nHence, if $I_{10} = -\\log_{10}(p)$ is the information in hartleys and $I_{2} = -\\log_{2}(p)$ is the information in bits, then\n$$\nI_{2} = \\frac{I_{10}}{\\log_{10}(2)} = I_{10}\\,\\log_{2}(10) = I_{10}\\,\\frac{\\ln 10}{\\ln 2}.\n$$\nGiven $I_{10} = 2.5$, we compute\n$$\nI_{2} = 2.5\\,\\frac{\\ln 10}{\\ln 2}.\n$$\nUsing numerical values to obtain the requested three-significant-figure approximation,\n$$\n\\ln 10 \\approx 2.302585093, \\quad \\ln 2 \\approx 0.693147180 \\quad \\Rightarrow \\quad \\frac{\\ln 10}{\\ln 2} \\approx 3.321928095,\n$$\nso\n$$\nI_{2} \\approx 2.5 \\times 3.321928095 \\approx 8.304820237 \\approx 8.30 \\text{ (to three significant figures)}.\n$$",
            "answer": "$$\\boxed{8.30}$$"
        },
        {
            "introduction": "Real-world systems often involve integrating data from multiple, independent sources, each potentially using a different native unit for information. This practice builds upon basic unit conversion by introducing the concept of joint entropy. You will learn how to combine information from two independent measurements, given in nats and bits, to find the total entropy in hartleys, reinforcing the principle that entropy is additive for independent events .",
            "id": "1666590",
            "problem": "A deep-space probe is equipped with two independent scientific instruments for analyzing the interstellar medium. The first instrument, a plasma wave detector, takes a measurement whose information content, or Shannon entropy, is quantified by its primary processor as $H_P = 2.5$ nats. The unit 'nat' signifies that the entropy calculation uses the natural logarithm (base $e$). The second instrument, a magnetometer, records the local magnetic field. The entropy of its measurement is calculated by a legacy subsystem to be $H_M = 4.0$ bits. The unit 'bit' signifies a calculation using the base-2 logarithm.\n\nA mission scientist needs to combine these data points for a unified report. The reporting standard for this mission requires all entropy values to be expressed in hartleys, a unit based on the base-10 logarithm. Assuming the measurements from the plasma wave detector and the magnetometer are statistically independent, what is the total joint entropy of a combined observation from both instruments?\n\nExpress your final answer in hartleys, rounded to four significant figures.",
            "solution": "We are given two entropy values expressed in different logarithmic bases: $H_{P}=2.5$ nats (natural logarithm) and $H_{M}=4.0$ bits (base-2 logarithm). The joint entropy of independent measurements adds, so in any common base,\n$$\nH_{\\text{joint}}=H_{P}+H_{M},\n$$\nprovided both terms are expressed in the same units. The reporting unit is hartleys (base-10 logarithm), so we convert each entropy to hartleys and then add.\n\nFor any base $b$, $\\log_{b}(x)=\\frac{\\ln x}{\\ln b}$. Therefore:\n- Converting nats to hartleys: if $H^{(\\mathrm{nat})}$ is in nats, then\n$$\nH^{(\\mathrm{hart})}=\\frac{H^{(\\mathrm{nat})}}{\\ln 10}.\n$$\nThus,\n$$\nH_{P}^{(\\mathrm{hart})}=\\frac{2.5}{\\ln 10}.\n$$\n- Converting bits to hartleys: if $H^{(\\mathrm{bit})}$ is in bits, then\n$$\nH^{(\\mathrm{hart})}=H^{(\\mathrm{bit})}\\log_{10}(2)=H^{(\\mathrm{bit})}\\frac{\\ln 2}{\\ln 10}.\n$$\nThus,\n$$\nH_{M}^{(\\mathrm{hart})}=4.0\\,\\frac{\\ln 2}{\\ln 10}.\n$$\n\nBy independence, the joint entropy in hartleys is\n$$\nH_{\\text{joint}}^{(\\mathrm{hart})}=\\frac{2.5}{\\ln 10}+4.0\\,\\frac{\\ln 2}{\\ln 10}.\n$$\nNumerically, using $\\ln 10\\approx 2.302585093$ and $\\ln 2\\approx 0.6931471806$,\n$$\n\\frac{2.5}{\\ln 10}\\approx 1.085736205,\\quad 4.0\\,\\frac{\\ln 2}{\\ln 10}=4.0\\,\\log_{10}(2)\\approx 1.204119983,\n$$\nso\n$$\nH_{\\text{joint}}^{(\\mathrm{hart})}\\approx 2.289856187.\n$$\nRounded to four significant figures, this is $2.290$ hartleys.",
            "answer": "$$\\boxed{2.290}$$"
        },
        {
            "introduction": "Moving beyond given entropy values, this final practice challenges you to calculate entropy from the ground up by analyzing a system's structure. By determining the total number of possible states for a security token using combinatorial principles, you will calculate its maximum possible information entropy . This exercise bridges the gap between abstract information theory and its practical application in assessing the complexity and security of real-world systems.",
            "id": "1666595",
            "problem": "A company is designing a new security system where access is granted via an 8-character token. The rules for generating a valid token are as follows:\n1. The first 3 characters must be a permutation of 3 distinct, case-sensitive letters chosen from the set {A, b, C, d, E}.\n2. The remaining 5 characters must be decimal digits (0 through 9), where repetition of digits is allowed.\n\nAssuming that every possible valid token is generated with equal probability, the system is operating at its maximum possible entropy. Calculate this information entropy for a single, randomly generated token. Your final answer should be a single closed-form analytic expression in units of hartleys. For the purpose of this problem, one hartley is the amount of information gained by observing an event with 10 equally likely outcomes.",
            "solution": "The total number of valid tokens equals the product of the number of valid choices for the first three characters and the number of valid choices for the last five characters, by the multiplication principle.\n\n- For the first three characters: they must be a permutation of three distinct letters chosen from a set of five distinct case-sensitive letters. The number of such 3-permutations is\n$$\nP(5,3)=\\frac{5!}{(5-3)!}=\\frac{5!}{2!}.\n$$\n\n- For the last five characters: each position can be any decimal digit from 0 through 9, with repetition allowed, giving\n$$\n10^{5}\n$$\npossibilities.\n\nThus the total number of valid tokens is\n$$\nN=\\frac{5!}{2!}\\cdot 10^{5}.\n$$\n\nWith all valid tokens equally likely, the maximum information entropy in hartleys (base-10 units) is\n$$\nH=\\log_{10}(N)=\\log_{10}\\!\\left(\\frac{5!}{2!}\\cdot 10^{5}\\right)=\\log_{10}\\!\\left(\\frac{5!}{2!}\\right)+5.\n$$\nEvaluating the factorial ratio gives $\\frac{5!}{2!}=60$, so\n$$H=\\log_{10}(60)+5 = \\log_{10}(6 \\times 10) + 5 = \\log_{10}(6) + \\log_{10}(10) + 5 = \\log_{10}(6) + 1 + 5 = 6+\\log_{10}(6).$$\nThis is the closed-form entropy in hartleys.",
            "answer": "$$\\boxed{6+\\log_{10}(6)}$$"
        }
    ]
}