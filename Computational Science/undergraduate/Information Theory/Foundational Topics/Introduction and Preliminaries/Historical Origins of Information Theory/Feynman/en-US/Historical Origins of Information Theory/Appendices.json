{
    "hands_on_practices": [
        {
            "introduction": "Before we could build theories about information, we first needed a way to measure it. This exercise takes us back to a time before the formalization of information theory, using the intuitive setting of a card game to explore how we can quantify the amount of \"surprise\" or information an event provides. By working through this problem , you will develop a concrete understanding of self-information and see how the 'bit' arises as a natural unit for measuring the reduction of uncertainty.",
            "id": "1629826",
            "problem": "Imagine yourself in the late 19th century, decades before the formalization of information theory. You are observing a card game where a single card is drawn from a well-shuffled, standard 52-card deck. The deck consists of four suits (Clubs, Diamonds, Hearts, Spades), and each suit has 13 ranks (2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King, Ace). The cards Jack, Queen, and King are known as \"face cards\".\n\nAn observer, who cannot see the card, receives two pieces of information in sequence. First, they are told, \"The card is a face card.\" After absorbing this fact, they are then told, \"The card is a spade.\"\n\nCalculate the total quantity of new information the observer has gained from both of these announcements combined. Express your answer in bits as a single, exact analytical expression.",
            "solution": "Let's denote the initial sample space of all possible outcomes as $\\Omega$, where $|\\Omega| = 52$. The problem asks for the total information gained from two successive announcements. We can calculate this by summing the information gained at each step.\n\nThe information content, or self-information, of an event $E$ is given by the formula $I(E) = -\\log_2(P(E))$, where $P(E)$ is the probability of the event. The base-2 logarithm ensures the result is in bits.\n\n**Step 1: Information from the first announcement**\n\nLet $A$ be the event that the drawn card is a face card.\nIn a standard 52-card deck, there are 3 face cards (Jack, Queen, King) in each of the 4 suits.\nThe total number of face cards is $N_A = 3 \\times 4 = 12$.\nThe probability of drawing a face card is:\n$$P(A) = \\frac{\\text{Number of face cards}}{\\text{Total number of cards}} = \\frac{12}{52} = \\frac{3}{13}$$\nThe information gained from learning that event $A$ has occurred is:\n$$I(A) = -\\log_2(P(A)) = -\\log_2\\left(\\frac{3}{13}\\right) = \\log_2\\left(\\frac{13}{3}\\right)$$\n\n**Step 2: Information from the second announcement**\n\nLet $B$ be the event that the drawn card is a spade.\nThe second announcement is made after it is already known that the card is a face card. Therefore, we need to calculate the conditional information of event $B$ given that event $A$ has already occurred. This is given by $I(B|A) = -\\log_2(P(B|A))$.\n\nThe conditional probability $P(B|A)$ is the probability that the card is a spade, given that it is a face card. Our sample space is now reduced to the 12 face cards.\nAmong these 12 face cards, there are 3 that are spades: the Jack of Spades, the Queen of Spades, and the King of Spades.\nSo, the number of outcomes corresponding to \"spade and face card\" is 3.\n\nThe conditional probability is:\n$$P(B|A) = \\frac{\\text{Number of face cards that are spades}}{\\text{Total number of face cards}} = \\frac{3}{12} = \\frac{1}{4}$$\nThe information gained from the second announcement, given the first, is:\n$$I(B|A) = -\\log_2(P(B|A)) = -\\log_2\\left(\\frac{1}{4}\\right) = \\log_2(4) = 2 \\text{ bits}$$\n\n**Step 3: Total Information**\n\nThe total information gained is the sum of the information from each successive announcement:\n$$I_{\\text{total}} = I(A) + I(B|A)$$\n$$I_{\\text{total}} = \\log_2\\left(\\frac{13}{3}\\right) + \\log_2(4)$$\nUsing the logarithm property $\\log(x) + \\log(y) = \\log(xy)$:\n$$I_{\\text{total}} = \\log_2\\left(\\frac{13}{3} \\times 4\\right) = \\log_2\\left(\\frac{52}{3}\\right)$$\n\n**Alternative Method (Verification)**\n\nThe total information gained is equivalent to the information content of the final event, which is that the card is both a face card and a spade. Let this intersection event be $C = A \\cap B$.\nThe outcomes for event $C$ are {Jack of Spades, Queen of Spades, King of Spades}. There are 3 such cards in the original 52-card deck.\nThe probability of this event is:\n$$P(C) = P(A \\cap B) = \\frac{3}{52}$$\nThe total information is:\n$$I(C) = -\\log_2(P(C)) = -\\log_2\\left(\\frac{3}{52}\\right) = \\log_2\\left(\\frac{52}{3}\\right)$$\nBoth methods yield the same result, confirming the solution.",
            "answer": "$$\\boxed{\\log_{2}\\left(\\frac{52}{3}\\right)}$$"
        },
        {
            "introduction": "After establishing a way to measure information, the next great challenge for early engineers was how to represent and transmit it efficiently. This practice problem  places you in the role of an engineer designing a simple communication system, forcing you to confront the difference between a practical, but suboptimal, code and the theoretical limit of compression defined by the source's entropy. Calculating the code's redundancy reveals the cost of inefficiency and motivates the search for optimal coding schemes, a central theme in source coding.",
            "id": "1629773",
            "problem": "An engineer in the early days of digital communication is designing a signaling system for a remote railway line. The system is simplified to transmit only three distinct messages: `NORMAL`, `CAUTION`, and `HALT`. Based on operational analysis, the engineer determines the long-term frequency of these messages: `NORMAL` signals constitute 50% of all transmissions, while `CAUTION` and `HALT` signals each constitute 25% of transmissions.\n\nFor simplicity of design and hardware, the engineer implements a fixed-length binary encoding scheme. In this scheme, each of the three messages is represented by a unique sequence of exactly 2 binary digits.\n\nThe redundancy of a code is defined as the average number of excess bits transmitted per message when compared to the theoretical minimum average number of bits required for an ideal encoding of the information source. Calculate the redundancy of the engineer's encoding scheme. Express your answer as a single numerical value in units of bits per symbol.",
            "solution": "The problem asks for the redundancy of a specific encoding scheme. Redundancy, as defined in the problem, is the difference between the actual average code length per symbol and the theoretical minimum average code length per symbol. The theoretical minimum is given by the entropy of the source.\n\nLet the set of messages be $S = \\{\\text{NORMAL}, \\text{CAUTION}, \\text{HALT}\\}$. We are given their probabilities and the lengths of their corresponding codewords.\n\nStep 1: Identify the probabilities and code lengths.\nThe probabilities of the symbols are:\n$p(\\text{NORMAL}) = 0.5$\n$p(\\text{CAUTION}) = 0.25$\n$p(\\text{HALT}) = 0.25$\n\nThe problem states that a fixed-length binary encoding of length 2 is used for all three messages. Thus, the code lengths are:\n$l(\\text{NORMAL}) = 2$\n$l(\\text{CAUTION}) = 2$\n$l(\\text{HALT}) = 2$\n\nStep 2: Calculate the entropy of the source.\nThe entropy, $H(S)$, gives the theoretical minimum average number of bits per symbol. The formula for entropy is:\n$$H(S) = -\\sum_{i \\in S} p(i) \\log_{2}(p(i))$$\n\nSubstituting the given probabilities:\n$$H(S) = - [p(\\text{NORMAL})\\log_{2}(p(\\text{NORMAL})) + p(\\text{CAUTION})\\log_{2}(p(\\text{CAUTION})) + p(\\text{HALT})\\log_{2}(p(\\text{HALT}))]$$\n$$H(S) = - [0.5 \\log_{2}(0.5) + 0.25 \\log_{2}(0.25) + 0.25 \\log_{2}(0.25)]$$\n\nWe evaluate the logarithm terms:\n$\\log_{2}(0.5) = \\log_{2}(2^{-1}) = -1$\n$\\log_{2}(0.25) = \\log_{2}(2^{-2}) = -2$\n\nNow substitute these values back into the entropy equation:\n$$H(S) = - [0.5(-1) + 0.25(-2) + 0.25(-2)]$$\n$$H(S) = - [-0.5 - 0.5 - 0.5]$$\n$$H(S) = - [-1.5] = 1.5 \\text{ bits/symbol}$$\n\nThis is the theoretical minimum average length for any code representing this source.\n\nStep 3: Calculate the average code length of the engineer's code.\nThe average code length, $L$, is the weighted average of the lengths of the codewords, where the weights are the probabilities of the symbols.\n$$L = \\sum_{i \\in S} p(i) l(i)$$\n\nSubstituting the given probabilities and code lengths:\n$$L = p(\\text{NORMAL})l(\\text{NORMAL}) + p(\\text{CAUTION})l(\\text{CAUTION}) + p(\\text{HALT})l(\\text{HALT})$$\n$$L = (0.5)(2) + (0.25)(2) + (0.25)(2)$$\n$$L = 1.0 + 0.5 + 0.5 = 2.0 \\text{ bits/symbol}$$\n\nThis is the actual average number of bits per symbol used by the engineer's fixed-length code.\n\nStep 4: Calculate the redundancy.\nThe redundancy, $R$, is the difference between the average code length and the entropy.\n$$R = L - H(S)$$\n\nSubstituting the values calculated in the previous steps:\n$$R = 2.0 \\text{ bits/symbol} - 1.5 \\text{ bits/symbol}$$\n$$R = 0.5 \\text{ bits/symbol}$$",
            "answer": "$$\\boxed{0.5}$$"
        },
        {
            "introduction": "Information is only useful if it can be transmitted reliably across a noisy channel. This final exercise examines a clever, early method for detecting errors in transmission—the two-dimensional parity check—which predates the more sophisticated codes of the modern era. By discovering the minimum number of bit-flips required to fool this error-detection scheme , you will gain a hands-on appreciation for the fundamental challenges of channel coding and the ingenuity required to ensure data integrity.",
            "id": "1629782",
            "problem": "In the early days of digital communication, before the development of more advanced techniques like Hamming codes, simple error detection schemes were employed. One such method involved arranging data bits into a two-dimensional grid and adding parity bits for each row and column.\n\nConsider a scheme where 9 data bits, denoted as $d_{ij}$ with $i, j \\in \\{1, 2, 3\\}$, are arranged in a $3 \\times 3$ grid. For each of the three rows, an additional parity bit $p_{i,c}$ is appended. For each of the three columns, an additional parity bit $p_{r,j}$ is appended. This creates a $4 \\times 3$ block of data and row-parity bits, and a $1 \\times 4$ block of column-parity bits (including a parity bit for the column of row-parity bits). The complete transmitted block is a $4 \\times 4$ grid of bits.\n\nAll parity bits are calculated using an even parity scheme, meaning the sum of bits in any complete row (3 data bits + 1 parity bit) or any complete column (3 data bits + 1 parity bit) must be an even number. An error occurs when one or more bits in the original 9-bit data block are flipped (0 becomes 1, or 1 becomes 0) during transmission. An error pattern is considered \"undetected\" if, after the bits are flipped, all row and column parity checks on the received $4 \\times 4$ block still pass.\n\nAssuming errors only occur in the 9 data bits, what is the minimum number of bit-flip errors that can result in an undetected error?",
            "solution": "Let $d_{ij}$, with $i,j \\in \\{1,2,3\\}$, denote the original data bits and let $e_{ij} \\in \\{0,1\\}$ indicate whether $d_{ij}$ is flipped during transmission ($e_{ij}=1$ means a flip at position $(i,j)$). Errors occur only in the data bits. For the even-parity checks on the received $4 \\times 4$ block to pass, the parity of each data row and each data column must be unchanged by the flips. This is equivalent to requiring that in each data row and each data column, the number of flips is even.\n\nFormally, the undetected error condition is that for each row $i \\in \\{1,2,3\\}$ and each column $j \\in \\{1,2,3\\}$:\n$$ \\sum_{j=1}^{3} e_{ij} \\equiv 0 \\pmod{2} \\quad \\text{and} \\quad \\sum_{i=1}^{3} e_{ij} \\equiv 0 \\pmod{2} $$\nLet the total number of flips be $w=\\sum_{i=1}^{3}\\sum_{j=1}^{3} e_{ij}$. First, $w$ must be an even number. This is because $w$ can be expressed as the sum of flips per row: $w = \\sum_{i=1}^{3} \\left( \\sum_{j=1}^{3} e_{ij} \\right)$. For the error to be undetected, each inner sum (the number of flips per row) must be an even number. The sum of even numbers is always even, so $w$ must be even. Hence, $w$ cannot be 1 or 3.\n\nConsider $w=2$. There are three possibilities for the locations of the two flips:\n(i) Same row, different columns: The affected row has a sum of 2 flips, which is even ($2 \\equiv 0 \\pmod{2}$). However, the two affected columns each have 1 flip, which is odd ($1 \\equiv 1 \\pmod{2}$), violating the column parity checks.\n(ii) Same column, different rows: The affected column has a sum of 2 flips (even), but the two affected rows each have 1 flip (odd), violating the row parity checks.\n(iii) Different rows and different columns: Two rows will have 1 flip each, and two columns will have 1 flip each, violating both row and column parity checks.\nTherefore, an undetected error with $w=2$ flips is impossible.\n\nNow consider $w=4$. Let's try to construct an error pattern. If we flip the bits forming a rectangle in the data grid, the parity conditions will be met. For example, choose two distinct rows $i_{1} \\neq i_{2}$ and two distinct columns $j_{1} \\neq j_{2}$. Set the flips $e_{i_{1}j_{1}}=1$, $e_{i_{1}j_{2}}=1$, $e_{i_{2}j_{1}}=1$, and $e_{i_{2}j_{2}}=1$, with all other $e_{ij}=0$.\nLet's check the parity:\n- For row $i_1$: Total flips = $e_{i_{1}j_{1}} + e_{i_{1}j_{2}} = 1 + 1 = 2$ (even).\n- For row $i_2$: Total flips = $e_{i_{2}j_{1}} + e_{i_{2}j_{2}} = 1 + 1 = 2$ (even).\n- For the third row: Total flips = 0 (even).\n- For column $j_1$: Total flips = $e_{i_{1}j_{1}} + e_{i_{2}j_{1}} = 1 + 1 = 2$ (even).\n- For column $j_2$: Total flips = $e_{i_{1}j_{2}} + e_{i_{2}j_{2}} = 1 + 1 = 2$ (even).\n- For the third column: Total flips = 0 (even).\nAll row and column parity checks on the data bits pass. Since the parity bits themselves were not flipped, the overall checks on the $4 \\times 4$ grid will also pass.\n\nTherefore, the minimal non-zero number of data-bit flips that can result in an undetected error is 4.",
            "answer": "$$\\boxed{4}$$"
        }
    ]
}