{
    "hands_on_practices": [
        {
            "introduction": "We begin our hands-on exploration with a foundational model of a stochastic process: a symmetric two-state Markov chain. This exercise  uses the hypothetical scenario of a neuronal ion channel to demonstrate how to calculate the entropy rate when the system has reached equilibrium. By mastering this simple case, where the stationary distribution is intuitive, you will build a strong foundation for tackling more complex systems with memory.",
            "id": "1621636",
            "problem": "A simplified model for a neuronal ion channel considers only two states: \"Open\" and \"Closed\". The state of the channel is observed at discrete time steps $t = 1, 2, 3, \\ldots$ and can be described by a stationary two-state Markov chain.\n\nLet State 1 be \"Open\" and State 2 be \"Closed\". At each time step, if the channel is in a particular state, it has a probability $p$ of transitioning to the other state. Consequently, it remains in its current state with a probability of $1-p$. The parameter $p$ is a constant such that $0  p  1$.\n\nThe sequence of states generated by this process carries information. The entropy rate, measured in bits per time step, quantifies the average amount of new information or uncertainty that the process generates at each step. Assuming the process has reached its stationary distribution, determine the entropy rate $H$ of this two-state Markov chain. Express your answer as a symbolic function of $p$.",
            "solution": "Let the two-state Markov chain have transition matrix\n$$\nP=\\begin{pmatrix}\n1-p  p \\\\\np  1-p\n\\end{pmatrix},\n$$\nwith $0p1$. The stationary distribution $\\pi=(\\pi_{1},\\pi_{2})$ satisfies $\\pi=\\pi P$ and $\\pi_{1}+\\pi_{2}=1$. Writing $\\pi_{1}=x$ and $\\pi_{2}=1-x$, the first stationarity equation is\n$$\nx=x(1-p)+(1-x)p \\quad \\Rightarrow \\quad x=x - 2xp + p \\quad \\Rightarrow \\quad 2xp=p,\n$$\nwhich, since $p\\neq 0$, gives $x=\\frac{1}{2}$. Thus $\\pi_{1}=\\pi_{2}=\\frac{1}{2}$.\n\nFor a stationary first-order Markov chain, the entropy rate in bits per time step is\n$$\nH=H(X_{t+1}\\mid X_{t})=\\sum_{i}\\pi_{i}\\left[-\\sum_{j}P_{ij}\\log_{2}(P_{ij})\\right].\n$$\nHere, each row of $P$ has the same two-point distribution: transition to the other state with probability $p$ and remain with probability $1-p$. Therefore,\n$$\nH=\\pi_{1}\\left[-(1-p)\\log_{2}(1-p)-p\\log_{2}(p)\\right]+\\pi_{2}\\left[-(1-p)\\log_{2}(1-p)-p\\log_{2}(p)\\right].\n$$\nUsing $\\pi_{1}=\\pi_{2}=\\frac{1}{2}$, this simplifies to\n$$\nH=-(1-p)\\log_{2}(1-p)-p\\log_{2}(p).\n$$\nThus, the entropy rate is the binary entropy function of $p$.",
            "answer": "$$\\boxed{-p \\log_{2}(p) - (1-p)\\log_{2}(1-p)}$$"
        },
        {
            "introduction": "Next, we explore a system with built-in constraints, a common feature in many real-world processes from language to data compression. This problem  models a source where certain symbols are forbidden from following others, creating an asymmetric Markov chain. This exercise will challenge you to first find the non-uniform stationary distribution before calculating the entropy rate, revealing how determinism in one part of a system impacts its overall average uncertainty.",
            "id": "1621587",
            "problem": "A specialized digital information source generates a binary sequence of symbols, '0' and '1'. The generation process is governed by two rules:\n1. Any time a '1' is generated, the next symbol must be a '0'.\n2. After a '0' is generated, the probability that the next symbol will be a '1' is a constant value $p$, where $0  p  1$.\n\nAssuming the process has been running for a long time and has reached a stationary state, determine the entropy rate of this source. Express your answer as an analytic function of $p$, in units of bits per symbol.",
            "solution": "The source generates a binary sequence under a first-order Markov rule:\n- From state 0: the next symbol is 1 with probability $p$ and 0 with probability $1-p$.\n- From state 1: the next symbol is deterministically 0.\n\nThus the transition matrix $P$ on states $\\{0,1\\}$ is\n$$\nP=\\begin{pmatrix}\n1-p  p\\\\\n1  0\n\\end{pmatrix}.\n$$\nLet $(\\mu_{0},\\mu_{1})$ be the stationary distribution, satisfying $\\mu=\\mu P$ and $\\mu_{0}+\\mu_{1}=1$. Writing the stationarity equations,\n$$\n\\mu_{0}=\\mu_{0}(1-p)+\\mu_{1}\\cdot 1,\\qquad \\mu_{1}=\\mu_{0}p+\\mu_{1}\\cdot 0.\n$$\nFrom the second equation, $\\mu_{1}=\\mu_{0}p$. Using normalization,\n$$\n\\mu_{0}+\\mu_{1}=\\mu_{0}+\\mu_{0}p=\\mu_{0}(1+p)=1 \\;\\Rightarrow\\; \\mu_{0}=\\frac{1}{1+p},\\quad \\mu_{1}=\\frac{p}{1+p}.\n$$\n\nFor a stationary first-order Markov source, the entropy rate is the stationary average of the next-symbol conditional entropy:\n$$\nH=-\\sum_{i\\in\\{0,1\\}}\\mu_{i}\\sum_{j\\in\\{0,1\\}}P_{ij}\\log_{2}(P_{ij})\n=\\sum_{i\\in\\{0,1\\}}\\mu_{i}H(X_{n+1}\\mid X_{n}=i).\n$$\nGiven $X_{n}=0$, the next symbol has distribution $\\{1-p,p\\}$, so\n$$\nH(X_{n+1}\\mid X_{n}=0)=-(1-p)\\log_{2}(1-p)-p\\log_{2}(p).\n$$\nGiven $X_{n}=1$, the next symbol is deterministically $0$, so $H(X_{n+1}\\mid X_{n}=1)=0$. Therefore,\n$$\nH=\\mu_{0}\\left[-(1-p)\\log_{2}(1-p)-p\\log_{2}(p)\\right]\n=\\frac{1}{1+p}\\left[-(1-p)\\log_{2}(1-p)-p\\log_{2}(p)\\right].\n$$\nThis is the entropy rate in bits per symbol.",
            "answer": "$$\\boxed{\\frac{1}{1+p}\\left[-p\\log_{2}(p)-(1-p)\\log_{2}(1-p)\\right]}$$"
        },
        {
            "introduction": "Our final practice with two-state systems culminates in the most general case, modeled here as a communication channel with memory-dependent errors. This capstone exercise  requires a complete application of the principles you've learned: calculating the stationary distribution for a general transition matrix and then finding the weighted average of the conditional entropies. Successfully navigating this problem demonstrates your ability to determine the fundamental information rate for a wide class of two-state stationary processes.",
            "id": "1621600",
            "problem": "Consider a simplified model for a digital communication channel that transmits a long sequence of binary symbols. This channel exhibits memory, meaning the probability of an error in transmission for a given symbol depends on whether the previous symbol was transmitted correctly or not.\n\nLet the state of the channel for each symbol transmission be described by a stochastic process $\\{X_n\\}_{n=1}^{\\infty}$, where $X_n=0$ if the $n$-th symbol is transmitted correctly, and $X_n=1$ if it is transmitted with an error (i.e., the bit is flipped).\n\nThe behavior of the channel is governed by the following conditional probabilities:\n- If the $(n-1)$-th symbol was transmitted correctly ($X_{n-1}=0$), the probability that the $n$-th symbol is transmitted with an error ($X_n=1$) is $\\alpha$.\n- If the $(n-1)$-th symbol was transmitted with an error ($X_{n-1}=1$), the probability that the $n$-th symbol is transmitted correctly ($X_n=0$) is $\\beta$.\n\nAssume that the process has been running for a long time, such that it has reached a stationary state. The parameters $\\alpha$ and $\\beta$ are constants in the open interval $(0, 1)$.\n\nUsing base-2 logarithms for entropy calculations, determine the entropy rate of the error process $\\{X_n\\}$. The entropy rate quantifies the average uncertainty per symbol in the stationary state. Express your final answer as a symbolic expression in terms of $\\alpha$ and $\\beta$.",
            "solution": "The error process $\\{X_n\\}$ can be modeled as a discrete-time Markov chain with two states: state 0 (correct transmission) and state 1 (error). The problem provides the conditional probabilities that define the transitions between these states.\n\nFirst, we construct the transition probability matrix $P$, where $P_{ij} = P(X_n = j | X_{n-1} = i)$.\nFrom the problem statement:\n- $P(X_n=1 | X_{n-1}=0) = P_{01} = \\alpha$. Since there are only two outcomes, $P(X_n=0 | X_{n-1}=0) = P_{00} = 1 - \\alpha$.\n- $P(X_n=0 | X_{n-1}=1) = P_{10} = \\beta$. Similarly, $P(X_n=1 | X_{n-1}=1) = P_{11} = 1 - \\beta$.\n\nSo, the transition probability matrix is:\n$$ P = \\begin{pmatrix} 1-\\alpha  \\alpha \\\\ \\beta  1-\\beta \\end{pmatrix} $$\n\nThe entropy rate $H(\\mathcal{X})$ of a stationary Markov chain is given by the conditional entropy of the next state given the current state:\n$$ H(\\mathcal{X}) = H(X_n | X_{n-1}) = \\sum_{i \\in \\{0, 1\\}} \\pi_i H(X_n | X_{n-1}=i) $$\nwhere $\\pi = (\\pi_0, \\pi_1)$ is the stationary distribution of the chain, with $\\pi_i = P(X_{n-1}=i)$.\n\nTo find the stationary distribution, we solve the system of equations $\\pi P = \\pi$ and $\\pi_0 + \\pi_1 = 1$.\nThe equation $\\pi P = \\pi$ expands to:\n$$ \\begin{pmatrix} \\pi_0  \\pi_1 \\end{pmatrix} \\begin{pmatrix} 1-\\alpha  \\alpha \\\\ \\beta  1-\\beta \\end{pmatrix} = \\begin{pmatrix} \\pi_0  \\pi_1 \\end{pmatrix} $$\nThis gives two linear equations:\n1. $\\pi_0(1-\\alpha) + \\pi_1\\beta = \\pi_0$\n2. $\\pi_0\\alpha + \\pi_1(1-\\beta) = \\pi_1$\n\nFrom the first equation:\n$$ \\pi_0 - \\pi_0\\alpha + \\pi_1\\beta = \\pi_0 \\implies \\pi_1\\beta = \\pi_0\\alpha $$\n(The second equation gives the same relationship).\nNow, we use the normalization condition $\\pi_1 = 1 - \\pi_0$:\n$$ (1-\\pi_0)\\beta = \\pi_0\\alpha $$\n$$ \\beta - \\pi_0\\beta = \\pi_0\\alpha $$\n$$ \\beta = \\pi_0(\\alpha + \\beta) $$\nSolving for $\\pi_0$, we get:\n$$ \\pi_0 = \\frac{\\beta}{\\alpha + \\beta} $$\nAnd for $\\pi_1$:\n$$ \\pi_1 = 1 - \\pi_0 = 1 - \\frac{\\beta}{\\alpha + \\beta} = \\frac{\\alpha}{\\alpha + \\beta} $$\nSince $\\alpha, \\beta \\in (0,1)$, the denominator $\\alpha+\\beta$ is non-zero.\n\nNext, we calculate the conditional entropies $H(X_n | X_{n-1}=i)$. These are the entropies of the rows of the transition matrix $P$.\nFor $i=0$, the conditional probability distribution is $(P_{00}, P_{01}) = (1-\\alpha, \\alpha)$. The conditional entropy is:\n$$ H(X_n | X_{n-1}=0) = -\\sum_{j \\in \\{0,1\\}} P_{0j} \\log_2(P_{0j}) = -(1-\\alpha)\\log_2(1-\\alpha) - \\alpha\\log_2(\\alpha) $$\nThis is the binary entropy function, sometimes denoted $H_b(\\alpha)$.\n\nFor $i=1$, the conditional probability distribution is $(P_{10}, P_{11}) = (\\beta, 1-\\beta)$. The conditional entropy is:\n$$ H(X_n | X_{n-1}=1) = -\\sum_{j \\in \\{0,1\\}} P_{1j} \\log_2(P_{1j}) = -\\beta\\log_2(\\beta) - (1-\\beta)\\log_2(1-\\beta) $$\nThis is the binary entropy function $H_b(\\beta)$.\n\nFinally, we substitute the stationary probabilities and conditional entropies into the formula for the entropy rate:\n$$ H(\\mathcal{X}) = \\pi_0 H(X_n | X_{n-1}=0) + \\pi_1 H(X_n | X_{n-1}=1) $$\n$$ H(\\mathcal{X}) = \\frac{\\beta}{\\alpha + \\beta} \\left( -(1-\\alpha)\\log_2(1-\\alpha) - \\alpha\\log_2(\\alpha) \\right) + \\frac{\\alpha}{\\alpha + \\beta} \\left( -\\beta\\log_2(\\beta) - (1-\\beta)\\log_2(1-\\beta) \\right) $$\nThis is the final expression for the entropy rate of the error process.",
            "answer": "$$\\boxed{\\frac{\\beta}{\\alpha + \\beta} \\left( -(1-\\alpha)\\log_{2}(1-\\alpha) - \\alpha\\log_{2}(\\alpha) \\right) + \\frac{\\alpha}{\\alpha + \\beta} \\left( -\\beta\\log_{2}(\\beta) - (1-\\beta)\\log_{2}(1-\\beta) \\right)}$$"
        }
    ]
}