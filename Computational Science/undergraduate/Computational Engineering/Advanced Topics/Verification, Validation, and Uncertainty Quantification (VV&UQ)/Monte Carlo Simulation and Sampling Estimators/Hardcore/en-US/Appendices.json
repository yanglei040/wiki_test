{
    "hands_on_practices": [
        {
            "introduction": "The most intuitive application of Monte Carlo is the \"hit-or-miss\" method, which allows us to estimate areas and volumes. Imagine throwing darts randomly at a square board that has a circle drawn inside it; the ratio of darts landing inside the circle to the total number of darts thrown gives an estimate of the ratio of the two areas. This exercise  extends this simple idea to higher dimensions, providing a hands-on way to explore the \"curse of dimensionality,\" a counter-intuitive phenomenon where the volume of a hypersphere becomes vanishingly small compared to its bounding hypercube as the dimension increases.",
            "id": "2415275",
            "problem": "You are to study how the ratio of a $d$-dimensional hypersphereâ€™s volume to that of a $d$-dimensional hypercube changes with dimension $d$. Let $C_d = [-1,1]^d \\subset \\mathbb{R}^d$ denote the hypercube of side length $2$ centered at the origin, and let $S_d = \\{x \\in \\mathbb{R}^d : \\lVert x \\rVert_2 \\le 1\\}$ denote the unit hypersphere in $d$-dimensional Euclidean space. Define the ratio $R(d)$ by\n$$\nR(d) = \\frac{\\mathrm{Vol}(S_d)}{\\mathrm{Vol}(C_d)}.\n$$\nFor a given positive integer $N$ and integer seed $s$, let $X_1,\\dots,X_N$ be independent and identically distributed random vectors uniformly distributed on $C_d$, determined by a pseudo-random number generator initialized with seed $s$. Define the estimator\n$$\n\\widehat{R}_N(d,s) \\equiv \\frac{1}{N} \\sum_{i=1}^N \\mathbf{1}\\{\\lVert X_i \\rVert_2 \\le 1\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function and $\\lVert \\cdot \\rVert_2$ is the Euclidean norm. For each test case below, compute the single real number $\\widehat{R}_N(d,s)$.\n\nAll answers must be dimensionless real numbers. Round each result to exactly $6$ decimal places.\n\nTest suite (each item is $(d,N,s)$):\n- $(1, 20000, 7)$\n- $(2, 100000, 11)$\n- $(8, 300000, 2025)$\n- $(12, 500000, 123)$\n- $(20, 500000, 99991)$\n\nFinal output format: Your program should produce a single line of output containing the results, in the same order as the test suite, as a comma-separated list enclosed in square brackets. For example, a generic format is [$x_1,x_2,x_3,x_4,x_5$], where each $x_i$ is a real number rounded to exactly $6$ decimal places as specified above.",
            "solution": "The problem requires the computation of a Monte Carlo estimator for the ratio of the volume of a $d$-dimensional hypersphere to that of a $d$-dimensional hypercube. I shall first validate the problem statement and then provide a complete solution based on fundamental principles of computational science.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem provides the following definitions and parameters:\n- Hypercube: $C_d = [-1,1]^d \\subset \\mathbb{R}^d$. This is a hypercube of side length $2$ centered at the origin.\n- Hypersphere: $S_d = \\{x \\in \\mathbb{R}^d : \\lVert x \\rVert_2 \\le 1\\}$. This is a unit hypersphere centered at the origin, fully inscribed within the hypercube $C_d$.\n- Ratio of volumes: $R(d) = \\frac{\\mathrm{Vol}(S_d)}{\\mathrm{Vol}(C_d)}$.\n- Random vectors: $X_1,\\dots,X_N$ are independent and identically distributed (i.i.d.) random vectors uniformly distributed on $C_d$. Their generation is determined by a pseudo-random number generator initialized with an integer seed $s$.\n- Estimator: $\\widehat{R}_N(d,s) \\equiv \\frac{1}{N} \\sum_{i=1}^N \\mathbf{1}\\{\\lVert X_i \\rVert_2 \\le 1\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function and $\\lVert \\cdot \\rVert_2$ is the Euclidean norm.\n- Test Cases (as tuples of $(d, N, s)$):\n    - $(1, 20000, 7)$\n    - $(2, 100000, 11)$\n    - $(8, 300000, 2025)$\n    - $(12, 500000, 123)$\n    - $(20, 500000, 99991)$\n- Output Specification: Each numerical result must be rounded to exactly $6$ decimal places.\n\n**Step 2: Validate Using Extracted Givens**\n\nI will now assess the validity of the problem based on the established criteria.\n- **Scientifically Grounded**: The problem is a classic application of Monte Carlo integration, a fundamental and widely used technique in computational physics, engineering, and applied mathematics. The definitions for the hypersphere, hypercube, and the Monte Carlo estimator are standard and mathematically correct. The problem rests on the principles of probability theory and statistical estimation. It is entirely sound.\n- **Well-Posed**: The problem is well-posed. For each test case, all necessary parameters ($d$, $N$, $s$) are provided. The use of a seeded pseudo-random number generator ensures that the sequence of random vectors $X_i$ is deterministic, leading to a unique and computable value for the estimator $\\widehat{R}_N(d,s)$.\n- **Objective**: The problem is stated using precise, unambiguous mathematical language. It is free from any subjective or speculative content.\n\nBased on this analysis, the problem does not exhibit any of the flaws listed in the validation checklist. It is scientifically sound, well-posed, objective, and directly related to the specified topic of Monte Carlo methods.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. I will proceed with formulating and presenting the solution.\n\n### Solution\n\nThe objective is to compute the estimator $\\widehat{R}_N(d,s)$ for several test cases. This estimator approximates the ratio $R(d) = \\frac{\\mathrm{Vol}(S_d)}{\\mathrm{Vol}(C_d)}$. The methodology is based on the principles of Monte Carlo integration.\n\nLet $X$ be a random vector uniformly distributed in the hypercube $C_d$. The probability that this vector lies within the inscribed hypersphere $S_d$ is given by the ratio of their volumes:\n$$\nP(X \\in S_d) = \\frac{\\int_{C_d} \\mathbf{1}\\{x \\in S_d\\} dx}{\\int_{C_d} 1 dx} = \\frac{\\mathrm{Vol}(S_d)}{\\mathrm{Vol}(C_d)} = R(d)\n$$\nwhere $\\mathbf{1}\\{x \\in S_d\\}$ is the indicator function that is $1$ if $x \\in S_d$ and $0$ otherwise.\n\nThe problem defines an estimator for this probability. We generate $N$ independent random vectors $X_1, X_2, \\dots, X_N$, each uniformly distributed in $C_d$. For each vector $X_i$, we define a Bernoulli random variable $Z_i = \\mathbf{1}\\{X_i \\in S_d\\}$. The condition $X_i \\in S_d$ is equivalent to $\\lVert X_i \\rVert_2 \\le 1$. The expectation of each $Z_i$ is precisely the probability $R(d)$:\n$$\nE[Z_i] = 1 \\cdot P(Z_i=1) + 0 \\cdot P(Z_i=0) = P(X_i \\in S_d) = R(d)\n$$\nThe estimator $\\widehat{R}_N(d,s)$ is the sample mean of these $N$ Bernoulli trials:\n$$\n\\widehat{R}_N(d,s) = \\frac{1}{N} \\sum_{i=1}^N Z_i = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{1}\\{\\lVert X_i \\rVert_2 \\le 1\\}\n$$\nBy the Law of Large Numbers, this sample mean converges to the true expectation as $N$ approaches infinity:\n$$\n\\lim_{N \\to \\infty} \\widehat{R}_N(d,s) = R(d)\n$$\nThis provides the theoretical justification for the method. The prescribed algorithm for computing $\\widehat{R}_N(d,s)$ for a given set of parameters $(d, N, s)$ is as follows:\n\n1.  **Initialization**: Initialize a pseudo-random number generator with the specified integer seed $s$. This ensures reproducibility of the results.\n\n2.  **Sample Generation**: Generate a set of $N$ random vectors, $\\{X_1, \\dots, X_N\\}$, where each $X_i = (x_{i1}, x_{i2}, \\dots, x_{id})$ is a point in $\\mathbb{R}^d$. To ensure the vectors are uniformly distributed in the hypercube $C_d = [-1, 1]^d$, each component $x_{ij}$ must be drawn from the uniform distribution $U[-1, 1]$. This is achieved computationally by generating an $N \\times d$ matrix of floating-point numbers where each entry is an independent sample from $U[-1, 1]$.\n\n3.  **Condition Checking**: For each vector $X_i$, determine if it lies inside the unit hypersphere $S_d$. This is done by checking if its Euclidean norm is less than or equal to $1$. For computational efficiency, it is preferable to check the squared norm:\n    $$\n    \\lVert X_i \\rVert_2 \\le 1 \\iff \\lVert X_i \\rVert_2^2 \\le 1^2 \\iff \\sum_{j=1}^d x_{ij}^2 \\le 1\n    $$\n    This avoids the computationally expensive square root operation.\n\n4.  **Estimation**: Count the number of vectors, let us call it $M$, that satisfy the condition from the previous step. This count is equivalent to the sum of the indicator functions: $M = \\sum_{i=1}^N \\mathbf{1}\\{\\lVert X_i \\rVert_2^2 \\le 1\\}$. The Monte Carlo estimate is then calculated as the ratio:\n    $$\n    \\widehat{R}_N(d,s) = \\frac{M}{N}\n    $$\n5.  **Finalization**: The procedure is repeated for each test case $(d, N, s)$, and the resulting estimate is rounded to $6$ decimal places as required. The phenomenon of the \"curse of dimensionality\" will become apparent, where for high dimensions $d$, the volume of the hypersphere becomes exceedingly small relative to the hypercube, and the estimate $\\widehat{R}_N(d,s)$ rapidly approaches zero. For $d=20$, it is statistically probable that $M=0$, yielding an estimate of $0$.\n\nThis completes the theoretical framework and algorithmic design. The implementation will proceed based on this logic.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Monte Carlo estimate of the ratio of a d-dimensional hypersphere's\n    volume to that of its enclosing hypercube for a given set of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is in the format (d, N, s), where:\n    # d: dimension of the space\n    # N: number of samples\n    # s: seed for the pseudo-random number generator\n    test_cases = [\n        (1, 20000, 7),\n        (2, 100000, 11),\n        (8, 300000, 2025),\n        (12, 500000, 123),\n        (20, 500000, 99991)\n    ]\n\n    results = []\n    for d, N, s in test_cases:\n        # Initialize the pseudo-random number generator with the specified seed for reproducibility.\n        rng = np.random.default_rng(s)\n\n        # Generate N random vectors in d-dimensional space.\n        # Each component of each vector is drawn from the uniform distribution on [-1, 1].\n        # This creates an N x d matrix of points uniformly sampled from the hypercube C_d.\n        # The size parameter is (number of rows, number of columns), i.e., (N, d).\n        samples = rng.uniform(-1, 1, size=(N, d))\n\n        # For each sample vector, calculate its squared Euclidean norm.\n        # This is more computationally efficient than calculating the norm itself,\n        # as it avoids the square root operation.\n        # The condition ||x||_2 = 1 is equivalent to ||x||_2^2 = 1.\n        # The sum is performed along axis=1 to sum the squared components for each vector (row).\n        squared_norms = np.sum(samples**2, axis=1)\n\n        # Count the number of samples whose squared norm is less than or equal to 1.\n        # These are the points that lie within the unit hypersphere S_d.\n        count_inside = np.sum(squared_norms = 1)\n\n        # The estimator is the ratio of points inside the hypersphere to the total number of points.\n        # This ratio approximates the volume ratio R(d).\n        estimate = count_inside / N\n\n        # Per the problem specification, the final result must be rounded to 6 decimal places.\n        # We format the number as a string to ensure exactly 6 decimal places are shown.\n        results.append(f\"{estimate:.6f}\")\n\n    # Final print statement in the exact required format: a comma-separated list in brackets.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Monte Carlo methods are not limited to geometric problems; their true power lies in their ability to estimate the expected value of any random variable, which can represent quantities across diverse fields. This practice problem  moves from geometry to number theory, challenging you to estimate the probability that a randomly chosen integer is \"square-free.\" By sampling integers and testing this property, you will reinforce the core principle that a probability can be estimated by the frequency of an event in a large number of random trials.",
            "id": "2415243",
            "problem": "You are to write a complete program that computes estimates of a probability defined from first principles. A positive integer $k$ is called square-free if there is no integer $d \\ge 2$ such that $d^2$ divides $k$. Let $p$ denote the limiting probability that a uniformly selected positive integer is square-free, understood as the natural density $$p = \\lim_{n \\to \\infty} \\frac{1}{n}\\sum_{k=1}^{n} \\mathbf{1}\\{\\text{$k$ is square-free}\\}.$$ For finite $n$, define the finite-population probability $$p_n = \\frac{1}{n}\\sum_{k=1}^{n} \\mathbf{1}\\{\\text{$k$ is square-free}\\}.$$ For each test case, you must output a single real number that is an estimate of $p_n$ based on random sampling subject to the following condition: the computation for a test case with parameters $(n,m,s)$ must depend only on exactly $m$ independent random variates drawn from the discrete uniform distribution on $\\{1,2,\\dots,n\\}$ produced by a Pseudo-Random Number Generator (PRNG) initialized with seed $s$. No external input is provided.\n\nYour program must take no input and must process the following test suite of ordered triples $(n,m,s)$:\n- $(n,m,s) = (10^6, 200000, 17)$\n- $(n,m,s) = (1, 10000, 99)$\n- $(n,m,s) = (100, 5000, 2024)$\n- $(n,m,s) = (10^5, 150000, 123456)$\n- $(n,m,s) = (50, 10000, 314159)$\n\nRequirements for the final output:\n- For each test case, produce a single real number equal to your estimate of $p_n$, rounded to $6$ decimal places.\n- Aggregate the results for all test cases, in the same order as listed, into a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[x_1,x_2,x_3,x_4,x_5]$ where each $x_i$ is a real number with exactly $6$ digits after the decimal point.",
            "solution": "The posed problem must first be validated for scientific soundness, clarity, and completeness.\n\n**Step 1: Extract Givens**\n- **Definition**: A positive integer $k$ is square-free if there is no integer $d \\ge 2$ such that $d^2$ divides $k$.\n- **Limiting Probability**: $p = \\lim_{n \\to \\infty} \\frac{1}{n}\\sum_{k=1}^{n} \\mathbf{1}\\{\\text{$k$ is square-free}\\}$.\n- **Finite-Population Probability**: $p_n = \\frac{1}{n}\\sum_{k=1}^{n} \\mathbf{1}\\{\\text{$k$ is square-free}\\}$.\n- **Task**: For each parameter set $(n, m, s)$, estimate $p_n$.\n- **Methodology**: The estimate must be derived from exactly $m$ independent random variates drawn from the discrete uniform distribution on $\\{1, 2, \\dots, n\\}$.\n- **Reproducibility**: The Pseudo-Random Number Generator (PRNG) must be initialized with a given seed $s$.\n- **Test Cases**:\n    - $(n, m, s) = (10^6, 200000, 17)$\n    - $(n, m, s) = (1, 10000, 99)$\n    - $(n, m, s) = (100, 5000, 2024)$\n    - $(n, m, s) = (10^5, 150000, 123456)$\n    - $(n, m, s) = (50, 10000, 314159)$\n- **Output Format**: A single line containing a comma-separated list of estimates, each rounded to $6$ decimal places.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed against the required criteria:\n- **Scientifically Grounded**: The concept of square-free integers is a fundamental topic in number theory. The use of Monte Carlo simulation to estimate a probability is a standard and robust technique in computational science and engineering. The problem is firmly rooted in established mathematical and computational principles.\n- **Well-Posed**: The problem provides a clear and unambiguous definition of the quantity to be estimated, $p_n$. It specifies the population ($\\{1, \\dots, n\\}$), the sampling distribution (uniform), the sample size ($m$), and the PRNG seed ($s$). This is a complete specification for a Monte Carlo experiment, which guarantees a unique, deterministic result for each test case.\n- **Objective**: The problem is stated in precise, mathematical language, free from any subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. It is a well-defined computational problem based on sound mathematical principles. A complete solution will now be developed.\n\n**Principle-Based Solution**\n\nThe objective is to estimate the probability, $p_n$, that a positive integer selected uniformly at random from the set $\\{1, 2, \\dots, n\\}$ is square-free. This probability is defined as the mean of an indicator random variable. Let $K$ be a random variable with a discrete uniform distribution on $\\{1, 2, \\dots, n\\}$. Then,\n$$p_n = \\mathbb{E}[\\mathbf{1}\\{K \\text{ is square-free}\\}]$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\nThe Monte Carlo method provides a powerful way to estimate such an expectation. By the Law of Large Numbers, the sample mean of independent and identically distributed random variables converges to the true expectation. We generate $m$ independent samples, $k_1, k_2, \\dots, k_m$, from the distribution of $K$. The Monte Carlo estimator for $p_n$, denoted $\\hat{p}_{n,m}$, is the sample mean of the indicator function evaluated at these samples:\n$$\\hat{p}_{n,m} = \\frac{1}{m} \\sum_{i=1}^{m} \\mathbf{1}\\{k_i \\text{ is square-free}\\}$$\nThis is equivalent to counting the number of samples that are square-free and dividing by the total number of samples, $m$.\n\nThe core computational task is to implement a function that correctly determines if an integer $k$ is square-free. A positive integer $k$ is square-free if it is not divisible by any perfect square greater than $1$. It is sufficient to check for divisibility by the squares of prime numbers. If $k$ were divisible by a composite square, $(ab)^2$, it would also be divisible by the squares of the prime factors of $ab$. Therefore, $k$ is square-free if and only if for every prime number $p$, $p^2$ does not divide $k$.\n\nFor a given integer $k$, we only need to perform this check for primes $p$ such that $p^2 \\le k$. To implement this test efficiently across many calls, it is advantageous to pre-compute a list of all primes up to $\\sqrt{\\max(n)}$, where $\\max(n)$ is the largest value of $n$ across all test cases. In this problem, $\\max(n) = 10^6$, so we require primes up to $\\sqrt{10^6} = 1000$. The Sieve of Eratosthenes is a classical and efficient algorithm for this purpose.\n\nThe overall algorithm for each test case $(n, m, s)$ is as follows:\n1.  **Pre-computation**: Generate a list of all prime numbers up to $1000$ using the Sieve of Eratosthenes. This is done once for all test cases.\n2.  **Initialization**: Initialize a Pseudo-Random Number Generator (PRNG) with the specified seed $s$.\n3.  **Sampling**: Draw $m$ random integers, $k_1, k_2, \\dots, k_m$, from the discrete uniform distribution on $\\{1, 2, \\dots, n\\}$.\n4.  **Counting**: Initialize a counter to zero. For each sample $k_i$, test if it is square-free using the pre-computed list of primes. If $k_i$ is square-free, increment the counter.\n5.  **Estimation**: Compute the estimate $\\hat{p}_{n,m}$ as the final count divided by the sample size $m$.\n6.  **Formatting**: Round the estimate to $6$ decimal places.\n\nThis procedure is deterministic for a given test case and will produce a single, verifiable estimate of $p_n$. The results for all test cases are then aggregated into the specified final output format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sieve_of_eratosthenes(limit):\n    \"\"\"\n    Generates a list of prime numbers up to a given limit using the Sieve of Eratosthenes.\n    \"\"\"\n    if limit  2:\n        return []\n    \n    prime_flags = [True] * (limit + 1)\n    prime_flags[0] = prime_flags[1] = False\n    \n    for i in range(2, int(np.sqrt(limit)) + 1):\n        if prime_flags[i]:\n            for multiple in range(i * i, limit + 1, i):\n                prime_flags[multiple] = False\n                \n    primes = [i for i, is_p in enumerate(prime_flags) if is_p]\n    return primes\n\ndef is_square_free(k, primes):\n    \"\"\"\n    Checks if an integer k is square-free using a pre-computed list of primes.\n    A number is square-free if it is not divisible by any square of a prime.\n    \"\"\"\n    if k == 1:\n        return True\n    \n    for p in primes:\n        p_squared = p * p\n        if p_squared  k:\n            # If p^2  k, no larger prime's square can be a factor.\n            break\n        if k % p_squared == 0:\n            return False\n            \n    # If no prime square up to k divides k, it is square-free.\n    return True\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and compute Monte Carlo estimates.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (10**6, 200000, 17),\n        (1, 10000, 99),\n        (100, 5000, 2024),\n        (10**5, 150000, 123456),\n        (50, 10000, 314159)\n    ]\n\n    # Pre-compute primes required for the largest n in the test suite.\n    # The check is efficient if we have primes up to sqrt(max_n).\n    max_n = max(tc[0] for tc in test_cases if tc[0]  1) if test_cases else 1\n    prime_limit = int(np.sqrt(max_n))\n    primes = sieve_of_eratosthenes(prime_limit)\n\n    results = []\n    for n, m, s in test_cases:\n        # Handle the trivial case where n=1. The only possible sample is 1,\n        # which is square-free. The probability is exactly 1.\n        if n == 1:\n            results.append(1.0)\n            continue\n            \n        # Initialize the random number generator with the specified seed.\n        rng = np.random.default_rng(seed=s)\n        \n        # Generate m independent random integers from {1, 2, ..., n}.\n        samples = rng.integers(low=1, high=n, size=m, endpoint=True)\n        \n        # Count how many of the samples are square-free.\n        square_free_count = 0\n        for k in samples:\n            if is_square_free(k, primes):\n                square_free_count += 1\n                \n        # The estimate for p_n is the fraction of square-free samples.\n        estimate = square_free_count / m\n        results.append(estimate)\n\n    # Format results to 6 decimal places and print in the required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While straightforward Monte Carlo integration is powerful, it can be inefficient for integrands with sharp peaks or singularities, leading to high variance and slow convergence. This is where variance reduction techniques like importance sampling become essential, as the core idea is to sample more from the \"important\" regions of the function's domain, thereby focusing computational effort where it matters most. This exercise  guides you through developing an importance sampling strategy to accurately integrate a function with endpoint singularities, demonstrating how a clever choice of sampling distribution can dramatically improve an estimator's performance.",
            "id": "2415223",
            "problem": "Construct a Monte Carlo importance sampling estimator to evaluate integrals of the form $$I=\\int_{0}^{1} x^{-a}(1-x)^{-b}\\,g(x)\\,dx$$, where $0 \\lt a \\lt 1$, $0 \\le b \\lt 1$, and $g(x)$ is a bounded, continuous function on $[0,1]$. Begin from the core definition of Monte Carlo integration and importance sampling: if $p(x)$ is any probability density function that is strictly positive on $(0,1)$, then $$I=\\int_{0}^{1} \\frac{x^{-a}(1-x)^{-b}\\,g(x)}{p(x)}\\,p(x)\\,dx=\\mathbb{E}_{p}\\!\\left[\\frac{x^{-a}(1-x)^{-b}\\,g(X)}{p(X)}\\right]$$, where $X \\sim p$. Your tasks are:\n- Derive, from first principles, conditions on $p(x)$ that ensure the estimator is unbiased and has finite variance for the given class of integrands with endpoint singularities.\n- Choose a density $p(x)$ that captures the endpoint behavior of $x^{-a}(1-x)^{-b}$ in a principled way and justify your choice from the structure of the integrand and the definition of the Beta function $B(\\alpha,\\beta)=\\int_{0}^{1} x^{\\alpha-1}(1-x)^{\\beta-1}\\,dx$ with $\\alpha \\gt 0$ and $\\beta \\gt 0$.\n- Translate your choice into a concrete, algorithmically implementable sampling scheme that produces an unbiased estimator for $I$ without numerical instability at the endpoints.\n\nImplement a complete, runnable program that:\n- Uses a single shared pseudorandom seed $s=20231105$ for reproducibility.\n- For each test case below, draws an independent sample of size $N$ from your chosen $p(x)$ and returns a single Monte Carlo estimate $\\widehat{I}$ of $I$.\n- Uses angles in radians for all trigonometric functions.\n- Produces exactly one line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases.\n\nTest suite (each case specifies $(a,b,g,N)$):\n- Case $1$: $a=\\tfrac{1}{2}$, $b=0$, $g(x)=\\exp(x)$, $N=200000$.\n- Case $2$: $a=0.9$, $b=0$, $g(x)=\\cos(x)$, $N=600000$.\n- Case $3$: $a=0.49$, $b=0.49$, $g(x)=\\sin(3x)$, $N=400000$.\n- Case $4$: $a=0.99$, $b=0$, $g(x)=1$, $N=100000$.\n- Case $5$: $a=0.1$, $b=0.9$, $g(x)=\\exp(2x)$, $N=500000$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ is the Monte Carlo estimate for case $i$. No physical units are involved, and the outputs are real numbers. Trigonometric functions must use angles in radians. Do not read any input; the program must run as is and use only the specified seed and the specified sample sizes.",
            "solution": "The problem statement is subjected to validation before any attempt at a solution.\n\n**Step 1: Extracted Givens**\n- The integral to be evaluated is of the form $I=\\int_{0}^{1} x^{-a}(1-x)^{-b}\\,g(x)\\,dx$.\n- The parameters $a$ and $b$ are constrained by $0  a  1$ and $0 \\le b  1$.\n- The function $g(x)$ is specified as a bounded, continuous function on the interval $[0,1]$.\n- The Monte Carlo importance sampling estimator is defined from its first principle: $I=\\mathbb{E}_{p}\\!\\left[\\frac{x^{-a}(1-x)^{-b}\\,g(X)}{p(X)}\\right]$, for a probability density function $p(x)$ with support on $(0,1)$ and $X \\sim p$.\n- The Beta function is defined as $B(\\alpha,\\beta)=\\int_{0}^{1} x^{\\alpha-1}(1-x)^{\\beta-1}\\,dx$ for $\\alpha  0$ and $\\beta  0$.\n- The implementation requires a pseudorandom seed $s=20231105$, independent samples of size $N$ for each case, and angles in radians for trigonometric functions.\n- The test suite is given as:\n  - Case $1$: $(a,b,g,N) = (\\tfrac{1}{2}, 0, \\exp(x), 200000)$.\n  - Case $2$: $(a,b,g,N) = (0.9, 0, \\cos(x), 600000)$.\n  - Case $3$: $(a,b,g,N) = (0.49, 0.49, \\sin(3x), 400000)$.\n  - Case $4$: $(a,b,g,N) = (0.99, 0, 1, 100000)$.\n  - Case $5$: $(a,b,g,N) = (0.1, 0.9, \\exp(2x), 500000)$.\n\n**Step 2: Validation of Givens**\nThe problem is scientifically sound, well-posed, and objective. It is a standard problem in computational science concerning Monte Carlo methods. The constraints $0  a  1$ and $0 \\le b  1$ ensure that the integrand's singularities at $x=0$ and $x=1$ are integrable, guaranteeing that the integral $I$ is finite and well-defined. All necessary data and definitions for constructing a solution are provided. The problem is free from contradictions, ambiguities, or reliance on non-scientific premises.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be provided.\n\nLet the integrand be denoted by $f(x) = x^{-a}(1-x)^{-b}\\,g(x)$. The integral is $I = \\int_{0}^{1} f(x) dx$. The Monte Carlo importance sampling estimator for $I$ based on $N$ independent and identically distributed samples $X_i \\sim p(x)$ is given by:\n$$ \\widehat{I}_N = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{f(X_i)}{p(X_i)} $$\n\n**Derivation of Conditions for Unbiasedness and Finite Variance**\n\nFirst, we address the condition for the estimator to be unbiased. The expectation of the estimator is:\n$$ \\mathbb{E}_p[\\widehat{I}_N] = \\mathbb{E}_p\\left[\\frac{1}{N} \\sum_{i=1}^{N} \\frac{f(X_i)}{p(X_i)}\\right] = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}_p\\left[\\frac{f(X_i)}{p(X_i)}\\right] = \\mathbb{E}_p\\left[\\frac{f(X)}{p(X)}\\right] $$\nBy the definition of expectation for a continuous random variable, this is:\n$$ \\mathbb{E}_p\\left[\\frac{f(X)}{p(X)}\\right] = \\int_{\\text{supp}(p)} \\frac{f(x)}{p(x)} p(x) dx = \\int_{\\text{supp}(p)} f(x) dx $$\nFor the estimator to be unbiased, i.e., $\\mathbb{E}_p[\\widehat{I}_N] = I = \\int_{0}^{1} f(x) dx$, we must have $\\int_{\\text{supp}(p)} f(x) dx = \\int_{0}^{1} f(x) dx$. This is guaranteed if the support of $p(x)$ includes the region where $f(x)$ is non-zero. Formally, the condition is that for any $x \\in (0,1)$, if $f(x) \\neq 0$, then $p(x)  0$. We will require $p(x)  0$ for all $x \\in (0,1)$.\n\nSecond, we address the condition for the estimator to have finite variance. The variance of the estimator is:\n$$ \\text{Var}_p(\\widehat{I}_N) = \\frac{1}{N} \\text{Var}_p\\left(\\frac{f(X)}{p(X)}\\right) $$\nThe variance is finite if and only if the second moment of the random variable $\\frac{f(X)}{p(X)}$ is finite. This requires:\n$$ \\mathbb{E}_p\\left[\\left(\\frac{f(X)}{p(X)}\\right)^2\\right] = \\int_{0}^{1} \\frac{f(x)^2}{p(x)} dx  \\infty $$\nSubstituting $f(x) = x^{-a}(1-x)^{-b}\\,g(x)$, the condition becomes:\n$$ \\int_{0}^{1} \\frac{\\left(x^{-a}(1-x)^{-b}\\,g(x)\\right)^2}{p(x)} dx = \\int_{0}^{1} \\frac{x^{-2a}(1-x)^{-2b}\\,g(x)^2}{p(x)} dx  \\infty $$\nSince $g(x)$ is bounded on $[0,1]$, there exists a constant $M$ such that $g(x)^2 \\le M^2$. A sufficient condition for finite variance is therefore:\n$$ \\int_{0}^{1} \\frac{x^{-2a}(1-x)^{-2b}}{p(x)} dx  \\infty $$\nThis condition dictates the behavior of the proposal density $p(x)$ near the endpoints. To ensure convergence, $p(x)$ must be at least as singular as $x^{-2a}$ near $x = 0$ and as singular as $(1-x)^{-2b}$ near $x=1$.\n\n**Choice of Proposal Density $p(x)$**\n\nThe principle of importance sampling suggests choosing a proposal density $p(x)$ that mimics the behavior of the integrand $|f(x)|$. The singular part of our integrand is $w(x) = x^{-a}(1-x)^{-b}$. This functional form is proportional to the kernel of a Beta distribution.\nThe probability density function of a Beta-distributed random variable with parameters $\\alpha  0$ and $\\beta  0$ is:\n$$ p(x; \\alpha, \\beta) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha, \\beta)} $$\nTo match the form of $w(x)$, we choose exponents such that $\\alpha-1 = -a$ and $\\beta-1 = -b$. This gives the parameter choices:\n$$ \\alpha = 1-a \\quad \\text{and} \\quad \\beta = 1-b $$\nThe given constraints $0  a  1$ and $0 \\le b  1$ ensure that $\\alpha = 1-a  0$ and $\\beta = 1-b  0$. Therefore, a Beta distribution with these parameters is well-defined. Our chosen proposal density is:\n$$ p(x) = \\frac{x^{-a}(1-x)^{-b}}{B(1-a, 1-b)} $$\nThis choice satisfies the unbiasedness condition as $p(x)  0$ for $x \\in (0,1)$.\nLet us verify the finite variance condition with this $p(x)$. The integral for the second moment becomes:\n$$ \\int_0^1 \\frac{x^{-2a}(1-x)^{-2b} g(x)^2}{\\frac{x^{-a}(1-x)^{-b}}{B(1-a, 1-b)}} dx = B(1-a, 1-b) \\int_0^1 x^{-a}(1-x)^{-b}g(x)^2 dx $$\nSince $g(x)^2$ is a bounded function and the integral $\\int_0^1 x^{-a}(1-x)^{-b} dx = B(1-a, 1-b)$ converges, the entire expression is finite. Thus, this choice of $p(x)$ guarantees a finite variance for the estimator.\n\n**Algorithmic Implementation**\n\nWith our chosen $p(x)$, the ratio $\\frac{f(x)}{p(x)}$ in the estimator simplifies significantly:\n$$ \\frac{f(x)}{p(x)} = \\frac{x^{-a}(1-x)^{-b}g(x)}{\\frac{x^{-a}(1-x)^{-b}}{B(1-a, 1-b)}} = g(x) B(1-a, 1-b) $$\nThe Monte Carlo estimator is then:\n$$ \\widehat{I}_N = \\frac{1}{N} \\sum_{i=1}^{N} \\left[ g(X_i) B(1-a, 1-b) \\right] = B(1-a, 1-b) \\left( \\frac{1}{N} \\sum_{i=1}^{N} g(X_i) \\right) $$\nwhere $X_i$ are samples drawn from the Beta distribution $\\text{Beta}(1-a, 1-b)$.\n\nThis formulation is numerically stable as it completely removes the singular terms from the summation. The algorithm is as follows:\n1.  For each test case $(a, b, g, N)$, determine the parameters for the proposal distribution: $\\alpha = 1-a$ and $\\beta = 1-b$.\n2.  Pre-calculate the constant $C = B(\\alpha, \\beta)$ using a numerical library function.\n3.  Generate $N$ random samples $X_1, X_2, \\dots, X_N$ from the distribution $\\text{Beta}(\\alpha, \\beta)$.\n4.  Evaluate $g(X_i)$ for each sample $X_i$.\n5.  Compute the sample mean of these values: $\\bar{g} = \\frac{1}{N} \\sum_{i=1}^{N} g(X_i)$.\n6.  The estimate of the integral is $\\widehat{I} = C \\times \\bar{g}$.\n\nThis procedure is implemented for each of the specified test cases.\n- **Case 1**: $a=\\tfrac{1}{2}, b=0 \\implies \\alpha=0.5, \\beta=1$.\n- **Case 2**: $a=0.9, b=0 \\implies \\alpha=0.1, \\beta=1$.\n- **Case 3**: $a=0.49, b=0.49 \\implies \\alpha=0.51, \\beta=0.51$.\n- **Case 4**: $a=0.99, b=0 \\implies \\alpha=0.01, \\beta=1$. For this case, $g(x)=1$, so the estimator simplifies to $\\widehat{I} = B(0.01, 1) \\times \\frac{1}{N}\\sum_i 1 = B(0.01, 1)$, which is the exact value of the integral.\n- **Case 5**: $a=0.1, b=0.9 \\implies \\alpha=0.9, \\beta=0.1$.\nThe implementation will use the specified random seed for reproducibility.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import beta as beta_function\n\ndef solve():\n    \"\"\"\n    Constructs and applies a Monte Carlo importance sampling estimator for integrals\n    with endpoint singularities, as specified in the problem statement.\n    \"\"\"\n\n    # Initialize a random number generator with the specified seed for reproducibility.\n    seed = 20231105\n    rng = np.random.default_rng(seed)\n\n    # Define the g(x) functions for each test case. Angles are in radians.\n    g1 = lambda x: np.exp(x)\n    g2 = lambda x: np.cos(x)\n    g3 = lambda x: np.sin(3 * x)\n    # For g(x)=1, return an array of ones to ensure correct vectorized computation.\n    g4 = lambda x: np.ones_like(x)\n    g5 = lambda x: np.exp(2 * x)\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (a, b, g(x) function, sample size N)\n    test_cases = [\n        (0.5, 0.0, g1, 200000),\n        (0.9, 0.0, g2, 600000),\n        (0.49, 0.49, g3, 400000),\n        (0.99, 0.0, g4, 100000),\n        (0.1, 0.9, g5, 500000),\n    ]\n\n    results = []\n    for a, b, g, N in test_cases:\n        # Step 1: Define parameters for the Beta proposal distribution.\n        # Naming beta_p to avoid shadowing the imported beta_function.\n        alpha_p = 1.0 - a\n        beta_p = 1.0 - b\n\n        # Step 2: Pre-calculate the constant C = B(alpha, beta).\n        beta_constant = beta_function(alpha_p, beta_p)\n\n        # Step 3: Generate N random samples from the Beta(alpha, beta) distribution.\n        samples = rng.beta(alpha_p, beta_p, size=N)\n\n        # Step 4: Evaluate g(x) on the generated samples.\n        g_values = g(samples)\n\n        # Step 5: Compute the sample mean of g(x) values.\n        mean_g = np.mean(g_values)\n\n        # Step 6: The final estimate is B(alpha, beta) * mean(g(x)).\n        estimate = beta_constant * mean_g\n        \n        results.append(estimate)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}