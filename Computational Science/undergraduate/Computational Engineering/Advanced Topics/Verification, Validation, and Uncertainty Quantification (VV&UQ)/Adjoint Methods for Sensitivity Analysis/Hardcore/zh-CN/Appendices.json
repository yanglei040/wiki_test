{
    "hands_on_practices": [
        {
            "introduction": "为什么伴随方法在计算工程中如此重要？主要原因在于其卓越的计算效率。本练习引导你对比伴随方法与更直观的有限差分法在计算成本上的差异 。通过分析两种方法的成本如何随设计参数数量的变化而变化，你将理解伴随方法在解决大规模优化问题时不可或缺的核心优势。",
            "id": "2371119",
            "problem": "考虑常微分方程 (ODE) $\\,\\dot{y}(t) = -p\\,y(t)\\,$ 的初值问题，其中 $\\,y(0)=1\\,$，并在给定的最终时间 $\\,T>0\\,$ 定义标量目标 $\\,J = y(T)\\,$。您希望计算在给定参数值 $\\,p\\,$ 下的灵敏度 $\\,\\dfrac{dJ}{dp}\\,$。\n\n现在考虑一个更广的设定，其中模型状态是高维的，但目标仍然是 $\\,J = \\psi(y(T))\\,$ 形式的标量，并且参数推广为进入动力学系统的向量 $\\,\\boldsymbol{p} \\in \\mathbb{R}^m\\,$，其中 $\\,m \\gg 1\\,$。假设：\n- 在 $[0,T]$ 上对状态方程进行一次正向求解的计算成本与 $\\,C_f\\,$ 相当。\n- 在 $[0,T]$ 上对伴随方程进行一次反向求解的计算成本与 $\\,C_f\\,$ 相当。\n- 有限差分梯度是通过使用步长为 $\\,h>0\\,$ 的前向差分，一次扰动一个参数分量来形成的，并且不在分量之间重用导数信息。\n- 忽略数值步长选择的问题，并假设所有求解的成本相当。\n\n在此设定下，对于有限差分法与伴随方法，以下哪个陈述最能描述获得完整梯度 $\\,\\dfrac{dJ}{d\\boldsymbol{p}} \\in \\mathbb{R}^m\\,$ 的计算成本随 $\\,m\\,$ 变化的规律？\n\nA. 有限差分法需要 $\\,m\\,$ 数量级的正向求解（对于中心差分则为 $\\,2m\\,$ 次），而伴随方法仅需大约一次正向求解和一次反向伴随求解，因此其获得 $\\,\\dfrac{dJ}{d\\boldsymbol{p}}\\,$ 的总成本基本上与 $\\,m\\,$ 无关。\n\nB. 对于标量目标，有限差分法和伴随方法都需要 $\\,m\\,$ 数量级的模型求解才能获得 $\\,\\dfrac{dJ}{d\\boldsymbol{p}}\\,$。\n\nC. 由于重用基准求解，有限差分法的成本基本上与 $\\,m\\,$ 无关，而伴随方法需要对每个参数进行单独的伴随求解，导致成本达到 $\\,m\\,$ 的数量级。\n\nD. 两种方法的成本都与 $\\,m\\,$ 无关，因为当 $\\,m \\gg 1\\,$ 时，主导成本的是状态维度，而不是参数数量。",
            "solution": "问题陈述要求比较两种灵敏度分析方法——有限差分法和伴随方法——在处理具有大量参数的常微分方程（ODE）控制的系统时的计算规模扩展性。首先，我们必须验证问题陈述的有效性。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n\n-   **系统动力学**：一个 ODE 的初值问题，$\\dot{y}(t) = -p\\,y(t)$ 且 $y(0)=1$，被用作一个简单的启发性例子。一般情况是高维状态 $y(t)$，其动力学受参数向量 $\\boldsymbol{p} \\in \\mathbb{R}^m$ 的影响，其中 $m \\gg 1$。我们可以将其一般地写为 $\\dot{y}(t) = f(y(t), t, \\boldsymbol{p})$。\n-   **目标函数**：在最终时间 $T > 0$ 定义的标量目标 $J = \\psi(y(T))$。\n-   **目标**：计算完整梯度 $\\dfrac{dJ}{d\\boldsymbol{p}} \\in \\mathbb{R}^m$。\n-   **计算成本假设**：\n    -   在 $[0,T]$ 上对状态方程进行一次正向求解的成本为 $C_f$。\n    -   在 $[0,T]$ 上对伴随方程进行一次反向求解的成本与 $C_f$ 相当。\n    -   有限差分（FD）梯度是通过使用前向差分，一次扰动一个参数分量来计算的。\n    -   假设 FD 不在分量之间重用导数信息。\n    -   诸如步长选择之类的数值问题将被忽略，并假设所有求解的成本相当。\n\n**第 2 步：使用提取的已知条件进行验证**\n\n-   **科学依据**：该问题在计算工程和应用数学领域内被正确地构建。有限差分和伴随方法是计算灵敏度（梯度）的标准、成熟的技术。它们的计算复杂性比较是优化和不确定性下设计中的一个经典且根本上重要的话题。关于计算成本的假设是用于分析这些算法规模扩展特性的标准简化。\n-   **适定性**：问题是适定的。它要求比较计算规模相对于参数数量 $m$ 的扩展性。所提供的假设足以得出一个确定且唯一的结论。\n-   **客观性**：语言技术性强、精确，且不含主观内容。\n\n**第 3 步：结论与行动**\n\n该问题在科学上是合理的、适定的和客观的。它不包含任何矛盾或模糊之处。因此，该问题是**有效的**。我们继续进行求解。\n\n### 推导与分析\n\n目标是计算梯度向量 $\\dfrac{dJ}{d\\boldsymbol{p}}$，它有 $m$ 个分量：\n$$\n\\dfrac{dJ}{d\\boldsymbol{p}} = \\begin{pmatrix} \\dfrac{\\partial J}{\\partial p_1}  \\dfrac{\\partial J}{\\partial p_2}  \\cdots  \\dfrac{\\partial J}{\\partial p_m} \\end{pmatrix}^T\n$$\n\n**1. 有限差分（FD）法成本**\n\n问题指定使用前向差分格式。关于第 $i$ 个参数 $p_i$ 的偏导数近似为：\n$$\n\\dfrac{\\partial J}{\\partial p_i} \\approx \\dfrac{J(\\boldsymbol{p} + h\\boldsymbol{e}_i) - J(\\boldsymbol{p})}{h}\n$$\n其中 $\\boldsymbol{e}_i$ 是第 $i$ 个位置为 1 且其他位置为零的标准基向量，而 $h$ 是一个小的扰动步长。\n\n要计算完整的梯度向量，我们必须计算所有 $m$ 个分量。一个高效的实现将按以下步骤进行：\n-   **第 1 步（基准求解）**：计算 $J(\\boldsymbol{p})$。这需要使用未扰动的参数向量 $\\boldsymbol{p}$ 从 $t=0$ 到 $t=T$ 对状态方程进行一次正向求解。成本为 $C_f$。\n-   **第 2 步（扰动求解）**：对于每个参数分量 $p_i$（从 $i=1$到 $m$）：\n    -   计算 $J(\\boldsymbol{p} + h\\boldsymbol{e}_i)$。这需要使用扰动后的参数向量 $\\boldsymbol{p} + h\\boldsymbol{e}_i$ 对状态方程进行一次正向求解。成本为 $C_f$。\n-   总成本：计算涉及一次基准求解和 $m$ 次扰动求解。因此，总计算成本为 $C_f + m \\times C_f = (m+1)C_f$。\n\n对于大量参数（$m \\gg 1$），成本主要由 $m$ 次扰动求解决定，因此总成本随 $m$ 线性扩展。成本在 $m$ 次正向求解的数量级上。\n\n**2. 伴随方法成本**\n\n伴随方法是专门为以低计算成本计算标量泛函相对于大量参数的梯度而设计的。该过程包括两个主要步骤：\n-   **第 1 步（正向求解）**：从 $t=0$ 到 $t=T$ 正向求解状态方程 $\\dot{y}(t) = f(y(t), t, \\boldsymbol{p})$。这提供了后续步骤所需的状态轨迹 $y(t)$。成本为 $C_f$。\n-   **第 2 步（反向伴随求解）**：从 $t=T$ 到 $t=0$ 反向求解伴随状态 $\\lambda(t)$ 的线性伴随方程。伴随方程的形式为 $-\\dot{\\lambda}(t) = \\left(\\frac{\\partial f}{\\partial y}\\right)^T \\lambda(t)$，终端条件为 $\\lambda(T) = \\left(\\frac{\\partial \\psi}{\\partial y}\\right)^T_{y=y(T)}$。根据假设，这次反向求解的成本为 $C_f$。\n-   **第 3 步（梯度计算）**：在正向和反向求解完成后，通过计算一个积分来获得整个梯度向量 $\\dfrac{dJ}{d\\boldsymbol{p}}$，该积分通常形式如下：\n$$\n\\dfrac{dJ}{d\\boldsymbol{p}} = \\int_0^T \\lambda(t)^T \\dfrac{\\partial f}{\\partial \\boldsymbol{p}}(y(t), t, \\boldsymbol{p}) \\, dt\n$$\n与求解 ODE 相比，这个积分的计算成本很低。\n\n总成本是正向求解和反向伴随求解的成本之和。因此，总成本约为 $C_f + C_f = 2C_f$。该成本与参数数量 $m$ 无关。\n\n**比较：**\n-   **FD 成本扩展性**：$\\approx (m+1)C_f$，即 $O(m)$。\n-   **伴随法成本扩展性**：$\\approx 2C_f$，相对于 $m$ 是 $O(1)$。\n\n当 $m \\gg 1$ 时，伴随方法效率显著更高。\n\n### 逐项分析\n\n**A. 有限差分法需要 $\\,m\\,$ 数量级的正向求解（对于中心差分则为 $\\,2m\\,$ 次），而伴随方法仅需大约一次正向求解和一次反向伴随求解，因此其获得 $\\,\\dfrac{dJ}{d\\boldsymbol{p}}\\,$ 的总成本基本上与 $\\,m\\,$ 无关。**\n-   对有限差分的分析是正确的。它需要 $(m+1)$ 次求解，数量级为 $m$。关于中心差分需要 $2m$ 次求解的括号内说明也是正确的。\n-   对伴随方法的分析是正确的。它需要一次正向和一次反向求解。\n-   伴随方法成本与 $m$ 无关的结论是正确的。\n-   **结论：正确。**\n\n**B. 对于标量目标，有限差分法和伴随方法都需要 $\\,m\\,$ 数量级的模型求解才能获得 $\\,\\dfrac{dJ}{d\\boldsymbol{p}}\\,$。**\n-   这个陈述是不正确的，因为对于标量目标，伴随方法的成本与 $m$ 无关。它错误地将 $O(m)$ 的扩展性归于伴随方法。\n-   **结论：不正确。**\n\n**C. 由于重用基准求解，有限差分法的成本基本上与 $\\,m\\,$ 无关，而伴随方法需要对每个参数进行单独的伴随求解，导致成本达到 $\\,m\\,$ 的数量级。**\n-   这个陈述完全颠倒了事实。除了基准求解外，有限差分法还需要 $m$ 次扰动求解，使其成本依赖于 $m$。而伴随方法仅需一次伴随求解即可同时获得所有参数的灵敏度。\n-   **结论：不正确。**\n\n**D. 两种方法的成本都与 $\\,m\\,$ 无关，因为当 $\\,m \\gg 1\\,$ 时，主导成本的是状态维度，而不是参数数量。**\n-   这是不正确的。有限差分法的成本与 $m$ 成正比。前提 $m \\gg 1$ 使得对 $m$ 的依赖成为 FD 方法总成本最关键的因素，而非无关紧要的因素。虽然状态维度影响常数 $C_f$，但它并不能消除 FD 方法的 $O(m)$ 扩展性。\n-   **结论：不正确。**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "理解了伴随方法为何高效之后，我们来探索其构建过程中的一个关键步骤：推导伴随方程本身。伴随方程的源项由目标函数的变分导数决定，而此练习  为一个依赖于状态变量梯度的目标函数，提供了一个推导该源项的具体范例。掌握这一涉及变分法的过程，对于将伴随方法应用于新的、多样化的问题至关重要。",
            "id": "2371076",
            "problem": "考虑一个有界开域 $\\Omega \\subset \\mathbb{R}^{d}$，其边界 $\\partial \\Omega$ 足够光滑。设状态场 $u:\\Omega \\to \\mathbb{R}$ 满足 $u \\in H_{0}^{1}(\\Omega)$，因此 $u$ 及所有容许变分在 $\\partial \\Omega$ 上为零。目标泛函为\n$$\nJ(u) \\;=\\; \\int_{\\Omega} \\left|\\nabla u(x)\\right|^{2}\\, \\mathrm{d}x.\n$$\n在推导一个在 $\\partial \\Omega$ 上具有齐次狄利克雷边界条件的偏微分方程（PDE）约束问题的伴随方程时，伴随方程右端的源项是 $J$ 关于 $u$ 的 $L^{2}(\\Omega)$ 变分导数。请用 $u$ 及其空间导数显式地确定该源项。请将您的最终答案以单个简化的符号表达式形式给出。不要包含单位。",
            "solution": "该问题陈述经过验证，被认为是科学上合理的、适定的、客观的且自洽的。这是一个变分法中的标准问题，而变分法是推导偏微分方程约束优化问题伴随方程的基石。因此，我们可以继续进行求解。\n\n问题要求解目标泛函 $J(u)$ 关于状态场 $u$ 的 $L^{2}(\\Omega)$ 变分导数。该泛函由下式给出：\n$$\nJ(u) = \\int_{\\Omega} |\\nabla u(x)|^2 \\, \\mathrm{d}x\n$$\n其中 $u \\in H_{0}^{1}(\\Omega)$。$L^{2}(\\Omega)$ 变分导数（我们记为 $\\frac{\\delta J}{\\delta u}$）由一阶变分 $\\delta J$ 的关系式定义：\n$$\n\\delta J = \\lim_{\\epsilon \\to 0} \\frac{J(u + \\epsilon \\delta u) - J(u)}{\\epsilon} = \\int_{\\Omega} \\frac{\\delta J}{\\delta u} \\delta u \\, \\mathrm{d}x\n$$\n对于所有容许变分 $\\delta u$。由于状态空间是 $H_{0}^{1}(\\Omega)$，容许变分 $\\delta u$ 也必须属于 $H_{0}^{1}(\\Omega)$，这意味着在边界 $\\partial \\Omega$ 上 $\\delta u = 0$。\n\n我们首先计算 $J(u)$ 沿任意变分 $\\delta u \\in H_{0}^{1}(\\Omega)$ 方向的 Gateaux 导数。我们考虑表达式 $J(u + \\epsilon \\delta u)$：\n$$\nJ(u + \\epsilon \\delta u) = \\int_{\\Omega} |\\nabla(u + \\epsilon \\delta u)|^2 \\, \\mathrm{d}x\n$$\n积分内的项可以使用点积的性质展开：\n$$\n|\\nabla(u + \\epsilon \\delta u)|^2 = (\\nabla u + \\epsilon \\nabla \\delta u) \\cdot (\\nabla u + \\epsilon \\nabla \\delta u) = |\\nabla u|^2 + 2\\epsilon (\\nabla u \\cdot \\nabla \\delta u) + \\epsilon^2 |\\nabla \\delta u|^2\n$$\n将其代回到 $J(u + \\epsilon \\delta u)$ 的积分中：\n$$\nJ(u + \\epsilon \\delta u) = \\int_{\\Omega} \\left( |\\nabla u|^2 + 2\\epsilon (\\nabla u \\cdot \\nabla \\delta u) + \\epsilon^2 |\\nabla \\delta u|^2 \\right) \\, \\mathrm{d}x\n$$\n一阶变分 $\\delta J$ 是 $J(u + \\epsilon \\delta u)$ 关于 $\\epsilon$ 的导数，在 $\\epsilon = 0$ 处取值：\n$$\n\\delta J = \\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} \\left[ \\int_{\\Omega} \\left( |\\nabla u|^2 + 2\\epsilon (\\nabla u \\cdot \\nabla \\delta u) + \\epsilon^2 |\\nabla \\delta u|^2 \\right) \\, \\mathrm{d}x \\right]_{\\epsilon=0}\n$$\n假设具有足够好的正则性以交换微分和积分的顺序，我们有：\n$$\n\\delta J = \\int_{\\Omega} \\frac{\\partial}{\\partial\\epsilon} \\left( |\\nabla u|^2 + 2\\epsilon (\\nabla u \\cdot \\nabla \\delta u) + \\epsilon^2 |\\nabla \\delta u|^2 \\right)_{\\epsilon=0} \\, \\mathrm{d}x\n$$\n$$\n\\delta J = \\int_{\\Omega} \\left( 2 (\\nabla u \\cdot \\nabla \\delta u) + 2\\epsilon |\\nabla \\delta u|^2 \\right)_{\\epsilon=0} \\, \\mathrm{d}x\n$$\n$$\n\\delta J = \\int_{\\Omega} 2 (\\nabla u \\cdot \\nabla \\delta u) \\, \\mathrm{d}x\n$$\n为了确定变分导数，我们必须将此积分重写为 $\\int_{\\Omega} (\\cdot) \\delta u \\, \\mathrm{d}x$ 的形式。这可以通过分部积分法实现，在此多维情境下，这是格林第一恒等式的一个应用。格林第一恒等式表明，对于标量场 $v$ 和 $w$：\n$$\n\\int_{\\Omega} (v \\Delta w + \\nabla v \\cdot \\nabla w) \\, \\mathrm{d}x = \\int_{\\partial \\Omega} v (\\nabla w \\cdot \\mathbf{n}) \\, \\mathrm{d}S\n$$\n其中 $\\mathbf{n}$ 是边界 $\\partial \\Omega$ 的单位外法向量，而 $\\Delta = \\nabla \\cdot \\nabla$ 是拉普拉斯算子。我们可以重新整理这个恒等式，以表达我们所拥有的项：\n$$\n\\int_{\\Omega} \\nabla v \\cdot \\nabla w \\, \\mathrm{d}x = \\int_{\\partial \\Omega} v (\\nabla w \\cdot \\mathbf{n}) \\, \\mathrm{d}S - \\int_{\\Omega} v \\Delta w \\, \\mathrm{d}x\n$$\n我们令 $v = \\delta u$ 和 $w = u$。一阶变分的表达式变为：\n$$\n\\delta J = 2 \\left( \\int_{\\partial \\Omega} \\delta u (\\nabla u \\cdot \\mathbf{n}) \\, \\mathrm{d}S - \\int_{\\Omega} \\delta u \\Delta u \\, \\mathrm{d}x \\right)\n$$\n问题陈述指出，状态场 $u$ 和所有容许变分 $\\delta u$ 都在 Sobolev 空间 $H_{0}^{1}(\\Omega)$ 中。该空间的一个定义性属性是函数在边界 $\\partial \\Omega$ 上的迹为零。因此，对于所有 $x \\in \\partial \\Omega$，$\\delta u = 0$。因此，边界积分为零：\n$$\n\\int_{\\partial \\Omega} \\delta u (\\nabla u \\cdot \\mathbf{n}) \\, \\mathrm{d}S = \\int_{\\partial \\Omega} 0 \\cdot (\\nabla u \\cdot \\mathbf{n}) \\, \\mathrm{d}S = 0\n$$\n一阶变分的表达式简化为：\n$$\n\\delta J = -2 \\int_{\\Omega} \\delta u \\Delta u \\, \\mathrm{d}x = \\int_{\\Omega} (-2 \\Delta u) \\delta u \\, \\mathrm{d}x\n$$\n通过将此结果与变分导数的定义 $\\delta J = \\int_{\\Omega} \\frac{\\delta J}{\\delta u} \\delta u \\, \\mathrm{d}x$ 进行比较，我们可以直接确定 $\\frac{\\delta J}{\\delta u}$ 的表达式：\n$$\n\\frac{\\delta J}{\\delta u} = -2 \\Delta u\n$$\n该表达式是 $J$ 关于 $u$ 的 $L^{2}(\\Omega)$ 变分导数。根据问题陈述，这就是伴随方程的源项。",
            "answer": "$$\\boxed{-2 \\Delta u}$$"
        },
        {
            "introduction": "伴随方法能够高效地提供目标函数的梯度，但我们如何利用这个梯度来实际寻找最优解呢？梯度是迭代优化算法的关键输入。本练习  探讨了诸如梯度下降法或更高级的拟牛顿法（如 L-BFGS）等标准算法，是如何利用伴随方法计算出的梯度来逐步改进设计的。这个最终步骤将伴随求解得到的灵敏度信息与优化的实用机制联系起来，完整地展示了基于伴随的设计优化在实际工作流程中是如何运作的。",
            "id": "2371088",
            "problem": "考虑一个降维优化问题，其中参数向量 $p \\in \\mathbb{R}^m$ 通过一个控制残差方程 $R(u,p)=0$ 影响状态 $u \\in \\mathbb{R}^n$，而目标量是降维目标函数 $j(p)=J(u(p),p)$。假设在某次迭代 $p_k$ 时，您已经使用伴随方法计算出了精确的降维梯度 $g_k=\\nabla j(p_k)$。您的目标是构建一个实用的迭代优化循环，从 $p_0$ 开始最小化 $j(p)$。\n\n以下哪个选项正确描述了在无约束问题的实用优化循环中，对伴随梯度的稳健和标准用法？\n\nA. 初始化 $p_0$。对于每个 $k \\in \\{0,1,2,\\dots\\}$：求解 $R(u_k,p_k)=0$ 得到 $u_k$，通过伴随方法评估 $j(p_k)=J(u_k,p_k)$ 和 $g_k=\\nabla j(p_k)$，检查终止条件（例如，$\\|g_k\\|$ 小于一个容差），设置搜索方向 $d_k=-g_k$，通过回溯 Armijo 线搜索选择一个步长 $\\alpha_k>0$ 以确保 $j$ 沿着 $d_k$ 有充分下降，并更新 $p_{k+1}=p_k+\\alpha_k d_k$。\n\nB. 初始化 $p_0$ 和一个初始逆 Hessian 矩阵近似 $H_0$（例如，一个缩放的单位矩阵）。对于每个 $k \\in \\{0,1,2,\\dots\\}$：求解 $R(u_k,p_k)=0$ 得到 $u_k$，通过伴随方法评估 $j(p_k)$ 和 $g_k$，检查终止条件（例如，$\\|g_k\\|$ 小于一个容差），计算搜索方向 $d_k=-H_k g_k$，通过一个强制执行 Wolfe 条件的线搜索选择 $\\alpha_k>0$，更新 $p_{k+1}=p_k+\\alpha_k d_k$，构造 $s_k=p_{k+1}-p_k$ 和 $y_k=g_{k+1}-g_k$（其中 $g_{k+1}=\\nabla j(p_{k+1})$ 来自一次新的状态和伴随求解），并使用有限内存 Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) 双循环递归和最近的 $\\{(s_i,y_i)\\}$ 对来更新 $H_{k+1}$。\n\nC. 初始化 $p_0$。对于每个 $k \\in \\{0,1,2,\\dots\\}$：在 $p_0$ 处通过伴随方法计算一次 $g_k$ 并在所有后续迭代中重复使用它，使用一个固定的较大 $\\alpha>0$ 设置 $p_{k+1}=p_k+\\alpha g_k$，并在 $J$ 在单步内停止下降时立即终止；为节省成本，不在新的 $p_k$ 处重新计算状态或伴随。\n\nD. 初始化 $p_0$。对于每个 $k \\in \\{0,1,2,\\dots\\}$：将伴随梯度 $g_k$ 视为一个增加的方向以逃离局部最小值，方法是设置 $d_k=+g_k$，不进行线搜索而采取一个完整步长 $\\alpha_k=1$，更新 $p_{k+1}=p_k+d_k$，并且仅当 $J$ 的增量超过一个固定阈值时才重新计算状态 $u$；不维护或更新任何曲率信息。",
            "solution": "问题陈述已提交以供验证。\n\n### 第1步：提取已知信息\n- 优化问题是最小化一个降维目标函数 $j(p)$，其中 $p \\in \\mathbb{R}^m$ 是参数向量。\n- 状态向量是 $u \\in \\mathbb{R}^n$。\n- 一个控制残差方程 $R(u,p)=0$ 将状态和参数耦合起来。\n- 降维目标函数定义为 $j(p)=J(u(p),p)$，其中对于给定的 $p$，$u(p)$ 是 $R(u,p)=0$ 的解。\n- 在任何迭代点 $p_k$，都可以通过伴随方法获得精确的降维梯度 $g_k=\\nabla j(p_k)$。\n- 目标是确定一个正确、稳健且标准的迭代优化算法，用于从初始猜测 $p_0$ 开始解决这个无约束最小化问题。\n\n### 第2步：使用提取的已知信息进行验证\n- **科学依据：** 该问题牢固地植根于计算科学与工程领域，特别是在偏微分方程约束优化（PDE-constrained optimization）或更广泛的由隐式函数控制的优化领域。使用降维目标函数、控制残差和伴随方法计算梯度，构成了设计优化和反演问题的标准且强大的框架。该表述在数学上和科学上都是合理的。\n- **适定性：** 问题是适定的。它不要求解决一个特定的优化问题，而是要求评估通用优化算法描述的正确性和标准性。任务是确定所提供的算法描述中哪些是使用梯度进行无约束最小化的有效程序。\n- **客观性：** 问题陈述是客观的，使用了精确的数学术语，没有歧义或主观内容。\n\n### 第3步：结论与行动\n问题陈述是有效的。这是一个关于在基于伴随的灵敏度分析背景下正确应用数值优化算法的明确定义的问题。现在将通过分析每个提供的选项来继续求解过程。\n\n目标是找到函数 $j(p)$ 的一个最小值。在点 $p^*$ 处存在局部最小值的必要条件是梯度为零，即 $\\nabla j(p^*) = 0$。迭代优化方法生成一系列点 $\\{p_k\\}$，使得当 $k \\to \\infty$ 时 $p_k \\to p^*$。一个最小化算法的基本要求是它能生成下降方向，这意味着对于一个搜索方向 $d_k$，我们必须有 $d_k^T \\nabla j(p_k) < 0$。这确保了在方向 $d_k$ 上走一小步会减小目标函数的值。\n\n我们基于这些原则分析每个选项。\n\n**选项A：** 这个选项描述了梯度下降法（或最速下降法）与回溯线搜索相结合。过程如下：\n1. 在迭代点 $p_k$，通过求解控制方程 $R(u_k, p_k) = 0$ 来找到状态 $u_k$。这是评估目标函数 $j(p_k)$ 及其梯度 $g_k$ 的必要前提。\n2. 使用伴随方法计算梯度 $g_k = \\nabla j(p_k)$。\n3. 检查终止条件，通常是 $\\|g_k\\| < \\epsilon$（对于某个小容差 $\\epsilon > 0$）。\n4. 搜索方向设为 $d_k = -g_k$。这是最速下降方向，并且是一个保证的下降方向，因为 $d_k^T g_k = -g_k^T g_k = -\\|g_k\\|^2 \\le 0$，且除非 $g_k=0$ 否则严格为负。\n5. 通过回溯 Armijo 线搜索确定一个步长 $\\alpha_k > 0$。这是一个稳健的过程，以确保所采取的步长能导致目标函数有“充分下降”，即对于某个常数 $c \\in (0,1)$，有 $j(p_k + \\alpha_k d_k) \\le j(p_k) + c \\alpha_k g_k^T d_k$。这可以防止越过最小值，并确保在弱假设下全局收敛到驻点。\n6. 更新参数向量：$p_{k+1} = p_k + \\alpha_k d_k$。\n这整个循环是标准梯度下降算法的教科书式描述。它是一种稳健但通常收敛较慢的方法。该描述在每个细节上都是正确的。\n结论：**正确**。\n\n**选项B：** 此选项描述了一种准牛顿法，特别是有限内存 Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) 算法。\n1. 循环正确地从求解状态方程得到 $u_k$ 开始，然后通过伴随方法计算目标函数 $j(p_k)$ 和梯度 $g_k$。\n2. 搜索方向计算为 $d_k = -H_k g_k$，其中 $H_k$ 是 Hessian 矩阵 $\\nabla^2 j(p_k)$ 的逆矩阵的近似。在 L-BFGS 中，$H_k$ 不是显式存储的，而是利用最近几次参数和梯度变化的记录 $\\{(s_i, y_i)\\}$ 来高效地计算其对 $g_k$ 的作用。\n3. 线搜索强制执行 Wolfe 条件。这些条件比 Armijo 条件更强，对准牛顿法至关重要。它们不仅确保充分下降，还确保步长不会太短，这是保证更新后的 Hessian 近似 $H_{k+1}$ 保持正定和高质量所必需的。\n4. 更新规则 $p_{k+1}=p_k+\\alpha_k d_k$ 是标准的。\n5. 至关重要的是，该算法正确地指出，必须在新点 $p_{k+1}$ 处计算新的梯度 $g_{k+1}$（这需要一次新的状态求解）。然后，这个新的梯度被用来构成对 $(s_k, y_k) = (p_{k+1}-p_k, g_{k+1}-g_k)$，该对包含用于更新下一次迭代的逆 Hessian 近似的曲率信息。\n这是 L-BFGS 方法的精确和正确的描述，它是大规模无约束优化的一种高效且标准的算法，并且通常是涉及伴随梯度计算问题的首选方法。\n结论：**正确**。\n\n**选项C：** 此选项提出了一种算法，该算法仅在初始点 $p_0$ 处计算一次梯度 $g_0$，并在所有后续迭代中重复使用它，并采用一个固定的较大步长。\n这个过程有根本性的缺陷。梯度 $\\nabla j(p)$ 是一个局部量，描述了在点 $p$ 处的最速上升方向。方向 $-g_0 = -\\nabla j(p_0)$ 仅在 $p_0$ 的紧邻域内是下降方向。对于一般的非线性函数 $j(p)$，没有理由相信 $-g_0$ 在远离 $p_0$ 的点 $p_k$ 处仍然是下降方向。该算法本质上是试图在整个优化过程中用 $p_0$ 处的一个切平面来近似一个非线性函数。这对于几乎所有有意义的问题都会失败。此外，使用一个固定的较大步长而不进行线搜索是导致发散的根源。不重新计算状态和伴随的成本节省措施使该算法对目标函数的真实形态视而不见。\n结论：**不正确**。\n\n**选项D：** 此选项建议沿梯度方向移动，$d_k = +g_k$。\n梯度 $g_k = \\nabla j(p_k)$ 先验地指向目标函数 $j$ 在 $p_k$ 处的最速*上升*方向。一个为最小化设计的算法必须沿下降方向移动。因此，沿 $+g_k$ 移动是一个*最大化*而非最小化的过程。虽然一些全局优化或随机方法可能会采取“上坡”步骤来逃离局部最小值，但这是在一个完全不同的、经过仔细控制的框架内完成的（例如，模拟退火），而不是作为主要的确定性步骤。使用固定步长 $\\alpha_k=1$ 而不进行线搜索是不稳健的。所提出的算法是梯度上升的一种形式，它解决了错误的问题。\n结论：**不正确**。\n\n总而言之，选项 A（带有 Armijo 线搜索的梯度下降）和选项 B（带有 Wolfe 线搜索的 L-BFGS）都描述了正确、稳健和标准的迭代优化循环，它们在每次迭代中都恰当地利用了伴随求解提供的梯度信息。选项 B 的性能通常更优，但选项 A 也是一种有效的基础方法。选项 C 和 D 基于根本上错误的逻辑。",
            "answer": "$$\\boxed{AB}$$"
        }
    ]
}