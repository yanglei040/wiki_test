{
    "hands_on_practices": [
        {
            "introduction": "This first practice grounds Bayesian inference in a tangible heat transfer problem: estimating thermal contact resistance. You will learn to combine a physical model with noisy data to refine your knowledge of a single, crucial parameter. The core task is to numerically construct a posterior distribution for the resistance parameter, $R$, by discretizing the parameter space, evaluating the likelihood and prior at each point, and then using numerical integration to find the posterior mean—a fundamental technique in computational Bayesian analysis. This exercise  builds your foundational skills in implementing Bayesian inference from scratch, showing how prior knowledge (a Log-Normal prior) is updated by experimental evidence (temperature measurements) to yield a posterior belief.",
            "id": "2374140",
            "problem": "Consider a one-dimensional, steady heat conduction experiment involving two solid bodies in contact. The net effect of microscopic roughness at the interface is modeled as a thermal contact resistance, denoted by $R$ in units of $\\mathrm{m^2\\,K/W}$. Under a uniform heat flux $q$ (in $\\mathrm{W/m^2}$) through the interface, the temperature drop across the interface is defined as $\\Delta T = T^{-} - T^{+}$, where $T^{-}$ and $T^{+}$ are the limiting interface temperatures on either side. Starting from Fourier’s law of heat conduction and the definition of interfacial thermal resistance, the interface jump condition is that the temperature drop across the interface satisfies $\\Delta T = q\\,R$.\n\nIn practice, measurements are noisy. Assume that the observed temperature drop $\\Delta T_i^{\\mathrm{obs}}$ for the $i$-th experimental condition obeys an additive Gaussian noise model\n$$\n\\Delta T_i^{\\mathrm{obs}} = q_i\\,R + \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2),\n$$\nwith known measurement noise standard deviation $\\sigma$ (in $\\mathrm{K}$). Measurements are conditionally independent given $R$. Adopt a strictly positive prior for $R$ given by a Log-Normal distribution: $R \\sim \\mathrm{LogNormal}(\\mu_0, s_0^2)$, meaning that $\\ln R \\sim \\mathcal{N}(\\mu_0, s_0^2)$. Use natural logarithms. The Log-Normal prior density is\n$$\n\\pi(R) = \\frac{1}{R\\,s_0\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln R - \\mu_0)^2}{2 s_0^2}\\right),\\quad R0.\n$$\n\nYour task is to perform Bayesian inference to estimate $R$ from the data by computing the posterior mean\n$$\n\\mathbb{E}[R \\mid \\{\\Delta T_i^{\\mathrm{obs}}, q_i\\}_{i=1}^{n}],\n$$\nusing numerical integration over $R \\in [R_{\\min}, R_{\\max}]$ with a logarithmically spaced grid. Use Bayes’ rule to construct the unnormalized posterior density on this grid, normalize it numerically, and then compute the posterior mean by numerical quadrature. All computations must be carried out in International System of Units (SI). Express the final estimates of $R$ in $\\mathrm{m^2\\,K/W}$ as plain decimal floats.\n\nFundamental base for modeling:\n- Fourier’s law for one-dimensional steady conduction: $q = -k\\,\\mathrm{d}T/\\mathrm{d}x$.\n- Interface jump condition from the definition of contact resistance: the temperature drop is proportional to the flux with proportionality $R$, so $\\Delta T = q\\,R$.\n- Independent Gaussian measurement errors for temperature drops, leading to a Gaussian likelihood.\n\nImplementation requirements:\n- Use a logarithmically spaced grid with $R_{\\min} = 10^{-6}\\,\\mathrm{m^2\\,K/W}$ and $R_{\\max} = 10^{-2}\\,\\mathrm{m^2\\,K/W}$. The grid must be sufficiently dense to produce stable numerical integrals.\n- Compute the unnormalized log-posterior at each grid point as the sum of the log-prior and the log-likelihood, stabilize by subtracting the maximum log-posterior value before exponentiation, then normalize with a numerical quadrature rule over $R$.\n- Compute the posterior mean $\\mathbb{E}[R \\mid \\text{data}]$ as the ratio of two numerical integrals over $R$: the integral of $R$ times the posterior density divided by the integral of the posterior density.\n\nPrior parameters:\n- $\\mu_0 = \\ln(1.5\\times 10^{-4})$,\n- $s_0 = 0.5$.\n\nTest suite:\nFor each test case below, use the specified $\\{q_i\\}$ array in $\\mathrm{W/m^2}$, the observed temperature drops $\\{\\Delta T_i^{\\mathrm{obs}}\\}$ in $\\mathrm{K}$, and the noise standard deviation $\\sigma$ in $\\mathrm{K}$.\n\n- Case $1$ (well-conditioned, multiple flux levels):\n  - $q = [1.00\\times 10^{4},\\, 1.20\\times 10^{4},\\, 0.80\\times 10^{4},\\, 1.50\\times 10^{4}]$,\n  - $\\Delta T^{\\mathrm{obs}} = [2.12,\\, 2.45,\\, 1.50,\\, 3.08]$,\n  - $\\sigma = 0.10$.\n- Case $2$ (broader flux range):\n  - $q = [0.50\\times 10^{4},\\, 2.00\\times 10^{4},\\, 3.00\\times 10^{4},\\, 4.00\\times 10^{4},\\, 1.00\\times 10^{4}]$,\n  - $\\Delta T^{\\mathrm{obs}} = [0.58,\\, 2.47,\\, 3.55,\\, 4.95,\\, 1.21]$,\n  - $\\sigma = 0.10$.\n- Case $3$ (edge case: low information, single low flux, larger noise):\n  - $q = [0.20\\times 10^{4}]$,\n  - $\\Delta T^{\\mathrm{obs}} = [0.64]$,\n  - $\\sigma = 0.30$.\n\nAngle units do not apply. There are no percentages in this problem.\n\nYour program must output a single line containing the three posterior mean estimates for $R$ corresponding to the three cases, as a comma-separated Python-style list enclosed in square brackets, for example, $[r_1,r_2,r_3]$, where each $r_i$ is a float in $\\mathrm{m^2\\,K/W}$.",
            "solution": "The problem statement has been analyzed and is deemed valid. It is a well-posed problem in Bayesian parameter estimation, grounded in the principles of heat transfer and statistical inference. The provided data and models are scientifically sound, complete, and consistent. We shall proceed with the derivation and numerical solution.\n\nThe objective is to compute the posterior mean of the thermal contact resistance, $R$, given a set of $n$ noisy measurements. The problem is defined by the following components:\n\n1.  **Physical Model:** The temperature drop $\\Delta T$ across an interface is related to the heat flux $q$ by $\\Delta T = qR$.\n2.  **Observation Model:** For the $i$-th measurement, the observed temperature drop $\\Delta T_i^{\\mathrm{obs}}$ is modeled as $\\Delta T_i^{\\mathrm{obs}} = q_i R + \\varepsilon_i$, where the noise term $\\varepsilon_i$ is drawn from a normal distribution with mean $0$ and known variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n3.  **Prior Distribution:** The parameter $R$ is assumed to be strictly positive. A Log-Normal prior is assigned: $R \\sim \\mathrm{LogNormal}(\\mu_0, s_0^2)$, which implies $\\ln R \\sim \\mathcal{N}(\\mu_0, s_0^2)$.\n\nLet the collected data be denoted by $\\mathcal{D} = \\{\\Delta T_i^{\\mathrm{obs}}, q_i\\}_{i=1}^{n}$. Our goal is to compute the posterior mean of $R$:\n$$\n\\mathbb{E}[R \\mid \\mathcal{D}] = \\int_0^\\infty R \\, p(R \\mid \\mathcal{D}) \\, dR\n$$\nwhere $p(R \\mid \\mathcal{D})$ is the posterior probability density function of $R$.\n\nAccording to Bayes' theorem, the posterior density is proportional to the product of the likelihood and the prior density:\n$$\np(R \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid R) \\, \\pi(R)\n$$\nwhere $p(\\mathcal{D} \\mid R)$ is the likelihood and $\\pi(R)$ is the prior.\n\n**Likelihood Function**\nFrom the observation model, each measurement $\\Delta T_i^{\\mathrm{obs}}$ is independently drawn from a normal distribution $\\mathcal{N}(q_i R, \\sigma^2)$. The probability density for a single observation is:\n$$\np(\\Delta T_i^{\\mathrm{obs}} \\mid R) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left( -\\frac{(\\Delta T_i^{\\mathrm{obs}} - q_i R)^2}{2\\sigma^2} \\right)\n$$\nSince the measurements are conditionally independent given $R$, the total likelihood for the dataset $\\mathcal{D}$ is the product of the individual densities:\n$$\np(\\mathcal{D} \\mid R) = \\prod_{i=1}^n p(\\Delta T_i^{\\mathrm{obs}} \\mid R)\n$$\nFor numerical stability, we work with the log-likelihood. Up to an additive constant that is independent of $R$, the log-likelihood is:\n$$\n\\ln p(\\mathcal{D} \\mid R) \\propto -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (\\Delta T_i^{\\mathrm{obs}} - q_i R)^2\n$$\n\n**Prior Distribution**\nThe prior for $R$ is a Log-Normal distribution with parameters $\\mu_0$ and $s_0^2$, given by the density:\n$$\n\\pi(R) = \\frac{1}{R\\,s_0\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln R - \\mu_0)^2}{2s_0^2}\\right) \\quad \\text{for } R  0\n$$\nThe log-prior, up to an additive constant, is:\n$$\n\\ln \\pi(R) \\propto -\\ln R - \\frac{(\\ln R - \\mu_0)^2}{2s_0^2}\n$$\n\n**Posterior Distribution**\nThe unnormalized log-posterior is the sum of the log-likelihood and the log-prior:\n$$\n\\ln p(R \\mid \\mathcal{D}) \\propto -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (\\Delta T_i^{\\mathrm{obs}} - q_i R)^2 - \\ln R - \\frac{(\\ln R - \\mu_0)^2}{2s_0^2}\n$$\n\n**Numerical Computation**\nThe posterior mean is computed by numerical quadrature over a discretized grid for $R$. We define a grid of points $\\{R_j\\}$ spanning the interval $[R_{\\min}, R_{\\max}]$. The problem specifies a logarithmically spaced grid.\n\nThe posterior mean is the ratio of two integrals:\n$$\n\\mathbb{E}[R \\mid \\mathcal{D}] = \\frac{\\int_{R_{\\min}}^{R_{\\max}} R \\, p_{\\text{unnorm}}(R \\mid \\mathcal{D}) \\, dR}{\\int_{R_{\\min}}^{R_{\\max}} p_{\\text{unnorm}}(R \\mid \\mathcal{D}) \\, dR}\n$$\nwhere $p_{\\text{unnorm}}(R \\mid \\mathcal{D}) = \\exp(\\ln p_{\\text{unnorm}}(R \\mid \\mathcal{D}))$ is the unnormalized posterior density.\n\nThe algorithmic steps are as follows:\n1.  Define a logarithmically spaced grid of $N_{grid}$ points, $\\{R_j\\}$, from $R_{\\min} = 10^{-6}$ to $R_{\\max} = 10^{-2}$.\n2.  For computational efficiency, pre-compute the sums needed for the likelihood term: $S_{qq} = \\sum_{i=1}^n q_i^2$, $S_{qT} = \\sum_{i=1}^n q_i \\Delta T_i^{\\mathrm{obs}}$, and $S_{TT} = \\sum_{i=1}^n (\\Delta T_i^{\\mathrm{obs}})^2$. The log-likelihood term becomes $-\\frac{1}{2\\sigma^2}(S_{TT} - 2R S_{qT} + R^2 S_{qq})$.\n3.  Evaluate the unnormalized log-posterior, $\\mathcal{L}(R_j)$, at each grid point $R_j$.\n4.  To prevent numerical underflow during exponentiation, stabilize the log-posterior by subtracting its maximum value: $\\mathcal{L}_{\\text{stab}}(R_j) = \\mathcal{L}(R_j) - \\max_j\\{\\mathcal{L}(R_j)\\}$.\n5.  Compute the unnormalized posterior values on the grid: $p_{\\text{unnorm}}(R_j) = \\exp(\\mathcal{L}_{\\text{stab}}(R_j))$.\n6.  Approximate the integrals for the numerator and the denominator (normalization constant) using the trapezoidal rule over the non-uniform grid $\\{R_j\\}$. Let $P_j = p_{\\text{unnorm}}(R_j)$.\n    - Normalization constant: $Z = \\int p_{\\text{unnorm}}(R) \\, dR \\approx \\text{trapz}(\\{P_j\\}, \\{R_j\\})$.\n    - Numerator integral: $N = \\int R \\, p_{\\text{unnorm}}(R) \\, dR \\approx \\text{trapz}(\\{R_j \\cdot P_j\\}, \\{R_j\\})$.\n7.  The posterior mean is then calculated as $\\mathbb{E}[R \\mid \\mathcal{D}] \\approx N/Z$.\n\nThis procedure is applied to each of the three test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import trapezoid\n\ndef solve():\n    \"\"\"\n    Main function to solve the Bayesian inference problem for all test cases.\n    \"\"\"\n    # Prior parameters\n    mu_0 = np.log(1.5e-4) # mu_0 = ln(1.5 * 10^-4)\n    s_0 = 0.5            # s_0 = 0.5\n\n    # Numerical integration parameters\n    R_min = 1e-6         # R_min = 10^-6 m^2K/W\n    R_max = 1e-2         # R_max = 10^-2 m^2K/W\n    N_grid = 20001       # Number of grid points for integration\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"q\": np.array([1.00e4, 1.20e4, 0.80e4, 1.50e4]), # W/m^2\n            \"delta_T_obs\": np.array([2.12, 2.45, 1.50, 3.08]), # K\n            \"sigma\": 0.10, # K\n        },\n        {\n            \"q\": np.array([0.50e4, 2.00e4, 3.00e4, 4.00e4, 1.00e4]), # W/m^2\n            \"delta_T_obs\": np.array([0.58, 2.47, 3.55, 4.95, 1.21]), # K\n            \"sigma\": 0.10, # K\n        },\n        {\n            \"q\": np.array([0.20e4]), # W/m^2\n            \"delta_T_obs\": np.array([0.64]), # K\n            \"sigma\": 0.30, # K\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_posterior_mean(\n            case[\"q\"],\n            case[\"delta_T_obs\"],\n            case[\"sigma\"],\n            mu_0,\n            s_0,\n            R_min,\n            R_max,\n            N_grid\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\ndef compute_posterior_mean(q, delta_T_obs, sigma, mu_0, s_0, R_min, R_max, N_grid):\n    \"\"\"\n    Computes the posterior mean of R using numerical integration.\n\n    Args:\n        q (np.ndarray): Array of heat flux values [W/m^2].\n        delta_T_obs (np.ndarray): Array of observed temperature drops [K].\n        sigma (float): Standard deviation of measurement noise [K].\n        mu_0 (float): Mean of the logarithm of R for the Log-Normal prior.\n        s_0 (float): Standard deviation of the logarithm of R for the Log-Normal prior.\n        R_min (float): Minimum value for the integration grid for R [m^2K/W].\n        R_max (float): Maximum value for the integration grid for R [m^2K/W].\n        N_grid (int): Number of points in the integration grid.\n\n    Returns:\n        float: The posterior mean of R [m^2K/W].\n    \"\"\"\n    # 1. Create a logarithmically spaced grid for R.\n    R_grid = np.logspace(np.log10(R_min), np.log10(R_max), N_grid)\n\n    # 2. Compute log-likelihood.\n    # Pre-calculate summary statistics for efficiency.\n    S_qq = np.sum(q**2)\n    S_qT = np.sum(q * delta_T_obs)\n    S_TT = np.sum(delta_T_obs**2)\n    \n    # The term in the likelihood sum is a quadratic in R: S_TT - 2*R*S_qT + R^2*S_qq\n    likelihood_quadratic = S_TT - 2 * R_grid * S_qT + R_grid**2 * S_qq\n    log_likelihood = -0.5 / (sigma**2) * likelihood_quadratic\n\n    # 3. Compute log-prior.\n    log_R_grid = np.log(R_grid)\n    log_prior = -log_R_grid - (log_R_grid - mu_0)**2 / (2 * s_0**2)\n\n    # 4. Compute unnormalized log-posterior.\n    log_posterior = log_likelihood + log_prior\n    \n    # 5. Stabilize log-posterior to avoid numerical underflow.\n    log_posterior_stable = log_posterior - np.max(log_posterior)\n    \n    # 6. Exponentiate to get unnormalized posterior.\n    posterior_unnorm = np.exp(log_posterior_stable)\n\n    # 7. Compute integrals using the trapezoidal rule.\n    # Numerator integral: integral of R * posterior(R) dR\n    numerator_integral = trapezoid(R_grid * posterior_unnorm, R_grid)\n    \n    # Denominator integral (normalization constant): integral of posterior(R) dR\n    norm_constant = trapezoid(posterior_unnorm, R_grid)\n\n    # 8. Compute the posterior mean.\n    posterior_mean = numerator_integral / norm_constant\n\n    return posterior_mean\n\nsolve()\n```"
        },
        {
            "introduction": "Moving beyond single-parameter estimation, this exercise tackles the common engineering task of fitting a linear model where both the model parameters and the measurement noise level are unknown. You will leverage the power of conjugate priors—specifically the Normal-Inverse-Gamma distribution—to derive analytical expressions for the posterior distribution of the model parameters and the noise variance. This demonstrates a powerful shortcut that bypasses the need for intensive numerical integration in certain well-behaved models. By completing this practice , you will gain experience with a cornerstone of Bayesian statistics and appreciate the elegance and computational efficiency of using conjugate families.",
            "id": "2374101",
            "problem": "You are given a signal-plus-noise model in which observed data $\\{(x_i, y_i)\\}_{i=1}^n$ satisfy\n$y_i = f(x_i; \\theta) + \\epsilon_i$,\nwhere $\\epsilon_i$ are independent and identically distributed Gaussian noise terms with mean $0$ and unknown variance $\\sigma^2$. Consider the parametric case where $f$ is linear in the parameters, specifically $f(x_i; \\theta) = \\theta_0 + \\theta_1 x_i$ with $\\theta = [\\theta_0, \\theta_1]^\\top$.\n\nThe task is to perform Bayesian inference for simultaneous estimation of the parameters $\\theta$ and the noise variance $\\sigma^2$. Use a conjugate Normal–Inverse-Gamma prior defined as follows:\n- Conditional prior for the parameters given the variance: $\\theta \\mid \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2 S_0)$, where $m_0 \\in \\mathbb{R}^2$ and $S_0 \\in \\mathbb{R}^{2 \\times 2}$ is symmetric positive definite.\n- Prior for the variance: $\\sigma^2 \\sim \\text{Inverse-Gamma}(\\alpha_0, \\beta_0)$, where the Inverse-Gamma (first appearance) has density $p(\\sigma^2 \\mid \\alpha, \\beta) \\propto (\\sigma^2)^{-(\\alpha+1)} \\exp(-\\beta/\\sigma^2)$ for $\\sigma^2  0$.\n\nYour tasks are:\n- Starting only from Bayes’ theorem, the Gaussian likelihood for independent observations, the definition of the multivariate Normal distribution, and the definition of the Inverse-Gamma distribution, derive the posterior distribution of $(\\theta, \\sigma^2)$ up to a normalization constant. Avoid any reliance on pre-memorized formulas not derivable from these bases.\n- Based on your derivation, design an algorithm to compute the posterior means $\\mathbb{E}[\\theta \\mid \\mathcal{D}]$ and $\\mathbb{E}[\\sigma^2 \\mid \\mathcal{D}]$, where $\\mathcal{D}$ denotes the dataset. Express these means entirely in terms of the prior hyperparameters, the data, and fundamental linear algebra operations.\n- Implement a complete, runnable program that, for each dataset in the test suite below, computes and outputs the posterior means of $\\theta_0$, $\\theta_1$, and $\\sigma^2$. All outputs must be real numbers rounded to $6$ decimal places.\n\nUse the following prior hyperparameters for all test cases:\n- $m_0 = [0, 0]^\\top$,\n- $S_0 = \\mathrm{diag}(100, 100)$,\n- $\\alpha_0 = 2$,\n- $\\beta_0 = 1$.\n\nTest suite (each case provides observed inputs $x$ and outputs $y$):\n- Case A (well-conditioned, moderate noise):\n  - $x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]$,\n  - $y = [1.5, 0.9, 0.04, -0.58, -1.34, -1.92, -2.72, -3.34, -4.18, -4.76]$.\n- Case B (small sample, near-noise-only):\n  - $x = [0.0, 0.1, 0.2]$,\n  - $y = [0.002, -0.001, 0.0005]$.\n- Case C (moderate slope, moderate noise, mild leverage):\n  - $x = [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5]$,\n  - $y = [2.03, 2.44, 3.015, 3.5, 4.06, 4.47, 4.985, 5.545]$.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each case, report the posterior means in the order $\\mathbb{E}[\\theta_0 \\mid \\mathcal{D}]$, $\\mathbb{E}[\\theta_1 \\mid \\mathcal{D}]$, $\\mathbb{E}[\\sigma^2 \\mid \\mathcal{D}]$.\n- Concatenate the results for Case A, Case B, and Case C, in this order, to form a single flat list of $9$ numbers.\n- Each number must be rounded to $6$ decimal places.\n- Example structural template (not actual values): $[a_1,a_2,a_3,a_4,a_5,a_6,a_7,a_8,a_9]$.",
            "solution": "The problem requires the derivation and implementation of a Bayesian inference algorithm for the parameters of a linear model with Gaussian noise. The analysis must proceed from first principles.\n\nLet the observed data be $\\mathcal{D} = \\{ (x_i, y_i) \\}_{i=1}^n$. The model is given by $y_i = \\theta_0 + \\theta_1 x_i + \\epsilon_i$, where the noise terms $\\epsilon_i$ are independent and identically distributed according to a Gaussian distribution with mean $0$ and unknown variance $\\sigma^2$, i.e., $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nThis model can be expressed in vector form. Let $y = [y_1, \\dots, y_n]^\\top$ be the vector of observations, $\\theta = [\\theta_0, \\theta_1]^\\top$ be the vector of parameters, and $X$ be the $n \\times 2$ design matrix, where the $i$-th row is $[1, x_i]$. The model is then $y = X\\theta + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, with $I_n$ being the $n \\times n$ identity matrix.\n\nThe likelihood of the data given the parameters $\\theta$ and variance $\\sigma^2$ is the probability density of observing $y$ under the model:\n$$\np(y \\mid X, \\theta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}(y - X\\theta)^\\top(y - X\\theta)\\right)\n$$\n\nThe problem specifies a conjugate Normal-Inverse-Gamma prior for $(\\theta, \\sigma^2)$. The joint prior distribution $p(\\theta, \\sigma^2)$ is factored as $p(\\theta \\mid \\sigma^2)p(\\sigma^2)$.\nThe conditional prior for $\\theta$ is a multivariate normal distribution, $\\theta \\mid \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2 S_0)$, where $k=2$ is the dimension of $\\theta$:\n$$\np(\\theta \\mid \\sigma^2) = (2\\pi\\sigma^2)^{-k/2} |\\det(S_0)|^{-1/2} \\exp\\left(-\\frac{1}{2\\sigma^2}(\\theta - m_0)^\\top S_0^{-1}(\\theta - m_0)\\right)\n$$\nThe prior for the variance $\\sigma^2$ is an Inverse-Gamma distribution, $\\sigma^2 \\sim \\text{Inverse-Gamma}(\\alpha_0, \\beta_0)$, with the specified density:\n$$\np(\\sigma^2) \\propto (\\sigma^2)^{-(\\alpha_0+1)} \\exp(-\\beta_0/\\sigma^2)\n$$\n\nAccording to Bayes' theorem, the joint posterior distribution of the parameters is proportional to the product of the likelihood and the prior:\n$$\np(\\theta, \\sigma^2 \\mid \\mathcal{D}) \\propto p(y \\mid X, \\theta, \\sigma^2) p(\\theta \\mid \\sigma^2) p(\\sigma^2)\n$$\nSubstituting the expressions for a given dataset $\\mathcal{D}=(X,y)$:\n$$\np(\\theta, \\sigma^2 \\mid \\mathcal{D}) \\propto \\left[ (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}(y - X\\theta)^\\top(y - X\\theta)\\right) \\right] \\times \\left[ (\\sigma^2)^{-k/2} \\exp\\left(-\\frac{1}{2\\sigma^2}(\\theta - m_0)^\\top S_0^{-1}(\\theta - m_0)\\right) \\right] \\times \\left[ (\\sigma^2)^{-(\\alpha_0+1)} \\exp(-\\beta_0/\\sigma^2) \\right]\n$$\nThe constant factors $(2\\pi)^{-n/2}$, $(2\\pi)^{-k/2}$, and $|\\det(S_0)|^{-1/2}$ are absorbed into the proportionality constant. Combining terms yields:\n$$\np(\\theta, \\sigma^2 \\mid \\mathcal{D}) \\propto (\\sigma^2)^{-(\\alpha_0 + n/2 + k/2 + 1)} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left[ (y - X\\theta)^\\top(y - X\\theta) + (\\theta - m_0)^\\top S_0^{-1}(\\theta - m_0) + 2\\beta_0 \\right] \\right\\}\n$$\nTo identify the structure of the posterior, we analyze the term in the exponent which is quadratic in $\\theta$. Let this quadratic form be $Q(\\theta) = (y - X\\theta)^\\top(y - X\\theta) + (\\theta - m_0)^\\top S_0^{-1}(\\theta - m_0)$. Expanding and collecting terms in $\\theta$:\n$$\n\\begin{aligned}\nQ(\\theta) = (y^\\top y - 2y^\\top X \\theta + \\theta^\\top X^\\top X \\theta) + (\\theta^\\top S_0^{-1}\\theta - 2\\theta^\\top S_0^{-1}m_0 + m_0^\\top S_0^{-1}m_0) \\\\\n= \\theta^\\top(X^\\top X + S_0^{-1})\\theta - 2\\theta^\\top(X^\\top y + S_0^{-1}m_0) + y^\\top y + m_0^\\top S_0^{-1}m_0\n\\end{aligned}\n$$\nThis expression can be completed to the square with respect to $\\theta$. Define the posterior precision matrix $S_n^{-1}$ and posterior mean vector $m_n$ as:\n$$\nS_n^{-1} = X^\\top X + S_0^{-1}\n$$\n$$\nm_n = (S_n^{-1})^{-1}(X^\\top y + S_0^{-1}m_0)\n$$\nNote that $S_n = (X^\\top X + S_0^{-1})^{-1}$. The quadratic form $Q(\\theta)$ becomes:\n$$\nQ(\\theta) = (\\theta - m_n)^\\top S_n^{-1}(\\theta - m_n) - m_n^\\top S_n^{-1} m_n + y^\\top y + m_0^\\top S_0^{-1}m_0\n$$\nSubstituting this back into the posterior expression:\n$$\np(\\theta, \\sigma^2 \\mid \\mathcal{D}) \\propto (\\sigma^2)^{-(\\alpha_0 + n/2 + k/2 + 1)} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left[ (\\theta - m_n)^\\top S_n^{-1}(\\theta - m_n) + y^\\top y + m_0^\\top S_0^{-1}m_0 - m_n^\\top S_n^{-1}m_n + 2\\beta_0 \\right] \\right\\}\n$$\nThis expression reveals that the posterior is also of the Normal-Inverse-Gamma form. We can factor it as $p(\\theta, \\sigma^2 \\mid \\mathcal{D}) = p(\\theta \\mid \\sigma^2, \\mathcal{D}) p(\\sigma^2 \\mid \\mathcal{D})$.\n\nThe conditional posterior for $\\theta$ is:\n$$\np(\\theta \\mid \\sigma^2, \\mathcal{D}) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}(\\theta - m_n)^\\top S_n^{-1}(\\theta - m_n)\\right)\n$$\nThis is the kernel of a multivariate normal distribution, $\\theta \\mid \\sigma^2, \\mathcal{D} \\sim \\mathcal{N}(m_n, \\sigma^2 S_n)$.\n\nThe marginal posterior for $\\sigma^2$ is obtained by integrating out $\\theta$. By recognizing the integral of the Gaussian kernel, we find:\n$$\np(\\sigma^2 \\mid \\mathcal{D}) \\propto (\\sigma^2)^{-(\\alpha_0 + n/2 + k/2 + 1)} (\\sigma^2)^{k/2} \\exp\\left\\{ -\\frac{1}{2\\sigma^2} \\left[ 2\\beta_0 + y^\\top y + m_0^\\top S_0^{-1}m_0 - m_n^\\top S_n^{-1}m_n \\right] \\right\\}\n$$\n$$\np(\\sigma^2 \\mid \\mathcal{D}) \\propto (\\sigma^2)^{-(\\alpha_n+1)} \\exp(-\\beta_n/\\sigma^2)\n$$\nwhere the posterior hyperparameters $\\alpha_n$ and $\\beta_n$ are:\n$$\n\\alpha_n = \\alpha_0 + \\frac{n}{2}\n$$\n$$\n\\beta_n = \\beta_0 + \\frac{1}{2}\\left(y^\\top y + m_0^\\top S_0^{-1}m_0 - m_n^\\top S_n^{-1}m_n\\right)\n$$\nThis is the kernel of an Inverse-Gamma distribution, $\\sigma^2 \\mid \\mathcal{D} \\sim \\text{Inverse-Gamma}(\\alpha_n, \\beta_n)$.\n\nThe posterior means are computed from the properties of these distributions.\nThe posterior mean of $\\theta$ is found using the law of total expectation:\n$$\n\\mathbb{E}[\\theta \\mid \\mathcal{D}] = \\mathbb{E}[\\mathbb{E}[\\theta \\mid \\sigma^2, \\mathcal{D}]] = \\mathbb{E}[m_n] = m_n\n$$\nThe posterior mean of $\\sigma^2$ is the mean of the Inverse-Gamma$(\\alpha_n, \\beta_n)$ distribution, which is $\\frac{\\beta_n}{\\alpha_n - 1}$ for $\\alpha_n  1$. Given $\\alpha_0=2$ and $n \\ge 1$, $\\alpha_n = 2 + n/2  1$, so the mean is well-defined.\n$$\n\\mathbb{E}[\\sigma^2 \\mid \\mathcal{D}] = \\frac{\\beta_n}{\\alpha_n - 1}\n$$\n\nThe algorithm for computing the posterior means is as follows:\n1.  Given data $\\{ (x_i, y_i) \\}_{i=1}^n$ and prior hyperparameters $m_0, S_0, \\alpha_0, \\beta_0$.\n2.  Construct the design matrix $X$ of size $n \\times 2$ and the observation vector $y$ of size $n \\times 1$.\n3.  Compute $S_0^{-1}$.\n4.  Compute the posterior parameter $S_n^{-1} = X^\\top X + S_0^{-1}$.\n5.  Compute the posterior mean for $\\theta$: $m_n = (S_n^{-1})^{-1}(X^\\top y + S_0^{-1}m_0)$. This gives $\\mathbb{E}[\\theta \\mid \\mathcal{D}]$.\n6.  Compute the posterior parameter $\\alpha_n = \\alpha_0 + n/2$.\n7.  Compute the posterior parameter $\\beta_n = \\beta_0 + \\frac{1}{2}(y^\\top y + m_0^\\top S_0^{-1}m_0 - m_n^\\top S_n^{-1} m_n)$. A computationally simpler form for the last term is $m_n^\\top(X^\\top y + S_0^{-1}m_0)$.\n8.  Compute the posterior mean for $\\sigma^2$: $\\mathbb{E}[\\sigma^2 \\mid \\mathcal{D}] = \\frac{\\beta_n}{\\alpha_n - 1}$.\n\nThis provides a complete, principled procedure for calculating the required posterior expectations.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs Bayesian inference for a linear model to estimate parameters\n    theta = [theta_0, theta_1] and noise variance sigma^2.\n    \"\"\"\n    # Prior hyperparameters\n    m0 = np.array([0.0, 0.0])\n    S0 = np.array([[100.0, 0.0], [0.0, 100.0]])\n    alpha0 = 2.0\n    beta0 = 1.0\n\n    # Test suite\n    test_cases = [\n        # Case A (well-conditioned, moderate noise)\n        {\n            \"x\": np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n            \"y\": np.array([1.5, 0.9, 0.04, -0.58, -1.34, -1.92, -2.72, -3.34, -4.18, -4.76]),\n        },\n        # Case B (small sample, near-noise-only)\n        {\n            \"x\": np.array([0.0, 0.1, 0.2]),\n            \"y\": np.array([0.002, -0.001, 0.0005]),\n        },\n        # Case C (moderate slope, moderate noise, mild leverage)\n        {\n            \"x\": np.array([0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5]),\n            \"y\": np.array([2.03, 2.44, 3.015, 3.5, 4.06, 4.47, 4.985, 5.545]),\n        },\n    ]\n\n    results = []\n\n    # Pre-compute S0 inverse\n    S0_inv = np.linalg.inv(S0)\n\n    for case in test_cases:\n        x_data = case[\"x\"]\n        y_data = case[\"y\"]\n        n = len(x_data)\n\n        # 1. Construct design matrix X and observation vector y\n        X = np.vstack((np.ones(n), x_data)).T\n        y = y_data.reshape(-1, 1)\n\n        # 2. Compute components for posterior updates\n        XTX = X.T @ X\n        XTy = X.T @ y\n\n        # 3. Compute posterior hyperparameters for theta\n        Sn_inv = XTX + S0_inv\n        Sn = np.linalg.inv(Sn_inv)\n        \n        # S0_inv @ m0 is a zero vector since m0 is zero\n        mn = Sn @ XTy\n        \n        # 4. Compute posterior hyperparameters for sigma^2\n        alpha_n = alpha0 + n / 2.0\n        \n        # y.T @ y + m0.T @ S0_inv @ m0 - mn.T @ Sn_inv @ mn\n        # The m0 term is zero\n        # The term mn.T @ Sn_inv @ mn simplifies to mn.T @ (XTy + S0_inv @ m0)\n        # which is mn.T @ XTy since m0 is zero\n        yTy = y.T @ y\n        mnT_Sn_inv_mn = mn.T @ XTy\n        \n        beta_n = beta0 + 0.5 * (yTy - mnT_Sn_inv_mn)\n\n        # 5. Compute posterior means\n        E_theta = mn\n        E_sigma2 = beta_n / (alpha_n - 1)\n\n        # Append results rounded to 6 decimal places\n        results.append(f\"{E_theta[0, 0]:.6f}\")\n        results.append(f\"{E_theta[1, 0]:.6f}\")\n        results.append(f\"{E_sigma2[0, 0]:.6f}\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This final, challenging practice applies Bayesian inference to a complex, non-linear dynamical system: the famous Lorenz attractor. You will estimate the hidden parameters governing its chaotic behavior using only a short, noisy time series of one of its states. When analytical solutions like conjugate priors are not available, we return to numerical methods; this problem requires you to combine a numerical ODE solver with the grid-based posterior approximation from the first practice. This capstone exercise  integrates skills across computational science, preparing you to tackle real-world inverse problems where the underlying physical model is a complex computer simulation.",
            "id": "2374071",
            "problem": "You are given a short, noisy time series of a single state variable generated from a deterministic, continuous-time dynamical system known as the Lorenz system. The Lorenz system is defined by the ordinary differential equations\n$$\n\\frac{dx}{dt} = \\sigma (y - x), \\quad\n\\frac{dy}{dt} = x (\\rho - z) - y, \\quad\n\\frac{dz}{dt} = x y - \\beta z,\n$$\nwhere $\\sigma$, $\\rho$, and $\\beta$ are unknown positive parameters. You will observe only the $x$-component at discrete times in the presence of additive noise. Assume the observation model\n$$\ny_i = x(t_i \\mid \\sigma,\\rho,\\beta) + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\tau^2),\n$$\nwith known noise standard deviation $\\tau$, known initial condition $(x(0),y(0),z(0))$, and known uniform time step $\\Delta t$ so that $t_i = i \\Delta t$. The forward map $(\\sigma,\\rho,\\beta) \\mapsto x(t_i \\mid \\sigma,\\rho,\\beta)$ is defined by numerically integrating the Lorenz system from the initial condition over the specified time grid. For numerical integration, use a fixed-step, fourth-order Runge–Kutta scheme with time step equal to $\\Delta t$.\n\nAssume independent uniform priors for the parameters:\n$$\n\\sigma \\sim \\mathrm{Uniform}([5,15]), \\quad \\rho \\sim \\mathrm{Uniform}([0,40]), \\quad \\beta \\sim \\mathrm{Uniform}([2,3]).\n$$\nUse a tensor-product parameter grid with $n_\\sigma = 11$ linearly spaced points spanning $[5,15]$, $n_\\rho = 17$ linearly spaced points spanning $[0,40]$, and $n_\\beta = 11$ linearly spaced points spanning $[2,3]$. Combine this prior with the Gaussian likelihood implied by the observation model to obtain the posterior distribution over $(\\sigma,\\rho,\\beta)$. Approximate the posterior using the grid and report the posterior mean of each parameter for each test case.\n\nThe test suite consists of $3$ cases that specify the true parameters used to generate the synthetic observations, the initial state, the time step, the number of observation intervals, the observation noise standard deviation, and a fixed random seed. For each case, generate the observations internally by simulating the Lorenz system with the true parameters and adding independent Gaussian noise using the given seed. Then, using only the noisy observations (not the true parameters), perform Bayesian inference on $(\\sigma,\\rho,\\beta)$ as described above, and output the posterior mean for each parameter.\n\nTest suite:\n- Case $1$ (chaotic, moderate noise):\n  - True parameters: $\\sigma^\\star = 10$, $\\rho^\\star = 28$, $\\beta^\\star = 8/3$.\n  - Initial condition: $(x(0),y(0),z(0)) = (-8, 8, 27)$.\n  - Time step: $\\Delta t = 0.02$.\n  - Number of intervals: $N = 60$ (so there are $N+1 = 61$ observation times $t_i$).\n  - Noise standard deviation: $\\tau = 1.0$.\n  - Random seed: $314159$.\n- Case $2$ (chaotic, short series, higher noise):\n  - True parameters: $\\sigma^\\star = 12$, $\\rho^\\star = 35$, $\\beta^\\star = 2.5$.\n  - Initial condition: $(x(0),y(0),z(0)) = (0, 1, 1.05)$.\n  - Time step: $\\Delta t = 0.02$.\n  - Number of intervals: $N = 40$ (so there are $N+1 = 41$ observation times $t_i$).\n  - Noise standard deviation: $\\tau = 2.0$.\n  - Random seed: $271828$.\n- Case $3$ (non-chaotic, low noise):\n  - True parameters: $\\sigma^\\star = 9$, $\\rho^\\star = 5$, $\\beta^\\star = 2.2$.\n  - Initial condition: $(x(0),y(0),z(0)) = (1, 1, 1)$.\n  - Time step: $\\Delta t = 0.02$.\n  - Number of intervals: $N = 40$ (so there are $N+1 = 41$ observation times $t_i$).\n  - Noise standard deviation: $\\tau = 0.2$.\n  - Random seed: $42$.\n\nImportant implementation constraints:\n- Use a fixed-step, fourth-order Runge–Kutta integrator with step $\\Delta t$ to obtain $x(t_i \\mid \\sigma,\\rho,\\beta)$ for $i=0,\\dots,N$. Do not use any adaptive scheme.\n- Use the specified tensor-product grid with $n_\\sigma = 11$, $n_\\rho = 17$, $n_\\beta = 11$ points.\n- Compute the posterior mean of each parameter by normalizing the unnormalized posterior weights on the grid. Use a numerically stable exponentiation strategy when needed.\n- For each case, return the posterior means of $(\\sigma,\\rho,\\beta)$ rounded to $3$ decimal places.\n\nFinal output format:\n- Your program should produce a single line containing a list of three lists, one per test case, where each inner list contains the three posterior means in the order $[\\hat{\\sigma},\\hat{\\rho},\\hat{\\beta}]$ for that case. The format must be exactly\n$$\n[[\\hat{\\sigma}_1,\\hat{\\rho}_1,\\hat{\\beta}_1],[\\hat{\\sigma}_2,\\hat{\\rho}_2,\\hat{\\beta}_2],[\\hat{\\sigma}_3,\\hat{\\rho}_3,\\hat{\\beta}_3]]\n$$\nwith each number rounded to $3$ decimal places and no spaces.",
            "solution": "The problem posed is an exercise in Bayesian parameter estimation for a system of ordinary differential equations, specifically the Lorenz system. The task is to infer the parameters $(\\sigma, \\rho, \\beta)$ from noisy observations of a single state variable, $x(t)$. Before proceeding to a solution, a rigorous validation of the problem statement is mandatory.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n\nThe problem provides the following information:\n- **Dynamical System**: The Lorenz system, defined by the equations:\n$$\n\\frac{dx}{dt} = \\sigma (y - x), \\quad\n\\frac{dy}{dt} = x (\\rho - z) - y, \\quad\n\\frac{dz}{dt} = x y - \\beta z\n$$\n- **Observation Model**: Noisy observations $y_i$ of the state variable $x$ at discrete times $t_i = i \\Delta t$:\n$$\ny_i = x(t_i \\mid \\sigma,\\rho,\\beta) + \\varepsilon_i, \\quad \\text{where } \\varepsilon_i \\sim \\mathcal{N}(0,\\tau^2)\n$$\n- **Knowns**: Initial condition $(x(0),y(0),z(0))$, uniform time step $\\Delta t$, and noise standard deviation $\\tau$ are known for the inference task.\n- **Numerical Method**: The forward map $(\\sigma,\\rho,\\beta) \\mapsto x(t_i)$ is to be computed using a fixed-step, fourth-order Runge-Kutta scheme with time step $\\Delta t$.\n- **Prior Distributions**: Independent uniform priors are assumed for the parameters:\n$$\n\\sigma \\sim \\mathrm{Uniform}([5,15]), \\quad \\rho \\sim \\mathrm{Uniform}([0,40]), \\quad \\beta \\sim \\mathrm{Uniform}([2,3])\n$$\n- **Parameter Grid**: A tensor-product grid with $n_\\sigma = 11$, $n_\\rho = 17$, and $n_\\beta = 11$ linearly spaced points covering the support of the priors.\n- **Task**: Compute the posterior mean of each parameter $(\\sigma, \\rho, \\beta)$ for three test cases.\n- **Test Cases**:\n    - **Case 1**: True parameters $(\\sigma^\\star, \\rho^\\star, \\beta^\\star) = (10, 28, 8/3)$. Initial condition $(x(0),y(0),z(0)) = (-8, 8, 27)$. Time step $\\Delta t = 0.02$. Number of intervals $N = 60$. Noise $\\tau = 1.0$. Random seed $= 314159$.\n    - **Case 2**: True parameters $(\\sigma^\\star, \\rho^\\star, \\beta^\\star) = (12, 35, 2.5)$. Initial condition $(x(0),y(0),z(0)) = (0, 1, 1.05)$. Time step $\\Delta t = 0.02$. Number of intervals $N = 40$. Noise $\\tau = 2.0$. Random seed $= 271828$.\n    - **Case 3**: True parameters $(\\sigma^\\star, \\rho^\\star, \\beta^\\star) = (9, 5, 2.2)$. Initial condition $(x(0),y(0),z(0)) = (1, 1, 1)$. Time step $\\Delta t = 0.02$. Number of intervals $N = 40$. Noise $\\tau = 0.2$. Random seed $= 42$.\n- **Output Format**: A single-line string representing a list of three lists, with each inner list containing the posterior means $[\\hat{\\sigma}, \\hat{\\rho}, \\hat{\\beta}]$ rounded to $3$ decimal places for each case.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the established criteria:\n1.  **Scientifically Grounded**: The problem is founded on standard principles. The Lorenz system is a canonical model in chaos theory, Bayesian inference is a fundamental methodology in statistics, and the Runge-Kutta method is a cornerstone of numerical analysis for ODEs. The problem is scientifically sound.\n2.  **Well-Posed**: The problem is well-posed. It requires the computation of posterior means based on a fully specified model, data generation procedure, and computational algorithm (grid-based approximation). The instructions are precise, leading to a unique, deterministic result for the given random seeds.\n3.  **Objective**: The problem is stated in clear, objective mathematical language, free from ambiguity or subjective interpretation.\n4.  **Completeness and Consistency**: The problem is self-contained. All necessary components—the model, priors, likelihood, numerical methods, data generation parameters, and grid specifications—are explicitly provided. There are no contradictions.\n5.  **Feasibility**: The specified task is a synthetic data experiment, a standard practice for method validation. The computational load, involving $11 \\times 17 \\times 11 = 2057$ integrations of a small ODE system over a short time horizon ($N \\le 60$ steps), is entirely feasible on modern hardware.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid, complete, and well-posed. We may proceed with the solution.\n\n**Solution Derivation**\n\nThe objective is to compute the posterior mean of the parameters $\\theta = (\\sigma, \\rho, \\beta)$ given a set of noisy observations $Y = \\{y_0, y_1, \\dots, y_N\\}$.\n\n**1. Bayesian Framework**\nAccording to Bayes' theorem, the posterior probability distribution of the parameters given the data is:\n$$\np(\\theta | Y) = \\frac{p(Y | \\theta) p(\\theta)}{p(Y)} \\propto p(Y | \\theta) p(\\theta)\n$$\nwhere $p(Y | \\theta)$ is the likelihood and $p(\\theta)$ is the prior.\n\nThe prior distribution $p(\\theta)$ is specified as uniform over a hyperrectangle:\n$$\np(\\theta) = p(\\sigma)p(\\rho)p(\\beta) \\propto \\mathbb{I}(\\sigma \\in [5,15]) \\cdot \\mathbb{I}(\\rho \\in [0,40]) \\cdot \\mathbb{I}(\\beta \\in [2,3])\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. Since the computational grid lies entirely within this support, the prior is constant for all grid points and can be ignored when computing posterior proportions.\n\nThe likelihood $p(Y | \\theta)$ is determined by the observation model. The noise terms $\\varepsilon_i$ are independent and identically distributed, drawn from a Gaussian distribution $\\mathcal{N}(0, \\tau^2)$. The likelihood of observing the data sequence $Y$ for a given set of parameters $\\theta$ is the product of the probabilities of each observation:\n$$\np(Y | \\theta) = \\prod_{i=0}^{N} p(y_i | \\theta) = \\prod_{i=0}^{N} \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left( -\\frac{(y_i - x(t_i | \\theta))^2}{2\\tau^2} \\right)\n$$\nwhere $x(t_i | \\theta)$ is the solution of the Lorenz system for the $x$-component at time $t_i$ with parameters $\\theta$.\n\nFor numerical stability, it is preferable to work with the log-likelihood:\n$$\n\\log p(Y | \\theta) = \\sum_{i=0}^{N} \\left( -\\frac{1}{2}\\log(2\\pi\\tau^2) - \\frac{(y_i - x(t_i | \\theta))^2}{2\\tau^2} \\right)\n$$\nSince the posterior is proportional to the likelihood on the grid, we have:\n$$\np(\\theta | Y) \\propto \\exp\\left( -\\frac{1}{2\\tau^2} \\sum_{i=0}^{N} (y_i - x(t_i | \\theta))^2 \\right)\n$$\nThe term $\\sum_{i=0}^{N} (y_i - x(t_i | \\theta))^2$ is the Sum of Squared Errors (SSE) between the observations and the model prediction for parameters $\\theta$.\n\n**2. Numerical Approximation**\nThe continuous parameter space is discretized into a finite grid. Let the grid points be denoted by $\\theta_j$ for $j=1, \\dots, M$, where $M = n_\\sigma n_\\rho n_\\beta$.\n\n- **Forward Model Simulation**: For each grid point $\\theta_j = (\\sigma, \\rho, \\beta)$, the Lorenz system is integrated numerically from the given initial condition to obtain the predicted trajectory $\\{x(t_i | \\theta_j)\\}_{i=0}^N$. The problem specifies a fourth-order Runge-Kutta (RK4) method with a fixed time step $\\Delta t$. For a state vector $\\mathbf{s} = (x,y,z)$ and ODE $\\frac{d\\mathbf{s}}{dt} = f(t, \\mathbf{s})$, one step of RK4 is:\n$$\n\\begin{aligned}\n\\mathbf{k}_1 = f(t_n, \\mathbf{s}_n) \\\\\n\\mathbf{k}_2 = f(t_n + \\frac{\\Delta t}{2}, \\mathbf{s}_n + \\frac{\\Delta t}{2}\\mathbf{k}_1) \\\\\n\\mathbf{k}_3 = f(t_n + \\frac{\\Delta t}{2}, \\mathbf{s}_n + \\frac{\\Delta t}{2}\\mathbf{k}_2) \\\\\n\\mathbf{k}_4 = f(t_n + \\Delta t, \\mathbf{s}_n + \\Delta t\\mathbf{k}_3) \\\\\n\\mathbf{s}_{n+1} = \\mathbf{s}_n + \\frac{\\Delta t}{6}(\\mathbf{k}_1 + 2\\mathbf{k}_2 + 2\\mathbf{k}_3 + \\mathbf{k}_4)\n\\end{aligned}\n$$\n\n- **Posterior Weight Calculation**: For each grid point $\\theta_j$, we compute the unnormalized log-posterior value $S_j = -\\frac{1}{2\\tau^2} \\text{SSE}(\\theta_j)$. The unnormalized posterior weight is $w_j = \\exp(S_j)$. To prevent numerical overflow or underflow, we use a stabilization technique. Let $S_{\\max} = \\max_j S_j$. The stabilized weights are calculated as $w'_j = \\exp(S_j - S_{\\max})$.\n\n- **Posterior Mean Estimation**: The discrete approximation of the posterior mean for each parameter is the weighted average over the grid. For parameter $\\sigma$, this is:\n$$\n\\hat{\\sigma} = \\mathbb{E}[\\sigma | Y] \\approx \\frac{\\sum_j \\sigma_j w'_j}{\\sum_j w'_j}\n$$\nAnalogous formulas apply for $\\hat{\\rho}$ and $\\hat{\\beta}$.\n\n**3. Algorithmic Procedure**\n\nFor each of the three test cases:\n1.  **Generate Data**: Using the specified true parameters $(\\sigma^\\star, \\rho^\\star, \\beta^\\star)$, initial condition, time step $\\Delta t$, and number of intervals $N$, simulate the Lorenz system with the RK4 integrator to generate the true trajectory $\\{x(t_i)\\}_{i=0}^N$. Add Gaussian noise with standard deviation $\\tau$ using the provided random seed to produce the observation set $\\{y_i\\}_{i=0}^N$.\n2.  **Define Parameter Grid**: Construct arrays of parameter values for $\\sigma$, $\\rho$, and $\\beta$ using the specified ranges and number of points.\n3.  **Iterate Over Grid**: For each point $(\\sigma_k, \\rho_l, \\beta_m)$ in the tensor product grid:\n    a. Simulate the Lorenz system with these parameters using the RK4 integrator to get the predicted trajectory $\\{\\hat{x}_i\\}_{i=0}^N$.\n    b. Compute the SSE: $\\sum_{i=0}^{N} (y_i - \\hat{x}_i)^2$.\n    c. Compute and store the unnormalized log-posterior value: $S_{klm} = -\\frac{\\text{SSE}}{2\\tau^2}$.\n4.  **Compute Posterior Means**:\n    a. Find the maximum value of all computed log-posterior values, $S_{\\max}$.\n    b. Calculate the normalized posterior weights: $W_{klm} = \\exp(S_{klm} - S_{\\max}) / \\sum_{k',l',m'} \\exp(S_{k'l'm'} - S_{\\max})$.\n    c. Compute the posterior means as weighted sums:\n    $$\n    \\hat{\\sigma} = \\sum_{k,l,m} \\sigma_k W_{klm}, \\quad \\hat{\\rho} = \\sum_{k,l,m} \\rho_l W_{klm}, \\quad \\hat{\\beta} = \\sum_{k,l,m} \\beta_m W_{klm}\n    $$\n5.  **Store and Format Result**: Store the triplet $[\\hat{\\sigma}, \\hat{\\rho}, \\hat{\\beta}]$ rounded to three decimal places. After processing all cases, format the collected results into the required string representation.\nThis systematic procedure will be implemented to derive the final answer.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef lorenz_rhs(state, sigma, rho, beta):\n    \"\"\"\n    Computes the derivatives of the Lorenz system.\n    Note: The system is autonomous, so the time `t` is not used.\n    \"\"\"\n    x, y, z = state\n    dx_dt = sigma * (y - x)\n    dy_dt = x * (rho - z) - y\n    dz_dt = x * y - beta * z\n    return np.array([dx_dt, dy_dt, dz_dt])\n\ndef rk4_integrator(rhs, initial_state, num_steps, dt, *params):\n    \"\"\"\n    Fixed-step fourth-order Runge-Kutta integrator.\n    Produces a trajectory of num_steps + 1 states.\n    \"\"\"\n    num_vars = len(initial_state)\n    states = np.zeros((num_steps + 1, num_vars))\n    states[0] = initial_state\n    \n    current_state = initial_state.copy()\n    \n    for i in range(num_steps):\n        k1 = rhs(current_state, *params)\n        k2 = rhs(current_state + 0.5 * dt * k1, *params)\n        k3 = rhs(current_state + 0.5 * dt * k2, *params)\n        k4 = rhs(current_state + dt * k3, *params)\n        \n        current_state += (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n        states[i + 1] = current_state\n        \n    return states\n\ndef solve():\n    \"\"\"\n    Main function to solve the Bayesian inference problem for the Lorenz system.\n    \"\"\"\n    test_cases = [\n        # Case 1 (chaotic, moderate noise)\n        {\n            \"true_params\": (10.0, 28.0, 8.0/3.0),\n            \"initial_cond\": np.array([-8.0, 8.0, 27.0]),\n            \"dt\": 0.02,\n            \"N\": 60,\n            \"tau\": 1.0,\n            \"seed\": 314159\n        },\n        # Case 2 (chaotic, short series, higher noise)\n        {\n            \"true_params\": (12.0, 35.0, 2.5),\n            \"initial_cond\": np.array([0.0, 1.0, 1.05]),\n            \"dt\": 0.02,\n            \"N\": 40,\n            \"tau\": 2.0,\n            \"seed\": 271828\n        },\n        # Case 3 (non-chaotic, low noise)\n        {\n            \"true_params\": (9.0, 5.0, 2.2),\n            \"initial_cond\": np.array([1.0, 1.0, 1.0]),\n            \"dt\": 0.02,\n            \"N\": 40,\n            \"tau\": 0.2,\n            \"seed\": 42\n        }\n    ]\n\n    # Define the parameter grid\n    n_sigma, n_rho, n_beta = 11, 17, 11\n    sigma_vals = np.linspace(5, 15, n_sigma)\n    rho_vals = np.linspace(0, 40, n_rho)\n    beta_vals = np.linspace(2, 3, n_beta)\n\n    all_results = []\n    \n    for case in test_cases:\n        # 1. Generate synthetic observations\n        rng = np.random.default_rng(case[\"seed\"])\n        true_trajectory = rk4_integrator(\n            lorenz_rhs, \n            case[\"initial_cond\"], \n            case[\"N\"], \n            case[\"dt\"], \n            *case[\"true_params\"]\n        )\n        x_true = true_trajectory[:, 0]\n        noise = rng.normal(loc=0.0, scale=case[\"tau\"], size=case[\"N\"] + 1)\n        observations_y = x_true + noise\n        \n        # 2. Perform grid search to compute posterior distribution\n        log_posterior = np.zeros((n_sigma, n_rho, n_beta))\n        \n        for i, sigma_val in enumerate(sigma_vals):\n            for j, rho_val in enumerate(rho_vals):\n                for k, beta_val in enumerate(beta_vals):\n                    # Simulate forward model for this grid point\n                    sim_params = (sigma_val, rho_val, beta_val)\n                    sim_trajectory = rk4_integrator(\n                        lorenz_rhs,\n                        case[\"initial_cond\"],\n                        case[\"N\"],\n                        case[\"dt\"],\n                        *sim_params\n                    )\n                    x_sim = sim_trajectory[:, 0]\n                    \n                    # Calculate Sum of Squared Errors (SSE)\n                    sse = np.sum((observations_y - x_sim)**2)\n                    \n                    # Calculate log posterior (proportional to -SSE), ignoring constants\n                    log_posterior[i, j, k] = -sse / (2 * case[\"tau\"]**2)\n\n        # 3. Calculate posterior means from the grid-based posterior\n        # Use log-sum-exp trick for numerical stability\n        log_posterior -= np.max(log_posterior)\n        posterior_weights = np.exp(log_posterior)\n        \n        total_weight = np.sum(posterior_weights)\n        \n        # Use numpy broadcasting to compute weighted sums efficiently\n        sigma_mean = np.sum(posterior_weights * sigma_vals[:, None, None]) / total_weight\n        rho_mean = np.sum(posterior_weights * rho_vals[None, :, None]) / total_weight\n        beta_mean = np.sum(posterior_weights * beta_vals[None, None, :]) / total_weight\n        \n        case_results = [\n            round(sigma_mean, 3), \n            round(rho_mean, 3), \n            round(beta_mean, 3)\n        ]\n        all_results.append(case_results)\n\n    # 4. Format the final output string\n    # Using str() and replace() to ensure the exact format without spaces\n    output_str = str(all_results).replace(' ', '')\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}