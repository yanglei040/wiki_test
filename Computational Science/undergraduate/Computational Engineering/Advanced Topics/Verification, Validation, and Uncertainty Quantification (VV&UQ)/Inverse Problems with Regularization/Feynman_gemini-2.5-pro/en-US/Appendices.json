{
    "hands_on_practices": [
        {
            "introduction": "We begin with a practical application inspired by geophysical imaging. This problem challenges you to reconstruct the sound speed profile of a one-dimensional medium from travel-time data, a simplified model of acoustic tomography. By implementing Tikhonov regularization with a smoothness penalty, you will directly confront the challenges of ill-posedness and discover how regularization provides a robust solution even with noisy or incomplete data .",
            "id": "2405385",
            "problem": "Consider a one-dimensional acoustic medium occupying the interval $[0,L]$ with $L = 1000 \\ \\text{m}$. A point source is located at $x=0$. The unknown sound speed field $c(x)$ is assumed piecewise constant over $N$ equal cells of width $\\Delta x = L/N$, with $N = 10$. Let $c_i$ denote the sound speed (in $\\text{m/s}$) in cell $i \\in \\{1,2,\\dots,N\\}$, and let $s_i$ denote the slowness in that cell, defined by $s_i = 1/c_i$ (in $\\text{s/m}$). Travel-time measurements are made by receivers placed at cell boundaries $x_k = k \\, \\Delta x$ for selected $k \\in \\{1,2,\\dots,N\\}$. The travel time to receiver $k$ is defined by\n$$\nt_k \\;=\\; \\int_{0}^{x_k} \\frac{1}{c(x)} \\, dx \\;+\\; \\varepsilon_k \\;=\\; \\sum_{i=1}^{k} \\Delta x \\, s_i \\;+\\; \\varepsilon_k,\n$$\nwhere $\\varepsilon_k$ is an additive perturbation (in $\\text{s}$). Define the measurement matrix $W \\in \\mathbb{R}^{K \\times N}$, where $K$ is the number of receivers used, by\n$$\nW_{k,i} \\;=\\; \\begin{cases}\n\\Delta x, & i \\le k,\\\\\n0, & i > k,\n\\end{cases}\n$$\nfor $k \\in \\{1,\\dots,K\\}$ and $i \\in \\{1,\\dots,N\\}$. Let $\\mathbf{t} \\in \\mathbb{R}^{K}$ collect the $t_k$, and $\\mathbf{s} \\in \\mathbb{R}^{N}$ collect the $s_i$. The discrete forward model is $\\mathbf{t} = W \\mathbf{s} + \\boldsymbol{\\varepsilon}$.\n\nReconstruct $\\mathbf{c}$ by first reconstructing $\\mathbf{s}$ as the unique minimizer of the following regularized least-squares problem with quadratic roughness penalty:\n$$\n\\min_{\\mathbf{s} \\in \\mathbb{R}^{N}} \\; J(\\mathbf{s}) \\;=\\; \\frac{1}{2} \\lVert W \\mathbf{s} - \\mathbf{t} \\rVert_2^2 \\;+\\; \\frac{\\lambda}{2} \\lVert D \\mathbf{s} \\rVert_2^2,\n$$\nwhere $\\lambda \\ge 0$ is a scalar parameter, and $D \\in \\mathbb{R}^{(N-1)\\times N}$ is the first-difference operator defined by\n$$\n(D\\mathbf{s})_i \\;=\\; s_{i+1} - s_i, \\quad \\text{for } i \\in \\{1,\\dots,N-1\\},\n$$\nthat is, $D_{i,i} = -1$, $D_{i,i+1} = 1$, and all other entries of $D$ are zero. After obtaining the minimizer $\\widehat{\\mathbf{s}}$, compute the reconstructed sound speeds cell-wise by $\\widehat{c}_i = 1 / \\widehat{s}_i$ (in $\\text{m/s}$). For evaluation, use the root-mean-square error (in $\\text{m/s}$)\n$$\n\\text{RMSE} \\;=\\; \\sqrt{ \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\widehat{c}_i - c_i^{\\star} \\right)^2 },\n$$\nwith respect to the ground-truth $\\mathbf{c}^{\\star}$ specified below.\n\nUse the following ground-truth and test suite. The ground-truth cell-wise sound speed vector (in $\\text{m/s}$) is\n$$\n\\mathbf{c}^{\\star} \\;=\\; [\\,1480,\\;1480,\\;1500,\\;1520,\\;1550,\\;1550,\\;1530,\\;1500,\\;1490,\\;1480\\,].\n$$\nThe perturbations are deterministic and defined for a given $\\sigma \\ge 0$ by\n$$\n\\varepsilon_k \\;=\\; \\sigma \\, (-1)^k, \\quad \\text{for } k \\in \\{1,\\dots,K\\},\n$$\nwith units of seconds.\n\nTest suite (each row defines one test case with its parameters):\n- Case $1$: $K = 10$, receivers at all boundaries $x_k = k \\Delta x$ for $k \\in \\{1,\\dots,10\\}$, $\\sigma = 0$, $\\lambda = 0$.\n- Case $2$: $K = 6$, receivers at $x_k = k \\Delta x$ for $k \\in \\{1,\\dots,6\\}$, $\\sigma = 0$, $\\lambda = 10^{-2}$.\n- Case $3$: $K = 10$, receivers at $x_k = k \\Delta x$ for $k \\in \\{1,\\dots,10\\}$, $\\sigma = 5 \\times 10^{-4}$, $\\lambda = 10^{-3}$.\n- Case $4$: $K = 10$, receivers at $x_k = k \\Delta x$ for $k \\in \\{1,\\dots,10\\}$, $\\sigma = 5 \\times 10^{-3}$, $\\lambda = 1$.\n\nAll travel times must be expressed in seconds, and all sound speeds must be expressed in meters per second. Angles are not used in this problem.\n\nYour program must, for each test case, assemble $W$ and $D$ exactly as defined, synthesize $\\mathbf{t}$ from $\\mathbf{c}^{\\star}$ and the specified $\\sigma$, compute the unique minimizer $\\widehat{\\mathbf{s}}$ of $J(\\mathbf{s})$, convert to $\\widehat{\\mathbf{c}}$, and report the $\\text{RMSE}$ in meters per second as a floating-point number.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where $r_j$ is the $\\text{RMSE}$ (in $\\text{m/s}$) for test case $j$. The order must be Case $1$, Case $2$, Case $3$, Case $4$.",
            "solution": "The problem statement has been subjected to rigorous validation and is deemed valid. It is scientifically grounded, well-posed, objective, and internally consistent. It describes a standard problem in computational science: a one-dimensional, regularized linear inverse problem for acoustic travel-time tomography. All parameters and required definitions are provided, allowing for a unique and meaningful solution to be computed for each specified test case. We may therefore proceed with the derivation and implementation of the solution.\n\nThe core of the problem is to find the slowness vector $\\mathbf{s} \\in \\mathbb{R}^{N}$ that minimizes the Tikhonov-regularized objective function:\n$$\nJ(\\mathbf{s}) \\;=\\; \\frac{1}{2} \\lVert W \\mathbf{s} - \\mathbf{t} \\rVert_2^2 \\;+\\; \\frac{\\lambda}{2} \\lVert D \\mathbf{s} \\rVert_2^2\n$$\nHere, $\\mathbf{s}$ is the vector of unknown cell slownesses, $\\mathbf{t}$ is the vector of measured travel times, $W$ is the forward model matrix, $D$ is the first-difference operator acting as a regularization matrix, and $\\lambda \\ge 0$ is the regularization parameter.\n\nThe function $J(\\mathbf{s})$ is a quadratic, convex function of $\\mathbf{s}$. Its unique minimizer, which we denote $\\widehat{\\mathbf{s}}$, can be found by computing the gradient of $J(\\mathbf{s})$ with respect to $\\mathbf{s}$ and setting it to the zero vector. The gradient is given by:\n$$\n\\nabla_{\\mathbf{s}} J(\\mathbf{s}) \\;=\\; \\nabla_{\\mathbf{s}} \\left( \\frac{1}{2} (W\\mathbf{s} - \\mathbf{t})^T (W\\mathbf{s} - \\mathbf{t}) \\right) \\;+\\; \\nabla_{\\mathbf{s}} \\left( \\frac{\\lambda}{2} (D\\mathbf{s})^T (D\\mathbf{s}) \\right)\n$$\nUsing standard rules of matrix calculus, the gradients of the two terms are:\n$$\n\\nabla_{\\mathbf{s}} \\left( \\frac{1}{2} \\lVert W \\mathbf{s} - \\mathbf{t} \\rVert_2^2 \\right) = W^T(W\\mathbf{s} - \\mathbf{t})\n$$\n$$\n\\nabla_{\\mathbf{s}} \\left( \\frac{\\lambda}{2} \\lVert D \\mathbf{s} \\rVert_2^2 \\right) = \\lambda D^T D \\mathbf{s}\n$$\nSetting the total gradient to zero yields the equation:\n$$\nW^T(W\\widehat{\\mathbf{s}} - \\mathbf{t}) + \\lambda D^T D \\widehat{\\mathbf{s}} = \\mathbf{0}\n$$\nRearranging the terms to isolate $\\widehat{\\mathbf{s}}$, we obtain the normal equations for this regularized least-squares problem:\n$$\n(W^T W + \\lambda D^T D) \\widehat{\\mathbf{s}} \\;=\\; W^T \\mathbf{t}\n$$\nThis is a system of linear equations of the form $A\\mathbf{x} = \\mathbf{b}$, where the system matrix is $A = (W^T W + \\lambda D^T D)$, the unknown vector is $\\mathbf{x} = \\widehat{\\mathbf{s}}$, and the right-hand side vector is $\\mathbf{b} = W^T \\mathbf{t}$. As established during validation, the matrix $A$ is invertible for all specified test cases, ensuring that a unique solution $\\widehat{\\mathbf{s}}$ exists and can be computed by solving this linear system.\n\nThe computational procedure for each test case is as follows:\n$1$. Define the physical and discretization parameters: $L = 1000 \\ \\text{m}$, $N = 10$, and $\\Delta x = L/N = 100 \\ \\text{m}$.\n$2$. Define the ground-truth sound speed vector $\\mathbf{c}^{\\star} \\in \\mathbb{R}^{N}$ and compute the corresponding ground-truth slowness vector $\\mathbf{s}^{\\star}$ component-wise as $s^{\\star}_i = 1/c^{\\star}_i$.\n$3$. For each test case, specified by parameters $K$, $\\sigma$, and $\\lambda$:\n    a. Construct the measurement matrix $W \\in \\mathbb{R}^{K \\times N}$ according to the definition $W_{k,i} = \\Delta x$ for $i \\le k$ and $W_{k,i} = 0$ for $i > k$, using $1$-based indexing for problem-domain clarity.\n    b. Construct the first-difference operator $D \\in \\mathbb{R}^{(N-1) \\times N}$ where each row $i$ implements the operation $s_{i+1} - s_i$.\n    c. Synthesize the measurement vector $\\mathbf{t} \\in \\mathbb{R}^{K}$. This involves calculating the ideal, unperturbed travel times $\\mathbf{t}_{ideal} = W \\mathbf{s}^{\\star}$ and adding the deterministic perturbation vector $\\boldsymbol{\\varepsilon}$, where $\\varepsilon_k = \\sigma (-1)^k$ for $k \\in \\{1, \\dots, K\\}$. The final measurement vector is $\\mathbf{t} = \\mathbf{t}_{ideal} + \\boldsymbol{\\varepsilon}$.\n    d. Form the system matrix $A = W^T W + \\lambda D^T D$ and the right-hand side vector $\\mathbf{b} = W^T \\mathbf{t}$.\n    e. Solve the linear system $A \\widehat{\\mathbf{s}} = \\mathbf{b}$ to obtain the estimated slowness vector $\\widehat{\\mathbf{s}}$.\n    f. Convert the estimated slowness vector $\\widehat{\\mathbf{s}}$ back to a sound speed vector $\\widehat{\\mathbf{c}}$ via $\\widehat{c}_i = 1/\\widehat{s}_i$.\n    g. Evaluate the reconstruction quality by computing the root-mean-square error ($\\text{RMSE}$) between the reconstructed sound speeds $\\widehat{\\mathbf{c}}$ and the ground-truth sound speeds $\\mathbf{c}^{\\star}$:\n    $$\n    \\text{RMSE} \\;=\\; \\sqrt{ \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\widehat{c}_i - c_i^{\\star} \\right)^2 }\n    $$\nThis procedure is applied to each of the four test cases provided, and the resulting $\\text{RMSE}$ values are reported.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the 1D acoustic tomography inverse problem for four test cases.\n    \"\"\"\n    # Define physical and discretization parameters\n    L = 1000.0  # m\n    N = 10      # number of cells\n    delta_x = L / N # m\n\n    # Ground-truth sound speed vector (m/s)\n    c_star = np.array([1480., 1480., 1500., 1520., 1550., 1550., 1530., 1500., 1490., 1480.])\n    \n    # Ground-truth slowness vector (s/m)\n    s_star = 1.0 / c_star\n\n    # Define the test suite\n    test_cases = [\n        # Case 1: Full data, no noise, no regularization\n        {'K': 10, 'sigma': 0.0, 'lam': 0.0},\n        # Case 2: Incomplete data, no noise, with regularization\n        {'K': 6, 'sigma': 0.0, 'lam': 1e-2},\n        # Case 3: Full data, small noise, with regularization\n        {'K': 10, 'sigma': 5e-4, 'lam': 1e-3},\n        # Case 4: Full data, larger noise, stronger regularization\n        {'K': 10, 'sigma': 5e-3, 'lam': 1.0},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        K = case['K']\n        sigma = case['sigma']\n        lam = case['lam']\n\n        # 1. Construct the measurement matrix W (K x N)\n        # W_ki = delta_x if i <= k, 0 otherwise (1-based indexing)\n        # In 0-based code: W[k_idx, i_idx] for i_idx <= k_idx\n        W = np.zeros((K, N))\n        for k in range(K):\n            for i in range(k + 1):\n                W[k, i] = delta_x\n\n        # 2. Construct the first-difference operator D ((N-1) x N)\n        # (Ds)_i = s_{i+1} - s_i\n        # This corresponds to a matrix with -1 on the main diagonal and 1 on the first super-diagonal.\n        D = np.eye(N, k=1) - np.eye(N, k=0)\n        D = D[:-1, :] # Keep the first N-1 rows\n\n        # 3. Synthesize the measurement vector t (K x 1)\n        # t = W * s_star + eps\n        t_ideal = W @ s_star\n        \n        # Perturbation vector eps, where eps_k = sigma * (-1)^k for k=1..K\n        k_indices = np.arange(1, K + 1)\n        eps = sigma * ((-1.0)**k_indices)\n        \n        t = t_ideal + eps\n\n        # 4. Form the system matrix A and right-hand side b for the normal equations\n        # (W^T W + lambda * D^T D) * s_hat = W^T * t\n        A = W.T @ W + lam * (D.T @ D)\n        b = W.T @ t\n\n        # 5. Solve the linear system for the estimated slowness s_hat\n        s_hat = np.linalg.solve(A, b)\n\n        # 6. Convert estimated slowness to sound speed\n        c_hat = 1.0 / s_hat\n\n        # 7. Calculate the Root-Mean-Square Error (RMSE)\n        rmse = np.sqrt(np.mean((c_hat - c_star)**2))\n        results.append(rmse)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After seeing the power of $\\ell_2$-based regularization, it's natural to ask about alternatives. This exercise explores the fundamental difference between $\\ell_2$ (Tikhonov) and $\\ell_1$ (LASSO) regularization through a carefully constructed thought experiment . You will discover how each regularizer imposes a different structural assumption on the solution and analyze a case where the non-sparse nature of the true signal makes one a much better choice than the other, giving you crucial insight into selecting the right tool for your problem.",
            "id": "2405389",
            "problem": "Consider the following deterministic one-dimensional linear inverse problem posed on a periodic grid of length $n=128$. The forward operator $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is the circulant matrix that represents circular convolution with a discrete Gaussian kernel of standard deviation $\\sigma$ (measured in grid-index units) that is normalized to have unit sum. Let $\\mathbf{k} \\in \\mathbb{R}^{n}$ denote the first column of $\\mathbf{A}$, with entries\n$$\nk_j \\propto \\exp\\!\\left(-\\frac{d(j)^2}{2\\sigma^2}\\right), \\quad d(j) = \\min\\{j,\\,n-j\\}, \\quad j=0,1,\\ldots,n-1,\n$$\nand then scaled so that $\\sum_{j=0}^{n-1} k_j = 1$. For $\\sigma=0$, interpret the kernel as the Kronecker delta $k_0=1$ and $k_j=0$ for $j\\neq 0$, so that $\\mathbf{A}$ is the identity matrix.\n\nLet the true signal be the constant vector $\\mathbf{x}_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ with entries $(\\mathbf{x}_{\\mathrm{true}})_i = 1$ for all $i$. Let the noiseless data be $\\mathbf{y}=\\mathbf{A}\\mathbf{x}_{\\mathrm{true}}$.\n\nFor a given regularization parameter $\\lambda>0$, define two reconstructions $\\widehat{\\mathbf{x}}_{2}$ and $\\widehat{\\mathbf{x}}_{1}$ as the unique minimizers of the convex objectives\n$$\n\\widehat{\\mathbf{x}}_{2} \\in \\underset{\\mathbf{x}\\in\\mathbb{R}^n}{\\arg\\min}\\ \\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\right\\|_2^2 + \\lambda \\left\\|\\mathbf{x}\\right\\|_2^2,\n\\qquad\n\\widehat{\\mathbf{x}}_{1} \\in \\underset{\\mathbf{x}\\in\\mathbb{R}^n}{\\arg\\min}\\ \\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\right\\|_2^2 + \\lambda \\left\\|\\mathbf{x}\\right\\|_1.\n$$\nFor each reconstruction, define the relative reconstruction error\n$$\ne_p = \\frac{\\left\\|\\widehat{\\mathbf{x}}_{p}-\\mathbf{x}_{\\mathrm{true}}\\right\\|_2}{\\left\\|\\mathbf{x}_{\\mathrm{true}}\\right\\|_2}, \\quad p\\in\\{1,2\\}.\n$$\nDefine the performance gap $g = e_1 - e_2$.\n\nYour task is to compute $g$ for each of the following test cases, where each test case specifies $(\\sigma,\\lambda)$:\n- Test case $1$: $(\\sigma,\\lambda)=(0.0,\\,1.5)$.\n- Test case $2$: $(\\sigma,\\lambda)=(2.0,\\,1.5)$.\n- Test case $3$: $(\\sigma,\\lambda)=(4.0,\\,3.0)$.\n\nRequirements:\n- Use the precise definitions above for $\\mathbf{A}$, $\\mathbf{y}$, $\\widehat{\\mathbf{x}}_{2}$, $\\widehat{\\mathbf{x}}_{1}$, and $e_p$.\n- No measurement noise is present; do not introduce any randomness.\n- Angles do not appear; no angle units are needed.\n- The final outputs must be real numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the three values of $g$ for the test cases, in order, as a comma-separated list enclosed in square brackets, with each value rounded to exactly six digits after the decimal point (for example, $[0.123456,0.000000,1.500000]$).",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of computational engineering and inverse problems, specifically using Tikhonov ($\\ell_2$) and LASSO ($\\ell_1$) regularization. It is well-posed, with all necessary data and definitions provided to find a unique solution for each reconstruction. The language is objective and mathematically precise. We may proceed with the solution.\n\nThe task is to compute the performance gap $g = e_1 - e_2$ for three test cases. The core of the problem is to find the reconstructions $\\widehat{\\mathbf{x}}_2$ and $\\widehat{\\mathbf{x}}_1$ and their corresponding relative errors $e_2$ and $e_1$.\n\nFirst, we analyze the data vector $\\mathbf{y}$. The forward operator $\\mathbf{A}$ is a circulant matrix generated by the kernel vector $\\mathbf{k} \\in \\mathbb{R}^n$. A property of any circulant matrix is that the sum of the elements in each row is constant across all rows. This sum is equal to the sum of the elements of the generating vector, which in this case is the first column $\\mathbf{k}$. The problem states that the kernel is normalized to have a unit sum:\n$$\n\\sum_{j=0}^{n-1} k_j = 1\n$$\nTherefore, the sum of elements in any row of $\\mathbf{A}$ is $1$. The true signal $\\mathbf{x}_{\\mathrm{true}}$ is a constant vector with all entries equal to $1$. Let us denote this vector by $\\mathbf{1} \\in \\mathbb{R}^n$. The noiseless data $\\mathbf{y}$ is given by $\\mathbf{y} = \\mathbf{A}\\mathbf{x}_{\\mathrm{true}}$. The $i$-th component of $\\mathbf{y}$ is:\n$$\n(\\mathbf{y})_i = (\\mathbf{A}\\mathbf{1})_i = \\sum_{j=0}^{n-1} A_{ij} \\cdot 1 = \\sum_{j=0}^{n-1} A_{ij} = 1\n$$\nThis holds for all $i = 0, 1, \\ldots, n-1$. Thus, the data vector is identical to the true signal:\n$$\n\\mathbf{y} = \\mathbf{1} = \\mathbf{x}_{\\mathrm{true}}\n$$\nThis is a critical simplification that holds irrespective of the standard deviation $\\sigma$ of the Gaussian kernel, as long as the kernel sum is normalized to $1$. This also holds for the special case $\\sigma=0$, where $\\mathbf{A}$ is the identity matrix $\\mathbf{I}$, since $\\mathbf{I}\\mathbf{1} = \\mathbf{1}$.\n\nNext, we solve for the Tikhonov reconstruction $\\widehat{\\mathbf{x}}_2$, which minimizes:\n$$\nJ_2(\\mathbf{x}) = \\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\right\\|_2^2 + \\lambda \\left\\|\\mathbf{x}\\right\\|_2^2 = \\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{1}\\right\\|_2^2 + \\lambda \\left\\|\\mathbf{x}\\right\\|_2^2\n$$\nGiven the constant nature of $\\mathbf{x}_{\\mathrm{true}}$ and $\\mathbf{y}$, we test the ansatz that the solution is also a constant vector, $\\mathbf{x} = c\\mathbf{1}$ for some scalar $c \\in \\mathbb{R}$. Substituting this into the objective function:\n$$\nJ_2(c\\mathbf{1}) = \\left\\|\\mathbf{A}(c\\mathbf{1})-\\mathbf{1}\\right\\|_2^2 + \\lambda \\left\\|c\\mathbf{1}\\right\\|_2^2 = \\left\\|c(\\mathbf{A}\\mathbf{1})-\\mathbf{1}\\right\\|_2^2 + \\lambda c^2 \\left\\|\\mathbf{1}\\right\\|_2^2\n$$\nUsing $\\mathbf{A}\\mathbf{1}=\\mathbf{1}$, $\\|\\mathbf{1}\\|_2^2=n$, the objective becomes a function of $c$:\n$$\nJ_2(c) = \\left\\|c\\mathbf{1}-\\mathbf{1}\\right\\|_2^2 + \\lambda n c^2 = n(c-1)^2 + \\lambda n c^2\n$$\nTo find the minimum, we set the derivative with respect to $c$ to zero:\n$$\n\\frac{dJ_2}{dc} = 2n(c-1) + 2n\\lambda c = 0 \\implies c-1+\\lambda c = 0 \\implies c(1+\\lambda)=1 \\implies c = \\frac{1}{1+\\lambda}\n$$\nThe solution is therefore $\\widehat{\\mathbf{x}}_2 = \\frac{1}{1+\\lambda}\\mathbf{1}$. The relative error $e_2$ is:\n$$\ne_2 = \\frac{\\|\\widehat{\\mathbf{x}}_2 - \\mathbf{x}_{\\mathrm{true}}\\|_2}{\\|\\mathbf{x}_{\\mathrm{true}}\\|_2} = \\frac{\\left\\|\\frac{1}{1+\\lambda}\\mathbf{1} - \\mathbf{1}\\right\\|_2}{\\|\\mathbf{1}\\|_2} = \\frac{\\left| \\frac{1}{1+\\lambda}-1 \\right| \\|\\mathbf{1}\\|_2}{\\|\\mathbf{1}\\|_2} = \\left| \\frac{-\\lambda}{1+\\lambda} \\right| = \\frac{\\lambda}{1+\\lambda}\n$$\n\nNow, we solve for the LASSO reconstruction $\\widehat{\\mathbf{x}}_1$, which minimizes:\n$$\nJ_1(\\mathbf{x}) = \\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\right\\|_2^2 + \\lambda \\left\\|\\mathbf{x}\\right\\|_1 = \\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{1}\\right\\|_2^2 + \\lambda \\left\\|\\mathbf{x}\\right\\|_1\n$$\nApplying the same ansatz $\\mathbf{x}=c\\mathbf{1}$, the objective becomes:\n$$\nJ_1(c\\mathbf{1}) = n(c-1)^2 + \\lambda \\|c\\mathbf{1}\\|_1 = n(c-1)^2 + \\lambda n |c|\n$$\nWe minimize $f(c) = (c-1)^2 + \\lambda|c|$.\nIf $c \\ge 0$, $f(c) = (c-1)^2 + \\lambda c$. The derivative is $f'(c) = 2(c-1)+\\lambda$, which is zero at $c = 1 - \\lambda/2$. If $\\lambda \\le 2$, this minimum is non-negative, so the minimizer for $c \\ge 0$ is $c = 1 - \\lambda/2$. If $\\lambda > 2$, the minimum occurs at a negative value, so on $c \\ge 0$, $f(c)$ is strictly increasing, and the minimum is at $c=0$.\nIf $c < 0$, $f(c) = (c-1)^2 - \\lambda c$. The derivative is $f'(c) = 2(c-1)-\\lambda$, which is zero at $c = 1 + \\lambda/2$. This contradicts the assumption $c<0$ for $\\lambda > 0$.\nThus, the global minimum is $c^* = \\max(0, 1 - \\lambda/2)$. This is the soft-thresholding operator $S_{\\lambda/2}(1)$. The solution is $\\widehat{\\mathbf{x}}_1 = \\max(0, 1 - \\lambda/2)\\mathbf{1}$. One can verify this satisfies the subgradient optimality condition for the full problem.\n\nThe relative error $e_1$ is calculated based on $c_1 = \\max(0, 1 - \\lambda/2)$:\n$$\ne_1 = \\frac{\\|\\widehat{\\mathbf{x}}_1 - \\mathbf{x}_{\\mathrm{true}}\\|_2}{\\|\\mathbf{x}_{\\mathrm{true}}\\|_2} = \\frac{\\|c_1\\mathbf{1} - \\mathbf{1}\\|_2}{\\|\\mathbf{1}\\|_2} = |c_1 - 1| = |\\max(0, 1-\\lambda/2) - 1|\n$$\nThis expression simplifies based on the value of $\\lambda$:\nIf $\\lambda \\le 2$: $c_1 = 1 - \\lambda/2$. Then $e_1 = |(1-\\lambda/2) - 1| = |-\\lambda/2| = \\lambda/2$.\nIf $\\lambda > 2$: $c_1 = 0$. Then $e_1 = |0 - 1| = 1$.\n\nFinally, we compute the performance gap $g = e_1 - e_2$. There are two cases depending on the value of $\\lambda$:\n\nIf $\\lambda \\le 2$:\n$$\ng = e_1 - e_2 = \\frac{\\lambda}{2} - \\frac{\\lambda}{1+\\lambda} = \\lambda\\left(\\frac{1}{2} - \\frac{1}{1+\\lambda}\\right) = \\lambda\\frac{(1+\\lambda)-2}{2(1+\\lambda)} = \\frac{\\lambda(\\lambda-1)}{2(1+\\lambda)}\n$$\nIf $\\lambda > 2$:\n$$\ng = e_1 - e_2 = 1 - \\frac{\\lambda}{1+\\lambda} = \\frac{(1+\\lambda)-\\lambda}{1+\\lambda} = \\frac{1}{1+\\lambda}\n$$\nThe performance gap $g$ is independent of both $\\sigma$ and $n$. We now apply these formulas to the given test cases.\n\nTest Case $1$: $(\\sigma, \\lambda) = (0.0, 1.5)$.\nHere $\\lambda = 1.5$, which satisfies $\\lambda \\le 2$.\n$$\ng = \\frac{1.5(1.5-1)}{2(1+1.5)} = \\frac{1.5(0.5)}{2(2.5)} = \\frac{0.75}{5.0} = 0.15\n$$\n\nTest Case $2$: $(\\sigma, \\lambda) = (2.0, 1.5)$.\nHere $\\lambda = 1.5$, which satisfies $\\lambda \\le 2$. The value of $\\sigma$ is irrelevant.\n$$\ng = \\frac{1.5(1.5-1)}{2(1+1.5)} = 0.15\n$$\n\nTest Case $3$: $(\\sigma, \\lambda) = (4.0, 3.0)$.\nHere $\\lambda = 3.0$, which satisfies $\\lambda > 2$.\n$$\ng = \\frac{1}{1+3.0} = \\frac{1}{4.0} = 0.25\n$$\nThe computed values for $g$ are $0.15$, $0.15$, and $0.25$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the performance gap g = e1 - e2 for the given test cases.\n    The problem setup allows for an analytical simplification where the result\n    depends only on the regularization parameter lambda, not on the kernel\n    standard deviation sigma or the grid size n.\n    \"\"\"\n    \n    # Test cases are given as (sigma, lambda) tuples.\n    test_cases = [\n        (0.0, 1.5),\n        (2.0, 1.5),\n        (4.0, 3.0),\n    ]\n\n    results = []\n    for sigma, lam in test_cases:\n        # The analytical formula for the performance gap g is piecewise,\n        # with the condition based on the value of lambda relative to 2.\n        \n        if lam <= 2.0:\n            # For lambda <= 2, the formula is: g = lambda * (lambda - 1) / (2 * (1 + lambda))\n            g = lam * (lam - 1.0) / (2.0 * (1.0 + lam))\n        else: # lam > 2.0\n            # For lambda > 2, the formula is: g = 1 / (1 + lambda)\n            g = 1.0 / (1.0 + lam)\n        \n        results.append(g)\n\n    # The problem requires the output to be rounded to exactly six digits after the decimal point.\n    formatted_results = [\"{:.6f}\".format(r) for r in results]\n    \n    # The final output must be in the format [value1,value2,value3].\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Solving an inverse problem is one thing; understanding the stability of that solution is another. This advanced practice pushes you to analyze the robustness of Tikhonov regularization by finding the specific noise pattern that an adversary would choose to maximally corrupt your solution . By linking the maximum error amplification to the singular values of the solution operator, this exercise provides a deep, quantitative understanding of how the regularization parameter $\\alpha$ directly controls the trade-off between fitting the data and ensuring a stable, reliable result.",
            "id": "2405424",
            "problem": "You are given a linear forward model represented by a real matrix $A \\in \\mathbb{R}^{m \\times n}$, a Tikhonov regularization parameter $\\alpha \\in \\mathbb{R}$ with $\\alpha \\gt 0$, and a data vector $y \\in \\mathbb{R}^{m}$. The Tikhonov-regularized solution $x_{\\alpha}(y) \\in \\mathbb{R}^{n}$ is defined as the unique minimizer of the strictly convex objective\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|A x - y\\|_{2}^{2} + \\alpha^{2} \\|x\\|_{2}^{2}.\n$$\nConsider a perturbation (noise) vector $z \\in \\mathbb{R}^{m}$ added to the data, yielding the perturbed solution $x_{\\alpha}(y+z)$. Define the perturbation in the solution as\n$$\n\\Delta x_{\\alpha}(z) \\coloneqq x_{\\alpha}(y+z) - x_{\\alpha}(y).\n$$\nAmong all perturbations $z$ with Euclidean norm $\\|z\\|_{2} = 1$, find the vector $z^{\\star}$ that maximizes the Euclidean norm of the solution perturbation $\\|\\Delta x_{\\alpha}(z)\\|_{2}$. In case of multiple maximizers, impose the following orientation convention to select a unique $z^{\\star}$: let $i$ be the smallest index such that $|z^{\\star}_{i}| \\gt 10^{-12}$; require $z^{\\star}_{i} \\gt 0$. If no such index exists, leave $z^{\\star}$ unchanged.\n\nFor each test case below, compute:\n1) The maximum amplification value $a^{\\star} \\coloneqq \\max_{\\|z\\|_{2} = 1} \\|\\Delta x_{\\alpha}(z)\\|_{2}$.\n2) One maximizing noise vector $z^{\\star}$ satisfying the orientation convention.\n\nYou must implement a program that processes the following test suite. For each case, use the given $A$ and $\\alpha$. The data vector $y$ is arbitrary and does not affect $\\Delta x_{\\alpha}(z)$ in this task.\n\nTest Suite:\n- Case 1: $A \\in \\mathbb{R}^{4 \\times 3}$ with rows $[\\,1,0,0\\,]$, $[\\,0,2,0\\,]$, $[\\,0,0,3\\,]$, $[\\,1,1,1\\,]$; $\\alpha = 0.7$.\n- Case 2: $A \\in \\mathbb{R}^{2 \\times 2}$ with rows $[\\,1,2\\,]$, $[\\,3,4\\,]$; $\\alpha = 10^{-3}$.\n- Case 3: $A \\in \\mathbb{R}^{3 \\times 2}$ with rows $[\\,1,2\\,]$, $[\\,2,4\\,]$, $[\\,0,0\\,]$; $\\alpha = 0.05$.\n- Case 4: $A \\in \\mathbb{R}^{3 \\times 3}$ with rows $[\\,3,0,0\\,]$, $[\\,0,10^{-2},0\\,]$, $[\\,0,0,2\\,]$; $\\alpha = 10.0$.\n\nYour program must output a single line containing a list of lists. For each test case, output a list whose first entry is the float $a^{\\star}$, followed by the components of $z^{\\star}$ in order. All floating-point numbers must be rounded to $6$ decimal places using standard rounding to nearest, with ties to even, and values of $-0.0$ printed as $0.0$.\n\nFinal Output Format:\nProduce exactly one line to standard output containing\n$$\n\\big[\\,[a^{\\star}_{1}, z^{\\star}_{1,1}, \\dots, z^{\\star}_{1,m_{1}}],\\ [a^{\\star}_{2}, z^{\\star}_{2,1}, \\dots, z^{\\star}_{2,m_{2}}],\\ [a^{\\star}_{3}, z^{\\star}_{3,1}, \\dots, z^{\\star}_{3,m_{3}}],\\ [a^{\\star}_{4}, z^{\\star}_{4,1}, \\dots, z^{\\star}_{4,m_{4}}]\\,\\big],\n$$\nwhere $m_{i}$ is the number of rows of $A$ in case $i$. For example, the output must have the syntactic form\n$$\n[[a_{1},z_{1,1},\\dots],[a_{2},z_{2,1},\\dots],[a_{3},z_{3,1},\\dots],[a_{4},z_{4,1},\\dots]].\n$$\nNo additional text should be printed.",
            "solution": "The problem as stated is mathematically and scientifically sound. It is a well-posed problem in the domain of computational engineering, specifically concerning the stability analysis of Tikhonov-regularized solutions to linear inverse problems. We shall proceed with its formal derivation and solution.\n\nThe problem asks for the noise vector $z \\in \\mathbb{R}^{m}$ with unit norm, $\\|z\\|_{2} = 1$, that maximizes the norm of the resulting perturbation in the Tikhonov-regularized solution, $\\|\\Delta x_{\\alpha}(z)\\|_{2}$.\n\nFirst, we must derive an explicit expression for the solution perturbation $\\Delta x_{\\alpha}(z)$. The Tikhonov-regularized solution $x_{\\alpha}(y)$ is the unique minimizer of the functional\n$$\nJ(x) = \\|A x - y\\|_{2}^{2} + \\alpha^{2} \\|x\\|_{2}^{2}\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and $\\alpha > 0$. The functional $J(x)$ is strictly convex, and its minimizer is found by setting its gradient with respect to $x$ to zero. The gradient is\n$$\n\\nabla_{x} J(x) = \\nabla_{x} \\left( (Ax - y)^T(Ax-y) + \\alpha^2 x^T x \\right) = 2 A^T(Ax - y) + 2 \\alpha^2 x.\n$$\nSetting the gradient to zero yields the normal equations for the Tikhonov problem:\n$$\n(A^T A + \\alpha^2 I) x = A^T y,\n$$\nwhere $I$ is the $n \\times n$ identity matrix. Since $A^T A$ is a positive semi-definite matrix and $\\alpha^2 > 0$, the matrix $(A^T A + \\alpha^2 I)$ is positive definite and thus invertible. The unique solution is\n$$\nx_{\\alpha}(y) = (A^T A + \\alpha^2 I)^{-1} A^T y.\n$$\nNow, consider a perturbation $z \\in \\mathbb{R}^{m}$ added to the data vector $y$. The perturbed data is $y+z$, and the corresponding solution is\n$$\nx_{\\alpha}(y+z) = (A^T A + \\alpha^2 I)^{-1} A^T (y+z).\n$$\nThe perturbation in the solution, $\\Delta x_{\\alpha}(z)$, is defined as the difference between the perturbed and original solutions:\n$$\n\\Delta x_{\\alpha}(z) \\coloneqq x_{\\alpha}(y+z) - x_{\\alpha}(y).\n$$\nSubstituting the expressions for the solutions, we find that the operator mapping data to solution is linear. Thus,\n$$\n\\Delta x_{\\alpha}(z) = (A^T A + \\alpha^2 I)^{-1} A^T (y+z) - (A^T A + \\alpha^2 I)^{-1} A^T y = (A^T A + \\alpha^2 I)^{-1} A^T z.\n$$\nThis expression demonstrates that the solution perturbation $\\Delta x_{\\alpha}(z)$ is a linear function of the data perturbation $z$ and is independent of the original data vector $y$. Let us define the operator $K \\in \\mathbb{R}^{n \\times m}$ as\n$$\nK \\coloneqq (A^T A + \\alpha^2 I)^{-1} A^T.\n$$\nThen, the solution perturbation is simply $\\Delta x_{\\alpha}(z) = Kz$.\n\nThe core of the problem is to solve the following optimization problem:\n$$\n\\max_{\\|z\\|_{2} = 1} \\|\\Delta x_{\\alpha}(z)\\|_{2} = \\max_{\\|z\\|_{2} = 1} \\|Kz\\|_{2}.\n$$\nBy definition, the maximum value of $\\|Kz\\|_{2}$ over all unit vectors $z$ is the spectral norm of the matrix $K$, denoted $\\|K\\|_{2}$. Thus, the maximum amplification is\n$$\na^{\\star} = \\|K\\|_{2}.\n$$\nThe spectral norm of a matrix is equal to its largest singular value, $\\sigma_{\\max}(K)$. The vector $z^{\\star}$ that achieves this maximum is the right singular vector of $K$ corresponding to this largest singular value.\n\nLet the Singular Value Decomposition (SVD) of $K$ be $K = U_K \\Sigma_K V_K^T$, where $U_K$ and $V_K$ are orthogonal matrices and $\\Sigma_K$ is the matrix of singular values ($\\sigma_1 \\ge \\sigma_2 \\ge \\dots$). Then,\n$$\na^{\\star} = \\sigma_{\\max}(K) = \\sigma_1(K).\n$$\nThe maximizing vector $z^{\\star}$ is the first column of the matrix $V_K$, which corresponds to the first row of $V_K^T$.\n\nRight singular vectors are unique only up to a sign. The problem imposes an orientation convention to select a unique $z^{\\star}$: for the computed $z^{\\star}$, find the smallest index $i$ such that $|z^{\\star}_{i}| > 10^{-12}$. If this component $z^{\\star}_{i}$ is negative, the entire vector $z^{\\star}$ must be negated. This ensures a unique result without altering the norm of the solution perturbation, since $\\|K(-z^{\\star})\\|_{2} = \\|-Kz^{\\star}\\|_{2} = \\|Kz^{\\star}\\|_{2}$.\n\nThe computational procedure is as follows:\n1. For a given matrix $A$ and parameter $\\alpha$, construct the matrix $K = (A^T A + \\alpha^2 I)^{-1} A^T$.\n2. Compute the SVD of $K$ to obtain its singular values and right singular vectors.\n3. The maximum amplification $a^{\\star}$ is the largest singular value.\n4. The initial maximizing noise vector $z^{\\star}$ is the right singular vector corresponding to $a^{\\star}$.\n5. Adjust the sign of $z^{\\star}$ according to the specified orientation convention.\n6. Round the results to $6$ decimal places for output, following the specified rounding and formatting rules.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef solve_case(A, alpha):\n    \"\"\"\n    Solves the amplification problem for a given matrix A and parameter alpha.\n\n    Args:\n        A (np.ndarray): The forward model matrix.\n        alpha (float): The Tikhonov regularization parameter.\n\n    Returns:\n        tuple: A tuple containing:\n            - a_star (float): The maximum amplification factor.\n            - z_star (np.ndarray): The maximizing noise vector.\n    \"\"\"\n    m, n = A.shape\n\n    # Step 1: Construct the matrix K = (A^T A + alpha^2 I)^-1 A^T\n    I_n = np.eye(n)\n    AtA = A.T @ A\n    # The matrix G = A^T A + alpha^2 I is always invertible for alpha > 0\n    G = AtA + (alpha**2) * I_n\n    G_inv = np.linalg.inv(G)\n    K = G_inv @ A.T\n\n    # Step 2: Compute the SVD of K.\n    # scipy.linalg.svd returns U, s, Vh, where Vh is V.T.\n    _U, s, Vh = svd(K)\n\n    # Step 3: a_star is the largest singular value.\n    a_star = s[0]\n\n    # Step 4: z_star is the corresponding right singular vector (first row of Vh).\n    z_star = Vh[0, :]\n\n    # Step 5: Apply the specified orientation convention.\n    # The convention ensures a unique maximizer z_star, as singular vectors are\n    # defined up to a sign.\n    # We find the first component with magnitude above a small tolerance\n    # and ensure it is positive.\n    tol = 1e-12\n    # np.where returns a tuple of arrays; we need the first element of the first array.\n    significant_indices = np.where(np.abs(z_star) > tol)[0]\n    if significant_indices.size > 0:\n        first_significant_idx = significant_indices[0]\n        if z_star[first_significant_idx] < 0:\n            z_star = -z_star\n            \n    return a_star, z_star\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        (np.array([[1.0, 0.0, 0.0], \n                   [0.0, 2.0, 0.0], \n                   [0.0, 0.0, 3.0], \n                   [1.0, 1.0, 1.0]]), 0.7),\n        # Case 2\n        (np.array([[1.0, 2.0], \n                   [3.0, 4.0]]), 1e-3),\n        # Case 3\n        (np.array([[1.0, 2.0], \n                   [2.0, 4.0], \n                   [0.0, 0.0]]), 0.05),\n        # Case 4\n        (np.array([[3.0, 0.0, 0.0], \n                   [0.0, 1e-2, 0.0], \n                   [0.0, 0.0, 2.0]]), 10.0)\n    ]\n    \n    all_results_str = []\n    \n    for A, alpha in test_cases:\n        a_star, z_star = solve_case(A, alpha)\n        \n        # Combine results into a single list for formatting\n        case_output = [a_star] + list(z_star)\n        \n        formatted_numbers = []\n        for num in case_output:\n            # Round to 6 decimal places using standard rounding (ties to even)\n            rounded_num = round(num, 6)\n            # Handle the special case where -0.0 should be printed as 0.0\n            if rounded_num == -0.0:\n                rounded_num = 0.0\n            # Format to ensure 6 decimal places are always shown\n            formatted_numbers.append(f\"{rounded_num:.6f}\")\n        \n        all_results_str.append(f\"[{','.join(formatted_numbers)}]\")\n        \n    # Print the final output in the required single-line format\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        }
    ]
}