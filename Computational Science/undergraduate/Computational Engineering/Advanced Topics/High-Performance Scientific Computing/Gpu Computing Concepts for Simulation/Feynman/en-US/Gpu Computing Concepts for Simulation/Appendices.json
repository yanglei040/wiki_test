{
    "hands_on_practices": [
        {
            "introduction": "Achieving high performance on GPUs is often a battle against memory latency and bandwidth limitations. A fundamental strategy to win this battle is *tiling*, where we partition a large problem into smaller blocks that fit into the GPU's fast on-chip shared memory. This allows for significant data reuse, reducing the number of slow accesses to global memory.\n\nThis first practice guides you through a quantitative analysis of one of the most important algorithms in scientific computing: matrix multiplication (GEMM). By modeling the memory traffic and computational workload, you will directly see how tile dimensions impact performance metrics like arithmetic intensity, providing a solid foundation for designing efficient GPU kernels .",
            "id": "2398448",
            "problem": "You are asked to design and implement a program that models the memory traffic and arithmetic intensity of a classic tiled General Matrix Multiply (GEMM) kernel as used in Graphics Processing Unit (GPU) computing for scientific simulation. The setting is the matrix product $C = A \\times B$ with non-square dimensions, where $A \\in \\mathbb{R}^{M \\times K}$, $B \\in \\mathbb{R}^{K \\times N}$, and $C \\in \\mathbb{R}^{M \\times N}$. The assumed numerical type is single-precision floating point ($32$-bit), so each matrix element occupies $4$ bytes in memory. The mathematical base for the reasoning is the definition of matrix multiplication and the counting of floating-point operations from that definition, together with simple tiling concepts used in high-performance computing.\n\nStart from the following fundamental bases:\n- The definition of matrix multiplication: for each $i \\in \\{1,\\dots,M\\}$ and $j \\in \\{1,\\dots,N\\}$, $C_{ij} = \\sum_{k=1}^{K} A_{ik} B_{kj}$.\n- Counting of floating-point operations for matrix multiplication: computing $C = A \\times B$ performs $M \\times N \\times K$ multiply-add pairs (each consisting of one multiplication and one addition), which is $2 \\times M \\times N \\times K$ floating-point operations.\n- In a tiled algorithm, the output matrix $C$ is partitioned into tiles of size $T_M \\times T_N$. For each output tile, the kernel iterates over the $K$ dimension in chunks of size $T_K$, loading one tile of $A$ of size $T_M \\times T_K$ and one tile of $B$ of size $T_K \\times T_N$ from global memory into on-chip shared memory, computing partial sums, and finally writing the $T_M \\times T_N$ results for that tile once to global memory.\n\nAssumptions to model memory movement and parallel decomposition:\n- There is no cache or shared-memory reuse across different thread blocks; only reuse within a block across the $K$-sweep is exploited. Consequently, data fetched for one output tile is not reused by other tiles.\n- The per-block shared memory footprint is dominated by simultaneously resident tiles of $A$ and $B$, i.e., $T_M \\times T_K$ elements for $A$ and $T_K \\times T_N$ elements for $B$.\n- Each matrix element occupies $4$ bytes.\n- Tiling along $M$ and $N$ uses ceiling division: the number of blocks (tiles) along $M$ is $\\lceil M / T_M \\rceil$ and along $N$ is $\\lceil N / T_N \\rceil$. Tiles at the boundaries may be partially filled when dimensions are not divisible by tile sizes.\n- Express all sizes in bytes. No physical units appear. No angles appear. Percentages are not required.\n\nTasks:\n- Derive, from the above bases, expressions for:\n  - The grid dimensions in tiles, namely the number of blocks along $M$ and along $N$.\n  - The total number of blocks.\n  - The per-block shared memory usage in bytes, given that both the $A$-tile and $B$-tile reside simultaneously.\n  - A boolean indicating whether the per-block shared memory usage fits within a given shared-memory budget $S_{\\max}$ (in bytes).\n  - The total global-memory bytes transferred by the kernel under the no-inter-block-reuse assumption, counting all reads and writes for the entire multiplication over all blocks and all $K$-tiles.\n  - The arithmetic intensity, defined as total floating-point operations divided by total global-memory bytes transferred, for the entire multiplication.\n- Implement a program that, for each test case below, computes and outputs:\n  - $\\lceil M / T_M \\rceil$,\n  - $\\lceil N / T_N \\rceil$,\n  - $\\lceil M / T_M \\rceil \\times \\lceil N / T_N \\rceil$,\n  - per-block shared memory in bytes,\n  - the fit boolean relative to $S_{\\max}$,\n  - total global-memory bytes transferred,\n  - arithmetic intensity as a floating-point number rounded to $10^{-6}$.\n- The program must not read any input. It must compute the results for the test suite embedded in the code and print a single line containing all results.\n\nTest suite:\n- Case $1$: $M = 1000$, $K = 750$, $N = 1100$, $T_M = 64$, $T_N = 64$, $T_K = 16$, $S_{\\max} = 49152$ bytes.\n- Case $2$: $M = 1000$, $K = 750$, $N = 1100$, $T_M = 128$, $T_N = 32$, $T_K = 32$, $S_{\\max} = 16384$ bytes.\n- Case $3$: $M = 1000$, $K = 750$, $N = 1100$, $T_M = 32$, $T_N = 128$, $T_K = 32$, $S_{\\max} = 49152$ bytes.\n- Case $4$: $M = 2048$, $K = 256$, $N = 96$, $T_M = 128$, $T_N = 16$, $T_K = 64$, $S_{\\max} = 49152$ bytes.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a list of per-test-case records, where each record is itself a list in the order $[\\lceil M / T_M \\rceil,\\ \\lceil N / T_N \\rceil,\\ \\lceil M / T_M \\rceil \\times \\lceil N / T_N \\rceil,\\ \\text{shared\\_bytes},\\ \\text{fits},\\ \\text{total\\_bytes},\\ \\text{arithmetic\\_intensity}]$.\n- The printed line must be exactly a single Python-style list, for example: $[[\\dots],[\\dots],\\dots]$.\n- The arithmetic intensity must be rounded to $10^{-6}$.",
            "solution": "The problem presented is a well-defined exercise in the performance modeling of parallel algorithms, specifically the tiled General Matrix Multiply (GEMM) on a Graphics Processing Unit (GPU). It is scientifically grounded in the principles of computational linear algebra and computer architecture. All necessary parameters are provided, and the assumptions are clearly stated, rendering the problem valid and solvable. We proceed with the derivation of the required performance metrics.\n\nThe problem context is the matrix multiplication $C = A \\times B$, where $A \\in \\mathbb{R}^{M \\times K}$, $B \\in \\mathbb{R}^{K \\times N}$, and $C \\in \\mathbb{R}^{M \\times N}$. Each matrix element is a single-precision float, occupying $S_{elem} = 4$ bytes. The algorithm partitions the output matrix $C$ into tiles, or blocks, of size $T_M \\times T_N$.\n\n1.  **Grid Dimensions and Total Blocks**\n    The computational grid is a two-dimensional arrangement of thread blocks, where each block is responsible for computing one $T_M \\times T_N$ tile of the output matrix $C$.\n    The number of blocks required along the $M$ dimension, which we denote as $GridDim_{M}$, is determined by how many tiles of height $T_M$ are needed to cover the $M$ rows of the matrix. This requires ceiling division.\n    $$GridDim_{M} = \\lceil M / T_M \\rceil$$\n    Similarly, the number of blocks along the $N$ dimension, $GridDim_{N}$, is:\n    $$GridDim_{N} = \\lceil N / T_N \\rceil$$\n    The total number of blocks in the grid, $N_{blocks}$, is the product of these two dimensions:\n    $$N_{blocks} = GridDim_{M} \\times GridDim_{N} = \\lceil M / T_M \\rceil \\times \\lceil N / T_N \\rceil$$\n\n2.  **Per-Block Shared Memory Usage**\n    For each output tile, a thread block iterates over the $K$ dimension in steps of size $T_K$. In each step, it loads a micro-tile of $A$ of size $T_M \\times T_K$ and a micro-tile of $B$ of size $T_K \\times T_N$ into its on-chip shared memory. The problem states that the shared memory footprint is dominated by the simultaneous residence of these two tiles.\n    The number of elements for the tile from $A$ is $T_M \\times T_K$.\n    The number of elements for the tile from $B$ is $T_K \\times T_N$.\n    The total required shared memory in bytes, $S_{shared}$, is the sum of the sizes of these two tiles:\n    $$S_{shared} = ( (T_M \\times T_K) + (T_K \\times T_N) ) \\times S_{elem}$$\n    $$S_{shared} = T_K \\times (T_M + T_N) \\times 4$$\n\n3.  **Shared Memory Fit Boolean**\n    This is a simple logical validation. The calculated required shared memory, $S_{shared}$, must not exceed the available shared memory budget per block, $S_{\\max}$. The boolean value, $B_{fit}$, is determined by the following condition:\n    $$B_{fit} = (S_{shared} \\le S_{\\max})$$\n\n4.  **Total Global Memory Bytes Transferred**\n    We analyze the total data movement between the GPU's global memory and the processing units, under the assumption of no data reuse between different thread blocks.\n    -   **Reads from Matrix A:** The entire matrix $A$ (of size $M \\times K$) must be read. Consider the tiling of $C$. For every column of tiles in $C$, the entire matrix $A$ must be loaded. There are $GridDim_{N} = \\lceil N / T_N \\rceil$ such tile columns. Therefore, each element of $A$ is read $\\lceil N / T_N \\rceil$ times.\n        Total bytes read from A: $Bytes_{A} = M \\times K \\times \\lceil N / T_N \\rceil \\times S_{elem}$.\n    -   **Reads from Matrix B:** Similarly, for every row of tiles in $C$, the entire matrix $B$ (of size $K \\times N$) must be read. There are $GridDim_{M} = \\lceil M / T_M \\rceil$ such tile rows. Therefore, each element of $B$ is read $\\lceil M / T_M \\rceil$ times.\n        Total bytes read from B: $Bytes_{B} = K \\times N \\times \\lceil M / T_M \\rceil \\times S_{elem}$.\n    -   **Writes to Matrix C:** The output matrix $C$ (of size $M \\times N$) is computed and written to global memory exactly once.\n        Total bytes written to C: $Bytes_{C} = M \\times N \\times S_{elem}$.\n\n    The total global memory transfer, $B_{total}$, is the sum of all reads and writes:\n    $$B_{total} = Bytes_{A} + Bytes_{B} + Bytes_{C}$$\n    $$B_{total} = (M \\times K \\times \\lceil N / T_N \\rceil + K \\times N \\times \\lceil M / T_M \\rceil + M \\times N) \\times S_{elem}$$\n\n5.  **Arithmetic Intensity**\n    Arithmetic intensity, $I$, is the ratio of total floating-point operations (FLOPs) to total bytes transferred from global memory.\n    The total number of FLOPs, $F$, for a standard matrix multiplication is $2 \\times M \\times N \\times K$, accounting for one multiplication and one addition for each inner-loop product.\n    $$F = 2 \\times M \\times N \\times K$$\n    The arithmetic intensity is therefore:\n    $$I = \\frac{F}{B_{total}} = \\frac{2 \\times M \\times N \\times K}{(M \\times K \\times \\lceil N / T_N \\rceil + K \\times N \\times \\lceil M / T_M \\rceil + M \\times N) \\times 4}$$\n    Simplifying by a factor of $2$:\n    $$I = \\frac{M \\times N \\times K}{2 \\times (M \\times K \\times \\lceil N / T_N \\rceil + K \\times N \\times \\lceil M / T_M \\rceil + M \\times N)}$$\n    This expression quantifies the number of operations performed per byte of data moved from global memory, a critical measure of algorithmic efficiency on memory-bandwidth-bound architectures like GPUs. A higher value is desirable.\n\nThese derived formulae are sufficient to solve the problem for the given test cases. We will now proceed with the implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates performance metrics for a tiled GEMM kernel based on a simplified GPU memory model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (M, K, N, T_M, T_N, T_K, S_max)\n    test_cases = [\n        (1000, 750, 1100, 64, 64, 16, 49152),\n        (1000, 750, 1100, 128, 32, 32, 16384),\n        (1000, 750, 1100, 32, 128, 32, 49152),\n        (2048, 256, 96, 128, 16, 64, 49152),\n    ]\n\n    results = []\n    \n    # Size of a single-precision float in bytes\n    S_elem = 4\n\n    # Helper function for integer ceiling division\n    def ceil_div(a, b):\n        return (a + b - 1) // b\n\n    for case in test_cases:\n        M, K, N, T_M, T_N, T_K, S_max = case\n\n        # 1. Grid dimensions\n        grid_dim_m = ceil_div(M, T_M)\n        grid_dim_n = ceil_div(N, T_N)\n\n        # 2. Total number of blocks\n        total_blocks = grid_dim_m * grid_dim_n\n\n        # 3. Per-block shared memory usage in bytes\n        shared_bytes = (T_M * T_K + T_K * T_N) * S_elem\n\n        # 4. Shared memory fit boolean\n        fits = shared_bytes = S_max\n\n        # 5. Total global-memory bytes transferred\n        # Bytes read from A: each element of A is read grid_dim_n times\n        bytes_read_A = M * K * grid_dim_n * S_elem\n        # Bytes read from B: each element of B is read grid_dim_m times\n        bytes_read_B = K * N * grid_dim_m * S_elem\n        # Bytes written to C: C is written once\n        bytes_written_C = M * N * S_elem\n        total_bytes = bytes_read_A + bytes_read_B + bytes_written_C\n\n        # 6. Arithmetic intensity\n        # Total floating-point operations\n        flops = 2 * M * N * K\n        # Intensity = FLOPs / Byte\n        # Avoid division by zero, though total_bytes should always be positive for valid inputs\n        if total_bytes > 0:\n            arithmetic_intensity = flops / total_bytes\n        else:\n            arithmetic_intensity = 0.0\n\n        # Round intensity to 10^-6\n        rounded_intensity = round(arithmetic_intensity, 6)\n\n        # Assemble the results for the current case\n        case_result = [\n            grid_dim_m,\n            grid_dim_n,\n            total_blocks,\n            shared_bytes,\n            fits,\n            total_bytes,\n            rounded_intensity\n        ]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    print(results)\n\nsolve()\n```"
        },
        {
            "introduction": "Having learned the importance of using shared memory, we now dive deeper into its micro-architecture. Shared memory is not a monolithic block; it is organized into a number of parallel memory banks. This design allows a warp of threads to access data simultaneously, but only if the threads access data in different banks. When multiple threads in a warp access different addresses that fall into the same bank, a *bank conflict* occurs, serializing the requests and negating the benefits of parallel access.\n\nThis practice presents a thought-provoking scenario involving a diagonal data access pattern. Your task is to predict the degree of bank conflicts by mapping thread indices to memory addresses and bank indices, revealing how seemingly innocent code can hide performance bottlenecks .",
            "id": "2398488",
            "problem": "You are asked to model and quantify bank conflicts in Graphics Processing Unit (GPU) shared memory for a warp under a non-obvious access pattern arising from a diagonal stencil on a two-dimensional grid. Consider a warp of $W$ threads accessing a tile stored in shared memory with $B$ banks. Each element of the tile occupies exactly one bank-width unit, and the base address of the tile is aligned so that the element at linear index $0$ maps to bank $0$. The memory layout is row-major with a leading dimension (row stride) of $L$ elements. For thread lane index $\\ell \\in \\{0,1,\\dots,W-1\\}$, the accessed tile coordinates are given by\n- row index $r(\\ell) = (r_0 + \\ell) \\bmod T_r$,\n- column index $c(\\ell) = (c_0 + \\ell \\cdot d) \\bmod T_c$,\n\nwhere $T_r$ and $T_c$ are the tile dimensions in rows and columns, $r_0$ and $c_0$ are the base row and column offsets within the tile, and $d$ is the diagonal step. The linear element index addressed by thread $\\ell$ is\n$$\n\\mathrm{idx}(\\ell) = r(\\ell)\\cdot L + c(\\ell),\n$$\nand the bank index is\n$$\n\\mathrm{bank}(\\ell) = \\mathrm{idx}(\\ell) \\bmod B.\n$$\n\nAssume the following for conflict modeling:\n- In one memory instruction, each bank can serve exactly one distinct element per cycle. If $m$ threads in the warp simultaneously access $m$ distinct element indices that map to the same bank, these are serialized and take $m$ cycles for that bank.\n- If multiple threads simultaneously access the same element index within the same bank, this is considered a broadcast and takes $1$ cycle for that bank.\n- The conflict degree for the warp for that memory instruction is defined as\n$$\nC = \\max_{b \\in \\{0,1,\\dots,B-1\\}} \\left(\\text{number of distinct }\\mathrm{idx}(\\ell)\\text{ among threads with }\\mathrm{bank}(\\ell)=b\\right).\n$$\n\nYour task is to write a complete program that, for each provided test case, computes the integer conflict degree $C$ as defined above.\n\nUse the following test suite, where each test case is the tuple $(B, W, L, T_r, T_c, d, r_0, c_0)$:\n- Case 1 (diagonal with mild conflicts due to padding): $(32, 32, 33, 32, 32, 1, 0, 0)$.\n- Case 2 (worst-case serialization in a single bank without broadcast): $(32, 32, 32, 32, 32, 0, 0, 0)$.\n- Case 3 (diagonal forming a permutation of banks, no conflicts): $(32, 32, 33, 32, 32, 0, 0, 0)$.\n- Case 4 (degenerate diagonal leading to broadcast in one bank): $(32, 32, 64, 1, 64, 0, 0, 0)$.\n- Case 5 (half-warp diagonal with stride causing even-bank mapping, but no conflicts because each bank is used once): $(32, 16, 33, 16, 16, 1, 0, 0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases, for example, $[x_1,x_2,x_3,x_4,x_5]$, where each $x_i$ is the integer conflict degree $C$ for the corresponding test case.",
            "solution": "We model bank conflicts from first principles of the shared memory banking mechanism. The shared memory is divided into $B$ banks, and each element occupies one bank-width unit. Under these assumptions and with base alignment, the bank mapping for an element with linear index $\\mathrm{idx}$ is $\\mathrm{bank} = \\mathrm{idx} \\bmod B$. For a warp of $W$ threads, each thread lane $\\ell \\in \\{0,1,\\dots,W-1\\}$ accesses the element at tile coordinates\n$$\nr(\\ell) = (r_0 + \\ell) \\bmod T_r,\\quad c(\\ell) = (c_0 + \\ell \\cdot d) \\bmod T_c,\n$$\nso the linear index is\n$$\n\\mathrm{idx}(\\ell) = r(\\ell)\\cdot L + c(\\ell),\n$$\nand the bank is\n$$\n\\mathrm{bank}(\\ell) = \\mathrm{idx}(\\ell) \\bmod B.\n$$\n\nBy definition, the conflict degree $C$ for the warp is the maximum, over all banks, of the number of distinct indices requested that map to that bank in the same instruction:\n$$\nC = \\max_{b \\in \\{0,1,\\dots,B-1\\}} \\left|\\left\\{ \\mathrm{idx}(\\ell) \\,\\big|\\, \\mathrm{bank}(\\ell)=b,\\ \\ell \\in \\{0,1,\\dots,W-1\\} \\right\\}\\right|.\n$$\nThis definition naturally captures the broadcast behavior: if multiple threads address the same $\\mathrm{idx}$ within a bank, they contribute only one to the count for that bank.\n\nWe compute $C$ per test case by enumerating $\\ell \\in \\{0,\\dots,W-1\\}$, evaluating $r(\\ell)$, $c(\\ell)$, $\\mathrm{idx}(\\ell)$, and $\\mathrm{bank}(\\ell)$, grouping by bank, counting distinct indices per bank, and taking the maximum of these counts.\n\nWe can also reason analytically for the provided cases:\n\n- Case $1$: $(B,W,L,T_r,T_c,d,r_0,c_0) = (32,32,33,32,32,1,0,0)$. For $\\ell \\in \\{0,\\dots,31\\}$,\n$$\nr(\\ell) = \\ell \\bmod 32 = \\ell,\\quad c(\\ell) = \\ell \\bmod 32 = \\ell,\n$$\nso $\\mathrm{idx}(\\ell) = \\ell\\cdot 33 + \\ell = \\ell\\cdot 34$. Then\n$$\n\\mathrm{bank}(\\ell) = (\\ell\\cdot 34) \\bmod 32 = \\ell\\cdot (34 \\bmod 32) \\bmod 32 = \\ell\\cdot 2 \\bmod 32.\n$$\nThus only even banks $0,2,4,\\dots,30$ are used. For $\\ell$ and $\\ell+16$, we have\n$$\n\\mathrm{bank}(\\ell+16) = 2(\\ell+16) \\bmod 32 = (2\\ell + 32) \\bmod 32 = 2\\ell \\bmod 32 = \\mathrm{bank}(\\ell),\n$$\nand $\\mathrm{idx}(\\ell+16) = (\\ell+16)\\cdot 34 \\neq \\ell\\cdot 34$, so each even bank receives exactly two distinct indices. Therefore $C=2$.\n\n- Case $2$: $(32,32,32,32,32,0,0,0)$. Here $r(\\ell)=\\ell$, $c(\\ell)=0$, so $\\mathrm{idx}(\\ell)=\\ell\\cdot 32$. Then\n$$\n\\mathrm{bank}(\\ell) = (\\ell\\cdot 32) \\bmod 32 = 0,\n$$\nso all threads map to bank $0$ with distinct indices (since $\\ell$ differs). Hence $C=32$.\n\n- Case $3$: $(32,32,33,32,32,0,0,0)$. Here $r(\\ell)=\\ell$, $c(\\ell)=0$, so $\\mathrm{idx}(\\ell)=\\ell\\cdot 33$. Then\n$$\n\\mathrm{bank}(\\ell) = (\\ell\\cdot 33) \\bmod 32 = \\ell\\cdot (33 \\bmod 32) \\bmod 32 = \\ell \\bmod 32 = \\ell.\n$$\nEach bank $b\\in\\{0,\\dots,31\\}$ receives exactly one index. Therefore $C=1$.\n\n- Case $4$: $(32,32,64,1,64,0,0,0)$. Here $T_r=1$ so $r(\\ell)=0$ for all $\\ell$, and $d=0$ so $c(\\ell)=0$. Then $\\mathrm{idx}(\\ell)=0$, $\\mathrm{bank}(\\ell)=0$. All threads access the same element, which is broadcast, so the count of distinct indices in bank $0$ is $1$. Therefore $C=1$.\n\n- Case $5$: $(32,16,33,16,16,1,0,0)$. For $\\ell \\in \\{0,\\dots,15\\}$, $r(\\ell)=\\ell \\bmod 16 = \\ell$, $c(\\ell)=\\ell \\bmod 16 = \\ell$, so $\\mathrm{idx}(\\ell)=\\ell\\cdot 34$, and $\\mathrm{bank}(\\ell)=2\\ell \\bmod 32$. The even banks $0,2,\\dots,30$ receive exactly one index each (since $\\ell$ ranges only over $16$ values), hence $C=1$.\n\nThus, the conflict degrees for the five cases are $[2, 32, 1, 1, 1]$. The program implements the enumerations defined above, computes the sets of distinct indices per bank, and outputs these integers in the specified single-line list format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef conflict_degree(B, W, L, T_r, T_c, d, r0, c0):\n    # Compute bank mapping and conflict degree C as defined in the problem.\n    # Base address aligned; element size equals bank width unit; bank = idx % B.\n    banks = {}  # bank_id - set of distinct indices (to model broadcast)\n    for ell in range(W):\n        r = (r0 + ell) % T_r\n        c = (c0 + (ell * d)) % T_c\n        idx = r * L + c\n        bank = idx % B\n        if bank not in banks:\n            banks[bank] = set()\n        banks[bank].add(idx)\n    # Conflict degree is the maximum number of distinct indices per bank.\n    if not banks:\n        return 0\n    return max((len(s) for s in banks.values()), default=0)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (B, W, L, T_r, T_c, d, r0, c0)\n    test_cases = [\n        (32, 32, 33, 32, 32, 1, 0, 0),  # Case 1\n        (32, 32, 32, 32, 32, 0, 0, 0),  # Case 2\n        (32, 32, 33, 32, 32, 0, 0, 0),  # Case 3\n        (32, 32, 64, 1, 64, 0, 0, 0),   # Case 4\n        (32, 16, 33, 16, 16, 1, 0, 0),  # Case 5\n    ]\n\n    results = []\n    for case in test_cases:\n        B, W, L, T_r, T_c, d, r0, c0 = case\n        result = conflict_degree(B, W, L, T_r, T_c, d, r0, c0)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The GPU memory hierarchy presents a series of trade-offs. While we have focused on the global-to-shared memory transition, there is an even faster level of storage: the register file. However, the number of registers available to each thread is finite and depends on how many threads are running concurrently on a streaming multiprocessor. Excessive demand for registers can lead to *register spilling*, where the compiler is forced to store variables in slow global memory, often with disastrous performance consequences.\n\nIn this final practice, you will model and compare two kernel designs: one that suffers from register spilling and another that mitigates the issue by explicitly using shared memory as a scratchpad. This exercise illuminates the delicate balance between register usage and occupancy, and demonstrates how shared memory can serve as a powerful tool for managing on-chip data and avoiding performance cliffs .",
            "id": "2398435",
            "problem": "A simulation code updates a large one-dimensional field using a per-thread loop on a Graphics Processing Unit (GPU). Consider two kernel designs for a single time-step of this update, each thread operating on one element and performing a fixed number of iterations. In the spill-limited design, each thread uses a demanded number of live scalar temporaries that exceeds the per-thread register allocation, causing register spills to global memory. In the shared-memory-optimized design, exactly those scalars that would spill are explicitly placed in on-chip shared memory to avoid global-memory spills. Assume single-precision values.\n\nAssumptions and definitions to make the model well-defined are as follows. In each iteration per thread, the baseline work consists of one global load and one global store of the element under update. The kernel also uses a demanded number of live scalar temporaries per thread, denoted by $A$, all of which are read and written once per iteration. The Streaming Multiprocessor (SM) register file has a total of $R_{\\mathrm{SM}}$ registers, and per-thread registers are bounded by the lesser of $R_{\\mathrm{thread}\\_\\mathrm{limit}}$ and $\\left\\lfloor \\dfrac{R_{\\mathrm{SM}}}{B_{\\mathrm{SM}} \\cdot T_{\\mathrm{block}}} \\right\\rfloor$, where $B_{\\mathrm{SM}}$ is the number of resident blocks per SM and $T_{\\mathrm{block}}$ is the threads per block; any excess $S=\\max\\!\\left(0,\\, A - \\min\\!\\left(R_{\\mathrm{thread}\\_\\mathrm{limit}},\\, \\left\\lfloor \\dfrac{R_{\\mathrm{SM}}}{B_{\\mathrm{SM}} \\cdot T_{\\mathrm{block}}} \\right\\rfloor\\right)\\right)$ scalars per thread spill. In the spill-limited design, each spilled scalar incurs one global load and one global store per iteration. In the shared-memory-optimized design, those $S$ spilled scalars are instead moved to shared memory, with the same per-iteration access counts, thereby eliminating their global-memory traffic. Let the data size be $s_{\\mathrm{b}}$ bytes per scalar, the number of iterations per thread be $K$, and the floating-point operations per iteration per thread be $F$.\n\nLet the device have $N_{\\mathrm{SM}}$ Streaming Multiprocessors (SMs), a per-SM compute throughput of $C_{\\mathrm{SM}}$ floating-point operations per second, an aggregate global-memory bandwidth of $B_{\\mathrm{g}}$ bytes per second, and an aggregate shared-memory bandwidth of $B_{\\mathrm{s}}$ bytes per second. Use a single-resource-bottleneck model in which the total time for each design equals the longest among the compute service time, the global-memory service time, and the shared-memory service time, respectively. Treat computation and the two memory spaces as independently serviced resources under this model. Let the total number of threads equal the problem size $N$.\n\nAll times must be computed in seconds and rounded to six decimal places. Use $s_{\\mathrm{b}}=4$, $R_{\\mathrm{SM}}=65536$, $R_{\\mathrm{thread}\\_\\mathrm{limit}}=255$, $N_{\\mathrm{SM}}=20$, $C_{\\mathrm{SM}}=1.5\\times 10^{11}$, $B_{\\mathrm{g}}=6.0\\times 10^{11}$, and $B_{\\mathrm{s}}=2.0\\times 10^{12}$. For each test case, compute the following three quantities:\n- The predicted runtime of the spill-limited design in seconds.\n- The predicted runtime of the shared-memory-optimized design in seconds.\n- The ratio of the spill-limited runtime to the shared-memory-optimized runtime, as a decimal.\n\nUse the following test suite, where each case is given as a tuple $(N, K, A, F, T_{\\mathrm{block}}, B_{\\mathrm{SM}})$:\n- Case $1$: $(10^{7}, 10, 80, 100, 256, 4)$.\n- Case $2$: $(10^{7}, 10, 64, 100, 256, 4)$.\n- Case $3$: $(10^{6}, 10, 80, 10000, 256, 4)$.\n- Case $4$: $(10^{7}, 20, 200, 100, 256, 4)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each result is itself a list of three values in the order described above, all rounded to six decimal places. For example, the format must be like this: \"[[t1_spill,t1_shared,s1],[t2_spill,t2_shared,s2],...]\" with no spaces and all times in seconds.",
            "solution": "The problem statement is a valid exercise in computational performance modeling. It presents a well-defined, self-contained, and scientifically grounded scenario for analyzing the performance of two GPU kernel designs. All necessary parameters and formulas are provided to calculate the runtimes based on a single-resource-bottleneck model. The problem is objective and its solution is verifiable through direct calculation. We proceed to solve it.\n\nThe analysis is based on a performance model where the total execution time is determined by the maximum of three service times: compute, global memory, and shared memory. We will first derive the general expressions for these quantities and then apply them to the two specified kernel designs.\n\nLet the input parameters for a given case be $(N, K, A, F, T_{\\mathrm{block}}, B_{\\mathrm{SM}})$.\nThe device constants are:\n- Data size per scalar: $s_{\\mathrm{b}} = 4$ bytes\n- Registers per SM: $R_{\\mathrm{SM}} = 65536$\n- Per-thread register limit: $R_{\\mathrm{thread}\\_\\mathrm{limit}} = 255$\n- Number of SMs: $N_{\\mathrm{SM}} = 20$\n- Per-SM compute throughput: $C_{\\mathrm{SM}} = 1.5 \\times 10^{11}$ FLOPS/s\n- Aggregate global memory bandwidth: $B_{\\mathrm{g}} = 6.0 \\times 10^{11}$ bytes/s\n- Aggregate shared memory bandwidth: $B_{\\mathrm{s}} = 2.0 \\times 10^{12}$ bytes/s\n\nFirst, we determine the number of registers allocated per thread, $R_{\\text{alloc}}$. This is limited by the architectural maximum per thread and the resources available on the Streaming Multiprocessor (SM) based on its occupancy.\n$$\nR_{\\text{alloc}} = \\min\\left(R_{\\mathrm{thread}\\_\\mathrm{limit}}, \\left\\lfloor \\frac{R_{\\mathrm{SM}}}{B_{\\mathrm{SM}} \\cdot T_{\\mathrm{block}}} \\right\\rfloor\\right)\n$$\n\nNext, we calculate the number of scalar temporaries per thread, $S$, that must be \"spilled\" from registers because the demand $A$ exceeds the allocation $R_{\\text{alloc}}$.\n$$\nS = \\max(0, A - R_{\\text{alloc}})\n$$\n\nWith $S$ determined, we can model the total time for each design. The total time $T$ is the bottleneck of the three primary resources.\n$$\nT = \\max(T_{\\text{compute}}, T_{\\text{global}}, T_{\\text{shared}})\n$$\n\nThe total computational work is the same for both designs. It is the product of the number of threads $N$, the iterations per thread $K$, and the floating-point operations per iteration $F$. The total compute throughput is the per-SM throughput $C_{\\mathrm{SM}}$ multiplied by the number of SMs $N_{\\mathrm{SM}}$. The compute service time $T_{\\text{compute}}$ is therefore:\n$$\nT_{\\text{compute}} = \\frac{\\text{Total Operations}}{\\text{Total Throughput}} = \\frac{N \\cdot K \\cdot F}{N_{\\mathrm{SM}} \\cdot C_{\\mathrm{SM}}}\n$$\n\nNow we analyze each design separately.\n\n**1. Spill-Limited Design**\n\nIn this design, the $S$ excess scalars are spilled to global memory. Each spill operation involves a load and a store from/to global memory per iteration.\n\n- **Global Memory Service Time ($T_{\\text{global, spill}}$):**\nThe total global memory traffic per thread per iteration includes the baseline work (one load and one store for the main field element) and the traffic from spilled scalars ($S$ loads and $S$ stores).\nTotal global memory accesses per thread per iteration = $2$ (baseline) + $2S$ (spills) = $2(1 + S)$.\nTotal global memory data transferred = $N \\cdot K \\cdot 2(1 + S) \\cdot s_{\\mathrm{b}}$ bytes.\nThe global memory service time is this total data divided by the aggregate bandwidth $B_{\\mathrm{g}}$.\n$$\nT_{\\text{global, spill}} = \\frac{N \\cdot K \\cdot 2(1 + S) \\cdot s_{\\mathrm{b}}}{B_{\\mathrm{g}}}\n$$\n\n- **Shared Memory Service Time ($T_{\\text{shared, spill}}$):**\nThis design does not explicitly use shared memory. Therefore, the traffic and service time are zero.\n$$\nT_{\\text{shared, spill}} = 0\n$$\n\n- **Total Time for Spill-Limited Design ($T_{\\text{spill}}$):**\n$$\nT_{\\text{spill}} = \\max(T_{\\text{compute}}, T_{\\text{global, spill}}, 0)\n$$\n\n**2. Shared-Memory-Optimized Design**\n\nIn this design, the $S$ scalars that would have spilled are explicitly managed in on-chip shared memory, avoiding the expensive global memory traffic.\n\n- **Global Memory Service Time ($T_{\\text{global, shared}}$):**\nThe global memory traffic is now reduced to only the baseline work, as the spills are handled by shared memory.\nTotal global memory accesses per thread per iteration = $2$.\nTotal global memory data transferred = $N \\cdot K \\cdot 2 \\cdot s_{\\mathrm{b}}$ bytes.\n$$\nT_{\\text{global, shared}} = \\frac{N \\cdot K \\cdot 2 \\cdot s_{\\mathrm{b}}}{B_{\\mathrm{g}}}\n$$\n\n- **Shared Memory Service Time ($T_{\\text{shared, shared}}$):**\nThe $S$ scalars are now read from and written to shared memory once per iteration.\nTotal shared memory accesses per thread per iteration = $S$ reads + $S$ writes = $2S$.\nTotal shared memory data transferred = $N \\cdot K \\cdot 2S \\cdot s_{\\mathrm{b}}$ bytes.\nThe shared memory service time is this total data divided by the aggregate bandwidth $B_{\\mathrm{s}}$.\n$$\nT_{\\text{shared, shared}} = \\frac{N \\cdot K \\cdot 2S \\cdot s_{\\mathrm{b}}}{B_{\\mathrm{s}}}\n$$\n\n- **Total Time for Shared-Memory-Optimized Design ($T_{\\text{shared}}$):**\n$$\nT_{\\text{shared}} = \\max(T_{\\text{compute}}, T_{\\text{global, shared}}, T_{\\text{shared, shared}})\n$$\n\nFinally, the performance ratio is computed as:\n$$\n\\text{Ratio} = \\frac{T_{\\text{spill}}}{T_{\\text{shared}}}\n$$\n\nThese formulas will be applied to each test case, and the results will be rounded to six decimal places as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the GPU performance modeling problem for the given test cases.\n    \"\"\"\n    \n    # Define constants from the problem statement\n    S_B = 4.0  # bytes per scalar\n    R_SM = 65536.0\n    R_THREAD_LIMIT = 255.0\n    N_SM = 20.0\n    C_SM = 1.5e11  # FLOPS/s\n    B_G = 6.0e11  # bytes/s\n    B_S = 2.0e12  # bytes/s\n\n    test_cases = [\n        # (N, K, A, F, T_block, B_SM)\n        (1.0e7, 10.0, 80.0, 100.0, 256.0, 4.0),\n        (1.0e7, 10.0, 64.0, 100.0, 256.0, 4.0),\n        (1.0e6, 10.0, 80.0, 10000.0, 256.0, 4.0),\n        (1.0e7, 20.0, 200.0, 100.0, 256.0, 4.0),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N, K, A, F, T_block, B_SM = case\n\n        # Step 1: Calculate per-thread register allocation\n        r_alloc_pool = math.floor(R_SM / (B_SM * T_block))\n        r_alloc = min(R_THREAD_LIMIT, r_alloc_pool)\n\n        # Step 2: Calculate spilled scalars per thread\n        S = max(0.0, A - r_alloc)\n\n        # Step 3: Calculate common compute time\n        total_ops = N * K * F\n        total_compute_throughput = N_SM * C_SM\n        t_compute = total_ops / total_compute_throughput\n\n        # Step 4: Analyze Spill-Limited Design\n        # Global memory traffic from baseline + spills\n        global_bytes_spill = N * K * (2.0 + 2.0 * S) * S_B\n        t_global_spill = global_bytes_spill / B_G\n        # Shared memory time is zero\n        t_shared_spill = 0.0\n        # Total time is the bottleneck\n        t_spill = max(t_compute, t_global_spill, t_shared_spill)\n\n        # Step 5: Analyze Shared-Memory-Optimized Design\n        # Global memory traffic from baseline only\n        global_bytes_shared = N * K * 2.0 * S_B\n        t_global_shared = global_bytes_shared / B_G\n        # Shared memory traffic from the S scalars\n        shared_bytes_shared = N * K * 2.0 * S * S_B\n        t_shared_shared = shared_bytes_shared / B_S\n        # Total time is the bottleneck\n        t_shared = max(t_compute, t_global_shared, t_shared_shared)\n\n        # Step 6: Calculate ratio\n        # Avoid division by zero if a runtime is zero, though unlikely here\n        ratio = t_spill / t_shared if t_shared > 0 else 0.0\n\n        # Step 7: Store rounded results\n        results.append([\n            round(t_spill, 6),\n            round(t_shared, 6),\n            round(ratio, 6)\n        ])\n\n    # Final print statement in the exact required format.\n    # Format: [[t1_spill,t1_shared,s1],[t2_spill,t2_shared,s2],...]\n    formatted_results = []\n    for res_list in results:\n        # Convert each float to a string with the necessary precision\n        # Then join them into a string like '[val1,val2,val3]'\n        sub_list_str = f\"[{res_list[0]:.6f},{res_list[1]:.6f},{res_list[2]:.6f}]\"\n        formatted_results.append(sub_list_str)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}