{
    "hands_on_practices": [
        {
            "introduction": "本练习将引导您使用多元多项式回归来为复杂的非线性关系建模。通过将原始输入特征转换为包含多项式和交互项的高维空间，我们可以利用熟悉的线性最小二乘法框架来拟合一个非线性模型。 该练习旨在锻炼您在特征工程方面的基本技能，并将其应用于化学领域的实际问题中。",
            "id": "2425201",
            "problem": "考虑使用多元多项式回归模型，学习从分子描述符到标准沸点的非线性映射任务。您将获得一个化合物训练集，其中包含三个描述符：分子量 $M$（单位：克/摩尔）、拓扑极性表面积 $A$（单位：平方埃）以及氢键供体数 $H$（无单位）。目标是标准沸点 $T_b$（单位：开尔文）。您的目标是设计并实现一个程序，从最小二乘估计的基本原理出发，将一个二阶多元多项式模型拟合到训练数据，然后预测一组测试化合物的沸点。\n\n基本原理和约束：\n- 从最小二乘估计的定义出发，即最小化观测目标与预测目标之间残差的平方和。\n- 通过在标准化输入上构建一个总阶数最高为 $2$ 的多项式特征映射来处理非线性关系。具体来说，对于一个描述符向量 $\\mathbf{x} = [M,A,H]^\\top$，将其标准化为 $\\mathbf{z} = ( \\mathbf{x} - \\boldsymbol{\\mu} ) \\oslash \\boldsymbol{\\sigma}$，其中 $\\boldsymbol{\\mu}$ 和 $\\boldsymbol{\\sigma}$ 是从训练描述符计算的逐分量均值和标准差，$\\oslash$ 表示逐元素除法。然后构建特征向量 $\\boldsymbol{\\phi}(\\mathbf{z})$，包含常数项、所有线性项、所有唯一的成对交互项以及所有最高为 $2$ 阶的平方项。\n- 通过最小化训练集上的残差平方和来拟合模型参数。不包含任何正则化项。\n- 使用拟合好的模型为提供的测试描述符预测 $T_b$。\n\n训练数据（每个元组为 $(M,A,H,T_b)$；$M$ 单位为克/摩尔，$A$ 单位为平方埃，$T_b$ 单位为开尔文）：\n- $\\left(60,\\,20,\\,1,\\,369.0\\right)$\n- $\\left(80,\\,35,\\,0,\\,382.6\\right)$\n- $\\left(120,\\,40,\\,2,\\,440.0\\right)$\n- $\\left(150,\\,70,\\,1,\\,503.95\\right)$\n- $\\left(90,\\,25,\\,2,\\,401.9\\right)$\n- $\\left(110,\\,60,\\,0,\\,446.45\\right)$\n- $\\left(140,\\,50,\\,1,\\,449.5\\right)$\n- $\\left(70,\\,15,\\,0,\\,343.6\\right)$\n- $\\left(180,\\,80,\\,2,\\,557.4\\right)$\n- $\\left(200,\\,30,\\,3,\\,433.7\\right)$\n- $\\left(100,\\,45,\\,1,\\,429.95\\right)$\n- $\\left(130,\\,55,\\,2,\\,478.4\\right)$\n\n特征构建要求：\n- 仅使用训练描述符计算 $M$、$A$ 和 $H$ 的训练集均值 $\\boldsymbol{\\mu}$ 和标准差 $\\boldsymbol{\\sigma}$。\n- 对每个描述符向量，计算标准化描述符 $\\mathbf{z} = \\left[z_1, z_2, z_3\\right]^\\top$，其中 $z_i = \\left(x_i - \\mu_i\\right)/\\sigma_i$，$i \\in \\{1,2,3\\}$。\n- 构建 $2$ 阶多项式特征向量\n$$\n\\boldsymbol{\\phi}(\\mathbf{z}) = \\left[1,\\, z_1,\\, z_2,\\, z_3,\\, z_1^2,\\, z_2^2,\\, z_3^2,\\, z_1 z_2,\\, z_1 z_3,\\, z_2 z_3 \\right]^\\top.\n$$\n\n模型拟合要求：\n- 在特征空间 $\\boldsymbol{\\phi}(\\mathbf{z})$ 中拟合一个线性模型来预测 $T_b$，方法是通过在训练集上使用无正则化的普通最小二乘法来最小化残差平方和。\n\n测试集：\n- 为以下四个描述符三元组 $(M,A,H)$ 预测 $T_b$：\n  1. $\\left(95,\\,38,\\,1\\right)$\n  2. $\\left(160,\\,65,\\,2\\right)$\n  3. $\\left(50,\\,10,\\,0\\right)$\n  4. $\\left(210,\\,85,\\,3\\right)$\n\n答案规格与单位：\n- 以开尔文为单位报告每个预测的标准沸点。\n- 将每个预测值四舍五入到最近的 $0.01$ 开尔文。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含四个四舍五入后的预测值，格式为逗号分隔的列表，并用方括号括起来，无任何额外文本。例如，要求的格式是 [$r_1$,$r_2$,$r_3$,$r_4$]，其中每个 $r_i$ 是一个四舍五入到最近 $0.01$ 开尔文的浮点数。",
            "solution": "提交审议的问题陈述是有效的。它提出了一个计算工程领域的适定问题，特别是在定量构效关系（QSPR）领域。任务是基于一组分子描述符，为化合物的标准沸点构建一个预测模型。该问题提供了所有必要的数据、清晰的模型结构定义，以及基于普通最小二乘法的精确参数估计方法。该问题具有科学依据，逻辑一致，且无歧义。因此，可以推导出严谨的解。\n\n问题的核心是拟合一个多元多项式回归模型。尽管目标变量（沸点 $T_b$）与原始描述符 $\\mathbf{x} = [M, A, H]^\\top$ 之间的关系是非线性的，但通过构建一组新的特征 $\\boldsymbol{\\phi}(\\mathbf{z})$，该问题被转化为一个线性回归问题。模型假设相对于这些构造的特征，在线性系数 $\\mathbf{w}$ 上是线性的：\n$$\n\\hat{T}_b = f(\\mathbf{x}; \\mathbf{w}) = \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{z}(\\mathbf{x}))\n$$\n其中 $\\mathbf{z}$ 是标准化后的描述符向量，$\\mathbf{w}$ 是待确定的模型参数向量。\n\n参数 $\\mathbf{w}$ 是通过最小化 $N$ 个样本训练集上的残差平方和（SSR）来找到的。SSR 作为成本函数 $J(\\mathbf{w})$，由下式给出：\n$$\nJ(\\mathbf{w}) = \\sum_{i=1}^{N} (T_{b,i} - \\hat{T}_{b,i})^2 = \\sum_{i=1}^{N} (T_{b,i} - \\mathbf{w}^\\top \\boldsymbol{\\phi}(\\mathbf{z}_i))^2\n$$\n其中 $T_{b,i}$ 是第 $i$ 个样本的观测沸点。\n\n为了便于推导，我们用矩阵表示法来表达。设 $\\mathbf{y}$ 是观测沸点的 $N \\times 1$ 向量，$\\Phi$ 是 $N \\times D$ 的设计矩阵，其中第 $i$ 行是第 $i$ 个训练样本的特征向量 $\\boldsymbol{\\phi}(\\mathbf{z}_i)^\\top$，$D$ 是特征数量（包括常数项）。在此问题中，$N=12$ 且 $D=10$。成本函数变为：\n$$\nJ(\\mathbf{w}) = (\\mathbf{y} - \\Phi\\mathbf{w})^\\top (\\mathbf{y} - \\Phi\\mathbf{w})\n$$\n展开此表达式可得：\n$$\nJ(\\mathbf{w}) = \\mathbf{y}^\\top\\mathbf{y} - \\mathbf{y}^\\top\\Phi\\mathbf{w} - (\\Phi\\mathbf{w})^\\top\\mathbf{y} + (\\Phi\\mathbf{w})^\\top(\\Phi\\mathbf{w}) = \\mathbf{y}^\\top\\mathbf{y} - 2\\mathbf{w}^\\top\\Phi^\\top\\mathbf{y} + \\mathbf{w}^\\top\\Phi^\\top\\Phi\\mathbf{w}\n$$\n为了找到这个二次函数的最小值，我们计算它关于 $\\mathbf{w}$ 的梯度，并将其设为零向量：\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = -2\\Phi^\\top\\mathbf{y} + 2\\Phi^\\top\\Phi\\mathbf{w} = \\mathbf{0}\n$$\n这就得到了正规方程：\n$$\n(\\Phi^\\top\\Phi)\\mathbf{w} = \\Phi^\\top\\mathbf{y}\n$$\n假设矩阵 $\\Phi^\\top\\Phi$ 是可逆的（如果 $\\Phi$ 的列是线性无关的，这通常成立），则最优权重向量 $\\mathbf{w}_{\\text{OLS}}$ 的唯一解是：\n$$\n\\mathbf{w}_{\\text{OLS}} = (\\Phi^\\top\\Phi)^{-1} \\Phi^\\top\\mathbf{y}\n$$\n这个方程提供了普通最小二乘法的解析解。项 $(\\Phi^\\top\\Phi)^{-1}\\Phi^\\top$ 被称为 $\\Phi$ 的 Moore-Penrose 伪逆。\n\n实现将遵循以下程序步骤：\n1.  **数据标准化：**\n    首先，使用训练描述符数据（一个 $N \\times 3$ 的矩阵 $\\mathbf{X}_{\\text{train}}$）来计算逐分量的均值向量 $\\boldsymbol{\\mu}$ 和标准差向量 $\\boldsymbol{\\sigma}$。标准差计算时分母使用 $N$，对应于总体标准差。\n    $$\n    \\mu_j = \\frac{1}{N} \\sum_{i=1}^{N} X_{ij}, \\quad \\sigma_j = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (X_{ij} - \\mu_j)^2}\n    $$\n    然后对每个训练描述符向量 $\\mathbf{x}_i$进行标准化：$\\mathbf{z}_i = (\\mathbf{x}_i - \\boldsymbol{\\mu}) \\oslash \\boldsymbol{\\sigma}$。\n\n2.  **特征工程：**\n    对于每个标准化向量 $\\mathbf{z}_i = \\left[z_{i,1}, z_{i,2}, z_{i,3}\\right]^\\top$，按规定构建特征向量 $\\boldsymbol{\\phi}(\\mathbf{z}_i)$：\n    $$\n    \\boldsymbol{\\phi}(\\mathbf{z}_i) = \\left[1,\\, z_{i,1},\\, z_{i,2},\\, z_{i,3},\\, z_{i,1}^2,\\, z_{i,2}^2,\\, z_{i,3}^2,\\, z_{i,1} z_{i,2},\\, z_{i,1} z_{i,3},\\, z_{i,2} z_{i,3} \\right]^\\top\n    $$\n    将这 $N$ 个特征向量堆叠成行，形成 $N \\times D$ 的设计矩阵 $\\Phi$，其中 $N=12$，$D=10$。\n\n3.  **求解正规方程：**\n    从给定的 $T_b$ 值构成训练目标向量 $\\mathbf{y}$。求解线性系统 $(\\Phi^\\top\\Phi)\\mathbf{w} = \\Phi^\\top\\mathbf{y}$ 以得到 $\\mathbf{w}$。虽然可以直接计算逆矩阵 $(\\Phi^\\top\\Phi)^{-1}$，但这在数值上不如使用专用的线性系统求解器稳定。\n\n4.  **预测：**\n    对于每个测试描述符三元组 $\\mathbf{x}_{\\text{test}}$，必须首先使用从训练集计算出的相同 $\\boldsymbol{\\mu}$ 和 $\\boldsymbol{\\sigma}$ 对其进行标准化。然后，构建相应的特征向量 $\\boldsymbol{\\phi}(\\mathbf{z}_{\\text{test}})$。预测值通过点积计算得出：\n    $$\n    \\hat{T}_{b, \\text{test}} = \\mathbf{w}_{\\text{OLS}}^\\top \\boldsymbol{\\phi}(\\mathbf{z}_{\\text{test}})\n    $$\n    最终结果四舍五入到两位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multivariate polynomial regression problem using ordinary least squares.\n    \"\"\"\n    # Training data: (M, A, H, T_b)\n    # M in g/mol, A in Angstrom^2, H is unitless, T_b in Kelvin.\n    training_data = np.array([\n        (60, 20, 1, 369.0),\n        (80, 35, 0, 382.6),\n        (120, 40, 2, 440.0),\n        (150, 70, 1, 503.95),\n        (90, 25, 2, 401.9),\n        (110, 60, 0, 446.45),\n        (140, 50, 1, 449.5),\n        (70, 15, 0, 343.6),\n        (180, 80, 2, 557.4),\n        (200, 30, 3, 433.7),\n        (100, 45, 1, 429.95),\n        (130, 55, 2, 478.4)\n    ])\n\n    # Test data: (M, A, H)\n    test_cases = np.array([\n        (95, 38, 1),\n        (160, 65, 2),\n        (50, 10, 0),\n        (210, 85, 3)\n    ])\n\n    # Step 1: Separate descriptors and targets from training data\n    X_train = training_data[:, :3]\n    y_train = training_data[:, 3]\n\n    # Step 2: Compute standardization parameters from the training set\n    mu = np.mean(X_train, axis=0)\n    # ddof=0 for population standard deviation, as is standard when not specified\n    sigma = np.std(X_train, axis=0, ddof=0)\n\n    # Standardize the training descriptors\n    Z_train = (X_train - mu) / sigma\n\n    # Step 3: Construct the design matrix Phi from the standardized training data\n    def construct_feature_matrix(Z):\n        \"\"\"Constructs the polynomial feature matrix from standardized descriptors.\"\"\"\n        n_samples = Z.shape[0]\n        z1, z2, z3 = Z[:, 0], Z[:, 1], Z[:, 2]\n        \n        # The feature vector is [1, z1, z2, z3, z1^2, z2^2, z3^2, z1*z2, z1*z3, z2*z3]\n        phi = np.zeros((n_samples, 10))\n        phi[:, 0] = 1\n        phi[:, 1] = z1\n        phi[:, 2] = z2\n        phi[:, 3] = z3\n        phi[:, 4] = z1**2\n        phi[:, 5] = z2**2\n        phi[:, 6] = z3**2\n        phi[:, 7] = z1 * z2\n        phi[:, 8] = z1 * z3\n        phi[:, 9] = z2 * z3\n        return phi\n\n    Phi_train = construct_feature_matrix(Z_train)\n\n    # Step 4: Solve the normal equations to find the weight vector w\n    # (Phi^T * Phi) * w = Phi^T * y\n    # Use np.linalg.solve for numerical stability instead of direct inversion.\n    try:\n        A = Phi_train.T @ Phi_train\n        b = Phi_train.T @ y_train\n        w = np.linalg.solve(A, b)\n    except np.linalg.LinAlgError:\n        # Fallback to least-squares solver if matrix is singular.\n        # This is more robust and directly minimizes the sum of squares.\n        w, _, _, _ = np.linalg.lstsq(Phi_train, y_train, rcond=None)\n\n    # Step 5: Predict T_b for the test cases\n    # Standardize test data using training set parameters\n    Z_test = (test_cases - mu) / sigma\n    \n    # Construct the feature matrix for the test data\n    Phi_test = construct_feature_matrix(Z_test)\n    \n    # Perform prediction\n    predictions = Phi_test @ w\n\n    # Round the predictions to the nearest 0.01 Kelvin\n    rounded_predictions = np.round(predictions, 2)\n    \n    # Format the final output string\n    result_str = \",\".join([f\"{p:.2f}\" for p in rounded_predictions])\n    print(f\"[{result_str}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "在普通最小二乘法的基础上，本练习将解决一个在真实世界数据中常见的挑战：异方差性，即测量误差的方差不是恒定的。您将实现迭代重加权最小二乘 (Iteratively Reweighted Least Squares, IRLS) 算法，这是一种通过为不同观测值分配不同权重以获得更精确参数估计的强大技术。 通过这个练习，您将更深刻地理解如何调整回归方法，以更好地反映潜在的数据生成过程。",
            "id": "2425232",
            "problem": "给定独立的配对观测值 $\\{(x_i,y_i)\\}_{i=1}^{n}$，要求在异方差噪声下拟合一个多项式回归模型，其中测量误差方差与均值响应的绝对值成正比。具体来说，假设一个 $d$ 次多项式，\n$$\n\\mu(x;\\boldsymbol{\\beta}) \\equiv \\beta_0 + \\beta_1 x + \\cdots + \\beta_d x^d,\n$$\n并假设误差满足 $y_i = \\mu(x_i;\\boldsymbol{\\beta}) + \\varepsilon_i$，其中 $\\mathbb{E}[\\varepsilon_i]=0$ 且 $\\operatorname{Var}(\\varepsilon_i)=\\sigma^2 \\,|\\mu(x_i;\\boldsymbol{\\beta})|$。这里 $\\sigma^2$ 是一个未知常数，$|\\,\\cdot\\,|$ 表示绝对值。因此，与方差倒数成正比的权重函数取决于未知的均值。使用基于迭代重加权最小二乘法的可行加权最小二乘程序来估计 $\\boldsymbol{\\beta}$：\n- 从一个未加权的普通最小二乘拟合开始，获得初始估计值 $\\widehat{\\boldsymbol{\\beta}}^{(0)}$。\n- 在第 $t \\in \\{1,2,\\dots\\}$ 次迭代中，构建预测均值 $\\widehat{\\mu}_i^{(t-1)} = \\mu(x_i;\\widehat{\\boldsymbol{\\beta}}^{(t-1)})$ 并设置权重\n$$\nw_i^{(t)} = \\frac{1}{\\max\\left(|\\widehat{\\mu}_i^{(t-1)}|,\\tau\\right)},\n$$\n其中 $\\tau$ 是一个小的正阈值，用以避免除以零。然后计算加权最小二乘估计\n$$\n\\widehat{\\boldsymbol{\\beta}}^{(t)} \\in \\arg\\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^{n} w_i^{(t)}\\left(y_i - \\mu(x_i;\\boldsymbol{\\beta})\\right)^2.\n$$\n- 当参数向量的相对变化很小时停止：\n$$\n\\frac{\\lVert \\widehat{\\boldsymbol{\\beta}}^{(t)} - \\widehat{\\boldsymbol{\\beta}}^{(t-1)} \\rVert_2}{\\max\\left(1,\\lVert \\widehat{\\boldsymbol{\\beta}}^{(t)} \\rVert_2\\right)}  \\text{tol},\n$$\n或者当达到最大迭代次数时。\n\n对所有测试用例使用以下固定的算法超参数：阈值 $\\tau = 10^{-3}$，容差 $\\text{tol} = 10^{-10}$，以及最大迭代次数 $T_{\\max} = 100$。\n\n对于每个测试用例，你的程序必须：\n- 构建设计矩阵，其列为 $[1,x,x^2,\\dots,x^d]$。\n- 运行如上定义的可行加权最小二乘迭代。\n- 终止后，报告：\n  1. 拟合的系数向量 $\\widehat{\\boldsymbol{\\beta}}$，以 $d+1$ 个浮点数的列表形式表示。\n  2. 最终的加权残差平方和\n     $$\n     S = \\sum_{i=1}^{n} w_i^{(\\text{final})}\\left(y_i - \\mu(x_i;\\widehat{\\boldsymbol{\\beta}})\\right)^2,\n     $$\n     其中 $w_i^{(\\text{final})}$ 是根据最终预测均值计算的最后一次迭代的权重。\n  3. 实际执行的迭代次数 $k$（整数）。\n\n将每个浮点输出四舍五入到 $6$ 位小数。整数输出 $k$ 不进行四舍五入。\n\n测试套件：\n- 案例 $1$（通用，2 次）：\n  - $d = 2$,\n  - $x = [0, 1, 2, 3, 4, 5, 6, 7]$,\n  - $y = [1.00, 0.72, 0.77, 1.34, 2.15, 3.60, 5.05, 7.50]$.\n- 案例 $2$（包含负响应和近零响应，2 次）：\n  - $d = 2$,\n  - $x = [-3, -2, -1, 0, 1, 2, 3]$,\n  - $y = [-2.10, -1.55, -0.60, -0.20, 0.12, 0.17, 0.11]$.\n- 案例 $3$（包含一个精确为零的测量值，2 次）：\n  - $d = 2$,\n  - $x = [0, 1, 2, 3]$,\n  - $y = [0.00, 0.05, 0.20, 0.45]$.\n\n最终输出格式：\n- 你的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表。每个测试用例的结果本身必须是以下形式的列表\n$$\n[[\\beta_0,\\beta_1,\\dots,\\beta_d], S, k],\n$$\n其中所有浮点数条目都四舍五入到 $6$ 位小数。对于以上三个案例，所需的输出格式为\n$$\n[[[\\beta_0,\\beta_1,\\beta_2],S,k], [[\\beta_0,\\beta_1,\\beta_2],S,k], [[\\beta_0,\\beta_1,\\beta_2],S,k]],\n$$\n打印时不带空格，例如\n$$\n[[[\\beta_0,\\beta_1,\\beta_2],S,k],[[\\beta_0,\\beta_1,\\beta_2],S,k],[[\\beta_0,\\beta_1,\\beta_2],S,k]].\n$$\n\n不涉及物理单位或角度。所有输出必须是指定的纯数字，并且必须精确格式化为单行，没有任何多余的字符或空格。",
            "solution": "问题陈述经评估是有效且适定的。它描述了计算统计学中的一个标准流程，用于在存在异方差噪声的情况下拟合多项式回归模型，其中误差项的方差与均值响应的绝对值成正比。所提出的方法，即可行加权最小二乘法（FWLS），通过迭代重加权最小二乘法（IRLS）实现，是解决此类问题的正确且成熟的技术。所有必要的数据、模型规格、算法步骤、超参数和输出格式均已提供，从而可以得到唯一且可验证的解。\n\n问题的核心是为多项式模型 $\\mu(x;\\boldsymbol{\\beta}) = \\sum_{j=0}^{d} \\beta_j x^j$ 估计参数向量 $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\dots, \\beta_d]^T$。观测数据 $y_i$ 假设遵循模型 $y_i = \\mu(x_i;\\boldsymbol{\\beta}) + \\varepsilon_i$，其误差方差为 $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2 |\\mu(x_i;\\boldsymbol{\\beta})|$。\n\n此类模型的最优估计量是加权最小二乘（WLS）估计量，它最小化加权残差平方和。权重 $w_i$ 应与观测值方差成反比，即 $w_i \\propto 1/|\\mu(x_i;\\boldsymbol{\\beta})|$。然而，真实均值 $\\mu(x_i;\\boldsymbol{\\beta})$ 是未知的。IRLS 算法通过迭代逼近最优权重来解决这个问题。\n\n流程如下：\n\n设给定数据为 $\\{(x_i, y_i)\\}_{i=1}^{n}$。首先，我们构建 $n \\times (d+1)$ 的设计矩阵 $\\mathbf{X}$，其中第 $i$ 行由 $[1, x_i, x_i^2, \\dots, x_i^d]$ 给出。然后，模型可以用矩阵形式表示为 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$。\n\n**步骤 0：初始化**\n通过忽略异方差性并执行普通最小二乘（OLS）拟合，获得初始估计值 $\\widehat{\\boldsymbol{\\beta}}^{(0)}$。这会最小化未加权误差平方和 $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2$。解由以下正规方程组给出：\n$$\n\\widehat{\\boldsymbol{\\beta}}^{(0)} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\n\n**步骤 1：迭代优化**\n对于每次迭代 $t=1, 2, \\dots, T_{\\max}$：\n1.  **估计均值响应**：使用前一次迭代的参数估计值 $\\widehat{\\boldsymbol{\\beta}}^{(t-1)}$，我们计算每个观测值的预测均值：\n    $$\n    \\widehat{\\boldsymbol{\\mu}}^{(t-1)} = \\mathbf{X}\\widehat{\\boldsymbol{\\beta}}^{(t-1)}\n    $$\n2.  **更新权重**：我们根据估计的均值更新权重。第 $i$ 个观测值的权重为：\n    $$\n    w_i^{(t)} = \\frac{1}{\\max\\left(|\\widehat{\\mu}_i^{(t-1)}|, \\tau\\right)}\n    $$\n    其中 $\\tau = 10^{-3}$ 是一个小的正阈值，用于在预测均值接近于零时防止除以零或权重过大。这些权重被组装成一个对角矩阵 $\\mathbf{W}^{(t)}$。\n\n3.  **求解 WLS 问题**：通过求解 WLS 问题找到新的参数估计值 $\\widehat{\\boldsymbol{\\beta}}^{(t)}$，该问题旨在最小化加权误差平方和：\n    $$\n    \\widehat{\\boldsymbol{\\beta}}^{(t)} = \\arg\\min_{\\boldsymbol{\\beta}} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T \\mathbf{W}^{(t)} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n    $$\n    解由以下加权正规方程组给出：\n    $$\n    \\widehat{\\boldsymbol{\\beta}}^{(t)} = (\\mathbf{X}^T\\mathbf{W}^{(t)}\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{W}^{(t)}\\mathbf{y}\n    $$\n    在数值上，这可以通过变换系统来稳健地求解。设 $\\mathbf{D}^{(t)}$ 是一个对角矩阵，其对角线元素为 $\\sqrt{w_i^{(t)}}$。该问题等价于为变换后的变量 $\\mathbf{y}' = \\mathbf{D}^{(t)}\\mathbf{y}$ 和 $\\mathbf{X}' = \\mathbf{D}^{(t)}\\mathbf{X}$ 求解一个 OLS 问题。\n\n**步骤 2：收敛性检查**\n当参数向量的相对变化小于指定的容差 $\\text{tol} = 10^{-10}$ 时，迭代过程终止：\n$$\n\\frac{\\lVert \\widehat{\\boldsymbol{\\beta}}^{(t)} - \\widehat{\\boldsymbol{\\beta}}^{(t-1)} \\rVert_2}{\\max\\left(1, \\lVert \\widehat{\\boldsymbol{\\beta}}^{(t)} \\rVert_2\\right)}  \\text{tol}\n$$\n或者当达到最大迭代次数 $T_{\\max} = 100$ 时。设最终迭代次数为 $k$，最终参数估计为 $\\widehat{\\boldsymbol{\\beta}}$。\n\n**步骤 3：最终输出计算**\n终止时，计算以下量：\n1.  最终系数向量 $\\widehat{\\boldsymbol{\\beta}}$。\n2.  最终的加权残差平方和 $S$。这需要计算最终的预测均值 $\\widehat{\\boldsymbol{\\mu}} = \\mathbf{X}\\widehat{\\boldsymbol{\\beta}}$ 和相应的最终权重 $w_i^{(\\text{final})} = 1/\\max(|\\widehat{\\mu}_i|, \\tau)$。然后，总和 $S$ 为：\n    $$\n    S = \\sum_{i=1}^{n} w_i^{(\\text{final})}\\left(y_i - \\widehat{\\mu}_i\\right)^2\n    $$\n3.  执行的总迭代次数 $k$。\n\n此流程将应用于问题中指定的每个测试用例。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the series of test cases for feasible weighted least squares\n    polynomial regression.\n    \"\"\"\n\n    def solve_case(d, x_data, y_data):\n        \"\"\"\n        Implements the Iteratively Reweighted Least Squares (IRLS) algorithm for a\n        single case.\n        \"\"\"\n        # Algorithmic hyperparameters\n        tau = 1e-3\n        tol = 1e-10\n        T_max = 100\n\n        x_np = np.asarray(x_data, dtype=float)\n        y_np = np.asarray(y_data, dtype=float)\n\n        # Construct the design matrix X with columns [1, x, x^2, ..., x^d]\n        X = np.vander(x_np, d + 1, increasing=True)\n        \n        # Step 0: Initial OLS fit\n        beta_current, _, _, _ = np.linalg.lstsq(X, y_np, rcond=None)\n        \n        k = 0\n        for t in range(1, T_max + 1):\n            k = t\n            beta_previous = beta_current.copy()\n\n            # a. Calculate predicted means\n            mu_hat_previous = X @ beta_previous\n\n            # b. Calculate weights\n            weights = 1.0 / np.maximum(np.abs(mu_hat_previous), tau)\n            \n            # c. Update beta using a weighted least squares fit.\n            # This is solved by transforming the system and using OLS.\n            D = np.diag(np.sqrt(weights))\n            X_prime = D @ X\n            y_prime = D @ y_np\n            \n            beta_current, _, _, _ = np.linalg.lstsq(X_prime, y_prime, rcond=None)\n\n            # d. Check for convergence\n            norm_beta_current = np.linalg.norm(beta_current)\n            diff_norm = np.linalg.norm(beta_current - beta_previous)\n            \n            relative_change = diff_norm / np.maximum(1.0, norm_beta_current)\n\n            if relative_change  tol:\n                break\n        \n        # After termination, compute final outputs\n        beta_final = beta_current\n        \n        # Calculate final predicted means from the final beta\n        mu_final = X @ beta_final\n        \n        # Calculate final weights based on the final predicted means\n        w_final = 1.0 / np.maximum(np.abs(mu_final), tau)\n        \n        # Calculate final weighted residual sum of squares (S)\n        residuals = y_np - mu_final\n        S = np.sum(w_final * (residuals**2))\n        \n        return beta_final, S, k\n\n    test_cases = [\n        # Case 1 (general, degree 2)\n        (2, [0., 1., 2., 3., 4., 5., 6., 7.], [1.00, 0.72, 0.77, 1.34, 2.15, 3.60, 5.05, 7.50]),\n        # Case 2 (includes negative and near-zero responses, degree 2)\n        (2, [-3., -2., -1., 0., 1., 2., 3.], [-2.10, -1.55, -0.60, -0.20, 0.12, 0.17, 0.11]),\n        # Case 3 (includes an exact zero measurement, degree 2)\n        (2, [0., 1., 2., 3.], [0.00, 0.05, 0.20, 0.45])\n    ]\n\n    results = []\n    for d_val, x_val, y_val in test_cases:\n        beta, S, k_val = solve_case(d_val, x_val, y_val)\n        \n        # Format results as specified: round floats to 6 decimal places.\n        beta_rounded = [round(b, 6) for b in beta]\n        S_rounded = round(S, 6)\n        \n        results.append([beta_rounded, S_rounded, k_val])\n\n    # Print the final result in the exact single-line format with no spaces.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "本练习将带您进入真正的非线性回归领域，其中模型在其参数上是固有非线性的。您将处理一个双室药代动力学模型，并推导其雅可比矩阵——这是优化算法的关键组成部分。 通过实现高斯-牛顿法 (Gauss–Newton method) 的单步迭代，您将深入了解驱动大多数非线性回归求解器的迭代数值技术的工作原理。",
            "id": "2425266",
            "problem": "您的任务是为双室药代动力学浓度-时间模型的非线性最小二乘拟合推导并实现雅可比矩阵。该模型由两个衰减指数项的和给出：对于时间 $t$（单位为小时）和浓度 $C(t)$（单位为毫克/升，mg/L），\n$$\nC(t) = A e^{-\\alpha t} + B e^{-\\beta t},\n$$\n其中 $A$ 和 $B$ 是振幅（单位为毫克/升），$\\alpha$ 和 $\\beta$ 是衰减率（单位为小时的倒数）。从非线性最小二乘法和雅可比矩阵的标准定义出发，推导残差向量关于参数向量 $\\theta = [A, B, \\alpha, \\beta]^T$ 的雅可比矩阵，并在程序中实现它。使用该雅可比矩阵，为一组测试用例执行采用莱文伯格-马夸特阻尼的单步高斯-牛顿法。\n\n您的推导应仅基于以下基本事实：\n- 对于数据 $\\{(t_i, y_i)\\}_{i=1}^N$，残差向量为 $r_i(\\theta) = y_i - C(t_i; \\theta)$。\n- 雅可比矩阵定义为 $J_{ij} = \\frac{\\partial r_i}{\\partial \\theta_j}$。\n- 高斯-牛顿步长 $\\Delta \\theta$ 通过求解线性系统来近似最小化残差平方和\n$$\n\\left(J^T J + \\lambda I\\right) \\Delta \\theta = J^T r,\n$$\n其中 $\\lambda$ 是阻尼参数，$I$ 是单位矩阵。\n\n实现要求：\n1. 从上述定义解析地推导雅可比矩阵。除模型定义外，不要假定任何预先推导的公式。\n2. 实现一个函数来计算 $C(t; \\theta)$ 和关于 $\\theta$ 的残差雅可比矩阵。\n3. 实现一次采用莱文伯格-马夸特阻尼的高斯-牛顿迭代来计算 $\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\Delta \\theta$，使用固定的阻尼 $\\lambda = 10^{-6}$。\n4. 使用以下测试套件。在每种情况下，使用给定的真实参数从模型中精确生成合成观测值 $y_i$（无噪声），然后从给定的初始猜测开始，精确计算一个更新步。所有时间输入单位为小时，所有浓度单位为毫克/升。输出参数估计值 $A$ 和 $B$（单位为毫克/升）以及 $\\alpha$ 和 $\\beta$（单位为小时的倒数）。\n   - 情况1（包含 $t=0$ 的一般情况）：\n     - 时间点：$0, 0.5, 1, 2, 4, 8$。\n     - 真实参数：$A = 5, B = 2, \\alpha = 0.8, \\beta = 0.1$。\n     - 初始猜测：$A = 4.5, B = 1, \\alpha = 0.6, \\beta = 0.2$。\n   - 情况2（近共线指数，测试条件数）：\n     - 时间点：$0, 1, 2, 3, 6$。\n     - 真实参数：$A = 3, B = 3, \\alpha = 0.2, \\beta = 0.2001$。\n     - 初始猜测：$A = 2.5, B = 3.5, \\alpha = 0.22, \\beta = 0.19$。\n   - 情况3（一个振幅等于零）：\n     - 时间点：$0, 1, 3, 5, 10$。\n     - 真实参数：$A = 2, B = 0, \\alpha = 0.5, \\beta = 0.2$。\n     - 初始猜测：$A = 1, B = 0.5, \\alpha = 0.3, \\beta = 0.3$。\n   - 情况4（长时间的缓慢衰减）：\n     - 时间点：$0, 6, 12, 18, 24$。\n     - 真实参数：$A = 1.5, B = 0.5, \\alpha = 0.05, \\beta = 0.01$。\n     - 初始猜测：$A = 1.2, B = 0.4, \\alpha = 0.06, \\beta = 0.008$。\n5. 最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。按顺序连接每个情况的更新后参数，格式为 $[A_1, B_1, \\alpha_1, \\beta_1, A_2, B_2, \\alpha_2, \\beta_2, A_3, B_3, \\alpha_3, \\beta_3, A_4, B_4, \\alpha_4, \\beta_4]$，其中下标表示情况编号。将 $A$ 和 $B$ 以毫克/升表示，将 $\\alpha$ 和 $\\beta$ 以小时的倒数表示。将每个数值打印并四舍五入到六位小数。不涉及角度。最终输出中不出现百分比。",
            "solution": "对问题陈述进行验证。\n\n**步骤1：提取给定信息**\n- 浓度 $C(t)$ 的模型：$C(t) = A e^{-\\alpha t} + B e^{-\\beta t}$。\n- 参数向量：$\\theta = [A, B, \\alpha, \\beta]^T$。\n- 残差向量分量：对于数据 $\\{(t_i, y_i)\\}_{i=1}^N$，$r_i(\\theta) = y_i - C(t_i; \\theta)$。\n- 雅可比矩阵定义：$J_{ij} = \\frac{\\partial r_i}{\\partial \\theta_j}$。\n- 带莱文伯格-马夸特阻尼的高斯-牛顿线性系统：$(J^T J + \\lambda I) \\Delta \\theta = J^T r$。\n- 阻尼参数：$\\lambda = 10^{-6}$。\n- 参数更新规则：$\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\Delta \\theta$。\n- 测试用例：提供了四种情况，每种情况都有一组时间点 $t_i$、用于生成合成数据 $y_i$ 的真实参数，以及一个初始参数猜测 $\\theta_{\\text{old}}$。\n\n**步骤2：使用提取的给定信息进行验证**\n- **科学依据：** 该问题基于双室药代动力学模型，这是药理学中的一个标准模型。求解方法，即带莱文伯格-马夸特阻尼的高斯-牛顿法，是一种用于非线性最小二乘优化的基础且广泛使用的算法。所有定义都是标准且正确的。\n- **适定性：** 该问题是适定的。它要求计算一个迭代算法的单次、明确定义的步骤。所有必要的数据和常数（$\\lambda$、初始猜测、时间点）都已提供。莱文伯格-马夸特修正 $(J^T J + \\lambda I)$ 确保了当 $\\lambda  0$ 时矩阵是正定的，因此是可逆的，从而保证了参数更新 $\\Delta \\theta$ 有唯一解。\n- **客观性：** 该问题使用精确、客观的数学语言陈述。\n- **结论：** 该问题在科学上是合理的、适定的、客观的，并包含足够的信息以获得唯一解。它是有效的。\n\n**步骤3：裁定与行动**\n问题有效。将提供完整的解决方案。\n\n**推导与实现**\n\n目标是为一个非线性最小二乘问题计算单次参数更新，使用带莱文伯格-马夸特阻尼的高斯-牛顿法。这需要对残差的雅可比矩阵进行解析推导。\n\n参数向量为 $\\theta = [\\theta_1, \\theta_2, \\theta_3, \\theta_4]^T = [A, B, \\alpha, \\beta]^T$。\n在时间 $t_i$ 的浓度模型函数为：\n$$C(t_i; \\theta) = A e^{-\\alpha t_i} + B e^{-\\beta t_i}$$\n第 $i$ 个观测值 $(t_i, y_i)$ 的残差为：\n$$r_i(\\theta) = y_i - C(t_i; \\theta)$$\n雅可比矩阵 $J$ 的元素为 $J_{ij} = \\frac{\\partial r_i}{\\partial \\theta_j}$。由于观测数据 $y_i$ 相对于参数 $\\theta$ 是常数，微分仅作用于模型函数：\n$$J_{ij} = \\frac{\\partial}{\\partial \\theta_j} (y_i - C(t_i; \\theta)) = -\\frac{\\partial C(t_i; \\theta)}{\\partial \\theta_j}$$\n雅可比矩阵将有 $N$ 行，对应于 $N$ 个数据点，和 4 列，对应于 $\\theta$ 中的四个参数。我们现在计算 $C(t_i; \\theta)$ 对每个参数的偏导数。\n\n1.  **关于 $A$ ($\\theta_1$) 的偏导数：**\n    $$\\frac{\\partial C(t_i)}{\\partial A} = \\frac{\\partial}{\\partial A} (A e^{-\\alpha t_i} + B e^{-\\beta t_i}) = e^{-\\alpha t_i}$$\n    因此，雅可比矩阵的第一列是 $J_{i1} = -e^{-\\alpha t_i}$。\n\n2.  **关于 $B$ ($\\theta_2$) 的偏导数：**\n    $$\\frac{\\partial C(t_i)}{\\partial B} = \\frac{\\partial}{\\partial B} (A e^{-\\alpha t_i} + B e^{-\\beta t_i}) = e^{-\\beta t_i}$$\n    因此，雅可比矩阵的第二列是 $J_{i2} = -e^{-\\beta t_i}$。\n\n3.  **关于 $\\alpha$ ($\\theta_3$) 的偏导数：**\n    $$\\frac{\\partial C(t_i)}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} (A e^{-\\alpha t_i} + B e^{-\\beta t_i}) = A \\cdot (e^{-\\alpha t_i} \\cdot (-t_i)) = -A t_i e^{-\\alpha t_i}$$\n    因此，雅可比矩阵的第三列是 $J_{i3} = -(-A t_i e^{-\\alpha t_i}) = A t_i e^{-\\alpha t_i}$。\n\n4.  **关于 $\\beta$ ($\\theta_4$) 的偏导数：**\n    $$\\frac{\\partial C(t_i)}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} (A e^{-\\alpha t_i} + B e^{-\\beta t_i}) = B \\cdot (e^{-\\beta t_i} \\cdot (-t_i)) = -B t_i e^{-\\beta t_i}$$\n    因此，雅可比矩阵的第四列是 $J_{i4} = -(-B t_i e^{-\\beta t_i}) = B t_i e^{-\\beta t_i}$。\n\n综合这些结果，雅可比矩阵 $J$ 的第 $i$ 行由下式给出：\n$$[J]_{i,:} = \\begin{bmatrix} -e^{-\\alpha t_i}  -e^{-\\beta t_i}  A t_i e^{-\\alpha t_i}  B t_i e^{-\\beta t_i} \\end{bmatrix}$$\n此雅可比矩阵使用当前的参数估计值 $\\theta_{\\text{old}}$ 进行评估。\n\n对于每个测试用例，高斯-牛顿算法按以下步骤进行：\n1.  使用提供的真实参数 $(\\theta_{\\text{true}})$ 和时间点 $t_i$ 生成合成数据点 $y_i$：$y_i = C(t_i; \\theta_{\\text{true}})$。\n2.  将当前参数 $\\theta$ 设置为初始猜测值 $\\theta_{\\text{old}}$。\n3.  计算残差向量 $r$，其中每个元素为 $r_i = y_i - C(t_i; \\theta_{\\text{old}})$。\n4.  使用推导出的公式计算雅可比矩阵 $J$，在 $\\theta_{\\text{old}}$ 和所有 $t_i$ 处进行评估。\n5.  构建近似Hessian矩阵 $H = J^T J + \\lambda I$，其中 $J^T$ 是 $J$ 的转置，$\\lambda = 10^{-6}$ 是阻尼参数，$I$ 是 $4 \\times 4$ 的单位矩阵。\n6.  构建右侧向量 $g = J^T r$。\n7.  求解正规方程组线性系统 $H \\Delta \\theta = g$，得到参数更新向量 $\\Delta \\theta$。\n8.  计算新的参数估计值 $\\theta_{\\text{new}} = \\theta_{\\text{old}} + \\Delta \\theta$。\n\n该过程将针对问题陈述中指定的四个测试用例中的每一个进行实现和执行。每个案例得到的 $\\theta_{\\text{new}}$ 向量将被连接起来形成最终输出。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements the Jacobian for a two-compartment pharmacokinetic model,\n    and performs a single Gauss-Newton step with Levenberg-Marquardt damping\n    for several test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general case with t=0 included)\n        {\n            \"times\": np.array([0, 0.5, 1, 2, 4, 8], dtype=np.float64),\n            \"true_params\": np.array([5.0, 2.0, 0.8, 0.1], dtype=np.float64),\n            \"initial_guess\": np.array([4.5, 1.0, 0.6, 0.2], dtype=np.float64),\n        },\n        # Case 2 (nearly collinear exponentials, tests conditioning)\n        {\n            \"times\": np.array([0, 1, 2, 3, 6], dtype=np.float64),\n            \"true_params\": np.array([3.0, 3.0, 0.2, 0.2001], dtype=np.float64),\n            \"initial_guess\": np.array([2.5, 3.5, 0.22, 0.19], dtype=np.float64),\n        },\n        # Case 3 (one amplitude equal to zero)\n        {\n            \"times\": np.array([0, 1, 3, 5, 10], dtype=np.float64),\n            \"true_params\": np.array([2.0, 0.0, 0.5, 0.2], dtype=np.float64),\n            \"initial_guess\": np.array([1.0, 0.5, 0.3, 0.3], dtype=np.float64),\n        },\n        # Case 4 (slow decays over long times)\n        {\n            \"times\": np.array([0, 6, 12, 18, 24], dtype=np.float64),\n            \"true_params\": np.array([1.5, 0.5, 0.05, 0.01], dtype=np.float64),\n            \"initial_guess\": np.array([1.2, 0.4, 0.06, 0.008], dtype=np.float64),\n        }\n    ]\n\n    lambda_damping = 1e-6\n    all_results = []\n\n    def model(t, params):\n        \"\"\"Computes C(t) = A*exp(-alpha*t) + B*exp(-beta*t)\"\"\"\n        A, B, alpha, beta = params\n        return A * np.exp(-alpha * t) + B * np.exp(-beta * t)\n\n    def jacobian(t, params):\n        \"\"\"\n        Computes the Jacobian of the residuals vector with respect to the parameters.\n        The i-th row is [d(r_i)/dA, d(r_i)/dB, d(r_i)/d_alpha, d(r_i)/d_beta].\n        Since r_i = y_i - C_i, d(r_i)/d(theta_j) = -d(C_i)/d(theta_j).\n        \"\"\"\n        A, B, alpha, beta = params\n        num_points = len(t)\n        J = np.zeros((num_points, 4), dtype=np.float64)\n\n        exp_alpha_t = np.exp(-alpha * t)\n        exp_beta_t = np.exp(-beta * t)\n\n        # dC/dA = exp(-alpha*t)\n        J[:, 0] = -exp_alpha_t\n        # dC/dB = exp(-beta*t)\n        J[:, 1] = -exp_beta_t\n        # dC/d_alpha = -A*t*exp(-alpha*t)\n        J[:, 2] = -(-A * t * exp_alpha_t)\n        # dC/d_beta = -B*t*exp(-beta*t)\n        J[:, 3] = -(-B * t * exp_beta_t)\n        \n        return J\n\n    for case in test_cases:\n        times = case[\"times\"]\n        true_params = case[\"true_params\"]\n        theta_old = case[\"initial_guess\"]\n\n        # Step 1: Generate synthetic observation data (without noise)\n        y_obs = model(times, true_params)\n\n        # Step 2: Calculate residuals based on the initial guess\n        y_pred = model(times, theta_old)\n        residuals = y_obs - y_pred\n\n        # Step 3: Calculate the Jacobian matrix at the initial guess\n        J = jacobian(times, theta_old)\n\n        # Step 4: Form and solve the Gauss-Newton system with LM damping\n        # (J^T J + lambda I) * delta_theta = J^T * r\n        J_T = J.T\n        J_T_J = J_T @ J\n        \n        # Add damping factor\n        H = J_T_J + lambda_damping * np.identity(4)\n        \n        rhs = J_T @ residuals\n\n        delta_theta = np.linalg.solve(H, rhs)\n\n        # Step 5: Update the parameters for one iteration\n        theta_new = theta_old + delta_theta\n        all_results.extend(theta_new)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.6f}' for x in all_results)}]\")\n\nsolve()\n```"
        }
    ]
}