## 引言
在计算工程领域，我们经常遇到无法用简单线性关系描述的数据。对这些复杂的[非线性](@entry_id:637147)模式进行建模，对于从[材料表征](@entry_id:161346)到系统控制等众多任务都至关重要。虽然[线性回归](@entry_id:142318)是一个基础工具，但它不足以捕捉许多真实世界系统中存在的丰富动态。这为那些需要超越直线拟合的学生和实践者造成了知识上的差距。本文旨在提供一份关于多项式与[非线性回归](@entry_id:178880)技术的综合指南，为您装备有效建模此类复杂性的工具。

本文分为三个章节，旨在引导您从理论走向实践。在第一章“原理与机制”中，我们将深入探讨多项式与[非线性回归](@entry_id:178880)的数学基础，并解决[数值稳定性](@entry_id:146550)、过拟合和[模型选择](@entry_id:155601)等关键挑战。接着，在“应用与跨学科联系”一章中，我们将穿越物理学、[材料科学](@entry_id:152226)和生物学等不同领域，了解这些技术如何被用于提取物理参数、构建代理模型，甚至发现控制方程。最后，“动手实践”章节将通过解决从实现高级算法到拟合复杂[药代动力学模型](@entry_id:264874)的实际问题，来巩固您的理解。让我们开始进入强大的非[线性建模](@entry_id:171589)世界的探索之旅。

## 原理与机制

在超越简单线性模型的探索中，我们经常遇到本质上并非直线的数据关系。多项式与[非线性回归](@entry_id:178880)技术为我们提供了一个强大而灵活的工具箱，用于在计算工程的各个领域中捕捉和建模此类复杂性。本章将深入探讨这些方法背后的核心原理与机制，阐明它们的数学基础、数值挑战以及实际应用中的关键考量。我们将从将线性模型扩展到多项式形式开始，然后深入到更普适的[非线性回归](@entry_id:178880)领域，并最终介绍一些用于模型正则化、选择和贝叶斯推断的先进技术。

### [多项式回归](@entry_id:176102)：扩展线性模型

最直接的非[线性建模](@entry_id:171589)方法之一是将[线性回归](@entry_id:142318)的框架应用于输入变量的多项式特征。一个$d$次[多项式模型](@entry_id:752298)可以表示为：

$$
f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \dots + \beta_d x^d = \sum_{k=0}^{d} \beta_k x^k
$$

尽管该函数相对于输入$x$是[非线性](@entry_id:637147)的，但它相对于系数向量$\boldsymbol{\beta} = [\beta_0, \beta_1, \dots, \beta_d]^T$是线性的。这意味着我们可以利用所有为[线性最小二乘法](@entry_id:165427)开发的工具来求解$\boldsymbol{\beta}$。通过定义一个[特征向量](@entry_id:151813)$\phi(x) = [1, x, x^2, \dots, x^d]^T$，模型可以写为$f(x) = \boldsymbol{\beta}^T \phi(x)$。对于一组$n$个观测数据$\{(x_i, y_i)\}_{i=1}^n$，我们可以构建一个**[设计矩阵](@entry_id:165826)**$\mathbf{\Phi}$，其中每一行是对应于一个数据点的[特征向量](@entry_id:151813)$\phi(x_i)^T$。求解系数就变成了解决线性系统$\min_{\boldsymbol{\beta}} ||\mathbf{y} - \mathbf{\Phi}\boldsymbol{\beta}||_2^2$的问题。

然而，这种方法的简洁性掩盖了几个重要的数值与建模挑战。

#### [数值稳定性](@entry_id:146550)与[正交多项式](@entry_id:146918)

当使用标准的**单项式基**$\{1, x, x^2, \dots, x^d\}$时，尤其是在数据点$x_i$都位于一个狭窄的区间（例如$[0,1]$）内时，[设计矩阵](@entry_id:165826)$\mathbf{\Phi}$（通常称为**范德蒙德矩阵**）的列会变得高度相关。例如，$x^8$和$x^9$在$[0,1]$上的函数形状非常相似。这种**多重共线性**导致[设计矩阵](@entry_id:165826)是**病态的 (ill-conditioned)**。

一个矩阵的[数值稳定性](@entry_id:146550)可以通过其**谱[条件数](@entry_id:145150)**$\kappa_2(\mathbf{\Phi}) = \sigma_{\max}/\sigma_{\min}$来量化，即其最大奇异值与最小[奇异值](@entry_id:152907)之比。一个巨大的条件数意味着对输入数据或$\mathbf{y}$的微小扰动（例如，由于测量噪声）都可能导致系数$\boldsymbol{\beta}$的解发生巨大变化，使得[模型拟合](@entry_id:265652)过程不稳定且结果不可靠。

为了解决这个问题，我们可以采用一组不同的[基函数](@entry_id:170178)，这些[基函数](@entry_id:170178)在其定义域上是**正交的**。例如，**[勒让德多项式](@entry_id:141510) (Legendre polynomials)** $P_k(t)$ 在区间$[-1, 1]$上是正交的。通过首先将输入数据$x_i$仿射变换到区间$[-1, 1]$，然后使用[勒让德多项式](@entry_id:141510)作为[基函数](@entry_id:170178)来构建[设计矩阵](@entry_id:165826)，可以显著改善其条件数。由于[基函数](@entry_id:170178)是正交的，[设计矩阵](@entry_id:165826)的列向量近似正交，从而使得$\mathbf{\Phi}^T\mathbf{\Phi}$接近于[对角矩阵](@entry_id:637782)，这极大地提高了[数值稳定性](@entry_id:146550)。实践证明，使用[正交多项式](@entry_id:146918)基（如[勒让德多项式](@entry_id:141510)）可以将设计矩阵的[条件数](@entry_id:145150)降低几个[数量级](@entry_id:264888)，从而实现更稳健、更精确的参数估计 。

#### [过拟合](@entry_id:139093)与外推的风险

[多项式回归](@entry_id:176102)的另一个主要挑战是**[过拟合](@entry_id:139093)**，特别是当多项式次数$d$较高时。一个高次多项式具有极大的灵活性，可以在训练数据区间内紧密地拟合几乎任何数据点，包括噪声。然而，这种灵活性往往导致模型在未见过的数据上表现不佳，这一现象称为泛化能力差。

这个问题在外推（即在训练数据范围之外进行预测）时尤为严重。非恒定的多项式函数在$|x| \to \infty$时必然趋向于$\pm\infty$。这种行为很少能代表真实的物理过程，这些过程通常会趋于某个饱和值或渐近线。

考虑一个源于物理学的例子：一个热物体在室温环境中冷却 。根据[牛顿冷却定律](@entry_id:142531)，其温度$T(t)$会随时间$t$指数衰减至环境温度$T_\infty$。一个基于该物理定律的简单[非线性模型](@entry_id:276864)，如$T(t) = T_\infty + (T_0 - T_\infty)e^{-kt}$，其结构保证了这种正确的渐近行为。相比之下，如果我们用一个高次多项式去拟合在短时间区间（例如$[0, 10]$分钟）内收集的温度数据，它或许能在该区间内完美拟合数据。但是，当被要求预测$30$分钟时的温度时，该多项式几乎肯定会发散到一个不合物理常理的值。而结构正确的物理模型，即使只估计一个参数$k$，其外推结果也远为可靠和可信。这个例子鲜明地警示我们：模型的结构假设至关重要，盲目增加[多项式模型](@entry_id:752298)的灵活性是危险的。

#### 模型选择：选择正确的复杂性

鉴于高次多项式的风险，如何选择合适的多项式次数$d$或（在多变量情况下）包含哪些项（如$x_1^2, x_1x_2$等）就成了一个核心问题。这本质上是**偏倚-[方差](@entry_id:200758)权衡 (bias-variance tradeoff)** 的一个体现。一个过于简单的模型（低次多项式）可能无法捕捉数据的真实结构（高偏倚），而一个过于复杂的模型（高次多项式）则可能过度拟合噪声（高[方差](@entry_id:200758)）。

自动[模型选择](@entry_id:155601)的一种有效方法是使用**[信息准则](@entry_id:636495)**，它通过惩罚模型的复杂性来平衡[拟合优度](@entry_id:637026)。一个常用的准则是**[贝叶斯信息准则](@entry_id:142416) (Bayesian Information Criterion, BIC)**：

$$
\text{BIC} = n \ln\left(\frac{\text{RSS}}{n}\right) + k \ln(n)
$$

其中，$n$是样本数量，RSS是[残差平方和](@entry_id:174395)， $k$是模型参数的数量（包括截距）。BIC的第一项随着拟合效果的改善而减小，而第二项则随着参数数量$k$的增加而增加。一个好的模型应该能最小化BIC。

为了在众多候选多项式项中找到最优[子集](@entry_id:261956)，我们可以采用**[逐步回归](@entry_id:635129) (stepwise regression)** 等[启发式搜索](@entry_id:637758)策略 。一种典型的**前向-后向逐步选择**算法如下：从一个只含截距的简单模型开始，首先执行一个**前向步骤**，尝试将每一个尚未包含在模型中的候选特征（例如，$x_1^2, x_2^3, x_1x_2$）加入模型，并选择那个能最大程度降低BIC的特征。在成功添加一个特征后，接着执行一个**后向步骤**，尝试移除模型中已有的每一个特征，并选择那个能最大程度降低BIC的移除操作。这个后向移除过程会重复进行，直到没有特征可以被移除以降低BIC为止。整个前向-后向循环会持续进行，直到没有新的特征可以被加入来降低BIC，此时[算法终止](@entry_id:143996)，最终的模型就是被选定的特征组合。这种方法为在巨大的模型空间中进行搜索提供了一个计算上可行的自动化框架。

### 通用[非线性回归](@entry_id:178880)

许多科学和工程模型中的参数并非以[线性形式](@entry_id:276136)出现。一个通用的**[非线性回归](@entry_id:178880)模型**可以写成：

$$
y = f(\boldsymbol{x}; \boldsymbol{\theta}) + \epsilon
$$

其中，模型函数$f$是关于参数向量$\boldsymbol{\theta}$的[非线性](@entry_id:637147)函数。例如，之前提到的冷却模型$T(t) = T_\infty + (T_0 - T_\infty)e^{-kt}$中，参数$k$就出现在指数函数的内部，因此这是一个[非线性模型](@entry_id:276864)。

#### 拟合与计算成本

与[多项式回归](@entry_id:176102)不同，拟合[非线性模型](@entry_id:276864)通常没有封闭解。我们需要使用迭代优化算法来最小化[残差平方和](@entry_id:174395) (Sum of Squared Residuals, SSR)：$\sum_{i=1}^n (y_i - f(\boldsymbol{x}_i; \boldsymbol{\theta}))^2$。

**[高斯-牛顿法](@entry_id:173233) (Gauss-Newton method)** 是一种广泛应用的算法。其核心思想是在当前[参数估计](@entry_id:139349)$\boldsymbol{\theta}_k$的邻域内，用一阶[泰勒展开](@entry_id:145057)来线性化模型函数$f$：

$$
f(\boldsymbol{x}_i; \boldsymbol{\theta}) \approx f(\boldsymbol{x}_i; \boldsymbol{\theta}_k) + \boldsymbol{J}_i (\boldsymbol{\theta} - \boldsymbol{\theta}_k)
$$

其中$\boldsymbol{J}_i$是$f$在$\boldsymbol{x}_i$处对$\boldsymbol{\theta}$的偏导数（[雅可比矩阵](@entry_id:264467)）的行向量。将这个线性近似代入SSR目标函数，最小化问题就转化为一个关于参数更新量$\Delta\boldsymbol{\theta} = \boldsymbol{\theta} - \boldsymbol{\theta}_k$的线性最小二乘问题。求解这个子问题得到$\Delta\boldsymbol{\theta}$，然后更新参数：$\boldsymbol{\theta}_{k+1} = \boldsymbol{\theta}_k + \Delta\boldsymbol{\theta}$。这个过程会不断重复，直到参数收敛。

这种迭代性质使得[非线性回归](@entry_id:178880)的计算成本与线性回归有显著不同 。对于一个有$N$个数据点和$p$个参数的线性（或多项式）回归，其主要计算成本来自于对一个$N \times p$[设计矩阵](@entry_id:165826)进行一次QR分解，成本约为$\mathcal{O}(Np^2)$。而对于一个有$m$个参数的[非线性模型](@entry_id:276864)，若[高斯-牛顿法](@entry_id:173233)需要$T$次迭代才能收敛，则总成本约为$T$乘以单次迭代的成本。由于每次迭代都需要求解一个$N \times m$的线性[最小二乘问题](@entry_id:164198)，其成本为$\mathcal{O}(Nm^2)$，因此总成本约为$\mathcal{O}(T N m^2)$。当参数数量相似时，[非线性回归](@entry_id:178880)的计算开销大致是线性回归的$T$倍，反映了其求解过程的复杂性。

#### 误差模型的重要性：[加性噪声](@entry_id:194447) vs. [乘性噪声](@entry_id:261463)

选择最小化[残差平方和](@entry_id:174395)作为拟合准则，隐式地假设了[测量误差](@entry_id:270998)是**加性的、独立的、且服从均值为零、[方差](@entry_id:200758)恒定的高斯分布**（即**[加性高斯白噪声](@entry_id:269320)**）。也就是说，我们假设真实模型是 $y_i = f(\boldsymbol{x}_i; \boldsymbol{\theta}) + \epsilon_i$，其中$\epsilon_i \sim \mathcal{N}(0, \sigma^2)$。在这种假设下，[最小二乘估计](@entry_id:262764)等价于**最大似然估计 (Maximum Likelihood Estimation, MLE)**。

然而，在许多工程应用中，误差的性质可能并非如此。一个常见的替代模型是**[乘性噪声](@entry_id:261463)**，其中误差的大小与信号本身的强度成正比。例如，一个传感器的[相对误差](@entry_id:147538)可能是恒定的。这可以建模为 $y_i = f(\boldsymbol{x}_i; \boldsymbol{\theta}) \cdot \eta_i$。如果噪声因子$\eta_i$服从对数正态分布，那么取对数后，模型就变成了一个带有加性[高斯噪声](@entry_id:260752)的线性（或线性化）模型：$\ln y_i = \ln f(\boldsymbol{x}_i; \boldsymbol{\theta}) + \varepsilon_i$，其中$\varepsilon_i = \ln \eta_i$服从[高斯分布](@entry_id:154414)。

对这两种误差模型的选择，会导致截然不同的拟合策略和结果 。考虑一个[指数响应](@entry_id:269644)模型 $y(x) = \beta_0 e^{\beta_1 x}$：

*   **直接[非线性最小二乘法](@entry_id:178660)**：直接最小化$\sum(y_i - \beta_0 e^{\beta_1 x_i})^2$。这种方法是[加性噪声](@entry_id:194447)假设下的最大似然估计。它对所有数据点一视同仁地赋权。

*   **[对数变换](@entry_id:267035)法**：对数据取对数，然后对线性化模型 $\ln y_i = \ln\beta_0 + \beta_1 x_i$ 应用[普通最小二乘法](@entry_id:137121)。这种方法是[乘性](@entry_id:187940)对数正态噪声假设下的最大似然估计。在原始尺度上，这种方法等价于一个**加权[非线性](@entry_id:637147)最小二乘**，其中权重与预测值的平方成反比 ($w_i \propto 1/\hat{y}_i^2$)。这给予了$y$值较小（即绝对误差较小）的数据点更大的影响力。

此外，[对数变换](@entry_id:267035)还引入了关于预测的微妙问题。从对数域反变换回原始域（即取指数）得到的预测值$\exp(\widehat{\ln y})$，是对$y$的**条件[中位数](@entry_id:264877)**的无偏估计，而不是对**条件均值**的无偏估计。由于[对数正态分布](@entry_id:261888)是[偏态](@entry_id:178163)的，其均值大于[中位数](@entry_id:264877)。因此，直接取指数会系统性地低估条件均值。要获得无偏的均值估计，需要一个修正因子，如$\exp(\hat{\sigma}^2/2)$。

### 正则化与先进技术

为了进一步提升模型的性能和可靠性，我们可以引入正则化、贝叶斯思想和更灵活的模型结构。

#### 通过正则化控制平滑度

除了通过特征选择来控制模型复杂性，我们还可以通过在[目标函数](@entry_id:267263)中加入一个**正则化项 (regularization term)** 或**惩罚项 (penalty term)** 来约束模型的行为。这种方法允许我们使用高自由度的模型，同时防止其[过拟合](@entry_id:139093)。

一种特别优雅的正则化形式是直接惩罚模型的**平滑度** 。对于一个函数$f(x)$，其曲率可以用[二阶导数](@entry_id:144508)$f''(x)$来衡量。一个“粗糙”或“摆动”的函数会有较大的[二阶导数](@entry_id:144508)值。我们可以通过惩罚$f''(x)$的积分范数来鼓励[模型拟合](@entry_id:265652)出更平滑的曲线。对于定义在$[0,1]$上的[多项式回归](@entry_id:176102)，一个带曲率惩罚的目标函数可以写成：

$$
\Phi(\boldsymbol{\beta}) = \frac{1}{n}\sum_{i=1}^{n}\left(y_i - f(x_i)\right)^2 + \lambda \int_{0}^{1} \left(f''(x)\right)^2 dx
$$

这里的$\lambda \ge 0$是**正则化参数**，它控制了数据拟合项和[平滑度惩罚](@entry_id:754985)项之间的权衡。$\lambda$越大，拟合出的曲线就越平滑。当$f(x)$是多项式时，积分项可以解析地计算出来，它最终是系数向量$\boldsymbol{\beta}$的一个二次型：$\lambda \boldsymbol{\beta}^T \mathbf{S} \boldsymbol{\beta}$，其中$\mathbf{S}$是一个**粗糙度矩阵**。最小化这个带惩罚的目标函数，最终会得到一个修正后的正规方程组：

$$
(\mathbf{\Phi}^T\mathbf{\Phi} + n\lambda\mathbf{S}) \boldsymbol{\beta}^\star = \mathbf{\Phi}^T\mathbf{y}
$$

这是一种**[吉洪诺夫正则化](@entry_id:140094) (Tikhonov regularization)**。通过引入惩罚项，即使原始的$\mathbf{\Phi}^T\mathbf{\Phi}$是病态或奇异的，正则化后的矩阵$(\mathbf{\Phi}^T\mathbf{\Phi} + n\lambda\mathbf{S})$也通常是良态且可逆的，从而保证了[解的唯一性](@entry_id:143619)和稳定性。

#### 使用[广义交叉验证](@entry_id:749781)选择[正则化参数](@entry_id:162917)

[正则化参数](@entry_id:162917)$\lambda$的选择至关重要。一个常用的方法是**[k-折交叉验证](@entry_id:177917) (k-fold cross-validation)**，但它需要将模型重新拟合$k$次，对于大型数据集计算成本高昂。

对于一类被称为**线性[平滑器](@entry_id:636528) (linear smoothers)** 的模型（包括带惩罚的[多项式回归](@entry_id:176102)），存在一种更高效的替代方法，称为**[广义交叉验证](@entry_id:749781) (Generalized Cross-Validation, GCV)** 。对于一个线性[平滑器](@entry_id:636528)，预测向量$\hat{\mathbf{y}}$可以写成$\hat{\mathbf{y}}_\lambda = \mathbf{S}_\lambda \mathbf{y}$，其中$\mathbf{S}_\lambda$是仅依赖于[设计矩阵](@entry_id:165826)和$\lambda$的**平滑矩阵**或**[帽子矩阵](@entry_id:174084)**。GCV是**[留一法交叉验证](@entry_id:637718) (Leave-One-Out Cross-Validation, [LOOCV](@entry_id:637718))** 的一个近似。[LOOCV](@entry_id:637718)的计算成本也很高，但对于线性平滑器，它有一个无需重复拟合的精确公式。GCV通过用平滑矩阵迹的平均值来近似[LOOCV](@entry_id:637718)公式中的个体杠杆值，从而得到了一个计算上极为高效的准则：

$$
\text{GCV}(\lambda) = \frac{ \frac{1}{n} ||\mathbf{y} - \hat{\mathbf{y}}_\lambda||^2 }{ \left( 1 - \frac{1}{n}\mathrm{tr}(\mathbf{S}_\lambda) \right)^2 }
$$

这个表达式只依赖于一次全数据拟合得到的[残差平方和](@entry_id:174395)（$||\mathbf{y} - \hat{\mathbf{y}}_\lambda||^2$）和平滑[矩阵的迹](@entry_id:139694)$\mathrm{tr}(\mathbf{S}_\lambda)$（也称为模型的**[有效自由度](@entry_id:161063)**）。因此，我们可以对一系列候选的$\lambda$值快速计算GCV得分，并选择使GCV得分最小的那个$\lambda$作为最优值。

#### 超越[点估计](@entry_id:174544)：贝叶斯方法

传统的（频率学派）回归方法提供的是参数的[点估计](@entry_id:174544)值$\hat{\boldsymbol{\beta}}$。然而，在许多情况下，我们更希望获得关于[参数不确定性](@entry_id:264387)的完整描述。**贝叶斯回归**通过将参数$\boldsymbol{\beta}$视为[随机变量](@entry_id:195330)，并为其赋予一个完整的[概率分布](@entry_id:146404)来实现这一点。

在贝叶斯[多项式回归](@entry_id:176102)的框架下 ，我们首先为参数$\boldsymbol{\beta}$设定一个**[先验分布](@entry_id:141376)** $p(\boldsymbol{\beta})$，这代表了我们在看到数据之前关于$\boldsymbol{\beta}$的信念。一个常见的选择是[高斯先验](@entry_id:749752)$\boldsymbol{\beta} \sim \mathcal{N}(\boldsymbol{m}_0, \mathbf{S}_0)$。然后，我们结合**似然函数** $p(\mathbf{y}|\boldsymbol{\beta}) \sim \mathcal{N}(\mathbf{\Phi}\boldsymbol{\beta}, \sigma^2 \mathbf{I})$，通过**[贝叶斯定理](@entry_id:151040)**来计算**后验分布** $p(\boldsymbol{\beta}|\mathbf{y})$：

$$
p(\boldsymbol{\beta}|\mathbf{y}) \propto p(\mathbf{y}|\boldsymbol{\beta}) p(\boldsymbol{\beta})
$$

由于[高斯先验](@entry_id:749752)是高斯[似然](@entry_id:167119)的**[共轭先验](@entry_id:262304)**，后验分布也将是一个[高斯分布](@entry_id:154414)$\mathcal{N}(\boldsymbol{m}_N, \mathbf{S}_N)$。其参数可以解析地推导出来：

*   **后验协[方差](@entry_id:200758)**：$S_N = \left(\mathbf{S}_0^{-1} + \frac{1}{\sigma^2}\mathbf{\Phi}^T \mathbf{\Phi}\right)^{-1}$
*   **[后验均值](@entry_id:173826)**：$\boldsymbol{m}_N = \mathbf{S}_N \left(\mathbf{S}_0^{-1} \boldsymbol{m}_0 + \frac{1}{\sigma^2}\mathbf{\Phi}^T \mathbf{y}\right)$

这些公式揭示了贝叶斯学习的本质：后验精度（协[方差](@entry_id:200758)的逆）是先验精度和数据似然精度的加和。[后验均值](@entry_id:173826)是先验均值和数据驱动的估计（即最大似然估计）的一个精度加权平均。最终，我们得到的不是一个单一的$\boldsymbol{\beta}$向量，而是一个完整的[概率分布](@entry_id:146404)，它量化了在给定数据和先验信念后，我们对每个可能$\boldsymbol{\beta}$值的信心程度。

#### 现代灵活模型：[神经网](@entry_id:276355)络与高斯过程

多项式与[非线性回归](@entry_id:178880)的思想在[现代机器学习](@entry_id:637169)中得到了进一步发展和推广。

一个带有单个隐藏层和sigmoid[激活函数](@entry_id:141784)的**[神经网](@entry_id:276355)络**可以被看作是一种特别灵活的[非线性回归](@entry_id:178880)模型 。其模型函数为$f(\boldsymbol{x};\theta) = \boldsymbol{v}^{T}\sigma(\boldsymbol{W}^{T}\boldsymbol{x} + \boldsymbol{b}) + c$。这可以被重新诠释为一种**[基函数](@entry_id:170178)回归**，其中[基函数](@entry_id:170178)的形式为$\phi_j(\boldsymbol{x}) = \sigma(\boldsymbol{w}_j^T \boldsymbol{x} + b_j)$，即一个经过[线性变换](@entry_id:149133)后再通过sigmoid函数激活的“脊[波函数](@entry_id:147440)”。模型的输出是这些[基函数](@entry_id:170178)的[线性组合](@entry_id:154743)。与传统[基函数](@entry_id:170178)回归（如[多项式回归](@entry_id:176102)）的关键区别在于，[神经网](@entry_id:276355)络在训练过程中不仅学习[基函数](@entry_id:170178)的[线性组合](@entry_id:154743)系数（$\boldsymbol{v}, c$），还同时学习[基函数](@entry_id:170178)本身（通过调整$\boldsymbol{W}, \boldsymbol{b}$）。这种自适应学习[基函数](@entry_id:170178)的能力赋予了[神经网](@entry_id:276355)络巨大的灵活性。**[通用近似定理](@entry_id:146978) (Universal Approximation Theorem)** 进一步从理论上保证了，只要隐藏单元足够多，这样的网络就可以任意精度地逼近任何[连续函数](@entry_id:137361)，使其成为一个极其强大的[非线性回归](@entry_id:178880)工具。

**[高斯过程回归](@entry_id:276025) (Gaussian Process Regression, GPR)** 是另一种先进的非参数贝叶斯方法，它直接在函数空间上定义先验 。GPR不仅提供预测值，还提供围绕这些预测值的、经过良好校准的**[不确定性估计](@entry_id:191096)**。与[多项式回归](@entry_id:176102)相比，GPR在[量化不确定性](@entry_id:272064)方面表现出更合理的行为。
*   **内插**：在数据密集的区域，GPR的预测不确定性会显著减小，其不确定性“收缩”的局部化程度由[核函数](@entry_id:145324)的**长度尺度 (length-scale)** 参数$\ell$控制。
*   **外推**：当远离所有训练数据点时，[多项式回归](@entry_id:176102)的预测[方差](@entry_id:200758)会以$|x|^{2d}$的速度无界增长，这通常是不切实际的。相比之下，使用平稳核函数（如[平方指数核](@entry_id:191141)）的GPR的预测不确定性会优雅地“回退”到其先验[方差](@entry_id:200758)。这代表了一种更符合直觉的“承认无知”的方式：在没有数据的地方，模型的不确定性应该回归到其初始的、基于[先验信念](@entry_id:264565)的不确定性水平。

通过这些机制，多项式与[非线性回归](@entry_id:178880)技术为我们提供了一个从简单到复杂、从[点估计](@entry_id:174544)到完整[概率分布](@entry_id:146404)、从参数化模型到灵活[非参数模型](@entry_id:201779)的完整谱系，以应对[计算工程](@entry_id:178146)中遇到的各种[数据建模](@entry_id:141456)挑战。