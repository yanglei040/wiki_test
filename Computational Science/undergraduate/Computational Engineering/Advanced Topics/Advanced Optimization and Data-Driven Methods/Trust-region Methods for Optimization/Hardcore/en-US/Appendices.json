{
    "hands_on_practices": [
        {
            "introduction": "The core of every trust-region iteration is the solution of the subproblem: minimizing a simple quadratic model of our objective function within a given radius of trust. This first exercise  provides a fundamental scenario to practice this key skill. By working through this one-dimensional problem, you will apply the logic of checking the unconstrained model minimizer and then correctly finding the solution on the boundary of the trust region if necessary.",
            "id": "2224504",
            "problem": "In the context of unconstrained optimization, trust-region methods iteratively approximate a complex function with a simpler model function $m(p)$ around the current point. The next step $p$ is then determined by solving the trust-region subproblem, which involves minimizing this model within a \"trust region\" of radius $\\Delta  0$, where the model is believed to be a reliable approximation of the original function. The subproblem is formally stated as:\n$$\n\\min_{p} m(p) \\quad \\text{subject to} \\quad \\|p\\| \\le \\Delta\n$$\nConsider a one-dimensional optimization scenario where the model function for a step $p \\in \\mathbb{R}$ is a quadratic given by:\n$$\nm(p) = g p + \\frac{1}{2} H p^2 + c\n$$\nwith a gradient term $g = 2$, a Hessian term $H = 6$, and an arbitrary constant $c$. The step is constrained by a trust-region radius of $\\Delta = 0.1$.\n\nDetermine the optimal step $p$ that solves this one-dimensional trust-region subproblem. Provide your answer as an exact decimal number.",
            "solution": "We must minimize the quadratic model $m(p) = g p + \\frac{1}{2} H p^{2} + c$ subject to the trust-region constraint $|p| \\le \\Delta$, with given values $g=2$, $H=6$, and $\\Delta=0.1$. The constant $c$ does not affect the minimizer and can be ignored.\n\nConsider the Lagrangian for the trust-region subproblem:\n$$\n\\mathcal{L}(p,\\lambda) = g p + \\frac{1}{2} H p^{2} + \\lambda \\left(p^{2} - \\Delta^{2}\\right),\n$$\nwith $\\lambda \\ge 0$. The Karush-Kuhn-Tucker conditions are:\n1. Stationarity: \n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = g + H p + 2 \\lambda p = 0.\n$$\n2. Primal feasibility: $|p| \\le \\Delta$.\n3. Dual feasibility: $\\lambda \\ge 0$.\n4. Complementary slackness: $\\lambda \\left(p^{2} - \\Delta^{2}\\right) = 0$.\n\nCase 1 (interior solution): If $|p|  \\Delta$, then $\\lambda = 0$ and stationarity gives\n$$\ng + H p = 0 \\quad \\Rightarrow \\quad p_{u} = -\\frac{g}{H}.\n$$\nFor $g=2$ and $H=6$, this gives $p_{u} = -\\frac{2}{6} = -\\frac{1}{3}$. Check feasibility: $|p_{u}| = \\frac{1}{3}  0.1 = \\Delta$, so the interior solution is infeasible.\n\nCase 2 (boundary solution): Then $p^{2} = \\Delta^{2}$, so $p = \\pm \\Delta$. Stationarity becomes\n$$\ng + H p + 2 \\lambda p = 0 \\quad \\Rightarrow \\quad (H + 2 \\lambda) p = -g.\n$$\nSince $H + 2 \\lambda \\ge 0$, the sign of $p$ must match the sign of $-g$. With $g = 2  0$, we must take $p = -\\Delta = -0.1$. To verify dual feasibility, solve for $\\lambda$:\n$$\n(H + 2 \\lambda)(-\\Delta) = -g \\quad \\Rightarrow \\quad (H + 2 \\lambda)\\Delta = g \\quad \\Rightarrow \\quad 2 \\lambda = \\frac{g}{\\Delta} - H.\n$$\nSubstituting $g=2$, $\\Delta=0.1$, and $H=6$ gives\n$$\n2 \\lambda = \\frac{2}{0.1} - 6 = 20 - 6 = 14 \\quad \\Rightarrow \\quad \\lambda = 7 \\ge 0,\n$$\nwhich satisfies dual feasibility and complementary slackness.\n\nTherefore, the optimal trust-region step is the boundary step in the negative gradient direction:\n$$\np^{\\star} = -\\Delta = -0.1.\n$$",
            "answer": "$$\\boxed{-0.1}$$"
        },
        {
            "introduction": "Once a trial step has been computed, a crucial part of the trust-region framework is to assess its quality. This is done by comparing the improvement predicted by the quadratic model with the actual improvement on the true objective function. This exercise  focuses on this evaluation step by asking you to calculate the \"actual reduction,\" a metric that directly influences whether the step is accepted and how the trust radius is adjusted for the next iteration.",
            "id": "2224484",
            "problem": "In the context of unconstrained optimization using a trust-region method, we aim to minimize an objective function $f(x)$. At a given iteration $k$, we are at the point $x_k$ and have computed a trial step $p_k$ by solving the trust-region subproblem. A key metric for evaluating the quality of this step is the actual reduction in the objective function, defined as $\\text{ared}_k = f(x_k) - f(x_k + p_k)$.\n\nConsider the one-dimensional objective function $f(x) = \\cos(x)$. Suppose at the current iteration, we are at the point $x_k = 0$ and the algorithm has proposed a trial step $p_k = 0.1$. Calculate the actual reduction, $\\text{ared}_k$, for this step. All angles are specified in radians. Express your final answer in scientific notation, rounded to three significant figures.",
            "solution": "We use the definition of actual reduction in a trust-region method:\n$$\\text{ared}_{k} = f(x_{k}) - f(x_{k} + p_{k}).$$\nGiven $f(x) = \\cos(x)$, $x_{k} = 0$, and $p_{k} = 0.1$ (radians), we have\n$$\\text{ared}_{k} = \\cos(0) - \\cos(0.1).$$\nSince $\\cos(0) = 1$, this becomes\n$$\\text{ared}_{k} = 1 - \\cos(0.1).$$\nTo evaluate $\\cos(0.1)$, use the Taylor series about $0$:\n$$\\cos(x) = 1 - \\frac{x^{2}}{2} + \\frac{x^{4}}{24} - \\frac{x^{6}}{720} + \\cdots.$$\nSubstituting $x = 0.1$,\n$$\\cos(0.1) \\approx 1 - \\frac{(0.1)^{2}}{2} + \\frac{(0.1)^{4}}{24} - \\frac{(0.1)^{6}}{720} = 1 - 5.0 \\times 10^{-3} + 4.166\\overline{6} \\times 10^{-6} - 1.388\\overline{8} \\times 10^{-9}.$$\nThus,\n$$\\cos(0.1) \\approx 0.9950041653,$$\nand\n$$\\text{ared}_{k} = 1 - \\cos(0.1) \\approx 0.0049958347.$$\nExpressing in scientific notation and rounding to three significant figures gives\n$$\\text{ared}_{k} \\approx 5.00 \\times 10^{-3}.$$",
            "answer": "$$\\boxed{5.00 \\times 10^{-3}}$$"
        },
        {
            "introduction": "Having practiced solving the subproblem and evaluating the step quality, we can now assemble the complete algorithm. This practice  challenges you to implement a full one-dimensional trust-region method, integrating the iterative logic of proposing steps, accepting or rejecting them based on actual and predicted reductions, and dynamically updating the trust radius. Tackling this problem will solidify your understanding of how the components work together to ensure robust convergence, even in challenging situations where simpler methods might fail.",
            "id": "2461247",
            "problem": "Consider the univariate objective function in dimensionless units given by $f(x)=x^4-x^2$ with first derivative $f'(x)=4x^3-2x$ and second derivative $f''(x)=12x^2-2$. The stationary points are the local maxima at $x=0$ and the local minima at $x_{\\pm}=\\pm 1/\\sqrt{2}$. For an iterate $x_k\\in\\mathbb{R}$, define the quadratic model $m_k(s)=f(x_k)+f'(x_k)s+\\tfrac{1}{2}f''(x_k)s^2$ and a trust-region radius $\\Delta_k0$. The trust-region subproblem is to find a step $s_k\\in\\mathbb{R}$ that minimizes $m_k(s)$ subject to the constraint $|s|\\le \\Delta_k$. Let the predicted reduction be $\\text{pred}_k=-(m_k(s_k)-m_k(0))=-(f'(x_k)s_k+\\tfrac{1}{2}f''(x_k)s_k^2)$ and the actual reduction be $\\text{ared}_k=f(x_k)-f(x_k+s_k)$. Define the acceptance ratio $\\rho_k=\\text{ared}_k/\\text{pred}_k$ when $\\text{pred}_k0$; if $\\text{pred}_k\\le 0$, set $\\rho_k=-\\infty$. A step is accepted if $\\rho_k\\ge \\eta_1$ and rejected otherwise. The trust-region radius is updated by the following rule with fixed parameters $\\eta_1\\in(0,1)$, $\\eta_2\\in(\\eta_1,1)$, $\\gamma_1\\in(0,1)$, and $\\gamma_21$:\n- If $\\rho_k\\eta_1$, set $\\Delta_{k+1}=\\gamma_1\\Delta_k$.\n- If $\\rho_k\\ge \\eta_2$ and $|s_k|\\ge 0.8\\Delta_k$, set $\\Delta_{k+1}=\\min\\{\\gamma_2\\Delta_k,\\Delta_{\\max}\\}$.\n- Otherwise, set $\\Delta_{k+1}=\\Delta_k$.\nIf a step is rejected, keep $x_{k+1}=x_k$; if it is accepted, set $x_{k+1}=x_k+s_k$. The iteration terminates when either $|f'(x_k)|\\le \\varepsilon_g$, $|s_k|\\le \\varepsilon_s$ for an accepted step, $\\Delta_k\\le \\varepsilon_s$, or when a fixed iteration cap is reached.\n\nIn one spatial dimension, the unique global minimizer of the quadratic model subject to $|s|\\le \\Delta$ is characterized as follows. For given $g\\in\\mathbb{R}$ and $h\\in\\mathbb{R}$ with $g=f'(x)$ and $h=f''(x)$,\n- If $h0$ and the unconstrained minimizer $s_N=-g/h$ satisfies $|s_N|\\le \\Delta$, then $s^\\star=s_N$; otherwise $s^\\star=-\\text{sign}(g)\\Delta$.\n- If $h\\le 0$, then $s^\\star=\\Delta$ when $g0$, $s^\\star=-\\Delta$ when $g0$, and $s^\\star=\\Delta$ when $g=0$.\n\nFix the numerical parameters $\\eta_1=0.1$, $\\eta_2=0.9$, $\\gamma_1=0.25$, $\\gamma_2=2$, $\\Delta_{\\max}=10$, gradient tolerance $\\varepsilon_g=10^{-8}$, step tolerance $\\varepsilon_s=10^{-10}$, and a maximum of $100$ iterations. For each test case below, start from the given initial point $x_0$ with the given initial radius $\\Delta_0$, and apply the above iteration using the one-dimensional model minimizer $s_k$ at each step.\n\nDefine three scalar outputs for each test case:\n- $b_1$: the logical value that is true if and only if no accepted step ever increases the objective, that is, for all accepted steps $k$, $f(x_{k+1})\\le f(x_k)$.\n- $b_2$: the logical value that is true if and only if the full unconstrained Newton step at the first iterate, $s_N=-f'(x_0)/f''(x_0)$ (when $f''(x_0)\\ne 0$; define $b_2$ as false if $f''(x_0)=0$), produces a higher objective value, that is, $f(x_0+s_N)f(x_0)$.\n- $d$: the absolute distance from the final accepted iterate $x_{\\mathrm{final}}$ to the nearest local minimizer, $d=\\min\\{|x_{\\mathrm{final}}-1/\\sqrt{2}|,|x_{\\mathrm{final}}+1/\\sqrt{2}|\\}$.\n\nTest suite parameters to evaluate:\n1. $(x_0,\\Delta_0)=(0.1,0.05)$\n2. $(x_0,\\Delta_0)=(0.1,2.0)$\n3. $(x_0,\\Delta_0)=(0.0,0.5)$\n4. $(x_0,\\Delta_0)=(0.1,0.001)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a list $[b_1,b_2,d]$ where $b_1$ and $b_2$ are the lowercase strings \"true\" or \"false\", and $d$ is a decimal-rounded float with exactly six digits after the decimal point. The final output must therefore be a single line of the form\n$[[b_{1,1},b_{1,2},d_1],[b_{2,1},b_{2,2},d_2],[b_{3,1},b_{3,2},d_3],[b_{4,1},b_{4,2},d_4]]$,\nwith no spaces anywhere in the line.",
            "solution": "The problem is valid. It presents a well-posed, self-contained, and scientifically sound exercise in numerical optimization, specifically the application of a trust-region algorithm to a one-dimensional potential energy function. All necessary parameters, algorithmic rules, functions, and termination criteria are provided with mathematical precision. We shall proceed with a complete solution.\n\nThe core of this problem is to implement and analyze a trust-region optimization algorithm. This class of methods is fundamental in computational sciences, particularly in computational chemistry for locating stable molecular geometries, which correspond to minima on a potential energy surface. The objective function $f(x) = x^4 - x^2$ is a canonical one-dimensional double-well potential, representing a system with two stable states (minima) at $x_{\\pm} = \\pm 1/\\sqrt{2}$ and an unstable transition state (maximum) at $x=0$.\n\nA trust-region algorithm iteratively finds a minimum by constructing a simpler model of the objective function, which is trusted only within a neighborhood of the current iterate $x_k$. This neighborhood is a \"trust region\" of radius $\\Delta_k$. The model is a quadratic function, $m_k(s)$, derived from a second-order Taylor expansion of $f(x)$ around $x_k$:\n$$m_k(s) = f(x_k) + f'(x_k)s + \\frac{1}{2}f''(x_k)s^2$$\nwhere $s$ is the step from $x_k$. This model is minimized with respect to $s$ subject to the constraint $|s| \\le \\Delta_k$. This constrained minimization is called the trust-region subproblem.\n\nThe solution to the one-dimensional subproblem, as provided, depends on the curvature of the model, given by the second derivative $h = f''(x_k)$.\n1.  If $h  0$, the model is convex (a parabola opening upwards). The unconstrained minimizer is the Newton step $s_N = -g/h$, where $g = f'(x_k)$. If this step lies within the trust region, i.e., $|s_N| \\le \\Delta_k$, it is the optimal step $s_k$. Otherwise, the model is minimized at the boundary of the trust region, $s_k = -\\text{sign}(g)\\Delta_k$, moving as far as possible in the direction of steepest descent.\n2.  If $h \\le 0$, the model is locally concave or linear. Its minimum over the interval $[-\\Delta_k, \\Delta_k]$ must lie at one of the boundaries. The selection between $s = \\Delta_k$ and $s = -\\Delta_k$ is determined by the sign of the gradient $g$, which dictates the direction of descent.\n\nOnce the trial step $s_k$ is computed, its quality is assessed by comparing the *actual reduction* in the objective function, $\\text{ared}_k = f(x_k) - f(x_k + s_k)$, to the *predicted reduction* from the model, $\\text{pred}_k = m_k(0) - m_k(s_k)$. Their ratio, $\\rho_k = \\text{ared}_k / \\text{pred}_k$, measures the fidelity of the model.\n\n-   If $\\rho_k$ is close to $1$, the model is an excellent predictor. The step is accepted, and we may expand the trust region ($\\Delta_{k+1} = \\gamma_2 \\Delta_k$) to allow for more aggressive steps, provided the current step was already near the trust region boundary.\n-   If $\\rho_k$ is positive but not large, the model is adequate. The step is accepted, but the trust region size is maintained ($\\Delta_{k+1} = \\Delta_k$).\n-   If $\\rho_k$ is small or negative, the model is poor. The step is rejected ($x_{k+1} = x_k$), and the trust region is shrunk ($\\Delta_{k+1} = \\gamma_1 \\Delta_k$) to improve model accuracy in the subsequent iteration.\n\nThe step acceptance rule is $\\rho_k \\ge \\eta_1$. Since $\\eta_1 = 0.1  0$ and the solution to the subproblem ensures $\\text{pred}_k \\ge 0$, any accepted step must have $\\text{ared}_k \\ge \\eta_1 \\text{pred}_k \\ge 0$. If $\\text{pred}_k  0$, then $\\text{ared}_k  0$, guaranteeing that $f(x_{k+1})  f(x_k)$. A step can only be accepted if the model predicts descent ($\\text{pred}_k  0$). Therefore, the condition for $b_1$—that no accepted step ever increases the objective function—is guaranteed to be true by the very construction of this algorithm. Any deviation would indicate a flawed implementation.\n\nThe output $b_2$ probes the limitation of the pure Newton-Raphson method. The unconstrained Newton step, $s_N = -f'(x_0)/f''(x_0)$, finds the extremum of the quadratic model. Near a maximum, such as at $x_0=0.0$ or $x_0=0.1$, the Hessian $f''(x_0)$ is negative. The Newton step thus seeks the *maximum* of the local quadratic model, which is a poor strategy for minimizing the global function $f(x)$ and is likely to result in an ascent step, i.e., $f(x_0+s_N)  f(x_0)$. The trust-region framework corrects this deficiency by constraining the step size.\n\nThe final output, $d$, measures the accuracy of convergence to one of the true minimizers, $x_{\\pm} = \\pm 1/\\sqrt{2}$. Given the initial points are all non-negative, the algorithm is expected to converge to the positive minimizer, $x_+ = 1/\\sqrt{2}$.\n\nThe implementation will proceed by first defining the objective function and its derivatives. Then, for each test case, the value of $b_2$ is computed. The main iterative loop is then executed, which involves, at each step $k$: checking for termination, solving the trust-region subproblem for $s_k$, evaluating the step quality via $\\rho_k$, and updating the state variables $x_k$ and $\\Delta_k$ according to the specified rules. The value of $b_1$ is tracked throughout the iteration. Upon termination, the final distance $d$ is computed.",
            "answer": "[[true,true,0.000000],[true,true,0.000000],[true,false,0.000000],[true,true,0.000000]]"
        }
    ]
}