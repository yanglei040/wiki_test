{
    "hands_on_practices": [
        {
            "introduction": "The best way to solidify your understanding of an algorithm is to build it yourself. This first practice challenges you to implement a complete Nonlinear Conjugate Gradient (NCG) solver from scratch, using the robust Polak–Ribière–Polyak (PRP+) update. By applying your solver to a variety of standard benchmark functions, you will gain hands-on experience with the core mechanics of NCG, from calculating gradients to performing a proper line search. ",
            "id": "2418452",
            "problem": "You are given several differentiable objective functions on Euclidean spaces, along with initial points. Your task is to write a complete program that, for each case, computes an approximate minimizer using only function values and exact first derivatives. The gradient that your program uses must be exact within machine precision for the specified objectives; do not use any finite-difference approximations. Do not use any information beyond first derivatives. Use a termination criterion based on the Euclidean norm of the gradient.\n\nMathematical setup:\n\n- Let $f:\\mathbb{R}^n \\to \\mathbb{R}$ be a continuously differentiable function, and let $\\nabla f(\\mathbf{x})$ denote its gradient. Starting from a given initial point $\\mathbf{x}_0 \\in \\mathbb{R}^n$, compute a sequence $\\{\\mathbf{x}_k\\}$ that attempts to minimize $f$ using only evaluations of $f$ and $\\nabla f$.\n- Terminate when $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon$ or when the number of iterations reaches a specified maximum. Use the Euclidean norm for $\\|\\cdot\\|_2$.\n- The algorithm must not use any second-order information (no Hessians or Hessian-vector products) and must not use finite-difference derivative approximations. The program must compute exact gradients of the specified functions within machine precision.\n\nTest suite:\n\nFor each case below, you are given the function $f(\\mathbf{x})$, the dimension $n$, and the initial point $\\mathbf{x}_0$.\n\n- Case A (nonconvex, narrow valley, two variables):\n  - Dimension: $n=2$.\n  - Objective:\n    $$f(\\mathbf{x}) = 100\\,(x_2 - x_1^2)^2 + (1 - x_1)^2.$$\n  - Initial point: $\\mathbf{x}_0 = (-1.2,\\; 1.0)$.\n\n- Case B (ill-conditioned separable quadratic, five variables):\n  - Dimension: $n=5$.\n  - Objective:\n    $$f(\\mathbf{x}) = \\tfrac{1}{2}\\sum_{i=1}^{5} \\lambda_i x_i^2,\\quad \\lambda_i = 10^{\\,i-1}.$$\n  - Initial point: $\\mathbf{x}_0 = (1,\\; -1,\\; 1,\\; -1,\\; 1)$.\n\n- Case C (smooth, coupled, nonconvex, three variables):\n  - Dimension: $n=3$.\n  - Objective:\n    $$f(\\mathbf{x}) = 0.1\\,(x_1^2 + x_2^2 + x_3^2) + \\sin(x_1)\\cos(x_2) + e^{x_3} - x_3.$$\n  - Initial point: $\\mathbf{x}_0 = (0.5,\\; -0.5,\\; 0.0)$.\n\n- Case D (already at the minimizer, four variables):\n  - Dimension: $n=4$.\n  - Objective:\n    $$f(\\mathbf{x}) = \\sum_{i=1}^{4} (x_i - 1)^2.$$\n  - Initial point: $\\mathbf{x}_0 = (1,\\; 1,\\; 1,\\; 1).$\n\nRequirements:\n\n- Stopping tolerance: use $\\varepsilon = 10^{-6}$ on the gradient norm.\n- Maximum number of iterations: use $N_{\\max} = 10000$ per case.\n- The gradient used by your program must be exact within machine precision for the specified objectives.\n- The program must not read any input and must not write any output other than the final line described below.\n\nOutput specification:\n\n- For each case, report the final objective value $f(\\mathbf{x}_\\star)$ at termination, rounded to exactly six digits after the decimal point.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the four rounded objective values for Case A, Case B, Case C, and Case D in that order. For example, a valid output format is:\n  - \"[0.000123,0.000000,1.234567,0.000000]\"\n\nNo physical units are involved. Angles, where used in trigonometric functions, are in radians by mathematical convention. The outputs must be real numbers and must follow the exact format described above.",
            "solution": "The problem posed is a standard numerical optimization task, requiring the minimization of several well-defined, differentiable functions. The method is constrained to be of first-order, meaning it may only use function values, $f(\\mathbf{x})$, and gradient values, $\\nabla f(\\mathbf{x})$. Second-order information, such as the Hessian matrix, is forbidden. Given these constraints, a highly suitable and efficient algorithm is the Nonlinear Conjugate Gradient (CG) method. While the simpler method of steepest descent also satisfies the constraints, its convergence rate is notoriously poor for problems with high curvature or ill-conditioning, such as the Rosenbrock function (Case A) and the given quadratic function with disparate eigenvalues (Case B). The CG method accelerates convergence by constructing search directions that are a conjugate-like extension of the gradients, effectively incorporating information from previous steps.\n\nThe iterative procedure for the Nonlinear CG algorithm, starting from an initial point $\\mathbf{x}_0$, is defined for $k=0, 1, 2, \\dots$ as follows:\n1. Compute the gradient at the current iterate: $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$.\n2. Check for convergence: if the Euclidean norm of the gradient $\\|\\mathbf{g}_k\\|_2$ is below a specified tolerance $\\varepsilon$, the algorithm terminates.\n3. Compute the search direction $\\mathbf{p}_k$. For the first iteration ($k=0$), the direction is the steepest descent direction, $\\mathbf{p}_0 = -\\mathbf{g}_0$. For subsequent iterations ($k  0$), the direction is a linear combination of the current negative gradient and the previous search direction:\n   $$\n   \\mathbf{p}_k = -\\mathbf{g}_k + \\beta_k \\mathbf{p}_{k-1}\n   $$\n   The scalar $\\beta_k$ determines the specific variant of the CG method.\n4. Perform a line search to determine an appropriate step size $\\alpha_k  0$ along the direction $\\mathbf{p}_k$.\n5. Update the iterate: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$.\n\nFor this implementation, the Polak–Ribière–Polyak (PRP) formula is selected for $\\beta_k$ due to its generally superior empirical performance compared to other formulas like Fletcher-Reeves. The PRP formula is:\n$$\n\\beta_k^{\\text{PRP}} = \\frac{\\mathbf{g}_k^T (\\mathbf{g}_k - \\mathbf{g}_{k-1})}{\\mathbf{g}_{k-1}^T \\mathbf{g}_{k-1}}\n$$\nTo improve the robustness and guarantee global convergence properties, this is augmented into the PRP+ method, where $\\beta_k = \\max(0, \\beta_k^{\\text{PRP}})$. This modification prevents the algorithm from taking poor steps if $\\beta_k^{\\text{PRP}}$ becomes negative, which can occur far from a local minimum. Additionally, as a safeguard, the search direction $\\mathbf{p}_k$ is forcibly reset to the steepest descent direction $-\\mathbf{g}_k$ if it ceases to be a descent direction (i.e., if $\\mathbf{p}_k^T \\mathbf{g}_k \\ge 0$).\n\nThe step size $\\alpha_k$ is found using a line search that satisfies the strong Wolfe conditions:\n1. Sufficient Decrease (Armijo) Condition: $f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha_k \\mathbf{g}_k^T \\mathbf{p}_k$\n2. Curvature Condition: $|\\nabla f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k)^T \\mathbf{p}_k| \\le c_2 |\\mathbf{g}_k^T \\mathbf{p}_k|$\nThe constants are chosen as standard values $c_1 = 10^{-4}$ and $c_2 = 0.9$. These conditions ensure that each step achieves a meaningful reduction in the objective function value. The `scipy.optimize.line_search` function is used to implement this step.\n\nThe analytical gradients are required to be exact. The gradients for the four test cases are derived as follows:\n\nCase A: Rosenbrock function, $f(\\mathbf{x}) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2$ for $\\mathbf{x} \\in \\mathbb{R}^2$.\n$$\n\\nabla f(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} -400x_1(x_2 - x_1^2) - 2(1 - x_1) \\\\ 200(x_2 - x_1^2) \\end{pmatrix}\n$$\n\nCase B: Ill-conditioned separable quadratic, $f(\\mathbf{x}) = \\frac{1}{2}\\sum_{i=1}^{5} \\lambda_i x_i^2$ with $\\lambda_i = 10^{i-1}$ for $\\mathbf{x} \\in \\mathbb{R}^5$.\nThe gradient component for each $x_j$ is $\\frac{\\partial f}{\\partial x_j} = \\lambda_j x_j$.\n$$\n\\nabla f(\\mathbf{x})_j = 10^{j-1} x_j, \\quad \\text{for } j=1, \\dots, 5\n$$\n\nCase C: Smooth, coupled, nonconvex function, $f(\\mathbf{x}) = 0.1(x_1^2 + x_2^2 + x_3^2) + \\sin(x_1)\\cos(x_2) + e^{x_3} - x_3$ for $\\mathbf{x} \\in \\mathbb{R}^3$.\n$$\n\\nabla f(\\mathbf{x}) = \\begin{pmatrix} 0.2x_1 + \\cos(x_1)\\cos(x_2) \\\\ 0.2x_2 - \\sin(x_1)\\sin(x_2) \\\\ 0.2x_3 + e^{x_3} - 1 \\end{pmatrix}\n$$\n\nCase D: Simple quadratic, $f(\\mathbf{x}) = \\sum_{i=1}^{4} (x_i - 1)^2$ for $\\mathbf{x} \\in \\mathbb{R}^4$.\nThe gradient component for each $x_j$ is $\\frac{\\partial f}{\\partial x_j} = 2(x_j-1)$.\n$$\n\\nabla f(\\mathbf{x})_j = 2(x_j - 1), \\quad \\text{for } j=1, \\dots, 4\n$$\nFor this case, the initial point $\\mathbf{x}_0 = (1, 1, 1, 1)$ is the function's unique global minimum. Therefore, $\\nabla f(\\mathbf{x}_0) = \\mathbf{0}$, and the algorithm terminates immediately at iteration $k=0$ with an objective value of $f(\\mathbf{x}_0)=0$.\n\nThe implementation combines these elements into a single program. A general solver function encapsulates the CG logic, and it is called for each test case with the respective objective function, gradient, and initial point. The program terminates when $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le 10^{-6}$ or after $10000$ iterations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\n\ndef conjugate_gradient_solver(f, grad_f, x0, tol=1e-6, max_iter=10000):\n    \"\"\"\n    Minimizes a function using the Nonlinear Conjugate Gradient method (Polak-Ribière-Polyak+).\n    \"\"\"\n    x_k = np.array(x0, dtype=float)\n    f_k = f(x_k)\n    g_k = grad_f(x_k)\n    grad_norm = np.linalg.norm(g_k)\n\n    if grad_norm = tol:\n        return f(x_k)\n\n    p_k = -g_k\n    \n    k = 0\n    while k  max_iter and grad_norm  tol:\n        # Perform line search to find alpha_k satisfying strong Wolfe conditions.\n        # c1=1e-4 and c2=0.9 are standard for CG.\n        try:\n            line_search_result = optimize.line_search(f, grad_f, x_k, p_k, gfk=g_k, old_fval=f_k, c1=1e-4, c2=0.9)\n            alpha_k = line_search_result[0]\n        except Exception:\n            # line_search can sometimes raise errors for extreme values\n            alpha_k = None\n\n        # If line search fails, restart with steepest descent.\n        if alpha_k is None:\n            p_k = -g_k\n            try:\n                line_search_result = optimize.line_search(f, grad_f, x_k, p_k, gfk=g_k, old_fval=f_k, c1=1e-4, c2=0.9)\n                alpha_k = line_search_result[0]\n            except Exception:\n                alpha_k = None\n            \n            if alpha_k is None:\n                # If it still fails, terminate. Could be due to precision limits.\n                break\n\n        x_k_plus_1 = x_k + alpha_k * p_k\n        g_k_plus_1 = grad_f(x_k_plus_1)\n\n        # Polak-Ribière-Polyak+ update for beta\n        g_k_dot_g_k = np.dot(g_k, g_k)\n        if g_k_dot_g_k == 0:\n            beta_k_plus_1 = 0.0\n        else:\n            beta_numerator = np.dot(g_k_plus_1, g_k_plus_1 - g_k)\n            beta_k_plus_1 = max(0, beta_numerator / g_k_dot_g_k)\n        \n        # New search direction\n        p_k_plus_1 = -g_k_plus_1 + beta_k_plus_1 * p_k\n\n        # Check for descent direction. If not, reset to steepest descent.\n        if np.dot(p_k_plus_1, g_k_plus_1) = 0:\n            p_k_plus_1 = -g_k_plus_1\n\n        # Update variables for the next iteration\n        x_k = x_k_plus_1\n        g_k = g_k_plus_1\n        p_k = p_k_plus_1\n        f_k = f(x_k) # Can be taken from line_search output, but re-evaluating is simple.\n        \n        grad_norm = np.linalg.norm(g_k)\n        k += 1\n\n    return f(x_k)\n\ndef solve():\n    # Final print statement in the exact required format.\n    \n    # Case A: Rosenbrock function\n    def f_A(x):\n        return 100.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2\n\n    def grad_f_A(x):\n        df_dx1 = -400.0 * x[0] * (x[1] - x[0]**2) - 2.0 * (1.0 - x[0])\n        df_dx2 = 200.0 * (x[1] - x[0]**2)\n        return np.array([df_dx1, df_dx2])\n\n    # Case B: Ill-conditioned separable quadratic\n    def f_B(x):\n        lambdas = 10.**np.arange(len(x))\n        return 0.5 * np.sum(lambdas * x**2)\n\n    def grad_f_B(x):\n        lambdas = 10.**np.arange(len(x))\n        return lambdas * x\n\n    # Case C: Smooth, coupled, nonconvex\n    def f_C(x):\n        term1 = 0.1 * np.sum(x**2)\n        term2 = np.sin(x[0]) * np.cos(x[1])\n        term3 = np.exp(x[2]) - x[2]\n        return term1 + term2 + term3\n\n    def grad_f_C(x):\n        df_dx1 = 0.2 * x[0] + np.cos(x[0]) * np.cos(x[1])\n        df_dx2 = 0.2 * x[1] - np.sin(x[0]) * np.sin(x[1])\n        df_dx3 = 0.2 * x[2] + np.exp(x[2]) - 1.0\n        return np.array([df_dx1, df_dx2, df_dx3])\n    \n    # Case D: Simple quadratic\n    def f_D(x):\n        return np.sum((x - 1.0)**2)\n\n    def grad_f_D(x):\n        return 2.0 * (x - 1.0)\n    \n    test_cases = [\n        {'f': f_A, 'grad_f': grad_f_A, 'x0': [-1.2, 1.0]},\n        {'f': f_B, 'grad_f': grad_f_B, 'x0': [1.0, -1.0, 1.0, -1.0, 1.0]},\n        {'f': f_C, 'grad_f': grad_f_C, 'x0': [0.5, -0.5, 0.0]},\n        {'f': f_D, 'grad_f': grad_f_D, 'x0': [1.0, 1.0, 1.0, 1.0]}\n    ]\n\n    results = []\n    for case in test_cases:\n        final_f_val = conjugate_gradient_solver(\n            f=case['f'],\n            grad_f=case['grad_f'],\n            x0=case['x0'],\n            tol=1e-6,\n            max_iter=10000\n        )\n        results.append(final_f_val)\n\n    # Format output as specified\n    formatted_results = [\"{:.6f}\".format(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A robust algorithm is not just about what it does, but also about the safeguards that prevent it from failing. This exercise provides a crucial lesson in algorithmic stability by asking you to compare a proper NCG solver against an intentionally flawed version that omits the Armijo sufficient decrease condition. Through carefully chosen examples, you will see firsthand why a backtracking line search is not an optional tweak but an essential component for guaranteeing convergence. ",
            "id": "2418455",
            "problem": "You are asked to implement and compare two variants of the nonlinear conjugate gradient method for unconstrained minimization of a continuously differentiable function. The first variant is a proper nonlinear conjugate gradient algorithm that enforces the first Wolfe condition (Armijo sufficient decrease) via backtracking line search. The second variant is intentionally faulty: it does not enforce the Armijo condition and instead uses a fixed unit step length at every iteration. Your task is to demonstrate, through carefully chosen test functions, that omitting the Armijo condition can lead to nonconvergence or divergence, even when a proper method converges.\n\nStart from the following foundational base:\n- For an objective function $f:\\mathbb{R}^n \\to \\mathbb{R}$ that is continuously differentiable, a descent method generates iterates $x_{k+1} = x_k + \\alpha_k d_k$ with search direction $d_k$ satisfying $g_k^\\top d_k  0$, where $g_k = \\nabla f(x_k)$ and $\\alpha_k  0$ is a step length.\n- The first Wolfe (Armijo) sufficient decrease condition requires that, for constants $c_1 \\in (0,1)$, the step length satisfies\n$$\nf(x_k + \\alpha_k d_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top d_k.\n$$\n- In a nonlinear conjugate gradient method, the search directions are constructed by\n$$\nd_0 = -g_0,\\quad d_k = -g_k + \\beta_k d_{k-1}\\ \\text{for}\\ k \\ge 1,\n$$\nwith a classical choice of $\\beta_k$ such as Polak–Ribiere–Plus, and with a safeguard to reset $d_k=-g_k$ if $g_k^\\top d_k \\ge 0$ to maintain descent.\n- For smooth convex quadratic objectives of the form $f(x) = \\tfrac{1}{2} x^\\top Q x$ with a symmetric positive-definite matrix $Q$, gradient descent with a fixed step $\\alpha$ yields the linear iteration $x_{k+1} = (I - \\alpha Q) x_k$. Convergence to the minimizer $x^\\star=0$ occurs if and only if the spectral radius satisfies $\\rho(I - \\alpha Q)  1$, equivalently $0  \\alpha  2/\\lambda_{\\max}(Q)$.\n\nYour program must implement:\n- A proper nonlinear conjugate gradient solver that uses a backtracking line search enforcing the Armijo condition with user-chosen constants $c_1 \\in (0,1)$ and backtracking ratio $\\tau \\in (0,1)$.\n- A faulty nonlinear conjugate gradient solver that uses $\\alpha_k \\equiv 1$ for all $k$ (no sufficient decrease check).\n\nDesign details you must adhere to:\n- Use the Polak–Ribiere–Plus choice for the conjugacy parameter $\\beta_k$ and a reset safeguard if the computed direction is not a descent direction.\n- Terminate when $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$ for a given tolerance $\\varepsilon  0$, or when a fixed iteration budget is exhausted.\n- Declare nonconvergence if the iteration budget is exhausted without meeting the gradient tolerance. Declare divergence if the objective value becomes not-a-number, exceeds a large threshold, or the iterate norm exceeds a large threshold.\n\nConstruct a test suite that demonstrates different behaviors:\n- Test A (divergence witness): a convex quadratic $f(x) = \\tfrac{1}{2} x^\\top Q x$ with $Q = \\mathrm{diag}(10.0, 0.1)$ and initial point $x_0 = [1.0, 1.0]$. By the spectral-radius criterion, a fixed step $\\alpha = 1$ violates $0  \\alpha  2/\\lambda_{\\max}(Q)$ because $\\lambda_{\\max}(Q) = 10.0$, and therefore the faulty method is expected to diverge, while the Armijo-enforced method should converge.\n- Test B (happy path): a well-conditioned convex quadratic with $Q = \\mathrm{diag}(0.5, 0.25)$ and $x_0 = [2.0, -3.0]$. Here $\\lambda_{\\max}(Q) = 0.5$, so a fixed step $\\alpha = 1$ satisfies $0  \\alpha  2/\\lambda_{\\max}(Q)$ and both methods should converge.\n- Test C (boundary condition): the same convex quadratic $Q = \\mathrm{diag}(1.0, 1.0)$ with $x_0 = [0.0, 0.0]$, which is already a minimizer. Both methods should detect convergence immediately.\n- Test D (nonconvex stress test): a scaled Rosenbrock function $f(x_1,x_2) = 10\\,(x_2 - x_1^2)^2 + (1 - x_1)^2$ with $x_0 = [-1.2, 1.0]$. The faulty method’s unit steps can cause blow-up on this curved valley, while the proper method with Armijo backtracking should converge to the minimizer near $[1,1]$.\n\nNumerical parameters to use in your program:\n- Gradient norm tolerance $\\varepsilon = 10^{-6}$, maximum iterations $N_{\\max} = 5000$, Armijo constant $c_1 = 10^{-4}$, backtracking ratio $\\tau = 0.5$.\n- Divergence thresholds: declare divergence if $\\|x_k\\|_2  10^{12}$ or $f(x_k)  10^{50}$ or $f(x_k)$ is not-a-number.\n\nYour program must:\n- Implement both solvers, run them on all four tests, and determine, for each test, an integer code based on the observed behavior:\n    - Output $1$ if the proper method converges and the faulty method does not converge (either nonconvergent or divergent).\n    - Output $0$ if both converge.\n    - Output $-1$ if neither converges.\n    - Output $2$ if the faulty method converges and the proper method does not.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For example, the output format must look like a single Python-style list literal such as [r1,r2,r3,r4], where each entry is one of the specified integer codes.\n\nAngle units are not involved. There are no physical units in this problem. All quantities are dimensionless. The output must strictly follow the specified single-line format.",
            "solution": "The problem posed is the implementation and comparative analysis of two variants of the nonlinear conjugate gradient (NCG) method for unconstrained optimization. One variant is correctly implemented, adhering to the fundamental principles of line search methods by enforcing the Armijo sufficient decrease condition. The second variant is intentionally flawed, employing a fixed unit step length, thereby omitting the crucial safeguard of a line search. The objective is to demonstrate computationally that the omission of the Armijo condition can lead to failure, specifically non-convergence or divergence, on problems where a correctly implemented algorithm succeeds. The problem is well-posed, scientifically sound, and provides a clear basis for algorithmic implementation and verification.\n\nA general unconstrained minimization problem is considered, seeking to find a local minimizer of a continuously differentiable objective function $f: \\mathbb{R}^n \\to \\mathbb{R}$. The NCG method is an iterative algorithm that generates a sequence of points $\\{x_k\\}_{k \\ge 0}$ using the update rule:\n$$\nx_{k+1} = x_k + \\alpha_k d_k\n$$\nHere, $x_k \\in \\mathbb{R}^n$ is the current iterate, $d_k \\in \\mathbb{R}^n$ is the search direction, and $\\alpha_k  0$ is the step length. The gradient of the objective function at $x_k$ is denoted by $g_k = \\nabla f(x_k)$.\n\nThe search direction $d_k$ is constructed to be a descent direction, meaning $g_k^\\top d_k  0$. The NCG directions are defined recursively. The initial direction is the steepest descent direction, $d_0 = -g_0$. For subsequent iterations $k \\ge 1$, the direction is a linear combination of the current negative gradient and the previous direction:\n$$\nd_k = -g_k + \\beta_k d_{k-1}\n$$\nThe scalar $\\beta_k$ is the conjugacy parameter. The problem specifies the Polak–Ribière–Plus variant, which is known for its strong numerical performance. It is defined as:\n$$\n\\beta_k = \\max \\left\\{ 0, \\frac{g_k^\\top(g_k - g_{k-1})}{\\|g_{k-1}\\|_2^2} \\right\\}\n$$\nThis choice incorporates a non-negativity constraint, which helps in ensuring global convergence under certain conditions. A vital safeguard is the reset condition: if the computed direction $d_k$ fails to be a descent direction (i.e., if $g_k^\\top d_k \\ge 0$), the method is reset by setting the search direction to that of steepest descent, $d_k = -g_k$.\n\nThe core of this investigation lies in the determination of the step length $\\alpha_k$.\n\nThe **proper NCG method** employs a backtracking line search to find a step length $\\alpha_k$ that satisfies the Armijo sufficient decrease condition. For a given constant $c_1 \\in (0, 1)$, this condition is:\n$$\nf(x_k + \\alpha_k d_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top d_k\n$$\nThis inequality ensures that the reduction in the objective function is at least a fraction of the decrease predicted by the linear approximation of $f$ at $x_k$. The backtracking procedure starts with an initial trial step, typically $\\alpha = 1$, and iteratively reduces it by a factor $\\tau \\in (0, 1)$ (e.g., $\\alpha \\leftarrow \\tau \\alpha$) until the condition is met. The parameters specified are $c_1 = 10^{-4}$ and $\\tau = 0.5$.\n\nThe **faulty NCG method** bypasses this critical check and naively sets $\\alpha_k = 1$ for all iterations $k \\ge 0$. While this may be acceptable for certain well-behaved functions or if the initial iterate is close to the solution, it is generally an unreliable strategy that can lead to failure.\n\nTermination of the algorithm is dictated by the norm of the gradient. The iteration is considered converged if $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$ for a tolerance $\\varepsilon = 10^{-6}$. The process is aborted if the number of iterations exceeds a budget $N_{\\max} = 5000$, which is classified as non-convergence. Divergence is declared if the iterate norm $\\|x_k\\|_2$ exceeds $10^{12}$, the function value $f(x_k)$ exceeds $10^{50}$, or if $f(x_k)$ becomes not-a-number (NaN).\n\nThe analysis is performed on a suite of four test cases designed to expose the differing behaviors of the two methods.\n\n**Test A**: A convex quadratic function $f(x) = \\frac{1}{2} x^\\top Q x$ with a poorly conditioned Hessian matrix $Q = \\mathrm{diag}(10.0, 0.1)$. For a quadratic function, the NCG method with a fixed step $\\alpha$ is equivalent to the linear iterative system $x_{k+1} = (I - \\alpha Q) x_k$. This system converges if and only if the spectral radius of the iteration matrix, $\\rho(I - \\alpha Q)$, is less than $1$. With $\\alpha=1$, the eigenvalues of $I-Q$ are $1-10.0 = -9.0$ and $1-0.1 = 0.9$. The spectral radius is $\\rho(I - Q) = \\max\\{|-9.0|, |0.9|\\} = 9.0$, which is greater than $1$. The faulty method is therefore guaranteed to diverge. The proper method, with its adaptive step length from the Armijo condition, is expected to converge.\n\n**Test B**: A well-conditioned convex quadratic function with $Q = \\mathrm{diag}(0.5, 0.25)$. Here, for the faulty method with $\\alpha=1$, the eigenvalues of $I-Q$ are $1-0.5 = 0.5$ and $1-0.25=0.75$. The spectral radius is $\\rho(I - Q) = 0.75  1$, satisfying the convergence condition. Thus, both the faulty and proper methods are expected to converge.\n\n**Test C**: A convex quadratic with $x_0 = [0.0, 0.0]$, which is the global minimum. The initial gradient is $\\nabla f(x_0) = 0$. Both algorithms must check the termination condition before the first iteration and declare convergence immediately.\n\n**Test D**: The nonconvex Rosenbrock function, $f(x_1,x_2) = 10(x_2 - x_1^2)^2 + (1-x_1)^2$, from the starting point $x_0 = [-1.2, 1.0]$. This function is a classic benchmark characterized by a narrow, curved valley. The fixed unit step of the faulty method is likely to cause the iterates to \"jump\" over the valley, leading to an increase in the function value and erratic behavior, likely causing divergence or non-convergence. In contrast, the backtracking line search of the proper method will systematically reduce the step length to ensure sufficient decrease, allowing the iterates to follow the valley toward the minimum at $[1,1]$.\n\nThe outcome for each test is an integer code: $1$ if the proper method converges and the faulty one does not; $0$ if both converge; $-1$ if neither converges; and $2$ if the faulty method converges but the proper one does not. This systematic comparison provides clear evidence for the indispensable role of the line search mechanism in ensuring the robustness of descent-based optimization algorithms.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares a proper and a faulty nonlinear conjugate gradient method.\n    \"\"\"\n\n    # --- Numerical Parameters ---\n    EPSILON = 1e-6\n    MAX_ITER = 5000\n    C1 = 1e-4\n    TAU = 0.5\n    DIV_NORM_THRESHOLD = 1e12\n    DIV_F_THRESHOLD = 1e50\n\n    def nonlinear_cg(f, grad_f, x0, use_armijo):\n        \"\"\"\n        Nonlinear Conjugate Gradient (NCG) solver.\n\n        Args:\n            f: Objective function.\n            grad_f: Gradient of the objective function.\n            x0: Initial point.\n            use_armijo: Boolean flag to use Armijo line search.\n\n        Returns:\n            A string indicating the outcome: \"converged\", \"nonconverged\", \"diverged\".\n        \"\"\"\n        x_k = np.copy(x0).astype(np.float64)\n        g_k = grad_f(x_k)\n        \n        # Initial check for convergence at x0\n        norm_g_k = np.linalg.norm(g_k)\n        if norm_g_k = EPSILON:\n            return \"converged\"\n\n        d_k = -g_k\n        k = 0\n\n        while k  MAX_ITER:\n            # Line Search\n            if use_armijo:\n                alpha_k = 1.0\n                descent_condition_val = C1 * np.dot(g_k, d_k)\n                # The dot product g_k.T @ d_k should be  0 due to safeguard\n                try:\n                    f_k = f(x_k)\n                    while f(x_k + alpha_k * d_k)  f_k + alpha_k * descent_condition_val:\n                        alpha_k *= TAU\n                        if alpha_k  1e-15: # Prevent infinite loop if step size becomes too small\n                           return \"nonconverged\"\n                except (OverflowError, ValueError):\n                    return \"diverged\" # f() evaluation might fail\n            else:\n                alpha_k = 1.0\n\n            # Update position\n            x_k_plus_1 = x_k + alpha_k * d_k\n\n            # Check for divergence\n            try:\n                f_next = f(x_k_plus_1)\n                if np.linalg.norm(x_k_plus_1)  DIV_NORM_THRESHOLD or f_next  DIV_F_THRESHOLD or np.isnan(f_next):\n                    return \"diverged\"\n            except (OverflowError, ValueError):\n                return \"diverged\"\n\n            g_k_plus_1 = grad_f(x_k_plus_1)\n            norm_g_k_plus_1 = np.linalg.norm(g_k_plus_1)\n\n            # Check for convergence\n            if norm_g_k_plus_1 = EPSILON:\n                return \"converged\"\n\n            # Polak-Ribiere-Plus (PR+) for beta\n            norm_g_k_sq = norm_g_k**2\n            if norm_g_k_sq  1e-14: # Safeguard against division by zero\n                beta_k_plus_1 = max(0, np.dot(g_k_plus_1, g_k_plus_1 - g_k) / norm_g_k_sq)\n            else:\n                beta_k_plus_1 = 0.0\n\n            # Update search direction\n            d_k_plus_1 = -g_k_plus_1 + beta_k_plus_1 * d_k\n\n            # Restart if not a descent direction\n            if np.dot(g_k_plus_1, d_k_plus_1) = 0:\n                d_k_plus_1 = -g_k_plus_1\n\n            # Prepare for next iteration\n            x_k = x_k_plus_1\n            g_k = g_k_plus_1\n            norm_g_k = norm_g_k_plus_1\n            d_k = d_k_plus_1\n            k += 1\n\n        return \"nonconverged\"\n    \n    # --- Test Case Definitions ---\n\n    # Test A: Divergence Witness\n    Q_A = np.diag([10.0, 0.1])\n    def f_A(x): return 0.5 * x.T @ Q_A @ x\n    def grad_f_A(x): return Q_A @ x\n    x0_A = np.array([1.0, 1.0])\n\n    # Test B: Happy Path\n    Q_B = np.diag([0.5, 0.25])\n    def f_B(x): return 0.5 * x.T @ Q_B @ x\n    def grad_f_B(x): return Q_B @ x\n    x0_B = np.array([2.0, -3.0])\n\n    # Test C: Boundary Condition\n    Q_C = np.diag([1.0, 1.0])\n    def f_C(x): return 0.5 * x.T @ Q_C @ x\n    def grad_f_C(x): return Q_C @ x\n    x0_C = np.array([0.0, 0.0])\n\n    # Test D: Nonconvex Stress Test (Scaled Rosenbrock)\n    def f_D(x): return 10.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2\n    def grad_f_D(x):\n        df_dx1 = -40.0 * x[0] * (x[1] - x[0]**2) - 2.0 * (1.0 - x[0])\n        df_dx2 = 20.0 * (x[1] - x[0]**2)\n        return np.array([df_dx1, df_dx2])\n    x0_D = np.array([-1.2, 1.0])\n\n    test_cases = [\n        (f_A, grad_f_A, x0_A),\n        (f_B, grad_f_B, x0_B),\n        (f_C, grad_f_C, x0_C),\n        (f_D, grad_f_D, x0_D),\n    ]\n\n    results = []\n    for f, grad_f, x0 in test_cases:\n        proper_status = nonlinear_cg(f, grad_f, x0, use_armijo=True)\n        faulty_status = nonlinear_cg(f, grad_f, x0, use_armijo=False)\n\n        proper_converged = (proper_status == \"converged\")\n        faulty_converged = (faulty_status == \"converged\")\n\n        if proper_converged and not faulty_converged:\n            results.append(1)\n        elif proper_converged and faulty_converged:\n            results.append(0)\n        elif not proper_converged and not faulty_converged:\n            results.append(-1)\n        elif not proper_converged and faulty_converged:\n            results.append(2)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The efficiency of the conjugate gradient method lies in its intelligent construction of search directions. This practice dives deep into the heart of the algorithm: the $\\beta_k$ update parameter. You will compare the standard Polak–Ribière–Polyak (PRP) formula with its modified 'plus' variant (PRP+) and discover how a simple non-negativity constraint provides a crucial safeguard against generating non-descent directions, especially on challenging nonconvex problems. ",
            "id": "2418475",
            "problem": "You will write a complete, runnable program that compares the nonlinear conjugate gradient method using the Polak–Ribière–Polyak (PRP) update against its modified variant PRP-plus (PRP+) on a nonconvex, twice continuously differentiable objective where the standard PRP method may fail to generate a descent direction. You must use only the fundamental definitions of unconstrained smooth minimization and the definition of the PRP and PRP-plus updates to construct the algorithm.\n\nThe unconstrained minimization problem is to minimize the function\n$$\nf(\\mathbf{x}) = \\tfrac{1}{4} x_1^4 - \\tfrac{1}{2} x_1^2 + \\tfrac{1}{2} x_2^2,\n$$\nwith gradient\n$$\n\\nabla f(\\mathbf{x}) = \\begin{bmatrix} x_1^3 - x_1 \\\\ x_2 \\end{bmatrix}.\n$$\nThis function is nonconvex because the Hessian has eigenvalues $3 x_1^2 - 1$ and $1$, so for $|x_1|  1$ there is negative curvature in the $x_1$ direction.\n\nImplement two nonlinear conjugate gradient solvers that share all components except the update coefficient:\n- Search direction initialization: $\\mathbf{d}_0 = -\\nabla f(\\mathbf{x}_0)$.\n- At iteration $k \\ge 1$, compute\n$$\n\\beta_k^{PRP} = \\frac{\\nabla f(\\mathbf{x}_k)^\\top\\big(\\nabla f(\\mathbf{x}_k) - \\nabla f(\\mathbf{x}_{k-1})\\big)}{\\|\\nabla f(\\mathbf{x}_{k-1})\\|_2^2},\n$$\nand let the directions update be $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k) + \\beta_k \\mathbf{d}_{k-1}$ with $\\beta_k$ chosen as:\n- PRP: $\\beta_k = \\beta_k^{PRP}$,\n- PRP-plus (PRP+): $\\beta_k = \\max\\{0, \\beta_k^{PRP}\\}$.\n\nUse a backtracking Armijo line search to choose the step length $\\alpha_k$ along $\\mathbf{d}_k$. Starting from an initial trial step $\\alpha_0$, repeatedly reduce by a fixed factor until the Armijo sufficient decrease condition holds. Specifically, for given parameters $c_1 \\in (0, 1)$ and $\\rho \\in (0, 1)$, find the smallest integer $m \\ge 0$ such that with $\\alpha = \\alpha_0 \\rho^m$,\n$$\nf(\\mathbf{x}_k + \\alpha \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k.\n$$\n\nStopping criterion: stop when $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon$ or when a maximum number of iterations is reached.\n\nDescent test: at each iteration $k$ before taking a step, record whether $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k  0$ (this indicates a descent direction). For each run, produce a boolean flag that is true if a non-descent direction occurs at least once, that is, if $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k \\ge 0$ for any $k$ during the run.\n\nParameters to use in all runs:\n- Initial trial step $ \\alpha_0 = 5.0$,\n- Sufficient decrease constant $ c_1 = 10^{-4}$,\n- Backtracking factor $ \\rho = 0.5$,\n- Tolerance $ \\varepsilon = 10^{-8}$,\n- Maximum number of iterations $ N_{\\max} = 50$,\n- Maximum number of backtracking reductions per line search $ M_{\\max} = 40$.\n\nYou must run both PRP and PRP+ starting from the following initial points (this is the test suite):\n1. $\\mathbf{x}_0^{(1)} = \\begin{bmatrix} 0.2 \\\\ 0.0 \\end{bmatrix}$, a point near a saddle along the $x_1$ direction where negative curvature can make the PRP method fail to generate a descent direction in subsequent iterations.\n2. $\\mathbf{x}_0^{(2)} = \\begin{bmatrix} 0.0 \\\\ 2.0 \\end{bmatrix}$, a point with a large gradient in the $x_2$ direction but zero $x_1$ component, probing separability and rapid decrease along $x_2$.\n3. $\\mathbf{x}_0^{(3)} = \\begin{bmatrix} -1.5 \\\\ 0.5 \\end{bmatrix}$, a point to the left of the left minimizer with a nonzero $x_2$ component.\n4. $\\mathbf{x}_0^{(4)} = \\begin{bmatrix} 1.2 \\\\ -1.0 \\end{bmatrix}$, a point to the right of the right minimizer with a nonzero $x_2$ component.\n\nFor each initial point, run both PRP and PRP+ and return the following list of four values:\n- The final objective value for PRP after termination, rounded to six decimal places,\n- The final objective value for PRP+ after termination, rounded to six decimal places,\n- A boolean indicating whether PRP generated a non-descent direction at least once,\n- A boolean indicating whether PRP+ generated a non-descent direction at least once.\n\nFinal output format: Your program should produce a single line of output containing a list of four lists, one per initial point, in the order listed above. Each inner list must be of the form $[f_{\\text{PRP}}, f_{\\text{PRP+}}, b_{\\text{PRP}}, b_{\\text{PRP+}}]$, where $f_{\\text{PRP}}$ and $f_{\\text{PRP+}}$ are floats rounded to six decimals and $b_{\\text{PRP}}$ and $b_{\\text{PRP+}}$ are booleans. For example, a syntactically correct output shape is\n$$\n[[0.0,0.0,False,True],[\\dots],[\\dots],[\\dots]].\n$$\nAs an important edge case, note that at $\\mathbf{x}_0^{(1)} = [0.2, 0.0]^\\top$, using $\\alpha_0 = 5.0$ and the Armijo rule described, the first step lands at $\\mathbf{x}_1 \\approx [1.16, 0.0]^\\top$, for which the PRP update yields a positive $\\beta_1^{PRP}$, and consequently the next search direction $\\mathbf{d}_1$ satisfies $\\nabla f(\\mathbf{x}_1)^\\top \\mathbf{d}_1  0$, i.e., a non-descent direction. The PRP+ update coincides with PRP in this case because $\\beta_1^{PRP}  0$ and therefore also exhibits a non-descent direction at this iteration. Your program must detect such events via the descent test.",
            "solution": "The user has provided a valid, well-posed problem in the field of computational engineering, specifically concerning unconstrained nonlinear optimization. The task is to implement and compare two variants of the nonlinear conjugate gradient (NCG) method—Polak–Ribière–Polyak (PRP) and its modification, PRP-plus (PRP+)—on a specified nonconvex objective function. All parameters, initial conditions, and evaluation criteria are explicitly defined. The problem is scientifically sound and requires the implementation of established numerical algorithms. Therefore, a complete solution will be developed.\n\nThe core of the problem is the iterative minimization of the objective function $f(\\mathbf{x}): \\mathbb{R}^2 \\to \\mathbb{R}$, given by:\n$$\nf(\\mathbf{x}) = \\frac{1}{4} x_1^4 - \\frac{1}{2} x_1^2 + \\frac{1}{2} x_2^2\n$$\nThe gradient of this function, $\\nabla f(\\mathbf{x})$, is essential for the NCG methods and is given by:\n$$\n\\nabla f(\\mathbf{x}) = \\begin{bmatrix} x_1^3 - x_1 \\\\ x_2 \\end{bmatrix}\n$$\nThe NCG method is an iterative algorithm that generates a sequence of points $\\mathbf{x}_k$ intended to converge to a local minimum of $f(\\mathbf{x})$. The update from $\\mathbf{x}_k$ to $\\mathbf{x}_{k+1}$ is performed via the rule:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k\n$$\nwhere $\\mathbf{d}_k$ is the search direction and $\\alpha_k  0$ is the step length.\n\nThe algorithm proceeds as follows for each iteration $k = 0, 1, 2, \\dots$:\n\n1.  **Search Direction Calculation**: The initial search direction is the steepest descent direction: $\\mathbf{d}_0 = -\\nabla f(\\mathbf{x}_0)$. For subsequent iterations ($k \\ge 1$), the search direction is a linear combination of the current negative gradient and the previous search direction:\n    $$\n    \\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k) + \\beta_k \\mathbf{d}_{k-1}\n    $$\n    The coefficient $\\beta_k$ distinguishes different NCG methods. This problem considers the PRP and PRP+ variants. The standard PRP update formula is:\n    $$\n    \\beta_k^{\\text{PRP}} = \\frac{\\nabla f(\\mathbf{x}_k)^\\top \\left( \\nabla f(\\mathbf{x}_k) - \\nabla f(\\mathbf{x}_{k-1}) \\right)}{\\|\\nabla f(\\mathbf{x}_{k-1})\\|_2^2}\n    $$\n    The two methods are defined by their choice of $\\beta_k$:\n    -   **PRP Method**: $\\beta_k = \\beta_k^{\\text{PRP}}$. This method can exhibit good performance but may fail to generate a descent direction on nonconvex problems, potentially leading to algorithmic failure.\n    -   **PRP+ Method**: $\\beta_k = \\max\\{0, \\beta_k^{\\text{PRP}}\\}$. This modification ensures that $\\beta_k$ is non-negative. If $\\beta_k^{PRP}  0$, $\\beta_k$ is reset to $0$, and the search direction becomes the steepest descent direction, $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)$, which guarantees descent (provided $\\nabla f(\\mathbf{x}_k) \\neq \\mathbf{0}$). This makes the PRP+ method more robust.\n\n2.  **Descent Direction Test**: Before determining the step length, it is crucial to verify that $\\mathbf{d}_k$ is a descent direction. This is satisfied if $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k  0$. We will record for each run whether a non-descent direction (i.e., $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k \\ge 0$) is ever generated.\n\n3.  **Step Length Calculation**: The step length $\\alpha_k$ is determined by a backtracking line search that satisfies the Armijo sufficient decrease condition. Starting with an initial trial step $\\alpha = \\alpha_0$, we iteratively reduce it by a factor $\\rho$ until the following condition is met for the smallest integer $m \\ge 0$:\n    $$\n    f(\\mathbf{x}_k + \\alpha \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k\n    $$\n    where $\\alpha = \\alpha_0 \\rho^m$. The parameters are provided as $c_1 = 10^{-4}$, $\\rho = 0.5$, and an initial trial step of $\\alpha_0 = 5.0$. The line search will be terminated after a maximum of $M_{\\max} = 40$ reductions.\n\n4.  **Termination**: The iterative process continues until the norm of the gradient is below a specified tolerance $\\varepsilon = 10^{-8}$, or a maximum of $N_{\\max} = 50$ iterations is reached.\n\nThe implementation will consist of a primary solver function that encapsulates this logic. This function will be called for each of the two methods (PRP and PRP+) on each of the four specified initial points $\\mathbf{x}_0$. For each run, the final objective value and a boolean flag indicating if a non-descent direction was encountered will be recorded. The final results will be aggregated and formatted as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the NCG comparison and print results.\n    \"\"\"\n    params = {\n        'alpha0_init': 5.0,\n        'c1': 1e-4,\n        'rho': 0.5,\n        'epsilon': 1e-8,\n        'N_max': 50,\n        'M_max': 40,\n    }\n\n    test_cases = [\n        np.array([0.2, 0.0]),\n        np.array([0.0, 2.0]),\n        np.array([-1.5, 0.5]),\n        np.array([1.2, -1.0])\n    ]\n\n    all_results = []\n    for x0 in test_cases:\n        f_prp, b_prp = run_cg_solver(x0, 'prp', params)\n        f_prp_plus, b_prp_plus = run_cg_solver(x0, 'prp+', params)\n        \n        inner_result = [\n            round(f_prp, 6),\n            round(f_prp_plus, 6),\n            b_prp,\n            b_prp_plus\n        ]\n        all_results.append(inner_result)\n\n    # Format the final output string as specified, without spaces in lists.\n    string_inner_lists = []\n    for res_list in all_results:\n        parts = [str(item) for item in res_list]\n        string_inner_lists.append(f\"[{','.join(parts)}]\")\n    \n    final_output = f\"[{','.join(string_inner_lists)}]\"\n    print(final_output)\n\ndef f(x):\n    \"\"\"Objective function.\"\"\"\n    return 0.25 * x[0]**4 - 0.5 * x[0]**2 + 0.5 * x[1]**2\n\ndef grad_f(x):\n    \"\"\"Gradient of the objective function.\"\"\"\n    return np.array([x[0]**3 - x[0], x[1]])\n\ndef line_search(xk, dk, fk, grad_fk_dot_dk, params):\n    \"\"\"Backtracking Armijo line search.\"\"\"\n    alpha = params['alpha0_init']\n    c1 = params['c1']\n    rho = params['rho']\n    M_max = params['M_max']\n\n    for _ in range(M_max):\n        # A very small step might result from a non-descent direction.\n        # This is expected behavior demonstrating the algorithm's failure.\n        if grad_fk_dot_dk = 0:\n            pass # Armijo condition check below will handle this\n\n        if f(xk + alpha * dk) = fk + c1 * alpha * grad_fk_dot_dk:\n            return alpha\n        alpha *= rho\n    \n    return alpha # Return the last (smallest) alpha if M_max is reached.\n\ndef run_cg_solver(x0, method_type, params):\n    \"\"\"\n    Runs the Nonlinear Conjugate Gradient solver.\n    \"\"\"\n    x_k = np.copy(x0)\n    k = 0\n    non_descent_occurred = False\n\n    N_max = params['N_max']\n    epsilon = params['epsilon']\n\n    grad_k = grad_f(x_k)\n    d_k = -grad_k\n    \n    while k  N_max:\n        grad_norm_k = np.linalg.norm(grad_k)\n        if grad_norm_k = epsilon:\n            break\n        \n        # Descent direction test for the current direction d_k\n        grad_fk_dot_dk = np.dot(grad_k, d_k)\n        if grad_fk_dot_dk = 0:\n            non_descent_occurred = True\n\n        # Perform line search to find an appropriate step length alpha_k\n        fk = f(x_k)\n        alpha_k = line_search(x_k, d_k, fk, grad_fk_dot_dk, params)\n        \n        # Update iterate\n        x_k_plus_1 = x_k + alpha_k * d_k\n        \n        # Compute gradient for the next iteration\n        grad_k_plus_1 = grad_f(x_k_plus_1)\n        \n        # Compute beta for the next direction\n        # Denominator for beta calculation\n        grad_k_norm_sq = np.dot(grad_k, grad_k)\n        \n        if grad_k_norm_sq  1e-16: # Safety for division by zero\n            beta_k_plus_1 = 0.0\n        else:\n            # Numerator for PRP beta\n            beta_prp_num = np.dot(grad_k_plus_1, grad_k_plus_1 - grad_k)\n            beta_prp = beta_prp_num / grad_k_norm_sq\n            \n            if method_type == 'prp':\n                beta_k_plus_1 = beta_prp\n            elif method_type == 'prp+':\n                beta_k_plus_1 = max(0.0, beta_prp)\n            else:\n                raise ValueError(\"Unknown method type\")\n\n        # Update direction for the next iteration\n        d_k_plus_1 = -grad_k_plus_1 + beta_k_plus_1 * d_k\n        \n        # Prepare for next iteration\n        x_k = x_k_plus_1\n        grad_k = grad_k_plus_1\n        d_k = d_k_plus_1\n        k += 1\n\n    final_f = f(x_k)\n    return final_f, non_descent_occurred\n\nsolve()\n```"
        }
    ]
}