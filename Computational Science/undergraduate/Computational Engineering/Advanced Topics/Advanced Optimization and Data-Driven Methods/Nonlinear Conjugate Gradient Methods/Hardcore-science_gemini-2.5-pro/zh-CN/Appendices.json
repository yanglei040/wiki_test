{
    "hands_on_practices": [
        {
            "introduction": "第一个练习是您构建实用优化工具的入门。您将从零开始实现一个完整的非线性共轭梯度求解器，它将采用稳健的 Polak-Ribière-Polyak (PRP+) 更新方法和精确的线搜索策略。通过在一系列经典的基准函数上测试您的代码，您将亲身体验该算法在不同类型、甚至具有挑战性的优化问题上的实际表现。",
            "id": "2418452",
            "problem": "您会获得欧几里得空间上的几个可微目标函数以及初始点。您的任务是编写一个完整的程序，在每种情况下，仅使用函数值和精确的一阶导数来计算近似最小值点。对于指定的目标函数，您的程序所使用的梯度必须在机器精度内是精确的；请勿使用任何有限差分近似。请勿使用任何一阶导数之外的信息。请使用基于梯度欧几里得范数的终止准则。\n\n数学设定：\n\n- 设 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 是一个连续可微函数，并用 $\\nabla f(\\mathbf{x})$ 表示其梯度。从给定的初始点 $\\mathbf{x}_0 \\in \\mathbb{R}^n$ 开始，仅通过计算 $f$ 和 $\\nabla f$ 的值来生成一个序列 $\\{\\mathbf{x}_k\\}$，以尝试最小化 $f$。\n- 当梯度的欧几里得范数满足 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon$ 或迭代次数达到指定的最大值时终止。使用欧几里得范数表示 $\\|\\cdot\\|_2$。\n- 该算法不得使用任何二阶信息（如海森矩阵或海森矩阵-向量积），也不得使用有限差分来近似导数。程序必须在机器精度内计算指定函数的精确梯度。\n\n测试套件：\n\n对于下文的每种情况，您都会获得函数 $f(\\mathbf{x})$、维度 $n$ 和初始点 $\\mathbf{x}_0$。\n\n- 情况 A（非凸，窄谷，双变量）：\n  - 维度：$n=2$。\n  - 目标函数：\n    $$f(\\mathbf{x}) = 100\\,(x_2 - x_1^2)^2 + (1 - x_1)^2.$$\n  - 初始点：$\\mathbf{x}_0 = (-1.2,\\; 1.0)$。\n\n- 情况 B（病态可分二次函数，五变量）：\n  - 维度：$n=5$。\n  - 目标函数：\n    $$f(\\mathbf{x}) = \\tfrac{1}{2}\\sum_{i=1}^{5} \\lambda_i x_i^2,\\quad \\lambda_i = 10^{\\,i-1}.$$\n  - 初始点：$\\mathbf{x}_0 = (1,\\; -1,\\; 1,\\; -1,\\; 1)$。\n\n- 情况 C（平滑，耦合，非凸，三变量）：\n  - 维度：$n=3$。\n  - 目标函数：\n    $$f(\\mathbf{x}) = 0.1\\,(x_1^2 + x_2^2 + x_3^2) + \\sin(x_1)\\cos(x_2) + e^{x_3} - x_3.$$\n  - 初始点：$\\mathbf{x}_0 = (0.5,\\; -0.5,\\; 0.0)$。\n\n- 情况 D（已在最小值点，四变量）：\n  - 维度：$n=4$。\n  - 目标函数：\n    $$f(\\mathbf{x}) = \\sum_{i=1}^{4} (x_i - 1)^2.$$\n  - 初始点：$\\mathbf{x}_0 = (1,\\; 1,\\; 1,\\; 1).$\n\n要求：\n\n- 停止容差：梯度范数使用 $\\varepsilon = 10^{-6}$。\n- 最大迭代次数：每种情况使用 $N_{\\max} = 10000$。\n- 您的程序所使用的梯度对于指定的目标函数必须在机器精度内是精确的。\n- 程序不得读取任何输入，并且除了下面描述的最后一行外，不得写入任何输出。\n\n输出规范：\n\n- 对于每种情况，报告终止时的最终目标函数值 $f(\\mathbf{x}_\\star)$，四舍五入到小数点后六位。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按顺序包含情况 A、情况 B、情况 C 和情况 D 的四个四舍五入的目标函数值。例如，一个有效的输出格式是：\n  - \"[0.000123,0.000000,1.234567,0.000000]\"\n\n不涉及物理单位。三角函数中使用的角度，按照数学惯例，以弧度为单位。输出必须是实数，并且必须遵循上述确切格式。",
            "solution": "所提出的问题是一个标准的数值优化任务，要求最小化几个定义明确的可微函数。该方法被限制为一阶方法，意味着它只能使用函数值 $f(\\mathbf{x})$ 和梯度值 $\\nabla f(\\mathbf{x})$。禁止使用二阶信息，例如海森矩阵。鉴于这些限制，一个非常合适且高效的算法是非线性共轭梯度（CG）法。虽然更简单的最速下降法也满足这些约束，但对于具有高曲率或病态条件的问题，例如 Rosenbrock 函数（情况 A）和给定的具有悬殊特征值的二次函数（情况 B），其收敛速度是出了名的慢。CG 方法通过构造作为梯度共轭式扩展的搜索方向来加速收敛，从而有效地结合了先前步骤的信息。\n\n非线性 CG 算法的迭代过程，从初始点 $\\mathbf{x}_0$ 开始，对 $k=0, 1, 2, \\dots$ 定义如下：\n1. 计算当前迭代点的梯度：$\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$。\n2. 检查收敛性：如果梯度的欧几里得范数 $\\|\\mathbf{g}_k\\|_2$ 低于指定的容差 $\\varepsilon$，则算法终止。\n3. 计算搜索方向 $\\mathbf{p}_k$。对于第一次迭代（$k=0$），方向是最速下降方向，即 $\\mathbf{p}_0 = -\\mathbf{g}_0$。对于后续迭代（$k  0$），方向是当前负梯度与前一个搜索方向的线性组合：\n   $$\n   \\mathbf{p}_k = -\\mathbf{g}_k + \\beta_k \\mathbf{p}_{k-1}\n   $$\n   标量 $\\beta_k$ 决定了 CG 方法的具体变体。\n4. 执行线搜索以确定沿方向 $\\mathbf{p}_k$ 的合适步长 $\\alpha_k  0$。\n5. 更新迭代点：$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$。\n\n对于此实现，我们选择 Polak–Ribière–Polyak (PRP) 公式来计算 $\\beta_k$，因为与其他公式（如 Fletcher-Reeves）相比，它通常具有更优越的经验性能。PRP 公式为：\n$$\n\\beta_k^{\\text{PRP}} = \\frac{\\mathbf{g}_k^T (\\mathbf{g}_k - \\mathbf{g}_{k-1})}{\\mathbf{g}_{k-1}^T \\mathbf{g}_{k-1}}\n$$\n为了提高鲁棒性并保证全局收敛性，该公式被增强为 PRP+ 方法，其中 $\\beta_k = \\max(0, \\beta_k^{\\text{PRP}})$。如果 $\\beta_k^{\\text{PRP}}$ 变为负值（这在远离局部最小值时可能发生），此修改可防止算法采取不良的步长。此外，作为一种保障措施，如果搜索方向 $\\mathbf{p}_k$ 不再是下降方向（即，如果 $\\mathbf{p}_k^T \\mathbf{g}_k \\ge 0$），它将被强制重置为最速下降方向 $-\\mathbf{g}_k$。\n\n步长 $\\alpha_k$ 通过满足强 Wolfe 条件的线搜索找到：\n1. 充分下降（Armijo）条件：$f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha_k \\mathbf{g}_k^T \\mathbf{p}_k$\n2. 曲率条件：$|\\nabla f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k)^T \\mathbf{p}_k| \\le c_2 |\\mathbf{g}_k^T \\mathbf{p}_k|$\n常数选择标准值 $c_1 = 10^{-4}$ 和 $c_2 = 0.9$。这些条件确保每一步都能在目标函数值上实现有意义的减少。使用 `scipy.optimize.line_search` 函数来实现此步骤。\n\n要求解析梯度是精确的。四个测试用例的梯度推导如下：\n\n情况 A：Rosenbrock 函数，$f(\\mathbf{x}) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2$ 对于 $\\mathbf{x} \\in \\mathbb{R}^2$。\n$$\n\\nabla f(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} -400x_1(x_2 - x_1^2) - 2(1 - x_1) \\\\ 200(x_2 - x_1^2) \\end{pmatrix}\n$$\n\n情况 B：病态可分二次函数，$f(\\mathbf{x}) = \\frac{1}{2}\\sum_{i=1}^{5} \\lambda_i x_i^2$ 且 $\\lambda_i = 10^{i-1}$ 对于 $\\mathbf{x} \\in \\mathbb{R}^5$。\n每个 $x_j$ 的梯度分量为 $\\frac{\\partial f}{\\partial x_j} = \\lambda_j x_j$。\n$$\n\\nabla f(\\mathbf{x})_j = 10^{j-1} x_j, \\quad \\text{对于 } j=1, \\dots, 5\n$$\n\n情况 C：平滑、耦合、非凸函数，$f(\\mathbf{x}) = 0.1(x_1^2 + x_2^2 + x_3^2) + \\sin(x_1)\\cos(x_2) + e^{x_3} - x_3$ 对于 $\\mathbf{x} \\in \\mathbb{R}^3$。\n$$\n\\nabla f(\\mathbf{x}) = \\begin{pmatrix} 0.2x_1 + \\cos(x_1)\\cos(x_2) \\\\ 0.2x_2 - \\sin(x_1)\\sin(x_2) \\\\ 0.2x_3 + e^{x_3} - 1 \\end{pmatrix}\n$$\n\n情况 D：简单二次函数，$f(\\mathbf{x}) = \\sum_{i=1}^{4} (x_i - 1)^2$ 对于 $\\mathbf{x} \\in \\mathbb{R}^4$。\n每个 $x_j$ 的梯度分量为 $\\frac{\\partial f}{\\partial x_j} = 2(x_j-1)$。\n$$\n\\nabla f(\\mathbf{x})_j = 2(x_j - 1), \\quad \\text{对于 } j=1, \\dots, 4\n$$\n对于这种情况，初始点 $\\mathbf{x}_0 = (1, 1, 1, 1)$ 是该函数的唯一全局最小值点。因此，$\\nabla f(\\mathbf{x}_0) = \\mathbf{0}$，算法在第 $k=0$ 次迭代时立即终止，目标值为 $f(\\mathbf{x}_0)=0$。\n\n该实现将这些元素组合成一个单一的程序。一个通用的求解器函数封装了 CG 逻辑，并为每个测试用例调用该函数，传入各自的目标函数、梯度和初始点。当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le 10^{-6}$ 或在 $10000$ 次迭代后，程序终止。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\n\ndef conjugate_gradient_solver(f, grad_f, x0, tol=1e-6, max_iter=10000):\n    \"\"\"\n    Minimizes a function using the Nonlinear Conjugate Gradient method (Polak-Ribière-Polyak+).\n    \"\"\"\n    x_k = np.array(x0, dtype=float)\n    f_k = f(x_k)\n    g_k = grad_f(x_k)\n    grad_norm = np.linalg.norm(g_k)\n\n    if grad_norm = tol:\n        return f(x_k)\n\n    p_k = -g_k\n    \n    k = 0\n    while k  max_iter and grad_norm > tol:\n        # Perform line search to find alpha_k satisfying strong Wolfe conditions.\n        # c1=1e-4 and c2=0.9 are standard for CG.\n        try:\n            line_search_result = optimize.line_search(f, grad_f, x_k, p_k, gfk=g_k, old_fval=f_k, c1=1e-4, c2=0.9)\n            alpha_k = line_search_result[0]\n        except Exception:\n            # line_search can sometimes raise errors for extreme values\n            alpha_k = None\n\n        # If line search fails, restart with steepest descent.\n        if alpha_k is None:\n            p_k = -g_k\n            try:\n                line_search_result = optimize.line_search(f, grad_f, x_k, p_k, gfk=g_k, old_fval=f_k, c1=1e-4, c2=0.9)\n                alpha_k = line_search_result[0]\n            except Exception:\n                alpha_k = None\n            \n            if alpha_k is None:\n                # If it still fails, terminate. Could be due to precision limits.\n                break\n\n        x_k_plus_1 = x_k + alpha_k * p_k\n        g_k_plus_1 = grad_f(x_k_plus_1)\n\n        # Polak-Ribière-Polyak+ update for beta\n        g_k_dot_g_k = np.dot(g_k, g_k)\n        if g_k_dot_g_k == 0:\n            beta_k_plus_1 = 0.0\n        else:\n            beta_numerator = np.dot(g_k_plus_1, g_k_plus_1 - g_k)\n            beta_k_plus_1 = max(0, beta_numerator / g_k_dot_g_k)\n        \n        # New search direction\n        p_k_plus_1 = -g_k_plus_1 + beta_k_plus_1 * p_k\n\n        # Check for descent direction. If not, reset to steepest descent.\n        if np.dot(p_k_plus_1, g_k_plus_1) >= 0:\n            p_k_plus_1 = -g_k_plus_1\n\n        # Update variables for the next iteration\n        x_k = x_k_plus_1\n        g_k = g_k_plus_1\n        p_k = p_k_plus_1\n        f_k = f(x_k) # Can be taken from line_search output, but re-evaluating is simple.\n        \n        grad_norm = np.linalg.norm(g_k)\n        k += 1\n\n    return f(x_k)\n\ndef solve():\n    # Final print statement in the exact required format.\n    \n    # Case A: Rosenbrock function\n    def f_A(x):\n        return 100.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2\n\n    def grad_f_A(x):\n        df_dx1 = -400.0 * x[0] * (x[1] - x[0]**2) - 2.0 * (1.0 - x[0])\n        df_dx2 = 200.0 * (x[1] - x[0]**2)\n        return np.array([df_dx1, df_dx2])\n\n    # Case B: Ill-conditioned separable quadratic\n    def f_B(x):\n        lambdas = 10.**np.arange(len(x))\n        return 0.5 * np.sum(lambdas * x**2)\n\n    def grad_f_B(x):\n        lambdas = 10.**np.arange(len(x))\n        return lambdas * x\n\n    # Case C: Smooth, coupled, nonconvex\n    def f_C(x):\n        term1 = 0.1 * np.sum(x**2)\n        term2 = np.sin(x[0]) * np.cos(x[1])\n        term3 = np.exp(x[2]) - x[2]\n        return term1 + term2 + term3\n\n    def grad_f_C(x):\n        df_dx1 = 0.2 * x[0] + np.cos(x[0]) * np.cos(x[1])\n        df_dx2 = 0.2 * x[1] - np.sin(x[0]) * np.sin(x[1])\n        df_dx3 = 0.2 * x[2] + np.exp(x[2]) - 1.0\n        return np.array([df_dx1, df_dx2, df_dx3])\n    \n    # Case D: Simple quadratic\n    def f_D(x):\n        return np.sum((x - 1.0)**2)\n\n    def grad_f_D(x):\n        return 2.0 * (x - 1.0)\n    \n    test_cases = [\n        {'f': f_A, 'grad_f': grad_f_A, 'x0': [-1.2, 1.0]},\n        {'f': f_B, 'grad_f': grad_f_B, 'x0': [1.0, -1.0, 1.0, -1.0, 1.0]},\n        {'f': f_C, 'grad_f': grad_f_C, 'x0': [0.5, -0.5, 0.0]},\n        {'f': f_D, 'grad_f': grad_f_D, 'x0': [1.0, 1.0, 1.0, 1.0]}\n    ]\n\n    results = []\n    for case in test_cases:\n        final_f_val = conjugate_gradient_solver(\n            f=case['f'],\n            grad_f=case['grad_f'],\n            x0=case['x0'],\n            tol=1e-6,\n            max_iter=10000\n        )\n        results.append(final_f_val)\n\n    # Format output as specified\n    formatted_results = [\"{:.6f}\".format(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "为什么像线搜索这样的算法组件是必不可少的？本练习通过直接对比来探讨这一问题。您将实现一个正确的NCG算法和一个省略了关键的Armijo充分下降条件的“错误”版本，从而清晰地展示为何这一设计对于保证算法在病态或高度非线性问题上的稳健收敛至关重要。",
            "id": "2418455",
            "problem": "要求您实现并比较非线性共轭梯度法的两种变体，用于无约束最小化一个连续可微函数。第一种变体是标准的非线性共轭梯度算法，它通过回溯线搜索来强制满足第一个Wolfe条件（Armijo充分下降条件）。第二种变体是故意设置的错误版本：它不强制满足Armijo条件，而是在每次迭代中使用固定的单位步长。您的任务是通过精心选择的测试函数来证明，即使在标准方法能够收敛的情况下，省略Armijo条件也可能导致不收敛或发散。\n\n从以下基础知识开始：\n- 对于一个目标函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$，如果它是连续可微的，一个下降方法会生成迭代点 $x_{k+1} = x_k + \\alpha_k d_k$，其中搜索方向 $d_k$ 满足 $g_k^\\top d_k  0$，其中 $g_k = \\nabla f(x_k)$ 且 $\\alpha_k  0$ 是一个步长。\n- 第一个Wolfe（Armijo）充分下降条件要求，对于常数 $c_1 \\in (0,1)$，步长满足\n$$\nf(x_k + \\alpha_k d_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top d_k.\n$$\n- 在非线性共轭梯度法中，搜索方向通过以下方式构造\n$$\nd_0 = -g_0,\\quad d_k = -g_k + \\beta_k d_{k-1}\\ \\text{for}\\ k \\ge 1,\n$$\n其中 $\\beta_k$ 的一个经典选择是Polak–Ribiere–Plus，并带有一个安全措施，即如果 $g_k^\\top d_k \\ge 0$ 则重置 $d_k=-g_k$ 以保持下降性。\n- 对于形式为 $f(x) = \\tfrac{1}{2} x^\\top Q x$ 的光滑凸二次目标函数，其中 $Q$ 是一个对称正定矩阵，使用固定步长 $\\alpha$ 的梯度下降法会产生线性迭代 $x_{k+1} = (I - \\alpha Q) x_k$。收敛到极小点 $x^\\star=0$ 的充要条件是谱半径满足 $\\rho(I - \\alpha Q)  1$，等价于 $0  \\alpha  2/\\lambda_{\\max}(Q)$。\n\n您的程序必须实现：\n- 一个标准的非线性共轭梯度求解器，它使用回溯线搜索来强制满足Armijo条件，其中包含用户选择的常数 $c_1 \\in (0,1)$ 和回溯比率 $\\tau \\in (0,1)$。\n- 一个错误的非线性共轭梯度求解器，它对所有 $k$ 都使用 $\\alpha_k \\equiv 1$（无充分下降检查）。\n\n您必须遵守的设计细节：\n- 使用Polak–Ribiere–Plus选择共轭参数 $\\beta_k$，并在计算出的方向不是下降方向时采用重置保障措施。\n- 当 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$（给定容差 $\\varepsilon  0$）或当固定的迭代预算用尽时终止。\n- 如果在未满足梯度容差的情况下迭代预算用尽，则声明为不收敛。如果目标函数值变为非数值、超过一个大阈值或迭代点范数超过一个大阈值，则声明为发散。\n\n构建一个测试套件以展示不同的行为：\n- 测试A（发散见证）：一个凸二次函数 $f(x) = \\tfrac{1}{2} x^\\top Q x$，其中 $Q = \\mathrm{diag}(10.0, 0.1)$，初始点 $x_0 = [1.0, 1.0]$。根据谱半径准则，固定的步长 $\\alpha = 1$ 违反了 $0  \\alpha  2/\\lambda_{\\max}(Q)$，因为 $\\lambda_{\\max}(Q) = 10.0$，因此，错误方法预计会发散，而强制执行Armijo条件的方法应该会收敛。\n- 测试B（顺利路径）：一个良态的凸二次函数，其中 $Q = \\mathrm{diag}(0.5, 0.25)$，初始点 $x_0 = [2.0, -3.0]$。这里 $\\lambda_{\\max}(Q) = 0.5$，所以固定的步长 $\\alpha = 1$ 满足 $0  \\alpha  2/\\lambda_{\\max}(Q)$，两种方法都应该收敛。\n- 测试C（边界条件）：同样的凸二次函数 $Q = \\mathrm{diag}(1.0, 1.0)$，初始点 $x_0 = [0.0, 0.0]$，这已经是一个极小点。两种方法都应该立即检测到收敛。\n- 测试D（非凸压力测试）：一个缩放的Rosenbrock函数 $f(x_1,x_2) = 10\\,(x_2 - x_1^2)^2 + (1 - x_1)^2$，初始点 $x_0 = [-1.2, 1.0]$。错误方法的单位步长可能导致在这个弯曲的山谷上发生数值爆炸，而带有Armijo回溯的标准方法应该会收敛到 $[1,1]$ 附近的极小点。\n\n在您的程序中使用的数值参数：\n- 梯度范数容差 $\\varepsilon = 10^{-6}$，最大迭代次数 $N_{\\max} = 5000$，Armijo常数 $c_1 = 10^{-4}$，回溯比率 $\\tau = 0.5$。\n- 发散阈值：如果 $\\|x_k\\|_2  10^{12}$ 或 $f(x_k)  10^{50}$ 或 $f(x_k)$ 为非数值，则声明为发散。\n\n您的程序必须：\n- 实现两个求解器，在所有四个测试上运行它们，并为每个测试根据观察到的行为确定一个整数代码：\n    - 如果标准方法收敛而错误方法不收敛（不收敛或发散），则输出 $1$。\n    - 如果两者都收敛，则输出 $0$。\n    - 如果两者都不收敛，则输出 $-1$。\n    - 如果错误方法收敛而标准方法不收敛，则输出 $2$。\n- 生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。例如，输出格式必须看起来像一个单独的Python风格列表字面量，如 [r1,r2,r3,r4]，其中每个条目是指定的整数代码之一。\n\n不涉及角度单位。此问题中没有物理单位。所有量纲均为无量纲。输出必须严格遵循指定的单行格式。",
            "solution": "所提出的问题是实现并比较用于无约束优化的非线性共轭梯度（NCG）法的两种变体。一种变体被正确实现，通过强制执行Armijo充分下降条件来遵循线搜索方法的基本原则。第二种变体是故意设置的错误版本，它采用固定的单位步长，从而省略了线搜索这一关键保障。目标是通过计算证明，在正确实现的算法能够成功的问题上，省略Armijo条件可能导致失败，具体表现为不收敛或发散。该问题定义明确，科学上合理，并为算法实现和验证提供了清晰的基础。\n\n考虑一个一般的无约束最小化问题，旨在寻找一个连续可微目标函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 的局部极小点。NCG方法是一种迭代算法，它使用以下更新规则生成一系列点 $\\{x_k\\}_{k \\ge 0}$：\n$$\nx_{k+1} = x_k + \\alpha_k d_k\n$$\n这里，$x_k \\in \\mathbb{R}^n$ 是当前迭代点，$d_k \\in \\mathbb{R}^n$ 是搜索方向，而 $\\alpha_k  0$ 是步长。目标函数在 $x_k$ 处的梯度表示为 $g_k = \\nabla f(x_k)$。\n\n搜索方向 $d_k$ 被构造成一个下降方向，意味着 $g_k^\\top d_k  0$。NCG方向是递归定义的。初始方向是最速下降方向，$d_0 = -g_0$。对于后续迭代 $k \\ge 1$，方向是当前负梯度和前一个方向的线性组合：\n$$\nd_k = -g_k + \\beta_k d_{k-1}\n$$\n标量 $\\beta_k$ 是共轭参数。问题指定了Polak–Ribière–Plus变体，该变体以其强大的数值性能而闻名。其定义为：\n$$\n\\beta_k = \\max \\left\\{ 0, \\frac{g_k^\\top(g_k - g_{k-1})}{\\|g_{k-1}\\|_2^2} \\right\\}\n$$\n这个选择包含了一个非负性约束，这有助于在某些条件下确保全局收敛。一个至关重要的保障是重置条件：如果计算出的方向 $d_k$ 未能成为下降方向（即，如果 $g_k^\\top d_k \\ge 0$），则通过将搜索方向设置为最速下降方向 $d_k = -g_k$ 来重置方法。\n\n本研究的核心在于步长 $\\alpha_k$ 的确定。\n\n**标准的NCG方法**采用回溯线搜索来寻找一个满足Armijo充分下降条件的步长 $\\alpha_k$。对于给定的常数 $c_1 \\in (0, 1)$，该条件为：\n$$\nf(x_k + \\alpha_k d_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top d_k\n$$\n这个不等式确保了目标函数的减少量至少是在 $x_k$ 处 $f$ 的线性近似所预测的减少量的一小部分。回溯过程从一个初始试探步长（通常是 $\\alpha = 1$）开始，并以一个因子 $\\tau \\in (0, 1)$（例如 $\\alpha \\leftarrow \\tau \\alpha$）迭代地减小它，直到满足该条件。指定的参数是 $c_1 = 10^{-4}$ 和 $\\tau = 0.5$。\n\n**错误的NCG方法**绕过了这一关键检查，并简单地对所有迭代 $k \\ge 0$ 设置 $\\alpha_k = 1$。虽然这对于某些行为良好的函数或者当初始迭代点接近解时可能是可接受的，但它通常是一种不可靠的策略，可能导致失败。\n\n算法的终止由梯度的范数决定。如果 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$（对于容差 $\\varepsilon = 10^{-6}$），则认为迭代收敛。如果迭代次数超过预算 $N_{\\max} = 5000$，则中止过程，这被归类为不收敛。如果迭代点范数 $\\|x_k\\|_2$ 超过 $10^{12}$，函数值 $f(x_k)$ 超过 $10^{50}$，或者 $f(x_k)$ 变为非数值（NaN），则声明为发散。\n\n分析是在一套旨在揭示两种方法不同行为的四个测试案例上进行的。\n\n**测试A**：一个凸二次函数 $f(x) = \\frac{1}{2} x^\\top Q x$，其Hessian矩阵 $Q = \\mathrm{diag}(10.0, 0.1)$ 条件数较差。对于二次函数，具有固定步长 $\\alpha$ 的NCG方法等价于线性迭代系统 $x_{k+1} = (I - \\alpha Q) x_k$。该系统收敛的充要条件是迭代矩阵的谱半径 $\\rho(I - \\alpha Q)$ 小于 $1$。当 $\\alpha=1$ 时，$I-Q$ 的特征值为 $1-10.0 = -9.0$ 和 $1-0.1 = 0.9$。谱半径为 $\\rho(I - Q) = \\max\\{|-9.0|, |0.9|\\} = 9.0$，大于 $1$。因此，错误方法保证会发散。而标准方法，凭借其来自Armijo条件的自适应步长，预计会收敛。\n\n**测试B**：一个良态的凸二次函数，其 $Q = \\mathrm{diag}(0.5, 0.25)$。在这里，对于步长 $\\alpha=1$ 的错误方法，$I-Q$ 的特征值为 $1-0.5 = 0.5$ 和 $1-0.25=0.75$。谱半径为 $\\rho(I - Q) = 0.75  1$，满足收敛条件。因此，错误方法和标准方法预计都会收敛。\n\n**测试C**：一个凸二次函数，初始点 $x_0 = [0.0, 0.0]$ 是全局极小点。初始梯度为 $\\nabla f(x_0) = 0$。两种算法都必须在第一次迭代前检查终止条件，并立即声明收敛。\n\n**测试D**：非凸的Rosenbrock函数 $f(x_1,x_2) = 10(x_2 - x_1^2)^2 + (1-x_1)^2$，从起始点 $x_0 = [-1.2, 1.0]$ 开始。这个函数是一个经典的基准测试，其特点是有一个狭窄、弯曲的山谷。错误方法的固定单位步长很可能会导致迭代点“跳过”山谷，导致函数值增加和不稳定的行为，很可能导致发散或不收敛。相反，标准方法的回溯线搜索将系统地减小步长以确保充分下降，从而使迭代点能够沿着山谷走向位于 $[1,1]$ 的最小值点。\n\n每个测试的结果是一个整数代码：如果标准方法收敛而错误方法不收敛，则为 $1$；如果两者都收敛，则为 $0$；如果两者都不收敛，则为 $-1$；如果错误方法收敛但标准方法不收敛，则为 $2$。这种系统性的比较为线搜索机制在确保基于下降的优化算法的鲁棒性方面所起的不可或缺的作用提供了明确的证据。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares a proper and a faulty nonlinear conjugate gradient method.\n    \"\"\"\n\n    # --- Numerical Parameters ---\n    EPSILON = 1e-6\n    MAX_ITER = 5000\n    C1 = 1e-4\n    TAU = 0.5\n    DIV_NORM_THRESHOLD = 1e12\n    DIV_F_THRESHOLD = 1e50\n\n    def nonlinear_cg(f, grad_f, x0, use_armijo):\n        \"\"\"\n        Nonlinear Conjugate Gradient (NCG) solver.\n\n        Args:\n            f: Objective function.\n            grad_f: Gradient of the objective function.\n            x0: Initial point.\n            use_armijo: Boolean flag to use Armijo line search.\n\n        Returns:\n            A string indicating the outcome: \"converged\", \"nonconverged\", \"diverged\".\n        \"\"\"\n        x_k = np.copy(x0).astype(np.float64)\n        g_k = grad_f(x_k)\n        \n        # Initial check for convergence at x0\n        norm_g_k = np.linalg.norm(g_k)\n        if norm_g_k = EPSILON:\n            return \"converged\"\n\n        d_k = -g_k\n        k = 0\n\n        while k  MAX_ITER:\n            # Line Search\n            if use_armijo:\n                alpha_k = 1.0\n                descent_condition_val = C1 * np.dot(g_k, d_k)\n                # The dot product g_k.T @ d_k should be  0 due to safeguard\n                try:\n                    f_k = f(x_k)\n                    while f(x_k + alpha_k * d_k) > f_k + alpha_k * descent_condition_val:\n                        alpha_k *= TAU\n                        if alpha_k  1e-15: # Prevent infinite loop if step size becomes too small\n                           return \"nonconverged\"\n                except (OverflowError, ValueError):\n                    return \"diverged\" # f() evaluation might fail\n            else:\n                alpha_k = 1.0\n\n            # Update position\n            x_k_plus_1 = x_k + alpha_k * d_k\n\n            # Check for divergence\n            try:\n                f_next = f(x_k_plus_1)\n                if np.linalg.norm(x_k_plus_1) > DIV_NORM_THRESHOLD or f_next > DIV_F_THRESHOLD or np.isnan(f_next):\n                    return \"diverged\"\n            except (OverflowError, ValueError):\n                return \"diverged\"\n\n            g_k_plus_1 = grad_f(x_k_plus_1)\n            norm_g_k_plus_1 = np.linalg.norm(g_k_plus_1)\n\n            # Check for convergence\n            if norm_g_k_plus_1 = EPSILON:\n                return \"converged\"\n\n            # Polak-Ribiere-Plus (PR+) for beta\n            norm_g_k_sq = norm_g_k**2\n            if norm_g_k_sq > 1e-14: # Safeguard against division by zero\n                beta_k_plus_1 = max(0, np.dot(g_k_plus_1, g_k_plus_1 - g_k) / norm_g_k_sq)\n            else:\n                beta_k_plus_1 = 0.0\n\n            # Update search direction\n            d_k_plus_1 = -g_k_plus_1 + beta_k_plus_1 * d_k\n\n            # Restart if not a descent direction\n            if np.dot(g_k_plus_1, d_k_plus_1) >= 0:\n                d_k_plus_1 = -g_k_plus_1\n\n            # Prepare for next iteration\n            x_k = x_k_plus_1\n            g_k = g_k_plus_1\n            norm_g_k = norm_g_k_plus_1\n            d_k = d_k_plus_1\n            k += 1\n\n        return \"nonconverged\"\n    \n    # --- Test Case Definitions ---\n\n    # Test A: Divergence Witness\n    Q_A = np.diag([10.0, 0.1])\n    def f_A(x): return 0.5 * x.T @ Q_A @ x\n    def grad_f_A(x): return Q_A @ x\n    x0_A = np.array([1.0, 1.0])\n\n    # Test B: Happy Path\n    Q_B = np.diag([0.5, 0.25])\n    def f_B(x): return 0.5 * x.T @ Q_B @ x\n    def grad_f_B(x): return Q_B @ x\n    x0_B = np.array([2.0, -3.0])\n\n    # Test C: Boundary Condition\n    Q_C = np.diag([1.0, 1.0])\n    def f_C(x): return 0.5 * x.T @ Q_C @ x\n    def grad_f_C(x): return Q_C @ x\n    x0_C = np.array([0.0, 0.0])\n\n    # Test D: Nonconvex Stress Test (Scaled Rosenbrock)\n    def f_D(x): return 10.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2\n    def grad_f_D(x):\n        df_dx1 = -40.0 * x[0] * (x[1] - x[0]**2) - 2.0 * (1.0 - x[0])\n        df_dx2 = 20.0 * (x[1] - x[0]**2)\n        return np.array([df_dx1, df_dx2])\n    x0_D = np.array([-1.2, 1.0])\n\n    test_cases = [\n        (f_A, grad_f_A, x0_A),\n        (f_B, grad_f_B, x0_B),\n        (f_C, grad_f_C, x0_C),\n        (f_D, grad_f_D, x0_D),\n    ]\n\n    results = []\n    for f, grad_f, x0 in test_cases:\n        proper_status = nonlinear_cg(f, grad_f, x0, use_armijo=True)\n        faulty_status = nonlinear_cg(f, grad_f, x0, use_armijo=False)\n\n        proper_converged = (proper_status == \"converged\")\n        faulty_converged = (faulty_status == \"converged\")\n\n        if proper_converged and not faulty_converged:\n            results.append(1)\n        elif proper_converged and faulty_converged:\n            results.append(0)\n        elif not proper_converged and not faulty_converged:\n            results.append(-1)\n        elif not proper_converged and faulty_converged:\n            results.append(2)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "优化算法的行为有时是反直觉的。本练习将深入探讨NCG方法中一个微妙的特性：梯度范数 $\\lVert \\nabla f(x_k) \\rVert_2$ 并非在每一步迭代中都必然减小。通过实现和观察不同的NCG变体，您将研究这种非单调行为，并对共轭梯度法复杂而高效的收敛路径获得更深刻的理解。",
            "id": "2418485",
            "problem": "您必须编写一个完整且可运行的程序，对于一组固定的目标函数和起始点，判断在基于非线性共轭梯度法 (NCG) 的优化过程中，梯度的欧几里得范数（表示为 $\\lVert \\nabla f(x_k) \\rVert_2$）是否非单调。该任务的形式化描述如下。\n\n设 $f : \\mathbb{R}^n \\to \\mathbb{R}$ 是连续可微的。考虑一个迭代格式，它生成一个序列 $\\{x_k\\}_{k \\ge 0}$，其中包含搜索方向 $\\{d_k\\}_{k \\ge 0}$、步长 $\\{\\alpha_k\\}_{k \\ge 0}$、梯度 $g_k \\equiv \\nabla f(x_k)$ 以及梯度范数 $\\lVert g_k \\rVert_2$。初始搜索方向为 $d_0 = -g_0$。对于 $k \\ge 0$，定义更新规则\n$$\nx_{k+1} = x_k + \\alpha_k d_k, \\quad d_{k+1} = -g_{k+1} + \\beta_k d_k,\n$$\n其中 $\\beta_k$ 根据经典的 NCG 公式之一进行选择：\n- Fletcher–Reeves (FR): $\\beta_k^{\\mathrm{FR}} = \\dfrac{\\langle g_{k+1}, g_{k+1} \\rangle}{\\langle g_k, g_k \\rangle}$,\n- Polak–Ribière–Polyak 非负修正 (PR+): $\\beta_k^{\\mathrm{PR}+} = \\max\\!\\left\\{0,\\; \\dfrac{\\langle g_{k+1}, g_{k+1} - g_k \\rangle}{\\langle g_k, g_k \\rangle}\\right\\}$,\n- Hestenes–Stiefel 非负修正 (HS+): $\\beta_k^{\\mathrm{HS}+} = \\max\\!\\left\\{0,\\; \\dfrac{\\langle g_{k+1}, g_{k+1} - g_k \\rangle}{\\langle d_k, g_{k+1} - g_k \\rangle}\\right\\}$,\n此处 $\\langle \\cdot,\\cdot \\rangle$ 表示 $\\mathbb{R}^n$ 中的标准内积。\n\n步长 $\\alpha_k$ 必须满足沿直线 $\\phi_k(\\alpha) = f(x_k + \\alpha d_k)$ 的强 Wolfe 条件，常数为 $c_1 \\in (0,1)$ 和 $c_2 \\in (c_1,1)$：\n$$\n\\phi_k(\\alpha_k) \\le \\phi_k(0) + c_1 \\alpha_k \\phi_k'(0), \n\\quad\n\\lvert \\phi_k'(\\alpha_k) \\rvert \\le c_2 \\lvert \\phi_k'(0) \\rvert,\n$$\n其中 $\\phi_k'(\\alpha) = \\nabla f(x_k + \\alpha d_k)^\\top d_k$。如果计算出的搜索方向 $d_k$ 未能成为下降方向，即 $\\langle d_k, g_k \\rangle \\ge 0$，则通过设置 $d_k \\leftarrow -g_k$ 强制重启。\n\n您的程序必须实现此方法，并对下面列出的每个测试用例，计算算法产生的有限序列 $\\{\\lVert g_k \\rVert_2\\}_{k=0}^{K}$，直到 $\\lVert g_k \\rVert_2 \\le \\varepsilon$ 或 $k$ 达到给定的迭代次数上限。然后，为每个用例输出一个布尔值，指示梯度范数序列是否非单调，即是否存在索引 $k$ 使得 $\\lVert g_{k+1} \\rVert_2  \\lVert g_k \\rVert_2$。\n\n对所有测试用例使用以下固定常数：$c_1 = 10^{-4}$，$c_2 = 0.9$，初始步长 $\\alpha_0 = 1$，梯度容差 $\\varepsilon = 10^{-8}$，以及最大迭代次数 $K_{\\max} = 500$。所有内积和范数均为欧几里得范数，所有数值均为实数。\n\n您必须使用以下由目标函数、维度、起始点和 $\\beta_k$ 变体组成的测试套件：\n\n- 测试用例 1（非凸，二维）：\n  - 目标函数：二维 Rosenbrock 函数\n    $$\n    f_1(x) = (1 - x_1)^2 + 100\\,(x_2 - x_1^2)^2,\n    $$\n    其中 $x = (x_1, x_2)^\\top \\in \\mathbb{R}^2$。\n  - 起始点：$x_0 = (-1.2,\\, 1.0)^\\top$。\n  - 变体：$\\beta_k = \\beta_k^{\\mathrm{PR}+}$。\n\n- 测试用例 2（严格凸二次函数，三维）：\n  - 目标函数：球面二次函数\n    $$\n    f_2(x) = \\tfrac{1}{2}\\,\\lVert x \\rVert_2^2,\n    $$\n    其中 $x \\in \\mathbb{R}^3$。\n  - 起始点：$x_0 = (3,\\, -4,\\, 1)^\\top$。\n  - 变体：$\\beta_k = \\beta_k^{\\mathrm{FR}}$。\n\n- 测试用例 3（非凸，五维）：\n  - 目标函数：$5$ 维 Rosenbrock 函数\n    $$\n    f_3(x) = \\sum_{i=1}^{4} \\left[(1 - x_i)^2 + 100\\,\\left(x_{i+1} - x_i^2\\right)^2\\right],\n    $$\n    其中 $x = (x_1,\\dots,x_5)^\\top \\in \\mathbb{R}^5$。\n  - 起始点：$x_0 = (-1.2,\\, 1.0,\\, -1.2,\\, 1.0,\\, -1.2)^\\top$。\n  - 变体：$\\beta_k = \\beta_k^{\\mathrm{HS}+}$。\n\n对于每个测试用例，您必须返回一个布尔值，定义如下：\n- 如果存在至少一个索引 $k$ 使得 $\\lVert g_{k+1} \\rVert_2  \\lVert g_k \\rVert_2$，则返回 true。\n- 否则返回 false。\n\n最终输出格式：您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[result1,result2,result3]”）。结果必须按上述测试用例的顺序出现。此问题不涉及物理单位。不使用角度。不使用百分比。输出元素为布尔值。",
            "solution": "所给出的问题是数值优化领域中一个明确定义的计算任务。它要求实现非线性共轭梯度法 (NCG)，以确定对于一组指定的目标函数和初始条件，梯度范数序列是否表现出非单调行为。该问题具有科学依据，形式化规范，并提供了所有必要的参数。因此，该问题被认为是有效的。\n\n问题的核心是求解一个无约束优化问题，形式为\n$$\n\\min_{x \\in \\mathbb{R}^n} f(x)\n$$\n其中 $f$ 是一个连续可微函数。NCG 方法从初始点 $x_0$ 开始，生成一个迭代序列 $\\{x_k\\}$。更新规则由下式给出\n$$\nx_{k+1} = x_k + \\alpha_k d_k\n$$\n其中 $d_k$ 是搜索方向，$\\alpha_k  0$ 是步长。\n\n初始搜索方向是最速下降方向，$d_0 = -g_0$，其中 $g_k \\equiv \\nabla f(x_k)$ 是目标函数在 $x_k$ 处的梯度。对于后续迭代 ($k \\ge 1$)，搜索方向计算为当前负梯度与前一个搜索方向的线性组合：\n$$\nd_k = -g_k + \\beta_k d_{k-1}\n$$\n系数 $\\beta_k$至关重要，它定义了 NCG 方法的具体变体。问题指定了三种经典的 $\\beta_k$ 公式：\n\n$1$. Fletcher–Reeves (FR):\n$$\n\\beta_k^{\\mathrm{FR}} = \\frac{\\langle g_k, g_k \\rangle}{\\langle g_{k-1}, g_{k-1} \\rangle}\n$$\n其中 $\\langle \\cdot, \\cdot \\rangle$ 表示标准的欧几里得内积。\n\n$2$. Polak–Ribière–Polyak 非负约束 (PR+):\n$$\n\\beta_k^{\\mathrm{PR}+} = \\max\\left\\{0, \\frac{\\langle g_k, g_k - g_{k-1} \\rangle}{\\langle g_{k-1}, g_{k-1} \\rangle}\\right\\}\n$$\n当方法远离解时，此修正可以防止 $\\beta_k$ 值过大，从而可能提高性能。\n\n$3$. Hestenes–Stiefel 非负约束 (HS+):\n$$\n\\beta_k^{\\mathrm{HS}+} = \\max\\left\\{0, \\frac{\\langle g_k, g_k - g_{k-1} \\rangle}{\\langle d_{k-1}, g_k - g_{k-1} \\rangle}\\right\\}\n$$\n这种变体通常被认为是有效的，但需要谨慎实现，因为分母理论上可能变为零。然而，当与满足强 Wolfe 条件的线性搜索结合使用时，可以保证分母为正。\n\n步长 $\\alpha_k$ 由线性搜索过程确定，以确保函数值充分下降和算法收敛。问题要求使用强 Wolfe 条件：\n$1$. Armijo (充分下降) 条件: $f(x_k + \\alpha_k d_k) \\le f(x_k) + c_1 \\alpha_k \\langle g_k, d_k \\rangle$。\n$2$. 强曲率条件: $|\\langle \\nabla f(x_k + \\alpha_k d_k), d_k \\rangle| \\le c_2 |\\langle g_k, d_k \\rangle|$。\n常数指定为 $c_1 = 10^{-4}$ 和 $c_2 = 0.9$，这是标准选择。这些条件确保步长既有效又不过小。为此，我们利用 `scipy.optimize.line_search` 函数，因为它为找到这样的 $\\alpha_k$ 提供了稳健的实现。\n\n现代 NCG 方法的一个关键方面是重启策略。如果计算出的搜索方向 $d_k$ 不是下降方向，即 $\\langle g_k, d_k \\rangle \\ge 0$，则通过丢弃先前的方向信息并设置 $d_k = -g_k$ 来重置方法。这确保了每一步都有助于最小化目标函数。\n\n整个算法流程如下：\n$1$. 初始化 $k=0$，$x_0$，并计算 $g_0 = \\nabla f(x_0)$。设置 $d_0 = -g_0$。记录梯度范数。\n$2$. 对于 $k = 0, 1, 2, \\dots, K_{\\max}-1$:\n    a. 检查收敛性：如果 $\\lVert g_k \\rVert_2 \\le \\varepsilon = 10^{-8}$，则终止。\n    b. 确保 $d_k$ 是下降方向。如果 $\\langle g_k, d_k \\rangle \\ge 0$，则设置 $d_k = -g_k$。\n    c. 执行线性搜索，找到满足强 Wolfe 条件（常数为 $c_1=10^{-4}$ 和 $c_2=0.9$）的 $\\alpha_k  0$，使用初始猜测 $\\alpha=1$。\n    d. 更新位置：$x_{k+1} = x_k + \\alpha_k d_k$。\n    e. 计算新梯度 $g_{k+1} = \\nabla f(x_{k+1})$。\n    f. 检查非单调性：比较 $\\lVert g_{k+1} \\rVert_2$ 与 $\\lVert g_k \\rVert_2$。如果 $\\lVert g_{k+1} \\rVert_2  \\lVert g_k \\rVert_2$，则发生了非单调事件。\n    g. 使用为该测试用例指定的公式（FR、PR+ 或 HS+）计算 $\\beta_{k+1}$。\n    h. 更新搜索方向：$d_{k+1} = -g_{k+1} + \\beta_{k+1} d_k$。\n$3$. 如果循环在未收敛的情况下完成，它将在 $k = K_{\\max} = 500$ 时终止。\n\n该过程应用于三个测试用例：\n- 用例 1：使用 PR+ 方法的二维 Rosenbrock 函数，一个经典的非凸基准测试。其梯度为 $\\nabla f_1(x) = [400x_1^3 - 400x_1x_2 + 2x_1 - 2, 200(x_2 - x_1^2)]^\\top$。\n- 用例 2：使用 FR 方法的一个简单的三维严格凸二次函数。其梯度为 $\\nabla f_2(x) = x$。\n- 用例 3：使用 HS+ 方法的五维 Rosenbrock 函数，扩展了用例 1 的挑战。其梯度表达式更为复杂，其第一、最后和中间分量有不同的公式。\n\n对于每个用例，都会跟踪一个布尔标志，以确定是否有任何步骤 $k$ 导致 $\\lVert g_{k+1} \\rVert_2  \\lVert g_k \\rVert_2$。最终输出是这些布尔值的列表。该实现将封装在一个 Python 脚本中，使用 `numpy` 进行线性代数运算，使用 `scipy.optimize.line_search` 进行线性搜索，并严格遵守问题的规范。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import line_search\n\ndef solve():\n    \"\"\"\n    Solves the main problem by running NCG for three test cases.\n    \"\"\"\n\n    # --- Global Constants ---\n    C1 = 1e-4\n    C2 = 0.9\n    EPSILON = 1e-8\n    K_MAX = 500\n\n    # --- Objective Functions and Gradients ---\n\n    # Test Case 1  3: Rosenbrock function and its gradient\n    def rosenbrock_f(x):\n        return np.sum(100.0 * (x[1:] - x[:-1]**2.0)**2.0 + (1.0 - x[:-1])**2.0)\n\n    def rosenbrock_grad(x):\n        n = len(x)\n        grad = np.zeros(n)\n        # Gradient for x_i, 1  i  n (0-indexed: 0  i  n-1)\n        grad[1:-1] = (200 * (x[1:-1] - x[:-2]**2)\n                      - 400 * (x[2:] - x[1:-1]**2) * x[1:-1]\n                      - 2 * (1 - x[1:-1]))\n        # Gradient for x_1 (0-indexed: x_0)\n        grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n        # Gradient for x_n (0-indexed: x_{n-1})\n        grad[-1] = 200 * (x[-1] - x[-2]**2)\n        return grad\n\n    # Test Case 2: Spherical quadratic function and its gradient\n    def quadratic_f(x):\n        return 0.5 * np.dot(x, x)\n\n    def quadratic_grad(x):\n        return x\n\n    # --- NCG Algorithm Implementation ---\n\n    def run_ncg(f, grad_f, x0, beta_method):\n        \"\"\"\n        Runs the Nonlinear Conjugate Gradient algorithm for a given function.\n\n        Args:\n            f: The objective function.\n            grad_f: The gradient of the objective function.\n            x0: The starting point.\n            beta_method: The formula for beta ('FR', 'PR+', 'HS+').\n\n        Returns:\n            A boolean indicating if the gradient norm was non-monotonic.\n        \"\"\"\n        k = 0\n        x = np.copy(x0)\n        \n        g = grad_f(x)\n        grad_norm = np.linalg.norm(g)\n        f_val = f(x)\n        \n        d = -g\n        \n        is_non_monotonic = False\n\n        while k  K_MAX and grad_norm > EPSILON:\n            # Restart if not a descent direction\n            if np.dot(g, d) >= 0:\n                d = -g\n\n            # Perform line search satisfying strong Wolfe conditions\n            # old_old_fval=None ensures the initial step guess is 1.0\n            alpha, _, _, f_val_new, _, g_new = line_search(\n                f=f, myfprime=grad_f, xk=x, pk=d, gfk=g, old_fval=f_val,\n                c1=C1, c2=C2, old_old_fval=None\n            )\n            \n            if alpha is None:\n                # Line search failed, terminate optimization\n                break\n\n            # Update position\n            x_new = x + alpha * d\n            \n            # Check for non-monotonicity in gradient norm\n            grad_norm_new = np.linalg.norm(g_new)\n            if grad_norm_new > grad_norm:\n                is_non_monotonic = True\n\n            # Calculate beta for the next iteration\n            y = g_new - g\n            g_dot_g = np.dot(g, g)\n            \n            if g_dot_g == 0:\n                beta = 0.0 # Should not happen due to termination condition\n            elif beta_method == 'FR':\n                beta = np.dot(g_new, g_new) / g_dot_g\n            elif beta_method == 'PR+':\n                beta = max(0.0, np.dot(g_new, y) / g_dot_g)\n            elif beta_method == 'HS+':\n                denom = np.dot(d, y)\n                if denom == 0:\n                    beta = 0.0 # Restart if denominator is zero\n                else:\n                    beta = max(0.0, np.dot(g_new, y) / denom)\n            else:\n                raise ValueError(\"Unknown beta method\")\n\n            # Update search direction\n            d_new = -g_new + beta * d\n            \n            # Prepare for next iteration\n            k += 1\n            x, g, f_val, d = x_new, g_new, f_val_new, d_new\n            grad_norm = grad_norm_new\n            \n        return is_non_monotonic\n\n    # --- Test Case Definitions ---\n    test_cases = [\n        {\n            'f': rosenbrock_f,\n            'grad_f': rosenbrock_grad,\n            'x0': np.array([-1.2, 1.0]),\n            'beta_method': 'PR+'\n        },\n        {\n            'f': quadratic_f,\n            'grad_f': quadratic_grad,\n            'x0': np.array([3.0, -4.0, 1.0]),\n            'beta_method': 'FR'\n        },\n        {\n            'f': rosenbrock_f,\n            'grad_f': rosenbrock_grad,\n            'x0': np.array([-1.2, 1.0, -1.2, 1.0, -1.2]),\n            'beta_method': 'HS+'\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_ncg(case['f'], case['grad_f'], case['x0'], case['beta_method'])\n        results.append(result)\n\n    # --- Final Output ---\n    # Convert booleans to lowercase strings as per implied format\n    output_str = ','.join(map(lambda b: str(b).lower(), results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        }
    ]
}