{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of any data-driven method, it's best to start with an ideal scenario. This first practice explores the concept of \"exact\" Dynamic Mode Decomposition, where we apply the algorithm to synthetic, noise-free data generated by a known linear system . By verifying that the DMD model can perfectly reconstruct the data, you will gain hands-on confirmation of the method's theoretical foundation and see how the DMD operator $\\widehat{A}$ serves as a data-driven estimate of the underlying system's evolution.",
            "id": "2387371",
            "problem": "You are given the task of assessing whether a data-driven linear model obtained via the exact Dynamic Mode Decomposition (DMD) achieves zero reconstruction error when applied to noiseless snapshots generated by a linear, time-invariant discrete-time system. Consider a state sequence $\\{x_k\\}_{k=0}^{m-1}$ in $\\mathbb{C}^n$ produced by the recursion $x_{k+1} = A x_k$, where $A \\in \\mathbb{C}^{n \\times n}$ is a constant matrix, and define the snapshot matrices $X_1 = [x_0, x_1, \\dots, x_{m-2}] \\in \\mathbb{C}^{n \\times (m-1)}$ and $X_2 = [x_1, x_2, \\dots, x_{m-1}] \\in \\mathbb{C}^{n \\times (m-1)}$. Let $\\widehat{A}$ denote the linear operator produced by the exact Dynamic Mode Decomposition from the pair $(X_1, X_2)$ using the full numerical rank of $X_1$. Define the normalized reconstruction error as\n$$\n\\varepsilon = \\frac{\\lVert X_2 - \\widehat{A} X_1 \\rVert_F}{\\lVert X_2 \\rVert_F},\n$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm. A dataset satisfies the conditions for exact DMD if it is noise-free, generated by a linear time-invariant model as above, and the operator $\\widehat{A}$ is formed using the full numerical rank of $X_1$.\n\nConstruct three synthetic datasets by specifying $A$, $x_0$, and $m$ that meet these conditions. For each dataset, generate the snapshots $\\{x_k\\}$ by $x_{k+1} = A x_k$ and form $X_1$ and $X_2$ as defined above. For each dataset, compute $\\varepsilon$ and return a boolean indicating whether $\\varepsilon \\leq 10^{-12}$.\n\nYour program must implement this for the following test suite. In all cases where complex numbers appear, angles are in radians, and all computations are over $\\mathbb{C}$.\n\nTest Suite (each bullet fully specifies one dataset):\n\n- Dataset 1 (diagonalizable with real eigenvalues):\n  - Dimension $n = 3$, number of snapshots $m = 6$.\n  - Choose $W_1 = \\begin{bmatrix} 1  2  0 \\\\ 0  1  1 \\\\ 1  0  1 \\end{bmatrix}$ and $\\Lambda_1 = \\mathrm{diag}(0.8,\\, 1.2,\\, -0.5)$, and define $A_1 = W_1 \\Lambda_1 W_1^{-1}$.\n  - Initial state $x_0^{(1)} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}$.\n\n- Dataset 2 (includes an oscillatory complex-conjugate pair):\n  - Dimension $n = 3$, number of snapshots $m = 7$.\n  - Choose $W_2 = I_3$ (the $3 \\times 3$ identity), $\\Lambda_2 = \\mathrm{diag}\\!\\left(\\mathrm{e}^{\\mathrm{i}\\pi/6},\\, \\mathrm{e}^{-\\mathrm{i}\\pi/6},\\, 0.9\\right)$, and define $A_2 = \\Lambda_2$.\n  - Initial state $x_0^{(2)} = \\begin{bmatrix} 2 \\\\ 1 \\\\ -1 \\end{bmatrix}$.\n\n- Dataset 3 (rank-deficient snapshots due to unexcited modes):\n  - Dimension $n = 4$, number of snapshots $m = 5$.\n  - Choose $W_3 = I_4$ (the $4 \\times 4$ identity), $\\Lambda_3 = \\mathrm{diag}(0.7,\\, 0.7,\\, 0.3,\\, 1.1)$, and define $A_3 = \\Lambda_3$.\n  - Initial state $x_0^{(3)} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n\nFor each dataset, you must:\n1. Construct $A$, generate $x_k$ via $x_{k+1} = A x_k$ for $k = 0, 1, \\dots, m-2$, and form $X_1$ and $X_2$.\n2. Compute $\\widehat{A}$ by applying the exact Dynamic Mode Decomposition to $(X_1, X_2)$ using the full numerical rank of $X_1$.\n3. Compute $\\varepsilon$ as defined above and compare it to $10^{-12}$ to obtain a boolean result.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one boolean per dataset in the order given above. For example, a valid output looks like \"[True,True,True]\".",
            "solution": "The goal is to verify that the exact Dynamic Mode Decomposition (DMD) produces zero reconstruction error for noiseless snapshots generated by a linear time-invariant system when the method uses the full numerical rank of the data. The data construction and the error metric are specified as follows. Given $A \\in \\mathbb{C}^{n \\times n}$, an initial state $x_0 \\in \\mathbb{C}^n$, and an integer $m \\ge 2$, define $x_{k+1} = A x_k$ for $k = 0, \\dots, m-2$, and set\n$$\nX_1 = [x_0, x_1, \\dots, x_{m-2}] \\in \\mathbb{C}^{n \\times (m-1)}, \\quad\nX_2 = [x_1, x_2, \\dots, x_{m-1}] \\in \\mathbb{C}^{n \\times (m-1)}.\n$$\nBy construction, $X_2 = A X_1$.\n\nThe exact DMD operator $\\widehat{A}$ is the data-driven linear map on the column space of $X_1$ that maps each $x_k$ in $X_1$ to $x_{k+1}$ in $X_2$. To construct $\\widehat{A}$ from data, one uses the singular value decomposition (SVD) of $X_1$. Let $X_1$ have the (thin) SVD\n$$\nX_1 = U_r S_r V_r^*,\n$$\nwhere $U_r \\in \\mathbb{C}^{n \\times r}$, $S_r \\in \\mathbb{R}^{r \\times r}$ is diagonal with strictly positive diagonal entries, $V_r \\in \\mathbb{C}^{(m-1) \\times r}$ has orthonormal columns, and $r = \\mathrm{rank}(X_1)$ is the full numerical rank determined from the singular values. The exact DMD operator is then defined by\n$$\n\\widehat{A} = X_2 V_r S_r^{-1} U_r^*.\n$$\nWe now show that this choice guarantees zero reconstruction error in exact arithmetic for noiseless sequential data.\n\nFirst, observe that\n$$\n\\widehat{A} X_1 = X_2 V_r S_r^{-1} U_r^* \\, (U_r S_r V_r^*) = X_2 V_r V_r^*.\n$$\nThe matrix $V_r V_r^*$ is the orthogonal projector in $\\mathbb{C}^{(m-1) \\times (m-1)}$ onto the row space of $X_1$. For sequential noiseless data, $X_2 = A X_1$, which implies that the row space of $X_2$ is contained in the row space of $X_1$ because left multiplication by $A$ forms linear combinations of the rows of $X_1$. Therefore, projecting $X_2$ onto the row space of $X_1$ leaves it unchanged:\n$$\nX_2 V_r V_r^* = X_2.\n$$\nHence,\n$$\n\\widehat{A} X_1 = X_2,\n$$\nwhich implies the reconstruction error defined by\n$$\n\\varepsilon = \\frac{\\lVert X_2 - \\widehat{A} X_1 \\rVert_F}{\\lVert X_2 \\rVert_F}\n$$\nis exactly zero in exact arithmetic.\n\nIn finite-precision arithmetic, numerical roundoff leads to a small nonzero value. Using a strict tolerance such as $10^{-12}$ captures this and validates the theoretical result.\n\nNext, we justify that each dataset in the test suite satisfies the conditions required:\n\n- Dataset 1: $A_1 = W_1 \\Lambda_1 W_1^{-1}$ with $W_1$ invertible and $\\Lambda_1$ diagonal with distinct real eigenvalues. This ensures $A_1$ is diagonalizable. The snapshots are noiseless and satisfy $X_2 = A_1 X_1$. The numerical rank of $X_1$ equals the dimension of the invariant subspace excited by $x_0^{(1)}$, which here will be full due to the choice of $x_0^{(1)}$ and distinct eigenvalues.\n\n- Dataset 2: $A_2 = \\Lambda_2$ with $\\Lambda_2 = \\mathrm{diag}\\!\\left(\\mathrm{e}^{\\mathrm{i}\\pi/6}, \\mathrm{e}^{-\\mathrm{i}\\pi/6}, 0.9\\right)$. This is diagonal over $\\mathbb{C}$ and thus diagonalizable, with an oscillatory complex-conjugate eigenpair and a real decaying eigenvalue. The snapshots are noiseless and generated by $x_{k+1} = A_2 x_k$, hence $X_2 = A_2 X_1$. The numerical rank is detected from the singular values of $X_1$.\n\n- Dataset 3: $A_3 = \\Lambda_3$ with $\\Lambda_3 = \\mathrm{diag}(0.7, 0.7, 0.3, 1.1)$ and $x_0^{(3)} = [3, -2, 0, 0]^T$. Only the first two eigenmodes are excited, so the snapshots lie in a two-dimensional invariant subspace, making $X_1$ rank-deficient (rank equal to $2$) even though $n = 4$. The exact DMD constructed with the full numerical rank $r = 2$ still satisfies $\\widehat{A} X_1 = X_2$ exactly, because the data remain noiseless and $X_2 = A_3 X_1$ with row space of $X_2$ contained in that of $X_1$.\n\nAlgorithmic plan for the program:\n1. For each dataset, construct $A$ as specified, generate $x_k$ iteratively, and form $X_1$ and $X_2$.\n2. Compute the thin SVD of $X_1$ to obtain $U, S, V^*$, determine the numerical rank $r$ using the threshold $S_i > \\tau$ with $\\tau = \\max(n, m-1) \\cdot \\epsilon \\cdot S_1$, where $\\epsilon$ is machine precision.\n3. Truncate to $U_r, S_r, V_r$ and compute $\\widehat{A} = X_2 V_r S_r^{-1} U_r^*$.\n4. Compute $\\varepsilon$ and compare to $10^{-12}$ to produce the boolean result for each dataset.\n5. Output the list of booleans in the required single-line format.\n\nBecause the datasets satisfy the exact DMD conditions, the reconstruction should be numerically zero within the specified tolerance for all three cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef numerical_rank(S, shape, eps=None):\n    \"\"\"\n    Determine the numerical rank based on singular values S and matrix shape.\n    Uses threshold tau = max(shape) * eps * S[0].\n    \"\"\"\n    if eps is None:\n        eps = np.finfo(float).eps\n    if S.size == 0:\n        return 0\n    tau = max(shape) * eps * S[0]\n    return int(np.sum(S  tau))\n\ndef exact_dmd_operator(X1, X2):\n    \"\"\"\n    Compute the exact DMD operator A_hat = X2 V_r S_r^{-1} U_r^* using the full numerical rank.\n    \"\"\"\n    # Compute SVD of X1\n    U, s, Vh = np.linalg.svd(X1, full_matrices=False)\n    # Determine numerical rank\n    r = numerical_rank(s, X1.shape)\n    if r == 0:\n        # Degenerate case: no information\n        return np.zeros((X2.shape[0], X1.shape[0]), dtype=X1.dtype)\n    U_r = U[:, :r]\n    S_r_inv = np.diag(1.0 / s[:r])\n    V_r = Vh.conj().T[:, :r]\n    # Exact DMD operator\n    A_hat = X2 @ V_r @ S_r_inv @ U_r.conj().T\n    return A_hat\n\ndef generate_snapshots(A, x0, m):\n    \"\"\"\n    Generate snapshots x_0, x_1, ..., x_{m-1} using x_{k+1} = A x_k.\n    Returns X1 = [x0 ... x_{m-2}] and X2 = [x1 ... x_{m-1}].\n    \"\"\"\n    n = A.shape[0]\n    X = np.zeros((n, m), dtype=complex)\n    X[:, 0] = x0\n    for k in range(m - 1):\n        X[:, k + 1] = A @ X[:, k]\n    X1 = X[:, :-1]\n    X2 = X[:, 1:]\n    return X1, X2\n\ndef build_dataset_1():\n    # Dataset 1: A = W * Lambda * W^{-1}, real diagonalizable\n    W = np.array([[1, 2, 0],\n                  [0, 1, 1],\n                  [1, 0, 1]], dtype=float)\n    Lambda = np.diag([0.8, 1.2, -0.5])\n    Winv = np.linalg.inv(W)\n    A = (W @ Lambda @ Winv).astype(complex)\n    x0 = np.array([1, -1, 2], dtype=complex)\n    m = 6\n    return A, x0, m\n\ndef build_dataset_2():\n    # Dataset 2: complex conjugate pair and a real eigenvalue\n    lam1 = np.exp(1j * np.pi / 6.0)\n    lam2 = np.exp(-1j * np.pi / 6.0)\n    lam3 = 0.9 + 0j\n    A = np.diag([lam1, lam2, lam3]).astype(complex)\n    x0 = np.array([2, 1, -1], dtype=complex)\n    m = 7\n    return A, x0, m\n\ndef build_dataset_3():\n    # Dataset 3: rank-deficient snapshots (only first two modes excited)\n    A = np.diag([0.7, 0.7, 0.3, 1.1]).astype(complex)\n    x0 = np.array([3, -2, 0, 0], dtype=complex)\n    m = 5\n    return A, x0, m\n\ndef reconstruction_boolean(X1, X2, tol=1e-12):\n    A_hat = exact_dmd_operator(X1, X2)\n    diff = X2 - (A_hat @ X1)\n    num = np.linalg.norm(diff, ord='fro')\n    den = np.linalg.norm(X2, ord='fro')\n    # Handle the degenerate case where X2 is zero matrix\n    rel_err = 0.0 if den == 0.0 else (num / den)\n    return rel_err = tol\n\ndef solve():\n    # Define the test cases from the problem statement.\n    datasets = [\n        build_dataset_1(),\n        build_dataset_2(),\n        build_dataset_3(),\n    ]\n\n    results = []\n    for A, x0, m in datasets:\n        X1, X2 = generate_snapshots(A, x0, m)\n        results.append(reconstruction_boolean(X1, X2, tol=1e-12))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "In real-world applications, data is never perfect, and the true rank of the system is often unknown. The power of DMD lies in its use of Singular Value Decomposition (SVD) to find a low-rank approximation of the dynamics, which requires choosing a truncation rank $r$. This exercise  guides you through investigating how this critical parameter affects mode identification, demonstrating the trade-offs involved when dealing with closely spaced frequencies or low-energy modes.",
            "id": "2387367",
            "problem": "You are asked to design and implement a program that investigates how truncating the rank in a low-rank factorization influences modal identification using Dynamic Mode Decomposition (DMD). Begin from the discrete-time linear time-invariant evolution model and the core linear algebra definition that a best-fit linear operator advancing one snapshot to the next can be inferred from data. Use this foundation to derive a computational procedure that, given a user-specified truncation rank $r$, estimates the dominant modes and their associated discrete-time eigenvalues from snapshot data. Your program must quantify how many true modes are correctly identified for several carefully constructed test cases, including systems with closely spaced frequencies and systems with a low-energy mode. All angles must be handled in radians.\n\nThe task must be framed purely in mathematical and algorithmic terms:\n\n- Consider a sequence of complex-valued snapshots $\\{x_k\\}_{k=0}^{m-1}$ with $x_k \\in \\mathbb{C}^n$ generated by a linear superposition of $p$ modes,\n  $$x_k = \\sum_{j=1}^{p} a_j \\, \\phi_j \\, e^{(\\sigma_j + \\mathrm{i}\\,\\omega_j)\\,k},$$\n  where $\\phi_j \\in \\mathbb{C}^n$ are spatial mode shapes, $a_j \\in \\mathbb{R}_+$ are mode amplitudes (energy proxies), $\\sigma_j \\in \\mathbb{R}$ are discrete-time growth rates per sample, and $\\omega_j \\in \\mathbb{R}$ are angular frequencies in radians per sample. The discrete-time eigenvalue associated with mode $j$ is $\\lambda_j^\\star = e^{\\sigma_j + \\mathrm{i}\\,\\omega_j}$. The snapshots are stacked into data matrices $X = [x_0,\\dots,x_{m-2}] \\in \\mathbb{C}^{n \\times (m-1)}$ and $Y = [x_1,\\dots,x_{m-1}] \\in \\mathbb{C}^{n \\times (m-1)}$.\n- Starting from the definition of least-squares fitting of a linear operator $A$ such that $Y \\approx A X$, and the use of Singular Value Decomposition (SVD) to stabilize and enable low-rank approximation, derive a computational procedure to estimate a reduced operator and its eigenvalues, under a user-chosen rank truncation $r$. Do not introduce any other assumptions beyond the linear time-invariant model and standard linear algebra.\n- Implement a matching rule to count correct identifications: A computed eigenvalue $\\mu$ is said to match a ground-truth $\\lambda_j^\\star$ if both of the following hold simultaneously:\n  1) The wrapped absolute angular difference between their arguments is at most a tolerance $\\tau_\\omega$, i.e., the smallest absolute difference between $\\arg(\\mu)$ and $\\omega_j$ modulo $2\\pi$ is $\\le \\tau_\\omega$ (angles in radians).\n  2) The magnitude difference satisfies $\\big|\\,|\\mu| - e^{\\sigma_j}\\,\\big| \\le \\tau_m$.\n  Use $\\tau_\\omega = 10^{-3}$ and $\\tau_m = 10^{-3}$.\n- For reproducibility, when random quantities are used (e.g., spatial modes $\\phi_j$), fix the random number generator seed to $12345$ before constructing the test cases.\n\nTest Suite. Your program must construct the following five test cases, generate the corresponding snapshots, execute the rank-$r$ DMD procedure, and compute the integer number of correctly identified modes for each case. Use $n=6$ sensors and complex-valued snapshots. In all cases, set the additive noise level to zero. Angles are in radians, and time is counted in integer sample steps.\n\n- Case $1$ (baseline, well-separated): $m=300$, $p=3$, $\\omega=[0.40,\\,1.10,\\,2.20]$, $\\sigma=[0.00,\\,0.00,\\,0.00]$, $a=[1.00,\\,0.80,\\,0.50]$, $r=3$.\n- Case $2$ (closely spaced pair, adequate rank): $m=400$, $p=3$, $\\omega=[0.20,\\,0.205,\\,1.00]$, $\\sigma=[0.00,\\,0.00,\\,0.00]$, $a=[1.00,\\,1.00,\\,0.30]$, $r=3$.\n- Case $3$ (closely spaced pair, insufficient rank): $m=400$, $p=3$, $\\omega=[0.20,\\,0.205,\\,1.00]$, $\\sigma=[0.00,\\,0.00,\\,0.00]$, $a=[1.00,\\,1.00,\\,0.30]$, $r=2$.\n- Case $4$ (low-energy, decaying mode): $m=300$, $p=3$, $\\omega=[0.50,\\,1.20,\\,1.80]$, $\\sigma=[0.00,\\,0.00,\\,-0.02]$, $a=[1.00,\\,0.80,\\,0.05]$, $r=3$.\n- Case $5$ (boundary rank): $m=300$, $p=3$, $\\omega=[0.25,\\,0.80,\\,1.60]$, $\\sigma=[0.00,\\,0.00,\\,0.00]$, $a=[1.00,\\,0.80,\\,0.50]$, $r=1$.\n\nFor each case:\n- Construct $p$ random spatial modes $\\phi_j \\in \\mathbb{C}^n$ with unit $2$-norm.\n- Form snapshots $x_k$ for $k=0,\\dots,m-1$ using the above superposition (with no noise).\n- Build $X$ and $Y$ from consecutive snapshots.\n- Apply your rank-$r$ DMD procedure to compute the set of eigenvalues $\\{\\mu_\\ell\\}$.\n- Count the integer number of correctly identified modes by one-to-one matching between $\\{\\mu_\\ell\\}$ and $\\{\\lambda_j^\\star\\}$ under the stated tolerances.\n\nFinal Output Format. Your program should produce a single line of output containing a Python-style list of $5$ integers, where the $i$-th integer is the number of correctly identified modes for Case $i$. The output must be exactly of the form\n\"[v1,v2,v3,v4,v5]\"\nwith no extra spaces or text. All angle computations must be in radians. No physical units are required beyond the instruction that all angles are in radians.",
            "solution": "The problem as stated is scientifically sound, mathematically well-posed, and contains all necessary information for a unique and verifiable solution. It presents a standard task in the field of computational engineering: the implementation and analysis of the Dynamic Mode Decomposition (DMD) algorithm. The investigation is focused on how rank truncation, a key parameter in DMD, affects the identification of dynamic modes, particularly in challenging scenarios involving closely spaced frequencies, low-energy modes, and severely constrained rank. I shall therefore proceed with the derivation of the required algorithm from first principles, followed by its implementation.\n\nThe fundamental premise of DMD is that the evolution of a system, represented by a sequence of state vectors (snapshots) $\\{x_k\\}_{k=0}^{m-1}$ with $x_k \\in \\mathbb{C}^n$, can be approximated by a linear time-invariant operator $A \\in \\mathbb{C}^{n \\times n}$. This implies the relationship:\n$$\nx_{k+1} \\approx A x_k\n$$\nThis relationship is extended to the entire sequence of snapshots by forming two data matrices, $X = [x_0, x_1, \\dots, x_{m-2}]$ and $Y = [x_1, x_2, \\dots, x_{m-1}]$, both in $\\mathbb{C}^{n \\times (m-1)}$. The system dynamics are then compactly expressed as:\n$$\nY \\approx A X\n$$\nThe operator $A$ that best models the transition from $X$ to $Y$ is found by solving the least-squares problem $\\min_A \\| Y - A X \\|_F$, where $\\| \\cdot \\|_F$ is the Frobenius norm. The solution is given by $A = Y X^+$, where $X^+$ denotes the Moore-Penrose pseudoinverse of $X$.\n\nFor systems with large state dimension $n$, forming and analyzing the $n \\times n$ matrix $A$ is computationally expensive and often intractable. The goal is to determine the spectral properties of $A$ (its eigenvalues and eigenvectors), which characterize the system's dynamic modes, without explicitly forming $A$. This is achieved through a projection-based approach using the Singular Value Decomposition (SVD) of the data matrix $X$.\n\nThe SVD of $X$ is given by $X = U \\Sigma V^H$, where $U \\in \\mathbb{C}^{n \\times q}$ and $V \\in \\mathbb{C}^{(m-1) \\times q}$ are matrices with orthonormal columns, and $\\Sigma \\in \\mathbb{R}^{q \\times q}$ is a diagonal matrix of positive singular values $\\sigma_i$, with $q = \\min(n, m-1)$. The columns of $U$ form a basis that optimally captures the energy in the snapshots. The core idea of reduced-rank DMD is to project the dynamics onto a lower-dimensional subspace of rank $r \\le q$ spanned by the first $r$ columns of $U$. This is achieved by truncating the SVD:\n$$\nX \\approx X_r = U_r \\Sigma_r V_r^H\n$$\nwhere $U_r \\in \\mathbb{C}^{n \\times r}$, $\\Sigma_r \\in \\mathbb{R}^{r \\times r}$, and $V_r \\in \\mathbb{C}^{(m-1) \\times r}$ are the truncated SVD matrices. The pseudoinverse of the rank-$r$ approximated data matrix is $X_r^+ = V_r \\Sigma_r^{-1} U_r^H$. The low-rank approximation of the operator $A$ is then:\n$$\nA \\approx Y X_r^+ = Y V_r \\Sigma_r^{-1} U_r^H\n$$\nThe dynamics are projected onto the subspace spanned by the POD modes (columns of $U_r$). The reduced operator $\\tilde{A} \\in \\mathbb{C}^{r \\times r}$, which represents the action of $A$ in this projected subspace, is defined as $\\tilde{A} = U_r^H A U_r$. The eigenvalues of $\\tilde{A}$ approximate the dominant eigenvalues of $A$. Substituting the expression for $A$ and using the property that $U_r^H U_r = I_r$ (the identity matrix of size $r$), we obtain the final expression for the reduced operator:\n$$\n\\tilde{A} = U_r^H (Y V_r \\Sigma_r^{-1} U_r^H) U_r = U_r^H Y V_r \\Sigma_r^{-1}\n$$\nThe eigenvalues of this small $r \\times r$ matrix $\\tilde{A}$ are the rank-$r$ DMD eigenvalues.\n\nThe computational procedure is thus as follows:\n$1$. Construct the data matrices $X, Y$ from the time-series of snapshots.\n$2$. Compute the SVD of $X = U \\Sigma V^H$.\n$3$. Truncate the SVD components to a specified rank $r$ to obtain $U_r$, $\\Sigma_r$, and $V_r$.\n$4$. Compute the low-rank system operator representation $\\tilde{A} = U_r^H Y V_r \\Sigma_r^{-1}$.\n$5$. Compute the eigenvalues of $\\tilde{A}$, which are the desired DMD eigenvalues $\\{\\mu_\\ell\\}_{\\ell=1}^r$.\n\nFor the given problem, synthetic data is generated from a known superposition of modes, $x_k = \\sum_{j=1}^{p} a_j \\phi_j \\lambda_j^{\\star k}$, where $\\lambda_j^\\star = e^{\\sigma_j + \\mathrm{i}\\omega_j}$ are the true discrete-time eigenvalues. The computed DMD eigenvalues $\\{\\mu_\\ell\\}$ are compared against the set of true eigenvalues $\\{\\lambda_j^\\star\\}$. A match is declared if both the magnitude difference $|\\,|\\mu_\\ell| - |\\lambda_j^\\star|\\,| \\le \\tau_m$ and the wrapped angular difference $\\min(|\\arg(\\mu_\\ell) - \\arg(\\lambda_j^\\star)|, 2\\pi - |\\arg(\\mu_\\ell) - \\arg(\\lambda_j^\\star)|) \\le \\tau_\\omega$ are satisfied, using the specified tolerances $\\tau_m=10^{-3}$ and $\\tau_\\omega=10^{-3}$. A one-to-one matching protocol is used to count the number of correctly identified modes. The following code implements this procedure for the five specified test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the DMD problem for a suite of test cases.\n    Derives DMD eigenvalues for different ranks and physical parameters,\n    and counts the number of correctly identified modes.\n    \"\"\"\n\n    # Global parameters as defined in the problem statement\n    n_sensors = 6\n    tau_omega = 1e-3\n    tau_m = 1e-3\n    rng_seed = 12345\n\n    # Define the five test cases\n    test_cases = [\n        # Case 1 (baseline, well-separated)\n        {'m': 300, 'p': 3, 'omega': np.array([0.40, 1.10, 2.20]), \n         'sigma': np.array([0.00, 0.00, 0.00]), 'a': np.array([1.00, 0.80, 0.50]), 'r': 3},\n        # Case 2 (closely spaced pair, adequate rank)\n        {'m': 400, 'p': 3, 'omega': np.array([0.20, 0.205, 1.00]), \n         'sigma': np.array([0.00, 0.00, 0.00]), 'a': np.array([1.00, 1.00, 0.30]), 'r': 3},\n        # Case 3 (closely spaced pair, insufficient rank)\n        {'m': 400, 'p': 3, 'omega': np.array([0.20, 0.205, 1.00]), \n         'sigma': np.array([0.00, 0.00, 0.00]), 'a': np.array([1.00, 1.00, 0.30]), 'r': 2},\n        # Case 4 (low-energy, decaying mode)\n        {'m': 300, 'p': 3, 'omega': np.array([0.50, 1.20, 1.80]), \n         'sigma': np.array([0.00, 0.00, -0.02]), 'a': np.array([1.00, 0.80, 0.05]), 'r': 3},\n        # Case 5 (boundary rank)\n        {'m': 300, 'p': 3, 'omega': np.array([0.25, 0.80, 1.60]), \n         'sigma': np.array([0.00, 0.00, 0.00]), 'a': np.array([1.00, 0.80, 0.50]), 'r': 1},\n    ]\n\n    # Fix the random number generator seed for reproducibility\n    np.random.seed(rng_seed)\n\n    # Generate spatial modes once, as they are used across all cases\n    # The problem implies new random modes for each case. The seed is reset before each construction.\n    # To be safe and compliant, let's create random modes inside the loop.\n\n    results = []\n    for case_params in test_cases:\n        # Unpack parameters for the current case\n        m = case_params['m']\n        p = case_params['p']\n        omega_true = case_params['omega']\n        sigma_true = case_params['sigma']\n        a_true = case_params['a']\n        r = case_params['r']\n\n        # --- 1. Generate Data ---\n        # Generate p random, orthonormal spatial modes phi_j in C^n\n        # Set seed before each case to ensure case-specific reproducibility\n        np.random.seed(rng_seed)\n        phi_modes = np.zeros((n_sensors, p), dtype=np.complex128)\n        for j in range(p):\n            vec = np.random.randn(n_sensors) + 1j * np.random.randn(n_sensors)\n            phi_modes[:, j] = vec / np.linalg.norm(vec)\n\n        # True discrete-time eigenvalues\n        lambda_true = np.exp(sigma_true + 1j * omega_true)\n        \n        # Time steps\n        k_steps = np.arange(m)\n        \n        # Temporal evolution of each mode (Vandermonde matrix)\n        temporal_dynamics = np.exp(np.outer(k_steps, sigma_true + 1j * omega_true))\n        \n        # Superposition of modes to generate snapshots\n        # snapshots = phi * diag(a) * V.T\n        snapshots = phi_modes @ np.diag(a_true) @ temporal_dynamics.T\n\n        # Create data matrices X and Y\n        X = snapshots[:, :-1]\n        Y = snapshots[:, 1:]\n\n        # --- 2. Apply Rank-r DMD ---\n        # SVD of X\n        U, s, Vh = np.linalg.svd(X, full_matrices=False)\n\n        # Truncate to rank r\n        Ur = U[:, :r]\n        Sr = s[:r]\n        Vr = Vh[:r, :].conj().T\n        \n        # Compute reduced operator A_tilde\n        # A_tilde = Ur^H * Y * Vr * inv(Sr)\n        Sr_inv = np.diag(1.0 / Sr)\n        A_tilde = Ur.conj().T @ Y @ Vr @ Sr_inv\n        \n        # Compute DMD eigenvalues\n        dmd_eigvals = np.linalg.eig(A_tilde)[0]\n        \n        # --- 3. Match Eigenvalues and Count ---\n        true_mags = np.exp(sigma_true)\n        true_angles = omega_true\n\n        computed_mags = np.abs(dmd_eigvals)\n        computed_angles = np.angle(dmd_eigvals)\n\n        count = 0\n        matched_computed_indices = set()\n        \n        # One-to-one matching: for each true mode, find one unique matching computed mode\n        for j in range(p):\n            for i in range(r):\n                if i in matched_computed_indices:\n                    continue\n\n                # Check magnitude condition\n                mag_diff = np.abs(computed_mags[i] - true_mags[j])\n                is_mag_match = mag_diff = tau_m\n\n                # Check angle condition (wrapped difference)\n                angle_diff = computed_angles[i] - true_angles[j]\n                wrapped_angle_diff = np.abs((angle_diff + np.pi) % (2 * np.pi) - np.pi)\n                is_angle_match = wrapped_angle_diff = tau_omega\n                \n                if is_mag_match and is_angle_match:\n                    count += 1\n                    matched_computed_indices.add(i)\n                    break # Move to the next true mode\n\n        results.append(count)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "A powerful algorithm can be misleading if its underlying assumptions are not respected. DMD is designed to extract coherent structures from systems with deterministic dynamics, but what happens when it is applied to purely random data? This thought experiment  challenges you to reason about the output of DMD when applied to a random walk, forcing you to think critically about the interpretation of DMD modes and eigenvalues in the absence of an underlying deterministic process.",
            "id": "2387414",
            "problem": "Consider a vector-valued discrete-time process in dimension $m$, defined by the purely stochastic random walk\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_{k} + \\boldsymbol{\\eta}_{k},\n$$\nwhere $\\{\\boldsymbol{\\eta}_{k}\\}$ is an independent and identically distributed sequence of zero-mean random vectors with covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{m \\times m}$ that is positive definite, and independent of $\\mathbf{x}_{k}$. You collect $n$ consecutive snapshots $\\{\\mathbf{x}_{0}, \\mathbf{x}_{1}, \\dots, \\mathbf{x}_{n}\\}$ and form the snapshot matrices $\\mathbf{X} = [\\mathbf{x}_{0}\\ \\mathbf{x}_{1}\\ \\dots\\ \\mathbf{x}_{n-1}]$ and $\\mathbf{X}^{\\prime} = [\\mathbf{x}_{1}\\ \\mathbf{x}_{2}\\ \\dots\\ \\mathbf{x}_{n}]$. Standard Dynamic Mode Decomposition (DMD) seeks a linear map $\\mathbf{A}$ that advances states in a least-squares sense from $\\mathbf{X}$ to $\\mathbf{X}^{\\prime}$, and then interprets the eigenvalues and eigenvectors of $\\mathbf{A}$ as temporal rates and spatial modes.\n\nUsing only the definitions above, basic properties of least squares, independence, and the law of large numbers, reason about the asymptotic behavior (as $n \\to \\infty$) of the DMD fit for this random walk and the interpretability of the resulting modes. Which of the following statements are most accurate? Select all that apply.\n\nA. In expectation as $n \\to \\infty$, the least-squares DMD operator converges to the identity map, so its eigenvalues cluster near $1$, and the associated modes are not uniquely defined and do not represent coherent deterministic structures of the dynamics.\n\nB. Even for a white-noise-driven random walk, DMD will generically identify oscillatory modes at frequencies determined by the sampling interval, reflecting hidden periodicities in the process.\n\nC. As $n$ increases, DMD modes converge to the principal components of the noise covariance and therefore represent physically meaningful coherent structures of the underlying dynamics.\n\nD. Employing time-delay embeddings (Hankel-DMD) on the random walk data will reveal a dominant stable eigenvalue strictly inside the unit circle, exposing latent decay rates of the process.\n\nE. In higher dimensions with anisotropic noise covariance, the estimated DMD modes tend to align with directions of largest variance in the data; however, these directions are artifacts of stochastic forcing rather than physically meaningful modal shapes of a deterministic evolution.",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n-   Vector-valued discrete-time process in dimension $m$: $\\{\\mathbf{x}_k\\}$.\n-   Dynamic equation: $\\mathbf{x}_{k+1} = \\mathbf{x}_{k} + \\boldsymbol{\\eta}_{k}$.\n-   Noise process $\\{\\boldsymbol{\\eta}_{k}\\}$ is an independent and identically distributed (i.i.d.) sequence of random vectors.\n-   Noise properties: $\\mathbb{E}[\\boldsymbol{\\eta}_{k}] = \\mathbf{0}$ and $\\mathbb{E}[\\boldsymbol{\\eta}_{k} \\boldsymbol{\\eta}_{k}^T] = \\boldsymbol{\\Sigma}$, where $\\boldsymbol{\\Sigma}$ is a positive definite matrix in $\\mathbb{R}^{m \\times m}$.\n-   Independence: $\\boldsymbol{\\eta}_{k}$ is independent of $\\mathbf{x}_{k}$.\n-   Data matrices: $\\mathbf{X} = [\\mathbf{x}_{0}\\ \\mathbf{x}_{1}\\ \\dots\\ \\mathbf{x}_{n-1}]$ and $\\mathbf{X}^{\\prime} = [\\mathbf{x}_{1}\\ \\mathbf{x}_{2}\\ \\dots\\ \\mathbf{x}_{n}]$.\n-   DMD operator definition: $\\mathbf{A}$ is the solution to the least-squares problem $\\min_{\\mathbf{A}} \\|\\mathbf{X}^{\\prime} - \\mathbf{A}\\mathbf{X}\\|_F^2$.\n-   Task: Analyze the asymptotic behavior (as $n \\to \\infty$) of the DMD operator $\\mathbf{A}$ and the interpretation of its modes.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, defining a standard random walk process and the standard Dynamic Mode Decomposition (DMD) algorithm. The concepts are from established fields of stochastic processes and computational engineering. The problem is well-posed, providing sufficient information to analyze the asymptotic properties of the DMD estimator. The language is objective and precise. The setup is self-contained and free of contradictions. The problem is valid for analysis.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed to the solution.\n\nThe DMD operator $\\mathbf{A}$ that minimizes the Frobenius norm of the residual, $\\|\\mathbf{X}^{\\prime} - \\mathbf{A}\\mathbf{X}\\|_F$, is given by the expression $\\mathbf{A} = \\mathbf{X}^{\\prime}\\mathbf{X}^{\\dagger}$, where $\\mathbf{X}^{\\dagger}$ denotes the Moore-Penrose pseudoinverse of $\\mathbf{X}$. For a large number of snapshots $n$ where $n  m$, the pseudoinverse is typically computed as $\\mathbf{X}^{\\dagger} = \\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)^{-1}$. The operator is thus:\n$$ \\mathbf{A} = (\\mathbf{X}^{\\prime}\\mathbf{X}^T)(\\mathbf{X}\\mathbf{X}^T)^{-1} $$\nLet us define the sample correlation matrices (up to a scaling factor of $1/n$):\n$$ \\mathbf{C}_{X'X} = \\mathbf{X}^{\\prime}\\mathbf{X}^T = \\sum_{k=0}^{n-1} \\mathbf{x}_{k+1}\\mathbf{x}_k^T $$\n$$ \\mathbf{C}_{XX} = \\mathbf{X}\\mathbf{X}^T = \\sum_{k=0}^{n-1} \\mathbf{x}_{k}\\mathbf{x}_k^T $$\nSo, $\\mathbf{A} = \\mathbf{C}_{X'X}\\mathbf{C}_{XX}^{-1}$.\n\nWe substitute the process definition $\\mathbf{x}_{k+1} = \\mathbf{x}_{k} + \\boldsymbol{\\eta}_{k}$ into the expression for $\\mathbf{C}_{X'X}$:\n$$ \\mathbf{C}_{X'X} = \\sum_{k=0}^{n-1} (\\mathbf{x}_{k} + \\boldsymbol{\\eta}_{k})\\mathbf{x}_k^T = \\sum_{k=0}^{n-1} \\mathbf{x}_{k}\\mathbf{x}_k^T + \\sum_{k=0}^{n-1} \\boldsymbol{\\eta}_{k}\\mathbf{x}_k^T = \\mathbf{C}_{XX} + \\sum_{k=0}^{n-1} \\boldsymbol{\\eta}_{k}\\mathbf{x}_k^T $$\nThe DMD operator can then be written as:\n$$ \\mathbf{A} = \\left(\\mathbf{C}_{XX} + \\sum_{k=0}^{n-1} \\boldsymbol{\\eta}_{k}\\mathbf{x}_k^T\\right) \\mathbf{C}_{XX}^{-1} = \\mathbf{I} + \\left(\\sum_{k=0}^{n-1} \\boldsymbol{\\eta}_{k}\\mathbf{x}_k^T\\right) \\left(\\sum_{k=0}^{n-1} \\mathbf{x}_{k}\\mathbf{x}_k^T\\right)^{-1} $$\nTo understand the asymptotic behavior as $n \\to \\infty$, we must analyze the second term. The process $\\mathbf{x}_k$ is a random walk, a non-stationary process. The covariance of $\\mathbf{x}_k$ (assuming $\\mathbf{x}_0 = \\mathbf{0}$ without loss of generality for this analysis) is $\\mathbb{E}[\\mathbf{x}_k \\mathbf{x}_k^T] = k\\boldsymbol{\\Sigma}$.\nThe denominator term $\\sum_{k=0}^{n-1} \\mathbf{x}_{k}\\mathbf{x}_k^T$ is a sum of matrices whose \"size\" grows with $k$. The sum scales as $O(n^2)$. Specifically, results from unit root econometrics show that $\\frac{1}{n^2}\\sum_{k=0}^{n-1} \\mathbf{x}_{k}\\mathbf{x}_k^T$ converges in distribution to a random matrix related to an integral of a Wiener process.\nThe numerator term is $\\sum_{k=0}^{n-1} \\boldsymbol{\\eta}_{k}\\mathbf{x}_k^T$. The variance of this term can be shown to scale as $O(n^2)$, implying the term itself scales stochastically as $O(n)$.\n\nTherefore, the perturbation term behaves as $O(n) / O(n^2) = O(1/n)$. As $n \\to \\infty$, this term converges to the zero matrix in probability.\n$$ \\mathbf{A}_n \\xrightarrow{p} \\mathbf{I} \\quad \\text{as } n \\to \\infty $$\nThe DMD operator consistently estimates the identity operator $\\mathbf{I}$, which correctly represents the conditional expectation of the dynamics: $\\mathbb{E}[\\mathbf{x}_{k+1}|\\mathbf{x}_k] = \\mathbf{x}_k = \\mathbf{I}\\mathbf{x}_k$.\n\nWith this foundation, we evaluate each option.\n\n**A. In expectation as $n \\to \\infty$, the least-squares DMD operator converges to the identity map, so its eigenvalues cluster near $1$, and the associated modes are not uniquely defined and do not represent coherent deterministic structures of the dynamics.**\nAs derived, the operator $\\mathbf{A}$ converges to the identity matrix $\\mathbf{I}$ as $n \\to \\infty$. This implies that the eigenvalues of $\\mathbf{A}$ must converge to the eigenvalues of $\\mathbf{I}$, which are all equal to $1$. The eigenspace associated with the eigenvalue $1$ for the identity matrix is the entire space $\\mathbb{R}^m$. This means any basis for $\\mathbb{R}^m$ constitutes a valid set of eigenvectors. Thus, in the limit, the eigenvectors (DMD modes) are not uniquely defined. Since the underlying process is purely stochastic, these modes cannot represent \"coherent deterministic structures,\" as no such structures exist. The phrasing \"in expectation ... converges\" is a slighly loose but common way to state that the estimator is consistent, which it is. This statement is a correct description of the mathematical limit.\n**Verdict: Correct**\n\n**B. Even for a white-noise-driven random walk, DMD will generically identify oscillatory modes at frequencies determined by the sampling interval, reflecting hidden periodicities in the process.**\nA random walk is the discrete-time integral of a white-noise process. Its power spectrum is proportional to $1/f^2$ (for the continuous-time analogue, Brownian motion) or $|\\sin(\\omega/2)|^{-2}$ (for the discrete case), showing power concentrated at low frequencies. It contains no periodicities, hidden or otherwise. While finite-data DMD can produce spurious complex-conjugate eigenvalue pairs (implying oscillations), these are artifacts of noise and finite sampling, not a reflection of any underlying periodic dynamics. The dominant behavior as $n \\to \\infty$ is the clustering of all eigenvalues at $1$. This statement makes a false claim about the existence of hidden periodicities.\n**Verdict: Incorrect**\n\n**C. As $n$ increases, DMD modes converge to the principal components of the noise covariance and therefore represent physically meaningful coherent structures of the underlying dynamics.**\nThe principal components of the noise covariance are the eigenvectors of $\\boldsymbol{\\Sigma}$. The dominant directions of variance in the random walk data $\\{\\mathbf{x}_k\\}$, known as Principal Orthogonal Decomposition (POD) modes, are indeed aligned with the eigenvectors of $\\boldsymbol{\\Sigma}$. It is a known phenomenon that for noisy systems, DMD modes tend to approximate POD modes. So, the first part of the statement is plausible. However, the conclusion that these modes \"represent physically meaningful coherent structures of the underlying dynamics\" is fundamentally false. These directions are entirely dictated by the statistical anisotropy of the random forcing term $\\boldsymbol{\\eta}_k$ and do not correspond to any deterministic, coherent evolution. They are statistical artifacts.\n**Verdict: Incorrect**\n\n**D. Employing time-delay embeddings (Hankel-DMD) on the random walk data will reveal a dominant stable eigenvalue strictly inside the unit circle, exposing latent decay rates of the process.**\nA random walk is an integrated process, characterized by a unit root. In autoregressive model terms, it is an AR(1) process with a coefficient of $1$. Applying Hankel-DMD is akin to fitting a higher-order autoregressive model. For a unit root process, any such fit will yield an estimated dominant eigenvalue that is very close to $1$. It will be on or extremely near the unit circle. The statement that it would be \"strictly inside\" and reveal a \"latent decay rate\" is incorrect. The process is non-stationary and its variance grows; it does not decay. An estimated eigenvalue less than $1$ (e.g., $0.999$) would be a well-known finite-sample bias, not evidence of a true stable dynamic.\n**Verdict: Incorrect**\n\n**E. In higher dimensions with anisotropic noise covariance, the estimated DMD modes tend to align with directions of largest variance in the data; however, these directions are artifacts of stochastic forcing rather than physically meaningful modal shapes of a deterministic evolution.**\nThis statement correctly synthesizes several key points. \"Anisotropic noise covariance\" means $\\boldsymbol{\\Sigma}$ is not proportional to $\\mathbf{I}$. The \"directions of largest variance in the data\" are the leading POD modes. As noted in the analysis of option C, for a random walk these directions are determined by the eigenvectors of $\\boldsymbol{\\Sigma}$. The claim that estimated DMD modes \"tend to align\" with these POD modes is a correct description of the behavior observed in practice for large but finite $n$. The crucial part of the statement is the interpretation: \"these directions are artifacts of stochastic forcing rather than physically meaningful modal shapes.\" This is precisely the correct conclusion. The structure found by DMD originates from the statistics of the noise, not from a deterministic system. This option provides a nuanced and physically correct account of what DMD finds when applied to this process.\n**Verdict: Correct**",
            "answer": "$$\\boxed{AE}$$"
        }
    ]
}