{
    "hands_on_practices": [
        {
            "introduction": "Data produced by large-scale simulations, such as in computational fluid dynamics (CFD), can be enormous, posing significant challenges for storage and analysis. A common task is to compress this data while preserving its most important features. This exercise provides a direct, quantitative comparison between two data reduction techniques: a simple spatial coarsening (block averaging) and the more sophisticated Singular Value Decomposition (SVD) truncation. By implementing both and measuring their reconstruction errors, you will gain a hands-on appreciation for why SVD is considered an optimal method for data compression, as it systematically identifies and retains the most energetic spatial modes present in the data .",
            "id": "2371486",
            "problem": "Consider a two-component velocity field from computational fluid dynamics (CFD), defined over a rectangular grid with $m$ rows and $n$ columns. Let the spatial domain be $[0,1] \\times [0,1]$, with grid point coordinates $x_j = \\frac{j}{n-1}$ for $j \\in \\{0,1,\\dots,n-1\\}$ and $y_i = \\frac{i}{m-1}$ for $i \\in \\{0,1,\\dots,m-1\\}$, with the convention that if $n=1$ then $x_0 = 0$ and if $m=1$ then $y_0 = 0$. Define the horizontal and vertical components of the velocity at each grid point by\n$$\nu(i,j) = \\sin\\!\\big(2\\pi x_j\\big)\\cos\\!\\big(2\\pi y_i\\big) + 0.3 \\cos\\!\\big(4\\pi x_j + 0.1\\big)\\sin\\!\\big(2\\pi y_i\\big) + 0.1\\, y_i,\n$$\n$$\nv(i,j) = -\\cos\\!\\big(2\\pi x_j\\big)\\sin\\!\\big(2\\pi y_i\\big) + 0.25 \\sin\\!\\big(2\\pi x_j\\big)\\sin\\!\\big(4\\pi y_i + 0.3\\big) + 0.1\\, x_j,\n$$\nfor all valid indices $(i,j)$. Angles in the trigonometric functions are in radians.\n\nAggregate the field into a single real matrix $M \\in \\mathbb{R}^{(2m)\\times n}$ by stacking the components vertically:\n$$\nM = \\begin{bmatrix} U \\\\ V \\end{bmatrix}, \\quad U_{i,j} = u(i,j), \\quad V_{i,j} = v(i,j).\n$$\n\nTwo data reduction strategies and their reconstructions are to be compared:\n\n1. Singular value decomposition (SVD) truncation of rank $r$: Let the singular value decomposition (SVD) of $M$ be $M = Q \\Sigma W^\\top$ with $Q \\in \\mathbb{R}^{(2m)\\times(2m)}$, $\\Sigma \\in \\mathbb{R}^{(2m)\\times n}$, and $W \\in \\mathbb{R}^{n\\times n}$. The rank-$r$ truncated reconstruction $M_r$ is\n$$\nM_r = Q_{[:,1:r]} \\,\\Sigma_{[1:r,1:r]} \\, W_{[:,1:r]}^\\top,\n$$\nwhere $r \\in \\mathbb{N}$ satisfies $1 \\le r \\le \\min(2m,n)$ and the notation $A_{[:,1:r]}$ selects the first $r$ columns and $A_{[1:r,1:r]}$ the leading $r\\times r$ principal submatrix.\n\n2. Spatial coarsening with block-averaged reconstruction: Given positive integers $s_y$ and $s_x$, partition the $m\\times n$ grids of $U$ and $V$ into non-overlapping blocks of size $s_y \\times s_x$, except possibly at the boundaries where blocks may be smaller. For each block, replace all entries in that block by the arithmetic mean of the entries originally in that block. Perform this operation independently on $U$ and $V$ to obtain $\\widehat{U}$ and $\\widehat{V}$, and define the coarsened reconstruction $\\widehat{M} = \\begin{bmatrix} \\widehat{U} \\\\ \\widehat{V} \\end{bmatrix}$.\n\nFor each reconstruction $\\widetilde{M} \\in \\{M_r, \\widehat{M}\\}$, define the relative reconstruction error using the Frobenius norm by\n$$\n\\varepsilon(\\widetilde{M}) = \\frac{\\lVert M - \\widetilde{M} \\rVert_F}{\\lVert M \\rVert_F}.\n$$\n\nYour task is to implement a program that, for each test case specified below, constructs $M$ from the given $(m,n)$, computes the SVD-truncated reconstruction $M_r$ of rank $r$, computes the spatially coarsened reconstruction $\\widehat{M}$ using block sizes $(s_y,s_x)$, evaluates the errors $\\varepsilon(M_r)$ and $\\varepsilon(\\widehat{M})$, and returns, for each test case, the single floating-point value\n$$\n\\Delta = \\varepsilon(\\widehat{M}) - \\varepsilon(M_r).\n$$\nA positive value of $\\Delta$ indicates that the rank-$r$ SVD reconstruction has a smaller relative error than the coarsened reconstruction for that test case.\n\nTest suite (angles in radians):\n- Case $1$: $(m,n,r,s_y,s_x) = (48,64,8,4,4)$.\n- Case $2$: $(m,n,r,s_y,s_x) = (32,30,1,64,64)$.\n- Case $3$: $(m,n,r,s_y,s_x) = (64,24,6,8,6)$.\n- Case $4$: $(m,n,r,s_y,s_x) = (24,96,5,6,8)$.\n- Case $5$: $(m,n,r,s_y,s_x) = (40,50,3,1,1)$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of floating-point numbers enclosed in square brackets, ordered by the cases above, that is,\n$$\n[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5].\n$$\nNo additional text should be printed on any line. There are no physical units in this problem, and all angles are specified in radians. Each $\\Delta_k$ must be output as a floating-point number. The answer must be computed exactly as specified by the definitions above without introducing any alternative normalization or weighting.",
            "solution": "We formalize both reconstructions and the error metric from first principles. The singular value decomposition (SVD) of a real matrix $M \\in \\mathbb{R}^{(2m)\\times n}$ is a factorization $M = Q \\Sigma W^\\top$ with $Q \\in \\mathbb{R}^{(2m)\\times(2m)}$ and $W \\in \\mathbb{R}^{n\\times n}$ orthogonal, and $\\Sigma \\in \\mathbb{R}^{(2m)\\times n}$ diagonal in the sense of having nonnegative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$ on its main diagonal (with possible zero padding off its square core). For any rank parameter $r$ satisfying $1 \\le r \\le \\min(2m,n)$, the rank-$r$ truncated reconstruction is\n$$\nM_r = \\sum_{k=1}^{r} \\sigma_k \\, q_k \\, w_k^\\top = Q_{[:,1:r]}\\,\\Sigma_{[1:r,1:r]}\\,W_{[:,1:r]}^\\top,\n$$\nwhere $q_k$ and $w_k$ are the $k$-th columns of $Q$ and $W$, respectively. By the Eckart–Young–Mirsky theorem, $M_r$ is a best approximation to $M$ among all matrices of rank at most $r$ with respect to the Frobenius norm. Therefore, its relative Frobenius error is\n$$\n\\varepsilon(M_r) = \\frac{\\left\\|M - M_r\\right\\|_F}{\\left\\|M\\right\\|_F} = \\frac{\\sqrt{\\sum_{k=r+1}^{\\rho} \\sigma_k^2}}{\\sqrt{\\sum_{k=1}^{\\rho} \\sigma_k^2}},\n$$\nwhere $\\rho = \\operatorname{rank}(M)$.\n\nThe spatial coarsening reconstruction is defined via a blockwise averaging operator. For given block sizes $(s_y,s_x)$ with $s_y \\in \\mathbb{N}$ and $s_x \\in \\mathbb{N}$, we partition the index set $\\{0,1,\\dots,m-1\\} \\times \\{0,1,\\dots,n-1\\}$ into Cartesian blocks\n$$\nB_{\\alpha,\\beta} = \\{(i,j) \\,\\mid\\, \\alpha s_y \\le i \\le \\min((\\alpha+1)s_y-1,m-1),\\ \\beta s_x \\le j \\le \\min((\\beta+1)s_x-1,n-1) \\},\n$$\nfor $\\alpha \\in \\{0,1,\\dots,\\lceil m/s_y \\rceil - 1\\}$ and $\\beta \\in \\{0,1,\\dots,\\lceil n/s_x \\rceil - 1\\}$. For any matrix $A \\in \\mathbb{R}^{m\\times n}$, define the piecewise-constant projection $\\mathcal{P}_{s_y,s_x}(A)$ by assigning, for each block $B_{\\alpha,\\beta}$, the value\n$$\n\\left(\\mathcal{P}_{s_y,s_x}(A)\\right)_{i,j} = \\frac{1}{|B_{\\alpha,\\beta}|}\\sum_{(p,q)\\in B_{\\alpha,\\beta}} A_{p,q} \\quad \\text{for all } (i,j)\\in B_{\\alpha,\\beta}.\n$$\nThis operator is the orthogonal projector (with respect to the Frobenius inner product) onto the subspace of matrices that are constant on each block $B_{\\alpha,\\beta}$. Applying this operator to each component yields $\\widehat{U}=\\mathcal{P}_{s_y,s_x}(U)$ and $\\widehat{V}=\\mathcal{P}_{s_y,s_x}(V)$, and thus the reconstructed stacked matrix $\\widehat{M} = \\begin{bmatrix}\\widehat{U} \\\\ \\widehat{V}\\end{bmatrix}$. The associated relative error is\n$$\n\\varepsilon(\\widehat{M}) = \\frac{\\left\\|M - \\widehat{M}\\right\\|_F}{\\left\\|M\\right\\|_F}.\n$$\n\nThe velocity field is prescribed deterministically by\n$$\nu(i,j) = \\sin(2\\pi x_j)\\cos(2\\pi y_i) + 0.3 \\cos(4\\pi x_j + 0.1)\\sin(2\\pi y_i) + 0.1\\, y_i,\n$$\n$$\nv(i,j) = -\\cos(2\\pi x_j)\\sin(2\\pi y_i) + 0.25 \\sin(2\\pi x_j)\\sin(4\\pi y_i + 0.3) + 0.1\\, x_j,\n$$\nwith $x_j = \\frac{j}{n-1}$ for $n>1$ (and $x_0=0$ if $n=1$) and $y_i = \\frac{i}{m-1}$ for $m>1$ (and $y_0=0$ if $m=1$). Constructing $U$ and $V$ as $m\\times n$ arrays and stacking yields $M \\in \\mathbb{R}^{(2m)\\times n}$.\n\nFor each test tuple $(m,n,r,s_y,s_x)$, the computational steps follow directly from these definitions:\n- Form $M$ from the specified $m$ and $n$ by evaluating $u(i,j)$ and $v(i,j)$ at the grid points and stacking.\n- Compute the rank-$r$ SVD truncation $M_r$ and the relative Frobenius error $\\varepsilon(M_r)$.\n- Compute the block-averaged reconstructions $\\widehat{U}$ and $\\widehat{V}$ with the given $(s_y,s_x)$, stack to $\\widehat{M}$, and compute $\\varepsilon(\\widehat{M})$.\n- Report the scalar difference $\\Delta = \\varepsilon(\\widehat{M}) - \\varepsilon(M_r)$.\n\nThe test suite covers a typical case with moderate block size and rank, a boundary case where block sizes exceed the domain (single-block averaging), a tall matrix case with $2m \\gg n$, a wide matrix case with $n \\gg 2m$, and an identity coarsening case with $(s_y,s_x)=(1,1)$ where coarsening error is exactly zero. The final output is a single list $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5]$ corresponding to the cases in order.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_velocity_field(m: int, n: int):\n    # Coordinates in [0,1], handle degenerate sizes\n    if n > 1:\n        x = np.linspace(0.0, 1.0, n)\n    else:\n        x = np.array([0.0])\n    if m > 1:\n        y = np.linspace(0.0, 1.0, m)\n    else:\n        y = np.array([0.0])\n\n    X, Y = np.meshgrid(x, y, indexing='xy')\n\n    # Define u and v components as specified\n    u = np.sin(2.0 * np.pi * X) * np.cos(2.0 * np.pi * Y) \\\n        + 0.3 * np.cos(4.0 * np.pi * X + 0.1) * np.sin(2.0 * np.pi * Y) \\\n        + 0.1 * Y\n\n    v = -np.cos(2.0 * np.pi * X) * np.sin(2.0 * np.pi * Y) \\\n        + 0.25 * np.sin(2.0 * np.pi * X) * np.sin(4.0 * np.pi * Y + 0.3) \\\n        + 0.1 * X\n\n    return u, v\n\ndef stack_components(u: np.ndarray, v: np.ndarray) -> np.ndarray:\n    # Stack u and v vertically: shape (2m, n)\n    return np.vstack([u, v])\n\ndef truncated_svd_reconstruction(M: np.ndarray, r: int) -> np.ndarray:\n    # Compute rank-r truncated SVD reconstruction\n    U, s, Vh = np.linalg.svd(M, full_matrices=False)\n    r = int(r)\n    Ur = U[:, :r]\n    sr = s[:r]\n    Vhr = Vh[:r, :]\n    # Equivalent to Ur @ np.diag(sr) @ Vhr but more efficient:\n    return (Ur * sr) @ Vhr\n\ndef block_mean_reconstruction(A: np.ndarray, sy: int, sx: int) -> np.ndarray:\n    m, n = A.shape\n    R = np.empty_like(A)\n    # Iterate over blocks\n    for r0 in range(0, m, sy):\n        r1 = min(r0 + sy, m)\n        for c0 in range(0, n, sx):\n            c1 = min(c0 + sx, n)\n            block = A[r0:r1, c0:c1]\n            mean_val = block.mean() if block.size > 0 else 0.0\n            R[r0:r1, c0:c1] = mean_val\n    return R\n\ndef relative_frobenius_error(A: np.ndarray, B: np.ndarray) -> float:\n    diff = A - B\n    num = np.linalg.norm(diff, ord='fro')\n    den = np.linalg.norm(A, ord='fro')\n    # In our construction, den should be > 0, but guard just in case\n    if den == 0.0:\n        return 0.0 if num == 0.0 else float('inf')\n    return float(num / den)\n\ndef solve():\n    # Define the test cases from the problem statement as (m, n, r, s_y, s_x)\n    test_cases = [\n        (48, 64, 8, 4, 4),\n        (32, 30, 1, 64, 64),\n        (64, 24, 6, 8, 6),\n        (24, 96, 5, 6, 8),\n        (40, 50, 3, 1, 1),\n    ]\n\n    results = []\n    for m, n, r, sy, sx in test_cases:\n        # Generate field and stack\n        u, v = generate_velocity_field(m, n)\n        M = stack_components(u, v)\n\n        # Truncated SVD reconstruction and error\n        Mr = truncated_svd_reconstruction(M, r)\n        err_svd = relative_frobenius_error(M, Mr)\n\n        # Block coarsening reconstruction and error (apply per component)\n        u_hat = block_mean_reconstruction(u, sy, sx)\n        v_hat = block_mean_reconstruction(v, sy, sx)\n        M_hat = stack_components(u_hat, v_hat)\n        err_coarse = relative_frobenius_error(M, M_hat)\n\n        # Delta = coarsening error - SVD error\n        delta = err_coarse - err_svd\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    # Format with a reasonable precision for readability.\n    print(\"[\" + \",\".join(f\"{val:.10f}\" for val in results) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Many challenges in computational engineering, from image deblurring to parameter estimation, can be formulated as linear inverse problems of the form $A x = b$. However, these problems are often ill-posed, where the matrix $A$ is ill-conditioned and the data $b$ is contaminated with noise, making naive solutions wildly unstable. This practice explores how Truncated SVD (TSVD) serves as a powerful regularization technique to overcome this instability. You will discover the critical trade-off between the solution residue $\\|A \\tilde{x}_{k} - b\\|_{2}$ and the actual solution error $\\|\\tilde{x}_{k} - x\\|_{2}$, illustrating how TSVD finds a stable, physically meaningful solution by filtering out noise-dominated components .",
            "id": "2371492",
            "problem": "Consider a linear inverse problem in computational engineering modeled as $A x = b$, where $A \\in \\mathbb{R}^{m \\times n}$ is ill-conditioned, $x \\in \\mathbb{R}^{n}$ is an unknown parameter vector, and $b \\in \\mathbb{R}^{m}$ is the given data. Ill-conditioning causes small perturbations in $b$ to induce large changes in solutions, making the problem effectively ill-posed in finite-precision computation. You will investigate a data reduction approach using the Singular Value Decomposition (SVD) to construct a truncated SVD solution and compare the solution residue and the solution error.\n\nFundamental base:\n- Use the definition of the Singular Value Decomposition (SVD): any real matrix $A \\in \\mathbb{R}^{m \\times n}$ admits a factorization $A = U \\Sigma V^{\\top}$ with $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ diagonal with nonnegative entries arranged in nonincreasing order.\n- Use the least-squares principle: for overdetermined systems, the solution minimizes the Euclidean norm of the residue $\\|A \\tilde{x} - b\\|_{2}$.\n- Use the Euclidean norm properties and orthogonal invariance of the Euclidean norm.\n\nYour task:\n- For each test case, build a matrix $A$, a ground-truth vector $x$, and a data vector $b = A x + e$ where $e$ is a deterministic perturbation vector playing the role of measurement noise.\n- Compute the truncated SVD (TSVD) solution $\\tilde{x}_{k}$ by retaining only the $k$ largest singular components of $A$.\n- For each case, compute:\n  1. The residue norm $r = \\|A \\tilde{x}_{k} - b\\|_{2}$.\n  2. The solution error norm $e_{x} = \\|\\tilde{x}_{k} - x\\|_{2}$.\n- Use the Euclidean norm for all norms and express all results as real numbers rounded to exactly $6$ decimal places.\n\nConstruction rules for all test cases:\n- Define the Hilbert-type matrix $A \\in \\mathbb{R}^{m \\times n}$ entrywise by $A_{ij} = \\dfrac{1}{i + j - 1}$ for $1 \\le i \\le m$, $1 \\le j \\le n$.\n- Define the ground-truth $x \\in \\mathbb{R}^{n}$ by $x_{i} = \\dfrac{(-1)^{i}}{i}$ for $1 \\le i \\le n$.\n- Define the deterministic perturbation vector $e \\in \\mathbb{R}^{m}$ by $e_{i} = \\sigma \\cdot (-1)^{i}$ for $1 \\le i \\le m$, with scalar $\\sigma \\ge 0$ given per test.\n- Form $b = A x + e$.\n- Construct the truncated SVD solution as follows: compute an SVD of $A$ and retain only the $k$ largest singular components to build $\\tilde{x}_{k}$. If $k = 0$, define $\\tilde{x}_{0}$ to be the zero vector in $\\mathbb{R}^{n}$.\n- Compute $r = \\|A \\tilde{x}_{k} - b\\|_{2}$ and $e_{x} = \\|\\tilde{x}_{k} - x\\|_{2}$.\n- Round $r$ and $e_{x}$ to exactly $6$ decimal places.\n\nTest suite:\n- Case $1$: $m = 8$, $n = 8$, $k = 0$, $\\sigma = 10^{-6}$.\n- Case $2$: $m = 8$, $n = 8$, $k = 2$, $\\sigma = 10^{-3}$.\n- Case $3$: $m = 8$, $n = 8$, $k = 4$, $\\sigma = 10^{-3}$.\n- Case $4$: $m = 8$, $n = 8$, $k = 4$, $\\sigma = 0$.\n- Case $5$: $m = 8$, $n = 8$, $k = 8$, $\\sigma = 10^{-3}$.\n- Case $6$: $m = 10$, $n = 6$, $k = 3$, $\\sigma = 10^{-4}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of pairs in a single set of square brackets. Each pair corresponds to a test case, in the same order as listed, and is formatted as $[r,e_{x}]$ with both numbers rounded to $6$ decimal places. No spaces are allowed anywhere in the line.\n- For example, the overall output should look like $[[r_{1},e_{x,1}],[r_{2},e_{x,2}],\\dots,[r_{6},e_{x,6}]]$ where each $r_{i}$ and $e_{x,i}$ are decimal numbers with exactly $6$ digits after the decimal point.",
            "solution": "The problem presented is valid. It is scientifically grounded in the well-established field of numerical linear algebra and inverse problems, specifically the application of Singular Value Decomposition (SVD) for regularization. The problem is well-posed, with all necessary parameters and construction rules defined explicitly, ensuring a unique and computable solution for each test case. There are no logical contradictions, ambiguities, or factually incorrect premises.\n\nThe problem under consideration is the solution of a linear system of equations $A x = b$, where $A \\in \\mathbb{R}^{m \\times n}$ is an ill-conditioned matrix, $x \\in \\mathbb{R}^{n}$ is the vector of unknown parameters, and $b \\in \\mathbb{R}^{m}$ is the data vector, which is contaminated with noise. The data vector is modeled as $b = A x_{\\text{true}} + e$, where $x_{\\text{true}}$ is the ground-truth solution and $e$ is a perturbation vector representing measurement noise. The ill-conditioning of $A$ means that small perturbations in $b$ can lead to large, unphysical oscillations in the solution obtained by naively inverting the system.\n\nA standard method to address this ill-posedness is regularization, and a powerful technique for this is the Truncated Singular Value Decomposition (TSVD). The SVD of the matrix $A$ is given by:\n$$ A = U \\Sigma V^{\\top} $$\nwhere $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices ($U^{\\top}U = I_m$, $V^{\\top}V = I_n$), and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix containing the singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$, where $r = \\text{rank}(A)$. Let the columns of $U$ be $\\{u_i\\}_{i=1}^m$ and the columns of $V$ be $\\{v_i\\}_{i=1}^n$.\n\nThe standard least-squares solution, which minimizes $\\|Ax - b\\|_2$, can be expressed via the Moore-Penrose pseudoinverse $A^{\\dagger} = V \\Sigma^{\\dagger} U^{\\top}$:\n$$ \\hat{x} = A^{\\dagger}b = \\sum_{i=1}^{r} \\frac{u_i^{\\top}b}{\\sigma_i} v_i $$\nFor an ill-conditioned matrix, many singular values $\\sigma_i$ are very small. If the numerator $u_i^{\\top}b$ is not correspondingly small, the division by small $\\sigma_i$ amplifies noise from the data vector $b$, corrupting the solution.\n\nThe TSVD method regularizes the solution by truncating the summation, including only the components associated with the $k$ largest singular values, where $k \\le r$ is the truncation parameter. The TSVD solution, denoted $\\tilde{x}_k$, is defined as:\n$$ \\tilde{x}_k = \\sum_{i=1}^{k} \\frac{u_i^{\\top}b}{\\sigma_i} v_i $$\nFor the case where the truncation parameter $k=0$, the summation is empty, and the solution is defined as the zero vector, $\\tilde{x}_0 = \\mathbf{0} \\in \\mathbb{R}^n$.\n\nWe are tasked to compute two key metrics: the residue norm, $r = \\|A\\tilde{x}_k - b\\|_2$, and the solution error norm, $e_x = \\|\\tilde{x}_k - x_{\\text{true}}\\|_2$.\n\nThe residue vector is $A\\tilde{x}_k - b$. Using $Av_i = \\sigma_i u_i$, we have:\n$$ A\\tilde{x}_k = A \\left(\\sum_{i=1}^{k} \\frac{u_i^{\\top}b}{\\sigma_i} v_i\\right) = \\sum_{i=1}^{k} \\frac{u_i^{\\top}b}{\\sigma_i} (Av_i) = \\sum_{i=1}^{k} (u_i^{\\top}b) u_i $$\nThe data vector $b$ can be expanded in the orthonormal basis of $U$ as $b = \\sum_{j=1}^{m} (u_j^{\\top}b) u_j$. Thus, the residue is:\n$$ A\\tilde{x}_k - b = \\sum_{i=1}^{k} (u_i^{\\top}b) u_i - \\sum_{j=1}^{m} (u_j^{\\top}b) u_j = - \\sum_{j=k+1}^{m} (u_j^{\\top}b) u_j $$\nBy the Pythagorean theorem for orthogonal vectors, the squared norm of the residue is:\n$$ r^2 = \\|A\\tilde{x}_k - b\\|_2^2 = \\sum_{j=k+1}^{m} (u_j^{\\top}b)^2 $$\n\nThe solution error vector is $\\tilde{x}_k - x_{\\text{true}}$. We expand $\\tilde{x}_k$ using $b = Ax_{\\text{true}} + e$:\n$$ \\tilde{x}_k = \\sum_{i=1}^{k} \\frac{u_i^{\\top}(Ax_{\\text{true}} + e)}{\\sigma_i} v_i = \\sum_{i=1}^{k} \\frac{u_i^{\\top}Ax_{\\text{true}}}{\\sigma_i} v_i + \\sum_{i=1}^{k} \\frac{u_i^{\\top}e}{\\sigma_i} v_i $$\nUsing the relation $u_i^{\\top}A = (A^{\\top}u_i)^{\\top} = (V\\Sigma^{\\top}U^{\\top}u_i)^{\\top} = (\\sigma_i v_i)^{\\top} = \\sigma_i v_i^{\\top}$, the first term becomes:\n$$ \\sum_{i=1}^{k} \\frac{\\sigma_i v_i^{\\top}x_{\\text{true}}}{\\sigma_i} v_i = \\sum_{i=1}^{k} (v_i^{\\top}x_{\\text{true}}) v_i $$\nThe true solution $x_{\\text{true}}$ can be expanded in the orthonormal basis of $V$ as $x_{\\text{true}} = \\sum_{j=1}^{n} (v_j^{\\top}x_{\\text{true}}) v_j$. The error vector is therefore:\n$$ \\tilde{x}_k - x_{\\text{true}} = \\left( \\sum_{i=1}^{k} (v_i^{\\top}x_{\\text{true}}) v_i - \\sum_{j=1}^{n} (v_j^{\\top}x_{\\text{true}}) v_j \\right) + \\sum_{i=1}^{k} \\frac{u_i^{\\top}e}{\\sigma_i} v_i $$\n$$ \\tilde{x}_k - x_{\\text{true}} = \\underbrace{- \\sum_{j=k+1}^{n} (v_j^{\\top}x_{\\text{true}}) v_j}_{\\text{truncation error}} + \\underbrace{\\sum_{i=1}^{k} \\frac{u_i^{\\top}e}{\\sigma_i} v_i}_{\\text{perturbation error}} $$\nThese two error components are orthogonal, as they are linear combinations of disjoint subsets of the orthonormal basis $\\{v_i\\}$. The squared norm of the solution error is the sum of the squared norms of these components:\n$$ e_x^2 = \\|\\tilde{x}_k - x_{\\text{true}}\\|_2^2 = \\sum_{j=k+1}^{n} (v_j^{\\top}x_{\\text{true}})^2 + \\sum_{i=1}^{k} \\left(\\frac{u_i^{\\top}e}{\\sigma_i}\\right)^2 $$\nThe optimal choice of the truncation parameter $k$ involves a trade-off: increasing $k$ reduces the truncation error but increases the perturbation error, especially when the noise $e$ is significant and the singular values $\\sigma_i$ are small.\n\nFor each test case, we apply the following procedure:\n1.  Construct the Hilbert matrix $A \\in \\mathbb{R}^{m \\times n}$ with entries $A_{ij} = \\frac{1}{(i+1) + (j+1) - 1} = \\frac{1}{i+j+1}$ for $0$-indexed $i,j$.\n2.  Construct the ground-truth vector $x \\in \\mathbb{R}^{n}$ with entries $x_i = \\frac{(-1)^{i+1}}{i+1}$ for $0$-indexed $i$.\n3.  Construct the perturbation vector $e \\in \\mathbb{R}^{m}$ with entries $e_i = \\sigma \\cdot (-1)^{i+1}$ for $0$-indexed $i$.\n4.  Form the data vector $b = Ax + e$.\n5.  Compute the SVD of $A$: $A = U \\Sigma V^{\\top}$.\n6.  If $k=0$, set $\\tilde{x}_0 = \\mathbf{0}$. Otherwise, compute $\\tilde{x}_k = \\sum_{i=1}^{k} \\frac{u_i^{\\top}b}{\\sigma_i} v_i$.\n7.  Calculate the residue norm $r = \\|A\\tilde{x}_k - b\\|_2$ and the solution error norm $e_x = \\|\\tilde{x}_k - x\\|_2$.\n8.  Round both results to $6$ decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of linear inverse problems using Truncated SVD (TSVD).\n\n    For each test case, it constructs a Hilbert matrix A, a ground-truth\n    solution x, and a perturbed data vector b. It then computes the TSVD\n    solution x_tilde_k for a given truncation level k. Finally, it calculates\n    the residue norm ||A*x_tilde_k - b||_2 and the solution error norm\n    ||x_tilde_k - x||_2.\n    \"\"\"\n\n    test_cases = [\n        # (m, n, k, sigma)\n        (8, 8, 0, 1e-6),\n        (8, 8, 2, 1e-3),\n        (8, 8, 4, 1e-3),\n        (8, 8, 4, 0.0),\n        (8, 8, 8, 1e-3),\n        (10, 6, 3, 1e-4),\n    ]\n\n    results_list = []\n\n    for m, n, k, sigma in test_cases:\n        # Step 1: Construct the Hilbert matrix A\n        # Using 1-based indexing in formula, A_ij = 1/(i+j-1)\n        # In 0-based numpy, this is 1/((i+1)+(j+1)-1) = 1/(i+j+1)\n        i_indices, j_indices = np.meshgrid(np.arange(m), np.arange(n), indexing='ij')\n        A = 1.0 / (i_indices + j_indices + 1)\n\n        # Step 2: Construct the ground-truth vector x\n        # Using 1-based indexing in formula, x_i = (-1)^i/i for i=1..n\n        idx_n = np.arange(1, n + 1)\n        x_true = ((-1)**idx_n) / idx_n\n\n        # Step 3: Construct the perturbation vector e\n        # Using 1-based indexing in formula, e_i = sigma * (-1)^i for i=1..m\n        idx_m = np.arange(1, m + 1)\n        e = sigma * ((-1)**idx_m)\n\n        # Step 4: Form the data vector b\n        b = A @ x_true + e\n\n        # Step 5: Compute the SVD of A\n        # full_matrices=True to match the problem statement's definition\n        U, s, Vh = np.linalg.svd(A, full_matrices=True)\n        # Vh is V.T\n\n        # Step 6: Compute the TSVD solution x_tilde_k\n        if k == 0:\n            x_tilde_k = np.zeros(n)\n        else:\n            # Slices for the truncated components\n            s_k = s[:k]\n            U_k = U[:, :k]\n            Vh_k = Vh[:k, :]\n\n            # Compute x_tilde_k = V_k * (S_k^-1 * (U_k^T * b))\n            # This is numerically more stable than forming the pseudoinverse matrix.\n            c = U_k.T @ b\n            w = c / s_k\n            x_tilde_k = Vh_k.T @ w\n\n        # Step 7: Calculate residue and solution error norms\n        residue_norm = np.linalg.norm(A @ x_tilde_k - b)\n        solution_error_norm = np.linalg.norm(x_tilde_k - x_true)\n        \n        # Step 8: Format results to 6 decimal places\n        r_str = f\"{residue_norm:.6f}\"\n        e_x_str = f\"{solution_error_norm:.6f}\"\n        \n        results_list.append(f\"[{r_str},{e_x_str}]\")\n\n    # Final print statement in the exact required format\n    final_output = f\"[{','.join(results_list)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "In many practical scenarios, from sensor networks to recommendation systems, our data is incomplete. This exercise introduces the modern problem of matrix completion, where the goal is to \"inpaint\" or fill in missing entries of a data matrix. We will assume the complete data has a low-rank structure, a common occurrence when measurements are highly correlated. You will implement an advanced iterative algorithm that uses SVD as a core building block, repeatedly projecting an evolving estimate onto the space of low-rank matrices to recover the missing values. This practice demonstrates how SVD moves beyond a one-shot analysis tool to become a key component in sophisticated algorithms for data recovery and machine learning .",
            "id": "2371448",
            "problem": "You are given a data-reduction task arising from a sensor array with correlated channels. The data from $m$ sensors recorded at $n$ time steps is represented as a real matrix $X_{\\text{true}} \\in \\mathbb{R}^{m \\times n}$. Due to communication losses and corruption, only a subset of entries is observed. Let $\\Omega \\subset \\{1,\\dots,m\\} \\times \\{1,\\dots,n\\}$ denote the index set of observed entries, and let $P_{\\Omega}$ be the sampling operator defined by $(P_{\\Omega}(Z))_{ij} = Z_{ij}$ if $(i,j) \\in \\Omega$ and $(P_{\\Omega}(Z))_{ij} = 0$ otherwise. The goal is to reconstruct a low-rank matrix $X \\in \\mathbb{R}^{m \\times n}$ that approximates $X_{\\text{true}}$ by exploiting the assumption that $X_{\\text{true}}$ has low rank (reflecting correlated sensor behavior) and that the best rank-$k$ approximation in Frobenius norm is obtained by truncating the Singular Value Decomposition (SVD).\n\nFundamental definitions and facts to use:\n- The Singular Value Decomposition (SVD) states that any $Z \\in \\mathbb{R}^{m \\times n}$ can be written as $Z = U \\Sigma V^{\\top}$ with $U \\in \\mathbb{R}^{m \\times m}$ orthogonal, $V \\in \\mathbb{R}^{n \\times n}$ orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ diagonal and nonnegative.\n- The Frobenius norm $\\|Z\\|_{F} = \\sqrt{\\sum_{i,j} Z_{ij}^{2}}$.\n- The Eckart–Young theorem: for a given $k \\in \\mathbb{N}$, the best rank-$k$ approximation to $Z$ in Frobenius norm is obtained by truncating the SVD of $Z$ to its top $k$ singular values and corresponding singular vectors.\n\nConsider the following fixed-point iterative scheme for inpainting via low-rank approximation:\n- Initialize $X^{(0)} = 0$.\n- For iteration $t = 0,1,2,\\dots$:\n  1. Form the filled matrix $Y^{(t)} = P_{\\Omega}(Y) + P_{\\Omega^{c}}(X^{(t)})$, where $Y = P_{\\Omega}(X_{\\text{true}} + N)$ is the observed data (possibly noisy), $N$ is a noise matrix (possibly zero), and $P_{\\Omega^{c}}$ is the complement sampling operator that keeps entries not in $\\Omega$ and zeros out entries in $\\Omega$.\n  2. Compute the rank-$k$ truncated SVD approximation $X^{(t+1)}$ of $Y^{(t)}$ by retaining the top $k$ singular values and corresponding singular vectors.\n- Stop when $\\|X^{(t+1)} - X^{(t)}\\|_{F} / \\max(1, \\|X^{(t)}\\|_{F}) \\le \\varepsilon$ or when $t$ reaches a prescribed maximum number of iterations, and return $X^{(t+1)}$.\n\nImplement this algorithm and evaluate it on the following test suite. For each test, compute the relative Frobenius error $\\|X_{\\text{est}} - X_{\\text{true}}\\|_{F} / \\|X_{\\text{true}}\\|_{F}$ as a floating-point number.\n\nTest suite (all numbers are real scalars, and every matrix is given explicitly):\n\n- Case A (happy path, moderately sampled, exact low rank):\n  - Dimensions: $m = 6$, $n = 5$.\n  - Construct $X_{\\text{true}} = A_{A} B_{A}^{\\top}$ with $A_{A} \\in \\mathbb{R}^{6 \\times 2}$ and $B_{A} \\in \\mathbb{R}^{5 \\times 2}$:\n    - $A_{A} = \\begin{bmatrix}\n      1 & 0 \\\\\n      0 & 1 \\\\\n      1 & 1 \\\\\n      2 & -1 \\\\\n      -1 & 2 \\\\\n      0.5 & 1.5\n      \\end{bmatrix}$,\n      $B_{A} = \\begin{bmatrix}\n      2 & 1 \\\\\n      1 & -1 \\\\\n      0 & 2 \\\\\n      -1 & 0.5 \\\\\n      1.5 & -0.5\n      \\end{bmatrix}$.\n  - Mask $M_{A} \\in \\{0,1\\}^{6 \\times 5}$ with ones for observed entries:\n    - $M_{A} = \\begin{bmatrix}\n      1 & 1 & 0 & 1 & 0 \\\\\n      0 & 1 & 1 & 0 & 1 \\\\\n      1 & 0 & 1 & 1 & 0 \\\\\n      1 & 1 & 0 & 0 & 1 \\\\\n      0 & 1 & 1 & 1 & 0 \\\\\n      1 & 0 & 0 & 1 & 1\n      \\end{bmatrix}$.\n  - Noise $N_{A} = 0$ (all entries zero).\n  - Rank target $k = 2$.\n  - Stopping parameters: maximum iterations $t_{\\max} = 1000$, tolerance $\\varepsilon = 10^{-10}$.\n\n- Case B (boundary, fully observed, exact recovery at true rank):\n  - Dimensions: $m = 4$, $n = 4$.\n  - Construct $X_{\\text{true}} = A_{B} B_{B}^{\\top}$ with $A_{B} \\in \\mathbb{R}^{4 \\times 2}$ and $B_{B} \\in \\mathbb{R}^{4 \\times 2}$:\n    - $A_{B} = \\begin{bmatrix}\n      2 & 0 \\\\\n      0 & 1 \\\\\n      1 & -1 \\\\\n      3 & 2\n      \\end{bmatrix}$,\n      $B_{B} = \\begin{bmatrix}\n      1 & 2 \\\\\n      0.5 & -1 \\\\\n      2 & 0 \\\\\n      1 & 1\n      \\end{bmatrix}$.\n  - Mask $M_{B}$ is all ones in $\\mathbb{R}^{4 \\times 4}$.\n  - Noise $N_{B} = 0$.\n  - Rank target $k = 2$.\n  - Stopping parameters: maximum iterations $t_{\\max} = 1000$, tolerance $\\varepsilon = 10^{-12}$.\n\n- Case C (edge case, one entire row missing):\n  - Dimensions: $m = 5$, $n = 5$.\n  - Construct $X_{\\text{true}} = A_{C} B_{C}^{\\top}$ with $A_{C} \\in \\mathbb{R}^{5 \\times 2}$ and $B_{C} \\in \\mathbb{R}^{5 \\times 2}$:\n    - $A_{C} = \\begin{bmatrix}\n      1 & 0 \\\\\n      0 & 1 \\\\\n      1 & 1 \\\\\n      2 & 1 \\\\\n      -1 & 2\n      \\end{bmatrix}$,\n      $B_{C} = \\begin{bmatrix}\n      1 & 1 \\\\\n      2 & -1 \\\\\n      -1 & 0.5 \\\\\n      0 & 2 \\\\\n      1 & -2\n      \\end{bmatrix}$.\n  - Mask $M_{C} \\in \\{0,1\\}^{5 \\times 5}$:\n    - $M_{C} = \\begin{bmatrix}\n      1 & 0 & 1 & 1 & 0 \\\\\n      1 & 1 & 0 & 0 & 1 \\\\\n      0 & 0 & 0 & 0 & 0 \\\\\n      1 & 1 & 1 & 0 & 1 \\\\\n      0 & 1 & 0 & 1 & 1\n      \\end{bmatrix}$.\n  - Noise $N_{C} = 0$.\n  - Rank target $k = 2$.\n  - Stopping parameters: maximum iterations $t_{\\max} = 2000$, tolerance $\\varepsilon = 10^{-12}$.\n\n- Case D (noisy observations, moderately sampled):\n  - Dimensions: $m = 5$, $n = 4$.\n  - Construct $X_{\\text{true}} = A_{D} B_{D}^{\\top}$ with $A_{D} \\in \\mathbb{R}^{5 \\times 2}$ and $B_{D} \\in \\mathbb{R}^{4 \\times 2}$:\n    - $A_{D} = \\begin{bmatrix}\n      1 & 0 \\\\\n      0 & 1 \\\\\n      1 & -1 \\\\\n      2 & 1 \\\\\n      -1 & 2\n      \\end{bmatrix}$,\n      $B_{D} = \\begin{bmatrix}\n      1 & 2 \\\\\n      2 & 1 \\\\\n      -1 & 1 \\\\\n      0.5 & -0.5\n      \\end{bmatrix}$.\n  - Mask $M_{D} \\in \\{0,1\\}^{5 \\times 4}$:\n    - $M_{D} = \\begin{bmatrix}\n      1 & 1 & 0 & 1 \\\\\n      1 & 0 & 1 & 1 \\\\\n      1 & 1 & 0 & 0 \\\\\n      0 & 1 & 1 & 1 \\\\\n      1 & 0 & 1 & 0\n      \\end{bmatrix}$.\n  - Additive noise $N_{D} \\in \\mathbb{R}^{5 \\times 4}$:\n    - $N_{D} = \\begin{bmatrix}\n      0.01 & -0.02 & 0 & 0 \\\\\n      -0.03 & 0 & 0.02 & -0.01 \\\\\n      0.02 & 0.01 & 0 & 0 \\\\\n      0 & -0.02 & 0.03 & -0.01 \\\\\n      0.01 & 0 & -0.02 & 0\n      \\end{bmatrix}$.\n  - Rank target $k = 2$.\n  - Stopping parameters: maximum iterations $t_{\\max} = 1500$, tolerance $\\varepsilon = 10^{-10}$.\n\nImplementation requirements:\n- Implement the iterative scheme exactly as stated, using the rank-$k$ truncated SVD at each iteration on the filled matrix $Y^{(t)}$. Use the Frobenius norm for measuring convergence.\n- For each case, compute the relative Frobenius error $\\|X_{\\text{est}} - X_{\\text{true}}\\|_{F} / \\|X_{\\text{true}}\\|_{F}$ as a floating-point number rounded to six decimal places.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_{A},r_{B},r_{C},r_{D}]$), where $r_{A}$, $r_{B}$, $r_{C}$, and $r_{D}$ are the rounded relative errors for Cases A, B, C, and D, respectively. No other text should be printed.",
            "solution": "The problem presented is a well-defined task in computational engineering, specifically in the domain of data analysis and signal processing for sensor arrays. It concerns the reconstruction of a data matrix from incomplete and potentially noisy observations, under the assumption that the underlying true data is of low rank. This is a classic matrix completion problem. The proposed method is an iterative algorithm based on singular value decomposition (SVD), a standard and powerful technique for low-rank approximation.\n\nBefore proceeding, a validation of the problem statement is required.\n\n**Step 1: Extracted Givens**\n- **Data Model:** An underlying true data matrix $X_{\\text{true}} \\in \\mathbb{R}^{m \\times n}$ of low rank.\n- **Observation Model:** A set of observed entries is given by an index set $\\Omega \\subset \\{1,\\dots,m\\} \\times \\{1,\\dots,n\\}$. The observations are $Y = P_{\\Omega}(X_{\\text{true}} + N)$, where $N$ is a noise matrix and $P_{\\Omega}$ is the sampling operator that keeps entries in $\\Omega$ and zeros out others.\n- **Iterative Algorithm:**\n  - Initialization: $X^{(0)} = 0$.\n  - Iteration for $t=0, 1, 2, \\dots$:\n    1. $Y^{(t)} = P_{\\Omega}(Y) + P_{\\Omega^{c}}(X^{(t)})$, where $P_{\\Omega^{c}}$ is the complement sampling operator.\n    2. $X^{(t+1)}$ is the best rank-$k$ approximation of $Y^{(t)}$ in the Frobenius norm, obtained by truncated SVD.\n- **Stopping Criteria:** The iteration terminates when $\\|X^{(t+1)} - X^{(t)}\\|_{F} / \\max(1, \\|X^{(t)}\\|_{F}) \\le \\varepsilon$ or a maximum of $t_{\\max}$ iterations is reached.\n- **Evaluation Metric:** The relative Frobenius error $\\|X_{\\text{est}} - X_{\\text{true}}\\|_{F} / \\|X_{\\text{true}}\\|_{F}$, where $X_{\\text{est}}$ is the final estimated matrix.\n- **Test Cases:** Four distinct cases (A, B, C, D) are provided, with explicit definitions for matrices $A, B$ (used to construct $X_{\\text{true}} = AB^{\\top}$), observation masks $M$, noise matrices $N$, dimensions $m, n$, target rank $k$, and stopping parameters $\\varepsilon, t_{\\max}$.\n\n**Step 2: Validation**\nThe problem is assessed against the required criteria:\n- **Scientifically Grounded:** The problem is firmly rooted in linear algebra and numerical optimization. The use of SVD for low-rank approximation is justified by the Eckart–Young theorem. The iterative procedure is a well-known method for solving matrix completion problems, related to singular value thresholding algorithms. The setup is scientifically rigorous.\n- **Well-Posed:** All necessary parameters, matrices, and conditions are explicitly defined for each test case. The algorithm is deterministic, and the objective function (minimizing rank subject to data constraints) with the given iterative solver implies a unique computational outcome.\n- **Objective:** The problem is stated in precise mathematical language, free of ambiguity or subjective claims.\n\n**Step 3: Verdict**\nThe problem is deemed **valid**. It is a standard, well-formulated problem in numerical linear algebra with clear instructions and verifiable test cases. A solution can now be constructed.\n\n**Solution Procedure**\n\nThe objective is to implement and evaluate the specified iterative algorithm for low-rank matrix completion. The algorithm functions by repeatedly enforcing two properties: consistency with the observed data and adherence to a low-rank model.\n\nThe core iterative step is defined as:\n$X^{(t+1)} = \\mathcal{S}_k(P_{\\Omega}(Y) + P_{\\Omega^{c}}(X^{(t)}))$\nwhere $\\mathcal{S}_k(Z)$ denotes the operation of computing the best rank-$k$ approximation of a matrix $Z$. By the Eckart-Young theorem, if the SVD of $Z$ is $Z = U \\Sigma V^{\\top}$, then $\\mathcal{S}_k(Z) = U_k \\Sigma_k V_k^{\\top}$, where $U_k$ and $V_k$ are the matrices formed by the first $k$ columns of $U$ and $V$ respectively, and $\\Sigma_k$ is the diagonal matrix of the first $k$ singular values.\n\nLet us reformulate the update rule using the provided mask matrices $M \\in \\{0,1\\}^{m \\times n}$, where $M_{ij}=1$ if $(i,j) \\in \\Omega$ and $M_{ij}=0$ otherwise. The sampling operators can be expressed using element-wise (Hadamard) product $\\circ$.\n- $P_{\\Omega}(Z) = M \\circ Z$\n- $P_{\\Omega^{c}}(Z) = (J - M) \\circ Z$, where $J$ is the matrix of all ones.\n\nThe observed data matrix is $Y_{\\text{obs}} = P_{\\Omega}(X_{\\text{true}} + N) = M \\circ (X_{\\text{true}} + N)$.\nThe iterative update proceeds as follows:\n1.  **Initialize:** The estimate is initialized to the zero matrix: $X^{(0)} = 0 \\in \\mathbb{R}^{m \\times n}$.\n2.  **Iterate** for $t = 0, 1, ..., t_{\\max}-1$:\n    a. **Store previous estimate:** $X_{\\text{prev}} = X^{(t)}$.\n    b. **Fill matrix:** A temporary matrix $Y^{(t)}$ is formed by combining the known observed entries with the current estimate for the unknown entries:\n       $$Y^{(t)} = Y_{\\text{obs}} + (J-M) \\circ X^{(t)}$$\n    c. **Project to low rank:** The new estimate $X^{(t+1)}$ is obtained by computing the rank-$k$ truncated SVD of $Y^{(t)}$:\n       $$X^{(t+1)} = \\mathcal{S}_k(Y^{(t)})$$\n    d. **Check for convergence:** The process stops if the relative change between consecutive estimates is below the tolerance $\\varepsilon$:\n       $$\\frac{\\|X^{(t+1)} - X_{\\text{prev}}\\|_F}{\\max(1, \\|X_{\\text{prev}}\\|_F)} \\le \\varepsilon$$\n3.  **Output:** The final computed matrix $X_{\\text{est}} = X^{(t+1)}$ is returned.\n\nThe final evaluation for each test case is the relative Frobenius error between the estimated matrix $X_{\\text{est}}$ and the ground truth $X_{\\text{true}}$:\n$$ \\text{Error} = \\frac{\\|X_{\\text{est}} - X_{\\text{true}}\\|_F}{\\|X_{\\text{true}}\\|_F} $$\n\n- **Case A** represents a standard application where a significant fraction of entries is missing, but the underlying matrix is exactly rank-$2$. The algorithm is expected to converge to a solution with low error.\n- **Case B** is a trivial case where all entries are observed ($M=J$). The algorithm should recover $X_{\\text{true}}$ perfectly in the first iteration. $Y^{(0)} = M \\circ X_{\\text{true}} + (J-M) \\circ X^{(0)} = X_{\\text{true}} + 0 = X_{\\text{true}}$. Since $X_{\\text{true}}$ is rank-$2$, $\\mathcal{S}_2(X_{\\text{true}}) = X_{\\text{true}}$. Thus, $X^{(1)} = X_{\\text{true}}$ and the error should be near machine precision.\n- **Case C** is an edge case where an entire row is unobserved. It is a known result in matrix completion theory that recovery is not possible in this situation, as there is no information to constrain the values in that row. The algorithm will converge, but the resulting error for the missing row will be arbitrary from the perspective of the ground truth, leading to a high overall reconstruction error.\n- **Case D** includes additive noise on the observations. The algorithm will attempt to find a rank-$2$ matrix that best fits the noisy data. The reconstruction will not be exact, as the algorithm will denoise the data by projecting it onto a low-rank subspace. The final error is expected to be non-zero but small, reflecting the algorithm's ability to reject some of the noise.\n\nThe implementation will follow this logic for each test case provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and evaluate the test cases for matrix completion.\n    \"\"\"\n\n    def run_completion_algorithm(X_true, mask, noise, k, t_max, epsilon):\n        \"\"\"\n        Implements the iterative SVD-based matrix completion algorithm.\n        \n        Args:\n            X_true (np.ndarray): The ground truth matrix.\n            mask (np.ndarray): The observation mask (1s for observed, 0s for missing).\n            noise (np.ndarray): The additive noise matrix.\n            k (int): The target rank.\n            t_max (int): The maximum number of iterations.\n            epsilon (float): The convergence tolerance.\n\n        Returns:\n            np.ndarray: The estimated matrix X_est.\n        \"\"\"\n        m, n = X_true.shape\n        X_est = np.zeros((m, n))\n        Y_obs = mask * (X_true + noise)\n        \n        for _ in range(t_max):\n            X_prev = X_est.copy()\n            \n            # 1. Form the filled matrix\n            Y_filled = Y_obs + (1 - mask) * X_est\n            \n            # 2. Compute the rank-k truncated SVD approximation\n            try:\n                U, s, Vt = np.linalg.svd(Y_filled, full_matrices=False)\n                # Reconstruct from top k singular values/vectors\n                X_est = U[:, :k] @ np.diag(s[:k]) @ Vt[:k, :]\n            except np.linalg.LinAlgError:\n                # In case of non-convergence of SVD, though unlikely for these test cases.\n                break\n\n            # 3. Check for convergence\n            norm_prev = np.linalg.norm(X_prev, 'fro')\n            diff_norm = np.linalg.norm(X_est - X_prev, 'fro')\n            \n            if diff_norm / max(1.0, norm_prev) < epsilon:\n                break\n                \n        return X_est\n\n    # Define Test Cases\n    \n    # Case A\n    A_A = np.array([\n        [1, 0], [0, 1], [1, 1],\n        [2, -1], [-1, 2], [0.5, 1.5]\n    ])\n    B_A = np.array([\n        [2, 1], [1, -1], [0, 2],\n        [-1, 0.5], [1.5, -0.5]\n    ])\n    X_true_A = A_A @ B_A.T\n    M_A = np.array([\n        [1, 1, 0, 1, 0], [0, 1, 1, 0, 1], [1, 0, 1, 1, 0],\n        [1, 1, 0, 0, 1], [0, 1, 1, 1, 0], [1, 0, 0, 1, 1]\n    ])\n    N_A = np.zeros_like(X_true_A)\n    params_A = {'X_true': X_true_A, 'mask': M_A, 'noise': N_A, 'k': 2, 't_max': 1000, 'epsilon': 1e-10}\n\n    # Case B\n    A_B = np.array([[2, 0], [0, 1], [1, -1], [3, 2]])\n    B_B = np.array([[1, 2], [0.5, -1], [2, 0], [1, 1]])\n    X_true_B = A_B @ B_B.T\n    M_B = np.ones((4, 4))\n    N_B = np.zeros_like(X_true_B)\n    params_B = {'X_true': X_true_B, 'mask': M_B, 'noise': N_B, 'k': 2, 't_max': 1000, 'epsilon': 1e-12}\n\n    # Case C\n    A_C = np.array([[1, 0], [0, 1], [1, 1], [2, 1], [-1, 2]])\n    B_C = np.array([[1, 1], [2, -1], [-1, 0.5], [0, 2], [1, -2]])\n    X_true_C = A_C @ B_C.T\n    M_C = np.array([\n        [1, 0, 1, 1, 0], [1, 1, 0, 0, 1], [0, 0, 0, 0, 0],\n        [1, 1, 1, 0, 1], [0, 1, 0, 1, 1]\n    ])\n    N_C = np.zeros_like(X_true_C)\n    params_C = {'X_true': X_true_C, 'mask': M_C, 'noise': N_C, 'k': 2, 't_max': 2000, 'epsilon': 1e-12}\n\n    # Case D\n    A_D = np.array([[1, 0], [0, 1], [1, -1], [2, 1], [-1, 2]])\n    B_D = np.array([[1, 2], [2, 1], [-1, 1], [0.5, -0.5]])\n    X_true_D = A_D @ B_D.T\n    M_D = np.array([\n        [1, 1, 0, 1], [1, 0, 1, 1], [1, 1, 0, 0],\n        [0, 1, 1, 1], [1, 0, 1, 0]\n    ])\n    N_D = np.array([\n        [0.01, -0.02, 0, 0], [-0.03, 0, 0.02, -0.01],\n        [0.02, 0.01, 0, 0], [0, -0.02, 0.03, -0.01],\n        [0.01, 0, -0.02, 0]\n    ])\n    params_D = {'X_true': X_true_D, 'mask': M_D, 'noise': N_D, 'k': 2, 't_max': 1500, 'epsilon': 1e-10}\n\n    test_cases = [params_A, params_B, params_C, params_D]\n    results = []\n\n    for case_params in test_cases:\n        X_est = run_completion_algorithm(**case_params)\n        X_true = case_params['X_true']\n        \n        # Calculate relative Frobenius error\n        error = np.linalg.norm(X_est - X_true, 'fro') / np.linalg.norm(X_true, 'fro')\n        \n        # Round to six decimal places\n        rounded_error = round(error, 6)\n        results.append(rounded_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}