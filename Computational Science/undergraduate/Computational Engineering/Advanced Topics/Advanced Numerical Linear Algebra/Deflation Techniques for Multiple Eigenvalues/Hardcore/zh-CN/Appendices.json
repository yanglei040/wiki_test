{
    "hands_on_practices": [
        {
            "introduction": "理论联系实际是掌握任何计算方法的关键。本节将引导你通过一系列动手实践，从标准的Hotelling降维法入手，深入探索其在特定情况下的局限性。我们将通过一个精心构造的有瑕矩阵（defective matrix）案例，揭示该方法的根本性缺陷。通过亲手计算，你将理解为何对于包含广义特征向量的特征空间，简单的秩一修正不足以完成降维任务，从而认识到探索更稳健方法的重要性。",
            "id": "2383519",
            "problem": "考虑 $4 \\times 4$ 矩阵\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1  1  0  0 \\\\\n0  1  1  0 \\\\\n0  0  1  0 \\\\\n0  0  0  2\n\\end{pmatrix}.\n$$\n特征值 $\\lambda=1$ 是 $A$ 的一个重根特征值。令 $v$ 为 $A$ 的与 $\\lambda=1$ 相关联的单位右特征向量。定义标准 Hotelling 降阶法（秩一降阶法）为\n$$\nA_{\\mathrm{def}} \\;=\\; A \\;-\\; \\lambda\\, v v^{\\top}.\n$$\n仅使用特征值、特征向量、广义特征空间和行列式的定义，执行以下操作：\n\n- 验证 $\\lambda=1$ 是 $A$ 的一个代数重数大于1的特征值，并确定 $\\lambda=1$ 的一个单位右特征向量 $v$。\n- 构建 $A_{\\mathrm{def}}$，并根据 $A$ 的结构和广义特征向量的定义，解释为什么这种标准降阶法无法揭示与 $\\lambda=1$ 对应的广义特征空间的完整基。\n- 计算 $\\det\\!\\big(A_{\\mathrm{def}} - \\lambda I\\big)$。\n\n答案规范：\n- 请以单个实数的形式提供最终答案。\n- 无需四舍五入。",
            "solution": "首先根据所需标准对问题陈述进行验证。\n\n**第1步：提取已知条件**\n- 矩阵 $A$：\n$$\nA = \\begin{pmatrix}\n1  1  0  0 \\\\\n0  1  1  0 \\\\\n0  0  1  0 \\\\\n0  0  0  2\n\\end{pmatrix}\n$$\n- 一个特征值 $\\lambda = 1$。\n- Hotelling 降阶法的定义：$A_{\\mathrm{def}} = A - \\lambda v v^{\\top}$，其中 $v$ 是 $A$ 的与 $\\lambda$ 相关联的单位右特征向量。\n- 任务是：\n    1. 验证 $\\lambda=1$ 是 $A$ 的一个重根特征值，并找到相应的单位右特征向量 $v$。\n    2. 构建 $A_{\\mathrm{def}}$ 并解释为什么标准降阶法无法揭示与 $\\lambda=1$ 对应的广义特征空间的完整基。\n    3. 计算 $\\det(A_{\\mathrm{def}} - \\lambda I)$，其中 $\\lambda=1$。\n\n**第2步：使用提取的已知条件进行验证**\n- **科学依据：** 该问题使用了线性代数和数值分析中标准的、公认的概念，即特征值、特征向量、广义特征空间、行列式和矩阵降阶技术。这些是计算工程中的基本主题。\n- **良态性：** 该问题提供了一个特定的矩阵和清晰的定义。任务明确，可导出一个唯一的、可验证的解。\n- **客观性：** 该问题使用精确的数学语言陈述，不含主观或推测性内容。\n\n**第3步：结论和行动**\n该问题是有效的。它是数值线性代数中一个标准的、良态的问题。我现在将着手求解。\n\n**第1部分：特征值重数和特征向量**\n\n为了确定 $A$ 的特征值，我们计算特征多项式 $p(\\lambda) = \\det(A - \\lambda I)$，其中 $I$ 是 $4 \\times 4$ 单位矩阵。\n$$\nA - \\lambda I = \\begin{pmatrix}\n1-\\lambda  1  0  0 \\\\\n0  1-\\lambda  1  0 \\\\\n0  0  1-\\lambda  0 \\\\\n0  0  0  2-\\lambda\n\\end{pmatrix}\n$$\n由于这是一个上三角矩阵，其行列式是对角元素的乘积：\n$$\n\\det(A - \\lambda I) = (1-\\lambda)(1-\\lambda)(1-\\lambda)(2-\\lambda) = (1-\\lambda)^{3}(2-\\lambda)\n$$\n特征方程 $\\det(A - \\lambda I) = 0$ 的根就是特征值。特征值为 $\\lambda_1 = 1$（代数重数为 $m_a(\\lambda_1) = 3$）和 $\\lambda_2 = 2$（代数重数为 $m_a(\\lambda_2) = 1$）。这验证了 $\\lambda=1$ 是一个重数大于1的特征值。\n\n接下来，我们通过求解方程组 $(A - 1 \\cdot I)x = 0$ 来找到与 $\\lambda=1$ 对应的右特征向量 $x$。\n$$\n(A - I)x = \\begin{pmatrix}\n0  1  0  0 \\\\\n0  0  1  0 \\\\\n0  0  0  0 \\\\\n0  0  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\nx_3 \\\\\nx_4\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix}\n$$\n这个矩阵-向量方程产生以下线性方程组：\n$x_2 = 0$\n$x_3 = 0$\n$x_4 = 0$\n变量 $x_1$ 是一个自由变量。因此，与 $\\lambda=1$ 对应的任何特征向量的形式为 $c \\begin{pmatrix} 1  0  0  0 \\end{pmatrix}^{\\top}$，其中 $c$ 为任意非零标量。该特征空间是一维的，所以几何重数为 $m_g(1) = 1$。问题要求一个单位特征向量 $v$。我们选择特征空间的基向量并将其单位化。\n令 $x = \\begin{pmatrix} 1  0  0  0 \\end{pmatrix}^{\\top}$。其欧几里得范数为 $\\|x\\|_2 = \\sqrt{1^2 + 0^2 + 0^2 + 0^2} = 1$。\n因此，一个单位右特征向量是 $v = \\begin{pmatrix} 1  0  0  0 \\end{pmatrix}^{\\top}$。\n\n**第2部分：降阶矩阵及失败原因解释**\n\n我们使用 $\\lambda=1$ 和 $v = \\begin{pmatrix} 1  0  0  0 \\end{pmatrix}^{\\top}$ 来构建降阶矩阵 $A_{\\mathrm{def}}$。\n首先，我们计算秩一矩阵 $v v^{\\top}$：\n$$\nv v^{\\top} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1  0  0  0 \\end{pmatrix} = \\begin{pmatrix}\n1  0  0  0 \\\\\n0  0  0  0 \\\\\n0  0  0  0 \\\\\n0  0  0  0\n\\end{pmatrix}\n$$\n现在，我们计算 $A_{\\mathrm{def}} = A - \\lambda v v^{\\top} = A - 1 \\cdot v v^{\\top}$：\n$$\nA_{\\mathrm{def}} = \\begin{pmatrix}\n1  1  0  0 \\\\\n0  1  1  0 \\\\\n0  0  1  0 \\\\\n0  0  0  2\n\\end{pmatrix} - \\begin{pmatrix}\n1  0  0  0 \\\\\n0  0  0  0 \\\\\n0  0  0  0 \\\\\n0  0  0  0\n\\end{pmatrix} = \\begin{pmatrix}\n0  1  0  0 \\\\\n0  1  1  0 \\\\\n0  0  1  0 \\\\\n0  0  0  2\n\\end{pmatrix}\n$$\n这种降阶方法的失败在于矩阵 $A$ 对于特征值 $\\lambda=1$ 是亏损的（defective）。如果一个矩阵的某个特征值的代数重数大于其几何重数，则该矩阵对于该特征值是亏损的。此处，$m_a(1)=3$ 而 $m_g(1)=1$。\n\n这种亏损性意味着存在一个维度为3的广义特征空间，它不仅由单个特征向量 $v$ 张成，还由两个广义特征向量张成。一个广义特征向量 $x_k$ 是一个链中的向量，该链由 $(A - \\lambda I)x_k = x_{k-1}$ 定义，其中 $x_1$ 是真特征向量。对于 $k > 1$，空间 $\\ker((A-\\lambda I)^k)$ 包含广义特征向量。\n\n标准的 Hotelling 降阶法 $A_{\\mathrm{def}} = A - \\lambda v v^{\\top}$，旨在将与特征向量 $v$ 对应的特征值 $\\lambda$ 移至 $0$，使得 $A_{\\mathrm{def}}v = (A - \\lambda v v^{\\top})v = Av - \\lambda v(v^{\\top}v) = \\lambda v - \\lambda v(1) = 0$。然而，这种秩一修正仅利用了由 $v$ 张成的一维特征空间的信息。它没有考虑广义特征空间（它是 $A$ 下的一个不变子空间）的更高维结构。该降阶法未能正确地转换与 $\\lambda=1$ 相关的整个若尔当块（Jordan block）。因此，若尔当链中其他向量的影响没有从矩阵中移除，降阶矩阵 $A_{\\mathrm{def}}$ 保留了与 $\\lambda=1$ 相关的原始特征系统的分量。降阶的目标是找到其他特征值的特征向量，但在这种亏损情况下，对 $A_{\\mathrm{def}}$ 进行后续的幂迭代将收敛到 $\\lambda=1$ 的同一广义特征空间内的另一个向量。\n\n为了明确地看到这一点，让我们找出 $A_{\\mathrm{def}}$ 的特征值。特征多项式为 $\\det(A_{\\mathrm{def}} - \\mu I)$：\n$$\n\\det\\begin{pmatrix}\n-\\mu  1  0  0 \\\\\n0  1-\\mu  1  0 \\\\\n0  0  1-\\mu  0 \\\\\n0  0  0  2-\\mu\n\\end{pmatrix} = (-\\mu)(1-\\mu)^2(2-\\mu)\n$$\n$A_{\\mathrm{def}}$ 的特征值是 $\\{0, 1, 1, 2\\}$。$A$ 的原始特征值是 $\\{1, 1, 1, 2\\}$。降阶仅将特征值 $1$ 的一个实例移到了 $0$，而特征值 $1$ 的另外两个实例仍然存在。一次成功的降阶会简化特征问题，但在这里它未能消除 $\\lambda=1$ 的重数，而这正是进行降阶的首要原因。\n\n**第3部分：最终计算**\n\n问题要求计算 $\\det(A_{\\mathrm{def}} - \\lambda I)$，其中 $\\lambda = 1$。这等价于求 $A_{\\mathrm{def}}$ 的特征多项式在 $\\mu=1$ 处的值。根据以上分析，我们知道 $1$ 是 $A_{\\mathrm{def}}$ 的一个特征值。根据定义，如果 $\\mu$ 是矩阵 $M$ 的一个特征值，那么 $\\det(M - \\mu I) = 0$。\n\n为了完整性，我们直接计算它。\n$$\nA_{\\mathrm{def}} - 1 \\cdot I = \\begin{pmatrix}\n0  1  0  0 \\\\\n0  1  1  0 \\\\\n0  0  1  0 \\\\\n0  0  0  2\n\\end{pmatrix} - \\begin{pmatrix}\n1  0  0  0 \\\\\n0  1  0  0 \\\\\n0  0  1  0 \\\\\n0  0  0  1\n\\end{pmatrix} = \\begin{pmatrix}\n-1  1  0  0 \\\\\n0  0  1  0 \\\\\n0  0  0  0 \\\\\n0  0  0  1\n\\end{pmatrix}\n$$\n结果矩阵有一行完全由零组成（第三行）。任何具有零行或零列的矩阵的行列式为零。\n因此，$\\det(A_{\\mathrm{def}} - I) = 0$。",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "在认识到有瑕矩阵带来的理论挑战后，我们将注意力转向一个在实践中更普遍的问题：数值不稳定性。当特征值靠得很近（即“聚集”）时，即使是理论上正确的算法，在有限精度计算中也可能表现迥异。本练习将让你通过一个假设的浮点误差模型，定量分析并比较Hotelling降维法与更为稳健的Wielandt缩减法在处理聚集特征值时的精度差异，从而深刻体会算法设计对数值稳定性的决定性影响。",
            "id": "2383542",
            "problem": "考虑一个实对称矩阵 $A \\in \\mathbb{R}^{2 \\times 2}$，其具有两个由机器精度分隔的聚集特征值。在整个过程中，我们假设单个标量运算（加法、减法或乘法）遵循标准的浮点（FP）舍入模型：如果 $z$ 表示对浮点输入进行运算的精确结果，则计算结果为 $\\operatorname{fl}(z) = z (1 + \\delta)$，其中 $|\\delta| \\leq \\varepsilon_{m}$，$\\varepsilon_{m}$ 是单位舍入（通常称为机器ε）。假设输入可以精确地用浮点数表示，且矩阵范数为谱范数。\n\n设在精确算术中，$A$ 由下式给出\n$$\nA \\;=\\; Q \\begin{pmatrix} \\lambda  0 \\\\ 0  \\lambda + \\delta \\end{pmatrix} Q^{\\top},\n$$\n其中 $Q$ 是正交的，$\\lambda > 0$ 且 $0  \\delta \\ll \\lambda$。假设第一个特征对 $(\\lambda, v)$ 是精确已知的，其中 $v$ 是 $Q$ 的第一列。当特征值间隔等于机器精度，即 $\\delta = \\varepsilon_{m}$ 时，您将比较两种降阶策略在有限精度下估计第二个特征值 $\\lambda + \\delta$ 的效果。\n\nHotelling降阶法构造降阶矩阵\n$$\nA_{H} \\;=\\; A \\;-\\; \\lambda \\, v v^{\\top},\n$$\n然后将 $A_{H}$ 的非零特征值作为剩余特征值的估计。Wielandt缩减法使用正交投影算子\n$$\nP \\;=\\; I \\;-\\; v v^{\\top},\n$$\n且仅在 $v$ 的正交补上进行操作，将 $P A P$ 限制在 $\\mathrm{span}\\{v\\}^{\\perp}$ 上来估计剩余的特征值。在精确算术中，两种方法都能恢复出相同的第二个特征值 $\\lambda + \\delta$。\n\n然而，在浮点运算中，这两种路径在消除大的 $O(\\lambda)$ 贡献时会产生不同的后向误差。按如下方式对每种路径进行建模。\n\n- Hotelling法：显式形成的浮点降阶矩阵为\n$$\n\\widehat{A}_{H} \\;=\\; \\operatorname{fl}\\!\\left(A - \\lambda v v^{\\top}\\right) \\;=\\; \\left(A - \\lambda v v^{\\top}\\right) \\;+\\; E_{H},\n$$\n带有加性扰动 $E_{H}$，满足 $\\|E_{H}\\|_{2} \\approx c_{H}\\,\\varepsilon_{m}\\,\\lambda$，其中 $c_{H}$ 是一个与 $\\lambda$ 和 $\\delta$ 无关的适中常数。\n\n- Wielandt法：$P A P$ 在 $\\mathrm{span}\\{v\\}^{\\perp}$ 上的作用是在不显式构造 $P A P$ 的情况下实现的；也就是说，向量 $x \\in \\mathrm{span}\\{v\\}^{\\perp}$ 通过计算得到的算子进行映射\n$$\n\\widehat{B}\\,x \\;=\\; \\operatorname{fl}\\!\\big( P\\,\\operatorname{fl}(A\\,\\operatorname{fl}(P x)) \\big),\n$$\n该算子在精确算术中等于 $B x := (P A P)x$，并且在 $\\mathrm{span}\\{v\\}^{\\perp}$ 上简化为 $B x = (\\lambda + \\delta)x$。在浮点模型下，并使用 $x \\in \\mathrm{span}\\{v\\}^{\\perp}$（因此 $P x = x$ 精确成立），证明在构造 $A x$ 时，$\\mathrm{span}\\{v\\}^{\\perp}$ 上引起的算子扰动满足 $\\|\\widehat{B} - B\\|_{2} \\approx c_{W}\\,\\varepsilon_{m}\\,(\\lambda + \\delta)$，但随后的到正交补的限制使得谱估计中的 $O(\\lambda)$ 部分被抵消，留下一个由 $\\delta$ 缩放部分控制的有效特征值扰动，其大小约为 $\\approx c_{W}\\,\\varepsilon_{m}\\,\\delta$，其中 $c_{W}$ 是一个适中的常数。\n\n进一步假设对称矩阵的一阶特征值扰动理论适用：如果 $\\mu$ 是对称矩阵 $M$ 的一个单特征值，其单位特征向量为 $w$，则在小的对称扰动 $E$ 下，一阶特征值误差为 $w^{\\top} E w$，因此其大小的界为 $\\|E\\|_{2}$。\n\n使用此模型，计算在归一化情况 $\\|A\\|_{2} = \\lambda + \\delta = 1$ 且 $\\delta = \\varepsilon_{m}$ 下的比率\n$$\nR(\\varepsilon_{m}) \\;=\\; \\frac{\\text{absolute error in the Hotelling estimate of } \\lambda + \\delta}{\\text{absolute error in the Wielandt estimate of } \\lambda + \\delta}\n$$\n您的最终答案必须是仅含 $\\varepsilon_{m}$ 的单个闭式表达式。不需要舍入，也不需要单位。",
            "solution": "所述问题是有效的。它在科学上基于数值线性代数的原理，特别是特征值算法中浮点误差的分析。问题提法得当，提供了所有必要的模型和数据，并以客观、明确的语言表述。我将开始解答。\n\n目标是在指定的有限精度模型下，计算使用 Hotelling 降阶法和 Wielandt 降阶法估计特征值 $\\lambda+\\delta$ 的绝对误差之比 $R(\\varepsilon_{m})$。\n\n首先，我们分析 Hotelling 降阶法的误差。该方法涉及显式构造矩阵 $\\widehat{A}_{H} = \\operatorname{fl}(A - \\lambda v v^{\\top})$。问题将其建模为 $\\widehat{A}_{H} = A_{H} + E_{H}$，其中 $A_{H} = A - \\lambda v v^{\\top}$ 是精确的降阶矩阵，而 $E_{H}$ 是由浮点运算引起的扰动。对称矩阵 $A_H$ 的特征值是 $0$ 和 $\\lambda + \\delta$。我们旨在估计 $\\lambda + \\delta$。\n\n根据对称矩阵的一阶特征值扰动理论，特征值的误差以扰动矩阵的谱范数为界。问题给出该扰动的大小为 $\\|E_{H}\\|_{2} \\approx c_{H}\\,\\varepsilon_{m}\\,\\lambda$。因此，Hotelling 估计的绝对误差（我们记为 $\\Delta\\lambda_{H}$）具有以下量级：\n$$\n\\Delta\\lambda_{H} \\approx c_{H}\\,\\varepsilon_{m}\\,\\lambda\n$$\n这个误差与较大的特征值 $\\lambda$ 成正比，这是涉及大数相减抵消的方法的一个典型结果。\n\n接下来，我们分析 Wielandt 降阶法的误差。该方法通过有效应用算子 $B = PAP$ 在子空间 $\\mathrm{span}\\{v\\}^{\\perp}$ 上操作，其中 $P = I - vv^{\\top}$。对于任何向量 $x \\in \\mathrm{span}\\{v\\}^{\\perp}$，我们有 $Px=x$，因此 $Bx = PAx = P((\\lambda+\\delta)x) = (\\lambda+\\delta)x$。在此子空间上的精确算子是按期望的特征值 $\\lambda+\\delta$ 进行的简单缩放。\n\n问题为此过程中的误差提供了一个精细的模型。它指出，虽然计算出的作用 $\\widehat{B}x$ 引起的算子扰动 $\\|\\widehat{B} - B\\|_{2}$ 与 $\\lambda+\\delta$ 成正比，但“有效特征值扰动”要小得多。这是因为投影 $P$ 消除了与大特征值 $\\lambda$ 相关的主要误差部分。由此产生的 Wielandt 估计的绝对误差 $\\Delta\\lambda_{W}$ 的量级为：\n$$\n\\Delta\\lambda_{W} \\approx c_{W}\\,\\varepsilon_{m}\\,\\delta\n$$\n这个误差与小的特征值间隔 $\\delta$ 成正比，表明其具有优越的数值稳定性。\n\n我们现在可以构造这两个绝对误差之比 $R(\\varepsilon_{m})$：\n$$\nR(\\varepsilon_{m}) = \\frac{\\Delta\\lambda_{H}}{\\Delta\\lambda_{W}} \\approx \\frac{c_{H}\\,\\varepsilon_{m}\\,\\lambda}{c_{W}\\,\\varepsilon_{m}\\,\\delta} = \\frac{c_{H}}{c_{W}} \\frac{\\lambda}{\\delta}\n$$\n因子 $c_{H}$ 和 $c_{W}$ 被描述为依赖于浮点实现细节的“适中常数”。在算法稳定性的高层次比较中，通常假设这些常数处于同一数量级，因此我们取近似值 $c_{H}/c_{W} \\approx 1$。这是一个合理的步骤，旨在分离两种方法误差传播的结构性差异。该比率随之简化为：\n$$\nR(\\varepsilon_{m}) \\approx \\frac{\\lambda}{\\delta}\n$$\n问题要求针对一个特定情况计算这个比率。条件是：\n1. $A$ 的谱范数被归一化：$\\|A\\|_{2} = \\lambda + \\delta = 1$。\n2. 特征值间隔等于机器ε：$\\delta = \\varepsilon_{m}$。\n\n根据第一个条件，我们将 $\\lambda$ 表示为 $\\delta$ 的函数：\n$$\n\\lambda = 1 - \\delta\n$$\n将第二个条件 $\\delta = \\varepsilon_{m}$ 代入此表达式，得到：\n$$\n\\lambda = 1 - \\varepsilon_{m}\n$$\n最后，我们将 $\\lambda$ 和 $\\delta$ 的这些表达式代入误差比率 $R(\\varepsilon_{m})$ 的公式中：\n$$\nR(\\varepsilon_{m}) \\approx \\frac{1 - \\varepsilon_{m}}{\\varepsilon_{m}}\n$$\n这个结果表明，当一个小特征值与一个大特征值聚集在一起时，Wielandt 降阶法在寻找该小特征值方面明显优于 Hotelling 方法。Hotelling 方法的误差大约要大一个因子 $1/\\varepsilon_{m}$。",
            "answer": "$$\n\\boxed{\\frac{1 - \\varepsilon_{m}}{\\varepsilon_{m}}}\n$$"
        },
        {
            "introduction": "除了改进经典的投影降维法，我们还可以另辟蹊径，采用基于优化的思想来处理该问题。本练习介绍了一种优雅的“软降维”技术，它通过在瑞利商 (Rayleigh quotient) 目标函数中加入一个惩罚项，巧妙地将寻找下一个特征向量的问题转化为一个新的、等价的特征值问题。这个动手编程任务将引导你实现这一强大的方法，它在现代大规模计算中尤为重要，因为它避免了对原矩阵进行显式的数值修改。",
            "id": "2383531",
            "problem": "给定一个实对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和一个单位向量 $v_1 \\in \\mathbb{R}^n$，该向量是 $A$ 的最小特征值 $\\lambda_1$ 所对应的特征向量的一个近似。考虑带罚项的瑞利商目标函数\n$$\nq(x) \\;=\\; \\frac{x^{\\top} A x}{x^{\\top} x} \\;+\\; \\gamma \\, (v_1^{\\top} x)^2,\n$$\n其罚参数为 $\\gamma > 0$，以及约束最小化问题 $\\min_{\\|x\\|_2 = 1} q(x)$。请仅从对称矩阵的特征对和瑞利商的基本定义出发，推导如何通过将其转换为一个等价的特征值问题来计算解此问题的向量 $x_\\star$。然后，解释如何将 $x_\\star$ 转换为 $A$ 在 $\\lambda_1$ 之后的“下一个”特征值（即，在与 $v_1$ 近似正交的向量中 $A$ 的最小特征值）的数值估计 $\\widehat{\\lambda}_2$。您的程序必须实现这一变换，并计算 $A$ 在 $x_\\star$ 处的瑞利商作为 $\\widehat{\\lambda}_2$。\n\n您的程序必须以数值可靠的方式实现以下算法任务：\n- 构建秩为1的更新 $B \\;=\\; A \\;+\\; \\gamma \\, v_1 v_1^{\\top}$。\n- 计算 $B$ 的最小特征值对应的单位范数特征向量 $x_\\star$。\n- 返回估计值 $\\widehat{\\lambda}_2 \\;=\\; x_\\star^{\\top} A x_\\star$。\n\n使用以下测试套件。对于每种情况，按规定计算并报告所需的输出。\n\n- 测试 1（理想情况，谱不重叠）：\n  - $A_1 \\,=\\, \\mathrm{diag}(1,\\,2,\\,4) \\in \\mathbb{R}^{3 \\times 3}$，\n  - $v_{1,1} \\,=\\, [1,\\,0,\\,0]^{\\top}$，\n  - $\\gamma_1 \\,=\\, 3$。\n  - 输出：一个等于 $\\widehat{\\lambda}_2$ 的浮点数。\n\n- 测试 2（前两个特征值近乎简并）：\n  - $A_2 \\,=\\, \\mathrm{diag}(1.0,\\,1.001,\\,5.0) \\in \\mathbb{R}^{3 \\times 3}$，\n  - $v_{1,2} \\,=\\, [1,\\,0,\\,0]^{\\top}$，\n  - $\\gamma_2 \\,=\\, 0.01$。\n  - 输出：一个等于 $\\widehat{\\lambda}_2$ 的浮点数。\n\n- 测试 3（最小特征值具有精确重数）：\n  - $A_3 \\,=\\, \\mathrm{diag}(2.0,\\,2.0,\\,7.0) \\in \\mathbb{R}^{3 \\times 3}$，\n  - $v_{1,3} \\,=\\, [1,\\,0,\\,0]^{\\top}$，\n  - $\\gamma_3 \\,=\\, 5$。\n  - 输出：一个等于 $\\widehat{\\lambda}_2$ 的浮点数。\n\n- 测试 4（不精确的紧缩方向）：\n  - $A_4 \\,=\\, \\mathrm{diag}(1.0,\\,2.0,\\,3.0) \\in \\mathbb{R}^{3 \\times 3}$，\n  - 令角度为 $\\theta \\,=\\, 5$ 度（角度必须以度为单位进行解释），\n  - $v_{1,4} \\,=\\, [\\cos(\\theta),\\,\\sin(\\theta),\\,0]^{\\top}$ 归一化为单位长度，\n  - $\\gamma_4 \\,=\\, 10$。\n  - 输出：一个等于 $\\widehat{\\lambda}_2$ 的浮点数。\n\n- 测试 5（罚项过小导致的失败情况）：\n  - $A_5 \\,=\\, \\mathrm{diag}(1.0,\\,1.001,\\,3.0) \\in \\mathbb{R}^{3 \\times 3}$，\n  - $v_{1,5} \\,=\\, [1,\\,0,\\,0]^{\\top}$，\n  - $\\gamma_5 \\,=\\, 0.0001$。\n  - 输出：一个按如下方式定义的布尔值。令 $\\widehat{\\lambda}_2$ 如上计算，并令 $\\lambda_1 \\,=\\, 1.0$ 和 $\\lambda_2 \\,=\\, 1.001$。输出表达式 $\\big|\\widehat{\\lambda}_2 - \\lambda_2\\big|  \\big|\\widehat{\\lambda}_2 - \\lambda_1\\big|$ 的布尔值。\n\n重要实现细节：\n- 所有矩阵均为实对称矩阵。所有向量在需要时必须归一化为单位长度。\n- 您不能依赖任何随机性；请使用确定性的线性代数例程。\n- 凡涉及角度，必须按规定以度为单位进行解释。\n- 本问题不涉及物理单位。\n\n最终输出格式：\n- 您的程序应产生单行输出，其中包含测试 1 到 5 的结果，形式为方括号内以逗号分隔的列表，例如 $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5]$。对于测试 1-4，每个 $\\text{result}_k$ 是一个浮点数。对于测试 5，$\\text{result}_5$ 是一个布尔值。不得打印任何额外文本。",
            "solution": "所给问题是数值线性代数中的一个有效练习，具体涉及特征值问题的紧缩技术。我们将进行形式化推导。\n\n目标是解决以下约束最小化问题：\n$$\n\\min_{x \\in \\mathbb{R}^n, \\|x\\|_2 = 1} q(x)\n$$\n其中目标函数是带罚项的瑞利商：\n$$\nq(x) = \\frac{x^{\\top} A x}{x^{\\top} x} + \\gamma (v_1^{\\top} x)^2\n$$\n此处，$A \\in \\mathbb{R}^{n \\times n}$ 是一个实对称矩阵，$v_1 \\in \\mathbb{R}^n$ 是一个给定的单位向量，它近似于 $A$ 最小特征值对应的特征向量，而 $\\gamma > 0$ 是一个标量罚参数。\n\n首先，我们将问题形式化。约束 $\\|x\\|_2 = 1$ 意味着 $x^{\\top} x = 1$。将此代入目标函数，可将问题简化为在单位球面上最小化以下函数：\n$$\nf(x) = x^{\\top} A x + \\gamma (v_1^{\\top} x)^2 \\quad \\text{subject to} \\quad x^{\\top} x = 1\n$$\n罚项 $\\gamma (v_1^{\\top} x)^2$ 可以重写为二次型。由于 $v_1^{\\top} x$ 是一个标量，其平方为 $(v_1^{\\top} x)(v_1^{\\top} x) = (x^{\\top} v_1)(v_1^{\\top} x)$。根据矩阵乘法的结合律，该表达式等价于 $x^{\\top} (v_1 v_1^{\\top}) x$。项 $v_1 v_1^{\\top}$ 是向量 $v_1$ 与自身的外积，这是一个对称的秩一矩阵。\n\n将此代回 $f(x)$ 的表达式，我们可以合并这两个二次型：\n$$\nf(x) = x^{\\top} A x + \\gamma x^{\\top} (v_1 v_1^{\\top}) x = x^{\\top} (A + \\gamma v_1 v_1^{\\top}) x\n$$\n让我们定义一个新矩阵 $B = A + \\gamma v_1 v_1^{\\top}$。由于 $A$ 是对称的且 $v_1 v_1^{\\top}$ 是对称的，它们的和 $B$ 也是一个实对称矩阵。最小化问题现在转换为：\n$$\n\\min_{x^{\\top} x = 1} x^{\\top} B x\n$$\n这是寻找矩阵 $B$ 的瑞利商最小值的标准问题。根据 Rayleigh-Ritz 定理，对于任何对称矩阵 $B$，商 $\\frac{x^{\\top} B x}{x^{\\top} x}$ 的最小值是 $B$ 的最小特征值，记为 $\\mu_{\\min}(B)$。这个最小值当且仅当 $x$ 是对应的特征向量时才能达到。因此，解决我们原始最小化问题的向量 $x_\\star$ 是与 $B$ 的最小特征值相关联的单位范数特征向量。\n\n现在，我们必须理解这个过程如何与寻找 $A$ 的“下一个”特征值相关。设 $A$ 的特征对为 $(\\lambda_i, u_i)$，$i=1, \\dots, n$，并按 $\\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n$ 的顺序排列。给定的向量 $v_1$ 是 $u_1$ 的一个近似。\n\n考虑近似是精确的理想情况，即 $v_1 = u_1$。矩阵为 $B = A + \\gamma u_1 u_1^{\\top}$。让我们考察 $B$ 对 $A$ 的特征向量的作用：\n1. 对于 $A$ 的任意特征向量 $u_i$（其中 $i > 1$），我们假设它与 $u_1$ 正交（对于对称矩阵，即使存在重特征值，这也总是可能的）。那么 $u_1^{\\top} u_i = 0$。将 $B$ 应用于 $u_i$ 可得：\n$$\nB u_i = A u_i + \\gamma u_1 (u_1^{\\top} u_i) = \\lambda_i u_i + \\gamma u_1 (0) = \\lambda_i u_i\n$$\n这表明对于 $i=2, \\dots, n$，$(\\lambda_i, u_i)$ 也是 $B$ 的特征对。\n\n2. 对于特征向量 $u_1$，由于它是单位向量，我们有 $u_1^{\\top} u_1 = 1$。将 $B$ 应用于 $u_1$ 可得：\n$$\nB u_1 = A u_1 + \\gamma u_1 (u_1^{\\top} u_1) = \\lambda_1 u_1 + \\gamma u_1(1) = (\\lambda_1 + \\gamma) u_1\n$$\n所以，$(\\lambda_1 + \\gamma, u_1)$ 是 $B$ 的一个特征对。\n\n因此，$B$ 的谱为 $\\{\\lambda_1 + \\gamma, \\lambda_2, \\lambda_3, \\dots, \\lambda_n\\}$。罚项的目的是将与已知特征向量方向 $u_1$ 对应的特征值“向上移动”，从而暴露出下一个特征值 $\\lambda_2$ 作为最小值。为此，$B$ 的最小特征值必须是 $\\lambda_2$。这要求 $\\lambda_2  \\lambda_1 + \\gamma$。整理可得，罚参数 $\\gamma$ 的选择必须满足 $\\gamma > \\lambda_2 - \\lambda_1$。如果此条件成立，$B$ 的最小特征值就是 $\\lambda_2$，其对应的特征向量是 $u_2$。\n\n在 $v_1$ 只是 $u_1$ 的一个近似的一般情况下，$A$ 的特征向量（$u_1$ 除外）并非严格是 $B$ 的特征向量。然而，对于足够大的 $\\gamma$，$B$ 的与方向 $v_1$ 相关的特征值会显著增加。此时，与 $B$ 的最小特征值对应的特征向量 $x_\\star$ 将是 $u_2$（即 $A$ 的第二小特征值 $\\lambda_2$ 对应的特征向量）的一个近似。该近似的质量取决于 $v_1$ 的准确性以及特征值之间的间隙。\n\n最后，为了获得特征值 $\\lambda_2$ 的数值估计 $\\widehat{\\lambda}_2$，我们使用已求得的向量 $x_\\star$。给定近似特征向量 $x_\\star$ 时，对 $A$ 的特征值的最佳估计是其关于 $A$ 的瑞利商。由于 $x_\\star$ 是单位向量，这可以简化为：\n$$\n\\widehat{\\lambda}_2 = x_\\star^{\\top} A x_\\star\n$$\n推导至此完成。算法与问题描述中陈述的完全一致：构造 $B = A + \\gamma v_1 v_1^{\\top}$，找到与 $B$ 的最小特征值对应的特征向量 $x_\\star$，并计算瑞利商 $\\widehat{\\lambda}_2 = x_\\star^{\\top} A x_\\star$ 作为所需的估计值。测试 5 展示了当 $\\gamma$ 过小以至于不满足条件 $\\gamma > \\lambda_2 - \\lambda_1$ 时该方法的失败情况，此时最小化向量 $x_\\star$ 仍与 $v_1$ 的方向对齐，而不会转移去近似 $u_2$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the series of eigenvalue deflation problems.\n    \"\"\"\n\n    test_cases = [\n        # Test 1: Happy path\n        {\n            'A': np.diag([1.0, 2.0, 4.0]),\n            'v1_unnormalized': np.array([1.0, 0.0, 0.0]),\n            'gamma': 3.0,\n            'task': 'compute_lambda_hat_2'\n        },\n        # Test 2: Near-degenerate eigenvalues\n        {\n            'A': np.diag([1.0, 1.001, 5.0]),\n            'v1_unnormalized': np.array([1.0, 0.0, 0.0]),\n            'gamma': 0.01,\n            'task': 'compute_lambda_hat_2'\n        },\n        # Test 3: Exact multiplicity\n        {\n            'A': np.diag([2.0, 2.0, 7.0]),\n            'v1_unnormalized': np.array([1.0, 0.0, 0.0]),\n            'gamma': 5.0,\n            'task': 'compute_lambda_hat_2'\n        },\n        # Test 4: Inexact deflation direction\n        {\n            'A': np.diag([1.0, 2.0, 3.0]),\n            'theta_deg': 5.0,\n            'gamma': 10.0,\n            'task': 'compute_lambda_hat_2'\n        },\n        # Test 5: Failure case with too-small penalty\n        {\n            'A': np.diag([1.0, 1.001, 3.0]),\n            'v1_unnormalized': np.array([1.0, 0.0, 0.0]),\n            'gamma': 0.0001,\n            'task': 'check_closeness',\n            'lambda1': 1.0,\n            'lambda2': 1.001\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case['A']\n        gamma = case['gamma']\n\n        # Construct the deflation vector v1\n        if 'v1_unnormalized' in case:\n            v1_unnormalized = case['v1_unnormalized']\n        else:  # Test 4, construct from angle\n            theta_rad = np.deg2rad(case['theta_deg'])\n            v1_unnormalized = np.array([np.cos(theta_rad), np.sin(theta_rad), 0.0])\n        \n        # Normalize v1 to ensure it is a unit vector\n        norm_v1 = np.linalg.norm(v1_unnormalized)\n        if norm_v1 == 0:\n            # Handle potential zero vector, though not expected from problem spec\n            v1 = v1_unnormalized\n        else:\n            v1 = v1_unnormalized / norm_v1\n\n        # 1. Construct the rank-1 updated matrix B\n        # B = A + gamma * v1 * v1^T\n        B = A + gamma * np.outer(v1, v1)\n\n        # 2. Compute the eigenvector x_star for the smallest eigenvalue of B.\n        # np.linalg.eigh returns eigenvalues in ascending order and corresponding\n        # eigenvectors as columns.\n        eigenvalues_B, eigenvectors_B = np.linalg.eigh(B)\n        x_star = eigenvectors_B[:, 0]\n\n        # 3. Compute the estimate lambda_hat_2 as the Rayleigh quotient of A at x_star.\n        # Since x_star is a unit vector, this is x_star^T * A * x_star.\n        lambda_hat_2 = x_star.T @ A @ x_star\n        \n        if case['task'] == 'compute_lambda_hat_2':\n            results.append(lambda_hat_2)\n        elif case['task'] == 'check_closeness':\n            lambda1 = case['lambda1']\n            lambda2 = case['lambda2']\n            # Evaluate the boolean expression |lambda_hat_2 - lambda2|  |lambda_hat_2 - lambda1|\n            is_closer = np.abs(lambda_hat_2 - lambda2)  np.abs(lambda_hat_2 - lambda1)\n            results.append(bool(is_closer))\n\n    # Format the final output string as specified in the problem\n    # Example: [2.0,1.001,2.0,2.0076..,False]\n    # The map(str, ...) correctly handles float and bool types.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}