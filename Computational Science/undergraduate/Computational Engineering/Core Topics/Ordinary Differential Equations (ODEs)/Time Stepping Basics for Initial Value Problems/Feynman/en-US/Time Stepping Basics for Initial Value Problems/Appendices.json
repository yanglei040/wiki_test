{
    "hands_on_practices": [
        {
            "introduction": "A central challenge in simulating physical and chemical systems is \"stiffness,\" which arises when a system involves processes occurring on vastly different time scales. This hands-on coding exercise delves into this crucial concept by modeling a stiff chemical reaction. You will discover why basic explicit methods like Forward Euler can be computationally impractical due to their strict stability limits, and then implement the robust Backward Euler method to see how implicit solvers efficiently handle such problems.",
            "id": "2446924",
            "problem": "Write a complete program that analyzes and time-steps a stiff initial value problem arising from a two-species irreversible reaction. The governing initial value problem is the linear system\n$$\n\\frac{d}{dt}\n\\begin{bmatrix}\ny_1(t)\\\\\ny_2(t)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-k_1 & 0\\\\\nk_1 & -k_2\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1(t)\\\\\ny_2(t)\n\\end{bmatrix},\\quad\n\\begin{bmatrix}\ny_1(0)\\\\\ny_2(0)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ny_{1,0}\\\\\ny_{2,0}\n\\end{bmatrix},\n$$\nwhere $y_1(t)$ and $y_2(t)$ denote species concentrations, and $k_1$ and $k_2$ are reaction rates in $\\mathrm{s}^{-1}$. Stiffness occurs when $k_1 \\gg k_2$. The focus is the basics of time stepping for initial value problems: stability limits for explicit time stepping and robust integration with an implicit method.\n\nYour program must, for each specified test case, do all of the following from first principles:\n\n- Using the scalar linear test equation as the fundamental base for stability analysis, determine the largest constant time step $h_{\\mathrm{FE}}$ that makes the Forward Euler method linearly stable for this system. Justify the mapping from the system to the scalar test equation via its eigenmodes and require stability for the mode with the fastest decay. Do not use any black-box stability tables.\n- Compute the smallest integer number of Forward Euler steps $N_{\\mathrm{FE}} = \\lceil T / h_{\\mathrm{FE}} \\rceil$ needed to reach the final time $T$.\n- Actually integrate the system numerically with the Backward Euler method (also known as implicit Euler) using a constant time step $h_{\\mathrm{BE}}$ provided in the test case. If $T$ is not an integer multiple of $h_{\\mathrm{BE}}$, take a final shorter step so that the last time equals exactly $T$.\n- Evaluate the analytical solution at $t=T$ and report the relative error of the Backward Euler solution at the final time,\n$$\n\\varepsilon = \\frac{\\left\\| \\mathbf{y}_{\\mathrm{BE}}(T) - \\mathbf{y}_{\\mathrm{exact}}(T) \\right\\|_2}{\\left\\| \\mathbf{y}_{\\mathrm{exact}}(T) \\right\\|_2},\n$$\nwhere $\\|\\cdot\\|_2$ is the Euclidean norm.\n\nYou must base your derivations on the following core definitions and facts:\n- The initial value problem definition $\\frac{d\\mathbf{y}}{dt} = \\mathbf{f}(t,\\mathbf{y})$ with given $\\mathbf{y}(0)$.\n- The Forward Euler method update $\\mathbf{y}^{n+1} = \\mathbf{y}^n + h \\mathbf{f}(t^n,\\mathbf{y}^n)$ and its scalar linear test equation $\\dot{z} = \\lambda z$ with amplification factor $1 + h \\lambda$.\n- The Backward Euler method update $\\mathbf{y}^{n+1} = \\mathbf{y}^n + h \\mathbf{f}(t^{n+1},\\mathbf{y}^{n+1})$, which for linear $\\mathbf{f}$ requires solving a linear system at each step.\n\nFor verification, use the closed-form analytical solution obtained by solving the linear system:\n- $y_1(t) = y_{1,0} e^{-k_1 t}$.\n- For $k_1 \\neq k_2$, $y_2(t) = y_{2,0} e^{-k_2 t} + \\dfrac{k_1 y_{1,0}}{k_2 - k_1}\\left( e^{-k_1 t} - e^{-k_2 t} \\right)$.\n- For $k_1 = k_2$, $y_2(t) = \\left( y_{2,0} + k_1 y_{1,0} t \\right) e^{-k_2 t}$.\n\nTest suite and coverage requirements. Your program must run the following independent test cases, which together exercise extreme stiffness, moderate stiffness, near-equal rates, and the exactly equal-rate boundary:\n- Case A (extreme stiffness, long horizon): $(k_1, k_2, T, h_{\\mathrm{BE}}, y_{1,0}, y_{2,0}) = (10^9, 1, 10, 10^{-2}, 1, 0)$.\n- Case B (moderate stiffness): $(k_1, k_2, T, h_{\\mathrm{BE}}, y_{1,0}, y_{2,0}) = (10^3, 1, 10, 2\\times 10^{-2}, 1, 0)$.\n- Case C (near-equal rates, distinct): $(k_1, k_2, T, h_{\\mathrm{BE}}, y_{1,0}, y_{2,0}) = (5, 5.000001, 3, 5\\times 10^{-2}, 0.7, 0.3)$.\n- Case D (exactly equal rates, boundary): $(k_1, k_2, T, h_{\\mathrm{BE}}, y_{1,0}, y_{2,0}) = (5, 5, 3, 5\\times 10^{-2}, 1, 0)$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes the pair $[N_{\\mathrm{FE}}, \\varepsilon]$ in that order. The required format is\n$$\n\\big[ [N_{\\mathrm{FE}}^{(A)}, \\varepsilon^{(A)}], [N_{\\mathrm{FE}}^{(B)}, \\varepsilon^{(B)}], [N_{\\mathrm{FE}}^{(C)}, \\varepsilon^{(C)}], [N_{\\mathrm{FE}}^{(D)}, \\varepsilon^{(D)}] \\big].\n$$\nThe first entry in each pair is an integer and the second is a floating-point number. No physical units are required in the output; report pure numbers. Your program must print exactly one line in this format, for example\n$$\n\\big[ [5000000000, 5.2\\times 10^{-2}], [\\dots], [\\dots], [\\dots] \\big],\n$$\nwhere the ellipses would be replaced by the computed values for the other cases.",
            "solution": "The problem as presented is valid. It constitutes a well-posed initial value problem that is a canonical example of a stiff linear system in computational science and engineering. It is scientifically grounded in the principles of chemical kinetics and numerical analysis, with all necessary data and definitions provided. We will proceed with a rigorous, first-principles derivation and solution.\n\nThe problem requires an analysis of the system of ordinary differential equations (ODEs)\n$$\n\\frac{d\\mathbf{y}}{dt} = \\mathbf{A}\\mathbf{y}, \\quad \\text{with} \\quad \\mathbf{y}(t) = \\begin{bmatrix} y_1(t) \\\\ y_2(t) \\end{bmatrix}, \\quad \\mathbf{A} = \\begin{bmatrix} -k_1 & 0 \\\\ k_1 & -k_2 \\end{bmatrix},\n$$\nand initial conditions $\\mathbf{y}(0) = [y_{1,0}, y_{2,0}]^T$. The parameters $k_1 > 0$ and $k_2 > 0$ are reaction rates.\n\nOur analysis will be conducted in three parts: first, the stability analysis of the explicit Forward Euler method; second, the implementation of the implicit Backward Euler method; and third, the calculation of the numerical error against the provided analytical solution.\n\n**Part 1: Stability of the Forward Euler Method**\n\nThe stability of a numerical method for the linear system $\\dot{\\mathbf{y}} = \\mathbf{A}\\mathbf{y}$ is determined by how the method treats the eigenmodes of the matrix $\\mathbf{A}$. The system can be decoupled by a change of basis into a set of scalar ordinary differential equations of the form $\\dot{z} = \\lambda z$, where $\\lambda$ are the eigenvalues of $\\mathbf{A}$.\n\nThe matrix $\\mathbf{A}$ is lower triangular, so its eigenvalues are its diagonal entries:\n$$\n\\lambda_1 = -k_1 \\quad \\text{and} \\quad \\lambda_2 = -k_2.\n$$\nSince $k_1, k_2 > 0$, both eigenvalues are real and negative. These eigenvalues dictate the time scales of the system's response. A large value of $|\\lambda|$ corresponds to a fast-decaying component (a \"stiff\" component), while a small value corresponds to a slow component.\n\nThe Forward Euler method is defined by the update rule $\\mathbf{y}^{n+1} = \\mathbf{y}^n + h \\mathbf{f}(t^n, \\mathbf{y}^n)$. For our linear system, this becomes $\\mathbf{y}^{n+1} = \\mathbf{y}^n + h \\mathbf{A} \\mathbf{y}^n = (\\mathbf{I} + h\\mathbf{A})\\mathbf{y}^n$. The stability of this iteration depends on the spectral radius of the amplification matrix $\\mathbf{G} = \\mathbf{I} + h\\mathbf{A}$ being less than or equal to $1$. This is equivalent to requiring that for every eigenvalue $\\lambda$ of $\\mathbf{A}$, the corresponding eigenvalue of $\\mathbf{G}$, which is $1+h\\lambda$, has a magnitude no greater than $1$.\n\nFor each eigenvalue $\\lambda_i$, we must satisfy the stability condition of the scalar test equation $\\dot{z} = \\lambda_i z$:\n$$\n|1 + h\\lambda_i| \\le 1.\n$$\nSince our eigenvalues $\\lambda_1 = -k_1$ and $\\lambda_2 = -k_2$ are real and negative, this inequality becomes:\n$$\n-1 \\le 1 + h\\lambda_i \\le 1.\n$$\nThe right side, $1 + h\\lambda_i \\le 1$, simplifies to $h\\lambda_i \\le 0$, which is always satisfied for $h > 0$ and $\\lambda_i < 0$. The left side, $-1 \\le 1 + h\\lambda_i$, gives $h\\lambda_i \\ge -2$, or $h \\le -2/\\lambda_i$. Substituting $\\lambda_i = -k_i$, this is $h \\le 2/k_i$.\n\nFor the numerical scheme to be stable for the entire system, the time step $h$ must be stable for all eigenmodes simultaneously. This imposes the most stringent constraint, which comes from the eigenvalue with the largest magnitude (the fastest time scale):\n$$\nh \\le \\frac{2}{\\max(k_1, k_2)}.\n$$\nTherefore, the largest permissible constant time step for the Forward Euler method is\n$$\nh_{\\mathrm{FE}} = \\frac{2}{\\max(k_1, k_2)}.\n$$\nAny larger step will cause at least one mode to be amplified, leading to catastrophic numerical instability. The number of steps required to reach time $T$ is then the smallest integer $N_{\\mathrm{FE}}$ such that $N_{\\mathrm{FE}} \\cdot h_{\\mathrm{FE}} \\ge T$, which is given by $N_{\\mathrm{FE}} = \\lceil T / h_{\\mathrm{FE}} \\rceil$.\n\n**Part 2: Integration with the Backward Euler Method**\n\nThe Backward Euler method is an implicit method defined by the update rule:\n$$\n\\mathbf{y}^{n+1} = \\mathbf{y}^n + h \\mathbf{f}(t^{n+1}, \\mathbf{y}^{n+1}).\n$$\nFor the linear system $\\dot{\\mathbf{y}} = \\mathbf{A}\\mathbf{y}$, this becomes:\n$$\n\\mathbf{y}^{n+1} = \\mathbf{y}^n + h \\mathbf{A} \\mathbf{y}^{n+1}.\n$$\nTo find the unknown state $\\mathbf{y}^{n+1}$, we must solve a linear system at each time step:\n$$\n(\\mathbf{I} - h\\mathbf{A})\\mathbf{y}^{n+1} = \\mathbf{y}^n.\n$$\nSubstituting the matrix $\\mathbf{A}$, we get:\n$$\n\\begin{bmatrix}\n1 + h k_1 & 0 \\\\\n-h k_1 & 1 + h k_2\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1^{n+1} \\\\\ny_2^{n+1}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ny_1^n \\\\\ny_2^n\n\\end{bmatrix}.\n$$\nThis lower triangular system is solved efficiently using forward substitution. First, we solve for $y_1^{n+1}$:\n$$\n(1 + h k_1)y_1^{n+1} = y_1^n \\implies y_1^{n+1} = \\frac{y_1^n}{1 + h k_1}.\n$$\nThen, we substitute this result into the second equation to solve for $y_2^{n+1}$:\n$$\n-h k_1 y_1^{n+1} + (1 + h k_2)y_2^{n+1} = y_2^n \\implies y_2^{n+1} = \\frac{y_2^n + h k_1 y_1^{n+1}}{1 + h k_2}.\n$$\nThe Backward Euler method is A-stable, meaning its region of absolute stability contains the entire left half of the complex plane. Consequently, it is stable for any stiff system, regardless of the time step size $h > 0$. The step size $h_{\\mathrm{BE}}$ is therefore chosen based on accuracy considerations, not stability. This is the fundamental reason for using implicit methods for stiff problems.\n\nThe integration proceeds from $t=0$ to $T$ using the given step size $h_{\\mathrm{BE}}$. If $T$ is not an integer multiple of $h_{\\mathrm{BE}}$, the algorithm takes $N = \\lfloor T/h_{\\mathrm{BE}} \\rfloor$ full steps of size $h_{\\mathrm{BE}}$ and one final, shorter step of size $h_{final} = T - N h_{\\mathrm{BE}}$.\n\n**Part 3: Error Calculation**\n\nAfter integrating the system up to $t=T$ to obtain the numerical solution $\\mathbf{y}_{\\mathrm{BE}}(T)$, we compare it to the exact analytical solution $\\mathbf{y}_{\\mathrm{exact}}(T)$. The analytical solution is given as:\n-   $y_1(t) = y_{1,0} e^{-k_1 t}$.\n-   For $k_1 \\neq k_2$, $y_2(t) = y_{2,0} e^{-k_2 t} + \\frac{k_1 y_{1,0}}{k_2 - k_1} (e^{-k_1 t} - e^{-k_2 t})$.\n-   For $k_1 = k_2$, $y_2(t) = (y_{2,0} + k_1 y_{1,0} t) e^{-k_2 t}$.\n\nThe relative error of the numerical solution at the final time $T$ is calculated using the Euclidean norm ($\\|\\cdot\\|_2$):\n$$\n\\varepsilon = \\frac{\\left\\| \\mathbf{y}_{\\mathrm{BE}}(T) - \\mathbf{y}_{\\mathrm{exact}}(T) \\right\\|_2}{\\left\\| \\mathbf{y}_{\\mathrm{exact}}(T) \\right\\|_2}.\n$$\nThis quantity measures the accuracy of the Backward Euler integration for the chosen step size $h_{\\mathrm{BE}}$. The denominator normalizes the error, making it a relative measure. The provided test cases will demonstrate the robustness of Backward Euler across different degrees of stiffness.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Format: (k1, k2, T, h_BE, y1_0, y2_0)\n    test_cases = [\n        (1e9, 1.0, 10.0, 1e-2, 1.0, 0.0),\n        (1e3, 1.0, 10.0, 2e-2, 1.0, 0.0),\n        (5.0, 5.000001, 3.0, 5e-2, 0.7, 0.3),\n        (5.0, 5.0, 3.0, 5e-2, 1.0, 0.0),\n    ]\n\n    results = []\n    for k1, k2, T, h_BE, y1_0, y2_0 in test_cases:\n        # Part 1: Forward Euler stability analysis\n        # The largest stable time step is h_FE = 2 / max(|lambda_i|).\n        # Eigenvalues are -k1 and -k2.\n        h_FE = 2.0 / max(k1, k2)\n        \n        # Number of steps to reach T. Use np.ceil for ceiling operation.\n        N_FE = int(np.ceil(T / h_FE))\n\n        # Part 2: Backward Euler integration\n        y = np.array([y1_0, y2_0], dtype=np.float64)\n        current_time = 0.0\n        \n        # Determine number of steps and handle the final step if T is not a multiple of h_BE\n        num_steps = int(np.floor(T / h_BE))\n        last_h = T - num_steps * h_BE\n\n        # Integration loop for full steps\n        for _ in range(num_steps):\n            # Solve (I - hA)y_new = y_old\n            # y1_new = y1_old / (1 + h*k1)\n            y[0] = y[0] / (1.0 + h_BE * k1)\n            # y2_new = (y2_old + h*k1*y1_new) / (1 + h*k2)\n            y[1] = (y[1] + h_BE * k1 * y[0]) / (1.0 + h_BE * k2)\n        \n        # Final shorter step, if necessary\n        if last_h > 1e-15: # Use a tolerance for floating point comparison\n            y[0] = y[0] / (1.0 + last_h * k1)\n            y[1] = (y[1] + last_h * k1 * y[0]) / (1.0 + last_h * k2)\n\n        y_BE_T = y\n\n        # Part 3: Analytical solution and error calculation\n        y_exact_T = np.zeros(2, dtype=np.float64)\n        \n        # Analytical solution for y1(T)\n        y_exact_T[0] = y1_0 * np.exp(-k1 * T)\n\n        # Analytical solution for y2(T) depends on whether k1 and k2 are equal\n        if k1 == k2:\n            y_exact_T[1] = (y2_0 + k1 * y1_0 * T) * np.exp(-k2 * T)\n        else:\n            term1 = y2_0 * np.exp(-k2 * T)\n            # Check for k1 != k2 for the fraction\n            if abs(k2 - k1) > 1e-15:\n                factor = (k1 * y1_0) / (k2 - k1)\n                term2 = factor * (np.exp(-k1 * T) - np.exp(-k2 * T))\n                y_exact_T[1] = term1 + term2\n            else: # This case is for near-equal rates, but formula is robust enough\n                factor = (k1 * y1_0) / (k2 - k1)\n                term2 = factor * (np.exp(-k1 * T) - np.exp(-k2 * T))\n                y_exact_T[1] = term1 + term2\n\n        # Calculate the relative error using Euclidean norm\n        norm_diff = np.linalg.norm(y_BE_T - y_exact_T)\n        norm_exact = np.linalg.norm(y_exact_T)\n        \n        epsilon = norm_diff / norm_exact if norm_exact > 1e-15 else 0.0\n\n        results.append(f\"[{N_FE}, {epsilon:.12e}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solver function\nsolve()\n```"
        },
        {
            "introduction": "It is a common misconception that a higher-order numerical method is always more accurate. This practice challenges that notion by creating a scenario where a simple first-order Forward Euler integrator yields a better result than the venerable fourth-order Runge-Kutta (RK4) method. By investigating this counter-intuitive result, you will gain critical insight into how a method's accuracy depends on its ability to resolve the underlying dynamics of the problem, a phenomenon closely related to signal processing concepts like aliasing.",
            "id": "2446910",
            "problem": "Consider the ordinary differential equation (ODE) initial value problem\n$$\n\\frac{dy}{dt} = f(t), \\quad y(0)=0,\n$$\nwith a time-periodic right-hand side defined as follows. Let $p>0$ be a period, $A>0$ be an amplitude, and $\\sigma \\in (0,0.5)$ be a nondimensional width parameter. Define the reduced phase\n$$\nr(t) = t - p \\left\\lfloor \\frac{t}{p} \\right\\rfloor \\in [0,p),\n$$\nthe centered phase\n$$\ns(t) = r(t) - \\frac{p}{2},\n$$\nand set\n$$\nf(t) = A \\exp\\!\\left(-\\frac{s(t)^2}{2\\,(\\sigma p)^2}\\right).\n$$\nFix the parameters $p=1.0$, $A=10.0$, $\\sigma=0.05$, and final time $T=64.0$. You will analyze approximations over $[0,T]$ for the following prescribed time steps (all in seconds): $\\Delta t_1 = 1.0$, $\\Delta t_2 = 2.0$, $\\Delta t_3 = 0.5$, and $\\Delta t_4 = 0.125$. Note that each $\\Delta t_i$ exactly divides $T$.\n\nFor this data, the exact solution at time $T$ is given by the fundamental theorem of calculus applied to the ODE, which here reduces to\n$$\ny(T) \\;=\\; \\int_{0}^{T} f(t)\\,dt.\n$$\nBecause $f(t)$ is periodic with period $p$ and $T$ is an integer multiple of $p$, the integral can be evaluated in closed form as\n$$\ny(T) \\;=\\; \\frac{T}{p}\\, \\int_{0}^{p} A \\exp\\!\\left(-\\frac{(t-\\frac{p}{2})^2}{2\\,(\\sigma p)^2}\\right) dt \n\\;=\\; T \\, A \\, \\sigma \\, \\sqrt{2\\pi}\\, \\operatorname{erf}\\!\\left(\\frac{1}{2\\sqrt{2}\\,\\sigma}\\right),\n$$\nwhere $\\operatorname{erf}(\\cdot)$ denotes the Gauss error function.\n\nYour task is to compute, for each $\\Delta t_i$, the absolute error at time $T$ using each of the following one-step time-stepping approximations applied on the uniform grid $t_n = n\\,\\Delta t_i$:\n\n1) Forward Euler:\n$$\ny^{n+1}_{\\mathrm{FE}} \\;=\\; y^n_{\\mathrm{FE}} \\;+\\; \\Delta t_i \\, f(t_n), \n\\quad y^0_{\\mathrm{FE}} = 0.\n$$\n\n2) Classical fourth-order Runge–Kutta method (RK4):\n\\begin{align*}\nk_1 &= f(t_n),\\\\\nk_2 &= f\\!\\left(t_n+\\tfrac{1}{2}\\Delta t_i\\right),\\\\\nk_3 &= f\\!\\left(t_n+\\tfrac{1}{2}\\Delta t_i\\right),\\\\\nk_4 &= f\\!\\left(t_n+\\Delta t_i\\right),\\\\\ny^{n+1}_{\\mathrm{RK4}} &= y^n_{\\mathrm{RK4}} \\;+\\; \\Delta t_i \\,\\frac{k_1 + 2k_2 + 2k_3 + k_4}{6}, \n\\quad y^0_{\\mathrm{RK4}} = 0.\n\\end{align*}\n\nFor each $\\Delta t_i$, compute the absolute errors\n$$\nE_{\\mathrm{FE}}(\\Delta t_i) \\;=\\; \\left|y_{\\mathrm{FE}}(T;\\Delta t_i) - y(T)\\right|, \n\\qquad\nE_{\\mathrm{RK4}}(\\Delta t_i) \\;=\\; \\left|y_{\\mathrm{RK4}}(T;\\Delta t_i) - y(T)\\right|.\n$$\n\nTest Suite:\n- Case $1$: $\\Delta t_1 = 1.0$ (a large step equal to the forcing period $p$).\n- Case $2$: $\\Delta t_2 = 2.0$ (a very large step equal to $2p$).\n- Case $3$: $\\Delta t_3 = 0.5$ (a moderate step equal to $p/2$).\n- Case $4$: $\\Delta t_4 = 0.125$ (a small step equal to $p/8$).\n\nFor each case, report the boolean value\n$$\nB_i = \\big( E_{\\mathrm{FE}}(\\Delta t_i) < E_{\\mathrm{RK4}}(\\Delta t_i) \\big),\n$$\nthat is, whether Forward Euler is more accurate than the fourth-order Runge–Kutta method for that specific $\\Delta t_i$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the booleans for the four cases as a comma-separated list enclosed in square brackets, for example, \"[True,False,True,False]\".",
            "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n- **ODE**: $\\frac{dy}{dt} = f(t)$, with initial condition $y(0)=0$.\n- **Forcing function definitions**:\n  - Period: $p > 0$.\n  - Amplitude: $A > 0$.\n  - Nondimensional width: $\\sigma \\in (0, 0.5)$.\n  - Reduced phase: $r(t) = t - p \\left\\lfloor \\frac{t}{p} \\right\\rfloor \\in [0,p)$.\n  - Centered phase: $s(t) = r(t) - \\frac{p}{2}$.\n  - Forcing function: $f(t) = A \\exp\\!\\left(-\\frac{s(t)^2}{2\\,(\\sigma p)^2}\\right)$.\n- **Fixed parameters**: $p=1.0$, $A=10.0$, $\\sigma=0.05$, final time $T=64.0$.\n- **Time steps**: $\\Delta t_1 = 1.0$, $\\Delta t_2 = 2.0$, $\\Delta t_3 = 0.5$, $\\Delta t_4 = 0.125$.\n- **Exact solution at $T$**: $y(T) = T \\, A \\, \\sigma \\, \\sqrt{2\\pi}\\, \\operatorname{erf}\\!\\left(\\frac{1}{2\\sqrt{2}\\,\\sigma}\\right)$.\n- **Numerical methods**:\n  - Forward Euler (FE): $y^{n+1}_{\\mathrm{FE}} = y^n_{\\mathrm{FE}} + \\Delta t_i \\, f(t_n)$, with $y^0_{\\mathrm{FE}} = 0$.\n  - Classical fourth-order Runge–Kutta (RK4):\n    $k_1 = f(t_n)$, $k_2 = f(t_n+\\tfrac{1}{2}\\Delta t_i)$, $k_3 = f(t_n+\\tfrac{1}{2}\\Delta t_i)$, $k_4 = f(t_n+\\Delta t_i)$,\n    $y^{n+1}_{\\mathrm{RK4}} = y^n_{\\mathrm{RK4}} + \\Delta t_i \\,\\frac{k_1 + 2k_2 + 2k_3 + k_4}{6}$, with $y^0_{\\mathrm{RK4}} = 0$.\n- **Task**: For each $\\Delta t_i$, compute the boolean $B_i = ( E_{\\mathrm{FE}}(\\Delta t_i) < E_{\\mathrm{RK4}}(\\Delta t_i) )$, where $E(\\Delta t_i) = |y_{\\text{approx}}(T;\\Delta t_i) - y(T)|$.\n\nStep 2: Validate Using Extracted Givens\nThe problem is a well-defined exercise in numerical analysis, specifically the study of one-step methods for ordinary differential equations.\n- **Scientifically Grounded**: The problem is based on fundamental concepts of calculus and numerical methods for solving ODEs. The equation $y' = f(t)$ is the simplest form of a non-autonomous ODE. The provided numerical schemes, Forward Euler and RK4, are canonical. The derivation of the exact solution is mathematically sound. The problem is free of scientific flaws.\n- **Well-Posed**: The initial value problem is well-posed. The function $f(t)$ is continuous and bounded, guaranteeing a unique solution. The computational task is specified unambiguously.\n- **Objective**: The problem is stated using precise mathematical language, devoid of any subjective or opinion-based content.\n- **Complete and Consistent**: All necessary parameters, equations, initial conditions, and methods are provided. The data are internally consistent (e.g., $T$ is an integer multiple of $p$, and all $\\Delta t_i$ divide $T$).\n- **Ill-Posed/Unrealistic**: The problem is a standard numerical experiment and is not ill-posed or unrealistic. The phenomenon it investigates—the potential for a higher-order method to be less accurate than a lower-order one for a specific, poorly chosen step size—is a known and important concept in numerical analysis related to sampling and aliasing.\n\nStep 3: Verdict and Action\nThe problem is **valid**. It is a correct and meaningful exercise in computational engineering. A solution will be furnished.\n\nThe objective is to solve the initial value problem $dy/dt = f(t)$ with $y(0)=0$ over the time interval $[0, T]$ using two different numerical methods, Forward Euler (FE) and fourth-order Runge-Kutta (RK4), for a set of four distinct time steps $\\Delta t_i$. For each case, we must determine if the first-order FE method produces a smaller absolute error at the final time $T$ than the fourth-order RK4 method.\n\nThe governing parameters are set to $p=1.0$, $A=10.0$, $\\sigma=0.05$, and $T=64.0$. The forcing function $f(t)$ is a periodic train of Gaussian-like pulses with period $p$. Because the ODE is of the form $dy/dt=f(t)$, the solution is the direct integral $y(t) = \\int_0^t f(\\tau)d\\tau$. The numerical schemes are thus approximations of this integral. Specifically, the total accumulated value $y(T)$ from the FE method corresponds to an approximation of the integral using the composite left rectangle rule, while the RK4 method, for this specific ODE form, is equivalent to the composite Simpson's rule.\n\nFirst, we compute the exact solution $y(T)$ using the provided formula:\n$$\ny(T) = T \\, A \\, \\sigma \\, \\sqrt{2\\pi}\\, \\operatorname{erf}\\!\\left(\\frac{1}{2\\sqrt{2}\\,\\sigma}\\right)\n$$\nWith the given parameters:\n$$\ny(T) = 64.0 \\cdot 10.0 \\cdot 0.05 \\cdot \\sqrt{2\\pi} \\cdot \\operatorname{erf}\\!\\left(\\frac{1}{0.1\\sqrt{2}}\\right) \\approx 80.2121\n$$\nThe argument of the error function is approximately $7.071$, for which $\\operatorname{erf}$ is effectively $1.0$.\n\nNext, we implement the numerical schemes for each $\\Delta t_i \\in \\{1.0, 2.0, 0.5, 0.125\\}$. For each $\\Delta t$, the number of steps is $N = T / \\Delta t$. The time grid is $t_n = n \\Delta t$ for $n=0, 1, \\dots, N$.\n\nThe procedure is as follows:\n For each $\\Delta t_i$:\n 1. Initialize $y_{\\mathrm{FE}}^0=0$ and $y_{\\mathrm{RK4}}^0=0$.\n 2. Iterate from $n=0$ to $N-1$:\n    - Compute $f(t_n)$ and update the FE solution: $y_{\\mathrm{FE}}^{n+1} = y_{\\mathrm{FE}}^{n} + \\Delta t_i f(t_n)$.\n    - Compute the RK4 stages $k_1 = f(t_n)$, $k_2=k_3 = f(t_n+\\tfrac{1}{2}\\Delta t_i)$, $k_4 = f(t_n+\\Delta t_i)$, and update the RK4 solution: $y_{\\mathrm{RK4}}^{n+1} = y_{\\mathrm{RK4}}^{n} + \\Delta t_i (k_1 + 4k_2 + k_4)/6$.\n 3. After the loop, the final approximations are $y_{\\mathrm{FE}}(T) = y_{\\mathrm{FE}}^N$ and $y_{\\mathrm{RK4}}(T) = y_{\\mathrm{RK4}}^N$.\n 4. Compute absolute errors: $E_{\\mathrm{FE}}(\\Delta t_i) = |y_{\\mathrm{FE}}(T) - y(T)|$ and $E_{\\mathrm{RK4}}(\\Delta t_i) = |y_{\\mathrm{RK4}}(T) - y(T)|$.\n 5. Evaluate the boolean $B_i = (E_{\\mathrm{FE}}(\\Delta t_i) < E_{\\mathrm{RK4}}(\\Delta t_i))$.\n\nAnalysis of each case:\n- **Case 1: $\\Delta t_1 = 1.0 = p$**:\n  - FE samples $f(t)$ at $t_n = n \\cdot p$. At these points, $r(t_n)=0$ and $s(t_n)=-p/2$. Thus, $f(t_n) = A \\exp(-1/(8\\sigma^2)) \\approx 10 e^{-50}$, which is nearly zero. The FE approximation $y_{\\mathrm{FE}}(T)$ will be extremely close to $0$. The error $E_{\\mathrm{FE}}$ is approximately $|0 - 80.2121| = 80.2121$.\n  - RK4 samples at $t_n$, $t_n+p/2$, and $t_n+p$. This is equivalent to applying Simpson's rule over one period. The total accumulation is the number of periods times the integral estimate per period: $y_{\\mathrm{RK4}}(T) \\approx (T/p) \\cdot (p/6)(f(0) + 4f(p/2) + f(p)) \\approx 64 \\cdot (1.0/6)(0 + 4(10) + 0) \\approx 426.67$. The error $E_{\\mathrm{RK4}}$ is $|426.67 - 80.2121| \\approx 346.45$.\n  - Here, $E_{\\mathrm{FE}} < E_{\\mathrm{RK4}}$. Thus, $B_1$ is True. This counter-intuitive result demonstrates aliasing: the time step is too large to resolve the features of $f(t)$, causing the high-order method to fail catastrophically.\n\n- **Case 2: $\\Delta t_2 = 2.0 = 2p$**:\n  - FE samples at $t_n = n \\cdot 2p$, giving $f(t_n)=f(0) \\approx 0$. The resulting $y_{\\mathrm{FE}}(T) \\approx 0$ and $E_{\\mathrm{FE}} \\approx 80.2121$.\n  - RK4 samples at $t_n=n \\cdot 2p$, $t_n+p$, and $t_n+2p$. $f(n \\cdot 2p)=f(0)$, $f(n \\cdot 2p+p)=f(p)=f(0)$, and $f(n \\cdot 2p+2p)=f(2p)=f(0)$. All sample points yield the same near-zero value.\n  - Both methods produce the same result: $y_{\\mathrm{FE}}(T) = y_{\\mathrm{RK4}}(T) \\approx 0$, leading to $E_{\\mathrm{FE}} = E_{\\mathrm{RK4}}$.\n  - The condition $E_{\\mathrm{FE}} < E_{\\mathrm{RK4}}$ is False. Thus, $B_2$ is False.\n\n- **Case 3: $\\Delta t_3 = 0.5 = p/2$**:\n  - FE samples at $t=0, 0.5, 1.0, 1.5, \\dots$. It alternately samples the near-zero tail $f(0)$ and the peak $f(0.5)=A=10$. This gives a large overestimation $y_{\\mathrm{FE}}(T) \\approx (T/\\Delta t) \\cdot \\Delta t \\cdot (f(0)+f(0.5))/2 = T \\cdot A/2 = 320$. Error $E_{\\mathrm{FE}} \\approx |320 - 80.2121| \\approx 239.79$.\n  - RK4 uses a finer sampling grid, including points like $t=0.25$ and $t=0.75$, which better resolves the shape of the Gaussian pulse. Its result is significantly more accurate. $y_{\\mathrm{RK4}}(T) \\approx 106.67$, giving $E_{\\mathrm{RK4}} \\approx |106.67 - 80.2121| \\approx 26.45$.\n  - $E_{\\mathrm{FE}} > E_{\\mathrm{RK4}}$. Thus, $B_3$ is False.\n\n- **Case 4: $\\Delta t_4 = 0.125 = p/8$**:\n  - With a smaller time step, the methods begin to operate within their expected asymptotic convergence regimes. RK4's error, which scales as $O((\\Delta t)^4)$, will be substantially smaller than FE's error, which scales as $O(\\Delta t)$.\n  - We expect RK4 to be significantly more accurate.\n  - $E_{\\mathrm{FE}} > E_{\\mathrm{RK4}}$. Thus, $B_4$ is False.\n\nThe final boolean results are computed by implementing this logic numerically.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Solves the given ODE problem for four different time steps and compares\n    the accuracy of Forward Euler and RK4 methods.\n    \"\"\"\n    # Define problem parameters\n    p = 1.0\n    A = 10.0\n    sigma = 0.05\n    T = 64.0\n\n    # Define test cases for time steps\n    test_cases = [1.0, 2.0, 0.5, 0.125]\n\n    # Define the forcing function f(t)\n    def f(t, p_val, a_val, sigma_val):\n        \"\"\"\n        Computes the value of the periodic forcing function f(t).\n        \"\"\"\n        r_t = t - p_val * np.floor(t / p_val)\n        s_t = r_t - p_val / 2.0\n        exponent = -s_t**2 / (2.0 * (sigma_val * p_val)**2)\n        return a_val * np.exp(exponent)\n\n    # Calculate the exact solution y(T)\n    arg_erf = 1.0 / (2.0 * np.sqrt(2.0) * sigma)\n    y_exact_T = T * A * sigma * np.sqrt(2.0 * np.pi) * erf(arg_erf)\n\n    results = []\n\n    for dt in test_cases:\n        # Ensure the number of steps is an integer\n        if T % dt != 0:\n            raise ValueError(f\"Time step dt={dt} does not evenly divide T={T}\")\n        N = int(T / dt)\n\n        # --- Forward Euler (FE) simulation ---\n        y_fe = 0.0\n        # The sum can be vectorized for efficiency\n        time_points_fe = np.arange(N) * dt\n        f_values_fe = f(time_points_fe, p, A, sigma)\n        y_fe = np.sum(f_values_fe) * dt\n        \n        error_fe = np.abs(y_fe - y_exact_T)\n\n        # --- RK4 simulation ---\n        y_rk4 = 0.0\n        # The ODE y'=f(t) simplifies the RK4 scheme, which is equivalent to\n        # composite Simpson's rule for integration.\n        time_points_rk4 = np.arange(N) * dt\n        \n        k1 = f(time_points_rk4, p, A, sigma)\n        k2 = f(time_points_rk4 + 0.5 * dt, p, A, sigma)\n        # For y'=f(t), k3 is identical to k2\n        k4 = f(time_points_rk4 + dt, p, A, sigma)\n        \n        y_rk4 = np.sum((dt / 6.0) * (k1 + 4.0 * k2 + k4))\n\n        error_rk4 = np.abs(y_rk4 - y_exact_T)\n\n        # Compare errors and store boolean result\n        results.append(error_fe < error_rk4)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond using existing numerical methods, it is vital to understand how they can be constructed and improved. This exercise introduces Richardson extrapolation, a powerful general technique for increasing the order of accuracy of an approximation. You will apply it to the A-stable second-order trapezoidal rule to create a new, fourth-order method, numerically verify its enhanced convergence rate, and analyze its stability properties, providing a practical lesson in method design and analysis.",
            "id": "2446856",
            "problem": "Consider the autonomous linear initial value problem defined by the ordinary differential equation $y^{\\prime}(t)=\\lambda\\,y(t)$ with initial condition $y(0)=y_0$, where $\\lambda \\in \\mathbb{C}$ and $y_0 \\in \\mathbb{C}$. Let $h>0$ denote a constant time step and $t_n = n h$. The trapezoidal one-step method is defined by the implicit update $y_{n+1}=y_n+\\tfrac{h}{2}\\left(f(t_n,y_n)+f(t_{n+1},y_{n+1})\\right)$ for $f(t,y)=\\lambda y$, and its stability function is $R(z)=\\dfrac{1+\\tfrac{z}{2}}{1-\\tfrac{z}{2}}$ with $z=h\\lambda$. Define a Richardson-extrapolated approximation at the same final time by combining two trapezoidal approximations computed with step sizes $h$ and $\\tfrac{h}{2}$ according to $y^{\\mathrm{RE}}(T;h)=\\dfrac{2^p\\,y^{\\mathrm{trap}}(T;\\tfrac{h}{2})-y^{\\mathrm{trap}}(T;h)}{2^p-1}$, where $p=2$ is the order of the trapezoidal method and $T>0$ is a fixed final time. For the linear test equation, the corresponding one-step stability function of the Richardson-extrapolated method is $R_{\\mathrm{RE}}(z)=\\dfrac{4\\,R\\!\\left(\\tfrac{z}{2}\\right)^2-R(z)}{3}$.\n\nYour task is to implement a program that, for the specified test suite below, computes error ratios to assess observed convergence orders and evaluates absolute stability (A-stability) numerically. Absolute stability (A-stability) is understood as follows: a one-step method with stability function $R(z)$ is absolutely stable for a step $h>0$ and problem parameter $\\lambda$ if $|R(h\\lambda)|\\leq 1$ whenever $\\mathrm{Re}(\\lambda)\\leq 0$.\n\nUse the exact solution $y(t)=y_0\\,e^{\\lambda t}$ to compute absolute errors at the final time $T$ when needed. All computations must be performed using the given parameter values without any user input. Angles do not appear in this problem. No physical units are involved.\n\nTest suite:\n- Case $1$ (observed order for the trapezoidal method): Use $\\lambda=-1$, $y_0=1$, $T=1$, with step sizes $h=0.2$ and $h/2=0.1$. Compute the ratio $\\dfrac{E_h}{E_{h/2}}$, where $E_h=\\left|y(T)-y^{\\mathrm{trap}}(T;h)\\right|$ and $E_{h/2}=\\left|y(T)-y^{\\mathrm{trap}}(T;\\tfrac{h}{2})\\right|$. Report this ratio as a float.\n- Case $2$ (observed order for the Richardson-extrapolated method): Use the same $\\lambda$, $y_0$, and $T$ as in Case $1$. Compute $y^{\\mathrm{RE}}(T;h)$ using $h=0.2$ and $y^{\\mathrm{RE}}(T;\\tfrac{h}{2})$ using $h/2=0.1$, where $y^{\\mathrm{RE}}$ combines trapezoidal approximations at step sizes $h$ and $\\tfrac{h}{2}$ as defined above. Compute the ratio $\\dfrac{E_h^{\\mathrm{RE}}}{E_{h/2}^{\\mathrm{RE}}}$ with $E_h^{\\mathrm{RE}}=\\left|y(T)-y^{\\mathrm{RE}}(T;h)\\right|$ and $E_{h/2}^{\\mathrm{RE}}=\\left|y(T)-y^{\\mathrm{RE}}(T;\\tfrac{h}{2})\\right|$. Report this ratio as a float.\n- Case $3$ (absolute stability check at moderate stiffness for the Richardson-extrapolated one-step stability function): Evaluate $|R_{\\mathrm{RE}}(z)|\\leq 1$ for $z=-1$. Report the result as a boolean.\n- Case $4$ (absolute stability check at severe stiffness for the Richardson-extrapolated one-step stability function): Evaluate $|R_{\\mathrm{RE}}(z)|\\leq 1$ for $z=-100$. Report the result as a boolean.\n- Case $5$ (consistency edge case): Use $\\lambda=0$, $y_0=1$, $T=1$, and $h=0.25$. Compute the absolute error $\\left|y(T)-y^{\\mathrm{RE}}(T;h)\\right|$ and report it as a float.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order of Cases $1$ through $5$, for example, $[r_1,r_2,r_3,r_4,r_5]$, where $r_1$ and $r_2$ are floats, $r_3$ and $r_4$ are booleans, and $r_5$ is a float.",
            "solution": "The problem presented is a well-defined exercise in computational science, specifically in the numerical analysis of ordinary differential equations. It is scientifically grounded, self-contained, and devoid of ambiguity. The task involves analyzing the trapezoidal rule and its Richardson extrapolation for the standard linear test problem. The problem is valid. I will proceed with a complete solution.\n\nThe fundamental object of study is the linear initial value problem (IVP):\n$$\ny^{\\prime}(t) = \\lambda y(t), \\quad y(0) = y_0\n$$\nwhere $\\lambda \\in \\mathbb{C}$ and $y_0 \\in \\mathbb{C}$. The exact solution to this IVP is given by $y(t) = y_0 e^{\\lambda t}$. A numerical method approximates this solution at discrete time points $t_n = n h$, where $h > 0$ is the time step.\n\nLet us represent the numerical approximation at time $t_n$ by $y_n$. The trapezoidal rule for this IVP is an implicit one-step method defined by the recurrence relation:\n$$\ny_{n+1} = y_n + \\frac{h}{2} (\\lambda y_n + \\lambda y_{n+1})\n$$\nThis equation must be solved for $y_{n+1}$:\n$$\ny_{n+1} \\left(1 - \\frac{h\\lambda}{2}\\right) = y_n \\left(1 + \\frac{h\\lambda}{2}\\right)\n$$\n$$\ny_{n+1} = \\frac{1 + \\frac{h\\lambda}{2}}{1 - \\frac{h\\lambda}{2}} y_n\n$$\nThe term multiplying $y_n$ is the stability function of the method, denoted by $R(z)$, where $z = h\\lambda$. Thus, $y_{n+1} = R(z) y_n$. By repeated application, the solution at time $T=Nh$ is given by $y_N = R(z)^N y_0$. Let us denote this final approximation by $y^{\\mathrm{trap}}(T;h) = R(h\\lambda)^{T/h} y_0$. This closed-form expression will be used for computations.\n\nRichardson extrapolation is a technique to improve the accuracy of a numerical approximation. For a method of order $p$, the error can be expressed as an asymptotic series in the step size $h$. For the trapezoidal method, which is symmetric, the error expansion contains only even powers of $h$:\n$$\ny^{\\mathrm{trap}}(T;h) = y(T) + C_p a(h)^p + C_{p+2} a(h)^{p+2} + \\dots\n$$\nwhere $a(h)$ is proportional to $h$ (here, $h$ itself) and $p=2$. Using two approximations, one with step $h$ and another with step $h/2$, we can eliminate the leading error term. The formula for the Richardson-extrapolated solution $y^{\\mathrm{RE}}(T;h)$ is given as:\n$$\ny^{\\mathrm{RE}}(T;h) = \\frac{2^p y^{\\mathrm{trap}}(T;\\tfrac{h}{2}) - y^{\\mathrm{trap}}(T;h)}{2^p - 1}\n$$\nWith $p=2$, this becomes:\n$$\ny^{\\mathrm{RE}}(T;h) = \\frac{4 y^{\\mathrm{trap}}(T;\\tfrac{h}{2}) - y^{\\mathrm{trap}}(T;h)}{3}\n$$\nThis new approximation $y^{\\mathrm{RE}}(T;h)$ has an error of order $O(h^{p+2}) = O(h^4)$, provided the next term in the expansion exists.\n\nWe now address each case presented in the test suite.\n\nCase $1$: Observed convergence order of the trapezoidal method.\nParameters are $\\lambda=-1$, $y_0=1$, $T=1$. We use $h=0.2$. The exact solution is $y(T) = e^{-1}$.\nThe numerical approximation with step size $h=0.2$ requires $N_h=T/h=5$ steps. The argument of the stability function is $z_h=h\\lambda=-0.2$.\n$$\ny^{\\mathrm{trap}}(T;h) = \\left( R(-0.2) \\right)^{5} y_0 = \\left( \\frac{1-0.1}{1+0.1} \\right)^5 = \\left(\\frac{9}{11}\\right)^5\n$$\nThe approximation with step size $h/2=0.1$ requires $N_{h/2}=T/(h/2)=10$ steps. The argument is $z_{h/2}=(h/2)\\lambda=-0.1$.\n$$\ny^{\\mathrm{trap}}(T;\\tfrac{h}{2}) = \\left( R(-0.1) \\right)^{10} y_0 = \\left( \\frac{1-0.05}{1+0.05} \\right)^{10} = \\left(\\frac{19}{21}\\right)^{10}\n$$\nThe absolute errors are $E_h = |y(T) - y^{\\mathrm{trap}}(T;h)|$ and $E_{h/2} = |y(T) - y^{\\mathrm{trap}}(T;\\tfrac{h}{2})|$. For a method of order $p=2$, the ratio of errors should be approximately $E_h/E_{h/2} \\approx 2^p = 4$.\n\nCase $2$: Observed convergence order of the Richardson-extrapolated method.\nWe use the same parameters as Case $1$. We need to compute two extrapolated approximations.\nFirst, $y^{\\mathrm{RE}}(T;h)$ with $h=0.2$:\n$$\ny^{\\mathrm{RE}}(T;0.2) = \\frac{4 y^{\\mathrm{trap}}(T;0.1) - y^{\\mathrm{trap}}(T;0.2)}{3}\n$$\nSecond, $y^{\\mathrm{RE}}(T;h/2)$ with $h/2=0.1$. This requires approximations at $0.1$ and $0.1/2=0.05$. The approximation at $h=0.05$ uses $N=T/0.05=20$ steps and $z=-0.05$.\n$$\ny^{\\mathrm{trap}}(T;0.05) = \\left(R(-0.05)\\right)^{20} y_0 = \\left(\\frac{1-0.025}{1+0.025}\\right)^{20} = \\left(\\frac{39}{41}\\right)^{20}\n$$\n$$\ny^{\\mathrm{RE}}(T;0.1) = \\frac{4 y^{\\mathrm{trap}}(T;0.05) - y^{\\mathrm{trap}}(T;0.1)}{3}\n$$\nThe errors are $E_h^{\\mathrm{RE}} = |y(T) - y^{\\mathrm{RE}}(T;0.2)|$ and $E_{h/2}^{\\mathrm{RE}} = |y(T) - y^{\\mathrm{RE}}(T;0.1)|$. Since the extrapolated method is of order $p_{\\mathrm{RE}}=4$, the expected error ratio is $E_h^{\\mathrm{RE}}/E_{h/2}^{\\mathrm{RE}} \\approx 2^4 = 16$.\n\nCase $3$: Absolute stability of the Richardson-extrapolated one-step method for $z=-1$.\nThe stability function for the combined one-step method is given as $R_{\\mathrm{RE}}(z)=\\dfrac{4\\,R\\!\\left(\\tfrac{z}{2}\\right)^2-R(z)}{3}$. We must evaluate $|R_{\\mathrm{RE}}(z)| \\le 1$ for $z=-1$.\nWith $z=-1$, we have $z/2=-0.5$.\n$$\nR(-1) = \\frac{1+(-1)/2}{1-(-1)/2} = \\frac{1/2}{3/2} = \\frac{1}{3}\n$$\n$$\nR(-0.5) = \\frac{1+(-0.5)/2}{1-(-0.5)/2} = \\frac{3/4}{5/4} = \\frac{3}{5}\n$$\n$$\nR_{\\mathrm{RE}}(-1) = \\frac{4(3/5)^2 - (1/3)}{3} = \\frac{4(9/25) - 1/3}{3} = \\frac{36/25 - 1/3}{3} = \\frac{(108-25)/75}{3} = \\frac{83}{225}\n$$\nThe magnitude is $|\\frac{83}{225}| = \\frac{83}{225} \\approx 0.3689$, which is less than or equal to $1$. The condition holds.\n\nCase $4$: Absolute stability for $z=-100$.\nWe evaluate $|R_{\\mathrm{RE}}(z)| \\le 1$ for $z=-100$.\nWith $z=-100$, we have $z/2=-50$.\n$$\nR(-100) = \\frac{1+(-100)/2}{1-(-100)/2} = \\frac{1-50}{1+50} = -\\frac{49}{51}\n$$\n$$\nR(-50) = \\frac{1+(-50)/2}{1-(-50)/2} = \\frac{1-25}{1+25} = -\\frac{24}{26} = -\\frac{12}{13}\n$$\n$$\nR_{\\mathrm{RE}}(-100) = \\frac{4(-12/13)^2 - (-49/51)}{3} = \\frac{4(144/169) + 49/51}{3} = \\frac{576/169 + 49/51}{3}\n$$\nNumerically, this evaluates to approximately $\\frac{3.40828 + 0.96078}{3} \\approx 1.45635$. This value is greater than $1$. The condition does not hold. This demonstrates that Richardson extrapolation of an A-stable method does not necessarily preserve A-stability.\n\nCase $5$: Consistency check for $\\lambda=0$.\nParameters are $\\lambda=0$, $y_0=1$, $T=1$, and $h=0.25$.\nThe IVP is $y'(t)=0$ with $y(0)=1$, so the exact solution is $y(t)=1$ for all $t$. The exact value at the final time is $y(T)=1$.\nFor $\\lambda=0$, the argument of the stability function is $z=h\\lambda=0$.\n$R(0) = \\frac{1+0}{1-0} = 1$.\nThe trapezoidal approximation at any step size $h$ is $y^{\\mathrm{trap}}(T;h) = R(0)^{T/h} y_0 = 1^{T/h} \\cdot 1 = 1$.\nTherefore, $y^{\\mathrm{trap}}(T;0.25) = 1$ and $y^{\\mathrm{trap}}(T;0.125)=1$.\nThe Richardson-extrapolated solution is:\n$$\ny^{\\mathrm{RE}}(T;0.25) = \\frac{4 y^{\\mathrm{trap}}(T;0.125) - y^{\\mathrm{trap}}(T;0.25)}{3} = \\frac{4(1) - 1}{3} = 1\n$$\nThe absolute error is $|y(T) - y^{\\mathrm{RE}}(T;0.25)| = |1 - 1| = 0$. This confirms the method is consistent, as it exactly solves the trivial case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Executes the computations for all five test cases as required by the problem statement.\n    \"\"\"\n\n    # Helper function to compute the stability function R(z)\n    def R_trap(z: complex) -> complex:\n        \"\"\"Stability function of the trapezoidal method.\"\"\"\n        # Handle the case z=2 where the denominator is zero\n        if np.isclose(z, 2.0):\n            return np.inf\n        return (1 + z / 2) / (1 - z / 2)\n\n    # Helper function for trapezoidal method solution\n    def trapezoidal_solver(h: float, T: float, y0: complex, lam: complex) -> complex:\n        \"\"\"\n        Computes the numerical solution y(T) using the trapezoidal method.\n        \"\"\"\n        # Ensure T is a multiple of h for integer number of steps\n        num_steps = round(T / h)\n        if not np.isclose(T, num_steps * h):\n            raise ValueError(\"T must be an integer multiple of h.\")\n        \n        z = h * lam\n        y_T = (R_trap(z)**num_steps) * y0\n        return y_T\n\n    # Helper function for Richardson extrapolation\n    def richardson_extrapolation(h: float, T: float, y0: complex, lam: complex, p: int) -> complex:\n        \"\"\"\n        Computes the Richardson-extrapolated solution y_RE(T; h).\n        \"\"\"\n        y_h = trapezoidal_solver(h, T, y0, lam)\n        y_h_half = trapezoidal_solver(h / 2, T, y0, lam)\n        \n        y_RE = (2**p * y_h_half - y_h) / (2**p - 1)\n        return y_RE\n\n    # Helper function for the RE stability function\n    def R_re(z: complex) -> complex:\n        \"\"\"One-step stability function of the Richardson-extrapolated method.\"\"\"\n        return (4 * R_trap(z / 2)**2 - R_trap(z)) / 3\n\n    results = []\n\n    # Case 1: Observed order for the trapezoidal method\n    lam1_2 = -1.0 + 0j\n    y0_1_2 = 1.0 + 0j\n    T1_2 = 1.0\n    h1_2 = 0.2\n    p_trap = 2\n\n    y_exact_1_2 = y0_1_2 * np.exp(lam1_2 * T1_2)\n    y_trap_h = trapezoidal_solver(h1_2, T1_2, y0_1_2, lam1_2)\n    y_trap_h_half = trapezoidal_solver(h1_2 / 2, T1_2, y0_1_2, lam1_2)\n    \n    E_h = abs(y_exact_1_2 - y_trap_h)\n    E_h_half = abs(y_exact_1_2 - y_trap_h_half)\n    \n    # Avoid division by zero, though not expected here\n    ratio1 = E_h / E_h_half if E_h_half != 0 else float('inf')\n    results.append(float(ratio1))\n\n    # Case 2: Observed order for the Richardson-extrapolated method\n    p_re_theory = 4\n\n    y_re_h = richardson_extrapolation(h1_2, T1_2, y0_1_2, lam1_2, p_trap)\n    y_re_h_half = richardson_extrapolation(h1_2 / 2, T1_2, y0_1_2, lam1_2, p_trap)\n\n    E_h_re = abs(y_exact_1_2 - y_re_h)\n    E_h_half_re = abs(y_exact_1_2 - y_re_h_half)\n\n    ratio2 = E_h_re / E_h_half_re if E_h_half_re != 0 else float('inf')\n    results.append(float(ratio2))\n    \n    # Case 3: Absolute stability check for RE at z = -1\n    z3 = -1.0 + 0j\n    is_stable3 = abs(R_re(z3)) <= 1.0\n    results.append(is_stable3)\n\n    # Case 4: Absolute stability check for RE at z = -100\n    z4 = -100.0 + 0j\n    is_stable4 = abs(R_re(z4)) <= 1.0\n    results.append(is_stable4)\n\n    # Case 5: Consistency edge case\n    lam5 = 0.0 + 0j\n    y0_5 = 1.0 + 0j\n    T5 = 1.0\n    h5 = 0.25\n\n    y_exact_5 = y0_5 * np.exp(lam5 * T5)\n    y_re_5 = richardson_extrapolation(h5, T5, y0_5, lam5, p_trap)\n    error5 = abs(y_exact_5 - y_re_5)\n    results.append(float(error5))\n\n    # Final print statement\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}