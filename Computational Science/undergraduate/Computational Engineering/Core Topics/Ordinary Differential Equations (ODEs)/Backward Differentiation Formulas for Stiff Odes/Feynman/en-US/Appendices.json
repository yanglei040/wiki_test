{
    "hands_on_practices": [
        {
            "introduction": "To truly appreciate why specialized methods like Backward Differentiation Formulas are indispensable, we must first quantify the problem they solve. This exercise provides a foundational, back-of-the-envelope analysis comparing the computational cost of a standard explicit method against a BDF method for a classically stiff problem. By estimating the number of function evaluations, you will gain a concrete understanding of the 'stiffness penalty' and the remarkable efficiency of implicit solvers in these challenging regimes .",
            "id": "2372592",
            "problem": "You are tasked with integrating the van der Pol oscillator, a nonlinear Ordinary Differential Equation (ODE), given by\n$$\n\\begin{cases}\nx'(t) = y(t),\\\\\ny'(t) = \\mu \\left(1 - x(t)^2\\right) y(t) - x(t),\n\\end{cases}\n$$\nover the interval $[0, T]$ with $T = 20$, for the stiff regime $\\mu = 10^3$. You must achieve a uniform target accuracy corresponding to a global error tolerance $\\varepsilon = 10^{-3}$.\n\nConsider two time-stepping strategies:\n- An explicit $s$-stage Runge–Kutta (RK) method with $s = 4$ and order $p_{\\mathrm{RK}} = 4$.\n- A $k$-step Backward Differentiation Formula (BDF) method with $k = 2$ and order $p_{\\mathrm{BDF}} = 2$, solved each step by Newton’s method using $m = 3$ residual (right-hand side) evaluations per step and a single Jacobian factorization reused across Newton iterations.\n\nAssume the following well-tested facts:\n- For stiff problems, linearization near attracting segments yields a dominant negative eigenvalue magnitude $|\\lambda| \\sim \\mathcal{O}(\\mu)$.\n- Explicit stability for a method applied to the scalar test equation $z'(t)=\\lambda z(t)$ requires $|\\Phi(\\lambda \\Delta t)| < 1$, which for the classical $4$-stage RK method implies a practical bound $|\\lambda| \\Delta t \\lesssim c_{\\mathrm{RK}}$ along the negative real axis. Use a conservative $c_{\\mathrm{RK}} = 2$.\n- BDF of order $2$ is $A$-stable (and stiffly stable) on the negative real axis, so its step size is not limited by stiffness but by accuracy.\n- For a method of order $p$, to meet a prescribed accuracy, the step size must be chosen small enough that the leading truncation-error scaling with $\\Delta t^p$ does not exceed the tolerance. Use this to set accuracy-driven step sizes.\n\nLet $N_{\\mathrm{RK}}$ and $N_{\\mathrm{BDF}}$ denote the number of time steps for the RK and BDF methods, respectively. Let $F_{\\mathrm{RK}}$ and $F_{\\mathrm{BDF}}$ denote the total number of right-hand side function evaluations. For the explicit $4$-stage RK method, each step costs $s = 4$ right-hand side evaluations, so $F_{\\mathrm{RK}} \\approx 4 N_{\\mathrm{RK}}$. For the BDF method with Newton’s method taking $m = 3$ residual evaluations per step, take $F_{\\mathrm{BDF}} \\approx 3 N_{\\mathrm{BDF}}$.\n\nUsing only the stiffness-induced stability restriction for the explicit method and the accuracy-induced restriction for both methods, estimate $F_{\\mathrm{RK}}$ and $F_{\\mathrm{BDF}}$ for the parameters above and select the best estimate below.\n\nA. $F_{\\mathrm{RK}} \\approx 4 \\times 10^4$ and $F_{\\mathrm{BDF}} \\approx 1.9 \\times 10^3$; the BDF method needs roughly $20$ times fewer function evaluations.\n\nB. $F_{\\mathrm{RK}} \\approx 1.6 \\times 10^5$ and $F_{\\mathrm{BDF}} \\approx 1.9 \\times 10^3$; the BDF method needs roughly $80$ times fewer function evaluations.\n\nC. $F_{\\mathrm{RK}} \\approx 1.0 \\times 10^4$ and $F_{\\mathrm{BDF}} \\approx 5.7 \\times 10^4$; the explicit method is more efficient.\n\nD. $F_{\\mathrm{RK}} \\approx F_{\\mathrm{BDF}} \\approx 2.0 \\times 10^3$; stability limitations are comparable, so both methods have similar cost.",
            "solution": "The problem statement must first be subjected to rigorous validation.\n\n### Step 1: Extract Givens\nThe provided information is as follows:\n- **Governing Equations**: The van der Pol oscillator, defined by the system of first-order Ordinary Differential Equations (ODEs):\n$$\n\\begin{cases}\nx'(t) = y(t),\\\\\ny'(t) = \\mu \\left(1 - x(t)^2\\right) y(t) - x(t),\n\\end{cases}\n$$\n- **Parameters**:\n  - Stiffness parameter: $\\mu = 10^3$.\n  - Integration interval: $[0, T]$ with $T = 20$.\n  - Global error tolerance: $\\varepsilon = 10^{-3}$.\n- **Method 1 (Explicit Runge-Kutta)**:\n  - Number of stages: $s = 4$.\n  - Order of accuracy: $p_{\\mathrm{RK}} = 4$.\n  - Function evaluations per step: $s = 4$.\n  - Total evaluations: $F_{\\mathrm{RK}} \\approx 4 N_{\\mathrm{RK}}$.\n- **Method 2 (Backward Differentiation Formula)**:\n  - Number of steps: $k = 2$.\n  - Order of accuracy: $p_{\\mathrm{BDF}} = 2$.\n  - Nonlinear solver: Newton's method.\n  - Function evaluations per step: $m = 3$.\n  - Total evaluations: $F_{\\mathrm{BDF}} \\approx 3 N_{\\mathrm{BDF}}$.\n- **Assumptions and Heuristics**:\n  - The problem is stiff, with a dominant negative eigenvalue magnitude $|\\lambda| \\sim \\mathcal{O}(\\mu)$.\n  - The stability constraint for the explicit $4$-stage RK method is $|\\lambda| \\Delta t \\lesssim c_{\\mathrm{RK}}$, with a provided effective constant $c_{\\mathrm{RK}} = 2$.\n  - The BDF method of order $2$ is A-stable, meaning its step size is limited by accuracy, not stability.\n  - An accuracy-driven step size $\\Delta t$ for a method of order $p$ is determined by the requirement that the global error, which scales with $\\Delta t^p$, meets the tolerance $\\varepsilon$.\n  - The cost analysis should use the stiffness-induced stability restriction for the explicit method and the accuracy-induced restriction for both methods.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the criteria for validity.\n- **Scientifically Grounded**: The problem is well-grounded in the field of numerical analysis for ordinary differential equations. The van der Pol equation is a classic example of a stiff system. The concepts of stiffness, spectral properties of the Jacobian, stability regions of explicit and implicit methods (like Runge-Kutta and BDF), and computational cost analysis based on function evaluations are all standard and fundamental to the discipline.\n- **Well-Posed**: The problem is well-posed. It provides sufficient data and a clear set of assumptions to perform the required estimation of computational costs. The question asks for an estimate, which is appropriate given the heuristic nature of the provided constants and scaling laws. A unique, meaningful estimate can be derived.\n- **Objective**: The problem is stated in precise, objective mathematical language. There are no subjective or ambiguous terms.\n\n### Step 3: Verdict and Action\nThe problem statement is scientifically sound, well-posed, and objective. There are no identifiable flaws. Therefore, the problem is **valid**. I will proceed to derive the solution.\n\n### Derivation of Solution\nThe task is to estimate the total number of right-hand side function evaluations, $F_{\\mathrm{RK}}$ and $F_{\\mathrm{BDF}}$, required to integrate the given stiff ODE over the interval $[0, 20]$ to a global accuracy of $\\varepsilon = 10^{-3}$.\n\n**1. Analysis of the Explicit Runge-Kutta Method (RK-4)**\nFor a stiff problem, the step size $\\Delta t_{\\mathrm{RK}}$ of an explicit method is severely restricted by stability requirements. The problem states this explicitly and provides the stability condition:\n$$|\\lambda| \\Delta t_{\\mathrm{RK}} \\lesssim c_{\\mathrm{RK}}$$\nWe are given that the magnitude of the dominant eigenvalue scales with $\\mu$, so we take $|\\lambda| \\approx \\mu = 10^3$. The stability constant is given as $c_{\\mathrm{RK}} = 2$.\nSubstituting these values into the stability condition gives:\n$$10^3 \\cdot \\Delta t_{\\mathrm{RK}} \\lesssim 2$$\nThis implies that the maximum stable step size is:\n$$\\Delta t_{\\mathrm{RK}} \\approx \\frac{2}{10^3} = 2 \\times 10^{-3}$$\nThe problem states to use this stability-induced restriction. The global error for this method is $\\mathcal{O}(\\Delta t_{\\mathrm{RK}}^4)$, which for such a small step size will be far below the required tolerance $\\varepsilon = 10^{-3}$. Indeed, the step size is dictated by stability, not accuracy.\n\nThe total number of steps, $N_{\\mathrm{RK}}$, required to cover the interval of length $T = 20$ is:\n$$N_{\\mathrm{RK}} = \\frac{T}{\\Delta t_{\\mathrm{RK}}} = \\frac{20}{2 \\times 10^{-3}} = 10 \\times 10^3 = 10^4$$\nThe explicit RK-4 method requires $s = 4$ function evaluations per step. The total number of evaluations is therefore:\n$$F_{\\mathrm{RK}} \\approx s \\cdot N_{\\mathrm{RK}} = 4 \\times 10^4$$\n\n**2. Analysis of the Backward Differentiation Formula Method (BDF-2)**\nThe BDF-2 method is A-stable, which means its step size is not constrained by the stiffness of the problem. Instead, $\\Delta t_{\\mathrm{BDF}}$ is determined by the accuracy requirement. The method has order $p_{\\mathrm{BDF}} = 2$.\n\nThe global error of a method of order $p$ scales as $\\mathcal{O}(\\Delta t^p)$. To achieve a global error tolerance of $\\varepsilon$, the step size $\\Delta t$ must satisfy a relation of the form $C (\\Delta t)^p \\approx \\varepsilon$, where $C$ is a constant that depends on the problem's solution and its derivatives. In the absence of more information, a standard heuristic for estimation is to assume $C \\approx 1$.\nApplying this to the BDF-2 method ($p=2$):\n$$(\\Delta t_{\\mathrm{BDF}})^2 \\approx \\varepsilon = 10^{-3}$$\nSolving for the accuracy-limited step size $\\Delta t_{\\mathrm{BDF}}$:\n$$\\Delta t_{\\mathrm{BDF}} \\approx \\sqrt{10^{-3}} = 10^{-1.5} = 10^{-1} \\sqrt{10} \\approx 3.162 \\times 10^{-2}$$\nThe total number of steps, $N_{\\mathrm{BDF}}$, is:\n$$N_{\\mathrm{BDF}} = \\frac{T}{\\Delta t_{\\mathrm{BDF}}} = \\frac{20}{\\sqrt{10^{-3}}} = \\frac{20}{10^{-1.5}} = 20 \\times 10^{1.5} = 200\\sqrt{10} \\approx 200 \\times 3.162 = 632.4$$\nWe can round this to $N_{\\mathrm{BDF}} \\approx 633$ steps.\n\nEach step of the BDF method requires solving a nonlinear system. It is stated that this is done using Newton's method, costing $m=3$ function (residual) evaluations per step. The total number of function evaluations is:\n$$F_{\\mathrm{BDF}} \\approx m \\cdot N_{\\mathrm{BDF}} = 3 \\times 633 = 1899$$\nThis value is closely approximated by $1.9 \\times 10^3$.\n\n**3. Comparison of Costs**\nOur estimates are:\n- $F_{\\mathrm{RK}} \\approx 4 \\times 10^4$\n- $F_{\\mathrm{BDF}} \\approx 1.9 \\times 10^3$\n\nThe ratio of the costs is:\n$$\\frac{F_{\\mathrm{RK}}}{F_{\\mathrm{BDF}}} \\approx \\frac{4 \\times 10^4}{1.9 \\times 10^3} = \\frac{40}{1.9} \\approx 21.05$$\nThis confirms that the implicit BDF method is significantly more efficient for this stiff problem, requiring roughly $21$ times fewer function evaluations than the explicit RK method.\n\n### Evaluation of Provided Options\n\n**A. $F_{\\mathrm{RK}} \\approx 4 \\times 10^4$ and $F_{\\mathrm{BDF}} \\approx 1.9 \\times 10^3$; the BDF method needs roughly $20$ times fewer function evaluations.**\n- The value $F_{\\mathrm{RK}} \\approx 4 \\times 10^4$ matches our derived estimate precisely.\n- The value $F_{\\mathrm{BDF}} \\approx 1.9 \\times 10^3$ matches our derived estimate ($1899$).\n- The comparison that the BDF method is \"roughly $20$ times\" more efficient is consistent with our calculated ratio of approximately $21$.\n- **Verdict: Correct.**\n\n**B. $F_{\\mathrm{RK}} \\approx 1.6 \\times 10^5$ and $F_{\\mathrm{BDF}} \\approx 1.9 \\times 10^3$; the BDF method needs roughly $80$ times fewer function evaluations.**\n- The value for $F_{\\mathrm{BDF}}$ is correct.\n- The value $F_{\\mathrm{RK}} \\approx 1.6 \\times 10^5$ is incorrect. It is four times larger than our derived estimate. This would imply $N_{\\mathrm{RK}} = 4 \\times 10^4$, not $10^4$.\n- **Verdict: Incorrect.**\n\n**C. $F_{\\mathrm{RK}} \\approx 1.0 \\times 10^4$ and $F_{\\mathrm{BDF}} \\approx 5.7 \\times 10^4$; the explicit method is more efficient.**\n- The value $F_{\\mathrm{RK}} \\approx 1.0 \\times 10^4$ is incorrect. It would imply $N_{\\mathrm{RK}} = 2500$, which violates the stability condition.\n- The value $F_{\\mathrm{BDF}} \\approx 5.7 \\times 10^4$ is incorrect. It would imply a far smaller step size than required for the specified accuracy.\n- The conclusion that the explicit method is more efficient is fundamentally wrong for a stiff problem of this nature.\n- **Verdict: Incorrect.**\n\n**D. $F_{\\mathrm{RK}} \\approx F_{\\mathrm{BDF}} \\approx 2.0 \\times 10^3$; stability limitations are comparable, so both methods have similar cost.**\n- The premise \"stability limitations are comparable\" is false. This is the defining characteristic of a stiff problem: the stability limit for an explicit method is drastically more restrictive than the accuracy limit. An A-stable method like BDF-2 does not have this stability limitation. Consequently, the costs are not similar. $F_{\\mathrm{RK}} \\approx 4 \\times 10^4$ is not close to $2.0 \\times 10^3$.\n- **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Moving from estimation to direct simulation, this hands-on coding exercise allows you to observe the defining characteristics of stiff systems and their solvers. You will implement simple explicit and BDF integrators on a canonical stiff problem and measure the 'collapse time' to the underlying slow manifold. This practice makes the abstract concepts of stability, fast-transient decay, and the failure of explicit methods tangibly clear .",
            "id": "2374906",
            "problem": "Consider the dimensionless linear stiff system of ordinary differential equations (ODEs)\n$$\n\\frac{d x}{d t} = -x, \\qquad \\varepsilon \\frac{d y}{d t} = -y,\n$$\nwith initial condition\n$$\nx(0)=x_0,\\quad y(0)=y_0,\n$$\nwhere $\\varepsilon > 0$ is a stiffness parameter. The slow manifold is the set\n$$\n\\mathcal{M} = \\{(x,y) \\in \\mathbb{R}^2 : y=0\\},\n$$\nand the fast subspace is the set\n$$\n\\mathcal{F} = \\{(x,y) \\in \\mathbb{R}^2 : x=0\\}.\n$$\nDefine the collapse time to the slow manifold for a discrete numerical trajectory $\\{(x_n,y_n)\\}_{n=0}^N$ computed on an equally spaced time grid $t_n = n h$ with fixed step size $h > 0$ as\n$$\n\\tau = \\min\\{t_n : |y_n| \\le \\delta\\},\n$$\nwith the convention that $\\tau=+\\infty$ if no index $n \\in \\{0, 1, \\dots, N\\}$ satisfies $|y_n| \\le \\delta$. All quantities are dimensionless.\n\nFor each test case below, compute two discrete trajectories on the same grid $t_n = n h$ for $n=0,1,\\dots,\\lfloor T/h\\rfloor$:\n- one trajectory generated by a second-order Backward Differentiation Formula (BDF) method of order two (BDF2) with fixed step size $h$,\n- one trajectory generated by an explicit method of first order (forward Euler) with the same step size $h$.\n\nFor each trajectory, report the collapse time $\\tau$ to the slow manifold $\\mathcal{M}$ using the tolerance $\\delta$ specified in the test case. If no collapse occurs on or before time $T$, report $+\\infty$ for that trajectory.\n\nTest suite (each case is a tuple $(\\varepsilon,h,T,x_0,y_0,\\delta)$):\n- Case A (happy path, strongly stiff with explicit instability): $(\\varepsilon,h,T,x_0,y_0,\\delta) = (10^{-3},\\,0.05,\\,2.0,\\,0,\\,1,\\,10^{-6})$.\n- Case B (stable explicit evolution near stability boundary): $(\\varepsilon,h,T,x_0,y_0,\\delta) = (10^{-2},\\,0.015,\\,2.0,\\,0,\\,1,\\,10^{-6})$.\n- Case C (boundary case starting on the slow manifold): $(\\varepsilon,h,T,x_0,y_0,\\delta) = (0.5,\\,0.25,\\,3.0,\\,2,\\,0,\\,10^{-12})$.\n\nYour program must produce a single line of output containing the six collapse times in the following order and format:\n$$\n[\\tau_{\\mathrm{BDF2}}^{\\mathrm{A}},\\ \\tau_{\\mathrm{FE}}^{\\mathrm{A}},\\ \\tau_{\\mathrm{BDF2}}^{\\mathrm{B}},\\ \\tau_{\\mathrm{FE}}^{\\mathrm{B}},\\ \\tau_{\\mathrm{BDF2}}^{\\mathrm{C}},\\ \\tau_{\\mathrm{FE}}^{\\mathrm{C}}],\n$$\nwhere $\\tau_{\\mathrm{BDF2}}^{\\mathrm{A}}$ is the BDF2 collapse time for Case A, $\\tau_{\\mathrm{FE}}^{\\mathrm{A}}$ is the forward Euler collapse time for Case A, and similarly for Cases B and C. Use the real number $+\\infty$ if a trajectory does not collapse by time $T$. The output must be exactly one line in the specified bracketed, comma-separated format. No other text should be printed.",
            "solution": "The problem presented is well-posed and scientifically sound, providing a canonical example for the study of numerical methods for stiff ordinary differential equations (ODEs). We shall proceed directly to the solution.\n\nThe system of ODEs is given by:\n$$\n\\frac{d x}{d t} = -x\n$$\n$$\n\\varepsilon \\frac{d y}{d t} = -y\n$$\nThis is a linear, decoupled system. With initial conditions $x(0)=x_0$ and $y(0)=y_0$, the analytical solution is:\n$$\nx(t) = x_0 e^{-t}\n$$\n$$\ny(t) = y_0 e^{-t/\\varepsilon}\n$$\nThe parameter $\\varepsilon > 0$ controls the stiffness of the system. When $0  \\varepsilon \\ll 1$, the system exhibits two distinct time scales: a slow scale for $x(t)$ characterized by a decay rate of $1$, and a fast scale for $y(t)$ with a decay rate of $1/\\varepsilon$. The fast dynamics of $y(t)$ drive the solution towards the slow manifold $\\mathcal{M} = \\{(x,y) : y=0\\}$. A numerical method must be able to handle this disparity in scales without requiring an excessively small time step $h$.\n\nWe will analyze the two specified numerical methods. Let the state vector be $\\mathbf{u}_n = (x_n, y_n)^\\top$ at time $t_n = n h$.\n\nThe first-order explicit Forward Euler (FE) method has the form $\\mathbf{u}_{n+1} = \\mathbf{u}_n + h f(\\mathbf{u}_n)$. For the given ODE, this yields the recurrence relations:\n$$\nx_{n+1} = (1 - h) x_n\n$$\n$$\ny_{n+1} = (1 - \\frac{h}{\\varepsilon}) y_n\n$$\nThe stability of the FE method for an equation $\\dot{z} = \\lambda z$ requires the Courant-Friedrichs-Lewy (CFL) condition $|1 + h\\lambda| \\le 1$. For the $y$-component, $\\lambda = -1/\\varepsilon$. This imposes a severe restriction on the time step: $|1 - h/\\varepsilon| \\le 1$, which implies $h \\le 2\\varepsilon$. If this condition is violated, the numerical solution for $y_n$ will exhibit unstable oscillations of growing amplitude.\n\nThe second-order Backward Differentiation Formula (BDF$2$) is an implicit, two-step method. For an ODE $\\dot{\\mathbf{u}} = f(\\mathbf{u})$, the formula is:\n$$\n\\frac{3}{2} \\mathbf{u}_{n+1} - 2\\mathbf{u}_n + \\frac{1}{2}\\mathbf{u}_{n-1} = h f(\\mathbf{u}_{n+1})\n$$\nFor our linear system, this becomes $(\\frac{3}{2}I - hA)\\mathbf{u}_{n+1} = 2\\mathbf{u}_n - \\frac{1}{2}\\mathbf{u}_{n-1}$, where $A = \\mathrm{diag}(-1, -1/\\varepsilon)$. This yields the component-wise recurrence relations:\n$$\nx_{n+1} = \\frac{2x_n - 0.5x_{n-1}}{1.5 + h}\n$$\n$$\ny_{n+1} = \\frac{2y_n - 0.5y_{n-1}}{1.5 + h/\\varepsilon}\n$$\nBeing a two-step method, BDF$2$ requires a starting procedure to find $\\mathbf{u}_1$ from $\\mathbf{u}_0$. We will employ a single step of the first-order implicit Backward Euler (BE) method (also known as BDF$1$), which is suitable for stiff problems. The BE update is $\\mathbf{u}_1 = (I - hA)^{-1}\\mathbf{u}_0$, giving:\n$$\nx_1 = \\frac{x_0}{1+h}, \\quad y_1 = \\frac{y_0}{1+h/\\varepsilon}\n$$\nThe BDF$2$ method is A-stable, meaning its region of absolute stability contains the entire left half of the complex plane. Consequently, it does not suffer from the same severe time step restriction as FE when applied to stiff systems and is expected to be stable for any $h > 0$.\n\nNow we analyze each test case:\n\nCase A: $(\\varepsilon,h,T,x_0,y_0,\\delta) = (10^{-3},\\,0.05,\\,2.0,\\,0,\\,1,\\,10^{-6})$\nThe stiffness is strong, with $\\varepsilon = 10^{-3}$. The stability condition for FE is $h \\le 2\\varepsilon \\implies 0.05 \\le 0.002$, which is violated. The amplification factor for $y_n$ is $(1 - h/\\varepsilon) = (1 - 0.05/10^{-3}) = -49$. The magnitude of $y_n$ will grow as $|y_n| = 49^n |y_0|$, so it will never collapse to the manifold. Thus, $\\tau_{\\mathrm{FE}}^{\\mathrm{A}} = +\\infty$.\nFor BDF$2$, the method is stable. After a brief initial transient, the numerical solution for $y$ will decay. A direct calculation shows $|y_5| \\approx 7.78 \\times 10^{-6} > \\delta$ and $|y_6| \\approx 3.38 \\times 10^{-7} \\le \\delta$. The collapse occurs at step $n=6$, so the collapse time is $\\tau_{\\mathrm{BDF2}}^{\\mathrm{A}} = 6 \\times h = 6 \\times 0.05 = 0.3$.\n\nCase B: $(\\varepsilon,h,T,x_0,y_0,\\delta) = (10^{-2},\\,0.015,\\,2.0,\\,0,\\,1,\\,10^{-6})$\nThe stiffness is moderate, with $\\varepsilon = 10^{-2}$. The FE stability condition is $h \\le 0.02$, which is satisfied since $h=0.015$. The FE method is stable. The amplification factor is $(1 - h/\\varepsilon) = (1 - 0.015/10^{-2}) = -0.5$. We need to find the smallest integer $n$ such that $|y_n| = |(-0.5)^n y_0| \\le 10^{-6}$. This requires $n \\ln(0.5) \\le \\ln(10^{-6})$, which gives $n \\ge 19.93$. Thus, collapse occurs at $n=20$. The collapse time is $\\tau_{\\mathrm{FE}}^{\\mathrm{B}} = 20 \\times h = 20 \\times 0.015 = 0.3$.\nFor BDF$2$, the method is also stable. The recurrence for $y_n$ is $y_{n+1} = (2y_n - 0.5y_{n-1}) / (1.5 + 1.5)$. With $y_0 = 1$ and $y_1 = y_0 / (1 + 1.5) = 0.4$, we find $y_2 = 0.1$ and $y_3=0$. The trajectory collapses exactly to the manifold at step $n=3$. The collapse time is $\\tau_{\\mathrm{BDF2}}^{\\mathrm{B}} = 3 \\times h = 3 \\times 0.015 = 0.045$.\n\nCase C: $(\\varepsilon,h,T,x_0,y_0,\\delta) = (0.5,\\,0.25,\\,3.0,\\,2,\\,0,\\,10^{-12})$\nThe initial condition $(x_0, y_0)=(2,0)$ lies on the slow manifold $\\mathcal{M}$. The analytical solution for $y(t)$ is identically zero for all $t \\ge 0$. Both FE and BDF$2$ preserve this invariant. For FE, if $y_n=0$, then $y_{n+1} = (1 - h/\\varepsilon) \\times 0 = 0$. For BDF$2$, if $y_{n-1}=0$ and $y_n=0$, then $y_{n+1}=0$.\nSince $y_0=0$, the numerical trajectory satisfies $y_n=0$ for all $n \\ge 0$ for both methods. The collapse condition $|y_n| \\le \\delta$ is satisfied at the very first step, $n=0$. Therefore, the collapse time is $\\tau = t_0 = 0$ for both methods. Thus, $\\tau_{\\mathrm{FE}}^{\\mathrm{C}} = 0.0$ and $\\tau_{\\mathrm{BDF2}}^{\\mathrm{C}} = 0.0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the collapse time to the slow manifold for a stiff ODE system\n    using Forward Euler and BDF2 methods for three test cases.\n    \"\"\"\n    test_cases = [\n        # Case A: (eps, h, T, x0, y0, delta)\n        (1e-3, 0.05, 2.0, 0, 1, 1e-6),\n        # Case B\n        (1e-2, 0.015, 2.0, 0, 1, 1e-6),\n        # Case C\n        (0.5, 0.25, 3.0, 2, 0, 1e-12),\n    ]\n\n    results = []\n    for case in test_cases:\n        eps, h, T, x0, y0, delta = case\n        N = int(T / h)\n\n        # --- Forward Euler Trajectory and Collapse Time ---\n        # Since the collapse time only depends on y, we only evolve y.\n        tau_fe = float('inf')\n        y_fe_n = float(y0)\n\n        if abs(y_fe_n) = delta:\n            tau_fe = 0.0\n        else:\n            for n in range(N):\n                y_fe_n = y_fe_n * (1.0 - h / eps)\n                # Check for numerical overflow in unstable cases\n                if not np.isfinite(y_fe_n):\n                    # The trajectory has diverged, it will never collapse.\n                    break\n                if abs(y_fe_n) = delta:\n                    tau_fe = (n + 1) * h\n                    break\n        \n        # --- BDF2 Trajectory and Collapse Time ---\n        tau_bdf2 = float('inf')\n        \n        # History for y component\n        y_bdf_hist = np.zeros(N + 1, dtype=float)\n        y_bdf_hist[0] = float(y0)\n\n        if abs(y_bdf_hist[0]) = delta:\n            tau_bdf2 = 0.0\n        else:\n            # First step using Backward Euler (BDF1)\n            y_bdf_hist[1] = y_bdf_hist[0] / (1.0 + h / eps)\n            \n            if abs(y_bdf_hist[1]) = delta:\n                tau_bdf2 = h\n            else:\n                # Subsequent steps (n=2 to N) using BDF2\n                for n in range(1, N):  # n from 1 to N-1\n                    # BDF2 formula to compute y_{n+1}\n                    y_bdf_hist[n+1] = (2.0 * y_bdf_hist[n] - 0.5 * y_bdf_hist[n-1]) / (1.5 + h / eps)\n                    if abs(y_bdf_hist[n+1]) = delta:\n                        tau_bdf2 = (n + 1) * h\n                        break\n        \n        results.append(tau_bdf2)\n        results.append(tau_fe)\n\n    # Use map(str, ...) to handle float('inf') becoming 'inf'\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having established the stability benefits of BDF methods, we now look inside the 'black box' of an implicit time step for nonlinear problems. This exercise focuses on the Newton-Raphson iteration, the workhorse for solving the implicit equations, and investigates a critical practical challenge: the conditioning of the Jacobian matrix. By exploring how stiffness impacts the linear algebra at the heart of the solver, you will develop a more nuanced understanding of the implementation and potential pitfalls of BDF methods .",
            "id": "2374964",
            "problem": "Consider the autonomous stiff system of ordinary differential equations (ODE) defined for the state vector $\\boldsymbol{y}(t) = [y_1(t), y_2(t)]^\\top$ by\n$$\n\\frac{d}{dt}\n\\begin{bmatrix}\ny_1 \\\\\ny_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n- k\\, y_1 + c\\,(y_2 - y_1) + a\\, y_1^2 \\\\\n\\frac{1}{\\varepsilon}\\,(y_1 - y_2) - b\\, y_2^2\n\\end{bmatrix},\n$$\nwith parameters $k = 1$, $c = 10$, $a = 1$, $b = 1$, and a small positive parameter $\\varepsilon > 0$ controlling a nearly degenerate fast coupling. All quantities are dimensionless and have no physical units. The task is to study, within a single step of the Backward Differentiation Formula (BDF) of order one (also known as implicit Euler), how the conditioning of the Jacobian affects the convergence of the Newton-Raphson iteration for the nonlinear algebraic system arising at the new time level.\n\nFor a given previous state $\\boldsymbol{y}_n \\in \\mathbb{R}^2$ at time $t_n$ and a step size $h > 0$, the BDF-1 implicit update $\\boldsymbol{y}_{n+1}$ at time $t_{n+1} = t_n + h$ is defined as the solution of the nonlinear residual equation\n$$\nG(\\boldsymbol{y}_{n+1}) = \\boldsymbol{y}_{n+1} - \\boldsymbol{y}_n - h f(\\boldsymbol{y}_{n+1}) = \\mathbf{0},\n$$\nwhere $f(\\boldsymbol{y})$ denotes the right-hand side of the ODE system. The Jacobian of the residual is\n$$\nJ(\\boldsymbol{y}) = \\frac{\\partial G}{\\partial \\boldsymbol{y}}(\\boldsymbol{y}) = I - h \\frac{\\partial f}{\\partial \\boldsymbol{y}}(\\boldsymbol{y}),\n$$\nwith\n$$\n\\frac{\\partial f}{\\partial \\boldsymbol{y}}(\\boldsymbol{y}) =\n\\begin{bmatrix}\n- k - c + 2 a y_1  c \\\\\n\\frac{1}{\\varepsilon}  -\\frac{1}{\\varepsilon} - 2 b y_2\n\\end{bmatrix}.\n$$\nUse the initial state $\\boldsymbol{y}_n = \\begin{bmatrix} 0.9 \\\\ 0.1 \\end{bmatrix}$, the Euclidean norm for residuals, the convergence tolerance $\\tau = 10^{-10}$ on $\\lVert G(\\boldsymbol{y}) \\rVert_2$, and the maximum allowed Newton-Raphson iterations $M = 50$. The initial Newton-Raphson iterate must be $\\boldsymbol{y}^{(0)} = \\boldsymbol{y}_n$. The spectral condition number of a matrix $A$ in the Euclidean norm is denoted $\\kappa_2(A)$.\n\nFor each test case below, apply a single BDF-1 step at $t_n = 0$ and report two quantities: \n$($i$)$ the integer number of Newton-Raphson iterations taken to satisfy $\\lVert G(\\boldsymbol{y}^{(k)}) \\rVert_2 \\le \\tau$ (use $-1$ if convergence is not achieved within $M$ iterations or if a linear solve fails$)$, and \n$($ii$)$ the base-ten logarithm $\\log_{10}$ of the maximum spectral condition number $\\kappa_2(J(\\boldsymbol{y}^{(k)}))$ encountered over all Newton-Raphson iterates in that step $($use the largest value observed up to termination, even if the iteration fails to converge$)$.\n\nTest suite (each case is a pair $(\\varepsilon, h)$):\n- Case $1$: $\\varepsilon = 1$, $h = 0.05$.\n- Case $2$: $\\varepsilon = 10^{-3}$, $h = 0.05$.\n- Case $3$: $\\varepsilon = 10^{-6}$, $h = 0.05$.\n- Case $4$: $\\varepsilon = 10^{-3}$, $h = 0.5$.\n- Case $5$: $\\varepsilon = 10^{-6}$, $h = 0.5$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, for each test case, first the integer iteration count, then the floating-point value of $\\log_{10}$ of the maximum condition number; thus the final list has length $10$. For example, the format is $[n_1,\\ell_1,n_2,\\ell_2,n_3,\\ell_3,n_4,\\ell_4,n_5,\\ell_5]$, where $n_i$ are integers and $\\ell_i$ are real numbers.",
            "solution": "The problem statement has been subjected to validation and is found to be scientifically grounded, well-posed, and complete. It represents a standard exercise in computational physics, specifically in the numerical analysis of stiff ordinary differential equations (ODEs). All parameters, initial conditions, and procedures are defined unambiguously. Thus, we proceed to the solution.\n\nThe task is to analyze the behavior of the Newton-Raphson method when solving the algebraic system that arises from a single step of the Backward Differentiation Formula of order one (BDF-1), also known as the implicit Euler method. The ODE system under consideration is\n$$\n\\frac{d\\boldsymbol{y}}{dt} = \\boldsymbol{f}(\\boldsymbol{y}) =\n\\begin{bmatrix}\n- k y_1 + c(y_2 - y_1) + a y_1^2 \\\\\n\\frac{1}{\\varepsilon}(y_1 - y_2) - b y_2^2\n\\end{bmatrix}\n$$\nwhere $\\boldsymbol{y}(t) = [y_1(t), y_2(t)]^\\top$ is the state vector. The parameters are given as $k=1$, $c=10$, $a=1$, and $b=1$. The parameter $\\varepsilon > 0$ controls the stiffness of the system; for $\\varepsilon \\ll 1$, the system exhibits dynamics on widely separated time scales, which is the definition of a stiff system.\n\nThe BDF-1 method approximates the solution at time $t_{n+1} = t_n + h$ using the value at $t_n$. The time derivative $\\frac{d\\boldsymbol{y}}{dt}$ at $t_{n+1}$ is approximated by $\\frac{\\boldsymbol{y}_{n+1} - \\boldsymbol{y}_n}{h}$. Substituting this into the ODE yields a nonlinear algebraic system for the unknown state $\\boldsymbol{y}_{n+1}$:\n$$\n\\frac{\\boldsymbol{y}_{n+1} - \\boldsymbol{y}_n}{h} = \\boldsymbol{f}(\\boldsymbol{y}_{n+1})\n$$\nThis is rearranged into a root-finding problem for the residual function $\\boldsymbol{G}(\\boldsymbol{y}_{n+1})$:\n$$\n\\boldsymbol{G}(\\boldsymbol{y}_{n+1}) = \\boldsymbol{y}_{n+1} - \\boldsymbol{y}_n - h \\boldsymbol{f}(\\boldsymbol{y}_{n+1}) = \\mathbf{0}\n$$\nThis nonlinear system is solved using the Newton-Raphson method. Starting with an initial guess $\\boldsymbol{y}^{(0)} = \\boldsymbol{y}_n$, successive approximations $\\boldsymbol{y}^{(k)}$ are generated by the iterative formula:\n$$\n\\boldsymbol{y}^{(k+1)} = \\boldsymbol{y}^{(k)} - [J(\\boldsymbol{y}^{(k)})]^{-1} \\boldsymbol{G}(\\boldsymbol{y}^{(k)})\n$$\nwhere $J(\\boldsymbol{y})$ is the Jacobian matrix of the residual function $\\boldsymbol{G}(\\boldsymbol{y})$ with respect to $\\boldsymbol{y}$. It is given by:\n$$\nJ(\\boldsymbol{y}) = \\frac{\\partial \\boldsymbol{G}}{\\partial \\boldsymbol{y}}(\\boldsymbol{y}) = \\boldsymbol{I} - h \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{y}}(\\boldsymbol{y})\n$$\nHere, $\\boldsymbol{I}$ is the $2 \\times 2$ identity matrix and $\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{y}}$ is the Jacobian of the ODE's right-hand side, provided as:\n$$\n\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{y}}(\\boldsymbol{y}) =\n\\begin{bmatrix}\n-k - c + 2 a y_1  c \\\\\n\\frac{1}{\\varepsilon}  -\\frac{1}{\\varepsilon} - 2 b y_2\n\\end{bmatrix}\n$$\nThe convergence of the Newton-Raphson method is critically dependent on the properties of the Jacobian matrix $J$. Specifically, the conditioning of $J$ is paramount. The spectral condition number, $\\kappa_2(J) = \\lVert J \\rVert_2 \\lVert J^{-1} \\rVert_2$, measures the sensitivity of the solution to the linear system $J \\Delta\\boldsymbol{y} = -\\boldsymbol{G}$ to perturbations. A large condition number indicates that $J$ is nearly singular, which can lead to large numerical errors in the computed step $\\Delta\\boldsymbol{y}$, thereby slowing down or preventing convergence.\n\nFor small $\\varepsilon$, the term $1/\\varepsilon$ becomes dominant in $\\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{y}}$. The Jacobian of the residual becomes:\n$$\nJ(\\boldsymbol{y}) =\n\\begin{bmatrix}\n1 - h(-k - c + 2 a y_1)  -hc \\\\\n-\\frac{h}{\\varepsilon}  1 - h(-\\frac{1}{\\varepsilon} - 2 b y_2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 + h(k + c - 2 a y_1)  -hc \\\\\n-\\frac{h}{\\varepsilon}  1 + \\frac{h}{\\varepsilon} + 2 h b y_2\n\\end{bmatrix}\n$$\nWhen the ratio $h/\\varepsilon$ is large, the entries involving this term dominate the matrix. This leads to a situation where the singular values of $J$ become widely separated, causing the condition number $\\kappa_2(J)$ to grow, approximately proportionally to $h/\\varepsilon$. This ill-conditioning is a direct consequence of applying a numerical method to a stiff system with a step size $h$ that is large relative to the fastest time scale (which is of order $\\varepsilon$).\n\nThe algorithm to be implemented will perform, for each test case $(\\varepsilon, h)$:\n$1$. Initialize the Newton-Raphson iteration with $\\boldsymbol{y}^{(0)} = \\boldsymbol{y}_n = [0.9, 0.1]^\\top$.\n$2$. For each iteration $k$ from $0$ up to a maximum of $M=50$:\n    a. Compute the residual $\\boldsymbol{G}(\\boldsymbol{y}^{(k)})$ and its Euclidean norm $\\lVert \\boldsymbol{G}(\\boldsymbol{y}^{(k)}) \\rVert_2$.\n    b. Compute the Jacobian $J(\\boldsymbol{y}^{(k)})$ and its spectral condition number $\\kappa_2(J(\\boldsymbol{y}^{(k)}))$. Store this value.\n    c. If $\\lVert \\boldsymbol{G}(\\boldsymbol{y}^{(k)}) \\rVert_2 \\le \\tau = 10^{-10}$, the iteration has converged. Record the number of iterations $k$ and the maximum condition number seen so far.\n    d. Otherwise, solve the linear system $J(\\boldsymbol{y}^{(k)}) \\Delta\\boldsymbol{y} = -\\boldsymbol{G}(\\boldsymbol{y}^{(k)})$ for the update $\\Delta\\boldsymbol{y}$.\n    e. Update the state: $\\boldsymbol{y}^{(k+1)} = \\boldsymbol{y}^{(k)} + \\Delta\\boldsymbol{y}$.\n$3$. If convergence is not achieved within $M$ iterations, or if a linear solve fails due to singularity, the iteration count is recorded as $-1$. In all cases, the reported condition number is the maximum value observed over all computed iterates.\n\nThis procedure is implemented in the provided Python script, which systematically evaluates each test case and computes the required quantities.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the specified stiff ODE problem using BDF-1 and Newton-Raphson,\n    and reports on convergence and Jacobian conditioning.\n    \"\"\"\n    # Define constants and initial conditions as specified in the problem.\n    K_PARAM = 1.0\n    C_PARAM = 10.0\n    A_PARAM = 1.0\n    B_PARAM = 1.0\n    Y_N = np.array([0.9, 0.1], dtype=np.float64)\n    TOLERANCE = 1e-10\n    MAX_ITERATIONS = 50\n\n    test_cases = [\n        # (epsilon, h)\n        (1.0, 0.05),\n        (1e-3, 0.05),\n        (1e-6, 0.05),\n        (1e-3, 0.5),\n        (1e-6, 0.5)\n    ]\n\n    def ode_rhs(y, eps):\n        \"\"\"Computes the right-hand side f(y) of the ODE.\"\"\"\n        y1, y2 = y\n        dy1_dt = -K_PARAM * y1 + C_PARAM * (y2 - y1) + A_PARAM * y1**2\n        dy2_dt = (1.0 / eps) * (y1 - y2) - B_PARAM * y2**2\n        return np.array([dy1_dt, dy2_dt], dtype=np.float64)\n\n    def ode_jacobian(y, eps):\n        \"\"\"Computes the Jacobian of the ODE's right-hand side, df/dy.\"\"\"\n        y1, y2 = y\n        return np.array([\n            [-K_PARAM - C_PARAM + 2.0 * A_PARAM * y1, C_PARAM],\n            [1.0 / eps, -1.0 / eps - 2.0 * B_PARAM * y2]\n        ], dtype=np.float64)\n\n    def run_single_bdf_step(eps, h):\n        \"\"\"\n        Performs a single BDF-1 step, solving the nonlinear system\n        with Newton-Raphson iteration.\n        \"\"\"\n        y_k = np.copy(Y_N)\n        cond_numbers = []\n\n        for k in range(MAX_ITERATIONS):  # Corresponds to iterations k=0, 1, ..., M-1\n            # At the start of iteration k, we evaluate the state y_k = y^{(k)}\n            f_val = ode_rhs(y_k, eps)\n            g_val = y_k - Y_N - h * f_val\n            \n            df_dy_val = ode_jacobian(y_k, eps)\n            j_val = np.identity(2) - h * df_dy_val\n\n            # Calculate and store condition number for the current iterate\n            try:\n                cond_numbers.append(np.linalg.cond(j_val, 2))\n            except np.linalg.LinAlgError:\n                # Jacobian is singular, Newton iteration cannot proceed.\n                max_c = max(cond_numbers) if cond_numbers else np.inf\n                return -1, np.log10(max_c if max_c  0 else 1.0)\n\n            # Check for convergence\n            if np.linalg.norm(g_val) = TOLERANCE:\n                return k, np.log10(max(cond_numbers))\n\n            # Solve the linear system for the Newton update\n            try:\n                delta_y = np.linalg.solve(j_val, -g_val)\n            except np.linalg.LinAlgError:\n                return -1, np.log10(max(cond_numbers))\n\n            # Update the iterate for the next step - y^{(k+1)}\n            y_k += delta_y\n        \n        # After MAX_ITERATIONS, we have y^{(M)}. Perform a final check.\n        f_val = ode_rhs(y_k, eps)\n        g_val = y_k - Y_N - h * f_val\n        \n        df_dy_val = ode_jacobian(y_k, eps)\n        j_val = np.identity(2) - h * df_dy_val\n        try:\n            cond_numbers.append(np.linalg.cond(j_val, 2))\n        except np.linalg.LinAlgError:\n            pass # The iteration already failed to converge, just record max condition number\n\n        max_c = max(cond_numbers) if cond_numbers else 1.0\n        if np.linalg.norm(g_val) = TOLERANCE:\n            return MAX_ITERATIONS, np.log10(max_c)\n        else:\n            return -1, np.log10(max_c)\n\n    # --- Main execution loop ---\n    results_list = []\n    for epsilon, h_step in test_cases:\n        num_iterations, log_max_cond_num = run_single_bdf_step(epsilon, h_step)\n        results_list.append(num_iterations)\n        results_list.append(log_max_cond_num)\n\n    # Print the final result in the exact specified format.\n    print(f\"[{','.join(map(str, results_list))}]\")\n\nsolve()\n```"
        }
    ]
}