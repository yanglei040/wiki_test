{
    "hands_on_practices": [
        {
            "introduction": "The formal order of accuracy is a cornerstone for classifying numerical methods, defining how quickly the error decreases as the step size $h$ is refined. This practice provides a hands-on implementation of the method of manufactured solutions, a fundamental technique in computational science for verifying that a coded algorithm achieves its theoretical order of convergence. By fitting the observed error to the expected power law $E(h) \\approx C h^p$, you will build a robust tool to validate your own numerical solvers. ",
            "id": "2376768",
            "problem": "Write a complete program that, given several explicit Runge–Kutta (RK) methods in the form of Butcher tableaus, numerically verifies each method’s order of accuracy by testing on a manufactured solution of an initial value problem for an Ordinary Differential Equation (ODE). The manufactured solution is defined as follows: let the exact solution be $y(t)=\\exp(\\sin t)$ for $t \\in [0,1]$, where angles are in radians. The ODE is $y'(t)=f(t,y(t))$ with $f(t,y)=\\cos(t)y$, the initial condition is $y(0)=1$, and the final time is $T=1$. Use the global error at the final time $t=T$ to quantify accuracy.\n\nUse the following explicit Runge–Kutta methods, each specified by its Butcher tableau $(A,b,c)$:\n- Method 1 (Explicit Euler, expected order 1): $s=1$, $A=\\begin{bmatrix}0\\end{bmatrix}$, $b=\\begin{bmatrix}1\\end{bmatrix}$, $c=\\begin{bmatrix}0\\end{bmatrix}$.\n- Method 2 (Explicit Midpoint, expected order 2): $s=2$, $A=\\begin{bmatrix}0  0\\\\ \\tfrac{1}{2}  0\\end{bmatrix}$, $b=\\begin{bmatrix}0  1\\end{bmatrix}$, $c=\\begin{bmatrix}0  \\tfrac{1}{2}\\end{bmatrix}$.\n- Method 3 (Kutta’s third-order method, expected order 3): $s=3$, $A=\\begin{bmatrix}0  0  0\\\\ \\tfrac{1}{2}  0  0\\\\ -1  2  0\\end{bmatrix}$, $b=\\begin{bmatrix}\\tfrac{1}{6}  \\tfrac{2}{3}  \\tfrac{1}{6}\\end{bmatrix}$, $c=\\begin{bmatrix}0  \\tfrac{1}{2}  1\\end{bmatrix}$.\n- Method 4 (Classical RK4, expected order 4): $s=4$, $A=\\begin{bmatrix}0  0  0  0\\\\ \\tfrac{1}{2}  0  0  0\\\\ 0  \\tfrac{1}{2}  0  0\\\\ 0  0  1  0\\end{bmatrix}$, $b=\\begin{bmatrix}\\tfrac{1}{6}  \\tfrac{1}{3}  \\tfrac{1}{3}  \\tfrac{1}{6}\\end{bmatrix}$, $c=\\begin{bmatrix}0  \\tfrac{1}{2}  \\tfrac{1}{2}  1\\end{bmatrix}$.\n\nFor each method, perform time integration on $[0,1]$ using uniform time steps of size $h=1/N$ for $N$ in the test suite $\\{10,20,40,80,160,320\\}$. For each $N$, compute the numerical approximation $y_N$ at $t=1$, compute the global error $E(h)=\\lvert y_N - y(1)\\rvert$, and then estimate the observed order $p$ as the least-squares slope of the line fitting the data $(\\log h,\\log E(h))$ across all values of $N$ in the test suite. Use the natural logarithm for $\\log$.\n\nYour program must output, in a single line, a comma-separated list enclosed in square brackets containing the four estimated orders $(p_1,p_2,p_3,p_4)$ for Methods $1$ through $4$, respectively, each rounded to two decimal places. No other text should be printed.\n\nTest Suite and Answer Specification:\n- The test suite consists of the four methods above, each tested with $N \\in \\{10,20,40,80,160,320\\}$.\n- The final answers are the four floats $p_1$, $p_2$, $p_3$, $p_4$, each being the observed order estimate for the corresponding method.\n- The final output format must be exactly a single line of the form $\\texttt{[p1,p2,p3,p4]}$, where each $p_k$ is rounded to two decimal places and printed as a decimal number.",
            "solution": "The problem statement has been rigorously analyzed. It is scientifically grounded, well-posed, and contains all necessary information for a unique and meaningful solution. The specified ordinary differential equation, its manufactured analytical solution, the definitions of the Runge-Kutta methods via their Butcher tableaus, and the procedure for numerical verification of the order of accuracy are all standard, correct, and self-consistent. The problem is valid. We will now construct the solution.\n\nThe fundamental task is to solve an initial value problem (IVP) of the form:\n$$ y'(t) = f(t, y(t)), \\quad y(t_0) = y_0 $$\nfor $t \\in [t_0, T]$. The problem provides the specific function $f(t, y) = \\cos(t) y$, the initial condition $y(0) = 1$, and the time interval $[0, 1]$. The exact solution is given as $y(t) = \\exp(\\sin t)$, which is readily verified by differentiation: $y'(t) = \\exp(\\sin t) \\cdot \\cos(t) = y(t)\\cos(t)$, and checking the initial condition: $y(0) = \\exp(\\sin 0) = \\exp(0) = 1$.\n\nAn $s$-stage explicit Runge-Kutta (RK) method approximates the solution by stepping forward in time with a step size $h$. From the solution $y_n$ at time $t_n$, the solution $y_{n+1}$ at time $t_{n+1} = t_n + h$ is calculated. The method is defined by a set of coefficients arranged in a Butcher tableau:\n$$\n\\begin{array}{c|c}\nc  A \\\\\n\\hline\n   b^T\n\\end{array} \\quad \\text{where } c \\in \\mathbb{R}^s, b \\in \\mathbb{R}^s, A \\in \\mathbb{R}^{s \\times s}\n$$\nFor an explicit method, the matrix $A$ is strictly lower triangular, meaning $a_{ij} = 0$ for $j \\ge i$. The computation proceeds in stages. First, the $s$ stage derivatives, $k_i$, are computed for $i=1, 2, \\dots, s$:\n$$ k_i = f\\left(t_n + c_i h, y_n + h \\sum_{j=1}^{i-1} a_{ij} k_j\\right) $$\nThe solution is then advanced using a weighted average of these stage derivatives:\n$$ y_{n+1} = y_n + h \\sum_{i=1}^{s} b_i k_i $$\n\nThe accuracy of a numerical method is characterized by its order of convergence, $p$. For a method of order $p$, the global error at a fixed final time $T$, denoted $E(h)$, is expected to decrease with the step size $h$ according to the relationship:\n$$ E(h) = |y_N - y(T)| \\approx C h^p $$\nwhere $y_N$ is the numerical solution at $T=Nh$ and $C$ is a constant that depends on the method and the problem, but not on $h$.\n\nTo numerically verify the order $p$, we can transform this relationship by taking the natural logarithm of both sides:\n$$ \\ln(E(h)) \\approx \\ln(C) + p \\ln(h) $$\nThis equation is of the form $Y = mX + B$, where $Y = \\ln(E(h))$, $X = \\ln(h)$, the slope is $m = p$, and the intercept is $B = \\ln(C)$. This linear relationship implies that a plot of $\\ln(E(h))$ versus $\\ln(h)$ will approximate a straight line whose slope is the order of the method, $p$.\n\nThe specified procedure is as follows:\n$1$. For each of the four given RK methods, a series of numerical integrations must be performed over the interval $[0, 1]$.\n$2$. The integrations will use a sequence of decreasing step sizes $h = 1/N$, for $N \\in \\{10, 20, 40, 80, 160, 320\\}$.\n$3$. For each integration with a specific step size $h$, the numerical approximation at the final time, $y_N$, is computed.\n$4$. The global error is calculated as $E(h) = |y_N - y(1)|$, where the exact value is $y(1) = \\exp(\\sin 1)$.\n$5$. After computing the errors for all step sizes, the data pairs $(\\ln(h), \\ln(E(h)))$ are collected.\n$6$. A linear least-squares regression is performed on these data points. The slope of the resulting best-fit line provides the experimental estimate of the order of accuracy, $p$. The slope $p$ for a set of data points $(x_i, y_i)$ is given by:\n$$ p = \\frac{\\sum_{i=1}^{M} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{M} (x_i - \\bar{x})^2} $$\nwhere $x_i = \\ln(h_i)$, $y_i = \\ln(E(h_i))$, $\\bar{x}$ and $\\bar{y}$ are the mean values, and $M=6$ is the number of step sizes in the test suite.\n\nThis procedure will be implemented for each of the four Butcher tableaus provided, yielding four estimated orders of accuracy, $(p_1, p_2, p_3, p_4)$, which are expected to be close to their theoretical values of $1, 2, 3,$ and $4$, respectively. The final result will be these four values, rounded to two decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Numerically verifies the order of accuracy of several explicit Runge-Kutta methods.\n    \"\"\"\n    # Define the Ordinary Differential Equation and its analytical solution\n    f = lambda t, y: np.cos(t) * y\n    t_start = 0.0\n    y_start = 1.0\n    t_end = 1.0\n    \n    # Pre-calculate the exact solution at the final time for error computation\n    y_exact_final = np.exp(np.sin(t_end))\n\n    # Define the Butcher tableaus for the four RK methods\n    methods = [\n        {\n            # Method 1: Explicit Euler (Order 1)\n            'A': np.array([[0.0]]),\n            'b': np.array([1.0]),\n            'c': np.array([0.0])\n        },\n        {\n            # Method 2: Explicit Midpoint (Order 2)\n            'A': np.array([[0.0, 0.0], [0.5, 0.0]]),\n            'b': np.array([0.0, 1.0]),\n            'c': np.array([0.0, 0.5])\n        },\n        {\n            # Method 3: Kutta's third-order method (Order 3)\n            'A': np.array([[0.0, 0.0, 0.0], [0.5, 0.0, 0.0], [-1.0, 2.0, 0.0]]),\n            'b': np.array([1.0/6.0, 2.0/3.0, 1.0/6.0]),\n            'c': np.array([0.0, 0.5, 1.0])\n        },\n        {\n            # Method 4: Classical RK4 (Order 4)\n            'A': np.array([\n                [0.0, 0.0, 0.0, 0.0],\n                [0.5, 0.0, 0.0, 0.0],\n                [0.0, 0.5, 0.0, 0.0],\n                [0.0, 0.0, 1.0, 0.0]\n            ]),\n            'b': np.array([1.0/6.0, 1.0/3.0, 1.0/3.0, 1.0/6.0]),\n            'c': np.array([0.0, 0.5, 0.5, 1.0])\n        }\n    ]\n\n    # Test suite of step counts\n    N_values = [10, 20, 40, 80, 160, 320]\n    h_values = np.array([1.0 / N for N in N_values])\n\n    estimated_orders = []\n\n    for method in methods:\n        A, b, c = method['A'], method['b'], method['c']\n        s = len(b)  # Number of stages\n        errors = []\n\n        for N in N_values:\n            h = (t_end - t_start) / N\n            y_current = y_start\n            \n            # Time integration loop\n            for n in range(N):\n                t_n = t_start + n * h\n                k_stages = np.zeros(s)\n                \n                # Calculate stage derivatives k_i\n                for i in range(s):\n                    stage_sum = 0.0\n                    for j in range(i):\n                        stage_sum += A[i, j] * k_stages[j]\n                    \n                    y_stage_input = y_current + h * stage_sum\n                    t_stage_input = t_n + c[i] * h\n                    k_stages[i] = f(t_stage_input, y_stage_input)\n                \n                # Update solution\n                y_current += h * np.dot(b, k_stages)\n            \n            # Store the global error at t=T\n            errors.append(np.abs(y_current - y_exact_final))\n\n        # Use natural logarithm for the log-log plot\n        log_h = np.log(h_values)\n        log_E = np.log(np.array(errors))\n        \n        # Perform linear regression (polynomial fit of degree 1)\n        # The slope of the line is the estimated order of accuracy\n        # np.polyfit returns [slope, intercept]\n        slope = np.polyfit(log_h, log_E, 1)[0]\n        estimated_orders.append(slope)\n        \n    # Format the output as specified: [p1,p2,p3,p4]\n    formatted_results = [f'{p:.2f}' for p in estimated_orders]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond theoretical order, the practical performance of a Runge-Kutta method depends on its behavior when applied to a specific problem. This exercise challenges you to implement and compare four classical RK methods, from first to fourth order, on a nonlinear differential equation. You will analyze not just the final error but also qualitative properties of the solution, such as monotonicity and positivity, providing crucial insights into the trade-offs between accuracy, computational cost, and numerical stability. ",
            "id": "2376808",
            "problem": "Consider the initial value problem defined by the ordinary differential equation $y'(t)=-y(t)^3$ with initial condition $y(0)=1$. The unique exact solution is $y(t)=\\dfrac{1}{\\sqrt{1+2t}}$. Let $f(t,y)=-y^3$.\n\nDefine four classical explicit Runge–Kutta methods to advance an approximation $y_n \\approx y(t_n)$ to $y_{n+1} \\approx y(t_{n+1})$ using a uniform time step $h>0$:\n- $RK1$: the forward Euler method (first order).\n- $RK2$: the explicit midpoint method (second order).\n- $RK3$: Kutta’s third-order method (third order).\n- $RK4$: the classical fourth-order method (fourth order).\n\nFor each test case specified below, start from $t_0=0$, $y_0=1$, and use a uniform step size $h$ to reach the final time $T$ in $N=T/h$ steps (assume $T/h$ is an integer). For each test case, compute the following four quantities:\n1. The absolute error at the final time, $E=\\lvert y_N - y(T)\\rvert$, where $y(T)=1/\\sqrt{1+2T}$. Report $E$ rounded to $10$ decimal places.\n2. A boolean indicating whether the sequence $\\{y_n\\}_{n=0}^N$ is monotonically nonincreasing, meaning $y_{n+1} \\le y_n+\\varepsilon$ for all $n$, where $\\varepsilon=10^{-12}$.\n3. A boolean indicating whether all iterates are nonnegative within tolerance, meaning $y_n \\ge -\\varepsilon$ for all $n$, where $\\varepsilon=10^{-12}$.\n4. The integer number of time steps $N$ taken to reach $T$.\n\nUse the following test suite, where each case is given as a triple $(\\text{method},T,h)$ with $T$ in the unit of time and $h$ in the unit of time:\n- Case $1$: $(RK1,\\,T=10,\\,h=1)$.\n- Case $2$: $(RK2,\\,T=10,\\,h=1)$.\n- Case $3$: $(RK3,\\,T=10,\\,h=1)$.\n- Case $4$: $(RK4,\\,T=10,\\,h=1)$.\n- Case $5$: $(RK1,\\,T=10,\\,h=2)$.\n- Case $6$: $(RK4,\\,T=10,\\,h=0.5)$.\n- Case $7$: $(RK1,\\,T=10,\\,h=0.5)$.\n- Case $8$: $(RK2,\\,T=100,\\,h=1)$.\n- Case $9$: $(RK3,\\,T=100,\\,h=1)$.\n- Case $10$: $(RK4,\\,T=100,\\,h=1)$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list should contain one entry per test case in the same order as above. Each entry must itself be a list of the four values $[E,\\,\\text{monotone},\\,\\text{nonnegative},\\,N]$ as defined above, where $E$ is a floating-point number rounded to $10$ decimal places, $\\text{monotone}$ and $\\text{nonnegative}$ are booleans, and $N$ is an integer. For example, the overall output format must be\n$$\n\\big[\\,[E_1,\\text{mon}_1,\\text{nonneg}_1,N_1],\\,[E_2,\\text{mon}_2,\\text{nonneg}_2,N_2],\\,\\dots,\\,[E_{10},\\text{mon}_{10},\\text{nonneg}_{10},N_{10}]\\,\\big].\n$$\nNo angles or physical units are involved; all quantities are dimensionless. All floating-point outputs must adhere to the rounding specification stated above.",
            "solution": "The problem statement has been critically examined and is determined to be valid. It is a well-posed, scientifically grounded problem in the field of computational engineering, specifically concerning the numerical solution of ordinary differential equations. All necessary data, definitions, and boundary conditions are provided, and no contradictions or ambiguities are present.\n\nThe task is to solve the initial value problem (IVP) defined by the ordinary differential equation (ODE):\n$$\ny'(t) = -y(t)^3\n$$\nwith the initial condition $y(0)=1$. The function on the right-hand side is $f(t,y) = -y^3$. This equation is a separable ODE, and its unique exact solution for the given initial condition is:\n$$\ny(t) = \\frac{1}{\\sqrt{1+2t}}\n$$\nWe are required to approximate this solution using four classical explicit Runge–Kutta (RK) methods. An approximation $y_n$ at time $t_n$ is advanced to $y_{n+1}$ at time $t_{n+1} = t_n + h$ using a uniform time step $h > 0$. The simulation for each test case starts at $t_0=0$ with $y_0=1$ and proceeds for $N=T/h$ steps to reach a final time $T$.\n\nThe four specified RK methods are:\n$1$. **RK1 (Forward Euler method, order 1):**\n$$\ny_{n+1} = y_n + h f(t_n, y_n)\n$$\n\n$2$. **RK2 (Explicit Midpoint method, order 2):**\n$$\n\\begin{aligned}\nk_1 = f(t_n, y_n) \\\\\nk_2 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2} k_1\\right) \\\\\ny_{n+1} = y_n + h k_2\n\\end{aligned}\n$$\n\n$3$. **RK3 (Kutta’s third-order method, order 3):**\n$$\n\\begin{aligned}\nk_1 = f(t_n, y_n) \\\\\nk_2 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2} k_1\\right) \\\\\nk_3 = f\\left(t_n + h, y_n - h k_1 + 2h k_2\\right) \\\\\ny_{n+1} = y_n + \\frac{h}{6}(k_1 + 4k_2 + k_3)\n\\end{aligned}\n$$\n\n$4$. **RK4 (Classical fourth-order method, order 4):**\n$$\n\\begin{aligned}\nk_1 = f(t_n, y_n) \\\\\nk_2 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2} k_1\\right) \\\\\nk_3 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2} k_2\\right) \\\\\nk_4 = f\\left(t_n + h, y_n + h k_3\\right) \\\\\ny_{n+1} = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n\\end{aligned}\n$$\n\nFor each test case, defined by a tuple $(\\text{method}, T, h)$, the following algorithm is executed:\nFirst, initialize the simulation with $t_0=0$, $y_0=1$. The total number of steps is $N = T/h$. A list is initialized to store the sequence of approximations $\\{y_n\\}_{n=0}^N$.\nThen, a loop runs for $n$ from $0$ to $N-1$. In each iteration, the value $y_{n+1}$ is computed from $y_n$ using the formulae of the specified RK method. The new value $y_{n+1}$ is appended to the history list.\n\nAfter completing all $N$ steps, four quantities are computed:\n$1$. The absolute error $E$ at the final time $T$, calculated as $E = \\lvert y_N - y(T) \\rvert$, where $y_N$ is the final numerical approximation and $y(T)$ is the exact solution evaluated at $T$. This value is reported rounded to $10$ decimal places.\n$2$. A boolean value indicating whether the computed sequence $\\{y_n\\}$ is monotonically nonincreasing. This is verified by checking if $y_{n+1} \\le y_n + \\varepsilon$ for all $n \\in \\{0, 1, \\dots, N-1\\}$, where $\\varepsilon = 10^{-12}$ is a small tolerance to account for floating-point inaccuracies. The exact solution is strictly decreasing, so this property is expected of a stable numerical solution.\n$3$. A boolean value indicating whether all iterates are non-negative. This is checked by verifying if $y_n \\ge -\\varepsilon$ for all $n \\in \\{0, 1, \\dots, N\\}$. The exact solution is strictly positive, so any negative values are numerical artifacts, typically arising from instability.\n$4$. The integer number of time steps $N$ taken to complete the simulation.\n\nThis procedure is applied to each of the $10$ test cases provided. The results are then aggregated into a single list of lists for final output. The implementation will use the specified Python environment and libraries.",
            "answer": "```python\nimport numpy as np\n\ndef f(t, y):\n    \"\"\"\n    Implements the right-hand side of the ODE y' = -y^3.\n    The parameter t is unused as the ODE is autonomous.\n    \"\"\"\n    return -y**3\n\ndef exact_solution(t):\n    \"\"\"\n    Calculates the exact solution y(t) = 1/sqrt(1 + 2t).\n    \"\"\"\n    return 1.0 / np.sqrt(1.0 + 2.0 * t)\n\ndef step_rk1(y_n, h):\n    \"\"\"Performs one step of the Forward Euler (RK1) method.\"\"\"\n    return y_n + h * f(0, y_n)\n\ndef step_rk2(y_n, h):\n    \"\"\"Performs one step of the explicit midpoint (RK2) method.\"\"\"\n    k1 = f(0, y_n)\n    k2 = f(0, y_n + h / 2.0 * k1)\n    return y_n + h * k2\n\ndef step_rk3(y_n, h):\n    \"\"\"Performs one step of Kutta's third-order (RK3) method.\"\"\"\n    k1 = f(0, y_n)\n    k2 = f(0, y_n + h / 2.0 * k1)\n    k3 = f(0, y_n - h * k1 + 2.0 * h * k2)\n    return y_n + (h / 6.0) * (k1 + 4.0 * k2 + k3)\n\ndef step_rk4(y_n, h):\n    \"\"\"Performs one step of the classical fourth-order (RK4) method.\"\"\"\n    k1 = f(0, y_n)\n    k2 = f(0, y_n + h / 2.0 * k1)\n    k3 = f(0, y_n + h / 2.0 * k2)\n    k4 = f(0, y_n + h * k3)\n    return y_n + (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n\ndef run_simulation(method, T, h):\n    \"\"\"\n    Runs the simulation for a given method, final time T, and step size h.\n    \"\"\"\n    steppers = {\n        'RK1': step_rk1,\n        'RK2': step_rk2,\n        'RK3': step_rk3,\n        'RK4': step_rk4,\n    }\n    stepper = steppers[method]\n    \n    y0 = 1.0\n    epsilon = 1e-12\n    # The problem statement guarantees T/h is an integer.\n    N = int(T / h)\n\n    y_history = [y0]\n    y_current = y0\n    for _ in range(N):\n        y_current = stepper(y_current, h)\n        y_history.append(y_current)\n    \n    y_N = y_history[-1]\n    y_exact_T = exact_solution(T)\n    \n    # 1. Absolute error at the final time, rounded to 10 decimal places.\n    E = abs(y_N - y_exact_T)\n    E_rounded = round(E, 10)\n    \n    # 2. Monotonically nonincreasing check.\n    is_monotone = all(y_history[i+1] = y_history[i] + epsilon for i in range(len(y_history) - 1))\n            \n    # 3. Nonnegative check.\n    is_nonnegative = all(y_val = -epsilon for y_val in y_history)\n            \n    return [E_rounded, is_monotone, is_nonnegative, N]\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite, then prints the formatted results.\n    \"\"\"\n    test_cases = [\n        ('RK1', 10.0, 1.0),\n        ('RK2', 10.0, 1.0),\n        ('RK3', 10.0, 1.0),\n        ('RK4', 10.0, 1.0),\n        ('RK1', 10.0, 2.0),\n        ('RK4', 10.0, 0.5),\n        ('RK1', 10.0, 0.5),\n        ('RK2', 100.0, 1.0),\n        ('RK3', 100.0, 1.0),\n        ('RK4', 100.0, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        method, T, h = case\n        result = run_simulation(method, T, h)\n        results.append(result)\n        \n    # Format the final output string as specified.\n    # The str() representation of a list is \"[item1, item2, ...]\"\n    # which matches the required sub-format for each test case result.\n    result_strings = [str(r) for r in results]\n    output_string = f\"[{','.join(result_strings)}]\"\n    \n    print(output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "A common misconception is that higher-order methods are universally superior. This problem presents a carefully constructed \"pathological\" case to demonstrate a critical concept: the region of absolute stability. You will analyze a scenario where the step size is large enough to fall outside the stability region of both a second-order and a fourth-order method, leading to the counter-intuitive result that the higher-order method produces a larger error. This exercise is invaluable for developing a nuanced understanding of numerical stability and why it can sometimes be more critical than the formal order of accuracy. ",
            "id": "2376761",
            "problem": "In computational engineering practice, a numerical method with higher formal order of accuracy can perform worse at a prescribed step size due to stability-function behavior. Consider the initial value problem\n$$\ny'(t)=\\lambda\\,y(t),\\qquad y(0)=1,\n$$\nwith constant coefficient $\\lambda=-5$. Using a single time step of size $h=1$ from $t=0$ to $t=h$, approximate $y(h)$ by both the classical fourth-order Runge–Kutta method and the explicit midpoint two-stage second-order Runge–Kutta method. Let\n$$\n\\rho=\\frac{\\left|y_{\\mathrm{RK4}}(h)-y(h)\\right|}{\\left|y_{\\mathrm{mid}}(h)-y(h)\\right|}.\n$$\nCompute $\\rho$ and provide its value as a pure number. Round your answer to $4$ significant figures.",
            "solution": "The problem statement has been validated and is deemed a valid, well-posed problem in the field of computational engineering and numerical analysis. We shall proceed with the solution.\n\nThe initial value problem (IVP) is given by the linear ordinary differential equation:\n$$\ny'(t) = \\lambda y(t), \\quad y(0) = 1\n$$\nwith the constant coefficient $\\lambda = -5$. We are asked to compute approximations at $t=h=1$ using two different explicit Runge-Kutta methods and compare their errors.\n\nFirst, we determine the exact solution of the IVP. This is a standard first-order linear homogeneous differential equation whose solution is of the form $y(t) = C \\exp(\\lambda t)$. Using the initial condition $y(0)=1$, we find $1 = C \\exp(\\lambda \\cdot 0)$, which implies $C=1$. Thus, the exact solution is:\n$$\ny(t) = \\exp(\\lambda t)\n$$\nAt the time $t=h=1$, with $\\lambda = -5$, the exact value is:\n$$\ny(h) = \\exp(-5 \\cdot 1) = \\exp(-5)\n$$\n\nNext, we analyze the application of an explicit Runge-Kutta (RK) method to this test problem. For an ODE of the form $y' = f(t,y)$, a single step of an explicit RK method takes the form $y_{n+1} = y_n + h \\Phi(t_n, y_n, h)$. When applied to $y' = \\lambda y$, where $f(t,y) = \\lambda y$ is independent of $t$, the formula simplifies. The next step $y_1$ is related to the previous step $y_0$ by a stability function, $R(z)$, which is a polynomial in $z=h\\lambda$:\n$$\ny_1 = R(h\\lambda) y_0\n$$\nIn our case, $y_0 = y(0) = 1$ and $z = h\\lambda = 1 \\cdot (-5) = -5$. Thus, the approximation at $t=h$ is simply the value of the method's stability function evaluated at $z=-5$.\n\nLet us compute the approximation using the classical fourth-order Runge-Kutta (RK4) method. The stability function for RK4 is the Taylor polynomial of the exponential function of degree $4$:\n$$\nR_{\\mathrm{RK4}}(z) = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\frac{z^4}{4!}\n$$\nWe evaluate this for $z=-5$:\n$$\ny_{\\mathrm{RK4}}(h) = R_{\\mathrm{RK4}}(-5) = 1 + (-5) + \\frac{(-5)^2}{2} + \\frac{(-5)^3}{6} + \\frac{(-5)^4}{24}\n$$\n$$\ny_{\\mathrm{RK4}}(h) = 1 - 5 + \\frac{25}{2} - \\frac{125}{6} + \\frac{625}{24}\n$$\nTo sum these fractions, we use a common denominator of $24$:\n$$\ny_{\\mathrm{RK4}}(h) = \\frac{24}{24} - \\frac{120}{24} + \\frac{300}{24} - \\frac{500}{24} + \\frac{625}{24} = \\frac{24 - 120 + 300 - 500 + 625}{24} = \\frac{329}{24}\n$$\n\nNow, we compute the approximation using the explicit two-stage second-order midpoint method. The stability function for this method is the Taylor polynomial of the exponential function of degree $2$:\n$$\nR_{\\mathrm{mid}}(z) = 1 + z + \\frac{z^2}{2!}\n$$\nWe evaluate this for $z=-5$:\n$$\ny_{\\mathrm{mid}}(h) = R_{\\mathrm{mid}}(-5) = 1 + (-5) + \\frac{(-5)^2}{2} = 1 - 5 + \\frac{25}{2} = -4 + 12.5 = 8.5 = \\frac{17}{2}\n$$\n\nThe problem asks for the ratio $\\rho$ of the absolute errors:\n$$\n\\rho = \\frac{\\left|y_{\\mathrm{RK4}}(h)-y(h)\\right|}{\\left|y_{\\mathrm{mid}}(h)-y(h)\\right|}\n$$\nSubstituting the computed values:\n$$\n\\rho = \\frac{\\left|\\frac{329}{24} - \\exp(-5)\\right|}{\\left|\\frac{17}{2} - \\exp(-5)\\right|}\n$$\nTo compute the final numerical value, we use the approximations:\n$\\exp(-5) \\approx 0.006737947$\n$\\frac{329}{24} \\approx 13.7083333$\n$\\frac{17}{2} = 8.5$\n\nThe numerator is the absolute error of the RK4 method:\n$$\n\\left|y_{\\mathrm{RK4}}(h)-y(h)\\right| = |13.7083333... - 0.0067379...| \\approx 13.7015954\n$$\nThe denominator is the absolute error of the midpoint method:\n$$\n\\left|y_{\\mathrm{mid}}(h)-y(h)\\right| = |8.5 - 0.0067379...| \\approx 8.4932621\n$$\nThe ratio $\\rho$ is therefore:\n$$\n\\rho \\approx \\frac{13.7015954}{8.4932621} \\approx 1.613245\n$$\nThis result demonstrates a crucial concept in numerical analysis. The step size $h=1$ results in $z = h\\lambda = -5$, which lies outside the region of absolute stability for both methods (which are approximately $[-2.78, 0]$ for RK4 and $[-2, 0]$ for the midpoint method). When $|R(z)|  1$, the numerical solution is unstable and grows, while the true solution $y(t) = \\exp(-5t)$ decays. In this unstable regime, the method with the larger stability function magnitude, $|R(z)|$, produces a larger error. Here, $|R_{\\mathrm{RK4}}(-5)| = \\frac{329}{24} \\approx 13.71$ is significantly larger than $|R_{\\mathrm{mid}}(-5)| = 8.5$. Consequently, the fourth-order method yields a less accurate result than the second-order method for this large step size, as confirmed by $\\rho  1$.\n\nRounding the value of $\\rho$ to $4$ significant figures, we get $1.613$.",
            "answer": "$$\n\\boxed{1.613}\n$$"
        }
    ]
}