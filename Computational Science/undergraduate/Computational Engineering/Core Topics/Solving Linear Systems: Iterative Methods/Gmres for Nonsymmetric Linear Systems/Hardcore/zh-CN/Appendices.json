{
    "hands_on_practices": [
        {
            "introduction": "我们的第一个实践练习是要求您亲自动手，从头开始构建GMRES方法。通过将阿诺尔迪过程 (Arnoldi process) 和最小二乘最小化问题转化为代码，您将对理论背后的具体机制有更深入的理解。这个练习  为理解GMRES如何在克雷洛夫子空间 (Krylov subspace) 中构造其解奠定了坚实的基础。",
            "id": "2397285",
            "problem": "给定一个实方阵 $A \\in \\mathbb{R}^{n \\times n}$、一个右端向量 $b \\in \\mathbb{R}^{n}$、一个初始猜测值 $x_0 \\in \\mathbb{R}^{n}$ 以及一个满足 $1 \\le m \\le n$ 的正整数 $m$，考虑广义最小残差 (GMRES) 近似解 $x_m \\in x_0 + \\mathcal{K}_m(A,r_0)$，它最小化残差的 $2$-范数，其中 $r_0 = b - A x_0$ 且 $\\mathcal{K}_m(A,r_0) = \\text{span}\\{r_0, A r_0, \\dots, A^{m-1} r_0\\}$。对于下方的每个测试用例，计算求解\n$$\n\\min_{x \\in x_0 + \\mathcal{K}_m(A,r_0)} \\| b - A x \\|_2\n$$\n的向量 $x_m$，并报告以下两个量：(i) 达到的残差范数 $\\| b - A x_m \\|_2$，四舍五入到八位小数；(ii) 实际执行的 Krylov 步数 $k$，定义为近似解所用的构造子空间的维度，其中 $k \\in \\{0,1,\\dots,m\\}$。$k=0$ 的情况对应于 $\\|r_0\\|_2 = 0$，此时 $x_m = x_0$ 且残差范数为 $0$。如果在 $m$ 步之前遇到子空间的不变性，则使用已达到的 $k  m$。\n\n测试套件包含五个独立的用例。每个用例指定了 $A$、$b$、$x_0$ 和 $m$：\n\n1) 用例 1 (方阵、非对称、全子空间)：\n- $A = \\begin{bmatrix}\n4  1  0  0 \\\\\n2  3  1  0 \\\\\n0  1  3  1 \\\\\n0  0  1  2\n\\end{bmatrix}$,\n$b = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$,\n$x_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n$m = 4$。\n\n2) 用例 2 (方阵、非对称、部分子空间)：\n- $A = \\begin{bmatrix}\n3  -1  0  0  0 \\\\\n2  4  1  0  0 \\\\\n0  -2  3  1  0 \\\\\n0  0  -1  2  1 \\\\\n0  0  0  -3  1\n\\end{bmatrix}$,\n$b = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$,\n$x_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n$m = 3$。\n\n3) 用例 3 (上三角矩阵，具有大的超对角线元素，全子空间)：\n- $A = \\begin{bmatrix}\n1  10  0  0  0 \\\\\n0  1  10  0  0 \\\\\n0  0  1  10  0 \\\\\n0  0  0  1  10 \\\\\n0  0  0  0  1\n\\end{bmatrix}$,\n$b = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$,\n$x_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n$m = 5$。\n\n4) 用例 4 (非零初始猜测，单次 Krylov 步)：\n- $A = \\begin{bmatrix}\n2  -1  0  0 \\\\\n1  2  -1  0 \\\\\n0  1  2  -1 \\\\\n0  0  1  2\n\\end{bmatrix}$,\n$b = \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\\\ 1 \\end{bmatrix}$,\n$x_0 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}$,\n$m = 1$。\n\n5) 用例 5 (在初始猜测值处提前精确收敛)：\n- $A = \\begin{bmatrix}\n2  1  0 \\\\\n0  3  1 \\\\\n1  0  2\n\\end{bmatrix}$,\n$x_{\\text{true}} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}$,\n$b = A x_{\\text{true}} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 5 \\end{bmatrix}$,\n$x_0 = x_{\\text{true}} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}$,\n$m = 3$。\n\n您的程序必须按规定顺序处理所有五个用例。对于每个用例，计算 $x_m$，然后计算残差范数 $\\|b - A x_m\\|_2$，将其四舍五入到八位小数，确定 $k$，并将结果汇总到单行输出中，格式如下：\n- 单行输出，包含一个由方括号括起来的逗号分隔列表，其中每个元素是一个二元列表 $[\\text{residual\\_norm}, \\text{krylov\\_steps}]$。\n- 例如，整体输出应类似于 $[[r_1,k_1],[r_2,k_2],[r_3,k_3],[r_4,k_4],[r_5,k_5]]$，其中每个 $r_i$ 是一个四舍五入到八位小数的浮点数，每个 $k_i$ 是一个整数。",
            "solution": "目标是广义最小残差 (GMRES) 近似解，其定义如下。给定 $A \\in \\mathbb{R}^{n \\times n}$、$b \\in \\mathbb{R}^{n}$、一个初始猜测值 $x_0 \\in \\mathbb{R}^{n}$ 以及一个满足 $1 \\le m \\le n$ 的整数 $m$，定义初始残差 $r_0 = b - A x_0$ 和 $m$ 维 Krylov 子空间 $\\mathcal{K}_m(A,r_0) = \\text{span}\\{ r_0, A r_0, \\dots, A^{m-1} r_0 \\}$。GMRES 近似解 $x_m$ 是仿射空间 $x_0 + \\mathcal{K}_m(A,r_0)$ 中最小化残差 $2$-范数的唯一向量，即，\n$$\nx_m = \\arg\\min_{x \\in x_0 + \\mathcal{K}_m(A,r_0)} \\| b - A x \\|_2.\n$$\n\n一种计算 $x_m$ 的原则性方法依赖于两个基本组成部分：Krylov 子空间的一组标准正交基和一个降维的最小二乘问题。使用 Arnoldi 过程构造 $\\mathcal{K}_m(A,r_0)$ 的一组标准正交基。从 $r_0$ 及其范数 $\\beta = \\|r_0\\|_2$ 开始。如果 $\\beta = 0$，则 $x_0$ 已是解，最小化残差范数为 $0$，且不需要 Krylov 步，因此我们设置 $k = 0$ 且 $x_m = x_0$。\n\n如果 $\\beta > 0$，定义 $v_1 = r_0 / \\beta$。Arnoldi 过程生成标准正交向量 $v_1, v_2, \\dots, v_{k+1}$（其中 $k \\le m$）和一个上 Hessenberg 矩阵 $\\bar{H}_k \\in \\mathbb{R}^{(k+1) \\times k}$，使得\n$$\nA V_k = V_{k+1} \\bar{H}_k,\n$$\n其中 $V_k = [v_1,\\dots,v_k] \\in \\mathbb{R}^{n \\times k}$ 且 $V_{k+1} = [v_1,\\dots,v_{k+1}] \\in \\mathbb{R}^{n \\times (k+1)}$。标准的修正 Gram–Schmidt 正交化过程产生 $\\bar{H}_k$ 的系数和标准正交基。如果某个次对角线元素 $h_{j+1,j}$ 在 $j  m$ 时在数值上变为零，则子空间变为 $A$-不变子空间，过程提前终止，此时 $k = j+1$。\n\n近似解 $x_m$ 可以参数化为 $x = x_0 + V_k y$，其中某个 $y \\in \\mathbb{R}^k$。将其代入残差并使用 Arnoldi 关系式可得\n$$\n\\| b - A (x_0 + V_k y) \\|_2 = \\| r_0 - A V_k y \\|_2 = \\| \\beta v_1 - V_{k+1} \\bar{H}_k y \\|_2 = \\| \\beta e_1 - \\bar{H}_k y \\|_2,\n$$\n其中 $e_1 \\in \\mathbb{R}^{k+1}$ 是第一个标准基向量。因此，系数向量 $y_m$ 求解降维后的最小二乘问题\n$$\ny_m \\in \\arg\\min_{y \\in \\mathbb{R}^k} \\| \\bar{H}_k y - \\beta e_1 \\|_2.\n$$\n任何数值稳定的最小二乘求解器（例如，基于奇异值分解的求解器）都能得出 $y_m$。最小化近似解则通过以下方式获得\n$$\nx_m = x_0 + V_k y_m,\n$$\n而最小残差范数为\n$$\n\\| b - A x_m \\|_2 = \\| \\beta e_1 - \\bar{H}_k y_m \\|_2.\n$$\n\n因此，该算法遵循第一性原理：\n1) 计算 $r_0 = b - A x_0$ 和 $\\beta = \\|r_0\\|_2$。如果 $\\beta = 0$，设置 $k = 0$，$x_m = x_0$，残差范数 $= 0$。\n2) 否则，通过使用修正 Gram–Schmidt 的 Arnoldi 关系式建立 $\\mathcal{K}_m(A,r_0)$ 的标准正交基，累加 $\\bar{H}_k$ 的元素，直到完成 $m$ 步或某个次对角线元素在数值上变为零（这将设定 $k$）。\n3) 求解降维后的最小二乘问题以得到 $y_m$，并构造 $x_m = x_0 + V_k y_m$。\n4) 计算残差范数 $\\|b - A x_m\\|_2$ 并将其四舍五入到八位小数。报告实际执行的 Krylov 步数 $k$。\n\n对于提供的测试用例，所有矩阵都是方的且非对称的，向量是明确给出的，整数 $m$ 满足 $1 \\le m \\le n$。用例 5 展示了在初始猜测值处提前精确收敛的情况，此时 $k=0$。最终输出必须是包含汇总列表 $[[r_1,k_1],[r_2,k_2],[r_3,k_3],[r_4,k_4],[r_5,k_5]]$ 的单行文本，其中每个 $r_i$ 是对应情况下四舍五入到八位小数的残差范数，每个 $k_i$ 是一个在 $\\{0,1,\\dots,m\\}$ 中的整数。",
            "answer": "```python\nimport numpy as np\n\ndef arnoldi_basis(A, r0, m, tol=1e-14):\n    \"\"\"\n    Perform the Arnoldi process with modified Gram-Schmidt to generate\n    an orthonormal basis V and upper Hessenberg matrix H_bar.\n    Returns:\n        V (n x (k+1)) with columns v1..v_{k+1} if k>=1; if k==0, V is empty\n        H ( (k+1) x k ) upper Hessenberg\n        beta (norm of r0)\n        k (number of Krylov steps actually performed, 0=k=m)\n    \"\"\"\n    n = A.shape[0]\n    beta = np.linalg.norm(r0)\n    if beta == 0.0:\n        # No steps needed; exact at initial guess\n        return np.zeros((n, 0)), np.zeros((1, 0)), 0.0, 0\n\n    # Preallocate maximum sizes; we'll slice by the actual k\n    V = np.zeros((n, m + 1), dtype=float)\n    H = np.zeros((m + 1, m), dtype=float)\n\n    V[:, 0] = r0 / beta\n    k = 0\n    for j in range(m):\n        w = A @ V[:, j]\n        # Modified Gram-Schmidt\n        for i in range(j + 1):\n            H[i, j] = np.dot(V[:, i], w)\n            w = w - H[i, j] * V[:, i]\n        H[j + 1, j] = np.linalg.norm(w)\n        if H[j + 1, j]  tol:\n            # Invariant subspace reached; breakdown\n            k = j + 1  # Number of columns in V_k\n            # We cannot form V[:, j+1]; stop here\n            break\n        V[:, j + 1] = w / H[j + 1, j]\n        k = j + 1  # Update number of Krylov steps performed\n    # Slice to actual sizes: V has (k+1) columns if k>=1; H is (k+1) x k\n    V_used = V[:, : (k + 1) ] if k >= 1 else np.zeros((n, 0))\n    H_used = H[: (k + 1), : k] if k >= 1 else np.zeros((1, 0))\n    return V_used, H_used, beta, k\n\ndef gmres_minres(A, b, x0, m, tol=1e-14):\n    \"\"\"\n    Compute x_m in x0 + K_m(A, r0) minimizing ||b - A x||_2 via Arnoldi and least squares.\n    Returns:\n        x_m (approximate solution),\n        res_norm (float residual norm),\n        k (int number of Krylov steps actually performed, 0=k=m)\n    \"\"\"\n    r0 = b - A @ x0\n    V, H, beta, k = arnoldi_basis(A, r0, m, tol=tol)\n    if k == 0:\n        # Exact at initial guess\n        return x0.copy(), 0.0, 0\n    # Solve min || H y - beta e1 ||_2\n    e1 = np.zeros((k + 1,), dtype=float)\n    e1[0] = 1.0\n    rhs = beta * e1\n    # Least squares solution using SVD-based solver\n    y, *_ = np.linalg.lstsq(H, rhs, rcond=None)\n    # Form x_m\n    x_m = x0 + V[:, :k] @ y\n    res = b - A @ x_m\n    res_norm = float(np.linalg.norm(res))\n    return x_m, res_norm, k\n\ndef solve():\n    # Define test cases as per the problem statement.\n    tests = []\n\n    # Case 1\n    A1 = np.array([\n        [4.0, 1.0, 0.0, 0.0],\n        [2.0, 3.0, 1.0, 0.0],\n        [0.0, 1.0, 3.0, 1.0],\n        [0.0, 0.0, 1.0, 2.0]\n    ], dtype=float)\n    b1 = np.array([1.0, 2.0, 3.0, 4.0], dtype=float)\n    x01 = np.zeros(4, dtype=float)\n    m1 = 4\n    tests.append((A1, b1, x01, m1))\n\n    # Case 2\n    A2 = np.array([\n        [3.0, -1.0, 0.0, 0.0, 0.0],\n        [2.0,  4.0, 1.0, 0.0, 0.0],\n        [0.0, -2.0, 3.0, 1.0, 0.0],\n        [0.0,  0.0,-1.0, 2.0, 1.0],\n        [0.0,  0.0, 0.0,-3.0, 1.0]\n    ], dtype=float)\n    b2 = np.array([1.0, 0.0, 1.0, 0.0, 1.0], dtype=float)\n    x02 = np.zeros(5, dtype=float)\n    m2 = 3\n    tests.append((A2, b2, x02, m2))\n\n    # Case 3\n    A3 = np.array([\n        [1.0, 10.0,  0.0,  0.0,  0.0],\n        [0.0,  1.0, 10.0,  0.0,  0.0],\n        [0.0,  0.0,  1.0, 10.0,  0.0],\n        [0.0,  0.0,  0.0,  1.0, 10.0],\n        [0.0,  0.0,  0.0,  0.0,  1.0]\n    ], dtype=float)\n    b3 = np.array([1.0, 1.0, 1.0, 1.0, 1.0], dtype=float)\n    x03 = np.zeros(5, dtype=float)\n    m3 = 5\n    tests.append((A3, b3, x03, m3))\n\n    # Case 4\n    A4 = np.array([\n        [2.0, -1.0,  0.0,  0.0],\n        [1.0,  2.0, -1.0,  0.0],\n        [0.0,  1.0,  2.0, -1.0],\n        [0.0,  0.0,  1.0,  2.0]\n    ], dtype=float)\n    b4 = np.array([1.0, 2.0, 2.0, 1.0], dtype=float)\n    x04 = np.array([0.5, -0.5, 0.5, -0.5], dtype=float)\n    m4 = 1\n    tests.append((A4, b4, x04, m4))\n\n    # Case 5\n    A5 = np.array([\n        [2.0, 1.0, 0.0],\n        [0.0, 3.0, 1.0],\n        [1.0, 0.0, 2.0]\n    ], dtype=float)\n    x_true5 = np.array([1.0, -1.0, 2.0], dtype=float)\n    b5 = A5 @ x_true5\n    x05 = x_true5.copy()\n    m5 = 3\n    tests.append((A5, b5, x05, m5))\n\n    results = []\n    for A, b, x0, m in tests:\n        _, res_norm, k = gmres_minres(A, b, x0, m, tol=1e-14)\n        # Round residual norm to eight decimal places as required\n        res_rounded = round(res_norm, 8)\n        results.append([res_rounded, int(k)])\n\n    # Print in the exact required single-line format\n    # Ensure standard Python list formatting\n    print(str(results))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "既然您已经实现了GMRES，理解其计算开销就变得至关重要，因为它决定了该方法在解决大规模问题时的实用性。这个分析性练习  要求您对单次GMRES迭代进行详细的浮点运算 (flop) 计数。这将帮助您识别算法中计算成本最高的部分，并理解选择重启参数 $m$ 时所涉及的权衡。",
            "id": "2397343",
            "problem": "考虑广义最小残差法（GMRES）应用于非对称线性系统的一次迭代，其系数矩阵为 $A \\in \\mathbb{R}^{n \\times n}$，其中 $A$ 有 $s$ 个非零元。假设一种实现方式，它使用修正的 Gram-Schmidt 过程构建一个标准正交的 Krylov 基 $\\{v_{1},\\dots,v_{m}\\}$，并使用 Givens 平面旋转来更新相应的 $(m+1)\\times m$ 阶上海森堡矩阵 $\\bar H_{m}$ 以及降维后的最小二乘问题的右端向量。在第 $m$ 次 GMRES 迭代中（即在 $\\{v_{1},\\dots,v_{m}\\}$ 已被构建之后，且在计算 $v_{m+1}$ 之前），按此顺序执行以下操作：\n1. 计算 $w = A v_{m}$。\n2. 对于 $j = 1,\\dots,m$，计算内积 $h_{j,m} = v_{j}^{\\top} w$ 并更新 $w \\leftarrow w - h_{j,m} v_{j}$。\n3. 计算 $h_{m+1,m} = \\lVert w \\rVert_{2}$，然后计算 $v_{m+1} = w / h_{m+1,m}$。\n4. 将先前累积的 $m-1$ 次 Givens 旋转应用于 $\\bar H_{m}$ 的新列，构造新的 Givens 旋转以将 $h_{m+1,m}$ 置零，将其应用于对 $(h_{m,m},h_{m+1,m})$，并更新降维后的右端向量中相应的两个条目。\n\n采用以下浮点运算（flop）模型：每次标量加法、减法、乘法、除法或平方根运算计为 $1$ 个 flop。忽略所有内存访问成本和数据移动。对于具有 $s$ 个非零元的稀疏矩阵-向量乘积，假设其成本恰好为 $2s$ 个 flop。对于长度为 $n$ 的向量内积，假设其成本恰好为 $2n-1$ 个 flop。对于形式为 $w \\leftarrow w - \\alpha v$ 的向量更新（向量长度为 $n$），假设其成本恰好为 $2n$ 个 flop。对于长度为 $n$ 的向量的欧几里得范数，假设其成本恰好为 $2n$ 个 flop（包括最后的平方根运算）。对于通过除以一个标量来归一化一个长度为 $n$ 的向量，假设其成本恰好为 $n$ 个 flop。对于将单次 Givens 旋转应用于一对标量，假设其成本恰好为 $6$ 个 flop。对于从两个标量构造 Givens 旋转的参数，假设其成本恰好为 $6$ 个 flop。\n\n在这些假设下，推导在此次 GMRES 单次迭代中执行的 flop 精确总数的闭式表达式 $F(n,s,m)$。你的最终答案必须是关于 $n$、$s$ 和 $m$ 的单个简化解析表达式。不要进行四舍五入。",
            "solution": "我们的目标是推导 GMRES 算法单次迭代的精确浮点运算（flop）总数 $F(n,s,m)$。我们将根据问题中定义的四个操作步骤和 flop 成本模型，逐一计算每个阶段的成本。\n\n**阶段 1：稀疏矩阵-向量乘积**\n此操作计算 $w = A v_{m}$。根据问题定义，对于一个具有 $s$ 个非零元的稀疏矩阵与一个长度为 $n$ 的向量相乘，其成本为：\n$$C_1 = 2s$$\n\n**阶段 2：修正的 Gram-Schmidt 正交化**\n此阶段通过一个从 $j=1$ 到 $m$ 的循环，将向量 $w$ 与已有的标准正交基向量 $\\{v_1, v_2, \\dots, v_m\\}$ 正交化。在循环的每次迭代中：\n-   计算内积 $h_{j,m} = v_{j}^{\\top} w$。对于长度为 $n$ 的向量，成本为 $2n-1$ 个 flop。\n-   更新向量 $w \\leftarrow w - h_{j,m} v_{j}$。对于长度为 $n$ 的向量，成本为 $2n$ 个 flop。\n因此，循环单次迭代的总成本为 $(2n-1) + 2n = 4n-1$ 个 flop。由于循环执行 $m$ 次，此阶段的总成本为：\n$$C_2 = m \\times (4n-1) = 4nm - m$$\n\n**阶段 3：归一化**\n此阶段在正交化后对向量 $w$ 进行归一化，以生成下一个基向量 $v_{m+1}$。\n-   计算 $w$ 的欧几里得范数 $h_{m+1,m} = \\lVert w \\rVert_{2}$。对于长度为 $n$ 的向量，成本为 $2n$ 个 flop。\n-   通过除以标量来归一化向量 $v_{m+1} = w / h_{m+1,m}$。成本为 $n$ 个 flop。\n此阶段的总成本为：\n$$C_3 = 2n + n = 3n$$\n\n**阶段 4：Givens 旋转更新**\n此阶段更新最小二乘问题。\n-   将先前累积的 $m-1$ 次 Givens 旋转应用于 $\\bar{H}_{m}$ 的新列。每次旋转成本为 $6$ 个 flop，总计 $6(m-1)$ 个 flop。\n-   根据新的列元素构造一次新的 Givens 旋转。成本为 $6$ 个 flop。\n-   将这个新的旋转应用于矩阵列中的一对标量。成本为 $6$ 个 flop。\n-   将同一个旋转应用于右端向量中的一对元素。成本为 $6$ 个 flop。\n此阶段的总成本为：\n$$C_4 = 6(m-1) + 6 + 6 + 6 = 6m - 6 + 18 = 6m + 12$$\n\n**总 Flop 数**\n第 $m$ 次迭代的总 flop 数 $F(n,s,m)$ 是所有四个阶段成本之和：\n$$F(n,s,m) = C_1 + C_2 + C_3 + C_4$$\n代入各项表达式：\n$$F(n,s,m) = 2s + (4nm - m) + 3n + (6m + 12)$$\n合并同类项，得到最终的简化闭式表达式：\n$$F(n,s,m) = 4nm + 3n + 5m + 2s + 12$$",
            "answer": "$$\\boxed{4nm + 3n + 5m + 2s + 12}$$"
        },
        {
            "introduction": "GMRES的效率不仅取决于每次迭代的成本，还取决于收敛所需的迭代次数。本练习  深入探讨了GMRES时而有趣甚至违反直觉的收敛行为。通过对与特征向量对齐的初始残差进行实验，您将亲眼见证问题的结构如何导致从立即停滞到单步收敛等各种不同的结果。",
            "id": "2397325",
            "problem": "您需要编写一个完整、可运行的程序，该程序实现用于求解非对称线性系统的广义最小残差方法（GMRES），然后用它来通过经验证明初始残差与系数矩阵的特征向量的对齐方式如何影响收敛性。您的实现必须仅基于以下基本定义和事实：\n\n- 由矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和向量 $r_0 \\in \\mathbb{R}^n$ 生成的 Krylov 子空间定义为\n$$\n\\mathcal{K}_k(A,r_0) = \\mathrm{span}\\{r_0, A r_0, A^2 r_0, \\dots, A^{k-1} r_0\\}.\n$$\n- 广义最小残差方法（GMRES）在仿射空间 $x_0 + \\mathcal{K}_k(A,r_0)$ 中寻找一个近似解 $x_k$，该解在每次迭代 $k$ 时最小化残差 $r_k = b - A x_k$ 的欧几里得范数。\n- Arnoldi 过程产生 $\\mathcal{K}_k(A,r_0)$ 的一个标准正交基和一个上 Hessenberg 矩阵，该矩阵编码了 $A$ 在此基上的投影。\n\n您的任务是使用 Arnoldi 过程、修正 Gram–Schmidt 正交化以及每次迭代中的一个小型最小二乘问题，来实现一个密集、双精度、非重启动的 GMRES。您的实现必须：\n- 在所有测试中均使用 $x_0 = 0$，因此 $r_0 = b$。\n- 在每次迭代 $k$ 时，构建与 Arnoldi 关系式相关的最小二乘问题，并产生最小化解 $x_k \\in x_0 + \\mathcal{K}_k(A,r_0)$。\n- 在每个 $k$ 计算并记录真实残差范数 $\\|b - A x_k\\|_2$，一旦其小于或等于容差就停止。\n- 当无法形成新的基向量时（即，当下一个次对角线 Arnoldi 系数在数值上为零时），检测 Arnoldi 过程中的崩溃。在这种情况下，使用当前的 Krylov 子空间计算当前 $k$ 的 $x_k$ 及其残差范数。如果 Arnoldi 过程无法进一步扩展且尚未达到容差，则宣布停滞，并表明在允许的迭代次数内未能达到要求的容差。\n\n您的程序必须在以下测试套件上运行 GMRES 实现。在每个测试中，使用容差 $\\text{tol} = 10^{-12}$ 和 $\\text{max\\_iter} = 5$。\n\n- 测试用例 1（边界情况：奇异矩阵，初始残差与零特征值对应的特征向量共线，导致停滞）：\n  - $A = \\begin{bmatrix} 0  1 \\\\ 0  0 \\end{bmatrix}$，\n  - $b = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$，\n  - $x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n  这个矩阵 $A$ 有一个特征值 $0$，对应的特征向量为 $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。此处 $r_0 = b$ 与该特征向量对齐。报告满足 $\\|b - A x_k\\|_2 \\le \\text{tol}$ 的最小迭代次数 $k$；如果在 $\\text{max\\_iter}$ 内从未发生此情况，或者 Arnoldi 过程在未达到容差的情况下崩溃，则返回 $-1$。\n\n- 测试用例 2（边界情况：初始残差与非零特征值对应的特征向量共线，在精确算术下可实现单步求解）：\n  - $A = \\begin{bmatrix} 2  1000 \\\\ 0  1 \\end{bmatrix}$，\n  - $b = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$，\n  - $x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n  这个矩阵 $A$ 是上三角且非对称的，有一个特征值 $2$ 和对应的特征向量 $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。报告满足容差的最小迭代次数 $k$，如果未达到则返回 $-1$。\n\n- 测试用例 3（理想情况：对于相同的非对称矩阵，使用一般残差）：\n  - $A = \\begin{bmatrix} 2  1000 \\\\ 0  1 \\end{bmatrix}$，\n  - $b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$，\n  - $x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n  报告满足容差的最小迭代次数 $k$，如果未达到则返回 $-1$。\n\n- 测试用例 4（近边界条件：初始残差与一个非常小的非零特征值对应的特征向量对齐）：\n  - $A = \\begin{bmatrix} 10^{-8}  1 \\\\ 0  1 \\end{bmatrix}$，\n  - $b = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$，\n  - $x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n  报告满足容差的最小迭代次数 $k$，如果未达到则返回 $-1$。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$\\texttt{[result1,result2,result3,result4]}$），其中每个条目是相应测试用例的整数迭代次数，如果未能在限制内达到容差或方法因崩溃而停滞，则为 $-1$。\n\n不应读取任何用户输入；所有数据都应按上述规定硬编码。",
            "solution": "对于给定的矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和向量 $b \\in \\mathbb{R}^n$，求解非对称线性系统 $Ax=b$ 以得到 $x \\in \\mathbb{R}^n$ 的问题，将通过实现广义最小残差（GMRES）方法来解决。解决方案必须按照概述的第一性原理构建。\n\nGMRES 的核心原理是在每次迭代 $k$ 时找到一个近似解 $x_k$，该解最小化残差的欧几里得范数 $\\|r_k\\|_2 = \\|b - Ax_k\\|_2$。对 $x_k$ 的搜索被限制在一个仿射子空间 $x_0 + \\mathcal{K}_k(A, r_0)$ 中，其中 $x_0$ 是初始猜测，$\\mathcal{K}_k(A, r_0)$ 是由 $A$ 和初始残差 $r_0 = b - Ax_0$ 生成的第 $k$ 个 Krylov 子空间。Krylov 子空间定义为：\n$$\n\\mathcal{K}_k(A,r_0) = \\mathrm{span}\\{r_0, A r_0, A^2 r_0, \\dots, A^{k-1} r_0\\}\n$$\n这个子空间的一个标准正交基通过 Arnoldi 过程构建。设这个基为 $V_k = [v_1, v_2, \\dots, v_k]$。带有修正 Gram-Schmidt（MGS）正交化的 Arnoldi 过程如下进行：\n1. 用 $v_1 = r_0 / \\|r_0\\|_2$ 进行初始化。\n2. 对于每个 $j = 1, 2, \\dots, k$：\n    a. 计算新的候选向量 $w = Av_j$。\n    b. 使用 MGS 将 $w$ 与现有的基向量 $\\{v_1, \\dots, v_j\\}$ 正交化。对于 $i=1, \\dots, j$，计算 $h_{ij} = v_i^T w$ 并更新 $w \\leftarrow w - h_{ij}v_i$。\n    c. 计算所得向量的范数，$h_{j+1,j} = \\|w\\|_2$。此系数是 Hessenberg 矩阵中的一个次对角线元素。\n    d. 如果 $h_{j+1,j}$ 在数值上为零，则称该过程发生崩溃，因为无法生成新的线性无关基向量。这意味着 Krylov 子空间在 $A$ 的作用下是不变的。\n    e. 标准化以找到下一个基向量：$v_{j+1} = w / h_{j+1,j}$。\n\n此过程生成一个标准正交基 $V_{k+1} = [v_1, \\dots, v_{k+1}]$ 和一个上 Hessenberg 矩阵 $\\bar{H}_k \\in \\mathbb{R}^{(k+1) \\times k}$，其元素为 $h_{ij}$，满足 Arnoldi 关系式：\n$$\nAV_k = V_{k+1} \\bar{H}_k\n$$\n近似解 $x_k$ 可以写成 $x_k = x_0 + z_k$，其中 $z_k \\in \\mathcal{K}_k(A, r_0)$。由于 $V_k$ 是 $\\mathcal{K}_k$ 的一个基，我们可以将 $z_k$ 表示为 $z_k = V_k y_k$，其中 $y_k \\in \\mathbb{R}^k$ 是某个系数向量。GMRES 最小化问题就是找到使残差范数最小化的 $y_k$：\n$$\n\\min_{y_k \\in \\mathbb{R}^k} \\|b - A(x_0 + V_k y_k)\\|_2 = \\min_{y_k \\in \\mathbb{R}^k} \\|r_0 - AV_k y_k\\|_2\n$$\n代入 Arnoldi 关系式 $AV_k = V_{k+1} \\bar{H}_k$ 并注意到 $r_0 = \\|r_0\\|_2 v_1$，目标函数变为：\n$$\n\\min_{y_k \\in \\mathbb{R}^k} \\| \\|r_0\\|_2 v_1 - V_{k+1} \\bar{H}_k y_k \\|_2\n$$\n由于 $V_{k+1}$ 的列是标准正交的，从左侧乘以 $V_{k+1}^T$ 会保持欧几里得范数。认识到 $V_{k+1}^T v_1 = e_1$，其中 $e_1 = [1, 0, \\dots, 0]^T \\in \\mathbb{R}^{k+1}$，我们得到等价的最小二乘问题：\n$$\n\\min_{y_k \\in \\mathbb{R}^k} \\| \\|r_0\\|_2 e_1 - \\bar{H}_k y_k \\|_2\n$$\n这是一个关于未知向量 $y_k$ 的小型 $(k+1) \\times k$ 线性最小二乘问题。可以使用标准方法（如对 $\\bar{H}_k$ 进行 QR 分解）高效地求解。一旦找到 $y_k$，解就更新为 $x_k = x_0 + V_k y_k$。\n该算法迭代进行。在从 $1$ 到 $\\text{max\\_iter}$ 的每一步 $k$，我们扩展 Krylov 基，构建并求解相关的最小二乘问题，并计算得到的真实残差范数 $\\|b - Ax_k\\|_2$。如果此范数小于或等于指定的容差 $\\text{tol}$，算法成功终止，返回迭代次数 $k$。如果在达到容差之前 Arnoldi 过程崩溃（$h_{k,k-1} \\approx 0$），则表明停滞，因为搜索空间无法进一步扩展。在这种情况下，或者如果达到最大迭代次数而未收敛，则认为该过程失败。\n所提供的实现遵循此公式，并设置 $x_0 = 0$，因此 $r_0=b$。它使用双精度浮点算术和 `numpy.linalg.lstsq` 来求解每次迭代中的最小二乘子问题。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef gmres_solver(A, b, x0, tol, max_iter):\n    \"\"\"\n    Solves the linear system Ax = b using the unrestarted GMRES method.\n\n    Args:\n        A (np.ndarray): The coefficient matrix of size (n, n).\n        b (np.ndarray): The right-hand side vector of size (n,).\n        x0 (np.ndarray): The initial guess for the solution of size (n,).\n        tol (float): The convergence tolerance for the true residual norm.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        int: The number of iterations to converge, or -1 if it fails.\n    \"\"\"\n    n = A.shape[0]\n    r0 = b - A @ x0\n\n    r0_norm = np.linalg.norm(r0)\n    if r0_norm = tol:\n        return 0\n\n    # V stores orthonormal basis vectors of the Krylov subspace.\n    # We allocate for max_iter + 1 vectors.\n    V = np.zeros((n, max_iter + 1), dtype=np.float64)\n    V[:, 0] = r0 / r0_norm\n\n    # H stores the upper Hessenberg matrix from the Arnoldi process.\n    # It has size (max_iter + 1) x max_iter.\n    H = np.zeros((max_iter + 1, max_iter), dtype=np.float64)\n\n    for k in range(1, max_iter + 1):\n        # Arnoldi process using Modified Gram-Schmidt for one step.\n        # This computes the k-th column of H and the (k+1)-th vector in V.\n        # The loop indices are 0-based, so iteration k corresponds to k-1 in array indices.\n        w = A @ V[:, k - 1]\n        for i in range(k):\n            H[i, k - 1] = V[:, i].T @ w\n            w = w - H[i, k - 1] * V[:, i]\n        \n        H[k, k - 1] = np.linalg.norm(w)\n\n        # Form the least-squares problem for the current iteration k.\n        # We need to solve || beta * e1 - H_k * y ||_2 where H_k is (k+1) x k.\n        H_k = H[0:k + 1, 0:k]\n        e1 = np.zeros(k + 1, dtype=np.float64)\n        e1[0] = 1.0\n        rhs = r0_norm * e1\n\n        # Solve the least-squares problem for y.\n        y, _, _, _ = np.linalg.lstsq(H_k, rhs, rcond=None)\n\n        # Compute the solution for the current iteration.\n        x_k = x0 + V[:, 0:k] @ y\n\n        # Check the true residual norm for convergence.\n        true_res_norm = np.linalg.norm(b - A @ x_k)\n\n        if true_res_norm = tol:\n            return k\n\n        # Check for breakdown. If h_{k+1, k} is near zero, the Krylov\n        # subspace can no longer be expanded. If we have not converged,\n        # it is a stagnation failure.\n        if H[k, k - 1]  1e-15:\n            return -1\n        \n        # Normalize the next basis vector.\n        V[:, k] = w / H[k, k - 1]\n        \n    # Maximum iterations reached without convergence.\n    return -1\n\ndef solve():\n    \"\"\"\n    Runs the GMRES solver on the specified test cases and prints the results.\n    \"\"\"\n    tol = 1e-12\n    max_iter = 5\n    x0 = np.array([0.0, 0.0], dtype=np.float64)\n\n    test_cases = [\n        # Test case 1: Singular matrix, initial residual aligned with null-space eigenvector.\n        {\n            \"A\": np.array([[0.0, 1.0], [0.0, 0.0]], dtype=np.float64),\n            \"b\": np.array([1.0, 0.0], dtype=np.float64),\n        },\n        # Test case 2: Nonsymmetric matrix, initial residual is an eigenvector.\n        {\n            \"A\": np.array([[2.0, 1000.0], [0.0, 1.0]], dtype=np.float64),\n            \"b\": np.array([1.0, 0.0], dtype=np.float64),\n        },\n        # Test case 3: Same matrix, generic initial residual.\n        {\n            \"A\": np.array([[2.0, 1000.0], [0.0, 1.0]], dtype=np.float64),\n            \"b\": np.array([1.0, 1.0], dtype=np.float64),\n        },\n        # Test case 4: Nearly singular matrix, initial residual is an eigenvector.\n        {\n            \"A\": np.array([[1e-8, 1.0], [0.0, 1.0]], dtype=np.float64),\n            \"b\": np.array([1.0, 0.0], dtype=np.float64),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        b = case[\"b\"]\n        result = gmres_solver(A, b, x0, tol, max_iter)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}