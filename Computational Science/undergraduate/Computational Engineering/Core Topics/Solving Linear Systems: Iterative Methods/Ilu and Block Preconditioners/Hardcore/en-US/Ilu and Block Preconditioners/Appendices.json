{
    "hands_on_practices": [
        {
            "introduction": "While Incomplete LU factorization is a powerful technique, its simplest form, ILU(0), is not universally stable. This exercise provides a concrete demonstration of a potential failure mode where the factorization process breaks down by generating a singular factor, even for a non-singular input matrix. By manually computing the factorization for a parameterized matrix , you will identify the precise condition that leads to this instability, gaining crucial insight into the limitations of basic ILU methods.",
            "id": "2401113",
            "problem": "Consider the parameterized symmetric tridiagonal matrix\n$$\nA(\\alpha)=\\begin{pmatrix}\n2  -1  0 \\\\\n-1  2+\\alpha  -1 \\\\\n0  -1  2\n\\end{pmatrix},\n$$\nwhere $\\alpha \\in \\mathbb{R}$. Apply the incomplete lower-upper factorization with zero fill, denoted $\\operatorname{ILU}(0)$, with the following conventions: no pivoting is used, the lower factor $L$ is unit lower triangular (diagonal entries equal to $1$) with the same sparsity pattern as the strictly lower triangular part of $A(\\alpha)$, and the upper factor $U$ is upper triangular with the same sparsity pattern as the upper triangular part of $A(\\alpha)$ (including the diagonal). Use the natural ordering of rows and columns.\n\nDetermine the value of $\\alpha$ for which the second diagonal entry of the incomplete upper factor $U$ produced by $\\operatorname{ILU}(0)$ equals zero. Provide your answer as an exact reduced fraction. Do not round.",
            "solution": "The problem is first validated for scientific and logical consistency.\n\n**Step 1: Extract Givens**\n- The matrix is $A(\\alpha)=\\begin{pmatrix} 2  -1  0 \\\\ -1  2+\\alpha  -1 \\\\ 0  -1  2 \\end{pmatrix}$.\n- The parameter $\\alpha$ is a real number, $\\alpha \\in \\mathbb{R}$.\n- The factorization is the incomplete lower-upper factorization with zero fill-in, denoted $\\operatorname{ILU}(0)$.\n- The factorization conventions are: no pivoting, the lower factor $L$ is unit lower triangular (diagonal entries are $1$), and the upper factor $U$ is upper triangular.\n- The sparsity patterns of $L$ and $U$ are constrained: $L$ takes the sparsity of the strictly lower triangular part of $A(\\alpha)$, and $U$ takes the sparsity of the upper triangular part of $A(\\alpha)$ (including the diagonal).\n- The goal is to find the value of $\\alpha$ such that the second diagonal entry of $U$, $u_{22}$, is equal to $0$.\n- The answer must be an exact reduced fraction.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, being a standard exercise in numerical linear algebra concerning preconditioners for iterative methods. It is well-posed, with a clearly defined matrix, a standard algorithm ($\\operatorname-ILU}(0)$), and a precise objective. The language is objective and unambiguous. The problem is self-contained and free from contradictions or missing information. The calculation is feasible and leads to a verifiable result.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A solution will be computed.\n\nThe problem requires the determination of the parameter $\\alpha$ for which the $\\operatorname{ILU}(0)$ factorization of the matrix $A(\\alpha)$ results in a zero value for the second diagonal element of the upper triangular factor $U$.\n\nThe given matrix is:\n$$\nA(\\alpha) = \\begin{pmatrix} 2  -1  0 \\\\ -1  2+\\alpha  -1 \\\\ 0  -1  2 \\end{pmatrix}\n$$\nThe $\\operatorname{ILU}(0)$ factorization requires that the factors $L$ and $U$ have the same sparsity pattern as the strictly lower and upper-or-diagonal parts of $A(\\alpha)$, respectively. The matrix $L$ must be unit lower triangular (i.e., $l_{ii} = 1$ for all $i$), and $U$ is upper triangular.\n\nGiven the structure of $A(\\alpha)$, where the non-zero off-diagonal entries are $a_{12}$, $a_{21}$, $a_{23}$, and $a_{32}$, the factors $L$ and $U$ must have the following form to respect the zero fill-in constraint:\n$$\nL = \\begin{pmatrix} 1  0  0 \\\\ l_{21}  1  0 \\\\ 0  l_{32}  1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} u_{11}  u_{12}  0 \\\\ 0  u_{22}  u_{23} \\\\ 0  0  u_{33} \\end{pmatrix}\n$$\nThe entries $l_{31}$ and $u_{13}$ are zero because the corresponding entries $a_{31}$ and $a_{13}$ in the matrix $A(\\alpha)$ are zero.\n\nThe factorization process computes the unknown entries of $L$ and $U$ such that the product $LU$ matches $A(\\alpha)$ on its defined sparsity pattern. The product $LU$ is:\n$$\nLU = \\begin{pmatrix} 1  0  0 \\\\ l_{21}  1  0 \\\\ 0  l_{32}  1 \\end{pmatrix} \\begin{pmatrix} u_{11}  u_{12}  0 \\\\ 0  u_{22}  u_{23} \\\\ 0  0  u_{33} \\end{pmatrix} = \\begin{pmatrix} u_{11}  u_{12}  0 \\\\ l_{21}u_{11}  l_{21}u_{12}+u_{22}  u_{23} \\\\ 0  l_{32}u_{22}  l_{32}u_{23}+u_{33} \\end{pmatrix}\n$$\nWe equate the elements of $LU$ to the corresponding elements of $A(\\alpha)$ following the natural row-wise ordering.\n\nFor the first row ($i=1$):\nThe entries of the first row of $U$ are taken directly from the first row of $A(\\alpha)$.\n$$\nu_{11} = a_{11} = 2\n$$\n$$\nu_{12} = a_{12} = -1\n$$\n\nFor the second row ($i=2$):\nWe first compute the sub-diagonal element of $L$.\n$$\nl_{21}u_{11} = a_{21} \\implies l_{21}(2) = -1 \\implies l_{21} = -\\frac{1}{2}\n$$\nNext, we compute the diagonal element of $U$. This is the term of interest.\n$$\nl_{21}u_{12} + u_{22} = a_{22} \\implies u_{22} = a_{22} - l_{21}u_{12}\n$$\nSubstituting the known values:\n$$\nu_{22} = (2+\\alpha) - \\left(-\\frac{1}{2}\\right)(-1) = 2+\\alpha - \\frac{1}{2} = \\frac{3}{2} + \\alpha\n$$\nThe super-diagonal element of $U$ is:\n$$\nu_{23} = a_{23} = -1\n$$\n\nThe problem requires us to find the value of $\\alpha$ for which the second diagonal entry of $U$ is zero.\n$$\nu_{22} = 0\n$$\nUsing our derived expression for $u_{22}$:\n$$\n\\frac{3}{2} + \\alpha = 0\n$$\nSolving this simple linear equation for $\\alpha$:\n$$\n\\alpha = -\\frac{3}{2}\n$$\nThis is the value of $\\alpha$ for which the $\\operatorname{ILU}(0)$ algorithm would encounter a zero pivot on the second step. The calculation for the remaining entries ($l_{32}$, $u_{33}$) would fail due to division by $u_{22}=0$. The problem, however, only asks for the value of $\\alpha$ that causes this condition, not for the full factorization.\n\nThe resulting value is an exact reduced fraction as required.",
            "answer": "$$\n\\boxed{-\\frac{3}{2}}\n$$"
        },
        {
            "introduction": "After exploring a limitation of ILU(0), we now turn to the more general ILU(p) factorization, where the level-of-fill parameter $p$ allows for a denser, and often more effective, preconditioner. The choice of $p$ creates a fundamental trade-off between memory usage and computational cost. This analytical exercise  challenges you to derive these costs from first principles for a standard 3D Laplacian problem, developing your ability to reason about the scalability and performance of preconditioners in large-scale scientific computing.",
            "id": "2401089",
            "problem": "Consider the linear system arising from the standard second-order finite-difference discretization of the three-dimensional Laplacian on a cubic grid with $n$ interior points along each axis, so there are $N = n^{3}$ unknowns. Let the unknowns be ordered lexicographically. We precondition the system using pointwise Incomplete Lower-Upper factorization of level $p$ (Incomplete LU (ILU)$(p)$) defined by the graph-distance model: the sparsity pattern retained in the factors is the set of entries whose associated grid points lie within graph distance at most $p$ from the diagonal entryâ€™s grid point in the three-dimensional grid graph (Manhattan distance in $\\mathbb{Z}^{3}$), with the strictly lower-triangular part containing only predecessors in the ordering and the strictly upper-triangular part containing only successors. Assume interior nodes dominate so that boundary effects are negligible, and that the diagonal is stored once (shared by the lower and upper factors).\n\nDefine the memory usage as the total number of stored scalar nonzeros across both factors and the diagonal. Define the computational complexity as the total number of floating-point multiply-add operations required to apply the preconditioner once (one forward substitution with the lower factor followed by one backward substitution with the upper factor), counting one multiply-add per strictly off-diagonal nonzero encountered.\n\nStarting from first principles under these assumptions, derive closed-form expressions (to leading order in $n$ and exactly in $p$) for:\n- the total memory usage as a function of $n$ and $p$, and\n- the total application cost as a function of $n$ and $p$.\n\nFinally, in the limit $n \\to \\infty$ with fixed $p$, express the ratio\n$$R(p) = \\lim_{n \\to \\infty} \\frac{\\text{application cost}}{\\text{memory usage}}$$\nas a single closed-form function of $p$ only. Provide $R(p)$ as your final answer. No units are required for the final answer.",
            "solution": "The problem as stated is subjected to rigorous validation.\n\n**Step 1: Extracted Givens**\n- **System**: Linear system from a standard second-order finite-difference discretization of the three-dimensional Laplacian.\n- **Grid**: A cubic grid with $n$ interior points along each axis.\n- **Unknowns**: Total number of unknowns is $N = n^{3}$.\n- **Ordering**: The unknowns are ordered lexicographically.\n- **Preconditioner**: Pointwise Incomplete Lower-Upper factorization of level $p$, denoted ILU($p$).\n- **Sparsity Pattern**: The pattern is defined by a graph-distance model. Retained entries $(i,j)$ in the factors $L$ and $U$ correspond to grid points where the graph distance (Manhattan distance in $\\mathbb{Z}^{3}$) between the grid points associated with row $i$ and column $j$ is at most $p$. The strictly lower-triangular part contains only predecessors in the ordering, and the strictly upper-triangular part contains only successors.\n- **Assumptions**: Interior nodes dominate, so boundary effects are considered negligible. The diagonal of the preconditioner is stored only once.\n- **Definitions**:\n    - **Memory Usage**: The total number of stored scalar nonzeros across both factors and the diagonal.\n    - **Computational Complexity (Application Cost)**: The total number of floating-point multiply-add operations required for one forward substitution followed by one backward substitution. This is counted as one multiply-add per strictly off-diagonal nonzero encountered during the substitutions.\n- **Objective**:\n    1. Derive a closed-form expression for the total memory usage as a function of $n$ and $p$.\n    2. Derive a closed-form expression for the total application cost as a function of $n$ and $p$.\n    3. Express the ratio $R(p) = \\lim_{n \\to \\infty} \\frac{\\text{application cost}}{\\text{memory usage}}$ as a function of $p$ only.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is a standard theoretical problem in the field of numerical analysis for partial differential equations, specifically concerning preconditioning techniques for large sparse linear systems. The definitions are precise, and the assumptions (e.g., negligible boundary effects) are standard for asymptotic analysis. The problem does not violate any scientific principles, is not based on false premises, is formalizable, and contains sufficient information for a unique solution under the given assumptions.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete, reasoned solution will be provided.\n\n**Derivation of Solution**\nThe analysis hinges on determining the number of nonzeros per row of the preconditioner for a typical interior node, as boundary effects are negligible. The total number of unknowns is $N = n^3$.\n\nFirst, we must determine the number of entries in the sparsity pattern for a single row. This is defined by the graph-distance model. For a grid point, the corresponding row in the ILU factors will have nonzeros for all other grid points within a Manhattan distance of $p$. Let us calculate the number of integer points $(x, y, z) \\in \\mathbb{Z}^3$ such that their Manhattan distance from the origin, $|x| + |y| + |z|$, is less than or equal to $p$. This count gives the number of nonzeros, $N_p$, for a row corresponding to an interior node in the combined pattern of the $L$ and $U$ factors.\n\nThe number of points with Manhattan distance exactly $d  0$ from the origin in a $3D$ grid is given by the formula $4d^2 + 2$. For $d=0$, there is only one point, the origin itself.\nThe total number of points $N_p$ within a distance $p$ is the sum of points at distances $d = 0, 1, \\dots, p$:\n$$N_p = 1 + \\sum_{d=1}^{p} (4d^2 + 2)$$\nWe evaluate the sum:\n$$N_p = 1 + 4\\sum_{d=1}^{p} d^2 + 2\\sum_{d=1}^{p} 1$$\nUsing the standard formulas for sums of powers, $\\sum_{d=1}^{p} 1 = p$ and $\\sum_{d=1}^{p} d^2 = \\frac{p(p+1)(2p+1)}{6}$, we have:\n$$N_p = 1 + 2p + 4 \\frac{p(p+1)(2p+1)}{6} = 1 + 2p + \\frac{2}{3} p(2p^2 + 3p + 1)$$\n$$N_p = 1 + 2p + \\frac{4}{3}p^3 + 2p^2 + \\frac{2}{3}p = \\frac{4}{3}p^3 + 2p^2 + \\frac{8}{3}p + 1$$\nTo write this with a common denominator of $3$:\n$$N_p = \\frac{4p^3 + 6p^2 + 8p + 3}{3}$$\nThis expression, $N_p$, represents the total number of nonzeros (diagonal and off-diagonal) in the sparsity pattern for a single row of the preconditioner, assuming an interior node.\n\n**Memory Usage**\nThe memory usage is defined as the total number of stored scalar nonzeros. Based on the problem statement that the diagonal is stored once and the sparsity pattern is determined by the union of $L$ and $U$ factors, the number of stored elements per row is precisely $N_p$. Since there are $N = n^3$ rows and we are ignoring boundary effects, the total memory usage, $\\text{Mem}(n,p)$, is:\n$$\\text{Mem}(n,p) = N \\times N_p = n^3 \\left( \\frac{4p^3 + 6p^2 + 8p + 3}{3} \\right)$$\nThis is the expression for memory usage, to leading order in $n$ and exact in $p$.\n\n**Application Cost**\nThe application cost is the number of multiply-add operations for one forward and one backward substitution. The problem specifies this is one operation per strictly off-diagonal nonzero.\n- Forward substitution with $L$ involves, for each row $i$, a number of operations equal to the number of nonzeros in the strictly lower part of row $i$ of $L$.\n- Backward substitution with $U$ involves, for each row $i$, a number of operations equal to the number of nonzeros in the strictly upper part of row $i$ of $U$.\n\nThe total cost for row $i$ is the number of its strictly off-diagonal nonzeros in the preconditioner. For an interior node, the total number of nonzeros is $N_p$. One of these is the diagonal entry. Thus, the number of strictly off-diagonal nonzeros is $N_p - 1$.\nThe total application cost, $\\text{Cost}(n,p)$, is the cost per row multiplied by the number of rows $N = n^3$:\n$$\\text{Cost}(n,p) = N \\times (N_p - 1) = n^3 \\left( \\left( \\frac{4p^3 + 6p^2 + 8p + 3}{3} \\right) - 1 \\right)$$\n$$\\text{Cost}(n,p) = n^3 \\left( \\frac{4p^3 + 6p^2 + 8p + 3 - 3}{3} \\right) = n^3 \\left( \\frac{4p^3 + 6p^2 + 8p}{3} \\right)$$\nThis is the expression for application cost, to leading order in $n$ and exact in $p$.\n\n**Ratio R(p)**\nFinally, we compute the ratio $R(p)$ in the limit as $n \\to \\infty$:\n$$R(p) = \\lim_{n \\to \\infty} \\frac{\\text{Cost}(n,p)}{\\text{Mem}(n,p)} = \\lim_{n \\to \\infty} \\frac{n^3 \\left( \\frac{4p^3 + 6p^2 + 8p}{3} \\right)}{n^3 \\left( \\frac{4p^3 + 6p^2 + 8p + 3}{3} \\right)}$$\nThe term $n^3$ and the denominator $3$ cancel, and since the expression is independent of $n$, the limit is the expression itself:\n$$R(p) = \\frac{4p^3 + 6p^2 + 8p}{4p^3 + 6p^2 + 8p + 3}$$\nThis is the final closed-form function of $p$.",
            "answer": "$$\n\\boxed{\\frac{4p^3 + 6p^2 + 8p}{4p^3 + 6p^2 + 8p + 3}}\n$$"
        },
        {
            "introduction": "While level-based methods like ILU(p) control sparsity based on graph structure, an alternative and highly practical approach is to drop entries based on their numerical magnitude. This exercise delves into this technique by having you implement ILUT (ILU with Thresholding), a robust variant that uses a dual-tolerance strategy for fine-grained control over memory usage. By coding this algorithm and testing it , you will gain direct experience with the trade-off between factorization accuracy and memory cost, a central challenge in designing efficient preconditioners.",
            "id": "2401050",
            "problem": "You are given a real square sparse matrix $A \\in \\mathbb{R}^{n \\times n}$. Your task is to implement an Incomplete Lower-Upper factorization with threshold (ILUT) equipped with a secondary drop tolerance to refine memory control. The sought factorization has the form $A \\approx L U$, where $L$ is unit lower triangular (ones on the diagonal) and $U$ is upper triangular. Let the primary threshold be $\\tau_1 \\ge 0$ and the secondary threshold be $\\tau_2 \\ge 0$. The following definitions are required.\n\n1) For each row index $i \\in \\{0,\\dots,n-1\\}$, denote the $i$-th row of $A$ by $a_i^\\top$ and let its infinity norm be $\\lVert a_i \\rVert_\\infty = \\max_j |a_{ij}|$. Define the working row $w$ for row $i$ by initializing $w_j \\leftarrow a_{ij}$ for all $j$ such that $a_{ij} \\ne 0$. The primary thresholding rule is as follows:\n- During the row-wise Gaussian elimination process that forms the multipliers $l_{ij}$ for $ji$, drop any candidate multiplier $l_{ij}$ if $|l_{ij}| \\le \\tau_1 \\lVert a_i \\rVert_\\infty$.\n- During the update of the working row $w$ for row $i$, after each operation $w_k \\leftarrow w_k - l_{ij} u_{jk}$, drop any entry $w_k$ that satisfies $|w_k| \\le \\tau_1 \\lVert a_i \\rVert_\\infty$.\n\n2) After completing the elimination for row $i$, define the provisional $L$-row and $U$-row by\n- $L_{ij} \\leftarrow l_{ij}$ for the kept multipliers with $ji$, and $L_{ii} \\leftarrow 1$,\n- $U_{ik} \\leftarrow w_k$ for all $k \\ge i$ remaining in the working row (with $U_{ii}$ never dropped; if $U_{ii}$ would be zero, replace it by a positive perturbation of negligible magnitude).\n\n3) The secondary thresholding rule is applied after forming the provisional rows:\n- For the off-diagonal part of $L$ in row $i$, compute $r_L^{(i)} = \\max_{ji} |L_{ij}|$ (define $r_L^{(i)} = 0$ if there is no off-diagonal entry). Drop any $L_{ij}$ with $ji$ satisfying $|L_{ij}|  \\tau_2 \\, r_L^{(i)}$. Always keep $L_{ii}=1$.\n- For $U$ in row $i$, compute $r_U^{(i)} = \\max_{ki} |U_{ik}|$ (define $r_U^{(i)} = 0$ if there is no off-diagonal entry). Drop any $U_{ik}$ with $ki$ satisfying $|U_{ik}|  \\tau_2 \\, r_U^{(i)}$. Always keep $U_{ii}$.\n\nDefine the factorization defect in the Frobenius norm by\n$$\n\\Delta_F = \\lVert A - L U \\rVert_F = \\sqrt{\\sum_{i=0}^{n-1}\\sum_{j=0}^{n-1} \\left( a_{ij} - (LU)_{ij} \\right)^2}.\n$$\nDefine the total stored nonzeros by $N_{\\text{nz}} = \\text{nnz}(L) + \\text{nnz}(U)$, where $\\text{nnz}(\\cdot)$ counts all stored entries, including the unit diagonal of $L$ and the diagonal of $U$.\n\nImplement a program that, for each test case specified below, constructs $A$, computes $(L,U)$ using the rules above with the given $(\\tau_1,\\tau_2)$, and outputs for that test case the pair $[\\Delta_F, N_{\\text{nz}}]$, where $\\Delta_F$ is rounded to $10$ decimal places and $N_{\\text{nz}}$ is an integer.\n\nUse the following test suite:\n- Test 1 (happy path): $A \\in \\mathbb{R}^{30 \\times 30}$ is the tridiagonal matrix with $2$ on the main diagonal and $-1$ on the first sub- and super-diagonals (a one-dimensional Poisson operator with Dirichlet boundary conditions). Use $\\tau_1 = 10^{-3}$ and $\\tau_2 = 0$.\n- Test 2 (secondary drop effect): Same $A$ as Test $1$. Use $\\tau_1 = 10^{-3}$ and $\\tau_2 = 10^{-2}$.\n- Test 3 (wider bandwidth, stricter primary): $A \\in \\mathbb{R}^{25 \\times 25}$ is symmetric pentadiagonal with main diagonal $6$, first off-diagonals $-1$, and second off-diagonals $-0.2$. Use $\\tau_1 = 10^{-4}$ and $\\tau_2 = 10^{-3}$.\n- Test 4 (block-structured, scaled blocks): $A \\in \\mathbb{R}^{24 \\times 24}$ is block diagonal with two square blocks of equal size $12$: the first block equals the Test $1$ matrix with $n=12$; the second block equals $0.1$ times that same matrix. Use $\\tau_1 = 5 \\cdot 10^{-4}$ and $\\tau_2 = 2 \\cdot 10^{-2}$.\n\nFinal output format: Your program should produce a single line of output containing a list of the $4$ test results in order, where each result is the two-element list $[\\Delta_F,N_{\\text{nz}}]$. The single line must be exactly of the form\n$[[\\Delta_F^{(1)},N_{\\text{nz}}^{(1)}],[\\Delta_F^{(2)},N_{\\text{nz}}^{(2)}],[\\Delta_F^{(3)},N_{\\text{nz}}^{(3)}],[\\Delta_F^{(4)},N_{\\text{nz}}^{(4)}]]$\nwith each $\\Delta_F^{(k)}$ rounded to $10$ decimal places and no spaces anywhere in the line.",
            "solution": "The problem presented is a request to implement a specific variant of an Incomplete Lower-Upper (ILU) factorization, known as ILUT, which incorporates a dual-thresholding strategy for controlling fill-in. This is a standard and well-defined problem in the field of numerical linear algebra, commonly encountered in the context of preconditioning iterative solvers for large, sparse linear systems arising from computational engineering and scientific modeling.\n\nThe problem statement is scientifically grounded, well-posed, and objective. It provides a complete, deterministic, and formalizable description of the algorithm to be implemented. All constants, variables, and procedural steps are specified without ambiguity. Therefore, the problem is valid, and we shall proceed with its solution.\n\nThe core of the task is to construct an approximation $A \\approx L U$, where $A \\in \\mathbb{R}^{n \\times n}$ is a given sparse matrix, $L \\in \\mathbb{R}^{n \\times n}$ is a unit lower triangular matrix, and $U \\in \\mathbb{R}^{n \\times n}$ is an upper triangular matrix. The construction of $L$ and $U$ follows a row-wise procedure that mimics Gaussian elimination, but with specific rules for dropping small entries to preserve sparsity.\n\nThe algorithm proceeds for each row $i = 0, 1, \\dots, n-1$.\n\n**1. Initialization for Row $i$**\n\nFor each row $i$, we first determine the primary dropping tolerance, which is scaled by the infinity norm of the original matrix row $a_i^\\top$. Let this norm be $\\eta_i = \\lVert a_i \\rVert_\\infty = \\max_j |a_{ij}|$. The primary dropping threshold for row $i$ is thus $\\delta_i = \\tau_1 \\eta_i$.\n\nWe initialize a temporary sparse vector, the working row $w$, with the contents of row $i$ of matrix $A$: $w \\leftarrow a_i^\\top$. In practice, this means creating a data structure containing pairs $(j, a_{ij})$ for all $j$ where $a_{ij} \\neq 0$.\n\n**2. Gaussian Elimination with Primary Dropping**\n\nThe core of the factorization for row $i$ involves eliminating the sub-diagonal entries. This is achieved by iterating through columns $j = 0, 1, \\dots, i-1$. An elimination step is performed for each column $j$ where the working row has a non-zero entry, $w_j \\neq 0$.\n\nFor each such $j$, in increasing order:\n- The multiplier, which will become the entry $L_{ij}$, is computed as $l_{ij} = w_j / U_{jj}$. Here, $U_{jj}$ is the diagonal element of the previously computed row $j$ of $U$.\n- **Primary Dropping of $L$ entries**: The first dropping rule is applied. If $|l_{ij}| \\le \\delta_i$, the multiplier is considered negligible. It is discarded, and the corresponding update to the working row $w$ is skipped.\n- **Update of Working Row $w$**: If $l_{ij}$ is kept, it is stored as an entry in the matrix $L$. The working row $w$ is then updated by subtracting the scaled $j$-th row of $U$: $w \\leftarrow w - l_{ij} u_j^\\top$. This operation, $w_k \\leftarrow w_k - l_{ij} U_{jk}$ for $k  j$, introduces new non-zero entries (fill-in) into $w$.\n- **Primary Dropping of Fill-in**: The second dropping rule is applied immediately after each scalar update. For each $k$ where $w_k$ is modified, if the new value satisfies $|w_k| \\le \\delta_i$, it is dropped from the working row (i.e., set to zero). This aggressively controls the growth of non-zeros.\n\nAn essential detail is the preservation of the diagonal element of $U$. The problem states that $U_{ii}$ must never be dropped. While the primary dropping rule for $w_k$ could theoretically target $w_i$, the final construction of the $U$-row must respect this constraint. Our implementation ensures that after the elimination loop, the value $w_i$ is explicitly assigned to $U_{ii}$. If its value has become zero (or negligibly small), it is replaced by a small positive perturbation $\\epsilon$ (e.g., $\\epsilon=10^{-15}$) to prevent a breakdown of subsequent factorization steps or the preconditioning process.\n\n**3. Formation of Provisional Rows and Secondary Dropping**\n\nAfter the elimination loop for row $i$ is complete, the working row $w$ contains the entries for row $i$ of $U$, and the computed multipliers form row $i$ of $L$.\n- The provisional $i$-th row of $L$ consists of the kept multipliers $l_{ij}$ for $ji$, and $L_{ii}=1$.\n- The provisional $i$-th row of $U$ consists of the remaining entries $w_k$ for $k \\ge i$.\n\nNext, the secondary dropping rules are applied to these provisional rows, based on the secondary threshold $\\tau_2$.\n- For the $L$-row, we find the largest magnitude among its off-diagonal entries, $r_L^{(i)} = \\max_{ji} |L_{ij}|$. Any off-diagonal entry $L_{ij}$ satisfying $|L_{ij}|  \\tau_2 \\, r_L^{(i)}$ is then dropped. If there are no off-diagonal entries, $r_L^{(i)}=0$ and no entries are dropped.\n- A similar rule applies to the $U$-row. We find $r_U^{(i)} = \\max_{ki} |U_{ik}|$, and any off-diagonal entry $U_{ik}$ satisfying $|U_{ik}|  \\tau_2 \\, r_U^{(i)}$ is dropped. Diagonal entries $L_{ii}$ and $U_{ii}$ are always preserved.\n\n**4. Finalization and Metrics**\n\nThe process is repeated for all rows. The resulting sparse matrices $L$ and $U$ are the final factors. To evaluate the quality of the factorization, we compute two metrics:\n- The factorization defect in the Frobenius norm: $\\Delta_F = \\lVert A - LU \\rVert_F$. This measures the error of the approximation.\n- The total number of stored non-zeros: $N_{\\text{nz}} = \\text{nnz}(L) + \\text{nnz}(U)$. This measures the memory cost of the factorization.\n\nThe implementation will utilize sparse matrix data structures from the `scipy.sparse` library, such as `lil_matrix` for efficient row-wise construction and `csr_matrix` for fast arithmetic operations.",
            "answer": "```python\nimport numpy as np\nfrom scipy import sparse\n\ndef ilut_dual_threshold(A, tau1, tau2):\n    \"\"\"\n    Computes an Incomplete LU factorization with dual thresholding (ILUT).\n\n    Args:\n        A (scipy.sparse.csr_matrix): The input square sparse matrix.\n        tau1 (float): The primary drop tolerance relative to row norm.\n        tau2 (float): The secondary drop tolerance relative to max entry in row.\n\n    Returns:\n        (scipy.sparse.csr_matrix, scipy.sparse.csr_matrix): The L and U factors.\n    \"\"\"\n    n = A.shape[0]\n    # LIL format is efficient for incremental sparse matrix construction.\n    L = sparse.lil_matrix((n, n), dtype=np.float64)\n    U = sparse.lil_matrix((n, n), dtype=np.float64)\n\n    for i in range(n):\n        # 1. Initialization for row i\n        row_slice = A.getrow(i)\n        row_norm_inf = np.max(np.abs(row_slice.data)) if row_slice.nnz  0 else 0.0\n        \n        # working row w represented as a dictionary\n        w = {j: val for j, val in zip(row_slice.indices, row_slice.data)}\n        \n        primary_drop_tol = tau1 * row_norm_inf\n\n        # 2. Gaussian Elimination with Primary Dropping\n        elim_indices = sorted([k for k in w.keys() if k  i])\n\n        for j in elim_indices:\n            u_jj = U[j, j]\n            if abs(u_jj)  1e-16:  # Should not happen with test cases\n                continue\n\n            # Multiplier calculation\n            multiplier = w.pop(j) / u_jj  # Use and remove w[j]\n\n            # Primary drop rule for L\n            if abs(multiplier) = primary_drop_tol:\n                continue\n\n            L[i, j] = multiplier\n\n            # Update working row w by subtracting a scaled row of U\n            # U.getrow(j) is efficient for lil_matrix\n            u_row = U.getrow(j)\n            for k, u_jk in zip(u_row.indices, u_row.data):\n                if k = j:\n                    continue\n                \n                fill_in_val = multiplier * u_jk\n                w_k_old = w.get(k, 0.0)\n                w_k_new = w_k_old - fill_in_val\n\n                # Primary drop rule for fill-in\n                if abs(w_k_new) = primary_drop_tol:\n                    w.pop(k, None)\n                else:\n                    w[k] = w_k_new\n        \n        # 3. Form Provisional L and U Rows\n        L[i, i] = 1.0\n        \n        # U_ii is never dropped. Perturb if it is zero.\n        u_ii_val = w.get(i, 0.0)\n        if abs(u_ii_val)  1e-15:\n            u_ii_val = np.sign(u_ii_val) * 1e-15 if u_ii_val != 0 else 1e-15\n        \n        U[i, i] = u_ii_val\n        \n        for k, v in w.items():\n            if k  i:\n                U[i, k] = v\n\n        # 4. Secondary thresholding\n        if tau2  0:\n            # L part\n            l_row_data = L.data[i]\n            l_row_indices = L.rows[i]\n            l_off_diag_items = {idx: val for idx, val in zip(l_row_indices, l_row_data) if idx  i}\n            if l_off_diag_items:\n                r_L_i = max(abs(v) for v in l_off_diag_items.values())\n                drop_thresh_L = tau2 * r_L_i\n                \n                # Keep only strong entries\n                new_l_indices = [i]\n                new_l_data = [1.0]\n                for idx, val in l_off_diag_items.items():\n                    if abs(val) = drop_thresh_L:\n                        new_l_indices.append(idx)\n                        new_l_data.append(val)\n                L.rows[i] = new_l_indices\n                L.data[i] = new_l_data\n\n            # U part\n            u_row_data = U.data[i]\n            u_row_indices = U.rows[i]\n            u_off_diag_items = {idx: val for idx, val in zip(u_row_indices, u_row_data) if idx  i}\n            if u_off_diag_items:\n                r_U_i = max(abs(v) for v in u_off_diag_items.values())\n                drop_thresh_U = tau2 * r_U_i\n                \n                new_u_indices = [i]\n                new_u_data = [U[i,i]]\n                for idx, val in u_off_diag_items.items():\n                    if abs(val) = drop_thresh_U:\n                        new_u_indices.append(idx)\n                        new_u_data.append(val)\n                U.rows[i] = new_u_indices\n                U.data[i] = new_u_data\n                \n    return L.tocsr(), U.tocsr()\n\ndef run_test_case(matrix_builder, params, tau1, tau2):\n    \"\"\"\n    Constructs a matrix, runs ILUT, and computes metrics.\n    \"\"\"\n    A = matrix_builder(*params)\n    L, U = ilut_dual_threshold(A.tocsr(), tau1, tau2)\n    \n    # Calculate factorization defect\n    Diff = A - (L @ U)\n    delta_F = np.linalg.norm(Diff.toarray(), 'fro')\n    \n    # Calculate non-zeros\n    N_nz = L.nnz + U.nnz\n    \n    return [round(delta_F, 10), N_nz]\n\ndef solve():\n    \"\"\"\n    Defines and runs all test cases, then prints the formatted result.\n    \"\"\"\n\n    def build_test_1_matrix(n):\n        diagonals = [[-1] * (n - 1), [2] * n, [-1] * (n - 1)]\n        return sparse.diags(diagonals, [-1, 0, 1], shape=(n, n), format='csr')\n\n    def build_test_3_matrix(n):\n        diagonals = [\n            [-0.2] * (n - 2),\n            [-1] * (n - 1),\n            [6] * n,\n            [-1] * (n - 1),\n            [-0.2] * (n - 2)\n        ]\n        return sparse.diags(diagonals, [-2, -1, 0, 1, 2], shape=(n, n), format='csr')\n    \n    def build_test_4_matrix(n_block):\n        block1 = build_test_1_matrix(n_block)\n        block2 = 0.1 * block1\n        return sparse.block_diag((block1, block2), format='csr')\n\n    test_cases = [\n        {\"builder\": build_test_1_matrix, \"params\": (30,), \"tau1\": 1e-3, \"tau2\": 0.0},\n        {\"builder\": build_test_1_matrix, \"params\": (30,), \"tau1\": 1e-3, \"tau2\": 1e-2},\n        {\"builder\": build_test_3_matrix, \"params\": (25,), \"tau1\": 1e-4, \"tau2\": 1e-3},\n        {\"builder\": build_test_4_matrix, \"params\": (12,), \"tau1\": 5e-4, \"tau2\": 2e-2}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(case[\"builder\"], case[\"params\"], case[\"tau1\"], case[\"tau2\"])\n        results.append(result)\n    \n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}