{
    "hands_on_practices": [
        {
            "introduction": "Incomplete LU factorizations are powerful, but not foolproof, as the process can fail if a zero pivot is encountered. This exercise provides a concrete, hands-on opportunity to see exactly how this breakdown can happen by constructing a specific matrix where the ILU(0) factorization fails . Working through this fundamental calculation will solidify your understanding of the mechanics of ILU and the importance of matrix properties for its stability.",
            "id": "2401113",
            "problem": "Consider the parameterized symmetric tridiagonal matrix\n$$\nA(\\alpha)=\\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2+\\alpha & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix},\n$$\nwhere $\\alpha \\in \\mathbb{R}$. Apply the incomplete lower-upper factorization with zero fill, denoted $\\operatorname{ILU}(0)$, with the following conventions: no pivoting is used, the lower factor $L$ is unit lower triangular (diagonal entries equal to $1$) with the same sparsity pattern as the strictly lower triangular part of $A(\\alpha)$, and the upper factor $U$ is upper triangular with the same sparsity pattern as the upper triangular part of $A(\\alpha)$ (including the diagonal). Use the natural ordering of rows and columns.\n\nDetermine the value of $\\alpha$ for which the second diagonal entry of the incomplete upper factor $U$ produced by $\\operatorname{ILU}(0)$ equals zero. Provide your answer as an exact reduced fraction. Do not round.",
            "solution": "The problem is first validated for scientific and logical consistency.\n\n**Step 1: Extract Givens**\n- The matrix is $A(\\alpha)=\\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2+\\alpha & -1 \\\\ 0 & -1 & 2 \\end{pmatrix}$.\n- The parameter $\\alpha$ is a real number, $\\alpha \\in \\mathbb{R}$.\n- The factorization is the incomplete lower-upper factorization with zero fill-in, denoted $\\operatorname{ILU}(0)$.\n- The factorization conventions are: no pivoting, the lower factor $L$ is unit lower triangular (diagonal entries are $1$), and the upper factor $U$ is upper triangular.\n- The sparsity patterns of $L$ and $U$ are constrained: $L$ takes the sparsity of the strictly lower triangular part of $A(\\alpha)$, and $U$ takes the sparsity of the upper triangular part of $A(\\alpha)$ (including the diagonal).\n- The goal is to find the value of $\\alpha$ such that the second diagonal entry of $U$, $u_{22}$, is equal to $0$.\n- The answer must be an exact reduced fraction.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, being a standard exercise in numerical linear algebra concerning preconditioners for iterative methods. It is well-posed, with a clearly defined matrix, a standard algorithm ($\\operatorname{ILU}(0)$), and a precise objective. The language is objective and unambiguous. The problem is self-contained and free from contradictions or missing information. The calculation is feasible and leads to a verifiable result.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A solution will be computed.\n\nThe problem requires the determination of the parameter $\\alpha$ for which the $\\operatorname{ILU}(0)$ factorization of the matrix $A(\\alpha)$ results in a zero value for the second diagonal element of the upper triangular factor $U$.\n\nThe given matrix is:\n$$\nA(\\alpha) = \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2+\\alpha & -1 \\\\ 0 & -1 & 2 \\end{pmatrix}\n$$\nThe $\\operatorname{ILU}(0)$ factorization requires that the factors $L$ and $U$ have the same sparsity pattern as the strictly lower and upper-or-diagonal parts of $A(\\alpha)$, respectively. The matrix $L$ must be unit lower triangular (i.e., $l_{ii} = 1$ for all $i$), and $U$ is upper triangular.\n\nGiven the structure of $A(\\alpha)$, where the non-zero off-diagonal entries are $a_{12}$, $a_{21}$, $a_{23}$, and $a_{32}$, the factors $L$ and $U$ must have the following form to respect the zero fill-in constraint:\n$$\nL = \\begin{pmatrix} 1 & 0 & 0 \\\\ l_{21} & 1 & 0 \\\\ 0 & l_{32} & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} u_{11} & u_{12} & 0 \\\\ 0 & u_{22} & u_{23} \\\\ 0 & 0 & u_{33} \\end{pmatrix}\n$$\nThe entries $l_{31}$ and $u_{13}$ are zero because the corresponding entries $a_{31}$ and $a_{13}$ in the matrix $A(\\alpha)$ are zero.\n\nThe factorization process computes the unknown entries of $L$ and $U$ such that the product $LU$ matches $A(\\alpha)$ on its defined sparsity pattern. The product $LU$ is:\n$$\nLU = \\begin{pmatrix} 1 & 0 & 0 \\\\ l_{21} & 1 & 0 \\\\ 0 & l_{32} & 1 \\end{pmatrix} \\begin{pmatrix} u_{11} & u_{12} & 0 \\\\ 0 & u_{22} & u_{23} \\\\ 0 & 0 & u_{33} \\end{pmatrix} = \\begin{pmatrix} u_{11} & u_{12} & 0 \\\\ l_{21}u_{11} & l_{21}u_{12}+u_{22} & u_{23} \\\\ 0 & l_{32}u_{22} & l_{32}u_{23}+u_{33} \\end{pmatrix}\n$$\nWe equate the elements of $LU$ to the corresponding elements of $A(\\alpha)$ following the natural row-wise ordering.\n\nFor the first row ($i=1$):\nThe entries of the first row of $U$ are taken directly from the first row of $A(\\alpha)$.\n$$\nu_{11} = a_{11} = 2\n$$\n$$\nu_{12} = a_{12} = -1\n$$\n\nFor the second row ($i=2$):\nWe first compute the sub-diagonal element of $L$.\n$$\nl_{21}u_{11} = a_{21} \\implies l_{21}(2) = -1 \\implies l_{21} = -\\frac{1}{2}\n$$\nNext, we compute the diagonal element of $U$. This is the term of interest.\n$$\nl_{21}u_{12} + u_{22} = a_{22} \\implies u_{22} = a_{22} - l_{21}u_{12}\n$$\nSubstituting the known values:\n$$\nu_{22} = (2+\\alpha) - \\left(-\\frac{1}{2}\\right)(-1) = 2+\\alpha - \\frac{1}{2} = \\frac{3}{2} + \\alpha\n$$\nThe super-diagonal element of $U$ is:\n$$\nu_{23} = a_{23} = -1\n$$\n\nThe problem requires us to find the value of $\\alpha$ for which the second diagonal entry of $U$ is zero.\n$$\nu_{22} = 0\n$$\nUsing our derived expression for $u_{22}$:\n$$\n\\frac{3}{2} + \\alpha = 0\n$$\nSolving this simple linear equation for $\\alpha$:\n$$\n\\alpha = -\\frac{3}{2}\n$$\nThis is the value of $\\alpha$ for which the $\\operatorname{ILU}(0)$ algorithm would encounter a zero pivot on the second step. The calculation for the remaining entries ($l_{32}$, $u_{33}$) would fail due to division by $u_{22}=0$. The problem, however, only asks for the value of $\\alpha$ that causes this condition, not for the full factorization.\n\nThe resulting value is an exact reduced fraction as required.",
            "answer": "$$\n\\boxed{-\\frac{3}{2}}\n$$"
        },
        {
            "introduction": "While the previous exercise highlighted a potential failure, the ILU(0) preconditioner is nonetheless robust and highly effective for certain important classes of matrices. This practice challenges you to think critically about these theoretical properties, such as the conditions that guarantee the factorization's existence or the special cases where it becomes an exact factorization of the original matrix . Analyzing these scenarios will build a more nuanced intuition for when ILU(0) is an appropriate and effective preconditioning strategy.",
            "id": "2401064",
            "problem": "Consider a sparse matrix $A\\in\\mathbb{R}^{n\\times n}$ that is strictly diagonally dominant by rows, meaning $|a_{ii}|>\\sum_{j\\neq i}|a_{ij}|$ for all $i$. The Incomplete Lower-Upper (ILU) factorization with zero fill, denoted $\\mathrm{ILU}(0)$, computes lower and upper triangular factors $L$ and $U$ such that $A\\approx LU$ while disallowing any fill-in beyond the sparsity pattern of the strictly lower and strictly upper parts of $A$; no pivoting is used. Suppose $M:=LU$ is used as a left preconditioner for the linear system $Ax=b$ in an iterative solver. Which of the following statements are true?\n\nA. If $A$ is symmetric positive definite (SPD) and tridiagonal (for example, from a standard $1$D Poisson discretization with Dirichlet boundary conditions under natural ordering), then $\\mathrm{ILU}(0)$ yields the exact $LU$ factors of $A$, so $M^{-1}A=I$.\n\nB. For any strictly diagonally dominant $A$, $\\mathrm{ILU}(0)$ guarantees that the spectrum of the preconditioned matrix $M^{-1}A$ consists only of the value $1$.\n\nC. For any strictly diagonally dominant $A$, Gaussian elimination without pivoting does not break down (all pivots are nonzero), so an $\\mathrm{ILU}(0)$ factorization exists without encountering zero pivots.\n\nD. If $A$ is strictly diagonally dominant, then $\\mathrm{ILU}(0)$ always reduces the spectral condition number of $A$ by at least a factor of $10$.\n\nE. For the $2$D $5$-point Laplacian on a rectangular grid with natural row-wise ordering, $\\mathrm{ILU}(0)$ equals the exact $LU$ factorization.\n\nF. The $\\mathrm{ILU}(0)$ factors $L$ and $U$ preserve the original sparsity pattern of the strictly lower and strictly upper parts of $A$ (plus the diagonal), so in general they require less storage than the exact $LU$ factors, which typically contain additional fill-in.",
            "solution": "We analyze each option from fundamental definitions and elimination properties.\n\nFirst, strict diagonal dominance by rows, $|a_{ii}|>\\sum_{j\\neq i}|a_{ij}|$, implies by the Levy–Desplanques theorem that $A$ is nonsingular. The Incomplete Lower-Upper (ILU) factorization with zero fill, $\\mathrm{ILU}(0)$, is defined by performing the same algebraic steps as Gaussian elimination but discarding any fill entries outside the original lower and upper sparsity patterns of $A$; thus the computed $L$ and $U$ satisfy $A-LU=R$, where $R$ contains only those entries that would have been fill-in in the exact $LU$ but were dropped.\n\nOption A: If $A$ is symmetric positive definite (SPD) and tridiagonal (such as the standard $1$D Poisson stiffness matrix under natural ordering), the exact $LU$ factorization of $A$ produces no fill beyond the existing sub- and super-diagonals. This follows from the structure of elimination on a tridiagonal matrix: eliminating a variable affects only its immediate neighbors, so the Schur complement remains tridiagonal at each step. Consequently, the exact $LU$ factors have the same sparsity as the strictly lower and strictly upper parts of $A$. Because $\\mathrm{ILU}(0)$ disallows any fill beyond that pattern, it coincides with the exact $LU$. Therefore $M=LU=A$, and left preconditioning yields $M^{-1}A=I$. Thus A is Correct.\n\nOption B: For a general strictly diagonally dominant $A$, $\\mathrm{ILU}(0)$ is not exact in general: the exact $LU$ of a sparse matrix typically contains fill-in outside the original sparsity pattern unless the graph of $A$ is a chordal graph compatible with the elimination ordering (which is not guaranteed by strict diagonal dominance). Because $\\mathrm{ILU}(0)$ drops such fill, $M^{-1}A\\neq I$ in general, and its eigenvalues are not all equal to $1$. Strict diagonal dominance improves the viability of elimination but does not imply that the preconditioned operator is the identity. Thus B is Incorrect.\n\nOption C: We show that strict diagonal dominance prevents pivot breakdown (zero pivots) in Gaussian elimination without pivoting. At the first step, $|a_{11}|>\\sum_{j\\neq 1}|a_{1j}|$ implies $a_{11}\\neq 0$, so the first pivot is nonzero. The elimination multipliers satisfy $|l_{i1}|=|a_{i1}/a_{11}|<1$ for $i>1$ because $|a_{11}|>\\sum_{j\\neq 1}|a_{1j}|\\ge |a_{i1}|$. Forming the Schur complement updates the trailing submatrix entries by $a_{ij}^{(1)}=a_{ij}-l_{i1}a_{1j}$. One can show (by induction on the elimination steps and using triangle inequalities) that strict diagonal dominance by rows is preserved or at least that the next pivot remains nonzero. In particular, for the $k$-th step, the pivot $a_{kk}^{(k-1)}$ remains nonzero because the update cannot cancel the strictly dominant diagonal term. Hence Gaussian elimination without pivoting proceeds without encountering zero pivots, and therefore an $\\mathrm{ILU}(0)$ factorization (which uses the same pivots but drops fills) exists without breakdown. Thus C is Correct.\n\nOption D: The spectral condition number $\\kappa_2(A)$ is not guaranteed to be reduced by any fixed factor by an approximate preconditioner. While $\\mathrm{ILU}(0)$ often improves clustering of eigenvalues and reduces iteration counts, there is no general lower bound such as a factor of $10$ for strictly diagonally dominant matrices. In some cases, the improvement can be modest; in pathological orderings, it may even degrade certain spectral properties. Therefore the blanket claim is unsupported. Thus D is Incorrect.\n\nOption E: The $2$D $5$-point Laplacian on a rectangular grid with natural row-wise ordering produces a sparse matrix whose exact $LU$ factorization introduces fill-in beyond the original $5$-point stencil; eliminating a grid point couples its neighbors, creating new nonzeros (graphically, elimination creates edges that complete cycles). Because $\\mathrm{ILU}(0)$ drops such fill, it does not match the exact $LU$ for this case. Therefore E is Incorrect.\n\nOption F: By definition, $\\mathrm{ILU}(0)$ restricts $L$ and $U$ to the sparsity pattern of the strictly lower and strictly upper parts of $A$ (with unit or specified diagonal in $L$ and the diagonal in $U$), so no new nonzeros are introduced. In contrast, the exact $LU$ typically contains additional fill-in entries, increasing storage relative to $A$. Therefore, in general, $\\mathrm{ILU}(0)$ requires less storage than exact $LU$. Thus F is Correct.\n\nIn summary, the true statements are A, C, and F.",
            "answer": "$$\\boxed{ACF}$$"
        },
        {
            "introduction": "The practical utility of a preconditioner depends critically on its computational cost, both in memory usage and floating-point operations. This advanced practice guides you through a first-principles analysis of the more general ILU($p$) preconditioner as applied to a common large-scale problem, the 3D Laplacian . By deriving expressions for how memory and application cost scale with the fill-level $p$, you will develop essential skills in the asymptotic analysis of numerical algorithms.",
            "id": "2401089",
            "problem": "Consider the linear system arising from the standard second-order finite-difference discretization of the three-dimensional Laplacian on a cubic grid with $n$ interior points along each axis, so there are $N = n^{3}$ unknowns. Let the unknowns be ordered lexicographically. We precondition the system using pointwise Incomplete Lower-Upper factorization of level $p$ (Incomplete LU (ILU)$(p)$) defined by the graph-distance model: the sparsity pattern retained in the factors is the set of entries whose associated grid points lie within graph distance at most $p$ from the diagonal entry’s grid point in the three-dimensional grid graph (Manhattan distance in $\\mathbb{Z}^{3}$), with the strictly lower-triangular part containing only predecessors in the ordering and the strictly upper-triangular part containing only successors. Assume interior nodes dominate so that boundary effects are negligible, and that the diagonal is stored once (shared by the lower and upper factors).\n\nDefine the memory usage as the total number of stored scalar nonzeros across both factors and the diagonal. Define the computational complexity as the total number of floating-point multiply-add operations required to apply the preconditioner once (one forward substitution with the lower factor followed by one backward substitution with the upper factor), counting one multiply-add per strictly off-diagonal nonzero encountered.\n\nStarting from first principles under these assumptions, derive closed-form expressions (to leading order in $n$ and exactly in $p$) for:\n- the total memory usage as a function of $n$ and $p$, and\n- the total application cost as a function of $n$ and $p$.\n\nFinally, in the limit $n \\to \\infty$ with fixed $p$, express the ratio\n$$R(p) = \\lim_{n \\to \\infty} \\frac{\\text{application cost}}{\\text{memory usage}}$$\nas a single closed-form function of $p$ only. Provide $R(p)$ as your final answer. No units are required for the final answer.",
            "solution": "The problem as stated is subjected to rigorous validation.\n\n**Step 1: Extracted Givens**\n- **System**: Linear system from a standard second-order finite-difference discretization of the three-dimensional Laplacian.\n- **Grid**: A cubic grid with $n$ interior points along each axis.\n- **Unknowns**: Total number of unknowns is $N = n^{3}$.\n- **Ordering**: The unknowns are ordered lexicographically.\n- **Preconditioner**: Pointwise Incomplete Lower-Upper factorization of level $p$, denoted ILU($p$).\n- **Sparsity Pattern**: The pattern is defined by a graph-distance model. Retained entries $(i,j)$ in the factors $L$ and $U$ correspond to grid points where the graph distance (Manhattan distance in $\\mathbb{Z}^{3}$) between the grid points associated with row $i$ and column $j$ is at most $p$. The strictly lower-triangular part contains only predecessors in the ordering, and the strictly upper-triangular part contains only successors.\n- **Assumptions**: Interior nodes dominate, so boundary effects are considered negligible. The diagonal of the preconditioner is stored only once.\n- **Definitions**:\n    - **Memory Usage**: The total number of stored scalar nonzeros across both factors and the diagonal.\n    - **Computational Complexity (Application Cost)**: The total number of floating-point multiply-add operations required for one forward substitution followed by one backward substitution. This is counted as one multiply-add per strictly off-diagonal nonzero encountered during the substitutions.\n- **Objective**:\n    1. Derive a closed-form expression for the total memory usage as a function of $n$ and $p$.\n    2. Derive a closed-form expression for the total application cost as a function of $n$ and $p$.\n    3. Express the ratio $R(p) = \\lim_{n \\to \\infty} \\frac{\\text{application cost}}{\\text{memory usage}}$ as a function of $p$ only.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is a standard theoretical problem in the field of numerical analysis for partial differential equations, specifically concerning preconditioning techniques for large sparse linear systems. The definitions are precise, and the assumptions (e.g., negligible boundary effects) are standard for asymptotic analysis. The problem does not violate any scientific principles, is not based on false premises, is formalizable, and contains sufficient information for a unique solution under the given assumptions.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete, reasoned solution will be provided.\n\n**Derivation of Solution**\nThe analysis hinges on determining the number of nonzeros per row of the preconditioner for a typical interior node, as boundary effects are negligible. The total number of unknowns is $N = n^3$.\n\nFirst, we must determine the number of entries in the sparsity pattern for a single row. This is defined by the graph-distance model. For a grid point, the corresponding row in the ILU factors will have nonzeros for all other grid points within a Manhattan distance of $p$. Let us calculate the number of integer points $(x, y, z) \\in \\mathbb{Z}^3$ such that their Manhattan distance from the origin, $|x| + |y| + |z|$, is less than or equal to $p$. This count gives the number of nonzeros, $N_p$, for a row corresponding to an interior node in the combined pattern of the $L$ and $U$ factors.\n\nThe number of points with Manhattan distance exactly $d > 0$ from the origin in a $3D$ grid is given by the formula $4d^2 + 2$. For $d=0$, there is only one point, the origin itself.\nThe total number of points $N_p$ within a distance $p$ is the sum of points at distances $d = 0, 1, \\dots, p$:\n$$N_p = 1 + \\sum_{d=1}^{p} (4d^2 + 2)$$\nWe evaluate the sum:\n$$N_p = 1 + 4\\sum_{d=1}^{p} d^2 + 2\\sum_{d=1}^{p} 1$$\nUsing the standard formulas for sums of powers, $\\sum_{d=1}^{p} 1 = p$ and $\\sum_{d=1}^{p} d^2 = \\frac{p(p+1)(2p+1)}{6}$, we have:\n$$N_p = 1 + 2p + 4 \\frac{p(p+1)(2p+1)}{6} = 1 + 2p + \\frac{2}{3} p(2p^2 + 3p + 1)$$\n$$N_p = 1 + 2p + \\frac{4}{3}p^3 + 2p^2 + \\frac{2}{3}p = \\frac{4}{3}p^3 + 2p^2 + \\frac{8}{3}p + 1$$\nTo write this with a common denominator of $3$:\n$$N_p = \\frac{4p^3 + 6p^2 + 8p + 3}{3}$$\nThis expression, $N_p$, represents the total number of nonzeros (diagonal and off-diagonal) in the sparsity pattern for a single row of the preconditioner, assuming an interior node.\n\n**Memory Usage**\nThe memory usage is defined as the total number of stored scalar nonzeros. Based on the problem statement that the diagonal is stored once and the sparsity pattern is determined by the union of $L$ and $U$ factors, the number of stored elements per row is precisely $N_p$. Since there are $N = n^3$ rows and we are ignoring boundary effects, the total memory usage, $\\text{Mem}(n,p)$, is:\n$$\\text{Mem}(n,p) = N \\times N_p = n^3 \\left( \\frac{4p^3 + 6p^2 + 8p + 3}{3} \\right)$$\nThis is the expression for memory usage, to leading order in $n$ and exact in $p$.\n\n**Application Cost**\nThe application cost is the number of multiply-add operations for one forward and one backward substitution. The problem specifies this is one operation per strictly off-diagonal nonzero.\n- Forward substitution with $L$ involves, for each row $i$, a number of operations equal to the number of nonzeros in the strictly lower part of row $i$ of $L$.\n- Backward substitution with $U$ involves, for each row $i$, a number of operations equal to the number of nonzeros in the strictly upper part of row $i$ of $U$.\n\nThe total cost for row $i$ is the number of its strictly off-diagonal nonzeros in the preconditioner. For an interior node, the total number of nonzeros is $N_p$. One of these is the diagonal entry. Thus, the number of strictly off-diagonal nonzeros is $N_p - 1$.\nThe total application cost, $\\text{Cost}(n,p)$, is the cost per row multiplied by the number of rows $N = n^3$:\n$$\\text{Cost}(n,p) = N \\times (N_p - 1) = n^3 \\left( \\left( \\frac{4p^3 + 6p^2 + 8p + 3}{3} \\right) - 1 \\right)$$\n$$\\text{Cost}(n,p) = n^3 \\left( \\frac{4p^3 + 6p^2 + 8p + 3 - 3}{3} \\right) = n^3 \\left( \\frac{4p^3 + 6p^2 + 8p}{3} \\right)$$\nThis is the expression for application cost, to leading order in $n$ and exact in $p$.\n\n**Ratio R(p)**\nFinally, we compute the ratio $R(p)$ in the limit as $n \\to \\infty$:\n$$R(p) = \\lim_{n \\to \\infty} \\frac{\\text{Cost}(n,p)}{\\text{Mem}(n,p)} = \\lim_{n \\to \\infty} \\frac{n^3 \\left( \\frac{4p^3 + 6p^2 + 8p}{3} \\right)}{n^3 \\left( \\frac{4p^3 + 6p^2 + 8p + 3}{3} \\right)}$$\nThe term $n^3$ and the denominator $3$ cancel, and since the expression is independent of $n$, the limit is the expression itself:\n$$R(p) = \\frac{4p^3 + 6p^2 + 8p}{4p^3 + 6p^2 + 8p + 3}$$\nThis is the final closed-form function of $p$.",
            "answer": "$$\n\\boxed{\\frac{4p^3 + 6p^2 + 8p}{4p^3 + 6p^2 + 8p + 3}}\n$$"
        }
    ]
}