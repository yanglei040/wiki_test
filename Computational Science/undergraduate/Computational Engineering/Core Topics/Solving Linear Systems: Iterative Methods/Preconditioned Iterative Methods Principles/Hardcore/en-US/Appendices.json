{
    "hands_on_practices": [
        {
            "introduction": "The Preconditioned Conjugate Gradient (PCG) method is a cornerstone for solving large symmetric positive definite (SPD) linear systems. Its mathematical derivation, however, relies on the preconditioner itself being SPD. This hands-on calculation reveals why this is not merely a theoretical guideline but a critical requirement for the algorithm to function . By exploring a carefully constructed counterexample, you will pinpoint the exact step where the method breaks down when a non-SPD preconditioner is used.",
            "id": "2427468",
            "problem": "Consider the linear system $A \\mathbf{x} = \\mathbf{b}$ with the symmetric positive definite (SPD) matrix\n$$\nA = \\begin{pmatrix}\n2 & 0 \\\\\n0 & 1\n\\end{pmatrix},\n$$\nthe right-hand side\n$$\n\\mathbf{b} = \\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix},\n$$\nand the initial guess $\\mathbf{x}_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. Let the preconditioner be the symmetric but non-SPD matrix\n$$\nM = \\begin{pmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{pmatrix}.\n$$\nDefine the initial residual $\\mathbf{r}_{0} = \\mathbf{b} - A \\mathbf{x}_{0}$ and the preconditioned residual $\\mathbf{z}_{0}$ as the unique solution of $M \\mathbf{z}_{0} = \\mathbf{r}_{0}$. Consider the scalar\n$$\ns = \\mathbf{r}_{0}^{\\top} \\mathbf{z}_{0}.\n$$\nCompute $s$ exactly. Express your final answer as a single real number. No rounding is required.",
            "solution": "The matrix $A$ is symmetric and positive definite because it is diagonal with positive diagonal entries $2$ and $1$. The matrix $M$ is symmetric but not positive definite because its eigenvalues are $1$ and $-1$, so it is indefinite.\n\nBy definition, the initial residual is\n$$\n\\mathbf{r}_{0} = \\mathbf{b} - A \\mathbf{x}_{0}.\n$$\nWith $\\mathbf{x}_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, we obtain\n$$\n\\mathbf{r}_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nThe preconditioned residual $\\mathbf{z}_{0}$ is defined by solving\n$$\nM \\mathbf{z}_{0} = \\mathbf{r}_{0}.\n$$\nSince $M = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}$ is invertible with $M^{-1} = M$, we have\n$$\n\\mathbf{z}_{0} = M^{-1} \\mathbf{r}_{0} = M \\mathbf{r}_{0} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n$$\nNow compute the scalar\n$$\ns = \\mathbf{r}_{0}^{\\top} \\mathbf{z}_{0} = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 1 \\cdot 1 + 1 \\cdot (-1) = 0.\n$$\nThis value directly shows a failure mode for the Preconditioned Conjugate Gradient (PCG) method: in the standard left-preconditioned formulation, the step size coefficient $\\alpha_{0}$ uses the numerator $(\\mathbf{r}_{0}^{\\top} \\mathbf{z}_{0})$, and the recurrence for the search direction uses the ratio $(\\mathbf{r}_{k+1}^{\\top} \\mathbf{z}_{k+1}) / (\\mathbf{r}_{k}^{\\top} \\mathbf{z}_{k})$. With $s = \\mathbf{r}_{0}^{\\top} \\mathbf{z}_{0} = 0$ and $\\mathbf{r}_{0} \\neq \\mathbf{0}$, the method encounters a breakdown at the first iteration when using the non-SPD preconditioner $M$.\n\nTherefore, the required scalar is $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Effective preconditioning often involves exploiting the structure of the underlying problem. This coding exercise challenges you to solve the classic 2D Poisson equation, a frequent task in computational engineering, by implementing and comparing several preconditioning strategies . You will directly contrast the performance of a basic point-Jacobi preconditioner with a more sophisticated line block-Jacobi preconditioner, providing concrete insight into the trade-off between preconditioner complexity and solver efficiency.",
            "id": "2427470",
            "problem": "Construct a program that, for a family of linear systems arising from the standard five-point finite difference discretization of the two-dimensional Poisson equation with homogeneous Dirichlet boundary conditions on a square grid of interior size $n \\times n$, builds and applies a block-Jacobi preconditioner whose blocks correspond to contiguous lines of nodes. The system matrix is the symmetric positive definite matrix $A \\in \\mathbb{R}^{N \\times N}$ with $N = n^2$ defined by\n$$\nA = I_n \\otimes T_n + T_n \\otimes I_n,\n$$\nwhere $T_n \\in \\mathbb{R}^{n \\times n}$ is the tridiagonal matrix with main diagonal entries equal to $2$ and sub- and super-diagonal entries equal to $-1$, and $\\otimes$ denotes the Kronecker product. The unknown vector is ordered in lexicographic row-major order. Let the right-hand side be $b = \\mathbf{1} \\in \\mathbb{R}^{N}$ and the initial guess be $x_0 = \\mathbf{0} \\in \\mathbb{R}^{N}$.\n\nDefine the block-Jacobi preconditioner $M \\in \\mathbb{R}^{N \\times N}$ by taking the block-diagonal of $A$ that corresponds to either:\n- lines parallel to the $x$-axis (row-wise blocks), or\n- lines parallel to the $y$-axis (column-wise blocks),\nso that each block is a tridiagonal matrix $B \\in \\mathbb{R}^{n \\times n}$ equal to $T_n + 2 I_n$, applied independently to each line. Also consider the point-Jacobi preconditioner $M_{\\mathrm{pt}} = \\mathrm{diag}(A)$.\n\nFor each parameter set in the test suite below, solve the linear system $A x = b$ using the Conjugate Gradient method (defined for symmetric positive definite systems) under three configurations: no preconditioning, point-Jacobi preconditioning, and the line block-Jacobi preconditioning with the specified orientation. In all cases, terminate the iteration at the smallest nonnegative integer $k$ such that the residual norm satisfies\n$$\n\\|r_k\\|_2 \\le \\varepsilon \\, \\|r_0\\|_2,\n$$\nwhere $r_k = b - A x_k$, with tolerance $\\varepsilon = 10^{-8}$, measured in the Euclidean norm, and constrain the iteration count to a maximum of $K_{\\max} = 5000$ steps. For each parameter set, report four values: the iteration counts $k_{\\mathrm{none}}$, $k_{\\mathrm{pt}}$, $k_{\\mathrm{blk}}$, and the ratio $k_{\\mathrm{none}} / k_{\\mathrm{blk}}$ expressed as a real number.\n\nTest suite (each case is a pair $(n, \\mathrm{orientation})$, where $\\mathrm{orientation} \\in \\{\\text{row}, \\text{col}\\}$ selects the line-block direction):\n- Case $1$: $(n, \\mathrm{orientation}) = (1, \\text{row})$.\n- Case $2$: $(n, \\mathrm{orientation}) = (8, \\text{row})$.\n- Case $3$: $(n, \\mathrm{orientation}) = (8, \\text{col})$.\n- Case $4$: $(n, \\mathrm{orientation}) = (32, \\text{row})$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces. Each test case result must itself be a list in the form $[k_{\\mathrm{none}},k_{\\mathrm{pt}},k_{\\mathrm{blk}},k_{\\mathrm{none}}/k_{\\mathrm{blk}}]$. The overall output format is thus\n$$\n\\bigl[ [k_{\\mathrm{none}},k_{\\mathrm{pt}},k_{\\mathrm{blk}},k_{\\mathrm{none}}/k_{\\mathrm{blk}}], \\ldots \\bigr],\n$$\nprinted on one line as, for example, $[[a,b,c,d],[e,f,g,h],\\ldots]$ with all entries numerical.",
            "solution": "The problem requires solving the linear system of equations $A x = b$, where the matrix $A \\in \\mathbb{R}^{N \\times N}$ with $N = n^2$ arises from the five-point finite difference discretization of the two-dimensional Poisson equation on a square grid. The matrix is given by the Kronecker product sum:\n$$\nA = I_n \\otimes T_n + T_n \\otimes I_n\n$$\nHere, $I_n$ is the $n \\times n$ identity matrix, and $T_n$ is the $n \\times n$ tridiagonal matrix with entries of $2$ on the main diagonal and $-1$ on the first sub-diagonal and super-diagonal. This construction assumes a lexicographic row-major ordering of the grid unknowns. The resulting matrix $A$ is symmetric and positive definite (SPD), which is a critical property for the choice of solver. The right-hand side is $b = \\mathbf{1}$ (a vector of all ones), and the initial guess is $x_0 = \\mathbf{0}$.\n\nThe chosen iterative solver is the Conjugate Gradient (CG) method, which is the standard and most efficient Krylov subspace method for SPD systems. We will implement the Preconditioned Conjugate Gradient (PCG) algorithm, which solves the system $M^{-1} A x = M^{-1} b$ where $M$ is the preconditioner. The algorithm is as follows for an initial guess $x_0$:\n\n1.  Initialize: $k=0$, $r_0 = b - A x_0$.\n2.  Set stopping criterion: $\\tau = \\varepsilon \\|r_0\\|_2$ with $\\varepsilon = 10^{-8}$.\n3.  Precondition: Solve $M z_0 = r_0$.\n4.  Set initial search direction: $p_0 = z_0$.\n5.  Compute $\\rho_0 = r_0^T z_0$.\n6.  Iterate for $k = 0, 1, 2, \\ldots, K_{\\max}-1$:\n    a. $q_k = A p_k$\n    b. $\\alpha_k = \\rho_k / (p_k^T q_k)$\n    c. $x_{k+1} = x_k + \\alpha_k p_k$\n    d. $r_{k+1} = r_k - \\alpha_k q_k$\n    e. If $\\|r_{k+1}\\|_2 \\le \\tau$, terminate and report $k+1$ iterations.\n    f. Solve $M z_{k+1} = r_{k+1}$.\n    g. $\\rho_{k+1} = r_{k+1}^T z_{k+1}$\n    h. $\\beta_k = \\rho_{k+1} / \\rho_k$\n    i. $p_{k+1} = z_{k+1} + \\beta_k p_k$\n    j. $\\rho_k \\leftarrow \\rho_{k+1}$\n\nWe will analyze three preconditioning strategies, defined by the choice of $M$.\n\n**1. No Preconditioning**\nThis case is equivalent to setting the preconditioner $M$ to the identity matrix, $M=I$. The preconditioning step \"Solve $M z_k = r_k$\" becomes trivial: $z_k = r_k$. The standard Conjugate Gradient algorithm is recovered. The convergence rate depends on the condition number of the original matrix $A$, $\\kappa(A)$, which for this problem is $O(n^2)$. This leads to slow convergence for large $n$.\n\n**2. Point-Jacobi Preconditioning**\nThe point-Jacobi preconditioner is the diagonal of the matrix $A$, so $M_{\\mathrm{pt}} = \\mathrm{diag}(A)$. The diagonal entries of $A = I_n \\otimes T_n + T_n \\otimes I_n$ are the sum of the diagonal entries of $I_n \\otimes T_n$ and $T_n \\otimes I_n$. The diagonal of $I_n \\otimes T_n$ is a vector of $n^2$ entries all equal to $2$. The diagonal of $T_n \\otimes I_n$ is also a vector of $n^2$ entries all equal to $2$. Therefore, the diagonal of $A$ is constant and all its entries are $2+2=4$. The preconditioner is $M_{\\mathrm{pt}} = 4I_N$. Applying its inverse, $M_{\\mathrm{pt}}^{-1}$, is a simple and computationally inexpensive operation: a scalar division of the vector by $4$.\n\n**3. Line Block-Jacobi Preconditioning**\nThis preconditioner, $M_{\\mathrm{blk}}$, is constructed by taking the block-diagonal of $A$, where blocks correspond to lines of nodes on the grid.\nThe matrix $A$, for a row-major ordering, is a block-tridiagonal matrix:\n$$\nA = \\begin{pmatrix}\nT_n+2I_n & -I_n & & \\\\\n-I_n & T_n+2I_n & -I_n & \\\\\n& \\ddots & \\ddots & \\ddots \\\\\n& & -I_n & T_n+2I_n\n\\end{pmatrix}\n$$\nThe diagonal blocks, denoted $B$, are $B = T_n + 2I_n$. This matrix is tridiagonal with $4$ on the diagonal and $-1$ on the off-diagonals. It is also SPD.\n\n-   **Row-wise orientation**: The blocks correspond to rows of the grid. With row-major ordering, unknowns on a single grid row are indexed contiguously in the solution vector. The block-Jacobi preconditioner is thus the block-diagonal of $A$, $M_{\\mathrm{blk}} = \\mathrm{diag}(B, B, ..., B)$. Applying $M_{\\mathrm{blk}}^{-1}$ to a residual vector $r$ requires solving $n$ independent linear systems of size $n \\times n$, i.e., $B z_i = r_i$ for each block $i=1, \\ldots, n$. Since $B$ is a banded matrix (tridiagonal), these systems can be solved efficiently.\n\n-   **Column-wise orientation**: The blocks correspond to columns of the grid. In a row-major ordered vector, the unknowns for a single grid column are not contiguous; they are separated by a stride of $n$. The submatrix of $A$ that couples the unknowns within a single column is also the matrix $B = T_n+2I_n$, due to the symmetry of the problem. Applying the preconditioner inverse involves: for each of the $n$ columns, (1) gathering the corresponding strided elements from the residual vector $r$, (2) solving the $n \\times n$ system with matrix $B$, and (3) scattering the results back into the solution vector $z$.\n\nThe preconditioned matrix for the column-wise case, $M_{\\mathrm{blk,col}}^{-1}A$, is similar to the one for the row-wise case, $M_{\\mathrm{blk,row}}^{-1}A$. Specifically, if $P$ is the permutation matrix that maps row-major ordering to column-major ordering, then $M_{\\mathrm{blk,col}} = P^T M_{\\mathrm{blk,row}} P$. Due to the symmetry of the operator and domain, it holds that $PAP^T = A$. Consequently, the eigenvalues of $M_{\\mathrm{blk,col}}^{-1}A$ are the same as the eigenvalues of $M_{\\mathrm{blk,row}}^{-1}A$. Since the convergence of the CG method depends on the eigenvalue distribution, we expect the number of iterations, $k_{\\mathrm{blk}}$, to be identical for both `row` and `col` orientations for a given $n$.\n\nThe implementation will construct the matrix $A$ and the necessary structures for the preconditioners. For the block-Jacobi preconditioner, the tridiagonal systems will be solved using a banded linear solver for efficiency. The PCG algorithm will be executed for each of the three configurations on each test case to determine the iteration counts $k_{\\mathrm{none}}$, $k_{\\mathrm{pt}}$, and $k_{\\mathrm{blk}}$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef create_A(n):\n    \"\"\"\n    Constructs the matrix A for the 2D Poisson problem on an n x n grid.\n    \"\"\"\n    if n == 0:\n        return np.array([[]])\n    if n == 1:\n        return np.array([[4.0]])\n        \n    diag_main = np.full(n, 2.0)\n    diag_off = np.full(n-1, -1.0)\n    T_n = np.diag(diag_main) + np.diag(diag_off, k=1) + np.diag(diag_off, k=-1)\n    \n    I_n = np.eye(n)\n    \n    A = np.kron(I_n, T_n) + np.kron(T_n, I_n)\n    return A\n\ndef apply_preconditioner_inv(r, n, precon_config):\n    \"\"\"\n    Applies the inverse of the preconditioner M to a vector r.\n    z = M_inv * r\n    precon_config is a tuple (type, orientation), e.g., ('block', 'row').\n    \"\"\"\n    precon_type, orientation = precon_config\n    \n    if precon_type == 'none':\n        return r.copy()\n    \n    if precon_type == 'point':\n        # M = diag(A), which is 4*I\n        return r / 4.0\n        \n    if precon_type == 'block':\n        N = n * n\n        if N == 0:\n            return np.array([])\n        z = np.zeros(N)\n        \n        # The block matrix B = T_n + 2*I_n is tridiagonal with\n        # 4 on the main diagonal and -1 on the off-diagonals.\n        # We use a banded solver for B*z_i = r_i.\n        # Banded matrix format for scipy.linalg.solve_banded:\n        # ab[0, :] are super-diagonal elements (u=1)\n        # ab[1, :] are main-diagonal elements\n        # ab[2, :] are sub-diagonal elements (l=1)\n        # Since u=1, first element of ab[0,:] is ignored.\n        # Since l=1, last element of ab[2,:] is ignored.\n        ab = np.zeros((3, n))\n        ab[0, 1:] = -1.0\n        ab[1, :] = 4.0\n        ab[2, :-1] = -1.0\n        \n        if orientation == 'row':\n            for i in range(n):\n                start, end = i * n, (i + 1) * n\n                r_i = r[start:end]\n                z_i = solve_banded((1, 1), ab, r_i)\n                z[start:end] = z_i\n        elif orientation == 'col':\n            for j in range(n):\n                r_j = r[j::n] # Gather elements for column j\n                z_j = solve_banded((1, 1), ab, r_j)\n                z[j::n] = z_j # Scatter back\n        return z\n        \n    raise ValueError(\"Unknown preconditioner type\")\n\ndef run_cg(n, precon_config):\n    \"\"\"\n    Runs the Preconditioned Conjugate Gradient method for a given n and preconditioner.\n    \"\"\"\n    N = n * n\n    if N == 0:\n        return 0\n        \n    A = create_A(n)\n    b = np.ones(N)\n    x = np.zeros(N)\n    \n    epsilon = 1e-8\n    max_iter = 5000\n    \n    r = b - A @ x\n    \n    norm_r0 = np.linalg.norm(r)\n    if norm_r0 == 0:\n        return 0\n    \n    threshold = epsilon * norm_r0\n    \n    z = apply_preconditioner_inv(r, n, precon_config)\n    p = z.copy()\n    rho_old = r @ z\n    \n    for k in range(max_iter):\n        Ap = A @ p\n        \n        # Handle potential breakdown for very small systems/singular cases\n        p_dot_Ap = p @ Ap\n        if p_dot_Ap = 0:\n            # CG requires SPD matrix. Negative or zero value indicates problem.\n            # Could happen with non-SPD preconditioner, but ours are SPD.\n            # More likely due to floating point error accumulation.\n            # Stop if direction is not a descent direction.\n            return k + 1\n\n        alpha = rho_old / p_dot_Ap\n        x += alpha * p\n        r -= alpha * Ap\n        \n        if np.linalg.norm(r) = threshold:\n            return k + 1\n            \n        z = apply_preconditioner_inv(r, n, precon_config)\n        rho_new = r @ z\n        \n        if rho_old == 0: # Should not happen if everything is correct\n            return k + 1\n\n        beta = rho_new / rho_old\n        p = z + beta * p\n        rho_old = rho_new\n        \n    return max_iter\n\ndef solve():\n    test_cases = [\n        (1, 'row'),\n        (8, 'row'),\n        (8, 'col'),\n        (32, 'row'),\n    ]\n    \n    results = []\n    for n, orientation in test_cases:\n        k_none = run_cg(n, ('none', None))\n        k_pt = run_cg(n, ('point', None))\n        k_blk = run_cg(n, ('block', orientation))\n        \n        # Handle division by zero if k_blk is 0\n        ratio = float(k_none) / k_blk if k_blk != 0 else float('inf')\n        \n        results.append(f\"[{k_none},{k_pt},{k_blk},{ratio}]\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While PCG is the method of choice for SPD systems, many real-world problems yield non-symmetric matrices, requiring different families of solvers like the Biconjugate Gradient (BiCG) method. These methods, however, can suffer from breakdowns not seen in PCG. This exercise uses a simple $3 \\times 3$ system to demonstrate a classic failure mode of BiCG and explains how the more robust Biconjugate Gradient Stabilized (BiCGSTAB) method is designed to avoid such pitfalls .",
            "id": "2427438",
            "problem": "Consider the linear system $A x = b$ with the $3 \\times 3$ matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n1  1  0\\\\\n0  1  1\\\\\n0  0  1\n\\end{pmatrix},\n$$\nthe right-hand side $b = \\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}$, and the initial guess $x_0 = \\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}$. Work in exact arithmetic and assume left preconditioning with the identity preconditioner $M = I$. Let the initial residual be $r_0 = b - A x_0$ and choose the shadow residual for the biconjugate gradient (BiCG) method as $r_0^{\\ast} = \\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}$. Initialize the BiCG search directions by $p_0 = r_0$ and $p_0^{\\ast} = r_0^{\\ast}$. \n\nUsing only the standard definitions of the biconjugate gradient (BiCG) method and the biconjugate gradient stabilized (BiCGSTAB) method, do the following:\n\n- Compute the BiCG first-iteration denominator $d_0 = p_0^{\\ast\\,T} A p_0$ and show that the BiCG iteration breaks down at the first step for this example.\n- Explain briefly, in terms of the defining operations of BiCGSTAB, why with the common choice $\\hat{r} = r_0$ the corresponding denominator at this step is nonzero for the same $A$, $b$, and $x_0$, so the BiCGSTAB iteration does not encounter this specific breakdown at the first step.\n\nAnswer specification:\n- Provide as your final answer only the exact value of $d_0$.\n- No rounding is required.\n- No units are involved.",
            "solution": "The problem asks to analyze the first iteration of the biconjugate gradient (BiCG) and biconjugate gradient stabilized (BiCGSTAB) methods for a given linear system $A x = b$.\n\nThe given data are:\nThe matrix $A$ is\n$$\nA = \\begin{pmatrix}\n1  1  0\\\\\n0  1  1\\\\\n0  0  1\n\\end{pmatrix}\n$$\nThe right-hand side vector $b$ is\n$$\nb = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe initial guess $x_0$ is the zero vector\n$$\nx_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe preconditioner is the identity matrix, $M=I$, which means we are considering the unpreconditioned versions of the algorithms.\n\nFirst, we address the BiCG method. The initial residual $r_0$ is computed as:\n$$\nr_0 = b - A x_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1  1  0\\\\ 0  1  1\\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe problem specifies the initial shadow residual $r_0^{\\ast}$ as:\n$$\nr_0^{\\ast} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThe initial search directions are set to $p_0 = r_0$ and $p_0^{\\ast} = r_0^{\\ast}$. Thus,\n$$\np_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\quad \\text{and} \\quad p_0^{\\ast} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThe first task is to compute the denominator $d_0 = p_0^{\\ast\\,T} A p_0$ for the BiCG update step. First, we compute the product $A p_0$:\n$$\nA p_0 = \\begin{pmatrix} 1  1  0\\\\ 0  1  1\\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 1 \\cdot 0 + 0 \\cdot 0 \\\\ 0 \\cdot 1 + 1 \\cdot 0 + 1 \\cdot 0 \\\\ 0 \\cdot 1 + 0 \\cdot 0 + 1 \\cdot 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nNow, we can compute $d_0$:\n$$\nd_0 = p_0^{\\ast\\,T} (A p_0) = \\begin{pmatrix} 0  1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0 \\cdot 1 + 1 \\cdot 0 + 0 \\cdot 0 = 0\n$$\nThe step size $\\alpha_k$ in the BiCG algorithm is calculated as $\\alpha_k = \\frac{r_k^{\\ast\\,T} r_k}{p_k^{\\ast\\,T} A p_k}$. For the first iteration, $k=0$, the step size is $\\alpha_0 = \\frac{r_0^{\\ast\\,T} r_0}{p_0^{\\ast\\,T} A p_0}$. The denominator of this expression is precisely $d_0$. Since $d_0 = 0$, the calculation of $\\alpha_0$ involves division by zero. This is a \"serious breakdown\" of the BiCG algorithm, which cannot proceed. This breakdown occurs because the chosen shadow search direction $p_0^\\ast$ is A-orthogonal to the search direction $p_0$, i.e. $p_0^{\\ast\\,T} A p_0 = 0$.\n\nNext, we analyze the first step of the BiCGSTAB method. For BiCGSTAB, there is no shadow residual sequence. Instead, the algorithm employs a \"stabilizing\" step involving an additional multiplication by $A$. The initial step size, denoted $\\alpha_0$, is computed as:\n$$\n\\alpha_0 = \\frac{\\rho_0}{\\hat{r}^T v_0}\n$$\nwhere $\\rho_0 = \\hat{r}^T r_0$, $v_0 = A p_0$, and $p_0 = r_0$. The vector $\\hat{r}$ is an arbitrary vector, but the common and standard choice is $\\hat{r} = r_0$. The problem asks us to consider this choice.\nWith $\\hat{r} = r_0$, the denominator for $\\alpha_0$ becomes $r_0^T v_0 = r_0^T A p_0$. Since $p_0$ is also initialized as $r_0$, the denominator is $r_0^T A r_0$.\n\nLet us compute this value for the given problem. We have $r_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ and, as previously calculated, $A r_0 = A p_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe denominator is:\n$$\nr_0^T A r_0 = \\begin{pmatrix} 1  0  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1 \\cdot 1 + 0 \\cdot 0 + 0 \\cdot 0 = 1\n$$\nThis denominator is non-zero. The numerator is $\\rho_0 = r_0^T r_0 = 1$, so the step size $\\alpha_0$ is well-defined and equals $1$. The BiCGSTAB iteration can proceed without encountering the breakdown that stopped the BiCG method.\n\nIn summary, the breakdown of BiCG is due to the specific, and somewhat artificial, choice of the initial shadow residual $r_0^{\\ast}$, which makes the shadow search direction $p_0^{\\ast}$ orthogonal to $A p_0$. BiCGSTAB avoids this specific breakdown mechanism by eliminating the need for a shadow residual sequence. Its corresponding denominator, $r_0^T A r_0$, relies only on the standard residual $r_0$ and the system matrix $A$. For a non-singular matrix $A$ and a non-zero residual $r_0$, this quantity $r_0^T A r_0$ is not guaranteed to be non-zero for a general non-symmetric matrix, but it is non-zero in this case, preventing a breakdown at this step.",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}