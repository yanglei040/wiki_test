{
    "hands_on_practices": [
        {
            "introduction": "The Preconditioned Conjugate Gradient (PCG) method is a powerful tool, but its mathematical derivation relies on specific properties of both the system matrix and the preconditioner. This practice  illustrates a critical requirement: the preconditioner $M$ must be symmetric positive definite (SPD). Through a direct calculation with a simple $2 \\times 2$ system, you will discover how using an indefinite preconditioner leads to a fatal breakdown in the algorithm, providing a clear and memorable lesson on the foundations of PCG.",
            "id": "2427468",
            "problem": "Consider the linear system $A \\mathbf{x} = \\mathbf{b}$ with the symmetric positive definite (SPD) matrix\n$$\nA = \\begin{pmatrix}\n2 & 0 \\\\\n0 & 1\n\\end{pmatrix},\n$$\nthe right-hand side\n$$\n\\mathbf{b} = \\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix},\n$$\nand the initial guess $\\mathbf{x}_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. Let the preconditioner be the symmetric but non-SPD matrix\n$$\nM = \\begin{pmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{pmatrix}.\n$$\nDefine the initial residual $\\mathbf{r}_{0} = \\mathbf{b} - A \\mathbf{x}_{0}$ and the preconditioned residual $\\mathbf{z}_{0}$ as the unique solution of $M \\mathbf{z}_{0} = \\mathbf{r}_{0}$. Consider the scalar\n$$\ns = \\mathbf{r}_{0}^{\\top} \\mathbf{z}_{0}.\n$$\nCompute $s$ exactly. Express your final answer as a single real number. No rounding is required.",
            "solution": "The matrix $A$ is symmetric and positive definite because it is diagonal with positive diagonal entries $2$ and $1$. The matrix $M$ is symmetric but not positive definite because its eigenvalues are $1$ and $-1$, so it is indefinite.\n\nBy definition, the initial residual is\n$$\n\\mathbf{r}_{0} = \\mathbf{b} - A \\mathbf{x}_{0}.\n$$\nWith $\\mathbf{x}_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, we obtain\n$$\n\\mathbf{r}_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nThe preconditioned residual $\\mathbf{z}_{0}$ is defined by solving\n$$\nM \\mathbf{z}_{0} = \\mathbf{r}_{0}.\n$$\nSince $M = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}$ is invertible with $M^{-1} = M$, we have\n$$\n\\mathbf{z}_{0} = M^{-1} \\mathbf{r}_{0} = M \\mathbf{r}_{0} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n$$\nNow compute the scalar\n$$\ns = \\mathbf{r}_{0}^{\\top} \\mathbf{z}_{0} = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 1 \\cdot 1 + 1 \\cdot (-1) = 0.\n$$\nThis value directly shows a failure mode for the Preconditioned Conjugate Gradient (PCG) method: in the standard left-preconditioned formulation, the step size coefficient $\\alpha_{0}$ uses the numerator $(\\mathbf{r}_{0}^{\\top} \\mathbf{z}_{0})$, and the recurrence for the search direction uses the ratio $(\\mathbf{r}_{k+1}^{\\top} \\mathbf{z}_{k+1}) / (\\mathbf{r}_{k}^{\\top} \\mathbf{z}_{k})$. With $s = \\mathbf{r}_{0}^{\\top} \\mathbf{z}_{0} = 0$ and $\\mathbf{r}_{0} \\neq \\mathbf{0}$, the method encounters a breakdown at the first iteration when using the non-SPD preconditioner $M$.\n\nTherefore, the required scalar is $0$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "The goal of preconditioning is to transform a problem into one that is easier to solve, but what makes a preconditioner 'good' or 'bad'? This exercise  challenges you to think counter-intuitively by designing a preconditioner that intentionally worsens the system's condition number. By exploring the spectral properties of this 'perverse' choice, you will gain a deeper understanding of the core principle that an effective preconditioner $M$ must approximate the original matrix $A$.",
            "id": "2427465",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be symmetric positive definite (SPD) with eigen-decomposition $A = Q \\Lambda Q^{\\top}$, where $Q$ is orthogonal and $\\Lambda = \\mathrm{diag}(\\lambda_{1}, \\dots, \\lambda_{n})$ with $0 < \\lambda_{1} \\le \\dots \\le \\lambda_{n}$. Consider left preconditioning of the linear system $A x = b$ with an SPD preconditioner $M$, leading to the preconditioned operator $M^{-1} A$. Let $\\kappa_{2}(X)$ denote the spectral condition number in the matrix $2$-norm, that is, $\\kappa_{2}(X) = \\sigma_{\\max}(X) / \\sigma_{\\min}(X)$, where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values of $X$, respectively.\n\nWhich option correctly describes a choice of preconditioner $M$ that deliberately makes the condition number $\\kappa_{2}(M^{-1} A)$ worse than $\\kappa_{2}(A)$, and correctly explains what this implies about the eigenspaces of $M$ and $A$?\n\nA. Choose $M = A^{-1}$. Then $M$ is SPD, $\\kappa_{2}(M^{-1} A) = \\kappa_{2}(A)^{2} > \\kappa_{2}(A)$ whenever $\\kappa_{2}(A) > 1$, and $M$ shares eigenvectors with $A$ so that $M^{-1} A$ has the same eigenvectors as $A$.\n\nB. Choose $M = \\alpha A$ with a scalar $\\alpha > 0$. Then $\\kappa_{2}(M^{-1} A)$ increases by a factor of $\\alpha$, and the eigenspaces of $M$ and $A$ become orthogonal.\n\nC. Choose $M$ whose eigenvectors are unrelated to those of $A$ so that $M^{-1} A$ is non-normal. This guarantees $\\kappa_{2}(M^{-1} A) > \\kappa_{2}(A)$ because misalignment always worsens conditioning.\n\nD. Choose $M$ with the same eigenvectors as $A$ but with eigenvalues $\\mu_{i} = c \\, \\lambda_{n+1-i}^{-1}$ for some $c > 0$. Then $\\kappa_{2}(M^{-1} A) = \\kappa_{2}(A)$, and $M^{-1} A$ shares eigenvectors with $A$.",
            "solution": "The problem statement is first validated for correctness and coherence.\n\n**Step 1: Extract Givens**\n- $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite (SPD) matrix.\n- The eigen-decomposition of $A$ is $A = Q \\Lambda Q^{\\top}$.\n- $Q$ is an orthogonal matrix, so $Q^{\\top} Q = Q Q^{\\top} = I$.\n- $\\Lambda = \\mathrm{diag}(\\lambda_{1}, \\dots, \\lambda_{n})$ with $0 < \\lambda_{1} \\le \\dots \\le \\lambda_{n}$. The columns of $Q$ are the eigenvectors of $A$ corresponding to the eigenvalues $\\lambda_i$.\n- We consider the linear system $A x = b$.\n- Left preconditioning is applied with an SPD preconditioner $M$.\n- The preconditioned operator is $M^{-1} A$.\n- The spectral condition number is defined as $\\kappa_{2}(X) = \\sigma_{\\max}(X) / \\sigma_{\\min}(X)$, where $\\sigma_{\\max}(X)$ and $\\sigma_{\\min}(X)$ are the largest and smallest singular values of a matrix $X$.\n- The task is to identify a choice of preconditioner $M$ that intentionally increases the condition number, i.e., $\\kappa_{2}(M^{-1} A) > \\kappa_{2}(A)$, and to correctly describe the associated properties of the eigenspaces of $M$ and $A$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is set within the standard mathematical framework of numerical linear algebra. All terms, such as SPD matrix, eigen-decomposition, condition number, and preconditioning, are well-defined and standard. The premises are factually sound and scientifically grounded. The problem is self-contained, objective, and well-posed. It asks for the evaluation of specific claims, which is a verifiable task.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. We proceed with the solution and analysis of each option.\n\n**Derivation and Analysis**\n\nFirst, let's establish the condition number of the original matrix $A$. Since $A$ is SPD, its eigenvalues are positive, real, and are also its singular values. The eigenvalues of $A$ are $\\lambda_1, \\dots, \\lambda_n$.\nThe largest singular value is $\\sigma_{\\max}(A) = \\lambda_{\\max}(A) = \\lambda_{n}$.\nThe smallest singular value is $\\sigma_{\\min}(A) = \\lambda_{\\min}(A) = \\lambda_{1}$.\nTherefore, the condition number of $A$ is:\n$$ \\kappa_{2}(A) = \\frac{\\lambda_{n}}{\\lambda_{1}} $$\nThe goal of preconditioning is typically to find an SPD matrix $M$ such that $M \\approx A$ and the system $M z = r$ is easy to solve. The preconditioned matrix $M^{-1}A$ should have a condition number smaller than $\\kappa_{2}(A)$, ideally close to $1$. The question asks for a \"perverse\" choice of $M$ that worsens the condition number.\n\nWe will now analyze each option.\n\n**Option A:**\n- **Choice of preconditioner:** $M = A^{-1}$.\n- **Properties of $M$:** Since $A$ is SPD, its inverse $A^{-1}$ is also SPD. The eigenvalues of $A^{-1}$ are $1/\\lambda_i$, which are all positive since $\\lambda_i > 0$. Also, $(A^{-1})^{\\top} = (A^{\\top})^{-1} = A^{-1}$, so $A^{-1}$ is symmetric. Thus, $M=A^{-1}$ is a valid SPD preconditioner.\n- **Eigenspace Relationship:** The eigen-decomposition of $A$ is $A = Q \\Lambda Q^{\\top}$. The inverse is $A^{-1} = (Q \\Lambda Q^{\\top})^{-1} = Q \\Lambda^{-1} Q^{\\top}$. This shows that $M = A^{-1}$ has the same eigenvectors as $A$ (the columns of $Q$).\n- **Preconditioned Operator:** The new operator is $M^{-1} A = (A^{-1})^{-1} A = A A = A^{2}$.\n- **Eigenvectors of $M^{-1}A$:** The eigenvectors of $A^2$ are the same as the eigenvectors of $A$. If $A v_i = \\lambda_i v_i$, then $A^2 v_i = A(A v_i) = A(\\lambda_i v_i) = \\lambda_i (A v_i) = \\lambda_i (\\lambda_i v_i) = \\lambda_i^2 v_i$. So, the statement that \"$M^{-1} A$ has the same eigenvectors as $A$\" is correct.\n- **Condition Number of $M^{-1}A$:** The eigenvalues of $M^{-1}A = A^2$ are $\\lambda_1^2, \\dots, \\lambda_n^2$. Since these are positive, they are also the singular values. The largest is $\\lambda_n^2$ and the smallest is $\\lambda_1^2$.\n$$ \\kappa_{2}(M^{-1} A) = \\kappa_{2}(A^2) = \\frac{\\lambda_{\\max}(A^2)}{\\lambda_{\\min}(A^2)} = \\frac{\\lambda_n^2}{\\lambda_1^2} = \\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^2 = (\\kappa_{2}(A))^2 $$\n- **Comparison:** The claim is that $\\kappa_{2}(M^{-1} A) = (\\kappa_{2}(A))^2 > \\kappa_{2}(A)$ whenever $\\kappa_{2}(A) > 1$. This is a correct algebraic statement. If $A$ is not a multiple of the identity matrix, then $\\lambda_n > \\lambda_1$, which implies $\\kappa_{2}(A) > 1$. In this case, choosing $M = A^{-1}$ indeed worsens the condition number.\n\n**Conclusion for A:** All statements within this option are mathematically correct. It describes a valid choice of $M$ that degrades the condition number and accurately portrays the relationships between the eigenspaces. This option is **Correct**.\n\n**Option B:**\n- **Choice of preconditioner:** $M = \\alpha A$ with a scalar $\\alpha > 0$.\n- **Preconditioned Operator:** $M^{-1} A = (\\alpha A)^{-1} A = \\frac{1}{\\alpha} A^{-1} A = \\frac{1}{\\alpha} I$, where $I$ is the identity matrix.\n- **Condition Number:** The operator is a scalar multiple of the identity matrix. Its singular values are all equal to $1/\\alpha$.\n$$ \\kappa_{2}(M^{-1} A) = \\kappa_{2}(\\frac{1}{\\alpha} I) = \\frac{1/\\alpha}{1/\\alpha} = 1 $$\nThis choice of $M$ does not increase the condition number; it improves it to the optimal value of $1$. The statement that $\\kappa_{2}(M^{-1} A)$ \"increases by a factor of $\\alpha$\" is false.\n- **Eigenspace Relationship:** The eigenvectors of $M = \\alpha A$ are identical to the eigenvectors of $A$. The statement that their eigenspaces \"become orthogonal\" is nonsensical and false.\n\n**Conclusion for B:** This option is incorrect on two fundamental points: the effect on the condition number and the relationship between the eigenspaces. This option is **Incorrect**.\n\n**Option C:**\n- **Choice of preconditioner:** $M$ is chosen such that its eigenvectors are \"unrelated\" to those of $A$, meaning $M$ and $A$ do not commute ($MA \\neq AM$).\n- **Claim:** The option claims that this non-commutation (or \"misalignment\") \"guarantees $\\kappa_{2}(M^{-1} A) > \\kappa_{2}(A)$\". This is a very strong claim.\n- **Analysis:** The entire purpose of effective preconditioning (e.g., Incomplete LU, multigrid, domain decomposition) is to construct an $M$ that does *not* commute with $A$ but for which $M^{-1}A$ has a much better condition number than $A$. For example, an ideal preconditioner is $M=A$, for which $M^{-1}A = I$ and $\\kappa_2(I)=1$. A good practical preconditioner $M$ approximates $A$ in some sense, so $M^{-1}A \\approx I$. The eigenvectors of such an $M$ will generally be different from those of $A$. Therefore, the central premise that \"misalignment always worsens conditioning\" is fundamentally false.\n\n**Conclusion for C:** The core assertion of this option is contrary to the foundational principles of preconditioning. This option is **Incorrect**.\n\n**Option D:**\n- **Choice of preconditioner:** $M$ has the same eigenvectors as $A$ (columns of $Q$), but its eigenvalues are $\\mu_{i} = c \\, \\lambda_{n+1-i}^{-1}$ for some constant $c > 0$.\n- **Properties of $M$**: Since $c>0$ and all $\\lambda_j>0$, all $\\mu_i$ are positive. As $M$ is symmetric (by virtue of sharing an orthonormal eigenbasis with symmetric $A$) with positive eigenvalues, it is SPD.\n- **Preconditioned Operator:** Since $M$ and $A$ commute, the eigenvalues of $M^{-1}A$ are $\\nu_i = \\lambda_i / \\mu_i$.\n$$ \\nu_i = \\frac{\\lambda_i}{\\mu_i} = \\frac{\\lambda_i}{c \\, \\lambda_{n+1-i}^{-1}} = \\frac{1}{c} \\lambda_i \\lambda_{n+1-i} $$\n- **Condition Number:** The condition number is $\\kappa_{2}(M^{-1}A) = \\frac{\\max_i \\nu_i}{\\min_i \\nu_i} = \\frac{\\max_i (\\lambda_i \\lambda_{n+1-i})}{\\min_i (\\lambda_i \\lambda_{n+1-i})}$. The claim is that this equals $\\kappa_2(A) = \\lambda_n/\\lambda_1$. This is not generally true. For many eigenvalue distributions, the products $\\lambda_i \\lambda_{n+1-i}$ are more clustered than the original $\\lambda_i$. For instance, if $\\lambda = [1, 2, 100]$, then $\\kappa_2(A)=100$. The products are $\\lambda_1\\lambda_3 = 100$ and $\\lambda_2^2=4$. The condition number of $M^{-1}A$ would be $100/4 = 25$, which is an improvement. This type of preconditioner is designed to *improve* conditioning, not preserve it.\n- **Eigenspace Relationship:** The statement \"$M^{-1} A$ shares eigenvectors with $A$\" is correct because $M$ and $A$ share eigenvectors.\n\n**Conclusion for D:** The claim about the condition number is false. This option is **Incorrect**.\n\nBased on the detailed analysis, only option A provides a fully correct description of a scenario where preconditioning deliberately worsens the condition number.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Theory becomes practice in this comprehensive exercise, where you will tackle a classic problem from computational physics: the Laplace equation. You will implement the Preconditioned Conjugate Gradient (PCG) method from first principles, using the Symmetric Successive Over-Relaxation (SSOR) method as a preconditioner . By comparing the iteration counts of the preconditioned and unpreconditioned solvers, you will directly observe the dramatic efficiency gains that a well-chosen preconditioner can provide for large-scale scientific computations.",
            "id": "2444004",
            "problem": "You are to write a complete, runnable program that investigates how Symmetric Successive Over-Relaxation (SSOR) behaves and how it functions as a preconditioner for other solvers when applied to Laplace-type equations. Begin from the fundamental mathematical model and derive all required discretizations and iterative methods within your solution. Do not use any pre-supplied formulas; instead, develop the necessary expressions from definitions and well-tested facts.\n\nConsider the two-dimensional Laplace-type problem with Dirichlet boundary conditions on the unit square. The continuous model is based on the Laplace operator defined by the divergence of the gradient. The discretization must be performed on a uniform grid of interior points. The result is a linear system with a real, symmetric positive definite matrix.\n\nYour program must:\n- Assemble the linear system by discretizing the continuous model on a uniform grid of size $N \\times N$ interior points using only fundamental finite-difference principles.\n- Use a nontrivial right-hand side constructed from a smooth function that has zero values on the boundary of the unit square to ensure consistency with homogeneous Dirichlet conditions.\n- Implement the Conjugate Gradient method (CG) for symmetric positive definite systems from first principles, stopping when the relative $2$-norm of the residual is less than a given tolerance.\n- Implement the Preconditioned Conjugate Gradient method (PCG) using the Symmetric Successive Over-Relaxation (SSOR) preconditioner. The SSOR preconditioner must be derived from the matrix splitting associated with Successive Over-Relaxation (SOR) and must be applied as an operator inside PCG rather than formed explicitly as a dense inverse. The relaxation parameter must be denoted by $\\omega$ and confined to the range $0 < \\omega < 2$.\n- For each test case, compute and record two integers: the iteration count for unpreconditioned CG and the iteration count for PCG with SSOR, both using the same stopping criterion based on the relative residual $2$-norm.\n\nFundamental base for derivations in your solution:\n- The Laplace operator is the divergence of the gradient. Its standard second-order, central-difference finite-difference discretization on a uniform grid yields a sparse linear system with a real, symmetric positive definite matrix when combined with homogeneous Dirichlet boundary conditions.\n- The Conjugate Gradient method for symmetric positive definite matrices is derived from minimizing the quadratic form associated with the system matrix along conjugate directions with respect to the matrix-induced inner product.\n- The Successive Over-Relaxation method is defined by a matrix splitting into diagonal, strictly lower, and strictly upper parts and a relaxation parameter. Symmetric Successive Over-Relaxation applies the forward and backward sweeps to define a symmetric positive definite preconditioner for $0 < \\omega < 2$.\n\nTest suite:\n- Use the following set of test cases, where each case is a tuple $(N, \\omega)$ with $N$ being the number of interior points in each direction and $\\omega$ the relaxation parameter for SSOR:\n    1. $(16, 1.0)$\n    2. $(16, 1.5)$\n    3. $(16, 1.9)$\n    4. $(8, 1.7)$\n\nAlgorithmic specifications:\n- Initial guess for all solvers must be the zero vector.\n- The right-hand side must be generated from a smooth function that is zero on the boundary of the unit square; construct it on the interior grid corresponding to each $N$ and assemble the vector corresponding to the discretized system.\n- The stopping criterion for both CG and PCG must be that the relative residual $2$-norm, defined as $\\|r_k\\|_2 / \\|r_0\\|_2$, is less than $10^{-8}$, where $r_k$ is the residual at iteration $k$ and $r_0$ is the initial residual. If this tolerance is not reached before $5000$ iterations, stop and return the number of iterations taken.\n\nOutput specification:\n- For each test case, output a list of two integers $[k_{\\mathrm{CG}}, k_{\\mathrm{PCG-SSOR}}]$, where $k_{\\mathrm{CG}}$ is the number of iterations for unpreconditioned Conjugate Gradient and $k_{\\mathrm{PCG-SSOR}}$ is the number of iterations for Preconditioned Conjugate Gradient with SSOR preconditioning using the given $\\omega$.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list of these two-integer lists, enclosed in square brackets. For example: $[[3,2],[4,2],[5,2],[3,2]]$.",
            "solution": "The problem presented is a well-posed and scientifically sound exercise in computational physics, requiring the implementation and comparison of numerical methods for solving a Laplace-type partial differential equation. It is grounded in fundamental principles of numerical analysis and is free of ambiguity or contradiction. We shall proceed with a formal derivation and solution.\n\nThe problem under consideration is the two-dimensional Laplace-type equation on the unit square domain $\\Omega = (0,1) \\times (0,1)$:\n$$\n-\\nabla^2 u(x,y) = f(x,y) \\quad \\text{for} \\quad (x,y) \\in \\Omega\n$$\nsubject to homogeneous Dirichlet boundary conditions, $u(x,y) = 0$ for $(x,y) \\in \\partial\\Omega$. The operator $\\nabla^2$ is the Laplacian, defined as the divergence of the gradient, $\\nabla^2 u = \\nabla \\cdot (\\nabla u) = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}$.\n\nFirst, we must discretize this continuous problem. We define a uniform grid with $N$ interior points in each direction. The mesh spacing is $h = \\frac{1}{N+1}$. The grid points are $(x_i, y_j) = (ih, jh)$ for $i,j = 0, 1, \\dots, N+1$. The unknown values of the function $u$ are approximated at the interior points, denoted by $u_{i,j} \\approx u(x_i, y_j)$ for $i,j = 1, \\dots, N$. The values on the boundary, for $i=0, N+1$ or $j=0, N+1$, are zero.\n\nWe employ the second-order central finite-difference approximation for the second derivatives:\n$$\n\\frac{\\partial^2 u}{\\partial x^2}\\bigg|_{(x_i, y_j)} \\approx \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2}\n$$\n$$\n\\frac{\\partial^2 u}{\\partial y^2}\\bigg|_{(x_i, y_j)} \\approx \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2}\n$$\nSubstituting these into the original equation yields the discrete equation at each interior grid point $(i,j)$:\n$$\n-\\left( \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} \\right) = f_{i,j}\n$$\nwhere $f_{i,j} = f(x_i, y_j)$. Rearranging this expression, we obtain the five-point stencil equation:\n$$\n4u_{i,j} - u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1} = h^2f_{i,j}\n$$\nThese $N^2$ linear equations for the $N^2$ unknowns $u_{i,j}$ form a linear system $A\\mathbf{u} = \\mathbf{b}$. Here, $\\mathbf{u}$ is a vector of the unknowns $u_{i,j}$ arranged in a specific order, typically lexicographical (row-wise or column-wise). The matrix $A$ is a real, sparse, block-tridiagonal matrix of size $N^2 \\times N^2$. It is well-known that for this problem, $A$ is symmetric and positive definite (SPD), properties which are crucial for the choice of iterative solver.\n\nTo ensure consistency with the homogeneous boundary conditions, we must construct a source function $f(x,y)$ such that the exact solution $u(x,y)$ is zero on the boundary. We can manufacture such a solution. Let us choose $u_{exact}(x,y) = \\sin(\\pi x)\\sin(\\pi y)$, which clearly satisfies $u=0$ on $\\partial\\Omega$. Then the corresponding source function is obtained by applying the negative Laplacian:\n$$\nf(x,y) = -\\nabla^2 u_{exact}(x,y) = -\\left( -\\pi^2\\sin(\\pi x)\\sin(\\pi y) - \\pi^2\\sin(\\pi x)\\sin(\\pi y) \\right) = 2\\pi^2\\sin(\\pi x)\\sin(\\pi y)\n$$\nThe right-hand side vector $\\mathbf{b}$ of our linear system then consists of entries $b_{k} = h^2 f(x_i, y_j)$, where the index $k$ corresponds to the grid point $(i,j)$.\n\nWith the linear system $A\\mathbf{u} = \\mathbf{b}$ established, we now turn to its solution.\n\nThe Conjugate Gradient (CG) method is an iterative algorithm designed for solving linear systems where the matrix $A$ is SPD. It is superior to steepest descent by choosing search directions that are $A$-orthogonal, effectively minimizing the $A$-norm of the error over an expanding Krylov subspace. The algorithm is as follows, starting with an initial guess $\\mathbf{x}_0 = \\mathbf{0}$:\n1. Initialize: $\\mathbf{r}_0 = \\mathbf{b} - A\\mathbf{x}_0 = \\mathbf{b}$, and the first search direction $\\mathbf{p}_0 = \\mathbf{r}_0$.\n2. For iteration $k = 0, 1, 2, \\dots$:\n   a. Compute step size: $\\alpha_k = \\frac{\\mathbf{r}_k^T \\mathbf{r}_k}{\\mathbf{p}_k^T A \\mathbf{p}_k}$.\n   b. Update solution: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$.\n   c. Update residual: $\\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k A \\mathbf{p}_k$.\n   d. Check for convergence, e.g., $\\|\\mathbf{r}_{k+1}\\|_2 < \\epsilon$.\n   e. Compute improvement for next search direction: $\\beta_k = \\frac{\\mathbf{r}_{k+1}^T \\mathbf{r}_{k+1}}{\\mathbf{r}_k^T \\mathbf{r}_k}$.\n   f. Update search direction: $\\mathbf{p}_{k+1} = \\mathbf{r}_{k+1} + \\beta_k \\mathbf{p}_k$.\n\nThe convergence rate of CG depends on the condition number $\\kappa(A)$. For ill-conditioned systems, convergence can be slow. Preconditioning is a technique to transform the system into one with more favorable spectral properties. We solve $M^{-1}A\\mathbf{x} = M^{-1}\\mathbf{b}$ where $M$ is the preconditioner, an SPD matrix that approximates $A$ and for which systems $M\\mathbf{z} = \\mathbf{r}$ are easy to solve.\n\nThe Preconditioned Conjugate Gradient (PCG) algorithm modifies CG to incorporate this preconditioning step.\n1. Initialize: $\\mathbf{x}_0 = \\mathbf{0}$, $\\mathbf{r}_0 = \\mathbf{b}$.\n2. Solve the preconditioning system: $M\\mathbf{z}_0 = \\mathbf{r}_0$.\n3. Set search direction: $\\mathbf{p}_0 = \\mathbf{z}_0$.\n4. For iteration $k = 0, 1, 2, \\dots$:\n   a. Compute step size: $\\alpha_k = \\frac{\\mathbf{r}_k^T \\mathbf{z}_k}{\\mathbf{p}_k^T A \\mathbf{p}_k}$.\n   b. Update solution: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$.\n   c. Update residual: $\\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k A \\mathbf{p}_k$.\n   d. Check for convergence.\n   e. Solve preconditioning system: $M\\mathbf{z}_{k+1} = \\mathbf{r}_{k+1}$.\n   f. Compute improvement: $\\beta_k = \\frac{\\mathbf{r}_{k+1}^T \\mathbf{z}_{k+1}}{\\mathbf{r}_k^T \\mathbf{z}_k}$.\n   g. Update search direction: $\\mathbf{p}_{k+1} = \\mathbf{z}_{k+1} + \\beta_k \\mathbf{p}_k$.\n\nWe are tasked with using the Symmetric Successive Over-Relaxation (SSOR) preconditioner. To derive its form, we start with the matrix splitting $A = D - L - U$, where $D$ is the diagonal of $A$, $-L$ is the strictly lower-triangular part, and $-U$ is the strictly upper-triangular part. For our symmetric matrix $A$, we have $U = L^T$. The SSOR stationary iteration consists of a forward SOR sweep followed by a backward SOR sweep. This structure gives rise to a symmetric preconditioner $M_{SSOR}$. The standard definition for the SSOR preconditioner is:\n$$\nM_{SSOR} = \\frac{1}{\\omega(2-\\omega)}(D - \\omega L) D^{-1} (D - \\omega U)\n$$\nFor our SPD matrix $A$ and a relaxation parameter $0 < \\omega < 2$, the preconditioner $M_{SSOR}$ is also SPD. The problem requires us to apply this preconditioner as an operator, i.e., to implement a function that computes $\\mathbf{z} = M_{SSOR}^{-1}\\mathbf{r}$ for a given vector $\\mathbf{r}$, without forming $M_{SSOR}$ explicitly.\nTo find the action of $M_{SSOR}^{-1}$, we solve the system $M_{SSOR}\\mathbf{z} = \\mathbf{r}$:\n$$\n\\frac{1}{\\omega(2-\\omega)}(D - \\omega L) D^{-1} (D - \\omega U)\\mathbf{z} = \\mathbf{r}\n$$\nLet us define an intermediate vector $\\mathbf{y} = D^{-1} (D - \\omega U)\\mathbf{z}$. The equation becomes $(D - \\omega L)\\mathbf{y} = \\omega(2-\\omega)\\mathbf{r}$. Since $(D - \\omega L)$ is a lower-triangular matrix, we can solve for $\\mathbf{y}$ using a forward substitution pass over the grid.\nHaving found $\\mathbf{y}$, we can find $\\mathbf{z}$ from the definition of $\\mathbf{y}$. We have $(D - \\omega U)\\mathbf{z} = D\\mathbf{y}$. Since $(D - \\omega U)$ is upper-triangular, we solve for $\\mathbf{z}$ using a backward substitution pass over the grid.\n\nThe procedure to compute $\\mathbf{z} = M_{SSOR}^{-1}\\mathbf{r}$ is therefore:\n1. First sweep (forward substitution): Solve $(D - \\omega L)\\mathbf{y} = \\omega(2-\\omega)\\mathbf{r}$ for $\\mathbf{y}$.\n2. Second sweep (backward substitution): Solve $(D - \\omega U)\\mathbf{z} = D\\mathbf{y}$ for $\\mathbf{z}$.\n\nFor our specific matrix $A$, the diagonal entries are all $4$, so $D = 4I$. The forward sweep equation at grid point $(i,j)$ is:\n$4y_{i,j} - \\omega(y_{i-1,j} + y_{i,j-1}) = \\omega(2-\\omega)r_{i,j}$.\nThe backward sweep equation is:\n$4z_{i,j} - \\omega(z_{i+1,j} + z_{i,j+1}) = 4y_{i,j}$.\nThese sweeps are computationally inexpensive and form the core of the preconditioning step in the PCG algorithm.\n\nNow, we can proceed to implement these algorithms and evaluate their performance on the specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef assemble_rhs(N, h):\n    \"\"\"\n    Assembles the right-hand side vector for the discretized Poisson equation\n    -div(grad(u)) = f on a unit square. The function f is derived from a\n    manufactured solution u_exact = sin(pi*x)sin(pi*y).\n    \"\"\"\n    coords = h * np.arange(1, N + 1)\n    X, Y = np.meshgrid(coords, coords, indexing='ij')\n    f_analytic = 2 * (np.pi**2) * np.sin(np.pi * X) * np.sin(np.pi * Y)\n    b = (h**2) * f_analytic\n    return b.flatten()\n\ndef matvec(v_flat, N):\n    \"\"\"\n    Computes the matrix-vector product Av for the 5-point Laplacian stencil.\n    The matrix A is never explicitly formed.\n    \"\"\"\n    v = v_flat.reshape((N, N))\n    v_padded = np.zeros((N + 2, N + 2))\n    v_padded[1:-1, 1:-1] = v\n    \n    # Apply the stencil: 4*v_ij - v_{i-1,j} - v_{i+1,j} - v_{i,j-1} - v_{i,j+1}\n    Av = 4 * v - (v_padded[0:-2, 1:-1] + v_padded[2:, 1:-1] +\n                    v_padded[1:-1, 0:-2] + v_padded[1:-1, 2:])\n    \n    return Av.flatten()\n\ndef ssor_preconditioner(r_flat, N, omega):\n    \"\"\"\n    Applies the SSOR preconditioner solve Mz = r.\n    This is implemented as a forward sweep followed by a backward sweep.\n    M = (1/(w(2-w))) * (D - wL) * D^-1 * (D - wU)\n    D = 4I\n    \"\"\"\n    r_grid = r_flat.reshape((N, N))\n    \n    # 1. Forward substitution: solve (D - omega*L)y = omega*(2-omega)*r\n    # D is 4I. L corresponds to connections to lower-indexed nodes.\n    # 4*y_ij - omega*y_{i-1,j} - omega*y_{i,j-1} = omega*(2-omega)*r_ij\n    y_grid = np.zeros((N, N))\n    rhs1_grid = omega * (2 - omega) * r_grid\n    for j in range(N):\n        for i in range(N):\n            term = rhs1_grid[i, j]\n            if i > 0:\n                term += omega * y_grid[i - 1, j]\n            if j > 0:\n                term += omega * y_grid[i, j - 1]\n            y_grid[i, j] = term / 4.0\n\n    # 2. Backward substitution: solve (D - omega*U)z = D*y\n    # D is 4I. U corresponds to connections to higher-indexed nodes.\n    # 4*z_ij - omega*z_{i+1,j} - omega*z_{i,j+1} = 4*y_ij\n    z_grid = np.zeros((N, N))\n    rhs2_grid = 4.0 * y_grid\n    for j in range(N - 1, -1, -1):\n        for i in range(N - 1, -1, -1):\n            term = rhs2_grid[i, j]\n            if i  N - 1:\n                term += omega * z_grid[i + 1, j]\n            if j  N - 1:\n                term += omega * z_grid[i, j + 1]\n            z_grid[i, j] = term / 4.0\n            \n    return z_grid.flatten()\n\ndef cg_solver(N, b, tol, max_iter):\n    \"\"\"\n    Conjugate Gradient solver for Ax=b.\n    \"\"\"\n    x = np.zeros_like(b)\n    r = b.copy()\n    p = r.copy()\n    \n    r0_norm = np.linalg.norm(r)\n    if r0_norm == 0:\n        return 0\n        \n    rs_old = np.dot(r, r)\n    \n    for k in range(max_iter):\n        Ap = matvec(p, N)\n        alpha = rs_old / np.dot(p, Ap)\n        \n        x += alpha * p\n        r -= alpha * Ap\n        \n        rs_new = np.dot(r, r)\n        \n        if np.sqrt(rs_new) / r0_norm  tol:\n            return k + 1\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    return max_iter\n\ndef pcg_ssor_solver(N, b, omega, tol, max_iter):\n    \"\"\"\n    Preconditioned Conjugate Gradient solver using SSOR as preconditioner.\n    \"\"\"\n    x = np.zeros_like(b)\n    r = b.copy()\n    \n    r0_norm = np.linalg.norm(r)\n    if r0_norm == 0:\n        return 0\n\n    z = ssor_preconditioner(r, N, omega)\n    p = z.copy()\n    rz_old = np.dot(r, z)\n\n    for k in range(max_iter):\n        Ap = matvec(p, N)\n        alpha = rz_old / np.dot(p, Ap)\n        \n        x += alpha * p\n        r -= alpha * Ap\n        \n        if np.linalg.norm(r) / r0_norm  tol:\n            return k + 1\n            \n        z = ssor_preconditioner(r, N, omega)\n        rz_new = np.dot(r, z)\n        \n        beta = rz_new / rz_old\n        p = z + beta * p\n        rz_old = rz_new\n        \n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # (N, omega)\n    test_cases = [\n        (16, 1.0),\n        (16, 1.5),\n        (16, 1.9),\n        (8, 1.7)\n    ]\n    \n    results = []\n    tol = 1e-8\n    max_iter = 5000\n\n    for case in test_cases:\n        N, omega = case\n        h = 1.0 / (N + 1)\n        \n        # Assemble the right-hand side vector\n        b = assemble_rhs(N, h)\n        \n        # Run unpreconditioned Conjugate Gradient\n        k_cg = cg_solver(N, b, tol, max_iter)\n        \n        # Run Preconditioned Conjugate Gradient with SSOR\n        k_pcg_ssor = pcg_ssor_solver(N, b, omega, tol, max_iter)\n        \n        results.append(f\"[{k_cg},{k_pcg_ssor}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}