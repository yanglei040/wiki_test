{
    "hands_on_practices": [
        {
            "introduction": "We have established that strict diagonal dominance is a powerful sufficient condition guaranteeing the convergence of the Jacobi method. This naturally leads to a critical question: is it also a necessary condition? This exercise  challenges you to probe the theoretical limits of convergence by exploring what happens when diagonal dominance is not present. By determining the smallest possible spectral radius $\\rho(B_J)$ for a non-diagonally dominant matrix, you will gain a deeper appreciation for the precise relationship between a matrix's structure and the behavior of iterative solvers.",
            "id": "2384207",
            "problem": "Consider a real $2 \\times 2$ matrix $A$ with entries $a_{11}=a$, $a_{12}=b$, $a_{21}=c$, $a_{22}=d$, where $a \\neq 0$ and $d \\neq 0$, and assume $A$ is nonsingular. The matrix $A$ is said to be diagonally dominant if for each row $i$, $|a_{ii}| \\geq \\sum_{j \\neq i} |a_{ij}|$. A matrix is called non-diagonally dominant if it is not diagonally dominant. The Jacobi iteration for solving $A\\mathbf{x} = \\mathbf{f}$ is defined by the splitting $A = D + L + U$, where $D = \\mathrm{diag}(a,d)$ is the diagonal of $A$, and $L$ and $U$ are the strictly lower and strictly upper triangular parts of $A$, respectively. The Jacobi iteration matrix is defined as $B_{J} = -D^{-1}(L+U)$. The spectral radius of a matrix $M$, denoted $\\rho(M)$, is the maximum modulus of the eigenvalues of $M$.\n\nAmong all real $2 \\times 2$ nonsingular matrices $A$ with $a \\neq 0$, $d \\neq 0$ that are non-diagonally dominant, and for which the Jacobi method converges (that is, $\\rho(B_{J})  1$), determine the smallest possible value of $\\rho(B_{J})$. Provide your answer as an exact number with no rounding.",
            "solution": "The problem as stated is well-defined and scientifically grounded. We shall proceed with its solution.\n\nLet the real $2 \\times 2$ matrix be denoted by $A$:\n$$\nA = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}\n$$\nThe problem specifies that $A$ is nonsingular, with $a \\neq 0$ and $d \\neq 0$.\n\nThe Jacobi iteration method splits the matrix $A$ into its diagonal, strictly lower triangular, and strictly upper triangular parts: $A = D + L + U$. For the given matrix $A$, these are:\n$$\nD = \\begin{pmatrix} a  0 \\\\ 0  d \\end{pmatrix}, \\quad L = \\begin{pmatrix} 0  0 \\\\ c  0 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 0  b \\\\ 0  0 \\end{pmatrix}\n$$\nThe Jacobi iteration matrix, $B_J$, is defined as $B_J = -D^{-1}(L+U)$. Given that $a \\neq 0$ and $d \\neq 0$, the inverse of $D$ is:\n$$\nD^{-1} = \\begin{pmatrix} \\frac{1}{a}  0 \\\\ 0  \\frac{1}{d} \\end{pmatrix}\n$$\nWe can now compute $B_J$:\n$$\nB_J = - \\begin{pmatrix} \\frac{1}{a}  0 \\\\ 0  \\frac{1}{d} \\end{pmatrix} \\begin{pmatrix} 0  b \\\\ c  0 \\end{pmatrix} = \\begin{pmatrix} 0  -\\frac{b}{a} \\\\ -\\frac{c}{d}  0 \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of $B_J$ are found from the characteristic equation $\\det(B_J - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} -\\lambda  -\\frac{b}{a} \\\\ -\\frac{c}{d}  -\\lambda \\end{pmatrix} = (-\\lambda)(-\\lambda) - \\left(-\\frac{b}{a}\\right)\\left(-\\frac{c}{d}\\right) = \\lambda^2 - \\frac{bc}{ad} = 0\n$$\nThis gives $\\lambda^2 = \\frac{bc}{ad}$, so the eigenvalues are $\\lambda_{1,2} = \\pm \\sqrt{\\frac{bc}{ad}}$. The spectral radius $\\rho(B_J)$ is the maximum of the moduli of the eigenvalues:\n$$\n\\rho(B_J) = \\max \\left\\{ \\left| \\sqrt{\\frac{bc}{ad}} \\right|, \\left| -\\sqrt{\\frac{bc}{ad}} \\right| \\right\\} = \\left| \\sqrt{\\frac{bc}{ad}} \\right| = \\sqrt{\\left| \\frac{bc}{ad} \\right|}\n$$\nThe problem requires us to find the smallest possible value of $\\rho(B_J)$ subject to a set of conditions. Let us list these conditions:\n1.  $a \\neq 0$ and $d \\neq 0$.\n2.  $A$ is nonsingular: $\\det(A) = ad - bc \\neq 0$.\n3.  The Jacobi method converges: $\\rho(B_J)  1$, which means $\\sqrt{\\left| \\frac{bc}{ad} \\right|}  1$, or $|bc|  |ad|$.\n4.  $A$ is non-diagonally dominant. A matrix is diagonally dominant if $|a_{ii}| \\geq \\sum_{j \\neq i} |a_{ij}|$ for all rows $i$. For our $2 \\times 2$ matrix, this is $|a| \\geq |b|$ and $|d| \\geq |c|$. A matrix is non-diagonally dominant if it is not diagonally dominant, which means at least one of these conditions is violated. Thus, the condition for non-diagonal dominance is:\n    $$\n    (|a|  |b|) \\quad \\text{or} \\quad (|d|  |c|)\n    $$\n\nOur objective is to find $\\inf\\left\\{\\rho(B_J)\\right\\}$ over the set of all matrices $A$ satisfying these four conditions.\n\nThe spectral radius, being a maximum of absolute values, is always non-negative: $\\rho(B_J) \\ge 0$. Therefore, the smallest possible value of $\\rho(B_J)$ must be greater than or equal to $0$.\n\nThe question reduces to whether a value of $\\rho(B_J) = 0$ is attainable for a matrix $A$ that satisfies all the given constraints.\nFrom the expression for the spectral radius, $\\rho(B_J) = \\sqrt{|bc/ad|} = 0$ if and only if $bc = 0$, since $a$ and $d$ are non-zero. This implies either $b=0$ or $c=0$. Let us examine if such a matrix can satisfy all the required conditions.\n\nCase 1: Let $c=0$ and $b$ be any real number.\nThe matrix is $A = \\begin{pmatrix} a  b \\\\ 0  d \\end{pmatrix}$.\nWe verify the conditions:\n1.  $a \\neq 0$, $d \\neq 0$. This can be chosen.\n2.  Nonsingularity: $\\det(A) = ad - b(0) = ad$. Since $a \\neq 0$ and $d \\neq 0$, $\\det(A) \\neq 0$. The condition is satisfied.\n3.  Convergence: With $c=0$, $\\rho(B_J) = \\sqrt{|b(0)/(ad)|} = 0$. Since $0  1$, this condition is satisfied.\n4.  Non-diagonal dominance: The condition is $(|a|  |b|)$ or $(|d|  |0|)$. The second part, $|d|  0$, is impossible for real $d \\neq 0$. Thus, we must satisfy the first part: $|a|  |b|$.\n\nWe must demonstrate that there exist real numbers $a, b, d$ such that $a \\neq 0$, $d \\neq 0$, and $|a|  |b|$. This is trivial to achieve. For example, let $a=1$, $b=2$, $c=0$, $d=1$.\nThe matrix is $A = \\begin{pmatrix} 1  2 \\\\ 0  1 \\end{pmatrix}$. Let us verify this specimen.\n- $a=1 \\neq 0$, $d=1 \\neq 0$.\n- $\\det(A) = (1)(1) - (2)(0) = 1 \\neq 0$.\n- $\\rho(B_J) = \\sqrt{|(2)(0)/((1)(1))|} = 0  1$.\n- Non-diagonally dominant, because for the first row, $|a_{11}| = |1| = 1$ and $|a_{12}| = |2| = 2$. Since $1  2$, the condition $|a|  |b|$ is satisfied.\n\nThis matrix fulfills all the criteria stipulated in the problem, and for this matrix, the spectral radius is $0$.\n\nSince we have established that $\\rho(B_J) \\ge 0$ for any valid matrix, and we have constructed a valid matrix for which $\\rho(B_J) = 0$, the smallest possible value of $\\rho(B_J)$ is $0$. A similar argument holds for the case $b=0$ and $|d||c|$.\nThe infimum is an attainable minimum.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "In many engineering and scientific models, the system matrix $A$ may not be conveniently diagonally dominant. This practice  moves from theoretical analysis to a practical, constructive challenge: how can we algorithmically enforce this desirable property? You will design a method to find the 'smallest' diagonal perturbation required to make a given matrix strictly diagonally dominant, a core idea behind techniques known as preconditioning. This hands-on coding task demonstrates how we can engineer matrices to ensure the reliable convergence of iterative methods.",
            "id": "2384230",
            "problem": "You are given a real square matrix $A \\in \\mathbb{R}^{n \\times n}$ that is not strictly diagonally dominant by rows. For a diagonal matrix $\\Delta = \\mathrm{diag}(\\delta_1,\\dots,\\delta_n)$, define the Frobenius norm by $\\|\\Delta\\|_{F} = \\sqrt{\\sum_{i=1}^{n} \\delta_i^2}$. For a given positive margin $\\tau \\in \\mathbb{R}_{0}$, a matrix $B$ is said to be $\\tau$-strictly diagonally dominant by rows if $\\left|b_{ii}\\right| \\ge \\sum_{j \\ne i} \\left|b_{ij}\\right| + \\tau$ holds for every row index $i \\in \\{1,\\dots,n\\}$. The goal is to find the diagonal matrix $\\Delta$ of minimal Frobenius norm such that $A + \\Delta$ is $\\tau$-strictly diagonally dominant by rows.\n\nAdditionally, let $D = \\mathrm{diag}(b_{11},\\dots,b_{nn})$ be the diagonal of $B = A+\\Delta$, and let $I$ denote the identity matrix of size $n$. Define the Jacobi iteration matrix $T_J$ associated with $B$ by $T_J = I - D^{-1} B$. The spectral radius (SR) of a square matrix $M$, denoted $\\rho(M)$, is the maximum of the absolute values of the eigenvalues of $M$.\n\nYour program must, for each test matrix below, with the same prescribed margin $\\tau = 10^{-3}$, compute:\n1. The minimal Frobenius norm $\\|\\Delta\\|_{F}$ among all diagonal matrices $\\Delta$ for which $A+\\Delta$ is $\\tau$-strictly diagonally dominant by rows. In the event of multiple minimizers (which can only occur when $a_{ii} = 0$ for some $i$), choose the minimizer satisfying $a_{ii} + \\delta_i \\ge 0$ for that index $i$.\n2. The spectral radius $\\rho(T_J)$ of the Jacobi iteration matrix built from $B = A+\\Delta$.\n\nAll matrices are real-valued and dimensionless. Use the following test suite with a single common margin $\\tau = 10^{-3}$:\n- Test 1:\n$$ A_1 = \\begin{bmatrix} 1  -2 \\\\ -2  1 \\end{bmatrix} $$\n- Test 2:\n$$ A_2 = \\begin{bmatrix} 4  1  0 \\\\ 0.5  3.5  -0.2 \\\\ 0.1  0.1  1.5 \\end{bmatrix} $$\n- Test 3:\n$$ A_3 = \\begin{bmatrix} 0  0.2 \\\\ -0.3  0 \\end{bmatrix} $$\n- Test 4:\n$$ A_4 = \\begin{bmatrix} -1  0.4  0.6 \\\\ 0.3  -0.9  0.6 \\\\ 0.2  0.5  -0.7 \\end{bmatrix} $$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets of the form\n$[ \\|\\Delta_1\\|_{F}, \\rho(T_{J,1}), \\|\\Delta_2\\|_{F}, \\rho(T_{J,2}), \\|\\Delta_3\\|_{F}, \\rho(T_{J,3}), \\|\\Delta_4\\|_{F}, \\rho(T_{J,4}) ]$,\nwhere $\\Delta_k$ and $T_{J,k}$ correspond to test $k \\in \\{1,2,3,4\\}$. Each entry must be a real number written in decimal or scientific notation. No angles or physical units are involved, and no rounding to a specific number of digits is required.",
            "solution": "The problem as stated is subjected to validation and is determined to be valid. It is scientifically grounded in the principles of numerical linear algebra, well-posed, objective, and contains all necessary information for a unique solution. We therefore proceed with a formal derivation.\n\nThe central task is to find a diagonal matrix $\\Delta = \\mathrm{diag}(\\delta_1, \\dots, \\delta_n)$ that minimizes the Frobenius norm $\\|\\Delta\\|_{F}$ subject to the constraint that $B = A + \\Delta$ is $\\tau$-strictly diagonally dominant by rows. The optimization problem is:\n$$\n\\underset{\\delta_1, \\dots, \\delta_n}{\\text{minimize}} \\quad \\|\\Delta\\|_{F}^2 = \\sum_{i=1}^{n} \\delta_i^2\n$$\n$$\n\\text{subject to} \\quad |b_{ii}| \\ge \\sum_{j \\ne i} |b_{ij}| + \\tau \\quad \\text{for all } i \\in \\{1, \\dots, n\\}\n$$\nThe entries of $B$ are related to those of $A$ and $\\Delta$ by $b_{ij} = a_{ij}$ for $i \\ne j$ and $b_{ii} = a_{ii} + \\delta_i$. Let us define the sum of the magnitudes of the off-diagonal elements of $A$ for each row $i$ as $S_i = \\sum_{j \\ne i} |a_{ij}|$. The constraints can then be rewritten as:\n$$\n|a_{ii} + \\delta_i| \\ge S_i + \\tau \\quad \\text{for all } i \\in \\{1, \\dots, n\\}\n$$\nThe objective function to be minimized, $\\sum_{i=1}^{n} \\delta_i^2$, is a sum of non-negative terms, each depending on a single variable $\\delta_i$. The constraints are also decoupled, with each constraint involving only one $\\delta_i$. Consequently, we can minimize the total sum by independently minimizing each term $\\delta_i^2$ for each row $i$. This is equivalent to minimizing $|\\delta_i|$.\n\nFor each row $i$, we must find the value of $\\delta_i$ with the smallest magnitude that satisfies the constraint $|a_{ii} + \\delta_i| \\ge C_i$, where we define the constant $C_i = S_i + \\tau$. Since $\\tau  0$ and $S_i \\ge 0$, we have $C_i  0$. The inequality $|x| \\ge C_i$ defines the valid region for $x=a_{ii}+\\delta_i$ as $(-\\infty, -C_i] \\cup [C_i, \\infty)$. We seek to find a point in this valid region that is closest to $a_{ii}$ in order to minimize the perturbation $|\\delta_i| = |x - a_{ii}|$.\n\nWe analyze the subproblem for each row $i$:\n1.  If the original matrix $A$ already satisfies the condition for row $i$, i.e., $|a_{ii}| \\ge C_i$, then the constraint is met with $\\delta_i = 0$. Since we wish to minimize $|\\delta_i|$, the optimal choice is indeed $\\delta_i = 0$.\n2.  If $|a_{ii}|  C_i$, the condition is not met, and a non-zero $\\delta_i$ is required. We must perturb $a_{ii}$ to become $b_{ii} = a_{ii} + \\delta_i$ such that $|b_{ii}| \\ge C_i$. To minimize $|\\delta_i| = |b_{ii} - a_{ii}|$, we must choose $b_{ii}$ to be the point in the valid set $(-\\infty, -C_i] \\cup [C_i, \\infty)$ that is closest to the value $a_{ii}$.\n    -   If $a_{ii} \\ge 0$, the closest valid point is $b_{ii} = C_i$. This gives $\\delta_i = C_i - a_{ii}$.\n    -   If $a_{ii}  0$, the closest valid point is $b_{ii} = -C_i$. This gives $\\delta_i = -C_i - a_{ii}$.\n    -   A special case arises if $a_{ii} = 0$. The two candidate values are $\\delta_i = C_i$ and $\\delta_i = -C_i$, both having the same magnitude $|C_i|$. The problem specifies that in this event, we must choose the minimizer such that $a_{ii} + \\delta_i \\ge 0$, which means $b_{ii} \\ge 0$. We must therefore choose $b_{ii} = C_i$, which implies $\\delta_i = C_i$. This case is correctly handled by the $a_{ii} \\ge 0$ logic.\n\nThis procedure uniquely determines the optimal value of $\\delta_i$ for each row $i$. The diagonal matrix $\\Delta = \\mathrm{diag}(\\delta_1, \\dots, \\delta_n)$ is thus uniquely determined. The first required value, the minimal Frobenius norm, is then calculated as $\\|\\Delta\\|_{F} = \\sqrt{\\sum_{i=1}^{n} \\delta_i^2}$.\n\nThe second required value is the spectral radius $\\rho(T_J)$ of the Jacobi iteration matrix $T_J$ associated with the newly formed matrix $B = A + \\Delta$. The matrix $B$ is, by construction, $\\tau$-strictly diagonally dominant. Its diagonal entries $b_{ii}$ are guaranteed to be non-zero since $|b_{ii}| \\ge C_i = S_i + \\tau  0$. Therefore, its diagonal part, $D = \\mathrm{diag}(b_{11}, \\dots, b_{nn})$, is invertible.\n\nThe Jacobi matrix is defined as $T_J = I - D^{-1}B$. Its elements are given by:\n$$\n(T_J)_{ij} = \\begin{cases}\n0  \\text{if } i = j \\\\\n-\\frac{b_{ij}}{b_{ii}} = -\\frac{a_{ij}}{b_{ii}}  \\text{if } i \\ne j\n\\end{cases}\n$$\nThe spectral radius $\\rho(T_J)$ is the maximum absolute value among the eigenvalues of $T_J$:\n$$\n\\rho(T_J) = \\max \\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } T_J \\}\n$$\nThe eigenvalues are found by numerically solving the characteristic equation $\\det(T_J - \\lambda I) = 0$.\n\nThe complete algorithm for each test case $(A, \\tau)$ is as follows:\n1.  Initialize an $n$-dimensional vector of zeros for the diagonal entries of $\\Delta$, denoted $(\\delta_1, \\dots, \\delta_n)$.\n2.  For each row $i$ from $1$ to $n$:\n    a. Compute the off-diagonal sum $S_i = \\sum_{j \\ne i} |a_{ij}|$.\n    b. Compute the target magnitude $C_i = S_i + \\tau$.\n    c. If $|a_{ii}|  C_i$, update $\\delta_i$: if $a_{ii} \\ge 0$, set $\\delta_i = C_i - a_{ii}$; otherwise, set $\\delta_i = -C_i - a_{ii}$.\n3.  Calculate the Frobenius norm $\\|\\Delta\\|_{F} = \\sqrt{\\sum_{i=1}^{n} \\delta_i^2}$.\n4.  Construct the matrix $B = A + \\mathrm{diag}(\\delta_1, \\dots, \\delta_n)$.\n5.  Construct the Jacobi matrix $T_J$ with entries $(T_J)_{ij} = -a_{ij}/b_{ii}$ for $i \\ne j$ and zeros on the diagonal.\n6.  Compute the eigenvalues of $T_J$.\n7.  Determine the spectral radius $\\rho(T_J)$ by finding the maximum absolute value of the computed eigenvalues.\n8.  Return $\\|\\Delta\\|_{F}$ and $\\rho(T_J)$.\nThis procedure is implemented for each provided test matrix.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the formatted output.\n    \"\"\"\n\n    def process_matrix(A, tau):\n        \"\"\"\n        Calculates the minimal Frobenius norm of Delta and the spectral radius\n        of the Jacobi matrix for a given matrix A and margin tau.\n\n        Args:\n            A (np.ndarray): The input square matrix.\n            tau (float): The positive margin for strict diagonal dominance.\n        \n        Returns:\n            tuple[float, float]: A tuple containing the Frobenius norm of Delta\n                                 and the spectral radius of the Jacobi matrix.\n        \"\"\"\n        n = A.shape[0]\n        delta_diag = np.zeros(n, dtype=float)\n\n        # 1. Determine the optimal diagonal matrix Delta\n        for i in range(n):\n            a_ii = A[i, i]\n            # Sum of absolute values of off-diagonal elements\n            S_i = np.sum(np.abs(A[i, :])) - np.abs(a_ii)\n            C_i = S_i + tau\n\n            if np.abs(a_ii)  C_i:\n                # Condition for modification is met\n                if a_ii = 0:\n                    # This case handles a_ii  0 and a_ii = 0 as per problem specification.\n                    delta_diag[i] = C_i - a_ii\n                else: # a_ii  0\n                    delta_diag[i] = -C_i - a_ii\n            # else, delta_diag[i] remains 0, which is the optimal choice.\n\n        # 2. Compute the Frobenius norm of Delta\n        norm_f_delta = np.linalg.norm(delta_diag)\n\n        # 3. Construct the modified matrix B = A + Delta\n        B = A + np.diag(delta_diag)\n\n        # 4. Construct the Jacobi iteration matrix T_J = I - D^-1 * B\n        b_diag = np.diag(B)\n        \n        # T_J has zeros on the diagonal\n        T_J = np.zeros_like(A, dtype=float)\n        for i in range(n):\n            for j in range(n):\n                if i != j:\n                    T_J[i, j] = -A[i, j] / b_diag[i]\n\n        # 5. Compute the spectral radius of T_J\n        eigenvalues = np.linalg.eigvals(T_J)\n        rho_T_J = np.max(np.abs(eigenvalues))\n\n        return norm_f_delta, rho_T_J\n\n    # Define the test cases from the problem statement.\n    tau = 1e-3\n    test_cases = [\n        np.array([[1, -2], [-2, 1]]),\n        np.array([[4, 1, 0], [0.5, 3.5, -0.2], [0.1, 0.1, 1.5]]),\n        np.array([[0, 0.2], [-0.3, 0]]),\n        np.array([[-1, 0.4, 0.6], [0.3, -0.9, 0.6], [0.2, 0.5, -0.7]])\n    ]\n\n    results = []\n    for A in test_cases:\n        norm_D, rho_Tj = process_matrix(A.astype(float), tau)\n        results.extend([norm_D, rho_Tj])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The proof that strict diagonal dominance guarantees convergence relies on the fundamental inequality $\\rho(B_J) \\le \\lVert B_J \\rVert_\\infty$. While mathematically true, how predictive is this bound in practice? This numerical experiment  casts you in the role of a computational researcher, investigating the 'tightness' of this bound for various matrix structures. By discovering when $\\rho(B_J)$ is nearly equal to its norm-based bound and when it is far from it, you will cultivate a more nuanced and practical intuition about theoretical convergence estimates.",
            "id": "2384241",
            "problem": "Consider solving a linear system with the Jacobi method. Let $A \\in \\mathbb{R}^{n \\times n}$ be a nonsingular matrix with diagonal $D$, strict lower part $L$, and strict upper part $U$, so that $A = D - L - U$. The Jacobi iteration takes the form $x^{(k+1)} = D^{-1}(b + (L + U)x^{(k)})$, with iteration matrix $B_J = D^{-1}(L + U)$. For a given matrix $B$, the spectral radius $\\rho(B)$ is defined as the maximum modulus of its eigenvalues. A core fact from induced matrix norms is that $\\rho(B) \\le \\lVert B \\rVert$ for any induced norm, and in particular $\\rho(B_J) \\le \\lVert B_J \\rVert_\\infty$, where $\\lVert \\cdot \\rVert_\\infty$ is the induced infinity norm (maximum absolute row sum).\n\nDesign a numerical experiment to compare the tightness of the infinity-norm-based upper bound on $\\rho(B_J)$ across different matrices $A$. All computations must be performed purely in $\\mathbb{R}$ (no physical units are involved). Start from the fundamental definitions of the Jacobi iteration matrix and induced infinity norm. For each test case, you must:\n- Construct the specified matrix $A$ using only the given parameters.\n- Form $B_J$ directly from $A$ via $B_J(i,j) = -a_{ij}/a_{ii}$ for $i \\ne j$ and $B_J(i,i) = 0$.\n- Compute the bound as $\\lVert B_J \\rVert_\\infty$ (the maximum absolute row sum of $B_J$).\n- Compute the spectral radius $\\rho(B_J)$ as the maximum modulus of the eigenvalues of $B_J$.\n- Report the nonnegative gap $\\lVert B_J \\rVert_\\infty - \\rho(B_J)$ as a float, which quantifies how loose the bound is (zero means tight).\n- Also report a boolean indicating whether the bound is numerically tight, defined here as $\\lVert B_J \\rVert_\\infty - \\rho(B_J) \\le 10^{-10}$.\n\nYou must implement the following test suite of matrices $A$ (each fully determined by the parameters below), chosen to probe different behaviors of tightness:\n\n- Case 1 (exactly or nearly tight via constant off-diagonal structure): $A \\in \\mathbb{R}^{n \\times n}$ with $a_{ii} = 1$ for all $i$ and $a_{ij} = -s/(n-1)$ for all $i \\ne j$. Parameters: $n = 5$, $s = 0.9$.\n- Case 2 (loose for small tridiagonal Toeplitz): $A = d I - c T$, where $T$ is the tridiagonal matrix with $1$ on the first sub- and super-diagonals and $0$ elsewhere. Parameters: $n = 3$, $d = 1.0$, $c = 0.45$.\n- Case 3 (extremely loose via nilpotent $B_J$): $A = I - U$, where $U$ has a constant value $t$ on the first super-diagonal and zeros elsewhere (so $U$ is strictly upper triangular). Parameters: $n = 6$, $t = 0.9$.\n- Case 4 (near-tight for large tridiagonal Toeplitz): $A = d I - c T$ with the same definition of $T$ as in Case 2. Parameters: $n = 50$, $d = 1.0$, $c = 0.45$.\n\nYour program must:\n- Build each $A$ exactly from its parameters as described, without using any randomness.\n- For each case, compute:\n  1) the bound $\\lVert B_J \\rVert_\\infty$,\n  2) the spectral radius $\\rho(B_J)$,\n  3) the gap $\\lVert B_J \\rVert_\\infty - \\rho(B_J)$,\n  4) the boolean tightness indicator as defined above.\n- Round all floating-point outputs to six decimal places before printing.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list containing four entries in the order specified above. For example, a valid output with two cases would look like $[[b_1, r_1, g_1, t_1], [b_2, r_2, g_2, t_2]]$, where $b_k$, $r_k$, and $g_k$ are floats (rounded to six decimals) and $t_k$ is a boolean.",
            "solution": "The problem as stated is valid. It is based on established principles of numerical linear algebra, specifically the convergence analysis of the Jacobi iterative method. All definitions, including the Jacobi iteration matrix $B_J$, the spectral radius $\\rho(B)$, and the induced infinity norm $\\lVert \\cdot \\rVert_\\infty$, are standard. The problem is self-contained, with all necessary parameters provided for each test case, and poses a well-defined computational task. No scientific inconsistencies, ambiguities, or contradictions are present.\n\nThe objective is to conduct a numerical experiment to assess the tightness of the bound $\\rho(B_J) \\le \\lVert B_J \\rVert_\\infty$. For each specified matrix $A$, we must compute the iteration matrix $B_J$, its spectral radius $\\rho(B_J)$, the infinity norm $\\lVert B_J \\rVert_\\infty$, the gap $\\lVert B_J \\rVert_\\infty - \\rho(B_J)$, and a boolean indicator for numerical tightness.\n\nLet us proceed with a formal, step-by-step derivation and implementation plan for each test case.\n\nThe Jacobi iteration matrix $B_J$ is defined as $B_J = D^{-1}(L+U)$, where $A = D-L-U$. Here, $D$ is the diagonal part of $A$, $-L$ is the strictly lower triangular part, and $-U$ is the strictly upper triangular part. An equivalent and computationally convenient formula for $B_J$ is $B_J = I - D^{-1}A$. The components of $B_J$ are given by $(B_J)_{ii} = 0$ for all $i$, and $(B_J)_{ij} = -a_{ij}/a_{ii}$ for $i \\ne j$. This requires $a_{ii} \\ne 0$ for all $i$, a condition that holds for all specified test cases.\n\nThe quantities to be computed are:\n$1$. The infinity norm, $\\lVert B_J \\rVert_\\infty = \\max_{1 \\le i \\le n} \\sum_{j=1}^{n} |(B_J)_{ij}|$.\n$2$. The spectral radius, $\\rho(B_J) = \\max \\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } B_J\\}$.\n$3$. The gap, $g = \\lVert B_J \\rVert_\\infty - \\rho(B_J)$.\n$4$. The tightness boolean, $t = (g \\le 10^{-10})$.\n\nWe will now analyze each case and construct the required matrices and compute the desired values.\n\n**Case 1: Constant Off-Diagonal Structure**\n- Parameters: $n=5$, $s=0.9$.\n- Matrix $A \\in \\mathbb{R}^{5 \\times 5}$ is defined by $a_{ii} = 1$ and $a_{ij} = -s/(n-1) = -0.9/4 = -0.225$ for $i \\ne j$.\n- The Jacobi matrix $B_J$ has elements $(B_J)_{ij} = -a_{ij}/a_{ii} = -(-0.225)/1 = 0.225$ for $i \\ne j$, and $(B_J)_{ii} = 0$.\n- The infinity norm is the maximum absolute row sum. For any row $i$, $\\sum_{j=1}^{n} |(B_J)_{ij}| = (n-1) \\times 0.225 = 4 \\times 0.225 = 0.9$. Thus, $\\lVert B_J \\rVert_\\infty = 0.9$.\n- The matrix $B_J$ can be written as $k(J-I)$, where $k=0.225$, $J$ is the all-ones matrix, and $I$ is the identity matrix. The eigenvalues of $J$ are $\\{n, 0, \\dots, 0\\}$. The eigenvalues of $J-I$ are $\\{n-1, -1, \\dots, -1\\}$. For $n=5$, they are $\\{4, -1, -1, -1, -1\\}$.\n- The eigenvalues of $B_J$ are $k$ times these values: $\\{0.225 \\times 4, 0.225 \\times (-1), \\dots\\} = \\{0.9, -0.225, -0.225, -0.225, -0.225\\}$.\n- The spectral radius is $\\rho(B_J) = \\max(|0.9|, |-0.225|) = 0.9$.\n- The gap is $\\lVert B_J \\rVert_\\infty - \\rho(B_J) = 0.9 - 0.9 = 0$. The bound is exactly tight.\n\n**Case 2: Small Tridiagonal Toeplitz Matrix**\n- Parameters: $n=3$, $d=1.0$, $c=0.45$.\n- Matrix $A = dI - cT$, where $T$ has $1$s on the first sub- and super-diagonals.\n$$A = \\begin{pmatrix} 1.0  -0.45  0 \\\\ -0.45  1.0  -0.45 \\\\ 0  -0.45  1.0 \\end{pmatrix}$$\n- Since $a_{ii} = d = 1.0$, the Jacobi matrix is $B_J = cT$.\n$$B_J = \\begin{pmatrix} 0  0.45  0 \\\\ 0.45  0  0.45 \\\\ 0  0.45  0 \\end{pmatrix}$$\n- The absolute row sums are $\\{0.45, 0.9, 0.45\\}$. The maximum is $\\lVert B_J \\rVert_\\infty = 0.9$.\n- The eigenvalues of such a tridiagonal matrix $T$ of size $n \\times n$ are $\\lambda_k = 2\\cos(\\frac{k\\pi}{n+1})$ for $k=1, \\dots, n$.\n- For $n=3$, the eigenvalues of $T$ are $2\\cos(\\pi/4) = \\sqrt{2}$, $2\\cos(2\\pi/4) = 0$, and $2\\cos(3\\pi/4) = -\\sqrt{2}$.\n- The eigenvalues of $B_J = cT$ are $\\{0.45\\sqrt{2}, 0, -0.45\\sqrt{2}\\}$.\n- The spectral radius is $\\rho(B_J) = |0.45\\sqrt{2}| \\approx 0.636396$.\n- The gap is $0.9 - 0.636396... \\approx 0.263604$. The bound is not tight.\n\n**Case 3: Nilpotent Jacobi Matrix**\n- Parameters: $n=6$, $t=0.9$.\n- Matrix $A = I - U$, where $U$ is a matrix with $t=0.9$ on the first super-diagonal and zeros elsewhere.\n$$A = \\begin{pmatrix} 1  -0.9  0  0  0  0 \\\\ 0  1  -0.9  0  0  0 \\\\ 0  0  1  -0.9  0  0 \\\\ 0  0  0  1  -0.9  0 \\\\ 0  0  0  0  1  -0.9 \\\\ 0  0  0  0  0  1 \\end{pmatrix}$$\n- The diagonal entries are $a_{ii}=1$. The Jacobi matrix is $B_J = I - D^{-1}A = I - I^{-1}(I - U) = U$.\n- So, $B_J$ has $0.9$ on its first super-diagonal and is zero elsewhere.\n- The absolute row sums are $\\{0.9, 0.9, 0.9, 0.9, 0.9, 0\\}$. The maximum is $\\lVert B_J \\rVert_\\infty = 0.9$.\n- $B_J$ is a strictly upper triangular matrix. Its eigenvalues are its diagonal entries, which are all $0$.\n- The spectral radius is $\\rho(B_J) = 0$.\n- The gap is $0.9 - 0 = 0.9$. The bound is extremely loose.\n\n**Case 4: Large Tridiagonal Toeplitz Matrix**\n- Parameters: $n=50$, $d=1.0$, $c=0.45$.\n- The setup is identical to Case $2$, but with $n=50$. $A = dI - cT$ and $B_J = cT$.\n- The absolute row sums are $\\{c, 2c, \\dots, 2c, c\\}$. With $c=0.45$, the sums are $\\{0.45, 0.9, \\dots, 0.9, 0.45\\}$. The maximum is $\\lVert B_J \\rVert_\\infty = 2c = 0.9$.\n- The eigenvalues of $B_J$ are $\\lambda_k = 2c\\cos(\\frac{k\\pi}{n+1})$ for $k=1, \\dots, n$.\n- We have $n=50$, so the eigenvalues are $0.9\\cos(\\frac{k\\pi}{51})$ for $k=1, \\dots, 50$.\n- The spectral radius is $\\rho(B_J) = \\max_{k} |0.9\\cos(\\frac{k\\pi}{51})| = 0.9\\cos(\\frac{\\pi}{51})$.\n- As $n$ becomes large, $\\frac{\\pi}{n+1} \\to 0$ and $\\cos(\\frac{\\pi}{n+1}) \\to 1$. Thus, $\\rho(B_J)$ approaches $\\lVert B_J \\rVert_\\infty$.\n- $\\rho(B_J) = 0.9 \\times \\cos(\\pi/51) \\approx 0.9 \\times 0.998108 \\approx 0.898297$.\n- The gap is $0.9 - 0.898297... \\approx 0.001703$. The bound is near-tight but does not meet the strict criterion of $g \\le 10^{-10}$.\n\nThe implementation will follow these steps for each case, using numerical libraries to construct matrices and compute eigenvalues. The final results will be formatted as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a numerical experiment to compare the tightness of the\n    infinity-norm-based upper bound on the spectral radius of the Jacobi\n    iteration matrix for several test cases.\n    \"\"\"\n\n    test_cases_params = [\n        # Case 1: n=5, s=0.9\n        {'type': 'constant_off_diagonal', 'n': 5, 's': 0.9},\n        # Case 2: n=3, d=1.0, c=0.45\n        {'type': 'tridiagonal', 'n': 3, 'd': 1.0, 'c': 0.45},\n        # Case 3: n=6, t=0.9\n        {'type': 'nilpotent_b_j', 'n': 6, 't': 0.9},\n        # Case 4: n=50, d=1.0, c=0.45\n        {'type': 'tridiagonal', 'n': 50, 'd': 1.0, 'c': 0.45},\n    ]\n\n    all_results = []\n\n    for params in test_cases_params:\n        # Construct matrix A based on the case type and parameters\n        n = params['n']\n        if params['type'] == 'constant_off_diagonal':\n            s = params['s']\n            A = np.full((n, n), -s / (n - 1))\n            np.fill_diagonal(A, 1.0)\n        elif params['type'] == 'tridiagonal':\n            d = params['d']\n            c = params['c']\n            T = np.diag(np.ones(n - 1), k=1) + np.diag(np.ones(n - 1), k=-1)\n            A = d * np.eye(n) - c * T\n        elif params['type'] == 'nilpotent_b_j':\n            t = params['t']\n            U = np.diag(np.full(n - 1, t), k=1)\n            A = np.eye(n) - U\n        else:\n            raise ValueError(\"Unknown test case type\")\n\n        # Construct the Jacobi iteration matrix B_J\n        # B_J = I - D^{-1}A\n        diag_A = np.diag(A)\n        if np.any(diag_A == 0):\n            # This case should not be reached with the given problems.\n            raise ValueError(\"Matrix A has zero on the diagonal, D is not invertible.\")\n        \n        D_inv = np.diag(1.0 / diag_A)\n        B_J = np.eye(n) - D_inv @ A\n\n        # 1. Compute the infinity norm of B_J (the bound)\n        bound = np.linalg.norm(B_J, ord=np.inf)\n\n        # 2. Compute the spectral radius of B_J\n        eigenvalues = np.linalg.eigvals(B_J)\n        spectral_radius = np.max(np.abs(eigenvalues))\n\n        # 3. Compute the gap\n        gap = bound - spectral_radius\n\n        # 4. Determine if the bound is numerically tight\n        is_tight = gap = 1e-10\n\n        all_results.append([bound, spectral_radius, gap, is_tight])\n\n    # Format the final output string as specified\n    formatted_cases = []\n    for res in all_results:\n        b, r, g, t = res\n        # Format floats to 6 decimal places and boolean to string\n        case_str = f\"[{b:.6f},{r:.6f},{g:.6f},{str(t)}]\"\n        formatted_cases.append(case_str)\n    \n    final_output = f\"[{','.join(formatted_cases)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}