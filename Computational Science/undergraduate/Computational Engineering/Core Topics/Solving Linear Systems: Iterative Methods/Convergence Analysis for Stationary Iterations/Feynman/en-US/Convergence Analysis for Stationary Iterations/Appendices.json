{
    "hands_on_practices": [
        {
            "introduction": "Theoretical guarantees of convergence are foundational, but seeing them in action provides invaluable intuition. This first practice challenges you to implement the Jacobi and Gauss-Seidel methods and observe their performance on matrices with varying degrees of diagonal dominance. By comparing the iteration counts, you will gain a concrete understanding of how matrix properties directly influence the speed of convergence .",
            "id": "2406932",
            "problem": "You must write a complete, runnable program that evaluates linear stationary iterations for specified linear systems. Consider square, real, symmetric matrices of size $n \\times n$ with right-hand side vector $\\mathbf{b} \\in \\mathbb{R}^n$. For each system, starting from the zero vector $\\mathbf{x}^{(0)} = \\mathbf{0}$, iterate until the infinity norm of the residual $\\|\\mathbf{r}^{(k)}\\|_{\\infty} = \\|\\mathbf{b} - A \\mathbf{x}^{(k)}\\|_{\\infty}$ is less than or equal to a tolerance $\\tau$, or until a maximum number of iterations $k_{\\max}$ is reached. Use the standard definitions of the Jacobi method and the Gauss–Seidel method. For each specified matrix, report the minimal iteration counts required by the Jacobi and Gauss–Seidel methods to satisfy the residual tolerance, and the spectral radius of the Jacobi iteration matrix. All computations are purely numerical, and no physical units are involved.\n\nUse the following parameters, which constitute the test suite:\n\n- Dimension $n = 6$.\n- Right-hand side $\\mathbf{b} = [1,2,3,4,5,6]^T$.\n- Initial guess $\\mathbf{x}^{(0)} = \\mathbf{0}$.\n- Residual tolerance $\\tau = 10^{-8}$.\n- Maximum iterations $k_{\\max} = 20000$.\n\nDefine three matrices $A \\in \\mathbb{R}^{6 \\times 6}$ as follows:\n\n1) Strongly diagonally dominant, dense with constant off-diagonals:\n- Parameters $\\alpha_{\\mathrm{s}} = 20.0$, $\\gamma = -1.0$.\n- Entries:\n  - $A_{ii} = \\alpha_{\\mathrm{s}}$ for all $i \\in \\{1,\\dots,6\\}$.\n  - $A_{ij} = \\gamma$ for all $i \\neq j$.\n\n2) Just-barely diagonally dominant, dense with constant off-diagonals:\n- Parameters $\\alpha_{\\mathrm{w}} = 5.1$, $\\gamma = -1.0$.\n- Entries:\n  - $A_{ii} = \\alpha_{\\mathrm{w}}$ for all $i \\in \\{1,\\dots,6\\}$.\n  - $A_{ij} = \\gamma$ for all $i \\neq j$.\n\n3) Edge case, symmetric tridiagonal (one-dimensional discrete Laplacian form):\n- Entries:\n  - $A_{ii} = 2.0$ for all $i \\in \\{1,\\dots,6\\}$.\n  - $A_{i,i+1} = A_{i+1,i} = -1.0$ for all $i \\in \\{1,\\dots,5\\}$.\n  - All other off-diagonal entries are $0.0$.\n\nFor each of the three matrices, do the following:\n- Using the Jacobi method, determine the minimal iteration count $k_{\\mathrm{J}}$ such that $\\|\\mathbf{b} - A \\mathbf{x}^{(k_{\\mathrm{J}})}\\|_{\\infty} \\le \\tau$, or return $k_{\\max}$ if such $k_{\\mathrm{J}}$ is not achieved within $k_{\\max}$ iterations.\n- Using the Gauss–Seidel method, determine the minimal iteration count $k_{\\mathrm{GS}}$ such that $\\|\\mathbf{b} - A \\mathbf{x}^{(k_{\\mathrm{GS}})}\\|_{\\infty} \\le \\tau$, or return $k_{\\max}$ if such $k_{\\mathrm{GS}}$ is not achieved within $k_{\\max}$ iterations.\n- For the Jacobi method, compute the spectral radius $\\rho_{\\mathrm{J}}$ of its iteration matrix. Report $\\rho_{\\mathrm{J}}$ rounded to six decimal places.\n\nYour program should produce a single line of output containing the nine results in the following order:\n- For the strongly diagonally dominant matrix: $k_{\\mathrm{J}}$, $k_{\\mathrm{GS}}$, $\\rho_{\\mathrm{J}}$ (rounded to six decimal places).\n- For the just-barely diagonally dominant matrix: $k_{\\mathrm{J}}$, $k_{\\mathrm{GS}}$, $\\rho_{\\mathrm{J}}$ (rounded to six decimal places).\n- For the tridiagonal edge-case matrix: $k_{\\mathrm{J}}$, $k_{\\mathrm{GS}}$, $\\rho_{\\mathrm{J}}$ (rounded to six decimal places).\n\nThe final output format must be a single line that is a comma-separated list enclosed in square brackets, for example\n$[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_9]$,\nwhere $k_{\\mathrm{J}}$ and $k_{\\mathrm{GS}}$ are integers and each $\\rho_{\\mathrm{J}}$ is a floating-point number rounded to six decimal places.",
            "solution": "The problem is subjected to validation and is determined to be valid. It is scientifically grounded in the field of numerical linear algebra, well-posed with all necessary parameters defined, and objective in its formulation. The problem asks for the implementation and evaluation of two fundamental linear stationary iterative methods, Jacobi and Gauss–Seidel, for solving a system of linear equations $A\\mathbf{x} = \\mathbf{b}$.\n\nThese methods are based on splitting the matrix $A$ into its constituent parts. A square matrix $A$ can be decomposed as $A = D + L + U$, where $D$ is a diagonal matrix containing the diagonal elements of $A$, $L$ is a strictly lower triangular matrix, and $U$ is a strictly upper triangular matrix. The system $A\\mathbf{x} = \\mathbf{b}$ can thus be written as $(D+L+U)\\mathbf{x} = \\mathbf{b}$.\n\n**Jacobi Method**\n\nThe Jacobi method rearranges the system as $D\\mathbf{x} = \\mathbf{b} - (L+U)\\mathbf{x}$. This leads to the iterative scheme:\n$$ D\\mathbf{x}^{(k+1)} = \\mathbf{b} - (L+U)\\mathbf{x}^{(k)} $$\nAssuming $D$ is invertible (i.e., no zero diagonal elements, which is true for all matrices in this problem), we obtain the iteration formula:\n$$ \\mathbf{x}^{(k+1)} = D^{-1}(\\mathbf{b} - (L+U)\\mathbf{x}^{(k)}) $$\nThis can be computed component-wise for each element $i$ of the vector $\\mathbf{x}^{(k+1)}$:\n$$ x_i^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j=1, j \\neq i}^{n} A_{ij} x_j^{(k)} \\right) $$\nA key characteristic of the Jacobi method is that the computation of each component $x_i^{(k+1)}$ depends only on the components of the vector from the previous iteration, $\\mathbf{x}^{(k)}$. This allows for parallel computation of the new vector components.\n\n**Gauss–Seidel Method**\n\nThe Gauss–Seidel method aims to improve the convergence rate by using the most up-to-date information available. It rearranges the system as $(D+L)\\mathbf{x} = \\mathbf{b} - U\\mathbf{x}$, leading to the iterative scheme:\n$$ (D+L)\\mathbf{x}^{(k+1)} = \\mathbf{b} - U\\mathbf{x}^{(k)} $$\nThis yields the iteration formula:\n$$ \\mathbf{x}^{(k+1)} = (D+L)^{-1}(\\mathbf{b} - U\\mathbf{x}^{(k)}) $$\nIn practice, this is implemented as a forward substitution. The component-wise formula is:\n$$ x_i^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} A_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^{n} A_{ij} x_j^{(k)} \\right) $$\nNotice that for computing $x_i^{(k+1)}$, we use the newly computed components $x_j^{(k+1)}$ for $j < i$ from the current iteration $k+1$, and the old components $x_j^{(k)}$ for $j > i$ from the previous iteration $k$. This sequential dependency means the components must be updated in order.\n\n**Convergence and Spectral Radius**\n\nAny linear stationary iteration can be written in the form $\\mathbf{x}^{(k+1)} = T \\mathbf{x}^{(k)} + \\mathbf{c}$, where $T$ is the iteration matrix. The method is guaranteed to converge for any initial guess $\\mathbf{x}^{(0)}$ if and only if the spectral radius of the iteration matrix, $\\rho(T)$, is strictly less than $1$. The spectral radius is defined as the maximum absolute value of the eigenvalues of $T$, i.e., $\\rho(T) = \\max_i |\\lambda_i(T)|$.\n\nFor the Jacobi method, the iteration matrix $T_J$ is given by:\n$$ T_J = -D^{-1}(L+U) = I - D^{-1}A $$\nThe spectral radius $\\rho(T_J)$ dictates the convergence of the Jacobi method. A smaller spectral radius implies a faster asymptotic rate of convergence. The problem requires the calculation of this value.\n\n**Implementation Strategy**\n\nThe solution will be implemented in Python using the `numpy` library.\n1.  **Matrix Construction**: The three specified matrices, $A_1$ (strongly diagonally dominant), $A_2$ (just-barely diagonally dominant), and $A_3$ (tridiagonal), will be constructed as `numpy` arrays. The problem parameters $n=6$, $\\mathbf{b}=[1,2,3,4,5,6]^T$, $\\mathbf{x}^{(0)}=\\mathbf{0}$, $\\tau=10^{-8}$, and $k_{\\max}=20000$ will be defined.\n2.  **Iterative Solvers**: Functions for the Jacobi and Gauss–Seidel methods will be implemented. Each function will take a matrix $A$, vector $\\mathbf{b}$, initial guess $\\mathbf{x}^{(0)}$, tolerance $\\tau$, and maximum iterations $k_{\\max}$ as input. The loop will run from $k=1$ to $k_{\\max}$, updating the solution vector $\\mathbf{x}$ at each step. After each update, the infinity norm of the residual, $\\|\\mathbf{r}^{(k)}\\|_{\\infty} = \\|\\mathbf{b} - A\\mathbf{x}^{(k)}\\|_{\\infty}$, will be checked against the tolerance $\\tau$. If the condition is met, the current iteration count $k$ is returned. If the loop completes without convergence, $k_{\\max}$ is returned. An initial check for $k=0$ is also performed.\n3.  **Spectral Radius Calculation**: A function will compute the Jacobi iteration matrix $T_J = I - D^{-1}A$. The eigenvalues of $T_J$ will be found using `numpy.linalg.eigvals`, and the spectral radius will be the maximum of their absolute values.\n4.  **Execution and Output**: The main part of the program will iterate through the three test cases (matrices). For each case, it will call the solver functions to get the iteration counts $k_J$ and $k_{GS}$, and the spectral radius function for $\\rho_J$. The results will be collected and formatted into a single string as specified in the problem statement.",
            "answer": "```python\nimport numpy as np\n\ndef jacobi(A: np.ndarray, b: np.ndarray, x0: np.ndarray, tol: float, k_max: int) -> int:\n    \"\"\"\n    Solves the system Ax=b using the Jacobi method.\n\n    Args:\n        A: The n x n coefficient matrix.\n        b: The n x 1 right-hand side vector.\n        x0: The initial guess vector.\n        tol: The residual tolerance.\n        k_max: The maximum number of iterations.\n\n    Returns:\n        The number of iterations required for convergence.\n    \"\"\"\n    n = A.shape[0]\n    x = x0.copy()\n\n    # Check for k=0\n    residual_norm = np.linalg.norm(b - A @ x, np.inf)\n    if residual_norm <= tol:\n        return 0\n\n    D = np.diag(A)\n    R = A - np.diag(D)  # R = L + U\n\n    for k in range(1, k_max + 1):\n        x_new = (b - R @ x) / D\n        x = x_new\n        residual_norm = np.linalg.norm(b - A @ x, np.inf)\n        if residual_norm <= tol:\n            return k\n    \n    return k_max\n\ndef gauss_seidel(A: np.ndarray, b: np.ndarray, x0: np.ndarray, tol: float, k_max: int) -> int:\n    \"\"\"\n    Solves the system Ax=b using the Gauss-Seidel method.\n\n    Args:\n        A: The n x n coefficient matrix.\n        b: The n x 1 right-hand side vector.\n        x0: The initial guess vector.\n        tol: The residual tolerance.\n        k_max: The maximum number of iterations.\n\n    Returns:\n        The number of iterations required for convergence.\n    \"\"\"\n    n = A.shape[0]\n    x = x0.copy()\n\n    # Check for k=0\n    residual_norm = np.linalg.norm(b - A @ x, np.inf)\n    if residual_norm <= tol:\n        return 0\n\n    for k in range(1, k_max + 1):\n        x_old = x.copy()\n        for i in range(n):\n            sum1 = np.dot(A[i, :i], x[:i])\n            sum2 = np.dot(A[i, i + 1:], x_old[i + 1:])\n            x[i] = (b[i] - sum1 - sum2) / A[i, i]\n        \n        residual_norm = np.linalg.norm(b - A @ x, np.inf)\n        if residual_norm <= tol:\n            return k\n            \n    return k_max\n\ndef get_spectral_radius_J(A: np.ndarray) -> float:\n    \"\"\"\n    Computes the spectral radius of the Jacobi iteration matrix.\n\n    Args:\n        A: The n x n coefficient matrix.\n\n    Returns:\n        The spectral radius of the Jacobi matrix T_J.\n    \"\"\"\n    D = np.diag(np.diag(A))\n    D_inv = np.linalg.inv(D)\n    T_J = np.eye(A.shape[0]) - D_inv @ A\n    eigenvalues = np.linalg.eigvals(T_J)\n    spectral_radius = np.max(np.abs(eigenvalues))\n    return spectral_radius\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Global parameters\n    n = 6\n    b = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n    x0 = np.zeros(n)\n    tol = 1e-8\n    k_max = 20000\n\n    # Define the three matrices\n    test_cases = []\n\n    # Case 1: Strongly diagonally dominant\n    alpha_s = 20.0\n    gamma1 = -1.0\n    A1 = np.full((n, n), gamma1)\n    np.fill_diagonal(A1, alpha_s)\n    test_cases.append(A1)\n\n    # Case 2: Just-barely diagonally dominant\n    alpha_w = 5.1\n    gamma2 = -1.0\n    A2 = np.full((n, n), gamma2)\n    np.fill_diagonal(A2, alpha_w)\n    test_cases.append(A2)\n\n    # Case 3: Tridiagonal edge case\n    A3 = 2.0 * np.eye(n) - np.eye(n, k=1) - np.eye(n, k=-1)\n    test_cases.append(A3)\n\n    results = []\n    for A in test_cases:\n        k_J = jacobi(A, b, x0, tol, k_max)\n        k_GS = gauss_seidel(A, b, x0, tol, k_max)\n        rho_J = get_spectral_radius_J(A)\n\n        results.append(k_J)\n        results.append(k_GS)\n        results.append(round(rho_J, 6))\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While Gauss-Seidel often converges faster than Jacobi, in certain special cases this relationship can be quantified exactly. This exercise focuses on a symmetric, tridiagonal matrix—a structure that frequently appears in engineering analysis—and asks you to analytically prove the beautiful relationship $\\rho(T_{GS}) = [\\rho(T_J)]^2$. Mastering this derivation provides insight into the theory behind more advanced methods like Successive Over-Relaxation (SOR) .",
            "id": "2381617",
            "problem": "You are to work with stationary iterations for a linear system in the context of computational engineering. Consider the $3 \\times 3$ linear system $A x = b$ with\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix}.\n$$\nUse the standard splittings for Jacobi and Gauss-Seidel methods defined by $A = D - L - U$, where $D$ is the diagonal of $A$, $L$ is the strictly lower triangular part of $A$ with nonnegative entries, and $U$ is the strictly upper triangular part of $A$ with nonnegative entries. The corresponding iteration matrices are $G_{J} = D^{-1}(L + U)$ for the Jacobi method and $G_{GS} = (D - L)^{-1} U$ for the Gauss-Seidel method. The matrix $A$ is symmetric positive definite (SPD). Determine the spectral radii $\\rho(G_{J})$ and $\\rho(G_{GS})$, verify that $\\rho(G_{GS}) = \\rho(G_{J})^{2}$, and provide the exact value of $\\rho(G_{J})$ as your final answer. Express the final answer as a single exact expression. No rounding is required.",
            "solution": "The problem statement is subjected to validation.\n\nStep $1$: Extract Givens.\n- A linear system $A x = b$.\n- The matrix $A$ is given as:\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix}.\n$$\n- Standard splittings for Jacobi and Gauss-Seidel methods are defined by $A = D - L - U$.\n- $D$ is the diagonal of $A$.\n- $L$ is the strictly lower triangular part of $A$ with nonnegative entries.\n- $U$ is the strictly upper triangular part of $A$ with nonnegative entries.\n- Jacobi iteration matrix: $G_{J} = D^{-1}(L + U)$.\n- Gauss-Seidel iteration matrix: $G_{GS} = (D - L)^{-1} U$.\n- The matrix $A$ is stated to be symmetric positive definite (SPD).\n- The task is to determine the spectral radii $\\rho(G_{J})$ and $\\rho(G_{GS})$, verify that $\\rho(G_{GS}) = \\rho(G_{J})^{2}$, and provide the exact value of $\\rho(G_{J})$.\n\nStep $2$: Validate Using Extracted Givens.\n- **Scientifically Grounded**: The problem is a standard exercise in numerical linear algebra, specifically concerning the convergence analysis of stationary iterative methods. The matrix $A$ is a well-known example of a symmetric tridiagonal matrix arising from the finite difference discretization of the one-dimensional Laplacian operator. It is indeed symmetric and positive definite. The definitions of the iteration matrices are standard. The relationship $\\rho(G_{GS}) = \\rho(G_J)^2$ is a known theorem for consistently ordered matrices, which $A$ is. The problem is scientifically sound.\n- **Well-Posed**: The problem is clearly defined, with all necessary information provided to compute the required quantities. The objectives are specific and lead to a unique solution.\n- **Objective**: The problem is stated using precise mathematical language, free from any subjectivity or ambiguity.\n\nStep $3$: Verdict and Action.\nThe problem is valid as it is scientifically grounded, well-posed, and objective. A complete solution will be provided.\n\nThe solution proceeds as follows. First, we decompose the matrix $A$ into its components $D$, $L$, and $U$. Then, we construct the iteration matrices $G_J$ and $G_{GS}$. We then find the eigenvalues of each iteration matrix to determine their spectral radii. Finally, we verify the required relationship.\n\nThe given matrix is\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix}.\n$$\nAccording to the definitions provided, the splitting $A = D - L - U$ is performed. The diagonal matrix $D$ is\n$$\nD \\;=\\; \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix}.\n$$\nThe matrices $L$ and $U$ are the strictly lower and upper triangular parts of $A$ with entries negated to be nonnegative, as per the definition.\n$$\nL \\;=\\; \\begin{pmatrix}\n0 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix}, \\quad U \\;=\\; \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix}.\n$$\nWe can verify this decomposition: $D-L-U = \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} = A$. The decomposition is correct.\n\nNow, we determine the Jacobi iteration matrix, $G_J = D^{-1}(L+U)$.\nThe inverse of $D$ is $D^{-1} = \\frac{1}{2}I = \\begin{pmatrix} 1/2 & 0 & 0 \\\\ 0 & 1/2 & 0 \\\\ 0 & 0 & 1/2 \\end{pmatrix}$.\nThe sum $L+U$ is\n$$\nL+U \\;=\\; \\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix}.\n$$\nTherefore, the Jacobi iteration matrix is\n$$\nG_J \\;=\\; D^{-1}(L+U) \\;=\\; \\frac{1}{2} \\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n0 & 1/2 & 0 \\\\\n1/2 & 0 & 1/2 \\\\\n0 & 1/2 & 0\n\\end{pmatrix}.\n$$\nTo find the spectral radius $\\rho(G_J)$, we must find the eigenvalues $\\lambda$ of $G_J$ by solving the characteristic equation $\\det(G_J - \\lambda I) = 0$.\n$$\n\\det\\begin{pmatrix}\n-\\lambda & 1/2 & 0 \\\\\n1/2 & -\\lambda & 1/2 \\\\\n0 & 1/2 & -\\lambda\n\\end{pmatrix} \\;=\\; 0.\n$$\nExpanding the determinant along the first row gives:\n$$\n-\\lambda \\left( (-\\lambda)(-\\lambda) - (1/2)(1/2) \\right) - \\frac{1}{2} \\left( (1/2)(-\\lambda) - (1/2)(0) \\right) \\;=\\; 0\n$$\n$$\n-\\lambda (\\lambda^2 - 1/4) - \\frac{1}{2} (-\\frac{1}{2}\\lambda) \\;=\\; 0\n$$\n$$\n-\\lambda^3 + \\frac{1}{4}\\lambda + \\frac{1}{4}\\lambda \\;=\\; 0\n$$\n$$\n-\\lambda^3 + \\frac{1}{2}\\lambda \\;=\\; 0\n$$\n$$\n-\\lambda(\\lambda^2 - 1/2) \\;=\\; 0.\n$$\nThe eigenvalues of $G_J$ are $\\lambda_1 = 0$, $\\lambda_2 = \\frac{1}{\\sqrt{2}}$, and $\\lambda_3 = -\\frac{1}{\\sqrt{2}}$. The spectral radius is the maximum of the absolute values of the eigenvalues:\n$$\n\\rho(G_J) \\;=\\; \\max\\left\\{|0|, \\left|\\frac{1}{\\sqrt{2}}\\right|, \\left|-\\frac{1}{\\sqrt{2}}\\right|\\right\\} \\;=\\; \\frac{1}{\\sqrt{2}}.\n$$\n\nNext, we determine the Gauss-Seidel iteration matrix, $G_{GS} = (D-L)^{-1}U$. First, we compute the matrix $D-L$ and its inverse.\n$$\nD-L \\;=\\; \\begin{pmatrix}\n2 & 0 & 0 \\\\\n-1 & 2 & 0 \\\\\n0 & -1 & 2\n\\end{pmatrix}.\n$$\nThe inverse $(D-L)^{-1}$ is found by solving $(D-L)X = I$ for $X$. Since $D-L$ is lower triangular, this can be solved by forward substitution.\n$$\n(D-L)^{-1} \\;=\\; \\begin{pmatrix}\n1/2 & 0 & 0 \\\\\n1/4 & 1/2 & 0 \\\\\n1/8 & 1/4 & 1/2\n\\end{pmatrix}.\n$$\nNow, we can compute $G_{GS}$:\n$$\nG_{GS} \\;=\\; (D-L)^{-1}U \\;=\\; \\begin{pmatrix}\n1/2 & 0 & 0 \\\\\n1/4 & 1/2 & 0 \\\\\n1/8 & 1/4 & 1/2\n\\end{pmatrix} \\begin{pmatrix}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n0 & 1/2 & 0 \\\\\n0 & 1/4 & 1/2 \\\\\n0 & 1/8 & 1/4\n\\end{pmatrix}.\n$$\nTo find the spectral radius $\\rho(G_{GS})$, we find its eigenvalues $\\mu$ by solving $\\det(G_{GS} - \\mu I) = 0$.\n$$\n\\det\\begin{pmatrix}\n-\\mu & 1/2 & 0 \\\\\n0 & 1/4 - \\mu & 1/2 \\\\\n0 & 1/8 & 1/4 - \\mu\n\\end{pmatrix} \\;=\\; 0.\n$$\nExpanding along the first column:\n$$\n-\\mu \\left( (1/4 - \\mu)(1/4 - \\mu) - (1/2)(1/8) \\right) \\;=\\; 0\n$$\n$$\n-\\mu \\left( (1/4 - \\mu)^2 - 1/16 \\right) \\;=\\; 0\n$$\n$$\n-\\mu \\left( 1/16 - 1/2\\mu + \\mu^2 - 1/16 \\right) \\;=\\; 0\n$$\n$$\n-\\mu (\\mu^2 - 1/2\\mu) \\;=\\; 0\n$$\n$$\n-\\mu^2 (\\mu - 1/2) \\;=\\; 0.\n$$\nThe eigenvalues of $G_{GS}$ are $\\mu_1 = 0$ (with algebraic multiplicity $2$) and $\\mu_2 = 1/2$. The spectral radius is:\n$$\n\\rho(G_{GS}) \\;=\\; \\max\\left\\{|0|, |1/2|\\right\\} \\;=\\; \\frac{1}{2}.\n$$\n\nFinally, we must verify that $\\rho(G_{GS}) = \\rho(G_{J})^{2}$. Using the values we have calculated:\n$$\n\\rho(G_J)^2 \\;=\\; \\left(\\frac{1}{\\sqrt{2}}\\right)^2 \\;=\\; \\frac{1}{2}.\n$$\nWe calculated $\\rho(G_{GS}) = \\frac{1}{2}$. Thus, the relation is verified:\n$$\n\\rho(G_{GS}) \\;=\\; \\frac{1}{2} \\;=\\; \\rho(G_J)^2.\n$$\nThe problem asks for the exact value of $\\rho(G_{J})$. This has been determined to be $\\frac{1}{\\sqrt{2}}$.",
            "answer": "$$\\boxed{\\frac{1}{\\sqrt{2}}}$$"
        },
        {
            "introduction": "Many large-scale engineering problems result in matrices with a block structure, where treating entire sub-matrices as single units is more efficient. This advanced practice guides you through the analysis of a block Jacobi method, a generalization of the point-wise scheme. You will derive the spectral radius of the block iteration matrix, learning how the convergence of the global system depends on the properties of its constituent blocks .",
            "id": "2381571",
            "problem": "Consider the linear system $A x = b$ with $A \\in \\mathbb{R}^{mn \\times mn}$ block tridiagonal, consisting of $n \\geq 2$ blocks of size $m \\times m$. Assume the diagonal blocks are all identical and equal to a symmetric positive definite (SPD) matrix $C \\in \\mathbb{R}^{m \\times m}$, and the sub- and super-diagonal blocks are all identical and equal to $-E$ with $E \\in \\mathbb{R}^{m \\times m}$ symmetric. That is, $A$ has the form\n$$\nA \\;=\\; \\begin{pmatrix}\nC & -E &  &  & \\\\\n-E & C & -E &  & \\\\\n & \\ddots & \\ddots & \\ddots & \\\\\n &  & -E & C & -E \\\\\n &  &  & -E & C\n\\end{pmatrix},\n$$\nwith no wrap-around blocks. Let the block Jacobi stationary iteration use the block diagonal of $A$ as the preconditioner, so that the iteration takes the form $x^{k+1} = x^{k} + M^{-1}(b - A x^{k})$, where $M = \\mathrm{diag}(C,\\ldots,C)$.\n\nDerive the block Jacobi iteration matrix $G_{J}$ associated with this splitting, and then, starting from the definitions of the iteration and of the spectral radius, determine a closed-form analytic expression for the spectral radius $\\rho(G_{J})$ in terms of $n$ and the eigenvalues of $C^{-1}E$.\n\nProvide your final answer as a single closed-form analytic expression for $\\rho(G_{J})$. No rounding is required and no units should be reported.",
            "solution": "The problem requires the derivation of the block Jacobi iteration matrix $G_J$ and its spectral radius $\\rho(G_J)$ for a given block tridiagonal system.\n\nFirst, we validate the problem statement.\nThe givens are:\n- A linear system $A x = b$.\n- The matrix $A \\in \\mathbb{R}^{mn \\times mn}$ is block tridiagonal with $n \\geq 2$ blocks of size $m \\times m$:\n$$ A = \\begin{pmatrix} C & -E & & \\\\ -E & C & -E & \\\\ & \\ddots & \\ddots & \\ddots \\\\ & & -E & C & -E \\\\ & & & -E & C \\end{pmatrix} $$\n- The diagonal blocks $C$ are symmetric positive definite (SPD).\n- The off-diagonal blocks are $-E$, where $E$ is symmetric.\n- The block Jacobi iteration is given by $x^{k+1} = x^{k} + M^{-1}(b - A x^{k})$.\n- The preconditioner is the block diagonal of $A$, $M = \\mathrm{diag}(C, \\dots, C)$.\n\nThe problem is scientifically grounded, well-posed, objective, and self-contained. The properties of the matrices are clearly defined and sufficient for the analysis. No flaws are identified. The problem is valid.\n\nWe now proceed with the solution.\n\nThe block Jacobi iteration is defined by the splitting $A = M - N$, where $M$ is the block diagonal of $A$. The iteration is $M x^{k+1} = N x^k + b$.\nThe given iterative form is $x^{k+1} = x^{k} + M^{-1}(b - A x^{k})$. This can be rewritten to identify the iteration matrix $G_J$:\n$$ x^{k+1} = x^{k} - M^{-1}A x^{k} + M^{-1}b = (I - M^{-1}A) x^{k} + M^{-1}b $$\nThus, the iteration matrix is $G_J = I - M^{-1}A$.\n\nThe matrix $A$ is block tridiagonal, so we can write it as $A = D - L - U$, where $D$ is the block diagonal, $-L$ is the strict block lower triangle, and $-U$ is the strict block upper triangle.\nFor our specific matrix $A$:\n$D = \\mathrm{diag}(C, C, \\dots, C) = M$.\n$-L$ and $-U$ contain the $-E$ blocks.\nThe matrix $A$ is composed of an $n \\times n$ grid of blocks, each of size $m \\times m$.\nThe preconditioner is $M = D = \\mathrm{diag}(C, \\dots, C)$. Since $C$ is SPD, it is invertible, and so $M$ is invertible. Its inverse is $M^{-1} = \\mathrm{diag}(C^{-1}, \\dots, C^{-1})$.\n\nThe iteration matrix $G_J$ is:\n$$ G_J = I - M^{-1}A = I - D^{-1}(D - L - U) = I - I + D^{-1}(L+U) = D^{-1}(L+U) $$\nLet's construct this matrix explicitly.\n$$ A = \\begin{pmatrix}\nC & -E & 0 & \\cdots \\\\\n-E & C & -E & \\cdots \\\\\n0 & -E & C & \\ddots \\\\\n\\vdots & \\vdots & \\ddots & \\ddots\n\\end{pmatrix} $$\n$$ M^{-1}A = \\begin{pmatrix}\nC^{-1} & & \\\\\n& C^{-1} & \\\\\n& & \\ddots\n\\end{pmatrix}\n\\begin{pmatrix}\nC & -E & & \\\\\n-E & C & -E & \\\\\n& \\ddots & \\ddots & \\ddots\n\\end{pmatrix}\n= \\begin{pmatrix}\nI_m & -C^{-1}E & & \\\\\n-C^{-1}E & I_m & -C^{-1}E & \\\\\n& \\ddots & \\ddots & \\ddots\n\\end{pmatrix} $$\nwhere $I_m$ is the $m \\times m$ identity matrix.\nThen, the iteration matrix $G_J$ is:\n$$ G_J = I_{mn} - M^{-1}A = \\begin{pmatrix}\n0_m & C^{-1}E & & \\\\\nC^{-1}E & 0_m & C^{-1}E & \\\\\n& \\ddots & \\ddots & \\ddots \\\\\n& & C^{-1}E & 0_m & C^{-1}E \\\\\n& & & C^{-1}E & 0_m\n\\end{pmatrix} $$\nwhere $0_m$ is the $m \\times m$ zero matrix. Let $B = C^{-1}E$. The matrix $G_J$ has the block structure:\n$$ G_J = \\begin{pmatrix}\n0 & B & & & \\\\\nB & 0 & B & & \\\\\n& \\ddots & \\ddots & \\ddots & \\\\\n& & B & 0 & B \\\\\n& & & B & 0\n\\end{pmatrix} $$\n\nTo find the spectral radius $\\rho(G_J)$, we must find the eigenvalues of $G_J$. Let $\\lambda$ be an eigenvalue of $G_J$ and $v$ be the corresponding eigenvector, partitioned into $n$ blocks $v_j \\in \\mathbb{C}^m$ for $j=1, \\dots, n$, such that $v = (v_1^T, v_2^T, \\dots, v_n^T)^T$.\nThe eigenvalue problem $G_J v = \\lambda v$ can be written as a system of block equations:\n\\begin{align*}\n    B v_2 &= \\lambda v_1 \\\\\n    B v_{j-1} + B v_{j+1} &= \\lambda v_j, \\quad \\text{for } j=2, \\dots, n-1 \\\\\n    B v_{n-1} &= \\lambda v_n\n\\end{align*}\nWe can define $v_0 = 0$ and $v_{n+1} = 0$. Then the system of equations can be written as a single recurrence relation for all $j=1, \\dots, n$:\n$$ B v_{j-1} + B v_{j+1} = \\lambda v_j $$\n\nSince $C$ is SPD and $E$ is symmetric, the generalized eigenvalue problem $Eu = \\mu Cu$ has $m$ real eigenvalues $\\mu_k$ and a complete basis of $C$-orthogonal eigenvectors $u_k$. The equation $Eu_k = \\mu_k C u_k$ is equivalent to $C^{-1}Eu_k = \\mu_k u_k$, so the matrix $B=C^{-1}E$ is diagonalizable with real eigenvalues $\\mu_k$.\n\nLet $u_k$ be an eigenvector of $B$ corresponding to an eigenvalue $\\mu_k$. We seek an eigenvector $v$ of $G_J$ of the form $v_j = s_j u_k$ for some scalars $s_j$. Substituting this ansatz into the recurrence relation:\n$$ B(s_{j-1} u_k) + B(s_{j+1} u_k) = \\lambda (s_j u_k) $$\n$$ s_{j-1} B u_k + s_{j+1} B u_k = \\lambda s_j u_k $$\n$$ (s_{j-1} + s_{j+1}) \\mu_k u_k = \\lambda s_j u_k $$\nSince $u_k$ is a non-zero vector, we can divide by it to obtain a scalar recurrence relation for the coefficients $s_j$:\n$$ \\mu_k (s_{j-1} + s_{j+1}) = \\lambda s_j $$\nThis must hold for $j=1, \\dots, n$, with boundary conditions $s_0=0$ and $s_{n+1}=0$.\nIf $\\mu_k = 0$, then $\\lambda=0$. So, $\\lambda=0$ is an eigenvalue of $G_J$ if $B$ is singular.\nIf $\\mu_k \\neq 0$, we can write:\n$$ s_{j-1} + s_{j+1} = \\frac{\\lambda}{\\mu_k} s_j $$\nThis is a standard second-order linear homogeneous recurrence relation. Its characteristic equation is $r^2 - \\frac{\\lambda}{\\mu_k} r + 1 = 0$.\nLet $\\frac{\\lambda}{\\mu_k} = 2 \\cos\\theta$. The roots are $r = \\cos\\theta \\pm i \\sin\\theta = e^{\\pm i\\theta}$.\nThe general solution for $s_j$ is $s_j = c_1 e^{ij\\theta} + c_2 e^{-ij\\theta}$.\nApplying the boundary condition $s_0=0$:\n$s_0 = c_1 + c_2 = 0 \\implies c_2 = -c_1$.\nSo, $s_j = c_1(e^{ij\\theta} - e^{-ij\\theta}) = 2i c_1 \\sin(j\\theta)$. Let $C = 2 i c_1$.\nThe solution is of the form $s_j = C \\sin(j\\theta)$.\nApplying the boundary condition $s_{n+1}=0$:\n$s_{n+1} = C \\sin((n+1)\\theta) = 0$.\nFor a non-trivial solution ($C \\neq 0$), we must have $\\sin((n+1)\\theta)=0$.\nThis implies $(n+1)\\theta = p\\pi$ for some integer $p$.\nThus, $\\theta_p = \\frac{p\\pi}{n+1}$.\nWe can take $p=1, 2, \\dots, n$ to obtain $n$ distinct non-trivial solutions for the vector $(s_1, \\dots, s_n)^T$.\nThe eigenvalues of the scalar problem are given by $\\frac{\\lambda}{\\mu_k} = 2 \\cos(\\theta_p) = 2 \\cos\\left(\\frac{p\\pi}{n+1}\\right)$.\nTherefore, for each eigenvalue $\\mu_k$ of $B=C^{-1}E$, there are $n$ corresponding eigenvalues of $G_J$, given by:\n$$ \\lambda_{k,p} = \\mu_k \\cdot 2 \\cos\\left(\\frac{p\\pi}{n+1}\\right), \\quad k=1, \\dots, m, \\quad p=1, \\dots, n $$\nThe spectral radius of $G_J$ is the maximum of the absolute values of its eigenvalues:\n$$ \\rho(G_J) = \\max_{k,p} |\\lambda_{k,p}| = \\max_{k,p} \\left| \\mu_k \\cdot 2 \\cos\\left(\\frac{p\\pi}{n+1}\\right) \\right| $$\nThis can be separated into two maximization problems:\n$$ \\rho(G_J) = \\left( \\max_{k=1,\\dots,m} |\\mu_k| \\right) \\cdot \\left( \\max_{p=1,\\dots,n} \\left| 2 \\cos\\left(\\frac{p\\pi}{n+1}\\right) \\right| \\right) $$\nThe first term is, by definition, the spectral radius of $B=C^{-1}E$:\n$$ \\max_{k=1,\\dots,m} |\\mu_k| = \\rho(C^{-1}E) $$\nFor the second term, we need to find the maximum of $\\left| 2 \\cos\\left(\\frac{p\\pi}{n+1}\\right) \\right|$ for $p=1, \\dots, n$.\nThe argument of the cosine function, $\\frac{p\\pi}{n+1}$, ranges from $\\frac{\\pi}{n+1}$ (for $p=1$) to $\\frac{n\\pi}{n+1}$ (for $p=n$). This range is within $(0, \\pi)$.\nThe function $|\\cos(x)|$ on $(0, \\pi)$ is symmetric about $x=\\pi/2$ and its maximum values are approached as $x \\to 0$ or $x \\to \\pi$.\nThe value of $p$ that makes $\\frac{p\\pi}{n+1}$ closest to $0$ is $p=1$.\nThe value of $p$ that makes $\\frac{p\\pi}{n+1}$ closest to $\\pi$ is $p=n$, which gives $\\frac{n\\pi}{n+1} = \\pi - \\frac{\\pi}{n+1}$.\nWe have $\\cos\\left(\\frac{n\\pi}{n+1}\\right) = \\cos\\left(\\pi - \\frac{\\pi}{n+1}\\right) = -\\cos\\left(\\frac{\\pi}{n+1}\\right)$.\nTherefore, $\\left|\\cos\\left(\\frac{n\\pi}{n+1}\\right)\\right| = \\left|\\cos\\left(\\frac{\\pi}{n+1}\\right)\\right| = \\cos\\left(\\frac{\\pi}{n+1}\\right)$, since $n \\ge 2$ implies $\\frac{\\pi}{n+1} \\in (0, \\pi/2)$.\nThe maximum absolute value is achieved for $p=1$ and $p=n$, and its value is $\\cos\\left(\\frac{\\pi}{n+1}\\right)$.\nSo,\n$$ \\max_{p=1,\\dots,n} \\left| 2 \\cos\\left(\\frac{p\\pi}{n+1}\\right) \\right| = 2 \\cos\\left(\\frac{\\pi}{n+1}\\right) $$\nCombining the terms, we get the final expression for the spectral radius:\n$$ \\rho(G_J) = \\rho(C^{-1}E) \\cdot 2 \\cos\\left(\\frac{\\pi}{n+1}\\right) $$\nThe problem asks for the answer in terms of $n$ and the eigenvalues of $C^{-1}E$, and $\\rho(C^{-1}E)$ is defined as the maximum modulus of these eigenvalues. This expression is the required closed-form analytic result.",
            "answer": "$$\\boxed{2\\rho(C^{-1}E)\\cos\\left(\\frac{\\pi}{n+1}\\right)}$$"
        }
    ]
}