{
    "hands_on_practices": [
        {
            "introduction": "Theory comes to life through implementation. This first practice invites you to write a program to solve linear systems using the Jacobi and Gauss-Seidel methods. By applying these algorithms to matrices with varying degrees of diagonal dominance, you will gain a tangible understanding of how a matrix's structure impacts convergence speed. This exercise directly connects the abstract concept of the spectral radius to the concrete number of iterations required to reach a solution, building a strong intuition for performance analysis .",
            "id": "2406932",
            "problem": "You must write a complete, runnable program that evaluates linear stationary iterations for specified linear systems. Consider square, real, symmetric matrices of size $n \\times n$ with right-hand side vector $\\mathbf{b} \\in \\mathbb{R}^n$. For each system, starting from the zero vector $\\mathbf{x}^{(0)} = \\mathbf{0}$, iterate until the infinity norm of the residual $\\|\\mathbf{r}^{(k)}\\|_{\\infty} = \\|\\mathbf{b} - A \\mathbf{x}^{(k)}\\|_{\\infty}$ is less than or equal to a tolerance $\\tau$, or until a maximum number of iterations $k_{\\max}$ is reached. Use the standard definitions of the Jacobi method and the Gauss–Seidel method. For each specified matrix, report the minimal iteration counts required by the Jacobi and Gauss–Seidel methods to satisfy the residual tolerance, and the spectral radius of the Jacobi iteration matrix. All computations are purely numerical, and no physical units are involved.\n\nUse the following parameters, which constitute the test suite:\n\n- Dimension $n = 6$.\n- Right-hand side $\\mathbf{b} = [1,2,3,4,5,6]^T$.\n- Initial guess $\\mathbf{x}^{(0)} = \\mathbf{0}$.\n- Residual tolerance $\\tau = 10^{-8}$.\n- Maximum iterations $k_{\\max} = 20000$.\n\nDefine three matrices $A \\in \\mathbb{R}^{6 \\times 6}$ as follows:\n\n1) Strongly diagonally dominant, dense with constant off-diagonals:\n- Parameters $\\alpha_{\\mathrm{s}} = 20.0$, $\\gamma = -1.0$.\n- Entries:\n  - $A_{ii} = \\alpha_{\\mathrm{s}}$ for all $i \\in \\{1,\\dots,6\\}$.\n  - $A_{ij} = \\gamma$ for all $i \\neq j$.\n\n2) Just-barely diagonally dominant, dense with constant off-diagonals:\n- Parameters $\\alpha_{\\mathrm{w}} = 5.1$, $\\gamma = -1.0$.\n- Entries:\n  - $A_{ii} = \\alpha_{\\mathrm{w}}$ for all $i \\in \\{1,\\dots,6\\}$.\n  - $A_{ij} = \\gamma$ for all $i \\neq j$.\n\n3) Edge case, symmetric tridiagonal (one-dimensional discrete Laplacian form):\n- Entries:\n  - $A_{ii} = 2.0$ for all $i \\in \\{1,\\dots,6\\}$.\n  - $A_{i,i+1} = A_{i+1,i} = -1.0$ for all $i \\in \\{1,\\dots,5\\}$.\n  - All other off-diagonal entries are $0.0$.\n\nFor each of the three matrices, do the following:\n- Using the Jacobi method, determine the minimal iteration count $k_{\\mathrm{J}}$ such that $\\|\\mathbf{b} - A \\mathbf{x}^{(k_{\\mathrm{J}})}\\|_{\\infty} \\le \\tau$, or return $k_{\\max}$ if such $k_{\\mathrm{J}}$ is not achieved within $k_{\\max}$ iterations.\n- Using the Gauss–Seidel method, determine the minimal iteration count $k_{\\mathrm{GS}}$ such that $\\|\\mathbf{b} - A \\mathbf{x}^{(k_{\\mathrm{GS}})}\\|_{\\infty} \\le \\tau$, or return $k_{\\max}$ if such $k_{\\mathrm{GS}}$ is not achieved within $k_{\\max}$ iterations.\n- For the Jacobi method, compute the spectral radius $\\rho_{\\mathrm{J}}$ of its iteration matrix. Report $\\rho_{\\mathrm{J}}$ rounded to six decimal places.\n\nYour program should produce a single line of output containing the nine results in the following order:\n- For the strongly diagonally dominant matrix: $k_{\\mathrm{J}}$, $k_{\\mathrm{GS}}$, $\\rho_{\\mathrm{J}}$ (rounded to six decimal places).\n- For the just-barely diagonally dominant matrix: $k_{\\mathrm{J}}$, $k_{\\mathrm{GS}}$, $\\rho_{\\mathrm{J}}$ (rounded to six decimal places).\n- For the tridiagonal edge-case matrix: $k_{\\mathrm{J}}$, $k_{\\mathrm{GS}}$, $\\rho_{\\mathrm{J}}$ (rounded to six decimal places).\n\nThe final output format must be a single line that is a comma-separated list enclosed in square brackets, for example\n$[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_9]$,\nwhere $k_{\\mathrm{J}}$ and $k_{\\mathrm{GS}}$ are integers and each $\\rho_{\\mathrm{J}}$ is a floating-point number rounded to six decimal places.",
            "solution": "The problem is subjected to validation and is determined to be valid. It is scientifically grounded in the field of numerical linear algebra, well-posed with all necessary parameters defined, and objective in its formulation. The problem asks for the implementation and evaluation of two fundamental linear stationary iterative methods, Jacobi and Gauss–Seidel, for solving a system of linear equations $A\\mathbf{x} = \\mathbf{b}$.\n\nThese methods are based on splitting the matrix $A$ into its constituent parts. A square matrix $A$ can be decomposed as $A = D + L + U$, where $D$ is a diagonal matrix containing the diagonal elements of $A$, $L$ is a strictly lower triangular matrix, and $U$ is a strictly upper triangular matrix. The system $A\\mathbf{x} = \\mathbf{b}$ can thus be written as $(D+L+U)\\mathbf{x} = \\mathbf{b}$.\n\n**Jacobi Method**\n\nThe Jacobi method rearranges the system as $D\\mathbf{x} = \\mathbf{b} - (L+U)\\mathbf{x}$. This leads to the iterative scheme:\n$$ D\\mathbf{x}^{(k+1)} = \\mathbf{b} - (L+U)\\mathbf{x}^{(k)} $$\nAssuming $D$ is invertible (i.e., no zero diagonal elements, which is true for all matrices in this problem), we obtain the iteration formula:\n$$ \\mathbf{x}^{(k+1)} = D^{-1}(\\mathbf{b} - (L+U)\\mathbf{x}^{(k)}) $$\nThis can be computed component-wise for each element $i$ of the vector $\\mathbf{x}^{(k+1)}$:\n$$ x_i^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j=1, j \\neq i}^{n} A_{ij} x_j^{(k)} \\right) $$\nA key characteristic of the Jacobi method is that the computation of each component $x_i^{(k+1)}$ depends only on the components of the vector from the previous iteration, $\\mathbf{x}^{(k)}$. This allows for parallel computation of the new vector components.\n\n**Gauss–Seidel Method**\n\nThe Gauss–Seidel method aims to improve the convergence rate by using the most up-to-date information available. It rearranges the system as $(D+L)\\mathbf{x} = \\mathbf{b} - U\\mathbf{x}$, leading to the iterative scheme:\n$$ (D+L)\\mathbf{x}^{(k+1)} = \\mathbf{b} - U\\mathbf{x}^{(k)} $$\nThis yields the iteration formula:\n$$ \\mathbf{x}^{(k+1)} = (D+L)^{-1}(\\mathbf{b} - U\\mathbf{x}^{(k)}) $$\nIn practice, this is implemented as a forward substitution. The component-wise formula is:\n$$ x_i^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} A_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^{n} A_{ij} x_j^{(k)} \\right) $$\nNotice that for computing $x_i^{(k+1)}$, we use the newly computed components $x_j^{(k+1)}$ for $j  i$ from the current iteration $k+1$, and the old components $x_j^{(k)}$ for $j  i$ from the previous iteration $k$. This sequential dependency means the components must be updated in order.\n\n**Convergence and Spectral Radius**\n\nAny linear stationary iteration can be written in the form $\\mathbf{x}^{(k+1)} = T \\mathbf{x}^{(k)} + \\mathbf{c}$, where $T$ is the iteration matrix. The method is guaranteed to converge for any initial guess $\\mathbf{x}^{(0)}$ if and only if the spectral radius of the iteration matrix, $\\rho(T)$, is strictly less than $1$. The spectral radius is defined as the maximum absolute value of the eigenvalues of $T$, i.e., $\\rho(T) = \\max_i |\\lambda_i(T)|$.\n\nFor the Jacobi method, the iteration matrix $T_J$ is given by:\n$$ T_J = -D^{-1}(L+U) = I - D^{-1}A $$\nThe spectral radius $\\rho(T_J)$ dictates the convergence of the Jacobi method. A smaller spectral radius implies a faster asymptotic rate of convergence. The problem requires the calculation of this value.\n\n**Implementation Strategy**\n\nThe solution will be implemented in Python using the `numpy` library.\n1.  **Matrix Construction**: The three specified matrices, $A_1$ (strongly diagonally dominant), $A_2$ (just-barely diagonally dominant), and $A_3$ (tridiagonal), will be constructed as `numpy` arrays. The problem parameters $n=6$, $\\mathbf{b}=[1,2,3,4,5,6]^T$, $\\mathbf{x}^{(0)}=\\mathbf{0}$, $\\tau=10^{-8}$, and $k_{\\max}=20000$ will be defined.\n2.  **Iterative Solvers**: Functions for the Jacobi and Gauss–Seidel methods will be implemented. Each function will take a matrix $A$, vector $\\mathbf{b}$, initial guess $\\mathbf{x}^{(0)}$, tolerance $\\tau$, and maximum iterations $k_{\\max}$ as input. The loop will run from $k=1$ to $k_{\\max}$, updating the solution vector $\\mathbf{x}$ at each step. After each update, the infinity norm of the residual, $\\|\\mathbf{r}^{(k)}\\|_{\\infty} = \\|\\mathbf{b} - A\\mathbf{x}^{(k)}\\|_{\\infty}$, will be checked against the tolerance $\\tau$. If the condition is met, the current iteration count $k$ is returned. If the loop completes without convergence, $k_{\\max}$ is returned. An initial check for $k=0$ is also performed.\n3.  **Spectral Radius Calculation**: A function will compute the Jacobi iteration matrix $T_J = I - D^{-1}A$. The eigenvalues of $T_J$ will be found using `numpy.linalg.eigvals`, and the spectral radius will be the maximum of their absolute values.\n4.  **Execution and Output**: The main part of the program will iterate through the three test cases (matrices). For each case, it will call the solver functions to get the iteration counts $k_J$ and $k_{GS}$, and the spectral radius function for $\\rho_J$. The results will be collected and formatted into a single string as specified in the problem statement.",
            "answer": "```python\nimport numpy as np\n\ndef jacobi(A: np.ndarray, b: np.ndarray, x0: np.ndarray, tol: float, k_max: int) -> int:\n    \"\"\"\n    Solves the system Ax=b using the Jacobi method.\n\n    Args:\n        A: The n x n coefficient matrix.\n        b: The n x 1 right-hand side vector.\n        x0: The initial guess vector.\n        tol: The residual tolerance.\n        k_max: The maximum number of iterations.\n\n    Returns:\n        The number of iterations required for convergence.\n    \"\"\"\n    n = A.shape[0]\n    x = x0.copy()\n\n    # Check for k=0\n    residual_norm = np.linalg.norm(b - A @ x, np.inf)\n    if residual_norm = tol:\n        return 0\n\n    D = np.diag(A)\n    R = A - np.diag(D)  # R = L + U\n\n    for k in range(1, k_max + 1):\n        x_new = (b - R @ x) / D\n        x = x_new\n        residual_norm = np.linalg.norm(b - A @ x, np.inf)\n        if residual_norm = tol:\n            return k\n    \n    return k_max\n\ndef gauss_seidel(A: np.ndarray, b: np.ndarray, x0: np.ndarray, tol: float, k_max: int) -> int:\n    \"\"\"\n    Solves the system Ax=b using the Gauss-Seidel method.\n\n    Args:\n        A: The n x n coefficient matrix.\n        b: The n x 1 right-hand side vector.\n        x0: The initial guess vector.\n        tol: The residual tolerance.\n        k_max: The maximum number of iterations.\n\n    Returns:\n        The number of iterations required for convergence.\n    \"\"\"\n    n = A.shape[0]\n    x = x0.copy()\n\n    # Check for k=0\n    residual_norm = np.linalg.norm(b - A @ x, np.inf)\n    if residual_norm = tol:\n        return 0\n\n    for k in range(1, k_max + 1):\n        x_old = x.copy()\n        for i in range(n):\n            sum1 = np.dot(A[i, :i], x[:i])\n            sum2 = np.dot(A[i, i + 1:], x_old[i + 1:])\n            x[i] = (b[i] - sum1 - sum2) / A[i, i]\n        \n        residual_norm = np.linalg.norm(b - A @ x, np.inf)\n        if residual_norm = tol:\n            return k\n            \n    return k_max\n\ndef get_spectral_radius_J(A: np.ndarray) -> float:\n    \"\"\"\n    Computes the spectral radius of the Jacobi iteration matrix.\n\n    Args:\n        A: The n x n coefficient matrix.\n\n    Returns:\n        The spectral radius of the Jacobi matrix T_J.\n    \"\"\"\n    D = np.diag(np.diag(A))\n    D_inv = np.linalg.inv(D)\n    T_J = np.eye(A.shape[0]) - D_inv @ A\n    eigenvalues = np.linalg.eigvals(T_J)\n    spectral_radius = np.max(np.abs(eigenvalues))\n    return spectral_radius\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Global parameters\n    n = 6\n    b = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n    x0 = np.zeros(n)\n    tol = 1e-8\n    k_max = 20000\n\n    # Define the three matrices\n    test_cases = []\n\n    # Case 1: Strongly diagonally dominant\n    alpha_s = 20.0\n    gamma1 = -1.0\n    A1 = np.full((n, n), gamma1)\n    np.fill_diagonal(A1, alpha_s)\n    test_cases.append(A1)\n\n    # Case 2: Just-barely diagonally dominant\n    alpha_w = 5.1\n    gamma2 = -1.0\n    A2 = np.full((n, n), gamma2)\n    np.fill_diagonal(A2, alpha_w)\n    test_cases.append(A2)\n\n    # Case 3: Tridiagonal edge case\n    A3 = 2.0 * np.eye(n) - np.eye(n, k=1) - np.eye(n, k=-1)\n    test_cases.append(A3)\n\n    results = []\n    for A in test_cases:\n        k_J = jacobi(A, b, x0, tol, k_max)\n        k_GS = gauss_seidel(A, b, x0, tol, k_max)\n        rho_J = get_spectral_radius_J(A)\n\n        results.append(k_J)\n        results.append(k_GS)\n        results.append(round(rho_J, 6))\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In our first exercise, you likely observed that the Gauss-Seidel method often outperforms the Jacobi method. This practice moves from numerical observation to analytical proof by examining a matrix that is fundamental in computational engineering: the discrete 1D Laplacian. You will derive the spectral radii for both iteration matrices and confirm the elegant relationship $\\rho(G_{GS}) = \\rho(G_{J})^{2}$. This demonstrates a powerful theoretical result for a large and important class of \"consistently ordered\" matrices, providing a solid mathematical foundation for the convergence behavior you observed earlier .",
            "id": "2381617",
            "problem": "You are to work with stationary iterations for a linear system in the context of computational engineering. Consider the $3 \\times 3$ linear system $A x = b$ with\n$$\nA \\;=\\; \\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix}.\n$$\nUse the standard splittings for Jacobi and Gauss-Seidel methods defined by $A = D - L - U$, where $D$ is the diagonal of $A$, $L$ is the strictly lower triangular part of $A$ with nonnegative entries, and $U$ is the strictly upper triangular part of $A$ with nonnegative entries. The corresponding iteration matrices are $G_{J} = D^{-1}(L + U)$ for the Jacobi method and $G_{GS} = (D - L)^{-1} U$ for the Gauss-Seidel method. The matrix $A$ is symmetric positive definite (SPD). Determine the spectral radii $\\rho(G_{J})$ and $\\rho(G_{GS})$, verify that $\\rho(G_{GS}) = \\rho(G_{J})^{2}$, and provide the exact value of $\\rho(G_{J})$ as your final answer. Express the final answer as a single exact expression. No rounding is required.",
            "solution": "The problem statement is subjected to validation.\n\nStep $1$: Extract Givens.\n- A linear system $A x = b$.\n- The matrix $A$ is given as:\n$$\nA \\;=\\; \\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix}.\n$$\n- Standard splittings for Jacobi and Gauss-Seidel methods are defined by $A = D - L - U$.\n- $D$ is the diagonal of $A$.\n- $L$ is the strictly lower triangular part of $A$ with nonnegative entries.\n- $U$ is the strictly upper triangular part of $A$ with nonnegative entries.\n- Jacobi iteration matrix: $G_{J} = D^{-1}(L + U)$.\n- Gauss-Seidel iteration matrix: $G_{GS} = (D - L)^{-1} U$.\n- The matrix $A$ is stated to be symmetric positive definite (SPD).\n- The task is to determine the spectral radii $\\rho(G_{J})$ and $\\rho(G_{GS})$, verify that $\\rho(G_{GS}) = \\rho(G_{J})^{2}$, and provide the exact value of $\\rho(G_{J})$.\n\nStep $2$: Validate Using Extracted Givens.\n- **Scientifically Grounded**: The problem is a standard exercise in numerical linear algebra, specifically concerning the convergence analysis of stationary iterative methods. The matrix $A$ is a well-known example of a symmetric tridiagonal matrix arising from the finite difference discretization of the one-dimensional Laplacian operator. It is indeed symmetric and positive definite. The definitions of the iteration matrices are standard. The relationship $\\rho(G_{GS}) = \\rho(G_J)^2$ is a known theorem for consistently ordered matrices, which $A$ is. The problem is scientifically sound.\n- **Well-Posed**: The problem is clearly defined, with all necessary information provided to compute the required quantities. The objectives are specific and lead to a unique solution.\n- **Objective**: The problem is stated using precise mathematical language, free from any subjectivity or ambiguity.\n\nStep $3$: Verdict and Action.\nThe problem is valid as it is scientifically grounded, well-posed, and objective. A complete solution will be provided.\n\nThe solution proceeds as follows. First, we decompose the matrix $A$ into its components $D$, $L$, and $U$. Then, we construct the iteration matrices $G_J$ and $G_{GS}$. We then find the eigenvalues of each iteration matrix to determine their spectral radii. Finally, we verify the required relationship.\n\nThe given matrix is\n$$\nA \\;=\\; \\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix}.\n$$\nAccording to the definitions provided, the splitting $A = D - L - U$ is performed. The diagonal matrix $D$ is\n$$\nD \\;=\\; \\begin{pmatrix}\n2  0  0 \\\\\n0  2  0 \\\\\n0  0  2\n\\end{pmatrix}.\n$$\nThe matrices $L$ and $U$ are the strictly lower and upper triangular parts of $A$ with entries negated to be nonnegative, as per the definition.\n$$\nL \\;=\\; \\begin{pmatrix}\n0  0  0 \\\\\n1  0  0 \\\\\n0  1  0\n\\end{pmatrix}, \\quad U \\;=\\; \\begin{pmatrix}\n0  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix}.\n$$\nWe can verify this decomposition: $D-L-U = \\begin{pmatrix} 2  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  2 \\end{pmatrix} = A$. The decomposition is correct.\n\nNow, we determine the Jacobi iteration matrix, $G_J = D^{-1}(L+U)$.\nThe inverse of $D$ is $D^{-1} = \\frac{1}{2}I = \\begin{pmatrix} 1/2  0  0 \\\\ 0  1/2  0 \\\\ 0  0  1/2 \\end{pmatrix}$.\nThe sum $L+U$ is\n$$\nL+U \\;=\\; \\begin{pmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  0\n\\end{pmatrix}.\n$$\nTherefore, the Jacobi iteration matrix is\n$$\nG_J \\;=\\; D^{-1}(L+U) \\;=\\; \\frac{1}{2} \\begin{pmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  0\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n0  1/2  0 \\\\\n1/2  0  1/2 \\\\\n0  1/2  0\n\\end{pmatrix}.\n$$\nTo find the spectral radius $\\rho(G_J)$, we must find the eigenvalues $\\lambda$ of $G_J$ by solving the characteristic equation $\\det(G_J - \\lambda I) = 0$.\n$$\n\\det\\begin{pmatrix}\n-\\lambda  1/2  0 \\\\\n1/2  -\\lambda  1/2 \\\\\n0  1/2  -\\lambda\n\\end{pmatrix} \\;=\\; 0.\n$$\nExpanding the determinant along the first row gives:\n$$\n-\\lambda \\left( (-\\lambda)(-\\lambda) - (1/2)(1/2) \\right) - \\frac{1}{2} \\left( (1/2)(-\\lambda) - (1/2)(0) \\right) \\;=\\; 0\n$$\n$$\n-\\lambda (\\lambda^2 - 1/4) - \\frac{1}{2} (-\\frac{1}{2}\\lambda) \\;=\\; 0\n$$\n$$\n-\\lambda^3 + \\frac{1}{4}\\lambda + \\frac{1}{4}\\lambda \\;=\\; 0\n$$\n$$\n-\\lambda^3 + \\frac{1}{2}\\lambda \\;=\\; 0\n$$\n$$\n-\\lambda(\\lambda^2 - 1/2) \\;=\\; 0.\n$$\nThe eigenvalues of $G_J$ are $\\lambda_1 = 0$, $\\lambda_2 = \\frac{1}{\\sqrt{2}}$, and $\\lambda_3 = -\\frac{1}{\\sqrt{2}}$. The spectral radius is the maximum of the absolute values of the eigenvalues:\n$$\n\\rho(G_J) \\;=\\; \\max\\left\\{|0|, \\left|\\frac{1}{\\sqrt{2}}\\right|, \\left|-\\frac{1}{\\sqrt{2}}\\right|\\right\\} \\;=\\; \\frac{1}{\\sqrt{2}}.\n$$\n\nNext, we determine the Gauss-Seidel iteration matrix, $G_{GS} = (D-L)^{-1}U$. First, we compute the matrix $D-L$ and its inverse.\n$$\nD-L \\;=\\; \\begin{pmatrix}\n2  0  0 \\\\\n-1  2  0 \\\\\n0  -1  2\n\\end{pmatrix}.\n$$\nThe inverse $(D-L)^{-1}$ is found by solving $(D-L)X = I$ for $X$. Since $D-L$ is lower triangular, this can be solved by forward substitution.\n$$\n(D-L)^{-1} \\;=\\; \\begin{pmatrix}\n1/2  0  0 \\\\\n1/4  1/2  0 \\\\\n1/8  1/4  1/2\n\\end{pmatrix}.\n$$\nNow, we can compute $G_{GS}$:\n$$\nG_{GS} \\;=\\; (D-L)^{-1}U \\;=\\; \\begin{pmatrix}\n1/2  0  0 \\\\\n1/4  1/2  0 \\\\\n1/8  1/4  1/2\n\\end{pmatrix} \\begin{pmatrix}\n0  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n0  1/2  0 \\\\\n0  1/4  1/2 \\\\\n0  1/8  1/4\n\\end{pmatrix}.\n$$\nTo find the spectral radius $\\rho(G_{GS})$, we find its eigenvalues $\\mu$ by solving $\\det(G_{GS} - \\mu I) = 0$.\n$$\n\\det\\begin{pmatrix}\n-\\mu  1/2  0 \\\\\n0  1/4 - \\mu  1/2 \\\\\n0  1/8  1/4 - \\mu\n\\end{pmatrix} \\;=\\; 0.\n$$\nExpanding along the first column:\n$$\n-\\mu \\left( (1/4 - \\mu)(1/4 - \\mu) - (1/2)(1/8) \\right) \\;=\\; 0\n$$\n$$\n-\\mu \\left( (1/4 - \\mu)^2 - 1/16 \\right) \\;=\\; 0\n$$\n$$\n-\\mu \\left( 1/16 - 1/2\\mu + \\mu^2 - 1/16 \\right) \\;=\\; 0\n$$\n$$\n-\\mu (\\mu^2 - 1/2\\mu) \\;=\\; 0\n$$\n$$\n-\\mu^2 (\\mu - 1/2) \\;=\\; 0.\n$$\nThe eigenvalues of $G_{GS}$ are $\\mu_1 = 0$ (with algebraic multiplicity $2$) and $\\mu_2 = 1/2$. The spectral radius is:\n$$\n\\rho(G_{GS}) \\;=\\; \\max\\left\\{|0|, |1/2|\\right\\} \\;=\\; \\frac{1}{2}.\n$$\n\nFinally, we must verify that $\\rho(G_{GS}) = \\rho(G_{J})^{2}$. Using the values we have calculated:\n$$\n\\rho(G_J)^2 \\;=\\; \\left(\\frac{1}{\\sqrt{2}}\\right)^2 \\;=\\; \\frac{1}{2}.\n$$\nWe calculated $\\rho(G_{GS}) = \\frac{1}{2}$. Thus, the relation is verified:\n$$\n\\rho(G_{GS}) \\;=\\; \\frac{1}{2} \\;=\\; \\rho(G_J)^2.\n$$\nThe problem asks for the exact value of $\\rho(G_{J})$. This has been determined to be $\\frac{1}{\\sqrt{2}}$.",
            "answer": "$$\\boxed{\\frac{1}{\\sqrt{2}}}$$"
        },
        {
            "introduction": "To master a concept, we must also understand its boundaries. While Gauss-Seidel often converges faster than Jacobi for many common problems, this is not a universal law. This final practice presents a carefully constructed system that serves as an important counterexample: one where the Jacobi method converges, but the Gauss-Seidel method diverges. By working through this analysis , you will reinforce the critical lesson that the spectral radius criterion, $\\rho(T)  1$, is the ultimate and only arbiter of convergence, cautioning against making broad generalizations from specific cases.",
            "id": "2381627",
            "problem": "Consider the stationary iterations defined by a matrix splitting for a linear system of equations. Let the system be $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ with a nonsingular matrix $\\mathbf{A}\\in\\mathbb{R}^{3\\times 3}$ and the standard splitting $\\mathbf{A}=\\mathbf{D}-\\mathbf{L}-\\mathbf{U}$, where $\\mathbf{D}$ is the diagonal of $\\mathbf{A}$, $\\mathbf{L}$ is the strictly lower triangular part, and $\\mathbf{U}$ is the strictly upper triangular part. The Jacobi iteration and the Gauss–Seidel iteration are given by\n$$\n\\mathbf{x}^{(k+1)}=\\mathbf{D}^{-1}\\big(\\mathbf{L}+\\mathbf{U}\\big)\\mathbf{x}^{(k)}+\\mathbf{D}^{-1}\\mathbf{b}\n\\quad\\text{and}\\quad\n\\mathbf{x}^{(k+1)}=(\\mathbf{D}-\\mathbf{L})^{-1}\\mathbf{U}\\,\\mathbf{x}^{(k)}+(\\mathbf{D}-\\mathbf{L})^{-1}\\mathbf{b},\n$$\nrespectively. Their convergence is governed by the spectral radii of the iteration matrices $\\mathbf{T}_{J}=-\\mathbf{D}^{-1}(\\mathbf{L}+\\mathbf{U})$ and $\\mathbf{T}_{GS}=-(\\mathbf{D}-\\mathbf{L})^{-1}\\mathbf{U}$.\n\nUsing only these core definitions and the spectral radius criterion for convergence, do the following:\n\n1. Design a concrete $3\\times 3$ system (i.e., specify a particular matrix $\\mathbf{A}$ with real entries and a nonsingular diagonal $\\mathbf{D}$) for which the Jacobi iteration converges but the Gauss–Seidel iteration diverges. Your design must be justified analytically from the definitions above, not by appeal to any prepackaged theorems about special matrix classes.\n\n2. Verify your design by deriving the characteristic polynomial of $\\mathbf{T}_{J}$ and showing that its spectral radius is strictly less than $1$.\n\n3. For the same system, derive the characteristic polynomial of the nontrivial part of $\\mathbf{T}_{GS}$ and show that its spectral radius is strictly greater than $1$.\n\nTo make the verification concrete, instantiate your design with the specific choice\n$$\n\\mathbf{A}=\\begin{bmatrix}\n1  1  \\sqrt{2} \\\\\n1  1  1 \\\\\n-\\sqrt{2}  1  1\n\\end{bmatrix}.\n$$\n\nCompute, in exact closed form, the spectral radius of the Gauss–Seidel iteration matrix $\\mathbf{T}_{GS}$ for this $\\mathbf{A}$. Your final answer must be a single exact analytic expression (no units, no rounding).",
            "solution": "The problem statement has been validated. It is scientifically grounded in the theory of iterative methods for linear systems. However, a significant flaw must be addressed. The problem provides contradictory definitions for the iteration matrices. The Jacobi iteration is stated as\n$$\n\\mathbf{x}^{(k+1)}=\\mathbf{D}^{-1}\\big(\\mathbf{L}+\\mathbf{U}\\big)\\mathbf{x}^{(k)}+\\mathbf{D}^{-1}\\mathbf{b}\n$$\nwhich implies that the iteration matrix is $\\mathbf{T}_{J} = \\mathbf{D}^{-1}(\\mathbf{L}+\\mathbf{U})$. Immediately following this, the problem defines the matrix as $\\mathbf{T}_{J}=-\\mathbf{D}^{-1}(\\mathbf{L}+\\mathbf{U})$. These statements are contradictory. A similar contradiction exists for the Gauss-Seidel method, where the iteration formula implies $\\mathbf{T}_{GS}=(\\mathbf{D}-\\mathbf{L})^{-1}\\mathbf{U}$, but the explicit definition states $\\mathbf{T}_{GS}=-(\\mathbf{D}-\\mathbf{L})^{-1}\\mathbf{U}$.\n\nSuch carelessness in a problem formulation is unacceptable. However, the core question of convergence depends on the spectral radius, $\\rho(\\mathbf{T}) = \\max_i |\\lambda_i(\\mathbf{T})|$. Since the eigenvalues of a matrix $-\\mathbf{T}$ are the negatives of the eigenvalues of $\\mathbf{T}$, their absolute values are identical. Thus, $\\rho(\\mathbf{T}) = \\rho(-\\mathbf{T})$. The contradiction, while a serious defect, does not affect the calculation of the spectral radii and the subsequent convergence analysis. I will proceed by adopting the definitions that are consistent with the derivation from the matrix splitting $\\mathbf{A}=\\mathbf{D}-\\mathbf{L}-\\mathbf{U}$, namely $\\mathbf{T}_J = \\mathbf{D}^{-1}(\\mathbf{L}+\\mathbf{U})$ and $\\mathbf{T}_{GS} = (\\mathbf{D}-\\mathbf{L})^{-1}\\mathbf{U}$. The problem then becomes a verification exercise for the provided matrix $\\mathbf{A}$.\n\nThe given system is defined by the matrix:\n$$\n\\mathbf{A}=\\begin{bmatrix}\n1  1  \\sqrt{2} \\\\\n1  1  1 \\\\\n-\\sqrt{2}  1  1\n\\end{bmatrix}\n$$\nWe perform the splitting $\\mathbf{A}=\\mathbf{D}-\\mathbf{L}-\\mathbf{U}$, where $\\mathbf{D}$ is diagonal, $\\mathbf{L}$ is strictly lower triangular, and $\\mathbf{U}$ is strictly upper triangular. The definitions imply that $\\mathbf{L}$ and $\\mathbf{U}$ are the negatives of the strictly lower and upper triangular parts of $\\mathbf{A}$, respectively.\n$$\n\\mathbf{D} = \\begin{bmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{bmatrix} = \\mathbf{I}\n$$\n$$\n\\mathbf{L} = -\\begin{bmatrix}\n0  0  0 \\\\\n1  0  0 \\\\\n-\\sqrt{2}  1  0\n\\end{bmatrix} = \\begin{bmatrix}\n0  0  0 \\\\\n-1  0  0 \\\\\n\\sqrt{2}  -1  0\n\\end{bmatrix}\n$$\n$$\n\\mathbf{U} = -\\begin{bmatrix}\n0  1  \\sqrt{2} \\\\\n0  0  1 \\\\\n0  0  0\n\\end{bmatrix} = \\begin{bmatrix}\n0  -1  -\\sqrt{2} \\\\\n0  0  -1 \\\\\n0  0  0\n\\end{bmatrix}\n$$\nWith these, $\\mathbf{D}-\\mathbf{L}-\\mathbf{U}$ correctly reconstructs $\\mathbf{A}$.\n\nFirst, we analyze the Jacobi iteration. The iteration matrix is $\\mathbf{T}_{J} = \\mathbf{D}^{-1}(\\mathbf{L}+\\mathbf{U})$. Since $\\mathbf{D}=\\mathbf{I}$, this simplifies to $\\mathbf{T}_{J} = \\mathbf{L}+\\mathbf{U}$:\n$$\n\\mathbf{T}_{J} = \\begin{bmatrix}\n0  -1  -\\sqrt{2} \\\\\n-1  0  -1 \\\\\n\\sqrt{2}  -1  0\n\\end{bmatrix}\n$$\nTo find the spectral radius, we derive the characteristic polynomial $p(\\lambda) = \\det(\\mathbf{T}_{J} - \\lambda\\mathbf{I})$:\n$$\np(\\lambda) = \\det\\begin{pmatrix}\n-\\lambda  -1  -\\sqrt{2} \\\\\n-1  -\\lambda  -1 \\\\\n\\sqrt{2}  -1  -\\lambda\n\\end{pmatrix}\n$$\nExpanding the determinant, we obtain:\n$$\np(\\lambda) = (-\\lambda)(\\lambda^2 - 1) - (-1)(\\lambda + \\sqrt{2}) + (-\\sqrt{2})(1 + \\lambda\\sqrt{2})\n$$\n$$\np(\\lambda) = -\\lambda^3 + \\lambda + \\lambda + \\sqrt{2} - \\sqrt{2} - 2\\lambda = -\\lambda^3\n$$\nThe characteristic equation is $-\\lambda^3 = 0$, which yields the eigenvalues $\\lambda_1 = \\lambda_2 = \\lambda_3 = 0$. The spectral radius is $\\rho(\\mathbf{T}_J) = \\max_i|\\lambda_i| = |0| = 0$. Since $\\rho(\\mathbf{T}_J) = 0  1$, the Jacobi iteration converges for this system.\n\nNext, we analyze the Gauss-Seidel iteration. The iteration matrix is $\\mathbf{T}_{GS} = (\\mathbf{D}-\\mathbf{L})^{-1}\\mathbf{U}$. First, we compute $\\mathbf{D}-\\mathbf{L}$ and its inverse:\n$$\n\\mathbf{D}-\\mathbf{L} = \\begin{bmatrix} 1  0  0 \\\\ 1  1  0 \\\\ -\\sqrt{2}  1  1 \\end{bmatrix}\n$$\nThe inverse is found to be:\n$$\n(\\mathbf{D}-\\mathbf{L})^{-1} = \\begin{bmatrix} 1  0  0 \\\\ -1  1  0 \\\\ 1+\\sqrt{2}  -1  1 \\end{bmatrix}\n$$\nNow, we compute $\\mathbf{T}_{GS} = (\\mathbf{D}-\\mathbf{L})^{-1}\\mathbf{U}$:\n$$\n\\mathbf{T}_{GS} = \\begin{bmatrix} 1  0  0 \\\\ -1  1  0 \\\\ 1+\\sqrt{2}  -1  1 \\end{bmatrix} \\begin{bmatrix} 0  -1  -\\sqrt{2} \\\\ 0  0  -1 \\\\ 0  0  0 \\end{bmatrix} = \\begin{bmatrix} 0  -1  -\\sqrt{2} \\\\ 0  1  \\sqrt{2}-1 \\\\ 0  -(1+\\sqrt{2})  -(1+\\sqrt{2})\\sqrt{2}+1 \\end{bmatrix}\n$$\n$$\n\\mathbf{T}_{GS} = \\begin{bmatrix} 0  -1  -\\sqrt{2} \\\\ 0  1  \\sqrt{2}-1 \\\\ 0  -(1+\\sqrt{2})  -1-\\sqrt{2} \\end{bmatrix}\n$$\nThe characteristic equation is $\\det(\\mathbf{T}_{GS} - \\lambda\\mathbf{I}) = 0$.\n$$\n\\det\\begin{pmatrix} -\\lambda  -1  -\\sqrt{2} \\\\ 0  1-\\lambda  \\sqrt{2}-1 \\\\ 0  -(1+\\sqrt{2})  -1-\\sqrt{2}-\\lambda \\end{pmatrix} = -\\lambda \\det\\begin{pmatrix} 1-\\lambda  \\sqrt{2}-1 \\\\ -(1+\\sqrt{2})  -1-\\sqrt{2}-\\lambda \\end{pmatrix} = 0\n$$\nOne eigenvalue is $\\lambda_1 = 0$. The other two are the eigenvalues of the nontrivial $2 \\times 2$ submatrix:\n$$\n\\mathbf{M} = \\begin{pmatrix} 1  \\sqrt{2}-1 \\\\ -(1+\\sqrt{2})  -1-\\sqrt{2} \\end{pmatrix}\n$$\nThe characteristic polynomial of $\\mathbf{M}$ is $\\lambda^2 - \\text{Tr}(\\mathbf{M})\\lambda + \\det(\\mathbf{M}) = 0$.\n$\\text{Tr}(\\mathbf{M}) = 1 + (-1-\\sqrt{2}) = -\\sqrt{2}$.\n$\\det(\\mathbf{M}) = (1)(-1-\\sqrt{2}) - (\\sqrt{2}-1)(-(1+\\sqrt{2})) = -1-\\sqrt{2} + (\\sqrt{2}-1)(\\sqrt{2}+1) = -1-\\sqrt{2} + (2-1) = -\\sqrt{2}$.\nSo the characteristic equation for the nontrivial part is:\n$$\n\\lambda^2 + \\sqrt{2}\\lambda - \\sqrt{2} = 0\n$$\nThe roots are given by the quadratic formula:\n$$\n\\lambda = \\frac{-\\sqrt{2} \\pm \\sqrt{(\\sqrt{2})^2 - 4(1)(-\\sqrt{2})}}{2} = \\frac{-\\sqrt{2} \\pm \\sqrt{2 + 4\\sqrt{2}}}{2}\n$$\nThe three eigenvalues of $\\mathbf{T}_{GS}$ are $\\lambda_1 = 0$, $\\lambda_2 = \\frac{-\\sqrt{2} + \\sqrt{2 + 4\\sqrt{2}}}{2}$, and $\\lambda_3 = \\frac{-\\sqrt{2} - \\sqrt{2 + 4\\sqrt{2}}}{2}$.\nThe spectral radius is the maximum of their absolute values.\n$|\\lambda_1|=0$.\n$|\\lambda_2| = \\frac{\\sqrt{2 + 4\\sqrt{2}} - \\sqrt{2}}{2}$ since $\\sqrt{2 + 4\\sqrt{2}}  \\sqrt{2}$.\n$|\\lambda_3| = \\frac{\\sqrt{2} + \\sqrt{2 + 4\\sqrt{2}}}{2}$.\nClearly, $|\\lambda_3|  |\\lambda_2|$, so the spectral radius is $\\rho(\\mathbf{T}_{GS}) = |\\lambda_3|$.\nWe must verify if $\\rho(\\mathbf{T}_{GS})  1$.\n$$\n\\frac{\\sqrt{2} + \\sqrt{2 + 4\\sqrt{2}}}{2}  1 \\iff \\sqrt{2} + \\sqrt{2 + 4\\sqrt{2}}  2 \\iff \\sqrt{2 + 4\\sqrt{2}}  2 - \\sqrt{2}\n$$\nSince $2-\\sqrt{2}  0$, we can square both sides:\n$$\n2 + 4\\sqrt{2}  (2-\\sqrt{2})^2 = 4 - 4\\sqrt{2} + 2 = 6 - 4\\sqrt{2}\n$$\n$$\n8\\sqrt{2}  4 \\iff 2\\sqrt{2}  1 \\iff \\sqrt{8}  1\n$$\nThis is true. Therefore, $\\rho(\\mathbf{T}_{GS})  1$, and the Gauss-Seidel iteration diverges.\nThe specific matrix provided is indeed an instance where Jacobi iteration converges while Gauss-Seidel iteration diverges. The spectral radius of the Gauss-Seidel iteration matrix is computed in exact form.",
            "answer": "$$\\boxed{\\frac{\\sqrt{2} + \\sqrt{2 + 4\\sqrt{2}}}{2}}$$"
        }
    ]
}