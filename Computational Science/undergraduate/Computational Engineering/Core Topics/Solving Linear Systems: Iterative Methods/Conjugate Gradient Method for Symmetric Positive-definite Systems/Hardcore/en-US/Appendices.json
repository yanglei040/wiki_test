{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, our first practice involves a direct, step-by-step application of the Conjugate Gradient algorithm. This exercise focuses on the fundamental mechanics of the first iteration, allowing you to get comfortable with the sequence of calculations involving residuals, search directions, and step sizes. By working through this small, manageable system by hand, you will gain a concrete understanding of how the algorithm begins its journey toward a solution .",
            "id": "2207655",
            "problem": "In scientific computing and engineering, solving a system of linear equations $A\\mathbf{x}=\\mathbf{b}$ where $A$ is a symmetric positive-definite matrix is a common and fundamental task. This often arises from the discretization of partial differential equations using methods like the Finite Element Method (FEM). The Conjugate Gradient (CG) algorithm is a powerful iterative method tailored for solving such systems, especially when the matrix $A$ is large and sparse.\n\nYou are given the following system of linear equations, $A\\mathbf{x}=\\mathbf{b}$:\n$$\nA = \\begin{pmatrix} 3 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 3 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}\n$$\nThe matrix $A$ is symmetric and positive-definite.\n\nStarting with an initial guess of $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$, perform the first full iteration of the Conjugate Gradient algorithm to compute the next approximation, $\\mathbf{x}_1$.\n\nWhat is the resulting vector $\\mathbf{x}_1$?",
            "solution": "We apply the Conjugate Gradient (CG) method to the symmetric positive-definite system $A\\mathbf{x}=\\mathbf{b}$ starting from $\\mathbf{x}_{0}=\\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix}$. The standard CG initialization and first iteration steps are:\n- Residual: $\\mathbf{r}_{0}=\\mathbf{b}-A\\mathbf{x}_{0}=\\mathbf{b}$.\n- Search direction: $\\mathbf{p}_{0}=\\mathbf{r}_{0}$.\n- Step size: $\\alpha_{0}=\\dfrac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{\\mathbf{p}_{0}^{T}A\\mathbf{p}_{0}}$.\n- Update: $\\mathbf{x}_{1}=\\mathbf{x}_{0}+\\alpha_{0}\\mathbf{p}_{0}$.\n\nWith $A=\\begin{pmatrix}3 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 3\\end{pmatrix}$ and $\\mathbf{b}=\\begin{pmatrix}1 \\\\ 2 \\\\ -1\\end{pmatrix}$, we have\n$$\n\\mathbf{r}_{0}=\\mathbf{b}=\\begin{pmatrix}1 \\\\ 2 \\\\ -1\\end{pmatrix}, \\quad \\mathbf{p}_{0}=\\mathbf{r}_{0}=\\begin{pmatrix}1 \\\\ 2 \\\\ -1\\end{pmatrix}.\n$$\nCompute $A\\mathbf{p}_{0}$:\n$$\nA\\mathbf{p}_{0}=\\begin{pmatrix}3 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 3\\end{pmatrix}\\begin{pmatrix}1 \\\\ 2 \\\\ -1\\end{pmatrix}\n=\\begin{pmatrix}3\\cdot 1+(-1)\\cdot 2+0\\cdot(-1) \\\\ (-1)\\cdot 1+2\\cdot 2+(-1)\\cdot(-1) \\\\ 0\\cdot 1+(-1)\\cdot 2+3\\cdot(-1)\\end{pmatrix}\n=\\begin{pmatrix}1 \\\\ 4 \\\\ -5\\end{pmatrix}.\n$$\nCompute the scalars in $\\alpha_{0}$:\n$$\n\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}=1^{2}+2^{2}+(-1)^{2}=6, \\quad \\mathbf{p}_{0}^{T}A\\mathbf{p}_{0}=\\begin{pmatrix}1 & 2 & -1\\end{pmatrix}\\begin{pmatrix}1 \\\\ 4 \\\\ -5\\end{pmatrix}=1+8+5=14.\n$$\nThus\n$$\n\\alpha_{0}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{\\mathbf{p}_{0}^{T}A\\mathbf{p}_{0}}=\\frac{6}{14}=\\frac{3}{7}.\n$$\nUpdate $\\mathbf{x}$:\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}+\\alpha_{0}\\mathbf{p}_{0}=\\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix}+\\frac{3}{7}\\begin{pmatrix}1 \\\\ 2 \\\\ -1\\end{pmatrix}\n=\\begin{pmatrix}\\frac{3}{7} \\\\ \\frac{6}{7} \\\\ -\\frac{3}{7}\\end{pmatrix}.\n$$\nThis is the required next approximation after the first full iteration step.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3}{7} \\\\ \\frac{6}{7} \\\\ -\\frac{3}{7}\\end{pmatrix}}$$"
        },
        {
            "introduction": "The Conjugate Gradient method is specifically designed for symmetric positive-definite (SPD) systems, and this practice explores *why* this condition is so crucial. You will investigate a system with a symmetric but indefinite matrix, discovering how a carefully chosen starting point can cause the algorithm to fail by leading to a division by zero during the step-size calculation . This exercise highlights the theoretical boundaries of the method and reinforces the importance of its underlying assumptions, illustrating a classic breakdown scenario.",
            "id": "2379067",
            "problem": "In computational engineering, the Conjugate Gradient (CG) method is classically formulated for symmetric positive-definite systems. To illustrate the breakdown that occurs when the positive-definite requirement is violated, consider the symmetric indefinite matrix $A \\in \\mathbb{R}^{2 \\times 2}$ and right-hand side vector $\\mathbf{b} \\in \\mathbb{R}^{2}$ given by\n$$\nA = \\begin{pmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{pmatrix}, \\quad\n\\mathbf{b} = \\begin{pmatrix}\n1 \\\\\n0\n\\end{pmatrix}.\n$$\nLet the initial guess be parameterized by a real scalar $t \\in \\mathbb{R}$ as\n$$\n\\mathbf{x}_{0}(t) = t \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nFor the standard Conjugate Gradient method applied to the linear system $A \\mathbf{x} = \\mathbf{b}$, determine the real value of $t$ for which the first iterationâ€™s step size computation would encounter a division by zero. Provide your answer as a single real number. No rounding is required.",
            "solution": "The problem requires finding the value of the parameter $t$ for which the first iteration of the Conjugate Gradient (CG) method fails due to a division by zero. The CG method is applied to the linear system $A \\mathbf{x} = \\mathbf{b}$, where the matrix $A$ is symmetric but not positive-definite.\n\nThe standard Conjugate Gradient algorithm begins with an initial guess $\\mathbf{x}_{0}$. The first iteration ($k=0$) proceeds as follows:\n1.  Compute the initial residual: $\\mathbf{r}_{0} = \\mathbf{b} - A \\mathbf{x}_{0}$.\n2.  Set the initial search direction: $\\mathbf{p}_{0} = \\mathbf{r}_{0}$.\n3.  Compute the step size: $\\alpha_{0} = \\frac{\\mathbf{r}_{0}^T \\mathbf{r}_{0}}{\\mathbf{p}_{0}^T A \\mathbf{p}_{0}}$.\n4.  Update the solution: $\\mathbf{x}_{1} = \\mathbf{x}_{0} + \\alpha_{0} \\mathbf{p}_{0}$.\n\nThe problem states that a division by zero occurs during the computation of the step size $\\alpha_{0}$. This implies that the denominator of the expression for $\\alpha_{0}$ must be zero, while the numerator is non-zero.\nThe denominator of $\\alpha_{0}$ is $\\mathbf{p}_{0}^T A \\mathbf{p}_{0}$. Since $\\mathbf{p}_{0} = \\mathbf{r}_{0}$, the condition for breakdown is $\\mathbf{r}_{0}^T A \\mathbf{r}_{0} = 0$.\n\nWe are given:\n$$\nA = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{x}_{0}(t) = t \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} t \\\\ t \\end{pmatrix}.\n$$\nFirst, we compute the initial residual $\\mathbf{r}_{0}$ as a function of $t$.\n$$\nA \\mathbf{x}_{0} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} t \\\\ t \\end{pmatrix} = \\begin{pmatrix} t \\\\ -t \\end{pmatrix}.\n$$\n$$\n\\mathbf{r}_{0} = \\mathbf{b} - A \\mathbf{x}_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} t \\\\ -t \\end{pmatrix} = \\begin{pmatrix} 1-t \\\\ t \\end{pmatrix}.\n$$\nThe initial search direction is $\\mathbf{p}_{0} = \\mathbf{r}_{0} = \\begin{pmatrix} 1-t \\\\ t \\end{pmatrix}$.\n\nNext, we compute the term $\\mathbf{p}_{0}^T A \\mathbf{p}_{0}$ which constitutes the denominator of $\\alpha_{0}$.\nFirst, we find the vector $A \\mathbf{p}_{0}$:\n$$\nA \\mathbf{p}_{0} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 1-t \\\\ t \\end{pmatrix} = \\begin{pmatrix} 1-t \\\\ -t \\end{pmatrix}.\n$$\nNow, we compute the dot product $\\mathbf{p}_{0}^T (A \\mathbf{p}_{0})$:\n$$\n\\mathbf{p}_{0}^T A \\mathbf{p}_{0} = \\begin{pmatrix} 1-t & t \\end{pmatrix} \\begin{pmatrix} 1-t \\\\ -t \\end{pmatrix} = (1-t)(1-t) + t(-t) = (1-t)^2 - t^2.\n$$\nFor division by zero to occur, this denominator must be equal to $0$:\n$$\n(1-t)^2 - t^2 = 0.\n$$\nExpanding the expression:\n$$\n(1 - 2t + t^2) - t^2 = 0.\n$$\n$$\n1 - 2t = 0.\n$$\nSolving for $t$:\n$$\n2t = 1 \\implies t = \\frac{1}{2}.\n$$\nFor the breakdown to be a division by zero, the numerator $\\mathbf{r}_{0}^T \\mathbf{r}_{0}$ must be non-zero for this value of $t$. Let's verify this.\nFor $t = \\frac{1}{2}$, the residual is:\n$$\n\\mathbf{r}_{0} = \\begin{pmatrix} 1-\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}.\n$$\nThe numerator term is the squared norm of the residual:\n$$\n\\mathbf{r}_{0}^T \\mathbf{r}_{0} = \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}.\n$$\nSince the numerator is $\\frac{1}{2} \\ne 0$ and the denominator is $0$, the computation of $\\alpha_{0}$ indeed results in a division by zero when $t = \\frac{1}{2}$. This phenomenon, where $\\mathbf{p}_{k}^T A \\mathbf{p}_{k} \\le 0$, is possible because the matrix $A$ is not positive-definite, and it is a primary reason why the standard CG method is restricted to symmetric positive-definite systems. For an indefinite matrix, the search direction $\\mathbf{p}_{k}$ may be $A$-orthogonal to itself, which is what we have found for $\\mathbf{p}_{0}$ when $t = \\frac{1}{2}$.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "Moving from manual calculation to computational practice, this final exercise connects theory to real-world performance. You will implement the CG algorithm to solve a series of linear systems with increasingly poor conditioning, observing the direct impact on convergence speed. By measuring the iteration count against the matrix's spectral condition number, $\\kappa_2(A)$, you will empirically verify one of the most important principles in iterative methods: the condition number is a key factor governing the rate of convergence .",
            "id": "2379089",
            "problem": "Design and implement a complete program that, for a specified set of symmetric positive-definite linear systems, empirically quantifies how the spectral condition number relates to the number of iterations required to achieve a prescribed residual reduction when solving the systems. The program must construct each system matrix $A \\in \\mathbb{R}^{n \\times n}$, the right-hand side $\\mathbf{b} \\in \\mathbb{R}^{n}$ with entries $b_i = 1$ for all $i$, and use the initial guess $\\mathbf{x}^{(0)} = \\mathbf{0}$. For each system, compute the spectral condition number $\\kappa_2(A)$ defined as the ratio of the largest to the smallest eigenvalue of $A$, and determine the smallest nonnegative integer $k$ such that the Euclidean norm of the residual satisfies\n$$\n\\frac{\\lVert \\mathbf{r}^{(k)} \\rVert_2}{\\lVert \\mathbf{r}^{(0)} \\rVert_2} \\leq 10^{-6},\n$$\nwhere $\\mathbf{r}^{(k)} = \\mathbf{b} - A \\mathbf{x}^{(k)}$. The iterative solver employed must be suitable for symmetric positive-definite matrices and operate purely by matrix-vector products and vector inner products. Use the Euclidean norm (that is, the $\\ell_2$-norm) for all residual norms. The program must use floating-point arithmetic.\n\nTest suite specification:\n- Case $1$: Identity system. Let $A = I_n$ with $n = 50$ and $\\mathbf{b} \\in \\mathbb{R}^{50}$ defined by $b_i = 1$ for all $i$.\n- Case $2$: Hilbert system of size $n=3$. Let $A = H_3$ with entries $A_{ij} = \\frac{1}{i + j - 1}$ for $1 \\leq i,j \\leq 3$ and $\\mathbf{b} \\in \\mathbb{R}^{3}$ defined by $b_i = 1$ for all $i$.\n- Case $3$: Hilbert system of size $n=4$. Let $A = H_4$ with entries $A_{ij} = \\frac{1}{i + j - 1}$ for $1 \\leq i,j \\leq 4$ and $\\mathbf{b} \\in \\mathbb{R}^{4}$ defined by $b_i = 1$ for all $i$.\n- Case $4$: Hilbert system of size $n=5$. Let $A = H_5$ with entries $A_{ij} = \\frac{1}{i + j - 1}$ for $1 \\leq i,j \\leq 5$ and $\\mathbf{b} \\in \\mathbb{R}^{5}$ defined by $b_i = 1$ for all $i$.\n- Case $5$: Hilbert system of size $n=6$. Let $A = H_6$ with entries $A_{ij} = \\frac{1}{i + j - 1}$ for $1 \\leq i,j \\leq 6$ and $\\mathbf{b} \\in \\mathbb{R}^{6}$ defined by $b_i = 1$ for all $i$.\n\nFor each case, the program must output the pair consisting of the spectral condition number $\\kappa_2(A)$ and the iteration count $k$ that attains the stopping criterion above. Aggregate all results into a single line as a comma-separated list enclosed in square brackets, in the following order and flattened as scalars:\n$$\n[\\ \\kappa_2(A_1),\\ k_1,\\ \\kappa_2(A_2),\\ k_2,\\ \\kappa_2(A_3),\\ k_3,\\ \\kappa_2(A_4),\\ k_4,\\ \\kappa_2(A_5),\\ k_5\\ ],\n$$\nwhere the subscript denotes the case index from $1$ to $5$. For example, a valid format would resemble $[r_1,k_1,r_2,k_2,\\ldots,r_5,k_5]$ where each $r_i$ is a floating-point number and each $k_i$ is an integer. No additional text or lines are permitted in the output.",
            "solution": "The problem statement is valid. It presents a standard, well-posed exercise in the field of numerical linear algebra and computational engineering. The task is to empirically investigate the relationship between the spectral condition number of a symmetric positive-definite (SPD) matrix and the convergence rate of an appropriate iterative solver. All definitions and parameters are provided, and the problem is scientifically grounded, objective, and internally consistent.\n\nThe core of this problem lies in solving the linear system $A\\mathbf{x}=\\mathbf{b}$ where the matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive-definite. This SPD property is fundamental, as it guarantees that all eigenvalues of $A$ are real and positive, making the spectral condition number $\\kappa_2(A)$ well-defined and positive. Furthermore, it ensures the existence of a unique solution $\\mathbf{x} = A^{-1}\\mathbf{b}$. Both the identity matrix $I_n$ and the Hilbert matrices $H_n$ are classic examples of SPD matrices.\n\nThe problem specifies an iterative solver that operates using only matrix-vector products and vector inner products. For SPD systems, the Conjugate Gradient (CG) method is the definitive algorithm that satisfies this constraint. It is an optimal method in the sense that it minimizes the $A$-norm of the error over the expanding Krylov subspace at each iteration.\n\nThe CG algorithm is implemented as follows. We are given the initial guess $\\mathbf{x}^{(0)} = \\mathbf{0}$.\nFirst, we initialize the state variables for iteration $k=0$:\nThe initial residual is $\\mathbf{r}^{(0)} = \\mathbf{b} - A \\mathbf{x}^{(0)} = \\mathbf{b}$.\nThe initial search direction is $\\mathbf{p}^{(0)} = \\mathbf{r}^{(0)}$.\n\nFor subsequent iterations, indexed by $k=1, 2, 3, \\ldots$, we compute the next approximation $\\mathbf{x}^{(k)}$ using the following sequence of operations:\n$1$. Compute the step size $\\alpha_{k-1}$:\n$$ \\alpha_{k-1} = \\frac{\\langle \\mathbf{r}^{(k-1)}, \\mathbf{r}^{(k-1)} \\rangle}{\\langle \\mathbf{p}^{(k-1)}, A \\mathbf{p}^{(k-1)} \\rangle} = \\frac{{\\mathbf{r}^{(k-1)}}^T \\mathbf{r}^{(k-1)}}{{{\\mathbf{p}^{(k-1)}}}^T A \\mathbf{p}^{(k-1)}} $$\n$2$. Update the solution vector:\n$$ \\mathbf{x}^{(k)} = \\mathbf{x}^{(k-1)} + \\alpha_{k-1} \\mathbf{p}^{(k-1)} $$\n$3$. Update the residual. A computationally efficient form is used to avoid recalculating $\\mathbf{b} - A\\mathbf{x}^{(k)}$ from scratch:\n$$ \\mathbf{r}^{(k)} = \\mathbf{r}^{(k-1)} - \\alpha_{k-1} A \\mathbf{p}^{(k-1)} $$\n$4$. Check the stopping criterion. The process terminates if the following condition is met, and the number of iterations taken is $k$:\n$$ \\frac{\\lVert \\mathbf{r}^{(k)} \\rVert_2}{\\lVert \\mathbf{r}^{(0)} \\rVert_2} \\leq 10^{-6} $$\nwhere $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm.\n$5$. If the criterion is not met, a new search direction is constructed. First, compute $\\beta_{k-1}$:\n$$ \\beta_{k-1} = \\frac{\\langle \\mathbf{r}^{(k)}, \\mathbf{r}^{(k)} \\rangle}{\\langle \\mathbf{r}^{(k-1)}, \\mathbf{r}^{(k-1)} \\rangle} = \\frac{{\\mathbf{r}^{(k)}}^T \\mathbf{r}^{(k)}}{{{\\mathbf{r}^{(k-1)}}}^T \\mathbf{r}^{(k-1)}} $$\n$6$. Update the search direction:\n$$ \\mathbf{p}^{(k)} = \\mathbf{r}^{(k)} + \\beta_{k-1} \\mathbf{p}^{(k-1)} $$\n\nThe spectral condition number, $\\kappa_2(A)$, is defined as the ratio of the largest to the smallest eigenvalue of $A$:\n$$ \\kappa_2(A) = \\frac{\\lambda_{\\text{max}}(A)}{\\lambda_{\\text{min}}(A)} $$\nSince $A$ is symmetric, its eigenvalues are real. They will be computed numerically. For a symmetric matrix, `numpy.linalg.eigvalsh` is an appropriate and efficient tool for this purpose.\n\nThe program will proceed by addressing each test case specified. For each case, it will first construct the matrix $A$ and the vector $\\mathbf{b}$. It will then compute the eigenvalues of $A$ to determine $\\kappa_2(A)$. Finally, it will execute the CG algorithm, counting the number of iterations $k$ until the prescribed residual reduction is achieved. The resulting pairs $(\\kappa_2(A), k)$ will be collected and formatted into the required output structure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import hilbert\n\ndef count_cg_iterations(A: np.ndarray, b: np.ndarray, tol: float = 1e-6) -> int:\n    \"\"\"\n    Solves the system Ax=b for a symmetric positive-definite matrix A\n    using the Conjugate Gradient method and counts the iterations.\n\n    Args:\n        A: The symmetric positive-definite matrix of the system.\n        b: The right-hand side vector.\n        tol: The relative tolerance for the residual norm.\n\n    Returns:\n        The number of iterations k to satisfy the stopping criterion.\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n, dtype=float)\n    r = b.copy()\n    p = r.copy()\n\n    # Initial residual norm for the stopping criterion denominator\n    norm_r0 = np.linalg.norm(r)\n    \n    # If b is the zero vector, the solution is x=0 in 0 iterations.\n    if norm_r0 == 0:\n        return 0\n\n    # Check the condition for k=0 (initial state)\n    if np.linalg.norm(r) / norm_r0 <= tol:\n        return 0\n\n    rs_old = np.dot(r, r)\n    \n    # In exact arithmetic, CG converges in at most n iterations.\n    # We use a slightly larger limit to account for floating-point effects.\n    max_k = 2 * n\n\n    for k in range(1, max_k + 1):\n        Ap = A @ p\n        p_dot_Ap = np.dot(p, Ap)\n\n        # For an SPD matrix A, p_dot_Ap > 0 unless p=0.\n        # If p=0, then the previous residual r_old must have been 0.\n        if p_dot_Ap <= 0:\n            break\n\n        alpha = rs_old / p_dot_Ap\n        \n        x += alpha * p\n        r -= alpha * Ap\n\n        if np.linalg.norm(r) / norm_r0 <= tol:\n            return k\n            \n        rs_new = np.dot(r, r)\n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n    \n    # This should not be reached for the given problems, as convergence is expected.\n    # It would indicate failure to converge within max_k iterations.\n    return max_k\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    test_cases_spec = [\n        {'type': 'identity', 'n': 50},\n        {'type': 'hilbert', 'n': 3},\n        {'type': 'hilbert', 'n': 4},\n        {'type': 'hilbert', 'n': 5},\n        {'type': 'hilbert', 'n': 6},\n    ]\n\n    results = []\n    \n    for spec in test_cases_spec:\n        n = spec['n']\n        \n        if spec['type'] == 'identity':\n            A = np.identity(n)\n        elif spec['type'] == 'hilbert':\n            A = hilbert(n)\n        else:\n            raise ValueError(\"Unknown matrix type specified.\")\n            \n        b = np.ones(n)\n\n        # Compute spectral condition number\n        # eigvalsh returns eigenvalues in ascending order for symmetric matrices\n        eigenvalues = np.linalg.eigvalsh(A)\n        # Check for singularity or non-positive definite matrix\n        if eigenvalues[0] <= 0:\n            cond_num = np.inf\n        else:\n            cond_num = eigenvalues[-1] / eigenvalues[0]\n        \n        # Compute number of iterations\n        iterations = count_cg_iterations(A, b, tol=1e-6)\n        \n        results.extend([cond_num, iterations])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}