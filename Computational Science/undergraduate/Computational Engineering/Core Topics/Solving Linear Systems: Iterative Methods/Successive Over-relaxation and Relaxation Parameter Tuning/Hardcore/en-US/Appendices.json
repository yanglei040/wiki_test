{
    "hands_on_practices": [
        {
            "introduction": "Before diving into complex implementations, it is crucial to grasp the theoretical underpinnings of SOR convergence. This exercise guides you through a pencil-and-paper analysis of a simple $2 \\times 2$ system, revealing a case where the standard Gauss-Seidel method ($\\omega=1$) diverges but SOR can be tuned to converge . By deriving the iteration matrix and its spectral radius, you will gain a concrete understanding of what makes SOR powerful and how the optimal relaxation parameter $\\omega^{\\star}$ is determined.",
            "id": "2441046",
            "problem": "Consider the linear system $A \\mathbf{x} = \\mathbf{b}$ with\n$$\nA \\;=\\; \\begin{pmatrix} 1  2 \\\\ -1  1 \\end{pmatrix}, \n\\qquad\n\\mathbf{b} \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nUse only the definitions of the Gaussâ€“Seidel (GS) method and the Successive Over-Relaxation (SOR) method to analyze convergence.\n\nTasks:\n1. Starting from the componentwise SOR update definition for a general relaxation parameter $\\omega \\in (0,1]$, derive the $2 \\times 2$ linear error-propagation map $T_{\\omega}$ for this system. Then specialize your expression to $\\omega = 1$ to recover the GS error-propagation map $T_{1}$.\n2. Determine whether GS converges by computing the spectral radius of $T_{1}$ and justifying your conclusion.\n3. For $\\omega \\in (0,1)$, determine the value $\\omega^{\\star}$ that minimizes the spectral radius of $T_{\\omega}$ and state whether the corresponding SOR iteration converges. Report $\\omega^{\\star}$ rounded to four significant figures.\n\nYour final answer must be the single numerical value of $\\omega^{\\star}$ rounded to four significant figures. No units are required.",
            "solution": "The provided problem is subject to validation.\n\n**Step 1: Extract Givens**\n- Linear system: $A \\mathbf{x} = \\mathbf{b}$\n- Matrix $A$: $A = \\begin{pmatrix} 1  2 \\\\ -1  1 \\end{pmatrix}$\n- Vector $\\mathbf{b}$: $\\mathbf{b} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- Method: Successive Over-Relaxation (SOR) and Gauss-Seidel (GS) as a special case.\n- Parameter: Relaxation parameter $\\omega \\in (0, 1]$. The optimization is for $\\omega \\in (0, 1)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard exercise in numerical linear algebra, focusing on the convergence analysis of the SOR iterative method. The concepts of error-propagation maps and spectral radius are fundamental and correctly applied.\n- **Well-Posed:** The problem is clearly stated with three distinct tasks. The objective to find an optimal relaxation parameter that minimizes the spectral radius is a well-defined mathematical optimization problem.\n- **Objective:** The problem is expressed in precise mathematical language, free from ambiguity or subjective content.\n- **Completeness and Consistency:** All necessary information (the matrix $A$) for analyzing the convergence of the iterative methods is provided. The vector $\\mathbf{b}$ is not required for the convergence analysis, which depends only on the iteration matrix, but its presence does not introduce any contradiction. The problem is self-contained.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\nThe analysis begins with the definition of the SOR iteration. For a linear system $A \\mathbf{x} = \\mathbf{b}$, the matrix $A$ is decomposed as $A = D - L - U$, where $D$ is the diagonal part of $A$, $-L$ is the strict lower-triangular part of $A$, and $-U$ is the strict upper-triangular part of $A$. The SOR iteration is defined by the update rule:\n$$\\mathbf{x}^{(k+1)} = (D - \\omega L)^{-1} \\left( ((1-\\omega)D + \\omega U) \\mathbf{x}^{(k)} + \\omega \\mathbf{b} \\right)$$\nThe convergence of the method is determined by the spectral radius of the SOR iteration matrix, $T_{\\omega}$, defined as:\n$$T_{\\omega} = (D - \\omega L)^{-1} ((1-\\omega)D + \\omega U)$$\nThe error vector $e^{(k)} = \\mathbf{x}^{(k)} - \\mathbf{x}^*$ (where $\\mathbf{x}^*$ is the exact solution) propagates according to $e^{(k+1)} = T_{\\omega} e^{(k)}$.\n\n**1. Derivation of the Error-Propagation Map $T_{\\omega}$**\n\nFor the given matrix $A = \\begin{pmatrix} 1  2 \\\\ -1  1 \\end{pmatrix}$, the decomposition is:\n$$D = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\quad L = \\begin{pmatrix} 0  0 \\\\ -(-1)  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 1  0 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 0  -2 \\\\ 0  0 \\end{pmatrix}$$\nFirst, we compute the matrix $(D - \\omega L)$:\n$$D - \\omega L = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\omega \\begin{pmatrix} 0  0 \\\\ 1  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ -\\omega  1 \\end{pmatrix}$$\nIts inverse is:\n$$(D - \\omega L)^{-1} = \\frac{1}{(1)(1) - (0)(-\\omega)} \\begin{pmatrix} 1  0 \\\\ \\omega  1 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ \\omega  1 \\end{pmatrix}$$\nNext, we compute the matrix $((1-\\omega)D + \\omega U)$:\n$$(1-\\omega)D + \\omega U = (1-\\omega) \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\omega \\begin{pmatrix} 0  -2 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1-\\omega  -2\\omega \\\\ 0  1-\\omega \\end{pmatrix}$$\nNow, we multiply these two matrices to find $T_{\\omega}$:\n$$T_{\\omega} = (D - \\omega L)^{-1} ((1-\\omega)D + \\omega U) = \\begin{pmatrix} 1  0 \\\\ \\omega  1 \\end{pmatrix} \\begin{pmatrix} 1-\\omega  -2\\omega \\\\ 0  1-\\omega \\end{pmatrix}$$\n$$T_{\\omega} = \\begin{pmatrix} (1)(1-\\omega) + (0)(0)  (1)(-2\\omega) + (0)(1-\\omega) \\\\ (\\omega)(1-\\omega) + (1)(0)  (\\omega)(-2\\omega) + (1)(1-\\omega) \\end{pmatrix} = \\begin{pmatrix} 1-\\omega  -2\\omega \\\\ \\omega(1-\\omega)  1-\\omega - 2\\omega^2 \\end{pmatrix}$$\nThe Gauss-Seidel (GS) method is a special case of SOR with $\\omega = 1$. The GS error-propagation map, $T_1$, is obtained by substituting $\\omega=1$ into the expression for $T_{\\omega}$:\n$$T_{1} = \\begin{pmatrix} 1-1  -2(1) \\\\ 1(1-1)  1-1 - 2(1)^2 \\end{pmatrix} = \\begin{pmatrix} 0  -2 \\\\ 0  -2 \\end{pmatrix}$$\n\n**2. Convergence Analysis of the Gauss-Seidel Method**\n\nThe convergence of an iterative method is guaranteed if and only if the spectral radius $\\rho$ of its iteration matrix is strictly less than $1$. We compute the eigenvalues of $T_1$ by solving the characteristic equation $\\det(T_1 - \\lambda I) = 0$.\n$$\\det \\begin{pmatrix} 0 - \\lambda  -2 \\\\ 0  -2 - \\lambda \\end{pmatrix} = (-\\lambda)(-2-\\lambda) - (0)(-2) = \\lambda(2+\\lambda) = 0$$\nThe eigenvalues are $\\lambda_1 = 0$ and $\\lambda_2 = -2$.\nThe spectral radius of $T_1$ is the maximum of the absolute values of its eigenvalues:\n$$\\rho(T_1) = \\max(|\\lambda_1|, |\\lambda_2|) = \\max(|0|, |-2|) = 2$$\nSince $\\rho(T_1) = 2 > 1$, the Gauss-Seidel method for this system does not converge.\n\n**3. Optimal Relaxation Parameter $\\omega^{\\star}$**\n\nTo find the optimal relaxation parameter $\\omega^{\\star}$ in the range $\\omega \\in (0, 1)$, we must find the value of $\\omega$ that minimizes the spectral radius $\\rho(T_{\\omega})$. We start by finding the eigenvalues of $T_{\\omega}$ from its characteristic equation $\\det(T_{\\omega} - \\lambda I) = 0$:\n$$\\det \\begin{pmatrix} 1-\\omega - \\lambda  -2\\omega \\\\ \\omega(1-\\omega)  1-\\omega - 2\\omega^2 - \\lambda \\end{pmatrix} = 0$$\n$$(1-\\omega - \\lambda)(1-\\omega - 2\\omega^2 - \\lambda) + 2\\omega^2(1-\\omega) = 0$$\n$$\\lambda^2 - (1-\\omega - 2\\omega^2 + 1-\\omega)\\lambda + (1-\\omega)(1-\\omega - 2\\omega^2) + 2\\omega^2(1-\\omega) = 0$$\n$$\\lambda^2 - (2 - 2\\omega - 2\\omega^2)\\lambda + (1-\\omega)(1-\\omega - 2\\omega^2 + 2\\omega^2) = 0$$\n$$\\lambda^2 - 2(1 - \\omega - \\omega^2)\\lambda + (1-\\omega)^2 = 0$$\nUsing the quadratic formula, the eigenvalues are:\n$$\\lambda = \\frac{2(1 - \\omega - \\omega^2) \\pm \\sqrt{4(1 - \\omega - \\omega^2)^2 - 4(1-\\omega)^2}}{2} = (1 - \\omega - \\omega^2) \\pm \\sqrt{(1 - \\omega - \\omega^2)^2 - (1-\\omega)^2}$$\nThe nature of the eigenvalues depends on the sign of the discriminant, $\\Delta_p = (1 - \\omega - \\omega^2)^2 - (1-\\omega)^2$.\n$$\\Delta_p = [(1 - \\omega - \\omega^2) - (1-\\omega)][(1 - \\omega - \\omega^2) + (1-\\omega)] = (-\\omega^2)(2 - 2\\omega - \\omega^2)$$\nFor $\\omega \\in (0, 1)$, $-\\omega^2$ is negative. The sign of $\\Delta_p$ is opposite to the sign of $f(\\omega) = 2 - 2\\omega - \\omega^2$. The roots of $f(\\omega) = 0$ are $\\omega = -1 \\pm \\sqrt{3}$. Since we are in the interval $\\omega \\in (0, 1)$, the critical point is $\\omega = \\sqrt{3} - 1 \\approx 0.732$.\n\nCase 1: $0  \\omega \\le \\sqrt{3}-1$.\nIn this interval, $f(\\omega) \\ge 0$, so $\\Delta_p \\le 0$. The eigenvalues are a complex conjugate pair (or real and equal at the boundary).\n$$\\lambda = (1 - \\omega - \\omega^2) \\pm i\\omega \\sqrt{2 - 2\\omega - \\omega^2}$$\nThe spectral radius is the modulus of these eigenvalues. For a complex number $z=a+ib$, $|z|^2=a^2+b^2$. The product of the complex conjugate roots is $\\lambda \\bar{\\lambda} = (1-\\omega)^2$, which is the constant term in the characteristic polynomial.\n$$\\rho(T_{\\omega})^2 = |\\lambda|^2 = (1-\\omega)^2$$\nSince $\\omega \\in (0, 1)$, $1-\\omega  0$, so $\\rho(T_{\\omega}) = 1-\\omega$. This is a linearly decreasing function of $\\omega$.\n\nCase 2: $\\sqrt{3}-1  \\omega  1$.\nIn this interval, $f(\\omega)  0$, so $\\Delta_p  0$. The eigenvalues are real and distinct.\n$$\\lambda_{1,2} = (1 - \\omega - \\omega^2) \\pm \\omega\\sqrt{\\omega^2 + 2\\omega - 2}$$\nThe product of the roots is $\\lambda_1\\lambda_2 = (1-\\omega)^2  0$, so they have the same sign. Their sum is $\\lambda_1+\\lambda_2 = 2(1-\\omega-\\omega^2)$. The roots of $1-\\omega-\\omega^2=0$ are $\\omega = \\frac{\\sqrt{5}-1}{2} \\approx 0.618$ and a negative root. Since $\\sqrt{3}-1  \\frac{\\sqrt{5}-1}{2}$, for any $\\omega$ in this case, $1-\\omega-\\omega^2  0$. Thus, both eigenvalues are negative.\nThe spectral radius is the maximum of their absolute values, which is the absolute value of the more negative root ($\\lambda_2$).\n$$\\rho(T_{\\omega}) = |\\lambda_2| = |(1 - \\omega - \\omega^2) - \\omega\\sqrt{\\omega^2 + 2\\omega - 2}| = -(1 - \\omega - \\omega^2) + \\omega\\sqrt{\\omega^2 + 2\\omega - 2}$$\n$$\\rho(T_{\\omega}) = \\omega^2 + \\omega - 1 + \\omega\\sqrt{\\omega^2 + 2\\omega - 2}$$\nTo find the minimum of this function, we examine its derivative with respect to $\\omega$:\n$$\\frac{d\\rho}{d\\omega} = 2\\omega + 1 + \\sqrt{\\omega^2 + 2\\omega - 2} + \\frac{\\omega(2\\omega+2)}{2\\sqrt{\\omega^2+2\\omega-2}} = 2\\omega + 1 + \\frac{(\\omega^2+2\\omega-2) + (\\omega^2+\\omega)}{\\sqrt{\\omega^2+2\\omega-2}}$$\n$$\\frac{d\\rho}{d\\omega} = 2\\omega + 1 + \\frac{2\\omega^2+3\\omega-2}{\\sqrt{\\omega^2+2\\omega-2}}$$\nFor $\\omega \\in (\\sqrt{3}-1, 1)$, all terms are positive:\n- $2\\omega+1  0$.\n- $\\sqrt{\\omega^2+2\\omega-2}  0$.\n- The numerator $2\\omega^2+3\\omega-2$ has roots at $\\omega=1/2$ and $\\omega=-2$. It is positive for $\\omega  1/2$. Since $\\sqrt{3}-1 \\approx 0.732  1/2$, this term is positive.\nTherefore, $\\frac{d\\rho}{d\\omega}  0$ for $\\omega \\in (\\sqrt{3}-1, 1)$, which means $\\rho(T_{\\omega})$ is an increasing function in this interval.\n\nCombining the two cases, $\\rho(T_{\\omega})$ decreases linearly for $\\omega \\in (0, \\sqrt{3}-1]$ and increases for $\\omega \\in (\\sqrt{3}-1, 1)$. The minimum value of the spectral radius must occur at the point connecting these two regions, which is $\\omega^{\\star} = \\sqrt{3}-1$.\n\nAt this optimal value, the spectral radius is $\\rho(T_{\\omega^\\star}) = 1 - \\omega^{\\star} = 1 - (\\sqrt{3}-1) = 2 - \\sqrt{3}$.\nSince $1  \\sqrt{3}  2$, we have $0  2-\\sqrt{3}  1$. Specifically, $2-\\sqrt{3} \\approx 0.268$, which is less than $1$. Therefore, the SOR iteration with $\\omega = \\omega^{\\star}$ converges.\n\nThe problem asks for the numerical value of $\\omega^{\\star}$ rounded to four significant figures.\n$\\omega^{\\star} = \\sqrt{3}-1 \\approx 1.7320508... - 1 = 0.7320508...$\nRounding to four significant figures gives $0.7321$.",
            "answer": "$$\\boxed{0.7321}$$"
        },
        {
            "introduction": "Theory comes to life when applied to real-world physical phenomena. This practice challenges you to model steady-state heat diffusion in a composite material by implementing a Successive Over-Relaxation type solver for the resulting discretized partial differential equation . You will explore the sophisticated concept of a spatially-varying relaxation parameter, $\\omega_{i,j}$, linking the numerical behavior of the solver directly to the material's thermal conductivity.",
            "id": "2441080",
            "problem": "Consider steady-state heat diffusion in a two-dimensional composite medium on the unit square domain $\\Omega=[0,1]\\times[0,1]$. The temperature field is denoted by $T(x,y)$ and the thermal conductivity field is denoted by $k(x,y)$. The governing equation is the variable-coefficient elliptic equation $-\\nabla\\cdot(k\\nabla T)=0$ in the interior with Dirichlet boundary conditions $T=1$ on the left boundary $x=0$ and $T=0$ on the other three boundaries $x=1$, $y=0$, and $y=1$. Treat temperature as a dimensionless scalar.\n\nDiscretize $\\Omega$ with a uniform Cartesian grid of $N_x$ by $N_y$ nodes, including the boundary nodes. Let the grid spacings be $h_x=1/(N_x-1)$ and $h_y=1/(N_y-1)$, and assume $h_x=h_y=h$. Let $T_{i,j}$ denote the discrete temperature at node $(i,j)$ where $i\\in\\{0,1,\\dots,N_x-1\\}$ and $j\\in\\{0,1,\\dots,N_y-1\\}$. The interior node set is $\\{(i,j):1\\le i\\le N_x-2,\\ 1\\le j\\le N_y-2\\}$. Assign the boundary values by $T_{0,j}=1$ for all $j$, and $T_{N_x-1,j}=0$, $T_{i,0}=0$, $T_{i,N_y-1}=0$ for all admissible $i$ and $j$.\n\nDefine the piecewise-constant conductivity field by a vertical interface at $x=s\\in(0,1)$:\n- For each grid node with coordinate $x_i=i/(N_x-1)$ and any $j$, set $k_{i,j}=k_{\\text{left}}$ if $x_i\\le s$, and $k_{i,j}=k_{\\text{right}}$ otherwise, where $k_{\\text{left}}0$ and $k_{\\text{right}}0$.\n\nFor each interior node $(i,j)$, define face conductivities at half nodes by harmonic means:\n- $k_{i+\\frac{1}{2},j}=\\dfrac{2\\,k_{i,j}\\,k_{i+1,j}}{k_{i,j}+k_{i+1,j}}$, $k_{i-\\frac{1}{2},j}=\\dfrac{2\\,k_{i,j}\\,k_{i-1,j}}{k_{i,j}+k_{i-1,j}}$, $k_{i,j+\\frac{1}{2}}=\\dfrac{2\\,k_{i,j}\\,k_{i,j+1}}{k_{i,j}+k_{i,j+1}}$, $k_{i,j-\\frac{1}{2}}=\\dfrac{2\\,k_{i,j}\\,k_{i,j-1}}{k_{i,j}+k_{i,j-1}}$.\nThen set the discrete coefficients\n- $a_E=\\dfrac{k_{i+\\frac{1}{2},j}}{h^2}$, $a_W=\\dfrac{k_{i-\\frac{1}{2},j}}{h^2}$, $a_N=\\dfrac{k_{i,j+\\frac{1}{2}}}{h^2}$, $a_S=\\dfrac{k_{i,j-\\frac{1}{2}}}{h^2}$,\nand $A_P=a_E+a_W+a_N+a_S$. The discrete interior equation is\n$$\nA_P\\,T_{i,j}-a_E\\,T_{i+1,j}-a_W\\,T_{i-1,j}-a_N\\,T_{i,j+1}-a_S\\,T_{i,j-1}=0,\n$$\nwith the boundary values $T_{i,j}$ fixed as specified.\n\nIntroduce a spatially varying relaxation field $\\omega_{i,j}$ defined pointwise from the conductivity by\n$$\n\\omega_{i,j}=\n\\begin{cases}\n\\omega_{\\min}+\\left(\\omega_{\\max}-\\omega_{\\min}\\right)\\dfrac{k_{i,j}-k_{\\min}}{k_{\\max}-k_{\\min}},  \\text{if } k_{\\max}k_{\\min},\\\\[1ex]\n\\dfrac{\\omega_{\\min}+\\omega_{\\max}}{2},  \\text{if } k_{\\max}=k_{\\min},\n\\end{cases}\n$$\nwhere $k_{\\min}=\\min_{i,j} k_{i,j}$ and $k_{\\max}=\\max_{i,j} k_{i,j}$, and where $0\\omega_{\\min}\\omega_{\\max}2$.\n\nStarting from the initial interior guess $T_{i,j}=0$ for all interior nodes $(i,j)$, perform in-place lexicographic relaxation sweeps on the interior nodes by the pointwise update\n$$\nT_{i,j}\\leftarrow (1-\\omega_{i,j})\\,T_{i,j}+\\omega_{i,j}\\,\\frac{a_E\\,T_{i+1,j}+a_W\\,T_{i-1,j}+a_N\\,T_{i,j+1}+a_S\\,T_{i,j-1}}{A_P}.\n$$\nAfter each full sweep, compute the discrete residual at each interior node\n$$\nr_{i,j}=a_E\\,T_{i+1,j}+a_W\\,T_{i-1,j}+a_N\\,T_{i,j+1}+a_S\\,T_{i,j-1}-A_P\\,T_{i,j},\n$$\nand its infinity norm $\\|r\\|_{\\infty}=\\max_{(i,j)\\ \\text{interior}} |r_{i,j}|$. Stop when $\\|r\\|_{\\infty}\\tau$ for a given tolerance $\\tau0$, or after a fixed maximum number of sweeps if convergence has not been achieved. The quantity to be reported for each test case is the integer number of full sweeps required to satisfy $\\|r\\|_{\\infty}\\tau$.\n\nTest suite. Use the following three test cases, each specified by $(N_x,N_y,s,k_{\\text{left}},k_{\\text{right}},\\omega_{\\min},\\omega_{\\max},\\tau)$:\n- Case $1$: $(N_x,N_y)=(3,3)$, $s=0.5$, $k_{\\text{left}}=1.0$, $k_{\\text{right}}=5.0$, $\\omega_{\\min}=1.0$, $\\omega_{\\max}=1.95$, $\\tau=1.0\\times 10^{-8}$.\n- Case $2$: $(N_x,N_y)=(3,3)$, $s=0.5$, $k_{\\text{left}}=3.0$, $k_{\\text{right}}=3.0$, $\\omega_{\\min}=1.0$, $\\omega_{\\max}=1.95$, $\\tau=1.0\\times 10^{-8}$.\n- Case $3$: $(N_x,N_y)=(3,3)$, $s=0.5$, $k_{\\text{left}}=1.0$, $k_{\\text{right}}=100.0$, $\\omega_{\\min}=1.0$, $\\omega_{\\max}=1.95$, $\\tau=1.0\\times 10^{-8}$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above (for example, $[n_1,n_2,n_3]$). Each $n_i$ must be the integer number of sweeps required. No other text should be printed. All requested outputs are dimensionless integers with no units.",
            "solution": "The problem requires the implementation of a successive over-relaxation (SOR) type iterative solver for a steady-state heat diffusion equation on a two-dimensional composite medium. The solver employs a spatially varying relaxation parameter. The task is to find the number of iterations required for convergence for three specified test cases.\n\nFirst, the problem statement must be validated for correctness and solvability.\n\n**Problem Validation**\n\nThe givens are:\n- A governing partial differential equation: $-\\nabla\\cdot(k\\nabla T)=0$ on the unit square $\\Omega=[0,1]\\times[0,1]$.\n- Dirichlet boundary conditions: $T=1$ on $x=0$, and $T=0$ on $x=1$, $y=0$, and $y=1$.\n- A uniform Cartesian grid discretization ($N_x \\times N_y$ nodes, with spacing $h$).\n- A finite volume discretization scheme using a harmonic mean for face conductivities.\n- A definition for the piecewise-constant conductivity field $k(x,y)$.\n- A specific formula for a spatially varying relaxation field $\\omega_{i,j}$.\n- An initial guess of $T_{i,j}=0$ for interior nodes.\n- A pointwise iterative update rule (a form of SOR).\n- A convergence criterion based on the infinity norm of the residual, $\\|r\\|_{\\infty}  \\tau$.\n- Three distinct parameter sets for testing.\n\nThe problem is scientifically grounded, describing a standard boundary value problem in heat transfer, and the numerical method proposed is a valid, though non-standard, iterative technique. The problem is well-posed. The discrete residual at each interior node, used for the convergence check, is defined as:\n$$r_{i,j} = a_E\\,T_{i+1,j}+a_W\\,T_{i-1,j}+a_N\\,T_{i,j+1}+a_S\\,T_{i,j-1}-A_P\\,T_{i,j}$$\nA minor ambiguity exists in the specification of boundary conditions at corners. For example, node $(0,0)$ is on both the left boundary ($T=1$) and the bottom boundary ($T=0$). Fortunately, this ambiguity does not affect the solution for the given test cases, as the single interior node's update depends only on neighbors that are not at these corners.\n\nThe core problem is valid and can be solved.\n\n**Methodology**\n\nThe solution will be obtained by implementing the described algorithm. For each test case, the following steps are performed:\n\n1.  **Grid and Field Initialization**: A $N_x \\times N_y$ grid is established. The temperature field $T$ is initialized with the specified boundary conditions and an initial guess of $T_{i,j}=0$ for all interior nodes. The thermal conductivity field $k$ and the relaxation parameter field $\\omega$ are initialized according to their definitions for every node $(i,j)$ on the grid.\n\n2.  **Pre-computation of Coefficients**: For efficiency, the discrete coefficients $a_E, a_W, a_N, a_S,$ and $A_P$ are pre-computed and stored for each interior node, as they do not change during the iteration. The face conductivities $k_{i\\pm 1/2, j}$ and $k_{i, j\\pm 1/2}$ are calculated using the harmonic mean to ensure physical correctness at material interfaces.\n\n3.  **Iterative Solution**: The algorithm proceeds in discrete sweeps. A counter for the number of sweeps is initialized to $0$. The main loop is as follows:\n    a. A full sweep is performed over all interior nodes $(i,j)$ in lexicographic order. For each node, the temperature $T_{i,j}$ is updated in-place using the provided SOR-like formula:\n       $$ T_{i,j} \\leftarrow (1-\\omega_{i,j})\\,T_{i,j}+\\omega_{i,j}\\,\\frac{a_E\\,T_{i+1,j}+a_W\\,T_{i-1,j}+a_N\\,T_{i,j+1}+a_S\\,T_{i,j-1}}{A_P} $$\n    b. The sweep counter is incremented.\n    c. After the sweep, the residual $r_{i,j}$ is computed for all interior nodes using the correct formula.\n    d. The infinity norm of the residual, $\\|r\\|_{\\infty} = \\max_{i,j} |r_{i,j}|$, is calculated.\n    e. If $\\|r\\|_{\\infty}  \\tau$, the process has converged. The loop terminates, and the current sweep count is the result.\n\nThis procedure is repeated for each of the three test cases. The special grid size of $N_x=3, N_y=3$ results in a single interior node. This simplifies the problem significantly. For Cases $1$ and $3$, the relaxation parameter at the interior node evaluates to $\\omega_{1,1} = \\omega_{\\min} = 1.0$, which corresponds to a Gauss-Seidel iteration. For a system with a single variable, this converges in exactly one iteration. For Case $2$, the medium is homogeneous, and the relaxation parameter is a constant $\\omega > 1$, requiring multiple iterations to converge. The Python implementation in the final answer formalizes this logic.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_case(Nx, Ny, s, k_left, k_right, omega_min, omega_max, tau):\n    \"\"\"\n    Solves the heat diffusion problem for a single test case.\n    \"\"\"\n    # Grid setup\n    if Nx = 2 or Ny = 2:\n        return 0  # No interior points to iterate on\n\n    h = 1.0 / (Nx - 1)\n    if not np.isclose(h, 1.0 / (Ny - 1)):\n        # Problem assumes hx=hy=h, this is a check.\n        # This part of code will not be reached for the given test cases.\n        raise ValueError(\"Grid spacing hx and hy must be equal.\")\n\n    # Initialize temperature field T[i, j]\n    T = np.zeros((Nx, Ny), dtype=np.float64)\n    \n    # Boundary conditions. Order of assignment sets corner values. Last one wins.\n    # Set T=0 on y=0, y=1, and x=1 boundaries.\n    T[:, 0] = 0.0\n    T[:, -1] = 0.0\n    T[-1, :] = 0.0\n    # Set T=1 on x=0 boundary.\n    T[0, :] = 1.0\n    \n    # Initial guess for interior points is already 0 from np.zeros.\n\n    # Conductivity field k[i, j]\n    k = np.zeros((Nx, Ny), dtype=np.float64)\n    x_coords = np.linspace(0, 1, Nx)\n    for i in range(Nx):\n        if x_coords[i] = s:\n            k[i, :] = k_left\n        else:\n            k[i, :] = k_right\n\n    # Relaxation parameter field omega[i, j]\n    omega = np.zeros((Nx, Ny), dtype=np.float64)\n    k_min_val = np.min(k)\n    k_max_val = np.max(k)\n    if not np.isclose(k_max_val, k_min_val):\n        omega = omega_min + (omega_max - omega_min) * (k - k_min_val) / (k_max_val - k_min_val)\n    else:\n        omega.fill((omega_min + omega_max) / 2.0)\n\n    # Pre-compute coefficients for interior nodes\n    num_interior_i = Nx - 2\n    num_interior_j = Ny - 2\n    aE = np.zeros((num_interior_i, num_interior_j), dtype=np.float64)\n    aW = np.zeros((num_interior_i, num_interior_j), dtype=np.float64)\n    aN = np.zeros((num_interior_i, num_interior_j), dtype=np.float64)\n    aS = np.zeros((num_interior_i, num_interior_j), dtype=np.float64)\n    AP = np.zeros((num_interior_i, num_interior_j), dtype=np.float64)\n\n    for i_int in range(num_interior_i):\n        for j_int in range(num_interior_j):\n            i, j = i_int + 1, j_int + 1\n            # Harmonic mean for face conductivities\n            k_ip_half_j = 2 * k[i, j] * k[i + 1, j] / (k[i, j] + k[i + 1, j])\n            k_im_half_j = 2 * k[i, j] * k[i - 1, j] / (k[i, j] + k[i - 1, j])\n            k_i_jp_half = 2 * k[i, j] * k[i, j + 1] / (k[i, j] + k[i, j + 1])\n            k_i_jm_half = 2 * k[i, j] * k[i, j - 1] / (k[i, j] + k[i, j - 1])\n            \n            # Coefficients\n            aE[i_int, j_int] = k_ip_half_j / h**2\n            aW[i_int, j_int] = k_im_half_j / h**2\n            aN[i_int, j_int] = k_i_jp_half / h**2\n            aS[i_int, j_int] = k_i_jm_half / h**2\n            AP[i_int, j_int] = aE[i_int, j_int] + aW[i_int, j_int] + aN[i_int, j_int] + aS[i_int, j_int]\n\n    sweeps = 0\n    max_sweeps = 50000 # Safety limit\n\n    while sweeps  max_sweeps:\n        # Perform a sweep over interior nodes (in-place lexicographic update)\n        for i_int in range(num_interior_i):\n            for j_int in range(num_interior_j):\n                i, j = i_int + 1, j_int + 1\n                \n                ap_val = AP[i_int, j_int]\n                if ap_val  0:\n                    gs_update = (aE[i_int, j_int] * T[i + 1, j] + \n                                 aW[i_int, j_int] * T[i - 1, j] + \n                                 aN[i_int, j_int] * T[i, j + 1] + \n                                 aS[i_int, j_int] * T[i, j - 1]) / ap_val\n                    T[i, j] = (1 - omega[i, j]) * T[i, j] + omega[i, j] * gs_update\n        \n        sweeps += 1\n\n        # After sweep, compute residual norm\n        max_r = 0.0\n        for i_int in range(num_interior_i):\n            for j_int in range(num_interior_j):\n                i, j = i_int + 1, j_int + 1\n                # Using the corrected residual formula\n                r_ij = (aE[i_int, j_int] * T[i + 1, j] + \n                        aW[i_int, j_int] * T[i - 1, j] + \n                        aN[i_int, j_int] * T[i, j + 1] + \n                        aS[i_int, j_int] * T[i, j - 1] - \n                        AP[i_int, j_int] * T[i, j])\n                \n                if abs(r_ij)  max_r:\n                    max_r = abs(r_ij)\n\n        if max_r  tau:\n            break\n            \n    return sweeps\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: (Nx,Ny), s, k_left, k_right, omega_min, omega_max, tau\n        ((3, 3), 0.5, 1.0, 5.0, 1.0, 1.95, 1.0e-8),\n        # Case 2\n        ((3, 3), 0.5, 3.0, 3.0, 1.0, 1.95, 1.0e-8),\n        # Case 3\n        ((3, 3), 0.5, 1.0, 100.0, 1.0, 1.95, 1.0e-8),\n    ]\n\n    results = []\n    for case in test_cases:\n        (Nx, Ny), s, k_left, k_right, omega_min, omega_max, tau = case\n        num_sweeps = solve_case(Nx, Ny, s, k_left, k_right, omega_min, omega_max, tau)\n        results.append(num_sweeps)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While theoretical analysis provides an optimal $\\omega$ for certain well-behaved matrices, finding it for complex, real-world systems is often impractical. This hands-on problem introduces a powerful, pragmatic solution: an adaptive SOR algorithm that tunes its own relaxation parameter on the fly . By implementing a feedback mechanism that adjusts $\\omega$ based on the observed convergence rate, you will build a more robust and automated solver.",
            "id": "2441060",
            "problem": "Implement a program that solves linear systems using an adaptive Successive Over-Relaxation (SOR) method in which the relaxation parameter is updated every fixed number of iterations based on a mathematically defined measure of convergence speed. Consider linear systems of the form $A \\mathbf{x} = \\mathbf{b}$ with $A \\in \\mathbb{R}^{n \\times n}$ and $\\mathbf{b} \\in \\mathbb{R}^{n}$. The program must implement the following components.\n\n1) Iterative scheme. Let $A = D - L - U$ be the decomposition of $A$ into its diagonal part $D$, the negative of its strictly lower-triangular part $L$, and the negative of its strictly upper-triangular part $U$. For a given relaxation parameter $\\omega \\in (0,2)$ and an iterate $\\mathbf{x}^{(k)}$, the next iterate $\\mathbf{x}^{(k+1)}$ is defined componentwise for $i = 1,\\dots,n$ by\n$$\nx^{(k+1)}_i = (1 - \\omega)\\,x^{(k)}_i + \\frac{\\omega}{a_{ii}}\\left(b_i - \\sum_{j=1}^{i-1} a_{ij}\\,x^{(k+1)}_j - \\sum_{j=i+1}^{n} a_{ij}\\,x^{(k)}_j\\right).\n$$\nEquivalently, in matrix form,\n$$\n(D - \\omega L) \\mathbf{x}^{(k+1)} = ((1-\\omega)D + \\omega U) \\mathbf{x}^{(k)} + \\omega \\mathbf{b}.\n$$\n\n2) Residual and convergence measure. Define the residual at iteration $k$ as $r^{(k)} = \\mathbf{b} - A \\mathbf{x}^{(k)}$, and its Euclidean norm (two-norm) as $\\|r^{(k)}\\|_2$. For a fixed positive integer $N_{\\text{upd}}$, define the windowed convergence factor at iteration $k$ (only for $k$ that are positive multiples of $N_{\\text{upd}}$) by\n$$\nq^{(k)} = \\left(\\frac{\\|r^{(k)}\\|_2}{\\|r^{(k - N_{\\text{upd}})}\\|_2}\\right)^{1/N_{\\text{upd}}}.\n$$\nUse the convention that if $\\|r^{(k - N_{\\text{upd}})}\\|_2 = 0$ then the iteration has already converged and the procedure terminates before any update is attempted.\n\n3) Adaptive update rule. Let $\\alpha \\in (0,1)$ be a gain, $q_{\\text{tgt}} \\in (0,1)$ be a target convergence factor, and let $\\omega_{\\min}$ and $\\omega_{\\max}$ satisfy $0  \\omega_{\\min}  \\omega_{\\max}  2$. At iterations $k$ that are positive multiples of $N_{\\text{upd}}$ and for which $q^{(k)}$ is well-defined, update the relaxation parameter by\n$$\n\\omega \\leftarrow \\operatorname{clip}\\!\\left(\\omega \\cdot \\left[1 + \\alpha \\left(q_{\\text{tgt}} - q^{(k)}\\right)\\right],\\; \\omega_{\\min},\\; \\omega_{\\max}\\right),\n$$\nwhere $\\operatorname{clip}(z, a, b) = \\min\\{\\max\\{z,a\\}, b\\}$. Initialize the update memory by setting the reference residual norm for the first update to $\\|r^{(0)}\\|_2$.\n\n4) Initialization and stopping. Let $\\mathbf{x}^{(0)}$ be the initial guess and $\\omega^{(0)}$ the initial relaxation parameter. Stop at the smallest $k \\ge 0$ for which $\\|r^{(k)}\\|_2 / \\|r^{(0)}\\|_2 \\le \\varepsilon$ for a prescribed relative tolerance $\\varepsilon \\in (0,1)$, or stop when a prescribed maximum number of iterations $k_{\\max}$ is reached. If $\\|r^{(0)}\\|_2 = 0$, then define the iteration count as $k=0$ and the procedure terminates immediately with the current $\\omega$.\n\n5) Required outputs. For each test case, your program must output a list $[k_{\\text{stop}}, \\omega_{\\text{final}}, \\rho_{\\text{rel}}]$, where $k_{\\text{stop}}$ is the integer iteration count at termination, $\\omega_{\\text{final}}$ is the final relaxation parameter value, and $\\rho_{\\text{rel}} = \\|r^{(k_{\\text{stop}})}\\|_2/\\|r^{(0)}\\|_2$ is the final relative residual norm. Report $\\omega_{\\text{final}}$ and $\\rho_{\\text{rel}}$ rounded to exactly six digits after the decimal point.\n\nTest suite. Implement and run your solver on the following four test cases. All matrices are symmetric positive definite by construction, and all random quantities must be generated using the specified seeds to ensure reproducibility.\n\n- Case A (one-dimensional Poisson, moderate size). Let $n = 50$. Define $A \\in \\mathbb{R}^{n \\times n}$ by $a_{ii} = 2$ for $i=1,\\dots,n$, $a_{i,i+1} = a_{i+1,i} = -1$ for $i=1,\\dots,n-1$, and all other entries $0$. Let $\\mathbf{b} \\in \\mathbb{R}^{n}$ be the vector of all ones. Let $\\mathbf{x}^{(0)}$ be the zero vector. Use $\\omega^{(0)} = 1.0$, $N_{\\text{upd}} = 10$, $\\alpha = 0.3$, $q_{\\text{tgt}} = 0.5$, $\\omega_{\\min} = 0.8$, $\\omega_{\\max} = 1.95$, $\\varepsilon = 1.0 \\times 10^{-8}$, and $k_{\\max} = 20000$.\n\n- Case B (dense random symmetric positive definite). Let $n = 40$. Using a pseudorandom generator seeded with $7$, draw a matrix $R \\in \\mathbb{R}^{n \\times n}$ with independent standard normal entries, and define $A = R^{\\top} R + \\gamma I$ with $\\gamma = 1.0$. Using a pseudorandom generator seeded with $11$, draw $\\mathbf{b} \\in \\mathbb{R}^{n}$ with independent standard normal entries. Let $\\mathbf{x}^{(0)}$ be the zero vector. Use $\\omega^{(0)} = 1.0$, $N_{\\text{upd}} = 5$, $\\alpha = 0.2$, $q_{\\text{tgt}} = 0.6$, $\\omega_{\\min} = 0.6$, $\\omega_{\\max} = 1.95$, $\\varepsilon = 1.0 \\times 10^{-8}$, and $k_{\\max} = 20000$.\n\n- Case C (immediate convergence edge case). Let $n = 10$. Using a pseudorandom generator seeded with $3$, draw $R \\in \\mathbb{R}^{n \\times n}$ with independent standard normal entries, and define $A = R^{\\top} R + \\gamma I$ with $\\gamma = 1.0$. Using a pseudorandom generator seeded with $5$, draw $\\mathbf{x}^{\\star} \\in \\mathbb{R}^{n}$ with independent standard normal entries and set $\\mathbf{b} = A \\mathbf{x}^{\\star}$. Initialize $\\mathbf{x}^{(0)} = \\mathbf{x}^{\\star}$. Use $\\omega^{(0)} = 1.5$, $N_{\\text{upd}} = 3$, $\\alpha = 0.5$, $q_{\\text{tgt}} = 0.5$, $\\omega_{\\min} = 0.5$, $\\omega_{\\max} = 1.95$, $\\varepsilon = 1.0 \\times 10^{-8}$, and $k_{\\max} = 20000$.\n\n- Case D (one-dimensional Poisson, frequent updates near upper bound). Let $n = 20$. Define $A$ as in Case A with size $n$. Let $\\mathbf{b}$ be the vector of all ones. Let $\\mathbf{x}^{(0)}$ be the zero vector. Use $\\omega^{(0)} = 1.9$, $N_{\\text{upd}} = 1$, $\\alpha = 0.4$, $q_{\\text{tgt}} = 0.4$, $\\omega_{\\min} = 1.0$, $\\omega_{\\max} = 1.95$, $\\varepsilon = 1.0 \\times 10^{-8}$, and $k_{\\max} = 20000$.\n\nFinal output format. Your program should produce a single line of output containing the results for the four cases as a comma-separated list of lists, in the order A, B, C, D. For each case, output the list $[k_{\\text{stop}}, \\omega_{\\text{final}}, \\rho_{\\text{rel}}]$ with $\\omega_{\\text{final}}$ and $\\rho_{\\text{rel}}$ rounded to exactly six decimal places. For example, a valid overall output format is\n\"[[k_A,omega_A,rel_A],[k_B,omega_B,rel_B],[k_C,omega_C,rel_C],[k_D,omega_D,rel_D]]\"\nwhere each symbol corresponds to the respective numeric values for the case indicated.",
            "solution": "The problem presented is a well-defined task in computational engineering, specifically in the field of numerical linear algebra. It requires the implementation of an adaptive Successive Over-Relaxation (SOR) method for solving a linear system of equations $A \\mathbf{x} = \\mathbf{b}$. The problem is scientifically grounded, internally consistent, and complete. All necessary parameters, initial conditions, and test cases are specified, allowing for a unique and verifiable solution. Therefore, I will proceed with its resolution.\n\nThe core of the problem is the SOR iterative method. For a linear system $A \\mathbf{x} = \\mathbf{b}$, where $A$ is an $n \\times n$ matrix, the SOR method generates a sequence of approximate solutions $\\mathbf{x}^{(k)}$. The transition from an iterate $\\mathbf{x}^{(k)}$ to the next, $\\mathbf{x}^{(k+1)}$, is governed by a relaxation parameter $\\omega \\in (0, 2)$. The update for each component $x_i^{(k+1)}$ is given by:\n$$\nx^{(k+1)}_i = (1 - \\omega)\\,x^{(k)}_i + \\frac{\\omega}{a_{ii}}\\left(b_i - \\sum_{j=1}^{i-1} a_{ij}\\,x^{(k+1)}_j - \\sum_{j=i+1}^{n} a_{ij}\\,x^{(k)}_j\\right)\n$$\nThis formula indicates that the computation of $x_i^{(k+1)}$ utilizes the most recently updated components $x_j^{(k+1)}$ for $j  i$. This structure is characteristic of Gauss-Seidel type methods and lends itself to an efficient in-place implementation where a single vector for the solution is progressively updated.\n\nA crucial aspect of this problem is the adaptive tuning of the relaxation parameter $\\omega$. The performance of the SOR method is highly sensitive to the choice of $\\omega$. The optimal value, $\\omega_{\\text{opt}}$, which minimizes the number of iterations, is generally unknown. The prescribed adaptive strategy is a heuristic feedback control mechanism designed to dynamically adjust $\\omega$ towards a more effective value. This is achieved by monitoring the convergence rate.\n\nThe empirical convergence factor over a window of $N_{\\text{upd}}$ iterations is defined as:\n$$\nq^{(k)} = \\left(\\frac{\\|r^{(k)}\\|_2}{\\|r^{(k - N_{\\text{upd}})}\\|_2}\\right)^{1/N_{\\text{upd}}}\n$$\nwhere $r^{(k)} = \\mathbf{b} - A\\mathbf{x}^{(k)}$ is the residual at iteration $k$, and $\\|\\cdot\\|_2$ denotes the Euclidean norm. This factor $q^{(k)}$ provides an estimate of the average reduction in the residual norm per iteration over the last $N_{\\text{upd}}$ steps. A smaller $q^{(k)}$ signifies faster convergence.\n\nThe adaptive update rule adjusts $\\omega$ based on the deviation of the measured convergence factor $q^{(k)}$ from a desired target factor $q_{\\text{tgt}}$:\n$$\n\\omega_{\\text{new}} = \\omega_{\\text{old}} \\cdot \\left[1 + \\alpha \\left(q_{\\text{tgt}} - q^{(k)}\\right)\\right]\n$$\nHere, $\\alpha$ is a gain parameter that controls the magnitude of the adjustment. If the observed convergence is slower than the target ($q^{(k)}  q_{\\text{tgt}}$), the term in the brackets becomes less than $1$, reducing $\\omega$. Conversely, if convergence is faster than the target ($q^{(k)}  q_{\\text{tgt}}$), $\\omega$ is increased. The updated value is then clipped to stay within a safe interval $[\\omega_{\\min}, \\omega_{\\max}]$ to maintain stability, as SOR is guaranteed to converge for symmetric positive-definite matrices only if $\\omega \\in (0, 2)$. This update is performed at regular intervals, every $N_{\\text{upd}}$ iterations.\n\nThe overall algorithm proceeds as follows:\n1.  Initialize the solution vector $\\mathbf{x}^{(0)}$, the relaxation parameter $\\omega^{(0)}$, and the iteration counter $k=0$.\n2.  Compute the initial residual $r^{(0)} = \\mathbf{b} - A \\mathbf{x}^{(0)}$ and its norm $\\|r^{(0)}\\|_2$. If $\\|r^{(0)}\\|_2 = 0$, the initial guess is the exact solution; terminate with $k=0$. This initial norm serves as the reference for convergence checking and for the first $\\omega$ update.\n3.  Begin the main iterative loop, for $k = 1, 2, \\ldots, k_{\\max}$.\n4.  Within each iteration, perform a full SOR sweep to update all components of $\\mathbf{x}$, computing $\\mathbf{x}^{(k)}$ from $\\mathbf{x}^{(k-1)}$. This is done using an in-place loop over the components $i = 1, \\ldots, n$.\n5.  After the sweep, compute the new residual $r^{(k)} = \\mathbf{b} - A \\mathbf{x}^{(k)}$ and its norm $\\|r^{(k)}\\|_2$.\n6.  Check for convergence by comparing the relative residual norm $\\|r^{(k)}\\|_2 / \\|r^{(0)}\\|_2$ against the tolerance $\\varepsilon$. If the condition is met, the loop terminates, and the current state $(k, \\omega, \\|r^{(k)}\\|_2/\\|r^{(0)}\\|_2)$ is recorded.\n7.  If the iteration count $k$ is a positive multiple of $N_{\\text{upd}}$, perform the adaptive update of $\\omega$ using the formulas for $q^{(k)}$ and the subsequent adjustment, followed by clipping.\n8.  If the loop completes without convergence, the process terminates due to reaching the maximum number of iterations, $k_{\\max}$.\n\nThis structured approach, combining a classical iterative solver with a modern adaptive control heuristic, is a practical method in computational science. The following implementation will precisely follow this logic for the specified test cases.",
            "answer": "```python\nimport numpy as np\n\ndef adaptive_sor(A, b, x0, omega0, N_upd, alpha, q_tgt, omega_min, omega_max, epsilon, k_max):\n    \"\"\"\n    Solves the linear system Ax = b using an adaptive Successive Over-Relaxation (SOR) method.\n\n    Args:\n        A (np.ndarray): The n x n coefficient matrix.\n        b (np.ndarray): The n-dimensional right-hand side vector.\n        x0 (np.ndarray): The initial guess for the solution vector.\n        omega0 (float): The initial relaxation parameter.\n        N_upd (int): The number of iterations between omega updates.\n        alpha (float): The gain for the omega update rule.\n        q_tgt (float): The target convergence factor.\n        omega_min (float): The minimum allowed value for omega.\n        omega_max (float): The maximum allowed value for omega.\n        epsilon (float): The relative tolerance for the stopping criterion.\n        k_max (int): The maximum number of iterations.\n\n    Returns:\n        list: A list containing [k_stop, omega_final, rho_rel], where\n              k_stop is the final iteration count,\n              omega_final is the final relaxation parameter, and\n              rho_rel is the final relative residual norm.\n    \"\"\"\n    n = A.shape[0]\n    x = x0.copy()\n    omega = omega0\n\n    # Initial state (k=0)\n    r0 = b - A @ x\n    norm_r0 = np.linalg.norm(r0)\n\n    # Handle immediate convergence case\n    if norm_r0 == 0:\n        return [0, omega, 0.0]\n\n    # Initialize memory for adaptive updates\n    norm_r_at_last_update = norm_r0\n    \n    k_stop = 0\n    rho_rel = 1.0\n\n    # Main iteration loop\n    for k in range(1, k_max + 1):\n        # Perform one full SOR sweep (in-place update)\n        for i in range(n):\n            sigma = np.dot(A[i, :i], x[:i]) + np.dot(A[i, i + 1:], x[i + 1:])\n            x[i] = (1 - omega) * x[i] + (omega / A[i, i]) * (b[i] - sigma)\n\n        # Compute residual and its norm for the current iterate\n        r_k = b - A @ x\n        norm_r_k = np.linalg.norm(r_k)\n\n        # Check stopping criterion\n        rho_rel = norm_r_k / norm_r0\n        if rho_rel = epsilon:\n            k_stop = k\n            return [k_stop, omega, rho_rel]\n\n        # Update omega if it's an update iteration\n        if k  0 and k % N_upd == 0:\n            if norm_r_at_last_update  0:\n                q_k = (norm_r_k / norm_r_at_last_update)**(1.0 / N_upd)\n                omega_new = omega * (1 + alpha * (q_tgt - q_k))\n                omega = np.clip(omega_new, omega_min, omega_max)\n            \n            # Store current norm for the next update window\n            norm_r_at_last_update = norm_r_k\n\n    # If loop finishes, max iterations was reached\n    k_stop = k_max\n    final_r = b - A @ x\n    final_norm_r = np.linalg.norm(final_r)\n    rho_rel = final_norm_r / norm_r0\n    \n    return [k_stop, omega, rho_rel]\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test cases for the adaptive SOR solver.\n    \"\"\"\n    test_cases = []\n\n    # Case A\n    n_A = 50\n    A_A = np.diag(np.full(n_A, 2.0)) - np.diag(np.ones(n_A - 1), 1) - np.diag(np.ones(n_A - 1), -1)\n    b_A = np.ones(n_A)\n    x0_A = np.zeros(n_A)\n    params_A = {'x0': x0_A, 'omega0': 1.0, 'N_upd': 10, 'alpha': 0.3, 'q_tgt': 0.5,\n                'omega_min': 0.8, 'omega_max': 1.95, 'epsilon': 1e-8, 'k_max': 20000}\n    test_cases.append(('A', A_A, b_A, params_A))\n\n    # Case B\n    n_B = 40\n    rng_B_A = np.random.default_rng(7)\n    R_B = rng_B_A.standard_normal((n_B, n_B))\n    A_B = R_B.T @ R_B + 1.0 * np.identity(n_B)\n    rng_B_b = np.random.default_rng(11)\n    b_B = rng_B_b.standard_normal(n_B)\n    x0_B = np.zeros(n_B)\n    params_B = {'x0': x0_B, 'omega0': 1.0, 'N_upd': 5, 'alpha': 0.2, 'q_tgt': 0.6,\n                'omega_min': 0.6, 'omega_max': 1.95, 'epsilon': 1e-8, 'k_max': 20000}\n    test_cases.append(('B', A_B, b_B, params_B))\n\n    # Case C\n    n_C = 10\n    rng_C_A = np.random.default_rng(3)\n    R_C = rng_C_A.standard_normal((n_C, n_C))\n    A_C = R_C.T @ R_C + 1.0 * np.identity(n_C)\n    rng_C_x = np.random.default_rng(5)\n    x_star_C = rng_C_x.standard_normal(n_C)\n    b_C = A_C @ x_star_C\n    x0_C = x_star_C\n    params_C = {'x0': x0_C, 'omega0': 1.5, 'N_upd': 3, 'alpha': 0.5, 'q_tgt': 0.5,\n                'omega_min': 0.5, 'omega_max': 1.95, 'epsilon': 1e-8, 'k_max': 20000}\n    test_cases.append(('C', A_C, b_C, params_C))\n\n    # Case D\n    n_D = 20\n    A_D = np.diag(np.full(n_D, 2.0)) - np.diag(np.ones(n_D - 1), 1) - np.diag(np.ones(n_D - 1), -1)\n    b_D = np.ones(n_D)\n    x0_D = np.zeros(n_D)\n    params_D = {'x0': x0_D, 'omega0': 1.9, 'N_upd': 1, 'alpha': 0.4, 'q_tgt': 0.4,\n                'omega_min': 1.0, 'omega_max': 1.95, 'epsilon': 1e-8, 'k_max': 20000}\n    test_cases.append(('D', A_D, b_D, params_D))\n\n    all_results = []\n    for _, A, b, params in test_cases:\n        result = adaptive_sor(A, b, **params)\n        \n        k_stop = result[0]\n        omega_final = result[1]\n        rho_rel = result[2]\n        \n        # Format the numbers for the final output string\n        formatted_result = [k_stop, f\"{omega_final:.6f}\", f\"{rho_rel:.6f}\"]\n        # Convert formatted floats back to string representation needed for a list\n        # This approach builds a list that can be easily converted to a string\n        all_results.append(f\"[{formatted_result[0]},{formatted_result[1]},{formatted_result[2]}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}