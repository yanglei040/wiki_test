## Introduction
Solving large systems of linear equations of the form $A x = b$ is a cornerstone of computational engineering, arising from the [discretization](@entry_id:145012) of physical models across countless disciplines. As problem sizes grow, direct solution methods become computationally infeasible, making iterative techniques essential. Among these, Krylov subspace methods stand out as the most powerful and widely used class of algorithms, offering a balance of speed, accuracy, and versatility. This article provides a comprehensive introduction to these essential numerical tools.

This article is structured to build a robust understanding from the ground up. In **"Principles and Mechanisms,"** we will delve into the fundamental theory, exploring how Krylov subspaces are constructed and used as an efficient search space for the solution. We will examine the projection principles that lead to distinct families of algorithms and differentiate between methods like Conjugate Gradient (CG) for symmetric systems and GMRES or BiCGSTAB for more general cases. Next, **"Applications and Interdisciplinary Connections"** will bridge theory and practice, showcasing how these methods enable [large-scale simulations](@entry_id:189129), solve complex nonlinear problems through Newton-Krylov techniques, and function in powerful matrix-free settings. Finally, **"Hands-On Practices"** will offer opportunities to solidify this knowledge through targeted computational exercises. We begin by establishing the foundational principles that make these methods so effective.

## Principles and Mechanisms

The solution of large-scale [linear systems](@entry_id:147850), $A x = b$, arising from the [discretization of partial differential equations](@entry_id:748527) and other scientific models, presents a formidable computational challenge. Direct methods, such as Gaussian elimination, become prohibitively expensive in terms of both memory and floating-point operations as the system size $n$ grows. Iterative methods provide a powerful alternative, constructing a sequence of approximate solutions $x_0, x_1, \dots, x_k$ that ideally converge to the true solution. Krylov subspace methods form the most successful and widely used class of such iterative techniques. This chapter elucidates the fundamental principles and mechanisms that govern their operation.

### The Krylov Subspace as a Search Space

The core strategy of a Krylov subspace method is to seek an improved approximation to the solution within a carefully constructed, low-dimensional subspace of the full solution space $\mathbb{R}^n$. Rather than searching in a fixed, general-purpose subspace, these methods generate a subspace that is tailored to the specific matrix $A$ and the initial state of the problem.

Given an initial guess $x_0$, the initial error is $e_0 = x - x_0$, and the corresponding initial residual is $r_0 = b - A x_0 = A(x - x_0) = A e_0$. The residual provides a computable measure of the error without knowledge of the true solution $x$. The sequence of vectors generated by repeated application of the matrix $A$ to the initial residual, $\{r_0, A r_0, A^2 r_0, \dots\}$, contains progressively more information about the action of the operator $A$ on the initial error. The vector space spanned by the first $k$ of these vectors is known as the **$k$-th Krylov subspace**, denoted by $\mathcal{K}_k(A, r_0)$:

$$
\mathcal{K}_k(A, r_0) = \text{span}\{r_0, A r_0, A^2 r_0, \dots, A^{k-1} r_0\}
$$

Krylov subspace methods generate the $k$-th iterate, $x_k$, by finding a correction term, $z_{k-1}$, within this subspace. The iterate is thus confined to the affine subspace $x_0 + \mathcal{K}_k(A, r_0)$. For example, the second iterate, $x_2$, must be of the form $x_2 = x_0 + c_0 r_0 + c_1 A r_0$ for some scalars $c_0$ and $c_1$, meaning the update vector $(x_2 - x_0)$ is a [linear combination](@entry_id:155091) of $r_0$ and $A r_0$ .

These subspaces are naturally nested, with $\mathcal{K}_k(A, r_0) \subseteq \mathcal{K}_{k+1}(A, r_0)$. A key property is that applying the matrix $A$ to any vector in $\mathcal{K}_k(A, r_0)$ yields a vector in $\mathcal{K}_{k+1}(A, r_0)$, a relation formally written as $A\mathcal{K}_k(A, r_0) \subseteq \mathcal{K}_{k+1}(A, r_0)$. The dimension of the Krylov subspace grows with each iteration, but it cannot exceed $n$. If at some step $k$ the vector $A^k r_0$ becomes linearly dependent on the preceding vectors $\{r_0, \dots, A^{k-1} r_0\}$, the subspace ceases to grow. This occurs when $k$ is equal to the degree of the [minimal polynomial](@entry_id:153598) of $A$ with respect to $r_0$. At this point, the Krylov subspace becomes an **$A$-invariant subspace**, meaning $A \mathcal{K}_k(A, r_0) \subseteq \mathcal{K}_k(A, r_0)$. If the initial residual $r_0$ lies in an $A$-[invariant subspace](@entry_id:137024) of dimension $m$, then the entire Krylov sequence is contained within it, and it can be shown that the exact solution can be found in at most $m$ steps in exact arithmetic . This property guarantees that Krylov methods terminate with the exact solution in at most $n$ steps, although their practical utility relies on achieving a sufficiently accurate approximation in a much smaller number of iterations, $k \ll n$.

### The Projection Principle: Defining the "Best" Approximation

Once the search space $x_0 + \mathcal{K}_k(A, r_0)$ is defined, we require a criterion to select the "best" approximation $x_k$ from it. This is achieved through a projection process that imposes an [orthogonality condition](@entry_id:168905) on the corresponding residual, $r_k = b - A x_k$. The general framework is the **Petrov-Galerkin condition**, which demands that the residual be orthogonal to a chosen **test subspace** $\mathcal{L}_k$:

$$
r_k \perp \mathcal{L}_k
$$

The choice of the test subspace $\mathcal{L}_k$ is what distinguishes the major classes of Krylov subspace methods. Two principal choices give rise to the most important algorithms:

1.  **Galerkin Condition**: The test subspace is chosen to be the Krylov subspace itself, $\mathcal{L}_k = \mathcal{K}_k(A, r_0)$. This requires $r_k \perp \mathcal{K}_k(A, r_0)$.

2.  **Minimum Residual Condition**: The test subspace is chosen to be the image of the Krylov subspace under $A$, i.e., $\mathcal{L}_k = A\mathcal{K}_k(A, r_0)$. This condition, $r_k \perp A\mathcal{K}_k(A, r_0)$, is mathematically equivalent to finding the iterate $x_k$ that minimizes the Euclidean norm of the residual, $\|r_k\|_2 = \|b - A x_k\|_2$, over the affine search space .

The properties of the matrix $A$—particularly symmetry and definiteness—determine which of these conditions can be implemented efficiently and what properties the resulting algorithm will possess.

### Symmetric Positive Definite Systems: The Conjugate Gradient Method

When the matrix $A$ is **symmetric and [positive definite](@entry_id:149459) (SPD)**, a scenario that commonly arises from the [discretization](@entry_id:145012) of [elliptic partial differential equations](@entry_id:141811) such as the Poisson equation , the system has a special structure that can be exploited for maximal efficiency. For an SPD matrix, the bilinear form $(u,v)_A = u^T A v$ defines a valid inner product, and the associated **$A$-norm**, $\|v\|_A = \sqrt{v^T A v}$, provides a natural way to measure error.

The **Conjugate Gradient (CG)** method is designed specifically for SPD systems. It is defined by an [optimality criterion](@entry_id:178183): at each iteration $k$, CG finds the unique iterate $x_k \in x_0 + \mathcal{K}_k(A, r_0)$ that minimizes the $A$-norm of the error, $\|x - x_k\|_A$. This energy minimization is equivalent to imposing the Galerkin condition, $r_k \perp \mathcal{K}_k(A, r_0)$ .

The profound consequence of this choice for SPD matrices is that it enforces two crucial orthogonality properties: the residuals are mutually orthogonal in the standard Euclidean inner product ($r_i^T r_j = 0$ for $i \neq j$), and the search directions used to build the iterates are mutually orthogonal in the $A$-inner product ($p_i^T A p_j = 0$ for $i \neq j$). This latter property is known as **$A$-[conjugacy](@entry_id:151754)**.

The true power of CG lies in the fact that these orthogonality and conjugacy conditions can be maintained with **short-term recurrences**. This means that to compute the updates for step $k+1$, the algorithm only needs information from step $k$ and step $k-1$. It does not need to store the entire basis of the Krylov subspace or explicitly enforce orthogonality against all previous vectors. This makes the storage requirements of CG minimal and fixed (typically 4-5 vectors of size $n$), and the computational work per iteration is also constant and low . It is this reliance on a short-term recurrence, which is only valid if $A$ is symmetric, that distinguishes CG from methods for general matrices . The optimality of CG is not just a theoretical curiosity; it results in convergence that is often significantly faster than that of non-optimal methods like the Heavy Ball iteration, which also uses a simple recurrence but lacks the global optimality of CG over the Krylov subspace .

### Methods for General Non-Symmetric Systems

When $A$ is non-symmetric, as is common in stabilized discretizations of convection-dominated problems , the CG method's foundations crumble. The [bilinear form](@entry_id:140194) $x^T A y$ is no longer symmetric, the $A$-norm is not a valid norm, and the short-term recurrences that grant CG its efficiency no longer hold. Two distinct families of methods have been developed to handle this general case.

#### Minimal Residual Methods: GMRES

This family of methods abandons the Galerkin condition and instead directly enforces the minimum residual property. The most prominent member is the **Generalized Minimal Residual (GMRES)** method. By definition, at each step $k$, GMRES finds the iterate $x_k \in x_0 + \mathcal{K}_k(A, r_0)$ that minimizes the Euclidean norm of the residual, $\|r_k\|_2$. As noted earlier, this is equivalent to the Petrov-Galerkin condition $r_k \perp A\mathcal{K}_k(A, r_0)$.

A direct consequence of this minimization property is that the sequence of [residual norms](@entry_id:754273) must be monotonically non-increasing: $\|r_{k+1}\|_2 \le \|r_k\|_2$, since the search space at step $k+1$ contains the space from step $k$ . This provides smooth and predictable convergence behavior.

To implement this minimization, GMRES must construct an explicit [orthonormal basis](@entry_id:147779) for the Krylov subspace $\mathcal{K}_k(A, r_0)$ via the **Arnoldi process**. This process requires a **long-term recurrence**: to find the $(k+1)$-th [basis vector](@entry_id:199546), it must be orthogonalized against all $k$ previous basis vectors. This has significant practical implications:
*   **Numerical Stability**: The explicit construction of an orthonormal basis makes GMRES very robust and numerically stable. The influence of rounding errors is well-controlled .
*   **Cost**: The need to store the entire basis means storage requirements grow linearly with the iteration count $k$. The number of arithmetic operations per iteration also grows linearly with $k$. For large $k$, this becomes prohibitive.

Due to this growing cost, GMRES is often used in a **restarted** variant, **GMRES(m)**, which runs for $m$ iterations and then restarts using the current solution as the new initial guess. This caps the memory and computational costs but sacrifices the global optimality and [guaranteed convergence](@entry_id:145667) of the full GMRES method.

The convergence rate of GMRES is intimately tied to the theory of polynomial approximation . The residual at step $k$ can be expressed as $r_k = p_k(A)r_0$, where $p_k$ is a polynomial of degree at most $k$ satisfying $p_k(0) = 1$. GMRES finds the specific polynomial that minimizes $\|p_k(A)r_0\|_2$. For [normal matrices](@entry_id:195370), this performance can be bounded by a problem of finding the best polynomial that is small on the spectrum of $A$. For [non-normal matrices](@entry_id:137153), the behavior is more complex and depends on the pseudospectrum, but the core idea remains: GMRES converges quickly if there exists a low-degree polynomial $p_k$ with $p_k(0)=1$ that effectively dampens vectors in the eigenspaces of $A$.

#### Biorthogonalization Methods: BiCGSTAB

The second family of methods for non-symmetric systems seeks to retain the low, fixed cost of a short-term recurrence. This is achieved by replacing the single [orthogonality condition](@entry_id:168905) of CG with a pair of coupled conditions. The foundational method in this class is the **Biconjugate Gradient (BiCG)** method.

BiCG generates two sequences of residuals, the primary residual $r_k$ (from $A x = b$) and a "shadow" residual $\tilde{r}_k$ from a process involving the transpose matrix, $A^T$. Instead of enforcing $r_i^T r_j = 0$, BiCG enforces **[bi-orthogonality](@entry_id:175698)**, $r_i^T \tilde{r}_j = 0$ for $i \neq j$. Similarly, it replaces $A$-conjugacy with **bi-conjugacy** of search directions. These paired conditions are sufficient to derive short-term recurrences, restoring the [computational efficiency](@entry_id:270255) of CG . This approach is fundamentally different from methods like CG on the normal equations (CGNE), which explicitly form $A^T A$ and solve $A^T A x = A^T b$. BiCG works with $A$ and $A^T$ separately.

However, this efficiency comes at a cost. The convergence of BiCG can be erratic, and the algorithm can break down if certain inner products become zero. The **Biconjugate Gradient Stabilized (BiCGSTAB)** method was developed to address these issues. It is a hybrid method that combines the BiCG steps with GMRES(1)-like steps to smooth out the convergence.

The key characteristics of BiCGSTAB, when compared to GMRES, are:
*   **Efficiency**: Like CG, it has fixed, low storage and computational costs per iteration due to its short recurrences.
*   **Irregular Convergence**: Unlike GMRES, BiCGSTAB does not minimize a norm over the entire Krylov subspace. It is a Petrov-Galerkin method that does not guarantee monotonic reduction of the [residual norm](@entry_id:136782). The norm $\|r_k\|_2$ can fluctuate and even increase temporarily before converging .
*   **Numerical Stability**: The short recurrences do not explicitly enforce global [biorthogonality](@entry_id:746831). In [finite precision arithmetic](@entry_id:142321), [rounding errors](@entry_id:143856) can accumulate and lead to a gradual loss of the theoretical [biorthogonality](@entry_id:746831), potentially causing instability or slower convergence compared to the more robust GMRES .

### Symmetric Indefinite Systems: MINRES

A third important class of matrices is **symmetric but indefinite**. These matrices arise, for example, in [saddle-point problems](@entry_id:174221) like the mixed [finite element discretization](@entry_id:193156) of Stokes flow . CG is not applicable because the matrix is not positive definite, and the term $p_k^T A p_k$ could be zero or negative, causing breakdown. GMRES could be used, but it is inefficient as it does not exploit the matrix's symmetry.

The ideal method for this class is the **Minimum Residual (MINRES)** method. Like GMRES, MINRES minimizes the Euclidean norm of the residual, $\|r_k\|_2$, at each step. However, because the matrix is symmetric, it can leverage the **Lanczos process**, which is the symmetric equivalent of the Arnoldi process. The Lanczos process generates the basis for the Krylov subspace using a short, [three-term recurrence](@entry_id:755957).

Thus, MINRES combines the desirable properties of both GMRES and CG for this specific matrix class: it has the robust minimal residual property (and thus monotonic convergence of $\|r_k\|_2$) while being implemented with efficient, low-cost short recurrences .

### The Role of Preconditioning

The convergence rate of any Krylov method is dictated by the spectral properties of the matrix $A$. A matrix with a large condition number or unfavorably distributed eigenvalues will lead to slow convergence. **Preconditioning** is a technique to transform the original system $A x = b$ into an equivalent one that is easier to solve.

A preconditioner is a matrix $M$ that approximates $A$ in some sense and for which the system $M z = c$ is easy to solve. Instead of $A x = b$, one can solve the **left-preconditioned** system $M^{-1}A x = M^{-1} b$ or the **right-preconditioned** system $A M^{-1} y = b$, where $x=M^{-1}y$. The goal is for the effective matrix, $M^{-1}A$ or $A M^{-1}$, to have better spectral properties (e.g., a smaller condition number) than $A$.

When [preconditioning](@entry_id:141204) is applied, the Krylov subspace is generated by the preconditioned operator and the preconditioned residual. For example, in left-preconditioned GMRES, the search space for the update is $\mathcal{K}_k(M^{-1}A, M^{-1}r_0)$ .

A crucial theoretical point arises when preconditioning SPD systems for use with CG. The preconditioned matrix $M^{-1}A$ is generally not symmetric, even if both $A$ and $M$ are SPD. This would seem to prohibit the use of CG. However, if $M$ is SPD, the operator $M^{-1}A$ is similar to the SPD matrix $M^{-1/2}A M^{-1/2}$. This similarity transformation guarantees that the eigenvalues of the non-symmetric matrix $M^{-1}A$ are all real and positive . This hidden symmetry is what allows the **Preconditioned Conjugate Gradient (PCG)** algorithm to work. PCG is formulated to implicitly operate on the symmetric system involving $M^{-1/2}A M^{-1/2}$ while only requiring matrix-vector products with $A$ and solves with $M$. Similarly, to use MINRES with a [preconditioner](@entry_id:137537) while preserving its short recurrence, the preconditioner must be SPD so that the preconditioned operator remains self-adjoint in an appropriate inner product .