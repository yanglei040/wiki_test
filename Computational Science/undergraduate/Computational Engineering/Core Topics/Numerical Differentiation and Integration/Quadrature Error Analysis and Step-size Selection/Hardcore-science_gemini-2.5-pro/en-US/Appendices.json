{
    "hands_on_practices": [
        {
            "introduction": "The theoretical convergence rates of quadrature rules, such as the $\\mathcal{O}(h^4)$ accuracy of Simpson's rule, are derived under the assumption that the integrand is sufficiently smooth. This exercise provides a crucial hands-on test of what happens when this assumption is violated. By applying Simpson's rule to a function with limited differentiability, you will numerically estimate its true convergence order and see firsthand how it deviates from the textbook rate, reinforcing the fundamental link between function properties and numerical performance .",
            "id": "2430715",
            "problem": "Consider the integral of a function with an interior cusp on a closed interval. Let the interval be $[a,b]=[0,1]$ and define, for a given parameter $c \\in (0,1)$, the function $f:[0,1]\\to\\mathbb{R}$ by\n$$\nf(x) = \\lvert x-c \\rvert^{\\alpha}, \\quad \\text{with } \\alpha=\\tfrac{3}{2}.\n$$\nThe function $f$ belongs to the class $C^1$ but not to the class $C^2$ on $[0,1]$. The exact value of the integral is given by\n$$\nI(c) = \\int_{0}^{1} \\lvert x-c \\rvert^{\\alpha}\\,dx = \\frac{(c-0)^{\\alpha+1} + (1-c)^{\\alpha+1}}{\\alpha+1}.\n$$\nDefine the composite Simpson quadrature on a uniform mesh with $N$ subintervals (where $N$ is an even integer) as follows. Let $h = \\frac{b-a}{N}$ and $x_j = a + jh$ for $j=0,1,\\dots,N$. The composite Simpson approximation $S_N(f)$ to $\\int_a^b f(x)\\,dx$ is\n$$\nS_N(f) = \\frac{h}{3}\\left[f(x_0) + 4\\sum_{j=1,\\,j\\ \\text{odd}}^{N-1} f(x_j) + 2\\sum_{j=2,\\,j\\ \\text{even}}^{N-2} f(x_j) + f(x_N)\\right].\n$$\nFor each parameter value $c$, let the absolute error be\n$$\nE_N(c) = \\left\\lvert I(c) - S_N(f) \\right\\rvert.\n$$\nYou must write a complete, runnable program that, for each of the following test cases for $c$, estimates the observed convergence order $p$ such that $E_N(c) = \\Theta(h^{p})$ as $h \\to 0$, using the provided sequence of $N$ values:\n- Test case $1$: $c = 0.3$,\n- Test case $2$: $c = \\tfrac{3}{8}$,\n- Test case $3$: $c = 10^{-3}$.\n\nFor each test case, evaluate $E_N(c)$ for the sequence of even subinterval counts $N \\in \\{8,16,32,64,128,256,512\\}$, and from these values, determine a single representative estimate of the observed convergence order $p$ for that test case. The final output of your program must be a single line containing a list of three floating-point numbers corresponding to the estimated orders for the three test cases, in the order listed above. Express each floating-point number rounded to three decimal places. The required final output format is a single line with a comma-separated list enclosed in square brackets, for example,\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3].\n$$\nNo physical units are involved in this problem. Angles do not appear. Percentages are not required. All numerical values in the output must be floats.",
            "solution": "The problem statement is well-posed, mathematically consistent, and scientifically grounded. It presents a standard task in numerical analysis: the experimental determination of the convergence order for a quadrature rule when applied to a function with limited smoothness. We shall proceed with the solution.\n\nThe composite Simpson's rule is a numerical integration method of order four. Its absolute error $E_N$ for an integral on $[a,b]$ typically behaves as $E_N = \\mathcal{O}(h^4)$, where $h = (b-a)/N$ is the step size. This order of convergence is guaranteed only if the integrand $f$ is four times continuously differentiable, i.e., $f \\in C^4([a,b])$.\n\nThe function provided is $f(x) = \\lvert x-c \\rvert^{\\alpha}$ with $\\alpha=\\frac{3}{2}$. Its derivatives are:\n$$ f'(x) = \\frac{3}{2} \\text{sgn}(x-c) \\lvert x-c \\rvert^{1/2} $$\n$$ f''(x) = \\frac{3}{4} \\lvert x-c \\rvert^{-1/2} $$\nThe first derivative $f'(x)$ is continuous on $[0,1]$, thus $f \\in C^1([0,1])$. However, the second derivative $f''(x)$ has a singularity at $x=c$, meaning $f \\notin C^2([0,1])$. The failure of the smoothness condition $f \\in C^4([0,1])$ leads to a degradation of the convergence order.\n\nFor integrands with an algebraic singularity of the form $|x-c|^{\\beta}$, the convergence order $p$ of an $m$-point Newton-Cotes rule, such as Simpson's rule, is given by the theory of quadrature for non-smooth functions. The order depends on whether the singularity at $c$ coincides with a quadrature node:\n1.  If the singularity at $c$ is not a quadrature node for any $N$ in the sequence, the order of convergence is $p = \\alpha+1$. For $\\alpha = \\frac{3}{2}$, this gives $p = \\frac{3}{2}+1 = 2.5$.\n2.  If the singularity at $c$ is a quadrature node for all sufficiently large $N$, the error is improved, and the order of convergence becomes $p = \\alpha+2$. For $\\alpha = \\frac{3}{2}$, this gives $p = \\frac{3}{2}+2 = 3.5$.\n\nWe analyze the test cases based on this theoretical foundation:\n- **Test case 1 ($c = 0.3$)**: The value $c=0.3 = \\frac{3}{10}$ is not a dyadic rational. The mesh points are $x_j = j/N$ where $N$ is a power of $2$ multiplied by $8$. Thus, $c$ will never be a mesh point. We expect a convergence order of $p \\approx 2.5$.\n- **Test case 2 ($c = \\frac{3}{8}$)**: The value $c=0.375=\\frac{3}{8}$ is a dyadic rational. For $N=8, 16, 32, \\dots$, the point $c$ is always a mesh point (e.g., for $N=8$, $x_3 = \\frac{3}{8}$). We expect a convergence order of $p \\approx 3.5$.\n- **Test case 3 ($c = 10^{-3}$)**: The value $c=0.001=\\frac{1}{1000}$ is not a dyadic rational. It will not be a mesh point. We expect a convergence order of $p \\approx 2.5$.\n\nTo estimate the order of convergence $p$ numerically, we assume the error follows the relation $E_N \\approx K h^p$ for some constant $K$, which can be rewritten as $E_N \\approx K' N^{-p}$. Taking the natural logarithm of this expression yields:\n$$ \\ln(E_N) \\approx \\ln(K') - p \\ln(N) $$\nThis equation shows a linear relationship between $\\ln(E_N)$ and $\\ln(N)$, with a slope of $-p$. To find a robust estimate for $p$, we will compute the errors $E_N(c)$ for the given sequence of $N \\in \\{8, 16, 32, 64, 128, 256, 512\\}$. We then perform a linear least-squares regression on the set of points $\\{(\\ln(N_i), \\ln(E_{N_i}))\\}_{i=1}^7$. The slope $m$ of the best-fit line is calculated as:\n$$ m = \\frac{\\sum_{i=1}^7 (\\ln(N_i) - \\overline{\\ln(N)}) (\\ln(E_{N_i}) - \\overline{\\ln(E)})}{\\sum_{i=1}^7 (\\ln(N_i) - \\overline{\\ln(N)})^2} $$\nThe estimated convergence order is then $p = -m$.\n\nThe following procedure is implemented for each test case:\n1.  Define the constants $\\alpha = \\frac{3}{2}$ and the test case parameter $c$.\n2.  Calculate the exact integral $I(c) = \\frac{c^{\\alpha+1} + (1-c)^{\\alpha+1}}{\\alpha+1}$.\n3.  For each $N$ in the sequence, compute the composite Simpson's rule approximation $S_N(f)$.\n4.  Calculate the absolute error $E_N(c) = |I(c) - S_N(f)|$.\n5.  Using the set of calculated errors and corresponding $N$ values, determine the slope $m$ of the log-log plot via linear regression.\n6.  The convergence order is $p = -m$. The final value is rounded to three decimal places.\nThis entire procedure is encapsulated in the provided program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef estimate_convergence_order(c, N_values, alpha):\n    \"\"\"\n    Estimates the convergence order for Simpson's rule for the function\n    f(x) = |x-c|^alpha.\n\n    Args:\n        c (float): The location of the cusp in the function.\n        N_values (list or np.ndarray): A sequence of even integers representing\n                                      the number of subintervals.\n        alpha (float): The exponent in the function definition.\n\n    Returns:\n        float: The estimated order of convergence p.\n    \"\"\"\n    # 1. Define the function and its exact integral\n    f = lambda x: np.abs(x - c)**alpha\n    alpha_p1 = alpha + 1\n    I_exact = (c**alpha_p1 + (1 - c)**alpha_p1) / alpha_p1\n\n    errors = []\n    # 2. Loop through N values to calculate quadrature error\n    for N in N_values:\n        a, b = 0.0, 1.0\n        h = (b - a) / N\n        x = np.linspace(a, b, N + 1)\n        y = f(x)\n\n        # Composite Simpson's rule formula\n        # S_N = (h/3) * [f(x_0) + 4*sum(f(x_odd)) + 2*sum(f(x_even)) + f(x_N)]\n        S_N = (h / 3) * (y[0] + 4 * np.sum(y[1:-1:2]) + 2 * np.sum(y[2:-1:2]) + y[-1])\n        \n        # Absolute error\n        error = np.abs(I_exact - S_N)\n        errors.append(error)\n\n    # 3. Estimate convergence order using linear regression on log-log data\n    # The error model is E_N ≈ K * N^(-p).\n    # Taking logs: log(E_N) ≈ log(K) - p * log(N).\n    # This is a linear relationship between log(E_N) and log(N).\n    # The slope of this line is -p.\n    \n    log_N = np.log(np.array(N_values, dtype=float))\n    log_E = np.log(np.array(errors, dtype=float))\n\n    # Calculate the slope 'm' of the best-fit line for y = mx + b,\n    # where y = log_E and x = log_N.\n    # m = Cov(x, y) / Var(x)\n    x = log_N\n    y = log_E\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    \n    slope = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean)**2)\n    \n    # The convergence order p is the negative of the slope.\n    p = -slope\n    \n    return p\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Define the parameters from the problem statement.\n    test_cases = [\n        0.3,\n        3.0 / 8.0,\n        1e-3\n    ]\n    \n    # Sequence of subintervals for convergence analysis\n    N_values = [8, 16, 32, 64, 128, 256, 512]\n    \n    # Exponent alpha\n    alpha = 1.5\n\n    results = []\n    for c in test_cases:\n        # Calculate the order p for the current test case\n        p_estimated = estimate_convergence_order(c, N_values, alpha)\n        \n        # Round the result to three decimal places\n        results.append(p_estimated)\n\n    # Format the final output as a string with 3 decimal places for each number\n    formatted_results = [f\"{res:.3f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Adaptive quadrature algorithms are powerful because they use local error estimators to intelligently place grid points where they are needed most. However, these estimators are not foolproof. This practice explores a carefully constructed, yet simple, scenario where a standard Richardson extrapolation-based error estimator is deceived into reporting zero error, causing the algorithm to terminate prematurely with a wildly inaccurate result . Working through this \"pathological case\" will sharpen your critical understanding of the assumptions and potential failure modes inherent in numerical methods.",
            "id": "2430730",
            "problem": "An adaptive integration routine uses the composite trapezoidal rule with step size $h$ and its halved version $h/2$, together with the standard Richardson extrapolation error estimate for a second-order method. Specifically, on an interval $[0,1]$ the composite trapezoidal approximations are denoted by $T_h$ and $T_{h/2}$, and the routine forms the error estimate $\\widehat{E}_h = \\left|T_{h/2} - T_h\\right|/3$. The routine starts from $h = 1$ and computes $T_1$ and $T_{1/2}$; if $\\widehat{E}_1 \\le \\text{tol}$ it accepts $T_{1/2}$ and stops.\n\nConsider the smooth function $f(x) = x^{2}\\left(x - \\tfrac{1}{2}\\right)^{2}(x - 1)^{2}$ on $[0,1]$ and suppose the tolerance is $\\text{tol} = 10^{-4}$. Using only the definitions of the composite trapezoidal rule and the Richardson extrapolation concept for second-order methods, analyze whether the routine will accept after the first refinement and compute the true absolute error that would be committed by returning $T_{1/2}$ for this $f$.\n\nYour final answer must be the exact value of the absolute error as a single number. Do not round; give the exact value.",
            "solution": "The problem requires an analysis of an adaptive integration routine based on the composite trapezoidal rule and Richardson extrapolation. We must determine if the routine terminates after the first step for a specific function and tolerance, and then compute the true error of its result.\n\nFirst, we formalize the components of the problem. The composite trapezoidal rule for approximating an integral $I = \\int_a^b f(x) dx$ using $n$ subintervals of width $h = (b-a)/n$ is given by\n$$ T(f, h) = h \\left( \\frac{f(a) + f(b)}{2} + \\sum_{k=1}^{n-1} f(a+kh) \\right) $$\nThe problem is set on the interval $[0, 1]$. The routine starts with a step size $h=1$. The approximations are denoted $T_h$. So, we start with $h=1$ and compute $T_1$ and $T_{1/2}$.\n\nThe function to be integrated is $f(x) = x^{2}\\left(x - \\frac{1}{2}\\right)^{2}(x - 1)^{2}$.\nThe integration nodes for the first two approximations are $x \\in \\{0, 1\\}$ for $T_1$, and $x \\in \\{0, \\frac{1}{2}, 1\\}$ for $T_{1/2}$. We evaluate the function at these points:\n$$ f(0) = 0^{2}\\left(0 - \\frac{1}{2}\\right)^{2}(0 - 1)^{2} = 0 $$\n$$ f(1) = 1^{2}\\left(1 - \\frac{1}{2}\\right)^{2}(1 - 1)^{2} = 0 $$\n$$ f\\left(\\frac{1}{2}\\right) = \\left(\\frac{1}{2}\\right)^{2}\\left(\\frac{1}{2} - \\frac{1}{2}\\right)^{2}\\left(\\frac{1}{2} - 1\\right)^{2} = 0 $$\n\nNow, we compute the trapezoidal approximations $T_1$ and $T_{1/2}$.\nFor $h=1$, we have $n=1$. The approximation is:\n$$ T_1 = 1 \\cdot \\frac{f(0) + f(1)}{2} = \\frac{0 + 0}{2} = 0 $$\nFor $h=1/2$, we have $n=2$. The approximation is:\n$$ T_{1/2} = \\frac{1}{2} \\left( \\frac{f(0) + f(1)}{2} + f\\left(0 + 1 \\cdot \\frac{1}{2}\\right) \\right) = \\frac{1}{2} \\left( \\frac{0 + 0}{2} + f\\left(\\frac{1}{2}\\right) \\right) = \\frac{1}{2} (0 + 0) = 0 $$\n\nThe adaptive routine computes the error estimate $\\widehat{E}_h$ based on the assumption that the method is second-order. For $h=1$, this estimate is:\n$$ \\widehat{E}_1 = \\frac{|T_{1/2} - T_1|}{3} = \\frac{|0 - 0|}{3} = 0 $$\nThe routine's stopping criterion is $\\widehat{E}_1 \\le \\text{tol}$, where the tolerance is given as $\\text{tol} = 10^{-4}$.\nWe check this condition:\n$$ 0 \\le 10^{-4} $$\nThe condition is satisfied. Therefore, the routine accepts the approximation $T_{1/2}$ and terminates. This addresses the first part of the problem.\n\nThe second part requires computing the true absolute error committed by this action. The returned result is $T_{1/2} = 0$. The true absolute error is $|I - T_{1/2}|$, where $I$ is the true value of the integral. We must compute $I = \\int_0^1 f(x) dx$.\nTo do this, we first expand the polynomial $f(x)$:\n$$ f(x) = \\left[x\\left(x - \\frac{1}{2}\\right)(x-1)\\right]^2 = \\left[x\\left(x^2 - \\frac{3}{2}x + \\frac{1}{2}\\right)\\right]^2 = \\left(x^3 - \\frac{3}{2}x^2 + \\frac{1}{2}x\\right)^2 $$\n$$ f(x) = (x^3)^2 + \\left(-\\frac{3}{2}x^2\\right)^2 + \\left(\\frac{1}{2}x\\right)^2 + 2(x^3)\\left(-\\frac{3}{2}x^2\\right) + 2(x^3)\\left(\\frac{1}{2}x\\right) + 2\\left(-\\frac{3}{2}x^2\\right)\\left(\\frac{1}{2}x\\right) $$\n$$ f(x) = x^6 + \\frac{9}{4}x^4 + \\frac{1}{4}x^2 - 3x^5 + x^4 - \\frac{3}{2}x^3 $$\n$$ f(x) = x^6 - 3x^5 + \\frac{13}{4}x^4 - \\frac{3}{2}x^3 + \\frac{1}{4}x^2 $$\nNow we integrate $f(x)$ from $0$ to $1$:\n$$ I = \\int_0^1 \\left( x^6 - 3x^5 + \\frac{13}{4}x^4 - \\frac{3}{2}x^3 + \\frac{1}{4}x^2 \\right) dx $$\n$$ I = \\left[ \\frac{x^7}{7} - \\frac{3x^6}{6} + \\frac{13x^5}{20} - \\frac{3x^4}{8} + \\frac{x^3}{12} \\right]_0^1 $$\n$$ I = \\frac{1}{7} - \\frac{3}{6} + \\frac{13}{20} - \\frac{3}{8} + \\frac{1}{12} = \\frac{1}{7} - \\frac{1}{2} + \\frac{13}{20} - \\frac{3}{8} + \\frac{1}{12} $$\nThe least common multiple of the denominators $7, 2, 20, 8, 12$ is $840$. We convert all terms to this common denominator:\n$$ I = \\frac{120}{840} - \\frac{420}{840} + \\frac{546}{840} - \\frac{315}{840} + \\frac{70}{840} $$\n$$ I = \\frac{120 - 420 + 546 - 315 + 70}{840} = \\frac{736 - 735}{840} = \\frac{1}{840} $$\nThe true value of the integral is $I = \\frac{1}{840}$. The routine returned the value $T_{1/2} = 0$.\nThe true absolute error is therefore:\n$$ |I - T_{1/2}| = \\left|\\frac{1}{840} - 0\\right| = \\frac{1}{840} $$\nThis case illustrates a failure of the adaptive algorithm. The error estimator, which assumes a second-order convergence, is deceived because all function values at the sampling points are zero. This leads to an estimated error of zero, causing premature termination with a result that has a significant true error relative to the tolerance.",
            "answer": "$$\\boxed{\\frac{1}{840}}$$"
        },
        {
            "introduction": "Having explored how function smoothness affects convergence and how error estimators can fail, we now turn to a practical showdown between two common strategies: fixed-step versus adaptive integration. You will implement and compare a high-order method on a uniform grid (Simpson's rule) against a lower-order method with an adaptive grid (trapezoidal rule) for integrating a function with a sharp, localized feature . This computational experiment powerfully demonstrates why adaptive step-size control is a cornerstone of modern numerical integration, enabling efficient and accurate results even for challenging integrands.",
            "id": "2430732",
            "problem": "You are given a family of integrals over the closed interval $\\left[0,1\\right]$ of the form\n$$\nI(c,w) \\;=\\; \\int_{0}^{1} f_{c,w}(x)\\,dx,\\quad \\text{where}\\quad f_{c,w}(x) \\;=\\; \\sin(\\omega x) \\;+\\; A \\exp\\!\\left(-\\frac{(x-c)^2}{2w^2}\\right).\n$$\nAll angles in the sine function must be interpreted in radians. For all computations in this problem, take $\\omega=20$ and $A=5$.\n\nFor each test case, you must compute two numerical approximations to $I(c,w)$ using at most $N$ evaluations of $f_{c,w}$ and then report their absolute errors with respect to a reference value that is accurate to within absolute error less than $10^{-12}$. The two required approximations are:\n\n1. A fixed-step composite Simpson’s rule on a uniform grid. This rule uses $M=\\left\\lfloor\\frac{N-1}{2}\\right\\rfloor$ composite subintervals and thus exactly $2M+1\\le N$ function evaluations at the nodes $x_i = a + i h$ for $i=0,1,\\dots,2M$, with $a=0$, $b=1$, and $h=\\frac{b-a}{2M}$. The approximation is\n$$\nS_N \\;=\\; \\frac{h}{3}\\left[f(x_0) + f(x_{2M}) + 4\\sum_{j=1}^{M} f(x_{2j-1}) + 2\\sum_{j=1}^{M-1} f(x_{2j})\\right].\n$$\n\n2. An adaptive trapezoidal rule under a strict function-evaluation budget. Define the adaptive estimator $T_N$ as follows. Start with the single interval $\\left[a,b\\right]=\\left[0,1\\right]$. For any interval $\\left[x_L,x_R\\right]$ for which $f(x_L)$, $f(x_R)$, and $f(x_M)$ are known at the midpoint $x_M=\\frac{x_L+x_R}{2}$, define the coarse trapezoidal approximation\n$$\nT_{\\text{coarse}} \\;=\\; \\frac{h}{2}\\left(f(x_L)+f(x_R)\\right), \\quad h = x_R - x_L,\n$$\nand the refined trapezoidal approximation using the bisection $\\left[x_L,x_M\\right]\\cup\\left[x_M,x_R\\right]$,\n$$\nT_{\\text{refined}} \\;=\\; \\frac{h}{4}\\left(f(x_L) + 2 f(x_M) + f(x_R)\\right).\n$$\nUse the local error estimate\n$$\nE \\;=\\; \\frac{\\left|T_{\\text{refined}} - T_{\\text{coarse}}\\right|}{3}.\n$$\nInitialize by evaluating $f$ at $x=0$, $x=1$, and $x=\\frac{1}{2}$ (that is, $x_L=0$, $x_R=1$, and $x_M=\\frac{1}{2}$), which uses $3$ evaluations. Maintain the current set of disjoint leaf intervals, each represented by its triplet $\\left(x_L,x_M,x_R\\right)$ with known $f(x_L)$, $f(x_M)$, and $f(x_R)$ and an associated $T_{\\text{refined}}$ and $E$. At each step, if adding two new function evaluations would not exceed the budget $N$, choose the leaf interval with the largest $E$, bisect it into its two halves $\\left[x_L,x_M\\right]$ and $\\left[x_M,x_R\\right]$, evaluate $f$ at each new half-midpoint $\\frac{x_L+x_M}{2}$ and $\\frac{x_M+x_R}{2}$ (this increases the count of distinct evaluations by $2$), form their $T_{\\text{refined}}$ and $E$, and replace the parent interval by these two children in the leaf set. Stop when the next bisection would require more than $N$ total distinct evaluations of $f$. Define the final adaptive estimate $T_N$ as the sum of $T_{\\text{refined}}$ over all current leaf intervals. In counting the budget, only distinct evaluation points $x$ are counted; shared endpoints are not reevaluated.\n\nFor each test case below, compute a high-accuracy reference value $I^\\star(c,w)$ for $I(c,w)$ that is accurate to within absolute error less than $10^{-12}$, and then compute the absolute errors\n$$\nE_S \\;=\\; \\left|S_N - I^\\star(c,w)\\right|, \\qquad E_T \\;=\\; \\left|T_N - I^\\star(c,w)\\right|.\n$$\n\nTest suite (four cases):\n- Case A: $N=33$, $c=0.5$, $w=0.02$.\n- Case B: $N=33$, $c=0.9$, $w=0.01$.\n- Case C: $N=65$, $c=0.3$, $w=0.005$.\n- Case D: $N=129$, $c=0.5$, $w=0.05$.\n\nYour program must output a single line containing the results as a comma-separated list of lists in the order $\\left[\\text{Case A},\\text{Case B},\\text{Case C},\\text{Case D}\\right]$, where each inner list is of the form $\\left[E_S,E_T\\right]$. Each floating-point number must be formatted in scientific notation with exactly $10$ digits after the decimal point (for example, $1.2345678900e-03$). That is, the output must have the form\n$$\n\\left[ [E_{S,A},E_{T,A}], [E_{S,B},E_{T,B}], [E_{S,C},E_{T,C}], [E_{S,D},E_{T,D}] \\right],\n$$\nprinted as a single line with no spaces after commas.",
            "solution": "We formalize the two quadrature methods and the evaluation-budget-constrained adaptivity, then derive how to implement them from first principles of numerical integration and error analysis.\n\nThe integrand is\n$$\nf_{c,w}(x) \\;=\\; \\sin(\\omega x) \\;+\\; A \\exp\\!\\left(-\\frac{(x-c)^2}{2w^2}\\right),\n$$\nwith $\\omega=20$ and $A=5$, on $x\\in[0,1]$. The sine function uses radians. The integral is\n$$\nI(c,w) \\;=\\; \\int_{0}^{1} f_{c,w}(x)\\,dx.\n$$\n\nFixed-step composite Simpson’s rule. For a uniform grid on $[a,b]=[0,1]$ with $2M$ subintervals (so $2M+1$ nodes), the composite Simpson rule is, by definition of Newton–Cotes formulas,\n$$\nS_N \\;=\\; \\frac{h}{3}\\left[f(x_0) + f(x_{2M}) + 4\\sum_{j=1}^{M} f(x_{2j-1}) + 2\\sum_{j=1}^{M-1} f(x_{2j})\\right],\n$$\nwhere $h=\\frac{b-a}{2M}$ and $x_i=a+ih$, $i=0,1,\\dots,2M$. The budget constraint is that we can use at most $N$ distinct function evaluations. The composite Simpson rule requires exactly $2M+1$ evaluations. Therefore we must choose\n$$\nM \\;=\\; \\left\\lfloor \\frac{N-1}{2} \\right\\rfloor,\n$$\nwhich guarantees $2M+1\\le N$ evaluations. The classical truncation error for Simpson’s rule for sufficiently smooth $f$ is $\\mathcal{O}(h^4)$, but here we do not assume a priori constants; we compute the approximation and measure the actual absolute error with respect to a high-accuracy reference.\n\nAdaptive trapezoidal rule under a strict budget. The trapezoidal rule on an interval $\\left[x_L,x_R\\right]$ has the approximation\n$$\nT_{\\text{coarse}} \\;=\\; \\frac{h}{2}\\left(f(x_L)+f(x_R)\\right), \\quad h=x_R-x_L.\n$$\nIf we bisect at the midpoint $x_M=\\frac{x_L+x_R}{2}$ and apply trapezoidal rule on the two halves, we obtain\n$$\nT_{\\text{refined}} \\;=\\; \\frac{h}{4}\\left(f(x_L)+2f(x_M)+f(x_R)\\right).\n$$\nFor sufficiently smooth $f$, the trapezoidal rule error on a single interval scales as $\\mathcal{O}(h^2)$. Under the assumption of an asymptotic error expansion $E(h)\\approx K h^2$ for some local constant $K$, Richardson extrapolation implies\n$$\nE(h) \\;\\approx\\; \\frac{T_{\\text{refined}} - T_{\\text{coarse}}}{3},\n$$\nso a local error estimate is\n$$\nE \\;=\\; \\frac{\\left|T_{\\text{refined}}-T_{\\text{coarse}}\\right|}{3}.\n$$\nAn adaptive refinement strategy aims to allocate smaller step sizes where this local error estimate is largest. We begin with the single interval $\\left[0,1\\right]$ and evaluate $f$ at $x=0$, $x=\\frac{1}{2}$, and $x=1$, consuming $3$ evaluations. For the current set of disjoint leaf intervals, each leaf is represented by its endpoints and midpoint $\\left(x_L,x_M,x_R\\right)$ with known $f(x_L)$, $f(x_M)$, $f(x_R)$, and associated $T_{\\text{refined}}$ and $E$. At each refinement step, if adding two new distinct function evaluations would keep the total at most $N$, we select the leaf interval with the largest $E$ and bisect it into its two halves. This creates two child intervals:\n- Left child $\\left[x_L,x_M\\right]$ with new midpoint $x_{LM}=\\frac{x_L+x_M}{2}$,\n- Right child $\\left[x_M,x_R\\right]$ with new midpoint $x_{MR}=\\frac{x_M+x_R}{2}$.\nWe evaluate $f$ at these two new midpoints (adding $2$ to the count of distinct samples), compute each child’s $T_{\\text{refined}}$ and $E$, and update the set of leaves. The current adaptive integral estimate is the sum of $T_{\\text{refined}}$ across all leaves. This greedy refinement continues until the next split would require more than $N$ total distinct evaluations. Because we begin with $2$ endpoints and $1$ midpoint (total $3$) and each bisection adds $2$ new midpoints, the number of distinct evaluations is $3+2k$ after $k$ bisections, which is always $\\le N$ by construction. This procedure concentrates subintervals where the local error is largest, which, for this problem, corresponds to regions with large local curvature such as the localized Gaussian feature near $x=c$ or regions of rapid oscillation.\n\nReference integral and absolute errors. We compute a high-accuracy reference value $I^\\star(c,w)$ for $I(c,w)$ to within absolute error less than $10^{-12}$ using a robust high-order quadrature routine so that the reported absolute errors\n$$\nE_S \\;=\\; \\left|S_N - I^\\star(c,w)\\right|,\\qquad E_T \\;=\\; \\left|T_N - I^\\star(c,w)\\right|\n$$\nare dominated by the discretization effects of the two methods being compared and not by the reference value.\n\nTest suite and expected outcomes. The four cases exercise different regimes:\n- Case A: $N=33$, $c=0.5$, $w=0.02$ (localized feature centered, moderate narrowness).\n- Case B: $N=33$, $c=0.9$, $w=0.01$ (localized feature near the boundary).\n- Case C: $N=65$, $c=0.3$, $w=0.005$ (very narrow feature, moderate budget).\n- Case D: $N=129$, $c=0.5$, $w=0.05$ (broader feature, larger budget).\n\nWe anticipate, from first principles, that fixed-step composite Simpson’s rule, being fourth order in the uniform step size $h$, will perform very well when the feature is not too narrow relative to the grid spacing and the function remains sufficiently smooth across cells. However, when the feature is sharply localized (small $w$) and especially when the budget $N$ is modest, the uniform mesh may underresolve the feature, leading to larger errors. The adaptive trapezoidal rule is only second order locally, but because the error estimator drives local refinement to the most challenging regions, it can outperform the fixed-step Simpson’s rule under the same evaluation budget by focusing evaluations within and around the localized Gaussian peak (and possibly near boundaries where asymmetry occurs). The results reported by the program will quantify these effects via the computed $E_S$ and $E_T$ for each test case. The final output format is a single line: a comma-separated list of four inner lists, each inner list being $\\left[E_S,E_T\\right]$ with each number formatted in scientific notation with exactly $10$ digits after the decimal point.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom math import sin, exp\nfrom scipy.integrate import quad\n\ndef f_cw(x, c, w, omega=20.0, A=5.0):\n    # Sine uses radians; exponential is the localized feature.\n    return sin(omega * x) + A * exp(-((x - c) ** 2) / (2.0 * w ** 2))\n\ndef reference_integral(c, w):\n    # High-accuracy reference integral with stringent tolerances.\n    val, err = quad(lambda x: f_cw(x, c, w), 0.0, 1.0, epsabs=1e-13, epsrel=1e-13, limit=500)\n    return val\n\ndef composite_simpson_budget(c, w, N, a=0.0, b=1.0):\n    # Use M = floor((N-1)/2) composite Simpson subintervals (2M+1 nodes = N).\n    M = int((N - 1) // 2)\n    if M  1:\n        # Fallback: with too few points, use simple trapezoid on [a,b].\n        fa = f_cw(a, c, w)\n        fb = f_cw(b, c, w)\n        return 0.5 * (b - a) * (fa + fb)\n    h = (b - a) / (2 * M)\n    xs = a + h * np.arange(0, 2 * M + 1, dtype=float)\n    fs = np.array([f_cw(x, c, w) for x in xs])\n    s0 = fs[0] + fs[-1]\n    sodds = fs[1:-1:2].sum()  # odd indices: 1,3,...,2M-1\n    sevens = fs[2:-1:2].sum() if (2 * M - 1) = 2 else 0.0  # even indices: 2,4,...,2M-2\n    return (h / 3.0) * (s0 + 4.0 * sodds + 2.0 * sevens)\n\nclass Interval:\n    __slots__ = (\"xL\",\"xM\",\"xR\",\"fL\",\"fM\",\"fR\",\"Tref\",\"E\",\"id\")\n    def __init__(self, xL, xM, xR, fL, fM, fR, Tref, E, idnum):\n        self.xL = xL; self.xM = xM; self.xR = xR\n        self.fL = fL; self.fM = fM; self.fR = fR\n        self.Tref = Tref; self.E = E; self.id = idnum\n\ndef adaptive_trapezoid_budget(c, w, N, a=0.0, b=1.0):\n    # Greedy adaptive trapezoidal rule under a strict evaluation budget of at most N distinct samples.\n    # Implementation matches the precise specification in the problem statement.\n    import heapq\n\n    # Map of distinct points to function values to avoid double counting.\n    values = {}\n\n    def eval_point(x):\n        # Evaluate f(x) if not already. Count distinct points only.\n        if x in values:\n            return values[x]\n        if len(values) = N:\n            # Budget exhausted; should not happen if caller checks before adding new points.\n            return values.get(x, None)\n        fx = f_cw(x, c, w)\n        values[x] = fx\n        return fx\n\n    # Initialize with endpoints and midpoint\n    xL = a; xR = b; xM = 0.5 * (xL + xR)\n    fL = eval_point(xL); fR = eval_point(xR); fM = eval_point(xM)\n\n    # Compute initial refined trapezoid and error estimate\n    h = xR - xL\n    Tcoarse = 0.5 * h * (fL + fR)\n    Tref = 0.25 * h * (fL + 2.0 * fM + fR)\n    E = abs(Tref - Tcoarse) / 3.0\n\n    # Priority queue of intervals by negative error (max-heap behavior)\n    heap = []\n    counter = 0  # to break ties\n    first = Interval(xL, xM, xR, fL, fM, fR, Tref, E, counter)\n    heapq.heappush(heap, (-E, counter, first))\n    counter += 1\n\n    # Sum of refined trapezoid contributions across leaf intervals\n    integral_sum = Tref\n\n    # Current number of distinct evaluations is len(values)\n    # Each split requires adding exactly 2 new midpoints\n    while len(values) + 2 = N and heap:\n        # Pop the interval with largest error\n        _, _, itv = heapq.heappop(heap)\n\n        # Prepare two children by bisecting\n        xL = itv.xL; xM = itv.xM; xR = itv.xR\n        fL = itv.fL; fM = itv.fM; fR = itv.fR\n\n        # Left child [xL, xM]\n        xLM = 0.5 * (xL + xM)\n        fLM = eval_point(xLM)\n        hL = xM - xL\n        Tcoarse_L = 0.5 * hL * (fL + fM)\n        Tref_L = 0.25 * hL * (fL + 2.0 * fLM + fM)\n        E_L = abs(Tref_L - Tcoarse_L) / 3.0\n\n        # Right child [xM, xR]\n        xMR = 0.5 * (xM + xR)\n        fMR = eval_point(xMR)\n        hR = xR - xM\n        Tcoarse_R = 0.5 * hR * (fM + fR)\n        Tref_R = 0.25 * hR * (fM + 2.0 * fMR + fR)\n        E_R = abs(Tref_R - Tcoarse_R) / 3.0\n\n        # Update integral sum: remove parent refined, add children refined\n        integral_sum += -itv.Tref + (Tref_L + Tref_R)\n\n        # Push children\n        left = Interval(xL, xLM, xM, fL, fLM, fM, Tref_L, E_L, counter); counter += 1\n        right = Interval(xM, xMR, xR, fM, fMR, fR, Tref_R, E_R, counter); counter += 1\n        heapq.heappush(heap, (-E_L, left.id, left))\n        heapq.heappush(heap, (-E_R, right.id, right))\n\n    return integral_sum\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (N, c, w)\n    test_cases = [\n        (33, 0.5, 0.02),   # Case A\n        (33, 0.9, 0.01),   # Case B\n        (65, 0.3, 0.005),  # Case C\n        (129, 0.5, 0.05),  # Case D\n    ]\n\n    results = []\n    for N, c, w in test_cases:\n        I_star = reference_integral(c, w)\n        S_N = composite_simpson_budget(c, w, N, a=0.0, b=1.0)\n        T_N = adaptive_trapezoid_budget(c, w, N, a=0.0, b=1.0)\n        err_S = abs(S_N - I_star)\n        err_T = abs(T_N - I_star)\n        results.append((err_S, err_T))\n\n    # Format: a single line with a comma-separated list of lists, each inner list [Es,Ea],\n    # with each float in scientific notation with exactly 10 digits after the decimal point.\n    formatted = \"[\" + \",\".join(f\"[{err_s:.10e},{err_t:.10e}]\" for (err_s, err_t) in results) + \"]\"\n    print(formatted)\n\nsolve()\n```"
        }
    ]
}