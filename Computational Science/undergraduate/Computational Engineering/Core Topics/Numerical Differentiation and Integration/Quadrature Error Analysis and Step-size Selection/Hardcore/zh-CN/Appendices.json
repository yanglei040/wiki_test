{
    "hands_on_practices": [
        {
            "introduction": "数值积分法则的理论收敛阶数是建立在被积函数具有一定光滑性的假设之上的。本实践旨在探究当这些光滑性假设不被满足时会发生什么。通过分析一个具有有限光滑性（具体为 $C^1$ 但非 $C^2$）的函数 $\\lvert x-c \\rvert^{3/2}$ 的积分，我们可以通过实验测量收敛阶数，并观察它如何偏离光滑函数下的理论预期。这个练习对于深刻理解数值方法为何以及何时会表现不佳至关重要。",
            "id": "2430715",
            "problem": "考虑一个在闭区间上具有内部尖点的函数的积分。令区间为 $[a,b]=[0,1]$，对于给定的参数 $c \\in (0,1)$，定义函数 $f:[0,1]\\to\\mathbb{R}$ 为\n$$\nf(x) = \\lvert x-c \\rvert^{\\alpha}, \\quad \\text{with } \\alpha=\\tfrac{3}{2}.\n$$\n函数 $f$ 在 $[0,1]$ 上属于 $C^1$ 类，但不属于 $C^2$ 类。该积分的精确值为\n$$\nI(c) = \\int_{0}^{1} \\lvert x-c \\rvert^{\\alpha}\\,dx = \\frac{(c-0)^{\\alpha+1} + (1-c)^{\\alpha+1}}{\\alpha+1}.\n$$\n定义在具有 $N$ 个子区间（其中 $N$ 是偶数）的均匀网格上的复合 Simpson 求积法则如下。令 $h = \\frac{b-a}{N}$ 且 $x_j = a + jh$，其中 $j=0,1,\\dots,N$。$\\int_a^b f(x)\\,dx$ 的复合 Simpson 近似值 $S_N(f)$ 为\n$$\nS_N(f) = \\frac{h}{3}\\left[f(x_0) + 4\\sum_{j=1,\\,j\\ \\text{odd}}^{N-1} f(x_j) + 2\\sum_{j=2,\\,j\\ \\text{even}}^{N-2} f(x_j) + f(x_N)\\right].\n$$\n对于每个参数值 $c$，令绝对误差为\n$$\nE_N(c) = \\left\\lvert I(c) - S_N(f) \\right\\rvert.\n$$\n您必须编写一个完整、可运行的程序，对下列每个关于 $c$ 的测试用例，使用提供的 $N$ 值序列，估计当 $h \\to 0$ 时满足 $E_N(c) = \\Theta(h^{p})$ 的观测收敛阶 $p$：\n- 测试用例 1：$c = 0.3$，\n- 测试用例 2：$c = \\tfrac{3}{8}$，\n- 测试用例 3：$c = 10^{-3}$。\n\n对于每个测试用例，计算偶数子区间数序列 $N \\in \\{8,16,32,64,128,256,512\\}$ 所对应的 $E_N(c)$，并从这些值中为该测试用例确定一个单一的、有代表性的观测收敛阶 $p$ 的估计值。您的程序的最终输出必须是单行，包含一个由三个浮点数组成的列表，这些浮点数按上述顺序列出了三个测试用例的估计阶数。将每个浮点数四舍五入到三位小数。要求的最终输出格式为单行，用方括号括起来的逗号分隔列表，例如：\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3].\n$$\n本问题不涉及物理单位。不出现角度。不需要百分比。输出中的所有数值都必须是浮点数。",
            "solution": "问题陈述是适定的、数学上一致的且有科学依据的。它提出了数值分析中的一个标准任务：当求积法则应用于光滑性有限的函数时，通过实验确定其收敛阶。我们将继续进行求解。\n\n复合 Simpson 法则是一种四阶数值积分方法。对于在 $[a,b]$ 上的积分，其绝对误差 $E_N$ 通常表现为 $E_N = \\mathcal{O}(h^4)$，其中 $h = (b-a)/N$ 是步长。仅当被积函数 $f$ 四次连续可微，即 $f \\in C^4([a,b])$ 时，才能保证这个收敛阶。\n\n给定的函数是 $f(x) = \\lvert x-c \\rvert^{\\alpha}$，其中 $\\alpha=\\frac{3}{2}$。其导数为：\n$$ f'(x) = \\frac{3}{2} \\operatorname{sgn}(x-c) \\lvert x-c \\rvert^{1/2} $$\n$$ f''(x) = \\frac{3}{4} \\lvert x-c \\rvert^{-1/2} $$\n一阶导数 $f'(x)$ 在 $[0,1]$ 上是连续的，因此 $f \\in C^1([0,1])$。然而，二阶导数 $f''(x)$ 在 $x=c$ 处有一个奇点，这意味着 $f \\notin C^2([0,1])$。光滑性条件 $f \\in C^4([0,1])$ 的不满足导致了收敛阶的降低。\n\n对于具有 $|x-c|^{\\beta}$ 形式的代数奇点的被积函数，像 Simpson 法则这样的 $m$ 点 Newton-Cotes 法则的收敛阶 $p$ 由非光滑函数的求积理论给出。这个阶数取决于 $c$ 处的奇点是否与求积节点重合：\n1.  如果对于序列中的任何 $N$，$c$ 处的奇点都不是求积节点，则收敛阶为 $p = \\alpha+1$。对于 $\\alpha = \\frac{3}{2}$，这得到 $p = \\frac{3}{2}+1 = 2.5$。\n2.  如果对于所有足够大的 $N$，$c$ 处的奇点都是一个求积节点，则误差会得到改善，收敛阶变为 $p = \\alpha+2$。对于 $\\alpha = \\frac{3}{2}$，这得到 $p = \\frac{3}{2}+2 = 3.5$。\n\n我们基于这个理论基础来分析测试用例：\n- **测试用例 1 ($c = 0.3$)**：值 $c=0.3 = \\frac{3}{10}$ 不是一个二进有理数。网格点是 $x_j = j/N$，其中 $N$ 是 8 乘以 2 的幂。因此，$c$ 永远不会是网格点。我们预期收敛阶为 $p \\approx 2.5$。\n- **测试用例 2 ($c = \\frac{3}{8}$)**：值 $c=0.375=\\frac{3}{8}$ 是一个二进有理数。对于 $N=8, 16, 32, \\dots$，点 $c$ 始终是网格点（例如，当 $N=8$ 时，$x_3 = \\frac{3}{8}$）。我们预期收敛阶为 $p \\approx 3.5$。\n- **测试用例 3 ($c = 10^{-3}$)**：值 $c=0.001=\\frac{1}{1000}$ 不是一个二进有理数。它不会是网格点。我们预期收敛阶为 $p \\approx 2.5$。\n\n为了数值上估计收敛阶 $p$，我们假设误差遵循关系 $E_N \\approx K h^p$（对于某个常数 $K$），这可以重写为 $E_N \\approx K' N^{-p}$。对该表达式取自然对数，得到：\n$$ \\ln(E_N) \\approx \\ln(K') - p \\ln(N) $$\n这个方程显示了 $\\ln(E_N)$ 和 $\\ln(N)$ 之间的线性关系，斜率为 $-p$。为了找到 $p$ 的一个稳健估计，我们将计算给定序列 $N \\in \\{8, 16, 32, 64, 128, 256, 512\\}$ 的误差 $E_N(c)$。然后我们对点集 $\\{(\\ln(N_i), \\ln(E_{N_i}))\\}_{i=1}^7$ 进行线性最小二乘回归。最佳拟合线的斜率 $m$ 计算如下：\n$$ m = \\frac{\\sum_{i=1}^7 (\\ln(N_i) - \\overline{\\ln(N)}) (\\ln(E_{N_i}) - \\overline{\\ln(E)})}{\\sum_{i=1}^7 (\\ln(N_i) - \\overline{\\ln(N)})^2} $$\n那么估计的收敛阶为 $p = -m$。\n\n对每个测试用例实施以下步骤：\n1.  定义常数 $\\alpha = \\frac{3}{2}$ 和测试用例参数 $c$。\n2.  计算精确积分 $I(c) = \\frac{c^{\\alpha+1} + (1-c)^{\\alpha+1}}{\\alpha+1}$。\n3.  对于序列中的每个 $N$，计算复合 Simpson 法则的近似值 $S_N(f)$。\n4.  计算绝对误差 $E_N(c) = |I(c) - S_N(f)|$。\n5.  使用计算出的误差集和相应的 $N$ 值，通过线性回归确定对数-对数图的斜率 $m$。\n6.  收敛阶为 $p = -m$。最终值四舍五入到三位小数。\n整个过程被封装在提供的程序中。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef estimate_convergence_order(c, N_values, alpha):\n    \"\"\"\n    Estimates the convergence order for Simpson's rule for the function\n    f(x) = |x-c|^alpha.\n\n    Args:\n        c (float): The location of the cusp in the function.\n        N_values (list or np.ndarray): A sequence of even integers representing\n                                      the number of subintervals.\n        alpha (float): The exponent in the function definition.\n\n    Returns:\n        float: The estimated order of convergence p.\n    \"\"\"\n    # 1. Define the function and its exact integral\n    f = lambda x: np.abs(x - c)**alpha\n    alpha_p1 = alpha + 1\n    I_exact = (c**alpha_p1 + (1 - c)**alpha_p1) / alpha_p1\n\n    errors = []\n    # 2. Loop through N values to calculate quadrature error\n    for N in N_values:\n        a, b = 0.0, 1.0\n        h = (b - a) / N\n        x = np.linspace(a, b, N + 1)\n        y = f(x)\n\n        # Composite Simpson's rule formula\n        # S_N = (h/3) * [f(x_0) + 4*sum(f(x_odd)) + 2*sum(f(x_even)) + f(x_N)]\n        S_N = (h / 3) * (y[0] + 4 * np.sum(y[1:-1:2]) + 2 * np.sum(y[2:-1:2]) + y[-1])\n        \n        # Absolute error\n        error = np.abs(I_exact - S_N)\n        errors.append(error)\n\n    # 3. Estimate convergence order using linear regression on log-log data\n    # The error model is E_N ≈ K * N^(-p).\n    # Taking logs: log(E_N) ≈ log(K) - p * log(N).\n    # This is a linear relationship between log(E_N) and log(N).\n    # The slope of this line is -p.\n    \n    log_N = np.log(np.array(N_values, dtype=float))\n    log_E = np.log(np.array(errors, dtype=float))\n\n    # Calculate the slope 'm' of the best-fit line for y = mx + b,\n    # where y = log_E and x = log_N.\n    # m = Cov(x, y) / Var(x)\n    x = log_N\n    y = log_E\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n    \n    slope = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean)**2)\n    \n    # The convergence order p is the negative of the slope.\n    p = -slope\n    \n    return p\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Define the parameters from the problem statement.\n    test_cases = [\n        0.3,\n        3.0 / 8.0,\n        1e-3\n    ]\n    \n    # Sequence of subintervals for convergence analysis\n    N_values = [8, 16, 32, 64, 128, 256, 512]\n    \n    # Exponent alpha\n    alpha = 1.5\n\n    results = []\n    for c in test_cases:\n        # Calculate the order p for the current test case\n        p_estimated = estimate_convergence_order(c, N_values, alpha)\n        \n        # Round the result to three decimal places\n        results.append(p_estimated)\n\n    # Format the final output as a string with 3 decimal places for each number\n    formatted_results = [f\"{res:.3f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在理解了收敛性的基本原理后，本实践将探讨一个核心主题：固定步长与自适应方法之间的权衡。我们将比较在统一网格上使用高阶方法（辛普森法则）与在非均匀、自适应加密的网格上使用低阶方法（梯形法则）的性能。通过在一个固定的函数求值次数预算下，处理一个带有尖锐局部特征的函数，我们可以直观地观察到将计算资源集中在最需要的地方所带来的强大优势。",
            "id": "2430732",
            "problem": "给你一个在闭区间 $\\left[0,1\\right]$ 上的积分族，其形式为\n$$\nI(c,w) \\;=\\; \\int_{0}^{1} f_{c,w}(x)\\,dx,\\quad \\text{其中}\\quad f_{c,w}(x) \\;=\\; \\sin(\\omega x) \\;+\\; A \\exp\\!\\left(-\\frac{(x-c)^2}{2w^2}\\right).\n$$\n正弦函数中的所有角度都必须以弧度为单位。在本问题的所有计算中，取 $\\omega=20$ 和 $A=5$。\n\n对于每个测试用例，你必须使用至多 $N$ 次对 $f_{c,w}$ 的求值来计算 $I(c,w)$ 的两种数值近似，然后报告它们与一个参考值之间的绝对误差，该参考值的绝对误差小于 $10^{-12}$。两种必需的近似方法是：\n\n1. 在均匀网格上使用固定步长的复合辛普森法则。该法则使用 $M=\\left\\lfloor\\frac{N-1}{2}\\right\\rfloor$ 个复合子区间，因此在节点 $x_i = a + i h$（其中 $i=0,1,\\dots,2M$，$a=0$, $b=1$, $h=\\frac{b-a}{2M}$）处恰好需要 $2M+1\\le N$ 次函数求值。近似值为\n$$\nS_N \\;=\\; \\frac{h}{3}\\left[f(x_0) + f(x_{2M}) + 4\\sum_{j=1}^{M} f(x_{2j-1}) + 2\\sum_{j=1}^{M-1} f(x_{2j})\\right].\n$$\n\n2. 在严格的函数求值预算下的自适应梯形法则。将自适应估计量 $T_N$ 定义如下。从单个区间 $\\left[a,b\\right]=\\left[0,1\\right]$ 开始。对于任何区间 $\\left[x_L,x_R\\right]$，若其中点 $x_M=\\frac{x_L+x_R}{2}$ 处的 $f(x_L)$、$f(x_R)$ 和 $f(x_M)$ 已知，则定义粗略梯形近似\n$$\nT_{\\text{coarse}} \\;=\\; \\frac{h}{2}\\left(f(x_L)+f(x_R)\\right), \\quad h = x_R - x_L,\n$$\n以及使用二分法 $\\left[x_L,x_M\\right]\\cup\\left[x_M,x_R\\right]$ 的精细梯形近似\n$$\nT_{\\text{refined}} \\;=\\; \\frac{h}{4}\\left(f(x_L) + 2 f(x_M) + f(x_R)\\right).\n$$\n使用局部误差估计\n$$\nE \\;=\\; \\frac{\\left|T_{\\text{refined}} - T_{\\text{coarse}}\\right|}{3}.\n$$\n通过在 $x=0$、$x=1$ 和 $x=\\frac{1}{2}$（即 $x_L=0$, $x_R=1$, $x_M=\\frac{1}{2}$）处求值 $f$ 来初始化，这使用了 3 次求值。维护一组当前的不相交叶子区间，每个区间由其三元组 $\\left(x_L,x_M,x_R\\right)$ 表示，其 $f(x_L)$、$f(x_M)$ 和 $f(x_R)$ 已知，并关联了 $T_{\\text{refined}}$ 和 $E$。在每一步，如果增加两次新的函数求值不会超过预算 $N$，则选择具有最大 $E$ 的叶子区间，将其二等分为两个半区间 $\\left[x_L,x_M\\right]$ 和 $\\left[x_M,x_R\\right]$，在每个新的半中点 $\\frac{x_L+x_M}{2}$ 和 $\\frac{x_M+x_R}{2}$ 处求值 $f$（这将不同求值的计数增加 2），形成它们的 $T_{\\text{refined}}$ 和 $E$，并在叶子集合中用这两个子区间替换父区间。当下一次二等分将需要超过 $N$ 次对 $f$ 的总不同求值时停止。将最终的自适应估计 $T_N$ 定义为所有当前叶子区间上 $T_{\\text{refined}}$ 的总和。在计算预算时，只计算不同的求值点 $x$；共享的端点不会被重新求值。\n\n对于下面的每个测试用例，计算 $I(c,w)$ 的一个高精度参考值 $I^\\star(c,w)$，其绝对误差小于 $10^{-12}$，然后计算绝对误差\n$$\nE_S \\;=\\; \\left|S_N - I^\\star(c,w)\\right|, \\qquad E_T \\;=\\; \\left|T_N - I^\\star(c,w)\\right|.\n$$\n\n测试套件（四个案例）：\n- 案例 A: $N=33$, $c=0.5$, $w=0.02$.\n- 案例 B: $N=33$, $c=0.9$, $w=0.01$.\n- 案例 C: $N=65$, $c=0.3$, $w=0.005$.\n- 案例 D: $N=129$, $c=0.5$, $w=0.05$.\n\n你的程序必须输出单行，内容为按 $\\left[\\text{案例 A},\\text{案例 B},\\text{案例 C},\\text{案例 D}\\right]$ 顺序排列的、由逗号分隔的列表的列表，其中每个内部列表的形式为 $\\left[E_S,E_T\\right]$。每个浮点数必须格式化为科学记数法，小数点后恰好有 $10$ 位数字（例如，$1.2345678900e-03$）。即，输出必须具有以下形式\n$$\n\\left[ [E_{S,A},E_{T,A}], [E_{S,B},E_{T,B}], [E_{S,C},E_{T,C}], [E_{S,D},E_{T,D}] \\right],\n$$\n打印为单行，逗号后没有空格。",
            "solution": "我们将两种求积方法和受求值预算约束的自适应性进行形式化，然后从数值积分和误差分析的第一性原理推导出如何实现它们。\n\n被积函数是\n$$\nf_{c,w}(x) \\;=\\; \\sin(\\omega x) \\;+\\; A \\exp\\!\\left(-\\frac{(x-c)^2}{2w^2}\\right),\n$$\n其中 $\\omega=20$ 和 $A=5$，在 $x\\in[0,1]$ 上。正弦函数使用弧度。积分为\n$$\nI(c,w) \\;=\\; \\int_{0}^{1} f_{c,w}(x)\\,dx.\n$$\n\n固定步长复合辛普森法则。对于在 $[a,b]=[0,1]$ 上具有 $2M$ 个子区间的均匀网格，根据 Newton–Cotes 公式的定义，复合辛普森法则是\n$$\nS_N \\;=\\; \\frac{h}{3}\\left[f(x_0) + f(x_{2M}) + 4\\sum_{j=1}^{M} f(x_{2j-1}) + 2\\sum_{j=1}^{M-1} f(x_{2j})\\right],\n$$\n其中 $h=\\frac{b-a}{2M}$ 且 $x_i=a+ih$, $i=0,1,\\dots,2M$。预算约束是我们最多可以使用 $N$ 次不同的函数求值。复合辛普森法则恰好需要 $2M+1$ 次求值。因此我们必须选择\n$$\nM \\;=\\; \\left\\lfloor \\frac{N-1}{2} \\right\\rfloor,\n$$\n这保证了 $2M+1\\le N$ 次求值。对于足够光滑的 $f$，辛普森法则的经典截断误差为 $\\mathcal{O}(h^4)$，但在这里我们不预先假设常数；我们计算近似值并测量其相对于高精度参考值的实际绝对误差。\n\n在严格预算下的自适应梯形法则。在区间 $\\left[x_L,x_R\\right]$ 上的梯形法则的近似值为\n$$\nT_{\\text{coarse}} \\;=\\; \\frac{h}{2}\\left(f(x_L)+f(x_R)\\right), \\quad h=x_R-x_L.\n$$\n如果我们在中点 $x_M=\\frac{x_L+x_R}{2}$ 处进行二等分，并在两个半区间上应用梯形法则，我们得到\n$$\nT_{\\text{refined}} \\;=\\; \\frac{h}{4}\\left(f(x_L)+2f(x_M)+f(x_R)\\right).\n$$\n对于足够光滑的 $f$，单个区间上的梯形法则误差的尺度为 $\\mathcal{O}(h^2)$。在存在某个局部常数 $K$ 的渐近误差展开 $E(h)\\approx K h^2$ 的假设下，Richardson 外推法意味着\n$$\nE(h) \\;\\approx\\; \\frac{T_{\\text{refined}} - T_{\\text{coarse}}}{3},\n$$\n所以一个局部误差估计是\n$$\nE \\;=\\; \\frac{\\left|T_{\\text{refined}}-T_{\\text{coarse}}\\right|}{3}.\n$$\n自适应细化策略旨在将较小的步长分配给局部误差估计最大的地方。我们从单个区间 $\\left[0,1\\right]$ 开始，并在 $x=0$, $x=\\frac{1}{2}$ 和 $x=1$ 处求值 $f$，消耗 3 次求值。对于当前的不相交叶子区间集合，每个叶子由其端点和中点 $\\left(x_L,x_M,x_R\\right)$ 表示，其 $f(x_L)$、$f(x_M)$、$f(x_R)$ 值已知，并关联了 $T_{\\text{refined}}$ 和 $E$。在每个细化步骤，如果增加两次新的不同函数求值将使总数保持在至多 $N$，我们就选择具有最大 $E$ 的叶子区间并将其二等分为两个半区间。这会创建两个子区间：\n- 左子区间 $\\left[x_L,x_M\\right]$，新中点为 $x_{LM}=\\frac{x_L+x_M}{2}$，\n- 右子区间 $\\left[x_M,x_R\\right]$，新中点为 $x_{MR}=\\frac{x_M+x_R}{2}$。\n我们在这两个新的中点处求值 $f$（使不同样本的计数增加 2），计算每个子区间的 $T_{\\text{refined}}$ 和 $E$，并更新叶子集合。当前的自适应积分估计是所有叶子上 $T_{\\text{refined}}$ 的总和。这种贪心细化策略持续进行，直到下一次分裂将需要超过 $N$ 次总的不同求值。因为我们从 2 个端点和 1 个中点（总共 3 个）开始，每次二等分增加 2 个新中点，所以在 $k$ 次二等分后，不同求值的次数是 $3+2k$，根据构造，这总是 $\\le N$。这个过程将子区间集中在局部误差最大的地方，对于本问题，这对应于具有大局部曲率的区域，例如 $x=c$ 附近的局部化高斯特征或快速振荡的区域。\n\n参考积分和绝对误差。我们使用一个鲁棒的高阶求积程序计算一个高精度参考值 $I^\\star(c,w)$，其绝对误差小于 $10^{-12}$，这样报告的绝对误差\n$$\nE_S \\;=\\; \\left|S_N - I^\\star(c,w)\\right|,\\qquad E_T \\;=\\; \\left|T_N - I^\\star(c,w)\\right|\n$$\n主要由被比较的两种方法的离散化效应决定，而不是由参考值决定。\n\n测试套件和预期结果。这四个案例测试了不同的情况：\n- 案例 A: $N=33$, $c=0.5$, $w=0.02$（局部化特征居中，中等狭窄）。\n- 案例 B: $N=33$, $c=0.9$, $w=0.01$（局部化特征靠近边界）。\n- 案例 C: $N=65$, $c=0.3$, $w=0.005$（非常狭窄的特征，中等预算）。\n- 案例 D: $N=129$, $c=0.5$, $w=0.05$（较宽的特征，较大预算）。\n\n我们从第一性原理预计，固定步长的复合辛普森法则，其在均匀步长 $h$ 上是四阶的，当特征相对于网格间距不是太窄且函数在各个单元上保持足够光滑时，会表现得非常好。然而，当特征是急剧局部化的（小的 $w$），特别是当预算 $N$ 不大时，均匀网格可能无法充分解析该特征，导致更大的误差。自适应梯形法则局部仅为二阶，但由于误差估计器将局部细化驱动到最具挑战性的区域，它可以通过将求值点集中在局部化高斯峰内部和周围（以及可能在出现不对称性的边界附近），在相同的求值预算下胜过固定步长的辛普森法则。程序报告的结果将通过为每个测试用例计算的 $E_S$ 和 $E_T$ 来量化这些效应。最终输出格式是单行：一个由四个内部列表组成的逗号分隔列表，每个内部列表为 $\\left[E_S,E_T\\right]$，其中每个数字都以科学记数法格式化，小数点后恰好有 10 位数字。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom math import sin, exp\nfrom scipy.integrate import quad\n\ndef f_cw(x, c, w, omega=20.0, A=5.0):\n    # Sine uses radians; exponential is the localized feature.\n    return sin(omega * x) + A * exp(-((x - c) ** 2) / (2.0 * w ** 2))\n\ndef reference_integral(c, w):\n    # High-accuracy reference integral with stringent tolerances.\n    val, err = quad(lambda x: f_cw(x, c, w), 0.0, 1.0, epsabs=1e-13, epsrel=1e-13, limit=500)\n    return val\n\ndef composite_simpson_budget(c, w, N, a=0.0, b=1.0):\n    # Use M = floor((N-1)/2) composite Simpson subintervals (2M+1 nodes = N).\n    M = int((N - 1) // 2)\n    if M < 1:\n        # Fallback: with too few points, use simple trapezoid on [a,b].\n        fa = f_cw(a, c, w)\n        fb = f_cw(b, c, w)\n        return 0.5 * (b - a) * (fa + fb)\n    h = (b - a) / (2 * M)\n    xs = a + h * np.arange(0, 2 * M + 1, dtype=float)\n    fs = np.array([f_cw(x, c, w) for x in xs])\n    s0 = fs[0] + fs[-1]\n    sodds = fs[1:-1:2].sum()  # odd indices: 1,3,...,2M-1\n    sevens = fs[2:-1:2].sum() if (2 * M - 1) >= 2 else 0.0  # even indices: 2,4,...,2M-2\n    return (h / 3.0) * (s0 + 4.0 * sodds + 2.0 * sevens)\n\nclass Interval:\n    __slots__ = (\"xL\",\"xM\",\"xR\",\"fL\",\"fM\",\"fR\",\"Tref\",\"E\",\"id\")\n    def __init__(self, xL, xM, xR, fL, fM, fR, Tref, E, idnum):\n        self.xL = xL; self.xM = xM; self.xR = xR\n        self.fL = fL; self.fM = fM; self.fR = fR\n        self.Tref = Tref; self.E = E; self.id = idnum\n\ndef adaptive_trapezoid_budget(c, w, N, a=0.0, b=1.0):\n    # Greedy adaptive trapezoidal rule under a strict evaluation budget of at most N distinct samples.\n    # Implementation matches the precise specification in the problem statement.\n    import heapq\n\n    # Map of distinct points to function values to avoid double counting.\n    values = {}\n\n    def eval_point(x):\n        # Evaluate f(x) if not already. Count distinct points only.\n        if x in values:\n            return values[x]\n        if len(values) >= N:\n            # Budget exhausted; should not happen if caller checks before adding new points.\n            return values.get(x, None)\n        fx = f_cw(x, c, w)\n        values[x] = fx\n        return fx\n\n    # Initialize with endpoints and midpoint\n    xL = a; xR = b; xM = 0.5 * (xL + xR)\n    fL = eval_point(xL); fR = eval_point(xR); fM = eval_point(xM)\n\n    # Compute initial refined trapezoid and error estimate\n    h = xR - xL\n    Tcoarse = 0.5 * h * (fL + fR)\n    Tref = 0.25 * h * (fL + 2.0 * fM + fR)\n    E = abs(Tref - Tcoarse) / 3.0\n\n    # Priority queue of intervals by negative error (max-heap behavior)\n    heap = []\n    counter = 0  # to break ties\n    first = Interval(xL, xM, xR, fL, fM, fR, Tref, E, counter)\n    heapq.heappush(heap, (-E, counter, first))\n    counter += 1\n\n    # Sum of refined trapezoid contributions across leaf intervals\n    integral_sum = Tref\n\n    # Current number of distinct evaluations is len(values)\n    # Each split requires adding exactly 2 new midpoints\n    while len(values) + 2 <= N and heap:\n        # Pop the interval with largest error\n        _, _, itv = heapq.heappop(heap)\n\n        # Prepare two children by bisecting\n        xL = itv.xL; xM = itv.xM; xR = itv.xR\n        fL = itv.fL; fM = itv.fM; fR = itv.fR\n\n        # Left child [xL, xM]\n        xLM = 0.5 * (xL + xM)\n        fLM = eval_point(xLM)\n        hL = xM - xL\n        Tcoarse_L = 0.5 * hL * (fL + fM)\n        Tref_L = 0.25 * hL * (fL + 2.0 * fLM + fM)\n        E_L = abs(Tref_L - Tcoarse_L) / 3.0\n\n        # Right child [xM, xR]\n        xMR = 0.5 * (xM + xR)\n        fMR = eval_point(xMR)\n        hR = xR - xM\n        Tcoarse_R = 0.5 * hR * (fM + fR)\n        Tref_R = 0.25 * hR * (fM + 2.0 * fMR + fR)\n        E_R = abs(Tref_R - Tcoarse_R) / 3.0\n\n        # Update integral sum: remove parent refined, add children refined\n        integral_sum += -itv.Tref + (Tref_L + Tref_R)\n\n        # Push children\n        left = Interval(xL, xLM, xM, fL, fLM, fM, Tref_L, E_L, counter); counter += 1\n        right = Interval(xM, xMR, xR, fM, fMR, fR, Tref_R, E_R, counter); counter += 1\n        heapq.heappush(heap, (-E_L, left.id, left))\n        heapq.heappush(heap, (-E_R, right.id, right))\n\n    return integral_sum\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (N, c, w)\n    test_cases = [\n        (33, 0.5, 0.02),   # Case A\n        (33, 0.9, 0.01),   # Case B\n        (65, 0.3, 0.005),  # Case C\n        (129, 0.5, 0.05),  # Case D\n    ]\n\n    results = []\n    for N, c, w in test_cases:\n        I_star = reference_integral(c, w)\n        S_N = composite_simpson_budget(c, w, N, a=0.0, b=1.0)\n        T_N = adaptive_trapezoid_budget(c, w, N, a=0.0, b=1.0)\n        err_S = abs(S_N - I_star)\n        err_T = abs(T_N - I_star)\n        results.append((err_S, err_T))\n\n    # Format: a single line with a comma-separated list of lists, each inner list [Es,Ea],\n    # with each float in scientific notation with exactly 10 digits after the decimal point.\n    formatted = \"[\" + \",\".join(f\"[{err_s:.10e},{err_t:.10e}]\" for (err_s, err_t) in results) + \"]\"\n    print(formatted)\n\nsolve()\n```"
        },
        {
            "introduction": "最后的这个实践将拓宽我们在方法选择上的视野。虽然自适应方法在通用积分问题中功能强大，但对于特定类型的函数，其他方法可能更为高效。在这里，我们将比较自适应辛普森法则与非自适应的高斯-勒让德求积法，在求解一个非常光滑（解析）的函数积分时的表现。这个练习将揭示高斯求积法在处理光滑被积函数时惊人的效率，并强调方法的最佳选择严重依赖于被积函数的内在属性。",
            "id": "2430710",
            "problem": "开发一个完整程序，对于一个绝对公差族 $\\{\\varepsilon\\}$，确定族中使得 Gauss–Legendre 高斯求积在 $[0,1]$ 上所需被积函数求值次数严格少于自适应 Simpson 法则的最小公差值。两种方法均用于在绝对误差至多为 $\\varepsilon$ 的条件下，近似计算函数 $f(x)=e^{-x^2}$ 在 $[0,1]$ 上的积分。所求积分是\n$$\nI=\\int_{0}^{1} e^{-x^2}\\,dx.\n$$\n就本问题而言，必须使用以下定义来量化效率和停止条件。\n\n1. 高斯求积的定义和成本模型：\n   - 对于每个 $n\\in\\mathbb{N}$，令 $Q_n$ 为 $[0,1]$ 上的 $n$ 点 Gauss–Legendre 求积，它是通过从 $[-1,1]$ 到 $[0,1]$ 的标准仿射变量变换得到的。其绝对误差为 $E_n=\\lvert I-Q_n\\rvert$。\n   - 对于给定的 $\\varepsilon>0$，将 $n_{\\mathrm{G}}(\\varepsilon)$ 定义为使得 $E_n\\le \\varepsilon$ 的最小 $n$。在公差 $\\varepsilon$ 下，此求积方法所需的函数求值次数为 $N_{\\mathrm{G}}(\\varepsilon)=n_{\\mathrm{G}}(\\varepsilon)$。\n\n2. 自适应 Simpson 法则的定义和成本模型：\n   - 对于一个区间 $[a,b]$，定义 Simpson 近似\n     $$\n     S([a,b])=\\frac{b-a}{6}\\Big(f(a)+4f\\Big(\\frac{a+b}{2}\\Big)+f(b)\\Big).\n     $$\n   - 对于中点为 $m=(a+b)/2$ 的区间 $[a,b]$，类似地定义子区间 Simpson 近似 $S([a,m])$ 和 $S([m,b])$，以及经典的局部误差估计量\n     $$\n     \\eta([a,b])=\\frac{1}{15}\\,\\big\\lvert S([a,m])+S([m,b]) - S([a,b])\\big\\rvert.\n     $$\n   - 自适应 Simpson 策略的定义是：当 $\\eta([a,b])$ 超过其局部公差时，递归地二分区间 $[a,b]$，当划分出的所有子区间都满足其局部公差，从而使全局绝对误差至多为 $\\varepsilon$ 时终止。局部公差的分配必须使得最终划分中所有局部公差之和以 $\\varepsilon$ 为界，例如，每次二分时将公差均分。\n   - 成本 $N_{\\mathrm{S}}(\\varepsilon)$ 是该自适应 Simpson 法则为保证绝对误差至多为 $\\varepsilon$ 而在 $[0,1]$ 上对 $f$ 进行的独立求值总次数。如果一个点被重复访问，其求值不被重复计数。\n\n对于给定有限集合中的每个公差值 $\\varepsilon$，效率比较定义如下：如果 $N_{\\mathrm{G}}(\\varepsilon) < N_{\\mathrm{S}}(\\varepsilon)$，则认为高斯求积更高效。对于每个提供的公差族，将交叉值定义为该族中（相对于 $\\mathbb{R}$ 上的常规全序）使得高斯求积在此意义下更高效的最小 $\\varepsilon$。如果族内不存在这样的 $\\varepsilon$，则交叉值未定义，并且必须报告为特殊浮点值 $\\mathrm{NaN}$。\n\n您的程序必须为以下公差族的测试套件计算这些交叉值：\n- 测试用例 1: $\\{\\varepsilon\\}=\\{10^{-2},\\,10^{-3},\\,10^{-4},\\,10^{-5}\\}$.\n- 测试用例 2: $\\{\\varepsilon\\}=\\{10^{-6},\\,10^{-7},\\,10^{-8},\\,10^{-9},\\,10^{-10},\\,10^{-11},\\,10^{-12}\\}$.\n- 测试用例 3: $\\{\\varepsilon\\}=\\{5\\times10^{-4},\\,2\\times10^{-6},\\,10^{-8},\\,5\\times10^{-10}\\}$.\n\n两种方法的绝对误差条件都必须与一个通过数值计算得到的 $I$ 的高精度参考值进行核对。成本的度量纯粹是 $f(x)$ 的求值次数，每次求值（不论 $x$ 为何值）的成本都计为 1。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按上述测试用例的顺序排列结果，例如 $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$。每个 $\\text{result}_k$ 必须是一个浮点数，等于相应族中使得 $N_{\\mathrm{G}}(\\varepsilon) < N_{\\mathrm{S}}(\\varepsilon)$ 的最小 $\\varepsilon$；如果该族中不存在这样的 $\\varepsilon$，则为 $\\mathrm{NaN}$。",
            "solution": "所述问题已经过验证，并被认定为有效。它在科学上基于数值分析的原理，特别是数值求积。该问题是适定的、客观的，并为获得唯一解提供了一套完整且一致的定义和约束。任务是比较两种标准数值积分方法的计算效率：固定阶 Gauss-Legendre 求积和自适应 Simpson 法则，用于近似计算函数 $f(x) = e^{-x^2}$ 在区间 $[0,1]$ 上的积分。效率被严格定义为被积函数 $f(x)$ 的求值次数。\n\n待近似的积分为\n$$\nI = \\int_{0}^{1} e^{-x^2} \\, dx\n$$\n该积分的精确值可以用误差函数 $\\mathrm{erf}(x)$ 表示为\n$$\nI = \\frac{\\sqrt{\\pi}}{2} \\mathrm{erf}(1)\n$$\n使用标准库函数计算 $I$ 的一个高精度数值，该值作为参考值，用以衡量求积法则的绝对误差。此参考值约为 $I \\approx 0.746824132812427$。\n\n求解方法如下。对于给定测试族中的每个指定公差 $\\varepsilon$，我们计算成本 $N_{\\mathrm{G}}(\\varepsilon)$ 和 $N_{\\mathrm{S}}(\\varepsilon)$。然后，我们在每个族中找出满足条件 $N_{\\mathrm{G}}(\\varepsilon) < N_{\\mathrm{S}}(\\varepsilon)$ 的最小 $\\varepsilon$。\n\n**1. Gauss-Legendre 求积的成本：$N_{\\mathrm{G}}(\\varepsilon)$**\n\n$n$ 点 Gauss-Legendre 求积法则 $Q_n$ 可近似计算该积分。该法则定义在标准区间 $[-1,1]$ 上。为了将其应用于区间 $[0,1]$，我们使用仿射变换 $x(t) = \\frac{1}{2}(t+1)$，其中 $t \\in [-1,1]$。积分为：\n$$\nI = \\int_{0}^{1} f(x) \\, dx = \\int_{-1}^{1} f\\left(\\frac{t+1}{2}\\right) \\frac{1}{2} \\, dt\n$$\n设 $\\{t_i^*, w_i^*\\}_{i=1}^n$ 为 $[-1,1]$ 上 $n$ 点 Gauss-Legendre 法则的节点和权重。则近似值 $Q_n$ 为：\n$$\nQ_n = \\sum_{i=1}^{n} \\frac{w_i^*}{2} f\\left(\\frac{t_i^*+1}{2}\\right)\n$$\n以函数求值次数衡量的成本就是 $n$。根据问题定义，$N_{\\mathrm{G}}(\\varepsilon)$ 是使得绝对误差 $E_n = |I - Q_n|$ 不大于 $\\varepsilon$ 的最小整数 $n$。\n为找到给定 $\\varepsilon$ 的 $N_{\\mathrm{G}}(\\varepsilon)$，我们对 $n=1, 2, 3, \\dots$ 计算 $Q_n$，并在满足误差界的第一个 $n$ 处停止。对于像 $e^{-x^2}$ 这样的解析函数，高斯求积的收敛是超代数的（近指数的，误差 $E_n \\sim e^{-cn}$，其中 $c>0$ 为某个常数），因此随着 $\\varepsilon$ 减小，$N_{\\mathrm{G}}(\\varepsilon)$ 增长非常缓慢，大约为 $N_{\\mathrm{G}}(\\varepsilon) \\propto \\ln(1/\\varepsilon)$。\n\n**2. 自适应 Simpson 法则的成本：$N_{\\mathrm{S}}(\\varepsilon)$**\n\n自适应 Simpson 方法构建了被积函数的分段二次近似。该方法的核心是由局部误差估计控制的积分区间的递归二分。\n对于一个区间 $[a,b]$，Simpson 法则的近似值为 $S([a,b])$。该算法将其与通过对两个半区间求和 Simpson 法则得到的更精确近似 $S_2 = S([a,m]) + S([m,b])$ 进行比较，其中 $m=(a+b)/2$。局部误差估计为：\n$$\n\\eta([a,b]) = \\frac{1}{15} |S_2 - S([a,b])|\n$$\n对于 $[0,1]$ 上的全局公差 $\\varepsilon$，递归过程如下：\n- 从区间 $[0,1]$ 和公差 $\\varepsilon$ 开始。\n- 对于当前区间 $[a,b]$ 和局部公差 $\\tau$，计算误差估计 $\\eta([a,b])$。此步骤需要在 5 个点上求 $f(x)$ 的值：$a, b, m, (a+m)/2, (m+b)/2$。\n- 如果 $\\eta([a,b]) \\le \\tau$，此区间的递归终止。$[a,b]$ 上的积分由 $S_2$ 近似。\n- 如果 $\\eta([a,b]) > \\tau$，则将区间 $[a,b]$ 二分为 $[a,m]$ 和 $[m,b]$，并对每个子区间递归调用此过程，局部公差减半：$\\tau/2$。\n总积分是所有终止子区间结果的总和。\n成本 $N_{\\mathrm{S}}(\\varepsilon)$ 是在整个递归过程中对 $f(x)$ 进行求值的独立点的总数。这通过使用记忆化技术来管理，即存储和重用已计算的函数值，以避免重复计算并正确统计唯一的求值点。对于一个光滑函数，所需的区间数量通常按 $\\varepsilon^{-1/4}$ 的比例缩放，导致成本 $N_{\\mathrm{S}}(\\varepsilon) \\propto \\varepsilon^{-1/4}$。\n\n**3. 交叉值确定**\n\n对于每个给定的公差族 $\\{\\varepsilon_j\\}$，首先将该集合按升序排序。然后，对于这个排序序列中的每个 $\\varepsilon_j$，我们计算 $N_{\\mathrm{G}}(\\varepsilon_j)$ 和 $N_{\\mathrm{S}}(\\varepsilon_j)$。第一个满足 $N_{\\mathrm{G}}(\\varepsilon_j) < N_{\\mathrm{S}}(\\varepsilon_j)$ 的 $\\varepsilon_j$ 值就是该族的所需交叉值。如果族中没有任何公差满足此条件，则交叉值报告为“非数字”（$\\mathrm{NaN}$）。$N_{\\mathrm{G}}$ 的对数增长与 $N_{\\mathrm{S}}$ 的多项式增长的比较表明，随着所需精度的提高（即 $\\varepsilon$ 变小），高斯求积将变得越来越有优势。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import erf, roots_legendre\nimport sys\n\n# It is a good practice to increase recursion limit for deep adaptive quadratures,\n# although for this specific problem and tolerances, it is not strictly necessary.\nsys.setrecursionlimit(2000)\n\nclass AdaptiveSimpson:\n    \"\"\"\n    Implements the adaptive Simpson's rule as specified in the problem.\n    The cost is the number of distinct function evaluations.\n    \"\"\"\n    def __init__(self, f):\n        self.f = f\n        self.memo = {}\n\n    def _eval_f(self, x):\n        \"\"\"Evaluates f at x, using memoization to count distinct points.\"\"\"\n        if x not in self.memo:\n            self.memo[x] = self.f(x)\n        return self.memo[x]\n\n    def _recursive_step(self, a, b, tol):\n        \"\"\"A single recursive step of the adaptive Simpson's algorithm.\"\"\"\n        m = (a + b) / 2\n        \n        # S1 is the Simpson's rule over the whole interval [a,b]\n        fa = self._eval_f(a)\n        fb = self._eval_f(b)\n        fm = self._eval_f(m)\n        S1 = (b-a)/6 * (fa + 4*fm + fb)\n\n        # S2 is the sum of Simpson's rule on the two half-intervals\n        ml = (a + m) / 2\n        mr = (m + b) / 2\n        fml = self._eval_f(ml)\n        fmr = self._eval_f(mr)\n        S2 = (m-a)/6 * (fa + 4*fml + fm) + (b-m)/6 * (fm + 4*fmr + fb)\n\n        # Local error estimate as defined in the problem\n        error_estimate = abs(S2 - S1) / 15.0\n\n        if error_estimate <= tol:\n            return S2  # The more accurate sum is returned\n        else:\n            # Recurse on sub-intervals with halved tolerance\n            return self._recursive_step(a, m, tol / 2.0) + self._recursive_step(m, b, tol / 2.0)\n\n    def integrate_cost(self, a, b, epsilon):\n        \"\"\"\n        Calculates the integral to a given tolerance and returns the number of\n        function evaluations.\n        \"\"\"\n        self.memo.clear()\n        self._recursive_step(a, b, epsilon)\n        return len(self.memo)\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    integrand = lambda x: np.exp(-x**2)\n    # High-precision reference value for the integral\n    I_ref = np.sqrt(np.pi) / 2.0 * erf(1)\n\n    # Pre-compute Gauss-Legendre quadrature errors for various numbers of points 'n'\n    # This avoids re-computation and speeds up the search for n_G(epsilon).\n    # n up to 30 is sufficient for tolerances down to machine epsilon for this function.\n    max_n_gauss = 30\n    gauss_errors = {}\n    for n in range(1, max_n_gauss + 1):\n        nodes, weights = roots_legendre(n)\n        # Transform nodes and weights from [-1, 1] to [0, 1]\n        t = 0.5 * (nodes + 1.0)\n        w = 0.5 * weights\n        integral_approx = np.sum(w * integrand(t))\n        error = abs(I_ref - integral_approx)\n        gauss_errors[n] = error\n\n    def get_NG(epsilon):\n        \"\"\"\n        Finds the smallest n (cost) for Gauss-Legendre quadrature to achieve\n        the given tolerance, using the pre-computed error table.\n        \"\"\"\n        for n in sorted(gauss_errors.keys()):\n            if gauss_errors[n] <= epsilon:\n                return n\n        # Should not be reached if max_n_gauss is large enough\n        raise ValueError(\"max_n_gauss is too small for the given tolerance.\")\n\n    # Initialize the adaptive Simpson solver\n    simpson_solver = AdaptiveSimpson(integrand)\n\n    def get_NS(epsilon):\n        \"\"\"\n        Calculates the cost (number of function evaluations) for the adaptive\n        Simpson's rule to achieve the given tolerance.\n        \"\"\"\n        return simpson_solver.integrate_cost(0, 1, epsilon)\n\n    test_cases = [\n        [1e-2, 1e-3, 1e-4, 1e-5],\n        [1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12],\n        [5e-4, 2e-6, 1e-8, 5e-10],\n    ]\n\n    final_results = []\n    for case in test_cases:\n        # Sort tolerances ascending to find the smallest epsilon that satisfies the condition\n        tolerances = sorted(case)\n        crossover_value = np.nan\n        \n        for eps in tolerances:\n            N_G = get_NG(eps)\n            N_S = get_NS(eps)\n            \n            if N_G < N_S:\n                crossover_value = eps\n                break  # Found the smallest epsilon in the family\n\n        final_results.append(crossover_value)\n    \n    # Format the final output string as per requirements. `str(np.nan)` gives 'nan'.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        }
    ]
}