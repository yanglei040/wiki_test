{
    "hands_on_practices": [
        {
            "introduction": "这第一个实践是应用最小二乘法的一个基础练习。我们将获取一组离散数据点，并找到“最佳拟合”的二次曲面，这是在工程中创建局部近似或代理模型的常见任务。这个练习将巩固你对于如何将一个数据拟合问题转化为一个线性方程组的理解，从而构筑最小二乘近似的代数基础。",
            "id": "2408246",
            "problem": "一位计算工程师正在点 $(0,0)$ 附近校准一个复杂的黑箱响应函数的局部代理模型。该代理模型是一个二元二次多项式，形式为 $f(x,y) \\approx c_1 + c_2 x + c_3 y + c_4 x^2 + c_5 y^2 + c_6 xy$。使用相同的实验方案，在 $6$ 个邻近位置对该函数进行了采样，得到以下数据（所有测量值权重相等）：\n\n- 在 $(0,0)$ 处，测量值为 $2$。\n- 在 $(1,0)$ 处，测量值为 $9$。\n- 在 $(0,1)$ 处，测量值为 $6$。\n- 在 $(1,1)$ 处，测量值为 $19$。\n- 在 $(-1,1)$ 处，测量值为 $1$。\n- 在 $(1,-1)$ 处，测量值为 $9$。\n\n确定系数 $c_1, c_2, c_3, c_4, c_5, c_6$，使得二次代理模型与给定测量值之间的残差平方和最小。将最终答案表示为单行矩阵 $\\begin{pmatrix} c_1  c_2  c_3  c_4  c_5  c_6 \\end{pmatrix}$。无需四舍五入。",
            "solution": "对数据 $\\{(x_k, y_k, f_k)\\}_{k=1}^{6}$ 进行二次模型 $f(x,y) = c_1 + c_2 x + c_3 y + c_4 x^2 + c_5 y^2 + c_6 xy$ 的等权重最小二乘拟合，即最小化\n$$\nS(c_1,\\dots,c_6) = \\sum_{k=1}^{6} \\left(c_1 + c_2 x_k + c_3 y_k + c_4 x_k^2 + c_5 y_k^2 + c_6 x_k y_k - f_k\\right)^{2}.\n$$\n令 $A \\in \\mathbb{R}^{6 \\times 6}$ 为设计矩阵，其第 $k$ 行为 $[1, x_k, y_k, x_k^2, y_k^2, x_k y_k]$，并令 $b \\in \\mathbb{R}^{6}$ 为测量值 $f_k$ 构成的向量。最小化问题的解 $c$ 满足正规方程组 $A^T A c = A^T b$。由于在本问题中 $A$ 是方阵，且如下文所示是可逆的，因此最小二乘解即为 $A c = b$ 的唯一解。\n\n根据数据，与这 $6$ 个点对应的 $6$ 个方程如下：\n- 在 $(0,0)$ 处：$c_1 = 2$。\n- 在 $(1,0)$ 处：$c_1 + c_2 + c_4 = 9$。\n- 在 $(0,1)$ 处：$c_1 + c_3 + c_5 = 6$。\n- 在 $(1,1)$ 处：$c_1 + c_2 + c_3 + c_4 + c_5 + c_6 = 19$。\n- 在 $(-1,1)$ 处：$c_1 - c_2 + c_3 + c_4 + c_5 - c_6 = 1$。\n- 在 $(1,-1)$ 处：$c_1 + c_2 - c_3 + c_4 + c_5 - c_6 = 9$。\n\n由第一个方程可知，$c_1 = 2$。定义\n$$\na := (c_2 + c_4) = 9 - c_1 = 9 - 2 = 7, \\qquad b := (c_3 + c_5) = 6 - c_1 = 6 - 2 = 4.\n$$\n从后三个方程中减去 $c_1$，得到\n$$\n\\begin{aligned}\n(1,1):\\quad c_2 + c_3 + c_4 + c_5 + c_6 = 19 - 2 = 17,\\\\\n(-1,1):\\quad -c_2 + c_3 + c_4 + c_5 - c_6 = 1 - 2 = -1,\\\\\n(1,-1):\\quad c_2 - c_3 + c_4 + c_5 - c_6 = 9 - 2 = 7.\n\\end{aligned}\n$$\n将最后两个关系式相加，得到\n$$\n\\left[-c_2 + c_3 + c_4 + c_5 - c_6\\right] + \\left[c_2 - c_3 + c_4 + c_5 - c_6\\right] = -1 + 7,\n$$\n化简为\n$$\n2(c_4 + c_5) - 2 c_6 = 6 \\quad \\Rightarrow \\quad c_4 + c_5 = 3 + c_6.\n$$\n将这两个关系式相减，得到\n$$\n\\left[c_2 - c_3 + c_4 + c_5 - c_6\\right] - \\left[-c_2 + c_3 + c_4 + c_5 - c_6\\right] = 7 - (-1),\n$$\n因此\n$$\n2(c_2 - c_3) = 8 \\quad \\Rightarrow \\quad c_2 - c_3 = 4.\n$$\n由 $(1,1)$ 的关系式，\n$$\n(c_2 + c_3) + (c_4 + c_5) + c_6 = 17.\n$$\n利用 $c_4 + c_5 = 3 + c_6$，上式变为\n$$\n(c_2 + c_3) + (3 + c_6) + c_6 = 17 \\quad \\Rightarrow \\quad c_2 + c_3 = 14 - 2 c_6.\n$$\n我们还有 $a = c_2 + c_4 = 7$ 和 $b = c_3 + c_5 = 4$，这意味着\n$$\n(c_2 + c_3) + (c_4 + c_5) = a + b = 11.\n$$\n代入 $c_4 + c_5 = 3 + c_6$，可得\n$$\n(c_2 + c_3) + (3 + c_6) = 11 \\quad \\Rightarrow \\quad c_2 + c_3 = 8 - c_6.\n$$\n令 $c_2 + c_3$ 的两个表达式相等，得到\n$$\n14 - 2 c_6 = 8 - c_6 \\quad \\Rightarrow \\quad c_6 = 6.\n$$\n于是 $c_4 + c_5 = 3 + c_6 = 9$。我们有 $c_2 - c_3 = 4$ 和 $c_2 + c_3 = 8 - c_6 = 2$，解得\n$$\nc_2 = \\frac{(c_2 + c_3) + (c_2 - c_3)}{2} = \\frac{2 + 4}{2} = 3, \\qquad c_3 = \\frac{(c_2 + c_3) - (c_2 - c_3)}{2} = \\frac{2 - 4}{2} = -1.\n$$\n最后，由 $a = c_2 + c_4 = 7$ 可得 $c_4 = 7 - c_2 = 4$，由 $b = c_3 + c_5 = 4$ 可得 $c_5 = 4 - c_3 = 5$。\n\n汇总所有系数：\n$$\nc_1 = 2, \\quad c_2 = 3, \\quad c_3 = -1, \\quad c_4 = 4, \\quad c_5 = 5, \\quad c_6 = 6.\n$$\n这些系数精确地满足所有六个方程，因此残差为零，该解即为唯一的最小二乘解。",
            "answer": "$$\\boxed{\\begin{pmatrix}2  3  -1  4  5  6\\end{pmatrix}}$$"
        },
        {
            "introduction": "在基础框架之上，本实践引入了一个关键的现实世界考量：并非所有数据点都是生而平等的。在许多科学和工程应用中，测量值伴随着不同程度的不确定性。通过这个编码练习，你将实现加权最小二乘法（WLS），让更可靠的数据点对最终模型产生更大的影响，这是稳健数据分析的一项基本技术。",
            "id": "2408206",
            "problem": "一组标量观测值是从单个传感器在离散时间点获得的，其已知的误差方差表明了不同的可靠性。设时间 $t_i$ 的观测值为 $y_i$，其误差方差为 $\\sigma_i^2$。考虑二次模型 $y_i \\approx \\beta_0 + \\beta_1 t_i + \\beta_2 t_i^2$，写成矩阵形式为 $y \\approx A \\beta$，其中 $A \\in \\mathbb{R}^{m \\times 3}$ 的行为 $[1, t_i, t_i^2]$，$y \\in \\mathbb{R}^m$ 是观测值的堆叠向量，$\\beta \\in \\mathbb{R}^3$ 是系数向量。定义对角权重矩阵 $W \\in \\mathbb{R}^{m \\times m}$ 为 $W = \\mathrm{diag}(w_1,\\dots,w_m)$，其中 $w_i = 1/\\sigma_i^2$。\n\n对于下述每个测试用例，计算使加权平方误差\n$$\nJ(\\beta) = (y - A \\beta)^\\top W (y - A \\beta),\n$$\n最小化的系数向量 $\\hat{\\beta} \\in \\mathbb{R}^3$，以及加权残差范数\n$$\n\\rho = \\sqrt{(y - A \\hat{\\beta})^\\top W (y - A \\hat{\\beta})}.\n$$\n所有算术运算均无单位。不涉及角度。报告所有浮点输出时，四舍五入到恰好 $6$ 位小数。\n\n测试套件：\n- 案例 $1$（异方差，超定）：时间 $t = [0, 1, 2, 3]$，观测值 $y = [1.1, 1.9, 1.0, -0.2]$，方差 $\\sigma^2 = [0.04, 0.09, 0.04, 0.16]$。\n- 案例 $2$（恰定，可靠性变化）：时间 $t = [0, 1, 3]$，观测值 $y = [1.0, -0.5, -0.5]$，方差 $\\sigma^2 = [0.25, 4.0, 0.25]$。\n- 案例 $3$（可靠性强烈变化，对称采样）：时间 $t = [-2, -1, 0, 1, 2]$，观测值 $y = [4.7, 2.2, 0.6, 2.1, 4.6]$，方差 $\\sigma^2 = [1.0, 0.25, 0.0004, 0.25, 100.0]$。\n\n要求的最终输出格式：\n- 您的程序必须生成一行输出来，其中包含一个列表，列表中的每个条目按顺序对应一个测试用例。每个条目都必须是 $[\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\rho]$ 形式的列表，其中每个值都四舍五入到 $6$ 位小数。因此，整体输出行应如列表的列表，例如：$[[b_0,b_1,b_2,r],[\\dots],[\\dots]]$。",
            "solution": "问题陈述已经过分析，并被认为是有效的。它具有科学依据，是适定的、客观且完整的。它代表了加权最小二乘法的标准应用，这是计算工程和统计学中的一个基础课题。我现在将提供解决方案。\n\n问题是找到系数向量 $\\hat{\\beta} \\in \\mathbb{R}^3$，使模型 $y_i \\approx \\beta_0 + \\beta_1 t_i + \\beta_2 t_i^2$ 最佳拟合一组 $m$ 个观测值 $(t_i, y_i)$，其中每个观测值 $y_i$ 都具有已知的误差方差 $\\sigma_i^2$。“最佳拟合”定义为使加权平方误差和最小化的拟合，该准则由以下目标函数表示：\n$$\nJ(\\beta) = (y - A \\beta)^\\top W (y - A \\beta)\n$$\n此处，$y \\in \\mathbb{R}^m$ 是观测值的列向量，$\\beta = [\\beta_0, \\beta_1, \\beta_2]^\\top$ 是系数向量，$A \\in \\mathbb{R}^{m \\times 3}$ 是设计矩阵，其第 $i$ 行为 $[1, t_i, t_i^2]$，$W$ 是权重对角矩阵，$W = \\mathrm{diag}(w_1, w_2, \\dots, w_m)$，其中 $w_i = 1/\\sigma_i^2$。\n\n这个目标函数并非任意设定的。在假设测量误差是独立的、服从零均值和方差为 $\\sigma_i^2$ 的正态分布的条件下，最小化该目标函数等同于对 $\\beta$ 进行最大似然估计（MLE）。这是因为最大化似然函数等价于最小化平方误差之和，其中每项误差都由其方差进行归一化，而这正是 $J(\\beta)$ 所表示的。\n\n为了找到使二次型 $J(\\beta)$ 最小化的向量 $\\hat{\\beta}$，我们必须通过将 $J(\\beta)$ 相对于 $\\beta$ 的梯度设为零来找到驻点。首先，我们展开目标函数：\n$$\nJ(\\beta) = y^\\top W y - y^\\top W A \\beta - (A \\beta)^\\top W y + (A \\beta)^\\top W A \\beta\n$$\n认识到 $y^\\top W A \\beta$ 是一个标量，因此等于其转置 $\\beta^\\top A^\\top W y$，我们合并线性项：\n$$\nJ(\\beta) = y^\\top W y - 2 \\beta^\\top A^\\top W y + \\beta^\\top (A^\\top W A) \\beta\n$$\n使用标准矩阵微积分恒等式，可以求得相对于 $\\beta$ 的梯度：\n$$\n\\nabla_{\\beta} J(\\beta) = \\frac{\\partial J(\\beta)}{\\partial \\beta} = -2 A^\\top W y + 2(A^\\top W A)\\beta\n$$\n将梯度设为零向量，即可得到最小值的条件：\n$$\n-2 A^\\top W y + 2(A^\\top W A)\\hat{\\beta} = 0\n$$\n这可以简化为称为 **加权正规方程组** (weighted normal equations) 的线性方程组：\n$$\n(A^\\top W A) \\hat{\\beta} = A^\\top W y\n$$\n为了使唯一解 $\\hat{\\beta}$ 存在，矩阵 $A^\\top W A$ 必须是可逆的。如果 $A$ 的列是线性无关的，这个条件就成立，在所有给定的测试用例中都是如此，因为它们都涉及至少三个不同的时间点 $t_i$。由于所有指定的方差 $\\sigma_i^2$ 都是正的，权重矩阵 $W$ 是正定的，从而保证了 $A^\\top W A$ 的可逆性。解的形式可以正式写为：\n$$\n\\hat{\\beta} = (A^\\top W A)^{-1} (A^\\top W y)\n$$\n为了获得更好的数值稳定性，应避免直接计算逆矩阵。一个更好的方法是将问题转换为一个标准的、非加权的最小二乘问题。令 $\\sqrt{W}$ 为对角矩阵，其元素为 $\\sqrt{w_i} = 1/\\sigma_i$。我们定义缩放后的变量 $\\tilde{A} = \\sqrt{W} A$ 和 $\\tilde{y} = \\sqrt{W} y$。目标函数变为：\n$$\nJ(\\beta) = (\\tilde{y} - \\tilde{A} \\beta)^\\top (\\tilde{y} - \\tilde{A} \\beta) = ||\\tilde{y} - \\tilde{A} \\beta||_2^2\n$$\n这是一个标准的最小二乘问题，可以使用诸如 QR 分解等方法稳健地求解 $\\hat{\\beta}$，例如 `numpy.linalg.lstsq` 中所实现的。\n\n一旦确定了最优系数向量 $\\hat{\\beta}$，第二个要求解的量是加权残差范数 $\\rho$。它就是最小化目标函数值 $J(\\hat{\\beta})$ 的平方根：\n$$\n\\rho = \\sqrt{J(\\hat{\\beta})} = \\sqrt{(y - A \\hat{\\beta})^\\top W (y - A \\hat{\\beta})} = \\sqrt{||\\tilde{y} - \\tilde{A} \\hat{\\beta}||_2^2}\n$$\n值 $||\\tilde{y} - \\tilde{A} \\hat{\\beta}||_2^2$ 是缩放后问题的残差平方和，通常由数值最小二乘求解器返回。\n\n在案例 2 中出现了一种特殊情况，观测数量 $m=3$ 等于参数数量 $n=3$。如果时间点 $t_i$ 是不同的，矩阵 $A$ 就是一个方阵且可逆。该问题是恰定的。在这种情况下，解就是 $\\hat{\\beta} = A^{-1} y$，这将得到一个完美拟合，$A\\hat{\\beta} = y$。残差向量 $y - A\\hat{\\beta}$ 是零向量，因此，无论权重如何，加权残差范数 $\\rho$ 都为零。\n\n对于每个测试用例，要实现的算法如下：\n1.  构建观测向量 $y$ 和时间点向量 $t$。\n2.  构建设计矩阵 $A \\in \\mathbb{R}^{m \\times 3}$，其中第 $i$ 行为 $[1, t_i, t_i^2]$。\n3.  根据方差 $\\sigma^2$ 计算权重 $w_i = 1/\\sigma_i^2$。\n4.  通过将 $A$ 的每一行乘以相应的 $\\sqrt{w_i}$ 来构成缩放后的矩阵 $\\tilde{A}$。\n5.  通过将 $y$ 的每个元素乘以相应的 $\\sqrt{w_i}$ 来构成缩放后的向量 $\\tilde{y}$。\n6.  求解标准最小二乘问题 $\\tilde{A}\\beta \\approx \\tilde{y}$，以找到 $\\hat{\\beta}$ 和残差平方和 $S_{res} = ||\\tilde{y} - \\tilde{A} \\hat{\\beta}||_2^2$。\n7.  计算加权残差范数 $\\rho = \\sqrt{S_{res}}$。\n8.  将结果向量 $[\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\rho]$ 格式化，其值四舍五入到 $6$ 位小数。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of weighted least-squares problems to fit a quadratic model.\n    For each case, it computes the optimal coefficient vector and the weighted residual norm.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"t\": np.array([0, 1, 2, 3], dtype=float),\n            \"y\": np.array([1.1, 1.9, 1.0, -0.2], dtype=float),\n            \"sigma2\": np.array([0.04, 0.09, 0.04, 0.16], dtype=float)\n        },\n        {\n            \"t\": np.array([0, 1, 3], dtype=float),\n            \"y\": np.array([1.0, -0.5, -0.5], dtype=float),\n            \"sigma2\": np.array([0.25, 4.0, 0.25], dtype=float)\n        },\n        {\n            \"t\": np.array([-2, -1, 0, 1, 2], dtype=float),\n            \"y\": np.array([4.7, 2.2, 0.6, 2.1, 4.6], dtype=float),\n            \"sigma2\": np.array([1.0, 0.25, 0.0004, 0.25, 100.0], dtype=float)\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        t = case[\"t\"]\n        y = case[\"y\"]\n        sigma2 = case[\"sigma2\"]\n        m = len(t)\n        \n        # Construct the design matrix A\n        A = np.zeros((m, 3))\n        A[:, 0] = 1.0\n        A[:, 1] = t\n        A[:, 2] = t**2\n        \n        # Calculate weights and their square roots\n        w = 1.0 / sigma2\n        sqrt_w = np.sqrt(w)\n        \n        # Scale the problem to a standard least-squares form\n        # A_tilde = sqrt(W) * A\n        # y_tilde = sqrt(W) * y\n        # We can do this efficiently by row-wise multiplication\n        A_tilde = A * sqrt_w[:, np.newaxis]\n        y_tilde = y * sqrt_w\n        \n        # Solve the standard least-squares problem for beta_hat\n        # lstsq returns: coefficients, residuals, rank, singular values\n        # The 'residuals' is the sum of squared errors ||y_tilde - A_tilde*beta_hat||^2\n        beta_hat, residuals, _, _ = np.linalg.lstsq(A_tilde, y_tilde, rcond=None)\n        \n        # The weighted residual norm rho is the square root of the sum of squared residuals\n        # of the scaled problem. If the system is overdetermined, residuals will be non-empty.\n        if residuals.size > 0:\n            rho = np.sqrt(residuals[0])\n        else:\n            # For an exactly determined system, the residual is zero.\n            rho = 0.0\n            \n        case_result = [beta_hat[0], beta_hat[1], beta_hat[2], rho]\n        \n        # Format the result with values rounded to 6 decimal places.\n        formatted_case_result = [f\"{val:.6f}\" for val in case_result]\n        all_results.append(f\"[{','.join(formatted_case_result)}]\")\n    \n    # Print the final output in the required list-of-lists format\n    print(f\"[{','.join(all_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "拟合复杂模型的能力伴随着一个重大风险：过拟合。这最后一个实践使用了一个经典的例子——龙格现象（Runge's phenomenon），来展示一个高次多项式如何能够完美匹配样本数据，却在数据点之间剧烈振荡，从而无法代表真实的潜在函数。通过编写这个模拟程序，您将对为何必须谨慎管理模型复杂度以及模型验证的重要性有一个切实的理解。",
            "id": "2408214",
            "problem": "给定一个定义在 $[-1,1]$ 上的实值函数 $f: [-1,1] \\to \\mathbb{R}$，其表达式为 $f(x) = \\frac{1}{1+25x^2}$。对于一个给定的正整数 $m \\ge 2$，定义等距采样点 $x_i = -1 + \\frac{2i}{m-1}$，其中 $i=0, 1, \\dots, m-1$。令 $y_i = f(x_i)$ 对所有 $i$ 成立。对于一个给定的非负整数 $n$ 且 $n \\le m-1$，考虑次数至多为 $n$ 的多项式空间 $\\mathcal{P}_n = \\{p: p(x) = \\sum_{j=0}^{n} c_j x^j\\}$。定义函数 $g, h: [-1,1] \\to \\mathbb{R}$ 上的离散内积为 $\\langle g, h \\rangle_m = \\sum_{i=0}^{m-1} g(x_i) h(x_i)$。设 $p_n \\in \\mathcal{P}_n$ 是在所有 $p \\in \\mathcal{P}_n$ 中使得离散平方和 $\\sum_{i=0}^{m-1} (p(x_i) - y_i)^2$ 最小的任意多项式。这个 $p_n$ 是 $f$ 在关于 $\\langle \\cdot, \\cdot \\rangle_m$ 的内积下到 $\\mathcal{P}_n$ 上的正交投影。\n\n定义一个验证网格 $G$，它由 $[-1,1]$ 上的 $N_v=1001$ 个等距点组成。$p_n$ 相对于 $f$ 的验证均方根 (RMS) 误差为\n$$\nE_{\\text{val}} = \\sqrt{\\frac{1}{N_v} \\sum_{x \\in G} (p_n(x) - f(x))^2}\\,.\n$$\n您必须编写一个完整的、可运行的程序，该程序能为每个指定的测试用例计算如上定义的 $E_{\\text{val}}$。不涉及任何物理单位。如果出现任何角度，则必须以弧度为单位进行解释。使用标准四舍五入将每个 $E_{\\text{val}}$ 圆整到10位小数。\n\n待使用的参数对 $(m,n)$ 测试套件：\n- 用例 1: $(21,5)$。\n- 用例 2: $(21,20)$。\n- 用例 3: $(2,1)$。\n- 用例 4: $(41,20)$。\n- 用例 5: $(9,8)$。\n\n您的程序应生成单行输出，其中包含按上述用例顺序排列、用方括号括起来的、以逗号分隔的结果列表。例如，一种可接受的格式是 $[e_1,e_2,e_3,e_4,e_5]$，其中每个 $e_k$ 是用例 $k$ 的四舍五入后的验证RMS误差，表示为小数值。",
            "solution": "该问题要求通过离散最小二乘法确定给定函数 $f(x)$ 的一个多项式逼近 $p_n(x)$，然后评估该逼近的准确性。\n\n待逼近的函数是龙格函数，定义为 $f(x) = \\dfrac{1}{1+25x^{2}}$，定义域为 $x \\in [-1, 1]$。对于每个测试用例，我们都给定一个整数 $m \\geq 2$（指定采样点数量）和一个整数 $n \\geq 0$（指定最大多项式次数），并受限于 $n \\leq m-1$。\n\n$m$ 个采样点（记为 $x_i$，其中 $i=0, 1, \\dots, m-1$）被规定为在区间 $[-1, 1]$ 内等距分布，使得 $x_i = -1 + \\dfrac{2i}{m-1}$。在这些点上对应的函数值为 $y_i = f(x_i)$。\n\n我们的任务是在所有次数至多为 $n$ 的多项式构成的空间 $\\mathcal{P}_n$ 中找到一个多项式 $p_n(x)$。该多项式在单项式基中定义为 $p_n(x) = \\sum_{j=0}^{n} c_j x^j$。系数（由向量 $\\mathbf{c} = [c_0, c_1, \\dots, c_n]^T$ 表示）的选择必须使得多项式与函数在采样点上的平方差之和最小化：\n$$\nS(\\mathbf{c}) = \\sum_{i=0}^{m-1} \\left( p_n(x_i) - y_i \\right)^2 = \\sum_{i=0}^{m-1} \\left( \\left( \\sum_{j=0}^{n} c_j x_i^j \\right) - y_i \\right)^2\n$$\n这个最小化问题是一个经典的线性最小二乘问题。它可以通过定义一个 $m \\times (n+1)$ 的范德蒙矩阵 $\\mathbf{A}$（其元素为 $A_{ij} = x_i^j$，其中 $i=0, \\dots, m-1$ 且 $j=0, \\dots, n$）以矩阵代数的形式来表述。如果我们令 $\\mathbf{y}$ 为 $m \\times 1$ 的采样值列向量，即 $\\mathbf{y} = [y_0, y_1, \\dots, y_{m-1}]^T$，那么目标就是最小化残差向量的欧几里德范数的平方：\n$$\nS(\\mathbf{c}) = \\| \\mathbf{A}\\mathbf{c} - \\mathbf{y} \\|_2^2\n$$\n最小化此表达式的唯一系数向量 $\\mathbf{c}$ 是正规方程组的解：\n$$\n\\mathbf{A}^T \\mathbf{A} \\mathbf{c} = \\mathbf{A}^T \\mathbf{y}\n$$\n约束 $n \\leq m-1$ 意味着至少有 $n+1$ 个不同的采样点，这确保了矩阵 $\\mathbf{A}$ 的列是线性无关的。这反过来保证了格拉姆矩阵 $\\mathbf{A}^T\\mathbf{A}$ 是对称正定的，因而是可逆的，从而确保了 $\\mathbf{c}$ 的唯一解。虽然正规方程组提供了理论解，但由于 $\\mathbf{A}^T\\mathbf{A}$ 可能的病态性，直接计算可能会遭受数值不稳定性的影响。在计算上，采用基于矩阵分解的方法（如对 $\\mathbf{A}$ 进行 QR 分解或奇异值分解 (SVD)）更为优越，这些方法已在高质量的数值库中实现。\n\n在 $n = m-1$ 的特定情况下，待定系数的数量 $n+1$ 与采样点的数量 $m$ 相同。矩阵 $\\mathbf{A}$ 成为一个可逆的方阵。此时，最小二乘解对应于线性系统 $\\mathbf{A}\\mathbf{c} = \\mathbf{y}$ 的精确解。这意味着多项式 $p_n(x)$ 精确地插值了这些数据点，满足对所有 $i$ 都有 $p_n(x_i) = y_i$。\n\n在为给定的 $(m,n)$ 对计算出最优系数向量 $\\mathbf{c}$ 后，多项式 $p_n(x)$ 就被定义了。接着，在一个由 $[-1, 1]$ 上的 $N_v = 1001$ 个等距点组成的精细验证网格 $G$ 上评估其准确性。验证均方根 (RMS) 误差被定义和计算如下：\n$$\nE_{\\text{val}} = \\sqrt{\\frac{1}{N_v} \\sum_{x \\in G} \\left( p_n(x) - f(x) \\right)^2}\n$$\n因此，为每个测试用例 $(m, n)$ 执行的算法如下：\n1.  生成 $m$ 个采样点 $x_i$ 并计算对应的函数值 $y_i=f(x_i)$。\n2.  求解线性最小二乘问题 $\\min_{\\mathbf{c}} \\| \\mathbf{A}\\mathbf{c} - \\mathbf{y} \\|_2^2$ 以获得多项式系数 $\\mathbf{c}$，其中 $A_{ij} = x_i^j$。为此目的，使用一个数值稳定的库函数。\n3.  建立包含其 $N_v=1001$ 个点的验证网格 $G$。\n4.  在所有点 $x \\in G$ 上，评估已确定的多项式 $p_n(x)$ 和原始函数 $f(x)$ 的值。\n5.  从这些评估值中计算 RMS 误差 $E_{\\text{val}}$。\n6.  将结果圆整到10位小数。\n\n这个系统性的步骤被应用于每个指定的测试用例，以产生最终结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the validation RMS error for polynomial least-squares approximations\n    of the Runge function for several test cases.\n    \"\"\"\n\n    def f(x: np.ndarray) - np.ndarray:\n        \"\"\"\n        The Runge function to be approximated.\n        f(x) = 1 / (1 + 25*x^2)\n        \"\"\"\n        return 1.0 / (1.0 + 25.0 * x**2)\n\n    # Test suite of parameter pairs (m, n)\n    # m: number of sample points\n    # n: degree of the polynomial\n    test_cases = [\n        (21, 5),\n        (21, 20),\n        (2, 1),\n        (41, 20),\n        (9, 8),\n    ]\n\n    results = []\n\n    # Define the validation grid G\n    N_v = 1001\n    x_val = np.linspace(-1.0, 1.0, N_v)\n    f_val = f(x_val)\n\n    for m, n in test_cases:\n        # Step 1: Generate m equispaced sample points and their function values.\n        x_samples = np.linspace(-1.0, 1.0, m)\n        y_samples = f(x_samples)\n\n        # Step 2: Find the polynomial p_n of degree n that best fits the\n        # (x_samples, y_samples) data in a least-squares sense.\n        # The numpy.polynomial.polynomial.polyfit function solves this by\n        # finding the coefficients c that minimize the squared error.\n        # The coefficients are returned for the basis 1, x, x^2, ..., x^n.\n        coeffs = np.polynomial.polynomial.polyfit(x_samples, y_samples, n)\n\n        # Step 3: Evaluate the obtained polynomial p_n on the validation grid.\n        p_n_val = np.polynomial.polynomial.polyval(x_val, coeffs)\n\n        # Step 4: Compute the validation root mean square (RMS) error.\n        squared_errors = (p_n_val - f_val)**2\n        mean_squared_error = np.mean(squared_errors)\n        rms_error = np.sqrt(mean_squared_error)\n\n        # Step 5: Round the result to 10 decimal places as specified.\n        rounded_error = round(rms_error, 10)\n        results.append(rounded_error)\n\n    # Format the final output as a comma-separated list in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}