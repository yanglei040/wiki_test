{
    "hands_on_practices": [
        {
            "introduction": "This first practice problem bridges the gap between raw data and the linear algebra framework of least squares. You will construct a design matrix for a multivariate polynomial model, a foundational skill for fitting complex surfaces to experimental or simulation data. This exercise illustrates how to translate a set of measurements into a solvable linear system .",
            "id": "2408246",
            "problem": "A computational engineer is calibrating a local surrogate for a complicated black-box response function near the point $(0,0)$. The surrogate is a bivariate quadratic polynomial of the form $f(x,y) \\approx c_{1} + c_{2} x + c_{3} y + c_{4} x^{2} + c_{5} y^{2} + c_{6} xy$. The function has been probed at $6$ nearby locations using the same experimental protocol, producing the following data (with equal weighting of all measurements):\n\n- At $(0,0)$, the measured value is $2$.\n- At $(1,0)$, the measured value is $9$.\n- At $(0,1)$, the measured value is $6$.\n- At $(1,1)$, the measured value is $19$.\n- At $(-1,1)$, the measured value is $1$.\n- At $(1,-1)$, the measured value is $9$.\n\nDetermine the coefficients $c_{1},c_{2},c_{3},c_{4},c_{5},c_{6}$ that minimize the sum of squared residuals between the quadratic surrogate and the given measurements. Express your final answer as a single row matrix $\\begin{pmatrix} c_{1} & c_{2} & c_{3} & c_{4} & c_{5} & c_{6} \\end{pmatrix}$. No rounding is required.",
            "solution": "The least-squares fit of the quadratic model $f(x,y) = c_{1} + c_{2} x + c_{3} y + c_{4} x^{2} + c_{5} y^{2} + c_{6} xy$ to data $\\{(x_{k},y_{k},f_{k})\\}_{k=1}^{6}$ with equal weights minimizes\n$$\nS(c_{1},\\dots,c_{6}) = \\sum_{k=1}^{6} \\left(c_{1} + c_{2} x_{k} + c_{3} y_{k} + c_{4} x_{k}^{2} + c_{5} y_{k}^{2} + c_{6} x_{k} y_{k} - f_{k}\\right)^{2}.\n$$\nLet $A \\in \\mathbb{R}^{6 \\times 6}$ be the design matrix whose $k$-th row is $\\left[1,\\, x_{k},\\, y_{k},\\, x_{k}^{2},\\, y_{k}^{2},\\, x_{k}y_{k}\\right]$, and let $b \\in \\mathbb{R}^{6}$ be the vector of measured values $f_{k}$. The minimizer satisfies the normal equations $A^{\\mathsf{T}} A\\, c = A^{\\mathsf{T}} b$. Since here $A$ is square and, as shown below, invertible, the least-squares solution equals the unique solution of $A c = b$.\n\nFrom the data, the $6$ equations corresponding to the $6$ points are:\n- At $(0,0)$: $c_{1} = 2$.\n- At $(1,0)$: $c_{1} + c_{2} + c_{4} = 9$.\n- At $(0,1)$: $c_{1} + c_{3} + c_{5} = 6$.\n- At $(1,1)$: $c_{1} + c_{2} + c_{3} + c_{4} + c_{5} + c_{6} = 19$.\n- At $(-1,1)$: $c_{1} - c_{2} + c_{3} + c_{4} + c_{5} - c_{6} = 1$.\n- At $(1,-1)$: $c_{1} + c_{2} - c_{3} + c_{4} + c_{5} - c_{6} = 9$.\n\nFrom the first equation, $c_{1} = 2$. Define\n$$\na := (c_{2} + c_{4}) = 9 - c_{1} = 9 - 2 = 7, \\qquad b := (c_{3} + c_{5}) = 6 - c_{1} = 6 - 2 = 4.\n$$\nSubtracting $c_{1}$ from the next three equations gives\n$$\n\\begin{aligned}\n(1,1):&\\quad c_{2} + c_{3} + c_{4} + c_{5} + c_{6} = 19 - 2 = 17,\\\\\n(-1,1):&\\quad -c_{2} + c_{3} + c_{4} + c_{5} - c_{6} = 1 - 2 = -1,\\\\\n(1,-1):&\\quad c_{2} - c_{3} + c_{4} + c_{5} - c_{6} = 9 - 2 = 7.\n\\end{aligned}\n$$\nAdding the last two relations yields\n$$\n\\left[-c_{2} + c_{3} + c_{4} + c_{5} - c_{6}\\right] + \\left[c_{2} - c_{3} + c_{4} + c_{5} - c_{6}\\right] = -1 + 7,\n$$\nwhich simplifies to\n$$\n2(c_{4} + c_{5}) - 2 c_{6} = 6 \\quad \\Rightarrow \\quad c_{4} + c_{5} = 3 + c_{6}.\n$$\nSubtracting the same two relations gives\n$$\n\\left[c_{2} - c_{3} + c_{4} + c_{5} - c_{6}\\right] - \\left[-c_{2} + c_{3} + c_{4} + c_{5} - c_{6}\\right] = 7 - (-1),\n$$\nhence\n$$\n2(c_{2} - c_{3}) = 8 \\quad \\Rightarrow \\quad c_{2} - c_{3} = 4.\n$$\nFrom the $(1,1)$ relation,\n$$\n(c_{2} + c_{3}) + (c_{4} + c_{5}) + c_{6} = 17.\n$$\nUsing $c_{4} + c_{5} = 3 + c_{6}$, this becomes\n$$\n(c_{2} + c_{3}) + (3 + c_{6}) + c_{6} = 17 \\quad \\Rightarrow \\quad c_{2} + c_{3} = 14 - 2 c_{6}.\n$$\nWe also have $a = c_{2} + c_{4} = 7$ and $b = c_{3} + c_{5} = 4$, which imply\n$$\n(c_{2} + c_{3}) + (c_{4} + c_{5}) = a + b = 11.\n$$\nSubstituting $c_{4} + c_{5} = 3 + c_{6}$ gives\n$$\n(c_{2} + c_{3}) + (3 + c_{6}) = 11 \\quad \\Rightarrow \\quad c_{2} + c_{3} = 8 - c_{6}.\n$$\nEquating the two expressions for $c_{2} + c_{3}$ yields\n$$\n14 - 2 c_{6} = 8 - c_{6} \\quad \\Rightarrow \\quad c_{6} = 6.\n$$\nThen $c_{4} + c_{5} = 3 + c_{6} = 9$. With $c_{2} - c_{3} = 4$ and $c_{2} + c_{3} = 8 - c_{6} = 2$, we solve\n$$\nc_{2} = \\frac{(c_{2} + c_{3}) + (c_{2} - c_{3})}{2} = \\frac{2 + 4}{2} = 3, \\qquad c_{3} = \\frac{(c_{2} + c_{3}) - (c_{2} - c_{3})}{2} = \\frac{2 - 4}{2} = -1.\n$$\nFinally, from $a = c_{2} + c_{4} = 7$ we get $c_{4} = 7 - c_{2} = 4$, and from $b = c_{3} + c_{5} = 4$ we get $c_{5} = 4 - c_{3} = 5$.\n\nCollecting all coefficients:\n$$\nc_{1} = 2, \\quad c_{2} = 3, \\quad c_{3} = -1, \\quad c_{4} = 4, \\quad c_{5} = 5, \\quad c_{6} = 6.\n$$\nThese satisfy all six equations exactly, so the residuals are zero and the solution is the unique least-squares minimizer.",
            "answer": "$$\\boxed{\\begin{pmatrix}2 & 3 & -1 & 4 & 5 & 6\\end{pmatrix}}$$"
        },
        {
            "introduction": "Real-world data is rarely perfect; some measurements are more reliable than others. This exercise introduces the powerful technique of weighted least squares (WLS), where you will incorporate data quality, expressed as error variances, directly into the fitting process. By assigning higher weights to more certain data points, you will learn how to obtain a more robust and statistically meaningful model from heteroscedastic data .",
            "id": "2408206",
            "problem": "A set of scalar observations is obtained from a single sensor at discrete times, with known error variances indicating different reliabilities. Let the observation at time $t_i$ be $y_i$, and let its error variance be $\\sigma_i^2$. Consider the quadratic model $y_i \\approx \\beta_0 + \\beta_1 t_i + \\beta_2 t_i^2$, written in matrix form as $y \\approx A \\beta$, where $A \\in \\mathbb{R}^{m \\times 3}$ has rows $[\\,1,\\ t_i,\\ t_i^2\\,]$, $y \\in \\mathbb{R}^m$ stacks the observations, and $\\beta \\in \\mathbb{R}^3$ is the coefficient vector. Define the diagonal weight matrix $W \\in \\mathbb{R}^{m \\times m}$ by $W = \\mathrm{diag}(w_1,\\dots,w_m)$ with $w_i = 1/\\sigma_i^2$.\n\nFor each test case below, compute the coefficient vector $\\hat{\\beta} \\in \\mathbb{R}^3$ that minimizes the weighted squared error\n$$\nJ(\\beta) = (y - A \\beta)^\\top W (y - A \\beta),\n$$\nand the weighted residual norm\n$$\n\\rho = \\sqrt{(y - A \\hat{\\beta})^\\top W (y - A \\hat{\\beta})}.\n$$\nAll arithmetic is unitless. Angles do not appear. Report all floating-point outputs rounded to exactly $6$ decimal places.\n\nTest suite:\n- Case $1$ (heteroscedastic, overdetermined): times $t = [0, 1, 2, 3]$, observations $y = [1.1, 1.9, 1.0, -0.2]$, variances $\\sigma^2 = [0.04, 0.09, 0.04, 0.16]$.\n- Case $2$ (exactly determined, varying reliability): times $t = [0, 1, 3]$, observations $y = [1.0, -0.5, -0.5]$, variances $\\sigma^2 = [0.25, 4.0, 0.25]$.\n- Case $3$ (strongly varying reliability, symmetric sampling): times $t = [-2, -1, 0, 1, 2]$, observations $y = [4.7, 2.2, 0.6, 2.1, 4.6]$, variances $\\sigma^2 = [1.0, 0.25, 0.0004, 0.25, 100.0]$.\n\nRequired final output format:\n- Your program must produce a single line containing a list with one entry per test case, in order. Each entry must be the list $[\\hat{\\beta}_0,\\hat{\\beta}_1,\\hat{\\beta}_2,\\rho]$, where each value is rounded to $6$ decimals. The overall line must therefore look like a list of lists, for example: $[[b_{0},b_{1},b_{2},r],[\\dots],[\\dots]]$.",
            "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded, well-posed, objective, and complete. It represents a standard application of the weighted least-squares method, a fundamental topic in computational engineering and statistics. I will now provide the solution.\n\nThe problem is to find the coefficient vector $\\hat{\\beta} \\in \\mathbb{R}^3$ that best fits the model $y_i \\approx \\beta_0 + \\beta_1 t_i + \\beta_2 t_i^2$ to a set of $m$ observations $(t_i, y_i)$, where each observation $y_i$ has a known error variance $\\sigma_i^2$. The \"best fit\" is defined as the one that minimizes the weighted sum of squared errors, a criterion expressed by the objective function:\n$$\nJ(\\beta) = (y - A \\beta)^\\top W (y - A \\beta)\n$$\nHere, $y \\in \\mathbb{R}^m$ is the column vector of observations, $\\beta = [\\beta_0, \\beta_1, \\beta_2]^\\top$ is the vector of coefficients, $A \\in \\mathbb{R}^{m \\times 3}$ is the design matrix whose $i$-th row is $[1, t_i, t_i^2]$, and $W$ is a diagonal matrix of weights, $W = \\mathrm{diag}(w_1, w_2, \\dots, w_m)$, with $w_i = 1/\\sigma_i^2$.\n\nThis objective function is not arbitrary. Its minimization corresponds to the Maximum Likelihood Estimation (MLE) of $\\beta$ under the assumption that the measurement errors are independent and normally distributed with zero mean and variances $\\sigma_i^2$. This is because maximizing the likelihood function is equivalent to minimizing the sum of squared errors, each normalized by its variance, which is precisely what $J(\\beta)$ represents.\n\nTo find the vector $\\hat{\\beta}$ that minimizes the quadratic form $J(\\beta)$, we must find the stationary point by setting the gradient of $J(\\beta)$ with respect to $\\beta$ to zero. First, we expand the objective function:\n$$\nJ(\\beta) = y^\\top W y - y^\\top W A \\beta - (A \\beta)^\\top W y + (A \\beta)^\\top W A \\beta\n$$\nRecognizing that $y^\\top W A \\beta$ is a scalar and thus equal to its transpose $\\beta^\\top A^\\top W y$, we combine the linear terms:\n$$\nJ(\\beta) = y^\\top W y - 2 \\beta^\\top A^\\top W y + \\beta^\\top (A^\\top W A) \\beta\n$$\nThe gradient with respect to $\\beta$ is found using standard matrix calculus identities:\n$$\n\\nabla_{\\beta} J(\\beta) = \\frac{\\partial J(\\beta)}{\\partial \\beta} = -2 A^\\top W y + 2(A^\\top W A)\\beta\n$$\nSetting the gradient to the zero vector gives the condition for the minimum:\n$$\n-2 A^\\top W y + 2(A^\\top W A)\\hat{\\beta} = 0\n$$\nThis simplifies to the system of linear equations known as the **weighted normal equations**:\n$$\n(A^\\top W A) \\hat{\\beta} = A^\\top W y\n$$\nFor a unique solution $\\hat{\\beta}$ to exist, the matrix $A^\\top W A$ must be invertible. This condition holds if the columns of $A$ are linearly independent, which is true in all given test cases as they involve at least three distinct time points $t_i$. Since all specified variances $\\sigma_i^2$ are positive, the weight matrix $W$ is positive definite, ensuring the invertibility of $A^\\top W A$. The solution is formally given by:\n$$\n\\hat{\\beta} = (A^\\top W A)^{-1} (A^\\top W y)\n$$\nFor superior numerical stability, direct computation of the inverse is avoided. A better approach is to transform the problem into a standard, unweighted least-squares problem. Let $\\sqrt{W}$ be the diagonal matrix with entries $\\sqrt{w_i} = 1/\\sigma_i$. We define scaled variables $\\tilde{A} = \\sqrt{W} A$ and $\\tilde{y} = \\sqrt{W} y$. The objective function becomes:\n$$\nJ(\\beta) = (\\tilde{y} - \\tilde{A} \\beta)^\\top (\\tilde{y} - \\tilde{A} \\beta) = ||\\tilde{y} - \\tilde{A} \\beta||_2^2\n$$\nThis is a standard least-squares problem, which can be robustly solved for $\\hat{\\beta}$ using methods such as QR decomposition, as implemented in `numpy.linalg.lstsq`.\n\nOnce the optimal coefficient vector $\\hat{\\beta}$ is determined, the second required quantity is the weighted residual norm, $\\rho$. This is simply the square root of the minimized objective function value, $J(\\hat{\\beta})$:\n$$\n\\rho = \\sqrt{J(\\hat{\\beta})} = \\sqrt{(y - A \\hat{\\beta})^\\top W (y - A \\hat{\\beta})} = \\sqrt{||\\tilde{y} - \\tilde{A} \\hat{\\beta}||_2^2}\n$$\nThe value $||\\tilde{y} - \\tilde{A} \\hat{\\beta}||_2^2$ is the sum of squared residuals of the scaled problem and is typically returned by numerical least-squares solvers.\n\nA special situation arises in Case 2, where the number of observations $m=3$ equals the number of parameters $n=3$. If the time points $t_i$ are distinct, the matrix $A$ is square and invertible. The problem is exactly determined. In this case, the solution is simply $\\hat{\\beta} = A^{-1} y$, which results in a perfect fit, $A\\hat{\\beta} = y$. The residual vector $y - A\\hat{\\beta}$ is the zero vector, and consequently, the weighted residual norm $\\rho$ is zero, irrespective of the weights.\n\nThe algorithm to be implemented is as follows for each test case:\n1.  Construct the observation vector $y$ and the vector of time points $t$.\n2.  Construct the design matrix $A \\in \\mathbb{R}^{m \\times 3}$, where the $i$-th row is $[1, t_i, t_i^2]$.\n3.  From the variances $\\sigma^2$, compute the weights $w_i = 1/\\sigma_i^2$.\n4.  Form the scaled matrix $\\tilde{A}$ by multiplying each row of $A$ by the corresponding $\\sqrt{w_i}$.\n5.  Form the scaled vector $\\tilde{y}$ by multiplying each element of $y$ by the corresponding $\\sqrt{w_i}$.\n6.  Solve the standard least-squares problem $\\tilde{A}\\beta \\approx \\tilde{y}$ to find $\\hat{\\beta}$ and the sum of squared residuals, $S_{res} = ||\\tilde{y} - \\tilde{A} \\hat{\\beta}||_2^2$.\n7.  Calculate the weighted residual norm $\\rho = \\sqrt{S_{res}}$.\n8.  Format the resulting vector $[\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\rho]$ with values rounded to $6$ decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of weighted least-squares problems to fit a quadratic model.\n    For each case, it computes the optimal coefficient vector and the weighted residual norm.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"t\": np.array([0, 1, 2, 3], dtype=float),\n            \"y\": np.array([1.1, 1.9, 1.0, -0.2], dtype=float),\n            \"sigma2\": np.array([0.04, 0.09, 0.04, 0.16], dtype=float)\n        },\n        {\n            \"t\": np.array([0, 1, 3], dtype=float),\n            \"y\": np.array([1.0, -0.5, -0.5], dtype=float),\n            \"sigma2\": np.array([0.25, 4.0, 0.25], dtype=float)\n        },\n        {\n            \"t\": np.array([-2, -1, 0, 1, 2], dtype=float),\n            \"y\": np.array([4.7, 2.2, 0.6, 2.1, 4.6], dtype=float),\n            \"sigma2\": np.array([1.0, 0.25, 0.0004, 0.25, 100.0], dtype=float)\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        t = case[\"t\"]\n        y = case[\"y\"]\n        sigma2 = case[\"sigma2\"]\n        m = len(t)\n        \n        # Construct the design matrix A\n        A = np.zeros((m, 3))\n        A[:, 0] = 1.0\n        A[:, 1] = t\n        A[:, 2] = t**2\n        \n        # Calculate weights and their square roots\n        w = 1.0 / sigma2\n        sqrt_w = np.sqrt(w)\n        \n        # Scale the problem to a standard least-squares form\n        # A_tilde = sqrt(W) * A\n        # y_tilde = sqrt(W) * y\n        # We can do this efficiently by row-wise multiplication\n        A_tilde = A * sqrt_w[:, np.newaxis]\n        y_tilde = y * sqrt_w\n        \n        # Solve the standard least-squares problem for beta_hat\n        # lstsq returns: coefficients, residuals, rank, singular values\n        # The 'residuals' is the sum of squared errors ||y_tilde - A_tilde*beta_hat||^2\n        beta_hat, residuals, _, _ = np.linalg.lstsq(A_tilde, y_tilde, rcond=None)\n        \n        # The weighted residual norm rho is the square root of the sum of squared residuals\n        # of the scaled problem. If the system is overdetermined, residuals will be non-empty.\n        if residuals.size > 0:\n            rho = np.sqrt(residuals[0])\n        else:\n            # For an exactly determined system, the residual is zero.\n            rho = 0.0\n            \n        case_result = [beta_hat[0], beta_hat[1], beta_hat[2], rho]\n        \n        # Format the result with values rounded to 6 decimal places.\n        formatted_case_result = [f\"{val:.6f}\" for val in case_result]\n        all_results.append(f\"[{','.join(formatted_case_result)}]\")\n    \n    # Print the final output in the required list-of-lists format\n    print(f\"[{','.join(all_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "A model that fits your data perfectly is not always the best model. This coding challenge confronts a classic pitfall in approximation theory and machine learning: overfitting. By fitting high-degree polynomials to a smooth function, you will observe Runge's phenomenon, where the fit oscillates wildly between data points, leading to poor predictive power .",
            "id": "2408214",
            "problem": "You are given a real-valued function $f:\\,[-1,1]\\to\\mathbb{R}$ defined by $f(x)=\\dfrac{1}{1+25x^{2}}$. For a given positive integer $m\\geq 2$, define equispaced sample points $x_i=-1+\\dfrac{2i}{m-1}$ for $i=0,1,\\dots,m-1$. Let $y_i=f(x_i)$ for all $i$. For a given nonnegative integer $n$ with $n\\leq m-1$, consider the polynomial space $\\mathcal{P}_n=\\{p:\\,p(x)=\\sum_{j=0}^{n}c_j x^{j}\\}$ of degree at most $n$. Define the discrete inner product on functions $g,h:[-1,1]\\to\\mathbb{R}$ by $\\langle g,h\\rangle_m=\\sum_{i=0}^{m-1} g(x_i)\\,h(x_i)$. Let $p_n\\in\\mathcal{P}_n$ be any polynomial that minimizes the discrete sum of squares $\\sum_{i=0}^{m-1}\\big(p(x_i)-y_i\\big)^2$ over all $p\\in\\mathcal{P}_n$. This $p_n$ is the orthogonal projection of $f$ onto $\\mathcal{P}_n$ with respect to $\\langle\\cdot,\\cdot\\rangle_m$.\n\nDefine a validation grid $G$ consisting of $N_v=1001$ equispaced points on $[-1,1]$. The validation root mean square (RMS) error of $p_n$ with respect to $f$ is\n$$\nE_{\\text{val}}=\\sqrt{\\frac{1}{N_v}\\sum_{x\\in G}\\big(p_n(x)-f(x)\\big)^2}\\,.\n$$\nYou must write a complete, runnable program that, for each specified test case, computes $E_{\\text{val}}$ as defined above. No physical units are involved. All angles, if any arise, must be interpreted in radians. Round each $E_{\\text{val}}$ to $10$ decimal places using standard rounding.\n\nTest suite of parameter pairs $(m,n)$ to be used:\n- Case $1$: $(m,n)=(21,5)$.\n- Case $2$: $(m,n)=(21,20)$.\n- Case $3$: $(m,n)=(2,1)$.\n- Case $4$: $(m,n)=(41,20)$.\n- Case $5$: $(m,n)=(9,8)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above. For example, an acceptable format is $[e_1,e_2,e_3,e_4,e_5]$, where each $e_k$ is the rounded validation RMS error for Case $k$ as a decimal number.",
            "solution": "The problem requires the determination of a polynomial approximation $p_n(x)$ to a given function $f(x)$ through the method of discrete least squares, followed by an evaluation of this approximation's accuracy.\n\nThe function to be approximated is the Runge function, defined as $f(x) = \\dfrac{1}{1+25x^{2}}$ on the domain $x \\in [-1, 1]$. For each test case, we are provided with an integer $m \\geq 2$ specifying the number of sample points and an integer $n \\geq 0$ specifying the maximum polynomial degree, constrained by $n \\leq m-1$.\n\nThe $m$ sample points, denoted by $x_i$ for $i=0, 1, \\dots, m-1$, are stipulated to be equispaced within the interval $[-1, 1]$, such that $x_i = -1 + \\dfrac{2i}{m-1}$. The corresponding function values at these points are $y_i = f(x_i)$.\n\nWe are tasked with finding a polynomial $p_n(x)$ within the space $\\mathcal{P}_n$ of all polynomials of degree at most $n$. This polynomial is defined in the monomial basis as $p_n(x) = \\sum_{j=0}^{n} c_j x^j$. The coefficients, represented by the vector $\\mathbf{c} = [c_0, c_1, \\dots, c_n]^T$, must be chosen to minimize the sum of the squared differences between the polynomial and the function at the sample points:\n$$\nS(\\mathbf{c}) = \\sum_{i=0}^{m-1} \\left( p_n(x_i) - y_i \\right)^2 = \\sum_{i=0}^{m-1} \\left( \\left( \\sum_{j=0}^{n} c_j x_i^j \\right) - y_i \\right)^2\n$$\nThis minimization problem is a classic linear least-squares problem. It can be formulated in matrix algebra by defining an $m \\times (n+1)$ Vandermonde matrix $\\mathbf{A}$ with entries $A_{ij} = x_i^j$ for $i=0, \\dots, m-1$ and $j=0, \\dots, n$. If we let $\\mathbf{y}$ be the $m \\times 1$ column vector of sampled values, $\\mathbf{y} = [y_0, y_1, \\dots, y_{m-1}]^T$, the objective is to minimize the squared Euclidean norm of the residual vector:\n$$\nS(\\mathbf{c}) = \\| \\mathbf{A}\\mathbf{c} - \\mathbf{y} \\|_2^2\n$$\nThe unique vector of coefficients $\\mathbf{c}$ that minimizes this expression is the solution to the system of normal equations:\n$$\n\\mathbf{A}^T \\mathbf{A} \\mathbf{c} = \\mathbf{A}^T \\mathbf{y}\n$$\nThe constraint $n \\leq m-1$ implies that there are at least $n+1$ distinct sample points, which ensures that the columns of the matrix $\\mathbf{A}$ are linearly independent. This, in turn, guarantees that the Gram matrix $\\mathbf{A}^T\\mathbf{A}$ is symmetric and positive definite, and therefore invertible, ensuring a unique solution for $\\mathbf{c}$. While the normal equations provide a theoretical solution, direct computation can suffer from numerical instability due to the potential ill-conditioning of $\\mathbf{A}^T\\mathbf{A}$. It is computationally superior to employ methods based on matrix factorizations such as QR decomposition or Singular Value Decomposition (SVD) of $\\mathbf{A}$, which are implemented in high-quality numerical libraries.\n\nIn the specific instance where $n = m-1$, the number of coefficients to determine, $n+1$, is identical to the number of sample points, $m$. The matrix $\\mathbf{A}$ becomes a square, invertible matrix. The least-squares solution then corresponds to the exact solution of the linear system $\\mathbf{A}\\mathbf{c} = \\mathbf{y}$. This signifies that the polynomial $p_n(x)$ exactly interpolates the data points, satisfying $p_n(x_i) = y_i$ for all $i$.\n\nAfter computing the optimal coefficient vector $\\mathbf{c}$ for a given $(m,n)$ pair, the polynomial $p_n(x)$ is defined. Its accuracy is then assessed on a fine validation grid $G$, which consists of $N_v = 1001$ equispaced points on $[-1, 1]$. The validation root mean square (RMS) error is defined and calculated as:\n$$\nE_{\\text{val}} = \\sqrt{\\frac{1}{N_v} \\sum_{x \\in G} \\left( p_n(x) - f(x) \\right)^2}\n$$\nThe algorithm executed for each test case $(m, n)$ is therefore as follows:\n1.  Generate the $m$ sample points $x_i$ and compute the corresponding function values $y_i=f(x_i)$.\n2.  Solve the linear least-squares problem $\\min_{\\mathbf{c}} \\| \\mathbf{A}\\mathbf{c} - \\mathbf{y} \\|_2^2$ to obtain the polynomial coefficients $\\mathbf{c}$, where $A_{ij} = x_i^j$. A numerically stable library function is used for this purpose.\n3.  Establish the validation grid $G$ with its $N_v=1001$ points.\n4.  Evaluate the determined polynomial $p_n(x)$ and the original function $f(x)$ at all points $x \\in G$.\n5.  Compute the RMS error $E_{\\text{val}}$ from these evaluations.\n6.  The result is rounded to $10$ decimal places.\n\nThis systematic procedure is applied to each of the specified test cases to produce the final results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the validation RMS error for polynomial least-squares approximations\n    of the Runge function for several test cases.\n    \"\"\"\n\n    def f(x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        The Runge function to be approximated.\n        f(x) = 1 / (1 + 25*x^2)\n        \"\"\"\n        return 1.0 / (1.0 + 25.0 * x**2)\n\n    # Test suite of parameter pairs (m, n)\n    # m: number of sample points\n    # n: degree of the polynomial\n    test_cases = [\n        (21, 5),\n        (21, 20),\n        (2, 1),\n        (41, 20),\n        (9, 8),\n    ]\n\n    results = []\n\n    # Define the validation grid G\n    N_v = 1001\n    x_val = np.linspace(-1.0, 1.0, N_v)\n    f_val = f(x_val)\n\n    for m, n in test_cases:\n        # Step 1: Generate m equispaced sample points and their function values.\n        x_samples = np.linspace(-1.0, 1.0, m)\n        y_samples = f(x_samples)\n\n        # Step 2: Find the polynomial p_n of degree n that best fits the\n        # (x_samples, y_samples) data in a least-squares sense.\n        # The numpy.polynomial.polynomial.polyfit function solves this by\n        # finding the coefficients c that minimize the squared error.\n        # The coefficients are returned for the basis 1, x, x^2, ..., x^n.\n        coeffs = np.polynomial.polynomial.polyfit(x_samples, y_samples, n)\n\n        # Step 3: Evaluate the obtained polynomial p_n on the validation grid.\n        p_n_val = np.polynomial.polynomial.polyval(x_val, coeffs)\n\n        # Step 4: Compute the validation root mean square (RMS) error.\n        squared_errors = (p_n_val - f_val)**2\n        mean_squared_error = np.mean(squared_errors)\n        rms_error = np.sqrt(mean_squared_error)\n\n        # Step 5: Round the result to 10 decimal places as specified.\n        rounded_error = round(rms_error, 10)\n        results.append(rounded_error)\n\n    # Format the final output as a comma-separated list in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}