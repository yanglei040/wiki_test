{
    "hands_on_practices": [
        {
            "introduction": "In real-world engineering applications, not all data points are created equal; some measurements are more reliable than others. This practice introduces the powerful method of Weighted Least Squares (WLS), which allows you to incorporate this uncertainty into your model fitting. By assigning higher weights to more certain data points (those with smaller error variance), you will learn how to build more robust and statistically sound models from experimental data .",
            "id": "2408206",
            "problem": "A set of scalar observations is obtained from a single sensor at discrete times, with known error variances indicating different reliabilities. Let the observation at time $t_i$ be $y_i$, and let its error variance be $\\sigma_i^2$. Consider the quadratic model $y_i \\approx \\beta_0 + \\beta_1 t_i + \\beta_2 t_i^2$, written in matrix form as $y \\approx A \\beta$, where $A \\in \\mathbb{R}^{m \\times 3}$ has rows $[1, t_i, t_i^2]$, $y \\in \\mathbb{R}^m$ stacks the observations, and $\\beta \\in \\mathbb{R}^3$ is the coefficient vector. Define the diagonal weight matrix $W \\in \\mathbb{R}^{m \\times m}$ by $W = \\mathrm{diag}(w_1,\\dots,w_m)$ with $w_i = 1/\\sigma_i^2$.\n\nFor each test case below, compute the coefficient vector $\\hat{\\beta} \\in \\mathbb{R}^3$ that minimizes the weighted squared error\n$$\nJ(\\beta) = (y - A \\beta)^\\top W (y - A \\beta),\n$$\nand the weighted residual norm\n$$\n\\rho = \\sqrt{(y - A \\hat{\\beta})^\\top W (y - A \\hat{\\beta})}.\n$$\nAll arithmetic is unitless. Angles do not appear. Report all floating-point outputs rounded to exactly $6$ decimal places.\n\nTest suite:\n- Case $1$ (heteroscedastic, overdetermined): times $t = [0, 1, 2, 3]$, observations $y = [1.1, 1.9, 1.0, -0.2]$, variances $\\sigma^2 = [0.04, 0.09, 0.04, 0.16]$.\n- Case $2$ (exactly determined, varying reliability): times $t = [0, 1, 3]$, observations $y = [1.0, -0.5, -0.5]$, variances $\\sigma^2 = [0.25, 4.0, 0.25]$.\n- Case $3$ (strongly varying reliability, symmetric sampling): times $t = [-2, -1, 0, 1, 2]$, observations $y = [4.7, 2.2, 0.6, 2.1, 4.6]$, variances $\\sigma^2 = [1.0, 0.25, 0.0004, 0.25, 100.0]$.\n\nRequired final output format:\n- Your program must produce a single line containing a list with one entry per test case, in order. Each entry must be the list $[\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\rho]$, where each value is rounded to $6$ decimals. The overall line must therefore look like a list of lists, for example: $[[b_{0},b_{1},b_{2},r],[\\dots],[\\dots]]$.",
            "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded, well-posed, objective, and complete. It represents a standard application of the weighted least-squares method, a fundamental topic in computational engineering and statistics. I will now provide the solution.\n\nThe problem is to find the coefficient vector $\\hat{\\beta} \\in \\mathbb{R}^3$ that best fits the model $y_i \\approx \\beta_0 + \\beta_1 t_i + \\beta_2 t_i^2$ to a set of $m$ observations $(t_i, y_i)$, where each observation $y_i$ has a known error variance $\\sigma_i^2$. The \"best fit\" is defined as the one that minimizes the weighted sum of squared errors, a criterion expressed by the objective function:\n$$\nJ(\\beta) = (y - A \\beta)^\\top W (y - A \\beta)\n$$\nHere, $y \\in \\mathbb{R}^m$ is the column vector of observations, $\\beta = [\\beta_0, \\beta_1, \\beta_2]^\\top$ is the vector of coefficients, $A \\in \\mathbb{R}^{m \\times 3}$ is the design matrix whose $i$-th row is $[1, t_i, t_i^2]$, and $W$ is a diagonal matrix of weights, $W = \\mathrm{diag}(w_1, w_2, \\dots, w_m)$, with $w_i = 1/\\sigma_i^2$.\n\nThis objective function is not arbitrary. Its minimization corresponds to the Maximum Likelihood Estimation (MLE) of $\\beta$ under the assumption that the measurement errors are independent and normally distributed with zero mean and variances $\\sigma_i^2$. This is because maximizing the likelihood function is equivalent to minimizing the sum of squared errors, each normalized by its variance, which is precisely what $J(\\beta)$ represents.\n\nTo find the vector $\\hat{\\beta}$ that minimizes the quadratic form $J(\\beta)$, we must find the stationary point by setting the gradient of $J(\\beta)$ with respect to $\\beta$ to zero. First, we expand the objective function:\n$$\nJ(\\beta) = y^\\top W y - y^\\top W A \\beta - (A \\beta)^\\top W y + (A \\beta)^\\top W A \\beta\n$$\nRecognizing that $y^\\top W A \\beta$ is a scalar and thus equal to its transpose $\\beta^\\top A^\\top W y$, we combine the linear terms:\n$$\nJ(\\beta) = y^\\top W y - 2 \\beta^\\top A^\\top W y + \\beta^\\top (A^\\top W A) \\beta\n$$\nThe gradient with respect to $\\beta$ is found using standard matrix calculus identities:\n$$\n\\nabla_{\\beta} J(\\beta) = \\frac{\\partial J(\\beta)}{\\partial \\beta} = -2 A^\\top W y + 2(A^\\top W A)\\beta\n$$\nSetting the gradient to the zero vector gives the condition for the minimum:\n$$\n-2 A^\\top W y + 2(A^\\top W A)\\hat{\\beta} = 0\n$$\nThis simplifies to the system of linear equations known as the **weighted normal equations**:\n$$\n(A^\\top W A) \\hat{\\beta} = A^\\top W y\n$$\nFor a unique solution $\\hat{\\beta}$ to exist, the matrix $A^\\top W A$ must be invertible. This condition holds if the columns of $A$ are linearly independent, which is true in all given test cases as they involve at least three distinct time points $t_i$. Since all specified variances $\\sigma_i^2$ are positive, the weight matrix $W$ is positive definite, ensuring the invertibility of $A^\\top W A$. The solution is formally given by:\n$$\n\\hat{\\beta} = (A^\\top W A)^{-1} (A^\\top W y)\n$$\nFor superior numerical stability, direct computation of the inverse is avoided. A better approach is to transform the problem into a standard, unweighted least-squares problem. Let $\\sqrt{W}$ be the diagonal matrix with entries $\\sqrt{w_i} = 1/\\sigma_i$. We define scaled variables $\\tilde{A} = \\sqrt{W} A$ and $\\tilde{y} = \\sqrt{W} y$. The objective function becomes:\n$$\nJ(\\beta) = (\\tilde{y} - \\tilde{A} \\beta)^\\top (\\tilde{y} - \\tilde{A} \\beta) = \\|\\tilde{y} - \\tilde{A} \\beta\\|_2^2\n$$\nThis is a standard least-squares problem, which can be robustly solved for $\\hat{\\beta}$ using methods such as QR decomposition, as implemented in `numpy.linalg.lstsq`.\n\nOnce the optimal coefficient vector $\\hat{\\beta}$ is determined, the second required quantity is the weighted residual norm, $\\rho$. This is simply the square root of the minimized objective function value, $J(\\hat{\\beta})$:\n$$\n\\rho = \\sqrt{J(\\hat{\\beta})} = \\sqrt{(y - A \\hat{\\beta})^\\top W (y - A \\hat{\\beta})} = \\sqrt{\\|\\tilde{y} - \\tilde{A} \\hat{\\beta}\\|_2^2}\n$$\nThe value $\\|\\tilde{y} - \\tilde{A} \\hat{\\beta}\\|_2^2$ is the sum of squared residuals of the scaled problem and is typically returned by numerical least-squares solvers.\n\nA special situation arises in Case 2, where the number of observations $m=3$ equals the number of parameters $n=3$. If the time points $t_i$ are distinct, the matrix $A$ is square and invertible. The problem is exactly determined. In this case, the solution is simply $\\hat{\\beta} = A^{-1} y$, which results in a perfect fit, $A\\hat{\\beta} = y$. The residual vector $y - A\\hat{\\beta}$ is the zero vector, and consequently, the weighted residual norm $\\rho$ is zero, irrespective of the weights.\n\nThe algorithm to be implemented is as follows for each test case:\n1.  Construct the observation vector $y$ and the vector of time points $t$.\n2.  Construct the design matrix $A \\in \\mathbb{R}^{m \\times 3}$, where the $i$-th row is $[1, t_i, t_i^2]$.\n3.  From the variances $\\sigma^2$, compute the weights $w_i = 1/\\sigma_i^2$.\n4.  Form the scaled matrix $\\tilde{A}$ by multiplying each row of $A$ by the corresponding $\\sqrt{w_i}$.\n5.  Form the scaled vector $\\tilde{y}$ by multiplying each element of $y$ by the corresponding $\\sqrt{w_i}$.\n6.  Solve the standard least-squares problem $\\tilde{A}\\beta \\approx \\tilde{y}$ to find $\\hat{\\beta}$ and the sum of squared residuals, $S_{res} = \\|\\tilde{y} - \\tilde{A} \\hat{\\beta}\\|_2^2$.\n7.  Calculate the weighted residual norm $\\rho = \\sqrt{S_{res}}$.\n8.  Format the resulting vector $[\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\rho]$ with values rounded to $6$ decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of weighted least-squares problems to fit a quadratic model.\n    For each case, it computes the optimal coefficient vector and the weighted residual norm.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"t\": np.array([0, 1, 2, 3], dtype=float),\n            \"y\": np.array([1.1, 1.9, 1.0, -0.2], dtype=float),\n            \"sigma2\": np.array([0.04, 0.09, 0.04, 0.16], dtype=float)\n        },\n        {\n            \"t\": np.array([0, 1, 3], dtype=float),\n            \"y\": np.array([1.0, -0.5, -0.5], dtype=float),\n            \"sigma2\": np.array([0.25, 4.0, 0.25], dtype=float)\n        },\n        {\n            \"t\": np.array([-2, -1, 0, 1, 2], dtype=float),\n            \"y\": np.array([4.7, 2.2, 0.6, 2.1, 4.6], dtype=float),\n            \"sigma2\": np.array([1.0, 0.25, 0.0004, 0.25, 100.0], dtype=float)\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        t = case[\"t\"]\n        y = case[\"y\"]\n        sigma2 = case[\"sigma2\"]\n        m = len(t)\n        \n        # Construct the design matrix A\n        A = np.zeros((m, 3))\n        A[:, 0] = 1.0\n        A[:, 1] = t\n        A[:, 2] = t**2\n        \n        # Calculate weights and their square roots\n        w = 1.0 / sigma2\n        sqrt_w = np.sqrt(w)\n        \n        # Scale the problem to a standard least-squares form\n        # A_tilde = sqrt(W) * A\n        # y_tilde = sqrt(W) * y\n        # We can do this efficiently by row-wise multiplication\n        A_tilde = A * sqrt_w[:, np.newaxis]\n        y_tilde = y * sqrt_w\n        \n        # Solve the standard least-squares problem for beta_hat\n        # lstsq returns: coefficients, residuals, rank, singular values\n        # The 'residuals' is the sum of squared errors ||y_tilde - A_tilde*beta_hat||^2\n        beta_hat, residuals, _, _ = np.linalg.lstsq(A_tilde, y_tilde, rcond=None)\n        \n        # The weighted residual norm rho is the square root of the sum of squared residuals\n        # of the scaled problem. If the system is overdetermined, residuals will be non-empty.\n        if residuals.size > 0:\n            rho = np.sqrt(residuals[0])\n        else:\n            # For an exactly determined system, the residual is zero.\n            rho = 0.0\n            \n        case_result = [beta_hat[0], beta_hat[1], beta_hat[2], rho]\n        \n        # Format the result with values rounded to 6 decimal places.\n        formatted_case_result = [f\"{val:.6f}\" for val in case_result]\n        all_results.append(f\"[{','.join(formatted_case_result)}]\")\n    \n    # Print the final output in the required list-of-lists format\n    print(f\"[{','.join(all_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "After learning how to fit a model, a natural impulse is to use a more complex one—such as a higher-degree polynomial—to achieve a \"better\" fit. This hands-on coding exercise serves as a crucial cautionary tale, demonstrating the concept of overfitting through the classic Runge's phenomenon . You will discover that a model that follows the data too closely can oscillate wildly between points, leading to poor predictive performance and highlighting the critical trade-off between model complexity and generalization.",
            "id": "2408214",
            "problem": "You are given a real-valued function $f: [-1,1] \\to \\mathbb{R}$ defined by $f(x)=\\dfrac{1}{1+25x^{2}}$. For a given positive integer $m\\geq 2$, define equispaced sample points $x_i=-1+\\dfrac{2i}{m-1}$ for $i=0,1,\\dots,m-1$. Let $y_i=f(x_i)$ for all $i$. For a given nonnegative integer $n$ with $n\\leq m-1$, consider the polynomial space $\\mathcal{P}_n=\\{p: p(x)=\\sum_{j=0}^{n}c_j x^{j}\\}$ of degree at most $n$. Define the discrete inner product on functions $g,h:[-1,1]\\to\\mathbb{R}$ by $\\langle g,h\\rangle_m=\\sum_{i=0}^{m-1} g(x_i)h(x_i)$. Let $p_n\\in\\mathcal{P}_n$ be any polynomial that minimizes the discrete sum of squares $\\sum_{i=0}^{m-1}\\big(p(x_i)-y_i\\big)^2$ over all $p\\in\\mathcal{P}_n$. This $p_n$ is the orthogonal projection of $f$ onto $\\mathcal{P}_n$ with respect to $\\langle\\cdot,\\cdot\\rangle_m$.\n\nDefine a validation grid $G$ consisting of $N_v=1001$ equispaced points on $[-1,1]$. The validation root mean square (RMS) error of $p_n$ with respect to $f$ is\n$$\nE_{\\text{val}}=\\sqrt{\\frac{1}{N_v}\\sum_{x\\in G}\\big(p_n(x)-f(x)\\big)^2}\\,.\n$$\nYou must write a complete, runnable program that, for each specified test case, computes $E_{\\text{val}}$ as defined above. No physical units are involved. All angles, if any arise, must be interpreted in radians. Round each $E_{\\text{val}}$ to $10$ decimal places using standard rounding.\n\nTest suite of parameter pairs $(m,n)$ to be used:\n- Case $1$: $(m,n)=(21,5)$.\n- Case $2$: $(m,n)=(21,20)$.\n- Case $3$: $(m,n)=(2,1)$.\n- Case $4$: $(m,n)=(41,20)$.\n- Case $5$: $(m,n)=(9,8)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above. For example, an acceptable format is $[e_1,e_2,e_3,e_4,e_5]$, where each $e_k$ is the rounded validation RMS error for Case $k$ as a decimal number.",
            "solution": "The problem requires the determination of a polynomial approximation $p_n(x)$ to a given function $f(x)$ through the method of discrete least squares, followed by an evaluation of this approximation's accuracy.\n\nThe function to be approximated is the Runge function, defined as $f(x) = \\dfrac{1}{1+25x^{2}}$ on the domain $x \\in [-1, 1]$. For each test case, we are provided with an integer $m \\geq 2$ specifying the number of sample points and an integer $n \\geq 0$ specifying the maximum polynomial degree, constrained by $n \\leq m-1$.\n\nThe $m$ sample points, denoted by $x_i$ for $i=0, 1, \\dots, m-1$, are stipulated to be equispaced within the interval $[-1, 1]$, such that $x_i = -1 + \\dfrac{2i}{m-1}$. The corresponding function values at these points are $y_i = f(x_i)$.\n\nWe are tasked with finding a polynomial $p_n(x)$ within the space $\\mathcal{P}_n$ of all polynomials of degree at most $n$. This polynomial is defined in the monomial basis as $p_n(x) = \\sum_{j=0}^{n} c_j x^j$. The coefficients, represented by the vector $\\mathbf{c} = [c_0, c_1, \\dots, c_n]^T$, must be chosen to minimize the sum of the squared differences between the polynomial and the function at the sample points:\n$$\nS(\\mathbf{c}) = \\sum_{i=0}^{m-1} \\left( p_n(x_i) - y_i \\right)^2 = \\sum_{i=0}^{m-1} \\left( \\left( \\sum_{j=0}^{n} c_j x_i^j \\right) - y_i \\right)^2\n$$\nThis minimization problem is a classic linear least-squares problem. It can be formulated in matrix algebra by defining an $m \\times (n+1)$ Vandermonde matrix $\\mathbf{A}$ with entries $A_{ij} = x_i^j$ for $i=0, \\dots, m-1$ and $j=0, \\dots, n$. If we let $\\mathbf{y}$ be the $m \\times 1$ column vector of sampled values, $\\mathbf{y} = [y_0, y_1, \\dots, y_{m-1}]^T$, the objective is to minimize the squared Euclidean norm of the residual vector:\n$$\nS(\\mathbf{c}) = \\| \\mathbf{A}\\mathbf{c} - \\mathbf{y} \\|_2^2\n$$\nThe unique vector of coefficients $\\mathbf{c}$ that minimizes this expression is the solution to the system of normal equations:\n$$\n\\mathbf{A}^T \\mathbf{A} \\mathbf{c} = \\mathbf{A}^T \\mathbf{y}\n$$\nThe constraint $n \\leq m-1$ implies that there are at least $n+1$ distinct sample points, which ensures that the columns of the matrix $\\mathbf{A}$ are linearly independent. This, in turn, guarantees that the Gram matrix $\\mathbf{A}^T\\mathbf{A}$ is symmetric and positive definite, and therefore invertible, ensuring a unique solution for $\\mathbf{c}$. While the normal equations provide a theoretical solution, direct computation can suffer from numerical instability due to the potential ill-conditioning of $\\mathbf{A}^T\\mathbf{A}$. It is computationally superior to employ methods based on matrix factorizations such as QR decomposition or Singular Value Decomposition (SVD) of $\\mathbf{A}$, which are implemented in high-quality numerical libraries.\n\nIn the specific instance where $n = m-1$, the number of coefficients to determine, $n+1$, is identical to the number of sample points, $m$. The matrix $\\mathbf{A}$ becomes a square, invertible matrix. The least-squares solution then corresponds to the exact solution of the linear system $\\mathbf{A}\\mathbf{c} = \\mathbf{y}$. This signifies that the polynomial $p_n(x)$ exactly interpolates the data points, satisfying $p_n(x_i) = y_i$ for all $i$.\n\nAfter computing the optimal coefficient vector $\\mathbf{c}$ for a given $(m,n)$ pair, the polynomial $p_n(x)$ is defined. Its accuracy is then assessed on a fine validation grid $G$, which consists of $N_v = 1001$ equispaced points on $[-1, 1]$. The validation root mean square (RMS) error is defined and calculated as:\n$$\nE_{\\text{val}} = \\sqrt{\\frac{1}{N_v} \\sum_{x \\in G} \\left( p_n(x) - f(x) \\right)^2}\n$$\nThe algorithm executed for each test case $(m, n)$ is therefore as follows:\n1.  Generate the $m$ sample points $x_i$ and compute the corresponding function values $y_i=f(x_i)$.\n2.  Solve the linear least-squares problem $\\min_{\\mathbf{c}} \\| \\mathbf{A}\\mathbf{c} - \\mathbf{y} \\|_2^2$ to obtain the polynomial coefficients $\\mathbf{c}$, where $A_{ij} = x_i^j$. A numerically stable library function is used for this purpose.\n3.  Establish the validation grid $G$ with its $N_v=1001$ points.\n4.  Evaluate the determined polynomial $p_n(x)$ and the original function $f(x)$ at all points $x \\in G$.\n5.  Compute the RMS error $E_{\\text{val}}$ from these evaluations.\n6.  The result is rounded to $10$ decimal places.\n\nThis systematic procedure is applied to each of the specified test cases to produce the final results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the validation RMS error for polynomial least-squares approximations\n    of the Runge function for several test cases.\n    \"\"\"\n\n    def f(x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        The Runge function to be approximated.\n        f(x) = 1 / (1 + 25*x^2)\n        \"\"\"\n        return 1.0 / (1.0 + 25.0 * x**2)\n\n    # Test suite of parameter pairs (m, n)\n    # m: number of sample points\n    # n: degree of the polynomial\n    test_cases = [\n        (21, 5),\n        (21, 20),\n        (2, 1),\n        (41, 20),\n        (9, 8),\n    ]\n\n    results = []\n\n    # Define the validation grid G\n    N_v = 1001\n    x_val = np.linspace(-1.0, 1.0, N_v)\n    f_val = f(x_val)\n\n    for m, n in test_cases:\n        # Step 1: Generate m equispaced sample points and their function values.\n        x_samples = np.linspace(-1.0, 1.0, m)\n        y_samples = f(x_samples)\n\n        # Step 2: Find the polynomial p_n of degree n that best fits the\n        # (x_samples, y_samples) data in a least-squares sense.\n        # The numpy.polynomial.polynomial.polyfit function solves this by\n        # finding the coefficients c that minimize the squared error.\n        # The coefficients are returned for the basis 1, x, x^2, ..., x^n.\n        coeffs = np.polynomial.polynomial.polyfit(x_samples, y_samples, n)\n\n        # Step 3: Evaluate the obtained polynomial p_n on the validation grid.\n        p_n_val = np.polynomial.polynomial.polyval(x_val, coeffs)\n\n        # Step 4: Compute the validation root mean square (RMS) error.\n        squared_errors = (p_n_val - f_val)**2\n        mean_squared_error = np.mean(squared_errors)\n        rms_error = np.sqrt(mean_squared_error)\n\n        # Step 5: Round the result to 10 decimal places as specified.\n        rounded_error = round(rms_error, 10)\n        results.append(rounded_error)\n\n    # Format the final output as a comma-separated list in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A theoretically sound model can fail spectacularly if its numerical implementation is unstable. This practice delves into the vital topic of numerical conditioning in least-squares problems, exploring why the mathematical representation of your model matters just as much as the model itself . By comparing the well-behaved orthogonal Legendre basis against the notoriously ill-conditioned monomial basis ($1, x, x^2, \\dots$), you will gain a deep appreciation for how the choice of basis functions can mean the difference between an accurate solution and a cascade of numerical errors.",
            "id": "2408241",
            "problem": "Consider least-squares approximation of a scalar function on an interval by polynomials of degree at most $p$ from $N$ pointwise samples, with $N \\ge p+1$. Let the sample locations be $x_i \\in [-1,1]$, for $i=1,\\dots,N$. Define two $N \\times (p+1)$ design matrices:\n- The monomial Vandermonde matrix $A_{\\mathrm{m}}$ with entries $[A_{\\mathrm{m}}]_{i,k} = x_i^{k}$ for $k=0,\\dots,p$.\n- The Legendre matrix $A_{\\mathrm{L}}$ with entries $[A_{\\mathrm{L}}]_{i,k} = \\widetilde{P}_k(x_i)$, where $\\{\\widetilde{P}_k\\}_{k=0}^p$ are the first $p+1$ Legendre polynomials normalized to be orthonormal in $L^2([-1,1])$ with respect to the unit weight, that is, $\\int_{-1}^{1} \\widetilde{P}_k(x)\\widetilde{P}_\\ell(x)\\,dx = \\delta_{k\\ell}$.\n\nFor a given matrix $A$ with full column rank, consider the $2$-norm condition number of $A$, denoted $\\kappa_2(A)$, and the $2$-norm condition number of the normal-equations matrix $A^\\top A$, denoted $\\kappa_2(A^\\top A)$. In some scenarios below, the nodes $x_i$ are taken as the $N$-point Gauss–Legendre quadrature nodes on $[-1,1]$ with corresponding positive weights $w_i$, $i=1,\\dots,N$, and in weighted least squares one can equivalently work with the weighted design matrix $B = W^{1/2} A$ where $W = \\mathrm{diag}(w_1,\\dots,w_N)$.\n\nSelect all statements that are correct.\n\nA. For $N$ equispaced nodes in $[-1,1]$, $\\kappa_2(A_{\\mathrm{m}})$ grows rapidly with $p$, whereas $\\kappa_2(A_{\\mathrm{L}})$ remains bounded by a constant that is independent of both $p$ and $N$.\n\nB. If the $N$ nodes are the Gauss–Legendre nodes on $[-1,1]$ with quadrature weights $\\{w_i\\}_{i=1}^N$, then for $N \\ge p+1$ the weighted Legendre design matrix $B_{\\mathrm{L}} = W^{1/2} A_{\\mathrm{L}}$ has orthonormal columns, and therefore $\\kappa_2(B_{\\mathrm{L}}) = 1$.\n\nC. For any full-column-rank design matrix $A$, the relation $\\kappa_2(A^\\top A) = \\kappa_2(A)^2$ holds.\n\nD. Let $[a,b]$ be an interval with $|b-a|>2$. If one replaces $x$ by the affine-scaled variable $t = \\dfrac{2x-(a+b)}{b-a}$ so that $t \\in [-1,1]$ and builds the monomial design matrix in the variable $t$ at $N$ equally spaced samples in $[a,b]$ mapped to $[-1,1]$, then this scaling reduces $\\kappa_2$ compared with building the monomial design matrix directly in $x$ on $[a,b]$ under otherwise comparable sampling.\n\nE. Using the orthogonal–triangular factorization (QR) before solving the least-squares problem both eliminates the ill-conditioning of $A_{\\mathrm{m}}$ and guarantees that the numerical sensitivity of the computed solution is independent of the chosen polynomial basis, so the basis choice no longer matters.\n\nF. For $N$ equispaced nodes and large $p$, the columns of $A_{\\mathrm{m}}$ become nearly linearly dependent, whereas the columns of $A_{\\mathrm{L}}$ remain exactly orthogonal in the discrete inner product without any weights, for all $p \\le N-1$.",
            "solution": "The problem statement has been validated and is scientifically sound, well-posed, and objective. It presents a standard set of propositions concerning the numerical stability of polynomial least-squares approximation. We will proceed to analyze each statement in turn.\n\nThe core of the problem lies in the properties of the design matrix $A$ for polynomial least squares. The condition number of this matrix, $\\kappa_2(A)$, dictates the sensitivity of the least-squares solution to perturbations in the input data. A large condition number signifies an ill-conditioned problem, prone to numerical instability. The choice of polynomial basis profoundly impacts $\\kappa_2(A)$.\n\n**Analysis of Statement A:**\n\"For $N$ equispaced nodes in $[-1,1]$, $\\kappa_2(A_{\\mathrm{m}})$ grows rapidly with $p$, whereas $\\kappa_2(A_{\\mathrm{L}})$ remains bounded by a constant that is independent of both $p$ and $N$.\"\n\nThe first part of this statement is correct. The monomial basis functions $\\{x^k\\}_{k=0}^p$ become nearly linearly dependent on the interval $[-1,1]$ as the degree $p$ increases. This causes the columns of the Vandermonde matrix $A_{\\mathrm{m}}$ to become nearly collinear, leading to a condition number $\\kappa_2(A_{\\mathrm{m}})$ that grows exponentially with $p$.\n\nThe second part of the statement, however, is incorrect. While using an orthogonal polynomial basis such as Legendre polynomials results in a much better-conditioned matrix $A_{\\mathrm{L}}$ compared to $A_{\\mathrm{m}}$, the condition number $\\kappa_2(A_{\\mathrm{L}})$ for equispaced nodes is not bounded by a constant independent of $p$. The columns of $A_{\\mathrm{L}}$ are samples of functions $\\{\\widetilde{P}_k(x)\\}$ that are orthogonal with respect to the continuous $L^2$ inner product, i.e., $\\int_{-1}^{1} \\widetilde{P}_k(x)\\,\\widetilde{P}_\\ell(x)\\,dx = \\delta_{k\\ell}$. However, for equispaced nodes, the columns of $A_{\\mathrm{L}}$ are not orthogonal with respect to the discrete inner product $\\sum_{i=1}^N v_i w_i$. Consequently, the matrix $A_{\\mathrm{L}}^\\top A_{\\mathrm{L}}$ is not diagonal, and its condition number is greater than $1$. Theoretical and numerical results show that for equispaced nodes, $\\kappa_2(A_{\\mathrm{L}})$ grows polynomially with $p$ (e.g., as $O(p^2)$). This is a significant improvement over exponential growth, but it is not bounded by a constant independent of $p$.\n\nTherefore, the statement is **Incorrect**.\n\n**Analysis of Statement B:**\n\"If the $N$ nodes are the Gauss–Legendre nodes on $[-1,1]$ with quadrature weights $\\{w_i\\}_{i=1}^N$, then for $N \\ge p+1$ the weighted Legendre design matrix $B_{\\mathrm{L}} = W^{1/2} A_{\\mathrm{L}}$ has orthonormal columns, and therefore $\\kappa_2(B_{\\mathrm{L}}) = 1$.\"\n\nTo verify this, we must check the orthonormality of the columns of $B_{\\mathrm{L}}$. The matrix $B_{\\mathrm{L}}$ has entries $[B_{\\mathrm{L}}]_{i,k} = \\sqrt{w_i} \\widetilde{P}_k(x_i)$. The inner product between the $k$-th and $\\ell$-th columns is given by:\n$$ (B_{\\mathrm{L}})_{:k}^\\top (B_{\\mathrm{L}})_{:\\ell} = \\sum_{i=1}^N (\\sqrt{w_i} \\widetilde{P}_k(x_i)) (\\sqrt{w_i} \\widetilde{P}_\\ell(x_i)) = \\sum_{i=1}^N w_i \\widetilde{P}_k(x_i) \\widetilde{P}_\\ell(x_i) $$\nThis sum is precisely the $N$-point Gauss-Legendre quadrature rule applied to the function $f(x) = \\widetilde{P}_k(x) \\widetilde{P}_\\ell(x)$. The function $f(x)$ is a polynomial of degree $k+\\ell$. Since $k, \\ell \\le p$, the maximum degree of this polynomial is $2p$. An $N$-point Gauss-Legendre quadrature rule is exact for all polynomials of degree up to $2N-1$. The problem states that $N \\ge p+1$, which is equivalent to $p \\le N-1$. This implies $2p \\le 2(N-1) = 2N-2$. Since $2N-2  2N-1$, the quadrature rule is exact for the polynomial $\\widetilde{P}_k(x) \\widetilde{P}_\\ell(x)$.\n\nTherefore, the sum is equal to the integral:\n$$ \\sum_{i=1}^N w_i \\widetilde{P}_k(x_i) \\widetilde{P}_\\ell(x_i) = \\int_{-1}^1 \\widetilde{P}_k(x) \\widetilde{P}_\\ell(x) dx $$\nBy definition of the orthonormal Legendre polynomials, this integral equals the Kronecker delta, $\\delta_{k\\ell}$.\nThis proves that the columns of $B_{\\mathrm{L}}$ are orthonormal. For any matrix $Q$ with orthonormal columns, $Q^\\top Q=I$. The singular values of such a matrix are all equal to $1$. The $2$-norm condition number is the ratio of the largest to the smallest singular value, so $\\kappa_2(B_{\\mathrm{L}}) = \\sigma_{\\max}/\\sigma_{\\min} = 1/1=1$.\n\nTherefore, the statement is **Correct**.\n\n**Analysis of Statement C:**\n\"For any full-column-rank design matrix $A$, the relation $\\kappa_2(A^\\top A) = \\kappa_2(A)^2$ holds.\"\n\nLet the singular value decomposition (SVD) of the matrix $A$ be $A = U\\Sigma V^\\top$. Since $A$ has full column rank, its singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_{p+1}$ are all strictly positive. The $2$-norm condition number of $A$ is defined as $\\kappa_2(A) = \\sigma_1/\\sigma_{p+1}$.\n\nNow consider the matrix $A^\\top A$:\n$$ A^\\top A = (U\\Sigma V^\\top)^\\top (U\\Sigma V^\\top) = V\\Sigma^\\top U^\\top U\\Sigma V^\\top $$\nSince $U$ is a matrix with orthonormal columns (if $Np+1$) or is orthogonal (if $N=p+1$), we have $U^\\top U = I$, the identity matrix. Thus:\n$$ A^\\top A = V (\\Sigma^\\top \\Sigma) V^\\top $$\nThis is the eigenvalue decomposition of the symmetric matrix $A^\\top A$. The eigenvalues of $A^\\top A$ are the diagonal entries of the diagonal matrix $\\Sigma^\\top \\Sigma$, which are $\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_{p+1}^2$.\nThe condition number of a square matrix like $A^\\top A$ is the ratio of its largest to its smallest eigenvalue (in magnitude). So,\n$$ \\kappa_2(A^\\top A) = \\frac{\\lambda_{\\max}(A^\\top A)}{\\lambda_{\\min}(A^\\top A)} = \\frac{\\sigma_1^2}{\\sigma_{p+1}^2} $$\nComparing this with the square of $\\kappa_2(A)$:\n$$ \\kappa_2(A)^2 = \\left(\\frac{\\sigma_1}{\\sigma_{p+1}}\\right)^2 = \\frac{\\sigma_1^2}{\\sigma_{p+1}^2} $$\nThe relation $\\kappa_2(A^\\top A) = \\kappa_2(A)^2$ holds true. This identity is fundamental to understanding why forming and solving the normal equations is numerically less stable than methods that operate directly on $A$.\n\nTherefore, the statement is **Correct**.\n\n**Analysis of Statement D:**\n\"Let $[a,b]$ be an interval with $|b-a|2$. If one replaces $x$ by the affine-scaled variable $t = \\dfrac{2x-(a+b)}{b-a}$ so that $t \\in [-1,1]$ and builds the monomial design matrix in the variable $t$ at $N$ equally spaced samples in $[a,b]$ mapped to $[-1,1]$, then this scaling reduces $\\kappa_2$ compared with building the monomial design matrix directly in $x$ on $[a,b]$ under otherwise comparable sampling.\"\n\nThe ill-conditioning of the monomial basis $\\{1, x, x^2, \\dots, x^p\\}$ is exacerbated when the interval $[a,b]$ is far from the origin or has a large width. On an interval like $[100, 101]$, the functions $x^k$ are almost constant multiples of each other, leading to nearly linearly dependent columns in the Vandermonde matrix. On a large interval like $[0, 100]$, the range of values of $x^p$ is enormous, leading to a poorly scaled matrix.\n\nThe affine transformation $t = \\frac{2x - (a+b)}{b-a}$ maps the interval $[a,b]$ to $[-1,1]$. Building the design matrix using the basis $\\{1, t, t^2, \\dots, t^p\\}$ is equivalent to using a shifted and scaled polynomial basis in the original variable $x$. This re-centering and scaling is a standard and effective technique for improving the conditioning of polynomial fitting problems. The functions $t^k$ are much \"better behaved\" on $[-1,1]$ than $x^k$ on a general interval $[a,b]$. The condition $|b-a|2$ is an arbitrary detail; the principle holds for almost any interval $[a,b]$, particularly those far from $0$ or not of width $2$. This preprocessing step mitigates the two main sources of ill-conditioning for the monomial basis: large magnitude of the variable and lack of centering around the origin. This always results in a reduction of the condition number of the design matrix, often by many orders of magnitude.\n\nTherefore, the statement is **Correct**.\n\n**Analysis of Statement E:**\n\"Using the orthogonal–triangular factorization (QR) before solving the least-squares problem both eliminates the ill-conditioning of $A_{\\mathrm{m}}$ and guarantees that the numerical sensitivity of the computed solution is independent of the chosen polynomial basis, so the basis choice no longer matters.\"\n\nThis statement contains two incorrect assertions.\nFirst, factorizing $A = QR$, where $Q$ has orthonormal columns and $R$ is upper triangular, does not \"eliminate\" ill-conditioning. The condition number is preserved in the $R$ factor: $\\kappa_2(A) = \\kappa_2(QR) = \\kappa_2(Q)\\kappa_2(R) = \\kappa_2(R)$ since $\\kappa_2(Q)=1$. The least-squares problem is transformed into solving the triangular system $Rc = Q^\\top y$. The sensitivity of this system is governed by $\\kappa_2(R)$, which is identical to $\\kappa_2(A)$. The advantage of QR over the normal equations is that it avoids squaring the condition number. The ill-conditioning inherent in $A$ is still present in $R$.\n\nSecond, since the numerical sensitivity depends on $\\kappa_2(A)$, and different bases (e.g., monomial vs. Legendre) produce design matrices with vastly different condition numbers, the basis choice remains critically important. Using a well-conditioned basis like Legendre polynomials leads to a small $\\kappa_2(A_{\\mathrm{L}})$ and thus a small $\\kappa_2(R_{\\mathrm{L}})$, ensuring a stable computation of the coefficient vector. Using the monomial basis leads to a large $\\kappa_2(A_{\\mathrm{m}})$ and a large $\\kappa_2(R_{\\mathrm{m}})$, making the computation of coefficients unstable. The basis choice absolutely matters.\n\nTherefore, the statement is **Incorrect**.\n\n**Analysis of Statement F:**\n\"For $N$ equispaced nodes and large $p$, the columns of $A_{\\mathrm{m}}$ become nearly linearly dependent, whereas the columns of $A_{\\mathrm{L}}$ remain exactly orthogonal in the discrete inner product without any weights, for all $p \\le N-1$.\"\n\nThe first part of the statement is correct, as explained in the analysis of statement A.\nThe second part is a claim of discrete orthogonality for Legendre polynomials on an equispaced grid. The discrete inner product without weights between the $k$-th and $\\ell$-th columns of $A_{\\mathrm{L}}$ is $\\sum_{i=1}^N \\widetilde{P}_k(x_i) \\widetilde{P}_\\ell(x_i)$. The statement claims this sum is zero for $k \\ne \\ell$. This property does not hold in general for Legendre polynomials on an equispaced grid. Discrete orthogonality is a special property that arises only for specific pairings of orthogonal polynomials and corresponding special node sets (e.g., Chebyshev polynomials on Chebyshev nodes, or as shown in statement B, Legendre polynomials on Gauss-Legendre nodes with appropriate weights). For an arbitrary grid like an equispaced one, there is no reason to expect the sample vectors of Legendre polynomials to be orthogonal.\n\nTherefore, the statement is **Incorrect**.",
            "answer": "$$\\boxed{BCD}$$"
        }
    ]
}