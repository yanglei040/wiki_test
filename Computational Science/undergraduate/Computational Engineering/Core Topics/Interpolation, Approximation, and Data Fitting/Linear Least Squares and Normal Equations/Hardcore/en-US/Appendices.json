{
    "hands_on_practices": [
        {
            "introduction": "This first practice grounds the theory of least squares in a common engineering task: data fitting. When we have a set of measurements that we believe follow a linear trend, but are subject to noise or error, the least squares method provides a robust way to find the \"best-fit\" line. This exercise  will guide you through the process of taking raw experimental data, constructing the linear system, and using the normal equations to determine the optimal model parameters.",
            "id": "2218047",
            "problem": "An engineer is calibrating a novel thermal sensor. The sensor's output voltage, $V$, is assumed to be a linear function of the ambient temperature, $T$. The relationship is modeled by the equation $V(T) = c_0 + c_1 T$, where $c_0$ and $c_1$ are the calibration constants to be determined. To find these constants, four measurements are taken in a controlled environment:\n\n*   At a temperature of $T=10$ degrees Celsius, the measured voltage is $V=2.6$ volts.\n*   At a temperature of $T=20$ degrees Celsius, the measured voltage is $V=3.4$ volts.\n*   At a temperature of $T=30$ degrees Celsius, the measured voltage is $V=4.7$ volts.\n*   At a temperature of $T=40$ degrees Celsius, the measured voltage is $V=5.4$ volts.\n\nThe parameters $c_0$ and $c_1$ are to be determined such that the sum of the squared differences between the measured voltages and the voltages predicted by the linear model is minimized. Let the resulting best-fit line be $\\hat{V}(T) = \\hat{c}_0 + \\hat{c}_1 T$.\n\nYour task is to calculate the Euclidean norm of the residual vector, where the components of the residual vector are the differences between the individually measured voltages and the corresponding voltages predicted by this best-fit line.\n\nExpress your final answer in volts, rounded to three significant figures.",
            "solution": "We model the voltage as a linear function of temperature, $V(T)=c_{0}+c_{1}T$, and determine $(\\hat{c}_{0},\\hat{c}_{1})$ by least squares using the four measurements $(T_{i},V_{i})=(10,2.6),(20,3.4),(30,4.7),(40,5.4)$. Let the design matrix be $X=\\begin{pmatrix}1 & 10 \\\\ 1 & 20 \\\\ 1 & 30 \\\\ 1 & 40\\end{pmatrix}$ and the observation vector be $\\boldsymbol{V}=\\begin{pmatrix}2.6 \\\\ 3.4 \\\\ 4.7 \\\\ 5.4\\end{pmatrix}$. The least-squares estimate satisfies\n$$\n\\begin{pmatrix}\\hat{c}_{0} \\\\ \\hat{c}_{1}\\end{pmatrix}=(X^{T}X)^{-1}X^{T}\\boldsymbol{V},\n$$\nequivalently the normal equations\n$$\n\\begin{pmatrix}n & \\sum T_{i} \\\\ \\sum T_{i} & \\sum T_{i}^{2}\\end{pmatrix}\\begin{pmatrix}\\hat{c}_{0} \\\\ \\hat{c}_{1}\\end{pmatrix}=\\begin{pmatrix}\\sum V_{i} \\\\ \\sum T_{i}V_{i}\\end{pmatrix}.\n$$\nCompute the sums: $n=4$, $\\sum T_{i}=10+20+30+40=100$, $\\sum T_{i}^{2}=10^{2}+20^{2}+30^{2}+40^{2}=3000$, $\\sum V_{i}=2.6+3.4+4.7+5.4=16.1$, and $\\sum T_{i}V_{i}=10\\cdot 2.6+20\\cdot 3.4+30\\cdot 4.7+40\\cdot 5.4=451$. Thus we solve\n$$\n\\begin{pmatrix}4 & 100 \\\\ 100 & 3000\\end{pmatrix}\\begin{pmatrix}\\hat{c}_{0} \\\\ \\hat{c}_{1}\\end{pmatrix}=\\begin{pmatrix}16.1 \\\\ 451\\end{pmatrix}.\n$$\nThe determinant is $4\\cdot 3000-100\\cdot 100=2000$, so\n$$\n\\hat{c}_{0}=\\frac{3000\\cdot 16.1-100\\cdot 451}{2000}=1.6,\\quad \\hat{c}_{1}=\\frac{-100\\cdot 16.1+4\\cdot 451}{2000}=0.097.\n$$\nHence the best-fit line is $\\hat{V}(T)=1.6+0.097\\,T$.\n\nCompute the residuals $r_{i}=V_{i}-\\hat{V}(T_{i})$ at the four temperatures:\n$$\n\\hat{V}(10)=1.6+0.097\\cdot 10=2.57,\\quad r_{1}=2.6-2.57=0.03,\n$$\n$$\n\\hat{V}(20)=1.6+0.097\\cdot 20=3.54,\\quad r_{2}=3.4-3.54=-0.14,\n$$\n$$\n\\hat{V}(30)=1.6+0.097\\cdot 30=4.51,\\quad r_{3}=4.7-4.51=0.19,\n$$\n$$\n\\hat{V}(40)=1.6+0.097\\cdot 40=5.48,\\quad r_{4}=5.4-5.48=-0.08.\n$$\nThe Euclidean norm of the residual vector $\\boldsymbol{r}$ is\n$$\n\\|\\boldsymbol{r}\\|_{2}=\\sqrt{\\sum_{i=1}^{4}r_{i}^{2}}=\\sqrt{(0.03)^{2}+(-0.14)^{2}+(0.19)^{2}+(-0.08)^{2}}=\\sqrt{0.063}.\n$$\nEvaluating the square root and rounding to three significant figures gives\n$$\n\\|\\boldsymbol{r}\\|_{2}\\approx 0.251.\n$$\nThis value is in volts because each residual is a voltage difference.",
            "answer": "$$\\boxed{0.251}$$"
        },
        {
            "introduction": "Having seen how least squares handles noisy data, it is crucial to understand its behavior in an ideal scenario. This exercise explores the special case where the system of equations $Ax=b$ has an exact solution, meaning the vector $b$ lies perfectly within the column space of $A$. By working through this problem , you will verify that the least squares method correctly identifies this exact solution, resulting in a residual norm of zero. This confirms that least squares is a general framework that encompasses both exact and approximate solutions.",
            "id": "2409652",
            "problem": "Consider the overdetermined linear system with matrix $A \\in \\mathbb{R}^{3 \\times 2}$ and vector $b \\in \\mathbb{R}^{3}$ given by\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n3 \\\\\n1 \\\\\n-2\n\\end{pmatrix}.\n$$\nLet $x \\in \\mathbb{R}^{2}$ be the vector that minimizes the squared Euclidean norm of the residual, that is, minimizes $\\|A x - b\\|_{2}^{2}$. Determine the least-squares minimizer $x_{\\mathrm{LS}} \\in \\mathbb{R}^{2}$ and compute the Euclidean norm of the residual $r = b - A x_{\\mathrm{LS}}$, namely $\\|r\\|_{2}$. Provide exact values (no rounding). For your final response, report the triple $(x_{1}, x_{2}, \\|r\\|_{2})$ in a single row as a $1 \\times 3$ matrix.",
            "solution": "The linear least-squares problem seeks to find a vector $x_{\\mathrm{LS}} \\in \\mathbb{R}^{2}$ that minimizes the objective function $J(x) = \\|Ax - b\\|_{2}^{2}$. A necessary condition for the minimum is that the gradient of $J(x)$ with respect to $x$ is zero. This condition leads to the set of linear equations known as the normal equations:\n$$ A^T A x_{\\mathrm{LS}} = A^T b $$\nFirst, we compute the matrix $A^T A$. The transpose of $A$ is:\n$$ A^T = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} $$\nThen, the product $A^T A$ is:\n$$ A^T A = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (1)(1) + (0)(0) & (1)(0) + (1)(1) + (0)(1) \\\\ (0)(1) + (1)(1) + (1)(0) & (0)(0) + (1)(1) + (1)(1) \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} $$\nNext, we compute the vector $A^T b$:\n$$ A^T b = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} (1)(3) + (1)(1) + (0)(-2) \\\\ (0)(3) + (1)(1) + (1)(-2) \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ -1 \\end{pmatrix} $$\nThe normal equations become:\n$$ \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ -1 \\end{pmatrix} $$\nThis corresponds to the system of two linear equations:\n$$ 2x_1 + x_2 = 4 $$\n$$ x_1 + 2x_2 = -1 $$\nFrom the first equation, we express $x_2$ in terms of $x_1$: $x_2 = 4 - 2x_1$. Substituting this into the second equation gives:\n$$ x_1 + 2(4 - 2x_1) = -1 $$\n$$ x_1 + 8 - 4x_1 = -1 $$\n$$ -3x_1 = -9 $$\n$$ x_1 = 3 $$\nNow, we substitute the value of $x_1$ back to find $x_2$:\n$$ x_2 = 4 - 2(3) = 4 - 6 = -2 $$\nThus, the least-squares solution is:\n$$ x_{\\mathrm{LS}} = \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix} $$\nThe next step is to compute the residual vector $r = b - A x_{\\mathrm{LS}}$. First, we calculate the product $A x_{\\mathrm{LS}}$:\n$$ A x_{\\mathrm{LS}} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} (1)(3) + (0)(-2) \\\\ (1)(3) + (1)(-2) \\\\ (0)(3) + (1)(-2) \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\\\ -2 \\end{pmatrix} $$\nThe residual vector is then:\n$$ r = b - A x_{\\mathrm{LS}} = \\begin{pmatrix} 3 \\\\ 1 \\\\ -2 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThis indicates that the vector $b$ lies in the column space of $A$, and the system $Ax = b$ had an exact solution, which the least-squares method has found. The Euclidean norm of the residual vector $r$ is:\n$$ \\|r\\|_{2} = \\sqrt{0^2 + 0^2 + 0^2} = 0 $$\nThe required triple $(x_1, x_2, \\|r\\|_{2})$ is therefore $(3, -2, 0)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 3 & -2 & 0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Theoretical formulas are one thing, but their behavior on a computer is another. This advanced computational practice delves into the critical concept of numerical stability in least squares problems. You will investigate how the solution's accuracy is affected when the columns of the matrix $A$ are nearly linearly dependent, a condition known as ill-conditioning. By implementing the solution for this problem , you will observe firsthand how small perturbations in the input data can be dramatically amplified in an ill-conditioned system, a key concern in computational engineering.",
            "id": "2409733",
            "problem": "Create a complete, runnable program that evaluates the sensitivity of the linear least-squares minimizer to perturbations in the right-hand side for a family of explicitly defined matrices with nearly collinear columns. Let the linear least-squares solution $x_{LS}$ be the minimizer of the Euclidean norm objective $\\lVert A x - b \\rVert_2$ for a given real matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^m$. Consider the following deterministic setup in $\\mathbb{R}^3$ with two columns, $m = 3$ and $n = 2$, based on the orthonormal vectors $c_1 = \\frac{1}{\\sqrt{2}}[1,1,0]^\\top$ and $w = \\frac{1}{\\sqrt{2}}[1,-1,0]^\\top$.\n\nFor each test case below, construct $A \\in \\mathbb{R}^{3 \\times 2}$, define $x_{\\text{true}} \\in \\mathbb{R}^2$, form $b = A x_{\\text{true}}$, and form a perturbation $\\Delta b$ with magnitude specified by a relative factor $r$ in the direction $w$; that is, $\\lVert \\Delta b \\rVert_2 = r \\, \\lVert b \\rVert_2$ and $\\Delta b = \\alpha \\, w$ with $\\alpha = r \\, \\lVert b \\rVert_2$. For each case, compute the least-squares solutions $x_{LS}(b)$ and $x_{LS}(b + \\Delta b)$ and then compute the amplification factor of the solution with respect to the perturbation in $b$ defined by\n$$\nR = \\frac{\\lVert x_{LS}(b + \\Delta b) - x_{LS}(b) \\rVert_2 / \\lVert x_{LS}(b) \\rVert_2}{\\lVert \\Delta b \\rVert_2 / \\lVert b \\rVert_2}.\n$$\nIf any denominator is zero in the above expression, interpret the corresponding ratio using a limiting value consistent with continuity, and in the implementation prevent division by zero using a strictly positive negligible constant.\n\nUse the following test suite, where all numbers are exact and dimensionless:\n- Test case $1$ (well-conditioned baseline): $A = [c_1,\\, w]$ (columns), $x_{\\text{true}} = [2,\\, 1]^\\top$, $r = 10^{-6}$.\n- Test case $2$ (moderate near collinearity): $A = [c_1,\\, c_1 + \\epsilon w]$ with $\\epsilon = 10^{-3}$, $x_{\\text{true}} = [2,\\, 1]^\\top$, $r = 10^{-6}$.\n- Test case $3$ (extreme near collinearity): $A = [c_1,\\, c_1 + \\epsilon w]$ with $\\epsilon = 10^{-9}$, $x_{\\text{true}} = [2,\\, 1]^\\top$, $r = 10^{-6}$.\n- Test case $4$ (exact collinearity and rank deficiency): $A = [c_1,\\, c_1]$, $x_{\\text{true}} = [2,\\, 1]^\\top$, $r = 10^{-6}$.\n\nFor each test case, the required answer is the single real number $R$ as defined above. Your program must process all four test cases in the specified order and produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, for example, $[R_1,R_2,R_3,R_4]$. No physical units are involved. Angles are not used. All numerical outputs must be real numbers (in decimal or scientific notation) on that single line only.",
            "solution": "The problem requires an analysis of the sensitivity of the linear least-squares solution to perturbations in the right-hand side vector $b$. We are asked to compute an amplification factor, $R$, for four specific test cases involving matrices with progressively more collinear columns.\n\nThe linear least-squares problem is defined as finding a vector $x_{LS} \\in \\mathbb{R}^n$ that minimizes the Euclidean norm of the residual, $\\lVert Ax - b \\rVert_2$, for a given matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^m$.\n\nThe solution to this problem can be formally expressed using the Moore-Penrose pseudoinverse of $A$, denoted $A^\\dagger$, as $x_{LS} = A^\\dagger b$. When the matrix $A$ has full column rank (i.e., $\\text{rank}(A) = n$), the pseudoinverse is given by $A^\\dagger = (A^\\top A)^{-1} A^\\top$, and the solution $x_{LS}$ is unique and can be found by solving the system of normal equations:\n$$\nA^\\top A x = A^\\top b\n$$\nIf $A$ is rank-deficient, the least-squares problem still has a solution, but it is not unique. In this situation, a unique solution is conventionally selected by requiring it to have the minimum Euclidean norm among all minimizers. The `numpy.linalg.lstsq` function, as is standard, returns this minimum-norm solution, which is also given by $x_{LS} = A^\\dagger b$.\n\nThe sensitivity of the solution $x_{LS}$ to a perturbation $\\Delta b$ in $b$ is central to this problem. Let the perturbed vector be $b' = b + \\Delta b$. The new solution is $x_{LS}(b + \\Delta b) = A^\\dagger (b + \\Delta b)$. The change in the solution, $\\Delta x$, is therefore:\n$$\n\\Delta x = x_{LS}(b + \\Delta b) - x_{LS}(b) = A^\\dagger (b + \\Delta b) - A^\\dagger b = A^\\dagger \\Delta b\n$$\nThe problem defines the amplification factor $R$ as the ratio of the relative change in the solution to the relative change in the right-hand side vector:\n$$\nR = \\frac{\\lVert \\Delta x \\rVert_2 / \\lVert x_{LS}(b) \\rVert_2}{\\lVert \\Delta b \\rVert_2 / \\lVert b \\rVert_2} = \\frac{\\lVert A^\\dagger \\Delta b \\rVert_2 / \\lVert A^\\dagger b \\rVert_2}{\\lVert \\Delta b \\rVert_2 / \\lVert b \\rVert_2}\n$$\nIn this problem, the relative perturbation in $b$ is fixed: $\\lVert \\Delta b \\rVert_2 / \\lVert b \\rVert_2 = r = 10^{-6}$. Thus, the formula simplifies to:\n$$\nR = \\frac{\\lVert x_{LS}(b + \\Delta b) - x_{LS}(b) \\rVert_2 / \\lVert x_{LS}(b) \\rVert_2}{r}\n$$\nThe procedure for each test case is to construct the matrix $A$, the true solution $x_{\\text{true}}$, the vector $b = A x_{\\text{true}}$, and the perturbation $\\Delta b$. Then we compute the least-squares solutions for $b$ and $b+\\Delta b$ to find $R$. The base vectors are the orthonormal vectors $c_1 = \\frac{1}{\\sqrt{2}}[1,1,0]^\\top$ and $w = \\frac{1}{\\sqrt{2}}[1,-1,0]^\\top$.\n\nCase 1: $A = [c_1, w]$, $x_{\\text{true}} = [2, 1]^\\top$, $r = 10^{-6}$.\nThe matrix $A$ has orthonormal columns. Thus, $A^\\top A = I$, where $I$ is the $2 \\times 2$ identity matrix. The condition number of $A$ is $\\kappa_2(A) = 1$. The system is perfectly well-conditioned.\n$b = A x_{\\text{true}} = 2c_1 + 1w$. Since $b$ is constructed to be in the column space of $A$ and $A$ has full rank, $x_{LS}(b) = x_{\\text{true}}$.\n$\\lVert x_{LS}(b) \\rVert_2 = \\lVert x_{\\text{true}} \\rVert_2 = \\sqrt{2^2 + 1^2} = \\sqrt{5}$.\nThe perturbation is $\\Delta b = r \\lVert b \\rVert_2 w$.\nThe change in solution is $\\Delta x = A^\\dagger \\Delta b = (A^\\top A)^{-1} A^\\top \\Delta b = A^\\top \\Delta b$.\n$\\Delta x = A^\\top (r \\lVert b \\rVert_2 w) = r \\lVert b \\rVert_2 A^\\top w = r \\lVert b \\rVert_2 \\begin{pmatrix} c_1^\\top w \\\\ w^\\top w \\end{pmatrix} = r \\lVert b \\rVert_2 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n$\\lVert \\Delta x \\rVert_2 = r \\lVert b \\rVert_2$.\n$\\lVert b \\rVert_2 = \\lVert 2c_1 + w \\rVert_2 = \\sqrt{4\\lVert c_1 \\rVert_2^2 + \\lVert w \\rVert_2^2} = \\sqrt{4+1} = \\sqrt{5}$.\nSo, $\\lVert \\Delta x \\rVert_2 = r\\sqrt{5}$.\nThe amplification factor is $R = \\frac{\\lVert \\Delta x \\rVert_2 / \\lVert x_{LS}(b) \\rVert_2}{r} = \\frac{(r\\sqrt{5}) / \\sqrt{5}}{r} = 1$.\n\nCase 2 and 3: $A = [c_1, c_1 + \\epsilon w]$ with $\\epsilon = 10^{-3}$ and $\\epsilon = 10^{-9}$.\nThe columns of $A$ are nearly collinear as $\\epsilon \\to 0$. The matrix $A$ becomes increasingly ill-conditioned.\nFrom analysis, $x_{LS}(b) = x_{\\text{true}}$.\nThe change in solution is $\\Delta x = \\frac{r}{\\epsilon}\\sqrt{9+\\epsilon^2} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\nThe norm is $\\lVert \\Delta x \\rVert_2 = \\frac{r\\sqrt{2}}{\\epsilon}\\sqrt{9+\\epsilon^2}$.\nThen $R = \\frac{\\lVert \\Delta x \\rVert_2 / \\lVert x_{LS}(b) \\rVert_2}{r} = \\frac{(\\frac{r\\sqrt{2}}{\\epsilon}\\sqrt{9+\\epsilon^2}) / \\sqrt{5}}{r} = \\frac{\\sqrt{2}\\sqrt{9+\\epsilon^2}}{\\epsilon\\sqrt{5}}$.\nAs $\\epsilon \\to 0$, $R$ grows proportionally to $1/\\epsilon$. For small $\\epsilon$, $R \\approx \\frac{3\\sqrt{2}}{\\epsilon\\sqrt{5}}$. This demonstrates extreme sensitivity to perturbations for ill-conditioned systems.\n\nCase 4: $A = [c_1, c_1]$.\nThe matrix $A$ is rank-deficient with $\\text{rank}(A)=1$. The columns are exactly collinear ($\\epsilon = 0$).\n$b = A x_{\\text{true}} = 2c_1 + 1c_1 = 3c_1$. $b$ is in the column space of $A$.\nThe set of exact solutions to $Ax=b$ is the line $x_1+x_2 = 3$. The minimum-norm solution is the point on this line closest to the origin, which is $x_{LS}(b) = [1.5, 1.5]^\\top$.\nThe perturbation is $\\Delta b = r \\lVert b \\rVert_2 w = 3rw$. This perturbation vector $w$ is orthogonal to the column space of $A$, which is $\\text{span}\\{c_1\\}$.\nThe least-squares solution projects the right-hand side onto the column space of $A$.\n$P_A(b+\\Delta b) = P_A(b) + P_A(\\Delta b) = b + 0 = b$.\nThe problem of finding $x_{LS}(b+\\Delta b)$ is therefore equivalent to finding $x_{LS}(b)$, because the perturbation is projected out.\nThus, $x_{LS}(b+\\Delta b) = x_{LS}(b) = [1.5, 1.5]^\\top$.\nThis means $\\Delta x = 0$, and consequently $\\lVert \\Delta x \\rVert_2 = 0$.\nThe amplification factor is $R = \\frac{0 / \\lVert x_{LS}(b) \\rVert_2}{r} = 0$.\nThe value $R=0$ shows a discontinuity from the limit of Case 2/3 as $\\epsilon \\to 0$. This occurs because for any $\\epsilon > 0$, the column space of $A$ is $\\text{span}\\{c_1, w\\}$, and the perturbation $\\Delta b \\propto w$ lies within it. At $\\epsilon = 0$, the column space collapses to $\\text{span}\\{c_1\\}$, and the same perturbation $\\Delta b$ becomes orthogonal to it. The behavior of the pseudoinverse solution is fundamentally different for perturbations inside versus outside the column space. The problem statement is valid and this result is correct.\n\nThe implementation will compute these values numerically.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Evaluates the sensitivity of the linear least-squares minimizer to perturbations\n    for a family of matrices with increasingly collinear columns.\n    \"\"\"\n    \n    # Define the orthonormal basis vectors\n    c1 = (1 / np.sqrt(2)) * np.array([1.0, 1.0, 0.0])\n    w = (1 / np.sqrt(2)) * np.array([1.0, -1.0, 0.0])\n    \n    # Define x_true and r as specified in the problem\n    x_true = np.array([2.0, 1.0])\n    r = 1e-6\n    \n    # Test cases parameters: (name, epsilon)\n    # Epsilon defines the matrix A. A special value of None is used for the first case.\n    test_params = [\n        (\"well-conditioned baseline\", None),\n        (\"moderate near collinearity\", 1e-3),\n        (\"extreme near collinearity\", 1e-9),\n        (\"exact collinearity\", 0.0)\n    ]\n    \n    results = []\n\n    # Small constant to prevent division by zero, as per problem instructions\n    ZERO_GUARD = 1e-15\n\n    for name, epsilon in test_params:\n        # Step 1: Construct matrix A\n        if epsilon is None:\n            # Case 1: A = [c1, w]\n            A = np.c_[c1, w]\n        else:\n            # Cases 2, 3, 4: A = [c1, c1 + epsilon * w]\n            col2 = c1 + epsilon * w\n            A = np.c_[c1, col2]\n            \n        # Step 2: Form b and the perturbation delta_b\n        b = A @ x_true\n        \n        norm_b = np.linalg.norm(b)\n        safe_norm_b = norm_b if norm_b > ZERO_GUARD else ZERO_GUARD\n        \n        # Perturbation is in the direction of w\n        delta_b = r * norm_b * w\n        \n        b_perturbed = b + delta_b\n\n        # Step 3: Compute the least-squares solutions\n        # Use rcond=None to adopt the new default behavior of numpy.linalg.lstsq\n        x_ls_b = np.linalg.lstsq(A, b, rcond=None)[0]\n        x_ls_b_perturbed = np.linalg.lstsq(A, b_perturbed, rcond=None)[0]\n        \n        # Step 4: Compute the amplification factor R\n        delta_x = x_ls_b_perturbed - x_ls_b\n        \n        norm_delta_x = np.linalg.norm(delta_x)\n        norm_x_ls_b = np.linalg.norm(x_ls_b)\n        \n        safe_norm_x_ls_b = norm_x_ls_b if norm_x_ls_b > ZERO_GUARD else ZERO_GUARD\n\n        # The relative error in x\n        rel_err_x = norm_delta_x / safe_norm_x_ls_b\n        \n        # The relative error in b is, by definition, r.\n        # R = rel_err_x / (||delta_b|| / ||b||) = rel_err_x / r\n        # We must handle the case where r is zero, though problem states r=10^-6\n        safe_r = r if r > ZERO_GUARD else ZERO_GUARD\n        \n        R = rel_err_x / safe_r\n        \n        results.append(R)\n\n    # Final print statement in the exact required format.\n    # The output format is a list of strings representing the numbers.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}