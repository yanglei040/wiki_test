{
    "hands_on_practices": [
        {
            "introduction": "Many fundamental relationships in science and engineering, such as reaction rates in chemistry, are inherently non-linear. This practice explores a powerful technique to handle such cases: model linearization. By applying a logarithmic transformation to the Arrhenius equation, you will convert a non-linear model into a linear one, making it suitable for analysis with standard Ordinary Least Squares (OLS) regression . This exercise provides hands-on experience in estimating key physical parameters, like activation energy, by fitting transformed experimental data.",
            "id": "2383201",
            "problem": "You are tasked with building a complete, runnable program to estimate activation energies from temperature-dependent reaction rate data by fitting a thermally activated rate model that is widely used in chemical kinetics. Start from the following well-tested model for temperature dependence: a thermally activated process has a rate constant $k(T)$ that obeys the Arrhenius law, which states that there exists a pre-exponential factor $A$ and an activation energy $E_a$ such that $k(T)$ depends on absolute temperature $T$ via an exponential temperature dependence. Your job is to construct and justify an estimation procedure that uses a transformation into a linear regression model based on first principles of Maximum Likelihood Estimation (MLE) under an appropriate noise model, and then implement it to estimate $E_a$ from provided data.\n\nFundamental base and task: You must begin from the model of thermally activated reaction rates and the basic definitions of linear regression and least squares estimation. Use the universal gas constant $R$ with the value $R = 8.314462618$ in units $\\mathrm{J \\cdot mol^{-1} \\cdot K^{-1}}$. Assume the data consist of pairs $(T_i, k_i)$ with $T_i$ in $\\mathrm{K}$ and $k_i$ in $\\mathrm{s^{-1}}$, and that an appropriate transformation yields a linear model suited for Ordinary Least Squares (OLS). From that base, derive a principled estimator for the activation energy $E_a$ and the pre-exponential factor $A$, and explain why the estimator is appropriate. Then implement the estimator to compute $E_a$ for several test cases.\n\nRequirements:\n- Derive the estimation method from first principles, starting from the thermally activated rate model and the definition of a linear regression model combined with a justified noise assumption.\n- Implement the estimation method as a program that takes the fixed test datasets provided below (hard-coded in your program), fits the model, and outputs the estimated activation energies.\n\nData and test suite:\nUse the following four datasets (each dataset is a list of temperatures and a corresponding list of rate constants). All temperatures are in $\\mathrm{K}$ and all rate constants are in $\\mathrm{s^{-1}}$. Your program must compute one activation energy estimate for each dataset independently. The datasets are:\n- Case $1$ (general multitemperature dataset):\n  - Temperatures $T$: $[290.0,310.0,330.0,350.0,370.0,390.0]$\n  - Rates $k$: $[0.0201,0.0954,0.38,1.49,5.37,17.9]$\n- Case $2$ (boundary case with the minimum number of points for a line fit):\n  - Temperatures $T$: $[300.0,360.0]$\n  - Rates $k$: $[0.10,3.20]$\n- Case $3$ (narrow temperature span, potential conditioning challenge):\n  - Temperatures $T$: $[345.0,350.0,355.0,360.0]$\n  - Rates $k$: $[2.10,2.95,4.10,5.60]$\n- Case $4$ (presence of an outlier in the highest temperature data point):\n  - Temperatures $T$: $[310.0,330.0,350.0,370.0,390.0]$\n  - Rates $k$: $[0.25,0.90,4.10,15.0,12.0]$\n\nOutput specification:\n- For each dataset, compute the activation energy estimate $\\widehat{E}_a$ in units $\\mathrm{J \\cdot mol^{-1}}$.\n- Round each $\\widehat{E}_a$ to the nearest integer (use standard rounding to the nearest integer).\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, in the order Case $1$, Case $2$, Case $3$, Case $4$. The output must be exactly one line of the form $[\\widehat{E}_{a,1},\\widehat{E}_{a,2},\\widehat{E}_{a,3},\\widehat{E}_{a,4}]$ and nothing else.\n\nAngle units are not applicable. No percentages are involved. All physical quantities are to be expressed in the stated units. Ensure that your implementation explicitly uses $R = 8.314462618$ with units $\\mathrm{J \\cdot mol^{-1} \\cdot K^{-1}}$ and temperatures in $\\mathrm{K}$.\n\nYour program must be self-contained, require no user input, and adhere strictly to the output format specified above. The final answers for the four cases must be numeric values as specified (integers after rounding). The program should be written to be runnable in any modern programming environment and must be deterministic given the fixed datasets above. The single line of output must aggregate the results for all four test cases in a single list, for example $[r_1,r_2,r_3,r_4]$.",
            "solution": "The problem presented is a standard and valid exercise in computational engineering and chemical kinetics. It requires the estimation of the activation energy, $E_a$, from temperature-dependent rate constant data, $k(T)$. The foundation of this task is the Arrhenius equation, a cornerstone of physical chemistry. The problem is well-posed, scientifically grounded, and provides all necessary data and constants for a unique solution. We shall proceed with a rigorous derivation of the estimation method, followed by its implementation.\n\nThe temperature dependence of the rate constant, $k(T)$, for a thermally activated process is described by the Arrhenius law:\n$$k(T) = A e^{-E_a / (RT)}$$\nHere, $A$ is the pre-exponential factor, $E_a$ is the activation energy, $R$ is the universal gas constant, and $T$ is the absolute temperature. This model is non-linear with respect to its parameters $A$ and $E_a$. To apply linear regression techniques, we must first transform the equation into a linear form. This is achieved by taking the natural logarithm of both sides:\n$$\\ln(k(T)) = \\ln\\left(A e^{-E_a / (RT)}\\right)$$\nUsing the properties of logarithms, this simplifies to:\n$$\\ln(k(T)) = \\ln(A) - \\frac{E_a}{R} \\frac{1}{T}$$\nThis equation is now in the form of a linear equation, $y = c + m x$, where we can make the following identifications:\n- The dependent variable is $y_i = \\ln(k_i)$.\n- The independent variable is $x_i = 1/T_i$.\n- The intercept is $c = \\ln(A)$.\n- The slope is $m = -E_a/R$.\n\nWe are given $n$ data pairs $(T_i, k_i)$. In the transformed space, our model for each data point is:\n$$y_i = c + m x_i + \\epsilon_i$$\nwhere $\\epsilon_i$ represents the noise or error associated with the $i$-th measurement. To derive a principled estimator, we must make a justifiable assumption about the statistical properties of this error. We will use the principle of Maximum Likelihood Estimation (MLE). A common and physically sensible assumption is that the errors $\\epsilon_i$ in the linearized model are independent and identically distributed (i.i.d.) normal random variables with zero mean and constant variance $\\sigma^2$, denoted as $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. This assumption is reasonable because experimental uncertainty in the measurement of the rate constant $k_i$ is often proportional to its value. The logarithmic transformation stabilizes this variance, making the assumption of homoscedasticity (constant variance) for $\\epsilon_i$ in the transformed domain plausible.\n\nUnder this Gaussian noise assumption, the likelihood of observing a single data point $y_i$ is given by the normal probability density function:\n$$p(y_i | x_i, c, m, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - (c + mx_i))^2}{2\\sigma^2}\\right)$$\nFor the entire dataset of $n$ independent points, the total likelihood function $L$ is the product of the individual probabilities:\n$$L(c, m, \\sigma^2 | \\mathbf{y}) = \\prod_{i=1}^n p(y_i | x_i, c, m, \\sigma^2) = \\left(2\\pi\\sigma^2\\right)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - c - mx_i)^2\\right)$$\nTo find the parameters $c$ and $m$ that maximize this likelihood, we can equivalently maximize the log-likelihood, $\\ln L$:\n$$\\ln L(c, m, \\sigma^2 | \\mathbf{y}) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - c - mx_i)^2$$\nMaximizing this expression with respect to $c$ and $m$ is equivalent to minimizing the sum of squared residuals (RSS):\n$$\\text{RSS}(c, m) = \\sum_{i=1}^n (y_i - c - mx_i)^2$$\nThis minimization problem is the definition of Ordinary Least Squares (OLS). Thus, applying OLS to the linearized data provides the Maximum Likelihood Estimates for the parameters $c$ and $m$.\n\nThe OLS problem can be elegantly expressed in matrix form. We define the observation vector $\\mathbf{y}$, the design matrix $\\mathbf{X}$, and the parameter vector $\\boldsymbol{\\beta}$ as follows:\n$$\\mathbf{y} = \\begin{pmatrix} \\ln(k_1) \\\\ \\ln(k_2) \\\\ \\vdots \\\\ \\ln(k_n) \\end{pmatrix}, \\quad \\mathbf{X} = \\begin{pmatrix} 1/T_1  1 \\\\ 1/T_2  1 \\\\ \\vdots  \\vdots \\\\ 1/T_n  1 \\end{pmatrix}, \\quad \\boldsymbol{\\beta} = \\begin{pmatrix} m \\\\ c \\end{pmatrix} = \\begin{pmatrix} -E_a/R \\\\ \\ln(A) \\end{pmatrix}$$\nThe linear model is then $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$. The OLS estimate $\\hat{\\boldsymbol{\\beta}}$ that minimizes the squared Euclidean norm of the residual vector, $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2$, is the solution to the normal equations:\n$$(\\mathbf{X}^T \\mathbf{X}) \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T \\mathbf{y}$$\nThe solution is given by $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$. This is computed numerically using a standard linear least-squares algorithm. From the estimated parameter vector $\\hat{\\boldsymbol{\\beta}} = [\\hat{m}, \\hat{c}]^T$, we extract the slope $\\hat{m}$. The estimate for the activation energy, $\\widehat{E}_a$, is then directly calculated:\n$$\\widehat{E}_a = -\\hat{m} R$$\nWe will use the provided value of the universal gas constant, $R = 8.314462618 \\mathrm{J \\cdot mol^{-1} \\cdot K^{-1}}$.\n\nThe implementation will process each of the four datasets by:\n1. Transforming the raw data $(T_i, k_i)$ into the linear coordinates $(x_i, y_i) = (1/T_i, \\ln(k_i))$.\n2. Constructing the design matrix $\\mathbf{X}$ and observation vector $\\mathbf{y}$.\n3. Solving the linear least-squares problem $\\mathbf{y} \\approx \\mathbf{X}\\boldsymbol{\\beta}$ to obtain the estimated slope $\\hat{m}$.\n4. Calculating $\\widehat{E}_a = -\\hat{m} R$.\n5. Rounding the result for $\\widehat{E}_a$ to the nearest integer, in units of $\\mathrm{J \\cdot mol^{-1}}$.\nThe results for all four cases will be aggregated into the specified output format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# The problem statement allows for scipy, but numpy.linalg.lstsq provides a more\n# fundamental implementation of the derived least-squares solution, which aligns\n# with the first-principles derivation.\n\ndef solve():\n    \"\"\"\n    Estimates activation energy from temperature-dependent rate data\n    using a linearized Arrhenius model and Ordinary Least Squares.\n    \"\"\"\n    # Define the universal gas constant as specified.\n    # R has units of J * mol^{-1} * K^{-1}.\n    R = 8.314462618\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple of (Temperatures in K, Rate constants in s^{-1}).\n    test_cases = [\n        # Case 1 (general multitemperature dataset)\n        (np.array([290.0, 310.0, 330.0, 350.0, 370.0, 390.0]),\n         np.array([0.0201, 0.0954, 0.38, 1.49, 5.37, 17.9])),\n\n        # Case 2 (boundary case with the minimum number of points)\n        (np.array([300.0, 360.0]),\n         np.array([0.10, 3.20])),\n\n        # Case 3 (narrow temperature span)\n        (np.array([345.0, 350.0, 355.0, 360.0]),\n         np.array([2.10, 2.95, 4.10, 5.60])),\n\n        # Case 4 (presence of an outlier)\n        (np.array([310.0, 330.0, 350.0, 370.0, 390.0]),\n         np.array([0.25, 0.90, 4.10, 15.0, 12.0]))\n    ]\n\n    estimated_energies = []\n    \n    for T_data, k_data in test_cases:\n        # Step 1: Linearize the data according to the Arrhenius equation.\n        # y = ln(k)\n        # x = 1/T\n        y = np.log(k_data)\n        x = 1.0 / T_data\n\n        # Step 2: Set up the linear least-squares problem y = X * beta.\n        # The model is y = m*x + c, so beta = [m, c]^T.\n        # The design matrix X has columns for x and a constant for the intercept.\n        X = np.vstack([x, np.ones(len(x))]).T\n\n        # Step 3: Solve for the parameters [slope, intercept] using OLS.\n        # numpy.linalg.lstsq solves the equation y = X * beta for beta.\n        # The first element of the returned solution vector is the slope, m.\n        # rcond=None is specified to use the machine-precision default.\n        slope, intercept = np.linalg.lstsq(X, y, rcond=None)[0]\n\n        # Step 4: Calculate the activation energy from the slope.\n        # The slope m = -E_a / R.\n        # Therefore, E_a = -slope * R.\n        # The result E_a will be in J * mol^{-1}.\n        activation_energy = -slope * R\n\n        # Step 5: Round the result to the nearest integer as required.\n        rounded_E_a = int(round(activation_energy))\n        estimated_energies.append(rounded_E_a)\n\n    # Final print statement in the exact required format.\n    # The output is a comma-separated list of integers enclosed in brackets.\n    print(f\"[{','.join(map(str, estimated_energies))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While Ordinary Least Squares (OLS) is a cornerstone of regression, its reliance on minimizing squared errors makes it extremely sensitive to outliers. This hands-on exercise demonstrates the concept of robust regression, a critical tool for analyzing real-world data that is often imperfect. You will implement and compare the classic OLS estimator against a robust alternative based on the Huber loss function, which cleverly combines quadratic and linear penalties to limit the influence of anomalous data points . This practice provides a stark and memorable illustration of why choosing the right loss function is crucial for building reliable models.",
            "id": "2383160",
            "problem": "You are asked to implement and compare two estimators for a simple linear model with one predictor and an intercept: the Ordinary Least Squares (OLS) estimator and the Huber-loss-based robust estimator. The model is\n$$\ny_i = a\\,x_i + b + \\varepsilon_i,\\quad i=1,\\dots,N,\n$$\nwhere $a$ is the slope, $b$ is the intercept, and $\\varepsilon_i$ are residuals. For OLS, the parameter estimate $(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}})$ is defined as the minimizer of the sum of squared residuals\n$$\n(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}}) \\in \\arg\\min_{a,b} \\sum_{i=1}^{N} \\left(y_i - a\\,x_i - b\\right)^2.\n$$\nFor the robust estimator, define the Huber loss with parameter $\\delta0$ by\n$$\n\\phi_\\delta(r) = \\begin{cases}\n\\dfrac{1}{2} r^2,  \\text{if } |r|\\le \\delta, \\\\\n\\delta\\left(|r| - \\dfrac{1}{2}\\delta\\right),  \\text{if } |r|  \\delta,\n\\end{cases}\n$$\nand set\n$$\n(\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}}) \\in \\arg\\min_{a,b} \\sum_{i=1}^{N} \\phi_\\delta\\!\\left(y_i - a\\,x_i - b\\right).\n$$\n\nImplement a program that constructs the following three deterministic test cases and computes $(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}})$ and $(\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}})$ for each. Use the same Huber parameter $\\delta$ for all tests, namely $\\delta = 0.1$.\n\nTest suite (each test specifies $x$, $y$, and $N$ explicitly):\n\n- Test $1$ (happy path, no outlier):\n  - $x_i = i$ for $i \\in \\{-5,-4,\\dots,4,5\\}$, hence $N=11$.\n  - $y_i = 2\\,x_i + 1$ for every $i$.\n\n- Test $2$ (single high-leverage outlier):\n  - Start from Test $1$ and append one additional point $(x_o, y_o) = (50, -100)$, hence $N=12$.\n  - Thus, $x$ is the sequence from Test $1$ with an appended $50$, and $y$ is the corresponding sequence with an appended $-100$.\n\n- Test $3$ (small-sample boundary case):\n  - $x = [0, 1]$, hence $N=2$.\n  - $y = [1, 3]$.\n\nYour program must, for each test case, compute both $(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}})$ and $(\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}})$ and report them as real numbers. There are no physical units or angles involved.\n\nFinal output format: Your program should produce a single line of output containing an outer list with three inner lists, one per test case, in the fixed order\n$$\n[\\,[\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}}, \\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}}]_{\\text{Test }1},\\; [\\cdot]_{\\text{Test }2},\\; [\\cdot]_{\\text{Test }3}\\,],\n$$\nprinted as a comma-separated list enclosed in square brackets and with each numerical entry rounded to six digits after the decimal point. For example, the outer list contains three inner lists, and each inner list contains four real numbers in the order specified.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique solution. The task is a standard exercise in computational statistics, comparing the classical Ordinary Least Squares (OLS) estimator with a modern robust alternative based on the Huber loss function.\n\nWe are to find parameter estimates $(\\hat a, \\hat b)$ for the linear model $y_i = a x_i + b + \\varepsilon_i$.\n\n**Ordinary Least Squares (OLS) Estimator**\n\nThe OLS estimator minimizes the sum of squared residuals, a loss function defined as $L_{\\mathrm{OLS}}(a, b) = \\sum_{i=1}^{N} (y_i - a\\,x_i - b)^2$. This is a quadratic, convex, and smooth function of the parameters $(a, b)$. The unique minimum is found by setting the partial derivatives with respect to $a$ and $b$ to zero. This yields the normal equations:\n$$\n\\begin{pmatrix}\n\\sum_{i=1}^{N} x_i^2  \\sum_{i=1}^{N} x_i \\\\\n\\sum_{i=1}^{N} x_i  N\n\\end{pmatrix}\n\\begin{pmatrix}\n\\hat a_{\\mathrm{OLS}} \\\\ \\hat b_{\\mathrm{OLS}}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sum_{i=1}^{N} x_i y_i \\\\\n\\sum_{i=1}^{N} y_i\n\\end{pmatrix}\n$$\nSolving this $2 \\times 2$ linear system provides the closed-form analytical solutions for the parameters:\n$$\n\\hat a_{\\mathrm{OLS}} = \\frac{N \\sum x_i y_i - (\\sum x_i)(\\sum y_i)}{N \\sum x_i^2 - (\\sum x_i)^2}\n$$\n$$\n\\hat b_{\\mathrm{OLS}} = \\bar{y} - \\hat a_{\\mathrm{OLS}} \\bar{x}\n$$\nwhere $\\bar{x} = \\frac{1}{N}\\sum x_i$ and $\\bar{y} = \\frac{1}{N}\\sum y_i$. These formulas will be implemented directly for computation.\n\n**Huber-Loss-Based Robust Estimator**\n\nThe robust estimator minimizes the sum of Huber losses, $L_{\\mathrm{Huber}}(a, b) = \\sum_{i=1}^{N} \\phi_\\delta\\!\\left(y_i - a\\,x_i - b\\right)$, with the Huber loss function $\\phi_\\delta(r)$ defined as:\n$$\n\\phi_\\delta(r) = \\begin{cases}\n\\dfrac{1}{2} r^2,  \\text{if } |r|\\le \\delta, \\\\\n\\delta\\left(|r| - \\dfrac{1}{2}\\delta\\right),  \\text{if } |r|  \\delta.\n\\end{cases}\n$$\nThis loss function behaves quadratically for small residuals (like OLS) and linearly for large residuals. This property makes the estimator robust to outliers, as large errors do not contribute quadratically to the total loss. The function $L_{\\mathrm{Huber}}(a, b)$ is convex but not continuously differentiable (it has \"kinks\" where $|y_i - a x_i - b| = \\delta$). Therefore, a closed-form solution like that for OLS does not exist.\n\nThis is a convex optimization problem, which can be solved using numerical methods. We will utilize a general-purpose numerical optimization routine, specifically `scipy.optimize.minimize`, to find the parameters $(\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}})$ that minimize the total Huber loss. The OLS estimates will serve as a suitable initial guess for the iterative solver. The parameter $\\delta$ is given as $0.1$.\n\n**Implementation Across Test Cases**\n\nFor each test case, we will first construct the data vectors $x$ and $y$. Then, we will compute the OLS estimates using the analytical formulas. Subsequently, we will define the Huber loss objective function and use `scipy.optimize.minimize` to find the Huber estimates.\n\n- **Test 1:** The data points lie perfectly on the line $y_i = 2x_i + 1$. The residuals for the exact parameters $(a,b) = (2,1)$ are all zero. Both the OLS and Huber loss functions achieve their global minimum of zero at this point. Thus, we expect $(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}}) = (\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}}) = (2, 1)$.\n\n- **Test 2:** An outlier at $(x_o, y_o) = (50, -100)$ is added to the data from Test $1$. This outlier has high leverage (its $x$-value is far from the mean of other $x$-values) and a large residual. OLS is known to be highly sensitive to such points; the quadratic penalty on the large residual will \"pull\" the regression line significantly toward the outlier, drastically altering the estimates from $(2, 1)$. In contrast, the Huber estimator's linear penalty for large residuals will limit the outlier's influence. We expect $(\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}})$ to remain much closer to the true underlying parameters $(2, 1)$ of the other $11$ data points, demonstrating the principle of robust regression.\n\n- **Test 3:** With only two data points, $(0, 1)$ and $(1, 3)$, there exists a unique line that passes through both. The equation of this line is $y = 2x + 1$. For the parameters $(a,b) = (2,1)$, the residuals are zero. As in Test $1$, both OLS and Huber estimators will identify these exact parameters, yielding $(\\hat a_{\\mathrm{OLS}}, \\hat b_{\\mathrm{OLS}}) = (\\hat a_{\\mathrm{Huber}}, \\hat b_{\\mathrm{Huber}}) = (2, 1)$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Implements and compares OLS and Huber estimators for linear regression\n    on three specified test cases.\n    \"\"\"\n\n    # --- Estimator Implementations ---\n\n    def calculate_ols(x: np.ndarray, y: np.ndarray) - tuple[float, float]:\n        \"\"\"\n        Calculates the Ordinary Least Squares (OLS) estimator for a and b.\n        y = a*x + b\n        \"\"\"\n        N = len(x)\n        if N  2:\n            return (np.nan, np.nan)\n\n        sum_x = np.sum(x)\n        sum_y = np.sum(y)\n        sum_xy = np.sum(x * y)\n        sum_x2 = np.sum(x**2)\n\n        denominator = N * sum_x2 - sum_x**2\n        if np.abs(denominator)  1e-12: # Check for collinearity (all x values are the same)\n            return (np.nan, np.nan)\n\n        a_ols = (N * sum_xy - sum_x * sum_y) / denominator\n        b_ols = (sum_y - a_ols * sum_x) / N\n        \n        return a_ols, b_ols\n\n    def calculate_huber(x: np.ndarray, y: np.ndarray, delta: float) - tuple[float, float]:\n        \"\"\"\n        Calculates the Huber-loss-based robust estimator for a and b.\n        y = a*x + b\n        \"\"\"\n        \n        def huber_loss_objective(params: np.ndarray, x_data: np.ndarray, y_data: np.ndarray, d: float) - float:\n            \"\"\"\n            Objective function: sum of Huber losses for a given set of parameters.\n            \"\"\"\n            a, b = params\n            residuals = y_data - (a * x_data + b)\n            abs_residuals = np.abs(residuals)\n            \n            # Quadratic part for small residuals\n            quadratic_loss = 0.5 * residuals[abs_residuals = d]**2\n            \n            # Linear part for large residuals\n            linear_loss = d * (abs_residuals[abs_residuals  d] - 0.5 * d)\n            \n            return np.sum(quadratic_loss) + np.sum(linear_loss)\n\n        # Use OLS estimates as a good initial guess\n        a_ols, b_ols = calculate_ols(x, y)\n        initial_guess = np.array([a_ols, b_ols])\n\n        result = minimize(\n            huber_loss_objective,\n            x0=initial_guess,\n            args=(x, y, delta),\n            method='BFGS' # A standard quasi-Newton method suitable for this problem\n        )\n\n        a_huber, b_huber = result.x\n        return a_huber, b_huber\n\n    # --- Test Cases Definition ---\n    \n    # Test 1: Happy path, no outlier\n    x1 = np.arange(-5, 6, 1, dtype=float)\n    y1 = 2 * x1 + 1\n    \n    # Test 2: Single high-leverage outlier\n    x2 = np.append(x1, 50.0)\n    y2 = np.append(y1, -100.0)\n    \n    # Test 3: Small-sample boundary case\n    x3 = np.array([0.0, 1.0])\n    y3 = np.array([1.0, 3.0])\n    \n    test_cases = [\n        (x1, y1),\n        (x2, y2),\n        (x3, y3)\n    ]\n    \n    delta = 0.1\n    all_results = []\n    \n    for x_data, y_data in test_cases:\n        a_ols, b_ols = calculate_ols(x_data, y_data)\n        a_huber, b_huber = calculate_huber(x_data, y_data, delta)\n        \n        case_results = [a_ols, b_ols, a_huber, b_huber]\n        all_results.append(case_results)\n        \n    # --- Format and Print Output ---\n    \n    outer_list_str = []\n    for inner_list in all_results:\n        inner_list_str = [f\"{val:.6f}\" for val in inner_list]\n        outer_list_str.append(f\"[{','.join(inner_list_str)}]\")\n        \n    final_output = f\"[{','.join(outer_list_str)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "In modern engineering applications, we often seek models that are not only accurate but also simple and interpretable, a property known as sparsity. This advanced practice explores how non-convex regularizers, such as the $L_{0.5}$ quasi-norm, can be used to achieve greater sparsity than standard methods like LASSO ($L_1$ regularization). You will confront the challenge of non-convex optimization by deriving and implementing a powerful Majorization-Minimization algorithm, resulting in an Iteratively Reweighted Least Squares (IRLS) solver . This exercise provides a deep dive into the optimization machinery behind cutting-edge sparse regression techniques.",
            "id": "2383204",
            "problem": "You are asked to implement and analyze a regression model with a non-convex quasi-norm regularizer within a principled optimization framework. The goal is to fit a linear model to data while penalizing the coefficients using the quasi-norm corresponding to the exponent $0.5$, often informally referred to as the $L_{0.5}$ norm, which induces stronger sparsity than the standard $L_{1}$ norm. The design must begin from core definitions of least-squares data fitting and proceed using fundamental inequalities and concavity properties to produce a convergent, implementable numerical scheme that can be run on a computer.\n\nProblem setting. Given a data matrix $X \\in \\mathbb{R}^{n \\times d}$ and a response vector $y \\in \\mathbb{R}^{n}$, consider the following regularized least-squares objective\n$$\n\\min_{w \\in \\mathbb{R}^{d}} \\; \\frac{1}{2}\\lVert y - X w \\rVert_{2}^{2} \\;+\\; \\lambda \\sum_{j=1}^{d} \\left( \\lvert w_j \\rvert^{2} + \\varepsilon \\right)^{p/2},\n$$\nwhere $p = 0.5$, $\\lambda  0$ is the regularization parameter, and $\\varepsilon  0$ is a smoothing parameter that makes the penalty finite and differentiable at $w_j = 0$. All angles in trigonometric functions are in radians. You must use the smoothing parameter $\\varepsilon = 10^{-2}$ and $p = 0.5$.\n\nTasks.\n1) Derive, from first principles, an iterative reweighted least-squares scheme grounded in a Majorizationâ€“Minimization (MM) argument. Start from the definition of least squares, the chain rule for differentiation, and the concavity of the function $t \\mapsto (t + \\varepsilon)^{p/2}$ for $t \\ge 0$ and $p \\in (0,1)$. Construct a quadratic majorizer in $w$ and obtain a linear system update for $w$ at each iteration. Do not assume a pre-existing formula; derive it.\n2) Implement the derived algorithm. Use the following settings for the numerical method: initialize with $w^{(0)} = 0$, use the stopping criterion $\\lVert w^{(k+1)} - w^{(k)} \\rVert_{2} \\le \\text{tol} \\cdot \\left(\\lVert w^{(k)} \\rVert_{2} + 10^{-12}\\right)$ with $\\text{tol} = 10^{-10}$ or stop after a maximum of $500$ iterations, whichever occurs first. For numerical stability, you may add a ridge term $10^{-12} I$ to any positive semidefinite system matrix you need to invert. Angles used in trigonometric functions must be interpreted in radians.\n3) For each test case described below, compute the minimized objective value\n$$\nJ^\\star = \\frac{1}{2}\\lVert y - X w^\\star \\rVert_{2}^{2} \\;+\\; \\lambda \\sum_{j=1}^{d} \\left( \\lvert w^\\star_j \\rvert^{2} + \\varepsilon \\right)^{p/2}\n$$\nattained by your algorithm.\n4) Output formatting: Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3]$. Each $r_i$ must be the minimized objective value $J^\\star$ for test case $i$, rounded to $6$ decimal places. No other text should be printed.\n\nTest suite. Construct each $(X,y,\\lambda)$ deterministically as instructed below. For all definitions, indices $i$ and $j$ are $1$-based in the mathematical description. Angles are in radians.\n\n- Test Case A (happy path, moderately sparse ground truth):\n  - Dimensions: $n = 20$, $d = 8$.\n  - For $1 \\le i \\le 20$, $1 \\le j \\le 8$:\n    $$\n    X_{ij} = \\sin(0.7\\, i + 0.3\\, j) \\;+\\; 0.1 \\cos(2\\, i - 0.2\\, j).\n    $$\n  - Ground-truth coefficients:\n    $$\n    w_{\\text{true}} = [\\,1.5,\\, 0,\\, -2.0,\\, 0,\\, 0,\\, 0.5,\\, 0,\\, 0\\,]^\\top.\n    $$\n  - Response:\n    $$\n    y_i = \\sum_{j=1}^{8} X_{ij}\\, (w_{\\text{true}})_j \\;+\\; 0.02 \\sin(3\\, i).\n    $$\n  - Regularization: $\\lambda = 0.1$.\n\n- Test Case B (ill-conditioned design with near-collinear features):\n  - Dimensions: $n = 16$, $d = 6$.\n  - Define sequences for $1 \\le i \\le 16$:\n    $$\n    b_i = \\sin(0.2\\, i),\\quad c_i = \\cos(0.3\\, i),\\quad s_i = \\sin(0.5\\, i),\\quad t_i = \\cos(0.05\\, i).\n    $$\n  - Columns of $X$ for $1 \\le i \\le 16$:\n    $$\n    X_{i1} = b_i,\\quad\n    X_{i2} = 0.995\\, b_i + 0.01\\, c_i,\\quad\n    X_{i3} = c_i,\\quad\n    X_{i4} = s_i,\\quad\n    X_{i5} = X_{i1} + X_{i3},\\quad\n    X_{i6} = t_i.\n    $$\n  - Ground-truth coefficients:\n    $$\n    w_{\\text{true}} = [\\,1.0,\\, -1.0,\\, 0.0,\\, 0.5,\\, 0.0,\\, 0.0\\,]^\\top.\n    $$\n  - Response:\n    $$\n    y_i = \\sum_{j=1}^{6} X_{ij}\\, (w_{\\text{true}})_j \\;+\\; 0.01 \\cos(4\\, i).\n    $$\n  - Regularization: $\\lambda = 0.05$.\n\n- Test Case C (high-regularization regime to induce strong shrinkage):\n  - Dimensions: $n = 12$, $d = 5$.\n  - For $1 \\le i \\le 12$, $1 \\le j \\le 5$:\n    $$\n    X_{ij} = \\sin(0.4\\, i - 0.2\\, j) \\;+\\; 0.05\\, j.\n    $$\n  - Ground-truth coefficients:\n    $$\n    w_{\\text{true}} = [\\,0.2,\\, -0.1,\\, 0.0,\\, 0.0,\\, 0.3\\,]^\\top.\n    $$\n  - Response:\n    $$\n    y_i = \\sum_{j=1}^{5} X_{ij}\\, (w_{\\text{true}})_j \\;+\\; 0.05 \\sin(2.5\\, i).\n    $$\n  - Regularization: $\\lambda = 1.0$.\n\nRequired final output format. Your program must print exactly one line:\n- A single list with $3$ floating-point numbers $[J_A,J_B,J_C]$, where $J_A$, $J_B$, and $J_C$ are the minimized objective values for Test Cases A, B, and C, respectively, each rounded to exactly $6$ decimal places, with no additional whitespace beyond commas and brackets.",
            "solution": "The problem is to devise and implement a numerical algorithm to solve a non-convex regularized least-squares optimization problem. The objective function to be minimized is given by\n$$\nJ(w) = \\frac{1}{2}\\lVert y - X w \\rVert_{2}^{2} \\;+\\; \\lambda \\sum_{j=1}^{d} \\left( w_j^2 + \\varepsilon \\right)^{p/2}\n$$\nwhere $w \\in \\mathbb{R}^d$ is the vector of coefficients, $X \\in \\mathbb{R}^{n \\times d}$ is the data matrix, $y \\in \\mathbb{R}^n$ is the response vector, $\\lambda  0$ is the regularization parameter, $\\varepsilon = 10^{-2}$ is a smoothing parameter, and the exponent is $p=0.5$. The penalty term is a quasi-norm that promotes sparsity. Due to its non-convex nature, direct minimization is difficult. We are instructed to derive a solution using a Majorization-Minimization (MM) framework.\n\nThe MM principle is an iterative procedure for optimization. At each iteration $k$, the difficult objective function $J(w)$ is replaced by a simpler surrogate function $Q(w | w^{(k)})$, called the majorizer, which satisfies two conditions:\n1.  $J(w) \\le Q(w | w^{(k)})$ for all $w$ (Majorization).\n2.  $J(w^{(k)}) = Q(w^{(k)} | w^{(k)})$ (Tangency).\n\nThe next iterate is found by minimizing the simpler majorizer: $w^{(k+1)} = \\arg\\min_{w} Q(w | w^{(k)})$. This procedure guarantees that the objective function value is non-increasing, i.e., $J(w^{(k+1)}) \\le J(w^{(k)})$.\n\nLet us decompose the objective function $J(w)$ into two parts: a data-fidelity term $f(w) = \\frac{1}{2}\\lVert y - Xw \\rVert_2^2$ and a regularization term $g(w) = \\lambda \\sum_{j=1}^{d} (w_j^2 + \\varepsilon)^{p/2}$. The term $f(w)$ is a convex quadratic and does not require majorization. We will construct a majorizer for the non-convex term $g(w)$.\n\nThe regularizer $g(w)$ is a sum of separable functions, $g(w) = \\sum_{j=1}^d g_j(w_j)$, where $g_j(w_j) = \\lambda (w_j^2 + \\varepsilon)^{p/2}$. Let us introduce a scalar function $h(t) = (t + \\varepsilon)^{p/2}$ for $t \\ge 0$, so that $g_j(w_j) = \\lambda h(w_j^2)$.\n\nThe problem directs us to use the concavity of $h(t)$. We verify this by examining its second derivative with respect to $t$. The first and second derivatives are:\n$$\nh'(t) = \\frac{p}{2} (t + \\varepsilon)^{\\frac{p}{2}-1}\n$$\n$$\nh''(t) = \\frac{p}{2}\\left(\\frac{p}{2}-1\\right) (t + \\varepsilon)^{\\frac{p}{2}-2}\n$$\nFor the given parameter $p=0.5$, we have $p/2 = 0.25$ and $p/2-1 = -0.75$. Both factors are non-zero. Since $t \\ge 0$ and $\\varepsilon = 10^{-2}  0$, the term $(t + \\varepsilon)$ is strictly positive. The sign of $h''(t)$ is determined by the sign of the product $\\frac{p}{2}(\\frac{p}{2}-1)$. With $p=0.5$, this product is $0.25 \\times (-0.75) = -0.1875$, which is negative. Thus, $h''(t)  0$ for all $t \\ge 0$, proving that $h(t)$ is a strictly concave function.\n\nFor a concave function, its tangent line at any point provides a global upper bound. That is, for any $t$ and $t_k$ in its domain:\n$$\nh(t) \\le h(t_k) + h'(t_k)(t - t_k)\n$$\nWe apply this inequality to majorize each term $h(w_j^2)$ around the value at the current iterate, $w^{(k)}$. Let $t = w_j^2$ and $t_k = (w_j^{(k)})^2$. The inequality becomes:\n$$\n(w_j^2 + \\varepsilon)^{p/2} \\le ((w_j^{(k)})^2 + \\varepsilon)^{p/2} + h'((w_j^{(k)})^2) (w_j^2 - (w_j^{(k)})^2)\n$$\nLet us define the weight $\\gamma_j^{(k)} = h'((w_j^{(k)})^2) = \\frac{p}{2}((w_j^{(k)})^2 + \\varepsilon)^{\\frac{p}{2}-1}$. The majorization for the $j$-th penalty component is:\n$$\n\\lambda (w_j^2 + \\varepsilon)^{p/2} \\le \\lambda \\gamma_j^{(k)} w_j^2 + C_j^{(k)}\n$$\nwhere $C_j^{(k)} = \\lambda (h((w_j^{(k)})^2) - \\gamma_j^{(k)} (w_j^{(k)})^2)$ is a constant with respect to $w_j$.\n\nSumming over all components $j=1, \\dots, d$, we obtain a majorizer for the full regularization term $g(w)$. Combining this with the exact data-fidelity term $f(w)$, we form the global majorizing function $Q(w | w^{(k)})$ for the objective $J(w)$:\n$$\nQ(w | w^{(k)}) = \\frac{1}{2}\\lVert y - Xw \\rVert_2^2 + \\lambda \\sum_{j=1}^d \\gamma_j^{(k)} w_j^2 + C^{(k)}\n$$\nwhere $C^{(k)} = \\sum_j C_j^{(k)}$ is a constant with respect to $w$. This majorizer $Q$ is a simple quadratic function of $w$.\n\nThe next iterate, $w^{(k+1)}$, is found by minimizing $Q(w | w^{(k)})$. We find the minimum by setting the gradient of $Q$ with respect to $w$ to zero. First, we express $Q$ in matrix notation:\n$$\nQ(w | w^{(k)}) = \\frac{1}{2}(y - Xw)^T(y - Xw) + \\lambda w^T \\Gamma^{(k)} w + C^{(k)}\n$$\nwhere $\\Gamma^{(k)}$ is a diagonal matrix with diagonal entries $\\Gamma_{jj}^{(k)} = \\gamma_j^{(k)}$. The gradient is:\n$$\n\\nabla_w Q(w | w^{(k)}) = -X^T(y - Xw) + 2\\lambda \\Gamma^{(k)} w = (X^T X + 2\\lambda \\Gamma^{(k)})w - X^T y\n$$\nSetting the gradient to zero, we obtain a linear system for the optimal $w$:\n$$\n(X^T X + 2\\lambda \\Gamma^{(k)}) w = X^T y\n$$\nThis gives the update rule for the MM algorithm:\n$$\nw^{(k+1)} = (X^T X + 2\\lambda \\Gamma^{(k)})^{-1} X^T y\n$$\nThis is an instance of an Iteratively Reweighted Least Squares (IRLS) algorithm. The \"weights\" are the diagonal elements of $2\\lambda \\Gamma^{(k)}$, which are updated at each iteration based on the current estimate $w^{(k)}$.\n\nThe complete algorithm is as follows:\n1.  **Initialization**: Set the iteration counter $k=0$ and initialize the coefficient vector $w^{(0)}=0$. Define constants: $p=0.5$, $\\varepsilon=10^{-2}$, tolerance $\\text{tol}=10^{-10}$, and maximum iterations $N_{\\text{max}}=500$.\n2.  **Iteration**: For $k=0, 1, 2, \\dots, N_{\\text{max}}-1$:\n    a.  Compute the weights for $j=1, \\dots, d$:\n        $$\n        \\gamma_j^{(k)} = \\frac{p}{2} \\left( (w_j^{(k)})^2 + \\varepsilon \\right)^{\\frac{p}{2}-1} = 0.25 \\left( (w_j^{(k)})^2 + 10^{-2} \\right)^{-0.75}\n        $$\n    b.  Construct the diagonal matrix of weights $\\Gamma^{(k)} = \\text{diag}(\\gamma_1^{(k)}, \\dots, \\gamma_d^{(k)})$.\n    c.  Solve the linear system for $w^{(k+1)}$:\n        $$\n        (X^T X + 2\\lambda \\Gamma^{(k)} + \\delta I) w^{(k+1)} = X^T y\n        $$\n        where $\\delta I$ is a small ridge term (with $\\delta=10^{-12}$) added for numerical stability, ensuring the system matrix is always well-conditioned and invertible.\n    d.  Check for convergence: If $\\lVert w^{(k+1)} - w^{(k)} \\rVert_{2} \\le \\text{tol} \\cdot (\\lVert w^{(k)} \\rVert_{2} + 10^{-12})$, terminate the loop.\n3.  **Output**: The final converged vector $w^\\star$ is the result. We then compute the objective value $J(w^\\star)$.\nThis principled derivation provides a robust and convergent numerical scheme to find a stationary point of the non-convex objective function.",
            "answer": "```python\nimport numpy as np\n\ndef mm_solver(X, y, lambda_reg):\n    \"\"\"\n    Solves the L0.5-regularized least squares problem using a\n    Majorization-Minimization (MM) algorithm, which results in an\n    Iteratively Reweighted Least Squares (IRLS) scheme.\n    \"\"\"\n    n, d = X.shape\n    \n    # Parameters from the problem statement\n    p = 0.5\n    epsilon = 1e-2\n    tol = 1e-10\n    max_iter = 500\n    ridge_term = 1e-12\n\n    # Initialize weights vector\n    w = np.zeros(d)\n    \n    # Pre-compute constant parts of the linear system\n    XtX = X.T @ X\n    Xty = X.T @ y\n\n    for k in range(max_iter):\n        w_old = w.copy()\n        \n        # 1. Compute weights for the current iterate w\n        w_sq = w_old**2\n        gamma_vals = (p / 2) * (w_sq + epsilon)**(p / 2 - 1)\n        Gamma_k = np.diag(gamma_vals)\n        \n        # 2. Form the system matrix and solve for the new w\n        # (XtX + 2*lambda*Gamma_k + delta*I) w_new = Xty\n        system_matrix = XtX + 2 * lambda_reg * Gamma_k + ridge_term * np.identity(d)\n        \n        try:\n            w = np.linalg.solve(system_matrix, Xty)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudoinverse if solve fails, though unlikely with ridge\n            w = np.linalg.pinv(system_matrix) @ Xty\n\n        # 3. Check for convergence\n        norm_w_old = np.linalg.norm(w_old)\n        norm_diff = np.linalg.norm(w - w_old)\n        \n        if norm_diff = tol * (norm_w_old + 1e-12):\n            break\n            \n    return w\n\ndef calculate_objective(X, y, w, lambda_reg):\n    \"\"\"\n    Calculates the value of the objective function.\n    \"\"\"\n    p = 0.5\n    epsilon = 1e-2\n    \n    residual = y - X @ w\n    least_squares_term = 0.5 * np.sum(residual**2)\n    \n    penalty_term = lambda_reg * np.sum((w**2 + epsilon)**(p/2))\n    \n    return least_squares_term + penalty_term\n\ndef generate_test_case_A():\n    n, d, lambda_reg = 20, 8, 0.1\n    X = np.zeros((n, d))\n    i_vals = np.arange(1, n + 1)\n    j_vals = np.arange(1, d + 1)\n    \n    for i_idx, i in enumerate(i_vals):\n        for j_idx, j in enumerate(j_vals):\n            X[i_idx, j_idx] = np.sin(0.7 * i + 0.3 * j) + 0.1 * np.cos(2 * i - 0.2 * j)\n            \n    w_true = np.array([1.5, 0, -2.0, 0, 0, 0.5, 0, 0])\n    noise = 0.02 * np.sin(3 * i_vals)\n    y = X @ w_true + noise\n    \n    return X, y, lambda_reg\n\ndef generate_test_case_B():\n    n, d, lambda_reg = 16, 6, 0.05\n    X = np.zeros((n, d))\n    i_vals = np.arange(1, n + 1)\n    \n    b = np.sin(0.2 * i_vals)\n    c = np.cos(0.3 * i_vals)\n    s = np.sin(0.5 * i_vals)\n    t = np.cos(0.05 * i_vals)\n    \n    X[:, 0] = b\n    X[:, 1] = 0.995 * b + 0.01 * c\n    X[:, 2] = c\n    X[:, 3] = s\n    X[:, 4] = X[:, 0] + X[:, 2]\n    X[:, 5] = t\n    \n    w_true = np.array([1.0, -1.0, 0.0, 0.5, 0.0, 0.0])\n    noise = 0.01 * np.cos(4 * i_vals)\n    y = X @ w_true + noise\n    \n    return X, y, lambda_reg\n    \ndef generate_test_case_C():\n    n, d, lambda_reg = 12, 5, 1.0\n    X = np.zeros((n, d))\n    i_vals = np.arange(1, n + 1)\n    j_vals = np.arange(1, d + 1)\n    \n    for i_idx, i in enumerate(i_vals):\n        for j_idx, j in enumerate(j_vals):\n            X[i_idx, j_idx] = np.sin(0.4 * i - 0.2 * j) + 0.05 * j\n            \n    w_true = np.array([0.2, -0.1, 0.0, 0.0, 0.3])\n    noise = 0.05 * np.sin(2.5 * i_vals)\n    y = X @ w_true + noise\n    \n    return X, y, lambda_reg\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases_generators = [\n        generate_test_case_A,\n        generate_test_case_B,\n        generate_test_case_C\n    ]\n    \n    results = []\n    \n    for generator in test_cases_generators:\n        X, y, lambda_reg = generator()\n        w_star = mm_solver(X, y, lambda_reg)\n        objective_value = calculate_objective(X, y, w_star, lambda_reg)\n        results.append(f\"{objective_value:.6f}\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}