{
    "hands_on_practices": [
        {
            "introduction": "Before exploring algorithmic solutions, it's essential to grasp what makes a linear system numerically \"fragile.\" This exercise asks you to analyze a simple $2 \\times 2$ matrix, a \"house of cards\" model, to understand the concept of the condition number. By deriving the condition number $\\kappa_{\\infty}(A_{\\varepsilon})$ as a function of a small parameter $\\varepsilon$, you will gain a concrete appreciation for how near-linear dependence among equations can lead to extreme sensitivity, the hallmark of an ill-conditioned problem. ",
            "id": "2424491",
            "problem": "In a computational engineering model of equilibrium, two balance equations are nearly linearly dependent, forming a fragile configuration analogous to a \"house of cards.\" Consider the parameterized coefficient matrix\n$$\nA_{\\varepsilon}=\\begin{pmatrix}\n1  1 \\\\\n1  1+\\varepsilon\n\\end{pmatrix},\n$$\nwhere $\\varepsilon  0$ is a small parameter. Using only first principles from linear algebra, determine the infinity-norm condition number $\\kappa_{\\infty}(A_{\\varepsilon})$ exactly as a function of $\\varepsilon$. Provide your answer as a single closed-form analytic expression in terms of $\\varepsilon$. No rounding is required.",
            "solution": "The problem as stated is valid. It is a well-posed, self-contained, and scientifically grounded question in the field of numerical linear algebra, directly relevant to the study of numerical stability in computational engineering. We shall proceed with the solution by applying first principles.\n\nThe definition of the infinity-norm condition number for an invertible square matrix $A$ is given by the expression:\n$$\n\\kappa_{\\infty}(A) = \\|A\\|_{\\infty} \\|A^{-1}\\|_{\\infty}\n$$\nwhere $\\|\\cdot\\|_{\\infty}$ represents the maximum absolute row sum norm of a matrix.\n\nThe problem provides the parameterized matrix:\n$$\nA_{\\varepsilon}=\\begin{pmatrix}\n1  1 \\\\\n1  1+\\varepsilon\n\\end{pmatrix}\n$$\nWe must first compute the infinity norm of $A_{\\varepsilon}$. The norm is the maximum of the sums of the absolute values of the elements in each row.\nFor the first row, the sum is $|1| + |1| = 2$.\nFor the second row, the sum is $|1| + |1+\\varepsilon|$. Given the constraint that $\\varepsilon > 0$, it follows that $1+\\varepsilon$ is positive, so the sum is $1 + 1+\\varepsilon = 2+\\varepsilon$.\nTo find $\\|A_{\\varepsilon}\\|_{\\infty}$, we compare the two row sums, $2$ and $2+\\varepsilon$. Since $\\varepsilon > 0$, we have $2+\\varepsilon > 2$.\nTherefore, the maximum absolute row sum is $2+\\varepsilon$.\n$$\n\\|A_{\\varepsilon}\\|_{\\infty} = 2+\\varepsilon\n$$\n\nNext, we must find the inverse of $A_{\\varepsilon}$. First, we calculate the determinant of $A_{\\varepsilon}$:\n$$\n\\det(A_{\\varepsilon}) = (1)(1+\\varepsilon) - (1)(1) = 1+\\varepsilon-1 = \\varepsilon\n$$\nSince $\\varepsilon > 0$, the determinant is non-zero, which confirms that the matrix $A_{\\varepsilon}$ is invertible. The inverse of a general $2 \\times 2$ matrix $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$ is given by the formula $\\frac{1}{ad-bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$.\nApplying this formula to $A_{\\varepsilon}$ yields its inverse, $A_{\\varepsilon}^{-1}$:\n$$\nA_{\\varepsilon}^{-1} = \\frac{1}{\\varepsilon} \\begin{pmatrix} 1+\\varepsilon  -1 \\\\ -1  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1+\\varepsilon}{\\varepsilon}  -\\frac{1}{\\varepsilon} \\\\ -\\frac{1}{\\varepsilon}  \\frac{1}{\\varepsilon} \\end{pmatrix}\n$$\n\nNow, we compute the infinity norm of $A_{\\varepsilon}^{-1}$.\nFor the first row of $A_{\\varepsilon}^{-1}$, the sum of absolute values is:\n$$\n\\left|\\frac{1+\\varepsilon}{\\varepsilon}\\right| + \\left|-\\frac{1}{\\varepsilon}\\right|\n$$\nAgain, using the condition $\\varepsilon > 0$, all terms are positive, so this simplifies to:\n$$\n\\frac{1+\\varepsilon}{\\varepsilon} + \\frac{1}{\\varepsilon} = \\frac{1+\\varepsilon+1}{\\varepsilon} = \\frac{2+\\varepsilon}{\\varepsilon}\n$$\nFor the second row, the sum of absolute values is:\n$$\n\\left|-\\frac{1}{\\varepsilon}\\right| + \\left|\\frac{1}{\\varepsilon}\\right| = \\frac{1}{\\varepsilon} + \\frac{1}{\\varepsilon} = \\frac{2}{\\varepsilon}\n$$\nTo find the norm, we must determine the maximum of these two row sums, $\\frac{2+\\varepsilon}{\\varepsilon}$ and $\\frac{2}{\\varepsilon}$. We can rewrite the first sum as $\\frac{2+\\varepsilon}{\\varepsilon} = \\frac{2}{\\varepsilon} + \\frac{\\varepsilon}{\\varepsilon} = \\frac{2}{\\varepsilon} + 1$.\nAs $\\varepsilon > 0$, it is clear that $\\frac{2}{\\varepsilon} + 1 > \\frac{2}{\\varepsilon}$.\nThus, the maximum absolute row sum is $\\frac{2+\\varepsilon}{\\varepsilon}$.\n$$\n\\|A_{\\varepsilon}^{-1}\\|_{\\infty} = \\frac{2+\\varepsilon}{\\varepsilon}\n$$\n\nFinally, we compute the condition number $\\kappa_{\\infty}(A_{\\varepsilon})$ by multiplying the norms of the matrix and its inverse:\n$$\n\\kappa_{\\infty}(A_{\\varepsilon}) = \\|A_{\\varepsilon}\\|_{\\infty} \\|A_{\\varepsilon}^{-1}\\|_{\\infty} = (2+\\varepsilon) \\left( \\frac{2+\\varepsilon}{\\varepsilon} \\right)\n$$\nThis simplifies to the exact, closed-form analytic expression:\n$$\n\\kappa_{\\infty}(A_{\\varepsilon}) = \\frac{(2+\\varepsilon)^2}{\\varepsilon}\n$$\nThis result demonstrates that as the parameter $\\varepsilon$ approaches $0$, the condition number increases without bound, specifically as $\\mathcal{O}(1/\\varepsilon)$. This quantifies the \"fragile\" nature of the system, where near-linear dependence of the equations leads to extreme sensitivity to perturbations, a hallmark of an ill-conditioned problem.",
            "answer": "$$\n\\boxed{\\frac{(2+\\varepsilon)^2}{\\varepsilon}}\n$$"
        },
        {
            "introduction": "An ill-conditioned system can lead to catastrophic failure during Gaussian elimination if not handled carefully. This hands-on calculation demonstrates this phenomenon by tracing the algorithm on a $3 \\times 3$ matrix with a deliberately small pivot on the diagonal. You will directly compare the intermediate values generated with and without partial pivoting, witnessing how the naive approach leads to an uncontrolled \"blow-up\" of numbers while pivoting maintains stability and controls element growth. ",
            "id": "2424558",
            "problem": "Let $\\delta$ be a fixed positive real parameter with $0  \\delta \\ll 1$. Consider the $3 \\times 3$ matrix\n$$\nA_{\\delta} \\;=\\;\n\\begin{pmatrix}\n\\delta  1  1 \\\\\n1  1  1+\\delta \\\\\n1  1+\\delta  1+2\\delta\n\\end{pmatrix}.\n$$\nDefine $M_{\\mathrm{nopiv}}$ to be the maximum absolute value across all entries of the final upper-triangular factor $U$ produced by applying Gaussian elimination without any pivoting to $A_{\\delta}$. Define $M_{\\mathrm{pp}}$ analogously for Gaussian elimination with partial pivoting, where at each elimination step the pivot is chosen as the entry of largest absolute value in the current column from the current row downward and rows are swapped accordingly. Let\n$$\nR(\\delta) \\;=\\; \\frac{M_{\\mathrm{nopiv}}}{M_{\\mathrm{pp}}}.\n$$\nFor $\\delta = 1.0 \\times 10^{-4}$, compute $R(\\delta)$ and round your final result to four significant figures. The answer must be a single real number with no units.",
            "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n- A fixed positive real parameter $\\delta$ with $0  \\delta \\ll 1$.\n- A $3 \\times 3$ matrix $A_{\\delta} = \\begin{pmatrix} \\delta  1  1 \\\\ 1  1  1+\\delta \\\\ 1  1+\\delta  1+2\\delta \\end{pmatrix}$.\n- $M_{\\mathrm{nopiv}}$: the maximum absolute value of entries in the upper-triangular factor $U$ from Gaussian elimination without pivoting on $A_{\\delta}$.\n- $M_{\\mathrm{pp}}$: the maximum absolute value of entries in the upper-triangular factor $U$ from Gaussian elimination with partial pivoting on $A_{\\delta}$.\n- $R(\\delta) = \\frac{M_{\\mathrm{nopiv}}}{M_{\\mathrm{pp}}}$.\n- A specific value $\\delta = 1.0 \\times 10^{-4}$ is to be used.\n- The final result for $R(\\delta)$ must be rounded to four significant figures.\n\nStep 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is a standard problem in numerical linear algebra concerning the stability of Gaussian elimination and the role of pivoting strategies. The concepts are fundamental to computational science and engineering. All terms are defined unambiguously, all necessary data is provided, and the problem is free of contradictions or logical flaws.\n\nStep 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe solution process consists of three main parts: performing Gaussian elimination without pivoting, performing it with partial pivoting, and then computing the required ratio.\n\nPart 1: Gaussian Elimination without Pivoting\n\nWe start with the matrix $A_{\\delta}$, which we denote as $A^{(1)}$:\n$$\nA^{(1)} =\n\\begin{pmatrix}\n\\delta  1  1 \\\\\n1  1  1+\\delta \\\\\n1  1+\\delta  1+2\\delta\n\\end{pmatrix}\n$$\nThe first pivot is $a_{11}^{(1)} = \\delta$. Since $\\delta$ is very small, we anticipate significant element growth. The multipliers for the first step are $m_{21} = \\frac{a_{21}^{(1)}}{a_{11}^{(1)}} = \\frac{1}{\\delta}$ and $m_{31} = \\frac{a_{31}^{(1)}}{a_{11}^{(1)}} = \\frac{1}{\\delta}$.\nThe row operations are $R_2 \\to R_2 - m_{21} R_1$ and $R_3 \\to R_3 - m_{31} R_1$.\nThe new entries are:\n$a_{22}^{(2)} = a_{22}^{(1)} - m_{21} a_{12}^{(1)} = 1 - \\frac{1}{\\delta} \\cdot 1 = 1 - \\frac{1}{\\delta}$.\n$a_{23}^{(2)} = a_{23}^{(1)} - m_{21} a_{13}^{(1)} = (1+\\delta) - \\frac{1}{\\delta} \\cdot 1 = 1 + \\delta - \\frac{1}{\\delta}$.\n$a_{32}^{(2)} = a_{32}^{(1)} - m_{31} a_{12}^{(1)} = (1+\\delta) - \\frac{1}{\\delta} \\cdot 1 = 1 + \\delta - \\frac{1}{\\delta}$.\n$a_{33}^{(2)} = a_{33}^{(1)} - m_{31} a_{13}^{(1)} = (1+2\\delta) - \\frac{1}{\\delta} \\cdot 1 = 1 + 2\\delta - \\frac{1}{\\delta}$.\n\nThe matrix after the first step of elimination is:\n$$\nA^{(2)} =\n\\begin{pmatrix}\n\\delta  1  1 \\\\\n0  1 - \\frac{1}{\\delta}  1 + \\delta - \\frac{1}{\\delta} \\\\\n0  1 + \\delta - \\frac{1}{\\delta}  1 + 2\\delta - \\frac{1}{\\delta}\n\\end{pmatrix}\n$$\nThe second pivot is $a_{22}^{(2)} = 1 - \\frac{1}{\\delta}$. The multiplier for the second step is $m_{32} = \\frac{a_{32}^{(2)}}{a_{22}^{(2)}} = \\frac{1 + \\delta - 1/\\delta}{1 - 1/\\delta} = \\frac{\\delta^2 + \\delta - 1}{\\delta - 1}$.\nThe operation is $R_3 \\to R_3 - m_{32} R_2$, which gives the new entry $a_{33}^{(3)}$:\n$$\na_{33}^{(3)} = a_{33}^{(2)} - m_{32} a_{23}^{(2)} = \\left(1 + 2\\delta - \\frac{1}{\\delta}\\right) - \\frac{\\delta^2 + \\delta - 1}{\\delta - 1} \\left(1 + \\delta - \\frac{1}{\\delta}\\right)\n$$\n$$\na_{33}^{(3)} = \\frac{\\delta + 2\\delta^2 - 1}{\\delta} - \\frac{\\delta^2+\\delta-1}{\\delta-1} \\frac{\\delta^2+\\delta-1}{\\delta} = \\frac{(\\delta+2\\delta^2-1)(\\delta-1) - (\\delta^2+\\delta-1)^2}{\\delta(\\delta-1)}\n$$\nThe numerator simplifies to $(2\\delta^3-\\delta^2-2\\delta+1) - (\\delta^4+2\\delta^3-\\delta^2-2\\delta+1) = -\\delta^4$.\n$$\na_{33}^{(3)} = \\frac{-\\delta^4}{\\delta(\\delta - 1)} = \\frac{-\\delta^3}{\\delta-1} = \\frac{\\delta^3}{1-\\delta}\n$$\nThe final upper-triangular matrix $U_{\\mathrm{nopiv}}$ is:\n$$\nU_{\\mathrm{nopiv}} =\n\\begin{pmatrix}\n\\delta  1  1 \\\\\n0  1 - \\frac{1}{\\delta}  1 + \\delta - \\frac{1}{\\delta} \\\\\n0  0  \\frac{\\delta^3}{1 - \\delta}\n\\end{pmatrix}\n$$\nTo find $M_{\\mathrm{nopiv}}$, we find the maximum absolute value of all entries in $U_{\\mathrm{nopiv}}$. For $0  \\delta \\ll 1$:\n$|u_{11}| = \\delta$.\n$|u_{12}| = 1$.\n$|u_{13}| = 1$.\n$|u_{22}| = |1 - \\frac{1}{\\delta}| = \\frac{1-\\delta}{\\delta} = \\frac{1}{\\delta} - 1$.\n$|u_{23}| = |1 + \\delta - \\frac{1}{\\delta}| = |\\frac{\\delta^2+\\delta-1}{\\delta}| = \\frac{1-\\delta-\\delta^2}{\\delta} = \\frac{1}{\\delta} - 1 - \\delta$.\n$|u_{33}| = |\\frac{\\delta^3}{1 - \\delta}| = \\frac{\\delta^3}{1-\\delta}$.\nComparing these values, the largest is $|u_{22}| = \\frac{1}{\\delta} - 1$. Thus, $M_{\\mathrm{nopiv}} = \\frac{1-\\delta}{\\delta}$.\n\nPart 2: Gaussian Elimination with Partial Pivoting\n\nWe again start with $A^{(1)}$. In the first column, the entries are $\\delta, 1, 1$. The largest absolute value is $1$. We swap row $1$ with row $2$ (swapping with row $3$ would yield an equivalent result).\n$$\nA'^{(1)} =\n\\begin{pmatrix}\n1  1  1+\\delta \\\\\n\\delta  1  1 \\\\\n1  1+\\delta  1+2\\delta\n\\end{pmatrix}\n$$\nThe pivot is $a'_{11} = 1$. The multipliers are $m_{21} = \\frac{\\delta}{1} = \\delta$ and $m_{31} = \\frac{1}{1} = 1$.\nRow operations yield:\n$a_{22}^{(2)} = 1 - \\delta \\cdot 1 = 1 - \\delta$.\n$a_{23}^{(2)} = 1 - \\delta(1+\\delta) = 1 - \\delta - \\delta^2$.\n$a_{32}^{(2)} = (1+\\delta) - 1 \\cdot 1 = \\delta$.\n$a_{33}^{(2)} = (1+2\\delta) - 1(1+\\delta) = \\delta$.\nThe matrix after the first step is:\n$$\nA^{(2)} =\n\\begin{pmatrix}\n1  1  1+\\delta \\\\\n0  1 - \\delta  1 - \\delta - \\delta^2 \\\\\n0  \\delta  \\delta\n\\end{pmatrix}\n$$\nFor the second step, we consider the second column from the diagonal downwards. The entries are $1-\\delta$ and $\\delta$. Since $0  \\delta \\ll 1$, we have $|1-\\delta| > |\\delta|$. No row swap is necessary.\nThe pivot is $a_{22}^{(2)} = 1 - \\delta$. The multiplier is $m_{32} = \\frac{\\delta}{1-\\delta}$.\nThe new entry $a_{33}^{(3)}$ is:\n$$\na_{33}^{(3)} = \\delta - \\frac{\\delta}{1-\\delta}(1-\\delta-\\delta^2) = \\frac{\\delta(1-\\delta) - \\delta(1-\\delta-\\delta^2)}{1-\\delta} = \\frac{(\\delta-\\delta^2) - (\\delta-\\delta^2-\\delta^3)}{1-\\delta} = \\frac{\\delta^3}{1-\\delta}\n$$\nThe final upper-triangular matrix $U_{\\mathrm{pp}}$ is:\n$$\nU_{\\mathrm{pp}} =\n\\begin{pmatrix}\n1  1  1+\\delta \\\\\n0  1 - \\delta  1 - \\delta - \\delta^2 \\\\\n0  0  \\frac{\\delta^3}{1 - \\delta}\n\\end{pmatrix}\n$$\nTo find $M_{\\mathrm{pp}}$, we find the maximum absolute value of entries in $U_{\\mathrm{pp}}$.\n$|u'_{11}| = 1$.\n$|u'_{12}| = 1$.\n$|u'_{13}| = 1+\\delta$.\n$|u'_{22}| = 1-\\delta$.\n$|u'_{23}| = 1-\\delta-\\delta^2$.\n$|u'_{33}| = \\frac{\\delta^3}{1-\\delta}$.\nFor $0  \\delta \\ll 1$, the largest of these values is clearly $1+\\delta$. Thus, $M_{\\mathrm{pp}} = 1+\\delta$.\n\nPart 3: Calculation of $R(\\delta)$\n\nThe ratio $R(\\delta)$ is given by:\n$$\nR(\\delta) = \\frac{M_{\\mathrm{nopiv}}}{M_{\\mathrm{pp}}} = \\frac{\\frac{1-\\delta}{\\delta}}{1+\\delta} = \\frac{1-\\delta}{\\delta(1+\\delta)}\n$$\nWe are asked to compute this for $\\delta = 1.0 \\times 10^{-4}$.\nSubstituting this value:\n$$\nM_{\\mathrm{nopiv}} = \\frac{1 - 10^{-4}}{10^{-4}} = \\frac{0.9999}{10^{-4}} = 9999\n$$\n$$\nM_{\\mathrm{pp}} = 1 + 10^{-4} = 1.0001\n$$\n$$\nR(10^{-4}) = \\frac{9999}{1.0001}\n$$\nPerforming the division:\n$$\nR(10^{-4}) \\approx 9998.00019998\n$$\nThe problem requires rounding this result to four significant figures. The first four significant figures are $9, 9, 9, 8$. The fifth significant digit is $0$, so we round down.\nThe final result is $9998$.",
            "answer": "$$\n\\boxed{9998}\n$$"
        },
        {
            "introduction": "Having established the \"why\" and \"how\" of pivoting, this final practice moves to a comprehensive, quantitative comparison of different strategies. You are tasked with implementing four variants of LU factorization to solve a linear system involving the Hilbert matrix, which is famous for its poor conditioning. By comparing the accuracy of the computed solutions from no pivoting, partial pivoting, scaled partial pivoting, and complete pivoting, you will gain practical insight into the performance trade-offs inherent in these fundamental numerical tools. ",
            "id": "2424559",
            "problem": "You are given the Hilbert matrix $H \\in \\mathbb{R}^{n \\times n}$ with entries $H_{ij} = \\dfrac{1}{i + j - 1}$ for $i,j \\in \\{1,\\dots,n\\}$. For a given $n$, define the vector $x_{\\mathrm{true}} \\in \\mathbb{R}^n$ by $(x_{\\mathrm{true}})_i = 1$ for all $i \\in \\{1,\\dots,n\\}$, and define $b = H x_{\\mathrm{true}}$. For each $n$ in a specified test suite, compute numerical approximations $\\hat{x}$ to the solution of the linear system $H x = b$ by constructing a factorization of $H$ under four pivoting strategies and then solving the triangular systems determined by that factorization. The four strategies are:\n(1) no pivoting, where $H = L U$ with $L$ unit lower triangular and $U$ upper triangular,\n(2) partial pivoting (row pivoting), where $P H = L U$ with $P$ a permutation matrix,\n(3) scaled partial pivoting (row pivoting with row scaling), where $P H = L U$ with $P$ a permutation matrix determined by row scaling considerations, and\n(4) complete pivoting (row and column pivoting), where $P H Q = L U$ with $P$ and $Q$ permutation matrices.\nFor each strategy, compute the relative $2$-norm solution error $e = \\dfrac{\\lVert \\hat{x} - x_{\\mathrm{true}} \\rVert_2}{\\lVert x_{\\mathrm{true}} \\rVert_2}$.\n\nUse the following test suite for $n$: $\\{1,3,8,12\\}$. For each $n$ in this set, return a list of four floating-point values corresponding, in order, to the relative errors for strategies $(1)$, $(2)$, $(3)$, and $(4)$. Aggregate the results for all $n$ into a single list of lists in the same order of $n$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, where each inner list corresponds to one $n$ in the test suite. Each floating-point value must be formatted in scientific notation with $6$ digits after the decimal point. For example, the output should have the form\n$[[e_{1,1},e_{1,2},e_{1,3},e_{1,4}],[e_{3,1},e_{3,2},e_{3,3},e_{3,4}],[e_{8,1},e_{8,2},e_{8,3},e_{8,4}],[e_{12,1},e_{12,2},e_{12,3},e_{12,4}]]$,\nwhere $e_{n,s}$ denotes the relative error for size $n$ and strategy $s$.\n\nThere are no physical units involved in this problem. Angles are not used. All final numerical answers must be real numbers printed as specified above. The answer types for all test cases are floating-point numbers. The problem is to be solved purely from the definitions of matrix factorization, permutation matrices, and the solution of triangular systems, and the description is fully self-contained and independent of any specific programming language or library.",
            "solution": "The problem as stated is valid. It is a well-posed, scientifically-grounded exercise in numerical linear algebra, centered on the critical topic of numerical stability in the solution of linear systems. All definitions and parameters are provided unambiguously.\n\nThe fundamental task is to solve the linear system of equations $H x = b$ for several dimensions $n$, where $H$ is the $n \\times n$ Hilbert matrix. The entries of $H$ are given by $H_{ij} = \\dfrac{1}{i+j-1}$ for $i,j \\in \\{1, \\dots, n\\}$. The vector $b$ is constructed such that the exact solution, denoted $x_{\\mathrm{true}}$, is a vector of ones: $(x_{\\mathrm{true}})_i = 1$ for all $i$. Therefore, the right-hand side is $b = H x_{\\mathrm{true}}$, which implies its components are $b_i = \\sum_{j=1}^{n} H_{ij} = \\sum_{j=1}^{n} \\dfrac{1}{i+j-1}$.\n\nThe Hilbert matrix is a classic example of an ill-conditioned matrix. Its condition number, $\\kappa(H) = \\lVert H \\rVert \\lVert H^{-1} \\rVert$, grows at a superexponential rate with $n$. This extreme sensitivity means that small floating-point rounding errors introduced during numerical computations can be amplified into large errors in the final solution $\\hat{x}$. The objective of this problem is to observe and quantify this effect and to demonstrate the efficacy of various pivoting strategies in mitigating such numerical instability.\n\nThe solution method is Gaussian elimination, which algorithmically corresponds to factoring the matrix $H$ into a product of a lower triangular matrix $L$ and an upper triangular matrix $U$. Once the factorization is obtained, the system is solved efficiently by successive forward and backward substitutions. We will implement and compare four variants of this procedure. For each numerical solution $\\hat{x}$, we will compute the relative error in the $2$-norm:\n$$e = \\frac{\\lVert \\hat{x} - x_{\\mathrm{true}} \\rVert_2}{\\lVert x_{\\mathrm{true}} \\rVert_2}$$\nGiven that $(x_{\\mathrm{true}})_i=1$ for all $i$, the norm of the true solution is $\\lVert x_{\\mathrm{true}} \\rVert_2 = \\sqrt{\\sum_{i=1}^n 1^2} = \\sqrt{n}$.\n\nThe four pivoting strategies are as follows:\n\n1.  **No Pivoting**:\n    The matrix $H$ is factorized directly as $H = LU$, where $L$ is a unit lower triangular matrix and $U$ is an upper triangular matrix. The system $Hx=b$ becomes $LUx=b$. This is solved in two stages:\n    -   First, solve $Ly=b$ for $y$ using forward substitution.\n    -   Then, solve $Ux=y$ for $x$ using backward substitution.\n    This naive approach is known to be numerically unstable if any pivot element $U_{kk}$ is small, as it can lead to large multipliers and catastrophic loss of precision. For the Hilbert matrix, the pivots become exceedingly small, guaranteeing poor performance.\n\n2.  **Partial Pivoting (Row Pivoting)**:\n    This is the most common strategy for improving the stability of Gaussian elimination. At each step $k$ of the elimination, the algorithm searches for the element with the largest absolute value in the current pivot column (from row $k$ downwards). Let this element be in row $p$, where $p \\ge k$. Row $k$ and row $p$ are then interchanged. This process is equivalent to finding a permutation matrix $P$ such that the factorization is performed on the permuted matrix: $PH=LU$. The permutation $P$ reorders the rows of $H$ to ensure that the pivot elements used in the elimination are as large as possible, thus keeping the multipliers in $L$ bounded by $1$ in magnitude. The system $Hx=b$ is rewritten as $PHx=Pb$, which leads to $LUx=Pb$. We solve $Ly=Pb$ and then $Ux=y$. For the specific case of the Hilbert matrix, it is a known property that no row swaps are performed, so this strategy should yield results identical to no pivoting.\n\n3.  **Scaled Partial Pivoting**:\n    This strategy is a refinement of partial pivoting. It is designed to prevent a large entry from being chosen as a pivot solely because its row contains large entries overall. Before elimination, a scale factor $s_i$ is computed for each row, defined as the maximum absolute value of any element in that row: $s_i = \\max_{1 \\le j \\le n} |H_{ij}|$. At step $k$, instead of choosing the raw largest element, the algorithm chooses the pivot row $p$ ($p \\ge k$) that maximizes the ratio of the pivot candidate's magnitude to its row's scale factor: $\\dfrac{|A_{pk}^{(k-1)}|}{s_p}$. The matrix $A^{(k-1)}$ here represents the matrix state after $k-1$ steps of elimination. After the pivot row is chosen, it is swapped with row $k$, and the elimination proceeds as in standard partial pivoting. The scale factors must also be swapped. The resulting factorization is again of the form $PH=LU$. For the Hilbert matrix, this strategy also results in no row swaps, so its performance should be identical to no pivoting.\n\n4.  **Complete Pivoting**:\n    This is the most robust, but also the most computationally expensive, pivoting strategy. At each step $k$, the algorithm searches the entire remaining submatrix $A^{(k-1)}_{i,j \\ge k}$ for the element with the largest absolute value. If this element is found at position $(p, q)$, row $k$ is swapped with row $p$, and column $k$ is swapped with column $q$. This corresponds to finding two permutation matrices, $P$ (for rows) and $Q$ (for columns), such that $PHQ=LU$. The system $Hx=b$ is transformed into $(PHQ)(Q^T x) = Pb$. Let $z = Q^T x$. We first solve the system $LUz = Pb$ for $z$ using forward and backward substitution. The final solution is then recovered by undoing the column permutations: $x = Qz$. Complete pivoting offers the best theoretical guarantees on numerical stability by minimizing the growth of elements during factorization but requires a significantly larger number of comparisons at each step.\n\nWe expect to observe a clear trend: the error will decrease as the sophistication of the pivoting strategy increases, from no pivoting (largest error) to complete pivoting (smallest error). This demonstrates the crucial role of pivoting in obtaining meaningful solutions for ill-conditioned systems in finite-precision arithmetic.",
            "answer": "[[0.000000e+00,0.000000e+00,0.000000e+00,0.000000e+00],[2.775558e-15,2.775558e-15,2.775558e-15,1.110223e-15],[4.664402e-08,4.664402e-08,4.664402e-08,4.425948e-09],[1.171239e+01,1.171239e+01,1.171239e+01,4.891398e-04]]"
        }
    ]
}