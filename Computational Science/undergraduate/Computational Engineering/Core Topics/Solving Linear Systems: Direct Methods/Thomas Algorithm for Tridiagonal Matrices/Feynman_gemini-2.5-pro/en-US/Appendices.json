{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering a numerical method is building a correct and robust implementation. This practice guides you through coding the Thomas algorithm, a highly efficient form of Gaussian elimination tailored for tridiagonal systems. Importantly, this exercise moves beyond a naïve implementation by incorporating a critical check for numerical stability, requiring you to monitor the magnitude of the pivot elements, denoted as $p_i$, against a tolerance $\\tau$ to handle potentially singular or ill-conditioned matrices .",
            "id": "2446297",
            "problem": "You are given a family of real tridiagonal linear systems of the form $A \\mathbf{x} = \\mathbf{d}$, where $A \\in \\mathbb{R}^{n \\times n}$ has main diagonal entries $\\{b_i\\}_{i=1}^{n}$, subdiagonal entries $\\{a_i\\}_{i=2}^{n}$, and superdiagonal entries $\\{c_i\\}_{i=1}^{n-1}$, so that\n$$\nA =\n\\begin{pmatrix}\nb_1 & c_1 & 0 & \\cdots & 0 \\\\\na_2 & b_2 & c_2 & \\ddots & \\vdots \\\\\n0 & a_3 & b_3 & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & c_{n-1} \\\\\n0 & \\cdots & 0 & a_n & b_n\n\\end{pmatrix}, \\quad\n\\mathbf{x} =\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n\n\\end{pmatrix}, \\quad\n\\mathbf{d} =\n\\begin{pmatrix}\nd_1 \\\\ d_2 \\\\ \\vdots \\\\ d_{n-1} \\\\ d_n\n\\end{pmatrix}.\n$$\nFor each system, you are also given a nonnegative tolerance $\\tau \\in \\mathbb{R}$, and you must apply the following rule: define sequences $\\{p_i\\}$, $\\{c'_i\\}$, and $\\{d'_i\\}$ by\n$$\np_1 = b_1,\n$$\nand for $i = 1$ set\n$$\nc'_1 = \\begin{cases}\n\\frac{c_1}{p_1}, & \\text{if } n \\ge 2, \\\\\n\\text{undefined}, & \\text{if } n = 1,\n\\end{cases}\n\\quad\nd'_1 = \\frac{d_1}{p_1}.\n$$\nFor $i = 2, 3, \\ldots, n$, set\n$$\np_i = b_i - a_i \\, c'_{i-1}.\n$$\nAt any index $i \\in \\{1,2,\\ldots,n\\}$, if $|p_i| \\le \\tau$, the computation is deemed to have failed due to singularity or near-singularity, and you must declare failure for that system. If $|p_i| > \\tau$ for all $i$, then for $i = 2, 3, \\ldots, n-1$ define\n$$\nc'_i = \\frac{c_i}{p_i},\n$$\nand for $i = 2, 3, \\ldots, n$ define\n$$\nd'_i = \\frac{d_i - a_i d'_{i-1}}{p_i}.\n$$\nIf no failure was declared, the unique solution $\\mathbf{x}$ is defined by the back-substitution relations\n$$\nx_n = d'_n, \\quad\nx_i = d'_i - c'_i x_{i+1} \\quad \\text{for } i = n-1, n-2, \\ldots, 1.\n$$\n\nImplement a program that, for each test case below, follows the rule above and outputs either the solution vector $\\mathbf{x}$ (as a list of real numbers) if the computation succeeds, or the boolean value $False$ if a failure was declared because there exists an index $i$ with $|p_i| \\le \\tau$.\n\nTest Suite. Each test case is specified by the tuple $(\\{a_i\\}_{i=2}^{n}, \\{b_i\\}_{i=1}^{n}, \\{c_i\\}_{i=1}^{n-1}, \\{d_i\\}_{i=1}^{n}, \\tau)$ written in zero-based array form for implementation, that is, as lists\n$$\na = [a_2, a_3, \\ldots, a_n], \\quad b = [b_1, b_2, \\ldots, b_n], \\quad c = [c_1, c_2, \\ldots, c_{n-1}], \\quad d = [d_1, d_2, \\ldots, d_n],\n$$\nwith $\\tau$ given separately.\n\nProvide results for the following six test cases:\n- Case $1$ (regular, strictly diagonally dominant): $a = [ -1, -1, -1, -1 ]$, $b = [ 2, 2, 2, 2, 2 ]$, $c = [ -1, -1, -1, -1 ]$, $d = [ 1, 1, 1, 1, 1 ]$, $\\tau = 10^{-12}$.\n- Case $2$ (boundary size $n = 1$, failure due to tiny first pivot): $a = [\\,]$ (empty list), $b = [ 10^{-12} ]$, $c = [\\,]$, $d = [ 1 ]$, $\\tau = 10^{-10}$.\n- Case $3$ (mid-computation exact zero pivot): $a = [ 1, 1 ]$, $b = [ 1, 1, 1 ]$, $c = [ 1, 1 ]$, $d = [ 1, 2, 3 ]$, $\\tau = 10^{-12}$.\n- Case $4$ (diagonal matrix): $a = [ 0, 0, 0 ]$, $b = [ 3, 4, 5, 6 ]$, $c = [ 0, 0, 0 ]$, $d = [ 3, 8, 10, 12 ]$, $\\tau = 10^{-12}$.\n- Case $5$ (near-singular but acceptable under strict tolerance): $a = [ 1 ]$, $b = [ 1, 1 + 10^{-9} ]$, $c = [ 1 ]$, $d = [ 1, 1 ]$, $\\tau = 10^{-12}$.\n- Case $6$ (same as Case $5$ but failure under looser tolerance): $a = [ 1 ]$, $b = [ 1, 1 + 10^{-9} ]$, $c = [ 1 ]$, $d = [ 1, 1 ]$, $\\tau = 10^{-8}$.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output either the solution vector as a list of real numbers or the boolean value $False$ if the rule declares failure. There must be no spaces anywhere in the printed line. For example, an output aggregating three hypothetical per-case results might look like\n$$\n[\\,[0.5,1.0],False,[1.0]\\,].\n$$\nYour program must output exactly one such line containing the six per-case results in order as $[r_1,r_2,r_3,r_4,r_5,r_6]$, with no additional text.",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n\nThe problem provides a procedure to solve a real tridiagonal linear system $A \\mathbf{x} = \\mathbf{d}$ of size $n \\times n$.\nThe matrix $A$ has:\n- Main diagonal entries: $\\{b_i\\}_{i=1}^{n}$\n- Subdiagonal entries: $\\{a_i\\}_{i=2}^{n}$\n- Superdiagonal entries: $\\{c_i\\}_{i=1}^{n-1}$\n\nThe algorithm is defined via sequences $\\{p_i\\}$, $\\{c'_i\\}$, and $\\{d'_i\\}$ and a nonnegative tolerance $\\tau$.\n\nForward Elimination:\n1.  Initialize with $i=1$:\n    $p_1 = b_1$.\n    A failure is declared if $|p_1| \\le \\tau$.\n    If successful, compute $c'_1 = c_1/p_1$ (for $n \\ge 2$) and $d'_1 = d_1/p_1$.\n\n2.  Iterate for $i = 2, 3, \\ldots, n$:\n    $p_i = b_i - a_i c'_{i-1}$.\n    A failure is declared if $|p_i| \\le \\tau$.\n    If successful, compute $c'_i = c_i/p_i$ (for $i < n$) and $d'_i = (d_i - a_i d'_{i-1})/p_i$.\n\nBack Substitution:\nIf no failure occurs, the solution $\\mathbf{x}$ is found by:\n$x_n = d'_n$.\n$x_i = d'_i - c'_i x_{i+1}$ for $i = n-1, n-2, \\ldots, 1$.\n\nOutput:\n- The boolean value `False` upon failure.\n- The solution vector $\\mathbf{x}$ upon success.\n\nTest Cases: Six specific instances of $(\\{a_i\\}, \\{b_i\\}, \\{c_i\\}, \\{d_i\\}, \\tau)$ are provided for implementation, using zero-based list formats for the vectors.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded:** The described algorithm is the Thomas algorithm, a well-established and computationally efficient method in numerical linear algebra for solving tridiagonal systems. It is a specialized form of Gaussian elimination. The use of a tolerance $\\tau$ to check the magnitude of pivots is a standard technique to ensure numerical stability. The problem is scientifically sound.\n-   **Well-Posed:** The algorithm is deterministic and provides a unique outcome (either a solution vector or a failure state) for any valid set of inputs. The conditions for its application and termination are clearly defined.\n-   **Objective:** The problem is stated using precise mathematical language and notation. All data and procedures are objective and formalizable.\n\nThe problem does not violate any of the invalidity criteria. It is scientifically sound, well-posed, complete, and verifiable.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be provided.\n\n**Principle-Based Solution**\n\nThe algorithm presented in the problem statement is a direct application of LU decomposition tailored for a tridiagonal matrix structure. This method, known as the Thomas algorithm, achieves high efficiency by avoiding operations on the zero elements of the matrix. The core principle is to factor the tridiagonal matrix $A$ into the product of a lower bidiagonal matrix $L$ and a unit upper bidiagonal matrix $U$, a form known as Crout's factorization, $A = LU$.\n\nLet the matrices $L$ and $U$ be defined as:\n$$\nL =\n\\begin{pmatrix}\np_1 & 0 & 0 & \\cdots & 0 \\\\\na_2 & p_2 & 0 & \\ddots & \\vdots \\\\\n0 & a_3 & p_3 & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & 0 \\\\\n0 & \\cdots & 0 & a_n & p_n\n\\end{pmatrix}, \\quad\nU =\n\\begin{pmatrix}\n1 & c'_1 & 0 & \\cdots & 0 \\\\\n0 & 1 & c'_2 & \\ddots & \\vdots \\\\\n0 & 0 & 1 & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & c'_{n-1} \\\\\n0 & \\cdots & 0 & 0 & 1\n\\end{pmatrix}\n$$\nBy equating the entries of $A$ with the product $LU$, we derive the recurrence relations for the unknown elements $p_i$ and $c'_i$.\nThe main diagonal of $A$ gives $b_i = (LU)_{ii}$:\n-   For $i=1$: $b_1 = p_1$.\n-   For $i=2, \\ldots, n$: $b_i = a_i c'_{i-1} + p_i$, which rearranges to $p_i = b_i - a_i c'_{i-1}$.\n\nThe superdiagonal of $A$ gives $c_i = (LU)_{i,i+1}$:\n-   For $i=1, \\ldots, n-1$: $c_i = p_i c'_i$, which gives $c'_i = c_i / p_i$.\n\nThese relations are precisely the forward-elimination steps of the prescribed algorithm for computing $\\{p_i\\}$ and $\\{c'_i\\}$. The elements $\\{p_i\\}$ are the pivots of the Gaussian elimination process. The condition $|p_i| \\le \\tau$ for a small tolerance $\\tau \\ge 0$ is a critical check for numerical stability. If a pivot $p_i$ is zero, the matrix $A$ is singular, and the algorithm fails. If $|p_i|$ is very small, the matrix is ill-conditioned (near-singular), and dividing by $p_i$ would introduce large floating-point errors, destabilizing the solution. The tolerance $\\tau$ provides a practical threshold to declare failure in such cases.\n\nOnce the factorization $A=LU$ is found, the original system $A\\mathbf{x} = \\mathbf{d}$ becomes $LU\\mathbf{x} = \\mathbf{d}$. This is solved in two stages:\n1.  **Forward Substitution:** Let $\\mathbf{y} = U\\mathbf{x}$. Solve the lower triangular system $L\\mathbf{y} = \\mathbf{d}$. This yields:\n    -   $p_1 y_1 = d_1 \\implies y_1 = d_1/p_1$.\n    -   $a_i y_{i-1} + p_i y_i = d_i \\implies y_i = (d_i - a_i y_{i-1})/p_i$ for $i=2, \\ldots, n$.\n    By observing the structure of this recurrence, we see that the vector $\\mathbf{y}$ is identical to the sequence $\\mathbf{d}' = \\{d'_i\\}_{i=1}^n$ defined in the problem. The forward elimination phase thus computes the LU factorization and solves $L\\mathbf{y} = \\mathbf{d}$ simultaneously.\n\n2.  **Backward Substitution:** Solve the unit upper triangular system $U\\mathbf{x} = \\mathbf{y}$ (which is now $U\\mathbf{x} = \\mathbf{d}'$). This yields:\n    -   $x_n = y_n = d'_n$.\n    -   $x_i + c'_i x_{i+1} = y_i = d'_i \\implies x_i = d'_i - c'_i x_{i+1}$ for $i=n-1, \\ldots, 1$.\n    This is identical to the back-substitution phase defined in the problem statement.\n\nIn summary, the provided algorithm is a numerically robust implementation of the Thomas algorithm, grounded in the principles of LU factorization for tridiagonal systems. The logic is sound, and its implementation will yield the correct solution or a failure state as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve_tridiagonal(a_vec, b_vec, c_vec, d_vec, tau):\n    \"\"\"\n    Solves a tridiagonal linear system Ax=d using the Thomas algorithm.\n\n    Args:\n        a_vec (list): The subdiagonal entries [a_2, ..., a_n].\n        b_vec (list): The main diagonal entries [b_1, ..., b_n].\n        c_vec (list): The superdiagonal entries [c_1, ..., c_{n-1}].\n        d_vec (list): The right-hand side vector [d_1, ..., d_n].\n        tau (float): The non-negative tolerance for pivot checking.\n\n    Returns:\n        list or bool: The solution vector x as a list of floats, or False if\n                      the computation fails due to a small pivot.\n    \"\"\"\n    n = len(b_vec)\n    if n == 0:\n        return []\n\n    # Handle the n=1 case separately as per the algorithm's structure\n    if n == 1:\n        p1 = b_vec[0]\n        if abs(p1) <= tau:\n            return False\n        return [d_vec[0] / p1]\n\n    # Allocate memory for modified coefficients\n    c_prime = [0.0] * (n - 1)\n    d_prime = [0.0] * n\n\n    # --- Forward Elimination Phase ---\n\n    # Step for i = 1 (Python index 0)\n    p1 = b_vec[0]\n    if abs(p1) <= tau:\n        return False\n    \n    c_prime[0] = c_vec[0] / p1\n    d_prime[0] = d_vec[0] / p1\n\n    # Loop for i = 2 to n (Python index 1 to n-1)\n    for i in range(1, n):\n        # Current mathematical index is i+1, Python index is i\n        # a_i in math corresponds to a_vec[i-2]\n        # a_{i+1} in math corresponds to a_vec[i-1]\n        \n        pi = b_vec[i] - a_vec[i-1] * c_prime[i-1]\n        if abs(pi) <= tau:\n            return False\n\n        if i < n - 1:\n            # c_i in math corresponds to c_vec[i-1]\n            # c_{i+1} in math corresponds to c_vec[i]\n            c_prime[i] = c_vec[i] / pi\n        \n        # d_i in math corresponds to d_vec[i-1]\n        # d_{i+1} in math corresponds to d_vec[i]\n        d_prime[i] = (d_vec[i] - a_vec[i-1] * d_prime[i-1]) / pi\n\n    # --- Backward Substitution Phase ---\n    x = [0.0] * n\n    x[n - 1] = d_prime[n - 1]\n    \n    # Loop for i = n-1 down to 1 (Python index n-2 down to 0)\n    for i in range(n - 2, -1, -1):\n        x[i] = d_prime[i] - c_prime[i] * x[i + 1]\n\n    return x\n\n\ndef solve():\n    \"\"\"\n    Runs the provided test suite and prints the formatted results.\n    \"\"\"\n    test_cases = [\n        # Case 1: regular, strictly diagonally dominant\n        {'a': [-1., -1., -1., -1.], 'b': [2., 2., 2., 2., 2.], 'c': [-1., -1., -1., -1.], 'd': [1., 1., 1., 1., 1.], 'tau': 1e-12},\n        # Case 2: boundary size n=1, failure due to tiny first pivot\n        {'a': [], 'b': [1e-12], 'c': [], 'd': [1.], 'tau': 1e-10},\n        # Case 3: mid-computation exact zero pivot\n        {'a': [1., 1.], 'b': [1., 1., 1.], 'c': [1., 1.], 'd': [1., 2., 3.], 'tau': 1e-12},\n        # Case 4: diagonal matrix\n        {'a': [0., 0., 0.], 'b': [3., 4., 5., 6.], 'c': [0., 0., 0.], 'd': [3., 8., 10., 12.], 'tau': 1e-12},\n        # Case 5: near-singular but acceptable under strict tolerance\n        {'a': [1.], 'b': [1., 1. + 1e-9], 'c': [1.], 'd': [1., 1.], 'tau': 1e-12},\n        # Case 6: same as Case 5 but failure under looser tolerance\n        {'a': [1.], 'b': [1., 1. + 1e-9], 'c': [1.], 'd': [1., 1.], 'tau': 1e-8},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = solve_tridiagonal(case['a'], case['b'], case['c'], case['d'], case['tau'])\n        all_results.append(result)\n\n    # Format the results into a single string with no spaces, as required.\n    string_results = []\n    for res in all_results:\n        if res is False:\n            string_results.append(\"False\")\n        else: # It is a list of numbers\n            list_as_string = \"[\" + \",\".join(map(str, res)) + \"]\"\n            string_results.append(list_as_string)\n            \n    final_output = \"[\" + \",\".join(string_results) + \"]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "With a working implementation in hand, we now ask a fundamental question: why is the Thomas algorithm so important? This practice addresses the 'why' by shifting our focus from coding to computational analysis. You will derive the exact number of floating-point operations (FLOPs) required by the Thomas algorithm and compare it to that of a standard dense Gaussian elimination routine, providing a concrete mathematical proof of its superior efficiency—an advantage that scales from $O(n^3)$ down to $O(n)$ .",
            "id": "2446371",
            "problem": "In computational engineering, a tridiagonal linear system has coefficient matrix $A \\in \\mathbb{R}^{n \\times n}$ with nonzero entries only on the main diagonal and the first sub- and super-diagonals, and a single right-hand side vector $\\mathbf{d} \\in \\mathbb{R}^{n}$. Consider solving $A \\mathbf{x} = \\mathbf{d}$ in two ways:\n\n- Using the Thomas algorithm, implemented in its classical form that performs a single forward sweep eliminating the subdiagonal with scalar multipliers while updating the main diagonal and the right-hand side, followed by a backward substitution on the resulting upper-triangular system. Assume no pivoting.\n- Using standard dense Gaussian elimination (GE) without exploiting sparsity and without pivoting, treating $A$ as a general dense matrix and performing full trailing submatrix updates at each step; then solving the resulting lower-upper (LU) triangular systems with one forward substitution and one backward substitution for the single right-hand side.\n\nAdopt the following floating-point operation (FLOP) model: each floating-point addition, subtraction, multiplication, or division counts as exactly $1$ FLOP; comparisons, assignments, and memory operations are not counted.\n\nDerive the exact total FLOP count as closed-form functions of $n$ for each method, including all steps necessary to compute $\\mathbf{x}$ for the single right-hand side. Provide your final answer as a row vector containing the Thomas algorithm FLOP count and the dense Gaussian elimination FLOP count, in that order. No rounding is required. Do not include units.",
            "solution": "The problem requires the derivation of the exact floating-point operation (FLOP) count for solving a tridiagonal linear system $A \\mathbf{x} = \\mathbf{d}$ of size $n \\times n$ using two distinct methods: the specialized Thomas algorithm and standard dense Gaussian elimination. The analysis will adhere to the provided FLOP model where each addition, subtraction, multiplication, or division counts as one FLOP.\n\nFirst, let us define the structure of the tridiagonal matrix $A$. The system of equations $A \\mathbf{x} = \\mathbf{d}$ can be written as:\n$$ a_i x_{i-1} + b_i x_i + c_i x_{i+1} = d_i $$\nfor $i=1, \\dots, n$, with the understanding that $a_1 = 0$ and $c_n = 0$. The coefficients $a_i$ form the subdiagonal, $b_i$ the main diagonal, and $c_i$ the superdiagonal.\n\n**1. Analysis of the Thomas Algorithm**\n\nThe Thomas algorithm is a highly efficient form of Gaussian elimination tailored for tridiagonal systems. It consists of two sequential stages: a forward elimination sweep followed by a backward substitution sweep.\n\n**Forward Elimination:**\nThe goal of this stage is to eliminate the subdiagonal elements $a_i$, transforming the system into an upper bidiagonal form. This is achieved by iterating from the second row to the last row. For each row $i$ (from $2$ to $n$), we perform the operation $R_i \\leftarrow R_i - m_i R_{i-1}$, where $R_i$ is the $i$-th row of the augmented system $[A|\\mathbf{d}]$ and $m_i$ is a multiplier chosen to eliminate the subdiagonal element in that row. The original matrix elements $b_i$ and right-hand side elements $d_i$ are modified in this process. Let a prime denote a modified value.\n\nFor each row $i=2, 3, \\dots, n$:\n1.  Compute the multiplier $m_i = \\frac{a_i}{b'_{i-1}}$. This requires $1$ division.\n2.  Update the main diagonal element: $b'_i = b_i - m_i c_{i-1}$. This involves $1$ multiplication and $1$ subtraction, for a total of $2$ FLOPs.\n3.  Update the right-hand side element: $d'_i = d_i - m_i d'_{i-1}$. This involves $1$ multiplication and $1$ subtraction, for a total of $2$ FLOPs.\nThe superdiagonal elements $c_i$ are not altered. Note that for the first step ($i=2$), $b'_{1} = b_1$ and $d'_{1} = d_1$.\n\nThe loop runs from $i=2$ to $n$, which is a total of $n-1$ iterations. Each iteration requires $1 + 2 + 2 = 5$ FLOPs.\nTherefore, the total FLOP count for the forward elimination phase is:\n$$ F_{\\text{forward}} = 5(n-1) $$\n\n**Backward Substitution:**\nAfter forward elimination, the system has the upper bidiagonal form:\n$$ b'_i x_i + c_i x_{i+1} = d'_i $$\nfor $i=1, \\dots, n-1$, and $b'_n x_n = d'_n$ for the last equation. We solve for $\\mathbf{x}$ by back-substituting, starting from $x_n$.\n\n1.  Solve for $x_n$: $x_n = \\frac{d'_n}{b'_n}$. This requires $1$ division.\n2.  For $i = n-1, n-2, \\dots, 1$: solve for $x_i$ using the already computed $x_{i+1}$:\n    $x_i = \\frac{d'_i - c_i x_{i+1}}{b'_i}$. Each such computation requires $1$ multiplication, $1$ subtraction, and $1$ division, for a total of $3$ FLOPs.\n\nThis loop runs for $n-1$ iterations.\nTherefore, the total FLOP count for the backward substitution phase is:\n$$ F_{\\text{backward}} = 1 + 3(n-1) $$\n\nThe total FLOP count for the Thomas algorithm is the sum of the counts for both phases:\n$$ F_{\\text{Thomas}} = F_{\\text{forward}} + F_{\\text{backward}} = 5(n-1) + (1 + 3(n-1)) = 8(n-1) + 1 = 8n - 7 $$\n\n**2. Analysis of Dense Gaussian Elimination**\n\nThis method treats the tridiagonal matrix $A$ as a general dense matrix, ignoring its sparse structure. The process involves an elimination phase to transform $A$ into an upper triangular matrix $U$ while similarly transforming the right-hand side vector $\\mathbf{d}$ into $\\mathbf{d}'$, followed by backward substitution to solve $U\\mathbf{x} = \\mathbf{d}'$.\n\n**Elimination Phase:**\nThis phase proceeds in $n-1$ steps, from $k=1$ to $n-1$. At step $k$, we use the $k$-th row to eliminate the elements below the pivot $A_{kk}$ in the $k$-th column.\n\nFor each step $k = 1, \\dots, n-1$:\n1.  For each row $i$ from $k+1$ to $n$:\n    a. Compute the multiplier $m_{ik} = \\frac{A_{ik}}{A_{kk}}$. This is $1$ division. Note that for a tridiagonal matrix, $A_{ik}=0$ for $i>k+1$, but we are treating it as dense, so we form the general algorithm.\n    b. Update the remaining elements in row $i$: For each column $j$ from $k+1$ to $n$, update $A_{ij} \\leftarrow A_{ij} - m_{ik} A_{kj}$. This is $1$ multiplication and $1$ subtraction, total $2$ FLOPs. The inner loop on $j$ runs for $n-k$ iterations.\n    c. Update the right-hand side vector: $d_i \\leftarrow d_i - m_{ik} d_k$. This costs $2$ FLOPs.\n\nThe number of rows $i$ to be updated is $n-k$.\nThus, the cost for step $k$ is:\n$$ C_k = \\sum_{i=k+1}^{n} \\left( 1 + \\sum_{j=k+1}^{n} (2) + 2 \\right) = (n-k) \\times (1 + 2(n-k) + 2) $$\nThe problem describes LU factorization followed by forward/backward substitution. A more direct way to count for solving $A\\mathbf{x}=\\mathbf{d}$ is to consider the operations on the augmented matrix $[A|\\mathbf{d}]$.\nAt step $k$:\n-   Multipliers: For each row $i=k+1, \\dots, n$, we compute one multiplier. This is $n-k$ divisions.\n-   Row updates: For each of the $n-k$ rows, we update $n-k$ elements of the matrix and $1$ element of the RHS vector. Each update is a multiplication and a subtraction (2 FLOPs). The elements to be updated are in columns $j=k+1, \\dots, n$ and the RHS. Total columns to update is $(n-k)+1$.\nCost at step $k$: $(n-k)$ divisions + $(n-k) \\times ((n-k)+1) \\times 2$ FLOPs for updates.\nTotal cost is $\\sum_{k=1}^{n-1} \\left( (n-k) + 2(n-k)(n-k+1) \\right)$.\nLet $p = n-k$. As $k$ goes from $1$ to $n-1$, $p$ goes from $n-1$ to $1$.\n$$ F_{\\text{elim}} = \\sum_{p=1}^{n-1} (p + 2p(p+1)) = \\sum_{p=1}^{n-1} (2p^2 + 3p) $$\nUsing standard summation formulas $\\sum_{p=1}^{N} p = \\frac{N(N+1)}{2}$ and $\\sum_{p=1}^{N} p^2 = \\frac{N(N+1)(2N+1)}{6}$ with $N=n-1$:\n$$ \\sum_{p=1}^{n-1} p = \\frac{(n-1)n}{2} $$\n$$ \\sum_{p=1}^{n-1} p^2 = \\frac{(n-1)n(2n-1)}{6} $$\nSo, the total FLOPs for elimination are:\n$$ F_{\\text{elim}} = 2 \\left( \\frac{(n-1)n(2n-1)}{6} \\right) + 3 \\left( \\frac{(n-1)n}{2} \\right) $$\n$$ F_{\\text{elim}} = \\frac{n(n-1)(2n-1)}{3} + \\frac{3n(n-1)}{2} = \\frac{2n(n-1)(2n-1) + 9n(n-1)}{6} $$\n$$ F_{\\text{elim}} = \\frac{n(n-1) [2(2n-1) + 9]}{6} = \\frac{n(n-1)(4n-2+9)}{6} = \\frac{n(n-1)(4n+7)}{6} $$\n$$ F_{\\text{elim}} = \\frac{(n^2-n)(4n+7)}{6} = \\frac{4n^3 + 7n^2 - 4n^2 - 7n}{6} = \\frac{4n^3 + 3n^2 - 7n}{6} $$\n\n**Backward Substitution:**\nAfter elimination, we solve the dense upper triangular system $U \\mathbf{x} = \\mathbf{d}'$.\n1.  Solve for $x_n = \\frac{d'_n}{U_{nn}}$. This is $1$ FLOP.\n2.  For $i = n-1, \\dots, 1$:\n    $x_i = \\frac{1}{U_{ii}} \\left( d'_i - \\sum_{j=i+1}^{n} U_{ij} x_j \\right)$.\n    The summation involves $n-i$ multiplications and $n-i-1$ additions. Then there is $1$ subtraction from $d'_i$ and $1$ division. Total FLOPs for each $x_i$: $(n-i) + (n-i-1+1) + 1 = 2(n-i) + 1$.\nThe total for backward substitution is the cost for $x_n$ plus the sum of costs for $x_{n-1}$ down to $x_1$:\n$$ F_{\\text{backward}} = 1 + \\sum_{i=1}^{n-1} [2(n-i)+1] $$\nLet $p=n-i$. The sum becomes $\\sum_{p=1}^{n-1} (2p+1)$.\n$$ F_{\\text{backward}} = 1 + 2\\sum_{p=1}^{n-1} p + \\sum_{p=1}^{n-1} 1 = 1 + 2\\frac{(n-1)n}{2} + (n-1) = 1 + n^2-n + n-1 = n^2 $$\nThe total FLOP count for dense Gaussian elimination is the sum of both phases:\n$$ F_{\\text{GE}} = F_{\\text{elim}} + F_{\\text{backward}} = \\frac{4n^3 + 3n^2 - 7n}{6} + n^2 = \\frac{4n^3 + 3n^2 - 7n + 6n^2}{6} = \\frac{4n^3 + 9n^2 - 7n}{6} $$\n\nIn conclusion, the exact FLOP counts are $8n-7$ for the Thomas algorithm and $\\frac{4n^3 + 9n^2 - 7n}{6}$ for dense Gaussian elimination.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 8n - 7 & \\frac{4n^3 + 9n^2 - 7n}{6} \\end{pmatrix} } $$"
        },
        {
            "introduction": "A powerful linear solver can be used for more than just finding a solution vector for a given right-hand side. This final practice demonstrates how to leverage your Thomas algorithm solver for a more sophisticated task in matrix analysis: computing a single column of a matrix inverse, $A^{-1}$, without the costly process of calculating the full inverse. By solving the system $A\\mathbf{x} = \\mathbf{e}_j$, where $\\mathbf{e}_j$ is a standard basis vector, you will unlock an efficient and widely used technique for probing the properties of large linear systems .",
            "id": "2446301",
            "problem": "Consider a nonsingular tridiagonal matrix $A \\in \\mathbb{R}^{n \\times n}$ with subdiagonal $a \\in \\mathbb{R}^{n-1}$, diagonal $b \\in \\mathbb{R}^{n}$, and superdiagonal $c \\in \\mathbb{R}^{n-1}$. You are asked to devise a method to compute a single column of the inverse matrix $A^{-1}$, namely $A^{-1}_{:,j}$ for a given column index $j$, without computing the full inverse. The Thomas algorithm is a specialized form of Gaussian elimination for tridiagonal systems that runs in linear time.\n\nStarting from the fundamental definition of the inverse matrix, the $j$-th column $A^{-1}_{:,j}$ is the unique vector $\\mathbf{x} \\in \\mathbb{R}^{n}$ that satisfies $A \\mathbf{x} = \\mathbf{e}_j$, where $\\mathbf{e}_j$ is the $j$-th standard basis vector. Your task is to implement a program that, given $(a,b,c)$ and a zero-based column index $j \\in \\{0,1,\\dots,n-1\\}$, constructs $\\mathbf{e}_j$ and solves the tridiagonal linear system $A \\mathbf{x} = \\mathbf{e}_j$ using a Thomas-algorithm-based forward elimination and back substitution. This yields $A^{-1}_{:,j}$ as $\\mathbf{x}$. You must not form the full inverse $A^{-1}$.\n\nYour program must:\n- Implement a solver for $A \\mathbf{x} = \\mathbf{d}$ where $A$ is tridiagonal with subdiagonal $a$, diagonal $b$, superdiagonal $c$, and right-hand side $\\mathbf{d}$, using a numerically stable version of the Thomas algorithm that avoids division by zero by requiring all pivots to be nonzero.\n- For each specified test case, set $\\mathbf{d} = \\mathbf{e}_j$ to obtain the desired column $A^{-1}_{:,j}$.\n- Round each component of the resulting vector to $6$ decimal places and print each with exactly $6$ digits after the decimal point.\n\nUse the following test suite, each case specified as $(a,b,c,j)$ with zero-based index $j$:\n- Test case $1$ (general, diagonally dominant): \n  - $n=5$\n  - $a = [-1,-1,-1,-1]$\n  - $b = [4,4,4,4,4]$\n  - $c = [-1,-1,-1,-1]$\n  - $j = 2$\n- Test case $2$ (symmetric positive definite with boundary column): \n  - $n=6$\n  - $a = [-1,-1,-1,-1,-1]$\n  - $b = [2,2,2,2,2,2]$\n  - $c = [-1,-1,-1,-1,-1]$\n  - $j = 0$\n- Test case $3$ (variable coefficients, last column): \n  - $n=6$\n  - $a = [0.5,-0.3,0.7,-0.2,1.1]$\n  - $b = [3.0,2.5,2.8,3.2,2.9,3.1]$\n  - $c = [-0.4,0.6,-0.5,0.9,-0.7]$\n  - $j = 5$\n- Test case $4$ (minimal dimension): \n  - $n=1$\n  - $a = []$\n  - $b = [5.0]$\n  - $c = []$\n  - $j = 0$\n\nFinal output format requirement:\n- Your program must produce a single line containing the results for all test cases in order, aggregated as a list of lists, with no spaces, and each floating-point number printed with exactly $6$ digits after the decimal point. For example, the format must be like $[\\,[x\\_1,x\\_2,\\dots],[y\\_1,y\\_2,\\dots],\\dots\\,]$ with no spaces, such as $[[0.125000,0.000000],[\\dots],\\dots]$.\n\nThere are no physical units involved. Angles are not used. All numerical answers are real numbers. The answer for each test case must be a list of floats, each rounded to $6$ decimal places, and the final program output must be a single line string in the specified format.",
            "solution": "The provided problem statement has been critically examined and found to be valid. It is scientifically sound, mathematically well-posed, and contains all necessary information for a unique solution. The problem requires the computation of a specific column of a matrix inverse, which is a standard task in computational science and engineering. We shall proceed with a rigorous solution.\n\nThe fundamental relationship defining the inverse of a nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$ is $A A^{-1} = I_n$, where $I_n$ is the $n \\times n$ identity matrix. Let $\\mathbf{x}^{(j)}$ denote the $j$-th column of $A^{-1}$ and $\\mathbf{e}_j$ denote the $j$-th column of $I_n$ (the standard basis vector with a $1$ at index $j$ and zeros elsewhere). The matrix-vector product $A \\mathbf{x}^{(j)}$ yields the $j$-th column of the product $A A^{-1}$. Therefore, computing the $j$-th column of the inverse is equivalent to solving the linear system of equations:\n$$\nA \\mathbf{x}^{(j)} = \\mathbf{e}_j\n$$\nThe problem specifies that the matrix $A$ is tridiagonal. Such systems are efficiently solved using the Thomas algorithm, also known as the Tridiagonal Matrix Algorithm (TDMA), which is a simplified form of Gaussian elimination with a computational complexity of $O(n)$.\n\nThe algorithm proceeds in two stages: a forward elimination pass and a backward substitution pass. Let the tridiagonal matrix $A$ be represented by its subdiagonal $a \\in \\mathbb{R}^{n-1}$, main diagonal $b \\in \\mathbb{R}^{n}$, and superdiagonal $c \\in \\mathbb{R}^{n-1}$. The system of equations $A\\mathbf{x}=\\mathbf{d}$ is written as:\n$$\n\\begin{cases}\nb_0 x_0 + c_0 x_1 = d_0 & \\text{for } i=0 \\\\\na_{i-1} x_{i-1} + b_i x_i + c_i x_{i+1} = d_i & \\text{for } i=1, \\dots, n-2 \\\\\na_{n-2} x_{n-2} + b_{n-1} x_{n-1} = d_{n-1} & \\text{for } i=n-1\n\\end{cases}\n$$\nwhere all indexing is zero-based.\n\n**1. Forward Elimination**\n\nThe goal of this stage is to eliminate the subdiagonal elements, transforming the system into an upper bidiagonal form. We achieve this by modifying the coefficients of the matrix and the right-hand side vector $\\mathbf{d}$. For each row $i$, we express $x_i$ in terms of $x_{i+1}$.\n\nStarting with the first equation ($i=0$), we divide by $b_0$ (which is non-zero as $A$ is nonsingular):\n$$\nx_0 + \\frac{c_0}{b_0} x_1 = \\frac{d_0}{b_0}\n$$\nWe define new coefficients $c'_0 = \\frac{c_0}{b_0}$ and $d'_0 = \\frac{d_0}{b_0}$.\n\nFor the subsequent rows $i = 1, \\dots, n-1$, we substitute the expression for $x_{i-1}$ from the modified previous row's equation, $x_{i-1} = d'_{i-1} - c'_{i-1}x_i$, into the current row's equation $a_{i-1}x_{i-1} + b_ix_i + c_ix_{i+1} = d_i$ (with $c_{n-1}=0$ understood):\n$$\na_{i-1}(d'_{i-1} - c'_{i-1}x_i) + b_i x_i + c_i x_{i+1} = d_i\n$$\nRearranging for $x_i$ yields:\n$$\n(b_i - a_{i-1}c'_{i-1})x_i + c_i x_{i+1} = d_i - a_{i-1}d'_{i-1}\n$$\nThe term $m_i = b_i - a_{i-1}c'_{i-1}$ serves as the pivot for row $i$. For the matrices specified in the problem (diagonally dominant or symmetric positive definite), these pivots are guaranteed to be non-zero. Dividing by the pivot, we obtain the modified equation for row $i$:\n$$\nx_i + \\frac{c_i}{m_i} x_{i+1} = \\frac{d_i - a_{i-1}d'_{i-1}}{m_i}\n$$\nThis gives us the recurrence relations for the modified coefficients $c'_i$ and $d'_i$:\n$$\nc'_i = \\frac{c_i}{b_i - a_{i-1}c'_{i-1}} \\quad \\text{for } i=1, \\dots, n-2\n$$\n$$\nd'_i = \\frac{d_i - a_{i-1}d'_{i-1}}{b_i - a_{i-1}c'_{i-1}} \\quad \\text{for } i=1, \\dots, n-1\n$$\nAfter this forward pass, the original system $A\\mathbf{x}=\\mathbf{d}$ is transformed into an equivalent upper triangular system $A'\\mathbf{x}=\\mathbf{d}'$ where $A'_{ii}=1$, $A'_{i,i+1}=c'_i$, and all other elements are zero.\n\n**2. Backward Substitution**\n\nWith the system in upper bidiagonal form, we can solve for $\\mathbf{x}$ by starting from the last equation and substituting backwards.\nThe last equation is simply $x_{n-1} = d'_{n-1}$.\nFor the remaining unknowns, we use the relation $x_i + c'_i x_{i+1} = d'_i$:\n$$\nx_i = d'_i - c'_i x_{i+1} \\quad \\text{for } i = n-2, \\dots, 0\n$$\nThis procedure yields the solution vector $\\mathbf{x} = \\mathbf{x}^{(j)}$.\n\nFor each test case, we construct the vector $\\mathbf{d}=\\mathbf{e}_j$ for the specified column index $j$, apply the Thomas algorithm as described, and report the resulting vector $\\mathbf{x}^{(j)}$ with components rounded to $6$ decimal places. The special case of $n=1$ reduces to the trivial equation $b_0 x_0 = d_0$, giving the solution $x_0 = d_0 / b_0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Test case 1\n        {\n            \"a\": [-1, -1, -1, -1],\n            \"b\": [4, 4, 4, 4, 4],\n            \"c\": [-1, -1, -1, -1],\n            \"j\": 2\n        },\n        # Test case 2\n        {\n            \"a\": [-1, -1, -1, -1, -1],\n            \"b\": [2, 2, 2, 2, 2, 2],\n            \"c\": [-1, -1, -1, -1, -1],\n            \"j\": 0\n        },\n        # Test case 3\n        {\n            \"a\": [0.5, -0.3, 0.7, -0.2, 1.1],\n            \"b\": [3.0, 2.5, 2.8, 3.2, 2.9, 3.1],\n            \"c\": [-0.4, 0.6, -0.5, 0.9, -0.7],\n            \"j\": 5\n        },\n        # Test case 4\n        {\n            \"a\": [],\n            \"b\": [5.0],\n            \"c\": [],\n            \"j\": 0\n        }\n    ]\n\n    def thomas_solver(a, b, c, d):\n        \"\"\"\n        Solves a tridiagonal linear system Ax=d using the Thomas algorithm.\n        \n        Args:\n            a (list or np.ndarray): Subdiagonal elements (length n-1).\n            b (list or np.ndarray): Diagonal elements (length n).\n            c (list or np.ndarray): Superdiagonal elements (length n-1).\n            d (list or np.ndarray): Right-hand side vector (length n).\n            \n        Returns:\n            np.ndarray: The solution vector x.\n        \"\"\"\n        n = len(b)\n        \n        if n == 0:\n            return np.array([])\n        if n == 1:\n            if b[0] == 0:\n                raise ValueError(\"Division by zero in n=1 case.\")\n            return np.array([d[0] / b[0]])\n\n        # Create copies to avoid modifying input lists\n        ac = np.array(a, dtype=float)\n        bc = np.array(b, dtype=float)\n        cc = np.array(c, dtype=float)\n        dc = np.array(d, dtype=float)\n\n        c_prime = np.zeros(n - 1)\n        d_prime = np.zeros(n)\n\n        # Forward elimination phase\n        if bc[0] == 0:\n            raise ValueError(\"Pivot is zero. Thomas algorithm requires non-zero pivots.\")\n        c_prime[0] = cc[0] / bc[0]\n        d_prime[0] = dc[0] / bc[0]\n\n        for i in range(1, n):\n            denom = bc[i] - ac[i-1] * c_prime[i-1]\n            if denom == 0:\n                raise ValueError(\"Pivot is zero. Thomas algorithm requires non-zero pivots.\")\n            \n            if i < n - 1:\n                c_prime[i] = cc[i] / denom\n            \n            d_prime[i] = (dc[i] - ac[i-1] * d_prime[i-1]) / denom\n\n        # Backward substitution phase\n        x = np.zeros(n)\n        x[n-1] = d_prime[n-1]\n        for i in range(n - 2, -1, -1):\n            x[i] = d_prime[i] - c_prime[i] * x[i+1]\n            \n        return x\n\n    all_results = []\n    for case in test_cases:\n        a, b, c, j = case[\"a\"], case[\"b\"], case[\"c\"], case[\"j\"]\n        n = len(b)\n        \n        # Construct the j-th standard basis vector e_j\n        d = np.zeros(n)\n        d[j] = 1.0\n        \n        # Solve Ax = e_j to get the j-th column of A^-1\n        x = thomas_solver(a, b, c, d)\n        \n        # The problem requires rounding to 6 decimal places.\n        # The string formatting `{:.6f}` handles this rounding.\n        all_results.append(x)\n\n    # Format the final output string as specified\n    result_strings = []\n    for res in all_results:\n        # Format each number to have exactly 6 decimal places.\n        formatted_numbers = [f\"{val:.6f}\" for val in res]\n        result_strings.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}