{
    "hands_on_practices": [
        {
            "introduction": "The first step in analyzing any partial differential equation is to determine its type, as this dictates the nature of its solutions and the appropriate numerical methods for solving it. While PDEs with constant coefficients have a single type across their entire domain, many equations in engineering feature variable coefficients, causing their character to change from one region to another. This exercise  provides hands-on practice in applying the discriminant test to a PDE with spatially varying coefficients, challenging you to map the domain into elliptic, parabolic, and hyperbolic regions.",
            "id": "2380287",
            "problem": "Consider the second-order linear partial differential equation (PDE)\n$$(x^{2}-1)\\,u_{xx} + 2\\,x y\\,u_{xy} + (y^{2}-1)\\,u_{yy} = 0,$$\nposed on the entire $\\mathbb{R}^{2}$, where $u=u(x,y)$ is twice continuously differentiable. Classify this PDE as elliptic, parabolic, or hyperbolic in different regions of the $x y$-plane by analyzing its principal part from first principles. Identify the precise boundary curve that separates regions of different type and specify which side of this boundary corresponds to each type.\n\nYour final reported answer must be a single simplified analytic expression in the variables $x$ and $y$ whose zero level set is exactly the classification boundary you identify. No units are involved. Do not provide inequalities or piecewise descriptions in the final reported answer.",
            "solution": "The classification of a general second-order linear partial differential equation in two variables,\n$$A(x,y) u_{xx} + 2B(x,y) u_{xy} + C(x,y) u_{yy} + \\dots = 0,$$\ndepends on the sign of the discriminant of its principal part, which is defined as $\\Delta(x,y) = B(x,y)^{2} - A(x,y)C(x,y)$. The type of the equation at a point $(x,y)$ is determined as follows:\n-   If $\\Delta > 0$, the equation is hyperbolic.\n-   If $\\Delta = 0$, the equation is parabolic.\n-   If $\\Delta < 0$, the equation is elliptic.\n\nThe given equation is\n$$(x^{2}-1)\\,u_{xx} + 2\\,x y\\,u_{xy} + (y^{2}-1)\\,u_{yy} = 0.$$\nBy comparing this with the general form, we identify the coefficients of the principal part:\n-   $A(x,y) = x^{2}-1$\n-   $2B(x,y) = 2xy \\implies B(x,y) = xy$\n-   $C(x,y) = y^{2}-1$\n\nNow, we compute the discriminant $\\Delta(x,y)$:\n$$\n\\Delta = B^{2} - AC = (xy)^{2} - (x^{2}-1)(y^{2}-1)\n$$\nExpanding the terms, we get:\n$$\n\\Delta = x^{2}y^{2} - (x^{2}y^{2} - x^{2} - y^{2} + 1)\n$$\n$$\n\\Delta = x^{2}y^{2} - x^{2}y^{2} + x^{2} + y^{2} - 4\n$$\n$$\n\\Delta = x^{2} + y^{2} - 1\n$$\nThe sign of $\\Delta$ is determined by the sign of the expression $x^{2} + y^{2} - 1$. We analyze the three cases.\n\nCase 1: Hyperbolic region ($\\Delta > 0$)\nThe equation is hyperbolic where $x^{2} + y^{2} - 1 > 0$. This simplifies to:\n$$x^{2} + y^{2} > 1$$\nThis inequality describes the set of all points $(x,y)$ in the plane that are outside the unit circle centered at the origin.\n\nCase 2: Parabolic boundary ($\\Delta = 0$)\nThe equation is parabolic where $x^{2} + y^{2} - 1 = 0$. This simplifies to:\n$$x^{2} + y^{2} = 1$$\nThis is the equation of the unit circle centered at the origin. This curve is the boundary that separates the regions of different types.\n\nCase 3: Elliptic region ($\\Delta < 0$)\nThe equation is elliptic where $x^{2} + y^{2} - 1 < 0$. This simplifies to:\n$$x^{2} + y^{2} < 1$$\nThis inequality describes the set of all points $(x,y)$ in the open disk of radius $1$ centered at the origin, which is the region inside the unit circle.\n\nIn summary:\n-   The PDE is **hyperbolic** for all points $(x,y)$ in the region exterior to the unit circle, where $x^{2} + y^{2} > 1$.\n-   The PDE is **parabolic** on the unit circle itself, where $x^{2} + y^{2} = 1$.\n-   The PDE is **elliptic** for all points $(x,y)$ in the region interior to the unit circle, where $x^{2} + y^{2} < 1$.\n\nThe problem asks for a single analytic expression whose zero level set is the classification boundary. The boundary is defined by the condition $\\Delta = 0$, which is equivalent to $x^{2} + y^{2} - 1 = 0$. The required expression is therefore $x^{2} + y^{2} - 1$.",
            "answer": "$$\n\\boxed{x^{2} + y^{2} - 1}\n$$"
        },
        {
            "introduction": "Beyond the algebraic formula for the discriminant, a deeper understanding of PDE classification comes from its geometric interpretation through characteristic curves. These curves represent paths along which information propagates, and their nature—real and distinct (hyperbolic), real and identical (parabolic), or complex (elliptic)—defines the PDE's type. This exercise  reverses the typical problem: starting from the geometric definition of the characteristic curves, you will construct the governing PDE, providing a powerful insight into the intrinsic link between the equation's form and its geometric properties.",
            "id": "2377078",
            "problem": "In computational engineering, the classification of second-order partial differential equations (PDEs) in two independent variables is based on the discriminant of the quadratic form associated with their characteristic curves. Consider a second-order linear homogeneous PDE in two variables of the form\n$$\na(x,y)\\,u_{xx} \\;+\\; 2\\,b(x,y)\\,u_{xy} \\;+\\; c(x,y)\\,u_{yy} \\;=\\; 0,\n$$\nwhere $u=u(x,y)$, and $a(x,y)$, $b(x,y)$, and $c(x,y)$ are sufficiently smooth functions. You are told that one family of characteristic curves is given by\n$$\nx^{2} - y^{2} \\;=\\; C,\n$$\nand that the other family of characteristic curves is the orthogonal trajectories of this family. Among all such PDEs whose characteristic families coincide with these two families, select the one normalized by $a(x,y)=1$. Let the classification discriminant be\n$$\n\\Delta(x,y) \\;=\\; b(x,y)^{2} - a(x,y)\\,c(x,y).\n$$\nCompute $\\Delta(2,1)$. Provide your answer as an exact value (no rounding).",
            "solution": "The analytical procedure is as follows. First, we must determine the equation for the second family of characteristic curves. Second, we will use the defining property of characteristic curves to establish relationships between the coefficients $a$, $b$, and $c$. Third, we apply the given normalization to find the specific expressions for these coefficients. Finally, we compute the discriminant and evaluate it at the specified point.\n\n**Step 1: Determination of the second family of characteristic curves.**\nThe first family of curves is given by $\\phi(x,y) = x^{2} - y^{2} = C$, where $C$ is a constant. The gradient of $\\phi(x,y)$ is $\\nabla\\phi = \\langle\\frac{\\partial\\phi}{\\partial x}, \\frac{\\partial\\phi}{\\partial y}\\rangle = \\langle 2x, -2y \\rangle$. The vector field $\\nabla\\phi$ is everywhere normal to the level curves of $\\phi$. The tangent to a level curve has a slope $m_{1} = \\frac{dy}{dx}$ satisfying $\\frac{\\partial\\phi}{\\partial x} + \\frac{\\partial\\phi}{\\partial y}\\frac{dy}{dx} = 0$, which gives $m_{1} = -\\frac{\\phi_{x}}{\\phi_{y}} = -\\frac{2x}{-2y} = \\frac{x}{y}$.\n\nThe orthogonal trajectories will have a slope $m_{2}$ that is the negative reciprocal of $m_{1}$, so $m_{2} = -\\frac{1}{m_{1}} = -\\frac{y}{x}$. This gives the ordinary differential equation (ODE) for the orthogonal family:\n$$\n\\frac{dy}{dx} = -\\frac{y}{x}\n$$\nThis is a separable ODE. We rearrange and integrate:\n$$\n\\int \\frac{dy}{y} = -\\int \\frac{dx}{x}\n$$\n$$\n\\ln|y| = -\\ln|x| + k\n$$\nwhere $k$ is the constant of integration. Rearranging terms, we find:\n$$\n\\ln|y| + \\ln|x| = k \\implies \\ln|xy| = k \\implies xy = C'\n$$\nwhere $C' = \\pm\\exp(k)$ is a new constant. Thus, the second family of characteristic curves is given by $\\psi(x,y) = xy = C'$.\n\n**Step 2: Relation of coefficients to characteristic curves.**\nA function $\\zeta(x,y)$ defines a family of characteristic curves for the PDE $a\\,u_{xx} + 2b\\,u_{xy} + c\\,u_{yy} = 0$ if it satisfies the nonlinear first-order PDE:\n$$\na\\left(\\frac{\\partial\\zeta}{\\partial x}\\right)^{2} + 2b\\left(\\frac{\\partial\\zeta}{\\partial x}\\right)\\left(\\frac{\\partial\\zeta}{\\partial y}\\right) + c\\left(\\frac{\\partial\\zeta}{\\partial y}\\right)^{2} = 0\n$$\nWe apply this condition to both $\\phi(x,y) = x^2 - y^2$ and $\\psi(x,y) = xy$.\n\nFor $\\phi(x,y) = x^{2} - y^{2}$, the partial derivatives are $\\phi_x = 2x$ and $\\phi_y = -2y$. Substituting these into the characteristic condition yields:\n$$\na(2x)^{2} + 2b(2x)(-2y) + c(-2y)^{2} = 0\n$$\n$$\n4x^{2}a - 8xyb + 4y^{2}c = 0\n$$\nDividing by $4$, we obtain our first relation:\n$$\nx^{2}a - 2xyb + y^{2}c = 0 \\quad (1)\n$$\nFor $\\psi(x,y) = xy$, the partial derivatives are $\\psi_x = y$ and $\\psi_y = x$. Substituting these gives:\n$$\na(y)^{2} + 2b(y)(x) + c(x)^{2} = 0\n$$\nThis yields our second relation:\n$$\ny^{2}a + 2xyb + x^{2}c = 0 \\quad (2)\n$$\nWe now solve the system of linear equations for the coefficients. Adding equation (1) and equation (2):\n$$\n(x^{2}a - 2xyb + y^{2}c) + (y^{2}a + 2xyb + x^{2}c) = 0\n$$\n$$\n(x^{2}+y^{2})a + (x^{2}+y^{2})c = 0\n$$\n$$\n(x^{2}+y^{2})(a+c) = 0\n$$\nFor this to hold over the domain where $(x,y) \\neq (0,0)$, we must have $a+c = 0$, which implies $c(x,y) = -a(x,y)$.\n\nSubstituting $c = -a$ into equation (2):\n$$\ny^{2}a + 2xyb + x^{2}(-a) = 0\n$$\n$$\n(y^{2}-x^{2})a + 2xyb = 0\n$$\n$$\n2xyb = (x^{2}-y^{2})a\n$$\n$$\nb(x,y) = \\frac{x^{2}-y^{2}}{2xy} a(x,y)\n$$\n\n**Step 3: Application of normalization and discriminant calculation.**\nThe problem specifies the normalization $a(x,y) = 1$. Using the relations derived above, we find the other coefficients:\n$$\nc(x,y) = -a(x,y) = -1\n$$\n$$\nb(x,y) = \\frac{x^{2}-y^{2}}{2xy} a(x,y) = \\frac{x^{2}-y^{2}}{2xy}\n$$\nNow, we compute the discriminant $\\Delta(x,y) = b(x,y)^{2} - a(x,y)c(x,y)$:\n$$\n\\Delta(x,y) = \\left(\\frac{x^{2}-y^{2}}{2xy}\\right)^{2} - (1)(-1)\n$$\n$$\n\\Delta(x,y) = \\frac{(x^{2}-y^{2})^{2}}{4x^{2}y^{2}} + 1\n$$\nTo simplify, we find a common denominator:\n$$\n\\Delta(x,y) = \\frac{x^{4} - 2x^{2}y^{2} + y^{4}}{4x^{2}y^{2}} + \\frac{4x^{2}y^{2}}{4x^{2}y^{2}}\n$$\n$$\n\\Delta(x,y) = \\frac{x^{4} - 2x^{2}y^{2} + y^{4} + 4x^{2}y^{2}}{4x^{2}y^{2}} = \\frac{x^{4} + 2x^{2}y^{2} + y^{4}}{4x^{2}y^{2}}\n$$\nRecognizing the numerator and denominator as perfect squares:\n$$\n\\Delta(x,y) = \\frac{(x^{2}+y^{2})^{2}}{(2xy)^{2}} = \\left(\\frac{x^{2}+y^{2}}{2xy}\\right)^{2}\n$$\nSince $\\Delta(x,y) > 0$ for $x, y \\neq 0$, the PDE is hyperbolic, which is consistent with the existence of two distinct real families of characteristics.\n\n**Step 4: Evaluation at the point $(2,1)$.**\nFinally, we substitute the coordinates $x=2$ and $y=1$ into the expression for the discriminant:\n$$\n\\Delta(2,1) = \\left(\\frac{2^{2}+1^{2}}{2(2)(1)}\\right)^{2} = \\left(\\frac{4+1}{4}\\right)^{2} = \\left(\\frac{5}{4}\\right)^{2}\n$$\n$$\n\\Delta(2,1) = \\frac{25}{16}\n$$\nThis is the required exact value.",
            "answer": "$$\\boxed{\\frac{25}{16}}$$"
        },
        {
            "introduction": "In modern computational engineering, we often face situations where we have abundant data from simulations or experiments but lack a precise governing equation. This 'inverse problem' requires us to infer the properties of the underlying physical system from its observed behavior. This practice  simulates this exact scenario, challenging you to develop a computational algorithm that analyzes a numerical solution on a grid and estimates whether the unknown underlying PDE is elliptic or hyperbolic. This exercise builds a crucial bridge from abstract mathematical theory to practical data-driven analysis and model identification.",
            "id": "2380244",
            "problem": "You are given only values of a scalar field $u(x,y)$ sampled on a uniform Cartesian grid, with no direct access to the underlying partial differential equation (PDE). The goal is to use only local correlations of the sampled data to computationally estimate whether the unknown linear, second-order PDE that $u$ plausibly satisfies in the region is elliptic or hyperbolic. The classification refers to the sign of the discriminant $D = B^2 - AC$ of the principal part $A u_{xx} + 2 B u_{xy} + C u_{yy}$: if $D < 0$ the PDE is elliptic, and if $D > 0$ the PDE is hyperbolic. All computations are dimensionless, and any trigonometric functions must use angles in radians.\n\nConstruct a program that, for each test case below, takes the provided $u(x,y)$ on a uniform grid and outputs an integer label: $+1$ if you estimate the underlying PDE to be elliptic, and $-1$ if you estimate it to be hyperbolic. Your method must rely solely on local data correlations of the sampled field values. No external input is allowed; all data are fully specified below.\n\nGrid specification common to all test cases:\n- Domain: $x \\in [-1,1]$, $y \\in [-1,1]$.\n- Grid size: $N \\times N$ with $N = 61$ uniformly spaced nodes in each direction.\n- Grid spacing: $h = \\dfrac{2}{N-1}$.\n- Coordinates: $(x_i,y_j)$ with $x_i = -1 + (i-1)h$, $y_j = -1 + (j-1)h$, for integer indices $i,j \\in \\{1,2,\\dots,N\\}$.\n\nTest suite of four scalar fields $u(x,y)$:\n- Case $\\#1$ (elliptic prototype): $u_1(x,y) = x^2 - y^2$.\n- Case $\\#2$ (hyperbolic prototype): $u_2(x,y) = \\cos\\!\\big(3 \\pi (x - y)\\big)$.\n- Case $\\#3$ (elliptic, higher-order harmonic): $u_3(x,y) = x^3 - 3 x y^2$.\n- Case $\\#4$ (hyperbolic with smooth contamination): $u_4(x,y) = \\cos\\!\\big(2 \\pi (x - y)\\big) + 0.05 \\cos(4 \\pi x)\\cos(4 \\pi y)$.\n\nRequired output:\n- Produce a single line of output containing the four integer labels in order of the test cases $(\\#1,\\#2,\\#3,\\#4)$, as a comma-separated list enclosed in square brackets, for example $[1,-1,1,-1]$.\n\nYour program must be self-contained and run without user input or external files. The final answers have no physical units, and angles must be in radians. The final output must be a single line exactly in the specified list format. The answer for each test case must be an integer from the set $\\{+1,-1\\}$.",
            "solution": "The core of the problem is to estimate the type (elliptic or hyperbolic) of an unknown linear, second-order PDE of the form\n$$\nA u_{xx} + 2B u_{xy} + C u_{yy} + \\dots = 0\n$$\ngiven only a discrete sampling of its solution, $u(x,y)$. The classification depends on the sign of the discriminant $D = B^2 - AC$. The method must be based on \"local data correlations.\" We formalize this by approximating the partial derivatives using finite difference stencils and finding the linear algebraic relationship that holds among these approximations.\n\nLet the grid data be $u_{i,j} = u(x_j, y_i)$. We approximate the second-order derivatives at interior grid points using second-order accurate central differences. The underlying PDE suggests that a linear combination of these derivatives is approximately zero. The stencils for these derivatives are:\n\\begin{align*}\ns_{xx}[u]_{i,j} &= u_{i,j+1} - 2u_{i,j} + u_{i,j-1} \\approx h^2 u_{xx} \\\\\ns_{yy}[u]_{i,j} &= u_{i+1,j} - 2u_{i,j} + u_{i,j-1} \\approx h^2 u_{yy} \\\\\ns_{xy}[u]_{i,j} &= u_{i+1,j+1} - u_{i+1,j-1} - u_{i-1,j+1} + u_{i-1,j-1} \\approx 4h^2 u_{xy}\n\\end{align*}\nSubstituting these into the PDE $A u_{xx} + 2B u_{xy} + C u_{yy} \\approx 0$ gives:\n$$\nA \\frac{s_{xx}}{h^2} + 2B \\frac{s_{xy}}{4h^2} + C \\frac{s_{yy}}{h^2} \\approx 0 \\implies A s_{xx} + \\frac{B}{2} s_{xy} + C s_{yy} \\approx 0\n$$\nWe can solve for a set of coefficients $(k_1, k_2, k_3)$ that best satisfies the linear relationship $k_1 s_{xx} + k_2 s_{xy} + k_3 s_{yy} \\approx 0$ over the grid. This is a total least squares problem, equivalent to finding the eigenvector corresponding to the smallest eigenvalue of the covariance matrix of the stencil response vectors. By comparing the regression model with the stencil-based PDE, we identify the PDE coefficients up to a scaling factor:\n$$\nA \\propto k_1, \\quad C \\propto k_3, \\quad \\frac{B}{2} \\propto k_2 \\implies B \\propto 2k_2\n$$\nThe sign of the discriminant $D = B^2 - AC$ will be the same as the sign of $(2k_2)^2 - k_1 k_3 = 4k_2^2 - k_1 k_3$. We call this the estimated discriminant, $D_{est}$.\n\nThe complete algorithm is:\n1. For each test case, generate the $N \\times N$ data grid for $u(x,y)$.\n2. Compute the stencil response grids $S_{xx}$, $S_{xy}$, and $S_{yy}$ for all interior points.\n3. Flatten these grids into vectors $\\mathbf{v}_{xx}$, $\\mathbf{v}_{xy}$, and $\\mathbf{v}_{yy}$.\n4. Following the principle of parsimony, identify any derivative fields that are numerically zero and exclude them from the analysis by setting their coefficients to zero.\n5. For the non-trivial fields, form a data matrix $M = [\\mathbf{v}_{xx} | \\mathbf{v}_{xy} | \\mathbf{v}_{yy}]$ (with null columns removed).\n6. Compute the covariance matrix $C_M = M^T M$.\n7. Find the eigenvector $\\mathbf{k}$ corresponding to the smallest eigenvalue of $C_M$. This vector contains the coefficients $(k_1, k_2, k_3)$ for the non-trivial fields.\n8. Reconstruct the full coefficient vector and calculate the discriminant estimate $D_{est} = 4k_2^2 - k_1 k_3$.\n9. Classify the PDE: if $D_{est} < 0$, the label is $+1$ (elliptic); otherwise, the label is $-1$ (hyperbolic or parabolic).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PDE classification problem for the given test cases.\n    \"\"\"\n    N = 61\n    h = 2.0 / (N - 1)\n    \n    # Create the grid coordinates\n    x_coords = np.linspace(-1, 1, N)\n    y_coords = np.linspace(-1, 1, N)\n    x, y = np.meshgrid(x_coords, y_coords, indexing='xy')\n\n    # Define the four test case functions\n    test_functions = [\n        lambda x, y: x**2 - y**2,\n        lambda x, y: np.cos(3 * np.pi * (x - y)),\n        lambda x, y: x**3 - 3 * x * y**2,\n        lambda x, y: np.cos(2 * np.pi * (x - y)) + 0.05 * np.cos(4 * np.pi * x) * np.cos(4 * np.pi * y),\n    ]\n\n    results = []\n    \n    for u_func in test_functions:\n        # 1. Sample the scalar field on the grid\n        U = u_func(x, y)\n\n        # 2. Compute stencil responses on the interior grid\n        s_xx_grid = U[1:-1, 2:] - 2 * U[1:-1, 1:-1] + U[1:-1, :-2]\n        s_yy_grid = U[2:, 1:-1] - 2 * U[1:-1, 1:-1] + U[:-2, 1:-1]\n        s_xy_grid = U[2:, 2:] - U[2:, :-2] - U[:-2, 2:] + U[:-2, :-2]\n\n        # 3. Flatten the stencil grids into vectors\n        v_xx = s_xx_grid.flatten()\n        v_yy = s_yy_grid.flatten()\n        v_xy = s_xy_grid.flatten()\n\n        # 4. Handle trivial derivatives (Principle of Parsimony)\n        norms = np.array([np.linalg.norm(v) for v in [v_xx, v_xy, v_yy]])\n        max_norm = np.max(norms)\n        \n        # Use a relative tolerance to detect null vectors\n        trivial_tol = 1e-9\n        is_trivial = norms  trivial_tol * max_norm if max_norm > 0 else np.ones(3, dtype=bool)\n\n        active_indices = np.where(~is_trivial)[0]\n        active_vectors = [v for i, v in enumerate([v_xx, v_xy, v_yy]) if i in active_indices]\n\n        # 5. Form the data matrix M\n        if not active_vectors:\n            k_full = np.zeros(3)\n        elif len(active_vectors) == 1:\n            k_full = np.zeros(3)\n        else:\n            M = np.stack(active_vectors, axis=1)\n\n            # 6. Compute the covariance-like matrix C_M = M^T * M\n            C_M = M.T @ M\n\n            # 7. Find the eigenvector corresponding to the smallest eigenvalue\n            eigenvalues, eigenvectors = np.linalg.eigh(C_M)\n            k_active = eigenvectors[:, 0]\n\n            # 8. Reconstruct the full coefficient vector\n            k_full = np.zeros(3)\n            k_full[active_indices] = k_active\n        \n        k1, k2, k3 = k_full\n        \n        # 9. Calculate the discriminant estimate\n        # From the derivation, the discriminant D = B^2 - AC\n        # is proportional to (2*k2)^2 - k1*k3.\n        discriminant = 4 * k2**2 - k1 * k3\n        \n        # 10. Classify and store the result\n        if discriminant  0:\n            results.append(1)  # Elliptic\n        else:\n            results.append(-1) # Hyperbolic (includes Parabolic case D=0)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}