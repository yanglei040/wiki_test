{
    "hands_on_practices": [
        {
            "introduction": "Finite difference formulas provide a way to approximate a derivative at a single point. However, in many scientific applications, we need to compute derivatives across an entire grid of data points. This practice guides you through the process of translating the second-order central difference scheme into a matrix operator, which is a crucial step in formulating and solving differential equations numerically. By constructing this differentiation matrix, you will gain insight into how local derivative approximations are assembled into a global system .",
            "id": "2391158",
            "problem": "A one-dimensional, uniform computational grid on the interval $[0,L]$ consists of $N+1$ nodes located at $x_i = i\\,h$ for $i \\in \\{0,1,\\dots,N\\}$, where $h = L/N$. Let $f:[0,L]\\to\\mathbb{R}$ be sufficiently smooth, with Dirichlet boundary conditions $f(0)=0$ and $f(L)=0$. Denote the nodal values by $f_i = f(x_i)$, so $f_0=f_N=0$. Consider approximating the first derivative $f'(x)$ at the interior nodes $x_i$ for $i \\in \\{1,2,\\dots,N-1\\}$ by a linear combination of nearby nodal values that is second-order accurate in $h$.\n\nUsing only Taylor series expansions of $f(x)$ about an interior node and the definition of the first derivative, derive the second-order accurate, centered finite difference approximation for $f'(x_i)$ and assemble the corresponding linear mapping $D \\in \\mathbb{R}^{(N-1)\\times(N-1)}$ that sends the interior value vector $\\mathbf{f} = [\\,f_1,\\;f_2,\\;\\dots,\\;f_{N-1}\\,]^{\\top}$ to the vector of derivative approximations $\\mathbf{g} = [\\,g_1,\\;g_2,\\;\\dots,\\;g_{N-1}\\,]^{\\top}$, where $g_i \\approx f'(x_i)$. Carefully account for the Dirichlet boundary values $f_0=f_N=0$ when constructing the first and last rows.\n\nFinally, for the specific case $L=1$ and $N=5$, provide the explicit matrix $D$. Your final answer must be a single analytical expression. No rounding is required, and no units are involved.",
            "solution": "The problem as stated is well-posed, scientifically grounded, and contains all necessary information for a unique solution. It is a standard exercise in numerical analysis. We shall proceed with the derivation.\n\nThe objective is to find a second-order accurate finite difference approximation for the first derivative, $f'(x_i)$, at an interior node $x_i$ of a uniform grid. We are given a sufficiently smooth function $f(x)$ and utilize Taylor series expansions around the point $x_i$. The grid points are defined as $x_j = j\\,h$, where $h = L/N$. The nodal values are denoted $f_j = f(x_j)$.\n\nLet us consider the Taylor series expansions for $f(x)$ at the neighboring points $x_{i+1} = x_i + h$ and $x_{i-1} = x_i - h$:\n$$f(x_{i+1}) = f(x_i) + h f'(x_i) + \\frac{h^2}{2!} f''(x_i) + \\frac{h^3}{3!} f'''(x_i) + O(h^4)$$\n$$f(x_{i-1}) = f(x_i) - h f'(x_i) + \\frac{h^2}{2!} f''(x_i) - \\frac{h^3}{3!} f'''(x_i) + O(h^4)$$\nUsing the notation $f_j = f(x_j)$, these can be written as:\n$$f_{i+1} = f_i + h f'(x_i) + \\frac{h^2}{2} f''(x_i) + \\frac{h^3}{6} f'''(x_i) + \\dots$$\n$$f_{i-1} = f_i - h f'(x_i) + \\frac{h^2}{2} f''(x_i) - \\frac{h^3}{6} f'''(x_i) + \\dots$$\n\nTo isolate the first derivative term, $f'(x_i)$, we subtract the second expansion from the first:\n$$f_{i+1} - f_{i-1} = (f_i - f_i) + (h - (-h))f'(x_i) + \\left(\\frac{h^2}{2} - \\frac{h^2}{2}\\right)f''(x_i) + \\left(\\frac{h^3}{6} - \\left(-\\frac{h^3}{6}\\right)\\right)f'''(x_i) + \\dots$$\n$$f_{i+1} - f_{i-1} = 2h f'(x_i) + \\frac{h^3}{3} f'''(x_i) + O(h^5)$$\nSolving for $f'(x_i)$, we obtain:\n$$f'(x_i) = \\frac{f_{i+1} - f_{i-1}}{2h} - \\frac{h^2}{6} f'''(x_i) + O(h^4)$$\nThe finite difference approximation, which we denote as $g_i$, is the first term on the right-hand side:\n$$g_i = \\frac{f_{i+1} - f_{i-1}}{2h}$$\nThis is the centered finite difference formula for the first derivative. The leading term of the truncation error, $T_i = f'(x_i) - g_i$, is $-\\frac{h^2}{6} f'''(x_i)$. Since the error is proportional to $h^2$, the approximation is second-order accurate, as required.\n\nNext, we construct the differentiation matrix $D \\in \\mathbb{R}^{(N-1)\\times(N-1)}$ that maps the vector of interior nodal values $\\mathbf{f} = [f_1, f_2, \\dots, f_{N-1}]^{\\top}$ to the vector of derivative approximations $\\mathbf{g} = [g_1, g_2, \\dots, g_{N-1}]^{\\top}$, such that $\\mathbf{g} = D\\mathbf{f}$. The components of $\\mathbf{g}$ are given by $g_i = \\sum_{j=1}^{N-1} D_{ij}f_j$.\n\nFor a generic interior node $i \\in \\{2, 3, \\dots, N-2\\}$, the formula $g_i = \\frac{1}{2h}(-f_{i-1} + f_{i+1})$ involves only interior nodal values. The corresponding row $i$ of the matrix $D$ will have non-zero elements only at columns $j=i-1$ and $j=i+1$:\n$$D_{i,i-1} = -\\frac{1}{2h}, \\quad D_{i,i} = 0, \\quad D_{i,i+1} = \\frac{1}{2h}$$\n\nWe must carefully handle the first and last rows of the matrix, which correspond to $i=1$ and $i=N-1$, incorporating the Dirichlet boundary conditions $f_0=0$ and $f_N=0$.\n\nFor the first row ($i=1$):\n$$g_1 = \\frac{f_{1+1} - f_{1-1}}{2h} = \\frac{f_2 - f_0}{2h}$$\nGiven $f_0=0$, this simplifies to:\n$$g_1 = \\frac{f_2}{2h} = (0)f_1 + \\left(\\frac{1}{2h}\\right)f_2 + (0)f_3 + \\dots$$\nThus, the first row of $D$ is $[0, \\frac{1}{2h}, 0, \\dots, 0]$. Specifically, $D_{1,1}=0$ and $D_{1,2}=\\frac{1}{2h}$.\n\nFor the last row ($i=N-1$):\n$$g_{N-1} = \\frac{f_{(N-1)+1} - f_{(N-1)-1}}{2h} = \\frac{f_N - f_{N-2}}{2h}$$\nGiven $f_N=0$, this simplifies to:\n$$g_{N-1} = \\frac{-f_{N-2}}{2h} = \\dots + (0)f_{N-3} + \\left(-\\frac{1}{2h}\\right)f_{N-2} + (0)f_{N-1}$$\nThus, the last row of $D$ is $[0, \\dots, 0, -\\frac{1}{2h}, 0]$. Specifically, $D_{N-1,N-2}=-\\frac{1}{2h}$ and $D_{N-1,N-1}=0$.\n\nThe general structure of the $(N-1) \\times (N-1)$ matrix $D$ is:\n$$\nD = \\frac{1}{2h}\n\\begin{pmatrix}\n0 & 1 & 0 & \\cdots & 0 & 0 & 0 \\\\\n-1 & 0 & 1 & \\cdots & 0 & 0 & 0 \\\\\n0 & -1 & 0 & \\cdots & 0 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 0 & 1 & 0 \\\\\n0 & 0 & 0 & \\cdots & -1 & 0 & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & -1 & 0\n\\end{pmatrix}\n$$\n\nFinally, we are asked to provide the explicit matrix for the specific case $L=1$ and $N=5$.\nThe step size is $h = L/N = 1/5$.\nThe pre-factor is $\\frac{1}{2h} = \\frac{1}{2(1/5)} = \\frac{5}{2}$.\nThe number of interior nodes is $N-1 = 4$, so the matrix $D$ will be of size $4 \\times 4$.\nThe interior vector is $\\mathbf{f} = [f_1, f_2, f_3, f_4]^{\\top}$.\nUsing the structure derived above for $N-1=4$:\n$$\nD = \\frac{5}{2}\n\\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n-1 & 0 & 1 & 0 \\\\\n0 & -1 & 0 & 1 \\\\\n0 & 0 & -1 & 0\n\\end{pmatrix}\n$$\nThis is the required differentiation matrix.",
            "answer": "$$\n\\boxed{\n\\frac{5}{2}\n\\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n-1 & 0 & 1 & 0 \\\\\n0 & -1 & 0 & 1 \\\\\n0 & 0 & -1 & 0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While finite difference methods work well for smooth, analytical functions, their application to real-world experimental data presents a major challenge: noise. This exercise explores the fundamental conflict between truncation error, which decreases as the step size $h$ gets smaller, and the amplification of measurement noise, which increases as $h$ decreases. Through a computational experiment, you will discover the practical limits of numerical differentiation and learn why simply choosing the smallest possible $h$ is often not the best strategy .",
            "id": "2391185",
            "problem": "Consider the scalar function of time $y(t) = \\cos(t)$ with time measured in radians. A sensor reports noisy samples $y(t_i) = \\cos(t_i) + \\epsilon_i$ at uniformly spaced times $t_i = i h$ for $i = 0, 1, \\dots, M$, where $h = \\dfrac{2\\pi}{M}$ and $\\epsilon_i$ are independent and identically distributed (i.i.d.) zero-mean Gaussian random variables with standard deviation $\\sigma$. The exact velocity is $y'(t) = -\\sin(t)$. Define the first-order forward difference approximation to the derivative at an index $i$ by $D_f y_i = \\dfrac{y_{i+1} - y_i}{h}$, valid for $i = 0, 1, \\dots, M-1$, and the second-order centered difference approximation by $D_c y_i = \\dfrac{y_{i+1} - y_{i-1}}{2h}$, valid for $i = 1, 2, \\dots, M-1$. For any set of valid indices $\\mathcal{I}$, define the root-mean-square (RMS) error by\n$$\nE = \\sqrt{\\frac{1}{|\\mathcal{I}|} \\sum_{i \\in \\mathcal{I}} \\left(D y_i - y'(t_i)\\right)^2 }.\n$$\nYou must quantify how the RMS error scales with respect to the grid spacing $h$ and the noise magnitude $\\sigma$.\n\nUse the following test suite. For all items involving random noise, use a fixed pseudorandom number generator seed to ensure reproducibility as specified.\n\n1. Noiseless centered-difference order test (expected scaling in $h$): Use $\\sigma = 0$ and $M \\in \\{40, 80, 160, 320, 640\\}$. For each $M$, compute $E$ for the centered difference using valid indices $\\mathcal{I} = \\{1,2,\\dots,M-1\\}$. Let $p_{\\mathrm{cd},0}$ be the slope obtained by least-squares fitting of $\\log_{10} E$ as a linear function of $\\log_{10} h$ over these five points.\n\n2. Noiseless forward-difference order test (expected scaling in $h$): Use $\\sigma = 0$ and $M \\in \\{40, 80, 160, 320, 640\\}$. For each $M$, compute $E$ for the forward difference using valid indices $\\mathcal{I} = \\{0,1,\\dots,M-1\\}$. Let $p_{\\mathrm{fd},0}$ be the slope obtained by least-squares fitting of $\\log_{10} E$ as a linear function of $\\log_{10} h$ over these five points.\n\n3. Noise scaling at fixed grid (expected linear scaling in $\\sigma$): Fix $M = 2000$ and let $\\sigma \\in \\{10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}\\}$. Generate a single i.i.d. standard normal base sequence $z_i$ for $i = 0,1,\\dots,M$ using the seed $2025$, and for each $\\sigma$ set $\\epsilon_i = \\sigma z_i$. For each $\\sigma$, compute $E$ for the centered difference using valid indices $\\mathcal{I} = \\{1,2,\\dots,M-1\\}$. Let $q_{\\sigma}$ be the slope obtained by least-squares fitting of $\\log_{10} E$ as a linear function of $\\log_{10} \\sigma$ over these five points.\n\n4. Noise-dominated scaling in $h$ (expected inverse scaling in $h$): Fix $\\sigma = 10^{-3}$ and $M \\in \\{500, 800, 1200, 2000\\}$. For each $M$, generate an i.i.d. standard normal base sequence $z_i$ for $i = 0,1,\\dots,M$ using the seed $3000 + M$, set $\\epsilon_i = \\sigma z_i$, and compute $E$ for the centered difference using valid indices $\\mathcal{I} = \\{1,2,\\dots,M-1\\}$. Let $r_{h,\\mathrm{noise}}$ be the slope obtained by least-squares fitting of $\\log_{10} E$ as a linear function of $\\log_{10} h$ over these four points.\n\nAll trigonometric functions must use angles in radians. Your program must compute the four exponents $p_{\\mathrm{cd},0}$, $p_{\\mathrm{fd},0}$, $q_{\\sigma}$, and $r_{h,\\mathrm{noise}}$, in that order, and produce a single line of output containing these four values rounded to three decimal places as a comma-separated list enclosed in square brackets. For example, the output format must be exactly like\n[1.234,2.345,3.456,4.567]\nand must contain no other text.",
            "solution": "The problem presented is a well-posed exercise in computational engineering, specifically in the error analysis of finite difference methods. It is scientifically sound, and all parameters and procedures are defined with sufficient rigor to permit a unique, verifiable solution. We shall therefore proceed with the analysis and solution.\n\nThe core of the problem lies in understanding the two primary sources of error when numerically differentiating sampled data: truncation error, which is intrinsic to the finite difference approximation, and noise error, which arises from imperfections in the data itself. The total error is a combination of these two components.\n\nLet the true, smooth function be $f(t) = \\cos(t)$, and the sampled data at times $t_i = ih$ be $y_i = f(t_i) + \\epsilon_i$, where $\\epsilon_i$ is a random noise term with mean zero and standard deviation $\\sigma$. The exact derivative is $f'(t_i)$. The error of a finite difference approximation $D y_i$ is $e_i = D y_i - f'(t_i)$.\n\nWe begin with the centered difference approximation, $D_c y_i$:\n$$\nD_c y_i = \\frac{y_{i+1} - y_{i-1}}{2h} = \\frac{(f(t_{i+1}) + \\epsilon_{i+1}) - (f(t_{i-1}) + \\epsilon_{i-1})}{2h}\n$$\n$$\nD_c y_i = \\left( \\frac{f(t_{i+1}) - f(t_{i-1})}{2h} \\right) + \\left( \\frac{\\epsilon_{i+1} - \\epsilon_{i-1}}{2h} \\right)\n$$\nThe first term can be analyzed using Taylor series expansion of $f(t)$ around $t_i$:\n$$\nf(t_i \\pm h) = f(t_i) \\pm h f'(t_i) + \\frac{h^2}{2} f''(t_i) \\pm \\frac{h^3}{6} f'''(t_i) + \\mathcal{O}(h^4)\n$$\nSubtracting these expansions gives:\n$$\nf(t_{i+1}) - f(t_{i-1}) = f(t_i+h) - f(t_i-h) = 2h f'(t_i) + \\frac{h^3}{3} f'''(t_i) + \\mathcal{O}(h^5)\n$$\nSubstituting this back, we find the expression for the numerical derivative:\n$$\nD_c y_i = \\left( f'(t_i) + \\frac{h^2}{6} f'''(t_i) + \\mathcal{O}(h^4) \\right) + \\left( \\frac{\\epsilon_{i+1} - \\epsilon_{i-1}}{2h} \\right)\n$$\nThe error at point $i$ is therefore:\n$$\ne_i = D_c y_i - f'(t_i) = \\underbrace{\\frac{h^2}{6} f'''(t_i) + \\mathcal{O}(h^4)}_{\\text{Truncation Error } (E_T)} + \\underbrace{\\frac{\\epsilon_{i+1} - \\epsilon_{i-1}}{2h}}_{\\text{Noise Error } (E_N)}\n$$\nThe root-mean-square (RMS) error is $E = \\sqrt{\\mathbb{E}[e_i^2]}$, where $\\mathbb{E}[\\cdot]$ denotes averaging over the sample points and noise realizations. Assuming the truncation and noise errors are uncorrelated, the mean squared error is the sum of the mean squared errors of the components:\n$$\nE^2 \\approx \\mathbb{E}[E_T^2] + \\mathbb{E}[E_N^2]\n$$\nThe truncation error term scales as $E_T \\propto h^2$, so its contribution to the mean square error is $\\mathbb{E}[E_T^2] \\propto h^4$.\nFor the noise error, since the $\\epsilon_i$ are i.i.d. with variance $\\sigma^2$, the variance of the noise term is $\\text{Var}(\\epsilon_{i+1} - \\epsilon_{i-1}) = \\text{Var}(\\epsilon_{i+1}) + \\text{Var}(\\epsilon_{i-1}) = 2\\sigma^2$. The mean squared noise error is thus:\n$$\n\\mathbb{E}[E_N^2] = \\frac{\\mathbb{E}[(\\epsilon_{i+1} - \\epsilon_{i-1})^2]}{4h^2} = \\frac{\\text{Var}(\\epsilon_{i+1} - \\epsilon_{i-1})}{4h^2} = \\frac{2\\sigma^2}{4h^2} = \\frac{\\sigma^2}{2h^2}\n$$\nSo, the total RMS error for the centered difference scheme behaves as:\n$$\nE_{\\text{cd}} \\approx \\sqrt{A h^4 + B \\frac{\\sigma^2}{h^2}}\n$$\nwhere $A$ and $B$ are constants dependent on the function's derivatives and statistical properties.\n\nA similar analysis for the first-order forward difference, $D_f y_i = (y_{i+1} - y_i)/h$, yields a truncation error $E_T = \\frac{h}{2} f''(t_i) + \\mathcal{O}(h^2)$ and a noise error $E_N = (\\epsilon_{i+1} - \\epsilon_i)/h$. The RMS error behaves as:\n$$\nE_{\\text{fd}} \\approx \\sqrt{C h^2 + D \\frac{\\sigma^2}{h^2}}\n$$\nwhere the truncation error contribution is now first order in $h$.\n\nWith this theoretical framework, we can predict the outcomes of the four experiments.\n1.  **Noiseless centered-difference ($p_{\\mathrm{cd},0}$)**: Here, $\\sigma=0$, so $E_{\\text{cd}} \\approx \\sqrt{A h^4} \\propto h^2$. On a log-log plot of $E$ versus $h$, we have $\\log_{10} E = 2 \\log_{10} h + \\text{const}$. The slope $p_{\\mathrm{cd},0}$ is expected to be $2$.\n2.  **Noiseless forward-difference ($p_{\\mathrm{fd},0}$)**: Here, $\\sigma=0$, so $E_{\\text{fd}} \\approx \\sqrt{C h^2} \\propto h^1$. On a log-log plot, $\\log_{10} E = 1 \\log_{10} h + \\text{const}$. The slope $p_{\\mathrm{fd},0}$ is expected to be $1$.\n3.  **Noise scaling at fixed grid ($q_{\\sigma}$)**: With a fixed, small $h$, the truncation error term $A h^4$ is a small constant. For sufficiently large $\\sigma$, the noise term $B \\sigma^2/h^2$ will dominate. Thus, $E_{\\text{cd}} \\approx \\sqrt{B \\sigma^2/h^2} = \\frac{\\sqrt{B}}{h}\\sigma \\propto \\sigma$. On a log-log plot of $E$ versus $\\sigma$, we have $\\log_{10} E = 1 \\log_{10} \\sigma + \\text{const}$. The slope $q_{\\sigma}$ is expected to be $1$.\n4.  **Noise-dominated scaling in $h$ ($r_{h,\\mathrm{noise}}$)**: With fixed $\\sigma$ and small $h$, the noise term again dominates. $E_{\\text{cd}} \\approx \\sqrt{B \\sigma^2/h^2} \\propto \\frac{1}{h} = h^{-1}$. On a log-log plot of $E$ versus $h$, we have $\\log_{10} E = -1 \\log_{10} h + \\text{const}$. The slope $r_{h,\\mathrm{noise}}$ is expected to be $-1$.\n\nThe following Python code implements these four tests to numerically verify these theoretical scaling laws.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef compute_rms_error(approx_deriv, exact_deriv):\n    \"\"\"Computes the root-mean-square error.\"\"\"\n    return np.sqrt(np.mean((approx_deriv - exact_deriv)**2))\n\ndef run_noiseless_centered_test():\n    \"\"\"\n    Test 1: Noiseless centered-difference order test.\n    Computes the scaling exponent of error with respect to h for a\n    noiseless centered-difference scheme. Expected value is 2.\n    \"\"\"\n    M_values = np.array([40, 80, 160, 320, 640])\n    h_values = 2 * np.pi / M_values\n    error_values = []\n\n    for M in M_values:\n        h = 2 * np.pi / M\n        t = np.linspace(0, 2 * np.pi, M + 1)\n        y = np.cos(t)\n\n        # Valid indices for centered difference are 1, ..., M-1\n        indices = np.arange(1, M)\n        y_cd = (y[indices + 1] - y[indices - 1]) / (2 * h)\n        \n        t_subset = t[indices]\n        y_prime_exact = -np.sin(t_subset)\n        \n        error = compute_rms_error(y_cd, y_prime_exact)\n        error_values.append(error)\n\n    log_h = np.log10(h_values)\n    log_E = np.log10(np.array(error_values))\n    slope, _, _, _, _ = linregress(log_h, log_E)\n    return slope\n\ndef run_noiseless_forward_test():\n    \"\"\"\n    Test 2: Noiseless forward-difference order test.\n    Computes the scaling exponent of error with respect to h for a\n    noiseless forward-difference scheme. Expected value is 1.\n    \"\"\"\n    M_values = np.array([40, 80, 160, 320, 640])\n    h_values = 2 * np.pi / M_values\n    error_values = []\n\n    for M in M_values:\n        h = 2 * np.pi / M\n        t = np.linspace(0, 2 * np.pi, M + 1)\n        y = np.cos(t)\n\n        # Valid indices for forward difference are 0, ..., M-1\n        indices = np.arange(0, M)\n        y_fd = (y[indices + 1] - y[indices]) / h\n        \n        t_subset = t[indices]\n        y_prime_exact = -np.sin(t_subset)\n        \n        error = compute_rms_error(y_fd, y_prime_exact)\n        error_values.append(error)\n        \n    log_h = np.log10(h_values)\n    log_E = np.log10(np.array(error_values))\n    slope, _, _, _, _ = linregress(log_h, log_E)\n    return slope\n\ndef run_noise_sigma_scaling_test():\n    \"\"\"\n    Test 3: Noise scaling at fixed grid.\n    Computes the scaling exponent of error with respect to sigma for a\n    fixed grid. Expected value is 1.\n    \"\"\"\n    M = 2000\n    sigma_values = np.array([1e-6, 1e-5, 1e-4, 1e-3, 1e-2])\n    error_values = []\n    \n    h = 2 * np.pi / M\n    t = np.linspace(0, 2 * np.pi, M + 1)\n    y_true = np.cos(t)\n\n    # Generate a single base noise sequence\n    rng = np.random.default_rng(seed=2025)\n    z = rng.normal(0, 1, size=M + 1)\n\n    for sigma in sigma_values:\n        epsilon = sigma * z\n        y_noisy = y_true + epsilon\n\n        # Valid indices for centered difference are 1, ..., M-1\n        indices = np.arange(1, M)\n        y_cd = (y_noisy[indices + 1] - y_noisy[indices - 1]) / (2 * h)\n        \n        t_subset = t[indices]\n        y_prime_exact = -np.sin(t_subset)\n        \n        error = compute_rms_error(y_cd, y_prime_exact)\n        error_values.append(error)\n        \n    log_sigma = np.log10(sigma_values)\n    log_E = np.log10(np.array(error_values))\n    slope, _, _, _, _ = linregress(log_sigma, log_E)\n    return slope\n\ndef run_noise_h_scaling_test():\n    \"\"\"\n    Test 4: Noise-dominated scaling in h.\n    Computes the scaling exponent of error with respect to h in the\n    noise-dominated regime. Expected value is -1.\n    \"\"\"\n    M_values = np.array([500, 800, 1200, 2000])\n    h_values = 2 * np.pi / M_values\n    sigma = 1e-3\n    error_values = []\n\n    for M in M_values:\n        h = 2 * np.pi / M\n        t = np.linspace(0, 2 * np.pi, M + 1)\n        y_true = np.cos(t)\n        \n        # Generate new noise for each M with specified seed\n        rng = np.random.default_rng(seed=3000 + M)\n        epsilon = sigma * rng.normal(0, 1, size=M + 1)\n        y_noisy = y_true + epsilon\n\n        # Valid indices for centered difference are 1, ..., M-1\n        indices = np.arange(1, M)\n        y_cd = (y_noisy[indices + 1] - y_noisy[indices - 1]) / (2 * h)\n        \n        t_subset = t[indices]\n        y_prime_exact = -np.sin(t_subset)\n        \n        error = compute_rms_error(y_cd, y_prime_exact)\n        error_values.append(error)\n\n    log_h = np.log10(h_values)\n    log_E = np.log10(np.array(error_values))\n    slope, _, _, _, _ = linregress(log_h, log_E)\n    return slope\n\ndef solve():\n    \"\"\"\n    Executes the four test cases and prints the results in the\n    specified format.\n    \"\"\"\n    p_cd_0 = run_noiseless_centered_test()\n    p_fd_0 = run_noiseless_forward_test()\n    q_sigma = run_noise_sigma_scaling_test()\n    r_h_noise = run_noise_h_scaling_test()\n\n    results = [p_cd_0, p_fd_0, q_sigma, r_h_noise]\n    \n    # Format the results to three decimal places and print\n    formatted_results = [f\"{res:.3f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Run the solution\nsolve()\n```"
        },
        {
            "introduction": "So far, we have derived schemes directly from Taylor series expansions. Is it possible to achieve a much higher order of accuracy without deriving complex, wide-stencil formulas from scratch? This practice introduces Richardson extrapolation, a powerful and general technique for combining results from a simple, low-order scheme to systematically cancel error terms and produce a far more accurate estimate. You will construct a sixth-order accurate scheme from a basic second-order operator, demonstrating a practical way to boost the performance of numerical methods .",
            "id": "2391187",
            "problem": "You are given a smooth scalar function $f(x) = \\sin(x)$ and the task of constructing a sixth-order accurate numerical approximation for its first derivative $f^{\\prime}(x)$ by combining second-order central differences evaluated at three step sizes. Let $D(h)$ denote the second-order central difference approximation to $f^{\\prime}(x)$ at point $x$ with step size $h$, defined by\n$$\nD(h) = \\frac{f(x+h) - f(x-h)}{2h}.\n$$\nUsing only the definition above and first principles (for example, Taylor series expansions about $x$), determine constants that form a linear combination of $D(h)$, $D\\left(\\frac{h}{2}\\right)$, and $D\\left(\\frac{h}{4}\\right)$ whose truncation error is of order $h^{6}$, and implement a program that computes this sixth-order approximation for specified input values.\n\nYour program must evaluate the absolute numerical error, defined as\n$$\nE(x,h) = \\left| \\widehat{f^{\\prime}}(x;h) - \\cos(x) \\right|,\n$$\nwhere $\\widehat{f^{\\prime}}(x;h)$ is your sixth-order approximation constructed from $D(h)$, $D\\left(\\frac{h}{2}\\right)$, and $D\\left(\\frac{h}{4}\\right)$, and $\\cos(x)$ is the exact derivative of $\\sin(x)$. All angles are to be in radians.\n\nTest Suite:\n- Case $1$: $x = 1.0$, $h = 0.2$ (radians).\n- Case $2$: $x = 0.0$, $h = 0.2$ (radians).\n- Case $3$: $x = \\frac{\\pi}{4}$, $h = 0.1$ (radians).\n- Case $4$: $x = \\frac{\\pi}{2}$, $h = 0.4$ (radians).\n- Case $5$: $x = 10.0$, $h = 0.05$ (radians).\n\nFinal Output Format:\n- Your program should produce a single line of output containing the absolute errors $E(x,h)$ for the five cases, in order, as a comma-separated list enclosed in square brackets.\n- Each number must be rounded to $10$ decimal places.\n- For example: $[e_1,e_2,e_3,e_4,e_5]$ where each $e_i$ is a float formatted to $10$ decimal places.",
            "solution": "The problem requires the construction of a sixth-order accurate finite difference approximation for the first derivative of a function $f(x)$ using a linear combination of second-order central difference operators evaluated at different step sizes. This is a standard application of Richardson extrapolation.\n\nFirst, we must validate the problem statement.\n\nStep 1: Extract Givens\n- Function: $f(x) = \\sin(x)$.\n- Second-order central difference operator: $D(h) = \\frac{f(x+h) - f(x-h)}{2h}$.\n- Sixth-order approximation to be constructed: $\\widehat{f^{\\prime}}(x;h)$, a linear combination of $D(h)$, $D\\left(\\frac{h}{2}\\right)$, and $D\\left(\\frac{h}{4}\\right)$.\n- Absolute numerical error definition: $E(x,h) = \\left| \\widehat{f^{\\prime}}(x;h) - \\cos(x) \\right|$.\n- Test Cases:\n  - Case 1: $x = 1.0$, $h = 0.2$.\n  - Case 2: $x = 0.0$, $h = 0.2$.\n  - Case 3: $x = \\frac{\\pi}{4}$, $h = 0.1$.\n  - Case 4: $x = \\frac{\\pi}{2}$, $h = 0.4$.\n  - Case 5: $x = 10.0$, $h = 0.05$.\n\nStep 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a classic exercise in numerical analysis, specifically Richardson extrapolation for numerical differentiation. The principles are well-established and mathematically sound.\n- **Well-Posed:** The problem is well-posed. It asks for a set of constants to achieve a specific order of accuracy, which leads to a solvable system of linear equations. A unique and meaningful solution exists.\n- **Objective:** The problem is stated objectively with precise mathematical definitions and no subjective language.\n\nStep 3: Verdict and Action\nThe problem is valid. It is a standard, well-posed problem in computational science. We proceed to the solution.\n\nThe foundation of the method is the Taylor series expansion of the function $f(x)$ around a point $x$. Assuming the function is sufficiently smooth, we have:\n$$ f(x+h) = f(x) + hf^{\\prime}(x) + \\frac{h^2}{2!}f^{\\prime\\prime}(x) + \\frac{h^3}{3!}f^{\\prime\\prime\\prime}(x) + \\frac{h^4}{4!}f^{(4)}(x) + \\frac{h^5}{5!}f^{(5)}(x) + \\frac{h^6}{6!}f^{(6)}(x) + \\frac{h^7}{7!}f^{(7)}(x) + \\mathcal{O}(h^8) $$\n$$ f(x-h) = f(x) - hf^{\\prime}(x) + \\frac{h^2}{2!}f^{\\prime\\prime}(x) - \\frac{h^3}{3!}f^{\\prime\\prime\\prime}(x) + \\frac{h^4}{4!}f^{(4)}(x) - \\frac{h^5}{5!}f^{(5)}(x) + \\frac{h^6}{6!}f^{(6)}(x) - \\frac{h^7}{7!}f^{(7)}(x) + \\mathcal{O}(h^8) $$\nSubtracting the second expansion from the first eliminates the even-powered terms in $h$:\n$$ f(x+h) - f(x-h) = 2hf^{\\prime}(x) + 2\\frac{h^3}{3!}f^{\\prime\\prime\\prime}(x) + 2\\frac{h^5}{5!}f^{(5)}(x) + 2\\frach^7}{7!}f^{(7)}(x) + \\mathcal{O}(h^9) $$\nDividing by $2h$ gives the expression for the central difference operator $D(h)$:\n$$ D(h) = \\frac{f(x+h) - f(x-h)}{2h} = f^{\\prime}(x) + \\frac{h^2}{6}f^{\\prime\\prime\\prime}(x) + \\frac{h^4}{120}f^{(5)}(x) + \\frac{h^6}{5040}f^{(7)}(x) + \\mathcal{O}(h^8) $$\nThis confirms that $D(h)$ is a second-order accurate approximation to $f^{\\prime}(x)$, as the leading error term is proportional to $h^2$. We can write this error structure more compactly as:\n$$ D(h) = f^{\\prime}(x) + c_2 h^2 + c_4 h^4 + c_6 h^6 + \\mathcal{O}(h^8) $$\nwhere $c_{2k} = \\frac{f^{(2k+1)}(x)}{(2k+1)!}$.\n\nWe seek a sixth-order approximation $\\widehat{f^{\\prime}}(x;h)$ as a linear combination of approximations at three different step sizes:\n$$ \\widehat{f^{\\prime}}(x;h) = \\alpha D(h) + \\beta D\\left(\\frac{h}{2}\\right) + \\gamma D\\left(\\frac{h}{4}\\right) $$\nSubstituting the series expansions for each term:\n$$ \\widehat{f^{\\prime}} = \\alpha \\left( f^{\\prime}(x) + c_2 h^2 + c_4 h^4 + \\dots \\right) + \\beta \\left( f^{\\prime}(x) + c_2 \\left(\\frac{h}{2}\\right)^2 + c_4 \\left(\\frac{h}{2}\\right)^4 + \\dots \\right) + \\gamma \\left( f^{\\prime}(x) + c_2 \\left(\\frac{h}{4}\\right)^2 + c_4 \\left(\\frac{h}{4}\\right)^4 + \\dots \\right) $$\nTo ensure the approximation is accurate to sixth-order, we must require that the combined expression for $\\widehat{f^{\\prime}}$ is equal to $f^{\\prime}(x) + \\mathcal{O}(h^6)$. This is achieved by collecting terms by powers of $h$ and setting the coefficients of unwanted terms to zero.\n\\begin{align*}\n\\widehat{f^{\\prime}} &= (\\alpha + \\beta + \\gamma)f^{\\prime}(x) \\\\\n&+ \\left(\\alpha + \\frac{\\beta}{4} + \\frac{\\gamma}{16}\\right)c_2 h^2 \\\\\n&+ \\left(\\alpha + \\frac{\\beta}{16} + \\frac{\\gamma}{256}\\right)c_4 h^4 \\\\\n&+ \\mathcal{O}(h^6)\n\\end{align*}\nThis leads to a system of three linear equations for the unknown constants $\\alpha$, $\\beta$, and $\\gamma$:\n1. Coefficient of $f^{\\prime}(x)$ must be $1$: $\\alpha + \\beta + \\gamma = 1$\n2. Coefficient of $h^2$ must be $0$: $\\alpha + \\frac{1}{4}\\beta + \\frac{1}{16}\\gamma = 0$\n3. Coefficient of $h^4$ must be $0$: $\\alpha + \\frac{1}{16}\\beta + \\frac{1}{256}\\gamma = 0$\n\nWe solve this system. For clarity, multiply the second equation by $16$ and the third by $256$:\n\\begin{align*}\n(1) \\quad \\alpha + \\beta + \\gamma &= 1 \\\\\n(2) \\quad 16\\alpha + 4\\beta + \\gamma &= 0 \\\\\n(3) \\quad 256\\alpha + 16\\beta + \\gamma &= 0\n\\end{align*}\nSubtracting equation (2) from (3):\n$$ (256 - 16)\\alpha + (16 - 4)\\beta = 0 \\implies 240\\alpha + 12\\beta = 0 \\implies \\beta = -20\\alpha $$\nSubtracting equation (1) from (2):\n$$ 15\\alpha + 3\\beta = -1 $$\nSubstitute $\\beta = -20\\alpha$ into this result:\n$$ 15\\alpha + 3(-20\\alpha) = -1 \\implies 15\\alpha - 60\\alpha = -1 \\implies -45\\alpha = -1 \\implies \\alpha = \\frac{1}{45} $$\nNow we find $\\beta$ and $\\gamma$:\n$$ \\beta = -20\\alpha = -20 \\cdot \\frac{1}{45} = -\\frac{20}{45} = -\\frac{4}{9} $$\n$$ \\gamma = 1 - \\alpha - \\beta = 1 - \\frac{1}{45} - \\left(-\\frac{20}{45}\\right) = 1 + \\frac{19}{45} = \\frac{45+19}{45} = \\frac{64}{45} $$\nThus, the sixth-order accurate approximation is:\n$$ \\widehat{f^{\\prime}}(x;h) = \\frac{1}{45}D(h) - \\frac{20}{45}D\\left(\\frac{h}{2}\\right) + \\frac{64}{45}D\\left(\\frac{h}{4}\\right) $$\nThis expression will be implemented to compute the numerical derivative for the given test cases. The absolute error is then computed as $E(x,h) = \\left| \\widehat{f^{\\prime}}(x;h) - \\cos(x) \\right|$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical differentiation problem by constructing and applying a\n    sixth-order accurate finite difference scheme.\n    \"\"\"\n\n    # Define the scalar function f(x) and its exact derivative\n    f = np.sin\n    f_prime_exact_func = np.cos\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 0.2),\n        (0.0, 0.2),\n        (np.pi / 4, 0.1),\n        (np.pi / 2, 0.4),\n        (10.0, 0.05)\n    ]\n\n    results = []\n\n    def central_difference(func, x_val, h_val):\n        \"\"\"\n        Computes the second-order central difference approximation D(h).\n        \"\"\"\n        return (func(x_val + h_val) - func(x_val - h_val)) / (2 * h_val)\n\n    # Coefficients for the sixth-order approximation\n    # f_hat' = a * D(h) + b * D(h/2) + g * D(h/4)\n    # The derivation shows:\n    # a = 1/45\n    # b = -20/45\n    # g = 64/45\n    alpha = 1.0 / 45.0\n    beta = -20.0 / 45.0\n    gamma = 64.0 / 45.0\n\n    for x, h in test_cases:\n        # Evaluate the second-order central differences at h, h/2, and h/4\n        d_h = central_difference(f, x, h)\n        d_h_2 = central_difference(f, x, h / 2.0)\n        d_h_4 = central_difference(f, x, h / 4.0)\n        \n        # Construct the sixth-order approximation using the derived coefficients\n        f_prime_approx = alpha * d_h + beta * d_h_2 + gamma * d_h_4\n        \n        # Calculate the exact derivative\n        f_prime_exact = f_prime_exact_func(x)\n        \n        # Calculate the absolute numerical error\n        error = np.abs(f_prime_approx - f_prime_exact)\n        \n        results.append(error)\n\n    # Final print statement in the exact required format.\n    # Each number is rounded to 10 decimal places.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}