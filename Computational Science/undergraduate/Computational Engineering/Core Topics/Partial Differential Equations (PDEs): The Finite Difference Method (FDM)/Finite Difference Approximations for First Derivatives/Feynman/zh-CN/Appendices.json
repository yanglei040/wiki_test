{
    "hands_on_practices": [
        {
            "introduction": "我们已经学习了如何使用中心差分等局部公式来近似一个点的导数。但是，在求解微分方程等实际问题时，我们通常需要同时计算一个空间网格上所有点的导数。这个练习旨在将局部近似的思想提升到全局操作的层面。你将推导出一个“微分矩阵”，它能将整个函数值向量一次性转换为其导数的近似向量，这是一个将微积分问题转化为线性代数问题的关键步骤 。",
            "id": "2391158",
            "problem": "在区间 $[0,L]$ 上的一个一维均匀计算网格由 $N+1$ 个节点组成，节点位置为 $x_i = i\\,h$，$i \\in \\{0,1,\\dots,N\\}$，其中 $h = L/N$。设 $f:[0,L]\\to\\mathbb{R}$ 为一个足够光滑的函数，具有狄利克雷边界条件 $f(0)=0$ 和 $f(L)=0$。将节点值记为 $f_i = f(x_i)$，因此 $f_0=f_N=0$。考虑在内部节点 $x_i$ ($i \\in \\{1,2,\\dots,N-1\\}$) 处，通过附近节点值的线性组合来逼近一阶导数 $f'(x)$，要求该逼近在 $h$ 上具有二阶精度。\n\n仅使用 $f(x)$ 在一个内部节点处的泰勒级数展开和一阶导数的定义，推导 $f'(x_i)$ 的二阶精度中心有限差分近似，并构建相应的线性映射 $D \\in \\mathbb{R}^{(N-1)\\times(N-1)}$，该映射将内部值向量 $\\mathbf{f} = [\\,f_1,\\;f_2,\\;\\dots,\\;f_{N-1}\\,]^{\\top}$ 变换为导数近似值向量 $\\mathbf{g} = [\\,g_1,\\;g_2,\\;\\dots,\\;g_{N-1}\\,]^{\\top}$，其中 $g_i \\approx f'(x_i)$。在构建第一行和最后一行时，请仔细考虑狄利克雷边界值 $f_0=f_N=0$。\n\n最后，对于 $L=1$ 和 $N=5$ 的特定情况，给出矩阵 $D$ 的显式形式。你的最终答案必须是单一的解析表达式。不需要四舍五入，也不涉及单位。",
            "solution": "所述问题是适定的，具有科学依据，并包含了得到唯一解所需的所有信息。这是数值分析中的一个标准练习。我们开始进行推导。\n\n目标是找到在均匀网格的内部节点 $x_i$ 处，一阶导数 $f'(x_i)$ 的二阶精度有限差分近似。我们给定一个足够光滑的函数 $f(x)$，并利用在点 $x_i$ 附近的泰勒级数展开。网格点定义为 $x_j = j\\,h$，其中 $h = L/N$。节点值记为 $f_j = f(x_j)$。\n\n让我们考虑 $f(x)$ 在相邻点 $x_{i+1} = x_i + h$ 和 $x_{i-1} = x_i - h$ 处的泰勒级数展开：\n$$f(x_{i+1}) = f(x_i) + h f'(x_i) + \\frac{h^2}{2!} f''(x_i) + \\frac{h^3}{3!} f'''(x_i) + O(h^4)$$\n$$f(x_{i-1}) = f(x_i) - h f'(x_i) + \\frac{h^2}{2!} f''(x_i) - \\frac{h^3}{3!} f'''(x_i) + O(h^4)$$\n使用记号 $f_j = f(x_j)$，这些展开式可以写成：\n$$f_{i+1} = f_i + h f'(x_i) + \\frac{h^2}{2} f''(x_i) + \\frac{h^3}{6} f'''(x_i) + \\dots$$\n$$f_{i-1} = f_i - h f'(x_i) + \\frac{h^2}{2} f''(x_i) - \\frac{h^3}{6} f'''(x_i) + \\dots$$\n\n为了分离出一阶导数项 $f'(x_i)$，我们将第二个展开式从第一个中减去：\n$$f_{i+1} - f_{i-1} = (f_i - f_i) + (h - (-h))f'(x_i) + \\left(\\frac{h^2}{2} - \\frac{h^2}{2}\\right)f''(x_i) + \\left(\\frac{h^3}{6} - \\left(-\\frac{h^3}{6}\\right)\\right)f'''(x_i) + \\dots$$\n$$f_{i+1} - f_{i-1} = 2h f'(x_i) + \\frac{h^3}{3} f'''(x_i) + O(h^5)$$\n对 $f'(x_i)$ 求解，我们得到：\n$$f'(x_i) = \\frac{f_{i+1} - f_{i-1}}{2h} - \\frac{h^2}{6} f'''(x_i) + O(h^4)$$\n我们记作 $g_i$ 的有限差分近似是等式右边的第一项：\n$$g_i = \\frac{f_{i+1} - f_{i-1}}{2h}$$\n这是一阶导数的中心有限差分公式。截断误差 $T_i = f'(x_i) - g_i$ 的主项是 $-\\frac{h^2}{6} f'''(x_i)$。由于误差与 $h^2$ 成正比，因此该近似是二阶精度的，符合要求。\n\n接下来，我们构造微分矩阵 $D \\in \\mathbb{R}^{(N-1)\\times(N-1)}$，它将内部节点值向量 $\\mathbf{f} = [f_1, f_2, \\dots, f_{N-1}]^{\\top}$ 映射到导数近似值向量 $\\mathbf{g} = [g_1, g_2, \\dots, g_{N-1}]^{\\top}$，使得 $\\mathbf{g} = D\\mathbf{f}$。$\\mathbf{g}$ 的分量由 $g_i = \\sum_{j=1}^{N-1} D_{ij}f_j$ 给出。\n\n对于一个通用的内部节点 $i \\in \\{2, 3, \\dots, N-2\\}$，公式 $g_i = \\frac{1}{2h}(-f_{i-1} + f_{i+1})$ 只涉及内部节点值。矩阵 $D$ 的对应行 $i$ 将仅在列 $j=i-1$ 和 $j=i+1$ 处有非零元素：\n$$D_{i,i-1} = -\\frac{1}{2h}, \\quad D_{i,i} = 0, \\quad D_{i,i+1} = \\frac{1}{2h}$$\n\n我们必须仔细处理矩阵的第一行和最后一行，它们对应于 $i=1$ 和 $i=N-1$，并要结合狄利克雷边界条件 $f_0=0$ 和 $f_N=0$。\n\n对于第一行（$i=1$）：\n$$g_1 = \\frac{f_{1+1} - f_{1-1}}{2h} = \\frac{f_2 - f_0}{2h}$$\n因为 $f_0=0$，这可以简化为：\n$$g_1 = \\frac{f_2}{2h} = (0)f_1 + \\left(\\frac{1}{2h}\\right)f_2 + (0)f_3 + \\dots$$\n因此，$D$ 的第一行是 $[0, \\frac{1}{2h}, 0, \\dots, 0]$。具体来说，$D_{1,1}=0$ 且 $D_{1,2}=\\frac{1}{2h}$。\n\n对于最后一行（$i=N-1$）：\n$$g_{N-1} = \\frac{f_{(N-1)+1} - f_{(N-1)-1}}{2h} = \\frac{f_N - f_{N-2}}{2h}$$\n因为 $f_N=0$，这可以简化为：\n$$g_{N-1} = \\frac{-f_{N-2}}{2h} = \\dots + (0)f_{N-3} + \\left(-\\frac{1}{2h}\\right)f_{N-2} + (0)f_{N-1}$$\n因此，$D$ 的最后一行是 $[0, \\dots, 0, -\\frac{1}{2h}, 0]$。具体来说，$D_{N-1,N-2}=-\\frac{1}{2h}$ 且 $D_{N-1,N-1}=0$。\n\n$(N-1) \\times (N-1)$ 矩阵 $D$ 的一般结构是：\n$$\nD = \\frac{1}{2h}\n\\begin{pmatrix}\n0 & 1 & 0 & \\cdots & 0 & 0 & 0 \\\\\n-1 & 0 & 1 & \\cdots & 0 & 0 & 0 \\\\\n0 & -1 & 0 & \\cdots & 0 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 0 & 1 & 0 \\\\\n0 & 0 & 0 & \\cdots & -1 & 0 & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & -1 & 0\n\\end{pmatrix}\n$$\n\n最后，我们被要求给出在特定情况 $L=1$ 和 $N=5$ 下的显式矩阵。\n步长为 $h = L/N = 1/5$。\n前置因子为 $\\frac{1}{2h} = \\frac{1}{2(1/5)} = \\frac{5}{2}$。\n内部节点的数量是 $N-1 = 4$，所以矩阵 $D$ 的大小将是 $4 \\times 4$。\n内部向量是 $\\mathbf{f} = [f_1, f_2, f_3, f_4]^{\\top}$。\n使用上面为 $N-1=4$ 推导出的结构：\n$$\nD = \\frac{5}{2}\n\\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n-1 & 0 & 1 & 0 \\\\\n0 & -1 & 0 & 1 \\\\\n0 & 0 & -1 & 0\n\\end{pmatrix}\n$$\n这就是所要求的微分矩阵。",
            "answer": "$$\n\\boxed{\n\\frac{5}{2}\n\\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n-1 & 0 & 1 & 0 \\\\\n0 & -1 & 0 & 1 \\\\\n0 & 0 & -1 & 0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "理论上的公式在处理干净、平滑的函数时表现优异，但现实世界的数据往往充满噪声。无论是来自传感器的读数还是实验测量，微小的随机误差都可能存在。这个实践将带你探索一个至关重要的问题：当数据含有噪声时，有限差分法的表现会如何？通过一个模拟实验，你将亲手分析和验证噪声误差如何随着步长 $h$ 和噪声强度 $\\sigma$ 而变化，并揭示数值微分一个深刻的特性——它对噪声的敏感性 。",
            "id": "2391185",
            "problem": "考虑一个标量时间函数 $y(t) = \\cos(t)$，其中时间以弧度为单位。一个传感器在均匀间隔的时间点 $t_i = i h$（$i = 0, 1, \\dots, M$）报告带有噪声的样本 $y(t_i) = \\cos(t_i) + \\epsilon_i$。其中 $h = \\dfrac{2\\pi}{M}$，$\\epsilon_i$ 是独立同分布 (i.i.d.) 的零均值高斯随机变量，标准差为 $\\sigma$。其精确速度为 $y'(t) = -\\sin(t)$。将索引 $i$ 处导数的一阶前向差分近似定义为 $D_f y_i = \\dfrac{y_{i+1} - y_i}{h}$（对 $i = 0, 1, \\dots, M-1$ 有效），二阶中心差分近似定义为 $D_c y_i = \\dfrac{y_{i+1} - y_{i-1}}{2h}$（对 $i = 1, 2, \\dots, M-1$ 有效）。对于任何有效的索引集 $\\mathcal{I}$，将均方根 (RMS) 误差定义为\n$$\nE = \\sqrt{\\frac{1}{|\\mathcal{I}|} \\sum_{i \\in \\mathcal{I}} \\left(D y_i - y'(t_i)\\right)^2 }.\n$$\n您必须量化均方根误差如何随网格间距 $h$ 和噪声幅度 $\\sigma$ 进行尺度变换。\n\n请使用以下测试套件。对于所有涉及随机噪声的项目，请使用指定的固定伪随机数生成器种子以确保可复现性。\n\n1. 无噪声中心差分阶数测试（预期误差随 $h$ 的尺度变换关系）：使用 $\\sigma = 0$ 和 $M \\in \\{40, 80, 160, 320, 640\\}$。对每个 $M$，使用有效索引 $\\mathcal{I} = \\{1,2,\\dots,M-1\\}$ 计算中心差分的 $E$。设 $p_{\\mathrm{cd},0}$ 是将这五个点的 $\\log_{10} E$ 对 $\\log_{10} h$ 进行线性函数的最小二乘拟合得到的斜率。\n\n2. 无噪声前向差分阶数测试（预期误差随 $h$ 的尺度变换关系）：使用 $\\sigma = 0$ 和 $M \\in \\{40, 80, 160, 320, 640\\}$。对每个 $M$，使用有效索引 $\\mathcal{I} = \\{0,1,\\dots,M-1\\}$ 计算前向差分的 $E$。设 $p_{\\mathrm{fd},0}$ 是将这五个点的 $\\log_{10} E$ 对 $\\log_{10} h$ 进行线性函数的最小二乘拟合得到的斜率。\n\n3. 固定网格下的噪声尺度变换测试（预期误差随 $\\sigma$ 呈线性尺度变换）：固定 $M = 2000$，并令 $\\sigma \\in \\{10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}\\}$。使用种子 $2025$ 为 $i = 0,1,\\dots,M$ 生成一个单一的独立同分布标准正态基准序列 $z_i$，并对每个 $\\sigma$ 设置 $\\epsilon_i = \\sigma z_i$。对每个 $\\sigma$，使用有效索引 $\\mathcal{I} = \\{1,2,\\dots,M-1\\}$ 计算中心差分的 $E$。设 $q_{\\sigma}$ 是将这五个点的 $\\log_{10} E$ 对 $\\log_{10} \\sigma$ 进行线性函数的最小二乘拟合得到的斜率。\n\n4. 噪声主导下随 $h$ 的尺度变换测试（预期误差随 $h$ 呈反比尺度变换）：固定 $\\sigma = 10^{-3}$ 和 $M \\in \\{500, 800, 1200, 2000\\}$。对每个 $M$，使用种子 $3000 + M$ 为 $i = 0,1,\\dots,M$ 生成一个独立同分布标准正态基准序列 $z_i$，设置 $\\epsilon_i = \\sigma z_i$，并使用有效索引 $\\mathcal{I} = \\{1,2,\\dots,M-1\\}$ 计算中心差分的 $E$。设 $r_{h,\\mathrm{noise}}$ 是将这四个点的 $\\log_{10} E$ 对 $\\log_{10} h$ 进行线性函数的最小二乘拟合得到的斜率。\n\n所有三角函数必须使用弧度作为角度单位。您的程序必须按顺序计算四个指数 $p_{\\mathrm{cd},0}$、$p_{\\mathrm{fd},0}$、$q_{\\sigma}$ 和 $r_{h,\\mathrm{noise}}$，并生成单行输出，其中包含这四个值（四舍五入到三位小数），以逗号分隔并用方括号括起来。例如，输出格式必须与\n[1.234,2.345,3.456,4.567]\n完全一致，且不得包含任何其他文本。",
            "solution": "所给出的问题是计算工程领域中一个定义明确的练习，特别是在有限差分方法的误差分析方面。它在科学上是合理的，所有参数和过程的定义都足够严谨，可以得出一个唯一的、可验证的解。因此，我们将进行分析和求解。\n\n问题的核心在于理解对采样数据进行数值微分时误差的两个主要来源：截断误差（有限差分近似的固有误差）和噪声误差（源于数据本身的不完美性）。总误差是这两个分量的组合。\n\n设真实的光滑函数为 $f(t) = \\cos(t)$，在时间点 $t_i = ih$ 的采样数据为 $y_i = f(t_i) + \\epsilon_i$，其中 $\\epsilon_i$ 是一个均值为零、标准差为 $\\sigma$ 的随机噪声项。精确导数为 $f'(t_i)$。有限差分近似 $D y_i$ 的误差是 $e_i = D y_i - f'(t_i)$。\n\n我们从中心差分近似 $D_c y_i$ 开始：\n$$\nD_c y_i = \\frac{y_{i+1} - y_{i-1}}{2h} = \\frac{(f(t_{i+1}) + \\epsilon_{i+1}) - (f(t_{i-1}) + \\epsilon_{i-1})}{2h}\n$$\n$$\nD_c y_i = \\left( \\frac{f(t_{i+1}) - f(t_{i-1})}{2h} \\right) + \\left( \\frac{\\epsilon_{i+1} - \\epsilon_{i-1}}{2h} \\right)\n$$\n第一项可以使用 $f(t)$ 在 $t_i$ 附近的 Taylor 级数展开进行分析：\n$$\nf(t_i \\pm h) = f(t_i) \\pm h f'(t_i) + \\frac{h^2}{2} f''(t_i) \\pm \\frac{h^3}{6} f'''(t_i) + \\mathcal{O}(h^4)\n$$\n将这两个展开式相减可得：\n$$\nf(t_{i+1}) - f(t_{i-1}) = f(t_i+h) - f(t_i-h) = 2h f'(t_i) + \\frac{h^3}{3} f'''(t_i) + \\mathcal{O}(h^5)\n$$\n将其代回，我们得到数值导数的表达式：\n$$\nD_c y_i = \\left( f'(t_i) + \\frac{h^2}{6} f'''(t_i) + \\mathcal{O}(h^4) \\right) + \\left( \\frac{\\epsilon_{i+1} - \\epsilon_{i-1}}{2h} \\right)\n$$\n因此，点 $i$ 处的误差为：\n$$\ne_i = D_c y_i - f'(t_i) = \\underbrace{\\frac{h^2}{6} f'''(t_i) + \\mathcal{O}(h^4)}_{\\text{截断误差 } (E_T)} + \\underbrace{\\frac{\\epsilon_{i+1} - \\epsilon_{i-1}}{2h}}_{\\text{噪声误差 } (E_N)}\n$$\n均方根 (RMS) 误差为 $E = \\sqrt{\\mathbb{E}[e_i^2]}$，其中 $\\mathbb{E}[\\cdot]$ 表示对采样点和噪声实现的平均。假设截断误差和噪声误差不相关，则均方误差是各分量均方误差之和：\n$$\nE^2 \\approx \\mathbb{E}[E_T^2] + \\mathbb{E}[E_N^2]\n$$\n截断误差项的尺度变换关系为 $E_T \\propto h^2$，因此其对均方误差的贡献为 $\\mathbb{E}[E_T^2] \\propto h^4$。\n对于噪声误差，由于 $\\epsilon_i$ 是独立同分布的，方差为 $\\sigma^2$，所以噪声项的方差为 $\\text{Var}(\\epsilon_{i+1} - \\epsilon_{i-1}) = \\text{Var}(\\epsilon_{i+1}) + \\text{Var}(\\epsilon_{i-1}) = 2\\sigma^2$。因此，均方噪声误差为：\n$$\n\\mathbb{E}[E_N^2] = \\frac{\\mathbb{E}[(\\epsilon_{i+1} - \\epsilon_{i-1})^2]}{4h^2} = \\frac{\\text{Var}(\\epsilon_{i+1} - \\epsilon_{i-1})}{4h^2} = \\frac{2\\sigma^2}{4h^2} = \\frac{\\sigma^2}{2h^2}\n$$\n因此，中心差分格式的总均方根误差的行为如下：\n$$\nE_{\\text{cd}} \\approx \\sqrt{A h^4 + B \\frac{\\sigma^2}{h^2}}\n$$\n其中 $A$ 和 $B$ 是依赖于函数导数和统计特性的常数。\n\n对一阶前向差分 $D_f y_i = (y_{i+1} - y_i)/h$ 进行类似分析，得到的截断误差为 $E_T = \\frac{h}{2} f''(t_i) + \\mathcal{O}(h^2)$，噪声误差为 $E_N = (\\epsilon_{i+1} - \\epsilon_i)/h$。其均方根误差的行为如下：\n$$\nE_{\\text{fd}} \\approx \\sqrt{C h^2 + D \\frac{\\sigma^2}{h^2}}\n$$\n其中截断误差的贡献现在是关于 $h$ 的一阶。\n\n有了这个理论框架，我们就可以预测这四个实验的结果。\n1.  **无噪声中心差分 ($p_{\\mathrm{cd},0}$)**：这里 $\\sigma=0$，因此 $E_{\\text{cd}} \\approx \\sqrt{A h^4} \\propto h^2$。在 $E$ 对 $h$ 的对数-对数图上，我们有 $\\log_{10} E = 2 \\log_{10} h + \\text{const}$。斜率 $p_{\\mathrm{cd},0}$ 预期为 $2$。\n2.  **无噪声前向差分 ($p_{\\mathrm{fd},0}$)**：这里 $\\sigma=0$，因此 $E_{\\text{fd}} \\approx \\sqrt{C h^2} \\propto h^1$。在对数-对数图上，我们有 $\\log_{10} E = 1 \\log_{10} h + \\text{const}$。斜率 $p_{\\mathrm{fd},0}$ 预期为 $1$。\n3.  **固定网格下的噪声尺度变换 ($q_{\\sigma}$)**：对于固定的、较小的 $h$，截断误差项 $A h^4$ 是一个小的常数。对于足够大的 $\\sigma$，噪声项 $B \\sigma^2/h^2$ 将占主导地位。因此，$E_{\\text{cd}} \\approx \\sqrt{B \\sigma^2/h^2} = \\frac{\\sqrt{B}}{h}\\sigma \\propto \\sigma$。在 $E$ 对 $\\sigma$ 的对数-对数图上，我们有 $\\log_{10} E = 1 \\log_{10} \\sigma + \\text{const}$。斜率 $q_{\\sigma}$ 预期为 $1$。\n4.  **噪声主导下随 $h$ 的尺度变换 ($r_{h,\\mathrm{noise}}$)**：对于固定的 $\\sigma$ 和较小的 $h$，噪声项再次占主导地位。$E_{\\text{cd}} \\approx \\sqrt{B \\sigma^2/h^2} \\propto \\frac{1}{h} = h^{-1}$。在 $E$ 对 $h$ 的对数-对数图上，我们有 $\\log_{10} E = -1 \\log_{10} h + \\text{const}$。斜率 $r_{h,\\mathrm{noise}}$ 预期为 $-1$。\n\n下面的 Python 代码实现了这四个测试，以数值方式验证这些理论尺度定律。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef compute_rms_error(approx_deriv, exact_deriv):\n    \"\"\"Computes the root-mean-square error.\"\"\"\n    return np.sqrt(np.mean((approx_deriv - exact_deriv)**2))\n\ndef run_noiseless_centered_test():\n    \"\"\"\n    Test 1: Noiseless centered-difference order test.\n    Computes the scaling exponent of error with respect to h for a\n    noiseless centered-difference scheme. Expected value is 2.\n    \"\"\"\n    M_values = np.array([40, 80, 160, 320, 640])\n    h_values = 2 * np.pi / M_values\n    error_values = []\n\n    for M in M_values:\n        h = 2 * np.pi / M\n        t = np.linspace(0, 2 * np.pi, M + 1)\n        y = np.cos(t)\n\n        # Valid indices for centered difference are 1, ..., M-1\n        indices = np.arange(1, M)\n        y_cd = (y[indices + 1] - y[indices - 1]) / (2 * h)\n        \n        t_subset = t[indices]\n        y_prime_exact = -np.sin(t_subset)\n        \n        error = compute_rms_error(y_cd, y_prime_exact)\n        error_values.append(error)\n\n    log_h = np.log10(h_values)\n    log_E = np.log10(np.array(error_values))\n    slope, _, _, _, _ = linregress(log_h, log_E)\n    return slope\n\ndef run_noiseless_forward_test():\n    \"\"\"\n    Test 2: Noiseless forward-difference order test.\n    Computes the scaling exponent of error with respect to h for a\n    noiseless forward-difference scheme. Expected value is 1.\n    \"\"\"\n    M_values = np.array([40, 80, 160, 320, 640])\n    h_values = 2 * np.pi / M_values\n    error_values = []\n\n    for M in M_values:\n        h = 2 * np.pi / M\n        t = np.linspace(0, 2 * np.pi, M + 1)\n        y = np.cos(t)\n\n        # Valid indices for forward difference are 0, ..., M-1\n        indices = np.arange(0, M)\n        y_fd = (y[indices + 1] - y[indices]) / h\n        \n        t_subset = t[indices]\n        y_prime_exact = -np.sin(t_subset)\n        \n        error = compute_rms_error(y_fd, y_prime_exact)\n        error_values.append(error)\n        \n    log_h = np.log10(h_values)\n    log_E = np.log10(np.array(error_values))\n    slope, _, _, _, _ = linregress(log_h, log_E)\n    return slope\n\ndef run_noise_sigma_scaling_test():\n    \"\"\"\n    Test 3: Noise scaling at fixed grid.\n    Computes the scaling exponent of error with respect to sigma for a\n    fixed grid. Expected value is 1.\n    \"\"\"\n    M = 2000\n    sigma_values = np.array([1e-6, 1e-5, 1e-4, 1e-3, 1e-2])\n    error_values = []\n    \n    h = 2 * np.pi / M\n    t = np.linspace(0, 2 * np.pi, M + 1)\n    y_true = np.cos(t)\n\n    # Generate a single base noise sequence\n    rng = np.random.default_rng(seed=2025)\n    z = rng.normal(0, 1, size=M + 1)\n\n    for sigma in sigma_values:\n        epsilon = sigma * z\n        y_noisy = y_true + epsilon\n\n        # Valid indices for centered difference are 1, ..., M-1\n        indices = np.arange(1, M)\n        y_cd = (y_noisy[indices + 1] - y_noisy[indices - 1]) / (2 * h)\n        \n        t_subset = t[indices]\n        y_prime_exact = -np.sin(t_subset)\n        \n        error = compute_rms_error(y_cd, y_prime_exact)\n        error_values.append(error)\n        \n    log_sigma = np.log10(sigma_values)\n    log_E = np.log10(np.array(error_values))\n    slope, _, _, _, _ = linregress(log_sigma, log_E)\n    return slope\n\ndef run_noise_h_scaling_test():\n    \"\"\"\n    Test 4: Noise-dominated scaling in h.\n    Computes the scaling exponent of error with respect to h in the\n    noise-dominated regime. Expected value is -1.\n    \"\"\"\n    M_values = np.array([500, 800, 1200, 2000])\n    h_values = 2 * np.pi / M_values\n    sigma = 1e-3\n    error_values = []\n\n    for M in M_values:\n        h = 2 * np.pi / M\n        t = np.linspace(0, 2 * np.pi, M + 1)\n        y_true = np.cos(t)\n        \n        # Generate new noise for each M with specified seed\n        rng = np.random.default_rng(seed=3000 + M)\n        epsilon = sigma * rng.normal(0, 1, size=M + 1)\n        y_noisy = y_true + epsilon\n\n        # Valid indices for centered difference are 1, ..., M-1\n        indices = np.arange(1, M)\n        y_cd = (y_noisy[indices + 1] - y_noisy[indices - 1]) / (2 * h)\n        \n        t_subset = t[indices]\n        y_prime_exact = -np.sin(t_subset)\n        \n        error = compute_rms_error(y_cd, y_prime_exact)\n        error_values.append(error)\n\n    log_h = np.log10(h_values)\n    log_E = np.log10(np.array(error_values))\n    slope, _, _, _, _ = linregress(log_h, log_E)\n    return slope\n\ndef solve():\n    \"\"\"\n    Executes the four test cases and prints the results in the\n    specified format.\n    \"\"\"\n    p_cd_0 = run_noiseless_centered_test()\n    p_fd_0 = run_noiseless_forward_test()\n    q_sigma = run_noise_sigma_scaling_test()\n    r_h_noise = run_noise_h_scaling_test()\n\n    results = [p_cd_0, p_fd_0, q_sigma, r_h_noise]\n    \n    # Format the results to three decimal places and print\n    formatted_results = [f\"{res:.3f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Run the solution\nsolve()\n```"
        },
        {
            "introduction": "在追求更高精度的道路上，我们是否必须从头推导越来越复杂的差分公式？这个练习将向你介绍一种更巧妙、更通用的技术——理查森外推法（Richardson Extrapolation）。你将学习如何通过线性组合同一个二阶中心差分在不同步长下的计算结果，系统地消除低阶误差项，从而构建出一个六阶精度的超高精度格式。这个练习不仅能让你掌握一项强大的数值技术，还能让你体会到利用误差结构来改进结果的深刻思想 。",
            "id": "2391187",
            "problem": "给定一个光滑标量函数 $f(x) = \\sin(x)$，任务是通过组合在三个步长下计算的二阶中心差分，为其一阶导数 $f^{\\prime}(x)$ 构造一个六阶精度的数值近似。令 $D(h)$ 表示在点 $x$ 处，步长为 $h$ 的 $f^{\\prime}(x)$ 的二阶中心差分近似，定义为\n$$\nD(h) = \\frac{f(x+h) - f(x-h)}{2h}.\n$$\n仅使用上述定义和第一性原理（例如，关于 $x$ 的泰勒级数展开），确定构成 $D(h)$、$D\\left(\\frac{h}{2}\\right)$ 和 $D\\left(\\frac{h}{4}\\right)$ 线性组合的常数，使其截断误差为 $h^{6}$ 阶，并实现一个程序，为指定的输入值计算这个六阶近似。\n\n您的程序必须计算绝对数值误差，定义为\n$$\nE(x,h) = \\left| \\widehat{f^{\\prime}}(x;h) - \\cos(x) \\right|,\n$$\n其中 $\\widehat{f^{\\prime}}(x;h)$ 是您用 $D(h)$、$D\\left(\\frac{h}{2}\\right)$ 和 $D\\left(\\frac{h}{4}\\right)$ 构造的六阶近似，而 $\\cos(x)$ 是 $\\sin(x)$ 的精确导数。所有角度均以弧度为单位。\n\n测试套件：\n- 案例 1：$x = 1.0$， $h = 0.2$ (弧度)。\n- 案例 2：$x = 0.0$， $h = 0.2$ (弧度)。\n- 案例 3：$x = \\frac{\\pi}{4}$， $h = 0.1$ (弧度)。\n- 案例 4：$x = \\frac{\\pi}{2}$， $h = 0.4$ (弧度)。\n- 案例 5：$x = 10.0$， $h = 0.05$ (弧度)。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含五个案例的绝对误差 $E(x,h)$，按顺序排列，作为一个用方括号括起来的逗号分隔列表。\n- 每个数字必须四舍五入到 $10$ 位小数。\n- 例如：$[e_1,e_2,e_3,e_4,e_5]$，其中每个 $e_i$ 是一个格式化为 $10$ 位小数的浮点数。",
            "solution": "该问题要求使用在不同步长下计算的二阶中心差分算子的线性组合，为函数 $f(x)$ 的一阶导数构造一个六阶精度的有限差分近似。这是 Richardson 外推法的一个标准应用。\n\n首先，我们必须验证问题陈述。\n\n步骤 1：提取给定信息\n- 函数：$f(x) = \\sin(x)$。\n- 二阶中心差分算子：$D(h) = \\frac{f(x+h) - f(x-h)}{2h}$。\n- 待构造的六阶近似：$\\widehat{f^{\\prime}}(x;h)$，是 $D(h)$、$D\\left(\\frac{h}{2}\\right)$ 和 $D\\left(\\frac{h}{4}\\right)$ 的线性组合。\n- 绝对数值误差定义：$E(x,h) = \\left| \\widehat{f^{\\prime}}(x;h) - \\cos(x) \\right|$。\n- 测试案例：\n  - 案例 1：$x = 1.0$, $h = 0.2$。\n  - 案例 2：$x = 0.0$, $h = 0.2$。\n  - 案例 3：$x = \\frac{\\pi}{4}$, $h = 0.1$。\n  - 案例 4：$x = \\frac{\\pi}{2}$, $h = 0.4$。\n  - 案例 5：$x = 10.0$, $h = 0.05$。\n\n步骤 2：使用提取的给定信息进行验证\n- **科学严谨性：** 该问题是数值分析中的一个经典练习，特别是用于数值微分的 Richardson 外推法。其原理是完善且数学上合理的。\n- **适定性：** 该问题是适定的。它要求一组常数以达到特定的精度阶数，这会导出一个可解的线性方程组。存在唯一且有意义的解。\n- **客观性：** 问题陈述客观，具有精确的数学定义，没有主观性语言。\n\n步骤 3：结论与行动\n该问题是有效的。它是计算科学中一个标准的、适定的问题。我们开始求解。\n\n该方法的基础是函数 $f(x)$ 在点 $x$ 附近的泰勒级数展开。假设函数足够光滑，我们有：\n$$ f(x+h) = f(x) + hf^{\\prime}(x) + \\frac{h^2}{2!}f^{\\prime\\prime}(x) + \\frac{h^3}{3!}f^{\\prime\\prime\\prime}(x) + \\frac{h^4}{4!}f^{(4)}(x) + \\frac{h^5}{5!}f^{(5)}(x) + \\frac{h^6}{6!}f^{(6)}(x) + \\frac{h^7}{7!}f^{(7)}(x) + \\mathcal{O}(h^8) $$\n$$ f(x-h) = f(x) - hf^{\\prime}(x) + \\frac{h^2}{2!}f^{\\prime\\prime}(x) - \\frac{h^3}{3!}f^{\\prime\\prime\\prime}(x) + \\frac{h^4}{4!}f^{(4)}(x) - \\frac{h^5}{5!}f^{(5)}(x) + \\frac{h^6}{6!}f^{(6)}(x) - \\frac{h^7}{7!}f^{(7)}(x) + \\mathcal{O}(h^8) $$\n从第一个展开式中减去第二个展开式，可以消除 $h$ 的偶次幂项：\n$$ f(x+h) - f(x-h) = 2hf^{\\prime}(x) + 2\\frac{h^3}{3!}f^{\\prime\\prime\\prime}(x) + 2\\frac{h^5}{5!}f^{(5)}(x) + 2\\frac{h^7}{7!}f^{(7)}(x) + \\mathcal{O}(h^9) $$\n除以 $2h$ 得到中心差分算子 $D(h)$ 的表达式：\n$$ D(h) = f^{\\prime}(x) + \\frac{h^2}{6}f^{\\prime\\prime\\prime}(x) + \\frac{h^4}{120}f^{(5)}(x) + \\frac{h^6}{5040}f^{(7)}(x) + \\mathcal{O}(h^8) $$\n这证实了 $D(h)$ 是 $f^{\\prime}(x)$ 的一个二阶精度近似，因为其主导误差项与 $h^2$ 成正比。我们可以将这个误差结构更紧凑地写为：\n$$ D(h) = f^{\\prime}(x) + c_2 h^2 + c_4 h^4 + c_6 h^6 + \\mathcal{O}(h^8) $$\n其中 $c_{2k} = \\frac{f^{(2k+1)}(x)}{(2k+1)!}$。\n\n我们寻求一个六阶近似 $\\widehat{f^{\\prime}}(x;h)$，作为在三个不同步长下近似值的线性组合：\n$$ \\widehat{f^{\\prime}}(x;h) = \\alpha D(h) + \\beta D\\left(\\frac{h}{2}\\right) + \\gamma D\\left(\\frac{h}{4}\\right) $$\n代入每一项的级数展开：\n$$ \\widehat{f^{\\prime}} = \\alpha \\left( f^{\\prime}(x) + c_2 h^2 + c_4 h^4 + \\dots \\right) + \\beta \\left( f^{\\prime}(x) + c_2 \\left(\\frac{h}{2}\\right)^2 + c_4 \\left(\\frac{h}{2}\\right)^4 + \\dots \\right) + \\gamma \\left( f^{\\prime}(x) + c_2 \\left(\\frac{h}{4}\\right)^2 + c_4 \\left(\\frac{h}{4}\\right)^4 + \\dots \\right) $$\n为确保近似达到六阶精度，我们必须要求 $\\widehat{f^{\\prime}}$ 的组合表达式等于 $f^{\\prime}(x) + \\mathcal{O}(h^6)$。这通过按 $h$ 的幂次收集项并将不需要的项的系数设为零来实现。\n\\begin{align*}\n\\widehat{f^{\\prime}} &= (\\alpha + \\beta + \\gamma)f^{\\prime}(x) \\\\\n&+ \\left(\\alpha + \\frac{\\beta}{4} + \\frac{\\gamma}{16}\\right)c_2 h^2 \\\\\n&+ \\left(\\alpha + \\frac{\\beta}{16} + \\frac{\\gamma}{256}\\right)c_4 h^4 \\\\\n&+ \\mathcal{O}(h^6)\n\\end{align*}\n这导出了一个包含三个未知常数 $\\alpha$、$\\beta$ 和 $\\gamma$ 的线性方程组：\n1. $f^{\\prime}(x)$ 的系数必须是 $1$：$\\alpha + \\beta + \\gamma = 1$\n2. $h^2$ 的系数必须是 $0$：$\\alpha + \\frac{1}{4}\\beta + \\frac{1}{16}\\gamma = 0$\n3. $h^4$ 的系数必须是 $0$：$\\alpha + \\frac{1}{16}\\beta + \\frac{1}{256}\\gamma = 0$\n\n我们求解这个方程组。为清晰起见，将第二个方程乘以 $16$，第三个方程乘以 $256$：\n\\begin{align*}\n(1) \\quad \\alpha + \\beta + \\gamma &= 1 \\\\\n(2) \\quad 16\\alpha + 4\\beta + \\gamma &= 0 \\\\\n(3) \\quad 256\\alpha + 16\\beta + \\gamma &= 0\n\\end{align*}\n方程 (3) 减去方程 (2)：\n$$ (256 - 16)\\alpha + (16 - 4)\\beta = 0 \\implies 240\\alpha + 12\\beta = 0 \\implies \\beta = -20\\alpha $$\n方程 (2) 减去方程 (1)：\n$$ 15\\alpha + 3\\beta = -1 $$\n将 $\\beta = -20\\alpha$ 代入此结果：\n$$ 15\\alpha + 3(-20\\alpha) = -1 \\implies 15\\alpha - 60\\alpha = -1 \\implies -45\\alpha = -1 \\implies \\alpha = \\frac{1}{45} $$\n现在我们求 $\\beta$ 和 $\\gamma$：\n$$ \\beta = -20\\alpha = -20 \\cdot \\frac{1}{45} = -\\frac{20}{45} = -\\frac{4}{9} $$\n$$ \\gamma = 1 - \\alpha - \\beta = 1 - \\frac{1}{45} - \\left(-\\frac{20}{45}\\right) = 1 + \\frac{19}{45} = \\frac{45+19}{45} = \\frac{64}{45} $$\n因此，六阶精度近似为：\n$$ \\widehat{f^{\\prime}}(x;h) = \\frac{1}{45}D(h) - \\frac{20}{45}D\\left(\\frac{h}{2}\\right) + \\frac{64}{45}D\\left(\\frac{h}{4}\\right) $$\n该表达式将用于实现计算给定测试用例的数值导数。然后，绝对误差计算为 $E(x,h) = \\left| \\widehat{f^{\\prime}}(x;h) - \\cos(x) \\right|$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical differentiation problem by constructing and applying a\n    sixth-order accurate finite difference scheme.\n    \"\"\"\n\n    # Define the scalar function f(x) and its exact derivative\n    f = np.sin\n    f_prime_exact_func = np.cos\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 0.2),\n        (0.0, 0.2),\n        (np.pi / 4, 0.1),\n        (np.pi / 2, 0.4),\n        (10.0, 0.05)\n    ]\n\n    results = []\n\n    def central_difference(func, x_val, h_val):\n        \"\"\"\n        Computes the second-order central difference approximation D(h).\n        \"\"\"\n        return (func(x_val + h_val) - func(x_val - h_val)) / (2 * h_val)\n\n    # Coefficients for the sixth-order approximation\n    # f_hat' = a * D(h) + b * D(h/2) + g * D(h/4)\n    # The derivation shows:\n    # a = 1/45\n    # b = -20/45\n    # g = 64/45\n    alpha = 1.0 / 45.0\n    beta = -20.0 / 45.0\n    gamma = 64.0 / 45.0\n\n    for x, h in test_cases:\n        # Evaluate the second-order central differences at h, h/2, and h/4\n        d_h = central_difference(f, x, h)\n        d_h_2 = central_difference(f, x, h / 2.0)\n        d_h_4 = central_difference(f, x, h / 4.0)\n        \n        # Construct the sixth-order approximation using the derived coefficients\n        f_prime_approx = alpha * d_h + beta * d_h_2 + gamma * d_h_4\n        \n        # Calculate the exact derivative\n        f_prime_exact = f_prime_exact_func(x)\n        \n        # Calculate the absolute numerical error\n        error = np.abs(f_prime_approx - f_prime_exact)\n        \n        results.append(error)\n\n    # Final print statement in the exact required format.\n    # Each number is rounded to 10 decimal places.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}