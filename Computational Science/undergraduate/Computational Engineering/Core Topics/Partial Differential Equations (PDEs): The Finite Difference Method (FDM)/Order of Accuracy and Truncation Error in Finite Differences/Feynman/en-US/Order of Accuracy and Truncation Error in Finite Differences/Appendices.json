{
    "hands_on_practices": [
        {
            "introduction": "The concept of 'order of accuracy' provides a powerful theoretical prediction for how the error of a numerical scheme should behave as the step size $h$ decreases. This first practice is designed to make this abstract theory concrete by putting it to the test. By implementing both first-order and second-order finite difference schemes and observing how their errors scale when the step size is halved, you will numerically verify the fundamental relationship between error and step size, gaining an intuitive feel for the practical implications of different convergence rates .",
            "id": "2421878",
            "problem": "Write a complete program that numerically demonstrates the scaling of truncation error with step size for first-order and second-order finite difference schemes when approximating the first derivative of a smooth function. For each test case below, compute the absolute error at step size $h$ and at the halved step size $h/2$, and then report the ratio $R = \\lvert E(h) \\rvert / \\lvert E(h/2) \\rvert$. Use the one-sided first-order forward difference approximation and the centered second-order difference approximation for the first derivative:\n- First-order forward difference: $\\left(f(x+h)-f(x)\\right)/h$.\n- Second-order central difference: $\\left(f(x+h)-f(x-h)\\right)/(2h)$.\nFor each test case, compute two ratios:\n- $R_{\\text{FD}}$: the ratio for the first-order forward difference scheme.\n- $R_{\\text{CD}}$: the ratio for the second-order central difference scheme.\nUse the exact analytical derivative to define the error. Angles are in radians. Round each reported ratio to $2$ decimal places.\n\nTest suite:\n- Case A (general smooth case): $f(x)=\\mathrm{e}^{x}$, $f'(x)=\\mathrm{e}^{x}$, $x=0.3$, $h=10^{-3}$.\n- Case B (trigonometric case): $f(x)=\\sin(x)$, $f'(x)=\\cos(x)$, $x=1.0$, $h=10^{-4}$.\n- Case C (edge case with cancellation in the leading error term for the first-order scheme): $f(x)=x^{3}$, $f'(x)=3x^{2}$, $x=0.0$, $h=10^{-3}$.\n- Case D (floating-point sensitivity with very small step): $f(x)=\\cos(x)$, $f'(x)=-\\sin(x)$, $x=2.0$, $h=10^{-8}$.\n\nYour program must output all results as a single line containing a flat list in the following order:\n$[R_{\\text{FD}}^{A},R_{\\text{CD}}^{A},R_{\\text{FD}}^{B},R_{\\text{CD}}^{B},R_{\\text{FD}}^{C},R_{\\text{CD}}^{C},R_{\\text{FD}}^{D},R_{\\text{CD}}^{D}]$.\nThe program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_{1},r_{2},\\dots]$).",
            "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard, verifiable problem in numerical analysis, a core topic in computational engineering.\n\nThe task is to analyze the convergence properties of two finite difference schemes for the first derivative, $f'(x)$. The order of accuracy, $p$, of a scheme determines how its truncation error, $E(h)$, scales with the step size, $h$. Specifically, for a sufficiently smooth function, the error is expected to behave as $E(h) = C h^p + O(h^{p+1})$, where $C$ is a constant independent of $h$.\n\nTo numerically verify the order of accuracy, we compute the ratio of absolute errors at two different step sizes, $h$ and $h/2$. The ratio $R$ is defined as:\n$$R = \\frac{\\lvert E(h) \\rvert}{\\lvert E(h/2) \\rvert}$$\nAssuming the leading error term is dominant, this ratio approximates:\n$$R \\approx \\frac{\\lvert C h^p \\rvert}{\\lvert C (h/2)^p \\rvert} = \\frac{h^p}{(h/2)^p} = \\frac{h^p}{h^p / 2^p} = 2^p$$\nTherefore, by computing $R$, we can numerically estimate the order of accuracy $p$.\n\nThe problem specifies two schemes:\n1.  **First-order forward difference (FD)**:\n    The approximation is $f'_{\\text{FD}}(x) = \\frac{f(x+h) - f(x)}{h}$.\n    A Taylor series expansion of $f(x+h)$ around $x$ gives:\n    $$f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + O(h^3)$$\n    Rearranging this, we find the truncation error:\n    $$E_{\\text{FD}}(h) = f'_{\\text{FD}}(x) - f'(x) = \\frac{h}{2}f''(x) + O(h^2)$$\n    This is a first-order scheme ($p=1$), so we expect the ratio $R_{\\text{FD}} \\approx 2^1 = 2$.\n\n2.  **Second-order central difference (CD)**:\n    The approximation is $f'_{\\text{CD}}(x) = \\frac{f(x+h) - f(x-h)}{2h}$.\n    Taylor series expansions for $f(x+h)$ and $f(x-h)$ yield:\n    $$f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + \\frac{h^3}{6}f'''(x) + O(h^4)$$\n    $$f(x-h) = f(x) - hf'(x) + \\frac{h^2}{2}f''(x) - \\frac{h^3}{6}f'''(x) + O(h^4)$$\n    Subtracting the second from the first and solving for $f'(x)$ gives the truncation error:\n    $$E_{\\text{CD}}(h) = f'_{\\text{CD}}(x) - f'(x) = \\frac{h^2}{6}f'''(x) + O(h^4)$$\n    This is a second-order scheme ($p=2$), so we expect the ratio $R_{\\text{CD}} \\approx 2^2 = 4$.\n\nWe now analyze each test case:\n\n- **Case A ($f(x)=\\mathrm{e}^{x}$) and Case B ($f(x)=\\sin(x)$)**: These are general cases where the functions and their derivatives are well-behaved, and the leading error terms (proportional to $f''(x)$ for FD and $f'''(x)$ for CD) do not vanish at the specified points. Thus, the standard analysis holds, and we expect $R_{\\text{FD}} \\approx 2$ and $R_{\\text{CD}} \\approx 4$.\n\n- **Case C ($f(x)=x^3$ at $x=0$)**: This is a special case. We have $f'(x)=3x^2$, $f''(x)=6x$, and $f'''(x)=6$. At the point $x=0$, the second derivative is $f''(0)=0$. This causes the leading error term of the first-order FD scheme, $\\frac{h}{2}f''(0)$, to vanish. The error is now dominated by the next term in the Taylor expansion, which is $\\frac{h^2}{6}f'''(0) = \\frac{h^2}{6}(6) = h^2$. The error for the FD scheme now scales as $O(h^2)$, making it behave like a second-order scheme. Consequently, we expect $R_{\\text{FD}} \\approx 2^2 = 4$. For the CD scheme, the leading error term is proportional to $f'''(0)=6$, which is non-zero. The scheme remains second-order, and we expect $R_{\\text{CD}} \\approx 4$.\n\n- **Case D ($f(x)=\\cos(x)$ with $h=10^{-8}$)**: This case demonstrates the practical limits of finite precision arithmetic. The total numerical error is a combination of the truncation error ($E_{\\text{trunc}}$) and the floating-point round-off error ($E_{\\text{round}}$). While $E_{\\text{trunc}}$ decreases with $h$ ($E_{\\text{trunc}} \\propto h^p$), $E_{\\text{round}}$ increases as $h$ decreases. The subtraction of two nearly equal numbers, e.g., $f(x+h)-f(x)$, leads to catastrophic cancellation, introducing an error that scales roughly as $\\epsilon_m / h$, where $\\epsilon_m$ is the machine epsilon (approx. $10^{-16}$ for double precision).\n    - For the CD scheme, $p=2$, so the truncation error $E_{\\text{trunc}} \\propto h^2 \\approx (10^{-8})^2 = 10^{-16}$. The round-off error $E_{\\text{round}} \\propto 1/h \\approx 10^{-8}$. Clearly, round-off error dominates. The error scales as $E(h) \\approx K/h$. The ratio is therefore $R_{\\text{CD}} \\approx \\frac{|K/h|}{|K/(h/2)|} = \\frac{|K/h|}{|2K/h|} = 0.5$.\n    - For the FD scheme, $p=1$, so the truncation error $E_{\\text{trunc}} \\propto h \\approx 10^{-8}$. The round-off error is also $E_{\\text{round}} \\propto 1/h \\approx 10^{-8}$. Here, both error sources are of comparable magnitude. The step size $h=10^{-8}$ is near the optimal step size for this scheme, where the total error is minimized. The behavior is in a transitional zone but is still primarily governed by the truncation error term, so we expect a ratio close to the theoretical value, $R_{\\text{FD}} \\approx 2$.\n\nThe program below will perform these calculations to numerically verify these theoretical predictions. For each case, it computes the approximate derivative with step sizes $h$ and $h/2$, finds the absolute errors relative to the exact derivative, and calculates the ratio of these errors.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the error ratio for finite difference schemes to demonstrate\n    their order of accuracy.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (function, derivative, evaluation point x, step size h)\n    test_cases = [\n        (lambda x: np.exp(x), lambda x: np.exp(x), 0.3, 1e-3),\n        (lambda x: np.sin(x), lambda x: np.cos(x), 1.0, 1e-4),\n        (lambda x: x**3, lambda x: 3*x**2, 0.0, 1e-3),\n        (lambda x: np.cos(x), lambda x: -np.sin(x), 2.0, 1e-8),\n    ]\n\n    results = []\n    \n    for f, f_prime, x, h in test_cases:\n        h_half = h / 2.0\n        \n        # Exact derivative value\n        d_exact = f_prime(x)\n\n        # --- First-Order Forward Difference (FD) ---\n        \n        # Approximation at step h\n        d_fd_h = (f(x + h) - f(x)) / h\n        err_fd_h = np.abs(d_fd_h - d_exact)\n        \n        # Approximation at step h/2\n        d_fd_h_half = (f(x + h_half) - f(x)) / h_half\n        err_fd_h_half = np.abs(d_fd_h_half - d_exact)\n        \n        # Ratio for FD\n        # Handle cases where error might be zero to avoid division by zero,\n        # although unlikely for these specific problems.\n        if err_fd_h_half == 0:\n            # If the smaller error is zero, the ratio is undefined or infinite.\n            # For this problem's context, this would imply perfect accuracy at h/2.\n            # We assign a placeholder NaN. In practice, this signals an issue.\n            R_fd = np.nan \n        else:\n            R_fd = err_fd_h / err_fd_h_half\n        \n        results.append(R_fd)\n\n        # --- Second-Order Central Difference (CD) ---\n\n        # Approximation at step h\n        d_cd_h = (f(x + h) - f(x - h)) / (2 * h)\n        err_cd_h = np.abs(d_cd_h - d_exact)\n        \n        # Approximation at step h/2\n        d_cd_h_half = (f(x + h_half) - f(x - h_half)) / (2 * h_half)\n        err_cd_h_half = np.abs(d_cd_h_half - d_exact)\n        \n        # Ratio for CD\n        if err_cd_h_half == 0:\n            R_cd = np.nan\n        else:\n            R_cd = err_cd_h / err_cd_h_half\n        \n        results.append(R_cd)\n\n    # Format the final results to 2 decimal places as strings\n    formatted_results = [f\"{res:.2f}\" for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "While theory suggests that decreasing step size $h$ indefinitely should improve accuracy, our computers operate with finite precision. This exercise explores the practical consequences of this limitation, revealing the critical trade-off between truncation error and floating-point round-off error . You will write a program to discover the optimal step size $h^{\\star}$ where the total error is minimized, learning to identify the point at which the benefits of reducing truncation error are overwhelmed by the catastrophic effects of round-off.",
            "id": "2421884",
            "problem": "Let $f:\\mathbb{R}\\to\\mathbb{R}$ be a smooth function. For a given point $x_0\\in\\mathbb{R}$ and a step size $h>0$, define two linear finite-difference operators for approximating the first derivative $f'(x_0)$:\n(1) The forward-difference operator $D_h^{\\mathrm{F}}$ defined by $D_h^{\\mathrm{F}} f(x_0) \\equiv \\dfrac{f(x_0+h)-f(x_0)}{h}$.\n(2) The central-difference operator $D_h^{\\mathrm{C}}$ defined by $D_h^{\\mathrm{C}} f(x_0) \\equiv \\dfrac{f(x_0+h)-f(x_0-h)}{2h}$.\nFor each chosen combination of $f$, $x_0$, and one of the operators above, consider a strictly decreasing geometric sequence of step sizes $\\{h_k\\}_{k=1}^{K}$ given by $h_k = 2^{-k}$ for $k\\in\\{1,2,\\dots,K\\}$, with $K$ a fixed positive integer. For each $h_k$, define the absolute error\n$$\nE(h_k) \\equiv \\left|D_{h_k} f(x_0) - f'(x_0)\\right|.\n$$\nAs $h$ is reduced along the sequence $\\{h_k\\}$, the total error $E(h)$ is influenced by truncation error and floating-point round-off error. In a fixed floating-point arithmetic, there typically exists a smallest error at some step size $h^\\star$ along the sequence, after which further reduction of $h$ causes $E(h)$ to increase. For the purposes of this problem, define $h^\\star$ as the element of $\\{h_k\\}$ at which $E(h_k)$ attains its minimum value over $k\\in\\{1,2,\\dots,K\\}$. If the minimum occurs at multiple indices, choose the largest $h_k$ among those minima as $h^\\star$.\nUse the following settings for the arithmetic model: Institute of Electrical and Electronics Engineers (IEEE) $754$ binary$64$ (commonly known as double precision) and IEEE $754$ binary$32$ (commonly known as single precision). Angles, when present, must be interpreted in radians. No physical units are involved.\nYour task is to write a program that, for each test case in the test suite defined below, computes $h^\\star$ according to the definition above.\nTest suite:\n- Test case $1$: $f(x)=\\exp(x)$, $f'(x)=\\exp(x)$, $x_0=1$, operator $D_h^{\\mathrm{C}}$, arithmetic precision binary$64$, with $K=60$.\n- Test case $2$: $f(x)=\\sin(x)$, $f'(x)=\\cos(x)$, $x_0=1$, operator $D_h^{\\mathrm{F}}$, arithmetic precision binary$64$, with $K=60$.\n- Test case $3$: $f(x)=\\ln(x)$, $f'(x)=1/x$, $x_0=1$, operator $D_h^{\\mathrm{C}}$, arithmetic precision binary$32$, with $K=60$.\n- Test case $4$: $f(x)=\\exp(x)$, $f'(x)=\\exp(x)$, $x_0=1$, operator $D_h^{\\mathrm{F}}$, arithmetic precision binary$32$, with $K=60$.\nAll logarithms are natural logarithms. Ensure $x_0-h_k>0$ whenever $f(x)=\\ln(x)$ is evaluated; this is satisfied by the specified data because $x_0=1$ and $h_k\\leq 2^{-1}$ for all $k\\geq 1$.\nFor each test case, output the value of $h^\\star$ as a real number. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $1$ through $4$; for example, an output with four results must look like $[r_1,r_2,r_3,r_4]$ where each $r_j$ is the computed $h^\\star$ for test case $j$.",
            "solution": "The problem statement is subjected to validation before a solution is attempted.\n\n**Step 1: Extracted Givens**\n- Functions: $f(x)=\\exp(x)$, $f(x)=\\sin(x)$, $f(x)=\\ln(x)$.\n- Derivatives: $f'(x)=\\exp(x)$, $f'(x)=\\cos(x)$, $f'(x)=1/x$.\n- Evaluation point: $x_0=1$.\n- Operators: Forward difference $D_h^{\\mathrm{F}} f(x_0) \\equiv \\frac{f(x_0+h)-f(x_0)}{h}$ and Central difference $D_h^{\\mathrm{C}} f(x_0) \\equiv \\frac{f(x_0+h)-f(x_0-h)}{2h}$.\n- Step sizes: A sequence $\\{h_k\\}_{k=1}^{K}$ where $h_k = 2^{-k}$ and $K=60$.\n- Error metric: $E(h_k) \\equiv |D_{h_k} f(x_0) - f'(x_0)|$.\n- Optimal step size $h^\\star$: The $h_k$ that minimizes $E(h_k)$. If multiple minima exist, the largest $h_k$ (corresponding to the smallest $k$) is chosen.\n- Arithmetic precision: IEEE $754$ binary$64$ (double) and binary$32$ (single).\n- Test cases: Four specific combinations of the above parameters are provided.\n\n**Step 2: Validation**\nThe problem is assessed against the required criteria.\n- **Scientifically Grounded**: The problem is a standard exercise in numerical analysis, focusing on the interplay between truncation and round-off errors in finite difference methods. These are core concepts in computational science and engineering.\n- **Well-Posed**: The problem is specified with mathematical precision. All functions, parameters, and evaluation rules, including a tie-breaking condition for the minimum, are explicitly defined. This ensures a unique, deterministic solution for each test case.\n- **Objective**: The problem is stated in objective, formal language, free of any subjective or ambiguous elements.\n\nThe problem does not violate any of the invalidity conditions. It is scientifically sound, fully specified, and computationally feasible.\n\n**Step 3: Verdict**\nThe problem is **valid**. A solution will be provided.\n\n**Theoretical Foundation**\nThe total error $E(h)$ in numerical differentiation is the sum of two components: truncation error $E_T(h)$ and round-off error $E_R(h)$.\n\n**Truncation Error**\nThe truncation error arises from approximating the true derivative with a discrete formula. Its form is derived from Taylor series expansions.\nFor the **forward-difference operator** $D_h^{\\mathrm{F}}$, the Taylor series of $f$ around $x_0$ is:\n$$ f(x_0+h) = f(x_0) + hf'(x_0) + \\frac{h^2}{2}f''(x_0) + \\mathcal{O}(h^3) $$\nRearranging for the derivative gives:\n$$ D_h^{\\mathrm{F}} f(x_0) = \\frac{f(x_0+h) - f(x_0)}{h} = f'(x_0) + \\frac{h}{2}f''(x_0) + \\mathcal{O}(h^2) $$\nThe leading-order truncation error is $E_T^{\\mathrm{F}}(h) = \\frac{h}{2}f''(x_0)$, which is of order $\\mathcal{O}(h)$. The method is first-order accurate.\n\nFor the **central-difference operator** $D_h^{\\mathrm{C}}$, we use two Taylor expansions:\n$$ f(x_0+h) = f(x_0) + hf'(x_0) + \\frac{h^2}{2}f''(x_0) + \\frac{h^3}{6}f'''(x_0) + \\mathcal{O}(h^4) $$\n$$ f(x_0-h) = f(x_0) - hf'(x_0) + \\frac{h^2}{2}f''(x_0) - \\frac{h^3}{6}f'''(x_0) + \\mathcal{O}(h^4) $$\nSubtracting the second from the first and rearranging yields:\n$$ D_h^{\\mathrm{C}} f(x_0) = \\frac{f(x_0+h) - f(x_0-h)}{2h} = f'(x_0) + \\frac{h^2}{6}f'''(x_0) + \\mathcal{O}(h^4) $$\nThe leading-order truncation error is $E_T^{\\mathrm{C}}(h) = \\frac{h^2}{6}f'''(x_0)$, which is of order $\\mathcal{O}(h^2)$. The method is second-order accurate.\n\n**Round-off Error**\nRound-off error originates from the finite precision of floating-point arithmetic. The evaluation of $f(x)$ produces a value $\\hat{f}(x)$ such that $|\\hat{f}(x) - f(x)| \\lesssim \\epsilon_M |f(x)|$, where $\\epsilon_M$ is the machine epsilon (approximately $2.2 \\times 10^{-16}$ for binary$64$ and $1.2 \\times 10^{-7}$ for binary$32$).\nFor small $h$, both difference formulas involve subtracting nearly equal numbers ($f(x_0+h) \\approx f(x_0)$ and $f(x_0+h) \\approx f(x_0-h)$). This operation, known as subtractive cancellation, amplifies the effect of round-off error. The error in the numerator of the difference formulas is roughly on the order of $\\epsilon_M|f(x_0)|$. When divided by a small denominator ($h$ or $2h$), the resulting round-off error in the derivative approximation becomes significant:\n$$ |E_R(h)| \\approx C \\frac{\\epsilon_M}{h} $$\nwhere the constant $C$ depends on the operator and $|f(x_0)|$.\n\n**Optimal Step Size $h^\\star$**\nThe total error magnitude is dominated by the sum of truncation and round-off error magnitudes:\n$$ E(h) \\approx |E_T(h)| + |E_R(h)| $$\nFor the first-order scheme, $E(h) \\approx A h + B \\frac{\\epsilon_M}{h}$. For the second-order scheme, $E(h) \\approx A h^2 + B \\frac{\\epsilon_M}{h}$. As $h$ decreases, the truncation error term diminishes while the round-off error term grows. This trade-off implies the existence of an optimal step size $h^\\star$ where the total error $E(h)$ is minimized. This is the value the program must find.\n\n**Computational Procedure**\nThe solution requires a direct simulation for each test case.\n1. Define the parameters for each case: the function $f$, its analytical derivative $f'$, the point $x_0$, the operator, and the floating-point precision (binary$32$ or binary$64$). All numerical values used in a calculation must be cast to the specified precision.\n2. For each case, iterate $k$ from $1$ to $K=60$.\n3. In each iteration, compute the step size $h_k = 2^{-k}$.\n4. Calculate the numerical derivative $D_{h_k}f(x_0)$ using the appropriate formula.\n5. Compute the exact derivative value $f'(x_0)$ for reference.\n6. Determine the absolute error $E(h_k) = |D_{h_k}f(x_0) - f'(x_0)|$.\n7. After computing the list of errors $\\{E(h_k)\\}_{k=1}^{K}$, find the minimum value $E_{\\min}$.\n8. Identify all step sizes $h_k$ that produce this minimum error $E_{\\min}$.\n9. According to the problem's tie-breaking rule, select the largest step size from this set as the optimal value $h^\\star$. This corresponds to the smallest index $k$.\nThe following program implements this procedure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal finite difference step size h_star for a suite of test cases.\n\n    For each case, it iterates through a geometrically decreasing sequence of step sizes h_k,\n    calculates the absolute error of the finite difference approximation against the\n    exact derivative, and finds the step size h_star that minimizes this error. The calculation\n    is performed using the specified floating-point precision (binary32 or binary64).\n    \"\"\"\n    test_cases = [\n        {\n            \"f\": np.exp,\n            \"df\": np.exp,\n            \"x0\": 1.0,\n            \"operator\": \"C\",\n            \"dtype\": np.float64,\n            \"K\": 60,\n        },\n        {\n            \"f\": np.sin,\n            \"df\": np.cos,\n            \"x0\": 1.0,\n            \"operator\": \"F\",\n            \"dtype\": np.float64,\n            \"K\": 60,\n        },\n        {\n            \"f\": np.log,\n            \"df\": lambda x: 1/x,\n            \"x0\": 1.0,\n            \"operator\": \"C\",\n            \"dtype\": np.float32,\n            \"K\": 60,\n        },\n        {\n            \"f\": np.exp,\n            \"df\": np.exp,\n            \"x0\": 1.0,\n            \"operator\": \"F\",\n            \"dtype\": np.float32,\n            \"K\": 60,\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        f = case[\"f\"]\n        df = case[\"df\"]\n        dtype = case[\"dtype\"]\n        x0 = dtype(case[\"x0\"])\n        operator = case[\"operator\"]\n        K = case[\"K\"]\n\n        # Calculate the exact derivative at the specified precision\n        df_exact = df(x0)\n\n        errors_and_hs = []\n\n        # Iterate through the sequence of step sizes h_k = 2^-k\n        for k in range(1, K + 1):\n            k_p = dtype(k)\n            h = dtype(2.0)**(-k_p)\n\n            # Calculate the finite difference approximation\n            if operator == 'C':\n                f_plus = f(x0 + h)\n                f_minus = f(x0 - h)\n                df_approx = (f_plus - f_minus) / (dtype(2.0) * h)\n            elif operator == 'F':\n                f_plus = f(x0 + h)\n                f_zero = f(x0)\n                df_approx = (f_plus - f_zero) / h\n            else:\n                # This path should not be reached with valid problem inputs\n                continue\n\n            # Calculate the absolute error\n            error = np.abs(df_approx - df_exact)\n            errors_and_hs.append((error, h))\n\n        # Find the minimum error achieved\n        if not errors_and_hs:\n            # Handle case of empty list, though not expected here\n            results.append(None) # Or some other indicator of failure\n            continue\n            \n        min_error = min(e for e, h in errors_and_hs)\n        \n        # Find all h values that resulted in the minimum error\n        h_at_min_error = [h for e, h in errors_and_hs if e == min_error]\n        \n        # Per problem spec, choose the largest h if there's a tie\n        h_star = max(h_at_min_error)\n        results.append(h_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Understanding the systematic nature of truncation error is not just for verification; it can be exploited to achieve higher accuracy. This practice introduces Richardson extrapolation, a powerful technique that uses results from a sequence of refined grids to cancel out the leading error term and produce a more accurate estimate of the true solution . By analyzing a provided set of simulation data, you will learn how to both estimate the unknown order of accuracy and extrapolate to a grid-converged value, a fundamental skill in performing rigorous grid convergence studies.",
            "id": "2421800",
            "problem": "A scalar quantity $Q$ is computed by a finite-difference discretization on uniform grids of spacing $h$. Assume that, in the asymptotic regime, the leading truncation error behaves as $Q(h) = Q^{\\ast} + C h^{p} + \\mathcal{O}(h^{p+1})$, where $Q^{\\ast}$ is the grid-converged value, $C$ is an $h$-independent constant, and $p$ is the (unknown) order of accuracy. In a grid-refinement study with refinement ratio $r=2$, the following data were recorded for three grids:\n- $h = 0.20$: absolute error $E(h) = 2.0000 \\times 10^{-2}$ and computed approximation $Q(h) = 1.43421356$.\n- $h = 0.10$: absolute error $E(h) = 5.0000 \\times 10^{-3}$ and computed approximation $Q(h) = 1.41921356$.\n- $h = 0.05$: absolute error $E(h) = 1.2500 \\times 10^{-3}$ and computed approximation $Q(h) = 1.41546356$.\n\nUsing only the stated asymptotic error model and the given data, use Richardson extrapolation to estimate both the grid-converged value $Q^{\\ast}$ and the order of accuracy $p$. Report your final answer as a row matrix with $Q^{\\ast}$ first and $p$ second. Round the estimate of $Q^{\\ast}$ to six significant figures. Do not include units.",
            "solution": "The problem statement will first be subjected to validation.\n\nStep 1: Extracted Givens.\nThe asymptotic error model is $Q(h) = Q^{\\ast} + C h^{p} + \\mathcal{O}(h^{p+1})$.\nThe grid refinement ratio is $r=2$.\nGrid data are provided for three uniform grids:\n- Grid 1: $h_1 = 0.20$, with computed value $Q(h_1) = 1.43421356$.\n- Grid 2: $h_2 = 0.10$, with computed value $Q(h_2) = 1.41921356$.\n- Grid 3: $h_3 = 0.05$, with computed value $Q(h_3) = 1.41546356$.\nThe problem also provides absolute errors for each grid, but these are redundant if the computed values and the exact value are related as stated, and can be used for verification. The task is to estimate the grid-converged value $Q^{\\ast}$ and the order of accuracy $p$ using Richardson extrapolation. The estimate for $Q^{\\ast}$ must be rounded to six significant figures.\n\nStep 2: Validation.\nThe problem is scientifically grounded, being a standard exercise in numerical analysis for computational engineering. It is well-posed, as there are two primary unknowns ($p$ and $Q^{\\ast}$) and sufficient data from three grids to solve for them. The language is objective and the data are internally consistent. For example, the ratio of differences in computed quantities, $\\frac{Q(h_1) - Q(h_2)}{Q(h_2) - Q(h_3)} = \\frac{1.43421356 - 1.41921356}{1.41921356 - 1.41546356} = \\frac{0.015}{0.00375} = 4$. This is expected to be equal to $r^p = 2^p$, which suggests $p=2$, a plausible integer order of accuracy. The problem is free of the invalidating flaws listed in the instructions.\n\nStep 3: Verdict.\nThe problem is valid and a solution will be derived.\n\nThe solution proceeds by first determining the order of accuracy $p$, and then using this value to extrapolate to the grid-converged value $Q^{\\ast}$.\n\nLet us denote the solutions on the three grids as $Q_1 = Q(h_1)$, $Q_2 = Q(h_2)$, and $Q_3 = Q(h_3)$, where $h_1 = 0.20$, $h_2 = 0.10$, and $h_3 = 0.05$. The constant refinement ratio is $r = \\frac{h_1}{h_2} = \\frac{h_2}{h_3} = 2$.\n\nFrom the asymptotic error model, ignoring higher-order terms, we can write:\n$Q_1 \\approx Q^{\\ast} + C h_1^{p}$\n$Q_2 \\approx Q^{\\ast} + C h_2^{p}$\n$Q_3 \\approx Q^{\\ast} + C h_3^{p}$\n\nSubtracting these equations consecutively yields the differences in computed values:\n$Q_1 - Q_2 \\approx C(h_1^p - h_2^p) = C( (r h_2)^p - h_2^p ) = C h_2^p (r^p - 1)$\n$Q_2 - Q_3 \\approx C(h_2^p - h_3^p) = C( (r h_3)^p - h_3^p ) = C h_3^p (r^p - 1)$\n\nThe ratio of these two differences eliminates the constant $C$:\n$$\n\\frac{Q_1 - Q_2}{Q_2 - Q_3} \\approx \\frac{C h_2^p (r^p - 1)}{C h_3^p (r^p - 1)} = \\left(\\frac{h_2}{h_3}\\right)^p = r^p\n$$\nThis allows for the determination of the apparent order of accuracy, $p$:\n$$\np = \\log_r\\left(\\frac{Q_1 - Q_2}{Q_2 - Q_3}\\right) = \\frac{\\ln\\left(\\frac{Q_1 - Q_2}{Q_2 - Q_3}\\right)}{\\ln(r)}\n$$\nSubstituting the given numerical values:\n$Q_1 - Q_2 = 1.43421356 - 1.41921356 = 0.015$\n$Q_2 - Q_3 = 1.41921356 - 1.41546356 = 0.00375$\nThe ratio is $\\frac{0.015}{0.00375} = 4$.\nWith a refinement ratio $r=2$, the order of accuracy is:\n$$\np = \\frac{\\ln(4)}{\\ln(2)} = \\frac{2\\ln(2)}{\\ln(2)} = 2\n$$\nThe order of accuracy is exactly $p=2$.\n\nNext, we use Richardson extrapolation to estimate $Q^{\\ast}$. To obtain the most accurate estimate, we use the solutions from the two finest grids, $Q_2$ and $Q_3$.\nWe have the system of two equations:\n$Q_2 \\approx Q^{\\ast} + C h_2^{p}$\n$Q_3 \\approx Q^{\\ast} + C h_3^{p}$\nWe can eliminate the error term coefficient $C$. From the first equation, $C \\approx \\frac{Q_2 - Q^{\\ast}}{h_2^p}$. Substituting this into the second equation gives:\n$Q_3 \\approx Q^{\\ast} + \\frac{Q_2 - Q^{\\ast}}{h_2^p} h_3^p = Q^{\\ast} + (Q_2 - Q^{\\ast})\\left(\\frac{h_3}{h_2}\\right)^p = Q^{\\ast} + (Q_2 - Q^{\\ast})\\frac{1}{r^p}$\nRearranging to solve for $Q^{\\ast}$:\n$Q_3(r^p) \\approx Q^{\\ast}(r^p) + Q_2 - Q^{\\ast}$\n$Q_3 r^p - Q_2 \\approx Q^{\\ast} (r^p - 1)$\n$$\nQ^{\\ast} \\approx \\frac{Q_3 r^p - Q_2}{r^p - 1}\n$$\nThis is the Richardson extrapolation formula. We now substitute the known values: $Q_2 = 1.41921356$, $Q_3 = 1.41546356$, $r=2$, and $p=2$. Thus, $r^p = 2^2 = 4$.\n$$\nQ^{\\ast} \\approx \\frac{4 \\times 1.41546356 - 1.41921356}{4 - 1}\n$$\nThe calculation proceeds as follows:\n$4 \\times 1.41546356 = 5.66185424$\n$4 Q_3 - Q_2 = 5.66185424 - 1.41921356 = 4.24264068$\n$$\nQ^{\\ast} \\approx \\frac{4.24264068}{3} = 1.41421356\n$$\nThe problem requires this estimate to be rounded to six significant figures. The first six significant figures of $1.41421356$ are $1, 4, 1, 4, 2, 1$. The subsequent digit is $3$, which is less than $5$, so we round down.\nThe rounded estimate is $Q^{\\ast} \\approx 1.41421$.\n\nThe two estimated parameters are the order of accuracy $p=2$ and the grid-converged value $Q^{\\ast} \\approx 1.41421$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.41421 & 2\n\\end{pmatrix}\n}\n$$"
        }
    ]
}