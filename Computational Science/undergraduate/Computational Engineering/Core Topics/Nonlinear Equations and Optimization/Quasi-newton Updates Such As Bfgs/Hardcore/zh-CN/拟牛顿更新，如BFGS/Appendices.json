{
    "hands_on_practices": [
        {
            "introduction": "在BFGS算法中，初始Hessian近似矩阵的选择对收敛速度有显著影响。虽然单位矩阵 $I$ 是一个简单且常见的选择，但在处理病态问题时，更精巧的缩放策略能够大幅提升初始阶段的进展。本练习将通过一个经典的测试函数，让您亲手实现并比较这两种初始化策略，从而直观地理解初始曲率信息的重要性。",
            "id": "2431054",
            "problem": "考虑使用拟牛顿法和Broyden-Fletcher-Goldfarb-Shanno (BFGS)更新来对一个光滑函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 进行无约束最小化。该方法维护一个对逆Hessian矩阵（二阶导数矩阵的逆）的近似 $B_k$，并通过 $p_k = - B_k \\nabla f(x_k)$ 计算在第 $k$ 次迭代时的下降方向 $p_k$。然后，线搜索会沿着 $p_k$ 选择一个步长 $\\alpha_k$，以满足强Wolfe条件，其中常数 $c_1 = 10^{-4}$ 和 $c_2 = 0.9$。初始的逆Hessian近似 $B_0$ 会极大地影响第一步 $p_0$，从而影响初始进展。\n\n您的任务是构建并评估一个可复现的场景：在该场景中，与使用缩放单位矩阵 $B_0 = \\gamma I$ 相比，选择 $B_0 = I$（单位矩阵）会导致非常缓慢的初始进展。其中\n$$\n\\gamma = \\frac{y_0^\\top s_0}{y_0^\\top y_0}, \\quad s_0 = -\\eta \\, \\nabla f(x_0), \\quad y_0 = \\nabla f(x_0 + s_0) - \\nabla f(x_0),\n$$\n$\\eta  0$ 是一个小的探测步长。这种缩放选择试图将初始逆Hessian矩阵调整到沿梯度方向测量的局部曲率。\n\n使用偶数维度 $n$ 下的广义双参数Rosenbrock函数作为目标函数：\n$$\nf(x) = \\sum_{i=1}^{n/2} \\left[ \\beta\\left(x_{2i} - x_{2i-1}^2\\right)^2 + \\left(1 - x_{2i-1}\\right)^2 \\right],\n$$\n其梯度分量为\n$$\n\\frac{\\partial f}{\\partial x_{2i-1}} = -4\\beta\\,x_{2i-1}\\left(x_{2i} - x_{2i-1}^2\\right) + 2\\left(x_{2i-1} - 1\\right), \\quad\n\\frac{\\partial f}{\\partial x_{2i}} = 2\\beta\\left(x_{2i} - x_{2i-1}^2\\right),\n$$\n其中 $i=1,\\dots,n/2$。\n\n实现一个具有两种初始化的单次BFGS迭代：\n- 情况A（未缩放）：$B_0 = I$。\n- 情况B（缩放）：$B_0 = \\gamma I$，其中 $\\gamma$ 如上所定，使用探测步长 $s_0 = -\\eta \\nabla f(x_0)$。\n\n对于每种情况，计算 $p_0 = -B_0 \\nabla f(x_0)$，并通过从 $\\alpha = 1$ 开始的强Wolfe线搜索选择 $\\alpha_0$（参数为 $c_1 = 10^{-4}$ 和 $c_2 = 0.9$）。如果强Wolfe线搜索未能返回步长，则回退到Armijo回溯法（仅充分下降），使用缩减因子 $0.5$、相同的 $c_1$ 和相同的起始 $\\alpha = 1$。然后，为每种情况构建 $x_1 = x_0 + \\alpha_0 p_0$。将初始进展定义为目标函数下降量 $f(x_0) - f(x_1)$。\n\n您的程序必须为每个测试用例计算比率\n$$\nR = \\frac{f(x_0) - f(x_1^{\\text{scaled}})}{\\max\\left(f(x_0) - f(x_1^{\\text{identity}}),\\,10^{-30}\\right)},\n$$\n其中 $x_1^{\\text{scaled}}$ 是通过缩放的 $B_0 = \\gamma I$ 获得的，而 $x_1^{\\text{identity}}$ 是通过 $B_0 = I$ 获得的。若 $R  1$，则表示在第一次迭代中，缩放初始化比未缩放的初始化取得了更大的进展。报告 $R$ 值，四舍五入到六位小数。\n\n测试套件：\n- 测试1（高曲率，二维）：$n=2$, $\\beta = 10^4$, $x_0 = (-1.2,\\,1.0)$, $\\eta = 10^{-3}$。\n- 测试2（中等曲率，四维）：$n=4$, $\\beta = 100$, $x_0 = (-1.2,\\,1.0,\\,-1.2,\\,1.0)$, $\\eta = 10^{-3}$。\n- 测试3（靠近最小值点的边缘情况）：$n=2$, $\\beta = 100$, $x_0 = (1.0,\\,1.0001)$, $\\eta = 10^{-3}$。\n\n您的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，例如，“[r1,r2,r3]”。每个 $r_i$ 必须是相应测试的 $R$ 的浮点值，四舍五入到六位小数。不允许有其他输出。不涉及物理单位和角度；所有量都是无量纲实数。",
            "solution": "所提出的问题经过了严格的验证。所有给定条件，包括目标函数、其梯度、算法参数和测试用例，都已被提取。该问题被发现其科学基础根植于已确立的数值优化领域。它是一个适定问题，为获得唯一的、确定性的解提供了所有必要信息。其语言客观而精确。因此，该问题被认为是有效的，并将提供一个解决方案。\n\n该问题要求分析拟牛顿优化方法的第一次迭代，具体来说是Broyden-Fletcher-Goldfarb-Shanno (BFGS)算法。我们需要比较初始逆Hessian近似 $B_0$ 的两种不同选择所带来的初始进展。\n\n拟牛顿法的一般迭代格式由下式给出：\n$$ x_{k+1} = x_k + \\alpha_k p_k $$\n其中 $p_k$ 是搜索方向，$\\alpha_k$ 是步长。搜索方向是使用当前逆Hessian矩阵的近似 $B_k$ 和目标函数的梯度 $\\nabla f(x_k)$ 来计算的：\n$$ p_k = -B_k \\nabla f(x_k) $$\n步长 $\\alpha_k$ 由线搜索过程确定，以确保函数值充分下降并满足曲率条件。问题指定了强Wolfe条件：\n1. Armijo（充分下降）条件：$f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k$\n2. 曲率条件：$|\\nabla f(x_k + \\alpha_k p_k)^\\top p_k| \\le c_2 |\\nabla f(x_k)^\\top p_k|$\n其中指定常数 $c_1 = 10^{-4}$ 和 $c_2 = 0.9$。\n\n这个问题的核心在于初始近似 $B_0$ 的选择，它决定了优化过程的第一步 $p_0 = -B_0 \\nabla f(x_0)$。我们研究两种情况。\n\n情况A：未缩放的单位矩阵初始化\n选择 $B_0 = I$（其中 $I$ 是单位矩阵）是最简单的可能选择。这导致初始搜索方向为 $p_0 = -\\nabla f(x_0)$，即最速下降方向。虽然直观，但对于病态问题，即函数水平集高度偏心的问题，这个方向可能非常低效，导致缓慢的、锯齿状的收敛。\n\n情况B：缩放的单位矩阵初始化\n一种更复杂的方法是缩放初始单位矩阵，$B_0 = \\gamma I$。选择缩放因子 $\\gamma$ 以近似函数的曲率。问题规定了一种基于探测步长寻找 $\\gamma$ 的特定方法。我们首先为一个小的 $\\eta > 0$ 计算一个试验步长 $s_0 = -\\eta \\nabla f(x_0)$。然后我们测量梯度的变化，$y_0 = \\nabla f(x_0 + s_0) - \\nabla f(x_0)$。缩放因子则由下式给出：\n$$ \\gamma = \\frac{y_0^\\top s_0}{y_0^\\top y_0} $$\n这个 $\\gamma$ 的公式是通过在最小二乘意义上寻找一个最能满足割线方程 $s_0 = B_0 y_0$ 的标量而导出的，即通过关于 $\\gamma$ 最小化 $\\| s_0 - \\gamma y_0 \\|_2^2$。这样做是为了赋予 $B_0$ 一些关于函数曲率的信息，可能导致一个更好缩放的初始步 $p_0 = -\\gamma \\nabla f(x_0)$。为了使这是一个下降方向，我们需要 $\\gamma > 0$，这在函数沿探测方向 $s_0$ 足够凸（即 $y_0^\\top s_0 > 0$）时成立。\n\n测试函数是偶数维度 $n$ 下的广义Rosenbrock函数：\n$$ f(x) = \\sum_{i=1}^{n/2} \\left[ \\beta\\left(x_{2i} - x_{2i-1}^2\\right)^2 + \\left(1 - x_{2i-1}\\right)^2 \\right] $$\n这个函数是优化算法的经典基准测试，因为它具有非凸性以及存在一个狭窄的抛物线形山谷。对于大的 $\\beta$，问题变得非常病态，这使其成为展示良好缩放的初始步相对于朴素最速下降方向优越性的极佳候选案例。\n\n计算过程如下：\n1. 对于每个测试用例（$n, \\beta, x_0, \\eta$），我们将实现Rosenbrock函数及其梯度。\n2. 我们将对情况A（$B_0 = I$）和情况B（$B_0 = \\gamma I$）各执行一次迭代。\n3. 对于情况B，首先计算因子 $\\gamma$，这需要计算在 $x_0$ 和探测点 $x_0 + s_0$ 处的梯度。当分母 $y_0^\\top y_0$ 接近于零时，需要有安全措施。\n4. 对于两种情况，计算搜索方向 $p_0$。我们必须验证它是一个下降方向（即 $\\nabla f(x_0)^\\top p_0  0$）。\n5. 沿着 $p_0$ 从 $\\alpha = 1$ 开始执行线搜索，以找到满足强Wolfe条件的步长 $\\alpha_0$。我们将使用 `scipy.optimize` 库中的 `line_search` 函数。\n6. 根据问题要求，如果强Wolfe线搜索失败，我们必须回退到手动实现的Armijo回溯法，其中我们迭代地将 $\\alpha$ 乘以因子 $0.5$，直到满足充分下降条件。\n7. 在确定了 $\\alpha_0$ 之后，我们计算下一个迭代点 $x_1 = x_0 + \\alpha_0 p_0$ 和相应的函数值 $f(x_1)$。\n8. 每种情况下的进展定义为目标函数的下降量 $f(x_0) - f(x_1)$。\n9. 最后，我们计算缩放方法取得的进展与未缩放方法取得的进展的比率 $R$：\n$$ R = \\frac{f(x_0) - f(x_1^{\\text{scaled}})}{\\max\\left(f(x_0) - f(x_1^{\\text{identity}}),\\,10^{-30}\\right)} $$\n分母被正则化以防止除以零或数值不稳定。每个测试用例得到的 $R$ 值将四舍五入到六位小数。\n\n这个过程将被封装在一个Python程序中，该程序构成最终答案。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import line_search\n\ndef rosenbrock(x, beta):\n    \"\"\"\n    Computes the value of the generalized Rosenbrock function.\n    \"\"\"\n    n = len(x)\n    if n % 2 != 0:\n        raise ValueError(\"Dimension n must be even for the generalized Rosenbrock function.\")\n    \n    val = 0.0\n    for i in range(n // 2):\n        # x-indices are 2*i and 2*i+1, corresponding to problem's x_2i-1 and x_2i for i=1..n/2\n        idx1 = 2 * i\n        idx2 = 2 * i + 1\n        term1 = beta * (x[idx2] - x[idx1]**2)**2\n        term2 = (1 - x[idx1])**2\n        val += term1 + term2\n    return val\n\ndef rosenbrock_grad(x, beta):\n    \"\"\"\n    Computes the gradient of the generalized Rosenbrock function.\n    \"\"\"\n    n = len(x)\n    if n % 2 != 0:\n        raise ValueError(\"Dimension n must be even for the generalized Rosenbrock function.\")\n        \n    grad = np.zeros(n)\n    for i in range(n // 2):\n        idx1 = 2 * i\n        idx2 = 2 * i + 1\n        common_term = 2 * beta * (x[idx2] - x[idx1]**2)\n        grad[idx1] = -2 * x[idx1] * common_term + 2 * (x[idx1] - 1)\n        grad[idx2] = common_term\n    return grad\n\ndef compute_progress(x0, beta, eta, initialization, c1, c2):\n    \"\"\"\n    Computes the progress f(x0) - f(x1) for a single Quasi-Newton iteration.\n    Handles both identity and scaled initializations, and includes line search logic.\n    \"\"\"\n    # Create lambda functions to pass beta parameter\n    f = lambda x: rosenbrock(x, beta)\n    grad = lambda x: rosenbrock_grad(x, beta)\n\n    f0 = f(x0)\n    g0 = grad(x0)\n\n    # If gradient is virtually zero, no progress can be made.\n    if np.linalg.norm(g0)  1e-12:\n        return 0.0\n\n    if initialization == 'identity':\n        p0 = -g0\n    elif initialization == 'scaled':\n        s0 = -eta * g0\n        \n        # Probing step to compute the scaling factor gamma\n        x_probe = x0 + s0\n        g_probe = grad(x_probe)\n        y0 = g_probe - g0\n\n        y0_dot_y0 = np.dot(y0, y0)\n        \n        # Guard against division by zero for ill-defined gamma\n        if y0_dot_y0  1e-20:\n            gamma = 1.0 # Fallback to identity scaling\n        else:\n            y0_dot_s0 = np.dot(y0, s0)\n            gamma = y0_dot_s0 / y0_dot_y0\n        \n        p0 = -gamma * g0\n    else:\n        raise ValueError(f\"Unknown initialization type: {initialization}\")\n\n    # The search direction must be a descent direction.\n    # If gamma = 0, this will not hold, and progress should be zero.\n    pk_dot_g0 = np.dot(g0, p0)\n    if pk_dot_g0 >= 0:\n        return 0.0\n\n    # Perform strong Wolfe line search using SciPy\n    alpha, _, _, f_new, _, _ = line_search(\n        f=f,\n        myfprime=grad,\n        xk=x0,\n        pk=p0,\n        gfk=g0,\n        old_fval=f0,\n        c1=c1,\n        c2=c2\n    )\n\n    # Fallback to Armijo backtracking if strong Wolfe search fails\n    if alpha is None:\n        alpha = 1.0\n        rho = 0.5\n        \n        # Limit backtracking steps to prevent infinite loops\n        for _ in range(100):\n            x_new_check = x0 + alpha * p0\n            f_new_check = f(x_new_check)\n            if f_new_check = f0 + c1 * alpha * pk_dot_g0:\n                f_new = f_new_check\n                break\n            alpha *= rho\n        else:\n            # If Armijo loop completes without break, step is negligible.\n            return 0.0\n\n    return f0 - f_new\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, beta, x0, eta)\n        (2, 1e4, np.array([-1.2, 1.0]), 1e-3),\n        (4, 100.0, np.array([-1.2, 1.0, -1.2, 1.0]), 1e-3),\n        (2, 100.0, np.array([1.0, 1.0001]), 1e-3)\n    ]\n    \n    # Line search parameters\n    c1 = 1e-4\n    c2 = 0.9\n\n    results = []\n    for n, beta, x0, eta in test_cases:\n        # Calculate progress for the unscaled (identity) case\n        progress_identity = compute_progress(x0, beta, eta, 'identity', c1, c2)\n        \n        # Calculate progress for the scaled case\n        progress_scaled = compute_progress(x0, beta, eta, 'scaled', c1, c2)\n\n        # Compute the ratio R, with a safeguard for the denominator\n        denominator = max(progress_identity, 1e-30)\n        R = progress_scaled / denominator\n        \n        results.append(round(R, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "BFGS并非唯一的拟牛顿更新方法，对称秩-1（SR1）更新是另一种重要的选择。然而，BFGS在数值稳定性方面具有关键优势，这正是其被广泛应用的核心原因之一。在本练习中，您将构造一个特殊的场景，该场景会导致SR1更新因分母趋近于零而失败，而BFGS更新则保持稳健，从而深刻揭示BFGS公式设计的精妙之处。",
            "id": "2431086",
            "problem": "开发一个完整的、可运行的程序，在一小组明确指定的案例中，比较对称秩1 (SR1) 拟牛顿更新与 Broyden–Fletcher–Goldfarb–Shanno (BFGS) 拟牛顿更新在 SR1 分母可能变得任意小的二次模型上的行为。对于每种情况，程序必须给定一个对称正定矩阵 $H \\in \\mathbb{R}^{n \\times n}$、一个对称矩阵 $B_k \\in \\mathbb{R}^{n \\times n}$ 和一个非零向量 $s_k \\in \\mathbb{R}^n$，并且必须定义 $y_k = H s_k$。然后，程序必须对每种情况执行以下计算：\n\n1. 计算 SR1 分母 $\\Delta_k = s_k^T\\left(y_k - B_k s_k\\right)$。定义相对近零检验参数 $r = 10^{-8}$ 并声明布尔值\n   $$\\text{SR1\\_near\\_zero} := \\left(|\\Delta_k| \\le r \\, \\|s_k\\|_2 \\, \\|y_k - B_k s_k\\|_2\\right)。$$\n   此标志旨在识别 SR1 更新\n   $$B_{k+1}^{\\text{SR1}} = B_k + \\frac{\\left(y_k - B_k s_k\\right)\\left(y_k - B_k s_k\\right)^T}{\\left(y_k - B_k s_k\\right)^T s_k}$$\n   何时会因分母近零而被认为在数值上不安全。如果分母恰好为零，则不等式成立，该标志必须设置为 true。\n\n2. 计算 BFGS 更新\n   $$B_{k+1}^{\\text{BFGS}} = B_k \\;-\\; \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} \\;+\\; \\frac{y_k y_k^T}{s_k^T y_k}。$$\n   然后通过检查其最小特征值是否超过正定性容差 $\\epsilon_{\\text{pd}} = 10^{-12}$ 来检验 $B_{k+1}^{\\text{BFGS}}$ 是否为数值正定。用 $\\text{BFGS\\_PD}$ 表示此布尔值。\n\n3. 计算 BFGS 更新后的矩阵与真实 $H$ 之间的弗罗贝尼乌斯范数误差，\n   $$E_{\\text{F}} = \\left\\|B_{k+1}^{\\text{BFGS}} - H\\right\\|_F。$$\n\n您的程序必须将这些计算应用于以下三个测试用例，其中每个矩阵和向量都已明确指定：\n\n- 测试用例 #1 (一个良态的、非病态的情况)：\n  - $n = 2$。\n  - $H^{(1)} = \\begin{bmatrix} 3  1 \\\\ 1  2 \\end{bmatrix}$。\n  - $B_k^{(1)} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$。\n  - $s_k^{(1)} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$。\n  - $y_k^{(1)} = H^{(1)} s_k^{(1)}$。\n\n- 测试用例 #2 ($s_k$ 和 $y_k - B_k s_k$ 之间的近正交性使得 SR1 分母相对接近于零，而 $y_k - B_k s_k$ 并不小)：\n  - $n = 2$。\n  - $H^{(2)} = \\begin{bmatrix} 2  1 \\\\ 1  3 \\end{bmatrix}$。\n  - 令 $\\varepsilon = 10^{-12}$。\n  - $B_k^{(2)} = \\begin{bmatrix} 2 - \\varepsilon  0 \\\\ 0  1 \\end{bmatrix}$。\n  - $s_k^{(2)} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。\n  - $y_k^{(2)} = H^{(2)} s_k^{(2)}$。\n\n- 测试用例 #3 (精确正交性使得 SR1 分母恰好为零)：\n  - $n = 2$。\n  - $H^{(3)} = \\begin{bmatrix} 2  1 \\\\ 1  3 \\end{bmatrix}$。\n  - $B_k^{(3)} = \\begin{bmatrix} 2  0 \\\\ 0  1 \\end{bmatrix}$。\n  - $s_k^{(3)} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。\n  - $y_k^{(3)} = H^{(3)} s_k^{(3)}$。\n\n对于每个测试用例 $i \\in \\{1,2,3\\}$，您的程序必须输出一个列表\n$$\\left[\\text{SR1\\_near\\_zero}^{(i)},\\, \\text{BFGS\\_PD}^{(i)},\\, E_{\\text{F}}^{(i)}\\right]，$$\n其中前两项是布尔值，最后一项是浮点数。该浮点数必须四舍五入到恰好 $8$ 位小数。\n\n最终输出格式：您的程序应生成单行输出，其中包含所有三个测试用例的结果，形式为三个列表的逗号分隔列表，任何地方都没有空格。例如，所需的结构是\n$$\\texttt{[[b11,b12,f1],[b21,b22,f2],[b31,b32,f3]]}，$$\n其中每个 $\\texttt{bij}$ 是 $\\texttt{True}$ 或 $\\texttt{False}$，每个 $\\texttt{fi}$ 是小数点后恰有 $8$ 位数字的小数。本问题中没有物理单位或角度；所有数值输出都是无量纲实数。",
            "solution": "问题陈述已经过严格验证，并被确定为有效。它在科学上基于数值优化的原理，是适定的，提供了所有必要信息，并以客观、明确的语言陈述。我们将给出一个完整的解法。\n\n这个问题的核心是研究和对比两种著名的拟牛顿更新公式，即对称秩1 (SR1) 和 Broyden–Fletcher–Goldfarb–Shanno (BFGS) 更新，在已知对前者有问题的条件下的数值行为。拟牛顿法迭代地构建目标函数海森矩阵的一个近似 $B_k$。从 $B_k$到 $B_{k+1}$的更新使用最近一步的信息，这些信息由向量 $s_k = x_{k+1} - x_k$（位置变化）和 $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$（梯度变化）封装。对于二次目标函数 $f(x) = \\frac{1}{2} x^T H x + c^T x + d$，其海森矩阵是常数 $H$，我们有关系式 $y_k = H s_k$。更新公式旨在满足割线方程 $B_{k+1}s_k = y_k$。\n\nSR1 更新由下式给出：\n$$B_{k+1}^{\\text{SR1}} = B_k + \\frac{(y_k - B_k s_k)(y_k - B_k s_k)^T}{(y_k - B_k s_k)^T s_k}$$\n它的主要缺陷在于分母 $\\Delta_k = (y_k - B_k s_k)^T s_k$。如果这一项为零或非常接近于零，更新将变得无定义或数值不稳定，可能导致 $B_k$ 发生巨大且无意义的变化。条件 $\\Delta_k = 0$ 意味着向量 $y_k - B_k s_k$（表示真实梯度变化与当前模型 $B_k$ 预测的梯度变化之间的差异）与步长方向 $s_k$ 正交。\n\nBFGS 更新由下式给出：\n$$B_{k+1}^{\\text{BFGS}} = B_k - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \\frac{y_k y_k^T}{s_k^T y_k}$$\nBFGS 更新的一个关键性质是，如果 $B_k$ 是正定的，并且满足曲率条件 $s_k^T y_k  0$，那么 $B_{k+1}^{\\text{BFGS}}$ 也是正定的。曲率条件通常由符合 Wolfe 条件的线搜索算法来保证。在本问题中，由于 $y_k = H s_k$ 且 $H$ 被指定为正定，因此对于任何非零 $s_k$，都有 $s_k^T y_k = s_k^T H s_k  0$。因此，BFGS 更新是良定义的，并预期会保持正定性。\n\n我们现在分析指定的三个测试用例。参数为 SR1 近零检验的 $r = 10^{-8}$ 和正定性检查的 $\\epsilon_{\\text{pd}} = 10^{-12}$。\n\n**测试用例 1：良态情况**\n给定：\n- $H^{(1)} = \\begin{bmatrix} 3  1 \\\\ 1  2 \\end{bmatrix}$， $B_k^{(1)} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$， $s_k^{(1)} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$。\n首先，我们计算必要的向量：\n- $y_k^{(1)} = H^{(1)} s_k^{(1)} = \\begin{bmatrix} 3  1 \\\\ 1  2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 3 \\end{bmatrix}$。\n- $B_k^{(1)} s_k^{(1)} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$。\n- 差异向量为 $v_k^{(1)} = y_k^{(1)} - B_k^{(1)} s_k^{(1)} = \\begin{bmatrix} 4 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}$。\n\n1.  **SR1 分母检查**：\n    - $\\Delta_k^{(1)} = (s_k^{(1)})^T v_k^{(1)} = \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix} = 5$。\n    - 范数：$\\|s_k^{(1)}\\|_2 = \\sqrt{1^2+1^2} = \\sqrt{2}$，$\\|v_k^{(1)}\\|_2 = \\sqrt{3^2+2^2} = \\sqrt{13}$。\n    - 检验：$|\\Delta_k^{(1)}| \\le r \\|s_k^{(1)}\\|_2 \\|v_k^{(1)}\\|_2$ 是否成立？\n      $5 \\le 10^{-8} \\sqrt{2} \\sqrt{13} \\approx 5.099 \\times 10^{-8}$。这不成立。\n    - 因此，$\\text{SR1\\_near\\_zero}^{(1)} = \\text{False}$。\n\n2.  **BFGS 更新与正定性**：\n    - 分母：$(s_k^{(1)})^T B_k^{(1)} s_k^{(1)} = 2$，$(s_k^{(1)})^T y_k^{(1)} = 7$。\n    - $B_{k+1}^{\\text{BFGS}, (1)} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} - \\frac{1}{2}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\begin{bmatrix} 1  1 \\end{bmatrix} + \\frac{1}{7}\\begin{bmatrix} 4 \\\\ 3 \\end{bmatrix}\\begin{bmatrix} 4  3 \\end{bmatrix} = \\begin{bmatrix} 19.5/7  8.5/7 \\\\ 8.5/7  12.5/7 \\end{bmatrix} \\approx \\begin{bmatrix} 2.7857  1.2143 \\\\ 1.2143  1.7857 \\end{bmatrix}$。\n    - 特征值为 $\\lambda = \\frac{16}{7} \\pm \\frac{\\sqrt{338}}{14}$。最小特征值为 $\\lambda_{\\min} = \\frac{32 - 13\\sqrt{2}}{14} \\approx 0.972$。\n    - 由于 $0.972  10^{-12}$，该矩阵是正定的。\n    - 因此，$\\text{BFGS\\_PD}^{(1)} = \\text{True}$。\n\n3.  **弗罗贝尼乌斯范数误差**：\n    - $B_{k+1}^{\\text{BFGS}, (1)} - H^{(1)} = \\begin{bmatrix} 19.5/7 - 3  8.5/7 - 1 \\\\ 8.5/7 - 1  12.5/7 - 2 \\end{bmatrix} = \\begin{bmatrix} -1.5/7  1.5/7 \\\\ 1.5/7  -1.5/7 \\end{bmatrix}$。\n    - $E_{\\text{F}}^{(1)} = \\left\\|B_{k+1}^{\\text{BFGS}, (1)} - H^{(1)}\\right\\|_F = \\sqrt{4 \\times (1.5/7)^2} = \\frac{3}{7} \\approx 0.42857143$。\n\n用例 1 的结果：$[\\text{False}, \\text{True}, 0.42857143]$。\n\n---\n\n**测试用例 2：近零 SR1 分母**\n给定：\n- $\\varepsilon = 10^{-12}$。\n- $H^{(2)} = \\begin{bmatrix} 2  1 \\\\ 1  3 \\end{bmatrix}$，$B_k^{(2)} = \\begin{bmatrix} 2 - \\varepsilon  0 \\\\ 0  1 \\end{bmatrix}$，$s_k^{(2)} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。\n计算向量：\n- $y_k^{(2)} = H^{(2)} s_k^{(2)} = \\begin{bmatrix} 2  1 \\\\ 1  3 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$。\n- $B_k^{(2)} s_k^{(2)} = \\begin{bmatrix} 2 - \\varepsilon  0 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2 - \\varepsilon \\\\ 0 \\end{bmatrix}$。\n- 差异向量：$v_k^{(2)} = y_k^{(2)} - B_k^{(2)} s_k^{(2)} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 2 - \\varepsilon \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} \\varepsilon \\\\ 1 \\end{bmatrix}$。\n\n1.  **SR1 分母检查**：\n    - $\\Delta_k^{(2)} = (s_k^{(2)})^T v_k^{(2)} = \\begin{bmatrix} 1  0 \\end{bmatrix} \\begin{bmatrix} \\varepsilon \\\\ 1 \\end{bmatrix} = \\varepsilon = 10^{-12}$。\n    - 范数：$\\|s_k^{(2)}\\|_2 = 1$，$ \\|v_k^{(2)}\\|_2 = \\sqrt{\\varepsilon^2+1^2} = \\sqrt{10^{-24}+1} \\approx 1$。\n    - 检验：$|\\Delta_k^{(2)}| \\le r \\|s_k^{(2)}\\|_2 \\|v_k^{(2)}\\|_2$ 是否成立？\n      $10^{-12} \\le 10^{-8} \\times 1 \\times \\sqrt{1+10^{-24}}$。这成立。\n    - 因此，$\\text{SR1\\_near\\_zero}^{(2)} = \\text{True}$。\n\n2.  **BFGS 更新与正定性**：\n    - 分母：$(s_k^{(2)})^T B_k^{(2)} s_k^{(2)} = 2 - \\varepsilon$，$(s_k^{(2)})^T y_k^{(2)} = 2$。\n    - $B_{k+1}^{\\text{BFGS}, (2)} = \\begin{bmatrix} 2-\\varepsilon  0 \\\\ 0  1 \\end{bmatrix} - \\frac{1}{2-\\varepsilon}\\begin{bmatrix} 2-\\varepsilon \\\\ 0 \\end{bmatrix}\\begin{bmatrix} 2-\\varepsilon  0 \\end{bmatrix} + \\frac{1}{2}\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\begin{bmatrix} 2  1 \\end{bmatrix}$。\n    - $B_{k+1}^{\\text{BFGS}, (2)} = \\begin{bmatrix} 2-\\varepsilon  0 \\\\ 0  1 \\end{bmatrix} - \\begin{bmatrix} 2-\\varepsilon  0 \\\\ 0  0 \\end{bmatrix} + \\begin{bmatrix} 2  1 \\\\ 1  0.5 \\end{bmatrix} = \\begin{bmatrix} 2  1 \\\\ 1  1.5 \\end{bmatrix}$。\n    - 该矩阵的特征值为 $\\lambda = \\frac{3.5 \\pm \\sqrt{4.25}}{2}$。最小特征值为 $\\lambda_{\\min} = \\frac{3.5 - \\sqrt{4.25}}{2} \\approx 0.719$。\n    - 由于 $0.719  10^{-12}$，该矩阵是正定的。\n    - 因此，$\\text{BFGS\\_PD}^{(2)} = \\text{True}$。\n\n3.  **弗罗贝尼乌斯范数误差**：\n    - $B_{k+1}^{\\text{BFGS}, (2)} - H^{(2)} = \\begin{bmatrix} 2  1 \\\\ 1  1.5 \\end{bmatrix} - \\begin{bmatrix} 2  1 \\\\ 1  3 \\end{bmatrix} = \\begin{bmatrix} 0  0 \\\\ 0  -1.5 \\end{bmatrix}$。\n    - $E_{\\text{F}}^{(2)} = \\left\\|B_{k+1}^{\\text{BFGS}, (2)} - H^{(2)}\\right\\|_F = \\sqrt{0^2+0^2+0^2+(-1.5)^2} = 1.5$。\n\n用例 2 的结果：$[\\text{True}, \\text{True}, 1.50000000]$。\n\n---\n\n**测试用例 3：SR1 分母恰好为零**\n该情况与用例 2 相同，只是设置了 $\\varepsilon=0$。\n给定：\n- $H^{(3)} = \\begin{bmatrix} 2  1 \\\\ 1  3 \\end{bmatrix}$，$B_k^{(3)} = \\begin{bmatrix} 2  0 \\\\ 0  1 \\end{bmatrix}$，$s_k^{(3)} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。\n计算向量：\n- $y_k^{(3)} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$。\n- $B_k^{(3)} s_k^{(3)} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$。\n- 差异向量：$v_k^{(3)} = y_k^{(3)} - B_k^{(3)} s_k^{(3)} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。\n\n1.  **SR1 分母检查**：\n    - $\\Delta_k^{(3)} = (s_k^{(3)})^T v_k^{(3)} = \\begin{bmatrix} 1  0 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = 0$。\n    - 条件 $|\\Delta_k^{(3)}| \\le ...$ 是平凡满足的，因为 $0$ 小于或等于任何非负数。\n    - 因此，$\\text{SR1\\_near\\_zero}^{(3)} = \\text{True}$。\n\n2.  **BFGS 更新与正定性**：\n    - 计算过程与 $\\varepsilon=0$ 的用例 2 相同。\n    - $B_{k+1}^{\\text{BFGS}, (3)} = \\begin{bmatrix} 2  1 \\\\ 1  1.5 \\end{bmatrix}$。\n    - 该矩阵及其特征值与用例 2 相同。最小特征值约为 $0.719  10^{-12}$。\n    - 因此，$\\text{BFGS\\_PD}^{(3)} = \\text{True}$。\n\n3.  **弗罗贝尼乌斯范数误差**：\n    - 得到的矩阵 $B_{k+1}^{\\text{BFGS}, (3)}$ 和真实海森矩阵 $H^{(3)}$ 与用例 2 相同。\n    - $E_{\\text{F}}^{(3)} = 1.5$。\n\n用例 3 的结果：$[\\text{True}, \\text{True}, 1.50000000]$。\n\n结果清楚地表明了 BFGS 更新在 SR1 更新失败的情况下的稳健性。在用例 2 和 3 中，SR1 分母变得数值上很小，然后恰好为零，使得更新不安全或无定义。相比之下，BFGS 更新则顺利进行，在所有三种情况下都生成了正定矩阵，从而证明了其在实际优化软件中广泛使用的合理性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the quasi-Newton update comparison problem for three test cases.\n    \"\"\"\n\n    # Define constants from the problem statement.\n    r = 1e-8\n    eps_pd = 1e-12\n\n    # Define the test cases. Each case is a dictionary for clarity.\n    test_cases = [\n        {\n            \"H\": np.array([[3.0, 1.0], [1.0, 2.0]]),\n            \"Bk\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"sk\": np.array([1.0, 1.0]),\n        },\n        {\n            \"H\": np.array([[2.0, 1.0], [1.0, 3.0]]),\n            \"Bk\": np.array([[2.0 - 1e-12, 0.0], [0.0, 1.0]]),\n            \"sk\": np.array([1.0, 0.0]),\n        },\n        {\n            \"H\": np.array([[2.0, 1.0], [1.0, 3.0]]),\n            \"Bk\": np.array([[2.0, 0.0], [0.0, 1.0]]),\n            \"sk\": np.array([1.0, 0.0]),\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        H, Bk, sk = case[\"H\"], case[\"Bk\"], case[\"sk\"]\n\n        # Define yk = H * sk\n        yk = H @ sk\n\n        # --- Part 1: SR1 Denominator Check ---\n        # vector vk = yk - Bk * sk\n        vk = yk - Bk @ sk\n        # SR1 denominator delta_k = sk.T * (yk - Bk * sk)\n        delta_k = sk.T @ vk\n        \n        # Norms for the near-zero test\n        norm_sk = np.linalg.norm(sk, 2)\n        norm_vk = np.linalg.norm(vk, 2)\n        \n        # The inequality handles the case where delta_k is exactly zero.\n        sr1_near_zero = abs(delta_k) = r * norm_sk * norm_vk\n\n        # --- Part 2: BFGS Update and Positive-Definiteness Test ---\n        # Compute denominators for BFGS update\n        sk_T_Bk_sk = sk.T @ Bk @ sk\n        sk_T_yk = sk.T @ yk\n\n        # BFGS update formula\n        term1 = Bk\n        term2 = np.outer(Bk @ sk, Bk @ sk) / sk_T_Bk_sk\n        term3 = np.outer(yk, yk) / sk_T_yk\n        \n        B_k_plus_1_bfgs = term1 - term2 + term3\n\n        # Check for positive definiteness. Using eigvalsh as matrix is symmetric.\n        eigenvalues = np.linalg.eigvalsh(B_k_plus_1_bfgs)\n        min_eigenvalue = np.min(eigenvalues)\n        bfgs_pd = min_eigenvalue > eps_pd\n\n        # --- Part 3: Frobenius-norm Error ---\n        E_f = np.linalg.norm(B_k_plus_1_bfgs - H, 'fro')\n\n        # Collect results for the current case\n        # The float must be rounded to exactly 8 decimal places for the output format.\n        current_result = [sr1_near_zero, bfgs_pd, round(E_f, 8)]\n        all_results.append(current_result)\n\n    # --- Final Output Formatting ---\n    # Construct the output string manually to ensure no spaces and correct format.\n    # e.g., \"[[False,True,0.42857143],[True,True,1.50000000],[True,True,1.50000000]]\"\n    result_strings = []\n    for res in all_results:\n        # Format the boolean and float components as strings\n        b1 = str(res[0])\n        b2 = str(res[1])\n        f_val = f\"{res[2]:.8f}\"\n        # Create the inner list string \"[b1,b2,f_val]\"\n        result_strings.append(f\"[{b1},{b2},{f_val}]\")\n    \n    # Join the inner list strings with a comma and wrap in brackets\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "对于大规模优化问题，存储和操作一个稠密的 $n \\times n$ Hessian近似矩阵是不可行的。有限内存BFGS（L-BFGS）算法正是为解决这一挑战而生，它已成为机器学习等领域的标准工具。这项高级练习将引导您从零开始实现L-BFGS的核心——高效的双循环递归算法，它仅利用最近的几步信息来计算搜索方向，让您掌握该算法在节省内存方面的强大能力。",
            "id": "2431082",
            "problem": "给定有限的曲率对序列 $\\{(s_i,y_i)\\}$、一个定义了 $H_0=\\gamma I$ 的初始标量 $\\gamma$，以及一个平滑目标函数的当前梯度 $g$。考虑如下定义的对称正定线性算子 $H$：在所有满足最近 $m$ 个曲率对 $\\{(s_i,y_i)\\}_{i=k-m+1}^k$（其中 $s_i^\\top y_i  0$）的割线条件 $H y_i = s_i$ 的对称正定矩阵中，算子 $H$ 是从 $H_0=\\gamma I$ 开始，并按照索引递增的顺序，以 Broyden-Fletcher-Goldfarb-Shanno (BFGS) 更新的方式强制执行这些割线条件所得到的唯一算子。对于下述每个测试用例，计算搜索方向 $p=-H g$。\n\n所有向量和矩阵均在实数域上。所有分量均应视为量纲一致的纯数（无物理单位）。使用以下测试套件；在每个用例中，仅需强制执行最近的 $m$ 个曲率对。\n\n测试用例 1（边界情况，无曲率对）：\n- 维度 $n=3$。\n- 内存大小 $m=0$（无曲率对）。\n- 初始缩放因子 $\\gamma = 0.5$。\n- 梯度 $g = \\left[1,-2,3\\right]$。\n\n测试用例 2（单个曲率对）：\n- 维度 $n=2$。\n- 内存大小 $m=1$。\n- 初始缩放因子 $\\gamma = 1$。\n- 曲率对 $\\left(s_1,y_1\\right) = \\left(\\left[1,2\\right],\\left[3,1\\right]\\right)$，其中 $s_1^\\top y_1 = 5$。\n- 梯度 $g = \\left[4,-1\\right]$。\n\n测试用例 3（多个曲率对张成空间，二次函数的精确恢复）：\n- 维度 $n=3$。\n- 内存大小 $m=3$。\n- 初始缩放因子 $\\gamma = 1$。\n- 定义 $A=\\mathrm{diag}\\!\\left(2,3,4\\right)$。令 $s_1=\\left[1,0,0\\right]$，$s_2=\\left[0,1,0\\right]$，$s_3=\\left[0,0,1\\right]$，且 $y_i = A s_i$，即 $y_1=\\left[2,0,0\\right]$，$y_2=\\left[0,3,0\\right]$，$y_3=\\left[0,0,4\\right]$。\n- 梯度 $g = \\left[5,-6,7\\right]$。\n\n测试用例 4（有限内存，仅使用最近的曲率对）：\n- 维度 $n=4$。\n- 内存大小 $m=2$（仅使用下述最后两个曲率对）。\n- 初始缩放因子 $\\gamma = 1$。\n- 定义对称正定矩阵\n$$\nA=\\begin{bmatrix}\n4  1  0  0\\\\\n1  3  0  0\\\\\n0  0  2  0\\\\\n0  0  0  1.5\n\\end{bmatrix}.\n$$\n- 通过 $y_i = A s_i$ 为以下向量定义曲率对\n$s_1=\\left[1,0,0,0\\right]$ 对应 $y_1=\\left[4,1,0,0\\right]$,\n$s_2=\\left[0,1,0,0\\right]$ 对应 $y_2=\\left[1,3,0,0\\right]$,\n$s_3=\\left[0,0,1,0\\right]$ 对应 $y_3=\\left[0,0,2,0\\right]$,\n$s_4=\\left[0,0,0,1\\right]$ 对应 $y_4=\\left[0,0,0,1.5\\right]$。\n- 梯度 $g = \\left[1,2,3,4\\right]$。\n- 仅需强制执行最后的 $m=2$ 个曲率对，即 $\\left(s_3,y_3\\right)$ 和 $\\left(s_4,y_4\\right)$。\n\n您的程序必须为每个测试用例计算相应的搜索方向 $p=-H g$。输出格式要求：您的程序应生成单行输出，其中按顺序列出四个搜索方向向量，不含空格，并使用十进制表示法。具体而言，输出必须采用格式\n$[[p^{(1)}_1,\\dots,p^{(1)}_{n_1}],[p^{(2)}_1,\\dots,p^{(2)}_{n_2}],[p^{(3)}_1,\\dots,p^{(3)}_{n_3}],[p^{(4)}_1,\\dots,p^{(4)}_{n_4}]]$,\n其中 $p^{(j)}$ 是测试用例 $j$ 的向量。每个数字都必须以十进制形式打印（例如，-2、-1.5 或 -2.6666666667 均可接受）。最终输出必须是严格符合此方括号、逗号分隔格式的单行文本。",
            "solution": "问题陈述经过严格审查，被认定为有效。它具有科学依据，是适定的、客观且内部一致的。它提出了一个数值优化中的标准计算任务：使用有限内存 Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) 算法计算搜索方向。\n\n该问题通过一个构造性过程来定义对称正定矩阵 $H$。从初始矩阵 $H_0 = \\gamma I$（其中 $\\gamma$ 为正标量，$I$ 为单位矩阵）开始，使用提供的曲率对 $\\{(s_i, y_i)\\}$ 应用 $m$ 次 BFGS 更新序列。问题规定，更新按其索引的递增顺序应用。用于从逆 Hessian 矩阵近似 $H_k$ 得到 $H_{k+1}$ 的 BFGS 更新公式为：\n$$ H_{k+1} = (I - \\rho_k s_k y_k^\\top) H_k (I - \\rho_k y_k s_k^\\top) + \\rho_k s_k s_k^\\top $$\n其中 $\\rho_k = (y_k^\\top s_k)^{-1}$。条件 $s_k^\\top y_k  0$ 确保 $\\rho_k$ 为正，并且在整个更新过程中保持正定性。\n\n任务是计算搜索方向 $p = -Hg$，其中 $g$ 是给定的梯度向量。对于 L-BFGS 所设计用于解决的大规模问题而言，通过先构建 $n \\times n$ 矩阵 $H$ 再执行矩阵向量乘法的直接计算方法，其计算成本过高。标准且高效的方法是 L-BFGS 双循环递归，该方法无需显式构造 $H$ 即可计算乘积 $Hg$。此过程仅利用存储的 $m$ 个曲率对和初始缩放因子 $\\gamma$。这是指导该解决方案设计的原则。\n\n计算 $r=Hg$ 的算法如下。设最近的 $m$ 个曲率对记为 $\\{(s_j, y_j)\\}_{j=1}^m$，在内存窗口中按从最旧到最新的顺序排列。对每个曲率对，令 $\\rho_j = (y_j^\\top s_j)^{-1}$。\n\n1.  初始化向量 $q \\leftarrow g$。\n2.  从最新的曲率对（$j=m$）到最旧的（$j=1$）执行一次后向传递（第一个循环）：\n    对于 $j = m, m-1, \\dots, 1$：\n    -   计算并存储 $\\alpha_j \\leftarrow \\rho_j s_j^\\top q$。\n    -   更新 $q \\leftarrow q - \\alpha_j y_j$。\n3.  用初始逆 Hessian 近似来缩放中间向量：$r \\leftarrow H_0 q = \\gamma q$。\n4.  从最旧的曲率对（$j=1$）到最新的（$j=m$）执行一次前向传递（第二个循环）：\n    对于 $j = 1, 2, \\dots, m$：\n    -   计算 $\\beta_j \\leftarrow \\rho_j y_j^\\top r$。\n    -   更新 $r \\leftarrow r + s_j(\\alpha_j - \\beta_j)$。\n\n最终得到的向量 $r$ 即为乘积 $Hg$。最终的搜索方向为 $p = -r$。此过程被实现以解决每个指定的测试用例。\n\n-   **测试用例 1**：当内存 $m=0$ 时，不使用曲率对。循环不被执行。计算简化为 $p = -H_0 g = -\\gamma g$。对于 $\\gamma = 0.5$ 和 $g = [1, -2, 3]^\\top$，我们得到 $p = -0.5 \\times [1, -2, 3]^\\top = [-0.5, 1, -1.5]^\\top$。\n\n-   **测试用例 2**：对于 $m=1$ 的曲率对 $(s_1, y_1)$，应用双循环递归。我们计算 $\\rho_1 = (y_1^\\top s_1)^{-1} = 1/5 = 0.2$。算法得出 $p = [-1.8, 3.4]^\\top$。\n\n-   **测试用例 3**：当 $m=n=3$ 且曲率对为 $(s_i, y_i)$（其中 $s_i$ 是标准基向量，$y_i=As_i$，$A$ 为对角矩阵）时，已知 L-BFGS 过程在 $n$ 次更新后能精确恢复逆 Hessian 矩阵，即 $H=A^{-1}$。因此，搜索方向为 $p = -A^{-1}g$。对于 $A=\\mathrm{diag}(2,3,4)$ 和 $g = [5, -6, 7]^\\top$，可得 $p = -[\\mathrm{diag}(0.5, 1/3, 0.25)] [5, -6, 7]^\\top = [-2.5, 2, -1.75]^\\top$。双循环递归证实了此结果。\n\n-   **测试用例 4**：当内存 $m=2$ 时，仅使用最后两个曲率对 $(s_3, y_3)$ 和 $(s_4, y_4)$。这两个曲率对是正交的。初始矩阵为 $H_0 = I$ ($\\gamma=1$)。L-BFGS 更新实际上只修改了逆 Hessian 近似中对应于由 $\\{s_3, s_4\\}$ 所张成子空间的分量。计算得出的搜索方向为 $p = [-1, -2, -1.5, -8/3]^\\top$。",
            "answer": "```python\nimport numpy as np\n\ndef compute_lbfgs_direction(m, gamma, s_pairs, y_pairs, g):\n    \"\"\"\n    Computes the L-BFGS search direction p = -Hg using the two-loop recursion.\n\n    Args:\n        m (int): The memory size.\n        gamma (float): The initial scaling factor for H_0.\n        s_pairs (list of np.ndarray): List of 's' vectors {s_k}.\n        y_pairs (list of np.ndarray): List of 'y' vectors {y_k}.\n        g (np.ndarray): The current gradient vector.\n\n    Returns:\n        np.ndarray: The search direction vector p.\n    \"\"\"\n    if m == 0:\n        return -gamma * g\n\n    rhos = [1.0 / (y.T @ s) for s, y in zip(s_pairs, y_pairs)]\n    alphas = np.zeros(m)\n    \n    q = g.copy()\n\n    # First loop: from newest to oldest pair\n    for i in range(m - 1, -1, -1):\n        alphas[i] = rhos[i] * (s_pairs[i].T @ q)\n        q = q - alphas[i] * y_pairs[i]\n\n    r = gamma * q\n\n    # Second loop: from oldest to newest pair\n    for i in range(m):\n        beta = rhos[i] * (y_pairs[i].T @ r)\n        r = r + s_pairs[i] * (alphas[i] - beta)\n        \n    p = -r\n    return p\n\ndef solve():\n    \"\"\"\n    Defines the test cases from the problem statement and computes the results.\n    \"\"\"\n    # Test case 1\n    case1 = {\n        \"m\": 0, \"gamma\": 0.5, \"s_pairs\": [], \"y_pairs\": [],\n        \"g\": np.array([1., -2., 3.])\n    }\n\n    # Test case 2\n    case2 = {\n        \"m\": 1, \"gamma\": 1.0,\n        \"s_pairs\": [np.array([1., 2.])],\n        \"y_pairs\": [np.array([3., 1.])],\n        \"g\": np.array([4., -1.])\n    }\n\n    # Test case 3\n    s1_c3 = np.array([1., 0., 0.])\n    s2_c3 = np.array([0., 1., 0.])\n    s3_c3 = np.array([0., 0., 1.])\n    A_c3 = np.diag([2., 3., 4.])\n    y1_c3 = A_c3 @ s1_c3\n    y2_c3 = A_c3 @ s2_c3\n    y3_c3 = A_c3 @ s3_c3\n    case3 = {\n        \"m\": 3, \"gamma\": 1.0,\n        \"s_pairs\": [s1_c3, s2_c3, s3_c3],\n        \"y_pairs\": [y1_c3, y2_c3, y3_c3],\n        \"g\": np.array([5., -6., 7.])\n    }\n\n    # Test case 4\n    A_c4 = np.array([[4., 1., 0., 0.],\n                     [1., 3., 0., 0.],\n                     [0., 0., 2., 0.],\n                     [0., 0., 0., 1.5]])\n    s1_c4 = np.array([1., 0., 0., 0.])\n    s2_c4 = np.array([0., 1., 0., 0.])\n    s3_c4 = np.array([0., 0., 1., 0.])\n    s4_c4 = np.array([0., 0., 0., 1.])\n    y1_c4 = A_c4 @ s1_c4\n    y2_c4 = A_c4 @ s2_c4\n    y3_c4 = A_c4 @ s3_c4\n    y4_c4 = A_c4 @ s4_c4\n    m_c4 = 2\n    case4 = {\n        \"m\": m_c4, \"gamma\": 1.0,\n        \"s_pairs\": [s3_c4, s4_c4], # Only last m=2 pairs\n        \"y_pairs\": [y3_c4, y4_c4],\n        \"g\": np.array([1., 2., 3., 4.])\n    }\n\n    test_cases = [case1, case2, case3, case4]\n    \n    results = []\n    for case in test_cases:\n        p = compute_lbfgs_direction(case[\"m\"], case[\"gamma\"], case[\"s_pairs\"], case[\"y_pairs\"], case[\"g\"])\n        results.append(p)\n\n    str_results = [str(p.tolist()) for p in results]\n    final_output = f\"[{','.join(str_results)}]\"\n    \n    # Remove all whitespace to match the required output format.\n    print(final_output.replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}