{
    "hands_on_practices": [
        {
            "introduction": "To truly understand the trade-offs between different numerical methods, there's no substitute for a direct comparison. This exercise challenges you to implement both the bisection and false position methods to see firsthand how their iteration counts differ under various conditions. By applying them to the well-behaved function $f(x) = \\arctan(x)$, you will build an intuition for the steady, predictable convergence of bisection versus the often faster, but more variable, performance of the false position method .",
            "id": "2375433",
            "problem": "You are given the real-valued function $f(x) = \\arctan(x)$, where the inverse tangent is taken in radians. Consider closed intervals $\\left[a,b\\right]$ satisfying $a  b$ together with two positive tolerances $\\tau_x  0$ and $\\tau_f  0$, and a maximum iteration budget $N_{\\max} \\in \\mathbb{N}$. For each test case below, determine two integers: the number of iterations required by the bisection method and the number of iterations required by the false position (regula falsi) method to satisfy the termination criteria defined below.\n\nMethod definitions and termination criteria are as follows (they apply to both methods, with only the interior point formula differing):\n- Function and bracket prerequisites:\n  - The method acts on $f(x) = \\arctan(x)$, in radians.\n  - The initial bracket $\\left[a,b\\right]$ must satisfy either $f(a)\\cdot f(b)  0$ or $f(a) = 0$ or $f(b) = 0$.\n  - Use real arithmetic.\n- Initial zero detection without iteration:\n  - If $\\lvert f(a) \\rvert \\le \\tau_f$, return $0$ iterations.\n  - Else if $\\lvert f(b) \\rvert \\le \\tau_f$, return $0$ iterations.\n- Iterative step counting and updates:\n  - Initialize the iteration counter $n := 0$.\n  - Repeat until termination or $n = N_{\\max}$:\n    - Compute one new interior candidate $x_k$ and evaluate $f(x_k)$:\n      - Bisection interior point: $x_k = \\dfrac{a+b}{2}$.\n      - False position interior point: $x_k = a - f(a)\\dfrac{b-a}{f(b)-f(a)}$.\n    - Increase the counter: $n := n + 1$.\n    - If $\\lvert f(x_k) \\rvert \\le \\tau_f$, terminate and return $n$.\n    - Otherwise, update the bracket by replacing the endpoint that has the same sign as $f(x_k)$:\n      - If $f(a)\\cdot f(x_k)  0$, set $b := x_k$; else set $a := x_k$.\n    - After the update, if the new bracket length $\\lvert b-a \\rvert \\le \\tau_x$, terminate and return $n$.\n  - If $n$ reaches $N_{\\max}$ without meeting a termination criterion, return $n$.\n- Angle unit: all angles are in radians.\n\nTest suite. For each parameter tuple $(a,b,\\tau_x,\\tau_f,N_{\\max})$ below, compute and report the ordered pair $\\left[N_{\\text{bis}}, N_{\\text{fp}}\\right]$:\n1. $a=-1000$, $b=1001$, $\\tau_x = 10^{-12}$, $\\tau_f = 10^{-12}$, $N_{\\max}=10^{6}$.\n2. $a=-1$, $b=1$, $\\tau_x = 10^{-8}$, $\\tau_f = 10^{-12}$, $N_{\\max}=10^{6}$.\n3. $a=0$, $b=1$, $\\tau_x = 10^{-12}$, $\\tau_f = 10^{-12}$, $N_{\\max}=10^{6}$.\n4. $a=-1$, $b=0$, $\\tau_x = 10^{-12}$, $\\tau_f = 10^{-12}$, $N_{\\max}=10^{6}$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of the four ordered pairs in the exact order above, enclosed in square brackets. For example, the format must be like\n\"[[n11,n12],[n21,n22],[n31,n32],[n41,n42]]\"\nwhere $n_{i1}$ is the bisection iteration count and $n_{i2}$ is the false position iteration count for test case $i$. The final outputs must be integers. No additional text should be printed.",
            "solution": "We must quantify the iteration counts needed for the bisection and false position methods applied to the function $f(x) = \\arctan(x)$ in radians. Both methods rely on the intermediate value property and sign changes to isolate a root in a bracketing interval $\\left[a,b\\right]$. The function $f(x) = \\arctan(x)$ is continuous for all real $x$, strictly increasing, and has a unique real root at $x=0$, with $f(0)=0$. For any valid bracket $\\left[a,b\\right]$ satisfying $f(a)\\cdot f(b)  0$ or either endpoint being a root, both methods are applicable.\n\nIteration counting and termination are governed by the problem’s precisely stated rules. We now connect fundamental properties to algorithmic implications:\n\n1. Well-posedness and initial detection:\n   - Because $f$ is continuous and strictly increasing, any $\\left[a,b\\right]$ with $a0b$ must bracket the root at $x=0$ with $f(a)  0  f(b)$, unless an endpoint is already the root, that is, $f(0)=0$ when $a=0$ or $b=0$.\n   - By the rule, if initially $\\lvert f(a) \\rvert \\le \\tau_f$ or $\\lvert f(b) \\rvert \\le \\tau_f$, the iteration count is $0$.\n\n2. Bisection method fundamentals:\n   - Each iteration halves the bracket length. If $L_0 = b-a$ and $L_k$ denotes the length after $k$ updates, then $L_k = \\dfrac{L_0}{2^k}$.\n   - Under the problem’s counting convention, the width criterion is checked after updating the bracket, so achieving $\\lvert b-a \\rvert \\le \\tau_x$ requires\n     $$ k \\ge \\left\\lceil \\log_2 \\left(\\dfrac{L_0}{\\tau_x}\\right) \\right\\rceil. $$\n   - The function-value criterion is checked immediately after computing the interior point $x_k$ (the midpoint), before updating the bracket. Because the root is at $x=0$ and $0\\in [a,b]$ at all times, the midpoint $m_k$ satisfies $\\lvert m_k \\rvert \\le \\dfrac{L_{k-1}}{2}$. Moreover, for $\\lvert x \\rvert \\le 1$, we have $0 \\le \\lvert \\arctan(x) \\rvert \\le \\lvert x \\rvert$. Therefore, $\\lvert f(m_k) \\rvert \\le \\dfrac{L_{k-1}}{2}$. This shows that the function-value criterion will be satisfied once $L_{k-1} \\le 2\\tau_f$. Hence, depending on the tolerances and initial length $L_0$, termination may occur via the function-value check (before update) or via the width check (after update), with the iteration count exactly as specified by the rule sequence.\n\n3. False position (regula falsi) fundamentals:\n   - Each iteration computes the secant-interpolated root\n     $$ x_k = a - f(a)\\dfrac{b-a}{f(b)-f(a)}, $$\n     evaluates $f(x_k)$, and then replaces the endpoint having the same sign as $f(x_k)$.\n   - Because $f$ is continuous and strictly increasing, the root remains bracketed after each update. The bracketing interval $\\left[a,b\\right]$ shrinks monotonically in length (although not necessarily by a factor of $\\tfrac{1}{2}$ at every step), and the sequence $\\lvert f(x_k) \\rvert$ decreases to $0$.\n   - The termination criteria from the problem statement apply exactly as written: the function-value criterion is checked right after evaluating $f(x_k)$; if not met, the interval is updated, and then the width criterion is checked.\n\n4. Application to the test suite:\n   - Test case $1$: $a=-1000$, $b=1001$, $\\tau_x = 10^{-12}$, $\\tau_f = 10^{-12}$, $N_{\\max}=10^{6}$. Bisection needs approximately\n     $$ \\left\\lceil \\log_2\\left(\\dfrac{2001}{10^{-12}}\\right) \\right\\rceil = \\left\\lceil \\log_2\\left(2.001\\times 10^{15}\\right) \\right\\rceil = 51 $$\n     iterations if termination occurs via the width criterion. The function-value criterion can trigger when the midpoint is sufficiently close to $0$; under the given counting rule (function check before update, width check after update), the exact iteration is determined programmatically. For false position, due to near symmetry of $f(a)$ and $f(b)$ in magnitude, the first secant iterate is extremely close to $0$, so very few iterations are expected before the function-value criterion is met.\n   - Test case $2$: $a=-1$, $b=1$, $\\tau_x = 10^{-8}$, $\\tau_f = 10^{-12}$, $N_{\\max}=10^{6}$. Bisection requires\n     $$ \\left\\lceil \\log_2\\left(\\dfrac{2}{10^{-8}}\\right) \\right\\rceil = \\left\\lceil \\log_2\\left(2\\times 10^8\\right) \\right\\rceil = 28 $$\n     iterations to meet the width criterion. For false position, because $f(x)$ is odd and the bracket is symmetric, the first secant point is exactly $x=0$, so the method terminates after computing one candidate.\n   - Test cases $3$ and $4$ place the root at an endpoint: $[0,1]$ and $[-1,0]$. By the initial zero detection rule, both methods return $0$ iterations.\n\n5. Numerical computation and final aggregation:\n   - Implement the two methods strictly per the iteration counting and termination order given.\n   - For each test case, compute the ordered pair $\\left[N_{\\text{bis}}, N_{\\text{fp}}\\right]$ and collect all four pairs into a single list printed on one line, as required.\n\nThis principle-based design, combined with careful termination ordering, yields unambiguous, reproducible iteration counts that can be validated by any correct implementation using real arithmetic in radians.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport math\nfrom typing import Callable, Tuple, List\n\ndef f(x: float) -> float:\n    # arctan in radians\n    return math.atan(x)\n\ndef bisection_iterations(\n    f: Callable[[float], float],\n    a: float,\n    b: float,\n    tau_x: float,\n    tau_f: float,\n    nmax: int\n) -> int:\n    fa = f(a)\n    if abs(fa) = tau_f:\n        return 0\n    fb = f(b)\n    if abs(fb) = tau_f:\n        return 0\n    # Require a sign change if neither endpoint is within function tolerance\n    if fa * fb > 0:\n        # Not a valid bracket; in this problem's test suite, this should not happen.\n        # To keep the function total, return nmax if invalid.\n        return nmax\n\n    n = 0\n    left, right = a, b\n    f_left, f_right = fa, fb\n\n    while n  nmax:\n        x = 0.5 * (left + right)\n        fx = f(x)\n        n += 1\n\n        # Function-value termination check immediately after evaluation\n        if abs(fx) = tau_f:\n            return n\n\n        # Update the bracket by sign\n        if f_left * fx  0:\n            right = x\n            f_right = fx\n        else:\n            left = x\n            f_left = fx\n\n        # Width termination check after the bracket update\n        if abs(right - left) = tau_x:\n            return n\n\n    return n  # Reached nmax\n\ndef false_position_iterations(\n    f: Callable[[float], float],\n    a: float,\n    b: float,\n    tau_x: float,\n    tau_f: float,\n    nmax: int\n) -> int:\n    fa = f(a)\n    if abs(fa) = tau_f:\n        return 0\n    fb = f(b)\n    if abs(fb) = tau_f:\n        return 0\n    if fa * fb > 0:\n        return nmax  # invalid bracket; not expected in test suite\n\n    n = 0\n    left, right = a, b\n    f_left, f_right = fa, fb\n\n    while n  nmax:\n        denom = (f_right - f_left)\n        # Guard against pathological zero denominator; fallback to mid-point if happens.\n        if denom == 0.0:\n            x = 0.5 * (left + right)\n        else:\n            x = left - f_left * (right - left) / denom\n\n        fx = f(x)\n        n += 1\n\n        # Function-value termination check immediately after evaluation\n        if abs(fx) = tau_f:\n            return n\n\n        # Update the bracket: replace the endpoint with same sign as f(x)\n        if f_left * fx  0:\n            right = x\n            f_right = fx\n        else:\n            left = x\n            f_left = fx\n\n        # Width termination check after the bracket update\n        if abs(right - left) = tau_x:\n            return n\n\n    return n  # Reached nmax\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (a, b, tau_x, tau_f, N_max)\n    test_cases = [\n        (-1000.0, 1001.0, 1e-12, 1e-12, 1_000_000),\n        (-1.0, 1.0, 1e-8, 1e-12, 1_000_000),\n        (0.0, 1.0, 1e-12, 1e-12, 1_000_000),\n        (-1.0, 0.0, 1e-12, 1e-12, 1_000_000),\n    ]\n\n    results: List[List[int]] = []\n    for a, b, tau_x, tau_f, nmax in test_cases:\n        n_bis = bisection_iterations(f, a, b, tau_x, tau_f, nmax)\n        n_fp = false_position_iterations(f, a, b, tau_x, tau_f, nmax)\n        results.append([int(n_bis), int(n_fp)])\n\n    # Final print statement in the exact required format.\n    # Ensure a single-line output with the nested list format.\n    print(str(results).replace(\" \", \"\"))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While the false position method can be impressively fast, its performance is highly dependent on the geometry of the function. This conceptual practice explores a classic scenario where its adaptive strategy leads to extremely slow convergence, a phenomenon known as stagnation. By analyzing the behavior of the method on a function like $f(x) = \\sqrt[3]{x}$, you will uncover how a function's curvature can \"trap\" one of the bracketing endpoints, highlighting a critical limitation that every computational scientist must understand .",
            "id": "2375443",
            "problem": "In a bracketing root-finding context, consider applying the false position (regula falsi) method to the continuous, strictly increasing function $f(x)=\\sqrt[3]{x-x_{\\star}}$, which has a simple root at $x=x_{\\star}$ but an infinite slope there in the sense that $f^{\\prime}(x)$ is unbounded as $x\\to x_{\\star}$. Let the initial bracket be $[a,b]=[x_{\\star}-s_{L},\\,x_{\\star}+s_{R}]$ with $s_{L}0$ and $s_{R}0$, so that $f(a)f(b)0$. Assume standard implementations of bisection and false position.\n\nWhich of the following statements are correct?\n\nA. For the specific function $f(x)=\\sqrt[3]{x}$ with symmetric initial bracket $[-s,s]$ where $s0$, the first false position iterate equals the exact root $x=0$.\n\nB. If the initial bracket is asymmetric with $s_{L}\\ll s_{R}$, then for $f(x)=\\sqrt[3]{x-x_{\\star}}$ the false position method will repeatedly replace the right endpoint and leave the left endpoint unchanged for many iterations, causing very slow progress toward $x_{\\star}$, whereas bisection halves the interval length at each iteration regardless of the behavior of $f^{\\prime}(x_{\\star})$.\n\nC. The false position method cannot be applied to $f(x)=\\sqrt[3]{x-x_{\\star}}$ because $f^{\\prime}(x_{\\star})$ is infinite and the secant construction breaks down.\n\nD. Near a root where $\\lvert f^{\\prime}(x_{\\star})\\rvert$ is unbounded, the false position method becomes superlinearly convergent and typically outperforms bisection.\n\nE. For any valid bracket $[a,b]$ with $f(a)f(b)0$, the standard false position method necessarily replaces both endpoints infinitely often; it cannot leave one endpoint fixed over many iterations.",
            "solution": "The false position (regula falsi) method computes the next approximation $c_{k+1}$ of a root in an interval $[a_k, b_k]$ by finding the x-intercept of the secant line connecting the points $(a_k, f(a_k))$ and $(b_k, f(b_k))$. The formula is:\n$$c_{k+1} = \\frac{a_k f(b_k) - b_k f(a_k)}{f(b_k) - f(a_k)}$$\nThe given function is $f(x) = \\sqrt[3]{x-x_{\\star}}$. This function is continuous and strictly increasing. Its first derivative is $f^{\\prime}(x) = \\frac{1}{3}(x-x_{\\star})^{-2/3}$, which is unbounded as $x \\to x_{\\star}$. The second derivative is $f^{\\prime\\prime}(x) = -\\frac{2}{9}(x-x_{\\star})^{-5/3}$.\nThe sign of the second derivative determines the concavity of the function:\n- For $x  x_{\\star}$, we have $x-x_{\\star}  0$, so $(x-x_{\\star})^{-5/3}  0$. Thus, $f^{\\prime\\prime}(x)  0$, and the function is concave down.\n- For $x  x_{\\star}$, we have $x-x_{\\star}  0$, so $(x-x_{\\star})^{-5/3}  0$. Thus, $f^{\\prime\\prime}(x)  0$, and the function is concave up.\n\nThe analysis of each option is as follows.\n\nA. For the specific function $f(x)=\\sqrt[3]{x}$ with symmetric initial bracket $[-s,s]$ where $s0$, the first false position iterate equals the exact root $x=0$.\n\nWe are given $f(x) = \\sqrt[3]{x}$, which means the root is at $x_{\\star} = 0$. The initial bracket is $[a, b] = [-s, s]$ for some $s0$.\nThe function values at the endpoints are:\n$f(a) = f(-s) = \\sqrt[3]{-s} = -s^{1/3}$\n$f(b) = f(s) = \\sqrt[3]{s} = s^{1/3}$\nWe calculate the first iterate, $c_1$, using the false position formula:\n$$c_1 = \\frac{a f(b) - b f(a)}{f(b) - f(a)} = \\frac{(-s)(s^{1/3}) - (s)(-s^{1/3})}{s^{1/3} - (-s^{1/3})}$$\n$$c_1 = \\frac{-s \\cdot s^{1/3} + s \\cdot s^{1/3}}{2s^{1/3}} = \\frac{0}{2s^{1/3}}$$\nSince $s0$, the denominator $2s^{1/3}$ is non-zero. Therefore, $c_1 = 0$.\nThe first iterate is exactly the root $x_{\\star} = 0$.\nVerdict: **Correct**.\n\nB. If the initial bracket is asymmetric with $s_{L}\\ll s_{R}$, then for $f(x)=\\sqrt[3]{x-x_{\\star}}$ the false position method will repeatedly replace the right endpoint and leave the left endpoint unchanged for many iterations, causing very slow progress toward $x_{\\star}$, whereas bisection halves the interval length at each iteration regardless of the behavior of $f^{\\prime}(x_{\\star})$.\n\nLet the initial bracket be $[a_0, b_0] = [x_{\\star}-s_{L}, x_{\\star}+s_{R}]$ with $s_{L}  0$ and $s_{R}  0$.\nThe function values are $f(a_0) = \\sqrt[3]{-s_L} = -s_L^{1/3}$ and $f(b_0) = \\sqrt[3]{s_R} = s_R^{1/3}$.\nThe new iterate $c_1$ lies on the same side of $x_{\\star}$ as the endpoint that is \"flatter\", meaning where the magnitude of the function is smaller relative to its distance from the root. To determine which endpoint is stationary, we find the location of the new iterate $c_1$ relative to the root $x_{\\star}$.\n$$c_1 - x_{\\star} = \\frac{a_0 f(b_0) - b_0 f(a_0)}{f(b_0) - f(a_0)} - x_{\\star} = \\frac{(a_0-x_{\\star}) f(b_0) - (b_0-x_{\\star}) f(a_0)}{f(b_0) - f(a_0)}$$\nSubstituting the values:\n$$c_1 - x_{\\star} = \\frac{(-s_L)(s_R^{1/3}) - (s_R)(-s_L^{1/3})}{s_R^{1/3} - (-s_L^{1/3})} = \\frac{-s_L s_R^{1/3} + s_R s_L^{1/3}}{s_R^{1/3} + s_L^{1/3}}$$\n$$c_1 - x_{\\star} = \\frac{s_L^{1/3}s_R^{1/3}(s_R^{2/3} - s_L^{2/3})}{s_R^{1/3} + s_L^{1/3}}$$\nSince $s_L, s_R  0$, the sign of $(c_1 - x_{\\star})$ is determined by the sign of $(s_R^{2/3} - s_L^{2/3})$. The problem states $s_L \\ll s_R$, which implies $s_L  s_R$, and thus $s_L^{2/3}  s_R^{2/3}$. Therefore, $(s_R^{2/3} - s_L^{2/3})  0$, which means $c_1 - x_{\\star}  0$, or $c_1  x_{\\star}$.\nSince $c_1  x_{\\star}$, we have $f(c_1)  0$. The new bracket must still contain the root, so we pair $c_1$ with the endpoint that has a negative function value, which is $a_0$. The new bracket is $[a_1, b_1] = [a_0, c_1]$.\nThe left endpoint $a_0$ is retained, while the right endpoint $b_0$ is replaced by $c_1$. This process will repeat: the next iterate $c_2$ will also be greater than $x_{\\star}$, leading to the new interval $[a_0, c_2]$, and so on. The left endpoint becomes \"stuck,\" and the right endpoint slowly converges to the root from one side. This one-sided convergence is linear and can be very slow, especially because the interval length $(c_k - a_0)$ does not shrink rapidly.\nIn contrast, the bisection method's convergence is guaranteed, with the interval length being halved at each step, i.e., $|b_{k+1}-a_{k+1}| = \\frac{1}{2}|b_k-a_k|$. This property is independent of the function's shape or its derivatives.\nVerdict: **Correct**.\n\nC. The false position method cannot be applied to $f(x)=\\sqrt[3]{x-x_{\\star}}$ because $f^{\\prime}(x_{\\star})$ is infinite and the secant construction breaks down.\n\nThe false position method requires function evaluations $f(a)$ and $f(b)$ at the endpoints of the bracket. It does not use the derivative $f'(x)$ at any point. As long as the initial bracket $[a, b]$ brackets the root ($f(a)f(b)0$) and does not include the root itself as an endpoint ($a, b \\neq x_{\\star}$), the function values $f(a)$ and $f(b)$ are finite and well-defined. Since $f$ is strictly increasing, $f(a) \\neq f(b)$, so the denominator $f(b)-f(a)$ in the formula is non-zero. The method is therefore perfectly applicable. The infinite derivative at the root impacts the *rate of convergence* of the method, not its *applicability*.\nVerdict: **Incorrect**.\n\nD. Near a root where $\\lvert f^{\\prime}(x_{\\star})\\rvert$ is unbounded, the false position method becomes superlinearly convergent and typically outperforms bisection.\n\nThis statement is the opposite of the truth for this function. As analyzed in option B, the specific characteristics of $f(x)=\\sqrt[3]{x-x_{\\star}}$ (unbounded derivative and change of concavity at the root) cause the standard false position method to suffer from one-sided convergence, where one endpoint of the bracket becomes stationary. This degrades the convergence rate from superlinear to linear. Furthermore, the linear rate can be very slow (close to $1$), making it much less efficient than the bisection method, which has a reliable linear convergence rate with a factor of $0.5$. The example function is a classic case where bisection is superior to the standard false position method.\nVerdict: **Incorrect**.\n\nE. For any valid bracket $[a,b]$ with $f(a)f(b)0$, the standard false position method necessarily replaces both endpoints infinitely often; it cannot leave one endpoint fixed over many iterations.\n\nThis is a false generalization. The phenomenon of one endpoint becoming \"stuck\" or stationary for many (or all) iterations is a well-known weakness of the false position method. This typically occurs when the function is convex or concave throughout the interval containing the root. Our analysis for option B provides a direct counterexample for the given function $f(x)=\\sqrt[3]{x-x_{\\star}}$, where one endpoint is guaranteed to remain fixed. Therefore, it is false that both endpoints must be replaced infinitely often.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "Having identified the stagnation problem in the standard false position method, we now move from analysis to design. This practice guides you in engineering a more robust, \"defensive\" algorithm that combines the best of both bracketing methods. You will implement a hybrid solver that uses false position by default but intelligently detects stagnation and performs a bisection step to ensure reliable progress, creating a practical and powerful tool for your numerical toolkit .",
            "id": "2375457",
            "problem": "Design and implement a program that, for each specified test case, computes an approximation of a real root $x^\\star$ of a continuous scalar function $f:\\mathbb{R}\\to\\mathbb{R}$ in a closed interval $[a,b]$ satisfying the bracketing condition $f(a)\\cdot f(b)\\le 0$. The program must maintain the bracketing property at all times. At each iteration, the trial point $c$ must be chosen as follows: use the unique point on the straight line passing through $(a,f(a))$ and $(b,f(b))$ where this line intersects the $x$-axis, namely\n$$\nc_{\\text{rf}}=\\frac{a\\,f(b)-b\\,f(a)}{f(b)-f(a)},\n$$\nexcept when either $f(a)=f(b)$ or exactly the same endpoint has remained unchanged for at least $2$ consecutive iterations, in which case the next trial point must be the midpoint\n$$\nc_{\\text{bis}}=\\frac{a+b}{2}.\n$$\nAfter computing $c$ and $f(c)$, update the interval to $[a,c]$ or $[c,b]$ so that the updated interval continues to satisfy $f(a)\\cdot f(b)\\le 0$. If $|f(c)|\\le \\varepsilon_f$, or if $|b-a|\\le \\varepsilon_x$, terminate and return the current $c$ as the approximation. If $|f(a)|\\le \\varepsilon_f$ or $|f(b)|\\le \\varepsilon_f$ at any time, return the corresponding endpoint immediately. The iteration count must not exceed a given maximum $N_{\\max}$.\n\nFor all test cases below, use absolute function tolerance $\\varepsilon_f=10^{-12}$, absolute interval-width tolerance $\\varepsilon_x=10^{-12}$, and maximum iterations $N_{\\max}=100$. All real numbers in the output must be rounded to $10$ decimal places.\n\nTest suite to be solved by a single run of the program:\n- Case $1$: $f(x)=x^3 - x - 2$, with $a=1$ and $b=2$.\n- Case $2$: $f(x)=e^{-x} - x$, with $a=0$ and $b=1$.\n- Case $3$: $f(x)=x^3 + 10^{-6}x - 1$, with $a=0$ and $b=1$.\n- Case $4$: $f(x)=x$, with $a=0$ and $b=3$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4]$), where each $r_i$ is the approximated root for Case $i$, rounded to $10$ decimal places as specified.",
            "solution": "The problem describes a hybrid root-finding algorithm that combines the Regula Falsi (False Position) and Bisection methods. The objective is to find a root $x^\\star$ of a continuous function $f(x)$ within an interval $[a, b]$ that brackets the root (i.e., $f(a)f(b) \\le 0$).\n\nThe primary method for choosing the next trial point, $c$, is Regula Falsi, which uses the x-intercept of the secant line connecting the interval endpoints:\n$$ c_{\\text{rf}} = \\frac{a f(b) - b f(a)}{f(b) - f(a)} $$\nThis method is typically faster than bisection. However, it can suffer from very slow convergence (a phenomenon called stagnation) if the function's curvature keeps one of the endpoints of the interval fixed over many iterations.\n\nTo mitigate this, the algorithm switches to a Bisection step under specific conditions. A bisection step, which computes the trial point as the simple midpoint $c_{\\text{bis}} = (a+b)/2$, is used if the Regula Falsi formula is undefined ($f(a)=f(b)$) or if stagnation is detected. Stagnation is defined as the same endpoint remaining unchanged for at least two consecutive iterations. This hybrid approach leverages the speed of Regula Falsi in well-behaved cases while falling back on the guaranteed (though slower) progress of Bisection to break out of stagnation.\n\nThe iterative process is as follows:\n1. Check if the initial endpoints $a$ or $b$ are already roots within the given tolerance $\\varepsilon_f$.\n2. In a loop, determine whether to use Regula Falsi or Bisection based on the stagnation detection logic.\n3. Compute the trial point $c$ and evaluate $f(c)$.\n4. Check for termination: the process stops if $|f(c)| \\le \\varepsilon_f$, if the interval width $|b-a| \\le \\varepsilon_x$, or if the maximum number of iterations $N_{\\max}$ is reached.\n5. If not terminating, update the interval $[a, b]$ to either $[a, c]$ or $[c, b]$ to ensure the root remains bracketed.\n6. Track which endpoint was unchanged to inform the stagnation counter for the next iteration. If a bisection step was used, the stagnation counter is reset.\n\nThis combination creates a robust algorithm that is generally fast but also reliable, avoiding the worst-case performance of the standard Regula Falsi method.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the specified test cases and print the results.\n    \"\"\"\n\n    def find_root(f, a, b, eps_f, eps_x, n_max):\n        \"\"\"\n        Computes a root of f in [a, b] using a hybrid of Regula Falsi and Bisection.\n\n        Args:\n            f (callable): The function for which to find a root.\n            a (float): The lower bound of the interval.\n            b (float): The upper bound of the interval.\n            eps_f (float): Absolute function value tolerance.\n            eps_x (float): Absolute interval width tolerance.\n            n_max (int): Maximum number of iterations.\n\n        Returns:\n            float: The approximated root.\n        \"\"\"\n        fa = f(a)\n        fb = f(b)\n\n        # Initial checks on endpoints as per problem statement\n        if abs(fa) = eps_f:\n            return a\n        if abs(fb) = eps_f:\n            return b\n\n        # The problem statement guarantees the bracketing condition f(a)*f(b) = 0\n        \n        stagnant_counter = 0\n        # This flag tracks which endpoint ('a' or 'b') was unchanged in the last update.\n        # True if 'a' was unchanged, False if 'b' was unchanged, None otherwise.\n        last_unchanged_endpoint_is_a = None\n        \n        c = a # Initialize c to a valid float\n\n        for _ in range(n_max):\n            \n            # Check for termination based on interval width before computing new c.\n            # The problem asks to return the *current* c, which means the c from the *previous* iteration.\n            # However, for the first iteration, c is not yet well-defined as a root approximation.\n            # A more robust interpretation is to check width after the interval update.\n            # The provided code in the original problem checks it after the update, let's follow that.\n\n            # Determine if a bisection step is required\n            use_bisection = (stagnant_counter >= 2) or (fa == fb)\n            \n            # Calculate trial point c\n            if use_bisection:\n                c = (a + b) / 2.0\n            else:\n                c = (a * fb - b * fa) / (fb - fa)\n\n            fc = f(c)\n\n            # Check for termination based on function value at c\n            if abs(fc) = eps_f:\n                return c\n\n            # Update the interval and stagnation state\n            is_bisect_step = use_bisection\n\n            if fa * fc  0:  # Root is in [a, c], so b is updated\n                b = c\n                fb = fc\n                # Endpoint 'a' has remained unchanged\n                if last_unchanged_endpoint_is_a is True:\n                    stagnant_counter += 1\n                else:\n                    last_unchanged_endpoint_is_a = True\n                    stagnant_counter = 1\n            elif fb * fc  0:  # Root is in [c, b], so a is updated\n                a = c\n                fa = fc\n                # Endpoint 'b' has remained unchanged\n                if last_unchanged_endpoint_is_a is False:\n                    stagnant_counter += 1\n                else:\n                    last_unchanged_endpoint_is_a = False\n                    stagnant_counter = 1\n            else:  # This case implies fc is 0.0, which is handled by the termination condition above\n                return c\n\n            # Check for termination based on interval width after update\n            if abs(b - a) = eps_x:\n                return c\n\n            # After a bisection step, reset the stagnation counter\n            if is_bisect_step:\n                stagnant_counter = 0\n                last_unchanged_endpoint_is_a = None\n\n        # Return the last computed c if max iterations is reached\n        return c\n\n    # Define common parameters for all test cases\n    eps_f = 1e-12\n    eps_x = 1e-12\n    n_max = 100\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        (lambda x: x**3 - x - 2, 1.0, 2.0),\n        (lambda x: np.exp(-x) - x, 0.0, 1.0),\n        (lambda x: x**3 + 1e-6 * x - 1, 0.0, 1.0),\n        (lambda x: x, 0.0, 3.0),\n    ]\n\n    results = []\n    for f, a, b in test_cases:\n        root = find_root(f, a, b, eps_f, eps_x, n_max)\n        # Format the result to 10 decimal places as a string\n        results.append(f\"{root:.10f}\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\n# Execute the main function\nsolve()\n```"
        }
    ]
}