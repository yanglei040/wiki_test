{
    "hands_on_practices": [
        {
            "introduction": "这个首个实践练习将通过一个经典的优化问题，来巩固你对线搜索的理解。你将为 Rosenbrock 函数实现一个回溯线搜索算法，该函数以其狭窄、弯曲的“山谷”而著称，是检验优化算法性能的标准测试基准。通过这项练习 ，你将牢固掌握确保优化算法取得进展的核心机制——Armijo 充分下降条件。",
            "id": "2409367",
            "problem": "给定一个带有可调曲率参数的双变量 Rosenbrock 函数，对于任意 $(x,y) \\in \\mathbb{R}^2$，其定义为\n$$\nf(x,y) = (1 - x)^2 + \\kappa \\,(y - x^2)^2,\n$$\n其中 $\\kappa > 0$ 控制了弯曲峡谷的狭窄程度。当 $\\kappa$ 很大时，峡谷在曲线 $y = x^2$ 周围变得非常狭窄。考虑在当前点 $x = (x,y)$ 处，沿着最速下降方向 $p = -\\nabla f(x,y)$ 选择一个步长 $\\alpha$，使得以下充分下降条件成立：\n$$\nf(x + \\alpha p) \\le f(x) + c_1 \\,\\alpha \\,\\nabla f(x)^{\\mathsf{T}} p,\n$$\n其中 $c_1 \\in (0,1)$ 是一个固定值。如果 $\\nabla f(x)^{\\mathsf{T}} p \\ge 0$，则定义 $\\alpha = 0$。否则，在固定 $\\alpha_0 > 0$ 和 $\\beta \\in (0,1)$ 的情况下，从 $\\alpha \\in \\{\\alpha_0, \\beta \\alpha_0, \\beta^2 \\alpha_0, \\dots\\}$ 这些值中，选择第一个满足充分下降条件的 $\\alpha$。如果在最多 $N_{\\max}$ 次缩减内没有找到这样的 $\\alpha$，则定义 $\\alpha = 0$。\n\n实现一个程序，对每个测试用例，根据上述规则计算所选的 $\\alpha$，使用 $p = -\\nabla f(x,y)$。你必须根据 $f(x,y)$ 的定义计算所有需要的导数。\n\n在所有情况下，使用常量 $c_1 = 10^{-4}$，$\\beta = \\tfrac{1}{2}$ 和 $N_{\\max} = 50$。所有计算都是无单位的实数。\n\n测试集：\n- 情况 A (一般窄谷情况)：$\\kappa = 100$，$(x,y) = (-1.2, 1.0)$，$\\alpha_0 = 1$。\n- 情况 B (非常窄的峡谷)：$\\kappa = 1000$，$(x,y) = (-1.2, 1.0)$，$\\alpha_0 = 1$。\n- 情况 C (边界，驻点)：$\\kappa = 100$，$(x,y) = (1.0, 1.0)$，$\\alpha_0 = 1$。\n- 情况 D (初始步长已足够小)：$\\kappa = 100$，$(x,y) = (-1.2, 1.0)$，$\\alpha_0 = 10^{-4}$。\n\n最终输出格式：\n你的程序必须生成单行输出，其中包含一个列表，内含按测试用例顺序排列的四个选定步长，四舍五入到小数点后六位。该列表必须以逗号分隔的序列形式打印，并用方括号括起来，例如，“[a,b,c,d]”。每个条目都必须是四舍五入到小数点后六位的实数。",
            "solution": "问题陈述已经过严格验证，并被证实是有效的。它在科学上基于数值优化的原理，是一个适定问题（所有必要信息均已提供），并且以客观、明确的术语陈述。不存在任何逻辑矛盾、事实不准确或其他妨碍形式化求解的缺陷。因此，我们可以继续进行分析和实现。\n\n目标是使用回溯线搜索方法为优化算法计算步长 $\\alpha$，特别是为了满足 Armijo 充分下降条件。所考虑的函数是双变量 Rosenbrock 函数，这是无约束优化算法的一个标准基准测试。\n\nRosenbrock 函数由下式给出：\n$$\nf(x,y) = (1 - x)^2 + \\kappa (y - x^2)^2\n$$\n其中 $\\mathbf{x} = (x,y)^{\\mathsf{T}} \\in \\mathbb{R}^2$ 且 $\\kappa > 0$ 是一个控制函数峡谷曲率的正常数。\n\n任何基于梯度的方法的第一步都是计算目标函数的梯度 $\\nabla f(\\mathbf{x})$。关于 $x$ 和 $y$ 的偏导数是：\n$$\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x} \\left[ (1 - x)^2 + \\kappa (y - x^2)^2 \\right] = 2(1 - x)(-1) + \\kappa \\cdot 2(y - x^2)(-2x) = -2(1 - x) - 4\\kappa x(y - x^2)\n$$\n$$\n\\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y} \\left[ (1 - x)^2 + \\kappa (y - x^2)^2 \\right] = \\kappa \\cdot 2(y - x^2)(1) = 2\\kappa(y - x^2)\n$$\n因此，梯度向量是：\n$$\n\\nabla f(x,y) = \\begin{pmatrix} -2(1 - x) - 4\\kappa x(y - x^2) \\\\ 2\\kappa(y - x^2) \\end{pmatrix}\n$$\n\n问题指定使用最速下降方向，该方向定义为梯度的负方向：\n$$\n\\mathbf{p} = -\\nabla f(x,y)\n$$\n只要梯度非零，该方向保证函数值局部下降。\n\n问题的核心是 Armijo 充分下降条件，它确保步长 $\\alpha$ 能提供有意义的函数值下降。该条件是：\n$$\nf(\\mathbf{x} + \\alpha \\mathbf{p}) \\le f(\\mathbf{x}) + c_1 \\alpha \\nabla f(\\mathbf{x})^{\\mathsf{T}} \\mathbf{p}\n$$\n其中 $c_1 \\in (0,1)$ 是一个常数，此处给定为 $c_1 = 10^{-4}$。项 $\\nabla f(\\mathbf{x})^{\\mathsf{T}} \\mathbf{p}$ 是 $f$ 沿着方向 $\\mathbf{p}$ 的方向导数。代入 $\\mathbf{p} = -\\nabla f(\\mathbf{x})$ 可得：\n$$\n\\nabla f(\\mathbf{x})^{\\mathsf{T}} \\mathbf{p} = \\nabla f(\\mathbf{x})^{\\mathsf{T}}(-\\nabla f(\\mathbf{x})) = -\\|\\nabla f(\\mathbf{x})\\|_2^2\n$$\n该量始终为非正数。仅当 $\\nabla f(\\mathbf{x}) = \\mathbf{0}$ 时，它才等于零，这对应于一个驻点。问题正确地指定了，如果 $\\nabla f(\\mathbf{x})^{\\mathsf{T}} \\mathbf{p} \\ge 0$，则必须将 $\\alpha$ 设为 $0$。这处理了驻点的情况，此时在最速下降方向上不可能取得任何进展。\n\n寻找 $\\alpha$ 的算法过程是一个回溯搜索：\n1.  对于给定的点 $\\mathbf{x}=(x,y)$ 和参数 $\\kappa$, $\\alpha_0$, $c_1$, $\\beta$, $N_{\\max}$：\n2.  计算梯度 $\\mathbf{g} = \\nabla f(\\mathbf{x})$。\n3.  计算搜索方向 $\\mathbf{p} = -\\mathbf{g}$。\n4.  计算方向导数项 $d = \\mathbf{g}^{\\mathsf{T}}\\mathbf{p} = -\\|\\mathbf{g}\\|_2^2$。\n5.  如果 $d \\ge 0$，将最终步长设为 $0$ 并终止。\n6.  初始化试探步长 $\\alpha = \\alpha_0$。\n7.  进行最多 $N_{\\max}$ 次缩减的迭代。对于 $j$ 从 $0$ 到 $N_{\\max}$：\n    a. 计算候选点 $\\mathbf{x}_{\\text{new}} = \\mathbf{x} + \\alpha \\mathbf{p}$。\n    b. 在当前点计算函数值 $f(\\mathbf{x})$，并在候选点计算函数值 $f(\\mathbf{x}_{\\text{new}})$。\n    c. 检查 Armijo 条件是否满足：$f(\\mathbf{x}_{\\text{new}}) \\le f(\\mathbf{x}) + c_1 \\alpha d$。\n    d. 如果条件满足，则此 $\\alpha$ 即为所求步长。终止搜索并返回该值。\n    e. 如果条件不满足，则缩减步长：$\\alpha \\leftarrow \\beta \\alpha$。\n8.  如果在测试了 $\\alpha_0, \\beta\\alpha_0, \\dots, \\beta^{N_{\\max}}\\alpha_0$ 后循环结束仍未满足条件，则搜索失败。根据问题陈述，步长定义为 $0$。\n\n实现将把这一精确逻辑应用于每个指定的测试用例，并使用所提供的常量 $c_1 = 10^{-4}$，$\\beta = \\frac{1}{2}$ 和 $N_{\\max} = 50$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of finding the step length using backtracking line search\n    for the Rosenbrock function, according to the specified problem statement.\n    \"\"\"\n\n    # Global constants as per the problem\n    C1 = 1e-4\n    BETA = 0.5\n    N_MAX = 50\n\n    def rosenbrock_f(x_vec, kappa):\n        \"\"\"\n        Computes the Rosenbrock function value.\n        f(x,y) = (1 - x)^2 + kappa * (y - x^2)^2\n        \"\"\"\n        x, y = x_vec[0], x_vec[1]\n        return (1 - x)**2 + kappa * (y - x**2)**2\n\n    def rosenbrock_grad(x_vec, kappa):\n        \"\"\"\n        Computes the gradient of the Rosenbrock function.\n        \"\"\"\n        x, y = x_vec[0], x_vec[1]\n        df_dx = -2 * (1 - x) - 4 * kappa * x * (y - x**2)\n        df_dy = 2 * kappa * (y - x**2)\n        return np.array([df_dx, df_dy])\n\n    def find_step_length(kappa, x_start, alpha_0):\n        \"\"\"\n        Implements the backtracking line search to find a suitable step length alpha.\n        \"\"\"\n        x_vec = np.array(x_start, dtype=float)\n        \n        grad_f = rosenbrock_grad(x_vec, kappa)\n        p = -grad_f\n        \n        # Directional derivative term\n        grad_f_dot_p = np.dot(grad_f, p)\n\n        # If direction is not a descent direction (or at a stationary point)\n        if grad_f_dot_p >= 0:\n            return 0.0\n\n        alpha = float(alpha_0)\n        f_x = rosenbrock_f(x_vec, kappa)\n\n        # Backtracking loop: test alpha_0, beta*alpha_0, ..., beta^N_max * alpha_0\n        for _ in range(N_MAX + 1):\n            x_new = x_vec + alpha * p\n            f_x_new = rosenbrock_f(x_new, kappa)\n            \n            # Armijo sufficient decrease condition\n            if f_x_new = f_x + C1 * alpha * grad_f_dot_p:\n                return alpha\n            \n            # Reduce alpha for the next iteration\n            alpha *= BETA\n        \n        # If no suitable alpha is found within N_MAX reductions\n        return 0.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (kappa, (x, y), alpha_0)\n        (100, (-1.2, 1.0), 1.0),       # Case A\n        (1000, (-1.2, 1.0), 1.0),      # Case B\n        (100, (1.0, 1.0), 1.0),        # Case C\n        (100, (-1.2, 1.0), 1e-4),      # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        kappa, x_start, alpha_0 = case\n        result = find_step_length(kappa, x_start, alpha_0)\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "理论上的算法必须面对有限精度计算机算术的现实。本练习将深入探讨数值稳定性这一至关重要但常被忽视的问题，在这种情况下，理论上有效的步长可能因浮点误差而失效。通过诊断涉及平坦区域和极大数值状态的场景 ，你将学会如何构建能够预见并处理这些实际挑战的、更为稳健的线搜索算法。",
            "id": "2409357",
            "problem": "给定两个可微标量目标函数 $f:\\mathbb{R}\\to\\mathbb{R}$ 及其精确梯度、一个起始点 $x\\_0\\in\\mathbb{R}$、一个固定的搜索方向 $d\\in\\mathbb{R}$，以及一组定义了基于充分下降条件的步长选择规则的参数。算术运算必须以电气和电子工程师协会 (IEEE) $754$ 双精度标准进行。目标是返回一个满足充分下降不等式的步长 $\\alpha$，或者在由于算术或逻辑约束而无法实现此步长时，确定性地返回一个失败值。\n\n设 $f\\_1(x) = (x-3)^2$，其梯度为 $\\nabla f\\_1(x) = 2(x-3)$；以及 $f\\_2(x) = x^8$，其梯度为 $\\nabla f\\_2(x) = 8x^7$。对于给定的函数对 $(f,\\nabla f)$、点 $x\\_0$、方向 $d$、初始尝试步长 $\\alpha\\_00$、缩减因子 $\\rho\\in(0,1)$、充分下降常数 $c\\_1\\in(0,1)$、最小步长 $\\alpha\\_{\\min}0$ 以及最大缩减次数 $k\\_{\\max}\\in\\mathbb{N}$，定义候选步长为 $\\alpha\\_k=\\alpha\\_0\\rho^k$，其中 $k\\in\\{0,1,2,\\dots,k\\_{\\max}\\}$。如果步长 $\\alpha\\_k$ 满足以下不等式\n$$\nf(x\\_0+\\alpha\\_k d)\\ \\le\\ f(x\\_0)\\ +\\ c\\_1\\,\\alpha\\_k\\,\\nabla f(x\\_0)\\,d,\n$$\n并且浮点更新产生一个不同的点，即在 IEEE $754$ 双精度下，计算值 $\\mathrm{fl}(x\\_0+\\alpha\\_k d)$ 不完全等于 $x\\_0$，则该步长是可接受的。如果 $\\nabla f(x\\_0)\\,d\\ge 0$，您必须不进行进一步检查，直接返回 $0.0$。否则，您必须在序列 $\\{\\alpha\\_k\\}$ 中返回第一个可接受的 $\\alpha\\_k$。如果对于所有的 $k\\in\\{0,1,\\dots,k\\_{\\max}\\}$，不等式不成立，或者计算出的更新满足 $\\mathrm{fl}(x\\_0+\\alpha\\_k d)=x\\_0$，或者如果 $\\alpha\\_k\\alpha\\_{\\min}$，则返回 $0.0$。\n\n对所有测试用例使用以下参数值：$\\alpha\\_0=1.0$，$\\rho=0.5$， $c\\_1=10^{-4}$，$\\alpha\\_{\\min}=10^{-16}$，$k\\_{\\max}=1000$。\n\n测试套件：\n- 情况 1 (一般良态下降)：使用 $f=f\\_1$，$x\\_0=0.0$ 和 $d=-\\nabla f\\_1(x\\_0)$。\n- 情况 2 (导致微小步长的极平坦区域)：使用 $f=f\\_2$，$x\\_0=10^{-3}$ 和 $d=-\\nabla f\\_2(x\\_0)$。\n- 情况 3 (大数量级状态导致的算术停滞)：使用 $f=f\\_1$，$x\\_0=10^{16}$ 和 $d=-1.0$。\n- 情况 4 (非下降方向)：使用 $f=f\\_1$，$x\\_0=0.0$ 和 $d=+\\nabla f\\_1(x\\_0)$。\n\n您的程序必须实现上述选择规则，并为每个情况生成返回的步长 $\\alpha$ (作为浮点数)。最终输出必须将所有情况的结果按顺序聚合为单行：一个无空格、逗号分隔的 Python 风格列表，例如 $[a\\_1,a\\_2,a\\_3,a\\_4]$，其中每个 $a\\_i$ 是为情况 $i$ 返回的步长。\n\n本问题中没有物理单位或角度。所有实数都必须视为无单位的标量。程序所需的最终输出是包含列表 $[a\\_1,a\\_2,a\\_3,a\\_4]$ 的单行文本。",
            "solution": "该问题要求实现一个回溯线搜索算法，以找到一个满足充分下降条件的可接受步长 $\\alpha$，同时该算法对有限精度浮点运算的限制具有鲁棒性。分析和实现必须遵循所提供的参数和测试用例。\n\n该算法的核心是充分下降条件，也称为 Armijo 条件，它确保步长 $\\alpha$ 能导致目标函数 $f$ 的有意义的减小。对于起始点 $x_0$ 和搜索方向 $d$，一个可接受的步长 $\\alpha_k$ 必须满足：\n$$\nf(x_0 + \\alpha_k d) \\le f(x_0) + c_1 \\alpha_k \\nabla f(x_0) d\n$$\n其中 $c_1 \\in (0, 1)$ 是一个常数。该条件仅在 $d$ 是下降方向时才有意义，即方向导数 $\\nabla f(x_0) d  0$。如果 $\\nabla f(x_0) d \\ge 0$，则该方向不是下降方向，搜索必须终止。\n\n该算法通过从初始猜测值 $\\alpha_0$ 开始，并以因子 $\\rho \\in (0,1)$ 迭代地减小它来搜索可接受的 $\\alpha$，从而生成一系列尝试步长 $\\alpha_k = \\alpha_0 \\rho^k$，其中 $k = 0, 1, 2, \\dots$。选择此序列中第一个满足条件的 $\\alpha_k$。\n\n引入了两个源于数值计算的实际约束：\n$1$. 最小步长 $\\alpha_{\\min}$：如果 $\\alpha_k$ 小于此阈值，则搜索终止，表明进一步的进展可以忽略不计。\n$2$. 算术停滞：由于 IEEE $754$ 双精度算术的有限精度，更新 $x_{new} = \\mathrm{fl}(x_0 + \\alpha_k d)$ 可能导致 $x_{new}$ 与 $x_0$ 完全相等。当变化量 $|\\alpha_k d|$ 相对于 $x_0$ 的量级太小而无法表示时，会发生这种情况。这样的步长是无效的，必须被拒绝。\n\n完整的指定流程如下：\n首先，通过检查 $\\nabla f(x_0) d \\ge 0$ 来验证搜索方向 $d$ 是否为下降方向。如果该条件成立，算法必须返回 $0.0$。否则，从 $0$ 到 $k_{\\max}$ 迭代 $k$。在每次迭代中，计算 $\\alpha_k = \\alpha_0 \\rho^k$。如果 $\\alpha_k  \\alpha_{\\min}$，则终止并返回 $0.0$。接下来，通过计算 $x_{new} = \\mathrm{fl}(x_0 + \\alpha_k d)$ 并测试是否 $x_{new} == x_0$ 来检查停滞。如果它们相等，则步长太小；拒绝 $\\alpha_k$ 并继续下一次迭代。如果没有停滞，则检查充分下降条件。如果满足该条件，$\\alpha_k$ 就是所需的步长，算法返回该值。如果循环完成仍未找到可接受的步长，则返回 $0.0$。\n\n我们将此算法应用于四个测试用例，使用参数 $\\alpha_0=1.0$，$\\rho=0.5$，$c_1=10^{-4}$，$\\alpha_{\\min}=10^{-16}$ 和 $k_{\\max}=1000$。\n\n情况 1：$f(x)=f_1(x)=(x-3)^2$，$x_0=0.0$，$d = -\\nabla f_1(x_0)$。\n首先，我们计算梯度和方向：$\\nabla f_1(x_0) = 2(0.0 - 3) = -6.0$。方向是 $d = -(-6.0) = 6.0$。\n方向导数为 $\\nabla f_1(x_0) d = (-6.0)(6.0) = -36.0$。因为这是负数，我们继续。我们有 $f(x_0) = (0.0-3)^2 = 9.0$。\n对于 $k=0$，$\\alpha_0=1.0$。新点是 $x_{new} = 0.0 + 1.0 \\cdot 6.0 = 6.0$。没有发生停滞。我们检查条件：\n$f(6.0) \\le f(0.0) + c_1 \\alpha_0 \\nabla f_1(0.0) d \\implies (6.0-3)^2 \\le 9.0 + 10^{-4}(1.0)(-36.0) \\implies 9.0 \\le 8.9964$。这是不成立的。\n对于 $k=1$，$\\alpha_1=0.5$。新点是 $x_{new} = 0.0 + 0.5 \\cdot 6.0 = 3.0$。没有发生停滞。我们检查条件：\n$f(3.0) \\le f(0.0) + c_1 \\alpha_1 \\nabla f_1(0.0) d \\implies (3.0-3)^2 \\le 9.0 + 10^{-4}(0.5)(-36.0) \\implies 0.0 \\le 8.9982$。这是成立的。\n第一个可接受的步长是 $\\alpha_1=0.5$。\n\n情况 2：$f(x)=f_2(x)=x^8$，$x_0=10^{-3}$，$d = -\\nabla f_2(x_0)$。\n梯度为 $\\nabla f_2(x_0) = 8(10^{-3})^7 = 8 \\cdot 10^{-21}$。方向为 $d = -8 \\cdot 10^{-21}$。\n方向导数为 $\\nabla f_2(x_0) d = (8 \\cdot 10^{-21})(-8 \\cdot 10^{-21}) = -64 \\cdot 10^{-42}  0$。\n点更新为 $x_{new} = \\mathrm{fl}(10^{-3} - \\alpha_k \\cdot 8 \\cdot 10^{-21})$。变化的量级是 $|\\alpha_k d| = \\alpha_k \\cdot 8 \\cdot 10^{-21}$。对于 $\\alpha_k \\le 1.0$，这个变化最多是 $8 \\cdot 10^{-21}$。在双精度下，$x_0=10^{-3}$ 的末位单位 (ULP) 约为 $1.36 \\cdot 10^{-19}$。由于变化的量级远小于 $x_0$ 的 ULP，加法操作将被吸收，导致对于所有的尝试步长，$\\mathrm{fl}(x_0 + \\alpha_k d) = x_0$。因此，对于每个 $k \\in \\{0, \\dots, k_{\\max}\\}$，停滞条件 $x_{new} == x_0$ 都将为真。循环将继续直到所有迭代都用尽，函数将返回 $0.0$。\n\n情况 3：$f(x)=f_1(x)=(x-3)^2$，$x_0=10^{16}$，$d=-1.0$。\n梯度为 $\\nabla f_1(x_0) = 2(10^{16}-3)$，约等于 $2 \\cdot 10^{16}$。\n方向导数为 $\\nabla f_1(x_0) d \\approx (2 \\cdot 10^{16})(-1.0) = -2 \\cdot 10^{16}  0$。\n点更新为 $x_{new} = \\mathrm{fl}(10^{16} - \\alpha_k)$。$x_0=10^{16}$ 的 ULP 是 $2.0$。尝试步长为 $\\alpha_k = 0.5^k$，所有这些值都 $\\le 1.0$。因为每个 $\\alpha_k$ 都小于 $x_0$ 的 ULP，减法操作将因吸收而丢失，$\\mathrm{fl}(10^{16} - \\alpha_k)$ 的计算结果将为 $10^{16}$。所有尝试步长都会发生停滞。循环完成时未找到有效步长，因此函数返回 $0.0$。\n\n情况 4：$f(x)=f_1(x)=(x-3)^2$，$x_0=0.0$，$d=+\\nabla f_1(x_0)$。\n梯度为 $\\nabla f_1(x_0) = -6.0$，所以方向是 $d=-6.0$。\n方向导数为 $\\nabla f_1(x_0) d = (-6.0)(-6.0) = 36.0$。\n由于方向导数 $36.0 \\ge 0$，方向 $d$ 不是下降方向。根据指定的流程，算法必须立即终止并返回 $0.0$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    # --- Fixed Parameters ---\n    ALPHA0 = 1.0\n    RHO = 0.5\n    C1 = 1e-4\n    ALPHA_MIN = 1e-16\n    K_MAX = 1000\n\n    # --- Objective Functions and Gradients ---\n    def f1(x: float) - float:\n        return (x - 3.0)**2\n\n    def grad_f1(x: float) - float:\n        return 2.0 * (x - 3.0)\n\n    def f2(x: float) - float:\n        return x**8\n\n    def grad_f2(x: float) - float:\n        return 8.0 * x**7\n\n    def line_search(f, grad_f, x0: float, d: float) - float:\n        \"\"\"\n        Implements the backtracking line search algorithm.\n\n        Args:\n            f: The objective function.\n            grad_f: The gradient of the objective function.\n            x0: The starting point.\n            d: The search direction.\n\n        Returns:\n            The acceptable step size alpha, or 0.0 on failure.\n        \"\"\"\n        # All arithmetic is performed in IEEE 754 double precision,\n        # which is the standard for Python's float type.\n        \n        directional_derivative = grad_f(x0) * d\n\n        # Condition: Must be a descent direction\n        if directional_derivative >= 0:\n            return 0.0\n\n        f_x0 = f(x0)\n\n        for k in range(K_MAX + 1):\n            alpha_k = ALPHA0 * (RHO**k)\n\n            # Condition: Step size must not be smaller than the minimum\n            if alpha_k  ALPHA_MIN:\n                return 0.0\n\n            # Compute the new point and check for arithmetic stagnation\n            x_new = x0 + alpha_k * d\n            if x_new == x0:\n                continue  # Step is too small to change x0, try a smaller alpha\n\n            # Condition: Sufficient decrease (Armijo condition)\n            if f(x_new) = f_x0 + C1 * alpha_k * directional_derivative:\n                return alpha_k  # Acceptable step size found\n\n        # Failure: No acceptable step size found within k_max iterations\n        return 0.0\n\n    # --- Test Suite Definition ---\n    # Each case is a tuple: (function, gradient, start_point, direction_lambda)\n    # The direction_lambda calculates d based on the start point x0.\n    test_cases = [\n        # Case 1: General well-conditioned decrease\n        (f1, grad_f1, 0.0, lambda x: -grad_f1(x)),\n        # Case 2: Very flat plateau causing microscopic steps\n        (f2, grad_f2, 1e-3, lambda x: -grad_f2(x)),\n        # Case 3: Arithmetic stagnation from large magnitude state\n        (f1, grad_f1, 1e16, lambda x: -1.0),\n        # Case 4: Non-descent direction\n        (f1, grad_f1, 0.0, lambda x: grad_f1(x)),\n    ]\n\n    results = []\n    for f_handle, grad_f_handle, x0_val, d_lambda in test_cases:\n        direction = d_lambda(x0_val)\n        alpha_result = line_search(f_handle, grad_f_handle, x0_val, direction)\n        results.append(alpha_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "许多现实世界中的优化问题并非无约束的，而是在确定的边界内进行。这最后一个练习将你的技能扩展到有约束优化领域，为你介绍处理箱式约束的方法。你将探索两种常用策略——投影法和截断法——以确保每一步都保持在可行域内 ，这是将优化方法应用于工程和数据科学问题的基本技能。",
            "id": "2409334",
            "problem": "给定一个连续可微的目标函数 $f : \\mathbb{R}^n \\to \\mathbb{R}$，其梯度为 $\\nabla f$，一个满足分量形式的箱式约束 $l \\le x \\le u$ 的当前可行点 $x \\in \\mathbb{R}^n$，以及一个搜索方向 $p \\in \\mathbb{R}^n$。考虑一个离散的候选步长集合\n$$\n\\mathcal{S} = \\{\\alpha_0 \\rho^k \\mid k \\in \\{0,1,\\dots,K\\}\\},\n$$\n其中 $\\alpha_0 \\in \\mathbb{R}_{0}$，$\\rho \\in \\mathbb{R}$ 满足 $0  \\rho  1$，且 $K \\in \\mathbb{N}$ 是有限的。将到箱体 $[l,u]$ 的分量投影定义为\n$$\n\\Pi_{[l,u]}(z)_i = \\min\\{\\max\\{z_i, l_i\\}, u_i\\}, \\quad i = 1,\\dots,n.\n$$\n\n考虑两种强制可行性的模式，用于从候选步长 $\\alpha \\in \\mathcal{S}$ 构建试验点：\n\n- 投影模式：$s(\\alpha) = \\Pi_{[l,u]}(x + \\alpha p) - x$ 且 $x_{\\text{trial}}(\\alpha) = x + s(\\alpha)$。\n- 截断模式：首先定义沿 $p$ 的最大可行步长\n$$\n\\alpha_{\\max}(x,p) = \\min_{i=1,\\dots,n} \\begin{cases}\n\\dfrac{u_i - x_i}{p_i},   \\text{若 } p_i  0, \\\\\n\\dfrac{l_i - x_i}{p_i},   \\text{若 } p_i  0, \\\\\n+\\infty,   \\text{若 } p_i = 0,\n\\end{cases}\n$$\n然后设置 $s(\\alpha) = \\min\\{\\alpha,\\alpha_{\\max}(x,p)\\} \\, p$ 且 $x_{\\text{trial}}(\\alpha) = x + s(\\alpha)$。\n\n对于任一模式，如果带有实际步长向量 $s(\\alpha)$ 的 Armijo 充分下降条件成立，则接受候选步长 $\\alpha \\in \\mathcal{S}$：\n$$\nf\\big(x_{\\text{trial}}(\\alpha)\\big) \\le f(x) + c_1 \\, \\nabla f(x)^\\top s(\\alpha),\n$$\n其中 $c_1 \\in \\mathbb{R}$ 满足 $0  c_1  1$。在 $\\mathcal{S}$ 中所有被接受的候选项中，将 $\\alpha^\\star$ 定义为值最大的那个。如果没有 $\\alpha \\in \\mathcal{S}$ 被接受，则将 $\\alpha^\\star$ 定义为 $\\mathcal{S}$ 中的最小元素，即 $\\alpha^\\star = \\alpha_0 \\rho^K$。\n\n你的任务是编写一个完整的程序，对于给定的测试套件，计算每个测试用例的 $\\alpha^\\star$ 并将所有结果输出到单行中。所有量都是无量纲的，不涉及物理单位。\n\n使用以下目标函数及其梯度：\n- 二维变量的二次函数（$n = 2$）：\n$$\nf_{\\text{quad}}(x) = \\tfrac{1}{2} x^\\top Q x + b^\\top x, \\quad \\nabla f_{\\text{quad}}(x) = Q x + b,\n$$\n其中\n$$\nQ = \\begin{bmatrix} 3  1 \\\\ 1  2 \\end{bmatrix}, \\quad b = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}.\n$$\n- 二维变量的 Rosenbrock 函数（$n = 2$）：\n$$\nf_{\\text{ros}}(x_1,x_2) = 100\\,(x_2 - x_1^2)^2 + (1 - x_1)^2,\n$$\n其梯度为\n$$\n\\nabla f_{\\text{ros}}(x_1,x_2) = \\begin{bmatrix}\n-400 x_1 (x_2 - x_1^2) - 2(1 - x_1) \\\\\n200 (x_2 - x_1^2)\n\\end{bmatrix}.\n$$\n\n为以下每个测试用例计算 $\\alpha^\\star$。每个测试用例指定了 $(f,\\nabla f)$、模式、点 $x$、方向 $p$、边界 $l,u$ 以及参数 $\\alpha_0$、$\\rho$、$c_1$ 和 $K$：\n\n- 测试用例 1（理想情况，内部点，投影模式）：$f = f_{\\text{quad}}$，$\\nabla f = \\nabla f_{\\text{quad}}$，模式 = 投影，$x = [0, 0]^\\top$，$p = -\\nabla f(x)$，$l = [-5,-5]^\\top$，$u = [5,5]^\\top$，$\\alpha_0 = 1.0$，$\\rho = 0.5$，$c_1 = 1 \\times 10^{-4}$，$K = 20$。\n- 测试用例 2（边界截断，方向受箱体限制）：$f = f_{\\text{quad}}$，$\\nabla f = \\nabla f_{\\text{quad}}$，模式 = 截断，$x = [1.9, -1.9]^\\top$，$p = [1.0, -50.0]^\\top$，$l = [-2,-2]^\\top$，$u = [2,2]^\\top$，$\\alpha_0 = 1.0$，$\\rho = 0.5$，$c_1 = 1 \\times 10^{-4}$，$K = 20$。\n- 测试用例 3（非凸情况，需要缩减，投影模式）：$f = f_{\\text{ros}}$，$\\nabla f = \\nabla f_{\\text{ros}}$，模式 = 投影，$x = [-1.2, 1.0]^\\top$，$p = -\\nabla f(x)$，$l = [-2,-1]^\\top$，$u = [2,3]^\\top$，$\\alpha_0 = 1.0$，$\\rho = 0.5$，$c_1 = 1 \\times 10^{-4}$，$K = 30$。\n- 测试用例 4（零方向边缘情况）：$f = f_{\\text{quad}}$，$\\nabla f = \\nabla f_{\\text{quad}}$，模式 = 投影，$x = [0.5, -0.5]^\\top$，$p = [0.0, 0.0]^\\top$，$l = [-1,-1]^\\top$，$u = [1,1]^\\top$，$\\alpha_0 = 1.0$，$\\rho = 0.5$，$c_1 = 1 \\times 10^{-4}$，$K = 10$。\n- 测试用例 5（投影在边界处截断一个坐标）：$f = f_{\\text{quad}}$，$\\nabla f = \\nabla f_{\\text{quad}}$，模式 = 投影，$x = [2.0, 0.0]^\\top$，$p = [1.0, -1.0]^\\top$，$l = [-2,-2]^\\top$，$u = [2,2]^\\top$，$\\alpha_0 = 1.0$，$\\rho = 0.5$，$c_1 = 1 \\times 10^{-4}$，$K = 20$。\n\n你的程序必须为每个测试用例计算 $\\alpha^\\star$，并生成一行输出，其中包含一个逗号分隔的十进制数列表，每个数精确到小数点后六位，并用方括号括起来。例如，输出格式必须是\n$$\n[\\alpha^\\star_1,\\alpha^\\star_2,\\alpha^\\star_3,\\alpha^\\star_4,\\alpha^\\star_5]\n$$\n其中每个 $\\alpha^\\star_i$ 都打印到小数点后六位。",
            "solution": "目标是为每个测试用例，在一个离散集合中确定满足 Armijo 充分下降条件并满足箱式约束的最大步长。箱式约束可以通过试验点投影或步长截断来强制执行。\n\n我们从基本原理开始。给定 $x \\in \\mathbb{R}^n$，一个箱体 $[l,u]$（其中 $l \\le u$ 分量成立），以及一个方向 $p \\in \\mathbb{R}^n$，步长的可行性通过投影或截断来定义。投影算子 $\\Pi_{[l,u]}$ 按分量定义为\n$$\n\\Pi_{[l,u]}(z)_i = \\min\\{\\max\\{z_i,l_i\\},u_i\\}.\n$$\n这是到欧几里得空间中闭凸集 $[l,u]$ 上的正交投影。它确保对于任何 $\\alpha \\ge 0$，$x_{\\text{trial}}(\\alpha) = \\Pi_{[l,u]}(x+\\alpha p)$ 都是可行的。\n\n或者，可以从每个分量的不等式 $l_i \\le x_i + \\alpha p_i \\le u_i$ 推导出沿射线 $x + \\alpha p$ 的最大可行步长。对这些不等式求解 $\\alpha$ 可得边界\n$$\n\\alpha \\le \\frac{u_i - x_i}{p_i} \\text{ 若 } p_i  0, \\quad\n\\alpha \\le \\frac{l_i - x_i}{p_i} \\text{ 若 } p_i  0, \\quad\n\\alpha \\in \\mathbb{R}_{\\ge 0} \\text{ 若 } p_i = 0 \\text{ 则任意}.\n$$\n能同时为所有分量保持可行性的最大 $\\alpha$ 是\n$$\n\\alpha_{\\max}(x,p) = \\min_{i=1,\\dots,n} \\begin{cases}\n\\dfrac{u_i - x_i}{p_i},   \\text{若 } p_i  0, \\\\\n\\dfrac{l_i - x_i}{p_i},   \\text{若 } p_i  0, \\\\\n+\\infty,   \\text{若 } p_i = 0,\n\\end{cases}\n$$\n在截断模式中，使用实际步长向量 $s(\\alpha) = \\min\\{\\alpha,\\alpha_{\\max}(x,p)\\} p$，这保证了 $x + s(\\alpha)$ 位于箱体内。\n\nArmijo 充分下降条件是一个一阶准则，它确保目标函数在试验点的下降与当前点的方向导数相称，并乘以一个因子 $c_1 \\in (0,1)$。对于实际步长向量 $s(\\alpha)$ 和试验点 $x_{\\text{trial}}(\\alpha) = x + s(\\alpha)$，条件为\n$$\nf\\big(x_{\\text{trial}}(\\alpha)\\big) \\le f(x) + c_1 \\nabla f(x)^\\top s(\\alpha).\n$$\n注意，右侧使用的是 $f$ 在 $x$ 点沿特定位移 $s(\\alpha)$ 的线性模型，在投影模式下，由于截断，$s(\\alpha)$ 可能与 $p$ 不共线。上述形式是两种模式的自然推广，因为它表达了相对于所采取的实际步长的充分下降。\n\n为确保选择是有限且适定的，我们将候选步长限制在一个有限的几何集 $\\mathcal{S} = \\{\\alpha_0 \\rho^k \\mid k = 0,1,\\dots,K\\}$，其中 $\\alpha_0  0$ 且 $0  \\rho  1$。任务是选择\n$$\n\\alpha^\\star = \\max\\{\\alpha \\in \\mathcal{S} \\mid f(x_{\\text{trial}}(\\alpha)) \\le f(x) + c_1 \\nabla f(x)^\\top s(\\alpha)\\},\n$$\n并约定如果没有这样的 $\\alpha$ 存在，则 $\\alpha^\\star = \\alpha_0 \\rho^K$。\n\n我们通过按 $\\alpha$ 的降序对候选项评估不等式来实现此选择，并接受第一个满足不等式的候选项；如果没有候选项满足，则返回最小的 $\\alpha$。\n\n具体的目标函数和梯度是：\n- 二次函数：\n$$\nf_{\\text{quad}}(x) = \\tfrac{1}{2} x^\\top Q x + b^\\top x, \\quad \\nabla f_{\\text{quad}}(x) = Q x + b, \\quad\nQ = \\begin{bmatrix} 3  1 \\\\ 1  2 \\end{bmatrix}, \\quad b = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}.\n$$\n- Rosenbrock 函数：\n$$\nf_{\\text{ros}}(x_1,x_2) = 100\\,(x_2 - x_1^2)^2 + (1 - x_1)^2,\n$$\n$$\n\\nabla f_{\\text{ros}}(x_1,x_2) = \\begin{bmatrix}\n-400 x_1 (x_2 - x_1^2) - 2(1 - x_1) \\\\\n200 (x_2 - x_1^2)\n\\end{bmatrix}.\n$$\n\n使用的关键属性：\n- 对于投影模式，根据 $\\Pi_{[l,u]}$ 的定义，对任何 $\\alpha \\ge 0$ 都保证可行性。\n- 对于截断模式，通过构造 $\\alpha_{\\max}(x,p)$ 并使用 $s(\\alpha) = \\min\\{\\alpha,\\alpha_{\\max}(x,p)\\} p$ 来保证可行性。\n- 如果 $s(\\alpha) = 0$，则 Armijo 不等式以等式形式成立，因为 $f(x_{\\text{trial}}(\\alpha)) = f(x)$ 且 $\\nabla f(x)^\\top s(\\alpha) = 0$。\n\n对测试套件中边缘情况的讨论：\n- 测试用例 1 在箱体内部运行良好；对于足够小的 $\\alpha$，投影不会改变步长。下降方向 $p = -\\nabla f(x)$ 确保对于某个 $\\alpha \\in \\mathcal{S}$，Armijo 不等式将得到满足。\n- 测试用例 2 展示了在边界处的截断。方向 $p$ 同时试图增加第一个坐标并大幅减少第二个坐标。值 $\\alpha_{\\max}(x,p)$ 由第二个坐标的下界决定，这决定了截断模式下的有效步长。如果 Armijo 条件在最大候选步长被截断后仍不满足，进一步减小 $\\alpha$ 最终会减小实际步长 $s(\\alpha)$ 并导致接受。\n- 测试用例 3 是非凸的；从 Rosenbrock 谷附近开始，完整步长可能不满足 Armijo 条件。通过 $\\rho$ 进行的离散化缩减确保了足够小的步长将满足不等式。\n- 测试用例 4 中 $p = 0$。此时对于所有 $\\alpha$ 都有 $s(\\alpha) = 0$，Armijo 条件平凡成立。根据定义选择最大的候选项，所以 $\\alpha^\\star = \\alpha_0$。\n- 测试用例 5 从第一个坐标的上界开始，并沿该坐标向外移动，因此投影将步长的第一个分量截断为零，同时允许在第二个分量上移动。Armijo 条件评估的是沿实际位移 $s(\\alpha)$ 的下降，对于小的 $\\alpha$，该位移在第二个分量上与负梯度对齐。\n\n程序的数值计算细节：\n- 每个测试用例在当前点 $x$ 处评估一次 $f$ 和 $\\nabla f$。\n- 对于 $\\mathcal{S}$ 中每个按降序排列的 $\\alpha$，根据指定模式计算 $s(\\alpha)$ 并检查 Armijo 不等式。\n- 选择可接受的最大 $\\alpha$，如果没有满足条件的，则选择最小的候选项。\n- 按照要求，将每个 $\\alpha^\\star$ 精确到小数点后六位进行报告。\n\n该程序直接实现了这些定义，确保了基于上述数学陈述的正确性。对于截断模式，$\\alpha_{\\max}(x,p)$ 是分量计算的，并在 $p_i = 0$ 时设为 $+\\infty$。对于投影模式，投影算子是分量应用的。在所有情况下，Armijo 条件都使用实际步长向量 $s(\\alpha)$ 进行评估，这与约束环境中的一阶充分下降原则相符。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f_quad(x):\n    Q = np.array([[3.0, 1.0],\n                  [1.0, 2.0]])\n    b = np.array([-1.0, 2.0])\n    return 0.5 * x @ (Q @ x) + b @ x\n\ndef grad_quad(x):\n    Q = np.array([[3.0, 1.0],\n                  [1.0, 2.0]])\n    b = np.array([-1.0, 2.0])\n    return Q @ x + b\n\ndef f_rosen(x):\n    x1, x2 = x[0], x[1]\n    return 100.0 * (x2 - x1**2)**2 + (1.0 - x1)**2\n\ndef grad_rosen(x):\n    x1, x2 = x[0], x[1]\n    df_dx1 = -400.0 * x1 * (x2 - x1**2) - 2.0 * (1.0 - x1)\n    df_dx2 = 200.0 * (x2 - x1**2)\n    return np.array([df_dx1, df_dx2], dtype=float)\n\ndef project_box(z, l, u):\n    return np.minimum(np.maximum(z, l), u)\n\ndef alpha_max_truncation(x, p, l, u):\n    # Compute maximum feasible alpha along direction p from x within [l,u]\n    alpha_max = np.inf\n    for i in range(len(x)):\n        if p[i] > 0:\n            cand = (u[i] - x[i]) / p[i]\n            if cand  alpha_max:\n                alpha_max = cand\n        elif p[i]  0:\n            cand = (l[i] - x[i]) / p[i]\n            if cand  alpha_max:\n                alpha_max = cand\n        else:\n            # p[i] == 0: no restriction from this component\n            pass\n    if not np.isfinite(alpha_max):\n        alpha_max = np.inf\n    if alpha_max  0:\n        # If negative due to numerical issues (should not happen if x is feasible)\n        alpha_max = 0.0\n    return alpha_max\n\ndef line_search_alpha_star(f, grad, x, p, l, u, mode, alpha0, rho, c1, K):\n    # Precompute current function value and gradient\n    fx = f(x)\n    g = grad(x)\n    # Generate candidate alphas in decreasing order\n    alphas = [alpha0 * (rho ** k) for k in range(0, K + 1)]\n    accepted_alpha = None\n\n    # Tolerance to guard against tiny numerical violations\n    tol = 1e-12\n\n    if mode == 'truncate':\n        amax = alpha_max_truncation(x, p, l, u)\n\n    for alpha in alphas:\n        if mode == 'project':\n            x_trial = project_box(x + alpha * p, l, u)\n            s = x_trial - x\n        elif mode == 'truncate':\n            alpha_eff = min(alpha, amax)\n            s = alpha_eff * p\n            x_trial = x + s\n            # Make sure numerical drift stays within bounds\n            x_trial = project_box(x_trial, l, u)\n        else:\n            raise ValueError(\"Unknown mode\")\n\n        lhs = f(x_trial)\n        rhs = fx + c1 * (g @ s)\n        if lhs = rhs + tol:\n            accepted_alpha = alpha\n            break\n\n    if accepted_alpha is None:\n        accepted_alpha = alphas[-1]\n    return accepted_alpha\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (f, grad, mode, x, p, l, u, alpha0, rho, c1, K)\n        # Test case 1\n        (f_quad, grad_quad, 'project',\n         np.array([0.0, 0.0]), None,\n         np.array([-5.0, -5.0]), np.array([5.0, 5.0]),\n         1.0, 0.5, 1e-4, 20),\n        # Test case 2\n        (f_quad, grad_quad, 'truncate',\n         np.array([1.9, -1.9]), np.array([1.0, -50.0]),\n         np.array([-2.0, -2.0]), np.array([2.0, 2.0]),\n         1.0, 0.5, 1e-4, 20),\n        # Test case 3\n        (f_rosen, grad_rosen, 'project',\n         np.array([-1.2, 1.0]), None,\n         np.array([-2.0, -1.0]), np.array([2.0, 3.0]),\n         1.0, 0.5, 1e-4, 30),\n        # Test case 4\n        (f_quad, grad_quad, 'project',\n         np.array([0.5, -0.5]), np.array([0.0, 0.0]),\n         np.array([-1.0, -1.0]), np.array([1.0, 1.0]),\n         1.0, 0.5, 1e-4, 10),\n        # Test case 5\n        (f_quad, grad_quad, 'project',\n         np.array([2.0, 0.0]), np.array([1.0, -1.0]),\n         np.array([-2.0, -2.0]), np.array([2.0, 2.0]),\n         1.0, 0.5, 1e-4, 20),\n    ]\n\n    results = []\n    for f, grad, mode, x, p, l, u, alpha0, rho, c1, K in test_cases:\n        # If p is None in projection cases where p = -grad(x)\n        if p is None:\n            p = -grad(x)\n        alpha_star = line_search_alpha_star(f, grad, x, p, l, u, mode, alpha0, rho, c1, K)\n        results.append(alpha_star)\n\n    # Format results to exactly six decimal places\n    formatted = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}