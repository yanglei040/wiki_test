{
    "hands_on_practices": [
        {
            "introduction": "A common intuition in approximation is that more terms in a Taylor series always lead to a better result. This practice  challenges that notion by exploring a scenario where a second-order approximation can have a larger potential error than a first-order one. By analyzing the Lagrange remainder, you will derive a condition that precisely defines when this counter-intuitive behavior occurs, highlighting that the quality of an approximation depends on a delicate balance between the order of the polynomial and the behavior of higher derivatives. This exercise is invaluable for developing a deeper, more critical understanding of error bounds and their practical implications in model selection.",
            "id": "2442165",
            "problem": "In a reduced-order modeling routine in computational engineering, a scalar response function $f:\\mathbb{R}\\to\\mathbb{R}$ is to be approximated near a point $a\\in\\mathbb{R}$ by Taylor polynomials. Assume $f$ is $3$ times continuously differentiable on the closed interval $[a-r,a+r]$ for some radius $r>0$. You are given only the following derivative bounds, valid for all $x$ in this interval:\n- $|f''(x)| \\le M_2$,\n- $|f'''(x)| \\le M_3$,\nwhere $M_{2}>0$ and $M_{3}>0$ are known constants.\n\nLet $T_{1}$ and $T_{2}$ denote, respectively, the first-order and second-order Taylor polynomials of $f$ about $a$. Define, for each radius $r>0$, the quantity $E_{k}(r)$ to be the smallest number that can be certified, using only the given bounds, to uniformly bound the absolute truncation error of $T_{k}$ over the closed neighborhood $\\{x\\in\\mathbb{R} : |x-a| \\le r\\}$, for $k\\in\\{1,2\\}$.\n\nDetermine the smallest radius $r_{c}=r_{c}(M_{2},M_{3})$ such that for every $r>r_{c}$, the a priori worst-case absolute truncation error bound of the second-order polynomial is strictly larger than that of the first-order polynomial; that is, $E_{2}(r)>E_{1}(r)$. Provide your final answer as a closed-form analytic expression in terms of $M_{2}$ and $M_{3}$. No numerical rounding is required.",
            "solution": "The problem requires the determination of a critical radius $r_{c}$ at which the certified error bound of a second-order Taylor approximation surpasses that of a first-order approximation. We will proceed by establishing the expressions for these error bounds.\n\nThe function $f: \\mathbb{R} \\to \\mathbb{R}$ is stated to be $3$ times continuously differentiable on the closed interval $[a-r, a+r]$. The first-order and second-order Taylor polynomials of $f$ expanded about the point $a$, denoted $T_{1}(x)$ and $T_{2}(x)$ respectively, are:\n$$T_{1}(x) = f(a) + f^{\\prime}(a)(x-a)$$\n$$T_{2}(x) = f(a) + f^{\\prime}(a)(x-a) + \\frac{f^{\\prime\\prime}(a)}{2!}(x-a)^{2}$$\n\nThe corresponding truncation errors, or remainders, are given by $R_{1}(x) = f(x) - T_{1}(x)$ and $R_{2}(x) = f(x) - T_{2}(x)$. According to Taylor's theorem with the Lagrange form of the remainder, for any $x$ in the domain of interest, there exist numbers $\\xi_{1}$ and $\\xi_{2}$, both lying strictly between $a$ and $x$, such that:\n$$R_{1}(x) = \\frac{f^{\\prime\\prime}(\\xi_{1})}{2!}(x-a)^{2}$$\n$$R_{2}(x) = \\frac{f^{\\prime\\prime\\prime}(\\xi_{2})}{3!}(x-a)^{3}$$\n\nThe quantity $E_{k}(r)$ is defined as the smallest certified uniform bound on the absolute truncation error $|R_{k}(x)|$ over the neighborhood $\\{x \\in \\mathbb{R} : |x-a| \\leq r\\}$, using only the provided derivative bounds.\n\nFor the first-order approximation ($k=1$), the absolute error is:\n$$|R_{1}(x)| = \\left| \\frac{f^{\\prime\\prime}(\\xi_{1})}{2!}(x-a)^{2} \\right| = \\frac{|f^{\\prime\\prime}(\\xi_{1})|}{2}|x-a|^{2}$$\nTo establish the uniform bound $E_{1}(r)$, we must find the supremum of this expression for all $x$ satisfying $|x-a| \\leq r$. We are given the bound $|f^{\\prime\\prime}(x)| \\leq M_{2}$ for all $x$ in this neighborhood. The term $|x-a|^{2}$ is maximized at the boundary of the neighborhood, where $|x-a| = r$. Thus, the tightest possible uniform bound that can be certified is:\n$$E_{1}(r) = \\sup_{|x-a|\\leq r} \\frac{|f^{\\prime\\prime}(\\xi_{1})|}{2}|x-a|^{2} = \\frac{M_{2}}{2}r^{2}$$\n\nSimilarly, for the second-order approximation ($k=2$), the absolute error is:\n$$|R_{2}(x)| = \\left| \\frac{f^{\\prime\\prime\\prime}(\\xi_{2})}{3!}(x-a)^{3} \\right| = \\frac{|f^{\\prime\\prime\\prime}(\\xi_{2})|}{6}|x-a|^{3}$$\nUsing the given bound $|f^{\\prime\\prime\\prime}(x)| \\leq M_{3}$ and maximizing $|x-a|^{3}$ over the neighborhood (which occurs at $|x-a|=r$), we obtain the certified uniform bound:\n$$E_{2}(r) = \\sup_{|x-a|\\leq r} \\frac{|f^{\\prime\\prime\\prime}(\\xi_{2})|}{6}|x-a|^{3} = \\frac{M_{3}}{6}r^{3}$$\n\nThe problem requires finding the smallest radius $r_{c}$ such that for all $r > r_{c}$, the inequality $E_{2}(r) > E_{1}(r)$ holds. We construct this inequality with the derived expressions:\n$$\\frac{M_{3}}{6}r^{3} > \\frac{M_{2}}{2}r^{2}$$\nThe problem specifies that $r > 0$, and the given constants $M_{2}$ and $M_{3}$ are positive. We can therefore divide both sides of the inequality by the positive quantity $r^{2}$ without altering the direction of the inequality:\n$$\\frac{M_{3}}{6}r > \\frac{M_{2}}{2}$$\nTo isolate $r$, we multiply both sides by the positive constant $\\frac{6}{M_{3}}$:\n$$r > \\frac{M_{2}}{2} \\cdot \\frac{6}{M_{3}}$$\n$$r > \\frac{3M_{2}}{M_{3}}$$\nThis inequality specifies the condition that $r$ must satisfy. The problem asks for the smallest value $r_{c}$ such that for any choice of radius $r$ greater than $r_{c}$, the condition holds. The set of all $r$ values satisfying the derived inequality is the open interval $(\\frac{3M_{2}}{M_{3}}, \\infty)$. The smallest number $r_{c}$ that can serve as a lower bound for all such $r$ is the infimum of this set.\nTherefore, the critical radius is:\n$$r_{c} = \\frac{3M_{2}}{M_{3}}$$",
            "answer": "$$\\boxed{\\frac{3M_{2}}{M_{3}}}$$"
        },
        {
            "introduction": "While theoretical bounds help us understand approximation behavior, their real power comes from guiding the design of efficient algorithms. This practice  focuses on a fundamental engineering task: calculating the number of terms required in a Maclaurin series to guarantee a desired level of accuracy. By applying the Alternating Series Remainder Theorem to the error function, $\\operatorname{erf}(x)$, you will directly link the abstract concept of a remainder to the concrete, practical goal of meeting a numerical tolerance. This skill is essential for writing code that is not only correct but also computationally efficient.",
            "id": "2442184",
            "problem": "In computational engineering, the error function (erf) is defined for real input by\n$$\\operatorname{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} \\exp\\!\\left(-t^{2}\\right)\\, dt.$$\nFor the Maclaurin series of $\\operatorname{erf}(x)$, consider the truncation\n$$S_{N}(x) = \\sum_{n=0}^{N} c_{n}\\, x^{2n+1},$$\nwhere $c_{n}$ are the coefficients obtained from the Maclaurin expansion of $\\operatorname{erf}(x)$ about $x=0$. Determine the smallest integer $N$ such that truncating at $S_{N}(x)$ guarantees that the absolute approximation error for $x=0$ to $x=0.5$ satisfies\n$$\\left|\\operatorname{erf}(0.5) - S_{N}(0.5)\\right| < 1.0 \\times 10^{-8}.$$\nProvide only the integer $N$ as your final answer.",
            "solution": "The solution begins by determining the Maclaurin series for $\\operatorname{erf}(x)$. The process starts with the known Maclaurin series for the exponential function:\n$$ \\exp(u) = \\sum_{k=0}^{\\infty} \\frac{u^{k}}{k!} $$\nBy substituting $u = -t^2$, we obtain the series for the integrand:\n$$ \\exp(-t^2) = \\sum_{k=0}^{\\infty} \\frac{(-t^2)^{k}}{k!} = \\sum_{k=0}^{\\infty} \\frac{(-1)^k t^{2k}}{k!} $$\nThis series has an infinite radius of convergence, which permits term-by-term integration over any finite interval. Integrating from $t=0$ to $t=x$:\n$$ \\int_{0}^{x} \\exp(-t^{2})\\, dt = \\int_{0}^{x} \\left( \\sum_{k=0}^{\\infty} \\frac{(-1)^k t^{2k}}{k!} \\right) dt = \\sum_{k=0}^{\\infty} \\frac{(-1)^k}{k!} \\int_{0}^{x} t^{2k}\\, dt $$\n$$ = \\sum_{k=0}^{\\infty} \\frac{(-1)^k}{k!} \\left[ \\frac{t^{2k+1}}{2k+1} \\right]_{0}^{x} = \\sum_{k=0}^{\\infty} \\frac{(-1)^k x^{2k+1}}{k!(2k+1)} $$\nMultiplying by the constant factor $\\frac{2}{\\sqrt{\\pi}}$ from the definition of $\\operatorname{erf}(x)$ gives its Maclaurin series. To match the problem's notation, we use the index $n$ instead of $k$:\n$$ \\operatorname{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\sum_{n=0}^{\\infty} \\frac{(-1)^n x^{2n+1}}{n!(2n+1)} $$\nThe truncated series is given as $S_N(x) = \\sum_{n=0}^{N} c_n x^{2n+1}$, where the coefficients are $c_n = \\frac{2}{\\sqrt{\\pi}} \\frac{(-1)^n}{n!(2n+1)}$. The approximation error is the remainder term, $R_N(x) = \\operatorname{erf}(x) - S_N(x)$.\n$$ R_N(x) = \\frac{2}{\\sqrt{\\pi}} \\sum_{n=N+1}^{\\infty} \\frac{(-1)^n x^{2n+1}}{n!(2n+1)} $$\nFor any fixed $x > 0$, the Maclaurin series for $\\operatorname{erf}(x)$ is an alternating series. To apply the Alternating Series Remainder Theorem, we must verify that for $x=0.5$, the absolute values of the terms are monotonically decreasing to zero. Let $a_n = \\frac{2}{\\sqrt{\\pi}} \\frac{x^{2n+1}}{n!(2n+1)}$. The ratio of the magnitudes of consecutive terms is:\n$$ \\frac{a_{n+1}}{a_n} = \\frac{x^{2(n+1)+1}}{(n+1)!(2(n+1)+1)} \\cdot \\frac{n!(2n+1)}{x^{2n+1}} = \\frac{x^2 (2n+1)}{(n+1)(2n+3)} $$\nFor $x=0.5$, this ratio becomes:\n$$ \\frac{(0.5)^2 (2n+1)}{(n+1)(2n+3)} = \\frac{0.25(2n+1)}{2n^2+5n+3} $$\nFor all $n \\ge 0$, the denominator $2n^2+5n+3$ is strictly greater than the numerator $0.5n+0.25$. Thus, the ratio is always less than $1$, confirming that the terms are monotonically decreasing in magnitude. The limit of the terms as $n \\to \\infty$ is zero due to the factorial in the denominator.\n\nThe Alternating Series Remainder Theorem states that the absolute error $|R_N(x)|$ is less than the absolute value of the first omitted term, which corresponds to $n=N+1$.\n$$ |\\operatorname{erf}(0.5) - S_N(0.5)| = |R_N(0.5)| \\le \\left| c_{N+1} (0.5)^{2(N+1)+1} \\right| $$\n$$ |R_N(0.5)| \\le \\frac{2}{\\sqrt{\\pi}} \\frac{(0.5)^{2N+3}}{(N+1)!(2N+3)} $$\nWe require this error bound to be less than $1.0 \\times 10^{-8}$:\n$$ \\frac{2}{\\sqrt{\\pi}} \\frac{1}{2^{2N+3}(N+1)!(2N+3)} < 10^{-8} $$\nThis simplifies to:\n$$ \\frac{1}{\\sqrt{\\pi} \\cdot 2^{2N+2}(N+1)!(2N+3)} < 10^{-8} $$\nWe must find the smallest integer $N$ that satisfies this inequality. We test values of $N$.\nFor $N=5$:\nThe error bound is $\\frac{1}{\\sqrt{\\pi} \\cdot 2^{12} \\cdot 6! \\cdot (13)} = \\frac{1}{\\sqrt{\\pi} \\cdot 4096 \\cdot 720 \\cdot 13} = \\frac{1}{\\sqrt{\\pi} \\cdot 38338560}$.\nUsing $\\sqrt{\\pi} \\approx 1.77245$, the denominator is approximately $1.77245 \\times 38338560 \\approx 6.795 \\times 10^7$.\nThe error bound is approximately $\\frac{1}{6.795 \\times 10^7} \\approx 1.47 \\times 10^{-8}$.\nSince $1.47 \\times 10^{-8} > 1.0 \\times 10^{-8}$, $N=5$ is not sufficient.\n\nFor $N=6$:\nThe error bound is $\\frac{1}{\\sqrt{\\pi} \\cdot 2^{14} \\cdot 7! \\cdot (15)} = \\frac{1}{\\sqrt{\\pi} \\cdot 16384 \\cdot 5040 \\cdot 15} = \\frac{1}{\\sqrt{\\pi} \\cdot 1238630400}$.\nUsing $\\sqrt{\\pi} \\approx 1.77245$, the denominator is approximately $1.77245 \\times 1238630400 \\approx 2.196 \\times 10^9$.\nThe error bound is approximately $\\frac{1}{2.196 \\times 10^9} \\approx 4.55 \\times 10^{-10}$.\nSince $4.55 \\times 10^{-10} < 1.0 \\times 10^{-8}$, $N=6$ is sufficient.\n\nTherefore, the smallest integer value for $N$ that guarantees the required accuracy is $6$.",
            "answer": "$$ \\boxed{6} $$"
        },
        {
            "introduction": "A theoretically perfect formula can produce wildly inaccurate results when implemented on a computer due to the limitations of floating-point arithmetic. This comprehensive practice  guides you through building a robust routine for calculating the sine function, moving beyond naive series evaluation to confront the issue of numerical instability head-on. You will implement argument reduction, a powerful strategy for improving accuracy and efficiency, and analyze the phenomenon of catastrophic cancellation that plagues direct series evaluation for large inputs. This exercise synthesizes error analysis, algorithm design, and an awareness of computer arithmetic—a crucial skill set for any computational engineer.",
            "id": "2442233",
            "problem": "You are to write a complete, runnable program that evaluates the sine function using a Maclaurin series with rigorously controlled truncation error, and that analyzes the numerical stability of this approach for large arguments. Work entirely in radians.\n\nStart from the following fundamental base:\n- The definition of the Maclaurin series for analytic functions and the Lagrange form of the remainder. For a sufficiently differentiable function $f$, its Maclaurin polynomial of degree $m$ at $x=0$ has a remainder $R_{m}(x) = \\dfrac{f^{(m+1)}(\\xi)}{(m+1)!} x^{m+1}$ for some $\\xi$ between $0$ and $x$.\n- The periodicity and parity identities for trigonometric functions: $ \\sin(x + 2\\pi k) = \\sin(x)$ for any integer $k$, and the angle-sum and quadrant symmetries that relate $ \\sin(x)$ to $ \\sin(r)$ or $ \\cos(r)$ for a small residual $r$ after reduction by integer multiples of $ \\dfrac{\\pi}{2}$.\n\nYour tasks:\n1) Derive, from the Maclaurin definition and the Lagrange remainder formula, a stopping criterion for truncating the Maclaurin series of $ \\sin(x)$ and $ \\cos(x)$ to achieve a user-prescribed absolute error tolerance $ \\varepsilon > 0$. In particular, justify bounds of the form\n$$\n\\left| R_{2N+1}^{\\sin}(x) \\right| \\le \\frac{|x|^{2N+3}}{(2N+3)!}, \n\\qquad\n\\left| R_{2N}^{\\cos}(x) \\right| \\le \\frac{|x|^{2N+2}}{(2N+2)!},\n$$\nby using that the absolute value of any derivative of $ \\sin(x)$ or $ \\cos(x)$ is at most $1$ on the real line. Then formulate a procedure that, given a small argument $z$, finds the smallest integer $N \\ge 0$ such that the corresponding remainder bound is less than or equal to $ \\varepsilon$.\n\n2) Devise and implement an argument reduction strategy that, given any real $x$, uses integer multiples of $ \\dfrac{\\pi}{2}$ to map $x$ to a residual $r$ in the interval $[-\\dfrac{\\pi}{4}, \\dfrac{\\pi}{4}]$ and a quadrant index $q \\in \\{0,1,2,3\\}$ such that\n$$\n\\sin(x) = \n\\begin{cases}\n\\phantom{-}\\sin(r), & q \\equiv 0 \\ (\\text{mod } 4),\\\\\n\\phantom{-}\\cos(r), & q \\equiv 1 \\ (\\text{mod } 4),\\\\\n-\\sin(r), & q \\equiv 2 \\ (\\text{mod } 4),\\\\\n-\\cos(r), & q \\equiv 3 \\ (\\text{mod } 4).\n\\end{cases}\n$$\nUse this mapping to select which Maclaurin series (sine or cosine) to evaluate at the small residual $r$. Use the stopping criterion from Task $1$ to determine the minimal number of series terms needed to guarantee that the truncation error does not exceed $ \\varepsilon$.\n\n3) Explain, from first principles, why the direct Maclaurin evaluation at a large $|x|$ is numerically unstable: analyze how the magnitude of intermediate terms behaves before factorial growth dominates and how finite-precision arithmetic exacerbates cancellation. Then contrast this with the stability of the reduced-argument strategy, and also explain the limitation that for extremely large $|x|$ the floating-point reduction $x \\mapsto r$ can itself lose accuracy because of limited mantissa bits.\n\n4) Implement the following programmatic outputs for a test suite of inputs. For each test case parameter pair $(x, \\varepsilon)$:\n- Compute $s_{\\text{approx}}$ using your argument-reduced Maclaurin method.\n- Compute a boolean $b_{\\text{ok}}$ that is true if and only if $|s_{\\text{approx}} - \\sin(x)| \\le \\varepsilon$, where $ \\sin(x)$ on the right-hand side is evaluated using a high-quality library function as a reference.\n- Report the minimal number of nonzero Maclaurin terms actually summed for the chosen series, denoted $T$ (for $ \\sin$ this is $T = N+1$ corresponding to degrees $1,3,\\dots,2N+1$, and for $ \\cos$ this is $T = N+1$ corresponding to degrees $0,2,\\dots,2N$).\n- Assess whether a naive, unreduced Maclaurin series for $ \\sin(x)$ is practically unusable under the same tolerance by checking if, before the remainder bound falls below $ \\varepsilon$, one must exceed a cap of $T_{\\max} = 1000$ nonzero terms, or if intermediate remainder terms overflow to non-finite values. If so, record a boolean $b_{\\text{naive\\_impractical}}$ as true, otherwise false.\n\nAngle unit: radians. There are no physical units. All percentages, if any, must be expressed as decimals.\n\nTest suite:\n- Case $1$: $(x, \\varepsilon) = (100.0, 10^{-12})$.\n- Case $2$: $(x, \\varepsilon) = (10^{6} + 0.1, 10^{-12})$.\n- Case $3$: $(x, \\varepsilon) = (10^{16} + 0.1, 10^{-12})$.\n- Case $4$: $(x, \\varepsilon) = \\left(\\dfrac{\\pi}{2} + 10^{-8}, 10^{-15}\\right)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For the $k$-th test case in the order above, output the triple $[b_{\\text{ok}}, T, b_{\\text{naive\\_impractical}}]$ and flatten these triples into a single list in test-case order. For example, the final line should look like\n$[b_{\\text{ok},1}, T_{1}, b_{\\text{naive\\_impractical},1}, b_{\\text{ok},2}, T_{2}, b_{\\text{naive\\_impractical},2}, b_{\\text{ok},3}, T_{3}, b_{\\text{naive\\_impractical},3}, b_{\\text{ok},4}, T_{4}, b_{\\text{naive\\_impractical},4}]$.",
            "solution": "The problem as stated is well-defined, internally consistent, and grounded in the fundamental principles of numerical analysis and calculus. It is neither ambiguous nor scientifically unsound. We shall therefore proceed with a rigorous derivation and implementation as requested.\n\nThe task is to implement a numerically stable method for evaluating the sine function, $\\sin(x)$, using its Maclaurin series expansion, and to analyze its performance against a naive, direct evaluation. The solution is structured into three principal parts: the derivation of a series truncation criterion, the development of an argument reduction strategy, and an analysis of numerical stability.\n\n**1. Derivation of the Truncation Criterion**\n\nThe Maclaurin series for an analytic function $f(x)$ expanded around $x=0$ is given by $f(x) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(0)}{k!} x^k$. When this series is truncated after the term of degree $m$, the result is the Maclaurin polynomial $P_m(x)$, and the error is the remainder term $R_m(x)$. According to the Lagrange form of the remainder, $R_m(x) = \\frac{f^{(m+1)}(\\xi)}{(m+1)!} x^{m+1}$ for some $\\xi$ between $0$ and $x$.\n\nFor $f(x) = \\sin(x)$, the derivatives are cyclical: $\\sin'(x) = \\cos(x)$, $\\sin''(x) = -\\sin(x)$, and so on. Critically, for any integer $k \\ge 0$, the $k$-th derivative satisfies $|f^{(k)}(x)| \\le 1$ for all real $x$. The Maclaurin series for $\\sin(x)$ contains only odd powers of $x$:\n$$\n\\sin(x) = \\sum_{k=0}^{N} \\frac{(-1)^k}{(2k+1)!} x^{2k+1} + R_{2N+1}(x)\n$$\nHere, we have truncated the series after the term corresponding to $k=N$, which has degree $2N+1$. The polynomial part, $P_{2N+1}(x)$, is identical to $P_{2N+2}(x)$ because the coefficient of $x^{2N+2}$ is zero. The remainder is therefore $R_{2N+2}(x)$, given by:\n$$\nR_{2N+2}(x) = \\frac{f^{(2N+3)}(\\xi)}{(2N+3)!} x^{2N+3}\n$$\nwhere $f^{(2N+3)}(x)$ is either $\\pm\\sin(x)$ or $\\pm\\cos(x)$. Using the bound $|f^{(2N+3)}(\\xi)| \\le 1$, we establish the error bound for the sine series truncated at degree $2N+1$:\n$$\n\\left| R_{2N+1}^{\\sin}(x) \\right| = \\left| R_{2N+2}^{\\sin}(x) \\right| \\le \\frac{|x|^{2N+3}}{(2N+3)!}\n$$\nThis confirms the inequality given in the problem statement.\n\nFor $f(x) = \\cos(x)$, the Maclaurin series contains only even powers of $x$:\n$$\n\\cos(x) = \\sum_{k=0}^{N} \\frac{(-1)^k}{(2k)!} x^{2k} + R_{2N}(x)\n$$\nThe polynomial $P_{2N}(x)$ is identical to $P_{2N+1}(x)$. The remainder is $R_{2N+1}(x)$:\n$$\nR_{2N+1}(x) = \\frac{f^{(2N+2)}(\\xi)}{(2N+2)!} x^{2N+2}\n$$\nAgain, using the bound $|f^{(2N+2)}(\\xi)| \\le 1$, we obtain the error bound for the cosine series truncated at degree $2N$:\n$$\n\\left| R_{2N}^{\\cos}(x) \\right| = \\left| R_{2N+1}^{\\cos}(x) \\right| \\le \\frac{|x|^{2N+2}}{(2N+2)!}\n$$\nThis also confirms the problem's formulation.\n\nTo satisfy a prescribed absolute error tolerance $\\varepsilon > 0$ for a small argument $z$, we must find the smallest non-negative integer $N$ such that the error bound does not exceed $\\varepsilon$.\nFor $\\sin(z)$, we must find the minimum $N \\ge 0$ such that $\\frac{|z|^{2N+3}}{(2N+3)!} \\le \\varepsilon$.\nFor $\\cos(z)$, we must find the minimum $N \\ge 0$ such that $\\frac{|z|^{2N+2}}{(2N+2)!} \\le \\varepsilon$.\nThis requires an iterative search, starting from $N=0$ and incrementing $N$ until the condition is met. The number of non-zero terms to be summed is then $T = N+1$.\n\n**2. Argument Reduction Strategy**\n\nDirect evaluation of the Maclaurin series is inefficient and numerically unstable for large $|x|$. A standard and robust technique is argument reduction, which leverages the periodic properties of trigonometric functions. Any real number $x$ can be expressed as $x = q \\cdot \\frac{\\pi}{2} + r$, where $q$ is an integer and $r$ is a small residual. We choose $q$ to be the integer nearest to the value of $x / (\\pi/2)$, which ensures that the residual $r$ lies in the interval $[-\\frac{\\pi}{4}, \\frac{\\pi}{4}]$.\nThe procedure is as follows:\n1. Compute $y = x / (\\pi/2)$.\n2. Find the nearest integer $q = \\text{round}(y)$.\n3. Calculate the residual $r = x - q \\cdot (\\pi/2)$. By construction, $|r| \\le \\frac{1}{2} \\cdot \\frac{\\pi}{2} = \\frac{\\pi}{4}$.\n\nThe value of $\\sin(x)$ is then related to either $\\sin(r)$ or $\\cos(r)$ based on the value of $q$ modulo $4$. Let $q_{\\text{mod} 4} = q \\pmod 4$. We use the angle-sum identities:\n- If $q_{\\text{mod} 4} = 0$: $\\sin(x) = \\sin(4k \\cdot \\frac{\\pi}{2} + r) = \\sin(2k\\pi + r) = \\sin(r)$.\n- If $q_{\\text{mod} 4} = 1$: $\\sin(x) = \\sin((4k+1) \\cdot \\frac{\\pi}{2} + r) = \\sin(2k\\pi + \\frac{\\pi}{2} + r) = \\cos(r)$.\n- If $q_{\\text{mod} 4} = 2$: $\\sin(x) = \\sin((4k+2) \\cdot \\frac{\\pi}{2} + r) = \\sin(2k\\pi + \\pi + r) = -\\sin(r)$.\n- If $q_{\\text{mod} 4} = 3$: $\\sin(x) = \\sin((4k+3) \\cdot \\frac{\\pi}{2} + r) = \\sin(2k\\pi + \\frac{3\\pi}{2} + r) = -\\cos(r)$.\n\nThis strategy reduces the problem of computing $\\sin(x)$ for any $x$ to computing either $\\sin(r)$ or $\\cos(r)$ for a small argument $|r| \\le \\pi/4$. For such small arguments, the Maclaurin series converges rapidly, and the truncation criterion derived in the previous section can be applied efficiently to determine the required number of terms.\n\n**3. Analysis of Numerical Stability**\n\nThe direct evaluation of the Maclaurin series for $\\sin(x)$ with large $|x|$ is numerically unstable due to two primary issues: intermediate term overflow and catastrophic cancellation.\n\n- **Intermediate Term Growth and Overflow**: The $k$-th term of the series for $\\sin(x)$ is $t_k = \\frac{(-1)^k x^{2k+1}}{(2k+1)!}$. The magnitude of these terms initially grows rapidly with $k$. The maximum term magnitude occurs when the ratio $|t_{k+1}/t_k| = \\frac{|x|^2}{(2k+2)(2k+3)} \\approx 1$, which implies $2k+2 \\approx |x|$. For a large value such as $x=100$, the terms grow to an astronomical size (e.g., for $k=49$, the term is on the order of $100^{99}/99!$, which can easily exceed the range of standard double-precision floating-point numbers, $\\approx 10^{308}$).\n\n- **Catastrophic Cancellation**: The final result, $\\sin(x)$, must lie in $[-1, 1]$. The series evaluation for large $x$ involves summing very large positive and negative terms to produce a small final result. Standard floating-point numbers have a fixed number of significant digits (the mantissa). When two large, nearly equal numbers are subtracted, the leading significant digits cancel, leaving a result with far fewer significant digits of precision. This loss of relative accuracy is known as catastrophic cancellation and renders the final result meaningless.\n\nThe reduced-argument strategy circumvents these issues entirely. The argument $r$ is small ($|r| \\le \\pi/4 \\approx 0.785$), so the Maclaurin series terms $\\frac{r^k}{k!}$ decrease monotonically in magnitude from the start. There is no growth of intermediate terms and thus no catastrophic cancellation. The method is numerically stable.\n\nHowever, the argument reduction itself has a limitation. The step $r = x - q \\cdot (\\pi/2)$ can suffer from catastrophic cancellation when $x$ is extremely large. The reason is that standard floating-point representations of $\\pi$ have finite precision. For a very large $x$ (e.g., $x=10^{16}+0.1$), the value of $x$ itself may be subject to rounding in standard `float64` arithmetic (the unit in the last place for $10^{16}$ is greater than $0.1$, so $10^{16}+0.1$ is stored as exactly $10^{16}$). Even if $x$ is representable, the product $q \\cdot (\\pi/2)$ will be a large number close to $x$. The finite precision of the `float64` representation of $\\pi$ introduces an absolute error into this product that scales with $q$. When $x$ is large, $q$ is large, and this error can become significant, potentially larger than $\\pi/2$ itself. The subtraction $x - q \\cdot (\\pi/2)$ then cancels most significant digits, yielding a value for $r$ with few, if any, correct digits. This demonstrates the fundamental limit of computations with fixed-precision arithmetic for extremely large arguments. This phenomenon is expected to cause the test case for $x = 10^{16} + 0.1$ to fail a precision check.\n\nThe provided program implements these principles to compute the sine function and analyze its numerical properties across the specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef get_required_terms_sin(z_abs, ε):\n    \"\"\"\n    Calculates the minimum number of terms T for the sin Maclaurin series.\n    The error bound after T terms (degree 2T-1) is |z|^(2T+1)/(2T+1)!.\n    \"\"\"\n    if z_abs == 0.0:\n        return 1\n    \n    z2 = z_abs * z_abs\n    \n    # Bound for T=1 term (degree 1)\n    bound = (z_abs**3) / 6.0\n    T = 1\n    \n    denom_fac1 = 4\n    denom_fac2 = 5\n    \n    while bound > ε:\n        T += 1\n        bound *= z2 / (denom_fac1 * denom_fac2)\n        denom_fac1 += 2\n        denom_fac2 += 2\n        if T > 1000: # Safety break\n            return T\n            \n    return T\n\ndef get_required_terms_cos(z_abs, ε):\n    \"\"\"\n    Calculates the minimum number of terms T for the cos Maclaurin series.\n    The error bound after T terms (degree 2T-2) is |z|^(2T)/(2T)!.\n    \"\"\"\n    if z_abs == 0.0:\n        return 1\n    \n    z2 = z_abs * z_abs\n    \n    # Bound for T=1 term (degree 0)\n    bound = z2 / 2.0\n    T = 1\n    \n    denom_fac1 = 3\n    denom_fac2 = 4\n\n    while bound > ε:\n        T += 1\n        bound *= z2 / (denom_fac1 * denom_fac2)\n        denom_fac1 += 2\n        denom_fac2 += 2\n        if T > 1000: # Safety break\n            return T\n            \n    return T\n\ndef eval_sin_series(z, T):\n    \"\"\"Evaluates the sin Maclaurin series for T terms.\"\"\"\n    if z == 0.0:\n        return 0.0\n\n    z2 = z * z\n    term = z\n    total = term\n    for k in range(1, T):\n        # term_k = -term_{k-1} * z^2 / ((2k)(2k+1))\n        term *= -z2 / ((2 * k) * (2 * k + 1))\n        total += term\n    return total\n\ndef eval_cos_series(z, T):\n    \"\"\"Evaluates the cos Maclaurin series for T terms.\"\"\"\n    z2 = z * z\n    term = 1.0\n    total = term\n    for k in range(1, T):\n        # term_k = -term_{k-1} * z^2 / ((2k-1)(2k))\n        term *= -z2 / ((2 * k - 1) * (2 * k))\n        total += term\n    return total\n\ndef compute_sin_reduced(x, ε):\n    \"\"\"\n    Computes sin(x) using argument reduction and Maclaurin series.\n    Returns the computed value and the number of terms used.\n    \"\"\"\n    pi_over_2 = np.pi / 2.0\n    \n    # Argument reduction\n    q_float = x / pi_over_2\n    q = np.round(q_float)\n    r = x - q * pi_over_2\n    \n    q_int = int(q)\n    quadrant = q_int % 4\n    \n    r_abs = abs(r)\n\n    if quadrant == 0:  # sin(r)\n        T = get_required_terms_sin(r_abs, ε)\n        val = eval_sin_series(r, T)\n        return val, T\n    elif quadrant == 1:  # cos(r)\n        T = get_required_terms_cos(r_abs, ε)\n        val = eval_cos_series(r, T)\n        return val, T\n    elif quadrant == 2:  # -sin(r)\n        T = get_required_terms_sin(r_abs, ε)\n        val = eval_sin_series(r, T)\n        return -val, T\n    else:  # quadrant == 3, -cos(r)\n        T = get_required_terms_cos(r_abs, ε)\n        val = eval_cos_series(r, T)\n        return -val, T\n\ndef check_naive_impractical(x, ε, T_max):\n    \"\"\"\n    Checks if a naive Maclaurin series evaluation of sin(x) is impractical.\n    Impractical if > T_max terms are needed or if intermediate terms overflow.\n    \"\"\"\n    x_abs = abs(x)\n    if x_abs == 0.0:\n        return False\n        \n    x2 = x_abs * x_abs\n    \n    # Check terms and remainder bound iteratively for T = 1, 2, ...\n    \n    # T=1 term magnitude (|x|)\n    term_mag = x_abs\n    if np.isinf(term_mag):\n        return True # Overflow\n\n    # Remainder bound for T=1 term\n    remainder_bound = term_mag * x2 / 6.0\n    if remainder_bound = ε:\n        return False # Practical\n    \n    for T in range(2, T_max + 1):\n        # Magnitude of the T-th term\n        # term_mag(T) = term_mag(T-1) * x^2 / ((2T-2)*(2T-1))\n        term_mag *= x2 / ((2*T - 2) * (2*T - 1))\n        if np.isinf(term_mag):\n            return True # Overflow of intermediate term\n\n        # Remainder bound for T terms\n        # bound(T) = term_mag(T) * x^2 / ((2T)*(2T+1))\n        remainder_bound = term_mag * x2 / ((2*T) * (2*T + 1))\n        if np.isinf(remainder_bound):\n            # This can happen if term_mag is huge but finite\n            # and gets multiplied by a large x2\n            return True\n            \n        if remainder_bound = ε:\n            return False # Practical, convergence within T_max terms\n\n    return True # Not converged within T_max terms\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (100.0, 1e-12),\n        (10**6 + 0.1, 1e-12),\n        (10**16 + 0.1, 1e-12),\n        (np.pi/2 + 1e-8, 1e-15),\n    ]\n\n    results = []\n    for x, ε in test_cases:\n        # 1. Compute s_approx and T using the reduced method.\n        s_approx, T = compute_sin_reduced(x, ε)\n        \n        # 2. Compute b_ok by comparing with a high-quality reference.\n        # Use np.longdouble for reference calculation where precision matters\n        ref_val = np.sin(np.longdouble(x))\n        b_ok = np.abs(s_approx - ref_val) = ε\n        \n        # 3. Assess if naive method is impractical.\n        T_max = 1000\n        b_naive_impractical = check_naive_impractical(x, ε, T_max)\n        \n        results.extend([b_ok, T, b_naive_impractical])\n\n    # Final print statement in the exact required format.\n    # Python's str() for a boolean is 'True' or 'False'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}