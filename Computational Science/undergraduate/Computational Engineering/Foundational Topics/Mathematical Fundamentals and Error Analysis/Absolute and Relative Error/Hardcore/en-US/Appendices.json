{
    "hands_on_practices": [
        {
            "introduction": "In computational engineering, we frequently replace exact analytical operations, like differentiation, with numerical approximations. This practice invites you to quantify the error introduced by one such common technique, the symmetric difference quotient. By calculating the relative error for a specific function, you will gain a concrete understanding of how to assess the accuracy of fundamental numerical methods .",
            "id": "2152074",
            "problem": "A numerical analyst is studying the accuracy of finite difference approximations for derivatives. The function under consideration is $f(x) = (x+1)\\exp(x)$. The analyst wishes to approximate the derivative of this function at the point $x=1$. For this purpose, the symmetric difference quotient is used, which is defined as:\n$$D_h(x) = \\frac{f(x+h) - f(x-h)}{2h}$$\nUsing a step size of $h=0.01$, calculate the relative error of the approximation $D_h(1)$ with respect to the true value of the derivative $f'(1)$.\n\nRound your final answer to three significant figures.",
            "solution": "We are given $f(x)=(x+1)\\exp(x)$ and the symmetric difference quotient\n$$D_{h}(x)=\\frac{f(x+h)-f(x-h)}{2h}.$$\nFirst, compute the exact derivative using the product rule:\n$$f'(x)=\\frac{d}{dx}\\big[(x+1)\\exp(x)\\big]=\\exp(x)+(x+1)\\exp(x)=(x+2)\\exp(x),$$\nso at $x=1$,\n$$f'(1)=3\\exp(1).$$\n\nNext, evaluate $D_{h}(1)$ symbolically. For $h0$,\n$$f(1+h)=(2+h)\\exp(1+h)=(2+h)\\exp(1)\\exp(h),$$\n$$f(1-h)=(2-h)\\exp(1-h)=(2-h)\\exp(1)\\exp(-h).$$\nHence\n$$D_{h}(1)=\\frac{\\exp(1)\\big[(2+h)\\exp(h)-(2-h)\\exp(-h)\\big]}{2h}.$$\nRewrite the numerator as\n$$(2+h)\\exp(h)-(2-h)\\exp(-h)=2\\big(\\exp(h)-\\exp(-h)\\big)+h\\big(\\exp(h)+\\exp(-h)\\big),$$\nso\n$$D_{h}(1)=\\exp(1)\\left[\\frac{\\exp(h)-\\exp(-h)}{h}+\\frac{\\exp(h)+\\exp(-h)}{2}\\right].$$\nDefine\n$$A(h)=\\frac{\\exp(h)-\\exp(-h)}{h}+\\frac{\\exp(h)+\\exp(-h)}{2},$$\nthen $D_{h}(1)=\\exp(1)\\,A(h)$.\n\nThe relative error of $D_{h}(1)$ with respect to $f'(1)$ is\n$$\\varepsilon_{\\text{rel}}=\\frac{|D_{h}(1)-f'(1)|}{|f'(1)|}=\\frac{|\\exp(1)A(h)-3\\exp(1)|}{3\\exp(1)}=\\frac{|A(h)-3|}{3}.$$\n\nNow set $h=0.01$ and compute numerically:\n$$\\exp(0.01)\\approx 1.010050167084168,\\quad \\exp(-0.01)\\approx 0.9900498337491681.$$\nThus,\n$$\\exp(h)-\\exp(-h)\\approx 0.0200003333350000,\\quad \\exp(h)+\\exp(-h)\\approx 2.0001000008333362,$$\nso\n$$\\frac{\\exp(h)-\\exp(-h)}{h}\\approx \\frac{0.0200003333350000}{0.01}=2.0000333335,$$\n$$\\frac{\\exp(h)+\\exp(-h)}{2}\\approx 1.0000500004166681,$$\nand hence\n$$A(0.01)\\approx 2.0000333335+1.0000500004166681=3.0000833339166681.$$\nTherefore,\n$$\\varepsilon_{\\text{rel}}=\\frac{|A(0.01)-3|}{3}\\approx \\frac{0.0000833339166681}{3}\\approx 2.77779722227\\times 10^{-5}.$$\n\nRounding to three significant figures gives $2.78\\times 10^{-5}$.",
            "answer": "$$\\boxed{2.78 \\times 10^{-5}}$$"
        },
        {
            "introduction": "Moving beyond simple calculation, this exercise explores a profound and sometimes counter-intuitive aspect of numerical analysis. In many problems, we can easily compute the residual (how well our solution satisfies the equation), but not the true error. This problem  uses a classic example of an ill-conditioned root-finding problem to demonstrate that a very small residual does not always guarantee an accurate solution, a critical lesson for interpreting computational results.",
            "id": "2370347",
            "problem": "In a root-finding computation for the scalar nonlinear equation $f(x) = (x - 1)^{10} = 0$, the true root is $x^{\\dagger} = 1$. An iterative solver returns an estimate $x^{*}$ with measured residual magnitude $\\lvert f(x^{*}) \\rvert = 10^{-12}$. Using only the definitions of residual and absolute error, determine the magnitude of the absolute error $\\lvert x^{*} - x^{\\dagger} \\rvert$. Round your answer to four significant figures. Express your answer as a pure number (no units).",
            "solution": "The definition of the absolute error, $e_{abs}$, is the magnitude of the difference between the approximate value $x^{*}$ and the true value $x^{\\dagger}$.\n$$e_{abs} = \\lvert x^{*} - x^{\\dagger} \\rvert$$\nGiven that the true root is $x^{\\dagger} = 1$, the absolute error is:\n$$e_{abs} = \\lvert x^{*} - 1 \\rvert$$\n\nThe residual is the value of the function evaluated at the approximate root, $f(x^{*})$. The problem provides the magnitude of this residual:\n$$\\lvert f(x^{*}) \\rvert = 10^{-12}$$\nThe function is given by $f(x) = (x - 1)^{10}$. Substituting $x^{*}$ into the function gives:\n$$f(x^{*}) = (x^{*} - 1)^{10}$$\nTherefore, the residual magnitude is:\n$$\\lvert (x^{*} - 1)^{10} \\rvert = 10^{-12}$$\nUsing the property of absolute values, $\\lvert a^{n} \\rvert = \\lvert a \\rvert^{n}$ for an even integer exponent $n=10$, we can rewrite the expression as:\n$$\\lvert x^{*} - 1 \\rvert^{10} = 10^{-12}$$\nWe recognize that the term $\\lvert x^{*} - 1 \\rvert$ is precisely the absolute error, $e_{abs}$. Substituting this into the equation yields:\n$$(e_{abs})^{10} = 10^{-12}$$\nTo solve for $e_{abs}$, we take the positive $10$-th root of both sides of the equation:\n$$e_{abs} = \\left( 10^{-12} \\right)^{\\frac{1}{10}}$$\nUsing the rule of exponents $(a^{b})^{c} = a^{bc}$, we simplify the expression:\n$$e_{abs} = 10^{-12 \\times \\frac{1}{10}} = 10^{-\\frac{12}{10}} = 10^{-1.2}$$\nThe problem requires a numerical answer rounded to four significant figures. We calculate the value of $10^{-1.2}$:\n$$e_{abs} \\approx 0.0630957344...$$\nTo round this to four significant figures, we identify the first four significant digits, which are $6$, $3$, $0$, and $9$. The digit following the $9$ is $5$, which requires rounding up the last significant digit. Thus, $0.06309$ rounded up becomes $0.06310$. The trailing zero is significant and must be included to indicate the precision.\n\nTherefore, the magnitude of the absolute error is approximately $0.06310$. This result demonstrates that for a root of high multiplicity, a very small residual (on the order of $10^{-12}$) can correspond to a relatively large error (on the order of $10^{-2}$), a hallmark of an ill-conditioned problem.",
            "answer": "$$\\boxed{0.06310}$$"
        },
        {
            "introduction": "The finite precision of computer arithmetic means that even simple operations can accumulate significant errors. This hands-on coding exercise  demonstrates how the order of operations in a simple summation can dramatically affect the final accuracy due to round-off error. Completing this practice will reveal a fundamental principle for writing robust numerical code: to minimize error, it is often best to sum a sequence of numbers from smallest to largest.",
            "id": "2370321",
            "problem": "Consider the finite harmonic sum defined by the partial series $$S(N)=\\sum_{n=1}^{N}\\frac{1}{n}.$$ Let $S_{\\text{asc}}(N)$ denote the result of evaluating the sum in ascending-index order $n=1,2,\\dots,N$ using double-precision floating-point arithmetic, and let $S_{\\text{desc}}(N)$ denote the result of evaluating the sum in descending-index order $n=N,N-1,\\dots,1$ using the same arithmetic. Define the absolute error of an approximate value $\\widehat{S}(N)$ with respect to the exact mathematical value $S(N)$ as $$E_{\\text{abs}}(\\widehat{S}(N))=\\left|\\widehat{S}(N)-S(N)\\right|.$$ Your task is to compute, for each specified $N$, the pair $$\\big(E_{\\text{abs}}(S_{\\text{asc}}(N)),\\;E_{\\text{abs}}(S_{\\text{desc}}(N))\\big),$$ with all arithmetic performed in Institute of Electrical and Electronics Engineers (IEEE) $754$ binary$64$ (double-precision) floating-point. The exact value $S(N)$ is the finite series value as defined above.\n\nThe test suite of $N$ values is: $N\\in\\{\\,1,\\;10^{4},\\;10^{6},\\;10^{7}\\,\\}$.\n\nThere are no physical units involved. Angles are not used. All outputs must be real numbers. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $$\\big[E_{\\text{abs}}(S_{\\text{asc}}(1)),\\,E_{\\text{abs}}(S_{\\text{desc}}(1)),\\,E_{\\text{abs}}(S_{\\text{asc}}(10^{4})),\\,E_{\\text{abs}}(S_{\\text{desc}}(10^{4})),\\,E_{\\text{abs}}(S_{\\text{asc}}(10^{6})),\\,E_{\\text{abs}}(S_{\\text{desc}}(10^{6})),\\,E_{\\text{abs}}(S_{\\text{asc}}(10^{7})),\\,E_{\\text{abs}}(S_{\\text{desc}}(10^{7}))\\big].$$ The final output must be a single line and must not contain any other characters or text.",
            "solution": "The problem requires an analysis of the accumulation of floating-point error when computing the finite harmonic sum $S(N)=\\sum_{n=1}^{N}\\frac{1}{n}$ for various values of $N$. We are to compare the results of two summation orders: ascending ($n=1, \\dots, N$) and descending ($n=N, \\dots, 1$), both performed using IEEE $754$ binary$64$ (double-precision) arithmetic. The objective is to calculate the absolute error of each computational result with respect to the true mathematical value of the finite sum.\n\nThe problem is valid. It is a well-posed problem in numerical analysis that demonstrates a fundamental principle of floating-point computation: the order of operations matters.\n\nThe core scientific principle at play is the management of round-off error in floating-point addition. An IEEE $754$ double-precision number has a fixed-size mantissa ($52$ bits), which limits its precision to approximately $15$ to $17$ decimal digits. When two floating-point numbers of vastly different magnitudes are added, precision is lost. Consider the addition $x+y$ where $|x| \\gg |y|$. To perform the addition, the exponents must be aligned. This involves shifting the mantissa of the smaller number, $y$, to the right. Bits that are shifted beyond the capacity of the mantissa are discarded. This phenomenon is known as absorption or swamping, and it leads to a loss of information. The result is that the contribution of the smaller number, $y$, is either partially or entirely lost.\n\nLet us analyze the two specified summation methods in light of this principle.\n\n$1$. **Ascending-Order Summation, $S_{\\text{asc}}(N)$**:\nThe summation is performed as $S_{\\text{asc}}(N) = (\\dots((1 + \\frac{1}{2}) + \\frac{1}{3}) + \\dots + \\frac{1}{N})$.\nThe terms $1/n$ are added in decreasing order of magnitude. The partial sum grows rapidly at the beginning. For large $n$, the term $1/n$ becomes very small compared to the accumulated partial sum. For instance, the harmonic series grows logarithmically, i.e., $S(N) \\approx \\ln(N) + \\gamma$, where $\\gamma$ is the Eulerâ€“Mascheroni constant. For $N=10^7$, $S(10^7) \\approx \\ln(10^7) \\approx 16.1$. The last term added is $1/10^7 = 10^{-7}$. Adding $10^{-7}$ to a number of magnitude $\\approx 16.1$ will result in a significant loss of precision. The accumulated sum has a large magnitude, and repeatedly adding small values to it leads to a systematic accumulation of absorption errors. We therefore predict that $S_{\\text{asc}}(N)$ will be the less accurate method.\n\n$2$. **Descending-Order Summation, $S_{\\text{desc}}(N)$**:\nThe summation is performed as $S_{\\text{desc}}(N) = (\\dots((\\frac{1}{N} + \\frac{1}{N-1}) + \\frac{1}{N-2}) + \\dots + 1)$.\nHere, the terms are added in increasing order of magnitude. The summation begins with the smallest terms. The partial sum grows slowly. At each step, the two numbers being added (the current partial sum and the next term) are more likely to be of comparable magnitude. This minimizes the right-shifting of the mantissa and thus reduces the loss of precision at each step. This technique is a general-purpose heuristic for improving the accuracy of summing a series of positive, monotonically decreasing terms. We predict that $S_{\\text{desc}}(N)$ will be significantly more accurate than $S_{\\text{asc}}(N)$.\n\n$3$. **Reference Value, $S(N)$**:\nTo compute the absolute error $E_{\\text{abs}}(\\widehat{S}(N))=\\left|\\widehat{S}(N)-S(N)\\right|$, we require a highly accurate representation of the true mathematical sum $S(N)$. Since our computations $\\widehat{S}(N)$ are performed in double-precision (approximately $16$ decimal digits of precision), our reference value must have substantially higher precision to be considered \"exact\" for this purpose. Using a precision of, for example, $50$ decimal digits is more than sufficient. We will employ a high-precision arithmetic library for this task. The Python `decimal` module provides the necessary functionality.\n\n$4$. **Algorithm**:\nFor each value of $N$ in the test set $\\{1, 10^4, 10^6, 10^7\\}$, the following steps will be executed:\n- For $N=1$, the case is trivial. $S(1) = 1$, $S_{\\text{asc}}(1) = 1.0$, and $S_{\\text{desc}}(1) = 1.0$. The number $1.0$ is exactly representable, so both errors will be exactly $0$.\n- For $N  1$:\n    - **Step 4.1: Compute Reference Sum.** We will set the precision of our arbitrary-precision arithmetic tool (Python's `decimal` module) to $50$ digits. We then compute $S(N) = \\sum_{n=1}^{N} \\frac{1}{n}$ using this high precision. The result will be stored as a `Decimal` object.\n    - **Step 4.2: Compute Ascending Sum.** A `numpy.float64` variable, `sum_asc`, is initialized to $0.0$. We iterate $n$ from $1$ to $N$ and accumulate the sum `sum_asc = sum_asc + 1.0/n`. The terms are explicitly cast to `float64` to ensure the arithmetic is performed at the correct precision.\n    - **Step 4.3: Compute Descending Sum.** A `numpy.float64` variable, `sum_desc`, is initialized to $0.0$. We iterate $n$ from $N$ down to $1$ and accumulate the sum `sum_desc = sum_desc + 1.0/n`.\n    - **Step 4.4: Calculate Absolute Errors.** The absolute errors are computed as:\n        $$E_{\\text{abs}}(S_{\\text{asc}}(N)) = \\left| \\text{Decimal}(\\text{sum\\_asc}) - S(N) \\right|$$\n        $$E_{\\text{abs}}(S_{\\text{desc}}(N)) = \\left| \\text{Decimal}(\\text{sum\\_desc}) - S(N) \\right|$$\n        By converting the `float64` results to `Decimal` objects before subtraction, we perform the subtraction at high precision, avoiding any loss of accuracy in the reference value itself. The final result is the absolute value of this difference.\n\nThis procedure will be implemented to generate the eight required error values, ordered as specified in the problem statement. The results are expected to show that for large $N$, the error of the ascending sum is markedly larger than the error of the descending sum, confirming our theoretical analysis.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [1, 10**4, 10**6, 10**7]\n\n    results = []\n\n    # Set precision for the high-accuracy reference calculation.\n    # 50 digits is sufficient to be an \"exact\" reference for double-precision (approx. 16 digits).\n    getcontext().prec = 50\n\n    for N in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        \n        # For N=1, the sums are trivially exact (1.0), so errors are 0.\n        if N == 1:\n            results.extend([0.0, 0.0])\n            continue\n\n        # 1. Compute the reference value S(N) using high-precision arithmetic.\n        # This serves as the \"exact\" value for error calculation.\n        s_exact = Decimal(0)\n        one_decimal = Decimal(1)\n        # Summation order for the exact value does not matter here due to high precision.\n        for i in range(1, N + 1):\n            s_exact += one_decimal / Decimal(i)\n\n        # 2. Compute S_asc(N) summing in ascending order (1 to N).\n        # This is expected to be less accurate due to adding small numbers to a large accumulator.\n        s_asc = np.float64(0.0)\n        for i in range(1, N + 1):\n            # Using np.float64 ensures IEEE 754 double-precision arithmetic.\n            s_asc += np.float64(1.0) / np.float64(i)\n\n        # 3. Compute S_desc(N) summing in descending order (N to 1).\n        # This is expected to be more accurate as it sums numbers of similar magnitude first.\n        s_desc = np.float64(0.0)\n        for i in range(N, 0, -1):\n            s_desc += np.float64(1.0) / np.float64(i)\n\n        # 4. Calculate absolute errors against the high-precision reference.\n        # The float64 results are converted to Decimal to perform the subtraction\n        # at high precision, preserving the accuracy of the reference value.\n        error_asc = abs(Decimal(s_asc) - s_exact)\n        error_desc = abs(Decimal(s_desc) - s_exact)\n        \n        # Append results as standard Python floats.\n        results.extend([float(error_asc), float(error_desc)])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}