{
    "hands_on_practices": [
        {
            "introduction": "方差是统计学中的一个基本量度，但在计算上却隐藏着一个常见的陷阱。一个看似高效的“单遍”算法，虽然在代数上是完全正确的，但在实际的浮点数运算中可能会因为“灾难性抵消”而导致完全错误的结果。这个练习将让你直面这个问题。\n\n通过实现并对比一个数值上不稳定的单遍公式和一个更稳健的两遍公式，你将亲眼见证在处理均值很大而偏差很小的数据集时，数值误差如何戏剧性地破坏计算的准确性 。这个实践将加深你对浮点数算术局限性的理解。",
            "id": "2447454",
            "problem": "您需要实现一个完整的、可运行的程序，以演示在计算具有大均值和小偏差的数据集的方差时出现的截断误差和舍入误差，并比较两种计算公式。其理论基础是方差的定义，即实值随机变量的二阶中心矩，以及标准的浮点舍入模型。请使用以下事实作为起点：\n- 对于具有有限二阶矩的实值随机变量 $X$，其方差由二阶中心矩定义：$\\operatorname{Var}(X) = \\mathbb{E}\\big[(X - \\mu)^2\\big]$，其中 $\\mu = \\mathbb{E}[X]$。\n- 对于实数，二阶原点矩满足 $\\mathbb{E}[X^2] = \\operatorname{Var}(X) + \\mu^2$。\n- 在电气和电子工程师协会 (IEEE) binary64 格式（常称为双精度）中的浮点运算，近似遵循舍入模型 $ \\operatorname{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b)(1 + \\delta)$，其中 $|\\delta| \\le \\epsilon_{\\text{mach}}$，$\\epsilon_{\\text{mach}}$ 是机器ε，$\\circ$ 是一种算术运算。对两个几乎相等的数进行相减会导致灾难性抵消，从而丢失有效数字。\n\n您的程序必须：\n- 构建指定的数据集，其元素具有大均值和小偏差。\n- 使用 IEEE binary64 算术通过两种方法计算方差：\n  1. 原点矩单遍形式：计算 $\\mathbb{E}[X]$ 和 $\\mathbb{E}[X^2]$，然后构成 $\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$。\n  2. 中心化双遍形式：首先计算 $\\mu = \\mathbb{E}[X]$，然后在第二遍中计算 $\\mathbb{E}[(X-\\mu)^2]$。\n- 使用具有足够高精度的十进制算术计算一个高精度参考方差，使得运算中的舍入误差与 IEEE binary64 相比可以忽略不计。在高精度计算中使用中心矩定义 $\\mathbb{E}[(X-\\mu)^2]$。\n- 量化每种浮点方法相对于高精度参考的绝对误差。\n- 为测试套件中的每个数据集，按顺序生成单行输出，其中包含一个含有五个值的列表：单遍法方差、双遍法方差、高精度参考方差、单遍法结果的绝对误差和双遍法结果的绝对误差。\n\n所有数据集均为纯数字；此问题不涉及物理单位，也不涉及角度。\n\n用于覆盖正常路径、边界重点和灾难性抵消边缘情况的测试套件：\n- 测试 1（大均值周围的对称小偏差，使用整数以避免输入量化）：设 $M = 10^{8}$ 且 $D = \\{-3,-1,0,1,3\\}$。数据集为 $X = \\{ M + d \\mid d \\in D\\}$。对于精确实数，真实方差等于偏差平方的平均值，即 $4$。\n- 测试 2（非对称小偏差，偏差均值非零）：设 $M = 10^{8}$ 且 $D = \\{0,1,2,3,4\\}$。数据集为 $X = \\{ M + d \\mid d \\in D\\}$。对于精确实数，真实方差等于 $2$。\n- 测试 3（样本量更大，具有微小、平滑变化的偏差）：设 $M = 10^{8}$ 且 $D = \\left\\{ \\frac{k-500}{1000} \\;\\middle|\\; k=0,1,\\dots,999 \\right\\}$。数据集为 $X = \\{ M + d \\mid d \\in D\\}$。对于精确实数，真实方差是此算术网格的总体方差；它接近于 $1/12$ 减去微小偏差均值的平方，并且必须由您的高精度程序精确计算。\n- 测试 4（相对于均值，偏差极小，接近分辨率极限）：设 $M = 10^{8}$ 且 $D = \\{10^{-8},-10^{-8}\\}$。数据集为 $X = \\{ M + d \\mid d \\in D\\}$。对于精确实数，真实方差等于 $10^{-16}$。\n\n高精度参考要求：\n- 使用至少 $p = 100$ 位精度的十进制算术构建参考。使用上述指定的 $M$ 和 $D$ 的精确十进制值构建数据集（例如，精确使用 $M = 100000000$ 以及诸如 $(k-500)/1000$ 的有理数偏差作为精确小数）。使用双遍中心矩定义 $\\mathbb{E}[(X-\\mu)^2]$ 计算总体方差，其中 $\\mathbb{E}[\\cdot]$ 是有限集上的算术平均值。\n\n浮点计算要求：\n- 通过数值库中的标准数组使用 IEEE binary64（双精度）来计算单遍原点矩和双遍中心矩的总体方差。不要应用任何补偿求和或数值稳定技巧；以直接的方式使用均值和求和，以便截断和舍入误差清晰可见。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个类似 Python 列表的结构，内含四个子列表，按测试 1 到 4 的顺序对应每个测试用例。每个子列表必须包含五个浮点数：$[\\text{var\\_one\\_pass}, \\text{var\\_two\\_pass}, \\text{var\\_ref}, \\text{abs\\_err\\_one}, \\text{abs\\_err\\_two}]$。例如，一个语法上有效的输出行看起来像 $[[v_{11},v_{12},v_{13},e_{11},e_{12}],[v_{21},v_{22},v_{23},e_{21},e_{22}],\\dots]$，其中的数值由您的程序填充。\n\n不得有用户输入和外部文件。程序必须完全确定测试数据，执行计算，并打印所需的单行输出。输出必须是浮点数。",
            "solution": "用户提出了一个计算工程问题，要求分析方差计算中的数值稳定性。该问题有效、适定且有科学依据。它探讨了数值方法中的一个基本问题：浮点运算中因灾难性抵消而导致的精度损失。\n\n核心任务是比较计算数据集 $X = \\{x_1, x_2, \\dots, x_N\\}$ 的总体方差的两种公式：\n\n1.  **单遍（或原点矩）公式：** 该方法源自代数恒等式 $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$。在计算上，它涉及单次遍历数据以计算值的总和和平方和，然后从中计算均值。公式为：\n    $$ \\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N} x_i^2 - \\left(\\frac{1}{N}\\sum_{i=1}^{N} x_i\\right)^2 $$\n    虽然该公式对于实数是数学上精确的，但当标准差 $\\sigma$ 与均值 $\\mu = \\mathbb{E}[X]$ 相比很小时，它在数值上是不稳定的。$\\mathbb{E}[X^2]$ 和 $(\\mathbb{E}[X])^2$ 这两项会变得非常接近。具体来说，$\\mathbb{E}[X^2] = \\sigma^2 + \\mu^2$，因此该公式相当于计算 $\\sigma^2 = (\\sigma^2 + \\mu^2) - \\mu^2$。当使用有限精度的浮点运算（如 IEEE binary64）进行求值时，这涉及两个非常大且几乎相等的数的相减。此操作是灾难性抵消的经典例子。两个数的首部数字相互抵消，导致微小差值的大部分甚至全部有效数字丢失。计算 $\\mathbb{E}[X^2]$ 和 $(\\mathbb{E}[X])^2$ 时的舍入误差（量级约为 $\\mu^2 \\epsilon_{\\text{mach}}$）成为最终结果的主导部分，可能产生一个高度不准确甚至为负的方差。\n\n2.  **双遍（或中心矩）公式：** 该方法更紧密地遵循方差的定义，即与均值的偏差平方的平均值。它需要对数据进行两次遍历。\n    $$ \\mu = \\frac{1}{N}\\sum_{i=1}^{N} x_i $$\n    $$ \\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N} (x_i - \\mu)^2 $$\n    在第一遍中，计算均值 $\\mu$。在第二遍中，使用这个计算出的均值来找到平方偏差 $(x_i - \\mu)^2$，然后对其求平均值。这种方法在数值上要稳健得多。虽然仍然执行减法 $x_i - \\mu$，但结果是一组小数（偏差）。对这些小数进行平方和求和不涉及大数相减。主要误差来源是 $\\mu$ 的初始计算。计算出的均值 $\\hat{\\mu}$ 中的误差会传播到偏差中。然而，这种误差通常很小。对于具有大均值 $M$ 和小偏差的数据，计算出的均值误差量级约为 $M\\epsilon_{\\text{mach}}$。只要这个误差相对于真实偏差的大小很小，双遍算法就能得出准确的结果。\n\n对于这个问题，还需要进行高精度参考计算。这将使用 Python 的 `decimal` 模块，以 $p=100$ 位的精度执行。在此精度水平下，与标准 binary64 浮点运算中的舍入误差相比，此处的舍入误差可以忽略不计，从而提供了一个“基准真相”，用以比较其他两种方法。\n\n程序将被构造成处理四个特定的测试用例。每个用例都使用一个具有大均值（$M=10^8$）和小偏差的数据集，旨在暴露单遍公式的数值缺陷。\n\n-   **测试 1 和 2：** 偏差为小整数。预计双遍法将非常准确。预计单遍法会因灾难性抵消而失败。\n-   **测试 3：** 一个具有平滑变化的、小的有理数偏差的更大数据集。预计行为类似。\n-   **测试 4：** 偏差极小（$d = \\pm 10^{-8}$），相对于均值，接近 binary64 的分辨率极限。对于 $x = M+d$，值 $d$ 小于量级为 $M$ 的数的最小可能增量（即 $M \\cdot \\epsilon_{\\text{mach}} \\approx 10^8 \\cdot 2.22 \\times 10^{-16} = 2.22 \\times 10^{-8}$）。因此，$fl(M+d)$ 将被舍入为 $M$ 本身。这展示了另一种误差来源：初始数据表示中的信息丢失，这将导致两种浮点方法计算出的方差都为 $0$。\n\n实现过程将首先定义一个函数，该函数接收一个数据集，使用三种方法（单遍浮点法、双遍浮点法和高精度参考法）计算方差，计算浮点方法的绝对误差，并返回结果。将为每个测试用例调用此函数，并将收集到的结果格式化为指定的输出字符串。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Computes and compares variance using three different methods to demonstrate\n    truncation and round-off errors.\n    \"\"\"\n    # Set the precision for decimal arithmetic to 100 digits, as required.\n    getcontext().prec = 100\n\n    def analyze_dataset(M_str: str, D_list: list):\n        \"\"\"\n        Performs the full analysis for a given dataset definition.\n\n        Args:\n            M_str: The large mean component as a string for exact Decimal conversion.\n            D_list: A list of deviation values (as Decimal objects).\n\n        Returns:\n            A list containing the five required values:\n            [var_one_pass, var_two_pass, var_ref, abs_err_one, abs_err_two]\n        \"\"\"\n        # 1. High-precision reference calculation using the decimal module.\n        # This serves as the ground truth.\n        M_dec = Decimal(M_str)\n        X_dec = [M_dec + d for d in D_list]\n        N_dec = Decimal(len(X_dec))\n        \n        # Use two-pass formula for the reference calculation.\n        mu_dec = sum(X_dec) / N_dec\n        var_ref = sum([(x - mu_dec)**2 for x in X_dec]) / N_dec\n\n        # 2. Floating-point calculations using numpy (IEEE binary64).\n        # Construct the dataset using standard float64.\n        # Note: float() conversion from Decimal can introduce small errors,\n        # but the dominant error source is the variance algorithm itself.\n        X_fp = np.array([float(x) for x in X_dec], dtype=np.float64)\n        \n        # Method 1: One-pass (raw-moment) formula. Prone to catastrophic cancellation.\n        # sigma^2 = E[X^2] - (E[X])^2\n        mean_of_squares = np.mean(X_fp**2)\n        square_of_mean = np.mean(X_fp)**2\n        var_one_pass = mean_of_squares - square_of_mean\n        \n        # Method 2: Two-pass (centered-moment) formula. More numerically stable.\n        # sigma^2 = E[(X - E[X])^2]\n        mu_fp = np.mean(X_fp)\n        var_two_pass = np.mean((X_fp - mu_fp)**2)\n        \n        # 3. Quantify absolute errors.\n        abs_err_one = abs(var_one_pass - float(var_ref))\n        abs_err_two = abs(var_two_pass - float(var_ref))\n        \n        return [var_one_pass, var_two_pass, float(var_ref), abs_err_one, abs_err_two]\n\n    # --- Define and run all test cases ---\n    test_cases_defs = [\n        # Test 1: Symmetric small integer deviations. True Var = 4.\n        {'M': '1e8', 'D': [Decimal(s) for s in ['-3', '-1', '0', '1', '3']]},\n        \n        # Test 2: Non-symmetric small integer deviations. True Var = 2.\n        {'M': '1e8', 'D': [Decimal(s) for s in ['0', '1', '2', '3', '4']]},\n        \n        # Test 3: Larger sample with small rational deviations.\n        {'M': '1e8', 'D': [(Decimal(k) - Decimal(500)) / Decimal(1000) for k in range(1000)]},\n        \n        # Test 4: Extremely small deviations at the limit of float64 resolution. True Var = 1e-16.\n        {'M': '1e8', 'D': [Decimal('1e-8'), Decimal('-1e-8')]}\n    ]\n\n    all_results = []\n    for case in test_cases_defs:\n        result = analyze_dataset(case['M'], case['D'])\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, e.g., [[v1,v2,...],[v1,v2,...]]\n    formatted_inner_lists = []\n    for res_list in all_results:\n        # Format each inner list as \"[v1,v2,v3,e1,e2]\"\n        formatted_list_str = f\"[{','.join(map(str, res_list))}]\"\n        formatted_inner_lists.append(formatted_list_str)\n    \n    # Join the inner lists into the final output format \"[ [...], [...], ... ]\"\n    final_output = f\"[{','.join(formatted_inner_lists)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "在科学与工程计算中，我们常常需要用数值方法来近似求解函数的导数。最简单的前向差分法引入了两种相互制约的误差：截断误差和舍入误差。步长 $h$ 的选择是关键：太大会导致截断误差过大，太小又会放大舍入误差。\n\n这个练习将引导你探索这个微妙的平衡 。你将首先从理论上推导出总误差模型，并依此计算出能使误差最小化的最优步长 $h^\\star$。随后，你将通过编程进行经验性搜索来验证你的理论预测，从而直观地理解数值计算中理论分析与经验验证相结合的重要性。",
            "id": "2447368",
            "problem": "实现一个程序，研究指数函数前向差分数值微分中的截断误差和舍入误差。考虑函数 $f(x) = \\exp(x)$ 在点 $x = 1$ 处的导数。使用前向差分近似 $f'(x) \\approx \\dfrac{f(x+h) - f(x)}{h}$。您的任务是：从第一性原理推导出一个误差模型，用它来获得理论上的最优步长 $h^\\star$，然后通过在浮点运算中扫描一系列步长 $h$ 来凭经验验证这一预测。\n\n您的推导只能基于以下广为接受的原理：\n- 一个足够光滑的函数在点 $x$ 附近的泰勒展开：当 $h \\to 0$ 时，$f(x+h) = f(x) + f'(x)h + \\dfrac{1}{2} f''(x) h^2 + \\mathcal{O}(h^3)$。\n- 电气与电子工程师协会（IEEE）754 算术中浮点就近舍入的标准模型：对于任何基本运算和数 $y$，$\\operatorname{fl}(y) = y(1+\\delta)$，其中 $|\\delta| \\le u$，$u$ 是单位舍入误差。对于给定的机器ε $\\varepsilon$（定义为1与下一个可表示数之间的间隙），$u = \\varepsilon/2$。\n\n要执行的任务：\n1. 根据上述原理，推导一个在前向差分近似 $x=1$ 处的主阶绝对误差模型，该模型结合了截断误差和舍入误差，用 $h$、$f(1)$、$f''(1)$ 和机器ε $\\varepsilon$ 表示。然后，从此模型中，获得使主阶误差最小化的理论最优步长 $h^\\star$。对于 $f(x) = \\exp(x)$ 在 $x=1$ 处的情况，用 $\\varepsilon$ 明确表示 $h^\\star$。所有量均为无量纲。\n2. 实现一个程序，该程序：\n   - 使用与指定浮点数据类型相关联的机器ε $\\varepsilon$ 计算理论上的最优步长 $h^\\star$。\n   - 通过在指定区间内扫描一组对数间隔的 $h$ 值，并选择使绝对误差 $|D(h) - f'(1)|$ 最小的 $h$，来经验性地估计最优步长 $h_{\\mathrm{emp}}$。其中 $D(h) = \\dfrac{f(1+h) - f(1)}{h}$ 是在目标浮点数据类型中计算的。参考值 $f'(1)$ 必须以足够高的精度进行评估，以避免对经验性评估产生偏差。所有量均为无量纲。\n   - 针对每个测试用例，报告一个包含三个浮点数的列表：理论最优步长 $h^\\star$、经验最优步长 $h_{\\mathrm{emp}}$ 以及在扫描中达到的最小绝对误差。\n3. 使用以下测试套件，该套件通过改变浮点精度和扫描区间来探测不同的误差区域。对于每个用例，在指定区间上扫描 $200$ 个对数间隔的 $h$ 值：\n   - 测试用例 A（正常路径，双精度）：数据类型 $\\texttt{float64}$，$h \\in [10^{-16}, 10^{-1}]$。\n   - 测试用例 B（单精度）：数据类型 $\\texttt{float32}$，$h \\in [10^{-10}, 10^{0}]$。\n   - 测试用例 C（边界，双精度中的舍入误差主导窗口）：数据类型 $\\texttt{float64}$，$h \\in [10^{-20}, 10^{-14}]$。\n4. 最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素本身就是一个三元素列表，顺序为 $[h^\\star, h_{\\mathrm{emp}}, \\min\\_h |D(h) - f'(1)|]$，对应一个测试用例。例如，结构必须严格遵循 $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$ 的形式。所有量均为无量纲实数。\n\n不需要外部输入，所有量均为无量纲。不涉及角度，输出中也没有百分比。",
            "solution": "在尝试任何解决方案之前，需对问题陈述进行验证。\n\n步骤1：提取已知条件\n- 需要微分的函数：$f(x) = \\exp(x)$。\n- 微分点：$x = 1$。\n- 数值微分公式：前向差分近似，$f'(x) \\approx \\dfrac{f(x+h) - f(x)}{h}$。\n- 原理1（泰勒展开）：当 $h \\to 0$ 时，$f(x+h) = f(x) + f'(x)h + \\dfrac{1}{2} f''(x) h^2 + \\mathcal{O}(h^3)$。\n- 原理2（浮点误差模型）：对于一个数 $y$，其浮点表示为 $\\operatorname{fl}(y) = y(1+\\delta)$，其中 $|\\delta| \\le u$，$u = \\varepsilon/2$ 是单位舍入误差，$\\varepsilon$ 是机器ε。\n- 任务1：推导在 $x=1$ 处近似的主阶绝对误差模型，该模型结合了截断误差和舍入误差。从此模型中，推导出对于 $f(x)=\\exp(x)$ 在 $x=1$ 处，用 $\\varepsilon$ 表示的理论最优步长 $h^\\star$。\n- 任务2：实现一个程序，计算理论上的 $h^\\star$，并通过在扫描的一系列 $h$ 值上最小化绝对误差来找到经验最优步长 $h_{\\mathrm{emp}}$。\n- 任务3：程序必须针对三个测试用例运行：\n    - 用例A：`float64` 精度，$h \\in [10^{-16}, 10^{-1}]$。\n    - 用例B：`float32` 精度，$h \\in [10^{-10}, 10^{0}]$。\n    - 用例C：`float64` 精度，$h \\in [10^{-20}, 10^{-14}]$。\n    - 对于所有用例，扫描必须使用200个对数间隔点。\n- 任务4：最终输出必须是表示列表的列表的单个字符串，其中每个内部列表为 $[h^\\star, h_{\\mathrm{emp}}, \\text{min_error}]$。所有量均为无量纲。\n\n步骤2：使用提取的已知条件进行验证\n- **科学依据充分：** 该问题牢固地定位于数值分析和计算工程的标准体系内。它探讨了有限差分方法中截断误差和舍入误差之间的基本权衡。所给函数、原理和误差模型都是标准的，并且在事实上是正确的。\n- **适定性良好：** 该问题定义明确。它指定了函数、近似方法、求值点、推导原理以及计算和报告的精确任务。可以推导出 $h^\\star$ 的唯一解析结果，并且明确指定了经验搜索方法。\n- **客观性：** 该问题以精确、客观的语言陈述，没有歧义或主观看法。\n\n步骤3：结论与行动\n问题陈述有效。这是一个计算科学中的标准、适定性良好的问题。我将继续进行推导和实现。\n\n**误差模型与最优步长的推导**\n\n目标是分析在计算 $f(x) = \\exp(x)$ 在 $x=1$ 处的导数的前向差分近似值 $\\hat{D}(h)$ 时产生的总误差。总误差是两个分量的和：截断误差，源于用有限差分近似导数；以及舍入误差，源于浮点运算的有限精度。\n\n1.  **截断误差分析**\n    精确的前向差分商为 $D(h) = \\dfrac{f(x+h) - f(x)}{h}$。我们使用 $f(x+h)$ 在 $x$ 点的泰勒展开：\n    $$f(x+h) = f(x) + h f'(x) + \\frac{h^2}{2} f''(x) + \\mathcal{O}(h^3)$$\n    将此代入 $D(h)$ 的表达式中：\n    $$D(h) = \\frac{\\left( f(x) + h f'(x) + \\frac{h^2}{2} f''(x) + \\mathcal{O}(h^3) \\right) - f(x)}{h} = f'(x) + \\frac{h}{2}f''(x) + \\mathcal{O}(h^2)$$\n    截断误差 $E_{\\text{trunc}}(h)$ 是近似值与真实导数之间的差：\n    $$E_{\\text{trunc}}(h) = D(h) - f'(x) = \\frac{h}{2}f''(x) + \\mathcal{O}(h^2)$$\n    因此，主阶绝对截断误差为 $|\\frac{h}{2}f''(x)|$。\n\n2.  **舍入误差分析**\n    在浮点运算中，我们计算 $\\hat{D}(h) = \\operatorname{fl}\\left(\\dfrac{\\operatorname{fl}(f(x+h)) - \\operatorname{fl}(f(x))}{h}\\right)$。让我们先分析分子中的误差。设 $y_1 = f(x)$ 和 $y_2 = f(x+h)$。它们的计算值为：\n    $$\\hat{y}_1 = \\operatorname{fl}(y_1) = y_1(1+\\delta_1)$$\n    $$\\hat{y}_2 = \\operatorname{fl}(y_2) = y_2(1+\\delta_2)$$\n    其中 $|\\delta_1|, |\\delta_2| \\le u$，$u = \\varepsilon/2$ 是单位舍入误差。\n    减法也受舍入影响：\n    $$\\operatorname{fl}(\\hat{y}_2 - \\hat{y}_1) = (\\hat{y}_2 - \\hat{y}_1)(1+\\delta_3) = (y_2(1+\\delta_2) - y_1(1+\\delta_1))(1+\\delta_3)$$\n    展开并仅保留 $\\delta_i$ 的一阶项：\n    $$\\operatorname{fl}(\\hat{y}_2 - \\hat{y}_1) \\approx (y_2 - y_1) + y_2\\delta_2 - y_1\\delta_1$$\n    分子中的舍入误差约为 $y_2\\delta_2 - y_1\\delta_1$。当 $h \\to 0$时，$y_2 = f(x+h) \\approx f(x) = y_1$。因此，计算出的分子中的误差有界：\n    $$|y_2\\delta_2 - y_1\\delta_1| \\le |y_2||\\delta_2| + |y_1||\\delta_1| \\approx |f(x)|u + |f(x)|u = 2u|f(x)| = \\varepsilon|f(x)|$$\n    然后将此误差除以 $h$。最终结果中的舍入误差 $E_{\\text{round}}(h)$ 主要由分子中的误差决定。因此，主阶绝对舍入误差为：\n    $$|E_{\\text{round}}(h)| \\approx \\frac{\\varepsilon |f(x)|}{h}$$\n\n3.  **总误差与最优步长**\n    总绝对误差 $\\mathcal{E}(h)$ 是主阶截断误差和舍入误差的量级之和：\n    $$\\mathcal{E}(h) \\approx |E_{\\text{trunc}}(h)| + |E_{\\text{round}}(h)| = \\frac{h}{2}|f''(x)| + \\frac{\\varepsilon |f(x)|}{h}$$\n    为了找到使总误差最小化的步长 $h^\\star$，我们对 $\\mathcal{E}(h)$ 关于 $h$ 求导，并令其结果为零：\n    $$\\frac{d\\mathcal{E}}{dh} = \\frac{1}{2}|f''(x)| - \\frac{\\varepsilon |f(x)|}{h^2} = 0$$\n    解出 $h^2$：\n    $$h^2 = \\frac{2\\varepsilon |f(x)|}{|f''(x)|}$$\n    这给出了最优步长：\n    $$h^\\star = \\sqrt{\\frac{2\\varepsilon |f(x)|}{|f''(x)|}}$$\n\n4.  **应用于 $f(x) = \\exp(x)$ 在 $x=1$ 的情况**\n    对于给定函数 $f(x) = e^x$，我们有 $f'(x) = e^x$ 和 $f''(x) = e^x$。在点 $x=1$ 处：\n    $$f(1) = e^1 = e$$\n    $$f''(1) = e^1 = e$$\n    由于 $e  0$，可以去掉绝对值符号。将这些代入 $h^\\star$ 的公式中：\n    $$h^\\star = \\sqrt{\\frac{2\\varepsilon \\cdot e}{e}} = \\sqrt{2\\varepsilon}$$\n    这就是 $e^x$ 在 $x=1$ 处导数的前向差分近似的理论最优步长。它仅取决于所用浮点运算的机器ε $\\varepsilon$。推导到此完成。接下来的实现将着手找到这个 $h^\\star$，并将其与经验确定的值进行比较。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Investigates truncation and round-off error in the forward-difference\n    numerical differentiation of f(x) = exp(x) at x = 1.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is (data_type_string, h_min, h_max).\n    test_cases = [\n        ('float64', 1e-16, 1e-1),\n        ('float32', 1e-10, 1e0),\n        ('float64', 1e-20, 1e-14),\n    ]\n\n    # Use a high-precision value for the true derivative f'(1) = e.\n    # np.longdouble provides higher precision than float64, preventing a bias\n    # in the error calculation.\n    true_derivative = np.exp(np.longdouble(1))\n\n    results = []\n    \n    for dtype_str, h_min, h_max in test_cases:\n        # 1. Set up the environment for the current test case.\n        if dtype_str == 'float64':\n            dtype = np.float64\n        elif dtype_str == 'float32':\n            dtype = np.float32\n        else:\n            raise ValueError(f\"Unsupported data type: {dtype_str}\")\n\n        # Get machine epsilon for the current data type.\n        eps = np.finfo(dtype).eps\n\n        # 2. Calculate the theoretical optimal step size h_star.\n        # As derived, h_star = sqrt(2 * epsilon).\n        h_star = np.sqrt(2 * eps)\n\n        # 3. Perform the empirical scan to find the optimal h.\n        \n        # Define the point of differentiation and the function f(x) = e^x,\n        # ensuring calculations use the specified data type.\n        x_val = dtype(1.0)\n        f = lambda val: np.exp(val, dtype=dtype)\n        \n        # Generate 200 logarithmically spaced values for h in the given interval.\n        # These values are cast to the target data type.\n        h_values = np.logspace(np.log10(h_min), np.log10(h_max), 200, dtype=dtype)\n        \n        min_abs_error = np.inf\n        h_emp = np.nan\n        \n        for h in h_values:\n            # Ensure h is not zero, which can happen with very small logspace ends.\n            if h == 0:\n                continue\n\n            # Calculate the forward-difference approximation D(h).\n            # The calculation is performed entirely in the target precision.\n            D_h = (f(x_val + h) - f(x_val)) / h\n            \n            # Calculate the absolute error. The subtraction is done with D_h promoted\n            # to the higher precision of true_derivative.\n            abs_error = np.abs(D_h - true_derivative)\n            \n            # Update the minimum error and corresponding h.\n            if abs_error  min_abs_error:\n                min_abs_error = abs_error\n                h_emp = h\n        \n        # Cast the minimum error back to a standard Python float for consistent output.\n        min_abs_error_float = float(min_abs_error)\n        \n        # 4. Store the results for this test case.\n        results.append([h_star, h_emp, min_abs_error_float])\n\n    # Final print statement in the exact required format.\n    # The output should look like [[a1,b1,c1],[a2,b2,c2],[a3,b3,c3]].\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "前面的练习揭示了在对大量数值进行求和时，微小的舍入误差会如何累积并最终导致精度的严重损失。既然问题已经明确，我们该如何设计更精确的算法来应对挑战呢？Kahan求和算法提供了一个优雅而强大的解决方案。\n\n在本练习中，你将实现这一经典的补偿求和算法，并将其与朴素的求和方法在几个精心设计的、极易产生巨大舍入误差的序列上进行比较 。这个实践不仅能让你掌握一种具体的数值技巧，更重要的是，它展示了一种通过巧妙地追踪并补偿误差来编写高精度数值代码的通用思想。",
            "id": "2447409",
            "problem": "你必须编写一个完整、可运行的程序，用于评估对实数序列求和时的舍入误差，并演示如何使用Kahan求和算法来减少误差。所有计算都必须在标准的双精度二进制浮点运算中执行。对于每个测试用例，计算同一序列的两个和：一个是朴素的从左到右浮点数求和，另一个是使用Kahan求和算法的补偿求和。对于每个和，计算其相对于高精度参考和的绝对误差。然后，你的程序必须输出一行，其中包含所有测试用例的所有绝对误差，并遵循指定的顺序和格式。\n\n对于一个计算和$\\hat{S}$，其相对于参考值$S^{\\star}$的绝对误差定义为 $E = \\lvert \\hat{S} - S^{\\star} \\rvert$。\n\n测试套件包含以下四个序列：\n\n- 测试用例 1（将许多微小增量添加到一个大基数上）：\n  - 序列 $S_1$ 的长度为 $N_1 + 1$，其中 $N_1 = 10^{6}$。第一项是 $s^{(1)}_0 = 1$，其余 $N_1$ 项为 $s^{(1)}_k = 10^{-16}$（$1 \\le k \\le N_1$）。\n\n- 测试用例 2（重复的灾难性抵消三元组）：\n  - 设 $M = 2 \\cdot 10^{5}$。序列 $S_2$ 是由 $M$ 个三项块 $(1, 10^{-16}, -1)$ 串联而成。\n\n- 测试用例 3（具有轻微偏差的确定性伪随机小数值）：\n  - 设模数 $m = 2^{64}$，乘数 $a = 6364136223846793005$，增量 $c = 1442695040888963407$，种子 $x_0 = 123456789123456789$。通过 $x_{k+1} \\equiv a x_k + c \\pmod{m}$（$k \\ge 0$）定义一个线性同余生成器。设 $N_3 = 5 \\cdot 10^{4}$。对于 $k = 1, 2, \\dots, N_3$，定义 $u_k = \\frac{x_k}{m} - \\frac{1}{2}$ 和序列项 $s^{(3)}_k = 10^{-12} u_k + 10^{-16}$。序列 $S_3$ 由这 $N_3$ 项组成。\n\n- 测试用例 4（短序列中的动态范围和抵消）：\n  - 序列 $S_4$ 有五项：$(10^{16}, 1, -10^{16}, 3, 4 \\cdot 10^{-16})$。\n\n对于每个测试用例 $i \\in \\{1,2,3,4\\}$，计算：\n- 朴素和 $\\hat{S}^{\\text{naive}}_i$，通过在双精度浮点运算中从左到右累加得到。\n- Kahan补偿和 $\\hat{S}^{\\text{Kahan}}_i$，使用双精度浮点运算的Kahan求和算法得到。\n- 高精度参考和 $S^{\\star}_i$，根据序列的数学定义，在可能的情况下使用精确算术计算，或使用至少有50位正确小数位的十进制任意精度算术计算，以确保双精度下的舍入不会污染 $S^{\\star}_i$。\n\n对于每个测试用例 $i$，计算绝对误差 $E^{\\text{naive}}_i = \\lvert \\hat{S}^{\\text{naive}}_i - S^{\\star}_i \\rvert$ 和 $E^{\\text{Kahan}}_i = \\lvert \\hat{S}^{\\text{Kahan}}_i - S^{\\star}_i \\rvert$。\n\n最终输出格式：\n- 生成单行输出，包含一个由方括号括起来的逗号分隔列表。该列表必须按以下顺序包含8个数字：\n  - $E^{\\text{naive}}_1, E^{\\text{Kahan}}_1, E^{\\text{naive}}_2, E^{\\text{Kahan}}_2, E^{\\text{naive}}_3, E^{\\text{Kahan}}_3, E^{\\text{naive}}_4, E^{\\text{Kahan}}_4$。\n- 每个数字必须四舍五入到12位有效数字，并以十进制表示（科学记数法可接受）。\n- 所需单行格式示例（仅为说明）：$[e_1,e_2,e_3,e_4,e_5,e_6,e_7,e_8]$。\n\n此问题不涉及任何物理单位或角度单位。程序必须是自包含的，且不应需要任何用户输入或外部文件。在任何遵循标准双精度浮点语义的现代编程语言中，结果必须能够根据上述定义精确地复现。",
            "solution": "问题陈述已经过分析，并被确定为有效。它在科学上基于数值分析的原理，特别是关于浮点运算和舍入误差。该问题是适定的，所有必要的数据和定义都已提供，以计算出唯一、可验证的解。它是客观的，没有歧义。\n\n这个问题的核心是展示和量化在对数量级差异巨大的浮点数求和过程中发生的精度损失，以及如何使用补偿求和算法来减轻这种误差。\n\n所有计算都使用标准双精度浮点运算进行，这对应于IEEE 754 64位格式。此格式大约有15到17位十进制精度。对于此格式，机器ε（epsilon），即满足 $1.0 + \\epsilon  1.0$ 的最小数，约为 $2.22 \\times 10^{-16}$。当两个数量级差异巨大的数相加时，较小的数可能会部分或完全丢失。这种现象被称为淹没。\n\n第一种求和方法是朴素的从左到右累加。对于一个序列 $s_0, s_1, \\dots, s_N$，朴素和 $\\hat{S}^{\\text{naive}}$ 计算为 $(\\dots((s_0 + s_1) + s_2) + \\dots + s_N)$。这种方法对舍入误差高度敏感。\n\n第二种方法是Kahan求和算法，这是一种补偿求和方法。它能显著减少对一系列有限精度浮点数求和时所产生的数值误差。该算法维护一个动态的补偿变量 $c$，用于累积本应丢失的误差。对于序列中的每一项 $s_k$，更新规则如下：\n$$y_k = s_k - c_{k-1}$$\n$$t_k = \\text{sum}_{k-1} + y_k$$\n$$c_k = (t_k - \\text{sum}_{k-1}) - y_k$$\n$$\\text{sum}_k = t_k$$\n这里，$\\text{sum}_0 = 0$ 且 $c_0 = 0$。项 $(t_k - \\text{sum}_{k-1})$ 恢复了 $y_k$ 的高位部分，再从此结果中减去 $y_k$ 则分离出低位部分（即舍入误差），该误差存储在 $c_k$ 中，并从下一项 $s_{k+1}$ 中减去。\n\n绝对误差定义为 $E = \\lvert \\hat{S} - S^{\\star} \\rvert$，其中 $\\hat{S}$ 是计算出的和，$S^{\\star}$ 是高精度参考和。\n\n测试用例分析：\n\n测试用例 1：\n序列为 $s^{(1)}_0 = 1$，后跟 $N_1 = 10^6$ 个项 $s^{(1)}_k = 10^{-16}$（$k \\ge 1$）。\n精确和为 $S^{\\star}_1 = 1 + 10^6 \\times 10^{-16} = 1 + 10^{-10}$。\n在朴素求和中，我们计算 $1 + 10^{-16} + 10^{-16} + \\dots$。相对于 $1.0$，$10^{-16}$ 这个项非常接近机器ε。在双精度运算中，$1.0 + 10^{-16}$ 的操作会遭受淹没效应；结果很可能会被舍入回 $1.0$。因此，大部分小数项将会丢失，$\\hat{S}^{\\text{naive}}_1$ 预计会非常接近 $1.0$，导致误差接近 $10^{-10}$。\nKahan算法将在每一步中捕获丢失的部分 $10^{-16}$ 到补偿变量 $c$ 中，并重新引入计算，从而得到一个非常接近 $S^{\\star}_1$ 的结果 $\\hat{S}^{\\text{Kahan}}_1$。误差 $E^{\\text{Kahan}}_1$ 应该接近机器精度级别。\n\n测试用例 2：\n序列由 $M = 2 \\cdot 10^5$ 个块 $(1, 10^{-16}, -1)$ 组成。\n一个块的精确和是 $1 + 10^{-16} - 1 = 10^{-16}$。总的精确和是 $S^{\\star}_2 = 2 \\cdot 10^5 \\times 10^{-16} = 2 \\cdot 10^{-11}$。\n朴素求和将计算 $(1 + 10^{-16}) - 1$。与第一种情况一样，$1 + 10^{-16}$ 很可能会舍入为 $1.0$，因此 $(1 + 10^{-16}) - 1$ 的计算结果为 $0$。对所有块重复此过程，$\\hat{S}^{\\text{naive}}_2$ 预计为 $0.0$，导致误差 $E^{\\text{naive}}_2$ 恰好为 $2 \\cdot 10^{-11}$。\nKahan算法将防止这种抵消误差，产生一个非常接近 $S^{\\star}_2$ 的和 $\\hat{S}^{\\text{Kahan}}_2$，以及一个更小的误差 $E^{\\text{Kahan}}_2$。\n\n测试用例 3：\n序列由 $N_3 = 5 \\cdot 10^4$ 个项 $s^{(3)}_k = 10^{-12} u_k + 10^{-16}$ 组成，其中 $u_k = \\frac{x_k}{m} - \\frac{1}{2}$，$x_k$ 来自一个线性同余生成器（LCG）。$u_k$ 的值是 $[-0.5, 0.5)$ 范围内的伪随机数。项 $s^{(3)}_k$ 很小，并带有 $10^{-16}$ 的微小正偏置。\n精确和为 $S^{\\star}_3 = \\sum_{k=1}^{N_3} (10^{-12} u_k + 10^{-16}) = 10^{-12} \\sum_{k=1}^{N_3} u_k + N_3 \\cdot 10^{-16}$。\n这个和必须使用高精度算术来计算，以作为参考值 $S^{\\star}_3$。LCG状态 $x_{k+1} \\equiv a x_k + c \\pmod{m}$ 是使用64位整数算术计算的。$\\sum x_k$ 是使用任意精度整数计算的，而 $S^{\\star}_3$ 的最终表达式是使用高精度十进制算术计算的。\n朴素求和将在 $5 \\cdot 10^4$ 次加法中累积小的舍入误差。Kahan算法预计将最小化这种累积，导致 $E^{\\text{Kahan}}_3 \\ll E^{\\text{naive}}_3$。\n\n测试用例 4：\n序列为 $(10^{16}, 1, -10^{16}, 3, 4 \\cdot 10^{-16})$。\n精确和为 $S^{\\star}_4 = (10^{16} - 10^{16}) + (1 + 3) + 4 \\cdot 10^{-16} = 4 + 4 \\cdot 10^{-16}$。\n朴素的从左到右求和分步计算如下：\n1. $10^{16} + 1 = 10^{16}$（淹没，因为 $1$ 小于 $10^{16}$ 的最低有效位的值）。\n2. $10^{16} - 10^{16} = 0$。\n3. $0 + 3 = 3$。\n4. $3 + 4 \\cdot 10^{-16} = 3$（淹没，因为 $4 \\cdot 10^{-16}$ 小于相对于 $3$ 的机器ε）。\n所以，$\\hat{S}^{\\text{naive}}_4 = 3$。误差为 $E^{\\text{naive}}_4 = \\lvert 3 - (4 + 4 \\cdot 10^{-16}) \\rvert \\approx 1$。\nKahan求和算法正是为处理这种情况而设计的。第一步中损失的 $1$ 将被补偿变量捕获。最终的和 $\\hat{S}^{\\text{Kahan}}_4$ 应该非常接近真实和 $S^{\\star}_4$，从而产生一个非常小的误差 $E^{\\text{Kahan}}_4$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport decimal\n\ndef solve():\n    \"\"\"\n    Computes and prints round-off errors for naive and Kahan summations\n    for four specific test cases, adhering to the problem specification.\n    \"\"\"\n\n    def naive_sum(sequence):\n        \"\"\"Computes the naive left-to-right sum of a sequence.\"\"\"\n        s = 0.0\n        for x in sequence:\n            s += x\n        return s\n\n    def kahan_sum(sequence):\n        \"\"\"Computes the sum of a sequence using Kahan's algorithm.\"\"\"\n        s = 0.0\n        c = 0.0\n        for x in sequence:\n            y = x - c\n            t = s + y\n            c = (t - s) - y\n            s = t\n        return s\n\n    def generate_test_cases():\n        \"\"\"Generates the sequences for all four test cases.\"\"\"\n        # Test Case 1: Many tiny increments\n        N1 = 10**6\n        seq1 = np.full(N1, 1e-16, dtype=np.float64)\n        seq1 = np.insert(seq1, 0, 1.0)\n        \n        # Test Case 2: Repeated catastrophic cancellation\n        M = 2 * 10**5\n        block = np.array([1.0, 1e-16, -1.0], dtype=np.float64)\n        seq2 = np.tile(block, M)\n        \n        # Test Case 3: LCG-based sequence\n        m = 2**64\n        a = 6364136223846793005\n        c = 1442695040888963407\n        x0 = 123456789123456789\n        N3 = 5 * 10**4\n        \n        seq3 = np.zeros(N3, dtype=np.float64)\n        x_current = x0\n        for k in range(N3):\n            x_current = (a * x_current + c) % m\n            u_k = x_current / m - 0.5\n            seq3[k] = 1e-12 * u_k + 1e-16\n\n        # Test Case 4: Dynamic range and cancellation\n        seq4 = np.array([1e16, 1.0, -1e16, 3.0, 4e-16], dtype=np.float64)\n        \n        return [seq1, seq2, seq3, seq4]\n\n    def get_reference_sums():\n        \"\"\"Computes high-accuracy reference sums for all test cases.\"\"\"\n        # Set precision for Decimal calculations\n        decimal.getcontext().prec = 100\n\n        # Reference Sum 1\n        N1 = 10**6\n        s_star_1 = decimal.Decimal(1) + decimal.Decimal(N1) * decimal.Decimal('1e-16')\n\n        # Reference Sum 2\n        M = 2 * 10**5\n        s_star_2 = decimal.Decimal(M) * decimal.Decimal('1e-16')\n\n        # Reference Sum 3\n        m = 2**64\n        a = 6364136223846793005\n        c = 1442695040888963407\n        x0 = 123456789123456789\n        N3 = 5 * 10**4\n        \n        sum_x = 0\n        x_current = x0\n        for _ in range(N3):\n            x_current = (a * x_current + c) % m\n            sum_x += x_current\n        \n        D_sum_x = decimal.Decimal(sum_x)\n        D_m = decimal.Decimal(m)\n        D_N3 = decimal.Decimal(N3)\n        D_1e_12 = decimal.Decimal('1e-12')\n        D_1e_16 = decimal.Decimal('1e-16')\n        D_half = decimal.Decimal('0.5')\n        \n        sum_u = D_sum_x / D_m - D_N3 * D_half\n        s_star_3 = D_1e_12 * sum_u + D_N3 * D_1e_16\n\n        # Reference Sum 4\n        s_star_4 = decimal.Decimal('4') + decimal.Decimal('4e-16')\n        \n        return [float(s_star_1), float(s_star_2), float(s_star_3), float(s_star_4)]\n\n    sequences = generate_test_cases()\n    reference_sums = get_reference_sums()\n    \n    results = []\n    \n    for i in range(4):\n        seq = sequences[i]\n        s_star = reference_sums[i]\n        \n        # Naive sum and its error\n        s_naive = naive_sum(seq)\n        e_naive = abs(s_naive - s_star)\n        \n        # Kahan sum and its error\n        s_kahan = kahan_sum(seq)\n        e_kahan = abs(s_kahan - s_star)\n        \n        results.extend([e_naive, e_kahan])\n\n    # Format output to 12 significant digits and print\n    formatted_results = [f\"{res:.12g}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}