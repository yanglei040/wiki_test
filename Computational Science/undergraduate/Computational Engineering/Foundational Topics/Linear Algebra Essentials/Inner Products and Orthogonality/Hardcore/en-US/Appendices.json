{
    "hands_on_practices": [
        {
            "introduction": "The concept of orthogonality is central to linear algebra and its applications, signifying that two vectors are in a sense perpendicular. In vector spaces involving complex numbers, as often seen in fields like signal processing and quantum mechanics, this geometric intuition is captured by the inner product equaling zero. This exercise  provides fundamental practice in applying the definition of orthogonality to determine a condition that makes two complex vectors orthogonal.",
            "id": "1374314",
            "problem": "Consider a three-level quantum system whose state space is spanned by a set of three orthonormal basis vectors, denoted as $\\{|e_1\\rangle, |e_2\\rangle, |e_3\\rangle\\}$. The orthonormality condition is given by the inner product $\\langle e_i | e_j \\rangle = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta.\n\nTwo states of this system, $|\\psi\\rangle$ and $|\\phi(\\alpha)\\rangle$, are defined as follows:\n$$ |\\psi\\rangle = i|e_1\\rangle + |e_2\\rangle + |e_3\\rangle $$\n$$ |\\phi(\\alpha)\\rangle = 3|e_1\\rangle + i\\alpha|e_2\\rangle + 5i|e_3\\rangle $$\nHere, $i$ is the imaginary unit ($i^2 = -1$) and $\\alpha$ is an unknown real-valued parameter.\n\nDetermine the specific numerical value of $\\alpha$ that makes the state $|\\psi\\rangle$ orthogonal to the state $|\\phi(\\alpha)\\rangle$.",
            "solution": "We seek the value of the real parameter $\\alpha$ such that the states $|\\psi\\rangle$ and $|\\phi(\\alpha)\\rangle$ are orthogonal. Orthogonality requires the inner product to vanish:\n$$\n\\langle \\psi | \\phi(\\alpha) \\rangle = 0.\n$$\nGiven the orthonormal basis $\\{|e_{1}\\rangle, |e_{2}\\rangle, |e_{3}\\rangle\\}$ with $\\langle e_{i} | e_{j} \\rangle = \\delta_{ij}$, and the states\n$$\n|\\psi\\rangle = i|e_{1}\\rangle + |e_{2}\\rangle + |e_{3}\\rangle, \\quad |\\phi(\\alpha)\\rangle = 3|e_{1}\\rangle + i\\alpha|e_{2}\\rangle + 5i|e_{3}\\rangle,\n$$\nthe bra corresponding to $|\\psi\\rangle$ is obtained by conjugating the coefficients:\n$$\n\\langle \\psi | = (-i)\\langle e_{1}| + \\langle e_{2}| + \\langle e_{3}|.\n$$\nCompute the inner product using linearity and orthonormality:\n$$\n\\langle \\psi | \\phi(\\alpha) \\rangle = (-i)\\cdot 3 \\langle e_{1}|e_{1}\\rangle + 1 \\cdot i\\alpha \\langle e_{2}|e_{2}\\rangle + 1 \\cdot 5i \\langle e_{3}|e_{3}\\rangle.\n$$\nSince $\\langle e_{j}|e_{j}\\rangle = 1$ and cross terms vanish, this simplifies to\n$$\n\\langle \\psi | \\phi(\\alpha) \\rangle = -3i + i\\alpha + 5i = i(\\alpha + 2).\n$$\nOrthogonality requires\n$$\ni(\\alpha + 2) = 0 \\quad \\Rightarrow \\quad \\alpha + 2 = 0 \\quad \\Rightarrow \\quad \\alpha = -2.\n$$",
            "answer": "$$\\boxed{-2}$$"
        },
        {
            "introduction": "Beyond simply checking for orthogonality, a more powerful application is using it to decompose vectors into meaningful components through projection. In computational engineering, this allows us to determine how a general vector, such as a force, aligns with a specific subspace, like one defined by a structure's mode shapes. This problem  walks you through the essential process of calculating an orthogonal projection, a technique fundamental to approximation theory and data analysis.",
            "id": "2403749",
            "problem": "In a computational structural model with $4$ degrees of freedom (degrees of freedom (DOF)), consider the standard Euclidean space $\\mathbb{R}^{4}$ equipped with the standard Euclidean inner product $\\langle x,y\\rangle = x^{\\mathsf{T}} y$. Let the force vector be $v = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4\\end{pmatrix}$ and consider the two mode shapes $u_{1} = \\begin{pmatrix}1 \\\\ 1 \\\\ 0 \\\\ 0\\end{pmatrix}$ and $u_{2} = \\begin{pmatrix}0 \\\\ 1 \\\\ 1 \\\\ 0\\end{pmatrix}$. Determine the orthogonal projection of $v$ onto the subspace $S \\subset \\mathbb{R}^{4}$ spanned by $u_{1}$ and $u_{2}$ with respect to the standard Euclidean inner product.\n\nProvide your answer as the projected vector written as a column vector with exact rational entries. No rounding is required.",
            "solution": "The objective is to find the orthogonal projection of the vector $v$ onto the subspace $S = \\text{span}\\{u_1, u_2\\}$. Let this projection be denoted by $p$. By definition, the projection $p$ is the unique vector in $S$ such that the error vector, $e = v - p$, is orthogonal to the subspace $S$. This orthogonality condition is satisfied if $e$ is orthogonal to every vector in a basis for $S$. Therefore, we must have:\n$$\n\\langle v - p, u_1 \\rangle = 0\n$$\n$$\n\\langle v - p, u_2 \\rangle = 0\n$$\nSince $p$ is in $S$, it can be expressed as a linear combination of the basis vectors $u_1$ and $u_2$:\n$$\np = c_1 u_1 + c_2 u_2\n$$\nfor some scalar coefficients $c_1$ and $c_2$. Substituting this expression for $p$ into the orthogonality conditions gives:\n$$\n\\langle v - (c_1 u_1 + c_2 u_2), u_1 \\rangle = \\langle v, u_1 \\rangle - c_1 \\langle u_1, u_1 \\rangle - c_2 \\langle u_2, u_1 \\rangle = 0\n$$\n$$\n\\langle v - (c_1 u_1 + c_2 u_2), u_2 \\rangle = \\langle v, u_2 \\rangle - c_1 \\langle u_1, u_2 \\rangle - c_2 \\langle u_2, u_2 \\rangle = 0\n$$\nThis forms a system of linear equations for the coefficients $c_1$ and $c_2$, known as the normal equations:\n$$\n\\begin{pmatrix} \\langle u_1, u_1 \\rangle  \\langle u_1, u_2 \\rangle \\\\ \\langle u_2, u_1 \\rangle  \\langle u_2, u_2 \\rangle \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = \\begin{pmatrix} \\langle v, u_1 \\rangle \\\\ \\langle v, u_2 \\rangle \\end{pmatrix}\n$$\nWe now compute the necessary inner products using the given vectors:\n$v = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4\\end{pmatrix}$, $u_{1} = \\begin{pmatrix}1 \\\\ 1 \\\\ 0 \\\\ 0\\end{pmatrix}$, $u_{2} = \\begin{pmatrix}0 \\\\ 1 \\\\ 1 \\\\ 0\\end{pmatrix}$.\n\nThe inner products for the Gram matrix on the left side are:\n$$\n\\langle u_1, u_1 \\rangle = 1^2 + 1^2 + 0^2 + 0^2 = 2\n$$\n$$\n\\langle u_2, u_2 \\rangle = 0^2 + 1^2 + 1^2 + 0^2 = 2\n$$\n$$\n\\langle u_1, u_2 \\rangle = \\langle u_2, u_1 \\rangle = (1)(0) + (1)(1) + (0)(1) + (0)(0) = 1\n$$\nThe inner products for the vector on the right side are:\n$$\n\\langle v, u_1 \\rangle = (1)(1) + (2)(1) + (3)(0) + (4)(0) = 3\n$$\n$$\n\\langle v, u_2 \\rangle = (1)(0) + (2)(1) + (3)(1) + (4)(0) = 5\n$$\nSubstituting these values, the normal equations become:\n$$\n\\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 5 \\end{pmatrix}\n$$\nWe solve this system for $c_1$ and $c_2$. The determinant of the $2 \\times 2$ matrix is $(2)(2) - (1)(1) = 3$. The inverse of the matrix is:\n$$\n\\frac{1}{3} \\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix}\n$$\nSolving for the coefficients:\n$$\n\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 5 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} (2)(3) + (-1)(5) \\\\ (-1)(3) + (2)(5) \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 6 - 5 \\\\ -3 + 10 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 1 \\\\ 7 \\end{pmatrix}\n$$\nThus, the coefficients are $c_1 = \\frac{1}{3}$ and $c_2 = \\frac{7}{3}$.\n\nFinally, the orthogonal projection $p$ is computed by substituting these coefficients back into the expression for $p$:\n$$\np = c_1 u_1 + c_2 u_2 = \\frac{1}{3} u_1 + \\frac{7}{3} u_2\n$$\n$$\np = \\frac{1}{3} \\begin{pmatrix}1 \\\\ 1 \\\\ 0 \\\\ 0\\end{pmatrix} + \\frac{7}{3} \\begin{pmatrix}0 \\\\ 1 \\\\ 1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \\\\ \\frac{1}{3} \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ \\frac{7}{3} \\\\ \\frac{7}{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \\\\ \\frac{1}{3} + \\frac{7}{3} \\\\ \\frac{7}{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \\\\ \\frac{8}{3} \\\\ \\frac{7}{3} \\\\ 0 \\end{pmatrix}\n$$\nTo verify the correctness of the result, one can check that the error vector $e = v - p$ is orthogonal to both $u_1$ and $u_2$.\n$$\ne = v - p = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4\\end{pmatrix} - \\begin{pmatrix} \\frac{1}{3} \\\\ \\frac{8}{3} \\\\ \\frac{7}{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{2}{3} \\\\ \\frac{2}{3} \\\\ 4 \\end{pmatrix}\n$$\n$$\n\\langle e, u_1 \\rangle = (\\frac{2}{3})(1) + (-\\frac{2}{3})(1) + (\\frac{2}{3})(0) + (4)(0) = \\frac{2}{3} - \\frac{2}{3} = 0\n$$\n$$\n\\langle e, u_2 \\rangle = (\\frac{2}{3})(0) + (-\\frac{2}{3})(1) + (\\frac{2}{3})(1) + (4)(0) = -\\frac{2}{3} + \\frac{2}{3} = 0\n$$\nThe orthogonality conditions are satisfied. The result is correct.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{3} \\\\ \\frac{8}{3} \\\\ \\frac{7}{3} \\\\ 0 \\end{pmatrix}}$$"
        },
        {
            "introduction": "While existing orthogonal vectors are useful, we often need to construct our own orthonormal set from a given basis. The Gram-Schmidt process is a cornerstone algorithm for this task, systematically converting a set of linearly independent vectors into an orthonormal basis. This procedure is the heart of QR factorization, a vital tool in numerical linear algebra for solving linear systems and eigenvalue problems, which you will practice in this exercise .",
            "id": "2179862",
            "problem": "Consider the real vector space $\\mathbb{R}^3$ equipped with the standard Euclidean inner product, which is the dot product. Let $A$ be a $3 \\times 2$ matrix whose columns are the vectors $a_1$ and $a_2$, where $a_1$ is a vector with components $(1, 2, 2)$ and $a_2$ is a vector with components $(1, 0, -1)$.\n\nThis matrix $A$ can be uniquely decomposed into a product $A = QR$, where $Q$ is a $3 \\times 2$ matrix with orthonormal columns, and $R$ is a $2 \\times 2$ upper triangular matrix with positive diagonal entries. This decomposition is known as the QR factorization.\n\nFind the matrix $R$. The final answer should be presented as a $2 \\times 2$ matrix.",
            "solution": "We perform the thin QR factorization of the $3 \\times 2$ matrix $A=[a_1 \\ a_2]$ with $a_1=(1,2,2)^T$ and $a_2=(1,0,-1)^T$ using Gramâ€“Schmidt, where $Q=[q_1 \\ q_2]$ has orthonormal columns, and $R$ is a $2 \\times 2$ upper triangular matrix with positive diagonal entries.\n\nFirst, set\n$$\nr_{11}=\\|a_1\\|=\\sqrt{1^2+2^2+2^2}=\\sqrt{9}=3,\\qquad q_1=\\frac{a_1}{r_{11}}=\\left(\\frac{1}{3},\\frac{2}{3},\\frac{2}{3}\\right)^T.\n$$\nThen compute\n$$\nr_{12}=q_1^T a_2=\\left(\\frac{1}{3},\\frac{2}{3},\\frac{2}{3}\\right)\\cdot(1,0,-1)=-\\frac{1}{3}.\n$$\nForm the orthogonal component of $a_2$ relative to $q_1$:\n$$\nu_2=a_2-r_{12}q_1=a_2+\\frac{1}{3}q_1=\\left(\\frac{10}{9},\\frac{2}{9},-\\frac{7}{9}\\right)^T.\n$$\nIts norm gives\n$$\nr_{22}=\\|u_2\\|=\\sqrt{\\left(\\frac{10}{9}\\right)^2+\\left(\\frac{2}{9}\\right)^2+\\left(-\\frac{7}{9}\\right)^2}=\\frac{1}{9}\\sqrt{153}=\\frac{\\sqrt{17}}{3},\n$$\nwhich is positive. Therefore the $R$ factor is\n$$\nR=\\begin{pmatrix}\nr_{11}  r_{12} \\\\\n0  r_{22}\n\\end{pmatrix}\n=\\begin{pmatrix}\n3  -\\frac{1}{3} \\\\\n0  \\frac{\\sqrt{17}}{3}\n\\end{pmatrix}.\n$$\nThis $R$ has positive diagonal entries, as required.",
            "answer": "$$\\boxed{\\begin{pmatrix}3  -\\frac{1}{3} \\\\ 0  \\frac{\\sqrt{17}}{3}\\end{pmatrix}}$$"
        }
    ]
}