{
    "hands_on_practices": [
        {
            "introduction": "This first practice is designed to build your fluency with the fundamental definitions of several key matrix norms. We will move beyond simple examples and apply these definitions to a Givens rotation matrix, a fundamental tool in numerical linear algebra used for tasks like QR factorization. By calculating induced norms like the $1$-norm, $2$-norm, and $\\infty$-norm, as well as the Frobenius norm, you will gain direct, hands-on experience and uncover how the \"size\" of a matrix is not a single number, but depends critically on the norm chosen to measure it.",
            "id": "2449542",
            "problem": "In numerical linear algebra for computational engineering, a Givens rotation is used to introduce zeros while preserving Euclidean lengths. Let $n \\ge 2$ be an integer and consider the $n \\times n$ Givens rotation matrix $G$ that acts on coordinate indices $p$ and $q$ with $1 \\le p < q \\le n$ by replacing the $2 \\times 2$ principal submatrix in rows and columns $\\{p,q\\}$ with\n$$\n\\begin{pmatrix}\n\\cos\\theta & \\sin\\theta \\\\\n-\\sin\\theta & \\cos\\theta\n\\end{pmatrix},\n$$\nand leaving all other diagonal entries equal to $1$ and all other off-diagonal entries equal to $0$. Using only the definitions of induced matrix norms and the Frobenius norm, determine exact expressions, as functions of $n$ and $\\theta$, for the matrix $1$-norm $\\|G\\|_{1}$, the matrix $2$-norm $\\|G\\|_{2}$, the matrix $\\infty$-norm $\\|G\\|_{\\infty}$, and the Frobenius norm $\\|G\\|_{F}$. Provide your final answer as exact analytic expressions. No rounding is required. Finally, briefly interpret your result in terms of length preservation and norm dependence.",
            "solution": "The problem statement is first subjected to validation.\n\n**Step 1: Extract Givens**\n- $n$ is an integer, $n \\ge 2$.\n- $G$ is an $n \\times n$ Givens rotation matrix.\n- The matrix acts on coordinate indices $p$ and $q$ where $1 \\le p < q \\le n$.\n- The $2 \\times 2$ principal submatrix at rows and columns $\\{p,q\\}$ is $\\begin{pmatrix} \\cos\\theta & \\sin\\theta \\\\ -\\sin\\theta & \\cos\\theta \\end{pmatrix}$.\n- All other diagonal entries are $1$.\n- All other off-diagonal entries are $0$.\n- The task is to determine exact expressions for $\\|G\\|_{1}$, $\\|G\\|_{2}$, $\\|G\\|_{\\infty}$, and $\\|G\\|_{F}$ using only the definitions of the norms.\n- An interpretation of the result is required.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on standard, well-defined concepts in numerical linear algebra, specifically Givens rotations and matrix norms. These are fundamental tools in computational engineering.\n- **Well-Posed:** The matrix $G$ is unambiguously defined for any valid $n$, $p$, $q$, and $\\theta$. The matrix norms are standard mathematical functions with unique outputs for any given matrix. The problem is well-posed.\n- **Objective:** The problem is stated using precise mathematical language, free from subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem is scientifically sound, well-posed, objective, and complete. It is therefore deemed **valid**. We may proceed with the solution.\n\nThe Givens rotation matrix $G$ is an $n \\times n$ matrix that is identical to the identity matrix $I_n$ except for four entries. The entries of $G$, denoted by $g_{ij}$, are:\n$g_{pp} = \\cos\\theta$\n$g_{qq} = \\cos\\theta$\n$g_{pq} = \\sin\\theta$\n$g_{qp} = -\\sin\\theta$\n$g_{ii} = 1$ for $i \\in \\{1, \\dots, n\\} \\setminus \\{p, q\\}$\n$g_{ij} = 0$ for all other pairs $(i, j)$ with $i \\ne j$.\n\nWe will now compute the required norms based on their fundamental definitions.\n\n**1. The Matrix $1$-Norm ($\\|G\\|_{1}$)**\nThe matrix $1$-norm, or column-sum norm, is defined as the maximum absolute column sum:\n$$ \\|G\\|_{1} = \\max_{1 \\le j \\le n} \\sum_{i=1}^{n} |g_{ij}| $$\nWe examine the columns of $G$:\n- For any column $j$ such that $j \\ne p$ and $j \\ne q$, the only non-zero entry is $g_{jj} = 1$. The sum of absolute values for such a column is $|1| = 1$.\n- For column $p$, the non-zero entries are $g_{pp} = \\cos\\theta$ and $g_{qp} = -\\sin\\theta$. The sum is $|g_{pp}| + |g_{qp}| = |\\cos\\theta| + |-\\sin\\theta| = |\\cos\\theta| + |\\sin\\theta|$.\n- For column $q$, the non-zero entries are $g_{pq} = \\sin\\theta$ and $g_{qq} = \\cos\\theta$. The sum is $|g_{pq}| + |g_{qq}| = |\\sin\\theta| + |\\cos\\theta|$.\n\nThe $1$-norm is the maximum of these sums:\n$$ \\|G\\|_{1} = \\max(1, |\\cos\\theta| + |\\sin\\theta|) $$\nWe know that for any real $\\theta$, $|\\cos\\theta| + |\\sin\\theta| \\ge \\sqrt{\\cos^2\\theta + \\sin^2\\theta} = 1$. The equality holds when $\\theta$ is an integer multiple of $\\frac{\\pi}{2}$. Therefore, the maximum is always $|\\cos\\theta| + |\\sin\\theta|$.\n$$ \\|G\\|_{1} = |\\cos\\theta| + |\\sin\\theta| $$\n\n**2. The Matrix $\\infty$-Norm ($\\|G\\|_{\\infty}$)**\nThe matrix $\\infty$-norm, or row-sum norm, is defined as the maximum absolute row sum:\n$$ \\|G\\|_{\\infty} = \\max_{1 \\le i \\le n} \\sum_{j=1}^{n} |g_{ij}| $$\nWe examine the rows of $G$:\n- For any row $i$ such that $i \\ne p$ and $i \\ne q$, the only non-zero entry is $g_{ii} = 1$. The sum of absolute values for such a row is $|1| = 1$.\n- For row $p$, the non-zero entries are $g_{pp} = \\cos\\theta$ and $g_{pq} = \\sin\\theta$. The sum is $|g_{pp}| + |g_{pq}| = |\\cos\\theta| + |\\sin\\theta|$.\n- For row $q$, the non-zero entries are $g_{qp} = -\\sin\\theta$ and $g_{qq} = \\cos\\theta$. The sum is $|g_{qp}| + |g_{qq}| = |-\\sin\\theta| + |\\cos\\theta| = |\\sin\\theta| + |\\cos\\theta|$.\n\nBy the same logic as for the $1$-norm, the maximum of these sums is:\n$$ \\|G\\|_{\\infty} = |\\cos\\theta| + |\\sin\\theta| $$\n\n**3. The Matrix $2$-Norm ($\\|G\\|_{2}$)**\nThe matrix $2$-norm, or spectral norm, is defined as the square root of the largest eigenvalue of the matrix $G^T G$:\n$$ \\|G\\|_{2} = \\sqrt{\\lambda_{\\max}(G^T G)} $$\nThe matrix $G$ is an orthogonal matrix. We verify this by computing $G^T G$. The transpose $G^T$ is identical to $G$ except that the roles of $g_{pq}$ and $g_{qp}$ are swapped. Thus, $g_{pq}^T = g_{qp} = -\\sin\\theta$ and $g_{qp}^T = g_{pq} = \\sin\\theta$.\nThe product $G^T G$ will be an identity matrix. To see this, consider the submatrix action. The $2 \\times 2$ submatrix of $G$ is $R(\\theta) = \\begin{pmatrix} \\cos\\theta & \\sin\\theta \\\\ -\\sin\\theta & \\cos\\theta \\end{pmatrix}$. Its transpose is $R(\\theta)^T = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix}$.\nThe product is:\n$$ R(\\theta)^T R(\\theta) = \\begin{pmatrix} \\cos^2\\theta + \\sin^2\\theta & \\cos\\theta\\sin\\theta - \\sin\\theta\\cos\\theta \\\\ \\sin\\theta\\cos\\theta - \\cos\\theta\\sin\\theta & \\sin^2\\theta + \\cos^2\\theta \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\nSince all other rows and columns of $G$ are standard basis vectors, the full matrix product $G^T G$ results in the $n \\times n$ identity matrix, $I_n$.\nThe eigenvalues of $I_n$ are all equal to $1$. Thus, $\\lambda_{\\max}(G^T G) = 1$.\nThe $2$-norm is therefore:\n$$ \\|G\\|_{2} = \\sqrt{1} = 1 $$\n\n**4. The Frobenius Norm ($\\|G\\|_{F}$)**\nThe Frobenius norm is defined as the square root of the sum of the squares of the magnitudes of all entries:\n$$ \\|G\\|_{F} = \\sqrt{\\sum_{i=1}^{n} \\sum_{j=1}^{n} |g_{ij}|^2} $$\nWe sum the squares of all non-zero entries of $G$:\n- $(n-2)$ entries are equal to $1$ (the diagonal entries for indices other than $p$ and $q$). Their contribution to the sum of squares is $(n-2) \\times 1^2 = n-2$.\n- The four entries in the $\\{p,q\\}$ sub-block are $\\cos\\theta$, $\\sin\\theta$, $-\\sin\\theta$, and $\\cos\\theta$. Their contribution to the sum of squares is $(\\cos\\theta)^2 + (\\sin\\theta)^2 + (-\\sin\\theta)^2 + (\\cos\\theta)^2 = 2\\cos^2\\theta + 2\\sin^2\\theta = 2(\\cos^2\\theta + \\sin^2\\theta) = 2$.\n\nThe total sum of squares is $(n-2) + 2 = n$.\nThe Frobenius norm is therefore:\n$$ \\|G\\|_{F} = \\sqrt{n} $$\n\n**Interpretation of Results**\nThe problem asks for an interpretation in terms of length preservation and norm dependence.\n- **Length Preservation:** The result $\\|G\\|_{2} = 1$ is the mathematical statement of the geometric property that Givens rotations are isometries in Euclidean space. The $2$-norm of a matrix measures the maximum factor by which it can stretch a vector's Euclidean length ($\\|x\\|_2$). A value of $1$ signifies that no vector's length is changed, i.e., $\\|Gx\\|_{2} = \\|x\\|_{2}$ for all $x \\in \\mathbb{R}^n$. This is precisely what is meant by \"preserving Euclidean lengths\".\n- **Norm Dependence:** The results demonstrate that the \"size\" of the matrix $G$ depends on the chosen norm. While it is a pure rotation of size $1$ in the Euclidean sense ($\\|G\\|_2=1$), its $1$-norm and $\\infty$-norm, $\\|G\\|_{1} = \\|G\\|_{\\infty} = |\\cos\\theta| + |\\sin\\theta|$, depend on the rotation angle $\\theta$ and can be as large as $\\sqrt{2}$. This shows that a transformation that is a pure rotation for the Euclidean norm can act as an amplification for vectors measured in other norms (e.g., the $1$-norm or $\\infty$-norm). The Frobenius norm, $\\|G\\|_{F}=\\sqrt{n}$, depends only on the dimension $n$ of the space, not the angle $\\theta$. This is because a rotation merely redistributes the matrix elements' squared magnitudes within the active sub-block, leaving their sum invariant, while the norm counts all entries, including the $n-2$ invariant unit entries on the diagonal. The different results for different norms highlight that there is no single, universal measure for the \"size\" of a linear transformation; the measure is dependent on the metric used.",
            "answer": "$$ \\boxed{\\begin{pmatrix} |\\cos\\theta| + |\\sin\\theta| & 1 & |\\cos\\theta| + |\\sin\\theta| & \\sqrt{n} \\end{pmatrix}} $$"
        },
        {
            "introduction": "While all matrix norms on a finite-dimensional space are mathematically equivalent, this can be a misleading statement in computational practice. The constants that bound one norm in terms of another can grow with the dimension of the matrix, a critical detail when analyzing algorithms for large-scale problems. This exercise demonstrates this principle by asking you to construct a sequence of matrices where the ratio of two common norms, the $1$-norm and the Frobenius norm, grows with the matrix dimension $n$. This will provide a clear, quantitative understanding of why the choice of norm can have significant implications for numerical analysis in high dimensions.",
            "id": "2449576",
            "problem": "In computational engineering, performance and stability analyses often compare different norms on spaces whose dimension grows with a discretization parameter. For each positive integer $n \\in \\mathbb{N}$, consider the matrix $A_n \\in \\mathbb{R}^{n \\times n}$ defined by entries $[A_n]_{i j} = 1$ if $j = 1$ and $[A_n]_{i j} = 0$ if $j \\neq 1$. Let the induced $1$-norm of a matrix $A$ be defined by $\\|A\\|_1 = \\max_{x \\in \\mathbb{R}^n \\setminus \\{0\\}} \\frac{\\|A x\\|_1}{\\|x\\|_1}$, where the vector $1$-norm is $\\|x\\|_1 = \\sum_{i=1}^{n} |x_i|$. Let the Frobenius norm be defined by $\\|A\\|_F = \\sqrt{\\sum_{i=1}^{n}\\sum_{j=1}^{n} ([A]_{i j})^2}$.\n\nCompute, in closed form as a function of $n$, the ratio $r(n) = \\frac{\\|A_n\\|_1}{\\|A_n\\|_F}$. Provide your final answer as a single analytic expression in $n$. No rounding is required.",
            "solution": "The problem as stated is valid. It is a well-posed mathematical exercise in computational engineering, free of any scientific, logical, or factual flaws. All terms are standard and defined precisely. We shall proceed with the solution.\n\nThe problem requires the computation of the ratio $r(n) = \\frac{\\|A_n\\|_1}{\\|A_n\\|_F}$ for a specific family of matrices $A_n \\in \\mathbb{R}^{n \\times n}$. The matrix $A_n$ is defined for each positive integer $n$ by its entries:\n$$\n[A_n]_{i j} = \\begin{cases} 1 & \\text{if } j = 1 \\\\ 0 & \\text{if } j \\neq 1 \\end{cases}\n$$\nThis means that for any given $n$, the matrix $A_n$ has a first column of all ones, and all other columns are zero.\n$$\nA_n = \\begin{pmatrix}\n1 & 0 & \\dots & 0 \\\\\n1 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & 0 & \\dots & 0\n\\end{pmatrix}\n$$\n\nFirst, we compute the induced $1$-norm, $\\|A_n\\|_1$. The problem provides the definition $\\|A\\|_1 = \\max_{x \\neq 0} \\frac{\\|Ax\\|_1}{\\|x\\|_1}$. It is a standard result in linear algebra that the induced $1$-norm of a matrix is equal to the maximum absolute column sum. That is, for any matrix $A \\in \\mathbb{R}^{m \\times n}$:\n$$\n\\|A\\|_1 = \\max_{1 \\le j \\le n} \\sum_{i=1}^{m} |[A]_{ij}|\n$$\nWe apply this formula to our matrix $A_n \\in \\mathbb{R}^{n \\times n}$. We must compute the sum of the absolute values of the elements for each column and find the maximum of these sums.\n\nFor the first column ($j=1$):\n$$\n\\sum_{i=1}^{n} |[A_n]_{i1}| = \\sum_{i=1}^{n} |1| = \\sum_{i=1}^{n} 1 = n\n$$\nFor any other column ($j > 1$):\n$$\n\\sum_{i=1}^{n} |[A_n]_{ij}| = \\sum_{i=1}^{n} |0| = 0\n$$\nThe set of column sums is $\\{n, 0, 0, \\ldots, 0\\}$. The maximum of this set is clearly $n$.\nTherefore, the induced $1$-norm of $A_n$ is:\n$$\n\\|A_n\\|_1 = n\n$$\n\nNext, we compute the Frobenius norm, $\\|A_n\\|_F$. The definition provided is:\n$$\n\\|A\\|_F = \\sqrt{\\sum_{i=1}^{n}\\sum_{j=1}^{n} ([A]_{i j})^2}\n$$\nWe apply this definition to our matrix $A_n$. The non-zero entries of $A_n$ are $[A_n]_{i1} = 1$ for $i=1, \\ldots, n$. All other entries are $0$.\nThe sum of the squares of all entries is:\n$$\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} ([A_n]_{i j})^2 = \\sum_{i=1}^{n} ([A_n]_{i 1})^2 + \\sum_{i=1}^{n} \\sum_{j=2}^{n} ([A_n]_{i j})^2\n$$\nSubstituting the values of the entries:\n$$\n\\sum_{i=1}^{n} (1)^2 + \\sum_{i=1}^{n} \\sum_{j=2}^{n} (0)^2 = \\sum_{i=1}^{n} 1 + 0 = n\n$$\nThe Frobenius norm is the square root of this sum.\n$$\n\\|A_n\\|_F = \\sqrt{n}\n$$\n\nFinally, we compute the required ratio $r(n)$.\n$$\nr(n) = \\frac{\\|A_n\\|_1}{\\|A_n\\|_F}\n$$\nSubstituting the expressions we found for the norms:\n$$\nr(n) = \\frac{n}{\\sqrt{n}}\n$$\nSince $n$ is a positive integer, $n \\ge 1$, we can simplify this expression.\n$$\nr(n) = \\frac{(\\sqrt{n})^2}{\\sqrt{n}} = \\sqrt{n}\n$$\nThe ratio, as a function of $n$, is $\\sqrt{n}$.",
            "answer": "$$\n\\boxed{\\sqrt{n}}\n$$"
        },
        {
            "introduction": "Vector and matrix norms are not just tools for measuring error or magnitude; they also define the geometry of vector spaces. This practice problem brings this geometric aspect to the forefront by challenging you to find the point on an $L_1$ unit ball that is closest to a given external point, as measured by the standard Euclidean ($L_2$) distance. Solving this problem requires you to blend concepts from optimization, such as Lagrange multipliers, with a clear understanding of the geometric shapes defined by different norms. It is an excellent exercise in synthesizing knowledge to solve a multi-faceted computational problem.",
            "id": "2449555",
            "problem": "Let $B_1 \\subset \\mathbb{R}^3$ denote the $L_1$ unit ball, defined by $B_1 = \\{ x \\in \\mathbb{R}^3 : \\|x\\|_1 \\le 1 \\}$, where $\\|x\\|_1 = |x_1| + |x_2| + |x_3|$. Consider the point $y = (2, 2, 2) \\in \\mathbb{R}^3$. Determine the unique point $x^{\\star} \\in B_1$ that minimizes the standard Euclidean distance $\\|x - y\\|_2$. Provide the exact coordinates of $x^{\\star}$.",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- The set is the $L_1$ unit ball in $\\mathbb{R}^3$: $B_1 = \\{ x \\in \\mathbb{R}^3 : \\|x\\|_1 \\le 1 \\}$.\n- The $L_1$ norm is defined as $\\|x\\|_1 = |x_1| + |x_2| + |x_3|$.\n- The point is $y = (2, 2, 2) \\in \\mathbb{R}^3$.\n- The objective is to find the unique point $x^{\\star} \\in B_1$ that minimizes the Euclidean distance $\\|x - y\\|_2$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a classic convex optimization problem, specifically the projection of a point onto a closed, convex set.\n- **Scientifically Grounded:** The concepts of $L_p$ norms, unit balls, and Euclidean distance are fundamental in linear algebra and functional analysis. The problem is mathematically sound.\n- **Well-Posed:** The set $B_1$ (a regular octahedron) is a compact and convex subset of $\\mathbb{R}^3$. The function to be minimized, the Euclidean distance from a fixed point, is a strictly convex function. The problem of minimizing a strictly convex function over a compact convex set has a unique solution.\n- **Objective:** The problem is stated using precise, unambiguous mathematical definitions.\n\n**Verdict:** The problem is valid as it is scientifically grounded, well-posed, and objective.\n\nWe now proceed with the solution.\nThe problem is to find a point $x^{\\star} \\in B_1$ that minimizes the Euclidean distance $\\|x - y\\|_2$ to the point $y = (2, 2, 2)$. This is equivalent to minimizing the squared Euclidean distance, $f(x) = \\|x - y\\|_2^2$, which is analytically more convenient due to its differentiability. The optimization problem is thus:\n$$ \\text{Minimize} \\quad f(x) = (x_1 - 2)^2 + (x_2 - 2)^2 + (x_3 - 2)^2 $$\n$$ \\text{Subject to} \\quad |x_1| + |x_2| + |x_3| \\le 1 $$\n\nFirst, we analyze the properties of the optimal solution $x^{\\star} = (x_1^{\\star}, x_2^{\\star}, x_3^{\\star})$. Let us assume, for the sake of contradiction, that one component of the solution is negative, for instance, $x_k^{\\star} < 0$ for some $k \\in \\{1, 2, 3\\}$. Consider a new point $\\tilde{x}$ defined by its components $\\tilde{x}_i = x_i^{\\star}$ for $i \\neq k$ and $\\tilde{x}_k = -x_k^{\\star} > 0$. The $L_1$ norm of $\\tilde{x}$ is $\\|\\tilde{x}\\|_1 = \\sum_{i \\neq k} |x_i^{\\star}| + |-x_k^{\\star}| = \\|x^{\\star}\\|_1 \\le 1$, so $\\tilde{x}$ is also an element of the feasible set $B_1$.\nNow, we compare the values of the objective function for $x^{\\star}$ and $\\tilde{x}$. The difference is:\n$$ f(x^{\\star}) - f(\\tilde{x}) = \\|x^{\\star}-y\\|_2^2 - \\|\\tilde{x}-y\\|_2^2 = (x_k^{\\star} - 2)^2 - (\\tilde{x}_k - 2)^2 $$\nSince $y_k=2 > 0$ and $x_k^{\\star} < 0$, let $a = -x_k^{\\star} > 0$. The expression becomes:\n$$ (-a - 2)^2 - (a - 2)^2 = (a+2)^2 - (a-2)^2 = (a^2 + 4a + 4) - (a^2 - 4a + 4) = 8a $$\nBecause $a>0$, this difference $8a$ is strictly positive. This implies $f(x^{\\star}) > f(\\tilde{x})$, which contradicts the assumption that $x^{\\star}$ is the minimum. Therefore, no component of the solution can be negative. We must have $x_i^{\\star} \\ge 0$ for all $i \\in \\{1, 2, 3\\}$.\n\nWith the knowledge that $x_i \\ge 0$, the constraint $|x_1| + |x_2| + |x_3| \\le 1$ simplifies to $x_1 + x_2 + x_3 \\le 1$.\nThe unconstrained minimum of $f(x)$ occurs at the point where its gradient is zero: $\\nabla f(x) = (2(x_1-2), 2(x_2-2), 2(x_3-2)) = (0, 0, 0)$, which yields the solution $x = (2, 2, 2)$. This point is the same as $y$. We check if this point is in the feasible set $B_1$: $\\|(2, 2, 2)\\|_1 = 2+2+2 = 6$, which is greater than $1$. So, the unconstrained minimum lies outside the feasible region. Because $f(x)$ is a convex function and $B_1$ is a convex set, the constrained minimum must lie on the boundary of $B_1$. The boundary is defined by $\\|x\\|_1 = 1$.\nGiven $x_i \\ge 0$, the boundary condition becomes $x_1 + x_2 + x_3 = 1$.\n\nThe problem is now reduced to minimizing $f(x_1, x_2, x_3)$ subject to the equality constraint $g(x_1, x_2, x_3) = x_1 + x_2 + x_3 - 1 = 0$ and non-negativity constraints $x_i \\ge 0$. We employ the method of Lagrange multipliers. The Lagrangian function is:\n$$ \\mathcal{L}(x_1, x_2, x_3, \\lambda) = (x_1 - 2)^2 + (x_2 - 2)^2 + (x_3 - 2)^2 + \\lambda(x_1 + x_2 + x_3 - 1) $$\nTaking the partial derivatives with respect to each $x_i$ and setting them to zero gives the stationarity conditions:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial x_1} = 2(x_1 - 2) + \\lambda = 0 \\implies x_1 = 2 - \\frac{\\lambda}{2} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial x_2} = 2(x_2 - 2) + \\lambda = 0 \\implies x_2 = 2 - \\frac{\\lambda}{2} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial x_3} = 2(x_3 - 2) + \\lambda = 0 \\implies x_3 = 2 - \\frac{\\lambda}{2} $$\nThis result indicates that the optimal solution must have equal components, i.e., $x_1 = x_2 = x_3$.\n\nWe substitute this result into the equality constraint $x_1 + x_2 + x_3 = 1$:\n$$ \\left(2 - \\frac{\\lambda}{2}\\right) + \\left(2 - \\frac{\\lambda}{2}\\right) + \\left(2 - \\frac{\\lambda}{2}\\right) = 1 $$\n$$ 6 - \\frac{3\\lambda}{2} = 1 $$\n$$ 5 = \\frac{3\\lambda}{2} \\implies \\lambda = \\frac{10}{3} $$\nNow we can compute the value of the components $x_i$:\n$$ x_1 = x_2 = x_3 = 2 - \\frac{1}{2} \\left(\\frac{10}{3}\\right) = 2 - \\frac{5}{3} = \\frac{6-5}{3} = \\frac{1}{3} $$\nThe solution is $x^{\\star} = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$. We verify that this solution satisfies the non-negativity constraint $x_i \\ge 0$, which it does as $\\frac{1}{3} > 0$. This point lies on the face of the octahedron $B_1$ in the first octant. Due to the strict convexity of the squared Euclidean distance and the convexity of the set $B_1$, this point is the unique global minimum.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\end{pmatrix}}$$"
        }
    ]
}