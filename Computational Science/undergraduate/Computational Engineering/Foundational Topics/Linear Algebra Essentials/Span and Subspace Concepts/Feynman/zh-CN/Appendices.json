{
    "hands_on_practices": [
        {
            "introduction": "这项练习将带领你从抽象的理论走向具体的建构。在计算工程中，许多物理或计算约束（例如质量守恒定律所施加的 $\\sum x_i = 0$ 约束）会隐式地定义一个子空间。本实践将指导你完成将这种隐式规则转换为一个显式基底的基本过程，这是为受限系统中的真实自由度进行参数化的一项关键技能。",
            "id": "2435999",
            "problem": "在一个用于计算工程中$5$节点离散化的质量守恒预处理步骤中，一个修正向量 $\\mathbf{x} \\in \\mathbb{R}^{5}$ 必须满足一个平衡约束，即其所有分量之和为零。令\n$$\nS \\equiv \\left\\{ \\mathbf{x} \\in \\mathbb{R}^{5} \\,\\middle|\\, \\sum_{i=1}^{5} x_{i} = 0 \\right\\}。\n$$\n仅从子空间、生成空间和线性无关的核心定义出发，并且不使用除这些定义之外的任何现成定理，完成以下任务：\n1. 从第一性原理出发，证明$S$是$\\mathbb{R}^{5}$的一个子空间。\n2. 通过将$S$中的一个一般向量表示为自由分量的形式，并将其重写为一组最小固定向量的线性组合，来为$S$构造一个显式基。使用线性无关的定义来确定其最小性。\n3. 使用你的构造，确定$S$的维数。\n\n仅报告$S$的维数作为你的最终答案。无需四舍五入，不涉及单位。",
            "solution": "我们从定义开始。一个子集$U \\subseteq \\mathbb{R}^{n}$是子空间，当且仅当它包含零向量，并且对加法和标量乘法封闭。一个向量集$\\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{k} \\}$生成一个子空间$U$，如果每个$\\mathbf{u} \\in U$都可以写成这些向量的线性组合。这样一个集合是线性无关的，如果满足$a_{1}\\mathbf{v}_{1} + \\cdots + a_{k}\\mathbf{v}_{k} = \\mathbf{0}$的唯一系数是$a_{1} = \\cdots = a_{k} = 0$。基是一个生成空间的、线性无关的集合，其基数即为维数。\n\n步骤1：证明$S$是一个子空间。首先，零向量$\\mathbf{0} \\in \\mathbb{R}^{5}$的分量之和为$0$，因此$\\mathbf{0} \\in S$。设$\\mathbf{x}, \\mathbf{y} \\in S$，使得$\\sum_{i=1}^{5} x_{i} = 0$且$\\sum_{i=1}^{5} y_{i} = 0$。那么\n$$\n\\sum_{i=1}^{5} (x_{i} + y_{i}) = \\sum_{i=1}^{5} x_{i} + \\sum_{i=1}^{5} y_{i} = 0 + 0 = 0,\n$$\n所以$\\mathbf{x} + \\mathbf{y} \\in S$。对于任意标量$\\alpha \\in \\mathbb{R}$和向量$\\mathbf{x} \\in S$，\n$$\n\\sum_{i=1}^{5} (\\alpha x_{i}) = \\alpha \\sum_{i=1}^{5} x_{i} = \\alpha \\cdot 0 = 0,\n$$\n所以$\\alpha \\mathbf{x} \\in S$。因此，$S$是$\\mathbb{R}^{5}$的一个子空间。\n\n步骤2：构造一个基。设$\\mathbf{x} = (x_{1}, x_{2}, x_{3}, x_{4}, x_{5})^{\\top} \\in S$。其定义约束为\n$$\nx_{1} + x_{2} + x_{3} + x_{4} + x_{5} = 0。\n$$\n我们可以仅用代数方法求解一个分量，用其他分量来表示。取$x_{5}$为因变量：\n$$\nx_{5} = - (x_{1} + x_{2} + x_{3} + x_{4})。\n$$\n那么$S$中的每个向量$\\mathbf{x}$都可以由四个自由变量$x_{1}, x_{2}, x_{3}, x_{4} \\in \\mathbb{R}$参数化表示为\n$$\n\\mathbf{x} = \\begin{pmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4} \\\\ - (x_{1} + x_{2} + x_{3} + x_{4}) \\end{pmatrix}\n= x_{1} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix}\n+ x_{2} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix}\n+ x_{3} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix}\n+ x_{4} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}。\n$$\n定义这四个向量\n$$\n\\mathbf{v}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad\n\\mathbf{v}_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad\n\\mathbf{v}_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad\n\\mathbf{v}_{4} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}。\n$$\n根据参数化表示，$S$中的任意向量$\\mathbf{x}$都是$\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}, \\mathbf{v}_{4} \\}$的线性组合，因此这些向量生成$S$。\n\n为了用定义验证线性无关性，假设\n$$\na_{1} \\mathbf{v}_{1} + a_{2} \\mathbf{v}_{2} + a_{3} \\mathbf{v}_{3} + a_{4} \\mathbf{v}_{4} = \\mathbf{0}。\n$$\n令各分量相等，得到\n$$\n\\begin{aligned}\n\\text{分量 } 1: & \\quad a_{1} = 0, \\\\\n\\text{分量 } 2: & \\quad a_{2} = 0, \\\\\n\\text{分量 } 3: & \\quad a_{3} = 0, \\\\\n\\text{分量 } 4: & \\quad a_{4} = 0, \\\\\n\\text{分量 } 5: & \\quad - (a_{1} + a_{2} + a_{3} + a_{4}) = 0,\n\\end{aligned}\n$$\n并且前四个方程已经强制$a_{1} = a_{2} = a_{3} = a_{4} = 0$。因此，集合$\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}, \\mathbf{v}_{4} \\}$是线性无关的。\n\n由于该集合生成$S$并且是线性无关的，所以它是$S$的一个基。\n\n步骤3：确定维数。根据定义，一个子空间的维数等于其任意基中向量的个数。我们构造的基有$4$个向量，因此\n$$\n\\dim(S) = 4。\n$$\n这样就仅使用基本定义完成了构造和维数确定。",
            "answer": "$$\\boxed{4}$$"
        },
        {
            "introduction": "虽然理解一组向量的生成空间至关重要，但在许多工程应用中，了解该空间*之外*存在什么也同等重要，例如在优化中识别新的搜索方向或检测系统误差。这个计算练习将使正交补集这一抽象概念变得具体可操作，指导你开发一种鲁棒的算法，以找到一个保证与给定向量集线性无关的向量。",
            "id": "2435961",
            "problem": "给你 $\\mathbb{R}^n$ 中的 $k$ 个向量，其中 $k  n$。根据定义，$\\mathbb{R}^n$ 中一个有限向量集的张成（span）是这些向量所有有限线性组合的集合，且该张成是 $\\mathbb{R}^n$ 的一个子空间。你的任务是设计并实现一种计算方法，该方法针对每个提供的测试用例，构造一个向量 $\\mathbf{w} \\in \\mathbb{R}^n$，并保证该向量位于给定向量的张成之外。你的方法必须从第一性原理出发进行论证，从张成、子空间和正交性的核心定义开始，并且必须确保对输入向量之间的线性相关性具有鲁棒性。不涉及任何物理单位。所有角度（如有）都应以弧度为单位；然而，本任务中不需要三角函数量。\n\n对于每个测试用例，你的程序必须：\n- 接受一组 $\\mathbb{R}^n$ 中的 $k$ 个向量，其中 $k  n$（这些向量硬编码在你的程序中；不需要用户输入）。\n- 仅基于张成和子空间之间正交性的基本定义，构造一个保证在给定向量张成之外的向量 $\\mathbf{w}$。\n- 计算给定向量集的秩 $r$（其张成的维数）、零度 $d = n - r$，并通过计算 $\\mathbf{w}$ 在张成上的正交投影后的残差范数来验证 $\\mathbf{w}$ 是否在张成之外。\n- 使用数值容差 $\\tau = 10^{-10}$ 来判断 $\\mathbf{w}$ 是否在张成之外，判据为：若残差范数严格大于 $\\tau \\cdot \\max(1, \\lVert \\mathbf{w} \\rVert_2)$，则声明其在张成之外，其中 $\\lVert \\cdot \\rVert_2$ 是欧几里得范数。\n- 为每个测试用例生成一个形式为 $[r, d, b]$ 的列表作为结果，其中 $r$ 是一个整数， $d$ 是一个整数， $b$ 是一个布尔值，表示在所述准则下 $\\mathbf{w}$ 是否在张成之外。\n\n测试集（每个用例列出 $n$、$k$ 和 $\\mathbb{R}^n$ 中的 $k$ 个列向量）：\n- 用例 $1$：$n = 3$，$k = 2$，向量 $\\left( $1$, $0$, $0$ \\right)$ 和 $\\left( $0$, $1$, $0$ \\right)$。\n- 用例 $2$：$n = 4$，$k = 3$，向量 $\\left( $1$, $1$, $0$, $0$ \\right)$、$\\left( $2$, $2$, $0$, $0$ \\right)$ 和 $\\left( $0$, $0$, $1$, $0$ \\right)$。\n- 用例 $3$：$n = 3$，$k = 2$，向量 $\\left( $1$, $0$, $0$ \\right)$ 和 $\\left( $1$, $1 \\times 10^{-12}$, $0$ \\right)$。\n- 用例 $4$：$n = 5$，$k = 4$，向量 $\\left( $1$, $0$, $0$, $0$, $0$ \\right)$、$\\left( $0$, $1$, $0$, $0$, $0$ \\right)$、$\\left( $0$, $0$, $1$, $0$, $0$ \\right)$ 和 $\\left( $0$, $0$, $0$, $1$, $0$ \\right)$。\n- 用例 $5$：$n = 3$，$k = 1$，向量 $\\left( $0$, $0$, $0$ \\right)$。\n\n你的程序应该生成单行输出，其中包含用方括号括起来的、以逗号分隔的各用例结果列表。具体来说，输出必须是\n$[\\,[r_1,d_1,b_1],[r_2,d_2,b_2],\\dots,[r_5,d_5,b_5]\\,]$\n形式的单行，不含任何额外文本。每个 $r_i$ 和 $d_i$ 必须是整数，每个 $b_i$ 必须是布尔值。不允许用户输入，也不允许文件输入或输出。计算过程必须是自洽且可复现的。",
            "solution": "所述问题是有效的。这是一个计算线性代数中的适定问题，其牢固地建立在公认的数学原理之上。所有必要的定义、数据和条件均已提供，不存在矛盾或歧义。对“零度”(nullity)定义为 $d = n - r$ 也已明确给出，排除了任何误解的可能。因此，我们可以进行形式化的求解。\n\n设给定的 $\\mathbb{R}^n$ 中 $k$ 个向量的集合为 $V = \\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k\\}$。这些向量的张成 $\\text{span}(V)$ 是它们所有线性组合的集合，并构成 $\\mathbb{R}^n$ 的一个向量子空间。我们将这个子空间记为 $S$。根据定义，$S = \\{ \\mathbf{s} \\in \\mathbb{R}^n \\mid \\mathbf{s} = \\sum_{i=1}^k c_i \\mathbf{v}_i \\text{ 对于某些标量 } c_i \\in \\mathbb{R} \\}$。该子空间的维度 $r = \\dim(S)$ 是这组向量的秩。由于该集合包含 $k$ 个向量，必有 $r \\le k$。问题陈述给出了关键条件 $k  n$，这意味着 $r \\le k  n$。这保证了 $S$ 是 $\\mathbb{R}^n$ 的一个真子空间。\n\n我们的任务是构造一个保证在 $S$ 之外的向量 $\\mathbf{w}$。一种基于原理的方法依赖于正交性的概念。线性代数基本定理指出，任何向量空间 $\\mathbb{R}^n$ 都可以分解为一个子空间 $S$ 与其正交补 $S^\\perp$ 的直和，即 $\\mathbb{R}^n = S \\oplus S^\\perp$。正交补 $S^\\perp$ 被定义为 $\\mathbb{R}^n$ 中与 $S$ 中每个向量都正交的所有向量的集合：\n$$ S^\\perp = \\{ \\mathbf{u} \\in \\mathbb{R}^n \\mid \\mathbf{u}^T \\mathbf{s} = 0 \\text{ for all } \\mathbf{s} \\in S \\} $$\n这些互补子空间的维数通过方程 $\\dim(S) + \\dim(S^\\perp) = \\dim(\\mathbb{R}^n) = n$ 关联。问题定义了一个量 $d = n-r$。通过代入 $r = \\dim(S)$，我们得到 $d = n - \\dim(S) = \\dim(S^\\perp)$。条件 $r  n$ 意味着 $d = \\dim(S^\\perp)  0$。这是一个关键结果：正交补 $S^\\perp$ 是一个非平凡子空间，并且包含非零向量。\n\n任何非零向量 $\\mathbf{w} \\in S^\\perp$ 都保证不属于 $S$。这可以用反证法证明。假设 $\\mathbf{w} \\in S^\\perp$，$\\mathbf{w} \\neq \\mathbf{0}$，并且 $\\mathbf{w} \\in S$。作为 $S^\\perp$ 的元素，$\\mathbf{w}$ 必须与 $S$ 中的每个向量都正交。由于 $\\mathbf{w}$ 本身就在 $S$ 中，它必须与自身正交。这意味着内积 $\\mathbf{w}^T \\mathbf{w} = \\lVert \\mathbf{w} \\rVert_2^2 = 0$，这当且仅当 $\\mathbf{w}$ 是零向量 $\\mathbf{w} = \\mathbf{0}$ 时成立。这与 $\\mathbf{w}$ 是非零向量的假设相矛盾。因此，在 $S^\\perp$ 中找到的任何非零向量都确定无疑地在张成 $S$ 之外。\n\n现在，任务简化为在 $S^\\perp$ 中找到一个非零向量。在计算上，我们可以通过首先将给定的向量 $\\mathbf{v}_i$ 排列成一个 $n \\times k$ 矩阵的列来实现这一点，$A = [\\mathbf{v}_1 | \\mathbf{v}_2 | \\dots | \\mathbf{v}_k]$。此时，子空间 $S$ 与 $A$ 的列空间 $\\text{Col}(A)$ 相同。其正交补 $S^\\perp$ 等价于 $A$ 的转置的零空间，即 $S^\\perp = (\\text{Col}(A))^\\perp = \\text{Nul}(A^T)$。\n\n分析这些基本子空间的最数值鲁棒的方法是奇异值分解（SVD）。矩阵 $A$ 的SVD是将其分解为 $A = U \\Sigma V^T$，其中：\n- $U$ 是一个 $n \\times n$ 的正交矩阵，其列 $\\{\\mathbf{u}_1, \\dots, \\mathbf{u}_n\\}$ 构成 $\\mathbb{R}^n$ 的一个标准正交基。\n- $\\Sigma$ 是一个 $n \\times k$ 的矩形矩阵，其主对角线上有非负实数，即奇异值 $\\sigma_i$。\n- $V$ 是一个 $k \\times k$ 的正交矩阵。\n\n矩阵 $A$ 的秩 $r$ 对应于非零奇异值的数量。矩阵 $U$ 的前 $r$ 列，即 $\\{\\mathbf{u}_1, \\dots, \\mathbf{u}_r\\}$，构成了列空间 $\\text{Col}(A) = S$ 的一个标准正交基。$U$ 随后的 $n-r$ 列，即 $\\{\\mathbf{u}_{r+1}, \\dots, \\mathbf{u}_n\\}$，构成了正交补 $S^\\perp$ 的一个标准正交基。\n\n这为构造向量 $\\mathbf{w}$ 提供了一个确定性算法。我们计算 $A$ 的完整SVD，并选择 $U$ 的最后 $n-r$ 列中的一列。我们将一致地选择最后一列，$\\mathbf{w} = \\mathbf{u}_n$。作为正交矩阵的列，这些基向量的长度为单位长度，即 $\\lVert \\mathbf{w} \\rVert_2 = 1$，从而确保 $\\mathbf{w}$ 非零。\n\n算法流程如下：\n$1$. 从输入的列向量组装 $n \\times k$ 矩阵 $A$。\n$2$. 计算 $A$ 的数值秩 $r$。然后根据 $d = n-r$ 计算问题中定义的“零度”。\n$3$. 对 $A$ 执行完整的SVD，以获得 $n \\times n$ 的正交矩阵 $U$。\n$4$. 选择 $U$ 的最后一列作为向量 $\\mathbf{w}$，即 $\\mathbf{w} = U[:, n-1]$。\n$5$. 为了验证，我们必须计算将 $\\mathbf{w}$ 正交投影到 $S$ 后的残差范数。到 $S$ 上的正交投影算子是 $P_S = U_r U_r^T$，其中 $U_r = [\\mathbf{u}_1 | \\dots | \\mathbf{u}_r]$。\n$6$. 对于我们选择的 $\\mathbf{w}=\\mathbf{u}_n$，其投影为 $P_S(\\mathbf{w}) = U_r U_r^T \\mathbf{u}_n$。由于 $U$ 的列是相互正交的，乘积 $U_r^T \\mathbf{u}_n$ 是一个零向量。因此，投影 $P_S(\\mathbf{u}_n)$ 也是零向量 $\\mathbf{0}$。\n$7$. 残差向量为 $\\mathbf{w} - P_S(\\mathbf{w}) = \\mathbf{u}_n - \\mathbf{0} = \\mathbf{u}_n$。该残差的范数为 $\\lVert \\mathbf{u}_n \\rVert_2 = 1$。\n$8$. 然后我们应用指定的检验：残差范数是否严格大于 $\\tau \\cdot \\max(1, \\lVert \\mathbf{w} \\rVert_2)$？代入数值，我们检验 $1  10^{-10} \\cdot \\max(1, 1)$ 是否成立，该式可简化为 $1  10^{-10}$。此不等式为真。因此，布尔结果 $b$ 为 `True`。此逻辑对所有 $r  n$ 的测试用例均成立，包括输入为零向量（此时 $r=0$）的平凡情况。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for a predefined test suite, constructing a vector outside the\n    span of a given set of vectors and performing the required analysis.\n    \"\"\"\n\n    # Test suite defined in the problem statement.\n    # Each case is a tuple: (n, list_of_column_vectors)\n    test_cases = [\n        (3, [(1, 0, 0), (0, 1, 0)]),\n        (4, [(1, 1, 0, 0), (2, 2, 0, 0), (0, 0, 1, 0)]),\n        (3, [(1, 0, 0), (1, 1e-12, 0)]),\n        (5, [(1, 0, 0, 0, 0), (0, 1, 0, 0, 0), (0, 0, 1, 0, 0), (0, 0, 0, 1, 0)]),\n        (3, [(0, 0, 0)])\n    ]\n\n    all_results = []\n    \n    # Numerical tolerance for verification check\n    tau = 1e-10\n\n    for n, vectors in test_cases:\n        # Step 1: Construct the n x k matrix A from the input vectors.\n        # The input vectors are column vectors, so we transpose the array.\n        # Handle the edge case of an empty list of vectors.\n        if not vectors or not vectors[0]:\n            A = np.zeros((n, 0))\n        else:\n            A = np.array(vectors).T\n        \n        # Step 2: Compute the rank r and the problem-defined nullity d.\n        # numpy.linalg.matrix_rank uses SVD and is numerically robust.\n        r = np.linalg.matrix_rank(A)\n        d = n - r\n\n        # Step 3: Compute the full SVD of A.\n        # full_matrices=True is essential to get the full n x n U matrix.\n        U, s, Vh = np.linalg.svd(A, full_matrices=True)\n\n        # Step 4: Select vector w from the orthogonal complement's basis.\n        # The last n-r columns of U form a basis for the orthogonal complement.\n        # For a deterministic choice, we take the last column of U.\n        w = U[:, -1]\n\n        # Step 5: Verify that w is outside the span S.\n        # The first r columns of U form an orthonormal basis for the span S.\n        Ur = U[:, :r]\n        \n        # Calculate the orthogonal projection of w onto the span S.\n        # If r is 0, the span is the zero vector, so the projection is zero.\n        # numpy handles matrix multiplication with shape (n, 0) correctly, resulting in a zero vector.\n        projection_w = Ur @ (Ur.T @ w)\n        \n        # Calculate the residual vector and its norm.\n        residual_vector = w - projection_w\n        residual_norm = np.linalg.norm(residual_vector)\n        \n        # Calculate the norm of w for the threshold calculation.\n        w_norm = np.linalg.norm(w)\n\n        # Apply the verification criterion from the problem statement.\n        threshold = tau * max(1.0, w_norm)\n        is_outside = residual_norm  threshold\n        \n        # Store the result [r, d, b] for this case.\n        all_results.append([int(r), int(d), is_outside])\n\n    # Format the final output string exactly as required.\n    # Individual list items are formatted as [r,d,b] without spaces.\n    def format_result_list(res_list):\n        r_val, d_val, b_val = res_list\n        return f\"[{r_val},{d_val},{str(b_val).lower()}]\"\n\n    formatted_results = [format_result_list(res) for res in all_results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "工程师经常面临在不丢失关键信息的前提下简化复杂高维数据的挑战。本实践正是为了解决这一问题，要求你找到“最佳”的低维子空间来近似给定的数据点集，从而最小化投影误差。这正是像主成分分析（Principal Component Analysis, PCA）这样强大的降维技术背后的核心思想，这些技术是数据分析、机器学习和模型降维中的基本工具。",
            "id": "2435976",
            "problem": "给定实欧几里得空间 $\\mathbb{R}^n$ 中的有限向量集。对于每个集合，考虑寻找一个固定维度为 $k$ 的线性子空间 $S \\subset \\mathbb{R}^n$，该子空间能够最小化给定向量到其在 $S$ 上的正交投影的总欧几里得距离平方和。形式上，对于一个集合 $V = \\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_m \\} \\subset \\mathbb{R}^n$ 和一个整数 $k$（$0 \\le k \\le n$），定义目标函数为\n$$\nE(S) = \\sum_{i=1}^{m} \\left\\| \\mathbf{v}_i - \\mathbf{P}_S(\\mathbf{v}_i) \\right\\|_2^2,\n$$\n其中 $\\mathbf{P}_S$ 表示到子空间 $S$ 上的正交投影算子。任务是，对于每个给定的测试用例，计算在所有 $k$ 维子空间 $S \\subset \\mathbb{R}^n$ 上 $E(S)$ 的可能最小值。距离为欧几里得距离，且不涉及任何物理单位。\n\n您的程序必须处理以下测试套件。对于每个用例，环境空间 $\\mathbb{R}^n$ 从向量的长度推断得出，子空间维度 $k$ 已被指定。当 $k = 0$ 时，将 $S$ 解释为零子空间 $\\{ \\mathbf{0} \\}$。\n\n测试套件：\n- 用例 $1$：$V = \\{ (1,2), (2,4), (3,6) \\} \\subset \\mathbb{R}^2$，其中 $k = 1$。\n- 用例 $2$：$V = \\{ (1,0,0), (0,1,0), (0,0,1) \\} \\subset \\mathbb{R}^3$，其中 $k = 2$。\n- 用例 $3$：$V = \\{ (1,0), (0,1), (0,0) \\} \\subset \\mathbb{R}^2$，其中 $k = 1$。\n- 用例 $4$：$V = \\{ (1,2), (-1,-2), (0,0) \\} \\subset \\mathbb{R}^2$，其中 $k = 0$。\n- 用例 $5$：$V = \\{ (1,1), (1,0) \\} \\subset \\mathbb{R}^2$，其中 $k = 1$。\n\n您的程序必须为每个用例输出最小总投影平方误差，表示为一个四舍五入到六位小数的实数。最终输出格式必须是单行，包含一个由逗号分隔的这五个数字组成的列表，并用方括号括起来，例如 $[a_1,a_2,a_3,a_4,a_5]$，其中每个 $a_i$ 是用例 $i$ 的结果，并按要求四舍五入到六位小数。没有输入；您的程序应将测试套件硬编码，并相应地生成单行输出。",
            "solution": "所述问题是线性代数领域中的一个良定问题，具体涉及用一个低维子空间对一组向量进行最优逼近。这是当数据未中心化时，主成分分析 (Principal Component Analysis, PCA) 的基础问题。我们将对解进行形式化推导。\n\n设给定的向量集为 $V = \\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_m \\}$，其中每个 $\\mathbf{v}_i \\in \\mathbb{R}^n$。我们的任务是找到一个固定维度为 $k$（$0 \\le k \\le n$）的线性子空间 $S \\subset \\mathbb{R}^n$，以最小化目标函数：\n$$\nE(S) = \\sum_{i=1}^{m} \\left\\| \\mathbf{v}_i - \\mathbf{P}_S(\\mathbf{v}_i) \\right\\|_2^2\n$$\n此处，$\\mathbf{P}_S$ 表示到子空间 $S$ 上的正交投影算子。项 $\\mathbf{v}_i - \\mathbf{P}_S(\\mathbf{v}_i)$ 是 $\\mathbf{v}_i$ 在 $S$ 的正交补（记为 $S^\\perp$）上的投影。根据勾股定理，任何向量 $\\mathbf{v}_i$ 都可以分解为正交分量：$\\mathbf{v}_i = \\mathbf{P}_S(\\mathbf{v}_i) + (\\mathbf{v}_i - \\mathbf{P}_S(\\mathbf{v}_i))$。这意味着：\n$$\n\\left\\| \\mathbf{v}_i \\right\\|_2^2 = \\left\\| \\mathbf{P}_S(\\mathbf{v}_i) \\right\\|_2^2 + \\left\\| \\mathbf{v}_i - \\mathbf{P}_S(\\mathbf{v}_i) \\right\\|_2^2\n$$\n因此，我们可以将目标函数重写为：\n$$\nE(S) = \\sum_{i=1}^{m} \\left( \\left\\| \\mathbf{v}_i \\right\\|_2^2 - \\left\\| \\mathbf{P}_S(\\mathbf{v}_i) \\right\\|_2^2 \\right) = \\left( \\sum_{i=1}^{m} \\left\\| \\mathbf{v}_i \\right\\|_2^2 \\right) - \\left( \\sum_{i=1}^{m} \\left\\| \\mathbf{P}_S(\\mathbf{v}_i) \\right\\|_2^2 \\right)\n$$\n第一项 $\\sum_{i=1}^{m} \\left\\| \\mathbf{v}_i \\right\\|_2^2$ 是给定向量的总范数平方和。这个量对于子空间 $S$ 的选择是一个常数。因此，最小化 $E(S)$ 等价于最大化第二项 $\\sum_{i=1}^{m} \\left\\| \\mathbf{P}_S(\\mathbf{v}_i) \\right\\|_2^2$，该项表示向量在 $S$ 上的投影的总长度平方和。\n\n我们构建一个数据矩阵 $X \\in \\mathbb{R}^{m \\times n}$，其中第 $i$ 行为向量 $\\mathbf{v}_i^T$。设 $\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\}$ 是 $k$ 维子空间 $S$ 的一个标准正交基。向量 $\\mathbf{v}_i$ 在 $S$ 上的投影为 $\\mathbf{P}_S(\\mathbf{v}_i) = \\sum_{j=1}^{k} (\\mathbf{v}_i^T \\mathbf{u}_j) \\mathbf{u}_j$。其范数平方为 $\\| \\mathbf{P}_S(\\mathbf{v}_i) \\|_2^2 = \\sum_{j=1}^{k} (\\mathbf{v}_i^T \\mathbf{u}_j)^2$。\n\n最大化问题是：\n$$\n\\max_{S: \\dim(S)=k} \\sum_{i=1}^{m} \\sum_{j=1}^{k} (\\mathbf{v}_i^T \\mathbf{u}_j)^2 = \\max_{\\{\\mathbf{u}_j\\} \\text{ orthonormal}} \\sum_{j=1}^{k} \\sum_{i=1}^{m} (\\mathbf{v}_i^T \\mathbf{u}_j)^2\n$$\n内层和可以看作是向量 $X \\mathbf{u}_j$ 的范数平方：\n$$\n\\sum_{i=1}^{m} (\\mathbf{v}_i^T \\mathbf{u}_j)^2 = \\left\\| X \\mathbf{u}_j \\right\\|_2^2 = (X \\mathbf{u}_j)^T (X \\mathbf{u}_j) = \\mathbf{u}_j^T X^T X \\mathbf{u}_j\n$$\n问题因此转化为最大化二次型之和：\n$$\n\\max_{\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\} \\text{ orthonormal}} \\sum_{j=1}^{k} \\mathbf{u}_j^T (X^T X) \\mathbf{u}_j\n$$\n矩阵 $A = X^T X \\in \\mathbb{R}^{n \\times n}$ 是散布矩阵（或未中心化的协方差矩阵）。它是对称和半正定的。根据 Courant-Fischer 定理（或扩展的 Rayleigh-Ritz 原理），当向量 $\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\}$ 是 $A$ 对应于其 $k$ 个最大特征值的标准正交特征向量时，该和达到最大值。设 $A$ 的特征值按 $\\lambda_1 \\ge \\lambda_2 \\ge \\ldots \\ge \\lambda_n \\ge 0$ 的顺序排列。投影平方和的最大值为 $\\sum_{j=1}^{k} \\lambda_j$。\n\n因此，最小误差 $E_{\\min}$ 为：\n$$\nE_{\\min} = \\left( \\sum_{i=1}^{m} \\left\\| \\mathbf{v}_i \\right\\|_2^2 \\right) - \\sum_{j=1}^{k} \\lambda_j\n$$\n总平方和可以与 $A$ 的特征值联系起来。它等于 $X$ 的 Frobenius 范数的平方，也即 $X^T X$ 的迹：\n$$\n\\sum_{i=1}^{m} \\left\\| \\mathbf{v}_i \\right\\|_2^2 = \\sum_{i=1}^{m} \\sum_{j=1}^{n} (X_{ij})^2 = \\|X\\|_F^2 = \\mathrm{Tr}(X^T X)\n$$\n一个矩阵的迹是其所有特征值之和。因此，$\\mathrm{Tr}(A) = \\sum_{j=1}^{n} \\lambda_j$。将此代入 $E_{\\min}$ 的表达式中：\n$$\nE_{\\min} = \\left( \\sum_{j=1}^{n} \\lambda_j \\right) - \\left( \\sum_{j=1}^{k} \\lambda_j \\right) = \\sum_{j=k+1}^{n} \\lambda_j\n$$\n最小总投影平方误差是散布矩阵 $X^T X$ 的 $n-k$ 个最小特征值之和。\n\n在数值计算方面，使用奇异值分解 (Singular Value Decomposition, SVD) 更为稳定。设数据矩阵 $X$ 的 SVD 为 $X = U \\Sigma V^T$。$X^T X$ 的特征值 $\\lambda_j$ 是 $X$ 的奇异值 $\\sigma_j$ 的平方。即 $\\lambda_j = \\sigma_j^2$。按照惯例，奇异值按降序排列：$\\sigma_1 \\ge \\sigma_2 \\ge \\ldots \\ge 0$。\n那么，最小误差由下式给出：\n$$\nE_{\\min} = \\sum_{j=k+1}^{n} \\sigma_j^2\n$$\n在计算上，我们构建矩阵 $X$，计算其奇异值 $\\sigma_j$，然后将从索引 $k$ 到末尾的奇异值的平方求和（对于奇异值数组使用从0开始的索引）。\n\n对于每个测试用例，我们执行以下步骤：\n1. 从向量集 $V$ 构建数据矩阵 $X \\in \\mathbb{R}^{m \\times n}$。\n2. 使用数值库计算 $X$ 的奇异值 $\\sigma_j$。设奇异值数组为 $s$。\n3. 所求的最小误差计算为 $s$ 中从索引 $k$ 开始的所有元素的平方和：$\\sum_{j=k}^{\\text{len}(s)-1} (s_j)^2$。\n\n让我们以用例 5 作为一个示例来说明这个过程：$V = \\{ (1,1), (1,0) \\} \\subset \\mathbb{R}^2$，其中 $k = 1$。\n数据矩阵为 $X = \\begin{pmatrix} 1  1 \\\\ 1  0 \\end{pmatrix}$。此处 $m=2, n=2$。\n散布矩阵为 $X^T X = \\begin{pmatrix} 1  1 \\\\ 1  0 \\end{pmatrix}^T \\begin{pmatrix} 1  1 \\\\ 1  0 \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  0 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 1  1 \\end{pmatrix}$。\n特征值由 $\\det(X^T X - \\lambda I) = (2-\\lambda)(1-\\lambda)-1 = \\lambda^2 - 3\\lambda + 1 = 0$ 给出。\n解为 $\\lambda = \\frac{3 \\pm \\sqrt{5}}{2}$。所以 $\\lambda_1 = \\frac{3+\\sqrt{5}}{2}$ 且 $\\lambda_2 = \\frac{3-\\sqrt{5}}{2}$。\n奇异值的平方为 $\\sigma_1^2 = \\lambda_1$ 和 $\\sigma_2^2 = \\lambda_2$。\n我们需要找到 $E_{\\min} = \\sum_{j=k+1}^{n} \\sigma_j^2 = \\sum_{j=2}^{2} \\sigma_j^2 = \\sigma_2^2$。\n$E_{\\min} = \\lambda_2 = \\frac{3-\\sqrt{5}}{2} \\approx 0.381966$。\n所有测试用例都遵循此过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite.\n    For each case, it computes the minimal total squared projection error\n    of a set of vectors onto a k-dimensional subspace.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: V = { (1,2), (2,4), (3,6) }, k = 1\n        {'V': np.array([[1, 2], [2, 4], [3, 6]]), 'k': 1},\n        # Case 2: V = { (1,0,0), (0,1,0), (0,0,1) }, k = 2\n        {'V': np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), 'k': 2},\n        # Case 3: V = { (1,0), (0,1), (0,0) }, k = 1\n        {'V': np.array([[1, 0], [0, 1], [0, 0]]), 'k': 1},\n        # Case 4: V = { (1,2), (-1,-2), (0,0) }, k = 0\n        {'V': np.array([[1, 2], [-1, -2], [0, 0]]), 'k': 0},\n        # Case 5: V = { (1,1), (1,0) }, k = 1\n        {'V': np.array([[1, 1], [1, 0]]), 'k': 1},\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case['V']\n        k = case['k']\n\n        # The problem is to find a k-dimensional subspace S that minimizes\n        # the sum of squared Euclidean distances from the given vectors to\n        # their orthogonal projections onto S.\n        # This is a classic result in linear algebra, related to Principal\n        # Component Analysis (PCA). The minimum error is the sum of the\n        # squares of the smallest (n-k) singular values of the data matrix X,\n        # where the vectors are rows of X.\n\n        # Compute the singular values of the data matrix X.\n        # np.linalg.svd returns singular values in descending order.\n        # We don't need the U and V* matrices, so compute_uv=False for efficiency.\n        singular_values = np.linalg.svd(X, compute_uv=False)\n\n        # The optimal k-dimensional subspace is spanned by the first k principal\n        # components (right singular vectors of X, or eigenvectors of X^T*X).\n        # The total projection error is the sum of the squared singular values\n        # that are NOT used for projection. These are the singular values\n        # from index k to the end of the list.\n        # Python's 0-based indexing means we slice from index k.\n        \n        # If k is greater or equal to the number of singular values,\n        # the slice s[k:] will be empty, and the sum will correctly be 0.\n        error_values = singular_values[k:]\n        min_squared_error = np.sum(error_values**2)\n        \n        results.append(min_squared_error)\n\n    # Format the results to six decimal places and join them into the specified string format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}