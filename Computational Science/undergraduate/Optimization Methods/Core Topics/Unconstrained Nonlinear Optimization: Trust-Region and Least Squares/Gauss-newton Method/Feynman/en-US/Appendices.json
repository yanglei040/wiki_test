{
    "hands_on_practices": [
        {
            "introduction": "The journey into the Gauss-Newton method begins with its fundamental goal: minimizing the sum of squared residuals. This first exercise  solidifies the concept of a residual, which measures the discrepancy between your model's prediction and the actual observed data. Calculating the residual vector is the essential first step in evaluating how well a given set of parameters fits the data.",
            "id": "2214281",
            "problem": "In the context of non-linear least squares fitting, a common task is to minimize the sum of the squares of residuals. Consider a model that describes a certain physical phenomenon, given by the function $f(x, \\beta) = \\beta_1 \\sqrt{x} + \\beta_2$, where $\\beta = (\\beta_1, \\beta_2)^T$ is the vector of parameters to be determined.\n\nAn experiment yields two data points $(x_i, y_i)$: the first point is $(4, 5)$ and the second point is $(9, 7)$.\n\nThe residual for the $i$-th data point is defined as $r_i(\\beta) = y_i - f(x_i, \\beta)$. The residual vector $r(\\beta)$ is a column vector whose components are the individual residuals $r_i(\\beta)$.\n\nGiven an initial estimate for the parameters $\\beta^{(0)} = (1, 3)^T$, calculate the corresponding residual vector $r(\\beta^{(0)})$. Present your final answer as a row matrix with two elements, corresponding to the components of the residual vector.",
            "solution": "We are given the model $f(x,\\beta)=\\beta_{1}\\sqrt{x}+\\beta_{2}$, residuals $r_{i}(\\beta)=y_{i}-f(x_{i},\\beta)$, data points $(x_{1},y_{1})=(4,5)$ and $(x_{2},y_{2})=(9,7)$, and the initial estimate $\\beta^{(0)}=(\\beta_{1}^{(0)},\\beta_{2}^{(0)})^{T}=(1,3)^{T}$.\n\nCompute the model predictions at the given $x_{i}$ using $\\beta^{(0)}$:\n$$\nf(x_{1},\\beta^{(0)})=\\beta_{1}^{(0)}\\sqrt{x_{1}}+\\beta_{2}^{(0)}=1\\cdot\\sqrt{4}+3=2+3=5,\n$$\n$$\nf(x_{2},\\beta^{(0)})=\\beta_{1}^{(0)}\\sqrt{x_{2}}+\\beta_{2}^{(0)}=1\\cdot\\sqrt{9}+3=3+3=6.\n$$\n\nCompute the residuals:\n$$\nr_{1}(\\beta^{(0)})=y_{1}-f(x_{1},\\beta^{(0)})=5-5=0,\n$$\n$$\nr_{2}(\\beta^{(0)})=y_{2}-f(x_{2},\\beta^{(0)})=7-6=1.\n$$\n\nThus the residual vector has components $0$ and $1$. Presented as a row matrix with two elements, it is $\\begin{pmatrix}0 & 1\\end{pmatrix}$.",
            "answer": "$$\\boxed{\\begin{pmatrix}0 & 1\\end{pmatrix}}$$"
        },
        {
            "introduction": "Having grasped how to calculate residuals, we now move to the heart of the algorithm: updating the parameters. This practice  guides you through a single, complete iteration of the Gauss-Newton method. You will compute the Jacobian, form the linear system known as the normal equations, and solve for the parameter update, providing a concrete look at how the method iteratively refines its solution.",
            "id": "2214282",
            "problem": "In an experimental study, a certain physical process is modeled by the function $y(x) = \\frac{x}{1+ax}$, where $a$ is an unknown parameter to be determined. A researcher has collected two data points $(x_i, y_i)$: the first point is $(1, 0.5)$ and the second point is $(2, 0.8)$.\n\nTo find the optimal value of the parameter $a$ that best fits the data in a least-squares sense, the researcher decides to use the Gauss-Newton method. Starting with an initial guess of $a_0 = 1$, perform exactly one iteration of the Gauss-Newton method to find the updated estimate for the parameter, denoted as $a_1$.\n\nExpress your answer for $a_1$ as an exact fraction in simplest form.",
            "solution": "We model the data by $y(x;a)=\\dfrac{x}{1+a x}$. For least squares with residuals $r_{i}(a)=y(x_{i};a)-y_{i}$, the Gauss-Newton update for a single parameter $a$ from $a_{0}$ is\n$$\n\\Delta a=-(J^{\\top}J)^{-1}J^{\\top}r,\n$$\nwhere $J_{i}=\\dfrac{\\partial r_{i}}{\\partial a}=\\dfrac{\\partial y(x_{i};a)}{\\partial a}$ evaluated at $a_{0}$, and $r$ is the residual vector evaluated at $a_{0}$. Then $a_{1}=a_{0}+\\Delta a$.\n\nFirst compute the derivative of the model with respect to $a$. Writing $y(x;a)=x(1+a x)^{-1}$, we obtain\n$$\n\\frac{\\partial y}{\\partial a}=-x^{2}(1+a x)^{-2}.\n$$\n\nWith data points $(x_{1},y_{1})=(1,\\tfrac{1}{2})$ and $(x_{2},y_{2})=(2,\\tfrac{4}{5})$, and initial guess $a_{0}=1$, the Jacobian entries are\n$$\nJ_{1}=\\left.-\\frac{x_{1}^{2}}{(1+a x_{1})^{2}}\\right|_{a=1}=-\\frac{1}{(1+1)^{2}}=-\\frac{1}{4},\\quad\nJ_{2}=\\left.-\\frac{x_{2}^{2}}{(1+a x_{2})^{2}}\\right|_{a=1}=-\\frac{4}{(1+2)^{2}}=-\\frac{4}{9}.\n$$\n\nThe residuals at $a_{0}=1$ are\n$$\nr_{1}=y(1;1)-\\frac{1}{2}=\\frac{1}{2}-\\frac{1}{2}=0,\\quad\nr_{2}=y(2;1)-\\frac{4}{5}=\\frac{2}{3}-\\frac{4}{5}=-\\frac{2}{15}.\n$$\n\nCompute the scalar quantities $J^{\\top}r$ and $J^{\\top}J$:\n$$\nJ^{\\top}r=J_{1}r_{1}+J_{2}r_{2}=0+\\left(-\\frac{4}{9}\\right)\\left(-\\frac{2}{15}\\right)=\\frac{8}{135},\n$$\n$$\nJ^{\\top}J=J_{1}^{2}+J_{2}^{2}=\\frac{1}{16}+\\frac{16}{81}=\\frac{81+256}{1296}=\\frac{337}{1296}.\n$$\n\nThus,\n$$\n\\Delta a=-\\frac{J^{\\top}r}{J^{\\top}J}=-\\frac{\\frac{8}{135}}{\\frac{337}{1296}}=-\\frac{8}{135}\\cdot\\frac{1296}{337}=-\\frac{8\\cdot 48}{5\\cdot 337}=-\\frac{384}{1685}.\n$$\n\nTherefore, the updated estimate is\n$$\na_{1}=a_{0}+\\Delta a=1-\\frac{384}{1685}=\\frac{1685-384}{1685}=\\frac{1301}{1685}.\n$$",
            "answer": "$$\\boxed{\\frac{1301}{1685}}$$"
        },
        {
            "introduction": "While powerful, the pure Gauss-Newton method can become unstable if the underlying linear subproblem is ill-conditioned. This advanced coding practice  demonstrates this instability and introduces a crucial fix: the Levenberg-Marquardt method. By implementing both algorithms, you will see firsthand how a \"damping\" parameter $\\lambda$ can prevent wild oscillations and ensure the algorithm converges reliably, a vital concept for building robust optimization routines.",
            "id": "3232802",
            "problem": "You are to write a complete, runnable program that analyzes the Gauss-Newton method and its Levenberg-Marquardt (LM) stabilization for a one-parameter nonlinear least-squares problem. The goal is to demonstrate, through a principled derivation and a concrete implementation, that undamped Gauss-Newton steps can oscillate or diverge when the Jacobian is nearly singular, and that adding a Levenberg-Marquardt damping term stabilizes the iterations.\n\nBegin from the following fundamental base:\n- The nonlinear least-squares objective is $F(x) = \\tfrac{1}{2}\\lVert r(x)\\rVert_2^2$, where $r(x)$ is a vector of residual functions.\n- The first-order Taylor linearization at $x$ is $r(x + \\Delta) \\approx r(x) + J(x)\\Delta$, where $J(x)$ is the Jacobian of $r(x)$.\n- Minimizing the linearized model yields the normal equations $J(x)^\\top J(x)\\Delta = -J(x)^\\top r(x)$.\n- The Levenberg-Marquardt (LM) method augments the normal matrix with a damping term $\\lambda I$ (where $I$ is the identity), solving $\\big(J(x)^\\top J(x) + \\lambda I\\big)\\Delta = -J(x)^\\top r(x)$ with $\\lambda > 0$.\n\nYour program must instantiate a one-dimensional residual model $r(x) = \\sin(x)$, with angles in radians, and perform both Gauss-Newton (undamped, corresponding to $\\lambda = 0$) and Levenberg-Marquardt (damped, with a specified $\\lambda > 0$) iterations. The residual model implies the least-squares objective $F(x) = \\tfrac{1}{2}\\sin^2(x)$. The Jacobian $J(x)$ is a scalar in this case. You must:\n- Derive from the base definitions the Gauss-Newton and LM update rules specialized to the one-dimensional residual $r(x) = \\sin(x)$.\n- Implement fixed-iteration solvers for both methods using the derived updates.\n- Demonstrate that near points where $J(x)$ is nearly singular (specifically near $x \\approx \\tfrac{\\pi}{2} + k\\pi$, where $k$ is an integer), the undamped Gauss-Newton iteration exhibits oscillation or divergence (large alternating steps or steps that exit a bounded domain), while the LM method with a positive damping parameter stabilizes the iterations by bounding the step size.\n\nAngles must be in radians. No physical units are involved. Your algorithm must not use any line search or trust-region adaptations beyond the constant LM damping.\n\nTest Suite and Output Specification:\n- Use the following test cases, each specified as a tuple $(x_0, \\lambda, N, L)$:\n  1. A near-singular Jacobian case to provoke instability: $x_0 = \\tfrac{\\pi}{2} - 10^{-6}$, $\\lambda = 1.0$, $N = 10$, $L = 50$.\n  2. An oscillatory-but-recoverable case: $x_0 = 1.4$, $\\lambda = 0.5$, $N = 10$, $L = 50$.\n  3. A happy-path case away from singularities: $x_0 = 2.0$, $\\lambda = 0.1$, $N = 10$, $L = 50$.\n- For each case, run $N$ iterations of undamped Gauss-Newton (i.e., set $\\lambda = 0$ for Gauss-Newton) and $N$ iterations of LM with the given $\\lambda$. For each method in each case, compute:\n  - The final residual norm $|r(x_N)|$ as a float.\n  - A stability flag as a boolean that is true if all iterates $x_k$ satisfy $|x_k| \\le L$, and false otherwise.\n  - The maximum step magnitude $\\max_k |\\Delta_k|$ as a float, where $\\Delta_k$ is the update at iteration $k$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The results must be structured as a list of lists, one inner list per test case, each inner list containing six entries in the following order: $[\\text{gn\\_final\\_residual}, \\text{lm\\_final\\_residual}, \\text{gn\\_stable}, \\text{lm\\_stable}, \\text{gn\\_max\\_step}, \\text{lm\\_max\\_step}]$. For example, the output format must be like $[[r_{11}, r_{12}, b_{11}, b_{12}, s_{11}, s_{12}], [r_{21}, r_{22}, b_{21}, b_{22}, s_{21}, s_{22}], [r_{31}, r_{32}, b_{31}, b_{32}, s_{31}, s_{32}]]$ where $r_{ij}$ are floats and $b_{ij}$ are booleans.\n\nScientific realism and coverage:\n- The near-singular case enforces a boundary condition where the undamped method encounters a Jacobian close to zero, which can cause large steps. The LM damping term $\\lambda I$ must mitigate this instability.\n- The oscillatory case exhibits large alternating steps for undamped Gauss-Newton that are reduced under LM damping.\n- The happy-path case demonstrates convergence for both methods.\n- Angles are in radians.\n- All outputs must be basic types as specified (booleans and floats).",
            "solution": "The problem requires a validation and implementation of the Gauss-Newton (GN) method and its Levenberg-Marquardt (LM) stabilization for a specific one-dimensional nonlinear least-squares problem. The objective is to demonstrate how LM stabilization corrects the instabilities inherent in the undamped Gauss-Newton method when the Jacobian matrix is nearly singular.\n\nFirst, we establish the theoretical foundation. The general nonlinear least-squares problem aims to minimize an objective function $F(x)$ defined as half the squared Euclidean norm of a residual vector $r(x)$:\n$$\nF(x) = \\frac{1}{2}\\lVert r(x)\\rVert_2^2\n$$\nThe Gauss-Newton method approximates the objective function at each iteration by linearizing the residual function $r(x)$ around the current iterate $x_k$. The update step $\\Delta$ is found by solving the linearized least-squares problem:\n$$\n\\min_{\\Delta} \\frac{1}{2}\\lVert r(x_k) + J(x_k)\\Delta\\rVert_2^2\n$$\nwhere $J(x_k)$ is the Jacobian of $r(x)$ at $x_k$. The solution to this linear problem is given by the normal equations:\n$$\nJ(x_k)^\\top J(x_k)\\Delta_{GN} = -J(x_k)^\\top r(x_k)\n$$\nThe Levenberg-Marquardt method introduces a damping parameter $\\lambda > 0$ to regularize the problem, which is particularly useful when the matrix $J(x_k)^\\top J(x_k)$ is singular or ill-conditioned. The LM update step $\\Delta_{LM}$ is found by solving the modified normal equations:\n$$\n\\left(J(x_k)^\\top J(x_k) + \\lambda I\\right)\\Delta_{LM} = -J(x_k)^\\top r(x_k)\n$$\nwhere $I$ is the identity matrix.\n\nNow, we specialize these general formulas to the given one-dimensional problem, where the residual is a scalar function $r(x) = \\sin(x)$.\nThe objective function becomes:\n$$\nF(x) = \\frac{1}{2}(r(x))^2 = \\frac{1}{2}\\sin^2(x)\n$$\nThe minima of $F(x)$ occur where $\\sin(x) = 0$, i.e., at $x = n\\pi$ for any integer $n$.\nIn this one-dimensional case, the Jacobian $J(x)$ is a $1 \\times 1$ matrix (a scalar) corresponding to the first derivative of $r(x)$:\n$$\nJ(x) = \\frac{dr}{dx} = \\frac{d}{dx}(\\sin(x)) = \\cos(x)\n$$\nThe Jacobian is (nearly) singular when $J(x) = \\cos(x) \\approx 0$, which occurs for $x \\approx \\frac{\\pi}{2} + k\\pi$ for any integer $k$.\n\nWe derive the specific update rules for both methods.\nFor Gauss-Newton (undamped, equivalent to $\\lambda=0$), the normal equation is:\n$$\n(\\cos(x_k))^\\top (\\cos(x_k))\\Delta_{GN} = -(\\cos(x_k))^\\top (\\sin(x_k))\n$$\n$$\n\\cos^2(x_k) \\Delta_{GN} = -\\sin(x_k)\\cos(x_k)\n$$\nAssuming $\\cos(x_k) \\neq 0$, we can solve for the step $\\Delta_{GN}$:\n$$\n\\Delta_{GN} = -\\frac{\\sin(x_k)\\cos(x_k)}{\\cos^2(x_k)} = -\\frac{\\sin(x_k)}{\\cos(x_k)} = -\\tan(x_k)\n$$\nThe Gauss-Newton update rule is therefore:\n$$\nx_{k+1} = x_k + \\Delta_{GN} = x_k - \\tan(x_k)\n$$\nThe instability of this method becomes evident when $x_k$ approaches a point where the Jacobian is singular, i.e., $x_k \\to \\frac{\\pi}{2} + k\\pi$. At these points, $\\cos(x_k) \\to 0$, causing $|\\tan(x_k)| \\to \\infty$. The update step $\\Delta_{GN}$ becomes arbitrarily large, leading to divergence or violent oscillations.\n\nFor Levenberg-Marquardt, the modified normal equation includes the damping term $\\lambda > 0$. In this $1D$ case, the identity matrix $I$ is the scalar $1$:\n$$\n(\\cos^2(x_k) + \\lambda)\\Delta_{LM} = -\\sin(x_k)\\cos(x_k)\n$$\nSince $\\cos^2(x_k) \\ge 0$ and $\\lambda > 0$, the term $(\\cos^2(x_k) + \\lambda)$ is always positive and bounded away from zero by $\\lambda$. Thus, the system is always well-conditioned. The LM update step is:\n$$\n\\Delta_{LM} = -\\frac{\\sin(x_k)\\cos(x_k)}{\\cos^2(x_k) + \\lambda}\n$$\nThe Levenberg-Marquardt update rule is:\n$$\nx_{k+1} = x_k + \\Delta_{LM} = x_k - \\frac{\\sin(x_k)\\cos(x_k)}{\\cos^2(x_k) + \\lambda}\n$$\nNow consider the behavior near a singularity, where $\\cos(x_k) \\to 0$. In this limit, the numerator $\\sin(x_k)\\cos(x_k)$ also approaches $0$. The denominator approaches $\\lambda$. Therefore, the step $\\Delta_{LM}$ approaches $0$. The damping term effectively bounds the step size, preventing the catastrophic behavior observed in the undamped Gauss-Newton method. This ensures stability and promotes convergence even when starting near a region of Jacobian singularity.\n\nThe implementation will proceed by iterating these two update rules for the given test cases to numerically demonstrate this derived behavior. The results will quantify the final residual, the stability of the iteration sequence within a given bound, and the magnitude of the steps taken.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes Gauss-Newton and Levenberg-Marquardt methods for a 1D\n    nonlinear least-squares problem, demonstrating LM stabilization.\n    \"\"\"\n\n    # Test cases as tuples of (x0, lambda, N, L)\n    test_cases = [\n        (np.pi / 2.0 - 1e-6, 1.0, 10, 50.0), # Near-singular Jacobian case\n        (1.4, 0.5, 10, 50.0),               # Oscillatory-but-recoverable case\n        (2.0, 0.1, 10, 50.0)                # Happy-path case\n    ]\n\n    results = []\n\n    def run_iterations(x0, N, L, damp_lambda):\n        \"\"\"\n        Performs N iterations of a solver for the objective F(x) = 0.5*sin^2(x).\n\n        Args:\n            x0 (float): Initial guess.\n            N (int): Number of iterations.\n            L (float): Stability bound for |x_k|.\n            damp_lambda (float): Damping parameter. If 0, use Gauss-Newton.\n                                 If > 0, use Levenberg-Marquardt.\n\n        Returns:\n            A tuple containing:\n            - final_residual (float): |r(x_N)|.\n            - is_stable (bool): True if all |x_k| <= L.\n            - max_step (float): Maximum magnitude of any step delta_k.\n        \"\"\"\n        x = x0\n        stable = True\n        max_step_mag = 0.0\n\n        for _ in range(N):\n            # The Jacobian is singular when cos(x) is zero.\n            # Avoid division by zero for the pure Gauss-Newton case if x is exactly pi/2 + k*pi.\n            # np.tan will handle large values gracefully, returning inf.\n            cos_x = np.cos(x)\n            sin_x = np.sin(x)\n\n            if damp_lambda == 0:  # Gauss-Newton\n                # Delta = -tan(x)\n                # Avoid explicit division by zero if cos_x is extremely small.\n                # np.tan handles this by returning large numbers or inf.\n                delta = -np.tan(x)\n\n            else:  # Levenberg-Marquardt\n                # Delta = -sin(x)cos(x) / (cos^2(x) + lambda)\n                numerator = -sin_x * cos_x\n                denominator = cos_x**2 + damp_lambda\n                delta = numerator / denominator\n\n            if np.isinf(delta) or np.isnan(delta):\n                # If step is infinite/NaN, it's definitively unstable and huge.\n                # To assign a finite but large value for max_step.\n                # Using 2*L is arbitrary but indicates a large step.\n                current_step_mag = 2 * L \n                x = x + (2 * L * np.sign(-delta) if not np.isnan(delta) else 0)\n            else:\n                current_step_mag = abs(delta)\n\n            if current_step_mag > max_step_mag:\n                max_step_mag = current_step_mag\n\n            x = x + delta\n            \n            if abs(x) > L:\n                stable = False\n        \n        final_residual = abs(np.sin(x))\n        \n        return final_residual, stable, max_step_mag\n\n    for x0, lm_lambda, N, L in test_cases:\n        # Run Gauss-Newton (undamped, lambda = 0)\n        gn_final_residual, gn_stable, gn_max_step = run_iterations(x0, N, L, 0)\n\n        # Run Levenberg-Marquardt (damped)\n        lm_final_residual, lm_stable, lm_max_step = run_iterations(x0, N, L, lm_lambda)\n\n        case_results = [\n            gn_final_residual,\n            lm_final_residual,\n            gn_stable,\n            lm_stable,\n            gn_max_step,\n            lm_max_step,\n        ]\n        results.append(case_results)\n\n    # Format the final output string exactly as specified.\n    # The default str() representation for lists includes spaces, which is acceptable.\n    # The boolean values will be represented as 'True' and 'False'.\n    output_str = str(results).replace(\"], [\", \"], \\n [\") # for better readability in some contexts\n    # The problem asks for comma-separated list, so default str is fine.\n    # I'll use a more compact representation to be safe\n    output_str = \"[\" + \", \".join(str(r) for r in results) + \"]\"\n    \n    print(output_str)\n\nsolve()\n```"
        }
    ]
}