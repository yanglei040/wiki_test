{
    "hands_on_practices": [
        {
            "introduction": "The success of any trust-region method hinges on a simple but critical assumption: that the quadratic model is a reasonably faithful approximation of the true objective function within a small neighborhood. This first practice invites you to investigate this core principle directly. By computing the difference between the actual reduction in a function and the reduction predicted by its model, you will quantify the model's fidelity . Through a controlled experiment on a family of functions with increasing nonconvexity, you will develop a concrete intuition for how local curvature impacts model accuracy and why the trust region is a necessary safeguard.",
            "id": "3193707",
            "problem": "Consider a one-dimensional trust-region method for unconstrained minimization. At an iterate $x_k$, a twice continuously differentiable function $f$ is approximated by its second-order Taylor polynomial, called the quadratic model, over a trust-region defined by a radius $\\Delta$. The quadratic model $m(s)$ about $x_k$ is constructed using the gradient $g$ and the Hessian $B$ of $f$ at $x_k$. The trust-region subproblem seeks a step $s$ that reduces the model subject to a bound on the step length. The actual reduction is measured by the change of the true function value, while the predicted reduction is measured by the decrease in the quadratic model. The discrepancy between actual and predicted reductions quantifies how well the quadratic model captures the true function behavior within the trust-region.\n\nStarting from these fundamental definitions, consider the following specific setup:\n\n- The family of functions is a parameterized quartic polynomial given by $f_{\\alpha}(x) = \\frac{1}{4}x^4 - \\frac{\\alpha}{2} x^2 + \\beta x$, where $\\alpha$ controls nonconvexity and $\\beta$ is a fixed small linear perturbation.\n- Use the fixed perturbation $\\beta = 0.1$.\n- Use the fixed iterate $x_k = 0.5$.\n- In one dimension, the quadratic model at $x_k$ can be expressed as $m(s) = f_{\\alpha}(x_k) + g_{\\alpha} s + \\frac{1}{2} B_{\\alpha} s^2$, where $g_{\\alpha} = f_{\\alpha}'(x_k)$ and $B_{\\alpha} = f_{\\alpha}''(x_k)$.\n- Define the trust-region subproblem in one dimension as minimizing $m(s)$ subject to the constraint $|s| \\le \\Delta$.\n\nFor the one-dimensional case, the steepest descent direction is the negative gradient direction. The step used in this problem should be the minimizer of $m(s)$ along the steepest descent direction subject to the trust-region bound, namely the step $s(\\Delta)$ on the ray in the direction $-g_{\\alpha}$ that minimizes $m(s)$ while satisfying $|s| \\le \\Delta$.\n\nYour tasks are:\n\n- For each choice of $\\alpha$, construct $f_{\\alpha}$ and compute the gradient $g_{\\alpha}$ and Hessian $B_{\\alpha}$ at $x_k$.\n- Determine the step $s(\\Delta)$ that minimizes the quadratic model $m(s)$ along the steepest descent direction with the constraint $|s| \\le \\Delta$.\n- Compute the actual reduction $AR(\\Delta) = f_{\\alpha}(x_k) - f_{\\alpha}(x_k + s(\\Delta))$ and the predicted reduction $PR(\\Delta) = m(0) - m(s(\\Delta))$.\n- Compute the difference $D(\\Delta) = AR(\\Delta) - PR(\\Delta)$ for each test case.\n\nTest Suite Specification:\n\n- Use the parameter set of nonconvexity levels $\\alpha \\in \\{0.0, 0.5, 0.75, 1.0, 2.0\\}$.\n- Use the trust-region radii $\\Delta \\in \\{0.01, 0.1, 0.3, 0.5\\}$.\n- The fixed iterate is $x_k = 0.5$.\n- The fixed perturbation is $\\beta = 0.1$.\n\nDesign for Coverage:\n\n- The set of $\\alpha$ values probes regimes from convex near $x_k$ to strongly nonconvex near $x_k$.\n- The set of $\\Delta$ values probes small radii (model locally accurate), boundary-limited steps, and moderately large radii.\n- Include the borderline case where the curvature at $x_k$ is zero for $\\alpha = 0.75$.\n\nFinal Output Specification:\n\n- Your program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets. Each inner list corresponds to one value of $\\alpha$ in ascending order, and contains the values of $D(\\Delta)$ for $\\Delta$ in ascending order. For example, the output format should look like $[[d_{0,0},d_{0,1},d_{0,2},d_{0,3}],[d_{1,0},\\dots],\\dots]$, where $d_{i,j}$ is the difference for the $i$-th $\\alpha$ and $j$-th $\\Delta$.\n- Each $d_{i,j}$ must be a real number (a float), expressed without any physical units, and printed directly as a numeric literal.\n\nNo user input is required; the program must be self-contained and compute the specified values using the given test suite. The angle unit is not applicable. No physical units or percentages are involved in this problem. Ensure scientific consistency and use only the definitions and well-tested formulas that follow from the second-order Taylor model within trust-region methods.",
            "solution": "The user has requested a solution to a problem concerning trust-region optimization methods. The task involves calculating the difference between the actual and predicted reduction of a model function for a given family of one-dimensional quartic polynomials.\n\n### Step 1: Preliminary Analysis and Formulation\n\nThe problem is centered around the function $f_{\\alpha}(x) = \\frac{1}{4}x^4 - \\frac{\\alpha}{2} x^2 + \\beta x$ with fixed parameters $x_k=0.5$ and $\\beta=0.1$. The core of the problem is to determine the step $s(\\Delta)$ that minimizes a quadratic model $m(s)$ and then use this step to compute the quantity $D(\\Delta) = AR(\\Delta) - PR(\\Delta)$.\n\nFirst, we establish the key components of the quadratic model, $m(s) = f_{\\alpha}(x_k) + g_{\\alpha} s + \\frac{1}{2} B_{\\alpha} s^2$. The gradient $g_{\\alpha}$ and Hessian $B_{\\alpha}$ at the iterate $x_k$ are the first and second derivatives of $f_{\\alpha}(x)$ evaluated at $x_k$.\n\nThe derivatives of $f_{\\alpha}(x)$ are:\n$f'_{\\alpha}(x) = x^3 - \\alpha x + \\beta$\n$f''_{\\alpha}(x) = 3x^2 - \\alpha$\n$f'''_{\\alpha}(x) = 6x$\n$f''''_{\\alpha}(x) = 6$\n\nEvaluating these at the fixed iterate $x_k = 0.5$ with $\\beta = 0.1$:\nThe gradient is $g_{\\alpha} = f'_{\\alpha}(0.5) = (0.5)^3 - \\alpha(0.5) + 0.1 = 0.125 - 0.5\\alpha + 0.1 = 0.225 - 0.5\\alpha$.\nThe Hessian (or curvature in 1D) is $B_{\\alpha} = f''_{\\alpha}(0.5) = 3(0.5)^2 - \\alpha = 0.75 - \\alpha$.\n\n### Step 2: Simplifying the Target Quantity $D(\\Delta)$\n\nThe target quantity is the difference between the actual reduction, $AR(\\Delta)$, and the predicted reduction, $PR(\\Delta)$.\n$AR(\\Delta) = f_{\\alpha}(x_k) - f_{\\alpha}(x_k + s(\\Delta))$\n$PR(\\Delta) = m(0) - m(s(\\Delta)) = f_{\\alpha}(x_k) - m(s(\\Delta))$\n\nTherefore, the difference is:\n$D(\\Delta) = AR(\\Delta) - PR(\\Delta) = (f_{\\alpha}(x_k) - f_{\\alpha}(x_k + s(\\Delta))) - (f_{\\alpha}(x_k) - m(s(\\Delta)))$\n$D(\\Delta) = m(s(\\Delta)) - f_{\\alpha}(x_k + s(\\Delta))$\n\nThis quantity represents the error between the quadratic model and the true function at the point $x_k + s(\\Delta)$. We can express this error using the higher-order terms of the Taylor expansion of $f_{\\alpha}$ around $x_k$. Since $f_{\\alpha}$ is a quartic polynomial, its Taylor series is finite and exact:\n$f_{\\alpha}(x_k+s) = f_{\\alpha}(x_k) + f'_{\\alpha}(x_k)s + \\frac{1}{2}f''_{\\alpha}(x_k)s^2 + \\frac{1}{6}f'''_{\\alpha}(x_k)s^3 + \\frac{1}{24}f''''_{\\alpha}(x_k)s^4$\nBy definition, $m(s) = f_{\\alpha}(x_k) + f'_{\\alpha}(x_k)s + \\frac{1}{2}f''_{\\alpha}(x_k)s^2$.\nSo, $f_{\\alpha}(x_k+s) = m(s) + \\frac{1}{6}f'''_{\\alpha}(x_k)s^3 + \\frac{1}{24}f''''_{\\alpha}(x_k)s^4$.\n\nSubstituting $f'''_{\\alpha}(0.5) = 6(0.5) = 3$ and $f''''_{\\alpha}(0.5) = 6$:\n$f_{\\alpha}(0.5+s) = m(s) + \\frac{1}{6}(3)s^3 + \\frac{1}{24}(6)s^4 = m(s) + \\frac{1}{2}s^3 + \\frac{1}{4}s^4$.\n\nThus, for any step $s$, the difference is:\n$m(s) - f_{\\alpha}(0.5+s) = -(\\frac{1}{2}s^3 + \\frac{1}{4}s^4)$.\nThis means $D(\\Delta) = -\\frac{1}{2} s(\\Delta)^3 - \\frac{1}{4} s(\\Delta)^4$, which significantly simplifies the calculation. The problem reduces to finding the step $s(\\Delta)$.\n\n### Step 3: Determining the Step $s(\\Delta)$\n\nThe step $s(\\Delta)$ is defined as the minimizer of the model $m(s)$ along the steepest descent direction, $-g_{\\alpha}$, subject to the trust-region constraint $|s| \\le \\Delta$.\nLet the step be of the form $s = \\tau \\cdot \\operatorname{dir}$, where $\\tau \\ge 0$ is the step length and $\\operatorname{dir} = -\\frac{g_{\\alpha}}{|g_{\\alpha}|} = -\\operatorname{sgn}(g_{\\alpha})$ is the unit direction. The constraint becomes $\\tau \\le \\Delta$.\n\nWe substitute $s = -\\tau \\operatorname{sgn}(g_{\\alpha})$ into the model. Since $g_{\\alpha} s = -\\tau |g_{\\alpha}|$, the model becomes a function of $\\tau$:\n$m(\\tau) = f_{\\alpha}(x_k) - |g_{\\alpha}|\\tau + \\frac{1}{2} B_{\\alpha} \\tau^2$.\n\nWe need to minimize this quadratic function of $\\tau$ over the interval $[0, \\Delta]$. We analyze two cases based on the sign of the curvature $B_{\\alpha}$.\n\n**Case 1: Convex Model ($B_{\\alpha} > 0$)**\nThe model is a convex parabola opening upward. The unconstrained minimum is found by setting the derivative with respect to $\\tau$ to zero:\n$m'(\\tau) = -|g_{\\alpha}| + B_{\\alpha}\\tau = 0 \\implies \\tau_{unc} = \\frac{|g_{\\alpha}|}{B_{\\alpha}}$.\nThe minimizer within the trust region is found by projecting this point onto the feasible interval $[0, \\Delta]$:\n$\\tau_{opt} = \\min(\\tau_{unc}, \\Delta) = \\min\\left(\\frac{|g_{\\alpha}|}{B_{\\alpha}}, \\Delta\\right)$.\n\n**Case 2: Concave or Linear Model ($B_{\\alpha} \\le 0$)**\nThe function $m(\\tau)$ is either a downward-opening parabola or a line with a negative slope (since $g_{\\alpha}$ is non-zero for all test cases). In either scenario, $m(\\tau)$ is a decreasing function on $[0, \\infty)$. To minimize $m(\\tau)$ on $[0, \\Delta]$, we must choose the largest possible value for $\\tau$.\n$\\tau_{opt} = \\Delta$.\n\nOnce $\\tau_{opt}$ is determined, the step is calculated as $s(\\Delta) = \\tau_{opt} \\cdot (-\\operatorname{sgn}(g_{\\alpha}))$.\n\n### Step 4: Final Calculation Algorithm\n\nFor each pair of $(\\alpha, \\Delta)$ from the specified test suites, we perform the following calculations:\n1.  Compute $g_{\\alpha} = 0.225 - 0.5\\alpha$.\n2.  Compute $B_{\\alpha} = 0.75 - \\alpha$.\n3.  Determine the optimal step length $\\tau_{opt}$:\n    - If $B_{\\alpha} > 0$, calculate $\\tau_{opt} = \\min(|g_{\\alpha}|/B_{\\alpha}, \\Delta)$.\n    - If $B_{\\alpha} \\le 0$, set $\\tau_{opt} = \\Delta$.\n4.  Compute the step $s(\\Delta) = \\tau_{opt} \\cdot (-\\operatorname{sgn}(g_{\\alpha}))$. Note that for $g_{\\alpha}=0$, this would correctly yield $s=0$.\n5.  Compute the difference $D(\\Delta) = -0.5 s(\\Delta)^3 - 0.25 s(\\Delta)^4$.\n6.  Collect the results $D(\\Delta)$ into a list for the current $\\alpha$.\n7.  After iterating through all $\\Delta$ values, append this list to the final list of lists.\nThis procedure is then implemented for all specified $\\alpha$ values to generate the final output.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the trust-region subproblem for a parameterized quartic polynomial\n    and computes the difference between actual and predicted reduction.\n    \"\"\"\n    \n    # Test Suite Specification\n    alphas = [0.0, 0.5, 0.75, 1.0, 2.0]\n    deltas = [0.01, 0.1, 0.3, 0.5]\n    \n    # Fixed parameters\n    x_k = 0.5\n    beta = 0.1\n    \n    # This list will hold lists of results, one for each alpha.\n    all_results = []\n    \n    for alpha in alphas:\n        # This list will hold the results for the current alpha over all deltas.\n        results_for_alpha = []\n        \n        # Step 1: Compute gradient g_alpha and Hessian B_alpha at x_k\n        # g_alpha = f'(x_k) = x_k^3 - alpha*x_k + beta\n        g_alpha = x_k**3 - alpha * x_k + beta\n        # B_alpha = f''(x_k) = 3*x_k^2 - alpha\n        B_alpha = 3 * x_k**2 - alpha\n        \n        for delta in deltas:\n            # Step 2: Determine the optimal step s(delta)\n            \n            # The minimization is along the steepest descent direction -g_alpha.\n            # We solve for the step length tau_opt along this direction.\n            # The model along this direction is m(tau) = c - |g_alpha|*tau + 0.5*B_alpha*tau^2.\n            \n            tau_opt = 0.0\n\n            if B_alpha > 0:\n                # Convex case: The unconstrained minimizer is at tau_unc = |g_alpha| / B_alpha.\n                # The constrained solution is the minimum of this and the trust-region radius.\n                if g_alpha != 0:\n                    tau_unc = abs(g_alpha) / B_alpha\n                    tau_opt = min(tau_unc, delta)\n                else:\n                    # If gradient is zero, step is zero.\n                    tau_opt = 0.0\n\n            else: # B_alpha <= 0\n                # Concave or linear case: The model decreases along the ray.\n                # The minimum is at the trust-region boundary.\n                tau_opt = delta\n                \n            # Compute the step vector s. The direction is -g_alpha.\n            # The sign of the step is -sign(g_alpha).\n            # np.sign(0) is 0, correctly handling the g_alpha=0 case.\n            step_s = tau_opt * (-np.sign(g_alpha))\n            \n            # Step 3: Compute the difference D(delta) = AR - PR = m(s) - f(x_k + s)\n            # As derived, D(delta) = -0.5 * s^3 - 0.25 * s^4.\n            diff_D = -0.5 * step_s**3 - 0.25 * step_s**4\n            results_for_alpha.append(diff_D)\n            \n        all_results.append(results_for_alpha)\n\n    # Final Output Specification\n    # Format: [[d_0,0, d_0,1, ...], [d_1,0, ...], ...]\n    # Using a list comprehension to format numbers without trailing '.0'\n    formatted_results = [\n        \"[\" + \",\".join(map(str, inner_list)) + \"]\"\n        for inner_list in all_results\n    ]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once we have a quadratic model, the central task in a trust-region method is to find the step $s$ that minimizes this model while remaining within the bounds of the trust region. This exercise guides you through the exact solution of this subproblem using the foundational Karush-Kuhn-Tucker (KKT) conditions for constrained optimization. You will derive the celebrated \"secular equation,\" which provides an elegant relationship between the trust-region radius $\\Delta$ and the Lagrange multiplier $\\lambda$ that enforces the constraint . Mastering this derivation provides a deep understanding of how the optimal step is determined when it lies on the boundary of the trust region, a common and crucial scenario in practice.",
            "id": "3193674",
            "problem": "Let $m(\\mathbf{s}) = g^{\\top} \\mathbf{s} + \\frac{1}{2} \\mathbf{s}^{\\top} Q \\mathbf{s}$ be the quadratic model for a twice continuously differentiable objective function at a given iterate, where $\\mathbf{s} \\in \\mathbb{R}^{2}$ is the step, $g \\in \\mathbb{R}^{2}$ is the gradient vector, and $Q \\in \\mathbb{R}^{2 \\times 2}$ is a symmetric positive definite matrix. The trust-region subproblem seeks the step that minimizes $m(\\mathbf{s})$ subject to the Euclidean norm constraint $\\|\\mathbf{s}\\| \\leq \\Delta$, where $\\Delta > 0$ is the trust-region radius.\n\nConsider the specific case where $Q = 3 I_{2}$ and $g = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$, with $I_{2}$ the $2 \\times 2$ identity matrix. Treat $\\Delta$ as a positive parameter, and assume that the minimizer lies on the boundary of the trust region for sufficiently small $\\Delta$.\n\nStarting solely from the first-order optimality conditions for constrained minimization (Karush-Kuhn-Tucker (KKT) conditions), derive the secular equation that determines the Lagrange multiplier $\\lambda \\geq 0$ associated with the trust-region constraint, solve this equation exactly for $\\lambda$ in terms of $\\Delta$, and then obtain the exact trust-region step $\\mathbf{s}^{\\star}(\\Delta)$. Finally, determine the sensitivity of the Lagrange multiplier with respect to the trust-region radius by computing $\\frac{d\\lambda}{d\\Delta}$ evaluated at $\\Delta = 1$.\n\nYour final answer must be the single real number value of $\\frac{d\\lambda}{d\\Delta}$ at $\\Delta = 1$. No rounding is required.",
            "solution": "The problem is to find the minimizer of a quadratic model $m(\\mathbf{s})$ subject to a trust-region constraint. The optimization problem is stated as:\n$$ \\min_{\\mathbf{s} \\in \\mathbb{R}^{2}} m(\\mathbf{s}) = g^{\\top} \\mathbf{s} + \\frac{1}{2} \\mathbf{s}^{\\top} Q \\mathbf{s} $$\n$$ \\text{subject to} \\quad \\|\\mathbf{s}\\| \\leq \\Delta $$\nwhere the Euclidean norm is used. We are given the specific values $Q = 3 I_{2}$ and $g = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$. The problem also specifies that the solution, denoted $\\mathbf{s}^{\\star}$, lies on the boundary of the feasible set, meaning $\\|\\mathbf{s}^{\\star}\\| = \\Delta$.\n\nTo solve this constrained optimization problem, we utilize the Karush-Kuhn-Tucker (KKT) conditions. The constraint can be written as $c(\\mathbf{s}) = \\frac{1}{2}(\\mathbf{s}^{\\top}\\mathbf{s} - \\Delta^2) \\leq 0$. The Lagrangian function $\\mathcal{L}(\\mathbf{s}, \\lambda)$ is:\n$$ \\mathcal{L}(\\mathbf{s}, \\lambda) = m(\\mathbf{s}) + \\lambda c(\\mathbf{s}) = g^{\\top} \\mathbf{s} + \\frac{1}{2} \\mathbf{s}^{\\top} Q \\mathbf{s} + \\frac{\\lambda}{2} (\\mathbf{s}^{\\top}\\mathbf{s} - \\Delta^2) $$\nwhere $\\lambda$ is the Lagrange multiplier associated with the norm constraint.\n\nThe first-order necessary conditions for optimality (KKT conditions) are:\n1.  **Stationarity:** The gradient of the Lagrangian with respect to $\\mathbf{s}$ must be the zero vector: $\\nabla_{\\mathbf{s}} \\mathcal{L}(\\mathbf{s}^{\\star}, \\lambda) = \\mathbf{0}$.\n2.  **Primal Feasibility:** The solution must satisfy the constraint: $(\\mathbf{s}^{\\star})^{\\top}\\mathbf{s}^{\\star} - \\Delta^2 \\leq 0$.\n3.  **Dual Feasibility:** The Lagrange multiplier must be non-negative: $\\lambda \\geq 0$.\n4.  **Complementary Slackness:** $\\lambda ((\\mathbf{s}^{\\star})^{\\top}\\mathbf{s}^{\\star} - \\Delta^2) = 0$.\n\nFirst, we compute the gradient of the Lagrangian:\n$$ \\nabla_{\\mathbf{s}} \\mathcal{L}(\\mathbf{s}, \\lambda) = g + Q\\mathbf{s} + \\lambda\\mathbf{s} = (Q + \\lambda I)\\mathbf{s} + g $$\nThe stationarity condition is therefore:\n$$ (Q + \\lambda I)\\mathbf{s}^{\\star} = -g $$\n\nWe are given that the solution lies on the boundary, so $\\|\\mathbf{s}^{\\star}\\| = \\Delta$. This implies that the constraint is active, i.e., $(\\mathbf{s}^{\\star})^{\\top}\\mathbf{s}^{\\star} - \\Delta^2 = 0$. By the complementary slackness condition, this allows for $\\lambda > 0$. The condition that the solution is on the boundary for a convex problem implies $\\lambda$ will be strictly positive.\n\nSubstituting the given matrix $Q = 3 I_{2}$ into the stationarity equation:\n$$ (3I_{2} + \\lambda I_{2})\\mathbf{s}^{\\star} = -g $$\n$$ (3 + \\lambda)\\mathbf{s}^{\\star} = -g $$\nSince $Q$ is positive definite (its eigenvalues are both $3$), and $\\lambda \\geq 0$, the matrix $Q + \\lambda I_2$ is symmetric and positive definite (its eigenvalues are $3+\\lambda > 0$). Thus, it is invertible. We can solve for $\\mathbf{s}^{\\star}$:\n$$ \\mathbf{s}^{\\star} = -\\frac{1}{3+\\lambda} g $$\n\nNow, we enforce the boundary condition $\\|\\mathbf{s}^{\\star}\\| = \\Delta$:\n$$ \\|\\mathbf{s}^{\\star}\\| = \\left\\| -\\frac{1}{3+\\lambda} g \\right\\| = \\frac{1}{3+\\lambda} \\|g\\| $$\nThis leads to the secular equation, which relates $\\lambda$ to $\\Delta$:\n$$ \\frac{\\|g\\|}{3+\\lambda} = \\Delta $$\nWe are given $g = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$. The Euclidean norm of $g$ is:\n$$ \\|g\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5 $$\nSubstituting this value into the secular equation gives:\n$$ \\frac{5}{3+\\lambda} = \\Delta $$\nThis equation determines the value of the Lagrange multiplier $\\lambda$ for a given trust-region radius $\\Delta$.\n\nThe next step is to solve this equation for $\\lambda$ as a function of $\\Delta$:\n$$ 5 = \\Delta (3+\\lambda) $$\n$$ \\frac{5}{\\Delta} = 3+\\lambda $$\n$$ \\lambda(\\Delta) = \\frac{5}{\\Delta} - 3 $$\nThis expression is valid for $\\lambda \\geq 0$, which corresponds to $\\frac{5}{\\Delta} - 3 \\geq 0$, or $\\Delta \\leq \\frac{5}{3}$. The problem considers $\\Delta=1$, which satisfies this condition.\n\nUsing this result, we can find the exact trust-region step $\\mathbf{s}^{\\star}(\\Delta)$:\n$$ 3 + \\lambda(\\Delta) = 3 + \\left(\\frac{5}{\\Delta} - 3\\right) = \\frac{5}{\\Delta} $$\nSo,\n$$ \\mathbf{s}^{\\star}(\\Delta) = -\\frac{1}{3+\\lambda(\\Delta)} g = -\\frac{1}{5/\\Delta} g = -\\frac{\\Delta}{5} g = -\\frac{\\Delta}{5} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} $$\n\nFinally, we need to compute the sensitivity of the Lagrange multiplier with respect to the trust-region radius, $\\frac{d\\lambda}{d\\Delta}$, evaluated at $\\Delta = 1$. We have the expression for $\\lambda(\\Delta)$:\n$$ \\lambda(\\Delta) = 5\\Delta^{-1} - 3 $$\nWe differentiate $\\lambda(\\Delta)$ with respect to $\\Delta$:\n$$ \\frac{d\\lambda}{d\\Delta} = \\frac{d}{d\\Delta} (5\\Delta^{-1} - 3) = -5\\Delta^{-2} = -\\frac{5}{\\Delta^2} $$\nEvaluating this derivative at $\\Delta = 1$:\n$$ \\left. \\frac{d\\lambda}{d\\Delta} \\right|_{\\Delta=1} = -\\frac{5}{1^2} = -5 $$\nThe sensitivity of the Lagrange multiplier with respect to the trust-region radius at $\\Delta=1$ is $-5$.",
            "answer": "$$\n\\boxed{-5}\n$$"
        },
        {
            "introduction": "A trust-region method is an adaptive algorithm, where the success of each step informs the size of the trust region for the next iteration. This final practice explores the dynamics of this feedback loop, focusing on how the radius $\\delta_k$ is updated based on the agreement ratio $\\rho_k$. You will analyze a carefully constructed scenario where a systematic mismatch between the model and the true function causes the algorithm to \"thrash,\" with the radius oscillating between expansion and contraction . By diagnosing this instability and deriving a condition for stabilization, you will gain insight into the crucial role of the update parameters ($\\eta_1, \\eta_2, \\gamma_{\\mathrm{dec}}, \\gamma_{\\mathrm{inc}}$) in ensuring robust and efficient convergence.",
            "id": "3193702",
            "problem": "Consider the one-dimensional trust-region method (TRM) applied to a nonlinear objective with a model-based step. Let the objective function be the cubic polynomial $$f(x) = \\frac{1}{2} a x^{2} + \\beta x^{3},$$ with parameters satisfying $a > 0$ and $\\beta > 0$. At iterate $x_k,$ the algorithm builds a quadratic model $$m_{k}(s) = f(x_{k}) + g_{k}^{\\text{mod}} s + \\frac{1}{2} a s^{2},$$ where $$g_{k}^{\\text{mod}} = a x_{k}$$ is a model gradient that neglects the cubic term in the objective. The trust-region step is chosen to saturate the trust-region ball along the steepest-descent direction of the model, i.e., $$s_{k} = - \\delta_{k} \\,\\mathrm{sign}\\!\\left(g_{k}^{\\text{mod}}\\right),$$ where $\\delta_{k} > 0$ is the current trust-region radius.\n\nThe trust-region ratio is defined by the reduction comparison $$\\rho_{k} = \\frac{f(x_{k}) - f(x_{k} + s_{k})}{m_{k}(0) - m_{k}(s_{k})}.$$ Suppose $x_{0} > 0,$ the thresholds satisfy $0  \\eta_{1}  \\eta_{2}  1,$ and the trust-region radius update rule is the standard three-way rule: if $\\rho_{k}  \\eta_{1}$ then $\\delta_{k+1} = \\gamma_{\\mathrm{dec}} \\,\\delta_{k}$ with $0  \\gamma_{\\mathrm{dec}}  1;$ if $\\eta_{1} \\le \\rho_{k}  \\eta_{2}$ then $\\delta_{k+1} = \\delta_{k};$ and if $\\rho_{k} \\ge \\eta_{2}$ then $\\delta_{k+1} = \\min\\!\\left(\\gamma_{\\mathrm{inc}} \\,\\delta_{k}, \\Delta_{\\max}\\right)$ with $\\gamma_{\\mathrm{inc}} > 1$ and $\\Delta_{\\max} > 0$ given.\n\nStarting from these definitions, do the following:\n\n1. Using the definitions above and the structure of $f,$ derive the expression for $\\rho_{k}$ in terms of $a,$ $\\beta,$ $x_{k},$ and $\\delta_{k},$ and show that when the sign of $s_{k}$ alternates across iterations, $\\rho_{k}$ oscillates between two values of the form $\\rho_{\\mathrm{high}} = 1 + c$ and $\\rho_{\\mathrm{low}} = 1 - c,$ with $c > 0$ given in terms of $a,$ $\\beta,$ $x_{k},$ and $\\delta_{k}.$\n\n2. Assume that for the chosen $a,$ $\\beta,$ initial $x_{0},$ and initial $\\delta_{0},$ the oscillation is such that $\\rho_{\\mathrm{low}}  \\eta_{1}$ and $\\rho_{\\mathrm{high}} \\ge \\eta_{2},$ so the algorithm alternates shrinking and expanding the trust-region radius: $\\delta_{k+1} = \\gamma_{\\mathrm{dec}} \\,\\delta_{k}$ followed by $\\delta_{k+2} = \\gamma_{\\mathrm{inc}} \\,\\delta_{k+1}.$ To stabilize the radius magnitude over two alternating iterations, require that $\\delta_{k+2} = \\delta_{k}$ whenever $\\rho_{k}$ continues to alternate in this way. Under this stabilization requirement, compute the value of $\\gamma_{\\mathrm{inc}}^{\\ast}$ in terms of $\\gamma_{\\mathrm{dec}}.$ Then, evaluate $\\gamma_{\\mathrm{inc}}^{\\ast}$ for the specific case $\\gamma_{\\mathrm{dec}} = 0.5.$\n\nExpress your final answer as a single number. No rounding is needed.",
            "solution": "The problem is evaluated as valid. It is a well-posed problem in numerical optimization, grounded in a standard analysis of trust-region methods. All definitions are clear and the premises are mathematically and scientifically sound.\n\nThe solution is presented in two parts as requested.\n\n### Part 1: Derivation of the Trust-Region Ratio and Its Oscillation\n\nThe trust-region ratio, $\\rho_{k}$, is defined as the ratio of the actual reduction in the objective function to the predicted reduction from the model:\n$$\n\\rho_{k} = \\frac{\\text{ared}_{k}}{\\text{pred}_{k}} = \\frac{f(x_{k}) - f(x_{k} + s_{k})}{m_{k}(0) - m_{k}(s_{k})}\n$$\nLet us first compute the predicted reduction, $\\text{pred}_{k}$. The model is given by $m_{k}(s) = f(x_{k}) + g_{k}^{\\text{mod}} s + \\frac{1}{2} a s^{2}$, where $m_{k}(0) = f(x_{k})$.\n$$\n\\text{pred}_{k} = m_{k}(0) - m_{k}(s_{k}) = -g_{k}^{\\text{mod}} s_{k} - \\frac{1}{2} a s_{k}^{2}\n$$\nThe model gradient is $g_{k}^{\\text{mod}} = a x_{k}$, and the step is $s_{k} = - \\delta_{k} \\,\\mathrm{sign}(g_{k}^{\\text{mod}})$. Since $a  0$, $\\mathrm{sign}(g_{k}^{\\text{mod}}) = \\mathrm{sign}(a x_{k}) = \\mathrm{sign}(x_{k})$. Thus, $s_{k} = - \\delta_{k} \\,\\mathrm{sign}(x_{k})$.\nSubstituting these into the expression for $\\text{pred}_{k}$:\n$$\n\\text{pred}_{k} = -(a x_{k})(-\\delta_{k} \\,\\mathrm{sign}(x_{k})) - \\frac{1}{2} a (-\\delta_{k} \\,\\mathrm{sign}(x_{k}))^{2}\n$$\nUsing the property $x_{k} \\mathrm{sign}(x_{k}) = |x_{k}|$, this simplifies to:\n$$\n\\text{pred}_{k} = a \\delta_{k} |x_{k}| - \\frac{1}{2} a \\delta_{k}^{2} = a \\delta_{k} \\left(|x_{k}| - \\frac{\\delta_{k}}{2}\\right)\n$$\nFor the predicted reduction to be positive (a necessary condition for a meaningful descent step), we must have $|x_{k}|  \\frac{\\delta_{k}}{2}$.\n\nNext, let us compute the actual reduction, $\\text{ared}_{k}$. The objective function is $f(x) = \\frac{1}{2} a x^{2} + \\beta x^{3}$.\n$$\n\\text{ared}_{k} = f(x_{k}) - f(x_{k} + s_{k}) = \\left(\\frac{1}{2} a x_{k}^{2} + \\beta x_{k}^{3}\\right) - \\left(\\frac{1}{2} a (x_{k} + s_{k})^{2} + \\beta (x_{k} + s_{k})^{3}\\right)\n$$\nExpanding the terms:\n$$\nf(x_{k} + s_{k}) = \\frac{1}{2} a (x_{k}^{2} + 2x_{k}s_{k} + s_{k}^{2}) + \\beta (x_{k}^{3} + 3x_{k}^{2}s_{k} + 3x_{k}s_{k}^{2} + s_{k}^{3})\n$$\n$$\nf(x_{k} + s_{k}) = \\left(\\frac{1}{2} a x_{k}^{2} + \\beta x_{k}^{3}\\right) + (a x_{k} s_{k} + \\frac{1}{2} a s_{k}^{2}) + (3\\beta x_{k}^{2} s_{k} + 3\\beta x_{k} s_{k}^{2} + \\beta s_{k}^{3})\n$$\nThe actual reduction is therefore:\n$$\n\\text{ared}_{k} = - (a x_{k} s_{k} + \\frac{1}{2} a s_{k}^{2}) - (3\\beta x_{k}^{2} s_{k} + 3\\beta x_{k} s_{k}^{2} + \\beta s_{k}^{3})\n$$\nThe first term is exactly $\\text{pred}_{k} = -g_{k}^{\\text{mod}} s_{k} - \\frac{1}{2} a s_{k}^{2}$. So we can write:\n$$\n\\text{ared}_{k} = \\text{pred}_{k} - (3\\beta x_{k}^{2} s_{k} + 3\\beta x_{k} s_{k}^{2} + \\beta s_{k}^{3})\n$$\nNow, we can express $\\rho_{k}$:\n$$\n\\rho_{k} = \\frac{\\text{ared}_{k}}{\\text{pred}_{k}} = 1 - \\frac{3\\beta x_{k}^{2} s_{k} + 3\\beta x_{k} s_{k}^{2} + \\beta s_{k}^{3}}{\\text{pred}_{k}}\n$$\nLet's analyze the numerator of the fraction. Let it be $N_k = 3\\beta x_{k}^{2} s_{k} + 3\\beta x_{k} s_{k}^{2} + \\beta s_{k}^{3}$. Substitute $s_{k} = - \\delta_{k} \\,\\mathrm{sign}(x_{k})$:\n$$\nN_k = 3\\beta x_{k}^{2} (-\\delta_{k}\\mathrm{sign}(x_{k})) + 3\\beta x_{k} (-\\delta_{k}\\mathrm{sign}(x_{k}))^{2} + \\beta (-\\delta_{k}\\mathrm{sign}(x_{k}))^{3}\n$$\n$$\nN_k = -3\\beta \\delta_{k} x_{k}^{2} \\mathrm{sign}(x_{k}) + 3\\beta \\delta_{k}^{2} x_{k} - \\beta \\delta_{k}^{3} (\\mathrm{sign}(x_{k}))^{3}\n$$\nUsing $(\\mathrm{sign}(x_{k}))^{3} = \\mathrm{sign}(x_{k})$ and factoring out $-\\delta_{k}\\mathrm{sign}(x_{k})$:\n$$\nN_k = -\\delta_{k}\\mathrm{sign}(x_{k}) \\left( 3\\beta x_{k}^{2} - 3\\beta \\delta_{k} x_{k} \\mathrm{sign}(x_{k}) + \\beta \\delta_{k}^{2} \\right)\n$$\nUsing $x_{k} \\mathrm{sign}(x_{k}) = |x_{k}|$ and $x_{k}^{2} = |x_{k}|^{2}$:\n$$\nN_k = -\\delta_{k}\\mathrm{sign}(x_{k}) \\left( 3\\beta |x_{k}|^{2} - 3\\beta \\delta_{k} |x_{k}| + \\beta \\delta_{k}^{2} \\right)\n$$\nThe ratio in the expression for $\\rho_k$ is then:\n$$\n\\frac{N_k}{\\text{pred}_{k}} = \\frac{-\\delta_{k}\\mathrm{sign}(x_{k}) \\beta \\left( 3|x_{k}|^{2} - 3\\delta_{k} |x_{k}| + \\delta_{k}^{2} \\right)}{a \\delta_{k} \\left(|x_{k}| - \\frac{\\delta_{k}}{2}\\right)} = - \\mathrm{sign}(x_{k}) \\frac{\\beta \\left( 3|x_{k}|^{2} - 3\\delta_{k} |x_{k}| + \\delta_{k}^{2} \\right)}{a \\left(|x_{k}| - \\frac{\\delta_{k}}{2}\\right)}\n$$\nLet us define the term $c_{k}$:\n$$\nc_{k} = \\frac{\\beta \\left( 3|x_{k}|^{2} - 3\\delta_{k} |x_{k}| + \\delta_{k}^{2} \\right)}{a \\left(|x_{k}| - \\frac{\\delta_{k}}{2}\\right)}\n$$\nThe numerator of $c_k$ is a quadratic in $|x_k|$, $3\\beta u^2 - (3\\beta\\delta_k)u + \\beta\\delta_k^2$. Its discriminant is $(3\\beta\\delta_k)^2 - 4(3\\beta)(\\beta\\delta_k^2) = 9\\beta^2\\delta_k^2 - 12\\beta^2\\delta_k^2 = -3\\beta^2\\delta_k^2  0$. Since the leading coefficient $3\\beta$ is positive, the numerator is always positive. The denominator is positive from the condition $\\text{pred}_{k}  0$. Thus, $c_{k}  0$.\n\nSubstituting back into the expression for $\\rho_{k}$:\n$$\n\\rho_{k} = 1 - (- \\mathrm{sign}(x_{k}) c_{k}) = 1 + \\mathrm{sign}(x_{k}) c_{k}\n$$\nThe problem states that the sign of $s_{k}$ alternates. Since $s_{k} = -\\delta_{k}\\mathrm{sign}(x_{k})$, this implies that the sign of $x_k$ alternates between iterations.\n- If $x_{k}  0$, then $\\mathrm{sign}(x_{k}) = 1$, and $\\rho_{k} = 1 + c_{k}$. This would be $\\rho_{\\mathrm{high}}$.\n- If $x_{k}  0$, then $\\mathrm{sign}(x_{k}) = -1$, and $\\rho_{k} = 1 - c_{k}$. This would be $\\rho_{\\mathrm{low}}$.\n\nThis shows that when the sign of $x_k$ (and thus $s_k$) alternates, $\\rho_k$ oscillates between a value greater than $1$ and a value less than $1$, corresponding to the forms $1+c$ and $1-c$ where $c$ is the positive quantity $c_k$. This completes the first part of the proof.\n\n### Part 2: Stabilization of Trust-Region Radius\n\nThe second part of the problem establishes a scenario based on the oscillation derived above.\nIt is assumed that the oscillation is such that:\n1.  One iteration yields $\\rho_k = \\rho_{\\mathrm{low}}  \\eta_1$. According to the update rules, the algorithm shrinks the trust-region radius: $\\delta_{k+1} = \\gamma_{\\mathrm{dec}} \\delta_k$.\n2.  The next iteration yields $\\rho_{k+1} = \\rho_{\\mathrm{high}} \\ge \\eta_2$. According to the update rules, the algorithm expands the trust-region radius: $\\delta_{k+2} = \\gamma_{\\mathrm{inc}} \\delta_{k+1}$. We assume the expansion is not limited by $\\Delta_{\\max}$.\n\nThis sets up a two-step cycle for the trust-region radius:\n$$\n\\delta_{k+1} = \\gamma_{\\mathrm{dec}} \\delta_{k}\n$$\n$$\n\\delta_{k+2} = \\gamma_{\\mathrm{inc}} \\delta_{k+1}\n$$\nWe can combine these two equations to relate $\\delta_{k+2}$ to $\\delta_{k}$:\n$$\n\\delta_{k+2} = \\gamma_{\\mathrm{inc}} (\\gamma_{\\mathrm{dec}} \\delta_{k}) = (\\gamma_{\\mathrm{inc}} \\gamma_{\\mathrm{dec}}) \\delta_{k}\n$$\nThe problem then imposes a stabilization requirement that the radius magnitude is stable over this two-iteration cycle. This is expressed as:\n$$\n\\delta_{k+2} = \\delta_{k}\n$$\nSubstituting this requirement into our combined equation:\n$$\n\\delta_{k} = (\\gamma_{\\mathrm{inc}}^{\\ast} \\gamma_{\\mathrm{dec}}) \\delta_{k}\n$$\nSince $\\delta_k  0$, we can divide both sides by $\\delta_k$:\n$$\n1 = \\gamma_{\\mathrm{inc}}^{\\ast} \\gamma_{\\mathrm{dec}}\n$$\nSolving for the required expansion factor, $\\gamma_{\\mathrm{inc}}^{\\ast}$:\n$$\n\\gamma_{\\mathrm{inc}}^{\\ast} = \\frac{1}{\\gamma_{\\mathrm{dec}}}\n$$\nFinally, we are asked to evaluate this expression for the specific case where $\\gamma_{\\mathrm{dec}} = 0.5$.\n$$\n\\gamma_{\\mathrm{inc}}^{\\ast} = \\frac{1}{0.5} = 2\n$$\nThe value of $\\gamma_{\\mathrm{inc}}^{\\ast}$ is $2$. This value is consistent with the constraint $\\gamma_{\\mathrm{inc}}  1$.",
            "answer": "$$\n\\boxed{2}\n$$"
        }
    ]
}