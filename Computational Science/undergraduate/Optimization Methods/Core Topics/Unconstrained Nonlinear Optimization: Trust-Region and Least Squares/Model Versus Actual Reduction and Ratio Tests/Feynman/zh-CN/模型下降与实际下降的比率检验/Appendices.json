{
    "hands_on_practices": [
        {
            "introduction": "比率测试为迭代优化提供了关键的反馈机制。为了真正掌握其功能，我们将从一个基础练习开始：手动计算实际减少量与预测减少量的比率 $\\rho_k$。这项练习  将巩固您对 $\\rho_k$ 计算方式的理解，并揭示它如何直接控制 Levenberg-Marquardt 阻尼参数的调整，从而确保算法的稳定性。",
            "id": "3152688",
            "problem": "考虑使用通过 Levenberg–Marquardt (LM) 修正计算的阻尼牛顿步来最小化一个二阶连续可微的标量函数 $f(x)$。在迭代点 $x_k$ 处，参数为 $\\lambda_k$ 的 LM 步 $s_k$ 由线性系统 $(\\nabla^{2} f(x_k) + \\lambda_k I)s_k = -\\nabla f(x_k)$ 隐式定义，其中 $I$ 是单位算子，$\\nabla f(x)$ 和 $\\nabla^{2} f(x)$ 分别表示梯度和 Hessian 矩阵。$f$ 在 $x_k$ 附近的局部二次泰勒模型为 $m_k(s) = f(x_k) + \\nabla f(x_k)^{\\top} s + \\tfrac{1}{2}s^{\\top} \\nabla^{2} f(x_k) s$。实际减少量为 $A_k(s) = f(x_k) - f(x_k + s)$，预测减少量为 $P_k(s) = f(x_k) - m_k(s)$。比率测试使用量 $\\rho_k = \\dfrac{A_k(s_k)}{P_k(s_k)}$ 来判断模型相对于实际减少量的质量。\n\n在一维情况下，使用函数 $f(x) = \\dfrac{1}{4}x^{4} - x^{3} + \\dfrac{3}{2}x^{2}$。在当前迭代点 $x_k = \\dfrac{1}{2}$，设置 LM 参数为 $\\lambda_k = \\dfrac{1}{20}$。使用二次泰勒模型的基本定义（在 $x_k$ 处将 $f$ 的泰勒级数截断到二阶）和 LM 步的定义，计算由 $(\\nabla^{2} f(x_k) + \\lambda_k I)s_k = -\\nabla f(x_k)$ 得到的步长 $s_k$ 的比率 $\\rho_k$。然后应用以下基于比率的 LM 更新策略：\n- 如果 $\\rho_k < \\underline{\\eta}$，其中 $\\underline{\\eta} = \\dfrac{1}{4}$，则设置 $\\lambda_{k+1} = 8 \\lambda_k$。\n- 如果 $\\underline{\\eta} \\le \\rho_k \\le \\overline{\\eta}$，其中 $\\overline{\\eta} = \\dfrac{3}{4}$，则设置 $\\lambda_{k+1} = \\lambda_k$。\n- 如果 $\\rho_k > \\overline{\\eta}$，则设置 $\\lambda_{k+1} = \\dfrac{\\lambda_k}{2}$。\n\n你的最终答案必须是 $\\lambda_{k+1}$ 的单个值，表示为精确分数。不需要四舍五入。该场景的构造使得过于激进的阻尼（$\\lambda_k$ 太小）会产生一个步长，其二次模型预测的减少量相对于实际减少量较差，从而触发 $\\lambda_k$ 的增加。",
            "solution": "问题是对于一个特定的一维函数 $f(x)$，在给定的点 $x_k$ 和给定的初始 LM 参数 $\\lambda_k$ 下，根据实际减少量与预测减少量的比率 $\\rho_k$ 来确定下一个 Levenberg-Marquardt (LM) 参数 $\\lambda_{k+1}$。\n\n首先，我们定义该函数并计算其一阶和二阶导数：\n函数为 $f(x) = \\dfrac{1}{4}x^{4} - x^{3} + \\dfrac{3}{2}x^{2}$。\n一阶导数（一维中的梯度）是 $f'(x) = \\dfrac{d f}{dx} = x^{3} - 3x^{2} + 3x$。\n二阶导数（一维中的 Hessian）是 $f''(x) = \\dfrac{d^{2} f}{dx^{2}} = 3x^{2} - 6x + 3 = 3(x-1)^{2}$。\n\n接下来，我们在当前迭代点 $x_k = \\dfrac{1}{2}$ 处计算函数及其导数的值：\n$f(x_k) = f\\left(\\dfrac{1}{2}\\right) = \\dfrac{1}{4}\\left(\\dfrac{1}{2}\\right)^{4} - \\left(\\dfrac{1}{2}\\right)^{3} + \\dfrac{3}{2}\\left(\\dfrac{1}{2}\\right)^{2} = \\dfrac{1}{4}\\left(\\dfrac{1}{16}\\right) - \\dfrac{1}{8} + \\dfrac{3}{2}\\left(\\dfrac{1}{4}\\right) = \\dfrac{1}{64} - \\dfrac{8}{64} + \\dfrac{24}{64} = \\dfrac{17}{64}$。\n$f'(x_k) = f'\\left(\\dfrac{1}{2}\\right) = \\left(\\dfrac{1}{2}\\right)^{3} - 3\\left(\\dfrac{1}{2}\\right)^{2} + 3\\left(\\dfrac{1}{2}\\right) = \\dfrac{1}{8} - \\dfrac{3}{4} + \\dfrac{3}{2} = \\dfrac{1}{8} - \\dfrac{6}{8} + \\dfrac{12}{8} = \\dfrac{7}{8}$。\n$f''(x_k) = f''\\left(\\dfrac{1}{2}\\right) = 3\\left(\\dfrac{1}{2} - 1\\right)^{2} = 3\\left(-\\dfrac{1}{2}\\right)^{2} = 3\\left(\\dfrac{1}{4}\\right) = \\dfrac{3}{4}$。\n\nLM 步长 $s_k$ 是使用给定的参数 $\\lambda_k = \\dfrac{1}{20}$ 计算的。一维的 LM 系统是：\n$$ (f''(x_k) + \\lambda_k) s_k = -f'(x_k) $$\n代入计算出的值：\n$$ \\left(\\dfrac{3}{4} + \\dfrac{1}{20}\\right) s_k = -\\dfrac{7}{8} $$\n$$ \\left(\\dfrac{15}{20} + \\dfrac{1}{20}\\right) s_k = -\\dfrac{7}{8} $$\n$$ \\dfrac{16}{20} s_k = -\\dfrac{7}{8} $$\n$$ \\dfrac{4}{5} s_k = -\\dfrac{7}{8} $$\n求解 $s_k$：\n$$ s_k = -\\dfrac{7}{8} \\cdot \\dfrac{5}{4} = -\\dfrac{35}{32} $$\n\n现在，我们计算比率 $\\rho_k = \\dfrac{A_k(s_k)}{P_k(s_k)}$。\n预测减少量为 $P_k(s_k) = -f'(x_k)s_k - \\dfrac{1}{2}f''(x_k)s_k^2$。\n实际减少量为 $A_k(s_k) = f(x_k) - f(x_k + s_k)$。\n\n由于 $f(x)$ 是一个四次多项式，其在 $x_k$ 附近的泰勒展开是精确的，并在四阶项后终止：\n$f(x_k + s_k) = f(x_k) + f'(x_k)s_k + \\dfrac{1}{2}f''(x_k)s_k^2 + \\dfrac{1}{6}f'''(x_k)s_k^3 + \\dfrac{1}{24}f^{(4)}(x_k)s_k^4$。\n二次模型是 $m_k(s_k) = f(x_k) + f'(x_k)s_k + \\dfrac{1}{2}f''(x_k)s_k^2$。\n因此，实际减少量 $A_k(s_k)$ 和预测减少量 $P_k(s_k)$ 可以表示为：\n$P_k(s_k) = -f'(x_k)s_k - \\dfrac{1}{2}f''(x_k)s_k^2$\n$A_k(s_k) = -f'(x_k)s_k - \\dfrac{1}{2}f''(x_k)s_k^2 - \\dfrac{1}{6}f'''(x_k)s_k^3 - \\dfrac{1}{24}f^{(4)}(x_k)s_k^4$\n比率可以写为 $\\rho_k = \\dfrac{A_k(s_k)}{P_k(s_k)} = 1 - \\dfrac{\\frac{1}{6}f'''(x_k)s_k^3 + \\frac{1}{24}f^{(4)}(x_k)s_k^4}{P_k(s_k)}$。\n\n我们计算 $P_k(s_k)$ 和高阶项。\n$P_k(s_k) = -\\left(\\dfrac{7}{8}\\right)\\left(-\\dfrac{35}{32}\\right) - \\dfrac{1}{2}\\left(\\dfrac{3}{4}\\right)\\left(-\\dfrac{35}{32}\\right)^2 = \\dfrac{245}{256} - \\dfrac{3}{8}\\left(\\dfrac{1225}{1024}\\right) = \\dfrac{245 \\cdot 32 - 3675}{8192} = \\dfrac{7840 - 3675}{8192} = \\dfrac{4165}{8192}$。\n\n三阶和四阶导数是：\n$f'''(x) = 6x-6$，在 $x_k = \\dfrac{1}{2}$ 处，$f'''(\\frac{1}{2}) = -3$。\n$f^{(4)}(x) = 6$，在 $x_k = \\dfrac{1}{2}$ 处，$f^{(4)}(\\frac{1}{2}) = 6$。\n高阶项 $R_k(s_k) = \\dfrac{1}{6}f'''(x_k)s_k^3 + \\dfrac{1}{24}f^{(4)}(x_k)s_k^4$：\n$R_k(s_k) = \\dfrac{1}{6}(-3)\\left(-\\dfrac{35}{32}\\right)^3 + \\dfrac{1}{24}(6)\\left(-\\dfrac{35}{32}\\right)^4 = -\\dfrac{1}{2}\\left(-\\dfrac{42875}{32768}\\right) + \\dfrac{1}{4}\\left(\\dfrac{1500625}{1048576}\\right)$\n$R_k(s_k) = \\dfrac{42875}{65536} + \\dfrac{1500625}{4194304} = \\dfrac{42875 \\cdot 64 + 1500625}{4194304} = \\dfrac{2744000 + 1500625}{4194304} = \\dfrac{4244625}{4194304}$。\n\n现在我们可以计算比率 $\\rho_k = 1 - \\dfrac{R_k(s_k)}{P_k(s_k)}$：\n$$ \\frac{R_k(s_k)}{P_k(s_k)} = \\frac{4244625/4194304}{4165/8192} = \\frac{4244625 \\cdot 8192}{4194304 \\cdot 4165} = \\frac{4244625}{512 \\cdot 4165} = \\frac{4244625}{2132480} $$\n将此分数约简，得到 $\\dfrac{17325}{8704}$。\n因此，\n$$ \\rho_k = 1 - \\frac{17325}{8704} = \\frac{8704 - 17325}{8704} = -\\frac{8621}{8704} $$\n\n最后，我们应用 $\\lambda_{k+1}$ 的更新规则。阈值为 $\\underline{\\eta} = \\dfrac{1}{4}$ 和 $\\overline{\\eta} = \\dfrac{3}{4}$。\n因为 $\\rho_k = -\\dfrac{8621}{8704}$ 是一个负数，它显然小于 $\\underline{\\eta} = \\dfrac{1}{4}$。\n规则规定：如果 $\\rho_k < \\underline{\\eta}$，则设置 $\\lambda_{k+1} = 8 \\lambda_k$。\n给定 $\\lambda_k = \\dfrac{1}{20}$，新的参数是：\n$$ \\lambda_{k+1} = 8 \\cdot \\lambda_k = 8 \\cdot \\dfrac{1}{20} = \\dfrac{8}{20} = \\dfrac{2}{5} $$\n二次模型质量差（由负的 $\\rho_k$ 表示）正确地触发了阻尼参数 $\\lambda$ 的增加，这将在下一次迭代中导致一个更小、更保守的步长。",
            "answer": "$$\\boxed{\\frac{2}{5}}$$"
        },
        {
            "introduction": "虽然手动计算可以建立直觉，但现代优化依赖于稳健的软件实现。下一个练习  要求您编写代码并比较三种经典的信赖域步长策略：柯西（Cauchy）步、狗腿（dogleg）步和精确解步。通过将它们应用于一个病态问题，您将获得关于它们性能的实践经验，并了解在实际场景中出现的数值挑战。",
            "id": "3152579",
            "problem": "考虑一个由 $$f(x) = \\tfrac{1}{2} x^\\top H x + b^\\top x + c,$$ 定义的二次连续可微的二次目标函数，其中 $$H \\in \\mathbb{R}^{n \\times n}$$ 是对称正定矩阵，$$b \\in \\mathbb{R}^n,$$ 且 $$c \\in \\mathbb{R}.$$ 在当前点 $$x_k,$$ 信赖域（TR）方法中使用的局部二次模型为 $$m_k(s) = f(x_k) + g_k^\\top s + \\tfrac{1}{2} s^\\top H s,$$ 其中 $$g_k = \\nabla f(x_k) = H x_k + b.$$ TR子问题是找到一个步长 $$s_k$$，使得在 $$\\|s\\|_2 \\le \\Delta_k$$ 的约束下，$$m_k(s)$$ 最小化，其中 $$\\Delta_k > 0$$ 是信赖域半径。模型预测的下降量为 $$\\text{pred}_k = m_k(0) - m_k(s_k),$$ 实际下降量为 $$\\text{ared}_k = f(x_k) - f(x_k + s_k).$$ 比率测试通过 $$\\rho_k = \\frac{\\text{ared}_k}{\\text{pred}_k}$$ 来比较模型下降量和实际下降量。\n\n您的任务是实现一个程序，针对一个病态二次函数和指定的测试用例，使用三种不同的TR步长计算 $$\\rho_k$$：\n- 柯西（最速下降）步，\n- 狗腿步，\n- 精确TR解步。\n\n您必须构建以下病态二次函数和评估设置：\n- 维度 $$n = 3.$$\n- Hessian矩阵 $$H = \\operatorname{diag}(10^6, 1, 10^{-6}).$$\n- 向量 $$b = (0, 0, 0)^\\top,$$ 因此 $$g_k = H x_k.$$\n- 标量偏移量 $$c$$ 可以被视为任意常数，因为它在计算下降量差异时会被抵消。\n- 为了在病态问题上模拟实际函数评估中的舍入效应，定义一个受扰动的评估函数 $$\\tilde{f}(x) = f(x) + \\delta(x)$$，其中 $$\\delta(x) = \\eta \\, \\sqrt{\\kappa(H)} \\, \\|x\\|_2 \\left(\\left|b^\\top x\\right| + \\tfrac{1}{2}\\left|x^\\top H x\\right|\\right),$$ 此处 $$\\kappa(H)$$ 是 $$H$$ 在 $$2$$-范数下的条件数，$$\\eta > 0$$ 是每个测试用例提供的一个小标量。在此问题中，由于 $$b = 0,$$ $$\\delta(x) = \\eta \\, \\sqrt{\\kappa(H)} \\, \\|x\\|_2 \\left(\\tfrac{1}{2}\\left|x^\\top H x\\right|\\right).$$\n- 预测下降量应根据无扰动的模型计算，$$\\text{pred}_k = -\\left(g_k^\\top s_k + \\tfrac{1}{2} s_k^\\top H s_k\\right),$$ 而实际下降量应使用 $$\\tilde{f}$$ 计算，$$\\text{ared}_k = \\tilde{f}(x_k) - \\tilde{f}(x_k + s_k).$$\n\n根据标准优化原理，按如下方式实现这三个步长：\n- 柯西步：沿负梯度方向移动，如果需要，则截断至信赖域边界。\n- 狗腿步：结合柯西步和牛顿步（当牛顿步在信赖域外时，取连接这两点的线段与信赖域边界的交点）。\n- 精确TR解：通过找到唯一的拉格朗日乘子 $$\\lambda \\ge 0$$ 来求解TR子问题，使得 $$s(\\lambda) = -(H + \\lambda I)^{-1} g_k$$ 满足 $$\\|s(\\lambda)\\|_2 = \\Delta_k,$$ 如果无约束的牛顿步位于信赖域内，则使用该牛顿步。\n\n使用以下测试套件，每个用例提供 $$x_k,$$ $$\\Delta_k,$$ 和 $$\\eta$$：\n- 用例 $$1$$ (理想路径，牛顿步在信赖域内): $$x_k = (10^{-3}, -10^{-3}, 2 \\cdot 10^{-3})^\\top,$$ $$\\Delta_k = 10^{-1},$$ $$\\eta = 10^{-16}.$$\n- 用例 $$2$$ (小信赖域，柯西步截断): $$x_k = (10^{-3}, -10^{-3}, 2 \\cdot 10^{-3})^\\top,$$ $$\\Delta_k = 10^{-4},$$ $$\\eta = 10^{-16}.$$\n- 用例 $$3$$ (狗腿路径分段情况，柯西步在内而牛顿步在外): $$x_k = (2 \\cdot 10^{-1}, -10^{-1}, 5 \\cdot 10^{-2})^\\top,$$ $$\\Delta_k = 2.2 \\cdot 10^{-1},$$ $$\\eta = 10^{-16}.$$\n- 用例 $$4$$ (病态条件凸显小特征值方向): $$x_k = (0, 0, 10^{3})^\\top,$$ $$\\Delta_k = 10^{-2},$$ $$\\eta = 10^{-16}.$$\n\n您的程序必须为每个用例计算列表 $$[\\rho_k^{\\text{Cauchy}}, \\rho_k^{\\text{dogleg}}, \\rho_k^{\\text{exact}}]$$ 并将所有用例的结果汇总到一个列表中。最终输出必须是包含此汇总列表的单行，格式为逗号分隔的方括号（例如，$$[ [\\rho_1^{\\text{C}}, \\rho_1^{\\text{D}}, \\rho_1^{\\text{E}}], [\\rho_2^{\\text{C}}, \\rho_2^{\\text{D}}, \\rho_2^{\\text{E}}], \\ldots ]$$），数值以标准十进制浮点数打印。\n\n不适用物理单位。不出现角度。不得使用百分比；将 $$\\rho_k$$ 值报告为小数。\n\n该任务应纯粹从数学和算法角度解决，从上述核心定义和经过充分检验的事实出发，不依赖于未说明的假设或捷径。您的实现必须是自包含的，并且不得要求任何用户输入。\n\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$$[ [r_{11}, r_{12}, r_{13}], [r_{21}, r_{22}, r_{23}], [r_{31}, r_{32}, r_{33}], [r_{41}, r_{42}, r_{43}] ]$$）。",
            "solution": "该问题是有效的，因为它在科学上基于数值优化理论，具有所有必要的数据和定义，问题陈述良好，并且其表述是客观的。我们将通过实现指定的算法来解决它。\n\n核心任务是为三种不同的信赖域（TR）步长计算比率 $\\rho_k = \\frac{\\text{ared}_k}{\\text{pred}_k}$：柯西步、狗腿步和精确TR解。其背景是最小化一个病态二次函数。\n\n目标函数是 $f(x) = \\frac{1}{2} x^\\top H x + b^\\top x + c$。根据问题的具体规定：\n- 维度为 $n=3$。\n- Hessian矩阵为 $H = \\operatorname{diag}(10^6, 1, 10^{-6})$。该矩阵是对称正定的，其特征值为 $\\lambda_{\\max} = 10^6$ 和 $\\lambda_{\\min} = 10^{-6}$。\n- 线性项系数为 $b = (0, 0, 0)^\\top$。\n- 标量偏移量 $c$ 是常数，在所有下降量的计算中都会被抵消，因此我们可以设 $c=0$。\n这简化了目标函数为 $f(x) = \\frac{1}{2} x^\\top H x$。\n\n在点 $x_k$ 处的梯度是 $g_k = \\nabla f(x_k) = Hx_k + b = Hx_k$。$f$ 在 $x_k$ 周围的二次模型是 $m_k(s) = f(x_k) + g_k^\\top s + \\frac{1}{2} s^\\top H s$。由于模型使用精确的Hessian矩阵 $H$，它等价于 $f(x_k+s)$。\n\n由步长 $s_k$ 预测的下降量基于模型：\n$$\n\\text{pred}_k = m_k(0) - m_k(s_k) = f(x_k) - (f(x_k) + g_k^\\top s_k + \\frac{1}{2} s_k^\\top H s_k) = -\\left(g_k^\\top s_k + \\frac{1}{2} s_k^\\top H s_k\\right)\n$$\n由于 $H$ 是正定的，并且选择步长 $s_k$ 是为了最小化模型，$\\text{pred}_k$ 将是非负的。\n\n实际下降量使用一个受扰动的函数评估 $\\tilde{f}(x)$ 来计算，以模拟舍入误差：\n$$\n\\text{ared}_k = \\tilde{f}(x_k) - \\tilde{f}(x_k + s_k)\n$$\n受扰动的函数定义为 $\\tilde{f}(x) = f(x) + \\delta(x)$，其中扰动 $\\delta(x)$ 由下式给出：\n$$\n\\delta(x) = \\eta \\, \\sqrt{\\kappa(H)} \\, \\|x\\|_2 \\left(\\frac{1}{2}\\left|x^\\top H x\\right|\\right)\n$$\n对角矩阵 $H$ 的2-范数条件数为 $\\kappa(H) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{10^6}{10^{-6}} = 10^{12}$。因此，$\\sqrt{\\kappa(H)} = 10^6$。由于 $H$ 是正定的，$x^\\top H x \\ge 0$，所以 $|x^\\top H x| = x^\\top H x = 2f(x)$。扰动简化为：\n$$\n\\delta(x) = \\eta \\cdot 10^6 \\cdot \\|x\\|_2 \\cdot f(x)\n$$\n受扰动的函数为：\n$$\n\\tilde{f}(x) = f(x) \\left(1 + \\eta \\cdot 10^6 \\cdot \\|x\\|_2\\right)\n$$\n\n对于每个由 $(x_k, \\Delta_k, \\eta)$ 指定的测试用例，我们将计算三种步长及其对应的 $\\rho_k$ 值。\n\n**1. 柯西步 ($s_C$)**\n柯西步是二次模型 $m_k(s)$ 沿最速下降方向 $d = -g_k$ 的最小化子，受限于信赖域 $\\|s\\|_2 \\le \\Delta_k$。\n沿此方向的无约束最小化子是 $s_{UCP} = \\tau^* d$，其中 $\\tau^* = \\frac{-g_k^\\top d}{d^\\top H d} = \\frac{g_k^\\top g_k}{g_k^\\top H g_k}$。\n- 如果 $\\|s_{UCP}\\|_2 \\le \\Delta_k$，柯西步为 $s_C = s_{UCP}$。\n- 否则，步长被截断到信赖域边界：$s_C = \\Delta_k \\frac{d}{\\|d\\|_2} = -\\frac{\\Delta_k}{\\|g_k\\|_2} g_k$。\n\n**2. 狗腿步 ($s_{DL}$)**\n狗腿步通过在柯西步和牛顿步之间插值，提供了对精确TR解的近似。牛顿步 $s_N$ 是 $m_k(s)$ 的无约束最小化子，由 $s_N = -H^{-1}g_k$ 给出。对于此问题，$s_N = -H^{-1}(Hx_k) = -x_k$。\n狗腿路径连接原点、无约束柯西点 $s_{UCP}$ 和牛顿点 $s_N$。步长 $s_{DL}$ 是此路径上在信赖域内离原点最远的点。\n- **情况 (a):** 如果 $\\|s_N\\|_2 \\le \\Delta_k$，牛顿步是可行的并且是最优的。$s_{DL} = s_N$。\n- **情况 (b):** 如果 $\\|s_N\\|_2 > \\Delta_k$ 且 $\\|s_{UCP}\\|_2 \\ge \\Delta_k$，狗腿路径沿最速下降方向与信赖域边界相交。$s_{DL}$ 是截断的柯西步，$s_{DL} = -\\frac{\\Delta_k}{\\|g_k\\|_2} g_k$。\n- **情况 (c):** 如果 $\\|s_N\\|_2 > \\Delta_k$ 且 $\\|s_{UCP}\\|_2 < \\Delta_k$，步长 $s_{DL}$ 位于连接 $s_{UCP}$ 和 $s_N$ 的线段上。我们通过求解 $\\|s_{UCP} + \\beta(s_N - s_{UCP})\\|_2^2 = \\Delta_k^2$ 来找到 $s_{DL} = s_{UCP} + \\beta(s_N - s_{UCP})$，其中 $\\beta \\in (0,1]$。这是一个关于 $\\beta$ 的二次方程，并且由于 $\\|s_{UCP}\\|_2 < \\Delta_k < \\|s_N\\|_2$，保证了在此范围内 $\\beta$ 有唯一解。\n\n**3. 精确信赖域步 ($s_{TR}$)**\nTR子问题 $\\min_{\\|s\\|_2 \\le \\Delta_k} m_k(s)$ 的精确解由最优性条件来表征。\n- 如果牛顿步 $s_N = -x_k$ 是可行的（即 $\\|s_N\\|_2 \\le \\Delta_k$），那么它就是最优解，所以 $s_{TR} = s_N$。这对应于拉格朗日乘子 $\\lambda = 0$。\n- 如果 $\\|s_N\\|_2 > \\Delta_k$，解位于边界上，即 $\\|s_{TR}\\|_2 = \\Delta_k$。步长由 $s_{TR}(\\lambda) = -(H + \\lambda I)^{-1} g_k$ 给出，对于唯一的 $\\lambda > 0$ 满足 $\\|s_{TR}(\\lambda)\\|_2 = \\Delta_k$。这需要求解关于 $\\lambda$ 的非线性长期方程：\n$$\n\\left\\| -(H + \\lambda I)^{-1} g_k \\right\\|_2 = \\Delta_k\n$$\n由于 $H$ 是对角矩阵，$H = \\operatorname{diag}(h_1, h_2, h_3)$，方程变为：\n$$\n\\sqrt{\\sum_{i=1}^3 \\left(\\frac{(g_k)_i}{h_i + \\lambda}\\right)^2} = \\Delta_k\n$$\n这个关于 $\\lambda$ 的一维求根问题可以使用数值方法（如Brent方法）高效求解。函数 $\\phi(\\lambda) = \\|s_{TR}(\\lambda)\\|_2$ 对于 $\\lambda \\ge 0$ 是单调递减的，这确保了唯一解的存在。\n\n实现将遵循这些步骤来处理每个提供的测试用例。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\ndef solve():\n    \"\"\"\n    Computes the model vs. actual reduction ratio (rho) for three types of trust-region steps\n    on an ill-conditioned quadratic function for a suite of test cases.\n    \"\"\"\n    # Define problem constants based on the problem statement\n    H = np.diag([1e6, 1.0, 1e-6])\n    # The condition number kappa(H) = max(eig)/min(eig) = 1e6 / 1e-6 = 1e12\n    sqrt_kappa_H = np.sqrt(1e12)  # This is 1e6\n\n    # Define the test cases\n    test_cases = [\n        {'x_k': np.array([1e-3, -1e-3, 2e-3]), 'delta_k': 1e-1, 'eta': 1e-16},\n        {'x_k': np.array([1e-3, -1e-3, 2e-3]), 'delta_k': 1e-4, 'eta': 1e-16},\n        {'x_k': np.array([2e-1, -1e-1, 5e-2]), 'delta_k': 2.2e-1, 'eta': 1e-16},\n        {'x_k': np.array([0.0, 0.0, 1e3]), 'delta_k': 1e-2, 'eta': 1e-16},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        x_k = case['x_k']\n        delta_k = case['delta_k']\n        eta = case['eta']\n        \n        # Gradient is g_k = Hx_k + b. With b=0, g_k = Hx_k.\n        g_k = H @ x_k\n        norm_g_k = np.linalg.norm(g_k, 2)\n\n        # Helper function for perturbed objective function f_tilde(x)\n        def f_pert(x):\n            # Since H is positive definite, x.T @ H @ x is always non-negative.\n            x_H_x = x.T @ H @ x\n            factor = 1.0 + eta * sqrt_kappa_H * np.linalg.norm(x, 2)\n            return 0.5 * x_H_x * factor\n\n        # Helper function to compute the rho ratio\n        def calculate_rho(s_k):\n            # Predicted reduction: pred_k = -(g_k^T * s_k + 0.5 * s_k^T * H * s_k)\n            pred_k = -(g_k.T @ s_k + 0.5 * s_k.T @ H @ s_k)\n            # Actual reduction: ared_k = f_tilde(x_k) - f_tilde(x_k + s_k)\n            ared_k = f_pert(x_k) - f_pert(x_k + s_k)\n            \n            if np.abs(pred_k) < 1e-30:\n                return 1.0 if np.abs(ared_k) < 1e-30 else np.sign(ared_k) * np.inf\n            return ared_k / pred_k\n\n        case_rhos = []\n\n        # --- Step 1: Cauchy Step ---\n        g_k_norm_sq = g_k.T @ g_k\n        g_k_H_g_k = g_k.T @ H @ g_k\n        \n        # Tau for unconstrained Cauchy step\n        tau_star = g_k_norm_sq / g_k_H_g_k if g_k_H_g_k > 0 else np.inf\n        s_UCP = -tau_star * g_k\n        norm_s_UCP = np.linalg.norm(s_UCP, 2)\n\n        if norm_s_UCP <= delta_k:\n            s_C = s_UCP\n        else:\n            s_C = -(delta_k / norm_g_k) * g_k\n        case_rhos.append(calculate_rho(s_C))\n        \n        # --- Step 2: Dogleg Step ---\n        # Newton step s_N = -H^-1 * g_k. Since g_k = Hx_k, s_N = -x_k.\n        s_N = -x_k\n        norm_s_N = np.linalg.norm(s_N, 2)\n\n        s_DL = None\n        if norm_s_N <= delta_k:\n            s_DL = s_N\n        elif norm_s_UCP >= delta_k:\n            s_DL = -(delta_k / norm_g_k) * g_k # Same as truncated Cauchy\n        else:\n            # Intersection of dogleg path with trust-region boundary\n            p_U = s_UCP\n            p_N = s_N\n            d = p_N - p_U\n            # Solve ||p_U + beta*d||^2 = delta_k^2 for beta\n            a = d.T @ d\n            b = 2 * (p_U.T @ d)\n            c = p_U.T @ p_U - delta_k**2\n            # Product of roots is c/a < 0, so one positive, one negative. We need the positive one.\n            beta = (-b + np.sqrt(b**2 - 4 * a * c)) / (2 * a)\n            s_DL = p_U + beta * d\n        case_rhos.append(calculate_rho(s_DL))\n\n        # --- Step 3: Exact TR Step ---\n        s_TR = None\n        if norm_s_N <= delta_k:\n            s_TR = s_N\n        else:\n            H_diag = np.diag(H)\n            def secular_eq(lam):\n                s_lam_components = g_k / (H_diag + lam)\n                return np.linalg.norm(s_lam_components, 2) - delta_k\n\n            # Find a bracketing interval [lam_low, lam_high] for the root finder\n            lam_low = 0.0\n            if secular_eq(1e-9) < 0: # Handle cases where lambda is very small\n                 lam_high = 1e-9\n            else:\n                 lam_high = 1.0\n                 while secular_eq(lam_high) > 0:\n                     lam_high *= 2.0\n            \n            sol = root_scalar(secular_eq, bracket=[lam_low, lam_high], method='brentq')\n            lam_star = sol.root\n            s_TR = -np.linalg.inv(H + lam_star * np.identity(3)) @ g_k\n        case_rhos.append(calculate_rho(s_TR))\n\n        all_results.append(case_rhos)\n    \n    # Format the final output string exactly as requested.\n    # The default str() for a list of lists matches the problem's example format: [ [r1, r2], [r3, r4] ]\n    print(str(all_results))\n\nsolve()\n```"
        },
        {
            "introduction": "一个熟练的实践者知道，优化不仅仅是应用公式，更在于解读结果。这最后一个练习  提出了一个场景，其中标准算法遇到了一个带有“扭结”的函数，导致模型预测能力很差，并产生了一个异常高的 $\\rho_k$ 值。您的任务是诊断这种情况，并设计一个比教科书中天真的更新规则更智能的算法响应，学会将比率测试作为一个强大的诊断工具。",
            "id": "3152655",
            "problem": "考虑一个用于无约束优化的信赖域方法，其模型为 $m_k(s) = f(x_k) + \\nabla f(x_k)^\\top s + \\tfrac{1}{2} s^\\top B_k s$，其中 $B_k$ 是 $\\nabla^2 f(x_k)$ 的近似。比率检验使用实际下降量和模型预测下降量，其定义为\n$$\\text{ared}_k = f(x_k) - f(x_k + s_k), \\quad \\text{pred}_k = m_k(0) - m_k(s_k) = -\\nabla f(x_k)^\\top s_k - \\tfrac{1}{2} s_k^\\top B_k s_k,$$\n以及比率\n$$\\rho_k = \\frac{\\text{ared}_k}{\\text{pred}_k}。$$\n假设目标函数 $f:\\mathbb{R}^2 \\to \\mathbb{R}$ 是分段三次函数，在 $x_1 = 0$ 处有一个拐折（kink），并且Hessian矩阵在该拐折处有跳跃：\n$$\nf(x) = \\begin{cases}\n\\tfrac{1}{3}\\alpha x_1^3 + \\tfrac{1}{2}\\beta_+ x_1^2 + \\gamma x_1 x_2 + \\tfrac{1}{2}\\delta x_2^2, & x_1 \\ge 0, \\\\\n\\tfrac{1}{3}\\alpha x_1^3 + \\tfrac{1}{2}\\beta_- x_1^2 + \\gamma x_1 x_2 + \\tfrac{1}{2}\\delta x_2^2, & x_1 < 0,\n\\end{cases}\n$$\n参数为 $\\alpha = 3$，$\\beta_+ = 4$，$\\beta_- = 1$，$\\gamma = -3$，以及 $\\delta = 2$。设当前迭代点为 $x_k = (0.05,\\, 0.2)$，信赖域半径为 $\\Delta_k = 1.0$。算法返回一个步长 $s_k = (-0.50,\\, -0.73)$，该步长穿过了拐折（因为 $x_{k,1} + s_{k,1} = -0.45 < 0$）。取 $B_k = \\nabla^2 f(x_k)$，接受阈值为 $\\eta_1 = 0.1$ 和 $\\eta_2 = 0.75$。\n\n任务：\n- 使用 $f$ 在 $x_1 \\ge 0$ 一侧的定义计算 $\\nabla f(x_k)$ 和 $B_k$。\n- 计算 $\\text{pred}_k$ 和 $\\text{ared}_k$。\n- 计算 $\\rho_k$ 并使用比率检验对其进行解释。\n- 在接受步长 $s_k$ 后，设计一个信赖域半径 $\\Delta_{k+1}$ 的更新规则，以避免重复穿越 $x_1 = 0$ 处的拐折。\n\n哪个选项正确报告了 $\\rho_k$ 的近似值，并给出了一个避免重复穿越 $x_1 = 0$ 的半径更新规则？\n\nA. $\\rho_k \\approx 0.24$。拒绝该步长并设置 $\\Delta_{k+1} = 0.5\\,\\Delta_k$。\n\nB. $\\rho_k \\approx 1.2$。接受该步长并设置 $\\Delta_{k+1} = 2\\,\\Delta_k$。\n\nC. $\\rho_k \\approx 40$。接受该步长并设置 $\\Delta_{k+1} := \\min\\{1.5\\,\\Delta_k,\\, 0.8\\,|x_{k+1,1}|\\}$ 以避免重复穿越 $x_1 = 0$。\n\nD. $\\rho_k \\approx 40$。接受该步长并设置 $\\Delta_{k+1} = 2\\,\\Delta_k$，因为高 $\\rho_k$ 值总是保证需要扩大半径。",
            "solution": "该问题陈述是数值优化中的一个有效练习。它呈现了一个明确定义、自成一体的场景，旨在测试信赖域算法在非光滑目标函数上的行为。所有参数和条件都已明确指定，问题是客观的且可以数学形式化的。\n\n我们将按概述的任务进行。\n\n目标函数 $f(x)$ 由下式给出：\n$$\nf(x) = \\begin{cases}\n\\tfrac{1}{3}\\alpha x_1^3 + \\tfrac{1}{2}\\beta_+ x_1^2 + \\gamma x_1 x_2 + \\tfrac{1}{2}\\delta x_2^2, & x_1 \\ge 0 \\\\\n\\tfrac{1}{3}\\alpha x_1^3 + \\tfrac{1}{2}\\beta_- x_1^2 + \\gamma x_1 x_2 + \\tfrac{1}{2}\\delta x_2^2, & x_1 < 0\n\\end{cases}\n$$\n使用参数 $\\alpha = 3$，$\\beta_+ = 4$，$\\beta_- = 1$，$\\gamma = -3$，以及 $\\delta = 2$，函数变为：\n$$\nf(x) = \\begin{cases}\nx_1^3 + 2 x_1^2 - 3 x_1 x_2 + x_2^2, & x_1 \\ge 0 \\\\\nx_1^3 + \\tfrac{1}{2} x_1^2 - 3 x_1 x_2 + x_2^2, & x_1 < 0\n\\end{cases}\n$$\n当前迭代点为 $x_k = (0.05, 0.2)$。建议的步长为 $s_k = (-0.50, -0.73)$。\n下一个点将是 $x_{k+1} = x_k + s_k = (0.05 - 0.50, 0.2 - 0.73) = (-0.45, -0.53)$。\n\n**1. 计算 $\\nabla f(x_k)$ 和 $B_k$。**\n\n由于 $x_{k,1} = 0.05 \\ge 0$，我们使用 $f(x)$ 对于 $x_1 \\ge 0$ 的定义来计算在 $x_k$ 处的梯度和Hessian矩阵。\n对于 $x_1 > 0$，梯度为：\n$$ \\nabla f(x) = \\begin{pmatrix} 3x_1^2 + 4x_1 - 3x_2 \\\\ -3x_1 + 2x_2 \\end{pmatrix} $$\n在 $x_k = (0.05, 0.2)$ 处求值：\n$$ \\nabla f(x_k) = \\begin{pmatrix} 3(0.05)^2 + 4(0.05) - 3(0.2) \\\\ -3(0.05) + 2(0.2) \\end{pmatrix} = \\begin{pmatrix} 3(0.0025) + 0.2 - 0.6 \\\\ -0.15 + 0.4 \\end{pmatrix} = \\begin{pmatrix} 0.0075 - 0.4 \\\\ 0.25 \\end{pmatrix} = \\begin{pmatrix} -0.3925 \\\\ 0.25 \\end{pmatrix} $$\n模型Hessian矩阵 $B_k$ 是真实的Hessian矩阵 $\\nabla^2 f(x_k)$。对于 $x_1 > 0$，Hessian矩阵为：\n$$ \\nabla^2 f(x) = \\begin{pmatrix} 6x_1 + 4 & -3 \\\\ -3 & 2 \\end{pmatrix} $$\n在 $x_k = (0.05, 0.2)$ 处求值：\n$$ B_k = \\nabla^2 f(x_k) = \\begin{pmatrix} 6(0.05) + 4 & -3 \\\\ -3 & 2 \\end{pmatrix} = \\begin{pmatrix} 0.3 + 4 & -3 \\\\ -3 & 2 \\end{pmatrix} = \\begin{pmatrix} 4.3 & -3 \\\\ -3 & 2 \\end{pmatrix} $$\n\n**2. 计算 $\\text{pred}_k$ 和 $\\text{ared}_k$。**\n\n预测下降量为 $\\text{pred}_k = -\\nabla f(x_k)^\\top s_k - \\tfrac{1}{2} s_k^\\top B_k s_k$。\n第一项是：\n$$ -\\nabla f(x_k)^\\top s_k = - \\begin{pmatrix} -0.3925 & 0.25 \\end{pmatrix} \\begin{pmatrix} -0.50 \\\\ -0.73 \\end{pmatrix} = - ((-0.3925)(-0.50) + (0.25)(-0.73)) = - (0.19625 - 0.1825) = -0.01375 $$\n第二项涉及二次型 $s_k^\\top B_k s_k$：\n$$ s_k^\\top B_k s_k = \\begin{pmatrix} -0.50 & -0.73 \\end{pmatrix} \\begin{pmatrix} 4.3 & -3 \\\\ -3 & 2 \\end{pmatrix} \\begin{pmatrix} -0.50 \\\\ -0.73 \\end{pmatrix} $$\n$$ = \\begin{pmatrix} (-0.50)(4.3) + (-0.73)(-3) & (-0.50)(-3) + (-0.73)(2) \\end{pmatrix} \\begin{pmatrix} -0.50 \\\\ -0.73 \\end{pmatrix} $$\n$$ = \\begin{pmatrix} -2.15 + 2.19 & 1.5 - 1.46 \\end{pmatrix} \\begin{pmatrix} -0.50 \\\\ -0.73 \\end{pmatrix} = \\begin{pmatrix} 0.04 & 0.04 \\end{pmatrix} \\begin{pmatrix} -0.50 \\\\ -0.73 \\end{pmatrix} $$\n$$ = (0.04)(-0.50) + (0.04)(-0.73) = -0.02 - 0.0292 = -0.0492 $$\n所以，$\\text{pred}_k$ 的第二项是 $-\\tfrac{1}{2} s_k^\\top B_k s_k = -\\tfrac{1}{2}(-0.0492) = 0.0246$。\n因此，预测下降量为：\n$$ \\text{pred}_k = -0.01375 + 0.0246 = 0.01085 $$\n实际下降量为 $\\text{ared}_k = f(x_k) - f(x_k + s_k)$。\n首先，我们使用 $x_1 \\ge 0$ 的公式计算 $f(x_k) = f(0.05, 0.2)$：\n$$ f(0.05, 0.2) = (0.05)^3 + 2(0.05)^2 - 3(0.05)(0.2) + (0.2)^2 = 0.000125 + 2(0.0025) - 0.03 + 0.04 = 0.015125 $$\n接下来，我们使用 $x_1 < 0$ 的公式计算 $f(x_k + s_k) = f(-0.45, -0.53)$：\n$$ f(-0.45, -0.53) = (-0.45)^3 + \\tfrac{1}{2}(-0.45)^2 - 3(-0.45)(-0.53) + (-0.53)^2 $$\n$$ = -0.091125 + \\tfrac{1}{2}(0.2025) - 3(0.2385) + 0.2809 $$\n$$ = -0.091125 + 0.10125 - 0.7155 + 0.2809 = 0.010125 - 0.4346 = -0.424475 $$\n实际下降量为：\n$$ \\text{ared}_k = 0.015125 - (-0.424475) = 0.4396 $$\n\n**3. 计算 $\\rho_k$ 并对其进行解释。**\n\n比率为：\n$$ \\rho_k = \\frac{\\text{ared}_k}{\\text{pred}_k} = \\frac{0.4396}{0.01085} \\approx 40.516 $$\n该值约等于 $40$。\n接受阈值为 $\\eta_1 = 0.1$ 和 $\\eta_2 = 0.75$。由于 $\\rho_k \\approx 40 > \\eta_2$，该步长被认为是“非常成功”的，应该被接受：$x_{k+1} = x_k + s_k$。\n\n**4. 设计信赖域半径 $\\Delta_{k+1}$ 的更新规则。**\n\n$\\rho_k$ 的极大值表明模型 $m_k(s)$ 与真实函数 $f(x)$ 在步长 $s_k$ 方向上存在严重不匹配。这种不匹配的发生是因为模型是在 $x_k$（位于 $x_1 > 0$ 区域）处构建的，而步长却进入了 $x_1 < 0$ 的区域，函数在该区域有不同的定义（和不同的Hessian矩阵）。一个标准的信赖域算法可能会因为这一非常成功的步长而扩大半径（例如，$\\Delta_{k+1} = 2\\Delta_k$）。然而，这将是一个天真的响应。在新点 $x_{k+1}$（其中 $x_{k+1,1} = -0.45 < 0$）处的一个大信赖域，很可能导致新的一步再次穿越拐折回到 $x_1 > 0$ 的区域，从而导致另一次糟糕的模型预测，并可能在拐折附近产生振荡。\n\n一个更精妙的策略是接受该步长，但为下一次迭代选择一个足够小的信赖域半径，以防止立即穿越回拐折的另一侧。这使得算法能够构建一个新模型 $m_{k+1}(s)$，该模型在 $x_{k+1}$ 周围的局部区域是准确的。一个合理的启发式方法是，将新半径 $\\Delta_{k+1}$ 设置为不大于新点到拐折的距离，并带有一个安全系数。从 $x_{k+1}$ 到直线 $x_1 = 0$ 的距离是 $|x_{k+1,1}| = |-0.45| = 0.45$。所以，我们应该要求 $\\Delta_{k+1} \\le c |x_{k+1,1}|$，其中某个 $c < 1$ （例如，$c = 0.8$）。\n\n现在我们根据这些发现来评估各个选项。\n\n**选项 A: $\\rho_k \\approx 0.24$。拒绝该步长并设置 $\\Delta_{k+1} = 0.5\\,\\Delta_k$。**\n$\\rho_k \\approx 0.24$ 这个值是错误的。我们的计算得出 $\\rho_k \\approx 40$。由于 $\\rho_k > \\eta_1$，该步长应该被接受，而不是被拒绝。\n**结论：错误。**\n\n**选项 B: $\\rho_k \\approx 1.2$。接受该步长并设置 $\\Delta_{k+1} = 2\\,\\Delta_k$。**\n$\\rho_k \\approx 1.2$ 这个值是错误的。我们的计算得出 $\\rho_k \\approx 40$。\n**结论：错误。**\n\n**选项 C: $\\rho_k \\approx 40$。接受该步长并设置 $\\Delta_{k+1} := \\min\\{1.5\\,\\Delta_k,\\, 0.8\\,|x_{k+1,1}|\\}$ 以避免重复穿越 $x_1 = 0$。**\n$\\rho_k \\approx 40$ 这个值与我们的计算相符。`接受该步长`是正确的操作，因为 $\\rho_k > \\eta_2$。建议的半径更新 $\\Delta_{k+1} := \\min\\{1.5\\,\\Delta_k,\\, 0.8\\,|x_{k+1,1}|\\}$ 实现了上述的精妙启发式策略。它将成功步长的标准半径扩大（$1.5\\,\\Delta_k = 1.5$）与一个防止穿越拐折的约束（$0.8\\,|x_{k+1,1}| = 0.8 \\cdot 0.45 = 0.36$）结合起来。新半径将是 $\\Delta_{k+1} = \\min\\{1.5, 0.36\\} = 0.36$。对于这个具体问题，这是一个绝佳的算法选择。\n**结论：正确。**\n\n**选项 D: $\\rho_k \\approx 40$。接受该步长并设置 $\\Delta_{k+1} = 2\\,\\Delta_k$ 因为高 $\\rho_k$ 值总是保证需要扩大半径。**\n$\\rho_k \\approx 40$ 这个值和`接受该步长`的操作是正确的。然而，其推理和建议的半径更新是有缺陷的。一个非常高的 $\\rho_k$ 值表明模型非常糟糕，盲目扩大信赖域可能会使穿越不连续性的问题永久化。认为高 $\\rho_k$ 值`总是`保证需要扩大半径的理由是一种错误且天真的启发式方法。选项C提供了一个更为稳健的策略。\n**结论：错误。**",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}