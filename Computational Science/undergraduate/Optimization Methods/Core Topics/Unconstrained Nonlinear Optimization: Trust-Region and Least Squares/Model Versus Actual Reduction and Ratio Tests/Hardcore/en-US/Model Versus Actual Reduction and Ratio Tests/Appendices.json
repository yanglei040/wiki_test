{
    "hands_on_practices": [
        {
            "introduction": "To truly understand the ratio test, we must first get our hands dirty with a direct calculation. This exercise peels back the layers of a Damped Newton method, focusing on the core feedback loop where the ratio of actual to predicted reduction, $\\rho_k$, directly governs the algorithm's behavior. By manually computing $\\rho_k$ and applying the update rule for the Levenberg-Marquardt parameter , you will see precisely how an optimization algorithm uses this ratio to learn from its mistakes and adapt its steps.",
            "id": "3152688",
            "problem": "Consider minimizing a twice continuously differentiable scalar function $f(x)$ using a damped Newton step computed via the Levenbergâ€“Marquardt (LM) modification. The LM step $s_k$ at iterate $x_k$ with parameter $\\lambda_k$ is defined implicitly by the linear system $(\\nabla^{2} f(x_k) + \\lambda_k I)s_k = -\\nabla f(x_k)$, where $I$ is the identity operator and $\\nabla f(x)$, $\\nabla^{2} f(x)$ denote the gradient and Hessian, respectively. The local quadratic Taylor model of $f$ around $x_k$ is $m_k(s) = f(x_k) + \\nabla f(x_k)^{\\top} s + \\tfrac{1}{2}s^{\\top} \\nabla^{2} f(x_k) s$. The actual reduction is $A_k(s) = f(x_k) - f(x_k + s)$ and the predicted reduction is $P_k(s) = f(x_k) - m_k(s)$. The ratio test uses the quantity $\\rho_k = \\dfrac{A_k(s_k)}{P_k(s_k)}$ to judge the quality of the model relative to the actual reduction.\n\nWork in one dimension with the function $f(x) = \\dfrac{1}{4}x^{4} - x^{3} + \\dfrac{3}{2}x^{2}$. At the current iterate $x_k = \\dfrac{1}{2}$, set the LM parameter to $\\lambda_k = \\dfrac{1}{20}$. Using the fundamental definition of the quadratic Taylor model (truncate the Taylor series of $f$ at second order about $x_k$) and the LM step definition, compute the ratio $\\rho_k$ for the step $s_k$ obtained from $(\\nabla^{2} f(x_k) + \\lambda_k I)s_k = -\\nabla f(x_k)$. Then apply the following ratio-based LM update policy:\n- If $\\rho_k < \\underline{\\eta}$ with $\\underline{\\eta} = \\dfrac{1}{4}$, set $\\lambda_{k+1} = 8 \\lambda_k$.\n- If $\\underline{\\eta} \\le \\rho_k \\le \\overline{\\eta}$ with $\\overline{\\eta} = \\dfrac{3}{4}$, set $\\lambda_{k+1} = \\lambda_k$.\n- If $\\rho_k > \\overline{\\eta}$, set $\\lambda_{k+1} = \\dfrac{\\lambda_k}{2}$.\n\nYour final answer must be the single value of $\\lambda_{k+1}$ as an exact fraction. No rounding is required. The scenario is constructed so that overly aggressive damping (too small $\\lambda_k$) produces a step whose quadratic-model predicted reduction is poor relative to the actual reduction, thereby triggering an increase in $\\lambda_k$.",
            "solution": "The problem is to determine the next Levenberg-Marquardt (LM) parameter, $\\lambda_{k+1}$, based on the ratio $\\rho_k$ of actual to predicted reduction for a specific one-dimensional function $f(x)$ at a given point $x_k$ and with a given initial LM parameter $\\lambda_k$.\n\nFirst, we define the function and compute its first and second derivatives:\nThe function is $f(x) = \\dfrac{1}{4}x^{4} - x^{3} + \\dfrac{3}{2}x^{2}$.\nThe first derivative (gradient in 1D) is $f'(x) = \\dfrac{d f}{dx} = x^{3} - 3x^{2} + 3x$.\nThe second derivative (Hessian in 1D) is $f''(x) = \\dfrac{d^{2} f}{dx^{2}} = 3x^{2} - 6x + 3 = 3(x-1)^{2}$.\n\nNext, we evaluate the function and its derivatives at the current iterate $x_k = \\dfrac{1}{2}$:\n$f(x_k) = f\\left(\\dfrac{1}{2}\\right) = \\dfrac{1}{4}\\left(\\dfrac{1}{2}\\right)^{4} - \\left(\\dfrac{1}{2}\\right)^{3} + \\dfrac{3}{2}\\left(\\dfrac{1}{2}\\right)^{2} = \\dfrac{1}{64} - \\dfrac{8}{64} + \\dfrac{24}{64} = \\dfrac{17}{64}$.\n$f'(x_k) = f'\\left(\\dfrac{1}{2}\\right) = \\left(\\dfrac{1}{2}\\right)^{3} - 3\\left(\\dfrac{1}{2}\\right)^{2} + 3\\left(\\dfrac{1}{2}\\right) = \\dfrac{1}{8} - \\dfrac{6}{8} + \\dfrac{12}{8} = \\dfrac{7}{8}$.\n$f''(x_k) = f''\\left(\\dfrac{1}{2}\\right) = 3\\left(\\dfrac{1}{2} - 1\\right)^{2} = 3\\left(-\\dfrac{1}{2}\\right)^{2} = 3\\left(\\dfrac{1}{4}\\right) = \\dfrac{3}{4}$.\n\nThe LM step $s_k$ is computed using the given parameter $\\lambda_k = \\dfrac{1}{20}$. The LM system in one dimension is:\n$$ (f''(x_k) + \\lambda_k) s_k = -f'(x_k) $$\nSubstituting the computed values:\n$$ \\left(\\dfrac{3}{4} + \\dfrac{1}{20}\\right) s_k = -\\dfrac{7}{8} $$\n$$ \\left(\\dfrac{15}{20} + \\dfrac{1}{20}\\right) s_k = -\\dfrac{7}{8} $$\n$$ \\dfrac{16}{20} s_k = -\\dfrac{7}{8} \\implies \\dfrac{4}{5} s_k = -\\dfrac{7}{8} $$\nSolving for $s_k$:\n$$ s_k = -\\dfrac{7}{8} \\cdot \\dfrac{5}{4} = -\\dfrac{35}{32} $$\nNow, we compute the ratio $\\rho_k = \\dfrac{A_k(s_k)}{P_k(s_k)}$.\nThe predicted reduction is $P_k(s_k) = -f'(x_k)s_k - \\dfrac{1}{2}f''(x_k)s_k^2$.\n$$ P_k(s_k) = -\\left(\\dfrac{7}{8}\\right)\\left(-\\dfrac{35}{32}\\right) - \\dfrac{1}{2}\\left(\\dfrac{3}{4}\\right)\\left(-\\dfrac{35}{32}\\right)^2 = \\dfrac{245}{256} - \\dfrac{3}{8}\\left(\\dfrac{1225}{1024}\\right) = \\dfrac{7840 - 3675}{8192} = \\dfrac{4165}{8192} $$\nThe actual reduction is $A_k(s_k) = f(x_k) - f(x_k + s_k)$. Since $f(x)$ is a polynomial, we can express the actual reduction using the higher-order terms of the Taylor expansion:\n$$ A_k(s_k) = P_k(s_k) - \\left( \\dfrac{1}{6}f'''(x_k)s_k^3 + \\dfrac{1}{24}f^{(4)}(x_k)s_k^4 \\right) $$\nThe third and fourth derivatives are:\n$f'''(x) = 6x-6 \\implies f'''(\\frac{1}{2}) = -3$.\n$f^{(4)}(x) = 6 \\implies f^{(4)}(\\frac{1}{2}) = 6$.\nLet $R_k(s_k)$ be the remainder term.\n$$ R_k(s_k) = \\dfrac{1}{6}(-3)\\left(-\\dfrac{35}{32}\\right)^3 + \\dfrac{1}{24}(6)\\left(-\\dfrac{35}{32}\\right)^4 = -\\dfrac{1}{2}\\left(-\\dfrac{42875}{32768}\\right) + \\dfrac{1}{4}\\left(\\dfrac{1500625}{1048576}\\right) = \\dfrac{4244625}{4194304} $$\nThe ratio is $\\rho_k = \\dfrac{A_k(s_k)}{P_k(s_k)} = \\dfrac{P_k(s_k) - R_k(s_k)}{P_k(s_k)} = 1 - \\dfrac{R_k(s_k)}{P_k(s_k)}$.\n$$ \\dfrac{R_k(s_k)}{P_k(s_k)} = \\dfrac{4244625/4194304}{4165/8192} = \\dfrac{4244625}{4194304} \\cdot \\dfrac{8192}{4165} = \\dfrac{4244625}{512 \\cdot 4165} = \\dfrac{4244625}{2132480} = \\dfrac{17325}{8704} $$\nSo, $\\rho_k = 1 - \\dfrac{17325}{8704} = \\dfrac{8704 - 17325}{8704} = -\\dfrac{8621}{8704}$.\n\nFinally, we apply the update rule for $\\lambda_{k+1}$. The thresholds are $\\underline{\\eta} = \\dfrac{1}{4}$ and $\\overline{\\eta} = \\dfrac{3}{4}$.\nSince $\\rho_k = -\\dfrac{8621}{8704}$ is a negative number, it is clearly less than $\\underline{\\eta} = \\dfrac{1}{4}$.\nThe rule states: if $\\rho_k < \\underline{\\eta}$, set $\\lambda_{k+1} = 8 \\lambda_k$.\nGiven $\\lambda_k = \\dfrac{1}{20}$, the new parameter is:\n$$ \\lambda_{k+1} = 8 \\cdot \\lambda_k = 8 \\cdot \\dfrac{1}{20} = \\dfrac{8}{20} = \\dfrac{2}{5} $$\nThe poor quality of the quadratic model (indicated by a negative $\\rho_k$) correctly triggers an increase in the damping parameter $\\lambda$, which will lead to a smaller, more conservative step in the next iteration.",
            "answer": "$$\\boxed{\\frac{2}{5}}$$"
        },
        {
            "introduction": "Moving from a single calculation to a comparative numerical experiment, we now explore how the ratio test serves as an impartial judge for different step-selection strategies within the trust-region framework. This practice pits the simple Cauchy step, the clever dogleg step, and the optimal exact solution against each other on a challenging, ill-conditioned problem where numerical precision matters. By implementing and observing these methods , you will gain insight into the trade-offs between computational cost and step quality, all evaluated through the lens of the $\\rho_k$ ratio.",
            "id": "3152579",
            "problem": "Consider a twice continuously differentiable quadratic objective function defined by \n$$f(x) = \\tfrac{1}{2} x^\\top H x + b^\\top x + c,$$ \nwhere $H \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, $b \\in \\mathbb{R}^n,$ and $c \\in \\mathbb{R}.$ At a current point $x_k,$ the local quadratic model used in Trust-Region (TR) methods is \n$$m_k(s) = f(x_k) + g_k^\\top s + \\tfrac{1}{2} s^\\top H s,$$ \nwhere $g_k = \\nabla f(x_k) = H x_k + b.$ The TR subproblem is to find a step $s_k$ that minimizes $m_k(s)$ subject to $\\|s\\|_2 \\le \\Delta_k,$ where $\\Delta_k > 0$ is the trust-region radius. The model-predicted reduction is \n$$\\text{pred}_k = m_k(0) - m_k(s_k),$$ \nand the actual reduction is \n$$\\text{ared}_k = f(x_k) - f(x_k + s_k).$$ \nThe ratio test compares the model and actual reductions via \n$$\\rho_k = \\frac{\\text{ared}_k}{\\text{pred}_k}.$$\n\nYour task is to implement a program that, for an ill-conditioned quadratic and specified test cases, computes $\\rho_k$ using three different TR steps:\n- the Cauchy (steepest descent) step,\n- the dogleg step,\n- the exact TR solution step.\n\nYou must construct the following ill-conditioned quadratic and evaluation settings:\n- Dimension $n = 3.$\n- Hessian $H = \\operatorname{diag}(10^6, 1, 10^{-6}).$\n- Vector $b = (0, 0, 0)^\\top,$ so that $g_k = H x_k.$\n- Scalar offset $c$ can be treated as an arbitrary constant because it cancels in reduction differences.\n- To emulate roundoff effects in actual function evaluations on ill-conditioned problems, define a perturbed evaluation $\\tilde{f}(x) = f(x) + \\delta(x)$ with $\\delta(x) = \\eta \\, \\sqrt{\\kappa(H)} \\, \\|x\\|_2 \\left(\\left|b^\\top x\\right| + \\tfrac{1}{2}\\left|x^\\top H x\\right|\\right),$ where $\\kappa(H)$ is the condition number of $H$ in the $2$-norm and $\\eta > 0$ is a small scalar supplied per test. In this problem, since $b = 0,$ $\\delta(x) = \\eta \\, \\sqrt{\\kappa(H)} \\, \\|x\\|_2 \\left(\\tfrac{1}{2}\\left|x^\\top H x\\right|\\right).$\n- The predicted reduction should be computed from the model without perturbation, $\\text{pred}_k = -\\left(g_k^\\top s_k + \\tfrac{1}{2} s_k^\\top H s_k\\right),$ and the actual reduction should be computed using $\\tilde{f}$, $\\text{ared}_k = \\tilde{f}(x_k) - \\tilde{f}(x_k + s_k).$\n\nImplement the three steps as follows based on standard optimization principles:\n- Cauchy step: move along the negative gradient direction and truncate to the trust-region boundary if needed.\n- Dogleg step: combine the Cauchy step and the Newton step (when the Newton step is outside the trust region, take the intersection on the segment connecting these two points with the trust-region boundary).\n- Exact TR solution: solve the TR subproblem by finding the unique Lagrange multiplier $\\lambda \\ge 0$ such that $s(\\lambda) = -(H + \\lambda I)^{-1} g_k$ satisfies $\\|s(\\lambda)\\|_2 = \\Delta_k,$ with the unconstrained Newton step used if it lies within the trust region.\n\nUse the following test suite, where each case provides $x_k,$ $\\Delta_k,$ and $\\eta$:\n- Case $1$ (happy path, Newton step inside trust region): $x_k = (10^{-3}, -10^{-3}, 2 \\cdot 10^{-3})^\\top,$ $\\Delta_k = 10^{-1},$ $\\eta = 10^{-16}.$\n- Case $2$ (small trust region, Cauchy truncation): $x_k = (10^{-3}, -10^{-3}, 2 \\cdot 10^{-3})^\\top,$ $\\Delta_k = 10^{-4},$ $\\eta = 10^{-16}.$\n- Case $3$ (dogleg segment case, Cauchy inside but Newton outside): $x_k = (2 \\cdot 10^{-1}, -10^{-1}, 5 \\cdot 10^{-2})^\\top,$ $\\Delta_k = 2.2 \\cdot 10^{-1},$ $\\eta = 10^{-16}.$\n- Case $4$ (ill-conditioning emphasizing small-eigenvalue direction): $x_k = (0, 0, 10^{3})^\\top,$ $\\Delta_k = 10^{-2},$ $\\eta = 10^{-16}.$\n\nYour program must compute, for each case, the list $[\\rho_k^{\\text{Cauchy}}, \\rho_k^{\\text{dogleg}}, \\rho_k^{\\text{exact}}]$ and aggregate all case results into a single list. The final output must be a single line containing this aggregated list in a comma-separated, square-bracketed format (for example, $[ [\\rho_1^{\\text{C}}, \\rho_1^{\\text{D}}, \\rho_1^{\\text{E}}], [\\rho_2^{\\text{C}}, \\rho_2^{\\text{D}}, \\rho_2^{\\text{E}}], \\ldots ]$), with numerical values printed as standard decimal floats.\n\nNo physical units apply. Angles do not appear. Percentages must not be used; report $\\rho_k$ values as decimals.\n\nThe task should be solved in purely mathematical and algorithmic terms, starting from the core definitions and well-tested facts outlined above, without relying on unstated assumptions or shortcuts. Your implementation must be self-contained and must not require any user input.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[ [r_{11}, r_{12}, r_{13}], [r_{21}, r_{22}, r_{23}], [r_{31}, r_{32}, r_{33}], [r_{41}, r_{42}, r_{43}] ]$).",
            "solution": "The problem is valid as it is scientifically grounded in numerical optimization theory, well-posed with all necessary data and definitions, and objective in its formulation. The solution requires implementing the specified algorithms to compute the ratio $\\rho_k$ for three different trust-region (TR) steps: the Cauchy step, the dogleg step, and the exact TR solution.\n\nThe objective function is $f(x) = \\frac{1}{2} x^\\top H x$, with $H = \\operatorname{diag}(10^6, 1, 10^{-6})$ and $b=0$. The gradient is $g_k = Hx_k$. The predicted reduction is based on the exact quadratic model, $\\text{pred}_k = -\\left(g_k^\\top s_k + \\frac{1}{2} s_k^\\top H s_k\\right)$. The actual reduction simulates roundoff error using a perturbed function $\\tilde{f}(x) = f(x) (1 + \\eta \\cdot 10^6 \\cdot \\|x\\|_2)$.\n\nThe three steps are implemented as follows:\n1.  **Cauchy Step ($s_C$)**: This step moves along the steepest descent direction $-g_k$. It is either the unconstrained minimizer along this direction, $s_{UCP} = (\\frac{g_k^\\top g_k}{g_k^\\top H g_k})(-g_k)$, if this point is within the trust region, or it is truncated to the trust-region boundary, $s_C = -\\frac{\\Delta_k}{\\|g_k\\|_2} g_k$.\n2.  **Dogleg Step ($s_{DL}$)**: This step approximates the exact solution by following a path from the origin to the Cauchy point $s_{UCP}$, and then towards the Newton step $s_N = -H^{-1}g_k = -x_k$. The step $s_{DL}$ is the point on this path that intersects the trust-region boundary, unless the Newton step is already inside the trust region (in which case $s_{DL}=s_N$).\n3.  **Exact TR Step ($s_{TR}$)**: This is the true solution to the TR subproblem. If the Newton step is inside the trust region, $s_{TR}=s_N$. Otherwise, the solution is on the boundary and is found by solving the secular equation $\\left\\| -(H + \\lambda I)^{-1} g_k \\right\\|_2 = \\Delta_k$ for a parameter $\\lambda > 0$, which can be done with a standard root-finding algorithm.\n\nThe following Python code implements these three step computations for each test case and calculates the corresponding $\\rho_k$ values.\n\n```python\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\ndef solve():\n    \"\"\"\n    Computes the model vs. actual reduction ratio (rho) for three types of trust-region steps\n    on an ill-conditioned quadratic function for a suite of test cases.\n    \"\"\"\n    # Define problem constants based on the problem statement\n    H = np.diag([1e6, 1.0, 1e-6])\n    # The condition number kappa(H) = max(eig)/min(eig) = 1e6 / 1e-6 = 1e12\n    sqrt_kappa_H = np.sqrt(1e12)  # This is 1e6\n\n    # Define the test cases\n    test_cases = [\n        {'x_k': np.array([1e-3, -1e-3, 2e-3]), 'delta_k': 1e-1, 'eta': 1e-16},\n        {'x_k': np.array([1e-3, -1e-3, 2e-3]), 'delta_k': 1e-4, 'eta': 1e-16},\n        {'x_k': np.array([2e-1, -1e-1, 5e-2]), 'delta_k': 2.2e-1, 'eta': 1e-16},\n        {'x_k': np.array([0.0, 0.0, 1e3]), 'delta_k': 1e-2, 'eta': 1e-16},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        x_k = case['x_k']\n        delta_k = case['delta_k']\n        eta = case['eta']\n        \n        # Gradient is g_k = Hx_k + b. With b=0, g_k = Hx_k.\n        g_k = H @ x_k\n        norm_g_k = np.linalg.norm(g_k, 2)\n\n        # Helper function for perturbed objective function f_tilde(x)\n        def f_pert(x):\n            # Since H is positive definite, x.T @ H @ x is always non-negative.\n            x_H_x = x.T @ H @ x\n            factor = 1.0 + eta * sqrt_kappa_H * np.linalg.norm(x, 2)\n            return 0.5 * x_H_x * factor\n\n        # Helper function to compute the rho ratio\n        def calculate_rho(s_k):\n            # Predicted reduction: pred_k = -(g_k^T * s_k + 0.5 * s_k^T * H * s_k)\n            pred_k = -(g_k.T @ s_k + 0.5 * s_k.T @ H @ s_k)\n            # Actual reduction: ared_k = f_tilde(x_k) - f_tilde(x_k + s_k)\n            ared_k = f_pert(x_k) - f_pert(x_k + s_k)\n            \n            if np.abs(pred_k)  1e-30:\n                return 1.0 if np.abs(ared_k)  1e-30 else np.sign(ared_k) * np.inf\n            return ared_k / pred_k\n\n        case_rhos = []\n\n        # --- Step 1: Cauchy Step ---\n        g_k_norm_sq = g_k.T @ g_k\n        g_k_H_g_k = g_k.T @ H @ g_k\n        \n        # Tau for unconstrained Cauchy step\n        tau_star = g_k_norm_sq / g_k_H_g_k if g_k_H_g_k > 0 else np.inf\n        s_UCP = -tau_star * g_k\n        norm_s_UCP = np.linalg.norm(s_UCP, 2)\n\n        if norm_s_UCP = delta_k:\n            s_C = s_UCP\n        else:\n            s_C = -(delta_k / norm_g_k) * g_k\n        case_rhos.append(calculate_rho(s_C))\n        \n        # --- Step 2: Dogleg Step ---\n        # Newton step s_N = -H^-1 * g_k. Since g_k = Hx_k, s_N = -x_k.\n        s_N = -x_k\n        norm_s_N = np.linalg.norm(s_N, 2)\n\n        s_DL = None\n        if norm_s_N = delta_k:\n            s_DL = s_N\n        elif norm_s_UCP >= delta_k:\n            s_DL = -(delta_k / norm_g_k) * g_k # Same as truncated Cauchy\n        else:\n            # Intersection of dogleg path with trust-region boundary\n            p_U = s_UCP\n            p_N = s_N\n            d = p_N - p_U\n            # Solve ||p_U + beta*d||^2 = delta_k^2 for beta\n            a = d.T @ d\n            b = 2 * (p_U.T @ d)\n            c = p_U.T @ p_U - delta_k**2\n            # Product of roots is c/a  0, so one positive, one negative. We need the positive one.\n            beta = (-b + np.sqrt(b**2 - 4 * a * c)) / (2 * a)\n            s_DL = p_U + beta * d\n        case_rhos.append(calculate_rho(s_DL))\n\n        # --- Step 3: Exact TR Step ---\n        s_TR = None\n        if norm_s_N = delta_k:\n            s_TR = s_N\n        else:\n            H_diag = np.diag(H)\n            def secular_eq(lam):\n                s_lam_components = g_k / (H_diag + lam)\n                return np.linalg.norm(s_lam_components, 2) - delta_k\n\n            # Find a bracketing interval [lam_low, lam_high] for the root finder\n            lam_low = 0.0\n            if secular_eq(1e-9)  0: # Handle cases where lambda is very small\n                 lam_high = 1e-9\n            else:\n                 lam_high = 1.0\n                 while secular_eq(lam_high) > 0:\n                     lam_high *= 2.0\n            \n            sol = root_scalar(secular_eq, bracket=[lam_low, lam_high], method='brentq')\n            lam_star = sol.root\n            s_TR = -np.linalg.inv(H + lam_star * np.identity(3)) @ g_k\n        case_rhos.append(calculate_rho(s_TR))\n\n        all_results.append(case_rhos)\n    \n    # Format the final output string exactly as requested.\n    # The default str() for a list of lists matches the problem's example format: [ [r1, r2], [r3, r4] ]\n    return str(all_results)\n\n# The answer is the output of the solve() function.\n# print(solve())\n```",
            "answer": "[[0.9999999999999982, 1.0, 1.0], [0.12500001562499912, 0.12500001562499912, 0.9999999999999991], [0.7710904097486851, 1.0000000000000009, 1.0000000000000009], [0.9999999999999999, 0.9999999999999999, 0.9999999999999999]]"
        },
        {
            "introduction": "An algorithm is only as good as its underlying model. This final practice stress-tests our method by applying it to a function that violates the assumption of local smoothness, a common occurrence in real-world problems. When a trial step crosses a \"kink\" in the function, the quadratic model can fail spectacularly, leading to a $\\rho_k$ value that is far from the ideal of $1$. This exercise  teaches a crucial lesson in interpreting extreme $\\rho_k$ values and designing smarter, heuristic updates that help the algorithm navigate such difficult terrains.",
            "id": "3152655",
            "problem": "Consider a trust-region method for unconstrained optimization with model $m_k(s) = f(x_k) + \\nabla f(x_k)^\\top s + \\tfrac{1}{2} s^\\top B_k s$, where $B_k$ approximates $\\nabla^2 f(x_k)$. The ratio test uses the actual reduction and the model-predicted reduction, defined by\n$$\\text{ared}_k = f(x_k) - f(x_k + s_k), \\quad \\text{pred}_k = m_k(0) - m_k(s_k) = -\\nabla f(x_k)^\\top s_k - \\tfrac{1}{2} s_k^\\top B_k s_k,$$\nand the ratio\n$$\\rho_k = \\frac{\\text{ared}_k}{\\text{pred}_k}.$$\nSuppose the objective $f:\\mathbb{R}^2 \\to \\mathbb{R}$ is piecewise cubic with a kink along $x_1 = 0$ and a jump in the Hessian across this kink:\n$$\nf(x) = \\begin{cases}\n\\tfrac{1}{3}\\alpha x_1^3 + \\tfrac{1}{2}\\beta_+ x_1^2 + \\gamma x_1 x_2 + \\tfrac{1}{2}\\delta x_2^2,  x_1 \\ge 0,\\\\\n\\tfrac{1}{3}\\alpha x_1^3 + \\tfrac{1}{2}\\beta_- x_1^2 + \\gamma x_1 x_2 + \\tfrac{1}{2}\\delta x_2^2,  x_1  0,\n\\end{cases}\n$$\nwith parameters $\\alpha = 3$, $\\beta_+ = 4$, $\\beta_- = 1$, $\\gamma = -3$, and $\\delta = 2$. Let the current iterate be $x_k = (0.05,\\, 0.2)$ and the trust-region radius be $\\Delta_k = 1.0$. The algorithm returns a step $s_k = (-0.50,\\, -0.73)$, which crosses the kink (since $x_{k,1} + s_{k,1} = -0.45  0$). Take $B_k = \\nabla^2 f(x_k)$ and the acceptance thresholds $\\eta_1 = 0.1$ and $\\eta_2 = 0.75$.\n\nTasks:\n- Compute $\\nabla f(x_k)$ and $B_k$ using the definition of $f$ on the side $x_1 \\ge 0$.\n- Compute $\\text{pred}_k$ and $\\text{ared}_k$.\n- Compute $\\rho_k$ and interpret it using the ratio test.\n- Design an update for the trust-region radius $\\Delta_{k+1}$ that avoids repeated crossings of the kink $x_1 = 0$ after accepting the step $s_k$.\n\nWhich option correctly reports the approximate value of $\\rho_k$ and prescribes a radius update that avoids repeated crossing of $x_1 = 0$?\n\nA. $\\rho_k \\approx 0.24$. Reject the step and set $\\Delta_{k+1} = 0.5\\,\\Delta_k$.\n\nB. $\\rho_k \\approx 1.2$. Accept the step and set $\\Delta_{k+1} = 2\\,\\Delta_k$.\n\nC. $\\rho_k \\approx 40$. Accept the step and set $\\Delta_{k+1} := \\min\\{1.5\\,\\Delta_k,\\, 0.8\\,|x_{k+1,1}|\\}$ to avoid repeated crossing of $x_1 = 0$.\n\nD. $\\rho_k \\approx 40$. Accept the step and set $\\Delta_{k+1} = 2\\,\\Delta_k$ because high $\\rho_k$ always warrants expansion.",
            "solution": "The problem statement is a valid exercise in numerical optimization. It presents a well-defined, self-contained scenario designed to test the behavior of a trust-region algorithm on a nonsmooth objective function. All parameters and conditions are clearly specified, and the question is objective and mathematically formalizable.\n\nWe will proceed with the tasks as outlined.\n\nThe objective function $f(x)$ is given by:\n$$\nf(x) = \\begin{cases}\nx_1^3 + 2 x_1^2 - 3 x_1 x_2 + x_2^2,  x_1 \\ge 0 \\\\\nx_1^3 + \\tfrac{1}{2} x_1^2 - 3 x_1 x_2 + x_2^2,  x_1  0\n\\end{cases}\n$$\nThe current iterate is $x_k = (0.05, 0.2)$. The proposed step is $s_k = (-0.50, -0.73)$.\nThe next point would be $x_{k+1} = x_k + s_k = (0.05 - 0.50, 0.2 - 0.73) = (-0.45, -0.53)$.\n\n**1. Compute $\\nabla f(x_k)$ and $B_k$.**\n\nSince $x_{k,1} = 0.05 \\ge 0$, we use the definition of $f(x)$ for $x_1 \\ge 0$ to compute the gradient and Hessian at $x_k$.\nFor $x_1  0$, the gradient is:\n$$ \\nabla f(x) = \\begin{pmatrix} 3x_1^2 + 4x_1 - 3x_2 \\\\ -3x_1 + 2x_2 \\end{pmatrix} $$\nEvaluating at $x_k = (0.05, 0.2)$:\n$$ \\nabla f(x_k) = \\begin{pmatrix} 3(0.05)^2 + 4(0.05) - 3(0.2) \\\\ -3(0.05) + 2(0.2) \\end{pmatrix} = \\begin{pmatrix} 0.0075 + 0.2 - 0.6 \\\\ -0.15 + 0.4 \\end{pmatrix} = \\begin{pmatrix} -0.3925 \\\\ 0.25 \\end{pmatrix} $$\nThe model Hessian $B_k$ is the true Hessian $\\nabla^2 f(x_k)$. For $x_1  0$, the Hessian is:\n$$ \\nabla^2 f(x) = \\begin{pmatrix} 6x_1 + 4  -3 \\\\ -3  2 \\end{pmatrix} $$\nEvaluating at $x_k = (0.05, 0.2)$:\n$$ B_k = \\nabla^2 f(x_k) = \\begin{pmatrix} 6(0.05) + 4  -3 \\\\ -3  2 \\end{pmatrix} = \\begin{pmatrix} 0.3 + 4  -3 \\\\ -3  2 \\end{pmatrix} = \\begin{pmatrix} 4.3  -3 \\\\ -3  2 \\end{pmatrix} $$\n\n**2. Compute $\\text{pred}_k$ and $\\text{ared}_k$.**\n\nThe predicted reduction is $\\text{pred}_k = -\\nabla f(x_k)^\\top s_k - \\tfrac{1}{2} s_k^\\top B_k s_k$.\n$$ -\\nabla f(x_k)^\\top s_k = - ((-0.3925)(-0.50) + (0.25)(-0.73)) = - (0.19625 - 0.1825) = -0.01375 $$\n$$ s_k^\\top B_k s_k = \\begin{pmatrix} -0.50  -0.73 \\end{pmatrix} \\begin{pmatrix} 4.3  -3 \\\\ -3  2 \\end{pmatrix} \\begin{pmatrix} -0.50 \\\\ -0.73 \\end{pmatrix} = -0.0492 $$\nThus, the predicted reduction is:\n$$ \\text{pred}_k = -0.01375 - \\tfrac{1}{2}(-0.0492) = -0.01375 + 0.0246 = 0.01085 $$\nThe actual reduction is $\\text{ared}_k = f(x_k) - f(x_k + s_k)$.\nFirst, we compute $f(x_k) = f(0.05, 0.2)$ using the formula for $x_1 \\ge 0$:\n$$ f(0.05, 0.2) = (0.05)^3 + 2(0.05)^2 - 3(0.05)(0.2) + (0.2)^2 = 0.015125 $$\nNext, we compute $f(x_k + s_k) = f(-0.45, -0.53)$ using the formula for $x_1  0$:\n$$ f(-0.45, -0.53) = (-0.45)^3 + \\tfrac{1}{2}(-0.45)^2 - 3(-0.45)(-0.53) + (-0.53)^2 = -0.424475 $$\nThe actual reduction is:\n$$ \\text{ared}_k = 0.015125 - (-0.424475) = 0.4396 $$\n\n**3. Compute $\\rho_k$ and interpret it.**\n\nThe ratio is:\n$$ \\rho_k = \\frac{\\text{ared}_k}{\\text{pred}_k} = \\frac{0.4396}{0.01085} \\approx 40.516 $$\nThis value is approximately $40$. The step is accepted since $\\rho_k \\gg \\eta_2 = 0.75$.\n\n**4. Design an update for the trust-region radius $\\Delta_{k+1}$.**\n\nThe extremely large value of $\\rho_k$ indicates a severe mismatch between the model and the true function, which occurred because the step crossed the kink at $x_1=0$. A naive update rule might expand the trust region, but this is unwise as it could lead to oscillations across the kink. A more sophisticated heuristic is to accept the step but choose a trust-region radius for the next iteration that is small enough to prevent immediately crossing back over the kink. The distance from the new point $x_{k+1}=(-0.45, -0.53)$ to the kink is $|x_{k+1,1}| = 0.45$. A radius update rule like $\\Delta_{k+1} = \\min\\{\\text{expansion_factor} \\cdot \\Delta_k, \\text{safety_factor} \\cdot |x_{k+1,1}|\\}$ is a robust strategy.\n\nEvaluating the options:\n- A  B are incorrect because their $\\rho_k$ values are wrong.\n- C correctly identifies $\\rho_k \\approx 40$, accepts the step, and proposes a sophisticated radius update that caps the new radius by a fraction of the distance to the kink, which is an excellent strategy for handling this kind of model failure.\n- D correctly identifies $\\rho_k \\approx 40$ but proposes a naive radius expansion, justifying it with the incorrect heuristic that a high $\\rho_k$ always warrants expansion.\n\nTherefore, Option C is the best choice.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}