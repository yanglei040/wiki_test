{
    "hands_on_practices": [
        {
            "introduction": "The core of Sequential Quadratic Programming (SQP) is to model a complex nonlinear problem with a sequence of simpler quadratic ones. This first exercise provides a foundational insight into why this process is iterative by exploring the discrepancy between the linearized model and the true nonlinear constraint . By solving a simple subproblem, you will quantify this approximation error and see firsthand how a step that is \"feasible\" for the local model can violate the original problem's constraints, a key challenge that globalization techniques aim to address.",
            "id": "3169627",
            "problem": "Consider Sequential Quadratic Programming (SQP), which constructs at a current iterate $x_k$ a quadratic approximation of the objective and a linearization of the constraints to define a search direction $d$. Let there be a single nonlinear inequality constraint $g(x) \\le 0$ with $x \\in \\mathbb{R}$. At the iterate $x_k = \\frac{3}{10}$, take the constraint to be $g(x) = x^2 - \\frac{1}{10}$. Use a quadratic model of the objective with Hessian $H_k = 1$ and gradient $\\nabla f(x_k) = -1$, so the SQP subproblem is\n$$\n\\min_{d \\in \\mathbb{R}} \\;\\; \\frac{1}{2} d^2 - d \\quad \\text{subject to} \\quad g(x_k) + \\nabla g(x_k)\\, d \\le 0.\n$$\nSolve this subproblem to obtain the step $d_k$, and verify that the linearized constraint is satisfied at $d_k$ while the true constraint $g(x_k + d_k)$ is violated. Then quantify the discrepancy between the true constraint and its linearized prediction at the step by computing\n$$\n\\Delta \\;=\\; g(x_k + d_k) \\;-\\; \\big( g(x_k) + \\nabla g(x_k)\\, d_k \\big).\n$$\nProvide the exact value of $\\Delta$ as a reduced fraction. No rounding is required.",
            "solution": "The user wants to solve a Sequential Quadratic Programming (SQP) subproblem and analyze the discrepancy between the linearized constraint and the true constraint.\n\n### Step 1: Extract Givens\n- **Optimization Method**: Sequential Quadratic Programming (SQP).\n- **Current Iterate**: $x_k = \\frac{3}{10}$, where $x \\in \\mathbb{R}$.\n- **Objective Function Model**:\n    - Hessian: $H_k = 1$.\n    - Gradient: $\\nabla f(x_k) = -1$.\n- **Constraint Function**:\n    - A single nonlinear inequality constraint: $g(x) \\le 0$.\n    - Specific form: $g(x) = x^2 - \\frac{1}{10}$.\n- **SQP Subproblem Definition**:\n    $$ \\min_{d \\in \\mathbb{R}} \\;\\; \\frac{1}{2} d^2 - d \\quad \\text{subject to} \\quad g(x_k) + \\nabla g(x_k)\\, d \\le 0 $$\n- **Tasks**:\n    1.  Solve the subproblem for the step $d_k$.\n    2.  Verify that the linearized constraint $g(x_k) + \\nabla g(x_k)\\, d_k \\le 0$ is satisfied.\n    3.  Verify that the true constraint $g(x_k + d_k) \\le 0$ is violated.\n    4.  Compute the discrepancy: $\\Delta = g(x_k + d_k) - \\big( g(x_k) + \\nabla g(x_k)\\, d_k \\big)$.\n    5.  Provide $\\Delta$ as an exact reduced fraction.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation.\n- **Scientific Groundedness**: The problem is well-grounded in the established theory of nonlinear optimization, specifically the SQP method. The formulation of the quadratic subproblem is standard.\n- **Well-Posedness**: The objective function $\\frac{1}{2} d^2 - d$ is a strictly convex quadratic function (its second derivative is $1 > 0$), and the constraint is linear in the variable $d$. This defines a convex Quadratic Program (QP), which is known to have a unique solution. The problem is well-posed.\n- **Objectivity**: The language is precise and mathematical, with no subjective or ambiguous terms.\n- **Completeness**: All necessary functions ($g(x)$), values ($x_k$), and model parameters ($H_k$, $\\nabla f(x_k)$) are provided to formulate and solve the subproblem.\n- **Other Flaws**: The problem does not exhibit any other flaws listed in the validation criteria. It is a standard textbook exercise.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation\nThe solution proceeds by first constructing the specific quadratic programming (QP) subproblem, solving it, and then performing the requested verifications and calculations.\n\n**1. Formulate the QP Subproblem**\n\nWe need to compute the terms for the linearized constraint $g(x_k) + \\nabla g(x_k)\\, d \\le 0$.\nThe constraint function is $g(x) = x^2 - \\frac{1}{10}$.\nThe current iterate is $x_k = \\frac{3}{10}$.\n\nFirst, evaluate the constraint function at $x_k$:\n$$ g(x_k) = g\\left(\\frac{3}{10}\\right) = \\left(\\frac{3}{10}\\right)^2 - \\frac{1}{10} = \\frac{9}{100} - \\frac{10}{100} = -\\frac{1}{100} $$\n\nNext, compute the gradient of the constraint function, $\\nabla g(x)$, and evaluate it at $x_k$:\n$$ \\nabla g(x) = \\frac{d}{dx}\\left(x^2 - \\frac{1}{10}\\right) = 2x $$\n$$ \\nabla g(x_k) = \\nabla g\\left(\\frac{3}{10}\\right) = 2\\left(\\frac{3}{10}\\right) = \\frac{6}{10} = \\frac{3}{5} $$\n\nThe objective function of the subproblem is given as $\\frac{1}{2}d^2 - d$.\nThe linearized constraint is $g(x_k) + \\nabla g(x_k) d \\le 0$, which becomes:\n$$ -\\frac{1}{100} + \\frac{3}{5}d \\le 0 $$\n\nThus, the complete SQP subproblem is:\n$$ \\min_{d \\in \\mathbb{R}} \\;\\; \\frac{1}{2} d^2 - d \\quad \\text{subject to} \\quad \\frac{3}{5}d \\le \\frac{1}{100} $$\n\n**2. Solve the Subproblem for the Step $d_k$**\n\nThis is a constrained optimization problem. We can solve it using the Karush-Kuhn-Tucker (KKT) conditions. Let the objective be $q(d) = \\frac{1}{2}d^2 - d$.\nThe Lagrangian is:\n$$ \\mathcal{L}(d, \\lambda) = \\frac{1}{2}d^2 - d + \\lambda \\left(-\\frac{1}{100} + \\frac{3}{5}d\\right) $$\nwhere $\\lambda \\ge 0$ is the Lagrange multiplier.\n\nThe KKT conditions are:\n1.  **Stationarity**: $\\frac{\\partial \\mathcal{L}}{\\partial d} = d - 1 + \\frac{3}{5}\\lambda = 0$\n2.  **Primal Feasibility**: $-\\frac{1}{100} + \\frac{3}{5}d \\le 0$\n3.  **Dual Feasibility**: $\\lambda \\ge 0$\n4.  **Complementary Slackness**: $\\lambda \\left(-\\frac{1}{100} + \\frac{3}{5}d\\right) = 0$\n\nFrom complementary slackness, we have two cases:\n\nCase A: The constraint is inactive, so $\\lambda = 0$.\nThe stationarity condition gives $d - 1 + 0 = 0$, which implies $d = 1$.\nWe check primal feasibility: $-\\frac{1}{100} + \\frac{3}{5}(1) = -\\frac{1}{100} + \\frac{60}{100} = \\frac{59}{100}$. This is not less than or equal to $0$. So, this case is not the solution. The unconstrained minimum of $q(d)$ occurs at $d=1$, which violates the constraint.\n\nCase B: The constraint is active, so $-\\frac{1}{100} + \\frac{3}{5}d = 0$.\nFrom this equation, we can solve for $d$:\n$$ \\frac{3}{5}d = \\frac{1}{100} \\implies d = \\frac{1}{100} \\cdot \\frac{5}{3} = \\frac{5}{300} = \\frac{1}{60} $$\nThis is our candidate for the step, $d_k = \\frac{1}{60}$.\nNow we use the stationarity condition to find $\\lambda$:\n$$ d - 1 + \\frac{3}{5}\\lambda = 0 \\implies \\frac{1}{60} - 1 + \\frac{3}{5}\\lambda = 0 $$\n$$ -\\frac{59}{60} + \\frac{3}{5}\\lambda = 0 \\implies \\frac{3}{5}\\lambda = \\frac{59}{60} $$\n$$ \\lambda = \\frac{59}{60} \\cdot \\frac{5}{3} = \\frac{59 \\cdot 5}{60 \\cdot 3} = \\frac{59}{12 \\cdot 3} = \\frac{59}{36} $$\nWe check dual feasibility: $\\lambda = \\frac{59}{36} > 0$. This is consistent with an active constraint.\nTherefore, the solution to the subproblem is $d_k = \\frac{1}{60}$.\n\n**3. Verify Constraint Satisfaction and Violation**\n\nLet's verify the two constraints as required.\n- **Linearized Constraint**: We need to check if $g(x_k) + \\nabla g(x_k) d_k \\le 0$.\n$$ g(x_k) + \\nabla g(x_k) d_k = -\\frac{1}{100} + \\frac{3}{5} \\left(\\frac{1}{60}\\right) = -\\frac{1}{100} + \\frac{3}{300} = -\\frac{1}{100} + \\frac{1}{100} = 0 $$\nSince $0 \\le 0$ is true, the linearized constraint is satisfied (it is active).\n\n- **True Constraint**: We need to check if $g(x_k + d_k) \\le 0$.\nFirst, find the new point $x_k + d_k$:\n$$ x_k + d_k = \\frac{3}{10} + \\frac{1}{60} = \\frac{18}{60} + \\frac{1}{60} = \\frac{19}{60} $$\nNow, evaluate the true constraint function $g(x)$ at this new point:\n$$ g(x_k + d_k) = g\\left(\\frac{19}{60}\\right) = \\left(\\frac{19}{60}\\right)^2 - \\frac{1}{10} = \\frac{19^2}{60^2} - \\frac{1}{10} = \\frac{361}{3600} - \\frac{1}{10} $$\n$$ g(x_k + d_k) = \\frac{361}{3600} - \\frac{360}{3600} = \\frac{1}{3600} $$\nThe value of the true constraint is $\\frac{1}{3600}$. Since $\\frac{1}{3600} > 0$, the condition $g(x_k + d_k) \\le 0$ is **violated**.\n\n**4. Compute the Discrepancy $\\Delta$**\n\nThe discrepancy is defined as $\\Delta = g(x_k + d_k) - \\big( g(x_k) + \\nabla g(x_k) d_k \\big)$.\nFrom our previous calculations:\n- $g(x_k + d_k) = \\frac{1}{3600}$\n- $g(x_k) + \\nabla g(x_k) d_k = 0$\n\nSubstituting these values into the expression for $\\Delta$:\n$$ \\Delta = \\frac{1}{3600} - 0 = \\frac{1}{3600} $$\nThis discrepancy arises because the linearization $g(x_k) + \\nabla g(x_k) d$ is a first-order Taylor approximation of $g(x_k+d)$. The error in this approximation is given by the higher-order terms. For a quadratic function like $g(x) = x^2 - \\frac{1}{10}$, the Taylor series is exact and terminates after the second-order term:\n$$ g(x_k + d) = g(x_k) + (\\nabla g(x_k)) d + \\frac{1}{2} (\\nabla^2 g(x_k)) d^2 $$\nThe discrepancy is therefore exactly the second-order term:\n$$ \\Delta = g(x_k+d_k) - (g(x_k) + \\nabla g(x_k) d_k) = \\frac{1}{2} (\\nabla^2 g(x_k)) (d_k)^2 $$\nWe can verify this. The second derivative is $\\nabla^2 g(x) = 2$. So, $\\nabla^2 g(x_k) = 2$.\n$$ \\Delta = \\frac{1}{2} (2) \\left(\\frac{1}{60}\\right)^2 = \\left(\\frac{1}{60}\\right)^2 = \\frac{1}{3600} $$\nThis confirms our result. The fraction $\\frac{1}{3600}$ is already in its most reduced form.",
            "answer": "$$\\boxed{\\frac{1}{3600}}$$"
        },
        {
            "introduction": "Building upon the local approximation, a full SQP iteration requires not only solving the quadratic subproblem but also ensuring the computed step promotes progress toward the solution. This practice guides you through a complete SQP step for a nonlinear program with two inequality constraints, from formulating and solving the QP subproblem to analyzing the result using an $\\ell_1$ penalty merit function . You will determine the minimum penalty parameter $\\mu$ that guarantees the step is a descent direction, illustrating the crucial interplay between the local model and the global convergence strategy.",
            "id": "3195671",
            "problem": "Consider the nonlinear program with two ($2$) inequality constraints:\n$$\n\\begin{aligned}\n\\min_{x \\in \\mathbb{R}^2}\\quad & f(x) \\\\\n\\text{s.t.}\\quad & g_1(x) \\le 0, \\\\\n& g_2(x) \\le 0,\n\\end{aligned}\n$$\nwhere\n$$\nf(x) = (x_1 - 3)^2 + (x_2 - 3)^2,\\quad\ng_1(x) = x_1 + x_2 - 2,\\quad\ng_2(x) = x_1^2 + x_2 - 2.\n$$\nStarting from the point $x^{(0)} = (1,\\;1.5)$, construct a single Sequential Quadratic Programming (SQP) step using the identity matrix $B^{(0)} = I_2$ as the initial symmetric positive definite approximation of the Hessian of the Lagrangian. Use the standard SQP quadratic subproblem formed by the first-order Taylor expansion of the constraints and the quadratic model with $B^{(0)}$ of the objective. Derive this quadratic subproblem explicitly and solve it to obtain the step $p^{(0)}$.\n\nTo assess globalization with an exact penalty merit function, consider the $\\ell_1$ exact penalty merit function\n$$\n\\phi_{\\mu}(x) = f(x) + \\mu\\left(|\\max\\{0, g_1(x)\\}| + |\\max\\{0, g_2(x)\\}|\\right),\n$$\nwith penalty parameter $\\mu \\ge 0$. Compute the directional derivative of $\\phi_{\\mu}$ at $x^{(0)}$ along the SQP step $p^{(0)}$, and determine the smallest threshold value $\\mu_{\\min}$ such that for all $\\mu > \\mu_{\\min}$, the SQP step is a strict descent direction for $\\phi_{\\mu}$ at $x^{(0)}$.\n\nYour final answer must be the single real number $\\mu_{\\min}$. No rounding is required; provide an exact value. Report only $\\mu_{\\min}$ as your final answer.",
            "solution": "The user wants me to solve a multi-part optimization problem.\n\nHere's a breakdown of the tasks:\n1.  **Validate the Problem**: Check if the problem statement is well-posed, scientifically sound, and complete.\n2.  **Construct SQP Subproblem**: Formulate the quadratic programming (QP) subproblem at the initial point $x^{(0)} = (1, 1.5)$. This involves:\n    *   Calculating the gradient of the objective function $f(x)$ at $x^{(0)}$.\n    *   Calculating the values and gradients of the constraint functions $g_1(x)$ and $g_2(x)$ at $x^{(0)}$.\n    *   Using the given approximation of the Hessian of the Lagrangian, $B^{(0)} = I_2$.\n    *   Writing down the QP subproblem in terms of the step $p = (p_1, p_2)$.\n3.  **Solve the QP Subproblem**: Solve the derived QP for the step $p^{(0)}$. This means finding the optimal $p_1$ and $p_2$. This is a constrained optimization problem in itself. I will likely need to use the Karush-Kuhn-Tucker (KKT) conditions for the subproblem.\n4.  **Analyze the Merit Function**:\n    *   Define the $\\ell_1$ exact penalty merit function $\\phi_{\\mu}(x)$.\n    *   Calculate its directional derivative at $x^{(0)}$ in the direction $p^{(0)}$, which is denoted as $D_{p^{(0)}} \\phi_{\\mu}(x^{(0)})$.\n    *   Determine the condition for $p^{(0)}$ to be a strict descent direction, which is $D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) < 0$.\n    *   Solve this inequality for $\\mu$ to find the smallest threshold $\\mu_{\\min}$.\n\n**Part 1: Problem Validation**\n\n*   **Extract Givens**:\n    *   Problem type: Nonlinear program with inequality constraints.\n    *   Objective function: $f(x) = (x_1 - 3)^2 + (x_2 - 3)^2$.\n    *   Constraint 1: $g_1(x) = x_1 + x_2 - 2 \\le 0$.\n    *   Constraint 2: $g_2(x) = x_1^2 + x_2 - 2 \\le 0$.\n    *   Starting point: $x^{(0)} = (1, 1.5)$.\n    *   Initial Hessian approximation: $B^{(0)} = I_2$ (the $2 \\times 2$ identity matrix).\n    *   Task 1: Construct and solve the SQP quadratic subproblem for the step $p^{(0)}$.\n    *   Merit function: $\\phi_{\\mu}(x) = f(x) + \\mu\\left(|\\max\\{0, g_1(x)\\}| + |\\max\\{0, g_2(x)\\}|\\right)$.\n    *   Task 2: Compute the directional derivative of $\\phi_{\\mu}$ at $x^{(0)}$ along $p^{(0)}$.\n    *   Task 3: Find the smallest $\\mu_{\\min}$ such that for $\\mu > \\mu_{\\min}$, $D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) < 0$.\n\n*   **Validate Using Extracted Givens**:\n    *   **Scientifically Grounded**: The problem is a standard exercise in the field of nonlinear optimization, specifically using the Sequential Quadratic Programming method. All functions are well-defined polynomials. The concepts of SQP, merit functions, and directional derivatives are standard in optimization theory. The problem is mathematically sound.\n    *   **Well-Posed**: The problem asks for specific calculations based on given data. The functions are smooth. The QP subproblem is convex and should have a unique solution. The final part asks for a threshold value, which should be uniquely determinable from the preceding calculations.\n    *   **Objective**: The language is precise and mathematical. No subjective terms are used.\n    *   **Completeness**: All necessary information is provided: the functions, the starting point, the Hessian approximation, the definition of the merit function, and the specific tasks to perform. The problem is self-contained.\n    *   **Feasibility/Realism**: The problem is a mathematical construct, so physical realism is not applicable. The values and functions are reasonable for a textbook-style problem.\n    *   **Ill-Posed/Poorly Structured**: No issues here. The structure is logical and guides the solver through a standard procedure.\n    *   **Trivial/Tautological**: The problem requires several steps of calculation and application of optimization theory. It is not trivial.\n    *   **Topic Relevance**: The problem is clearly about *unconstrained and constrained optimization* within the field *optimization methods*. It specifically deals with SQP, a core algorithm for constrained optimization.\n\n*   **Verdict**: The problem is valid. I will proceed with the solution.\n\n**Part 2: Solution Steps**\n\n**Step A: Construct the SQP Subproblem**\n\nThe general form of the SQP subproblem at iteration $k$ is:\n$$\n\\begin{aligned}\n\\min_{p \\in \\mathbb{R}^n}\\quad & f(x^{(k)}) + \\nabla f(x^{(k)})^T p + \\frac{1}{2} p^T B^{(k)} p \\\\\n\\text{s.t.}\\quad & g_i(x^{(k)}) + \\nabla g_i(x^{(k)})^T p \\le 0, \\quad i \\in \\mathcal{I}\n\\end{aligned}\n$$\nHere, $k=0$ and $n=2$. $x^{(0)} = (1, 1.5)$. $B^{(0)} = I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n\nFirst, I need to compute the function values and gradients at $x^{(0)}$.\n\n*   **Objective function $f(x)$**:\n    *   $f(x) = (x_1 - 3)^2 + (x_2 - 3)^2$\n    *   $\\nabla f(x) = \\begin{pmatrix} 2(x_1 - 3) \\\\ 2(x_2 - 3) \\end{pmatrix}$\n    *   At $x^{(0)} = (1, 1.5)$:\n        *   $f(x^{(0)}) = (1 - 3)^2 + (1.5 - 3)^2 = (-2)^2 + (-1.5)^2 = 4 + 2.25 = 6.25$.\n        *   $\\nabla f(x^{(0)}) = \\begin{pmatrix} 2(1 - 3) \\\\ 2(1.5 - 3) \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ -3 \\end{pmatrix}$.\n\n*   **Constraint function $g_1(x)$**:\n    *   $g_1(x) = x_1 + x_2 - 2$\n    *   $\\nabla g_1(x) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n    *   At $x^{(0)} = (1, 1.5)$:\n        *   $g_1(x^{(0)}) = 1 + 1.5 - 2 = 0.5$.\n        *   $\\nabla g_1(x^{(0)}) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\n*   **Constraint function $g_2(x)$**:\n    *   $g_2(x) = x_1^2 + x_2 - 2$\n    *   $\\nabla g_2(x) = \\begin{pmatrix} 2x_1 \\\\ 1 \\end{pmatrix}$\n    *   At $x^{(0)} = (1, 1.5)$:\n        *   $g_2(x^{(0)}) = 1^2 + 1.5 - 2 = 1 + 1.5 - 2 = 0.5$.\n        *   $\\nabla g_2(x^{(0)}) = \\begin{pmatrix} 2(1) \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$.\n\nNow, substitute these into the subproblem template. Let $p = (p_1, p_2)^T$.\nThe objective of the subproblem is:\n$$\n\\min_{p \\in \\mathbb{R}^2} \\quad 6.25 + \\begin{pmatrix} -4 \\\\ -3 \\end{pmatrix}^T \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix}^T \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix}\n$$\nThe constant term $f(x^{(0)})$ does not affect the minimizer $p$, so we can drop it. The objective simplifies to:\n$$\n\\min_{p \\in \\mathbb{R}^2} \\quad -4 p_1 - 3 p_2 + \\frac{1}{2} (p_1^2 + p_2^2)\n$$\nThe constraints of the subproblem are:\n1.  \n    $g_1(x^{(0)}) + \\nabla g_1(x^{(0)})^T p \\le 0$\n    $0.5 + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}^T \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix} \\le 0 \\implies p_1 + p_2 + 0.5 \\le 0$\n2.  \n    $g_2(x^{(0)}) + \\nabla g_2(x^{(0)})^T p \\le 0$\n    $0.5 + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}^T \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix} \\le 0 \\implies 2p_1 + p_2 + 0.5 \\le 0$\n\nSo, the explicit QP subproblem is:\n$$\n\\begin{aligned}\n\\min_{p_1, p_2} \\quad & \\frac{1}{2} p_1^2 + \\frac{1}{2} p_2^2 - 4 p_1 - 3 p_2 \\\\\n\\text{s.t.} \\quad & p_1 + p_2 + 0.5 \\le 0 \\\\\n& 2p_1 + p_2 + 0.5 \\le 0\n\\end{aligned}\n$$\n\n**Step B: Solve the QP Subproblem**\n\nThis is a convex QP. I will use the KKT conditions to solve it.\nLet the objective be $Q(p) = \\frac{1}{2} p_1^2 + \\frac{1}{2} p_2^2 - 4 p_1 - 3 p_2$.\nLet the constraints be $c_1(p) = p_1 + p_2 + 0.5 \\le 0$ and $c_2(p) = 2p_1 + p_2 + 0.5 \\le 0$.\nThe Lagrangian is:\n$$\n\\mathcal{L}(p_1, p_2, \\lambda_1, \\lambda_2) = Q(p) + \\lambda_1 c_1(p) + \\lambda_2 c_2(p)\n$$\nwhere $\\lambda_1, \\lambda_2 \\ge 0$ are the Lagrange multipliers.\n\nThe KKT conditions are:\n1.  **Stationarity**: $\\nabla_p \\mathcal{L} = 0$\n    $$\n    \\nabla_p Q(p) + \\lambda_1 \\nabla_p c_1(p) + \\lambda_2 \\nabla_p c_2(p) = 0\n    $$\n    $$\n    \\begin{pmatrix} p_1 - 4 \\\\ p_2 - 3 \\end{pmatrix} + \\lambda_1 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\lambda_2 \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n    $$\n    This gives two equations:\n    (i) $p_1 - 4 + \\lambda_1 + 2\\lambda_2 = 0 \\implies p_1 = 4 - \\lambda_1 - 2\\lambda_2$\n    (ii) $p_2 - 3 + \\lambda_1 + \\lambda_2 = 0 \\implies p_2 = 3 - \\lambda_1 - \\lambda_2$\n\n2.  **Primal Feasibility**:\n    (iii) $p_1 + p_2 + 0.5 \\le 0$\n    (iv) $2p_1 + p_2 + 0.5 \\le 0$\n\n3.  **Dual Feasibility**:\n    (v) $\\lambda_1 \\ge 0$\n    (vi) $\\lambda_2 \\ge 0$\n\n4.  **Complementary Slackness**:\n    (vii) $\\lambda_1 (p_1 + p_2 + 0.5) = 0$\n    (viii) $\\lambda_2 (2p_1 + p_2 + 0.5) = 0$\n\nLet's test the cases for active constraints.\n\n*   Case 1: No constraints are active. $\\lambda_1 = 0, \\lambda_2 = 0$.\n    From (i), $p_1 = 4$. From (ii), $p_2 = 3$.\n    Check feasibility:\n    $c_1(p) = 4 + 3 + 0.5 = 7.5 \\not\\le 0$. This case is not the solution.\n\n*   Case 2: Constraint 1 is active, constraint 2 is not. $p_1 + p_2 + 0.5 = 0, \\lambda_2 = 0, \\lambda_1 > 0$.\n    From (i), $p_1 = 4 - \\lambda_1$.\n    From (ii), $p_2 = 3 - \\lambda_1$.\n    Substitute into the active constraint:\n    $(4 - \\lambda_1) + (3 - \\lambda_1) + 0.5 = 0$\n    $7.5 - 2\\lambda_1 = 0 \\implies \\lambda_1 = 3.75$.\n    This satisfies $\\lambda_1 > 0$.\n    Now find $p_1, p_2$:\n    $p_1 = 4 - 3.75 = 0.25$.\n    $p_2 = 3 - 3.75 = -0.75$.\n    Check feasibility for constraint 2:\n    $c_2(p) = 2(0.25) + (-0.75) + 0.5 = 0.5 - 0.75 + 0.5 = 0.25 \\not\\le 0$. This case is not the solution.\n\n*   Case 3: Constraint 2 is active, constraint 1 is not. $2p_1 + p_2 + 0.5 = 0, \\lambda_1 = 0, \\lambda_2 > 0$.\n    From (i), $p_1 = 4 - 2\\lambda_2$.\n    From (ii), $p_2 = 3 - \\lambda_2$.\n    Substitute into the active constraint:\n    $2(4 - 2\\lambda_2) + (3 - \\lambda_2) + 0.5 = 0$\n    $8 - 4\\lambda_2 + 3 - \\lambda_2 + 0.5 = 0$\n    $11.5 - 5\\lambda_2 = 0 \\implies \\lambda_2 = 11.5 / 5 = 2.3$.\n    This satisfies $\\lambda_2 > 0$.\n    Now find $p_1, p_2$:\n    $p_1 = 4 - 2(2.3) = 4 - 4.6 = -0.6$.\n    $p_2 = 3 - 2.3 = 0.7$.\n    Check feasibility for constraint 1:\n    $c_1(p) = -0.6 + 0.7 + 0.5 = 0.6 \\not\\le 0$. This case is not the solution.\n\n*   Case 4: Both constraints are active. $p_1 + p_2 + 0.5 = 0, 2p_1 + p_2 + 0.5 = 0, \\lambda_1 > 0, \\lambda_2 > 0$.\n    This is a system of two linear equations for $p_1, p_2$:\n    (a) $p_1 + p_2 = -0.5$\n    (b) $2p_1 + p_2 = -0.5$\n    Subtract (a) from (b): $(2p_1 - p_1) + (p_2 - p_2) = -0.5 - (-0.5) \\implies p_1 = 0$.\n    Substitute $p_1=0$ into (a): $0 + p_2 = -0.5 \\implies p_2 = -0.5$.\n    So $p^{(0)} = (0, -0.5)^T$.\n    Now find $\\lambda_1, \\lambda_2$ using the stationarity conditions (i) and (ii):\n    $p_1 = 0 \\implies 4 - \\lambda_1 - 2\\lambda_2 = 0 \\implies \\lambda_1 + 2\\lambda_2 = 4$.\n    $p_2 = -0.5 \\implies 3 - \\lambda_1 - \\lambda_2 = -0.5 \\implies \\lambda_1 + \\lambda_2 = 3.5$.\n    This is a system for $\\lambda_1, \\lambda_2$.\n    Subtract the second from the first: $(\\lambda_1 - \\lambda_1) + (2\\lambda_2 - \\lambda_2) = 4 - 3.5 \\implies \\lambda_2 = 0.5$.\n    Substitute $\\lambda_2=0.5$ into the second equation: $\\lambda_1 + 0.5 = 3.5 \\implies \\lambda_1 = 3$.\n    Check dual feasibility: $\\lambda_1 = 3 > 0$ and $\\lambda_2 = 0.5 > 0$. All conditions are satisfied.\n\nSo the solution to the QP subproblem is $p^{(0)} = \\begin{pmatrix} 0 \\\\ -0.5 \\end{pmatrix}$.\nThe Lagrange multipliers for the subproblem are $\\lambda^{(1)} = (\\lambda_1, \\lambda_2)^T = (3, 0.5)^T$. These are the estimates for the multipliers of the original problem for the next iteration.\n\nSummary of results so far:\n$x^{(0)} = (1, 1.5)^T$\n$p^{(0)} = (0, -0.5)^T$\n$\\nabla f(x^{(0)}) = (-4, -3)^T$\n$g_1(x^{(0)}) = 0.5$\n$g_2(x^{(0)}) = 0.5$\n$\\nabla g_1(x^{(0)}) = (1, 1)^T$\n$\\nabla g_2(x^{(0)}) = (2, 1)^T$\nMultipliers from QP: $\\lambda_1=3, \\lambda_2=0.5$.\n\n**Step C: Directional Derivative of the Merit Function**\n\nThe $\\ell_1$ exact penalty merit function is:\n$$\n\\phi_{\\mu}(x) = f(x) + \\mu\\left(|\\max\\{0, g_1(x)\\}| + |\\max\\{0, g_2(x)\\}|\\right)\n$$\nWe need to compute the directional derivative of $\\phi_{\\mu}$ at $x^{(0)}$ in the direction $p^{(0)}$.\nThe formula for the directional derivative of $\\phi_{\\mu}$ at a point $x$ in a direction $p$ is:\n$$\nD_p \\phi_{\\mu}(x) = \\nabla f(x)^T p + \\mu D_p \\left( \\sum_{i=1}^m |\\max\\{0, g_i(x)\\}| \\right)\n$$\nAt points where the constraints are violated, i.e., $g_i(x) > 0$, the term $|\\max\\{0, g_i(x)\\}|$ is simply $g_i(x)$, which is differentiable. Its directional derivative is $\\nabla g_i(x)^T p$.\nAt points where the constraints are satisfied, i.e., $g_i(x) \\le 0$, the term $|\\max\\{0, g_i(x)\\}|$ is $0$ in a neighborhood if $g_i(x) < 0$. If $g_i(x)=0$, the directional derivative is $\\max\\{0, \\nabla g_i(x)^T p\\}$.\nIn our case, at $x^{(0)}$, we have $g_1(x^{(0)}) = 0.5 > 0$ and $g_2(x^{(0)}) = 0.5 > 0$.\nSo, $\\phi_{\\mu}(x)$ is differentiable at $x^{(0)}$ and in its neighborhood, it is given by:\n$$\n\\phi_{\\mu}(x) = f(x) + \\mu(g_1(x) + g_2(x))\n$$\nThe directional derivative is therefore:\n$$\nD_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = \\nabla \\phi_{\\mu}(x^{(0)})^T p^{(0)}\n$$\nwhere $\\nabla \\phi_{\\mu}(x^{(0)}) = \\nabla f(x^{(0)}) + \\mu(\\nabla g_1(x^{(0)}) + \\nabla g_2(x^{(0)}))$.\nSo,\n$$\nD_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = (\\nabla f(x^{(0)}) + \\mu(\\nabla g_1(x^{(0)}) + \\nabla g_2(x^{(0)})))^T p^{(0)}\n$$\n$$\nD_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = \\nabla f(x^{(0)})^T p^{(0)} + \\mu(\\nabla g_1(x^{(0)})^T p^{(0)} + \\nabla g_2(x^{(0)})^T p^{(0)})\n$$\nLet's compute the components.\n*   $\\nabla f(x^{(0)})^T p^{(0)} = \\begin{pmatrix} -4 \\\\ -3 \\end{pmatrix}^T \\begin{pmatrix} 0 \\\\ -0.5 \\end{pmatrix} = (-4)(0) + (-3)(-0.5) = 1.5$.\n*   $\\nabla g_1(x^{(0)})^T p^{(0)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}^T \\begin{pmatrix} 0 \\\\ -0.5 \\end{pmatrix} = (1)(0) + (1)(-0.5) = -0.5$.\n*   $\\nabla g_2(x^{(0)})^T p^{(0)} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}^T \\begin{pmatrix} 0 \\\\ -0.5 \\end{pmatrix} = (2)(0) + (1)(-0.5) = -0.5$.\n\nSubstituting these into the expression for the directional derivative:\n$$\nD_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = 1.5 + \\mu(-0.5 + (-0.5)) = 1.5 - \\mu\n$$\n\nAlternatively, there's a general formula for the directional derivative of the $\\ell_1$ penalty function. For a general NLP $\\min f(x)$ s.t. $c(x) \\le 0$, the directional derivative of $\\phi_\\mu(x) = f(x) + \\mu \\sum_i \\max(0, c_i(x))$ in direction $p$ is:\n$$ D_p \\phi_\\mu(x) = \\nabla f(x)^T p + \\mu \\sum_{i \\in I_0(x)} \\max(0, \\nabla c_i(x)^T p) + \\mu \\sum_{i \\in I_+(x)} \\nabla c_i(x)^T p $$\nwhere $I_+(x) = \\{i | c_i(x) > 0\\}$ and $I_0(x) = \\{i | c_i(x) = 0\\}$.\nAt our point $x^{(0)}$, $g_1(x^{(0)})=0.5$ and $g_2(x^{(0)})=0.5$. So both are in $I_+(x^{(0)})$. The set $I_0(x^{(0)})$ is empty.\nSo the formula becomes:\n$$ D_{p^{(0)}} \\phi_\\mu(x^{(0)}) = \\nabla f(x^{(0)})^T p^{(0)} + \\mu (\\nabla g_1(x^{(0)})^T p^{(0)} + \\nabla g_2(x^{(0)})^T p^{(0)}) $$\nThis is exactly what I derived earlier. My previous derivation was correct.\nSo, $D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = 1.5 - \\mu$.\n\n**Step D: Find the Threshold $\\mu_{\\min}$**\n\nThe SQP step $p^{(0)}$ is a strict descent direction for $\\phi_{\\mu}$ at $x^{(0)}$ if the directional derivative is strictly negative:\n$$\nD_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) < 0\n$$\nSubstituting the expression we found:\n$$\n1.5 - \\mu < 0\n$$\n$$\n1.5 < \\mu\n$$\nSo, for all $\\mu > 1.5$, the step $p^{(0)}$ is a strict descent direction.\nThe question asks for the smallest threshold value $\\mu_{\\min}$ such that for all $\\mu > \\mu_{\\min}$, the condition holds.\nThis directly gives $\\mu_{\\min} = 1.5$.\n\nLet's double-check the theory.\nThe directional derivative is the key. $p$ is a descent direction if $D_p \\phi < 0$.\nThe problem requires a *strict* descent direction. $D_p \\phi < 0$.\nMy inequality $1.5 - \\mu < 0$ seems correct. This implies $\\mu > 1.5$.\nThe smallest value that works as a lower bound for $\\mu$ is $1.5$.\nSo $\\mu_{\\min}=1.5$.\n\nLet's check this result against a known theorem in numerical optimization (e.g., Nocedal & Wright, \"Numerical Optimization\").\nA common condition for the penalty parameter $\\mu$ is that it must satisfy $\\mu \\ge \\|\\lambda^{(k+1)}\\|_\\infty$, where $\\lambda^{(k+1)}$ are the multipliers from the QP subproblem.\nIn our case, the multipliers are $\\lambda^{(1)} = (\\lambda_1, \\lambda_2)^T = (3, 0.5)^T$.\nThe infinity norm is $\\|\\lambda^{(1)}\\|_\\infty = \\max\\{|\\lambda_1|, |\\lambda_2|\\} = \\max\\{3, 0.5\\} = 3$.\nThis suggests that perhaps $\\mu$ should be greater than $3$. Why is my result different?\n\nLet's re-examine the condition for descent.\nThe objective function of the QP is $Q(p) = \\nabla f(x^{(k)})^T p + \\frac{1}{2} p^T B^{(k)} p$.\nThe constraints are $\\nabla g_i(x^{(k)})^T p \\le -g_i(x^{(k)})$.\nLet $p^{(k)}$ be the solution of the QP and $\\lambda^{(k+1)}$ be the multipliers.\nThe KKT conditions for the QP are:\n$\\nabla f(x^{(k)}) + B^{(k)} p^{(k)} + \\sum_i \\lambda_i^{(k+1)} \\nabla g_i(x^{(k)}) = 0$.\nThis means $\\nabla f(x^{(k)})^T p^{(k)} + (p^{(k)})^T B^{(k)} p^{(k)} + \\sum_i \\lambda_i^{(k+1)} \\nabla g_i(x^{(k)})^T p^{(k)} = 0$. This is multiplying by $(p^{(k)})^T$.\nFrom this, we get $\\nabla f(x^{(k)})^T p^{(k)} = - (p^{(k)})^T B^{(k)} p^{(k)} - \\sum_i \\lambda_i^{(k+1)} \\nabla g_i(x^{(k)})^T p^{(k)}$.\n\nThe directional derivative is:\n$D_{p^{(k)}} \\phi_{\\mu}(x^{(k)}) = \\nabla f(x^{(k)})^T p^{(k)} + \\mu \\sum_{i \\in \\mathcal{I}} D_{p^{(k)}} (|\\max\\{0, g_i(x^{(k)})\\}|)$.\nLet's re-evaluate the derivative part for the constraints.\n$D_{p^{(k)}} \\sum_i |\\max(0,g_i)| = \\sum_{g_i > 0} \\nabla g_i^T p^{(k)} + \\sum_{g_i=0} \\max(0, \\nabla g_i^T p^{(k)})$.\nAt $x^{(0)}$, $g_1=0.5>0$ and $g_2=0.5>0$.\nSo $D = \\nabla f^T p + \\mu(\\nabla g_1^T p + \\nabla g_2^T p)$.\nThis is correct.\nThe linearized constraints are active at the solution $p^{(0)}$:\n$g_1(x^{(0)}) + \\nabla g_1(x^{(0)})^T p^{(0)} = 0 \\implies \\nabla g_1(x^{(0)})^T p^{(0)} = -g_1(x^{(0)}) = -0.5$.\n$g_2(x^{(0)}) + \\nabla g_2(x^{(0)})^T p^{(0)} = 0 \\implies \\nabla g_2(x^{(0)})^T p^{(0)} = -g_2(x^{(0)}) = -0.5$.\n\nMy earlier calculation of these terms was:\n$\\nabla g_1(x^{(0)})^T p^{(0)} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}^T \\begin{pmatrix} 0 \\\\ -0.5 \\end{pmatrix} = -0.5$. Correct.\n$\\nabla g_2(x^{(0)})^T p^{(0)} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}^T \\begin{pmatrix} 0 \\\\ -0.5 \\end{pmatrix} = -0.5$. Correct.\nThis is consistent with the constraints of the QP being active.\n\nNow let's use the KKT condition for the QP.\n$\\nabla f(x^{(0)}) + B^{(0)} p^{(0)} + \\lambda_1 \\nabla g_1(x^{(0)}) + \\lambda_2 \\nabla g_2(x^{(0)}) = 0$.\n$\\begin{pmatrix} -4 \\\\ -3 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -0.5 \\end{pmatrix} + \\lambda_1 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\lambda_2 \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 0$\n$\\begin{pmatrix} -4 \\\\ -3 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -0.5 \\end{pmatrix} + \\begin{pmatrix} \\lambda_1 \\\\ \\lambda_1 \\end{pmatrix} + \\begin{pmatrix} 2\\lambda_2 \\\\ \\lambda_2 \\end{pmatrix} = 0$\n$\\begin{pmatrix} -4 + \\lambda_1 + 2\\lambda_2 \\\\ -3.5 + \\lambda_1 + \\lambda_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThis is the same system I solved for $\\lambda_1, \\lambda_2$ before using $p_1, p_2$.\n$p_1-4 + \\lambda_1 + 2\\lambda_2 = 0 \\implies 0-4+\\lambda_1+2\\lambda_2=0 \\implies \\lambda_1+2\\lambda_2 = 4$.\n$p_2-3 + \\lambda_1 + \\lambda_2 = 0 \\implies -0.5-3+\\lambda_1+\\lambda_2=0 \\implies \\lambda_1+\\lambda_2 = 3.5$.\nSolution was $\\lambda_1 = 3, \\lambda_2 = 0.5$. Correct.\n\nLet's find a general expression for the directional derivative in terms of the QP solution.\n$D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = \\nabla f(x^{(0)})^T p^{(0)} + \\mu \\left( \\nabla g_1(x^{(0)})^T p^{(0)} + \\nabla g_2(x^{(0)})^T p^{(0)} \\right)$.\nFrom the linearized constraints (which are active):\n$\\nabla g_1(x^{(0)})^T p^{(0)} = -g_1(x^{(0)})$.\n$\\nabla g_2(x^{(0)})^T p^{(0)} = -g_2(x^{(0)})$.\nSo, $D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = \\nabla f(x^{(0)})^T p^{(0)} - \\mu (g_1(x^{(0)}) + g_2(x^{(0)}))$.\n\nNow, let's look at $\\nabla f(x^{(0)})^T p^{(0)}$.\nFrom the KKT condition of the QP, multiply by $(p^{(0)})^T$:\n$(p^{(0)})^T \\nabla f(x^{(0)}) + (p^{(0)})^T B^{(0)} p^{(0)} + \\lambda_1 (p^{(0)})^T \\nabla g_1(x^{(0)}) + \\lambda_2 (p^{(0)})^T \\nabla g_2(x^{(0)}) = 0$.\n$\\nabla f(x^{(0)})^T p^{(0)} = - (p^{(0)})^T B^{(0)} p^{(0)} - \\lambda_1 (\\nabla g_1(x^{(0)})^T p^{(0)}) - \\lambda_2 (\\nabla g_2(x^{(0)})^T p^{(0)})$.\nSubstitute $\\nabla g_i^T p^{(0)} = -g_i(x^{(0)})$:\n$\\nabla f(x^{(0)})^T p^{(0)} = - (p^{(0)})^T B^{(0)} p^{(0)} - \\lambda_1 (-g_1(x^{(0)})) - \\lambda_2 (-g_2(x^{(0)}))$.\n$\\nabla f(x^{(0)})^T p^{(0)} = - p^{(0)T} B^{(0)} p^{(0)} + \\lambda_1 g_1(x^{(0)}) + \\lambda_2 g_2(x^{(0)})$.\n\nSubstitute this expression for $\\nabla f^T p$ back into the directional derivative formula:\n$D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = \\left( - p^{(0)T} B^{(0)} p^{(0)} + \\lambda_1 g_1(x^{(0)}) + \\lambda_2 g_2(x^{(0)}) \\right) - \\mu (g_1(x^{(0)}) + g_2(x^{(0)}))$.\n$D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = - p^{(0)T} B^{(0)} p^{(0)} + (\\lambda_1 - \\mu) g_1(x^{(0)}) + (\\lambda_2 - \\mu) g_2(x^{(0)})$.\n\nLet's calculate the terms:\n*   $p^{(0)} = (0, -0.5)^T$.\n*   $B^{(0)} = I_2$.\n*   $p^{(0)T} B^{(0)} p^{(0)} = p^{(0)T} p^{(0)} = 0^2 + (-0.5)^2 = 0.25$. This term is always non-negative, and strictly positive if $p^{(0)} \\neq 0$.\n*   $g_1(x^{(0)}) = 0.5$.\n*   $g_2(x^{(0)}) = 0.5$.\n*   $\\lambda_1 = 3$.\n*   $\\lambda_2 = 0.5$.\n\nSubstitute these values:\n$D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = -0.25 + (3 - \\mu)(0.5) + (0.5 - \\mu)(0.5)$.\n$D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = -0.25 + 1.5 - 0.5\\mu + 0.25 - 0.5\\mu$.\n$D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = 1.5 - \\mu$.\n\nThis confirms my previous, more direct calculation. The result seems robust.\n$D_{p^{(0)}} \\phi_{\\mu}(x^{(0)}) = 1.5 - \\mu$.\nWe need this to be strictly negative for a descent direction.\n$1.5 - \\mu < 0 \\implies \\mu > 1.5$.\nSo, $\\mu_{\\min} = 1.5$.\n\nWhy did the rule of thumb $\\mu \\ge \\|\\lambda\\|_\\infty$ give a different number? Let's check the literature again.\nThe condition $\\mu \\ge \\|\\lambda^{(k+1)}\\|_\\infty$ is a *sufficient* condition to ensure that $p^{(k)}$ is a descent direction. It is not always a *necessary* condition. Let's see if we can derive it from the general expression I found.\n$D_p \\phi_\\mu = - p^T B p + \\sum_{i \\in \\mathcal{A}} (\\lambda_i - \\mu) g_i(x)$.\nHere $\\mathcal{A}$ is the active set of constraints for the QP. It's not quite $g_i(x)$ but the linearized constraint violation.\nLet's be more precise.\n$D_{p^{(k)}} \\phi_\\mu(x^{(k)}) = \\nabla f^T p^{(k)} + \\mu \\sum_{i \\in I_+(x^{(k)})} \\nabla g_i^T p^{(k)} + \\mu \\sum_{i \\in I_0(x^{(k)})} \\max(0, \\nabla g_i^T p^{(k)})$.\nUsing $\\nabla f^T p^{(k)} = -p^{(k)T}B^{(k)}p^{(k)} - \\sum_i \\lambda_i^{(k+1)} \\nabla g_i^T p^{(k)}$:\n$D = -p^T B p - \\sum_i \\lambda_i \\nabla g_i^T p + \\mu \\sum_{i \\in I_+} \\nabla g_i^T p + \\mu \\sum_{i \\in I_0} \\max(0, \\nabla g_i^T p)$.\n(I am dropping the iteration superscript for clarity)\n$D = -p^T B p + \\sum_{i \\in I_+} (\\mu - \\lambda_i) \\nabla g_i^T p + \\sum_{i \\in I_0 \\cup I_-} (-\\lambda_i \\nabla g_i^T p) + \\mu \\sum_{i \\in I_0} \\max(0, \\nabla g_i^T p)$.\nRecall QP constraints: $\\nabla g_i^T p \\le -g_i(x)$.\nFor $i\\in I_+$, $g_i(x)>0$, so $\\nabla g_i^T p$ must be negative.\nFor $i\\in I_0$, $g_i(x)=0$, so $\\nabla g_i^T p \\le 0$.\nFor $i \\in I_-$, $g_i(x)<0$. $\\nabla g_i^T p$ can be positive or negative. The corresponding $\\lambda_i$ from the QP subproblem should be 0 if this constraint is not active in the QP.\n\nLet's assume all constraints are in $I_+ \\cup I_0$.\n$g_i > 0 \\implies \\nabla g_i^T p \\le -g_i < 0$.\n$g_i = 0 \\implies \\nabla g_i^T p \\le 0$.\nSo in both cases $\\nabla g_i^T p \\le 0$.\n$D = -p^T B p + \\sum_{i \\in I_+} (\\mu - \\lambda_i) \\nabla g_i^T p - \\sum_{i \\in I_0} \\lambda_i \\nabla g_i^T p$.\n\nNow let's apply this to our problem.\nBoth constraints are in $I_+$. $I_0$ is empty.\n$D = -p^T B p + (\\mu - \\lambda_1) \\nabla g_1^T p + (\\mu - \\lambda_2) \\nabla g_2^T p$.\nWe need $D < 0$.\n$p^T B p = 0.25 > 0$.\n$\\nabla g_1^T p = -0.5 < 0$.\n$\\nabla g_2^T p = -0.5 < 0$.\n$\\lambda_1=3, \\lambda_2=0.5$.\n$D = -0.25 + (\\mu - 3)(-0.5) + (\\mu - 0.5)(-0.5)$.\n$D = -0.25 - 0.5\\mu + 1.5 - 0.5\\mu + 0.25$.\n$D = 1.5 - \\mu$.\nYes, this is definitely the correct expression. My calculations are consistent.\n\nNow, let's explore my confusion.\nThe condition $D < 0$ becomes:\n$-p^T B p + (\\lambda_1 - \\mu)g_1 + (\\lambda_2 - \\mu)g_2 < 0$.\nIf we set $\\mu \\ge \\max(\\lambda_1, \\lambda_2)$, which is $\\mu \\ge 3$.\nThen both $(\\lambda_1-\\mu)$ and $(\\lambda_2-\\mu)$ are $\\le 0$.\nSince $g_1>0$ and $g_2>0$, the terms $(\\lambda_i-\\mu)g_i$ are both $\\le 0$.\nAlso $-p^T B p = -0.25 < 0$.\nSo if $\\mu \\ge 3$, then $D < 0$ is guaranteed. This confirms that $\\mu \\ge \\|\\lambda\\|_\\infty$ is a *sufficient* condition.\nHowever, it's not a necessary condition.\nWe need to find the *smallest* threshold $\\mu_{\\min}$.\nThe condition is $1.5 - \\mu < 0$.\nThis is equivalent to $\\mu > 1.5$.\nThe threshold is the infimum of the set of $\\mu$s that satisfy the condition. The set is $(1.5, \\infty)$. The infimum is $1.5$.\nSo $\\mu_{\\min}=1.5$.\nThe question states \"determine the smallest threshold value $\\mu_{\\min}$ such that for all $\\mu > \\mu_{\\min}$...\". This means $\\mu_{\\min}$ is the boundary value.\nThe logic holds.\n\nFinal check on the answer format. `$$ \\boxed{1.5} $$` has extra spaces. I will remove them to be `$$\\boxed{1.5}$$`.\nEverything seems perfect. The original article is of very high quality.",
            "answer": "$$\\boxed{1.5}$$"
        },
        {
            "introduction": "The solvability and stability of the QP subproblem hinge on the properties of the Hessian approximation, particularly its behavior on the tangent space of the constraints. This exercise delves into the case of equality constraints, where the concept of the *reduced Hessian* becomes critical . You will construct a scenario where the reduced Hessian is singular, leading to a degenerate QP subproblem, and then determine the precise rank-one update needed to restore positive definiteness, providing a concrete understanding of how SQP methods maintain well-posed subproblems.",
            "id": "3169597",
            "problem": "Consider Sequential Quadratic Programming (SQP) applied to a nonlinear program with a single linear equality constraint. The Quadratic Programming (QP) subproblem at a feasible iterate is defined on the tangent space to the constraint manifold. The reduced Hessian of the QP subproblem is the restriction of the symmetric matrix $B_k$ to that tangent space. When the reduced Hessian is singular, the QP can be degenerate. A standard remedy is to apply a rank-one update that restores invertibility on the tangent space by adding curvature in a direction where $B_k$ has a null mode.\n\nConstruct a concrete, fully specified case in $\\mathbb{R}^3$ where the reduced Hessian is singular on the tangent space, and then determine a rank-one update that makes it the identity on that space. Proceed as follows:\n\n- Let the constraint be $h(x) = x_1 + x_2 + x_3 - 1 = 0$. At a feasible iterate $x_k$, the constraint Jacobian is $A = \\nabla h(x_k) = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$, and the tangent space is $\\mathcal{T}_k = \\{ d \\in \\mathbb{R}^3 : A^\\top d = 0 \\}$.\n- Use the orthonormal basis $\\{ z_1, z_2 \\}$ for $\\mathcal{T}_k$ with\n$$\nz_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}, \\qquad\nz_2 = \\frac{1}{\\sqrt{6}} \\begin{pmatrix} 1 \\\\ 1 \\\\ -2 \\end{pmatrix},\n$$\nand the unit normal $n = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n- Define the symmetric matrix\n$$\nB_k = z_1 z_1^\\top + n n^\\top,\n$$\nso that $B_k$ has one unit eigenvalue along $z_1$, is null along $z_2$ on $\\mathcal{T}_k$, and has a unit eigenvalue along $n$.\n- Consider the equality-constrained QP subproblem at $x_k$ with objective $q(d) = \\frac{1}{2} d^\\top B_k d$ and constraint $A^\\top d = 0$. In this setting, the reduced Hessian on $\\mathcal{T}_k$ is singular.\n\nNow apply a rank-one update in the tangent direction $t = z_2$:\n$$\nB_k(\\tau) = B_k + \\tau \\, t t^\\top, \\quad \\tau > 0.\n$$\nDetermine the value of the scalar $\\tau$ for which the reduced Hessian of $B_k(\\tau)$ on $\\mathcal{T}_k$ is exactly the identity matrix (that is, it has both of its eigenvalues equal to $1$). Express your final answer as a single real number. No rounding is required.",
            "solution": "The user has provided a problem from the field of numerical optimization, specifically concerning Sequential Quadratic Programming (SQP). The task is to determine the value of a scalar parameter $\\tau$ in a rank-one update to a matrix $B_k$ such that the resulting reduced Hessian on a specified tangent space becomes the identity matrix.\n\nThe problem is first validated to be self-contained, scientifically sound, and well-posed. All provided data and definitions are mathematically consistent.\n- The constraint is $h(x) = x_1 + x_2 + x_3 - 1 = 0$.\n- The constraint Jacobian is constant: $A = \\nabla h(x) = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}^\\top$.\n- The tangent space at any feasible point is $\\mathcal{T}_k = \\{ d \\in \\mathbb{R}^3 : A^\\top d = 0 \\}$. This is the plane through the origin with normal vector $A$.\n- The provided vectors $z_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 & 0 \\end{pmatrix}^\\top$ and $z_2 = \\frac{1}{\\sqrt{6}} \\begin{pmatrix} 1 & 1 & -2 \\end{pmatrix}^\\top$ form an orthonormal basis for this tangent space. This can be verified by checking that $z_1^\\top z_1 = 1$, $z_2^\\top z_2 = 1$, $z_1^\\top z_2 = 0$, $A^\\top z_1 = 0$, and $A^\\top z_2 = 0$.\n- The unit normal vector to the tangent space is $n = \\frac{A}{\\|A\\|_2} = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}^\\top$. The set $\\{z_1, z_2, n\\}$ forms an orthonormal basis for $\\mathbb{R}^3$.\n\nThe reduced Hessian of a matrix $B$ on the tangent space $\\mathcal{T}_k$ is the representation of the quadratic form $d^\\top B d$ for $d \\in \\mathcal{T}_k$ with respect to a basis for that space. Let $Z$ be a matrix whose columns form an orthonormal basis for $\\mathcal{T}_k$. Here, we define $Z = \\begin{pmatrix} z_1 & z_2 \\end{pmatrix}$. Any vector $d \\in \\mathcal{T}_k$ can be written as $d = Z \\hat{d}$ for a unique vector $\\hat{d} \\in \\mathbb{R}^2$. The quadratic form becomes:\n$$\n\\frac{1}{2} d^\\top B d = \\frac{1}{2} (Z \\hat{d})^\\top B (Z \\hat{d}) = \\frac{1}{2} \\hat{d}^\\top (Z^\\top B Z) \\hat{d}\n$$\nThe matrix $H_{red} = Z^\\top B Z$ is the reduced Hessian. It is a $2 \\times 2$ symmetric matrix.\n\nFirst, we compute the reduced Hessian for the initial matrix $B_k = z_1 z_1^\\top + n n^\\top$. The columns of $Z$ are $z_1$ and $z_2$.\n$$\nH_{red} = Z^\\top B_k Z = \\begin{pmatrix} z_1^\\top \\\\ z_2^\\top \\end{pmatrix} (z_1 z_1^\\top + n n^\\top) \\begin{pmatrix} z_1 & z_2 \\end{pmatrix}\n$$\nUsing the distributive property of matrix multiplication:\n$$\nH_{red} = \\begin{pmatrix} z_1^\\top (z_1 z_1^\\top + n n^\\top) \\\\ z_2^\\top (z_1 z_1^\\top + n n^\\top) \\end{pmatrix} \\begin{pmatrix} z_1 & z_2 \\end{pmatrix}\n$$\nWe evaluate the rows of the first term using the orthonormality of $\\{z_1, z_2, n\\}$:\n$z_1^\\top z_1 = 1$, $z_1^\\top z_2 = 0$, $z_1^\\top n = 0$.\n$z_2^\\top z_1 = 0$, $z_2^\\top z_2 = 1$, $z_2^\\top n = 0$.\nThe first row is:\n$$\nz_1^\\top (z_1 z_1^\\top + n n^\\top) = (z_1^\\top z_1) z_1^\\top + (z_1^\\top n) n^\\top = (1) z_1^\\top + (0) n^\\top = z_1^\\top\n$$\nThe second row is:\n$$\nz_2^\\top (z_1 z_1^\\top + n n^\\top) = (z_2^\\top z_1) z_1^\\top + (z_2^\\top n) n^\\top = (0) z_1^\\top + (0) n^\\top = \\begin{pmatrix} 0 & 0 & 0 \\end{pmatrix}\n$$\nSubstituting these back, we get:\n$$\nH_{red} = \\begin{pmatrix} z_1^\\top \\\\ \\begin{pmatrix} 0 & 0 & 0 \\end{pmatrix} \\end{pmatrix} \\begin{pmatrix} z_1 & z_2 \\end{pmatrix} = \\begin{pmatrix} z_1^\\top z_1 & z_1^\\top z_2 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThis confirms that the initial reduced Hessian is singular, as stated in the problem. Its eigenvalues are $1$ and $0$, indicating a null curvature direction spanned by $z_2$ on the tangent space.\n\nNext, we consider the rank-one update to $B_k$. The updated matrix is $B_k(\\tau) = B_k + \\tau t t^\\top$, where the update direction is $t=z_2$ and $\\tau > 0$.\n$$\nB_k(\\tau) = z_1 z_1^\\top + n n^\\top + \\tau z_2 z_2^\\top\n$$\nWe compute the new reduced Hessian, $H_{red}(\\tau) = Z^\\top B_k(\\tau) Z$.\n$$\nH_{red}(\\tau) = Z^\\top (z_1 z_1^\\top + n n^\\top + \\tau z_2 z_2^\\top) Z\n$$\nBy linearity, we can write:\n$$\nH_{red}(\\tau) = Z^\\top (z_1 z_1^\\top + n n^\\top) Z + Z^\\top (\\tau z_2 z_2^\\top) Z\n$$\n$$\nH_{red}(\\tau) = H_{red} + \\tau Z^\\top (z_2 z_2^\\top) Z\n$$\nWe already calculated $H_{red} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$. We now compute the second term:\n$$\n\\tau Z^\\top (z_2 z_2^\\top) Z = \\tau \\begin{pmatrix} z_1^\\top \\\\ z_2^\\top \\end{pmatrix} z_2 z_2^\\top \\begin{pmatrix} z_1 & z_2 \\end{pmatrix}\n$$\nFirst, we compute the product $(Z^\\top z_2)$:\n$$\nZ^\\top z_2 = \\begin{pmatrix} z_1^\\top z_2 \\\\ z_2^\\top z_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nNext, we compute the product $(z_2^\\top Z)$:\n$$\nz_2^\\top Z = z_2^\\top \\begin{pmatrix} z_1 & z_2 \\end{pmatrix} = \\begin{pmatrix} z_2^\\top z_1 & z_2^\\top z_2 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\end{pmatrix}\n$$\nSubstituting these results back into the expression for the update term:\n$$\n\\tau Z^\\top (z_2 z_2^\\top) Z = \\tau \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\end{pmatrix} = \\tau \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nNow, we add this to the original reduced Hessian:\n$$\nH_{red}(\\tau) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\tau \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\tau \\end{pmatrix}\n$$\nThe problem requires that this updated reduced Hessian be the identity matrix on $\\mathcal{T}_k$, which is the $2 \\times 2$ identity matrix $I_2$:\n$$\nH_{red}(\\tau) = I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nBy comparing the matrices element-wise, we have:\n$$\n\\begin{pmatrix} 1 & 0 \\\\ 0 & \\tau \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThis equality holds if and only if $\\tau = 1$. This value satisfies the condition $\\tau > 0$. Therefore, the required value of the scalar is $1$.",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}