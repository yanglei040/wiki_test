{
    "hands_on_practices": [
        {
            "introduction": "The most fundamental skill in applying the gradient projection method is the ability to compute the projection itself. This first exercise  tasks you with projecting a point onto a polyhedron, a versatile feasible set defined by a system of linear inequalities of the form $Ax \\le b$. By methodically applying the Karush-Kuhn-Tucker (KKT) conditions, you will develop a systematic process to find the closest point in the set, an exercise that builds core intuition for how constrained optimization works at the boundary.",
            "id": "3134385",
            "problem": "Consider the Euclidean projection operator onto a closed convex polyhedron $C \\subset \\mathbb{R}^{n}$ defined by $C = \\{ x \\in \\mathbb{R}^{n} : A x \\le b \\}$, where $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^{m}$. The projection of a point $z \\in \\mathbb{R}^{n}$ onto $C$ is the unique minimizer of the strictly convex quadratic optimization problem that minimizes the squared Euclidean distance to $z$ subject to the linear inequality constraints. This projection, and its first-order characterization, underpins the gradient projection method in constrained optimization. Starting from the fundamental definitions of Euclidean projection, convex sets, and the Lagrangian of a constrained optimization problem, derive the Karush-Kuhn-Tucker (KKT) conditions that characterize the projection $\\Pi_{C}(z)$. Then, use these conditions to compute the exact value of $\\Pi_{C}(z)$ for the concrete instance with $n = 2$, $m = 3$, \n$$\nA = \\begin{pmatrix}\n1  1\\\\\n-1  0\\\\\n0  -1\n\\end{pmatrix}, \n\\qquad\nb = \\begin{pmatrix}\n1\\\\\n0\\\\\n0\n\\end{pmatrix},\n\\qquad\nz = \\begin{pmatrix}\n2\\\\\n-\\frac{1}{2}\n\\end{pmatrix}.\n$$\nExpress the final answer as the projected point $\\Pi_{C}(z)$ in exact form as a $1 \\times 2$ row vector. No rounding is required, and no units are involved.",
            "solution": "The problem statement is critically validated and deemed valid. It is a well-posed problem in convex optimization, resting on established mathematical principles, and provides all necessary information for a unique solution.\n\nThe task is to find the Euclidean projection of a point $z \\in \\mathbb{R}^{n}$ onto a closed convex polyhedron $C = \\{ x \\in \\mathbb{R}^{n} : A x \\le b \\}$. By definition, the projection $\\Pi_{C}(z)$ is the unique point in $C$ that is closest to $z$. This can be formulated as a strictly convex quadratic optimization problem:\n$$\n\\begin{aligned}\n\\text{minimize}  \\quad f(x) = \\frac{1}{2} \\|x - z\\|_{2}^{2} \\\\\n\\text{subject to}  \\quad Ax \\le b\n\\end{aligned}\n$$\nThe factor of $\\frac{1}{2}$ is included for convenience as it simplifies the gradient without changing the minimizer, $x^* = \\Pi_{C}(z)$.\n\nThe constraints $Ax \\le b$ can be written component-wise as $g_i(x) = a_i^T x - b_i \\le 0$ for $i = 1, \\dots, m$, where $a_i^T$ is the $i$-th row of matrix $A$ and $b_i$ is the $i$-th component of vector $b$.\nSince the objective function $f(x)$ is convex and the constraints are linear (and thus convex), this is a convex optimization problem. Slater's condition holds if there exists a point $x_0$ such that $A x_0  b$ (strictly feasible). The existence of such a point is not guaranteed but is not necessary for strong duality for QPs. The Karush-Kuhn-Tucker (KKT) conditions are necessary and sufficient for optimality in this case.\n\nFirst, we derive the general KKT conditions. The Lagrangian function for this problem is:\n$$ L(x, \\lambda) = f(x) + \\sum_{i=1}^{m} \\lambda_i g_i(x) = \\frac{1}{2} (x-z)^T(x-z) + \\lambda^T(Ax - b) $$\nwhere $\\lambda \\in \\mathbb{R}^{m}$ is the vector of Lagrange multipliers.\n\nThe KKT conditions for a point $x^*$ to be the optimal solution are:\n1.  **Stationarity**: The gradient of the Lagrangian with respect to $x$ must be zero at the optimal point $x^*$:\n    $$ \\nabla_x L(x^*, \\lambda^*) = 0 $$\n    The gradient of the objective function is $\\nabla_x f(x) = x - z$. The gradient of the constraint term is $\\nabla_x(\\lambda^T(Ax-b)) = A^T \\lambda$.\n    Thus, the stationarity condition is:\n    $$ x^* - z + A^T \\lambda^* = 0 \\quad \\implies \\quad x^* = z - A^T \\lambda^* $$\n\n2.  **Primal Feasibility**: The optimal point $x^*$ must satisfy the problem constraints:\n    $$ A x^* \\le b $$\n\n3.  **Dual Feasibility**: The Lagrange multipliers must be non-negative:\n    $$ \\lambda^* \\ge 0 \\quad (\\text{i.e., } \\lambda_i^* \\ge 0 \\text{ for all } i = 1, \\dots, m) $$\n\n4.  **Complementary Slackness**: For each constraint, either the constraint is active (holds with equality) or the corresponding Lagrange multiplier is zero:\n    $$ \\lambda_i^*(a_i^T x^* - b_i) = 0 \\quad \\text{for all } i = 1, \\dots, m $$\n\nThese four conditions collectively characterize the projection $\\Pi_C(z) = x^*$.\n\nNext, we apply these conditions to the specific instance provided:\n$n=2$, $m=3$, and\n$$\nA = \\begin{pmatrix} 1  1\\\\ -1  0\\\\ 0  -1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1\\\\ 0\\\\ 0 \\end{pmatrix}, \\quad z = \\begin{pmatrix} 2\\\\ -\\frac{1}{2} \\end{pmatrix}\n$$\nThe feasible set $C$ is defined by the following three inequalities:\n1. $x_1 + x_2 \\le 1$\n2. $-x_1 \\le 0 \\implies x_1 \\ge 0$\n3. $-x_2 \\le 0 \\implies x_2 \\ge 0$\n\nThis set is a triangle in the $x_1x_2$-plane with vertices at $(0,0)$, $(1,0)$, and $(0,1)$.\nThe point to be projected is $z = (2, -1/2)$. We check if $z$ is in $C$:\n$z_1 + z_2 = 2 - 1/2 = 3/2  1$. The first constraint is violated.\n$z_2 = -1/2  0$. The third constraint is violated.\nThus, $z \\notin C$, and the projection $x^* = \\Pi_C(z)$ must lie on the boundary of $C$.\n\nFrom the stationarity condition $x^* = z - A^T \\lambda^*$, we have:\n$$ \\begin{pmatrix} x_1^* \\\\ x_2^* \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1/2 \\end{pmatrix} - \\begin{pmatrix} 1  -1  0 \\\\ 1  0  -1 \\end{pmatrix} \\begin{pmatrix} \\lambda_1^* \\\\ \\lambda_2^* \\\\ \\lambda_3^* \\end{pmatrix} $$\nThis gives the component-wise equations:\n$$ x_1^* = 2 - \\lambda_1^* + \\lambda_2^* $$\n$$ x_2^* = -1/2 - \\lambda_1^* + \\lambda_3^* $$\n\nWe now use the complementary slackness conditions to find the solution. We must determine which constraints are active (i.e., hold with equality) at the solution $x^*$. We can test the possible combinations of active constraints.\n\nLet's hypothesize which constraints are active based on the position of $z$. Since $z_1+z_2  1$ and $z_2  0$, it is plausible that constraints $1$ and $3$ are active.\n\n**Hypothesis: Constraints $1$ and $3$ are active.**\nThis implies $x_1^* + x_2^* = 1$ and $x_2^* = 0$. Solving this simple system gives the candidate point $x^* = (1, 0)$.\nAccording to complementary slackness, since these constraints are active, we may have $\\lambda_1^*  0$ and $\\lambda_3^*  0$. Since constraint $2$ is not assumed to be active, we must have $\\lambda_2^* = 0$.\n\nLet's check if there exist $\\lambda_1^* \\ge 0$ and $\\lambda_3^* \\ge 0$ that satisfy the stationarity conditions for $x^*=(1,0)$ and $\\lambda_2^*=0$.\nSubstitute $x_1^*=1$, $x_2^*=0$, and $\\lambda_2^*=0$ into the stationarity equations:\nFor $x_1^*$:\n$$ 1 = 2 - \\lambda_1^* + 0 \\implies \\lambda_1^* = 1 $$\nFor $x_2^*$:\n$$ 0 = -1/2 - \\lambda_1^* + \\lambda_3^* \\implies 0 = -1/2 - 1 + \\lambda_3^* \\implies \\lambda_3^* = \\frac{3}{2} $$\n\nNow we verify if this candidate solution, $x^* = (1, 0)$ and $\\lambda^* = (1, 0, 3/2)$, satisfies all KKT conditions.\n\n1.  **Stationarity**: Satisfied by construction.\n    $1 = 2 - 1 + 0 \\implies 1=1$.\n    $0 = -1/2 - 1 + 3/2 \\implies 0=0$.\n\n2.  **Primal Feasibility ($x^* \\in C$)**: We check $x^*=(1,0)$ against the constraints.\n    - $x_1^* + x_2^* = 1+0 = 1 \\le 1$. (Satisfied)\n    - $x_1^* = 1 \\ge 0$. (Satisfied)\n    - $x_2^* = 0 \\ge 0$. (Satisfied)\n    The point $x^*=(1,0)$ is in $C$.\n\n3.  **Dual Feasibility ($\\lambda^* \\ge 0$)**:\n    - $\\lambda_1^*=1 \\ge 0$. (Satisfied)\n    - $\\lambda_2^*=0 \\ge 0$. (Satisfied)\n    - $\\lambda_3^*=3/2 \\ge 0$. (Satisfied)\n    The vector $\\lambda^*$ is feasible.\n\n4.  **Complementary Slackness ($\\lambda_i^*(a_i^T x^* - b_i) = 0$)**:\n    - $\\lambda_1^*(x_1^*+x_2^*-1) = 1(1+0-1) = 0$. (Satisfied)\n    - $\\lambda_2^*(-x_1^*) = 0(-1) = 0$. (Satisfied because $\\lambda_2^*=0$)\n    - $\\lambda_3^*(-x_2^*) = (3/2)(-0) = 0$. (Satisfied)\n\nAll four KKT conditions are met. Since the optimization problem is strictly convex and the constraints are affine, satisfying the KKT conditions is both necessary and sufficient for a point to be the unique global minimum. Therefore, the projection of $z = (2, -1/2)$ onto the set $C$ is the point $x^* = (1, 0)$.\n\nThe final answer is the projected point $\\Pi_C(z) = (1,0)$, expressed as a $1 \\times 2$ row vector.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the general KKT approach is powerful, many practical problems involve sets with special structures that can be exploited for more efficient projections. This practice  focuses on the $\\ell_1$ ball, a constraint set famous for promoting sparsity in solutions, which is a desirable property in fields like machine learning and statistics. You will derive how the projection onto this set takes the elegant form of a \"soft-thresholding\" operator, a key component in widely used algorithms like the LASSO for sparse regression.",
            "id": "3134290",
            "problem": "Consider the constrained optimization problem of minimizing a differentiable function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ subject to membership in the convex set $C=\\{x\\in\\mathbb{R}^{n}:\\|x\\|_{1}\\leq \\tau\\}$, where $\\|x\\|_{1}=\\sum_{i=1}^{n}|x_{i}|$ is the $\\ell_{1}$ norm. The projection of a point $y\\in\\mathbb{R}^{n}$ onto $C$ is defined as the unique minimizer of the strictly convex problem\n$$\n\\min_{x\\in\\mathbb{R}^{n}}\\;\\frac{1}{2}\\|x-y\\|_{2}^{2}\\quad\\text{subject to}\\quad \\|x\\|_{1}\\leq \\tau,\n$$\nwhere $\\|x-y\\|_{2}$ is the Euclidean norm. Starting from this definition and the Karush–Kuhn–Tucker (KKT) conditions (Karush–Kuhn–Tucker (KKT)), derive the coordinatewise structure of the projection onto the $\\ell_{1}$ ball, showing how it acquires a soft-thresholding-like form. Then, integrate this projection into a single projected gradient step for a concrete instance as follows.\n\nLet $n=4$, $\\tau=2$, and\n$$\nf(x)=\\frac{1}{2}\\|x-c\\|_{2}^{2},\\qquad c=\\begin{pmatrix}3\\\\ -1\\\\ 0.8\\\\ 0.1\\end{pmatrix}.\n$$\nTake the starting point $x^{0}=\\begin{pmatrix}0\\\\ 0\\\\ 0\\\\ 0\\end{pmatrix}$ and the stepsize $\\alpha=1$. Perform one projected gradient iteration by taking a gradient step from $x^{0}$ and projecting back onto $C$ using the projection formula you derived. Report the resulting iterate $x^{1}$ as a row vector. No rounding is required.",
            "solution": "### Part 1: Derivation of the Projection onto the $\\ell_1$ Ball\n\nThe projection of a point $y \\in \\mathbb{R}^n$ onto the convex set $C = \\{x \\in \\mathbb{R}^n : \\|x\\|_1 \\le \\tau\\}$ is the solution to the strictly convex optimization problem:\n$$ \\min_{x \\in \\mathbb{R}^n} \\frac{1}{2} \\|x-y\\|_2^2 \\quad \\text{subject to} \\quad \\|x\\|_1 \\le \\tau $$\nThe objective is $\\frac{1}{2}\\sum_{i=1}^n (x_i - y_i)^2$ and the constraint is $\\sum_{i=1}^n |x_i| \\le \\tau$. The Lagrangian is:\n$$ L(x, \\lambda) = \\frac{1}{2} \\|x-y\\|_2^2 + \\lambda (\\|x\\|_1 - \\tau) $$\nwhere $\\lambda \\ge 0$ is the Lagrange multiplier.\n\nThe KKT stationarity condition states that zero must be in the subdifferential of the Lagrangian with respect to $x$ at the optimum $x^*$: $0 \\in \\partial_x L(x^*, \\lambda) = (x^* - y) + \\lambda \\partial \\|x^*\\|_1$. This means there exists a subgradient $g \\in \\partial \\|x^*\\|_1$ such that $y - x^* = \\lambda g$. The components $g_i$ of the subgradient are $g_i = \\mathrm{sign}(x_i^*)$ if $x_i^* \\neq 0$, and $g_i \\in [-1, 1]$ if $x_i^* = 0$.\n\nAnalyzing component-wise ($y_i - x_i^* = \\lambda g_i$), we find that the solution has the form of a soft-thresholding operation:\n$$ x_i^* = S_\\lambda(y_i) = \\mathrm{sign}(y_i) \\max(|y_i| - \\lambda, 0) $$\nThe value of $\\lambda \\ge 0$ is determined by the complementary slackness condition $\\lambda(\\|x^*\\|_1 - \\tau) = 0$.\n- If $\\|y\\|_1 \\le \\tau$, we can choose $\\lambda = 0$, which gives $x^* = y$. The point is its own projection.\n- If $\\|y\\|_1 > \\tau$, we must have $\\lambda > 0$, which implies $\\|x^*\\|_1 = \\tau$. This leads to the equation $\\sum_{i=1}^n \\max(|y_i| - \\lambda, 0) = \\tau$, which must be solved for the unique $\\lambda > 0$.\n\n### Part 2: Application to the Concrete Instance\n\nThe projected gradient update is $x^{1} = P_C(x^0 - \\alpha \\nabla f(x^0))$.\n- Given: $n=4$, $\\tau=2$, $f(x)=\\frac{1}{2}\\|x-c\\|_{2}^{2}$ with $c=(3, -1, 0.8, 0.1)^T$, $x^0 = 0$, $\\alpha=1$.\n- Gradient: $\\nabla f(x) = x-c$. At $x^0$, $\\nabla f(x^0) = -c = (-3, 1, -0.8, -0.1)^T$.\n- Gradient step: The point to project is $y = x^0 - \\alpha \\nabla f(x^0) = 0 - 1(-c) = c = (3, -1, 0.8, 0.1)^T$.\n\nNow, we project $y$ onto the $\\ell_1$ ball $C = \\{x \\in \\mathbb{R}^4 : \\|x\\|_1 \\le 2\\}$.\n- Check norm: $\\|y\\|_1 = |3| + |-1| + |0.8| + |0.1| = 4.9 > 2$. The point is outside the ball, so we must find $\\lambda > 0$.\n- Solve for $\\lambda$: We need to find $\\lambda$ such that $\\sum \\max(|y_i| - \\lambda, 0) = 2$.\nThe absolute values $|y_i|$ are $3, 1, 0.8, 0.1$.\nThe equation is $\\max(3-\\lambda,0) + \\max(1-\\lambda,0) + \\max(0.8-\\lambda,0) + \\max(0.1-\\lambda,0) = 2$.\nBy inspection or using a systematic algorithm, we test ranges for $\\lambda$. If we assume $1 \\le \\lambda  3$, the equation becomes $(3-\\lambda) = 2$, which gives $\\lambda=1$. This value is consistent with the assumed range. Thus, the threshold is $\\lambda=1$.\n\n- Apply soft-thresholding with $\\lambda=1$:\n$x_1^1 = S_1(3) = \\mathrm{sign}(3)\\max(|3|-1, 0) = 2$.\n$x_2^1 = S_1(-1) = \\mathrm{sign}(-1)\\max(|-1|-1, 0) = 0$.\n$x_3^1 = S_1(0.8) = \\mathrm{sign}(0.8)\\max(|0.8|-1, 0) = 0$.\n$x_4^1 = S_1(0.1) = \\mathrm{sign}(0.1)\\max(|0.1|-1, 0) = 0$.\n\nThe resulting iterate is $x^1 = (2, 0, 0, 0)^T$. Checking the constraint, $\\|x^1\\|_1 = 2$, as required.",
            "answer": "$$\n\\boxed{\\begin{pmatrix}\n2  0  0  0\n\\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having mastered the projection step, we now turn to the full Gradient Projection algorithm and its iterative behavior. This coding practice  uses the accessible case of box constraints, where the projection is a simple component-wise clipping operation. Your task is to devise and experimentally validate a criterion that predicts when an iterate will \"stick\" to a boundary, providing a hands-on understanding of active-set identification and the dynamics of convergence in constrained optimization.",
            "id": "3134326",
            "problem": "Consider the task of minimizing a continuously differentiable convex quadratic function $f:\\mathbb{R}^n \\to \\mathbb{R}$ over a box-constrained convex set. The objective function is $f(x) = \\frac{1}{2} x^\\top Q x + c^\\top x$, where $Q \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and $c \\in \\mathbb{R}^n$. The feasible set is $[l, u] = \\{x \\in \\mathbb{R}^n : l \\le x \\le u\\}$ for given vectors $l,u \\in \\mathbb{R}^n$ satisfying $l_i \\le u_i$ for all indices $i$. Define the gradient $\\nabla f(x) = Qx + c$. The gradient projection method generates iterates by $x^{t+1} = P_{[l,u]}(x^t - \\alpha \\nabla f(x^t))$, where $P_{[l,u]}(\\cdot)$ is the Euclidean projection onto the box $[l,u]$ performed component-wise by clipping, and $\\alpha  0$ is a fixed stepsize.\n\nStarting from the core definitions of projection onto convex sets and the first-order optimality conditions for constrained convex optimization, devise and justify a principled criterion that uses the gradient to predict whether coordinates that are currently at their bounds will remain active (i.e., will stay exactly on the same bounds) at the very next iterate produced by the gradient projection method. Then implement an experiment that:\n- Detects when coordinates hit bounds (become active) during the iterations.\n- At each iteration $t$, for each coordinate $i$ that is active at either $l_i$ or $u_i$, uses your gradient-based criterion at $x^t$ to predict whether that coordinate will remain active at the next iterate $x^{t+1}$.\n- Performs the gradient projection update and verifies whether each prediction was correct.\n- Aggregates the prediction accuracy across all iterations and all active coordinates, defined as the number of correct predictions divided by the total number of predictions.\n\nUse the following test suite, which covers a general case, coupling effects, and an edge case involving zero gradient at an active coordinate. In all cases, treat a coordinate $i$ as active at the lower bound if $|x_i^t - l_i| \\le 10^{-12}$ and at the upper bound if $|x_i^t - u_i| \\le 10^{-12}$.\n\n- Test Case 1 (happy path, diagonal Hessian):\n  - Dimension $n = 3$.\n  - Matrix $Q = \\mathrm{diag}(2,3,4)$.\n  - Vector $c = \\begin{bmatrix} -2 \\\\ 1 \\\\ 0.5 \\end{bmatrix}$.\n  - Lower bounds $l = \\begin{bmatrix} 0 \\\\ -1 \\\\ -0.5 \\end{bmatrix}$.\n  - Upper bounds $u = \\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix}$.\n  - Initial point $x^0 = \\begin{bmatrix} 0 \\\\ 2 \\\\ -0.5 \\end{bmatrix}$.\n  - Stepsize $\\alpha = 0.3$.\n  - Number of iterations $T = 20$.\n\n- Test Case 2 (coupled Hessian with off-diagonal entries):\n  - Dimension $n = 4$.\n  - Matrix $Q = \\begin{bmatrix} 4  1  0  0 \\\\ 1  3  1  0 \\\\ 0  1  2  1 \\\\ 0  0  1  1.5 \\end{bmatrix}$.\n  - Vector $c = \\begin{bmatrix} 0.5 \\\\ -1 \\\\ 0 \\\\ 1 \\end{bmatrix}$.\n  - Lower bounds $l = \\begin{bmatrix} -1 \\\\ -1 \\\\ -1 \\\\ -1 \\end{bmatrix}$.\n  - Upper bounds $u = \\begin{bmatrix} 1 \\\\ 0.5 \\\\ 0.5 \\\\ 1 \\end{bmatrix}$.\n  - Initial point $x^0 = \\begin{bmatrix} 1 \\\\ 0.5 \\\\ -1 \\\\ -0.5 \\end{bmatrix}$.\n  - Stepsize $\\alpha = 0.25$.\n  - Number of iterations $T = 30$.\n\n- Test Case 3 (edge case with zero gradient at an active coordinate):\n  - Dimension $n = 3$.\n  - Matrix $Q = \\mathrm{diag}(1,1,1)$.\n  - Vector $c = \\begin{bmatrix} -1 \\\\ 0 \\\\ 1 \\end{bmatrix}$.\n  - Lower bounds $l = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n  - Upper bounds $u = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n  - Initial point $x^0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$.\n  - Stepsize $\\alpha = 0.5$.\n  - Number of iterations $T = 10$.\n\nYour program must:\n1. Implement the gradient projection iteration for the specified $f$, $[l,u]$, and $\\alpha$ for each test case.\n2. At each iteration and for each currently active coordinate, compute a prediction based on your gradient-based criterion about whether it will remain active at the next iterate, and then verify the prediction after performing the update.\n3. Compute the accuracy for each test case as a real number in $[0,1]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, such as `[r_1,r_2,r_3]`, where each $r_k$ is the prediction accuracy for Test Case $k$ represented as a decimal number. No other text should be printed.\n- Angles are not involved, and there are no physical units. All reported numbers must be unitless.",
            "solution": "The problem requires the derivation of a criterion to predict whether an active coordinate in a box-constrained optimization task will remain active in the next iterate of the gradient projection method. This criterion must be principled, grounded in the mechanics of the algorithm and optimization theory.\n\nThe optimization problem is to minimize a convex quadratic function $f(x) = \\frac{1}{2} x^\\top Q x + c^\\top x$ subject to the constraint $x \\in [l, u]$, where $[l, u]$ is a box defined by lower and upper bound vectors $l$ and $u$. The matrix $Q$ is symmetric positive definite, ensuring $f(x)$ is strictly convex. The gradient of the objective function is $\\nabla f(x) = Qx + c$.\n\nThe gradient projection method generates a sequence of iterates $\\{x^t\\}$ via the update rule:\n$$\nx^{t+1} = P_{[l,u]}(x^t - \\alpha \\nabla f(x^t))\n$$\nwhere $\\alpha  0$ is a fixed stepsize and $P_{[l,u]}(\\cdot)$ is the Euclidean projection onto the feasible set $[l,u]$. For a box constraint, this projection is separable and can be computed component-wise. For each coordinate $i \\in \\{1, \\dots, n\\}$, the update is:\n$$\nx_i^{t+1} = P_{[l_i, u_i]}(x_i^t - \\alpha g_i^t)\n$$\nwhere $g_i^t = (\\nabla f(x^t))_i$ is the $i$-th component of the gradient at $x^t$. The one-dimensional projection operator, also known as clipping, is defined as:\n$$\nP_{[l_i, u_i]}(y_i) = \\max(l_i, \\min(u_i, y_i))\n$$\n\nA coordinate $x_i^t$ is considered active if it lies on one of its bounds, i.e., $x_i^t = l_i$ or $x_i^t = u_i$. We need to devise a criterion using the gradient $g^t$ to predict whether $x_i^{t+1}$ will be equal to the same bound value.\n\n**Derivation of the Prediction Criterion**\n\nLet's analyze the conditions under which an active coordinate remains active.\n\n**Case 1: Coordinate $i$ is active at the lower bound.**\nSuppose at iteration $t$, coordinate $i$ is at its lower bound, so $x_i^t = l_i$. The update rule for this component becomes:\n$$\nx_i^{t+1} = \\max(l_i, \\min(u_i, l_i - \\alpha g_i^t))\n$$\nFor this coordinate to remain active at the lower bound, we must have $x_i^{t+1} = l_i$. This equality holds if and only if the argument to the outer $\\max$ function is less than or equal to $l_i$:\n$$\n\\min(u_i, l_i - \\alpha g_i^t) \\le l_i\n$$\nSince we are given that $l_i \\le u_i$, the minimum of $u_i$ and any other value can be at most $u_i$. The condition above is satisfied if the second argument of the $\\min$ function is less than or equal to $l_i$:\n$$\nl_i - \\alpha g_i^t \\le l_i\n$$\nSubtracting $l_i$ from both sides gives:\n$$\n-\\alpha g_i^t \\le 0\n$$\nSince the stepsize $\\alpha$ is strictly positive ($\\alpha  0$), we can divide by $-\\alpha$ and reverse the inequality sign:\n$$\ng_i^t \\ge 0\n$$\nTherefore, a coordinate $i$ active at its lower bound $l_i$ at iteration $t$ will remain active at $l_i$ at iteration $t+1$ if and only if the $i$-th component of the gradient, $g_i^t = (\\nabla f(x^t))_i$, is non-negative.\n\nThis criterion is intuitive. A non-negative gradient component $g_i^t \\ge 0$ at $x_i^t = l_i$ implies that the function $f$ is either stationary or increasing along coordinate $i$ into the feasible region. The gradient descent step $x_i^t - \\alpha g_i^t$ attempts to move to a value less than or equal to $l_i$. The projection operator then ensures the point remains at the boundary $l_i$.\n\n**Case 2: Coordinate $i$ is active at the upper bound.**\nNow, suppose at iteration $t$, coordinate $i$ is at its upper bound, so $x_i^t = u_i$. The update rule is:\n$$\nx_i^{t+1} = \\max(l_i, \\min(u_i, u_i - \\alpha g_i^t))\n$$\nFor this coordinate to remain active at the upper bound, we must have $x_i^{t+1} = u_i$. This occurs if the inner term $\\min(u_i, u_i - \\alpha g_i^t)$ evaluates to $u_i$. This is true if and only if:\n$$\nu_i \\le u_i - \\alpha g_i^t\n$$\nSubtracting $u_i$ from both sides gives:\n$$\n0 \\le -\\alpha g_i^t\n$$\nAgain, since $\\alpha  0$, this simplifies to:\n$$\ng_i^t \\le 0\n$$\nTherefore, a coordinate $i$ active at its upper bound $u_i$ at iteration $t$ will remain active at $u_i$ at iteration $t+1$ if and only if the $i$-th component of the gradient, $g_i^t$, is non-positive.\n\nThis criterion is also consistent with optimization principles. A non-positive gradient component $g_i^t \\le 0$ at $x_i^t = u_i$ indicates that the function is stationary or increasing as we move away from the boundary $u_i$ towards the exterior of the feasible set. A gradient descent step thus attempts to move to a value greater than or equal to $u_i$, which the projection clips back to $u_i$.\n\n**Final Criterion**\n\nThe gradient-based criterion to predict if an active coordinate remains active is as follows:\nLet $x^t$ be the current iterate and $g^t = \\nabla f(x^t)$ be the gradient. For any coordinate $i$:\n1.  If $x_i^t$ is active at the lower bound $l_i$, it is predicted to **remain active** if and only if $g_i^t \\ge 0$.\n2.  If $x_i^t$ is active at the upper bound $u_i$, it is predicted to **remain active** if and only if $g_i^t \\le 0$.\n\nThis set of conditions is directly related to the first-order necessary conditions for optimality (Karush-Kuhn-Tucker conditions) for this problem. A point $x^*$ is a local minimum only if for every coordinate $i$:\n- If $l_i  x_i^*  u_i$, then $(\\nabla f(x^*))_i = 0$.\n- If $x_i^* = l_i$, then $(\\nabla f(x^*))_i \\ge 0$.\n- If $x_i^* = u_i$, then $(\\nabla f(x^*))_i \\le 0$.\nOur criterion essentially checks if the iterate $x^t$ satisfies a portion of the optimality conditions for its active set. If it does, the gradient projection step for that coordinate is stationary.\n\nThe implementation will simulate the gradient projection method for a fixed number of iterations. At each iteration, for every coordinate identified as active (within a numerical tolerance of $10^{-12}$), it will apply the criterion above to make a prediction. It will then compute the next iterate and verify if the prediction was correct, aggregating these results to compute the overall accuracy.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_experiment(Q, c, l, u, x0, alpha, T):\n    \"\"\"\n    Runs the gradient projection experiment for a single test case.\n\n    Args:\n        Q (np.ndarray): The Hessian matrix of the quadratic term.\n        c (np.ndarray): The linear term vector.\n        l (np.ndarray): The lower bounds vector.\n        u (np.ndarray): The upper bounds vector.\n        x0 (np.ndarray): The initial point.\n        alpha (float): The stepsize.\n        T (int): The number of iterations.\n\n    Returns:\n        float: The prediction accuracy.\n    \"\"\"\n    n = len(x0)\n    x = np.array(x0, dtype=float)\n    l = np.array(l, dtype=float)\n    u = np.array(u, dtype=float)\n    Q = np.array(Q, dtype=float)\n    c = np.array(c, dtype=float)\n\n    total_predictions = 0\n    correct_predictions = 0\n    tolerance = 1e-12\n\n    for _ in range(T):\n        # 1. Calculate gradient at the current iterate x\n        grad = Q @ x + c\n\n        # 2. Identify active coordinates and make predictions\n        # A prediction is a dictionary storing info about an active coordinate\n        predictions_this_step = []\n        for i in range(n):\n            is_active_lower = np.abs(x[i] - l[i]) = tolerance\n            is_active_upper = np.abs(x[i] - u[i]) = tolerance\n            \n            predicted_will_remain = None\n            bound_type = None\n\n            if is_active_lower:\n                # Criterion for remaining at lower bound: grad_i = 0\n                predicted_will_remain = (grad[i] = 0)\n                bound_type = 'lower'\n            elif is_active_upper:\n                # Criterion for remaining at upper bound: grad_i = 0\n                predicted_will_remain = (grad[i] = 0)\n                bound_type = 'upper'\n\n            if predicted_will_remain is not None:\n                total_predictions += 1\n                predictions_this_step.append({\n                    'index': i,\n                    'predicted_will_remain': predicted_will_remain,\n                    'bound_type': bound_type\n                })\n\n        # 3. Perform the gradient projection update\n        x_unprojected = x - alpha * grad\n        x_next = np.clip(x_unprojected, l, u)\n\n        # 4. Verify predictions against the actual outcome\n        for pred_info in predictions_this_step:\n            i = pred_info['index']\n            predicted_will_remain = pred_info['predicted_will_remain']\n            bound_type = pred_info['bound_type']\n            \n            actual_remained = False\n            if bound_type == 'lower':\n                if np.abs(x_next[i] - l[i]) = tolerance:\n                    actual_remained = True\n            elif bound_type == 'upper':\n                if np.abs(x_next[i] - u[i]) = tolerance:\n                    actual_remained = True\n\n            if predicted_will_remain == actual_remained:\n                correct_predictions += 1\n                \n        # 5. Update x for the next iteration\n        x = x_next\n        \n    if total_predictions == 0:\n        # If no predictions were ever made, accuracy is vacuously 100%.\n        return 1.0\n    \n    return correct_predictions / total_predictions\n\ndef solve():\n    \"\"\"\n    Defines the test cases, runs the experiments, and prints the results.\n    \"\"\"\n    test_cases = [\n        # Test Case 1\n        {\n            \"Q\": np.diag([2.0, 3.0, 4.0]),\n            \"c\": np.array([-2.0, 1.0, 0.5]),\n            \"l\": np.array([0.0, -1.0, -0.5]),\n            \"u\": np.array([1.0, 2.0, 1.0]),\n            \"x0\": np.array([0.0, 2.0, -0.5]),\n            \"alpha\": 0.3,\n            \"T\": 20\n        },\n        # Test Case 2\n        {\n            \"Q\": np.array([\n                [4.0, 1.0, 0.0, 0.0],\n                [1.0, 3.0, 1.0, 0.0],\n                [0.0, 1.0, 2.0, 1.0],\n                [0.0, 0.0, 1.0, 1.5]\n            ]),\n            \"c\": np.array([0.5, -1.0, 0.0, 1.0]),\n            \"l\": np.array([-1.0, -1.0, -1.0, -1.0]),\n            \"u\": np.array([1.0, 0.5, 0.5, 1.0]),\n            \"x0\": np.array([1.0, 0.5, -1.0, -0.5]),\n            \"alpha\": 0.25,\n            \"T\": 30\n        },\n        # Test Case 3\n        {\n            \"Q\": np.diag([1.0, 1.0, 1.0]),\n            \"c\": np.array([-1.0, 0.0, 1.0]),\n            \"l\": np.array([0.0, 0.0, 0.0]),\n            \"u\": np.array([1.0, 1.0, 1.0]),\n            \"x0\": np.array([0.0, 0.0, 1.0]),\n            \"alpha\": 0.5,\n            \"T\": 10\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        accuracy = run_experiment(**params)\n        results.append(accuracy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}