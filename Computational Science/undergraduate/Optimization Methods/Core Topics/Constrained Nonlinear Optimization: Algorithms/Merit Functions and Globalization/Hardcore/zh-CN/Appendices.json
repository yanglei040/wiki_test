{
    "hands_on_practices": [
        {
            "introduction": "罚函数中的罚参数 $\\mu$ 在平衡目标函数下降与约束可行性方面起着至关重要的作用。本练习通过一个假设场景，旨在让你亲手计算并比较两种常用罚函数（二次罚函数与 $\\ell_1$ 罚函数）为确保一个给定的搜索方向是下降方向所需的最小罚参数。通过这个计算 ，你将深刻理解不同罚项对算法全局化行为的敏感性差异。",
            "id": "3149223",
            "problem": "考虑一个具有单个标量约束的光滑等式约束优化问题。在迭代点 $x_k$ 处，假设我们使用关于一个标量罚函数的线搜索来对候选步长 $p$ 进行全局化。定义两个基于相同罚参数 $\\mu  0$ 构建的罚函数：\n- 平方二范数罚函数：$\\phi_{2}(x;\\mu) = f(x) + \\frac{\\mu}{2}\\,c(x)^2$。\n- 一范数罚函数：$\\phi_{1}(x;\\mu) = f(x) + \\mu\\,|c(x)|$。\n\n假设在 $x_k$ 处的局部数据如下：\n- 标量约束违反量为 $c(x_k) = 10^{-3}  0$。\n- 沿着候选步长 $p$，约束的方向导数为 $Jc(x_k)\\,p = -1$，因此沿 $p$ 方向移动会以单位速率减小约束违反量。\n- 沿着 $p$，目标函数方向导数为 $g(x_k)^{\\top}p = 0.05$，因此沿 $p$ 方向移动会增加目标函数值。\n\n标准的 Armijo 回溯线搜索要求罚函数在 $t=0$ 处具有严格为负的方向导数，以便接受足够小的步长。仅使用方向导数的基本定义和链式法则，对每个罚函数确定最小的罚参数 $\\mu$，使得 $p$ 在 $x_k$ 处是一个下降方向。然后计算比率\n$$\nR \\;=\\; \\frac{\\mu_{2,\\min}}{\\mu_{1,\\min}},\n$$\n其中 $\\mu_{2,\\min}$ 对应于 $\\phi_{2}$，$\\mu_{1,\\min}$ 对应于 $\\phi_{1}$。提供 $R$ 的数值，四舍五入到四位有效数字。不需要单位。",
            "solution": "该问题是有效的，因为它科学地基于数值优化的原理，是适定的、客观的且内部一致的。我们可以进行形式化的求解。\n\n设函数 $\\psi(x)$ 定义在 $\\mathbb{R}^n$ 上。$\\psi$ 在点 $x_k$ 处沿向量 $p$ 方向的方向导数由下式给出\n$$\nD_p \\psi(x_k) = \\lim_{t \\to 0^+} \\frac{\\psi(x_k + tp) - \\psi(x_k)}{t}\n$$\n如果 $\\psi$ 在 $x_k$ 处是连续可微的，其方向导数由 $D_p \\psi(x_k) = \\nabla \\psi(x_k)^\\top p$ 给出。如果 $D_p \\psi(x_k)  0$，则方向 $p$ 是 $\\psi$ 在 $x_k$ 处的下降方向。问题要求我们为两个不同的罚函数找到最小的罚参数 $\\mu$，使得给定的步长 $p$ 是一个下降方向。\n\n我们在迭代点 $x_k$ 处有以下已知数据：\n-   目标函数梯度分量：$\\nabla f(x_k)^\\top p = 0.05$。\n-   约束函数值：$c(x_k) = 10^{-3}$。\n-   约束梯度分量：$\\nabla c(x_k)^\\top p = -1$。\n\n**平方二范数罚函数 $\\phi_{2}(x;\\mu)$ 的分析**\n\n平方二范数罚函数定义为\n$$\n\\phi_{2}(x;\\mu) = f(x) + \\frac{\\mu}{2}\\,c(x)^2\n$$\n其中 $\\mu  0$ 是罚参数。由于 $f(x)$ 和 $c(x)$ 都是光滑函数，因此 $\\phi_2(x;\\mu)$ 也是光滑的。我们可以使用链式法则求出它在 $x_k$ 处沿方向 $p$ 的方向导数：\n$$\nD_p \\phi_{2}(x_k; \\mu) = D_p f(x_k) + D_p \\left( \\frac{\\mu}{2} c(x)^2 \\right) \\Big|_{x_k}\n$$\n罚项的导数是\n$$\nD_p \\left( \\frac{\\mu}{2} c(x)^2 \\right) \\Big|_{x_k} = \\frac{\\mu}{2} \\cdot 2 c(x_k) \\cdot D_p c(x_k) = \\mu \\, c(x_k) \\, (\\nabla c(x_k)^\\top p)\n$$\n因此，罚函数的方向导数是\n$$\nD_p \\phi_{2}(x_k; \\mu) = \\nabla f(x_k)^\\top p + \\mu \\, c(x_k) \\, (\\nabla c(x_k)^\\top p)\n$$\n为使 $p$ 成为下降方向，我们必须有 $D_p \\phi_{2}(x_k; \\mu)  0$。代入给定值：\n$$\n0.05 + \\mu \\, (10^{-3}) \\, (-1)  0\n$$\n$$\n0.05 - 10^{-3}\\mu  0\n$$\n$$\n0.05  10^{-3}\\mu\n$$\n$$\n\\mu > \\frac{0.05}{10^{-3}} = 50\n$$\n使 $p$ 成为下降方向的罚参数集合是 $(50, \\infty)$。问题要求的是该集合的最小参数，即该集合的下确界。\n$$\n\\mu_{2,\\min} = 50\n$$\n\n**一范数罚函数 $\\phi_{1}(x;\\mu)$ 的分析**\n\n一范数罚函数定义为\n$$\n\\phi_{1}(x;\\mu) = f(x) + \\mu\\,|c(x)|\n$$\n为了求方向导数，我们必须处理绝对值项 $|c(x)|$。我们对 $\\mu\\,|c(x)|$ 这一项使用方向导数的基本定义：\n$$\nD_p (\\mu\\,|c(x)|) \\Big|_{x_k} = \\mu \\lim_{t \\to 0^+} \\frac{|c(x_k+tp)| - |c(x_k)|}{t}\n$$\n给定 $c(x_k) = 10^{-3}  0$。由于 $c(x)$ 是一个光滑函数，对于小的 $t  0$，我们可以使用一阶泰勒展开：\n$$\nc(x_k+tp) = c(x_k) + t (\\nabla c(x_k)^\\top p) + O(t^2)\n$$\n代入给定值：\n$$\nc(x_k+tp) = 10^{-3} + t(-1) + O(t^2) = 10^{-3} - t + O(t^2)\n$$\n对于足够小的 $t  0$，$c(x_k+tp)$ 的值将保持为正。因此，当 $t \\to 0^+$ 时，我们有 $|c(x_k+tp)| = c(x_k+tp)$ 和 $|c(x_k)| = c(x_k)$。极限变为：\n$$\n\\lim_{t \\to 0^+} \\frac{c(x_k+tp) - c(x_k)}{t}\n$$\n这就是 $c(x)$ 在 $x_k$ 处沿方向 $p$ 的方向导数的定义，即 $\\nabla c(x_k)^\\top p$。所以，\n$$\nD_p (\\mu\\,|c(x)|) \\Big|_{x_k} = \\mu (\\nabla c(x_k)^\\top p)\n$$\n完整的罚函数 $\\phi_1$ 的方向导数是：\n$$\nD_p \\phi_{1}(x_k; \\mu) = \\nabla f(x_k)^\\top p + \\mu \\, (\\nabla c(x_k)^\\top p)\n$$\n为使 $p$ 成为下降方向，我们需要 $D_p \\phi_{1}(x_k; \\mu)  0$。代入给定值：\n$$\n0.05 + \\mu(-1)  0\n$$\n$$\n0.05 - \\mu  0\n$$\n$$\n\\mu > 0.05\n$$\n最小的罚参数是集合 $(0.05, \\infty)$ 的下确界。\n$$\n\\mu_{1,\\min} = 0.05\n$$\n\n**比率 R 的计算**\n\n问题要求计算比率 $R = \\frac{\\mu_{2,\\min}}{\\mu_{1,\\min}}$。使用我们计算出的值：\n$$\nR = \\frac{50}{0.05} = \\frac{50}{5 \\times 10^{-2}} = \\frac{50}{5} \\times 10^2 = 10 \\times 100 = 1000\n$$\n问题要求答案应四舍五入到四位有效数字。确切值为 $1000$。为了用四位有效数字表示，我们可以将其写成科学记数法 $1.000 \\times 10^3$。",
            "answer": "$$\\boxed{1.000 \\times 10^{3}}$$"
        },
        {
            "introduction": "尽管 $\\ell_1$ 精确罚函数在理论上具有优良性质，但它在可行点处引入了非光滑的“扭结”（kink），给基于梯度的优化算法带来了挑战。本练习  将引导你从基本定义出发，分析这种不可微性，并计算它对线搜索步长选择的具体影响。这个过程将揭示在非光滑点附近进行全局化时，为何必须采用如次梯度等更鲁棒的策略。",
            "id": "3149246",
            "problem": "考虑具有单变量 $x \\in \\mathbb{R}$ 的等式约束非线性规划问题：\n最小化 $f(x)$，约束为 $g(x) = 0$，\n其中 $f(x) = (x - 2)^{2}$ 且 $g(x) = x^{2} - 1$。设精确罚函数为 $\\Phi_{\\rho}(x) = f(x) + \\rho \\, |g(x)|$，其中罚参数 $\\rho  0$。\n\n任务A：仅使用绝对值和方向导数的核心定义，解释为什么 $\\Phi_{\\rho}(x)$ 在每个可行点（即满足 $g(x) = 0$ 的任意 $x$）处都有一个不可微的扭结。你的解释应明确引用 $|g(x)|$ 在可行点处的行为，以及它如何影响 $\\Phi_{\\rho}(x)$ 的可微性。\n\n任务B：通过线搜索实现的全局化依赖于充分下降（Armijo）条件。假设我们位于可行点 $x^{\\ast} = 1$，并考虑单位搜索方向 $d = 1$。设罚参数为 $\\rho = \\tfrac{1}{2}$，Armijo参数为 $c_{1} = 0.1$。如果线搜索使用Clarke方向导数（即在 $x^{\\ast}$ 处沿 $d$ 方向的单侧斜率）作为斜率估计，计算满足Armijo检验\n$$\n\\Phi_{\\rho}(x^{\\ast} + \\alpha d) \\le \\Phi_{\\rho}(x^{\\ast}) + c_{1} \\, \\alpha \\, s_{g},\n$$\n的最大正步长 $\\alpha_{\\max}$，其中 $s_{g}$ 表示 $\\Phi_{\\rho}$ 在 $x^{\\ast}$ 处沿 $d$ 方向的方向导数。请将最终答案表示为一个精确数，无需四舍五入。\n\n任务C：简要提出一种次梯度保障措施，一个实用的线搜索算法可以在接近可行点时采用该措施来稳健地处理扭结。你的提议应使用可由 $f$、$g$、$\\rho$ 和搜索方向 $d$ 计算出的量来表述，并且不应依赖于除方向导数和次梯度的基本定义之外的任何外部“捷径”公式。",
            "solution": "该问题被验证为自洽的，在非线性优化领域具有科学依据，并且是适定的。所有必要的函数、参数和条件都已明确给出，没有内部矛盾。我们可以开始求解。\n\n该问题要求分析一个等式约束非线性规划问题，我们要最小化 $f(x) = (x-2)^2$，约束为 $g(x) = x^2-1=0$。该优化过程使用了一个精确罚函数 $\\Phi_{\\rho}(x) = f(x) + \\rho \\, |g(x)|$，其中罚参数 $\\rho  0$。\n\n**任务A：不可微性解释**\n\n如果一个函数在某点的方向导数在所有方向上都存在，并且是方向向量的线性函数，那么该函数在该点是可微的。函数 $\\Psi(x)$ 在点 $x$ 处沿方向 $d$ 的方向导数定义为：\n$$\n\\Psi'(x; d) = \\lim_{\\alpha \\to 0^+} \\frac{\\Psi(x + \\alpha d) - \\Psi(x)}{\\alpha}\n$$\n要使 $\\Psi(x)$ 在 $x$ 处可微，必须对所有方向 $d$ 满足 $\\Psi'(x; d) = -\\Psi'(x; -d)$。\n\n设 $x_{\\text{feas}}$ 为一个可行点，即 $g(x_{\\text{feas}}) = 0$。对于给定的问题，可行点是 $x^2-1=0$ 的根，即 $x=1$ 和 $x=-1$。在任何这样的点上，罚函数的值为 $\\Phi_{\\rho}(x_{\\text{feas}}) = f(x_{\\text{feas}}) + \\rho |g(x_{\\text{feas}})| = f(x_{\\text{feas}})$。\n\n我们来计算 $\\Phi_{\\rho}(x)$ 在 $x_{\\text{feas}}$ 处沿方向 $d$ 的方向导数。\n$$\n\\Phi_{\\rho}'(x_{\\text{feas}}; d) = \\lim_{\\alpha \\to 0^+} \\frac{\\Phi_{\\rho}(x_{\\text{feas}} + \\alpha d) - \\Phi_{\\rho}(x_{\\text{feas}})}{\\alpha}\n$$\n代入 $\\Phi_{\\rho}$ 的定义并使用 $g(x_{\\text{feas}})=0$：\n$$\n\\Phi_{\\rho}'(x_{\\text{feas}}; d) = \\lim_{\\alpha \\to 0^+} \\frac{[f(x_{\\text{feas}} + \\alpha d) + \\rho |g(x_{\\text{feas}} + \\alpha d)|] - f(x_{\\text{feas}})}{\\alpha}\n$$\n由于 $f$ 和 $g$ 都是可微的，这个极限可以被拆分：\n$$\n\\Phi_{\\rho}'(x_{\\text{feas}}; d) = \\lim_{\\alpha \\to 0^+} \\frac{f(x_{\\text{feas}} + \\alpha d) - f(x_{\\text{feas}})}{\\alpha} + \\rho \\lim_{\\alpha \\to 0^+} \\frac{|g(x_{\\text{feas}} + \\alpha d)|}{\\alpha}\n$$\n第一项是 $f(x)$ 的方向导数的定义，即 $f'(x_{\\text{feas}})d$。\n对于第二项，我们使用 $g(x)$ 在 $x_{\\text{feas}}$ 周围的一阶泰勒展开：\n$g(x_{\\text{feas}} + \\alpha d) = g(x_{\\text{feas}}) + \\alpha \\, g'(x_{\\text{feas}})d + O(\\alpha^2) = \\alpha \\, g'(x_{\\text{feas}})d + O(\\alpha^2)$。\n将此代入极限中：\n$$\n\\lim_{\\alpha \\to 0^+} \\frac{|\\alpha \\, g'(x_{\\text{feas}})d + O(\\alpha^2)|}{\\alpha} = \\lim_{\\alpha \\to 0^+} \\frac{\\alpha |g'(x_{\\text{feas}})d + O(\\alpha)|}{\\alpha} = |g'(x_{\\text{feas}})d|\n$$\n合并各项，罚函数在可行点处的方向导数为：\n$$\n\\Phi_{\\rho}'(x_{\\text{feas}}; d) = f'(x_{\\text{feas}})d + \\rho |g'(x_{\\text{feas}})d|\n$$\n项 $|g'(x_{\\text{feas}})d|$ 通常不是关于 $d$ 的线性函数。要使 $\\Phi_{\\rho}$ 在 $x_{\\text{feas}}$ 处可微，我们需要满足 $\\Phi_{\\rho}'(x_{\\text{feas}}; d) = -\\Phi_{\\rho}'(x_{\\text{feas}}; -d)$。我们来检验这个条件：\n$$\n-\\Phi_{\\rho}'(x_{\\text{feas}}; -d) = -[f'(x_{\\text{feas}})(-d) + \\rho |g'(x_{\\text{feas}})(-d)|] = -[-f'(x_{\\text{feas}})d + \\rho |-g'(x_{\\text{feas}})d|] = f'(x_{\\text{feas}})d - \\rho |g'(x_{\\text{feas}})d|\n$$\n为满足可微性，我们需要 $f'(x_{\\text{feas}})d + \\rho |g'(x_{\\text{feas}})d| = f'(x_{\\text{feas}})d - \\rho |g'(x_{\\text{feas}})d|$，这可以简化为 $2\\rho |g'(x_{\\text{feas}})d| = 0$。因为 $\\rho > 0$，所以这要求 $g'(x_{\\text{feas}})d = 0$。\n\n对于我们这个具体问题，$g'(x) = 2x$。在可行点 $x=1$ 和 $x=-1$ 处，梯度分别为 $g'(1)=2$ 和 $g'(-1)=-2$，两者都非零。因此，对于任何方向 $d \\ne 0$，我们有 $g'(x_{\\text{feas}})d \\neq 0$。因此，条件 $g'(x_{\\text{feas}})d=0$ 并非对所有方向 $d$ 都成立，所以函数 $\\Phi_{\\rho}(x)$ 在任何可行点都不可微。项 $|g'(x_{\\text{feas}})d|$ 的存在在 $x_{\\text{feas}}$ 处造成了函数图像的一个“扭结”。\n\n**任务B：Armijo条件计算**\n\n给定点 $x^{\\ast} = 1$，搜索方向 $d=1$，罚参数 $\\rho = \\frac{1}{2}$，以及Armijo参数 $c_{1} = 0.1 = \\frac{1}{10}$。我们需要找到满足Armijo条件的最大正数 $\\alpha_{\\max}0$：\n$$\n\\Phi_{\\rho}(x^{\\ast} + \\alpha d) \\le \\Phi_{\\rho}(x^{\\ast}) + c_{1} \\, \\alpha \\, s_{g}\n$$\n首先，我们计算不等式中的每一项。\n\n1.  **在 $x^{\\ast}$ 处的值**：\n    $x^{\\ast}=1$ 是可行点，因为 $g(1) = 1^2 - 1 = 0$。\n    $f(x^{\\ast}) = f(1) = (1-2)^2 = 1$。\n    $\\Phi_{\\rho}(x^{\\ast}) = \\Phi_{\\frac{1}{2}}(1) = f(1) + \\frac{1}{2}|g(1)| = 1 + \\frac{1}{2}|0| = 1$。\n\n2.  **方向导数 $s_{g}$**：\n    $s_{g}$ 是 $\\Phi_{\\rho}$ 在 $x^{\\ast}=1$ 处沿 $d=1$ 方向的方向导数。使用任务A中的公式：\n    $s_{g} = \\Phi_{\\rho}'(1; 1) = f'(1)(1) + \\rho |g'(1)(1)|$。\n    我们有 $f'(x) = 2(x-2)$，所以 $f'(1) = 2(1-2) = -2$。\n    我们有 $g'(x) = 2x$，所以 $g'(1) = 2(1) = 2$。\n    $s_{g} = (-2) + \\frac{1}{2}|2| = -2 + 1 = -1$。\n    此处提及Clarke方向导数是一致的，因为对于该函数在该点，它与标准的单侧方向导数重合。\n\n3.  **在试验点处的值**：\n    试验点为 $x^{\\ast} + \\alpha d = 1 + \\alpha$。对于 $\\alpha  0$。\n    $f(1+\\alpha) = ((1+\\alpha) - 2)^2 = (\\alpha - 1)^2 = \\alpha^2 - 2\\alpha + 1$。\n    $g(1+\\alpha) = (1+\\alpha)^2 - 1 = 1 + 2\\alpha + \\alpha^2 - 1 = \\alpha^2 + 2\\alpha$。\n    因为 $\\alpha  0$，所以 $g(1+\\alpha) = \\alpha(\\alpha+2)  0$，因此 $|g(1+\\alpha)| = \\alpha^2 + 2\\alpha$。\n    $\\Phi_{\\rho}(1+\\alpha) = f(1+\\alpha) + \\rho |g(1+\\alpha)| = (\\alpha^2 - 2\\alpha + 1) + \\frac{1}{2}(\\alpha^2 + 2\\alpha) = \\frac{3}{2}\\alpha^2 - \\alpha + 1$。\n\n现在，我们将这些代入Armijo不等式：\n$$\n\\frac{3}{2}\\alpha^2 - \\alpha + 1 \\le 1 + \\left(\\frac{1}{10}\\right) \\alpha (-1)\n$$\n$$\n\\frac{3}{2}\\alpha^2 - \\alpha \\le -\\frac{1}{10}\\alpha\n$$\n将所有项移到一边：\n$$\n\\frac{3}{2}\\alpha^2 - \\alpha + \\frac{1}{10}\\alpha \\le 0\n$$\n$$\n\\frac{3}{2}\\alpha^2 - \\frac{9}{10}\\alpha \\le 0\n$$\n提出因子 $\\alpha$：\n$$\n\\alpha \\left(\\frac{3}{2}\\alpha - \\frac{9}{10}\\right) \\le 0\n$$\n因为我们寻求的是正步长 $\\alpha  0$，我们可以用 $\\alpha$ 除以不等式两边而不改变不等号方向：\n$$\n\\frac{3}{2}\\alpha - \\frac{9}{10} \\le 0\n$$\n$$\n\\frac{3}{2}\\alpha \\le \\frac{9}{10}\n$$\n$$\n\\alpha \\le \\frac{9}{10} \\cdot \\frac{2}{3} = \\frac{18}{30} = \\frac{3}{5}\n$$\nArmijo条件对区间 $(0, \\frac{3}{5}]$ 中的所有 $\\alpha$ 都成立。因此，最大正步长为 $\\alpha_{\\max} = \\frac{3}{5}$。\n\n**任务C：次梯度保障措施提议**\n\n对于非光滑罚函数，线搜索的挑战出现在当迭代点 $x_k$ 非常接近不可微点时（即 $|g(x_k)|$ 很小时）。在这样一个 $g(x_k) \\neq 0$ 的点 $x_k$ 处，罚函数 $\\Phi_{\\rho}(x)$ 是可微的，其沿方向 $d$ 的方向导数为：\n$$\ns_k = \\nabla\\Phi_{\\rho}(x_k)^Td = f'(x_k)d + \\rho \\, \\text{sign}(g(x_k)) g'(x_k)d\n$$\n如果所取的步长 $\\alpha$ 使得线搜索穿过可行集（即 $g(x_k)$ 和 $g(x_k+\\alpha d)$ 符号相反），罚函数的梯度会发生突变。斜率 $s_k$ 就不再能很好地预测函数的行为，可能导致线搜索失败或需要多次回溯。\n\n当迭代点接近可行集时，可以通过修改Armijo条件中使用的斜率来实现一个稳健的保障措施。该保障措施应使用一个能解释扭结现象的斜率模型。\n\n**提议：** 一个实用的线搜索算法可以采用以下次梯度保障措施：\n\n1.  定义一个小的正容差 $\\tau$。\n2.  在当前迭代点 $x$ 处，计算约束值 $g(x)$ 及其导数 $g'(x)$。\n3.  计算导数 $f'(x)$ 和搜索方向 $d$。\n4.  如果 $|g(x)| \\ge \\tau$，则认为迭代点远离扭结。在Armijo检验中使用标准方向导数作为斜率：\n    $$s_{\\text{slope}} = f'(x)d + \\rho \\, \\text{sign}(g(x)) g'(x)d$$\n5.  如果 $|g(x)|  \\tau$，则认为迭代点接近扭结。为防止线搜索被一个可能过于乐观的下降估计所误导，斜率被替换为扭结本身处的方向导数，该值总是大于或等于标准值：\n    $$s_{\\text{slope}} = f'(x)d + \\rho \\, |g'(x)d|$$\n然后，将这个有保障的斜率 $s_{\\text{slope}}$ 用于Armijo条件：$\\Phi_{\\rho}(x + \\alpha d) \\le \\Phi_{\\rho}(x) + c_{1} \\, \\alpha \\, s_{\\text{slope}}$。这个过程仅使用可从问题数据（$f, g, \\rho$）和当前迭代点/方向计算出的量，并通过在接近可行集时使用一个更保守的局部下降模型来稳健地处理扭结的几何特性。",
            "answer": "$$\\boxed{\\frac{3}{5}}$$"
        },
        {
            "introduction": "理论上的非光滑性问题在实践中会表现为可观察到的算法行为，例如“之字形”振荡（zig-zagging）。本编程练习  将让你通过代码实现来直观地体验这一现象，并验证光滑化技术如何有效地缓解它。通过亲手实现并比较非光滑与光滑罚函数的性能，你将获得关于全局化策略在实际应用中如何工作的宝贵经验。",
            "id": "3149286",
            "problem": "考虑一个结合了光滑目标和等式约束惩罚的价值函数，并对其进行带线搜索全局化的无约束最小化。设目标函数为 $f(\\mathbf{x}) = (x_1 - 1)^2 + \\kappa (x_2 - 1)^2$，其中 $\\mathbf{x} \\in \\mathbb{R}^2$，单个等式约束的残差为 $c(\\mathbf{x}) = x_1 + x_2 - 1$。一个通用的价值函数为 $\\Phi(\\mathbf{x}) = f(\\mathbf{x}) + \\mu \\,\\psi(c(\\mathbf{x}))$，其中 $\\mu  0$ 是一个惩罚参数，$\\psi$ 是一个从 $\\mathbb{R}$ 到 $\\mathbb{R}_{\\ge 0}$ 的惩罚映射。\n\n基本基础和定义：\n- 使用基于梯度的下降法，配合回溯线搜索和 Armijo 充分下降条件。在第 $k$ 次迭代时，下降方向为 $p_k = -\\nabla \\Phi(\\mathbf{x}_k)$，步长 $\\alpha_k$ 通过回溯选择，直到满足 Armijo 条件 $\\Phi(\\mathbf{x}_k + \\alpha_k p_k) \\le \\Phi(\\mathbf{x}_k) + \\sigma \\alpha_k \\nabla \\Phi(\\mathbf{x}_k)^\\top p_k$，其中 $\\sigma \\in (0,1)$ 是一个固定值。\n- 回溯过程以一个常数因子 $\\beta \\in (0,1)$ 减小步长，直到满足 Armijo 条件。\n- 全局化指的是使用价值函数和线搜索来确保从任意起始点都能收敛。\n\n非光滑与光滑化惩罚：\n- 非光滑惩罚：$\\psi_{\\text{ns}}(t) = |t|$，它在 $t = 0$ 处不可微，与基于梯度的方法结合时可能导致“之”字形（zig-zagging）现象。\n- 为减轻“之”字形现象的光滑化惩罚：\n  1. 光滑绝对值：$\\psi_{\\sqrt{}}(t) = \\sqrt{t^2 + \\varepsilon^2}$，其中 $\\varepsilon  0$，是 $|t|$ 的一个可微近似。\n  2. Huber 型光滑化：$\\psi_{\\text{Huber}}(t) = \\begin{cases} \\dfrac{t^2}{2\\varepsilon},  |t| \\le \\varepsilon \\\\ |t| - \\dfrac{\\varepsilon}{2},  |t|  \\varepsilon \\end{cases}$，其中 $\\varepsilon  0$，它是连续可微的，并在原点的一个邻域之外与绝对值函数一致。\n\n你的任务：\n- 使用 Armijo 条件实现带回溯线搜索的梯度下降法，分别应用于非光滑价值函数 $\\Phi_{\\text{ns}}(\\mathbf{x}) = f(\\mathbf{x}) + \\mu |c(\\mathbf{x})|$ 和光滑化价值函数 $\\Phi_{\\text{sm}}(\\mathbf{x}) = f(\\mathbf{x}) + \\mu \\psi_{\\text{sm}}(c(\\mathbf{x}))$，其中 $\\psi_{\\text{sm}}$ 根据每个测试用例的指定为 $\\psi_{\\sqrt{}}$ 或 $\\psi_{\\text{Huber}}$。\n- 使用以下固定的线搜索参数：Armijo 参数 $\\sigma = 10^{-4}$，回溯缩减因子 $\\beta = \\tfrac{1}{2}$，每次迭代的最大回溯次数 $M_{\\text{bt}} = 50$，梯度范数的终止容差 $\\tau = 10^{-6}$，以及最大迭代次数 $N_{\\max} = 200$。\n- 对于非光滑情况，在 $c(\\mathbf{x}) = 0$ 时，为确定起见，选择次梯度 $\\nabla |c| = \\operatorname{sign}(c) \\nabla c$ 并定义 $\\operatorname{sign}(0) = 0$。\n\n“之”字形现象与全局化指标：\n- 定义“之”字形计数为迭代次数 $k \\ge 1$ 中，连续下降方向 $p_{k-1}$ 和 $p_k$ 之间的夹角（以弧度计）超过 $\\tfrac{\\pi}{2}$ 的次数。角度必须以弧度计算和比较。\n- 定义约束符号翻转计数为迭代次数 $k \\ge 1$ 中，满足 $\\operatorname{sign}(c(\\mathbf{x}_k)) \\neq \\operatorname{sign}(c(\\mathbf{x}_{k-1}))$ 的次数。\n- 定义回溯计数为在整个回溯线搜索过程中缩减步长的总次数（即，导致 $\\alpha \\leftarrow \\beta \\alpha$ 的不成功 Armijo 测试的总次数）。\n- 记录终止时的最终价值函数值 $\\Phi(\\mathbf{x}_{\\text{final}})$。\n\n测试套件：\n对每个测试用例，运行算法两次：一次使用非光滑价值函数，另一次使用指定的光滑化价值函数。必须精确使用所述参数。\n\n- 测试用例 1：$\\mu = 50.0$，$\\kappa = 100.0$，$\\mathbf{x}_0 = (2.0, -1.0)$，$\\varepsilon = 10^{-3}$，光滑化方法 $\\psi_{\\sqrt{}}$。\n- 测试用例 2：$\\mu = 200.0$，$\\kappa = 10.0$，$\\mathbf{x}_0 = (0.8, 0.8)$，$\\varepsilon = 10^{-3}$，光滑化方法 $\\psi_{\\sqrt{}}$。\n- 测试用例 3：$\\mu = 100.0$，$\\kappa = 1000.0$，$\\mathbf{x}_0 = (-1.0, 2.0)$，$\\varepsilon = 10^{-4}$，光滑化方法 $\\psi_{\\text{Huber}}$。\n- 测试用例 4：$\\mu = 20.0$，$\\kappa = 1.0$，$\\mathbf{x}_0 = (0.6, 0.4)$，$\\varepsilon = 10^{-3}$，光滑化方法 $\\psi_{\\sqrt{}}$。\n\n输出规格：\n- 对于每个测试用例，按顺序 $[\\text{zigzag}_{\\text{ns}}, \\text{zigzag}_{\\text{sm}}, \\text{signflip}_{\\text{ns}}, \\text{signflip}_{\\text{sm}}, \\text{backtracks}_{\\text{ns}}, \\text{backtracks}_{\\text{sm}}, \\Phi_{\\text{ns, final}}, \\Phi_{\\text{sm, final}}]$ 输出一个包含八个值的列表。所有角度、比较和计数都必须使用弧度。每个值必须是布尔值、整数或浮点数。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素本身也是一个包含上述八个值的列表（例如，$[[\\dots],[\\dots],[\\dots],[\\dots]]$）。",
            "solution": "该问题要求实现一个带有回溯线搜索的梯度下降算法来最小化一个价值函数。该价值函数由一个二次目标函数和一个等式约束的惩罚项组成。任务的核心是比较算法在使用非光滑绝对值惩罚与两种不同的光滑近似惩罚时的行为。性能通过旨在量化收敛困难（如“之”字形现象）的指标进行评估。\n\n首先，我们定义问题的各个组成部分。\n目标函数为 $f(\\mathbf{x}): \\mathbb{R}^2 \\to \\mathbb{R}$，由下式给出：\n$$ f(\\mathbf{x}) = (x_1 - 1)^2 + \\kappa (x_2 - 1)^2 $$\n其梯度 $\\nabla f(\\mathbf{x})$ 为：\n$$ \\nabla f(\\mathbf{x}) = \\begin{pmatrix} 2(x_1 - 1) \\\\ 2\\kappa(x_2 - 1) \\end{pmatrix} $$\n等式约束通过残差函数 $c(\\mathbf{x}): \\mathbb{R}^2 \\to \\mathbb{R}$ 表示：\n$$ c(\\mathbf{x}) = x_1 + x_2 - 1 $$\n约束残差的梯度 $\\nabla c(\\mathbf{x})$ 是一个常数向量：\n$$ \\nabla c(\\mathbf{x}) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n\n价值函数 $\\Phi(\\mathbf{x})$ 结合了目标函数和约束惩罚：\n$$ \\Phi(\\mathbf{x}) = f(\\mathbf{x}) + \\mu \\psi(c(\\mathbf{x})) $$\n其中 $\\mu  0$ 是惩罚参数，$\\psi$ 是一个惩罚映射。我们分析 $\\psi$ 的三种形式。\n\n1. **非光滑价值函数 ($\\Phi_{\\text{ns}}$)**\n   惩罚函数是绝对值函数 $\\psi_{\\text{ns}}(t) = |t|$。\n   价值函数为：\n   $$ \\Phi_{\\text{ns}}(\\mathbf{x}) = (x_1 - 1)^2 + \\kappa (x_2 - 1)^2 + \\mu |x_1 + x_2 - 1| $$\n   该函数在 $c(\\mathbf{x}) = 0$ 处不可微。我们使用其次梯度，该次梯度通过次微分的链式法则定义。$|t|$ 的次导数在 $t \\neq 0$ 时为 $\\operatorname{sign}(t)$，在 $t=0$ 时为区间 $[-1, 1]$。问题指定使用 $\\operatorname{sign}(0) = 0$。\n   次梯度为：\n   $$ \\nabla \\Phi_{\\text{ns}}(\\mathbf{x}) = \\nabla f(\\mathbf{x}) + \\mu \\operatorname{sign}(c(\\mathbf{x})) \\nabla c(\\mathbf{x}) = \\begin{pmatrix} 2(x_1 - 1) \\\\ 2\\kappa(x_2 - 1) \\end{pmatrix} + \\mu \\operatorname{sign}(x_1 + x_2 - 1) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n\n2. **光滑绝对值价值函数 ($\\Phi_{\\sqrt{}}$)**\n   惩罚函数是绝对值的光滑近似 $\\psi_{\\sqrt{}}(t) = \\sqrt{t^2 + \\varepsilon^2}$，其中 $\\varepsilon  0$。\n   价值函数为：\n   $$ \\Phi_{\\sqrt{}}(\\mathbf{x}) = f(\\mathbf{x}) + \\mu \\sqrt{c(\\mathbf{x})^2 + \\varepsilon^2} $$\n   该函数处处可微。其梯度通过链式法则求得。$\\psi_{\\sqrt{}}(t)$ 的导数是 $\\psi'_{\\sqrt{}}(t) = \\frac{t}{\\sqrt{t^2 + \\varepsilon^2}}$。\n   梯度为：\n   $$ \\nabla \\Phi_{\\sqrt{}}(\\mathbf{x}) = \\nabla f(\\mathbf{x}) + \\mu \\frac{c(\\mathbf{x})}{\\sqrt{c(\\mathbf{x})^2 + \\varepsilon^2}} \\nabla c(\\mathbf{x}) = \\begin{pmatrix} 2(x_1 - 1) \\\\ 2\\kappa(x_2 - 1) \\end{pmatrix} + \\mu \\frac{x_1 + x_2 - 1}{\\sqrt{(x_1 + x_2 - 1)^2 + \\varepsilon^2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n\n3. **Huber 型光滑价值函数 ($\\Phi_{\\text{Huber}}$)**\n   惩罚函数是 Huber 损失函数 $\\psi_{\\text{Huber}}(t)$，它是连续可微的。\n   $$ \\psi_{\\text{Huber}}(t) = \\begin{cases} \\frac{t^2}{2\\varepsilon},  |t| \\le \\varepsilon \\\\ |t| - \\frac{\\varepsilon}{2},  |t|  \\varepsilon \\end{cases} $$\n   价值函数为 $\\Phi_{\\text{Huber}}(\\mathbf{x}) = f(\\mathbf{x}) + \\mu \\psi_{\\text{Huber}}(c(\\mathbf{x}))$。$\\psi_{\\text{Huber}}(t)$ 的导数为：\n   $$ \\psi'_{\\text{Huber}}(t) = \\begin{cases} \\frac{t}{\\varepsilon},  |t| \\le \\varepsilon \\\\ \\operatorname{sign}(t),  |t|  \\varepsilon \\end{cases} $$\n   价值函数的梯度为：\n   $$ \\nabla \\Phi_{\\text{Huber}}(\\mathbf{x}) = \\nabla f(\\mathbf{x}) + \\mu \\psi'_{\\text{Huber}}(c(\\mathbf{x})) \\nabla c(\\mathbf{x}) = \\begin{pmatrix} 2(x_1 - 1) \\\\ 2\\kappa(x_2 - 1) \\end{pmatrix} + \\mu \\psi'_{\\text{Huber}}(x_1+x_2-1) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n\n优化算法是梯度下降法。从初始点 $\\mathbf{x}_0$ 开始，在每次迭代 $k$ 中，下一个点 $\\mathbf{x}_{k+1}$ 通过以下方式找到：\n$$ \\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k p_k $$\n其中 $p_k = -\\nabla \\Phi(\\mathbf{x}_k)$ 是下降方向（对于 $\\Phi_{\\text{ns}}$ 则是基于次梯度的方向）。步长 $\\alpha_k$ 由回溯线搜索确定，该搜索从 $\\alpha=1$ 开始，并以因子 $\\beta \\in (0,1)$ 连续减小步长，直到满足 Armijo 充分下降条件：\n$$ \\Phi(\\mathbf{x}_k + \\alpha p_k) \\le \\Phi(\\mathbf{x}_k) + \\sigma \\alpha \\nabla \\Phi(\\mathbf{x}_k)^\\top p_k $$\n对于一个固定的 $\\sigma \\in (0,1)$。当梯度（或次梯度）的范数低于容差 $\\tau$，即 $\\|\\nabla \\Phi(\\mathbf{x}_k)\\|_2 \\le \\tau$，或达到最大迭代次数 $N_{\\max}$ 时，过程终止。\n\n计算以下指标以评估算法的性能：\n- **“之”字形计数**：迭代次数 $k \\ge 1$ 中，连续下降方向 $p_{k-1}$ 和 $p_k$ 之间的夹角大于 $\\frac{\\pi}{2}$ 弧度的次数。计算公式为 $k \\ge 1$ 中满足 $\\arccos\\left(\\frac{p_k^\\top p_{k-1}}{\\|p_k\\|_2 \\|p_{k-1}\\|_2}\\right)  \\frac{\\pi}{2}$ 的计数。\n- **约束符号翻转计数**：迭代次数 $k \\ge 1$ 中，约束残差的符号发生变化的次数，即 $\\operatorname{sign}(c(\\mathbf{x}_k)) \\neq \\operatorname{sign}(c(\\mathbf{x}_{k-1}))$。\n- **回溯计数**：在整个优化运行期间，步长 $\\alpha$ 被减小的总次数。\n- **最终价值函数值**：在终止点的价值函数值 $\\Phi(\\mathbf{x}_{\\text{final}})$。\n\n实现将包含一个执行梯度下降的主循环，应用指定的价值函数及其梯度。终止后，使用迭代点和下降方向的历史记录来计算所需的指标。对每个测试用例重复此过程，一次使用非光滑惩罚，另一次使用指定的光滑化惩罚。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Global parameters defined in the problem statement\n    SIGMA = 1e-4\n    BETA = 0.5\n    M_BT = 50\n    TAU = 1e-6\n    N_MAX = 200\n\n    # Objective function and its gradient\n    def f(x, kappa):\n        return (x[0] - 1.0)**2 + kappa * (x[1] - 1.0)**2\n\n    def grad_f(x, kappa):\n        return np.array([2.0 * (x[0] - 1.0), 2.0 * kappa * (x[1] - 1.0)])\n\n    # Constraint residual and its gradient\n    def c(x):\n        return x[0] + x[1] - 1.0\n\n    def grad_c():\n        return np.array([1.0, 1.0])\n\n    # --- Merit Function Classes ---\n    class MeritFunction:\n        def __init__(self, kappa, mu):\n            self.kappa = kappa\n            self.mu = mu\n\n        def phi(self, x):\n            raise NotImplementedError\n\n        def grad_phi(self, x):\n            raise NotImplementedError\n\n    class NonsmoothMerit(MeritFunction):\n        def phi(self, x):\n            return f(x, self.kappa) + self.mu * np.abs(c(x))\n\n        def grad_phi(self, x):\n            c_val = c(x)\n            return grad_f(x, self.kappa) + self.mu * np.sign(c_val) * grad_c()\n\n    class SqrtMerit(MeritFunction):\n        def __init__(self, kappa, mu, epsilon):\n            super().__init__(kappa, mu)\n            self.epsilon = epsilon\n            self.epsilon_sq = epsilon**2\n\n        def phi(self, x):\n            c_val = c(x)\n            return f(x, self.kappa) + self.mu * np.sqrt(c_val**2 + self.epsilon_sq)\n\n        def grad_phi(self, x):\n            c_val = c(x)\n            denominator = np.sqrt(c_val**2 + self.epsilon_sq)\n            penalty_grad_factor = self.mu * c_val / denominator if denominator > 0 else 0\n            return grad_f(x, self.kappa) + penalty_grad_factor * grad_c()\n\n    class HuberMerit(MeritFunction):\n        def __init__(self, kappa, mu, epsilon):\n            super().__init__(kappa, mu)\n            self.epsilon = epsilon\n\n        def phi(self, x):\n            c_val = c(x)\n            if np.abs(c_val) = self.epsilon:\n                penalty = c_val**2 / (2.0 * self.epsilon)\n            else:\n                penalty = np.abs(c_val) - self.epsilon / 2.0\n            return f(x, self.kappa) + self.mu * penalty\n\n        def grad_phi(self, x):\n            c_val = c(x)\n            if np.abs(c_val) = self.epsilon:\n                penalty_deriv = c_val / self.epsilon\n            else:\n                penalty_deriv = np.sign(c_val)\n            return grad_f(x, self.kappa) + self.mu * penalty_deriv * grad_c()\n\n    # --- Optimizer ---\n    def run_optimizer(x0, merit_function):\n        x = np.array(x0, dtype=float)\n        \n        x_history = [x]\n        p_history = []\n        total_backtracks = 0\n        \n        for _ in range(N_MAX):\n            grad = merit_function.grad_phi(x)\n            \n            grad_norm = np.linalg.norm(grad)\n            if grad_norm = TAU:\n                break\n                \n            p = -grad\n            p_history.append(p)\n            \n            # Backtracking line search\n            phi_k = merit_function.phi(x)\n            grad_p_dot = np.dot(grad, p)\n            \n            alpha = 1.0\n            num_bt = 0\n            while num_bt  M_BT:\n                x_next = x + alpha * p\n                if merit_function.phi(x_next) = phi_k + SIGMA * alpha * grad_p_dot:\n                    break\n                alpha *= BETA\n                total_backtracks += 1\n                num_bt += 1\n            \n            x = x + alpha * p\n            x_history.append(x)\n            \n        x_final = x_history[-1]\n        phi_final = merit_function.phi(x_final)\n\n        # Zig-zag count\n        zigzag_count = 0\n        if len(p_history) > 1:\n            for i in range(1, len(p_history)):\n                p_prev, p_curr = p_history[i-1], p_history[i]\n                norm_prev, norm_curr = np.linalg.norm(p_prev), np.linalg.norm(p_curr)\n                if norm_prev > 1e-12 and norm_curr > 1e-12:\n                    cos_theta = np.dot(p_curr, p_prev) / (norm_curr * norm_prev)\n                    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n                    angle = np.arccos(cos_theta)\n                    if angle > np.pi / 2.0:\n                        zigzag_count += 1\n        \n        # Sign-flip count\n        signflip_count = 0\n        if len(x_history) > 1:\n            for i in range(1, len(x_history)):\n                c_prev = c(x_history[i-1])\n                c_curr = c(x_history[i])\n                if np.sign(c_curr) != np.sign(c_prev):\n                    signflip_count += 1\n                    \n        return zigzag_count, signflip_count, total_backtracks, phi_final\n\n    # --- Test Suite ---\n    test_cases = [\n        {'mu': 50.0, 'kappa': 100.0, 'x0': (2.0, -1.0), 'epsilon': 1e-3, 'smoothing': 'sqrt'},\n        {'mu': 200.0, 'kappa': 10.0, 'x0': (0.8, 0.8), 'epsilon': 1e-3, 'smoothing': 'sqrt'},\n        {'mu': 100.0, 'kappa': 1000.0, 'x0': (-1.0, 2.0), 'epsilon': 1e-4, 'smoothing': 'huber'},\n        {'mu': 20.0, 'kappa': 1.0, 'x0': (0.6, 0.4), 'epsilon': 1e-3, 'smoothing': 'sqrt'},\n    ]\n    \n    all_results = []\n    \n    for case in test_cases:\n        mu, kappa, x0, epsilon = case['mu'], case['kappa'], case['x0'], case['epsilon']\n        \n        # Run nonsmooth case\n        ns_merit = NonsmoothMerit(kappa, mu)\n        ns_metrics = run_optimizer(x0, ns_merit)\n        \n        # Run smoothed case\n        if case['smoothing'] == 'sqrt':\n            sm_merit = SqrtMerit(kappa, mu, epsilon)\n        else: # huber\n            sm_merit = HuberMerit(kappa, mu, epsilon)\n        \n        sm_metrics = run_optimizer(x0, sm_merit)\n        \n        case_results = [\n            ns_metrics[0], sm_metrics[0],  # zigzag\n            ns_metrics[1], sm_metrics[1],  # signflip\n            ns_metrics[2], sm_metrics[2],  # backtracks\n            ns_metrics[3], sm_metrics[3]   # phi_final\n        ]\n        all_results.append(case_results)\n        \n    formatted_results = []\n    for res_list in all_results:\n        formatted_list = f\"[{','.join(map(str, res_list))}]\"\n        formatted_results.append(formatted_list)\n    \n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}