{
    "hands_on_practices": [
        {
            "introduction": "二次罚函数法将有约束问题转化为一系列无约束问题。为了从根本上理解这种转化，我们可以从一个典型的线性约束二次规划问题入手。通过推导其惩罚目标的最小化点的精确解析轨迹，你将清晰地看到解是如何随着罚参数 $\\rho$ 的变化而移动的，从而建立对该方法核心机制的直观认识。",
            "id": "495741",
            "problem": "考虑一般的线性约束二次规划（LCQP）问题：\n$$\n\\begin{aligned}\n\\min \\quad  f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T Q \\mathbf{x} + \\mathbf{c}^T \\mathbf{x} \\\\\n\\text{s.t.} \\quad  A\\mathbf{x} = \\mathbf{b}\n\\end{aligned}\n$$\n其中，$\\mathbf{x} \\in \\mathbb{R}^n$ 是优化变量。问题数据包括一个对称正定矩阵 $Q \\in \\mathbb{R}^{n \\times n}$，一个向量 $\\mathbf{c} \\in \\mathbb{R}^n$，一个满行秩的矩阵 $A \\in \\mathbb{R}^{m \\times n}$（$m < n$），以及一个向量 $\\mathbf{b} \\in \\mathbb{R}^m$。\n\n为了解决这个约束问题，我们采用二次惩罚法。该方法通过向目标函数添加一个惩罚项，将问题转化为一系列无约束优化问题。由此产生的无约束目标函数，称为增广目标函数，由下式给出：\n$$\nL_\\rho(\\mathbf{x}) = f(\\mathbf{x}) + \\frac{\\rho}{2} \\|A\\mathbf{x} - \\mathbf{b}\\|_2^2\n$$\n其中 $\\rho > 0$ 是一个标量惩罚参数。对于任何给定的 $\\rho > 0$，函数 $L_\\rho(\\mathbf{x})$ 都有一个唯一的最小化子，我们记为 $\\mathbf{x}^*(\\rho)$。当 $\\rho$ 变化时，这个最小化子所描绘的路径被称为最小化子的轨迹。\n\n请根据惩罚参数 $\\rho$ 以及问题数据矩阵和向量 $(Q, \\mathbf{c}, A, \\mathbf{b})$，推导该轨迹 $\\mathbf{x}^*(\\rho)$ 的显式解析表达式。",
            "solution": "二次惩罚法旨在为一系列递增的惩罚参数 $\\rho$ 找到增广目标函数 $L_\\rho(\\mathbf{x})$ 的最小值。对于给定的 $\\rho$，唯一的最小化子记为 $\\mathbf{x}^*(\\rho)$。\n\n增广目标函数为：\n$$\nL_\\rho(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T Q \\mathbf{x} + \\mathbf{c}^T \\mathbf{x} + \\frac{\\rho}{2} \\|A\\mathbf{x} - \\mathbf{b}\\|_2^2\n$$\n我们可以展开惩罚项，即欧几里得范数的平方：\n$$\n\\|A\\mathbf{x} - \\mathbf{b}\\|_2^2 = (A\\mathbf{x} - \\mathbf{b})^T(A\\mathbf{x} - \\mathbf{b}) = (\\mathbf{x}^T A^T - \\mathbf{b}^T)(A\\mathbf{x} - \\mathbf{b}) = \\mathbf{x}^T A^T A \\mathbf{x} - \\mathbf{x}^T A^T \\mathbf{b} - \\mathbf{b}^T A \\mathbf{x} + \\mathbf{b}^T \\mathbf{b}\n$$\n由于 $\\mathbf{b}^T A \\mathbf{x}$ 是一个标量，它等于其转置 $(\\mathbf{b}^T A \\mathbf{x})^T = \\mathbf{x}^T A^T \\mathbf{b}$。因此，惩罚项变为：\n$$\n\\|A\\mathbf{x} - \\mathbf{b}\\|_2^2 = \\mathbf{x}^T A^T A \\mathbf{x} - 2\\mathbf{b}^T A \\mathbf{x} + \\mathbf{b}^T \\mathbf{b}\n$$\n将其代回 $L_\\rho(\\mathbf{x})$ 得：\n$$\nL_\\rho(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T Q \\mathbf{x} + \\mathbf{c}^T \\mathbf{x} + \\frac{\\rho}{2}(\\mathbf{x}^T A^T A \\mathbf{x} - 2\\mathbf{b}^T A \\mathbf{x} + \\mathbf{b}^T \\mathbf{b})\n$$\n为了找到最小化子 $\\mathbf{x}^*(\\rho)$，我们必须找到 $L_\\rho(\\mathbf{x})$ 关于 $\\mathbf{x}$ 的梯度为零的点。我们计算梯度 $\\nabla_{\\mathbf{x}} L_\\rho(\\mathbf{x})$：\n$$\n\\nabla_{\\mathbf{x}} L_\\rho(\\mathbf{x}) = \\nabla_{\\mathbf{x}}\\left(\\frac{1}{2}\\mathbf{x}^T Q \\mathbf{x}\\right) + \\nabla_{\\mathbf{x}}(\\mathbf{c}^T \\mathbf{x}) + \\frac{\\rho}{2} \\nabla_{\\mathbf{x}}(\\mathbf{x}^T A^T A \\mathbf{x} - 2\\mathbf{b}^T A \\mathbf{x} + \\mathbf{b}^T \\mathbf{b})\n$$\n使用矩阵微积分的标准法则（对于对称的 $Q$ 和 $A^T A$）：\n- 对于对称矩阵 $M$，$\\nabla_{\\mathbf{x}}(\\frac{1}{2}\\mathbf{x}^T M \\mathbf{x}) = M\\mathbf{x}$。\n- $\\nabla_{\\mathbf{x}}(\\mathbf{v}^T \\mathbf{x}) = \\mathbf{v}$。\n\n梯度为：\n$$\n\\nabla_{\\mathbf{x}} L_\\rho(\\mathbf{x}) = Q\\mathbf{x} + \\mathbf{c} + \\frac{\\rho}{2}(2A^T A \\mathbf{x} - 2A^T\\mathbf{b})\n$$\n$$\n\\nabla_{\\mathbf{x}} L_\\rho(\\mathbf{x}) = Q\\mathbf{x} + \\mathbf{c} + \\rho(A^T A \\mathbf{x} - A^T\\mathbf{b})\n$$\n将梯度设为零向量以找到驻点 $\\mathbf{x}^*(\\rho)$：\n$$\nQ\\mathbf{x}^*(\\rho) + \\mathbf{c} + \\rho A^T A \\mathbf{x}^*(\\rho) - \\rho A^T\\mathbf{b} = \\mathbf{0}\n$$\n现在，我们求解 $\\mathbf{x}^*(\\rho)$。将包含 $\\mathbf{x}^*(\\rho)$ 的项组合在一起：\n$$\n(Q + \\rho A^T A)\\mathbf{x}^*(\\rho) = \\rho A^T\\mathbf{b} - \\mathbf{c}\n$$\n为了确认这个驻点是一个最小值点，我们考察 $L_\\rho(\\mathbf{x})$ 的海森矩阵（Hessian matrix），即 $\\nabla^2_{\\mathbf{x}} L_\\rho(\\mathbf{x})$：\n$$\n\\nabla^2_{\\mathbf{x}} L_\\rho(\\mathbf{x}) = Q + \\rho A^T A\n$$\n问题陈述 $Q$ 是对称正定矩阵（$Q \\succ 0$）。对于任何矩阵 $A$，矩阵 $A^T A$ 总是半正定的。由于 $\\rho > 0$，海森矩阵 $Q + \\rho A^T A$ 是一个正定矩阵与一个半正定矩阵之和，结果仍然是一个正定矩阵。因此，$L_\\rho(\\mathbf{x})$ 是一个严格凸函数，其驻点是唯一的全局最小值点。\n\n为了找到 $\\mathbf{x}^*(\\rho)$ 的表达式，我们可以对矩阵 $(Q + \\rho A^T A)$求逆：\n$$\n\\mathbf{x}^*(\\rho) = (Q + \\rho A^T A)^{-1} (\\rho A^T\\mathbf{b} - \\mathbf{c})\n$$\n这就是最小化子轨迹作为 $\\rho$ 函数的显式表达式。",
            "answer": "$$\n\\boxed{(Q + \\rho A^T A)^{-1} (\\rho A^T\\mathbf{b} - \\mathbf{c})}\n$$"
        },
        {
            "introduction": "理论上的简洁并不意味着实践中的一帆风顺，二次罚函数法的一个核心挑战在于其数值不稳定性。当罚参数 $\\rho$ 变得非常大时，求解无约束子问题的难度会急剧增加。这个编码实践将通过模拟牛顿法的求解过程，让你亲眼见证罚函数的海森矩阵条件数是如何随着 $\\rho$ 的增大而恶化的，并理解这为什么是该方法在实际应用中必须谨慎处理的关键问题。",
            "id": "3169152",
            "problem": "考虑在有限维欧几里得空间中，通过二次惩罚方法对等式约束问题进行无约束重构。设 $\\mathbf{x} \\in \\mathbb{R}^n$，目标函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 和约束映射 $c:\\mathbb{R}^n \\to \\mathbb{R}^m$ 均为二次连续可微函数。定义二次惩罚目标函数 $\\Phi(\\mathbf{x};\\rho) = f(\\mathbf{x}) + \\tfrac{\\rho}{2}\\lVert c(\\mathbf{x})\\rVert_2^2$，其中惩罚参数 $\\rho > 0$。将牛顿法应用于 $\\Phi(\\mathbf{x};\\rho)$，通过求解由二阶泰勒展开式导出的线性系统，在基点 $\\mathbf{x}$ 处寻求一个步长 $\\mathbf{p}$。Hessian 矩阵的条件数影响数值稳定性，而步长影响每次迭代的进展。\n\n从梯度 $\\nabla$、Hessian 矩阵 $\\nabla^2$、链式法则以及通过求解包含 $\\Phi(\\mathbf{x};\\rho)$ 的 Hessian 矩阵和梯度的系统所确定的牛顿步长的基本定义出发，推导计算以下各项所需的表达式：\n1. 对于给定的 $\\rho$，在固定基点 $\\mathbf{x}$ 处的牛顿步长 $\\mathbf{p}$。\n2. 该步长的欧几里得范数 $\\lVert \\mathbf{p} \\rVert_2$。\n3. $\\Phi(\\mathbf{x};\\rho)$ 的 Hessian 矩阵的条件数，其定义为最大奇异值与最小奇异值之比，使用奇异值分解 (SVD) 计算。如果最小奇异值为零，则条件数定义为 $+\\infty$。\n\n然后，实现一个仿真，研究对于几个测试案例，逐步增加惩罚参数如何影响 Hessian 矩阵的条件数和牛顿步长的大小。使用更新规则 $\\rho_{k+1} = 10 \\rho_k$，初始值 $\\rho_0 = 10^{-2}$，并对 $K=6$ 个值 $\\{\\rho_0,\\rho_1,\\dots,\\rho_5\\}$ 进行仿真。\n\n仿真协议：\n- 对于每个测试案例，固定一个基点 $\\mathbf{x}_0$，并为每个 $\\rho_k$ 计算在 $\\mathbf{x}_0$ 处的牛顿步长 $\\mathbf{p}_k$，在不同的 $\\rho_k$ 值之间不更新 $\\mathbf{x}_0$。这可以分离出 $\\rho$ 对 Hessian 条件数和步长的影响。\n- 对于每个 $\\rho_k$，通过 SVD 计算 $\\nabla^2 \\Phi(\\mathbf{x}_0;\\rho_k)$ 的 Hessian 条件数，并通过求解牛顿系统获得欧几里得步长范数 $\\lVert \\mathbf{p}_k \\rVert_2$。如果 Hessian 矩阵是奇异或病态的，则使用伪逆来计算最小范数步长。\n\n测试套件 (均在 $\\mathbb{R}^2$ 中)：\n- 案例 A (对称正定，线性约束)：$f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^\\top Q \\mathbf{x}$，其中 $Q = \\begin{bmatrix}1  0 \\\\ 0  2\\end{bmatrix}$；$c(\\mathbf{x}) = [x_1 + x_2 - 1]$；基点 $\\mathbf{x}_0 = [0,0]^\\top$。\n- 案例 B (可能导致不确定性的非线性约束)：$f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^\\top Q \\mathbf{x}$，其中 $Q = \\begin{bmatrix}2  0 \\\\ 0  1\\end{bmatrix}$；$c(\\mathbf{x}) = [x_1^2 + x_2 - 1]$；基点 $\\mathbf{x}_0 = [0.5,-0.2]^\\top$。\n- 案例 C (病态目标函数，线性约束改善条件数)：$f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^\\top Q \\mathbf{x}$，其中 $Q = \\begin{bmatrix}10^{-3}  0 \\\\ 0  1\\end{bmatrix}$；$c(\\mathbf{x}) = [x_1 - 0.5]$；基点 $\\mathbf{x}_0 = [1.0,1.0]^\\top$。\n\n使用的科学基础：\n- 对于二次可微目标函数的牛顿法定义：步长 $\\mathbf{p}$ 求解 $\\nabla^2 \\Phi(\\mathbf{x};\\rho)\\,\\mathbf{p} = -\\nabla \\Phi(\\mathbf{x};\\rho)$。\n- 复合函数与和函数的梯度和 Hessian 矩阵的链式法则。\n- $c(\\mathbf{x})$ 的雅可比矩阵 $J(\\mathbf{x})$ 和分量 Hessian 矩阵 $\\nabla^2 c_i(\\mathbf{x})$ 的定义。\n- 用于计算可能为不定的对称矩阵的条件数的奇异值分解。\n\n不涉及角度单位。不涉及物理单位。\n\n您的程序必须为每个测试案例生成两个包含 $K$ 个浮点数的列表：\n- 对应于 $\\rho_k$，$k=0,\\dots,5$ 的 Hessian 条件数列表。\n- 对应于 $\\rho_k$，$k=0,\\dots,5$ 的牛顿步长欧几里得范数列表。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含三个测试案例摘要的列表，每个摘要是如上所述的两个列表的列表。具体来说，输出必须采用以下格式\n$$[\\,[\\,[\\text{cond}_{A,0},\\dots,\\text{cond}_{A,5}],\\,[\\lVert \\mathbf{p}_{A,0}\\rVert_2,\\dots,\\lVert \\mathbf{p}_{A,5}\\rVert_2]\\,],\\,[\\,[\\text{cond}_{B,0},\\dots,\\text{cond}_{B,5}],\\,[\\lVert \\mathbf{p}_{B,0}\\rVert_2,\\dots,\\lVert \\mathbf{p}_{B,5}\\rVert_2]\\,],\\,[\\,[\\text{cond}_{C,0},\\dots,\\text{cond}_{C,5}],\\,[\\lVert \\mathbf{p}_{C,0}\\rVert_2,\\dots,\\lVert \\mathbf{p}_{C,5}\\rVert_2]\\,]\\,]$$\n\n输出值必须是浮点数。不得打印任何额外文本。",
            "solution": "该问题要求推导用于分析二次惩罚法的关键量，并对其行为进行数值仿真。该问题是自洽的，其科学基础根植于数值优化的原理，并且是适定的。因此，我们可以着手求解。\n\n首先，我们推导二次惩罚目标函数的梯度和 Hessian 矩阵、牛顿步长以及 Hessian 条件数的必要表达式。\n\n二次惩罚目标函数定义为：\n$$\n\\Phi(\\mathbf{x};\\rho) = f(\\mathbf{x}) + \\frac{\\rho}{2}\\lVert c(\\mathbf{x})\\rVert_2^2\n$$\n其中 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 是目标函数，$c:\\mathbb{R}^n \\to \\mathbb{R}^m$ 是约束映射，$\\rho > 0$ 是惩罚参数。约束向量的范数平方可以写为其分量 $c_i(\\mathbf{x})$ 的平方和：\n$$\n\\lVert c(\\mathbf{x})\\rVert_2^2 = c(\\mathbf{x})^\\top c(\\mathbf{x}) = \\sum_{i=1}^{m} c_i(\\mathbf{x})^2\n$$\n\n**1. 惩罚目标的梯度 $\\nabla \\Phi(\\mathbf{x};\\rho)$**\n\n为了求出 $\\Phi(\\mathbf{x};\\rho)$ 相对于 $\\mathbf{x}$ 的梯度，我们逐项对其进行微分。$f(\\mathbf{x})$ 的梯度就是 $\\nabla f(\\mathbf{x})$。对于惩罚项，我们应用链式法则。设 $g(\\mathbf{x}) = \\frac{\\rho}{2} \\sum_{i=1}^{m} c_i(\\mathbf{x})^2$。其梯度的第 $j$ 个分量是：\n$$\n\\frac{\\partial g}{\\partial x_j} = \\frac{\\rho}{2} \\sum_{i=1}^{m} 2 c_i(\\mathbf{x}) \\frac{\\partial c_i(\\mathbf{x})}{\\partial x_j} = \\rho \\sum_{i=1}^{m} c_i(\\mathbf{x}) \\frac{\\partial c_i(\\mathbf{x})}{\\partial x_j}\n$$\n该表达式可以写成向量形式。约束映射 $c(\\mathbf{x})$ 的雅可比矩阵是一个 $m \\times n$ 矩阵 $J(\\mathbf{x})$，其元素为 $(J(\\mathbf{x}))_{ij} = \\frac{\\partial c_i(\\mathbf{x})}{\\partial x_j}$。表达式 $\\sum_{i=1}^{m} c_i(\\mathbf{x}) \\frac{\\partial c_i(\\mathbf{x})}{\\partial x_j}$ 是向量 $J(\\mathbf{x})^\\top c(\\mathbf{x})$ 的第 $j$ 个分量。因此，惩罚项的梯度是 $\\rho J(\\mathbf{x})^\\top c(\\mathbf{x})$。\n\n合并各项，惩罚目标的完整梯度为：\n$$\n\\nabla \\Phi(\\mathbf{x};\\rho) = \\nabla f(\\mathbf{x}) + \\rho J(\\mathbf{x})^\\top c(\\mathbf{x})\n$$\n\n**2. 惩罚目标的 Hessian 矩阵 $\\nabla^2 \\Phi(\\mathbf{x};\\rho)$**\n\n为了求出 Hessian 矩阵，我们将梯度表达式对 $\\mathbf{x}^\\top$ 求导。$f(\\mathbf{x})$ 的 Hessian 矩阵是 $\\nabla^2 f(\\mathbf{x})$。我们需要计算 $\\rho J(\\mathbf{x})^\\top c(\\mathbf{x})$ 这一项的 Hessian 矩阵。该项可以看作是 $\\frac{\\rho}{2}c(\\mathbf{x})^\\top c(\\mathbf{x})$ 的梯度。所以我们需要求 $\\frac{\\rho}{2}c(\\mathbf{x})^\\top c(\\mathbf{x})$ 的 Hessian 矩阵。\n$$\n\\nabla^2 \\left( \\frac{\\rho}{2} \\sum_{i=1}^m c_i(\\mathbf{x})^2 \\right) = \\frac{\\rho}{2} \\sum_{i=1}^m \\nabla^2(c_i(\\mathbf{x})^2)\n$$\n使用 Hessian 矩阵的乘积法则，$\\nabla^2(u^2) = \\nabla(2u\\nabla u) = 2(\\nabla u)(\\nabla u)^\\top + 2u\\nabla^2 u$，我们得到：\n$$\n\\nabla^2(c_i(\\mathbf{x})^2) = 2(\\nabla c_i(\\mathbf{x}))(\\nabla c_i(\\mathbf{x}))^\\top + 2c_i(\\mathbf{x})\\nabla^2 c_i(\\mathbf{x})\n$$\n其中 $\\nabla c_i(\\mathbf{x})$ 是一个列向量，$\\nabla^2 c_i(\\mathbf{x})$ 是第 $i$ 个约束函数的 Hessian 矩阵。对所有 $i$ 求和：\n$$\n\\frac{\\rho}{2} \\sum_{i=1}^m \\left( 2(\\nabla c_i(\\mathbf{x}))(\\nabla c_i(\\mathbf{x}))^\\top + 2c_i(\\mathbf{x})\\nabla^2 c_i(\\mathbf{x}) \\right) = \\rho \\left( \\sum_{i=1}^m (\\nabla c_i(\\mathbf{x}))(\\nabla c_i(\\mathbf{x}))^\\top + \\sum_{i=1}^m c_i(\\mathbf{x})\\nabla^2 c_i(\\mathbf{x}) \\right)\n$$\n第一个求和项 $\\sum_{i=1}^m (\\nabla c_i(\\mathbf{x}))(\\nabla c_i(\\mathbf{x}))^\\top$ 正是矩阵乘积 $J(\\mathbf{x})^\\top J(\\mathbf{x})$ 的定义。\n因此，惩罚目标的 Hessian 矩阵为：\n$$\n\\nabla^2 \\Phi(\\mathbf{x};\\rho) = \\nabla^2 f(\\mathbf{x}) + \\rho \\left( J(\\mathbf{x})^\\top J(\\mathbf{x}) + \\sum_{i=1}^m c_i(\\mathbf{x}) \\nabla^2 c_i(\\mathbf{x}) \\right)\n$$\n\n**3. 牛顿步长 $\\mathbf{p}$ 及其范数**\n\n最小化 $\\Phi(\\mathbf{x};\\rho)$ 的牛顿法涉及通过求解在当前点 $\\mathbf{x}$ 附近的二阶泰勒展开式导出的线性系统来找到搜索方向（步长）$\\mathbf{p}$。该系统是：\n$$\n\\nabla^2 \\Phi(\\mathbf{x};\\rho) \\mathbf{p} = -\\nabla \\Phi(\\mathbf{x};\\rho)\n$$\n牛顿步长 $\\mathbf{p}$ 是这个 $n \\times n$ 线性方程组的解。当 Hessian 矩阵 $\\nabla^2 \\Phi(\\mathbf{x};\\rho)$ 是奇异或病态时，我们使用伪逆（记为 $(\\cdot)^+$）来计算最小范数解：\n$$\n\\mathbf{p} = -(\\nabla^2 \\Phi(\\mathbf{x};\\rho))^+ \\nabla \\Phi(\\mathbf{x};\\rho)\n$$\n然后，该步长的欧几里得范数计算为 $\\lVert \\mathbf{p} \\rVert_2 = \\sqrt{\\mathbf{p}^\\top\\mathbf{p}}$。\n\n**4. Hessian 条件数**\n\nHessian 矩阵 $H_\\Phi = \\nabla^2 \\Phi(\\mathbf{x};\\rho)$ 的条件数用于量化其对扰动的敏感性。对于对称矩阵，其奇异值是其特征值的绝对值。条件数定义为最大奇异值 $\\sigma_{\\max}$ 与最小奇异值 $\\sigma_{\\min}$ 之比：\n$$\n\\text{cond}(H_\\Phi) = \\frac{\\sigma_{\\max}(H_\\Phi)}{\\sigma_{\\min}(H_\\Phi)}\n$$\n这些奇异值是通过 $H_\\Phi$ 的奇异值分解（SVD）计算的。如果 $\\sigma_{\\min} = 0$，则矩阵是奇异的，条件数定义为 $+\\infty$。\n\n**仿真流程**\n\n仿真将为三个测试案例中的每一个实现这些推导出的公式。对于每个案例，在固定的基点 $\\mathbf{x}_0$ 处，我们将遍历一系列惩罚参数 $\\rho_k = 10^{-2} \\times 10^k$，$k \\in \\{0, 1, 2, 3, 4, 5\\}$。在每次迭代中，我们将：\n1.  在 $\\mathbf{x}_0$ 处评估必要的组成部分：$\\nabla f(\\mathbf{x}_0)$, $\\nabla^2 f(\\mathbf{x}_0)$, $c(\\mathbf{x}_0)$, $J(\\mathbf{x}_0)$ 和 $\\nabla^2 c_i(\\mathbf{x}_0)$。\n2.  组装梯度 $\\nabla \\Phi(\\mathbf{x}_0;\\rho_k)$ 和 Hessian 矩阵 $H_\\Phi = \\nabla^2 \\Phi(\\mathbf{x}_0;\\rho_k)$。\n3.  计算 $H_\\Phi$ 的 SVD 以找到其奇异值，并计算条件数。\n4.  使用 $H_\\Phi$ 的伪逆求解牛顿步长 $\\mathbf{p}_k$。\n5.  计算欧几里得范数 $\\lVert \\mathbf{p}_k \\rVert_2$。\n6.  为每个测试案例收集条件数和步长范数的列表。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the quadratic penalty method simulation problem.\n    Derives and computes Newton steps and Hessian condition numbers for three test cases\n    as the penalty parameter rho increases.\n    \"\"\"\n\n    def run_simulation_case(case_params):\n        \"\"\"\n        Runs the simulation for a single test case.\n\n        Args:\n            case_params (dict): A dictionary containing the functions and data for a case.\n                - 'grad_f': Gradient of the objective function.\n                - 'hess_f': Hessian of the objective function.\n                - 'c_func': Constraint function(s).\n                - 'jac_c': Jacobian of the constraint function(s).\n                - 'hess_c': List of Hessians of each constraint component.\n                - 'x0': The base point for the simulation.\n\n        Returns:\n            list: A list containing two lists:\n                  - The list of Hessian condition numbers for each rho.\n                  - The list of Newton step norms for each rho.\n        \"\"\"\n        grad_f_func = case_params['grad_f']\n        hess_f_func = case_params['hess_f']\n        c_func = case_params['c_func']\n        jac_c_func = case_params['jac_c']\n        hess_c_funcs = case_params['hess_c']\n        x0 = case_params['x0']\n\n        rho_values = [10**(-2 + k) for k in range(6)]\n        \n        condition_numbers = []\n        step_norms = []\n\n        # Evaluate problem functions at the fixed base point x0\n        grad_f_x0 = grad_f_func(x0)\n        hess_f_x0 = hess_f_func(x0)\n        c_x0 = c_func(x0)\n        jac_c_x0 = jac_c_func(x0)\n        \n        # Calculate the sum term in the Hessian formula: sum(c_i * H_ci)\n        sum_c_hess_c = np.zeros_like(hess_f_x0)\n        for i in range(len(c_x0)):\n            sum_c_hess_c += c_x0[i] * hess_c_funcs[i](x0)\n\n        for rho in rho_values:\n            # 1. Assemble the gradient and Hessian of the penalty function\n            # Gradient: grad_Phi = grad_f + rho * J^T * c\n            grad_phi = grad_f_x0 + rho * jac_c_x0.T @ c_x0\n\n            # Hessian: hess_Phi = hess_f + rho * (J^T * J + sum(c_i * H_ci))\n            hess_phi = hess_f_x0 + rho * (jac_c_x0.T @ jac_c_x0 + sum_c_hess_c)\n\n            # 2. Compute the condition number from SVD\n            try:\n                singular_values = np.linalg.svd(hess_phi, compute_uv=False)\n                s_min = singular_values[-1]\n                s_max = singular_values[0]\n                \n                # Check for singularity\n                if s_min < 1e-16:\n                    cond_num = np.inf\n                else:\n                    cond_num = s_max / s_min\n            except np.linalg.LinAlgError:\n                cond_num = np.inf\n            condition_numbers.append(cond_num)\n\n            # 3. Compute the Newton step p using pseudoinverse\n            # H_phi * p = -g_phi  => p = -pinv(H_phi) * g_phi\n            try:\n                p = -np.linalg.pinv(hess_phi) @ grad_phi\n            except np.linalg.LinAlgError:\n                p = np.full_like(x0, np.nan) # Should not happen with pinv, but for robustness\n            \n            # 4. Compute the Euclidean norm of the step\n            p_norm = np.linalg.norm(p)\n            step_norms.append(p_norm)\n            \n        return [condition_numbers, step_norms]\n\n    # Definition of test cases\n    test_cases = [\n        { # Case A\n            'grad_f': lambda x: np.array([x[0], 2 * x[1]]),\n            'hess_f': lambda x: np.array([[1.0, 0.0], [0.0, 2.0]]),\n            'c_func': lambda x: np.array([x[0] + x[1] - 1.0]),\n            'jac_c': lambda x: np.array([[1.0, 1.0]]),\n            'hess_c': [lambda x: np.zeros((2, 2))],\n            'x0': np.array([0.0, 0.0])\n        },\n        { # Case B\n            'grad_f': lambda x: np.array([2 * x[0], x[1]]),\n            'hess_f': lambda x: np.array([[2.0, 0.0], [0.0, 1.0]]),\n            'c_func': lambda x: np.array([x[0]**2 + x[1] - 1.0]),\n            'jac_c': lambda x: np.array([[2 * x[0], 1.0]]),\n            'hess_c': [lambda x: np.array([[2.0, 0.0], [0.0, 0.0]])],\n            'x0': np.array([0.5, -0.2])\n        },\n        { # Case C\n            'grad_f': lambda x: np.array([1e-3 * x[0], x[1]]),\n            'hess_f': lambda x: np.array([[1e-3, 0.0], [0.0, 1.0]]),\n            'c_func': lambda x: np.array([x[0] - 0.5]),\n            'jac_c': lambda x: np.array([[1.0, 0.0]]),\n            'hess_c': [lambda x: np.zeros((2, 2))],\n            'x0': np.array([1.0, 1.0])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_simulation_case(case)\n        all_results.append(result)\n\n    # Format output to match [[case_A_res], [case_B_res], ...]\n    # where each case_res is [[conds],[norms]].\n    # The map(str, ...) and join will create a string like \"[[...]],[[...]]\"\n    # which is then wrapped in \"[\" and \"]\".\n    results_str = ','.join(map(str, all_results))\n    print(f\"[{results_str}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "优化算法的性能不仅取决于其数学理论，还取决于它对不完美数据的鲁棒性。二次罚函数法由于其平方项的特性，对约束中的“离群点”或大的违反值非常敏感。这项练习将引导你对比二次惩罚与更稳健的Huber惩罚，分析它们在处理大残差时的不同表现，并思考如何诊断和改进算法对异常值的敏感性。",
            "id": "3169167",
            "problem": "考虑一个等式约束优化问题，其目标是最小化可微目标函数 $f(x)$，约束条件为 $h(x)=0$，其中 $h:\\mathbb{R}^n\\to\\mathbb{R}^m$ 是连续可微的，其雅可比矩阵为 $J(x)\\in\\mathbb{R}^{m\\times n}$。二次惩罚法通过最小化带惩罚的目标函数 $F_{\\text{quad}}(x;\\mu)=f(x)+\\frac{\\mu}{2}\\sum_{i=1}^m h_i(x)^2$ 来替代该约束问题，其中惩罚参数 $\\mu>0$。一种稳健的替代方法是在约束残差上逐元素使用 Huber 损失，即 $F_{\\text{hub}}(x;\\mu,\\tau)=f(x)+\\mu\\sum_{i=1}^m \\phi_\\tau\\big(h_i(x)\\big)$，其中 Huber 函数 $\\phi_\\tau:\\mathbb{R}\\to\\mathbb{R}$ 定义为：如果 $|r|\\le \\tau$，则 $\\phi_\\tau(r)=\\frac{1}{2}r^2$；如果 $|r|>\\tau$，则 $\\phi_\\tau(r)=\\tau\\big(|r|-\\frac{1}{2}\\tau\\big)$，阈值 $\\tau>0$。$\\phi_\\tau$ 的导数满足：当 $|r|\\le \\tau$ 时 $\\phi_\\tau'(r)=r$，当 $|r|>\\tau$ 时 $\\phi_\\tau'(r)=\\tau\\,\\mathrm{sign}(r)$；其二阶导数满足：当 $|r|\\le \\tau$ 时 $\\phi_\\tau''(r)=1$，当 $|r|>\\tau$ 时 $\\phi_\\tau''(r)=0$，但在 $|r|=\\tau$ 处除外，该点处函数不是二阶可微的，但存在次导数。\n\n从这些定义以及多元微积分中关于梯度和 Hessian 矩阵的链式法则出发，分析大的约束残差 $|h_i(x)|$ 如何影响 $F_{\\text{quad}}$ 与 $F_{\\text{hub}}$ 中惩罚项的梯度和曲率贡献。此外，考虑一些诊断方法，这些方法可以在使用一阶或二阶步长（例如，最速下降法、拟牛顿法或高斯-牛顿法）的迭代算法中揭示对约束残差中离群值的敏感性。\n\n选择所有正确的陈述。\n\nA. 对于二次惩罚，来自约束的梯度贡献是 $\\mu\\,J(x)^\\top h(x)$。对于 Huber 惩罚，它是 $\\mu\\,J(x)^\\top w\\big(h(x)\\big)$，其中 $w:\\mathbb{R}^m\\to\\mathbb{R}^m$ 的分量为：如果 $|r_i|\\le \\tau$，则 $w_i(r_i)=r_i$；如果 $|r_i|>\\tau$，则 $w_i(r_i)=\\tau\\,\\mathrm{sign}(r_i)$。因此，在 Huber 化下，非常大的残差对梯度的影响是有界的，这降低了对离群值的敏感性。\n\nB. 二次惩罚总是为任何残差大小产生一个良态的 Hessian 矩阵，因为增加 $\\mu$ 会减小曲率。\n\nC. 一种用于约束中离群值敏感性的实用诊断方法是，在迭代过程中监控比率 $\\max_i |h_i(x)|\\,/\\,\\mathrm{median}_i |h_i(x)|$。持续存在的大比率表明残差是重尾的，对于这种情况，用 Huber 化变体替换平方惩罚可以提高稳健性。\n\nD. 对约束使用 Huber 惩罚保证在有限的惩罚参数 $\\mu$ 下精确满足 $h(x^\\star)=0$，因为大残差的饱和梯度可以防止步长问题。\n\nE. 一种面向曲率的诊断方法是比较二次惩罚的高斯-牛顿型项 $\\mu\\,J(x)^\\top J(x)$ 的条件数与相应的 Huber 化项 $\\mu\\,J(x)^\\top W(x) J(x)$ 的条件数，其中 $W(x)=\\mathrm{diag}\\big(\\phi_\\tau''(h_i(x))\\big)$。如果大残差在二次惩罚下导致病态，并且 Huber 化的曲率通过对离群值降权（其中 $\\phi_\\tau''=0$ 的条目）来减小该条件数，这表明稳健性得到了改善。",
            "solution": "问题陈述是数值优化方法分析中的一个有效练习。它在约束优化理论中有科学依据，作为一个分析任务是适定的，并使用了客观、标准的术语。设置中没有矛盾、歧义或事实错误。因此，我们可以进行推导和评估。\n\n问题的核心是比较用于等式约束问题 $\\min f(x)$ s.t. $h(x)=0$ 的两种惩罚函数的性质。这两个带惩罚的目标函数是：\n1.  二次惩罚：$F_{\\text{quad}}(x;\\mu) = f(x) + \\frac{\\mu}{2} \\|h(x)\\|_2^2 = f(x) + \\frac{\\mu}{2} \\sum_{i=1}^m h_i(x)^2$。\n2.  Huber 惩罚：$F_{\\text{hub}}(x;\\mu,\\tau) = f(x) + \\mu \\sum_{i=1}^m \\phi_\\tau(h_i(x))$。\n\n我们将分析来自惩罚项的梯度和 Hessian 贡献。设 $P_{\\text{quad}}(x) = \\frac{\\mu}{2} \\|h(x)\\|_2^2$ 和 $P_{\\text{hub}}(x) = \\mu \\sum_{i=1}^m \\phi_\\tau(h_i(x))$。\n\n**梯度推导**\n\n使用链式法则，复合函数 $g(h(x))$ 的梯度是 $\\nabla_x g(h(x)) = J(x)^\\top \\nabla_h g(h(x))$，其中 $J(x)$ 是 $h(x)$ 的雅可比矩阵。\n\n对于二次惩罚，惩罚函数是 $P_{\\text{quad}}(x) = \\frac{\\mu}{2} h(x)^\\top h(x)$。它关于 $h$ 的梯度是 $\\mu h$。因此，关于 $x$ 的梯度是：\n$$ \\nabla_x P_{\\text{quad}}(x) = \\mu J(x)^\\top h(x) $$\n\n对于 Huber 惩罚，$P_{\\text{hub}}(x) = \\mu \\sum_{i=1}^m \\phi_\\tau(h_i(x))$。这个和关于向量 $h$ 的梯度是一个向量，其第 $i$ 个分量是 $\\mu \\phi_\\tau'(h_i(x))$。我们定义一个向量值函数 $w:\\mathbb{R}^m \\to \\mathbb{R}^m$，其分量为 $w_i(r_i) = \\phi_\\tau'(r_i)$。因此，关于 $h$ 的梯度是 $\\mu w(h(x))$。关于 $x$ 的梯度是：\n$$ \\nabla_x P_{\\text{hub}}(x) = \\mu J(x)^\\top w(h(x)) $$\n根据问题定义，函数 $w_i(r_i)$ 定义为：如果 $|r_i| \\le \\tau$，$w_i(r_i) = r_i$；如果 $|r_i| > \\tau$，$w_i(r_i) = \\tau\\,\\mathrm{sign}(r_i)$。\n\n**Hessian (曲率) 推导**\n\n惩罚项的 Hessian 矩阵需要再次应用链式法则。完整的 Hessian 矩阵是 $\\nabla_x^2 P(x) = J(x)^\\top (\\nabla_h^2 P(h)) J(x) + \\sum_{i=1}^m (\\nabla_h P(h))_i \\nabla_x^2 h_i(x)$。在许多优化算法（如高斯-牛顿法）中，第二项（涉及约束的二阶导数）被省略。剩下的项 $J(x)^\\top (\\nabla_h^2 P(h)) J(x)$ 被称为 Hessian 矩阵的高斯-牛顿近似。\n\n对于二次惩罚，$\\nabla_h^2 P_{\\text{quad}}(h) = \\mu I$，其中 $I$ 是 $m \\times m$ 的单位矩阵。高斯-牛顿 Hessian 贡献是：\n$$ H_{\\text{GN,quad}} = \\mu J(x)^\\top J(x) $$\n\n对于 Huber 惩罚，关于 $h$ 的 Hessian 矩阵是一个对角矩阵，因为 $\\phi_\\tau$ 是逐元素应用的。设这个对角矩阵为 $W(x) = \\mathrm{diag}(\\phi_\\tau''(h_1(x)), \\dots, \\phi_\\tau''(h_m(x)))$。那么 $\\nabla_h^2 P_{\\text{hub}}(h) = \\mu W(x)$。高斯-牛顿 Hessian 贡献是：\n$$ H_{\\text{GN,hub}} = \\mu J(x)^\\top W(x) J(x) $$\n根据问题陈述，$W(x)$ 的对角线元素是 $\\phi_\\tau''(h_i(x)) = 1$（如果 $|h_i(x)| \\le \\tau$）和 $\\phi_\\tau''(h_i(x)) = 0$（如果 $|h_i(x)| > \\tau$）。\n\n现在我们评估每个陈述。\n\n**A. 对于二次惩罚，来自约束的梯度贡献是 $\\mu\\,J(x)^\\top h(x)$。对于 Huber 惩罚，它是 $\\mu\\,J(x)^\\top w\\big(h(x)\\big)$，其中 $w:\\mathbb{R}^m\\to\\mathbb{R}^m$ 的分量为：如果 $|r_i|\\le \\tau$，则 $w_i(r_i)=r_i$；如果 $|r_i|>\\tau$，则 $w_i(r_i)=\\tau\\,\\mathrm{sign}(r_i)$。因此，在 Huber 化下，非常大的残差对梯度的影响是有界的，这降低了对离群值的敏感性。**\n我们的推导证实了梯度表达式是正确的。对于二次惩罚，来自第 i 个约束的梯度贡献与 $\\mu h_i(x)$ 成正比，随着残差 $|h_i(x)|$ 的增加而无界增长。对于 Huber 惩罚，其贡献与 $\\mu w_i(h_i(x))$ 成正比，当 $|h_i(x)| > \\tau$ 时，其大小饱和在 $\\mu\\tau$。这种对大残差影响的限制是实现对离群值稳健性的关键机制。一个离群的约束违反不会产生一个任意大的梯度分量来主导优化步骤。\n**结论：正确。**\n\n**B. 二次惩罚总是为任何残差大小产生一个良态的 Hessian 矩阵，因为增加 $\\mu$ 会减小曲率。**\n这个陈述根本上是错误的。带惩罚目标函数的 Hessian 矩阵为 $\\nabla^2 F_{\\text{quad}} \\approx \\nabla^2 f(x) + \\mu J(x)^\\top J(x)$。当惩罚参数 $\\mu$ 增大以更好地强制执行约束时，第二项占主导地位。Hessian 矩阵 $\\nabla^2 F_{\\text{quad}}$ 的条件数通常随着 $\\mu \\to \\infty$ 而变得非常大，这就是病态的定义。这使得无约束子问题越来越难以解决。增加 $\\mu$ 会*增加*由惩罚引起的曲率，而不是减小它。\n**结论：错误。**\n\n**C. 一种用于约束中离群值敏感性的实用诊断方法是，在迭代过程中监控比率 $\\max_i |h_i(x)|\\,/\\,\\mathrm{median}_i |h_i(x)|$。持续存在的大比率表明残差是重尾的，对于这种情况，用 Huber 化变体替换平方惩罚可以提高稳健性。**\n这描述了一种合理且实用的启发式方法。中位数是一种稳健的尺度估计量，不受少数大值的影响。相反，最大值由单个最大的离群值决定。最大值与中位数之间的巨大比率表明至少有一个残差显著大于其他残差，这是离群值或残差重尾分布的一个标志。在这种情况下，对大值敏感的二次惩罚可能导致性能不佳。切换到像 Huber 损失这样的稳健惩罚函数，它会降低大残差的影响，是改善算法行为的标准且正确的回应。\n**结论：正确。**\n\n**D. 对约束使用 Huber 惩罚保证在有限的惩罚参数 $\\mu$ 下精确满足 $h(x^\\star)=0$，因为大残差的饱和梯度可以防止步长问题。**\n这个陈述是错误的。如果通过求解惩罚问题（对于一个足够大但*有限*的 $\\mu$ 值）可以恢复约束问题的解，那么这个惩罚函数就是“精确的”。对于等式约束，精确惩罚的一个常见要求是在原点处不可微，例如在 $L_1$ 惩罚函数 $\\mu \\sum_i |h_i(x)|$ 中。Huber 函数 $\\phi_\\tau(r)$ 在 $r=0$ 处是可微的，且 $\\phi_\\tau(0)=0$ 和 $\\phi_\\tau'(0)=0$。在原点附近（对于 $|r| \\le \\tau$），它的行为与二次惩罚完全相同。与二次惩罚一样，Huber 惩罚不是一个精确惩罚函数；理论上仍必须取极限 $\\mu \\to \\infty$ 来强制 $h(x)=0$。大残差梯度的饱和解决了对离群值的稳健性问题，但它不改变 $h_i=0$ 附近决定惩罚是否精确的局部行为。\n**结论：错误。**\n\n**E. 一种面向曲率的诊断方法是比较二次惩罚的高斯-牛顿型项 $\\mu\\,J(x)^\\top J(x)$ 的条件数与相应的 Huber 化项 $\\mu\\,J(x)^\\top W(x) J(x)$ 的条件数，其中 $W(x)=\\mathrm{diag}\\big(\\phi_\\tau''(h_i(x))\\big)$。如果大残差在二次惩罚下导致病态，并且 Huber 化的曲率通过对离群值降权（其中 $\\phi_\\tau''=0$ 的条目）来减小该条件数，这表明稳健性得到了改善。**\n我们的推导证实了高斯-牛顿 Hessian 近似的表达式。矩阵 $W(x)$ 的对角线元素为 $W_{ii}(x) = \\phi_\\tau''(h_i(x))$，对于小残差（$|h_i(x)| \\le \\tau$）为 1，对于大残差（$|h_i(x)| > \\tau$）为 0。项 $\\mu J(x)^\\top J(x)$ 可以写成 $\\mu J(x)^\\top I J(x)$，其中所有约束都由 1 进行等权重加权。Huber化的项 $\\mu J(x)^\\top W(x) J(x)$ 有效地为任何被视为离群值的约束的曲率贡献分配了 0 的权重。如果这些离群约束导致了 Hessian 矩阵的病态（例如，通过冲突或大幅值的梯度），通过 $W(x)$ 矩阵移除它们将会稳定问题并降低 Hessian 近似矩阵的条件数。因此，比较这两个矩阵的条件数是量化 Huber 惩罚所带来的数值稳定性改善的绝佳方法。\n**结论：正确。**",
            "answer": "$$\\boxed{ACE}$$"
        }
    ]
}