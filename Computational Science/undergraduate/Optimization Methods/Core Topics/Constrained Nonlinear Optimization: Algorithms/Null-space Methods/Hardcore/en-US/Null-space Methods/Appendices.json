{
    "hands_on_practices": [
        {
            "introduction": "This first exercise serves as a foundational walkthrough of the entire null-space method. By solving a simple two-dimensional quadratic program, you will practice every key step: using a $QR$ factorization to find a basis for the null space, parameterizing the feasible set, and solving the resulting unconstrained problem. This hands-on calculation solidifies the core procedure and connects it back to the familiar Karush-Kuhn-Tucker (KKT) conditions, providing a comprehensive understanding of how the method works from start to finish .",
            "id": "3158284",
            "problem": "Consider the equality-constrained quadratic optimization problem\n$$\\min_{x \\in \\mathbb{R}^{2}} \\;\\; \\frac{1}{2} x^{\\top} H x + g^{\\top} x \\quad \\text{subject to} \\quad A x = b,$$\nwhere the data are\n$$H = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix}, \\quad g = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad A = \\begin{pmatrix} 1 & 1 \\end{pmatrix}, \\quad b = 2.$$\nThis problem is strictly convex because the Hessian matrix $H$ is symmetric positive definite. Using the null-space method grounded in linear algebra and the optimality conditions for equality-constrained optimization, carry out the following:\n\n1. Compute the full $QR$ factorization of the transpose $A^{\\top}$, i.e., find an orthogonal matrix $Q \\in \\mathbb{R}^{2 \\times 2}$ and an upper-triangular (in the rectangular sense) matrix $R \\in \\mathbb{R}^{2 \\times 1}$ such that $A^{\\top} = Q R$. From $Q$, identify a matrix $Z \\in \\mathbb{R}^{2 \\times 1}$ whose column forms an orthonormal basis for the null space of $A$.\n\n2. Find any feasible point $x_{0} \\in \\mathbb{R}^{2}$ satisfying $A x_{0} = b$.\n\n3. Using the null-space parameterization $x = x_{0} + Z p$ with $p \\in \\mathbb{R}$, derive the reduced one-dimensional quadratic in $p$ obtained by substituting into the objective. Solve for the minimizer $p^{\\star}$ and compute the corresponding primal minimizer $x^{\\star} = x_{0} + Z p^{\\star}$.\n\n4. Verify that $x^{\\star}$ satisfies feasibility $A x^{\\star} = b$ and that the reduced optimality condition in the null space holds.\n\n5. Independently, solve the equality-constrained problem via the Karush–Kuhn–Tucker (KKT) conditions, which for equality constraints assert stationarity and feasibility. Form and solve the linear KKT system to obtain the primal solution. Compare it to the null-space solution.\n\nReport your final answer as the optimal primal vector $x^{\\star}$ in row-matrix form. No rounding is required; provide exact values.",
            "solution": "The problem is a well-posed, equality-constrained quadratic program. It is scientifically sound, self-contained, and all data required for its solution are provided. The Hessian matrix $H$ is symmetric and positive definite, which guarantees that the objective function is strictly convex and that a unique minimizer exists. The constraint matrix $A$ has full row rank, ensuring the feasibility of the constraints. Therefore, the problem is valid, and we may proceed with the solution by following the specified steps.\n\nThe objective is to minimize $f(x) = \\frac{1}{2} x^{\\top} H x + g^{\\top} x$ subject to $A x = b$, with the given data:\n$$H = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix}, \\quad g = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad A = \\begin{pmatrix} 1 & 1 \\end{pmatrix}, \\quad b = 2$$\n\n**1. QR Factorization and Null-Space Basis**\n\nWe compute the full $QR$ factorization of $A^{\\top}$. The matrix $A^{\\top}$ is a column vector in $\\mathbb{R}^{2}$:\n$$A^{\\top} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nWe need to find an orthogonal matrix $Q \\in \\mathbb{R}^{2 \\times 2}$ and an upper-triangular matrix $R \\in \\mathbb{R}^{2 \\times 1}$ such that $A^{\\top} = QR$. We can use the Gram-Schmidt process. Let the columns of $Q$ be $q_1$ and $q_2$.\n\nThe first column $q_1$ is a normalized version of $A^{\\top}$:\n$$\\|A^{\\top}\\| = \\sqrt{1^2 + 1^2} = \\sqrt{2}$$\n$$q_1 = \\frac{A^{\\top}}{\\|A^{\\top}\\|} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix}$$\nThe second column $q_2$ must be a unit vector orthogonal to $q_1$. We can choose:\n$$q_2 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix}$$\nWe can verify that $q_1^{\\top} q_2 = 0$ and $\\|q_2\\|=1$.\nThe orthogonal matrix $Q$ is formed by these columns:\n$$Q = \\begin{pmatrix} q_1 & q_2 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\ 1/\\sqrt{2} & -1/\\sqrt{2} \\end{pmatrix}$$\nThe matrix $R$ is given by $R = Q^{\\top} A^{\\top}$:\n$$R = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} \\\\ 1/\\sqrt{2} & -1/\\sqrt{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ 0 \\end{pmatrix}$$\nThis $R$ is upper-triangular.\n\nThe null space of $A$, denoted $\\mathcal{N}(A)$, consists of all vectors $z$ such that $Az=0$. For $A = \\begin{pmatrix} 1 & 1 \\end{pmatrix}$, this means $z_1+z_2=0$. The columns of $Q$ can be partitioned as $Q = \\begin{pmatrix} Y & Z \\end{pmatrix}$, where the columns of $Y$ form a basis for the range of $A^{\\top}$ and the columns of $Z$ form a basis for the null space of $A$. Here, $Y$ corresponds to $q_1$ and $Z$ to $q_2$.\n$$Z = q_2 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix}$$\nWe verify that $AZ = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix} = 1/\\sqrt{2} - 1/\\sqrt{2} = 0$. So, $Z$ indeed forms an orthonormal basis for $\\mathcal{N}(A)$.\n\n**2. Finding a Feasible Point**\n\nWe need to find any particular solution $x_0$ to the linear system $Ax=b$:\n$$\\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = 2 \\quad \\implies \\quad x_1 + x_2 = 2$$\nA simple choice is $x_1=1$, $x_2=1$. Thus, a feasible point is:\n$$x_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n\n**3. Null-Space Parameterization and Solution**\n\nAny feasible point $x$ can be expressed as the sum of a particular feasible point $x_0$ and a vector from the null space of $A$. Since $Z$ is a basis for $\\mathcal{N}(A)$, any such vector can be written as $Zp$ for some $p \\in \\mathbb{R}$.\n$$x = x_0 + Zp = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix} p$$\nWe substitute this into the objective function $f(x)$ to obtain a reduced quadratic function $q(p)$:\n$$f(x_0+Zp) = \\frac{1}{2}(x_0+Zp)^{\\top}H(x_0+Zp) + g^{\\top}(x_0+Zp)$$\n$$q(p) = \\frac{1}{2} p^{\\top}(Z^{\\top}HZ)p + (x_0^{\\top}HZ + g^{\\top}Z)p + (\\frac{1}{2}x_0^{\\top}Hx_0 + g^{\\top}x_0)$$\nSince $p$ is a scalar, we can write this as:\n$$q(p) = \\frac{1}{2} (Z^{\\top}HZ) p^2 + (Z^{\\top}(Hx_0+g)) p + \\text{const.}$$\nWe compute the coefficients. The reduced Hessian is:\n$$H Z = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix} = \\begin{pmatrix} 2/\\sqrt{2} \\\\ -4/\\sqrt{2} \\end{pmatrix}$$\n$$Z^{\\top}HZ = \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\end{pmatrix} \\begin{pmatrix} 2/\\sqrt{2} \\\\ -4/\\sqrt{2} \\end{pmatrix} = \\frac{2}{2} - \\frac{-4}{2} = 1+2 = 3$$\nThe reduced gradient term's coefficient is:\n$$Hx_0 + g = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}$$\n$$Z^{\\top}(Hx_0+g) = \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} = \\frac{3}{\\sqrt{2}} - \\frac{3}{\\sqrt{2}} = 0$$\nThe reduced objective function is:\n$$q(p) = \\frac{3}{2} p^2 + \\text{const.}$$\nTo find the minimizer $p^{\\star}$, we set the derivative with respect to $p$ to zero:\n$$\\frac{d q}{d p} = 3p = 0 \\quad \\implies \\quad p^{\\star} = 0$$\nThe optimal primal solution $x^{\\star}$ is then:\n$$x^{\\star} = x_0 + Z p^{\\star} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix} (0) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n\n**4. Verification**\n\nFirst, we check feasibility of $x^{\\star}$:\n$$A x^{\\star} = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 1+1=2 = b$$\nThe constraint is satisfied.\nSecond, we check the reduced optimality condition. The gradient of the reduced problem, $\\nabla_p q(p) = (Z^{\\top}HZ)p + Z^{\\top}(Hx_0+g)$, must be zero at $p^{\\star}$. This is equivalent to checking that the gradient of the original objective function at $x^{\\star}$, $\\nabla f(x^{\\star}) = Hx^{\\star}+g$, is orthogonal to the null space, i.e., $Z^{\\top}(Hx^{\\star}+g)=0$.\n$$Hx^{\\star}+g = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}$$\n$$Z^{\\top}(Hx^{\\star}+g) = \\begin{pmatrix} 1/\\sqrt{2} & -1/\\sqrt{2} \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} = 0$$\nThe reduced optimality condition holds.\n\n**5. Solution via KKT Conditions**\n\nThe Karush-Kuhn-Tucker (KKT) conditions for this equality-constrained problem are stationarity and primal feasibility. Let $\\lambda \\in \\mathbb{R}$ be the Lagrange multiplier for the single constraint.\n1. Stationarity: $\\nabla f(x) + A^{\\top}\\lambda = 0 \\implies Hx + g + A^{\\top}\\lambda = 0$\n2. Primal Feasibility: $Ax=b$\n\nThis forms a system of linear equations:\n$$\\begin{pmatrix} H & A^{\\top} \\\\ A & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ \\lambda \\end{pmatrix} = \\begin{pmatrix} -g \\\\ b \\end{pmatrix}$$\nSubstituting the given matrices and vectors:\n$$\\begin{pmatrix} 2 & 0 & 1 \\\\ 0 & 4 & 1 \\\\ 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\lambda \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\\\ 2 \\end{pmatrix}$$\nThis represents the system of equations:\n(i) $2x_1 + \\lambda = -1$\n(ii) $4x_2 + \\lambda = 1$\n(iii) $x_1 + x_2 = 2$\n\nFrom (iii), $x_2 = 2-x_1$. From (i), $\\lambda = -1 - 2x_1$. Substituting these into (ii):\n$$4(2-x_1) + (-1-2x_1) = 1$$\n$$8 - 4x_1 - 1 - 2x_1 = 1$$\n$$7 - 6x_1 = 1$$\n$$6x_1 = 6 \\implies x_1 = 1$$\nThen, $x_2 = 2 - 1 = 1$.\nThe optimal primal solution is $x^{\\star} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nThis result is identical to the one obtained via the null-space method, confirming the correctness of our solution. The corresponding optimal Lagrange multiplier is $\\lambda = -1 - 2(1) = -3$.\n\nThe final answer is the optimal primal vector $x^{\\star}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 1 & 1 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Moving beyond the basic mechanics, this practice focuses on developing intuition for constructing an efficient null-space basis. You will tackle a problem where the constraints possess a simple, decoupled structure, making it possible to define the null space using intuitive coordinate differences . This exercise hones the critical skill of translating the algebraic form of constraints directly into a practical parameterization of the feasible search space.",
            "id": "3158219",
            "problem": "Consider the equality-constrained quadratic optimization problem in $\\mathbb{R}^{4}$:\nminimize $f(x) = \\tfrac{1}{2}\\,x^{\\top} H x$ subject to $A x = b$, where $H \\in \\mathbb{R}^{4 \\times 4}$ is diagonal, $A \\in \\mathbb{R}^{2 \\times 4}$ enforces sums of disjoint coordinate pairs, and $b \\in \\mathbb{R}^{2}$ sets the pairwise sums to specified values. Specifically, let\n$$\nH = \\operatorname{diag}(2,4,1,3), \\quad\nA = \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\end{pmatrix}, \\quad\nb = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}.\n$$\nUsing only the definitions of the Lagrangian for equality-constrained problems and the null space of a matrix as the starting point, proceed as follows:\n\n- Construct a feasible particular solution $x_{p}$ satisfying $A x_{p} = b$.\n- Derive explicitly a full-column-rank matrix $Z \\in \\mathbb{R}^{4 \\times 2}$ whose columns form a basis of the null space of $A$, choosing basis vectors as coordinate differences that naturally encode the constraints on sums.\n- Express all feasible $x$ in the form $x = x_{p} + Z z$ with $z \\in \\mathbb{R}^{2}$, form the reduced quadratic in $z$, and solve analytically for the unique minimizer $z^{\\star}$.\n- Recover the corresponding optimizer $x^{\\star}$ and evaluate the optimal objective value $f(x^{\\star})$.\n\nReport only the exact value of $f(x^{\\star})$ as a reduced fraction. No rounding is required and no units are involved.",
            "solution": "We begin from first principles for equality-constrained quadratic optimization. The objective is the quadratic function $f(x) = \\tfrac{1}{2}\\,x^{\\top} H x$ with a symmetric positive definite matrix $H$. The constraints are linear equalities $A x = b$. The method of Lagrange multipliers introduces the Lagrangian\n$$\n\\mathcal{L}(x,\\lambda) = \\tfrac{1}{2}\\,x^{\\top} H x + \\lambda^{\\top}(A x - b),\n$$\nand the Karush–Kuhn–Tucker (KKT) first-order optimality conditions are\n$$\nH x + A^{\\top} \\lambda = 0, \\quad A x = b.\n$$\nAn equivalent approach, called the null-space method, parameterizes all feasible points via a particular feasible point and a basis of the null space of $A$. We follow this route.\n\nStep 1: Construct a feasible particular solution $x_{p}$.\nThe constraints are $x_{1} + x_{2} = 2$ and $x_{3} + x_{4} = 2$. A convenient choice is to split each sum evenly:\n$$\nx_{p} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix},\n$$\nwhich satisfies $A x_{p} = b$.\n\nStep 2: Derive a null-space basis $Z$ as differences of coordinates.\nThe null space $\\mathcal{N}(A)$ consists of vectors $v \\in \\mathbb{R}^{4}$ with $A v = 0$, i.e., vectors satisfying $v_{1} + v_{2} = 0$ and $v_{3} + v_{4} = 0$. Natural basis vectors are coordinate differences within each pair:\n$$\nz_{1} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\nz_{2} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}.\n$$\nCollect these as columns of\n$$\nZ = \\begin{pmatrix}\n1 & 0 \\\\\n-1 & 0 \\\\\n0 & 1 \\\\\n0 & -1\n\\end{pmatrix}.\n$$\nOne verifies $A Z = 0$, so $Z$ has columns that span $\\mathcal{N}(A)$.\n\nStep 3: Parameterize feasibility and form the reduced problem.\nEvery feasible $x$ can be written as $x = x_{p} + Z z$ with $z \\in \\mathbb{R}^{2}$. Substituting into $f$ yields the reduced quadratic in $z$:\n$$\n\\phi(z) \\equiv f(x_{p} + Z z) = \\tfrac{1}{2}\\,(x_{p} + Z z)^{\\top} H (x_{p} + Z z).\n$$\nExpanding and using symmetry of $H$,\n$$\n\\phi(z) = \\tfrac{1}{2}\\,z^{\\top}(Z^{\\top} H Z) z + (Z^{\\top} H x_{p})^{\\top} z + \\tfrac{1}{2}\\,x_{p}^{\\top} H x_{p}.\n$$\nThe reduced Hessian is $Z^{\\top} H Z \\in \\mathbb{R}^{2 \\times 2}$ and, because $H$ is positive definite and $Z$ has full column rank, $Z^{\\top} H Z$ is positive definite. The unique minimizer $z^{\\star}$ satisfies the first-order condition\n$$\n\\nabla \\phi(z) = (Z^{\\top} H Z)\\,z + Z^{\\top} H x_{p} = 0,\n$$\nso\n$$\nz^{\\star} = -\\,(Z^{\\top} H Z)^{-1} Z^{\\top} H x_{p}.\n$$\n\nStep 4: Compute $Z^{\\top} H Z$ and $Z^{\\top} H x_{p}$, solve for $z^{\\star}$, recover $x^{\\star}$.\nFirst compute $H Z$ by acting $H = \\operatorname{diag}(2,4,1,3)$ on the columns of $Z$:\n$$\nH z_{1} = \\begin{pmatrix} 2 \\\\ -4 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\nH z_{2} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ -3 \\end{pmatrix}.\n$$\nTherefore,\n$$\nZ^{\\top} H Z\n= \\begin{pmatrix}\nz_{1}^{\\top} H z_{1} & z_{1}^{\\top} H z_{2} \\\\\nz_{2}^{\\top} H z_{1} & z_{2}^{\\top} H z_{2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n6 & 0 \\\\\n0 & 4\n\\end{pmatrix}.\n$$\nNext compute $H x_{p}$:\n$$\nH x_{p} = \\operatorname{diag}(2,4,1,3)\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n= \\begin{pmatrix} 2 \\\\ 4 \\\\ 1 \\\\ 3 \\end{pmatrix}.\n$$\nThen\n$$\nZ^{\\top} H x_{p}\n= \\begin{pmatrix}\nz_{1}^{\\top} H x_{p} \\\\\nz_{2}^{\\top} H x_{p}\n\\end{pmatrix}\n= \\begin{pmatrix}\n(1)(2) + (-1)(4) + (0)(1) + (0)(3) \\\\\n(0)(2) + (0)(4) + (1)(1) + (-1)(3)\n\\end{pmatrix}\n= \\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix}.\n$$\nThus,\n$$\nz^{\\star} = - \\begin{pmatrix} 6 & 0 \\\\ 0 & 4 \\end{pmatrix}^{-1} \\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix}\n= - \\begin{pmatrix} \\tfrac{1}{6} & 0 \\\\ 0 & \\tfrac{1}{4} \\end{pmatrix} \\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix}\n= \\begin{pmatrix} \\tfrac{1}{3} \\\\ \\tfrac{1}{2} \\end{pmatrix}.\n$$\nRecover $x^{\\star} = x_{p} + Z z^{\\star}$:\n$$\nx^{\\star} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n+ \\begin{pmatrix}\n1 & 0 \\\\\n-1 & 0 \\\\\n0 & 1 \\\\\n0 & -1\n\\end{pmatrix}\n\\begin{pmatrix} \\tfrac{1}{3} \\\\ \\tfrac{1}{2} \\end{pmatrix}\n= \\begin{pmatrix}\n1 + \\tfrac{1}{3} \\\\\n1 - \\tfrac{1}{3} \\\\\n1 + \\tfrac{1}{2} \\\\\n1 - \\tfrac{1}{2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\tfrac{4}{3} \\\\ \\tfrac{2}{3} \\\\ \\tfrac{3}{2} \\\\ \\tfrac{1}{2}\n\\end{pmatrix}.\n$$\n\nStep 5: Evaluate the optimal objective value $f(x^{\\star})$.\nCompute $x^{\\star \\top} H x^{\\star}$ using $H = \\operatorname{diag}(2,4,1,3)$:\n$$\nx^{\\star \\top} H x^{\\star}\n= 2\\left(\\tfrac{4}{3}\\right)^{2} + 4\\left(\\tfrac{2}{3}\\right)^{2} + 1\\left(\\tfrac{3}{2}\\right)^{2} + 3\\left(\\tfrac{1}{2}\\right)^{2}\n= \\tfrac{32}{9} + \\tfrac{16}{9} + \\tfrac{9}{4} + \\tfrac{3}{4}.\n$$\nCombine terms:\n$$\n\\tfrac{32}{9} + \\tfrac{16}{9} = \\tfrac{48}{9} = \\tfrac{16}{3}, \\quad\n\\tfrac{9}{4} + \\tfrac{3}{4} = \\tfrac{12}{4} = 3,\n$$\nso\n$$\nx^{\\star \\top} H x^{\\star} = \\tfrac{16}{3} + 3 = \\tfrac{25}{3}.\n$$\nTherefore,\n$$\nf(x^{\\star}) = \\tfrac{1}{2}\\,x^{\\star \\top} H x^{\\star} = \\tfrac{1}{2} \\cdot \\tfrac{25}{3} = \\tfrac{25}{6}.\n$$\nBecause $Z^{\\top} H Z$ is positive definite, this minimizer is unique and the computed value is the unique optimal objective value.",
            "answer": "$$\\boxed{\\tfrac{25}{6}}$$"
        },
        {
            "introduction": "This advanced practice bridges the gap from solving single quadratic programs to developing general-purpose algorithms for nonlinear optimization. You are tasked with implementing a null-space variant of the BFGS quasi-Newton method, a powerful iterative technique used in modern solvers . By building this algorithm, you will see how the core concepts of the reduced gradient and an evolving approximation of the reduced Hessian are used to efficiently navigate the feasible set and find a solution.",
            "id": "3158303",
            "problem": "You are asked to design and implement a null-space variant of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method that enforces linear equality constraints by operating entirely within the null space. Consider the equality-constrained smooth optimization problem: minimize a differentiable objective function $f(x)$ subject to a set of linear equality constraints $A x = b$, where $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^n$, and $b \\in \\mathbb{R}^m$. The null-space method parameterizes feasible steps using any matrix $Z \\in \\mathbb{R}^{n \\times k}$ whose columns form a basis of the null space $\\mathcal{N}(A) = \\{d \\in \\mathbb{R}^n : A d = 0\\}$, i.e., $A Z = 0$, with $k = n - \\operatorname{rank}(A)$. A feasible iterate is always of the form $x = x_{\\mathrm{feas}} + Z z$, where $x_{\\mathrm{feas}}$ is any point that satisfies $A x_{\\mathrm{feas}} = b$. Search directions should be confined to $\\operatorname{Range}(Z)$ to preserve feasibility. Your null-space BFGS variant must maintain a symmetric positive definite inverse Hessian approximation $H_k \\in \\mathbb{R}^{k \\times k}$ in the reduced coordinates and perform updates that satisfy a secant condition expressed within the null space.\n\nFundamental principles you may use:\n- Feasible directions for $A x = b$ satisfy $A d = 0$, hence lie in $\\mathcal{N}(A)$.\n- If $Z$ is a basis of $\\mathcal{N}(A)$, then any feasible step is $s = Z p$ for some $p \\in \\mathbb{R}^k$.\n- The reduced gradient is $g_z = Z^\\top \\nabla f(x)$.\n- The Broyden-Fletcher-Goldfarb-Shanno (BFGS) method is a well-tested quasi-Newton method that updates either a Hessian or an inverse Hessian approximation to satisfy a suitable secant condition, while preserving symmetry and positive definiteness under standard curvature conditions.\n\nYour tasks:\n- Derive a null-space BFGS algorithm from these principles. Your algorithm should:\n  - Construct a basis $Z$ for $\\mathcal{N}(A)$ using a numerically stable procedure.\n  - Compute a feasible starting point $x_0$ by projecting a given initial guess onto the affine subspace $\\{x : A x = b\\}$.\n  - At each iteration, compute the reduced gradient $g_z = Z^\\top \\nabla f(x_k)$, a reduced-space search direction $p_k$ using the current inverse Hessian approximation $H_k$, and a full-space step $s_k = Z p_k$ that preserves feasibility. Use a backtracking line search with the Armijo condition to select a step size $\\alpha_k \\in (0,1]$.\n  - Update $H_k$ in the reduced space using a BFGS-type update that enforces the reduced-space secant condition, provided a standard curvature condition holds.\n  - Terminate when the norm of the reduced gradient $\\lVert g_z \\rVert_2$ is below a tolerance.\n  - After termination, compute and report three quantitative diagnostics for each test case:\n    $1)$ feasibility error $\\lVert A x_{\\mathrm{final}} - b \\rVert_2$,\n    $2)$ projected-stationarity error $\\lVert Z^\\top \\nabla f(x_{\\mathrm{final}}) \\rVert_2$,\n    $3)$ the reduced-space secant residual for the final successful update, measured as the relative residual of the inverse-form secant condition $H_{k+1} y_k = s_k$ in reduced coordinates, i.e., $\\frac{\\lVert H_{k+1} y_k - s_k \\rVert_2}{\\lVert s_k \\rVert_2 + \\lVert y_k \\rVert_2}$, where $s_k$ and $y_k$ are the last reduced-space step and gradient difference, respectively. If no update occurred, use the value corresponding to zero residual.\n\nImplementation requirements:\n- Use a numerically stable method to compute a basis $Z$ of $\\mathcal{N}(A)$, such as one based on singular value decomposition, and handle edge cases including $m = 0$ (no constraints) and rank-deficient $A$.\n- Use a backtracking Armijo line search with Armijo constant $c_1 = 10^{-4}$ and contraction factor $\\tau = 1/2$.\n- The algorithm must handle the following test suite. For each case, start from the given initial $x_{\\mathrm{init}}$ and project it to a feasible point before iterating.\n\nTest suite:\n- Case $1$ (quadratic objective, full-row-rank constraint):\n  - Dimension: $n = 3$, $m = 1$.\n  - $Q = \\begin{bmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}$, $c = \\begin{bmatrix} -1 \\\\ 2 \\\\ 0.5 \\end{bmatrix}$, $f(x) = \\tfrac{1}{2} x^\\top Q x + c^\\top x$.\n  - $A = \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\end{bmatrix}$.\n  - $x_{\\mathrm{init}} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n- Case $2$ (smooth nonquadratic objective, no constraints):\n  - Dimension: $n = 4$, $m = 0$.\n  - Rows $v_i^\\top$ of $V \\in \\mathbb{R}^{5 \\times 4}$ are\n    $\\begin{bmatrix}\n    1 & 0 & -1 & 2 \\\\\n    -2 & 1 & 0 & 1 \\\\\n    0.5 & -1 & 1 & -1 \\\\\n    1.5 & 0.5 & -0.5 & 0 \\\\\n    -0.5 & 2 & 1 & -1.5\n    \\end{bmatrix}$.\n  - $f(x) = \\sum_{i=1}^{5} \\log\\big(1 + \\exp(v_i^\\top x)\\big) + 0.05 \\lVert x \\rVert_2^2$.\n  - $A$ has shape $0 \\times 4$ and $b$ has shape $0 \\times 1$.\n  - $x_{\\mathrm{init}} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n- Case $3$ (quadratic objective, redundant constraints with rank deficiency):\n  - Dimension: $n = 3$, $m = 2$.\n  - $Q = \\begin{bmatrix} 6 & 2 & 1 \\\\ 2 & 5 & 0 \\\\ 1 & 0 & 3 \\end{bmatrix}$, $c = \\begin{bmatrix} 0 \\\\ -1 \\\\ 0 \\end{bmatrix}$, $f(x) = \\tfrac{1}{2} x^\\top Q x + c^\\top x$.\n  - $A = \\begin{bmatrix} 1 & 1 & 0 \\\\ 2 & 2 & 0 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$.\n  - $x_{\\mathrm{init}} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$.\n\nNumerical tolerances and stopping:\n- Terminate when $\\lVert Z^\\top \\nabla f(x_k) \\rVert_2 \\le 10^{-8}$ or after at most $200$ iterations, whichever occurs first.\n- For the BFGS update in reduced space, only perform the update when the curvature condition $s_k^\\top y_k > 10^{-12}$ holds; otherwise skip the update.\n\nRequired final output format:\n- For each case, compute the triplet of floats $[\\lVert A x_{\\mathrm{final}} - b \\rVert_2, \\lVert Z^\\top \\nabla f(x_{\\mathrm{final}}) \\rVert_2, \\text{secant-residual}]$. Round each float to $8$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of these triplets, enclosed in square brackets; that is, a list of lists of length $3$. For example, a valid output shape is $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$, where each $a_i$, $b_i$, $c_i$ is a float rounded to $8$ decimals.",
            "solution": "The user has provided a valid problem statement. The task is to derive and implement a null-space Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm for solving equality-constrained optimization problems of the form:\n$$\n\\min_{x \\in \\mathbb{R}^n} f(x) \\quad \\text{subject to} \\quad A x = b\n$$\nwhere $f: \\mathbb{R}^n \\to \\mathbb{R}$ is a continuously differentiable function, $A \\in \\mathbb{R}^{m \\times n}$, and $b \\in \\mathbb{R}^m$.\n\n### Principle: Null-Space Decomposition and Reduced Problem\nThe core of the null-space method is to transform the constrained optimization problem in $\\mathbb{R}^n$ into an unconstrained problem in a lower-dimensional space. This is achieved by parameterizing the feasible set $\\{x \\in \\mathbb{R}^n : A x = b\\}$.\n\nAny feasible point $x$ can be expressed as the sum of a particular solution $x_p$ (any vector satisfying $A x_p = b$) and a vector from the null-space of $A$, $\\mathcal{N}(A) = \\{d \\in \\mathbb{R}^n : A d = 0\\}$.\nLet $Z \\in \\mathbb{R}^{n \\times k}$ be a matrix whose columns form a basis for $\\mathcal{N}(A)$. The dimension of the null-space is $k = n - \\operatorname{rank}(A)$. Any vector $d \\in \\mathcal{N}(A)$ can be written as $d = Z z$ for some unique $z \\in \\mathbb{R}^k$.\n\nThus, any feasible solution can be written as:\n$$\nx(z) = x_p + Z z\n$$\nSubstituting this into the objective function, we obtain an unconstrained optimization problem in the reduced coordinates $z \\in \\mathbb{R}^k$:\n$$\n\\min_{z \\in \\mathbb{R}^k} \\phi(z) = f(x_p + Z z)\n$$\n\nThe gradient of this reduced objective function $\\phi(z)$ is found using the chain rule:\n$$\n\\nabla_z \\phi(z) = Z^\\top \\nabla_x f(x_p + Z z)\n$$\nThis vector, $g_z = Z^\\top \\nabla f(x)$, is known as the reduced gradient. The first-order necessary condition for optimality is $\\nabla_z \\phi(z) = 0$, which is equivalent to the projected stationarity condition $Z^\\top \\nabla f(x) = 0$.\n\nThe Hessian of the reduced objective function is:\n$$\n\\nabla_z^2 \\phi(z) = Z^\\top \\left( \\nabla_x^2 f(x_p + Z z) \\right) Z\n$$\nThis is the reduced Hessian. The null-space BFGS method avoids computing this matrix directly, instead building a quasi-Newton approximation of its inverse.\n\n### The Null-Space BFGS Algorithm\n\nWe apply the BFGS algorithm to the unconstrained minimization of $\\phi(z)$. This involves iteratively computing a search direction in the reduced space and updating an approximation to the inverse of the reduced Hessian.\n\n**1. Initialization**\n-   **Null-Space Basis $Z$**: A numerically stable method to compute an orthonormal basis for $\\mathcal{N}(A)$ is the Singular Value Decomposition (SVD). Given $A = U S V^\\top$, the last $n - r$ columns of $V$, where $r = \\operatorname{rank}(A)$, form the basis $Z$. This handles both full-rank and rank-deficient cases for $A$. If there are no constraints ($m=0$), $\\mathcal{N}(A) = \\mathbb{R}^n$ and we can set $Z = I_n$.\n-   **Feasible Starting Point $x_0$**: Given an initial guess $x_{\\mathrm{init}}$, we find the closest feasible point $x_0$ by projecting $x_{\\mathrm{init}}$ onto the affine subspace $\\{x : Ax=b\\}$. This is the solution to $\\min \\|x - x_{\\mathrm{init}}\\|_2^2$ subject to $Ax=b$. The solution is $x_0 = x_{\\mathrm{init}} + A^+ (b - A x_{\\mathrm{init}})$, where $A^+$ is the Moore-Penrose pseudoinverse of $A$.\n-   **Inverse Hessian Approximation $H_0$**: We initialize the approximation of the inverse reduced Hessian as the identity matrix, $H_0 = I_k \\in \\mathbb{R}^{k \\times k}$.\n\n**2. Iterative Procedure**\nAt each iteration $k$ with current iterate $x_k$:\n1.  **Compute Reduced Gradient**: Calculate the full gradient $g_k = \\nabla f(x_k)$ and then the reduced gradient $g_{z,k} = Z^\\top g_k$.\n2.  **Check for Convergence**: The algorithm terminates if the norm of the reduced gradient is below a specified tolerance, $\\|g_{z,k}\\|_2 \\le \\epsilon_{\\mathrm{grad}}$. This corresponds to the projected stationarity condition.\n3.  **Compute Reduced-Space Search Direction**: A quasi-Newton step is taken in the reduced space:\n    $$\n    p_k = -H_k g_{z,k}\n    $$\n    Since $H_k$ is maintained to be positive definite, $p_k$ defines a descent direction for $\\phi(z)$.\n4.  **Line Search**: The full-space search direction is $d_k = Z p_k$. By construction, $A d_k = A Z p_k = 0$, so feasibility is maintained for any step size $\\alpha > 0$. We find a suitable step size $\\alpha_k$ using a backtracking line search that satisfies the Armijo condition:\n    $$\n    f(x_k + \\alpha_k d_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top d_k\n    $$\n    where $c_1 \\in (0, 1)$ is a constant. Note that $g_k^\\top d_k = g_k^\\top Z p_k = g_{z,k}^\\top p_k$.\n5.  **Update Iterate**: The next iterate is $x_{k+1} = x_k + \\alpha_k d_k$.\n6.  **Update Inverse Hessian Approximation**: The BFGS update is performed in the reduced space.\n    -   Compute the step in reduced coordinates: $s_{z,k} = \\alpha_k p_k$.\n    -   Compute the change in the reduced gradient: $y_{z,k} = g_{z,k+1} - g_{z,k} = Z^\\top(\\nabla f(x_{k+1}) - \\nabla f(x_k))$.\n    -   **Curvature Condition**: To ensure the updated Hessian approximation remains positive definite, the update is only performed if $s_{z,k}^\\top y_{z,k} > \\epsilon_{\\mathrm{curv}}$ for some small $\\epsilon_{\\mathrm{curv}} > 0$.\n    -   **BFGS Update Formula**: If the curvature condition holds, with $\\rho_k = 1 / (y_{z,k}^\\top s_{z,k})$, the update is:\n        $$\n        H_{k+1} = (I - \\rho_k s_{z,k} y_{z,k}^\\top) H_k (I - \\rho_k y_{z,k} s_{z,k}^\\top) + \\rho_k s_{z,k} s_{z,k}^\\top\n        $$\n    -   If the condition fails, we skip the update: $H_{k+1} = H_k$.\n\n**3. Required Diagnostic Metrics**\nUpon termination, the following quantities are reported:\n1.  **Feasibility Error**: The norm of the constraint violation, $\\|\\boldsymbol{A} x_{\\mathrm{final}} - \\boldsymbol{b}\\|_2$. This should be close to machine precision due to the construction of the algorithm.\n2.  **Projected-Stationarity Error**: The norm of the final reduced gradient, $\\|Z^\\top \\nabla f(x_{\\mathrm{final}})\\|_2$. This is the primary measure of convergence to a stationary point.\n3.  **Secant Residual**: For the last successful BFGS update, this measures how well the new inverse Hessian approximation $H_{k+1}$ satisfies the reduced-space secant equation $H_{k+1} y_k = s_k$. It is calculated as the relative residual:\n    $$\n    \\frac{\\| H_{k+1} y_k - s_k \\|_2}{\\| s_k \\|_2 + \\| y_k \\|_2}\n    $$\n    where $s_k$ and $y_k$ are the final reduced-space step and gradient difference vectors that triggered an update.\n\nThis completes the derivation of a robust algorithm for equality-constrained optimization, which will be implemented as specified.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import expit\n\ndef get_null_space_basis(A, tol=1e-10):\n    \"\"\"\n    Computes an orthonormal basis for the null-space of A using SVD.\n    \"\"\"\n    m, n = A.shape\n    if m == 0:\n        return np.eye(n)\n    \n    # SVD: A = U S Vh. The columns of V corresponding to zero singular values\n    # form a basis for N(A). These are the last n-r columns of V, where\n    # r is the rank. Vh is V.T, so we take the last n-r rows of Vh and transpose.\n    try:\n        _, s, Vh = np.linalg.svd(A, full_matrices=True)\n        rank = np.sum(s > tol)\n        Z = Vh[rank:].T\n        return Z\n    except np.linalg.LinAlgError:\n        # This fallback should rarely be needed with robust SVD implementations\n        if m == 0:\n            return np.eye(n)\n        else:\n            raise\n\ndef null_space_bfgs(f, grad_f, A, b, x_init, max_iter=200, grad_tol=1e-8, ls_c1=1e-4, ls_tau=0.5, curv_tol=1e-12):\n    \"\"\"\n    Implements the null-space BFGS method for equality-constrained optimization.\n    \"\"\"\n    m, n = A.shape\n\n    # 1. Construct null-space basis Z\n    Z = get_null_space_basis(A)\n    k_dim = Z.shape[1]\n\n    # 2. Compute a feasible starting point x_0 by projection\n    if m == 0:\n        x = x_init.copy()\n    else:\n        # Project x_init onto the feasible affine subspace {x | Ax=b}\n        # x = x_init + A_pinv * (b - A @ x_init)\n        b_vec = b.flatten()\n        # np.linalg.pinv is robust and handles rank-deficient A\n        x = x_init + np.linalg.pinv(A) @ (b_vec - A @ x_init)\n\n    # Initialize inverse Hessian approximation and diagnostics\n    H = np.eye(k_dim)\n    last_secant_residual = 0.0\n\n    # Main iteration loop\n    for _ in range(max_iter):\n        grad = grad_f(x)\n        g_z = Z.T @ grad\n        \n        # Check termination condition\n        norm_gz = np.linalg.norm(g_z)\n        if norm_gz <= grad_tol:\n            break\n\n        # Compute search direction in reduced space\n        p = -H @ g_z\n\n        # Backtracking line search\n        alpha = 1.0\n        f_x = f(x)\n        # Directional derivative: g_z.T @ p must be < 0 for a descent direction\n        dir_deriv = g_z.T @ p\n\n        while True:\n            x_next_trial = x + alpha * (Z @ p)\n            try:\n                f_next_trial = f(x_next_trial)\n            except (ValueError, FloatingPointError):\n                # In case of numerical issues (e.g., log of non-positive)\n                f_next_trial = np.inf\n\n            if f_next_trial <= f_x + ls_c1 * alpha * dir_deriv:\n                break\n            \n            alpha *= ls_tau\n            if alpha < 1e-16: # Prevent indefinite looping\n                break\n        \n        if alpha < 1e-16: # Line search failed, terminate\n            break\n\n        # Update iterate\n        s_full = alpha * (Z @ p)\n        x_next = x + s_full\n\n        # Prepare for BFGS update in reduced space\n        s_reduced = alpha * p\n        grad_next = grad_f(x_next)\n        g_z_next = Z.T @ grad_next\n        y_reduced = g_z_next - g_z\n\n        # Curvature condition and BFGS update\n        sTy = s_reduced.T @ y_reduced\n        if sTy > curv_tol:\n            rho = 1.0 / sTy\n            I = np.eye(k_dim)\n            s_col = s_reduced.reshape(-1, 1)\n            y_col = y_reduced.reshape(-1, 1)\n            \n            term1 = I - rho * s_col @ y_col.T\n            term2 = I - rho * y_col @ s_col.T\n            H_next = term1 @ H @ term2 + rho * s_col @ s_col.T\n\n            # Compute secant residual for diagnostics\n            num_vec = H_next @ y_reduced - s_reduced\n            den = np.linalg.norm(s_reduced) + np.linalg.norm(y_reduced)\n            if den > 1e-15:\n                last_secant_residual = np.linalg.norm(num_vec) / den\n            else: # s and y are zero, so is the numerator. Residual is 0.\n                last_secant_residual = 0.0\n\n            H = H_next\n        \n        x = x_next\n    \n    # Compute final diagnostic metrics\n    x_final = x\n\n    # 1. Feasibility error\n    if m > 0:\n        feasibility_error = np.linalg.norm(A @ x_final - b.flatten())\n    else:\n        feasibility_error = 0.0\n        \n    # 2. Projected-stationarity error\n    final_grad = grad_f(x_final)\n    final_gz = Z.T @ final_grad\n    stationarity_error = np.linalg.norm(final_gz)\n\n    # 3. Last secant residual (already stored)\n    \n    return [feasibility_error, stationarity_error, last_secant_residual]\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the null-space BFGS algorithm.\n    \"\"\"\n    \n    # Case 1 (quadratic, full-row-rank constraint)\n    Q1 = np.array([[4., 1., 0.], [1., 3., 0.], [0., 0., 2.]])\n    c1 = np.array([-1., 2., 0.5])\n    f_1 = lambda x: 0.5 * x.T @ Q1 @ x + c1.T @ x\n    grad_f_1 = lambda x: Q1 @ x + c1\n    A1 = np.array([[1., 1., 1.]])\n    b1 = np.array([[1.]])\n    x_init1 = np.array([0., 0., 0.])\n    \n    # Case 2 (non-quadratic, no constraints)\n    V2 = np.array([\n        [1., 0., -1., 2.],\n        [-2., 1., 0., 1.],\n        [0.5, -1., 1., -1.],\n        [1.5, 0.5, -0.5, 0.],\n        [-0.5, 2., 1., -1.5]\n    ])\n    def f_2(x):\n        z = V2 @ x\n        # Numerically stable calculation for sum of log(1 + exp(z))\n        stable_f = np.sum(np.maximum(z, 0) + np.log(1 + np.exp(-np.abs(z))))\n        return stable_f + 0.05 * np.linalg.norm(x)**2\n    def grad_f_2(x):\n        z = V2 @ x\n        # expit is the numerically stable sigmoid function, 1/(1+exp(-z))\n        sigmoids = expit(z)\n        return V2.T @ sigmoids + 0.1 * x\n    A2 = np.empty((0, 4))\n    b2 = np.empty((0, 1))\n    x_init2 = np.array([0., 0., 0., 0.])\n\n    # Case 3 (quadratic, rank-deficient constraints)\n    Q3 = np.array([[6., 2., 1.], [2., 5., 0.], [1., 0., 3.]])\n    c3 = np.array([0., -1., 0.])\n    f_3 = lambda x: 0.5 * x.T @ Q3 @ x + c3.T @ x\n    grad_f_3 = lambda x: Q3 @ x + c3\n    A3 = np.array([[1., 1., 0.], [2., 2., 0.]])\n    b3 = np.array([[1.], [2.]])\n    x_init3 = np.array([0., 1., 0.])\n\n    test_cases = [\n        (f_1, grad_f_1, A1, b1, x_init1),\n        (f_2, grad_f_2, A2, b2, x_init2),\n        (f_3, grad_f_3, A3, b3, x_init3)\n    ]\n    \n    all_results = []\n    for f, grad_f, A, b, x_init in test_cases:\n        case_result = null_space_bfgs(f, grad_f, A, b, x_init)\n        # Round to 8 decimal places as required for final output\n        all_results.append([round(v, 8) for v in case_result])\n        \n    # Format the output string exactly as specified without extra spaces.\n    inner_parts = [f\"[{','.join(f'{v:.8f}' for v in r)}]\" for r in all_results]\n    print(f\"[{','.join(inner_parts)}]\")\n\nsolve()\n```"
        }
    ]
}