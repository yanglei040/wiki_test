{
    "hands_on_practices": [
        {
            "introduction": "Mastering any new method begins with a solid grasp of its fundamental components. This first exercise provides a direct, hands-on opportunity to construct the range-space reduced system from first principles. By working through a concrete example with specific matrices, you will solidify your understanding of how the Hessian ($H$) and constraint matrix ($A$) combine to form the crucial matrix $S = AH^{-1}A^{\\top}$.",
            "id": "3171154",
            "problem": "Consider the equality-constrained quadratic program with objective function $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^{\\top}H\\mathbf{x}$ and equality constraints $A\\mathbf{x} = \\mathbf{b}$, where $H \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and $A \\in \\mathbb{R}^{m \\times n}$ has full row rank. In the range-space method, one eliminates the primal variable to obtain a reduced system for the Lagrange multipliers whose coefficient matrix is denoted by $S$.\n\nConstruct a toy instance where $n = 5$ and $m = 2$ with a diagonally scaled Hessian $H$ and a constraints matrix $A$ that forms sums of components, specifically:\n- Let $H = \\mathrm{diag}(1,\\,0.1,\\,10,\\,2,\\,0.5)$.\n- Let $A \\in \\mathbb{R}^{2 \\times 5}$ have its first row equal to $[1,\\,1,\\,1,\\,0,\\,0]$ and its second row equal to $[0,\\,0,\\,1,\\,1,\\,1]$.\n\nStarting from first principles for equality-constrained quadratic optimization, derive the reduced multiplier system and compute the determinant of the matrix $S$ for this instance. Your final answer must be a single real-valued number. Provide the exact value; do not round.",
            "solution": "The problem is a standard equality-constrained quadratic program (QP), which is deemed valid as it is scientifically grounded, well-posed, and all necessary information to solve for the determinant of the matrix $S$ is provided.\n\nThe optimization problem is formulated as:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad  f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^{\\top}H\\mathbf{x} \\\\\n\\text{subject to} \\quad  A\\mathbf{x} = \\mathbf{b}\n\\end{aligned}\n$$\nwhere $\\mathbf{x} \\in \\mathbb{R}^n$, $H \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, $A \\in \\mathbb{R}^{m \\times n}$ has full row rank, and $\\mathbf{b} \\in \\mathbb{R}^m$.\n\nThe Lagrangian function for this problem is:\n$$ \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}) = \\frac{1}{2}\\mathbf{x}^{\\top}H\\mathbf{x} + \\boldsymbol{\\lambda}^{\\top}(A\\mathbf{x} - \\mathbf{b}) $$\nwhere $\\boldsymbol{\\lambda} \\in \\mathbb{R}^m$ is the vector of Lagrange multipliers.\n\nThe first-order necessary conditions for optimality (Karush-Kuhn-Tucker or KKT conditions) are obtained by setting the gradients of the Lagrangian with respect to $\\mathbf{x}$ and $\\boldsymbol{\\lambda}$ to zero:\n$$ \\nabla_{\\mathbf{x}} \\mathcal{L} = H\\mathbf{x} + A^{\\top}\\boldsymbol{\\lambda} = \\mathbf{0} $$\n$$ \\nabla_{\\boldsymbol{\\lambda}} \\mathcal{L} = A\\mathbf{x} - \\mathbf{b} = \\mathbf{0} $$\nThis is a system of linear equations often written in matrix form as:\n$$ \\begin{pmatrix} H  A^{\\top} \\\\ A  0 \\end{pmatrix} \\begin{pmatrix} \\mathbf{x} \\\\ \\boldsymbol{\\lambda} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{0} \\\\ \\mathbf{b} \\end{pmatrix} $$\n\nThe range-space method eliminates the primal variable $\\mathbf{x}$ to solve for the dual variable $\\boldsymbol{\\lambda}$ first. From the first KKT equation, $H\\mathbf{x} = -A^{\\top}\\boldsymbol{\\lambda}$. Since $H$ is given as symmetric positive definite, it is invertible. We can express $\\mathbf{x}$ as:\n$$ \\mathbf{x} = -H^{-1}A^{\\top}\\boldsymbol{\\lambda} $$\nSubstituting this expression for $\\mathbf{x}$ into the second KKT equation (the feasibility constraint) yields:\n$$ A(-H^{-1}A^{\\top}\\boldsymbol{\\lambda}) - \\mathbf{b} = \\mathbf{0} $$\n$$ -(AH^{-1}A^{\\top})\\boldsymbol{\\lambda} = \\mathbf{b} $$\nThis can be rewritten as a reduced system for the Lagrange multipliers:\n$$ (AH^{-1}A^{\\top})\\boldsymbol{\\lambda} = -\\mathbf{b} $$\nThe coefficient matrix of this reduced system is denoted by $S$. Therefore,\n$$ S = AH^{-1}A^{\\top} $$\nThe problem requires us to compute the determinant of this matrix, $\\det(S)$, for the given instance.\n\nThe given matrices are:\n- $H = \\mathrm{diag}(1,\\,0.1,\\,10,\\,2,\\,0.5)$. We can write the decimal values as fractions for exact computation: $H = \\mathrm{diag}(1, \\frac{1}{10}, 10, 2, \\frac{1}{2})$.\n- $A \\in \\mathbb{R}^{2 \\times 5}$ given by $A = \\begin{pmatrix} 1  1  1  0  0 \\\\ 0  0  1  1  1 \\end{pmatrix}$.\n\nFirst, we compute the inverse of $H$. Since $H$ is a diagonal matrix, its inverse $H^{-1}$ is a diagonal matrix whose diagonal elements are the reciprocals of the corresponding elements of $H$:\n$$ H^{-1} = \\mathrm{diag}(1^{-1}, (\\frac{1}{10})^{-1}, 10^{-1}, 2^{-1}, (\\frac{1}{2})^{-1}) = \\mathrm{diag}(1, 10, \\frac{1}{10}, \\frac{1}{2}, 2) $$\nNumerically, this is $H^{-1} = \\mathrm{diag}(1, 10, 0.1, 0.5, 2)$.\n\nNext, we compute the transpose of $A$:\n$$ A^{\\top} = \\begin{pmatrix} 1  0 \\\\ 1  0 \\\\ 1  1 \\\\ 0  1 \\\\ 0  1 \\end{pmatrix} $$\n\nNow, we can compute the matrix $S = AH^{-1}A^{\\top}$. Let us compute the product $H^{-1}A^{\\top}$ first:\n$$ H^{-1}A^{\\top} = \\begin{pmatrix} 1  0  0  0  0 \\\\ 0  10  0  0  0 \\\\ 0  0  \\frac{1}{10}  0  0 \\\\ 0  0  0  \\frac{1}{2}  0 \\\\ 0  0  0  0  2 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 1  0 \\\\ 1  1 \\\\ 0  1 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1  1 \\cdot 0 \\\\ 10 \\cdot 1  10 \\cdot 0 \\\\ \\frac{1}{10} \\cdot 1  \\frac{1}{10} \\cdot 1 \\\\ \\frac{1}{2} \\cdot 0  \\frac{1}{2} \\cdot 1 \\\\ 2 \\cdot 0  2 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 10  0 \\\\ \\frac{1}{10}  \\frac{1}{10} \\\\ 0  \\frac{1}{2} \\\\ 0  2 \\end{pmatrix} $$\n\nFinally, we pre-multiply this result by $A$ to obtain $S$:\n$$ S = A(H^{-1}A^{\\top}) = \\begin{pmatrix} 1  1  1  0  0 \\\\ 0  0  1  1  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 10  0 \\\\ \\frac{1}{10}  \\frac{1}{10} \\\\ 0  \\frac{1}{2} \\\\ 0  2 \\end{pmatrix} $$\nThe resulting matrix $S$ is a $2 \\times 2$ matrix. Its elements are:\n$$ S_{11} = (1)(1) + (1)(10) + (1)(\\frac{1}{10}) + (0)(0) + (0)(0) = 1 + 10 + \\frac{1}{10} = 11 + \\frac{1}{10} = \\frac{111}{10} $$\n$$ S_{12} = (1)(0) + (1)(0) + (1)(\\frac{1}{10}) + (0)(\\frac{1}{2}) + (0)(2) = \\frac{1}{10} $$\n$$ S_{21} = (0)(1) + (0)(10) + (1)(\\frac{1}{10}) + (1)(0) + (1)(0) = \\frac{1}{10} $$\n$$ S_{22} = (0)(0) + (0)(0) + (1)(\\frac{1}{10}) + (1)(\\frac{1}{2}) + (1)(2) = \\frac{1}{10} + \\frac{5}{10} + \\frac{20}{10} = \\frac{26}{10} $$\nSo the matrix $S$ is:\n$$ S = \\begin{pmatrix} \\frac{111}{10}  \\frac{1}{10} \\\\ \\frac{1}{10}  \\frac{26}{10} \\end{pmatrix} $$\nThe determinant of a $2 \\times 2$ matrix $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$ is $ad - bc$. Thus, the determinant of $S$ is:\n$$ \\det(S) = \\left(\\frac{111}{10}\\right)\\left(\\frac{26}{10}\\right) - \\left(\\frac{1}{10}\\right)\\left(\\frac{1}{10}\\right) $$\n$$ \\det(S) = \\frac{111 \\times 26}{100} - \\frac{1}{100} $$\nLet's compute the product $111 \\times 26$:\n$$ 111 \\times 26 = 111 \\times (20 + 6) = 2220 + 666 = 2886 $$\nSubstituting this back into the determinant expression:\n$$ \\det(S) = \\frac{2886}{100} - \\frac{1}{100} = \\frac{2885}{100} = 28.85 $$\nThe determinant is an exact real-valued number.",
            "answer": "$$\\boxed{28.85}$$"
        },
        {
            "introduction": "A powerful algorithm is not just correct, but also numerically robust. This practice explores the crucial concept of conditioning in the context of range-space methods, investigating how the properties of the Hessian $H$ translate to the reduced system matrix $S$. Through a carefully designed thought experiment, you will discover the surprising but vital role that the alignment between constraints and curvature plays in determining the numerical stability of the solution process.",
            "id": "3171127",
            "problem": "Consider the equality-constrained quadratic program minimize $f(x) = \\tfrac{1}{2} x^{\\top} H x$ subject to $A x = b$, where $H \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD) and $A \\in \\mathbb{R}^{m \\times n}$ has full row rank. The range-space method reduces the Karush–Kuhn–Tucker (KKT) system to a linear system in the Lagrange multipliers. Starting only from the KKT optimality conditions and the definition of the two-norm condition number, carry out this reduction to obtain the reduced system and its system matrix. Then evaluate the conditioning of this reduced system for the following specific construction, which is designed so that $H$ is ill-conditioned but the reduced system matrix is well-conditioned due to alignment of the rows of $A$ with the dominant eigenvectors of $H$.\n\nConstruction: Let $n = 3$, $m = 2$, and choose\n$$\nH = \\begin{pmatrix}\n10^{6}  0  0 \\\\\n0  10^{6}  0 \\\\\n0  0  1\n\\end{pmatrix}, \\quad\nA = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0\n\\end{pmatrix}, \\quad\nb \\in \\mathbb{R}^{2} \\text{ arbitrary.}\n$$\nYou may assume $H$ is SPD and that the two-norm condition number of an SPD matrix $M$ is $\\kappa_{2}(M) = \\lambda_{\\max}(M) / \\lambda_{\\min}(M)$, where $\\lambda_{\\max}(M)$ and $\\lambda_{\\min}(M)$ are the largest and smallest eigenvalues of $M$. No other specialized formulas may be used without derivation.\n\nTasks:\n1. Starting from the KKT conditions, eliminate the primal variable to obtain the reduced linear system in the multipliers and identify its system matrix.\n2. Using the construction above, compute the two-norm condition number of the reduced system matrix and explain, using eigenvector alignment, why it is well-conditioned even though $H$ is ill-conditioned.\n\nWhat is the value of the two-norm condition number of the reduced system matrix for this construction? Provide an exact value. Do not round. Express your final answer as a real number with no units.",
            "solution": "The problem asks for the derivation of the reduced system matrix for an equality-constrained quadratic program and the calculation of its condition number for a specific construction.\n\n### Step 1: Derivation of the Reduced System\n\nThe problem is an equality-constrained quadratic program (EQP) of the form:\n$$\n\\text{minimize } f(x) = \\frac{1}{2} x^{\\top} H x \\quad \\text{subject to } Ax = b\n$$\nwhere $x \\in \\mathbb{R}^{n}$, $H \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD), $A \\in \\mathbb{R}^{m \\times n}$ has full row rank ($m \\le n$), and $b \\in \\mathbb{R}^{m}$.\n\nThe Lagrangian function for this problem is:\n$$\nL(x, \\lambda) = f(x) + \\lambda^{\\top}(Ax - b) = \\frac{1}{2} x^{\\top} H x + \\lambda^{\\top}Ax - \\lambda^{\\top}b\n$$\nwhere $\\lambda \\in \\mathbb{R}^{m}$ is the vector of Lagrange multipliers.\n\nThe first-order Karush–Kuhn–Tucker (KKT) optimality conditions are found by setting the gradients of the Lagrangian with respect to $x$ and $\\lambda$ to zero:\n1.  $\\nabla_{x} L(x, \\lambda) = Hx + A^{\\top}\\lambda = 0$ (Stationarity)\n2.  $\\nabla_{\\lambda} L(x, \\lambda) = Ax - b = 0$ (Primal Feasibility)\n\nThese two conditions form a block linear system, known as the KKT system:\n$$\n\\begin{pmatrix}\nH  A^{\\top} \\\\\nA  0\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\\n\\lambda\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\\nb\n\\end{pmatrix}\n$$\nThe range-space method involves eliminating the primal variable $x$ to obtain a smaller system solely in terms of the dual variable $\\lambda$.\n\nFrom the stationarity condition, $Hx + A^{\\top}\\lambda = 0$, we can express $x$ in terms of $\\lambda$. Since $H$ is given as SPD, it is invertible. Therefore:\n$$\nHx = -A^{\\top}\\lambda\n$$\n$$\nx = -H^{-1}A^{\\top}\\lambda\n$$\nNow, substitute this expression for $x$ into the primal feasibility condition, $Ax = b$:\n$$\nA(-H^{-1}A^{\\top}\\lambda) = b\n$$\n$$\n-(AH^{-1}A^{\\top})\\lambda = b\n$$\n$$\n(AH^{-1}A^{\\top})\\lambda = -b\n$$\nThis is the reduced linear system in the Lagrange multipliers $\\lambda$. The system matrix, which we denote as $M_R$, is:\n$$\nM_R = AH^{-1}A^{\\top}\n$$\nThis matrix is an $m \\times m$ matrix. It is also SPD, since $H^{-1}$ is SPD (as $H$ is SPD) and $A$ has full row rank. This completes the first task.\n\n### Step 2: Analysis of the Specific Construction\n\nWe are given the following specific matrices for $n=3$ and $m=2$:\n$$\nH = \\begin{pmatrix}\n10^{6}  0  0 \\\\\n0  10^{6}  0 \\\\\n0  0  1\n\\end{pmatrix}, \\quad\nA = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0\n\\end{pmatrix}\n$$\nFirst, we compute the inverse of $H$. Since $H$ is a diagonal matrix, its inverse is the diagonal matrix of the reciprocals of its diagonal entries:\n$$\nH^{-1} = \\begin{pmatrix}\n10^{-6}  0  0 \\\\\n0  10^{-6}  0 \\\\\n0  0  1\n\\end{pmatrix}\n$$\nNext, we compute the reduced system matrix $M_R = AH^{-1}A^{\\top}$. The transpose of $A$ is:\n$$\nA^{\\top} = \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n0  0\n\\end{pmatrix}\n$$\nNow we perform the matrix multiplication:\n$$\nM_R = AH^{-1}A^{\\top} = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0\n\\end{pmatrix}\n\\begin{pmatrix}\n10^{-6}  0  0 \\\\\n0  10^{-6}  0 \\\\\n0  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n0  0\n\\end{pmatrix}\n$$\nFirst, compute the product $AH^{-1}$:\n$$\nAH^{-1} = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0\n\\end{pmatrix}\n\\begin{pmatrix}\n10^{-6}  0  0 \\\\\n0  10^{-6}  0 \\\\\n0  0  1\n\\end{pmatrix} = \\begin{pmatrix}\n10^{-6}  0  0 \\\\\n0  10^{-6}  0\n\\end{pmatrix}\n$$\nNow, multiply by $A^{\\top}$:\n$$\nM_R = (AH^{-1})A^{\\top} = \\begin{pmatrix}\n10^{-6}  0  0 \\\\\n0  10^{-6}  0\n\\end{pmatrix}\n\\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n0  0\n\\end{pmatrix} = \\begin{pmatrix}\n10^{-6}  0 \\\\\n0  10^{-6}\n\\end{pmatrix}\n$$\nSo, the reduced system matrix is $M_R = 10^{-6} I_{2}$, where $I_2$ is the $2 \\times 2$ identity matrix.\n\nTo compute the two-norm condition number $\\kappa_2(M_R)$, we use the formula for an SPD matrix: $\\kappa_2(M_R) = \\lambda_{\\max}(M_R) / \\lambda_{\\min}(M_R)$. The eigenvalues of a diagonal matrix are its diagonal entries. The eigenvalues of $M_R$ are both $10^{-6}$.\n$$\n\\lambda_{\\max}(M_R) = 10^{-6}\n$$\n$$\n\\lambda_{\\min}(M_R) = 10^{-6}\n$$\nTherefore, the condition number is:\n$$\n\\kappa_2(M_R) = \\frac{10^{-6}}{10^{-6}} = 1\n$$\n\n### Explanation of Conditioning\n\nThe conditioning of $H$ itself is poor. The eigenvalues of $H$ are its diagonal entries: $\\lambda_1 = 10^6$, $\\lambda_2 = 10^6$, and $\\lambda_3 = 1$. The condition number of $H$ is:\n$$\n\\kappa_2(H) = \\frac{\\lambda_{\\max}(H)}{\\lambda_{\\min}(H)} = \\frac{10^6}{1} = 10^6\n$$\nA condition number of $10^6$ indicates that $H$ is ill-conditioned. However, the reduced system matrix $M_R$ is perfectly conditioned with $\\kappa_2(M_R) = 1$.\n\nThis favorable outcome is due to the special alignment between the constraint matrix $A$ and the eigenvectors of the Hessian matrix $H$. The eigenvectors of the diagonal matrix $H$ are the standard basis vectors:\n- $v_1 = (1, 0, 0)^{\\top}$ corresponding to $\\lambda = 10^6$.\n- $v_2 = (0, 1, 0)^{\\top}$ corresponding to $\\lambda = 10^6$.\n- $v_3 = (0, 0, 1)^{\\top}$ corresponding to $\\lambda = 1$.\n\nThe rows of the constraint matrix $A$ are $a_1^{\\top} = (1, 0, 0)$ and $a_2^{\\top} = (0, 1, 0)$. The range of $A^{\\top}$, denoted $\\text{range}(A^{\\top})$, is spanned by the columns of $A^{\\top}$, which are precisely the eigenvectors $v_1$ and $v_2$. This subspace, $\\text{span}\\{v_1, v_2\\}$, is the eigenspace of $H$ corresponding to the large eigenvalue $\\lambda=10^6$.\n\nThe matrix $M_R = AH^{-1}A^{\\top}$ effectively measures the action of $H^{-1}$ on the subspace defined by the constraints, which is $\\text{range}(A^{\\top})$. For any vector $y \\in \\mathbb{R}^2$, the vector $A^{\\top}y$ lies in $\\text{range}(A^{\\top})$. Since this subspace is an eigenspace of $H$ (and thus of $H^{-1}$), the action of $H^{-1}$ on any vector $z \\in \\text{range}(A^{\\top})$ is simple scalar multiplication by the corresponding eigenvalue of $H^{-1}$, which is $1/10^6 = 10^{-6}$.\n$$\nH^{-1}(A^{\\top}y) = 10^{-6}(A^{\\top}y)\n$$\nSubstituting this into the expression for $M_R$:\n$$\nM_R y = A (H^{-1} A^{\\top} y) = A(10^{-6} A^{\\top} y) = 10^{-6} (AA^{\\top})y\n$$\nWe compute $AA^{\\top}$:\n$$\nAA^{\\top} = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0\n\\end{pmatrix}\n\\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n0  0\n\\end{pmatrix} = \\begin{pmatrix}\n1  0 \\\\\n0  1\n\\end{pmatrix} = I_2\n$$\nThus, $M_R y = 10^{-6} I_2 y$, which implies $M_R = 10^{-6} I_2$.\n\nThe ill-conditioning of $H$ arises from the vast difference in how it scales vectors in the eigenspace $\\text{span}\\{v_1, v_2\\}$ versus the eigenspace $\\text{span}\\{v_3\\}$. However, the structure of $A$ ensures that the reduced system matrix $M_R$ only \"sees\" the uniform scaling behavior of $H^{-1}$ within the single eigenspace spanned by the rows of $A$. The other dynamic mode of $H^{-1}$ (associated with eigenvalue $1$ and eigenvector $v_3$) is related to the null space of $A$, and its effect is completely filtered out in the product $AH^{-1}A^{\\top}$. As a result, $M_R$ is a scalar multiple of the identity matrix, which is perfectly conditioned.\n\nThe value of the two-norm condition number of the reduced system matrix for this construction is exactly $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Optimization algorithms are often iterative, with the set of active constraints changing from one step to the next. This final practice delves into the dynamic and computational aspects of range-space methods, demonstrating how to efficiently update the inverse of the reduced system matrix when a new constraint is added. By applying block matrix inversion formulas, you will learn a technique that is central to the efficiency of many real-world active-set solvers.",
            "id": "3171093",
            "problem": "In equality-constrained quadratic programming, range-space methods reduce a Karush–Kuhn–Tucker (KKT) system to solving linear systems with the symmetric positive definite matrix $S = A H^{-1} A^{\\top}$, where $H \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, $A \\in \\mathbb{R}^{m \\times n}$ has full row rank, and the constraints are $A x = b$. Consider augmenting the active set by appending one new equality constraint $a^{\\top} x = b_{+}$, forming the augmented matrix $A_{+} \\in \\mathbb{R}^{(m+1) \\times n}$ by stacking $a^{\\top}$ below $A$. Starting from the core definitions of the range-space matrix and standard block matrix identities, derive the expression for the inverse of the augmented range-space matrix $S_{+} = A_{+} H^{-1} A_{+}^{\\top}$ in terms of $S^{-1}$, $A$, $H$, and $a$, and identify the scalar that appears in the lower-right corner of $S_{+}^{-1}$.\n\nThen, for the specific data\n$$\nH = \\operatorname{diag}(2,\\,3,\\,6), \\quad\nA = \\begin{pmatrix}\n1  1  0 \\\\\n0  1  2\n\\end{pmatrix}, \\quad\na = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix},\n$$\ncompute the exact value (no rounding) of the lower-right entry of $S_{+}^{-1}$. Provide your final answer as a single real number with no units.",
            "solution": "The analysis of the problem will be conducted in two parts. First, a general symbolic derivation for the inverse of the augmented range-space matrix, $S_{+}^{-1}$, will be performed. Second, the specific numerical data provided will be used to compute the value of the lower-right entry of $S_{+}^{-1}$.\n\n### Part 1: Symbolic Derivation of $S_{+}^{-1}$\n\nThe problem defines the initial range-space matrix as $S = A H^{-1} A^{\\top}$, where $H \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and $A \\in \\mathbb{R}^{m \\times n}$ has full row rank. These conditions ensure that $S \\in \\mathbb{R}^{m \\times m}$ is symmetric and positive definite, and therefore invertible.\n\nA new equality constraint is introduced, leading to an augmented matrix $A_{+}$ formed by appending the new constraint row $a^{\\top}$ to $A$:\n$$\nA_{+} = \\begin{pmatrix} A \\\\ a^{\\top} \\end{pmatrix} \\in \\mathbb{R}^{(m+1) \\times n}\n$$\nThe corresponding augmented range-space matrix, $S_{+}$, is defined as:\n$$\nS_{+} = A_{+} H^{-1} A_{+}^{\\top}\n$$\nWe can express $S_{+}$ in a $2 \\times 2$ block matrix form by substituting the block structure of $A_{+}$:\n$$\nS_{+} = \\begin{pmatrix} A \\\\ a^{\\top} \\end{pmatrix} H^{-1} \\begin{pmatrix} A^{\\top}  a \\end{pmatrix}\n$$\nPerforming the block matrix multiplication, we get:\n$$\nS_{+} = \\begin{pmatrix} A H^{-1} A^{\\top}  A H^{-1} a \\\\ a^{\\top} H^{-1} A^{\\top}  a^{\\top} H^{-1} a \\end{pmatrix}\n$$\nWe can identify the blocks in this matrix. The top-left block is the original matrix $S = A H^{-1} A^{\\top}$. Let us define the following quantities:\n- The vector $u = A H^{-1} a \\in \\mathbb{R}^{m \\times 1}$.\n- The scalar $\\delta = a^{\\top} H^{-1} a \\in \\mathbb{R}$.\n\nSince $H$ is symmetric, $H^{-1}$ is also symmetric. Therefore, the lower-left block is $(a^{\\top} H^{-1} A^{\\top}) = (A (H^{-1})^{\\top} a)^{\\top} = (A H^{-1} a)^{\\top} = u^{\\top}$.\nThus, $S_{+}$ can be written concisely as:\n$$\nS_{+} = \\begin{pmatrix} S  u \\\\ u^{\\top}  \\delta \\end{pmatrix}\n$$\nTo find the inverse $S_{+}^{-1}$, we use the formula for the inverse of a $2 \\times 2$ block matrix. The formula relies on the Schur complement of either the top-left block $S$ or the bottom-right block $\\delta$. Let $\\gamma$ be the Schur complement of $S$, which is a scalar in this case:\n$$\n\\gamma = \\delta - u^{\\top} S^{-1} u\n$$\nFor $S_{+}^{-1}$ to exist, $S_{+}$ must be non-singular. This requires the new constraint $a^{\\top}$ to be linearly independent of the rows of $A$, which ensures that $\\gamma \\neq 0$. The inverse $S_{+}^{-1}$ is then given by the block matrix inversion formula (also known as the partitioned inverse formula):\n$$\nS_{+}^{-1} = \\begin{pmatrix} S^{-1} + S^{-1} u \\gamma^{-1} u^{\\top} S^{-1}  -S^{-1} u \\gamma^{-1} \\\\ -\\gamma^{-1} u^{\\top} S^{-1}  \\gamma^{-1} \\end{pmatrix}\n$$\nThis is the general expression for the inverse of the augmented range-space matrix. The scalar that appears in the lower-right corner of $S_{+}^{-1}$ is $\\gamma^{-1}$.\n$$\n\\text{Lower-right entry of } S_{+}^{-1} = \\gamma^{-1} = (\\delta - u^{\\top} S^{-1} u)^{-1} = \\left( (a^{\\top} H^{-1} a) - (a^{\\top} H^{-1} A^{\\top}) (A H^{-1} A^{\\top})^{-1} (A H^{-1} a) \\right)^{-1}\n$$\n\n### Part 2: Numerical Calculation\n\nWe are given the following specific data:\n$$\nH = \\operatorname{diag}(2,\\,3,\\,6) = \\begin{pmatrix} 2  0  0 \\\\ 0  3  0 \\\\ 0  0  6 \\end{pmatrix}, \\quad\nA = \\begin{pmatrix} 1  1  0 \\\\ 0  1  2 \\end{pmatrix}, \\quad\na = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}\n$$\nTo find the lower-right entry of $S_{+}^{-1}$, we need to compute $\\gamma^{-1}$. We proceed step-by-step.\n\n1.  **Compute $H^{-1}$**: Since $H$ is a diagonal matrix, its inverse is the diagonal matrix of the reciprocals of its diagonal entries.\n    $$\n    H^{-1} = \\operatorname{diag}\\left(\\frac{1}{2},\\,\\frac{1}{3},\\,\\frac{1}{6}\\right) = \\begin{pmatrix} \\frac{1}{2}  0  0 \\\\ 0  \\frac{1}{3}  0 \\\\ 0  0  \\frac{1}{6} \\end{pmatrix}\n    $$\n\n2.  **Compute $\\delta = a^{\\top} H^{-1} a$**:\n    First, compute $H^{-1}a$:\n    $$\n    H^{-1}a = \\begin{pmatrix} \\frac{1}{2}  0  0 \\\\ 0  \\frac{1}{3}  0 \\\\ 0  0  \\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{3} \\\\ \\frac{1}{6} \\end{pmatrix}\n    $$\n    Then, compute the dot product $a^{\\top}(H^{-1}a)$:\n    $$\n    \\delta = \\begin{pmatrix} 1  -1  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{3} \\\\ \\frac{1}{6} \\end{pmatrix} = (1)\\left(\\frac{1}{2}\\right) + (-1)\\left(-\\frac{1}{3}\\right) + (1)\\left(\\frac{1}{6}\\right) = \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{6} = \\frac{3+2+1}{6} = \\frac{6}{6} = 1\n    $$\n\n3.  **Compute $u = A H^{-1} a$**:\n    Using the previously calculated $H^{-1}a$:\n    $$\n    u = \\begin{pmatrix} 1  1  0 \\\\ 0  1  2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{3} \\\\ \\frac{1}{6} \\end{pmatrix} = \\begin{pmatrix} (1)(\\frac{1}{2}) + (1)(-\\frac{1}{3}) + (0)(\\frac{1}{6}) \\\\ (0)(\\frac{1}{2}) + (1)(-\\frac{1}{3}) + (2)(\\frac{1}{6}) \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} - \\frac{1}{3} \\\\ -\\frac{1}{3} + \\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{6} \\\\ 0 \\end{pmatrix}\n    $$\n    So, $u^{\\top} = \\begin{pmatrix} \\frac{1}{6}  0 \\end{pmatrix}$.\n\n4.  **Compute $S^{-1} = (A H^{-1} A^{\\top})^{-1}$**:\n    First, compute $S = A H^{-1} A^{\\top}$.\n    $$\n    A H^{-1} = \\begin{pmatrix} 1  1  0 \\\\ 0  1  2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2}  0  0 \\\\ 0  \\frac{1}{3}  0 \\\\ 0  0  \\frac{1}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{3}  0 \\\\ 0  \\frac{1}{3}  \\frac{2}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{3}  0 \\\\ 0  \\frac{1}{3}  \\frac{1}{3} \\end{pmatrix}\n    $$\n    $$\n    S = (A H^{-1}) A^{\\top} = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{3}  0 \\\\ 0  \\frac{1}{3}  \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 0  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}(1) + \\frac{1}{3}(1)  \\frac{1}{2}(0) + \\frac{1}{3}(1) \\\\ \\frac{1}{3}(1)  \\frac{1}{3}(1) + \\frac{1}{3}(2) \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{6}  \\frac{1}{3} \\\\ \\frac{1}{3}  1 \\end{pmatrix}\n    $$\n    Now, compute the inverse $S^{-1}$. The determinant of $S$ is:\n    $$\n    \\det(S) = \\left(\\frac{5}{6}\\right)(1) - \\left(\\frac{1}{3}\\right)\\left(\\frac{1}{3}\\right) = \\frac{5}{6} - \\frac{1}{9} = \\frac{15 - 2}{18} = \\frac{13}{18}\n    $$\n    The inverse is:\n    $$\n    S^{-1} = \\frac{1}{\\det(S)} \\begin{pmatrix} 1  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{5}{6} \\end{pmatrix} = \\frac{18}{13} \\begin{pmatrix} 1  -\\frac{1}{3} \\\\ -\\frac{1}{3}  \\frac{5}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{13}  -\\frac{6}{13} \\\\ -\\frac{6}{13}  \\frac{15}{13} \\end{pmatrix}\n    $$\n\n5.  **Compute $u^{\\top} S^{-1} u$**:\n    $$\n    u^{\\top} S^{-1} u = \\begin{pmatrix} \\frac{1}{6}  0 \\end{pmatrix} \\begin{pmatrix} \\frac{18}{13}  -\\frac{6}{13} \\\\ -\\frac{6}{13}  \\frac{15}{13} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{6} \\\\ 0 \\end{pmatrix}\n    $$\n    Let's first compute $S^{-1}u$:\n    $$\n    S^{-1}u = \\begin{pmatrix} \\frac{18}{13}  -\\frac{6}{13} \\\\ -\\frac{6}{13}  \\frac{15}{13} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{6} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{13 \\cdot 6} \\\\ -\\frac{6}{13 \\cdot 6} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{13} \\\\ -\\frac{1}{13} \\end{pmatrix}\n    $$\n    Now multiply by $u^{\\top}$:\n    $$\n    u^{\\top} (S^{-1} u) = \\begin{pmatrix} \\frac{1}{6}  0 \\end{pmatrix} \\begin{pmatrix} \\frac{3}{13} \\\\ -\\frac{1}{13} \\end{pmatrix} = \\left(\\frac{1}{6}\\right)\\left(\\frac{3}{13}\\right) = \\frac{3}{78} = \\frac{1}{26}\n    $$\n\n6.  **Compute $\\gamma = \\delta - u^{\\top} S^{-1} u$**:\n    $$\n    \\gamma = 1 - \\frac{1}{26} = \\frac{25}{26}\n    $$\n\n7.  **Compute the final answer, $\\gamma^{-1}$**:\n    The lower-right entry of $S_{+}^{-1}$ is $\\gamma^{-1}$.\n    $$\n    \\gamma^{-1} = \\left(\\frac{25}{26}\\right)^{-1} = \\frac{26}{25}\n    $$",
            "answer": "$$\\boxed{\\frac{26}{25}}$$"
        }
    ]
}