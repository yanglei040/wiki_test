{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the Maratos effect, there is no substitute for seeing it in action. This first exercise is a numerical deep-dive where you will implement a core component of a Sequential Quadratic Programming (SQP) algorithm. By applying it to a simple problem with a curved constraint, you will observe firsthand how an exact penalty merit function can reject a full, Newton-like step ($\\alpha=1$) near the solution, forcing the step length $\\alpha$ to be cut unnecessarily . This hands-on diagnosis builds crucial intuition by connecting the abstract concept of constraint curvature to the concrete behavior of a line search algorithm.",
            "id": "3147380",
            "problem": "Consider the equality-constrained nonlinear optimization problem with objective function $f(x) = x_1^2 + x_2^2$ and nonlinear equality constraint $c(x) = x_2 - \\sin x_1 = 0$. The Karush-Kuhn-Tucker (KKT) point is the solution $x^\\star$ satisfying first-order necessary conditions for optimality under equality constraints. Sequential Quadratic Programming (SQP) methods construct a quadratic subproblem by linearizing the constraints and using a symmetric positive definite approximation to the Lagrangian Hessian. The resulting step $p_k$ is then used in a line search with a merit function to enforce progress and handle constraints. Near $x^\\star$, the Maratos effect describes a phenomenon where the full SQP step $p_k$ is rejected by a merit-function-based line search due to second-order constraint curvature, even though $p_k$ is a valid Newton-like step that would lead to fast local convergence if accepted.\n\nStarting from the fundamental definitions:\n- The objective gradient is $\\nabla f(x) = \\begin{bmatrix}2 x_1 \\\\ 2 x_2\\end{bmatrix}$.\n- The constraint Jacobian is $\\nabla c(x) = \\begin{bmatrix}-\\cos x_1 & 1\\end{bmatrix}$.\n- The exact penalty merit function is $\\phi_\\sigma(x) = f(x) + \\sigma \\lvert c(x) \\rvert$, where $\\sigma > 0$ is the penalty parameter.\n- The Armijo sufficient decrease condition requires $\\phi_\\sigma(x + \\alpha p_k) \\le \\phi_\\sigma(x) + m \\alpha d_\\phi(x; p_k)$, where $m \\in (0, 1)$ is the sufficient decrease parameter, $\\alpha \\in (0, 1]$ is the step length, and $d_\\phi(x; p_k)$ is the directional derivative of $\\phi_\\sigma$ at $x$ along $p_k$.\n- For the exact penalty merit function, the directional derivative along $p$ at a point $x$ is defined by $d_\\phi(x; p) = \\nabla f(x)^\\top p + \\sigma \\operatorname{sgn}(c(x)) \\nabla c(x) p$, where $\\operatorname{sgn}$ is the sign function and is taken to be $0$ when $c(x)=0$.\n\nThe SQP step $p_k$ at a point $x_k$ is defined as the solution of the quadratic programming subproblem with linearized constraints:\n- Minimize $q(p) = \\nabla f(x_k)^\\top p + \\tfrac{1}{2} p^\\top B_k p$ subject to $\\nabla c(x_k) p + c(x_k) = 0$,\nwhere $B_k$ is a symmetric positive definite matrix approximating the Lagrangian Hessian. In this problem, use $B_k = 2 I$, where $I$ is the identity matrix. The optimality conditions of this quadratic program yield the linear KKT system\n$$\n\\begin{bmatrix}\nB_k & \\nabla c(x_k)^\\top \\\\\n\\nabla c(x_k) & 0\n\\end{bmatrix}\n\\begin{bmatrix}\np_k \\\\ \\lambda_k\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\n\\nabla f(x_k) \\\\ c(x_k)\n\\end{bmatrix}.\n$$\n\nImplement a backtracking line search along $p_k$ using the exact penalty merit function $\\phi_\\sigma$, with the Armijo parameters $m = 10^{-4}$ and backtracking factor $\\rho = \\tfrac{1}{2}$, and penalty parameter $\\sigma = 1000$. Begin with $\\alpha = 1$ and repeatedly multiply $\\alpha$ by $\\rho$ until the Armijo condition holds, or until a maximum of $50$ backtracking steps is reached. If the directional derivative $d_\\phi(x_k; p_k) \\ge 0$, the step is not a descent direction for the merit function; in that case, report $\\alpha = 0$.\n\nYour program must:\n- Compute the SQP step $p_k$ at each provided test point $x_k$ by solving the KKT system with $B_k = 2 I$ and $\\nabla c(x_k)$ defined above.\n- Perform the backtracking line search along $p_k$ with the exact penalty merit function $\\phi_\\sigma$ and Armijo condition.\n- Return, for each test point, the accepted step length $\\alpha$.\n\nTest suite:\n1. $x_k = (0.1, \\sin(0.1))$ (feasible near the KKT point).\n2. $x_k = (0.05, \\sin(0.05))$ (feasible closer to the KKT point).\n3. $x_k = (0.01, \\sin(0.01))$ (feasible very close to the KKT point).\n4. $x_k = (0.1, \\sin(0.1) + 10^{-3})$ (slightly infeasible near the KKT point).\n5. $x_k = (10^{-6}, \\sin(10^{-6}))$ (feasible extremely close to the KKT point).\n\nFinal output format:\n- Your program should produce a single line of output containing the accepted step lengths for the five test cases, as a comma-separated list enclosed in square brackets, in the same order as the test suite (for example, $[\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4,\\alpha_5]$). Each $\\alpha_i$ must be a floating-point number.\n\nThe design intentionally explores the Maratos effect: for feasible iterates near the KKT point, the linearized constraint makes $\\nabla c(x_k) p_k + c(x_k) = 0$, so first-order feasibility is preserved along $p_k$. However, the true constraint $c(x)$ exhibits curvature, and $c(x_k + \\alpha p_k)$ contains second-order terms that scale as $\\mathcal{O}(\\alpha^2)$, which can cause the merit function $\\phi_\\sigma$ to increase and violate the Armijo condition for $\\alpha = 1$, thus rejecting the full SQP step despite the near-optimality of $p_k$. Your implementation must numerically detect this behavior via the computed accepted step lengths.",
            "solution": "The problem is valid. It is a well-defined numerical exercise in the field of nonlinear optimization, specifically designed to demonstrate the Maratos effect within the context of Sequential Quadratic Programming (SQP) methods. All necessary functions, parameters, and procedures are provided, and the problem is mathematically and algorithmically sound.\n\n**1. Theoretical Framework**\n\nThe problem asks to analyze the behavior of a line search procedure for an SQP method on a specific nonlinear constrained optimization problem. The core components are:\n\n- **Objective function**: $f(x) = x_1^2 + x_2^2$\n- **Equality constraint**: $c(x) = x_2 - \\sin x_1 = 0$\n\nThe optimal solution to this problem is $x^\\star=(0, 0)$, where the objective function is minimized.\n\n**2. The SQP Method**\n\nSQP methods iteratively solve the optimization problem by forming and solving a quadratic programming (QP) subproblem at each iterate $x_k$. The QP subproblem is constructed by using a quadratic model of the Lagrangian function and linearizing the constraints.\n\nThe step $p_k$ is found by solving:\n$$\n\\min_{p} \\quad \\nabla f(x_k)^\\top p + \\frac{1}{2} p^\\top B_k p\n$$\n$$\n\\text{s.t.} \\quad \\nabla c(x_k) p + c(x_k) = 0\n$$\n\nHere, $B_k=2I$ is the given positive definite approximation to the Hessian of the Lagrangian. The solution to this QP subproblem is given by the Karush-Kuhn-Tucker (KKT) linear system:\n$$\n\\begin{bmatrix}\nB_k & \\nabla c(x_k)^\\top \\\\\n\\nabla c(x_k) & 0\n\\end{bmatrix}\n\\begin{bmatrix}\np_k \\\\ \\lambda_k\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\n\\nabla f(x_k) \\\\ c(x_k)\n\\end{bmatrix}\n$$\nwhere $\\lambda_k$ is the Lagrange multiplier estimate for the constraints. For our specific problem, this system is:\n$$\n\\begin{bmatrix}\n2 & 0 & -\\cos x_{k,1} \\\\\n0 & 2 & 1 \\\\\n-\\cos x_{k,1} & 1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\np_{k,1} \\\\ p_{k,2} \\\\ \\lambda_k\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\n2x_{k,1} \\\\\n2x_{k,2} \\\\\nx_{k,2} - \\sin x_{k,1}\n\\end{bmatrix}\n$$\nThis $3 \\times 3$ linear system is solved at each test point $x_k$ to find the SQP step $p_k$.\n\n**3. The Merit Function and Line Search**\n\nTo ensure that the step $p_k$ makes progress towards the constrained optimum, a merit function is used. The merit function combines the objective and constraint violation into a single value. The problem specifies the exact penalty merit function:\n$$\n\\phi_\\sigma(x) = f(x) + \\sigma |c(x)|\n$$\nwith a penalty parameter $\\sigma=1000$. A line search is performed along the direction $p_k$ to find a step length $\\alpha \\in (0, 1]$ that provides sufficient decrease in this merit function. The new iterate would be $x_{k+1} = x_k + \\alpha p_k$.\n\nThe sufficient decrease is governed by the Armijo condition:\n$$\n\\phi_\\sigma(x_k + \\alpha p_k) \\le \\phi_\\sigma(x_k) + m \\alpha d_\\phi(x_k; p_k)\n$$\nwhere $m = 10^{-4}$ is a small constant and $d_\\phi(x_k; p_k)$ is the directional derivative of $\\phi_\\sigma$ at $x_k$ along $p_k$, defined as:\n$$\nd_\\phi(x_k; p_k) = \\nabla f(x_k)^\\top p_k + \\sigma \\operatorname{sgn}(c(x_k)) \\nabla c(x_k) p_k\n$$\nThe line search procedure is a backtracking algorithm:\n1.  Initialize $\\alpha = 1$.\n2.  If the Armijo condition is satisfied, accept this $\\alpha$.\n3.  If not, reduce the step length by a factor $\\rho = 1/2$ (i.e., $\\alpha \\leftarrow \\alpha \\rho$) and repeat, up to a maximum of $50$ iterations.\n\nA crucial prerequisite is that $p_k$ must be a descent direction for the merit function, i.e., $d_\\phi(x_k; p_k) < 0$. If $d_\\phi(x_k; p_k) \\ge 0$, no positive step length can satisfy the Armijo condition, and we report $\\alpha = 0$.\n\n**4. The Maratos Effect**\n\nThe Maratos effect is a phenomenon where a \"good\" step (like the full SQP step, $\\alpha=1$) is rejected by the line search, forcing small, inefficient steps. This typically occurs near the solution $x^\\star$ when the iterate $x_k$ lies on or very close to the constraint manifold ($c(x_k) \\approx 0$).\n\nFrom the QP subproblem, the step $p_k$ satisfies the linearized constraint $\\nabla c(x_k) p_k + c(x_k) = 0$. If $x_k$ is feasible ($c(x_k)=0$), then $\\nabla c(x_k) p_k = 0$. This means the step $p_k$ is tangent to the constraint manifold at $x_k$, to first order. However, the constraint $c(x)$ is nonlinear (curved). When we take the step, the new point $x_k + p_k$ may move away from the true constraint manifold. A Taylor expansion of $c(x_k+p_k)$ reveals this:\n$$\nc(x_k+p_k) \\approx c(x_k) + \\nabla c(x_k) p_k + \\frac{1}{2} p_k^\\top \\nabla^2 c(x_k) p_k = \\frac{1}{2} p_k^\\top \\nabla^2 c(x_k) p_k\n$$\nThis second-order term, representing the constraint's curvature, means $|c(x_k+p_k)| \\ne 0$. For a large penalty parameter $\\sigma$, the increase in the merit function due to this new constraint violation, $\\sigma |c(x_k+p_k)|$, can overwhelm the decrease achieved in the objective function, causing the Armijo condition to fail for $\\alpha=1$. The algorithm is then forced to cut the step length, destroying the fast quadratic convergence rate of the full-step SQP method.\n\n**5. Implementation Algorithm**\n\nFor each test point $x_k=(x_{k,1}, x_{k,2})$:\n1.  **Calculate problem functions**: Compute $c(x_k) = x_{k,2} - \\sin(x_{k,1})$, $\\nabla f(x_k) = [2x_{k,1}, 2x_{k,2}]^\\top$, and $\\nabla c(x_k) = [-\\cos(x_{k,1}), 1]$.\n2.  **Solve for the SQP step**: Construct the KKT matrix $A$ and right-hand side vector $b$ and solve the linear system $A z = b$ for $z=[p_k^\\top, \\lambda_k]^\\top$.\n3.  **Compute directional derivative**: Calculate $d_\\phi(x_k; p_k) = \\nabla f(x_k)^\\top p_k + \\sigma \\operatorname{sgn}(c(x_k)) (\\nabla c(x_k) p_k)$.\n4.  **Check for descent direction**: If $d_\\phi(x_k; p_k) \\ge 0$, set the accepted step length $\\alpha_k = 0$ and terminate for this test case.\n5.  **Perform backtracking line search**:\n    - Initialize $\\alpha = 1.0$.\n    - Calculate the initial merit function value $\\phi_{\\text{current}} = f(x_k) + \\sigma|c(x_k)|$.\n    - Loop for a maximum of $50$ iterations:\n        - Let $x_{\\text{new}} = x_k + \\alpha p_k$.\n        - Calculate the new merit function value $\\phi_{\\text{new}} = f(x_{\\text{new}}) + \\sigma|c(x_{\\text{new}})|$.\n        - Check if $\\phi_{\\text{new}} \\le \\phi_{\\text{current}} + m \\alpha d_\\phi(x_k; p_k)$.\n        - If the condition holds, break the loop and accept the current $\\alpha$.\n        - Otherwise, update $\\alpha \\leftarrow \\alpha \\times 0.5$.\n6.  **Store result**: The final value of $\\alpha$ is the accepted step length $\\alpha_k$ for the test point $x_k$.\n\nThis procedure is applied to each of the five test cases provided. The results will numerically demonstrate the Maratos effect: for points close to the solution on the feasible manifold, the full step $\\alpha=1$ will be rejected, but as the points get extremely close, the second-order curvature term becomes small enough for the full step to be accepted. For an infeasible point, the step aims to reduce infeasibility, which often helps satisfy the Armijo condition.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the accepted Armijo step length for a series of test cases\n    in an SQP framework, demonstrating the Maratos effect.\n    \"\"\"\n    \n    # Define optimization problem parameters and functions\n    sigma = 1000.0\n    m = 1e-4\n    rho = 0.5\n    max_backtrack_steps = 50\n\n    def f(x):\n        \"\"\"Objective function f(x) = x1^2 + x2^2\"\"\"\n        return x[0]**2 + x[1]**2\n\n    def grad_f(x):\n        \"\"\"Gradient of the objective function\"\"\"\n        return np.array([2 * x[0], 2 * x[1]])\n\n    def c(x):\n        \"\"\"Equality constraint c(x) = x2 - sin(x1)\"\"\"\n        return x[1] - np.sin(x[0])\n\n    def grad_c(x):\n        \"\"\"Jacobian of the equality constraint\"\"\"\n        return np.array([-np.cos(x[0]), 1.0])\n    \n    def merit_function(x_val, sigma_val):\n        \"\"\"Exact penalty merit function\"\"\"\n        return f(x_val) + sigma_val * np.abs(c(x_val))\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.1, np.sin(0.1)),\n        (0.05, np.sin(0.05)),\n        (0.01, np.sin(0.01)),\n        (0.1, np.sin(0.1) + 1e-3),\n        (1e-6, np.sin(1e-6))\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x_k = np.array(case)\n        \n        # 1. Calculate values at the current iterate x_k\n        grad_f_k = grad_f(x_k)\n        c_k = c(x_k)\n        grad_c_k = grad_c(x_k)\n        \n        # 2. Construct and solve the KKT system for the SQP step p_k\n        # [ B_k   grad_c(x_k)^T ] [ p_k     ] = - [ grad_f(x_k) ]\n        # [ grad_c(x_k)    0    ] [ lambda_k]   - [   c(x_k)    ]\n        # B_k = 2*I\n        \n        kkt_matrix = np.array([\n            [2.0, 0.0, grad_c_k[0]],\n            [0.0, 2.0, grad_c_k[1]],\n            [grad_c_k[0], grad_c_k[1], 0.0]\n        ])\n        \n        rhs_vector = -np.array([grad_f_k[0], grad_f_k[1], c_k])\n        \n        try:\n            kkt_solution = np.linalg.solve(kkt_matrix, rhs_vector)\n        except np.linalg.LinAlgError:\n            # Handle cases where the matrix might be singular, though not expected for this problem.\n            results.append(0.0) # No valid step can be computed\n            continue\n\n        p_k = kkt_solution[:2]\n        \n        # 3. Compute directional derivative of the merit function\n        d_phi = grad_f_k @ p_k + sigma * np.sign(c_k) * (grad_c_k @ p_k)\n        \n        # 4. Check if p_k is a descent direction\n        if d_phi >= 0:\n            results.append(0.0)\n            continue\n            \n        # 5. Perform backtracking line search\n        alpha = 1.0\n        current_phi = merit_function(x_k, sigma)\n        \n        accepted_alpha = alpha\n        for _ in range(max_backtrack_steps + 1): # +1 to handle the initial alpha=1 check\n            x_new = x_k + alpha * p_k\n            new_phi = merit_function(x_new, sigma)\n            \n            armijo_condition = new_phi = current_phi + m * alpha * d_phi\n            \n            if armijo_condition:\n                accepted_alpha = alpha\n                break\n            \n            alpha *= rho\n        else: # Loop completed without break\n            accepted_alpha = alpha / rho # Use the last successful alpha value. Here it means the smallest.\n                                         # But problem wants the value that satisfies it or the last one after 50 steps\n                                         # Last alpha isrho^50. So alpha is the final one.\n            accepted_alpha = alpha\n            \n        results.append(accepted_alpha)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having diagnosed the Maratos effect, the next logical step is to implement a cure. This practice guides you through enhancing a baseline SQP solver with a Second-Order Correction (SOC), a powerful technique designed to counteract the very constraint curvature that causes the issue . You will compare the performance of the baseline solver against the SOC-enhanced version on a problem with a tunable curvature parameter, $\\kappa$. This allows you to directly quantify the improvement and solidifies your understanding of how SOC restores the fast local convergence that the Maratos effect otherwise spoils.",
            "id": "3147419",
            "problem": "Implement a numerical experiment to diagnose and mitigate the Maratos effect within Sequential Quadratic Programming (SQP) for a family of smooth nonlinear inequality constrained optimization problems. Your program must implement two solvers: a baseline SQP with a backtracking line search on an exact penalty merit function, and an SQP enhanced with a Second-Order Correction (SOC) that compensates for curvature-induced linearization errors. Compare their iteration counts on a parameterized curved constraint and report the improvement in iteration count due to SOC as a function of the constraint curvature parameter.\n\nThe optimization problem is parameterized by a nonnegative curvature parameter $\\,\\kappa \\in \\mathbb{R}_{\\ge 0}\\,$:\n- Decision variable: $\\,x = (x_1,x_2) \\in \\mathbb{R}^2$.\n- Objective: minimize $\\,f(x) = \\tfrac{1}{2}(x_1 - 1)^2 + \\tfrac{1}{2} x_2^2$.\n- Single inequality constraint: $\\,c_\\kappa(x) \\le 0\\,$ with $\\,c_\\kappa(x) = \\tfrac{1}{2}\\kappa x_1^2 - x_2$.\n\nInterpretation: the feasible set is $\\,\\{x \\in \\mathbb{R}^2 \\,:\\, x_2 \\ge \\tfrac{1}{2}\\kappa x_1^2\\}\\,$, i.e., the region above a parabola whose curvature increases with $\\,\\kappa\\,$. The unconstrained minimizer is at $\\,x^\\star = (1,0)\\,$ and becomes infeasible as $\\,\\kappa$ increases.\n\nYou must use the following fundamental base and core definitions to justify your algorithmic design:\n- Karush–Kuhn–Tucker (KKT) conditions for inequality constrained smooth optimization define first-order necessary conditions at a local minimizer: stationarity $\\,\\nabla f(x) + \\lambda \\nabla c_\\kappa(x) = 0\\,$ for some multiplier $\\,\\lambda \\ge 0\\,$, primal feasibility $\\,c_\\kappa(x) \\le 0\\,$, dual feasibility $\\,\\lambda \\ge 0\\,$, and complementarity $\\,\\lambda\\,c_\\kappa(x) = 0$.\n- Sequential Quadratic Programming (SQP) constructs steps by solving a quadratic approximation of the Lagrangian subject to the linearized constraints at the current iterate.\n- The Maratos effect refers to the phenomenon where an SQP step that is linearly feasible (to first order) becomes infeasible when evaluated in the original nonlinear constraints due to second-order terms in the constraint Taylor expansion, yielding poor merit function decrease and causing line search rejection with many backtracking reductions.\n- Second-order correction (SOC) remedies this by adding a small correction computed at the trial point to restore linearized feasibility there, counteracting the second-order residual.\n\nImplementation requirements:\n- Start every run from the same initial point $\\,x^{(0)} = (0.5,\\,0.0)\\,$.\n- Use the identity matrix $\\,B = I\\,$ as the Hessian approximation in the QP subproblem.\n- At each outer iteration, form and solve the one-constraint QP subproblem that minimizes the quadratic model of $\\,f\\,$ subject to the linearization of $\\,c_\\kappa\\,$ at the current iterate. Use the exact solution for the single-inequality case by checking whether the unconstrained step is linearly feasible; otherwise, enforce the linearized constraint as active.\n- Use a backtracking line search on the exact penalty merit function $\\,\\phi(x) = f(x) + \\mu \\max\\{0, c_\\kappa(x)\\}\\,$ with fixed penalty weight $\\,\\mu = 100\\,$. Use an Armijo condition with sufficient decrease parameter $\\,\\sigma = 10^{-4}\\,$ and backtracking factor $\\,\\beta = 0.5\\,$.\n- For the SOC-enhanced variant, after computing the QP step $\\,p\\,$ at $\\,x\\,$, form the trial point $\\,x^+ = x + p\\,$ and, if $\\,c_\\kappa(x^+) > 0\\,$, compute a minimum-norm correction $\\,d\\,$ that restores linearized feasibility at $\\,x^+\\,$ by solving the linearized inequality as an equality: $\\,\\nabla c_\\kappa(x^+)^\\top d = -c_\\kappa(x^+)\\,$. Use the combined step $\\,s = p + d\\,$ for the line search. If $\\,c_\\kappa(x^+) \\le 0\\,$, set $\\,d = 0\\,$ and $\\,s = p\\,$.\n- Terminate when the KKT residual $\\,r(x)\\,$ is below tolerance $\\,\\varepsilon = 10^{-6}\\,$ or when the iteration count reaches a hard cap of $\\,200\\,$. Define the KKT residual as\n$$\nr(x) \\;=\\; \\left\\|\\nabla f(x) + \\lambda^\\star(x)\\,\\nabla c_\\kappa(x)\\right\\|_2 \\;+\\; \\max\\{0, c_\\kappa(x)\\},\n$$\nwhere $\\,\\lambda^\\star(x) = \\max\\left\\{0,\\; -\\dfrac{\\nabla f(x)^\\top \\nabla c_\\kappa(x)}{\\|\\nabla c_\\kappa(x)\\|_2^2}\\right\\}\\,$ is the nonnegative scalar that minimizes $\\,\\left\\|\\nabla f(x) + \\lambda \\nabla c_\\kappa(x)\\right\\|_2\\,$ over $\\,\\lambda \\ge 0\\,$.\n\nTest suite and outputs:\n- Run both the baseline SQP and the SOC-enhanced SQP for each curvature parameter in the list $\\,\\kappa \\in \\{0.0,\\,0.5,\\,2.0,\\,5.0,\\,10.0\\}\\,$.\n- For each $\\,\\kappa\\,$, record the number of outer iterations to convergence for the baseline solver, denoted $\\,N_{\\text{base}}(\\kappa)\\,$, and for the SOC-enhanced solver, denoted $\\,N_{\\text{soc}}(\\kappa)\\,$.\n- For each $\\,\\kappa\\,$, compute the integer improvement $\\,\\Delta(\\kappa) = N_{\\text{base}}(\\kappa) - N_{\\text{soc}}(\\kappa)\\,$. Positive values indicate fewer iterations with SOC; zero indicates no change; negative values indicate SOC required more iterations.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[3,1,0,2,4]\"), corresponding to the values of $\\,\\Delta(\\kappa)\\,$ in the order of $\\,\\kappa \\in \\{0.0,\\,0.5,\\,2.0,\\,5.0,\\,10.0\\}\\,$.\n\nNo physical units or angles are involved in this problem; all outputs are unitless integers.",
            "solution": "The user-provided problem is valid. It is a well-posed, scientifically grounded problem in the field of numerical optimization. All necessary definitions, parameters, and algorithmic components are provided, and there are no internal contradictions or ambiguities. The problem is a standard textbook example used to demonstrate the Maratos effect and the efficacy of the Second-Order Correction (SOC) in Sequential Quadratic Programming (SQP).\n\nWe proceed with a detailed solution.\n\n### 1. Problem Formulation and Preliminaries\n\nThe problem is to minimize a smooth nonlinear objective function subject to a single smooth nonlinear inequality constraint. The problem is parameterized by a non-negative curvature parameter $\\kappa \\in \\mathbb{R}_{\\ge 0}$.\n\n- **Objective Function:** $f(x) = \\frac{1}{2}(x_1 - 1)^2 + \\frac{1}{2} x_2^2$, where $x = (x_1, x_2) \\in \\mathbb{R}^2$.\n- **Constraint Function:** $c_\\kappa(x) = \\frac{1}{2}\\kappa x_1^2 - x_2 \\le 0$.\n\nThe gradients of these functions are essential for the SQP algorithm:\n- **Gradient of Objective:** $\\nabla f(x) = \\begin{pmatrix} x_1 - 1 \\\\ x_2 \\end{pmatrix}$.\n- **Gradient of Constraint:** $\\nabla c_\\kappa(x) = \\begin{pmatrix} \\kappa x_1 \\\\ -1 \\end{pmatrix}$.\n\nThe Karush-Kuhn-Tucker (KKT) conditions provide first-order necessary conditions for a point $x^\\star$ to be a local minimizer. There must exist a Lagrange multiplier $\\lambda^\\star \\ge 0$ such that:\n1.  **Stationarity:** $\\nabla f(x^\\star) + \\lambda^\\star \\nabla c_\\kappa(x^\\star) = 0$.\n2.  **Primal Feasibility:** $c_\\kappa(x^\\star) \\le 0$.\n3.  **Dual Feasibility:** $\\lambda^\\star \\ge 0$.\n4.  **Complementarity:** $\\lambda^\\star c_\\kappa(x^\\star) = 0$.\n\n### 2. Sequential Quadratic Programming (SQP) Framework\n\nSQP is an iterative method that models the original nonlinear problem with a sequence of Quadratic Programming (QP) subproblems. At each iterate $x^{(k)}$, we compute a search direction $p^{(k)}$ by solving a QP that consists of a quadratic model of the Lagrangian function subject to a linear model of the constraints.\n\n#### 2.1. The QP Subproblem\n\nAt a given iterate $x^{(k)}$, the QP subproblem is defined as:\n$$\n\\min_{p \\in \\mathbb{R}^2} \\quad \\frac{1}{2} p^\\top B^{(k)} p + \\nabla f(x^{(k)})^\\top p\n$$\n$$\n\\text{s.t.} \\quad \\nabla c_\\kappa(x^{(k)})^\\top p + c_\\kappa(x^{(k)}) \\le 0\n$$\nAs specified, we use the identity matrix for the Hessian approximation, $B^{(k)} = I$. Let $g^{(k)} = \\nabla f(x^{(k)})$, $A^{(k)} = \\nabla c_\\kappa(x^{(k)})^\\top$, and $b^{(k)} = -c_\\kappa(x^{(k)})$. The QP simplifies to:\n$$\n\\min_{p \\in \\mathbb{R}^2} \\quad \\frac{1}{2} p^\\top p + (g^{(k)})^\\top p \\quad \\text{s.t.} \\quad A^{(k)} p \\le b^{(k)}\n$$\nThe unconstrained minimizer of the QP objective is $p_{unc} = -g^{(k)}$. We check if this step satisfies the linearized constraint:\n- **Case 1: Constraint Inactive.** If $A^{(k)} p_{unc} \\le b^{(k)}$, or equivalently $c_\\kappa(x^{(k)}) \\le \\nabla c_\\kappa(x^{(k)})^\\top \\nabla f(x^{(k)})$, the unconstrained step is feasible for the QP. The solution is $p^{(k)} = -g^{(k)}$. The QP multiplier is $\\nu^{(k)}=0$.\n- **Case 2: Constraint Active.** If $A^{(k)} p_{unc}  b^{(k)}$, the solution must lie on the boundary of the feasible set, $A^{(k)} p = b^{(k)}$. The solution to this equality-constrained QP is given by $p^{(k)} = -g^{(k)} - \\nu^{(k)} (A^{(k)})^\\top$, where the QP multiplier $\\nu^{(k)}$ is found by substituting $p^{(k)}$ into the active constraint:\n$$\n\\nu^{(k)} = \\frac{-b^{(k)} - A^{(k)} g^{(k)}}{A^{(k)} (A^{(k)})^\\top} = \\frac{c_\\kappa(x^{(k)}) - \\nabla c_\\kappa(x^{(k)})^\\top \\nabla f(x^{(k)})}{\\|\\nabla c_\\kappa(x^{(k)})\\|_2^2}\n$$\nThe search direction is then $p^{(k)} = -\\nabla f(x^{(k)}) - \\nu^{(k)} \\nabla c_\\kappa(x^{(k)})$. Since we are in this case, the numerator is positive, ensuring $\\nu^{(k)}  0$.\n\n#### 2.2. Globalization via Merit Function and Line Search\n\nThe step $p^{(k)}$ provides a good local search direction. To ensure global convergence, we perform a backtracking line search to find a step length $\\alpha^{(k)} \\in (0, 1]$ that yields a sufficient decrease in a merit function. We use the exact penalty (or $l_1$) merit function:\n$$\n\\phi(x) = f(x) + \\mu \\max\\{0, c_\\kappa(x)\\}\n$$\nwith a penalty parameter $\\mu = 100$. The line search aims to satisfy the Armijo condition:\n$$\n\\phi(x^{(k)} + \\alpha s^{(k)}) \\le \\phi(x^{(k)}) + \\sigma \\alpha D^{(k)}\n$$\nwhere $s^{(k)}$ is the search direction (to be defined), $\\sigma=10^{-4}$ is the sufficient decrease constant, and $D^{(k)}$ is the predicted directional derivative. A standard choice for $D^{(k)}$ in this context is based on the linearized model that motivates the SQP step:\n$$\nD^{(k)} = \\nabla f(x^{(k)})^\\top p^{(k)} - \\mu \\max\\{0, c_\\kappa(x^{(k)})\\}\n$$\nFor the baseline SQP algorithm, the search direction is simply the QP solution, $s^{(k)} = p^{(k)}$.\n\n### 3. The Maratos Effect and Second-Order Correction (SOC)\n\nThe Maratos effect occurs when an iterate is close to the optimal solution but not on the constraint boundary. The SQP step $p^{(k)}$ provides an excellent move towards the optimum, reducing the Lagrangian model. However, due to the curvature of the true constraint $c_\\kappa$, the trial point $x^{(k+1)} = x^{(k)} + p^{(k)}$ may land far outside the feasible set, i.e., $c_\\kappa(x^{(k+1)})  0$. This increase in constraint violation can outweigh the decrease in the objective function $f$, causing the merit function $\\phi$ to increase. The line search rejects the full step ($\\alpha=1$) and requires multiple backtracking steps, severely slowing down the rate of convergence. This effect is more pronounced for larger $\\kappa$, which corresponds to higher constraint curvature.\n\nThe Second-Order Correction (SOC) is a remedy. After computing the primary step $p^{(k)}$, we form a trial point $x^+ = x^{(k)} + p^{(k)}$. We then evaluate the true constraint violation $c_\\kappa(x^+)$ at this point. If $c_\\kappa(x^+)  0$, we compute a \"correction\" step $d^{(k)}$ to restore feasibility, at least to first order. This correction is the minimum-norm vector that satisfies the linearized constraint equation at $x^+$:\n$$\n\\nabla c_\\kappa(x^+)^\\top d = -c_\\kappa(x^+)\n$$\nThe solution to this minimum-norm problem is:\n$$\nd^{(k)} = -\\frac{c_\\kappa(x^+)}{\\|\\nabla c_\\kappa(x^+)\\|_2^2} \\nabla c_\\kappa(x^+)\n$$\nThe final search direction for the line search is the composite step $s^{(k)} = p^{(k)} + d^{(k)}$. This new direction incorporates the second-order information about the constraint (via evaluation at $x^+$) and aims to satisfy the Armijo condition with $\\alpha=1$, thus restoring fast local convergence. If $c_\\kappa(x^+) \\le 0$, no correction is needed, and we set $s^{(k)} = p^{(k)}$.\n\n### 4. Termination Criterion\n\nThe algorithm terminates when the KKT residual falls below a tolerance $\\varepsilon = 10^{-6}$. The residual $r(x)$ measures the violation of the KKT conditions:\n$$\nr(x) = \\underbrace{\\|\\nabla f(x) + \\lambda^\\star(x)\\,\\nabla c_\\kappa(x)\\|_2}_{\\text{Stationarity error}} + \\underbrace{\\max\\{0, c_\\kappa(x)\\}}_{\\text{Feasibility error}}\n$$\nwhere $\\lambda^\\star(x)$ is an estimate of the Lagrange multiplier. It is chosen as the non-negative least-squares solution for the multiplier that minimizes the stationarity part of the residual:\n$$\n\\lambda^\\star(x) = \\arg\\min_{\\lambda \\ge 0} \\left\\|\\nabla f(x) + \\lambda\\,\\nabla c_\\kappa(x)\\right\\|_2^2\n$$\nSolving this single-variable non-negatively constrained least squares problem yields:\n$$\n\\lambda^\\star(x) = \\max\\left\\{0,\\; -\\frac{\\nabla f(x)^\\top \\nabla c_\\kappa(x)}{\\|\\nabla c_\\kappa(x)\\|_2^2}\\right\\}\n$$\nA hard limit of $200$ iterations is also imposed.\n\n### 5. Algorithmic Summary\n\n**Baseline SQP and SOC-SQP Algorithm:**\n\n1.  Initialize $k=0$, $x^{(0)} = (0.5, 0.0)$, $\\mu=100$, $\\sigma=10^{-4}$, $\\beta=0.5$, $\\varepsilon=10^{-6}$.\n2.  **For** $k = 0, 1, 2, \\dots, 199$:\n    a.  Compute $f(x^{(k)})$, $\\nabla f(x^{(k)})$, $c_\\kappa(x^{(k)})$, and $\\nabla c_\\kappa(x^{(k)})$.\n    b.  Compute the KKT residual $r(x^{(k)})$. If $r(x^{(k)})  \\varepsilon$, terminate and return $k$.\n    c.  Solve the QP subproblem to find the step $p^{(k)}$ and the QP multiplier $\\nu^{(k)}$.\n    d.  **Set search direction $s^{(k)}$**:\n        -   **Baseline:** $s^{(k)} = p^{(k)}$.\n        -   **SOC-Enhanced:**\n            i.   Set trial point $x^+ = x^{(k)} + p^{(k)}$.\n            ii.  If $c_\\kappa(x^+)  0$, compute $d^{(k)} = -\\frac{c_\\kappa(x^+)}{\\|\\nabla c_\\kappa(x^+)\\|_2^2} \\nabla c_\\kappa(x^+)$ and set $s^{(k)} = p^{(k)} + d^{(k)}$.\n            iii. Else, set $s^{(k)} = p^{(k)}$.\n    e.  **Backtracking Line Search**:\n        i.   Initialize step length $\\alpha = 1$.\n        ii.  Compute current merit value $\\phi(x^{(k)})$.\n        iii. Compute predicted derivative $D^{(k)} = \\nabla f(x^{(k)})^\\top p^{(k)} - \\mu \\max\\{0, c_\\kappa(x^{(k)})\\} $.\n        iv.  **While** $\\phi(x^{(k)} + \\alpha s^{(k)})  \\phi(x^{(k)}) + \\sigma \\alpha D^{(k)}$:\n             $\\alpha \\leftarrow \\beta \\alpha$.\n    f.  **Update:** $x^{(k+1)} = x^{(k)} + \\alpha s^{(k)}$.\n3.  If loop finishes, terminate and return $200$.\n\nThe experiment will execute this procedure for both the baseline and SOC-enhanced versions across the specified range of $\\kappa$ values and report the difference in the number of iterations required for convergence.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a numerical experiment to diagnose and mitigate the Maratos effect\n    within Sequential Quadratic Programming (SQP) for a family of smooth nonlinear\n    inequality constrained optimization problems.\n    \"\"\"\n\n    # --- Problem Definition ---\n    def objective_func(x):\n        \"\"\"Computes the objective function value and its gradient.\"\"\"\n        f_val = 0.5 * (x[0] - 1.0)**2 + 0.5 * x[1]**2\n        grad_f = np.array([x[0] - 1.0, x[1]])\n        return f_val, grad_f\n\n    def constraint_func(x, kappa):\n        \"\"\"Computes the constraint function value and its gradient.\"\"\"\n        c_val = 0.5 * kappa * x[0]**2 - x[1]\n        grad_c = np.array([kappa * x[0], -1.0])\n        return c_val, grad_c\n\n    def merit_func(x, kappa, mu):\n        \"\"\"Computes the exact penalty merit function.\"\"\"\n        f_val, _ = objective_func(x)\n        c_val, _ = constraint_func(x, kappa)\n        return f_val + mu * max(0, c_val)\n\n    def kkt_residual(x, kappa):\n        \"\"\"Computes the KKT residual for termination checking.\"\"\"\n        _, grad_f = objective_func(x)\n        c_val, grad_c = constraint_func(x, kappa)\n        \n        grad_c_norm_sq = np.dot(grad_c, grad_c)\n        \n        # This clause handles the case where grad_c is the zero vector, which should not happen here.\n        if grad_c_norm_sq  1e-12:\n            lambda_star = 0.0\n        else:\n            lambda_star = max(0, -np.dot(grad_f, grad_c) / grad_c_norm_sq)\n\n        stationarity_err = np.linalg.norm(grad_f + lambda_star * grad_c)\n        feasibility_err = max(0, c_val)\n        \n        return stationarity_err + feasibility_err\n\n    # --- SQP Solver ---\n    def sqp_solver(kappa, use_soc):\n        \"\"\"\n        Solves the optimization problem for a given kappa using SQP.\n        \n        Args:\n            kappa (float): The curvature parameter.\n            use_soc (bool): If True, enables the Second-Order Correction.\n            \n        Returns:\n            int: The number of iterations to convergence.\n        \"\"\"\n        \n        # Parameters\n        x = np.array([0.5, 0.0])\n        mu = 100.0\n        sigma = 1e-4\n        beta = 0.5\n        tol = 1e-6\n        max_iter = 200\n\n        for i in range(max_iter):\n            # 1. Check for termination\n            if kkt_residual(x, kappa)  tol:\n                return i\n\n            # 2. Evaluate functions and gradients at the current iterate\n            f_val, grad_f = objective_func(x)\n            c_val, grad_c = constraint_func(x, kappa)\n\n            # 3. Solve the QP subproblem for the step p\n            # QP: min 0.5*p'p + grad_f'*p s.t. grad_c'*p + c_val = 0\n            p = np.zeros(2)\n            \n            # Check if the unconstrained step p = -grad_f is feasible\n            if np.dot(grad_c, -grad_f) + c_val = 0:\n                p = -grad_f\n            else:\n                # Constraint is active, solve for p on the boundary\n                grad_c_norm_sq = np.dot(grad_c, grad_c)\n                # QP multiplier nu\n                nu = (c_val - np.dot(grad_c, grad_f)) / grad_c_norm_sq\n                p = -grad_f - nu * grad_c\n\n            # 4. Determine search direction s (with optional SOC)\n            s = p\n            if use_soc:\n                x_plus = x + p\n                c_val_plus, grad_c_plus = constraint_func(x_plus, kappa)\n\n                if c_val_plus > 0:\n                    # Compute minimum-norm correction step d\n                    grad_c_plus_norm_sq = np.dot(grad_c_plus, grad_c_plus)\n                    if grad_c_plus_norm_sq > 1e-12:\n                        d = -(c_val_plus / grad_c_plus_norm_sq) * grad_c_plus\n                        s = p + d\n            \n            # 5. Backtracking Line Search\n            alpha = 1.0\n            phi_x = f_val + mu * max(0, c_val)\n            # Predicted directional derivative for Armijo condition\n            dir_deriv = np.dot(grad_f, p) - mu * max(0, c_val)\n            \n            if dir_deriv >= 0: # Should not happen for a valid descent direction\n                # Failsafe: if p is not a descent direction, terminate.\n                 return max_iter\n            \n            # Smallest allowed step length\n            alpha_min = 1e-10 \n            \n            while alpha > alpha_min:\n\n                x_new = x + alpha * s\n                phi_new = merit_func(x_new, kappa, mu)\n\n                if phi_new = phi_x + sigma * alpha * dir_deriv:\n                    break # Armijo condition satisfied\n                \n                alpha *= beta\n            else: # Loop finished without break (alpha became too small)\n                return max_iter # Line search failed\n            \n            # 6. Update iterate\n            x = x + alpha * s\n            \n        return max_iter\n        \n    # --- Main Experiment ---\n    kappa_values = [0.0, 0.5, 2.0, 5.0, 10.0]\n    improvements = []\n    \n    for kappa in kappa_values:\n        n_base = sqp_solver(kappa, use_soc=False)\n        n_soc = sqp_solver(kappa, use_soc=True)\n        delta = n_base - n_soc\n        improvements.append(delta)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, improvements))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Our final practice shifts from implementation to conceptual synthesis, challenging you to think like an algorithm designer. This exercise presents a \"reverse-engineering\" scenario: given the classic symptoms of the Maratos effect—the persistent rejection of full steps ($\\alpha_k=1$) near a solution—you must deduce its fundamental cause and identify the most appropriate remedies from a list of possibilities . This test of theoretical understanding is crucial for moving beyond rote application, enabling you to diagnose algorithmic failures and select effective strategies based on first principles.",
            "id": "3147430",
            "problem": "You are given the equality-constrained nonlinear program minimize $f(x)$ subject to $c(x)=0$, where $f:\\mathbb{R}^n\\to\\mathbb{R}$ and $c:\\mathbb{R}^n\\to\\mathbb{R}^m$ are twice continuously differentiable. Assume the Linear Independence Constraint Qualification (LICQ) holds at a local solution $x^*$ with associated Karush–Kuhn–Tucker (KKT) multiplier $\\lambda^*$, and the Second-Order Sufficient Condition (SOSC) holds, i.e., the Hessian of the Lagrangian $\\nabla_{xx}^2 L(x^*,\\lambda^*)$ is positive definite on the tangent space $\\{p:\\nabla c(x^*)p=0\\}$. Consider a Sequential Quadratic Programming (SQP) method that computes the step $d_k$ by solving a quadratic model with linearized constraints $\\nabla c(x_k)d_k=-c(x_k)$, and uses a merit-function backtracking line search on $\\phi_\\rho(x)=f(x)+\\tfrac{1}{2}\\rho\\|c(x)\\|^2$ with some penalty parameter $\\rho0$. In a neighborhood of $x^*$, the line search repeatedly rejects the full step $\\alpha=1$ (i.e., eventually takes $\\alpha_k  1$ infinitely often) even though $x_k\\to x^*$ and $d_k\\to 0$.\n\nUsing only first principles (Taylor expansions, definitions of LICQ, KKT, and SOSC), reverse-engineer what this persistent rejection of $\\alpha=1$ reveals about the curvature of the constraints, and propose algorithmic corrections that can restore local superlinear step acceptance and convergence.\n\nSelect all statements that are most consistent with the above phenomenon.\n\nA. There exists at least one active constraint component $c_i$ and a nonzero tangent vector $p$ with $\\nabla c(x^*)p=0$ such that $p^\\top \\nabla^2 c_i(x^*)\\,p\\neq 0$. The resulting $O(\\|d_k\\|^2)$ feasibility error at $x_k+d_k$ can dominate the predicted model decrease in the merit function and trigger the Maratos effect, causing rejection of $\\alpha=1$ near $x^*$.\n\nB. The repeated rejection of $\\alpha=1$ implies that $\\nabla_{xx}^2 L(x^*,\\lambda^*)$ must be indefinite on the tangent space; otherwise, $\\alpha=1$ would always be accepted sufficiently close to $x^*$.\n\nC. Increasing the penalty parameter to a sufficiently large $\\rho$ guarantees that $\\alpha=1$ will be accepted near $x^*$ without modifying the algorithmic step, thereby eliminating the Maratos effect.\n\nD. Introducing a second-order correction (SOC) step that explicitly compensates the $O(\\|d_k\\|^2)$ constraint violation (for example, by solving $\\nabla c(x_k+d_k)s_k=-c(x_k+d_k)$ and using $d_k+s_k$ in the line search), or employing a filter line search augmented with such SOC steps, restores local superlinear convergence and acceptance of $\\alpha=1$ under LICQ and SOSC.\n\nE. The observed behavior implies that $\\nabla^2 c(x^*)=0$ on the tangent space; the constraints are effectively affine near $x^*$, and the rejections must stem from the objective model rather than constraint curvature.\n\nChoose all that apply.",
            "solution": "The user requires a critical validation of the problem statement, followed by a first-principles derivation of the cause of the described phenomenon (the Maratos effect), and a proposal for algorithmic corrections. Finally, each option must be evaluated.\n\n### Step 1: Extract Givens\n- **Problem Type**: Equality-constrained nonlinear program (NLP).\n- **Objective**: Minimize $f(x)$, where $f: \\mathbb{R}^n \\to \\mathbb{R}$.\n- **Constraints**: $c(x) = 0$, where $c: \\mathbb{R}^n \\to \\mathbb{R}^m$. (Note: the problem states $c: \\mathbb{R}^m \\to \\mathbb{R}^m$, which is a clear typographical error given the use of $x \\in \\mathbb{R}^n$. The context unambiguously implies the domain of $c$ is $\\mathbb{R}^n$.)\n- **Smoothness**: $f$ and $c$ are twice continuously differentiable ($C^2$).\n- **Solution Properties**:\n    - $x^*$ is a local solution.\n    - The Linear Independence Constraint Qualification (LICQ) holds at $x^*$, meaning the constraint gradient vectors $\\{\\nabla c_i(x^*)\\}_{i=1}^m$ are linearly independent, so the Jacobian $\\nabla c(x^*)$ has full row rank $m$.\n    - The Second-Order Sufficient Condition (SOSC) holds at $(x^*, \\lambda^*)$, where $\\lambda^*$ is the Karush-Kuhn-Tucker (KKT) multiplier vector. This means the Hessian of the Lagrangian, $\\nabla_{xx}^2 L(x^*, \\lambda^*) = \\nabla^2 f(x^*) + \\sum_{i=1}^m \\lambda_i^* \\nabla^2 c_i(x^*)$, is positive definite on the tangent space $T(x^*) = \\{p \\in \\mathbb{R}^n \\mid \\nabla c(x^*) p = 0 \\}$.\n- **Algorithm**: A Sequential Quadratic Programming (SQP) method.\n    - **Step Calculation**: The search direction $d_k$ is computed by solving a quadratic program (QP) involving a quadratic model of the Lagrangian and linearized constraints of the form $\\nabla c(x_k) d_k = -c(x_k)$.\n- **Globalization Strategy**: A backtracking line search on the $\\ell_2$ merit function $\\phi_\\rho(x) = f(x) + \\frac{1}{2}\\rho \\|c(x)\\|^2$ for a penalty parameter $\\rho  0$.\n- **Observed Phenomenon**: Near the solution $x^*$, as $x_k \\to x^*$ and $d_k \\to 0$, the line search persistently rejects the full step $\\alpha=1$ (i.e., $\\alpha_k  1$ is taken infinitely often). This prevents superlinear convergence.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Grounding**: The problem is firmly grounded in the established theory of nonlinear constrained optimization. The described phenomenon, known as the Maratos effect, is a classic and well-understood issue in SQP methods. All terms (LICQ, SOSC, SQP, merit functions) are standard.\n2.  **Well-Posedness**: The problem is well-posed. It asks for an explanation of a specific, formalizable algorithmic behavior and for standard remedies, for which definite answers exist based on optimization theory.\n3.  **Objectivity**: The problem is stated in precise, objective, mathematical language.\n4.  **Consistency and Completeness**: The problem setup is internally consistent. The minor typo in the domain of the function $c$ is inconsequential and correctable from context. The provided information (LICQ, SOSC, SQP step definition, merit function) is sufficient to analyze the Maratos effect from first principles.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. We may proceed to the solution.\n\n### First-Principles Analysis of the Maratos Effect\n\nThe core of the issue lies in a mismatch between the progress predicted by the QP subproblem's linearized model and the actual progress measured by the nonlinear merit function $\\phi_\\rho(x)$. The line search condition for accepting a full step ($\\alpha=1$) typically requires a sufficient decrease, e.g., $\\phi_\\rho(x_k+d_k) \\le \\phi_\\rho(x_k) + \\eta \\Delta_k$, where $\\eta \\in (0,1)$ and $\\Delta_k  0$ is a predicted decrease. The rejection of $\\alpha=1$ implies this condition fails; specifically, the actual reduction is insufficient, or there might even be an increase in the merit function value, i.e., $\\phi_\\rho(x_k+d_k)  \\phi_\\rho(x_k)$.\n\nLet's analyze the value of the constraints at the trial point $x_k+d_k$ using a Taylor expansion around $x_k$:\n$$c(x_k+d_k) = c(x_k) + \\nabla c(x_k)d_k + \\frac{1}{2}\\mathcal{C}(x_k, d_k) + O(\\|d_k\\|^3)$$\nwhere $\\mathcal{C}(x_k, d_k)$ is a vector whose $i$-th component is $d_k^\\top \\nabla^2 c_i(x_k) d_k$.\nThe SQP step $d_k$ is defined by the linearized constraint equation $\\nabla c(x_k)d_k = -c(x_k)$. Substituting this into the expansion gives:\n$$c(x_k+d_k) = \\frac{1}{2}\\mathcal{C}(x_k, d_k) + O(\\|d_k\\|^3)$$\nThis is a critical result. The SQP step does not, in general, yield a feasible point. The new constraint violation, $c(x_k+d_k)$, is of order $O(\\|d_k\\|^2)$, and its value is directly determined by the **curvature of the constraints**, encoded in the Hessian terms $\\nabla^2 c_i(x_k)$. If all constraints were affine (i.e., $\\nabla^2 c_i(x) = 0$ for all $i$), then $c(x_k+d_k)$ would be zero (up to higher-order terms), and the step would yield a feasible point. The Maratos effect is thus fundamentally caused by constraint nonlinearity.\n\nThe SQP step $d_k$ is a fast, Newton-like step towards the solution $(x^*, \\lambda^*)$, meaning that $x_k+d_k$ is a superlinearly better approximation to $x^*$ than $x_k$. However, the merit function $\\phi_\\rho(x)$ might not recognize this progress. The algorithm takes a step $d_k$ that improves feasibility from $\\|c(x_k)\\| = O(\\|d_k\\|)$ to $\\|c(x_k+d_k)\\| = O(\\|d_k\\|^2)$. This large reduction in the feasibility term $\\frac{1}{2}\\rho\\|c(x)\\|^2$ is balanced against the change in the objective term, $f(x)$.\n\nThe path from an infeasible point $x_k$ to a nearly-feasible point $x_k+d_k$ on a curved constraint manifold may require moving \"uphill\" with respect to the objective function $f$. This means $f(x_k+d_k)  f(x_k)$ is possible. When this happens, the increase in $f(x)$ can overwhelm the decrease from the feasibility term in $\\phi_\\rho(x)$, causing the merit function to increase, i.e., $\\phi_\\rho(x_k+d_k)  \\phi_\\rho(x_k)$, leading to the rejection of the step. This trade-off is precisely the Maratos effect.\n\n### Algorithmic Corrections\n\nTo overcome this, algorithms must be modified to tolerate the behavior of the SQP step near the solution.\n1.  **Second-Order Correction (SOC)**: After computing the SQP step $d_k$, if the trial point $x_k+d_k$ is not accepted, a correction step $s_k$ is computed to further improve feasibility. This step $s_k$ is typically computed by solving a simplified system based on the residual constraint violation at $x_k+d_k$. A common choice is to solve for $s_k$ from the linear system $\\nabla c(x_k) s_k = -c(x_k+d_k)$. Since $c(x_k+d_k) = O(\\|d_k\\|^2)$, LICQ implies $s_k=O(\\|d_k\\|^2)$. The new trial point is $x_k+d_k+s_k$. The constraint violation at this new point becomes $c(x_k+d_k+s_k) = O(\\|d_k\\|^4)$, a much smaller value. This correction is small enough not to interfere with the desirable properties of $d_k$, but powerful enough to \"bend\" the step back towards the feasible region, ensuring the merit function registers sufficient decrease.\n2.  **Non-monotone line search / Filter Methods**: Instead of requiring a strict decrease in a merit function at every iteration, these methods relax the acceptance criteria. A filter method maintains a set of previous \"unacceptable\" pairs of $(f(x_i), \\|c(x_i)\\|)$ and accepts a new trial point if it is not \"dominated\" by any point in the filter (i.e., it sufficiently improves either the objective or the constraint violation). This approach naturally accommodates the behavior of SQP steps, which might temporarily increase the objective to achieve a large gain in feasibility. To ensure convergence, filter methods are often augmented with SOC steps when the SQP step is rejected.\n\n### Evaluation of Options\n\n**A. There exists at least one active constraint component $c_i$ and a nonzero tangent vector $p$ with $\\nabla c(x^*)p=0$ such that $p^\\top \\nabla^2 c_i(x^*)\\,p\\neq 0$. The resulting $O(\\|d_k\\|^2)$ feasibility error at $x_k+d_k$ can dominate the predicted model decrease in the merit function and trigger the Maratos effect, causing rejection of $\\alpha=1$ near $x^*$.**\nThis statement is perfectly aligned with our first-principles analysis. The condition $p^\\top \\nabla^2 c_i(x^*)p \\neq 0$ for a tangent vector $p$ is the mathematical expression for the constraints being curved at the solution. This curvature leads to a residual constraint violation $c(x_k+d_k)$ of order $O(\\|d_k\\|^2)$, which in turn can cause the merit function to reject the unit step. This is the precise mechanism of the Maratos effect.\n**Verdict: Correct.**\n\n**B. The repeated rejection of $\\alpha=1$ implies that $\\nabla_{xx}^2 L(x^*,\\lambda^*)$ must be indefinite on the tangent space; otherwise, $\\alpha=1$ would always be accepted sufficiently close to $x^*$.**\nThis statement is false. The problem explicitly assumes that the Second-Order Sufficient Condition (SOSC) holds, meaning $\\nabla_{xx}^2 L(x^*, \\lambda^*)$ is *positive definite* on the tangent space. The Maratos effect is a phenomenon that occurs *despite* the problem satisfying the standard conditions (LICQ and SOSC) for rapid local convergence. The failure is not in the local optimality conditions but in the globalization mechanism (the merit function line search).\n**Verdict: Incorrect.**\n\n**C. Increasing the penalty parameter to a sufficiently large $\\rho$ guarantees that $\\alpha=1$ will be accepted near $x^*$ without modifying the algorithmic step, thereby eliminating the Maratos effect.**\nWhile it is true that for any given step $k$, a sufficiently large $\\rho$ can force a decrease in $\\phi_\\rho$, it is not a robust or guaranteed solution. The required value of $\\rho$ may need to grow unbounded as $k \\to \\infty$. Furthermore, a very large $\\rho$ leads to an ill-conditioned Hessian of the merit function, which cripples the algorithm's performance far from the solution. The optimization community does not consider this a reliable fix. The term \"guarantees\" makes this statement definitively false.\n**Verdict: Incorrect.**\n\n**D. Introducing a second-order correction (SOC) step that explicitly compensates the $O(\\|d_k\\|^2)$ constraint violation (for example, by solving $\\nabla c(x_k+d_k)s_k=-c(x_k+d_k)$ and using $d_k+s_k$ in the line search), or employing a filter line search augmented with such SOC steps, restores local superlinear convergence and acceptance of $\\alpha=1$ under LICQ and SOSC.**\nThis statement accurately describes the two primary, provably effective remedies for the Maratos effect. The Second-Order Correction step is specifically designed to reduce the $O(\\|d_k\\|^2)$ constraint violation to a higher order ($O(\\|d_k\\|^4)$), which resolves the conflict with the merit function. Filter methods provide an alternative globalization strategy that is not susceptible to the Maratos effect in the same way, and they are often fortified with SOC steps. These are the standard, state-of-the-art techniques.\n**Verdict: Correct.**\n\n**E. The observed behavior implies that $\\nabla^2 c(x^*)=0$ on the tangent space; the constraints are effectively affine near $x^*$, and the rejections must stem from the objective model rather than constraint curvature.**\nThis statement is the logical opposite of the truth. As shown in the first-principles analysis, if the constraints were affine ($\\nabla^2 c_i=0$), the SQP step would produce a new point that is perfectly feasible (i.e., $c(x_k+d_k)=0$). In this scenario, the Maratos effect would not occur. The effect is fundamentally and necessarily a consequence of non-zero constraint curvature.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}