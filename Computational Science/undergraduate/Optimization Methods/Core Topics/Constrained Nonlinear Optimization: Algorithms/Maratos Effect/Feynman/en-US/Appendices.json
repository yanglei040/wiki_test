{
    "hands_on_practices": [
        {
            "introduction": "This first exercise puts you in the driver's seat to witness the Maratos effect firsthand. You will implement a fundamental Sequential Quadratic Programming (SQP) step combined with a line search on an exact penalty merit function. By applying your code to a simple problem with a curved constraint , you will discover how the algorithm, despite approaching the optimal solution, begins to reject the full, efficient step, forcing it to take smaller, less productive ones. This counter-intuitive behavior is the classic symptom of the Maratos effect, and observing it numerically is a crucial first step to understanding why it occurs.",
            "id": "3147380",
            "problem": "Consider the equality-constrained nonlinear optimization problem with objective function $f(x) = x_1^2 + x_2^2$ and nonlinear equality constraint $c(x) = x_2 - \\sin x_1 = 0$. The Karush-Kuhn-Tucker (KKT) point is the solution $x^\\star$ satisfying first-order necessary conditions for optimality under equality constraints. Sequential Quadratic Programming (SQP) methods construct a quadratic subproblem by linearizing the constraints and using a symmetric positive definite approximation to the Lagrangian Hessian. The resulting step $p_k$ is then used in a line search with a merit function to enforce progress and handle constraints. Near $x^\\star$, the Maratos effect describes a phenomenon where the full SQP step $p_k$ is rejected by a merit-function-based line search due to second-order constraint curvature, even though $p_k$ is a valid Newton-like step that would lead to fast local convergence if accepted.\n\nStarting from the fundamental definitions:\n- The objective gradient is $\\nabla f(x) = \\begin{bmatrix}2 x_1 \\\\ 2 x_2\\end{bmatrix}$.\n- The constraint Jacobian is $\\nabla c(x) = \\begin{bmatrix}-\\cos x_1  1\\end{bmatrix}$.\n- The exact penalty merit function is $\\phi_\\sigma(x) = f(x) + \\sigma \\lvert c(x) \\rvert$, where $\\sigma > 0$ is the penalty parameter.\n- The Armijo sufficient decrease condition requires $\\phi_\\sigma(x + \\alpha p_k) \\le \\phi_\\sigma(x) + m \\alpha d_\\phi(x; p_k)$, where $m \\in (0, 1)$ is the sufficient decrease parameter, $\\alpha \\in (0, 1]$ is the step length, and $d_\\phi(x; p_k)$ is the directional derivative of $\\phi_\\sigma$ at $x$ along $p_k$.\n- For the exact penalty merit function, the directional derivative along $p$ at a point $x$ is defined by $d_\\phi(x; p) = \\nabla f(x)^\\top p + \\sigma \\operatorname{sgn}(c(x)) \\nabla c(x) p$, where $\\operatorname{sgn}$ is the sign function and is taken to be $0$ when $c(x)=0$.\n\nThe SQP step $p_k$ at a point $x_k$ is defined as the solution of the quadratic programming subproblem with linearized constraints:\n- Minimize $q(p) = \\nabla f(x_k)^\\top p + \\tfrac{1}{2} p^\\top B_k p$ subject to $\\nabla c(x_k) p + c(x_k) = 0$,\nwhere $B_k$ is a symmetric positive definite matrix approximating the Lagrangian Hessian. In this problem, use $B_k = 2 I$, where $I$ is the identity matrix. The optimality conditions of this quadratic program yield the linear KKT system\n$$\n\\begin{bmatrix}\nB_k  \\nabla c(x_k)^\\top \\\\\n\\nabla c(x_k)  0\n\\end{bmatrix}\n\\begin{bmatrix}\np_k \\\\ \\lambda_k\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\n\\nabla f(x_k) \\\\ c(x_k)\n\\end{bmatrix}.\n$$\n\nImplement a backtracking line search along $p_k$ using the exact penalty merit function $\\phi_\\sigma$, with the Armijo parameters $m = 10^{-4}$ and backtracking factor $\\rho = \\tfrac{1}{2}$, and penalty parameter $\\sigma = 1000$. Begin with $\\alpha = 1$ and repeatedly multiply $\\alpha$ by $\\rho$ until the Armijo condition holds, or until a maximum of $50$ backtracking steps is reached. If the directional derivative $d_\\phi(x_k; p_k) \\ge 0$, the step is not a descent direction for the merit function; in that case, report $\\alpha = 0$.\n\nYour program must:\n- Compute the SQP step $p_k$ at each provided test point $x_k$ by solving the KKT system with $B_k = 2 I$ and $\\nabla c(x_k)$ defined above.\n- Perform the backtracking line search along $p_k$ with the exact penalty merit function $\\phi_\\sigma$ and Armijo condition.\n- Return, for each test point, the accepted step length $\\alpha$.\n\nTest suite:\n1. $x_k = (0.1, \\sin(0.1))$ (feasible near the KKT point).\n2. $x_k = (0.05, \\sin(0.05))$ (feasible closer to the KKT point).\n3. $x_k = (0.01, \\sin(0.01))$ (feasible very close to the KKT point).\n4. $x_k = (0.1, \\sin(0.1) + 10^{-3})$ (slightly infeasible near the KKT point).\n5. $x_k = (10^{-6}, \\sin(10^{-6}))$ (feasible extremely close to the KKT point).\n\nFinal output format:\n- Your program should produce a single line of output containing the accepted step lengths for the five test cases, as a comma-separated list enclosed in square brackets, in the same order as the test suite (for example, $[\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4,\\alpha_5]$). Each $\\alpha_i$ must be a floating-point number.\n\nThe design intentionally explores the Maratos effect: for feasible iterates near the KKT point, the linearized constraint makes $\\nabla c(x_k) p_k + c(x_k) = 0$, so first-order feasibility is preserved along $p_k$. However, the true constraint $c(x)$ exhibits curvature, and $c(x_k + \\alpha p_k)$ contains second-order terms that scale as $\\mathcal{O}(\\alpha^2)$, which can cause the merit function $\\phi_\\sigma$ to increase and violate the Armijo condition for $\\alpha = 1$, thus rejecting the full SQP step despite the near-optimality of $p_k$. Your implementation must numerically detect this behavior via the computed accepted step lengths.",
            "solution": "The problem is valid. It is a well-defined numerical exercise in the field of nonlinear optimization, specifically designed to demonstrate the Maratos effect within the context of Sequential Quadratic Programming (SQP) methods. All necessary functions, parameters, and procedures are provided, and the problem is mathematically and algorithmically sound.\n\n**1. Theoretical Framework**\n\nThe problem asks to analyze the behavior of a line search procedure for an SQP method on a specific nonlinear constrained optimization problem. The core components are:\n\n- **Objective function**: $f(x) = x_1^2 + x_2^2$\n- **Equality constraint**: $c(x) = x_2 - \\sin x_1 = 0$\n\nThe optimal solution to this problem is $x^\\star=(0, 0)$, where the objective function is minimized.\n\n**2. The SQP Method**\n\nSQP methods iteratively solve the optimization problem by forming and solving a quadratic programming (QP) subproblem at each iterate $x_k$. The QP subproblem is constructed by using a quadratic model of the Lagrangian function and linearizing the constraints.\n\nThe step $p_k$ is found by solving:\n$$\n\\min_{p} \\quad \\nabla f(x_k)^\\top p + \\frac{1}{2} p^\\top B_k p\n$$\n$$\n\\text{s.t.} \\quad \\nabla c(x_k) p + c(x_k) = 0\n$$\n\nHere, $B_k=2I$ is the given positive definite approximation to the Hessian of the Lagrangian. The solution to this QP subproblem is given by the Karush-Kuhn-Tucker (KKT) linear system:\n$$\n\\begin{bmatrix}\nB_k  \\nabla c(x_k)^\\top \\\\\n\\nabla c(x_k)  0\n\\end{bmatrix}\n\\begin{bmatrix}\np_k \\\\ \\lambda_k\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\n\\nabla f(x_k) \\\\ c(x_k)\n\\end{bmatrix}\n$$\nwhere $\\lambda_k$ is the Lagrange multiplier estimate for the constraints. For our specific problem, this system is:\n$$\n\\begin{bmatrix}\n2  0  -\\cos x_{k,1} \\\\\n0  2  1 \\\\\n-\\cos x_{k,1}  1  0\n\\end{bmatrix}\n\\begin{bmatrix}\np_{k,1} \\\\ p_{k,2} \\\\ \\lambda_k\n\\end{bmatrix}\n=\n-\n\\begin{bmatrix}\n2x_{k,1} \\\\\n2x_{k,2} \\\\\nx_{k,2} - \\sin x_{k,1}\n\\end{bmatrix}\n$$\nThis $3 \\times 3$ linear system is solved at each test point $x_k$ to find the SQP step $p_k$.\n\n**3. The Merit Function and Line Search**\n\nTo ensure that the step $p_k$ makes progress towards the constrained optimum, a merit function is used. The merit function combines the objective and constraint violation into a single value. The problem specifies the exact penalty merit function:\n$$\n\\phi_\\sigma(x) = f(x) + \\sigma |c(x)|\n$$\nwith a penalty parameter $\\sigma=1000$. A line search is performed along the direction $p_k$ to find a step length $\\alpha \\in (0, 1]$ that provides sufficient decrease in this merit function. The new iterate would be $x_{k+1} = x_k + \\alpha p_k$.\n\nThe sufficient decrease is governed by the Armijo condition:\n$$\n\\phi_\\sigma(x_k + \\alpha p_k) \\le \\phi_\\sigma(x_k) + m \\alpha d_\\phi(x_k; p_k)\n$$\nwhere $m = 10^{-4}$ is a small constant and $d_\\phi(x_k; p_k)$ is the directional derivative of $\\phi_\\sigma$ at $x_k$ along $p_k$, defined as:\n$$\nd_\\phi(x_k; p_k) = \\nabla f(x_k)^\\top p_k + \\sigma \\operatorname{sgn}(c(x_k)) \\nabla c(x_k) p_k\n$$\nThe line search procedure is a backtracking algorithm:\n1.  Initialize $\\alpha = 1$.\n2.  If the Armijo condition is satisfied, accept this $\\alpha$.\n3.  If not, reduce the step length by a factor $\\rho = 1/2$ (i.e., $\\alpha \\leftarrow \\alpha \\rho$) and repeat, up to a maximum of $50$ iterations.\n\nA crucial prerequisite is that $p_k$ must be a descent direction for the merit function, i.e., $d_\\phi(x_k; p_k)  0$. If $d_\\phi(x_k; p_k) \\ge 0$, no positive step length can satisfy the Armijo condition, and we report $\\alpha = 0$.\n\n**4. The Maratos Effect**\n\nThe Maratos effect is a phenomenon where a \"good\" step (like the full SQP step, $\\alpha=1$) is rejected by the line search, forcing small, inefficient steps. This typically occurs near the solution $x^\\star$ when the iterate $x_k$ lies on or very close to the constraint manifold ($c(x_k) \\approx 0$).\n\nFrom the QP subproblem, the step $p_k$ satisfies the linearized constraint $\\nabla c(x_k) p_k + c(x_k) = 0$. If $x_k$ is feasible ($c(x_k)=0$), then $\\nabla c(x_k) p_k = 0$. This means the step $p_k$ is tangent to the constraint manifold at $x_k$, to first order. However, the constraint $c(x)$ is nonlinear (curved). When we take the step, the new point $x_k + p_k$ may move away from the true constraint manifold. A Taylor expansion of $c(x_k+p_k)$ reveals this:\n$$\nc(x_k+p_k) \\approx c(x_k) + \\nabla c(x_k) p_k + \\frac{1}{2} p_k^\\top \\nabla^2 c(x_k) p_k = \\frac{1}{2} p_k^\\top \\nabla^2 c(x_k) p_k\n$$\nThis second-order term, representing the constraint's curvature, means $|c(x_k+p_k)| \\ne 0$. For a large penalty parameter $\\sigma$, the increase in the merit function due to this new constraint violation, $\\sigma |c(x_k+p_k)|$, can overwhelm the decrease achieved in the objective function, causing the Armijo condition to fail for $\\alpha=1$. The algorithm is then forced to cut the step length, destroying the fast quadratic convergence rate of the full-step SQP method.\n\n**5. Implementation Algorithm**\n\nFor each test point $x_k=(x_{k,1}, x_{k,2})$:\n1.  **Calculate problem functions**: Compute $c(x_k) = x_{k,2} - \\sin(x_{k,1})$, $\\nabla f(x_k) = [2x_{k,1}, 2x_{k,2}]^\\top$, and $\\nabla c(x_k) = [-\\cos(x_{k,1}), 1]$.\n2.  **Solve for the SQP step**: Construct the KKT matrix $A$ and right-hand side vector $b$ and solve the linear system $A z = b$ for $z=[p_k^\\top, \\lambda_k]^\\top$.\n3.  **Compute directional derivative**: Calculate $d_\\phi(x_k; p_k) = \\nabla f(x_k)^\\top p_k + \\sigma \\operatorname{sgn}(c(x_k)) (\\nabla c(x_k) p_k)$.\n4.  **Check for descent direction**: If $d_\\phi(x_k; p_k) \\ge 0$, set the accepted step length $\\alpha_k = 0$ and terminate for this test case.\n5.  **Perform backtracking line search**:\n    - Initialize $\\alpha = 1.0$.\n    - Calculate the initial merit function value $\\phi_{\\text{current}} = f(x_k) + \\sigma|c(x_k)|$.\n    - Loop for a maximum of $50$ iterations:\n        - Let $x_{\\text{new}} = x_k + \\alpha p_k$.\n        - Calculate the new merit function value $\\phi_{\\text{new}} = f(x_{\\text{new}}) + \\sigma|c(x_{\\text{new}})|$.\n        - Check if $\\phi_{\\text{new}} \\le \\phi_{\\text{current}} + m \\alpha d_\\phi(x_k; p_k)$.\n        - If the condition holds, break the loop and accept the current $\\alpha$.\n        - Otherwise, update $\\alpha \\leftarrow \\alpha \\times 0.5$.\n6.  **Store result**: The final value of $\\alpha$ is the accepted step length $\\alpha_k$ for the test point $x_k$.\n\nThis procedure is applied to each of the five test cases provided. The results will numerically demonstrate the Maratos effect: for points close to the solution on the feasible manifold, the full step $\\alpha=1$ will be rejected, but as the points get extremely close, the second-order curvature term becomes small enough for the full step to be accepted. For an infeasible point, the step aims to reduce infeasibility, which often helps satisfy the Armijo condition.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the accepted Armijo step length for a series of test cases\n    in an SQP framework, demonstrating the Maratos effect.\n    \"\"\"\n    \n    # Define optimization problem parameters and functions\n    sigma = 1000.0\n    m = 1e-4\n    rho = 0.5\n    max_backtrack_steps = 50\n\n    def f(x):\n        \"\"\"Objective function f(x) = x1^2 + x2^2\"\"\"\n        return x[0]**2 + x[1]**2\n\n    def grad_f(x):\n        \"\"\"Gradient of the objective function\"\"\"\n        return np.array([2 * x[0], 2 * x[1]])\n\n    def c(x):\n        \"\"\"Equality constraint c(x) = x2 - sin(x1)\"\"\"\n        return x[1] - np.sin(x[0])\n\n    def grad_c(x):\n        \"\"\"Jacobian of the equality constraint\"\"\"\n        return np.array([-np.cos(x[0]), 1.0])\n    \n    def merit_function(x_val, sigma_val):\n        \"\"\"Exact penalty merit function\"\"\"\n        return f(x_val) + sigma_val * np.abs(c(x_val))\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.1, np.sin(0.1)),\n        (0.05, np.sin(0.05)),\n        (0.01, np.sin(0.01)),\n        (0.1, np.sin(0.1) + 1e-3),\n        (1e-6, np.sin(1e-6))\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x_k = np.array(case)\n        \n        # 1. Calculate values at the current iterate x_k\n        grad_f_k = grad_f(x_k)\n        c_k = c(x_k)\n        grad_c_k = grad_c(x_k)\n        \n        # 2. Construct and solve the KKT system for the SQP step p_k\n        # [ B_k   grad_c(x_k)^T ] [ p_k     ] = - [ grad_f(x_k) ]\n        # [ grad_c(x_k)    0    ] [ lambda_k]   - [   c(x_k)    ]\n        # B_k = 2*I\n        \n        kkt_matrix = np.array([\n            [2.0, 0.0, grad_c_k[0]],\n            [0.0, 2.0, grad_c_k[1]],\n            [grad_c_k[0], grad_c_k[1], 0.0]\n        ])\n        \n        rhs_vector = -np.array([grad_f_k[0], grad_f_k[1], c_k])\n        \n        try:\n            kkt_solution = np.linalg.solve(kkt_matrix, rhs_vector)\n        except np.linalg.LinAlgError:\n            # Handle cases where the matrix might be singular, though not expected for this problem.\n            results.append(0.0) # No valid step can be computed\n            continue\n\n        p_k = kkt_solution[:2]\n        \n        # 3. Compute directional derivative of the merit function\n        d_phi = grad_f_k @ p_k + sigma * np.sign(c_k) * (grad_c_k @ p_k)\n        \n        # 4. Check if p_k is a descent direction\n        if d_phi >= 0:\n            results.append(0.0)\n            continue\n            \n        # 5. Perform backtracking line search\n        alpha = 1.0\n        current_phi = merit_function(x_k, sigma)\n        \n        accepted_alpha = alpha\n        for _ in range(max_backtrack_steps + 1): # +1 to handle the initial alpha=1 check\n            x_new = x_k + alpha * p_k\n            new_phi = merit_function(x_new, sigma)\n            \n            armijo_condition = new_phi = current_phi + m * alpha * d_phi\n            \n            if armijo_condition:\n                accepted_alpha = alpha\n                break\n            \n            alpha *= rho\n        else: # Loop completed without break\n            accepted_alpha = alpha / rho # Use the last successful alpha value. Here it means the smallest.\n                                         # But problem wants the value that satisfies it or the last one after 50 steps\n                                         # Last alpha isrho^50. So alpha is the final one.\n            accepted_alpha = alpha\n            \n        results.append(accepted_alpha)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having observed the Maratos effect, we now turn to investigating one of the key mechanisms behind it: the merit function. A merit function is designed to balance the competing goals of reducing the objective function and satisfying the constraints, using a penalty parameter $\\mu$ to weigh the importance of constraint violations. This exercise  challenges you to explore this delicate trade-off by observing how a large penalty parameter can paradoxically worsen convergence, making the Maratos effect more severe and highlighting the conflict between the linearized model and the true nonlinear problem.",
            "id": "3147344",
            "problem": "Consider the constrained nonlinear optimization problem in two variables with one inequality constraint. The objective is to minimize the smooth function $f:\\mathbb{R}^2\\to\\mathbb{R}$ subject to a smooth inequality constraint $g:\\mathbb{R}^2\\to\\mathbb{R}$. The fundamental base for this problem is the Karush–Kuhn–Tucker (KKT) optimality conditions, the definition of the exact penalty merit function, and the Armijo backtracking line search, which together underpin Sequential Quadratic Programming (SQP) methods.\n\nYou are to implement a simplified Sequential Quadratic Programming iteration using the following well-tested definitions and facts:\n\n1. The objective function is\n$$\nf(x) = (x_1 - 1)^2 + (x_2 - 1)^2,\n$$\nwith gradient\n$$\n\\nabla f(x) = \\begin{bmatrix} 2(x_1 - 1) \\\\ 2(x_2 - 1) \\end{bmatrix}.\n$$\n\n2. The inequality constraint is\n$$\ng(x) = x_1^2 + x_2^2 - 1 \\leq 0,\n$$\nwith gradient\n$$\n\\nabla g(x) = \\begin{bmatrix} 2x_1 \\\\ 2x_2 \\end{bmatrix},\n$$\nand Hessian\n$$\n\\nabla^2 g(x) = 2 I,\n$$\nwhere $I$ is the $2\\times 2$ identity matrix.\n\n3. The exact $\\ell_1$ penalty merit function is\n$$\n\\phi_\\mu(x) = f(x) + \\mu \\max(0, g(x)),\n$$\nwith penalty parameter $\\mu > 0$. This exact penalty is known to regularize constraint violations in line searches.\n\n4. In each iteration, you must compute a search direction $p_k$ by solving the quadratic model with identity Hessian (i.e., a Gauss–Newton-like approximation) and linearized constraint:\n   - Let $B = I$ and $a_k = \\nabla g(x_k)$, $g_k = g(x_k)$, $c_k = \\nabla f(x_k)$. The unconstrained quadratic minimizer is\n     $$\n     p^{\\text{unc}}_k = - B^{-1} c_k = - c_k.\n     $$\n   - If the current point is strictly feasible and the unconstrained step maintains linearized feasibility, i.e.,\n     $$\n     g_k  0 \\quad \\text{and} \\quad a_k^\\top p^{\\text{unc}}_k + g_k \\le 0,\n     $$\n     then take $p_k = p^{\\text{unc}}_k$ (constraint predicted inactive).\n   - Otherwise, enforce the linearized constraint at equality and minimize the quadratic model:\n     $$\n     \\min_{p} \\;\\; \\tfrac{1}{2} p^\\top B p + c_k^\\top p \\quad \\text{subject to} \\quad a_k^\\top p + g_k = 0.\n     $$\n     Using KKT conditions with $B=I$, the solution is\n     $$\n     p_k = -c_k - \\lambda_k a_k, \\quad \\text{where} \\quad \\lambda_k = \\frac{-a_k^\\top c_k + g_k}{a_k^\\top a_k}.\n     $$\n     Note that when $g_k = 0$, this yields $a_k^\\top p_k = 0$, i.e., a tangential step.\n\n5. Perform an Armijo backtracking line search on the merit function:\n   - Initialize $\\alpha_k = 1$, with backtracking factor $\\tau \\in (0,1)$ and sufficient decrease constant $\\sigma \\in (0,1)$.\n   - Define the directional derivative of the merit function at $x_k$ along $p_k$ as\n     $$\n     d\\phi_\\mu(x_k; p_k) = \\nabla f(x_k)^\\top p_k + \n     \\begin{cases}\n     \\mu \\, a_k^\\top p_k,  \\text{if } g_k > 0, \\\\\n     \\mu \\, \\max(0, a_k^\\top p_k),  \\text{if } g_k = 0, \\\\\n     0,  \\text{if } g_k  0.\n     \\end{cases}\n     $$\n   - Accept $\\alpha_k$ when\n     $$\n     \\phi_\\mu(x_k + \\alpha_k p_k) \\le \\phi_\\mu(x_k) + \\sigma \\alpha_k \\, d\\phi_\\mu(x_k; p_k).\n     $$\n     Otherwise, set $\\alpha_k \\leftarrow \\tau \\alpha_k$ and repeat until the condition holds or $\\alpha_k$ becomes smaller than a fixed minimal threshold. Finally, set $x_{k+1} = x_k + \\alpha_k p_k$.\n\nThis setup gives rise to the Maratos effect when the constraint is curved and the step computed by enforcing only linearized feasibility (e.g., tangential at the boundary) triggers second-order constraint violations. At a feasible boundary point $g(x_k)=0$ and $a_k^\\top p_k = 0$, the exact merit function includes a quadratic penalty term proportional to $\\mu \\alpha_k^2 p_k^\\top \\nabla^2 g(x_k) p_k$ that is not visible to the linearized model. This often causes the Armijo rule to shrink $\\alpha_k$ dramatically, yielding a characteristic pattern of small step sizes and potential zig-zagging as the inequality constraint switches between active ($g(x_k) \\ge 0$) and inactive ($g(x_k)  0$) across iterations.\n\nYour task is to implement the above iteration for a fixed number of iterations and report the line search step sizes $\\alpha_k$ selected in each iteration for multiple test cases. Use the following test suite:\n\n- Use $\\sigma = 10^{-4}$ and $\\tau = 0.5$ in all cases.\n- Run $K = 6$ iterations for each case.\n- Case A (happy path near the boundary with moderate penalty): $\\mu = 10$, $x_0 = (\\cos(0.6), \\sin(0.6))$.\n- Case B (large penalty making the Maratos effect severe): $\\mu = 100$, $x_0 = (\\cos(0.6), \\sin(0.6))$.\n- Case C (smaller penalty and different boundary angle): $\\mu = 2$, $x_0 = (\\cos(1.0), \\sin(1.0))$.\n\nYour program must compute, for each case, the sequence $\\{\\alpha_0, \\alpha_1, \\ldots, \\alpha_5\\}$ resulting from the Armijo merit line search described above. The final output must be a single line containing all $18$ values in a single comma-separated list enclosed in square brackets, ordered by case, i.e., first Case A’s six values, then Case B’s six values, then Case C’s six values:\n$$\n[\\alpha_0^{(A)}, \\alpha_1^{(A)}, \\ldots, \\alpha_5^{(A)}, \\alpha_0^{(B)}, \\ldots, \\alpha_5^{(B)}, \\alpha_0^{(C)}, \\ldots, \\alpha_5^{(C)}].\n$$\n\nNo physical units are involved in this problem. Angles (used only to define initial points) are in radians. Each reported $\\alpha_k$ must be a floating-point number. The program must be self-contained and produce exactly one line of output in the format specified above, with no additional text.",
            "solution": "The user has provided a valid, well-posed problem in the field of numerical optimization.\n\nThe problem requires the implementation of a simplified Sequential Quadratic Programming (SQP) algorithm to solve a constrained nonlinear optimization problem. The task is to minimize an objective function $f(x)$ subject to an inequality constraint $g(x) \\le 0$, where $x \\in \\mathbb{R}^2$.\n\nThe objective function is a smooth quadratic function given by:\n$$f(x) = (x_1 - 1)^2 + (x_2 - 1)^2$$\nIts gradient is:\n$$\\nabla f(x) = \\begin{bmatrix} 2(x_1 - 1) \\\\ 2(x_2 - 1) \\end{bmatrix}$$\n\nThe inequality constraint defines a feasible region that is a disk of radius $1$ centered at the origin:\n$$g(x) = x_1^2 + x_2^2 - 1 \\le 0$$\nThe gradient and Hessian of the constraint function are given by:\n$$\\nabla g(x) = \\begin{bmatrix} 2x_1 \\\\ 2x_2 \\end{bmatrix}, \\quad \\nabla^2 g(x) = \\begin{bmatrix} 2  0 \\\\ 0  2 \\end{bmatrix} = 2I$$\nwhere $I$ is the $2 \\times 2$ identity matrix.\n\nThe core of the method is an iterative process where, at each step $k$, a search direction $p_k$ is computed from the current point $x_k$, and the next iterate is found by $x_{k+1} = x_k + \\alpha_k p_k$. The step size $\\alpha_k$ is determined by a line search procedure to ensure progress toward the solution.\n\nFirst, we detail the computation of the search direction $p_k$. At each iterate $x_k$, we solve a Quadratic Programming (QP) subproblem that models the original problem. The QP subproblem minimizes a quadratic approximation of the Lagrangian function, subject to a linearization of the constraints. For this problem, a simplified Hessian of the Lagrangian, $B=I$, is used. We use the notation $c_k = \\nabla f(x_k)$, $g_k = g(x_k)$, and $a_k = \\nabla g(x_k)$.\n\nThe search direction $p_k$ is determined by one of two cases:\n1.  If the current point $x_k$ is strictly feasible ($g_k  0$) and a full step in the unconstrained direction $p^{\\text{unc}}_k = -B^{-1}c_k = -c_k$ is predicted to maintain feasibility in the linearized model (i.e., $a_k^\\top p^{\\text{unc}}_k + g_k \\le 0$), then the constraint is considered inactive. The search direction is taken as this unconstrained Newton-like step:\n    $$p_k = p^{\\text{unc}}_k = -c_k$$\n2.  Otherwise, the constraint is considered active. The search direction $p_k$ is found by minimizing the quadratic model subject to the linearized constraint being satisfied with equality:\n    $$\\min_{p} \\;\\; \\tfrac{1}{2} p^\\top I p + c_k^\\top p \\quad \\text{subject to} \\quad a_k^\\top p + g_k = 0$$\n    The solution to this equality-constrained QP is derived from its Karush-Kuhn-Tucker (KKT) conditions, yielding:\n    $$p_k = -c_k - \\lambda_k a_k, \\quad \\text{where} \\quad \\lambda_k = \\frac{-a_k^\\top c_k + g_k}{a_k^\\top a_k}$$\n    This provides a step that is a combination of a step towards the unconstrained minimum and a step controlled by the constraint gradient. When $x_k$ is on the boundary ($g_k=0$), this step becomes tangential to the constraint ($a_k^\\top p_k = 0$).\n\nNext, to ensure global convergence, a line search is performed using the exact $\\ell_1$ penalty merit function, defined as:\n$$\\phi_\\mu(x) = f(x) + \\mu \\max(0, g(x))$$\nwhere $\\mu > 0$ is the penalty parameter. This function balances decreasing the objective function with satisfying the constraints.\n\nThe line search is an Armijo backtracking procedure. Starting with a trial step size of $\\alpha_k = 1$, we seek the largest $\\alpha_k$ in the sequence $\\{1, \\tau, \\tau^2, \\ldots\\}$ that satisfies the sufficient decrease condition:\n$$\\phi_\\mu(x_k + \\alpha_k p_k) \\le \\phi_\\mu(x_k) + \\sigma \\alpha_k \\, d\\phi_\\mu(x_k; p_k)$$\nHere, $\\sigma \\in (0,1)$ is a small constant (given as $10^{-4}$), $\\tau \\in (0,1)$ is the backtracking factor (given as $0.5$), and $d\\phi_\\mu(x_k; p_k)$ is the directional derivative of the non-smooth merit function $\\phi_\\mu$ at $x_k$ along $p_k$:\n$$d\\phi_\\mu(x_k; p_k) = \\nabla f(x_k)^\\top p_k + \\begin{cases} \\mu \\, \\nabla g(x_k)^\\top p_k,  \\text{if } g(x_k) > 0 \\\\ \\mu \\, \\max(0, \\nabla g(x_k)^\\top p_k),  \\text{if } g(x_k) = 0 \\\\ 0,  \\text{if } g(x_k)  0 \\end{cases}$$\n\nThis problem is set up to demonstrate the Maratos effect. This effect occurs when an iterate $x_k$ is near the solution on a curved constraint boundary. A tangential step $p_k$ (i.e., $a_k^\\top p_k \\approx 0$), while satisfying the linearized constraint, can violate the true nonlinear constraint due to its curvature. A Taylor expansion of $g(x_k + \\alpha_k p_k)$ shows this:\n$$g(x_k + \\alpha_k p_k) \\approx g(x_k) + \\alpha_k a_k^\\top p_k + \\frac{1}{2}\\alpha_k^2 p_k^\\top \\nabla^2 g(x_k) p_k$$\nIf $g_k \\approx 0$ and $a_k^\\top p_k \\approx 0$, this simplifies to $g(x_k + \\alpha_k p_k) \\approx \\frac{1}{2}\\alpha_k^2 p_k^\\top \\nabla^2 g(x_k) p_k$. Given $\\nabla^2 g(x) = 2I$, which is positive definite, this term is positive. The merit function value $\\phi_\\mu(x_k + \\alpha_k p_k)$ increases significantly due to the penalty term $\\mu \\cdot g(x_k + \\alpha_k p_k)$, causing the Armijo condition to fail for $\\alpha_k=1$. The line search is then forced to choose a very small step size $\\alpha_k$, drastically slowing convergence. The effect is more pronounced for larger penalty parameters $\\mu$, as tested in Case B versus Case A.\n\nThe implementation will execute this algorithm for $K=6$ iterations for each of three test cases defined by the initial point $x_0$ and the penalty parameter $\\mu$. For each iteration $k=0, \\ldots, 5$, the accepted step size $\\alpha_k$ is recorded. The final output will be the concatenated list of these step sizes.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a simplified SQP iteration to solve a constrained nonlinear\n    optimization problem and demonstrates the Maratos effect.\n    \"\"\"\n    # Global parameters as defined in the problem statement\n    SIGMA = 1e-4\n    TAU = 0.5\n    K_ITER = 6\n    ALPHA_MIN_THRESHOLD = 1e-12  # Safeguard for the line search loop\n\n    # Define the objective function and its gradient\n    def f(x):\n        return (x[0] - 1)**2 + (x[1] - 1)**2\n\n    def grad_f(x):\n        return np.array([2 * (x[0] - 1), 2 * (x[1] - 1)])\n\n    # Define the constraint function and its gradient\n    def g(x):\n        return x[0]**2 + x[1]**2 - 1\n\n    def grad_g(x):\n        return np.array([2 * x[0], 2 * x[1]])\n\n    def run_one_case(x0, mu):\n        \"\"\"\n        Runs the SQP algorithm for one test case.\n        \n        Args:\n            x0 (tuple or list): The initial point (x1, x2).\n            mu (float): The penalty parameter.\n\n        Returns:\n            list: A list of the accepted step sizes (alpha) for each iteration.\n        \"\"\"\n        x_k = np.array(x0, dtype=float)\n        alphas = []\n\n        for _ in range(K_ITER):\n            # 1. Calculate values at the current iterate x_k\n            f_k = f(x_k)\n            g_k = g(x_k)\n            c_k = grad_f(x_k)\n            a_k = grad_g(x_k)\n\n            # 2. Compute the search direction p_k\n            p_unc = -c_k\n            # Check condition for unconstrained step\n            if g_k  -1e-9 and (a_k @ p_unc + g_k) = 0:\n                p_k = p_unc\n            else:\n                # Compute constrained step\n                a_k_dot_a_k = a_k @ a_k\n                # Denominator is 4*(x1^2+x2^2), only zero at origin where g  0 anyway.\n                # A safeguard is good practice but not strictly required here.\n                if a_k_dot_a_k  1e-12:\n                    lambda_k = 0.0\n                else:\n                    lambda_k = (-a_k @ c_k + g_k) / a_k_dot_a_k\n                p_k = -c_k - lambda_k * a_k\n\n            # 3. Perform Armijo backtracking line search on the merit function\n            phi_k = f_k + mu * max(0, g_k)\n\n            # Calculate the directional derivative of the merit function\n            if g_k > 1e-9:  # Using tolerance for float comparison\n                d_phi_k = c_k @ p_k + mu * (a_k @ p_k)\n            elif g_k  -1e-9:\n                d_phi_k = c_k @ p_k\n            else:  # g_k is approximately 0\n                d_phi_k = c_k @ p_k + mu * max(0, a_k @ p_k)\n\n            alpha = 1.0\n            while True:\n                x_new = x_k + alpha * p_k\n                f_new = f(x_new)\n                g_new = g(x_new)\n                phi_new = f_new + mu * max(0, g_new)\n\n                # Armijo condition\n                if phi_new = phi_k + SIGMA * alpha * d_phi_k:\n                    break\n\n                alpha *= TAU\n\n                # Safeguard against infinitely small steps\n                if alpha  ALPHA_MIN_THRESHOLD:\n                    break\n            \n            # 4. Store result and update for next iteration\n            alphas.append(alpha)\n            x_k = x_k + alpha * p_k\n        \n        return alphas\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (mu, angle for x0)\n        (10, 0.6),  # Case A\n        (100, 0.6), # Case B\n        (2, 1.0),   # Case C\n    ]\n\n    results = []\n    for mu_val, angle in test_cases:\n        x0_val = (np.cos(angle), np.sin(angle))\n        case_alphas = run_one_case(x0_val, mu_val)\n        results.extend(case_alphas)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Observing and understanding a problem is critical, but the ultimate goal of an optimization practitioner is to solve it. In this final practice, you will implement a powerful and widely used remedy for the Maratos effect: the Second-Order Correction (SOC) . By computing a small, additional step that accounts for constraint curvature, the SOC method helps the main SQP step satisfy the merit function's descent criteria. Your task is to build both a baseline and an SOC-enhanced solver, allowing you to directly quantify the dramatic improvement in convergence speed and robustness that this correction provides.",
            "id": "3147419",
            "problem": "Implement a numerical experiment to diagnose and mitigate the Maratos effect within Sequential Quadratic Programming (SQP) for a family of smooth nonlinear inequality constrained optimization problems. Your program must implement two solvers: a baseline SQP with a backtracking line search on an exact penalty merit function, and an SQP enhanced with a Second-Order Correction (SOC) that compensates for curvature-induced linearization errors. Compare their iteration counts on a parameterized curved constraint and report the improvement in iteration count due to SOC as a function of the constraint curvature parameter.\n\nThe optimization problem is parameterized by a nonnegative curvature parameter $\\,\\kappa \\in \\mathbb{R}_{\\ge 0}\\,$:\n- Decision variable: $\\,x = (x_1,x_2) \\in \\mathbb{R}^2$.\n- Objective: minimize $\\,f(x) = \\tfrac{1}{2}(x_1 - 1)^2 + \\tfrac{1}{2} x_2^2$.\n- Single inequality constraint: $\\,c_\\kappa(x) \\le 0\\,$ with $\\,c_\\kappa(x) = \\tfrac{1}{2}\\kappa x_1^2 - x_2$.\n\nInterpretation: the feasible set is $\\,\\{x \\in \\mathbb{R}^2 \\,:\\, x_2 \\ge \\tfrac{1}{2}\\kappa x_1^2\\}\\,$, i.e., the region above a parabola whose curvature increases with $\\,\\kappa\\,$. The unconstrained minimizer is at $\\,x^\\star = (1,0)\\,$ and becomes infeasible as $\\,\\kappa$ increases.\n\nYou must use the following fundamental base and core definitions to justify your algorithmic design:\n- Karush–Kuhn–Tucker (KKT) conditions for inequality constrained smooth optimization define first-order necessary conditions at a local minimizer: stationarity $\\,\\nabla f(x) + \\lambda \\nabla c_\\kappa(x) = 0\\,$ for some multiplier $\\,\\lambda \\ge 0\\,$, primal feasibility $\\,c_\\kappa(x) \\le 0\\,$, dual feasibility $\\,\\lambda \\ge 0\\,$, and complementarity $\\,\\lambda\\,c_\\kappa(x) = 0$.\n- Sequential Quadratic Programming (SQP) constructs steps by solving a quadratic approximation of the Lagrangian subject to the linearized constraints at the current iterate.\n- The Maratos effect refers to the phenomenon where an SQP step that is linearly feasible (to first order) becomes infeasible when evaluated in the original nonlinear constraints due to second-order terms in the constraint Taylor expansion, yielding poor merit function decrease and causing line search rejection with many backtracking reductions.\n- Second-order correction (SOC) remedies this by adding a small correction computed at the trial point to restore linearized feasibility there, counteracting the second-order residual.\n\nImplementation requirements:\n- Start every run from the same initial point $\\,x^{(0)} = (0.5,\\,0.0)\\,$.\n- Use the identity matrix $\\,B = I\\,$ as the Hessian approximation in the QP subproblem.\n- At each outer iteration, form and solve the one-constraint QP subproblem that minimizes the quadratic model of $\\,f\\,$ subject to the linearization of $\\,c_\\kappa\\,$ at the current iterate. Use the exact solution for the single-inequality case by checking whether the unconstrained step is linearly feasible; otherwise, enforce the linearized constraint as active.\n- Use a backtracking line search on the exact penalty merit function $\\,\\phi(x) = f(x) + \\mu \\max\\{0, c_\\kappa(x)\\}\\,$ with fixed penalty weight $\\,\\mu = 100\\,$. Use an Armijo condition with sufficient decrease parameter $\\,\\sigma = 10^{-4}\\,$ and backtracking factor $\\,\\beta = 0.5\\,$.\n- For the SOC-enhanced variant, after computing the QP step $\\,p\\,$ at $\\,x\\,$, form the trial point $\\,x^+ = x + p\\,$ and, if $\\,c_\\kappa(x^+) > 0\\,$, compute a minimum-norm correction $\\,d\\,$ that restores linearized feasibility at $\\,x^+\\,$ by solving the linearized inequality as an equality: $\\,\\nabla c_\\kappa(x^+)^\\top d = -c_\\kappa(x^+)\\,$. Use the combined step $\\,s = p + d\\,$ for the line search. If $\\,c_\\kappa(x^+) \\le 0\\,$, set $\\,d = 0\\,$ and $\\,s = p\\,$.\n- Terminate when the KKT residual $\\,r(x)\\,$ is below tolerance $\\,\\varepsilon = 10^{-6}\\,$ or when the iteration count reaches a hard cap of $\\,200\\,$. Define the KKT residual as\n$$\nr(x) \\;=\\; \\left\\|\\nabla f(x) + \\lambda^\\star(x)\\,\\nabla c_\\kappa(x)\\right\\|_2 \\;+\\; \\max\\{0, c_\\kappa(x)\\},\n$$\nwhere $\\,\\lambda^\\star(x) = \\max\\left\\{0,\\; -\\dfrac{\\nabla f(x)^\\top \\nabla c_\\kappa(x)}{\\|\\nabla c_\\kappa(x)\\|_2^2}\\right\\}\\,$ is the nonnegative scalar that minimizes $\\,\\left\\|\\nabla f(x) + \\lambda \\nabla c_\\kappa(x)\\right\\|_2\\,$ over $\\,\\lambda \\ge 0\\,$.\n\nTest suite and outputs:\n- Run both the baseline SQP and the SOC-enhanced SQP for each curvature parameter in the list $\\,\\kappa \\in \\{0.0,\\,0.5,\\,2.0,\\,5.0,\\,10.0\\}\\,$.\n- For each $\\,\\kappa\\,$, record the number of outer iterations to convergence for the baseline solver, denoted $\\,N_{\\text{base}}(\\kappa)\\,$, and for the SOC-enhanced solver, denoted $\\,N_{\\text{soc}}(\\kappa)\\,$.\n- For each $\\,\\kappa\\,$, compute the integer improvement $\\,\\Delta(\\kappa) = N_{\\text{base}}(\\kappa) - N_{\\text{soc}}(\\kappa)\\,$. Positive values indicate fewer iterations with SOC; zero indicates no change; negative values indicate SOC required more iterations.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[3,1,0,2,4]\"), corresponding to the values of $\\,\\Delta(\\kappa)\\,$ in the order $\\,\\kappa \\in \\{0.0,\\,0.5,\\,2.0,\\,5.0,\\,10.0\\}\\,$.\n\nNo physical units or angles are involved in this problem; all outputs are unitless integers.",
            "solution": "The user-provided problem is valid. It is a well-posed, scientifically grounded problem in the field of numerical optimization. All necessary definitions, parameters, and algorithmic components are provided, and there are no internal contradictions or ambiguities. The problem is a standard textbook example used to demonstrate the Maratos effect and the efficacy of the Second-Order Correction (SOC) in Sequential Quadratic Programming (SQP).\n\nWe proceed with a detailed solution.\n\n### 1. Problem Formulation and Preliminaries\n\nThe problem is to minimize a smooth nonlinear objective function subject to a single smooth nonlinear inequality constraint. The problem is parameterized by a non-negative curvature parameter $\\kappa \\in \\mathbb{R}_{\\ge 0}$.\n\n- **Objective Function:** $f(x) = \\frac{1}{2}(x_1 - 1)^2 + \\frac{1}{2} x_2^2$, where $x = (x_1, x_2) \\in \\mathbb{R}^2$.\n- **Constraint Function:** $c_\\kappa(x) = \\frac{1}{2}\\kappa x_1^2 - x_2 \\le 0$.\n\nThe gradients of these functions are essential for the SQP algorithm:\n- **Gradient of Objective:** $\\nabla f(x) = \\begin{pmatrix} x_1 - 1 \\\\ x_2 \\end{pmatrix}$.\n- **Gradient of Constraint:** $\\nabla c_\\kappa(x) = \\begin{pmatrix} \\kappa x_1 \\\\ -1 \\end{pmatrix}$.\n\nThe Karush-Kuhn-Tucker (KKT) conditions provide first-order necessary conditions for a point $x^\\star$ to be a local minimizer. There must exist a Lagrange multiplier $\\lambda^\\star \\ge 0$ such that:\n1.  **Stationarity:** $\\nabla f(x^\\star) + \\lambda^\\star \\nabla c_\\kappa(x^\\star) = 0$.\n2.  **Primal Feasibility:** $c_\\kappa(x^\\star) \\le 0$.\n3.  **Dual Feasibility:** $\\lambda^\\star \\ge 0$.\n4.  **Complementarity:** $\\lambda^\\star c_\\kappa(x^\\star) = 0$.\n\n### 2. Sequential Quadratic Programming (SQP) Framework\n\nSQP is an iterative method that models the original nonlinear problem with a sequence of Quadratic Programming (QP) subproblems. At each iterate $x^{(k)}$, we compute a search direction $p^{(k)}$ by solving a QP that consists of a quadratic model of the Lagrangian function subject to a linear model of the constraints.\n\n#### 2.1. The QP Subproblem\n\nAt a given iterate $x^{(k)}$, the QP subproblem is defined as:\n$$\n\\min_{p \\in \\mathbb{R}^2} \\quad \\frac{1}{2} p^\\top B^{(k)} p + \\nabla f(x^{(k)})^\\top p\n$$\n$$\n\\text{s.t.} \\quad \\nabla c_\\kappa(x^{(k)})^\\top p + c_\\kappa(x^{(k)}) \\le 0\n$$\nAs specified, we use the identity matrix for the Hessian approximation, $B^{(k)} = I$. Let $g^{(k)} = \\nabla f(x^{(k)})$, $A^{(k)} = \\nabla c_\\kappa(x^{(k)})^\\top$, and $b^{(k)} = -c_\\kappa(x^{(k)})$. The QP simplifies to:\n$$\n\\min_{p \\in \\mathbb{R}^2} \\quad \\frac{1}{2} p^\\top p + (g^{(k)})^\\top p \\quad \\text{s.t.} \\quad A^{(k)} p \\le b^{(k)}\n$$\nThe unconstrained minimizer of the QP objective is $p_{unc} = -g^{(k)}$. We check if this step satisfies the linearized constraint:\n- **Case 1: Constraint Inactive.** If $A^{(k)} p_{unc} \\le b^{(k)}$, or equivalently $c_\\kappa(x^{(k)}) \\le \\nabla c_\\kappa(x^{(k)})^\\top \\nabla f(x^{(k)})$, the unconstrained step is feasible for the QP. The solution is $p^{(k)} = -g^{(k)}$. The QP multiplier is $\\nu^{(k)}=0$.\n- **Case 2: Constraint Active.** If $A^{(k)} p_{unc} > b^{(k)}$, the solution must lie on the boundary of the feasible set, $A^{(k)} p = b^{(k)}$. The solution to this equality-constrained QP is given by $p^{(k)} = -g^{(k)} - \\nu^{(k)} (A^{(k)})^\\top$, where the QP multiplier $\\nu^{(k)}$ is found by substituting $p^{(k)}$ into the active constraint:\n$$\n\\nu^{(k)} = \\frac{-b^{(k)} - A^{(k)} g^{(k)}}{A^{(k)} (A^{(k)})^\\top} = \\frac{c_\\kappa(x^{(k)}) - \\nabla c_\\kappa(x^{(k)})^\\top \\nabla f(x^{(k)})}{\\|\\nabla c_\\kappa(x^{(k)})\\|_2^2}\n$$\nThe search direction is then $p^{(k)} = -\\nabla f(x^{(k)}) - \\nu^{(k)} \\nabla c_\\kappa(x^{(k)})$. Since we are in this case, the numerator is positive, ensuring $\\nu^{(k)} > 0$.\n\n#### 2.2. Globalization via Merit Function and Line Search\n\nThe step $p^{(k)}$ provides a good local search direction. To ensure global convergence, we perform a backtracking line search to find a step length $\\alpha^{(k)} \\in (0, 1]$ that yields a sufficient decrease in a merit function. We use the exact penalty (or $l_1$) merit function:\n$$\n\\phi(x) = f(x) + \\mu \\max\\{0, c_\\kappa(x)\\}\n$$\nwith a penalty parameter $\\mu = 100$. The line search aims to satisfy the Armijo condition:\n$$\n\\phi(x^{(k)} + \\alpha s^{(k)}) \\le \\phi(x^{(k)}) + \\sigma \\alpha D^{(k)}\n$$\nwhere $s^{(k)}$ is the search direction (to be defined), $\\sigma=10^{-4}$ is the sufficient decrease constant, and $D^{(k)}$ is the predicted directional derivative. A standard choice for $D^{(k)}$ in this context is based on the linearized model that motivates the SQP step:\n$$\nD^{(k)} = \\nabla f(x^{(k)})^\\top p^{(k)} - \\mu \\max\\{0, c_\\kappa(x^{(k)})\\}\n$$\nFor the baseline SQP algorithm, the search direction is simply the QP solution, $s^{(k)} = p^{(k)}$.\n\n### 3. The Maratos Effect and Second-Order Correction (SOC)\n\nThe Maratos effect occurs when an iterate is close to the optimal solution but not on the constraint boundary. The SQP step $p^{(k)}$ provides an excellent move towards the optimum, reducing the Lagrangian model. However, due to the curvature of the true constraint $c_\\kappa$, the trial point $x^{(k+1)} = x^{(k)} + p^{(k)}$ may land far outside the feasible set, i.e., $c_\\kappa(x^{(k+1)}) > 0$. This increase in constraint violation can outweigh the decrease in the objective function $f$, causing the merit function $\\phi$ to increase. The line search rejects the full step ($\\alpha=1$) and requires multiple backtracking steps, severely slowing down the rate of convergence. This effect is more pronounced for larger $\\kappa$, which corresponds to higher constraint curvature.\n\nThe Second-Order Correction (SOC) is a remedy. After computing the primary step $p^{(k)}$, we form a trial point $x^+ = x^{(k)} + p^{(k)}$. We then evaluate the true constraint violation $c_\\kappa(x^+)$ at this point. If $c_\\kappa(x^+) > 0$, we compute a \"correction\" step $d^{(k)}$ to restore feasibility, at least to first order. This correction is the minimum-norm vector that satisfies the linearized constraint equation at $x^+$:\n$$\n\\nabla c_\\kappa(x^+)^\\top d = -c_\\kappa(x^+)\n$$\nThe solution to this minimum-norm problem is:\n$$\nd^{(k)} = -\\frac{c_\\kappa(x^+)}{\\|\\nabla c_\\kappa(x^+)\\|_2^2} \\nabla c_\\kappa(x^+)\n$$\nThe final search direction for the line search is the composite step $s^{(k)} = p^{(k)} + d^{(k)}$. This new direction incorporates the second-order information about the constraint (via evaluation at $x^+$) and aims to satisfy the Armijo condition with $\\alpha=1$, thus restoring fast local convergence. If $c_\\kappa(x^+) \\le 0$, no correction is needed, and we set $s^{(k)} = p^{(k)}$.\n\n### 4. Termination Criterion\n\nThe algorithm terminates when the KKT residual falls below a tolerance $\\varepsilon = 10^{-6}$. The residual $r(x)$ measures the violation of the KKT conditions:\n$$\nr(x) = \\underbrace{\\|\\nabla f(x) + \\lambda^\\star(x)\\,\\nabla c_\\kappa(x)\\|_2}_{\\text{Stationarity error}} + \\underbrace{\\max\\{0, c_\\kappa(x)\\}}_{\\text{Feasibility error}}\n$$\nwhere $\\lambda^\\star(x)$ is an estimate of the Lagrange multiplier. It is chosen as the non-negative least-squares solution for the multiplier that minimizes the stationarity part of the residual:\n$$\n\\lambda^\\star(x) = \\arg\\min_{\\lambda \\ge 0} \\left\\|\\nabla f(x) + \\lambda\\,\\nabla c_\\kappa(x)\\right\\|_2^2\n$$\nSolving this single-variable non-negatively constrained least squares problem yields:\n$$\n\\lambda^\\star(x) = \\max\\left\\{0,\\; -\\frac{\\nabla f(x)^\\top \\nabla c_\\kappa(x)}{\\|\\nabla c_\\kappa(x)\\|_2^2}\\right\\}\n$$\nA hard limit of $200$ iterations is also imposed.\n\n### 5. Algorithmic Summary\n\n**Baseline SQP and SOC-SQP Algorithm:**\n\n1.  Initialize $k=0$, $x^{(0)} = (0.5, 0.0)$, $\\mu=100$, $\\sigma=10^{-4}$, $\\beta=0.5$, $\\varepsilon=10^{-6}$.\n2.  **For** $k = 0, 1, 2, \\dots, 199$:\n    a.  Compute $f(x^{(k)})$, $\\nabla f(x^{(k)})$, $c_\\kappa(x^{(k)})$, and $\\nabla c_\\kappa(x^{(k)})$.\n    b.  Compute the KKT residual $r(x^{(k)})$. If $r(x^{(k)})  \\varepsilon$, terminate and return $k$.\n    c.  Solve the QP subproblem to find the step $p^{(k)}$ and the QP multiplier $\\nu^{(k)}$.\n    d.  **Set search direction $s^{(k)}$**:\n        -   **Baseline:** $s^{(k)} = p^{(k)}$.\n        -   **SOC-Enhanced:**\n            i.   Set trial point $x^+ = x^{(k)} + p^{(k)}$.\n            ii.  If $c_\\kappa(x^+) > 0$, compute $d^{(k)} = -\\frac{c_\\kappa(x^+)}{\\|\\nabla c_\\kappa(x^+)\\|_2^2} \\nabla c_\\kappa(x^+)$ and set $s^{(k)} = p^{(k)} + d^{(k)}$.\n            iii. Else, set $s^{(k)} = p^{(k)}$.\n    e.  **Backtracking Line Search**:\n        i.   Initialize step length $\\alpha = 1$.\n        ii.  Compute current merit value $\\phi(x^{(k)})$.\n        iii. Compute predicted derivative $D^{(k)} = \\nabla f(x^{(k)})^\\top p^{(k)} - \\mu \\max\\{0, c_\\kappa(x^{(k)})\\} $.\n        iv.  **While** $\\phi(x^{(k)} + \\alpha s^{(k)}) > \\phi(x^{(k)}) + \\sigma \\alpha D^{(k)}$:\n             $\\alpha \\leftarrow \\beta \\alpha$.\n    f.  **Update:** $x^{(k+1)} = x^{(k)} + \\alpha s^{(k)}$.\n3.  If loop finishes, terminate and return $200$.\n\nThe experiment will execute this procedure for both the baseline and SOC-enhanced versions across the specified range of $\\kappa$ values and report the difference in the number of iterations required for convergence.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a numerical experiment to diagnose and mitigate the Maratos effect\n    within Sequential Quadratic Programming (SQP) for a family of smooth nonlinear\n    inequality constrained optimization problems.\n    \"\"\"\n\n    # --- Problem Definition ---\n    def objective_func(x):\n        \"\"\"Computes the objective function value and its gradient.\"\"\"\n        f_val = 0.5 * (x[0] - 1.0)**2 + 0.5 * x[1]**2\n        grad_f = np.array([x[0] - 1.0, x[1]])\n        return f_val, grad_f\n\n    def constraint_func(x, kappa):\n        \"\"\"Computes the constraint function value and its gradient.\"\"\"\n        c_val = 0.5 * kappa * x[0]**2 - x[1]\n        grad_c = np.array([kappa * x[0], -1.0])\n        return c_val, grad_c\n\n    def merit_func(x, kappa, mu):\n        \"\"\"Computes the exact penalty merit function.\"\"\"\n        f_val, _ = objective_func(x)\n        c_val, _ = constraint_func(x, kappa)\n        return f_val + mu * max(0, c_val)\n\n    def kkt_residual(x, kappa):\n        \"\"\"Computes the KKT residual for termination checking.\"\"\"\n        _, grad_f = objective_func(x)\n        c_val, grad_c = constraint_func(x, kappa)\n        \n        grad_c_norm_sq = np.dot(grad_c, grad_c)\n        \n        # This clause handles the case where grad_c is the zero vector, which should not happen here.\n        if grad_c_norm_sq  1e-12:\n            lambda_star = 0.0\n        else:\n            lambda_star = max(0, -np.dot(grad_f, grad_c) / grad_c_norm_sq)\n\n        stationarity_err = np.linalg.norm(grad_f + lambda_star * grad_c)\n        feasibility_err = max(0, c_val)\n        \n        return stationarity_err + feasibility_err\n\n    # --- SQP Solver ---\n    def sqp_solver(kappa, use_soc):\n        \"\"\"\n        Solves the optimization problem for a given kappa using SQP.\n        \n        Args:\n            kappa (float): The curvature parameter.\n            use_soc (bool): If True, enables the Second-Order Correction.\n            \n        Returns:\n            int: The number of iterations to convergence.\n        \"\"\"\n        \n        # Parameters\n        x = np.array([0.5, 0.0])\n        mu = 100.0\n        sigma = 1e-4\n        beta = 0.5\n        tol = 1e-6\n        max_iter = 200\n\n        for i in range(max_iter):\n            # 1. Check for termination\n            if kkt_residual(x, kappa)  tol:\n                return i\n\n            # 2. Evaluate functions and gradients at the current iterate\n            f_val, grad_f = objective_func(x)\n            c_val, grad_c = constraint_func(x, kappa)\n\n            # 3. Solve the QP subproblem for the step p\n            # QP: min 0.5*p'p + grad_f'*p s.t. grad_c'*p + c_val = 0\n            p = np.zeros(2)\n            \n            # Check if the unconstrained step p = -grad_f is feasible\n            if np.dot(grad_c, -grad_f) + c_val = 0:\n                p = -grad_f\n            else:\n                # Constraint is active, solve for p on the boundary\n                grad_c_norm_sq = np.dot(grad_c, grad_c)\n                # QP multiplier nu\n                nu = (c_val - np.dot(grad_c, grad_f)) / grad_c_norm_sq\n                p = -grad_f - nu * grad_c\n\n            # 4. Determine search direction s (with optional SOC)\n            s = p\n            if use_soc:\n                x_plus = x + p\n                c_val_plus, grad_c_plus = constraint_func(x_plus, kappa)\n\n                if c_val_plus > 0:\n                    # Compute minimum-norm correction step d\n                    grad_c_plus_norm_sq = np.dot(grad_c_plus, grad_c_plus)\n                    if grad_c_plus_norm_sq > 1e-12:\n                        d = -(c_val_plus / grad_c_plus_norm_sq) * grad_c_plus\n                        s = p + d\n            \n            # 5. Backtracking Line Search\n            alpha = 1.0\n            phi_x = f_val + mu * max(0, c_val)\n            # Predicted directional derivative for Armijo condition\n            dir_deriv = np.dot(grad_f, p) - mu * max(0, c_val)\n            \n            if dir_deriv >= 0: # Should not happen for a valid descent direction\n                # Failsafe: if p is not a descent direction, terminate.\n                 return max_iter\n            \n            # Smallest allowed step length\n            alpha_min = 1e-10 \n            \n            while alpha > alpha_min:\n\n                x_new = x + alpha * s\n                phi_new = merit_func(x_new, kappa, mu)\n\n                if phi_new = phi_x + sigma * alpha * dir_deriv:\n                    break # Armijo condition satisfied\n                \n                alpha *= beta\n            else: # Loop finished without break (alpha became too small)\n                return max_iter # Line search failed\n            \n            # 6. Update iterate\n            x = x + alpha * s\n            \n        return max_iter\n        \n    # --- Main Experiment ---\n    kappa_values = [0.0, 0.5, 2.0, 5.0, 10.0]\n    improvements = []\n    \n    for kappa in kappa_values:\n        n_base = sqp_solver(kappa, use_soc=False)\n        n_soc = sqp_solver(kappa, use_soc=True)\n        delta = n_base - n_soc\n        improvements.append(delta)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, improvements))}]\")\n\nsolve()\n```"
        }
    ]
}