{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a foundational application of the Lagrangian method to a classic resource allocation problem. We will minimize a simple quadratic cost function subject to a linear budget constraint. This practice is designed to build your core skills in setting up the Lagrangian function and solving the resulting first-order conditions, which in this case form a straightforward system of linear equations .",
            "id": "2216747",
            "problem": "Consider the problem of allocating a fixed, normalized resource between two independent processes. Let $x$ and $y$ represent the fraction of the resource allocated to the first and second process, respectively, such that $x \\geq 0$ and $y \\geq 0$. The total resource must be fully utilized, which imposes the constraint that the allocations must sum to one. The cost associated with this allocation is given by the function $f(x,y) = \\alpha x^2 + \\beta y^2$, where $\\alpha$ and $\\beta$ are given positive real constants representing the cost coefficients for each process.\n\nYour task is to find the optimal allocation $(x^*, y^*)$ that minimizes the total cost, subject to the full utilization constraint. You must also find the value of the Lagrange multiplier, $\\lambda^*$, at this optimal point.\n\nProvide the triplet of values $(x^*, y^*, \\lambda^*)$ in terms of the constants $\\alpha$ and $\\beta$.",
            "solution": "The problem is to minimize the objective function $f(x,y) = \\alpha x^2 + \\beta y^2$ subject to the equality constraint $g(x,y) = x+y-1=0$. We can solve this using the method of Lagrange multipliers.\n\nFirst, we define the Lagrangian function $\\mathcal{L}(x, y, \\lambda)$, which is given by:\n$$\n\\mathcal{L}(x, y, \\lambda) = f(x,y) - \\lambda g(x,y)\n$$\nSubstituting the given functions, we get:\n$$\n\\mathcal{L}(x, y, \\lambda) = \\alpha x^2 + \\beta y^2 - \\lambda(x+y-1)\n$$\nTo find the stationary points of the Lagrangian, which correspond to the constrained extrema of the objective function, we must find the points where the gradient of $\\mathcal{L}$ is zero. This gives us a system of three equations, obtained by taking the partial derivatives of $\\mathcal{L}$ with respect to $x$, $y$, and $\\lambda$ and setting them to zero. These are the first-order necessary conditions for optimality.\n\n1.  The partial derivative with respect to $x$:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial x} = 2\\alpha x - \\lambda = 0\n    $$\n    From this equation, we can express $x$ in terms of $\\lambda$:\n    $$\n    x = \\frac{\\lambda}{2\\alpha}\n    $$\n\n2.  The partial derivative with respect to $y$:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial y} = 2\\beta y - \\lambda = 0\n    $$\n    Similarly, we express $y$ in terms of $\\lambda$:\n    $$\n    y = \\frac{\\lambda}{2\\beta}\n    $$\n\n3.  The partial derivative with respect to $\\lambda$:\n    $$\n    \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = -(x+y-1) = 0\n    $$\n    This simply recovers our original constraint equation:\n    $$\n    x+y=1\n    $$\n\nNow, we have a system of three linear equations for the three unknowns $x$, $y$, and $\\lambda$. We can solve this system by substituting the expressions for $x$ and $y$ from the first two equations into the third equation.\n\nSubstituting $x = \\frac{\\lambda}{2\\alpha}$ and $y = \\frac{\\lambda}{2\\beta}$ into $x+y=1$:\n$$\n\\frac{\\lambda}{2\\alpha} + \\frac{\\lambda}{2\\beta} = 1\n$$\nFactor out $\\lambda$ from the left-hand side:\n$$\n\\lambda \\left( \\frac{1}{2\\alpha} + \\frac{1}{2\\beta} \\right) = 1\n$$\nTo solve for $\\lambda$, we first find a common denominator for the terms in the parenthesis:\n$$\n\\lambda \\left( \\frac{\\beta + \\alpha}{2\\alpha\\beta} \\right) = 1\n$$\nNow, we can isolate $\\lambda$ to find its optimal value, $\\lambda^*$:\n$$\n\\lambda^* = \\frac{2\\alpha\\beta}{\\alpha+\\beta}\n$$\nWith the value of $\\lambda^*$ found, we can now find the optimal allocations $x^*$ and $y^*$ by substituting $\\lambda^*$ back into the expressions for $x$ and $y$.\n\nFor $x^*$:\n$$\nx^* = \\frac{\\lambda^*}{2\\alpha} = \\frac{1}{2\\alpha} \\left( \\frac{2\\alpha\\beta}{\\alpha+\\beta} \\right) = \\frac{\\beta}{\\alpha+\\beta}\n$$\n\nFor $y^*$:\n$$\ny^* = \\frac{\\lambda^*}{2\\beta} = \\frac{1}{2\\beta} \\left( \\frac{2\\alpha\\beta}{\\alpha+\\beta} \\right) = \\frac{\\alpha}{\\alpha+\\beta}\n$$\nSince $\\alpha$ and $\\beta$ are positive constants, it follows that $x^* > 0$ and $y^* > 0$, satisfying the non-negativity conditions mentioned in the problem. The triplet of optimal values is therefore $(x^*, y^*, \\lambda^*) = \\left(\\frac{\\beta}{\\alpha+\\beta}, \\frac{\\alpha}{\\alpha+\\beta}, \\frac{2\\alpha\\beta}{\\alpha+\\beta}\\right)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\beta}{\\alpha+\\beta} & \\frac{\\alpha}{\\alpha+\\beta} & \\frac{2\\alpha\\beta}{\\alpha+\\beta} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having mastered the basic mechanics, we now turn to a problem with a strong geometric flavor: finding the point on a curve that is closest to the origin. This exercise involves minimizing a quadratic objective (the squared distance) subject to a non-linear constraint defining a hyperbola. This practice will help you visualize the interplay between an objective function and a constraint surface, reinforcing the principle that the gradient of the objective must be proportional to the gradient of the constraint at the optimal point .",
            "id": "2216754",
            "problem": "In a materials science laboratory, researchers are developing a new composite material by combining two substances, Substance X and Substance Y. Let $x$ and $y$ represent the positive, dimensionless quantities of Substance X and Substance Y used in the mixture, respectively. Due to the chemical kinetics of the synthesis process, the quantities must satisfy the strict relationship $xy = 18$. The structural integrity of the final composite is found to be optimal when a quality metric, $Q(x,y) = x^2 + y^2$, is minimized. Your task is to determine the exact quantities of Substance X and Substance Y that should be used to achieve this optimal structural integrity.\n\nPresent your answer as a pair of values $(x, y)$.",
            "solution": "We are asked to minimize the function $Q(x,y) = x^{2} + y^{2}$ subject to the constraint $xy = 18$ with $x>0$ and $y>0$. This is a constrained optimization problem, which we solve using Lagrange multipliers.\n\nDefine $f(x,y) = x^{2} + y^{2}$ and the constraint function $g(x,y) = xy - 18 = 0$. The method of Lagrange multipliers sets\n$$\n\\nabla f(x,y) = \\lambda \\nabla g(x,y),\n$$\nwhich yields the system\n$$\n\\frac{\\partial f}{\\partial x} = 2x = \\lambda \\frac{\\partial g}{\\partial x} = \\lambda y, \\quad \\frac{\\partial f}{\\partial y} = 2y = \\lambda \\frac{\\partial g}{\\partial y} = \\lambda x,\n$$\ntogether with the constraint\n$$\nxy = 18.\n$$\nFrom $2x = \\lambda y$ we obtain $\\lambda = \\frac{2x}{y}$, and from $2y = \\lambda x$ we obtain $\\lambda = \\frac{2y}{x}$. Equating these expressions for $\\lambda$ gives\n$$\n\\frac{2x}{y} = \\frac{2y}{x} \\;\\;\\Longrightarrow\\;\\; x^{2} = y^{2}.\n$$\nSince $x>0$ and $y>0$, it follows that $x = y$. Substituting into the constraint $xy = 18$ yields\n$$\nx^{2} = 18 \\;\\;\\Longrightarrow\\;\\; x = \\sqrt{18}, \\quad y = \\sqrt{18}.\n$$\nTo confirm this is a minimum, note that for positive $x,y$ with fixed product $xy=18$, the inequality $(x-y)^{2} \\geq 0$ implies $x^{2} + y^{2} \\geq 2xy = 36$, with equality if and only if $x=y$. Thus the found point achieves the global minimum of $Q$ under the constraint.\n\nTherefore, the exact quantities are $x = \\sqrt{18} = 3\\sqrt{2}$ and $y = \\sqrt{18} = 3\\sqrt{2}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 3\\sqrt{2} & 3\\sqrt{2} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Most real-world optimization problems involve not just equalities but also inequalities, such as resource limitations or non-negativity requirements. This exercise introduces the full power of the Lagrangian framework by incorporating inequality constraints, leading to the Karush-Kuhn-Tucker (KKT) conditions. You will learn to identify which constraints are \"active\" (binding) at the optimum and use the concept of complementary slackness to find the solution, a critical skill for tackling more complex, realistic optimization scenarios .",
            "id": "3192411",
            "problem": "Consider the convex quadratic program with two decision variables $x_1$ and $x_2$:\nminimize $f(x_1,x_2) = (x_1 - 3.5)^2 + (x_2 - 1.5)^2$ subject to the four inequality constraints\n$g_1(x_1,x_2) = x_1 + x_2 - 3 \\leq 0$, $g_2(x_1,x_2) = x_1 - 2 \\leq 0$, $g_3(x_1,x_2) = -x_1 \\leq 0$, and $g_4(x_1,x_2) = -x_2 \\leq 0$.\nFormulate the Lagrangian function using nonnegative multipliers $\\lambda_1$, $\\lambda_2$, $\\lambda_3$, and $\\lambda_4$ associated with $g_1$, $g_2$, $g_3$, and $g_4$, respectively. Using first principles for convex optimization, derive the necessary optimality conditions and solve for the optimal primal variables $(x_1^{\\star}, x_2^{\\star})$ and optimal multipliers $(\\lambda_1^{\\star}, \\lambda_2^{\\star}, \\lambda_3^{\\star}, \\lambda_4^{\\star})$. Identify which constraints bind by interpreting the sign and magnitude of the optimal multipliers and verify your identification using complementary slackness. Express your final answer as a single row vector $\\begin{pmatrix} x_1^{\\star} & x_2^{\\star} & \\lambda_1^{\\star} & \\lambda_2^{\\star} & \\lambda_3^{\\star} & \\lambda_4^{\\star} \\end{pmatrix}$.",
            "solution": "The problem is to solve a convex quadratic program. We start by formulating the Lagrangian function $L(x_1, x_2, \\boldsymbol{\\lambda})$ for the problem:\n$$L(x_1, x_2, \\boldsymbol{\\lambda}) = (x_1 - 3.5)^2 + (x_2 - 1.5)^2 + \\lambda_1(x_1 + x_2 - 3) + \\lambda_2(x_1 - 2) + \\lambda_3(-x_1) + \\lambda_4(-x_2)$$\nwhere $\\boldsymbol{\\lambda} = (\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4)$ are non-negative Lagrange multipliers.\n\nSince the problem is convex, the Karush-Kuhn-Tucker (KKT) conditions are necessary and sufficient for optimality. These conditions are:\n1.  **Stationarity**: $\\nabla_{\\mathbf{x}} L = 0$\n    $$ \\frac{\\partial L}{\\partial x_1} = 2(x_1 - 3.5) + \\lambda_1 + \\lambda_2 - \\lambda_3 = 0 $$\n    $$ \\frac{\\partial L}{\\partial x_2} = 2(x_2 - 1.5) + \\lambda_1 - \\lambda_4 = 0 $$\n2.  **Primal Feasibility**: $g_i(\\mathbf{x}) \\leq 0$ for $i=1,..,4$.\n3.  **Dual Feasibility**: $\\lambda_i \\geq 0$ for $i=1,..,4$.\n4.  **Complementary Slackness**: $\\lambda_i g_i(\\mathbf{x}) = 0$ for $i=1,..,4$.\n\nThe unconstrained minimum of the objective function is at $(3.5, 1.5)$. This point is not feasible as it violates the first two constraints ($x_1+x_2-3 \\leq 0$ and $x_1-2 \\leq 0$). This suggests that the optimal solution lies on the boundary of the feasible region, likely where these constraints are active.\n\nLet's assume constraints $g_1$ and $g_2$ are active, meaning they hold with equality:\n$$ x_1 + x_2 - 3 = 0 $$\n$$ x_1 - 2 = 0 $$\nSolving this system gives $x_1^{\\star} = 2$ and $x_2^{\\star} = 1$. This point $(2,1)$ is feasible.\n\nSince $x_1^{\\star} > 0$ and $x_2^{\\star} > 0$, constraints $g_3(-x_1 \\leq 0)$ and $g_4(-x_2 \\leq 0)$ are inactive. By complementary slackness, their corresponding multipliers must be zero: $\\lambda_3^{\\star} = 0$ and $\\lambda_4^{\\star} = 0$.\n\nWe can now find the remaining multipliers $\\lambda_1^{\\star}$ and $\\lambda_2^{\\star}$ using the stationarity conditions with $(x_1^{\\star}, x_2^{\\star})=(2,1)$ and $(\\lambda_3^{\\star}, \\lambda_4^{\\star})=(0,0)$:\n$$ 2(2 - 3.5) + \\lambda_1^{\\star} + \\lambda_2^{\\star} - 0 = 0 \\implies -3 + \\lambda_1^{\\star} + \\lambda_2^{\\star} = 0 $$\n$$ 2(1 - 1.5) + \\lambda_1^{\\star} - 0 = 0 \\implies -1 + \\lambda_1^{\\star} = 0 $$\nFrom the second equation, we get $\\lambda_1^{\\star} = 1$. Substituting this into the first equation gives $-3 + 1 + \\lambda_2^{\\star} = 0$, which yields $\\lambda_2^{\\star} = 2$.\n\nOur solution candidate is $(x_1^{\\star}, x_2^{\\star}) = (2, 1)$ and $(\\lambda_1^{\\star}, \\lambda_2^{\\star}, \\lambda_3^{\\star}, \\lambda_4^{\\star}) = (1, 2, 0, 0)$. We must verify this solution against all KKT conditions.\n- **Primal Feasibility**: The point $(2,1)$ satisfies all four inequalities. (Checked)\n- **Dual Feasibility**: The multipliers $(1, 2, 0, 0)$ are all non-negative. (Checked)\n- **Stationarity**: Our derivation ensures this. (Checked)\n- **Complementary Slackness**:\n  - $\\lambda_1^{\\star}(x_1^{\\star} + x_2^{\\star} - 3) = 1(2+1-3) = 0$. (Checked)\n  - $\\lambda_2^{\\star}(x_1^{\\star} - 2) = 2(2-2) = 0$. (Checked)\n  - $\\lambda_3^{\\star}(-x_1^{\\star}) = 0(-2) = 0$. (Checked)\n  - $\\lambda_4^{\\star}(-x_2^{\\star}) = 0(-1) = 0$. (Checked)\n\nAll KKT conditions are satisfied. Thus, we have found the unique optimal solution. The vector of optimal primal and dual variables is $(2, 1, 1, 2, 0, 0)$. The positive multipliers $\\lambda_1^{\\star}=1$ and $\\lambda_2^{\\star}=2$ confirm that constraints $g_1$ and $g_2$ are indeed binding at the optimum.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 2 & 1 & 1 & 2 & 0 & 0 \\end{pmatrix} } $$"
        }
    ]
}