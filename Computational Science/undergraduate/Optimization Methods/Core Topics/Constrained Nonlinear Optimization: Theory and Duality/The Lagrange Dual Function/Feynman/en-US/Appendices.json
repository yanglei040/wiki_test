{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a hands-on derivation of the Lagrange dual for a classic problem: finding the projection of a point onto an affine subspace. This is a fundamental task in fields from computer graphics to machine learning. By working through this problem (), you will practice the core mechanics of forming the Lagrangian for a problem with equality constraints and finding the dual function by minimizing over the primal variables.",
            "id": "2167450",
            "problem": "Consider the problem of finding the projection of a point onto an affine subspace. This is equivalent to finding the point $x$ in the subspace that is closest in Euclidean distance to a given external point $x_0$. The affine subspace is defined by the set of all points $x \\in \\mathbb{R}^n$ that satisfy the linear system $Ax = b$, where $A$ is a given $m \\times n$ matrix and $b$ is a given vector in $\\mathbb{R}^m$. The external point is $x_0 \\in \\mathbb{R}^n$.\n\nTo find this projection, we solve the following optimization problem:\n$$\n\\begin{array}{ll}\n\\text{minimize}  \\frac{1}{2}\\|x - x_0\\|_2^2 \\\\\n\\text{subject to}  Ax = b\n\\end{array}\n$$\nwhere $\\| \\cdot \\|_2$ denotes the standard Euclidean norm.\n\nYour task is to find the objective function of the Lagrange dual problem. This dual objective function, denoted as $g(\\nu)$, is a function of the Lagrange multiplier vector $\\nu \\in \\mathbb{R}^m$ associated with the equality constraints. Express your answer as a symbolic expression in terms of $\\nu$, $A$, $b$, and $x_0$.",
            "solution": "We form the Lagrangian for the equality-constrained problem. With multiplier $\\nu \\in \\mathbb{R}^{m}$ for the constraint $Ax=b$, the Lagrangian is\n$$\nL(x,\\nu)=\\frac{1}{2}\\|x-x_{0}\\|_{2}^{2}+\\nu^{T}(Ax-b).\n$$\nTo compute the dual function $g(\\nu)=\\inf_{x}L(x,\\nu)$, we minimize $L$ over $x$. The stationarity condition is obtained by setting the gradient with respect to $x$ to zero:\n$$\n\\nabla_{x}L(x,\\nu)=(x-x_{0})+A^{T}\\nu=0 \\quad \\Longrightarrow \\quad x^{\\star}(\\nu)=x_{0}-A^{T}\\nu.\n$$\nSubstituting $x^{\\star}(\\nu)$ back into $L$ gives\n$$\ng(\\nu)=L(x^{\\star}(\\nu),\\nu)=\\frac{1}{2}\\|x_{0}-A^{T}\\nu - x_{0}\\|_{2}^{2}+\\nu^{T}\\big(A(x_{0}-A^{T}\\nu)-b\\big).\n$$\nSimplifying term by term,\n$$\n\\frac{1}{2}\\|x_{0}-A^{T}\\nu - x_{0}\\|_{2}^{2}=\\frac{1}{2}\\|A^{T}\\nu\\|_{2}^{2}=\\frac{1}{2}\\nu^{T}AA^{T}\\nu,\n$$\nand\n$$\n\\nu^{T}\\big(A(x_{0}-A^{T}\\nu)-b\\big)=\\nu^{T}Ax_{0}-\\nu^{T}AA^{T}\\nu-\\nu^{T}b.\n$$\nTherefore,\n$$\ng(\\nu)=\\frac{1}{2}\\nu^{T}AA^{T}\\nu+\\nu^{T}Ax_{0}-\\nu^{T}AA^{T}\\nu-\\nu^{T}b\n=-\\frac{1}{2}\\nu^{T}AA^{T}\\nu+\\nu^{T}(Ax_{0}-b).\n$$\nEquivalently, $g(\\nu)=-\\frac{1}{2}\\|A^{T}\\nu\\|_{2}^{2}+(Ax_{0}-b)^{T}\\nu$.",
            "answer": "$$\\boxed{-\\frac{1}{2}\\nu^{T}AA^{T}\\nu+\\nu^{T}(Ax_{0}-b)}$$"
        },
        {
            "introduction": "While strong duality is a cornerstone of convex optimization, it's crucial to understand the subtleties of what it guarantees. This exercise () presents a simple, one-dimensional convex problem to explore the concept of attainment. You will see firsthand how a primal problem can have an optimal value that is never actually reached by any feasible point, even while its corresponding dual problem has a perfectly well-defined and attainable maximum.",
            "id": "3191751",
            "problem": "Consider the convex optimization problem in one dimension\n$$\n\\begin{aligned}\n\\text{minimize}_{x \\in \\mathbb{R}} \\quad  f(x) \\equiv \\exp(x) \\\\\n\\text{subject to} \\quad  x \\leq 0.\n\\end{aligned}\n$$\nWork from first principles using the definitions of the Lagrangian and the Lagrange dual function.\n\nTasks:\n- Using only the definition of the Lagrangian for inequality constraints, define the Lagrangian and derive explicitly the Lagrange dual function $g(\\lambda)$ for $\\lambda \\geq 0$. Identify all values of $\\lambda$ for which $g(\\lambda)$ is finite.\n- Formulate the dual problem and determine whether a dual maximizer exists. If it does, identify it.\n- Analyze the primal problem to determine its infimum $p^{\\star}$ and whether the infimum is attained by any feasible $x$. Justify your conclusions directly from the properties of $f(x)$ and the feasible set.\n- State the dual optimal value $d^{\\star}$, compare it with $p^{\\star}$, and comment on the duality gap and on attainment in both primal and dual.\n\nAs your final answer, report the dual optimal value $d^{\\star}$ as a single real number. No rounding is required.",
            "solution": "The problem is a convex optimization problem. We validate its structure and content before proceeding.\n\n### Step 1: Extract Givens\n- **Objective function**: $f(x) \\equiv \\exp(x)$\n- **Optimization goal**: minimize\n- **Domain**: $x \\in \\mathbb{R}$\n- **Constraint**: $x \\leq 0$\n- **Tasks**:\n    1. Define the Lagrangian and derive the dual function $g(\\lambda)$ for $\\lambda \\geq 0$. Identify where $g(\\lambda)$ is finite.\n    2. Formulate the dual problem and find the dual maximizer if it exists.\n    3. Analyze the primal problem to find its infimum $p^{\\star}$ and determine if it is attained.\n    4. State the dual optimal value $d^{\\star}$, compare with $p^{\\star}$, and comment on the duality gap and attainment.\n- **Final Answer Requirement**: Report the dual optimal value $d^{\\star}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It is a standard convex optimization problem, as the objective function $f(x) = \\exp(x)$ is convex and the constraint function $c(x) = x$ is affine (and thus convex), defining a convex feasible set. The problem is well-posed, with all necessary information provided, and contains no contradictions or ambiguities. It is a textbook example illustrating the concepts of Lagrange duality.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. We proceed to construct the solution.\n\nThe optimization problem can be written in the standard form as:\n$$\n\\begin{aligned}\n\\text{minimize}_{x \\in \\mathbb{R}} \\quad  f_0(x) \\\\\n\\text{subject to} \\quad  f_1(x) \\leq 0,\n\\end{aligned}\n$$\nwhere the objective function is $f_0(x) = \\exp(x)$ and the inequality constraint function is $f_1(x) = x$.\n\nFirst, we define the Lagrangian and derive the Lagrange dual function. The Lagrangian $L(x, \\lambda)$ is defined as $L(x, \\lambda) = f_0(x) + \\lambda f_1(x)$, where $\\lambda$ is the Lagrange multiplier associated with the inequality constraint. For an inequality constraint of the form $f_1(x) \\leq 0$, the multiplier must be non-negative, i.e., $\\lambda \\geq 0$.\n$$\nL(x, \\lambda) = \\exp(x) + \\lambda x\n$$\nThe Lagrange dual function, $g(\\lambda)$, is defined as the infimum of the Lagrangian over the primal variable $x$:\n$$\ng(\\lambda) = \\inf_{x \\in \\mathbb{R}} L(x, \\lambda) = \\inf_{x \\in \\mathbb{R}} (\\exp(x) + \\lambda x)\n$$\nTo find this infimum, we analyze the function $h(x) = \\exp(x) + \\lambda x$ for a fixed $\\lambda \\geq 0$. We consider two cases for $\\lambda$:\nCase 1: $\\lambda  0$. To find the infimum of $h(x)$, we examine its derivative with respect to $x$:\n$$\n\\frac{\\partial L}{\\partial x} = \\exp(x) + \\lambda\n$$\nSince $\\exp(x)  0$ for all $x \\in \\mathbb{R}$ and we assume $\\lambda  0$, the derivative $\\frac{\\partial L}{\\partial x}$ is strictly positive. This implies that $L(x, \\lambda)$ is a strictly increasing function of $x$. The infimum is thus attained as $x$ approaches $-\\infty$:\n$$\ng(\\lambda) = \\lim_{x \\to -\\infty} (\\exp(x) + \\lambda x) = 0 + \\lambda(-\\infty) = -\\infty\n$$\nCase 2: $\\lambda = 0$. The Lagrangian becomes $L(x, 0) = \\exp(x)$. This is also a strictly increasing function of $x$. Its infimum is:\n$$\ng(0) = \\inf_{x \\in \\mathbb{R}} \\exp(x) = \\lim_{x \\to -\\infty} \\exp(x) = 0\n$$\nCombining these results, the Lagrange dual function for $\\lambda \\geq 0$ is:\n$$\ng(\\lambda) = \\begin{cases} 0  \\text{if } \\lambda=0 \\\\ -\\infty  \\text{if } \\lambda0 \\end{cases}\n$$\nThe dual function $g(\\lambda)$ is finite only for the value $\\lambda = 0$.\n\nSecond, we formulate and solve the dual problem. The dual problem is to maximize the dual function $g(\\lambda)$ subject to the non-negativity constraint on the multiplier:\n$$\n\\begin{aligned}\n\\text{maximize}_{\\lambda} \\quad  g(\\lambda) \\\\\n\\text{subject to} \\quad  \\lambda \\geq 0.\n\\end{aligned}\n$$\nGiven the derived form of $g(\\lambda)$, we are maximizing over the set of values $\\{0, -\\infty, -\\infty, \\dots\\}$. The maximum value is clearly $0$, which is achieved at $\\lambda = 0$. Therefore, a dual maximizer exists and is unique: $\\lambda^{\\star} = 0$. The optimal value of the dual problem is $d^{\\star} = g(\\lambda^{\\star}) = g(0) = 0$.\n\nThird, we analyze the primal problem. We need to find the infimum of $f(x) = \\exp(x)$ over the feasible set $S = \\{x \\in \\mathbb{R} \\mid x \\leq 0\\}$, which is the interval $(-\\infty, 0]$. The objective function $f(x) = \\exp(x)$ is a strictly increasing function over its entire domain $\\mathbb{R}$. To minimize an increasing function, one must choose the smallest possible value for its argument from the feasible set. Since the feasible set $(-\\infty, 0]$ is unbounded below, the infimum is the limit of the function as $x$ approaches $-\\infty$.\n$$\np^{\\star} = \\inf_{x \\leq 0} \\exp(x) = \\lim_{x \\to -\\infty} \\exp(x) = 0\n$$\nThe primal infimum (or optimal value) is $p^{\\star} = 0$. For this value to be attained, there must exist a feasible point $x^{\\star} \\leq 0$ such that $f(x^{\\star}) = p^{\\star}$. This would require $\\exp(x^{\\star}) = 0$. However, the exponential function is strictly positive for all real $x$. Thus, there is no $x^{\\star}$ that satisfies this condition, and the primal infimum is not attained.\n\nFourth, we state the dual optimal value, compare it with the primal infimum, and comment on the duality gap and attainment.\nThe primal optimal value is $p^{\\star} = 0$.\nThe dual optimal value is $d^{\\star} = 0$.\nThe duality gap is $p^{\\star} - d^{\\star} = 0 - 0 = 0$. This indicates that strong duality holds for this problem. The holding of strong duality is guaranteed by Slater's condition, since the problem is convex and there exists a strictly feasible point (e.g., $x=-1$, for which $-1  0$).\nRegarding attainment, the dual optimal value $d^{\\star}=0$ is attained at the dual maximizer $\\lambda^{\\star}=0$. In contrast, the primal optimal value $p^{\\star}=0$ is not attained by any feasible $x$.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Duality theory is not just an analytical tool; it is the engine behind many state-of-the-art optimization algorithms. This final practice () bridges the gap between theory and computation by tasking you with implementing a \"primal recovery\" heuristic. After deriving the dual for a standard quadratic program, you will write code that uses information from the dual space to construct and evaluate a high-quality feasible solution for the original primal problem.",
            "id": "3191747",
            "problem": "Consider the convex quadratic optimization problem with only componentwise nonnegativity constraints: minimize the objective function $f(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\top}\\mathbf{Q}\\,\\mathbf{x} + \\mathbf{c}^{\\top}\\mathbf{x}$ subject to $\\mathbf{x} \\succeq \\mathbf{0}$, where $\\mathbf{Q}\\in\\mathbb{R}^{n\\times n}$ is symmetric positive definite and $\\mathbf{c}\\in\\mathbb{R}^{n}$. The inequality $\\mathbf{x} \\succeq \\mathbf{0}$ means $x_i \\ge 0$ for all indices $i$. The Lagrangian is defined by the fundamental construction $L(\\mathbf{x},\\boldsymbol{\\lambda}) = f(\\mathbf{x}) + \\boldsymbol{\\lambda}^{\\top}(\\mathbf{A}\\mathbf{x}-\\mathbf{b})$ for inequality constraints $\\mathbf{A}\\mathbf{x} \\preceq \\mathbf{b}$ with dual variables $\\boldsymbol{\\lambda} \\succeq \\mathbf{0}$. For the constraint $\\mathbf{x} \\succeq \\mathbf{0}$, take $\\mathbf{A} = -\\mathbf{I}$ and $\\mathbf{b} = \\mathbf{0}$. The Lagrange dual function $g(\\boldsymbol{\\lambda})$ is defined by $g(\\boldsymbol{\\lambda}) = \\inf_{\\mathbf{x}} L(\\mathbf{x},\\boldsymbol{\\lambda})$ and is always a concave function of $\\boldsymbol{\\lambda}$ over $\\boldsymbol{\\lambda} \\succeq \\mathbf{0}$.\n\nTask: Starting from these definitions, derive expressions for the minimizer $\\mathbf{x}(\\boldsymbol{\\lambda}) = \\arg\\min_{\\mathbf{x}} L(\\mathbf{x},\\boldsymbol{\\lambda})$ and for the dual function $g(\\boldsymbol{\\lambda})$ specialized to this problem class. Then, implement the following primal recovery heuristic using a provided sequence $\\{\\boldsymbol{\\lambda}^{k}\\}_{k=1}^{K}$ of dual-feasible points (each $\\boldsymbol{\\lambda}^{k} \\succeq \\mathbf{0}$):\n\n- Step $\\mathbf{1}$: For each $k \\in \\{1,\\dots,K\\}$, compute $\\mathbf{x}(\\boldsymbol{\\lambda}^{k})$.\n- Step $\\mathbf{2}$: Average the primal iterates to obtain $\\bar{\\mathbf{x}} = \\tfrac{1}{K}\\sum_{k=1}^{K} \\mathbf{x}(\\boldsymbol{\\lambda}^{k})$.\n- Step $\\mathbf{3}$: Project $\\bar{\\mathbf{x}}$ onto the feasible set to obtain the recovered primal point $\\mathbf{x}_{\\mathrm{rec}} = \\operatorname{proj}_{\\{\\mathbf{x}\\succeq\\mathbf{0}\\}}(\\bar{\\mathbf{x}})$, defined by $\\mathbf{x}_{\\mathrm{rec}} = \\arg\\min_{\\mathbf{x}\\succeq\\mathbf{0}} \\|\\mathbf{x}-\\bar{\\mathbf{x}}\\|_{2}$. For nonnegativity constraints, this projection is the componentwise operation $(\\mathbf{x}_{\\mathrm{rec}})_i = \\max\\{\\bar{x}_i, 0\\}$.\n- Step $\\mathbf{4}$: Average the dual iterates to obtain $\\bar{\\boldsymbol{\\lambda}} = \\tfrac{1}{K}\\sum_{k=1}^{K}\\boldsymbol{\\lambda}^{k}$.\n- Step $\\mathbf{5}$: Evaluate the primal-dual gap value $\\Delta = f(\\mathbf{x}_{\\mathrm{rec}}) - g(\\bar{\\boldsymbol{\\lambda}})$.\n\nUsing the above steps, compute $\\Delta$ for each of the following test cases. Each test case supplies $\\mathbf{Q}$, $\\mathbf{c}$, and the sequence $\\{\\boldsymbol{\\lambda}^{k}\\}$.\n\n- Test case $1$ (dimension $n=2$):\n  - $\\mathbf{Q} = \\begin{bmatrix} 2  0 \\\\ 0  1 \\end{bmatrix}$,\n  - $\\mathbf{c} = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}$,\n  - Dual sequence $\\{\\boldsymbol{\\lambda}^{k}\\}_{k=1}^{3}$ given by $\\boldsymbol{\\lambda}^{1} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\boldsymbol{\\lambda}^{2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, $\\boldsymbol{\\lambda}^{3} = \\begin{bmatrix} 2 \\\\ 0.5 \\end{bmatrix}$.\n- Test case $2$ (dimension $n=1$):\n  - $\\mathbf{Q} = \\begin{bmatrix} 4 \\end{bmatrix}$,\n  - $\\mathbf{c} = \\begin{bmatrix} 1 \\end{bmatrix}$,\n  - Dual sequence $\\{\\boldsymbol{\\lambda}^{k}\\}_{k=1}^{3}$ given by $\\boldsymbol{\\lambda}^{1} = \\begin{bmatrix} 0 \\end{bmatrix}$, $\\boldsymbol{\\lambda}^{2} = \\begin{bmatrix} 2 \\end{bmatrix}$, $\\boldsymbol{\\lambda}^{3} = \\begin{bmatrix} 4 \\end{bmatrix}$.\n- Test case $3$ (dimension $n=3$):\n  - $\\mathbf{Q} = \\begin{bmatrix} 1  0  0 \\\\ 0  3  0 \\\\ 0  0  2 \\end{bmatrix}$,\n  - $\\mathbf{c} = \\begin{bmatrix} -1 \\\\ 0.5 \\\\ 2 \\end{bmatrix}$,\n  - Dual sequence $\\{\\boldsymbol{\\lambda}^{k}\\}_{k=1}^{4}$ given by $\\boldsymbol{\\lambda}^{1} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $\\boldsymbol{\\lambda}^{2} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix}$, $\\boldsymbol{\\lambda}^{3} = \\begin{bmatrix} 0.5 \\\\ 1 \\\\ 0 \\end{bmatrix}$, $\\boldsymbol{\\lambda}^{4} = \\begin{bmatrix} 2 \\\\ 0.5 \\\\ 1 \\end{bmatrix}$.\n\nYour program must:\n- Implement the derivation-based formulas you obtained for $\\mathbf{x}(\\boldsymbol{\\lambda})$ and $g(\\boldsymbol{\\lambda})$ and then execute the five-step heuristic for each test case.\n- Produce a single line of output containing a Python-style list with the three computed gap values $\\Delta$ in the order of the test cases. For example, the output format must be exactly like $[\\Delta_1,\\Delta_2,\\Delta_3]$ with no spaces.\n- All numerical results must be printed as plain decimal numbers. There are no physical units or angles involved in this problem.\n\nEnsure that your implementation is self-contained and does not require external input. The final output must aggregate the results for all provided test cases into a single line in the specified format.",
            "solution": "The user has presented a well-posed problem in the field of convex optimization. The task is to derive the Lagrange dual function for a specific class of quadratic programs and then use the derived expressions to implement a primal recovery heuristic. All aspects of the problem are scientifically sound, self-contained, and algorithmically specified.\n\nThe optimization problem, often referred to as a quadratic program (QP) with nonnegativity constraints, is stated as:\n$$\n\\begin{aligned}\n \\underset{\\mathbf{x}}{\\text{minimize}}   f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^{\\top}\\mathbf{Q}\\mathbf{x} + \\mathbf{c}^{\\top}\\mathbf{x} \\\\\n \\text{subject to}   \\mathbf{x} \\succeq \\mathbf{0}\n\\end{aligned}\n$$\nwhere $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite matrix, $\\mathbf{c} \\in \\mathbb{R}^{n}$, and $\\mathbf{x} \\in \\mathbb{R}^{n}$. The constraint $\\mathbf{x} \\succeq \\mathbf{0}$ signifies that each component $x_i$ of the vector $\\mathbf{x}$ must be non-negative, i.e., $x_i \\ge 0$ for $i=1, \\dots, n$.\n\nOur first task is to derive the analytical expressions for the minimizer $\\mathbf{x}(\\boldsymbol{\\lambda})$ of the Lagrangian and the corresponding Lagrange dual function $g(\\boldsymbol{\\lambda})$.\n\n**Step 1: Formulation of the Lagrangian**\n\nThe standard form for inequality constraints is $\\mathbf{A}\\mathbf{x} \\preceq \\mathbf{b}$. The given constraint $\\mathbf{x} \\succeq \\mathbf{0}$ can be rewritten as $-\\mathbf{x} \\preceq \\mathbf{0}$. By identifying $\\mathbf{A} = -\\mathbf{I}$ (where $\\mathbf{I}$ is the $n \\times n$ identity matrix) and $\\mathbf{b} = \\mathbf{0}$, we can construct the Lagrangian $L(\\mathbf{x}, \\boldsymbol{\\lambda})$ for a vector of Lagrange multipliers $\\boldsymbol{\\lambda} \\succeq \\mathbf{0}$:\n$$\nL(\\mathbf{x}, \\boldsymbol{\\lambda}) = f(\\mathbf{x}) + \\boldsymbol{\\lambda}^{\\top}(-\\mathbf{I} \\mathbf{x} - \\mathbf{0})\n$$\nSubstituting the expression for $f(\\mathbf{x})$:\n$$\nL(\\mathbf{x}, \\boldsymbol{\\lambda}) = \\frac{1}{2}\\mathbf{x}^{\\top}\\mathbf{Q}\\mathbf{x} + \\mathbf{c}^{\\top}\\mathbf{x} - \\boldsymbol{\\lambda}^{\\top}\\mathbf{x}\n$$\nThis expression can be consolidated as:\n$$\nL(\\mathbf{x}, \\boldsymbol{\\lambda}) = \\frac{1}{2}\\mathbf{x}^{\\top}\\mathbf{Q}\\mathbf{x} + (\\mathbf{c} - \\boldsymbol{\\lambda})^{\\top}\\mathbf{x}\n$$\n\n**Step 2: Derivation of the Minimizer $\\mathbf{x}(\\boldsymbol{\\lambda})$**\n\nThe Lagrange dual function $g(\\boldsymbol{\\lambda})$ is defined as the infimum of the Lagrangian over the primal variable $\\mathbf{x}$:\n$$\ng(\\boldsymbol{\\lambda}) = \\inf_{\\mathbf{x} \\in \\mathbb{R}^n} L(\\mathbf{x}, \\boldsymbol{\\lambda})\n$$\nThe Lagrangian $L(\\mathbf{x}, \\boldsymbol{\\lambda})$ is a quadratic function of $\\mathbf{x}$. Since $\\mathbf{Q}$ is positive definite, the function is strictly convex with respect to $\\mathbf{x}$ for any fixed $\\boldsymbol{\\lambda}$. The unique minimizer, which we denote $\\mathbf{x}(\\boldsymbol{\\lambda})$, can be found by setting the gradient of the Lagrangian with respect to $\\mathbf{x}$ to zero.\n\nThe gradient $\\nabla_{\\mathbf{x}} L(\\mathbf{x}, \\boldsymbol{\\lambda})$ is:\n$$\n\\nabla_{\\mathbf{x}} L(\\mathbf{x}, \\boldsymbol{\\lambda}) = \\nabla_{\\mathbf{x}} \\left( \\frac{1}{2}\\mathbf{x}^{\\top}\\mathbf{Q}\\mathbf{x} + (\\mathbf{c} - \\boldsymbol{\\lambda})^{\\top}\\mathbf{x} \\right)\n$$\nUsing standard matrix calculus identities and the symmetry of $\\mathbf{Q}$, we get:\n$$\n\\nabla_{\\mathbf{x}} L(\\mathbf{x}, \\boldsymbol{\\lambda}) = \\mathbf{Q}\\mathbf{x} + (\\mathbf{c} - \\boldsymbol{\\lambda})\n$$\nSetting the gradient to the zero vector, $\\mathbf{0}$:\n$$\n\\mathbf{Q}\\mathbf{x} + \\mathbf{c} - \\boldsymbol{\\lambda} = \\mathbf{0}\n$$\nSince $\\mathbf{Q}$ is positive definite, it is invertible. We can solve for $\\mathbf{x}$ to find the minimizer $\\mathbf{x}(\\boldsymbol{\\lambda})$:\n$$\n\\mathbf{Q}\\mathbf{x} = \\boldsymbol{\\lambda} - \\mathbf{c}\n$$\n$$\n\\mathbf{x}(\\boldsymbol{\\lambda}) = \\mathbf{Q}^{-1}(\\boldsymbol{\\lambda} - \\mathbf{c})\n$$\n\n**Step 3: Derivation of the Dual Function $g(\\boldsymbol{\\lambda})$**\n\nTo find the dual function $g(\\boldsymbol{\\lambda})$, we substitute the expression for the minimizer $\\mathbf{x}(\\boldsymbol{\\lambda})$ back into the Lagrangian:\n$$\ng(\\boldsymbol{\\lambda}) = L(\\mathbf{x}(\\boldsymbol{\\lambda}), \\boldsymbol{\\lambda}) = \\frac{1}{2}\\mathbf{x}(\\boldsymbol{\\lambda})^{\\top}\\mathbf{Q}\\mathbf{x}(\\boldsymbol{\\lambda}) + (\\mathbf{c} - \\boldsymbol{\\lambda})^{\\top}\\mathbf{x}(\\boldsymbol{\\lambda})\n$$\nSubstituting $\\mathbf{x}(\\boldsymbol{\\lambda}) = \\mathbf{Q}^{-1}(\\boldsymbol{\\lambda} - \\mathbf{c})$:\n$$\ng(\\boldsymbol{\\lambda}) = \\frac{1}{2} \\left( \\mathbf{Q}^{-1}(\\boldsymbol{\\lambda} - \\mathbf{c}) \\right)^{\\top} \\mathbf{Q} \\left( \\mathbf{Q}^{-1}(\\boldsymbol{\\lambda} - \\mathbf{c}) \\right) + (\\mathbf{c} - \\boldsymbol{\\lambda})^{\\top} \\left( \\mathbf{Q}^{-1}(\\boldsymbol{\\lambda} - \\mathbf{c}) \\right)\n$$\nLet's simplify both terms. For the first term, we use the property $(AB)^{\\top} = B^{\\top}A^{\\top}$ and the fact that $\\mathbf{Q}^{-1}$ is also symmetric since $\\mathbf{Q}$ is symmetric:\n$$\n\\frac{1}{2} (\\boldsymbol{\\lambda} - \\mathbf{c})^{\\top} (\\mathbf{Q}^{-1})^{\\top} \\mathbf{Q} \\mathbf{Q}^{-1} (\\boldsymbol{\\lambda} - \\mathbf{c}) = \\frac{1}{2} (\\boldsymbol{\\lambda} - \\mathbf{c})^{\\top} \\mathbf{Q}^{-1} (\\boldsymbol{\\lambda} - \\mathbf{c})\n$$\nFor the second term, we can write $(\\mathbf{c} - \\boldsymbol{\\lambda})^{\\top} = -(\\boldsymbol{\\lambda} - \\mathbf{c})^{\\top}$:\n$$\n-(\\boldsymbol{\\lambda} - \\mathbf{c})^{\\top} \\mathbf{Q}^{-1} (\\boldsymbol{\\lambda} - \\mathbf{c})\n$$\nCombining the two simplified terms:\n$$\ng(\\boldsymbol{\\lambda}) = \\frac{1}{2} (\\boldsymbol{\\lambda} - \\mathbf{c})^{\\top} \\mathbf{Q}^{-1} (\\boldsymbol{\\lambda} - \\mathbf{c}) - (\\boldsymbol{\\lambda} - \\mathbf{c})^{\\top} \\mathbf{Q}^{-1} (\\boldsymbol{\\lambda} - \\mathbf{c})\n$$\nThis simplifies to the final expression for the dual function:\n$$\ng(\\boldsymbol{\\lambda}) = -\\frac{1}{2}(\\boldsymbol{\\lambda} - \\mathbf{c})^{\\top}\\mathbf{Q}^{-1}(\\boldsymbol{\\lambda} - \\mathbf{c})\n$$\n\n**Step 4: Implementation of the Primal Recovery Heuristic**\n\nWith the derived formulas for $\\mathbf{x}(\\boldsymbol{\\lambda})$ and $g(\\boldsymbol{\\lambda})$, we can now implement the five-step heuristic described in the problem statement for each test case.\n\n- **Step 1**: For each dual-feasible point $\\boldsymbol{\\lambda}^{k}$ in the given sequence $\\{\\boldsymbol{\\lambda}^{k}\\}_{k=1}^{K}$, compute the corresponding primal minimizer of the Lagrangian: $\\mathbf{x}(\\boldsymbol{\\lambda}^{k}) = \\mathbf{Q}^{-1}(\\boldsymbol{\\lambda}^{k} - \\mathbf{c})$.\n\n- **Step 2**: Average the primal iterates: $\\bar{\\mathbf{x}} = \\frac{1}{K}\\sum_{k=1}^{K} \\mathbf{x}(\\boldsymbol{\\lambda}^{k})$.\n\n- **Step 3**: Project the averaged primal iterate onto the feasible set $\\{\\mathbf{x} \\mid \\mathbf{x} \\succeq \\mathbf{0}\\}$. The projection onto the non-negative orthant is a component-wise operation: $(\\mathbf{x}_{\\mathrm{rec}})_i = \\max\\{\\bar{x}_i, 0\\}$.\n\n- **Step 4**: Average the dual iterates: $\\bar{\\boldsymbol{\\lambda}} = \\frac{1}{K}\\sum_{k=1}^{K} \\boldsymbol{\\lambda}^{k}$.\n\n- **Step 5**: Evaluate the primal-dual gap $\\Delta = f(\\mathbf{x}_{\\mathrm{rec}}) - g(\\bar{\\boldsymbol{\\lambda}})$. The two components are calculated as:\n  $$\n  f(\\mathbf{x}_{\\mathrm{rec}}) = \\frac{1}{2}\\mathbf{x}_{\\mathrm{rec}}^{\\top}\\mathbf{Q}\\mathbf{x}_{\\mathrm{rec}} + \\mathbf{c}^{\\top}\\mathbf{x}_{\\mathrm{rec}}\n  $$\n  $$\n  g(\\bar{\\boldsymbol{\\lambda}}) = -\\frac{1}{2}(\\bar{\\boldsymbol{\\lambda}} - \\mathbf{c})^{\\top}\\mathbf{Q}^{-1}(\\bar{\\boldsymbol{\\lambda}} - \\mathbf{c})\n  $$\nThis procedure will be systematically applied to each of the three test cases provided. For each case, the matrices $\\mathbf{Q}$ are diagonal, which simplifies the computation of their inverses $\\mathbf{Q}^{-1}$ as the diagonal entries of $\\mathbf{Q}^{-1}$ are simply the reciprocals of the diagonal entries of $\\mathbf{Q}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the quadratic optimization problem for the given test cases\n    by implementing a primal recovery heuristic.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"Q\": np.array([[2, 0], [0, 1]]),\n            \"c\": np.array([1, -2]),\n            \"lambdas\": [\n                np.array([0, 0]),\n                np.array([1, 1]),\n                np.array([2, 0.5])\n            ]\n        },\n        {\n            \"Q\": np.array([[4]]),\n            \"c\": np.array([1]),\n            \"lambdas\": [\n                np.array([0]),\n                np.array([2]),\n                np.array([4])\n            ]\n        },\n        {\n            \"Q\": np.array([[1, 0, 0], [0, 3, 0], [0, 0, 2]]),\n            \"c\": np.array([-1, 0.5, 2]),\n            \"lambdas\": [\n                np.array([0, 0, 0]),\n                np.array([1, 0, 3]),\n                np.array([0.5, 1, 0]),\n                np.array([2, 0.5, 1])\n            ]\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        Q = case[\"Q\"]\n        c = case[\"c\"]\n        lambdas_k = case[\"lambdas\"]\n        K = len(lambdas_k)\n        \n        # Pre-compute the inverse of Q.\n        # For diagonal Q, inv(Q) is simple, but linalg.inv is general.\n        Q_inv = np.linalg.inv(Q)\n\n        # Step 1: For each k, compute x(lambda^k)\n        # x(lambda) = Q_inv @ (lambda - c)\n        x_k_list = [Q_inv @ (lk - c) for lk in lambdas_k]\n\n        # Step 2: Average the primal iterates\n        x_bar = np.mean(x_k_list, axis=0)\n\n        # Step 3: Project x_bar onto the feasible set {x >= 0}\n        # For nonnegativity constraints, this is component-wise max(x_bar_i, 0)\n        x_rec = np.maximum(x_bar, 0)\n\n        # Step 4: Average the dual iterates\n        lambda_bar = np.mean(lambdas_k, axis=0)\n\n        # Step 5: Evaluate the primal-dual gap value Delta = f(x_rec) - g(lambda_bar)\n        \n        # Evaluate primal objective f(x_rec)\n        # f(x) = 0.5 * x.T @ Q @ x + c.T @ x\n        f_x_rec = 0.5 * x_rec.T @ Q @ x_rec + c.T @ x_rec\n        \n        # Evaluate dual function g(lambda_bar)\n        # g(lambda) = -0.5 * (lambda - c).T @ Q_inv @ (lambda - c)\n        g_lambda_bar = -0.5 * (lambda_bar - c).T @ Q_inv @ (lambda_bar - c)\n        \n        delta = f_x_rec - g_lambda_bar\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    # Convert numerical results to plain decimal numbers.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}