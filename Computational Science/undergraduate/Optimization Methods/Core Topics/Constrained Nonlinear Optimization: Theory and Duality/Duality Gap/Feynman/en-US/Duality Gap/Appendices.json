{
    "hands_on_practices": [
        {
            "introduction": "While strong duality ($p^*=d^*$) is common in convex optimization, it's crucial to understand what happens when the primal optimum is not attained. This practice provides geometric intuition for such a scenario, where the optimal solution lies on the boundary of a feasible set that is not closed . By projecting a point onto an open ball, you will find that strong duality still holds. However, you will also see firsthand how the inability to attain the primal infimum leads to a non-zero \"pointwise\" gap between any suboptimal primal point and the dual optimum.",
            "id": "3123554",
            "problem": "Consider the Euclidean projection problem in $\\mathbb{R}^{2}$ with objective defined by minimizing the squared distance to a given point. Let the point be $c = (2, 0)$ and the feasible set be the open unit ball $S_{\\mathrm{open}} = \\{x \\in \\mathbb{R}^{2} : \\|x\\|_{2} < 1\\}$, which is convex but not closed. The projection problem is written as minimizing the function $x \\mapsto \\frac{1}{2}\\|x - c\\|_{2}^{2}$ over $x \\in S_{\\mathrm{open}}$. Use the following fundamental bases: the definition of convex sets and closure, the indicator function of a set, the Fenchel conjugate and the support function of a set. Starting from these definitions (without invoking any pre-packaged duality theorems), perform the following:\n\n1. Formulate the problem as minimizing the sum of a convex function and an indicator function, and use the definition of the Fenchel conjugate to derive the corresponding Fenchel dual problem. Clearly state the dual objective and characterize its optimizer using first-order optimality in $\\mathbb{R}^{2}$.\n2. Determine the primal infimum for the projection onto $S_{\\mathrm{open}}$ and explain whether a minimizer exists. Justify your conclusion using the geometry of the open ball and the properties of the squared distance.\n3. Consider a specific feasible primal point $x_{\\varepsilon} = (1 - \\varepsilon, 0)$ with $\\varepsilon = 0.2$, and use the dual optimizer you found to compute the pointwise duality gap defined as the primal objective at $x_{\\varepsilon}$ minus the dual objective at the dual optimizer.\n4. Now replace the feasible set by the closed unit ball $S_{\\mathrm{closed}} = \\{x \\in \\mathbb{R}^{2} : \\|x\\|_{2} \\leq 1\\}$ and repeat the reasoning to explain why strong duality holds and the duality gap at the primal-dual optimizers is zero. You do not need to recompute any numbers for this part; focus on the reasoning that changes when the set is closed.\n\nReport the numerical value obtained in part 3. Round your answer to four significant figures.",
            "solution": "The user-provided problem is a valid exercise in convex optimization, specifically concerning Fenchel duality and the duality gap. We will proceed with a full solution.\n\nThe primal problem is to minimize the function $f(x) = \\frac{1}{2}\\|x - c\\|_{2}^{2}$ for $x$ in a feasible set, where $c = (2, 0) \\in \\mathbb{R}^2$.\n\n**1. Fenchel Dual Formulation for the Open Unit Ball**\n\nThe feasible set is the open unit ball $S_{\\mathrm{open}} = \\{x \\in \\mathbb{R}^{2} : \\|x\\|_{2} < 1\\}$. The optimization problem can be formulated as an unconstrained problem by using the indicator function of the set $S_{\\mathrm{open}}$, which is defined as $\\mathbb{I}_{S_{\\mathrm{open}}}(x) = 0$ if $x \\in S_{\\mathrm{open}}$ and $\\mathbb{I}_{S_{\\mathrm{open}}}(x) = +\\infty$ otherwise.\n\nThe primal problem is:\n$$ \\min_{x \\in \\mathbb{R}^2} \\left( \\frac{1}{2}\\|x - c\\|_{2}^{2} + \\mathbb{I}_{S_{\\mathrm{open}}}(x) \\right) $$\nThis is of the form $\\min_x (F(x) + G(x))$, where $F(x) = \\frac{1}{2}\\|x - c\\|_{2}^{2}$ and $G(x) = \\mathbb{I}_{S_{\\mathrm{open}}}(x)$. The Fenchel dual problem is given by $\\max_y (-F^*(y) - G^*(-y))$, where $F^*$ and $G^*$ are the Fenchel conjugates of $F$ and $G$, respectively.\n\nFirst, we compute the Fenchel conjugate of $F(x)$:\n$$ F^*(y) = \\sup_{x \\in \\mathbb{R}^2} (\\langle y, x \\rangle - F(x)) = \\sup_{x \\in \\mathbb{R}^2} \\left( y^T x - \\frac{1}{2}(x - c)^T(x - c) \\right) $$\nThe term inside the supremum is a strictly concave quadratic function of $x$. Its maximum is found by setting the gradient with respect to $x$ to zero:\n$$ \\nabla_x \\left( y^T x - \\frac{1}{2}(x^T x - 2c^T x + c^T c) \\right) = y - (x - c) = 0 $$\nThis gives the maximizing $x = y + c$. Substituting this back into the expression:\n$$ F^*(y) = y^T(y+c) - \\frac{1}{2}((y+c)-c)^T((y+c)-c) = y^T y + y^T c - \\frac{1}{2}y^T y = \\frac{1}{2}\\|y\\|_{2}^{2} + y^T c $$\nNext, we compute the Fenchel conjugate of $G(x) = \\mathbb{I}_{S_{\\mathrm{open}}}(x)$. The conjugate of an indicator function of a set is the support function of that set, $\\sigma_{S_{\\mathrm{open}}}(y)$.\n$$ G^*(y) = \\sigma_{S_{\\mathrm{open}}}(y) = \\sup_{x \\in S_{\\mathrm{open}}} \\langle y, x \\rangle = \\sup_{\\|x\\|_2 < 1} y^T x $$\nBy the Cauchy-Schwarz inequality, $y^T x \\le \\|y\\|_2 \\|x\\|_2$. Since $\\|x\\|_2 < 1$, we have $y^T x < \\|y\\|_2$. The supremum is $\\|y\\|_2$, which is approached by taking $x$ in the direction of $y$ with a norm approaching $1$. The support function of a convex set is equal to the support function of its closure. Thus, $\\sigma_{S_{\\mathrm{open}}}(y) = \\sigma_{\\overline{S_{\\mathrm{open}}}}(y) = \\max_{\\|x\\|_2 \\le 1} y^T x = \\|y\\|_2$.\nSo, $G^*(y) = \\|y\\|_2$.\n\nThe dual problem is to maximize the dual objective function $D(y)$:\n$$ D(y) = -F^*(y) - G^*(-y) = -\\left(\\frac{1}{2}\\|y\\|_{2}^{2} + y^T c\\right) - \\|-y\\|_2 = -\\left(\\frac{1}{2}\\|y\\|_{2}^{2} + y^T c + \\|y\\|_2\\right) $$\nMaximizing $D(y)$ is equivalent to minimizing its negative, let's call it $D_{obj}(y)$:\n$$ \\min_{y \\in \\mathbb{R}^2} D_{obj}(y) = \\min_{y \\in \\mathbb{R}^2} \\left(\\frac{1}{2}\\|y\\|_{2}^{2} + y^T c + \\|y\\|_2\\right) $$\nThis objective function is convex and is differentiable for $y \\ne 0$. We find the optimizer by setting its gradient to zero. With $c=(2,0)^T$:\n$$ \\nabla D_{obj}(y) = y + c + \\frac{y}{\\|y\\|_2} = y\\left(1 + \\frac{1}{\\|y\\|_2}\\right) + c = 0 $$\nThis implies $y$ must be collinear with and oppositely directed to $c$, so $y = -kc$ for some scalar $k>0$.\n$$ -kc\\left(1 + \\frac{1}{\\|-kc\\|_2}\\right) + c = 0 \\implies c\\left(1 - k\\left(1 + \\frac{1}{k\\|c\\|_2}\\right)\\right) = 0 $$\nSince $c \\ne 0$, we have $1 - k(1 + \\frac{1}{k\\|c\\|_2}) = 0 \\implies 1 = k + \\frac{1}{\\|c\\|_2}$.\nGiven $c=(2,0)$, we have $\\|c\\|_2 = 2$. Therefore, $k = 1 - \\frac{1}{\\|c\\|_2} = 1 - \\frac{1}{2} = \\frac{1}{2}$.\nThe dual optimizer is $y^* = -kc = -\\frac{1}{2}(2, 0) = (-1, 0)$.\n\n**2. Primal Infimum and Existence of a Minimizer**\n\nThe primal problem is to find the point in the open unit ball $S_{\\mathrm{open}}$ that is closest to $c=(2,0)$. Geometrically, the point in the closed unit ball $\\overline{S_{\\mathrm{open}}} = \\{x: \\|x\\|_2 \\le 1\\}$ closest to $c$ is the projection of $c$ onto the ball, which is $x_{proj} = \\frac{c}{\\|c\\|_2} = \\frac{(2,0)}{2} = (1,0)$. The minimum squared distance would be $\\|(1,0) - (2,0)\\|_2^2 = 1$, and the minimum objective value would be $\\frac{1}{2}(1) = \\frac{1}{2}$.\n\nFor the problem over the open ball $S_{\\mathrm{open}}$, any feasible point $x$ must satisfy $\\|x\\|_2 < 1$. The point $(1,0)$ is not in $S_{\\mathrm{open}}$. We can, however, construct a sequence of points within $S_{\\mathrm{open}}$ that converges to $(1,0)$, for example $x_n = (1 - \\frac{1}{n}, 0)$ for $n \\in \\mathbb{Z}, n>1$. For this sequence, the objective value is:\n$$ f(x_n) = \\frac{1}{2}\\|(1-\\frac{1}{n}, 0) - (2,0)\\|_2^2 = \\frac{1}{2}\\|(-1-\\frac{1}{n}, 0)\\|_2^2 = \\frac{1}{2}\\left(1+\\frac{1}{n}\\right)^2 $$\nAs $n \\to \\infty$, $x_n \\to (1,0)$ and $f(x_n) \\to \\frac{1}{2}$. The infimum of the objective function over $S_{\\mathrm{open}}$ is thus $p^* = \\frac{1}{2}$.\n\nHowever, no minimizer exists in $S_{\\mathrm{open}}$. If a minimizer $x^*$ existed, it would have to satisfy $f(x^*) = \\frac{1}{2}$, which means $\\frac{1}{2}\\|x^*-c\\|_2^2 = \\frac{1}{2}$, or $\\|x^*-c\\|_2 = 1$. The only point satisfying this and $\\|x^*\\|_2 \\le 1$ is $x^*=(1,0)$. Since $(1,0) \\notin S_{\\mathrm{open}}$, the infimum is not attained in the feasible set. This is a consequence of minimizing a continuous function over a non-closed set.\n\n**3. Pointwise Duality Gap Calculation**\n\nWe are given the point $x_{\\varepsilon} = (1 - \\varepsilon, 0)$ with $\\varepsilon = 0.2$, so $x_{0.2} = (0.8, 0)$. This point is feasible since $\\|x_{0.2}\\|_2 = 0.8 < 1$.\n\nThe primal objective value at this point is:\n$$ f(x_{0.2}) = \\frac{1}{2}\\|(0.8, 0) - (2, 0)\\|_2^2 = \\frac{1}{2}\\|(-1.2, 0)\\|_2^2 = \\frac{1}{2}(-1.2)^2 = \\frac{1}{2}(1.44) = 0.72 $$\nThe dual objective value at the dual optimizer $y^* = (-1, 0)$ is the dual optimal value $d^*$.\n$$ d^* = D(y^*) = -\\left(\\frac{1}{2}\\|y^*\\|_{2}^{2} + (y^*)^T c + \\|y^*\\|_2\\right) = -\\left(\\frac{1}{2}\\|(-1,0)\\|_2^2 + \\langle (-1,0), (2,0) \\rangle + \\|(-1,0)\\|_2\\right) $$\n$$ d^* = -\\left(\\frac{1}{2}(1)^2 + (-2) + 1\\right) = -\\left(\\frac{1}{2} - 1\\right) = -(-\\frac{1}{2}) = 0.5 $$\nThe pointwise duality gap is the difference between the primal objective at the specific point $x_{0.2}$ and the optimal dual objective value $d^*$:\n$$ \\text{Gap} = f(x_{0.2}) - d^* = 0.72 - 0.5 = 0.22 $$\nTo four significant figures, this is $0.2200$.\n\n**4. Analysis for the Closed Unit Ball**\n\nWhen the feasible set is changed to the closed unit ball $S_{\\mathrm{closed}} = \\{x \\in \\mathbb{R}^{2} : \\|x\\|_{2} \\leq 1\\}$, the problem becomes:\n$$ \\min_{x \\in S_{\\mathrm{closed}}} \\frac{1}{2}\\|x - c\\|_{2}^{2} $$\nThe set $S_{\\mathrm{closed}}$ is closed and bounded in $\\mathbb{R}^2$, hence it is compact. The objective function $f(x)$ is continuous. By the Weierstrass Extreme Value Theorem, a continuous function on a compact set must attain its minimum. Therefore, a primal minimizer $x^*$ exists. As established previously, this minimizer is the projection $x^*=(1,0)$, and the primal optimal value is $p^* = f(x^*) = \\frac{1}{2}$.\n\nThe dual problem does not change because the support function of a convex set is the same as that of its closure, i.e., $\\sigma_{S_{\\mathrm{closed}}}(y) = \\sigma_{S_{\\mathrm{open}}}(y) = \\|y\\|_2$. Thus, the dual optimal value is still $d^* = \\frac{1}{2}$.\n\nIn this case, the primal and dual optimal values are equal, $p^* = d^* = \\frac{1}{2}$, so strong duality holds. The duality gap, defined as $p^* - d^*$, is $0$. Strong duality is guaranteed for this convex problem by Slater's constraint qualification. The constraint is $g(x) = \\|x\\|_2^2 - 1 \\le 0$. There exists a point $x=0$ which is strictly feasible, i.e., $g(0) = -1 < 0$. This ensures that strong duality holds. The zero duality gap is a direct consequence. At the primal-dual optimal pair, the gap is zero, unlike the pointwise gap calculated for a non-optimal primal point in Part 3. The crucial difference is the closed nature of the feasible set, which guarantees the existence of a primal minimizer where the infimum is attained.",
            "answer": "$$\\boxed{0.2200}$$"
        },
        {
            "introduction": "After exploring a scenario with a non-zero gap, we now investigate conditions that guarantee strong duality. This exercise presents a convex optimization problem where Slater's condition, a standard test for strong duality, appears to fail due to restrictive equality constraints . By carefully deriving the Lagrangian dual and solving both the primal and dual problems, you will discover that the duality gap is surprisingly zero, illustrating that Slater's condition is sufficient but not always necessary for strong duality to hold.",
            "id": "3123598",
            "problem": "Consider the convex optimization problem with variables $x \\in \\mathbb{R}^4$:\n- Objective: minimize $f(x) = \\sum_{i=1}^{4} x_i \\ln x_i$ with the convention $0 \\ln 0 := 0$.\n- Constraints: $x \\ge 0$, $\\mathbf{e}^{\\top} x = 1$, and the equalities $x_3 = 0$, $x_4 = 0$.\n\nYour tasks are:\n- Using first principles of Lagrangian duality, decide whether strict feasibility (Slater’s condition for the inequality constraints) holds for this problem and justify your answer from definitions.\n- Form the Lagrangian for the problem and derive the dual function by explicitly computing the infimum over $x$.\n- Maximize the dual function to obtain the dual optimal value.\n- Compute the primal optimal value by analyzing the objective over the feasible set.\n- From these, compute the duality gap $p^{\\star} - d^{\\star}$.\n\nProvide the final answer as a single exact real number. No rounding is required. Do not include units.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a standard convex optimization problem, and the given data and constraints are self-contained and consistent.\n\nThe optimization problem can be formulated as:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad & f(x) = \\sum_{i=1}^{4} x_i \\ln x_i \\\\\n\\text{subject to} \\quad & g_i(x) = -x_i \\le 0, \\quad i \\in \\{1, 2, 3, 4\\} \\\\\n& h_1(x) = \\sum_{i=1}^{4} x_i - 1 = 0 \\\\\n& h_2(x) = x_3 = 0 \\\\\n& h_3(x) = x_4 = 0\n\\end{aligned}\n$$\nThe domain of the objective function is $\\text{dom}(f) = \\{x \\in \\mathbb{R}^4 \\mid x_i \\ge 0 \\text{ for } i=1,2,3,4\\}$. The convention $0 \\ln 0 := 0$ is used.\n\n**Part 1: Strict Feasibility (Slater's Condition)**\n\nStrict feasibility, by its definition, requires the existence of a feasible point $\\tilde{x}$ that satisfies all inequality constraints strictly. For this problem, the inequality constraints are $g_i(x) = -x_i \\le 0$ for $i \\in \\{1, 2, 3, 4\\}$. A strictly feasible point $\\tilde{x}$ must therefore satisfy:\n$1$. $\\tilde{x}$ is feasible, meaning it satisfies all equality and inequality constraints.\n$2$. $g_i(\\tilde{x}) < 0$ for all $i \\in \\{1, 2, 3, 4\\}$.\n\nThe second condition implies $-\\tilde{x}_i < 0$, which is equivalent to $\\tilde{x}_i > 0$ for all $i \\in \\{1, 2, 3, 4\\}$.\nHowever, the feasibility condition (the first condition) requires $\\tilde{x}$ to satisfy the equality constraints, which include $h_2(\\tilde{x}) = \\tilde{x}_3 = 0$ and $h_3(\\tilde{x}) = \\tilde{x}_4 = 0$.\n\nWe have a contradiction. Strict feasibility requires $\\tilde{x}_3 > 0$ and $\\tilde{x}_4 > 0$, while feasibility requires $\\tilde{x}_3 = 0$ and $\\tilde{x}_4 = 0$. No point can satisfy both conditions simultaneously. Therefore, no strictly feasible point exists for this problem, and strict feasibility (in this sense) does not hold.\n\nIt is worth noting that a weaker condition, often called the refined Slater's condition, is sufficient for strong duality in convex problems. This condition requires only that the non-affine inequality constraints be strictly satisfied for some feasible point. Since all inequality constraints in this problem are affine, this condition reduces to requiring a non-empty feasible set. The point $x = (1/2, 1/2, 0, 0)^{\\top}$ is feasible, so the refined Slater's condition holds, and we expect a duality gap of zero.\n\n**Part 2: Lagrangian and Dual Function**\n\nTo form the Lagrangian, we can incorporate the non-negativity constraints $x_i \\ge 0$ into the domain of the minimization, rather than treating them as explicit constraints with corresponding Lagrange multipliers. We introduce Lagrange multipliers $\\nu_1, \\nu_3, \\nu_4 \\in \\mathbb{R}$ for the three equality constraints. The Lagrangian $L(x, \\nu)$ is:\n$$\nL(x, \\nu) = f(x) + \\nu_1 h_1(x) + \\nu_3 h_2(x) + \\nu_4 h_3(x)\n$$\n$$\nL(x, \\nu) = \\sum_{i=1}^{4} x_i \\ln x_i + \\nu_1 \\left(\\sum_{i=1}^{4} x_i - 1\\right) + \\nu_3 x_3 + \\nu_4 x_4\n$$\nThe dual function $g(\\nu)$ is the infimum of the Lagrangian over its domain, $x \\in \\text{dom}(f)$, which is $\\{x \\in \\mathbb{R}^4 \\mid x_i \\ge 0\\}$.\n$$\ng(\\nu) = \\inf_{x \\ge 0} L(x, \\nu)\n$$\nWe can regroup terms in the Lagrangian by $x_i$:\n$$\nL(x, \\nu) = (x_1 \\ln x_1 + \\nu_1 x_1) + (x_2 \\ln x_2 + \\nu_1 x_2) + (x_3 \\ln x_3 + (\\nu_1+\\nu_3)x_3) + (x_4 \\ln x_4 + (\\nu_1+\\nu_4)x_4) - \\nu_1\n$$\nThe minimization problem decouples into four independent problems over $x_1, x_2, x_3, x_4$. We need to compute $\\inf_{t \\ge 0} (t \\ln t + c t)$ for a constant $c$. Let $\\phi(t) = t \\ln t + c t$. The derivative is $\\phi'(t) = \\ln t + 1 + c$. Setting $\\phi'(t) = 0$ gives $t_{min} = \\exp(-1-c)$. Since $\\phi''(t) = 1/t > 0$ for $t > 0$, this is a minimum.\nThe minimum value is $\\phi(t_{min}) = \\exp(-1-c) \\ln(\\exp(-1-c)) + c \\exp(-1-c) = \\exp(-1-c)(-1-c) + c \\exp(-1-c) = -\\exp(-1-c)$.\n\nApplying this result to each term in the Lagrangian:\n- For $x_1$: $c_1 = \\nu_1$, infimum is $-\\exp(-1-\\nu_1)$.\n- For $x_2$: $c_2 = \\nu_1$, infimum is $-\\exp(-1-\\nu_1)$.\n- For $x_3$: $c_3 = \\nu_1 + \\nu_3$, infimum is $-\\exp(-1-\\nu_1-\\nu_3)$.\n- For $x_4$: $c_4 = \\nu_1 + \\nu_4$, infimum is $-\\exp(-1-\\nu_1-\\nu_4)$.\n\nThe dual function is the sum of these infima minus $\\nu_1$:\n$$\ng(\\nu_1, \\nu_3, \\nu_4) = -2\\exp(-1-\\nu_1) - \\exp(-1-\\nu_1-\\nu_3) - \\exp(-1-\\nu_1-\\nu_4) - \\nu_1\n$$\n\n**Part 3: Maximize the Dual Function**\n\nThe dual problem is to maximize $g(\\nu)$ over $\\nu_1, \\nu_3, \\nu_4$. We take partial derivatives with respect to the dual variables:\n$$\n\\frac{\\partial g}{\\partial \\nu_3} = \\exp(-1-\\nu_1-\\nu_3)\n$$\n$$\n\\frac{\\partial g}{\\partial \\nu_4} = \\exp(-1-\\nu_1-\\nu_4)\n$$\nSince these partial derivatives are always positive, $g(\\nu)$ is a monotonically increasing function of $\\nu_3$ and $\\nu_4$. To maximize $g$, we must let $\\nu_3 \\to \\infty$ and $\\nu_4 \\to \\infty$. In this limit, the exponential terms involving $\\nu_3$ and $\\nu_4$ approach $0$.\nThe dual function to be maximized with respect to $\\nu_1$ becomes:\n$$\ng_{lim}(\\nu_1) = \\lim_{\\nu_3, \\nu_4 \\to \\infty} g(\\nu_1, \\nu_3, \\nu_4) = -2\\exp(-1-\\nu_1) - \\nu_1\n$$\nTo find the maximum, we set the derivative with respect to $\\nu_1$ to zero:\n$$\n\\frac{d g_{lim}}{d \\nu_1} = -2\\exp(-1-\\nu_1)(-1) - 1 = 2\\exp(-1-\\nu_1) - 1 = 0\n$$\nThis yields $\\exp(-1-\\nu_1) = 1/2$. Taking the natural logarithm gives $-1-\\nu_1 = \\ln(1/2) = -\\ln 2$, so the optimal $\\nu_1^{\\star} = \\ln 2 - 1$.\nThe dual optimal value, $d^{\\star}$, is found by substituting $\\nu_1^{\\star}$ back into $g_{lim}(\\nu_1)$:\n$$\nd^{\\star} = -2\\exp(-1-(\\ln 2 - 1)) - (\\ln 2 - 1) = -2\\exp(-\\ln 2) - \\ln 2 + 1 = -2\\left(\\frac{1}{2}\\right) - \\ln 2 + 1 = -1 - \\ln 2 + 1 = -\\ln 2\n$$\n\n**Part 4: Primal Optimal Value**\n\nWe can solve the primal problem directly by substituting the equality constraints into the objective function. The constraints $x_3 = 0$ and $x_4 = 0$, along with $\\sum x_i = 1$, simplify the problem to:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad & x_1 \\ln x_1 + x_2 \\ln x_2 \\\\\n\\text{subject to} \\quad & x_1 + x_2 = 1 \\\\\n& x_1 \\ge 0, x_2 \\ge 0\n\\end{aligned}\n$$\n(The terms for $x_3, x_4$ are $0 \\ln 0 = 0$).\nSubstitute $x_2 = 1 - x_1$ to get a function of a single variable, $\\phi(x_1)$, for $x_1 \\in [0, 1]$:\n$$\n\\phi(x_1) = x_1 \\ln x_1 + (1-x_1) \\ln(1-x_1)\n$$\nTo find the minimum, we differentiate with respect to $x_1$ and set the derivative to zero:\n$$\n\\phi'(x_1) = (\\ln x_1 + 1) + (-\\ln(1-x_1) - 1) = \\ln x_1 - \\ln(1-x_1) = 0\n$$\nThis gives $\\ln x_1 = \\ln(1-x_1)$, which implies $x_1 = 1-x_1$, so $2x_1 = 1$ and $x_1^{\\star} = 1/2$.\nThen $x_2^{\\star} = 1 - 1/2 = 1/2$. The second derivative $\\phi''(x_1) = 1/x_1 + 1/(1-x_1)$ is positive on $(0, 1)$, confirming this is a minimum.\nThe primal optimal point is $x^{\\star} = (1/2, 1/2, 0, 0)^{\\top}$.\nThe primal optimal value, $p^{\\star}$, is the objective evaluated at this point:\n$$\np^{\\star} = f(x^{\\star}) = \\frac{1}{2} \\ln\\left(\\frac{1}{2}\\right) + \\frac{1}{2} \\ln\\left(\\frac{1}{2}\\right) + 0 + 0 = 2 \\cdot \\left(\\frac{1}{2} \\ln\\left(\\frac{1}{2}\\right)\\right) = \\ln\\left(\\frac{1}{2}\\right) = -\\ln 2\n$$\n\n**Part 5: Duality Gap**\n\nThe duality gap is defined as the difference between the primal optimal value and the dual optimal value, $p^{\\star} - d^{\\star}$.\nUsing our computed values:\n$$\np^{\\star} - d^{\\star} = (-\\ln 2) - (-\\ln 2) = 0\n$$\nThe duality gap is $0$.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "We now elevate our analysis from vector spaces to the more abstract and powerful domain of Semidefinite Programming (SDP), where we optimize over a cone of positive semidefinite matrices. This advanced practice examines an SDP where, once again, the strict feasibility required by Slater's condition does not hold . Through deriving the dual SDP, you will reinforce the principle that strong duality can persist even without Slater's condition and encounter the intriguing phenomenon where the dual optimal value is achieved only in the limit, meaning a dual optimizer does not exist.",
            "id": "3123532",
            "problem": "Consider the Semidefinite Programming (SDP) problem, where Semidefinite Programming (SDP) denotes optimization of a linear functional over the cone of positive semidefinite matrices:\n$$\\min \\ \\langle C, X \\rangle \\quad \\text{subject to} \\quad \\mathcal{A}(X) = b, \\quad X \\succeq 0,$$\nwith decision variable $X \\in \\mathbb{S}^{2}$, the set of real symmetric $2 \\times 2$ matrices. The data are specified as follows:\n- The cost matrix is $C = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$.\n- The linear constraint operator $\\mathcal{A} : \\mathbb{S}^{2} \\to \\mathbb{R}^{2}$ is defined by\n$$\\mathcal{A}(X) = \\big(\\langle E_{11}, X \\rangle, \\ \\langle E_{22}, X \\rangle\\big),$$\nwhere $E_{ij}$ denotes the $2 \\times 2$ matrix with a $1$ in entry $(i,j)$ and zeros elsewhere, and $\\langle \\cdot,\\cdot \\rangle$ is the trace inner product $\\langle U, V \\rangle = \\operatorname{tr}(UV)$.\n- The right-hand side is $b = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n\nTasks:\n1. Using only the definitions of positive semidefinite matrices and the trace inner product, characterize the feasible set $\\{X \\succeq 0 : \\mathcal{A}(X) = b\\}$ and show that every feasible $X$ is rank-deficient. Conclude that Slater’s condition (existence of a strictly feasible $X \\succ 0$) fails for this SDP.\n2. Starting from the Lagrangian definition $L(X,y) = \\langle C, X \\rangle + \\langle y, b - \\mathcal{A}(X) \\rangle$ and the definition of the adjoint operator $\\mathcal{A}^{*}$ via $\\langle \\mathcal{A}(X), y \\rangle = \\langle X, \\mathcal{A}^{*}(y) \\rangle$, derive the dual problem and express it explicitly as a maximization over $y \\in \\mathbb{R}^{2}$ with a single positive semidefinite constraint on a $2 \\times 2$ matrix.\n3. Compute the primal optimal value and the dual optimal value and then compute the duality gap, defined as $p^{\\star} - d^{\\star}$, where $p^{\\star}$ is the primal optimal value and $d^{\\star}$ is the dual optimal value. Your final answer must be a single real number. No rounding is required.",
            "solution": "We proceed from first principles in convex optimization and matrix analysis.\n\nFirst, we characterize the feasible set. The constraints are\n$$\\langle E_{11}, X \\rangle = X_{11} = 0, \\qquad \\langle E_{22}, X \\rangle = X_{22} = 1,$$\nand the conic constraint is $X \\succeq 0$. Let $X = \\begin{pmatrix} X_{11} & X_{12} \\\\ X_{12} & X_{22} \\end{pmatrix}$. The equalities impose $X_{11} = 0$ and $X_{22} = 1$, so any feasible $X$ has the form\n$$X = \\begin{pmatrix} 0 & X_{12} \\\\ X_{12} & 1 \\end{pmatrix}.$$\nFor $X \\succeq 0$, every principal minor must be nonnegative. In particular, the leading principal minors give:\n- $X_{11} = 0 \\ge 0$ (satisfied),\n- The determinant must satisfy\n$$\\det(X) = X_{11} X_{22} - X_{12}^{2} = 0 \\cdot 1 - X_{12}^{2} = - X_{12}^{2} \\ge 0,$$\nwhich forces $X_{12} = 0$. Therefore, the only feasible matrix is\n$$X = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}.$$\nThis matrix has rank $1$ and is not strictly positive definite. Hence, there is no strictly feasible point $X \\succ 0$ that satisfies the constraints, and Slater’s condition fails.\n\nNext, we derive the dual problem from the Lagrangian. The Lagrangian for the equality-constrained cone program (incorporating the equality constraints via Lagrange multipliers and handling the cone constraint through the dual function) is\n$$L(X, y) = \\langle C, X \\rangle + \\langle y, b - \\mathcal{A}(X) \\rangle = \\langle C, X \\rangle - \\langle \\mathcal{A}^{*}(y), X \\rangle + \\langle y, b \\rangle,$$\nwhere the adjoint operator $\\mathcal{A}^{*}$ is defined by\n$$\\langle \\mathcal{A}(X), y \\rangle = \\langle X, \\mathcal{A}^{*}(y) \\rangle \\quad \\text{for all } X \\in \\mathbb{S}^{2}, \\ y \\in \\mathbb{R}^{2}.$$\nFor our specific $\\mathcal{A}$, we have\n$$\\langle \\mathcal{A}(X), y \\rangle = y_{1} \\langle E_{11}, X \\rangle + y_{2} \\langle E_{22}, X \\rangle = \\langle y_{1} E_{11} + y_{2} E_{22}, X \\rangle,$$\nso\n$$\\mathcal{A}^{*}(y) = y_{1} E_{11} + y_{2} E_{22}.$$\nThus the Lagrangian becomes\n$$L(X, y) = \\langle C - \\mathcal{A}^{*}(y), X \\rangle + \\langle y, b \\rangle.$$\nThe dual function is\n$$g(y) = \\inf_{X \\succeq 0} L(X, y) = \\inf_{X \\succeq 0} \\left( \\langle C - \\mathcal{A}^{*}(y), X \\rangle + \\langle y, b \\rangle \\right).$$\nFor the infimum over $X \\succeq 0$ to be finite, we must have $C - \\mathcal{A}^{*}(y) \\succeq 0$; otherwise, since $\\langle C - \\mathcal{A}^{*}(y), X \\rangle$ can be made arbitrarily negative by scaling a direction in the positive semidefinite cone, $g(y) = -\\infty$. When $C - \\mathcal{A}^{*}(y) \\succeq 0$, the infimum over $X \\succeq 0$ of the linear form $\\langle C - \\mathcal{A}^{*}(y), X \\rangle$ is $0$ (achieved at $X = 0$), hence\n$$g(y) = \\begin{cases}\n\\langle y, b \\rangle, & \\text{if } C - \\mathcal{A}^{*}(y) \\succeq 0, \\\\\n-\\infty, & \\text{otherwise.}\n\\end{cases}$$\nTherefore, the dual problem is\n$$\\max_{y \\in \\mathbb{R}^{2}} \\ \\langle y, b \\rangle \\quad \\text{subject to} \\quad C - \\mathcal{A}^{*}(y) \\succeq 0.$$\nWith $C = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$ and $\\mathcal{A}^{*}(y) = y_{1} E_{11} + y_{2} E_{22}$, the dual constraint becomes\n$$C - \\mathcal{A}^{*}(y) = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} - \\begin{pmatrix} y_{1} & 0 \\\\ 0 & y_{2} \\end{pmatrix} = \\begin{pmatrix} -y_{1} & 1 \\\\ 1 & -y_{2} \\end{pmatrix} \\succeq 0.$$\nThe dual objective is $\\langle y, b \\rangle = y_{1} \\cdot 0 + y_{2} \\cdot 1 = y_{2}$. Hence the dual problem is\n$$\\max \\ y_{2} \\quad \\text{subject to} \\quad \\begin{pmatrix} -y_{1} & 1 \\\\ 1 & -y_{2} \\end{pmatrix} \\succeq 0.$$\n\nWe now compute the primal and dual optimal values. For the primal, the unique feasible matrix is\n$$X^{\\star} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix},$$\nand the primal objective value is\n$$p^{\\star} = \\langle C, X^{\\star} \\rangle = \\operatorname{tr}(C X^{\\star}) = \\operatorname{tr}\\left( \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\right) = 0.$$\n\nFor the dual, we analyze the positive semidefiniteness of the $2 \\times 2$ symmetric matrix\n$$M(y) = \\begin{pmatrix} -y_{1} & 1 \\\\ 1 & -y_{2} \\end{pmatrix}.$$\nA $2 \\times 2$ symmetric matrix is positive semidefinite if and only if both diagonal entries are nonnegative and the determinant is nonnegative. Thus we need\n$$-y_{1} \\ge 0 \\quad \\text{and} \\quad -y_{2} \\ge 0 \\quad \\text{and} \\quad \\det(M(y)) = (-y_{1})(-y_{2}) - 1 = y_{1} y_{2} - 1 \\ge 0.$$\nEquivalently,\n$$y_{1} \\le 0, \\quad y_{2} \\le 0, \\quad y_{1} y_{2} \\ge 1.$$\nWe aim to maximize $y_{2}$ subject to these constraints. Because $y_{2} \\le 0$, the dual objective is bounded above by $0$. The product constraint $y_{1} y_{2} \\ge 1$ with $y_{1} \\le 0$ and $y_{2} \\le 0$ implies\n$$y_{2} \\le \\frac{1}{y_{1}},$$\nsince dividing both sides of $y_{1} y_{2} \\ge 1$ by $y_{1} \\le 0$ reverses the inequality. For any feasible $y_{1} \\le 0$, we have $1/y_{1} < 0$, so the tightest upper bound for $y_{2}$ is the minimum of $0$ and $1/y_{1}$, i.e.,\n$$y_{2} \\le \\min\\left(0, \\frac{1}{y_{1}}\\right).$$\nAs $y_{1} \\to -\\infty$, $\\frac{1}{y_{1}} \\to 0^{-}$, so the supremum of feasible $y_{2}$ values is $0$, but it is not attained because $y_{2} = 0$ would violate $y_{1} y_{2} \\ge 1$. Thus the dual optimal value is\n$$d^{\\star} = 0.$$\n\nFinally, the duality gap is\n$$p^{\\star} - d^{\\star} = 0 - 0 = 0.$$\nThis example exhibits failure of Slater’s condition (no strictly feasible primal point) while still having a zero duality gap; the dual supremum is not attained.\n\nTherefore, the requested explicit duality gap is $0$.",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}