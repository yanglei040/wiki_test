{
    "hands_on_practices": [
        {
            "introduction": "Mastering the Karush-Kuhn-Tucker (KKT) conditions begins with applying them to a clear, well-behaved problem. This first exercise provides a foundational walkthrough, where you will minimize a linear function over a region defined by a parabolic constraint. By systematically applying the KKT conditions—from checking the constraint qualification to solving for the optimal point and multiplier—you will build a concrete understanding of the mechanics and see firsthand how the algebraic condition of gradient alignment corresponds to the geometric principle of tangency at the optimal solution. ",
            "id": "3140454",
            "problem": "Consider the minimization problem in two variables $x_1$ and $x_2$ with objective function $f(x) = x_1 + x_2$ and a single inequality constraint $g(x) = x_1^{2} - x_2 \\le 0$. Starting from the fundamental definitions of the Lagrangian and the Karush-Kuhn-Tucker (KKT) necessary conditions for inequality-constrained optimization, and invoking a valid constraint qualification from first principles, perform the following:\n\n- Establish whether a constraint qualification holds for this problem and explain why the KKT conditions are applicable.\n- Use the KKT framework to determine the candidate optimal point $(x_1^{\\star}, x_2^{\\star})$ and the corresponding Lagrange multiplier $\\lambda^{\\star}$.\n- Demonstrate that the solution lies on the boundary of the feasible set and that the objective’s level set is tangent to the parabolic boundary described by $x_2 = x_1^{2}$ at the optimal point, using geometric or differential arguments derived from first principles.\n- Report the exact optimal objective value $f(x^{\\star})$.\n\nExpress the final reported value as an exact simplified number. Do not round.",
            "solution": "The problem is first validated against the required criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Objective Function**: $f(x) = x_1 + x_2$, where $x = (x_1, x_2)$.\n- **Variables**: $x_1$ and $x_2$.\n- **Constraint**: $g(x) = x_1^{2} - x_2 \\le 0$.\n- **Task**: Minimize $f(x)$ subject to $g(x) \\le 0$. Use the Karush-Kuhn-Tucker (KKT) framework, including verification of a constraint qualification, determination of the optimal point $(x_1^{\\star}, x_2^{\\star})$ and multiplier $\\lambda^{\\star}$, geometric analysis of the solution, and calculation of the optimal objective value $f(x^{\\star})$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is a standard exercise in nonlinear constrained optimization, a core topic in applied mathematics and engineering. It is based on well-established mathematical principles.\n- **Well-Posed**: The problem is well-posed. The objective function is linear (and thus convex), and the constraint function $g(x_1, x_2) = x_1^2 - x_2$ is convex. The Hessian of $g$ is $\\nabla^2 g(x) = \\begin{pmatrix} 2  0 \\\\ 0  0 \\end{pmatrix}$, which is positive semidefinite. Therefore, this is a convex optimization problem, for which a unique global minimum is expected to exist.\n- **Objective**: The problem statement is clear, precise, and uses standard mathematical terminology.\n- **Completeness and Consistency**: The problem is self-contained and provides all necessary information. There are no contradictions.\n- **Other Flaws**: The problem is not unrealistic, ill-posed, trivial, or outside the scope of scientific verifiability. It is a direct application of the specified method.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution Derivation\n\nThe optimization problem is to minimize the objective function $f(x_1, x_2) = x_1 + x_2$ subject to the inequality constraint $g(x_1, x_2) = x_1^2 - x_2 \\le 0$.\n\n**Constraint Qualification**\nBefore applying the KKT conditions, we must verify that a constraint qualification (CQ) holds. The CQ ensures that the geometry of the feasible set is well-behaved at the boundary, which guarantees that the KKT conditions are necessary for a point to be a local minimum.\n\nWe will use the Linear Independence Constraint Qualification (LICQ). For a point $x$ where a set of constraints $\\{g_i(x)=0\\}$ is active, LICQ requires that the gradients of these active constraints, $\\{\\nabla g_i(x)\\}$, are linearly independent.\n\nIn this problem, there is only one constraint, $g(x_1, x_2) = x_1^2 - x_2 \\le 0$. The gradient of the constraint function is:\n$$ \\nabla g(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial g}{\\partial x_1} \\\\ \\frac{\\partial g}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 \\\\ -1 \\end{pmatrix} $$\nFor LICQ to fail, the set of gradients of active constraints must be linearly dependent. With a single constraint, this would mean the gradient vector is the zero vector. However, $\\nabla g(x_1, x_2)$ can never be the zero vector, as its second component is always $-1$. Thus, the gradient is non-zero for all points $(x_1, x_2)$. This means that LICQ holds for all feasible points. Since a CQ is satisfied, the KKT conditions are necessary for optimality.\n\n**Karush-Kuhn-Tucker (KKT) Conditions**\nThe Lagrangian function for this problem is:\n$$ L(x_1, x_2, \\lambda) = f(x_1, x_2) + \\lambda g(x_1, x_2) = (x_1 + x_2) + \\lambda(x_1^2 - x_2) $$\nThe KKT conditions for a candidate optimal point $(x_1^{\\star}, x_2^{\\star})$ and a corresponding Lagrange multiplier $\\lambda^{\\star}$ are:\n\n1.  **Stationarity**: The gradient of the Lagrangian with respect to $x$ must be zero.\n    $$ \\nabla_x L(x_1^{\\star}, x_2^{\\star}, \\lambda^{\\star}) = \\begin{pmatrix} 1 + 2\\lambda^{\\star}x_1^{\\star} \\\\ 1 - \\lambda^{\\star} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\n2.  **Primal Feasibility**: The point must satisfy the problem's constraints.\n    $$ x_1^{\\star 2} - x_2^{\\star} \\le 0 $$\n3.  **Dual Feasibility**: The Lagrange multiplier for an inequality constraint of the form $g(x) \\le 0$ must be non-negative.\n    $$ \\lambda^{\\star} \\ge 0 $$\n4.  **Complementary Slackness**: The product of the Lagrange multiplier and the constraint function must be zero.\n    $$ \\lambda^{\\star}(x_1^{\\star 2} - x_2^{\\star}) = 0 $$\n\nNow, we solve this system of equations and inequalities.\nFrom the second component of the stationarity condition (1):\n$$ 1 - \\lambda^{\\star} = 0 \\implies \\lambda^{\\star} = 1 $$\nThis value satisfies the dual feasibility condition (3), since $1 \\ge 0$.\n\nBecause $\\lambda^{\\star} = 1 \\neq 0$, the complementary slackness condition (4) requires that the constraint must be active:\n$$ x_1^{\\star 2} - x_2^{\\star} = 0 \\implies x_2^{\\star} = x_1^{\\star 2} $$\nThis result demonstrates that the optimal solution must lie on the boundary of the feasible region, which is the parabola $x_2 = x_1^2$.\n\nNow, we use the first component of the stationarity condition (1) and substitute $\\lambda^{\\star} = 1$:\n$$ 1 + 2\\lambda^{\\star}x_1^{\\star} = 0 \\implies 1 + 2(1)x_1^{\\star} = 0 \\implies 2x_1^{\\star} = -1 \\implies x_1^{\\star} = -\\frac{1}{2} $$\nUsing the relation $x_2^{\\star} = x_1^{\\star 2}$, we find $x_2^{\\star}$:\n$$ x_2^{\\star} = \\left(-\\frac{1}{2}\\right)^2 = \\frac{1}{4} $$\nThe candidate optimal point is $(x_1^{\\star}, x_2^{\\star}) = (-\\frac{1}{2}, \\frac{1}{4})$ with the Lagrange multiplier $\\lambda^{\\star} = 1$.\nWe must verify that this point satisfies the primal feasibility condition (2):\n$$ \\left(-\\frac{1}{2}\\right)^2 - \\frac{1}{4} = \\frac{1}{4} - \\frac{1}{4} = 0 $$\nSince $0 \\le 0$, the condition is satisfied. All KKT conditions are met.\n\n**Geometric Interpretation**\nThe KKT framework requires tangency between the level set of the objective function and the boundary of the feasible set at the optimum. This geometric condition is captured algebraically by the stationarity condition $\\nabla f(x^{\\star}) + \\lambda^{\\star} \\nabla g(x^{\\star}) = 0$, which implies $\\nabla f(x^{\\star}) = -\\lambda^{\\star} \\nabla g(x^{\\star})$. The gradients of the objective and the active constraint must be parallel.\n\nLet's verify this at our solution point $x^{\\star} = (-\\frac{1}{2}, \\frac{1}{4})$.\nThe gradient of the objective function is constant:\n$$ \\nabla f(x_1, x_2) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\nThe gradient of the constraint function at the solution is:\n$$ \\nabla g(x_1^{\\star}, x_2^{\\star}) = \\begin{pmatrix} 2x_1^{\\star} \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2(-\\frac{1}{2}) \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix} $$\nThe stationarity condition requires:\n$$ \\nabla f(x^{\\star}) = -\\lambda^{\\star} \\nabla g(x^{\\star}) \\implies \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = -(1) \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix} $$\nThis holds, confirming the gradients are parallel.\n\nAlternatively, we can compare the slopes. The level sets of the objective function are lines $x_1 + x_2 = c$, which can be written as $x_2 = -x_1 + c$. The slope of these lines is $\\frac{dx_2}{dx_1} = -1$.\nThe boundary of the feasible region is the parabola $x_2 = x_1^2$. The slope of the tangent line to this parabola is given by its derivative: $\\frac{dx_2}{dx_1} = 2x_1$.\nAt the optimal point, $x_1^{\\star} = -\\frac{1}{2}$, the slope of the tangent to the parabola is:\n$$ \\frac{dx_2}{dx_1}\\bigg|_{x_1 = -1/2} = 2\\left(-\\frac{1}{2}\\right) = -1 $$\nSince the slope of the objective's level set $(-1)$ is equal to the slope of the constraint boundary at the point $(-\\frac{1}{2}, \\frac{1}{4})$, the two curves are tangent at this point. This fulfills the geometric requirement.\n\n**Optimal Objective Value**\nThe optimal point is $(x_1^{\\star}, x_2^{\\star}) = (-\\frac{1}{2}, \\frac{1}{4})$. The optimal value of the objective function is:\n$$ f(x^{\\star}) = f\\left(-\\frac{1}{2}, \\frac{1}{4}\\right) = x_1^{\\star} + x_2^{\\star} = -\\frac{1}{2} + \\frac{1}{4} = -\\frac{2}{4} + \\frac{1}{4} = -\\frac{1}{4} $$",
            "answer": "$$ \\boxed{-\\frac{1}{4}} $$"
        },
        {
            "introduction": "Real-world optimization problems are not always cleanly formulated and may include redundant constraints. This practice explores such a scenario to reveal a crucial theoretical nuance of the KKT conditions. By analyzing a problem where two constraints define the same boundary, you will investigate the failure of the Linear Independence Constraint Qualification (LICQ) and its direct consequence: the non-uniqueness of the Lagrange multipliers. This exercise deepens your understanding of why constraint qualifications are important and how the KKT framework behaves when they are not met. ",
            "id": "3140504",
            "problem": "Consider the constrained minimization problem with decision vector $x \\in \\mathbb{R}^2$:\n$$\\text{minimize } f(x) = (x_1 - 2)^2 + x_2^2$$\nsubject to the inequality constraints\n$$g_1(x):\\ x_1 \\le 1,\\qquad g_2(x):\\ 2x_1 \\le 2.$$\nThe constraint $g_2(x)$ is algebraically equivalent to $g_1(x)$ and thus redundant. Using only the fundamental definitions of constrained optimality and the Karush-Kuhn-Tucker (KKT) necessary conditions (without invoking any pre-stated shortcut formulas), reason about the optimizer, the activity of constraints, and the behavior of the Lagrange multipliers. Identify the implications of redundancy and whether KKT reveals it directly through the multipliers.\n\nSelect all statements that are correct:\n\nA. The unique optimizer is $x^\\star = (1, 0)$.\n\nB. At $x^\\star$, both constraints are active and the Linear Independence Constraint Qualification (LICQ) holds.\n\nC. The KKT multipliers associated with $g_1(x)$ and $g_2(x)$ are not unique; all pairs $(\\lambda_1, \\lambda_2)$ with $\\lambda_1 \\ge 0$, $\\lambda_2 \\ge 0$, and $\\lambda_1 + 2\\lambda_2 = 2$ satisfy the KKT stationarity and complementary slackness at $x^\\star$.\n\nD. KKT complementary slackness forces either $\\lambda_1 = 0$ or $\\lambda_2 = 0$ for any solution, thereby directly identifying that one constraint is redundant.\n\nE. If the redundant constraint $g_2(x)$ is removed and only $g_1(x)$ is kept, LICQ holds at the optimizer and the corresponding single KKT multiplier is unique.",
            "solution": "The user has provided a constrained optimization problem and a set of statements to evaluate. The task is to first validate the problem statement and then, if valid, derive the solution and analyze each statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Decision vector: $x \\in \\mathbb{R}^2$, where $x = (x_1, x_2)$.\n- Objective function to minimize: $f(x) = (x_1 - 2)^2 + x_2^2$.\n- Inequality constraints:\n  - $g_1(x): x_1 \\le 1$.\n  - $g_2(x): 2x_1 \\le 2$.\n- The problem notes that $g_2(x)$ is redundant.\n- The task requires using the Karush-Kuhn-Tucker (KKT) necessary conditions from fundamental definitions.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is a standard exercise in nonlinear programming, a well-established branch of mathematical optimization. The functions and constraints are mathematically sound.\n- **Well-Posedness:** The objective function $f(x)$ is a strictly convex function (its Hessian is $\\nabla^2 f(x) = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix}$, which is positive definite). The feasible set, defined by $x_1 \\le 1$, is a closed and convex set. The minimization of a strictly convex function over a non-empty convex set has a unique solution. The problem is well-posed.\n- **Objectivity:** The problem is stated using precise mathematical language without ambiguity or subjective claims.\n- **Flaw Checklist:**\n    1.  **Scientific/Factual Unsoundness:** None.\n    2.  **Non-Formalizable/Irrelevant:** The problem is directly relevant to the topic of KKT conditions.\n    3.  **Incomplete/Contradictory Setup:** The problem is self-contained. The redundancy of the constraints is an intentional feature designed to test understanding of KKT conditions, not a flaw. The constraints $x_1 \\le 1$ and $2x_1 \\le 2$ are equivalent and not contradictory.\n    4.  **Unrealistic/Infeasible:** The problem is purely mathematical; this category is not applicable.\n    5.  **Ill-Posed/Poorly Structured:** The problem is well-posed and clearly structured.\n    6.  **Pseudo-Profound/Trivial:** The problem is not trivial. It addresses the important concept of constraint qualifications and their implications for the KKT multipliers, a key topic in optimization theory.\n    7.  **Outside Scientific Verifiability:** The problem is a mathematical question verifiable by logical derivation.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the solution derivation.\n\n### Solution Derivation\n\n**1. Identify the Optimizer**\n\nThe objective function $f(x_1, x_2) = (x_1 - 2)^2 + x_2^2$ represents the squared Euclidean distance from the point $(2, 0)$ to a point $(x_1, x_2)$. The constraints are $x_1 \\le 1$ and $2x_1 \\le 2$, which both define the same feasible region: the closed half-plane $\\mathcal{F} = \\{ (x_1, x_2) \\in \\mathbb{R}^2 \\mid x_1 \\le 1 \\}$.\n\nThe problem is equivalent to finding the point in $\\mathcal{F}$ that is closest to $(2, 0)$. Geometrically, the point $(2, 0)$ lies outside the feasible region to the right of the boundary line $x_1 = 1$. The closest point in the feasible region to an external point is the orthogonal projection of the external point onto the boundary of the feasible set. The orthogonal projection of $(2, 0)$ onto the line $x_1 = 1$ is the point $(1, 0)$.\n\nTherefore, the unique optimizer is $x^\\star = (1, 0)$.\n\n**2. Formulate and Analyze KKT Conditions for the Original Problem**\n\nThe standard form for inequality constraints is $g_i(x) \\le 0$.\n- $g_1(x) = x_1 - 1 \\le 0$.\n- $g_2(x) = 2x_1 - 2 \\le 0$.\n\nThe Lagrangian function is given by:\n$$ \\mathcal{L}(x, \\lambda_1, \\lambda_2) = f(x) + \\lambda_1 g_1(x) + \\lambda_2 g_2(x) $$\n$$ \\mathcal{L}(x_1, x_2, \\lambda_1, \\lambda_2) = (x_1 - 2)^2 + x_2^2 + \\lambda_1(x_1 - 1) + \\lambda_2(2x_1 - 2) $$\n\nThe KKT necessary conditions at a local minimum $x^\\star$ are:\n1.  **Stationarity:** $\\nabla_x \\mathcal{L}(x^\\star, \\lambda^\\star) = 0$.\n2.  **Primal Feasibility:** $g_1(x^\\star) \\le 0$, $g_2(x^\\star) \\le 0$.\n3.  **Dual Feasibility:** $\\lambda_1^\\star \\ge 0$, $\\lambda_2^\\star \\ge 0$.\n4.  **Complementary Slackness:** $\\lambda_1^\\star g_1(x^\\star) = 0$, $\\lambda_2^\\star g_2(x^\\star) = 0$.\n\nLet's evaluate these conditions at the candidate optimizer $x^\\star = (1, 0)$.\n\n- **Primal Feasibility:**\n  - $g_1(1, 0) = 1 - 1 = 0$. The constraint holds.\n  - $g_2(1, 0) = 2(1) - 2 = 0$. The constraint holds.\n  Since $g_1(x^\\star) = 0$ and $g_2(x^\\star) = 0$, both constraints are **active** at the optimizer.\n\n- **Complementary Slackness:**\n  - $\\lambda_1 g_1(x^\\star) = \\lambda_1 \\cdot 0 = 0$. This is satisfied for any $\\lambda_1$.\n  - $\\lambda_2 g_2(x^\\star) = \\lambda_2 \\cdot 0 = 0$. This is satisfied for any $\\lambda_2$.\n  These conditions impose no restriction on the multipliers.\n\n- **Stationarity:**\n  The gradient of the Lagrangian with respect to $x$ is:\n  $$ \\nabla_x \\mathcal{L} = \\begin{pmatrix} \\frac{\\partial \\mathcal{L}}{\\partial x_1} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2(x_1 - 2) + \\lambda_1 + 2\\lambda_2 \\\\ 2x_2 \\end{pmatrix} $$\n  Setting $\\nabla_x \\mathcal{L} = 0$ at $x^\\star = (1, 0)$:\n  $$ \\begin{pmatrix} 2(1 - 2) + \\lambda_1 + 2\\lambda_2 \\\\ 2(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\n  This gives two equations:\n  1.  $-2 + \\lambda_1 + 2\\lambda_2 = 0 \\implies \\lambda_1 + 2\\lambda_2 = 2$.\n  2.  $0 = 0$.\n\n- **Dual Feasibility:**\n  We must have $\\lambda_1 \\ge 0$ and $\\lambda_2 \\ge 0$.\n\nCombining the conditions, for $x^\\star = (1, 0)$ to be a KKT point, there must exist Lagrange multipliers $(\\lambda_1, \\lambda_2)$ satisfying:\n- $\\lambda_1 + 2\\lambda_2 = 2$\n- $\\lambda_1 \\ge 0$\n- $\\lambda_2 \\ge 0$\n\nThis system defines a line segment in the $(\\lambda_1, \\lambda_2)$-plane with endpoints at $(2, 0)$ (when $\\lambda_2=0$) and $(0, 1)$ (when $\\lambda_1=0$). Since there is more than one pair $(\\lambda_1, \\lambda_2)$ that satisfies these conditions, the KKT multipliers are **not unique**.\n\n**3. Analyze Constraint Qualifications**\n\nThe Linear Independence Constraint Qualification (LICQ) requires the gradients of the active constraints at the point $x^\\star$ to be linearly independent.\nAt $x^\\star = (1, 0)$, both constraints are active. Their gradients are:\n- $\\nabla g_1(x) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- $\\nabla g_2(x) = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$\n\nThese two vectors are linearly dependent, since $\\nabla g_2(x) = 2 \\nabla g_1(x)$. Therefore, LICQ **does not hold** at the optimizer $x^\\star$. The failure of LICQ is the reason the KKT multipliers are not unique.\n\n### Option-by-Option Analysis\n\n**A. The unique optimizer is $x^\\star = (1, 0)$.**\nAs determined by the geometric argument of minimizing the distance from $(2, 0)$ to the half-plane $x_1 \\le 1$, the minimizer is indeed $x^\\star=(1,0)$. Since the objective function $f(x)$ is strictly convex and the feasible set is convex, this optimizer is unique.\n**Verdict: Correct.**\n\n**B. At $x^\\star$, both constraints are active and the Linear Independence Constraint Qualification (LICQ) holds.**\nWe found that at $x^\\star = (1, 0)$, both $g_1(x^\\star) = 0$ and $g_2(x^\\star) = 0$, so both constraints are active. However, we also showed that the gradients of these active constraints, $\\nabla g_1 = (1, 0)^T$ and $\\nabla g_2 = (2, 0)^T$, are linearly dependent. Thus, LICQ fails to hold. The statement is a conjunction, and since the second part is false, the entire statement is false.\n**Verdict: Incorrect.**\n\n**C. The KKT multipliers associated with $g_1(x)$ and $g_2(x)$ are not unique; all pairs $(\\lambda_1, \\lambda_2)$ with $\\lambda_1 \\ge 0$, $\\lambda_2 \\ge 0$, and $\\lambda_1 + 2\\lambda_2 = 2$ satisfy the KKT stationarity and complementary slackness at $x^\\star$.**\nOur analysis of the KKT conditions at $x^\\star = (1, 0)$ led precisely to the conditions $\\lambda_1 + 2\\lambda_2 = 2$, $\\lambda_1 \\ge 0$, and $\\lambda_2 \\ge 0$. Any pair $(\\lambda_1, \\lambda_2)$ satisfying these conditions is a valid set of KKT multipliers. This set represents a line segment and is therefore not a unique point. The statement accurately describes the set of valid multipliers.\n**Verdict: Correct.**\n\n**D. KKT complementary slackness forces either $\\lambda_1 = 0$ or $\\lambda_2 = 0$ for any solution, thereby directly identifying that one constraint is redundant.**\nThe complementary slackness conditions are $\\lambda_1 g_1(x^\\star) = 0$ and $\\lambda_2 g_2(x^\\star) = 0$. Since both constraints are active ($g_1(x^\\star) = 0$ and $g_2(x^\\star) = 0$), these equations become $\\lambda_1 \\cdot 0 = 0$ and $\\lambda_2 \\cdot 0 = 0$. These are satisfied for any values of $\\lambda_1$ and $\\lambda_2$ and impose no restrictions on them. It is not true that either $\\lambda_1=0$ or $\\lambda_2=0$ must hold. For example, from option C, we can choose $\\lambda_2 = 0.5$, which gives $\\lambda_1 = 1$. The pair $(1, 0.5)$ is a valid set of multipliers where both are strictly positive. Therefore, complementary slackness does not force either multiplier to be zero.\n**Verdict: Incorrect.**\n\n**E. If the redundant constraint $g_2(x)$ is removed and only $g_1(x)$ is kept, LICQ holds at the optimizer and the corresponding single KKT multiplier is unique.**\nLet's analyze the simplified problem: minimize $f(x)$ subject to $g_1(x) = x_1 - 1 \\le 0$.\nThe optimizer remains $x^\\star = (1, 0)$. At this point, the single constraint $g_1(x)$ is active.\n- **LICQ:** The set of active constraint gradients consists of the single vector $\\nabla g_1(x^\\star) = (1, 0)^T$. A set containing one non-zero vector is always linearly independent. So, LICQ holds.\n- **KKT Multiplier:** The Lagrangian is $\\mathcal{L}(x, \\mu_1) = (x_1 - 2)^2 + x_2^2 + \\mu_1(x_1 - 1)$. The stationarity condition is $\\nabla_x \\mathcal{L}(x^\\star, \\mu_1) = 0$.\n$$ \\begin{pmatrix} 2(1 - 2) + \\mu_1 \\\\ 2(0) \\end{pmatrix} = \\begin{pmatrix} -2 + \\mu_1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\nThis gives $-2 + \\mu_1 = 0$, so $\\mu_1 = 2$. This is a unique value. We also check dual feasibility: $\\mu_1 = 2 \\ge 0$, which is satisfied.\nSince LICQ holds, the KKT multiplier is guaranteed to be unique. Our calculation confirms this uniqueness. The statement is correct.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "Finding points that satisfy the KKT conditions is a necessary first step, but it does not guarantee a minimum. This is especially true for problems with non-convex objective functions, where KKT points can correspond to local minima, local maxima, or saddle points. This advanced practice challenges you to go beyond first-order analysis by employing second-order necessary conditions to classify the candidate points. You will analyze the curvature of the Lagrangian on the tangent space of the constraints, providing a definitive method to distinguish true local minimizers from other stationary points. ",
            "id": "3140538",
            "problem": "Consider the constrained optimization problem with a nonconvex objective and a convex feasible set:\n$$\\text{minimize } f(x,y) = x^{2} + y^{4} - 2 y^{2} \\quad \\text{subject to } h(x,y) = x + y = 0.$$\nThe feasible set is the affine subspace defined by the linear equality constraint, which is convex. Assume differentiability of all functions and that the Linear Independence Constraint Qualification (LICQ) holds at all feasible points. Starting from fundamental definitions in constrained optimization, and without assuming any ready-made classification shortcut, proceed as follows:\n\n1. Derive the Karush-Kuhn-Tucker (KKT) necessary conditions for equality-constrained problems by introducing the Lagrangian and enforcing stationarity together with primal feasibility. Write down these conditions explicitly for the given problem and solve for all KKT points.\n\n2. To differentiate local minima from other stationary behaviors at the KKT points, start from the second-order necessary condition: for a local minimum, every feasible direction must have a nonnegative second directional derivative of the Lagrangian. Restrict attention to the tangent space of the constraint at each KKT point, characterize the feasible directions, and evaluate the quadratic form of the Hessian of the Lagrangian along those directions to classify each KKT point as a local minimum, a local maximum, or neither.\n\n3. Report the lowest objective value attained among all local minima. Provide the final answer as a single exact real number with no units. If an approximation were required, you would be instructed about significant figures, but here an exact value is expected.\n\nYour final answer must be a single real number.",
            "solution": "The problem asks for the minimum value attained at local minima for a constrained optimization problem. The solution proceeds in three stages as per the problem description: first, we find all points satisfying the Karush-Kuhn-Tucker (KKT) necessary conditions; second, we use second-order conditions to classify these points; and third, we compute the objective function at the local minima to find the lowest value.\n\nThe optimization problem is:\n$$\n\\begin{aligned}\n\\text{minimize }  f(x,y) = x^{2} + y^{4} - 2 y^{2} \\\\\n\\text{subject to }  h(x,y) = x + y = 0\n\\end{aligned}\n$$\n\n**1. Derivation and Solution of KKT Conditions**\n\nFor an equality-constrained optimization problem of the form $\\min f(x)$ subject to $h(x) = 0$, where functions are differentiable and a constraint qualification like LICQ holds, a necessary condition for a point $x^*$ to be a local minimum is the existence of a Lagrange multiplier $\\lambda^*$ such that the KKT conditions are satisfied. These conditions are stationarity and primal feasibility.\n\nFirst, we define the Lagrangian function $\\mathcal{L}(x, y, \\lambda)$:\n$$\n\\mathcal{L}(x, y, \\lambda) = f(x,y) + \\lambda h(x,y) = (x^{2} + y^{4} - 2 y^{2}) + \\lambda(x + y)\n$$\n\nThe KKT conditions are:\n- **Stationarity:** The gradient of the Lagrangian with respect to the decision variables $(x, y)$ must be the zero vector.\n$$\n\\nabla_{x,y} \\mathcal{L}(x, y, \\lambda) = 0\n$$\n- **Primal Feasibility:** The point $(x, y)$ must satisfy the original constraints.\n$$\nh(x,y) = 0\n$$\n\nLet's write these conditions explicitly for the given problem.\n\nThe partial derivatives of the Lagrangian are:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x} = 2x + \\lambda = 0 \\quad (1)\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial y} = 4y^{3} - 4y + \\lambda = 0 \\quad (2)\n$$\n\nThe primal feasibility condition is:\n$$\nx + y = 0 \\quad (3)\n$$\n\nWe now solve this system of three equations for $x$, $y$, and $\\lambda$.\nFrom (3), we have $x = -y$.\nSubstituting $x = -y$ into (1) gives $\\lambda = -2x = -2(-y) = 2y$.\nNow, we substitute this expression for $\\lambda$ into (2):\n$$\n4y^{3} - 4y + (2y) = 0\n$$\n$$\n4y^{3} - 2y = 0\n$$\n$$\n2y(2y^{2} - 1) = 0\n$$\nThis equation yields three possible solutions for $y$:\n- $y = 0$\n- $2y^{2} - 1 = 0 \\implies y^{2} = \\frac{1}{2} \\implies y = \\pm \\frac{1}{\\sqrt{2}} = \\pm \\frac{\\sqrt{2}}{2}$\n\nFor each value of $y$, we find the corresponding $x$ and $\\lambda$ to identify the KKT points $(x, y, \\lambda)$.\n\n- **Case 1: $y_1 = 0$**\n  - $x_1 = -y_1 = 0$\n  - $\\lambda_1 = 2y_1 = 0$\n  - The first KKT point is $(x_1, y_1) = (0, 0)$ with multiplier $\\lambda_1 = 0$.\n\n- **Case 2: $y_2 = \\frac{\\sqrt{2}}{2}$**\n  - $x_2 = -y_2 = -\\frac{\\sqrt{2}}{2}$\n  - $\\lambda_2 = 2y_2 = 2\\left(\\frac{\\sqrt{2}}{2}\\right) = \\sqrt{2}$\n  - The second KKT point is $(x_2, y_2) = (-\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2})$ with multiplier $\\lambda_2 = \\sqrt{2}$.\n\n- **Case 3: $y_3 = -\\frac{\\sqrt{2}}{2}$**\n  - $x_3 = -y_3 = \\frac{\\sqrt{2}}{2}$\n  - $\\lambda_3 = 2y_3 = 2\\left(-\\frac{\\sqrt{2}}{2}\\right) = -\\sqrt{2}$\n  - The third KKT point is $(x_3, y_3) = (\\frac{\\sqrt{2}}{2}, -\\frac{\\sqrt{2}}{2})$ with multiplier $\\lambda_3 = -\\sqrt{2}$.\n\nWe have found three KKT points: $(0, 0)$, $(-\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2})$, and $(\\frac{\\sqrt{2}}{2}, -\\frac{\\sqrt{2}}{2})$.\n\n**2. Second-Order Analysis**\n\nTo classify these stationary points, we use the second-order necessary and sufficient conditions. For a KKT point $(x^*, \\lambda^*)$ to be a local minimum, the Hessian of the Lagrangian with respect to the decision variables, $\\nabla^2_{x,y} \\mathcal{L}(x^*, \\lambda^*)$, must be positive semidefinite on the tangent space of the constraints at $x^*$. That is, $d^T \\nabla^2_{x,y} \\mathcal{L}(x^*, \\lambda^*) d \\geq 0$ for all feasible directions $d$. If the inequality is strict ($0$) for all non-zero feasible directions, the point is a strict local minimum.\n\nFirst, we find the tangent space. The constraint is $h(x,y) = x+y=0$. The gradient of the constraint is $\\nabla h(x,y) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. The tangent space at any feasible point is the set of vectors $d = \\begin{pmatrix} d_x \\\\ d_y \\end{pmatrix}$ such that $\\nabla h(x,y)^T d = 0$.\n$$\n\\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} d_x \\\\ d_y \\end{pmatrix} = d_x + d_y = 0\n$$\nThis implies $d_y = -d_x$. Thus, any feasible direction vector $d$ is proportional to $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. We can choose $d = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ as a basis vector for the tangent space.\n\nNext, we calculate the Hessian of the Lagrangian, $\\nabla^2_{x,y} \\mathcal{L}$:\n$$\n\\frac{\\partial^2 \\mathcal{L}}{\\partial x^2} = 2\n$$\n$$\n\\frac{\\partial^2 \\mathcal{L}}{\\partial y \\partial x} = \\frac{\\partial^2 \\mathcal{L}}{\\partial x \\partial y} = 0\n$$\n$$\n\\frac{\\partial^2 \\mathcal{L}}{\\partial y^2} = 12y^{2} - 4\n$$\nThe Hessian matrix is:\n$$\n\\nabla^2_{x,y} \\mathcal{L}(x,y,\\lambda) = \\begin{pmatrix} 2  0 \\\\ 0  12y^{2} - 4 \\end{pmatrix}\n$$\nNote that the Hessian depends only on $y$. We now evaluate the quadratic form $d^T \\nabla^2_{x,y} \\mathcal{L} d$ for $d = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ at each KKT point.\n$$\nd^T \\nabla^2_{x,y} \\mathcal{L} d = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  12y^{2} - 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 2(1)^2 + (12y^{2} - 4)(-1)^2 = 2 + 12y^{2} - 4 = 12y^{2} - 2\n$$\n\nNow we test each KKT point:\n- **For point $(0, 0)$:**\n  Here $y=0$. The value of the quadratic form is $12(0)^2 - 2 = -2$. Since $-2  0$, the point $(0,0)$ is a **local maximum**.\n\n- **For point $(-\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2})$:**\n  Here $y = \\frac{\\sqrt{2}}{2}$, so $y^2 = \\frac{1}{2}$. The value of the quadratic form is $12(\\frac{1}{2}) - 2 = 6 - 2 = 4$. Since $4  0$, the point $(-\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2})$ is a **strict local minimum**.\n\n- **For point $(\\frac{\\sqrt{2}}{2}, -\\frac{\\sqrt{2}}{2})$:**\n  Here $y = -\\frac{\\sqrt{2}}{2}$, so $y^2 = \\frac{1}{2}$. The value of the quadratic form is $12(\\frac{1}{2}) - 2 = 6 - 2 = 4$. Since $4  0$, the point $(\\frac{\\sqrt{2}}{2}, -\\frac{\\sqrt{2}}{2})$ is also a **strict local minimum**.\n\n**3. Lowest Objective Value**\n\nThe problem asks for the lowest objective value attained among all local minima. We have two such points. We evaluate the objective function $f(x,y) = x^2 + y^4 - 2y^2$ at each of them.\n\n- At $(x,y) = (-\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2})$:\n  $x^2 = (-\\frac{\\sqrt{2}}{2})^2 = \\frac{1}{2}$\n  $y^2 = (\\frac{\\sqrt{2}}{2})^2 = \\frac{1}{2}$\n  $y^4 = (\\frac{1}{2})^2 = \\frac{1}{4}$\n  $f(-\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2}) = \\frac{1}{2} + \\frac{1}{4} - 2\\left(\\frac{1}{2}\\right) = \\frac{1}{2} + \\frac{1}{4} - 1 = \\frac{3}{4} - 1 = -\\frac{1}{4}$.\n\n- At $(x,y) = (\\frac{\\sqrt{2}}{2}, -\\frac{\\sqrt{2}}{2})$:\n  $x^2 = (\\frac{\\sqrt{2}}{2})^2 = \\frac{1}{2}$\n  $y^2 = (-\\frac{\\sqrt{2}}{2})^2 = \\frac{1}{2}$\n  $y^4 = (\\frac{1}{2})^2 = \\frac{1}{4}$\n  $f(\\frac{\\sqrt{2}}{2}, -\\frac{\\sqrt{2}}{2}) = \\frac{1}{2} + \\frac{1}{4} - 2\\left(\\frac{1}{2}\\right) = \\frac{1}{2} + \\frac{1}{4} - 1 = \\frac{3}{4} - 1 = -\\frac{1}{4}$.\n\nBoth local minima yield the same objective value of $-\\frac{1}{4}$. Therefore, the lowest objective value attained among all local minima is $-\\frac{1}{4}$.\nSince the feasible set $x+y=0$ is unbounded ($y \\to \\pm \\infty$), we must consider if the function is unbounded below on this set. Substituting $x=-y$ into $f(x,y)$ gives $g(y) = y^4 - y^2$. As $y \\to \\pm \\infty$, $g(y) \\to +\\infty$. So the function is bounded below, and the value we found, $-\\frac{1}{4}$, is the global minimum value.",
            "answer": "$$\n\\boxed{-\\frac{1}{4}}\n$$"
        }
    ]
}