{
    "hands_on_practices": [
        {
            "introduction": "本练习是理解最速下降法收敛性的基础。通过构建一个具有已知特征值结构的二次函数，我们可以精确控制其条件数。这使我们能够通过编程实验，亲手验证收敛所需的迭代次数与目标精度之间的对数关系，并直观地感受条件数如何影响算法的收敛速度。",
            "id": "3149696",
            "problem": "你需要在一个严格凸二次目标函数上实现带有精确线搜索的最速下降法，并经验性地检验将优化误差降低一个指定因子需要多少次迭代。该研究应验证所需的迭代次数与要求的误差缩减因子成对数关系。使用一个具有已知特征结构的二次目标函数，以便其条件数可以被精确控制。\n\n使用以下基本依据：\n- 严格凸二次目标函数 $f(\\mathbf{x})$ 的形式为 $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} \\mathbf{A} \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}$，其中 $\\mathbf{A}$ 是实对称正定矩阵。对称正定（SPD）意味着 $\\mathbf{A} = \\mathbf{A}^{\\top}$ 并且对所有非零向量 $\\mathbf{z}$ 都有 $\\mathbf{z}^{\\top}\\mathbf{A}\\mathbf{z} > 0$。\n- 唯一极小值点 $\\mathbf{x}^{\\star}$ 满足 $\\mathbf{A}\\mathbf{x}^{\\star} = \\mathbf{b}$。\n- 最速下降法生成迭代序列 $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k)$，其中 $\\alpha_k$ 是通过在 $\\mathbf{x}_k$ 处的负梯度方向上进行精确一维最小化来选择的。\n\n你的任务：\n- 对于下述每个测试用例，通过将 $\\mathbf{A}$ 设为以指定特征值为对角元素的对角矩阵来构建SPD矩阵 $\\mathbf{A}$，选择极小值点为 $\\mathbf{x}^{\\star}$，其分量为 $x^{\\star}_i = i$（$i = 1, \\dots, n$），设置 $\\mathbf{b} = \\mathbf{A}\\mathbf{x}^{\\star}$，并初始化 $\\mathbf{x}_0 = \\mathbf{0}$。\n- 从 $\\mathbf{x}_0$ 开始，实现带精确线搜索的最速下降法。令 $f^{\\star} = f(\\mathbf{x}^{\\star})$ 并定义初始误差 $E_0 = f(\\mathbf{x}_0) - f^{\\star}$。\n- 对于给定的目标指数 $p$，运行迭代，直到找到满足 $f(\\mathbf{x}_k) - f^{\\star} \\leq E_0 \\cdot 10^{-p}$ 的第一个索引 $k$。将此 $k$ 记录为该测试用例所需的迭代次数。所有角度（如有）都应解释为弧度，但本问题中不使用角度。\n- 目标是观察这个 $k$ 如何依赖于 $p$ 和 $\\mathbf{A}$ 的谱，从而验证对于固定的 $\\mathbf{A}$，所需的迭代次数与 $\\log(10^p) = p \\log(10)$ 成正比增长。\n\n测试套件：\n- 用例1：维度 $n = 6$；特征值：三个元素等于 $1$，三个元素等于 $9$（条件数 $\\kappa = 9$）；目标指数 $p = 1$。\n- 用例2：维度 $n = 6$；特征值：三个元素等于 $1$，三个元素等于 $9$（条件数 $\\kappa = 9$）；目标指数 $p = 3$。\n- 用例3：维度 $n = 5$；特征值：所有元素等于 $7$（条件数 $\\kappa = 1$）；目标指数 $p = 5$。\n- 用例4：维度 $n = 8$；特征值：四个元素等于 $1$，四个元素等于 $100$（条件数 $\\kappa = 100$）；目标指数 $p = 2$。\n- 用例5：维度 $n = 10$；特征值：五个元素等于 $1$，五个元素等于 $1000$（条件数 $\\kappa = 1000$）；目标指数 $p = 3$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按上述顺序包含五个测试用例所需的迭代次数（整数）。例如，输出应类似于 $[k_1,k_2,k_3,k_4,k_5]$，其中每个 $k_i$ 是为用例 $i$ 计算出的整数迭代次数。",
            "solution": "问题陈述已经过分析，并被认为是有效的。这是一个来自数值优化领域的适定的、有科学依据的客观问题。为得到唯一且有意义的解，所有必要的数据和定义均已提供。\n\n核心任务是为一类特定的二次目标函数实现带有精确线搜索的最速下降法，并确定达到优化误差的指定缩减所需的迭代次数。\n\n目标函数是以下形式的严格凸二次函数：\n$$\nf(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^{\\top} \\mathbf{A} \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}\n$$\n其中 $\\mathbf{A}$ 是一个 $n \\times n$ 的实对称正定（SPD）矩阵，且 $\\mathbf{x}, \\mathbf{b} \\in \\mathbb{R}^n$。\n\n该函数的梯度由下式给出：\n$$\n\\nabla f(\\mathbf{x}) = \\mathbf{A} \\mathbf{x} - \\mathbf{b}\n$$\n函数 $f(\\mathbf{x})$ 有一个唯一的全局极小值点，记为 $\\mathbf{x}^{\\star}$，它是线性系统 $\\nabla f(\\mathbf{x}^{\\star}) = \\mathbf{0}$ 的解，即：\n$$\n\\mathbf{A}\\mathbf{x}^{\\star} = \\mathbf{b}\n$$\n函数的最小值为 $f^{\\star} = f(\\mathbf{x}^{\\star})$。将 $\\mathbf{b} = \\mathbf{A}\\mathbf{x}^{\\star}$ 代入 $f^{\\star}$ 的表达式中可得：\n$$\nf^{\\star} = \\frac{1}{2} (\\mathbf{x}^{\\star})^{\\top} \\mathbf{A} \\mathbf{x}^{\\star} - (\\mathbf{A}\\mathbf{x}^{\\star})^{\\top} \\mathbf{x}^{\\star} = \\frac{1}{2} (\\mathbf{x}^{\\star})^{\\top} \\mathbf{A} \\mathbf{x}^{\\star} - (\\mathbf{x}^{\\star})^{\\top} \\mathbf{A}^{\\top} \\mathbf{x}^{\\star}\n$$\n由于 $\\mathbf{A}$ 是对称的（$\\mathbf{A} = \\mathbf{A}^{\\top}$），上式可简化为：\n$$\nf^{\\star} = -\\frac{1}{2} (\\mathbf{x}^{\\star})^{\\top} \\mathbf{A} \\mathbf{x}^{\\star} = -\\frac{1}{2} (\\mathbf{x}^{\\star})^{\\top} \\mathbf{b}\n$$\n最速下降法是一种迭代算法，它从一个初始猜测值 $\\mathbf{x}_0$ 开始，生成一个点序列 $\\{\\mathbf{x}_k\\}$。其更新规则为：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k\n$$\n其中 $\\mathbf{p}_k$ 是最速下降方向，定义为在 $\\mathbf{x}_k$ 处的负梯度：\n$$\n\\mathbf{p}_k = - \\nabla f(\\mathbf{x}_k) = -(\\mathbf{A}\\mathbf{x}_k - \\mathbf{b})\n$$\n让我们将第 $k$ 步的梯度记为 $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$。更新规则变为：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\mathbf{g}_k\n$$\n步长 $\\alpha_k$ 的选择是为了最小化函数 $f$ 沿着从 $\\mathbf{x}_k$ 出发、方向为 $\\mathbf{p}_k$ 的射线上的值。这被称为精确线搜索。我们寻求找到一个 $\\alpha_k > 0$ 来最小化一维函数 $\\phi(\\alpha) = f(\\mathbf{x}_k - \\alpha \\mathbf{g}_k)$。通过将关于 $\\alpha$ 的导数设为零来找到最小值：\n$$\n\\frac{d\\phi}{d\\alpha} = \\nabla f(\\mathbf{x}_k - \\alpha \\mathbf{g}_k)^{\\top} \\cdot (-\\mathbf{g}_k) = 0\n$$\n代入梯度的表达式：\n$$\n(\\mathbf{A}(\\mathbf{x}_k - \\alpha \\mathbf{g}_k) - \\mathbf{b})^{\\top} (-\\mathbf{g}_k) = 0\n$$\n$$\n((\\mathbf{A}\\mathbf{x}_k - \\mathbf{b}) - \\alpha \\mathbf{A} \\mathbf{g}_k)^{\\top} \\mathbf{g}_k = 0\n$$\n认识到 $\\mathbf{g}_k = \\mathbf{A}\\mathbf{x}_k - \\mathbf{b}$：\n$$\n(\\mathbf{g}_k - \\alpha \\mathbf{A} \\mathbf{g}_k)^{\\top} \\mathbf{g}_k = 0\n$$\n$$\n\\mathbf{g}_k^{\\top}\\mathbf{g}_k - \\alpha \\mathbf{g}_k^{\\top} \\mathbf{A} \\mathbf{g}_k = 0\n$$\n求解 $\\alpha$，我们得到二次函数情况下的最优步长：\n$$\n\\alpha_k = \\frac{\\mathbf{g}_k^{\\top}\\mathbf{g}_k}{\\mathbf{g}_k^{\\top}\\mathbf{A}\\mathbf{g}_k}\n$$\n对每个测试用例，实现将按以下步骤进行。\n$1$. 将矩阵 $\\mathbf{A}$ 构建为以指定特征值为对角元素的对角矩阵。\n$2$. 定义极小值点 $\\mathbf{x}^{\\star}$，其分量为 $x^{\\star}_i = i$，$i \\in \\{1, \\dots, n\\}$。\n$3$. 由 $\\mathbf{b} = \\mathbf{A}\\mathbf{x}^{\\star}$ 计算向量 $\\mathbf{b}$。\n$4$. 初始点为 $\\mathbf{x}_0 = \\mathbf{0}$。\n$5$. 最小函数值 $f^{\\star}$ 计算为 $f^{\\star} = -0.5 \\cdot (\\mathbf{x}^{\\star})^{\\top}\\mathbf{b}$。\n$6$. 初始误差为 $E_0 = f(\\mathbf{x}_0) - f^{\\star}$。由于 $\\mathbf{x}_0 = \\mathbf{0}$，所以 $f(\\mathbf{x}_0) = 0$，因此 $E_0 = -f^{\\star}$。\n$7$. 目标误差值为 $E_{target} = E_0 \\cdot 10^{-p}$，其中 $p$ 是给定的目标指数。\n$8$. 算法从 $k=0$ 和 $\\mathbf{x}_k = \\mathbf{x}_0$ 开始迭代。在每次迭代中，计算一个新的点 $\\mathbf{x}_{k+1}$。当找到第一个满足 $f(\\mathbf{x}_k) - f^{\\star} \\leq E_{target}$ 的索引 $k$ 时，循环终止。这个 $k$ 的值就是该测试用例的结果。\n\n当条件数 $\\kappa(\\mathbf{A}) = \\lambda_{\\max}/\\lambda_{\\min} = 1$ 时，会出现一种特殊情况。这发生在所有特征值都相等时，即对于某个常数 $c > 0$，有 $\\mathbf{A} = c\\mathbf{I}$。在这种情况下，最速下降法保证在单次迭代中收敛。这是因为 $f(\\mathbf{x})$ 的等值集是超球面，在任何点 $\\mathbf{x}_k$ 的梯度都直接指向中心（即极小值点 $\\mathbf{x}^{\\star}$）。这将在用例3中观察到。",
            "answer": "```python\nimport numpy as np\n\ndef run_steepest_descent(dimension, eigenvalues, p_exponent):\n    \"\"\"\n    Implements the method of steepest descent to find the number of iterations\n    required to reduce the optimization error by a factor of 10**(-p).\n\n    Args:\n        dimension (int): The dimension of the vector space, n.\n        eigenvalues (list): A list of eigenvalues for the matrix A.\n        p_exponent (int): The target exponent for error reduction.\n\n    Returns:\n        int: The number of iterations k.\n    \"\"\"\n    n = dimension\n    p = p_exponent\n\n    # 1. Build the SPD matrix A\n    A = np.diag(eigenvalues)\n\n    # 2. Choose the minimizer x_star and compute b\n    x_star = np.arange(1, n + 1)\n    b = A @ x_star\n\n    # 3. Compute the minimum function value f_star\n    f_star = -0.5 * x_star.T @ b\n\n    # 4. Initialize steepest descent\n    x_k = np.zeros(n)\n    k = 0\n\n    # 5. Calculate initial error and target error\n    f_at_x0 = 0.5 * x_k.T @ A @ x_k - b.T @ x_k  # This will be 0\n    E_0 = f_at_x0 - f_star\n    \n    # Handle the trivial case where initial error is zero or negative.\n    if E_0 = 1e-14:\n        return 0\n        \n    target_error_value = E_0 * (10**(-p))\n    \n    # 6. Check if initial point already meets the criterion\n    # This check is implicitly handled by the loop structure.\n    # The loop will not execute if the condition is already met, returning k=0.\n    \n    # 7. Iteration loop\n    while True:\n        # Check current error before performing an iteration step\n        f_at_xk = 0.5 * x_k.T @ A @ x_k - b.T @ x_k\n        current_error = f_at_xk - f_star\n        if current_error = target_error_value:\n            break\n\n        # Calculate gradient\n        g_k = A @ x_k - b\n\n        # If gradient is zero, we are at the minimum.\n        if np.allclose(g_k, 0):\n            break\n\n        # Calculate optimal step size alpha_k\n        gkTg = g_k.T @ g_k\n        gkT_A_gk = g_k.T @ A @ g_k\n        alpha_k = gkTg / gkT_A_gk\n\n        # Update x_k\n        x_k = x_k - alpha_k * g_k\n        \n        # Increment iteration counter\n        k += 1\n\n    return k\n\ndef solve():\n    \"\"\"\n    Defines and runs the test cases, printing the results in the required format.\n    \"\"\"\n    test_cases = [\n        # Case 1: n=6, kappa=9, p=1\n        {\"dimension\": 6, \"eigenvalues\": [1, 1, 1, 9, 9, 9], \"p_exponent\": 1},\n        # Case 2: n=6, kappa=9, p=3\n        {\"dimension\": 6, \"eigenvalues\": [1, 1, 1, 9, 9, 9], \"p_exponent\": 3},\n        # Case 3: n=5, kappa=1, p=5\n        {\"dimension\": 5, \"eigenvalues\": [7, 7, 7, 7, 7], \"p_exponent\": 5},\n        # Case 4: n=8, kappa=100, p=2\n        {\"dimension\": 8, \"eigenvalues\": [1, 1, 1, 1, 100, 100, 100, 100], \"p_exponent\": 2},\n        # Case 5: n=10, kappa=1000, p=3\n        {\"dimension\": 10, \"eigenvalues\": [1]*5 + [1000]*5, \"p_exponent\": 3},\n    ]\n\n    results = []\n    for case in test_cases:\n        k = run_steepest_descent(**case)\n        results.append(k)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "在掌握了基本的最速下降法之后，一个自然的问题是如何选择更有效的步长。本练习将引导你探索超越精确线搜索的先进步长策略——Barzilai-Borwein (BB) 方法。你将实现两种经典的BB步长公式，并通过实验验证其步长与问题海森矩阵谱（即特征值）之间的深刻联系，并将其收敛性能与理论界进行比较。",
            "id": "3149736",
            "problem": "考虑严格凸二次函数 $f(x) = \\tfrac{1}{2} x^{\\top} Q x$ 的无约束最小化问题，其中 $Q \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。其梯度为 $\\nabla f(x) = Q x$。最速下降法生成迭代点 $x_{k+1} = x_k - \\alpha_k \\nabla f(x_k)$，其中步长 $\\alpha_k  0$。Barzilai–Borwein 步长利用了连续迭代点之间的曲率信息。令 $s_{k-1} = x_k - x_{k-1}$ 且 $y_{k-1} = \\nabla f(x_k) - \\nabla f(x_{k-1})$。对于二次函数情况，$y_{k-1} = Q s_{k-1}$。两种 Barzilai–Borwein 变体是：\n- BB1: $\\alpha_k = \\dfrac{s_{k-1}^{\\top} s_{k-1}}{s_{k-1}^{\\top} y_{k-1}}$，\n- BB2: $\\alpha_k = \\dfrac{s_{k-1}^{\\top} y_{k-1}}{y_{k-1}^{\\top} y_{k-1}}$。\n从 $x_0$ 开始迭代，并使用沿最速下降方向进行精确线搜索得到的初始步长，即 $\\alpha_0 = \\dfrac{\\nabla f(x_0)^{\\top} \\nabla f(x_0)}{\\nabla f(x_0)^{\\top} Q \\nabla f(x_0)}$。\n\n需要用到的基本事实：\n- 如果 $Q$ 是对称正定的，则其特征值满足 $0  \\lambda_{\\min} \\le \\lambda_{\\max}$，并且对于任何非零向量 $z$，瑞利商 $R_Q(z) = \\dfrac{z^{\\top} Q z}{z^{\\top} z}$ 位于 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 区间内。\n- 梯度和误差的递推关系为 $g_{k+1} = (I - \\alpha_k Q) g_k$ 和 $e_{k+1} = (I - \\alpha_k Q) e_k$，其中 $g_k = \\nabla f(x_k)$，$e_k = x_k - x^\\star$，且 $x^\\star = 0$。\n- 因此，经过 $m$ 步后，$g_m = P_m(Q) g_0$，其中 $P_m(t) = \\prod_{k=0}^{m-1} (1 - \\alpha_k t)$。根据谱定理，$\\|g_m\\|_2 \\le \\left(\\max_{i} |P_m(\\lambda_i(Q))|\\right) \\|g_0\\|_2$。\n\n任务：\n1) 对函数 $f(x) = \\tfrac{1}{2} x^{\\top} Q x$ 实现最速下降法，采用两种 Barzilai–Borwein 步长选择（BB1 和 BB2），并使用上面给出的精确线搜索步长 $\\alpha_0$ 进行第一次更新。运行最多 $m$ 次迭代，或直到 $\\|\\nabla f(x_k)\\|_2$ 在数值上为零（低于 $10^{-14}$）。对于每次运行，记录实际执行的步长序列 $\\{\\alpha_k\\}_{k=0}^{K-1}$，其中 $K \\le m$ 是实际执行的步数。\n2) 对每次运行，验证步长的包含性质：对于特征值在 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 内的对称正定矩阵 $Q$，每个 $\\alpha_k$ 必须位于 $\\left[\\dfrac{1}{\\lambda_{\\max}}, \\dfrac{1}{\\lambda_{\\min}}\\right]$ 区间内。报告一个布尔值，指示所有记录的步长是否在 $10^{-12}$ 的容差内满足此条件。\n3) 对每次运行，计算 $K$ 步后的经验收缩比 $r_{\\text{emp}} = \\dfrac{\\|g_K\\|_2}{\\|g_0\\|_2}$。\n4) 通过多项式谱界将经验收缩映射到 $Q$ 的谱上。使用 $Q$ 的真实特征值计算精确的谱界 $B = \\max_{i} \\left| \\prod_{k=0}^{K-1} (1 - \\alpha_k \\lambda_i(Q)) \\right|$，并报告 $B$。理论保证 $r_{\\text{emp}} \\le B$。\n\n测试套件：\n- 案例 A (理想情况，对角矩阵): $Q = \\operatorname{diag}(1, 3, 10)$, $x_0 = [1, -2, 3]^{\\top}$, $m = 30$。\n- 案例 B (非对角对称正定矩阵): $Q = \\begin{bmatrix} 4  1  0 \\\\ 1  3  0.5 \\\\ 0  0.5  2 \\end{bmatrix}$, $x_0 = [2, 1, -1]^{\\top}$, $m = 30$。\n- 案例 C (类边界情况，缩放单位矩阵): $Q = 5 I_3$, $x_0 = [1, 2, 3]^{\\top}$, $m = 30$。\n- 案例 D (病态对角矩阵): $Q = \\operatorname{diag}(0.01, 1, 100)$, $x_0 = [1, 1, 1]^{\\top}$, $m = 50$。\n\n输出要求：\n- 对于每个测试案例，分别运行 BB1 和 BB2，从相同的 $x_0$ 开始并使用相同的 $\\alpha_0$ 公式。对于每次运行，生成：\n  - 项目 2) 中包含性质的布尔值，\n  - 项目 3) 中的经验收缩比 $r_{\\text{emp}}$（浮点数），\n  - 项目 4) 中的谱界 $B$（浮点数）。\n- 按以下顺序汇总每个测试案例的结果：$[\\text{BB1\\_inclusion}, \\text{BB2\\_inclusion}, r_{\\text{emp}}^{\\text{BB1}}, B^{\\text{BB1}}, r_{\\text{emp}}^{\\text{BB2}}, B^{\\text{BB2}}]$。\n- 您的程序应生成单行输出，其中包含所有测试案例结果的列表，格式为逗号分隔并用方括号括起，其中每个测试案例的结果本身也是一个列表，例如：$[[\\dots],[\\dots],[\\dots],[\\dots]]$。\n- 不涉及物理单位。所有数值应按规定报告为浮点数或布尔值。",
            "solution": "所提出的问题是数值优化领域一个有效且适定的练习。它要求实现并分析使用 Barzilai–Borwein (BB) 步长的最速下降法，以最小化一个严格凸二次函数。所有给出的定义、初始条件和理论论断在优化和线性代数领域都是标准的且事实正确。测试案例涉及对称正定矩阵，确保函数有唯一的最小值，且理论框架适用。该问题是自包含的，并为得出唯一且有意义的解提供了所有必要信息。\n\n问题的核心是函数 $f(x) = \\frac{1}{2} x^{\\top} Q x$ 的无约束最小化，其中 $Q \\in \\mathbb{R}^{n \\times n}$ 是一个对称正定 (SPD) 矩阵。$f(x)$ 的严格凸性保证了在 $x^{\\star} = 0$ 处存在唯一的全局最小值。该函数的梯度由 $\\nabla f(x) = Qx$ 给出。\n\n主要算法是最速下降法，它通过以下更新规则生成一系列迭代点 $\\{x_k\\}$：\n$$x_{k+1} = x_k - \\alpha_k \\nabla f(x_k)$$\n其中 $\\nabla f(x_k)$ 是在 $x_k$ 处的最速下降方向，$\\alpha_k  0$ 是步长。该方法的性能关键取决于步长序列 $\\{\\alpha_k\\}$ 的选择。\n\n问题指定了一种混合策略来选择 $\\alpha_k$：\n1.  **初始步长 ($\\alpha_0$)**: 第一个步长 $\\alpha_0$ 是通过精确线搜索确定的，它沿着初始下降方向 $-\\nabla f(x_0)$ 最小化 $f(x)$。公式为：\n    $$\\alpha_0 = \\frac{\\nabla f(x_0)^{\\top} \\nabla f(x_0)}{\\nabla f(x_0)^{\\top} Q \\nabla f(x_0)}$$\n    令 $g_k = \\nabla f(x_k)$，则 $\\alpha_0 = \\frac{g_0^{\\top} g_0}{g_0^{\\top} Q g_0}$。\n\n2.  **后续步长 ($\\alpha_k$ for $k \\ge 1$)**: 对于后续迭代，使用两种 Barzilai–Borwein 步长之一。这些方法使用最近两次迭代的曲率信息。我们定义位移向量 $s_{k-1} = x_k - x_{k-1}$ 和梯度差分向量 $y_{k-1} = g_k - g_{k-1}$。对于二次目标函数，我们有精确关系 $y_{k-1} = Q s_{k-1}$。两种 BB 步长是：\n    -   **BB1**: 这种步长源于对海森矩阵 $Q$ 的割线近似。\n        $$\\alpha_k = \\frac{s_{k-1}^{\\top} s_{k-1}}{s_{k-1}^{\\top} y_{k-1}} = \\frac{s_{k-1}^{\\top} s_{k-1}}{s_{k-1}^{\\top} Q s_{k-1}}$$\n    -   **BB2**: 这种步长源于对逆海森矩阵 $Q^{-1}$ 的割线近似。\n        $$\\alpha_k = \\frac{s_{k-1}^{\\top} y_{k-1}}{y_{k-1}^{\\top} y_{k-1}} = \\frac{s_{k-1}^{\\top} Q s_{k-1}}{s_{k-1}^{\\top} Q^2 s_{k-1}}$$\n\n这些步长的一个重要理论性质是它们与海森矩阵 $Q$ 的谱相关。设 $Q$ 的特征值为 $0  \\lambda_{\\min} \\le \\lambda_2 \\le \\dots \\le \\lambda_{\\max}$。\n- BB1 步长和初始步长 $\\alpha_0$ 可以用瑞利商 $R_Q(z) = \\frac{z^{\\top} Q z}{z^{\\top} z}$ 来表示。具体来说，$\\alpha_k = 1 / R_Q(s_{k-1})$ 且 $\\alpha_0 = 1 / R_Q(g_0)$。由于对称正定矩阵的瑞利商受其最小和最大特征值限制（对于任何非零 $z$，有 $\\lambda_{\\min} \\le R_Q(z) \\le \\lambda_{\\max}$），因此这些步长必须位于区间 $\\left[\\frac{1}{\\lambda_{\\max}}, \\frac{1}{\\lambda_{\\min}}\\right]$ 内。\n- 对于 BB2 步长，通过对 $s_{k-1}$ 相对于 $Q$ 的特征向量进行谱分解，可以证明 $1/\\alpha_k$ 是 $Q$ 的特征值的加权平均值。因此，$\\lambda_{\\min} \\le 1/\\alpha_k \\le \\lambda_{\\max}$，这也再次意味着 $\\alpha_k \\in \\left[\\frac{1}{\\lambda_{\\max}}, \\frac{1}{\\lambda_{\\min}}\\right]$。\n任务 2 要求在 $10^{-12}$ 的数值容差内验证所有计算出的步长都满足这一包含性质。\n\n该方法的收敛性通过检查梯度范数 $\\|g_k\\|_2$ 来分析。梯度向量遵循线性递推关系：\n$$g_{k+1} = Qx_{k+1} = Q(x_k - \\alpha_k g_k) = g_k - \\alpha_k Q g_k = (I - \\alpha_k Q) g_k$$\n将此递推关系展开 $K$ 步可得：\n$$g_K = \\left( \\prod_{k=0}^{K-1} (I - \\alpha_k Q) \\right) g_0 = P_K(Q) g_0$$\n其中 $P_K(t) = \\prod_{k=0}^{K-1} (1 - \\alpha_k t)$ 是一个 $K$ 次多项式。最终梯度的范数受矩阵多项式 $P_K(Q)$ 的谱半径限制：\n$$\\|g_K\\|_2 \\le \\|P_K(Q)\\|_2 \\|g_0\\|_2 = \\left( \\max_{i} |P_K(\\lambda_i(Q))| \\right) \\|g_0\\|_2$$\n这导出了关系 $r_{\\text{emp}} \\le B$，其中：\n-   $r_{\\text{emp}} = \\frac{\\|g_K\\|_2}{\\|g_0\\|_2}$ 是经验收缩比（任务 3）。\n-   $B = \\max_{i} |P_K(\\lambda_i(Q))|$ 是理论谱界（任务 4），使用实际的步长序列 $\\{\\alpha_k\\}_{k=0}^{K-1}$ 和 $Q$ 的特征值计算得出。\n\n实现过程将为每个测试案例和每种 BB 方法（BB1 和 BB2）执行最速下降算法。对于每次运行，算法将从 $x_0$ 开始，计算 $\\alpha_0$，然后迭代，使用指定的 BB 规则计算 $\\alpha_k$（$k \\ge 1$）。循环将在达到最大迭代次数 $m$ 或梯度范数低于 $10^{-14}$ 时终止。终止后，收集到的步长序列 $\\{\\alpha_k\\}$ 以及初始和最终梯度范数将用于计算所需的输出：包含性质布尔值、经验收缩比 $r_{\\text{emp}}$ 和谱界 $B$。然后将这些结果汇总成指定的输出格式。",
            "answer": "```python\nimport numpy as np\n\ndef run_bb(Q, x0, m, bb_type):\n    \"\"\"\n    Runs the steepest descent method with Barzilai-Borwein steps.\n\n    Args:\n        Q (np.ndarray): The symmetric positive definite matrix.\n        x0 (np.ndarray): The starting vector.\n        m (int): The maximum number of iterations.\n        bb_type (str): The type of BB step to use ('BB1' or 'BB2').\n\n    Returns:\n        tuple: A tuple containing:\n            - bool: True if all steps satisfy the inclusion property.\n            - float: The empirical contraction ratio r_emp.\n            - float: The spectral bound B.\n    \"\"\"\n    g = Q @ x0\n    g0_norm = np.linalg.norm(g)\n    \n    if g0_norm  1e-14:\n        return True, 1.0, 1.0\n\n    x = x0.copy()\n    alphas = []\n    \n    # --- Step k=0: Exact line search step ---\n    alpha0_num = g @ g\n    alpha0_den = g @ Q @ g\n    if alpha0_den == 0: # Should not happen for SPD Q and g != 0\n        return False, np.inf, np.inf\n    alpha = alpha0_num / alpha0_den\n    alphas.append(alpha)\n    \n    x_prev = x\n    g_prev = g\n    x = x - alpha * g\n    g = Q @ x\n    \n    # --- Main Loop: BB steps for k >= 1 ---\n    for k in range(1, m):\n        if np.linalg.norm(g)  1e-14:\n            break\n            \n        s = x - x_prev\n        y = g - g_prev\n        \n        s_dot_s = s @ s\n        s_dot_y = s @ y\n        y_dot_y = y @ y\n        \n        if bb_type == 'BB1':\n            if abs(s_dot_y)  1e-15: break # Avoid division by zero\n            alpha = s_dot_s / s_dot_y\n        elif bb_type == 'BB2':\n            if abs(y_dot_y)  1e-15: break # Avoid division by zero\n            alpha = s_dot_y / y_dot_y\n        else:\n            raise ValueError(\"Invalid bb_type specified.\")\n            \n        alphas.append(alpha)\n        \n        x_prev = x\n        g_prev = g\n        x = x - alpha * g\n        g = Q @ x\n\n    K = len(alphas)\n    gK_norm = np.linalg.norm(g)\n    \n    # Task 2: Verify inclusion property\n    eigvals = np.linalg.eigvalsh(Q)\n    lambda_min = eigvals[0]\n    lambda_max = eigvals[-1]\n    \n    lower_bound = 1 / lambda_max - 1e-12\n    upper_bound = 1 / lambda_min + 1e-12\n    inclusion_holds = all(lower_bound = a = upper_bound for a in alphas)\n\n    # Task 3: Compute empirical contraction ratio\n    r_emp = gK_norm / g0_norm if g0_norm > 0 else 0.0\n\n    # Task 4: Compute spectral bound\n    poly_vals_abs = []\n    for lam in eigvals:\n        # P_K(lambda) = product of (1 - alpha_k * lambda)\n        p = np.prod([(1 - a * lam) for a in alphas])\n        poly_vals_abs.append(abs(p))\n    B = np.max(poly_vals_abs) if poly_vals_abs else 1.0\n\n    return inclusion_holds, r_emp, B\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (np.diag([1., 3., 10.]), np.array([1., -2., 3.]), 30),\n        (np.array([[4., 1., 0.], [1., 3., 0.5], [0., 0.5, 2.]]), np.array([2., 1., -1.]), 30),\n        (5 * np.identity(3), np.array([1., 2., 3.]), 30),\n        (np.diag([0.01, 1., 100.]), np.array([1., 1., 1.]), 50),\n    ]\n\n    all_results = []\n    for Q, x0, m in test_cases:\n        # Corrected the matrix for case B to be symmetric\n        if not np.allclose(Q, Q.T):\n           Q = np.array([[4., 1., 0.], [1., 3., 0.5], [0., 0.5, 2.]])\n           \n        incl_bb1, r_emp_bb1, B_bb1 = run_bb(Q, x0, m, 'BB1')\n        incl_bb2, r_emp_bb2, B_bb2 = run_bb(Q, x0, m, 'BB2')\n        \n        case_results = [\n            incl_bb1, incl_bb2,\n            r_emp_bb1, B_bb1,\n            r_emp_bb2, B_bb2\n        ]\n        all_results.append(case_results)\n\n    # Produce the final output string in the specified format without spaces.\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n\n```"
        },
        {
            "introduction": "优化理论不仅限于理想化的凸问题，在机器学习等前沿领域，非凸优化才是常态。本练习将带领你进入非凸优化的世界，通过分析一个含有鞍点的平滑非凸函数，你将观察到最速下降法在特定初始条件下如何被鞍点“捕获”。更重要的是，你将亲手实现并验证一个关键技巧——引入随机扰动，以帮助算法逃离鞍点，继续寻找局部最小值。",
            "id": "3149710",
            "problem": "您需要在一个拥有鞍点的光滑非凸函数背景下，构建并分析一个最速下降法的具体实例。请从基本定义开始：可微函数的梯度、海森矩阵以及欧几里得内积下的最速下降法。利用这些定义，实现一个程序，该程序在特定的光滑非凸函数上执行最速下降法，展示在某些初始条件下收敛到鞍点的情况，然后说明微小的随机扰动如何使其能够逃离鞍点。\n\n要执行的任务：\n- 定义函数 $f : \\mathbb{R}^2 \\to \\mathbb{R}$ 为\n$$\nf(x,y) = x^2 - y^2 + y^4,\n$$\n该函数是光滑且非凸的。相对于欧几里得内积的最速下降方向是负梯度 $-\\nabla f(x,y)$。构建梯度映射 $g(x,y) = \\nabla f(x,y)$。\n- 使用基本定义，通过验证 $g(0,0) = (0,0)$ 以及海森矩阵 $H(0,0)$ 是不定的，来证明 $(0,0)$ 是 $f$ 的一个鞍点。\n- 实现具有固定步长 $\\alpha$ 的最速下降法：\n$$\n\\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\end{pmatrix}\n=\n\\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix}\n-\n\\alpha \\, g(x_k,y_k),\n$$\n以及一个基于梯度范数变得很小或达到最大迭代次数的停止准则。此外，实现一个可选的扰动机制：当当前迭代点接近鞍点（在预定半径内）且梯度范数很小时，向当前点注入一个单一的微小随机扰动（从具有给定标准差的零均值正态分布中抽取），然后从扰动后的点继续执行最速下降法。\n- 证明对于在 $x$ 轴上的初始值（即 $y_0 = 0$），最速下降法收敛到鞍点 $(0,0)$；而对于不在 $x$ 轴上的初始值（即 $y_0 \\neq 0$），最速下降法收敛到一个局部最小值（$(0,1/\\sqrt{2})$ 或 $(0,-1/\\sqrt{2})$ 中的一个）。然后证明，在鞍点附近增加一个微小的随机扰动可以使其逃离并随后收敛到一个局部最小值。\n\n程序要求：\n- 在一个单一、自包含的程序中实现上述内容，该程序不接受任何输入，并使用下面指定的测试套件。\n- 对于每个测试用例，使用给定的参数运行最速下降法，并返回一个布尔值，指示算法是否收敛到鞍点 $(0,0)$（定义为最终迭代点的欧几里得范数小于指定的容差）。四个测试用例是：\n    1. 初始点 $(x_0,y_0) = (1.0,0.0)$，步长 $\\alpha = 0.3$，最大迭代次数 $200$，无扰动。这测试了沿稳定流形向鞍点收敛的情况。\n    2. 初始点 $(x_0,y_0) = (1.0,0.01)$，步长 $\\alpha = 0.3$，最大迭代次数 $400$，无扰动。这测试了从鞍点稳定流形之外开始时逃离鞍点并收敛到局部最小值的情况。\n    3. 初始点 $(x_0,y_0) = (1.0,0.0)$，步长 $\\alpha = 0.3$，最大迭代次数 $400$，在接近鞍点时允许一次微小扰动。使用梯度范数扰动阈值 $10^{-12}$，鞍点邻近半径 $10^{-3}$，扰动标准差 $10^{-3}$，以及固定的随机种子 $12345$。这测试了即使从稳定流形开始，也能通过扰动逃离鞍点的情况。\n    4. 初始点 $(x_0,y_0) = (0.0,0.0)$，步长 $\\alpha = 0.3$，最大迭代次数 $400$，允许一次微小扰动，使用与案例3相同的阈值和随机种子。这测试了从鞍点正中心开始时逃离鞍点的情况。\n\n输出规范：\n- 对于每个测试用例，输出一个布尔值，指示最终迭代点是否在 $(0,0)$ 的鞍点容差 $10^{-8}$ 范围内；也就是说，如果终止时 $\\sqrt{x^2+y^2}  10^{-8}$，则输出 $\\mathrm{True}$，否则输出 $\\mathrm{False}$。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[result_1,result_2,result_3,result_4]$）。\n- 此问题不涉及物理单位、角度单位或百分比；所有量均为无量纲实数。\n\n程序必须遵守指定的执行环境和库。",
            "solution": "问题陈述已经过验证，被认为是数值优化领域中一个提法恰当、有科学依据的问题。我们将继续提供解决方案。\n\n问题的核心是分析最速下降法在光滑非凸函数 $f: \\mathbb{R}^2 \\to \\mathbb{R}$ 上的行为，该函数定义为\n$$\nf(x,y) = x^2 - y^2 + y^4\n$$\n这个函数作为一个模型，用于理解优化算法如何在包含鞍点的地形中导航。\n\n首先，我们建立必要的分析工具。最速下降方向由负梯度给出。梯度映射，记为 $g(x,y) = \\nabla f(x,y)$，由 $f(x,y)$ 的偏导数计算得出：\n$$\n\\frac{\\partial f}{\\partial x} = 2x\n$$\n$$\n\\frac{\\partial f}{\\partial y} = -2y + 4y^3\n$$\n因此，梯度向量为：\n$$\ng(x,y) = \\nabla f(x,y) = \\begin{pmatrix} 2x \\\\ 4y^3 - 2y \\end{pmatrix}\n$$\n\n接下来，我们验证点 $(0,0)$ 是一个鞍点。如果梯度在某点为零，则该点为临界点。计算在 $(0,0)$ 处的梯度：\n$$\ng(0,0) = \\begin{pmatrix} 2(0) \\\\ 4(0)^3 - 2(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n这证实了 $(0,0)$ 是一个临界点。为了对此点进行分类，我们分析海森矩阵 $H(x,y)$，它是二阶偏导数矩阵：\n$$\nH(x,y) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2}  \\frac{\\partial^2 f}{\\partial y \\partial x} \\\\ \\frac{\\partial^2 f}{\\partial x \\partial y}  \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  12y^2 - 2 \\end{pmatrix}\n$$\n计算在临界点 $(0,0)$ 处的海森矩阵：\n$$\nH(0,0) = \\begin{pmatrix} 2  0 \\\\ 0  12(0)^2 - 2 \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  -2 \\end{pmatrix}\n$$\n$H(0,0)$ 的特征值是对角线元素 $\\lambda_1 = 2$ 和 $\\lambda_2 = -2$。由于一个特征值为正，另一个为负，因此海森矩阵在 $(0,0)$ 处是不定的。根据定义，具有不定海森矩阵的临界点是鞍点。对应于正特征值 $\\lambda_1=2$ 的特征向量是 $(1,0)$，张成了向上弯曲的方向（对于最小化问题，这是稳定子空间）。对应于 $\\lambda_2=-2$ 的特征向量是 $(0,1)$，张成了向下弯曲的方向（对于最小化问题，这是不稳定子空间）。\n\n具有固定步长 $\\alpha  0$ 的最速下降算法根据以下规则生成一系列迭代点 $(x_k, y_k)$：\n$$\n\\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\end{pmatrix} = \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix} - \\alpha \\, g(x_k,y_k)\n$$\n代入梯度的分量，我们得到具体的更新规则：\n$$\nx_{k+1} = x_k - \\alpha (2x_k) = (1 - 2\\alpha)x_k\n$$\n$$\ny_{k+1} = y_k - \\alpha (4y_k^3 - 2y_k) = (1 + 2\\alpha)y_k - 4\\alpha y_k^3\n$$\n这个动力系统的行为关键取决于初始点 $(x_0, y_0)$。\n\n如果我们在 $x$ 轴上初始化，即 $y_0 = 0$，第二个更新规则意味着 $y_1 = (1+2\\alpha)(0) - 4\\alpha(0)^3 = 0$。通过归纳法可知，如果 $y_k=0$，那么 $y_{k+1}=0$。因此，优化过程被限制在 $x$ 轴上，动力学简化为 $x_{k+1} = (1 - 2\\alpha)x_k$。为了收敛到 $x=0$，我们需要 $|1 - 2\\alpha|  1$，这对于 $0  \\alpha  1$ 成立。指定的步长 $\\alpha=0.3$ 满足此条件。因此，从 $y_0=0$ 开始，迭代点会收敛到鞍点 $(0,0)$。这对应于沿着梯度流的鞍点的稳定流形移动。\n\n如果我们用 $y_0 \\neq 0$ 初始化，动力学行为则不同。在鞍点附近，$y_k$ 非常小，三次项 $y_k^3$ 与线性项相比可以忽略不计。$y$ 的更新可以近似为 $y_{k+1} \\approx (1 + 2\\alpha)y_k$。由于 $\\alpha  0$，因子 $(1+2\\alpha)$ 大于 $1$。这意味着任何微小的、非零的 $y$ 分量都将被放大，将迭代点推离 $x$ 轴。这是沿着不稳定流形的运动。迭代点会逃离鞍点附近区域，并最终收敛到函数的某个局部最小值，位于 $(0, \\pm 1/\\sqrt{2})$。\n\n实现将包含一个执行此迭代过程给定步数的函数。扰动机制旨在证明，即使算法正处于通往鞍点的路径上（例如，恰好在稳定流形上初始化），一个微小的随机扰动也足以将其推入不稳定区域并导致逃逸。当迭代点非常接近鞍点（欧几里得范数低于半径阈值）且动力学已经减慢（梯度范数低于阈值）时，扰动会被应用一次。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases for steepest descent\n    on a nonconvex function and report convergence to the saddle point.\n    \"\"\"\n\n    def run_steepest_descent(x0, y0, alpha, max_iter, use_perturbation, pert_params=None):\n        \"\"\"\n        Executes the steepest descent algorithm for a single test case.\n\n        Args:\n            x0 (float): Initial x-coordinate.\n            y0 (float): Initial y-coordinate.\n            alpha (float): The fixed step size.\n            max_iter (int): The maximum number of iterations.\n            use_perturbation (bool): Flag to enable/disable the perturbation mechanism.\n            pert_params (dict, optional): Parameters for the perturbation.\n\n        Returns:\n            bool: True if the final iterate is within the saddle tolerance, False otherwise.\n        \"\"\"\n\n        def grad_f(p):\n            \"\"\"Computes the gradient of f(x,y) = x^2 - y^2 + y^4.\"\"\"\n            x, y = p\n            gx = 2.0 * x\n            gy = 4.0 * y**3 - 2.0 * y\n            return np.array([gx, gy])\n\n        rng = None\n        if use_perturbation and pert_params:\n            rng = np.random.default_rng(pert_params['seed'])\n\n        p = np.array([float(x0), float(y0)])\n        perturbation_applied = False\n\n        for _ in range(max_iter):\n            # Check for and apply perturbation if conditions are met\n            if use_perturbation and not perturbation_applied and pert_params:\n                grad_norm = np.linalg.norm(grad_f(p))\n                saddle_dist = np.linalg.norm(p)\n\n                if (grad_norm  pert_params['grad_norm_thresh'] and\n                        saddle_dist  pert_params['saddle_radius']):\n                    \n                    perturbation = rng.normal(loc=0.0, scale=pert_params['std_dev'], size=2)\n                    p += perturbation\n                    perturbation_applied = True\n            \n            # Calculate gradient and perform update step\n            grad = grad_f(p)\n            p -= alpha * grad\n\n        final_norm = np.linalg.norm(p)\n        saddle_tolerance = 1e-8\n        \n        return final_norm  saddle_tolerance\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Start on stable manifold, no perturbation. Expect convergence to saddle.\n        {'x0': 1.0, 'y0': 0.0, 'alpha': 0.3, 'max_iter': 200, 'use_perturbation': False},\n        \n        # Case 2: Start near stable manifold, no perturbation. Expect escape.\n        {'x0': 1.0, 'y0': 0.01, 'alpha': 0.3, 'max_iter': 400, 'use_perturbation': False},\n\n        # Case 3: Start on stable manifold, with perturbation. Expect escape.\n        {'x0': 1.0, 'y0': 0.0, 'alpha': 0.3, 'max_iter': 400, 'use_perturbation': True,\n         'pert_params': {'grad_norm_thresh': 1e-12, 'saddle_radius': 1e-3, 'std_dev': 1e-3, 'seed': 12345}},\n        \n        # Case 4: Start at the saddle, with perturbation. Expect escape.\n        {'x0': 0.0, 'y0': 0.0, 'alpha': 0.3, 'max_iter': 400, 'use_perturbation': True,\n         'pert_params': {'grad_norm_thresh': 1e-12, 'saddle_radius': 1e-3, 'std_dev': 1e-3, 'seed': 12345}}\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_steepest_descent(**params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # str(True) -> 'True', str(False) -> 'False'\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}