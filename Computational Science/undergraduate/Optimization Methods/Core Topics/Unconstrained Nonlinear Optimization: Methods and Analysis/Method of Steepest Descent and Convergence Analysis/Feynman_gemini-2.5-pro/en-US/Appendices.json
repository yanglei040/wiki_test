{
    "hands_on_practices": [
        {
            "introduction": "The method of steepest descent is appealing for its simplicity, but its practical performance can be notoriously slow. This exercise provides a foundational understanding of *why* this happens by dissecting the algorithm's behavior in a worst-case scenario. By constructing a specific 2D quadratic problem, you will analytically derive the one-step convergence rate and discover the conditions that lead to the infamous \"zig-zag\" pattern, where the algorithm makes very slow progress towards the minimum . This hands-on theoretical derivation is key to grasping the intimate connection between an optimization problem's condition number, $\\kappa$, and the algorithm's efficiency.",
            "id": "3149658",
            "problem": "Consider a twice continuously differentiable objective in two variables defined by the quadratic model $$f(x) = \\frac{1}{2} x^{\\top} Q x,$$ where $Q \\in \\mathbb{R}^{2 \\times 2}$ is symmetric positive definite (SPD). Let the eigen-decomposition of $Q$ be given by $Q = V \\Lambda V^{\\top}$ with orthonormal eigenvectors $v_{1}$ and $v_{2}$ and eigenvalues $0 < m < L$, and define the condition number $$\\kappa = \\frac{L}{m}.$$ The unique minimizer is $x^{\\star} = 0$. The method of steepest descent with exact line search generates iterates $$x_{k+1} = x_{k} - \\alpha_{k} \\nabla f(x_{k}),$$ where $\\alpha_{k}$ minimizes $f(x_{k} - \\alpha \\nabla f(x_{k}))$ over real $\\alpha$. \n\nYour tasks are:\n- Using only fundamental definitions (gradient, exact line search, and properties of SPD matrices and their eigen-decomposition), construct a worst-case initial iterate $x_{0}$ by choosing coefficients $a$ and $b$ such that $$x_{0} = a v_{1} + b v_{2}$$ leads to the slowest single-step decrease of the energy $f(x)$ under steepest descent with exact line search. Interpret this construction geometrically to explain why, for large $\\kappa$, the negative gradient direction $-\\nabla f(x_{0})$ becomes nearly orthogonal to the direction toward $x^{\\star}$, causing a zig-zag and stagnation pattern.\n- Derive, from first principles, the exact worst-case one-step reduction factor of the squared error measured in the $Q$-norm, $$\\frac{\\|x_{1}\\|_{Q}^{2}}{\\|x_{0}\\|_{Q}^{2}}, \\quad \\text{where } \\|x\\|_{Q}^{2} = x^{\\top} Q x,$$ expressed solely as a function of $\\kappa$.\n\nProvide the final answer as a single closed-form analytic expression in terms of $\\kappa$. No numerical rounding is required.",
            "solution": "The problem asks for the construction of a worst-case initial iterate for the method of steepest descent on a $2$-dimensional quadratic objective and the derivation of the corresponding one-step error reduction factor.\n\nLet the objective function be $f(x) = \\frac{1}{2} x^{\\top} Q x$, where $x \\in \\mathbb{R}^{2}$ and $Q$ is a symmetric positive definite (SPD) matrix. The gradient is $\\nabla f(x) = Qx$. The unique minimizer is $x^{\\star} = 0$. The matrix $Q$ has orthonormal eigenvectors $v_1$ and $v_2$ with corresponding eigenvalues $0 < m < L$. We assume $Qv_1 = mv_1$ and $Qv_2 = Lv_2$. Any vector $x \\in \\mathbb{R}^2$ can be expressed in this eigenbasis as $x = c_1 v_1 + c_2 v_2$.\n\nThe steepest descent iteration is given by $x_{k+1} = x_k - \\alpha_k \\nabla f(x_k)$, where $\\nabla f(x_k) = g_k = Qx_k$. The step size $\\alpha_k$ is chosen by an exact line search to minimize $\\phi(\\alpha) = f(x_k - \\alpha g_k)$.\n$$ \\phi(\\alpha) = \\frac{1}{2} (x_k - \\alpha g_k)^{\\top} Q (x_k - \\alpha g_k) = \\frac{1}{2} (x_k^{\\top}Qx_k - 2\\alpha g_k^{\\top}Qx_k + \\alpha^2 g_k^{\\top}Qg_k) $$\nTo find the minimum, we set the derivative with respect to $\\alpha$ to zero:\n$$ \\frac{d\\phi}{d\\alpha} = -g_k^{\\top}Qx_k + \\alpha g_k^{\\top}Qg_k = 0 $$\nSince $g_k = Qx_k$, this gives the optimal step size:\n$$ \\alpha_k = \\frac{g_k^{\\top}Qx_k}{g_k^{\\top}Qg_k} = \\frac{g_k^{\\top}g_k}{g_k^{\\top}Qg_k} $$\nThe error is measured in the $Q$-norm, defined as $\\|x\\|_Q^2 = x^{\\top}Qx = 2f(x)$. The one-step reduction in this error metric is:\n$$ \\|x_{k+1}\\|_Q^2 = (x_k - \\alpha_k g_k)^{\\top} Q (x_k - \\alpha_k g_k) = \\|x_k\\|_Q^2 - 2\\alpha_k g_k^{\\top}Qx_k + \\alpha_k^2 g_k^{\\top}Qg_k $$\nSubstituting $\\alpha_k = \\frac{g_k^{\\top}g_k}{g_k^{\\top}Qg_k}$ and $g_k=Qx_k$:\n$$ \\|x_{k+1}\\|_Q^2 = \\|x_k\\|_Q^2 - 2\\frac{(g_k^{\\top}g_k)}{(g_k^{\\top}Qg_k)}g_k^{\\top}g_k + \\left(\\frac{g_k^{\\top}g_k}{g_k^{\\top}Qg_k}\\right)^2 g_k^{\\top}Qg_k = \\|x_k\\|_Q^2 - \\frac{(g_k^{\\top}g_k)^2}{g_k^{\\top}Qg_k} $$\nThe reduction factor is therefore:\n$$ \\frac{\\|x_{k+1}\\|_Q^2}{\\|x_k\\|_Q^2} = 1 - \\frac{(g_k^{\\top}g_k)^2}{\\|x_k\\|_Q^2 (g_k^{\\top}Qg_k)} = 1 - \\frac{(x_k^{\\top}Q^2x_k)^2}{(x_k^{\\top}Qx_k)(x_k^{\\top}Q^3x_k)} $$\nTo find the worst-case one-step reduction, we must maximize this ratio, which is equivalent to minimizing the subtracted term. Let the initial iterate be $x_0 = a v_1 + b v_2$, where $a, b$ are real coefficients not both zero.\nUsing the eigen-decomposition, we can express the terms as:\n$x_0^{\\top}Q^p x_0 = (av_1+bv_2)^{\\top}Q^p(av_1+bv_2) = (av_1+bv_2)^{\\top}(a m^p v_1 + b L^p v_2) = a^2 m^p + b^2 L^p$.\nThe term to be minimized is:\n$$ H(a,b) = \\frac{(a^2 m^2 + b^2 L^2)^2}{(a^2 m + b^2 L)(a^2 m^3 + b^2 L^3)} $$\nSince this expression is homogeneous in $a^2$ and $b^2$, its value depends only on the ratio $t = b^2/a^2$ (for $a \\neq 0$). Let's analyze the function:\n$$ H(t) = \\frac{(m^2 + t L^2)^2}{(m + t L)(m^3 + t L^3)} $$\nTo find the minimum, we set its derivative with respect to $t$ to zero. It is easier to use the logarithmic derivative: $\\frac{d}{dt} \\ln H(t) = 0$.\n$$ \\frac{2L^2}{m^2+tL^2} - \\frac{L}{m+tL} - \\frac{L^3}{m^3+tL^3} = 0 $$\nMultiplying by the common denominator and focusing on the numerator gives:\n$$ 2L^2(m+tL)(m^3+tL^3) - L(m^2+tL^2)(m^3+tL^3) - L^3(m^2+tL^2)(m+tL) = 0 $$\nExpanding and collecting terms based on powers of $t$:\nThe coefficient of $t^2$ is $2L^2(L \\cdot L^3) - L(L^2 \\cdot L^3) - L^3(L^2 \\cdot L) = 2L^6 - L^6 - L^6 = 0$.\nThe equation is linear in $t$. The constant term (t=0) is $2L^2(m \\cdot m^3) - L(m^2 \\cdot m^3) - L^3(m^2 \\cdot m) = 2L^2m^4 - Lm^5 - L^3m^3 = Lm^3(2Lm - m^2 - L^2) = -Lm^3(L-m)^2$.\nThe coefficient of $t$ is $2L^2(m L^3 + L m^3) - L(m^2 L^3 + L^2 m^3) - L^3(m^2 L + L^2 m) = (2L^5m + 2L^3m^3) - (L^4m^2 + L^3m^3) - (L^5m + L^4m^2) = L^5m + L^3m^3 - 2L^4m^2 = L^3m(L^2+m^2-2Lm)=L^3m(L-m)^2$.\nThe equation becomes: $-Lm^3(L-m)^2 + t L^3m(L-m)^2 = 0$.\nAssuming $L \\ne m$, we can divide by $Lm(L-m)^2$, which gives $-m^2 + tL^2 = 0$, so $t = m^2/L^2$.\nThus, the slowest single-step decrease occurs when $b^2/a^2 = m^2/L^2$, or $|b/a| = m/L$.\nThe worst-case initial iterate $x_0$ is any vector of the form $x_0 = c(v_1 \\pm \\frac{m}{L} v_2)$ or $x_0=c(\\frac{L}{m}v_1 \\pm v_2)$ for any non-zero scalar $c$. For simplicity, we can choose $x_0 = L v_1 + m v_2$.\n\nFor the geometric interpretation, consider a large condition number $\\kappa = L/m \\gg 1$. The level sets of $f(x)$ are ellipses defined by $m c_1^2 + L c_2^2 = \\text{const}$, which are highly elongated along the $v_1$ axis (the \"slow\" direction). The worst-case starting point $x_0 = L v_1 + m v_2$ has a large component along the slow direction $v_1$ and a small component along the fast direction $v_2$. The direction toward the minimum $x^\\star=0$ is $-x_0 = -L v_1 - m v_2$. For large $\\kappa$, this direction is almost parallel to $-v_1$.\nThe gradient is $g_0 = Qx_0 = Q(Lv_1+mv_2) = Lmv_1 + Lmv_2 = Lm(v_1+v_2)$.\nThe steepest descent direction is $-g_0 = -Lm(v_1+v_2)$. This direction is angled at $45^\\circ$ to both $-v_1$ and $-v_2$.\nFor large $\\kappa$, the direction to the minimum ($-x_0$) is almost $-v_1$, while the search direction ($-g_0$) is $- (v_1+v_2)$. The angle between these directions is $\\arccos\\left(\\frac{L+m}{\\sqrt{2(L^2+m^2)}}\\right)$, which approaches $45^\\circ$ as $\\kappa \\to \\infty$. This significant mismatch between the search direction and the optimal path to the minimum causes the algorithm to take a step that overshoots the \"valley floor\" (the $v_1$-axis). The next iterate $x_1$ lies on the other side of the valley, in a symmetric configuration. This leads to the classic zig-zagging behavior, where progress towards the minimum is very slow.\n\nTo derive the worst-case reduction factor, we substitute $t=m^2/L^2$ into the expression for the ratio $\\frac{\\|x_{1}\\|_Q^2}{\\|x_{0}\\|_Q^2}$:\n$$ \\frac{\\|x_{1}\\|_Q^2}{\\|x_{0}\\|_Q^2} = 1 - H(m^2/L^2) = 1 - \\frac{(m^2 + (m^2/L^2) L^2)^2}{(m + (m^2/L^2) L)(m^3 + (m^2/L^2) L^3)} $$\n$$ = 1 - \\frac{(m^2 + m^2)^2}{(m + m^2/L)(m^3 + m^2L)} = 1 - \\frac{(2m^2)^2}{m(1+m/L)m^2(m+L)} $$\n$$ = 1 - \\frac{4m^4}{m^3 \\frac{L+m}{L} (m+L)} = 1 - \\frac{4mL}{(L+m)^2} $$\n$$ = \\frac{(L+m)^2 - 4mL}{(L+m)^2} = \\frac{L^2+2mL+m^2-4mL}{(L+m)^2} = \\frac{L^2-2mL+m^2}{(L+m)^2} = \\left(\\frac{L-m}{L+m}\\right)^2 $$\nFinally, expressing this in terms of the condition number $\\kappa = L/m$:\n$$ \\left(\\frac{L/m - 1}{L/m + 1}\\right)^2 = \\left(\\frac{\\kappa-1}{\\kappa+1}\\right)^2 $$\nThis is the exact worst-case one-step reduction factor for the squared $Q$-norm.",
            "answer": "$$\n\\boxed{\\left(\\frac{\\kappa-1}{\\kappa+1}\\right)^{2}}\n$$"
        },
        {
            "introduction": "Having explored the theoretical limits of steepest descent, this practice shifts the focus to empirical validation through code. Theory predicts that for a quadratic function, the number of iterations required to achieve a certain accuracy grows with the logarithm of the desired precision and is heavily influenced by the condition number. This exercise challenges you to implement the steepest descent algorithm and run it on a series of test cases where you have precise control over the problem's eigenstructure, and thus its condition number . By measuring the iteration counts, you will directly observe and confirm the algorithm's convergence properties predicted by the theory.",
            "id": "3149696",
            "problem": "You are to implement the method of steepest descent with exact line search on a strictly convex quadratic objective and empirically examine how many iterations are needed to reduce the optimization error by a prescribed factor. The study should validate that the required number of iterations scales logarithmically with the requested error reduction factor. Work with a quadratic objective that has a known eigenstructure so that its condition number is precisely controlled.\n\nUse the following fundamental base:\n- A strictly convex quadratic objective function $f(\\mathbf{x})$ has the form $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} \\mathbf{A} \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}$, where $\\mathbf{A}$ is real, symmetric, and positive definite. Symmetric Positive Definite (SPD) means $\\mathbf{A} = \\mathbf{A}^{\\top}$ and $\\mathbf{z}^{\\top}\\mathbf{A}\\mathbf{z} > 0$ for all nonzero $\\mathbf{z}$.\n- The unique minimizer $\\mathbf{x}^{\\star}$ satisfies $\\mathbf{A}\\mathbf{x}^{\\star} = \\mathbf{b}$.\n- The method of steepest descent generates iterates $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k)$, where $\\alpha_k$ is chosen by exact one-dimensional minimization along the negative gradient direction at $\\mathbf{x}_k$.\n\nYour task:\n- For each test case below, build the SPD matrix $\\mathbf{A}$ with the specified eigenvalues by taking $\\mathbf{A}$ diagonal with those eigenvalues, choose the minimizer as $\\mathbf{x}^{\\star}$ with components $x^{\\star}_i = i$ for $i = 1, \\dots, n$, set $\\mathbf{b} = \\mathbf{A}\\mathbf{x}^{\\star}$, and initialize $\\mathbf{x}_0 = \\mathbf{0}$.\n- Implement steepest descent with exact line search from $\\mathbf{x}_0$. Let $f^{\\star} = f(\\mathbf{x}^{\\star})$ and define the initial error $E_0 = f(\\mathbf{x}_0) - f^{\\star}$.\n- For a given target exponent $p$, run the iterations until the first index $k$ such that $f(\\mathbf{x}_k) - f^{\\star} \\leq E_0 \\cdot 10^{-p}$. Record this $k$ as the required number of iterations for that test case. All angles, if any, are to be interpreted in radians, but no angles are used in this problem.\n- The goal is to observe how this $k$ depends on $p$ and the spectrum of $\\mathbf{A}$, thereby validating that the required iterations grow proportionally to $\\log(10^p) = p \\log(10)$ for fixed $\\mathbf{A}$.\n\nTest suite:\n- Case $1$: dimension $n = 6$; eigenvalues: three entries equal to $1$ and three entries equal to $9$ (condition number $\\kappa = 9$); target exponent $p = 1$.\n- Case $2$: dimension $n = 6$; eigenvalues: three entries equal to $1$ and three entries equal to $9$ (condition number $\\kappa = 9$); target exponent $p = 3$.\n- Case $3$: dimension $n = 5$; eigenvalues: all entries equal to $7$ (condition number $\\kappa = 1$); target exponent $p = 5$.\n- Case $4$: dimension $n = 8$; eigenvalues: four entries equal to $1$ and four entries equal to $100$ (condition number $\\kappa = 100$); target exponent $p = 2$.\n- Case $5$: dimension $n = 10$; eigenvalues: five entries equal to $1$ and five entries equal to $1000$ (condition number $\\kappa = 1000$); target exponent $p = 3$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the required iteration counts (integers) for the five test cases in the order specified above. For example, the output should look like $[k_1,k_2,k_3,k_4,k_5]$, where each $k_i$ is the computed integer number of iterations for case $i$.",
            "solution": "The problem statement has been analyzed and is deemed valid. It is a well-posed, scientifically grounded, and objective problem from the field of numerical optimization. All necessary data and definitions for a unique and meaningful solution are provided.\n\nThe core task is to implement the method of steepest descent with exact line search for a specific class of quadratic objective functions and to determine the number of iterations required to achieve a prescribed reduction in the optimization error.\n\nThe objective function is a strictly convex quadratic of the form:\n$$\nf(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^{\\top} \\mathbf{A} \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}\n$$\nwhere $\\mathbf{A}$ is a real, symmetric positive-definite (SPD) matrix of size $n \\times n$, and $\\mathbf{x}, \\mathbf{b} \\in \\mathbb{R}^n$.\n\nThe gradient of this function is given by:\n$$\n\\nabla f(\\mathbf{x}) = \\mathbf{A} \\mathbf{x} - \\mathbf{b}\n$$\nThe function $f(\\mathbf{x})$ has a unique global minimizer, denoted by $\\mathbf{x}^{\\star}$, which is the solution to the linear system $\\nabla f(\\mathbf{x}^{\\star}) = \\mathbf{0}$, or:\n$$\n\\mathbf{A}\\mathbf{x}^{\\star} = \\mathbf{b}\n$$\nThe minimum value of the function is $f^{\\star} = f(\\mathbf{x}^{\\star})$. Substituting $\\mathbf{b} = \\mathbf{A}\\mathbf{x}^{\\star}$ into the expression for $f^{\\star}$ yields:\n$$\nf^{\\star} = \\frac{1}{2} (\\mathbf{x}^{\\star})^{\\top} \\mathbf{A} \\mathbf{x}^{\\star} - (\\mathbf{A}\\mathbf{x}^{\\star})^{\\top} \\mathbf{x}^{\\star} = \\frac{1}{2} (\\mathbf{x}^{\\star})^{\\top} \\mathbf{A} \\mathbf{x}^{\\star} - (\\mathbf{x}^{\\star})^{\\top} \\mathbf{A}^{\\top} \\mathbf{x}^{\\star}\n$$\nSince $\\mathbf{A}$ is symmetric ($\\mathbf{A} = \\mathbf{A}^{\\top}$), this simplifies to:\n$$\nf^{\\star} = -\\frac{1}{2} (\\mathbf{x}^{\\star})^{\\top} \\mathbf{A} \\mathbf{x}^{\\star} = -\\frac{1}{2} (\\mathbf{x}^{\\star})^{\\top} \\mathbf{b}\n$$\nThe method of steepest descent is an iterative algorithm that generates a sequence of points $\\{\\mathbf{x}_k\\}$ starting from an initial guess $\\mathbf{x}_0$. The update rule is:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k\n$$\nwhere $\\mathbf{p}_k$ is the direction of steepest descent, defined as the negative of the gradient at $\\mathbf{x}_k$:\n$$\n\\mathbf{p}_k = - \\nabla f(\\mathbf{x}_k) = -(\\mathbf{A}\\mathbf{x}_k - \\mathbf{b})\n$$\nLet's denote the gradient at step $k$ as $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$. The update rule becomes:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k \\mathbf{g}_k\n$$\nThe step size $\\alpha_k$ is chosen to minimize the function $f$ along the ray starting from $\\mathbf{x}_k$ in the direction $\\mathbf{p}_k$. This is known as exact line search. We seek to find $\\alpha_k > 0$ that minimizes the one-dimensional function $\\phi(\\alpha) = f(\\mathbf{x}_k - \\alpha \\mathbf{g}_k)$. The minimum is found by setting the derivative with respect to $\\alpha$ to zero:\n$$\n\\frac{d\\phi}{d\\alpha} = \\nabla f(\\mathbf{x}_k - \\alpha \\mathbf{g}_k)^{\\top} \\cdot (-\\mathbf{g}_k) = 0\n$$\nSubstituting the expression for the gradient:\n$$\n(\\mathbf{A}(\\mathbf{x}_k - \\alpha \\mathbf{g}_k) - \\mathbf{b})^{\\top} (-\\mathbf{g}_k) = 0\n$$\n$$\n((\\mathbf{A}\\mathbf{x}_k - \\mathbf{b}) - \\alpha \\mathbf{A} \\mathbf{g}_k)^{\\top} \\mathbf{g}_k = 0\n$$\nRecognizing that $\\mathbf{g}_k = \\mathbf{A}\\mathbf{x}_k - \\mathbf{b}$:\n$$\n(\\mathbf{g}_k - \\alpha \\mathbf{A} \\mathbf{g}_k)^{\\top} \\mathbf{g}_k = 0\n$$\n$$\n\\mathbf{g}_k^{\\top}\\mathbf{g}_k - \\alpha \\mathbf{g}_k^{\\top} \\mathbf{A} \\mathbf{g}_k = 0\n$$\nSolving for $\\alpha$, we obtain the optimal step size for the quadratic case:\n$$\n\\alpha_k = \\frac{\\mathbf{g}_k^{\\top}\\mathbf{g}_k}{\\mathbf{g}_k^{\\top}\\mathbf{A}\\mathbf{g}_k}\n$$\nThe implementation will proceed as follows for each test case.\n$1$. The matrix $\\mathbf{A}$ is constructed as a diagonal matrix with the specified eigenvalues.\n$2$. The minimizer $\\mathbf{x}^{\\star}$ is defined with components $x^{\\star}_i = i$ for $i \\in \\{1, \\dots, n\\}$.\n$3$. The vector $\\mathbf{b}$ is computed from $\\mathbf{b} = \\mathbf{A}\\mathbf{x}^{\\star}$.\n$4$. The initial point is $\\mathbf{x}_0 = \\mathbf{0}$.\n$5$. The minimum function value $f^{\\star}$ is calculated as $f^{\\star} = -0.5 \\cdot (\\mathbf{x}^{\\star})^{\\top}\\mathbf{b}$.\n$6$. The initial error is $E_0 = f(\\mathbf{x}_0) - f^{\\star}$. Since $\\mathbf{x}_0 = \\mathbf{0}$, $f(\\mathbf{x}_0) = 0$, so $E_0 = -f^{\\star}$.\n$7$. The target error value is $E_{target} = E_0 \\cdot 10^{-p}$, where $p$ is the given target exponent.\n$8$. The algorithm iterates, starting with $k=0$ and $\\mathbf{x}_k = \\mathbf{x}_0$. In each iteration, a new point $\\mathbf{x}_{k+1}$ is computed. The loop terminates when the first index $k$ is found such that $f(\\mathbf{x}_k) - f^{\\star} \\leq E_{target}$. This value of $k$ is the result for the test case.\n\nA special case arises when the condition number $\\kappa(\\mathbf{A}) = \\lambda_{\\max}/\\lambda_{\\min} = 1$. This occurs when all eigenvalues are equal, i.e., $\\mathbf{A} = c\\mathbf{I}$ for some constant $c > 0$. In this scenario, the steepest descent method is guaranteed to converge in a single iteration. This is because the level sets of $f(\\mathbf{x})$ are hyperspheres, and the gradient at any point $\\mathbf{x}_k$ points directly towards the center (the minimizer $\\mathbf{x}^{\\star}$). This will be observed in Case $3$.",
            "answer": "```python\nimport numpy as np\n\ndef run_steepest_descent(dimension, eigenvalues, p_exponent):\n    \"\"\"\n    Implements the method of steepest descent to find the number of iterations\n    required to reduce the optimization error by a factor of 10**(-p).\n\n    Args:\n        dimension (int): The dimension of the vector space, n.\n        eigenvalues (list): A list of eigenvalues for the matrix A.\n        p_exponent (int): The target exponent for error reduction.\n\n    Returns:\n        int: The number of iterations k.\n    \"\"\"\n    n = dimension\n    p = p_exponent\n\n    # 1. Build the SPD matrix A\n    A = np.diag(eigenvalues)\n\n    # 2. Choose the minimizer x_star and compute b\n    x_star = np.arange(1, n + 1)\n    b = A @ x_star\n\n    # 3. Compute the minimum function value f_star\n    f_star = -0.5 * x_star.T @ b\n\n    # 4. Initialize steepest descent\n    x_k = np.zeros(n)\n    k = 0\n\n    # 5. Calculate initial error and target error\n    f_at_x0 = 0.5 * x_k.T @ A @ x_k - b.T @ x_k  # This will be 0\n    E_0 = f_at_x0 - f_star\n    \n    # Handle the trivial case where initial error is zero or negative.\n    if E_0 <= 0:\n        return 0\n        \n    target_error_value = E_0 * (10**(-p))\n    \n    # 6. Check if initial point already meets the criterion\n    # This check is implicitly handled by the loop structure.\n    # The loop will not execute if the condition is already met, returning k=0.\n    \n    # 7. Iteration loop\n    while True:\n        # Check current error before performing an iteration step\n        f_at_xk = 0.5 * x_k.T @ A @ x_k - b.T @ x_k\n        current_error = f_at_xk - f_star\n        if current_error <= target_error_value:\n            break\n\n        # Calculate gradient\n        g_k = A @ x_k - b\n\n        # If gradient is zero, we are at the minimum.\n        if np.allclose(g_k, 0):\n            break\n\n        # Calculate optimal step size alpha_k\n        gkTg = g_k.T @ g_k\n        gkT_A_gk = g_k.T @ A @ g_k\n        alpha_k = gkTg / gkT_A_gk\n\n        # Update x_k\n        x_k = x_k - alpha_k * g_k\n        \n        # Increment iteration counter\n        k += 1\n\n    return k\n\ndef solve():\n    \"\"\"\n    Defines and runs the test cases, printing the results in the required format.\n    \"\"\"\n    test_cases = [\n        # Case 1: n=6, kappa=9, p=1\n        {\"dimension\": 6, \"eigenvalues\": [1, 1, 1, 9, 9, 9], \"p_exponent\": 1},\n        # Case 2: n=6, kappa=9, p=3\n        {\"dimension\": 6, \"eigenvalues\": [1, 1, 1, 9, 9, 9], \"p_exponent\": 3},\n        # Case 3: n=5, kappa=1, p=5\n        {\"dimension\": 5, \"eigenvalues\": [7, 7, 7, 7, 7], \"p_exponent\": 5},\n        # Case 4: n=8, kappa=100, p=2\n        {\"dimension\": 8, \"eigenvalues\": [1, 1, 1, 1, 100, 100, 100, 100], \"p_exponent\": 2},\n        # Case 5: n=10, kappa=1000, p=3\n        {\"dimension\": 10, \"eigenvalues\": [1]*5 + [1000]*5, \"p_exponent\": 3},\n    ]\n\n    results = []\n    for case in test_cases:\n        k = run_steepest_descent(**case)\n        results.append(k)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Optimization in modern applications, such as training neural networks, often involves navigating complex, nonconvex landscapes. In these settings, the challenges extend beyond slow convergence to include the risk of getting trapped at saddle points, which are not local minima. This practice explores this critical issue by having you implement steepest descent for a simple nonconvex function . You will first demonstrate how an unfortunate initialization can lead the algorithm directly to a saddle point, then show how a small, random perturbation—a foundational idea in stochastic optimization—can provide the necessary \"nudge\" to escape the saddle and continue towards a true minimum.",
            "id": "3149710",
            "problem": "You are to construct and analyze a concrete instance of the method of steepest descent in the setting of a smooth nonconvex function that possesses a saddle point. Start from the foundational definitions: the gradient of a differentiable function, the Hessian matrix, and the method of steepest descent under the Euclidean inner product. Using these, implement a program that executes steepest descent on a specific smooth nonconvex function, demonstrates convergence to a saddle point for certain initializations, and then shows how small random perturbations can enable escape from the saddle.\n\nTasks to perform:\n- Define the function $f : \\mathbb{R}^2 \\to \\mathbb{R}$ by\n$$\nf(x,y) = x^2 - y^2 + y^4,\n$$\nwhich is smooth and nonconvex. The steepest descent direction with respect to the Euclidean inner product is the negative gradient $-\\nabla f(x,y)$. Construct the gradient mapping $g(x,y) = \\nabla f(x,y)$.\n- Use the fundamental definitions to justify that $(0,0)$ is a saddle point of $f$ by verifying $g(0,0) = (0,0)$ and that the Hessian $H(0,0)$ is indefinite.\n- Implement steepest descent with a fixed step size $\\alpha$:\n$$\n\\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\end{pmatrix}\n=\n\\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix}\n-\n\\alpha \\, g(x_k,y_k),\n$$\nand a stopping rule based on the gradient norm becoming small or after a maximum number of iterations. Additionally, implement an optional perturbation mechanism: when the current iterate is close to the saddle (within a prescribed radius) and the gradient norm is small, inject a single small random perturbation (drawn from a zero-mean normal distribution with a given standard deviation) to the current point, then continue steepest descent from the perturbed point.\n- Demonstrate that for initializations on the $x$-axis (i.e., $y_0 = 0$), steepest descent converges to the saddle $(0,0)$; whereas for initializations off the $x$-axis (i.e., $y_0 \\neq 0$), steepest descent converges to a local minimum (one of $(0,1/\\sqrt{2})$ or $(0,-1/\\sqrt{2})$). Then demonstrate that adding a small random perturbation near the saddle can enable escape and subsequent convergence to a local minimum.\n\nProgram requirements:\n- Implement the above in a single, self-contained program that takes no input and uses the specified test suite below.\n- For each test case, run steepest descent with the parameters given, and return a boolean indicating whether the algorithm converged to the saddle $(0,0)$ (defined as the final Euclidean norm of the iterate being less than a specified tolerance). The four test cases are:\n    1. Initial point $(x_0,y_0) = (1.0,0.0)$, step size $\\alpha = 0.3$, maximum iterations $200$, no perturbation. This tests convergence to the saddle along the stable manifold.\n    2. Initial point $(x_0,y_0) = (1.0,0.01)$, step size $\\alpha = 0.3$, maximum iterations $400$, no perturbation. This tests escape from the saddle to a local minimum when starting off the saddle’s stable manifold.\n    3. Initial point $(x_0,y_0) = (1.0,0.0)$, step size $\\alpha = 0.3$, maximum iterations $400$, with a single small perturbation allowed when near the saddle. Use perturbation threshold on the gradient norm $10^{-12}$, saddle proximity radius $10^{-3}$, perturbation standard deviation $10^{-3}$, and a fixed random seed $12345$. This tests escape from the saddle via perturbation even when starting on the stable manifold.\n    4. Initial point $(x_0,y_0) = (0.0,0.0)$, step size $\\alpha = 0.3$, maximum iterations $400$, with a single small perturbation allowed using the same thresholds and random seed as in case $3$. This tests escape from the saddle when starting exactly at the saddle.\n\nOutput specification:\n- For each test case, output a boolean indicating whether the final iterate is within the saddle tolerance $10^{-8}$ of $(0,0)$; that is, output $\\mathrm{True}$ if $\\sqrt{x^2+y^2} < 10^{-8}$ at termination, and $\\mathrm{False}$ otherwise.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$).\n- No physical units, angle units, or percentages are involved in this problem; all quantities are dimensionless real numbers.\n\nThe program must adhere to the specified execution environment and libraries.",
            "solution": "The problem statement has been validated and is deemed a well-posed, scientifically grounded problem in numerical optimization. We will proceed with a solution.\n\nThe core of the problem is to analyze the behavior of the method of steepest descent on the smooth, nonconvex function $f: \\mathbb{R}^2 \\to \\mathbb{R}$ defined as\n$$\nf(x,y) = x^2 - y^2 + y^4\n$$\nThis function serves as a model for understanding how optimization algorithms navigate landscapes with saddle points.\n\nFirst, we establish the necessary analytical tools. The steepest descent direction is given by the negative of the gradient. The gradient mapping, denoted by $g(x,y) = \\nabla f(x,y)$, is calculated from the partial derivatives of $f(x,y)$:\n$$\n\\frac{\\partial f}{\\partial x} = 2x\n$$\n$$\n\\frac{\\partial f}{\\partial y} = -2y + 4y^3\n$$\nThus, the gradient vector is:\n$$\ng(x,y) = \\nabla f(x,y) = \\begin{pmatrix} 2x \\\\ 4y^3 - 2y \\end{pmatrix}\n$$\n\nNext, we verify that the point $(0,0)$ is a saddle point. A point is a critical point if the gradient vanishes there. Evaluating the gradient at $(0,0)$:\n$$\ng(0,0) = \\begin{pmatrix} 2(0) \\\\ 4(0)^3 - 2(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis confirms that $(0,0)$ is a critical point. To classify this point, we analyze the Hessian matrix, $H(x,y)$, which is the matrix of second-order partial derivatives:\n$$\nH(x,y) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial y \\partial x} \\\\ \\frac{\\partial^2 f}{\\partial x \\partial y} & \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 12y^2 - 2 \\end{pmatrix}\n$$\nEvaluating the Hessian at the critical point $(0,0)$:\n$$\nH(0,0) = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}\n$$\nThe eigenvalues of $H(0,0)$ are the diagonal entries, $\\lambda_1 = 2$ and $\\lambda_2 = -2$. Since one eigenvalue is positive and the other is negative, the Hessian matrix is indefinite at $(0,0)$. A critical point with an indefinite Hessian is, by definition, a saddle point. The eigenvector corresponding to the positive eigenvalue $\\lambda_1=2$ is $(1,0)$, spanning the direction of upward curvature (the unstable subspace for a maximization problem, stable for minimization). The eigenvector for $\\lambda_2=-2$ is $(0,1)$, spanning the direction of downward curvature (the unstable subspace for minimization).\n\nThe steepest descent algorithm with a fixed step size $\\alpha > 0$ generates a sequence of iterates $(x_k, y_k)$ according to the rule:\n$$\n\\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\end{pmatrix} = \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix} - \\alpha \\, g(x_k,y_k)\n$$\nSubstituting the components of the gradient, we get the specific update rules:\n$$\nx_{k+1} = x_k - \\alpha (2x_k) = (1 - 2\\alpha)x_k\n$$\n$$\ny_{k+1} = y_k - \\alpha (4y_k^3 - 2y_k) = (1 + 2\\alpha)y_k - 4\\alpha y_k^3\n$$\nThe behavior of this dynamical system depends critically on the initial point $(x_0, y_0)$.\n\nIf we initialize on the $x$-axis, i.e., $y_0 = 0$, the second update rule implies $y_1 = (1+2\\alpha)(0) - 4\\alpha(0)^3 = 0$. By induction, if $y_k=0$, then $y_{k+1}=0$. The optimization is thus confined to the $x$-axis, and the dynamics reduce to $x_{k+1} = (1 - 2\\alpha)x_k$. For convergence to $x=0$, we require $|1 - 2\\alpha| < 1$, which holds for $0 < \\alpha < 1$. The specified step size $\\alpha=0.3$ satisfies this condition. Therefore, starting with $y_0=0$, the iterates converge to the saddle point $(0,0)$. This corresponds to moving along the stable manifold of the saddle point for the gradient flow.\n\nIf we initialize with $y_0 \\neq 0$, the dynamics are different. Near the saddle point where $y_k$ is very small, the cubic term $y_k^3$ is negligible compared to the linear term. The update for $y$ can be approximated by $y_{k+1} \\approx (1 + 2\\alpha)y_k$. Since $\\alpha > 0$, the factor $(1+2\\alpha)$ is greater than $1$. This means that any small, non-zero $y$-component will be amplified, pushing the iterate away from the $x$-axis. This is movement along the unstable manifold. The iterate escapes the vicinity of the saddle point and eventually converges to one of the function's local minima, which are located at $(0, \\pm 1/\\sqrt{2})$.\n\nThe implementation will consist of a function executing this iterative process for a given number of steps. The perturbation mechanism is designed to demonstrate that even if the algorithm is on a path to the saddle (e.g., initialized exactly on the stable manifold), a small random nudge is sufficient to push it into the region of instability and lead to escape. The perturbation is applied once when the iterate is very close to the saddle (Euclidean norm below a radius threshold) and the dynamics have slowed down (gradient norm below a threshold).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases for steepest descent\n    on a nonconvex function and report convergence to the saddle point.\n    \"\"\"\n\n    def run_steepest_descent(x0, y0, alpha, max_iter, use_perturbation, pert_params=None):\n        \"\"\"\n        Executes the steepest descent algorithm for a single test case.\n\n        Args:\n            x0 (float): Initial x-coordinate.\n            y0 (float): Initial y-coordinate.\n            alpha (float): The fixed step size.\n            max_iter (int): The maximum number of iterations.\n            use_perturbation (bool): Flag to enable/disable the perturbation mechanism.\n            pert_params (dict, optional): Parameters for the perturbation.\n\n        Returns:\n            bool: True if the final iterate is within the saddle tolerance, False otherwise.\n        \"\"\"\n\n        def grad_f(p):\n            \"\"\"Computes the gradient of f(x,y) = x^2 - y^2 + y^4.\"\"\"\n            x, y = p\n            gx = 2.0 * x\n            gy = 4.0 * y**3 - 2.0 * y\n            return np.array([gx, gy])\n\n        rng = None\n        if use_perturbation and pert_params:\n            rng = np.random.default_rng(pert_params['seed'])\n\n        p = np.array([float(x0), float(y0)])\n        perturbation_applied = False\n\n        for _ in range(max_iter):\n            # Check for and apply perturbation if conditions are met\n            if use_perturbation and not perturbation_applied and pert_params:\n                grad_norm = np.linalg.norm(grad_f(p))\n                saddle_dist = np.linalg.norm(p)\n\n                if (grad_norm < pert_params['grad_norm_thresh'] and\n                        saddle_dist < pert_params['saddle_radius']):\n                    \n                    perturbation = rng.normal(loc=0.0, scale=pert_params['std_dev'], size=2)\n                    p += perturbation\n                    perturbation_applied = True\n            \n            # Calculate gradient and perform update step\n            grad = grad_f(p)\n            p -= alpha * grad\n\n        final_norm = np.linalg.norm(p)\n        saddle_tolerance = 1e-8\n        \n        return final_norm < saddle_tolerance\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Start on stable manifold, no perturbation. Expect convergence to saddle.\n        {'x0': 1.0, 'y0': 0.0, 'alpha': 0.3, 'max_iter': 200, 'use_perturbation': False},\n        \n        # Case 2: Start near stable manifold, no perturbation. Expect escape.\n        {'x0': 1.0, 'y0': 0.01, 'alpha': 0.3, 'max_iter': 400, 'use_perturbation': False},\n\n        # Case 3: Start on stable manifold, with perturbation. Expect escape.\n        {'x0': 1.0, 'y0': 0.0, 'alpha': 0.3, 'max_iter': 400, 'use_perturbation': True,\n         'pert_params': {'grad_norm_thresh': 1e-12, 'saddle_radius': 1e-3, 'std_dev': 1e-3, 'seed': 12345}},\n        \n        # Case 4: Start at the saddle, with perturbation. Expect escape.\n        {'x0': 0.0, 'y0': 0.0, 'alpha': 0.3, 'max_iter': 400, 'use_perturbation': True,\n         'pert_params': {'grad_norm_thresh': 1e-12, 'saddle_radius': 1e-3, 'std_dev': 1e-3, 'seed': 12345}}\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_steepest_descent(**params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # str(True) -> 'True', str(False) -> 'False'\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}