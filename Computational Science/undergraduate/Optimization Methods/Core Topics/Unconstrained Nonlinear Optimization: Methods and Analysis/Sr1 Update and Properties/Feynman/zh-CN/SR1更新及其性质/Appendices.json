{
    "hands_on_practices": [
        {
            "introduction": "要真正理解一个数值方法，最好的方式往往是从最简单的情形入手。本练习将带领大家在一维设定下探索 SR1 更新。通过从第一性原理出发推导更新公式，并将其应用于二次函数，你将看到 SR1 如何在单步内获得精确的曲率信息，从而揭示其与牛顿法的深刻联系。",
            "id": "3184208",
            "problem": "考虑一个一维无约束平滑最小化问题，其目标函数 $f:\\mathbb{R}\\to\\mathbb{R}$ 是二次连续可微的，梯度为 $g(x)=\\nabla f(x)$。在拟牛顿（QN）方法中，会维持一个对海森矩阵 $\\nabla^2 f(x_k)$ 的标量曲率近似 $B_k$，步长 $s_k$ 由线性模型 $B_k s_k = -g(x_k)$ 定义，新的迭代点为 $x_{k+1}=x_k+s_k$。对称秩一（SR1）更新通过对 $B_k$ 进行秩一校正来构造 $B_{k+1}$，以满足 QN 割线条件 $B_{k+1}s_k = y_k$，其中 $y_k := g(x_{k+1}) - g(x_k)$。在一维情况下，该更新在 $y_k - B_k s_k \\neq 0$ 时是良定义的。\n\n- 任务 1（分析）：仅从 QN 割线条件 $B_{k+1}s_k = y_k$ 以及 SR1 在一维中对 $B_k$ 进行秩一改变的事实出发，推导当 $y_k - B_k s_k \\neq 0$ 时 $B_{k+1}$ 关于 $y_k$ 和 $s_k$ 的表达式。然后，利用具有恒定曲率 $a > 0$ 的二次函数 $f(x) = \\frac{1}{2} a x^2 + b x + c$ 的结构，解释为什么在这种情况下，一维 SR1 更新在单步之后能达到精确的 Newton 曲率，并说明这如何意味着后续的 QN 步与该二次函数的 Newton 步重合。\n\n- 任务 2（分段线性二次函数示例）：考虑分段二次目标函数\n$$\nf(x) = \n\\begin{cases}\n\\frac{3}{2} x^2 - x,  & x \\leq 0, \\\\\n\\frac{1}{2} x^2 - x,  & x > 0,\n\\end{cases}\n$$\n该函数在 $x=0$ 处连续。设 $x_0 = -\\frac{1}{4}$ 且 $B_0 = 2$。执行一步 QN 步骤，从 $B_0 s_0 = -g(x_0)$ 得到 $x_1 = x_0 + s_0$，然后计算 $y_0 = g(x_1) - g(x_0)$ 并使用任务 1 的结果得到 $B_1$。最后，从 $x_1$ 出发，使用 $B_1$ 再进行一步 QN 步骤，通过 $B_1 s_1 = -g(x_1)$ 得到 $x_2 = x_1 + s_1$。提供保留五位有效数字的 $x_2$ 值。不涉及物理单位；将数字作为纯实数值报告。",
            "solution": "用户提供了一个关于一维拟牛顿（QN）优化方法中对称秩一（SR1）更新的两部分问题。\n\n首先，评估问题的有效性。\n- **第1步：提取已知条件**\n  - 问题背景：对一个二次连续可微的目标函数 $f:\\mathbb{R}\\to\\mathbb{R}$ 进行一维无约束平滑最小化。\n  - 梯度：$g(x) = \\nabla f(x)$。\n  - QN 步：$x_{k+1} = x_k + s_k$，其中 $B_k s_k = -g(x_k)$。$B_k$ 是对海森矩阵 $\\nabla^2 f(x_k)$ 的一个标量近似。\n  - 割线条件：$B_{k+1}s_k = y_k$，其中 $y_k = g(x_{k+1}) - g(x_k)$。\n  - SR1 属性：$B_{k+1}$ 是通过对 $B_k$ 进行对称秩一校正形成的。\n  - 一维良定性条件：当 $y_k - B_k s_k \\neq 0$ 时，更新是良定义的。\n  - **任务1**：推导 $B_{k+1}$ 的一维 SR1 更新公式。分析其在二次函数 $f(x) = \\frac{1}{2} a x^2 + b x + c$ ($a > 0$) 上的行为，证明它在一步内计算出精确曲率，并且后续的 QN 步是牛顿步。\n  - **任务2**：一个使用 $f(x) = \\begin{cases} \\frac{3}{2} x^2 - x, & x \\leq 0, \\\\ \\frac{1}{2} x^2 - x, & x > 0 \\end{cases}$ 的数值示例。\n  - 任务2的初始条件：$x_0 = -\\frac{1}{4}$ 和 $B_0 = 2$。\n  - 任务2的要求输出：第二个迭代点 $x_2$ 的值，保留五位有效数字。\n\n- **第2步：使用提取的已知条件进行验证**\n  - 该问题在数值优化领域具有科学依据。\n  - 该问题是良构的，为两个任务提供了所有必要的信息。\n  - 存在一个微小的不一致之处：引言部分指定了一个二次连续可微的函数，而任务2使用的函数仅为一次连续可微（其二阶导数在 $x=0$ 处不连续）。然而，QN 步和 SR1 更新的机制仅要求梯度的存在，对于给定的分段函数，其梯度是连续的。因此，所有计算都是良定义的，并且可以按要求执行。这个不一致之处并未使问题无效，而是用于测试算法在非理想但可处理场景下的应用。\n  \n- **第3步：结论与行动**\n  - 问题被判定为有效。将提供完整的解答。\n\n### 任务1：分析推导与性质\n\nSR1 更新要求新的海森矩阵近似 $B_{k+1}$ 是通过对前一个近似 $B_k$ 进行对称秩一校正得到的。在一维情况下，$B_k$ 和 $B_{k+1}$ 都是标量。秩一校正矩阵也是一个标量。我们用 $c_k$ 表示这个校正。因此，更新的形式为：\n$$B_{k+1} = B_k + c_k$$\n由于标量是自对称的，对称性要求被自然满足。\n\n这个新的近似必须满足割线条件：\n$$B_{k+1} s_k = y_k$$\n其中 $s_k = x_{k+1} - x_k$ 且 $y_k = g(x_{k+1}) - g(x_k)$。\n\n将更新形式代入割线条件得到：\n$$(B_k + c_k) s_k = y_k$$\n$$B_k s_k + c_k s_k = y_k$$\n我们可以解出校正项 $c_k$。假设我们不在稳定点上，即 $g(x_k) \\neq 0$，这意味着 $s_k \\neq 0$（除非 $B_k=0$，这是一个退化情况）。因此我们可以除以 $s_k$：\n$$c_k = \\frac{y_k - B_k s_k}{s_k}$$\n问题陈述，当 $y_k - B_k s_k \\neq 0$ 时更新是良定义的。这个条件确保了校正项 $c_k$ 非零，意味着近似得到了有意义的更新。\n\n将 $c_k$ 的表达式代回 $B_{k+1}$ 的更新公式中：\n$$B_{k+1} = B_k + \\frac{y_k - B_k s_k}{s_k} = B_k + \\frac{y_k}{s_k} - \\frac{B_k s_k}{s_k} = B_k + \\frac{y_k}{s_k} - B_k$$\n这可以简化为：\n$$B_{k+1} = \\frac{y_k}{s_k}$$\n这个表达式就是一维 SR1 更新公式。它等价于割线法对二阶导数的近似。\n\n现在，考虑具有恒定曲率 $a > 0$ 的二次目标函数 $f(x) = \\frac{1}{2} a x^2 + b x + c$。\n梯度为 $g(x) = a x + b$。\n对于所有 $x$，精确的海森矩阵为 $\\nabla^2 f(x) = a$。\n\n我们来分析从任意迭代点 $x_k$ 和任意初始海森矩阵近似 $B_k$ 开始的一步 QN 方法。步长为 $x_{k+1} = x_k + s_k$。梯度差为：\n$$y_k = g(x_{k+1}) - g(x_k) = (a x_{k+1} + b) - (a x_k + b) = a (x_{k+1} - x_k) = a s_k$$\n现在，我们使用推导出的 SR1 公式计算更新后的海森矩阵近似 $B_{k+1}$：\n$$B_{k+1} = \\frac{y_k}{s_k} = \\frac{a s_k}{s_k} = a$$\n这表明，在二次函数上经过单步迭代后，一维 SR1 更新会得到精确的海森矩阵（曲率）$a$，无论初始近似 $B_k$ 是什么。\n\n后续的 QN 步长 $s_{k+1}$ 由方程 $B_{k+1} s_{k+1} = -g(x_{k+1})$ 决定。因为我们已经证明了 $B_{k+1} = a$，这个方程变为：\n$$a s_{k+1} = -g(x_{k+1})$$\n一个牛顿步 $p_{k+1}$ 由方程 $\\nabla^2 f(x_{k+1}) p_{k+1} = -g(x_{k+1})$ 定义。对于二次函数，$\\nabla^2 f(x_{k+1}) = a$。牛顿步方程为：\n$$a p_{k+1} = -g(x_{k+1})$$\n比较这两个方程，我们看到 $s_{k+1} = p_{k+1}$。因此，SR1 更新后的 QN 步与牛顿步完全相同。对于二次函数，从任意点 $x$ 开始的牛顿步会在一次迭代中收敛到最小值点。\n\n### 任务2：分段线性二次函数示例\n\n给定目标函数：\n$$\nf(x) = \n\\begin{cases}\n\\frac{3}{2} x^2 - x,  & x \\leq 0, \\\\\n\\frac{1}{2} x^2 - x,  & x > 0,\n\\end{cases}\n$$\n梯度为：\n$$\ng(x) = f'(x) = \n\\begin{cases}\n3x - 1,  & x \\leq 0, \\\\\nx - 1,  & x > 0.\n\\end{cases}\n$$\n初始条件为 $x_0 = -\\frac{1}{4}$ 和 $B_0 = 2$。\n\n**第1步：计算 $x_1$**\n首先，我们计算在 $x_0$ 处的梯度。由于 $x_0 = -0.25 \\leq 0$，我们使用 $g(x)$ 的第一种情况：\n$$g(x_0) = g(-0.25) = 3(-0.25) - 1 = -0.75 - 1 = -1.75$$\n第一步 QN 步长 $s_0$ 从 $B_0 s_0 = -g(x_0)$ 求得：\n$$2 s_0 = -(-1.75) = 1.75$$\n$$s_0 = \\frac{1.75}{2} = 0.875$$\n下一个迭代点 $x_1$ 是：\n$$x_1 = x_0 + s_0 = -0.25 + 0.875 = 0.625$$\n\n**第2步：计算 $B_1$**\n为了计算更新后的海森矩阵近似 $B_1$，我们使用公式 $B_1 = y_0/s_0$。我们首先需要计算 $y_0 = g(x_1) - g(x_0)$。\n我们有 $x_1 = 0.625 > 0$，所以我们使用 $g(x)$ 的第二种情况：\n$$g(x_1) = g(0.625) = 0.625 - 1 = -0.375$$\n现在我们计算 $y_0$：\n$$y_0 = g(x_1) - g(x_0) = -0.375 - (-1.75) = 1.375$$\n现在我们可以求出 $B_1$：\n$$B_1 = \\frac{y_0}{s_0} = \\frac{1.375}{0.875} = \\frac{11/8}{7/8} = \\frac{11}{7}$$\n\n**第3步：计算 $x_2$**\n第二步 QN 步长 $s_1$ 从 $B_1 s_1 = -g(x_1)$ 求得：\n$$\\frac{11}{7} s_1 = -(-0.375) = 0.375 = \\frac{3}{8}$$\n解出 $s_1$：\n$$s_1 = \\frac{3}{8} \\cdot \\frac{7}{11} = \\frac{21}{88}$$\n最终的迭代点 $x_2$ 是：\n$$x_2 = x_1 + s_1 = 0.625 + \\frac{21}{88} = \\frac{5}{8} + \\frac{21}{88} = \\frac{5 \\cdot 11}{88} + \\frac{21}{88} = \\frac{55+21}{88} = \\frac{76}{88} = \\frac{19}{22}$$\n为了提供数值答案，我们计算 $x_2$ 的十进制值：\n$$x_2 = \\frac{19}{22} \\approx 0.86363636...$$\n保留五位有效数字得到：\n$$x_2 \\approx 0.86364$$",
            "answer": "$$\n\\boxed{0.86364}\n$$"
        },
        {
            "introduction": "与 BFGS 等更常见的方法相比，SR1 方法的主要优势在于其处理非凸问题的能力。本练习将引导你在鞍点——非凸问题的一个典型特征——处对这两种方法进行直接比较。你将研究 SR1 如何生成不定海森矩阵近似，从而捕捉逃离鞍点所需的负曲率方向，而这正是标准 BFGS 算法刻意避免的。",
            "id": "3184299",
            "problem": "考虑为 $x \\in \\mathbb{R}^2$ 定义的二次目标函数\n$$\nf(x) = x_1^2 - x_2^2 + \\alpha x_1 x_2,\n$$\n其中 $\\alpha \\in \\mathbb{R}$ 是一个参数。令 $H \\in \\mathbb{R}^{2 \\times 2}$ 表示 $f$ 的对称Hessian矩阵，令 $g(x) = \\nabla f(x)$ 表示梯度。点 $x = 0$ 是一个鞍点。在拟牛顿法的背景下，两种标准的Hessian近似更新方法是对称秩一（SR1）更新和Broyden-Fletcher-Goldfarb-Shanno（BFGS）更新。SR1更新允许不定近似，而BFGS更新通常与一个强制正定性的曲率条件一起使用。\n\n从以下基本基础出发：\n- 对于形式为 $f(x) = \\tfrac{1}{2} x^\\top H x$ 的二次连续可微目标函数，其中 $H$ 是对称的，梯度为 $g(x) = Hx$，Hessian矩阵为常数矩阵 $H$。\n- 一个二次可微函数的鞍点的局部特征是一个不定的Hessian矩阵，意味着 $H$ 同时具有正特征值和负特征值。\n- 拟牛顿更新构建一个对称矩阵序列 $\\{B_k\\}$，旨在通过强制执行割线条件 $B_{k+1} s_k \\approx y_k$ 来近似真实的Hessian矩阵 $H$，其中 $s_k = x_{k+1} - x_k$ 且 $y_k = g(x_{k+1}) - g(x_k)$。\n\n任务：\n1. 从上述基本基础出发，为给定的 $f(x)$ 构建 $H$ 和 $g(x)$，并确定 $H$ 的特征值和特征向量。利用这些来识别与 $H$ 的最小（最负）特征值 $\\lambda_{-}$ 相关联的负曲率特征方向 $v_{-}$。\n2. 使用第一性原理和割线条件解释，为什么在近似Hessian矩阵中强制正定性可能会抑制在鞍点附近检测到负曲率，以及为什么允许不定性（如SR1所做）可以将负曲率方向编码到近似中。\n3. 按如下方式实现单步更新比较实验：\n   - 用单位矩阵 $B_0 = I$ 初始化近似Hessian矩阵。\n   - 对于给定的 $\\alpha$ 和给定的起始点 $x^{(0)} \\in \\mathbb{R}^2$，形成一个小步长 $s = h \\, v_{-}$，其中 $h = 0.1$，$v_{-}$ 是 $H$ 对应于 $\\lambda_{-}$ 的单位特征向量。设置 $x^{(1)} = x^{(0)} + s$ 并计算 $y = g(x^{(1)}) - g(x^{(0)})$。对于二次函数 $f$，这可以简化为 $y = H s$。\n   - 应用SR1更新，从 $B_0$ 和对 $(s,y)$ 得到 $B_{\\mathrm{SR1}}$。\n   - 应用BFGS更新，从 $B_0$ 和对 $(s,y)$ 得到 $B_{\\mathrm{BFGS}}$，但通过在 $s^\\top y \\le 0$ 时完全跳过更新来强制执行标准曲率条件，因此在这种情况下 $B_{\\mathrm{BFGS}} = B_0$。\n   - 对于 $\\{B_{\\mathrm{SR1}}, B_{\\mathrm{BFGS}}\\}$ 中的每个 $B$，计算其特征分解，并提取其最小特征值及对应的单位特征向量。确定 $B$ 是否捕捉到负曲率（即，具有负的最小特征值），以及其最小特征向量是否与真实的负曲率特征方向 $v_{-}$ 对齐，对齐程度由单位向量之间的绝对余弦值 $|\\langle v_{-}, v_B \\rangle|$ 来量化。使用 $0.9$ 的阈值来判断是否为强对齐。\n4. 使用上述过程，评估以下参数设置的测试套件：\n   - 测试用例1：$\\alpha = 0$, $x^{(0)} = (0.2, -0.05)$。\n   - 测试用例2：$\\alpha = 1$, $x^{(0)} = (-0.1, 0.15)$。\n   - 测试用例3：$\\alpha = 3$, $x^{(0)} = (0.05, 0.06)$。\n   在所有情况下，构造 $s$ 的标量步长均为 $h = 0.1$。\n5. 对于每个测试用例，产生一个布尔结果，该结果为真的条件是当且仅当 $B_{\\mathrm{SR1}}$ 同时满足 (i) 具有一个负的最小特征值，且 (ii) 其最小特征向量与 $v_{-}$ 的绝对余弦值至少为 $0.9$，而与此同时 $B_{\\mathrm{BFGS}}$ 没有任何负特征值。此布尔值旨在量化SR1允许不定性在编码鞍点附近正确逃逸方向方面的优势。\n   \n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按顺序包含三个测试用例的结果，例如：$[{\\tt result1},{\\tt result2},{\\tt result3}]$，其中每个 ${\\tt resulti}$ 为 ${\\tt True}$ 或 ${\\tt False}$。\n- 不涉及物理单位；所有量均为无量纲实数。",
            "solution": "该问题经评估为有效，因为它科学地基于数值优化和多元微积分的原理，问题设定良好，目标明确，数据充分，并使用了精确、客观的语言。因此，我们可以着手提供解答。\n\n问题的核心是比较对称秩一（SR1）和Broyden-Fletcher-Goldfarb-Shanno（BFGS）拟牛顿更新在鞍点附近的行为。分析的关键在于，当沿着负曲率方向迈出一步时，每种方法的Hessian近似矩阵 $B$ 如何演化。\n\n### 任务1：系统表征（梯度、Hessian矩阵和特征分解）\n\n给定的目标函数为 $f(x) = x_1^2 - x_2^2 + \\alpha x_1 x_2$，其中 $x = (x_1, x_2)^\\top \\in \\mathbb{R}^2$。\n\n首先，我们计算梯度向量 $g(x) = \\nabla f(x)$:\n$$\ng(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 + \\alpha x_2 \\\\ -2x_2 + \\alpha x_1 \\end{pmatrix}\n$$\n这可以写成矩阵形式 $g(x) = Hx$，其中矩阵 $H$ 为：\n$$\nH = \\begin{pmatrix} 2 & \\alpha \\\\ \\alpha & -2 \\end{pmatrix}\n$$\n$f(x)$ 的Hessian矩阵是二阶偏导数矩阵 $\\nabla^2 f(x)$，它确实是常数对称矩阵 $H$。问题指出，对于形式为 $f(x) = \\frac{1}{2} x^\\top H x$ 的二次函数，梯度为 $g(x) = Hx$。我们的函数满足此结构。\n\n接下来，我们通过求解特征方程 $\\det(H - \\lambda I) = 0$ 来找到 $H$ 的特征值：\n$$\n\\det \\begin{pmatrix} 2-\\lambda & \\alpha \\\\ \\alpha & -2-\\lambda \\end{pmatrix} = (2-\\lambda)(-2-\\lambda) - \\alpha^2 = -(4-\\lambda^2) - \\alpha^2 = \\lambda^2 - 4 - \\alpha^2 = 0\n$$\n这得到 $\\lambda^2 = 4 + \\alpha^2$，因此特征值为 $\\lambda = \\pm\\sqrt{4 + \\alpha^2}$。\n由于对于任何实数 $\\alpha$，$\\sqrt{4+\\alpha^2} > 0$，$H$ 有一个正特征值 $\\lambda_+ = \\sqrt{4+\\alpha^2}$ 和一个负特征值 $\\lambda_- = -\\sqrt{4+\\alpha^2}$。这证实了Hessian矩阵是不定的，且原点 $x=0$（其中 $g(0)=0$）是一个鞍点。\n\n负曲率特征方向 $v_{-}$ 是对应于最小（最负）特征值 $\\lambda_{-}$ 的特征向量。我们求解方程组 $(H - \\lambda_{-}I)v_{-} = 0:$\n$$\n\\begin{pmatrix} 2 - \\lambda_{-} & \\alpha \\\\ \\alpha & -2 - \\lambda_{-} \\end{pmatrix} v_{-} = \\begin{pmatrix} 2 + \\sqrt{4+\\alpha^2} & \\alpha \\\\ \\alpha & -2 + \\sqrt{4+\\alpha^2} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n从第一行，我们得到 $(2 + \\sqrt{4+\\alpha^2})v_1 + \\alpha v_2 = 0$。因此，一个未归一化的特征向量与 $\\begin{pmatrix} \\alpha \\\\ -(2+\\sqrt{4+\\alpha^2}) \\end{pmatrix}$ 成比例。问题所要求的向量 $v_{-}$ 是该向量的单位范数版本。\n\n### 任务2：SR1和BFGS更新的理论基础\n\n割线条件 $B_{k+1}s_k = y_k$ 是拟牛顿方法的核心。这里，$s_k = x_{k+1} - x_k$ 且 $y_k = g_{k+1} - g_k$。对于具有Hessian矩阵 $H$ 的二次函数，我们有 $y_k = H s_k$。\n\n**BFGS更新与正定性：**\n标准的BFGS更新公式为：\n$$\nB_{k+1} = B_k - \\frac{B_k s_k s_k^\\top B_k}{s_k^\\top B_k s_k} + \\frac{y_k y_k^\\top}{y_k^\\top s_k}\n$$\n此更新的一个关键性质是，如果 $B_k$ 是正定的，并且曲率条件 $y_k^\\top s_k > 0$ 成立，那么 $B_{k+1}$ 也是正定的。曲率条件 $y_k^\\top s_k = s_k^\\top H s_k$ 检查函数沿方向 $s_k$ 的曲率是否为正。在鞍点附近，可以选择一个步长 $s_k$ 使得 $s_k^\\top H s_k \\le 0$。这对应于沿着非正曲率的方向移动。如问题所述，使用BFGS的优化算法通常通过在 $y_k^\\top s_k \\le 0$ 时跳过更新来强制正定性。如果我们从 $B_0 = I$（它是正定的）开始，并且只在 $y_k^\\top s_k > 0$ 时执行更新，那么整个Hessian近似序列 $\\{B_k\\}$ 都将保持正定。一个正定矩阵只能有正特征值。因此，这种策略阻止了Hessian近似矩阵捕捉到鞍点附近真实Hessian矩阵 $H$ 的负曲率（即负特征值）。\n\n**SR1更新与不定性：**\nSR1更新公式为：\n$$\nB_{k+1} = B_k + \\frac{(y_k - B_k s_k)(y_k - B_k s_k)^\\top}{(y_k - B_k s_k)^\\top s_k}\n$$\n与BFGS不同，SR1公式中的分母 $(y_k - B_k s_k)^\\top s_k$ 可以是正、负或零。如果分母为负，从 $B_k$ 中减去的秩一项可以引入负特征值，从而允许 $B_{k+1}$ 变为不定的。这对于建模鞍点至关重要。\n\n考虑问题中提出的特定步长：$s = h v_{-}$，其中 $v_{-}$ 是负曲率特征方向。对于此步长，$y = H s = H(h v_{-}) = h (H v_{-}) = h (\\lambda_{-} v_{-}) = \\lambda_{-} s$。\n让我们从 $B_0 = I$ 开始分析SR1更新：\n分子项是 $(y - B_0 s)(y - B_0 s)^\\top = (\\lambda_{-} s - s)(\\lambda_{-} s - s)^\\top = (\\lambda_{-}-1)^2 s s^\\top$。\n分母是 $(y - B_0 s)^\\top s = (\\lambda_{-} s - s)^\\top s = (\\lambda_{-} - 1)s^\\top s = (\\lambda_{-} - 1)\\|s\\|^2$。\n由于 $\\lambda_{-} = -\\sqrt{4+\\alpha^2} \\le -2$，分母是严格为负的。\n更新为：\n$$\nB_{\\mathrm{SR1}} = I + \\frac{(\\lambda_{-}-1)^2 s s^\\top}{(\\lambda_{-} - 1)\\|s\\|^2} = I + \\frac{\\lambda_{-}-1}{\\|s\\|^2} s s^\\top\n$$\n让我们测试 $B_{\\mathrm{SR1}}$ 对步长向量 $s$ 的作用：\n$$\nB_{\\mathrm{SR1}} s = \\left( I + \\frac{\\lambda_{-}-1}{\\|s\\|^2} s s^\\top \\right) s = s + \\frac{\\lambda_{-}-1}{\\|s\\|^2} s (s^\\top s) = s + (\\lambda_{-} - 1)s = \\lambda_{-} s\n$$\n这表明 $s$（因此也包括 $v_{-}$）是更新后矩阵 $B_{\\mathrm{SR1}}$ 的一个特征向量，其特征值为 $\\lambda_{-}$。由于 $\\lambda_{-}$ 是负的，SR1更新已成功地将真实Hessian矩阵 $H$ 的负曲率纳入其近似中。任何与 $s$ 正交的向量 $w$ 都是特征值为1的特征向量，因为 $s^\\top w=0$ 意味着 $B_{\\mathrm{SR1}}w = (I + \\dots)w = Iw = w$。因此，$B_{\\mathrm{SR1}}$ 的特征值为 $\\lambda_{-}$ 和 1，正确地捕捉了负曲率。\n\n### 任务3-5：实现与评估\n\n上述逻辑针对三个指定的测试用例进行了实现。对于每个用例，我们构造 $H$，找到其负曲率特征方向 $v_{-}$，并计算步长 $s$ 和梯度差 $y$。然后我们应用SR1和BFGS更新规则。\n\n对于BFGS更新，关键量是曲率 $s^\\top y$。在我们的设置中，$s^\\top y = s^\\top (\\lambda_{-} s) = \\lambda_{-} \\|s\\|^2$。由于 $\\lambda_{-} < 0$ 且 $\\|s\\|^2 > 0$，曲率总是负的。因此，在所有测试用例中都会跳过BFGS更新，而 $B_{\\mathrm{BFGS}}$ 保持为单位矩阵 $I$，它没有负特征值。\n\n对于SR1更新，如解析所示，得到的矩阵 $B_{\\mathrm{SR1}}$ 将以 $v_{-}$ 作为特征向量，特征值为 $\\lambda_{-}$。这意味着 $B_{\\mathrm{SR1}}$ 将有一个负特征值，并且其对应的特征向量将与真实的负曲率方向 $v_{-}$ 完全对齐。\n\n因此，对于所有测试用例，布尔结果的条件都将得到满足：\n1. $B_{\\mathrm{SR1}}$ 有一个负的最小特征值 ($\\lambda_{-}$) 。\n2. 对应的特征向量 $v_{B,SR1}$ 是 $v_{-}$，因此绝对余弦值 $|\\langle v_{-}, v_{B,SR1} \\rangle|$ 是1，大于等于 $0.9$。\n3. $B_{\\mathrm{BFGS}}$ 保持为 $I$ 且没有负特征值。\n\n最终的布尔结果对于所有三个测试用例都将是true。所提供的Python代码以数值方式执行此过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    针对给定的二次函数，在鞍点附近执行SR1和BFGS更新的单步比较。\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, x0_tuple)\n        (0.0, (0.2, -0.05)),\n        (1.0, (-0.1, 0.15)),\n        (3.0, (0.05, 0.06)),\n    ]\n\n    results = []\n    \n    # Parameters for the experiment\n    h = 0.1\n    alignment_threshold = 0.9\n\n    for alpha, x0_tuple in test_cases:\n        x0 = np.array(x0_tuple) # x0 is not used in the calculations as per problem logic\n        \n        # 1. Construct H and find its negative-curvature eigendirection\n        H = np.array([\n            [2.0, alpha],\n            [alpha, -2.0]\n        ])\n        \n        # np.linalg.eigh returns eigenvalues in ascending order for symmetric matrices\n        eigenvalues, eigenvectors = np.linalg.eigh(H)\n        lambda_minus = eigenvalues[0]\n        v_minus = eigenvectors[:, 0]\n\n        # 3. Form the step s and gradient difference y\n        B0 = np.eye(2)\n        s = h * v_minus\n        \n        # For a quadratic function f(x) = 1/2 * x^T*H*x, y = g(x1)-g(x0) = Hx1-Hx0 = H(x1-x0) = Hs\n        y = H @ s\n        \n        # --- SR1 Update ---\n        y_minus_B0s = y - B0 @ s\n        den_sr1 = (y_minus_B0s).T @ s\n        # As shown in the analysis, den_sr1 is non-zero for this problem setup.\n        B_sr1 = B0 + np.outer(y_minus_B0s, y_minus_B0s) / den_sr1\n\n        # --- BFGS Update (with curvature check) ---\n        sTy = s.T @ y\n        if sTy > 0:\n            # This branch is not taken in this specific problem, as s^T*y will be negative.\n            B0s = B0 @ s\n            sTB0s = s.T @ B0s\n            B_bfgs = B0 - np.outer(B0s, B0s) / sTB0s + np.outer(y, y) / sTy\n        else:\n            # Curvature condition s^T*y = 0 is met, so we skip the BFGS update.\n            B_bfgs = B0\n\n            \n        # 4. Analyze the resulting Hessian approximations\n        \n        # SR1 Analysis\n        eigvals_sr1, eigvecs_sr1 = np.linalg.eigh(B_sr1)\n        sr1_min_eigval = eigvals_sr1[0]\n        v_B_sr1 = eigvecs_sr1[:, 0]\n        \n        sr1_has_neg_curv = sr1_min_eigval  0\n        # The dot product of two unit vectors is the cosine of the angle between them.\n        sr1_is_aligned = abs(v_minus.T @ v_B_sr1) >= alignment_threshold\n        sr1_condition_met = sr1_has_neg_curv and sr1_is_aligned\n\n        # BFGS Analysis\n        eigvals_bfgs, _ = np.linalg.eigh(B_bfgs)\n        bfgs_min_eigval = eigvals_bfgs[0]\n        bfgs_has_neg_curv = bfgs_min_eigval  0\n\n        # 5. Determine the final boolean result for the test case\n        # The result is True if SR1 succeeds and BFGS fails to capture negative curvature.\n        final_result = sr1_condition_met and not bfgs_has_neg_curv\n        results.append(final_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "SR1 方法虽然功能强大，但其一个众所周知的数值弱点是更新公式中的分母可能趋近于零。本练习将深入探讨这个关键问题，探索导致这种“崩溃”的各种情景。通过设计并测试这些有意触发数值不稳定性的案例，你将深刻理解在稳健的实现中为何以及何时必须跳过 SR1 更新，这是从理论走向实践的关键一步。",
            "id": "3184285",
            "problem": "考虑使用带有对称秩一 (SR1) 更新和强沃尔夫 (Strong Wolfe) 线搜索的拟牛顿法，对一个二次连续可微函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 进行无约束最小化。令 $x_k\\in\\mathbb{R}^n$ 表示迭代点，$g_k=\\nabla f(x_k)$ 表示梯度，$B_k$ 表示海森矩阵的一个对称近似，$p_k=-B_k^{-1}g_k$ 表示搜索方向，$s_k=\\alpha_k p_k$ 表示步长，$y_k=g_{k+1}-g_k$ 表示梯度差。SR1 更新是根据其定义的割线条件 $B_{k+1}s_k\\approx y_k$ 和在某个合适范数下的最小变化原则构造的，但众所周知，当标量分母 $(y_k - B_k s_k)^\\top s_k$ 接近于零时，它在数值上是不安全的；在这种情况下，实现时会跳过 SR1 更新并保持 $B_{k+1}=B_k$。强沃尔夫线搜索选择一个 $\\alpha_k0$，使其同时满足充分下降条件 $f(x_k+\\alpha_k p_k)\\le f(x_k)+c_1\\alpha_k g_k^\\top p_k$ 和曲率条件 $|\\nabla f(x_k+\\alpha_k p_k)^\\top p_k|\\le c_2| g_k^\\top p_k|$，其中 $c_1\\in(0,1)$ 和 $c_2\\in(c_1,1)$ 是固定常数。\n\n您的任务是探索、设计和测试序列 $(s_k,y_k)$，通过使 $(y_k - B_k s_k)^\\top s_k\\approx 0$ 来导致频繁的 SR1 跳过，并研究在强沃尔夫线搜索下的收敛行为。您的推理和实现应仅基于以下基本组成部分：梯度定义 $g(x)=\\nabla f(x)$、割线条件 $B_{k+1}s_k\\approx y_k$ 和强沃尔夫线搜索条件。请勿在问题陈述中使用或说明任何闭式 SR1 更新表达式；应在解答中根据上述原则推导和实现它。\n\n实现一个程序，该程序：\n- 使用基于 SR1 的拟牛顿法、强沃尔夫线搜索、分母崩溃跳过准则和固定的梯度范数收敛容差，构造并最小化三个测试函数。在每个测试中，统计因分母接近于零而导致的 SR1 更新跳过次数。\n- 对于给定的阈值 $\\delta0$，使用跳过准则 $\\lvert(y_k - B_k s_k)^\\top s_k\\rvert \\le \\delta \\lVert s_k\\rVert \\,\\lVert y_k - B_k s_k\\rVert$。\n- 当 $\\lVert g_k\\rVert \\le \\varepsilon$ 或达到最大迭代次数时停止。\n- 为每个测试报告整数跳过次数、表示是否收敛的布尔值以及作为浮点数的最终梯度范数。\n\n测试套件规范：\n1. 曲率精确匹配的二次函数（强制永久跳过，理想路径）：\n   - 维度 $n=2$。\n   - 海森矩阵 $H=\\operatorname{diag}(1,1)$。\n   - 目标函数 $f(x)=\\tfrac{1}{2}x^\\top H x$。\n   - 初始点 $x_0=\\begin{bmatrix}10\\\\-7\\end{bmatrix}$。\n   - 初始矩阵 $B_0=H$。\n   - 强沃尔夫参数 $c_1=10^{-4}$，$c_2=0.9$。\n   - 跳过阈值 $\\delta=10^{-8}$。\n   - 收敛容差 $\\varepsilon=10^{-12}$。\n   - 最大迭代次数 $N_{\\max}=50$。\n   由于 $y_k=H s_k$ 和 $B_k=H$，此情况会强制在所有迭代中都精确地有 $(y_k-B_k s_k)^\\top s_k=0$，从而导致永久跳过，但仍能快速收敛。\n\n2. 构造的第一步崩溃的二次函数（边界条件）：\n   - 维度 $n=2$。\n   - 海森矩阵 $H=\\operatorname{diag}(2,1)$。\n   - 目标函数 $f(x)=\\tfrac{1}{2}x^\\top H x$。\n   - 初始矩阵 $B_0=\\operatorname{diag}(3,0.5)$。\n   - 选择满足 $s_0^\\top(H-B_0)s_0=0$ 的第一步方向 $s_0=\\begin{bmatrix}1\\\\\\sqrt{2}\\end{bmatrix}$。设置初始点 $x_0=-H^{-1}B_0 s_0=\\begin{bmatrix}-\\tfrac{3}{2}\\\\-\\tfrac{\\sqrt{2}}{2}\\end{bmatrix}$，使得当 $\\alpha_0=1$ 时，算法的第一个搜索方向等于 $s_0$；这强制在第一次更新时精确地有 $(y_0-B_0 s_0)^\\top s_0=0$。\n   - 强沃尔夫参数 $c_1=10^{-4}$，$c_2=0.9$。\n   - 跳过阈值 $\\delta=10^{-8}$。\n   - 收敛容差 $\\varepsilon=10^{-12}$。\n   - 最大迭代次数 $N_{\\max}=50$。\n\n3. 受扰动的二次函数（非线性曲率，近崩溃行为）：\n   - 维度 $n=2$。\n   - 基础海森矩阵 $H=\\operatorname{diag}(2,1)$。\n   - 目标函数 $f(x)=\\tfrac{1}{2}x^\\top H x + \\tfrac{\\eta}{4}\\sum_{i=1}^2 x_i^4$，其中小扰动参数 $\\eta=10^{-3}$。\n   - 初始点 $x_0=\\begin{bmatrix}1\\\\-1.5\\end{bmatrix}$。\n   - 初始矩阵 $B_0=H$。\n   - 强沃尔夫参数 $c_1=10^{-4}$，$c_2=0.9$。\n   - 跳过阈值 $\\delta=10^{-8}$。\n   - 收敛容差 $\\varepsilon=10^{-10}$。\n   - 最大迭代次数 $N_{\\max}=50$。\n   在这种情况下，$(y_k-B_k s_k)^\\top s_k$ 通常很小但在许多步中不完全为零，因为四次扰动使得海森矩阵与位置相关。因此，SR1 的跳过可能会根据所选的 $\\delta$ 和局部几何形状而间歇性触发。\n\n输出格式要求：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果必须是 $[\\text{skips},\\text{converged},\\text{final\\_grad\\_norm}]$ 形式的列表。\n- 例如，对于三个测试，一个有效的总体输出是 $[[2,True,0.0001],[1,False,0.0123],[10,True,1.0e-08]]$。\n\n本问题陈述中的所有数学量均以 LaTeX 表示。角度和物理单位在此不适用。答案为指定的整数、布尔值和浮点数。该测试套件涵盖了一个理想路径（曲率精确匹配下永久跳过）、一个边界构造的第一步崩溃，以及一个强沃尔夫线搜索下具有近崩溃行为的非线性边缘情况。",
            "solution": "该问题要求设计并实现一个拟牛顿优化算法，该算法使用对称秩一 (SR1) 更新公式，具备防止数值不稳定性的保护措施，并与强沃尔夫 (Strong Wolfe) 线搜索相结合。任务是在三个为探究 SR1 更新跳过机制而专门构造的测试用例上，分析该算法的行为。\n\n解决方案分为三个阶段：首先，从基本原理推导 SR1 更新公式；其次，详细设计迭代算法；第三，分析指定的测试用例。\n\n### 1. SR1 更新公式的推导\n\nSR1 更新将当前的海森近似 $B_k$ 修改为新的近似 $B_{k+1}$，使其满足割线条件。割线条件是真实海森矩阵定义性质 $\\nabla^2 f(x) s \\approx \\nabla f(x+s) - \\nabla f(x)$ 的有限差分近似。对于从 $x_k$ 到 $x_{k+1}$ 的一步，其中 $s_k = x_{k+1} - x_k$ 且 $y_k = g_{k+1} - g_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$，新海森近似 $B_{k+1}$ 的割线条件为：\n$$\nB_{k+1} s_k = y_k\n$$\nSR1 公式的推导假设更新具有最简单的非平凡形式：对前一个矩阵 $B_k$ 进行对称秩一校正。即，\n$$\nB_{k+1} = B_k + \\sigma u u^\\top\n$$\n对于某个标量 $\\sigma \\in \\mathbb{R}$ 和向量 $u \\in \\mathbb{R}^n$。项 $\\sigma u u^\\top$ 是一个对称秩一矩阵。\n\n为了确定 $\\sigma$ 和 $u$，我们对 $B_{k+1}$ 强制施加割线条件：\n$$\n(B_k + \\sigma u u^\\top) s_k = y_k\n$$\n整理各项可得：\n$$\nB_k s_k + \\sigma u (u^\\top s_k) = y_k\n$$\n$$\n\\sigma (u^\\top s_k) u = y_k - B_k s_k\n$$\n这个方程表明，向量 $u$ 必须与向量 $y_k - B_k s_k$ 成比例。一个方便的选择是设置 $u = y_k - B_k s_k$。令该向量为 $v_k = y_k - B_k s_k$。如果 $v_k=0$，则意味着前一个矩阵 $B_k$ 已经满足了沿方向 $s_k$ 的割线条件。在这种情况下，不需要更新，因此可以设置 $B_{k+1}=B_k$。\n\n如果 $v_k \\neq 0$，将 $u = v_k$ 代入方程中得到：\n$$\n\\sigma (v_k^\\top s_k) v_k = v_k\n$$\n由于 $v_k \\neq 0$，我们可以消去向量 $v_k$，得到一个标量方程：\n$$\n\\sigma (v_k^\\top s_k) = 1\n$$\n如果标量分母 $(v_k^\\top s_k) = (y_k - B_k s_k)^\\top s_k$ 非零，我们可以解出 $\\sigma$：\n$$\n\\sigma = \\frac{1}{(y_k - B_k s_k)^\\top s_k}\n$$\n将 $\\sigma$ 和 $u$ 代回更新形式，得到 SR1 更新公式：\n$$\nB_{k+1} = B_k + \\frac{(y_k - B_k s_k)(y_k - B_k s_k)^\\top}{(y_k - B_k s_k)^\\top s_k}\n$$\n如果分母 $(y_k - B_k s_k)^\\top s_k = 0$，此公式未定义。在实践中，如果此分母接近于零，则数值上不稳定。\n\n### 2. 算法设计：带有 SR1 和强沃尔夫线搜索的拟牛顿法\n\n实现的核心是一个通用的拟牛顿迭代过程。\n\n**算法：SR1 拟牛顿法**\n1.  **初始化：**给定初始点 $x_0$、初始对称海森近似 $B_0$（通常为单位矩阵）、收敛容差 $\\varepsilon  0$ 和最大迭代次数 $N_{\\max}$。设置 $k=0$，并计算初始梯度 $g_0 = \\nabla f(x_0)$。初始化跳过计数器 `skips = 0`。\n\n2.  **迭代：**对于 $k = 0, 1, 2, \\dots, N_{\\max}-1$：\n    a.  **收敛性检查：**如果 $\\lVert g_k \\rVert \\le \\varepsilon$，则终止并报告收敛。\n    b.  **计算搜索方向：**求解线性系统 $B_k p_k = -g_k$ 以获得搜索方向 $p_k$。这是牛顿类方法的定义步骤。\n    c.  **线搜索：**寻找一个步长 $\\alpha_k  0$，使其满足强沃尔夫条件：\n        i.  充分下降：$f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top p_k$\n        ii. 曲率条件：$\\lvert g(x_k + \\alpha_k p_k)^\\top p_k \\rvert \\le c_2 \\lvert g_k^\\top p_k \\rvert$\n        对于常数 $0  c_1  c_2  1$。此搜索确保步长在减小目标函数方面取得足够进展，并且步长不会过短或过长。为此目的，采用了一个鲁棒的线搜索程序。如果找不到这样的 $\\alpha_k$，则优化终止。\n    d.  **更新状态：**计算步长 $s_k = \\alpha_k p_k$ 和新的迭代点 $x_{k+1} = x_k + s_k$。计算新梯度 $g_{k+1} = \\nabla f(x_{k+1})$ 和梯度差 $y_k = g_{k+1} - g_k$。\n    e.  **更新海森近似 ($B_{k+1}$):**\n        i.  计算向量 $v_k = y_k - B_k s_k$ 和分母 $d_k = v_k^\\top s_k$。\n        ii. **跳过条件：**检查更新是否数值上安全。使用给定的准则，如果对于一个小的阈值 $\\delta  0$ 有 $\\lvert d_k \\rvert \\le \\delta \\lVert s_k \\rVert \\lVert v_k \\rVert$，则认为更新不安全。在这种情况下，通过设置 $B_{k+1} = B_k$ 来跳过更新，并增加跳过计数器。\n        iii. **SR1 更新：**否则，应用 SR1 公式：$B_{k+1} = B_k + \\frac{v_k v_k^\\top}{d_k}$。\n\n3.  **终止：**如果循环因达到 $N_{\\max}$ 而完成，则报告不收敛。最终状态包括跳过次数、收敛状态和最终梯度范数。\n\n### 3. 测试用例分析\n\n这三个测试用例旨在系统地评估 SR1 更新及其跳过逻辑在不同曲率条件下的行为。\n\n1.  **曲率精确匹配的二次函数：**对于二次函数 $f(x) = \\frac{1}{2}x^\\top H x$，梯度为 $g(x) = Hx$。梯度差为 $y_k = g_{k+1} - g_k = Hx_{k+1} - Hx_k = H(x_{k+1} - x_k) = H s_k$。初始海森近似被设置为真实海森矩阵，$B_0 = H$。在第一次迭代中，$v_0 = y_0 - B_0 s_0 = H s_0 - H s_0 = 0$。因此分母为 $(y_0 - B_0 s_0)^\\top s_0 = 0$。跳过条件 $\\lvert 0 \\rvert \\le \\delta \\lVert s_0 \\rVert \\lVert 0 \\rVert$（简化为 $0 \\le 0$）得到满足。更新被跳过，且 $B_1 = B_0 = H$。根据归纳法，如果 $B_k=H$，则后续更新也将被跳过，导致 $B_{k+1}=H$。因此，在每次迭代中都会发生跳过。对于这个特定问题，使用精确海森矩阵的牛顿步 $p_k = -H^{-1}(Hx_k) = -x_k$，当 $\\alpha_k=1$ 时，可以在单步内找到最小值 $x=0$。因此，我们预期有一次迭代和一次跳过的更新。\n\n2.  **构造的第一步崩溃的二次函数：**这个案例构建了一个情景，其中尽管 $B_0 \\neq H$，SR1 的分母在第一步（$k=0$）时也精确为零。初始条件被设计成使得 $p_0$ 是一个特定的向量 $s_0$，并且线搜索很可能选择 $\\alpha_0=1$。分母是 $(y_0 - B_0 s_0)^\\top s_0$。由于 $f(x)$ 是海森矩阵为 $H$ 的二次函数，$y_0 = H s_0$。分母变为 $(H s_0 - B_0 s_0)^\\top s_0 = s_0^\\top(H-B_0)s_0$。问题给出的 $s_0$、$H$ 和 $B_0$ 使得此量精确为零。这强制在 $k=0$ 时跳过。此步之后，$B_1=B_0$，后续迭代将使用这个次优的海森近似进行，可能会收敛而不再发生跳过，除非几何形状触发了另一个接近零的分母。\n\n3.  **受扰动的二次函数：**该函数 $f(x)=\\tfrac{1}{2}x^\\top H x + \\tfrac{\\eta}{4}\\sum_{i} x_i^4$ 不再是纯二次函数。其海森矩阵 $\\nabla^2 f(x) = H + 3\\eta \\cdot \\operatorname{diag}(x_1^2, x_2^2, \\dots)$ 现在依赖于位置 $x$。初始近似 $B_0=H$ 仅在 $x=0$ 处精确。在其他点，$B_k$ 将落后于真实海森矩阵。$y_k$ 项可以通过中值定理近似为 $y_k = \\bar{H}_k s_k$，其中 $\\bar{H}_k = \\int_0^1 \\nabla^2 f(x_k + \\tau s_k) d\\tau$ 是沿步长 $s_k$ 的平均海森矩阵。分母变为 $((\\bar{H}_k - B_k)s_k)^\\top s_k$。由于非线性，$\\bar{H}_k - B_k$ 通常是非零的，但对于小步长或当 $B_k$ 是一个很好的近似时，这个差异可能很小。因此，当迭代探索曲率变化的区域时，分母可能相对于向量的范数变得很小，从而间歇性地触发跳过条件。这个测试用例模拟了一个更现实的场景，其中由于问题不断演变的几何形状而发生近崩溃。\n\n现在的实现将遵循此理论框架。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import line_search\n\ndef quasi_newton_sr1(func, grad, x0, B0, c1, c2, delta, epsilon, max_iter):\n    \"\"\"\n    Performs unconstrained optimization using a Quasi-Newton method with\n    the SR1 update, a Strong Wolfe line search, and a skip condition.\n    \"\"\"\n    skips = 0\n    converged = False\n    \n    x_k = np.copy(x0)\n    B_k = np.copy(B0)\n    g_k = grad(x_k)\n\n    for k in range(max_iter):\n        g_norm = np.linalg.norm(g_k)\n        if g_norm = epsilon:\n            converged = True\n            break\n        \n        try:\n            # Step 1: Compute search direction\n            p_k = np.linalg.solve(B_k, -g_k)\n        except np.linalg.LinAlgError:\n            # Failed to compute search direction, terminate\n            break\n\n        # Ensure it's a descent direction\n        if g_k.T @ p_k >= 0:\n            break\n\n        # Step 2: Strong Wolfe Line Search\n        f_k = func(x_k)\n        # Using scipy's line search implementation\n        alpha_k, _, _, _, _, g_kp1 = line_search(\n            func, grad, x_k, p_k, gfk=g_k, old_fval=f_k, c1=c1, c2=c2, maxiter=50\n        )\n\n        if alpha_k is None:\n            # Line search failed to find a suitable step length\n            break\n\n        # Step 3: Update state\n        s_k = alpha_k * p_k\n        x_kp1 = x_k + s_k\n        # g_kp1 is returned by line_search\n        y_k = g_kp1 - g_k\n\n        # Step 4: Update Hessian approximation (SR1)\n        v = y_k - B_k @ s_k\n        s_norm = np.linalg.norm(s_k)\n        \n        if s_norm == 0:\n            # Zero step, no progress, terminate\n            break\n            \n        v_norm = np.linalg.norm(v)\n        denom = v.T @ s_k\n\n        # Check skip condition\n        if np.abs(denom) = delta * s_norm * v_norm:\n            B_kp1 = B_k # Skip update\n            skips += 1\n        else:\n            B_kp1 = B_k + np.outer(v, v) / denom\n\n        # Prepare for next iteration\n        x_k = x_kp1\n        g_k = g_kp1\n        B_k = B_kp1\n    \n    final_grad_norm = np.linalg.norm(g_k)\n    # Check convergence one last time after the loop\n    if not converged and final_grad_norm = epsilon:\n        converged = True\n\n    return [skips, converged, final_grad_norm]\n\ndef solve():\n    \"\"\"\n    Sets up and runs the three test cases for the SR1 Quasi-Newton method.\n    \"\"\"\n    test_cases = []\n\n    # Test Case 1: Quadratic with exact curvature match\n    H1 = np.diag([1.0, 1.0])\n    f1 = lambda x: 0.5 * x.T @ H1 @ x\n    g1 = lambda x: H1 @ x\n    x0_1 = np.array([10.0, -7.0])\n    B0_1 = np.copy(H1)\n    params1 = {\n        'func': f1, 'grad': g1, 'x0': x0_1, 'B0': B0_1,\n        'c1': 1e-4, 'c2': 0.9, 'delta': 1e-8,\n        'epsilon': 1e-12, 'max_iter': 50\n    }\n    test_cases.append(params1)\n\n    # Test Case 2: Quadratic with engineered first-step breakdown\n    H2 = np.diag([2.0, 1.0])\n    f2 = lambda x: 0.5 * x.T @ H2 @ x\n    g2 = lambda x: H2 @ x\n    B0_2 = np.diag([3.0, 0.5])\n    s0_2 = np.array([1.0, np.sqrt(2.0)])\n    # x0 is engineered: x0 = -inv(H) * B0 * s0\n    x0_2 = -np.linalg.inv(H2) @ B0_2 @ s0_2\n    params2 = {\n        'func': f2, 'grad': g2, 'x0': x0_2, 'B0': B0_2,\n        'c1': 1e-4, 'c2': 0.9, 'delta': 1e-8,\n        'epsilon': 1e-12, 'max_iter': 50\n    }\n    test_cases.append(params2)\n\n    # Test Case 3: Perturbed quadratic\n    H3 = np.diag([2.0, 1.0])\n    eta3 = 1e-3\n    f3 = lambda x: 0.5 * x.T @ H3 @ x + (eta3 / 4.0) * (x[0]**4 + x[1]**4)\n    g3 = lambda x: H3 @ x + eta3 * np.array([x[0]**3, x[1]**3])\n    x0_3 = np.array([1.0, -1.5])\n    B0_3 = np.copy(H3)\n    params3 = {\n        'func': f3, 'grad': g3, 'x0': x0_3, 'B0': B0_3,\n        'c1': 1e-4, 'c2': 0.9, 'delta': 1e-8,\n        'epsilon': 1e-10, 'max_iter': 50\n    }\n    test_cases.append(params3)\n\n    results = []\n    for params in test_cases:\n        result = quasi_newton_sr1(\n            params['func'], params['grad'], params['x0'], params['B0'],\n            params['c1'], params['c2'], params['delta'],\n            params['epsilon'], params['max_iter']\n        )\n        results.append(result)\n\n    final_output = []\n    for res in results:\n        final_output.append(f\"[{res[0]},{'True' if res[1] else 'False'},{res[2]}]\")\n\n    print(f\"[{','.join(final_output)}]\")\n\n\nsolve()\n```"
        }
    ]
}