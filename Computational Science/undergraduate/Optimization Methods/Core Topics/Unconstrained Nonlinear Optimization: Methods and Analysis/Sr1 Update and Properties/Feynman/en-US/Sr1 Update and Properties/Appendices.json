{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the Symmetric Rank-1 (SR1) update, we begin with its simplest form. This exercise guides you through deriving the SR1 formula in a one-dimensional setting, revealing its elegant connection to the classic secant method. By applying it to a quadratic function, you will demonstrate a core property of quasi-Newton methods: the ability to learn the exact curvature in a finite number of steps .",
            "id": "3184208",
            "problem": "Consider a one-dimensional unconstrained smooth minimization problem for a twice continuously differentiable objective function $f:\\mathbb{R}\\to\\mathbb{R}$ with gradient $g(x)=\\nabla f(x)$. In a Quasi-Newton (QN) method, a scalar curvature approximation $B_k$ to the Hessian $\\nabla^2 f(x_k)$ is maintained, and the step $s_k$ is defined by the linear model $B_k s_k = -g(x_k)$ with the new iterate $x_{k+1}=x_k+s_k$. The Symmetric Rank-One (SR1) update constructs $B_{k+1}$ to satisfy the QN secant condition $B_{k+1}s_k = y_k$, where $y_k := g(x_{k+1}) - g(x_k)$, using a rank-one correction to $B_k$ and requiring the update to be well-defined in the one-dimensional case whenever $y_k - B_k s_k \\neq 0$.\n\n- Task 1 (analytical): Starting only from the QN secant condition $B_{k+1}s_k = y_k$ and the fact that SR1 makes a rank-one change to $B_k$ in one dimension, derive the expression for $B_{k+1}$ in terms of $y_k$ and $s_k$ when $y_k - B_k s_k \\neq 0$. Then, using the structure of a quadratic function $f(x) = \\frac{1}{2} a x^2 + b x + c$ with constant curvature $a > 0$, explain why the one-dimensional SR1 update attains the exact Newton curvature after a single step in this setting, and show how this implies that the subsequent QN step coincides with the Newton step for this quadratic.\n\n- Task 2 (piecewise linear-quadratic illustration): Consider the piecewise quadratic objective\n$$\nf(x) = \n\\begin{cases}\n\\frac{1}{2}\\cdot 3\\, x^2 - 1\\cdot x, & x \\leq 0, \\\\\n\\frac{1}{2}\\cdot 1\\, x^2 - 1\\cdot x, & x > 0,\n\\end{cases}\n$$\nwhich is continuous at $x=0$. Let $x_0 = -\\frac{1}{4}$ and $B_0 = 2$. Perform one QN step to get $x_1 = x_0 + s_0$ from $B_0 s_0 = -g(x_0)$, then compute $y_0 = g(x_1) - g(x_0)$ and use your result from Task 1 to obtain $B_1$. Finally, take one more QN step from $x_1$ using $B_1$ to obtain $x_2 = x_1 + s_1$ with $B_1 s_1 = -g(x_1)$. Provide the value of $x_2$ rounded to five significant figures. No physical units are involved; report the number as a pure real value.",
            "solution": "The user has provided a two-part problem concerning the Symmetric Rank-One (SR1) update for a one-dimensional Quasi-Newton (QN) optimization method.\n\nFirst, the problem's validity is assessed.\n- **Step 1: Extract Givens**\n  - Problem context: One-dimensional unconstrained smooth minimization of a twice continuously differentiable objective function $f:\\mathbb{R}\\to\\mathbb{R}$.\n  - Gradient: $g(x) = \\nabla f(x)$.\n  - QN step: $x_{k+1} = x_k + s_k$, where $B_k s_k = -g(x_k)$. $B_k$ is a scalar approximation to the Hessian $\\nabla^2 f(x_k)$.\n  - Secant condition: $B_{k+1}s_k = y_k$, where $y_k = g(x_{k+1}) - g(x_k)$.\n  - SR1 property: $B_{k+1}$ is formed by a symmetric rank-one correction to $B_k$.\n  - 1D well-definedness condition: The update is well-defined whenever $y_k - B_k s_k \\neq 0$.\n  - **Task 1**: Derive the 1D SR1 update for $B_{k+1}$. Analyze its behavior for a quadratic function $f(x) = \\frac{1}{2} a x^2 + b x + c$ ($a > 0$), showing it computes the exact curvature in one step and that the subsequent QN step is a Newton step.\n  - **Task 2**: A numerical illustration with $f(x) = \\begin{cases} \\frac{1}{2}\\cdot 3\\, x^2 - 1\\cdot x, & x \\leq 0, \\\\ \\frac{1}{2}\\cdot 1\\, x^2 - 1\\cdot x, & x > 0. \\end{cases}$\n  - Initial conditions for Task 2: $x_0 = -\\frac{1}{4}$ and $B_0 = 2$.\n  - Required output for Task 2: Value of the second iterate $x_2$, rounded to five significant figures.\n\n- **Step 2: Validate Using Extracted Givens**\n  - The problem is scientifically grounded in the field of numerical optimization.\n  - The problem is well-posed, with all necessary information provided for both tasks.\n  - There is a minor inconsistency: the introduction specifies a twice continuously differentiable function, while Task 2 uses a function that is only once continuously differentiable (the second derivative is discontinuous at $x=0$). However, the mechanics of the QN steps and the SR1 update only require the existence of the gradient, which is continuous for the given piecewise function. Therefore, all calculations are well-defined and can be performed as requested. This inconsistency does not invalidate the problem but rather serves to test the application of the algorithm in a non-ideal but tractable scenario.\n  \n- **Step 3: Verdict and Action**\n  - The problem is deemed valid. A full solution will be provided.\n\n### Task 1: Analytical Derivation and Properties\n\nThe SR1 update requires that the new Hessian approximation, $B_{k+1}$, is obtained by a symmetric rank-one correction to the previous approximation, $B_k$. In a one-dimensional setting, $B_k$ and $B_{k+1}$ are scalars. A rank-one correction matrix is also a scalar. Let us denote this correction by $c_k$. Thus, the update has the form:\n$$B_{k+1} = B_k + c_k$$\nThe symmetry requirement is trivially satisfied as scalars are symmetric.\n\nThis new approximation must satisfy the secant condition:\n$$B_{k+1} s_k = y_k$$\nwhere $s_k = x_{k+1} - x_k$ and $y_k = g(x_{k+1}) - g(x_k)$.\n\nSubstituting the update form into the secant condition gives:\n$$(B_k + c_k) s_k = y_k$$\n$$B_k s_k + c_k s_k = y_k$$\nWe can solve for the correction term $c_k$. Assuming we are not at a stationary point, $g(x_k) \\neq 0$, which implies $s_k \\neq 0$ (unless $B_k=0$, a degenerate case). We can therefore divide by $s_k$:\n$$c_k = \\frac{y_k - B_k s_k}{s_k}$$\nThe problem states that the update is well-defined whenever $y_k - B_k s_k \\neq 0$. This condition ensures that the correction $c_k$ is non-zero, meaning the approximation is meaningfully updated.\n\nSubstituting the expression for $c_k$ back into the update formula for $B_{k+1}$:\n$$B_{k+1} = B_k + \\frac{y_k - B_k s_k}{s_k} = B_k + \\frac{y_k}{s_k} - \\frac{B_k s_k}{s_k} = B_k + \\frac{y_k}{s_k} - B_k$$\nThis simplifies to:\n$$B_{k+1} = \\frac{y_k}{s_k}$$\nThis expression is the one-dimensional SR1 update formula. It is equivalent to the secant method's approximation of the second derivative.\n\nNow, consider the quadratic objective function $f(x) = \\frac{1}{2} a x^2 + b x + c$, with constant curvature $a > 0$.\nThe gradient is $g(x) = a x + b$.\nThe exact Hessian is $\\nabla^2 f(x) = a$ for all $x$.\n\nLet's analyze one step of the QN method starting from an arbitrary iterate $x_k$ and an arbitrary initial Hessian approximation $B_k$. The step is $x_{k+1} = x_k + s_k$. The gradient difference is:\n$$y_k = g(x_{k+1}) - g(x_k) = (a x_{k+1} + b) - (a x_k + b) = a (x_{k+1} - x_k) = a s_k$$\nNow, we compute the updated Hessian approximation $B_{k+1}$ using the derived SR1 formula:\n$$B_{k+1} = \\frac{y_k}{s_k} = \\frac{a s_k}{s_k} = a$$\nThis demonstrates that after a single step on a quadratic function, the 1D SR1 update yields the exact Hessian (curvature), $a$, regardless of the initial approximation $B_k$.\n\nThe subsequent QN step, $s_{k+1}$, is determined by the equation $B_{k+1} s_{k+1} = -g(x_{k+1})$. Since we have shown that $B_{k+1} = a$, this equation becomes:\n$$a s_{k+1} = -g(x_{k+1})$$\nA Newton step, $p_{k+1}$, is defined by the equation $\\nabla^2 f(x_{k+1}) p_{k+1} = -g(x_{k+1})$. For the quadratic function, $\\nabla^2 f(x_{k+1}) = a$. The Newton step equation is:\n$$a p_{k+1} = -g(x_{k+1})$$\nComparing the two equations, we see that $s_{k+1} = p_{k+1}$. Therefore, the QN step following the SR1 update is identical to the Newton step. For a quadratic function, the Newton step from any point $x$ converges to the minimizer in a single iteration.\n\n### Task 2: Piecewise Linear-Quadratic Illustration\n\nWe are given the objective function:\n$$f(x) = \n\\begin{cases}\n\\frac{3}{2} x^2 - x, & x \\leq 0, \\\\\n\\frac{1}{2} x^2 - x, & x > 0,\n\\end{cases}\n$$\nThe gradient is:\n$$g(x) = f'(x) = \n\\begin{cases}\n3x - 1, & x \\leq 0, \\\\\nx - 1, & x > 0.\n\\end{cases}\n$$\nThe initial conditions are $x_0 = -\\frac{1}{4}$ and $B_0 = 2$.\n\n**Step 1: Compute $x_1$**\nFirst, we compute the gradient at $x_0$. Since $x_0 = -0.25 \\leq 0$, we use the first case for $g(x)$:\n$$g(x_0) = g(-0.25) = 3(-0.25) - 1 = -0.75 - 1 = -1.75$$\nThe first QN step $s_0$ is found from $B_0 s_0 = -g(x_0)$:\n$$2 s_0 = -(-1.75) = 1.75$$\n$$s_0 = \\frac{1.75}{2} = 0.875$$\nThe next iterate $x_1$ is:\n$$x_1 = x_0 + s_0 = -0.25 + 0.875 = 0.625$$\n\n**Step 2: Compute $B_1$**\nTo compute the updated Hessian approximation $B_1$, we use the formula $B_1 = y_0/s_0$. We first need to compute $y_0 = g(x_1) - g(x_0)$.\nWe have $x_1 = 0.625 > 0$, so we use the second case for $g(x)$:\n$$g(x_1) = g(0.625) = 0.625 - 1 = -0.375$$\nNow we compute $y_0$:\n$$y_0 = g(x_1) - g(x_0) = -0.375 - (-1.75) = 1.375$$\nWe can now find $B_1$:\n$$B_1 = \\frac{y_0}{s_0} = \\frac{1.375}{0.875} = \\frac{11/8}{7/8} = \\frac{11}{7}$$\n\n**Step 3: Compute $x_2$**\nThe second QN step $s_1$ is found from $B_1 s_1 = -g(x_1)$:\n$$\\frac{11}{7} s_1 = -(-0.375) = 0.375 = \\frac{3}{8}$$\nSolving for $s_1$:\n$$s_1 = \\frac{3}{8} \\cdot \\frac{7}{11} = \\frac{21}{88}$$\nThe final iterate $x_2$ is:\n$$x_2 = x_1 + s_1 = 0.625 + \\frac{21}{88} = \\frac{5}{8} + \\frac{21}{88} = \\frac{5 \\cdot 11}{88} + \\frac{21}{88} = \\frac{55+21}{88} = \\frac{76}{88} = \\frac{19}{22}$$\nTo provide the numerical answer, we compute the decimal value of $x_2$:\n$$x_2 = \\frac{19}{22} \\approx 0.86363636...$$\nRounding to five significant figures gives:\n$$x_2 \\approx 0.86364$$",
            "answer": "$$\n\\boxed{0.86364}\n$$"
        },
        {
            "introduction": "The SR1 method's most significant advantage over its popular counterpart, the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method, is its ability to effectively handle non-convexity. This practice delves into the behavior of SR1 near a saddle point, a common feature in complex optimization landscapes. Through a carefully designed experiment, you will see firsthand how SR1's capacity to generate indefinite Hessian approximations allows it to identify and exploit escape directions from saddle points, a task where standard BFGS often falters .",
            "id": "3184299",
            "problem": "Consider the quadratic objective function defined for $x \\in \\mathbb{R}^2$ by\n$$\nf(x) = x_1^2 - x_2^2 + \\alpha x_1 x_2,\n$$\nwhere $\\alpha \\in \\mathbb{R}$ is a parameter. Let $H \\in \\mathbb{R}^{2 \\times 2}$ denote the symmetric Hessian matrix of $f$, and let $g(x) = \\nabla f(x)$ denote the gradient. The point $x = 0$ is a saddle point. In the context of quasi-Newton methods, two standard Hessian approximation updates are Symmetric Rank-One (SR1) and Broyden-Fletcher-Goldfarb-Shanno (BFGS). The SR1 update allows indefinite approximations, while the BFGS update is commonly used with a curvature condition that enforces positive definiteness.\n\nStarting from the following fundamental base:\n- For a twice continuously differentiable quadratic objective of the form $f(x) = \\tfrac{1}{2} x^\\top H x$ with $H$ symmetric, the gradient is $g(x) = H x$ and the Hessian is the constant matrix $H$.\n- A saddle point of a twice differentiable function is characterized locally by an indefinite Hessian, meaning $H$ has both positive and negative eigenvalues.\n- Quasi-Newton updates construct a sequence of symmetric matrices $\\{B_k\\}$ that aim to approximate the true Hessian $H$ by enforcing the secant condition $B_{k+1} s_k \\approx y_k$, where $s_k = x_{k+1} - x_k$ and $y_k = g(x_{k+1}) - g(x_k)$.\n\nTasks:\n1. Construct $H$ and $g(x)$ for the given $f(x)$ starting from the fundamental base above, and determine the eigenvalues and eigenvectors of $H$. Use these to identify the negative-curvature eigendirection $v_{-}$ associated with the smallest (most negative) eigenvalue $\\lambda_{-}$ of $H$.\n2. Explain, using first principles and the secant condition, why enforcing positive definiteness in the approximate Hessian may suppress detection of negative curvature near a saddle, and why allowing indefiniteness (as SR1 does) can encode the negative curvature direction into the approximation.\n3. Implement a single-update comparison experiment as follows:\n   - Initialize the approximate Hessian with $B_0 = I$, the identity matrix.\n   - For a given $\\alpha$ and a given starting point $x^{(0)} \\in \\mathbb{R}^2$, form a small step $s = h \\, v_{-}$ with $h = 0.1$, where $v_{-}$ is the unit eigenvector of $H$ corresponding to $\\lambda_{-}$. Set $x^{(1)} = x^{(0)} + s$ and compute $y = g(x^{(1)}) - g(x^{(0)})$. For a quadratic $f$, this simplifies to $y = H s$.\n   - Apply the SR1 update to obtain $B_{\\mathrm{SR1}}$ from $B_0$ and the pair $(s,y)$.\n   - Apply the BFGS update to obtain $B_{\\mathrm{BFGS}}$ from $B_0$ and the pair $(s,y)$, but enforce the standard curvature condition by skipping the update entirely when $s^\\top y \\le 0$ so that $B_{\\mathrm{BFGS}} = B_0$ in that case.\n   - For each $B$ in $\\{B_{\\mathrm{SR1}}, B_{\\mathrm{BFGS}}\\}$, compute its eigen-decomposition and extract its smallest eigenvalue and corresponding unit eigenvector. Determine whether $B$ captures negative curvature (i.e., has a negative smallest eigenvalue) and whether its smallest-eigenvector aligns with the true negative-curvature eigendirection $v_{-}$, quantified by the absolute cosine $|\\langle v_{-}, v_B \\rangle|$ between the unit vectors. Use a threshold of $0.9$ for declaring a strong alignment.\n4. Using the above procedure, evaluate the following test suite of parameter settings:\n   - Test case $1$: $\\alpha = 0$, $x^{(0)} = (0.2, -0.05)$.\n   - Test case $2$: $\\alpha = 1$, $x^{(0)} = (-0.1, 0.15)$.\n   - Test case $3$: $\\alpha = 3$, $x^{(0)} = (0.05, 0.06)$.\n   The scalar step size in the construction of $s$ is $h = 0.1$ in all cases.\n5. For each test case, produce a boolean result that is true if and only if $B_{\\mathrm{SR1}}$ both (i) has a negative smallest eigenvalue and (ii) its smallest-eigenvector has absolute cosine with $v_{-}$ at least $0.9$, while simultaneously $B_{\\mathrm{BFGS}}$ fails to have any negative eigenvalue. This boolean is intended to quantify the advantage of SR1â€™s allowance of indefiniteness for encoding the correct escape direction near the saddle.\n   \nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the three test cases in order, for example: $[{\\tt result1},{\\tt result2},{\\tt result3}]$, where each ${\\tt resulti}$ is either ${\\tt True}$ or ${\\tt False}$.\n- No physical units are involved; all quantities are dimensionless real numbers.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of numerical optimization and multivariable calculus, is well-posed with a clear objective and sufficient data, and uses precise, objective language. We may therefore proceed with a solution.\n\nThe core of the problem is to compare the behavior of the Symmetric Rank-One (SR1) and Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton updates in the vicinity of a saddle point. The analysis hinges on how each method's Hessian approximation, $B$, evolves when a step is taken along a direction of negative curvature.\n\n### Task 1: System Characterization (Gradient, Hessian, and Eigendecomposition)\n\nThe given objective function is $f(x) = x_1^2 - x_2^2 + \\alpha x_1 x_2$ for $x = (x_1, x_2)^\\top \\in \\mathbb{R}^2$.\n\nFirst, we compute the gradient vector $g(x) = \\nabla f(x)$:\n$$\ng(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 + \\alpha x_2 \\\\ -2x_2 + \\alpha x_1 \\end{pmatrix}\n$$\nThis can be written in matrix form as $g(x) = Hx$, where the matrix $H$ is:\n$$\nH = \\begin{pmatrix} 2 & \\alpha \\\\ \\alpha & -2 \\end{pmatrix}\n$$\nThe Hessian of $f(x)$ is the matrix of second partial derivatives, $\\nabla^2 f(x)$, which is indeed the constant, symmetric matrix $H$. The problem states that for a quadratic function of the form $f(x) = \\frac{1}{2} x^\\top H x$, the gradient is $g(x) = Hx$. Our function satisfies this structure.\n\nNext, we find the eigenvalues of $H$ by solving the characteristic equation $\\det(H - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 2-\\lambda & \\alpha \\\\ \\alpha & -2-\\lambda \\end{pmatrix} = (2-\\lambda)(-2-\\lambda) - \\alpha^2 = -(4-\\lambda^2) - \\alpha^2 = \\lambda^2 - 4 - \\alpha^2 = 0\n$$\nThis gives $\\lambda^2 = 4 + \\alpha^2$, so the eigenvalues are $\\lambda = \\pm\\sqrt{4 + \\alpha^2}$.\nAs $\\sqrt{4+\\alpha^2} > 0$ for any real $\\alpha$, $H$ has one positive eigenvalue, $\\lambda_+ = \\sqrt{4+\\alpha^2}$, and one negative eigenvalue, $\\lambda_- = -\\sqrt{4+\\alpha^2}$. This confirms that the Hessian is indefinite and the origin $x=0$ (where $g(0)=0$) is a saddle point.\n\nThe negative-curvature eigendirection $v_{-}$ is the eigenvector corresponding to the smallest (most negative) eigenvalue $\\lambda_{-}$. We solve the system $(H - \\lambda_{-}I)v_{-} = 0$:\n$$\n\\begin{pmatrix} 2 - \\lambda_{-} & \\alpha \\\\ \\alpha & -2 - \\lambda_{-} \\end{pmatrix} v_{-} = \\begin{pmatrix} 2 + \\sqrt{4+\\alpha^2} & \\alpha \\\\ \\alpha & -2 + \\sqrt{4+\\alpha^2} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first row, we get $(2 + \\sqrt{4+\\alpha^2})v_1 + \\alpha v_2 = 0$. An unnormalized eigenvector is thus proportional to $\\begin{pmatrix} \\alpha \\\\ -(2+\\sqrt{4+\\alpha^2}) \\end{pmatrix}$. The vector $v_{-}$ required by the problem is the unit-norm version of this vector.\n\n### Task 2: Theoretical Underpinnings of SR1 and BFGS Updates\n\nThe secant condition, $B_{k+1}s_k = y_k$, is central to quasi-Newton methods. Here, $s_k = x_{k+1} - x_k$ and $y_k = g_{k+1} - g_k$. For a quadratic function with Hessian $H$, we have $y_k = H s_k$.\n\n**BFGS Update and Positive Definiteness:**\nThe standard BFGS update formula is:\n$$\nB_{k+1} = B_k - \\frac{B_k s_k s_k^\\top B_k}{s_k^\\top B_k s_k} + \\frac{y_k y_k^\\top}{y_k^\\top s_k}\n$$\nA key property of this update is that if $B_k$ is positive definite and the curvature condition $y_k^\\top s_k > 0$ holds, then $B_{k+1}$ is also positive definite. The curvature condition $y_k^\\top s_k = s_k^\\top H s_k$ checks if the function's curvature along the direction $s_k$ is positive. Near a saddle point, it is possible to choose a step $s_k$ such that $s_k^\\top H s_k \\le 0$. This corresponds to moving along a direction of non-positive curvature. As specified, optimization algorithms using BFGS often enforce positive definiteness by skipping the update when $y_k^\\top s_k \\le 0$. If we start with $B_0 = I$ (which is positive definite) and only perform updates when $y_k^\\top s_k > 0$, the entire sequence of Hessian approximations $\\{B_k\\}$ remains positive definite. A positive definite matrix can only have positive eigenvalues. Consequently, such a strategy prevents the Hessian approximation from ever capturing the negative curvature (i.e., negative eigenvalues) of the true Hessian $H$ near a saddle point.\n\n**SR1 Update and Indefiniteness:**\nThe SR1 update formula is:\n$$\nB_{k+1} = B_k + \\frac{(y_k - B_k s_k)(y_k - B_k s_k)^\\top}{(y_k - B_k s_k)^\\top s_k}\n$$\nUnlike BFGS, the denominator $(y_k - B_k s_k)^\\top s_k$ in the SR1 formula can be positive, negative, or zero. If the denominator is negative, the rank-one term subtracted from $B_k$ can introduce negative eigenvalues, allowing $B_{k+1}$ to become indefinite. This is crucial for modeling saddle points.\n\nConsider the specific step proposed in the problem: $s = h v_{-}$, where $v_{-}$ is the negative-curvature eigendirection. For this step, $y = H s = H(h v_{-}) = h (H v_{-}) = h (\\lambda_{-} v_{-}) = \\lambda_{-} s$.\nLet's analyze the SR1 update starting from $B_0 = I$:\nThe numerator term is $(y - B_0 s)(y - B_0 s)^\\top = (\\lambda_{-} s - s)(\\lambda_{-} s - s)^\\top = (\\lambda_{-}-1)^2 s s^\\top$.\nThe denominator is $(y - B_0 s)^\\top s = (\\lambda_{-} s - s)^\\top s = (\\lambda_{-} - 1)s^\\top s = (\\lambda_{-} - 1)\\|s\\|^2$.\nSince $\\lambda_{-} = -\\sqrt{4+\\alpha^2} \\le -2$, the denominator is strictly negative.\nThe update is:\n$$\nB_{\\mathrm{SR1}} = I + \\frac{(\\lambda_{-}-1)^2 s s^\\top}{(\\lambda_{-} - 1)\\|s\\|^2} = I + \\frac{\\lambda_{-}-1}{\\|s\\|^2} s s^\\top\n$$\nLet's test the action of $B_{\\mathrm{SR1}}$ on the step vector $s$:\n$$\nB_{\\mathrm{SR1}} s = \\left( I + \\frac{\\lambda_{-}-1}{\\|s\\|^2} s s^\\top \\right) s = s + \\frac{\\lambda_{-}-1}{\\|s\\|^2} s (s^\\top s) = s + (\\lambda_{-} - 1)s = \\lambda_{-} s\n$$\nThis shows that $s$ (and thus $v_{-}$) is an eigenvector of the updated matrix $B_{\\mathrm{SR1}}$ with the eigenvalue $\\lambda_{-}$. Since $\\lambda_{-}$ is negative, the SR1 update has successfully incorporated the negative curvature of the true Hessian $H$ into its approximation. Any vector $w$ orthogonal to $s$ is an eigenvector with eigenvalue $1$, since $s^\\top w=0$ implies $B_{\\mathrm{SR1}}w = (I + \\dots)w = Iw = w$. Thus, $B_{\\mathrm{SR1}}$ has eigenvalues $\\lambda_{-}$ and $1$, correctly capturing the negative curvature.\n\n### Tasks 3-5: Implementation and Evaluation\n\nThe logic described above is implemented for the three specified test cases. For each case, we construct $H$, find its negative eigendirection $v_{-}$, and compute the step $s$ and gradient difference $y$. We then apply the SR1 and BFGS update rules.\n\nFor the BFGS update, the crucial quantity is the curvature $s^\\top y$. In our setup, $s^\\top y = s^\\top (\\lambda_{-} s) = \\lambda_{-} \\|s\\|^2$. Since $\\lambda_{-} < 0$ and $\\|s\\|^2 > 0$, the curvature is always negative. Therefore, the BFGS update is skipped in all test cases, and $B_{\\mathrm{BFGS}}$ remains the identity matrix $I$, which has no negative eigenvalues.\n\nFor the SR1 update, as shown analytically, the resulting matrix $B_{\\mathrm{SR1}}$ will have $v_{-}$ as an eigenvector with eigenvalue $\\lambda_{-}$. This means $B_{\\mathrm{SR1}}$ will have a negative eigenvalue, and its corresponding eigenvector will be perfectly aligned with the true negative-curvature direction $v_{-}$.\n\nConsequently, for all test cases, the condition for the boolean result will be met:\n1. $B_{\\mathrm{SR1}}$ has a negative smallest eigenvalue ($\\lambda_{-}$).\n2. The corresponding eigenvector $v_{B,SR1}$ is $v_{-}$, so the absolute cosine $|\\langle v_{-}, v_{B,SR1} \\rangle|$ is $1$, which is $\\ge 0.9$.\n3. $B_{\\mathrm{BFGS}}$ remains $I$ and has no negative eigenvalues.\n\nThe final boolean result will be true for all three test cases. The provided Python code executes this procedure numerically.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a single-step comparison of SR1 and BFGS updates near a saddle point\n    for a given quadratic function.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, x0_tuple)\n        (0.0, (0.2, -0.05)),\n        (1.0, (-0.1, 0.15)),\n        (3.0, (0.05, 0.06)),\n    ]\n\n    results = []\n    \n    # Parameters for the experiment\n    h = 0.1\n    alignment_threshold = 0.9\n\n    for alpha, x0_tuple in test_cases:\n        x0 = np.array(x0_tuple) # x0 is not used in the calculations as per problem logic\n        \n        # 1. Construct H and find its negative-curvature eigendirection\n        H = np.array([\n            [2.0, alpha],\n            [alpha, -2.0]\n        ])\n        \n        # np.linalg.eigh returns eigenvalues in ascending order for symmetric matrices\n        eigenvalues, eigenvectors = np.linalg.eigh(H)\n        lambda_minus = eigenvalues[0]\n        v_minus = eigenvectors[:, 0]\n\n        # 3. Form the step s and gradient difference y\n        B0 = np.eye(2)\n        s = h * v_minus\n        \n        # For a quadratic function f(x) = 1/2 * x^T*H*x, y = g(x1)-g(x0) = Hx1-Hx0 = H(x1-x0) = Hs\n        y = H @ s\n        \n        # --- SR1 Update ---\n        y_minus_B0s = y - B0 @ s\n        den_sr1 = (y_minus_B0s).T @ s\n        # As shown in the analysis, den_sr1 is non-zero for this problem setup.\n        B_sr1 = B0 + np.outer(y_minus_B0s, y_minus_B0s) / den_sr1\n\n        # --- BFGS Update (with curvature check) ---\n        sTy = s.T @ y\n        if sTy > 0:\n            # This branch is not taken in this specific problem, as s^T*y will be negative.\n            B0s = B0 @ s\n            sTB0s = s.T @ B0s\n            B_bfgs = B0 - np.outer(B0s, B0s) / sTB0s + np.outer(y, y) / sTy\n        else:\n            # Curvature condition s^T*y <= 0 is met, so we skip the BFGS update.\n            B_bfgs = B0\n\n            \n        # 4. Analyze the resulting Hessian approximations\n        \n        # SR1 Analysis\n        eigvals_sr1, eigvecs_sr1 = np.linalg.eigh(B_sr1)\n        sr1_min_eigval = eigvals_sr1[0]\n        v_B_sr1 = eigvecs_sr1[:, 0]\n        \n        sr1_has_neg_curv = sr1_min_eigval < 0\n        # The dot product of two unit vectors is the cosine of the angle between them.\n        sr1_is_aligned = abs(v_minus.T @ v_B_sr1) >= alignment_threshold\n        sr1_condition_met = sr1_has_neg_curv and sr1_is_aligned\n\n        # BFGS Analysis\n        eigvals_bfgs, _ = np.linalg.eigh(B_bfgs)\n        bfgs_min_eigval = eigvals_bfgs[0]\n        bfgs_has_neg_curv = bfgs_min_eigval < 0\n\n        # 5. Determine the final boolean result for the test case\n        # The result is True if SR1 succeeds and BFGS fails to capture negative curvature.\n        final_result = sr1_condition_met and not bfgs_has_neg_curv\n        results.append(final_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In practical optimization, no single method is universally superior; the best approach often combines the strengths of multiple techniques. This final practice challenges you to build a sophisticated hybrid algorithm that leverages the robustness of BFGS in convex regions and the agility of SR1 when non-convexity is detected. By implementing this switching strategy, you will synthesize your understanding of both methods to create a powerful and practical optimization tool .",
            "id": "3184218",
            "problem": "Consider the unconstrained minimization of a continuously differentiable function $f:\\mathbb{R}^n\\to\\mathbb{R}$. Let $x_k\\in\\mathbb{R}^n$ denote the iterate at step $k$, $g_k=\\nabla f(x_k)$ the gradient, and $H_k\\in\\mathbb{R}^{n\\times n}$ an approximation to the inverse Hessian matrix. A quasi-Newton method computes a search direction $p_k=-H_k g_k$ and updates $H_k$ using information from the step $s_k=x_{k+1}-x_k$ and the gradient change $y_k=g_{k+1}-g_k$.\n\nProblem: Implement a multi-stage quasi-Newton algorithm that begins with the Broyden-Fletcher-Goldfarb-Shanno (BFGS) update and, upon detecting nonconvexity through negative curvature, permanently switches to the Symmetric Rank-One (SR1) update. The algorithm must adhere to the following specification derived from core definitions and well-tested optimization facts:\n\n- Start with $H_0=I$.\n- At iteration $k$, compute $p_k=-H_k g_k$. If $g_k^\\top p_k\\ge 0$, replace $p_k$ by the steepest descent direction $p_k=-g_k$ to enforce a descent step.\n- Use a backtracking Armijo line search to find a step length $\\alpha_k$: initialize with $\\alpha_k=1$, and reduce $\\alpha_k$ by a constant factor until the Armijo condition $f(x_k+\\alpha_k p_k)\\le f(x_k)+c_1\\alpha_k g_k^\\top p_k$ is satisfied. Use $c_1=10^{-4}$ and a shrink factor of $1/2$. All trigonometric function arguments are angles in radians.\n- Define $s_k=x_{k+1}-x_k$ and $y_k=g_{k+1}-g_k$ after the line search is accepted.\n- Negative curvature detection and stage switching rule: while in BFGS stage, if the curvature condition $s_k^\\top y_k>\\theta$ holds, continue with BFGS; otherwise, detect negative curvature and permanently switch to SR1 updates from the next update onward. Use the threshold $\\theta=10^{-10}$.\n- In SR1 stage, apply SR1 updates whenever the update is well-defined; if the SR1 denominator is too small in magnitude, skip the update.\n- Terminate when $\\lVert g_k\\rVert_2\\le\\varepsilon$ with $\\varepsilon=10^{-6}$ or when the iteration count reaches the maximum of $200$.\n\nYou must test the implementation on the following set of objective functions, which includes convex and nonconvex cases:\n\n$1.$ Convex quadratic: $f(x)=\\tfrac{1}{2}x^\\top A x + b^\\top x$ with $A=\\begin{bmatrix}4 & 1\\\\ 1 & 3\\end{bmatrix}$ and $b=\\begin{bmatrix}-1\\\\ 2\\end{bmatrix}$, starting point $x_0=\\begin{bmatrix}-2\\\\ 2\\end{bmatrix}$.\n\n$2.$ Rosenbrock function: $f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2$, starting point $x_0=\\begin{bmatrix}-1.2\\\\ 1\\end{bmatrix}$.\n\n$3.$ Saddle-quartic (nonconvex but bounded below): $f(x_1,x_2)=x_1^2-x_2^2+0.1(x_1^4+x_2^4)$, starting point $x_0=\\begin{bmatrix}0\\\\ 0.2\\end{bmatrix}$.\n\n$4.$ Quadratic plus trigonometric perturbation (mixed curvature): $f(x)=\\tfrac{1}{2}x^\\top A x+0.2\\sin(3x_1)+0.2\\cos(2x_2)$ with $A=\\begin{bmatrix}2 & 0\\\\ 0 & 1\\end{bmatrix}$, starting point $x_0=\\begin{bmatrix}1.5\\\\ -1\\end{bmatrix}$.\n\nFor each test case, your program must return three values: the final objective value $f(x_{\\text{final}})$ rounded to six decimal places, the number of iterations (an integer), and a boolean flag indicating whether the algorithm switched to SR1 at least once. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result itself formatted as a list, for example $[ [f_1,i_1,\\text{flag}_1],[f_2,i_2,\\text{flag}_2],\\dots ]$. The boolean flag must be printed in lowercase as either `true` or `false`. No other output is permitted.\n\nTest Suite Summary:\n- Case $1$: Convex quadratic with $A=\\begin{bmatrix}4 & 1\\\\ 1 & 3\\end{bmatrix}$, $b=\\begin{bmatrix}-1\\\\ 2\\end{bmatrix}$, $x_0=\\begin{bmatrix}-2\\\\ 2\\end{bmatrix}$.\n- Case $2$: Rosenbrock with $x_0=\\begin{bmatrix}-1.2\\\\ 1\\end{bmatrix}$.\n- Case $3$: Saddle-quartic with $x_0=\\begin{bmatrix}0\\\\ 0.2\\end{bmatrix}$.\n- Case $4$: Quadratic plus trigonometric with $A=\\begin{bmatrix}2 & 0\\\\ 0 & 1\\end{bmatrix}$, $x_0=\\begin{bmatrix}1.5\\\\ -1\\end{bmatrix}$.\n\nAll functions are purely mathematical; no physical units are involved. Angles for trigonometric functions are in radians. The final output format must strictly follow the single-line specification above.",
            "solution": "The user has provided a well-defined problem in numerical optimization. The task is to implement a hybrid quasi-Newton algorithm that starts with the Broyden-Fletcher-Goldfarb-Shanno (BFGS) update for the inverse Hessian approximation and permanently switches to the Symmetric Rank-One (SR1) update upon detection of non-positive curvature. This response will detail the validation of the problem statement and present a full solution and implementation.\n\n**Problem Validation**\n\nThe problem is subjected to rigorous validation against the specified criteria.\n\n-   **Step 1: Extracted Givens**\n    -   **Algorithm Type**: Unconstrained minimization using a multi-stage quasi-Newton method.\n    -   **Objective Function**: $f:\\mathbb{R}^n\\to\\mathbb{R}$, continuously differentiable.\n    -   **Initial State**: Iterate $x_0$, inverse Hessian approximation $H_0=I$ (identity matrix).\n    -   **Iterative Process**:\n        -   Gradient: $g_k = \\nabla f(x_k)$.\n        -   Search Direction: $p_k = -H_k g_k$.\n        -   Descent Safeguard: If $g_k^\\top p_k \\ge 0$, set $p_k = -g_k$.\n        -   Line Search: Backtracking Armijo with condition $f(x_k+\\alpha_k p_k)\\le f(x_k)+c_1\\alpha_k g_k^\\top p_k$.\n        -   Line Search Parameters: Initial $\\alpha_k=1$, $c_1=10^{-4}$, shrink factor $1/2$.\n        -   Updates: $s_k=x_{k+1}-x_k = \\alpha_k p_k$, $y_k=g_{k+1}-g_k = \\nabla f(x_{k+1})-\\nabla f(x_k)$.\n    -   **Hybrid Strategy**:\n        -   Starts in BFGS mode.\n        -   Curvature Condition: $s_k^\\top y_k > \\theta$ with $\\theta=10^{-10}$.\n        -   Switching Rule: If $s_k^\\top y_k \\le \\theta$, permanently switch to SR1 mode.\n    -   **Quasi-Newton Updates**:\n        -   BFGS (if in BFGS mode and curvature condition holds).\n        -   SR1 (if in SR1 mode), with a skip condition if the denominator is \"too small in magnitude\".\n    -   **Termination**: $\\lVert g_k\\rVert_2 \\le \\varepsilon$ with $\\varepsilon=10^{-6}$, or maximum iterations $k_{\\text{max}} = 200$.\n    -   **Test Suite**: Four specified objective functions with their respective starting points.\n    -   **Output Requirement**: For each test case, return $[f(x_{\\text{final}}), \\text{iterations}, \\text{SR1 used flag}]$. The final objective value is rounded to six decimal places.\n\n-   **Step 2: Validation Using Extracted Givens**\n    -   **Scientific Grounding**: The problem is well-grounded in the theory of numerical optimization. BFGS and SR1 are canonical quasi-Newton methods. The condition $s_k^\\top y_k > 0$ is essential for BFGS to maintain positive definiteness. Using its violation as a trigger to switch to the more general SR1 method (which does not require positive definiteness) is a standard and sound heuristic for handling non-convexity. The Armijo line search and descent direction safeguards are also standard components. The test functions are classical benchmarks in the field.\n    -   **Well-Posedness**: The problem is well-posed. The algorithm is clearly specified, and for the given smooth functions, it is expected to generate a sequence of iterates that converges to a stationary point. The termination criteria ensure the algorithm halts. The only minor ambiguity (\"denominator is too small in magnitude\" for SR1) is a standard implementation detail that can be resolved with a reasonable threshold (e.g., $|(s_k - H_k y_k)^\\top y_k| < 10^{-8}$), making the problem fully formalizable.\n    -   **Objectivity**: The problem is stated in precise, objective mathematical language.\n    -   **Conclusion**: No flaws are identified. The problem is valid.\n\n-   **Step 3: Verdict and Action**\n    The problem is valid. A complete solution will be provided.\n\n**Solution Methodology**\n\nThe specified hybrid quasi-Newton algorithm is implemented to solve the unconstrained optimization problem $\\min_{x \\in \\mathbb{R}^n} f(x)$. The method generates a sequence of iterates $\\{x_k\\}$ via the update rule $x_{k+1} = x_k + s_k$, where $s_k = \\alpha_k p_k$ is the step. Here, $p_k$ is the search direction and $\\alpha_k$ is the step length.\n\n1.  **Initialization**: The algorithm starts at a given point $x_0$. The iteration counter is set to $k=0$. The initial approximation to the inverse Hessian matrix is the $n \\times n$ identity matrix, $H_0 = I$. A flag tracks the algorithm's mode, initialized to `BFGS`.\n\n2.  **Main Iteration Loop**: At each iteration $k = 0, 1, 2, \\dots$:\n    a. **Termination Check**: Compute the gradient $g_k = \\nabla f(x_k)$. If its Euclidean norm satisfies $\\lVert g_k \\rVert_2 \\le \\varepsilon = 10^{-6}$, the algorithm terminates, as a stationary point has been found with sufficient accuracy. The loop also terminates if $k$ reaches the maximum of $200$.\n\n    b. **Search Direction**: The search direction is computed using the current inverse Hessian approximation: $p_k = -H_k g_k$. To guarantee that $p_k$ is a descent direction (i.e., $g_k^\\top p_k < 0$), a check is performed. If $g_k^\\top p_k \\ge 0$, which can occur if $H_k$ is not positive definite, the direction is reset to the steepest descent direction, $p_k = -g_k$.\n\n    c. **Line Search**: The step length $\\alpha_k$ is determined by a backtracking line search to satisfy the Armijo condition:\n    $$f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top p_k$$\n    The search starts with a trial step length $\\alpha_k=1$. If the condition is not met, $\\alpha_k$ is repeatedly multiplied by a shrink factor of $1/2$ until the condition is satisfied. The parameter $c_1$ is set to $10^{-4}$.\n\n    d. **Update Iterate**: The new iterate is computed as $x_{k+1} = x_k + \\alpha_k p_k$. The step vector is $s_k = x_{k+1} - x_k$, and the gradient difference vector is $y_k = g_{k+1} - g_k$, where $g_{k+1} = \\nabla f(x_{k+1})$.\n\n    e. **Inverse Hessian Update**: The core of the hybrid strategy lies in how $H_k$ is updated to $H_{k+1}$.\n        -   **BFGS Mode**: If the algorithm is in the BFGS mode, the curvature is checked. If $s_k^\\top y_k > \\theta = 10^{-10}$, the curvature is sufficiently positive, and the BFGS update is applied:\n            $$H_{k+1} = \\left(I - \\rho_k s_k y_k^\\top\\right) H_k \\left(I - \\rho_k y_k s_k^\\top\\right) + \\rho_k s_k s_k^\\top, \\quad \\text{where } \\rho_k = \\frac{1}{s_k^\\top y_k}$$\n            If the curvature condition is not met ($s_k^\\top y_k \\le \\theta$), it indicates non-positive curvature. The algorithm permanently switches to SR1 mode. This switch is flagged.\n        -   **SR1 Mode**: Once in SR1 mode (either by switching in the current step or from a previous one), the SR1 update is attempted. The update formula is:\n            $$H_{k+1} = H_k + \\frac{(s_k - H_k y_k)(s_k - H_k y_k)^\\top}{(s_k - H_k y_k)^\\top y_k}$$\n            This update is only performed if the denominator is not close to zero. A practical check is implemented: if $|(s_k - H_k y_k)^\\top y_k| \\le 10^{-8}$, the update is skipped, and we set $H_{k+1} = H_k$.\n\n    f. **Advance**: Increment the iteration counter, $k \\leftarrow k+1$, and proceed to the next iteration.\n\nThis procedure is applied to each of the four test functions provided in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hybrid_quasi_newton(func, grad, x0, A=None, b=None):\n    \"\"\"\n    Implements a hybrid BFGS/SR1 quasi-Newton optimization algorithm.\n\n    Args:\n        func (callable): The objective function f(x, A, b).\n        grad (callable): The gradient of the objective function grad(x, A, b).\n        x0 (np.ndarray): The starting point.\n        A (np.ndarray, optional): Matrix parameter for the function.\n        b (np.ndarray, optional): Vector parameter for the function.\n\n    Returns:\n        tuple: A tuple containing:\n            - float: The final objective function value, rounded to 6 decimal places.\n            - int: The total number of iterations.\n            - bool: True if the algorithm switched to SR1, False otherwise.\n    \"\"\"\n    # Parameters from problem statement\n    c1 = 1e-4\n    shrink_factor = 0.5\n    grad_tol = 1e-6\n    max_iter = 200\n    theta = 1e-10\n    sr1_denom_tol = 1e-8\n\n    n = len(x0)\n    x = np.copy(x0).astype(float)\n    H = np.eye(n)\n    \n    k = 0\n    switched_to_sr1 = False\n    mode = 'bfgs'\n\n    while k < max_iter:\n        g = grad(x, A, b)\n\n        # Termination condition on gradient norm\n        if np.linalg.norm(g) <= grad_tol:\n            break\n\n        # Compute search direction\n        p = -H @ g\n\n        # Ensure search direction is a descent direction\n        if g.T @ p >= 0:\n            p = -g\n        \n        # Backtracking Armijo line search\n        alpha = 1.0\n        fk = func(x, A, b)\n        g_dot_p = g.T @ p\n        \n        # The line search loop\n        while func(x + alpha * p, A, b) > fk + c1 * alpha * g_dot_p:\n            alpha *= shrink_factor\n        \n        # Update iterate and compute step and gradient difference\n        s = alpha * p\n        x_next = x + s\n        g_next = grad(x_next, A, b)\n        y = g_next - g\n\n        s_dot_y = s.T @ y\n\n        # Mode-dependent update of the inverse Hessian approximation\n        if mode == 'bfgs':\n            if s_dot_y > theta:\n                # Apply BFGS update\n                rho = 1.0 / s_dot_y\n                I = np.eye(n)\n                term1 = I - rho * np.outer(s, y)\n                H = term1 @ H @ term1.T + rho * np.outer(s, s)\n            else:\n                # Switch to SR1 and fall through to apply SR1 in this iteration\n                mode = 'sr1'\n                switched_to_sr1 = True\n\n        if mode == 'sr1':\n            v = s - H @ y\n            denom = v.T @ y\n            if abs(denom) > sr1_denom_tol:\n                # Apply SR1 update\n                H = H + np.outer(v, v) / denom\n            # Else: skip the update, H_{k+1} = H_k\n        \n        # Prepare for the next iteration\n        x = x_next\n        k += 1\n\n    final_f = func(x, A, b)\n    return round(final_f, 6), k, switched_to_sr1\n\n# --- Test Case Definitions ---\n\n# 1. Convex quadratic\ndef func1(x, A, b):\n    return 0.5 * x.T @ A @ x + b.T @ x\ndef grad1(x, A, b):\n    return A @ x + b\n\n# 2. Rosenbrock function\ndef func2(x, A, b): # A, b are placeholders\n    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\ndef grad2(x, A, b): # A, b are placeholders\n    return np.array([\n        -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2),\n        200 * (x[1] - x[0]**2)\n    ])\n\n# 3. Saddle-quartic function\ndef func3(x, A, b): # A, b are placeholders\n    return x[0]**2 - x[1]**2 + 0.1 * (x[0]**4 + x[1]**4)\ndef grad3(x, A, b): # A, b are placeholders\n    return np.array([\n        2 * x[0] + 0.4 * x[0]**3,\n        -2 * x[1] + 0.4 * x[1]**3\n    ])\n\n# 4. Quadratic plus trigonometric perturbation\ndef func4(x, A, b): # b is a placeholder\n    return 0.5 * x.T @ A @ x + 0.2 * np.sin(3 * x[0]) + 0.2 * np.cos(2 * x[1])\ndef grad4(x, A, b): # b is a placeholder\n    return A @ x + np.array([\n        0.6 * np.cos(3 * x[0]),\n        -0.4 * np.sin(2 * x[1])\n    ])\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"func\": func1, \"grad\": grad1, \"x0\": np.array([-2.0, 2.0]), \n            \"A\": np.array([[4.0, 1.0], [1.0, 3.0]]), \"b\": np.array([-1.0, 2.0])\n        },\n        {\n            \"func\": func2, \"grad\": grad2, \"x0\": np.array([-1.2, 1.0]),\n            \"A\": None, \"b\": None\n        },\n        {\n            \"func\": func3, \"grad\": grad3, \"x0\": np.array([0.0, 0.2]),\n            \"A\": None, \"b\": None\n        },\n        {\n            \"func\": func4, \"grad\": grad4, \"x0\": np.array([1.5, -1.0]),\n            \"A\": np.array([[2.0, 0.0], [0.0, 1.0]]), \"b\": None\n        }\n    ]\n    \n    results = []\n    for case in test_cases:\n        f_val, iters, flag = hybrid_quasi_newton(\n            case[\"func\"], case[\"grad\"], case[\"x0\"], A=case[\"A\"], b=case[\"b\"]\n        )\n        # Format boolean as required ('true' or 'false')\n        flag_str = str(flag).lower()\n        results.append(f\"[{f_val:.6f},{iters},{flag_str}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}