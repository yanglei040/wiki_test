{
    "hands_on_practices": [
        {
            "introduction": "SR1 更新方法的一个关键优势在于它能够处理非凸问题，这主要归功于其可以生成不定海森矩阵（Hessian）近似。本练习旨在通过一个涉及鞍点的具体例子，直观地对比 SR1 和 BFGS 方法的行为 。你将通过实践编码发现，SR1 如何成功捕捉负曲率方向，从而为逃离鞍点提供有效信息，而标准的 BFGS 方法在此情况下则会受限。",
            "id": "3184299",
            "problem": "考虑为 $x \\in \\mathbb{R}^2$ 定义的二次目标函数\n$$\nf(x) = x_1^2 - x_2^2 + \\alpha x_1 x_2,\n$$\n其中 $\\alpha \\in \\mathbb{R}$ 是一个参数。令 $H \\in \\mathbb{R}^{2 \\times 2}$ 表示 $f$ 的对称Hessian矩阵，令 $g(x) = \\nabla f(x)$ 表示梯度。点 $x = 0$ 是一个鞍点。在拟牛顿法的背景下，两种标准的海森矩阵近似更新方法是对称秩一（Symmetric Rank-One, SR1）和Broyden-Fletcher-Goldfarb-Shanno（BFGS）。SR1更新允许不定近似，而BFGS更新通常与一个强制正定性的曲率条件一起使用。\n\n从以下基本原理出发：\n- 对于形式为 $f(x) = \\tfrac{1}{2} x^\\top H x$ 且 $H$ 对称的二次连续可微目标函数，其梯度为 $g(x) = H x$，Hessian矩阵为常数矩阵 $H$。\n- 一个二次可微函数的鞍点在局部由一个不定Hessian矩阵来表征，这意味着 $H$ 同时具有正特征值和负特征值。\n- 拟牛顿更新通过强制执行割线条件 $B_{k+1} s_k \\approx y_k$ 来构造一系列对称矩阵 $\\{B_k\\}$，以逼近真实的Hessian矩阵 $H$，其中 $s_k = x_{k+1} - x_k$ 且 $y_k = g(x_{k+1}) - g(x_k)$。\n\n任务：\n1. 从上述基本原理出发，为给定的 $f(x)$ 构建 $H$ 和 $g(x)$，并确定 $H$ 的特征值和特征向量。利用这些来确定与 $H$ 的最小（最负）特征值 $\\lambda_{-}$ 相关联的负曲率特征方向 $v_{-}$。\n2. 使用第一性原理和割线条件解释，为什么在近似Hessian矩阵中强制正定性可能会抑制在鞍点附近检测到负曲率，以及为什么允许不定性（如SR1所做的）可以将负曲率方向编码到近似中。\n3. 按如下方式实现一个单步更新比较实验：\n   - 用单位矩阵 $B_0 = I$ 初始化近似Hessian矩阵。\n   - 对于给定的 $\\alpha$ 和给定的起始点 $x^{(0)} \\in \\mathbb{R}^2$，构造一个小步长 $s = h \\, v_{-}$，其中 $h = 0.1$，$v_{-}$ 是 $H$ 对应于 $\\lambda_{-}$ 的单位特征向量。设 $x^{(1)} = x^{(0)} + s$ 并计算 $y = g(x^{(1)}) - g(x^{(0)})$。对于二次函数 $f$，这可以简化为 $y = H s$。\n   - 应用SR1更新，从 $B_0$ 和数据对 $(s,y)$ 获得 $B_{\\mathrm{SR1}}$。\n   - 应用BFGS更新，从 $B_0$ 和数据对 $(s,y)$ 获得 $B_{\\mathrm{BFGS}}$，但通过在 $s^\\top y \\le 0$ 时完全跳过更新来强制执行标准曲率条件，因此在这种情况下 $B_{\\mathrm{BFGS}} = B_0$。\n   - 对 $\\{B_{\\mathrm{SR1}}, B_{\\mathrm{BFGS}}\\}$ 中的每个 $B$，计算其特征分解，并提取其最小特征值及对应的单位特征向量。确定 $B$ 是否捕捉到了负曲率（即，是否具有负的最小特征值），以及其最小特征向量是否与真实的负曲率特征方向 $v_{-}$ 对齐，对齐程度由单位向量间的绝对余弦值 $|\\langle v_{-}, v_B \\rangle|$ 量化。使用 $0.9$ 的阈值来判断是否为强对齐。\n4. 使用上述过程，评估以下参数设置的测试套件：\n   - 测试用例1：$\\alpha = 0$, $x^{(0)} = (0.2, -0.05)$。\n   - 测试用例2：$\\alpha = 1$, $x^{(0)} = (-0.1, 0.15)$。\n   - 测试用例3：$\\alpha = 3$, $x^{(0)} = (0.05, 0.06)$。\n   在所有情况下，构造 $s$ 时使用的标量步长均为 $h = 0.1$。\n5. 对每个测试用例，生成一个布尔结果。当且仅当 $B_{\\mathrm{SR1}}$ 同时满足 (i) 具有负的最小特征值，且 (ii) 其最小特征向量与 $v_{-}$ 的绝对余弦值至少为 $0.9$，并且同时 $B_{\\mathrm{BFGS}}$ 没有任何负特征值时，该结果为真。这个布尔值旨在量化SR1允许不定性在编码鞍点附近正确逃逸方向方面的优势。\n   \n最终输出格式：\n- 你的程序应生成单行输出，其中包含按顺序排列的三个测试用例的结果，形式为方括号内以逗号分隔的列表，例如：$[{\\tt result1},{\\tt result2},{\\tt result3}]$，其中每个 ${\\tt resulti}$ 为 ${\\tt True}$ 或 ${\\tt False}$。\n- 不涉及物理单位；所有量均为无量纲实数。",
            "solution": "问题的核心是比较对称秩一（SR1）和Broyden-Fletcher-Goldfarb-Shanno（BFGS）拟牛顿更新在鞍点附近的行为。分析的关键在于，当沿着负曲率方向迈出一步时，每种方法的海森矩阵近似 $B$ 是如何演化的。\n\n### 任务1：系统表征（梯度、Hessian矩阵和特征分解）\n\n给定的目标函数为 $f(x) = x_1^2 - x_2^2 + \\alpha x_1 x_2$，其中 $x = (x_1, x_2)^\\top \\in \\mathbb{R}^2$。\n\n首先，我们计算梯度向量 $g(x) = \\nabla f(x)$：\n$$\ng(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 + \\alpha x_2 \\\\ -2x_2 + \\alpha x_1 \\end{pmatrix}\n$$\n这可以写成矩阵形式 $g(x) = Hx$，其中矩阵 $H$ 为：\n$$\nH = \\begin{pmatrix} 2  \\alpha \\\\ \\alpha  -2 \\end{pmatrix}\n$$\n$f(x)$ 的Hessian矩阵是二阶偏导数矩阵 $\\nabla^2 f(x)$，它确实是常数对称矩阵 $H$。问题指出，对于形式为 $f(x) = \\frac{1}{2} x^\\top H x$ 的二次函数，其梯度为 $g(x) = Hx$。我们的函数满足此结构。\n\n接下来，我们通过求解特征方程 $\\det(H - \\lambda I) = 0$ 来求 $H$ 的特征值：\n$$\n\\det \\begin{pmatrix} 2-\\lambda  \\alpha \\\\ \\alpha  -2-\\lambda \\end{pmatrix} = (2-\\lambda)(-2-\\lambda) - \\alpha^2 = -(4-\\lambda^2) - \\alpha^2 = \\lambda^2 - 4 - \\alpha^2 = 0\n$$\n这得到 $\\lambda^2 = 4 + \\alpha^2$，所以特征值为 $\\lambda = \\pm\\sqrt{4 + \\alpha^2}$。\n因为对于任何实数 $\\alpha$，$\\sqrt{4+\\alpha^2} > 0$，所以 $H$ 有一个正特征值 $\\lambda_+ = \\sqrt{4+\\alpha^2}$ 和一个负特征值 $\\lambda_- = -\\sqrt{4+\\alpha^2}$。这证实了Hessian矩阵是不定的，并且原点 $x=0$（其中 $g(0)=0$）是一个鞍点。\n\n负曲率特征方向 $v_{-}$ 是对应于最小（最负）特征值 $\\lambda_{-}$ 的特征向量。我们求解方程组 $(H - \\lambda_{-}I)v_{-} = 0$：\n$$\n\\begin{pmatrix} 2 - \\lambda_{-}  \\alpha \\\\ \\alpha  -2 - \\lambda_{-} \\end{pmatrix} v_{-} = \\begin{pmatrix} 2 + \\sqrt{4+\\alpha^2}  \\alpha \\\\ \\alpha  -2 + \\sqrt{4+\\alpha^2} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n从第一行，我们得到 $(2 + \\sqrt{4+\\alpha^2})v_1 + \\alpha v_2 = 0$。因此，一个未归一化的特征向量与 $\\begin{pmatrix} \\alpha \\\\ -(2+\\sqrt{4+\\alpha^2}) \\end{pmatrix}$ 成比例。问题所要求的向量 $v_{-}$ 是该向量的单位范数版本。\n\n### 任务2：SR1和BFGS更新的理论基础\n\n割线条件 $B_{k+1}s_k = y_k$ 是拟牛顿法的核心。这里，$s_k = x_{k+1} - x_k$ 且 $y_k = g_{k+1} - g_k$。对于具有Hessian矩阵 $H$ 的二次函数，我们有 $y_k = H s_k$。\n\n**BFGS更新与正定性：**\n标准的BFGS更新公式为：\n$$\nB_{k+1} = B_k - \\frac{B_k s_k s_k^\\top B_k}{s_k^\\top B_k s_k} + \\frac{y_k y_k^\\top}{y_k^\\top s_k}\n$$\n此更新的一个关键性质是，如果 $B_k$ 是正定的并且曲率条件 $y_k^\\top s_k > 0$ 成立，那么 $B_{k+1}$ 也是正定的。曲率条件 $y_k^\\top s_k = s_k^\\top H s_k$ 检查函数沿方向 $s_k$ 的曲率是否为正。在鞍点附近，可以选择一个步长 $s_k$ 使得 $s_k^\\top H s_k \\le 0$。这对应于沿着非正曲率的方向移动。如题目所规定，使用BFGS的优化算法通常通过在 $y_k^\\top s_k \\le 0$ 时跳过更新来强制正定性。如果我们从 $B_0 = I$（它是正定的）开始，并且仅在 $y_k^\\top s_k > 0$ 时执行更新，那么整个Hessian近似序列 $\\{B_k\\}$ 都将保持正定。一个正定矩阵只能有正特征值。因此，这种策略阻止了Hessian近似在鞍点附近捕捉到真实Hessian矩阵 $H$ 的负曲率（即负特征值）。\n\n**SR1更新与不定性：**\nSR1更新公式为：\n$$\nB_{k+1} = B_k + \\frac{(y_k - B_k s_k)(y_k - B_k s_k)^\\top}{(y_k - B_k s_k)^\\top s_k}\n$$\n与BFGS不同，SR1公式中的分母 $(y_k - B_k s_k)^\\top s_k$ 可以是正、负或零。如果分母为负，从 $B_k$ 中减去的秩一项可能会引入负特征值，从而允许 $B_{k+1}$ 变为不定的。这对于建模鞍点至关重要。\n\n考虑问题中提出的具体步长：$s = h v_{-}$，其中 $v_{-}$ 是负曲率特征方向。对于此步长，$y = H s = H(h v_{-}) = h (H v_{-}) = h (\\lambda_{-} v_{-}) = \\lambda_{-} s$。\n我们从 $B_0 = I$ 开始分析SR1更新：\n分子项为 $(y - B_0 s)(y - B_0 s)^\\top = (\\lambda_{-} s - s)(\\lambda_{-} s - s)^\\top = (\\lambda_{-}-1)^2 s s^\\top$。\n分母为 $(y - B_0 s)^\\top s = (\\lambda_{-} s - s)^\\top s = (\\lambda_{-} - 1)s^\\top s = (\\lambda_{-} - 1)\\|s\\|^2$。\n由于 $\\lambda_{-} = -\\sqrt{4+\\alpha^2} \\le -2$，分母是严格为负的。\n更新为：\n$$\nB_{\\mathrm{SR1}} = I + \\frac{(\\lambda_{-}-1)^2 s s^\\top}{(\\lambda_{-} - 1)\\|s\\|^2} = I + \\frac{\\lambda_{-}-1}{\\|s\\|^2} s s^\\top\n$$\n我们来测试 $B_{\\mathrm{SR1}}$ 对步长向量 $s$ 的作用：\n$$\nB_{\\mathrm{SR1}} s = \\left( I + \\frac{\\lambda_{-}-1}{\\|s\\|^2} s s^\\top \\right) s = s + \\frac{\\lambda_{-}-1}{\\|s\\|^2} s (s^\\top s) = s + (\\lambda_{-} - 1)s = \\lambda_{-} s\n$$\n这表明 $s$（并因此 $v_{-}$）是更新后矩阵 $B_{\\mathrm{SR1}}$ 的一个特征向量，其特征值为 $\\lambda_{-}$。由于 $\\lambda_{-}$ 是负的，SR1更新成功地将真实Hessian矩阵 $H$ 的负曲率纳入了其近似中。任何与 $s$ 正交的向量 $w$ 都是特征值为1的特征向量，因为 $s^\\top w=0$ 意味着 $B_{\\mathrm{SR1}}w = (I + \\dots)w = Iw = w$。因此，$B_{\\mathrm{SR1}}$ 的特征值为 $\\lambda_{-}$ 和 $1$，正确地捕捉了负曲率。\n\n### 任务3-5：实现与评估\n\n上述逻辑将针对三个指定的测试用例进行实现。对于每个用例，我们构建 $H$，找到其负特征方向 $v_{-}$，并计算步长 $s$ 和梯度差 $y$。然后我们应用SR1和BFGS更新规则。\n\n对于BFGS更新，关键量是曲率 $s^\\top y$。在我们的设置中，$s^\\top y = s^\\top (\\lambda_{-} s) = \\lambda_{-} \\|s\\|^2$。由于 $\\lambda_{-}  0$ 且 $\\|s\\|^2 > 0$，曲率总是负的。因此，在所有测试用例中，BFGS更新都会被跳过，$B_{\\mathrm{BFGS}}$ 保持为单位矩阵 $I$，它没有负特征值。\n\n对于SR1更新，如解析所示，得到的矩阵 $B_{\\mathrm{SR1}}$ 将以 $v_{-}$ 为特征向量，特征值为 $\\lambda_{-}$。这意味着 $B_{\\mathrm{SR1}}$ 将有一个负特征值，并且其对应的特征向量将与真实的负曲率方向 $v_{-}$ 完全对齐。\n\n因此，对于所有测试用例，布尔结果的条件都将得到满足：\n1. $B_{\\mathrm{SR1}}$ 有一个负的最小特征值 ($\\lambda_{-}$)。\n2. 对应的特征向量 $v_{B,SR1}$ 是 $v_{-}$，所以绝对余弦值 $|\\langle v_{-}, v_{B,SR1} \\rangle|$ 为 $1$，即 $\\ge 0.9$。\n3. $B_{\\mathrm{BFGS}}$ 保持为 $I$ 且没有负特征值。\n\n最终的布尔结果对于所有三个测试用例都将为真。提供的Python代码以数值方式执行此过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a single-step comparison of SR1 and BFGS updates near a saddle point\n    for a given quadratic function.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, x0_tuple)\n        (0.0, (0.2, -0.05)),\n        (1.0, (-0.1, 0.15)),\n        (3.0, (0.05, 0.06)),\n    ]\n\n    results = []\n    \n    # Parameters for the experiment\n    h = 0.1\n    alignment_threshold = 0.9\n\n    for alpha, x0_tuple in test_cases:\n        x0 = np.array(x0_tuple) # x0 is not used in the calculations as per problem logic\n        \n        # 1. Construct H and find its negative-curvature eigendirection\n        H = np.array([\n            [2.0, alpha],\n            [alpha, -2.0]\n        ])\n        \n        # np.linalg.eigh returns eigenvalues in ascending order for symmetric matrices\n        eigenvalues, eigenvectors = np.linalg.eigh(H)\n        lambda_minus = eigenvalues[0]\n        v_minus = eigenvectors[:, 0]\n\n        # 3. Form the step s and gradient difference y\n        B0 = np.eye(2)\n        s = h * v_minus\n        \n        # For a quadratic function f(x) = 1/2 * x^T*H*x, y = g(x1)-g(x0) = Hx1-Hx0 = H(x1-x0) = Hs\n        y = H @ s\n        \n        # --- SR1 Update ---\n        y_minus_B0s = y - B0 @ s\n        den_sr1 = (y_minus_B0s).T @ s\n        # As shown in the analysis, den_sr1 is non-zero for this problem setup.\n        B_sr1 = B0 + np.outer(y_minus_B0s, y_minus_B0s) / den_sr1\n\n        # --- BFGS Update (with curvature check) ---\n        sTy = s.T @ y\n        if sTy > 0:\n            # This branch is not taken in this specific problem, as s^T*y will be negative.\n            B0s = B0 @ s\n            sTB0s = s.T @ B0s\n            B_bfgs = B0 - np.outer(B0s, B0s) / sTB0s + np.outer(y, y) / sTy\n        else:\n            # Curvature condition s^T*y = 0 is met, so we skip the BFGS update.\n            B_bfgs = B0\n\n            \n        # 4. Analyze the resulting Hessian approximations\n        \n        # SR1 Analysis\n        eigvals_sr1, eigvecs_sr1 = np.linalg.eigh(B_sr1)\n        sr1_min_eigval = eigvals_sr1[0]\n        v_B_sr1 = eigvecs_sr1[:, 0]\n        \n        sr1_has_neg_curv = sr1_min_eigval  0\n        # The dot product of two unit vectors is the cosine of the angle between them.\n        sr1_is_aligned = abs(v_minus.T @ v_B_sr1) >= alignment_threshold\n        sr1_condition_met = sr1_has_neg_curv and sr1_is_aligned\n\n        # BFGS Analysis\n        eigvals_bfgs, _ = np.linalg.eigh(B_bfgs)\n        bfgs_min_eigval = eigvals_bfgs[0]\n        bfgs_has_neg_curv = bfgs_min_eigval  0\n\n        # 5. Determine the final boolean result for the test case\n        # The result is True if SR1 succeeds and BFGS fails to capture negative curvature.\n        final_result = sr1_condition_met and not bfgs_has_neg_curv\n        results.append(final_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "尽管 SR1 在处理非凸性方面表现出色，但它存在一个固有的数值不稳定性问题：更新公式中的分母可能接近于零。本练习将引导你探讨这种“分母失效”现象，并学习在实际应用中如何通过“跳过更新”策略来保证算法的稳健性 。通过设计和测试一些特殊构造的例子，你将深入理解 SR1 算法在实践中必须考虑的数值稳定性问题。",
            "id": "3184285",
            "problem": "考虑使用拟牛顿法（Quasi-Newton method），结合对称秩一（Symmetric Rank-One, SR1）更新和强沃尔夫（Strong Wolfe）线搜索，对一个二次连续可微函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 进行无约束最小化。令 $x_k\\in\\mathbb{R}^n$ 表示迭代点，$g_k=\\nabla f(x_k)$ 表示梯度，$B_k$ 表示海森矩阵的一个对称近似，$p_k=-B_k^{-1}g_k$ 表示搜索方向，$s_k=\\alpha_k p_k$ 表示步长，$y_k=g_{k+1}-g_k$ 表示梯度差。SR1 更新是根据其定义的割线条件 $B_{k+1}s_k\\approx y_k$ 和在某个合适范数下的最小变化原则构造的，但众所周知，当标量分母 $(y_k - B_k s_k)^\\top s_k$ 接近零时，其在数值上是不安全的；在这种情况下，实现时会跳过 SR1 更新，并保持 $B_{k+1}=B_k$。强沃尔夫（Strong Wolfe）线搜索选择一个 $\\alpha_k>0$，使其同时满足充分下降条件 $f(x_k+\\alpha_k p_k)\\le f(x_k)+c_1\\alpha_k g_k^\\top p_k$ 和曲率条件 $|\\nabla f(x_k+\\alpha_k p_k)^\\top p_k|\\le c_2| g_k^\\top p_k|$，其中 $c_1\\in(0,1)$ 和 $c_2\\in(c_1,1)$ 是固定常数。\n\n您的任务是探索、设计和测试通过使 $(y_k - B_k s_k)^\\top s_k\\approx 0$ 来导致 SR1 更新频繁跳过的序列 $(s_k,y_k)$，并研究在强沃尔夫（Strong Wolfe）线搜索下的收敛行为。您的推理和实现应仅基于以下基本组成部分：梯度的定义 $g(x)=\\nabla f(x)$、割线条件 $B_{k+1}s_k\\approx y_k$ 以及强沃尔夫（Strong Wolfe）线搜索条件。不要在问题陈述中使用或声明任何 SR1 更新的闭式表达式；应在解答中基于上述原则推导并实现它。\n\n实现一个程序，该程序：\n- 使用基于 SR1 的拟牛顿法、强沃尔夫线搜索、分母崩溃跳过准则和固定的梯度范数收敛容差，构造并最小化三个测试函数。在每个测试中，统计由分母接近零引起的 SR1 更新跳过次数。\n- 对于给定的阈值 $\\delta>0$，使用跳过准则 $|\\left(y_k - B_k s_k\\right)^\\top s_k| \\le \\delta \\lVert s_k\\rVert \\,\\lVert y_k - B_k s_k\\rVert$。\n- 当 $\\lVert g_k\\rVert \\le \\varepsilon$ 或达到最大迭代次数时停止。\n- 为每个测试报告整数形式的跳过次数、一个表示是否达到收敛的布尔值以及浮点数形式的最终梯度范数。\n\n测试套件规范：\n1. 精确曲率匹配的二次函数（强制永久跳过，理想情况）：\n   - 维度 $n=2$。\n   - 海森矩阵 $H=\\operatorname{diag}(1,1)$。\n   - 目标函数 $f(x)=\\tfrac{1}{2}x^\\top H x$。\n   - 初始点 $x_0=\\begin{bmatrix}10\\\\-7\\end{bmatrix}$。\n   - 初始矩阵 $B_0=H$。\n   - 强沃尔夫参数 $c_1=10^{-4}$，$c_2=0.9$。\n   - 跳过阈值 $\\delta=10^{-8}$。\n   - 收敛容差 $\\varepsilon=10^{-12}$。\n   - 最大迭代次数 $N_{\\max}=50$。\n   由于 $y_k=H s_k$ 和 $B_k=H$，此情况精确地强制 $(y_k-B_k s_k)^\\top s_k=0$ 对所有迭代成立，导致永久跳过，但仍允许快速收敛。\n\n2. 精心设计的首步崩溃的二次函数（边界条件）：\n   - 维度 $n=2$。\n   - 海森矩阵 $H=\\operatorname{diag}(2,1)$。\n   - 目标函数 $f(x)=\\tfrac{1}{2}x^\\top H x$。\n   - 初始矩阵 $B_0=\\operatorname{diag}(3,0.5)$。\n   - 选择一个满足 $s_0^\\top(H-B_0)s_0=0$ 的第一步方向 $s_0=\\begin{bmatrix}1\\\\\\sqrt{2}\\end{bmatrix}$。设置初始点 $x_0=-H^{-1}B_0 s_0=\\begin{bmatrix}-\\tfrac{3}{2}\\\\-\\tfrac{\\sqrt{2}}{2}\\end{bmatrix}$，使得当 $\\alpha_0=1$ 时，算法的第一个搜索方向等于 $s_0$；这会在第一次更新时精确地强制 $(y_0-B_0 s_0)^\\top s_0=0$。\n   - 强沃尔夫参数 $c_1=10^{-4}$，$c_2=0.9$。\n   - 跳过阈值 $\\delta=10^{-8}$。\n   - 收敛容差 $\\varepsilon=10^{-12}$。\n   - 最大迭代次数 $N_{\\max}=50$。\n\n3. 扰动的二次函数（非线性曲率，近崩溃行为）：\n   - 维度 $n=2$。\n   - 基础海森矩阵 $H=\\operatorname{diag}(2,1)$。\n   - 目标函数 $f(x)=\\tfrac{1}{2}x^\\top H x + \\tfrac{\\eta}{4}\\sum_{i=1}^2 x_i^4$，其中小扰动参数 $\\eta=10^{-3}$。\n   - 初始点 $x_0=\\begin{bmatrix}1\\\\-1.5\\end{bmatrix}$。\n   - 初始矩阵 $B_0=H$。\n   - 强沃尔夫参数 $c_1=10^{-4}$，$c_2=0.9$。\n   - 跳过阈值 $\\delta=10^{-8}$。\n   - 收敛容差 $\\varepsilon=10^{-10}$。\n   - 最大迭代次数 $N_{\\max}=50$。\n   由于四次项扰动使海森矩阵与位置相关，该情况通常会在许多步骤中表现出 $(y_k-B_k s_k)^\\top s_k$ 很小但不完全为零。因此，SR1 的跳过可能会根据所选的 $\\delta$ 和局部几何形状而间歇性地触发。\n\n输出格式要求：\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。每个测试用例的结果必须是一个 $[\\text{skips},\\text{converged},\\text{final\\_grad\\_norm}]$ 形式的列表。\n- 例如，一个包含三个测试的有效总输出是 $[[2,True,0.0001],[1,False,0.0123],[10,True,1.0e-08]]$。\n\n本问题陈述中的所有数学量均以 LaTeX 表示。角度和物理单位在此不适用。答案为指定的整数、布尔值和浮点数。该测试套件涵盖了一个理想情况（具有精确曲率的永久跳过）、一个在第一步发生边界设计的崩溃，以及一个在强沃尔夫线搜索下具有近崩溃行为的非线性边缘案例。",
            "solution": "解决方案分三个阶段进行：首先，从基本原理推导 SR1 更新公式；其次，详细设计迭代算法；第三，分析指定的测试用例。\n\n### 1. SR1 更新公式的推导\n\nSR1 更新将当前的海森矩阵近似 $B_k$ 修改为一个满足割线条件的新近似 $B_{k+1}$。割线条件是真实海森矩阵定义属性 $\\nabla^2 f(x) s \\approx \\nabla f(x+s) - \\nabla f(x)$ 的一个有限差分近似。对于从 $x_k$ 到 $x_{k+1}$ 的步，其中 $s_k = x_{k+1} - x_k$ 且 $y_k = g_{k+1} - g_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$，新海森矩阵近似 $B_{k+1}$ 的割线条件为：\n$$\nB_{k+1} s_k = y_k\n$$\nSR1 公式的推导基于一个假设，即更新具有最简单的非平凡形式：对前一个矩阵 $B_k$ 的对称秩一校正。即，\n$$\nB_{k+1} = B_k + \\sigma u u^\\top\n$$\n对于某个标量 $\\sigma \\in \\mathbb{R}$ 和向量 $u \\in \\mathbb{R}^n$。项 $\\sigma u u^\\top$ 是一个对称秩一矩阵。\n\n为了确定 $\\sigma$ 和 $u$，我们在 $B_{k+1}$ 上强制执行割线条件：\n$$\n(B_k + \\sigma u u^\\top) s_k = y_k\n$$\n整理各项可得：\n$$\nB_k s_k + \\sigma u (u^\\top s_k) = y_k\n$$\n$$\n\\sigma (u^\\top s_k) u = y_k - B_k s_k\n$$\n此方程表明，向量 $u$ 必须与向量 $y_k - B_k s_k$ 成比例。一个方便的选择是令 $u = y_k - B_k s_k$。设该向量为 $v_k = y_k - B_k s_k$。如果 $v_k=0$，这意味着前一个矩阵 $B_k$ 已经满足了沿方向 $s_k$ 的割线条件。在这种情况下，不需要更新，因此我们可以设置 $B_{k+1}=B_k$。\n\n如果 $v_k \\neq 0$，将 $u = v_k$ 代入方程得到：\n$$\n\\sigma (v_k^\\top s_k) v_k = v_k\n$$\n由于 $v_k \\neq 0$，我们可以用向量 $v_k$ 去除，得到一个标量方程：\n$$\n\\sigma (v_k^\\top s_k) = 1\n$$\n如果标量分母 $(v_k^\\top s_k) = (y_k - B_k s_k)^\\top s_k$ 非零，我们可以解出 $\\sigma$：\n$$\n\\sigma = \\frac{1}{(y_k - B_k s_k)^\\top s_k}\n$$\n将 $\\sigma$ 和 $u$ 代回更新形式，得到 SR1 更新公式：\n$$\nB_{k+1} = B_k + \\frac{(y_k - B_k s_k)(y_k - B_k s_k)^\\top}{(y_k - B_k s_k)^\\top s_k}\n$$\n如果分母 $(y_k - B_k s_k)^\\top s_k = 0$，则此公式无定义。在实践中，如果该分母接近于零，则其在数值上是不稳定的。\n\n### 2. 算法设计：带有 SR1 和强沃尔夫线搜索的拟牛顿法\n\n实现的核心是一个通用的拟牛顿迭代过程。\n\n**算法：SR1 拟牛顿法**\n1.  **初始化：** 给定初始点 $x_0$、初始对称海森矩阵近似 $B_0$（通常为单位矩阵）、收敛容差 $\\varepsilon > 0$ 以及最大迭代次数 $N_{\\max}$。设置 $k=0$，并计算初始梯度 $g_0 = \\nabla f(x_0)$。初始化跳过计数器 `skips = 0`。\n\n2.  **迭代：** 对于 $k = 0, 1, 2, \\dots, N_{\\max}-1$：\n    a.  **收敛性检查：** 如果 $\\lVert g_k \\rVert \\le \\varepsilon$，终止并报告收敛。\n    b.  **计算搜索方向：** 求解线性系统 $B_k p_k = -g_k$ 以获得搜索方向 $p_k$。这是牛顿类方法的一个定义步骤。\n    c.  **线搜索：** 寻找一个步长 $\\alpha_k > 0$ 以满足强沃尔夫条件：\n        i.  充分下降：$f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top p_k$\n        ii. 曲率条件：$|\\nabla f(x_k + \\alpha_k p_k)^\\top p_k| \\le c_2 |g_k^\\top p_k|$\n        对于常数 $0  c_1  c_2  1$。此搜索确保步长在减小目标函数方面取得足够进展，并且步长不会过短或过长。为此目的，采用了一个稳健的线搜索程序。如果找不到这样的 $\\alpha_k$，则优化终止。\n    d.  **更新状态：** 计算步长 $s_k = \\alpha_k p_k$ 和新的迭代点 $x_{k+1} = x_k + s_k$。计算新梯度 $g_{k+1} = \\nabla f(x_{k+1})$ 和梯度差 $y_k = g_{k+1} - g_k$。\n    e.  **更新海森矩阵近似（$B_{k+1}$）：**\n        i.  计算向量 $v_k = y_k - B_k s_k$ 和分母 $d_k = v_k^\\top s_k$。\n        ii. **跳过条件：** 检查更新在数值上是否安全。使用给定的准则，如果对于一个小的阈值 $\\delta > 0$ 有 $|\\left(y_k - B_k s_k\\right)^\\top s_k| \\le \\delta \\lVert s_k\\rVert \\lVert y_k - B_k s_k\\rVert$，则认为更新是不安全的。在这种情况下，通过设置 $B_{k+1} = B_k$ 来跳过更新，并增加跳过计数器。\n        iii. **SR1 更新：** 否则，应用 SR1 公式：$B_{k+1} = B_k + \\frac{v_k v_k^\\top}{d_k}$。\n\n3.  **终止：** 如果循环因达到 $N_{\\max}$ 而完成，则报告未收敛。最终状态包括跳过次数、收敛状态和最终梯度范数。\n\n### 3. 测试用例分析\n\n这三个测试用例旨在系统地评估 SR1 更新及其跳过逻辑在不同曲率条件下的行为。\n\n1.  **精确曲率匹配的二次函数：** 对于二次函数 $f(x) = \\frac{1}{2}x^\\top H x$，梯度为 $g(x) = Hx$。梯度差为 $y_k = g_{k+1} - g_k = Hx_{k+1} - Hx_k = H(x_{k+1} - x_k) = H s_k$。初始海森矩阵近似被设置为真实的海森矩阵，即 $B_0 = H$。在第一次迭代中，$v_0 = y_0 - B_0 s_0 = H s_0 - H s_0 = 0$。因此分母为 $(y_0 - B_0 s_0)^\\top s_0 = 0$。跳过条件 $|0| \\le \\delta \\lVert s_0 \\rVert \\lVert 0 \\rVert$（简化为 $0 \\le 0$）得到满足。更新被跳过，且 $B_1 = B_0 = H$。通过归纳法，如果 $B_k=H$，则后续更新也将被跳过，导致 $B_{k+1}=H$。因此，每次迭代都会发生跳过。对于这个特定问题，使用精确海森矩阵的牛顿步 $p_k = -H^{-1}(Hx_k) = -x_k$，当 $\\alpha_k=1$ 时，只需一步即可在 $x=0$ 处找到最小值。因此，我们预期有一次迭代和一次跳过的更新。\n\n2.  **精心设计的首步崩溃的二次函数：** 该案例构建了一个场景，其中 SR1 的分母在第一步（$k=0$）时恰好为零，尽管 $B_0 \\neq H$。初始条件被精心设计，使得 $p_0$ 是一个特定的向量 $s_0$，并且线搜索很可能选择 $\\alpha_0=1$。分母是 $(y_0 - B_0 s_0)^\\top s_0$。由于 $f(x)$ 是具有海森矩阵 $H$ 的二次函数，所以 $y_0 = H s_0$。分母变为 $(H s_0 - B_0 s_0)^\\top s_0 = s_0^\\top(H-B_0)s_0$。问题提供的 $s_0$、$H$ 和 $B_0$ 使得该量恰好为零。这强制在 $k=0$ 时发生跳过。此步之后，$B_1=B_0$，后续迭代将使用这个次优的海森矩阵近似继续进行，除非几何结构触发了另一个接近零的分母，否则可能会在没有进一步跳过的情况下收敛。\n\n3.  **扰动的二次函数：** 这个函数 $f(x)=\\tfrac{1}{2}x^\\top H x + \\tfrac{\\eta}{4}\\sum_{i} x_i^4$ 不再是纯粹的二次函数。其海森矩阵 $\\nabla^2 f(x) = H + 3\\eta \\cdot \\operatorname{diag}(x_1^2, x_2^2, \\dots)$ 现在依赖于位置 $x$。初始近似 $B_0=H$ 仅在 $x=0$ 处是精确的。在其他点，$B_k$ 将落后于真实的海森矩阵。根据中值定理，$y_k$ 项可以近似为 $y_k = \\bar{H}_k s_k$，其中 $\\bar{H}_k = \\int_0^1 \\nabla^2 f(x_k + \\tau s_k) d\\tau$ 是沿步长 $s_k$ 的平均海森矩阵。分母变为 $((\\bar{H}_k - B_k)s_k)^\\top s_k$。由于非线性，$\\bar{H}_k - B_k$ 通常不为零，但对于小步长或当 $B_k$ 是一个很好的近似时，这个差异可能很小。因此，分母可能相对于向量的范数变得很小，从而在迭代探索曲率变化的区域时，间歇性地触发跳过条件。这个测试案例模拟了一个更现实的场景，其中由于问题的几何结构不断演化而出现近崩溃。\n\n接下来的实现将遵循此理论框架。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import line_search\n\ndef quasi_newton_sr1(func, grad, x0, B0, c1, c2, delta, epsilon, max_iter):\n    \"\"\"\n    Performs unconstrained optimization using a Quasi-Newton method with\n    the SR1 update, a Strong Wolfe line search, and a skip condition.\n    \"\"\"\n    k = 0\n    skips = 0\n    converged = False\n    \n    x_k = np.copy(x0)\n    B_k = np.copy(B0)\n    g_k = grad(x_k)\n\n    for k in range(max_iter):\n        g_norm = np.linalg.norm(g_k)\n        if g_norm = epsilon:\n            converged = True\n            break\n        \n        try:\n            # Step 1: Compute search direction\n            p_k = np.linalg.solve(B_k, -g_k)\n        except np.linalg.LinAlgError:\n            # Failed to compute search direction, terminate\n            break\n\n        # Ensure it's a descent direction (should be if Bk is PD)\n        if g_k.T @ p_k >= 0:\n            break\n\n        # Step 2: Strong Wolfe Line Search\n        f_k = func(x_k)\n        # Using scipy's line search implementation\n        alpha_k, _, _, _, _, g_kp1 = line_search(\n            func, grad, x_k, p_k, gfk=g_k, old_fval=f_k, c1=c1, c2=c2, maxiter=50\n        )\n\n        if alpha_k is None:\n            # Line search failed to find a suitable step length\n            break\n\n        # Step 3: Update state\n        s_k = alpha_k * p_k\n        x_kp1 = x_k + s_k\n        # g_kp1 is returned by line_search\n        y_k = g_kp1 - g_k\n\n        # Step 4: Update Hessian approximation (SR1)\n        v = y_k - B_k @ s_k\n        s_norm = np.linalg.norm(s_k)\n        \n        if s_norm == 0:\n            # Zero step, no progress, terminate\n            break\n            \n        v_norm = np.linalg.norm(v)\n        denom = v.T @ s_k\n\n        # Check skip condition\n        if v_norm == 0 or np.abs(denom) = delta * s_norm * v_norm:\n            B_kp1 = B_k # Skip update\n            skips += 1\n        else:\n            B_kp1 = B_k + np.outer(v, v) / denom\n\n        # Prepare for next iteration\n        x_k = x_kp1\n        g_k = g_kp1\n        B_k = B_kp1\n        \n    final_grad_norm = np.linalg.norm(g_k)\n    if not converged:\n        converged = final_grad_norm = epsilon\n\n    return [skips, converged, final_grad_norm]\n\ndef solve():\n    \"\"\"\n    Sets up and runs the three test cases for the SR1 Quasi-Newton method.\n    \"\"\"\n    test_cases = []\n\n    # Test Case 1: Quadratic with exact curvature match\n    H1 = np.diag([1.0, 1.0])\n    f1 = lambda x: 0.5 * x.T @ H1 @ x\n    g1 = lambda x: H1 @ x\n    x0_1 = np.array([10.0, -7.0])\n    B0_1 = np.copy(H1)\n    params1 = {\n        'func': f1, 'grad': g1, 'x0': x0_1, 'B0': B0_1,\n        'c1': 1e-4, 'c2': 0.9, 'delta': 1e-8,\n        'epsilon': 1e-12, 'max_iter': 50\n    }\n    test_cases.append(params1)\n\n    # Test Case 2: Quadratic with engineered first-step breakdown\n    H2 = np.diag([2.0, 1.0])\n    f2 = lambda x: 0.5 * x.T @ H2 @ x\n    g2 = lambda x: H2 @ x\n    B0_2 = np.diag([3.0, 0.5])\n    s0_2 = np.array([1.0, np.sqrt(2.0)])\n    # x0 is engineered: x0 = -inv(H) * B0 * s0\n    x0_2 = -np.linalg.inv(H2) @ B0_2 @ s0_2\n    params2 = {\n        'func': f2, 'grad': g2, 'x0': x0_2, 'B0': B0_2,\n        'c1': 1e-4, 'c2': 0.9, 'delta': 1e-8,\n        'epsilon': 1e-12, 'max_iter': 50\n    }\n    test_cases.append(params2)\n\n    # Test Case 3: Perturbed quadratic\n    H3 = np.diag([2.0, 1.0])\n    eta3 = 1e-3\n    f3 = lambda x: 0.5 * x.T @ H3 @ x + (eta3 / 4.0) * (x[0]**4 + x[1]**4)\n    g3 = lambda x: H3 @ x + eta3 * np.array([x[0]**3, x[1]**3])\n    x0_3 = np.array([1.0, -1.5])\n    B0_3 = np.copy(H3)\n    params3 = {\n        'func': f3, 'grad': g3, 'x0': x0_3, 'B0': B0_3,\n        'c1': 1e-4, 'c2': 0.9, 'delta': 1e-8,\n        'epsilon': 1e-10, 'max_iter': 50\n    }\n    test_cases.append(params3)\n\n    results = []\n    for params in test_cases:\n        result = quasi_newton_sr1(\n            params['func'], params['grad'], params['x0'], params['B0'],\n            params['c1'], params['c2'], params['delta'],\n            params['epsilon'], params['max_iter']\n        )\n        results.append(result)\n    \n    final_output = []\n    for res in results:\n        final_output.append(f\"[{res[0]},{'True' if res[1] else 'False'},{res[2]}]\")\n\n    print(f\"[{','.join(final_output)}]\")\n\n\nsolve()\n\n```"
        },
        {
            "introduction": "在实际优化问题中，我们常常希望结合不同方法的优点来构建更强大的算法。本练习将指导你实现一个混合拟牛顿法，它结合了 BFGS 方法的稳健性和 SR1 方法处理非凸性的能力 。你将从一个稳定的 BFGS 更新开始，并在检测到负曲率时智能地切换到 SR1 更新，从而创建一个在各种函数上都表现出色的实用优化器。",
            "id": "3184218",
            "problem": "考虑一个连续可微函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 的无约束最小化问题。设 $x_k\\in\\mathbb{R}^n$ 表示第 $k$ 步的迭代点，$g_k=\\nabla f(x_k)$ 为梯度，$H_k\\in\\mathbb{R}^{n\\times n}$ 为逆Hessian矩阵的一个近似。拟牛顿法计算搜索方向 $p_k=-H_k g_k$，并利用步长 $s_k=x_{k+1}-x_k$ 和梯度变化 $y_k=g_{k+1}-g_k$ 的信息来更新 $H_k$。\n\n问题：实现一个多阶段拟牛顿算法，该算法以Broyden-Fletcher-Goldfarb-Shanno (BFGS) 更新开始，当通过负曲率检测到非凸性时，永久切换到对称秩一 (SR1) 更新。该算法必须遵循以下源自核心定义和经过充分检验的优化事实的规范：\n\n- 从 $H_0=I$ 开始。\n- 在迭代 $k$ 步，计算 $p_k=-H_k g_k$。如果 $g_k^\\top p_k\\ge 0$，则将 $p_k$ 替换为最速下降方向 $p_k=-g_k$，以强制执行下降步。\n- 使用回溯 Armijo 线搜索来寻找步长 $\\alpha_k$：以 $\\alpha_k=1$ 初始化，并不断将 $\\alpha_k$ 乘以一个常数因子，直到满足 Armijo 条件 $f(x_k+\\alpha_k p_k)\\le f(x_k)+c_1\\alpha_k g_k^\\top p_k$。使用 $c_1=10^{-4}$ 和收缩因子 $1/2$。所有三角函数的参数均为弧度制角。\n- 在线搜索被接受后，定义 $s_k=x_{k+1}-x_k$ 和 $y_k=g_{k+1}-g_k$。\n- 负曲率检测和阶段切换规则：在 BFGS 阶段，如果曲率条件 $s_k^\\top y_k>\\theta$ 成立，则继续使用 BFGS；否则，检测到负曲率，并从下一次更新开始永久切换到 SR1 更新。使用阈值 $\\theta=10^{-10}$。\n- 在 SR1 阶段，只要更新是良定义的，就应用 SR1 更新；如果 SR1 分母的绝对值太小，则跳过更新。\n- 当 $\\lVert g_k\\rVert_2\\le\\varepsilon$（其中 $\\varepsilon=10^{-6}$）或迭代次数达到最大值 $200$ 时终止。\n\n您必须在以下一组目标函数上测试该实现，其中包括凸和非凸案例：\n\n$1.$ 凸二次函数：$f(x)=\\tfrac{1}{2}x^\\top A x + b^\\top x$，其中 $A=\\begin{bmatrix}4  1\\\\ 1  3\\end{bmatrix}$，$b=\\begin{bmatrix}-1\\\\ 2\\end{bmatrix}$，起始点 $x_0=\\begin{bmatrix}-2\\\\ 2\\end{bmatrix}$。\n\n$2.$ Rosenbrock 函数：$f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2$，起始点 $x_0=\\begin{bmatrix}-1.2\\\\ 1\\end{bmatrix}$。\n\n$3.$ 鞍点四次函数（非凸但有下界）：$f(x_1,x_2)=x_1^2-x_2^2+0.1(x_1^4+x_2^4)$，起始点 $x_0=\\begin{bmatrix}0\\\\ 0.2\\end{bmatrix}$。\n\n$4.$ 二次函数加三角扰动（混合曲率）：$f(x)=\\tfrac{1}{2}x^\\top A x+0.2\\sin(3x_1)+0.2\\cos(2x_2)$，其中 $A=\\begin{bmatrix}2  0\\\\ 0  1\\end{bmatrix}$，起始点 $x_0=\\begin{bmatrix}1.5\\\\ -1\\end{bmatrix}$。\n\n对于每个测试案例，您的程序必须返回三个值：最终目标值 $f(x_{\\text{final}})$（四舍五入到六位小数），迭代次数（一个整数），以及一个布尔标志，指示算法是否至少切换到 SR1 一次。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表，每个测试案例的结果本身格式化为一个列表，例如 $[ [f_1,i_1,\\text{flag}_1],[f_2,i_2,\\text{flag}_2],\\dots ]$。布尔标志必须以小写形式打印为 $true$ 或 $false$。不允许有其他输出。\n\n测试套件摘要：\n- 案例 $1$：凸二次函数，其中 $A=\\begin{bmatrix}4  1\\\\ 1  3\\end{bmatrix}$，$b=\\begin{bmatrix}-1\\\\ 2\\end{bmatrix}$，$x_0=\\begin{bmatrix}-2\\\\ 2\\end{bmatrix}$。\n- 案例 $2$：Rosenbrock 函数，其中 $x_0=\\begin{bmatrix}-1.2\\\\ 1\\end{bmatrix}$。\n- 案例 $3$：鞍点四次函数，其中 $x_0=\\begin{bmatrix}0\\\\ 0.2\\end{bmatrix}$。\n- 案例 $4$：二次函数加三角扰动，其中 $A=\\begin{bmatrix}2  0\\\\ 0  1\\end{bmatrix}$，$x_0=\\begin{bmatrix}1.5\\\\ -1\\end{bmatrix}$。\n\n所有函数都是纯数学的，不涉及物理单位。三角函数的角度单位是弧度。最终输出格式必须严格遵守上述单行规范。",
            "solution": "指定的混合拟牛顿算法被实现用来解决无约束优化问题 $\\min_{x \\in \\mathbb{R}^n} f(x)$。该方法通过更新规则 $x_{k+1} = x_k + s_k$ 生成一个迭代序列 $\\{x_k\\}$，其中 $s_k = \\alpha_k p_k$ 是步长。这里，$p_k$ 是搜索方向，$\\alpha_k$ 是步长长度。\n\n1.  **初始化**：算法从一个给定的点 $x_0$ 开始。迭代计数器设置为 $k=0$。逆Hessian矩阵的初始近似是 $n \\times n$ 的单位矩阵，$H_0 = I$。一个标志用于跟踪算法的模式，初始化为 `BFGS`。\n\n2.  **主迭代循环**：在每次迭代 $k = 0, 1, 2, \\dots$：\n    a. **终止检查**：计算梯度 $g_k = \\nabla f(x_k)$。如果其欧几里得范数满足 $\\lVert g_k \\rVert_2 \\le \\varepsilon = 10^{-6}$，算法终止，因为已经足够精确地找到了一个驻点。如果 $k$ 达到最大值 $200$，循环也会终止。\n\n    b. **搜索方向**：使用当前的逆Hessian近似计算搜索方向：$p_k = -H_k g_k$。为保证 $p_k$ 是一个下降方向（即 $g_k^\\top p_k  0$），执行一次检查。如果 $g_k^\\top p_k \\ge 0$（这可能在 $H_k$ 不是正定的时候发生），则将方向重置为最速下降方向，$p_k = -g_k$。\n\n    c. **线搜索**：步长 $\\alpha_k$ 由回溯线搜索确定，以满足 Armijo 条件：\n    $$f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top p_k$$\n    搜索从试探步长 $\\alpha_k=1$ 开始。如果不满足条件，则将 $\\alpha_k$ 反复乘以收缩因子 $1/2$，直到条件满足。参数 $c_1$ 设置为 $10^{-4}$。\n\n    d. **更新迭代点**：新的迭代点计算为 $x_{k+1} = x_k + \\alpha_k p_k$。步长向量为 $s_k = x_{k+1} - x_k$，梯度差向量为 $y_k = g_{k+1} - g_k$，其中 $g_{k+1} = \\nabla f(x_{k+1})$。\n\n    e. **逆Hessian更新**：混合策略的核心在于如何将 $H_k$ 更新为 $H_{k+1}$。\n        -   **BFGS 模式**：如果算法处于 BFGS 模式，检查曲率。如果 $s_k^\\top y_k > \\theta = 10^{-10}$，则曲率是充分正的，应用 BFGS 更新：\n            $$H_{k+1} = \\left(I - \\rho_k s_k y_k^\\top\\right) H_k \\left(I - \\rho_k y_k s_k^\\top\\right) + \\rho_k s_k s_k^\\top, \\quad \\text{其中 } \\rho_k = \\frac{1}{s_k^\\top y_k}$$\n            如果曲率条件不满足 ($s_k^\\top y_k \\le \\theta$)，则表明存在非正曲率。算法永久切换到 SR1 模式。此切换被标记。\n        -   **SR1 模式**：一旦进入 SR1 模式（无论是在当前步骤切换还是从之前的步骤切换），便尝试进行 SR1 更新。更新公式为：\n            $$H_{k+1} = H_k + \\frac{(s_k - H_k y_k)(s_k - H_k y_k)^\\top}{(s_k - H_k y_k)^\\top y_k}$$\n            仅当分母不接近于零时才执行此更新。实现了一个实际的检查：如果 $|(s_k - H_k y_k)^\\top y_k| \\le 10^{-8}$，则跳过更新，我们设置 $H_{k+1} = H_k$。\n\n    f. **前进**：增加迭代计数器，$k \\leftarrow k+1$，并进行下一次迭代。\n\n此过程将应用于问题陈述中提供的四个测试函数中的每一个。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hybrid_quasi_newton(func, grad, x0, A=None, b=None):\n    \"\"\"\n    Implements a hybrid BFGS/SR1 quasi-Newton optimization algorithm.\n\n    Args:\n        func (callable): The objective function f(x, A, b).\n        grad (callable): The gradient of the objective function grad(x, A, b).\n        x0 (np.ndarray): The starting point.\n        A (np.ndarray, optional): Matrix parameter for the function.\n        b (np.ndarray, optional): Vector parameter for the function.\n\n    Returns:\n        tuple: A tuple containing:\n            - float: The final objective function value, rounded to 6 decimal places.\n            - int: The total number of iterations.\n            - bool: True if the algorithm switched to SR1, False otherwise.\n    \"\"\"\n    # Parameters from problem statement\n    c1 = 1e-4\n    shrink_factor = 0.5\n    grad_tol = 1e-6\n    max_iter = 200\n    theta = 1e-10\n    sr1_denom_tol = 1e-8\n\n    n = len(x0)\n    x = np.copy(x0).astype(float)\n    H = np.eye(n)\n    \n    k = 0\n    switched_to_sr1 = False\n    mode = 'bfgs'\n\n    while k  max_iter:\n        g = grad(x, A, b)\n\n        # Termination condition on gradient norm\n        if np.linalg.norm(g) = grad_tol:\n            break\n\n        # Compute search direction\n        p = -H @ g\n\n        # Ensure search direction is a descent direction\n        if g.T @ p >= 0:\n            p = -g\n        \n        # Backtracking Armijo line search\n        alpha = 1.0\n        fk = func(x, A, b)\n        g_dot_p = g.T @ p\n        \n        # The line search loop\n        while func(x + alpha * p, A, b) > fk + c1 * alpha * g_dot_p:\n            alpha *= shrink_factor\n        \n        # Update iterate and compute step and gradient difference\n        s = alpha * p\n        x_next = x + s\n        g_next = grad(x_next, A, b)\n        y = g_next - g\n\n        s_dot_y = s.T @ y\n\n        # Mode-dependent update of the inverse Hessian approximation\n        if mode == 'bfgs':\n            if s_dot_y > theta:\n                # Apply BFGS update\n                rho = 1.0 / s_dot_y\n                I = np.eye(n)\n                term1 = I - rho * np.outer(s, y)\n                H = term1 @ H @ term1.T + rho * np.outer(s, s)\n            else:\n                # Switch to SR1 and fall through to apply SR1 in this iteration\n                mode = 'sr1'\n                switched_to_sr1 = True\n\n        if mode == 'sr1':\n            v = s - H @ y\n            denom = v.T @ y\n            if abs(denom) > sr1_denom_tol:\n                # Apply SR1 update\n                H = H + np.outer(v, v) / denom\n            # Else: skip the update, H_{k+1} = H_k\n        \n        # Prepare for the next iteration\n        x = x_next\n        k += 1\n\n    final_f = func(x, A, b)\n    return round(final_f, 6), k, switched_to_sr1\n\n# --- Test Case Definitions ---\n\n# 1. Convex quadratic\ndef func1(x, A, b):\n    return 0.5 * x.T @ A @ x + b.T @ x\ndef grad1(x, A, b):\n    return A @ x + b\n\n# 2. Rosenbrock function\ndef func2(x, A, b): # A, b are placeholders\n    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\ndef grad2(x, A, b): # A, b are placeholders\n    return np.array([\n        -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2),\n        200 * (x[1] - x[0]**2)\n    ])\n\n# 3. Saddle-quartic function\ndef func3(x, A, b): # A, b are placeholders\n    return x[0]**2 - x[1]**2 + 0.1 * (x[0]**4 + x[1]**4)\ndef grad3(x, A, b): # A, b are placeholders\n    return np.array([\n        2 * x[0] + 0.4 * x[0]**3,\n        -2 * x[1] + 0.4 * x[1]**3\n    ])\n\n# 4. Quadratic plus trigonometric perturbation\ndef func4(x, A, b): # b is a placeholder\n    return 0.5 * x.T @ A @ x + 0.2 * np.sin(3 * x[0]) + 0.2 * np.cos(2 * x[1])\ndef grad4(x, A, b): # b is a placeholder\n    return A @ x + np.array([\n        0.6 * np.cos(3 * x[0]),\n        -0.4 * np.sin(2 * x[1])\n    ])\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"func\": func1, \"grad\": grad1, \"x0\": np.array([-2.0, 2.0]), \n            \"A\": np.array([[4.0, 1.0], [1.0, 3.0]]), \"b\": np.array([-1.0, 2.0])\n        },\n        {\n            \"func\": func2, \"grad\": grad2, \"x0\": np.array([-1.2, 1.0]),\n            \"A\": None, \"b\": None\n        },\n        {\n            \"func\": func3, \"grad\": grad3, \"x0\": np.array([0.0, 0.2]),\n            \"A\": None, \"b\": None\n        },\n        {\n            \"func\": func4, \"grad\": grad4, \"x0\": np.array([1.5, -1.0]),\n            \"A\": np.array([[2.0, 0.0], [0.0, 1.0]]), \"b\": None\n        }\n    ]\n    \n    results = []\n    for case in test_cases:\n        f_val, iters, flag = hybrid_quasi_newton(\n            case[\"func\"], case[\"grad\"], case[\"x0\"], A=case[\"A\"], b=case[\"b\"]\n        )\n        # Format boolean as required ('true' or 'false')\n        flag_str = str(flag).lower()\n        results.append(f\"[{f_val:.6f},{iters},{flag_str}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}