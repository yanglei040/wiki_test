{
    "hands_on_practices": [
        {
            "introduction": "Before applying an algorithm, it's crucial to understand its fundamental guarantees. This first exercise  grounds your understanding of bracketing methods in the principles of calculus. You will start by proving that a given function is unimodal by analyzing its derivatives, and then use this analytical insight to predict the step-by-step decisions of the Golden-section and Fibonacci search methods, revealing the core logic that ensures they successfully narrow down the search for a minimum.",
            "id": "3196269",
            "problem": "Consider the real-valued objective function $f:\\mathbb{R}\\to\\mathbb{R}$ defined by $f(x)=\\exp(\\alpha x)-\\beta x$, where $\\alpha0$ and $\\beta0$ are fixed parameters. Let $[a,b]$ be a closed interval with $ab$ and suppose the unique minimizer $x^{\\star}$ of $f$ lies strictly inside the interval, i.e., $ax^{\\star}b$. Your tasks are:\n\n1. Starting from the fundamental definitions of convexity and differentiability, prove that $f$ is strictly unimodal on $\\mathbb{R}$ and derive a closed-form expression for the unique minimizer $x^{\\star}$.\n\n2. Apply the first bracketing iteration of the golden-section search on $[a,b]$. Denote the two interior test points by $c$ and $d$ with $acdb$, chosen according to the standard GSS rule. Using only first principles (derivatives and the monotonicity implied by convexity around $x^{\\star}$), determine analytically which subinterval is discarded in the first GSS iteration under the condition $x^{\\star}c$. Then, express symbolically which previously evaluated interior test point is reused in the next iteration and give the location of the newly introduced point inside the updated interval in terms of $a$, $b$, and the internal constant of the method.\n\n3. Repeat the early-step decision analysis for the Fibonacci search configured for a fixed total number of function evaluations $N\\geq 3$. Let the first interior point be $c_{N}$ and the second $d_{N}$ with $ac_{N}d_{N}b$, chosen according to the standard FS rule for $N$ planned evaluations. Under the condition $x^{\\star}c_{N}$, determine analytically which subinterval is discarded in the first FS iteration and identify which one of $c_{N}$ or $d_{N}$ is reused in the next step.\n\nReport your final answer as the closed-form expression for the minimizer $x^{\\star}$, simplified as much as possible. No numerical rounding is required and no units are involved.",
            "solution": "The problem as stated is scientifically sound, self-contained, and well-posed. All terms are formally defined within the fields of calculus and numerical optimization. Therefore, a rigorous solution can be constructed.\n\n**Part 1: Proof of Strict Unimodality and Derivation of the Minimizer**\n\nA real-valued function is strictly unimodal on an interval if it possesses a unique minimizer within that interval and is strictly monotonic on either side of the minimizer. A sufficient condition for a twice-differentiable function $f(x)$ to be strictly unimodal on $\\mathbb{R}$ is for it to be strictly convex on $\\mathbb{R}$. Strict convexity is established by demonstrating that the second derivative is strictly positive for all $x \\in \\mathbb{R}$.\n\nThe given objective function is $f(x) = \\exp(\\alpha x) - \\beta x$, with parameters $\\alpha > 0$ and $\\beta > 0$.\n\nFirst, we compute the first derivative of $f(x)$ with respect to $x$:\n$$ f'(x) = \\frac{d}{dx} \\left( \\exp(\\alpha x) - \\beta x \\right) = \\alpha \\exp(\\alpha x) - \\beta $$\n\nNext, we compute the second derivative:\n$$ f''(x) = \\frac{d}{dx} \\left( \\alpha \\exp(\\alpha x) - \\beta \\right) = \\alpha^2 \\exp(\\alpha x) $$\n\nGiven that $\\alpha > 0$, its square $\\alpha^2$ is strictly positive. The exponential function $\\exp(y)$ is strictly positive for any real argument $y$. Therefore, for all $x \\in \\mathbb{R}$, the term $\\exp(\\alpha x)$ is strictly positive. The product of two strictly positive numbers is strictly positive, so we have:\n$$ f''(x) = \\alpha^2 \\exp(\\alpha x) > 0 \\quad \\forall x \\in \\mathbb{R} $$\n\nSince the second derivative is strictly positive everywhere, the function $f(x)$ is strictly convex on $\\mathbb{R}$. A strictly convex function defined on the entire real line has at most one minimizer. To find this minimizer, we apply the first-order necessary condition for optimality, which states that the first derivative must be zero at the minimizer $x^{\\star}$:\n$$ f'(x^{\\star}) = 0 $$\n$$ \\alpha \\exp(\\alpha x^{\\star}) - \\beta = 0 $$\n\nSolving for $x^{\\star}$:\n$$ \\alpha \\exp(\\alpha x^{\\star}) = \\beta $$\n$$ \\exp(\\alpha x^{\\star}) = \\frac{\\beta}{\\alpha} $$\nSince $\\alpha > 0$ and $\\beta > 0$, the ratio $\\frac{\\beta}{\\alpha}$ is positive, ensuring a real-valued solution exists. Taking the natural logarithm of both sides:\n$$ \\ln\\left(\\exp(\\alpha x^{\\star})\\right) = \\ln\\left(\\frac{\\beta}{\\alpha}\\right) $$\n$$ \\alpha x^{\\star} = \\ln(\\beta) - \\ln(\\alpha) $$\n$$ x^{\\star} = \\frac{\\ln(\\beta) - \\ln(\\alpha)}{\\alpha} = \\frac{1}{\\alpha}\\ln\\left(\\frac{\\beta}{\\alpha}\\right) $$\nBecause $f(x)$ is strictly convex, this single critical point is guaranteed to be the unique global minimizer. The strict convexity also implies that $f'(x)  0$ for $x  x^{\\star}$ (function is strictly decreasing) and $f'(x)  0$ for $x  x^{\\star}$ (function is strictly increasing). This confirms that $f(x)$ is strictly unimodal on $\\mathbb{R}$.\n\n**Part 2: Golden-Section Search (GSS) First Iteration Analysis**\n\nThe golden-section search algorithm operates on an interval $[a, b]$ of length $L_1 = b-a$. The internal constant of the method is $\\tau = \\frac{\\sqrt{5}-1}{2}$. The two interior test points, $c$ and $d$, are placed symmetrically:\n$$ c = a + (1-\\tau)(b-a) = a + \\tau^2(b-a) $$\n$$ d = b - (1-\\tau)(b-a) = b - \\tau^2(b-a) = a + \\tau(b-a) $$\nThese placements ensure $a  c  d  b$.\n\nThe problem imposes the condition $x^{\\star}  c$. Combined with the known ordering of the points, we have the relation:\n$$ a  x^{\\star}  c  d  b $$\nFrom Part 1, we established that $f(x)$ is strictly increasing for all $x  x^{\\star}$. Since both $c$ and $d$ are greater than $x^{\\star}$ and $c  d$, it follows directly that:\n$$ f(c)  f(d) $$\nThe GSS algorithm's rule for this case is to eliminate the subinterval to the right of the point with the higher function value. Thus, the subinterval $[d, b]$ is discarded.\n\nThe new search interval for the next iteration becomes $[a', b'] = [a, d]$.\nThe point $c$ from the first iteration lies within this new interval. The GSS method is designed such that this point $c$ will be one of the two interior points for the next iteration, thus saving one function evaluation. Specifically, the old point $c$ becomes the new point $d'$. Let the new interval be $[a_2, b_2]=[a,d]$ and its length be $L_2 = d-a = \\tau(b-a)$. The new points are $c_2 = a_2 + \\tau^2 L_2$ and $d_2 = b_2 - \\tau^2 L_2$.\nLet's verify which one corresponds to the old point $c$:\n$d_2 = b_2 - \\tau^2 L_2 = d - \\tau^2(\\tau(b-a)) = d - \\tau^3(b-a)$.\nUsing $d = a + \\tau(b-a)$, we get $d_2 = a + \\tau(b-a) - \\tau^3(b-a) = a + (\\tau - \\tau^3)(b-a)$.\nUsing the property $\\tau^2 = 1-\\tau$, we find $\\tau^3 = \\tau - \\tau^2 = \\tau - (1-\\tau) = 2\\tau-1$.\nThus, $\\tau - \\tau^3 = \\tau - (2\\tau-1) = 1-\\tau = \\tau^2$.\nSo, $d_2 = a + \\tau^2(b-a)$, which is precisely the definition of the original point $c$.\nTherefore, the point $c$ is reused as the right-hand interior point in the new interval $[a, d]$.\n\nThe newly introduced point for the second iteration is $c_2 = a_2 + \\tau^2 L_2 = a + \\tau^2(\\tau(b-a)) = a + \\tau^3(b-a)$. Expressed in terms of $a$, $b$, and the constant $\\tau = \\frac{\\sqrt{5}-1}{2}$, this new point is located at $a + \\left(\\frac{\\sqrt{5}-1}{2}\\right)^3(b-a)$.\n\n**Part 3: Fibonacci Search (FS) First Iteration Analysis**\n\nThe Fibonacci search on an interval $[a, b]$ with a planned total of $N \\geq 3$ function evaluations uses ratios of Fibonacci numbers to place its test points. Let the Fibonacci sequence be defined by $F_0=0, F_1=1, F_2=1, F_3=2, \\dots$. The first two interior points, $c_N$ and $d_N$, are:\n$$ c_N = a + \\frac{F_{N-2}}{F_N}(b-a) $$\n$$ d_N = b - \\frac{F_{N-2}}{F_N}(b-a) = a + \\frac{F_{N-1}}{F_N}(b-a) $$\nFor $N \\ge 3$, we have $F_{N-1} > F_{N-2}$, which ensures $a  c_N  d_N  b$.\n\nThe analysis proceeds under the same condition as before: $x^{\\star}  c_N$. This gives the ordering:\n$$ a  x^{\\star}  c_N  d_N  b $$\nAgain, since $f(x)$ is strictly increasing for $x  x^{\\star}$, it follows that $f(c_N)  f(d_N)$.\nThe Fibonacci search algorithm follows the same reduction rule as GSS. The subinterval $[d_N, b]$ is discarded.\n\nThe new search interval is $[a, d_N]$. The next iteration will be configured for $N-1$ evaluations. We must determine which of the original points, $c_N$ or $d_N$, is reused.\nThe new interval $[a', b'] = [a, d_N]$ has length $L' = d_N - a = \\frac{F_{N-1}}{F_N}(b-a)$.\nThe interior points for the second iteration (which is set for $N-1$ evaluations) are $c'_{N-1} = a' + \\frac{F_{(N-1)-2}}{F_{N-1}}(b'-a')$ and $d'_{N-1} = b' - \\frac{F_{(N-1)-2}}{F_{N-1}}(b'-a')$.\nWe check if $c_N$ matches one of these new points. Let's analyze $d'_{N-1}$:\n$$ d'_{N-1} = d_N - \\frac{F_{N-3}}{F_{N-1}}\\left(\\frac{F_{N-1}}{F_N}(b-a)\\right) = d_N - \\frac{F_{N-3}}{F_N}(b-a) $$\nSubstitute the expression for $d_N$:\n$$ d'_{N-1} = \\left(a + \\frac{F_{N-1}}{F_N}(b-a)\\right) - \\frac{F_{N-3}}{F_N}(b-a) = a + \\frac{F_{N-1}-F_{N-3}}{F_N}(b-a) $$\nUsing the Fibonacci recurrence relation, $F_{N-1} = F_{N-2} + F_{N-3}$, which implies $F_{N-1} - F_{N-3} = F_{N-2}$.\nSubstituting this into the expression for $d'_{N-1}$:\n$$ d'_{N-1} = a + \\frac{F_{N-2}}{F_N}(b-a) $$\nThis is exactly the expression for the original point $c_N$.\nTherefore, in the first iteration of the Fibonacci search under the condition $x^{\\star}  c_N$, the subinterval $[d_N, b]$ is discarded, and the test point $c_N$ is reused for the subsequent iteration.\n\nThe problem asks for the closed-form expression for the minimizer $x^{\\star}$ as the final answer. This was derived in Part 1.",
            "answer": "$$\\boxed{\\frac{1}{\\alpha}\\ln\\left(\\frac{\\beta}{\\alpha}\\right)}$$"
        },
        {
            "introduction": "The performance of some optimization algorithms can degrade significantly depending on the landscape of the objective function. A common challenge is a \"flat\" region around the minimum, where the function's value changes very little. This thought experiment  invites you to analyze how the golden-section search behaves on such a function. By working through this problem, you will discover that the method's convergence rate is purely geometric and remarkably insensitive to the function's local curvature, a key property that makes it exceptionally robust.",
            "id": "3196252",
            "problem": "You are asked to analyze how the golden-section search behaves when minimizing a unimodal function with a very flat region near its minimum. Consider the function\n$$f(x) = (x - x^{\\star})^{4}, \\quad x^{\\star} = 0.4,$$\ndefined on the interval\n$$[a,b] = [0,1].$$\nThe function is unimodal on $[0,1]$, and because $f''(x^{\\star}) = 0$, it has a flat plateau in the sense that the curvature near the minimum is nearly zero. You apply the golden-section search (a bracketing method that uses only function evaluations and maintains a shrinking bracket containing the minimizer) with an absolute interval-length termination criterion\n$$L_{\\text{tol}} = 10^{-3},$$\nand no derivative or curvature information.\n\nBased on first principles about bracketing methods and the invariance of the golden-section construction, which statement best describes the termination behavior and its dependence on the flatness of $f$ near $x^{\\star}$? Choose the single best option.\n\nA. The method inherently slows down on flat plateaus because it estimates curvature from function values and adapts its step size; consequently, the number of iterations increases in proportion to the inverse curvature near $x^{\\star}$.\n\nB. The number of iterations is determined by the geometric reduction of the bracket and the target $L_{\\text{tol}}$, not by curvature. Starting from $[0,1]$ and using an absolute interval criterion, it will take about $15$ iterations to reach $L_{\\text{tol}} = 10^{-3}$, and this count is essentially independent of the flatness of $f$.\n\nC. The method may not terminate on a flat quartic because the function values at interior points can become numerically indistinguishable, causing an infinite loop unless second derivatives are supplied.\n\nD. On flat plateaus, one should replace the interval-length criterion with a function-value tolerance (stop when $|f(x_{1}) - f(x_{2})| \\le \\varepsilon_{f}$) to guarantee accurate localization of $x^{\\star}$; this improves termination reliability compared with interval-length stopping.",
            "solution": "The user has asked for an analysis of the termination behavior of the golden-section search method when applied to a unimodal function with a flat region near its minimum. I will first validate the problem statement and, if it is valid, proceed to a full derivation and evaluation of the options.\n\n### Step 1: Extract Givens\n- The function to be minimized is $f(x) = (x - x^{\\star})^{4}$, where $x^{\\star} = 0.4$.\n- The initial search interval is $[a,b] = [0,1]$.\n- The function is stated to be unimodal on $[0,1]$ and has a flat plateau near the minimum because $f''(x^{\\star}) = 0$.\n- The optimization algorithm is the golden-section search.\n- The termination criterion is an absolute interval-length tolerance, $L_{\\text{tol}} = 10^{-3}$.\n- The algorithm does not use derivative or curvature information.\n- The question asks to identify the statement that best describes the termination behavior and its dependence on the flatness of $f$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is subjected to validation based on established scientific principles.\n\n1.  **Scientific Groundedness**: The problem is scientifically sound. The function $f(x) = (x - 0.4)^{4}$ is a standard mathematical function. Its unimodality can be verified: the first derivative is $f'(x) = 4(x - 0.4)^{3}$, which is zero only at $x = 0.4$. For $x \\in [0, 0.4)$, $f'(x)  0$, and for $x \\in (0.4, 1]$, $f'(x)  0$. This confirms the function is strictly unimodal on the interval $[0,1]$ with a unique minimum at $x^{\\star} = 0.4$. The second derivative is $f''(x) = 12(x-0.4)^2$, which evaluates to $f''(0.4) = 0$. This correctly corresponds to the description of a \"flat plateau\" (zero curvature) at the minimum. The golden-section search is a fundamental and well-established algorithm in numerical optimization for one-dimensional unimodal functions.\n\n2.  **Well-Posedness and Objectivity**: The problem is well-posed. It specifies the function, the interval, the algorithm, and the termination criterion, and asks for a qualitative and quantitative analysis of the algorithm's behavior. The language is precise and objective. All components required for a rigorous analysis are present.\n\n3.  **Other Flaws**: The problem statement is free from contradictions, ambiguities, pseudo-profound claims, or other flaws listed in the validation criteria. It poses a standard, non-trivial conceptual question about the properties of a core optimization algorithm.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived.\n\n### Derivation and Option Analysis\n\nThe golden-section search is a bracketing method for finding the minimum of a unimodal function. Its defining characteristic is that it reduces the length of the interval of uncertainty by a fixed ratio at each iteration, independent of the function's specific values, as long as their ordinal relationship can be determined.\n\nLet the interval at iteration $k$ be $[a_k, b_k]$, with length $L_k = b_k - a_k$. The algorithm maintains two interior points, $x_{1,k}$ and $x_{2,k}$. The positions of these points are determined by the golden ratio, $\\phi = \\frac{1 + \\sqrt{5}}{2} \\approx 1.61803$. The interval is reduced by a constant factor $\\rho = \\phi - 1 = \\frac{\\sqrt{5}-1}{2} \\approx 0.61803$ at each step.\n\nSpecifically, if $f(x_{1,k})  f(x_{2,k})$, the new interval becomes $[a_{k+1}, b_{k+1}] = [a_k, x_{2,k}]$. If $f(x_{1,k}) \\ge f(x_{2,k})$, the new interval becomes $[a_{k+1}, b_{k+1}] = [x_{1,k}, b_k]$. In either case, the length of the new interval is $L_{k+1} = \\rho \\cdot L_k$.\n\nThis relationship is purely geometric. The algorithm's progression depends only on the inequality check between $f(x_{1,k})$ and $f(x_{2,k})$, not on the magnitude of the function values or their difference. The \"flatness\" of the function, which is related to its second derivative (curvature), has no influence on the sequence of interval lengths.\n\nThe algorithm terminates when the interval length $L_N$ becomes less than or equal to the specified tolerance $L_{\\text{tol}}$. Starting with an initial interval length $L_0 = b-a$, the length after $N$ iterations is given by:\n$$L_N = \\rho^N L_0$$\nThe termination condition is $\\rho^N L_0 \\le L_{\\text{tol}}$. To find the number of iterations $N$, we solve for $N$:\n$$\\rho^N \\le \\frac{L_{\\text{tol}}}{L_0}$$\n$$N \\log(\\rho) \\le \\log\\left(\\frac{L_{\\text{tol}}}{L_0}\\right)$$\nSince $\\rho  1$, its logarithm $\\log(\\rho)$ is negative. Dividing by a negative number reverses the inequality:\n$$N \\ge \\frac{\\log(L_{\\text{tol}} / L_0)}{\\log(\\rho)}$$\nFor this specific problem:\n-   Initial interval length: $L_0 = 1 - 0 = 1$.\n-   Termination tolerance: $L_{\\text{tol}} = 10^{-3}$.\n-   Reduction factor: $\\rho = \\frac{\\sqrt{5}-1}{2}$.\n\nPlugging in the values:\n$$N \\ge \\frac{\\log(10^{-3} / 1)}{\\log((\\sqrt{5}-1)/2)} = \\frac{\\log(10^{-3})}{\\log(\\rho)}$$\nUsing natural logarithms:\n$$N \\ge \\frac{-3 \\ln(10)}{\\ln(\\rho)} \\approx \\frac{-3 \\times 2.302585}{-0.481212} \\approx \\frac{-6.907755}{-0.481212} \\approx 14.355$$\nSince the number of iterations $N$ must be an integer, we take the ceiling of this value:\n$$N = \\lceil 14.355 \\rceil = 15$$\nTherefore, the method will take approximately $15$ iterations to reduce the interval length from $1$ to $10^{-3}$. This number is determined solely by the initial and final interval lengths and the constant geometric reduction factor $\\rho$. It is completely independent of the function $f(x)$'s shape, including its flatness, provided the function is unimodal.\n\nNow, let us evaluate each option based on this derivation.\n\n**A. The method inherently slows down on flat plateaus because it estimates curvature from function values and adapts its step size; consequently, the number of iterations increases in proportion to the inverse curvature near $x^{\\star}$.**\n-   **Analysis:** This statement is fundamentally incorrect. The golden-section search does not estimate curvature, nor does it adapt its step size based on function values. The interval reduction is governed by the fixed geometric factor $\\rho$. Methods that rely on curvature (like Newton's method) are indeed sensitive to it, but the golden-section search is not one of them. Its rate of convergence is linear with a constant factor $\\rho$, independent of the function's properties beyond unimodality.\n-   **Verdict:** Incorrect.\n\n**B. The number of iterations is determined by the geometric reduction of the bracket and the target $L_{\\text{tol}}$, not by curvature. Starting from $[0,1]$ and using an absolute interval criterion, it will take about $15$ iterations to reach $L_{\\text{tol}} = 10^{-3}$, and this count is essentially independent of the flatness of $f$.**\n-   **Analysis:** This statement perfectly matches our derivation. The number of iterations is a direct consequence of the geometric progression $L_N = \\rho^N L_0$ and the termination condition $L_N \\le L_{\\text{tol}}$. Our calculation confirms that for the given parameters, $N=15$ iterations are required. This process is independent of the function's curvature (\"flatness\").\n-   **Verdict:** Correct.\n\n**C. The method may not terminate on a flat quartic because the function values at interior points can become numerically indistinguishable, causing an infinite loop unless second derivatives are supplied.**\n-   **Analysis:** This addresses a potential floating-point arithmetic limitation, not a fundamental property of the algorithm itself. In exact arithmetic, for any two distinct points $x_1$ and $x_2$, the function values $(x_1-x^{\\star})^4$ and $(x_2-x^{\\star})^4$ are distinct, so a comparison is always possible. Even with floating-point numbers, while the comparison $f(x_1)  f(x_2)$ might become unreliable if $|f(x_1) - f(x_2)|$ is smaller than machine precision, this does not cause an \"infinite loop\". The algorithm would still make a choice (perhaps arbitrarily) and reduce the interval. The interval length will still decrease geometrically at each step, guaranteeing that the termination criterion $L_N \\le L_{\\text{tol}}$ is eventually met. The problem might be that the *true* minimum is discarded due to a wrong comparison, but the algorithm *will* terminate. The suggestion to supply second derivatives is irrelevant, as the golden-section search algorithm has no provision to use them.\n-   **Verdict:** Incorrect.\n\n**D. On flat plateaus, one should replace the interval-length criterion with a function-value tolerance (stop when $|f(x_{1}) - f(x_{2})| \\le \\varepsilon_{f}$) to guarantee accurate localization of $x^{\\star}$; this improves termination reliability compared with interval-length stopping.**\n-   **Analysis:** This statement proposes a strategy that is counterproductive for flat functions. On a flat plateau, the function values $f(x)$ are nearly constant over a wide range of $x$. A stopping criterion based on the difference in function values, such as $|f(x_1) - f(x_2)| \\le \\varepsilon_f$, would be satisfied very early in the search, while the interval containing the minimizer $[a_k, b_k]$ is still large. This leads to poor localization of the minimizer's position, $x^{\\star}$. An interval-length criterion, $L_k \\le L_{\\text{tol}}$, is precisely the correct choice to guarantee that $x^{\\star}$ is localized within a small, user-specified interval, regardless of the function's flatness.\n-   **Verdict:** Incorrect.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "In many scientific and engineering applications, from tuning parameters in a machine learning model to conducting physical experiments, we cannot evaluate an objective function with perfect precision. This final practice  moves from idealized theory to a more realistic scenario where function evaluations are subject to noise or uncertainty. You are tasked with designing and implementing a modified golden-section search that can operate with interval-valued feedback, developing rules for making decisions and allocating an evaluation budget to robustly find the minimum despite the uncertainty.",
            "id": "3196296",
            "problem": "You are given the task of implementing a robust bracketing algorithm for minimizing a unimodal function when each function query at a point returns an interval due to uncertainty. A function is unimodal on an interval if there exists a point where it transitions from strictly decreasing to strictly increasing. Specifically, suppose the goal is to minimize a function $f(x)$ known to be unimodal on a closed interval $[a,b]$. The available oracle for pointwise evaluation does not return a single value; instead, for a query at a point $x$ with $n$ accumulated samples at $x$, it returns an interval $[f^{-}(x,n), f^{+}(x,n)]$, where you are guaranteed that the true value satisfies $f(x) \\in [f^{-}(x,n), f^{+}(x,n)]$ and the half-width obeys $f^{+}(x,n) - f(x) = f(x) - f^{-}(x,n) = s \\cdot n^{-1/2}$ for a known noise-scaling parameter $s  0$. You may adaptively increase $n$ at selected points $x$ by requesting additional samples; each additional sample at $x$ increases $n$ by $1$, shrinking the interval half-width deterministically to $s \\cdot n^{-1/2}$. Each single-sample increment consumes one unit of a global evaluation budget $N_{\\max}$.\n\nYour program must implement a golden-section-style bracketing algorithm adapted to interval-valued feedback under the following requirements.\n\n- Fundamental base and modeling assumptions:\n  - The function $f(x)$ is unimodal on $[a,b]$.\n  - The oracle returns interval feedback $[f^{-}(x,n), f^{+}(x,n)]$ with exact containment and half-width $s \\cdot n^{-1/2}$ for the $n$-th sample accumulation at $x$.\n  - The parameter $s$ is given and fixed for a test case. The budget $N_{\\max}$ is given and fixed for a test case.\n  - Initialize every newly introduced interior point with $n=1$ sample, which consumes $1$ unit from the budget and yields initial interval $[f(x)-s, f(x)+s]$.\n\n- Interval-aware bracketing rules:\n  - Maintain a current bracket $[a_k,b_k]$ and two interior points $c_k$ and $d_k$ satisfying $a_k  c_k  d_k  b_k$. Construct $c_k$ and $d_k$ using the golden-section placement so that when the bracket shrinks, one interior point is reused at the next iteration without recomputation of its location.\n  - Let $[f^{-}(c_k,n_c), f^{+}(c_k,n_c)]$ and $[f^{-}(d_k,n_d), f^{+}(d_k,n_d)]$ denote the current intervals at $c_k$ and $d_k$ based on $n_c$ and $n_d$ samples accumulated at each point.\n  - Define the dominance relation “certainly-less-than” by the following: $c_k$ is certainly better than $d_k$ if $f^{+}(c_k,n_c)  f^{-}(d_k,n_d)$; symmetrically, $d_k$ is certainly better than $c_k$ if $f^{+}(d_k,n_d)  f^{-}(c_k,n_c)$.\n  - Elimination rules that preserve the bracketing of the unknown minimizer $x^{\\star}$ under unimodality:\n    - If $f^{+}(c_k,n_c)  f^{-}(d_k,n_d)$, then update the bracket to $[a_{k+1}, b_{k+1}] = [a_k, d_k]$ (eliminate the right subinterval $[d_k,b_k]$).\n    - If $f^{+}(d_k,n_d)  f^{-}(c_k,n_c)$, then update the bracket to $[a_{k+1}, b_{k+1}] = [c_k, b_k]$ (eliminate the left subinterval $[a_k,c_k]$).\n  - If neither dominance condition is satisfied, you may request additional samples at $c_k$ or $d_k$ to shrink the intervals. At each such step, increment $n_c$ or $n_d$ by $1$ (consuming $1$ unit of budget) at the point with larger current interval width; if widths are equal, break ties by alternating between $c_k$ and $d_k$.\n\n- Reuse property of the golden-section construction:\n  - Choose the interior points so that after an elimination step, one of the previous interior points becomes an interior point of the new bracket. This implies the well-known invariant ratio construction of the golden-section search that uses a fixed ratio for interior placement to achieve constant reuse and minimal new point evaluations per iteration. You must derive this ratio from first principles in your solution.\n\n- Termination:\n  - Terminate when the bracket width satisfies $b_k - a_k \\le \\varepsilon$, where $\\varepsilon$ is a given tolerance, or when the total budget $N_{\\max}$ is exhausted. If an elimination step is decided but no budget remains to initialize a newly introduced interior point, stop immediately after updating the bracket.\n  - The returned estimate of the minimizer must be the midpoint $(a_k + b_k)/2$ at termination.\n\n- Output format:\n  - For each test case, your program must output the final midpoint estimate as a floating-point number. Round each result to exactly $6$ digits after the decimal point.\n  - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[r_1,r_2,r_3,r_4]$ for four test cases, where each $r_i$ is the rounded midpoint estimate as specified above.\n\nImplement your program to solve the following test suite. All functions and parameters are expressed in pure mathematics; there are no angle or physical units. Ensure scientific realism by respecting the unimodality and the deterministic confidence bounds.\n\n- Test case $1$ (happy path with low uncertainty and ample budget):\n  - Function: $f_1(x) = (x - 2)^2$.\n  - Initial bracket: $[a,b] = [0,5]$.\n  - Noise parameter: $s = 0.1$.\n  - Tolerance: $\\varepsilon = 10^{-6}$.\n  - Budget: $N_{\\max} = 300$.\n\n- Test case $2$ (high uncertainty and tight budget):\n  - Function: $f_1(x) = (x - 2)^2$.\n  - Initial bracket: $[a,b] = [0,5]$.\n  - Noise parameter: $s = 2.0$.\n  - Tolerance: $\\varepsilon = 10^{-6}$.\n  - Budget: $N_{\\max} = 40$.\n\n- Test case $3$ (different unimodal shape):\n  - Function: $f_2(x) = e^{x} - 3x$.\n  - Initial bracket: $[a,b] = [0,2]$.\n  - Noise parameter: $s = 0.2$.\n  - Tolerance: $\\varepsilon = 10^{-5}$.\n  - Budget: $N_{\\max} = 200$.\n\n- Test case $4$ (moderate uncertainty, moderate budget, different location of minimizer):\n  - Function: $f_3(x) = (x+1)^2 + 1$.\n  - Initial bracket: $[a,b] = [-3,0]$.\n  - Noise parameter: $s = 0.05$.\n  - Tolerance: $\\varepsilon = 0.5$.\n  - Budget: $N_{\\max} = 50$.\n\nYour program must implement the interval-aware golden-section bracketing method as above and produce a single line containing the four rounded midpoint estimates as a comma-separated list in square brackets.",
            "solution": "The posed problem requires the implementation of a golden-section search algorithm adapted for optimizing a unimodal function $f(x)$ on an interval $[a,b]$ where function evaluations are subject to a specific model of uncertainty. Each function query at a point $x$ returns an interval $[f^{-}(x,n), f^{+}(x,n)]$ that is guaranteed to contain the true value $f(x)$. The half-width of this interval is given by $s \\cdot n^{-1/2}$, where $s > 0$ is a known noise-scaling parameter and $n$ is the number of samples accumulated at $x$. The algorithm must operate within a total budget of $N_{\\max}$ single-sample evaluations.\n\nThe solution proceeds first by deriving the fundamental constant for the golden-section search from first principles, and then by detailing the iterative algorithm that incorporates interval-based comparisons.\n\n### Derivation of the Golden-Section Ratio\n\nThe core of the golden-section search is its reuse property: after reducing the search bracket, one of the previously computed interior points can be reused in the next iteration. This minimizes the number of new function evaluations required per step. We derive the placement ratio that enables this property.\n\nLet the search interval at iteration $k$ be $[a_k, b_k]$ of length $L_k = b_k - a_k$. We introduce two interior points, $c_k$ and $d_k$, such that $a_k  c_k  d_k  b_k$. For symmetry and reuse, we define their positions relative to the interval endpoints using a single ratio $\\tau \\in (1/2, 1)$:\n$$ c_k = b_k - \\tau \\cdot L_k = a_k + (1-\\tau) \\cdot L_k $$\n$$ d_k = a_k + \\tau \\cdot L_k $$\nThe condition $\\tau > 1/2$ ensures $c_k  d_k$.\n\nThe algorithm proceeds by comparing function values (or their intervals) at $c_k$ and $d_k$ to eliminate a portion of the interval. Suppose the comparison leads to the elimination of the subinterval $[a_k, c_k]$. The new bracket becomes $[a_{k+1}, b_{k+1}] = [c_k, b_k]$. The length of this new bracket is:\n$$ L_{k+1} = b_k - c_k = b_k - (b_k - \\tau \\cdot L_k) = \\tau \\cdot L_k $$\nThe single interior point from the previous step that remains in the new bracket is $d_k$. For the reuse property to hold, $d_k$ must coincide with one of the new interior points, $c_{k+1}$ or $d_{k+1}$. The new interior points are located at:\n$$ c_{k+1} = b_{k+1} - \\tau \\cdot L_{k+1} = b_k - \\tau^2 \\cdot L_k $$\n$$ d_{k+1} = a_{k+1} + \\tau \\cdot L_{k+1} = c_k + \\tau^2 \\cdot L_k $$\nLet us check which of these could be equal to $d_k$. The old point $d_k$ is situated further from the new left endpoint $a_{k+1}=c_k$ than the midpoint of $[c_k, b_k]$, so we expect it to become the new right interior point, $d_{k+1}$. However, due to the symmetric definition, it is simpler to check against the general formula related to the new endpoints. It is conventional for the point closer to $a_k$ to be reused as the point closer to $b_{k+1}$ in the case of eliminating $[d_k, b_k]$, and vice-versa. Let's analyze the other case.\n\nSuppose we eliminate $[d_k, b_k]$. The new bracket is $[a_{k+1}, b_{k+1}] = [a_k, d_k]$. The new length is:\n$$ L_{k+1} = d_k - a_k = (a_k + \\tau \\cdot L_k) - a_k = \\tau \\cdot L_k $$\nThe point $c_k$ is within this new bracket. We require $c_k$ to be one of the new interior points, $c_{k+1}$ or $d_{k+1}$.\n$$ c_{k+1} = b_{k+1} - \\tau \\cdot L_{k+1} = d_k - \\tau^2 \\cdot L_k $$\n$$ d_{k+1} = a_{k+1} + \\tau \\cdot L_{k+1} = a_k + \\tau^2 \\cdot L_k $$\nLet's test the hypothesis $c_k = d_{k+1}$:\n$$ a_k + (1-\\tau) \\cdot L_k = a_k + \\tau^2 \\cdot L_k $$\n$$ 1 - \\tau = \\tau^2 $$\nThis yields the quadratic equation $\\tau^2 + \\tau - 1 = 0$. Solving for $\\tau > 0$:\n$$ \\tau = \\frac{-1 + \\sqrt{1^2 - 4(1)(-1)}}{2} = \\frac{\\sqrt{5}-1}{2} $$\nThis value, approximately $0.618034$, is the reciprocal of the golden ratio $\\phi = (1+\\sqrt{5})/2$. This $\\tau$ is the required ratio for point placement. The distance of the interior points from the nearest endpoint will be $(1-\\tau)L_k$, while the distance from the farthest endpoint will be $\\tau L_k$. Since $1-\\tau = \\tau^2$, the points are at distance $\\tau^2 L_k$ from the nearest endpoint.\n\n### Interval-Aware Bracketing Algorithm\n\nThe standard golden-section search is adapted to handle interval-valued feedback. The core logic remains the same: maintain a bracket $[a,b]$ and two interior points $c$ and $d$. However, the comparison step is modified.\n\n**State Variables:**\n- $a, b$: The endpoints of the current bracket.\n- $c_x, d_x$: The positions of the two interior points.\n- $c_n, d_n$: The number of samples accumulated at $c_x$ and $d_x$, respectively.\n- $c_{ftrue}, d_{ftrue}$: The true (but unknown to the algorithm except via the oracle) function values at $c_x$ and $d_x$. These are used to generate the uncertainty intervals.\n- $N_{used}$: The total number of single-sample evaluations consumed.\n- $N_{\\max}$: The maximum budget for evaluations.\n- $\\varepsilon$: The desired final bracket width tolerance.\n\n**Initialization:**\n1.  Initialize $a$ and $b$ from the problem statement. Set $N_{used} = 0$.\n2.  The defined ratio is $\\tau = (\\sqrt{5}-1)/2$.\n3.  Check if the initial budget $N_{\\max}$ is at least $2$. If not, terminate, as two initial points cannot be evaluated.\n4.  Calculate the initial interior points: $c_x = a + (1-\\tau)(b-a)$ and $d_x = b - (1-\\tau)(b-a) = a + \\tau(b-a)$.\n5.  Evaluate the function at $c_x$ and $d_x$, each with $n=1$ sample. This consumes $2$ units of budget. Let $c_{ftrue} = f(c_x)$ and $d_{ftrue} = f(d_x)$, and set $c_n=1$, $d_n=1$, $N_{used}=2$.\n\n**Iterative Process:**\nThe algorithm proceeds in a loop, which terminates when $b-a \\le \\varepsilon$ or $N_{used} \\ge N_{\\max}$.\n\nAt each iteration $k$:\n1.  Calculate the current uncertainty intervals for $c_k$ and $d_k$:\n    - For $c_k$: $[f^{-}(c_k), f^{+}(c_k)]$ where $f^{\\pm}(c_k) = c_{ftrue} \\pm s/\\sqrt{c_n}$.\n    - For $d_k$: $[f^{-}(d_k), f^{+}(d_k)]$ where $f^{\\pm}(d_k) = d_{ftrue} \\pm s/\\sqrt{d_n}$.\n\n2.  Apply the interval-aware elimination rules:\n    - **Case 1: $c_k$ is certainly better than $d_k$**. This holds if $f^{+}(c_k)  f^{-}(d_k)$. The minimum $x^\\star$ cannot be in $[d_k,b_k]$ due to unimodality.\n        - Update the bracket: $b_{k+1} = d_k$.\n        - Reuse the point $c_k$ as the new $d_{k+1}$: set $d_{k+1} \\leftarrow c_k$ (i.e., $d_{x} \\leftarrow c_{x}$, $d_{n} \\leftarrow c_{n}$, $d_{ftrue} \\leftarrow c_{ftrue}$).\n        - Calculate the new interior point $c_{k+1} = a_{k+1} + (1-\\tau)(b_{k+1}-a_{k+1})$.\n        - If budget remains ($N_{used}  N_{\\max}$), evaluate $f(c_{k+1})$ with $n=1$, increment $N_{used}$, and update the state for $c_{k+1}$. Otherwise, terminate after the bracket update.\n\n    - **Case 2: $d_k$ is certainly better than $c_k$**. This holds if $f^{+}(d_k)  f^{-}(c_k)$. The minimum $x^\\star$ cannot be in $[a_k,c_k]$.\n        - Update the bracket: $a_{k+1} = c_k$.\n        - Reuse the point $d_k$ as the new $c_{k+1}$: set $c_{k+1} \\leftarrow d_k$.\n        - Calculate the new interior point $d_{k+1} = a_{k+1} + \\tau(b_{k+1}-a_{k+1})$.\n        - If budget remains ($N_{used}  N_{\\max}$), evaluate $f(d_{k+1})$ with $n=1$, increment $N_{used}$, and update state. Otherwise, terminate.\n\n    - **Case 3: Indecision**. Neither dominance condition is met; the intervals overlap. To resolve this, we must shrink one or both intervals by taking more samples.\n        - If budget is exhausted ($N_{used} \\ge N_{\\max}$), terminate.\n        - Otherwise, select the point with the currently larger interval width to sample. The width is $2s/\\sqrt{n}$. This is equivalent to selecting the point with the smaller sample count $n$.\n        - If $c_n  d_n$, increment $c_n$. If $d_n  c_n$, increment $d_n$.\n        - If $c_n = d_n$, break the tie by alternating between sampling $c_k$ and $d_k$.\n        - Increment $N_{used}$ by $1$. The algorithm then loops back to re-evaluate the dominance conditions with the new, smaller interval.\n\n**Termination and Output:**\nThe loop terminates when the bracket width $b-a$ is no longer greater than $\\varepsilon$, or when the budget $N_{used}$ reaches $N_{\\max}$. The final estimate for the minimizer is the midpoint of the final bracket, $(a+b)/2$.",
            "answer": "```python\nimport numpy as np\n\ndef f1(x: float) - float:\n    \"\"\"Unimodal function for test cases 1 and 2: f(x) = (x - 2)^2.\"\"\"\n    return (x - 2.0)**2\n\ndef f2(x: float) - float:\n    \"\"\"Unimodal function for test case 3: f(x) = e^x - 3x.\"\"\"\n    return np.exp(x) - 3.0 * x\n\ndef f3(x: float) - float:\n    \"\"\"Unimodal function for test case 4: f(x) = (x + 1)^2 + 1.\"\"\"\n    return (x + 1.0)**2 + 1.0\n\ndef golden_section_interval_search(f, a, b, s, eps, n_max):\n    \"\"\"\n    Performs a golden-section search for a unimodal function with interval-valued feedback.\n\n    Args:\n        f: The unimodal function to minimize.\n        a: The lower bound of the initial bracket.\n        b: The upper bound of the initial bracket.\n        s: The noise-scaling parameter for interval uncertainty.\n        eps: The tolerance for the final bracket width.\n        n_max: The maximum budget of function evaluations.\n\n    Returns:\n        The midpoint of the final bracket as an estimate of the minimizer.\n    \"\"\"\n    if n_max  2:\n        return (a + b) / 2.0\n\n    tau = (np.sqrt(5.0) - 1.0) / 2.0\n    budget_used = 0\n    \n    # Tie-breaking state for sampling when interval widths are equal\n    tie_break_c_next = True\n\n    # Initialize two interior points\n    c_x = a + (1.0 - tau) * (b - a)\n    d_x = a + tau * (b - a)\n    \n    # Evaluate initial points, consuming budget\n    c_ftrue = f(c_x)\n    c_n = 1\n    d_ftrue = f(d_x)\n    d_n = 1\n    budget_used = 2\n\n    while True:\n        if b - a = eps:\n            break\n        if budget_used = n_max:\n            break\n\n        # Calculate current interval bounds\n        c_half_width = s / np.sqrt(c_n)\n        c_f_plus = c_ftrue + c_half_width\n        c_f_minus = c_ftrue - c_half_width\n\n        d_half_width = s / np.sqrt(d_n)\n        d_f_plus = d_ftrue + d_half_width\n        d_f_minus = d_ftrue - d_half_width\n\n        # Case 1: c is certainly better, eliminate [d, b]\n        if c_f_plus  d_f_minus:\n            b = d_x\n            d_x, d_ftrue, d_n = c_x, c_ftrue, c_n\n            \n            c_x = a + (1.0 - tau) * (b - a)\n            if budget_used = n_max:\n                break\n            \n            c_ftrue = f(c_x)\n            c_n = 1\n            budget_used += 1\n\n        # Case 2: d is certainly better, eliminate [a, c]\n        elif d_f_plus  c_f_minus:\n            a = c_x\n            c_x, c_ftrue, c_n = d_x, d_ftrue, d_n\n\n            d_x = a + tau * (b - a)\n            if budget_used = n_max:\n                break\n            \n            d_ftrue = f(d_x)\n            d_n = 1\n            budget_used += 1\n            \n        # Case 3: Indecision, sample more at one point to shrink interval\n        else:\n            if c_half_width  d_half_width:\n                c_n += 1\n            elif d_half_width  c_half_width:\n                d_n += 1\n            else: # Tie-break by alternating\n                if tie_break_c_next:\n                    c_n += 1\n                    tie_break_c_next = False\n                else:\n                    d_n += 1\n                    tie_break_c_next = True\n            budget_used += 1\n            \n    return (a + b) / 2.0\n\ndef solve():\n    \"\"\"\n    Solves the provided test suite using the interval-aware golden-section search.\n    \"\"\"\n    test_cases = [\n        {'f': f1, 'a': 0.0, 'b': 5.0, 's': 0.1, 'eps': 1e-6, 'n_max': 300},\n        {'f': f1, 'a': 0.0, 'b': 5.0, 's': 2.0, 'eps': 1e-6, 'n_max': 40},\n        {'f': f2, 'a': 0.0, 'b': 2.0, 's': 0.2, 'eps': 1e-5, 'n_max': 200},\n        {'f': f3, 'a': -3.0, 'b': 0.0, 's': 0.05, 'eps': 0.5, 'n_max': 50},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = golden_section_interval_search(\n            case['f'], case['a'], case['b'], case['s'], case['eps'], case['n_max']\n        )\n        results.append(f\"{result:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        }
    ]
}