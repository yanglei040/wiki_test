{
    "hands_on_practices": [
        {
            "introduction": "第一个练习将带你实践牛顿法在非凸问题中面临的挑战。通过将该方法应用于一个简单的双势阱函数，你将亲眼看到纯牛顿步为何会失效，以及通过回溯线搜索实现的阻尼策略对于确保收敛是何等关键。这个实践也将帮助你理解不同的阻尼策略如何影响算法的性能。",
            "id": "3115942",
            "problem": "您需要在一个非凸一维测试函数上，实现并比较牛顿法框架内的两种阻尼策略。此比较必须是定量的：对于每种策略和每个指定的起始点，报告达到全局最小值（在规定容差范围内）所需的外层迭代次数。\n\n从以下基本要素开始：\n- 目标是一个二阶连续可微函数 $f:\\mathbb{R}\\to\\mathbb{R}$，其梯度为 $\\nabla f(x)$，Hessian矩阵（二阶导数）为 $f''(x)$。\n- 牛顿法源于最小化关于 $p$ 的二阶泰勒模型 $m_x(p) = f(x) + \\nabla f(x)\\,p + \\frac{1}{2} f''(x)\\,p^2$，这给出了求解 $f''(x)\\,p_N(x) = -\\nabla f(x)$ 的无阻尼牛顿步 $p_N(x)$。\n- 步长的阻尼使用回溯线搜索，该搜索满足Armijo（充分下降）条件 $f(x+\\alpha p) \\le f(x) + c_1\\,\\alpha\\,\\nabla f(x)\\,p$，其中 $c_1 \\in (0,1)$，步长 $\\alpha \\in (0,1]$ 是通过以收缩因子 $\\rho \\in (0,1)$ 连续缩减 $\\alpha \\leftarrow \\rho \\alpha$ 获得的。\n\n目标函数及已知性质：\n- 考虑非凸双阱势函数 $f(x) = x^4 - 2x^2$。其梯度和Hessian矩阵分别定义为 $\\nabla f(x) = 4x^3 - 4x$ 和 $f''(x) = 12x^2 - 4$。全局最小值点出现在 $x^\\star \\in \\{-1,+1\\}$，最优值为 $f(x^\\star) = -1$。\n\n为处理非凸性而修正的牛顿方向：\n- 当 $f''(x) \\le 0$（非正曲率）时，无阻尼牛顿步 $p_N(x) = -\\nabla f(x)/f''(x)$ 并不定义一个下降方向。在这种情况下，使用最速下降方向 $p(x) = -\\nabla f(x)$，这保证了当 $\\nabla f(x) \\ne 0$ 时，$\\nabla f(x)\\,p(x) = -\\|\\nabla f(x)\\|^2  0$。\n- 鞍点逃逸约定：如果 $|\\nabla f(x)| \\le \\varepsilon_g$ 且 $f''(x) \\le 0$（例如，$x=0$），通过选择一个单位方向 $p(x) = +1$ 来打破对称性，并依靠线搜索阻尼来确保函数值下降。\n\n待比较的阻尼策略：\n- 激进阻尼策略：Armijo参数 $c_1 = 10^{-4}$，收缩因子 $\\rho = 0.7$，初始步长 $\\alpha_0 = 1$，最大回溯步数 $B = 50$。\n- 保守阻尼策略：Armijo参数 $c_1 = 10^{-2}$，收缩因子 $\\rho = 0.3$，初始步长 $\\alpha_0 = 1$，最大回溯步数 $B = 50$。\n\n停止准则与迭代计数：\n- 设 $\\varepsilon_f = 10^{-10}$。当 $f(x_k) \\le -1 + \\varepsilon_f$ 时，宣告成功。只计算外层迭代次数 $k$（接受的步），不计回溯尝试次数。如果在 $K_{\\max} = 200$ 次外层迭代内未成功，则该次运行返回 $K_{\\max}$。\n- 在上述鞍点逃逸规则中使用梯度范数阈值 $\\varepsilon_g = 10^{-12}$。\n\n在当前点 $x_k$ 处，每次外层迭代 $k$ 需要实现的算法摘要：\n1. 计算梯度 $\\nabla f(x_k)$ 和Hessian矩阵 $f''(x_k)$。\n2. 按如下方式选择方向 $p_k$：\n   - 如果 $f''(x_k) > 0$，则设置 $p_k = -\\nabla f(x_k)/f''(x_k)$。\n   - 否则，如果 $|\\nabla f(x_k)| > \\varepsilon_g$，则设置 $p_k = -\\nabla f(x_k)$。\n   - 否则，设置 $p_k = +1$。\n3. 从 $\\alpha_0 = 1$ 开始，对 $\\alpha$ 进行回溯，通过 $\\alpha \\leftarrow \\rho \\alpha$ 进行缩减，直到满足Armijo条件 $f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)p_k$ 或尝试了 $B$ 次缩减。使用最终获得的 $\\alpha$。\n4. 更新 $x_{k+1} = x_k + \\alpha p_k$。如果 $f(x_{k+1}) \\le -1 + \\varepsilon_f$，则停止并报告 $k+1$；否则继续迭代，直到达到上限 $K_{\\max}$。\n\n测试集：\n- 针对以下起始点 $x_0$ 评估两种阻尼策略：\n  1. $x_0 = 2.0$（理想路径，远右阱）。\n  2. $x_0 = -2.0$（理想路径，远左阱）。\n  3. $x_0 = 0.2$（靠近具有负曲率的中心区域）。\n  4. $x_0 = 0.0$（精确鞍点，梯度为零）。\n  5. $x_0 = 0.57735$（靠近拐点，此处 $f''(x) \\approx 0$）。\n  6. $x_0 = -0.57735$（与前一情况对称）。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的、逗号分隔的对列表形式的结果，每个测试用例对应一对。每对的形式为 $[n_{\\text{aggressive}},n_{\\text{conservative}}]$，不含空格，其中 $n_{\\text{aggressive}}$ 和 $n_{\\text{conservative}}$ 是整数。例如：$[[a_1,b_1],[a_2,b_2],\\dots,[a_6,b_6]]$。\n\n角度、物理单位和百分比不适用于此纯粹的数学任务；无需进行单位转换。",
            "solution": "用户要求实现并比较一种改进的牛顿法中的两种不同阻尼策略，用于最小化一个一维非凸函数。\n\n### 步骤1：提取给定信息\n\n- **目标函数**：$f(x) = x^4 - 2x^2$，其中 $x \\in \\mathbb{R}$。\n- **梯度**：$\\nabla f(x) = 4x^3 - 4x$。\n- **Hessian矩阵**：$f''(x) = 12x^2 - 4$。\n- **全局最小值点**：$x^\\star \\in \\{-1, +1\\}$，其最优值为 $f(x^\\star) = -1$。\n- **在 $x_k$ 处的修正搜索方向 $p_k$**：\n    - 如果 $f''(x_k) > 0$：$p_k = -\\nabla f(x_k)/f''(x_k)$ (牛顿步)。\n    - 否则，如果 $|\\nabla f(x_k)| > \\varepsilon_g$：$p_k = -\\nabla f(x_k)$ (最速下降)。\n    - 否则 (即 $f''(x_k) \\le 0$ 且 $|\\nabla f(x_k)| \\le \\varepsilon_g$)：$p_k = +1$ (鞍点逃逸)。\n- **线搜索**：满足Armijo条件 $f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k) p_k$ 的回溯。\n    - 初始步长：$\\alpha_0 = 1$。\n    - 收缩：$\\alpha \\leftarrow \\rho \\alpha$。\n    - 最大回溯步数：$B=50$。\n- **阻尼策略**：\n    - 激进策略：Armijo参数 $c_1 = 10^{-4}$，收缩因子 $\\rho = 0.7$。\n    - 保守策略：Armijo参数 $c_1 = 10^{-2}$，收缩因子 $\\rho = 0.3$。\n- **容差和限制**：\n    - 成功时的函数值容差：$\\varepsilon_f = 10^{-10}$。如果 $f(x_k) \\le -1 + \\varepsilon_f$，则宣告成功。\n    - 用于鞍点逃逸的梯度范数阈值：$\\varepsilon_g = 10^{-12}$。\n    - 最大外层迭代次数：$K_{\\max} = 200$。\n- **测试集（起始点 $x_0$）**：$2.0, -2.0, 0.2, 0.0, 0.57735, -0.57735$。\n\n### 步骤2：使用提取的信息进行验证\n\n- **科学依据**：该问题基于数值优化的标准、公认的原则，特别是牛顿法、线搜索技术（Armijo条件）以及针对非凸函数的修正。函数及其导数均已正确指定。\n- **适定性**：问题定义清晰，包含所有必要的参数（$c_1, \\rho, \\varepsilon_f, \\varepsilon_g, K_{\\max}, B$）、一个特定的算法和一组测试用例。预期会得到一个唯一且有意义的解（每种情况下的迭代次数）。\n- **客观性**：问题使用精确的数学语言，并为性能评估和终止设定了客观的、定量的标准。它不含主观论断。\n\n该问题是自洽、一致且科学合理的。它没有违反任何无效性标准。\n\n### 步骤3：结论与行动\n\n问题陈述是**有效的**。将提供完整解决方案。\n\n### 求解推导\n\n问题的核心是实现一个稳健的、带线搜索的牛顿法，能够处理非凸性。这是通过在Hessian矩阵非正定时修正搜索方向来实现的，确保所选方向始终是下降方向（或在鞍点处的特殊逃逸方向），并且步长能保证目标函数有充分的下降。\n\n**1. 牛顿法基础**\n用于优化的标准牛顿法在点 $x_k$ 处使用二次模型来近似目标函数 $f(x)$：\n$$m_{x_k}(p) = f(x_k) + \\nabla f(x_k) p + \\frac{1}{2} f''(x_k) p^2$$\n最小化此模型的步长 $p$ 即为牛顿步 $p_N(x_k)$，通过求解 $\\nabla m_{x_k}(p) = 0$ 得到，这会产生线性系统 $f''(x_k) p = -\\nabla f(x_k)$。对于我们的一维情况，这简化为 $p_k = -\\nabla f(x_k) / f''(x_k)$。只有当Hessian矩阵 $f''(x_k)$ 为正定时，才能保证此步长是一个下降方向（即 $\\nabla f(x_k) p_k  0$）。\n\n**2. 处理非凸性**\n目标函数 $f(x) = x^4 - 2x^2$ 是非凸的。其Hessian矩阵 $f''(x) = 12x^2 - 4$ 在区间 $|x| \\le 1/\\sqrt{3} \\approx 0.57735$ 内是非正的（$f''(x) \\le 0$）。在此区域内，纯牛顿步不是一个下降方向，并可能指向一个最大值点。指定的算法通过一个标准修正来解决此问题：\n- 如果 $f''(x_k) > 0$，则该区域是局部凸的，使用牛顿步 $p_k = -\\nabla f(x_k)/f''(x_k)$。\n- 如果 $f''(x_k) \\le 0$，纯牛顿步是不可靠的。算法会转而使用最速下降法，即使用方向 $p_k = -\\nabla f(x_k)$。只要梯度非零，这个方向就始终是下降方向，因为方向导数为 $\\nabla f(x_k) p_k = -\\|\\nabla f(x_k)\\|^2  0$。\n- 在像 $x_k=0$ 这样的鞍点处会出现一个特殊情况，此时 $\\nabla f(0) = 0$ 且 $f''(0) = -4 \\le 0$。在这里，牛顿方向和最速下降方向都为零，导致算法停滞。问题指定了一个鞍点逃逸机制：如果在非正曲率区域内，梯度小于一个容差 $\\varepsilon_g = 10^{-12}$，则选择一个固定方向 $p_k=+1$ 来打破对称性，并将迭代点移离鞍点。\n\n**3. 带回溯线搜索的阻尼步**\n即使有了一个有效的下降方向，一个完整的步长 $x_{k+1} = x_k + p_k$ 也可能增加函数值。通过线搜索寻找步长 $\\alpha \\in (0, 1]$ 的阻尼方法可以确保进展。该算法采用回溯线搜索来满足Armijo（或充分下降）条件：\n$$f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k) p_k$$\n这里，$c_1 \\in (0,1)$ 是一个参数。项 $\\nabla f(x_k) p_k$ 是 $f$ 在 $x_k$ 点沿 $p_k$ 方向的方向导数。该条件要求实际的函数值减少量至少是函数线性近似所预测减少量的一小部分 $c_1$。算法从一个完整步长 $\\alpha=1$ 开始，并以一个因子 $\\rho \\in (0,1)$ 连续减小它（即 $\\alpha \\leftarrow \\rho \\alpha$），直到条件满足。\n\n**4. 阻尼策略比较**\n这两种策略由其参数 $c_1$ 和 $\\rho$ 来区分：\n- **激进策略 ($c_1 = 10^{-4}, \\rho = 0.7$)**：较小的 $c_1$ 值使得Armijo条件更容易满足，从而鼓励采用更大的步长。较大的收缩因子 $\\rho$ 意味着在回溯过程中步长减小得更慢。这种策略试图取得快速进展，但有超调的风险。\n- **保守策略 ($c_1 = 10^{-2}, \\rho = 0.3$)**：较大的 $c_1$ 值对函数下降提出了更严格的要求，导致步长更小、更谨慎。如果初始步长不佳，较小的收缩因子 $\\rho$ 会在回溯期间积极地减小步长。这种策略通常更稳健，但收敛速度可能更慢。\n\n**5. 实现算法**\n对于每个起始点 $x_0$ 和每种策略，执行以下迭代过程：\n1. 初始化 $k=0$ 和 $x_0$。\n2. 对于 $k$ 从 $0$ 到 $K_{\\max}-1$：\n    a. 计算 $f(x_k)$、$\\nabla f(x_k)$ 和 $f''(x_k)$。\n    b. 使用修正的牛顿逻辑选择搜索方向 $p_k$。\n    c. 执行回溯线搜索：\n        i. 初始化 $\\alpha = 1$。\n        ii. 对于 $j=0$ 到 $B-1$：\n            如果 $f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k) p_k$，则跳出内循环。\n            否则，$\\alpha \\leftarrow \\rho \\alpha$。\n    d. 更新迭代点：$x_{k+1} = x_k + \\alpha p_k$。\n    e. 检查收敛性：如果 $f(x_{k+1}) \\le -1 + \\varepsilon_f$，则停止并报告总外层迭代次数 $k+1$。\n3. 如果循环完成但未收敛，则报告 $K_{\\max}$。\n\n将此过程系统地应用于所有测试用例，以生成所需的比较数据。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares two damping strategies for a modified Newton method\n    on a nonconvex 1D function.\n    \"\"\"\n\n    def f(x):\n        \"\"\"Objective function.\"\"\"\n        return x**4 - 2 * x**2\n\n    def grad_f(x):\n        \"\"\"Gradient of the objective function.\"\"\"\n        return 4 * x**3 - 4 * x\n\n    def hess_f(x):\n        \"\"\"Hessian of the objective function.\"\"\"\n        return 12 * x**2 - 4\n\n    def run_newton_method(x0, c1, rho):\n        \"\"\"\n        Executes the modified Newton method for a given starting point and damping policy.\n\n        Args:\n            x0 (float): The starting point.\n            c1 (float): The Armijo condition parameter.\n            rho (float): The backtracking line search contraction factor.\n\n        Returns:\n            int: The number of outer iterations required for convergence, or K_max if not converged.\n        \"\"\"\n        x = float(x0)\n        \n        # Constants and tolerances\n        K_max = 200\n        B_max = 50\n        eps_f = 1e-10\n        eps_g = 1e-12\n        f_target = -1.0 + eps_f\n\n        for k in range(K_max):\n            # Check for convergence at the beginning of the iteration\n            if f(x) = f_target:\n                return k\n\n            grad_fx = grad_f(x)\n            hess_fx = hess_f(x)\n\n            # Step 2: Choose the search direction p_k\n            if hess_fx > 0:\n                p = -grad_fx / hess_fx\n            elif abs(grad_fx) > eps_g:\n                p = -grad_fx\n            else: # Saddle point escape\n                p = 1.0\n\n            # Step 3: Backtracking line search for step size alpha\n            alpha = 1.0\n            fx = f(x)\n            \n            # Pre-calculate the directional derivative for the Armijo check\n            directional_deriv = grad_fx * p\n            \n            for _ in range(B_max):\n                x_new = x + alpha * p\n                fx_new = f(x_new)\n                \n                # Armijo condition\n                if fx_new = fx + c1 * alpha * directional_deriv:\n                    break\n                \n                alpha *= rho\n            else:\n                # If backtracking fails to find a step after B_max attempts,\n                # we proceed with the last computed (and very small) alpha.\n                # In many cases, this may stall the algorithm, leading to K_max.\n                x_new = x + alpha * p\n\n            # Step 4: Update and check for convergence for the next iteration\n            x = x_new\n            \n            if k == K_max - 1 and f(x) = f_target:\n                 return k + 1\n\n        return K_max\n\n    # Define the test suite and damping policies\n    test_cases = [\n        2.0,\n        -2.0,\n        0.2,\n        0.0,\n        0.57735,\n        -0.57735,\n    ]\n    \n    aggressive_policy = {'c1': 1e-4, 'rho': 0.7}\n    conservative_policy = {'c1': 1e-2, 'rho': 0.3}\n\n    results = []\n    for x0_val in test_cases:\n        # Run with aggressive policy\n        n_aggressive = run_newton_method(x0_val, **aggressive_policy)\n        \n        # Run with conservative policy\n        n_conservative = run_newton_method(x0_val, **conservative_policy)\n        \n        results.append(f\"[{n_aggressive},{n_conservative}]\")\n\n    # Print the final output in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "在之前概念的基础上，本实践将展示一种更稳健的方法来处理不定海森矩阵，而不仅仅是切换到最速下降法。你将实现一个使用特征值裁剪的修正牛顿法，该技术系统地确保搜索方向始终是下降方向。通过在一个含有鞍点的函数上，将这种稳健方法与简单的阻尼牛顿法进行比较，你将清楚地看到海森矩阵修正对于实现全局收敛的重要性。",
            "id": "3115922",
            "problem": "考虑无约束最小化问题，其中光滑函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 按分量定义为\n$$\nf(x) \\;=\\; \\sum_{i=1}^n \\big(x_i^2 - 1\\big)^2.\n$$\n该函数有多个全局最小值点，这些点的坐标全为 $+1$ 或 $-1$。它在原点处有一个鞍点。对于此 $f(x)$，其梯度和Hessian矩阵由众所周知的定义给出\n$$\n\\nabla f(x) \\;=\\; \\left[\\frac{\\partial f}{\\partial x_1}(x),\\dots,\\frac{\\partial f}{\\partial x_n}(x)\\right]^\\top,\\qquad \\nabla^2 f(x) \\;=\\; \\left[\\frac{\\partial^2 f}{\\partial x_i \\partial x_j}(x)\\right]_{i,j=1}^n,\n$$\n并且可以从第一性原理验证\n$$\n\\nabla f(x)_i \\;=\\; 4\\,x_i\\,(x_i^2-1),\\qquad \\nabla^2 f(x) \\;=\\; \\mathrm{diag}\\big(12 x_1^2 - 4,\\,\\dots,\\,12 x_n^2 - 4\\big).\n$$\n在坐标 $x_i\\approx 0$ 附近，曲率 $12 x_i^2 - 4$ 为负，因此Hessian矩阵是不定的，经典的牛顿方向不一定是下降方向。\n\n您的任务是编写一个完整的程序，为该函数 $f(x)$ 实现并比较两种二阶优化方法：\n\n1) 带回溯线搜索的阻尼牛顿法：\n- 在迭代点 $x_k$ 处，通过求解 $\\nabla^2 f(x_k)\\,p_k = -\\nabla f(x_k)$ 来构建牛顿步 $p_k$。\n- 如果 $p_k$ 不是下降方向，即 $\\nabla f(x_k)^\\top p_k \\ge 0$，则声明此测试用例失败（对于非下降方向，没有步长可以满足充分下降条件）。\n- 否则，使用回溯Armijo线搜索找到一个步长 $\\alpha_k \\in (0,1]$，使得\n$$\nf(x_k + \\alpha_k p_k) \\;\\le\\; f(x_k) + c_1\\,\\alpha_k\\,\\nabla f(x_k)^\\top p_k,\n$$\n其中Armijo常数 $c_1\\in(0,1)$ 和收缩因子 $\\beta\\in(0,1)$ 是固定的（使用 $c_1 = 10^{-4}$ 和 $\\beta = 1/2$）。\n- 更新 $x_{k+1} = x_k + \\alpha_k p_k$ 并重复，直到达到最大迭代次数或梯度范数足够小。\n\n2) 带有特征值裁剪和相同回溯线搜索的修正牛顿法：\n- 在迭代点 $x_k$ 处，对Hessian矩阵进行对称特征分解 $\\nabla^2 f(x_k) = Q \\Lambda Q^\\top$，其中 $Q$ 是正交矩阵，$\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)$ 是实对角矩阵。\n- 定义一个裁剪后的谱 $\\Lambda' = \\mathrm{diag}(\\lambda'_1,\\dots,\\lambda'_n)$，其中\n$$\n\\lambda'_i \\;=\\; \\max\\{\\lambda_i,\\;\\delta\\},\\qquad \\delta \\;=\\; 10^{-3}\\,\\max\\{1,\\;\\max_j|\\lambda_j|\\}.\n$$\n- 定义修正后的Hessian矩阵 $H_k' = Q \\Lambda' Q^\\top$（根据构造，该矩阵是对称正定的），并通过求解 $H_k' p_k = -\\nabla f(x_k)$ 来计算 $p_k$。\n- 由于 $H_k'$ 是对称正定的，只要 $\\nabla f(x_k)\\neq 0$，$p_k$ 就保证是一个严格下降方向。使用与上述相同的Armijo回溯法来获得 $\\alpha_k$ 并更新 $x_{k+1} = x_k + \\alpha_k p_k$。\n\n对于这两种方法，请使用以下实现参数，这些参数使问题完全确定且可测试：\n- 对于下面的所有测试用例，维度 $n=3$。\n- 最大迭代次数 $k_{\\max} = 100$。\n- 梯度范数容差 $\\|\\nabla f(x_k)\\|_2 \\le 10^{-8}$ 仅作为迭代停止条件；不要将任何驻点本身视为成功。\n- 当且仅当最终目标函数值满足 $f(x_{\\mathrm{final}}) \\le 10^{-10}$ 时，声明并记录测试用例成功，这对应于在数值精度内达到全局最小值点的盆地。否则声明失败。\n\n测试套件。对以下初始点列表运行两种方法：\n- 用例 T1（原点附近的负曲率）：$x_0 = (0.1,\\,0.1,\\,0.1)$。\n- 用例 T2（混合曲率，其中两个分量远在负曲率区域之外）：$x_0 = (2.0,\\,-2.0,\\,0.5)$。\n- 用例 T3（精确鞍点）：$x_0 = (0.0,\\,0.0,\\,0.0)$。\n- 用例 T4（远离最小值点，但具有正曲率）：$x_0 = (10.0,\\,10.0,\\,10.0)$。\n\n对于每个测试用例，运行两种方法，并按顺序记录每个用例的两个布尔值：\n- $b_{\\mathrm{DN},\\mathrm{T}i}$ 代表阻尼牛顿法在用例 $\\mathrm{T}i$ 上的结果，\n- $b_{\\mathrm{MN},\\mathrm{T}i}$ 代表带特征值裁剪的修正牛顿法在用例 $\\mathrm{T}i$ 上的结果，\n其中每个布尔值在方法根据 $f(x_{\\mathrm{final}}) \\le 10^{-10}$ 成功时为真，否则为假。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个逗号分隔的Python风格列表，该列表按此确切顺序包含八个布尔值：\n$$\n\\big[ b_{\\mathrm{DN},\\mathrm{T}1},\\; b_{\\mathrm{MN},\\mathrm{T}1},\\; b_{\\mathrm{DN},\\mathrm{T}2},\\; b_{\\mathrm{MN},\\mathrm{T}2},\\; b_{\\mathrm{DN},\\mathrm{T}3},\\; b_{\\mathrm{MN},\\mathrm{T}3},\\; b_{\\mathrm{DN},\\mathrm{T}4},\\; b_{\\mathrm{MN},\\mathrm{T}4} \\big].\n$$\n此问题不涉及物理单位。角度不适用。每个测试的最终答案是按规定指定的布尔值。",
            "solution": "用户提供的问题被评估为有效。\n\n### 第一步：提取已知信息\n- **要最小化的函数**：$f(x) = \\sum_{i=1}^n (x_i^2 - 1)^2$，其中 $f:\\mathbb{R}^n\\to\\mathbb{R}$。\n- **梯度**：$\\nabla f(x)_i = 4x_i(x_i^2-1)$。\n- **Hessian矩阵**：$\\nabla^2 f(x) = \\mathrm{diag}(12 x_1^2 - 4, \\dots, 12 x_n^2 - 4)$。\n- **维度**：$n=3$。\n- **方法1（阻尼牛顿法）**：\n    - 牛顿步：求解 $\\nabla^2 f(x_k) p_k = -\\nabla f(x_k)$。\n    - 失败条件：如果 $p_k$ 不是下降方向，即 $\\nabla f(x_k)^\\top p_k \\ge 0$，则声明失败。\n    - 线搜索：回溯Armijo准则，$f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k$。\n    - 线搜索参数：Armijo常数 $c_1 = 10^{-4}$，收缩因子 $\\beta = 1/2$。\n- **方法2（修正牛顿法）**：\n    - 特征分解：$\\nabla^2 f(x_k) = Q \\Lambda Q^\\top$。\n    - 特征值裁剪：$\\lambda'_i = \\max\\{\\lambda_i, \\delta\\}$。\n    - 裁剪阈值：$\\delta = 10^{-3} \\max\\{1, \\max_j|\\lambda_j|\\}$。\n    - 修正后的Hessian矩阵：$H_k' = Q \\Lambda' Q^\\top$。\n    - 修正步：求解 $H_k' p_k = -\\nabla f(x_k)$。\n    - 线搜索：与方法1相同。\n- **通用参数**：\n    - 最大迭代次数：$k_{\\max} = 100$。\n    - 迭代停止条件：$\\|\\nabla f(x_k)\\|_2 \\le 10^{-8}$。\n    - 成功准则：最终目标函数值 $f(x_{\\mathrm{final}}) \\le 10^{-10}$。\n- **测试套件（初始点 $x_0$）**：\n    - T1: $(0.1, 0.1, 0.1)$。\n    - T2: $(2.0, -2.0, 0.5)$。\n    - T3: $(0.0, 0.0, 0.0)$。\n    - T4: $(10.0, 10.0, 10.0)$。\n- **输出**：一个包含8个布尔值的列表，指示每种方法在每个测试用例上的成功（$True$）或失败（$False$）。\n\n### 第二步：使用提取的已知信息进行验证\n该问题在科学上和数学上都是合理的。\n- **科学依据**：该问题是数值优化领域的标准练习，该领域是应用数学和计算机科学的一个子领域。函数、其导数以及优化算法（阻尼和修正牛顿法）在该领域都是典型的。\n- **适定性**：该问题是明确定义的。所有参数、条件和成功/失败的标准都已明确给出，从而对每个测试用例产生唯一、可验证的结果。算法是确定性的。\n- **客观性**：语言是精确和定量的。成功和失败由客观的数值阈值定义。\n\n该问题没有表现出任何无效性缺陷。它是完整的、一致的且可形式化的。\n\n### 第三步：结论与行动\n该问题是**有效的**。将提供一个解决方案。\n\n***\n\n### 求解推导\n任务是实现并比较两种二阶优化方法，用于最小化函数 $f(x) = \\sum_{i=1}^3 (x_i^2 - 1)^2$。问题的核心在于每种方法如何处理非正定的Hessian矩阵 $\\nabla^2 f(x)$。\n\n函数 $f(x)$ 的梯度和Hessian矩阵如下：\n$$\n\\nabla f(x)_i = 4x_i(x_i^2 - 1)\n$$\n$$\n\\nabla^2 f(x) = \\mathrm{diag}(12x_1^2 - 4, 12x_2^2 - 4, 12x_3^2 - 4)\n$$\nHessian矩阵是一个对角矩阵。这极大地简化了计算。对于任何分量 $i$，Hessian矩阵对应的特征值是 $\\lambda_i = 12x_i^2 - 4$。Hessian矩阵是正定的当且仅当其所有特征值都为正，即对于所有 $i=1, 2, 3$，都有 $12x_i^2 - 4 > 0$。这等价于对于所有 $i$，都有 $|x_i| > 1/\\sqrt{3} \\approx 0.577$。例如，在原点附近，如果 $|x_i|  1/\\sqrt{3}$，则曲率为负，Hessian矩阵非正定。\n\n**1. 阻尼牛顿法**\n\n在每次迭代 $k$ 中，从点 $x_k$ 开始，该方法尝试找到一个搜索方向 $p_k$ 和一个步长 $\\alpha_k$。\n\n- **搜索方向 $p_k$**：方向 $p_k$ 是牛顿方程组 $\\nabla^2 f(x_k) p_k = -\\nabla f(x_k)$ 的解。由于 $\\nabla^2 f(x_k)$ 是对角矩阵，解可以按元素计算：\n$$\np_{k,i} = -\\frac{\\nabla f(x_k)_i}{(\\nabla^2 f(x_k))_{ii}} = -\\frac{4x_{k,i}(x_{k,i}^2-1)}{12x_{k,i}^2-4}\n$$\n- **下降条件**：如果搜索方向 $p_k$ 与梯度向量形成一个钝角，即 $\\nabla f(x_k)^\\top p_k  0$，则它是一个下降方向。如果此条件成立，沿 $p_k$ 方向移动一个足够小的步长保证会减小函数值。\n$$\n\\nabla f(x_k)^\\top p_k = \\sum_{i=1}^3 \\nabla f(x_k)_i \\, p_{k,i} = -\\sum_{i=1}^3 \\frac{(\\nabla f(x_k)_i)^2}{12x_{k,i}^2-4}\n$$\n如果Hessian矩阵是正定的（对所有 $i$，$12x_{k,i}^2 - 4 > 0$），则 $\\nabla f(x_k)^\\top p_k  0$（假设 $\\nabla f(x_k) \\neq 0$），$p_k$ 是一个下降方向。然而，如果Hessian矩阵的某些分量为负，则求和中的相应项为正。如果这些项足够大，整个和可能变为正，即 $\\nabla f(x_k)^\\top p_k \\ge 0$。在这种情况下，$p_k$ 不是下降方向，问题中指定的方法将声明该测试用例失败。\n\n- **步长 $\\alpha_k$**：如果 $p_k$ 是一个下降方向，则执行回溯线搜索以找到步长 $\\alpha_k \\in (0,1]$。它从 $\\alpha_k = 1$ 开始，并重复乘以收缩因子 $\\beta=1/2$，直到满足Armijo充分下降条件：\n$$\nf(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k, \\quad \\text{其中 } c_1 = 10^{-4}\n$$\n- **更新**：下一个迭代点是 $x_{k+1} = x_k + \\alpha_k p_k$。重复此过程，直到达到最大迭代次数 $k_{\\max}=100$ 或满足停止条件 $\\|\\nabla f(x_k)\\|_2 \\le 10^{-8}$。\n\n**2. 带特征值裁剪的修正牛顿法**\n\n该方法修改Hessian矩阵以确保其始终为正定，从而保证得到一个下降方向。\n\n- **Hessian矩阵修正**：在每次迭代 $k$ 中，计算 $\\nabla^2 f(x_k)$ 的特征值。由于Hessian矩阵是对角矩阵，其特征值就是其对角线元素，$\\lambda_i = 12x_{k,i}^2 - 4$。计算一个裁剪阈值 $\\delta$：\n$$\n\\delta = 10^{-3} \\max\\{1, \\max_j|\\lambda_j|\\}\n$$\n任何小于此阈值的特征值都被“裁剪”为 $\\delta$：\n$$\n\\lambda'_i = \\max\\{\\lambda_i, \\delta\\}\n$$\n根据构造，所有 $\\lambda'_i \\ge \\delta > 0$。修正后的Hessian矩阵 $H'_k$ 是一个对角线上为这些裁剪后特征值的对角矩阵。由于其所有特征值都为正，$H'_k$ 是对称正定的。\n\n- **搜索方向 $p_k$**：通过求解修正后的方程组 $H'_k p_k = -\\nabla f(x_k)$ 来找到搜索方向。按元素计算为：\n$$\np_{k,i} = -\\frac{\\nabla f(x_k)_i}{\\lambda'_i}\n$$\n- **下降条件**：与梯度的内积为：\n$$\n\\nabla f(x_k)^\\top p_k = -\\sum_{i=1}^3 \\frac{(\\nabla f(x_k)_i)^2}{\\lambda'_i}\n$$\n由于对所有 $i$，$\\lambda'_i > 0$，只要 $\\nabla f(x_k) \\neq 0$，这个和就严格为负。因此，$p_k$ 始终是一个严格下降方向，该方法在此阶段不会失败。\n\n- **步长 $\\alpha_k$ 和更新**：使用与阻尼牛顿法相同的回溯线搜索和更新规则。\n\n**测试用例分析**\n\n- **T1: $x_0 = (0.1, 0.1, 0.1)$**：此处， $|x_i|  1/\\sqrt{3}$，因此所有Hessian特征值 $\\lambda_i = 12(0.1)^2 - 4 = -3.88$ 均为负。阻尼牛顿法将计算一个指向原点的方向 $p_k$，而原点是一个鞍点和局部最大值点。这个方向是一个上升方向（$\\nabla f(x_k)^\\top p_k > 0$），因此该方法将失败。修正牛顿法会将负特征值裁剪为一个小的正值 $\\delta$，有效地将搜索方向转变为一个缩放的负梯度方向，这将使迭代点离开原点，朝向一个最小值点移动。\n\n- **T2: $x_0 = (2.0, -2.0, 0.5)$**：Hessian矩阵有两个正特征值（$\\lambda_{1,2} = 12(2)^2 - 4 = 44$）和一个负特征值（$\\lambda_3 = 12(0.5)^2 - 4 = -1$）。Hessian矩阵是不定的。对于阻尼牛顿法，下降条件取决于正曲率分量和负曲率分量之间的平衡。计算表明 $\\nabla f(x_k)^\\top p_k  0$，所以它是一个下降方向，方法可以继续进行。修正牛顿法将裁剪那个负特征值，确保得到一个稳健的下降方向。预计两种方法都将成功。\n\n- **T3: $x_0 = (0.0, 0.0, 0.0)$**：原点是一个驻点，$\\nabla f(0) = 0$。两种算法都会在开始时检查梯度范数，发现它为零，然后立即终止。最终点是 $x_{\\mathrm{final}}=(0,0,0)$，其中 $f(0)=3$。该值不满足成功条件 $f(x_{\\mathrm{final}}) \\le 10^{-10}$。因此，两种方法都将被声明为在此测试用例中失败。\n\n- **T4: $x_0 = (10.0, 10.0, 10.0)$**：远离原点， $|x_i| > 1/\\sqrt{3}$，所有Hessian特征值 $\\lambda_i = 12(10)^2-4=1196$ 都很大且为正。Hessian矩阵是强正定的。在这个区域，阻尼牛顿法等同于标准牛顿法，它将非常快地收敛。修正牛顿法的裁剪机制不会被触发，因为所有特征值都已经是大的正数（$\\lambda_i > \\delta$）。因此，两种方法的行为将相同，预计都会成功。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f(x: np.ndarray) -> float:\n    \"\"\"Objective function f(x) = sum((x_i^2 - 1)^2).\"\"\"\n    return np.sum((x**2 - 1)**2)\n\ndef grad_f(x: np.ndarray) -> np.ndarray:\n    \"\"\"Gradient of f(x).\"\"\"\n    return 4 * x * (x**2 - 1)\n\ndef hess_f_diag(x: np.ndarray) -> np.ndarray:\n    \"\"\"Diagonal of the Hessian of f(x).\"\"\"\n    return 12 * x**2 - 4\n\ndef damped_newton(x0: np.ndarray) -> tuple[bool, np.ndarray]:\n    \"\"\"\n    Implements the damped Newton method with backtracking line search.\n\n    Returns:\n        A tuple (success, x_final), where success is a boolean indicating\n        if the method converged to a global minimum, and x_final is the\n        final iterate.\n    \"\"\"\n    x = np.copy(x0)\n    k_max = 100\n    grad_tol = 1e-8\n    success_tol_f = 1e-10\n    c1 = 1e-4\n    beta = 0.5\n\n    for _ in range(k_max):\n        g = grad_f(x)\n        if np.linalg.norm(g) = grad_tol:\n            break\n\n        H_diag = hess_f_diag(x)\n        \n        # Newton step, allowing for division by zero which results in inf\n        with np.errstate(divide='ignore'):\n            p = -g / H_diag\n\n        # Check for descent direction as per problem statement\n        gTp = g @ p\n        if not np.isfinite(gTp) or gTp >= 0:\n            return False, x  # Declare failure for the test case\n\n        # Backtracking line search\n        alpha = 1.0\n        fx = f(x)\n        for _ in range(50): # Failsafe for line search\n            if f(x + alpha * p) = fx + c1 * alpha * gTp:\n                break\n            alpha *= beta\n        else: # if loop finishes without finding a suitable alpha\n            return f(x) = success_tol_f, x\n        \n        x = x + alpha * p\n        \n    final_f = f(x)\n    return final_f = success_tol_f, x\n\ndef modified_newton(x0: np.ndarray) -> tuple[bool, np.ndarray]:\n    \"\"\"\n    Implements the modified Newton method with eigenvalue clipping.\n\n    Returns:\n        A tuple (success, x_final) as in damped_newton.\n    \"\"\"\n    x = np.copy(x0)\n    k_max = 100\n    grad_tol = 1e-8\n    success_tol_f = 1e-10\n    c1 = 1e-4\n    beta = 0.5\n    delta_factor = 1e-3\n\n    for _ in range(k_max):\n        g = grad_f(x)\n        if np.linalg.norm(g) = grad_tol:\n            break\n\n        # For a diagonal Hessian, eigenvalues are the diagonal entries\n        lambda_vals = hess_f_diag(x)\n\n        # Calculate clipping threshold\n        delta = delta_factor * np.max([1.0, np.max(np.abs(lambda_vals))])\n        \n        # Clip eigenvalues\n        lambda_prime = np.maximum(lambda_vals, delta)\n        \n        # Modified Newton step\n        p = -g / lambda_prime\n        \n        # Descent is guaranteed, no explicit check needed\n        gTp = g @ p\n\n        # Backtracking line search\n        alpha = 1.0\n        fx = f(x)\n        for _ in range(50): # Failsafe for line search\n            if f(x + alpha * p) = fx + c1 * alpha * gTp:\n                break\n            alpha *= beta\n        else: # if loop finishes without finding a suitable alpha\n            return f(x) = success_tol_f, x\n        \n        x = x + alpha * p\n        \n    final_f = f(x)\n    return final_f = success_tol_f, x\n\ndef solve():\n    \"\"\"\n    Runs the defined test suite and prints the results in the required format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([0.1, 0.1, 0.1]),   # T1\n        np.array([2.0, -2.0, 0.5]),  # T2\n        np.array([0.0, 0.0, 0.0]),   # T3\n        np.array([10.0, 10.0, 10.0]), # T4\n    ]\n\n    results = []\n    for x0 in test_cases:\n        # Run Damped Newton method\n        success_dn, _ = damped_newton(x0)\n        results.append(success_dn)\n        \n        # Run Modified Newton method\n        success_mn, _ = modified_newton(x0)\n        results.append(success_mn)\n\n    # Convert boolean True/False to string \"True\"/\"False\" for printing\n    str_results = [str(r) for r in results]\n    print(f\"[{','.join(str_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "既然已经明确了修正海森矩阵的必要性，我们现在来探索实现它的不同方法。本练习比较了两种流行且强大的技术：特征值校正（翻转负特征值）和对角线平移（加上单位矩阵的倍数）。通过在一个更复杂的非可分函数上实现这两种方法，你将深入了解这些稳定牛顿法的基本策略的机理和相对行为。",
            "id": "3115918",
            "problem": "考虑在 $\\mathbb{R}^n$ 中一个二阶连续可微函数的无约束最小化问题。经典牛顿法在迭代点 $\\mathbf{x}_k$ 处使用局部二次模型，通过求解线性系统 $H_k \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ 来提出一个步长 $\\mathbf{p}_k$，其中 $H_k$ 表示 $f$ 在 $\\mathbf{x}_k$ 处的海森矩阵。当 $H_k$ 是不定的或近乎奇异时，纯牛顿方向可能不再是下降方向，导致该方法停滞或发散。两种标准的修正方法是：(i) 谱修正，将 $H_k$ 的负特征值翻转为其绝对值以强制其为正定；以及 (ii) 阻尼牛顿法，通过加上 $\\lambda I$（其中 $\\lambda \\ge 0$，$I$ 是单位矩阵）来平移 $H_k$ 的谱。\n\n从二阶泰勒展开的基本原理\n$$\nf(\\mathbf{x}_k + \\mathbf{p}) \\approx f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p} + \\tfrac{1}{2} \\mathbf{p}^\\top H_k \\mathbf{p},\n$$\n以及当 $H_k$ 为正定时牛顿步能最小化二次模型的原理出发，设计并实现一个实验，比较以下方法的收敛行为：\n- 通过特征值修正的修正牛顿法：计算对称特征值分解 $H_k = Q_k \\Lambda_k Q_k^\\top$；构造 $\\tilde{\\Lambda}_k$，其对角线元素为 $\\tilde{\\lambda}_{k,i} = |\\lambda_{k,i}| + \\delta$（其中 $0  \\delta \\ll 1$ 是一个小的正则化参数）；设置 $\\tilde{H}_k = Q_k \\tilde{\\Lambda}_k Q_k^\\top$ 并计算步长 $\\mathbf{p}_k = -\\tilde{H}_k^{-1} \\nabla f(\\mathbf{x}_k)$。\n- 通过平移的阻尼牛顿法：对于给定的 $\\lambda \\ge 0$，使用 $H_k^\\lambda = H_k + \\lambda I$ 并计算 $\\mathbf{p}_k = -(H_k^\\lambda)^{-1} \\nabla f(\\mathbf{x}_k)$。\n\n在两种方法中，都通过应用满足Armijo充分下降条件的回溯线搜索来强制全局收敛。具体来说，从步长 $\\alpha_k = 1$ 开始，用 $\\beta \\in (0,1)$ 重复地以 $\\alpha_k \\leftarrow \\beta \\alpha_k$ 替换 $\\alpha_k$，直到满足\n$$\nf(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha_k \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k,\n$$\n其中 $c_1 \\in (0,1)$ 是一个小常数。如果在任何迭代中 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k \\ge 0$，则使用最速下降方向 $\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ 以保证下降方向。\n\n使用 $\\mathbb{R}^2$ 中的非凸四次测试函数：\n$$\nf(x,y) = x^4 + y^4 - 3 x^2 - 3 y^2 + 0.5\\, x y + 0.8\\, x^2 y^2,\n$$\n其梯度为\n$$\n\\nabla f(x,y) = \\begin{bmatrix}\n4x^3 - 6x + 0.5\\, y + 1.6\\, x y^2 \\\\\n4y^3 - 6y + 0.5\\, x + 1.6\\, y x^2\n\\end{bmatrix},\n$$\n海森矩阵为\n$$\nH(x,y) =\n\\begin{bmatrix}\n12 x^2 - 6 + 1.6\\, y^2  0.5 + 3.2\\, x y \\\\\n0.5 + 3.2\\, x y  12 y^2 - 6 + 1.6\\, x^2\n\\end{bmatrix}.\n$$\n\n用相同的基于梯度范数的停止准则来实现这两种方法：当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon$ 或迭代次数达到 $k_{\\max}$ 时停止。使用以下固定的算法参数：$\\varepsilon = 10^{-6}$，$k_{\\max} = 200$，$c_1 = 10^{-4}$，$\\beta = 0.5$，以及 $\\delta = 10^{-6}$。\n\n测试集：\n- 情况1：初始点 $(x_0,y_0) = (1.5,-1.5)$，阻尼牛顿位移 $\\lambda = 1.0$。\n- 情况2：初始点 $(x_0,y_0) = (0.1,0.1)$，阻尼牛顿位移 $\\lambda = 1.0$。\n- 情况3：初始点 $(x_0,y_0) = (3.0,3.0)$，阻尼牛顿位移 $\\lambda = 1.0$。\n- 情况4：初始点 $(x_0,y_0) = (-2.0,0.5)$，阻尼牛顿位移 $\\lambda = 7.0$。\n\n对于每种情况，运行这两种方法并记录满足停止准则 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon$ 所需的迭代次数（如果未满足则记录 $k_{\\max}$）。您的程序应产生单行输出，其中包含这八个结果，这些结果按 $[\\text{flip\\_iters\\_case1}, \\text{shift\\_iters\\_case1}, \\text{flip\\_iters\\_case2}, \\text{shift\\_iters\\_case2}, \\text{flip\\_iters\\_case3}, \\text{shift\\_iters\\_case3}, \\text{flip\\_iters\\_case4}, \\text{shift\\_iters\\_case4}]$ 的顺序列于单个扁平列表中，作为一个用方括号括起来的逗号分隔列表（例如，$[3,5,7,8,2,4,3,6]$）。所有值都必须是整数。",
            "solution": "该问题提出了一个定义明确的数值实验，用于比较两种修正牛顿法在非凸函数无约束优化中的表现。验证证实了该问题陈述在科学上是合理的、内容是自洽的，并且在算法上是精确的。\n\n牛顿法的核心在于，在迭代点 $\\mathbf{x}_k$ 处，用其二阶泰勒展开式导出的二次模型来近似函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$：\n$$\nm_k(\\mathbf{p}) = f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p} + \\frac{1}{2} \\mathbf{p}^\\top H_k \\mathbf{p}\n$$\n其中 $\\mathbf{p}$ 是要采取的步长，$\\nabla f(\\mathbf{x}_k)$ 是 $f$ 在 $\\mathbf{x}_k$ 处的梯度，而 $H_k$ 是 $f$ 在 $\\mathbf{x}_k$ 处的海森矩阵。当海森矩阵 $H_k$ 是对称正定时，这个二次模型有一个唯一的最小化子，可以通过将其关于 $\\mathbf{p}$ 的梯度设为零来找到：$\\nabla_p m_k(\\mathbf{p}) = \\nabla f(\\mathbf{x}_k) + H_k \\mathbf{p} = \\mathbf{0}$。这就得出了纯牛顿步 $\\mathbf{p}_k = -H_k^{-1} \\nabla f(\\mathbf{x}_k)$，通常通过求解线性系统 $H_k \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ 来计算。\n\n对于非凸函数，海森矩阵 $H_k$ 可能是不定的（同时拥有正负特征值）或奇异的。在这种情况下，二次模型 $m_k(\\mathbf{p})$ 没有唯一的最小化子，并且纯牛顿步要么没有明确定义，要么可能不是一个下降方向（即，可能不满足 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k  0$）。这可能导致优化算法发散或收敛到鞍点或局部最大值。所提出的两种方法是通过确保使用正定矩阵来计算步长，从而解决这一缺陷的标准策略。\n\n**方法1：通过特征值修正的修正牛顿法**\n\n这种方法直接修改海森矩阵的谱特性。在第 $k$ 次迭代的步骤如下：\n1. 计算海森矩阵的对称特征值分解：$H_k = Q_k \\Lambda_k Q_k^\\top$，其中 $Q_k$ 是一个正交矩阵，其列是 $H_k$ 的特征向量，$\\Lambda_k$ 是由相应特征值 $\\lambda_{k,i}$ 构成的对角矩阵。\n2. 构造一个修正的特征值对角矩阵 $\\tilde{\\Lambda}_k$。对于 $H_k$ 的每个特征值 $\\lambda_{k,i}$，相应的修正特征值被设置为 $\\tilde{\\lambda}_{k,i} = |\\lambda_{k,i}| + \\delta$。$|\\lambda_{k,i}|$ 项将任何负特征值“翻转”为正值，而小的正则化参数 $\\delta > 0$（给定为 $10^{-6}$）确保所有得到的特征值都严格为正，从而防止奇异性。\n3. 重构一个修正的海森矩阵 $\\tilde{H}_k = Q_k \\tilde{\\Lambda}_k Q_k^\\top$。根据构造，$\\tilde{H}_k$ 是对称正定的。\n4. 通过求解线性系统 $\\tilde{H}_k \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ 来计算搜索方向 $\\mathbf{p}_k$。由于 $\\tilde{H}_k$ 是正定的，对于任何非零梯度，得到的步长 $\\mathbf{p}_k$ 都保证是下降方向，因为 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k = - \\nabla f(\\mathbf{x}_k)^\\top \\tilde{H}_k^{-1} \\nabla f(\\mathbf{x}_k)  0$。\n\n**方法2：通过平移的阻尼牛顿法**\n\n这种方法通常与 Levenberg-Marquardt 算法相关联，它通过添加单位矩阵的倍数来修正海森矩阵。\n1. 形成一个修正的海森矩阵 $H_k^\\lambda = H_k + \\lambda I$，其中 $\\lambda \\ge 0$ 是阻尼参数，$I$ 是单位矩阵。\n2. 这种修正将 $H_k$ 的每个特征值都平移了 $\\lambda$。如果 $\\lambda_{k,\\min}$ 是 $H_k$ 的最小特征值，那么 $H_k^\\lambda$ 的特征值是 $\\lambda_{k,i} + \\lambda$。为保证 $H_k^\\lambda$ 是正定的，必须选择 $\\lambda$ 使得 $\\lambda_{k,\\min} + \\lambda > 0$，即 $\\lambda > -\\lambda_{k,\\min}$。问题为每个测试用例指定了固定的 $\\lambda$ 值。\n3. 搜索方向通过求解系统 $(H_k + \\lambda I) \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ 来计算。如果所选的 $\\lambda$ 足够大，使得 $H_k + \\lambda I$ 为正定，那么就保证了下降方向。\n\n**通过线搜索和安全措施实现全局化**\n\n即使保证了下降方向，完整步长 $\\mathbf{p}_k$ 也可能过长，导致函数值增加。需要一个全局化策略来确保从远离解的初始点开始也能收敛。问题指定了一个回溯线搜索，它强制执行 Armijo 充分下降条件。\n在每次迭代中，我们寻找一个步长 $\\alpha_k > 0$ 使得：\n$$\nf(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha_k \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k\n$$\n其中给定常数 $c_1 \\in (0,1)$，这里 $c_1=10^{-4}$。该过程从完整步长（$\\alpha_k=1$）开始，并用一个因子 $\\beta \\in (0,1)$（这里 $\\beta=0.5$）重复减小它，直到满足条件。然后新的迭代点是 $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$。\n\n还包括一个最后的安全措施：如果计算出的搜索方向 $\\mathbf{p}_k$ 由于某种原因未能成为下降方向（即 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k \\ge 0$），算法必须回退到最速下降方向 $\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$。这确保了方向导数 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k = -\\|\\nabla f(\\mathbf{x}_k)\\|_2^2$ 总是负的（除非在驻点处），从而保证线搜索最终能找到一个有效的步长 $\\alpha_k > 0$。\n\n实现将遵循这些原则，对每个测试用例进行迭代，直到梯度范数 $\\|\\nabla f(\\mathbf{x}_k)\\|_2$ 低于容差 $\\varepsilon=10^{-6}$ 或达到最大迭代次数 $k_{\\max}=200$。",
            "answer": "```python\nimport numpy as np\n\ndef f(x_vec):\n    \"\"\"The nonconvex quartic test function.\"\"\"\n    x, y = x_vec[0], x_vec[1]\n    return x**4 + y**4 - 3*x**2 - 3*y**2 + 0.5*x*y + 0.8*x**2*y**2\n\ndef grad_f(x_vec):\n    \"\"\"The gradient of the test function.\"\"\"\n    x, y = x_vec[0], x_vec[1]\n    df_dx = 4*x**3 - 6*x + 0.5*y + 1.6*x*y**2\n    df_dy = 4*y**3 - 6*y + 0.5*x + 1.6*y*x**2\n    return np.array([df_dx, df_dy])\n\ndef hess_f(x_vec):\n    \"\"\"The Hessian of the test function.\"\"\"\n    x, y = x_vec[0], x_vec[1]\n    d2f_dx2 = 12*x**2 - 6 + 1.6*y**2\n    d2f_dy2 = 12*y**2 - 6 + 1.6*x**2\n    d2f_dxdy = 0.5 + 3.2*x*y\n    return np.array([[d2f_dx2, d2f_dxdy], [d2f_dxdy, d2f_dy2]])\n\ndef run_optimization(x0, method, params):\n    \"\"\"\n    Executes a modified Newton method.\n    \n    Args:\n        x0 (np.ndarray): Initial point.\n        method (str): 'flip' or 'shift'.\n        params (dict): Dictionary of algorithmic parameters.\n    \n    Returns:\n        int: Number of iterations taken.\n    \"\"\"\n    x_k = np.copy(x0)\n    \n    k_max = params['k_max']\n    eps = params['eps']\n    delta = params['delta']\n    lambda_val = params.get('lambda', 0.0) # Use get for lambda to avoid error for 'flip'\n    c1 = params['c1']\n    beta = params['beta']\n    \n    for k in range(k_max):\n        g = grad_f(x_k)\n        \n        # Check stopping criterion\n        if np.linalg.norm(g) = eps:\n            return k\n            \n        H = hess_f(x_k)\n        \n        # Compute search direction p_k\n        if method == 'flip':\n            eigenvalues, Q = np.linalg.eigh(H)\n            Lambda_tilde = np.diag(np.abs(eigenvalues) + delta)\n            H_tilde = Q @ Lambda_tilde @ Q.T\n            try:\n                p_k = np.linalg.solve(H_tilde, -g)\n            except np.linalg.LinAlgError:\n                p_k = -g\n        \n        elif method == 'shift':\n            H_lambda = H + lambda_val * np.eye(len(x0))\n            try:\n                p_k = np.linalg.solve(H_lambda, -g)\n            except np.linalg.LinAlgError:\n                p_k = -g\n        \n        else:\n            raise ValueError(\"Unknown method specified\")\n            \n        # Safeguard: ensure descent direction\n        if g.T @ p_k >= 0:\n            p_k = -g\n            \n        # Backtracking line search (Armijo condition)\n        alpha = 1.0\n        f_k = f(x_k)\n        g_dot_p = g.T @ p_k\n        \n        # A safeguard for the line search loop to prevent extremely small steps\n        ls_max_iter = 50 \n        for _ in range(ls_max_iter):\n            if f(x_k + alpha * p_k) = f_k + c1 * alpha * g_dot_p:\n                break\n            alpha *= beta\n        else: # if loop finishes without break\n            alpha = 0 # Failed to find a step, stay at the same point\n        \n        # Update iterate\n        x_k = x_k + alpha * p_k\n        \n    return k_max\n\ndef solve():\n    \"\"\"Main function to run the experiment and print results.\"\"\"\n    params = {\n        'eps': 1e-6,\n        'k_max': 200,\n        'c1': 1e-4,\n        'beta': 0.5,\n        'delta': 1e-6\n    }\n    \n    test_cases = [\n        {'x0': np.array([1.5, -1.5]), 'lambda': 1.0},\n        {'x0': np.array([0.1, 0.1]), 'lambda': 1.0},\n        {'x0': np.array([3.0, 3.0]), 'lambda': 1.0},\n        {'x0': np.array([-2.0, 0.5]), 'lambda': 7.0}\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        x0 = case['x0']\n        \n        # Run eigenvalue correction method\n        flip_iters = run_optimization(x0, 'flip', params)\n        results.append(flip_iters)\n        \n        # Run damped Newton method\n        shift_params = params.copy()\n        shift_params['lambda'] = case['lambda']\n        shift_iters = run_optimization(x0, 'shift', shift_params)\n        results.append(shift_iters)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}