{
    "hands_on_practices": [
        {
            "introduction": "掌握拟牛顿法的第一步是亲手实践其核心的更新公式。本练习将引导你对一个具体的二维问题手动计算DFP和BFGS更新矩阵。通过这个过程，并分析其在一个极限情况下的表现()，你将对这两种关键更新公式的力学机制建立起坚实而直观的理解。",
            "id": "3119496",
            "problem": "考虑一个二次连续可微函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ 和一个拟牛顿迭代过程，该过程维持 $f$ 的逆海森矩阵的一个对称正定近似 $H_{k}$。下一次的近似 $H_{k+1}$ 需要满足拟牛顿割线条件 $H_{k+1}y_{k}=s_{k}$，并且是通过在适当的内积空间中遵循最小变化原则对 $H_{k}$ 进行二秩对称校正得到的。有两种经典的构造方法：Davidon–Fletcher–Powell (DFP) 更新和 Broyden–Fletcher–Goldfarb–Shanno (BFGS) 更新，两者都为逆海森近似定义，并保证在 $s_{k}^{\\top}y_{k}>0$ 时保持正定性。\n\n从基本的拟牛顿割线条件和对称性出发，推导对应于 DFP 和 BFGS 构造的两种逆海森更新。然后，在 $n=2$ 的具体情况下，给定 $H_{0}=I$，$s=(1,0)^{\\top}$，以及 $y=(1,\\epsilon)^{\\top}$（其中 $\\epsilon$ 为标量且满足 $s^{\\top}y=1>0$），手动计算更新后的矩阵 $H_{1}^{\\mathrm{DFP}}$ 和 $H_{1}^{\\mathrm{BFGS}}$。\n\n定义标量函数\n$$\n\\Delta(\\epsilon) \\equiv \\ln\\!\\big(\\det(H_{1}^{\\mathrm{DFP}})\\big) - \\ln\\!\\big(\\det(H_{1}^{\\mathrm{BFGS}})\\big).\n$$\n求极限\n$$\n\\lim_{\\epsilon\\to 0} \\frac{\\Delta(\\epsilon)}{\\epsilon^{2}}.\n$$\n将你的最终答案表示为一个实数。无需四舍五入。",
            "solution": "该问题陈述清晰，具有科学依据，并包含唯一解所需的所有信息。其背景是数值优化领域，特别是拟牛顿法。我将开始解题。\n\n该问题要求推导和应用 Davidon–Fletcher–Powell (DFP) 和 Broyden–Fletcher–Goldfarb–Shanno (BFGS) 的逆海森近似 $H_k$ 的更新公式。这些更新是无约束优化的拟牛顿方法的核心。它们源于最小变化原则，即寻找一个新的近似 $H_{k+1}$，使其在满足割线条件 $H_{k+1} y_k = s_k$ 的同时，在特定范数下最接近当前的近似 $H_k$。这里，$s_k = x_{k+1} - x_k$ 是变量的步长，$y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ 是梯度的变化。\n\n逆海森近似 $H_k$ 的 DFP 更新公式为：\n$$\nH_{k+1}^{\\mathrm{DFP}} = H_k + \\frac{s_k s_k^\\top}{s_k^\\top y_k} - \\frac{H_k y_k y_k^\\top H_k}{y_k^\\top H_k y_k}\n$$\n此更新是通过在与海森矩阵诱导的几何相关的加权弗罗贝尼乌斯范数下最小化 $\\|H_{k+1} - H_k\\|$ 得出的。\n\n逆海森近似 $H_k$ 的 BFGS 更新公式为：\n$$\nH_{k+1}^{\\mathrm{BFGS}} = H_k + \\left(1 + \\frac{y_k^\\top H_k y_k}{s_k^\\top y_k}\\right) \\frac{s_k s_k^\\top}{s_k^\\top y_k} - \\frac{H_k y_k s_k^\\top + s_k y_k^\\top H_k}{s_k^\\top y_k}\n$$\nBFGS 更新是 DFP 更新的对偶；它是通过最小化海森近似 $B_k = H_k^{-1}$ 的变化，然后使用 Sherman-Morrison-Woodbury 公式对结果求逆得到的。如果 $s_k^\\top y_k > 0$，两种更新都会生成对称矩阵并保持正定性。\n\n我们被给予 $k=0$ 且 $n=2$ 的具体情况：\n- 初始逆海森近似：$H_0 = I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n- 步长向量：$s \\equiv s_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- 梯度差向量：$y \\equiv y_0 = \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix}$\n\n首先，我们计算必要的标量积：\n- $s^\\top y = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix} = 1(1) + 0(\\epsilon) = 1$。条件 $s^\\top y > 0$ 得到满足。\n- $H_0 y = I y = y = \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix}$。\n- $y^\\top H_0 y = y^\\top (I y) = y^\\top y = \\begin{pmatrix} 1 & \\epsilon \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix} = 1^2 + \\epsilon^2 = 1 + \\epsilon^2$。\n\n接下来，我们计算必要的外积：\n- $s s^\\top = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$。\n- $H_0 y y^\\top H_0 = (H_0 y)(H_0 y)^\\top = y y^\\top = \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix} \\begin{pmatrix} 1 & \\epsilon \\end{pmatrix} = \\begin{pmatrix} 1 & \\epsilon \\\\ \\epsilon & \\epsilon^2 \\end{pmatrix}$。\n\n现在我们将这些代入 DFP 公式来计算 $H_1^{\\mathrm{DFP}}$：\n$$\nH_1^{\\mathrm{DFP}} = H_0 + \\frac{s s^\\top}{s^\\top y} - \\frac{H_0 y y^\\top H_0}{y^\\top H_0 y}\n$$\n$$\nH_1^{\\mathrm{DFP}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\frac{1}{1} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} - \\frac{1}{1 + \\epsilon^2} \\begin{pmatrix} 1 & \\epsilon \\\\ \\epsilon & \\epsilon^2 \\end{pmatrix}\n$$\n$$\nH_1^{\\mathrm{DFP}} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} \\frac{1}{1 + \\epsilon^2} & \\frac{\\epsilon}{1 + \\epsilon^2} \\\\ \\frac{\\epsilon}{1 + \\epsilon^2} & \\frac{\\epsilon^2}{1 + \\epsilon^2} \\end{pmatrix} = \\begin{pmatrix} 2 - \\frac{1}{1 + \\epsilon^2} & -\\frac{\\epsilon}{1 + \\epsilon^2} \\\\ -\\frac{\\epsilon}{1 + \\epsilon^2} & 1 - \\frac{\\epsilon^2}{1 + \\epsilon^2} \\end{pmatrix}\n$$\n$$\nH_1^{\\mathrm{DFP}} = \\begin{pmatrix} \\frac{2(1 + \\epsilon^2) - 1}{1 + \\epsilon^2} & -\\frac{\\epsilon}{1 + \\epsilon^2} \\\\ -\\frac{\\epsilon}{1 + \\epsilon^2} & \\frac{1 + \\epsilon^2 - \\epsilon^2}{1 + \\epsilon^2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1 + 2\\epsilon^2}{1 + \\epsilon^2} & -\\frac{\\epsilon}{1 + \\epsilon^2} \\\\ -\\frac{\\epsilon}{1 + \\epsilon^2} & \\frac{1}{1 + \\epsilon^2} \\end{pmatrix}\n$$\n\n接下来，我们计算 $H_1^{\\mathrm{BFGS}}$。我们需要额外的交叉项：\n- $H_0 y s^\\top = y s^\\top = \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ \\epsilon & 0 \\end{pmatrix}$。\n- $s y^\\top H_0 = s y^\\top = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & \\epsilon \\end{pmatrix} = \\begin{pmatrix} 1 & \\epsilon \\\\ 0 & 0 \\end{pmatrix}$。\n代入 BFGS 公式：\n$$\nH_1^{\\mathrm{BFGS}} = H_0 + \\left(1 + \\frac{y^\\top H_0 y}{s^\\top y}\\right) \\frac{s s^\\top}{s^\\top y} - \\frac{H_0 y s^\\top + s y^\\top H_0}{s^\\top y}\n$$\n$$\nH_1^{\\mathrm{BFGS}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\left(1 + \\frac{1 + \\epsilon^2}{1}\\right) \\frac{1}{1} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} - \\frac{1}{1} \\left( \\begin{pmatrix} 1 & 0 \\\\ \\epsilon & 0 \\end{pmatrix} + \\begin{pmatrix} 1 & \\epsilon \\\\ 0 & 0 \\end{pmatrix} \\right)\n$$\n$$\nH_1^{\\mathrm{BFGS}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + (2 + \\epsilon^2) \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} 2 & \\epsilon \\\\ \\epsilon & 0 \\end{pmatrix}\n$$\n$$\nH_1^{\\mathrm{BFGS}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 2 + \\epsilon^2 & 0 \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} 2 & \\epsilon \\\\ \\epsilon & 0 \\end{pmatrix} = \\begin{pmatrix} 1 + 2 + \\epsilon^2 - 2 & 0 + 0 - \\epsilon \\\\ 0 + 0 - \\epsilon & 1 + 0 - 0 \\end{pmatrix}\n$$\n$$\nH_1^{\\mathrm{BFGS}} = \\begin{pmatrix} 1 + \\epsilon^2 & -\\epsilon \\\\ -\\epsilon & 1 \\end{pmatrix}\n$$\n\n现在我们计算这些矩阵的行列式。\n对于 $H_1^{\\mathrm{DFP}}$：\n$$\n\\det(H_1^{\\mathrm{DFP}}) = \\left(\\frac{1 + 2\\epsilon^2}{1 + \\epsilon^2}\\right) \\left(\\frac{1}{1 + \\epsilon^2}\\right) - \\left(-\\frac{\\epsilon}{1 + \\epsilon^2}\\right) \\left(-\\frac{\\epsilon}{1 + \\epsilon^2}\\right)\n$$\n$$\n\\det(H_1^{\\mathrm{DFP}}) = \\frac{1 + 2\\epsilon^2}{(1 + \\epsilon^2)^2} - \\frac{\\epsilon^2}{(1 + \\epsilon^2)^2} = \\frac{1 + 2\\epsilon^2 - \\epsilon^2}{(1 + \\epsilon^2)^2} = \\frac{1 + \\epsilon^2}{(1 + \\epsilon^2)^2} = \\frac{1}{1 + \\epsilon^2}\n$$\n对于 $H_1^{\\mathrm{BFGS}}$：\n$$\n\\det(H_1^{\\mathrm{BFGS}}) = (1 + \\epsilon^2)(1) - (-\\epsilon)(-\\epsilon) = 1 + \\epsilon^2 - \\epsilon^2 = 1\n$$\n这些结果与 DFP 和 BFGS 的通用行列式更新公式一致：\n$\\det(H_{k+1}^{\\mathrm{DFP}}) = \\det(H_k) \\frac{s_k^\\top y_k}{y_k^\\top H_k y_k}$ 和 $\\det(H_{k+1}^{\\mathrm{BFGS}}) = \\det(H_k) \\frac{s_k^\\top H_k^{-1} s_k}{s_k^\\top y_k}$。\n在我们的例子中，$\\det(H_0)=1$, $s^\\top y = 1$, $y^\\top H_0 y = 1+\\epsilon^2$, $s^\\top H_0^{-1} s = s^\\top I s = s^\\top s = 1$。\n$\\det(H_1^{\\mathrm{DFP}}) = 1 \\cdot \\frac{1}{1+\\epsilon^2} = \\frac{1}{1+\\epsilon^2}$。\n$\\det(H_1^{\\mathrm{BFGS}}) = 1 \\cdot \\frac{1}{1} = 1$。\n\n我们被要求计算当 $\\epsilon \\to 0$ 时 $\\frac{\\Delta(\\epsilon)}{\\epsilon^2}$ 的极限，其中\n$$\n\\Delta(\\epsilon) = \\ln\\big(\\det(H_1^{\\mathrm{DFP}})\\big) - \\ln\\big(\\det(H_1^{\\mathrm{BFGS}})\\big)\n$$\n代入我们计算出的行列式结果：\n$$\n\\Delta(\\epsilon) = \\ln\\left(\\frac{1}{1 + \\epsilon^2}\\right) - \\ln(1) = -\\ln(1 + \\epsilon^2) - 0 = -\\ln(1 + \\epsilon^2)\n$$\n需要计算的极限是：\n$$\n\\lim_{\\epsilon\\to 0} \\frac{\\Delta(\\epsilon)}{\\epsilon^2} = \\lim_{\\epsilon\\to 0} \\frac{-\\ln(1 + \\epsilon^2)}{\\epsilon^2}\n$$\n这是一个 $\\frac{0}{0}$ 的不定形式。我们可以使用 $\\ln(1+x)$ 在 $x=0$ 附近的泰勒级数展开来解决这个问题，即 $\\ln(1+x) = x - \\frac{x^2}{2} + O(x^3)$。\n令 $x = \\epsilon^2$，我们有：\n$$\n\\ln(1 + \\epsilon^2) = \\epsilon^2 - \\frac{(\\epsilon^2)^2}{2} + O((\\epsilon^2)^3) = \\epsilon^2 - \\frac{\\epsilon^4}{2} + O(\\epsilon^6)\n$$\n将此代入极限表达式：\n$$\n\\lim_{\\epsilon\\to 0} \\frac{-(\\epsilon^2 - \\frac{\\epsilon^4}{2} + O(\\epsilon^6))}{\\epsilon^2} = \\lim_{\\epsilon\\to 0} \\left(-1 + \\frac{\\epsilon^2}{2} - O(\\epsilon^4)\\right)\n$$\n当 $\\epsilon \\to 0$ 时，除了常数项外的所有项都趋于零。\n$$\n\\lim_{\\epsilon\\to 0} \\frac{-\\ln(1 + \\epsilon^2)}{\\epsilon^2} = -1\n$$\n或者，对变量 $u = \\epsilon^2$ 应用洛必达法则：\n$$\n\\lim_{u\\to 0^+} \\frac{-\\ln(1 + u)}{u} = \\lim_{u\\to 0^+} \\frac{-\\frac{d}{du}\\ln(1 + u)}{\\frac{d}{du}u} = \\lim_{u\\to 0^+} \\frac{-1/(1+u)}{1} = \\frac{-1}{1+0} = -1\n$$\n两种方法得到相同的结果。",
            "answer": "$$\n\\boxed{-1}\n$$"
        },
        {
            "introduction": "尽管DFP和BFGS公式在形式上具有对偶性，但在实践中BFGS方法却被压倒性地优先选择。本练习旨在揭示其背后的关键原因：数值稳定性。通过分析一个精心设计的算例()，你将亲眼见证DFP更新如何因分母项过小而变得不稳定，而BFGS则保持稳健，从而深刻理解BFGS在实际应用中为何更受青睐。",
            "id": "3119434",
            "problem": "考虑严格凸二次函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ 的无约束最小化问题，该函数定义为 $f(x) = \\tfrac{1}{2} x^{\\top} Q x$，其中\n$$\nQ = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix},\n$$\n假设我们应用拟牛顿法，其初始逆 Hessian 矩阵近似为\n$$\nH_{0} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^{-8} \\end{pmatrix}.\n$$\n通过要求第一个拟牛顿搜索方向 $p_{0}$（由 $p_{0} = - H_{0} \\nabla f(x_{0})$ 给出）等于向量\n$$\nv = \\begin{pmatrix} -\\frac{1}{3} \\\\ \\frac{2}{3} \\end{pmatrix}\n$$\n来隐式地定义起始点 $x_{0}$。\n取单位步长，使得 $x_{1} = x_{0} + p_{0}$，并将位移和梯度变化定义为 $s_{0} = x_{1}-x_{0} = p_{0}$ 和 $y_{0} = \\nabla f(x_{1}) - \\nabla f(x_{0})$。对于这个严格凸二次函数，利用基本事实 $\\nabla f(x) = Q x$ 和 $y_{0} = Q s_{0}$。\n\n你的任务是分析为何在这种情况下 Davidon-Fletcher-Powell (DFP) 更新会变得病态，而 Broyden-Fletcher-Goldfarb-Shanno (BFGS) 更新却能保持良好的尺度，重点关注曲率量 $y_{0}^{\\top} H_{0} y_{0}$（出现在 DFP 中）和 $s_{0}^{\\top} y_{0}$（出现在 BFGS 中）。特别地，请计算上述构造中 DFP 的关键分母 $y_{0}^{\\top} H_{0} y_{0}$ 的精确值。\n\n你的最终答案应为一个实数，无需四舍五入。",
            "solution": "我们从二次模型的核心定义开始。对于 $f(x) = \\tfrac{1}{2} x^{\\top} Q x$，其中 $Q$ 是对称正定矩阵，其梯度为 $\\nabla f(x) = Q x$。对于一个拟牛顿步，搜索方向是 $p_{0} = - H_{0} \\nabla f(x_{0})$。步长位移和梯度位移分别是 $s_{0} = x_{1} - x_{0}$ 和 $y_{0} = \\nabla f(x_{1}) - \\nabla f(x_{0})$。对于二次函数，梯度是仿射的，因此\n$$\ny_{0} = \\nabla f(x_{0} + s_{0}) - \\nabla f(x_{0}) = Q s_{0}.\n$$\n\n根据构造，我们要求 $p_{0} = v$，其中\n$$\nv = \\begin{pmatrix} -\\frac{1}{3} \\\\ \\frac{2}{3} \\end{pmatrix}.\n$$\n我们也取单位步长，因此 $s_{0} = p_{0} = v$。所以，\n$$\ny_{0} = Q s_{0} = Q v = \n\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n\\begin{pmatrix} -\\frac{1}{3} \\\\ \\frac{2}{3} \\end{pmatrix}\n=\n\\begin{pmatrix} 2\\cdot(-\\frac{1}{3}) + 1\\cdot(\\frac{2}{3}) \\\\ 1\\cdot(-\\frac{1}{3}) + 2\\cdot(\\frac{2}{3}) \\end{pmatrix}\n=\n\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n$$\n\n因此，$y_{0}$ 正是第二个坐标单位向量。DFP 更新在其第二项的分母中使用了曲率量 $y_{0}^{\\top} H_{0} y_{0}$。我们计算\n$$\ny_{0}^{\\top} H_{0} y_{0}\n=\n\\begin{pmatrix} 0 & 1 \\end{pmatrix}\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 10^{-8} \\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n=\n10^{-8}.\n$$\n\n这解释了为什么 Davidon-Fletcher-Powell (DFP) 更新可能会变得数值病态：分母 $y_{0}^{\\top} H_{0} y_{0}$ 极小，因此涉及除以 $y_{0}^{\\top} H_{0} y_{0}$ 的秩一校正项的幅度会过大，这会使 $H_{1}$ 不稳定并可能导致迭代停滞。相比之下，对于 Broyden-Fletcher-Goldfarb-Shanno (BFGS) 更新，相关的曲率量是 $s_{0}^{\\top} y_{0}$，此处\n$$\ns_{0}^{\\top} y_{0} = v^{\\top} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\frac{2}{3},\n$$\n这个值具有良好的尺度。因此，BFGS 保持表现良好，而 DFP 容易出现数值上危险的放大效应，这提供了所期望的对比。\n\n因此，所要求的 $y_{0}^{\\top} H_{0} y_{0}$ 的精确值为 $10^{-8}$。",
            "answer": "$$\\boxed{1 \\times 10^{-8}}$$"
        },
        {
            "introduction": "拟牛顿法的一个强大理论性质是，当应用于二次函数并使用精确线搜索时，它能在有限步内收敛。这个练习将理论与编程实践相结合。通过编写代码实现一个完整的拟牛顿算法，并在不同维度的二次函数上验证其“至多$n$步收敛”的特性()，你将把抽象的理论转化为可触摸的计算结果，从而加深对算法效率和能力的理解。",
            "id": "2417331",
            "problem": "设 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 是一个二次函数，定义为 $f(x) = \\tfrac{1}{2} x^T A x + b^T x$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵，$b \\in \\mathbb{R}^n$。考虑一个算法，该算法根据 $x_{k+1} = x_k + \\alpha_k p_k$ 生成迭代点 $\\{x_k\\}$，其中 $p_k = -B_k \\nabla f(x_k)$。$B_k \\in \\mathbb{R}^{n \\times n}$ 是某个对称正定矩阵，在每一步都会更新以满足割线条件 $B_{k+1} y_k = s_k$，其中 $s_k = x_{k+1} - x_k$，$y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$。步长 $\\alpha_k$ 的选择是为了精确地最小化 $f(x_k + \\alpha p_k)$（对于 $\\alpha \\in \\mathbb{R}$）。使用 $B_0 = I$ 和给定的 $x_0$ 进行初始化。所有向量范数均使用欧几里得范数。\n\n对于下面的测试集，数值验证达到 $\\|\\nabla f(x_{k_\\star})\\|_2 \\le 10^{-10}$ 所需的迭代次数 $k_\\star$至多为问题的维数 $n$。对于每个测试用例，输出一个布尔值，表示 $k_\\star \\le n$ 是否成立。\n\n使用以下输入 $(A,b,x_0)$ 的测试集：\n- 测试用例 $1$ ($n = 1$)：\n  - $A_1 = \\begin{bmatrix} 2 \\end{bmatrix}$，\n  - $b_1 = \\begin{bmatrix} -4 \\end{bmatrix}$，\n  - $x_{0,1} = \\begin{bmatrix} 10 \\end{bmatrix}$。\n- 测试用例 $2$ ($n = 2$)：\n  - $A_2 = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}$，\n  - $b_2 = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}$，\n  - $x_{0,2} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 测试用例 $3$ ($n = 3$)：\n  - $A_3 = \\begin{bmatrix} 3  1  0 \\\\ 1  4  1 \\\\ 0  1  2 \\end{bmatrix}$，\n  - $b_3 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 0 \\end{bmatrix}$，\n  - $x_{0,3} = \\begin{bmatrix} -\\tfrac{11}{19} \\\\ \\tfrac{14}{19} \\\\ -\\tfrac{7}{19} \\end{bmatrix}$。\n- 测试用例 $4$ ($n = 4$)：\n  - $A_4 = \\begin{bmatrix} 4  1  0  0 \\\\ 1  3  1  0 \\\\ 0  1  3  1 \\\\ 0  0  1  2 \\end{bmatrix}$，\n  - $b_4 = \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 2 \\end{bmatrix}$，\n  - $x_{0,4} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$。\n- 测试用例 $5$ ($n = 5$)：\n  - $A_5 = \\begin{bmatrix} 1  0  0  0  0 \\\\ 0  2  0  0  0 \\\\ 0  0  4  0  0 \\\\ 0  0  0  8  0 \\\\ 0  0  0  0  16 \\end{bmatrix}$，\n  - $b_5 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\\\ -2 \\\\ 3 \\end{bmatrix}$，\n  - $x_{0,5} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n\n对于每个测试用例，将 $n$ 定义为 $A$ 和 $b$ 的维数。当 $\\|\\nabla f(x_k)\\|_2 \\le 10^{-10}$ 时终止迭代，并记录迭代次数 $k_\\star$（约定如果 $\\|\\nabla f(x_0)\\|_2 \\le 10^{-10}$，则 $k_\\star = 0$）。如果该方法在 $n$ 次迭代内未达到容差，则为 $k_\\star$ 记录一个严格大于 $n$ 的值，以使布尔检查失败。\n\n最终输出格式：您的程序应生成一行输出，其中包含一个用方括号括起来的布尔字面量逗号分隔列表，按测试 1 到 5 的顺序排列，例如 $\\texttt{[True,False,True,True,True]}$。",
            "solution": "问题陈述经过验证，确认有效。它在计算工程和优化领域内提出了一个清晰、可用数值验证的任务。该问题描述了一类用于最小化二次函数的拟牛顿法，其中逆Hessian矩阵近似 $B_k$ 的更新仅受割线条件的约束。这里存在轻微的模糊性，因为仅凭割线条件无法唯一确定更新规则。为了解决这个问题，我将采用Broyden-Fletcher-Goldfarb-Shanno（BFGS）更新公式，这是该类方法中应用最广泛、最稳健的成员。待验证的性质——对于一个 $n$ 维二次函数，在至多 $n$ 次迭代内收敛——是像BFGS这类采用精确线搜索的方法的一个已知理论结果。\n\n目标函数是一个二次型 $f:\\mathbb{R}^n \\to \\mathbb{R}$，由下式给出：\n$$f(x) = \\tfrac{1}{2} x^T A x + b^T x$$\n其中 $A$ 是一个对称正定（SPD）矩阵，$b \\in \\mathbb{R}^n$。该函数的梯度为 $\\nabla f(x) = A x + b$。$f(x)$ 的最小值位于 $x^*$ 处，其中 $\\nabla f(x^*) = 0$，由此得出 $x^* = -A^{-1}b$。\n\n该算法使用以下更新规则生成一系列迭代点 $\\{x_k\\}$：\n$$x_{k+1} = x_k + \\alpha_k p_k$$\n搜索方向 $p_k$ 由拟牛顿模型定义：\n$$p_k = -B_k \\nabla f(x_k)$$\n其中 $B_k$ 是对逆Hessian矩阵 $A^{-1}$ 的当前近似。初始近似为 $B_0 = I$，即单位矩阵。\n\n步长 $\\alpha_k$ 由精确线搜索确定，该搜索最小化了关于 $\\alpha \\in \\mathbb{R}$ 的 $f(x_k + \\alpha p_k)$。线搜索的目标函数为 $\\phi(\\alpha) = f(x_k + \\alpha p_k)$。其关于 $\\alpha$ 的导数为：\n$$\\phi'(\\alpha) = \\nabla f(x_k + \\alpha p_k)^T p_k = (A(x_k + \\alpha p_k) + b)^T p_k = (A x_k + b)^T p_k + \\alpha p_k^T A p_k$$\n令 $\\phi'(\\alpha)=0$ 以找到最小值，并使用 $\\nabla f(x_k) = A x_k + b$，我们求解 $\\alpha$：\n$$\\alpha_k = -\\frac{\\nabla f(x_k)^T p_k}{p_k^T A p_k}$$\n由于 $B_k$ 保持为对称正定矩阵且 $\\nabla f(x_k) \\neq 0$，搜索方向 $p_k = -B_k \\nabla f(x_k)$ 是一个下降方向，即 $\\nabla f(x_k)^T p_k = -\\nabla f(x_k)^T B_k \\nabla f(x_k) < 0$。此外，由于 $A$ 是对称正定矩阵，$p_k^T A p_k > 0$。因此，$\\alpha_k > 0$。\n\n每一步之后，逆Hessian矩阵的近似从 $B_k$ 更新为 $B_{k+1}$。该更新必须满足割线条件 $B_{k+1}y_k = s_k$，其中 $s_k = x_{k+1} - x_k$，$y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$。对于二次函数，$y_k = A s_k$。$B_{k+1}$ 的 BFGS 更新公式为：\n$$B_{k+1} = \\left(I - \\frac{s_k y_k^T}{y_k^T s_k}\\right) B_k \\left(I - \\frac{y_k s_k^T}{y_k^T s_k}\\right) + \\frac{s_k s_k^T}{y_k^T s_k}$$\n该更新确保了 $B_{k+1}$ 保持对称和正定，前提是曲率条件 $y_k^T s_k > 0$ 成立。对于我们的问题，$y_k^T s_k = (A s_k)^T s_k = s_k^T A s_k$，由于 $A$ 是对称正定且 $s_k = \\alpha_k p_k \\neq 0$，该值为正。\n\n对于每个测试用例 $(A, b, x_0)$，算法流程如下：\n1.  初始化 $k=0$，$x_k = x_0$ 和 $B_k = I$。维数 $n$ 由 $A$ 确定。\n2.  计算初始梯度 $g_0 = \\nabla f(x_0) = A x_0 + b$。\n3.  如果 $\\|\\nabla f(x_0)\\|_2 \\le 10^{-10}$，则设置 $k_\\star = 0$ 并终止。这是问题中指定的一个特殊情况，对于测试用例 3 尤其重要。\n4.  对于 $k=0, 1, \\dots, n-1$：\n    a. 计算搜索方向 $p_k = -B_k \\nabla f(x_k)$。\n    b. 计算最优步长 $\\alpha_k$。\n    c. 更新位置：$x_{k+1} = x_k + \\alpha_k p_k$。\n    d. 检查收敛性：如果 $\\|\\nabla f(x_{k+1})\\|_2 \\le 10^{-10}$，则设置 $k_\\star = k+1$ 并终止循环。\n    e. 如果未收敛，计算 $s_k = x_{k+1} - x_k$ 和 $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$。\n    f. 使用BFGS公式将逆Hessian近似更新为 $B_{k+1}$。\n5.  如果循环在 $n$ 次迭代后仍未收敛，则条件 $k_\\star \\le n$ 为假。我们设置 $k_\\star > n$。\n6.  该测试用例的最终结果是表达式 $k_\\star \\le n$ 的布尔值。\n\n该实现将证实一个理论性质：对于正定二次函数，带有精确线搜索的BFGS方法至多在 $n$ 次迭代内找到最小值点。因此，所有测试用例的预期结果均为 `True`。",
            "answer": "```python\nimport numpy as np\n\ndef run_quasi_newton(A, b, x0, tol=1e-10):\n    \"\"\"\n    Performs quasi-Newton optimization (BFGS) on a quadratic function.\n\n    Args:\n        A (np.ndarray): The symmetric positive definite matrix of the quadratic term.\n        b (np.ndarray): The vector of the linear term.\n        x0 (np.ndarray): The initial point.\n        tol (float): The tolerance for the norm of the gradient.\n\n    Returns:\n        int: The number of iterations k_star to reach the tolerance.\n             Returns n + 1 if convergence is not achieved within n iterations.\n    \"\"\"\n    n = A.shape[0]\n    xk = x0.astype(float) # Ensure float type for calculations\n    \n    # Calculate initial gradient\n    g = A @ xk + b\n    \n    # Check if initial point is already the solution\n    if np.linalg.norm(g) = tol:\n        return 0\n\n    Bk = np.eye(n, dtype=float)\n    \n    # Loop for at most n iterations\n    for k in range(n):\n        # Step 1: Compute search direction\n        pk = -Bk @ g\n        \n        # Step 2: Perform exact line search\n        # Denominator pk.T @ A @ pk must be positive as A is SPD and pk is non-zero\n        alpha_k = -(g @ pk) / (pk @ A @ pk)\n        \n        # Step 3: Update position\n        xk_plus_1 = xk + alpha_k * pk\n        \n        # Step 4: Compute new gradient and check for convergence\n        g_plus_1 = A @ xk_plus_1 + b\n        if np.linalg.norm(g_plus_1) = tol:\n            return k + 1\n            \n        # Step 5: Update Bk using BFGS formula\n        sk = xk_plus_1 - xk\n        yk = g_plus_1 - g\n        \n        # Curvature condition yk.T @ sk must be positive\n        rho_k = 1.0 / (yk @ sk)\n        \n        # Standard compact BFGS update for B (approximation of H^-1)\n        # B_k+1 = (I - rho*s*y.T) B (I - rho*y*s.T) + rho*s*s.T\n        term1 = np.eye(n) - rho_k * np.outer(sk, yk)\n        Bk = term1 @ Bk @ term1.T + rho_k * np.outer(sk, sk)\n        \n        # Prepare for the next iteration\n        xk = xk_plus_1\n        g = g_plus_1\n        \n    # If the method has not converged within n iterations\n    return n + 1\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the quasi-Newton verification problem.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (n=1)\n        (np.array([[2.]]), np.array([-4.]), np.array([10.])),\n        # Test case 2 (n=2)\n        (np.array([[4., 1.], [1., 3.]]), np.array([-1., 2.]), np.array([0., 0.])),\n        # Test case 3 (n=3)\n        (np.array([[3., 1., 0.], [1., 4., 1.], [0., 1., 2.]]), \n         np.array([1., -2., 0.]), \n         np.array([-11./19., 14./19., -7./19.])),\n        # Test case 4 (n=4)\n        (np.array([[4., 1., 0., 0.], [1., 3., 1., 0.], [0., 1., 3., 1.], [0., 0., 1., 2.]]),\n         np.array([1., 0., -1., 2.]),\n         np.array([1., 1., 1., 1.])),\n        # Test case 5 (n=5)\n        (np.diag([1., 2., 4., 8., 16.]),\n         np.array([1., -1., 2., -2., 3.]),\n         np.array([0., 0., 0., 0., 0.]))\n    ]\n\n    results = []\n    for A, b, x0 in test_cases:\n        n = A.shape[0]\n        k_star = run_quasi_newton(A, b, x0)\n        # Verify if the number of iterations is at most the dimension n\n        is_converged_in_n_steps = (k_star = n)\n        results.append(str(is_converged_in_n_steps))\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        }
    ]
}