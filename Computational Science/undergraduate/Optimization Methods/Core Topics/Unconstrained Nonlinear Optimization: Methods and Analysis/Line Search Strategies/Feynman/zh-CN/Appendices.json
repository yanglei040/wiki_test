{
    "hands_on_practices": [
        {
            "introduction": "理论与实践的第一步是理解理想情况与现实方法的差异。对于二次函数这类简单的目标函数，我们可以通过解析方式求得最优步长，这被称为精确线搜索。本练习将引导您推导二次函数在给定搜索方向下的精确最优步长 $\\alpha^{\\star}$，并将其与应用广泛的非精确线搜索算法——Armijo回溯法——所找到的第一个可接受步长 $\\alpha_{\\mathrm{BT}}$ 进行比较。通过这个过程，您将深刻理解非精确线搜索是如何在计算成本和优化效果之间取得平衡的。",
            "id": "3143370",
            "problem": "考虑二次函数 $f(x) = \\tfrac{1}{2} x^{\\top} Q x + b^{\\top} x$，其中 $Q \\in \\mathbb{R}^{2 \\times 2}$ 是对称正定矩阵，$b \\in \\mathbb{R}^{2}$。令\n$$\nQ = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}, \\quad x_{k} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\n定义搜索方向 $p$ 为在 $x_{k}$ 处的负梯度，即 $p = - \\nabla f(x_{k})$。\n\n任务：\n- 从沿某一方向进行精确线搜索的定义出发，将 $\\varphi(\\alpha) = f(x_{k} + \\alpha p)$ 视为关于 $\\alpha \\in \\mathbb{R}$ 的一元函数，并仅使用核心定义（$f$ 的梯度、方向导数以及一元二次函数的一阶最优性），推导出最优解 $\\alpha^{\\star}$ 关于 $Q$、$\\nabla f(x_{k})$ 和 $p$ 的封闭形式表达式。\n- 对于上面给定的 $Q$, $b$, $x_{k}$ 和 $p$，计算推导出的 $\\alpha^{\\star}$ 的值。\n- 使用 Armijo 回溯准则（也称充分下降准则），参数为 $c = \\tfrac{1}{4}$，$\\rho = \\tfrac{1}{2}$，初始试验步长为 $\\alpha_{0} = 1$，确定从 $x_{k}$ 出发沿方向 $p$ 的第一个可接受的步长 $\\alpha_{\\mathrm{BT}}$。对于每个候选步长 $\\alpha$ 需要检验的 Armijo 条件是\n$$\nf(x_{k} + \\alpha p) \\leq f(x_{k}) + c \\, \\alpha \\, \\nabla f(x_{k})^{\\top} p.\n$$\n- 以简化的精确分数形式报告比值 $\\alpha_{\\mathrm{BT}} / \\alpha^{\\star}$。你的最终答案应为这一个比值。无需四舍五入。",
            "solution": "用户提供了一个问题陈述，在进行解答之前需要对其进行验证。\n\n### 步骤 1：提取已知条件\n问题提供了以下信息：\n- 一个二次函数 $f(x) = \\frac{1}{2} x^{\\top} Q x + b^{\\top} x$。\n- 矩阵 $Q = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix}$ 是一个在 $\\mathbb{R}^{2 \\times 2}$ 中的对称正定矩阵。\n- 向量 $b = \\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}$ 是一个在 $\\mathbb{R}^{2}$ 中的向量。\n- 当前点是 $x_{k} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n- 搜索方向定义为 $p = - \\nabla f(x_{k})$。\n- 精确线搜索旨在找到最小化一元函数 $\\varphi(\\alpha) = f(x_{k} + \\alpha p)$ 的 $\\alpha^{\\star}$。\n- 将使用 Armijo 准则执行回溯线搜索，参数为 $c = \\frac{1}{4}$，$\\rho = \\frac{1}{2}$，初始试验步长为 $\\alpha_{0} = 1$。\n- Armijo 条件指定为 $f(x_{k} + \\alpha p) \\leq f(x_{k}) + c \\, \\alpha \\, \\nabla f(x_{k})^{\\top} p$。\n- 最终输出应为比值 $\\frac{\\alpha_{\\mathrm{BT}}}{\\alpha^{\\star}}$，形式为简化的精确分数。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题经过了严格的验证过程。\n- **科学依据**：该问题属于数值优化这一成熟领域。函数、概念（梯度下降、线搜索）和方法（精确和回溯）都是优化理论中的标准主题。提供的矩阵 $Q$ 是对称的，其主子式为 $\\det(4) = 4 > 0$ 和 $\\det(Q) = (4)(3) - (1)(1) = 11 > 0$，证实了它是正定的。这确保了函数 $f(x)$ 是严格凸函数，这是这些方法的标准假设。因此，该问题在科学上和数学上都是合理的。\n- **适定性**：问题陈述清晰，提供了找到唯一解所需的所有数据和定义。函数已定义，起始点已给出，搜索方向规则明確，回溯算法的参数也都已指定。\n- **客观性**：问题以精确的数学语言陈述，没有任何主观性或模糊性。\n\n### 步骤 3：结论与行动\n该问题被认为是**有效的**，因为它是自洽的、科学合理的、客观的和适定的。将提供解答。\n\n### 解答推导\n\n根据问题陈述的要求，解答过程包括四个主要部分。\n\n**第一部分：精确线搜索步长 $\\alpha^{\\star}$ 的推导**\n\n需要最小化的一元函数是 $\\varphi(\\alpha) = f(x_{k} + \\alpha p)$，其中 $\\alpha \\in \\mathbb{R}$。我们将 $f(x)$ 的表达式代入 $\\varphi(\\alpha)$：\n$$\n\\varphi(\\alpha) = \\frac{1}{2} (x_{k} + \\alpha p)^{\\top} Q (x_{k} + \\alpha p) + b^{\\top} (x_{k} + \\alpha p)\n$$\n展开此表达式，我们得到：\n$$\n\\varphi(\\alpha) = \\frac{1}{2} (x_{k}^{\\top}Qx_{k} + \\alpha x_{k}^{\\top}Qp + \\alpha p^{\\top}Qx_{k} + \\alpha^2 p^{\\top}Qp) + b^{\\top}x_{k} + \\alpha b^{\\top}p\n$$\n由于 $Q$ 是对称的，$x_{k}^{\\top}Qp = (p^{\\top}Q^{\\top}x_{k})^{\\top} = (p^{\\top}Qx_{k})^{\\top}$。因为这是一个标量，它等于 $p^{\\top}Qx_{k}$。我们可以按 $\\alpha$ 的幂次对各项进行分组：\n$$\n\\varphi(\\alpha) = \\left(\\frac{1}{2} p^{\\top}Qp\\right)\\alpha^2 + (p^{\\top}Qx_{k} + b^{\\top}p)\\alpha + \\left(\\frac{1}{2}x_{k}^{\\top}Qx_{k} + b^{\\top}x_{k}\\right)\n$$\n$f(x)$ 的梯度是 $\\nabla f(x) = Qx + b$。关于 $\\alpha$ 的线性项可以重写为：\n$$\np^{\\top}Qx_{k} + b^{\\top}p = p^{\\top}(Qx_{k} + b) = p^{\\top}\\nabla f(x_k)\n$$\n常数项就是 $f(x_k)$。因此，$\\varphi(\\alpha)$ 是关于 $\\alpha$ 的二次函数：\n$$\n\\varphi(\\alpha) = \\left(\\frac{1}{2} p^{\\top}Qp\\right)\\alpha^2 + \\left(p^{\\top}\\nabla f(x_k)\\right)\\alpha + f(x_k)\n$$\n为了找到最小值点 $\\alpha^{\\star}$，我们对 $\\alpha$ 求导并令其为零。\n$$\n\\varphi'(\\alpha) = ( p^{\\top}Qp )\\alpha + p^{\\top}\\nabla f(x_k) = 0\n$$\n解出 $\\alpha$ 得到最优步长 $\\alpha^{\\star}$：\n$$\n\\alpha^{\\star} = - \\frac{p^{\\top}\\nabla f(x_k)}{p^{\\top}Qp}\n$$\n二阶导数是 $\\varphi''(\\alpha) = p^{\\top}Qp$。由于 $Q$ 是正定的且 $p \\neq 0$（因为我们不在最优点），$p^{\\top}Qp > 0$，这证实了 $\\alpha^{\\star}$ 确实是一个最小值点。问题定义 $p = -\\nabla f(x_k)$，所以我们也可以将 $\\alpha^{\\star}$写作：\n$$\n\\alpha^{\\star} = - \\frac{(-\\nabla f(x_k))^{\\top}\\nabla f(x_k)}{(-\\nabla f(x_k))^{\\top}Q(-\\nabla f(x_k))} = \\frac{\\nabla f(x_k)^{\\top}\\nabla f(x_k)}{\\nabla f(x_k)^{\\top}Q\\nabla f(x_k)}\n$$\n在计算中，我们将使用包含 $p$ 的形式。\n\n**第二部分：$\\alpha^{\\star}$ 的求值**\n\n首先，我们计算在 $x_{k} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 处的梯度 $\\nabla f(x_k)$：\n$$\n\\nabla f(x_k) = Qx_k + b = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n$$\n搜索方向是 $p = -\\nabla f(x_k)$：\n$$\np = - \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix}\n$$\n现在我们计算 $\\alpha^{\\star}$ 公式中的各项：\n分子是 $p^{\\top}\\nabla f(x_k)$：\n$$\np^{\\top}\\nabla f(x_k) = \\begin{pmatrix} -2  -2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = (-2)(2) + (-2)(2) = -4 - 4 = -8\n$$\n分母是 $p^{\\top}Qp$：\n$$\nQp = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 4(-2) + 1(-2) \\\\ 1(-2) + 3(-2) \\end{pmatrix} = \\begin{pmatrix} -8 - 2 \\\\ -2 - 6 \\end{pmatrix} = \\begin{pmatrix} -10 \\\\ -8 \\end{pmatrix}\n$$\n$$\np^{\\top}Qp = \\begin{pmatrix} -2  -2 \\end{pmatrix} \\begin{pmatrix} -10 \\\\ -8 \\end{pmatrix} = (-2)(-10) + (-2)(-8) = 20 + 16 = 36\n$$\n最后，我们计算 $\\alpha^{\\star}$：\n$$\n\\alpha^{\\star} = - \\frac{-8}{36} = \\frac{8}{36} = \\frac{2}{9}\n$$\n\n**第三部分：回溯步长 $\\alpha_{\\mathrm{BT}}$ 的确定**\n\nArmijo 回溯条件是 $f(x_{k} + \\alpha p) \\leq f(x_{k}) + c \\, \\alpha \\, \\nabla f(x_{k})^{\\top} p$。\n参数为 $c = \\frac{1}{4}$，$\\rho = \\frac{1}{2}$，且 $\\alpha_0 = 1$。\n我们来计算不等式中的各项。\n函数在 $x_k$ 处的值是：\n$$\nf(x_k) = \\frac{1}{2}x_{k}^{\\top}Qx_{k} + b^{\\top}x_{k} = \\frac{1}{2}\\begin{pmatrix} 1  0 \\end{pmatrix}\\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} -2  1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{1}{2}(4) + (-2) = 2 - 2 = 0\n$$\nArmijo 条件的右侧 (RHS) 是：\n$$\n\\text{RHS} = f(x_k) + c \\alpha \\nabla f(x_k)^{\\top} p = 0 + \\left(\\frac{1}{4}\\right)\\alpha(-8) = -2\\alpha\n$$\n从第一部分可知，左侧 (LHS) 是 $\\varphi(\\alpha) = f(x_k + \\alpha p) = (\\frac{1}{2} p^{\\top}Qp)\\alpha^2 + (p^{\\top}\\nabla f(x_k))\\alpha + f(x_k)$。使用我们求得的值：\n$$\n\\text{LHS} = \\frac{1}{2}(36)\\alpha^2 + (-8)\\alpha + 0 = 18\\alpha^2 - 8\\alpha\n$$\nArmijo 条件变为：\n$$\n18\\alpha^2 - 8\\alpha \\leq -2\\alpha\n$$\n$$\n18\\alpha^2 - 6\\alpha \\leq 0\n$$\n$$\n6\\alpha(3\\alpha - 1) \\leq 0\n$$\n由于步长 $\\alpha$ 必须为正，我们可以除以 $6\\alpha$ 而不改变不等号方向：\n$$\n3\\alpha - 1 \\leq 0 \\implies \\alpha \\leq \\frac{1}{3}\n$$\n现在我们从 $\\alpha_0 = 1$ 开始执行回溯搜索：\n- **尝试 $\\alpha = 1$**：$1 \\leq \\frac{1}{3}$ 成立吗？不成立。条件不满足。更新 $\\alpha \\leftarrow \\rho \\alpha = \\frac{1}{2} \\times 1 = \\frac{1}{2}$。\n- **尝试 $\\alpha = \\frac{1}{2}$**：$\\frac{1}{2} \\leq \\frac{1}{3}$ 成立吗？不成立。（因为 $3 \\leq 2$ 是假的）。条件不满足。更新 $\\alpha \\leftarrow \\rho \\alpha = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$。\n- **尝试 $\\alpha = \\frac{1}{4}$**：$\\frac{1}{4} \\leq \\frac{1}{3}$ 成立吗？成立。（因为 $3 \\leq 4$ 是真的）。条件满足。\n第一个可接受的步长是 $\\alpha_{\\mathrm{BT}} = \\frac{1}{4}$。\n\n**第四部分：计算比值**\n\n最后的任务是计算比值 $\\frac{\\alpha_{\\mathrm{BT}}}{\\alpha^{\\star}}$：\n$$\n\\frac{\\alpha_{\\mathrm{BT}}}{\\alpha^{\\star}} = \\frac{\\frac{1}{4}}{\\frac{2}{9}} = \\frac{1}{4} \\times \\frac{9}{2} = \\frac{9}{8}\n$$\n要求比值为简化的精确分数。",
            "answer": "$$\\boxed{\\frac{9}{8}}$$"
        },
        {
            "introduction": "简单的回溯线搜索虽然可靠，但有时收敛速度较慢，因为它要求函数值在每一步都严格下降。为了提升效率，我们可以利用更多信息来构造一个更精确的函数近似模型，从而得到更好的步长候选值。本练习将介绍一种更精妙的方法：三次多项式插值。您将学习如何利用区间两端的函数值和导数值来构建一个三次模型，并通过求解其极小值点来预测最优步长。更重要的是，您将分析该模型预测可能失效的情况，从而理解为何在实际算法中“安全保障”（safeguarding）机制至关重要。",
            "id": "3143435",
            "problem": "考虑对一个光滑目标函数沿固定方向进行线搜索，其中一维约束由映射 $\\varphi(\\alpha) = f(x + \\alpha p)$ 定义。在区间限定阶段，假设有两个点 $\\alpha = a$ 和 $\\alpha = b$ 可用，以及在这些点上的函数值和方向导数。构造一个三次多项式插值 $q(\\alpha)$ 以满足赫米特(Hermite)插值条件 $q(a) = \\varphi(a)$、$q'(a) = \\varphi'(a)$、$q(b) = \\varphi(b)$ 和 $q'(b) = \\varphi'(b)$。根据多项式插值和方向导数的基本定义，显式地推导出 $q(\\alpha)$ 作为 $\\alpha$ 的多项式，并证明 $q'(\\alpha)$ 是一个二次函数，其根是 $q(\\alpha)$ 的驻点，其中之一是三次候选极小值点。\n\n然后，对于具体的区间限定数据\n$\\alpha$-区间端点：$a = 0$，$b = 1$，\n函数值和导数：$\\varphi(0) = 5$，$\\varphi'(0) = -0.4$，$\\varphi(1) = \\frac{1447}{300}$，以及 $\\varphi'(1) = -0.02$，\n通过求解 $q'(\\alpha) = 0$ 并选择对应于局部极小值的驻点，计算三次候选极小值点 $\\alpha_{\\mathrm{cubic}}$。说明 $\\alpha_{\\mathrm{cubic}}$ 是否位于当前区间 $[a,b]$ 内，并根据你的推导简要解释，在何种情况下三次极小值点会位于区间之外，从而使得标准线搜索策略中必须采用安全措施。\n\n将 $\\alpha_{\\mathrm{cubic}}$ 的最终数值四舍五入到四位有效数字。",
            "solution": "该问题陈述清晰，在数值优化领域有科学依据，并包含足以确定唯一解的充分、一致的信息。因此，该问题被认为是有效的。\n\n### 第一部分：三次插值函数及其导数的推导\n\n问题要求构造一个三次多项式 $q(\\alpha)$，它满足区间限定的端点 $a$ 和 $b$ 处的四个赫米特插值条件：\n1.  $q(a) = \\varphi(a)$\n2.  $q'(a) = \\varphi'(a)$\n3.  $q(b) = \\varphi(b)$\n4.  $q'(b) = \\varphi'(b)$\n\n为简化系数的推导，我们进行变量替换。令 $\\theta = \\alpha - a$ 和 $h = b - a$。$\\theta$ 的插值区间是 $[0, h]$。我们定义一个新的三次多项式 $g(\\theta) = q(a+\\theta)$，其形式为 $g(\\theta) = c_3 \\theta^3 + c_2 \\theta^2 + c_1 \\theta + c_0$。$g(\\theta)$ 上的插值条件是：\n1.  $g(0) = q(a) = \\varphi(a)$\n2.  $g'(0) = q'(a) = \\varphi'(a)$\n3.  $g(h) = q(b) = \\varphi(b)$\n4.  $g'(h) = q'(b) = \\varphi'(b)$\n\n从 $g(\\theta)$ 及其导数 $g'(\\theta) = 3c_3 \\theta^2 + 2c_2 \\theta + c_1$ 的形式，我们可以立即使用在 $\\theta=0$ 处的条件来确定前两个系数：\n$g(0) = c_0 = \\varphi(a)$\n$g'(0) = c_1 = \\varphi'(a)$\n\n接下来，我们使用在 $\\theta=h$ 处的条件来求 $c_2$ 和 $c_3$：\n$g(h) = c_3 h^3 + c_2 h^2 + c_1 h + c_0 = \\varphi(b)$\n$g'(h) = 3c_3 h^2 + 2c_2 h + c_1 = \\varphi'(b)$\n\n代入 $c_0$ 和 $c_1$ 的已知值：\n$c_3 h^3 + c_2 h^2 + \\varphi'(a) h + \\varphi(a) = \\varphi(b) \\implies c_3 h^3 + c_2 h^2 = \\varphi(b) - \\varphi(a) - \\varphi'(a) h$\n$3c_3 h^2 + 2c_2 h + \\varphi'(a) = \\varphi'(b) \\implies 3c_3 h^2 + 2c_2 h = \\varphi'(b) - \\varphi'(a)$\n\n这是一个关于 $c_2$ 和 $c_3$ 的 $2 \\times 2$ 线性系统。两边除以 $h$ 的幂（假设 $h \\neq 0$）：\n(I) $c_3 h + c_2 = \\frac{\\varphi(b) - \\varphi(a)}{h^2} - \\frac{\\varphi'(a)}{h}$\n(II) $3c_3 h + 2c_2 = \\frac{\\varphi'(b) - \\varphi'(a)}{h}$\n\n将方程(I)乘以 $2$ 并从方程(II)中减去：\n$(3c_3 h + 2c_2) - 2(c_3 h + c_2) = \\frac{\\varphi'(b) - \\varphi'(a)}{h} - 2\\left(\\frac{\\varphi(b) - \\varphi(a)}{h^2} - \\frac{\\varphi'(a)}{h}\\right)$\n$c_3 h = \\frac{\\varphi'(b) - \\varphi'(a) + 2\\varphi'(a)}{h} - \\frac{2(\\varphi(b) - \\varphi(a))}{h^2}$\n$c_3 h = \\frac{\\varphi'(a) + \\varphi'(b)}{h} - \\frac{2(\\varphi(b) - \\varphi(a))}{h^2}$\n$c_3 = \\frac{\\varphi'(a) + \\varphi'(b)}{h^2} - \\frac{2(\\varphi(b) - \\varphi(a))}{h^3}$\n\n将 $c_3$ 代回方程(I)以求 $c_2$：\n$c_2 = \\frac{\\varphi(b) - \\varphi(a)}{h^2} - \\frac{\\varphi'(a)}{h} - c_3 h$\n$c_2 = \\frac{\\varphi(b) - \\varphi(a)}{h^2} - \\frac{\\varphi'(a)}{h} - \\left(\\frac{\\varphi'(a) + \\varphi'(b)}{h} - \\frac{2(\\varphi(b) - \\varphi(a))}{h^2}\\right)$\n$c_2 = \\frac{3(\\varphi(b) - \\varphi(a))}{h^2} - \\frac{2\\varphi'(a) + \\varphi'(b)}{h}$\n\n三次插值函数是 $q(\\alpha) = g(\\alpha-a) = c_3(\\alpha-a)^3 + c_2(\\alpha-a)^2 + c_1(\\alpha-a) + c_0$。其导数是 $q'(\\alpha) = g'(\\alpha-a) = 3c_3(\\alpha-a)^2 + 2c_2(\\alpha-a) + c_1$。这显然是关于 $\\alpha$ 的二次多项式。根据定义，$q(\\alpha)$ 的驻点是使 $q'(\\alpha)=0$ 的 $\\alpha$ 值。这些值是二次导数函数的实根。其中一个根将对应于候选极小值点。\n\n### 第二部分：三次极小值点的计算\n\n给定以下数据：\n- 区间端点：$a=0$, $b=1$。这意味着 $h = b-a = 1$。因为 $a=0$，所以 $\\alpha=\\theta$。\n- 函数值和导数：\n    - $\\varphi(0) = 5$\n    - $\\varphi'(0) = -0.4$\n    - $\\varphi(1) = \\frac{1447}{300}$\n    - $\\varphi'(1) = -0.02$\n\n我们计算系数 $c_1, c_2, c_3$（$c_0 = \\varphi(0)$ 不需要用于寻找极小值点）：\n$c_1 = \\varphi'(0) = -0.4$\n\n$c_2 = \\frac{3(\\varphi(1) - \\varphi(0))}{1^2} - \\frac{2\\varphi'(0) + \\varphi'(1)}{1} = 3\\left(\\frac{1447}{300} - 5\\right) - (2(-0.4) + (-0.02))$\n$c_2 = 3\\left(\\frac{1447 - 1500}{300}\\right) - (-0.8 - 0.02) = 3\\left(-\\frac{53}{300}\\right) - (-0.82)$\n$c_2 = -\\frac{53}{100} + 0.82 = -0.53 + 0.82 = 0.29$\n\n$c_3 = \\frac{\\varphi'(0) + \\varphi'(1)}{1^2} - \\frac{2(\\varphi(1) - \\varphi(0))}{1^3} = (-0.4 - 0.02) - 2\\left(-\\frac{53}{300}\\right)$\n$c_3 = -0.42 + \\frac{106}{300} = -\\frac{42}{100} + \\frac{53}{150} = -\\frac{126}{300} + \\frac{106}{300} = -\\frac{20}{300} = -\\frac{1}{15}$\n\n插值三次函数的导数是 $q'(\\alpha) = 3c_3 \\alpha^2 + 2c_2 \\alpha + c_1$。我们令它为零以寻找驻点：\n$3\\left(-\\frac{1}{15}\\right)\\alpha^2 + 2(0.29)\\alpha + (-0.4) = 0$\n$-\\frac{1}{5}\\alpha^2 + 0.58\\alpha - 0.4 = 0$\n\n为简化，我们乘以 $-5$：\n$\\alpha^2 - 2.9\\alpha + 2 = 0$\n\n我们使用二次公式求解这个关于 $\\alpha$ 的二次方程：\n$\\alpha = \\frac{-(-2.9) \\pm \\sqrt{(-2.9)^2 - 4(1)(2)}}{2(1)} = \\frac{2.9 \\pm \\sqrt{8.41 - 8}}{2} = \\frac{2.9 \\pm \\sqrt{0.41}}{2}$\n\n两个驻点是 $\\alpha_1 = \\frac{2.9 - \\sqrt{0.41}}{2}$ 和 $\\alpha_2 = \\frac{2.9 + \\sqrt{0.41}}{2}$。\n\n为了确定哪个点是极小值点，我们考察二阶导数 $q''(\\alpha) = 6c_3 \\alpha + 2c_2$。\n$q''(\\alpha) = 6\\left(-\\frac{1}{15}\\right)\\alpha + 2(0.29) = -\\frac{2}{5}\\alpha + 0.58$\n对于局部极小值，我们要求 $q''(\\alpha)  0$。\n$-\\frac{2}{5}\\alpha + 0.58  0 \\implies 0.58  \\frac{2}{5}\\alpha \\implies \\alpha  \\frac{5}{2}(0.58) = 1.45$\n\n两个根约等于 $\\alpha \\approx \\frac{2.9 \\pm 0.6403}{2}$，这给出 $\\alpha_1 \\approx 1.13$ 和 $\\alpha_2 \\approx 1.77$。\n由于 $\\alpha_1  1.45$，它对应于局部极小值。\n三次候选极小值点是 $\\alpha_{\\mathrm{cubic}} = \\frac{2.9 - \\sqrt{0.41}}{2}$。\n\n我们计算数值并四舍五入到四位有效数字：\n$\\alpha_{\\mathrm{cubic}} \\approx \\frac{2.9 - 0.6403124}{2} = \\frac{2.2596876}{2} = 1.1298438 \\approx 1.130$\n\n### 第三部分：结果分析\n\n计算出的极小值点是 $\\alpha_{\\mathrm{cubic}} \\approx 1.130$。区间限定是 $[a, b] = [0, 1]$。由于 $1.130 > 1$，三次候选极小值点 $\\alpha_{\\mathrm{cubic}}$ 位于当前区间之外。\n\n这种情况可能发生，因为赫米特插值仅保证多项式 $q(\\alpha)$ 及其导数 $q'(\\alpha)$ 在端点 $a$ 和 $b$ 处与函数 $\\varphi(\\alpha)$ 及其导数 $\\varphi'(\\alpha)$ 相匹配。它对多项式在这些点之间的行为不施加任何约束。$q(\\alpha)$ 的驻点位置由四个数据值决定。\n\n在许多线搜索算法中，确保极小值被限定的一个关键条件是端点处的导数符号相反，即 $\\varphi'(a)  0$ 和 $\\varphi'(b)  0$。在这种情况下，介值定理保证了真实函数的驻点存在于 $(a, b)$ 区间内，并且插值函数也可能将其极小值点置于该区间内。\n\n然而，在给定的问题中，两个导数都是负的：$\\varphi'(0) = -0.4$ 和 $\\varphi'(1) = -0.02$。这表明函数在区间的两端都是递减的。数据表明，极小值很可能在 $\\alpha  1$ 处找到，而三次插值函数通过将其极小值点置于 $\\alpha_{\\mathrm{cubic}} \\approx 1.130$ 来反映这一点。\n\n正是这种可能性，使得实际的线搜索算法会采用安全措施。如果插值（三次或二次）产生的步长候选值超出了可接受的范围，或者未能使函数值充分下降，算法必须回退到一种更稳健但更慢的方法，如二分法，以在可信区间内生成新的试验点。这能确保收敛性和稳定性。",
            "answer": "$$\n\\boxed{1.130}\n$$"
        },
        {
            "introduction": "理论学习的最终目的是解决实际问题。本练习将带您从理论计算走向代码实现，亲手测试不同线搜索策略在复杂问题上的表现。您将面对著名的 Rosenbrock 函数——一个以其狭窄、弯曲的山谷地形而闻名的优化难题。除了标准的单调回溯线搜索，您还将实现一种更高级的非单调线搜索策略（GLL），该策略允许函数值在短期内适度上升，以期“抄近道”穿越复杂区域，从而可能获得更快的全局收敛速度。通过对比两种方法在求解 Rosenbrock 问题时的迭代次数，您将获得关于算法性能的宝贵直观认识。",
            "id": "3143371",
            "problem": "设计并实现两种线搜索策略，在一个梯度下降框架下最小化一个二阶连续可微函数，并比较它们的迭代次数。目标函数是二维变量的 Rosenbrock 函数，对于 $x = (x_1,x_2)$ 定义为 $f(x) = (1 - x_1)^2 + 100(x_2 - x_1^2)^2$。使用以下基本原理：可微函数在点 $x$ 沿方向 $p$ 的一阶泰勒模型，以及将 $p$ 选为负梯度 $p = -\\nabla f(x)$ 的下降方向概念。从这些基本原理出发进行推导，不要引用任何未由此证明的专门公式。\n\n您的任务是：\n- 实现一个梯度下降方法，在第 $k$ 次迭代时，计算搜索方向 $p_k$ 和步长 $\\alpha_k$，然后更新 $x_{k+1} = x_k + \\alpha_k p_k$。\n- 实现一个单调回溯线搜索，确保相对于当前迭代点有充分下降。其接受准则必须基于从一阶模型推导出的充分下降概念，并使用参数 $\\alpha_0$、$c_1$ 和 $\\beta$，其中 $0  c_1  1$ 且 $0  \\beta  1$。\n- 实现一个 Grippo–Lampariello–Lucidi (GLL) 意义上的非单调线搜索，该方法通过与一个由近期函数值固定大小窗口定义的参考值进行比较，允许 $f(x)$ 暂时增加。使用窗口大小参数 $m \\in \\mathbb{N}$。\n- 两种策略都使用搜索方向 $p_k = -\\nabla f(x_k)$。\n- 当梯度的欧几里得范数满足 $\\|\\nabla f(x_k)\\|_2 \\leq \\text{tol}$ 或达到最大迭代次数时终止。\n\n实现这两种策略，并在 Rosenbrock 函数上进行测试。对于每个测试用例，测量每种方法达到终止准则所需的迭代次数，并以整数对 $[\\text{iters}_{\\text{monotone}}, \\text{iters}_{\\text{nonmonotone}}]$ 的形式报告迭代次数。\n\n测试套件：\n- 情况1（在弯曲谷部附近的常规行为）：初始点 $x^{(0)} = (-1.2, 1.0)$，容差 $\\text{tol} = 10^{-6}$，最大迭代次数 $N_{\\max} = 10000$，初始步长 $\\alpha_0 = 1.0$，充分下降参数 $c_1 = 10^{-4}$，回溯因子 $\\beta = 0.5$，GLL 内存 $m = 5$。\n- 情况2（在最小值点的边界条件）：初始点 $x^{(0)} = (1.0, 1.0)$，容差 $\\text{tol} = 10^{-6}$，最大迭代次数 $N_{\\max} = 10000$，$\\alpha_0 = 1.0$，$c_1 = 10^{-4}$，$\\beta = 0.5$，$m = 5$。\n- 情况3（退化的非单调窗口等同于单调情况）：初始点 $x^{(0)} = (-1.2, 1.0)$，容差 $\\text{tol} = 10^{-6}$，最大迭代次数 $N_{\\max} = 10000$，$\\alpha_0 = 1.0$，$c_1 = 10^{-4}$，$\\beta = 0.5$，$m = 1$。\n- 情况4（从不同的盆地开始）：初始点 $x^{(0)} = (0.0, 0.0)$，容差 $\\text{tol} = 10^{-6}$，最大迭代次数 $N_{\\max} = 10000$，$\\alpha_0 = 1.0$，$c_1 = 10^{-4}$，$\\beta = 0.5$，$m = 10$。\n- 情况5（更严格的容差）：初始点 $x^{(0)} = (-1.2, 1.0)$，容差 $\\text{tol} = 10^{-8}$，最大迭代次数 $N_{\\max} = 20000$，$\\alpha_0 = 1.0$，$c_1 = 10^{-4}$，$\\beta = 0.5$，$m = 5$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素对应一个测试用例，并且本身是一个包含两个整数 $[\\text{iters}_{\\text{monotone}}, \\text{iters}_{\\text{nonmonotone}}]$ 的列表。例如：$[[12,9],[0,0],[...],...]$。\n- 此问题不涉及物理单位或角度，因此输出中无需包含。",
            "solution": "该问题要求在一个梯度下降优化框架内，设计、实现并比较两种线搜索策略——单调回溯和非单调 Grippo–Lampariello–Lucidi (GLL) 策略。目标是最小化二维 Rosenbrock 函数。\n\n**1. 预备知识：Rosenbrock 函数与梯度下降**\n\n最小化的目标是 Rosenbrock 函数，这是一个用于优化算法的标准基准测试，以其狭窄、弯曲的抛物线形山谷而闻名。在二维空间中，对于向量 $x = (x_1, x_2)$，它被定义为：\n$$f(x) = (1 - x_1)^2 + 100(x_2 - x_1^2)^2$$\n该函数是二阶连续可微的。其全局最小值点在 $x = (1, 1)$，在该点处 $f(1, 1) = 0$。\n\n优化方法是梯度下降法，一种迭代式的一阶算法。在每次迭代 $k$ 中，当前迭代点 $x_k$ 通过沿最速下降方向（即梯度的负方向 $p_k = -\\nabla f(x_k)$）移动一定距离（步长 $\\alpha_k$）来更新到新的迭代点 $x_{k+1}$。更新规则为：\n$$x_{k+1} = x_k + \\alpha_k p_k$$\n为实现此方法，我们必须首先计算 Rosenbrock 函数的梯度 $\\nabla f(x)$。其偏导数如下：\n$$\n\\frac{\\partial f}{\\partial x_1} = -2(1 - x_1) + 100 \\cdot 2(x_2 - x_1^2)(-2x_1) = 400x_1^3 - 400x_1x_2 + 2x_1 - 2\n$$\n$$\n\\frac{\\partial f}{\\partial x_2} = 100 \\cdot 2(x_2 - x_1^2)(1) = 200(x_2 - x_1^2)\n$$\n因此，梯度向量为：\n$$\n\\nabla f(x) = \\begin{pmatrix} 400x_1^3 - 400x_1x_2 + 2x_1 - 2 \\\\ 200(x_2 - x_1^2) \\end{pmatrix}\n$$\n问题的核心在于每次迭代时使用两种不同的线搜索策略来确定步长 $\\alpha_k$。\n\n**2. 单调回溯线搜索**\n\n该策略确保在每次迭代中函数值都有充分的下降。接受步长 $\\alpha$ 的准则源于函数 $f$ 在 $x_k$ 点沿搜索方向 $p_k$ 的一阶泰勒展开：\n$$f(x_k + \\alpha p_k) \\approx f(x_k) + \\alpha \\nabla f(x_k)^T p_k$$\n由于 $p_k = -\\nabla f(x_k)$，方向导数 $\\nabla f(x_k)^T p_k = -\\nabla f(x_k)^T \\nabla f(x_k) = -\\|\\nabla f(x_k)\\|_2^2$ 是负的（只要 $x_k$ 不是一个驻点）。因此，线性模型预测对于任何正的 $\\alpha$，$f$ 都会下降。\n\n然而，函数是曲线的，因此我们不能期望实际的下降量与线性预测相匹配。充分下降条件，也称为 Armijo 条件，要求实际下降量至少是预测下降量的一个分数 $c_1$：\n$$f(x_k + \\alpha p_k) \\leq f(x_k) + c_1 \\alpha \\nabla f(x_k)^T p_k$$\n其中 $c_1 \\in (0, 1)$ 是一个很小的常数，通常在 $10^{-4}$ 的数量级。\n\n寻找合适 $\\alpha_k$ 的回溯算法如下：\n1. 从一个初始试验步长 $\\alpha = \\alpha_0$ 开始（例如 $\\alpha_0 = 1.0$）。\n2. 当充分下降条件 $f(x_k + \\alpha p_k) \\leq f(x_k) + c_1 \\alpha \\nabla f(x_k)^T p_k$ 不满足时：\n3. 将步长乘以一个回溯因子 $\\beta \\in (0, 1)$ 进行缩减：$\\alpha \\leftarrow \\beta \\alpha$。\n4. 第一个满足条件的 $\\alpha$ 值被选为步长 $\\alpha_k$。\n\n**3. 非单调线搜索 (Grippo–Lampariello–Lucidi)**\n\n在每一步都严格强制下降在狭窄、蜿蜒的山谷中可能效率低下，因为它可能迫使采用过小的步长。GLL 非单调策略通过允许函数值偶尔增加来放宽这一要求。这是通过修改 Armijo 条件的右侧来实现的。它不是将 $f(x_k + \\alpha p_k)$ 与当前值 $f(x_k)$ 比较，而是与一个代表近期函数值历史的参考值 $R_k$进行比较。\n\n参考值 $R_k$ 定义为包括当前迭代在内的最近 $m$ 次迭代的函数最大值：\n$$R_k = \\max_{0 \\leq j \\leq \\min(k, m-1)} \\{ f(x_{k-j}) \\}$$\n其中 $m \\in \\mathbb{N}$ 是窗口的内存大小。\n\n非单调充分下降条件则为：\n$$f(x_k + \\alpha p_k) \\leq R_k + c_1 \\alpha \\nabla f(x_k)^T p_k$$\n寻找 $\\alpha_k$ 的回溯过程与单调情况相同，但使用的是这个修改后的条件。通过允许新点与来自最近历史中可能更高的参考值进行评估，该策略可以接受更大的步长，这可能使其能更有效地“横穿”山谷的曲线，从而可能导致更快的整体收敛。\n\n如果内存参数设置为 $m=1$，那么 $R_k = \\max\\{f(x_k)\\} = f(x_k)$，GLL 线搜索就完全退化为单调回溯线搜索。\n\n**4. 实现与终止条件**\n\n两种策略都将在一个梯度下降求解器中实现。每次迭代 $k$ 的过程如下：\n1. 计算梯度 $g_k = \\nabla f(x_k)$。\n2. 检查终止条件：如果梯度的欧几里得范数 $\\|\\nabla f(x_k)\\|_2$ 小于或等于指定的容差 $\\text{tol}$，则算法收敛。\n3. 定义搜索方向 $p_k = -g_k$。\n4. 使用单调或非单调回溯过程找到一个可接受的步长 $\\alpha_k$。\n5. 更新迭代点：$x_{k+1} = x_k + \\alpha_k p_k$。\n如果达到最大迭代次数 $N_{\\max}$，过程也将终止。记录每种方法和每个测试用例收敛所需的迭代次数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import deque\n\ndef solve():\n    \"\"\"\n    Designs, implements, and compares monotone and non-monotone line search\n    strategies for minimizing the Rosenbrock function using gradient descent.\n    \"\"\"\n\n    # === Function Definitions ===\n\n    def rosenbrock(x: np.ndarray) - float:\n        \"\"\"Computes the Rosenbrock function value.\"\"\"\n        return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n\n    def rosenbrock_grad(x: np.ndarray) - np.ndarray:\n        \"\"\"Computes the gradient of the Rosenbrock function.\"\"\"\n        grad_x1 = 400 * x[0]**3 - 400 * x[0] * x[1] + 2 * x[0] - 2\n        grad_x2 = 200 * (x[1] - x[0]**2)\n        return np.array([grad_x1, grad_x2])\n\n    # === Gradient Descent Implementations ===\n\n    def gradient_descent_monotone(x0: np.ndarray, tol: float, n_max: int, \n                                  alpha0: float, c1: float, beta: float) - int:\n        \"\"\"Gradient descent with monotone backtracking line search.\"\"\"\n        x = np.copy(x0)\n        for k in range(n_max):\n            g = rosenbrock_grad(x)\n            \n            if np.linalg.norm(g) = tol:\n                return k\n\n            p = -g\n            \n            # Monotone backtracking line search (Armijo condition)\n            alpha = alpha0\n            f_x = rosenbrock(x)\n            grad_f_dot_p = np.dot(g, p)\n            \n            while rosenbrock(x + alpha * p)  f_x + c1 * alpha * grad_f_dot_p:\n                alpha *= beta\n                \n            x = x + alpha * p\n            \n        return n_max\n\n    def gradient_descent_nonmonotone(x0: np.ndarray, tol: float, n_max: int, \n                                     alpha0: float, c1: float, beta: float, m: int) - int:\n        \"\"\"Gradient descent with non-monotone (GLL) line search.\"\"\"\n        x = np.copy(x0)\n        # Use a deque for an efficient fixed-size history of function values\n        f_history = deque(maxlen=m)\n        \n        for k in range(n_max):\n            f_k = rosenbrock(x)\n            g = rosenbrock_grad(x)\n\n            if np.linalg.norm(g) = tol:\n                return k\n\n            # Add current function value to history before computing reference\n            f_history.append(f_k)\n            reference_f = np.max(f_history)\n            \n            p = -g\n            \n            # Non-monotone line search\n            alpha = alpha0\n            grad_f_dot_p = np.dot(g, p)\n            \n            while rosenbrock(x + alpha * p)  reference_f + c1 * alpha * grad_f_dot_p:\n                alpha *= beta\n            \n            x = x + alpha * p\n            \n        return n_max\n\n    # === Test Suite Execution ===\n\n    test_cases = [\n        # Case 1: general behavior\n        {'x0': np.array([-1.2, 1.0]), 'tol': 1e-6, 'n_max': 10000, 'alpha0': 1.0, 'c1': 1e-4, 'beta': 0.5, 'm': 5},\n        # Case 2: start at minimizer\n        {'x0': np.array([1.0, 1.0]), 'tol': 1e-6, 'n_max': 10000, 'alpha0': 1.0, 'c1': 1e-4, 'beta': 0.5, 'm': 5},\n        # Case 3: nonmonotone with m=1 (should equal monotone)\n        {'x0': np.array([-1.2, 1.0]), 'tol': 1e-6, 'n_max': 10000, 'alpha0': 1.0, 'c1': 1e-4, 'beta': 0.5, 'm': 1},\n        # Case 4: different starting point\n        {'x0': np.array([0.0, 0.0]), 'tol': 1e-6, 'n_max': 10000, 'alpha0': 1.0, 'c1': 1e-4, 'beta': 0.5, 'm': 10},\n        # Case 5: stricter tolerance\n        {'x0': np.array([-1.2, 1.0]), 'tol': 1e-8, 'n_max': 20000, 'alpha0': 1.0, 'c1': 1e-4, 'beta': 0.5, 'm': 5},\n    ]\n\n    results_str_list = []\n    for case in test_cases:\n        iters_mono = gradient_descent_monotone(\n            case['x0'], case['tol'], case['n_max'], case['alpha0'], case['c1'], case['beta']\n        )\n        \n        iters_nonmono = gradient_descent_nonmonotone(\n            case['x0'], case['tol'], case['n_max'], case['alpha0'], case['c1'], case['beta'], case['m']\n        )\n        \n        results_str_list.append(f\"[{iters_mono},{iters_nonmono}]\")\n\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```"
        }
    ]
}