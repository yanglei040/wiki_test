{
    "hands_on_practices": [
        {
            "introduction": "To begin, let's ground our understanding of coordinate descent with a concrete calculation. This first exercise  guides you through one full cycle of the algorithm on a simple, two-variable quadratic function. By performing the step-by-step minimization along each coordinate axis, you will directly apply the core principle of the method and build a foundational intuition for how it navigates the optimization landscape.",
            "id": "2164480",
            "problem": "Consider the function of two variables $f(x,y) = 2x^2 + y^2 + xy - 6x - 5y$. An optimization routine is employed to find the minimum of this function using the coordinate descent algorithm. The process starts from the initial point $(x_0, y_0) = (0, 0)$.\n\nYou are asked to perform exactly one full cycle of coordinate descent. The update order within the cycle is as follows: first, minimize the function with respect to the $x$-coordinate, and then, using the newly found $x$-coordinate, minimize the function with respect to the $y$-coordinate.\n\nDetermine the coordinates $(x_1, y_1)$ of the point after this single cycle. Your answer should consist of the two coordinate values, expressed as exact fractions.",
            "solution": "We perform one full cycle of coordinate descent on the quadratic function $f(x,y) = 2x^{2} + y^{2} + xy - 6x - 5y$ starting from $(x_{0}, y_{0}) = (0, 0)$, updating first $x$ and then $y$. For a fixed coordinate, minimizing a differentiable function with respect to that coordinate is done by setting the corresponding partial derivative to zero; since $f$ is quadratic and strictly convex in each coordinate (as verified by positive second partial derivatives), this yields the unique minimizer in that coordinate.\n\nFirst, minimize with respect to $x$ holding $y = y_{0} = 0$ fixed. Compute the partial derivative with respect to $x$:\n$$\n\\frac{\\partial f}{\\partial x} = 4x + y - 6.\n$$\nSet it to zero at $y=0$:\n$$\n4x + 0 - 6 = 0 \\quad \\Longrightarrow \\quad 4x = 6 \\quad \\Longrightarrow \\quad x = \\frac{3}{2}.\n$$\nSince $\\frac{\\partial^{2} f}{\\partial x^{2}} = 4 > 0$, this is the unique minimizer in $x$ for fixed $y=0$. Thus, after the $x$-update we have $x_{1} = \\frac{3}{2}$ and $y$ remains $0$.\n\nNext, minimize with respect to $y$ holding $x = x_{1} = \\frac{3}{2}$ fixed. Compute the partial derivative with respect to $y$:\n$$\n\\frac{\\partial f}{\\partial y} = 2y + x - 5.\n$$\nSet it to zero at $x=\\frac{3}{2}$:\n$$\n2y + \\frac{3}{2} - 5 = 0 \\quad \\Longrightarrow \\quad 2y - \\frac{7}{2} = 0 \\quad \\Longrightarrow \\quad 2y = \\frac{7}{2} \\quad \\Longrightarrow \\quad y = \\frac{7}{4}.\n$$\nSince $\\frac{\\partial^{2} f}{\\partial y^{2}} = 2 > 0$, this is the unique minimizer in $y$ for fixed $x=\\frac{3}{2}$. Therefore, after one full cycle, the updated point is $\\left(x_{1}, y_{1}\\right) = \\left(\\frac{3}{2}, \\frac{7}{4}\\right)$.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3}{2}  \\frac{7}{4}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Having mastered the basic mechanics, we now explore a more subtle but crucial aspect of coordinate descent: the effect of the update order. For functions where variables are coupled, the path the algorithm takes depends on the sequence in which we update the coordinates. This practice  asks you to compare the results of two different update permutations, revealing how this choice can lead to different points after a single cycle and underscoring that the path to the minimum is not unique.",
            "id": "2164438",
            "problem": "Consider the optimization of a two-variable objective function given by\n$$\nf(x, y) = (x + 2y - c_1)^2 + (3x - y - c_2)^2\n$$\nwhere $c_1$ and $c_2$ are real-valued constants.\n\nAn optimization procedure is initiated from the starting point $(x_0, y_0) = (0, 0)$ using the coordinate descent method. In each step of this method, a single coordinate is updated to the value that minimizes the objective function, while the other coordinate is held fixed at its current value. A \"full cycle\" of the algorithm consists of updating each coordinate exactly once.\n\nWe wish to compare the results after one full cycle for two different update orders:\n1.  Let $P_{xy} = (x_1, y_1)$ be the point obtained after one full cycle with the update order $(x, \\text{then } y)$. This means we first update $x$ (call it $x_1$) holding $y$ at $y_0$, and then update $y$ (call it $y_1$) holding $x$ at its new value $x_1$.\n2.  Let $P_{yx} = (x'_1, y'_1)$ be the point obtained after one full cycle with the update order $(y, \\text{then } x)$. This means we first update $y$ (call it $y'_1$) holding $x$ at $x_0$, and then update $x$ (call it $x'_1$) holding $y$ at its new value $y'_1$.\n\nDetermine the four coordinates $x_1, y_1, x'_1$, and $y'_1$ as analytic expressions in terms of the constants $c_1$ and $c_2$. Present your answer as a single $1 \\times 4$ row matrix containing the coordinates in the order $(x_1, y_1, x'_1, y'_1)$.",
            "solution": "We minimize $f(x,y)=(x+2y-c_{1})^{2}+(3x-y-c_{2})^{2}$ coordinate-wise. For fixed $y$, the minimizer in $x$ is obtained by setting $\\frac{\\partial f}{\\partial x}=0$:\n$$\n\\frac{\\partial f}{\\partial x}=2(x+2y-c_{1})+6(3x-y-c_{2})=20x-2y-2c_{1}-6c_{2}.\n$$\nSet to zero and solve for $x$ to get\n$$\n20x-2y-2c_{1}-6c_{2}=0 \\;\\;\\Rightarrow\\;\\; x^{*}(y)=\\frac{y+c_{1}+3c_{2}}{10}.\n$$\nFor fixed $x$, the minimizer in $y$ is obtained by setting $\\frac{\\partial f}{\\partial y}=0$:\n$$\n\\frac{\\partial f}{\\partial y}=4(x+2y-c_{1})-2(3x-y-c_{2})=-2x+10y-4c_{1}+2c_{2}.\n$$\nSet to zero and solve for $y$ to get\n$$\n-2x+10y-4c_{1}+2c_{2}=0 \\;\\;\\Rightarrow\\;\\; y^{*}(x)=\\frac{x+2c_{1}-c_{2}}{5}.\n$$\n\nOrder $(x,\\text{ then }y)$ from $(x_{0},y_{0})=(0,0)$:\n- Update $x$ with $y=y_{0}=0$: \n$$\nx_{1}=x^{*}(0)=\\frac{0+c_{1}+3c_{2}}{10}=\\frac{c_{1}+3c_{2}}{10}.\n$$\n- Update $y$ with $x=x_{1}$:\n$$\ny_{1}=y^{*}(x_{1})=\\frac{x_{1}+2c_{1}-c_{2}}{5}=\\frac{\\frac{c_{1}+3c_{2}}{10}+2c_{1}-c_{2}}{5}=\\frac{21c_{1}-7c_{2}}{50}.\n$$\n\nOrder $(y,\\text{ then }x)$ from $(x_{0},y_{0})=(0,0)$:\n- Update $y$ with $x=x_{0}=0$:\n$$\ny'_{1}=y^{*}(0)=\\frac{0+2c_{1}-c_{2}}{5}=\\frac{2c_{1}-c_{2}}{5}.\n$$\n- Update $x$ with $y=y'_{1}$:\n$$\nx'_{1}=x^{*}(y'_{1})=\\frac{y'_{1}+c_{1}+3c_{2}}{10}=\\frac{\\frac{2c_{1}-c_{2}}{5}+c_{1}+3c_{2}}{10}=\\frac{7c_{1}+14c_{2}}{50}.\n$$\n\nTherefore, the requested row matrix in the order $(x_{1},y_{1},x'_{1},y'_{1})$ is\n$$\n\\begin{pmatrix}\n\\frac{c_{1}+3c_{2}}{10}  \\frac{21c_{1}-7c_{2}}{50}  \\frac{7c_{1}+14c_{2}}{50}  \\frac{2c_{1}-c_{2}}{5}\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{c_{1}+3c_{2}}{10}  \\frac{21c_{1}-7c_{2}}{50}  \\frac{7c_{1}+14c_{2}}{50}  \\frac{2c_{1}-c_{2}}{5}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Coordinate descent's utility extends beyond the smooth, differentiable functions we have considered so far, and many important problems in statistics and machine learning involve non-smooth objectives. This exercise  investigates the algorithm's behavior on the absolute value function $f(x, y) = |x - y|$, a classic example of a non-smooth but convex function. By analyzing its path, you will gain insight into how coordinate descent can navigate landscapes with \"sharp corners\" and why it has become a powerful tool for modern optimization challenges.",
            "id": "2164429",
            "problem": "Consider a function of two variables $f: \\mathbb{R}^2 \\to \\mathbb{R}$ given by $f(x, y) = |x - y|$. We will analyze the behavior of the coordinate descent algorithm on this function. The algorithm starts at an initial point $(x_0, y_0)$ and generates a sequence of points that iteratively minimize the function.\n\nThe coordinate descent algorithm proceeds as follows: for each iteration $k=0, 1, 2, \\dots$, starting from a point $(x_k, y_k)$, it generates the next point $(x_{k+1}, y_{k+1})$ in two steps:\n1.  First, update the $x$-coordinate by solving the one-dimensional minimization problem:\n    $x_{new} = \\arg\\min_{z \\in \\mathbb{R}} f(z, y_k)$.\n2.  Then, update the $y$-coordinate using the new $x$-coordinate:\n    $y_{new} = \\arg\\min_{w \\in \\mathbb{R}} f(x_{new}, w)$.\nThe point for the next iteration is then $(x_{k+1}, y_{k+1}) = (x_{new}, y_{new})$.\n\nLet the sequence of points generated by every single coordinate update be $P_0, P_{0.5}, P_1, P_{1.5}, \\dots$, where $P_0=(x_0, y_0)$, $P_{0.5}=(x_1, y_0)$, $P_1=(x_1, y_1)$, etc. The total path length is the sum of the Euclidean lengths of the segments connecting consecutive points in this sequence until the algorithm converges (i.e., the points no longer change).\n\nConsider two scenarios based on the starting point:\n- **Scenario A**: The starting point is $(x_0, y_0) = (c, c)$ for some constant $c  0$. Let the total path length be $L_A$.\n- **Scenario B**: The starting point is $(x_0, y_0) = (c, -c)$ for the same constant $c  0$. Let the total path length be $L_B$.\n\nCalculate the value of the ratio $\\frac{L_B}{c}$.",
            "solution": "We analyze coordinate descent for $f(x,y)=|x-y|$. For a fixed $y$, define $g(z)=|z-y|$. The function $g$ is minimized when $z=y$, since $|z-y|\\geq 0$ with equality if and only if $z=y$. Thus,\n$$\nx_{\\text{new}}=\\arg\\min_{z\\in\\mathbb{R}}|z-y_{k}|=y_{k}.\n$$\nSimilarly, for a fixed $x$, define $h(w)=|x-w|$, minimized uniquely at $w=x$, so\n$$\ny_{\\text{new}}=\\arg\\min_{w\\in\\mathbb{R}}|x_{\\text{new}}-w|=x_{\\text{new}}.\n$$\nTherefore, each full iteration maps $(x_{k},y_{k})$ to $(x_{k+1},y_{k+1})=(y_{k},y_{k})$.\n\nScenario A: Start at $(x_{0},y_{0})=(c,c)$ with $c0$. The first $x$-update gives $x_{\\text{new}}=y_{0}=c$, so $P_{0.5}=(c,c)=P_{0}$. The $y$-update gives $y_{\\text{new}}=x_{\\text{new}}=c$, so $P_{1}=(c,c)=P_{0.5}$. No movement occurs at any step, hence the total path length is $L_{A}=0$.\n\nScenario B: Start at $(x_{0},y_{0})=(c,-c)$ with $c0$. The first $x$-update gives $x_{\\text{new}}=y_{0}=-c$, so\n$$\nP_{0.5}=(-c,-c).\n$$\nThe Euclidean length of the segment from $P_{0}$ to $P_{0.5}$ is\n$$\n\\sqrt{\\left((-c)-c\\right)^{2}+\\left((-c)-(-c)\\right)^{2}}=\\sqrt{(-2c)^{2}}=2c.\n$$\nThe $y$-update then gives $y_{\\text{new}}=x_{\\text{new}}=-c$, so $P_{1}=(-c,-c)=P_{0.5}$ and contributes zero additional length. Since $x=y$ at $(-c,-c)$, all subsequent updates keep the point unchanged. Therefore, the total path length is $L_{B}=2c$.\n\nHence,\n$$\n\\frac{L_{B}}{c}=\\frac{2c}{c}=2.\n$$",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}