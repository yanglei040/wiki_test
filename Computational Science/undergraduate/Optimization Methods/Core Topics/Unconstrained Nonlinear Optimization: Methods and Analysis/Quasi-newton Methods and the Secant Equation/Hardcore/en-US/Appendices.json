{
    "hands_on_practices": [
        {
            "introduction": "Quasi-Newton methods are designed to \"learn\" the curvature of a function as they proceed. This exercise provides a concrete, step-by-step look at this learning process in action. Starting with the simplest possible guess for the Hessian—the identity matrix—we will perform a single BFGS update and observe how it generates a more sophisticated approximation that begins to capture the true, coupled nature of the objective function's curvature .",
            "id": "3170225",
            "problem": "Consider unconstrained smooth minimization of a strictly convex quadratic objective in $\\mathbb{R}^{2}$ given by $f(x) = \\frac{1}{2} x^T Q x$, where $Q \\in \\mathbb{R}^{2 \\times 2}$ is symmetric positive definite. The gradient is $\\nabla f(x) = Q x$ and the Hessian is the constant matrix $Q$. A quasi-Newton method builds successive symmetric positive definite approximations $B_{k}$ to the Hessian by enforcing the secant equation $B_{k+1} s_{k} = y_{k}$, where $s_{k} = x_{k+1} - x_{k}$ and $y_{k} = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$. The Broyden–Fletcher–Goldfarb–Shanno (BFGS) update is obtained by choosing $B_{k+1}$ that satisfies the secant equation and makes the least possible change to $B_{k}$ under a positive-definite metric while preserving symmetry and positive definiteness.\n\nWork with the specific data $Q = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix}$, initial point $x_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, and initial Hessian approximation $B_{0} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$. Take one quasi-Newton step using the search direction $p_{0} = - B_{0}^{-1} \\nabla f(x_{0})$ and the step length $\\alpha_{0} = \\frac{1}{2}$ to form $x_{1} = x_{0} + \\alpha_{0} p_{0}$. Define $s_{0} = x_{1} - x_{0}$ and $y_{0} = \\nabla f(x_{1}) - \\nabla f(x_{0})$. Then perform one symmetric rank-two BFGS update to obtain $B_{1}$.\n\nCompute the $(1,2)$ (upper-right off-diagonal) entry of $B_{1}$ and express your final numeric value exactly as a simplified fraction. Briefly interpret, in terms of the secant equation and curvature coupling, why an off-diagonal entry can emerge from an initially diagonal $B_{0}$ even after a single update. The final reported quantity must be the single number equal to the $(1,2)$ entry of $B_{1}$.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard application of the BFGS quasi-Newton method for a convex quadratic function. All necessary data are provided and are internally consistent.\n\nThe objective function is $f(x) = \\frac{1}{2} x^T Q x$ with $Q = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix}$. The gradient is $\\nabla f(x) = Qx$.\nThe initial conditions are the point $x_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and the Hessian approximation $B_{0} = I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$.\n\nFirst, we compute the gradient at the initial point $x_{0}$:\n$$\n\\nabla f(x_{0}) = Q x_{0} = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n$$\n\nNext, we compute the search direction $p_{0}$ using the initial Hessian approximation $B_{0}$:\n$$\np_{0} = -B_{0}^{-1} \\nabla f(x_{0}) = -I^{-1} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix}\n$$\n\nUsing the given step length $\\alpha_{0} = \\frac{1}{2}$, we find the next iterate $x_{1}$ and the step vector $s_{0}$:\n$$\ns_{0} = \\alpha_{0} p_{0} = \\frac{1}{2} \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\end{pmatrix}\n$$\n$$\nx_{1} = x_{0} + s_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix}\n$$\n\nNow, we compute the gradient at the new point $x_{1}$ to find the change in gradient $y_{0}$:\n$$\n\\nabla f(x_{1}) = Q x_{1} = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{3}{2} \\end{pmatrix}\n$$\n$$\ny_{0} = \\nabla f(x_{1}) - \\nabla f(x_{0}) = \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{3}{2} \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -\\frac{5}{2} \\\\ -\\frac{5}{2} \\end{pmatrix}\n$$\nFor a quadratic function, we have the property $y_{k} = Q s_{k}$. We can verify this:\n$$\nQ s_{0} = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} -2 - \\frac{1}{2} \\\\ -1 - \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{5}{2} \\\\ -\\frac{5}{2} \\end{pmatrix} = y_{0}\n$$\nThe calculation is consistent.\n\nThe BFGS update formula for $B_{1}$ is:\n$$\nB_{1} = B_{0} - \\frac{B_{0} s_{0} s_{0}^T B_{0}}{s_{0}^T B_{0} s_{0}} + \\frac{y_{0} y_{0}^T}{y_{0}^T s_{0}}\n$$\nSince $B_{0} = I$, the formula simplifies to:\n$$\nB_{1} = I - \\frac{s_{0} s_{0}^T}{s_{0}^T s_{0}} + \\frac{y_{0} y_{0}^T}{y_{0}^T s_{0}}\n$$\n\nWe need to compute the scalar denominators and the outer product matrices.\nThe first denominator is:\n$$\ns_{0}^T s_{0} = (-1)^{2} + \\left(-\\frac{1}{2}\\right)^{2} = 1 + \\frac{1}{4} = \\frac{5}{4}\n$$\nThe second denominator is:\n$$\ny_{0}^T s_{0} = \\begin{pmatrix} -\\frac{5}{2}  -\\frac{5}{2} \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\left(-\\frac{5}{2}\\right)(-1) + \\left(-\\frac{5}{2}\\right)\\left(-\\frac{1}{2}\\right) = \\frac{5}{2} + \\frac{5}{4} = \\frac{15}{4}\n$$\nThe value $y_{0}^T s_{0}$ must be positive to ensure $B_{1}$ can be positive definite, which holds here.\n\nThe problem asks for the $(1,2)$ entry of $B_{1}$, which we denote as $(B_{1})_{1,2}$. We can compute this entry directly without forming the entire matrix $B_1$:\n$$\n(B_{1})_{1,2} = (I)_{1,2} - \\frac{(s_{0} s_{0}^T)_{1,2}}{s_{0}^T s_{0}} + \\frac{(y_{0} y_{0}^T)_{1,2}}{y_{0}^T s_{0}}\n$$\nThe $(1,2)$ entry of the identity matrix $I$ is $0$.\nThe outer product matrices' entries are:\n$$\n(s_{0} s_{0}^T)_{1,2} = (s_{0})_{1} (s_{0})_{2} = (-1)\\left(-\\frac{1}{2}\\right) = \\frac{1}{2}\n$$\n$$\n(y_{0} y_{0}^T)_{1,2} = (y_{0})_{1} (y_{0})_{2} = \\left(-\\frac{5}{2}\\right)\\left(-\\frac{5}{2}\\right) = \\frac{25}{4}\n$$\nSubstituting these values into the expression for $(B_{1})_{1,2}$:\n$$\n(B_{1})_{1,2} = 0 - \\frac{\\frac{1}{2}}{\\frac{5}{4}} + \\frac{\\frac{25}{4}}{\\frac{15}{4}}\n$$\n$$\n(B_{1})_{1,2} = -\\left(\\frac{1}{2} \\cdot \\frac{4}{5}\\right) + \\frac{25}{15} = -\\frac{4}{10} + \\frac{5}{3} = -\\frac{2}{5} + \\frac{5}{3}\n$$\n$$\n(B_{1})_{1,2} = \\frac{-2 \\cdot 3 + 5 \\cdot 5}{15} = \\frac{-6 + 25}{15} = \\frac{19}{15}\n$$\n\nBrief interpretation:\nAn off-diagonal entry emerges in $B_{1}$ because the update process incorporates information about the true curvature of the objective function, which is encoded in the non-diagonal Hessian matrix $Q$. The initial approximation $B_{0}=I$ assumes no coupling between the variables. The BFGS update corrects this. The core of the update is enforcing the secant equation, $B_{1}s_0 = y_0$. Here, $s_0 = (-1, -1/2)^T$ and $y_0 = (-5/2, -5/2)^T$. Since $y_0$ is not a scalar multiple of $s_0$, the matrix $B_1$ required to perform this mapping cannot be a multiple of the identity matrix; it must contain off-diagonal terms to represent the rotation and shearing effect of the true Hessian. The term $\\frac{y_0 y_0^T}{y_0^T s_0}$ in the BFGS formula explicitly introduces this coupling. The vector $y_0=Q s_0$ captures the effect of the true Hessian $Q$ on the step $s_0$. The outer product $y_0 y_0^T$ creates a rank-one matrix whose structure reflects this interaction. Because both components of $y_0$ are non-zero, the off-diagonal entries of $y_0 y_0^T$ are non-zero, thus injecting the observed curvature coupling into the new Hessian approximation $B_1$.",
            "answer": "$$\\boxed{\\frac{19}{15}}$$"
        },
        {
            "introduction": "While BFGS is the workhorse of quasi-Newton methods, its insistence on maintaining positive-definite Hessian approximations can be a limitation when dealing with non-convex functions. This practice contrasts BFGS with the Symmetric Rank-One (SR1) update in the presence of negative curvature, such as at a saddle point. By working through this example, you will gain insight into how the SR1 method can incorporate information about negative curvature, a feature that can be advantageous in certain optimization scenarios .",
            "id": "3170247",
            "problem": "Consider the unconstrained minimization of the quadratic model function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ given by $f(x)=\\tfrac{1}{2}x^T Qx$, where $Q=\\begin{pmatrix}1  0\\\\ 0  -0.2\\end{pmatrix}$. You observe two consecutive iterates $x_{k}$ and $x_{k+1}$ with displacement $s_{k}=x_{k+1}-x_{k}=\\begin{pmatrix}1\\\\ 1\\end{pmatrix}$. Let $g(x)=\\nabla f(x)=Qx$, so the gradient displacement is $y_{k}=g(x_{k+1})-g(x_{k})$. Take the initial Hessian approximation $B_{k}=I$.\n\nUse only the fundamental definitions that quasi-Newton methods impose through the secant equation and symmetry, and the standard construction principles of the symmetric rank-one (SR1) and Broyden–Fletcher–Goldfarb–Shanno (BFGS) Hessian updates, to do the following:\n\n1) Compute $y_{k}$ from $Q$ and $s_{k}$.\n\n2) Using the SR1 Hessian update from $B_{k}$ with the curvature pair $(s_{k},y_{k})$, construct $B_{k+1}^{\\text{SR1}}$. Using the BFGS Hessian update from $B_{k}$ with the same $(s_{k},y_{k})$, construct $B_{k+1}^{\\text{BFGS}}$. Explain, using the vector $(y_{k}-B_{k}s_{k})$, why the SR1 update is able to incorporate negative curvature aligned with the saddle direction of $Q$ while the BFGS update, under the condition $s_{k}^T y_{k}0$, preserves positive definiteness.\n\n3) Let $x_{k+1}=\\begin{pmatrix}0\\\\ 1\\end{pmatrix}$, so that $g_{k+1}=g(x_{k+1})$. Form one Newton-like step from each updated Hessian, $p_{k+1}^{\\text{SR1}}=-\\left(B_{k+1}^{\\text{SR1}}\\right)^{-1}g_{k+1}$ and $p_{k+1}^{\\text{BFGS}}=-\\left(B_{k+1}^{\\text{BFGS}}\\right)^{-1}g_{k+1}$.\n\n4) The negative curvature eigendirection of $Q$ is the unit vector $v_{-}=\\begin{pmatrix}0\\\\ 1\\end{pmatrix}$. Compute the squared cosines $\\cos^{2}(\\theta_{\\text{SR1}})$ and $\\cos^{2}(\\theta_{\\text{BFGS}})$, where $\\theta_{\\text{method}}$ is the angle between $p_{k+1}^{\\text{method}}$ and $v_{-}$. Then compute the ratio\n$$\nR=\\frac{\\cos^{2}(\\theta_{\\text{SR1}})}{\\cos^{2}(\\theta_{\\text{BFGS}})}.\n$$\n\nExpress the final result for $R$ as a simplified exact fraction. No rounding is required.",
            "solution": "The problem is subjected to validation and is found to be scientifically grounded, well-posed, and objective. All data and conditions are self-contained and consistent. We may proceed with the solution.\n\nThe problem asks for a sequence of calculations related to the SR1 and BFGS quasi-Newton updates for a quadratic function with an indefinite Hessian. We will address each of the four parts of the problem in order.\n\nThe function is $f(x)=\\tfrac{1}{2}x^T Qx$ with $Q=\\begin{pmatrix}1  0\\\\ 0  -0.2\\end{pmatrix}$.\nThe gradient is $g(x)=\\nabla f(x)=Qx$.\nThe initial Hessian approximation is $B_{k}=I=\\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix}$.\nThe displacement vector is $s_{k}=x_{k+1}-x_{k}=\\begin{pmatrix}1\\\\ 1\\end{pmatrix}$.\n\n**Part 1: Compute $y_{k}$**\nThe gradient displacement $y_{k}$ is defined as $y_{k}=g(x_{k+1})-g(x_{k})$. Since $g(x)=Qx$ is a linear function of $x$, we can write:\n$$y_{k} = Qx_{k+1} - Qx_{k} = Q(x_{k+1}-x_{k}) = Qs_{k}$$\nSubstituting the given matrices for $Q$ and $s_{k}$:\n$$y_{k} = \\begin{pmatrix}1  0\\\\ 0  -0.2\\end{pmatrix} \\begin{pmatrix}1\\\\ 1\\end{pmatrix} = \\begin{pmatrix}(1)(1) + (0)(1)\\\\ (0)(1) + (-0.2)(1)\\end{pmatrix} = \\begin{pmatrix}1\\\\ -0.2\\end{pmatrix}$$\n\n**Part 2: Construct $B_{k+1}^{\\text{SR1}}$ and $B_{k+1}^{\\text{BFGS}}$ and analyze**\nFirst, we compute the necessary intermediate quantities.\nThe curvature condition term is $s_{k}^T y_{k}$:\n$$s_{k}^T y_{k} = \\begin{pmatrix}1  1\\end{pmatrix} \\begin{pmatrix}1\\\\ -0.2\\end{pmatrix} = (1)(1) + (1)(-0.2) = 1 - 0.2 = 0.8$$\nSince $s_{k}^T y_{k}  0$, the standard BFGS update is well-defined and will preserve the positive definiteness of $B_{k}$.\n\nFor the SR1 update, we need the vector $y_{k}-B_{k}s_{k}$:\n$$B_{k}s_{k} = I s_{k} = \\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix} \\begin{pmatrix}1\\\\ 1\\end{pmatrix} = \\begin{pmatrix}1\\\\ 1\\end{pmatrix}$$\n$$y_{k}-B_{k}s_{k} = \\begin{pmatrix}1\\\\ -0.2\\end{pmatrix} - \\begin{pmatrix}1\\\\ 1\\end{pmatrix} = \\begin{pmatrix}0\\\\ -1.2\\end{pmatrix}$$\nThe denominator for the SR1 update is $(y_{k}-B_{k}s_{k})^T s_{k}$:\n$$(y_{k}-B_{k}s_{k})^T s_{k} = \\begin{pmatrix}0  -1.2\\end{pmatrix} \\begin{pmatrix}1\\\\ 1\\end{pmatrix} = (0)(1) + (-1.2)(1) = -1.2$$\nSince this denominator is non-zero, the SR1 update is well-defined.\n\nNow, we construct the updated Hessian approximations.\nThe SR1 update formula is:\n$$B_{k+1}^{\\text{SR1}} = B_{k} + \\frac{(y_{k}-B_{k}s_{k})(y_{k}-B_{k}s_{k})^T}{(y_{k}-B_{k}s_{k})^T s_{k}}$$\n$$B_{k+1}^{\\text{SR1}} = \\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix} + \\frac{1}{-1.2} \\begin{pmatrix}0\\\\ -1.2\\end{pmatrix} \\begin{pmatrix}0  -1.2\\end{pmatrix} = \\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix} - \\frac{1}{1.2} \\begin{pmatrix}0  0\\\\ 0  1.44\\end{pmatrix}$$\n$$B_{k+1}^{\\text{SR1}} = \\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix} - \\begin{pmatrix}0  0\\\\ 0  1.2\\end{pmatrix} = \\begin{pmatrix}1  0\\\\ 0  -0.2\\end{pmatrix}$$\nRemarkably, $B_{k+1}^{\\text{SR1}} = Q$. The SR1 update has recovered the exact Hessian of the quadratic function in one step.\n\nThe BFGS update formula is:\n$$B_{k+1}^{\\text{BFGS}} = B_{k} - \\frac{B_{k}s_{k}s_{k}^T B_{k}}{s_{k}^T B_{k}s_{k}} + \\frac{y_{k}y_{k}^T}{y_{k}^T s_{k}}$$\nWe need the terms $s_{k}^T B_{k}s_{k} = s_{k}^T Is_{k} = s_{k}^T s_{k} = 1^{2}+1^{2}=2$, and $B_{k}s_{k} = s_{k}$.\n$$s_{k}s_{k}^T = \\begin{pmatrix}1\\\\ 1\\end{pmatrix}\\begin{pmatrix}1  1\\end{pmatrix} = \\begin{pmatrix}1  1\\\\ 1  1\\end{pmatrix}$$\n$$y_{k}y_{k}^T = \\begin{pmatrix}1\\\\ -0.2\\end{pmatrix}\\begin{pmatrix}1  -0.2\\end{pmatrix} = \\begin{pmatrix}1  -0.2\\\\ -0.2  0.04\\end{pmatrix}$$\nSubstituting into the BFGS formula:\n$$B_{k+1}^{\\text{BFGS}} = \\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix} - \\frac{1}{2}\\begin{pmatrix}1  1\\\\ 1  1\\end{pmatrix} + \\frac{1}{0.8}\\begin{pmatrix}1  -0.2\\\\ -0.2  0.04\\end{pmatrix}$$\n$$B_{k+1}^{\\text{BFGS}} = \\begin{pmatrix}1  0\\\\ 0  1\\end{pmatrix} - \\begin{pmatrix}0.5  0.5\\\\ 0.5  0.5\\end{pmatrix} + \\begin{pmatrix}1.25  -0.25\\\\ -0.25  0.05\\end{pmatrix}$$\n$$B_{k+1}^{\\text{BFGS}} = \\begin{pmatrix}1-0.5+1.25  0-0.5-0.25 \\\\ 0-0.5-0.25  1-0.5+0.05\\end{pmatrix} = \\begin{pmatrix}1.75  -0.75\\\\ -0.75  0.55\\end{pmatrix}$$\n\nAnalysis: The vector $y_{k}-B_{k}s_{k}$ represents the discrepancy between the observed gradient change $y_{k}$ and the change predicted by the current Hessian model $B_{k}$.\nThe SR1 update formula directly incorporates this discrepancy vector in a rank-one correction: $B_{k+1} = B_{k} + \\frac{v v^T}{v^T s_{k}}$, where $v = y_{k}-B_{k}s_{k}$. In this problem, the denominator $v^T s_{k} = -1.2$ is negative. This means the rank-one matrix added to $B_{k}$ is negative semidefinite. This allows the SR1 update to decrease the eigenvalues of the Hessian approximation, and in this specific case, it introduces a negative eigenvalue, correctly identifying the negative curvature of $Q$.\nIn contrast, the BFGS update is constructed to preserve positive definiteness when the curvature condition $y_{k}^T s_{k}0$ holds. Although derived from the same secant equation $B_{k+1}s_{k}=y_{k}$, its structure as a sum of $B_{k}$ (with a rank-one matrix subtracted) and another rank-one matrix $\\frac{y_{k}y_{k}^T}{y_{k}^T s_{k}}$ guarantees that if $B_{k}$ is positive definite, $B_{k+1}^{\\text{BFGS}}$ remains so. It cannot introduce negative curvature. It effectively \"averages\" the curvature information contained in the pair $(s_{k}, y_{k})$ in a way that produces a safe, positive definite model, even if the true Hessian is indefinite.\n\n**Part 3: Newton-like steps**\nWe are given $x_{k+1}=\\begin{pmatrix}0\\\\ 1\\end{pmatrix}$. The gradient at this point is:\n$$g_{k+1} = g(x_{k+1}) = Qx_{k+1} = \\begin{pmatrix}1  0\\\\ 0  -0.2\\end{pmatrix} \\begin{pmatrix}0\\\\ 1\\end{pmatrix} = \\begin{pmatrix}0\\\\ -0.2\\end{pmatrix}$$\nThe search directions are $p_{k+1} = -B_{k+1}^{-1}g_{k+1}$.\n\nFor the SR1 update:\n$$B_{k+1}^{\\text{SR1}} = Q = \\begin{pmatrix}1  0\\\\ 0  -0.2\\end{pmatrix} \\implies (B_{k+1}^{\\text{SR1}})^{-1} = \\begin{pmatrix}1  0\\\\ 0  -5\\end{pmatrix}$$\n$$p_{k+1}^{\\text{SR1}} = -\\begin{pmatrix}1  0\\\\ 0  -5\\end{pmatrix} \\begin{pmatrix}0\\\\ -0.2\\end{pmatrix} = -\\begin{pmatrix}0\\\\ 1\\end{pmatrix} = \\begin{pmatrix}0\\\\ -1\\end{pmatrix}$$\n\nFor the BFGS update:\n$$B_{k+1}^{\\text{BFGS}} = \\begin{pmatrix}1.75  -0.75\\\\ -0.75  0.55\\end{pmatrix} = \\begin{pmatrix}\\frac{7}{4}  -\\frac{3}{4}\\\\ -\\frac{3}{4}  \\frac{11}{20}\\end{pmatrix}$$\nThe determinant is $\\det(B_{k+1}^{\\text{BFGS}}) = (\\frac{7}{4})(\\frac{11}{20}) - (-\\frac{3}{4})^{2} = \\frac{77}{80} - \\frac{9}{16} = \\frac{77-45}{80} = \\frac{32}{80} = \\frac{2}{5}$.\nThe inverse is:\n$$(B_{k+1}^{\\text{BFGS}})^{-1} = \\frac{1}{2/5} \\begin{pmatrix}\\frac{11}{20}  \\frac{3}{4}\\\\ \\frac{3}{4}  \\frac{7}{4}\\end{pmatrix} = \\frac{5}{2} \\begin{pmatrix}\\frac{11}{20}  \\frac{15}{20}\\\\ \\frac{15}{20}  \\frac{35}{20}\\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix}11  15\\\\ 15  35\\end{pmatrix}$$\nThe search direction is:\n$$p_{k+1}^{\\text{BFGS}} = -\\frac{1}{8} \\begin{pmatrix}11  15\\\\ 15  35\\end{pmatrix} \\begin{pmatrix}0\\\\ -0.2\\end{pmatrix} = -\\frac{1}{8} \\begin{pmatrix}11  15\\\\ 15  35\\end{pmatrix} \\begin{pmatrix}0\\\\ -\\frac{1}{5}\\end{pmatrix}$$\n$$p_{k+1}^{\\text{BFGS}} = -\\frac{1}{8} \\begin{pmatrix}-3\\\\ -7\\end{pmatrix} = \\begin{pmatrix}\\frac{3}{8}\\\\ \\frac{7}{8}\\end{pmatrix}$$\n\n**Part 4: Compute the ratio R**\nThe negative curvature eigendirection of $Q$ is $v_{-}=\\begin{pmatrix}0\\\\ 1\\end{pmatrix}$. We have $\\|v_{-}\\| = 1$. The squared cosine of the angle $\\theta$ between a vector $p$ and $v_{-}$ is given by $\\cos^{2}(\\theta) = \\frac{(p^T v_{-})^{2}}{\\|p\\|^{2}\\|v_{-}\\|^{2}} = \\frac{(p^T v_{-})^{2}}{\\|p\\|^{2}}$.\n\nFor the SR1 step:\n$$p_{k+1}^{\\text{SR1}} = \\begin{pmatrix}0\\\\ -1\\end{pmatrix}$$\n$$p_{k+1}^{\\text{SR1}T}v_{-} = \\begin{pmatrix}0  -1\\end{pmatrix}\\begin{pmatrix}0\\\\ 1\\end{pmatrix} = -1$$\n$$\\|p_{k+1}^{\\text{SR1}}\\|^{2} = 0^{2} + (-1)^{2} = 1$$\n$$\\cos^{2}(\\theta_{\\text{SR1}}) = \\frac{(-1)^{2}}{1} = 1$$\n\nFor the BFGS step:\n$$p_{k+1}^{\\text{BFGS}} = \\begin{pmatrix}\\frac{3}{8}\\\\ \\frac{7}{8}\\end{pmatrix}$$\n$$p_{k+1}^{\\text{BFGS}T}v_{-} = \\begin{pmatrix}\\frac{3}{8}  \\frac{7}{8}\\end{pmatrix}\\begin{pmatrix}0\\\\ 1\\end{pmatrix} = \\frac{7}{8}$$\n$$\\|p_{k+1}^{\\text{BFGS}}\\|^{2} = \\left(\\frac{3}{8}\\right)^{2} + \\left(\\frac{7}{8}\\right)^{2} = \\frac{9}{64} + \\frac{49}{64} = \\frac{58}{64} = \\frac{29}{32}$$\n$$\\cos^{2}(\\theta_{\\text{BFGS}}) = \\frac{(\\frac{7}{8})^{2}}{\\frac{29}{32}} = \\frac{\\frac{49}{64}}{\\frac{29}{32}} = \\frac{49}{64} \\cdot \\frac{32}{29} = \\frac{49}{2 \\cdot 29} = \\frac{49}{58}$$\n\nFinally, we compute the ratio $R$:\n$$R = \\frac{\\cos^{2}(\\theta_{\\text{SR1}})}{\\cos^{2}(\\theta_{\\text{BFGS}})} = \\frac{1}{\\frac{49}{58}} = \\frac{58}{49}$$",
            "answer": "$$\\boxed{\\frac{58}{49}}$$"
        },
        {
            "introduction": "The theoretical elegance of the BFGS update relies on a crucial partnership with the line search algorithm, which must ensure the curvature condition $s_k^T y_k  0$ is met. This computational practice dives into this synergy, exploring how the parameters of the strong Wolfe line search conditions directly impact the numerical stability and quality of the Hessian approximation. By monitoring key stability metrics, you will develop an appreciation for the practical challenges and trade-offs involved in implementing a robust quasi-Newton solver .",
            "id": "3170182",
            "problem": "You are to implement a quasi-Newton optimization algorithm that uses a line search satisfying strong Wolfe conditions to investigate how the choice of Wolfe constants affects the stability of the secant update. Work in purely mathematical terms on a deterministic test function. The algorithm must be fully specified and reproducible.\n\nDefine the test function $f:\\mathbb{R}^2\\to\\mathbb{R}$ by the classical Rosenbrock function,\n$$\nf(x) = 100\\,(x_2 - x_1^2)^2 + (1 - x_1)^2,\n$$\nwith gradient $\\nabla f(x)$ computed exactly. Start the iterations at $x_0 = (-1.2,\\,1.0)$. At each iteration $k$, form a descent direction $p_k = -B_k \\nabla f(x_k)$, where $B_k$ is the current approximation of the inverse Hessian matrix. Compute a step length $\\alpha_k$ using a line search that enforces the strong Wolfe conditions with the given coefficients $c_1$ and $c_2$, and then set $s_k = \\alpha_k p_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ with $x_{k+1} = x_k + s_k$.\n\nUpdate $B_k$ using a quasi-Newton formula that satisfies the inverse secant equation $B_{k+1} y_k = s_k$. Only perform the update if $s_k^T y_k  \\varepsilon_{\\text{small}}$ to avoid numerical instability. Otherwise, skip the update and retain $B_{k+1} = B_k$. Use $B_0 = I$ as the initial inverse Hessian approximation.\n\nYou are to monitor and count three stability-related events over the iterations:\n- A “shrink” event occurs if, for $k \\ge 1$, the quantity $s_k^T y_k$ decreases by at least a fixed factor compared to the previous iteration, specifically when $s_k^T y_k  \\gamma\\, s_{k-1}^T y_{k-1}$.\n- An “acute” event occurs when the normalized alignment ratio $r_k = \\dfrac{s_k^T y_k}{\\|s_k\\|\\cdot\\|y_k\\|}$ falls below a threshold $r_{\\text{thr}}$, indicating near-orthogonality of $s_k$ and $y_k$ and potential ill-conditioning of the update.\n- A “skip” event occurs when $s_k^T y_k \\le \\varepsilon_{\\text{small}}$, in which case the update is not performed.\n\nTerminate the algorithm when $\\|\\nabla f(x_k)\\| \\le \\tau_{\\text{grad}}$ or when $k$ reaches $k_{\\max}$. Report whether convergence occurred as a boolean equal to true if and only if the gradient termination criterion was met.\n\nUse the following fixed numerical parameters across all test cases:\n- Gradient tolerance $\\tau_{\\text{grad}} = 10^{-6}$.\n- Maximum iterations $k_{\\max} = 100$.\n- Shrink factor $\\gamma = 0.5$.\n- Acute alignment threshold $r_{\\text{thr}} = 0.05$.\n- Small positivity threshold $\\varepsilon_{\\text{small}} = 10^{-12}$.\n\nYour program must run the algorithm for each of the following test cases, each specifying the Wolfe constants $(c_1,c_2)$:\n1. $(c_1,c_2) = (10^{-4},\\,0.9)$\n2. $(c_1,c_2) = (10^{-4},\\,0.5)$\n3. $(c_1,c_2) = (10^{-2},\\,0.9)$\n4. $(c_1,c_2) = (10^{-4},\\,0.99)$\n5. $(c_1,c_2) = (10^{-1},\\,0.11)$\n\nFor each test case, your program must return a list of four values $[N_{\\text{shrink}},N_{\\text{acute}},N_{\\text{skip}},\\text{converged}]$, where $N_{\\text{shrink}}$ is the number of shrink events, $N_{\\text{acute}}$ is the number of acute events, $N_{\\text{skip}}$ is the number of skip events, and $\\text{converged}$ is the boolean convergence flag.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of the per-test-case lists, with no spaces, enclosed in square brackets, for example, $[[1,0,0,\\text{True}],[2,1,0,\\text{False}]]$ (this is only an example format; your program must output the actual computed values for the specified test suite).",
            "solution": "The problem requires the implementation and analysis of a quasi-Newton optimization algorithm, specifically to study the numerical stability of the inverse Hessian update as a function of the parameters for a strong Wolfe line search. The analysis will be performed on the Rosenbrock benchmark function.\n\nThe problem is computationally well-defined, scientifically sound, and all necessary parameters are provided. It represents a standard task in numerical optimization. The only minor ambiguity is the choice of the quasi-Newton update formula. As the Broyden–Fletcher–Goldfarb–Shanno (BFGS) update is the most prevalent and robust method in practice, it will be used for this solution.\n\nThe core of the algorithm is an iterative procedure to find the minimum of a function $f(x)$. Starting from an initial point $x_0$, the algorithm generates a sequence of points $\\{x_k\\}$ that are expected to converge to a local minimum.\n\n**1. Objective Function and Gradient**\n\nThe test function is the Rosenbrock function in $\\mathbb{R}^2$, defined as:\n$$\nf(x_1, x_2) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2\n$$\nThis function is non-convex and has a unique global minimum at $x^* = (1, 1)$ where $f(x^*) = 0$. The gradient, $\\nabla f(x)$, is computed analytically for precision:\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} -400x_1(x_2 - x_1^2) - 2(1 - x_1) \\\\ 200(x_2 - x_1^2) \\end{pmatrix}\n$$\n\n**2. Quasi-Newton Iteration**\n\nAt each iteration $k$, the next point $x_{k+1}$ is found by moving from the current point $x_k$ along a search direction $p_k$ by a step length $\\alpha_k$:\n$$\nx_{k+1} = x_k + \\alpha_k p_k\n$$\nThe search direction $p_k$ is determined using an approximation of the inverse Hessian matrix, denoted by $B_k$. Newton's method would use the true inverse Hessian, but quasi-Newton methods build an approximation. The search direction is given by:\n$$\np_k = -B_k \\nabla f(x_k)\n$$\nThis direction is guaranteed to be a descent direction (i.e., $\\nabla f(x_k)^T p_k  0$) if $B_k$ is positive definite. The algorithm starts with the identity matrix $B_0 = I$, which is positive definite.\n\n**3. Line Search with Strong Wolfe Conditions**\n\nThe step length $\\alpha_k$ is crucial for convergence. It is determined by a line search procedure that seeks to find an $\\alpha_k  0$ satisfying the strong Wolfe conditions. For given constants $0  c_1  c_2  1$, these conditions are:\n1.  **Sufficient Decrease (Armijo) Condition**: Ensures the step makes sufficient progress in decreasing the objective function.\n    $$\n    f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^T p_k\n    $$\n2.  **Strong Curvature Condition**: Ensures the step length is not excessively small and that the gradient slope has changed sufficiently.\n    $$\n    |\\nabla f(x_k + \\alpha_k p_k)^T p_k| \\le c_2 |\\nabla f(x_k)^T p_k|\n    $$\nA line search that enforces these conditions produces a step length that is both effective and contributes to the stability of the quasi-Newton update.\n\n**4. BFGS Update**\n\nAfter determining $\\alpha_k$ and computing $x_{k+1}$, the inverse Hessian approximation $B_k$ is updated to $B_{k+1}$. The update is based on the most recent step information, encapsulated in two vectors:\n$$\ns_k = x_{k+1} - x_k = \\alpha_k p_k\n$$\n$$\ny_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)\n$$\nThe update must satisfy the inverse secant equation, $B_{k+1} y_k = s_k$. The BFGS formula for updating the inverse Hessian is:\n$$\nB_{k+1} = \\left(I - \\frac{s_k y_k^T}{y_k^T s_k}\\right) B_k \\left(I - \\frac{y_k s_k^T}{y_k^T s_k}\\right) + \\frac{s_k s_k^T}{y_k^T s_k}\n$$\nA critical requirement for this update to be well-defined and to preserve the positive definiteness of the matrix sequence $\\{B_k\\}$ is that the curvature condition $s_k^T y_k  0$ holds. The second Wolfe condition with $c_2  1$ theoretically guarantees this inequality. Numerically, however, $s_k^T y_k$ can become very small, leading to instability. Therefore, the update is performed only if $s_k^T y_k  \\varepsilon_{\\text{small}}$ for a small positive constant $\\varepsilon_{\\text{small}} = 10^{-12}$.\n\n**5. Stability Monitoring**\n\nThe core of the problem is to monitor three events related to the stability of the update, which are tied to the quantity $s_k^T y_k$:\n-   **Skip Event ($N_{\\text{skip}}$)**: If $s_k^T y_k \\le \\varepsilon_{\\text{small}}$, the BFGS update is numerically unstable and is skipped. We set $B_{k+1} = B_k$. This event is counted.\n-   **Acute Event ($N_{\\text{acute}}$)**: The normalized alignment ratio $r_k = \\frac{s_k^T y_k}{\\|s_k\\|\\|y_k\\|}$ measures the cosine of the angle between $s_k$ and $y_k$. A value near zero indicates near-orthogonality, which can lead to ill-conditioning. An event is counted if $r_k  r_{\\text{thr}} = 0.05$.\n-   **Shrink Event ($N_{\\text{shrink}}$)**: A rapid decrease in the curvature $s_k^T y_k$ from one iteration to the next can signal deteriorating quality of the Hessian approximation. An event is counted for $k \\ge 1$ if $s_k^T y_k  \\gamma s_{k-1}^T y_{k-1}$, where the shrink factor is $\\gamma = 0.5$.\n\n**6. Algorithm Termination**\n\nThe algorithm terminates if either the norm of the gradient falls below a tolerance, $\\|\\nabla f(x_k)\\| \\le \\tau_{\\text{grad}} = 10^{-6}$, indicating convergence to a stationary point, or if the maximum number of iterations, $k_{\\max} = 100$, is reached. A boolean flag, `converged`, will report whether the gradient norm criterion was met.\n\nThe implementation will execute this complete algorithm for each specified pair of Wolfe constants $(c_1, c_2)$, count the three types of stability events, and report the final statistics.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import line_search\n\ndef solve():\n    \"\"\"\n    Main function to run the quasi-Newton algorithm for all test cases\n    and print the results in the specified format.\n    \"\"\"\n\n    def f(x):\n        \"\"\"Rosenbrock function.\"\"\"\n        return 100.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2\n\n    def grad_f(x):\n        \"\"\"Gradient of the Rosenbrock function.\"\"\"\n        df_dx1 = -400.0 * x[0] * (x[1] - x[0]**2) - 2.0 * (1.0 - x[0])\n        df_dx2 = 200.0 * (x[1] - x[0]**2)\n        return np.array([df_dx1, df_dx2])\n\n    def run_quasi_newton(c1, c2, x0, k_max, tau_grad, gamma, r_thr, eps_small):\n        \"\"\"\n        Executes the quasi-Newton algorithm for a given set of parameters.\n        \"\"\"\n        x_k = np.copy(x0)\n        B_k = np.eye(2)\n        \n        n_shrink = 0\n        n_acute = 0\n        n_skip = 0\n        converged = False\n        \n        s_y_prev = np.nan\n        \n        k = 0\n        while k  k_max:\n            grad_f_k = grad_f(x_k)\n            grad_norm = np.linalg.norm(grad_f_k)\n            \n            if grad_norm = tau_grad:\n                converged = True\n                break\n            \n            # Search direction\n            p_k = -B_k @ grad_f_k\n            \n            # Line search for step size alpha_k\n            f_k = f(x_k)\n            # A larger maxiter helps line search succeed in tricky cases\n            alpha_k_tuple = line_search(f, grad_f, x_k, p_k, gfk=grad_f_k, old_fval=f_k, c1=c1, c2=c2, maxiter=50)\n            alpha_k = alpha_k_tuple[0]\n\n            if alpha_k is None:\n                # Line search failed, cannot proceed.\n                converged = False\n                break\n                \n            s_k = alpha_k * p_k\n            x_k_plus_1 = x_k + s_k\n            \n            grad_f_k_plus_1 = grad_f(x_k_plus_1)\n            y_k = grad_f_k_plus_1 - grad_f_k\n            \n            s_k_T_y_k = np.dot(s_k, y_k)\n            \n            # --- Stability Monitoring ---\n            \n            # Shrink event\n            if k  0 and not np.isnan(s_y_prev):\n                if s_k_T_y_k  gamma * s_y_prev:\n                    n_shrink += 1\n\n            # Acute event\n            norm_s_k = np.linalg.norm(s_k)\n            norm_y_k = np.linalg.norm(y_k)\n            if norm_s_k  0 and norm_y_k  0:\n                ratio = s_k_T_y_k / (norm_s_k * norm_y_k)\n                if ratio  r_thr:\n                    n_acute += 1\n            elif s_k_T_y_k = 0:\n                # If either norm is zero, and curvature is non-positive,\n                # it's a pathological case of non-alignment.\n                n_acute += 1\n\n            # --- Update Inverse Hessian Approximation ---\n\n            # Skip event\n            if s_k_T_y_k = eps_small:\n                n_skip += 1\n                # B_{k+1} = B_k is implicit (no update)\n            else:\n                rho_k = 1.0 / s_k_T_y_k\n                I = np.eye(2)\n                term1 = I - rho_k * np.outer(s_k, y_k)\n                B_k = term1 @ B_k @ term1.T + rho_k * np.outer(s_k, s_k)\n\n            # Prepare for next iteration\n            x_k = x_k_plus_1\n            s_y_prev = s_k_T_y_k\n            k += 1\n\n        # Final check for convergence if loop completed at k_max\n        if not converged and np.linalg.norm(grad_f(x_k)) = tau_grad:\n            converged = True\n            \n        return [n_shrink, n_acute, n_skip, converged]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (c1, c2)\n        (1e-4, 0.9),\n        (1e-4, 0.5),\n        (1e-2, 0.9),\n        (1e-4, 0.99),\n        (1e-1, 0.11),\n    ]\n\n    # Fixed parameters\n    x0 = np.array([-1.2, 1.0])\n    tau_grad = 1e-6\n    k_max = 100\n    gamma = 0.5\n    r_thr = 0.05\n    eps_small = 1e-12\n\n    results = []\n    for case in test_cases:\n        c1, c2 = case\n        result = run_quasi_newton(c1, c2, x0, k_max, tau_grad, gamma, r_thr, eps_small)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The str() representation of a list uses spaces, so we remove them.\n    # This also handles the boolean True/False correctly.\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}