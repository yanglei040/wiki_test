{
    "hands_on_practices": [
        {
            "introduction": "A common but naive heuristic is to branch on the variable with the largest objective coefficient, as it seems to offer the most direct impact. This exercise challenges that intuition by presenting a scenario where a variable with a negligible objective coefficient provides a surprisingly large bound improvement. By working through this problem , you will gain a deeper appreciation for how tight constraint coupling can make a seemingly insignificant variable a powerful choice for branching, training you to look beyond the objective function and analyze the constraint structure.",
            "id": "3104763",
            "problem": "Consider the following mixed-integer linear program (MILP) for a maximization problem with binary decision variables $x_1$, $x_2$, and $x_3$:\nmaximize $8 x_1 + 7 x_2 + 0.1 x_3$\nsubject to\n$2 x_1 + 2 x_2 + 3 x_3 \\le 4$,\n$x_1 + x_2 - x_3 \\le 1$,\n$0 \\le x_i \\le 1$ for $i \\in \\{1,2,3\\}$,\n$x_i \\in \\{0,1\\}$ for $i \\in \\{1,2,3\\}$.\n\nAt the root node of a Branch and Bound (B) tree, use the linear programming (LP) relaxation, that is, drop the integrality requirements $x_i \\in \\{0,1\\}$ and keep only $0 \\le x_i \\le 1$. Let $z_{\\text{parent}}$ denote the optimal LP objective value at the root. Consider branching on variable $x_3$ at the root node, creating two child nodes with $x_3 = 0$ and $x_3 = 1$, respectively. Let $z_0$ and $z_1$ denote the optimal LP objective values of these two children.\n\nDefine the average bound improvement from branching on $x_3$ as\n$\\Delta_{\\text{avg}} = \\dfrac{\\left(z_{\\text{parent}} - z_0\\right) + \\left(z_{\\text{parent}} - z_1\\right)}{2}$.\nCompute $\\Delta_{\\text{avg}}$ for this instance. Then, in your solution, explain why branching on $x_3$ (whose objective coefficient $0.1$ is small) nonetheless yields a large average bound improvement by referencing how the side constraints interact with $x_3$.\n\nRound your final numerical value of $\\Delta_{\\text{avg}}$ to $3$ significant figures. No units are required or permitted in the final answer.",
            "solution": "We begin from the foundational definitions in mixed-integer optimization: the linear programming (LP) relaxation of a mixed-integer linear program (MILP) provides an upper bound on the true optimal value in a maximization problem, and Branch and Bound (B) branching on a variable splits the feasible region into disjoint subproblems that can tighten this bound. The bound improvement at a branch is measured by the reduction in the LP upper bound relative to the parent. We will compute the parent LP bound $z_{\\text{parent}}$, the child LP bounds $z_0$ and $z_1$, and then the average bound improvement $\\Delta_{\\text{avg}}$.\n\nStep $1$: Solve the root LP relaxation. The root LP relaxation is\nmaximize $8 x_1 + 7 x_2 + 0.1 x_3$\nsubject to\n$2 x_1 + 2 x_2 + 3 x_3 \\le 4$,\n$x_1 + x_2 - x_3 \\le 1$,\n$0 \\le x_i \\le 1$ for $i \\in \\{1,2,3\\}$.\n\nIntroduce the aggregate variable $s = x_1 + x_2$. The constraints become\n$2 s + 3 x_3 \\le 4$,\n$s \\le 1 + x_3$,\n$0 \\le s \\le 2$,\n$0 \\le x_3 \\le 1$,\nand the objective becomes $8 x_1 + 7 x_2 + 0.1 x_3$. For fixed $s$, the expression $8 x_1 + 7 x_2$ is maximized by allocating as much of $s$ as possible to the variable with the larger coefficient. Therefore, the optimal allocation is\nif $s \\le 1$, set $x_1 = s$, $x_2 = 0$, yielding $8 x_1 + 7 x_2 = 8 s$,\nif $1 \\le s \\le 2$, set $x_1 = 1$, $x_2 = s - 1$, yielding $8 x_1 + 7 x_2 = 7 s + 1$.\n\nThus the objective becomes\n$O(s,x_3) = \\begin{cases}\n8 s + 0.1 x_3,  0 \\le s \\le 1, \\\\\n7 s + 1 + 0.1 x_3,  1 \\le s \\le 2,\n\\end{cases}$\nwith the constraints\n$s \\le \\min\\{2 - 1.5 x_3, 1 + x_3\\}$,\n$0 \\le x_3 \\le 1$.\n\nFor each $x_3$, the optimal $s$ is the maximum feasible $s$ because $O(s,x_3)$ is increasing in $s$ in both regimes. Compare the two upper bounds:\n$2 - 1.5 x_3 \\le 1 + x_3$ holds when $1 - 2.5 x_3 \\le 0$, i.e., $x_3 \\ge 0.4$.\nTherefore,\nfor $0 \\le x_3 \\le 0.4$, the tighter is $s \\le 1 + x_3$, so choose $s = 1 + x_3$,\nfor $0.4 \\le x_3 \\le 1$, the tighter is $s \\le 2 - 1.5 x_3$, so choose $s = 2 - 1.5 x_3$.\n\nCase $1$: $0 \\le x_3 \\le 0.4$. Then $s = 1 + x_3 \\in [1,1.4]$, so we are in the regime $1 \\le s \\le 2$ and\n$O(x_3) = 7 s + 1 + 0.1 x_3 = 7(1 + x_3) + 1 + 0.1 x_3 = 8 + 7.1 x_3$,\nwhich is increasing in $x_3$, so within this case the maximum occurs at $x_3 = 0.4$.\n\nCase $2$: $0.4 \\le x_3 \\le 1$. Then $s = 2 - 1.5 x_3$. There are two subcases depending on whether $s \\ge 1$.\nSubcase $2\\text{a}$: $0.4 \\le x_3 \\le \\tfrac{2}{3}$. Here $s \\ge 1$, so\n$O(x_3) = 7 s + 1 + 0.1 x_3 = 7(2 - 1.5 x_3) + 1 + 0.1 x_3 = 15 - 10.4 x_3$,\nwhich is decreasing in $x_3$, so the maximum over this subinterval is at $x_3 = 0.4$.\nSubcase $2\\text{b}$: $\\tfrac{2}{3} \\le x_3 \\le 1$. Here $s \\le 1$, so\n$O(x_3) = 8 s + 0.1 x_3 = 8(2 - 1.5 x_3) + 0.1 x_3 = 16 - 11.9 x_3$,\nwhich is also decreasing in $x_3$, with its maximum at $x_3 = \\tfrac{2}{3}$, but that value is strictly below the value at $x_3 = 0.4$ computed next.\n\nTherefore, the global maximizer is at the boundary $x_3 = 0.4$, with the corresponding $s = 1.4$. In the regime $1 \\le s \\le 2$, the allocation is $x_1 = 1$, $x_2 = 0.4$. Hence the root LP optimal objective value is\n$z_{\\text{parent}} = 8 \\cdot 1 + 7 \\cdot 0.4 + 0.1 \\cdot 0.4 = 8 + 2.8 + 0.04 = 10.84$.\n\nStep $2$: Solve the LP relaxations of the two child nodes that branch on $x_3$.\n\nChild with $x_3 = 0$:\nConstraints become $2 x_1 + 2 x_2 \\le 4$ and $x_1 + x_2 \\le 1$. Combining gives $x_1 + x_2 \\le 1$. To maximize $8 x_1 + 7 x_2$, set $x_1 = 1$, $x_2 = 0$. The child LP bound is\n$z_0 = 8 \\cdot 1 + 7 \\cdot 0 + 0.1 \\cdot 0 = 8$.\n\nChild with $x_3 = 1$:\nConstraints become $2 x_1 + 2 x_2 + 3 \\le 4$, i.e., $x_1 + x_2 \\le 0.5$, and $x_1 + x_2 \\le 2$ (nonbinding here). To maximize $8 x_1 + 7 x_2$, allocate all of $0.5$ to $x_1$: $x_1 = 0.5$, $x_2 = 0$. The child LP bound is\n$z_1 = 8 \\cdot 0.5 + 7 \\cdot 0 + 0.1 \\cdot 1 = 4 + 0.1 = 4.1$.\n\nStep $3$: Compute the average bound improvement. By definition,\n$\\Delta_{\\text{avg}} = \\dfrac{\\left(z_{\\text{parent}} - z_0\\right) + \\left(z_{\\text{parent}} - z_1\\right)}{2}\n= \\dfrac{\\left(10.84 - 8\\right) + \\left(10.84 - 4.1\\right)}{2}\n= \\dfrac{2.84 + 6.74}{2}\n= \\dfrac{9.58}{2}\n= 4.79$.\n\nInterpretation and explanation of the phenomenon: Although the objective coefficient of $x_3$ is small, namely $0.1$, the side constraint $x_1 + x_2 - x_3 \\le 1$ tightly couples $x_3$ to the high-profit variables $x_1$ and $x_2$. In the root LP relaxation, increasing $x_3$ by $\\Delta$ allows the sum $x_1 + x_2$ to increase by up to $\\Delta$, which can be exploited to increase the objective by roughly $7 \\Delta$ to $8 \\Delta$ due to the large coefficients on $x_1$ and $x_2$. This is why the LP sets $x_3 = 0.4$ even though its direct contribution is only $0.04$. When we branch on $x_3$ and force $x_3 = 0$ or $x_3 = 1$, we destroy this fractional “enabling” effect: with $x_3 = 0$, the coupling caps $x_1 + x_2 \\le 1$, and with $x_3 = 1$, the knapsack constraint $2 x_1 + 2 x_2 + 3 x_3 \\le 4$ caps $x_1 + x_2 \\le 0.5$. Both branches severely reduce the LP upper bounds, producing a large average bound improvement $\\Delta_{\\text{avg}} = 4.79$. This illustrates that branching on a variable with a small objective coefficient can deliver unexpectedly strong bound improvement when tight side constraints strongly link that variable to high-profit variables.",
            "answer": "$$\\boxed{4.79}$$"
        },
        {
            "introduction": "Building on the idea that local information can be misleading, this practice explores another key metric from linear programming: the reduced cost. A variable with a zero reduced cost has no first-order effect on the objective function, suggesting it might be a poor branching choice. This problem  presents a counterintuitive case where branching on such a variable is extremely effective, as it triggers a cascade of logical deductions that rapidly solve the subproblem. This exercise highlights the immense power of inferential constraints in pruning the search tree.",
            "id": "3104676",
            "problem": "Consider the mixed-integer linear programming (MILP) problem\n$$\\min \\sum_{i=1}^{4} w_i x_i$$\nsubject to\n$$x_i - y = 0 \\quad \\text{for } i \\in \\{1,2,3,4\\},$$\n$$x_1 + x_2 + x_3 + x_4 \\ge 3,$$\n$$x_i \\in \\{0,1\\} \\text{ for } i \\in \\{1,2,3,4\\}, \\quad 0 \\le y \\le 1,$$\nwhere the weights are given by\n$$w_1 = 3, \\quad w_2 = 5, \\quad w_3 = 7, \\quad w_4 = 11.$$\n\nThis problem is to be analyzed through the lens of branching variable selection in branch and bound (B), with a focus on the case where branching on a variable with zero reduced cost can be effective due to strong inferential constraints. Use foundational concepts from linear programming (LP) duality and reduced costs: the reduced cost of a variable in an LP is defined as $r_j = c_j - a_j^{\\top} u$, where $c_j$ is the objective coefficient of variable $j$, $a_j$ is the column of constraint coefficients for variable $j$, and $u$ is the vector of dual prices corresponding to the constraints.\n\nTasks:\n1. Solve the LP relaxation of the MILP (i.e., replace $x_i \\in \\{0,1\\}$ by $0 \\le x_i \\le 1$) and determine the optimal LP solution $(x_1,x_2,x_3,x_4,y)$ and the optimal LP objective value.\n2. Justify, using the fundamental properties of LP bases and reduced costs, that the variable $y$ has zero reduced cost at the optimal LP solution.\n3. Perform B branching on the variable $y$ at the root node. For the branch $y=0$, infer the implications on $(x_1,x_2,x_3,x_4)$ and feasibility. For the branch $y=1$, infer the implications on $(x_1,x_2,x_3,x_4)$ and compute the resulting LP bound (objective value).\n4. Define the immediate lower bound improvement from branching on $y$ to be the difference between the minimum of the two child LP bounds and the root LP bound. Compute this improvement as a real number.\n\nExplain briefly why this construction demonstrates that branching on a zero reduced cost variable can be counterintuitively effective due to strong inferential constraints (the equalities $x_i - y = 0$ and the covering constraint $x_1 + x_2 + x_3 + x_4 \\ge 3$). Your final answer must be the numerical value of the lower bound improvement. No rounding is necessary.",
            "solution": "The problem provided is a mixed-integer linear programming (MILP) problem defined as:\n$$ \\min \\sum_{i=1}^{4} w_i x_i $$\nsubject to\n$$ x_i - y = 0 \\quad \\text{for } i \\in \\{1,2,3,4\\} $$\n$$ x_1 + x_2 + x_3 + x_4 \\ge 3 $$\n$$ x_i \\in \\{0,1\\} \\text{ for } i \\in \\{1,2,3,4\\}, \\quad 0 \\le y \\le 1 $$\nwith weights $w_1 = 3$, $w_2 = 5$, $w_3 = 7$, and $w_4 = 11$.\n\nThe validation confirms that the problem is scientifically grounded, well-posed, objective, and contains all necessary information. It is a valid problem in the domain of optimization methods. We proceed with the solution.\n\n**1. LP Relaxation Solution**\n\nTo find the LP relaxation, we replace the integrality constraint $x_i \\in \\{0,1\\}$ with the continuous constraint $0 \\le x_i \\le 1$. The problem becomes:\n$$ \\min \\quad 3x_1 + 5x_2 + 7x_3 + 11x_4 $$\nsubject to:\n$$ x_i - y = 0, \\quad i=1,2,3,4 $$\n$$ x_1 + x_2 + x_3 + x_4 \\ge 3 $$\n$$ 0 \\le x_i \\le 1, \\quad i=1,2,3,4 $$\n$$ 0 \\le y \\le 1 $$\nThe constraints $x_i - y = 0$ imply that $x_1 = x_2 = x_3 = x_4 = y$. We can substitute $y$ for each $x_i$ in the problem. The objective function becomes:\n$$ \\min \\quad (3+5+7+11)y = 26y $$\nThe constraints on $x_i$ and $y$ are simplified. The constraint $\\sum_{i=1}^{4} x_i \\ge 3$ becomes $y+y+y+y \\ge 3$, which is $4y \\ge 3$, or $y \\ge \\frac{3}{4}$. The bounds $0 \\le x_i \\le 1$ become $0 \\le y \\le 1$. The bound on $y$ is also $0 \\le y \\le 1$. The intersection of these constraints on $y$ is $\\frac{3}{4} \\le y \\le 1$.\nThe simplified LP is:\n$$ \\min \\quad 26y $$\n$$ \\text{s.t.} \\quad \\frac{3}{4} \\le y \\le 1 $$\nTo minimize the objective $26y$, we must select the smallest feasible value for $y$, which is $y^* = \\frac{3}{4}$.\nThe optimal solution to the LP relaxation is therefore:\n$$ y^* = \\frac{3}{4} $$\n$$ x_i^* = \\frac{3}{4} \\quad \\text{for } i \\in \\{1,2,3,4\\} $$\nThe optimal LP objective value, which serves as the lower bound at the root node of the B tree, is:\n$$ Z_{LP} = 26 \\times \\frac{3}{4} = \\frac{78}{4} = \\frac{39}{2} = 19.5 $$\n\n**2. Justification of Zero Reduced Cost for y**\n\nThe reduced cost of a variable that is in the basis of an optimal LP solution is, by definition, zero. We can show that $y$ is a basic variable. In the LP relaxation, all variables $x_1, x_2, x_3, x_4, y$ are strictly between their lower and upper bounds ($0$ and $1$). Variables that are not at their bounds must be basic variables in a non-degenerate optimal solution. Since five variables ($x_1, x_2, x_3, x_4, y$) are not at their bounds, and there are five equality constraints (four for $x_i-y=0$ and one for $\\sum x_i \\ge 3$, which is active), these five variables can form a valid basis, confirming they are basic variables. As $y$ is a basic variable, its reduced cost is $0$.\n\nAlternatively, we can use the definition of reduced cost $r_j = c_j - a_j^\\top u$ and LP duality. Let $u_1, u_2, u_3, u_4$ be the dual variables for the constraints $x_i - y = 0$, and let $v \\ge 0$ be the dual variable for $\\sum x_i \\ge 3$. The objective coefficient of $y$ is $c_y=0$. The column for $y$ in the constraint matrix is $a_y = (-1, -1, -1, -1, 0)^\\top$. The reduced cost of $y$ is:\n$$ r_y = c_y - \\begin{pmatrix} u_1  u_2  u_3  u_4  v \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -1 \\\\ -1 \\\\ -1 \\\\ 0 \\end{pmatrix} = 0 - (-u_1 - u_2 - u_3 - u_4) = \\sum_{i=1}^{4} u_i $$\nBy complementary slackness, since $x_i^* = \\frac{3}{4} > 0$, the corresponding dual constraints for each $x_i$ must be active: $u_i + v = w_i$. Also, strong duality implies the primal and dual objective values are equal: $3v = 19.5$, which yields $v = 6.5$. We can solve for the $u_i$:\n$u_1 = w_1 - v = 3 - 6.5 = -3.5$\n$u_2 = w_2 - v = 5 - 6.5 = -1.5$\n$u_3 = w_3 - v = 7 - 6.5 = 0.5$\n$u_4 = w_4 - v = 11 - 6.5 = 4.5$\nThe sum is $\\sum_{i=1}^{4} u_i = -3.5 - 1.5 + 0.5 + 4.5 = 0$.\nThus, the reduced cost of $y$ is $r_y = 0$.\n\n**3. Branch-and-Bound on Variable y**\n\nWe perform branching by creating two subproblems from the root node, corresponding to the branches $y=0$ and $y=1$. This is a valid partitioning because in any integer feasible solution, y must ultimately be $0$ or $1$ due to its coupling with the binary $x_i$ variables.\n\n**Branch 1: $y=0$**\nWe add the constraint $y=0$ to the LP relaxation.\nThe constraints $x_i - y = 0$ immediately imply $x_i = 0$ for all $i \\in \\{1,2,3,4\\}$.\nWe check feasibility against the covering constraint:\n$$ \\sum_{i=1}^{4} x_i = 0+0+0+0 = 0 $$\nThe constraint $\\sum x_i \\ge 3$ becomes $0 \\ge 3$, which is a contradiction.\nThis branch is infeasible. The lower bound for this node is $+\\infty$. The node is pruned.\n\n**Branch 2: $y=1$**\nWe add the constraint $y=1$ to the LP relaxation.\nThe constraints $x_i - y = 0$ imply $x_i = 1$ for all $i \\in \\{1,2,3,4\\}$.\nWe check feasibility:\n- $x_i = 1$ satisfies $0 \\le x_i \\le 1$.\n- $y=1$ satisfies $0 \\le y \\le 1$.\n- The covering constraint: $\\sum x_i = 1+1+1+1=4 \\ge 3$. This is satisfied.\nThis branch has a single feasible point $(x_1, x_2, x_3, x_4, y) = (1,1,1,1,1)$. This is an integer feasible solution.\nThe objective value for this solution is the LP bound for this node:\n$$ Z_{y=1} = 3(1) + 5(1) + 7(1) + 11(1) = 26 $$\nThis node yields an integer solution, so we find an upper bound for the MILP ($Z_{UB}=26$). The node is fathomed (pruned) because its LP solution is integer.\n\n**4. Lower Bound Improvement**\n\nThe root node had a lower bound of $Z_{LP} = 19.5$.\nAfter branching on $y$, the child nodes have bounds of $\\infty$ and $26$. The new global lower bound is the minimum of the active node bounds. Since both children are pruned, we have solved the problem, and the optimal objective is $26$. The immediate lower bound for the subproblem is updated to:\n$$ Z_{new} = \\min(\\infty, 26) = 26 $$\nThe immediate lower bound improvement is the difference between this new lower bound and the root node's lower bound:\n$$ \\text{Improvement} = Z_{new} - Z_{LP} = 26 - 19.5 = 6.5 $$\n\nThe effectiveness of branching on $y$ despite its zero reduced cost stems from the strong inferential power of the equality constraints $x_i - y = 0$. These constraints tightly link the continuous variable $y$ to all binary variables $x_i$. Forcing $y$ to take on values ($0$ and $1$) that are consistent with the final integer solution immediately forces all $x_i$ to integer values. This resolves all fractionalities at once and leads to either a quick proof of infeasibility or an integer solution, thus closing the integrality gap ($26 - 19.5 = 6.5$) completely in a single step. This demonstrates that variable selection rules in B can be more effective when they consider the structural role of variables and the logical implications of branching, rather than relying solely on local LP information like reduced costs.",
            "answer": "$$\\boxed{6.5}$$"
        },
        {
            "introduction": "Since simple, local heuristics can fail, we turn to more robust, global methods. This coding exercise introduces strong branching, a powerful technique that directly measures the bound degradation by simulating the \"up\" and \"down\" branches for each candidate variable. You will implement this method and also address a practical issue: how to break ties when multiple variables yield similar strong branching scores. By implementing a secondary tie-breaking rule based on \"implied constraint tightening\" , you will construct a sophisticated, hierarchical decision process that mirrors the logic used in state-of-the-art optimization solvers.",
            "id": "3104701",
            "problem": "You are given a set of binary decision variables $x \\in \\{0,1\\}^n$ and a Mixed Integer Linear Program with a linear objective and linear constraints. The Branch-and-Bound (BB) method relies on selecting a branching variable when the current Linear Programming (LP) relaxation solution has fractional entries. In strong branching, the variable selection is guided by evaluating the LP relaxation bounds of the two child nodes resulting from fixing a candidate variable $x_i$ to $0$ and to $1$, respectively, and scoring the expected bound improvement. In practice, multiple fractional variables can be tightly coupled by one or more shared constraints, which may lead to similar strong branching scores across those variables. In such cases, a secondary criterion (tiebreaker) is needed to select the variable that maximizes implied tightening of the binding constraints.\n\nStarting from the following foundational facts:\n- The LP relaxation of a Mixed Integer Linear Program replaces the integrality constraints $x_i \\in \\{0,1\\}$ with $0 \\le x_i \\le 1$ and yields an upper bound in the case of maximization.\n- Branching on variable $x_i$ creates two child subproblems by adding $x_i = 0$ or $x_i = 1$ (implemented via bounds), which can only decrease or maintain the LP relaxation bound in a maximization setting.\n- A constraint $A x \\le b$ is binding at a solution $x^*$ if $A x^* = b$; an equality constraint $A x = b$ is always binding at any feasible solution.\n\nYour task is to build a program that, for each test case provided below, performs the following steps:\n1. Solve the parent LP relaxation of a maximization problem\n   $$\n   \\max_{x \\in \\mathbb{R}^n} \\; p^\\top x \\quad \\text{subject to} \\quad A_{\\text{ub}} x \\le b_{\\text{ub}}, \\; A_{\\text{eq}} x = b_{\\text{eq}}, \\; 0 \\le x \\le 1,\n   $$\n   where $p \\in \\mathbb{R}^n$ is the profit vector, $A_{\\text{ub}} \\in \\mathbb{R}^{m_{\\text{ub}} \\times n}$ and $b_{\\text{ub}} \\in \\mathbb{R}^{m_{\\text{ub}}}$ define the inequality constraints, and $A_{\\text{eq}} \\in \\mathbb{R}^{m_{\\text{eq}} \\times n}$ and $b_{\\text{eq}} \\in \\mathbb{R}^{m_{\\text{eq}}}$ define the equality constraints. The constraints may represent, for instance, a budget limit and a cardinality requirement. Let the optimal LP relaxation solution be $x^*$ with objective value $z^* = p^\\top x^*$.\n2. Identify the set of fractional variables $\\mathcal{F} = \\{ i \\in \\{1,\\dots,n\\} \\mid \\epsilon  x^*_i  1 - \\epsilon \\}$ for a small tolerance $\\epsilon$.\n3. For each $i \\in \\mathcal{F}$, perform strong branching by solving two child LP relaxations:\n   - The \"down\" branch adds $x_i = 0$,\n   - The \"up\" branch adds $x_i = 1$.\n   Let $z^{\\downarrow}_i$ and $z^{\\uparrow}_i$ denote the optimal values of the child LPs. Define the strong branching score of variable $i$ by\n   $$\n   S_i = \\max(0, z^* - z^{\\downarrow}_i) + \\max(0, z^* - z^{\\uparrow}_i).\n   $$\n   This quantifies the total reduction in the LP upper bound (for maximization) caused by branching on $x_i$.\n4. Analyze whether branching on any fractional variable yields a similar bound by checking if\n   $$\n   \\max_{i \\in \\mathcal{F}} S_i - \\min_{i \\in \\mathcal{F}} S_i \\le \\tau,\n   $$\n   where $\\tau$ is a small nonnegative tolerance.\n5. Propose and apply a tiebreaker that maximizes implied constraint tightening among binding constraints. Specifically, let $\\mathcal{B}_{\\text{ub}} = \\{ j \\mid b_{\\text{ub},j} - (A_{\\text{ub}} x^*)_j \\le \\delta \\}$ be the set of binding inequality constraints at $x^*$ for a small tolerance $\\delta$, and treat all equality constraints as binding. For a fractional variable $i$, define the rounding target $\\mathrm{round}(x^*_i) = 1$ if $x^*_i \\ge \\tfrac{1}{2}$ and $0$ otherwise, and let $\\Delta_i = |\\mathrm{round}(x^*_i) - x^*_i|$. Then define the implied tightening score\n   $$\n   T_i = \\sum_{j \\in \\mathcal{B}_{\\text{ub}}} \\max(0, A_{\\text{ub},j i}) \\, \\Delta_i \\;+\\; \\sum_{j=1}^{m_{\\text{eq}}} |A_{\\text{eq},j i}| \\, \\Delta_i.\n   $$\n   This measures the magnitude by which fixing $x_i$ to its nearest integer would oblige the binding constraints to tighten via adjustments in other variables. When strong branching scores are similar within $\\tau$, select the variable with maximal $T_i$; otherwise, select the variable with maximal $S_i$. In case of ties, prefer the larger $T_i$, and if still tied, the smallest index $i$.\n\nImplement the above logic and apply it to the following test suite. Each test case is specified by $(p, A_{\\text{ub}}, b_{\\text{ub}}, A_{\\text{eq}}, b_{\\text{eq}})$:\n- Test Case 1 (happy path with multiple fractional variables coupled by one equality and one inequality):\n  - $n = 5$\n  - $p = [5, 5, 5, 1, 1]$\n  - $A_{\\text{ub}} = \\begin{bmatrix} 4  4  4  1  1 \\end{bmatrix}$, $b_{\\text{ub}} = [10]$\n  - $A_{\\text{eq}} = \\begin{bmatrix} 1  1  1  1  1 \\end{bmatrix}$, $b_{\\text{eq}} = [3]$\n- Test Case 2 (boundary case where one fractional variable may dominate strong branching improvement):\n  - $n = 6$\n  - $p = [6, 6, 6, 2, 2, 2]$\n  - $A_{\\text{ub}} = \\begin{bmatrix} 5  5  5  1  1  1 \\end{bmatrix}$, $b_{\\text{ub}} = [11]$\n  - $A_{\\text{eq}} = \\begin{bmatrix} 1  1  1  1  1  1 \\end{bmatrix}$, $b_{\\text{eq}} = [4]$\n- Test Case 3 (edge case with two inequality constraints creating fractional variables via dual coupling):\n  - $n = 5$\n  - $p = [3, 3, 3, 3, 3]$\n  - $A_{\\text{ub}} = \\begin{bmatrix} 4  4  4  1  1 \\\\ 1  1  1  1  1 \\end{bmatrix}$, $b_{\\text{ub}} = [9, 3.2]$\n  - $A_{\\text{eq}}$ absent, $b_{\\text{eq}}$ absent\n\nFor numerical robustness, use $\\epsilon = 10^{-8}$ to detect fractional variables, $\\tau = 10^{-6}$ for the strong branching similarity check, and $\\delta = 10^{-8}$ to detect binding inequalities.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-item list $[\\text{similar}, \\text{index}]$ corresponding to one test case. The first item $\\text{similar}$ is a boolean indicating whether the strong branching scores across fractional variables are within tolerance $\\tau$, and the second item $\\text{index}$ is the zero-based index of the selected branching variable according to the described rule (use $-1$ if there are no fractional variables). For example: $[[\\text{True}, 2],[\\text{False}, 1],[\\text{True}, 3]]$.\n\nNo physical units, angles, or percentages are involved in this problem. The program must be a complete, runnable program.",
            "solution": "The problem requires the implementation of a specific two-level variable selection strategy for a Branch-and-Bound algorithm applied to a maximization Mixed Integer Linear Program (MILP). The primary selection criterion is strong branching, with a novel \"implied tightening score\" serving as a tiebreaker. The procedure is to be applied to a set of test cases.\n\nThe solution methodology precisely follows the steps detailed in the problem statement.\n\n1.  **Parent LP Relaxation Solution**: For each test case, we first solve the parent Linear Programming (LP) relaxation of the given MILP. The integrality constraints $x_i \\in \\{0,1\\}$ are relaxed to $0 \\le x_i \\le 1$. The problem is formulated as:\n    $$\n    \\max_{x \\in \\mathbb{R}^n} \\; p^\\top x \\quad \\text{subject to} \\quad A_{\\text{ub}} x \\le b_{\\text{ub}}, \\; A_{\\text{eq}} x = b_{\\text{eq}}, \\; 0 \\le x \\le 1\n    $$\n    We utilize the `scipy.optimize.linprog` function for this task. As this function performs minimization, the objective is inverted by providing $c = -p$. The optimal solution vector is denoted by $x^*$, and the optimal objective value (the upper bound) by $z^* = -(\\text{result.fun})$.\n\n    A critical subtlety is that an LP may possess multiple optimal solutions (i.e., the optimal solution set is a face of the feasible polytope, not necessarily a single vertex). The specific vector $x^*$ returned can depend on the solver's algorithm. Consequently, the set of fractional variables $\\mathcal{F}$ and the set of binding constraints $\\mathcal{B}_{\\text{ub}}$ may differ depending on the choice of $x^*$. However, the problem is computationally well-defined as a deterministic LP solver, such as the one provided in SciPy, will return a single, specific solution, making the subsequent steps of the algorithm unambiguous.\n\n2.  **Identification of Fractional Variables**: From the parent LP solution $x^*$, we identify the set of fractional variables $\\mathcal{F} = \\{ i \\mid \\epsilon  x^*_i  1 - \\epsilon \\}$, using the specified tolerance $\\epsilon = 10^{-8}$. If this set is empty, no branching is needed, and the process for that test case terminates.\n\n3.  **Strong Branching Score ($S_i$) Computation**: For each fractional variable $x_i$ (where $i \\in \\mathcal{F}$), we simulate branching by solving two child LPs:\n    -   **Down Branch**: The constraint $x_i = 0$ is added to the parent LP by setting the bounds of variable $i$ to $(0, 0)$. The resulting optimal value is $z^{\\downarrow}_i$.\n    -   **Up Branch**: The constraint $x_i = 1$ is added by setting the bounds of variable $i$ to $(1, 1)$. The resulting optimal value is $z^{\\uparrow}_i$.\n\n    The strong branching score for variable $i$ is then calculated as the sum of the objective degradations in both branches:\n    $$\n    S_i = \\max(0, z^* - z^{\\downarrow}_i) + \\max(0, z^* - z^{\\uparrow}_i)\n    $$\n    The $\\max(0, \\cdot)$ term ensures non-negativity, which is expected since branching on a maximization problem cannot improve the LP relaxation bound.\n\n4.  **Similarity Check**: We assess if the strong branching scores are clustered together. If there are at least two fractional variables, we compute the range of the scores. The scores are deemed \"similar\" if $\\max_{i \\in \\mathcal{F}} S_i - \\min_{i \\in \\mathcal{F}} S_i \\le \\tau$, where $\\tau = 10^{-6}$. This boolean outcome determines which selection criterion to use.\n\n5.  **Implied Tightening Score ($T_i$) Computation**: This score serves as the tiebreaker. It is calculated for each fractional variable $i \\in \\mathcal{F}$:\n    - First, we identify binding constraints at the parent solution $x^*$. All equality constraints are binding. An inequality constraint $j$ is considered binding if its slack, $b_{\\text{ub},j} - (A_{\\text{ub}} x^*)_j$, is less than or equal to a tolerance $\\delta = 10^{-8}$.\n    - For each variable $i \\in \\mathcal{F}$, we determine its rounding target, $\\mathrm{round}(x^*_i)$, and the required change, $\\Delta_i = |\\mathrm{round}(x^*_i) - x^*_i|$.\n    - The score $T_i$ is the sum of weighted coefficient magnitudes across all binding constraints, scaled by $\\Delta_i$:\n      $$\n      T_i = \\Delta_i \\left( \\sum_{j \\in \\mathcal{B}_{\\text{ub}}} \\max(0, A_{\\text{ub},j i}) + \\sum_{j=1}^{m_{\\text{eq}}} |A_{\\text{eq},j i}| \\right)\n      $$\n      where $\\mathcal{B}_{\\text{ub}}$ is the index set of binding inequality constraints. For inequalities, only positive coefficients contribute, as these are the ones that enforce tightening when the variable value changes.\n\n6.  **Final Variable Selection**: The choice of the branching variable follows a strict hierarchical rule:\n    -   If the strong branching scores are similar, the primary criterion is to select the variable with the maximum tightening score $T_i$.\n    -   If the scores are not similar, the primary criterion is to select the variable with the maximum strong branching score $S_i$.\n    -   Any ties in the primary criterion are broken by selecting the variable with the larger $T_i$ score.\n    -   If a tie persists, it is resolved by choosing the variable with the smallest index $i$.\n\nThis complete logic is encapsulated in the provided Python program, which processes each test case and reports the outcome of the selection process.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the variable selection process for all test cases.\n    \"\"\"\n    # Define the test cases as specified in the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"p\": np.array([5, 5, 5, 1, 1], dtype=float),\n            \"A_ub\": np.array([[4, 4, 4, 1, 1]], dtype=float),\n            \"b_ub\": np.array([10], dtype=float),\n            \"A_eq\": np.array([[1, 1, 1, 1, 1]], dtype=float),\n            \"b_eq\": np.array([3], dtype=float),\n        },\n        # Test Case 2\n        {\n            \"p\": np.array([6, 6, 6, 2, 2, 2], dtype=float),\n            \"A_ub\": np.array([[5, 5, 5, 1, 1, 1]], dtype=float),\n            \"b_ub\": np.array([11], dtype=float),\n            \"A_eq\": np.array([[1, 1, 1, 1, 1, 1]], dtype=float),\n            \"b_eq\": np.array([4], dtype=float),\n        },\n        # Test Case 3\n        {\n            \"p\": np.array([3, 3, 3, 3, 3], dtype=float),\n            \"A_ub\": np.array([[4, 4, 4, 1, 1], [1, 1, 1, 1, 1]], dtype=float),\n            \"b_ub\": np.array([9, 3.2], dtype=float),\n            \"A_eq\": None,\n            \"b_eq\": None,\n        }\n    ]\n\n    # Tolerances\n    epsilon = 1e-8\n    tau = 1e-6\n    delta = 1e-8\n\n    results = []\n    for case in test_cases:\n        result = process_case(case, epsilon, tau, delta)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list of lists matches the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef process_case(case_data, epsilon, tau, delta):\n    \"\"\"\n    Processes a single test case according to the specified logic.\n    \"\"\"\n    p = case_data[\"p\"]\n    A_ub = case_data[\"A_ub\"]\n    b_ub = case_data[\"b_ub\"]\n    A_eq = case_data[\"A_eq\"]\n    b_eq = case_data[\"b_eq\"]\n    \n    n = len(p)\n    c = -p\n    bounds = [(0, 1)] * n\n\n    # 1. Solve the parent LP relaxation.\n    res_parent = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n    \n    if not res_parent.success:\n        # This case is not expected for the given problems.\n        # Fallback for ill-defined problems.\n        return [False, -1]\n        \n    x_star = res_parent.x\n    z_star = -res_parent.fun\n\n    # 2. Identify the set of fractional variables.\n    F_indices = [i for i, x_val in enumerate(x_star) if epsilon  x_val  1 - epsilon]\n    \n    if not F_indices:\n        return [False, -1]\n    \n    s_scores = {}\n    \n    # 3. For each fractional variable, perform strong branching.\n    for i in F_indices:\n        # Down branch (x_i = 0)\n        bounds_down = bounds[:]\n        bounds_down[i] = (0, 0)\n        res_down = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds_down, method='highs')\n        z_down_i = -res_down.fun if res_down.success else -np.inf\n        \n        # Up branch (x_i = 1)\n        bounds_up = bounds[:]\n        bounds_up[i] = (1, 1)\n        res_up = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds_up, method='highs')\n        z_up_i = -res_up.fun if res_up.success else -np.inf\n        \n        s_scores[i] = max(0, z_star - z_down_i) + max(0, z_star - z_up_i)\n\n    # 4. Analyze strong branching score similarity.\n    similar = False\n    if len(F_indices) > 1:\n        max_s = max(s_scores.values())\n        min_s = min(s_scores.values())\n        if max_s - min_s = tau:\n            similar = True\n\n    # 5. Propose and apply the tiebreaker score T_i.\n    t_scores = {}\n    \n    # Identify binding inequality constraints.\n    binding_ub_indices = []\n    if A_ub is not None:\n        slacks = b_ub - (A_ub @ x_star)\n        binding_ub_indices = [j for j, slack in enumerate(slacks) if slack = delta]\n    \n    for i in F_indices:\n        round_xi = 1.0 if x_star[i] >= 0.5 else 0.0\n        delta_i = abs(round_xi - x_star[i])\n        \n        tightening_sum = 0.0\n        # Sum over binding inequality constraints.\n        if A_ub is not None:\n             for j in binding_ub_indices:\n                tightening_sum += max(0, A_ub[j, i])\n        \n        # Sum over all equality constraints (always binding).\n        if A_eq is not None:\n            for j in range(A_eq.shape[0]):\n                tightening_sum += abs(A_eq[j, i])\n                \n        t_scores[i] = delta_i * tightening_sum\n        \n    # 6. Select the branching variable based on the hierarchical rule.\n    candidates = [(s_scores[i], t_scores[i], i) for i in F_indices]\n    \n    if similar:\n        # Primary sort: T_i (desc), Secondary sort: index (asc)\n        candidates.sort(key=lambda x: (-x[1], x[2]))\n    else:\n        # Primary sort: S_i (desc), Secondary: T_i (desc), Tertiary: index (asc)\n        candidates.sort(key=lambda x: (-x[0], -x[1], x[2]))\n        \n    best_index = candidates[0][2]\n    \n    return [similar, best_index]\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}