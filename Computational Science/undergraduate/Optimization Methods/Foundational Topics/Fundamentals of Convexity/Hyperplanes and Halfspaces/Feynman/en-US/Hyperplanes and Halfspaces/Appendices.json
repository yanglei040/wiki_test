{
    "hands_on_practices": [
        {
            "introduction": "Hyperplanes are not just abstract geometric objects; they are fundamental tools in machine learning for tasks like binary classification. This exercise  delves into the practical implications of feature scaling, a common step in data preprocessing. By analyzing how a linear transformation of the feature space affects the orientation and margin of a separating hyperplane, you will develop a deeper intuition for the interplay between data representation and model performance.",
            "id": "3137844",
            "problem": "Consider a binary classification dataset with $m \\ge n$ samples in $\\mathbb{R}^n$. Let the design matrix be $A \\in \\mathbb{R}^{m \\times n}$ whose $i$-th row is $x_i^\\top$, and labels be $y_i \\in \\{-1, +1\\}$ for $i \\in \\{1,\\dots,m\\}$. A separating hyperplane is given by parameters $(w,b)$ with $w \\in \\mathbb{R}^n$ and $b \\in \\mathbb{R}$, so that $y_i (w^\\top x_i + b) > 0$ for all $i$. The (hard) margin of $(w,b)$ with respect to the standard Euclidean norm is defined as\n$$\n\\gamma(w,b; \\{(x_i,y_i)\\}_{i=1}^m) \\;=\\; \\min_{1 \\le i \\le m} \\frac{y_i \\big(w^\\top x_i + b\\big)}{\\lVert w \\rVert_2}.\n$$\nSuppose we apply an invertible linear feature scaling (preconditioning) $x' = S x$ with $S \\in \\mathbb{R}^{n \\times n}$ symmetric positive definite (SPD). The transformed dataset has rows $x_i'^\\top = x_i^\\top S^\\top$, and we will evaluate margins in the transformed coordinates using the same Euclidean norm. Independently, we aim to choose $S$ to “isotropize” $A$ in the sense that the empirical second moment $A'^\\top A'$ of the transformed design matrix $A' = A S^\\top$ is the identity matrix.\n\nSelect all statements that are true.\n\nA. Under the change of variables $x' = S x$, the unique linear functional that represents the same geometric separating set as $(w,b)$ in the new coordinates is $(w',b)$ with $w' = S^{-\\top} w$, and the transformed margin satisfies\n$$\n\\gamma'(w',b; \\{(x_i',y_i)\\}_{i=1}^m) \\;=\\; \\min_i \\frac{y_i \\big(w'^\\top x_i' + b\\big)}{\\lVert w' \\rVert_2} \\;=\\; \\min_i \\frac{y_i \\big(w^\\top x_i + b\\big)}{\\lVert S^{-\\top} w \\rVert_2}.\n$$\n\nB. For any invertible scaling $S$, the maximum achievable margin over all separating hyperplanes is invariant; that is, the value of the maximum margin in the transformed space equals the value in the original space.\n\nC. If $A$ has full column rank, then choosing the SPD preconditioner\n$$\nS \\;=\\; \\big(A^\\top A\\big)^{-1/2}\n$$\nmakes the transformed second moment isotropic, that is, $A'^\\top A' = I_n$.\n\nD. Scaling features to unit variance by taking $S = \\operatorname{diag}(A^\\top A)^{-1/2}$ always guarantees isotropy $A'^\\top A' = I_n$.\n\nE. Choosing $S = \\big(A A^\\top\\big)^{-1/2}$ achieves isotropy in feature space because then $A'^\\top A' = I_n$ holds.",
            "solution": "We proceed from first principles about linear transformations, hyperplanes, and norms.\n\nFundamental definitions and facts:\n1) A hyperplane in $\\mathbb{R}^n$ is the set $\\{x \\in \\mathbb{R}^n : w^\\top x + b = 0\\}$ for some nonzero $w \\in \\mathbb{R}^n$ and $b \\in \\mathbb{R}$. Its oriented distance at a point $x$ is $(w^\\top x + b)/\\lVert w \\rVert_2$.\n2) The hard margin of $(w,b)$ on labeled data $\\{(x_i,y_i)\\}$ is $\\gamma = \\min_i y_i (w^\\top x_i + b)/\\lVert w \\rVert_2$.\n3) Under an invertible linear change of coordinates $x' = S x$ with $S$ invertible, the same geometric hyperplane as a subset of $\\mathbb{R}^n$ is represented in $x'$-coordinates by $(w',b)$ satisfying $w'^\\top x' + b = 0$ if and only if $w'^\\top S x + b = 0$ for all $x$ with $w^\\top x + b = 0$. This requires $w'^\\top S = w^\\top$, equivalently $w' = S^{-\\top} w$.\n4) The transformed design matrix is $A' = A S^\\top$ because $x_i'^\\top = x_i^\\top S^\\top$. Its empirical (uncentered) second moment is $A'^\\top A' = S A^\\top A S^\\top$.\n5) For a symmetric positive definite (SPD) matrix $M$, the unique SPD square root $M^{1/2}$ is SPD and $M^{1/2} M^{1/2} = M$, and $M^{-1/2}$ is SPD with $M^{-1/2} M^{-1/2} = M^{-1}$.\n\nDerivation for statement A:\n- The representation: From item $3$, to represent the same geometric hyperplane after the coordinate change, $w'$ must satisfy $w' = S^{-\\top} w$, with the same $b$.\n- The margin in transformed coordinates is defined by the same formula but applied to $\\{(x_i',y_i)\\}$ and $(w',b)$:\n$$\n\\gamma' \\;=\\; \\min_i \\frac{y_i \\big(w'^\\top x_i' + b\\big)}{\\lVert w' \\rVert_2}.\n$$\n- Using $x_i' = S x_i$ and $w' = S^{-\\top} w$, the numerator becomes $y_i \\big((S^{-\\top} w)^\\top (S x_i) + b\\big) = y_i \\big(w^\\top x_i + b\\big)$.\n- The denominator is $\\lVert w' \\rVert_2 = \\lVert S^{-\\top} w \\rVert_2$.\n- Therefore\n$$\n\\gamma' \\;=\\; \\min_i \\frac{y_i \\big(w^\\top x_i + b\\big)}{\\lVert S^{-\\top} w \\rVert_2}.\n$$\nThis establishes statement A as true.\n\nDerivation for statement B:\n- Consider scalar isotropic scaling $S = \\alpha I_n$ with $\\alpha > 0$. Then for any fixed separator $(w,b)$, the transformed margin from statement A is\n$$\n\\gamma' \\;=\\; \\min_i \\frac{y_i \\big(w^\\top x_i + b\\big)}{\\lVert (\\alpha I_n)^{-\\top} w \\rVert_2} \\;=\\; \\min_i \\frac{y_i \\big(w^\\top x_i + b\\big)}{\\lVert \\alpha^{-1} w \\rVert_2} \\;=\\; \\alpha \\min_i \\frac{y_i \\big(w^\\top x_i + b\\big)}{\\lVert w \\rVert_2} \\;=\\; \\alpha \\gamma.\n$$\n- Because this linear scaling affects the margin of every separator by the same positive factor $\\alpha$, the maximum margin value over all separators is also scaled by $\\alpha$, not preserved. Thus the maximum margin in transformed coordinates (computed with the standard Euclidean norm in the transformed space) is not invariant under general invertible $S$.\n- Therefore statement B is false.\n\nDerivation for statement C:\n- We desire isotropy $A'^\\top A' = I_n$ with $A' = A S^\\top$. From item $4$, this is equivalent to $S A^\\top A S^\\top = I_n$.\n- Suppose $A$ has full column rank $n$ so that $A^\\top A$ is SPD. Let $A^\\top A = Q \\Lambda Q^\\top$ be an eigendecomposition with $Q$ orthogonal and $\\Lambda$ diagonal with positive entries. Then\n$$\nS \\;=\\; (A^\\top A)^{-1/2} \\;=\\; Q \\Lambda^{-1/2} Q^\\top\n$$\nis SPD and invertible.\n- Compute\n$$\nS A^\\top A S^\\top \\;=\\; (A^\\top A)^{-1/2} (A^\\top A) (A^\\top A)^{-1/2} \\;=\\; I_n,\n$$\nusing item $5$ and symmetry of $(A^\\top A)^{-1/2}$. Hence $A'^\\top A' = I_n$.\n- Therefore statement C is true.\n\nDerivation for statement D:\n- Let $D = \\operatorname{diag}(A^\\top A)$ denote the diagonal matrix of feature-wise uncentered second moments, and take $S = D^{-1/2}$ (diagonal, positive). Then\n$$\nA'^\\top A' \\;=\\; S A^\\top A S^\\top \\;=\\; D^{-1/2} (A^\\top A) D^{-1/2}.\n$$\n- This equals the identity if and only if $A^\\top A$ is diagonal and its diagonal equals $D$, that is, if and only if the features are already uncorrelated in the empirical second moment sense. In general, $A^\\top A$ has nonzero off-diagonal elements capturing feature correlations, and these persist under mere diagonal rescaling.\n- Therefore diagonal variance normalization alone does not guarantee $A'^\\top A' = I_n$. Statement D is false.\n\nDerivation for statement E:\n- The matrix $A A^\\top \\in \\mathbb{R}^{m \\times m}$, so $(A A^\\top)^{-1/2}$ is also $m \\times m$. The feature-scaling map $x' = S x$ requires $S \\in \\mathbb{R}^{n \\times n}$ acting on feature vectors in $\\mathbb{R}^n$ on the right of $A$ as $A' = A S^\\top$.\n- Therefore $S = (A A^\\top)^{-1/2}$ has incompatible dimensions for right-multiplication by $A$, and even if one considered left-multiplication preconditioning (acting on samples), it would not generally yield $A'^\\top A' = I_n$ in feature space. The correct isotropizing condition is $S A^\\top A S^\\top = I_n$, which is solved by $S = (A^\\top A)^{-1/2}$ as in statement C.\n- Hence statement E is false.\n\nOption-by-option verdicts:\n- A: Correct. It follows from the transformation of linear functionals and the margin definition.\n- B: Incorrect. A simple scalar scaling $S = \\alpha I_n$ changes the maximum margin by the factor $\\alpha$.\n- C: Correct. Spectral calculus shows $S = (A^\\top A)^{-1/2}$ yields $A'^\\top A' = I_n$ when $A$ has full column rank.\n- D: Incorrect. Diagonal scaling does not remove cross-feature correlations, so isotropy is not guaranteed.\n- E: Incorrect. The matrix has wrong dimensions for feature scaling and does not satisfy the required isotropy condition in feature space.",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "While a separating hyperplane provides an elegant geometric model, real-world data is rarely perfect. This practice  explores the critical concept of moving from 'hard' constraints, which require every point to be correctly classified, to 'soft' constraints using a penalty function. You will investigate the properties of the hinge loss, a cornerstone of modern support vector machines, and analyze how this modeling choice impacts the solution and its convergence properties.",
            "id": "3137852",
            "problem": "Consider a family of halfspace constraints in $\\mathbb{R}^n$ given by $a_i^\\top x \\le b_i$ for $i \\in \\{1,\\dots,m\\}$ with $a_i \\in \\mathbb{R}^n$ and $b_i \\in \\mathbb{R}$. Define the objective $f(x) = \\tfrac{1}{2}\\|x\\|_2^2$. The hard-constrained problem seeks exact feasibility:\n- Hard problem $\\mathcal{P}_{\\mathrm{hard}}$: minimize $f(x)$ subject to $a_i^\\top x \\le b_i$ for all $i \\in \\{1,\\dots,m\\}$.\n\nTo explore soft enforcement of halfspaces, define the hinge penalty $[t]_+ := \\max\\{0,t\\}$ and consider, for a penalty weight $\\mu > 0$, the unconstrained soft-penalized problem:\n- Soft problem $\\mathcal{P}_{\\mathrm{soft}}(\\mu)$: minimize $F_\\mu(x) := f(x) + \\mu \\sum_{i=1}^m [a_i^\\top x - b_i]_+$ over $x \\in \\mathbb{R}^n$.\n\nAssume throughout that $f$ and $[\\,\\cdot\\,]_+$ are used as defined above and that $a_i$ and $b_i$ are fixed data. When a statement refers to Lagrange multipliers, interpret it under standard convex duality assumptions for inequality constraints (for example, Slater’s condition so that optimal Karush-Kuhn-Tucker (KKT) multipliers exist).\n\nSelect all statements that are correct.\n\nA. For every $\\mu > 0$, the function $F_\\mu$ is convex and coercive, and since $f$ is strongly convex, $\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ has a unique minimizer in $\\mathbb{R}^n$.\n\nB. If the feasible set $\\{x \\in \\mathbb{R}^n : a_i^\\top x \\le b_i \\ \\forall i\\}$ is nonempty, then any sequence $\\{x_\\mu\\}$ of minimizers of $\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ with $\\mu \\to +\\infty$ has all its cluster points solving $\\mathcal{P}_{\\mathrm{hard}}$. Because $f$ is strongly convex, the hard problem has a unique solution $x^\\star$, and $x_\\mu \\to x^\\star$ as $\\mu \\to +\\infty$.\n\nC. If the feasible set is nonempty, then for every fixed $\\mu > 0$ the unique minimizer of $\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ is necessarily feasible for the hard constraints.\n\nD. If the feasible set is empty, let $h(x) := \\sum_{i=1}^m [a_i^\\top x - b_i]_+$. Any cluster point of minimizers $x_\\mu$ of $\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ as $\\mu \\to +\\infty$ solves the lexicographic optimization: first minimize $h(x)$ over $x \\in \\mathbb{R}^n$, and among all minimizers of $h$, minimize $f(x)$.\n\nE. In the $1$-dimensional case $n=1$ with the two inconsistent halfspaces $x \\le 0$ and $x \\ge 1$, the unique minimizer of $\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ is $x=0$ for every $\\mu>0$.\n\nChoose all that apply:\nA, B, C, D, E.",
            "solution": "## PROBLEM VALIDATION\n\n### Step 1: Extract Givens\n- **Domain**: $\\mathbb{R}^n$\n- **Halfspace Constraints**: $a_i^\\top x \\le b_i$ for $i \\in \\{1,\\dots,m\\}$, with $a_i \\in \\mathbb{R}^n$ and $b_i \\in \\mathbb{R}$.\n- **Objective Function**: $f(x) = \\tfrac{1}{2}\\|x\\|_2^2$.\n- **Hard-Constrained Problem ($\\mathcal{P}_{\\mathrm{hard}}$)**: minimize $f(x)$ subject to $a_i^\\top x \\le b_i$ for all $i \\in \\{1,\\dots,m\\}$.\n- **Hinge Penalty Function**: $[t]_+ := \\max\\{0,t\\}$.\n- **Penalty Weight**: $\\mu > 0$.\n- **Soft-Penalized Objective Function**: $F_\\mu(x) := f(x) + \\mu \\sum_{i=1}^m [a_i^\\top x - b_i]_+$.\n- **Soft-Penalized Problem ($\\mathcal{P}_{\\mathrm{soft}}(\\mu)$)**: minimize $F_\\mu(x)$ over $x \\in \\mathbb{R}^n$.\n- **Assumption for Duality**: Standard convex duality assumptions (e.g., Slater's condition) are assumed to hold when Lagrange multipliers are mentioned, ensuring the existence of optimal Karush-Kuhn-Tucker (KKT) multipliers.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically and mathematically sound. It describes a standard application of the penalty method to a convex quadratic programming problem. The functions, variables, and optimization problems are well-defined within the field of convex optimization.\n\n1.  **Scientific or Factual Unsoundness**: None. The problem is based on established principles of convex analysis and optimization theory.\n2.  **Non-Formalizable or Irrelevant**: None. The problem is a formal mathematical question directly related to hyperplanes, halfspaces, and optimization methods.\n3.  **Incomplete or Contradictory Setup**: None. The problem is self-contained. The options correctly explore different scenarios, such as the feasibility of the constraint set, which is a key part of the analysis.\n4.  **Unrealistic or Infeasible**: None. The problem is abstract and mathematical; physical realism is not a relevant criterion.\n5.  **Ill-Posed or Poorly Structured**: None. The problems $\\mathcal{P}_{\\mathrm{hard}}$ and $\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ are standard and well-posed. The existence and uniqueness of solutions are consequences of the properties of the functions involved, which is a topic for analysis.\n6.  **Pseudo-Profound, Trivial, or Tautological**: None. The problem requires a solid understanding of convex functions, penalty methods, and their convergence properties.\n7.  **Outside Scientific Verifiability**: None. All statements can be mathematically proven or disproven.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. The solution will proceed with a detailed derivation and evaluation of each option.\n\n---\n\n### Analysis of the Functions\n\nThe objective function is $f(x) = \\tfrac{1}{2}\\|x\\|_2^2$. This function is coercive, as $f(x) \\to +\\infty$ when $\\|x\\|_2 \\to +\\infty$. Its Hessian is the identity matrix, $\\nabla^2 f(x) = I$, which is positive definite. Therefore, $f(x)$ is a $1$-strongly convex function.\n\nThe penalty term involves the hinge function $[t]_+ = \\max\\{0,t\\}$, which is convex. The functions $g_i(x) = a_i^\\top x - b_i$ are affine, hence convex. The composition $[g_i(x)]_+$ is convex because $[\\cdot]_+$ is a convex and non-decreasing function. The sum of convex functions $\\sum_{i=1}^m [a_i^\\top x - b_i]_+$ is also convex.\n\nThe soft problem objective is $F_\\mu(x) = f(x) + \\mu \\sum_{i=1}^m [a_i^\\top x - b_i]_+$.\n\n### Option-by-Option Analysis\n\n**A. For every $\\mu > 0$, the function $F_\\mu$ is convex and coercive, and since $f$ is strongly convex, $\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ has a unique minimizer in $\\mathbb{R}^n$.**\n\n-   **Convexity**: $F_\\mu(x)$ is the sum of $f(x)$ and $\\mu \\sum_{i=1}^m [a_i^\\top x - b_i]_+$. As established, $f(x)$ is strongly convex, and the penalty term is convex (as a non-negative-weighted sum of convex functions). The sum of a strongly convex function and a convex function is strongly convex. Thus, $F_\\mu(x)$ is strongly convex for any $\\mu > 0$. Strong convexity implies strict convexity, which in turn implies convexity.\n-   **Coercivity**: The penalty term $\\mu \\sum_{i=1}^m [a_i^\\top x - b_i]_+$ is non-negative. Therefore, $F_\\mu(x) \\ge f(x) = \\tfrac{1}{2}\\|x\\|_2^2$. Since $f(x)$ is coercive (i.e., $f(x) \\to +\\infty$ as $\\|x\\|_2 \\to +\\infty$), $F_\\mu(x)$ must also be coercive.\n-   **Existence and Uniqueness of a Minimizer**: A continuous function that is coercive has at least one minimizer in $\\mathbb{R}^n$. A function that is strictly convex has at most one minimizer. Since $F_\\mu(x)$ is strongly convex (and thus strictly convex) and coercive, it possesses a unique minimizer in $\\mathbb{R}^n$. The reasoning in the statement is correct.\n\n**Verdict for A: Correct.**\n\n**B. If the feasible set $\\{x \\in \\mathbb{R}^n : a_i^\\top x \\le b_i \\ \\forall i\\}$ is nonempty, then any sequence $\\{x_\\mu\\}$ of minimizers of $\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ with $\\mu \\to +\\infty$ has all its cluster points solving $\\mathcal{P}_{\\mathrm{hard}}$. Because $f$ is strongly convex, the hard problem has a unique solution $x^\\star$, and $x_\\mu \\to x^\\star$ as $\\mu \\to +\\infty$.**\n\nThis statement describes the convergence of the penalty method for a feasible problem.\n-   Let $S = \\{x \\in \\mathbb{R}^n : a_i^\\top x \\le b_i \\ \\forall i\\}$ be the feasible set. Since $S$ is a non-empty closed convex set and $f(x)$ is strongly convex, $\\mathcal{P}_{\\mathrm{hard}}$ has a unique solution, which we denote by $x^\\star$. Let $f^\\star = f(x^\\star)$.\n-   Let $x_\\mu$ be the unique minimizer of $F_\\mu(x)$. By definition, for any $x \\in \\mathbb{R}^n$, $F_\\mu(x_\\mu) \\le F_\\mu(x)$.\n-   Let's choose $x = x^\\star$. Since $x^\\star \\in S$, we have $a_i^\\top x^\\star - b_i \\le 0$ for all $i$, which implies $[a_i^\\top x^\\star - b_i]_+ = 0$. Thus, $F_\\mu(x^\\star) = f(x^\\star) = f^\\star$.\n-   The inequality becomes: $f(x_\\mu) + \\mu \\sum_{i=1}^m [a_i^\\top x_\\mu - b_i]_+ \\le f^\\star$.\n-   From this, we deduce two facts:\n    1.  $f(x_\\mu) \\le f^\\star$, which implies $\\tfrac{1}{2}\\|x_\\mu\\|_2^2 \\le f^\\star$. This shows that the sequence $\\{x_\\mu\\}$ is bounded. By the Bolzano-Weierstrass theorem, it has at least one cluster point.\n    2.  $\\sum_{i=1}^m [a_i^\\top x_\\mu - b_i]_+ \\le \\frac{f^\\star - f(x_\\mu)}{\\mu} \\le \\frac{f^\\star}{\\mu}$. As $\\mu \\to +\\infty$, the right-hand side goes to $0$. So, $\\sum_{i=1}^m [a_i^\\top x_\\mu - b_i]_+ \\to 0$.\n-   Let $\\bar{x}$ be any cluster point of $\\{x_\\mu\\}$. There is a subsequence $\\{x_{\\mu_k}\\}$ such that $x_{\\mu_k} \\to \\bar{x}$.\n-   By continuity, $\\lim_{k\\to\\infty} \\sum_{i=1}^m [a_i^\\top x_{\\mu_k} - b_i]_+ = \\sum_{i=1}^m [a_i^\\top \\bar{x} - b_i]_+ = 0$. Since each term is non-negative, this implies $[a_i^\\top \\bar{x} - b_i]_+ = 0$ for all $i$, meaning $a_i^\\top \\bar{x} \\le b_i$ for all $i$. Thus, $\\bar{x}$ is feasible for $\\mathcal{P}_{\\mathrm{hard}}$.\n-   Also, taking the limit of $f(x_{\\mu_k}) \\le f^\\star$ gives $f(\\bar{x}) \\le f^\\star$.\n-   Since $\\bar{x}$ is a feasible point for $\\mathcal{P}_{\\mathrm{hard}}$ and $x^\\star$ is the minimizer, we must have $f(\\bar{x}) \\ge f^\\star$.\n-   Combining these gives $f(\\bar{x}) = f^\\star$. Since $x^\\star$ is the unique minimizer, it must be that $\\bar{x} = x^\\star$.\n-   A bounded sequence with a single cluster point converges to that point. Therefore, $x_\\mu \\to x^\\star$ as $\\mu \\to +\\infty$. The statement is a classic result in penalty methods.\n\n**Verdict for B: Correct.**\n\n**C. If the feasible set is nonempty, then for every fixed $\\mu > 0$ the unique minimizer of $\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ is necessarily feasible for the hard constraints.**\n\nThis statement claims that the penalty method finds a feasible solution for any finite penalty parameter $\\mu > 0$. This is generally not true. The solution $x_\\mu$ typically approaches the feasible region as $\\mu$ increases.\nLet's construct a counterexample.\nLet $n=1$ and the single constraint be $x \\le -1$. The feasible set is $(-\\infty, -1]$, which is non-empty.\n$\\mathcal{P}_{\\mathrm{hard}}$: minimize $\\tfrac{1}{2}x^2$ subject to $x \\le -1$. The solution is $x^\\star = -1$.\n$\\mathcal{P}_{\\mathrm{soft}}(\\mu)$: minimize $F_\\mu(x) = \\tfrac{1}{2}x^2 + \\mu[x - (-1)]_+ = \\tfrac{1}{2}x^2 + \\mu[x+1]_+$.\nTo find the minimizer $x_\\mu$, we analyze the subgradient of $F_\\mu(x)$: $\\partial F_\\mu(x) = x + \\mu \\partial[x+1]_+$.\n-   If $x > -1$, $\\partial F_\\mu(x) = \\{x+\\mu\\}$. Setting $x+\\mu=0$ yields $x = -\\mu$. This is a valid candidate if $-\\mu > -1$, which means $\\mu < 1$.\n-   If $x < -1$, $\\partial F_\\mu(x) = \\{x\\}$. Setting $x=0$ contradicts $x < -1$. No solution here.\n-   If $x = -1$, $\\partial F_\\mu(-1) = -1 + \\mu[0,1] = [-1, -1+\\mu]$. The subgradient contains $0$ if $-1+\\mu \\ge 0$, i.e., $\\mu \\ge 1$.\nSo, the minimizer is $x_\\mu = -\\mu$ for $\\mu \\in (0, 1)$, and $x_\\mu=-1$ for $\\mu \\ge 1$.\nFor any $\\mu \\in (0, 1)$, the minimizer is $x_\\mu = -\\mu$. Since $-\\mu > -1$, this minimizer is not feasible for the hard constraint $x \\le -1$. For example, if $\\mu=0.5$, $x_{0.5} = -0.5$, which violates $x \\le -1$.\n\n**Verdict for C: Incorrect.**\n\n**D. If the feasible set is empty, let $h(x) := \\sum_{i=1}^m [a_i^\\top x - b_i]_+$. Any cluster point of minimizers $x_\\mu$ of $\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ as $\\mu \\to +\\infty$ solves the lexicographic optimization: first minimize $h(x)$ over $x \\in \\mathbb{R}^n$, and among all minimizers of $h$, minimize $f(x)$.**\n\nThis describes the behavior of the regularized problem when the constraints are inconsistent.\n-   Let $h^\\star = \\inf_{x \\in \\mathbb{R}^n} h(x)$. Since the feasible set is empty, $h(x) > 0$ for all $x$, so $h^\\star > 0$ (assuming the infimum is attained). Let $S_h = \\{x \\in \\mathbb{R}^n \\mid h(x) = h^\\star\\}$ be the set of minimizers of $h(x)$. Since $h(x)$ is convex, $S_h$ is a convex set. Assuming $S_h$ is non-empty, we can define the lexicographic solution.\n-   Let $x^\\star_{lex}$ be the unique solution to $\\min_{x \\in S_h} f(x)$. This exists and is unique because $f(x)$ is strongly convex and $S_h$ is a closed convex set.\n-   Let $x_\\mu$ be the minimizer of $F_\\mu(x) = f(x) + \\mu h(x)$. For any $x \\in S_h$, we have $f(x_\\mu) + \\mu h(x_\\mu) \\le f(x) + \\mu h(x) = f(x) + \\mu h^\\star$. In particular, using $x = x^\\star_{lex}$: $f(x_\\mu) + \\mu h(x_\\mu) \\le f(x^\\star_{lex}) + \\mu h^\\star$.\n-   This implies $f(x_\\mu) \\le f(x^\\star_{lex}) + \\mu(h^\\star - h(x_\\mu))$. Since $h(x_\\mu) \\ge h^\\star$, the second term is non-positive, so $f(x_\\mu) \\le f(x^\\star_{lex})$. This shows the sequence $\\{x_\\mu\\}$ is bounded.\n-   From the same inequality, we have $h(x_\\mu) - h^\\star \\le \\frac{f(x^\\star_{lex}) - f(x_\\mu)}{\\mu} \\le \\frac{f(x^\\star_{lex})}{\\mu}$. As $\\mu \\to +\\infty$, this implies $h(x_\\mu) \\to h^\\star$.\n-   Let $\\bar{x}$ be a cluster point of $\\{x_\\mu\\}$. By continuity of $h(x)$, $h(\\bar{x}) = h^\\star$. So, any cluster point $\\bar{x}$ is a minimizer of $h(x)$, i.e., $\\bar{x} \\in S_h$.\n-   Taking the limit superior of the inequality $f(x_\\mu) \\le f(x^\\star_{lex})$ along the subsequence converging to $\\bar{x}$, we get $f(\\bar{x}) \\le f(x^\\star_{lex})$.\n-   But by definition of $x^\\star_{lex}$ as the minimizer of $f(x)$ on $S_h$, and since $\\bar{x} \\in S_h$, we must have $f(\\bar{x}) \\ge f(x^\\star_{lex})$.\n-   Thus, $f(\\bar{x}) = f(x^\\star_{lex})$. Since $x^\\star_{lex}$ is the unique minimizer of $f$ on $S_h$, we must have $\\bar{x} = x^\\star_{lex}$.\n-   Since the bounded sequence $\\{x_\\mu\\}$ has a unique cluster point $x^\\star_{lex}$, the entire sequence converges to it. The statement is correct.\n\n**Verdict for D: Correct.**\n\n**E. In the $1$-dimensional case $n=1$ with the two inconsistent halfspaces $x \\le 0$ and $x \\ge 1$, the unique minimizer of $\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ is $x=0$ for every $\\mu>0$.**\n\nThe constraints are $x \\le 0$ and $-x \\le -1$. The feasible set is empty.\nThe soft objective is $F_\\mu(x) = \\tfrac{1}{2}x^2 + \\mu([x-0]_+ + [-x-(-1)]_+) = \\tfrac{1}{2}x^2 + \\mu([x]_+ + [1-x]_+)$.\nLet's analyze the penalty term $h(x) = [x]_+ + [1-x]_+$:\n-   If $x < 0$, $h(x) = 0 + (1-x) = 1-x$.\n-   If $0 \\le x \\le 1$, $h(x) = x + (1-x) = 1$.\n-   If $x > 1$, $h(x) = x + 0 = x$.\nSo, we minimize:\n$$ F_\\mu(x) = \\begin{cases} \\tfrac{1}{2}x^2 + \\mu(1-x) & \\text{if } x < 0 \\\\ \\tfrac{1}{2}x^2 + \\mu & \\text{if } 0 \\le x \\le 1 \\\\ \\tfrac{1}{2}x^2 + \\mu x & \\text{if } x > 1 \\end{cases} $$\nLet's find the derivatives on the open intervals:\n-   For $x < 0$, $F'_\\mu(x) = x - \\mu$. Since $x < 0$ and $\\mu > 0$, $F'_\\mu(x) < 0$. The function is decreasing.\n-   For $0 < x < 1$, $F'_\\mu(x) = x$. Since $x>0$, $F'_\\mu(x) > 0$. The function is increasing.\n-   For $x > 1$, $F'_\\mu(x) = x + \\mu$. Since $x>1$ and $\\mu > 0$, $F'_\\mu(x) > 0$. The function is increasing.\nThe function $F_\\mu(x)$ is decreasing for $x<0$ and increasing for $x>0$. The global minimum must be at $x=0$.\nTo be rigorous, we check the subgradient at $x=0$.\n$\\partial F_\\mu(0) = \\nabla(\\tfrac{1}{2}x^2)|_{x=0} + \\mu \\partial h(x)|_{x=0}$.\n$\\partial h(0) = \\partial [x]_+|_{x=0} + \\partial [1-x]_+|_{x=0} = [0,1] + \\{-1\\} = [-1,0]$.\nSo, $\\partial F_\\mu(0) = \\{0\\} + \\mu[-1,0] = [-\\mu, 0]$.\nFor $x=0$ to be a minimizer, $0$ must be in the subgradient: $0 \\in [-\\mu, 0]$. This is true for all $\\mu > 0$.\nThus, $x=0$ is indeed the unique minimizer for any $\\mu > 0$.\n\n**Verdict for E: Correct.**",
            "answer": "$$\\boxed{ABDE}$$"
        },
        {
            "introduction": "Many optimization problems, including finding the closest feasible point to an initial guess, boil down to projecting onto a convex set defined by an intersection of halfspaces. This coding challenge  asks you to implement Dykstra's algorithm, a powerful iterative method that accomplishes this by cleverly cycling through projections onto individual halfspaces. By building this algorithm from the ground up, you will gain hands-on experience with the computational geometry that underpins many advanced optimization solvers.",
            "id": "3137818",
            "problem": "Consider the Euclidean projection problem in a finite-dimensional real vector space. Let $n \\in \\mathbb{N}$ and let a family of $m \\in \\mathbb{N}$ closed halfspaces in $\\mathbb{R}^n$ be given by linear inequalities of the form $C_i = \\{x \\in \\mathbb{R}^n : a_i^\\top x \\le b_i\\}$ for $i \\in \\{1,\\dots,m\\}$, where $a_i \\in \\mathbb{R}^n$ and $b_i \\in \\mathbb{R}$. Let $x_0 \\in \\mathbb{R}^n$ be a point for which we seek the Euclidean projection onto the intersection $C = \\bigcap_{i=1}^m C_i$, that is, a point $x^\\star \\in C$ minimizing the objective $x \\mapsto \\tfrac{1}{2}\\|x - x_0\\|_2^2$.\n\nStarting from the fundamental definition of Euclidean projection onto a nonempty closed convex set in $\\mathbb{R}^n$ and the well-tested fact that the intersection of closed halfspaces is a closed convex set, derive and implement an iterative method that uses cyclic projections onto the individual halfspaces and per-set correction terms to converge to the projection of $x_0$ onto $C$. The method must be designed so that the corrections accumulate along the normals $a_i$ of the halfspaces, providing a direct way to observe this accumulation numerically. The projection onto a single closed halfspace must be treated by solving the corresponding minimal-distance problem subject to one linear inequality. The iterative method must terminate when the change in the iterate after a full cycle is below a prescribed tolerance or a maximum number of cycles is reached.\n\nYour program must implement this projection procedure and, for each test case in the test suite below, produce as its result a list of real numbers containing:\n- First, the coordinates of the projected point $x^\\star$ in order.\n- Next, for each halfspace in the order given, a scalar quantifying the accumulation of the final correction term along the corresponding normal. Use, for the $i$-th halfspace, the scalar $s_i = \\dfrac{a_i^\\top y_i}{\\|a_i\\|_2^2}$, where $y_i \\in \\mathbb{R}^n$ denotes the final correction vector stored for set $i$ by the iterative method at termination. This scalar equals the signed step length taken along the normal direction $a_i$ at the last correction for that set and is a concise measure of how corrections accumulate along $a_i$.\n\nUse the stopping tolerance $10^{-10}$ and a maximum of $10000$ cycles through all halfspaces. There are no physical units involved in this problem. All angles, if any appear implicitly in vector operations, are handled in radians by the standard inner product; no explicit angle unit is needed. All answers must be expressed as real numbers.\n\nTest suite:\n- Case $1$ ($2$-dimensional, two halfspaces forming the nonpositive quadrant): $n = 2$, $m = 2$, $a_1 = [1, 0]^\\top$, $b_1 = 0$, $a_2 = [0, 1]^\\top$, $b_2 = 0$, $x_0 = [1, 2]^\\top$.\n- Case $2$ ($2$-dimensional, three halfspaces forming a wedge with a floor): $n = 2$, $m = 3$, $a_1 = [1, 1]^\\top$, $b_1 = 1$, $a_2 = [-1, 2]^\\top$, $b_2 = 2$, $a_3 = [0, -1]^\\top$, $b_3 = 0$, $x_0 = [2, 3]^\\top$.\n- Case $3$ ($3$-dimensional axis-aligned box): $n = 3$, $m = 6$, $a_1 = [1, 0, 0]^\\top$, $b_1 = 1$, $a_2 = [-1, 0, 0]^\\top$, $b_2 = 1$, $a_3 = [0, 1, 0]^\\top$, $b_3 = 1$, $a_4 = [0, -1, 0]^\\top$, $b_4 = 1$, $a_5 = [0, 0, 1]^\\top$, $b_5 = 1$, $a_6 = [0, 0, -1]^\\top$, $b_6 = 1$, $x_0 = [2, -3, 0.5]^\\top$.\n- Case $4$ ($2$-dimensional point intersection from four halfspaces): $n = 2$, $m = 4$, $a_1 = [1, 0]^\\top$, $b_1 = 1$, $a_2 = [-1, 0]^\\top$, $b_2 = -1$, $a_3 = [0, 1]^\\top$, $b_3 = -1$, $a_4 = [0, -1]^\\top$, $b_4 = 1$, $x_0 = [-2, 4]^\\top$.\n- Case $5$ ($2$-dimensional interior point in the wedge of Case $2$): $n = 2$, $m = 3$, $a_1 = [1, 1]^\\top$, $b_1 = 1$, $a_2 = [-1, 2]^\\top$, $b_2 = 2$, $a_3 = [0, -1]^\\top$, $b_3 = 0$, $x_0 = [0.2, 0.3]^\\top$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this list must be the Python list representation for one test case, constructed by concatenating the coordinates of $x^\\star$ followed by the scalars $s_i$ in the order of the halfspaces for that case. For example, an output for three cases would look like $[ [x^\\star\\_1\\_1, x^\\star\\_1\\_2, s\\_{1,1}, s\\_{1,2}, \\dots], [x^\\star\\_2\\_1, x^\\star\\_2\\_2, s\\_{2,1}, s\\_{2,2}, \\dots], [x^\\star\\_3\\_1, x^\\star\\_3\\_2, x^\\star\\_3\\_3, s\\_{3,1}, \\dots ] ]$, with no additional text.",
            "solution": "The user-provided problem is valid. It is scientifically grounded in the field of convex optimization, specifically Euclidean projection onto a convex set. The problem is well-posed, as the projection of a point onto a nonempty closed convex set is unique. The problem statement is objective, complete, and contains no contradictions. We may proceed with a solution.\n\nThe problem asks for the Euclidean projection of a point $x_0 \\in \\mathbb{R}^n$ onto a closed convex set $C$, where $C$ is the intersection of $m$ closed halfspaces $C_i = \\{x \\in \\mathbb{R}^n : a_i^\\top x \\le b_i\\}$. The projection, denoted $x^\\star$, is the unique solution to the optimization problem:\n$$\nx^\\star = \\arg\\min_{x \\in C} \\frac{1}{2} \\|x - x_0\\|_2^2\n$$\nThe problem requires the implementation of a specific iterative method known as Dykstra's projection algorithm. This algorithm finds the projection onto an intersection of convex sets by performing a cycle of projections onto the individual sets, incorporating a set of auxiliary correction vectors that account for the iterative displacement.\n\n### Projection onto a Single Halfspace\n\nThe fundamental operation in Dykstra's algorithm is the projection onto a single halfspace, $P_{C_i}(y)$, which finds the point in $C_i$ closest to a given point $y \\in \\mathbb{R}^n$. This is a convex optimization subproblem:\n$$\n\\text{minimize} \\quad \\frac{1}{2}\\|x - y\\|_2^2 \\quad \\text{subject to} \\quad a_i^\\top x \\le b_i\n$$\nThe solution can be derived using the Karush-Kuhn-Tucker (KKT) conditions. The Lagrangian is $L(x, \\mu) = \\frac{1}{2}\\|x - y\\|_2^2 + \\mu(a_i^\\top x - b_i)$, with $\\mu \\ge 0$.\nThe KKT conditions are:\n1.  **Stationarity:** $\\nabla_x L = x - y + \\mu a_i = 0 \\implies x = y - \\mu a_i$.\n2.  **Primal feasibility:** $a_i^\\top x \\le b_i$.\n3.  **Dual feasibility:** $\\mu \\ge 0$.\n4.  **Complementary slackness:** $\\mu(a_i^\\top x - b_i) = 0$.\n\nTwo cases arise:\n- If $y$ is already in the halfspace $C_i$, then $a_i^\\top y \\le b_i$. We can set $\\mu = 0$. From stationarity, $x = y$. All KKT conditions are satisfied, so $P_{C_i}(y) = y$.\n- If $y$ is outside $C_i$, then $a_i^\\top y > b_i$. This requires $\\mu > 0$. By complementary slackness, the constraint must be active: $a_i^\\top x = b_i$. Substituting $x = y - \\mu a_i$ into this equality gives $a_i^\\top (y - \\mu a_i) = b_i$, which solves for $\\mu = \\frac{a_i^\\top y - b_i}{\\|a_i\\|_2^2}$. Since $a_i^\\top y > b_i$, we have $\\mu > 0$ as required. The projection is $x = y - \\frac{a_i^\\top y - b_i}{\\|a_i\\|_2^2} a_i$.\n\nCombining both cases using the positive part function $(z)_+ = \\max(0, z)$, the projection operator is given by:\n$$\nP_{C_i}(y) = y - \\frac{(a_i^\\top y - b_i)_+}{\\|a_i\\|_2^2} a_i\n$$\nThis formula assumes $a_i \\neq 0$, which is true for any well-defined halfspace.\n\n### Dykstra's Projection Algorithm\n\nDykstra's algorithm is an iterative procedure to find the projection onto the intersection $C=\\bigcap_{i=1}^m C_i$. It maintains a primary iterate, which converges to the solution $x^\\star$, and a set of $m$ auxiliary correction vectors, one for each halfspace. These vectors, denoted $y_i$, 'store' the displacement component normal to the respective halfspace boundary from previous projection steps. The problem statement accurately describes the role of these corrections.\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization:**\n    -   Set the initial iterate $x^{(0)} = x_0$.\n    -   Initialize $m$ correction vectors $y_i^{(0)} = \\mathbf{0} \\in \\mathbb{R}^n$ for $i=1, \\dots, m$.\n\n2.  **Iteration:** For each cycle $k = 0, 1, 2, \\dots$ up to a maximum number of cycles:\n    a.  Store the iterate at the start of the cycle: $x_{\\text{start of cycle}} = x^{(k)}$.\n    b.  Initialize the intermediate point for the cycle: $z_0 = x^{(k)}$.\n    c.  Perform a full cycle of projections through all halfspaces $i = 1, \\dots, m$:\n        i.   Define the point to be projected onto $C_i$: $v_i = z_{i-1} + y_i^{(k)}$.\n        ii.  Project this point: $z_i = P_{C_i}(v_i)$.\n        iii. Update the correction vector for the next cycle: $y_i^{(k+1)} = v_i - z_i$.\n    d.  Update the primary iterate for the next cycle: $x^{(k+1)} = z_m$.\n    e.  **Termination Check:** If $\\|x^{(k+1)} - x_{\\text{start of cycle}}\\|_2 < \\text{tolerance}$, the algorithm has converged.\n\nThe update rule for the correction vector, $y_i^{(k+1)} = v_i - P_{C_i}(v_i)$, combined with the formula for the projection operator, shows that $y_i^{(k+1)}$ is always a non-negative scalar multiple of the normal vector $a_i$:\n$$\ny_i^{(k+1)} = \\frac{(a_i^\\top v_i - b_i)_+}{\\|a_i\\|_2^2} a_i\n$$\nThis confirms the problem's description that \"corrections accumulate along the normals $a_i$\".\n\n### Final Output Calculation\n\nUpon termination, the algorithm yields the projected point $x^\\star \\approx x^{(k+1)}$ and the final set of correction vectors $\\{y_i^{\\text{final}}\\}$. The problem requires calculating a scalar $s_i$ for each halfspace, defined as:\n$$\ns_i = \\frac{a_i^\\top y_i^{\\text{final}}}{\\|a_i\\|_2^2}\n$$\nSince $y_i^{\\text{final}}$ must be of the form $\\lambda_i a_i$ for some scalar $\\lambda_i \\ge 0$, we can substitute this into the formula for $s_i$:\n$$\ns_i = \\frac{a_i^\\top (\\lambda_i a_i)}{\\|a_i\\|_2^2} = \\frac{\\lambda_i (a_i^\\top a_i)}{\\|a_i\\|_2^2} = \\lambda_i\n$$\nThus, $s_i$ is precisely the final scalar multiplier that relates the correction vector $y_i^{\\text{final}}$ to the normal vector $a_i$. This value represents the cumulative correction magnitude in the direction of $a_i$.\n\nThe implementation will follow this derived algorithm, compute the final coordinates of $x^\\star$, and then use the final correction vectors $\\{y_i^{\\text{final}}\\}$ to compute the scalars $\\{s_i\\}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Euclidean projection problem for a set of test cases using Dykstra's algorithm.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: 2D, nonpositive quadrant\n        {\n            \"x0\": np.array([1, 2], dtype=float),\n            \"a\": [np.array([1, 0], dtype=float), np.array([0, 1], dtype=float)],\n            \"b\": [0.0, 0.0]\n        },\n        # Case 2: 2D, wedge with floor\n        {\n            \"x0\": np.array([2, 3], dtype=float),\n            \"a\": [np.array([1, 1], dtype=float), np.array([-1, 2], dtype=float), np.array([0, -1], dtype=float)],\n            \"b\": [1.0, 2.0, 0.0]\n        },\n        # Case 3: 3D, axis-aligned box\n        {\n            \"x0\": np.array([2, -3, 0.5], dtype=float),\n            \"a\": [\n                np.array([1, 0, 0], dtype=float), np.array([-1, 0, 0], dtype=float),\n                np.array([0, 1, 0], dtype=float), np.array([0, -1, 0], dtype=float),\n                np.array([0, 0, 1], dtype=float), np.array([0, 0, -1], dtype=float)\n            ],\n            \"b\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n        },\n        # Case 4: 2D, point intersection\n        {\n            \"x0\": np.array([-2, 4], dtype=float),\n            \"a\": [\n                np.array([1, 0], dtype=float), np.array([-1, 0], dtype=float),\n                np.array([0, 1], dtype=float), np.array([0, -1], dtype=float)\n            ],\n            \"b\": [1.0, -1.0, -1.0, 1.0]\n        },\n        # Case 5: 2D, interior point\n        {\n            \"x0\": np.array([0.2, 0.3], dtype=float),\n            \"a\": [np.array([1, 1], dtype=float), np.array([-1, 2], dtype=float), np.array([0, -1], dtype=float)],\n            \"b\": [1.0, 2.0, 0.0]\n        }\n    ]\n\n    # Algorithm parameters\n    TOLERANCE = 1e-10\n    MAX_CYCLES = 10000\n\n    results = []\n\n    def project_halfspace(y_proj, a_i, b_i):\n        \"\"\"\n        Projects a point y_proj onto the halfspace a_i^T x <= b_i.\n        \"\"\"\n        a_norm_sq = np.dot(a_i, a_i)\n        if a_norm_sq == 0:\n            return y_proj\n\n        violation = np.dot(a_i, y_proj) - b_i\n        \n        if violation <= 0:\n            return y_proj\n        \n        lambda_val = violation / a_norm_sq\n        return y_proj - lambda_val * a_i\n\n    for case in test_cases:\n        x0 = case[\"x0\"]\n        a_vectors = case[\"a\"]\n        b_scalars = case[\"b\"]\n        m = len(a_vectors)\n        n = len(x0)\n\n        # Initialization\n        x_k = np.copy(x0)\n        y_corrections = [np.zeros(n, dtype=float) for _ in range(m)]\n\n        for k in range(MAX_CYCLES):\n            x_start_of_cycle = np.copy(x_k)\n            \n            x_intermediate = x_k\n            for i in range(m):\n                a_i = a_vectors[i]\n                b_i = b_scalars[i]\n                \n                point_to_project = x_intermediate + y_corrections[i]\n                x_projected = project_halfspace(point_to_project, a_i, b_i)\n                \n                # Update correction vector for this halfspace\n                y_corrections[i] = point_to_project - x_projected\n                \n                # Update the intermediate point for the next projection in the cycle\n                x_intermediate = x_projected\n            \n            x_k = x_intermediate # Final point after one full cycle\n            \n            # Termination check\n            change = np.linalg.norm(x_k - x_start_of_cycle)\n            if change < TOLERANCE:\n                break\n        \n        x_star = x_k\n        \n        # Calculate final scalars s_i\n        s_values = []\n        for i in range(m):\n            a_i = a_vectors[i]\n            y_i = y_corrections[i]\n            a_norm_sq = np.dot(a_i, a_i)\n            if a_norm_sq > 0:\n                s_i = np.dot(a_i, y_i) / a_norm_sq\n            else:\n                s_i = 0.0 # Should not happen with valid halfspaces\n            s_values.append(s_i)\n            \n        # Combine results for this case\n        case_result = list(x_star) + s_values\n        results.append(case_result)\n\n    # Format the final output string\n    # str() on a list gives a nice '[...]' representation.\n    # We join these string representations with commas.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}