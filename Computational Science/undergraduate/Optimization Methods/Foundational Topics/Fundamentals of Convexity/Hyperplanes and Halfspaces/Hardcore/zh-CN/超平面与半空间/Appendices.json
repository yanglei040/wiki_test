{
    "hands_on_practices": [
        {
            "introduction": "理解基本几何对象（如半空间和仿射子空间）之间的相互作用，是掌握优化理论的关键。这个练习  将引导你分析一个基本算法——交替投影法。通过推导一个简单而富有启发性的案例的收敛速度，你将亲身体验到几何直觉如何转化为严谨的数学分析，并揭示投影算子背后深刻的几何关系。",
            "id": "3137760",
            "problem": "考虑欧几里得空间 $\\mathbb{R}^{2}$，其配备了标准欧几里得范数 $\\|\\cdot\\|_{2}$ 和内积 $x^{\\top}y$。设 $H$ 为闭半空间\n$$\nH \\;=\\; \\{\\, x \\in \\mathbb{R}^{2} \\mid n^{\\top} x \\le 0 \\,\\},\n$$\n其中 $n \\in \\mathbb{R}^{2}$ 是满足 $\\|n\\|_{2} = 1$ 的单位法向量。设 $S$ 是由直线给出的仿射子空间\n$$\nS \\;=\\; \\{\\, p + t\\,u \\mid t \\in \\mathbb{R} \\,\\},\n$$\n其中 $u \\in \\mathbb{R}^{2}$ 是满足 $\\|u\\|_{2}=1$ 的单位方向向量，而 $p \\in \\mathbb{R}^{2}$ 是一个满足 $n^{\\top}p = 0$ 的点（因此 $p$ 位于 $H$ 的边界超平面上）。定义交替投影序列 $x_{k+1} = P_{H}(P_{S}(x_{k}))$，其中对于任何非空闭凸集 $C$，$P_{C}(x)$ 表示 $x$ 在 $C$ 上的欧几里得投影，即在 $y \\in C$ 上 $\\|x - y\\|_{2}$ 的唯一最小化子。\n\n通过 $p$ 平移坐标，使得交点 $p$ 位于原点，并定义 $y_{k} = P_{H}(x_{k})$（在第 $k$ 个周期中经过半空间投影后的点）。假设初始点满足 $y_{0}$ 位于 $H$ 的边界超平面上，即 $n^{\\top}y_{0}=0$，并且该序列对于所有 $k$ 都满足 $n^{\\top}P_{S}(y_{k}) \\ge 0$（也就是说，每次到 $S$ 上的投影都产生一个位于 $H$ 上或外部的点，因此后续到 $H$ 上的投影表现为到边界超平面上的正交投影）。设 $\\theta \\in [0,\\pi]$ 表示单位向量 $n$ 和 $u$ 之间的夹角，即 $\\cos\\theta = n^{\\top}u$。\n\n从欧几里得投影的基本定义出发，推导迭代更新 $y_{k+1}$ 关于 $y_{k}$ 以及几何量 $n$、$u$ 和 $\\theta$ 的闭式表达式。然后，利用此表达式，确定每个交替投影完整周期的线性收敛速度，该速度由标量 $r(\\theta)$ 量化，使得对于所有足够大的 $k$，\n$$\n\\|y_{k+1}\\|_{2} \\;=\\; r(\\theta)\\,\\|y_{k}\\|_{2}.\n$$\n你的最终答案必须是关于 $\\theta$ 的单个闭式解析表达式。无需四舍五入，也不涉及单位。",
            "solution": "在尝试解答之前，需对问题进行验证。\n\n### 第 1 步：提取已知条件\n- 空间是欧几里得空间 $\\mathbb{R}^{2}$，配备标准欧几里得范数 $\\|\\cdot\\|_{2}$ 和内积 $x^{\\top}y$。\n- 闭半空间为 $H = \\{\\, x \\in \\mathbb{R}^{2} \\mid n^{\\top} x \\le 0 \\,\\}$，其中 $n \\in \\mathbb{R}^{2}$ 是单位法向量，$\\|n\\|_{2} = 1$。\n- 仿射子空间是直线 $S = \\{\\, p + t\\,u \\mid t \\in \\mathbb{R} \\,\\}$，其中 $u \\in \\mathbb{R}^{2}$ 是单位方向向量，$\\|u\\|_{2}=1$，且 $p \\in \\mathbb{R}^{2}$。\n- 给定 $p$ 的一个条件：$n^{\\top}p = 0$，这意味着 $p$ 位于 $H$ 的边界上，记作 $\\partial H = \\{\\, x \\in \\mathbb{R}^{2} \\mid n^{\\top} x = 0 \\,\\}$。\n- 交替投影序列定义为 $x_{k+1} = P_{H}(P_{S}(x_{k}))$，其中 $P_{C}(x)$ 是点 $x$ 在闭凸集 $C$ 上的欧几里得投影。\n- 定义了一个辅助序列 $y_{k} = P_{H}(x_{k})$。\n- 给定辅助序列的初始条件：$y_{0}$ 位于边界超平面上，$n^{\\top}y_{0}=0$。\n- 提供了一个关于序列的关键假设：对于所有 $k \\ge 0$，$n^{\\top}P_{S}(y_{k}) \\ge 0$。\n- $n$ 和 $u$ 之间的夹角 $\\theta \\in [0,\\pi]$ 定义为 $\\cos\\theta = n^{\\top}u$。\n- 给定一条指令：“通过 $p$ 平移坐标，使得交点 $p$ 位于原点”。\n- 目标是推导 $y_{k+1}$ 关于 $y_k$ 的更新规则，然后找到由关系式 $\\|y_{k+1}\\|_{2} = r(\\theta)\\,\\|y_{k}\\|_{2}$（对于足够大的 $k$）定义的线性收敛速度 $r(\\theta)$。\n\n### 第 2 步：使用提取的已知条件进行验证\n该问题是优化方法领域中一个定义明确的练习，具体涉及交替投影算法的收敛性。\n- **科学依据（关键）**：该问题基于欧几里得几何、线性代数和凸分析（投影到凸集上）的基本概念。它在数学上是合理的。\n- **适定性**：该问题是适定的。关于主序列 $x_k$ 和辅助序列 $y_k$ 之间关系的模糊性已由问题结构解决。假设 $n^{\\top}P_{S}(y_{k}) \\ge 0$ 以及找到 $\\|y_k\\|_2$ 递推关系的目标，意味着问题意图是对有效序列 $y_{k+1} = P_H(P_S(y_k))$ 进行分析。在这种标准解释下，$r(\\theta)$ 存在唯一解。\n- **客观性（关键）**：该问题使用精确、客观的数学语言陈述。\n- **所有其他标准**：该问题不违反任何其他验证标准。它不是不完整、矛盾、不切实际或微不足道的。\n\n### 第 3 步：结论与行动\n该问题是**有效的**。将提供完整解答。\n\n### 解答推导\n该问题要求分析一个几何迭代。我们将遵循所提供的指令，并从第一性原理出发推导结果。\n\n首先，我们采纳指定的坐标平移。设新的坐标系用撇号表示，其中 $x' = x - p$。直线和超平面边界的交点现在位于原点，$p' = p-p=0$。\n- 仿射集 $S$ 变为一个线性子空间：$S' = S - p = \\{\\, tu \\mid t \\in \\mathbb{R} \\,\\}$。\n- 半空间 $H$ 由 $n^{\\top}x \\le 0$ 描述。在新的坐标系中，$n^{\\top}(x'+p) \\le 0$，简化为 $n^{\\top}x' + n^{\\top}p \\le 0$。由于给定 $n^{\\top}p = 0$，半空间在新坐标系中具有相同的形式：$H' = \\{\\, x' \\in \\mathbb{R}^{2} \\mid n^{\\top} x' \\le 0 \\,\\}$。\n- 迭代点相应地平移为 $y'_k = y_k - p$。初始条件 $n^{\\top}y_0 = 0$ 变为 $n^{\\top}(y'_0+p) = 0$，这意味着 $n^{\\top}y'_0 = 0$。因此，平移后的初始点 $y'_0$ 位于平移后的边界 $\\partial H'$ 上。\n\n为简单起见，从现在开始我们只在这个平移后的坐标系中工作，并省略撇号。现在的问题是分析一个从超平面 $\\partial H = \\{\\, x \\mid n^{\\top}x = 0 \\,\\}$ 上的 $y_0$ 开始的迭代，其中 $S = \\{\\, tu \\mid t \\in \\mathbb{R} \\,\\}$ 是一条过原点的直线。\n\n该问题要求找到 $y_{k+1}$ 和 $y_k$ 之间的关系。如在验证步骤中所论证的，我们分析有效迭代 $y_{k+1} = P_{H}(P_{S}(y_{k}))$。该序列从满足 $n^\\top y_0 = 0$ 的 $y_0$ 开始。\n\n该迭代包含两个投影步骤：\n1.  **到 $S$ 上的投影**：集合 $S$ 是一条沿单位向量 $u$ 方向穿过原点的直线。点 $y_k$ 到 $S$ 上的欧几里得投影由下式给出：\n    $$\n    P_{S}(y_k) = (y_k^{\\top}u)u\n    $$\n    这是通过关于 $t$ 最小化 $\\|y_k - tu\\|_2^2$ 得出的，给出最优 $t = y_k^{\\top}u / \\|u\\|_2^2 = y_k^{\\top}u$。我们用 $z_k = P_S(y_k)$ 表示结果。\n\n2.  **到 $H$ 上的投影**：然后将点 $z_k$ 投影到半空间 $H = \\{\\, x \\mid n^{\\top}x \\le 0 \\,\\}$ 上。该投影的公式取决于 $z_k$ 是在 $H$ 内部还是外部。问题提供了一个关键假设，即对于所有 $k$，$n^{\\top}P_{S}(y_{k}) \\ge 0$。这意味着 $n^{\\top}z_k \\ge 0$。\n    - 如果 $n^{\\top}z_k = 0$，那么 $z_k$ 位于边界 $\\partial H$ 上，所以 $z_k \\in H$，且 $P_H(z_k) = z_k$。\n    - 如果 $n^{\\top}z_k  0$，那么 $z_k$ 在 $H$ 外部。$z_k$ 在闭凸集 $H$ 上的投影是其在边界超平面 $\\partial H = \\{\\, x \\mid n^{\\top}x = 0 \\,\\}$ 上的正交投影。该投影由下式给出：\n    $$\n    P_{H}(z_k) = z_k - (n^{\\top}z_k)n\n    $$\n    这个相同的公式也涵盖了 $n^{\\top}z_k = 0$ 的情况。因此，在给定假设下，到 $H$ 上的投影总是由这个表达式给出。\n\n现在，我们结合这两个步骤来找到 $y_{k+1}$ 的更新规则。\n$$\ny_{k+1} = P_{H}(z_k) = P_{H}(P_{S}(y_k)) = P_S(y_k) - (n^{\\top}P_S(y_k))n\n$$\n代入 $P_S(y_k)$ 的表达式：\n$$\ny_{k+1} = (y_k^{\\top}u)u - (n^{\\top}((y_k^{\\top}u)u))n\n$$\n$y_k^{\\top}u$ 项是标量，所以我们可以把它提取出来：\n$$\ny_{k+1} = (y_k^{\\top}u)u - (y_k^{\\top}u)(n^{\\top}u)n\n$$\n这就得出了迭代更新的闭式表达式：\n$$\ny_{k+1} = (y_k^{\\top}u) [u - (n^{\\top}u)n]\n$$\n这个递推关系的一个重要性质是，如果 $y_k \\in \\partial H$，那么 $y_{k+1} \\in \\partial H$。我们可以验证这一点：\n$n^{\\top}y_{k+1} = n^{\\top}((y_k^{\\top}u)[u - (n^{\\top}u)n]) = (y_k^{\\top}u)[n^{\\top}u - (n^{\\top}u)(n^{\\top}n)]$。由于 $\\|n\\|_2=1$，我们有 $n^{\\top}n=1$。因此，$n^{\\top}y_{k+1} = (y_k^{\\top}u)[n^{\\top}u - n^{\\top}u] = 0$。因为我们从边界上的 $y_0$ 开始，所有后续的迭代点 $y_k$ 都保持在边界 $\\partial H$ 上。\n\n接下来，我们通过求范数之比 $\\|y_{k+1}\\|_2 / \\|y_k\\|_2$ 来确定线性收敛速度 $r(\\theta)$。\n对更新方程取欧几里得范数：\n$$\n\\|y_{k+1}\\|_2 = \\| (y_k^{\\top}u) [u - (n^{\\top}u)n] \\|_2 = |y_k^{\\top}u| \\, \\|u - (n^{\\top}u)n\\|_2\n$$\n我们分析右侧的两项。\n\n第 1 项：$|y_k^{\\top}u|$。\n向量 $y_k$ 位于直线 $\\partial H = \\{\\, x \\mid n^{\\top}x = 0 \\,\\}$ 上。这意味着 $y_k$ 与单位向量 $n$ 正交。$y_k^{\\top}u$ 项是 $y_k$ 和单位向量 $u$ 之间的内积。设 $\\phi$ 为 $y_k$ 和 $u$ 之间的夹角。那么 $y_k^{\\top}u = \\|y_k\\|_2 \\|u\\|_2 \\cos\\phi = \\|y_k\\|_2 \\cos\\phi$。\n向量 $u$ 与 $n$ 形成夹角 $\\theta$。由于 $y_k$ 与 $n$ 正交，所以 $u$ 与包含 $y_k$ 的直线之间的夹角是 $\\frac{\\pi}{2} - \\theta$。因此，$u$ 和 $y_k$ 之间的夹角 $\\phi$ 必须是 $\\frac{\\pi}{2} - \\theta$ 或 $\\frac{\\pi}{2} + \\theta$（取决于 $y_k$ 沿直线 $\\partial H$ 的方向）。在这两种情况下，都有 $|\\cos\\phi| = |\\cos(\\frac{\\pi}{2} \\pm \\theta)| = |\\mp\\sin\\theta| = |\\sin\\theta|$。\n由于 $\\theta \\in [0, \\pi]$，$\\sin\\theta \\ge 0$，所以 $|\\sin\\theta| = \\sin\\theta$。\n因此，$|y_k^{\\top}u| = \\|y_k\\|_2 \\sin\\theta$。\n\n第 2 项：$\\|u - (n^{\\top}u)n\\|_2$。\n向量 $v = u - (n^{\\top}u)n$ 是单位向量 $u$ 正交于 $n$ 的分量。这是应用格拉姆-施密特过程的结果。我们可以计算其范数的平方：\n$$\n\\|u - (n^{\\top}u)n\\|_2^2 = (u - (n^{\\top}u)n)^{\\top}(u - (n^{\\top}u)n)\n$$\n$$\n= u^{\\top}u - 2(n^{\\top}u)(u^{\\top}n) + (n^{\\top}u)^2(n^{\\top}n)\n$$\n由于 $u$ 和 $n$ 是单位向量，$u^{\\top}u = 1$ 且 $n^{\\top}n = 1$。并且，$n^{\\top}u = \\cos\\theta$。\n$$\n= 1 - 2(\\cos\\theta)^2 + (\\cos\\theta)^2 = 1 - \\cos^2\\theta = \\sin^2\\theta\n$$\n取平方根可得 $\\|u - (n^{\\top}u)n\\|_2 = \\sqrt{\\sin^2\\theta} = |\\sin\\theta|$。由于 $\\theta \\in [0, \\pi]$，$\\sin\\theta \\ge 0$，所以范数是 $\\sin\\theta$。\n\n结合这两项，我们得到 $y_{k+1}$ 的范数：\n$$\n\\|y_{k+1}\\|_2 = (\\|y_k\\|_2 \\sin\\theta) (\\sin\\theta) = (\\sin^2\\theta) \\|y_k\\|_2\n$$\n关系式 $\\|y_{k+1}\\|_2 = r(\\theta)\\|y_k\\|_2$ 对所有 $k \\ge 0$ 成立。因此，每个周期的线性收敛速度由标量因子 $r(\\theta)$ 给出。\n\n$$\nr(\\theta) = \\sin^2\\theta\n$$",
            "answer": "$$\\boxed{\\sin^2(\\theta)}$$"
        },
        {
            "introduction": "虽然半空间定义了严格的边界，但在许多现实世界的优化问题中，尤其是在机器学习领域，我们需要更具弹性的建模工具。这个练习  探讨了“硬”约束（要求严格满足）与“软”约束（允许一定程度的违反并施加惩罚）之间的深刻联系。通过分析基于铰链损失（hinge loss）的惩罚方法，你将更好地理解如何利用超平面来构建更稳健、更实用的模型。",
            "id": "3137852",
            "problem": "考虑$\\mathbb{R}^n$中的一个半空间约束族，由 $a_i^\\top x \\le b_i$ 给出，其中 $i \\in \\{1,\\dots,m\\}$，$a_i \\in \\mathbb{R}^n$ 且 $b_i \\in \\mathbb{R}$。定义目标函数 $f(x) = \\tfrac{1}{2}\\|x\\|_2^2$。硬约束问题寻求精确的可行性：\n- 硬问题 $\\mathcal{P}_{\\mathrm{hard}}$：最小化 $f(x)$，约束条件为对所有 $i \\in \\{1,\\dots,m\\}$ 都有 $a_i^\\top x \\le b_i$。\n\n为了探索半空间的软执行，定义折页惩罚（hinge penalty） $[t]_+ := \\max\\{0,t\\}$，并考虑对于一个惩罚权重 $\\mu  0$ 的无约束软惩罚问题：\n- 软问题 $\\mathcal{P}_{\\mathrm{soft}}(\\mu)$：在 $x \\in \\mathbb{R}^n$ 上最小化 $F_\\mu(x) := f(x) + \\mu \\sum_{i=1}^m [a_i^\\top x - b_i]_+$。\n\n在整个问题中，假设 $f$ 和 $[\\,\\cdot\\,]_+$ 的定义如上，且 $a_i$ 和 $b_i$ 是固定的数据。当一个陈述提到拉格朗日乘子时，应在不等式约束的标准凸对偶假设下（例如，Slater 条件，以确保最优 Karush-Kuhn-Tucker (KKT) 乘子存在）来解释它。\n\n选择所有正确的陈述。\n\nA. 对于每个 $\\mu  0$，函数 $F_\\mu$ 是凸的且是强制的（coercive），并且由于 $f$ 是强凸的，$\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ 在 $\\mathbb{R}^n$ 中有唯一的最小值点。\n\nB. 如果可行集 $\\{x \\in \\mathbb{R}^n : a_i^\\top x \\le b_i \\ \\forall i\\}$ 非空，则当 $\\mu \\to +\\infty$ 时，$\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ 的任意最小值点序列 $\\{x_\\mu\\}$ 的所有聚点都是 $\\mathcal{P}_{\\mathrm{hard}}$ 的解。因为 $f$ 是强凸的，硬问题有唯一解 $x^\\star$，并且当 $\\mu \\to +\\infty$ 时，$x_\\mu \\to x^\\star$。\n\nC. 如果可行集非空，那么对于每个固定的 $\\mu  0$，$\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ 的唯一最小值点必然对于硬约束是可行的。\n\nD. 如果可行集为空，令 $h(x) := \\sum_{i=1}^m [a_i^\\top x - b_i]_+$。当 $\\mu \\to +\\infty$ 时，$\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ 的最小值点 $x_\\mu$ 的任意聚点都是以下字典序优化的解：首先在 $x \\in \\mathbb{R}^n$ 上最小化 $h(x)$，然后在 $h$ 的所有最小值点中最小化 $f(x)$。\n\nE. 在一维情况 $n=1$ 下，对于两个不相容的半空间 $x \\le 0$ 和 $x \\ge 1$，对每个 $\\mu0$，$\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ 的唯一最小值点是 $x=0$。\n\n选择所有适用的选项：\nA, B, C, D, E。",
            "solution": "## 问题验证\n\n### 第一步：提取已知条件\n- **定义域**：$\\mathbb{R}^n$\n- **半空间约束**：$a_i^\\top x \\le b_i$，其中 $i \\in \\{1,\\dots,m\\}$，$a_i \\in \\mathbb{R}^n$ 且 $b_i \\in \\mathbb{R}$。\n- **目标函数**：$f(x) = \\tfrac{1}{2}\\|x\\|_2^2$。\n- **硬约束问题 ($\\mathcal{P}_{\\mathrm{hard}}$)**：最小化 $f(x)$，约束条件为对所有 $i \\in \\{1,\\dots,m\\}$ 都有 $a_i^\\top x \\le b_i$。\n- **折页惩罚函数**：$[t]_+ := \\max\\{0,t\\}$。\n- **惩罚权重**：$\\mu  0$。\n- **软惩罚目标函数**：$F_\\mu(x) := f(x) + \\mu \\sum_{i=1}^m [a_i^\\top x - b_i]_+$。\n- **软惩罚问题 ($\\mathcal{P}_{\\mathrm{soft}}(\\mu)$)**：在 $x \\in \\mathbb{R}^n$ 上最小化 $F_\\mu(x)$。\n- **对偶性假设**：当提及拉格朗日乘子时，假定标准凸对偶假设（例如 Slater 条件）成立，以确保最优 Karush-Kuhn-Tucker (KKT) 乘子的存在。\n\n### 第二步：使用提取的已知条件进行验证\n问题陈述在科学上和数学上是合理的。它描述了惩罚方法在凸二次规划问题中的一个标准应用。这些函数、变量和优化问题在凸优化领域内都有明确的定义。\n\n1.  **科学或事实不合理性**：无。该问题基于凸分析和优化理论的既定原则。\n2.  **无法形式化或不相关**：无。该问题是一个形式化的数学问题，直接关系到超平面、半空间和优化方法。\n3.  **不完整或矛盾的设置**：无。该问题是自洽的。选项正确地探讨了不同的情景，例如约束集的可行性，这是分析的关键部分。\n4.  **不切实际或不可行**：无。该问题是抽象和数学的；物理现实主义不是一个相关的标准。\n5.  **病态或结构不良**：无。问题 $\\mathcal{P}_{\\mathrm{hard}}$ 和 $\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ 是标准的且适定的。解的存在性和唯一性是所涉及函数性质的结果，这是一个需要分析的课题。\n6.  **伪深刻、琐碎或同义反复**：无。该问题需要对凸函数、惩罚方法及其收敛性质有扎实的理解。\n7.  **无法进行科学验证**：无。所有陈述都可以通过数学方法证明或证伪。\n\n### 第三步：结论和行动\n问题是 **有效的**。解答将继续对每个选项进行详细的推导和评估。\n\n---\n\n### 函数分析\n\n目标函数是 $f(x) = \\tfrac{1}{2}\\|x\\|_2^2$。这个函数是强制的（coercive），因为当 $\\|x\\|_2 \\to +\\infty$ 时 $f(x) \\to +\\infty$。它的 Hessian 矩阵是单位矩阵，$\\nabla^2 f(x) = I$，这是正定的。因此，$f(x)$ 是一个 $1$-强凸函数。\n\n惩罚项涉及折页函数（hinge function）$[t]_+ = \\max\\{0,t\\}$，它是凸的。函数 $g_i(x) = a_i^\\top x - b_i$ 是仿射的，因此是凸的。复合函数 $[g_i(x)]_+$ 是凸的，因为 $[\\cdot]_+$ 是一个凸的非减函数。凸函数的和 $\\sum_{i=1}^m [a_i^\\top x - b_i]_+$ 也是凸的。\n\n软问题的目标函数是 $F_\\mu(x) = f(x) + \\mu \\sum_{i=1}^m [a_i^\\top x - b_i]_+$。\n\n### 逐项分析\n\n**A. 对于每个 $\\mu  0$，函数 $F_\\mu$ 是凸的且是强制的，并且由于 $f$ 是强凸的，$\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ 在 $\\mathbb{R}^n$ 中有唯一的最小值点。**\n\n-   **凸性**：$F_\\mu(x)$ 是 $f(x)$ 和 $\\mu \\sum_{i=1}^m [a_i^\\top x - b_i]_+$ 的和。如前所述，$f(x)$ 是强凸的，惩罚项是凸的（作为凸函数的非负加权和）。一个强凸函数和一个凸函数的和是强凸的。因此，对于任何 $\\mu  0$，$F_\\mu(x)$ 都是强凸的。强凸性意味着严格凸性，而严格凸性又意味着凸性。\n-   **强制性**：惩罚项 $\\mu \\sum_{i=1}^m [a_i^\\top x - b_i]_+$ 是非负的。因此，$F_\\mu(x) \\ge f(x) = \\tfrac{1}{2}\\|x\\|_2^2$。由于 $f(x)$ 是强制的（即当 $\\|x\\|_2 \\to +\\infty$ 时 $f(x) \\to +\\infty$），$F_\\mu(x)$ 也必定是强制的。\n-   **最小值点的存在性与唯一性**：一个在 $\\mathbb{R}^n$ 上连续且强制的函数至少有一个最小值点。一个严格凸的函数至多有一个最小值点。由于 $F_\\mu(x)$ 是强凸的（因此是严格凸的）和强制的，它在 $\\mathbb{R}^n$ 中拥有唯一的最小值点。该陈述中的推理是正确的。\n\n**对 A 的结论：正确。**\n\n**B. 如果可行集 $\\{x \\in \\mathbb{R}^n : a_i^\\top x \\le b_i \\ \\forall i\\}$ 非空，则当 $\\mu \\to +\\infty$ 时，$\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ 的任意最小值点序列 $\\{x_\\mu\\}$ 的所有聚点都是 $\\mathcal{P}_{\\mathrm{hard}}$ 的解。因为 $f$ 是强凸的，硬问题有唯一解 $x^\\star$，并且当 $\\mu \\to +\\infty$ 时，$x_\\mu \\to x^\\star$。**\n\n这个陈述描述了惩罚方法对于一个可行问题的收敛性。\n-   令 $S = \\{x \\in \\mathbb{R}^n : a_i^\\top x \\le b_i \\ \\forall i\\}$ 为可行集。由于 $S$ 是一个非空闭凸集且 $f(x)$ 是强凸的，$\\mathcal{P}_{\\mathrm{hard}}$ 有一个唯一的解，我们记为 $x^\\star$。令 $f^\\star = f(x^\\star)$。\n-   令 $x_\\mu$ 为 $F_\\mu(x)$ 的唯一最小值点。根据定义，对于任何 $x \\in \\mathbb{R}^n$，都有 $F_\\mu(x_\\mu) \\le F_\\mu(x)$。\n-   我们选择 $x = x^\\star$。由于 $x^\\star \\in S$，我们有对所有 $i$ 都有 $a_i^\\top x^\\star - b_i \\le 0$，这意味着 $[a_i^\\top x^\\star - b_i]_+ = 0$。因此，$F_\\mu(x^\\star) = f(x^\\star) = f^\\star$。\n-   不等式变为：$f(x_\\mu) + \\mu \\sum_{i=1}^m [a_i^\\top x_\\mu - b_i]_+ \\le f^\\star$。\n-   由此，我们推断出两个事实：\n    1.  $f(x_\\mu) \\le f^\\star$，这意味着 $\\tfrac{1}{2}\\|x_\\mu\\|_2^2 \\le f^\\star$。这表明序列 $\\{x_\\mu\\}$ 是有界的。根据 Bolzano-Weierstrass 定理，它至少有一个聚点。\n    2.  $\\sum_{i=1}^m [a_i^\\top x_\\mu - b_i]_+ \\le \\frac{f^\\star - f(x_\\mu)}{\\mu} \\le \\frac{f^\\star}{\\mu}$。当 $\\mu \\to +\\infty$ 时，右侧趋向于 $0$。所以，$\\sum_{i=1}^m [a_i^\\top x_\\mu - b_i]_+ \\to 0$。\n-   令 $\\bar{x}$ 为 $\\{x_\\mu\\}$ 的任意聚点。存在一个子序列 $\\{x_{\\mu_k}\\}$ 使得 $x_{\\mu_k} \\to \\bar{x}$。\n-   根据连续性，$\\lim_{k\\to\\infty} \\sum_{i=1}^m [a_i^\\top x_{\\mu_k} - b_i]_+ = \\sum_{i=1}^m [a_i^\\top \\bar{x} - b_i]_+ = 0$。由于每一项都是非负的，这意味着对所有 $i$ 都有 $[a_i^\\top \\bar{x} - b_i]_+ = 0$，即对所有 $i$ 都有 $a_i^\\top \\bar{x} \\le b_i$。因此，$\\bar{x}$ 对于 $\\mathcal{P}_{\\mathrm{hard}}$ 是可行的。\n-   此外，对 $f(x_{\\mu_k}) \\le f^\\star$ 取极限得到 $f(\\bar{x}) \\le f^\\star$。\n-   由于 $\\bar{x}$ 是 $\\mathcal{P}_{\\mathrm{hard}}$ 的一个可行点，而 $x^\\star$ 是最小值点，我们必然有 $f(\\bar{x}) \\ge f^\\star$。\n-   结合这两点得到 $f(\\bar{x}) = f^\\star$。由于 $x^\\star$ 是唯一的最小值点，必然有 $\\bar{x} = x^\\star$。\n-   一个有界的且只有一个聚点的序列会收敛到该点。因此，当 $\\mu \\to +\\infty$ 时，$x_\\mu \\to x^\\star$。该陈述是惩罚方法中的一个经典结果。\n\n**对 B 的结论：正确。**\n\n**C. 如果可行集非空，那么对于每个固定的 $\\mu  0$，$\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ 的唯一最小值点必然对于硬约束是可行的。**\n\n这个陈述声称，对于任何有限的惩罚参数 $\\mu  0$，惩罚方法都能找到一个可行解。这通常是不正确的。解 $x_\\mu$ 通常随着 $\\mu$ 的增加而趋近于可行域。\n我们来构造一个反例。\n设 $n=1$，单一约束为 $x \\le -1$。可行集为 $(-\\infty, -1]$，非空。\n$\\mathcal{P}_{\\mathrm{hard}}$：最小化 $\\tfrac{1}{2}x^2$，约束为 $x \\le -1$。解为 $x^\\star = -1$。\n$\\mathcal{P}_{\\mathrm{soft}}(\\mu)$：最小化 $F_\\mu(x) = \\tfrac{1}{2}x^2 + \\mu[x - (-1)]_+ = \\tfrac{1}{2}x^2 + \\mu[x+1]_+$。\n为了找到最小值点 $x_\\mu$，我们分析 $F_\\mu(x)$ 的次梯度：$\\partial F_\\mu(x) = x + \\mu \\partial[x+1]_+$。\n-   如果 $x  -1$，$\\partial F_\\mu(x) = \\{x+\\mu\\}$。令 $x+\\mu=0$ 得到 $x = -\\mu$。如果 $-\\mu  -1$，即 $\\mu  1$，这是一个有效的候选解。\n-   如果 $x  -1$，$\\partial F_\\mu(x) = \\{x\\}$。令 $x=0$ 与 $x  -1$ 矛盾。此处无解。\n-   如果 $x = -1$，$\\partial F_\\mu(-1) = -1 + \\mu[0,1] = [-1, -1+\\mu]$。如果 $-1+\\mu \\ge 0$，即 $\\mu \\ge 1$，次梯度包含 $0$。\n所以，当 $\\mu \\in (0, 1)$ 时，最小值点是 $x_\\mu = -\\mu$；当 $\\mu \\ge 1$ 时，最小值点是 $x_\\mu=-1$。\n对于任何 $\\mu \\in (0, 1)$，最小值点是 $x_\\mu = -\\mu$。由于 $-\\mu  -1$，这个最小值点对于硬约束 $x \\le -1$ 是不可行的。例如，如果 $\\mu=0.5$，$x_{0.5} = -0.5$，这违反了 $x \\le -1$。\n\n**对 C 的结论：不正确。**\n\n**D. 如果可行集为空，令 $h(x) := \\sum_{i=1}^m [a_i^\\top x - b_i]_+$。当 $\\mu \\to +\\infty$ 时，$\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ 的最小值点 $x_\\mu$ 的任意聚点都是以下字典序优化的解：首先在 $x \\in \\mathbb{R}^n$ 上最小化 $h(x)$，然后在 $h$ 的所有最小值点中最小化 $f(x)$。**\n\n这描述了当约束不相容时，正则化问题的行为。\n-   令 $h^\\star = \\inf_{x \\in \\mathbb{R}^n} h(x)$。由于可行集为空，$h(x)  0$ 对所有 $x$ 成立，所以 $h^\\star  0$（假设下确界可以达到）。令 $S_h = \\{x \\in \\mathbb{R}^n \\mid h(x) = h^\\star\\}$ 为 $h(x)$ 的最小值点集合。由于 $h(x)$ 是凸的，$S_h$ 是一个凸集。假设 $S_h$ 非空，我们可以定义字典序解。\n-   令 $x^\\star_{lex}$ 为 $\\min_{x \\in S_h} f(x)$ 的唯一解。该解存在且唯一，因为 $f(x)$ 是强凸的，且 $S_h$ 是一个闭凸集。\n-   令 $x_\\mu$ 为 $F_\\mu(x) = f(x) + \\mu h(x)$ 的最小值点。对于任意 $x \\in S_h$，我们有 $f(x_\\mu) + \\mu h(x_\\mu) \\le f(x) + \\mu h(x) = f(x) + \\mu h^\\star$。特别地，使用 $x = x^\\star_{lex}$：$f(x_\\mu) + \\mu h(x_\\mu) \\le f(x^\\star_{lex}) + \\mu h^\\star$。\n-   这意味着 $f(x_\\mu) \\le f(x^\\star_{lex}) + \\mu(h^\\star - h(x_\\mu))$。由于 $h(x_\\mu) \\ge h^\\star$，第二项是非正的，所以 $f(x_\\mu) \\le f(x^\\star_{lex})$。这表明序列 $\\{x_\\mu\\}$ 是有界的。\n-   从同一个不等式，我们有 $h(x_\\mu) - h^\\star \\le \\frac{f(x^\\star_{lex}) - f(x_\\mu)}{\\mu} \\le \\frac{f(x^\\star_{lex})}{\\mu}$。当 $\\mu \\to +\\infty$ 时，这意味着 $h(x_\\mu) \\to h^\\star$。\n-   令 $\\bar{x}$ 为 $\\{x_\\mu\\}$ 的一个聚点。根据 $h(x)$ 的连续性，$h(\\bar{x}) = h^\\star$。所以，任何聚点 $\\bar{x}$ 都是 $h(x)$ 的一个最小值点，即 $\\bar{x} \\in S_h$。\n-   沿着收敛到 $\\bar{x}$ 的子序列，对不等式 $f(x_\\mu) \\le f(x^\\star_{lex})$ 取上极限，我们得到 $f(\\bar{x}) \\le f(x^\\star_{lex})$。\n-   但根据 $x^\\star_{lex}$ 作为 $f(x)$ 在 $S_h$ 上的最小值点的定义，并且由于 $\\bar{x} \\in S_h$，我们必然有 $f(\\bar{x}) \\ge f(x^\\star_{lex})$。\n-   因此，$f(\\bar{x}) = f(x^\\star_{lex})$。由于 $x^\\star_{lex}$ 是 $f$ 在 $S_h$ 上的唯一最小值点，我们必然有 $\\bar{x} = x^\\star_{lex}$。\n-   由于有界序列 $\\{x_\\mu\\}$ 只有一个聚点 $x^\\star_{lex}$，整个序列都收敛于它。该陈述是正确的。\n\n**对 D 的结论：正确。**\n\n**E. 在一维情况 $n=1$ 下，对于两个不相容的半空间 $x \\le 0$ 和 $x \\ge 1$，对每个 $\\mu0$，$\\mathcal{P}_{\\mathrm{soft}}(\\mu)$ 的唯一最小值点是 $x=0$。**\n\n约束是 $x \\le 0$ 和 $-x \\le -1$。可行集为空。\n软目标函数是 $F_\\mu(x) = \\tfrac{1}{2}x^2 + \\mu([x-0]_+ + [-x-(-1)]_+) = \\tfrac{1}{2}x^2 + \\mu([x]_+ + [1-x]_+)$。\n我们来分析惩罚项 $h(x) = [x]_+ + [1-x]_+$：\n-   如果 $x  0$，$h(x) = 0 + (1-x) = 1-x$。\n-   如果 $0 \\le x \\le 1$，$h(x) = x + (1-x) = 1$。\n-   如果 $x  1$，$h(x) = x + 0 = x$。\n所以，我们要最小化：\n$$ F_\\mu(x) = \\begin{cases} \\tfrac{1}{2}x^2 + \\mu(1-x)  \\text{if } x  0 \\\\ \\tfrac{1}{2}x^2 + \\mu  \\text{if } 0 \\le x \\le 1 \\\\ \\tfrac{1}{2}x^2 + \\mu x  \\text{if } x  1 \\end{cases} $$\n我们来求在开区间上的导数：\n-   对于 $x  0$，$F'_\\mu(x) = x - \\mu$。因为 $x  0$ 且 $\\mu  0$，所以 $F'_\\mu(x)  0$。函数是递减的。\n-   对于 $0  x  1$，$F'_\\mu(x) = x$。因为 $x0$，所以 $F'_\\mu(x)  0$。函数是递增的。\n-   对于 $x  1$，$F'_\\mu(x) = x + \\mu$。因为 $x1$ 且 $\\mu  0$，所以 $F'_\\mu(x)  0$。函数是递增的。\n函数 $F_\\mu(x)$ 在 $x0$ 时递减，在 $x0$ 时递增。全局最小值必定在 $x=0$ 处。\n为了严谨起见，我们检查在 $x=0$ 处的次梯度。\n$\\partial F_\\mu(0) = \\nabla(\\tfrac{1}{2}x^2)|_{x=0} + \\mu \\partial h(x)|_{x=0}$。\n$\\partial h(0) = \\partial [x]_+|_{x=0} + \\partial [1-x]_+|_{x=0} = [0,1] + \\{-1\\} = [-1,0]$。\n所以，$\\partial F_\\mu(0) = \\{0\\} + \\mu[-1,0] = [-\\mu, 0]$。\n要使 $x=0$ 成为最小值点，0 必须在次梯度中：$0 \\in [-\\mu, 0]$。这对所有 $\\mu  0$ 都成立。\n因此，对于任何 $\\mu  0$，$x=0$ 确实是唯一的最小值点。\n\n**对 E 的结论：正确。**",
            "answer": "$$\\boxed{ABDE}$$"
        },
        {
            "introduction": "理论知识最终需要通过计算实践来落地。这个练习  提出了一个非常实际的问题：如何在一个由多个半空间交集定义的可行域（一个多面体）中，找到距离给定点最近的点？你将实现Dykstra算法，这是一个优雅的迭代方法，它将这个复杂的投影问题分解为一系列在单个半空间上的简单投影，清晰地展示了化繁为简的算法思想。",
            "id": "3137818",
            "problem": "考虑一个有限维实向量空间中的欧几里得投影问题。设 $n \\in \\mathbb{N}$，且在 $\\mathbb{R}^n$ 中有一族 $m \\in \\mathbb{N}$ 个闭半空间，由线性不等式 $C_i = \\{x \\in \\mathbb{R}^n : a_i^\\top x \\le b_i\\}$（$i \\in \\{1,\\dots,m\\}$）给出，其中 $a_i \\in \\mathbb{R}^n$ 且 $b_i \\in \\mathbb{R}$。设 $x_0 \\in \\mathbb{R}^n$ 为一个点，我们寻求其在交集 $C = \\bigcap_{i=1}^m C_i$ 上的欧几里得投影，即一个点 $x^\\star \\in C$，它最小化目标函数 $x \\mapsto \\tfrac{1}{2}\\|x - x_0\\|_2^2$。\n\n从 $\\mathbb{R}^n$ 中点到非空闭凸集上欧几里得投影的基本定义出发，并利用闭半空间的交集是闭凸集这一熟知事实，推导并实现一个迭代方法。该方法使用对单个半空间的循环投影和逐集的校正项，以收敛到 $x_0$ 在 $C$ 上的投影。该方法的设计必须使得校正项沿着半空间的法向量 $a_i$ 累积，从而提供一种直接从数值上观察这种累积的方式。投影到单个闭半空间的问题必须通过求解相应约束于一个线性不等式的最小距离问题来处理。当完整一轮循环后迭代点的变化量低于预设容差或达到最大循环次数时，该迭代方法必须终止。\n\n您的程序必须实现此投影过程，并对下面测试套件中的每个测试用例，生成一个包含以下内容的实数列表作为其结果：\n- 首先，按顺序排列的投影点 $x^\\star$ 的坐标。\n- 接着，对于给定的每个半空间，按顺序给出一个标量，该标量量化了最终校正项沿相应法向量的累积。对于第 $i$ 个半空间，使用标量 $s_i = \\dfrac{a_i^\\top y_i}{\\|a_i\\|_2^2}$，其中 $y_i \\in \\mathbb{R}^n$ 表示迭代方法在终止时为集合 $i$ 存储的最终校正向量。该标量等于对该集合进行最后一次校正时沿法向量方向 $a_i$ 所取的有符号步长，是校正如何沿 $a_i$ 累积的简明度量。\n\n使用 $10^{-10}$ 的停止容差和最多 $10000$ 次遍历所有半空间的循环。此问题不涉及任何物理单位。所有角度（如果隐含地出现在向量运算中）均通过标准内积以弧度处理；不需要明确的角度单位。所有答案必须表示为实数。\n\n测试套件：\n- 案例1（二维，两个半空间构成非正象限）：$n = 2$, $m = 2$, $a_1 = [1, 0]^\\top$, $b_1 = 0$, $a_2 = [0, 1]^\\top$, $b_2 = 0$, $x_0 = [1, 2]^\\top$。\n- 案例2（二维，三个半空间构成一个带底的楔形）：$n = 2$, $m = 3$, $a_1 = [1, 1]^\\top$, $b_1 = 1$, $a_2 = [-1, 2]^\\top$, $b_2 = 2$, $a_3 = [0, -1]^\\top$, $b_3 = 0$, $x_0 = [2, 3]^\\top$。\n- 案例3（三维，轴对齐的盒子）：$n = 3$, $m = 6$, $a_1 = [1, 0, 0]^\\top$, $b_1 = 1$, $a_2 = [-1, 0, 0]^\\top$, $b_2 = 1$, $a_3 = [0, 1, 0]^\\top$, $b_3 = 1$, $a_4 = [0, -1, 0]^\\top$, $b_4 = 1$, $a_5 = [0, 0, 1]^\\top$, $b_5 = 1$, $a_6 = [0, 0, -1]^\\top$, $b_6 = 1$, $x_0 = [2, -3, 0.5]^\\top$。\n- 案例4（二维，四个半空间交于一点）：$n = 2$, $m = 4$, $a_1 = [1, 0]^\\top$, $b_1 = 1$, $a_2 = [-1, 0]^\\top$, $b_2 = -1$, $a_3 = [0, 1]^\\top$, $b_3 = -1$, $a_4 = [0, -1]^\\top$, $b_4 = 1$, $x_0 = [-2, 4]^\\top$。\n- 案例5（二维，位于案例2楔形内部的点）：$n = 2$, $m = 3$, $a_1 = [1, 1]^\\top$, $b_1 = 1$, $a_2 = [-1, 2]^\\top$, $b_2 = 2$, $a_3 = [0, -1]^\\top$, $b_3 = 0$, $x_0 = [0.2, 0.3]^\\top$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。此列表的每个元素必须是单个测试用例的 Python 列表表示，由投影点 $x^\\star$ 的坐标和按半空间顺序排列的标量 $s_i$ 串联而成。例如，三个案例的输出应类似于 $[ [x^\\star\\_1\\_1, x^\\star\\_1\\_2, s\\_{1,1}, s\\_{1,2}, \\dots], [x^\\star\\_2\\_1, x^\\star\\_2\\_2, s\\_{2,1}, s\\_{2,2}, \\dots], [x^\\star\\_3\\_1, x^\\star\\_3\\_2, x^\\star\\_3\\_3, s\\_{3,1}, \\dots ] ]$，不带任何附加文本。",
            "solution": "用户提供的问题是有效的。它在科学上基于凸优化领域，特别是欧几里得投影到凸集上的问题。该问题是良构的，因为点在非空闭凸集上的投影是唯一的。问题陈述客观、完整且无矛盾。我们可以着手求解。\n\n该问题要求解点 $x_0 \\in \\mathbb{R}^n$ 在闭凸集 $C$ 上的欧几里得投影，其中 $C$ 是 $m$ 个闭半空间 $C_i = \\{x \\in \\mathbb{R}^n : a_i^\\top x \\le b_i\\}$ 的交集。该投影记为 $x^\\star$，是以下优化问题的唯一解：\n$$\nx^\\star = \\arg\\min_{x \\in C} \\frac{1}{2} \\|x - x_0\\|_2^2\n$$\n该问题要求实现一种称为Dykstra投影算法的特定迭代方法。该算法通过对单个集合进行循环投影，并引入一组辅助校正向量来解释迭代位移，从而找到点在凸集交集上的投影。\n\n### 投影到单个半空间\n\nDykstra算法中的基本操作是到单个半空间的投影 $P_{C_i}(y)$，它找到 $C_i$ 中距离给定点 $y \\in \\mathbb{R}^n$ 最近的点。这是一个凸优化子问题：\n$$\n\\text{minimize} \\quad \\frac{1}{2}\\|x - y\\|_2^2 \\quad \\text{subject to} \\quad a_i^\\top x \\le b_i\n$$\n其解可以使用Karush-Kuhn-Tucker (KKT) 条件推导得出。拉格朗日函数为 $L(x, \\mu) = \\frac{1}{2}\\|x - y\\|_2^2 + \\mu(a_i^\\top x - b_i)$，其中 $\\mu \\ge 0$。\nKKT条件为：\n1.  **驻点条件：** $\\nabla_x L = x - y + \\mu a_i = 0 \\implies x = y - \\mu a_i$。\n2.  **原始可行性：** $a_i^\\top x \\le b_i$。\n3.  **对偶可行性：** $\\mu \\ge 0$。\n4.  **互补松弛性：** $\\mu(a_i^\\top x - b_i) = 0$。\n\n由此产生两种情况：\n- 如果 $y$ 已在半空间 $C_i$ 内，则 $a_i^\\top y \\le b_i$。我们可以设 $\\mu = 0$。由驻点条件得 $x = y$。所有KKT条件均满足，因此 $P_{C_i}(y) = y$。\n- 如果 $y$ 在 $C_i$ 之外，则 $a_i^\\top y  b_i$。这要求 $\\mu  0$。根据互补松弛性，约束必须是激活的：$a_i^\\top x = b_i$。将 $x = y - \\mu a_i$ 代入此等式，得到 $a_i^\\top (y - \\mu a_i) = b_i$，解出 $\\mu = \\frac{a_i^\\top y - b_i}{\\|a_i\\|_2^2}$。因为 $a_i^\\top y  b_i$，我们有 $\\mu  0$ 符合要求。投影点为 $x = y - \\frac{a_i^\\top y - b_i}{\\|a_i\\|_2^2} a_i$。\n\n使用正部函数 $(z)_+ = \\max(0, z)$ 结合两种情况，投影算子可表示为：\n$$\nP_{C_i}(y) = y - \\frac{(a_i^\\top y - b_i)_+}{\\|a_i\\|_2^2} a_i\n$$\n此公式假设 $a_i \\neq 0$，对于任何明确定义的半空间，这都是成立的。\n\n### Dykstra投影算法\n\nDykstra算法是一种迭代过程，用于寻找点在交集 $C=\\bigcap_{i=1}^m C_i$ 上的投影。它维护一个主迭代点（收敛到解 $x^\\star$）和一组 $m$ 个辅助校正向量（每个半空间一个）。这些向量记为 $y_i$，'存储'了先前投影步骤中垂直于相应半空间边界的位移分量。问题陈述准确地描述了这些校正项的作用。\n\n算法过程如下：\n\n1.  **初始化：**\n    -   设置初始迭代点 $x^{(0)} = x_0$。\n    -   为 $i=1, \\dots, m$ 初始化 $m$ 个校正向量 $y_i^{(0)} = \\mathbf{0} \\in \\mathbb{R}^n$。\n\n2.  **迭代：** 对每个循环 $k = 0, 1, 2, \\dots$ 直到达到最大循环次数：\n    a.  存储循环开始时的迭代点：$x_{\\text{start of cycle}} = x^{(k)}$。\n    b.  初始化循环的中间点：$z_0 = x^{(k)}$。\n    c.  对所有半空间 $i = 1, \\dots, m$ 执行一轮完整的投影循环：\n        i.   定义要投影到 $C_i$ 上的点：$v_i = z_{i-1} + y_i^{(k)}$。\n        ii.  投影该点：$z_i = P_{C_i}(v_i)$。\n        iii. 更新用于下一轮循环的校正向量：$y_i^{(k+1)} = v_i - z_i$。\n    d.  更新用于下一轮循环的主迭代点：$x^{(k+1)} = z_m$。\n    e.  **终止检查：** 如果 $\\|x^{(k+1)} - x_{\\text{start of cycle}}\\|_2  \\text{tolerance}$，则算法已收敛。\n\n校正向量的更新规则 $y_i^{(k+1)} = v_i - P_{C_i}(v_i)$ 与投影算子的公式相结合，表明 $y_i^{(k+1)}$ 始终是法向量 $a_i$ 的一个非负标量倍数：\n$$\ny_i^{(k+1)} = \\frac{(a_i^\\top v_i - b_i)_+}{\\|a_i\\|_2^2} a_i\n$$\n这证实了问题描述中所述的“校正项沿着法向量 $a_i$ 累积”。\n\n### 最终输出计算\n\n算法终止时，会得到投影点 $x^\\star \\approx x^{(k+1)}$ 和最终的校正向量集 $\\{y_i^{\\text{final}}\\}$。问题要求为每个半空间计算一个标量 $s_i$，定义为：\n$$\ns_i = \\frac{a_i^\\top y_i^{\\text{final}}}{\\|a_i\\|_2^2}\n$$\n由于 $y_i^{\\text{final}}$ 必为 $\\lambda_i a_i$ 的形式（其中 $\\lambda_i \\ge 0$ 为某个标量），我们可以将其代入 $s_i$ 的公式中：\n$$\ns_i = \\frac{a_i^\\top (\\lambda_i a_i)}{\\|a_i\\|_2^2} = \\frac{\\lambda_i (a_i^\\top a_i)}{\\|a_i\\|_2^2} = \\lambda_i\n$$\n因此，$s_i$ 正是将校正向量 $y_i^{\\text{final}}$ 与法向量 $a_i$ 关联起来的最终标量乘数。该值表示在 $a_i$ 方向上累积的校正幅度。\n\n实现将遵循此推导的算法，计算 $x^\\star$ 的最终坐标，然后使用最终的校正向量 $\\{y_i^{\\text{final}}\\}$ 来计算标量 $\\{s_i\\}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Euclidean projection problem for a set of test cases using Dykstra's algorithm.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: 2D, nonpositive quadrant\n        {\n            \"x0\": np.array([1, 2], dtype=float),\n            \"a\": [np.array([1, 0], dtype=float), np.array([0, 1], dtype=float)],\n            \"b\": [0.0, 0.0]\n        },\n        # Case 2: 2D, wedge with floor\n        {\n            \"x0\": np.array([2, 3], dtype=float),\n            \"a\": [np.array([1, 1], dtype=float), np.array([-1, 2], dtype=float), np.array([0, -1], dtype=float)],\n            \"b\": [1.0, 2.0, 0.0]\n        },\n        # Case 3: 3D, axis-aligned box\n        {\n            \"x0\": np.array([2, -3, 0.5], dtype=float),\n            \"a\": [\n                np.array([1, 0, 0], dtype=float), np.array([-1, 0, 0], dtype=float),\n                np.array([0, 1, 0], dtype=float), np.array([0, -1, 0], dtype=float),\n                np.array([0, 0, 1], dtype=float), np.array([0, 0, -1], dtype=float)\n            ],\n            \"b\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n        },\n        # Case 4: 2D, point intersection\n        {\n            \"x0\": np.array([-2, 4], dtype=float),\n            \"a\": [\n                np.array([1, 0], dtype=float), np.array([-1, 0], dtype=float),\n                np.array([0, 1], dtype=float), np.array([0, -1], dtype=float)\n            ],\n            \"b\": [1.0, -1.0, -1.0, 1.0]\n        },\n        # Case 5: 2D, interior point\n        {\n            \"x0\": np.array([0.2, 0.3], dtype=float),\n            \"a\": [np.array([1, 1], dtype=float), np.array([-1, 2], dtype=float), np.array([0, -1], dtype=float)],\n            \"b\": [1.0, 2.0, 0.0]\n        }\n    ]\n\n    # Algorithm parameters\n    TOLERANCE = 1e-10\n    MAX_CYCLES = 10000\n\n    results = []\n\n    def project_halfspace(y_proj, a_i, b_i):\n        \"\"\"\n        Projects a point y_proj onto the halfspace a_i^T x = b_i.\n        \"\"\"\n        a_norm_sq = np.dot(a_i, a_i)\n        if a_norm_sq == 0:\n            return y_proj\n\n        violation = np.dot(a_i, y_proj) - b_i\n        \n        if violation = 0:\n            return y_proj\n        \n        lambda_val = violation / a_norm_sq\n        return y_proj - lambda_val * a_i\n\n    for case in test_cases:\n        x0 = case[\"x0\"]\n        a_vectors = case[\"a\"]\n        b_scalars = case[\"b\"]\n        m = len(a_vectors)\n        n = len(x0)\n\n        # Initialization\n        x_k = np.copy(x0)\n        y_corrections = [np.zeros(n, dtype=float) for _ in range(m)]\n\n        for k in range(MAX_CYCLES):\n            x_start_of_cycle = np.copy(x_k)\n            \n            x_intermediate = x_k\n            for i in range(m):\n                a_i = a_vectors[i]\n                b_i = b_scalars[i]\n                \n                point_to_project = x_intermediate + y_corrections[i]\n                x_projected = project_halfspace(point_to_project, a_i, b_i)\n                \n                # Update correction vector for this halfspace\n                y_corrections[i] = point_to_project - x_projected\n                \n                # Update the intermediate point for the next projection in the cycle\n                x_intermediate = x_projected\n            \n            x_k = x_intermediate # Final point after one full cycle\n            \n            # Termination check\n            change = np.linalg.norm(x_k - x_start_of_cycle)\n            if change  TOLERANCE:\n                break\n        \n        x_star = x_k\n        \n        # Calculate final scalars s_i\n        s_values = []\n        for i in range(m):\n            a_i = a_vectors[i]\n            y_i = y_corrections[i]\n            a_norm_sq = np.dot(a_i, a_i)\n            if a_norm_sq > 0:\n                s_i = np.dot(a_i, y_i) / a_norm_sq\n            else:\n                s_i = 0.0 # Should not happen with valid halfspaces\n            s_values.append(s_i)\n            \n        # Combine results for this case\n        case_result = list(x_star) + s_values\n        results.append(case_result)\n\n    # Format the final output string\n    # str() on a list gives a nice '[...]' representation.\n    # We join these string representations with commas.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}