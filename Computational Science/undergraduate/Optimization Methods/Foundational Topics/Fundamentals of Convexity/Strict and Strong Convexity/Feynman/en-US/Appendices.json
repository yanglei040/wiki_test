{
    "hands_on_practices": [
        {
            "introduction": "Strong convexity is a powerful property that ensures a function is not just convex, but \"sufficiently curved\" in all directions, which has significant benefits for optimization. This exercise provides a clear geometric intuition for this concept by examining a simple quadratic function that is convex but fails to be strongly convex due to a \"flat direction\" . You will then see how restricting the domain with a simple constraint can eliminate this flat direction, restoring strong convexity and illustrating how problem structure can influence its geometric properties.",
            "id": "3188365",
            "problem": "Consider the function $f:\\mathbb{R}^2 \\to \\mathbb{R}$ defined by $f(x,y)=x^2$. Work with the Euclidean norm $\\|\\cdot\\|_2$. Use only core definitions of convexity and strong convexity, and basic multivariable calculus, as the fundamental base of your reasoning.\n\n(a) Using the definition of convexity and the definition of strong convexity on $\\mathbb{R}^2$, justify that $f$ is convex but not strongly convex on $\\mathbb{R}^2$.\n\n(b) Now constrain the domain to the affine line\n$$\n\\mathcal{C}_{\\alpha,\\beta}=\\{(x,y)\\in\\mathbb{R}^2: y=\\alpha x+\\beta\\},\n$$\nwhere $\\alpha\\in\\mathbb{R}$ and $\\beta\\in\\mathbb{R}$ are fixed. Using the definition of strong convexity on a convex set with respect to the Euclidean norm, determine the largest constant $m(\\alpha)$ such that for all $z_1,z_2\\in \\mathcal{C}_{\\alpha,\\beta}$,\n$$\nf(z_2)\\ge f(z_1)+\\nabla f(z_1)^{\\mathsf{T}}(z_2-z_1)+\\frac{m(\\alpha)}{2}\\|z_2-z_1\\|_2^2.\n$$\nYour final answer must be a single closed-form expression for $m(\\alpha)$ that depends only on $\\alpha$ (and not on $\\beta$). Do not provide an inequality or an equation as the final answer. No rounding is required.",
            "solution": "The problem is analyzed in two parts as specified.\n\nPart (a): Analysis of convexity and strong convexity of $f(x,y) = x^2$ on $\\mathbb{R}^2$.\n\nFirst, we assess the convexity of $f$. A twice-differentiable function is convex on a convex set if and only if its Hessian matrix is positive semidefinite over that set. The domain $\\mathbb{R}^2$ is a convex set. The function is $f(x,y) = x^2$. Its gradient vector is:\n$$\n\\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 2x \\\\ 0 \\end{pmatrix}\n$$\nThe Hessian matrix is the matrix of second-order partial derivatives:\n$$\n\\nabla^2 f(x,y) = H = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2}  \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x}  \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  0 \\end{pmatrix}\n$$\nThe Hessian matrix is constant for all $(x,y) \\in \\mathbb{R}^2$. To check for positive semidefiniteness, we examine the quadratic form $v^{\\mathsf{T}}Hv$ for an arbitrary vector $v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} \\in \\mathbb{R}^2$.\n$$\nv^{\\mathsf{T}}Hv = \\begin{pmatrix} v_1  v_2 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 2v_1  0 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = 2v_1^2\n$$\nSince $v_1^2 \\ge 0$, we have $v^{\\mathsf{T}}Hv = 2v_1^2 \\ge 0$ for all $v \\in \\mathbb{R}^2$. Therefore, the Hessian matrix is positive semidefinite, and the function $f(x,y)=x^2$ is convex on $\\mathbb{R}^2$.\n\nNext, we assess the strong convexity of $f$. A differentiable function $f$ is strongly convex on $\\mathbb{R}^2$ if there exists a constant $m  0$ such that for all $z_1, z_2 \\in \\mathbb{R}^2$, the following inequality holds:\n$$\nf(z_2) \\ge f(z_1) + \\nabla f(z_1)^{\\mathsf{T}}(z_2 - z_1) + \\frac{m}{2} \\|z_2 - z_1\\|_2^2\n$$\nLet $z_1 = (x_1, y_1)$ and $z_2 = (x_2, y_2)$. Substituting the expressions for $f$ and its gradient:\n$$\nx_2^2 \\ge x_1^2 + \\begin{pmatrix} 2x_1  0 \\end{pmatrix} \\begin{pmatrix} x_2-x_1 \\\\ y_2-y_1 \\end{pmatrix} + \\frac{m}{2} \\left( (x_2-x_1)^2 + (y_2-y_1)^2 \\right)\n$$\n$$\nx_2^2 \\ge x_1^2 + 2x_1(x_2-x_1) + \\frac{m}{2} \\left( \\|z_2 - z_1\\|_2^2 \\right)\n$$\n$$\nx_2^2 \\ge x_1^2 + 2x_1x_2 - 2x_1^2 + \\frac{m}{2} \\|z_2 - z_1\\|_2^2\n$$\n$$\nx_2^2 - 2x_1x_2 + x_1^2 \\ge \\frac{m}{2} \\|z_2 - z_1\\|_2^2\n$$\n$$\n(x_2-x_1)^2 \\ge \\frac{m}{2} \\left( (x_2-x_1)^2 + (y_2-y_1)^2 \\right)\n$$\nFor $f$ to be strongly convex, this inequality must hold for some constant $m0$ and for all pairs of points $z_1, z_2 \\in \\mathbb{R}^2$. To show it is not strongly convex, we need only find a pair of points for which the inequality is violated for any $m0$. Let us choose points with the same $x$-coordinate, for instance, $x_1=x_2=0$. Let $y_1=0$ and $y_2=1$.\nSo, $z_1=(0,0)$ and $z_2=(0,1)$.\nSubstituting these values into the inequality:\n$$\n(0-0)^2 \\ge \\frac{m}{2} \\left( (0-0)^2 + (1-0)^2 \\right)\n$$\n$$\n0 \\ge \\frac{m}{2} \\left( 0 + 1 \\right)\n$$\n$$\n0 \\ge \\frac{m}{2}\n$$\nThis inequality is false for any $m0$. Since the condition for strong convexity must hold for all points in the domain, and we have found a case where it fails for any positive $m$, we conclude that $f(x,y)=x^2$ is not strongly convex on $\\mathbb{R}^2$.\n\nPart (b): Strong convexity of $f$ on the affine line $\\mathcal{C}_{\\alpha,\\beta}$.\n\nWe are tasked with finding the largest constant $m(\\alpha)$ such that $f$ is strongly convex with this parameter when its domain is restricted to the line $\\mathcal{C}_{\\alpha,\\beta}=\\{(x,y)\\in\\mathbb{R}^2: y=\\alpha x+\\beta\\}$.\nLet $z_1 = (x_1, y_1)$ and $z_2 = (x_2, y_2)$ be two distinct points on the line $\\mathcal{C}_{\\alpha,\\beta}$. This means $y_1 = \\alpha x_1 + \\beta$ and $y_2 = \\alpha x_2 + \\beta$.\nWe use the first-order inequality for strong convexity:\n$$\nf(z_2) \\ge f(z_1) + \\nabla f(z_1)^{\\mathsf{T}}(z_2 - z_1) + \\frac{m(\\alpha)}{2} \\|z_2 - z_1\\|_2^2\n$$\nWe need to find the largest $m(\\alpha)$ for which this inequality holds for all $z_1, z_2 \\in \\mathcal{C}_{\\alpha,\\beta}$.\nLet us express each term as a function of $x_1$ and $x_2$.\nThe left-hand side is $f(z_2) = x_2^2$.\nThe terms on the right-hand side are:\n$f(z_1) = x_1^2$.\n$\\nabla f(z_1) = \\begin{pmatrix} 2x_1 \\\\ 0 \\end{pmatrix}$.\nThe vector difference $z_2-z_1$ is:\n$$\nz_2 - z_1 = \\begin{pmatrix} x_2 - x_1 \\\\ y_2 - y_1 \\end{pmatrix} = \\begin{pmatrix} x_2 - x_1 \\\\ (\\alpha x_2 + \\beta) - (\\alpha x_1 + \\beta) \\end{pmatrix} = \\begin{pmatrix} x_2 - x_1 \\\\ \\alpha(x_2 - x_1) \\end{pmatrix}\n$$\nThe inner product term is:\n$$\n\\nabla f(z_1)^{\\mathsf{T}}(z_2 - z_1) = \\begin{pmatrix} 2x_1  0 \\end{pmatrix} \\begin{pmatrix} x_2 - x_1 \\\\ \\alpha(x_2 - x_1) \\end{pmatrix} = 2x_1(x_2 - x_1)\n$$\nThe squared Euclidean norm is:\n$$\n\\|z_2 - z_1\\|_2^2 = (x_2-x_1)^2 + (y_2-y_1)^2 = (x_2-x_1)^2 + (\\alpha(x_2-x_1))^2 = (1+\\alpha^2)(x_2-x_1)^2\n$$\nSubstituting these expressions back into the strong convexity inequality:\n$$\nx_2^2 \\ge x_1^2 + 2x_1(x_2 - x_1) + \\frac{m(\\alpha)}{2} (1+\\alpha^2)(x_2-x_1)^2\n$$\nRearranging the terms to isolate the quadratic term in $(x_2-x_1)$:\n$$\nx_2^2 - (x_1^2 + 2x_1x_2 - 2x_1^2) \\ge \\frac{m(\\alpha)}{2} (1+\\alpha^2)(x_2-x_1)^2\n$$\n$$\nx_2^2 - 2x_1x_2 + x_1^2 \\ge \\frac{m(\\alpha)}{2} (1+\\alpha^2)(x_2-x_1)^2\n$$\n$$\n(x_2-x_1)^2 \\ge \\frac{m(\\alpha)}{2} (1+\\alpha^2)(x_2-x_1)^2\n$$\nThis inequality must hold for all $x_1, x_2 \\in \\mathbb{R}$. If $x_1=x_2$, the inequality becomes $0 \\ge 0$, which is true. If $x_1 \\ne x_2$, we can divide both sides by the positive quantity $(x_2-x_1)^2$:\n$$\n1 \\ge \\frac{m(\\alpha)}{2} (1+\\alpha^2)\n$$\nTo find the largest possible value for $m(\\alpha)$ that satisfies this condition for all choices of points on the line, we solve for $m(\\alpha)$:\n$$\nm(\\alpha) \\le \\frac{2}{1+\\alpha^2}\n$$\nThe inequality must hold for this $m(\\alpha)$. The largest value that $m(\\alpha)$ can take is therefore the upper bound of this inequality.\nThe largest constant $m(\\alpha)$ is thus $\\frac{2}{1+\\alpha^2}$. This expression depends only on $\\alpha$, as required. Note that $\\beta$ does not appear, as expected, because the function $f$ is independent of $y$, and $\\beta$ only shifts the line vertically.",
            "answer": "$$\\boxed{\\frac{2}{1+\\alpha^2}}$$"
        },
        {
            "introduction": "A function's global properties are often dictated by its \"worst-case\" local behavior. This principle is particularly true for strong convexity, which must hold uniformly across the entire domain. In this practice, you will analyze a piecewise function constructed from different segments, each with its own curvature . By calculating the second derivative for each piece, you will determine the global strong convexity modulus as the minimum of these local values, providing a concrete example of how the \"flattest\" part of a function limits its overall convexity.",
            "id": "3188413",
            "problem": "Consider the function $f : \\mathbb{R} \\to \\mathbb{R}$ defined piecewise by\n$$\nf(x) =\n\\begin{cases}\nx^{2},  x \\leq -1, \\\\\n-2x - 1,  -1  x  1, \\\\\n\\frac{1}{2}x^{2} - 3x - \\frac{1}{2},  x \\geq 1.\n\\end{cases}\n$$\nThis construction ensures that $f$ is continuous and once differentiable at the junction points $x=-1$ and $x=1$.\n\nStarting from the fundamental definitions of convexity and strong convexity on $\\mathbb{R}$, and using the well-tested characterization that a twice differentiable function $f$ is strongly convex with modulus $m0$ if and only if its second derivative satisfies $f''(x) \\geq m$ for all $x \\in \\mathbb{R}$, justify why the global strong convexity modulus of $f$ can be obtained as the infimum over $\\mathbb{R}$ of its local second derivatives (the one-dimensional Hessian). Compute this global strong convexity modulus $m^{\\star}$ for the given $f$, and explain whether the result demonstrates that the modulus may be zero. Your final answer must be the single real number $m^{\\star}$; no rounding is required.",
            "solution": "The problem asks for the computation of the global strong convexity modulus $m^{\\star}$ for a given piecewise function $f(x)$ and an interpretation of the result. The process begins with the fundamental definitions of convexity and strong convexity and justifies the method used for this specific function, which is continuously differentiable but not twice differentiable everywhere.\n\nA differentiable function $f: \\mathbb{R} \\to \\mathbb{R}$ is defined as convex if its graph lies above all of its tangents. This is formally stated as:\n$$f(y) \\geq f(x) + f'(x)(y-x) \\quad \\forall x, y \\in \\mathbb{R}$$\nA function $f$ is said to be strongly convex with modulus $m > 0$ if the function $g(x) = f(x) - \\frac{m}{2}x^2$ is convex. Applying the definition of convexity to $g(x)$, we find that $f$ is $m$-strongly convex if and only if:\n$$f(y) - \\frac{m}{2}y^2 \\geq \\left(f(x) - \\frac{m}{2}x^2\\right) + (f'(x) - mx)(y-x) \\quad \\forall x, y \\in \\mathbb{R}$$\nRearranging this inequality gives the standard definition of strong convexity for a differentiable function:\n$$f(y) \\geq f(x) + f'(x)(y-x) + \\frac{m}{2}(y-x)^2 \\quad \\forall x, y \\in \\mathbb{R}$$\nAn equivalent characterization for a differentiable function $g(x)$ to be convex is that its first derivative, $g'(x)$, must be a non-decreasing function. For strong convexity of $f$, this means the derivative of $g(x) = f(x) - \\frac{m}{2}x^2$, which is $g'(x) = f'(x) - mx$, must be non-decreasing.\n\nThe problem statement refers to the characterization for twice-differentiable functions, where $f$ is $m$-strongly convex if and only if $f''(x) \\geq m$ for all $x$. This follows directly from the non-decreasing nature of $g'(x)$: if $g'(x)$ is itself differentiable, it is non-decreasing if and only if its derivative is non-negative, i.e., $(g'(x))' = f''(x) - m \\geq 0$.\n\nThe given function $f(x)$ is continuous and, as verified in the problem statement, once differentiable on all of $\\mathbb{R}$. However, it is only piecewise twice-differentiable. Let's compute the first and second derivatives.\nThe function is:\n$$\nf(x) =\n\\begin{cases}\nx^2,  x \\leq -1 \\\\\n-2x - 1,  -1  x  1 \\\\\n\\frac{1}{2}x^2 - 3x - \\frac{1}{2},  x \\geq 1\n\\end{cases}\n$$\nThe first derivative $f'(x)$ is obtained by differentiating each piece:\n$$\nf'(x) =\n\\begin{cases}\n2x,  x  -1 \\\\\n-2,  -1  x  1 \\\\\nx - 3,  x  1\n\\end{cases}\n$$\nAt the junction points, the left and right derivatives are:\nAt $x=-1$: $\\lim_{x \\to -1^-} f'(x) = 2(-1) = -2$ and $\\lim_{x \\to -1^+} f'(x) = -2$.\nAt $x=1$: $\\lim_{x \\to 1^-} f'(x) = -2$ and $\\lim_{x \\to 1^+} f'(x) = 1 - 3 = -2$.\nSince the limits match at the junctions, $f'(x)$ is continuous and defined for all $x \\in \\mathbb{R}$, with $f'(-1) = -2$ and $f'(1) = -2$.\nThe second derivative $f''(x)$ exists for all $x \\in \\mathbb{R}$ except at $x=-1$ and $x=1$:\n$$\nf''(x) =\n\\begin{cases}\n2,  x  -1 \\\\\n0,  -1  x  1 \\\\\n1,  x  1\n\\end{cases}\n$$\nFor our function $f$, which is $C^1$ and piecewise $C^2$, the characterization via the second derivative can be adapted. The condition that $g'(x) = f'(x) - mx$ must be non-decreasing still holds. As $g'(x)$ is a continuous and piecewise differentiable function, it is non-decreasing if and only if its derivative is non-negative wherever it exists. The derivative of $g'(x)$ is $g''(x) = f''(x) - m$.\nTherefore, we require $f''(x) - m \\geq 0$, or $f''(x) \\geq m$, for all $x \\in \\mathbb{R} \\setminus \\{-1, 1\\}$.\n\nThe global strong convexity modulus, $m^{\\star}$, is the supremum of all values of $m$ for which $f$ is $m$-strongly convex. To satisfy the condition $f''(x) \\geq m$ for all $x$ where $f''(x)$ is defined, $m$ must be less than or equal to every value that $f''(x)$ takes. This means $m$ must be a lower bound for the set of values of $f''(x)$. The largest such $m$ is the greatest lower bound, or the infimum, of $f''(x)$.\n$$m^{\\star} = \\inf_{x \\in \\mathbb{R} \\setminus \\{-1, 1\\}} f''(x)$$\nWe can now compute this value. The values that $f''(x)$ takes on its domain of definition are $2$, $0$, and $1$. The set of values is $\\{0, 1, 2\\}$.\nThe infimum of this set is:\n$$m^{\\star} = \\inf \\{0, 1, 2\\} = 0$$\nThe global strong convexity modulus for the function $f(x)$ is $m^{\\star}=0$.\n\nBy definition, a function is strongly convex if its strong convexity modulus $m$ is strictly positive ($m0$). Our result, $m^{\\star}=0$, represents the supremum of all $m$ for which $f$ is $m$-strongly convex. This means that for any $m0$, the function $f$ is not $m$-strongly convex. For example, if we test $m=0.5$, the condition $f''(x) \\geq 0.5$ fails on the interval $(-1, 1)$ where $f''(x)=0$. The case $m=0$ corresponds to standard convexity. Thus, the result $m^{\\star}=0$ demonstrates that the function $f$ is convex, but it is not strongly convex. This shows that the modulus, when calculated as the infimum of the second derivative, can indeed be zero, signifying the boundary between strong convexity and mere convexity.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Understanding how convexity is preserved or altered by operations like addition or taking an infimum is crucial in optimization theory. This exercise presents a fascinating and somewhat counterintuitive scenario: the pointwise infimum of a family of strictly convex functions is not always strictly convex itself . After constructing such an example, you will then explore a powerful and common technique in modern optimization and machine learning: adding a simple quadratic term to enforce strong convexity, a process often related to regularization.",
            "id": "3188379",
            "problem": "Consider the family of functions on the real line defined by $f_{t}:\\mathbb{R}\\to\\mathbb{R}$ with $f_{t}(x)=\\left(x-t\\right)^{2}$ for each parameter $t\\in[-1,1]$. Let $h:\\mathbb{R}\\to\\mathbb{R}$ be the pointwise infimum $h(x)=\\inf_{t\\in[-1,1]}f_{t}(x)$.\n\nUsing only the core definitions of convexity, strict convexity, and strong convexity on $\\mathbb{R}$ (Euclidean norm is the absolute value), do the following:\n\n1. Prove that for each fixed $t\\in[-1,1]$, the function $f_{t}$ is strictly convex, and then derive an explicit expression for $h(x)$ by solving the inner minimization over $t$.\n2. Show that $h$ is convex but not strictly convex by directly analyzing its form and the behavior of its slope.\n3. For a fixed parameter $\\mu0$, define $g_{\\mu}:\\mathbb{R}\\to\\mathbb{R}$ by\n$$\ng_{\\mu}(x)=\\inf_{t\\in[-1,1]}\\Big(f_{t}(x)+\\frac{\\mu}{2}\\,x^{2}\\Big).\n$$\nDerive a fully simplified, explicit expression for $g_{\\mu}(x)$, and justify that $g_{\\mu}$ is strongly convex by identifying its strong convexity parameter from first principles.\n\nYour final answer should be the explicit expression for $g_{\\mu}(x)$ as a single closed-form formula (piecewise is allowed). No numerical rounding is required.",
            "solution": "We begin from the core definitions. A function $f:\\mathbb{R}\\to\\mathbb{R}$ is convex if for all $x,y\\in\\mathbb{R}$ and all $\\lambda\\in[0,1]$ one has $f(\\lambda x+(1-\\lambda)y)\\leq \\lambda f(x)+(1-\\lambda)f(y)$. It is strictly convex if the inequality is strict whenever $x\\neq y$ and $\\lambda\\in(0,1)$. A function $f$ is $\\mu$-strongly convex with parameter $\\mu0$ if the function $x\\mapsto f(x)-\\frac{\\mu}{2}|x|^{2}$ is convex. Equivalently, for all $x,y\\in\\mathbb{R}$ and $\\lambda\\in[0,1]$,\n$$\nf(\\lambda x+(1-\\lambda)y)\\leq \\lambda f(x)+(1-\\lambda)f(y)-\\frac{\\mu}{2}\\lambda(1-\\lambda)|x-y|^{2}.\n$$\n\nStep 1: Strict convexity of $f_{t}$ and explicit form of $h$. For a fixed $t\\in[-1,1]$, the function $f_{t}(x)=\\left(x-t\\right)^{2}$ is twice differentiable with second derivative $f_{t}''(x)=2$, which is strictly positive for all $x\\in\\mathbb{R}$. A standard consequence of the second-derivative test for convexity on $\\mathbb{R}$ is that a twice differentiable function with strictly positive second derivative is strictly convex. Hence each $f_{t}$ is strictly convex.\n\nTo compute $h(x)=\\inf_{t\\in[-1,1]}(x-t)^{2}$, fix $x\\in\\mathbb{R}$ and minimize the quadratic in $t$ over the interval $[-1,1]$. The unconstrained minimizer in $t$ is obtained by setting the derivative with respect to $t$ to zero:\n$$\n\\frac{\\partial}{\\partial t}(x-t)^{2}=-2(x-t)=0\\quad\\Longrightarrow\\quad t=x.\n$$\nApplying the interval constraint $t\\in[-1,1]$, the constrained minimizer $t^{\\star}(x)$ is the projection (in the Euclidean metric on $\\mathbb{R}$) of $x$ onto $[-1,1]$:\n$$\nt^{\\star}(x)=\n\\begin{cases}\n-1, x-1,\\\\\nx, -1\\leq x\\leq 1,\\\\\n1, x1.\n\\end{cases}\n$$\nTherefore,\n$$\nh(x)=\\min_{t\\in[-1,1]}(x-t)^{2}=\n\\begin{cases}\n(x+1)^{2}, x-1,\\\\\n0, -1\\leq x\\leq 1,\\\\\n(x-1)^{2}, x1.\n\\end{cases}\n$$\nEquivalently, this can be written in a single compact formula as\n$$\nh(x)=\\big(\\max\\{|x|-1,\\,0\\}\\big)^{2}.\n$$\n\nStep 2: Convexity but not strict convexity of $h$. The explicit formula shows that $h(x)=0$ on the entire interval $[-1,1]$, so $h$ has a flat region containing more than one point. Hence $h$ cannot be strictly convex, because strict convexity would force $h(\\lambda x+(1-\\lambda)y)\\lambda h(x)+(1-\\lambda)h(y)$ for $x\\neq y$, which fails when $x,y\\in[-1,1]$ since all such values equal $0$. To see convexity directly, observe that outside $[-1,1]$, $h$ coincides with convex quadratics $(x+1)^{2}$ on $(-\\infty,-1)$ and $(x-1)^{2}$ on $(1,\\infty)$, and it is $0$ on $[-1,1]$. The derivative of $h$ is\n$$\nh'(x)=\n\\begin{cases}\n2(x+1), x-1,\\\\\n0, -1x1,\\\\\n2(x-1), x1,\n\\end{cases}\n$$\nwith one-sided limits $h'(-1^{-})=0$, $h'(-1^{+})=0$, $h'(1^{-})=0$, and $h'(1^{+})=0$. The derivative is nondecreasing on $\\mathbb{R}$, which is equivalent to convexity on $\\mathbb{R}$. Thus $h$ is convex but not strictly convex.\n\nStep 3: Adding a quadratic and strong convexity of the infimum. For $\\mu0$, define\n$$\ng_{\\mu}(x)=\\inf_{t\\in[-1,1]}\\left((x-t)^{2}+\\frac{\\mu}{2}x^{2}\\right).\n$$\nThe term $\\frac{\\mu}{2}x^{2}$ is independent of $t$, so it can be taken outside the infimum:\n$$\ng_{\\mu}(x)=\\frac{\\mu}{2}x^{2}+\\inf_{t\\in[-1,1]}(x-t)^{2}=\\frac{\\mu}{2}x^{2}+h(x).\n$$\nUsing the expression for $h$, we obtain the explicit form\n$$\ng_{\\mu}(x)=\n\\begin{cases}\n\\frac{\\mu}{2}x^{2}+(x+1)^{2}, x-1,\\\\\n\\frac{\\mu}{2}x^{2}, -1\\leq x\\leq 1,\\\\\n\\frac{\\mu}{2}x^{2}+(x-1)^{2}, x1.\n\\end{cases}\n$$\nEquivalently, in a single compact form,\n$$\ng_{\\mu}(x)=\\frac{\\mu}{2}x^{2}+\\big(\\max\\{|x|-1,\\,0\\}\\big)^{2}.\n$$\nTo justify strong convexity, observe from the definition that\n$$\ng_{\\mu}(x)-\\frac{\\mu}{2}x^{2}=h(x),\n$$\nand we have already shown $h$ is convex. By the definition of strong convexity, this implies $g_{\\mu}$ is $\\mu$-strongly convex. Therefore, adding the quadratic $\\frac{\\mu}{2}x^{2}$ enforces strong convexity of the infimum.\n\nThe requested final explicit formula for $g_{\\mu}(x)$ is the piecewise expression above (or its compact equivalent).",
            "answer": "$$\\boxed{g_{\\mu}(x)=\\begin{cases}\\dfrac{\\mu}{2}x^{2}+(x+1)^{2}, x-1,\\\\[6pt]\\dfrac{\\mu}{2}x^{2}, -1\\leq x\\leq 1,\\\\[6pt]\\dfrac{\\mu}{2}x^{2}+(x-1)^{2}, x1.\\end{cases}}$$"
        }
    ]
}