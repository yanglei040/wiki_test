{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of analyzing optimization algorithms is determining the smoothness of the objective function, quantified by the Lipschitz constant of its gradient. This first exercise  provides foundational practice in computing this constant from first principles for a non-trivial function. By calculating the Hessian matrix and finding the supremum of its spectral norm, you will gain a concrete understanding of how the function's curvature determines this critical parameter and see how a function's saturating behavior can lead to a bounded global constant.",
            "id": "3144663",
            "problem": "Consider the function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ defined by $f(x)=\\frac{\\|x\\|_{2}^{2}}{1+\\|x\\|_{2}^{2}}$, where $\\|x\\|_{2}$ denotes the Euclidean norm. Starting from the definition of a Lipschitz continuous gradient with respect to the Euclidean norm, use core multivariable calculus facts to determine a global Lipschitz constant $L$ such that the gradient $\\nabla f$ satisfies $ \\|\\nabla f(x)-\\nabla f(y)\\|_{2}\\leq L\\,\\|x-y\\|_{2}$ for all $x,y\\in\\mathbb{R}^{n}$. Your derivation must be based on first principles and must be scientifically consistent. As part of your reasoning, explain how the saturating behavior of $f$ for large $\\|x\\|_{2}$ influences the bound you obtain for $L$. Provide the final value of $L$ as a single exact number. No rounding is required, and no units are involved.",
            "solution": "We begin with the definition: a function has a Lipschitz continuous gradient with respect to the Euclidean norm if there exists a constant $L\\geq 0$ such that for all $x,y\\in\\mathbb{R}^{n}$,\n$$\n\\|\\nabla f(x)-\\nabla f(y)\\|_{2}\\leq L\\,\\|x-y\\|_{2}.\n$$\nA well-tested fact from multivariable calculus and the mean value theorem for vector-valued functions is that, when $f$ is twice continuously differentiable and its Hessian $\\nabla^{2}f(x)$ exists for all $x$, a sufficient (and in many smooth settings, tight) global Lipschitz constant is given by the uniform bound on the spectral norm of the Hessian:\n$$\nL=\\sup_{x\\in\\mathbb{R}^{n}}\\|\\nabla^{2}f(x)\\|_{2},\n$$\nwhere $\\|\\cdot\\|_{2}$ denotes the operator norm induced by the Euclidean norm. To compute such a bound, we will derive the gradient and Hessian of $f$ and then evaluate the largest absolute eigenvalue of $\\nabla^{2}f(x)$ uniformly over $x$.\n\nLet $s=\\|x\\|_{2}^{2}=x^{\\top}x$. Define $g(s)=\\frac{s}{1+s}$. Then $f(x)=g(s)$ as a composition of $g$ with $s(x)$. By the chain rule and the known gradient $\\nabla s(x)=2x$, we have\n$$\ng'(s)=\\frac{(1+s)-s}{(1+s)^{2}}=\\frac{1}{(1+s)^{2}},\n\\quad\\text{so}\\quad\n\\nabla f(x)=g'(s)\\,\\nabla s(x)=\\frac{2x}{(1+s)^{2}}.\n$$\nTo find the Hessian, write $\\nabla f(x)=a(x)\\,x$ with $a(x)=\\frac{2}{(1+s)^{2}}$. The Jacobian of $a(x)\\,x$ is\n$$\n\\nabla^{2}f(x)=a(x)\\,I + x\\,(\\nabla a(x))^{\\top},\n$$\nwhere $I$ is the identity matrix and $\\nabla a(x)$ is the gradient of the scalar field $a(x)$. Differentiate $a(x)$ with respect to $x$ via $s$:\n$$\na(x)=2(1+s)^{-2},\\quad \\frac{\\mathrm{d}a}{\\mathrm{d}s} = -4(1+s)^{-3},\n\\quad \\nabla a(x)=\\frac{\\mathrm{d}a}{\\mathrm{d}s}\\,\\nabla s(x) = -4(1+s)^{-3}\\cdot 2x = -\\frac{8x}{(1+s)^{3}}.\n$$\nTherefore,\n$$\n\\nabla^{2}f(x)=\\frac{2}{(1+s)^{2}}\\,I \\;-\\; \\frac{8}{(1+s)^{3}}\\,x x^{\\top}.\n$$\nThis is a symmetric matrix of the form $\\alpha\\,I - \\beta\\,u u^{\\top}$ with $\\alpha=\\frac{2}{(1+s)^{2}}$, $\\beta=\\frac{8\\|x\\|_{2}^{2}}{(1+s)^{3}}=\\frac{8s}{(1+s)^{3}}$, and $u$ any unit vector in the direction of $x$ when $x\\neq 0$ (if $x=0$, the second term is $0$). The spectrum can be characterized using the eigen-structure of rank-one updates. Specifically:\n- In any direction orthogonal to $u$, the eigenvalue is $\\lambda_{\\perp}=\\alpha=\\frac{2}{(1+s)^{2}}$ (with multiplicity $n-1$).\n- In the direction $u$, since $x x^{\\top}$ acts as $s\\,u u^{\\top}$, the eigenvalue is\n$$\n\\lambda_{\\parallel}=\\alpha - \\beta = \\frac{2}{(1+s)^{2}} - \\frac{8s}{(1+s)^{3}} = \\frac{2 - 6s}{(1+s)^{3}}.\n$$\nThe spectral norm $\\|\\nabla^{2}f(x)\\|_{2}$ for symmetric matrices equals the largest absolute value among the eigenvalues. Therefore,\n$$\n\\|\\nabla^{2}f(x)\\|_{2}=\\max\\!\\left\\{\\left|\\lambda_{\\perp}\\right|,\\left|\\lambda_{\\parallel}\\right|\\right\\}=\\max\\!\\left\\{\\frac{2}{(1+s)^{2}},\\;\\frac{|2-6s|}{(1+s)^{3}}\\right\\}, \\quad s=\\|x\\|_{2}^{2}\\ge 0.\n$$\nWe now maximize this expression over $s\\ge 0$.\n\nFirst, observe that $\\lambda_{\\perp}(s)=\\frac{2}{(1+s)^{2}}$ is strictly decreasing in $s$ and attains its maximum at $s=0$:\n$$\n\\max_{s\\ge 0}\\lambda_{\\perp}(s)=\\lambda_{\\perp}(0)=\\frac{2}{(1+0)^{2}}=2.\n$$\nNext, consider $\\left|\\lambda_{\\parallel}(s)\\right|=\\frac{|2-6s|}{(1+s)^{3}}$. Split into two regions:\n- For $0\\le s\\le \\frac{1}{3}$, we have $\\left|\\lambda_{\\parallel}(s)\\right|=\\frac{2-6s}{(1+s)^{3}}$. Differentiate\n$$\nh(s)=\\frac{2-6s}{(1+s)^{3}},\\quad h'(s)=\\frac{12(s-1)}{(1+s)^{4}},\n$$\nwhich is negative on $[0,\\frac{1}{3}]$. Thus $h$ decreases there, and its maximum on this interval is at $s=0$:\n$$\n\\max_{0\\le s\\le \\frac{1}{3}}\\left|\\lambda_{\\parallel}(s)\\right|=h(0)=\\frac{2}{1^{3}}=2.\n$$\n- For $s\\ge \\frac{1}{3}$, we have $\\left|\\lambda_{\\parallel}(s)\\right|=\\frac{6s-2}{(1+s)^{3}}$. Differentiate\n$$\nk(s)=\\frac{6s-2}{(1+s)^{3}},\\quad k'(s)=\\frac{12(1-s)}{(1+s)^{4}},\n$$\nwhich is positive on $\\left[\\frac{1}{3},1\\right]$ and negative for $s1$. Hence the maximum over $s\\ge \\frac{1}{3}$ occurs at $s=1$:\n$$\n\\max_{s\\ge \\frac{1}{3}}\\left|\\lambda_{\\parallel}(s)\\right|=k(1)=\\frac{6\\cdot 1-2}{(1+1)^{3}}=\\frac{4}{8}=\\frac{1}{2}.\n$$\nCombining both regions, the global maximum of $\\left|\\lambda_{\\parallel}(s)\\right|$ is $\\max\\{2,\\frac{1}{2}\\}=2$. Therefore,\n$$\n\\sup_{x\\in\\mathbb{R}^{n}}\\|\\nabla^{2}f(x)\\|_{2}=\\sup_{s\\ge 0}\\max\\!\\left\\{\\frac{2}{(1+s)^{2}},\\;\\frac{|2-6s|}{(1+s)^{3}}\\right\\}=2.\n$$\nBy the earlier principle, this supremum provides a valid global Lipschitz constant for the gradient:\n$$\nL=2.\n$$\n\nFinally, we discuss the influence of saturation. As $\\|x\\|_{2}\\to\\infty$, the function $f(x)=\\frac{\\|x\\|_{2}^{2}}{1+\\|x\\|_{2}^{2}}$ approaches the saturating value $1$, and the gradient $\\nabla f(x)=\\frac{2x}{(1+\\|x\\|_{2}^{2})^{2}}$ tends to $0$. The Hessian $\\nabla^{2}f(x)=\\frac{2}{(1+s)^{2}}I-\\frac{8}{(1+s)^{3}}x x^{\\top}$ also decays to the zero matrix in operator norm as $s\\to\\infty$. This means curvature is largest near the origin and diminishes as the function saturates, so the global Lipschitz constant $L$ is determined by the behavior at small $\\|x\\|_{2}$, specifically attaining its maximum at $x=0$ where $\\nabla^{2}f(0)=2I$ and $\\|\\nabla^{2}f(0)\\|_{2}=2$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "In machine learning, objective functions are typically averages over many individual loss components. This practice problem  reveals a crucial insight: the smoothness of an average can be significantly better than the smoothness of its parts. Using a clear example with quadratic functions, you will demonstrate how averaging functions with high curvature in different, complementary directions can result in a combined function with a much smaller, more uniform curvature. This principle helps explain the effectiveness of many large-scale optimization methods.",
            "id": "3144694",
            "problem": "Consider $L \\in \\mathbb{R}$ with $L0$ and define functions $\\phi_1,\\phi_2:\\mathbb{R}^2 \\to \\mathbb{R}$ by\n$$\n\\phi_1(x) \\;=\\; \\tfrac{1}{2}\\, x^{\\top} A_1 x\n\\quad\\text{and}\\quad\n\\phi_2(x) \\;=\\; \\tfrac{1}{2}\\, x^{\\top} A_2 x,\n$$\nwhere\n$$\nA_1 \\;=\\; \\begin{pmatrix} L  0 \\\\ 0  0 \\end{pmatrix}\n\\quad\\text{and}\\quad\nA_2 \\;=\\; \\begin{pmatrix} 0  0 \\\\ 0  L \\end{pmatrix}.\n$$\nLet $f:\\mathbb{R}^2 \\to \\mathbb{R}$ be the empirical average\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\big(\\phi_1(x)+\\phi_2(x)\\big).\n$$\nStarting only from the definition of gradient Lipschitz continuity and basic linear algebra facts about symmetric matrices, first justify that each $\\phi_i$ has a gradient that is $L$-Lipschitz with respect to the Euclidean norm, and then determine the smallest constant $L_f$ such that\n$$\n\\|\\nabla f(x)-\\nabla f(y)\\|_2 \\;\\le\\; L_f \\,\\|x-y\\|_2\n\\quad\\text{for all}\\quad x,y \\in \\mathbb{R}^2.\n$$\nProvide your final answer as a closed-form expression in terms of $L$. No rounding is required, and no units are involved. The answer must be a single expression.",
            "solution": "For a quadratic function of the form $\\phi(x) = \\frac{1}{2} x^\\top A x$ with a symmetric matrix $A$, the gradient is $\\nabla \\phi(x) = Ax$ and the Hessian is the constant matrix $\\nabla^2 \\phi(x) = A$. The smallest Lipschitz constant for the gradient is given by the spectral norm of its Hessian, $\\|A\\|_2$. For a symmetric matrix, this is the maximum absolute value of its eigenvalues.\n\nFirst, we analyze $\\phi_1(x) = \\frac{1}{2} x^\\top A_1 x$ and $\\phi_2(x) = \\frac{1}{2} x^\\top A_2 x$.\n- The Hessian of $\\phi_1$ is $A_1 = \\begin{pmatrix} L  0 \\\\ 0  0 \\end{pmatrix}$. Its eigenvalues are $L$ and $0$. Since $L>0$, the spectral norm is $\\|A_1\\|_2 = \\max(|L|, |0|) = L$. Thus, the gradient of $\\phi_1$ is $L$-Lipschitz.\n- The Hessian of $\\phi_2$ is $A_2 = \\begin{pmatrix} 0  0 \\\\ 0  L \\end{pmatrix}$. Its eigenvalues are $0$ and $L$. The spectral norm is $\\|A_2\\|_2 = \\max(|0|, |L|) = L$. Thus, the gradient of $\\phi_2$ is also $L$-Lipschitz.\n\nNext, we analyze the average function $f(x) = \\frac{1}{2}(\\phi_1(x) + \\phi_2(x))$. Due to the linearity of differentiation, the Hessian of $f$ is the average of the Hessians of $\\phi_1$ and $\\phi_2$:\n$$\n\\nabla^2 f(x) \\;=\\; \\frac{1}{2} \\left( \\nabla^2 \\phi_1(x) + \\nabla^2 \\phi_2(x) \\right) \\;=\\; \\frac{1}{2} (A_1 + A_2).\n$$\nWe compute the sum of the matrices:\n$$\nA_1 + A_2 \\;=\\; \\begin{pmatrix} L  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  L \\end{pmatrix} \\;=\\; \\begin{pmatrix} L  0 \\\\ 0  L \\end{pmatrix} \\;=\\; L I,\n$$\nwhere $I$ is the $2 \\times 2$ identity matrix.\nTherefore, the Hessian of $f$ is:\n$$\n\\nabla^2 f(x) \\;=\\; \\frac{1}{2} (L I) \\;=\\; \\frac{L}{2} I.\n$$\nThe smallest Lipschitz constant for the gradient of $f$, denoted $L_f$, is the spectral norm of its Hessian:\n$$\nL_f \\;=\\; \\|\\nabla^2 f(x)\\|_2 \\;=\\; \\left\\| \\frac{L}{2} I \\right\\|_2.\n$$\nThe eigenvalues of the matrix $\\frac{L}{2}I$ are both $\\frac{L}{2}$. The spectral norm is therefore $\\frac{L}{2}$.\nThe smallest Lipschitz constant is $L_f = \\frac{L}{2}$.",
            "answer": "$$\n\\boxed{\\frac{L}{2}}\n$$"
        },
        {
            "introduction": "Gradient clipping is a practical heuristic used to stabilize training in deep learning, but what are its theoretical underpinnings? This exercise  moves from theory to the analysis of a widely used algorithm component. You will dissect the effect of clipping on the descent guarantees for an $L$-smooth function, clarifying how modifying the gradient alters the conditions for decreasing the function value. This analysis builds a bridge between the idealized world of gradient descent theory and the practical realities of modern optimization algorithms.",
            "id": "3144630",
            "problem": "Consider a differentiable function $f:\\mathbb{R}^d\\to\\mathbb{R}$ whose gradient is $L$-Lipschitz continuous (that is, $f$ is $L$-smooth): for all $x,y\\in\\mathbb{R}^d$, $\\|\\nabla f(x)-\\nabla f(y)\\|\\le L\\|x-y\\|$. Define the clipped gradient at $x$ by\n$$\n\\widetilde{g}(x)\\;=\\;\\mathrm{clip}\\big(\\nabla f(x),G\\big)\\;=\\;\\min\\!\\left(1,\\;\\frac{G}{\\|\\nabla f(x)\\|}\\right)\\,\\nabla f(x),\n$$\nwhere $G0$ is a given clipping radius, and consider the clipped-gradient update\n$$\nx_+\\;=\\;x-\\alpha\\,\\widetilde{g}(x),\n$$\nwith step size $\\alpha0$. Which of the following statements about effective smoothness or descent guarantees under clipping are true? Select all that apply.\n\nA. For any $x\\in\\mathbb{R}^d$, the $L$-smoothness of $f$ implies\n$$\nf(x_+)\\;\\le\\;f(x)\\;-\\;\\alpha\\Big(1-\\tfrac{L\\alpha}{2}\\Big)\\,\\big\\|\\widetilde{g}(x)\\big\\|^2,\n$$\nand hence $f(x_+)\\le f(x)$ whenever $0\\alpha\\tfrac{2}{L}$.\n\nB. The mapping $x\\mapsto \\mathrm{clip}\\big(\\nabla f(x),G\\big)$ is globally Lipschitz with constant at most $L$; that is, for all $x,y$,\n$$\n\\big\\|\\mathrm{clip}\\big(\\nabla f(x),G\\big)-\\mathrm{clip}\\big(\\nabla f(y),G\\big)\\big\\|\\;\\le\\;L\\,\\|x-y\\|.\n$$\n\nC. If $\\|\\nabla f(x)\\|\\ge G$, then\n$$\nf(x_+)\\;\\le\\;f(x)\\;-\\;\\alpha\\,G\\,\\|\\nabla f(x)\\|\\;+\\;\\frac{L\\alpha^2}{2}\\,G^2,\n$$\nand thus $f(x_+)\\le f(x)$ whenever $0\\alpha\\tfrac{2}{L}$.\n\nD. Clipping the gradient reduces the function’s smoothness constant from $L$ to $G$; in other words, after clipping one can treat $f$ as $G$-smooth.",
            "solution": "The user wants to validate and solve a problem concerning the properties of clipped gradient descent for an $L$-smooth function.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- A function $f:\\mathbb{R}^d\\to\\mathbb{R}$ is differentiable.\n- The gradient $\\nabla f$ is $L$-Lipschitz continuous: $\\|\\nabla f(x)-\\nabla f(y)\\|\\le L\\|x-y\\|$ for all $x,y\\in\\mathbb{R}^d$. This property is also known as $L$-smoothness.\n- The clipped gradient at a point $x$ is defined as $\\widetilde{g}(x) = \\mathrm{clip}\\big(\\nabla f(x),G\\big) = \\min\\!\\left(1,\\;\\frac{G}{\\|\\nabla f(x)\\|}\\right)\\,\\nabla f(x)$, for a given clipping radius $G0$.\n- The clipped-gradient update rule is $x_+ = x-\\alpha\\,\\widetilde{g}(x)$, where $\\alpha0$ is the step size.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly rooted in the mathematical theory of optimization, a core area of study in machine learning and numerical analysis. The concepts of $L$-smoothness, gradient descent, and gradient clipping are standard and well-defined.\n- **Well-Posed:** The problem provides a clear set of definitions and asks for the verification of specific mathematical statements. Each statement can be rigorously proven or disproven.\n- **Objective:** The language is precise and mathematical, free from any subjective or ambiguous terminology.\n- **Completeness and Consistency:** All necessary definitions and conditions are provided, and they are consistent with one another and with the standard literature on the topic.\n- **Other Flaws:** The problem is not trivial, metaphorical, or ill-posed. It presents a standard, non-trivial analysis task in optimization theory.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed to derive the solution and evaluate each option.\n\n### Solution Derivation\n\nThe starting point for the analysis is the descent lemma, a key consequence of the $L$-smoothness of $f$. For any two points $x, y \\in \\mathbb{R}^d$, this property implies:\n$$\nf(y) \\le f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{L}{2} \\|y-x\\|^2\n$$\nWe apply this lemma to the clipped-gradient update by setting $y = x_+ = x - \\alpha \\widetilde{g}(x)$. The displacement is $y-x = -\\alpha \\widetilde{g}(x)$. Substituting this into the inequality gives:\n$$\nf(x_+) \\le f(x) + \\langle \\nabla f(x), -\\alpha \\widetilde{g}(x) \\rangle + \\frac{L}{2} \\|-\\alpha \\widetilde{g}(x)\\|^2\n$$\n$$\nf(x_+) \\le f(x) - \\alpha \\langle \\nabla f(x), \\widetilde{g}(x) \\rangle + \\frac{L\\alpha^2}{2} \\|\\widetilde{g}(x)\\|^2\n$$\nThis fundamental inequality will be used to evaluate the options.\n\nLet's analyze the definition of the clipped gradient $\\widetilde{g}(x) = \\min\\left(1, \\frac{G}{\\|\\nabla f(x)\\|}\\right) \\nabla f(x)$.\nWe consider two cases based on the norm of the gradient.\n\n**Case 1: $\\|\\nabla f(x)\\| \\le G$**\nIn this case, $\\min\\left(1, \\frac{G}{\\|\\nabla f(x)\\|}\\right) = 1$.\nThe clipped gradient is identical to the full gradient: $\\widetilde{g}(x) = \\nabla f(x)$.\nTherefore, $\\|\\widetilde{g}(x)\\| = \\|\\nabla f(x)\\|$ and $\\langle \\nabla f(x), \\widetilde{g}(x) \\rangle = \\langle \\nabla f(x), \\nabla f(x) \\rangle = \\|\\nabla f(x)\\|^2 = \\|\\widetilde{g}(x)\\|^2$.\n\n**Case 2: $\\|\\nabla f(x)\\|  G$**\nIn this case, $\\min\\left(1, \\frac{G}{\\|\\nabla f(x)\\|}\\right) = \\frac{G}{\\|\\nabla f(x)\\|}$.\nThe gradient is scaled down to have norm $G$: $\\widetilde{g}(x) = \\frac{G}{\\|\\nabla f(x)\\|} \\nabla f(x)$.\nThe norm of the clipped gradient is $\\|\\widetilde{g}(x)\\| = G$.\nThe inner product term is $\\langle \\nabla f(x), \\widetilde{g}(x) \\rangle = \\left\\langle \\nabla f(x), \\frac{G}{\\|\\nabla f(x)\\|} \\nabla f(x) \\right\\rangle = \\frac{G}{\\|\\nabla f(x)\\|} \\|\\nabla f(x)\\|^2 = G \\|\\nabla f(x)\\|$.\n\nNow we evaluate each option.\n\n**A. For any $x\\in\\mathbb{R}^d$, the $L$-smoothness of $f$ implies $f(x_+)\\;\\le\\;f(x)\\;-\\;\\alpha\\Big(1-\\tfrac{L\\alpha}{2}\\Big)\\,\\big\\|\\widetilde{g}(x)\\big\\|^2$, and hence $f(x_+)\\le f(x)$ whenever $0\\alpha\\tfrac{2}{L}$.**\n\nWe check the inequality for the two cases:\n- **Case 1: $\\|\\nabla f(x)\\| \\le G$**\nHere, $\\widetilde{g}(x) = \\nabla f(x)$ and $\\langle \\nabla f(x), \\widetilde{g}(x) \\rangle = \\|\\widetilde{g}(x)\\|^2$.\nThe descent lemma gives:\n$f(x_+) \\le f(x) - \\alpha \\|\\widetilde{g}(x)\\|^2 + \\frac{L\\alpha^2}{2} \\|\\widetilde{g}(x)\\|^2 = f(x) - \\alpha\\left(1 - \\frac{L\\alpha}{2}\\right) \\|\\widetilde{g}(x)\\|^2$.\nThe inequality in statement A holds with equality in this case.\n\n- **Case 2: $\\|\\nabla f(x)\\|  G$**\nHere, $\\|\\widetilde{g}(x)\\| = G$ and $\\langle \\nabla f(x), \\widetilde{g}(x) \\rangle = G \\|\\nabla f(x)\\|$.\nThe descent lemma gives:\n$f(x_+) \\le f(x) - \\alpha G \\|\\nabla f(x)\\| + \\frac{L\\alpha^2}{2} G^2$.\nThe inequality we want to prove is $f(x_+) \\le f(x) - \\alpha(1-\\frac{L\\alpha}{2})\\|\\widetilde{g}(x)\\|^2 = f(x) - \\alpha G^2 + \\frac{L\\alpha^2}{2} G^2$.\nThis requires showing that $f(x) - \\alpha G \\|\\nabla f(x)\\| + \\frac{L\\alpha^2}{2} G^2 \\le f(x) - \\alpha G^2 + \\frac{L\\alpha^2}{2} G^2$.\nThis simplifies to $-\\alpha G \\|\\nabla f(x)\\| \\le -\\alpha G^2$.\nSince $\\alpha0$ and $G0$, we can divide by $-\\alpha G$ and reverse the inequality sign:\n$\\|\\nabla f(x)\\| \\ge G$.\nThis condition is true by the definition of Case 2 (in fact, $\\|\\nabla f(x)\\|  G$).\nThus, the inequality holds in this case as well.\n\nSince the inequality holds in all cases, the first part of statement A is correct.\nFor the second part, if $0  \\alpha  \\frac{2}{L}$, then $1-\\frac{L\\alpha}{2}  0$. Since $\\alpha  0$ and $\\|\\widetilde{g}(x)\\|^2 \\ge 0$, the term $-\\alpha(1-\\frac{L\\alpha}{2})\\|\\widetilde{g}(x)\\|^2 \\le 0$. This guarantees $f(x_+) \\le f(x)$.\n\n**Verdict on A: Correct**\n\n**B. The mapping $x\\mapsto \\mathrm{clip}\\big(\\nabla f(x),G\\big)$ is globally Lipschitz with constant at most $L$; that is, for all $x,y$, $\\big\\|\\mathrm{clip}\\big(\\nabla f(x),G\\big)-\\mathrm{clip}\\big(\\nabla f(y),G\\big)\\big\\|\\;\\le\\;L\\,\\|x-y\\|$.**\n\nLet $h(x) = \\mathrm{clip}\\big(\\nabla f(x),G\\big)$. We can view this as a composition of two mappings: $h(x) = \\phi(\\psi(x))$, where $\\psi(x) = \\nabla f(x)$ and $\\phi(v) = \\mathrm{clip}(v,G)$.\n\nThe mapping $\\psi: \\mathbb{R}^d \\to \\mathbb{R}^d$ is $L$-Lipschitz by the problem definition:\n$\\|\\psi(x) - \\psi(y)\\| = \\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x - y\\|$.\n\nThe clipping function $\\phi(v) = \\min(1, G/\\|v\\|)v$ is the projection of the vector $v$ onto the closed ball of radius $G$ centered at the origin, $B_G = \\{z \\in \\mathbb{R}^d : \\|z\\| \\le G \\}$. A projection operator onto a non-empty closed convex set is non-expansive, which means it is $1$-Lipschitz.\nTherefore, for any $v_1, v_2 \\in \\mathbb{R}^d$:\n$\\|\\phi(v_1) - \\phi(v_2)\\| \\le 1 \\cdot \\|v_1 - v_2\\|$.\n\nNow we analyze the composition:\n$\\|h(x) - h(y)\\| = \\|\\phi(\\nabla f(x)) - \\phi(\\nabla f(y))\\|$.\nUsing the $1$-Lipschitz property of $\\phi$:\n$\\|\\phi(\\nabla f(x)) - \\phi(\\nabla f(y))\\| \\le \\|\\nabla f(x) - \\nabla f(y)\\|$.\nUsing the $L$-Lipschitz property of $\\nabla f$:\n$\\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x - y\\|$.\nCombining these inequalities, we get:\n$\\|h(x) - h(y)\\| \\le L \\|x - y\\|$.\nThis shows that the mapping $x \\mapsto \\widetilde{g}(x)$ is $L$-Lipschitz.\n\n**Verdict on B: Correct**\n\n**C. If $\\|\\nabla f(x)\\|\\ge G$, then $f(x_+)\\;\\le\\;f(x)\\;-\\;\\alpha\\,G\\,\\|\\nabla f(x)\\|\\;+\\;\\frac{L\\alpha^2}{2}\\,G^2$, and thus $f(x_+)\\le f(x)$ whenever $0\\alpha\\tfrac{2}{L}$.**\n\nThe first part of the statement concerns the descent guarantee when the gradient is clipped. This is exactly our Case 2 scenario.\nGiven $\\|\\nabla f(x)\\| \\ge G$, we have $\\widetilde{g}(x) = G \\frac{\\nabla f(x)}{\\|\\nabla f(x)\\|}$.\nThis implies $\\|\\widetilde{g}(x)\\| = G$ and $\\langle \\nabla f(x), \\widetilde{g}(x) \\rangle = G \\|\\nabla f(x)\\|$.\nSubstituting these into the general descent lemma $f(x_+) \\le f(x) - \\alpha \\langle \\nabla f(x), \\widetilde{g}(x) \\rangle + \\frac{L\\alpha^2}{2} \\|\\widetilde{g}(x)\\|^2$:\n$f(x_+) \\le f(x) - \\alpha G \\|\\nabla f(x)\\| + \\frac{L\\alpha^2}{2} G^2$.\nThis matches the inequality in statement C exactly.\n\nFor the second part, we must check if $f(x_+) \\le f(x)$ is guaranteed for $0  \\alpha  \\frac{2}{L}$. This is equivalent to checking if the right-hand side of the inequality is less than or equal to $f(x)$, which means we need to prove:\n$-\\alpha G \\|\\nabla f(x)\\| + \\frac{L\\alpha^2}{2} G^2 \\le 0$.\nSince $\\|\\nabla f(x)\\| \\ge G$ and $-\\alpha G  0$, we have $-\\alpha G \\|\\nabla f(x)\\| \\le -\\alpha G^2$.\nSo, $-\\alpha G \\|\\nabla f(x)\\| + \\frac{L\\alpha^2}{2} G^2 \\le -\\alpha G^2 + \\frac{L\\alpha^2}{2} G^2$.\nLet's factor the right side: $-\\alpha G^2 (1 - \\frac{L\\alpha}{2})$.\nThe acondition $0  \\alpha  \\frac{2}{L}$ implies $0  \\frac{L\\alpha}{2}  1$, so $1 - \\frac{L\\alpha}{2}  0$.\nSince $\\alpha  0$ and $G^2  0$, we have $-\\alpha G^2  0$.\nTherefore, $-\\alpha G^2 (1 - \\frac{L\\alpha}{2})  0$.\nThis gives $f(x_+) - f(x) \\le -\\alpha G^2 (1 - \\frac{L\\alpha}{2})  0$, which implies $f(x_+)  f(x)$. Hence, $f(x_+) \\le f(x)$ is guaranteed.\n\n**Verdict on C: Correct**\n\n**D. Clipping the gradient reduces the function’s smoothness constant from $L$ to $G$; in other words, after clipping one can treat $f$ as $G$-smooth.**\n\nThis statement is an interpretation and is incorrect.\nFirst, clipping the gradient does not change the function $f$ itself. The function $f$ remains $L$-smooth. The statement must be interpreted as a claim about the properties of the update.\n\nThere are two main ways to interpret \"treat $f$ as $G$-smooth\":\n1.  The descent guarantee becomes similar to that for a $G$-smooth function. The standard gradient descent guarantee for a $G$-smooth function $h$ is $h(x_+) \\le h(x) - \\alpha(1 - G\\alpha/2)\\|\\nabla h(x)\\|^2$. Our guarantee from statement A is $f(x_+) \\le f(x) - \\alpha(1 - L\\alpha/2)\\|\\widetilde{g}(x)\\|^2$. The constant in the guarantee is $L$, not $G$. So this interpretation is false.\n2.  The clipped gradient field $\\widetilde{g}(x)$ becomes $G$-Lipschitz. As shown in the analysis for option B, the mapping $x \\mapsto \\widetilde{g}(x)$ is $L$-Lipschitz, not necessarily $G$-Lipschitz. We can construct a simple counterexample. Let $f(x) = \\frac{L}{2}x^2$ in one dimension, so $\\nabla f(x) = Lx$. This function is $L$-smooth. Let $LG$. Consider two points $x_1 = 0$ and $x_2 = (G/L) - \\epsilon$ for a small $\\epsilon  0$.\nThen $\\nabla f(x_1) = 0$ and $\\nabla f(x_2) = G - L\\epsilon$. Since both have magnitudes less than $G$, clipping has no effect: $\\widetilde{g}(x_1) = 0$ and $\\widetilde{g}(x_2) = G - L\\epsilon$.\nThe ratio of change is:\n$\\frac{\\|\\widetilde{g}(x_2) - \\widetilde{g}(x_1)\\|}{\\|x_2 - x_1\\|} = \\frac{|(G - L\\epsilon) - 0|}{|(G/L - \\epsilon) - 0|} = \\frac{G - L\\epsilon}{G/L - \\epsilon} = \\frac{L(G/L - \\epsilon)}{G/L - \\epsilon} = L$.\nThis shows that the Lipschitz constant of the clipped gradient field is $L$, not $G$.\n\nThe magnitude of the gradient is bounded by $G$, but its rate of change (which defines smoothness) is still governed by $L$. The statement is a common misconception.\n\n**Verdict on D: Incorrect**",
            "answer": "$$\\boxed{ABC}$$"
        }
    ]
}