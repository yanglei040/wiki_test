{
    "hands_on_practices": [
        {
            "introduction": "Induced matrix norms quantify the maximum \"stretching\" a matrix can apply to a vector. This exercise  provides a concrete, hands-on calculation to build intuition for this concept. By computing the ratio $\\frac{\\|Ax\\|_1}{\\|x\\|_1}$ for a specific matrix $A$ and vector $x$, you will verify that the induced norm $\\|A\\|_1$ serves as an upper bound on this amplification.",
            "id": "2179377",
            "problem": "In numerical linear algebra, vector norms and their corresponding induced matrix norms are fundamental tools for analyzing the behavior of linear transformations. The 1-norm (or Manhattan norm) for a vector $v \\in \\mathbb{R}^n$ is defined as $\\|v\\|_1 = \\sum_{i=1}^n |v_i|$. The induced matrix 1-norm for a matrix $M \\in \\mathbb{R}^{m \\times n}$ is the maximum absolute column sum, defined as $\\|M\\|_1 = \\max_{1 \\le j \\le n} \\sum_{i=1}^m |M_{ij}|$.\n\nConsider the matrix $A$ and the vector $x$ given by:\n$$\nA = \\begin{pmatrix} -3  5 \\\\ 2  -1 \\end{pmatrix}, \\quad x = \\begin{pmatrix} 2 \\\\ -3 \\end{pmatrix}\n$$\nThe induced matrix norm $\\|A\\|_1$ provides an upper bound on the \"stretching\" effect of the matrix $A$ on any vector $x$ when measured with the 1-norm, according to the inequality $\\|Ax\\|_1 \\le \\|A\\|_1 \\|x\\|_1$.\n\nCalculate the specific \"stretching factor\" for the given vector $x$, which is the ratio $\\frac{\\|Ax\\|_1}{\\|x\\|_1}$. Report your answer as a decimal rounded to three significant figures.",
            "solution": "We use the definition of the induced inequality for the 1-norm: for any matrix $A$ and vector $x$, the stretching factor for $x$ is $\\frac{\\|Ax\\|_{1}}{\\|x\\|_{1}}$. Compute $Ax$:\n$$\nAx=\\begin{pmatrix}-3  5 \\\\ 2  -1\\end{pmatrix}\\begin{pmatrix}2 \\\\ -3\\end{pmatrix}\n=\\begin{pmatrix}-3\\cdot 2+5\\cdot(-3) \\\\ 2\\cdot 2+(-1)\\cdot(-3)\\end{pmatrix}\n=\\begin{pmatrix}-6-15 \\\\ 4+3\\end{pmatrix}\n=\\begin{pmatrix}-21 \\\\ 7\\end{pmatrix}.\n$$\nCompute the 1-norms:\n$$\n\\|x\\|_{1}=|2|+|-3|=2+3=5,\\qquad \\|Ax\\|_{1}=|-21|+|7|=21+7=28.\n$$\nTherefore, the stretching factor is\n$$\n\\frac{\\|Ax\\|_{1}}{\\|x\\|_{1}}=\\frac{28}{5}=5.6.\n$$\nRounding to three significant figures gives $5.60$.",
            "answer": "$$\\boxed{5.60}$$"
        },
        {
            "introduction": "Moving from calculation to derivation, this practice  challenges you to prove the formula for the induced matrix 1-norm from first principles. The problem then connects this mathematical tool to a critical application in machine learningâ€”feature scaling. You will see how imbalanced data features can impact model stability, a phenomenon neatly quantified by the $\\|A\\|_1$ norm.",
            "id": "3148401",
            "problem": "Consider a linear model used in optimization for machine learning, where a dataset is represented by a matrix $A \\in \\mathbb{R}^{m \\times n}$. Each column of $A$ corresponds to a feature, and each row corresponds to a sample. Assume the model uses coordinate-wise updates whose stability is influenced by how the linear map $x \\mapsto A x$ amplifies perturbations measured by the vector $1$-norm. Use only the following fundamental definitions: for a vector $x \\in \\mathbb{R}^{n}$, the $1$-norm is defined by $\\|x\\|_{1} = \\sum_{j=1}^{n} |x_{j}|$, and the induced matrix $1$-norm of $A$ is defined by\n$$\n\\|A\\|_{1} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{1}}{\\|x\\|_{1}}.\n$$\nStarting from these definitions and standard properties of absolute values and inequalities, derive a closed-form expression for $\\|A\\|_{1}$ in terms of the entries of $A$ and then evaluate it for the following dataset matrix (constructed to reflect a situation in which one feature is much more heavily scaled than the others):\n$$\nA = \\begin{pmatrix}\n7.5  0.2  -0.1 \\\\\n8.3  -0.15  0.05 \\\\\n6.9  0.1  0.2 \\\\\n9.1  -0.05  -0.15 \\\\\n8.7  0.25  0.05\n\\end{pmatrix}.\n$$\nExplain, in words, how the derived expression connects to the sensitivity of feature-wise scaling in optimization, specifically why an imbalanced feature scale can dominate the induced matrix $1$-norm and thus the worst-case amplification of perturbations under the map $x \\mapsto A x$. Finally, report the exact numerical value of $\\|A\\|_{1}$ for the given matrix $A$. No rounding is required; provide the exact value.",
            "solution": "First, we will derive the formula for the induced matrix $1$-norm, denoted $\\|A\\|_1$, for a matrix $A \\in \\mathbb{R}^{m \\times n}$. The definition provided is:\n$$\n\\|A\\|_{1} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{1}}{\\|x\\|_{1}}\n$$\nwhere $x \\in \\mathbb{R}^{n}$ and the vector $1$-norm is $\\|x\\|_1 = \\sum_{j=1}^{n} |x_j|$.\n\nLet $y = Ax$. The $i$-th component of the vector $y \\in \\mathbb{R}^m$ is given by $y_i = \\sum_{j=1}^{n} a_{ij} x_j$, where $a_{ij}$ are the entries of $A$. The $1$-norm of $Ax$ is then:\n$$\n\\|Ax\\|_1 = \\|y\\|_1 = \\sum_{i=1}^{m} |y_i| = \\sum_{i=1}^{m} \\left| \\sum_{j=1}^{n} a_{ij} x_j \\right|\n$$\nApplying the triangle inequality for absolute values, which states that $|\\sum_k z_k| \\le \\sum_k |z_k|$, to the inner sum, we get:\n$$\n\\left| \\sum_{j=1}^{n} a_{ij} x_j \\right| \\le \\sum_{j=1}^{n} |a_{ij} x_j| = \\sum_{j=1}^{n} |a_{ij}| |x_j|\n$$\nSubstituting this inequality back into the expression for $\\|Ax\\|_1$:\n$$\n\\|Ax\\|_1 \\le \\sum_{i=1}^{m} \\left( \\sum_{j=1}^{n} |a_{ij}| |x_j| \\right)\n$$\nSince all terms are non-negative, we can interchange the order of summation:\n$$\n\\|Ax\\|_1 \\le \\sum_{j=1}^{n} \\sum_{i=1}^{m} |a_{ij}| |x_j| = \\sum_{j=1}^{n} \\left( |x_j| \\sum_{i=1}^{m} |a_{ij}| \\right)\n$$\nLet us define $C_j = \\sum_{i=1}^{m} |a_{ij}|$ as the sum of the absolute values of the entries in the $j$-th column of $A$. The inequality becomes:\n$$\n\\|Ax\\|_1 \\le \\sum_{j=1}^{n} C_j |x_j|\n$$\nNow, let $C_{\\max} = \\max_{1 \\le j \\le n} C_j = \\max_{1 \\le j \\le n} \\sum_{i=1}^{m} |a_{ij}|$. Since $C_j \\le C_{\\max}$ for all $j$, we can write:\n$$\n\\sum_{j=1}^{n} C_j |x_j| \\le \\sum_{j=1}^{n} C_{\\max} |x_j| = C_{\\max} \\sum_{j=1}^{n} |x_j| = C_{\\max} \\|x\\|_1\n$$\nCombining these inequalities, we have established an upper bound for the ratio $\\frac{\\|Ax\\|_1}{\\|x\\|_1}$:\n$$\n\\|Ax\\|_1 \\le C_{\\max} \\|x\\|_1 \\implies \\frac{\\|Ax\\|_1}{\\|x\\|_1} \\le C_{\\max} \\quad \\text{for all } x \\neq 0\n$$\nThis implies that the supremum of the ratio is also less than or equal to $C_{\\max}$:\n$$\n\\|A\\|_1 = \\sup_{x \\neq 0} \\frac{\\|Ax\\|_1}{\\|x\\|_1} \\le C_{\\max}\n$$\nTo complete the derivation, we must show that this upper bound is attainable. That is, we must find a specific non-zero vector $x_0$ for which the equality $\\frac{\\|Ax_0\\|_1}{\\|x_0\\|_1} = C_{\\max}$ holds.\n\nLet $k$ be the index of the column for which the maximum absolute column sum is achieved, such that $C_k = C_{\\max}$. Consider the vector $x_0 = e_k$, where $e_k$ is the $k$-th standard basis vector in $\\mathbb{R}^n$. This vector has a $1$ in the $k$-th position and $0$s elsewhere.\nThe $1$-norm of this vector is $\\|x_0\\|_1 = \\|e_k\\|_1 = 1$.\nThe product $Ax_0 = Ae_k$ is the $k$-th column of the matrix $A$. Let's denote this column vector as $a_k$.\nThe $1$-norm of $Ax_0$ is therefore:\n$$\n\\|Ax_0\\|_1 = \\|a_k\\|_1 = \\sum_{i=1}^{m} |a_{ik}|\n$$\nBy our definition of the index $k$, this sum is exactly $C_k = C_{\\max}$.\nFor this specific choice of $x_0 = e_k$, the ratio becomes:\n$$\n\\frac{\\|Ax_0\\|_1}{\\|x_0\\|_1} = \\frac{C_{\\max}}{1} = C_{\\max}\n$$\nSince we have found a vector $x_0$ for which the ratio equals $C_{\\max}$, and we have also shown that the ratio can never exceed $C_{\\max}$, we conclude that the supremum must be exactly $C_{\\max}$. Thus, the closed-form expression for the induced matrix $1$-norm is the maximum absolute column sum:\n$$\n\\|A\\|_{1} = \\max_{1 \\le j \\le n} \\sum_{i=1}^{m} |a_{ij}|\n$$\n\nNext, we evaluate this expression for the given matrix:\n$$\nA = \\begin{pmatrix}\n7.5  0.2  -0.1 \\\\\n8.3  -0.15  0.05 \\\\\n6.9  0.1  0.2 \\\\\n9.1  -0.05  -0.15 \\\\\n8.7  0.25  0.05\n\\end{pmatrix}\n$$\nHere, $m=5$ and $n=3$. We calculate the absolute column sums $C_1$, $C_2$, and $C_3$.\nFor the first column ($j=1$):\n$$\nC_1 = |7.5| + |8.3| + |6.9| + |9.1| + |8.7| = 7.5 + 8.3 + 6.9 + 9.1 + 8.7 = 40.5\n$$\nFor the second column ($j=2$):\n$$\nC_2 = |0.2| + |-0.15| + |0.1| + |-0.05| + |0.25| = 0.2 + 0.15 + 0.1 + 0.05 + 0.25 = 0.75\n$$\nFor the third column ($j=3$):\n$$\nC_3 = |-0.1| + |0.05| + |0.2| + |-0.15| + |0.05| = 0.1 + 0.05 + 0.2 + 0.15 + 0.05 = 0.55\n$$\nThe induced matrix $1$-norm is the maximum of these sums:\n$$\n\\|A\\|_{1} = \\max(C_1, C_2, C_3) = \\max(40.5, 0.75, 0.55) = 40.5\n$$\n\nFinally, we explain the connection to feature scaling. The derived formula, $\\|A\\|_1 = \\max_j \\sum_i |a_{ij}|$, reveals that the induced $1$-norm is determined solely by the column with the largest sum of absolute values. In the context given, each column of $A$ represents a feature, and its entries are the values of that feature across different samples. The sum $\\sum_i |a_{ij}|$ can be interpreted as a measure of the total magnitude or scale of the $j$-th feature in the dataset.\n\nThe induced norm $\\|A\\|_1$ quantifies the maximum amplification of a vector's $1$-norm under the linear transformation $x \\mapsto Ax$. It represents a worst-case sensitivity metric: any input perturbation $\\delta x$ to a vector $x$ will result in a perturbation $\\delta y = A\\delta x$ in the output, bounded by $\\|\\delta y\\|_1 \\le \\|A\\|_1 \\|\\delta x\\|_1$.\n\nThe given matrix $A$ was constructed to have one feature (the first column) with a much larger scale than the others. As our calculation shows, this imbalanced scaling causes the absolute sum of the first column ($C_1=40.5$) to completely dominate the sums of the other columns ($C_2=0.75$, $C_3=0.55$). Consequently, the overall norm $\\|A\\|_1$ is determined entirely by this single, heavily scaled feature. This implies that the stability and behavior of any optimization algorithm sensitive to this norm (like certain coordinate-wise methods) will be dictated by the worst-scaled feature. Perturbations aligned with this feature (i.e., concentrated in the first component of the input vector $x$) are amplified by a factor of $40.5$, while perturbations along other feature directions would be amplified by much smaller factors. Such a large, imbalanced amplification factor can lead to poor conditioning and instability in numerical methods, which is why feature scaling (e.g., standardizing columns to have a similar scale) is a critical preprocessing step for many optimization and machine learning algorithms.\n\nThe exact numerical value of $\\|A\\|_1$ is $40.5$.",
            "answer": "$$\n\\boxed{40.5}\n$$"
        },
        {
            "introduction": "This final practice  delves into the crucial role of matrix norms in assessing the stability of linear systems. You will analyze a specially constructed matrix to see how the choice of norm (1-norm vs. $\\infty$-norm) leads to dramatically different condition numbers, $\\kappa_1(A)$ and $\\kappa_\\infty(A)$. This exercise demonstrates that a system's sensitivity to error is not an absolute property but depends fundamentally on how we choose to measure it.",
            "id": "3148445",
            "problem": "Consider a square matrix of dimension $n \\geq 2$. Define the matrix $S \\in \\mathbb{R}^{n \\times n}$ by $s_{1j} = 1$ for $j = 2, 3, \\dots, n$ and $s_{ij} = 0$ otherwise. Construct the matrix $A = I - S$, where $I$ is the identity matrix. Use this explicit construction to analyze the sensitivity of solving the linear system $A x = b$ under perturbations of the data vector $b$.\n\nTasks:\n- Show that $A$ is invertible and find $A^{-1}$ using only fundamental properties of matrices and nilpotent operators.\n- Compute $\\|A^{-1}\\|_{1}$ and $\\|A^{-1}\\|_{\\infty}$ for this $A$.\n- Compute $\\|A\\|_{1}$ and $\\|A\\|_{\\infty}$ and then the corresponding condition numbers $\\kappa_{1}(A)$ and $\\kappa_{\\infty}(A)$, where $\\kappa_{p}(A) = \\|A\\|_{p} \\|A^{-1}\\|_{p}$ for an induced matrix norm.\n- For a perturbation $b \\mapsto b + \\delta b$, derive the worst-case upper bound on the relative solution error $\\|x - \\hat{x}\\|_{p} / \\|x\\|_{p}$ for $p \\in \\{1, \\infty\\}$ using only the definition of induced norms and submultiplicativity. Here, $x$ is the exact solution to $A x = b$ and $\\hat{x}$ solves $A \\hat{x} = b + \\delta b$.\n- Under the additional assumption that the relative data error is identical in both norms, i.e., $\\|\\delta b\\|_{1} / \\|b\\|_{1} = \\|\\delta b\\|_{\\infty} / \\|b\\|_{\\infty}$, determine the ratio of the two worst-case relative error bounds (in the $\\infty$-norm versus in the $1$-norm) as a function of $n$.\n\nProvide your final answer as a single analytic expression in $n$.",
            "solution": "The problem asks for a multi-step analysis of a specific matrix $A = I - S$. Let's proceed through each task.\n\nFirst, we define the matrix $S \\in \\mathbb{R}^{n \\times n}$ for $n \\ge 2$. Its elements are given by $s_{1j} = 1$ for $j \\in \\{2, 3, \\dots, n\\}$ and $s_{ij} = 0$ for all other pairs $(i, j)$. This means the only non-zero entries of $S$ are in the first row, starting from the second column.\nThe matrix $S$ has the form:\n$$S = \\begin{pmatrix}\n0  1  1  \\dots  1 \\\\\n0  0  0  \\dots  0 \\\\\n0  0  0  \\dots  0 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  0  \\dots  0\n\\end{pmatrix}$$\n\n**Task 1: Show that $A$ is invertible and find $A^{-1}$.**\n\nThe matrix $A$ is defined as $A = I - S$. To show it is invertible and find its inverse, we first analyze the properties of $S$. Let's compute $S^2$. The element $(i, k)$ of $S^2$ is given by $(S^2)_{ik} = \\sum_{j=1}^{n} s_{ij} s_{jk}$.\n- If $i  1$, then $s_{ij} = 0$ for all $j$, so $(S^2)_{ik} = 0$.\n- If $i = 1$, then $(S^2)_{1k} = \\sum_{j=1}^{n} s_{1j} s_{jk}$. The only non-zero terms $s_{1j}$ are for $j \\in \\{2, 3, \\dots, n\\}$. Thus, the sum becomes $(S^2)_{1k} = \\sum_{j=2}^{n} s_{1j} s_{jk} = \\sum_{j=2}^{n} (1) s_{jk}$.\nFor $s_{jk}$ to be non-zero, we must have $j=1$. However, the sum runs from $j=2$ to $n$. Therefore, for every $j$ in the sum, $s_{jk} = 0$.\nAs a result, $(S^2)_{1k} = \\sum_{j=2}^{n} 0 = 0$ for all $k$.\nSince all elements of $S^2$ are zero, $S^2 = O$, where $O$ is the $n \\times n$ zero matrix. This shows that $S$ is a nilpotent matrix of order $2$.\n\nThe matrix $A$ is given by $A = I - S$. Because $S$ is nilpotent, the geometric series (Neumann series) for the inverse of $A$ is finite:\n$$(I-S)^{-1} = I + S + S^2 + S^3 + \\dots$$\nSince $S^k = O$ for all $k \\ge 2$, the series terminates.\n$$A^{-1} = (I - S)^{-1} = I + S$$\nThe existence of the inverse $A^{-1}$ proves that $A$ is invertible.\nThe explicit forms of $A$ and $A^{-1}$ are:\n$$A = I - S = \\begin{pmatrix}\n1  -1  -1  \\dots  -1 \\\\\n0  1  0  \\dots  0 \\\\\n0  0  1  \\dots  0 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  0  \\dots  1\n\\end{pmatrix}$$\n$$A^{-1} = I + S = \\begin{pmatrix}\n1  1  1  \\dots  1 \\\\\n0  1  0  \\dots  0 \\\\\n0  0  1  \\dots  0 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  0  \\dots  1\n\\end{pmatrix}$$\n\n**Task 2: Compute $\\|A^{-1}\\|_{1}$ and $\\|A^{-1}\\|_{\\infty}$.**\n\nThe induced $1$-norm of a matrix is the maximum absolute column sum, $\\|M\\|_1 = \\max_{1 \\le j \\le n} \\sum_{i=1}^{n} |m_{ij}|$.\nFor $A^{-1} = I + S$:\n- The first column ($j=1$) has a sum of absolute values of $|1| = 1$.\n- For any other column $j \\in \\{2, \\dots, n\\}$, the non-zero entries are $(A^{-1})_{1j}=1$ and $(A^{-1})_{jj}=1$. The sum of absolute values is $|1| + |1| = 2$.\nThus, $\\|A^{-1}\\|_1 = \\max(1, 2, \\dots, 2) = 2$.\n\nThe induced $\\infty$-norm of a matrix is the maximum absolute row sum, $\\|M\\|_{\\infty} = \\max_{1 \\le i \\le n} \\sum_{j=1}^{n} |m_{ij}|$.\nFor $A^{-1} = I + S$:\n- The first row ($i=1$) has $n$ entries equal to $1$. The sum of absolute values is $\\sum_{j=1}^{n} |1| = n$.\n- For any other row $i \\in \\{2, \\dots, n\\}$, the only non-zero entry is $(A^{-1})_{ii}=1$. The sum of absolute values is $|1| = 1$.\nThus, $\\|A^{-1}\\|_{\\infty} = \\max(n, 1, \\dots, 1) = n$.\n\n**Task 3: Compute $\\|A\\|_{1}$, $\\|A\\|_{\\infty}$ and the condition numbers.**\n\nWe first compute the norms of $A = I - S$.\nFor the $1$-norm of $A$:\n- The first column ($j=1$) has a sum of absolute values of $|1| = 1$.\n- For any other column $j \\in \\{2, \\dots, n\\}$, the non-zero entries are $a_{1j}=-1$ and $a_{jj}=1$. The sum of absolute values is $|-1| + |1| = 2$.\nThus, $\\|A\\|_1 = \\max(1, 2, \\dots, 2) = 2$.\n\nFor the $\\infty$-norm of $A$:\n- The first row ($i=1$) has entries $1, -1, \\dots, -1$. The sum of absolute values is $|1| + \\sum_{j=2}^{n} |-1| = 1 + (n-1) = n$.\n- For any other row $i \\in \\{2, \\dots, n\\}$, the only non-zero entry is $a_{ii}=1$. The sum of absolute values is $|1|=1$.\nThus, $\\|A\\|_{\\infty} = \\max(n, 1, \\dots, 1) = n$.\n\nThe condition number $\\kappa_p(A)$ is defined as $\\kappa_p(A) = \\|A\\|_p \\|A^{-1}\\|_p$.\n- For $p=1$: $\\kappa_1(A) = \\|A\\|_1 \\|A^{-1}\\|_1 = 2 \\cdot 2 = 4$.\n- For $p=\\infty$: $\\kappa_{\\infty}(A) = \\|A\\|_{\\infty} \\|A^{-1}\\|_{\\infty} = n \\cdot n = n^2$.\n\n**Task 4: Derive the worst-case upper bound on the relative solution error.**\n\nLet $x$ be the solution to $Ax=b$ and $\\hat{x}$ be the solution to the perturbed system $A\\hat{x} = b + \\delta b$.\nThe error in the solution is $\\delta x = \\hat{x} - x$.\nSubtracting the first equation from the second gives:\n$A\\hat{x} - Ax = (b + \\delta b) - b \\implies A(\\hat{x} - x) = \\delta b \\implies A \\delta x = \\delta b$.\nSince $A$ is invertible, we can write $\\delta x = A^{-1} \\delta b$.\nTaking the $p$-norm of both sides and using the property of induced norms $\\|Mv\\|_p \\le \\|M\\|_p \\|v\\|_p$, we get:\n$\\|\\delta x\\|_p = \\|A^{-1} \\delta b\\|_p \\le \\|A^{-1}\\|_p \\|\\delta b\\|_p$.\n\nTo find the relative error $\\|\\delta x\\|_p / \\|x\\|_p$, we need a bound involving $\\|x\\|_p$. From the original equation $Ax=b$, we have $\\|b\\|_p = \\|Ax\\|_p \\le \\|A\\|_p \\|x\\|_p$. Assuming $b \\ne 0$ (and thus $x \\ne 0$), we can write $1/\\|x\\|_p \\le \\|A\\|_p/\\|b\\|_p$.\n\nCombining these two inequalities:\n$$\\frac{\\|\\delta x\\|_p}{\\|x\\|_p} \\le \\frac{\\|A^{-1}\\|_p \\|\\delta b\\|_p}{\\|x\\|_p} \\le \\|A^{-1}\\|_p \\|\\delta b\\|_p \\left( \\frac{\\|A\\|_p}{\\|b\\|_p} \\right)$$\nRearranging terms, we obtain the standard worst-case relative error bound:\n$$\\frac{\\|\\hat{x} - x\\|_p}{\\|x\\|_p} \\le \\|A\\|_p \\|A^{-1}\\|_p \\frac{\\|\\delta b\\|_p}{\\|b\\|_p} = \\kappa_p(A) \\frac{\\|\\delta b\\|_p}{\\|b\\|_p}$$\nThis bound is considered \"worst-case\" because for any matrix $A$, there exist vectors $b$ and $\\delta b$ for which this inequality becomes an equality.\n\n**Task 5: Determine the ratio of the two worst-case relative error bounds.**\n\nThe worst-case upper bound for the relative error in the $\\infty$-norm is:\n$$E_{\\infty} = \\kappa_{\\infty}(A) \\frac{\\|\\delta b\\|_{\\infty}}{\\|b\\|_{\\infty}} = n^2 \\frac{\\|\\delta b\\|_{\\infty}}{\\|b\\|_{\\infty}}$$\nThe worst-case upper bound for the relative error in the $1$-norm is:\n$$E_{1} = \\kappa_{1}(A) \\frac{\\|\\delta b\\|_{1}}{\\|b\\|_{1}} = 4 \\frac{\\|\\delta b\\|_{1}}{\\|b\\|_{1}}$$\nWe are given the additional assumption that the relative data error is identical in both norms:\n$$\\frac{\\|\\delta b\\|_{1}}{\\|b\\|_{1}} = \\frac{\\|\\delta b\\|_{\\infty}}{\\|b\\|_{\\infty}}$$\nLet this common value be denoted by $\\epsilon$. The error bounds become $E_{\\infty} = n^2 \\epsilon$ and $E_1 = 4 \\epsilon$.\nThe problem asks for the ratio of the two worst-case relative error bounds, in the $\\infty$-norm versus in the $1$-norm. This ratio is:\n$$\\frac{E_{\\infty}}{E_{1}} = \\frac{n^2 \\epsilon}{4 \\epsilon} = \\frac{n^2}{4}$$\nThis result is valid for $n \\ge 2$.",
            "answer": "$$\\boxed{\\frac{n^2}{4}}$$"
        }
    ]
}