{
    "hands_on_practices": [
        {
            "introduction": "在优化领域，许多实际问题天然地表现为非光滑函数，例如著名的支持向量机中的铰链损失函数。本练习将引导我们处理一类重要的非光滑凸函数——线性函数之最大值，并探索一种强大的平滑化技术，即“log-sum-exp”技巧。通过计算原始非光滑函数的 Lipschitz 常数和其光滑近似的平滑度常数，我们将深入理解函数性质如何通过数学变换得到改善，为应用梯度方法铺平道路 。",
            "id": "3183315",
            "problem": "设 $m=3$ 且 $d=2$。考虑由下式定义的凸函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$\n$$\nf(x)=\\max_{i\\in\\{1,2,3\\}} a_{i}^{\\top}x,\n$$\n其中 $a_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$，$a_{2}=\\begin{pmatrix}0\\\\2\\end{pmatrix}$，且 $a_{3}=\\begin{pmatrix}-1\\\\-1\\end{pmatrix}$。在欧几里得范数 $\\|\\cdot\\|_{2}$下进行计算。对于任意 $\\gamma>0$，定义平滑近似（log-sum-exp 平滑）$f_{\\gamma}:\\mathbb{R}^{2}\\to\\mathbb{R}$ 为\n$$\nf_{\\gamma}(x)=\\gamma\\log\\!\\left(\\sum_{i=1}^{3}\\exp\\!\\left(\\frac{a_{i}^{\\top}x}{\\gamma}\\right)\\right).\n$$\n从利普希茨连续性和梯度利普希茨连续性的基本定义出发，确定：\n1. 函数 $f$ 关于 $\\|\\cdot\\|_{2}$ 的最小利普希茨常数。\n2. 梯度 $\\nabla f_{\\gamma}(x)$ 关于 $\\|\\cdot\\|_{2}$ 的最小全局利普希茨常数，表示为 $\\gamma$ 的闭式解析函数 $L(\\gamma)$。\n\n请将您的最终答案表示为一个行矩阵，其第一个元素是 $f$ 的利普希茨常数，第二个元素是 $L(\\gamma)$。无需四舍五入。",
            "solution": "用户要求我解决一个分为两部分的问题，该问题与一个特定的凸函数 $f(x)$ 及其平滑近似 $f_\\gamma(x)$ 有关。\n\n**第 1 部分：$f(x)$ 的利普希茨常数**\n\n一个函数 $f:\\mathbb{R}^{d}\\to\\mathbb{R}$ 被称为关于范数 $\\|\\cdot\\|$ 是 $L_f$-利普希茨连续的，如果存在一个常数 $L_f \\ge 0$，使得对于所有 $x, y \\in \\mathbb{R}^{d}$，以下不等式成立：\n$$|f(x) - f(y)| \\le L_f \\|x - y\\|$$\n满足条件的最小常数 $L_f$ 就是 $f$ 的利普希茨常数。\n\n对于一个凸函数，其利普希茨常数由该函数次梯度的最大可能范数给出。用于次梯度的范数是定义域上所用范数的对偶范数。鉴于我们使用的是自对偶的欧几里得范数 $\\|\\cdot\\|_{2}$，利普希茨常数为：\n$$L_f = \\sup_{x \\in \\mathbb{R}^{d}} \\sup_{g \\in \\partial f(x)} \\|g\\|_{2}$$\n其中 $\\partial f(x)$ 是 $f$ 在 $x$ 处的次微分。\n\n给定函数为 $f(x) = \\max_{i \\in \\{1, 2, 3\\}} a_{i}^{\\top}x$。这是一个有限个线性函数的最大值，是一个著名的凸函数。$f(x)$ 在点 $x$ 处的次微分是那些在该点达到最大值的活跃线性函数的梯度的凸包，即：\n$$\\partial f(x) = \\text{conv}\\{a_i \\mid a_i^\\top x = f(x)\\}$$\n在整个定义域上所有可能的次梯度的集合是所有向量 $a_i$ 的凸包：\n$$\\bigcup_{x \\in \\mathbb{R}^{d}} \\partial f(x) = \\text{conv}\\{a_1, a_2, ..., a_m\\}$$\n在我们的例子中，$m=3$，所以这个集合是 $\\text{conv}\\{a_1, a_2, a_3\\}$。\n\n利普希茨常数是此集合中任意元素范数的上确界：\n$$L_f = \\sup_{g \\in \\text{conv}\\{a_1, a_2, a_3\\}} \\|g\\|_2$$\n由于范数 $\\|\\cdot\\|_2$ 是一个凸函数，其在一个紧凸集（有限点集的凸包是紧凸的）上的最大值在该集合的一个极点上取得。$\\text{conv}\\{a_1, a_2, a_3\\}$ 的极点是 $\\{a_1, a_2, a_3\\}$ 的一个子集。因此，问题简化为在给定向量 $a_i$ 中寻找最大范数：\n$$L_f = \\max_{i \\in \\{1, 2, 3\\}} \\|a_i\\|_2$$\n给定向量为 $a_1=\\begin{pmatrix}1\\\\0\\end{pmatrix}$，$a_2=\\begin{pmatrix}0\\\\2\\end{pmatrix}$，和 $a_3=\\begin{pmatrix}-1\\\\-1\\end{pmatrix}$。我们计算它们的欧几里得范数：\n$$\\|a_1\\|_2 = \\sqrt{1^2 + 0^2} = \\sqrt{1} = 1$$\n$$\\|a_2\\|_2 = \\sqrt{0^2 + 2^2} = \\sqrt{4} = 2$$\n$$\\|a_3\\|_2 = \\sqrt{(-1)^2 + (-1)^2} = \\sqrt{1+1} = \\sqrt{2}$$\n这些值的最大值是：\n$$L_f = \\max\\{1, 2, \\sqrt{2}\\} = 2$$\n\n**第 2 部分：$f_\\gamma(x)$ 的梯度利普希茨常数**\n\n一个可微函数 $g:\\mathbb{R}^{d}\\to\\mathbb{R}$ 的梯度是利普希茨连续的，常数为 $L$（也称为 $L$-平滑），如果对于所有 $x, y \\in \\mathbb{R}^{d}$，下式成立：\n$$\\|\\nabla g(x) - \\nabla g(y)\\|_2 \\le L \\|x - y\\|_2$$\n对于一个二次可微函数，最小的此类常数 $L$ 是其海森矩阵在整个定义域上谱范数（对于对称半正定矩阵，即最大特征值）的上确界：\n$$L = \\sup_{x \\in \\mathbb{R}^{d}} \\|\\nabla^2 g(x)\\|_2 = \\sup_{x \\in \\mathbb{R}^{d}} \\lambda_{\\max}(\\nabla^2 g(x))$$\n函数为 $f_{\\gamma}(x)=\\gamma\\log\\left(\\sum_{i=1}^{3}\\exp\\left(\\frac{a_{i}^{\\top}x}{\\gamma}\\right)\\right)$。我们来计算它的梯度和海森矩阵。\n设 $p_i(x) = \\frac{\\exp(a_i^\\top x / \\gamma)}{\\sum_{k=1}^3 \\exp(a_k^\\top x / \\gamma)}$。注意 $p_i(x) > 0$ 且 $\\sum_{i=1}^3 p_i(x) = 1$。\n$f_\\gamma(x)$ 的梯度是：\n$$\\nabla f_\\gamma(x) = \\sum_{i=1}^3 p_i(x) a_i$$\n海森矩阵 $\\nabla^2 f_\\gamma(x)$ 的元素为 $(\\nabla^2 f_\\gamma(x))_{jk} = \\frac{\\partial^2 f_\\gamma(x)}{\\partial x_j \\partial x_k}$。一个标准的计算得出：\n$$\\nabla^2 f_\\gamma(x) = \\frac{1}{\\gamma} \\left( \\sum_{i=1}^3 p_i(x) a_i a_i^\\top - \\left(\\sum_{i=1}^3 p_i(x) a_i\\right) \\left(\\sum_{i=1}^3 p_i(x) a_i\\right)^\\top \\right)$$\n这个矩阵表示一个向量值随机变量 $A$ 的协方差矩阵，该随机变量以概率 $p_i(x)$ 取值 $a_i$，并由 $\\frac{1}{\\gamma}$ 进行缩放。设 $\\bar{a}(x) = \\mathbb{E}_{p(x)}[A] = \\sum_i p_i(x) a_i$。那么 $\\nabla^2 f_\\gamma(x) = \\frac{1}{\\gamma} \\text{Cov}_{p(x)}(A) = \\frac{1}{\\gamma}(\\mathbb{E}_{p(x)}[AA^\\top] - \\bar{a}(x)\\bar{a}(x)^\\top)$。\n\n梯度的最小全局利普希茨常数 $L(\\gamma)$ 是：\n$$L(\\gamma) = \\sup_{x \\in \\mathbb{R}^2} \\|\\nabla^2 f_\\gamma(x)\\|_2 = \\frac{1}{\\gamma} \\sup_{x \\in \\mathbb{R}^2} \\lambda_{\\max}(\\text{Cov}_{p(x)}(A))$$\n谱范数为 $\\lambda_{\\max}(M) = \\sup_{\\|v\\|_2=1} v^\\top M v$。对于协方差矩阵，我们有：\n$$v^\\top (\\text{Cov}_{p(x)}(A)) v = \\mathbb{E}_{p(x)}[(v^\\top A)^2] - (\\mathbb{E}_{p(x)}[v^\\top A])^2 = \\text{Var}_{p(x)}(a^\\top v)$$\n值 $p(x)$ 可以任意接近概率单纯形 $\\Delta_3 = \\{p \\in \\mathbb{R}^3 \\mid p_i \\ge 0, \\sum p_i = 1\\}$ 中的任意一点。因此，我们需要找到在 $p \\in \\Delta_3$ 上的上确界：\n$$L(\\gamma) = \\frac{1}{\\gamma} \\sup_{p \\in \\Delta_3} \\sup_{\\|v\\|_2=1} \\text{Var}_p(a^\\top v) = \\frac{1}{\\gamma} \\sup_{\\|v\\|_2=1} \\sup_{p \\in \\Delta_3} \\text{Var}_p(a^\\top v)$$\n对于一个固定的向量 $v$，设 $c_i = a_i^\\top v$。我们想要找到 $\\sup_{p \\in \\Delta_3} \\text{Var}_p(c)$。具有有限支撑的随机变量的方差上限为 $\\frac{1}{4}(\\max(c) - \\min(c))^2$。当概率质量在最大值和最小值上各分配 $\\frac{1}{2}$ 时，达到此最大值。因此：\n$$\\sup_{p \\in \\Delta_3} \\text{Var}_p(c) = \\frac{1}{4} \\max_{i,j} (c_i - c_j)^2 = \\frac{1}{4} \\max_{i,j} ((a_i - a_j)^\\top v)^2$$\n将此代入 $L(\\gamma)$ 的表达式中：\n$$L(\\gamma) = \\frac{1}{\\gamma} \\sup_{\\|v\\|_2=1} \\left( \\frac{1}{4} \\max_{i,j} ((a_i - a_j)^\\top v)^2 \\right) = \\frac{1}{4\\gamma} \\max_{i,j} \\left( \\sup_{\\|v\\|_2=1} ((a_i - a_j)^\\top v)^2 \\right)$$\n根据对偶范数的定义和柯西-施瓦茨不等式，$\\sup_{\\|v\\|_2=1} ((a_i - a_j)^\\top v)^2 = \\|a_i - a_j\\|_2^2$。因此，最小的全局利普希茨常数是：\n$$L(\\gamma) = \\frac{1}{4\\gamma} \\max_{i,j \\in \\{1,2,3\\}} \\|a_i - a_j\\|_2^2$$\n我们现在计算向量之间差的平方范数：\n$$a_1 - a_2 = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}0 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}1 \\\\ -2\\end{pmatrix} \\implies \\|a_1 - a_2\\|_2^2 = 1^2 + (-2)^2 = 5$$\n$$a_1 - a_3 = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}-1 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}2 \\\\ 1\\end{pmatrix} \\implies \\|a_1 - a_3\\|_2^2 = 2^2 + 1^2 = 5$$\n$$a_2 - a_3 = \\begin{pmatrix}0 \\\\ 2\\end{pmatrix} - \\begin{pmatrix}-1 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 3\\end{pmatrix} \\implies \\|a_2 - a_3\\|_2^2 = 1^2 + 3^2 = 10$$\n这些值的最大值是 $10$。\n将此代入 $L(\\gamma)$ 的公式中：\n$$L(\\gamma) = \\frac{10}{4\\gamma} = \\frac{5}{2\\gamma}$$\n\n最终答案是一个行矩阵，包含 $f$ 的利普希茨常数 $2$ 和 $\\nabla f_\\gamma$ 的利普希茨常数 $\\frac{5}{2\\gamma}$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 & \\frac{5}{2\\gamma}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "函数的平滑度常数 $L$ 不仅仅是一个抽象的数学概念，它直接决定了优化算法（如梯度下降法）的稳定性和收敛行为。理论分析表明，梯度下降法的步长 $\\alpha$ 必须满足一个关键条件 $\\alpha  2/L$ 才能保证收敛。这个编码练习将理论付诸实践，通过设计一个数值实验来直观地观察当步长跨越这个临界边界时，算法是如何从稳定收敛转变为振荡甚至发散的 。",
            "id": "3183363",
            "problem": "您需要设计并实现一个数值实验，通过光滑性和梯度的利普希茨连续性来检验无约束二次最小化问题的梯度下降法的稳定性。实验应完全在二次目标函数 $$f(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x,$$ 的框架下进行，其中 $A$ 是对称正定 (Symmetric Positive Definite, SPD) 矩阵，$b$ 是一个向量。$f$ 的梯度由下式给出：$$\\nabla f(x) = A x - b.$$ 如果对于定义域中的所有 $x,y$ 满足 $$\\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x-y\\|$$，则称梯度映射是以常数 $L$ 利普希茨连续的，其中 $\\|\\cdot\\|$ 表示欧几里得范数。在这种二次型且 $A$ 为对称正定的情况下，梯度的利普希茨常数等于 $A$ 的谱范数，即 $A$ 的最大特征值。\n\n从基本的梯度下降迭代 $$x_{k+1} = x_k - \\alpha \\nabla f(x_k),$$ 开始，其中 $\\alpha$ 是一个固定的步长，请表达相对于最小化点 $x^{\\star}$（满足 $A x^{\\star} = b$）的误差 $e_k = x_k - x^{\\star}$，并分析线性误差动态。迭代的稳定性可以通过一个由 $A$ 和 $\\alpha$ 导出的特定矩阵的谱半径来表征。基于此特性，存在一个与梯度利普希茨常数相关的临界步长边界，它将稳定行为与振荡或发散行为区分开来。您的程序必须：\n\n- 计算梯度利普希茨常数 $L$，即 $A$ 的最大特征值。\n- 通过检查线性误差传播算子的谱半径是否超过 $1$ 来计算理论上的稳定性/发散性分类。\n- 从指定的初始点开始，对误差动态进行固定次数迭代的梯度下降数值模拟，并确定是否在数值上观察到发散。对于本实验，如果以下两个条件同时成立，则定义为“观察到发散”：\n  1. 运行期间的最大误差范数至少是初始误差范数的 $r_{\\text{grow}}$ 倍。\n  2. 最终误差范数至少是初始误差范数的 $r_{\\text{final}}$ 倍。\n  \n使用下面指定的 $N$ 次迭代，以及下面指定的固定阈值 $r_{\\text{grow}}$ 和 $r_{\\text{final}}$。对于每个测试用例，您的输出必须是一个布尔值，指示观察到的发散是否与理论上的发散分类相匹配。\n\n需要使用的基本定义：\n- 梯度的利普希茨连续性：$$\\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x-y\\|.$$\n- 梯度下降更新：$$x_{k+1} = x_k - \\alpha \\nabla f(x_k).$$\n- 相对于满足 $$A x^{\\star} = b$$ 的最小化点 $$x^{\\star}$$ 的误差动态。\n\n实现以下测试套件。每个用例都指定了 $A$、$b$、$x_0$ 和 $\\alpha$。将一维情况视为 $A$ 是一个 $1 \\times 1$ 矩阵，向量长度为 $1$。对所有用例使用 $N = 200$ 次迭代、$r_{\\text{grow}} = 1.5$ 和 $r_{\\text{final}} = 1.2$。\n\n- 用例 $1$（一维，步长低于边界）：$$A = [5],\\quad b = [5],\\quad x_0 = [10],\\quad \\alpha = 0.30.$$\n- 用例 $2$（一维，步长位于边界）：$$A = [5],\\quad b = [5],\\quad x_0 = [10],\\quad \\alpha = 0.40.$$\n- 用例 $3$（一维，步长高于边界）：$$A = [5],\\quad b = [5],\\quad x_0 = [10],\\quad \\alpha = 0.41.$$\n- 用例 $4$（二维，步长低于边界）：$$A = \\begin{bmatrix}3  1 \\\\ 1  3\\end{bmatrix},\\quad b = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ 4\\end{bmatrix},\\quad \\alpha = 0.49.$$\n- 用例 $5$（二维，步长位于边界）：$$A = \\begin{bmatrix}3  1 \\\\ 1  3\\end{bmatrix},\\quad b = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ 4\\end{bmatrix},\\quad \\alpha = 0.50.$$\n- 用例 $6$（二维，步长高于边界）：$$A = \\begin{bmatrix}3  1 \\\\ 1  3\\end{bmatrix},\\quad b = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ 4\\end{bmatrix},\\quad \\alpha = 0.51.$$\n- 用例 $7$（三维，步长低于边界）：$$A = \\operatorname{diag}(1,3,7),\\quad b = \\begin{bmatrix}2 \\\\ -1 \\\\ 5\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ -3 \\\\ 1\\end{bmatrix},\\quad \\alpha = 0.28.$$\n- 用例 $8$（三维，步长高于边界）：$$A = \\operatorname{diag}(1,3,7),\\quad b = \\begin{bmatrix}2 \\\\ -1 \\\\ 5\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ -3 \\\\ 1\\end{bmatrix},\\quad \\alpha = 0.29.$$\n\n您的程序必须：\n- 对每个用例，计算 $A$ 的最大特征值 $L$。\n- 通过评估线性误差传播算子的谱半径并检查其是否超过 $1$ 来计算理论上的发散分类。\n- 使用指定的 $\\alpha$ 对误差动态进行 $N = 200$ 次迭代的数值梯度下降，并使用上面定义的 $r_{\\text{grow}} = 1.5$ 和 $r_{\\text{final}} = 1.2$ 来分类是否观察到发散。\n- 为每个用例生成一个布尔值，指示观察到的发散是否与理论分类相匹配。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$$[result_1,result_2,\\dots,result_8]$$）。每个 $$result_i$$ 都必须是一个布尔值。",
            "solution": "该问题要求对应用于无约束二次最小化问题的梯度下降法的稳定性理论进行数值验证。给定目标函数 $f(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$，其中 $A$ 是一个对称正定 (SPD) 矩阵。梯度下降更新规则为 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$，其中步长 $\\alpha  0$ 为固定值。\n\n首先，我们通过分析误差动态来建立稳定性的理论框架。目标函数的梯度是 $\\nabla f(x) = A x - b$。唯一的最小化点，记为 $x^{\\star}$，是线性系统 $\\nabla f(x^{\\star}) = 0$ 的解，即 $A x^{\\star} = b$。\n\n第 $k$ 次迭代的误差定义为 $e_k = x_k - x^{\\star}$。为了推导误差的演化，我们将梯度和关系式 $b = A x^{\\star}$ 代入梯度下降更新规则中：\n$$x_{k+1} = x_k - \\alpha (A x_k - b)$$\n$$x_{k+1} = x_k - \\alpha (A x_k - A x^{\\star})$$\n两边同时减去 $x^{\\star}$，得到第 $k+1$ 次迭代的误差：\n$$x_{k+1} - x^{\\star} = (x_k - x^{\\star}) - \\alpha A (x_k - x^{\\star})$$\n$$e_{k+1} = e_k - \\alpha A e_k$$\n这可以简化为关于误差向量的线性动力系统：\n$$e_{k+1} = (I - \\alpha A) e_k$$\n其中 $I$ 是单位矩阵。令 $B = I - \\alpha A$ 为误差传播矩阵。第 $k$ 次迭代的误差由 $e_k = B^k e_0$ 给出，其中 $e_0$ 是初始误差。\n\n迭代的稳定性，即对于任意初始误差 $e_0$，当 $k \\to \\infty$ 时误差 $e_k$ 收敛到零，是由传播矩阵 $B$ 的谱半径决定的。谱半径 $\\rho(B)$ 是矩阵 $B$ 的特征值绝对值的最大值。当且仅当 $\\rho(B)  1$ 时，迭代是稳定且收敛的。如果 $\\rho(B)  1$，则迭代发散。如果 $\\rho(B) = 1$，误差不会收敛到零（除非它已经为零），但其范数不一定会增长到无穷大。\n\n由于 $A$ 是对称的，它有实数特征值 $\\lambda_i$。$B = I - \\alpha A$ 的特征值是 $\\mu_i = 1 - \\alpha \\lambda_i$。因此，谱半径为：\n$$\\rho(B) = \\max_i |1 - \\alpha \\lambda_i|$$\n由于 $A$ 是正定的，所有 $\\lambda_i  0$。收敛条件 $\\rho(B)  1$ 等价于对所有 $i$ 都有 $-1  1 - \\alpha \\lambda_i  1$。\n右侧的不等式 $1 - \\alpha \\lambda_i  1$ 简化为 $-\\alpha \\lambda_i  0$，对于 $\\alpha  0$ 和 $\\lambda_i  0$ 总是成立的。\n左侧的不等式 $-1  1 - \\alpha \\lambda_i$ 简化为 $\\alpha \\lambda_i  2$，即 $\\alpha  2/\\lambda_i$。这个条件必须对所有特征值 $\\lambda_i$ 都成立。最严格的条件是由最大特征值 $\\lambda_{\\max}(A)$ 决定的。因此，步长必须满足：\n$$\\alpha  \\frac{2}{\\lambda_{\\max}(A)}$$\n问题陈述指出，梯度的利普希茨常数 $L$ 等于 $\\lambda_{\\max}(A)$。稳定性条件是 $\\alpha  2/L$。\n\n如果稳定性条件被违反，使得 $\\rho(I - \\alpha A)  1$，则分类为理论上发散。\n\n数值实验旨在验证这一理论预测。对于每个测试用例，我们执行以下步骤：\n1.  **理论分类**：我们计算矩阵 $A$ 的特征值，找到谱半径 $\\rho(I - \\alpha A)$，并确定它是否大于 $1$。这给出了 `theoretical_divergence` 布尔分类。\n2.  **数值模拟**：我们对误差动态 $e_{k+1} = (I - \\alpha A)e_k$ 进行 $N=200$ 次迭代的模拟。\n    - 首先，通过求解 $A x^{\\star} = b$ 找到最小化点 $x^{\\star}$。\n    - 计算初始误差 $e_0 = x_0 - x^{\\star}$ 及其欧几里得范数 $\\|e_0\\|$。\n    - 我们迭代计算 $e_1, e_2, \\dots, e_N$ 及其范数。\n    - **观察到的发散**：我们检查模拟是否满足指定的发散标准：\n      - (1) 运行期间的最大误差范数 $\\max_{0 \\le k \\le N} \\|e_k\\|$ 至少是初始误差范数 $\\|e_0\\|$ 的 $r_{\\text{grow}} = 1.5$ 倍。\n      - (2) 最终误差范数 $\\|e_N\\|$ 至少是初始误差范数 $\\|e_0\\|$ 的 $r_{\\text{final}} = 1.2$ 倍。\n    - 如果两个条件都满足，则将行为分类为 `observed_divergence`。\n3.  **比较**：对于每个用例，我们比较理论分类和观察分类。预期的结果是它们匹配，从而得到布尔值 `True`。当 $\\rho(B)  1$ 时，与 $B$ 的主导特征值相对应的误差分量会以 $(\\rho(B))^k$ 的形式指数级增长。对于足够大的迭代次数 $N$，这种增长将不可避免地超过固定的阈值 $r_{\\text{grow}}$ 和 $r_{\\text{final}}$。相反，当 $\\rho(B) \\le 1$ 时，误差范数不会增长，因此不会满足发散标准。\n\n实现将根据此逻辑处理八个测试用例中的每一个。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the stability of gradient descent for quadratic minimization problems,\n    comparing theoretical predictions with numerical simulations.\n    \"\"\"\n    # Use float for all numerical values to ensure consistency.\n    test_cases = [\n        # Case 1 (1D, stable)\n        {'A': np.array([[5.0]]), 'b': np.array([5.0]), 'x0': np.array([10.0]), 'alpha': 0.30},\n        # Case 2 (1D, boundary)\n        {'A': np.array([[5.0]]), 'b': np.array([5.0]), 'x0': np.array([10.0]), 'alpha': 0.40},\n        # Case 3 (1D, divergent)\n        {'A': np.array([[5.0]]), 'b': np.array([5.0]), 'x0': np.array([10.0]), 'alpha': 0.41},\n        # Case 4 (2D, stable)\n        {'A': np.array([[3.0, 1.0], [1.0, 3.0]]), 'b': np.array([1.0, -2.0]), 'x0': np.array([5.0, 4.0]), 'alpha': 0.49},\n        # Case 5 (2D, boundary)\n        {'A': np.array([[3.0, 1.0], [1.0, 3.0]]), 'b': np.array([1.0, -2.0]), 'x0': np.array([5.0, 4.0]), 'alpha': 0.50},\n        # Case 6 (2D, divergent)\n        {'A': np.array([[3.0, 1.0], [1.0, 3.0]]), 'b': np.array([1.0, -2.0]), 'x0': np.array([5.0, 4.0]), 'alpha': 0.51},\n        # Case 7 (3D, stable)\n        {'A': np.diag([1.0, 3.0, 7.0]), 'b': np.array([2.0, -1.0, 5.0]), 'x0': np.array([5.0, -3.0, 1.0]), 'alpha': 0.28},\n        # Case 8 (3D, divergent)\n        {'A': np.diag([1.0, 3.0, 7.0]), 'b': np.array([2.0, -1.0, 5.0]), 'x0': np.array([5.0, -3.0, 1.0]), 'alpha': 0.29},\n    ]\n\n    results = []\n    \n    # Simulation parameters\n    N = 200\n    r_grow = 1.5\n    r_final = 1.2\n    \n    for case in test_cases:\n        A, b, x0, alpha = case['A'], case['b'], case['x0'], case['alpha']\n        \n        # 1. Theoretical Analysis\n        # Since A is symmetric, eigvalsh is appropriate and returns real eigenvalues.\n        eigenvalues_A = np.linalg.eigvalsh(A)\n        # The eigenvalues of the error propagation matrix B = I - alpha * A\n        eigenvalues_B = 1.0 - alpha * eigenvalues_A\n        # The spectral radius of B\n        rho_B = np.max(np.abs(eigenvalues_B))\n        # Theoretical divergence occurs if the spectral radius is  1\n        theoretical_divergence = rho_B  1.0\n        \n        # 2. Numerical Simulation\n        # Solve for the minimizer x_star from A * x_star = b\n        x_star = np.linalg.solve(A, b)\n        \n        # Initial error vector and its norm\n        e_k = x0 - x_star\n        initial_error_norm = np.linalg.norm(e_k)\n        \n        # Handle the edge case where the initial guess is the solution\n        if initial_error_norm  1e-12:\n            observed_divergence = False\n        else:\n            max_error_norm = initial_error_norm\n            \n            # Error propagation matrix B = I - alpha * A\n            B = np.identity(A.shape[0]) - alpha * A\n            \n            # Run the simulation on the error dynamics\n            for _ in range(N):\n                e_k = B @ e_k\n                current_norm = np.linalg.norm(e_k)\n                if current_norm  max_error_norm:\n                    max_error_norm = current_norm\n            \n            final_error_norm = np.linalg.norm(e_k)\n            \n            # Check the conditions for observed divergence\n            cond1 = (max_error_norm = r_grow * initial_error_norm)\n            cond2 = (final_error_norm = r_final * initial_error_norm)\n            observed_divergence = cond1 and cond2\n\n        # 3. Compare theoretical prediction with numerical observation\n        # The result is True if they match, False otherwise.\n        match = (observed_divergence == theoretical_divergence)\n        results.append(match)\n\n    # Format the final output as a string representing a list of booleans\n    # The default string representation of Python booleans is 'True' or 'False'.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在加速优化算法中，收敛速率通常与目标函数的条件数（如 $\\sqrt{L/\\mu}$）密切相关，其中 $L$ 是平滑度常数，$\\mu$ 是强凸性常数。这个练习引导我们扮演“优化工程师”的角色，通过添加二次正则化项来主动改善一个函数的性质。我们将分析这种正则化如何权衡平滑度 $L$ 和强凸性 $\\mu$ 的变化，并通过求解一个约束优化问题来找到最佳的正则化参数，从而最小化关键的收敛因子 。",
            "id": "3183345",
            "problem": "设 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ 是一个可微的凸函数，其梯度是 $L$-利普希茨连续的，即对所有 $x,y\\in\\mathbb{R}^{n}$，都有 $|\\!|\\nabla f(x)-\\nabla f(y)|\\!|\\leq L\\,|\\!|x-y|\\!|$。并假设 $f$ 是 $\\mu_{0}$-强凸的，即对所有 $x,y\\in\\mathbb{R}^{n}$，都有 $f(y)\\geq f(x)+\\nabla f(x)^{\\top}(y-x)+\\frac{\\mu_{0}}{2}|\\!|y-x|\\!|^{2}$，其中 $L\\mu_{0}\\geq 0$。考虑添加一个二次正则化项来构成\n$$\nF_{\\alpha}(x)\\;=\\;f(x)\\;+\\;\\frac{\\alpha}{2}\\,|\\!|x|\\!|^{2},\n$$\n其中 $\\alpha\\geq 0$ 是一个可调参数。请仅使用梯度利普希茨连续性和强凸性的基本定义，确定 $F_{\\alpha}$ 的平滑度常数 $L_{\\alpha}$ 和强凸性模数 $\\mu_{\\alpha}$。\n\n在光滑强凸优化中，Nesterov 加速梯度 (NAG) 方法的迭代复杂度，在不计与 $\\alpha$ 无关的乘法常数的情况下，其量级为 $\\sqrt{L_{\\alpha}/\\mu_{\\alpha}}$ 乘以一个关于目标精度的对数因子。为量化在“不过多改变 $L$”的情况下增加强凸性的权衡，对于给定的参数 $0  \\rho  1$，施加工程约束 $L_{\\alpha}\\leq (1+\\rho)\\,L$。请选择 $\\alpha$ 以在该约束条件下最小化主要的条件数因子 $\\sqrt{L_{\\alpha}/\\mu_{\\alpha}}$，并将最小化的因子以 $L$、$\\mu_{0}$ 和 $\\rho$ 的单个闭式表达式形式报告。不要对您的答案进行四舍五入。",
            "solution": "用户希望我在优化背景下解决一个由两部分组成的问题。\n\n**第 1 部分：求正则化函数的光滑度和强凸性常数。**\n\n1.  **分析函数：** 新函数为 $F_{\\alpha}(x) = f(x) + \\frac{\\alpha}{2} ||x||^2$。\n2.  **定义性质：**\n    *   **光滑性（L-光滑）：** 如果一个函数 $g$ 的梯度是 $L_g$-利普希茨连续的，即 $||\\nabla g(x) - \\nabla g(y)|| \\leq L_g ||x - y||$，则该函数是 $L_g$-光滑的。\n    *   **强凸性（$\\mu$-强凸）：** 如果一个可微函数 $g$ 满足 $g(y) \\geq g(x) + \\nabla g(x)^T(y-x) + \\frac{\\mu_g}{2} ||y-x||^2$，则该函数是 $\\mu_g$-强凸的。\n3.  **将这些定义应用于 $F_{\\alpha}(x)$。**\n\n    *   **$F_{\\alpha}$ 的光滑性：**\n        *   首先，我需要求 $F_{\\alpha}(x)$ 的梯度。\n        *   $f(x)$ 的梯度是 $\\nabla f(x)$。\n        *   正则化项是 $g(x) = \\frac{\\alpha}{2} ||x||^2 = \\frac{\\alpha}{2} \\sum_{i=1}^n x_i^2$。\n        *   正则化项的梯度是 $\\nabla g(x) = \\alpha x$。\n        *   所以，$\\nabla F_{\\alpha}(x) = \\nabla f(x) + \\alpha x$。\n        *   现在，我需要检验 $\\nabla F_{\\alpha}(x)$ 的利普希茨连续性。\n        *   $||\\nabla F_{\\alpha}(x) - \\nabla F_{\\alpha}(y)|| = ||(\\nabla f(x) + \\alpha x) - (\\nabla f(y) + \\alpha y)||$。\n        *   $= ||(\\nabla f(x) - \\nabla f(y)) + \\alpha(x - y)||$。\n        *   使用三角不等式：$||\\nabla F_{\\alpha}(x) - \\nabla F_{\\alpha}(y)|| \\leq ||\\nabla f(x) - \\nabla f(y)|| + ||\\alpha(x - y)||$。\n        *   $= ||\\nabla f(x) - \\nabla f(y)|| + |\\alpha| ||x - y||$。\n        *   因为 $\\alpha \\geq 0$，所以上式为 $||\\nabla f(x) - \\nabla f(y)|| + \\alpha ||x - y||$。\n        *   我们已知 $\\nabla f$ 是 $L$-利普希茨的，所以 $||\\nabla f(x) - \\nabla f(y)|| \\leq L ||x - y||$。\n        *   将此代入，我们得到 $||\\nabla F_{\\alpha}(x) - \\nabla F_{\\alpha}(y)|| \\leq L ||x - y|| + \\alpha ||x - y|| = (L + \\alpha) ||x - y||$。\n        *   这意味着 $\\nabla F_{\\alpha}$ 是 $(L+\\alpha)$-利普希茨连续的。\n        *   因此，$F_{\\alpha}$ 的平滑度常数是 $L_{\\alpha} = L + \\alpha$。\n\n    *   **$F_{\\alpha}$ 的强凸性：**\n        *   我需要使用定义：$F_{\\alpha}(y) \\geq F_{\\alpha}(x) + \\nabla F_{\\alpha}(x)^T(y-x) + \\frac{\\mu_{\\alpha}}{2} ||y-x||^2$。\n        *   让我们展开这些项。$F_{\\alpha}(y) = f(y) + \\frac{\\alpha}{2} ||y||^2$。\n        *   $F_{\\alpha}(x) = f(x) + \\frac{\\alpha}{2} ||x||^2$。\n        *   $\\nabla F_{\\alpha}(x) = \\nabla f(x) + \\alpha x$。\n        *   不等式变为：\n            $f(y) + \\frac{\\alpha}{2} ||y||^2 \\geq (f(x) + \\frac{\\alpha}{2} ||x||^2) + (\\nabla f(x) + \\alpha x)^T(y-x) + \\frac{\\mu_{\\alpha}}{2} ||y-x||^2$。\n        *   这看起来很复杂。我们试试另一种方法。\n        *   一个函数 $g$ 是 $\\mu_g$-强凸的，当且仅当函数 $h(x) = g(x) - \\frac{\\mu_g}{2} ||x||^2$ 是凸的。\n        *   让我们直接使用定义，但要更巧妙一些。\n        *   设 $g(x) = \\frac{\\alpha}{2} ||x||^2$。这是一个二次函数。\n        *   我们知道 $f$ 是 $\\mu_0$-强凸的：\n            $f(y) \\geq f(x) + \\nabla f(x)^T(y-x) + \\frac{\\mu_0}{2} ||y-x||^2$。\n        *   $g(x)$ 怎么样呢？$\\nabla g(x) = \\alpha x$。\n            $g(y) = \\frac{\\alpha}{2} ||y||^2$。\n            $g(x) + \\nabla g(x)^T(y-x) + \\frac{\\alpha}{2} ||y-x||^2 = \\frac{\\alpha}{2} ||x||^2 + (\\alpha x)^T(y-x) + \\frac{\\alpha}{2} (||y||^2 - 2y^T x + ||x||^2)$。\n            $= \\frac{\\alpha}{2} ||x||^2 + \\alpha y^T x - \\alpha ||x||^2 + \\frac{\\alpha}{2} ||y||^2 - \\alpha y^T x + \\frac{\\alpha}{2} ||x||^2$。\n            $= \\frac{\\alpha}{2} ||y||^2 = g(y)$。\n        *   因此，对于 $g(x) = \\frac{\\alpha}{2} ||x||^2$，我们有等式 $g(y) = g(x) + \\nabla g(x)^T(y-x) + \\frac{\\alpha}{2} ||y-x||^2$。这意味着 $g(x)$ 恰好是 $\\alpha$-强凸的。\n        *   现在考虑 $F_{\\alpha}(x) = f(x) + g(x)$。\n        *   一个 $\\mu_1$-强凸函数与一个 $\\mu_2$-强凸函数的和是 $(\\mu_1 + \\mu_2)$-强凸的。\n        *   我们来证明这一点。设 $h_1$ 是 $\\mu_1$-强凸的，$h_2$ 是 $\\mu_2$-强凸的。\n            $h_1(y) \\geq h_1(x) + \\nabla h_1(x)^T(y-x) + \\frac{\\mu_1}{2} ||y-x||^2$。\n            $h_2(y) \\geq h_2(x) + \\nabla h_2(x)^T(y-x) + \\frac{\\mu_2}{2} ||y-x||^2$。\n        *   将这两个不等式相加：\n            $(h_1(y) + h_2(y)) \\geq (h_1(x) + h_2(x)) + (\\nabla h_1(x) + \\nabla h_2(x))^T(y-x) + \\frac{(\\mu_1 + \\mu_2)}{2} ||y-x||^2$。\n        *   这正是 $h_1+h_2$ 以模数 $\\mu_1 + \\mu_2$ 的强凸性定义。\n        *   在我们的例子中，$f(x)$ 是 $\\mu_0$-强凸的，$g(x) = \\frac{\\alpha}{2} ||x||^2$ 是 $\\alpha$-强凸的。\n        *   因此，$F_{\\alpha}(x) = f(x) + g(x)$ 是 $(\\mu_0 + \\alpha)$-强凸的。\n        *   强凸性模数是 $\\mu_{\\alpha} = \\mu_0 + \\alpha$。\n\n    *   **第 1 部分总结：**\n        *   $L_{\\alpha} = L + \\alpha$。\n        *   $\\mu_{\\alpha} = \\mu_0 + \\alpha$。\n        *   这些结果看起来是正确的，并且是按要求使用基本定义推导出来的。\n\n**第 2 部分：在约束条件下优化条件数。**\n\n1.  **陈述目标：** 最小化关于 $\\alpha \\geq 0$ 的 $\\sqrt{\\frac{L_{\\alpha}}{\\mu_{\\alpha}}}$。\n2.  **陈述第 1 部分的量：**\n    *   $L_{\\alpha} = L + \\alpha$。\n    *   $\\mu_{\\alpha} = \\mu_0 + \\alpha$。\n3.  **陈述约束条件：**\n    *   $L_{\\alpha} \\leq (1+\\rho)L$。\n    *   代入 $L_{\\alpha}$：$L + \\alpha \\leq (1+\\rho)L = L + \\rho L$。\n    *   这简化为 $\\alpha \\leq \\rho L$。\n    *   我们还有一个隐含的约束 $\\alpha \\geq 0$。\n    *   因此，$\\alpha$ 的可行范围是 $0 \\leq \\alpha \\leq \\rho L$。\n4.  **构建优化问题：**\n    *   最小化 $g(\\alpha) = \\sqrt{\\frac{L + \\alpha}{\\mu_0 + \\alpha}}$\n    *   约束条件为 $0 \\leq \\alpha \\leq \\rho L$。\n5.  **分析目标函数 $g(\\alpha)$。**\n    *   最小化 $\\sqrt{x}$ 等价于最小化 $x$（对于 $x \\geq 0$）。\n    *   所以，我们来最小化 $h(\\alpha) = \\frac{L + \\alpha}{\\mu_0 + \\alpha}$。\n    *   为了找到 $h(\\alpha)$ 的行为，我们求它关于 $\\alpha$ 的导数。\n    *   $h'(\\alpha) = \\frac{d}{d\\alpha} \\left( \\frac{L + \\alpha}{\\mu_0 + \\alpha} \\right)$。\n    *   使用商法则：\n        $h'(\\alpha) = \\frac{(1)(\\mu_0 + \\alpha) - (L + \\alpha)(1)}{(\\mu_0 + \\alpha)^2}$。\n        $h'(\\alpha) = \\frac{\\mu_0 + \\alpha - L - \\alpha}{(\\mu_0 + \\alpha)^2} = \\frac{\\mu_0 - L}{(\\mu_0 + \\alpha)^2}$。\n6.  **分析导数的符号。**\n    *   分母 $(\\mu_0 + \\alpha)^2$ 总是正的。\n    *   $h'(\\alpha)$ 的符号由分子 $\\mu_0 - L$ 的符号决定。\n    *   问题陈述中说 $L  \\mu_0 \\ge 0$。\n    *   因此，$\\mu_0 - L  0$。\n    *   所以，对于定义域中的所有 $\\alpha$，$h'(\\alpha)  0$。\n7.  **解释结果。**\n    *   由于 $h(\\alpha)$ 的导数总是负的，函数 $h(\\alpha)$ 是关于 $\\alpha$ 的严格递减函数。\n    *   函数 $g(\\alpha) = \\sqrt{h(\\alpha)}$ 也是严格递减的。\n    *   要在闭区间 $[a, b]$ 上最小化一个严格递减的函数，我们必须选择区间中可能的最大值。\n    *   $\\alpha$ 的区间是 $[0, \\rho L]$。\n    *   可能的最大值是 $\\alpha = \\rho L$。\n8.  **计算最小化的因子。**\n    *   $\\alpha$ 的最优值是 $\\alpha_{opt} = \\rho L$。\n    *   现在，将这个值代回条件数因子的表达式中。\n    *   最小化的因子 = $\\sqrt{\\frac{L + \\alpha_{opt}}{\\mu_0 + \\alpha_{opt}}}$。\n    *   $= \\sqrt{\\frac{L + \\rho L}{\\mu_0 + \\rho L}}$。\n    *   $= \\sqrt{\\frac{L(1 + \\rho)}{\\mu_0 + \\rho L}}$。\n9.  **最后检查步骤和约束条件。**\n    *   问题陈述：$f$ 是可微凸函数，$\\nabla f$ 是 $L$-利普希茨的，$f$ 是 $\\mu_0$-强凸的。\n    *   $L  \\mu_0 \\geq 0$。\n    *   $F_{\\alpha}(x) = f(x) + \\frac{\\alpha}{2} ||x||^2$，其中 $\\alpha \\geq 0$。\n    *   约束条件：$L_{\\alpha} \\leq (1+\\rho)L$，对于 $0  \\rho  1$。\n    *   $L_{\\alpha} = L+\\alpha$ 和 $\\mu_{\\alpha} = \\mu_0+\\alpha$ 的推导是可靠的，并且遵循定义。\n    *   约束 $\\alpha \\leq \\rho L$ 是正确的。\n    *   对目标函数 $\\frac{L+\\alpha}{\\mu_0+\\alpha}$ 的分析是正确的。导数确实是 $\\frac{\\mu_0 - L}{(\\mu_0+\\alpha)^2}$。\n    *   因为 $L  \\mu_0$，所以导数为负。函数是递减的。\n    *   最小值在 $\\alpha$ 的最大允许值处取得。\n    *   最大允许值是 $\\alpha = \\rho L$。\n    *   得到的最小值为 $\\sqrt{\\frac{L(1+\\rho)}{\\mu_0 + \\rho L}}$。\n    *   这个表达式按要求以 $L$、$\\mu_0$ 和 $\\rho$ 表示。它是一个单一的闭式表达式。\n    *   所有步骤在逻辑上都是合理的，在数学上是正确的。",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{L(1+\\rho)}{\\mu_{0}+\\rho L}}}\n$$"
        }
    ]
}