{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering a new concept is often developing a strong visual intuition. This practice moves beyond simple cases with unique minimizers to a scenario where the solution set, or $\\operatorname{argmin}$, forms a distinct geometric shape. By analyzing a straightforward function on a square domain, you will see how a collection of points, not just one, can achieve the minimum value, providing a clear, tangible example of a structured minimizer set .",
            "id": "3098641",
            "problem": "Let $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ be defined by $f(x,y)=x^{2}y^{2}$ and let the feasible set be the closed square $D=[-1,1]^{2}=\\{(x,y)\\in\\mathbb{R}^{2}:-1\\leq x\\leq 1,\\,-1\\leq y\\leq 1\\}$. Using only core definitions from optimization, determine the $\\operatorname{argmin}$ set of $f$ over $D$, and interpret its geometry within $D$ based on first principles. Then, compute the total one-dimensional Euclidean length of this $\\operatorname{argmin}$ set, understood as the sum of the lengths of the line segments that comprise it, counting intersections only once. Express your final answer as an exact number.",
            "solution": "The user wants to solve the problem: Let $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ be defined by $f(x,y)=x^{2}y^{2}$ and let the feasible set be the closed square $D=[-1,1]^{2}=\\{(x,y)\\in\\mathbb{R}^{2}:-1\\leq x\\leq 1,\\,-1\\leq y\\leq 1\\}$. Using only core definitions from optimization, determine the $\\operatorname{argmin}$ set of $f$ over $D$, and interpret its geometry within $D$ based on first principles. Then, compute the total one-dimensional Euclidean length of this $\\operatorname{argmin}$ set, understood as the sum of the lengths of the line segments that comprise it, counting intersections only once. Express your final answer as an exact number.\n\nThis problem is a valid optimization problem. It is self-contained, mathematically and scientifically grounded, and well-posed. The function $f(x,y) = x^2 y^2$ is continuous on the compact set $D = [-1,1]^2$, so by the Extreme Value Theorem, a global minimum exists. The problem asks for the set of points where this minimum is achieved (the $\\operatorname{argmin}$ set) and its geometric properties, which are well-defined mathematical tasks. All provided information is clear, consistent, and sufficient for a unique solution.\n\nWe begin by recalling the formal definition of the $\\operatorname{argmin}$ set. For a function $f$ defined on a set $D$, the $\\operatorname{argmin}$ of $f$ over $D$ is the set of points in $D$ at which $f$ attains its minimum value. Let $m = \\min_{(x,y) \\in D} f(x,y)$. Then, the $\\operatorname{argmin}$ set, which we denote as $S^*$, is given by:\n$$S^* = \\operatorname{argmin}_{(x,y) \\in D} f(x,y) = \\{(x,y) \\in D \\mid f(x,y) = m\\}$$\n\nThe first step is to determine the minimum value $m$ of the function $f(x,y) = x^2y^2$ on the feasible set $D = [-1,1]^2$. The function can be written as $f(x,y) = (xy)^2$. For any real numbers $x$ and $y$, the terms $x^2$ and $y^2$ are non-negative. That is, $x^2 \\geq 0$ and $y^2 \\geq 0$. Consequently, their product, $f(x,y) = x^2y^2$, must also be non-negative for all $(x,y) \\in \\mathbb{R}^2$, and therefore for all $(x,y) \\in D$.\n$$f(x,y) = x^2y^2 \\geq 0$$\nThe lowest possible value that a non-negative function can take is $0$. We must verify if this value can be attained by $f(x,y)$ for some point $(x,y)$ within the domain $D$. The condition $f(x,y) = 0$ is equivalent to:\n$$x^2y^2 = 0$$\nThis equation holds if and only if $x^2 = 0$ or $y^2 = 0$, which simplifies to $x=0$ or $y=0$.\n\nSo, the function $f(x,y)$ attains the value $0$ at any point $(x,y)$ where at least one of the coordinates is zero. Since we have established that $f(x,y) \\geq 0$ for all points and that the value $0$ is attainable within $D$ (for example, at $(0,0) \\in D$, $f(0,0)=0$), the global minimum value of $f$ over $D$ is $m=0$.\n\nNow we can determine the $\\operatorname{argmin}$ set $S^*$. This is the set of all points $(x,y)$ in the domain $D$ for which $f(x,y) = 0$.\n$$S^* = \\{(x,y) \\in D \\mid f(x,y) = 0\\}$$\nAs determined above, this condition is satisfied if and only if $x=0$ or $y=0$. So, we are looking for the set of points in the square $D = [-1,1]^2$ that have either their $x$-coordinate or their $y$-coordinate equal to zero.\n$$S^* = \\{(x,y) \\in [-1,1]^2 \\mid x=0 \\text{ or } y=0\\}$$\nThis set can be expressed as the union of two subsets:\n1.  The set of points where $x=0$: $S_x = \\{(0,y) \\mid -1 \\leq y \\leq 1\\}$.\n2.  The set of points where $y=0$: $S_y = \\{(x,0) \\mid -1 \\leq x \\leq 1\\}$.\nSo, the $\\operatorname{argmin}$ set is $S^* = S_x \\cup S_y$.\n\nNext, we interpret the geometry of this set. The domain $D$ is a square centered at the origin with side length $2$. The set $S_x$ is the vertical line segment on the $y$-axis from the point $(0,-1)$ to $(0,1)$. The set $S_y$ is the horizontal line segment on the $x$-axis from the point $(-1,0)$ to $(1,0)$. The union of these two segments, $S^*$, forms a cross shape \"+\" centered at the origin, with its arms extending to the boundaries of the square $D$. The two segments intersect at the origin, $(0,0)$.\n\nFinally, we compute the total one-dimensional Euclidean length of the set $S^*$. The problem specifies to sum the lengths of the line segments, counting intersections only once. The set $S^*$ is the union of the segment $S_x$ and the segment $S_y$.\nThe length of the vertical segment $S_x$ from $(0,-1)$ to $(0,1)$ is the Euclidean distance between its endpoints:\n$$L_x = \\sqrt{(0-0)^2 + (1 - (-1))^2} = \\sqrt{0^2 + 2^2} = 2$$\nThe length of the horizontal segment $S_y$ from $(-1,0)$ to $(1,0)$ is:\n$$L_y = \\sqrt{(1 - (-1))^2 + (0 - 0)^2} = \\sqrt{2^2 + 0^2} = 2$$\nThe total length of the union of two sets is the sum of their individual lengths minus the length of their intersection. The intersection of the two segments is the single point where both $x=0$ and $y=0$:\n$$S_x \\cap S_y = \\{(0,0)\\}$$\nA single point has a one-dimensional measure (length) of $0$. Therefore, the total length $L$ of $S^*$ is:\n$$L = L_x + L_y - \\text{Length}(S_x \\cap S_y) = 2 + 2 - 0 = 4$$\nAlternatively, we can view the cross shape as being composed of four distinct line segments of length $1$, all meeting at the origin: the segment from $(-1,0)$ to $(0,0)$, the segment from $(0,0)$ to $(1,0)$, the segment from $(0,-1)$ to $(0,0)$, and the segment from $(0,0)$ to $(0,1)$. The sum of their lengths is $1+1+1+1 = 4$. Both methods yield the same result. The total length is $4$.",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "Building on the idea of structured solution sets, this practice explores how problem constraints can fundamentally shape the set of maximizers. We will compare the outcomes of maximizing a linear function over two different geometric domains: the smooth $\\ell_2$ unit ball and the \"pointy\" $\\ell_1$ unit ball. This comparison is not just a theoretical exercise; it lies at the heart of understanding why some optimization methods used in machine learning, like Lasso regression, tend to produce \"sparse\" solutions .",
            "id": "3098617",
            "problem": "Let $n \\in \\mathbb{N}$ and let $a \\in \\mathbb{R}^{n}$ be a nonzero vector. Consider the two optimization problems of maximizing the same linear functional over two different unit balls:\n- $A_{1} \\coloneqq \\operatorname{argmax}\\{a^{\\top}x : \\|x\\|_{1} \\leq 1\\}$,\n- $A_{2} \\coloneqq \\operatorname{argmax}\\{a^{\\top}x : \\|x\\|_{2} \\leq 1\\}$,\nwhere $\\|\\cdot\\|_{1}$ denotes the $\\ell_{1}$ norm and $\\|\\cdot\\|_{2}$ denotes the Euclidean norm.\n\nStarting only from the definitions of $\\operatorname{argmax}$, $\\ell_{1}$ and $\\ell_{2}$ norms, the triangle inequality, and the Cauchy–Schwarz inequality, determine the exact forms of the sets $A_{1}$ and $A_{2}$, and identify their affine dimensions (that is, the dimension of the smallest affine subspace containing each set). Let $M \\coloneqq \\max_{1 \\leq i \\leq n} |a_{i}|$ and $J \\coloneqq \\{i \\in \\{1,\\dots,n\\} : |a_{i}| = M\\}$, and denote $k \\coloneqq |J|$. Assume $k \\geq 1$ since $a \\neq 0$.\n\nCompute the quantity $\\Delta \\coloneqq \\dim(A_{1}) - \\dim(A_{2})$ in closed form as a function of $k$. Your final answer must be a single analytic expression in $k$. No rounding is required.",
            "solution": "We first recall the definitions and fundamental inequalities to be used. For a set $S \\subset \\mathbb{R}^{n}$ and a function $f : \\mathbb{R}^{n} \\to \\mathbb{R}$, $\\operatorname{argmax}_{x \\in S} f(x)$ denotes the set of points $x \\in S$ at which $f$ attains its maximum over $S$. The $\\ell_{1}$ norm is $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$, and the $\\ell_{2}$ norm is $\\|x\\|_{2} = \\left(\\sum_{i=1}^{n} x_{i}^{2}\\right)^{1/2}$. We will use the triangle inequality $|u+v| \\leq |u| + |v|$ and the Cauchy–Schwarz inequality $|u^{\\top}v| \\leq \\|u\\|_{2}\\|v\\|_{2}$, together with the elementary bound $\\sum_{i=1}^{n} |a_{i}||x_{i}| \\leq \\left(\\max_{i} |a_{i}|\\right)\\sum_{i=1}^{n} |x_{i}|$.\n\nWe analyze the two problems separately and then compare the resulting argmax sets.\n\nStep $1$: Characterization of $A_{2}$ under the $\\ell_{2}$ constraint. For any $x$ with $\\|x\\|_{2} \\leq 1$, apply the Cauchy–Schwarz inequality:\n$$\na^{\\top}x \\leq \\|a\\|_{2}\\|x\\|_{2} \\leq \\|a\\|_{2}.\n$$\nHence, the optimal value is at most $\\|a\\|_{2}$. Equality in the Cauchy–Schwarz inequality holds if and only if $x$ is a scalar multiple of $a$, that is, $x = \\lambda a$ for some $\\lambda \\in \\mathbb{R}$. To also satisfy $\\|x\\|_{2} \\leq 1$ and achieve the upper bound $\\|a\\|_{2}$, we need $\\|x\\|_{2} = 1$ and $a^{\\top}x = \\|a\\|_{2}\\|x\\|_{2} = \\|a\\|_{2}$. This requires $x$ to be in the same direction as $a$, i.e., $x = a/\\|a\\|_{2}$. Thus,\n$$\nA_{2} = \\left\\{\\frac{a}{\\|a\\|_{2}}\\right\\}.\n$$\nThis set is a singleton, so its affine dimension is $0$, that is, $\\dim(A_{2}) = 0$.\n\nStep $2$: Characterization of $A_{1}$ under the $\\ell_{1}$ constraint. For any $x$ with $\\|x\\|_{1} \\leq 1$, we bound:\n$$\na^{\\top}x = \\sum_{i=1}^{n} a_{i} x_{i} \\leq \\sum_{i=1}^{n} |a_{i}||x_{i}| \\leq \\left(\\max_{1 \\leq i \\leq n} |a_{i}|\\right) \\sum_{i=1}^{n} |x_{i}| \\leq M.\n$$\nThus, the optimal value is at most $M$. We now characterize when equality holds simultaneously in the chain of inequalities:\n- For the first inequality, we require $a_{i}x_{i} = |a_{i}||x_{i}|$ for each $i$, which is equivalent to $x_{i}$ having the same sign as $a_{i}$ (or $x_{i} = 0$).\n- For the second inequality, we require that whenever $|x_{i}|  0$, it must be that $|a_{i}| = M$; that is, the support of $x$ must be contained in $J = \\{i : |a_{i}| = M\\}$.\n- For the third inequality, we need $\\|x\\|_{1} = 1$.\n\nCombining these conditions, the equality case is exactly the set of $x \\in \\mathbb{R}^{n}$ such that:\n$$\nx_{i} = 0 \\text{ for } i \\notin J,\\quad x_{i} \\text{ has sign } \\operatorname{sign}(a_{i}) \\text{ for } i \\in J,\\quad \\sum_{i \\in J} |x_{i}| = 1.\n$$\nEquivalently, letting $t_{i} \\coloneqq |x_{i}| \\geq 0$ for $i \\in J$, the condition becomes\n$$\n\\sum_{i \\in J} t_{i} = 1,\\quad t_{i} \\geq 0 \\text{ for each } i \\in J,\n$$\nwith $x_{i} = \\operatorname{sign}(a_{i})\\,t_{i}$ for $i \\in J$ and $x_{i} = 0$ for $i \\notin J$. The set $\\{t \\in \\mathbb{R}^{k}_{\\ge 0} : \\sum_{i \\in J} t_{i} = 1\\}$ is the standard simplex of dimension $k-1$, and the map $t \\mapsto x$ that assigns the fixed signs and embeds into $\\mathbb{R}^{n}$ is an affine isomorphism onto $A_{1}$. Therefore,\n$$\n\\dim(A_{1}) = k - 1.\n$$\n\nStep $3$: Compute the difference in affine dimensions. From the above,\n$$\n\\Delta \\coloneqq \\dim(A_{1}) - \\dim(A_{2}) = (k - 1) - 0 = k - 1.\n$$\nThis formula is valid for any nonzero $a$ with $k = |J| \\geq 1$. In the special case $k = 1$, both argmax sets are singletons and $\\Delta = 0$; when $k  1$, the $\\ell_{1}$ argmax set is a $(k-1)$-dimensional face, while the $\\ell_{2}$ argmax set remains a single point, yielding $\\Delta = k - 1$ as derived.",
            "answer": "$$\\boxed{k-1}$$"
        },
        {
            "introduction": "Theory comes to life when tested in practice. This hands-on exercise transitions from pure analysis to computational exploration by examining how an algorithm navigates the landscape around a minimum. You will implement Gradient Descent on the function $f(x) = \\sqrt{|x|}$ and observe how its behavior is affected by the function's extreme steepness near its unique minimizer, which introduces the practical relevance of $\\varepsilon$-near-argmin sets .",
            "id": "3098631",
            "problem": "Consider the function $f:\\mathbb{R}\\to\\mathbb{R}$ defined by $f(x)=\\sqrt{|x|}$. The aim is to analyze the $\\operatorname{argmin}$ set and $\\varepsilon$-near-$\\operatorname{argmin}$ sets, verify regularity properties, and empirically demonstrate how a Gradient Descent (GD) method can wander in and out of a nontrivial near-$\\operatorname{argmin}$ region for a non-Lipschitz function. Use only the definitions and fundamental properties specified below.\n\nFundamental definitions to employ:\n- The $\\operatorname{argmin}$ set of $f$ is $\\operatorname{argmin} f := \\{x\\in\\mathbb{R} : f(x) = \\inf_{y\\in\\mathbb{R}} f(y)\\}$.\n- The $\\varepsilon$-near-$\\operatorname{argmin}$ set (also called the $\\varepsilon$-approximate minimizer set) is $A_{\\varepsilon} := \\{x\\in\\mathbb{R} : f(x) \\le \\inf_{y\\in\\mathbb{R}} f(y) + \\varepsilon \\}$ for any $\\varepsilon  0$.\n- A function $f$ is Lipschitz continuous with constant $L0$ if $|f(x)-f(y)| \\le L|x-y|$ for all $x,y\\in\\mathbb{R}$.\n- Gradient Descent (GD) with fixed step size $\\alpha0$ updates $x_{k+1} := x_k - \\alpha \\nabla f(x_k)$ when the gradient exists; if the gradient does not exist at a point, take $\\nabla f(0):=0$ for the purpose of this simulation to avoid undefined iteration.\n\nTasks:\n1. Prove from first principles that $f(x)=\\sqrt{|x|}$ has a unique minimizer, determine $\\operatorname{argmin} f$, and verify that $f$ is not Lipschitz continuous on any neighborhood of $x=0$.\n2. Characterize $A_{\\varepsilon}$ explicitly for general $\\varepsilon0$ and derive its width in terms of $\\varepsilon$.\n3. Implement GD for $f$ using the derivative where it exists: for $x\\neq 0$, use $\\nabla f(x)=\\frac{1}{2}\\frac{\\operatorname{sign}(x)}{\\sqrt{|x|}}$, and for $x=0$ set $\\nabla f(0):=0$. Run the iteration $x_{k+1} := x_k - \\alpha \\nabla f(x_k)$ for $k=0,1,\\dots,N-1$.\n4. For each simulation, compute the following quantitative indicators:\n   - The width of $A_{\\varepsilon}$ in the $x$-domain, which is the length of the interval $A_{\\varepsilon}$.\n   - The fraction of iterates inside $A_{\\varepsilon}$, defined as $\\frac{1}{N}\\sum_{k=0}^{N-1} \\mathbf{1}\\{|x_k|\\le \\varepsilon^2\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n   - The number of boundary crossings of $A_{\\varepsilon}$, defined as the count of toggles of membership in $A_{\\varepsilon}$ over the iteration sequence $\\{x_k\\}_{k=0}^{N-1}$ (i.e., how many times the sequence switches between being inside and outside $A_{\\varepsilon}$).\n   - The number of sign changes in $\\{x_k\\}$, defined as the total count of indices $k$ such that $\\operatorname{sign}(x_{k}) \\ne \\operatorname{sign}(x_{k-1})$ with the convention $\\operatorname{sign}(0):=0$.\n5. Use the following test suite, which is designed to cover the general behavior and edge cases:\n   - Test 1 (happy path, moderate step size): $(x_0,\\alpha,\\varepsilon,N) = (1.0,\\,0.2,\\,0.1,\\,200)$.\n   - Test 2 (smaller step size): $(x_0,\\alpha,\\varepsilon,N) = (1.0,\\,0.02,\\,0.1,\\,200)$.\n   - Test 3 (boundary minimizer start): $(x_0,\\alpha,\\varepsilon,N) = (0.0,\\,0.2,\\,0.1,\\,200)$.\n   - Test 4 (near-zero start with moderate step size): $(x_0,\\alpha,\\varepsilon,N) = (10^{-6},\\,0.2,\\,0.1,\\,200)$.\n\nFinal output specification:\n- For each test case, your program must output a list $[w,\\rho,c,s]$ where $w$ is the width of $A_{\\varepsilon}$ (a float), $\\rho$ is the inside fraction (a float), $c$ is the number of boundary crossings (an integer), and $s$ is the number of sign changes (an integer).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the four test-case results in order. For example, the output format must look like $[[w_1,\\rho_1,c_1,s_1],[w_2,\\rho_2,c_2,s_2],[w_3,\\rho_3,c_3,s_3],[w_4,\\rho_4,c_4,s_4]]$.\n- No physical units or angle units are involved in this problem. All outputs are real-valued floats or integers as specified.",
            "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded in real analysis and optimization theory, well-posed with all necessary definitions and parameters, and objective in its formulation. We may therefore proceed with a full solution.\n\nThe problem requires a multi-part analysis of the function $f(x) = \\sqrt{|x|}$, focusing on its minimization properties and the behavior of the Gradient Descent (GD) algorithm applied to it. This function is a canonical example of a non-smooth, non-Lipschitz objective that poses challenges for standard optimization methods.\n\n### Part 1: Analysis of the Minimizer and Lipschitz Continuity\n\nFirst, we prove that $f(x)=\\sqrt{|x|}$ has a unique minimizer and determine the $\\operatorname{argmin}$ set. The domain of $f$ is $\\mathbb{R}$. The absolute value function $|x|$ is always non-negative, i.e., $|x| \\ge 0$ for all $x \\in \\mathbb{R}$. The square root function is monotonically increasing for non-negative inputs. Therefore, $f(x) = \\sqrt{|x|} \\ge \\sqrt{0} = 0$. The minimum possible value of $f(x)$ is $0$.\n\nThe infimum of the function is $\\inf_{y\\in\\mathbb{R}} f(y) = 0$. We now identify the set of points where this infimum is attained. This is the $\\operatorname{argmin}$ set:\n$$ \\operatorname{argmin} f = \\{x\\in\\mathbb{R} : f(x) = 0\\} $$\n$$ f(x) = 0 \\iff \\sqrt{|x|} = 0 \\iff |x| = 0 \\iff x = 0 $$\nThus, the minimizer is unique and is located at $x=0$.\n$$ \\operatorname{argmin} f = \\{0\\} $$\n\nNext, we verify that $f$ is not Lipschitz continuous on any neighborhood of $x=0$. A function $f$ is Lipschitz continuous on a set $S \\subseteq \\mathbb{R}$ if there exists a constant $L0$ such that for all $x, y \\in S$, the inequality $|f(x)-f(y)| \\le L|x-y|$ holds.\nLet's consider any neighborhood of $0$, say $(-\\delta, \\delta)$ for some $\\delta  0$. To show that $f$ is not Lipschitz on this neighborhood, we must show that the ratio $\\frac{|f(x)-f(y)|}{|x-y|}$ is unbounded for $x, y \\in (-\\delta, \\delta)$.\nLet's choose $y=0$ and $x \\in (0, \\delta)$.\nThen $f(y) = f(0) = 0$ and $f(x) = \\sqrt{x}$. The ratio becomes:\n$$ \\frac{|f(x)-f(0)|}{|x-0|} = \\frac{|\\sqrt{x}-0|}{|x-0|} = \\frac{\\sqrt{x}}{x} = \\frac{1}{\\sqrt{x}} $$\nAs we let $x$ approach $0$ from the positive side ($x \\to 0^+$), the value of this ratio $\\frac{1}{\\sqrt{x}}$ tends to infinity. Since this ratio is unbounded on any interval $(0, \\delta)$, no finite Lipschitz constant $L$ can be found that satisfies the definition for all points in any neighborhood of $0$. Therefore, $f(x)=\\sqrt{|x|}$ is not Lipschitz continuous on any neighborhood of $x=0$.\n\n### Part 2: Characterization of the $\\varepsilon$-near-$\\operatorname{argmin}$ Set\n\nThe $\\varepsilon$-near-$\\operatorname{argmin}$ set, $A_{\\varepsilon}$, is defined as:\n$$ A_{\\varepsilon} := \\{x\\in\\mathbb{R} : f(x) \\le \\inf_{y\\in\\mathbb{R}} f(y) + \\varepsilon \\} $$\nFrom Part 1, we know that $\\inf_{y\\in\\mathbb{R}} f(y) = 0$. Substituting this into the definition gives:\n$$ A_{\\varepsilon} = \\{x\\in\\mathbb{R} : \\sqrt{|x|} \\le \\varepsilon \\} $$\nSince $\\varepsilon  0$ and $\\sqrt{|x|} \\ge 0$, we can square both sides of the inequality without changing its direction:\n$$ |x| \\le \\varepsilon^2 $$\nThis inequality is equivalent to the interval $[-\\varepsilon^2, \\varepsilon^2]$. So, the explicit characterization of the set is:\n$$ A_{\\varepsilon} = [-\\varepsilon^2, \\varepsilon^2] $$\nThe width of $A_{\\varepsilon}$ is the length of this interval, which is calculated as the difference between its endpoints:\n$$ w = \\varepsilon^2 - (-\\varepsilon^2) = 2\\varepsilon^2 $$\nThis confirms that the condition $\\{|x_k|\\le \\varepsilon^2\\}$ used in the problem statement for calculating the fraction of iterates inside $A_\\varepsilon$ is indeed the correct condition for membership in $A_\\varepsilon$.\n\n### Part 3: Gradient Descent Simulation Analysis\n\nThe Gradient Descent (GD) update rule is $x_{k+1} := x_k - \\alpha \\nabla f(x_k)$. The gradient of $f(x)=\\sqrt{|x|}$ for $x \\neq 0$ is $\\nabla f(x) = \\frac{d}{dx} \\sqrt{|x|} = \\frac{\\operatorname{sign}(x)}{2\\sqrt{|x|}}$. The problem specifies using $\\nabla f(0) := 0$ for the simulation. This choice is a valid subgradient, as the subdifferential of $f$ at $x=0$ is $\\partial f(0)=\\mathbb{R}$.\n\nThe key feature of $\\nabla f(x)$ is that its magnitude, $|\\nabla f(x)| = \\frac{1}{2\\sqrt{|x|}}$, approaches infinity as $x \\to 0$. This violates the standard condition for GD convergence, which requires the gradient to be Lipschitz continuous (i.e., bounded rate of change). This unboundedness causes the algorithm to take extremely large steps when an iterate $x_k$ is near the minimizer $x=0\".\n\nThe GD update can be written piece-wise:\n$$ x_{k+1} = \\begin{cases} x_k - \\alpha \\frac{1}{2\\sqrt{x_k}}  \\text{if } x_k  0 \\\\ x_k - \\alpha \\frac{-1}{2\\sqrt{-x_k}}  \\text{if } x_k  0 \\\\ 0  \\text{if } x_k = 0 \\end{cases} $$\nIf an iterate lands exactly on $x=0$, it will remain there for all subsequent iterations. However, due to the nature of floating-point arithmetic and the update rule, this is only guaranteed if the starting point is $x_0=0$.\n\nWhen an iterate $x_k$ is small but non-zero (i.e., inside $A_\\varepsilon$), the term $\\frac{\\alpha}{2\\sqrt{|x_k|}}$ becomes very large. The next iterate $x_{k+1}$ will be approximately $-\\alpha \\nabla f(x_k)$, causing a large jump that overshoots the minimum at $x=0$ and lands far away on the other side of the origin. This leads to the \"wandering\" behavior where the sequence of iterates repeatedly enters the small neighborhood $A_\\varepsilon$ only to be immediately ejected. This will manifest as a high number of boundary crossings of $A_\\varepsilon$ and a high number of sign changes in the sequence $\\{x_k\\}$.\n\n- **Test 1 ($x_0=1.0, \\alpha=0.2, \\varepsilon=0.1, N=200$)**: With a relatively large step size $\\alpha$, the overshoot phenomenon will be pronounced. We expect a low fraction of iterates to remain inside $A_\\varepsilon$, and a high number of boundary crossings and sign changes.\n- **Test 2 ($x_0=1.0, \\alpha=0.02, \\varepsilon=0.1, N=200$)**: The step size is ten times smaller. The step magnitude, $\\alpha |\\nabla f(x_k)|$, will be smaller, leading to smaller overshoots. The iterates will still wander, but less violently. We expect a higher fraction of iterates inside $A_\\varepsilon$ and fewer crossings and sign changes compared to Test 1.\n- **Test 3 ($x_0=0.0, \\alpha=0.2, \\varepsilon=0.1, N=200$)**: The starting point is the minimizer. Since we define $\\nabla f(0)=0$, the first update is $x_1 = 0 - \\alpha \\cdot 0 = 0$. All subsequent iterates will be $0$. Thus, all iterates will be inside $A_\\varepsilon$, and there will be zero boundary crossings and zero sign changes.\n- **Test 4 ($x_0=10^{-6}, \\alpha=0.2, \\varepsilon=0.1, N=200$)**: The starting point is very close to $0$ and inside $A_\\varepsilon = [-0.01, 0.01]$. The gradient magnitude $|\\nabla f(10^{-6})|$ will be immense ($\\approx 500$). The update step will be enormous, causing a dramatic overshoot. This case will most clearly demonstrate the instability, exhibiting a very low fraction of iterates inside $A_\\varepsilon$ (after the first one) and many crossings and sign changes.\n\nThese theoretical expectations will be confirmed by the numerical simulation implemented in the final answer.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by running Gradient Descent simulations\n    for the function f(x) = sqrt(|x|) and calculating specified metrics.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (x_0, alpha, epsilon, N)\n        (1.0, 0.2, 0.1, 200),\n        (1.0, 0.02, 0.1, 200),\n        (0.0, 0.2, 0.1, 200),\n        (1e-6, 0.2, 0.1, 200),\n    ]\n\n    results = []\n    for x0, alpha, epsilon, N in test_cases:\n        # --- Run Gradient Descent Simulation ---\n        x_iterates = np.zeros(N, dtype=np.float64)\n        x_iterates[0] = x0\n\n        for k in range(N - 1):\n            x_k = x_iterates[k]\n            \n            # Calculate gradient based on the problem's definition\n            if x_k == 0.0:\n                grad_f = 0.0\n            else:\n                grad_f = np.sign(x_k) / (2.0 * np.sqrt(np.abs(x_k)))\n\n            # Update rule for Gradient Descent\n            x_k_plus_1 = x_k - alpha * grad_f\n            x_iterates[k + 1] = x_k_plus_1\n            \n        # --- Compute Quantitative Indicators ---\n\n        # 1. Width of A_epsilon\n        # A_epsilon is the interval [-epsilon^2, epsilon^2]\n        # Width w = epsilon^2 - (-epsilon^2) = 2 * epsilon^2\n        w = 2.0 * epsilon**2\n\n        # 2. Fraction of iterates inside A_epsilon\n        # The condition for being inside A_epsilon is |x_k| = epsilon^2\n        is_inside = np.abs(x_iterates) = epsilon**2\n        rho = np.mean(is_inside)\n\n        # 3. Number of boundary crossings of A_epsilon\n        # A crossing occurs if an iterate is inside and the next is outside, or vice versa.\n        # This is equivalent to checking where the 'is_inside' status toggles.\n        # We compare element k with element k-1 for k=1...N-1.\n        crossings = np.sum(is_inside[1:] != is_inside[:-1])\n        c = int(crossings)\n        \n        # 4. Number of sign changes in {x_k}\n        # A sign change occurs if sign(x_k) != sign(x_{k-1})\n        # np.sign(0) is 0, which correctly handles iterates that are exactly zero.\n        signs = np.sign(x_iterates)\n        sign_changes = np.sum(signs[1:] != signs[:-1])\n        s = int(sign_changes)\n\n        # Append the results for the current test case\n        results.append([w, rho, c, s])\n\n    # Final print statement in the exact required format.\n    # The map(str, results) ensures each inner list is converted to its string representation.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}