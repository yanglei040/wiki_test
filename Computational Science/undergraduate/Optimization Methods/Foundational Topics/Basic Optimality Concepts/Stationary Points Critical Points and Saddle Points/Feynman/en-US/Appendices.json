{
    "hands_on_practices": [
        {
            "introduction": "Mastering optimization begins with the fundamental skill of identifying and classifying stationary points. This first exercise provides a complete workout on a classic quadratic function, guiding you from finding a stationary point using the gradient to classifying it with the Hessian matrix. By diagonalizing the Hessian, you will directly uncover the principal curvatures and see how a change of coordinates reveals the function's intrinsic saddle geometry, connecting abstract matrix properties to a tangible shape .",
            "id": "3184889",
            "problem": "Consider the twice continuously differentiable function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by $f(x,y)=x^{2}+xy-y^{2}$. Use the fundamental definition that a stationary point is a point where the gradient is zero, and classify stationary points by the definiteness of the Hessian matrix. Then, starting from the fact that any real symmetric matrix is orthogonally diagonalizable, determine a rotation that diagonalizes the Hessian and write the quadratic form in the rotated coordinates to reveal the saddle nature.\n\nTasks:\n- Derive and solve the stationary point conditions $\\nabla f(x,y)=\\mathbf{0}$.\n- Use the Hessian to classify the stationary point by definiteness.\n- Construct an orthogonal rotation matrix $R(\\varphi)$ with angle $\\varphi$ (in radians) that diagonalizes the Hessian, and show that in the rotated coordinates $(u,v)=R(\\varphi)^{\\top}(x,y)$ the quadratic form has no cross term.\n- Provide the exact values of the principal curvatures (the eigenvalues of the Hessian) and the exact rotation angle $\\varphi$.\n\nAnswer specification:\n- Express the angle in radians as an exact symbolic expression.\n- Express the principal curvatures exactly (do not approximate square roots).\n- As your final answer, give a single row matrix containing, in order: the rotation angle $\\varphi$, the larger principal curvature, and the smaller principal curvature. Do not include units inside the final row matrix.",
            "solution": "The problem is to find and classify the stationary points of the function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by $f(x,y)=x^{2}+xy-y^{2}$, and to find a coordinate rotation that diagonalizes the associated quadratic form.\n\nFirst, we validate the problem statement.\n**Step 1: Extract Givens**\n- Function: $f(x,y)=x^{2}+xy-y^{2}$, twice continuously differentiable.\n- Definition of stationary point: $\\nabla f(x,y)=\\mathbf{0}$.\n- Classification method: Definiteness of the Hessian matrix.\n- Task: Find an orthogonal rotation matrix $R(\\varphi)$ to diagonalize the Hessian.\n- Task: Express the quadratic form in rotated coordinates $(u,v)=R(\\varphi)^{\\top}(x,y)$.\n- Required outputs: The stationary point, its classification, the principal curvatures (eigenvalues), and the rotation angle $\\varphi$.\n- Final answer format: A row matrix containing the rotation angle, the larger principal curvature, and the smaller principal curvature.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard exercise in multivariable calculus, involving gradients, Hessians, eigenvalues, and matrix diagonalization. These are core mathematical concepts. The problem is scientifically sound.\n- **Well-Posed:** The function is explicitly defined, and the tasks are unambiguous. The procedures for finding stationary points and diagonalizing symmetric matrices are standard, leading to a unique and meaningful solution. The problem is well-posed.\n- **Objective:** The problem is stated using formal mathematical language, free of any subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We will proceed with a full solution.\n\nThe function is given by $f(x,y) = x^{2}+xy-y^{2}$.\n\n**1. Find the Stationary Point**\nA stationary point $(x_0, y_0)$ is a point where the gradient of the function is the zero vector, i.e., $\\nabla f(x_0, y_0) = \\mathbf{0}$. The gradient of $f$ is:\n$$\n\\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 2x+y \\\\ x-2y \\end{pmatrix}\n$$\nSetting the gradient to zero gives the following system of linear equations:\n$$\n\\begin{cases}\n2x+y = 0 \\\\\nx-2y = 0\n\\end{cases}\n$$\nFrom the second equation, we have $x=2y$. Substituting this into the first equation:\n$$\n2(2y)+y=0 \\implies 4y+y=0 \\implies 5y=0 \\implies y=0\n$$\nSubstituting $y=0$ back into $x=2y$ gives $x=0$.\nThus, the only stationary point is $(0,0)$.\n\n**2. Classify the Stationary Point using the Hessian Matrix**\nThe Hessian matrix $H$ is the matrix of second partial derivatives:\n$$\nH(x,y) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix}\n$$\nThe second partial derivatives are:\n$$\n\\frac{\\partial^2 f}{\\partial x^2} = \\frac{\\partial}{\\partial x}(2x+y) = 2\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}(2x+y) = 1\n$$\n$$\n\\frac{\\partial^2 f}{\\partial y^2} = \\frac{\\partial}{\\partial y}(x-2y) = -2\n$$\nSince $f$ is twice continuously differentiable, $\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y} = 1$. The Hessian matrix is constant for all $(x,y)$:\n$$\nH = \\begin{pmatrix} 2 & 1 \\\\ 1 & -2 \\end{pmatrix}\n$$\nTo classify the stationary point $(0,0)$, we analyze the definiteness of $H$. We do this by finding its eigenvalues, which are the principal curvatures. The characteristic equation is $\\det(H-\\lambda I)=0$:\n$$\n\\det\\begin{pmatrix} 2-\\lambda & 1 \\\\ 1 & -2-\\lambda \\end{pmatrix} = 0\n$$\n$$\n(2-\\lambda)(-2-\\lambda) - (1)(1) = 0\n$$\n$$\n-4-2\\lambda+2\\lambda+\\lambda^2 - 1 = 0\n$$\n$$\n\\lambda^2 - 5 = 0 \\implies \\lambda = \\pm\\sqrt{5}\n$$\nThe eigenvalues (principal curvatures) are $\\lambda_1 = \\sqrt{5}$ and $\\lambda_2 = -\\sqrt{5}$. Since one eigenvalue is positive and the other is negative, the Hessian matrix is indefinite. A stationary point with an indefinite Hessian is a saddle point.\n\n**3. Determine the Rotation to Diagonalize the Hessian**\nWe seek an orthogonal rotation matrix $R(\\varphi) = \\begin{pmatrix} \\cos\\varphi & -\\sin\\varphi \\\\ \\sin\\varphi & \\cos\\varphi \\end{pmatrix}$ that diagonalizes $H$. The columns of $R(\\varphi)$ must be the normalized eigenvectors of $H$. The transformation is given by $D = R(\\varphi)^\\top H R(\\varphi)$, where $D$ is the diagonal matrix of eigenvalues.\nThe angle of rotation $\\varphi$ that aligns the coordinate axes with the principal axes of the quadratic form $Ax^2+Bxy+Cy^2$ is given by $\\tan(2\\varphi) = \\frac{B}{A-C}$. The quadratic form associated with the Hessian $H = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}$ is $q(x,y) = ax^2 + 2bxy + cy^2$. For our Hessian $H = \\begin{pmatrix} 2 & 1 \\\\ 1 & -2 \\end{pmatrix}$, we have $a=2$, $b=1$, $c=-2$. The formula for the rotation angle to diagonalize the matrix itself is $\\tan(2\\varphi)=\\frac{2b}{a-c}$.\n$$\n\\tan(2\\varphi) = \\frac{2(1)}{2 - (-2)} = \\frac{2}{4} = \\frac{1}{2}\n$$\nWe can solve for $\\varphi$. Let us choose $2\\varphi$ to be in the first quadrant, so $\\varphi$ is in $(0, \\pi/4)$. We use the half-angle identity for tangent: $\\tan(2\\varphi) = \\frac{2\\tan\\varphi}{1-\\tan^2\\varphi}$.\n$$\n\\frac{1}{2} = \\frac{2\\tan\\varphi}{1-\\tan^2\\varphi}\n$$\n$$\n1-\\tan^2\\varphi = 4\\tan\\varphi\n$$\n$$\n\\tan^2\\varphi + 4\\tan\\varphi - 1 = 0\n$$\nThis is a quadratic equation for $\\tan\\varphi$. Using the quadratic formula:\n$$\n\\tan\\varphi = \\frac{-4 \\pm \\sqrt{4^2 - 4(1)(-1)}}{2(1)} = \\frac{-4 \\pm \\sqrt{16+4}}{2} = \\frac{-4 \\pm \\sqrt{20}}{2} = \\frac{-4 \\pm 2\\sqrt{5}}{2} = -2 \\pm \\sqrt{5}\n$$\nSince we chose $\\varphi \\in (0, \\pi/4)$, $\\tan\\varphi$ must be positive and less than $1$.\nThe value $\\sqrt{5}-2 \\approx 2.236-2=0.236$ satisfies $0 < \\sqrt{5}-2 < 1$.\nThe other value, $-2-\\sqrt{5}$, is negative.\nThus, we must have $\\tan\\varphi = \\sqrt{5}-2$. The rotation angle is $\\varphi = \\arctan(\\sqrt{5}-2)$.\n\n**4. Express Quadratic Form in Rotated Coordinates**\nThe coordinate transformation is given by $\\begin{pmatrix} x \\\\ y \\end{pmatrix} = R(\\varphi)\\begin{pmatrix} u \\\\ v \\end{pmatrix}$. The quadratic part of the function is the function itself, $f(x,y)=x^2+xy-y^2$. This corresponds to the matrix $A = \\begin{pmatrix} 1 & 1/2 \\\\ 1/2 & -1 \\end{pmatrix}$. Note that $H=2A$. The quadratic form is $\\mathbf{x}^\\top A \\mathbf{x}$.\nIn the new coordinates $(u,v)$, the quadratic form becomes:\n$$\nf(u,v) = \\left(R(\\varphi)\\begin{pmatrix} u \\\\ v \\end{pmatrix}\\right)^\\top A \\left(R(\\varphi)\\begin{pmatrix} u \\\\ v \\end{pmatrix}\\right) = \\begin{pmatrix} u \\\\ v \\end{pmatrix}^\\top (R(\\varphi)^\\top A R(\\varphi)) \\begin{pmatrix} u \\\\ v \\end{pmatrix}\n$$\nThe matrix $R(\\varphi)^\\top A R(\\varphi)$ is a diagonal matrix whose diagonal entries are the eigenvalues of $A$. The eigenvalues of $A = \\frac{1}{2}H$ are half the eigenvalues of $H$, which are $\\frac{\\sqrt{5}}{2}$ and $-\\frac{\\sqrt{5}}{2}$.\nThus, in the rotated coordinate system, the function is:\n$$\nf(u,v) = \\frac{\\sqrt{5}}{2}u^2 - \\frac{\\sqrt{5}}{2}v^2\n$$\nThis form explicitly shows the saddle nature of the function at the origin, with parabolic curvature upwards along the $u$-axis and downwards along the $v$-axis. There is no cross term $uv$.\n\n**Summary of Results**\n- The stationary point is $(0,0)$, which is a saddle point.\n- The principal curvatures are the eigenvalues of the Hessian, which are $\\lambda_{\\text{max}} = \\sqrt{5}$ and $\\lambda_{\\text{min}} = -\\sqrt{5}$.\n- The rotation angle required to diagonalize the Hessian is $\\varphi = \\arctan(\\sqrt{5}-2)$ radians.\n\nThe final answer requires a single row matrix containing the rotation angle $\\varphi$, the larger principal curvature, and the smaller principal curvature, in that order.\n$$\n\\begin{pmatrix} \\varphi & \\lambda_{\\text{max}} & \\lambda_{\\text{min}} \\end{pmatrix} = \\begin{pmatrix} \\arctan(\\sqrt{5}-2) & \\sqrt{5} & -\\sqrt{5} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\arctan(\\sqrt{5}-2) & \\sqrt{5} & -\\sqrt{5} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The second-derivative test is a powerful tool, but what happens when it yields an inconclusive result? This practice explores such a scenario, where the Hessian matrix at the critical points is singular, rendering the standard test useless. You will learn to move beyond quadratic approximations and classify points by directly analyzing the function's behavior along different directions, revealing a rich landscape of non-strict minima and a subtle saddle point that standard tests miss .",
            "id": "3184903",
            "problem": "Let $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ be defined by $f(x,y)=x^{2}y$. Using only core definitions from unconstrained optimization, proceed as follows:\n- Identify the complete set of critical points by solving $\\nabla f(x,y)=\\mathbf{0}$ from first principles.\n- For each critical point, discuss how the Hessian matrix’s rank affects the applicability of the classical second-order test, and explain why the classification may be inconclusive when the Hessian is rank-deficient.\n- Classify the critical points into local minima, local maxima, or saddle points using rigorous arguments based on restrictions of $f$ along curves. In particular, classify the origin by restricting $f$ to rays of the form $(x,y)=t\\mathbf{u}$ with $\\|\\mathbf{u}\\|=1$ and small $t$, and justify your conclusion from first principles without invoking any unproven shortcut formulas.\n- Define the leading-order directional profile $g(u_{x},u_{y})$ by the smallest positive integer $k$ for which the expansion $f(tu_{x},tu_{y})=t^{k}\\,g(u_{x},u_{y})$ holds for all unit directions $(u_{x},u_{y})$ and sufficiently small $t$, and use this $g$ to support your classification of the origin.\n\nReport your final answer as the single closed-form analytic expression for $g(u_{x},u_{y})$. No rounding is required. Do not include units.",
            "solution": "We start from the foundational definitions. A point $(x^{\\star},y^{\\star})$ is a critical point of $f$ if and only if the gradient $\\nabla f(x^{\\star},y^{\\star})=\\mathbf{0}$. The Hessian matrix determines the second-order approximation of $f$ and is used in the classical second-order condition; when the Hessian is singular (rank-deficient), the quadratic form does not capture all nearby variations and the test may be inconclusive, in which case higher-order or directional analyses are required.\n\nStep $1$: Compute the gradient and critical set. The partial derivatives are\n$$\n\\frac{\\partial f}{\\partial x}(x,y)=2xy,\\qquad \\frac{\\partial f}{\\partial y}(x,y)=x^{2}.\n$$\nSetting $\\nabla f(x,y)=\\mathbf{0}$ yields the system\n$$\n2xy=0,\\qquad x^{2}=0.\n$$\nFrom $x^{2}=0$ we obtain $x=0$. Substituting into $2xy=0$ gives $0=0$, which imposes no further restriction on $y$. Therefore, the entire vertical axis is critical:\n$$\n\\mathcal{C}=\\{(0,y):y\\in\\mathbb{R}\\}.\n$$\n\nStep $2$: Examine the Hessian and the second-order test. The Hessian of $f$ is\n$$\n\\nabla^{2} f(x,y)=\\begin{pmatrix}\n\\frac{\\partial^{2}f}{\\partial x^{2}} & \\frac{\\partial^{2}f}{\\partial x\\partial y}\\\\\n\\frac{\\partial^{2}f}{\\partial y\\partial x} & \\frac{\\partial^{2}f}{\\partial y^{2}}\n\\end{pmatrix}\n=\\begin{pmatrix}\n2y & 2x\\\\\n2x & 0\n\\end{pmatrix}.\n$$\nAt a critical point $(0,y_{0})$, this reduces to\n$$\n\\nabla^{2} f(0,y_{0})=\\begin{pmatrix}\n2y_{0} & 0\\\\\n0 & 0\n\\end{pmatrix}.\n$$\nFor $y_{0}\\neq 0$, the Hessian has rank $1$ (one nonzero eigenvalue and one zero eigenvalue), hence it is singular. For $y_{0}=0$ (the origin), the Hessian is the zero matrix and has rank $0$. In all cases on $\\mathcal{C}$ the Hessian is rank-deficient, so the classical second-order test is inconclusive: a positive semidefinite Hessian does not guarantee a local minimum, a negative semidefinite Hessian does not guarantee a local maximum, and a singular Hessian cannot certify a saddle without further analysis.\n\nStep $3$: Classify nonzero critical points $(0,y_{0})$ with $y_{0}\\neq 0$. Fix $y_{0}>0$. Consider any sufficiently small neighborhood of $(0,y_{0})$ in which $y>0$ is maintained, for instance any ball of radius $r$ with $r<y_{0}$. In such a neighborhood, $f(x,y)=x^{2}y\\ge 0$ with equality if and only if $x=0$. Since $f(0,y_{0})=0$, it follows that $(0,y_{0})$ is a local minimum, but not strict, because $f(0,y)=0$ for all $y$ in the neighborhood along the line $x=0$. Similarly, for $y_{0}<0$, in a sufficiently small neighborhood one has $y<0$, so $f(x,y)=x^{2}y\\le 0$, with equality along $x=0$, implying that $(0,y_{0})$ is a local maximum, again non-strict.\n\nStep $4$: Classify the origin using directional tests beyond second order. At the origin, the Hessian is the zero matrix, so the quadratic approximation vanishes and cannot inform the local behavior. We therefore examine the restriction of $f$ along rays through the origin. Let $\\mathbf{u}=(u_{x},u_{y})$ be any unit direction, so $\\|\\mathbf{u}\\|=1$, and consider the curve $(x,y)=(tu_{x},tu_{y})$ parameterized by $t$. Substituting into $f$ gives\n$$\nf(tu_{x},tu_{y})=(tu_{x})^{2}\\,(tu_{y})=t^{3}\\,u_{x}^{2}\\,u_{y}.\n$$\nThis identity holds exactly for all $t$ because $f$ is a homogeneous polynomial of degree $3$. The leading-order term in $t$ is of order $t^{3}$, and its directional coefficient is the cubic form\n$$\ng(u_{x},u_{y})=u_{x}^{2}\\,u_{y}.\n$$\nBecause there exist unit directions with $g(u_{x},u_{y})>0$ (for example, $u_{x}=\\frac{1}{\\sqrt{2}}$, $u_{y}=\\frac{1}{\\sqrt{2}}$) and also unit directions with $g(u_{x},u_{y})<0$ (for example, $u_{x}=\\frac{1}{\\sqrt{2}}$, $u_{y}=-\\frac{1}{\\sqrt{2}}$), it follows that in every neighborhood of the origin there are points where $f>0$ and points where $f<0$. Therefore, the origin is a saddle point. This conclusion illustrates precisely how the rank-deficiency of the Hessian complicates classification: the vanishing quadratic form forces us to appeal to higher-order (here cubic) directional behavior to detect the change of sign that certifies a saddle.\n\nSummary of classifications:\n- For $(0,y_{0})$ with $y_{0}>0$: local (non-strict) minimum.\n- For $(0,y_{0})$ with $y_{0}<0$: local (non-strict) maximum.\n- For $(0,0)$: saddle point, detected via the directional cubic profile $g(u_{x},u_{y})$.\n\nPer the problem’s reporting requirement, the analytic expression for the leading-order directional profile is $g(u_{x},u_{y})=u_{x}^{2}u_{y}$.",
            "answer": "$$\\boxed{u_{x}^{2}u_{y}}$$"
        },
        {
            "introduction": "Most real-world optimization problems involve constraints, which fundamentally changes how we locate and analyze stationary points. This exercise transitions from unconstrained to constrained optimization, introducing the powerful framework of Lagrange multipliers and the Karush-Kuhn-Tucker (KKT) conditions. You will find stationary points on a constraint surface and learn to classify them using the bordered Hessian, a critical tool for determining optimality under constraints .",
            "id": "3184940",
            "problem": "Consider the equality-constrained optimization problem of minimizing the quadratic function $f(x)=x_1^2+x_2^2$ subject to the smooth constraint $g(x)=x_1x_2-1=0$. Starting from the core definitions of constrained stationarity and the Karush–Kuhn–Tucker (KKT) conditions for equality constraints, and using second-order analysis based on the bordered Hessian, do the following: identify all constrained stationary points and classify each as a constrained local minimum, constrained local maximum, or constrained saddle point. Then compute the minimal objective value attained under the constraint. Provide as your final response only the minimal objective value as a single number. No rounding is required.",
            "solution": "The problem requires finding and classifying the stationary points of the function $f(x_1, x_2) = x_1^2 + x_2^2$ subject to the equality constraint $g(x_1, x_2) = x_1x_2 - 1 = 0$, and then determining the minimal value of $f$. We will use the method of Lagrange multipliers.\n\nFirst, we define the Lagrangian function $L(x_1, x_2, \\lambda)$, which combines the objective function and the constraint:\n$$L(x_1, x_2, \\lambda) = f(x_1, x_2) - \\lambda g(x_1, x_2) = x_1^2 + x_2^2 - \\lambda(x_1x_2 - 1)$$\nThe Karush–Kuhn–Tucker (KKT) conditions for a stationary point state that the gradient of the Lagrangian must be zero. This gives a system of equations:\n$$ \\nabla L(x_1, x_2, \\lambda) = 0 $$\nThe components of the gradient yield the following three equations:\n$$ \\frac{\\partial L}{\\partial x_1} = 2x_1 - \\lambda x_2 = 0 \\quad (1) $$\n$$ \\frac{\\partial L}{\\partial x_2} = 2x_2 - \\lambda x_1 = 0 \\quad (2) $$\n$$ \\frac{\\partial L}{\\partial \\lambda} = -(x_1x_2 - 1) = 0 \\implies x_1x_2 = 1 \\quad (3) $$\nFrom constraint $(3)$, we know that neither $x_1$ nor $x_2$ can be zero. We can thus solve for $\\lambda$ from equation $(1)$: $\\lambda = \\frac{2x_1}{x_2}$. Substituting this expression into equation $(2)$:\n$$ 2x_2 - \\left(\\frac{2x_1}{x_2}\\right)x_1 = 0 $$\n$$ 2x_2^2 = 2x_1^2 \\implies x_2^2 = x_1^2 $$\nThis implies that either $x_2 = x_1$ or $x_2 = -x_1$.\n\nCase 1: $x_2 = x_1$.\nSubstituting into the constraint equation $(3)$:\n$$ x_1(x_1) = 1 \\implies x_1^2 = 1 $$\nThis gives two solutions for $x_1$: $x_1 = 1$ and $x_1 = -1$.\nIf $x_1 = 1$, then $x_2 = 1$. This gives the stationary point $P_1 = (1, 1)$.\nIf $x_1 = -1$, then $x_2 = -1$. This gives the stationary point $P_2 = (-1, -1)$.\n\nCase 2: $x_2 = -x_1$.\nSubstituting into the constraint equation $(3)$:\n$$ x_1(-x_1) = 1 \\implies -x_1^2 = 1 $$\nThis equation has no real solutions for $x_1$.\n\nTherefore, the only two constrained stationary points are $P_1 = (1, 1)$ and $P_2 = (-1, -1)$. We find the corresponding Lagrange multiplier $\\lambda$ for each point using $\\lambda = \\frac{2x_1}{x_2}$.\nFor $P_1 = (1, 1)$, the multiplier is $\\lambda_1 = \\frac{2(1)}{1} = 2$.\nFor $P_2 = (-1, -1)$, the multiplier is $\\lambda_2 = \\frac{2(-1)}{-1} = 2$.\n\nTo classify these points, we perform a second-order analysis. The second-order sufficient condition requires examining the definiteness of the Hessian of the Lagrangian with respect to $x=(x_1, x_2)$, denoted $\\nabla_x^2 L$, when restricted to the tangent space of the constraint at the stationary point.\nThe Hessian of the Lagrangian is:\n$$ \\nabla_x^2 L = \\begin{pmatrix} \\frac{\\partial^2 L}{\\partial x_1^2} & \\frac{\\partial^2 L}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 L}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 L}{\\partial x_2^2} \\end{pmatrix} = \\begin{pmatrix} 2 & -\\lambda \\\\ -\\lambda & 2 \\end{pmatrix} $$\nThe tangent space at a point $x^*$ is the set of all vectors $y \\in \\mathbb{R}^2$ such that $\\nabla g(x^*)^T y = 0$. The gradient of the constraint is $\\nabla g(x_1, x_2) = (x_2, x_1)^T$.\n\nFor point $P_1 = (1, 1)$ with $\\lambda_1 = 2$:\nThe Hessian matrix is $\\nabla_x^2 L(P_1, \\lambda_1) = \\begin{pmatrix} 2 & -2 \\\\ -2 & 2 \\end{pmatrix}$.\nThe constraint gradient is $\\nabla g(P_1) = (1, 1)^T$.\nThe tangent space is defined by $y = (y_1, y_2)^T$ such that $\\nabla g(P_1)^T y = y_1 + y_2 = 0$, which implies $y_2 = -y_1$. Any non-zero vector in this space can be written as $y = k(1, -1)^T$ for some non-zero scalar $k$.\nWe evaluate the quadratic form $y^T(\\nabla_x^2 L)y$:\n$$ y^T(\\nabla_x^2 L)y = \\begin{pmatrix} k & -k \\end{pmatrix} \\begin{pmatrix} 2 & -2 \\\\ -2 & 2 \\end{pmatrix} \\begin{pmatrix} k \\\\ -k \\end{pmatrix} = \\begin{pmatrix} 4k & -4k \\end{pmatrix} \\begin{pmatrix} k \\\\ -k \\end{pmatrix} = 4k^2 + 4k^2 = 8k^2 $$\nSince $k \\neq 0$, $8k^2 > 0$. The Hessian is positive definite on the tangent space, so $P_1 = (1, 1)$ is a strict constrained local minimum.\n\nFor point $P_2 = (-1, -1)$ with $\\lambda_2 = 2$:\nThe Hessian matrix is $\\nabla_x^2 L(P_2, \\lambda_2) = \\begin{pmatrix} 2 & -2 \\\\ -2 & 2 \\end{pmatrix}$.\nThe constraint gradient is $\\nabla g(P_2) = (-1, -1)^T$.\nThe tangent space is defined by $\\nabla g(P_2)^T y = -y_1 - y_2 = 0$, which also implies $y_2 = -y_1$. The tangent space is the same as for $P_1$.\nThe quadratic form $y^T(\\nabla_x^2 L)y$ is therefore also $8k^2 > 0$, so $P_2 = (-1, -1)$ is also a strict constrained local minimum.\n\nThis classification is confirmed by examining the determinant of the bordered Hessian matrix, $H_B$:\n$$ H_B = \\begin{pmatrix} 0 & \\frac{\\partial g}{\\partial x_1} & \\frac{\\partial g}{\\partial x_2} \\\\ \\frac{\\partial g}{\\partial x_1} & \\frac{\\partial^2 L}{\\partial x_1^2} & \\frac{\\partial^2 L}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial g}{\\partial x_2} & \\frac{\\partial^2 L}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 L}{\\partial x_2^2} \\end{pmatrix} = \\begin{pmatrix} 0 & x_2 & x_1 \\\\ x_2 & 2 & -\\lambda \\\\ x_1 & -\\lambda & 2 \\end{pmatrix} $$\nAt both points $P_1$ and $P_2$, $\\lambda=2$.\nAt $P_1=(1,1)$, $\\det(H_B) = \\det \\begin{pmatrix} 0 & 1 & 1 \\\\ 1 & 2 & -2 \\\\ 1 & -2 & 2 \\end{pmatrix} = -8$.\nAt $P_2=(-1,-1)$, $\\det(H_B) = \\det \\begin{pmatrix} 0 & -1 & -1 \\\\ -1 & 2 & -2 \\\\ -1 & -2 & 2 \\end{pmatrix} = -8$.\nFor a problem with $n=2$ variables and $m=1$ constraint, a negative determinant of the bordered Hessian ($\\det(H_B) < 0$) signifies a constrained local minimum.\n\nFinally, we compute the value of the objective function $f(x_1, x_2) = x_1^2 + x_2^2$ at the identified minima.\nAt $P_1 = (1, 1)$, the value is $f(1, 1) = 1^2 + 1^2 = 2$.\nAt $P_2 = (-1, -1)$, the value is $f(-1, -1) = (-1)^2 + (-1)^2 = 2$.\nBoth constrained local minima yield the same value. The objective function $f(x_1, x_2)$ represents the squared distance from the origin. The constraint $x_1x_2=1$ defines a hyperbola. As a point on the hyperbola moves far from the origin, $f(x_1, x_2)$ increases without bound. Therefore, the local minima must be global minima for the constrained problem.\nThe minimal objective value attained under the constraint is $2$.",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}