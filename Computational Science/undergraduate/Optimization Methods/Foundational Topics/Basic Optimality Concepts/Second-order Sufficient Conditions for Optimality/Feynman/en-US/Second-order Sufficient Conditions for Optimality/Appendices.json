{
    "hands_on_practices": [
        {
            "introduction": "Mastering optimization theory requires hands-on application. This first practice provides a foundational exercise in applying second-order sufficient conditions to an equality-constrained problem. By finding the stationary point and then constructing the reduced Hessian, you will directly test the curvature of the Lagrangian on the tangent space to determine if the point is a strict local minimum.",
            "id": "3176409",
            "problem": "Consider the equality-constrained optimization problem with objective function $f(x,y)=x^{2}y^{2}+x^{2}+y^{2}$ and constraint $g(x,y)=x+y=0$. Work from first principles of equality-constrained optimality, beginning with the definition of a constrained stationary point via the gradient of the Lagrangian and the condition that the constraint Jacobian has full rank. Then, use the definition of the reduced Hessian constructed on an orthonormal basis for the nullspace of the constraint Jacobian to assess second-order sufficiency.\n\nTasks:\n1. Determine the constrained stationary point(s) by solving the first-order necessary conditions of Karush-Kuhn-Tucker (KKT) optimality for equality constraints.\n2. Construct the reduced Hessian of the Lagrangian at the stationary point found in Task $1$ using the orthonormal basis vector $z=\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\-1\\end{pmatrix}$ for the nullspace of the constraint Jacobian.\n3. Using the second-order sufficient condition for equality-constrained problems, decide whether the stationary point is a strict local minimum.\n\nReport as your final answer only the scalar value of the reduced Hessian obtained in Task $2$. No rounding is required; provide the exact value. Do not include any units.",
            "solution": "The problem asks for the analysis of an equality-constrained optimization problem. We are given the objective function $f(x,y)=x^{2}y^{2}+x^{2}+y^{2}$ and the equality constraint $g(x,y)=x+y=0$. We will follow the specified tasks to find the constrained stationary points and analyze the second-order conditions.\n\nThe first step is to define the Lagrangian function, $L(x, y, \\lambda)$, for this problem. The Lagrangian is given by $L(x, y, \\lambda) = f(x, y) - \\lambda g(x, y)$.\n$$L(x,y,\\lambda) = x^{2}y^{2}+x^{2}+y^{2} - \\lambda(x+y)$$\nwhere $\\lambda$ is the Lagrange multiplier associated with the constraint.\n\n**Task 1: Determine the constrained stationary point(s).**\n\nWe find the stationary points by applying the first-order necessary conditions, which state that the gradient of the Lagrangian with respect to all variables must be zero. These are the Karush-Kuhn-Tucker (KKT) conditions for equality constraints.\n\nThe gradient of the Lagrangian is:\n$$ \\nabla L(x,y,\\lambda) = \\begin{pmatrix} \\frac{\\partial L}{\\partial x} \\\\ \\frac{\\partial L}{\\partial y} \\\\ \\frac{\\partial L}{\\partial \\lambda} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n\nThis yields the following system of equations:\n1. $\\frac{\\partial L}{\\partial x} = 2xy^{2} + 2x - \\lambda = 0$\n2. $\\frac{\\partial L}{\\partial y} = 2x^{2}y + 2y - \\lambda = 0$\n3. $\\frac{\\partial L}{\\partial \\lambda} = -(x+y) = 0 \\implies x+y=0$\n\nFrom a purely mathematical viewpoint, the condition on $\\lambda$ is that the gradient with respect to the primal variables is zero, and the final equation is the constraint itself. Setting $\\frac{\\partial L}{\\partial \\lambda}=0$ is a convenient way to recover the constraint.\n\nFrom the third equation, we have $y = -x$. We substitute this into the first two equations.\nSubstituting $y=-x$ into equation (1):\n$$ 2x(-x)^{2} + 2x - \\lambda = 0 \\implies 2x^{3} + 2x - \\lambda = 0 $$\n$$ \\lambda = 2x^{3} + 2x $$\nSubstituting $y=-x$ into equation (2):\n$$ 2x^{2}(-x) + 2(-x) - \\lambda = 0 \\implies -2x^{3} - 2x - \\lambda = 0 $$\n$$ \\lambda = -2x^{3} - 2x $$\n\nEquating the two expressions for $\\lambda$:\n$$ 2x^{3} + 2x = -2x^{3} - 2x $$\n$$ 4x^{3} + 4x = 0 $$\n$$ 4x(x^{2} + 1) = 0 $$\n\nThe term $x^{2}+1$ is always positive for any real number $x$. Therefore, the only real solution is $x=0$.\nIf $x=0$, then $y = -x = 0$.\nWe can find the corresponding value of $\\lambda$ using either expression. For instance, using $\\lambda = 2x^{3} + 2x$, we get $\\lambda = 2(0)^{3} + 2(0) = 0$.\n\nThus, there is a single constrained stationary point $(x^*, y^*) = (0,0)$, with the associated Lagrange multiplier $\\lambda^* = 0$.\n\n**Task 2: Construct the reduced Hessian of the Lagrangian.**\n\nThe second-order conditions involve the Hessian of the Lagrangian with respect to the variables $x$ and $y$, denoted as $\\nabla_{xx}^{2} L$.\nFirst, we compute the second partial derivatives of $L(x,y,\\lambda)$:\n$$ \\frac{\\partial^{2} L}{\\partial x^{2}} = \\frac{\\partial}{\\partial x}(2xy^{2} + 2x - \\lambda) = 2y^{2} + 2 $$\n$$ \\frac{\\partial^{2} L}{\\partial y^{2}} = \\frac{\\partial}{\\partial y}(2x^{2}y + 2y - \\lambda) = 2x^{2} + 2 $$\n$$ \\frac{\\partial^{2} L}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}(2xy^{2} + 2x - \\lambda) = 4xy $$\n\nThe Hessian matrix of the Lagrangian is:\n$$ \\nabla_{xx}^{2} L(x,y,\\lambda) = \\begin{pmatrix} 2y^{2}+2 & 4xy \\\\ 4xy & 2x^{2}+2 \\end{pmatrix} $$\n\nNext, we evaluate this Hessian at the stationary point $(x^*, y^*, \\lambda^*) = (0,0,0)$:\n$$ H_L = \\nabla_{xx}^{2} L(0,0,0) = \\begin{pmatrix} 2(0)^{2}+2 & 4(0)(0) \\\\ 4(0)(0) & 2(0)^{2}+2 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix} $$\n\nThe reduced Hessian is constructed by projecting this Hessian onto the nullspace of the constraint Jacobian. The constraint Jacobian, $A$, is the transpose of the gradient of the constraint function $g(x,y)$:\n$$ \\nabla g(x,y) = \\begin{pmatrix} \\frac{\\partial g}{\\partial x} \\\\ \\frac{\\partial g}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\n$$ A(x,y) = \\nabla g(x,y)^{T} = \\begin{pmatrix} 1 & 1 \\end{pmatrix} $$\nThe nullspace of $A$ consists of all vectors $v = \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix}$ such that $Av=0$, which means $1 \\cdot v_x + 1 \\cdot v_y = 0$, or $v_y = -v_x$. Any vector in the nullspace is a multiple of $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nThe problem provides an orthonormal basis for this nullspace, which we denote by the matrix $Z$. In this case, the nullspace is one-dimensional, so $Z$ is a column vector:\n$$ Z = z = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n\nThe reduced Hessian, $H_R$, is defined as $H_R = Z^{T} H_L Z$. We now compute this product:\n$$ H_R = \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & -1 \\end{pmatrix}\\right) \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix} \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\right) $$\n$$ H_R = \\frac{1}{2} \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nFirst, we perform the multiplication of the first two matrices:\n$$ \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix} = \\begin{pmatrix} (1)(2)+(-1)(0) & (1)(0)+(-1)(2) \\end{pmatrix} = \\begin{pmatrix} 2 & -2 \\end{pmatrix} $$\nNow, we multiply this result by the last vector:\n$$ \\begin{pmatrix} 2 & -2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = (2)(1) + (-2)(-1) = 2 + 2 = 4 $$\nFinally, we multiply by the scalar factor $\\frac{1}{2}$:\n$$ H_R = \\frac{1}{2} (4) = 2 $$\nThe reduced Hessian is a $1 \\times 1$ matrix, which is the scalar value $2$.\n\n**Task 3: Decide whether the stationary point is a strict local minimum.**\n\nThe second-order sufficient condition for a point $x^*$ to be a strict local minimum is that the reduced Hessian $H_R = Z^{T}(\\nabla_{xx}^{2} L(x^*, \\lambda^*))Z$ is positive definite.\nIn our case, the reduced Hessian is the scalar $2$. A scalar is positive definite if and only if it is a positive number.\nSince $H_R = 2 > 0$, the reduced Hessian is positive definite.\nTherefore, the stationary point $(x^*, y^*) = (0,0)$ is a strict local minimum for the constrained problem.\n\nThe final answer requested is the scalar value of the reduced Hessian obtained in Task 2.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "Building on the basic procedure, this exercise explores a more profound concept: how constraints can fundamentally alter the nature of an optimization problem. Here, you will analyze a quadratic function with an indefinite Hessian, meaning it is not convex. You will discover that by restricting the problem to a feasible subspace, the negative curvature is eliminated, allowing for a strict local minimum to exist where one would not in the unconstrained case.",
            "id": "3176387",
            "problem": "Consider the equality-constrained quadratic program in dimension $3$ with objective function $f:\\mathbb{R}^{3}\\to\\mathbb{R}$ given by\n$$\nf(x) \\;=\\; \\frac{1}{2}\\,x^{\\top}Qx \\;+\\; c^{\\top}x,\n$$\nwhere\n$$\nQ \\;=\\; \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix}, \n\\qquad\nc \\;=\\; \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}.\n$$\nImpose the single linear equality constraint\n$$\nA x \\;=\\; b, \n\\qquad \nA \\;=\\; \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix}, \n\\qquad \nb \\;=\\; 1.\n$$\nUsing only foundational definitions—namely, the definition of a constrained local minimizer, the notion of feasible directions and the tangent space (defined here by the first-order constraint linearization), the Taylor expansion of $f$ up to second order, and the Karush-Kuhn-Tucker (KKT) optimality conditions—do the following:\n\n1) Establish that the symmetric matrix $Q$ is indefinite by analyzing its eigenvalues.\n\n2) Identify the set of feasible directions $d$ at any feasible point $\\bar{x}$ by linearizing the constraint and characterizing the tangent space $\\{d \\in \\mathbb{R}^{3} : A d = 0\\}$.\n\n3) Show that the quadratic form $d^{\\top}Qd$ is strictly positive for all nonzero feasible directions $d$ you identified in part $2$, and explain how this enforces positive curvature in feasible directions even though $Q$ is indefinite in $\\mathbb{R}^{3}$.\n\n4) Derive the first-order stationary conditions for equality-constrained problems from the definition of a constrained local minimizer and the Lagrangian, and solve the resulting linear system to obtain the candidate optimizer $x^{\\star}$.\n\n5) Using the second-order Taylor expansion of $f$ along feasible directions and your result in part $3$, argue that the second-order sufficient conditions for optimality hold at your candidate, thereby proving that $x^{\\star}$ is a strict local minimizer.\n\nYour final answer must be the optimizer $x^{\\star}$ written as a single row vector using the parentheses matrix notation. Express your answer exactly; no rounding is required.",
            "solution": "The problem as stated is mathematically well-posed, internally consistent, and free of any scientific or factual unsoundness. All necessary information is provided to determine a unique solution. We may therefore proceed with a full derivation as requested. The solution will follow the five specified steps.\n\nThe problem is to find a strict local minimizer for the objective function $f:\\mathbb{R}^{3}\\to\\mathbb{R}$,\n$$f(x) = \\frac{1}{2}x^{\\top}Qx + c^{\\top}x$$\nwith\n$$Q = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix}, \\quad c = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}$$\nsubject to the linear equality constraint $Ax=b$, where\n$$A = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix}, \\quad b = 1$$\n\n$1$) We first establish that the symmetric matrix $Q$ is indefinite. A symmetric matrix is indefinite if it has at least one positive eigenvalue and at least one negative eigenvalue. The matrix $Q$ is a diagonal matrix, so its eigenvalues are its diagonal entries. The eigenvalues are $\\lambda_1 = 1$, $\\lambda_2 = -1$, and $\\lambda_3 = 2$. Since $\\lambda_1$ and $\\lambda_3$ are positive and $\\lambda_2$ is negative, the matrix $Q$ is, by definition, indefinite. This implies that the quadratic function $f(x)$ is non-convex and has both positive and negative curvature in $\\mathbb{R}^{3}$. Specifically, the quadratic form $x^{\\top}Qx$ is a saddle.\n\n$2$) Next, we identify the set of feasible directions at any feasible point $\\bar{x}$. A point $\\bar{x}$ is feasible if it satisfies the constraint $A\\bar{x} = b$. For a linear equality constraint, the set of feasible directions at any feasible point is the tangent space, which is constant for all feasible points. A direction $d \\in \\mathbb{R}^{3}$ is a feasible direction if for a feasible point $\\bar{x}$, the point $\\bar{x}+\\alpha d$ also satisfies the constraint for all sufficiently small scalars $\\alpha$. This requires $A(\\bar{x}+\\alpha d) = b$. Expanding this gives $A\\bar{x} + \\alpha Ad = b$. Since $\\bar{x}$ is feasible, $A\\bar{x} = b$, which simplifies the condition to $\\alpha Ad = 0$. For this to hold for any non-zero $\\alpha$, we must have $Ad=0$. Thus, the set of feasible directions is the null space of the matrix $A$.\nGiven $A = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix}$, we find the null space by solving $Ad=0$:\n$$ \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} d_1 \\\\ d_2 \\\\ d_3 \\end{pmatrix} = 0 $$\nThis equation simplifies to $d_2 = 0$. The components $d_1$ and $d_3$ can be any real numbers. Therefore, the tangent space, representing the set of all feasible directions, is\n$$ T = \\{ d \\in \\mathbb{R}^{3} \\mid Ad = 0 \\} = \\left\\{ \\begin{pmatrix} d_1 \\\\ 0 \\\\ d_3 \\end{pmatrix} \\mid d_1, d_3 \\in \\mathbb{R} \\right\\} $$\nThis is a $2$-dimensional subspace of $\\mathbb{R}^{3}$ spanned by the basis vectors $\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ and $\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n\n$3$) We now examine the curvature of the objective function along these feasible directions. This is determined by the sign of the quadratic form $d^{\\top}Qd$ for any non-zero feasible direction $d \\in T$. Let $d$ be a non-zero vector in the tangent space, so $d = \\begin{pmatrix} d_1 \\\\ 0 \\\\ d_3 \\end{pmatrix}$ where $d_1$ and $d_3$ are not both zero. We compute $d^{\\top}Qd$:\n$$ d^{\\top}Qd = \\begin{pmatrix} d_1 & 0 & d_3 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} d_1 \\\\ 0 \\\\ d_3 \\end{pmatrix} $$\n$$ = \\begin{pmatrix} d_1 & 0 & 2d_3 \\end{pmatrix} \\begin{pmatrix} d_1 \\\\ 0 \\\\ d_3 \\end{pmatrix} = d_1^2 + 2d_3^2 $$\nSince $d_1, d_3 \\in \\mathbb{R}$, we have $d_1^2 \\ge 0$ and $2d_3^2 \\ge 0$. The sum $d_1^2 + 2d_3^2$ is zero if and only if $d_1 = 0$ and $d_3 = 0$, which corresponds to the zero vector $d=0$. As we are considering non-zero feasible directions, we have $d_1^2 + 2d_3^2 > 0$. This demonstrates that the quadratic form $d^{\\top}Qd$ is strictly positive for all nonzero feasible directions. This means the Hessian of the objective function, $Q$, is positive definite on the tangent space $T$. The negative curvature associated with the eigenvalue $\\lambda_2 = -1$ corresponds to the $x_2$ direction. However, the constraint $Ax=b$ (which is $x_2=1$) restricts feasible points to a plane, and feasible directions to the subspace where the second component is zero. This effectively neutralizes the direction of negative curvature, leaving only positive curvature in all allowable directions of movement.\n\n$4$) We derive and solve the first-order necessary conditions for optimality, known as the Karush-Kuhn-Tucker (KKT) conditions for equality constraints. We define the Lagrangian function $L(x, \\lambda) = f(x) - \\lambda (Ax - b)$, where $\\lambda$ is the scalar Lagrange multiplier.\n$$ L(x, \\lambda) = \\frac{1}{2}x^{\\top}Qx + c^{\\top}x - \\lambda(A x - b) $$\nThe first-order necessary conditions for a point $x^{\\star}$ to be a local minimizer are the existence of a multiplier $\\lambda^{\\star}$ such that the gradient of the Lagrangian with respect to $x$ is zero, and the point is feasible:\n$$ \\nabla_x L(x^{\\star}, \\lambda^{\\star}) = Qx^{\\star} + c - A^{\\top}\\lambda^{\\star} = 0 $$\n$$ Ax^{\\star} - b = 0 $$\nSubstituting the given matrices and vectors, we obtain a system of linear equations:\n$$ \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\lambda = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n$$ \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = 1 $$\nThe feasibility constraint $Ax=b$ gives $x_2 = 1$. The gradient equation yields a system of three scalar equations:\n$1$) $x_1 + 1 = 0 \\implies x_1 = -1$\n$2$) $-x_2 - 2 - \\lambda = 0$\n$3$) $2x_3 + 3 = 0 \\implies x_3 = -\\frac{3}{2}$\nSubstituting $x_2=1$ into the second equation allows us to find the multiplier $\\lambda$:\n$-1 - 2 - \\lambda = 0 \\implies \\lambda = -3$.\nThe candidate optimizer is the point $x^{\\star}$ with components $x_1 = -1$, $x_2 = 1$, and $x_3 = -\\frac{3}{2}$.\n$$ x^{\\star} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -\\frac{3}{2} \\end{pmatrix} $$\n\n$5$) Finally, we verify that $x^{\\star}$ is a strict local minimizer using the second-order sufficient conditions (SOSC). For equality constraints, the SOSC are satisfied at a KKT point $(x^{\\star}, \\lambda^{\\star})$ if the Hessian of the Lagrangian with respect to $x$, evaluated at $x^{\\star}$, is positive definite on the tangent space $T(x^{\\star})$. That is, $d^{\\top} \\nabla_{xx}^2 L(x^{\\star}, \\lambda^{\\star}) d > 0$ for all nonzero $d \\in T(x^{\\star})$.\nThe Hessian of the Lagrangian is $\\nabla_{xx}^2 L(x, \\lambda) = Q$.\nThe SOSC then requires that $d^{\\top}Qd > 0$ for all nonzero $d$ such that $Ad=0$. This is precisely the condition we verified in part $3$). We showed that for any $d \\in T, d \\neq 0$, the quadratic form evaluates to $d^{\\top}Qd = d_1^2 + 2d_3^2 > 0$.\nTo see why this guarantees a strict local minimum, consider a Taylor expansion of $f(x)$ around $x^{\\star}$ along a feasible direction $d \\in T$. For a point $x = x^{\\star} + \\alpha d$, with $\\alpha \\in \\mathbb{R}$:\n$$ f(x^{\\star}+\\alpha d) - f(x^{\\star}) = \\alpha \\nabla f(x^{\\star})^{\\top}d + \\frac{1}{2}\\alpha^2 d^{\\top} \\nabla^2 f(x^{\\star}) d + o(\\alpha^2) $$\nFrom the KKT condition $\\nabla_x L(x^{\\star}, \\lambda^{\\star}) = 0$, we have $\\nabla f(x^{\\star}) = A^{\\top}\\lambda^{\\star}$. The first-order term is $\\alpha (A^{\\top}\\lambda^{\\star})^{\\top}d = \\alpha (\\lambda^{\\star})^{\\top} (Ad)$. Since $d \\in T$, we have $Ad=0$, so the first-order term vanishes. The expansion becomes:\n$$ f(x^{\\star}+\\alpha d) - f(x^{\\star}) = \\frac{1}{2}\\alpha^2 d^{\\top} Q d + o(\\alpha^2) $$\nAs established, $d^{\\top}Qd > 0$ for any nonzero $d \\in T$. Thus, for any nonzero $\\alpha$, the term $\\frac{1}{2}\\alpha^2 d^{\\top} Q d$ is strictly positive. For $\\alpha$ sufficiently small, this second-order term dominates the higher-order terms $o(\\alpha^2)$, implying that $f(x^{\\star}+\\alpha d) - f(x^{\\star}) > 0$. This holds for any feasible direction $d$. Consequently, $x^{\\star}$ is a strict local minimizer. The candidate point $x^{\\star} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -\\frac{3}{2} \\end{pmatrix}$ is confirmed to be a strict local minimizer of the constrained problem.",
            "answer": "$$\\boxed{\\begin{pmatrix} -1 & 1 & -\\frac{3}{2} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Finally, it is crucial to understand the limitations of our analytical tools. This problem presents a special case where the second-order sufficient condition is inconclusive, as the curvature along the feasible direction is zero. By analyzing the function directly, you will see that the point is nevertheless a strict local minimum, a fact determined by higher-order terms, illustrating that our second-order test is sufficient, but not always necessary.",
            "id": "3176305",
            "problem": "Consider the constrained optimization problem: minimize the function $f:\\mathbb{R}^2 \\to \\mathbb{R}$ given by $f(x) = x_1^2 + x_2^4$ subject to the equality constraint $h(x) = x_1 = 0$. Let $x^\\star = (0,0)$. Using only the fundamental definitions of first-order stationarity via Lagrange multipliers for equality constraints, the tangent space of the constraint, and the role of curvature measured by the Hessian matrix on feasible directions, determine which of the following statements are true. Select all that apply.\n\nA. The point $x^\\star$ is feasible and satisfies the first-order necessary condition with Lagrange multiplier $\\lambda^\\star = 0$.\n\nB. The second-order sufficient condition for a strict local minimum holds at $x^\\star$ because the Hessian of the Lagrangian is positive semidefinite on the tangent space.\n\nC. Even though the second-order sufficient condition does not hold, the point $x^\\star$ is a strict local minimizer of the constrained problem.\n\nD. The second-order necessary condition fails at $x^\\star$ because the Hessian of the Lagrangian is zero along the tangent space direction.\n\nE. There exists $\\varepsilon > 0$ such that any feasible point $(x_1,x_2)$ with $\\|(x_1,x_2)\\| < \\varepsilon$ and $(x_1,x_2) \\neq (0,0)$ satisfies $f(x_1,x_2) > f(0,0)$.",
            "solution": "The user has provided a constrained optimization problem and a series of statements to evaluate. I will first validate the problem statement and then proceed with a detailed analysis.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Objective function: $f:\\mathbb{R}^2 \\to \\mathbb{R}$, $f(x) = x_1^2 + x_2^4$, where $x=(x_1, x_2)$.\n-   Equality constraint: $h(x) = x_1 = 0$.\n-   Point of interest: $x^\\star = (0,0)$.\n-   Task: Analyze the optimality of $x^\\star$ using first-order stationarity, the tangent space, and the Hessian matrix, and determine the truthfulness of the provided statements.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a standard exercise in constrained nonlinear optimization. The functions $f(x)$ and $h(x)$ are both polynomials and are therefore infinitely differentiable ($C^\\infty$) on $\\mathbb{R}^2$. The concepts invoked—Lagrange multipliers, tangent spaces, and second-order optimality conditions—are fundamental to optimization theory. The problem is mathematically well-defined, self-contained, factually sound, and objective. It does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. I will proceed to the solution.\n\n### Solution Derivation\n\nThe problem is to minimize $f(x_1, x_2) = x_1^2 + x_2^4$ subject to $h(x_1, x_2) = x_1 = 0$. We analyze the point $x^\\star = (0,0)$.\n\n**1. Feasibility and First-Order Necessary Conditions**\n\nA point $x^\\star$ is a candidate for a local minimum only if it is feasible and satisfies the first-order necessary conditions (also known as the Karush-Kuhn-Tucker or KKT conditions).\n\nFirst, we check feasibility. The point $x^\\star = (0,0)$ must satisfy the constraint $h(x)=0$.\n$h(0,0) = 0$, so the point is feasible.\n\nNext, we establish the Lagrangian function, $L(x, \\lambda) = f(x) + \\lambda h(x)$:\n$$L(x_1, x_2, \\lambda) = x_1^2 + x_2^4 + \\lambda x_1$$\nThe first-order necessary condition states that if $x^\\star$ is a local minimum, then there must exist a Lagrange multiplier $\\lambda^\\star$ such that the gradient of the Lagrangian with respect to $x$ is zero at $(x^\\star, \\lambda^\\star)$, provided $x^\\star$ is a regular point.\nThe gradient of the Lagrangian is:\n$$\\nabla_x L(x, \\lambda) = \\begin{pmatrix} \\frac{\\partial L}{\\partial x_1} \\\\ \\frac{\\partial L}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 + \\lambda \\\\ 4x_2^3 \\end{pmatrix}$$\nAt the point $x^\\star = (0,0)$, we have:\n$$\\nabla_x L(0,0, \\lambda^\\star) = \\begin{pmatrix} 2(0) + \\lambda^\\star \\\\ 4(0)^3 \\end{pmatrix} = \\begin{pmatrix} \\lambda^\\star \\\\ 0 \\end{pmatrix}$$\nFor the stationarity condition $\\nabla_x L(x^\\star, \\lambda^\\star) = 0$ to hold, we must have $\\lambda^\\star = 0$.\nThe point $x^\\star$ is regular if the gradient of the constraint function at $x^\\star$ is a non-zero vector (and hence linearly independent).\n$$\\nabla h(x) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\nAt $x^\\star = (0,0)$, $\\nabla h(0,0) = (1, 0) \\neq (0,0)$. So, $x^\\star$ is a regular point.\nThus, $x^\\star = (0,0)$ is a feasible point that satisfies the first-order necessary condition with the unique Lagrange multiplier $\\lambda^\\star = 0$.\n\n**2. Second-Order Conditions**\n\nTo analyze the second-order conditions, we need the Hessian of the Lagrangian and the tangent space at $x^\\star$.\n\nThe tangent space $T(x^\\star)$ is the set of directions $d$ orthogonal to the constraint gradient at $x^\\star$:\n$$T(x^\\star) = \\{ d \\in \\mathbb{R}^2 \\mid \\nabla h(x^\\star)^T d = 0 \\}$$\nWith $\\nabla h(x^\\star) = (1,0)$, the condition becomes:\n$$(1, 0) \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix} = d_1 = 0$$\nSo, the tangent space is the set of all vectors of the form $d = (0, d_2)$ where $d_2 \\in \\mathbb{R}$. This is the $x_2$-axis.\n\nNext, we compute the Hessian of the Lagrangian with respect to $x$:\n$$\\nabla_{xx}^2 L(x, \\lambda) = \\begin{pmatrix} \\frac{\\partial^2 L}{\\partial x_1^2} & \\frac{\\partial^2 L}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 L}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 L}{\\partial x_2^2} \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 12x_2^2 \\end{pmatrix}$$\nWe evaluate this Hessian at the point $(x^\\star, \\lambda^\\star) = ((0,0), 0)$:\n$$\\nabla_{xx}^2 L(x^\\star, \\lambda^\\star) = \\begin{pmatrix} 2 & 0 \\\\ 0 & 12(0)^2 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 0 \\end{pmatrix}$$\nThe second-order conditions involve evaluating the curvature $d^T \\nabla_{xx}^2 L(x^\\star, \\lambda^\\star) d$ for directions $d$ in the tangent space $T(x^\\star)$. Let $d = (0, d_2) \\in T(x^\\star)$.\n$$d^T \\nabla_{xx}^2 L(x^\\star, \\lambda^\\star) d = (0, d_2) \\begin{pmatrix} 2 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ d_2 \\end{pmatrix} = (0, d_2) \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = 0$$\n\n-   **Second-Order Necessary Condition (SONC):** For $x^\\star$ to be a local minimizer, we must have $d^T \\nabla_{xx}^2 L(x^\\star, \\lambda^\\star) d \\ge 0$ for all $d \\in T(x^\\star)$. Since the value is $0$ for all $d \\in T(x^\\star)$, and $0 \\ge 0$, the SONC is satisfied.\n-   **Second-Order Sufficient Condition (SOSC):** If $d^T \\nabla_{xx}^2 L(x^\\star, \\lambda^\\star) d > 0$ for all non-zero $d \\in T(x^\\star)$, then $x^\\star$ is a strict local minimizer. In our case, for any non-zero vector in the tangent space, e.g., $d=(0,1)$, the curvature is $0$. Since $0$ is not strictly greater than $0$, the SOSC does not hold. The test is inconclusive.\n\n**3. Direct Analysis of the Minimum**\n\nWhen the standard second-order tests are inconclusive, we can analyze the problem more directly. The constraint $x_1 = 0$ confines the problem to the $x_2$-axis. By substituting the constraint into the objective function, the problem becomes:\nMinimize $g(x_2) = f(0, x_2) = 0^2 + x_2^4 = x_2^4$.\nThe point $x^\\star = (0,0)$ corresponds to $x_2=0$ in this reduced, unconstrained problem. The value of the function at this point is $g(0) = 0^4 = 0$.\nFor any other feasible point, which has the form $(0, x_2)$ with $x_2 \\neq 0$, the function value is:\n$$f(0, x_2) = x_2^4 > 0 = f(0,0)$$\nThis inequality shows that any feasible point in any neighborhood of $(0,0)$, other than $(0,0)$ itself, has a strictly greater function value. Therefore, $x^\\star = (0,0)$ is a strict local minimizer (and also a strict global minimizer) of the constrained problem.\n\n### Option-by-Option Analysis\n\n**A. The point $x^\\star$ is feasible and satisfies the first-order necessary condition with Lagrange multiplier $\\lambda^\\star = 0$.**\nAs shown in our analysis, $x^\\star=(0,0)$ is feasible as $h(0,0)=0$. The stationary condition $\\nabla_x L(0,0, \\lambda) = (\\lambda, 0) = (0,0)$ is satisfied for $\\lambda=0$. Thus, $x^\\star$ satisfies the first-order necessary condition with $\\lambda^\\star = 0$.\n**Verdict: Correct.**\n\n**B. The second-order sufficient condition for a strict local minimum holds at $x^\\star$ because the Hessian of the Lagrangian is positive semidefinite on the tangent space.**\nThe second-order sufficient condition requires the Hessian of the Lagrangian to be *positive definite* on the tangent space (i.e., $d^T H d > 0$ for all $d \\in T(x^\\star), d \\neq 0$), not merely positive semidefinite ($d^T H d \\ge 0$). We found that for any non-zero $d \\in T(x^\\star)$, $d^T \\nabla_{xx}^2 L d = 0$, which violates the positive definite requirement. Therefore, the SOSC does not hold. The premise of the statement is false.\n**Verdict: Incorrect.**\n\n**C. Even though the second-order sufficient condition does not hold, the point $x^\\star$ is a strict local minimizer of the constrained problem.**\nOur analysis confirmed that the SOSC does not hold, making the test inconclusive. However, by substituting the constraint into the objective function, we proved that $x^\\star = (0,0)$ is indeed a strict local minimizer. Both clauses of the statement are true.\n**Verdict: Correct.**\n\n**D. The second-order necessary condition fails at $x^\\star$ because the Hessian of the Lagrangian is zero along the tangent space direction.**\nThe second-order necessary condition (SONC) requires $d^T \\nabla_{xx}^2 L d \\ge 0$ for all $d$ in the tangent space. We calculated this value to be exactly $0$. Since $0 \\ge 0$ is true, the SONC is satisfied. It does not fail.\n**Verdict: Incorrect.**\n\n**E. There exists $\\varepsilon > 0$ such that any feasible point $(x_1,x_2)$ with $\\|(x_1,x_2)\\| < \\varepsilon$ and $(x_1,x_2) \\neq (0,0)$ satisfies $f(x_1,x_2) > f(0,0)$.**\nThis statement is the formal definition of a strict local minimizer for a constrained problem. A feasible point must satisfy $x_1=0$. Let such a point be $x=(0, x_2)$. The condition $\\|x\\| = \\sqrt{0^2 + x_2^2} = |x_2| < \\varepsilon$ combined with $x \\neq (0,0)$ means $0 < |x_2| < \\varepsilon$. For such points, we evaluate the inequality:\n$$f(0, x_2) > f(0,0)$$\n$$x_2^4 > 0^2 + 0^4$$\n$$x_2^4 > 0$$\nThis is true for any $x_2 \\neq 0$. Therefore, the condition holds for any choice of $\\varepsilon > 0$. Since we established that $x^\\star$ is a strict local minimizer, this definitional statement must be true.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ACE}$$"
        }
    ]
}