{
    "hands_on_practices": [
        {
            "introduction": "The best way to grasp an iterative algorithm is to perform a few steps manually. This exercise guides you through the core mechanics of Kelley's cutting-plane method, starting from an initial point. You will practice querying an oracle for a subgradient, constructing the linear underestimator (the \"cut\"), and solving the resulting master linear program to find the next iterate. ",
            "id": "3141056",
            "problem": "Consider the convex minimization problem with objective function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by $f(x)=\\max\\{x_{1}+x_{2},\\,2x_{1}-x_{2},\\,1-x_{1}\\}$ over the box $X=\\{x\\in\\mathbb{R}^{2}:\\|x\\|_{\\infty}\\le 2\\}$, where $\\|x\\|_{\\infty}=\\max\\{|x_{1}|,|x_{2}|\\}$. The pointwise maximum of affine functions is convex, and for a convex function $f$, any subgradient $g\\in\\partial f(y)$ at a point $y$ obeys the subgradient inequality $f(x)\\ge f(y)+g^{\\top}(x-y)$ for all $x$. Kelley's cutting-plane method constructs linear underestimators using this inequality and solves a sequence of Linear Programming (LP) master problems over the epigraph of $f$.\n\nUse Kelley's cutting-plane method starting from $y^{0}=(0,0)$. At each iteration $k$, query the oracle at $y^{k}$, which returns one subgradient of $f$ at $y^{k}$ arising from an affine function active in the maximum at $y^{k}$. Form the Kelley cut $t\\ge f(y^{k})+g^{k\\top}(x-y^{k})$ in variables $(x,t)$ and define the master LP that minimizes $t$ subject to all collected cuts and $x\\in X$. Let $y^{k+1}$ be the $x$-component of an optimal solution of the master LP. If multiple optimal solutions exist, select $y^{k+1}$ as the one with the smallest Euclidean norm $\\|x\\|_{2}$.\n\nTasks:\n- Compute explicitly the first three Kelley cuts generated at $y^{0}$, $y^{1}$, and $y^{2}$.\n- Write down the corresponding three LP master problems.\n- Solve each master problem to obtain $y^{1}$, $y^{2}$, $y^{3}$ and their optimal objective values $t^{1}$, $t^{2}$, $t^{3}$.\n- Provide, as your final answer, the exact value of $t^{3}$.\n\nNo rounding is required; express your final answer as an exact number.",
            "solution": "The problem requires the application of Kelley's cutting-plane method to a convex minimization problem. We are tasked with performing three iterations of the algorithm, starting from a given initial point, and determining the optimal objective value of the third master problem.\n\nThe problem is to minimize $f(x) = \\max\\{x_1+x_2, 2x_1-x_2, 1-x_1\\}$ over the feasible set $X = \\{x \\in \\mathbb{R}^2 : \\|x\\|_\\infty \\le 2\\}$. The set $X$ can be written explicitly as $X = \\{(x_1, x_2) \\in \\mathbb{R}^2 : -2 \\le x_1 \\le 2, -2 \\le x_2 \\le 2\\}$.\n\nThe Kelley's cutting-plane method generates a sequence of points $\\{y^k\\}$ and a sequence of lower bounds $\\{t^k\\}$ on the optimal value. At each iteration $k$, we solve a linear programming (LP) master problem to find the next iterate $y^{k+1}$ and the updated lower bound $t^{k+1}$. The master problem minimizes a variable $t$ subject to a set of linear inequalities, called cuts, which are derived from the subgradient inequality.\n\nThe $k$-th cut is given by $t \\ge f(y^k) + g^{k\\top}(x-y^k)$, where $g^k$ is a subgradient of $f$ at $y^k$. For a function defined as the maximum of several differentiable functions, a subgradient at a point can be taken as the gradient of any one of the functions that is active (i.e., achieves the maximum) at that point.\n\nLet the three affine functions be $f_1(x) = x_1+x_2$, $f_2(x) = 2x_1-x_2$, and $f_3(x) = 1-x_1$. Their gradients are, respectively, $g_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, $g_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$, and $g_3 = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$.\n\n**Iteration 0 ($k=0$)**\n\nWe start with the initial point $y^0 = (0,0)$.\n\n1.  **Oracle Query**: We first evaluate the objective function at $y^0$:\n    $$f(y^0) = f(0,0) = \\max\\{0+0, 2(0)-0, 1-0\\} = \\max\\{0, 0, 1\\} = 1$$\n    The active function is $f_3(x) = 1-x_1$. We choose the corresponding gradient as the subgradient:\n    $$g^0 = g_3 = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$$\n\n2.  **First Kelley Cut**: The first cut (Cut 1) is constructed using the subgradient inequality:\n    $$t \\ge f(y^0) + g^{0\\top}(x-y^0)$$\n    $$t \\ge 1 + \\begin{pmatrix} -1  0 \\end{pmatrix} \\begin{pmatrix} x_1 - 0 \\\\ x_2 - 0 \\end{pmatrix}$$\n    $$t \\ge 1 - x_1$$\n\n3.  **First Master Problem (LP1)**: We minimize $t$ subject to the first cut and the constraint $x \\in X$:\n    $$\n    \\begin{aligned}\n    (LP1) \\quad \\min_{x, t} \\quad  t \\\\\n    \\text{s.t.} \\quad  t \\ge 1 - x_1 \\\\\n     -2 \\le x_1 \\le 2 \\\\\n     -2 \\le x_2 \\le 2\n    \\end{aligned}\n    $$\n\n4.  **Solution to LP1**: To minimize $t$, we must minimize the right-hand side of the inequality, $1-x_1$. This is achieved by maximizing $x_1$ over the feasible set $X$. The maximum value is $x_1=2$.\n    The optimal value for $t$ is thus $t^1 = 1 - 2 = -1$.\n    The set of optimal solutions for $x$ is $\\{(x_1, x_2) : x_1=2, -2 \\le x_2 \\le 2\\}$.\n    According to the tie-breaking rule, we must select the solution with the smallest Euclidean norm $\\|x\\|_2 = \\sqrt{x_1^2 + x_2^2}$. For $x_1=2$, this requires minimizing $\\sqrt{2^2 + x_2^2}$, which occurs at $x_2=0$.\n    Therefore, the next iterate is $y^1 = (2,0)$, with the objective value of the master problem being $t^1=-1$.\n\n**Iteration 1 ($k=1$)**\n\nWe proceed with the point $y^1 = (2,0)$.\n\n1.  **Oracle Query**: We evaluate $f$ at $y^1$:\n    $$f(y^1) = f(2,0) = \\max\\{2+0, 2(2)-0, 1-2\\} = \\max\\{2, 4, -1\\} = 4$$\n    The active function is $f_2(x) = 2x_1-x_2$. The subgradient is:\n    $$g^1 = g_2 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$$\n\n2.  **Second Kelley Cut**: The second cut (Cut 2) is:\n    $$t \\ge f(y^1) + g^{1\\top}(x-y^1)$$\n    $$t \\ge 4 + \\begin{pmatrix} 2  -1 \\end{pmatrix} \\begin{pmatrix} x_1 - 2 \\\\ x_2 - 0 \\end{pmatrix}$$\n    $$t \\ge 4 + 2(x_1-2) - x_2 = 4 + 2x_1 - 4 - x_2$$\n    $$t \\ge 2x_1 - x_2$$\n\n3.  **Second Master Problem (LP2)**: We add the new cut to the master problem:\n    $$\n    \\begin{aligned}\n    (LP2) \\quad \\min_{x, t} \\quad  t \\\\\n    \\text{s.t.} \\quad  t \\ge 1 - x_1 \\quad (\\text{Cut 1}) \\\\\n     t \\ge 2x_1 - x_2 \\quad (\\text{Cut 2}) \\\\\n     -2 \\le x_1 \\le 2, \\quad -2 \\le x_2 \\le 2\n    \\end{aligned}\n    $$\n\n4.  **Solution to LP2**: This is equivalent to finding $\\min_{x \\in X} \\max\\{1-x_1, 2x_1-x_2\\}$. The minimum of this pointwise maximum function will occur either at a vertex of the domain $X$ or at a point where $1-x_1=2x_1-x_2$.\n    - Check vertices of $X = [-2,2]^2$:\n      - $x=(2,2): \\max\\{1-2, 4-2\\} = \\max\\{-1, 2\\}=2$.\n      - $x=(2,-2): \\max\\{1-2, 4-(-2)\\} = \\max\\{-1, 6\\}=6$.\n      - $x=(-2,2): \\max\\{1-(-2), -4-2\\} = \\max\\{3, -6\\}=3$.\n      - $x=(-2,-2): \\max\\{1-(-2), -4-(-2)\\} = \\max\\{3, -2\\}=3$.\n    - Check points where $1-x_1=2x_1-x_2$, which simplifies to $x_2=3x_1-1$. On this line, the value of the max function is $1-x_1$. We need to find the minimum of $1-x_1$ for points $(x_1, 3x_1-1)$ that lie in $X$. This means maximizing $x_1$.\n      The line segment of $x_2 = 3x_1-1$ within $X$ is bounded by its intersections with the boundary of the box.\n      - At $x_2=2$: $2 = 3x_1-1 \\implies 3x_1=3 \\implies x_1=1$. Point is $(1,2)$.\n      - At $x_2=-2$: $-2 = 3x_1-1 \\implies 3x_1=-1 \\implies x_1=-1/3$. Point is $(-1/3, -2)$.\n      The maximum $x_1$ on this segment is $x_1=1$. At this point, $(1,2)$, the objective value is $1-1=0$.\n    Comparing all candidate values $\\{2,6,3,0,1-(-1/3)=4/3\\}$, the minimum is $0$.\n    This minimum occurs at the unique point $(1,2)$.\n    Thus, the solution is $y^2 = (1,2)$ and $t^2=0$.\n\n**Iteration 2 ($k=2$)**\n\nWe now use the point $y^2 = (1,2)$.\n\n1.  **Oracle Query**: Evaluate $f$ at $y^2$:\n    $$f(y^2) = f(1,2) = \\max\\{1+2, 2(1)-2, 1-1\\} = \\max\\{3, 0, 0\\} = 3$$\n    The active function is $f_1(x) = x_1+x_2$. The subgradient is:\n    $$g^2 = g_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n\n2.  **Third Kelley Cut**: The third cut (Cut 3) is:\n    $$t \\ge f(y^2) + g^{2\\top}(x-y^2)$$\n    $$t \\ge 3 + \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} x_1 - 1 \\\\ x_2 - 2 \\end{pmatrix}$$\n    $$t \\ge 3 + (x_1-1) + (x_2-2) = 3 + x_1 - 1 + x_2 - 2$$\n    $$t \\ge x_1 + x_2$$\n\n3.  **Third Master Problem (LP3)**: Add the new cut to form LP3:\n    $$\n    \\begin{aligned}\n    (LP3) \\quad \\min_{x, t} \\quad  t \\\\\n    \\text{s.t.} \\quad  t \\ge 1 - x_1 \\quad (\\text{Cut 1}) \\\\\n     t \\ge 2x_1 - x_2 \\quad (\\text{Cut 2}) \\\\\n     t \\ge x_1 + x_2 \\quad (\\text{Cut 3}) \\\\\n     -2 \\le x_1 \\le 2, \\quad -2 \\le x_2 \\le 2\n    \\end{aligned}\n    $$\n\n4.  **Solution to LP3**: This is equivalent to finding $t^3 = \\min_{x \\in X} \\max\\{1 - x_1, 2x_1 - x_2, x_1 + x_2\\}$.\n    The function $h(x) = \\max\\{1 - x_1, 2x_1 - x_2, x_1 + x_2\\}$ is convex. Its unconstrained minimum on $\\mathbb{R}^2$ occurs at a point where the subdifferential contains the zero vector. A candidate for such a point is where all three affine functions are equal:\n    $$1 - x_1 = 2x_1 - x_2 = x_1 + x_2$$\n    From $2x_1 - x_2 = x_1 + x_2$, we get $x_1 = 2x_2$.\n    Substitute this into $1 - x_1 = x_1 + x_2$:\n    $$1 - 2x_2 = 2x_2 + x_2$$\n    $$1 = 5x_2 \\implies x_2 = \\frac{1}{5}$$\n    Then $x_1 = 2x_2 = 2(\\frac{1}{5}) = \\frac{2}{5}$.\n    Let's check this with the third equality $1 - x_1 = 2x_1 - x_2$:\n    $1 - \\frac{2}{5} = \\frac{3}{5}$ and $2(\\frac{2}{5}) - \\frac{1}{5} = \\frac{4}{5} - \\frac{1}{5} = \\frac{3}{5}$. The equalities hold.\n    The point where the three functions are equal is $x^* = (\\frac{2}{5}, \\frac{1}{5})$.\n    We check if this point is in the feasible set $X$:\n    $|x_1^*| = \\frac{2}{5} \\le 2$ and $|x_2^*| = \\frac{1}{5} \\le 2$. The point is inside $X$.\n    Since the unconstrained minimizer of the convex function $h(x)$ lies within the feasible set $X$, it is also the minimizer for the constrained problem.\n    The optimal solution for the x-variables is $y^3 = (\\frac{2}{5}, \\frac{1}{5})$.\n    The optimal value for $t$ is the value of the functions at this point:\n    $$t^3 = 1 - x_1 = 1 - \\frac{2}{5} = \\frac{3}{5}$$\n    The optimal value of the third master problem is $t^3 = \\frac{3}{5}$.\n\nSummary of results:\n- Cut 1: $t \\ge 1-x_1$. LP1 solution: $(y^1, t^1) = ((2,0), -1)$.\n- Cut 2: $t \\ge 2x_1-x_2$. LP2 solution: $(y^2, t^2) = ((1,2), 0)$.\n- Cut 3: $t \\ge x_1+x_2$. LP3 solution: $(y^3, t^3) = ((\\frac{2}{5}, \\frac{1}{5}), \\frac{3}{5})$.\nThe question asks for the exact value of $t^3$.",
            "answer": "$$\\boxed{\\frac{3}{5}}$$"
        },
        {
            "introduction": "How does the behavior of Kelley's method compare to a more familiar algorithm like gradient descent? This problem explores the fundamental difference between minimizing a global linear model versus taking a local descent step. It reveals the important, and sometimes counter-intuitive, property that cutting-plane methods do not guarantee that the true objective function will decrease at each iteration. ",
            "id": "3141039",
            "problem": "Consider the convex minimization of the Euclidean norm $f(x)=\\|x\\|_{2}$ over the cube $X=[-1,1]^{2}$. At iteration $k$, suppose the current point is $x_{k}=(\\tfrac{1}{2},\\tfrac{1}{2})\\neq 0$. Kelley's cutting-plane method constructs an affine underestimator using a subgradient at $x_{k}$ and, with a single cut from this iterate, solves a master Linear Programming (LP) problem over $X$ to obtain the next point $x_{k+1}$. In contrast, gradient descent with step size $0\\alpha\\|x_{k}\\|_{2}$ computes $x^{\\mathrm{GD}}=x_{k}-\\alpha\\nabla f(x_{k})$. Using only fundamental properties of convex functions and subgradients, and the definition of the gradient of the Euclidean norm at nonzero points, determine which of the following statements are correct in this setting.\n\nA. With a single cut at $x_{k}$, the Kelley master LP solution $x_{k+1}$ solves $\\min_{y\\in X} g_{k}^{\\top}y$, where $g_{k}$ is a subgradient at $x_{k}$, and therefore $x_{k+1}$ is the vertex $(-1,-1)$. The piecewise linear model decreases, yet the true objective $\\|x\\|_{2}$ at $x_{k+1}$ increases relative to $\\|x_{k}\\|_{2}$.\n\nB. For any step size $0\\alpha\\|x_{k}\\|_{2}$, the gradient descent update $x^{\\mathrm{GD}}=x_{k}-\\alpha\\nabla f(x_{k})$ strictly decreases the objective value from $\\|x_{k}\\|_{2}$ to $\\|x_{k}\\|_{2}-\\alpha$ and remains in $X$.\n\nC. The Kelley master LP always returns the origin $x_{k+1}=(0,0)$ when minimizing $f(x)=\\|x\\|_{2}$ over a cube, because the origin minimizes both the true function and any affine linearization at $x_{k}$.\n\nD. The Kelley master LP update $x_{k+1}$ and the gradient descent update $x^{\\mathrm{GD}}$ coincide for $f(x)=\\|x\\|_{2}$, since both use the subgradient direction at $x_{k}$.\n\nE. If more cutting planes are added (multiple past cuts), the Kelley master LP immediately corrects the underestimation for $f(x)=\\|x\\|_{2}$ so that $x_{k+1}$ cannot lie at a vertex of $X$.\n\nSelect all correct options.",
            "solution": "The user wants me to solve a problem in optimization theory.\n\n### Problem Validation\n\nFirst, I will validate the problem statement.\n\n**Step 1: Extract Givens**\n-   **Objective function**: Convex minimization of the Euclidean norm $f(x)=\\|x\\|_{2}$.\n-   **Feasible set**: The cube $X=[-1,1]^{2}$.\n-   **Current iterate**: At iteration $k$, the point is $x_{k}=(\\tfrac{1}{2},\\tfrac{1}{2})$.\n-   **Condition on iterate**: $x_{k} \\neq 0$.\n-   **Method 1 (Kelley's cutting-plane method)**: Constructs an affine underestimator using a subgradient at $x_{k}$. With a single cut from this iterate, it solves a master Linear Programming (LP) problem over $X$ to obtain the next point $x_{k+1}$.\n-   **Method 2 (Gradient descent)**: With step size $0\\alpha\\|x_{k}\\|_{2}$, it computes $x^{\\mathrm{GD}}=x_{k}-\\alpha\\nabla f(x_{k})$.\n-   **Question**: Using fundamental properties of convex functions, subgradients, and the gradient of the Euclidean norm, determine which of the provided statements are correct.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Groundedness**: The problem is well-grounded in the theory of convex optimization. It involves standard elements: a convex function ($f(x)=\\|x\\|_{2}$), a compact convex set ($X=[-1,1]^{2}$), a well-defined starting point, and two standard algorithms (Kelley's cutting-plane method and gradient descent). All concepts are mathematically rigorous.\n-   **Well-Posedness**: The problem is well-posed. It provides all necessary information to determine the next iterate for both algorithms and to evaluate the claims made in the options. A unique, verifiable answer can be derived.\n-   **Objectivity**: The problem is stated using precise, objective mathematical language.\n-   **Flaw Checklist**:\n    1.  **Scientific/Factual Unsoundness**: None. The premises are standard in convex analysis.\n    2.  **Non-Formalizable/Irrelevant**: None. The problem is a concrete application of optimization algorithms.\n    3.  **Incomplete/Contradictory Setup**: None. The problem is self-contained. The condition $x_k \\neq 0$ is crucial and correctly stated, as the gradient of $\\|x\\|_2$ is undefined at $x=0$.\n    4.  **Unrealistic/Infeasible**: None. The setup is a common textbook-style example.\n    5.  **Ill-Posed/Poorly Structured**: None.\n    6.  **Pseudo-Profound/Trivial**: None. The problem requires a solid understanding of the mechanics of the two distinct algorithms.\n    7.  **Outside Scientific Verifiability**: None. All claims are mathematically verifiable.\n\n**Step 3: Verdict and Action**\n-   **Verdict**: The problem statement is valid.\n-   **Action**: Proceed to the solution.\n\n###\n### Solution Derivation\n\nThe problem requires analyzing one step of two different optimization algorithms for minimizing $f(x) = \\|x\\|_2$ over the set $X = [-1,1]^2$, starting from the point $x_k = (\\frac{1}{2}, \\frac{1}{2})$.\n\n**1. Preliminaries: Subgradient of the Objective Function**\nThe objective function is $f(x) = \\|x\\|_2 = \\sqrt{x_1^2 + x_2^2}$.\nFor any $x \\neq 0$, $f(x)$ is differentiable, and its gradient is the unique subgradient:\n$$ \\nabla f(x) = \\frac{x}{\\|x\\|_2} $$\nThe current point is $x_k = (\\frac{1}{2}, \\frac{1}{2})$. Its Euclidean norm is:\n$$ \\|x_k\\|_2 = \\sqrt{(\\tfrac{1}{2})^2 + (\\tfrac{1}{2})^2} = \\sqrt{\\tfrac{1}{4} + \\tfrac{1}{4}} = \\sqrt{\\tfrac{1}{2}} = \\frac{1}{\\sqrt{2}} $$\nThe subgradient $g_k$ at $x_k$ is:\n$$ g_k = \\nabla f(x_k) = \\frac{x_k}{\\|x_k\\|_2} = \\frac{(\\frac{1}{2}, \\frac{1}{2})}{\\frac{1}{\\sqrt{2}}} = \\left(\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2}\\right) $$\n\n**2. Analysis of Kelley's Cutting-Plane Method**\nKelley's method approximates the convex function $f(x)$ with a piecewise linear lower model. With a single cut at $x_k$, the model consists of one affine function, which is the first-order Taylor approximation (or subgradient-based support line) at $x_k$:\n$$ l(y) = f(x_k) + g_k^\\top(y - x_k) $$\nBy definition, $l(y) \\le f(y)$ for all $y$. The algorithm finds the next iterate $x_{k+1}$ by minimizing this linear approximation over the feasible set $X$.\n$$ x_{k+1} = \\arg\\min_{y \\in X} l(y) = \\arg\\min_{y \\in X} \\left( f(x_k) + g_k^\\top(y - x_k) \\right) $$\nSince $f(x_k)$ and $g_k^\\top x_k$ are constants with respect to the optimization variable $y$, this is equivalent to solving the Linear Program (LP):\n$$ x_{k+1} = \\arg\\min_{y \\in X} g_k^\\top y $$\nWe must solve $\\min_{y \\in [-1,1]^2} g_k^\\top y$ with $g_k = (\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2})$. The objective function is $\\frac{\\sqrt{2}}{2}y_1 + \\frac{\\sqrt{2}}{2}y_2$. Since the coefficients of $y_1$ and $y_2$ are positive, this linear function is minimized over the square $[-1,1]^2$ when $y_1$ and $y_2$ take their most negative possible values, which are $y_1 = -1$ and $y_2 = -1$.\nThus, the next iterate is:\n$$ x_{k+1} = (-1, -1) $$\n\n**3. Analysis of Gradient Descent**\nThe gradient descent update is given by $x^{\\mathrm{GD}} = x_k - \\alpha \\nabla f(x_k)$, with a step size $\\alpha$ such that $0  \\alpha  \\|x_k\\|_2$.\nSubstituting the values for $x_k$ and $\\nabla f(x_k) = g_k$:\n$$ x^{\\mathrm{GD}} = x_k - \\alpha g_k = x_k - \\alpha \\frac{x_k}{\\|x_k\\|_2} = x_k \\left(1 - \\frac{\\alpha}{\\|x_k\\|_2}\\right) $$\nLet's analyze the properties of this update.\n-   **Objective Value**: The new objective value is:\n    $$ \\|x^{\\mathrm{GD}}\\|_2 = \\left\\| x_k \\left(1 - \\frac{\\alpha}{\\|x_k\\|_2}\\right) \\right\\|_2 = \\left| 1 - \\frac{\\alpha}{\\|x_k\\|_2} \\right| \\|x_k\\|_2 $$\n    The condition $0  \\alpha  \\|x_k\\|_2$ implies $0  \\frac{\\alpha}{\\|x_k\\|_2}  1$, so $1 - \\frac{\\alpha}{\\|x_k\\|_2}$ is positive. Therefore:\n    $$ \\|x^{\\mathrm{GD}}\\|_2 = \\left(1 - \\frac{\\alpha}{\\|x_k\\|_2}\\right) \\|x_k\\|_2 = \\|x_k\\|_2 - \\alpha $$\n-   **Feasibility**: We must check if $x^{\\mathrm{GD}}$ remains in $X = [-1,1]^2$.\n    The update vector is:\n    $$ x^{\\mathrm{GD}} = \\left(\\frac{1}{2}, \\frac{1}{2}\\right) - \\alpha \\left(\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2}\\right) = \\left(\\frac{1}{2} - \\frac{\\alpha\\sqrt{2}}{2}, \\frac{1}{2} - \\frac{\\alpha\\sqrt{2}}{2}\\right) $$\n    From the condition $0  \\alpha  \\|x_k\\|_2 = \\frac{1}{\\sqrt{2}}$, we have $0  \\alpha\\sqrt{2}  1$.\n    Let's check the bounds on the components of $x^{\\mathrm{GD}}$:\n    $$ \\frac{1}{2} - \\frac{1 \\cdot \\sqrt{2}}{2}  x_1^{\\mathrm{GD}}  \\frac{1}{2} - \\frac{0 \\cdot \\sqrt{2}}{2} $$\n    $$ \\frac{1}{2}(1 - \\sqrt{2})  x_1^{\\mathrm{GD}}  \\frac{1}{2} $$\n    The upper bound is $\\frac{1}{2}$, which is in $[-1,1]$. The lower bound is $\\frac{1}{2}(1-\\sqrt{2}) \\approx \\frac{1}{2}(1 - 1.414) = -0.207$, which is also in $[-1,1]$.\n    Hold on, the derivation $x^{\\mathrm{GD}} = x_k(1 - \\frac{\\alpha}{\\|x_k\\|_2})$ shows that $x^{\\mathrm{GD}}$ is a positive scaling of $x_k = (\\frac{1}{2}, \\frac{1}{2})$. Since the scaling factor $(1 - \\frac{\\alpha}{\\|x_k\\|_2})$ is between $0$ and $1$, the point $x^{\\mathrm{GD}}$ must lie on the line segment connecting the origin to $x_k$. Both coordinates of $x^{\\mathrm{GD}}$ will be positive and less than $\\frac{1}{2}$. Thus, $x^{\\mathrm{GD}}$ is definitely in $X = [-1,1]^2$.\n\n### Option-by-Option Analysis\n\n**A. With a single cut at $x_{k}$, the Kelley master LP solution $x_{k+1}$ solves $\\min_{y\\in X} g_{k}^{\\top}y$, where $g_{k}$ is a subgradient at $x_{k}$, and therefore $x_{k+1}$ is the vertex $(-1,-1)$. The piecewise linear model decreases, yet the true objective $\\|x\\|_{2}$ at $x_{k+1}$ increases relative to $\\|x_{k}\\|_{2}$.**\n-   The first part, that the LP solves $\\min_{y\\in X} g_{k}^{\\top}y$ and that the solution is $x_{k+1} = (-1, -1)$, was verified in our analysis of Kelley's method. This is correct.\n-   The second part compares the true objective value.\n    -   $\\|x_k\\|_2 = \\|(\\frac{1}{2}, \\frac{1}{2})\\|_2 = \\frac{1}{\\sqrt{2}} \\approx 0.707$.\n    -   $\\|x_{k+1}\\|_2 = \\|(-1, -1)\\|_2 = \\sqrt{(-1)^2 + (-1)^2} = \\sqrt{2} \\approx 1.414$.\n    -   Indeed, $\\|x_{k+1}\\|_2  \\|x_k\\|_2$. The true objective increases. This is a classic behavior of cutting-plane methods, which do not guarantee monotonic descent for the true objective function.\n-   The phrase \"the piecewise linear model decreases\" means that the value of the linear model at the new point $x_{k+1}$ is less than its value at the old point $x_k$.\n    -   $l(x_k) = f(x_k) = \\frac{1}{\\sqrt{2}}$.\n    -   $l(x_{k+1}) = f(x_k) + g_k^\\top(x_{k+1}-x_k) = \\frac{1}{\\sqrt{2}} + (\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2})^\\top((-\\frac{3}{2}), (-\\frac{3}{2})) = \\frac{1}{\\sqrt{2}} - \\frac{3\\sqrt{2}}{2} = \\frac{\\sqrt{2}}{2} - \\frac{3\\sqrt{2}}{2} = -\\sqrt{2}$.\n    -   Since $-\\sqrt{2}  \\frac{1}{\\sqrt{2}}$, the model value decreases. This is guaranteed by the algorithm's design.\n-   All parts of this statement are correct.\n**Verdict: Correct.**\n\n**B. For any step size $0\\alpha\\|x_{k}\\|_{2}$, the gradient descent update $x^{\\mathrm{GD}}=x_{k}-\\alpha\\nabla f(x_{k})$ strictly decreases the objective value from $\\|x_{k}\\|_{2}$ to $\\|x_{k}\\|_{2}-\\alpha$ and remains in $X$.**\n-   The first part, that the objective value becomes $\\|x_{k}\\|_{2}-\\alpha$, was verified in our analysis of gradient descent. Since $\\alpha  0$, this is a strict decrease. This is correct.\n-   The second part, that $x^{\\mathrm{GD}}$ remains in $X = [-1,1]^2$, was also verified. The new point $x^{\\mathrm{GD}}$ lies on the line segment between $(0,0)$ and $x_k = (\\frac{1}{2}, \\frac{1}{2})$, so it is guaranteed to be in $X$. This is correct.\n-   All parts of this statement are correct.\n**Verdict: Correct.**\n\n**C. The Kelley master LP always returns the origin $x_{k+1}=(0,0)$ when minimizing $f(x)=\\|x\\|_{2}$ over a cube, because the origin minimizes both the true function and any affine linearization at $x_{k}$.**\n-   The Kelley master LP solves $\\min_{y \\in X} g_k^\\top y$. In our specific case, this gave $x_{k+1}=(-1,-1)$, not $(0,0)$. So the first claim is immediately falsified by our counterexample.\n-   The reasoning is also flawed. The origin $(0,0)$ minimizes the true function $f(x)=\\|x\\|_2$. However, it does not necessarily minimize the affine linearization $l(y) = f(x_k) + g_k^\\top(y-x_k)$. Minimizing $l(y)$ is equivalent to minimizing $g_k^\\top y$. For our $g_k = (\\frac{\\sqrt{2}}{2}, \\frac{\\sqrt{2}}{2})$, the value $g_k^\\top y$ at $y=(0,0)$ is $0$. But for $y=(-1,-1) \\in X$, the value is $-\\sqrt{2}$, which is smaller. So the origin does not minimize the linearization.\n**Verdict: Incorrect.**\n\n**D. The Kelley master LP update $x_{k+1}$ and the gradient descent update $x^{\\mathrm{GD}}$ coincide for $f(x)=\\|x\\|_{2}$, since both use the subgradient direction at $x_{k}$.**\n-   Our analysis shows $x_{k+1} = (-1, -1)$ and $x^{\\mathrm{GD}}$ is a point on the open line segment between $(0,0)$ and $(\\frac{1}{2}, \\frac{1}{2})$. These points are clearly not the same.\n-   The reasoning is faulty. While both methods use the subgradient, they do so differently. Gradient descent takes a local step in the negative gradient direction. Kelley's method performs a global minimization of a linear model over the entire feasible set. These are fundamentally different operations.\n**Verdict: Incorrect.**\n\n**E. If more cutting planes are added (multiple past cuts), the Kelley master LP immediately corrects the underestimation for $f(x)=\\|x\\|_{2}$ so that $x_{k+1}$ cannot lie at a vertex of $X$.**\n-   Let's consider the next iteration after finding $x_{k+1} = (-1, -1)$. We add a new cut based on $x_{k+1}$. The new master problem will be to minimize $\\max\\{l_k(x), l_{k+1}(x)\\}$ over $X$.\n-   The problem to be solved for the next point, say $x_{k+2}$, is a Linear Program (though over a higher-dimensional space if we introduce the epigraph variable $\\theta$). The solution to an LP over a polyhedral set occurs at a vertex of that set. The projection of this vertex solution onto the $x$-space may or may not be a vertex of the original set $X$.\n-   Let's analyze the specific problem. After two cuts, one at $x_k=(\\frac{1}{2}, \\frac{1}{2})$ and one at $x_{k+1}=(-1,-1)$, the problem is to minimize $\\theta$ subject to $x \\in X$, $\\theta \\ge l_k(x)$, and $\\theta \\ge l_{k+1}(x)$. As shown in the thought process, this is equivalent to $\\min_{x \\in [-1,1]^2} \\frac{1}{\\sqrt{2}}|x_1+x_2|$. The minimum value is $0$, which is achieved for any $x$ on the line segment where $x_1+x_2=0$ within the cube $X$. This solution set includes the vertices $(1, -1)$ and $(-1, 1)$ of the cube $X$. An LP solver could return one of these vertices as the solution for $x_{k+2}$.\n-   Therefore, the statement that the next iterate *cannot* lie at a vertex of $X$ is false.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "This practice applies Kelley's method to a smooth, curved function common in regression and machine learning, of the form $f(x) = \\|Ax - b\\|_2$. Beyond reinforcing the process of calculating a cut from a gradient, this problem addresses the method's practical performance. It illustrates a key limitation: the convergence of Kelley's method can be slow because linear cuts struggle to effectively approximate a function with significant curvature. ",
            "id": "3141057",
            "problem": "Consider the convex function $f(x) = \\|A x - b\\|_{2}$ with $A \\in \\mathbb{R}^{2 \\times 2}$, $b \\in \\mathbb{R}^{2}$, and $x \\in \\mathbb{R}^{2}$. The epigraph of $f$ is the set $\\{(x,t) \\in \\mathbb{R}^{2} \\times \\mathbb{R} \\mid t \\ge f(x)\\}$. Kelley's cutting-plane method constructs linear underestimators (cuts) that support the epigraph of $f$ at previous iterates. A valid cut at a point $x_{k}$ is built from a subgradient of $f$ at $x_{k}$ and must be a supporting hyperplane of the epigraph at $(x_{k}, f(x_{k}))$. Let\n$$\nA = \\begin{bmatrix} 2  -1 \\\\ 0  1 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad x_{k} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}.\n$$\nAssume the residual $r_{k} = A x_{k} - b$ is nonzero, and you are given the vector $s_{k} = A^{\\top}(A x_{k} - b)$. Choose the option that gives a valid Kelley cut at $x_{k}$ for the function $f(x) = \\|A x - b\\|_{2}$ and correctly explains why using only linear cuts can be slow when the epigraph of $f$ is smoothly curved.\n\nA. $t \\ge \\sqrt{5} + \\dfrac{-4 x_{1} + x_{2} - 1}{\\sqrt{5}}$. Linear cuts can be slow on smooth curved epigraphs because they capture only first-order (tangent) information; a piecewise-linear underestimator requires many cuts to approximate curvature well, leading to small improvements per iteration.\n\nB. $t \\ge \\sqrt{5} + \\left(-4 x_{1} + x_{2} - 1\\right)$. Linear cuts are fast on smooth epigraphs because high curvature guarantees large steps with each added plane.\n\nC. $t \\ge -\\dfrac{4}{\\sqrt{5}} x_{1} + \\dfrac{1}{\\sqrt{5}} x_{2}$. Linear cuts are slow here because $\\|A x - b\\|_{2}$ is not convex, so planes cannot support the epigraph.\n\nD. $t \\ge \\sqrt{5} - \\dfrac{-4 x_{1} + x_{2} - 1}{\\sqrt{5}}$. Linear cuts are slow because norms are non-differentiable everywhere, preventing effective tangential approximation.",
            "solution": "The problem asks for the derivation of a valid Kelley cut for a given convex function and point, and for an evaluation of the method's performance characteristics.\n\nFirst, we establish the theoretical basis for a Kelley cut. For a convex function $f(x)$, the epigraph of $f$ is the set of points $(x, t)$ in $\\mathbb{R}^{n} \\times \\mathbb{R}$ such that $t \\ge f(x)$. A Kelley cut at an iterate $x_k$ is a linear inequality that defines a supporting hyperplane to the epigraph at the point $(x_k, f(x_k))$. This is given by the first-order approximation of the function:\n$$\nt \\ge f(x_k) + g_k^\\top (x - x_k)\n$$\nwhere $g_k$ is any subgradient of $f$ at $x_k$, i.e., $g_k \\in \\partial f(x_k)$.\n\nThe problem provides the function $f(x) = \\|A x - b\\|_{2}$ and the specific values:\n$$\nA = \\begin{bmatrix} 2  -1 \\\\ 0  1 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad x_{k} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}.\n$$\nLet's compute the components required for the Kelley cut inequality.\n\n1.  **Calculate $f(x_k)$:**\n    The residual at $x_k$ is $r_k = A x_k - b$.\n    $$\n    A x_k = \\begin{bmatrix} 2  -1 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} (2)(0) + (-1)(1) \\\\ (0)(0) + (1)(1) \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}.\n    $$\n    $$\n    r_k = A x_k - b = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix}.\n    $$\n    The problem statement assumes this residual is nonzero, which it is. Now we can calculate the function value:\n    $$\n    f(x_k) = \\|A x_k - b\\|_{2} = \\left\\| \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix} \\right\\|_{2} = \\sqrt{(-2)^2 + (-1)^2} = \\sqrt{4 + 1} = \\sqrt{5}.\n    $$\n\n2.  **Calculate a subgradient $g_k \\in \\partial f(x_k)$:**\n    The function $f(x) = \\|A x - b\\|_{2}$ is the composition of the Euclidean norm function $g(z) = \\|z\\|_2$ and the affine function $h(x) = Ax-b$. The chain rule for subdifferentials states that $\\partial f(x) = A^\\top \\partial g(h(x))$.\n    The subdifferential of the Euclidean norm $\\|z\\|_2$ is $\\{\\frac{z}{\\|z\\|_2}\\}$ if $z \\neq 0$, and the closed unit ball $\\{u \\mid \\|u\\|_2 \\le 1\\}$ if $z = 0$.\n    Since $A x_k - b = \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix} \\neq 0$, the function $f$ is differentiable at $x_k$. The subdifferential $\\partial f(x_k)$ contains only one element, the gradient $\\nabla f(x_k)$.\n    $$\n    g_k = \\nabla f(x_k) = A^\\top \\frac{A x_k - b}{\\|A x_k - b\\|_{2}}.\n    $$\n    We have $A x_k - b = \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix}$ and $\\|A x_k - b\\|_{2} = \\sqrt{5}$. The transpose of $A$ is:\n    $$\n    A^\\top = \\begin{bmatrix} 2  0 \\\\ -1  1 \\end{bmatrix}.\n    $$\n    So, the subgradient is:\n    $$\n    g_k = \\begin{bmatrix} 2  0 \\\\ -1  1 \\end{bmatrix} \\frac{1}{\\sqrt{5}} \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix} = \\frac{1}{\\sqrt{5}} \\begin{bmatrix} (2)(-2) + (0)(-1) \\\\ (-1)(-2) + (1)(-1) \\end{bmatrix} = \\frac{1}{\\sqrt{5}} \\begin{bmatrix} -4 \\\\ 1 \\end{bmatrix}.\n    $$\n    Note that the problem gives the vector $s_k = A^\\top(A x_k - b)$, which is $\\begin{bmatrix} -4 \\\\ 1 \\end{bmatrix}$. Thus, the subgradient is $g_k = s_k / f(x_k)$.\n\n3.  **Construct the Kelley cut inequality:**\n    Substituting the calculated values into the general form $t \\ge f(x_k) + g_k^\\top (x - x_k)$:\n    $$\n    t \\ge \\sqrt{5} + \\left( \\frac{1}{\\sqrt{5}} \\begin{bmatrix} -4 \\\\ 1 \\end{bmatrix} \\right)^\\top \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\right)\n    $$\n    $$\n    t \\ge \\sqrt{5} + \\frac{1}{\\sqrt{5}} \\begin{bmatrix} -4  1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 - 1 \\end{bmatrix}\n    $$\n    $$\n    t \\ge \\sqrt{5} + \\frac{1}{\\sqrt{5}} (-4x_1 + (x_2 - 1))\n    $$\n    $$\n    t \\ge \\sqrt{5} + \\frac{-4x_1 + x_2 - 1}{\\sqrt{5}}\n    $$\n    This is the valid Kelley cut at $x_k$.\n\nNow, we evaluate each option.\n\n**A. $t \\ge \\sqrt{5} + \\dfrac{-4 x_{1} + x_{2} - 1}{\\sqrt{5}}$. Linear cuts can be slow on smooth curved epigraphs because they capture only first-order (tangent) information; a piecewise-linear underestimator requires many cuts to approximate curvature well, leading to small improvements per iteration.**\n-   **Cut Formula**: The inequality matches our derived result precisely.\n-   **Explanation**: This explanation is correct. Kelley's cutting-plane method approximates the convex epigraph from below with a polyhedral set formed by the intersection of half-spaces (the cuts). For a function with a smoothly curved epigraph, such as $f(x) = \\|Ax-b\\|_2$, each linear cut is tangent to the epigraph at only one point. The gap between the linear underestimator and the actual function grows quadratically away from the tangency point. Consequently, many cuts are required to create a reasonably accurate approximation of the curved surface, which means the algorithm often takes small steps and converges slowly.\n-   **Verdict**: **Correct**.\n\n**B. $t \\ge \\sqrt{5} + \\left(-4 x_{1} + x_{2} - 1\\right)$. Linear cuts are fast on smooth epigraphs because high curvature guarantees large steps with each added plane.**\n-   **Cut Formula**: This is incorrect. The term containing $x_1$ and $x_2$ should be divided by $\\sqrt{5}$. This formula incorrectly uses $s_k = A^\\top(A x_k - b)$ as the subgradient, rather than the correctly normalized $g_k = s_k / f(x_k)$.\n-   **Explanation**: This explanation is incorrect. High curvature is precisely what makes linear approximations poor and causes slow convergence for first-order methods like Kelley's.\n-   **Verdict**: **Incorrect**.\n\n**C. $t \\ge -\\dfrac{4}{\\sqrt{5}} x_{1} + \\dfrac{1}{\\sqrt{5}} x_{2}$. Linear cuts are slow here because $\\|A x - b\\|_{2}$ is not convex, so planes cannot support the epigraph.**\n-   **Cut Formula**: This is incorrect. It is missing the constant term $f(x_k) - g_k^\\top x_k$. The full inequality is $t \\ge f(x_k) + g_k^\\top(x-x_k) = (f(x_k) - g_k^\\top x_k) + g_k^\\top x$. The constant term evaluates to $\\sqrt{5} - \\frac{1}{\\sqrt{5}}(-4(0) + 1(1)) = \\sqrt{5} - \\frac{1}{\\sqrt{5}} = \\frac{4}{\\sqrt{5}}$. So the full inequality should be $t \\ge \\frac{4}{\\sqrt{5}} + \\frac{-4x_1+x_2}{\\sqrt{5}}$. The formula provided is incomplete.\n-   **Explanation**: This explanation is fundamentally wrong. The function $f(x) = \\|A x - b\\|_{2}$ is a well-known example of a convex function. It is a composition of a convex function (the Euclidean norm) with an affine function. The entire theoretical foundation of Kelley's method rests on the function being convex, which guarantees that the subgradient-based cuts are valid global underestimators.\n-   **Verdict**: **Incorrect**.\n\n**D. $t \\ge \\sqrt{5} - \\dfrac{-4 x_{1} + x_{2} - 1}{\\sqrt{5}}$. Linear cuts are slow because norms are non-differentiable everywhere, preventing effective tangential approximation.**\n-   **Cut Formula**: This is incorrect. The sign between the $f(x_k)$ term and the subgradient term is wrong. The supporting hyperplane inequality is $t \\ge f(x_k) + g_k^\\top(x-x_k)$, not $t \\ge f(x_k) - g_k^\\top(x-x_k)$.\n-   **Explanation**: This explanation is incorrect. The Euclidean norm $\\|z\\|_2$ is differentiable everywhere except at $z=0$. The statement that norms are \"non-differentiable everywhere\" is false. At the point $x_k$, the function is differentiable because $A x_k - b \\neq 0$. The slowness is due to the poor quality of the linear approximation for a curved function, not a general lack of differentiability.\n-   **Verdict**: **Incorrect**.\n\nOnly option A provides both the correct mathematical formula for the Kelley cut and a correct conceptual explanation for the method's performance.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}