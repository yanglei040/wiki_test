## 引言
交替最小化（Alternating Minimization, AM）是一种强大而直观的迭代[优化算法](@entry_id:147840)，它通过将一个难以处理的[多变量优化](@entry_id:186720)[问题分解](@entry_id:272624)为一系列更简单的子问题来寻求解决方案。在数据科学、工程和机器学习等领域，我们经常面临需要同时优化多个相互依赖的变量集的复杂情况，直接联合优化所有变量往往在计算上不可行或非常困难。交替最小化通过一次只优化一个变量（或变量块），同时固定其他变量，巧妙地绕过了这一障碍，从而将一个大的联合[优化问题](@entry_id:266749)转化为一系列小规模、更易处理的子问题。这种“[分而治之](@entry_id:273215)”的策略不仅简化了[算法设计](@entry_id:634229)，还常常能利用子问题的特殊结构（如凸性或闭式解）来提高计算效率。

本文旨在系统地介绍交替最小化方法。首先，在“原理与机制”一章中，我们将深入探讨该算法的核心思想，揭示其与块[坐标下降](@entry_id:137565)和经典数值方法（如[高斯-赛德尔法](@entry_id:145727)）的深刻联系，并详细分析其在不同条件下的收敛特性，包括凸问题中的[线性收敛](@entry_id:163614)和非凸场景下的潜在挑战。接着，在“应用与跨学科联系”一章中，我们将展示交替最小化在矩阵分解、信号处理、计算机视觉、[机器人学](@entry_id:150623)乃至量子物理等众多领域的广泛应用，阐明其作为一种通用建模[范式](@entry_id:161181)的重要价值。最后，通过“动手实践”部分，读者将有机会通过具体的编程练习，加深对算法动态行为和实际应用中关键技巧的理解。

## 原理与机制

交替最小化（Alternating Minimization, AM）是一种强大而直观的迭代优化算法，用于解决涉及多个变量或变量块的复杂问题。其核心思想是“分而治之”：将一个困难的多变量联合最小化问题分解为一系列更简单的、针对单个变量（或变量块）的子问题。本章将深入探讨交替最小化的基本原理、其与经典数值方法的联系、收敛特性以及在[非凸优化](@entry_id:634396)中的行为。

### 基本机制：块[坐标下降](@entry_id:137565)

面对一个涉及多个变量块的目标函数 $f(x, y, \dots)$，我们希望找到使该[函数最小化](@entry_id:138381)的变量值。交替最小化的策略不是同时优化所有变量，而是在每次迭代中，固定除一个变量块之外的所有其他变量块，然后精确地最小化关于该活动变量块的[目标函数](@entry_id:267263)。这个过程循环往复，直至收敛。

对于一个双变量问题 $\min_{x, y} f(x, y)$，交替最小化的迭代过程如下：从一个初始点 $(x_0, y_0)$ 开始，第 $k+1$ 次迭代包括两个步骤：
1.  **$x$-更新**：固定 $y$ 为其当前值 $y_k$，求解关于 $x$ 的最小化问题：
    $x_{k+1} \in \arg\min_{x} f(x, y_k)$
2.  **$y$-更新**：固定 $x$ 为其新计算出的值 $x_{k+1}$，求解关于 $y$ 的最小化问题：
    $y_{k+1} \in \arg\min_{y} f(x_{k+1}, y)$

这种方法本质上是**块[坐标下降](@entry_id:137565)**（Block Coordinate Descent, BCD）的一个特例，其中每个变量块被单独优化。由于在每一步中，我们都寻求对当前子问题的**精确**最小化，因此目标函数的值在迭代过程中必然是单调不增的 ：
$f(x_{k+1}, y_{k+1}) \le f(x_{k+1}, y_k) \le f(x_k, y_k)$。
这种[单调性](@entry_id:143760)是[算法稳定性](@entry_id:147637)的一个基本保证，但并不足以保证收敛到全局或局部最小值，特别是在非凸问题中。

### 与块高斯-赛德尔方法的等价性

交替最小化不仅是一种启发式策略，它与[数值线性代数](@entry_id:144418)中的经典迭代方法有着深刻的联系。对于二次规划（Quadratic Programming, QP）问题，交替最小化在数学上等价于**块高斯-赛德尔**（Block Gauss-Seidel）方法。

考虑一个一般的无约束二次[优化问题](@entry_id:266749)  ：
$$ \min_{x \in \mathbb{R}^{n}, y \in \mathbb{R}^{m}} f(x,y) = \frac{1}{2} \begin{pmatrix} x \\ y \end{pmatrix}^{\top} \begin{pmatrix} Q  S \\ S^{\top}  R \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} + \begin{pmatrix} d \\ e \end{pmatrix}^{\top} \begin{pmatrix} x \\ y \end{pmatrix} $$
其中，$Q$ 和 $R$ 是对称正定矩阵。该问题的最优解满足一阶[平稳性条件](@entry_id:191085) $\nabla f(x,y) = 0$，这等价于求解一个块[线性系统](@entry_id:147850)：
$$ \begin{pmatrix} Q  S \\ S^{\top}  R \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = - \begin{pmatrix} d \\ e \end{pmatrix} $$
现在，我们推导交替最小化的更新规则。
1.  **$x$-更新**：固定 $y=y_k$，我们最小化 $f(x, y_k)$。其关于 $x$ 的梯度为 $\nabla_x f(x, y_k) = Qx + Sy_k + d$。令其为零，得到：
    $Qx_{k+1} + Sy_k + d = 0 \implies x_{k+1} = -Q^{-1}(Sy_k + d)$
    由于 $Q$ 是正定的，所以 $Q^{-1}$ 存在，保证了 $x_{k+1}$ 的解是唯一的。

2.  **$y$-更新**：固定 $x=x_{k+1}$，我们最小化 $f(x_{k+1}, y)$。其关于 $y$ 的梯度为 $\nabla_y f(x_{k+1}, y) = Ry + S^{\top}x_{k+1} + e$。令其为零，得到：
    $Ry_{k+1} + S^{\top}x_{k+1} + e = 0 \implies y_{k+1} = -R^{-1}(S^{\top}x_{k+1} + e)$

对比这组更新规则和应用于上述[线性系统](@entry_id:147850)的块[高斯-赛德尔迭代](@entry_id:136271)，我们会发现它们是完全相同的。块高斯-赛德尔方法顺序地求解每个变量块，并在求解后续块时立即使用最新计算出的值，这正是交替最小化的操作方式。这种等价性为分析交替最小化算法提供了强大的理论工具。

一个重要的特例是**[最小二乘问题](@entry_id:164198)** 。考虑最小化 $f(x) = \frac{1}{2} \|Ax - y\|^2$，其中 $A$ 是一个 $m \times 2$ 的矩阵，其列为 $a$ 和 $b$，即 $A = \begin{pmatrix} a  b \end{pmatrix}$，$x = (x_1, x_2)^\top$。该问题的最优解满足**[正规方程](@entry_id:142238)**（normal equations） $A^\top A x = A^\top y$。将交替最小化应用于 $x_1$ 和 $x_2$ 两个变量，其[更新过程](@entry_id:273573)在几何上可以解释为：在每一步中，将当前残差向量正交投影到矩阵 $A$ 的一列所张成的[子空间](@entry_id:150286)上。通过代数推导可以证明，这个过程与在 $2 \times 2$ 的[正规方程](@entry_id:142238)系统上执行[高斯-赛德尔迭代](@entry_id:136271)是完全等价的。

### [收敛性分析](@entry_id:151547)

#### 凸二次问题：[线性收敛](@entry_id:163614)

对于 Hessian 矩阵为正定的凸二次问题，交替最小化（即块高斯-赛德尔方法）保证收敛，且[收敛速度](@entry_id:636873)是**线性**的。收敛因子由块之间的**耦合程度**决定。

让我们回到一般的二次问题。通过分析[误差传播](@entry_id:147381)，可以推导出 $y$ 变量块的误差向量 $\delta_y^k = y_k - y^*$ 的迭代关系 ：
$$ \delta_y^{k+1} = (R^{-1}S^{\top}Q^{-1}S) \delta_y^k $$
整个 $(x,y)$ 系统的渐近[线性收敛](@entry_id:163614)因子由[迭代矩阵](@entry_id:637346)的谱半径（spectral radius）决定。经过推导，该谱半径为 ：
$$ \rho = \|Q^{-1/2} S R^{-1/2}\|_2^2 $$
这里的矩阵 $Q^{-1/2}$ 和 $R^{-1/2}$ 通常指 $Q$ 和 $R$ 的 Cholesky 分解的逆的[转置](@entry_id:142115)，它们用于“白化”或“归一化”[坐标系](@entry_id:156346)。范数 $\|Q^{-1/2} S R^{-1/2}\|_2$ 度量了经过 $x$ 和 $y$ 内部几何（由 $Q$ 和 $R$ 定义）归一化后的块间耦合强度。

这个公式告诉我们：
-   **耦合越弱，收敛越快**。如果块间没有耦合，即 $S=0$，则 $\rho=0$，算法在一次迭代内收敛。这在直觉上是显然的，因为变量块是独立的。
-   **耦合越强，收敛越慢**。随着[耦合矩阵](@entry_id:191757) $S$ 的范数增加，[谱半径](@entry_id:138984) $\rho$ 趋近于 $1$，收敛变得非常缓慢。

在[最小二乘问题](@entry_id:164198)  的例子中，这个[谱半径](@entry_id:138984)有一个非常直观的形式：$\rho = \frac{\beta^2}{\alpha\gamma}$，其中 $\alpha = \|a\|^2$, $\gamma = \|b\|^2$，而 $\beta = a^\top b$。这正是两列向量 $a$ 和 $b$ 之间夹角余弦的平方。如果两列正交（$\beta=0$），则一步收敛。如果两列几乎共线（$|\beta| \approx \sqrt{\alpha\gamma}$），则收敛因子接近 $1$，算法收敛缓慢。

#### 作为隐式预处理器

交替最小化一个显著的优点是它能够有效应对[病态问题](@entry_id:137067)。考虑一个 Hessian [矩阵条件数](@entry_id:142689)很高的二次函数，标准的[梯度下降法](@entry_id:637322)可能会因为步长选择困难而发散或收敛极慢。

在一个精心设计的例子中 ，目标函数为 $f(u,v) = \frac{1}{2} M u^2 + \frac{1}{2} m v^2 + \gamma uv$，其中 $M \gg m$（例如 $M=100, m=0.01$），导致 Hessian 矩阵 $\begin{pmatrix} M  \gamma \\ \gamma  m \end{pmatrix}$ 具有很大的[条件数](@entry_id:145150)。对于这样的问题，即使是精心选择的步长，[梯度下降法](@entry_id:637322)也可能表现不佳，甚至发散。

然而，交替最小化通过在每一步中**精确求解**子问题，隐式地对问题进行了**预处理**（preconditioning）。$u$-更新求解 $Mu + \gamma v_k = 0$，相当于用 $M^{-1}$ 作用于该方程。$v$-更新同理。这种操作等同于使用一个[块对角矩阵](@entry_id:145530) $\mathrm{diag}(M,m)$ 作为预处理器，但以高斯-赛德尔的串行方式而非雅可比（Jacobi）的并行方式应用。通过独立处理每个块内的尺度问题（如 $M$ 和 $m$ 的巨大差异），交替最小化能够有效地缓解整体的病态性，并以一个仅依赖于块间耦合（由 $\gamma^2/(mM)$ 决定）的速率[稳定收敛](@entry_id:199422)。

#### 曲率的作用与正则化

[线性收敛](@entry_id:163614)的保证依赖于每个子问题都具有足够的**曲率**（curvature），即在数学上是**强凸**的。当某个块的[目标函数](@entry_id:267263)在其最小值附近变得“平坦”时，[收敛速度](@entry_id:636873)会显著下降。

考虑这样一个例子 ：$f_p(x,y) = \frac{1}{p}x^p + \frac{1}{2}(y-x)^2$，其中 $p \ge 4$ 是一个偶数。该函数是凸的，其[全局最小值](@entry_id:165977)在 $(0,0)$。$y$-更新很简单：$y_{k+1}=x_k$。而 $x$-更新则满足 $x_{k+1}^{p-1} + x_{k+1} = x_k$。当 $x_k \to 0$ 时，$x$ 子问题的 Hessian $\nabla_{xx}^2 f_p = (p-1)x^{p-2}+1$ 在 $x=0$ 处为 $1$。然而，对于 $p>2$，该子问题在解 $x=0$ 附近并非强凸，导致曲率不足。

其后果是[收敛速度](@entry_id:636873)变为**次线性**（sublinear），$x_k$ 的衰减速度约为 $k^{-1/(p-2)}$。通过选择一个非常大的 $p$，收敛可以变得任意缓慢。

解决这个问题的一个标准技术是**正则化**（regularization）。通过向[目标函数](@entry_id:267263)添加一个二次惩罚项 $\frac{\mu}{2}x^2$（其中 $\mu > 0$），新的 $x$-子问题 Hessian 变为 $(p-1)x^{p-2} + 1 + \mu$。在 $x=0$ 处，它的值为 $1+\mu > 0$，从而保证了在解附近 $x$-子问题是强凸的。这种简单的修改将收敛行为从缓慢的次线性恢复为快速的**[局部线性收敛](@entry_id:751402)**，收敛因子约为 $1/(1+\mu)$ 。这揭示了曲率对于快速收敛的关键作用以及正则化作为一种工具来引入所需曲率的强大能力。

### 应用与扩展

#### 约束优化

交替最小化可以自然地扩展到[约束优化](@entry_id:635027)问题，特别是当约束是**可分离**的（separable），即每个约束只涉及一个变量块。考虑问题：
$$ \min_{x,y} f(x,y) \quad \text{subject to} \quad x \in X, y \in Y $$
其中 $X$ 和 $Y$ 是闭凸集。此时，交替最小化的每一步都变成一个约束优化子问题。例如，$x$-更新变为：
$$ x_{k+1} \in \arg\min_{x \in X} f(x, y_k) $$
如果 $f$ 是二次函数，这个子问题在一些特殊情况下可以通过首先计算无约束的最小化点 $x_{unc}^*$，然后将其**投影**（project）到可行集 $X$ 上来解决 。即：
$$ x_{k+1} = P_X(x_{unc}^*) $$
例如，如果 $X$ 是一个区间 $[-L, L]$，投影操作就是简单的截断。如果算法的收敛点位于可行集的内部，那么在迭代的后期，投影操作将变得无效（即 $x_{unc}^* \in X$），算法的行为退化为无约束的版本。

#### 机器学习：$k$-均值[聚类](@entry_id:266727)

$k$-均值[聚类](@entry_id:266727)（$k$-Means Clustering）是交替最小化在机器学习中的一个经典应用。其目标是将 $n$ 个数据点 $\{x_i\}$ 分配到 $K$ 个[聚类](@entry_id:266727)中，并找到每个[聚类](@entry_id:266727)的中心 $\{c_k\}$，以最小化簇内平方和。如果用一个二元分配矩阵 $Z \in \{0,1\}^{n \times K}$ 来表示哪个点属于哪个簇（$Z_{ik}=1$ 表示点 $i$ 属于簇 $k$），则 $k$-均值[目标函数](@entry_id:267263)可以写为：
$$ f(Z, C) = \sum_{i=1}^{n} \sum_{k=1}^{K} Z_{ik} \|x_i - c_k\|^2 $$
$k$-均值算法正是对这个问题应用了交替最小化：
1.  **固定中心 $C$，优化分配 $Z$**：对于每个数据点 $x_i$，我们需要找到最佳的簇 $k$ 来最小化 $\|x_i - c_k\|^2$。这等价于将每个点分配给其最近的中心。
2.  **固定分配 $Z$，优化中心 $C$**：对于每个簇 $k$，我们需要找到最佳的中心 $c_k$ 来最小化 $\sum_{i: Z_{ik}=1} \|x_i - c_k\|^2$。这个问题的解是该簇所有点的[算术平均值](@entry_id:165355)。

有趣的是，第一步（分配）虽然看起来是离散的，但可以从一个[凸松弛](@entry_id:636024)的角度来理解 。如果我们允许 $Z_{i\cdot}$ 是一个[概率分布](@entry_id:146404)（即在[概率单纯形](@entry_id:635241)中取值）而不是一个 one-hot 向量，那么对每个点 $i$，我们需要解决一个关于 $Z_{i\cdot}$ 的[线性规划](@entry_id:138188)问题。根据[凸分析](@entry_id:273238)的基本定理，线性函数在[多胞体](@entry_id:635589)（polytope）上的最小值必然在其顶点处达到。[概率单纯形](@entry_id:635241)的顶点正是 one-hot 向量。因此，即使在松弛问题中，最优解也对应于一个“硬”分配，这为 $k$-均值算法中的贪心分配步骤提供了深刻的理论依据。

### [非凸优化](@entry_id:634396)中的挑战

当[目标函数](@entry_id:267263) $f(x,y)$ 非凸时，交替最小化的行为变得更加复杂，理论保证也更弱。

#### 收敛到[鞍点](@entry_id:142576)

对于非凸问题，交替最小化不再保证收敛到局部最小值。它可能收敛到**[鞍点](@entry_id:142576)**（saddle points）——那些梯度为零但并非局部最小值的点。

考虑一个简单的非凸二次函数 $f(x,y) = \frac{1}{2}x^2 + \frac{1}{2}y^2 - 2xy$ 。其 Hessian 矩阵 $\begin{pmatrix} 1  -2 \\ -2  1 \end{pmatrix}$ 是不定的（有正负[特征值](@entry_id:154894)），表明原点 $(0,0)$ 是一个[鞍点](@entry_id:142576)。交替最小化的更新规则是 $x_{k+1} = 2y_k$ 和 $y_{k+1} = 2x_{k+1}$。如果从 $(0,0)$ 开始，那么 $x_1=0, y_1=0$，算法将永远停留在这个[鞍点](@entry_id:142576)。尽管该点不是最小值，但沿着任何坐标轴方向（$x$ 或 $y$）的子问题都是凸的，并且在该点达到最优。这是因为在 $x$-更新时，你看的是函数在切片 $y=0$ 上的行为，即 $f(x,0)=\frac{1}{2}x^2$，其在 $x=0$ 处是最小值。同理于 $y$-更新。交替最小化“看不到”沿着如 $(1,1)$ 方向（Hessian 负[特征值](@entry_id:154894)对应的方向）的下降曲率。

另一个例子是双线性函数 $f(x,y) = xy$ 在单位正方形 $[-1,1] \times [-1,1]$ 上的最小化 。原点 $(0,0)$ 是一个[鞍点](@entry_id:142576)。如果从 $(0,0)$ 开始，并且 tie-breaking 规则（当子问题有多个解时如何选择）恰好选择 $0$，那么算法也会卡在[鞍点](@entry_id:142576)。然而，对于这个问题，只要初始点不在坐标轴上，交替最小化会在一步之内收敛到[全局最小值](@entry_id:165977) $(-1,1)$ 或 $(1,-1)$，从而成功逃离[鞍点](@entry_id:142576)区域。这说明在非凸情况下，算法的行为可能对初始化和 tie-breaking 规则高度敏感。

#### 极限环

在更病态的情况下，交替最小化甚至可能不会收敛到一个点，而是进入一个**[极限环](@entry_id:274544)**（limit cycle）。这种情况的发生需要一个关键条件：子问题的最小化点**非唯一**。

如果对于所有子问题，最小值点都是唯一的，那么目标函数值将严格下降（除非已到达一个[固定点](@entry_id:156394)），这排除了循环的可能性。然而，如果子问题有多个解，算法就可以在这些解之间来回“跳跃”，同时保持[目标函数](@entry_id:267263)值不变。

一个经典的例子是 Powell 构造的函数 ，$f(x,y) = \max\{|x - \frac{1}{2}|, |y - \frac{1}{2}|\}$ 在 $[0,1] \times [0,1]$ 上。对于一个固定的 $y_k$，最小化 $f(x,y_k)$ 的解不是一个点，而是一个区间。如果采用一个“最差”的 tie-breaking 规则，例如“选择离当前 $x_k$ 最远的解”，算法可能会在两个不同的点之间无限循环，例如 $(\frac{1}{2}+r, \frac{1}{2}-r)$ 和 $(\frac{1}{2}-r, \frac{1}{2}+r)$。虽然这种情况在实践中不常见，但它揭示了交替最小化在非凸、非光滑或子问题解非唯一的情况下可能面临的理论困境。

总之，交替最小化是一种强大、灵活且易于实现的优化框架。在凸问题中，它通常表现出可靠的收敛性，并且其与经典数值方法的联系为性能分析提供了坚实的理论基础。在非凸世界中，尽管它已成为许多实用算法（如 $k$-均值）的基石，但使用者必须警惕其可能收敛到[鞍点](@entry_id:142576)或在病态情况下表现出更复杂行为的潜在风险。