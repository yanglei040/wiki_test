{
    "hands_on_practices": [
        {
            "introduction": "The heart of the proximal gradient method is the proximal operator. Before applying the full algorithm, it's essential to understand this key component. This first exercise demystifies the proximal operator by guiding you through its calculation for a simple quadratic function, directly from its definition. You will see that finding the operator is equivalent to solving a small optimization problem, which in this case can be done using basic calculus. ",
            "id": "2195112",
            "problem": "In the field of numerical optimization, the proximal operator is a fundamental tool used in algorithms designed to solve non-differentiable or constrained problems. For a given scalar function $g(x)$ and a positive scaling parameter $\\lambda > 0$, the proximal operator of $\\lambda g$ applied to a point $v$ is defined as the value of $x$ that minimizes a composite objective function.\n\nThe definition is formally given by:\n$$\n\\text{prox}_{\\lambda g}(v) = \\arg\\min_{x \\in \\mathbb{R}} \\left( g(x) + \\frac{1}{2\\lambda} (x-v)^2 \\right)\n$$\nYour task is to find the proximal operator for a general quadratic function. Consider the function $g(x) = \\frac{1}{2}ax^2 + bx$, where $a$ and $b$ are real-valued constants and $a > 0$.\n\nDerive a closed-form analytic expression for $\\text{prox}_{\\lambda g}(v)$ in terms of the parameters $a$, $b$, $v$, and $\\lambda$.",
            "solution": "We are asked to compute $\\text{prox}_{\\lambda g}(v)$ for $g(x)=\\frac{1}{2}a x^{2}+b x$ with $a0$ and $\\lambda0$. By definition,\n$$\n\\text{prox}_{\\lambda g}(v)=\\arg\\min_{x\\in\\mathbb{R}}\\left(\\frac{1}{2}a x^{2}+b x+\\frac{1}{2\\lambda}(x-v)^{2}\\right).\n$$\nDefine the objective\n$$\nJ(x)=\\frac{1}{2}a x^{2}+b x+\\frac{1}{2\\lambda}(x-v)^{2}.\n$$\nSince $a0$ and $\\lambda0$, the function $J$ is strictly convex because its second derivative is\n$$\nJ''(x)=a+\\frac{1}{\\lambda}0,\n$$\nso it has a unique minimizer characterized by the first-order optimality condition $J'(x)=0$. Compute the derivative:\n$$\nJ'(x)=a x+b+\\frac{1}{\\lambda}(x-v)=(a+\\frac{1}{\\lambda})x+b-\\frac{v}{\\lambda}.\n$$\nSet $J'(x)=0$ and solve for $x$:\n$$\n(a+\\frac{1}{\\lambda})x+b-\\frac{v}{\\lambda}=0\n\\;\\;\\Longrightarrow\\;\\;\nx=\\frac{\\frac{v}{\\lambda}-b}{a+\\frac{1}{\\lambda}}.\n$$\nMultiplying numerator and denominator by $\\lambda$ gives\n$$\nx=\\frac{v-b\\lambda}{1+a\\lambda}.\n$$\nTherefore,\n$$\n\\text{prox}_{\\lambda g}(v)=\\frac{v-b\\lambda}{1+a\\lambda}.\n$$",
            "answer": "$$\\boxed{\\frac{v-b\\lambda}{1+a\\lambda}}$$"
        },
        {
            "introduction": "Now that you've seen how to derive a proximal operator, let's put it into practice within a single iteration of the proximal gradient method. This exercise illustrates the two-stage nature of each update: a standard gradient step on the smooth part of the function, followed by a 'correction' using the proximal operator to handle the non-smooth constraint. By working through a concrete numerical example, you'll gain a tangible feel for how the algorithm navigates toward a solution while respecting constraints. ",
            "id": "2195110",
            "problem": "Consider the optimization problem of finding a point $\\mathbf{x} = (x_1, x_2) \\in \\mathbb{R}^2$ that minimizes a function $F(\\mathbf{x})$ subject to non-negativity constraints on its components, i.e., $x_1 \\ge 0$ and $x_2 \\ge 0$. The function to be minimized is the squared Euclidean distance from $\\mathbf{x}$ to a target point $\\mathbf{a}$, given by $F(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{x} - \\mathbf{a}\\|_2^2$.\n\nThis problem can be cast into the standard form for proximal algorithms, $\\min_{\\mathbf{x}} f(\\mathbf{x}) + g(\\mathbf{x})$, by defining the smooth part as $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{x} - \\mathbf{a}\\|_2^2$ and the non-smooth part $g(\\mathbf{x})$ as the indicator function for the non-negative orthant. The indicator function $g(\\mathbf{x})$ is zero if $x_1 \\ge 0$ and $x_2 \\ge 0$, and infinity otherwise.\n\nYou are tasked with applying the proximal gradient method to solve this problem. The iterative update rule for the proximal gradient method is given by:\n$$ \\mathbf{x}_{k+1} = \\text{prox}_{\\gamma g}(\\mathbf{x}_k - \\gamma \\nabla f(\\mathbf{x}_k)) $$\nwhere $\\gamma$ is the step size and $\\text{prox}_{\\gamma g}$ is the proximal operator associated with the function $g$.\n\nGiven the target point $\\mathbf{a} = (5, -4)$, the initial point $\\mathbf{x}_0 = (1, 1)$, and a step size of $\\gamma = 0.2$, compute the next iterate, $\\mathbf{x}_1$. Express your answer as a row vector $(x_{1,1}, x_{1,2})$, where $x_{1,1}$ and $x_{1,2}$ are the components of the vector $\\mathbf{x}_1$.",
            "solution": "We are minimizing $F(\\mathbf{x})=\\frac{1}{2}\\|\\mathbf{x}-\\mathbf{a}\\|_{2}^{2}$ over the non-negative orthant. In the proximal gradient decomposition, set $f(\\mathbf{x})=\\frac{1}{2}\\|\\mathbf{x}-\\mathbf{a}\\|_{2}^{2}$ and $g(\\mathbf{x})=\\iota_{\\mathbb{R}_{+}^{2}}(\\mathbf{x})$, the indicator of the feasible set $\\mathbb{R}_{+}^{2}=\\{\\mathbf{x}\\in\\mathbb{R}^{2}:x_{1}\\ge 0,\\ x_{2}\\ge 0\\}$.\n\nThe gradient of $f$ is given by\n$$\n\\nabla f(\\mathbf{x})=\\mathbf{x}-\\mathbf{a}.\n$$\nThe proximal operator of $\\gamma g$ at a point $\\mathbf{z}$ is the Euclidean projection onto $\\mathbb{R}_{+}^{2}$:\n$$\n\\text{prox}_{\\gamma g}(\\mathbf{z})=\\operatorname*{arg\\,min}_{\\mathbf{y}\\in\\mathbb{R}^{2}}\\left\\{\\iota_{\\mathbb{R}_{+}^{2}}(\\mathbf{y})+\\frac{1}{2\\gamma}\\|\\mathbf{y}-\\mathbf{z}\\|_{2}^{2}\\right\\}=P_{\\mathbb{R}_{+}^{2}}(\\mathbf{z}),\n$$\nwhich is the componentwise truncation at zero:\n$$\nP_{\\mathbb{R}_{+}^{2}}(\\mathbf{z})=\\big(\\max\\{z_{1},0\\},\\ \\max\\{z_{2},0\\}\\big).\n$$\n\nWith $\\mathbf{a}=(5,-4)$, $\\mathbf{x}_{0}=(1,1)$, and $\\gamma=0.2$, compute the gradient at $\\mathbf{x}_{0}$:\n$$\n\\nabla f(\\mathbf{x}_{0})=\\mathbf{x}_{0}-\\mathbf{a}=(1,1)-(5,-4)=(-4,5).\n$$\nPerform the gradient step:\n$$\n\\mathbf{x}_{0}-\\gamma \\nabla f(\\mathbf{x}_{0})=(1,1)-0.2\\,(-4,5)=(1+0.8,\\ 1-1)=\\left(\\frac{9}{5},\\ 0\\right).\n$$\nApply the proximal map, i.e., the projection onto $\\mathbb{R}_{+}^{2}$:\n$$\n\\mathbf{x}_{1}=\\text{prox}_{\\gamma g}\\big(\\mathbf{x}_{0}-\\gamma \\nabla f(\\mathbf{x}_{0})\\big)=P_{\\mathbb{R}_{+}^{2}}\\left(\\frac{9}{5},0\\right)=\\left(\\frac{9}{5},0\\right),\n$$\nsince both components are already non-negative.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{9}{5}  0 \\end{pmatrix}}$$"
        },
        {
            "introduction": "An algorithm's mechanical correctness is one thing; its practical performance is another. This final practice moves from the 'how' to the 'why,' exploring a factor that critically impacts the convergence speed of the proximal gradient method. You will investigate how the structure of your data—specifically, the correlation between data features—influences the Lipschitz constant of the gradient, which in turn dictates the maximum allowable step size. This exercise provides a crucial insight into why the same algorithm can perform very differently on different datasets. ",
            "id": "2195111",
            "problem": "In machine learning, the Least Absolute Shrinkage and Selection Operator (LASSO) is a regression analysis method that performs both variable selection and regularization. The LASSO optimization problem is formulated as finding a vector of coefficients $\\mathbf{x} \\in \\mathbb{R}^n$ that minimizes the objective function:\n$$ F(\\mathbf{x}) = f(\\mathbf{x}) + g(\\mathbf{x}) = \\frac{1}{2} \\|A\\mathbf{x} - \\mathbf{b}\\|_2^2 + \\lambda \\|\\mathbf{x}\\|_1 $$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is the data matrix, $\\mathbf{b} \\in \\mathbb{R}^m$ is the observation vector, and $\\lambda  0$ is a regularization parameter.\n\nThis problem is often solved using the proximal gradient method, an iterative algorithm with the update rule:\n$$ \\mathbf{x}_{k+1} = \\text{prox}_{\\gamma g}(\\mathbf{x}_k - \\gamma \\nabla f(\\mathbf{x}_k)) $$\nwhere $\\gamma  0$ is the step size and $\\text{prox}_{\\gamma g}$ is the proximal operator of the function $g$.\n\nThe theoretical convergence rate of the proximal gradient method depends critically on the Lipschitz constant, $L$, of the gradient of the smooth component, $\\nabla f(\\mathbf{x})$. For guaranteed convergence, the step size must satisfy $\\gamma  2/L$. A conservative and common choice is to set the step size proportional to $1/L$. A larger value of $L$ therefore forces the use of a smaller step size, which generally leads to slower convergence.\n\nConsider a simplified scenario where the data matrix $A \\in \\mathbb{R}^{m \\times 2}$ (for $m \\ge 2$) consists of two column vectors, $a_1$ and $a_2$. The columns are normalized such that $\\|a_1\\|_2 = 1$ and $\\|a_2\\|_2 = 1$. The Pearson correlation coefficient between the columns is given by $\\rho = a_1^T a_2$, where we consider $\\rho \\in [0, 1)$.\n\nYour task is to quantify how the column correlation $\\rho$ affects the Lipschitz constant $L$. Specifically, calculate the ratio $L_{0.8} / L_{0.2}$, where $L_{\\rho}$ denotes the Lipschitz constant of $\\nabla f(\\mathbf{x})$ when the column correlation is $\\rho$. This ratio represents the factor by which the maximum allowable step size must be decreased when the column correlation increases from $0.2$ to $0.8$.\n\nExpress your answer as an exact fraction in simplest form.",
            "solution": "We want the Lipschitz constant of the gradient of the smooth part $f(\\mathbf{x}) = \\frac{1}{2}\\|A\\mathbf{x} - \\mathbf{b}\\|_{2}^{2}$. The gradient is\n$$\n\\nabla f(\\mathbf{x}) = A^{T}(A\\mathbf{x} - \\mathbf{b}) = A^{T}A\\,\\mathbf{x} - A^{T}\\mathbf{b}.\n$$\nFor a quadratic $f$ with Hessian $H = A^{T}A$, the gradient is $L$-Lipschitz with\n$$\nL = \\|A^{T}A\\|_{2},\n$$\nwhich, since $A^{T}A$ is symmetric positive semidefinite, equals its largest eigenvalue.\n\nWith $A = [a_{1}\\; a_{2}]$ where $\\|a_{1}\\|_{2} = \\|a_{2}\\|_{2} = 1$ and $\\rho = a_{1}^{T}a_{2} \\in [0,1)$, the Gram matrix is\n$$\nA^{T}A = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}.\n$$\nThe eigenvalues solve\n$$\n\\det\\!\\left(\\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix} - \\lambda I\\right) = (1 - \\lambda)^{2} - \\rho^{2} = 0,\n$$\nso\n$$\n\\lambda_{\\pm} = 1 \\pm \\rho.\n$$\nTherefore, the Lipschitz constant is\n$$\nL_{\\rho} = \\lambda_{\\max}(A^{T}A) = 1 + \\rho.\n$$\nThe requested ratio is\n$$\n\\frac{L_{0.8}}{L_{0.2}} = \\frac{1 + 0.8}{1 + 0.2} = \\frac{1 + \\frac{4}{5}}{1 + \\frac{1}{5}} = \\frac{\\frac{9}{5}}{\\frac{6}{5}} = \\frac{9}{6} = \\frac{3}{2}.\n$$\nThus the maximum allowable step size (proportional to $1/L$) must be decreased by a factor of $\\frac{3}{2}$ when the correlation increases from $0.2$ to $0.8$.",
            "answer": "$$\\boxed{\\frac{3}{2}}$$"
        }
    ]
}