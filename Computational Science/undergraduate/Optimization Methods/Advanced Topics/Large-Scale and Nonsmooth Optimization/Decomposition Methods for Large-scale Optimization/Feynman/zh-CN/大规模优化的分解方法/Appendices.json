{
    "hands_on_practices": [
        {
            "introduction": "本练习将探讨一种最基础的分解技术：拉格朗日松弛。通过使用像“价格”一样的拉格朗日乘子，将一个棘手的耦合约束“松弛”到目标函数中，我们可以将一个大问题分解为多个独立的子问题。这个练习  将指导你通过实现对偶次梯度法来迭代更新这个“价格”，并观察不同的算法参数如何影响解的质量和可行性。",
            "id": "3116747",
            "problem": "给定一个系列的凸可分优化问题，其中包含一个单一的线性耦合约束。对于整数 $m \\ge 2$，每个问题的形式如下\n$$\n\\min_{x \\in \\mathbb{R}^m} \\;\\; f(x) = \\sum_{i=1}^m f_i(x_i)\n\\quad \\text{subject to} \\quad \\sum_{i=1}^m A_i x_i = b, \\quad \\ell_i \\le x_i \\le u_i,\n$$\n其中每个分量函数都是二次的，\n$$\nf_i(x_i) = \\tfrac{1}{2} q_i x_i^2 + c_i x_i,\n$$\n且 $q_i > 0$，边界满足 $\\ell_i  u_i$，因此可行集非空有界。耦合是一个单一的标量等式约束，其系数为 $A_i$，右侧项为 $b$。\n\n从（非增广）拉格朗日函数及其对偶函数的定义出发，考虑使用对偶变量 $\\lambda \\in \\mathbb{R}$ 的标准拉格朗日松弛，\n$$\n\\mathcal{L}(x,\\lambda) = \\sum_{i=1}^m f_i(x_i) + \\lambda \\Big( \\sum_{i=1}^m A_i x_i - b \\Big),\n$$\n以及对偶函数 $g(\\lambda) = \\inf_{x \\in [\\ell,u]} \\mathcal{L}(x,\\lambda)$，其中 $[\\ell,u]$ 表示盒子（box）$\\prod_{i=1}^m [\\ell_i,u_i]$。对偶问题是在 $\\lambda \\in \\mathbb{R}$ 上最大化 $g(\\lambda)$。在对偶问题上使用定步长次梯度上升法，步长等于 $1/\\rho$，其中 $\\rho > 0$ 是用户选择的罚参数。具体来说，从初始值 $\\lambda_0 = 0$ 开始，对 $k = 0,1,\\ldots,T-1$ 进行迭代：\n- 计算 $x^{(k)} \\in \\arg\\min_{\\ell \\le x \\le u}\\, \\mathcal{L}(x,\\lambda_k)$（此计算在索引 $i$ 上是可分的）。\n- 计算标量次梯度 $r_k = \\sum_{i=1}^m A_i x_i^{(k)} - b$。\n- 更新 $\\lambda_{k+1} = \\lambda_k + \\frac{1}{\\rho}\\, r_k$。\n\n经过 $T$ 次迭代后，构建平均原始候选解 $\\bar{x} = \\frac{1}{T} \\sum_{k=0}^{T-1} x^{(k)}$。令 $p^\\star$ 表示原始约束问题的最优原始值，令 $d_{\\text{best}}$ 表示在 $T$ 次迭代中观察到的最佳对偶值，即 $d_{\\text{best}} = \\max_{0 \\le k \\le T-1} g(\\lambda_k)$。请量化：\n- 对偶间隙 $p^\\star - d_{\\text{best}}$（一个非负浮点数）。\n- 恢复的平均解的原始可行性残差 $|\\sum_{i=1}^m A_i \\bar{x}_i - b|$（一个非负浮点数）。\n\n你的程序必须：\n1. 仅使用问题的核心定义和一阶最优性原理，高精度地计算 $p^\\star$。你不能假定任何预先推导的闭式解。对与等式约束相关的单一拉格朗日乘子使用数值稳健的标量求根方法，并使用 Karush–Kuhn–Tucker 条件来处理箱式约束。\n2. 实现上述描述的、步长为常数 $1/\\rho$ 的对偶次梯度上升法，在每次迭代中使用最小化 $x^{(k)}$ 计算 $g(\\lambda_k)$，并跟踪 $d_{\\text{best}}$。\n3. 恢复 $\\bar{x}$ 并计算可行性残差。\n4. 对每个测试用例，报告一对浮点数：首先是 $p^\\star - d_{\\text{best}}$，然后是 $|\\sum_{i=1}^m A_i \\bar{x}_i - b|$。每个浮点数应四舍五入到 $6$ 位小数。\n\n你必须使用的基本原理：\n- 等式约束的拉格朗日函数 $\\mathcal{L}(x,\\lambda)$ 的定义。\n- 对偶函数 $g(\\lambda) = \\inf_x \\mathcal{L}(x,\\lambda)$ 的定义，以及 $g$ 在 $\\lambda$ 处的次梯度是原始残差 $\\sum_i A_i x_i^\\star(\\lambda) - b$ 这一事实，其中 $x^\\star(\\lambda)$ 是 $\\mathcal{L}(\\cdot,\\lambda)$ 在箱式约束上的任何最小化子。\n- 针对具有单一线性等式和边界约束的凸二次规划的 Karush–Kuhn–Tucker 条件，用于通过对拉格朗日乘子进行标量搜索来计算 $p^\\star$。\n\n测试套件：\n使用以下五个测试用例，每个用例指定为 $(m, q, c, A, \\ell, u, b, T, \\rho)$，其中向量按坐标顺序给出：\n- 案例 1 (happy path, medium scale): $m = 4$, $q = [\\,2,\\,1,\\,4,\\,3\\,]$, $c = [\\,0,\\,-1,\\,2,\\,-0.5\\,]$, $A = [\\,1,\\,-2,\\,1.5,\\,1\\,]$, $\\ell = [\\,-1,\\,0,\\,-0.5,\\,-2\\,]$, $u = [\\,2,\\,2,\\,1.5,\\,1\\,]$, $b = 0.7$, $T = 200$, $\\rho = 0.5$。\n- 案例 2 (same data, milder step): $m = 4$, $q = [\\,2,\\,1,\\,4,\\,3\\,]$, $c = [\\,0,\\,-1,\\,2,\\,-0.5\\,]$, $A = [\\,1,\\,-2,\\,1.5,\\,1\\,]$, $\\ell = [\\,-1,\\,0,\\,-0.5,\\,-2\\,]$, $u = [\\,2,\\,2,\\,1.5,\\,1\\,]$, $b = 0.7$, $T = 200$, $\\rho = 2$。\n- 案例 3 (same data, small step): $m = 4$, $q = [\\,2,\\,1,\\,4,\\,3\\,]$, $c = [\\,0,\\,-1,\\,2,\\,-0.5\\,]$, $A = [\\,1,\\,-2,\\,1.5,\\,1\\,]$, $\\ell = [\\,-1,\\,0,\\,-0.5,\\,-2\\,]$, $u = [\\,2,\\,2,\\,1.5,\\,1\\,]$, $b = 0.7$, $T = 200$, $\\rho = 20$。\n- 案例 4 (boundary optimum on upper bounds): $m = 3$, $q = [\\,1,\\,1,\\,1\\,]$, $c = [\\,0.1,\\,-0.2,\\,0.3\\,]$, $A = [\\,1,\\,1,\\,1\\,]$, $\\ell = [\\,0,\\,0,\\,0\\,]$, $u = [\\,1,\\,1,\\,1\\,]$, $b = 3$, $T = 200$, $\\rho = 2$。\n- 案例 5 (ill-conditioned curvature): $m = 4$, $q = [\\,0.1,\\,10,\\,0.5,\\,8\\,]$, $c = [\\,0,\\,0,\\,0,\\,0\\,]$, $A = [\\,1,\\,1,\\,-1,\\,2\\,]$, $\\ell = [\\,-1,\\,-1,\\,-1,\\,-1\\,]$, $u = [\\,1,\\,1,\\,1,\\,1\\,]$, $b = 0.3$, $T = 200$, $\\rho = 2$。\n\n最终输出格式：\n你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按顺序包含五个案例中每个案例的对偶间隙和可行性残差，两者均四舍五入到 $6$ 位小数。例如，输出必须如下所示\n$$\n[\\;g_1,\\;r_1,\\;g_2,\\;r_2,\\;g_3,\\;r_3,\\;g_4,\\;r_4,\\;g_5,\\;r_5\\;],\n$$\n其中 $g_j$ 是案例 $j$ 的 $p^\\star - d_{\\text{best}}$，$r_j$ 是案例 $j$ 的 $|\\sum_i A_i \\bar{x}_i - b|$。不得打印任何其他文本。",
            "solution": "该问题提出了一个带有单线性耦合约束和箱式约束的凸可分二次优化问题。任务是计算精确的原始最优值，实现对偶次梯度上升算法，并报告由此产生的对偶间隙和原始可行性残差。\n\n该问题是有效的。它在科学上基于凸优化和对偶理论的原理。目标函数 $f(x) = \\sum_{i=1}^m f_i(x_i)$，其中每个 $f_i(x_i) = \\frac{1}{2} q_i x_i^2 + c_i x_i$ 且 $q_i > 0$，是严格凸的。可行集由超平面 $\\sum_{i=1}^m A_i x_i = b$ 和一个紧致的箱式区域 $[\\ell, u] = \\prod_{i=1}^m [\\ell_i, u_i]$ 的交集定义，是凸且紧致的。问题陈述保证了该集合非空。因此，存在唯一的解 $x^\\star$，该问题是适定的。所有参数和算法细节都已精确指定，使得该问题是自包含且客观的。\n\n我们的解决方案主要分两个阶段进行：首先，通过 Karush-Kuhn-Tucker (KKT) 条件计算精确的原始最优值 $p^\\star$；其次，实现指定的对偶次梯度方法以找到最佳对偶界 $d_{\\text{best}}$ 和平均原始解 $\\bar{x}$。\n\n### 第1部分：通过KKT条件求解精确原始解\n\n为了找到精确的原始最优值 $p^\\star$，我们直接求解原始问题。该问题是：\n$$\n\\min_{x \\in \\mathbb{R}^m} \\;\\; \\sum_{i=1}^m \\left(\\tfrac{1}{2} q_i x_i^2 + c_i x_i\\right)\n\\quad \\text{s.t.} \\quad \\sum_{i=1}^m A_i x_i = b, \\quad \\ell_i \\le x_i \\le u_i \\text{ for } i=1,\\ldots,m.\n$$\n这是一个凸优化问题。KKT 条件为最优性提供了必要和充分条件。我们为等式约束引入一个拉格朗日乘子 $\\nu \\in \\mathbb{R}$，为下界和上界约束 $x_i \\ge \\ell_i$ 和 $x_i \\le u_i$ 分别引入乘子 $\\mu_{l,i} \\ge 0$ 和 $\\mu_{u,i} \\ge 0$。完整的拉格朗日函数是：\n$$\n\\mathcal{L}_{\\text{full}}(x, \\nu, \\mu_l, \\mu_u) = \\sum_{i=1}^m \\left(\\tfrac{1}{2} q_i x_i^2 + c_i x_i\\right) + \\nu \\left(\\sum_{i=1}^m A_i x_i - b\\right) - \\sum_{i=1}^m \\mu_{l,i} (x_i - \\ell_i) + \\sum_{i=1}^m \\mu_{u,i} (u_i - x_i).\n$$\nKKT 定常性条件要求对每个 $i=1,\\ldots,m$ 都有 $\\nabla_{x_i} \\mathcal{L}_{\\text{full}} = 0$：\n$$\nq_i x_i + c_i + \\nu A_i - \\mu_{l,i} + \\mu_{u,i} = 0.\n$$\n互补松弛条件是 $\\mu_{l,i} (x_i - \\ell_i) = 0$ 和 $\\mu_{u,i} (u_i - x_i) = 0$。\n我们分析 $x_i$ 在最优解处的三种可能性：\n1.  如果 $\\ell_i  x_i  u_i$，则 $\\mu_{l,i} = \\mu_{u,i} = 0$，这意味着 $q_i x_i + c_i + \\nu A_i = 0$，因此 $x_i = \\frac{-c_i - \\nu A_i}{q_i}$。\n2.  如果 $x_i = \\ell_i$，则 $\\mu_{u,i} = 0$。定常性给出 $\\mu_{l,i} = q_i \\ell_i + c_i + \\nu A_i$。对偶可行性 $\\mu_{l,i} \\ge 0$ 要求 $q_i \\ell_i + c_i + \\nu A_i \\ge 0$，或 $\\frac{-c_i - \\nu A_i}{q_i} \\le \\ell_i$。\n3.  如果 $x_i = u_i$，则 $\\mu_{l,i} = 0$。定常性给出 $\\mu_{u,i} = -(q_i u_i + c_i + \\nu A_i)$。对偶可行性 $\\mu_{u,i} \\ge 0$ 要求 $q_i u_i + c_i + \\nu A_i \\le 0$，或 $\\frac{-c_i - \\nu A_i}{q_i} \\ge u_i$。\n\n综合这些情况，对于给定的最优乘子 $\\nu^\\star$，最优解的每个分量 $x_i^\\star$ 由无约束最小化子在可行区间 $[\\ell_i, u_i]$ 上的投影给出：\n$$\nx_i^\\star(\\nu) = \\text{proj}_{[\\ell_i, u_i]} \\left( \\frac{-c_i - \\nu A_i}{q_i} \\right) = \\max\\left(\\ell_i, \\min\\left(u_i, \\frac{-c_i - \\nu A_i}{q_i}\\right)\\right).\n$$\n最优乘子 $\\nu^\\star$ 是确保满足原始等式约束的值：\n$$\n\\phi(\\nu) := \\sum_{i=1}^m A_i x_i^\\star(\\nu) - b = 0.\n$$\n函数 $\\phi(\\nu)$ 是连续且单调不增的，因为每个分量 $A_i x_i^\\star(\\nu)$ 都是 $\\nu$ 的不增函数。问题保证了可行解的存在，这意味着值 $b$ 位于 $\\sum_{i=1}^m A_i x_i$ 在 $x \\in [\\ell, u]$ 时的可达范围内。这确保了方程 $\\phi(\\nu) = 0$ 至少有一个解 $\\nu^\\star$。我们可以使用数值标量求根方法（如 Brent's method）在一个足够大的区间上找到这个根。一旦找到 $\\nu^\\star$，最优原始解就是 $x^\\star = (x_1^\\star(\\nu^\\star), \\ldots, x_m^\\star(\\nu^\\star))$，最优值为 $p^\\star = f(x^\\star) = \\sum_{i=1}^m (\\frac{1}{2} q_i (x_i^\\star)^2 + c_i x_i^\\star)$。\n\n### 第2部分：对偶次梯度上升法\n\n对偶次梯度法作用于对偶问题，即最大化对偶函数 $g(\\lambda)$。（非增广）拉格朗日函数是：\n$$\n\\mathcal{L}(x,\\lambda) = \\sum_{i=1}^m f_i(x_i) + \\lambda \\left( \\sum_{i=1}^m A_i x_i - b \\right).\n$$\n对偶函数是 $g(\\lambda) = \\inf_{\\ell \\le x \\le u} \\mathcal{L}(x,\\lambda)$。$g(\\lambda)$ 定义中的最小化在 $x$ 的分量上是可分的：\n$$\ng(\\lambda) = \\sum_{i=1}^m \\left( \\min_{\\ell_i \\le x_i \\le u_i} \\left\\{ \\tfrac{1}{2} q_i x_i^2 + (c_i + \\lambda A_i) x_i \\right\\} \\right) - \\lambda b.\n$$\n对于给定的 $\\lambda$，最小化子 $x_i(\\lambda)$ 如同在 KKT 分析中一样，通过将二次项的无约束最小化子投影到 $[\\ell_i, u_i]$ 上来找到：\n$$\nx_i(\\lambda) = \\text{proj}_{[\\ell_i, u_i]} \\left( \\frac{-(c_i + \\lambda A_i)}{q_i} \\right).\n$$\n凹函数 $g(\\lambda)$ 在点 $\\lambda_k$ 处的一个次梯度由原始残差 $r_k = \\sum_{i=1}^m A_i x_i^{(k)} - b$ 给出，其中 $x^{(k)}$ 是任意一个最小化子 $x(\\lambda_k)$。次梯度上升算法沿着次梯度的方向更新对偶变量 $\\lambda$。从 $\\lambda_0 = 0$ 开始，对 $k = 0, 1, \\ldots, T-1$ 的迭代过程如下：\n1.  为当前对偶变量 $\\lambda_k$ 计算原始最小化子：\n    $x^{(k)} = (x_1(\\lambda_k), \\ldots, x_m(\\lambda_k))$。\n2.  计算对偶函数值 $g(\\lambda_k) = \\mathcal{L}(x^{(k)}, \\lambda_k)$ 并更新迄今为止的最佳对偶值：$d_{\\text{best}} = \\max(d_{\\text{best}}, g(\\lambda_k))$。\n3.  计算次梯度（原始残差）：$r_k = \\sum_{i=1}^m A_i x_i^{(k)} - b$。\n4.  使用指定的常数步长 $1/\\rho$ 更新对偶变量：\n    $\\lambda_{k+1} = \\lambda_k + \\frac{1}{\\rho} r_k$。\n\n经过 $T$ 次迭代后，我们得到平均原始候选解 $\\bar{x} = \\frac{1}{T} \\sum_{k=0}^{T-1} x^{(k)}$。\n\n### 第3部分：量化指标\n\n使用上面计算出的值，我们计算两个所需的指标：\n1.  **对偶间隙**：这衡量了真实原始最优值与算法找到的最佳对偶界之间的差异：$p^\\star - d_{\\text{best}}$。根据弱对偶性，这个值总为非负。\n2.  **原始可行性残差**：这衡量了恢复的平均解在多大程度上满足耦合约束：$|\\sum_{i=1}^m A_i \\bar{x}_i - b|$。\n\n对每个提供的测试用例都执行这些计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results in the required format.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, medium scale)\n        (4, [2.0, 1.0, 4.0, 3.0], [0.0, -1.0, 2.0, -0.5], [1.0, -2.0, 1.5, 1.0], \n         [-1.0, 0.0, -0.5, -2.0], [2.0, 2.0, 1.5, 1.0], 0.7, 200, 0.5),\n        # Case 2 (same data, milder step)\n        (4, [2.0, 1.0, 4.0, 3.0], [0.0, -1.0, 2.0, -0.5], [1.0, -2.0, 1.5, 1.0],\n         [-1.0, 0.0, -0.5, -2.0], [2.0, 2.0, 1.5, 1.0], 0.7, 200, 2.0),\n        # Case 3 (same data, small step)\n        (4, [2.0, 1.0, 4.0, 3.0], [0.0, -1.0, 2.0, -0.5], [1.0, -2.0, 1.5, 1.0],\n         [-1.0, 0.0, -0.5, -2.0], [2.0, 2.0, 1.5, 1.0], 0.7, 200, 20.0),\n        # Case 4 (boundary optimum on upper bounds)\n        (3, [1.0, 1.0, 1.0], [0.1, -0.2, 0.3], [1.0, 1.0, 1.0], \n         [0.0, 0.0, 0.0], [1.0, 1.0, 1.0], 3.0, 200, 2.0),\n        # Case 5 (ill-conditioned curvature)\n        (4, [0.1, 10.0, 0.5, 8.0], [0.0, 0.0, 0.0, 0.0], [1.0, 1.0, -1.0, 2.0],\n         [-1.0, -1.0, -1.0, -1.0], [1.0, 1.0, 1.0, 1.0], 0.3, 200, 2.0),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        m, q, c, A, ll, u, b, T, rho = [np.array(v) if isinstance(v, list) else v for v in case]\n        \n        # --- Part 1: Compute p_star (exact primal optimum) via KKT and root-finding ---\n        \n        def get_x_star_from_nu(nu, q, c, A, ll, u):\n            \"\"\"Calculates x*(nu) = proj_[l,u] ( (-c - nu*A) / q )\"\"\"\n            unconstrained_x = (-c - nu * A) / q\n            return np.maximum(ll, np.minimum(u, unconstrained_x))\n\n        def phi(nu, q, c, A, ll, u, b):\n            \"\"\"The function sum(A_i * x_i*(nu)) - b, whose root is nu_star\"\"\"\n            x_star = get_x_star_from_nu(nu, q, c, A, ll, u)\n            return np.dot(A, x_star) - b\n\n        # Find nu_star by solving phi(nu)=0 using a robust bracketing strategy\n        nu_search_min, nu_search_max = -1e5, 1e5\n        phi_at_min = phi(nu_search_min, q, c, A, ll, u, b)\n        phi_at_max = phi(nu_search_max, q, c, A, ll, u, b)\n\n        # Since feasibility is guaranteed, phi_at_max = 0 = phi_at_min\n        if np.isclose(phi_at_min, 0.0):\n            nu_star = nu_search_min\n        elif np.isclose(phi_at_max, 0.0):\n            nu_star = nu_search_max\n        else:\n            # A root must exist in the interval\n            nu_star = brentq(phi, nu_search_min, nu_search_max, args=(q, c, A, ll, u, b))\n\n        x_star = get_x_star_from_nu(nu_star, q, c, A, ll, u)\n        p_star = np.sum(0.5 * q * x_star**2 + c * x_star)\n\n        # --- Part 2: Dual Subgradient Ascent ---\n        \n        lambda_k = 0.0\n        x_sum = np.zeros(m)\n        d_best = -np.inf\n\n        for _ in range(T):\n            # Compute x_k which minimizes L(x, lambda_k) over the box\n            unconstrained_x = (-c - lambda_k * A) / q\n            x_k = np.maximum(ll, np.minimum(u, unconstrained_x))\n            x_sum += x_k\n\n            # Compute subgradient r_k\n            r_k = np.dot(A, x_k) - b\n            \n            # Compute dual function value g(lambda_k) = inf_x L(x, lambda_k)\n            g_lambda_k = np.sum(0.5 * q * x_k**2 + c * x_k) + lambda_k * r_k\n            if g_lambda_k > d_best:\n                d_best = g_lambda_k\n            \n            # Update lambda\n            lambda_k += (1.0 / rho) * r_k\n        \n        # --- Part 3: Final Computations ---\n        x_bar = x_sum / T\n        \n        duality_gap = p_star - d_best\n        feasibility_residual = np.abs(np.dot(A, x_bar) - b)\n        \n        all_results.extend([duality_gap, feasibility_residual])\n        \n    # Format and print the final output as a single-line string\n    print(f\"[{','.join(f'{r:.6f}' for r in all_results)}]\")\n\n# Execute the self-contained solution process\nsolve()\n```"
        },
        {
            "introduction": "与基于价格的方法不同，Benders 分解法根据变量来划分问题。对于含有“复杂变量”的问题，这种强大的技术非常理想——固定这些变量的值后，剩余的问题就简化为一个易于求解的线性规划。在这个练习中 ，你将推导 Benders 分解的理论机制，从子问题的对偶中导出最优性割和可行性割，从而迭代地优化解。",
            "id": "3116814",
            "problem": "考虑以下适用于 Benders 分解的两阶段线性优化问题。主决策变量为 $x \\in [0,3]$。对于任意固定的 $x$，子问题是以下线性规划 (LP) 问题\n$$\n\\begin{aligned}\n\\text{minimize}_{y_1,y_2} \\quad  y_1 + y_2 \\\\\n\\text{subject to} \\quad  y_1 + y_2 = d(x), \\\\\n y_1 \\leq 2, \\\\\n y_2 \\leq 2, \\\\\n y_1 \\geq 0,\\; y_2 \\geq 0,\n\\end{aligned}\n$$\n其中 $d(x) = 3 - 2x$。主目标是最小化 $5x + \\theta$，其中 $\\theta$ 是子问题成本的代理变量，并将通过 Benders 切割进行约束。当子问题可行时，它具有退化的多重最优解，因为任何满足 $y_1 + y_2 = d(x)$ 的可行对 $(y_1,y_2)$ 都达到相同的目标值 $y_1 + y_2$。\n\n任务：\n1. 使用线性规划的基本原始-对偶配对和弱/强对偶性原理，从第一性原理推导出子问题的对偶问题。\n2. 识别一个在子问题可行时能产生有限最优对偶解的极点，并推导出相关的关于 $\\theta$ 的 Benders 最优性切割（作为 $x$ 的函数）。\n3. 识别对偶可行集的极射线，这些极射线导致对某些 $x$ 值对应的子问题不可行性而产生的对偶无界性，并推导出相关的 Benders 可行性切割（用 $x$ 显式表示）。\n4. 通过 LP 后处理执行切割冗余检测：在你推导的可行性切割中，通过构建并求解一个在一维主定义域上最大化候选切割违反量的一维 LP，识别出在 $x \\in [0,3]$ 时是冗余的任何切割。\n5. 仅使用非冗余切割和最优性切割，写出最终的主问题并求解，以获得 $5x + \\theta$ 的最优目标值。\n\n你的最终答案必须是等于 $5x + \\theta$ 最优值的单个实数。无需四舍五入，不涉及单位。在你的答案中只表达最终的数字。",
            "solution": "用户要求使用 Benders 分解法解决一个两阶段线性优化问题。将通过完成五个指定的任务来继续。\n\n### 问题验证\n问题陈述描述了一个标准的、包含一个第一阶段变量 $x$ 和一个涉及变量 $y_1, y_2$ 的第二阶段（子问题）的两阶段线性规划。该结构适用于 Benders 分解。所有变量、函数和约束都已明确定义。该问题在数学上是适定的，在优化理论方面具有科学依据，并且陈述客观。未检测到缺陷。继续进行求解。\n\n### 任务 1：推导子问题的对偶问题\n\n对于一个固定的 $x$，子问题由下式给出：\n$$\n\\begin{aligned}\n\\text{minimize}_{y_1,y_2} \\quad  y_1 + y_2 \\\\\n\\text{subject to} \\quad  y_1 + y_2 = d(x) \\\\\n y_1 \\leq 2 \\\\\n y_2 \\leq 2 \\\\\n y_1 \\geq 0,\\; y_2 \\geq 0\n\\end{aligned}\n$$\n其中 $d(x) = 3 - 2x$。为了推导对偶问题，我们首先将这个 LP 表示成标准形式。我们为 $y_1, y_2$ 的上界引入非负松弛变量 $s_1, s_2$。子问题变为：\n$$\n\\begin{aligned}\n\\text{minimize} \\quad  y_1 + y_2 + 0s_1 + 0s_2 \\\\\n\\text{subject to} \\quad  y_1 + y_2 \\quad\\quad\\quad = 3 - 2x \\\\\n y_1 \\quad\\quad + s_1 \\quad = 2 \\\\\n \\quad\\quad y_2 \\quad\\quad + s_2 = 2 \\\\\n y_1, y_2, s_1, s_2 \\geq 0\n\\end{aligned}\n$$\n这是一个标准形式的 LP：$\\min \\{c^T \\mathbf{y} \\mid A\\mathbf{y} = b, \\mathbf{y} \\geq 0\\}$，其中 $\\mathbf{y} = (y_1, y_2, s_1, s_2)^T$。相应的对偶问题是 $\\max \\{\\pi^T b \\mid A^T\\pi \\leq c\\}$，其中 $\\pi = (\\pi_0, \\pi_1, \\pi_2)^T$ 是与三个等式约束相关联的对偶变量向量，这些对偶变量的符号不受限制。\n\n矩阵和向量是：\n$$ c = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad A = \\begin{pmatrix} 1  1  0  0 \\\\ 1  0  1  0 \\\\ 0  1  0  1 \\end{pmatrix}, \\quad b(x) = \\begin{pmatrix} 3-2x \\\\ 2 \\\\ 2 \\end{pmatrix} $$\n对偶约束 $A^T\\pi \\leq c$ 是：\n$$\n\\begin{pmatrix}\n1  1  0 \\\\\n1  0  1 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n\\begin{pmatrix} \\pi_0 \\\\ \\pi_1 \\\\ \\pi_2 \\end{pmatrix}\n\\leq\n\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n这给出以下不等式：\n1. $\\pi_0 + \\pi_1 \\leq 1$\n2. $\\pi_0 + \\pi_2 \\leq 1$\n3. $\\pi_1 \\leq 0$\n4. $\\pi_2 \\leq 0$\n\n要最大化的对偶目标函数是 $\\pi^T b(x)$：\n$$ (3-2x)\\pi_0 + 2\\pi_1 + 2\\pi_2 $$\n因此，子问题的对偶问题是：\n$$\n\\begin{aligned}\n\\text{maximize}_{\\pi_0, \\pi_1, \\pi_2} \\quad  (3-2x)\\pi_0 + 2\\pi_1 + 2\\pi_2 \\\\\n\\text{subject to} \\quad  \\pi_0 + \\pi_1 \\leq 1 \\\\\n \\pi_0 + \\pi_2 \\leq 1 \\\\\n \\pi_1 \\leq 0 \\\\\n \\pi_2 \\leq 0\n\\end{aligned}\n$$\n\n### 任务 2：推导 Benders 最优性切割\n\n最优性切割是在子问题可行且具有有限最优值的 $x$ 值处，从对偶可行集的一个极点生成的。原始子问题是可行的，如果存在 $y_1, y_2$ 使得 $y_1+y_2 = d(x)$ 且 $0 \\leq y_1, y_2 \\leq 2$。和 $y_1+y_2$ 可以在 $[0,4]$ 内取任何值。因此，子问题可行的充要条件是 $0 \\leq d(x) \\leq 4$。\n$$ 0 \\leq 3-2x \\implies 2x \\leq 3 \\implies x \\leq 1.5 $$\n$$ 3-2x \\leq 4 \\implies -1 \\leq 2x \\implies x \\geq -0.5 $$\n给定主变量定义域 $x \\in [0,3]$，子问题对于 $x \\in [0, 1.5]$ 是可行的。\n\n对于此范围内的 $x$，对偶问题必须有有限最优解。我们来找一个对偶可行域的极点。极点是由至少三个紧约束平面相交形成的顶点。我们来测试这些约束：$\\pi_0 + \\pi_1 = 1$, $\\pi_1 = 0$, $\\pi_2 = 0$。这得出 $\\pi_0=1$。该点为 $(\\pi_0, \\pi_1, \\pi_2) = (1,0,0)$。我们检查它是否可行：\n- $1 + 0 \\leq 1$ (满足)\n- $1 + 0 \\leq 1$ (满足)\n- $0 \\leq 0$ (满足)\n- $0 \\leq 0$ (满足)\n点 $(1,0,0)$ 是对偶可行集的一个极点。\n在该点处，对偶目标函数为 $(3-2x)(1)+2(0)+2(0) = 3-2x$。\n当子问题可行时，其最优目标值由 $Q(x) = d(x) = 3-2x$ 给出。根据强对偶性，这也是对偶问题的最优值。最优性切割是 $\\theta \\geq Q(x)$，其中 $Q(x)$ 由其某个极点处的对偶目标值来近似。使用极点 $\\pi^*=(1,0,0)$，我们得到 Benders 最优性切割：\n$$ \\theta \\geq (3-2x)\\pi_0^* + 2\\pi_1^* + 2\\pi_2^* $$\n$$ \\theta \\geq 3-2x $$\n\n### 任务 3：推导 Benders 可行性切割\n\n可行性切割是为子问题不可行的 $x$ 值生成的。这种情况发生在对偶问题无界时。无界方向是对偶问题衰退锥的极射线。\n衰退锥由以下齐次不等式组定义：\n$$\n\\begin{aligned}\nd_0 + d_1 \\leq 0 \\\\\nd_0 + d_2 \\leq 0 \\\\\nd_1 \\leq 0 \\\\\nd_2 \\leq 0\n\\end{aligned}\n$$\n如果这个锥的一条极射线 $r=(\\pi_0, \\pi_1, \\pi_2)$ 为对偶目标提供了一个无界方向，即 $r^T c_D > 0$（其中 $c_D = (3-2x, 2, 2)^T$），那么就可以从中导出一个 Benders 可行性切割。该切割的形式为 $r^T b(x) \\leq 0$。\n对于 $x \\in [0,3]$，当 $x>1.5$ (因为 $d(x)  0$) 或 $x-0.5$ (因为 $d(x)>4$) 时，子问题不可行。\n\n情况 1: $x > 1.5$，这意味着 $3-2x  0$。\n我们来找一条导致目标函数无界的极射线。考虑射线 $r_1 = (-1, 0, 0)$。它在衰退锥中：$-1+0\\leq0$, $-1+0\\leq0$, $0\\leq0$, $0\\leq0$。\n沿此射线的对偶目标函数变化为 $(3-2x)(-1) + 2(0) + 2(0) = 2x-3$。对于 $x>1.5$，$2x-3 > 0$，因此对偶问题沿此射线无界。\n相应的可行性切割为 $r_1^T b(x) \\leq 0$：\n$$ (-1,0,0) \\cdot (3-2x, 2, 2)^T \\leq 0 \\implies -(3-2x) \\leq 0 \\implies 2x-3 \\leq 0 \\implies x \\leq 1.5 $$\n此切割从主问题的可行集中移除了区域 $x > 1.5$。\n\n情况 2: $x  -0.5$，这意味着 $3-2x > 4$。\n我们来找另一条极射线。考虑 $r_2 = (1, -1, -1)$。它在衰退锥中：$1-1=0\\leq0$, $1-1=0\\leq0$, $-1\\leq0$, $-1\\leq0$。\n沿此射线的对偶目标函数变化为 $(3-2x)(1) + 2(-1) + 2(-1) = 3-2x-4 = -2x-1$。\n对于 $x  -0.5$，$-2x > 1$，因此 $-2x-1 > 0$。对偶问题沿此射线无界。\n相应的可行性切割为 $r_2^T b(x) \\leq 0$：\n$$ (1,-1,-1) \\cdot (3-2x, 2, 2)^T \\leq 0 \\implies (3-2x) - 2 - 2 \\leq 0 \\implies -2x-1 \\leq 0 \\implies x \\geq -0.5 $$\n此切割移除了区域 $x  -0.5$。\n\n两个可行性切割是 $x \\leq 1.5$ 和 $x \\geq -0.5$。\n\n### 任务 4：执行切割冗余检测\n\n$x$ 的主问题受到其原始定义域 $[0,3]$ 和可行性切割的约束。最终的 $x$ 可行域由下式给出：\n$$ \\{ x \\mid 0 \\leq x \\leq 3, \\quad x \\leq 1.5, \\quad x \\geq -0.5 \\} $$\n合并这些不等式，我们得到 $0 \\leq x \\leq 1.5$。\n我们在主定义域 $x \\in [0,3]$ 上检查可行性切割 $x \\leq 1.5$ 和 $x \\geq -0.5$ 之间的冗余性。\n\n如果一个切割被其他切割和定义域约束所蕴含，那么它就是冗余的。\n约束 $x \\geq -0.5$ 是一个冗余候选。我们检查在给定主定义域约束 $x \\in [0,3]$ 的情况下它是否是冗余的。定义域是 $[0,3]$。此域中的每个 $x$ 都已经满足 $x \\geq -0.5$。因此，切割 $x \\geq -0.5$ 是冗余的。\n\n为了使用指定的 LP 方法将其形式化：让我们检查切割 $x \\geq -0.5$ 是否冗余。问题是在由其他约束（即 $x \\in [0,3]$ 和 $x \\leq 1.5$）定义的域上最大化其违反量。$x \\geq -0.5$ 的违反量可以表示为 $-0.5 - x$。我们试图最大化这个值。\n$$ \\max_{x} \\quad -0.5 - x \\quad \\text{s.t.} \\quad 0 \\leq x \\leq 1.5 $$\n最大值出现在 $x$ 的最小值处，即 $x=0$。最大违反量为 $-0.5-0 = -0.5$。由于最大违反量不为正，切割 $x \\geq -0.5$ 在由其他约束定义的可行域上永远不会被违反。因此它是冗余的。\n\n非冗余的可行性切割是 $x \\leq 1.5$。\n\n### 任务 5：求解主问题\n\n最终的主问题包含了非冗余的可行性切割和最优性切割：\n$$\n\\begin{aligned}\n\\text{minimize}_{x, \\theta} \\quad  5x + \\theta \\\\\n\\text{subject to} \\quad  0 \\leq x \\leq 1.5 \\\\\n \\theta \\geq 3-2x\n\\end{aligned}\n$$\n目标是最小化 $5x+\\theta$。为了实现这一点，对于任何给定的 $x$，我们应该选择 $\\theta$ 可能的最小值，根据约束即为 $\\theta = 3-2x$。将此代入目标函数，我们得到一个仅关于 $x$ 的问题：\n$$ \\text{minimize}_{x} \\quad 5x + (3-2x) \\quad \\text{s.t.} \\quad 0 \\leq x \\leq 1.5 $$\n目标函数简化为：\n$$ \\text{minimize}_{x} \\quad 3x+3 \\quad \\text{s.t.} \\quad 0 \\leq x \\leq 1.5 $$\n函数 $f(x)=3x+3$ 是一个斜率为正 ($3$) 的线性函数。因此，它在区间 $[0, 1.5]$ 上的最小值在左端点 $x=0$ 处取得。\n$x$ 的最优值为 $x^*=0$。\n对应的最优目标函数值为 $3(0)+3=3$。\n\n整个问题的最优解是 $x^*=0$，子问题的成本为 $Q(0)=3-2(0)=3$。\n主目标函数的最优值为 $5x^* + Q(x^*) = 5(0) + 3 = 3$。",
            "answer": "$$\n\\boxed{3}\n$$"
        },
        {
            "introduction": "交替方向乘子法（ADMM）是现代大规模优化领域的一个强大工具，它既有对偶方法的易分解性，又具备增广拉格朗日方法的优良收敛性。它在分布式共识问题中尤其有效，能够让各个子系统通过协作找到全局最优解。本实践  提供了一个动手实现 ADMM 的机会，将其应用于一个实际工程问题——电网中的最优潮流问题，并探索其在分布式环境下的性能。",
            "id": "3116714",
            "problem": "考虑一个小型电网上的直流最优潮流 (DC-OPF) 模型，该电网被划分为两个区域，使用乘子交替方向法 (ADMM) 的分布式一致性公式求解。DC-OPF 使用线性化的潮流关系 $$P = B \\theta$$，其中 $P$ 是节点净有功注入向量，$B$ 是由线路电抗构成的节点电纳拉普拉斯矩阵，$\\theta$ 是节点电压相角向量（单位为弧度）。连接节点 $i$ 和 $j$ 的线路上的潮流由 $$f_{ij} = \\frac{\\theta_i - \\theta_j}{x_{ij}}$$ 给出，其中 $x_{ij}$ 是线路电抗。节点 $i$ 的净注入满足 $$P_i = \\sum_{j \\in \\mathcal{N}(i)} \\frac{\\theta_i - \\theta_j}{x_{ij}},$$，其中 $\\mathcal{N}(i)$ 是由输电线路连接的相邻节点。\n\n区域 $1$ 包含节点 $\\{1,2,3\\}$，内部线路 $(1,2)$ 的电抗为 $x_{12} = 0.1$，$(2,3)$ 的电抗为 $x_{23} = 0.2$。区域 $2$ 包含节点 $\\{4,5\\}$，内部线路 $(4,5)$ 的电抗为 $x_{45} = 0.15$。连接这两个区域的联络线是 $(3,4)$，其电抗为 $x_{34} = 0.25$。定义电纳 $$b_{12} = \\frac{1}{x_{12}}, \\quad b_{23} = \\frac{1}{x_{23}}, \\quad b_{45} = \\frac{1}{x_{45}}, \\quad b_{34} = \\frac{1}{x_{34}}.$$ 负荷（单位为标幺值）为 $$d_1 = 0.0, \\quad d_2 = 0.2, \\quad d_3 = 1.0, \\quad d_4 = 0.8, \\quad d_5 = 0.3.$$ 发电机位于节点 $1$、$2$ 和 $5$，其二次成本为 $$C_1(g_1) = c_{2,1} g_1^2 + c_{1,1} g_1, \\quad C_2(g_2) = c_{2,2} g_2^2 + c_{1,2} g_2, \\quad C_5(g_5) = c_{2,5} g_5^2 + c_{1,5} g_5,$$ 其中 $$c_{2,1} = 0.02, \\quad c_{1,1} = 1.0, \\quad c_{2,2} = 0.04, \\quad c_{1,2} = 1.2, \\quad c_{2,5} = 0.03, \\quad c_{1,5} = 1.1.$$ 相角变量单位为弧度，功率量单位为标幺值。\n\n区域 $1$ 使用平衡节点相角约束 $\\theta_1 = 0$ 来设置参考。区域 $2$ 不固定平衡节点，联络线一致性将锚定跨区域的相角。局部 DC-OPF 约束是节点功率平衡方程。对于区域 $1$：\n- 在节点 $1$：$$b_{12}(\\theta_1 - \\theta_2) = g_1 - d_1.$$\n- 在节点 $2$：$$b_{12}(\\theta_2 - \\theta_1) + b_{23}(\\theta_2 - \\theta_3) = g_2 - d_2.$$\n- 在节点 $3$：$$b_{23}(\\theta_3 - \\theta_2) + b_{34}(\\theta_3 - \\phi_4) = 0 - d_3,$$ 其中 $\\phi_4$ 是区域 $1$ 对节点 $4$ 相角的本地副本，用于计算联络线潮流。\n\n对于区域 $2$：\n- 在节点 $4$：$$b_{45}(\\theta_4 - \\theta_5) + b_{34}(\\theta_4 - \\phi_3) = 0 - d_4,$$ 其中 $\\phi_3$ 是区域 $2$ 对节点 $3$ 相角的本地副本。\n- 在节点 $5$：$$b_{45}(\\theta_5 - \\theta_4) = g_5 - d_5.$$\n\n为强制跨区域边界变量的一致性，引入分别用于节点 $3$ 和 $4$ 的一致性变量 $z_3$ 和 $z_4$。ADMM 惩罚参数表示为 $\\rho > 0$。在步骤 $k$ 的缩放 ADMM 迭代包括：\n- 区域 $1$ 对其变量 $\\theta_1, \\theta_2, \\theta_3, g_1, g_2, \\phi_4$ 进行局部最小化，目标是局部成本函数与二次惩罚项之和 $$\\frac{\\rho}{2}\\left(\\theta_3 - z_3^{(k)} + u_{3,\\mathrm{A}}^{(k)}\\right)^2 + \\frac{\\rho}{2}\\left(\\phi_4 - z_4^{(k)} + u_{4,\\mathrm{A}}^{(k)}\\right)^2$$，受区域 $1$ 的节点约束和平衡约束 $\\theta_1 = 0$ 的限制。\n- 区域 $2$ 对其变量 $\\theta_4, \\theta_5, g_5, \\phi_3$ 进行局部最小化，目标是局部成本函数与二次惩罚项之和 $$\\frac{\\rho}{2}\\left(\\theta_4 - z_4^{(k)} + u_{4,\\mathrm{B}}^{(k)}\\right)^2 + \\frac{\\rho}{2}\\left(\\phi_3 - z_3^{(k)} + u_{3,\\mathrm{B}}^{(k)}\\right)^2$$，受区域 $2$ 的节点约束的限制。\n- 一致性更新 $$z_3^{(k+1)} = \\frac{\\theta_3^{(k+1)} + u_{3,\\mathrm{A}}^{(k)} + \\phi_3^{(k+1)} + u_{3,\\mathrm{B}}^{(k)}}{2}, \\quad z_4^{(k+1)} = \\frac{\\phi_4^{(k+1)} + u_{4,\\mathrm{A}}^{(k)} + \\theta_4^{(k+1)} + u_{4,\\mathrm{B}}^{(k)}}{2}.$$\n- 对偶变量更新 $$u_{3,\\mathrm{A}}^{(k+1)} = u_{3,\\mathrm{A}}^{(k)} + \\theta_3^{(k+1)} - z_3^{(k+1)}, \\quad u_{3,\\mathrm{B}}^{(k+1)} = u_{3,\\mathrm{B}}^{(k)} + \\phi_3^{(k+1)} - z_3^{(k+1)},$$ $$u_{4,\\mathrm{A}}^{(k+1)} = u_{4,\\mathrm{A}}^{(k)} + \\phi_4^{(k+1)} - z_4^{(k+1)}, \\quad u_{4,\\mathrm{B}}^{(k+1)} = u_{4,\\mathrm{B}}^{(k)} + \\theta_4^{(k+1)} - z_4^{(k+1)}.$$\n\n将迭代 $k$ 的边界原始残差范数定义为 $$r^{(k)} = \\sqrt{\\left(\\theta_3^{(k)} - \\phi_3^{(k)}\\right)^2 + \\left(\\phi_4^{(k)} - \\theta_4^{(k)}\\right)^2}.$$ 我们将通过满足 $$r^{(k)} \\le \\varepsilon$$ 所需的迭代次数来衡量收敛速度，容差为 $\\varepsilon = 10^{-6}$，最大迭代次数为 $500$。相角单位必须为弧度，所有功率量必须为标幺值。\n\n任务：\n1. 实现上述带有 ADMM 的分布式 DC-OPF，其中每个局部区域求解是一个带线性等式约束的二次规划，通过卡罗需-库恩-塔克 (KKT) 条件求解。\n2. 通过对惩罚值 $\\rho \\in \\{0.01, 0.1, 1.0, 10.0, 100.0\\}$ 运行 ADMM，测试惩罚参数 $\\rho$ 的选择如何影响收敛速度和边界不匹配。\n3. 对于每个 $\\rho$，报告两个量：达到容差所需的整数迭代次数（如果未达到容差，则为最大迭代次数），以及最终的边界残差范数 $r$（浮点数）。\n\n您的程序必须产生单行输出，包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素是针对特定 $\\rho$ 的一个双项列表 $[n, r]$，即 $$[[n_1, r_1],[n_2, r_2],[n_3, r_3],[n_4, r_4],[n_5, r_5]].$$\n\n相角单位：弧度。功率单位：标幺值。所有数值输出均表示为不带单位、不带百分号的纯浮点数和整数。\n\n测试套件：\n- 惩罚值：$\\rho = 0.01$, $\\rho = 0.1$, $\\rho = 1.0$, $\\rho = 10.0$, $\\rho = 100.0$。\n- 最大迭代次数：$500$。\n- 容差：$\\varepsilon = 10^{-6}$。",
            "solution": "该问题要求实现一种分布式优化算法，特别是乘子交替方向法 (ADMM)，以解决一个小型电力网络的直流最优潮流 (DC-OPF) 问题。该网络被划分为两个区域，必须通过协调局部子问题来找到解决方案。\n\n首先，我们验证问题陈述。\n\n### 步骤 1：提取给定条件\n-   **模型**：使用线性化潮流关系 $P = B \\theta$ 的直流最优潮流 (DC-OPF)。\n-   **网络划分**：区域 $1$ 包含节点 $\\{1,2,3\\}$，区域 $2$ 包含节点 $\\{4,5\\}$。\n-   **线路电抗（标幺值）**：$x_{12} = 0.1$, $x_{23} = 0.2$, $x_{45} = 0.15$。联络线电抗为 $x_{34} = 0.25$。线路电纳定义为 $b_{ij} = 1/x_{ij}$。\n-   **负荷（标幺值）**：$d_1 = 0.0$, $d_2 = 0.2$, $d_3 = 1.0$, $d_4 = 0.8$, $d_5 = 0.3$。\n-   **发电成本**：位于节点 $1$、$2$ 和 $5$ 的发电机具有二次成本 $C_i(g_i) = c_{2,i} g_i^2 + c_{1,i} g_i$。系数给定为 $c_{2,1} = 0.02, c_{1,1} = 1.0$；$c_{2,2} = 0.04, c_{1,2} = 1.2$；$c_{2,5} = 0.03, c_{1,5} = 1.1$。\n-   **参考相角**：平衡节点约束 $\\theta_1 = 0$。\n-   **节点功率平衡方程**：问题明确给出了每个节点的直流潮流方程，这些方程作为优化子问题的线性等式约束。这些方程涉及局部变量 $(\\theta_i, g_i)$ 和边界变量的副本 $(\\phi_i)$。\n-   **ADMM 结构**：指定了一个一致性 ADMM 公式。\n    -   **一致性变量**：用于边界节点相角的 $z_3$ 和 $z_4$。\n    -   **ADMM 惩罚参数**：$\\rho > 0$。\n    -   **局部子问题**：每个区域解决一个局部优化问题，该问题最小化其自身的发电成本加上增广拉格朗日惩罚项。对于区域 $1$，目标是 $C_1(g_1) + C_2(g_2) + \\frac{\\rho}{2}(\\theta_3 - z_3^{(k)} + u_{3,\\mathrm{A}}^{(k)})^2 + \\frac{\\rho}{2}(\\phi_4 - z_4^{(k)} + u_{4,\\mathrm{A}}^{(k)})^2$。为区域 $2$ 定义了类似的目标。\n    -   **更新规则**：提供了用于更新一致性变量 $z_3, z_4$ 和缩放对偶变量 $u$ 的显式公式。例如，$z_3^{(k+1)} = \\frac{1}{2}(\\theta_3^{(k+1)} + u_{3,\\mathrm{A}}^{(k)} + \\phi_3^{(k+1)} + u_{3,\\mathrm{B}}^{(k)})$。\n-   **任务参数**：\n    -   **子问题求解器**：卡罗需-库恩-塔克 (KKT) 条件。\n    -   **惩罚值**：$\\rho \\in \\{0.01, 0.1, 1.0, 10.0, 100.0\\}$。\n    -   **收敛容差**：边界原始残差范数 $r^{(k)} = \\sqrt{(\\theta_3^{(k)} - \\phi_3^{(k)})^2 + (\\phi_4^{(k)} - \\theta_4^{(k)})^2}$ 的容差为 $\\varepsilon = 10^{-6}$。\n    -   **最大迭代次数**：$500$。\n-   **输出格式**：一个由数对 $[n, r]$ 组成的列表，其中 $n$ 是迭代次数，$r$ 是最终的残差范数，对应于每个 $\\rho$ 值。\n\n### 步骤 2：使用提取的给定条件进行验证\n该问题在科学上和数学上都是合理的。\n-   **科学依据**：DC-OPF 模型是电力系统工程中用于分析有功潮流和经济调度的一个标准且被广泛接受的简化方法。ADMM 是一种功能强大且理论基础扎实的分布式凸优化方法，常用于解决电力系统问题。\n-   **良定性**：由于目标函数是凸二次成本之和，且约束是线性的，因此整个问题是一个凸二次规划 (QP)。这确保了唯一全局最优解的存在。局部子问题也是凸 QP。对于任何 $\\rho > 0$，ADMM 保证能对此类问题收敛到最优解。\n-   **完整性和一致性**：问题陈述是自洽的，提供了所有必要的数值数据、模型方程和算法步骤。所提供的信息中没有矛盾。定义的变量和方程与基于 ADMM 的 DC-OPF 问题分解的标准方法一致。\n\n### 步骤 3：结论与行动\n问题被认为是有效的。将开发并实现一个分步解决方案。\n\n### 求解推导\n\n任务的核心是实现指定的 ADMM 算法。ADMM 算法迭代地解决每个区域的局部子问题，然后更新协调解决方案的一致性变量和对偶变量。\n\n**子问题公式化**\n\n区域 $1$ 和区域 $2$ 的子问题是带有线性等式约束的二次规划问题。按照规定，我们通过构建它们的卡罗需-库恩-塔克 (KKT) 条件来求解，这些条件构成一个线性方程组。\n\n**区域 1 子问题**\n\n区域 $1$ 的优化变量是 $x_{\\mathrm{A}} = [\\theta_2, \\theta_3, g_1, g_2, \\phi_4]^T$。平衡约束 $\\theta_1=0$ 是一个固定值。\n在迭代 $k+1$ 时要最小化的目标函数是：\n$$ L_{\\mathrm{A}}(x_{\\mathrm{A}}) = C_1(g_1) + C_2(g_2) + \\frac{\\rho}{2}(\\theta_3 - z_3^{(k)} + u_{3,\\mathrm{A}}^{(k)})^2 + \\frac{\\rho}{2}(\\phi_4 - z_4^{(k)} + u_{4,\\mathrm{A}}^{(k)})^2 $$\n受以下线性约束：\n\\begin{align*}\nb_{12}\\theta_2 + g_1 = d_1 \\\\\n(b_{12}+b_{23})\\theta_2 - b_{23}\\theta_3 - g_2 = -d_2 \\\\\n-b_{23}\\theta_2 + (b_{23}+b_{34})\\theta_3 - b_{34}\\phi_4 = -d_3\n\\end{align*}\n为这些约束引入拉格朗日乘子 $\\lambda_{\\mathrm{A}} = [\\lambda_{1,1}, \\lambda_{1,2}, \\lambda_{1,3}]^T$，KKT 条件（平稳性和可行性）产生一个形如 $K_1 y_1 = v_1$ 的线性系统，其中 $y_1 = [x_{\\mathrm{A}}; \\lambda_{\\mathrm{A}}]$。KKT 矩阵 $K_1$ 和向量 $v_1$ 是：\n$$\nK_1 = \\begin{pmatrix} Q_1  A_1^T \\\\ A_1  0 \\end{pmatrix}, \\quad\nv_1 = \\begin{pmatrix} -c_1 \\\\ b_1 \\end{pmatrix}\n$$\n其中 $Q_1$ 是目标的 Hessian 矩阵，$A_1$ 是约束矩阵，$c_1$ 包含来自目标函数的线性项，$b_1$ 是约束的右侧项。具体来说：\n- $Q_1 = \\mathrm{diag}(0, \\rho, 2c_{2,1}, 2c_{2,2}, \\rho)$\n- $c_1 = [0, -\\rho(z_3^{(k)} - u_{3,\\mathrm{A}}^{(k)}), c_{1,1}, c_{1,2}, -\\rho(z_4^{(k)} - u_{4,\\mathrm{A}}^{(k)})]^T$\n- $b_1 = [d_1, -d_2, -d_3]^T$\n- $A_1 = \\begin{pmatrix} b_{12}  0  1  0  0 \\\\ b_{12}+b_{23}  -b_{23}  0  -1  0 \\\\ -b_{23}  b_{23}+b_{34}  0  0  -b_{34} \\end{pmatrix}$\n\n**区域 2 子问题**\n\n区域 $2$ 的优化变量是 $x_{\\mathrm{B}} = [\\theta_4, \\theta_5, g_5, \\phi_3]^T$。\n目标函数是：\n$$ L_{\\mathrm{B}}(x_{\\mathrm{B}}) = C_5(g_5) + \\frac{\\rho}{2}(\\theta_4 - z_4^{(k)} + u_{4,\\mathrm{B}}^{(k)})^2 + \\frac{\\rho}{2}(\\phi_3 - z_3^{(k)} + u_{3,\\mathrm{B}}^{(k)})^2 $$\n受以下线性约束：\n\\begin{align*}\n(b_{45}+b_{34})\\theta_4 - b_{45}\\theta_5 - b_{34}\\phi_3 = -d_4 \\\\\n-b_{45}\\theta_4 + b_{45}\\theta_5 - g_5 = -d_5\n\\end{align*}\n类似地，这形成一个 KKT 系统 $K_2 y_2 = v_2$，其中 $y_2 = [x_{\\mathrm{B}}; \\lambda_{\\mathrm{B}}]$，且 $\\lambda_{\\mathrm{B}} = [\\lambda_{2,1}, \\lambda_{2,2}]^T$。其组成部分是：\n- $Q_2 = \\mathrm{diag}(\\rho, 0, 2c_{2,5}, \\rho)$\n- $c_2 = [-\\rho(z_4^{(k)} - u_{4,\\mathrm{B}}^{(k)}), 0, c_{1,5}, -\\rho(z_3^{(k)} - u_{3,\\mathrm{B}}^{(k)})]^T$\n- $b_2 = [-d_4, -d_5]^T$\n- $A_2 = \\begin{pmatrix} b_{45}+b_{34}  -b_{45}  0  -b_{34} \\\\ -b_{45}  b_{45}  -1  0 \\end{pmatrix}$\n\nKKT 矩阵 $K_1$ 和 $K_2$ 依赖于 $\\rho$，但对于给定的 $\\rho$，在整个 ADMM 迭代过程中是恒定的。\n\n**ADMM 算法**\n\n对于每个给定的 $\\rho$ 值，算法按以下步骤进行：\n1.  **初始化**：将所有变量设为零：局部变量 $(\\theta, g, \\phi)$、一致性变量 $(z_3, z_4)$ 和缩放对偶变量 $(u)$。\n2.  **迭代**：对于 $k = 0, 1, 2, \\dots$ 直到最大迭代次数：\n    a. **局部求解**：\n       i.  使用 $z^{(k)}$ 和 $u^{(k)}$ 构造向量 $v_1$。求解线性系统 $K_1 y_1^{(k+1)} = v_1$ 以获得更新后的区域 $1$ 变量，包括 $\\theta_3^{(k+1)}$ 和 $\\phi_4^{(k+1)}$。\n       ii. 使用 $z^{(k)}$ 和 $u^{(k)}$ 构造向量 $v_2$。求解线性系统 $K_2 y_2^{(k+1)} = v_2$ 以获得更新后的区域 $2$ 变量，包括 $\\theta_4^{(k+1)}$ 和 $\\phi_3^{(k+1)}$。\n    b. **收敛性检查**：计算边界原始残差范数 $r^{(k+1)} = \\sqrt{(\\theta_3^{(k+1)} - \\phi_3^{(k+1)})^2 + (\\phi_4^{(k+1)} - \\theta_4^{(k+1)})^2}$。如果 $r^{(k+1)} \\le \\varepsilon$，则终止并报告当前迭代次数和残差。\n    c. **一致性更新**：使用新的局部变量和旧的对偶变量更新一致性变量：\n       $$z_3^{(k+1)} = \\frac{1}{2}(\\theta_3^{(k+1)} + u_{3,\\mathrm{A}}^{(k)} + \\phi_3^{(k+1)} + u_{3,\\mathrm{B}}^{(k)})$$\n       $$z_4^{(k+1)} = \\frac{1}{2}(\\phi_4^{(k+1)} + u_{4,\\mathrm{A}}^{(k)} + \\theta_4^{(k+1)} + u_{4,\\mathrm{B}}^{(k)})$$\n    d. **对偶更新**：更新缩放的对偶变量：\n       $$u_{3,\\mathrm{A}}^{(k+1)} = u_{3,\\mathrm{A}}^{(k)} + \\theta_3^{(k+1)} - z_3^{(k+1)}$$\n       $$u_{3,\\mathrm{B}}^{(k+1)} = u_{3,\\mathrm{B}}^{(k)} + \\phi_3^{(k+1)} - z_3^{(k+1)}$$\n       $$u_{4,\\mathrm{A}}^{(k+1)} = u_{4,\\mathrm{A}}^{(k)} + \\phi_4^{(k+1)} - z_4^{(k+1)}$$\n       $$u_{4,\\mathrm{B}}^{(k+1)} = u_{4,\\mathrm{B}}^{(k)} + \\theta_4^{(k+1)} - z_4^{(k+1)}$$\n3.  **终止**：如果循环在未满足容差的情况下完成，则报告最大迭代次数和最终计算的残差。\n\n对每个 $\\rho$ 值实施此程序，以研究其对收敛性的影响。",
            "answer": "```python\nimport numpy as np\n\ndef run_admm_for_rho(rho, max_iter, tol):\n    \"\"\"\n    Solves the distributed DC-OPF problem for a given rho.\n    \"\"\"\n    # --- 1. Define problem data ---\n    # Reactances (p.u.)\n    x12, x23, x45, x34 = 0.1, 0.2, 0.15, 0.25\n    # Susceptances (p.u.)\n    b12, b23, b45, b34 = 1.0/x12, 1.0/x23, 1.0/x45, 1.0/x34\n    \n    # Loads (p.u.)\n    d1, d2, d3, d4, d5 = 0.0, 0.2, 1.0, 0.8, 0.3\n    \n    # Cost coefficients\n    c21, c11 = 0.02, 1.0\n    c22, c12 = 0.04, 1.2\n    c25, c15 = 0.03, 1.1\n\n    # --- 2. Build KKT matrices ---\n    # Area 1 KKT System: K1 * y1 = v1\n    # Variables y1 = [theta2, theta3, g1, g2, phi4, lam11, lam12, lam13]\n    K1 = np.zeros((8, 8))\n    # Q block (Hessian of local objective)\n    K1[1, 1] = rho\n    K1[2, 2] = 2 * c21\n    K1[3, 3] = 2 * c22\n    K1[4, 4] = rho\n    # A^T block (Jacobian of constraints)\n    A1_T = np.array([\n        [b12, b12 + b23, -b23],\n        [0.0, -b23, b23 + b34],\n        [1.0, 0.0, 0.0],\n        [0.0, -1.0, 0.0],\n        [0.0, 0.0, -b34]\n    ])\n    K1[0:5, 5:8] = A1_T\n    # A block\n    K1[5:8, 0:5] = A1_T.T\n\n    # Area 2 KKT System: K2 * y2 = v2\n    # Variables y2 = [theta4, theta5, g5, phi3, lam21, lam22]\n    K2 = np.zeros((6, 6))\n    # Q block\n    K2[0, 0] = rho\n    K2[2, 2] = 2 * c25\n    K2[3, 3] = rho\n    # A^T block\n    A2_T = np.array([\n        [b45 + b34, -b45],\n        [-b45, b45],\n        [0.0, -1.0],\n        [-b34, 0.0]\n    ])\n    K2[0:4, 4:6] = A2_T\n    # A block\n    K2[4:6, 0:4] = A2_T.T\n    \n    # --- 3. Initialize ADMM variables ---\n    # Consensus variables\n    z3, z4 = 0.0, 0.0\n    # Scaled dual variables\n    u3A, u4A = 0.0, 0.0  # Area 1 duals\n    u3B, u4B = 0.0, 0.0  # Area 2 duals\n    \n    final_r = np.inf\n\n    # --- 4. ADMM Iterations ---\n    for k in range(1, max_iter + 1):\n        # --- Area 1 subproblem solve ---\n        v1 = np.array([\n            -c11,\n            -c12,\n            rho * (z3 - u3A),\n            rho * (z4 - u4A),\n            d1,\n            -d2,\n            -d3\n        ])\n        # Reorder to match KKT matrix structure\n        v1_ordered = np.array([v1[2] - A1_T[1,0]*0, v1[2]*0, v1[0], v1[1], v1[3], v1[4], v1[5], v1[6]])\n        v1_ordered[0] = 0.\n        v1_ordered[1] = rho * (z3 - u3A)\n\n\n        y1 = np.linalg.solve(K1, v1_ordered)\n        _theta2, theta3_kplus1, _g1, _g2, phi4_kplus1 = y1[0], y1[1], y1[2], y1[3], y1[4]\n\n        # --- Area 2 subproblem solve ---\n        v2 = np.array([\n            rho * (z4 - u4B),\n            0.0,\n            -c15,\n            rho * (z3 - u3B),\n            -d4,\n            -d5\n        ])\n        y2 = np.linalg.solve(K2, v2)\n        theta4_kplus1, _theta5, _g5, phi3_kplus1 = y2[0], y2[1], y2[2], y2[3]\n\n        # --- Check convergence ---\n        final_r = np.sqrt((theta3_kplus1 - phi3_kplus1)**2 + (phi4_kplus1 - theta4_kplus1)**2)\n        if final_r = tol:\n            return k, final_r\n\n        # --- Consensus (z) update ---\n        z3_kplus1 = (theta3_kplus1 + u3A + phi3_kplus1 + u3B) / 2.0\n        z4_kplus1 = (phi4_kplus1 + u4A + theta4_kplus1 + u4B) / 2.0\n\n        # --- Dual (u) update ---\n        u3A_kplus1 = u3A + theta3_kplus1 - z3_kplus1\n        u4A_kplus1 = u4A + phi4_kplus1 - z4_kplus1\n        u3B_kplus1 = u3B + phi3_kplus1 - z3_kplus1\n        u4B_kplus1 = u4B + theta4_kplus1 - z4_kplus1\n        \n        # --- Update state for next iteration ---\n        z3, z4 = z3_kplus1, z4_kplus1\n        u3A, u4A = u3A_kplus1, u4A_kplus1\n        u3B, u4B = u3B_kplus1, u4B_kplus1\n\n    # If max iterations reached\n    return max_iter, final_r\n\ndef solve():\n    \"\"\"\n    Main function to run the ADMM simulation for different rho values.\n    \"\"\"\n    # Define test parameters from the problem statement\n    test_cases = [0.01, 0.1, 1.0, 10.0, 100.0]\n    max_iterations = 500\n    tolerance = 1e-6\n\n    # KKT matrix and RHS vector construction correction\n    # Correct mapping: ∂L/∂x = Qx + c + Aᵀλ = 0 => Qx + Aᵀλ = -c\n    # KKT System: [Q, Aᵀ; A, 0] [x; λ] = [-c; b]\n    \n    # Area 1 RHS vector construction\n    c11, c12 = 1.0, 1.2\n    d1, d2, d3 = 0.0, 0.2, 1.0\n    def get_v1(rho, z3, z4, u3A, u4A):\n        c1_vec = np.array([0, -rho*(z3-u3A), c11, c12, -rho*(z4-u4A)])\n        b1_vec = np.array([d1, -d2, -d3])\n        return np.concatenate((-c1_vec, b1_vec))\n    \n    # Area 2 RHS vector construction\n    c15 = 1.1\n    d4, d5 = 0.8, 0.3\n    def get_v2(rho, z3, z4, u3B, u4B):\n        c2_vec = np.array([-rho*(z4-u4B), 0, c15, -rho*(z3-u3B)])\n        b2_vec = np.array([-d4, -d5])\n        return np.concatenate((-c2_vec, b2_vec))\n\n    # This corrected logic will replace the one in run_admm_for_rho\n    def run_admm_corrected(rho, max_iter, tol):\n        # Data and KKT matrices setup (same as before)\n        x12, x23, x45, x34 = 0.1, 0.2, 0.15, 0.25\n        b12, b23, b45, b34 = 1.0/x12, 1.0/x23, 1.0/x45, 1.0/x34\n        c21, c22, c25 = 0.02, 0.04, 0.03\n        K1 = np.zeros((8, 8)); K2 = np.zeros((6, 6))\n        K1[1, 1] = rho; K1[2, 2] = 2*c21; K1[3, 3] = 2*c22; K1[4, 4] = rho\n        A1_T = np.array([[b12,b12+b23,-b23],[0,-b23,b23+b34],[1,0,0],[0,-1,0],[0,0,-b34]])\n        K1[0:5, 5:8] = A1_T; K1[5:8, 0:5] = A1_T.T\n        K2[0, 0] = rho; K2[2, 2] = 2*c25; K2[3, 3] = rho\n        A2_T = np.array([[b45+b34,-b45],[-b45,b45],[0,-1],[-b34,0]])\n        K2[0:4, 4:6] = A2_T; K2[4:6, 0:4] = A2_T.T\n\n        # ADMM Variables\n        z3, z4 = 0.0, 0.0\n        u3A, u4A, u3B, u4B = 0.0, 0.0, 0.0, 0.0\n        final_r = np.inf\n\n        for k in range(1, max_iter + 1):\n            # Area 1 Solve\n            v1 = get_v1(rho, z3, z4, u3A, u4A)\n            y1 = np.linalg.solve(K1, v1)\n            theta3_kplus1, phi4_kplus1 = y1[1], y1[4]\n            # Area 2 Solve\n            v2 = get_v2(rho, z3, z4, u3B, u4B)\n            y2 = np.linalg.solve(K2, v2)\n            theta4_kplus1, phi3_kplus1 = y2[0], y2[3]\n\n            final_r = np.sqrt((theta3_kplus1 - phi3_kplus1)**2 + (phi4_kplus1 - theta4_kplus1)**2)\n            if final_r = tol:\n                return k, final_r\n\n            # Consensus Update\n            z3_kplus1 = (theta3_kplus1 + u3A + phi3_kplus1 + u3B) / 2.0\n            z4_kplus1 = (phi4_kplus1 + u4A + theta4_kplus1 + u4B) / 2.0\n            # Dual Update\n            u3A_kplus1 = u3A + theta3_kplus1 - z3_kplus1\n            u4A_kplus1 = u4A + phi4_kplus1 - z4_kplus1\n            u3B_kplus1 = u3B + phi3_kplus1 - z3_kplus1\n            u4B_kplus1 = u4B + theta4_kplus1 - z4_kplus1\n            \n            # State Update\n            z3, z4 = z3_kplus1, z4_kplus1\n            u3A, u4A, u3B, u4B = u3A_kplus1, u4A_kplus1, u3B_kplus1, u4B_kplus1\n\n        return max_iter, final_r\n\n    results = []\n    for rho_val in test_cases:\n        # Use the corrected implementation\n        n_iters, final_r = run_admm_corrected(rho_val, max_iterations, tolerance)\n        results.append([n_iters, final_r])\n\n    # Format output string exactly as required\n    result_str_list = []\n    for item in results:\n        # Ensure standard float representation\n        formatted_r = str(item[1])\n        result_str_list.append(f\"[{item[0]}, {formatted_r}]\")\n        \n    print(f\"[{','.join(result_str_list)}]\")\n\nsolve()\n\n```"
        }
    ]
}