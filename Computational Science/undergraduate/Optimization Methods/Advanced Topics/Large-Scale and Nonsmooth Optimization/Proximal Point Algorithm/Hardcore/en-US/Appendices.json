{
    "hands_on_practices": [
        {
            "introduction": "The $\\ell_1$ norm is fundamental for promoting sparsity in machine learning and signal processing, and its proximal operator, the soft-thresholding function, is a cornerstone of many optimization algorithms. This practice guides you through the first-principles derivation of this crucial operator, starting from the subdifferential optimality conditions. By completing this exercise, you will gain a concrete understanding of how the proximal operator systematically shrinks coefficients and sets small ones to zero, which is the essence of sparsity .",
            "id": "3168299",
            "problem": "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be a closed proper convex function, and recall the definition of the proximal operator of $f$ at $y\\in\\mathbb{R}^n$ as the unique minimizer $x^{\\star}$ of the strongly convex problem $\\min_{x\\in\\mathbb{R}^n}\\left\\{f(x)+\\frac{1}{2}\\|x-y\\|_2^2\\right\\}$. The first-order optimality condition for this problem is the monotone inclusion $0\\in \\partial f(x^{\\star})+x^{\\star}-y$, which coincides with a single implicit step of the proximal point algorithm (PPA) on the subdifferential operator $\\partial f$. Let $f_{\\lambda}(x)=\\lambda\\|x\\|_1$ for $\\lambda>0$, where $\\|x\\|_1=\\sum_{i=1}^n |x_i|$, and consider computing $\\operatorname{prox}_{f_{\\lambda}}(y)$ by solving the inclusion $0\\in \\lambda\\,\\partial\\|x\\|_1 + x - y$. \n\nFirst, starting from the subdifferential characterization of the absolute value, derive explicitly the coordinate-wise solution of the inclusion $0\\in \\lambda\\,\\partial\\|x\\|_1 + x - y$ and hence obtain a closed-form expression for $\\operatorname{prox}_{\\lambda\\|x\\|_1}(y)$ in terms of $y$ and $\\lambda$. Second, consider the perturbed function $f_{\\lambda,\\varepsilon}(x)=\\lambda\\|x\\|_1 + \\frac{\\varepsilon}{2}\\|x\\|_2^2$ with $\\varepsilon>0$, and, again by applying the PPA optimality condition, derive the explicit coordinate-wise expression for $\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)$.\n\nNow take the specific data $y\\in\\mathbb{R}^3$ with $y=\\big(\\frac{3}{2},-\\frac{1}{3},\\frac{4}{5}\\big)$, parameters $\\lambda=\\frac{1}{2}$ and $\\varepsilon=\\frac{1}{4}$. Compute $\\operatorname{prox}_{\\lambda\\|x\\|_1}(y)$ and $\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)$, and then compute the squared Euclidean distance between these two proximal points, namely the value of $\\|\\operatorname{prox}_{\\lambda\\|x\\|_1}(y)-\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)\\|_2^2$. Provide your final answer as an exact rational number. No rounding is required and no physical units are involved.",
            "solution": "The solution is derived in three parts: deriving the proximal operator for $f_{\\lambda}(x)=\\lambda\\|x\\|_1$, deriving the proximal operator for the perturbed function $f_{\\lambda,\\varepsilon}(x)=\\lambda\\|x\\|_1 + \\frac{\\varepsilon}{2}\\|x\\|_2^2$, and finally computing the squared Euclidean distance between these two proximal points for the given data.\n\n**Part 1: Derivation of $\\operatorname{prox}_{\\lambda\\|\\cdot\\|_1}(y)$**\n\nThe proximal operator of $f_{\\lambda}(x) = \\lambda\\|x\\|_1$, which we denote $x^{(1)}$, is the unique solution to the separable minimization problem:\n$$ x^{(1)} = \\operatorname{prox}_{\\lambda\\|\\cdot\\|_1}(y) = \\arg\\min_{x\\in\\mathbb{R}^n}\\left\\{ \\lambda\\|x\\|_1 + \\frac{1}{2}\\|x-y\\|_2^2 \\right\\} = \\sum_{i=1}^n \\arg\\min_{x_i\\in\\mathbb{R}} \\left( \\lambda|x_i| + \\frac{1}{2}(x_i - y_i)^2 \\right) $$\nThe first-order optimality condition for the $i$-th component, $x_i$, is $0 \\in \\lambda\\,\\partial|x_i| + x_i - y_i$. We analyze this based on the sign of the solution $x_i$:\n*   **Case 1: $x_i > 0$**. The subdifferential is $\\partial|x_i| = \\{1\\}$, so the condition becomes $y_i - x_i = \\lambda$, implying $x_i = y_i - \\lambda$. This is consistent only if $y_i - \\lambda > 0$, i.e., $y_i > \\lambda$.\n*   **Case 2: $x_i  0$**. The subdifferential is $\\partial|x_i| = \\{-1\\}$, so $y_i - x_i = -\\lambda$, implying $x_i = y_i + \\lambda$. This is consistent only if $y_i + \\lambda  0$, i.e., $y_i  -\\lambda$.\n*   **Case 3: $x_i = 0$**. The subdifferential is $\\partial|x_i| = [-1, 1]$, so the condition becomes $y_i \\in \\lambda[-1, 1]$, or $|y_i| \\le \\lambda$.\n\nCombining these cases gives the well-known soft-thresholding operator:\n$$ x_i^{(1)} = \\begin{cases} y_i - \\lambda  \\text{if } y_i > \\lambda \\\\ 0  \\text{if } |y_i| \\le \\lambda \\\\ y_i + \\lambda  \\text{if } y_i  -\\lambda \\end{cases} = \\operatorname{sgn}(y_i) \\max(|y_i| - \\lambda, 0) $$\n\n**Part 2: Derivation of $\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)$**\n\nLet $x^{(2)} = \\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)$. The optimality condition is $0 \\in \\partial f_{\\lambda,\\varepsilon}(x^{(2)}) + x^{(2)} - y$. Using the sum rule, $\\partial f_{\\lambda,\\varepsilon}(x) = \\lambda\\,\\partial\\|x\\|_1 + \\varepsilon x$, the inclusion becomes:\n$$ 0 \\in \\lambda\\,\\partial\\|x^{(2)}\\|_1 + \\varepsilon x^{(2)} + x^{(2)} - y \\implies y_i - (1+\\varepsilon)x_i^{(2)} \\in \\lambda\\,\\partial|x_i^{(2)}| $$\nAnalyzing this coordinate-wise inclusion similarly to Part 1:\n*   **Case 1: $x_i^{(2)} > 0$**. The condition is $y_i - (1+\\varepsilon)x_i^{(2)} = \\lambda$, which yields $x_i^{(2)} = \\frac{y_i - \\lambda}{1+\\varepsilon}$. This holds if $y_i > \\lambda$.\n*   **Case 2: $x_i^{(2)}  0$**. The condition is $y_i - (1+\\varepsilon)x_i^{(2)} = -\\lambda$, which yields $x_i^{(2)} = \\frac{y_i + \\lambda}{1+\\varepsilon}$. This holds if $y_i  -\\lambda$.\n*   **Case 3: $x_i^{(2)} = 0$**. The condition is $|y_i| \\le \\lambda$.\n\nBy comparing with the result from Part 1, we see a simple relationship:\n$$ x_i^{(2)} = \\frac{1}{1+\\varepsilon} \\operatorname{sgn}(y_i) \\max(|y_i| - \\lambda, 0) = \\frac{1}{1+\\varepsilon} x_i^{(1)} $$\nThus, the vector solution is $x^{(2)} = \\frac{1}{1+\\varepsilon}x^{(1)}$.\n\n**Part 3: Numerical Calculation**\n\nGiven $y=\\big(\\frac{3}{2},-\\frac{1}{3},\\frac{4}{5}\\big)$, $\\lambda=\\frac{1}{2}$, and $\\varepsilon=\\frac{1}{4}$, we first compute $x^{(1)} = \\operatorname{prox}_{\\lambda\\|x\\|_1}(y)$:\n*   $y_1 = \\frac{3}{2} > \\lambda = \\frac{1}{2} \\implies x_1^{(1)} = \\frac{3}{2} - \\frac{1}{2} = 1$.\n*   $|y_2| = \\frac{1}{3} \\le \\lambda = \\frac{1}{2} \\implies x_2^{(1)} = 0$.\n*   $y_3 = \\frac{4}{5} > \\lambda = \\frac{1}{2} \\implies x_3^{(1)} = \\frac{4}{5} - \\frac{1}{2} = \\frac{3}{10}$.\nSo, $x^{(1)} = \\left(1, 0, \\frac{3}{10}\\right)$.\n\nThe quantity to compute is $\\|x^{(1)} - x^{(2)}\\|_2^2$. Using the relationship from Part 2:\n$$ \\|x^{(1)} - x^{(2)}\\|_2^2 = \\left\\|x^{(1)} - \\frac{1}{1+\\varepsilon}x^{(1)}\\right\\|_2^2 = \\left\\|\\left(1 - \\frac{1}{1+\\varepsilon}\\right)x^{(1)}\\right\\|_2^2 = \\left(\\frac{\\varepsilon}{1+\\varepsilon}\\right)^2 \\|x^{(1)}\\|_2^2 $$\nFirst, compute the scalar factor:\n$$ \\left(\\frac{\\varepsilon}{1+\\varepsilon}\\right)^2 = \\left(\\frac{\\frac{1}{4}}{1+\\frac{1}{4}}\\right)^2 = \\left(\\frac{1/4}{5/4}\\right)^2 = \\left(\\frac{1}{5}\\right)^2 = \\frac{1}{25} $$\nNext, compute the squared norm of $x^{(1)}$:\n$$ \\|x^{(1)}\\|_2^2 = 1^2 + 0^2 + \\left(\\frac{3}{10}\\right)^2 = 1 + \\frac{9}{100} = \\frac{109}{100} $$\nFinally, combine the results:\n$$ \\|x^{(1)} - x^{(2)}\\|_2^2 = \\frac{1}{25} \\times \\frac{109}{100} = \\frac{109}{2500} $$",
            "answer": "$$\\boxed{\\frac{109}{2500}}$$"
        },
        {
            "introduction": "While the $\\ell_1$ norm is ubiquitous, other norms like the $\\ell_\\infty$ norm also play important roles, for instance in ensuring uniform bounds on model parameters. Its proximal operator behaves quite differently from the soft-thresholding operator, illustrating a mechanism of value redistribution rather than independent shrinkage. This exercise challenges you to compute the proximal operator for the non-separable $\\ell_\\infty$ norm, providing deeper insight into the geometry of proximal mappings and the techniques required to analyze them .",
            "id": "3168264",
            "problem": "Let $f:\\mathbb{R}^{4}\\to\\mathbb{R}$ be the convex function $f(x)=\\|x\\|_{\\infty}$, where for $x=(x_{1},x_{2},x_{3},x_{4})$, $\\|x\\|_{\\infty}=\\max\\{|x_{1}|,|x_{2}|,|x_{3}|,|x_{4}|\\}$. Consider the resolvent of the set-valued subdifferential operator, namely $(I+\\lambda\\,\\partial f)^{-1}$, which appears in the proximal point algorithm for minimizing $f$.\n\n1) Using only the definition of the subdifferential and standard facts about dual norms, construct a concrete point $x\\in\\mathbb{R}^{4}$ at which the subdifferential $\\partial f(x)$ is set-valued (i.e., contains more than one vector), and justify why it is set-valued at that point. Your construction should be explicit and self-contained.\n\n2) With $\\lambda=2$ and $y=(3,\\,2,\\,-1,\\,0.5)$, compute the resolvent value $(I+\\lambda\\,\\partial f)^{-1}(y)$ exactly. Your final numerical answer must be a single $1\\times 4$ row vector. No rounding is required.\n\n3) Briefly interpret your computed point from part 2 in terms of how the proximal mapping for the infinity norm redistributes the influence of the largest-magnitude coordinates. Make your interpretation precise using the language of subgradients and convex combinations.\n\nThe final answer to be reported must be the vector from part 2 only, written exactly (no rounding or units).",
            "solution": "This problem involves analyzing the subdifferential and proximal operator (resolvent) of the infinity norm.\n\n**Part 1: A point with a set-valued subdifferential**\n\nThe subdifferential of $f(x) = \\|x\\|_{\\infty}$ is given by the set $\\partial f(x) = \\{ g \\in \\mathbb{R}^4 \\mid \\|g\\|_1 \\le 1 \\text{ and } g^\\top x = \\|x\\|_{\\infty} \\}$. The subdifferential is set-valued (contains more than one vector) at points where the maximum absolute value is achieved by more than one component.\n\nConsider the point $x = (5, -5, 1, 2)$. For this point, $\\|x\\|_{\\infty} = \\max\\{|5|, |-5|, |1|, |2|\\} = 5$. The active indices are $1$ and $2$.\n\nLet's find two distinct subgradients:\n1.  Consider $g^{(1)} = (1, 0, 0, 0)$. We check the conditions:\n    *   $\\|g^{(1)}\\|_1 = 1$, which satisfies $\\|g\\|_1 \\le 1$.\n    *   $(g^{(1)})^\\top x = 1(5) + 0(-5) + 0(1) + 0(2) = 5 = \\|x\\|_{\\infty}$.\n    Thus, $g^{(1)} \\in \\partial f(x)$.\n\n2.  Consider $g^{(2)} = (0, -1, 0, 0)$. We check the conditions:\n    *   $\\|g^{(2)}\\|_1 = 1$, which satisfies $\\|g\\|_1 \\le 1$.\n    *   $(g^{(2)})^\\top x = 0(5) - 1(-5) + 0(1) + 0(2) = 5 = \\|x\\|_{\\infty}$.\n    Thus, $g^{(2)} \\in \\partial f(x)$.\n\nSince $g^{(1)} \\neq g^{(2)}$, the subdifferential $\\partial f(x)$ at $x = (5, -5, 1, 2)$ is set-valued.\n\n**Part 2: Computation of the resolvent**\n\nWe must compute $x_{\\text{out}} = (I + \\lambda \\partial f)^{-1}(y)$, which is equivalent to computing the proximal operator $x_{\\text{out}} = \\text{prox}_{\\lambda f}(y)$ for $\\lambda = 2$ and $y = (3, 2, -1, 0.5)$. The problem is:\n$$ x_{\\text{out}} = \\arg\\min_{x \\in \\mathbb{R}^4} \\left\\{ 2 \\|x\\|_{\\infty} + \\frac{1}{2} \\|x - y\\|_2^2 \\right\\} $$\nLet $\\alpha = \\|x\\|_{\\infty}$. The problem is equivalent to $\\min_{\\alpha \\ge 0} H(\\alpha)$, where\n$$ H(\\alpha) = 2\\alpha + \\min_{x: \\|x\\|_\\infty \\le \\alpha} \\frac{1}{2} \\|x - y\\|_2^2 $$\nThe inner minimization finds the projection of $y$ onto the $\\ell_\\infty$-ball of radius $\\alpha$. The solution is $x_i(\\alpha) = \\text{sign}(y_i) \\min\\{|y_i|, \\alpha\\}$. Substituting this back, the objective becomes:\n$$ H(\\alpha) = 2\\alpha + \\frac{1}{2} \\sum_{i=1}^4 (\\text{sign}(y_i) \\min\\{|y_i|, \\alpha\\} - y_i)^2 = 2\\alpha + \\frac{1}{2} \\sum_{i \\text{ s.t. } |y_i| > \\alpha} (|y_i| - \\alpha)^2 $$\nTo find the optimal $\\alpha^*$, we set the derivative of this convex function to zero:\n$$ H'(\\alpha) = 2 - \\sum_{i \\text{ s.t. } |y_i| > \\alpha} (|y_i| - \\alpha) = 0 $$\nThe absolute values of the components of $y$ are $\\{3, 2, 1, 0.5\\}$, sorted as $3 > 2 > 1 > 0.5$. Let's test intervals for $\\alpha^*$.\nIf we assume $1 \\le \\alpha  2$, the active components in the sum are those with magnitudes 3 and 2.\n$$ H'(\\alpha) = 2 - \\left( (3-\\alpha) + (2-\\alpha) \\right) = 2 - (5 - 2\\alpha) = 2\\alpha - 3 $$\nSetting $H'(\\alpha) = 0$ gives $2\\alpha - 3 = 0$, so $\\alpha = 1.5$. Since $1 \\le 1.5  2$, our assumption was correct and $\\alpha^*=1.5$.\n\nNow we compute the solution vector $x_{\\text{out}}$ using $x_{\\text{out}, i} = \\text{sign}(y_i) \\min\\{|y_i|, 1.5\\}$:\n*   $y_1=3 \\implies x_{\\text{out}, 1} = \\text{sign}(3) \\min\\{3, 1.5\\} = 1.5 = \\frac{3}{2}$.\n*   $y_2=2 \\implies x_{\\text{out}, 2} = \\text{sign}(2) \\min\\{2, 1.5\\} = 1.5 = \\frac{3}{2}$.\n*   $y_3=-1 \\implies x_{\\text{out}, 3} = \\text{sign}(-1) \\min\\{1, 1.5\\} = -1$.\n*   $y_4=0.5 \\implies x_{\\text{out}, 4} = \\text{sign}(0.5) \\min\\{0.5, 1.5\\} = 0.5 = \\frac{1}{2}$.\nThe resolvent value is $x_{\\text{out}} = (\\frac{3}{2}, \\frac{3}{2}, -1, \\frac{1}{2})$.\n\n**Part 3: Interpretation**\n\nThe proximal operator for the infinity norm shrinks components with magnitude larger than a certain threshold $\\alpha^*$ down to that threshold, while leaving components with magnitude smaller than $\\alpha^*$ unchanged. In this example, $\\alpha^* = 1.5$. The original components $y_1=3$ and $y_2=2$ are shrunk to $1.5$, while $y_3=-1$ and $y_4=0.5$ are preserved. This creates a solution where the largest-magnitude components are \"leveled off\". This is a redistribution of values, unlike the independent soft-thresholding of the $\\ell_1$ norm. The threshold $\\alpha^*$ is chosen such that the total amount of shrinkage, $\\sum_{i:|y_i|>\\alpha^*} (|y_i|-\\alpha^*)$, equals the parameter $\\lambda$. Here, $(3-1.5) + (2-1.5) = 1.5 + 0.5 = 2 = \\lambda$. The subgradient $\\frac{y-x_{\\text{out}}}{\\lambda} = \\frac{(1.5, 0.5, 0, 0)}{2} = (0.75, 0.25, 0, 0)$ is a convex combination of the elementary subgradients $(1,0,0,0)$ and $(0,1,0,0)$ associated with the active components of the output $x_{\\text{out}}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} \\frac{3}{2}  \\frac{3}{2}  -1  \\frac{1}{2} \\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The true power of the Proximal Point Algorithm (PPA) shines when it is applied to solve complex, real-world optimization problems. This practice immerses you in the task of training a logistic regression model with a group Lasso penalty, a technique used to select or discard entire groups of features simultaneously. Here, you will bridge theory and practice by deriving the group-wise proximal operator and implementing a complete solver using a nested PPA-PGM scheme, solidifying your ability to apply the PPA framework to sophisticated machine learning tasks .",
            "id": "3168254",
            "problem": "Consider the convex optimization problem of binary classification with a sparsity-inducing group penalty. Let the data matrix be $X \\in \\mathbb{R}^{n \\times d}$ and the label vector be $y \\in \\{-1,+1\\}^n$. Define the logistic loss\n$$\nf(w) \\equiv \\frac{1}{n} \\sum_{i=1}^{n} \\log\\bigl(1 + \\exp(-y_i \\, x_i^\\top w)\\bigr),\n$$\nwhere $w \\in \\mathbb{R}^d$ and $x_i^\\top$ is the $i$-th row of $X$. Suppose the feature indices are partitioned into disjoint groups $\\{G_1, G_2, \\dots, G_m\\}$ that cover $\\{1,2,\\dots,d\\}$, and define the group Lasso penalty\n$$\nR(w) \\equiv \\sum_{g=1}^m \\|w_{G_g}\\|_2,\n$$\nwhere $w_{G_g}$ collects the coordinates of $w$ indexed by the group $G_g$. For a regularization parameter $\\lambda \\ge 0$, the learning problem is\n$$\n\\min_{w \\in \\mathbb{R}^d} \\; F(w) \\equiv f(w) + \\lambda R(w).\n$$\n\nYour tasks are:\n1) Starting from the definition of the proximal operator for a proper, closed, convex function $\\phi$,\n$$\n\\mathrm{prox}_{\\alpha \\phi}(v) \\equiv \\arg\\min_{u \\in \\mathbb{R}^d} \\left\\{ \\phi(u) + \\frac{1}{2\\alpha}\\|u - v\\|_2^2 \\right\\}, \\quad \\alpha  0,\n$$\nderive the explicit form of the proximal map associated with the group-wise penalty $\\phi(w) = \\sum_{g=1}^m \\|w_{G_g}\\|_2$, acting group-by-group on any $v \\in \\mathbb{R}^d$.\n\n2) Apply the Proximal Point Algorithm (PPA) to $F(w)$. The PPA iterates are\n$$\nw^{k+1} \\in \\arg\\min_{w \\in \\mathbb{R}^d} \\left\\{ f(w) + \\lambda R(w) + \\frac{1}{2t}\\|w - w^k\\|_2^2 \\right\\},\n$$\nwith step parameter $t  0$. Each subproblem is strongly convex. For numerical implementation, solve each PPA subproblem by a proximal-gradient method on the composite objective $f(w) + \\frac{1}{2t}\\|w - w^k\\|_2^2 + \\lambda R(w)$, using a constant stepsize based on an upper bound of the Lipschitz constant of the gradient of the smooth part. Use the fact that the Hessian of the logistic loss satisfies $\\nabla^2 f(w) \\preceq \\frac{1}{4n} X^\\top X$ for all $w$, hence an admissible Lipschitz constant is $L_f \\le \\frac{\\lambda_{\\max}(X^\\top X)}{4n}$, and the smooth part of the subproblem has Lipschitz constant $L_{\\text{sub}} = L_f + \\frac{1}{t}$. Use stepsize $\\frac{1}{L_{\\text{sub}}}$.\n\n3) Evaluate grouping effects as $\\lambda$ varies by counting the number of active groups at convergence, where a group $G_g$ is called active if $\\|w_{G_g}\\|_2  \\epsilon$ for a small threshold $\\epsilon$. Also report the number of outer PPA iterations taken to reach convergence and the final objective value $F(w)$ at the converged solution.\n\nUse the following fixed instance:\n- Dimensions: $n = 12$, $d = 6$, and $m = 3$ groups with $G_1 = \\{1,2\\}$, $G_2 = \\{3,4\\}$, $G_3 = \\{5,6\\}$.\n- Data matrix $X$ (rows $x_i^\\top$):\n  - Row $1$: $[0.50, -1.20, 0.30, 0.80, -0.50, 1.00]$\n  - Row $2$: $[1.50, 0.20, -0.30, 0.40, 0.70, -1.20]$\n  - Row $3$: $[-0.80, 0.90, 1.10, -1.30, 0.20, 0.50]$\n  - Row $4$: $[0.00, 0.30, -0.70, 0.60, -1.00, 0.90]$\n  - Row $5$: $[1.20, -0.50, 0.60, -0.20, 0.40, -0.80]$\n  - Row $6$: $[-1.10, 1.40, -0.40, 0.50, -0.60, 0.30]$\n  - Row $7$: $[0.70, -0.90, 0.20, -0.40, 1.30, -0.70]$\n  - Row $8$: $[-0.60, 0.80, -1.20, 1.00, -0.30, 0.20]$\n  - Row $9$: $[0.90, -0.40, 0.10, -0.90, 0.80, -0.60]$\n  - Row $10$: $[-0.30, 1.10, -0.50, 0.20, -0.70, 1.20]$\n  - Row $11$: $[0.40, -1.00, 0.90, -0.10, 0.60, -0.40]$\n  - Row $12$: $[-0.90, 0.60, -0.80, 1.20, -0.20, 0.10]$\n- Labels $y$: $[+1, +1, -1, -1, +1, -1, +1, -1, +1, -1, +1, -1]$.\n\nImplementation details:\n- Initialize $w^0 = 0$.\n- Use constant $t = 1.0$.\n- Outer PPA stopping rule: stop when $\\|w^{k+1} - w^k\\|_2 \\le 10^{-6}$ or after $100$ outer iterations, whichever occurs first.\n- Inner proximal-gradient loop for each subproblem: use stepsize $\\frac{1}{L_{\\text{sub}}}$ with $L_{\\text{sub}} = \\frac{\\lambda_{\\max}(X^\\top X)}{4n} + \\frac{1}{t}$, stop when the iterate change has Euclidean norm $\\le 10^{-8}$ or after $500$ inner iterations, whichever occurs first.\n- Active group threshold: $\\epsilon = 10^{-6}$.\n\nTest suite:\n- Four regularization levels $\\lambda \\in \\{0.0, 0.1, 0.5, 2.0\\}$.\n- For each $\\lambda$, run the PPA as specified and compute:\n  - the number of active groups at convergence (an integer),\n  - the number of outer iterations used (an integer),\n  - the final objective value $F(w)$ at the converged $w$, rounded to six digits after the decimal point (a float).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each entry corresponds to one $\\lambda$ in the given order, and is itself a list of three values: $[\\text{active\\_groups}, \\text{outer\\_iterations}, \\text{final\\_objective}]$. For example, the overall output must look like\n$$\n[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3],[a_4,b_4,c_4]],\n$$\nwith no additional text.",
            "solution": "This problem requires deriving the proximal operator for the group Lasso penalty and then implementing a nested algorithm to solve a group-Lasso regularized logistic regression problem.\n\n**Part 1: Derivation of the Proximal Operator for the Group Lasso Penalty**\n\nThe proximal operator for $\\phi(w) = R(w) = \\sum_{g=1}^m \\|w_{G_g}\\|_2$ is defined as:\n$$ \\mathrm{prox}_{\\alpha R}(v) = \\arg\\min_{u \\in \\mathbb{R}^d} \\left\\{ \\sum_{g=1}^m \\|u_{G_g}\\|_2 + \\frac{1}{2\\alpha}\\|u - v\\|_2^2 \\right\\} $$\nBecause the groups of coordinates are disjoint, this problem is separable. We can solve for each group $G_g$ independently:\n$$ (\\mathrm{prox}_{\\alpha R}(v))_{G_g} = \\arg\\min_{u_g \\in \\mathbb{R}^{|G_g|}} \\left\\{ \\|u_g\\|_2 + \\frac{1}{2\\alpha}\\|u_g - v_g\\|_2^2 \\right\\} $$\nwhere $u_g = u_{G_g}$ and $v_g = v_{G_g}$. The optimality condition is $0 \\in \\partial \\|u_g^*\\|_2 + \\frac{1}{\\alpha}(u_g^* - v_g)$.\n*   If $\\|v_g\\|_2 \\le \\alpha$, the minimizer is $u_g^* = 0$.\n*   If $\\|v_g\\|_2 > \\alpha$, the minimizer is non-zero. The optimality condition implies that $u_g^*$ is a positive scaling of $v_g$. By solving for the scaling factor, we find $u_g^* = \\left(1 - \\frac{\\alpha}{\\|v_g\\|_2}\\right) v_g$.\n\nCombining these cases gives the group-wise soft-thresholding operator:\n$$ (\\mathrm{prox}_{\\alpha R}(v))_{G_g} = \\left(1 - \\frac{\\alpha}{\\|v_{G_g}\\|_2}\\right)_+ v_{G_g}, \\quad \\text{where } (x)_+ = \\max(0, x). $$\n\n**Part 2: Algorithmic Specification**\n\nThe problem is solved using a nested approach: an outer Proximal Point Algorithm (PPA) and an inner Proximal Gradient Method (PGM).\n\n*   **Outer PPA Loop:** Iterates $w^{k+1} = \\arg\\min_{w} \\left\\{ f(w) + \\lambda R(w) + \\frac{1}{2t}\\|w - w^k\\|_2^2 \\right\\}$.\n*   **Inner PGM Loop:** Solves the PPA subproblem by treating it as a composite optimization problem:\n    *   Smooth part: $S_k(w) = f(w) + \\frac{1}{2t}\\|w - w^k\\|_2^2$.\n    *   Non-smooth part: $N(w) = \\lambda R(w)$.\n    The PGM update for the inner iterates $z^j$ is:\n    $$ z^{j+1} = \\mathrm{prox}_{\\eta (\\lambda R)}(z^j - \\eta \\nabla S_k(z^j)) $$\n    where $\\nabla S_k(z^j) = \\nabla f(z^j) + \\frac{1}{t}(z^j - w^k)$ and the step size is $\\eta = 1/L_{\\text{sub}} = 1/(L_f + 1/t)$. The proximal operator is the group-wise soft-thresholding derived above, with a threshold of $\\eta\\lambda$.\n\n**Part 3: Implementation and Evaluation**\n\nThe algorithm is implemented according to the specifications in the problem statement. For each value of $\\lambda$, the PPA is run until convergence, and the required metrics (number of active groups, outer iterations, and final objective value) are computed from the final solution vector $w^*$. The code to perform this computation is provided in the answer block.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the group-Lasso regularized logistic regression problem using a \n    Proximal Point Algorithm, with subproblems solved by a Proximal Gradient Method.\n    \"\"\"\n    \n    # ------------------ Problem Data and Fixed Parameters ------------------\n    X = np.array([\n        [0.50, -1.20, 0.30, 0.80, -0.50, 1.00],\n        [1.50, 0.20, -0.30, 0.40, 0.70, -1.20],\n        [-0.80, 0.90, 1.10, -1.30, 0.20, 0.50],\n        [0.00, 0.30, -0.70, 0.60, -1.00, 0.90],\n        [1.20, -0.50, 0.60, -0.20, 0.40, -0.80],\n        [-1.10, 1.40, -0.40, 0.50, -0.60, 0.30],\n        [0.70, -0.90, 0.20, -0.40, 1.30, -0.70],\n        [-0.60, 0.80, -1.20, 1.00, -0.30, 0.20],\n        [0.90, -0.40, 0.10, -0.90, 0.80, -0.60],\n        [-0.30, 1.10, -0.50, 0.20, -0.70, 1.20],\n        [0.40, -1.00, 0.90, -0.10, 0.60, -0.40],\n        [-0.90, 0.60, -0.80, 1.20, -0.20, 0.10],\n    ])\n    y = np.array([1, 1, -1, -1, 1, -1, 1, -1, 1, -1, 1, -1])\n    n, d, m = 12, 6, 3\n    groups = [(0, 2), (2, 4), (4, 6)]\n\n    # Algorithmic parameters\n    t = 1.0\n    outer_tol = 1e-6\n    max_outer_iter = 100\n    inner_tol = 1e-8\n    max_inner_iter = 500\n    epsilon = 1e-6\n    \n    # Test suite\n    lambdas = [0.0, 0.1, 0.5, 2.0]\n\n    # ------------------ Pre-computation for PGM Step Size ------------------\n    X_T_X = X.T @ X\n    lambda_max_XTX = np.linalg.eigvalsh(X_T_X)[-1]\n    L_f = lambda_max_XTX / (4 * n)\n    L_sub = L_f + 1.0 / t\n    eta = 1.0 / L_sub\n\n    # ------------------ Helper Functions ------------------\n    def logistic_loss(w):\n        \"\"\"Computes f(w), the logistic loss.\"\"\"\n        y_scores = y * (X @ w)\n        return np.mean(np.logaddexp(0, -y_scores))\n\n    def grad_logistic_loss(w):\n        \"\"\"Computes gradient of f(w).\"\"\"\n        y_scores = y * (X @ w)\n        sigmoid_vals = 1.0 / (1.0 + np.exp(y_scores))\n        coeffs = -y * sigmoid_vals\n        return (X.T @ coeffs) / n\n\n    def group_lasso_penalty(w):\n        \"\"\"Computes R(w), the group lasso penalty.\"\"\"\n        penalty = 0.0\n        for start, end in groups:\n            penalty += np.linalg.norm(w[start:end])\n        return penalty\n\n    def objective_function(w, _lambda):\n        \"\"\"Computes F(w) = f(w) + lambda * R(w).\"\"\"\n        return logistic_loss(w) + _lambda * group_lasso_penalty(w)\n\n    def prox_group_lasso(v, alpha):\n        \"\"\"Computes the proximal operator for the group lasso penalty.\"\"\"\n        u = np.zeros_like(v)\n        for start, end in groups:\n            v_g = v[start:end]\n            norm_v_g = np.linalg.norm(v_g)\n            if norm_v_g > 0:\n                scale = max(0, 1 - alpha / norm_v_g)\n                u[start:end] = scale * v_g\n        return u\n\n    # ------------------ Main Algorithm ------------------\n    all_results = []\n    \n    for _lambda in lambdas:\n        w = np.zeros(d)\n        \n        # Outer PPA loop\n        for k_outer in range(max_outer_iter):\n            w_prev_outer = w.copy()\n            \n            # Inner PGM loop for the PPA subproblem\n            z = w.copy()  # Warm start\n            for k_inner in range(max_inner_iter):\n                z_prev_inner = z.copy()\n                \n                # PGM update step\n                grad_smooth = grad_logistic_loss(z) + (z - w_prev_outer) / t\n                v = z - eta * grad_smooth\n                z = prox_group_lasso(v, eta * _lambda)\n                \n                if np.linalg.norm(z - z_prev_inner) = inner_tol:\n                    break\n            \n            w = z\n            if np.linalg.norm(w - w_prev_outer) = outer_tol:\n                break\n        \n        num_outer_iter = k_outer + 1\n        \n        # ------------------ Post-processing and Result Collection ------------------\n        active_groups = 0\n        for start, end in groups:\n            if np.linalg.norm(w[start:end]) > epsilon:\n                active_groups += 1\n\n        final_objective = objective_function(w, _lambda)\n        \n        all_results.append([active_groups, num_outer_iter, round(final_objective, 6)])\n\n    # ------------------ Final Output Formatting ------------------\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}