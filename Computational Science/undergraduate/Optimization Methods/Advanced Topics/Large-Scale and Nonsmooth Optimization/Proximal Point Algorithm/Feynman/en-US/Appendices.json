{
    "hands_on_practices": [
        {
            "introduction": "This practice is foundational to understanding how the Proximal Point Algorithm (PPA) is used in modern data science. We will derive the proximal operator for the $\\ell_1$ norm, a key building block in sparse optimization known as the soft-thresholding operator. This exercise connects the abstract definition of the PPA to a concrete, widely used algorithm for promoting sparsity in solutions, and explores how the operator behaves under a simple quadratic perturbation .",
            "id": "3168299",
            "problem": "Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be a closed proper convex function, and recall the definition of the proximal operator of $f$ at $y\\in\\mathbb{R}^n$ as the unique minimizer $x^{\\star}$ of the strongly convex problem $\\min_{x\\in\\mathbb{R}^n}\\left\\{f(x)+\\frac{1}{2}\\|x-y\\|_2^2\\right\\}$. The first-order optimality condition for this problem is the monotone inclusion $0\\in \\partial f(x^{\\star})+x^{\\star}-y$, which coincides with a single implicit step of the proximal point algorithm (PPA) on the subdifferential operator $\\partial f$. Let $f_{\\lambda}(x)=\\lambda\\|x\\|_1$ for $\\lambda>0$, where $\\|x\\|_1=\\sum_{i=1}^n |x_i|$, and consider computing $\\operatorname{prox}_{f_{\\lambda}}(y)$ by solving the inclusion $0\\in \\lambda\\,\\partial\\|x\\|_1 + x - y$. \n\nFirst, starting from the subdifferential characterization of the absolute value, derive explicitly the coordinate-wise solution of the inclusion $0\\in \\lambda\\,\\partial\\|x\\|_1 + x - y$ and hence obtain a closed-form expression for $\\operatorname{prox}_{\\lambda\\|x\\|_1}(y)$ in terms of $y$ and $\\lambda$. Second, consider the perturbed function $f_{\\lambda,\\varepsilon}(x)=\\lambda\\|x\\|_1 + \\frac{\\varepsilon}{2}\\|x\\|_2^2$ with $\\varepsilon>0$, and, again by applying the PPA optimality condition, derive the explicit coordinate-wise expression for $\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)$.\n\nNow take the specific data $y\\in\\mathbb{R}^3$ with $y=\\big(\\frac{3}{2},-\\frac{1}{3},\\frac{4}{5}\\big)$, parameters $\\lambda=\\frac{1}{2}$ and $\\varepsilon=\\frac{1}{4}$. Compute $\\operatorname{prox}_{\\lambda\\|x\\|_1}(y)$ and $\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)$, and then compute the squared Euclidean distance between these two proximal points, namely the value of $\\|\\operatorname{prox}_{\\lambda\\|x\\|_1}(y)-\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)\\|_2^2$. Provide your final answer as an exact rational number. No rounding is required and no physical units are involved.",
            "solution": "The problem requires the derivation of two proximal operators and a subsequent numerical calculation. The process is broken down into three main parts: first, deriving the proximal operator for the function $f_{\\lambda}(x)=\\lambda\\|x\\|_1$; second, deriving the proximal operator for the perturbed function $f_{\\lambda,\\varepsilon}(x)=\\lambda\\|x\\|_1 + \\frac{\\varepsilon}{2}\\|x\\|_2^2$; and third, computing the squared Euclidean distance between these two proximal points for specific given data.\n\nThe problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. All necessary information is provided, and the problem is a standard exercise in convex optimization.\n\nPart 1: Derivation of $\\operatorname{prox}_{\\lambda\\|\\cdot\\|_1}(y)$\n\nThe proximal operator of $f_{\\lambda}(x) = \\lambda\\|x\\|_1$ is defined as the unique solution to the minimization problem:\n$$ x^{(1)} = \\operatorname{prox}_{\\lambda\\|\\cdot\\|_1}(y) = \\arg\\min_{x\\in\\mathbb{R}^n}\\left\\{ \\lambda\\|x\\|_1 + \\frac{1}{2}\\|x-y\\|_2^2 \\right\\} $$\nThe objective function is separable, meaning it can be written as a sum of functions of individual components of $x$:\n$$ \\sum_{i=1}^n \\left( \\lambda|x_i| + \\frac{1}{2}(x_i - y_i)^2 \\right) $$\nTherefore, we can find the minimizer by solving $n$ independent scalar minimization problems. The first-order optimality condition for the $i$-th component, $x_i$, is given by the inclusion:\n$$ 0 \\in \\lambda\\,\\partial|x_i| + x_i - y_i $$\nThis can be rewritten as $y_i - x_i \\in \\lambda\\,\\partial|x_i|$. The subdifferential of the absolute value function, $g(t)=|t|$, is:\n$$ \\partial|t| = \\begin{cases} \\{1\\} & \\text{if } t > 0 \\\\ \\{-1\\} & \\text{if } t < 0 \\\\ [-1, 1] & \\text{if } t = 0 \\end{cases} $$\nWe analyze the inclusion based on the sign of the solution $x_i$:\nCase 1: $x_i > 0$. The subdifferential is $\\partial|x_i| = \\{1\\}$. The inclusion becomes $y_i - x_i = \\lambda$, which implies $x_i = y_i - \\lambda$. This is consistent only if $y_i - \\lambda > 0$, i.e., $y_i > \\lambda$.\nCase 2: $x_i < 0$. The subdifferential is $\\partial|x_i| = \\{-1\\}$. The inclusion becomes $y_i - x_i = -\\lambda$, which implies $x_i = y_i + \\lambda$. This is consistent only if $y_i + \\lambda < 0$, i.e., $y_i < -\\lambda$.\nCase 3: $x_i = 0$. The subdifferential is $\\partial|x_i| = [-1, 1]$. The inclusion becomes $y_i - 0 \\in \\lambda[-1, 1]$, which simplifies to $y_i \\in [-\\lambda, \\lambda]$, or $|y_i| \\le \\lambda$.\n\nCombining these three cases, the coordinate-wise solution is:\n$$ x_i^{(1)} = \\begin{cases} y_i - \\lambda & \\text{if } y_i > \\lambda \\\\ 0 & \\text{if } |y_i| \\le \\lambda \\\\ y_i + \\lambda & \\text{if } y_i < -\\lambda \\end{cases} $$\nThis operator is known as the soft-thresholding operator, which can be written compactly as $x_i^{(1)} = S_{\\lambda}(y_i) = \\operatorname{sgn}(y_i) \\max(|y_i| - \\lambda, 0)$. Thus, $\\operatorname{prox}_{\\lambda\\|\\cdot\\|_1}(y) = S_{\\lambda}(y)$, where the operator is applied component-wise.\n\nPart 2: Derivation of $\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)$\n\nLet $x^{(2)} = \\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)$. The function is $f_{\\lambda,\\varepsilon}(x)=\\lambda\\|x\\|_1 + \\frac{\\varepsilon}{2}\\|x\\|_2^2$. The optimality condition for $x^{(2)}$ is:\n$$ 0 \\in \\partial f_{\\lambda,\\varepsilon}(x^{(2)}) + x^{(2)} - y $$\nUsing the sum rule for subdifferentials (since $\\frac{\\varepsilon}{2}\\|x\\|_2^2$ is continuously differentiable), we have $\\partial f_{\\lambda,\\varepsilon}(x) = \\lambda\\,\\partial\\|x\\|_1 + \\varepsilon x$. The inclusion becomes:\n$$ 0 \\in \\lambda\\,\\partial\\|x^{(2)}\\|_1 + \\varepsilon x^{(2)} + x^{(2)} - y = \\lambda\\,\\partial\\|x^{(2)}\\|_1 + (1+\\varepsilon)x^{(2)} - y $$\nThis problem is also separable. The coordinate-wise inclusion is:\n$$ y_i - (1+\\varepsilon)x_i^{(2)} \\in \\lambda\\,\\partial|x_i^{(2)}| $$\nWe analyze this inclusion in the same manner as before:\nCase 1: $x_i^{(2)} > 0$. Then $\\partial|x_i^{(2)}| = \\{1\\}$, so $y_i - (1+\\varepsilon)x_i^{(2)} = \\lambda$. Solving for $x_i^{(2)}$ yields $x_i^{(2)} = \\frac{y_i - \\lambda}{1+\\varepsilon}$. This holds if $y_i - \\lambda > 0 \\iff y_i > \\lambda$.\nCase 2: $x_i^{(2)} < 0$. Then $\\partial|x_i^{(2)}| = \\{-1\\}$, so $y_i - (1+\\varepsilon)x_i^{(2)} = -\\lambda$. Solving for $x_i^{(2)}$ yields $x_i^{(2)} = \\frac{y_i + \\lambda}{1+\\varepsilon}$. This holds if $y_i + \\lambda < 0 \\iff y_i < -\\lambda$.\nCase 3: $x_i^{(2)} = 0$. Then $y_i \\in \\lambda[-1, 1]$, which means $|y_i| \\le \\lambda$.\n\nComparing the solution for $x_i^{(2)}$ with the soft-thresholding operator $S_{\\lambda}(y_i)$, we find:\n$$ x_i^{(2)} = \\frac{1}{1+\\varepsilon} \\begin{cases} y_i - \\lambda & \\text{if } y_i > \\lambda \\\\ 0 & \\text{if } |y_i| \\le \\lambda \\\\ y_i + \\lambda & \\text{if } y_i < -\\lambda \\end{cases} = \\frac{1}{1+\\varepsilon} S_{\\lambda}(y_i) = \\frac{1}{1+\\varepsilon} x_i^{(1)} $$\nThis relationship holds for all components $i=1, \\dots, n$. Therefore, the vector solution is $x^{(2)} = \\frac{1}{1+\\varepsilon}x^{(1)}$.\n\nPart 3: Numerical Calculation\n\nWe are given the data $y=\\big(\\frac{3}{2},-\\frac{1}{3},\\frac{4}{5}\\big)$, $\\lambda=\\frac{1}{2}$, and $\\varepsilon=\\frac{1}{4}$. We need to compute $\\|\\operatorname{prox}_{\\lambda\\|x\\|_1}(y)-\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)\\|_2^2 = \\|x^{(1)} - x^{(2)}\\|_2^2$.\n\nUsing the relationship derived above, $x^{(1)} - x^{(2)} = x^{(1)} - \\frac{1}{1+\\varepsilon}x^{(1)} = \\left(1 - \\frac{1}{1+\\varepsilon}\\right)x^{(1)} = \\frac{\\varepsilon}{1+\\varepsilon}x^{(1)}$.\nThe squared Euclidean distance is then:\n$$ \\|x^{(1)} - x^{(2)}\\|_2^2 = \\left\\| \\frac{\\varepsilon}{1+\\varepsilon}x^{(1)} \\right\\|_2^2 = \\left(\\frac{\\varepsilon}{1+\\varepsilon}\\right)^2 \\|x^{(1)}\\|_2^2 $$\nFirst, we compute the scalar factor with $\\varepsilon = \\frac{1}{4}$:\n$$ \\left(\\frac{\\varepsilon}{1+\\varepsilon}\\right)^2 = \\left(\\frac{\\frac{1}{4}}{1+\\frac{1}{4}}\\right)^2 = \\left(\\frac{\\frac{1}{4}}{\\frac{5}{4}}\\right)^2 = \\left(\\frac{1}{5}\\right)^2 = \\frac{1}{25} $$\nNext, we compute $x^{(1)} = \\operatorname{prox}_{\\lambda\\|x\\|_1}(y)$ with $\\lambda = \\frac{1}{2}$:\nFor $i=1$: $y_1 = \\frac{3}{2}$. Since $y_1 = \\frac{3}{2} > \\lambda = \\frac{1}{2}$, we have $x_1^{(1)} = y_1 - \\lambda = \\frac{3}{2} - \\frac{1}{2} = 1$.\nFor $i=2$: $y_2 = -\\frac{1}{3}$. Since $|y_2| = \\frac{1}{3} \\le \\lambda = \\frac{1}{2}$, we have $x_2^{(1)} = 0$.\nFor $i=3$: $y_3 = \\frac{4}{5}$. Since $y_3 = \\frac{4}{5} > \\lambda = \\frac{1}{2}$ (as $\\frac{4}{5}=0.8$ and $\\frac{1}{2}=0.5$), we have $x_3^{(1)} = y_3 - \\lambda = \\frac{4}{5} - \\frac{1}{2} = \\frac{8-5}{10} = \\frac{3}{10}$.\nSo, $x^{(1)} = \\left(1, 0, \\frac{3}{10}\\right)$.\n\nNow, we compute the squared Euclidean norm of $x^{(1)}$:\n$$ \\|x^{(1)}\\|_2^2 = 1^2 + 0^2 + \\left(\\frac{3}{10}\\right)^2 = 1 + \\frac{9}{100} = \\frac{100}{100} + \\frac{9}{100} = \\frac{109}{100} $$\nFinally, we compute the desired quantity:\n$$ \\|x^{(1)} - x^{(2)}\\|_2^2 = \\left(\\frac{\\varepsilon}{1+\\varepsilon}\\right)^2 \\|x^{(1)}\\|_2^2 = \\frac{1}{25} \\times \\frac{109}{100} = \\frac{109}{2500} $$\nThe result is an exact rational number as requested.",
            "answer": "$$\\boxed{\\frac{109}{2500}}$$"
        },
        {
            "introduction": "Building on our understanding of proximal operators, this exercise explores the proximal map for the infinity norm, which is the dual to the $\\ell_1$ norm. You will first construct a point where the function is non-differentiable to understand the structure of the subdifferential, and then compute the operator itself, revealing how it levels out large values rather than setting them to zero. This practice deepens your intuition about how different regularizers shape solutions and reinforces the connection between a function and its subgradient .",
            "id": "3168264",
            "problem": "Let $f:\\mathbb{R}^{4}\\to\\mathbb{R}$ be the convex function $f(x)=\\|x\\|_{\\infty}$, where for $x=(x_{1},x_{2},x_{3},x_{4})$, $\\|x\\|_{\\infty}=\\max\\{|x_{1}|,|x_{2}|,|x_{3}|,|x_{4}|\\}$. Consider the resolvent of the set-valued subdifferential operator, namely $(I+\\lambda\\,\\partial f)^{-1}$, which appears in the proximal point algorithm for minimizing $f$.\n\n1) Using only the definition of the subdifferential and standard facts about dual norms, construct a concrete point $x\\in\\mathbb{R}^{4}$ at which the subdifferential $\\partial f(x)$ is set-valued (i.e., contains more than one vector), and justify why it is set-valued at that point. Your construction should be explicit and self-contained.\n\n2) With $\\lambda=2$ and $y=(3,\\,2,\\,-1,\\,0.5)$, compute the resolvent value $(I+\\lambda\\,\\partial f)^{-1}(y)$ exactly. Your final numerical answer must be a single $1\\times 4$ row vector. No rounding is required.\n\n3) Briefly interpret your computed point from part 2 in terms of how the proximal mapping for the infinity norm redistributes the influence of the largest-magnitude coordinates. Make your interpretation precise using the language of subgradients and convex combinations.\n\nThe final answer to be reported must be the vector from part 2 only, written exactly (no rounding or units).",
            "solution": "The user wants me to solve a three-part problem related to the subdifferential and proximal operator of the infinity norm.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Function: $f:\\mathbb{R}^{4}\\to\\mathbb{R}$, defined as $f(x)=\\|x\\|_{\\infty}$, where for $x=(x_{1},x_{2},x_{3},x_{4})$, $\\|x\\|_{\\infty}=\\max\\{|x_{1}|,|x_{2}|,|x_{3}|,|x_{4}|\\}$.\n- The function $f$ is stated to be convex.\n- Operator of interest: The resolvent of the subdifferential operator, $(I+\\lambda\\,\\partial f)^{-1}$.\n- Part 1: Construct an explicit point $x\\in\\mathbb{R}^{4}$ where the subdifferential $\\partial f(x)$ is set-valued (contains more than one vector).\n- Part 2: Compute the resolvent value $x_{\\text{out}} = (I+\\lambda\\,\\partial f)^{-1}(y)$ for $\\lambda=2$ and $y=(3,\\,2,\\,-1,\\,0.5)$.\n- Part 3: Provide a brief, precise interpretation of the result from Part 2.\n- The final answer is specified to be only the vector from Part 2.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is well-grounded in the field of convex analysis and optimization. The function $f(x)=\\|x\\|_{\\infty}$ is a standard example of a convex, non-differentiable function. The subdifferential $\\partial f$, the resolvent $(I+\\lambda\\,\\partial f)^{-1}$, and the proximal point algorithm are fundamental, well-defined concepts in this field. The dual of the infinity norm being the L1 norm is a standard result. All premises are factually correct.\n- **Well-Posed**: The problem is well-posed.\n  1. For Part 1, since the infinity norm is non-differentiable at any point where the maximum absolute value is attained by more than one component, such points exist, and constructing one is a standard exercise.\n  2. For Part 2, the resolvent of a maximal monotone operator like $\\partial f$ is a single-valued and well-defined mapping for any positive $\\lambda$. This is equivalent to evaluating the proximal operator of $\\lambda f$, which has a unique solution for a proper, convex, lower semi-continuous function like $f$. The problem provides all necessary data ($\\lambda$, $y$).\n  3. For Part 3, the request for an interpretation is a standard conceptual check, not a question with a non-unique or unstable answer.\n- **Objective**: The problem is stated using precise mathematical language and contains no subjective or ambiguous terms.\n- **Completeness and Consistency**: The problem is self-contained. All necessary definitions and values are provided. There are no contradictions.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is scientifically sound, well-posed, objective, and complete. I will now proceed with the solution.\n\n### Solution\n\n**Part 1: Construction of a point with a set-valued subdifferential**\n\nThe subdifferential of a convex function $f$ at a point $x$ is the set of all subgradients $g$ at that point, defined by the inequality $f(z) \\ge f(x) + g^T(z-x)$ for all $z$.\nFor the specific function $f(x) = \\|x\\|_{\\infty}$, the subdifferential $\\partial f(x)$ is given by the set:\n$$ \\partial f(x) = \\{ g \\in \\mathbb{R}^4 \\mid \\|g\\|_1 \\le 1 \\text{ and } g^T x = \\|x\\|_{\\infty} \\} $$\nwhere $\\|g\\|_1 = \\sum_{i=1}^4 |g_i|$ is the L1 norm, which is the dual norm to the infinity norm.\nThe subdifferential $\\partial f(x)$ is set-valued (contains more than one vector) at points where $f$ is not differentiable. For the infinity norm, this occurs when the maximum of the absolute values of the components is achieved at more than one index.\n\nLet's choose a point $x \\in \\mathbb{R}^4$ where this condition holds. Consider the point $x = (5, -5, 1, 2)$.\nFor this point, $\\|x\\|_{\\infty} = \\max\\{|5|, |-5|, |1|, |2|\\} = 5$. The maximum magnitude is achieved by two components, $x_1$ and $x_2$.\n\nNow, we must find at least two distinct vectors $g^{(1)}$ and $g^{(2)}$ that satisfy the conditions to be in $\\partial f(x)$. The conditions are:\n1. $\\|g\\|_1 \\le 1$\n2. $g^T x = g_1(5) + g_2(-5) + g_3(1) + g_4(2) = 5$\n\nLet's test the vector $g^{(1)} = (1, 0, 0, 0)$.\n1. $\\|g^{(1)}\\|_1 = |1| + |0| + |0| + |0| = 1$. The condition $\\|g^{(1)}\\|_1 \\le 1$ is satisfied.\n2. $(g^{(1)})^T x = (1)(5) + (0)(-5) + (0)(1) + (0)(2) = 5$. This matches $\\|x\\|_{\\infty} = 5$.\nSo, $g^{(1)} = (1, 0, 0, 0)$ is a valid subgradient in $\\partial f(x)$.\n\nNext, let's test the vector $g^{(2)} = (0, -1, 0, 0)$.\n1. $\\|g^{(2)}\\|_1 = |0| + |-1| + |0| + |0| = 1$. The condition $\\|g^{(2)}\\|_1 \\le 1$ is satisfied.\n2. $(g^{(2)})^T x = (0)(5) + (-1)(-5) + (0)(1) + (0)(2) = 5$. This also matches $\\|x\\|_{\\infty} = 5$.\nSo, $g^{(2)} = (0, -1, 0, 0)$ is another valid subgradient in $\\partial f(x)$.\n\nSince $g^{(1)} \\neq g^{(2)}$, we have shown that the subdifferential $\\partial f(x)$ at $x = (5, -5, 1, 2)$ contains at least two distinct vectors. Therefore, it is set-valued at this point. In fact, $\\partial f(x)$ is the convex hull of such \"elementary\" subgradients. For this $x$, $\\partial f(x) = \\text{conv}\\{(1,0,0,0), (0,-1,0,0)\\}$.\n\n**Part 2: Computation of the resolvent**\n\nWe need to compute $x_{\\text{out}} = (I + \\lambda \\partial f)^{-1}(y)$ with $\\lambda = 2$ and $y = (3, 2, -1, 0.5)$.\nThis is equivalent to finding the unique point $x_{\\text{out}}$ that satisfies the relation $y \\in x_{\\text{out}} + \\lambda \\partial f(x_{\\text{out}})$. This can be rearranged to $\\frac{y - x_{\\text{out}}}{\\lambda} \\in \\partial f(x_{\\text{out}})$.\nThis definition corresponds to the proximal operator. We seek to find:\n$$ x_{\\text{out}} = \\text{prox}_{\\lambda f}(y) = \\arg\\min_{x \\in \\mathbb{R}^4} \\left\\{ \\lambda f(x) + \\frac{1}{2} \\|x - y\\|_2^2 \\right\\} $$\nSubstituting the given values and the definition of $f$:\n$$ x_{\\text{out}} = \\arg\\min_{x \\in \\mathbb{R}^4} \\left\\{ 2 \\|x\\|_{\\infty} + \\frac{1}{2} \\|x - (3, 2, -1, 0.5)\\|_2^2 \\right\\} $$\nLet $\\alpha = \\|x\\|_{\\infty}$. The optimization problem can be formulated as minimizing $2\\alpha + \\frac{1}{2} \\sum_{i=1}^4 (x_i - y_i)^2$ subject to the constraint $|x_i| \\le \\alpha$ for all $i=1, \\dots, 4$.\nFor a fixed value of $\\alpha \\ge 0$, we first minimize with respect to $x$. The problem decouples for each component $x_i$:\n$$ \\min_{x_i} \\frac{1}{2} (x_i - y_i)^2 \\quad \\text{subject to} \\quad -\\alpha \\le x_i \\le \\alpha $$\nThe solution is the projection of $y_i$ onto the interval $[-\\alpha, \\alpha]$, which is given by $x_i(\\alpha) = \\text{sign}(y_i) \\min\\{|y_i|, \\alpha\\}$.\nSubstituting this back into the objective function gives a problem in terms of $\\alpha$ alone:\n$$ \\min_{\\alpha \\ge 0} H(\\alpha) = \\min_{\\alpha \\ge 0} \\left\\{ 2\\alpha + \\frac{1}{2} \\sum_{i=1}^4 \\left( \\text{sign}(y_i) \\min\\{|y_i|, \\alpha\\} - y_i \\right)^2 \\right\\} $$\nThe term inside the sum is zero if $|y_i| \\le \\alpha$, and it is $(\\text{sign}(y_i)\\alpha - \\text{sign}(y_i)|y_i|)^2 = (\\alpha - |y_i|)^2$ if $|y_i| > \\alpha$. So, we have:\n$$ H(\\alpha) = 2\\alpha + \\frac{1}{2} \\sum_{i \\text{ s.t. } |y_i| > \\alpha} (|y_i| - \\alpha)^2 $$\nTo find the minimum of this convex function $H(\\alpha)$, we can set its derivative with respect to $\\alpha$ to zero. The derivative is:\n$$ H'(\\alpha) = 2 + \\sum_{i \\text{ s.t. } |y_i| > \\alpha} \\frac{1}{2} \\cdot 2(|y_i| - \\alpha) \\cdot (-1) = 2 - \\sum_{i \\text{ s.t. } |y_i| > \\alpha} (|y_i| - \\alpha) $$\nThe absolute values of the components of $y=(3, 2, -1, 0.5)$ are $\\{3, 2, 1, 0.5\\}$. Let's test intervals for the optimal $\\alpha^*$.\n- If $1 \\le \\alpha < 2$: The components with magnitude greater than $\\alpha$ are $\\{3, 2\\}$.\n  $H'(\\alpha) = 2 - ((3-\\alpha) + (2-\\alpha)) = 2 - (5 - 2\\alpha) = 2\\alpha - 3$.\n  Setting $H'(\\alpha) = 0$ gives $2\\alpha - 3 = 0$, which yields $\\alpha = \\frac{3}{2} = 1.5$.\n  Since $1.5 \\in [1, 2)$, this is our optimal value, $\\alpha^* = 1.5$.\n\nNow we compute the components of the solution vector $x_{\\text{out}}$ using $\\alpha^* = 1.5$:\n$x_{\\text{out}, i} = \\text{sign}(y_i) \\min\\{|y_i|, 1.5\\}$.\n- $y_1=3$: $x_{\\text{out}, 1} = \\text{sign}(3) \\min\\{3, 1.5\\} = 1 \\cdot 1.5 = 1.5 = \\frac{3}{2}$.\n- $y_2=2$: $x_{\\text{out}, 2} = \\text{sign}(2) \\min\\{2, 1.5\\} = 1 \\cdot 1.5 = 1.5 = \\frac{3}{2}$.\n- $y_3=-1$: $x_{\\text{out}, 3} = \\text{sign}(-1) \\min\\{|-1|, 1.5\\} = -1 \\cdot 1 = -1$.\n- $y_4=0.5$: $x_{\\text{out}, 4} = \\text{sign}(0.5) \\min\\{0.5, 1.5\\} = 1 \\cdot 0.5 = 0.5 = \\frac{1}{2}$.\nSo, the computed resolvent value is $x_{\\text{out}} = (1.5, 1.5, -1, 0.5) = (\\frac{3}{2}, \\frac{3}{2}, -1, \\frac{1}{2})$.\n\n**Part 3: Interpretation of the result**\n\nThe proximal operator $x_{\\text{out}} = \\text{prox}_{\\lambda f}(y)$ finds a point $x$ that balances two objectives: being close to $y$ (minimizing $\\frac{1}{2}\\|x-y\\|_2^2$) and having a small infinity norm (minimizing $\\lambda \\|x\\|_\\infty$).\nThe solution $x_{\\text{out}} = (1.5, 1.5, -1, 0.5)$ reveals a \"shrinking\" mechanism characteristic of the infinity norm's proximal map.\nLet the threshold be $\\alpha^* = \\|x_{\\text{out}}\\|_{\\infty} = 1.5$.\n1. For components of $y$ with magnitude less than or equal to this threshold ($|y_3|=1 < 1.5$ and $|y_4|=0.5 < 1.5$), the corresponding components of $x_{\\text{out}}$ are left unchanged: $x_{\\text{out},3} = y_3$ and $x_{\\text{out},4} = y_4$. Minimizing proximity to $y$ dominates for these components as they do not contribute to the final infinity norm.\n2. For components of $y$ with magnitude greater than the threshold ($|y_1|=3 > 1.5$ and $|y_2|=2 > 1.5$), the corresponding components of $x_{\\text{out}}$ are \"shrunk\" down to the threshold level while preserving their sign. Thus, $x_{\\text{out},1} = 1.5$ and $x_{\\text{out},2} = 1.5$. These components now form the active set of $x_{\\text{out}}$, i.e., the set of indices $i$ for which $|x_i| = \\|x\\|_{\\infty}$.\n\nThis behavior is dictated by the subgradient condition. The vector $g = \\frac{y-x_{\\text{out}}}{\\lambda} = \\frac{(3-1.5, 2-1.5, -1-(-1), 0.5-0.5)}{2} = (0.75, 0.25, 0, 0)$ must be a subgradient of $f$ at $x_{\\text{out}}$. The subdifferential $\\partial f(x_{\\text{out}})$ is the convex hull of the elementary subgradients corresponding to the active set, which are $e_1=(1,0,0,0)$ and $e_2=(0,1,0,0)$. The computed subgradient is $g = 0.75 e_1 + 0.25 e_2$, a convex combination. This shows that the influence of the original largest components ($y_1, y_2$) has been redistributed and leveled. The total amount of magnitude reduction, $\\sum_{i \\in \\{1,2\\}} (|y_i| - \\alpha^*) = (3-1.5) + (2-1.5) = 1.5 + 0.5 = 2$, is exactly equal to the parameter $\\lambda$. The proximal operator effectively uses the \"budget\" $\\lambda$ to shrink the largest components until the total shrinkage cost equals $\\lambda$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} \\frac{3}{2} & \\frac{3}{2} & -1 & \\frac{1}{2} \\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The power of the Proximal Point Algorithm extends beyond the realm of convex optimization. This practice ventures into the nonconvex world, introducing the crucial concept of prox-regularity, a condition that ensures local convergence even when global convexity is absent. By analyzing a specific nonconvex function, you will determine the step-size constraints that make the algorithm well-defined and implement it to observe convergence to a local minimum, a common objective in many advanced engineering and machine learning applications .",
            "id": "3168250",
            "problem": "You are to work with the Proximal Point Algorithm (PPA) in one dimension and demonstrate local convergence on a nonconvex yet prox-regular objective. Begin from the following fundamental bases: the definition of the proximal mapping and its optimality condition, the definition of prox-regularity, and the elementary properties of twice continuously differentiable functions. Consider the explicit function\n$$\nf(x) = (x^2 - 1)^2 + \\mu x\n$$\nwith parameter $\\mu = 0.2$. This function is nonconvex but twice continuously differentiable.\n\nTasks:\n1) Prove that $f$ is prox-regular on $\\mathbb{R}$. You may proceed from the following base fact: every function that is twice continuously differentiable with locally Lipschitz continuous gradient is prox-regular at each point. Show that $f$ is twice continuously differentiable, compute $f''(x)$, and argue that $f''(x) \\ge -m$ for some finite $m > 0$ on $\\mathbb{R}$. Use this to deduce that there exists a step size $t > 0$ such that for every $x \\in \\mathbb{R}$ the proximal subproblem of minimizing in $y$ the function\n$$\n\\phi_x(y) = f(y) + \\frac{1}{2t}\\|y - x\\|^2\n$$\nis strongly convex, hence has a unique minimizer. Do not use any shortcut formulas beyond these core definitions and facts; derive the bound on $t$ from first principles.\n\n2) Using prox-regularity and the strong convexity of the proximal subproblem for sufficiently small $t$, demonstrate local convergence of the Proximal Point Algorithm (PPA) $x_{k+1} \\in \\operatorname{argmin}_{y \\in \\mathbb{R}} \\left\\{ f(y) + \\frac{1}{2t}\\|y - x_k\\|^2 \\right\\}$ to a critical point $x^\\star$ satisfying $f'(x^\\star) = 0$ and $f''(x^\\star) > 0$. Your derivation should start from the optimality condition of the proximal step,\n$$\n0 = f'(x_{k+1}) + \\frac{1}{t}(x_{k+1} - x_k),\n$$\nand apply principle-based reasoning (for example, the implicit function theorem) to show that, for $t$ small enough, the mapping $T(x) = \\operatorname{prox}_{t f}(x)$ is single-valued and locally contractive around $x^\\star$, ensuring local convergence for starting points sufficiently close to $x^\\star$.\n\n3) Provide a numerical illustration by implementing the Proximal Point Algorithm for the given $f$. In one dimension, each proximal step amounts to minimizing $\\phi_x(y)$. Implement this by solving the scalar optimality equation\n$$\nf'(y) + \\frac{1}{t}(y - x) = 0\n$$\nfor $y$ at each iteration, using a robust root-finding method that does not rely on external input. Use a fixed iteration cap and a stopping tolerance on consecutive iterates to terminate.\n\nCompute the real critical points of $f$ by solving $f'(x) = 0$ and classify them by $f''(x)$. For each test case, run the PPA and report the distance from the final iterate to the nearest critical point.\n\nTest suite:\n- Case 1 (happy path): $t = 0.2$, $x_0 = -2.0$, maximum iterations $N = 100$, stopping tolerance $\\varepsilon = 10^{-10}$.\n- Case 2 (boundary condition): $t = 0.25$, $x_0 = 0.0$, maximum iterations $N = 100$, stopping tolerance $\\varepsilon = 10^{-10}$.\n- Case 3 (strongly damped): $t = 0.1$, $x_0 = 2.0$, maximum iterations $N = 100$, stopping tolerance $\\varepsilon = 10^{-10}$.\n- Case 4 (near the basin boundary): $t = 0.05$, $x_0 = 0.5$, maximum iterations $N = 100$, stopping tolerance $\\varepsilon = 10^{-10}$.\n\nFinal output specification:\nYour program should produce a single line of output containing the distances for the four test cases as a comma-separated list enclosed in square brackets (e.g., `[d_1,d_2,d_3,d_4]`), where each $d_i$ is the final distance to the nearest critical point as a real number.",
            "solution": "The problem asks for a theoretical analysis and numerical implementation of the Proximal Point Algorithm (PPA) for a specific one-dimensional nonconvex function, $f(x) = (x^2 - 1)^2 + \\mu x$ with $\\mu = 0.2$. The analysis proceeds in three parts as requested.\n\n**Part 1: Prox-Regularity and Step Size Condition**\n\nFirst, we analyze the properties of the objective function $f(x) = (x^2 - 1)^2 + 0.2x$. This can be expanded to $f(x) = x^4 - 2x^2 + 1 + 0.2x$. As a polynomial, $f(x)$ is infinitely differentiable on $\\mathbb{R}$, so it is at least twice continuously differentiable ($C^2$).\n\nThe first and second derivatives of $f(x)$ are:\n$$\nf'(x) = \\frac{d}{dx} \\left( x^4 - 2x^2 + 0.2x + 1 \\right) = 4x^3 - 4x + 0.2\n$$\n$$\nf''(x) = \\frac{d}{dx} \\left( 4x^3 - 4x + 0.2 \\right) = 12x^2 - 4\n$$\nThe problem states a base fact that a $C^2$ function with a locally Lipschitz continuous gradient is prox-regular. A function's gradient is locally Lipschitz if its second derivative is continuous (in one dimension, if $f''(x)$ is continuous), which is true here since $f''(x)$ is a polynomial. Therefore, $f(x)$ is prox-regular on all of $\\mathbb{R}$.\n\nTo ensure that the proximal subproblem has a unique solution, we need to find a condition on the step size $t$ such that the auxiliary function $\\phi_x(y)$ is strongly convex. The function is defined as:\n$$\n\\phi_x(y) = f(y) + \\frac{1}{2t}\\|y - x\\|^2 = f(y) + \\frac{1}{2t}(y - x)^2\n$$\nA function is strongly convex if its second derivative is strictly positive and bounded below by a positive constant. The second derivative of $\\phi_x(y)$ with respect to $y$ is:\n$$\n\\phi_x''(y) = \\frac{d^2}{dy^2} \\left( f(y) + \\frac{1}{2t}(y - x)^2 \\right) = f''(y) + \\frac{1}{t}\n$$\nFor $\\phi_x(y)$ to be strongly convex for all $y \\in \\mathbb{R}$, we require $\\phi_x''(y) > 0$ for all $y$. This must hold even for the value of $y$ that minimizes $f''(y)$. We have $f''(y) = 12y^2 - 4$. This is a parabola opening upwards with a minimum at $y=0$. The minimum value is:\n$$\n\\inf_{y \\in \\mathbb{R}} f''(y) = f''(0) = 12(0)^2 - 4 = -4\n$$\nThus, we have established that $f''(y) \\ge -m$ for $m=4$. The condition for strong convexity of $\\phi_x(y)$ becomes:\n$$\n\\inf_{y \\in \\mathbb{R}} \\phi_x''(y) = \\left( \\inf_{y \\in \\mathbb{R}} f''(y) \\right) + \\frac{1}{t} > 0\n$$\n$$\n-4 + \\frac{1}{t} > 0\n$$\n$$\n\\frac{1}{t} > 4 \\implies t < \\frac{1}{4}\n$$\nTherefore, for any step size $t \\in (0, 0.25)$, the proximal subproblem is strongly convex on $\\mathbb{R}$, which guarantees that the proximal operator $\\operatorname{prox}_{tf}(x)$ returns a unique minimizer for any $x \\in \\mathbb{R}$.\n\n**Part 2: Local Convergence of the Proximal Point Algorithm**\n\nThe Proximal Point Algorithm generates a sequence $\\{x_k\\}$ via the update rule:\n$$\nx_{k+1} = \\operatorname{prox}_{tf}(x_k) = \\operatorname{argmin}_{y \\in \\mathbb{R}} \\left\\{ f(y) + \\frac{1}{2t}(y - x_k)^2 \\right\\}\n$$\nThe first-order optimality condition for the unique minimizer $x_{k+1}$ is that the gradient of the subproblem objective at $x_{k+1}$ is zero:\n$$\nf'(x_{k+1}) + \\frac{1}{t}(x_{k+1} - x_k) = 0\n$$\nWe define the iteration map as $T(x) = \\operatorname{prox}_{tf}(x)$, so that $x_{k+1} = T(x_k)$. The fixed points $x^\\star$ of this map satisfy $x^\\star = T(x^\\star)$. Substituting this into the optimality condition gives:\n$$\nf'(x^\\star) + \\frac{1}{t}(x^\\star - x^\\star) = 0 \\implies f'(x^\\star) = 0\n$$\nThus, the fixed points of the PPA are precisely the critical points of $f(x)$.\n\nTo demonstrate local convergence to a fixed point $x^\\star$, we can analyze the iteration map $T(x)$ using a Taylor expansion around $x^\\star$, which is equivalent to studying its derivative, $T'(x^\\star)$. Rearranging the optimality condition, we have an implicit definition for $T(x)$:\n$$\nx = T(x) + t f'(T(x))\n$$\nWe differentiate this identity with respect to $x$ using the chain rule:\n$$\n1 = T'(x) + t f''(T(x)) \\cdot T'(x)\n$$\n$$\n1 = T'(x) \\left( 1 + t f''(T(x)) \\right)\n$$\nSolving for $T'(x)$ yields:\n$$\nT'(x) = \\frac{1}{1 + t f''(T(x))}\n$$\nLocal convergence to a fixed point $x^\\star$ is guaranteed if the map $T$ is a contraction in a neighborhood of $x^\\star$, which requires $|T'(x^\\star)| < 1$. We are interested in convergence to a critical point $x^\\star$ that is a local minimum, which means it must satisfy $f''(x^\\star) > 0$.\n\nEvaluating the derivative at such a point $x^\\star$:\n$$\nT'(x^\\star) = \\frac{1}{1 + t f''(x^\\star)}\n$$\nSince $t > 0$ and we assume $f''(x^\\star) > 0$, the denominator $1 + t f''(x^\\star)$ is strictly greater than $1$. This implies:\n$$\n0 < T'(x^\\star) < 1\n$$\nBecause $|T'(x^\\star)| < 1$, the map is locally contractive around any stable critical point $x^\\star$. By the Banach fixed-point theorem (or more generally, Ostrowski's theorem on the convergence of iterations), for any starting point $x_0$ sufficiently close to $x^\\star$, the sequence $x_k$ generated by the PPA will converge to $x^\\star$. This establishes the local convergence of the PPA to the local minima of $f(x)$.\n\n**Part 3: Numerical Illustration**\n\nThe numerical implementation requires finding the critical points of $f(x)$ and then executing the PPA for the given test cases.\n\nFirst, we find the critical points by solving $f'(x) = 0$:\n$$\n4x^3 - 4x + 0.2 = 0\n$$\nThis is a cubic equation. Numerically solving for the roots gives three real critical points:\n$x^\\star_1 \\approx -1.02435866$, $x^\\star_2 \\approx 0.05030845$, $x^\\star_3 \\approx 0.97405021$.\n\nNext, we classify these points using the second derivative, $f''(x) = 12x^2 - 4$:\n- For $x^\\star_1 \\approx -1.02435866$: $f''(x^\\star_1) = 12(-1.02435866)^2 - 4 \\approx 8.5866 > 0$. This is a local minimum.\n- For $x^\\star_2 \\approx 0.05030845$: $f''(x^\\star_2) = 12(0.05030845)^2 - 4 \\approx -3.9696 < 0$. This is a local maximum.\n- For $x^\\star_3 \\approx 0.97405021$: $f''(x^\\star_3) = 12(0.97405021)^2 - 4 \\approx 7.3980 > 0$. This is a local minimum.\n\nThe PPA iteration requires solving for $y$ in $f'(y) + \\frac{1}{t}(y - x_k) = 0$ at each step $k$. This is equivalent to finding the root of the function:\n$$\ng(y; x_k, t) = 4y^3 + \\left(\\frac{1}{t} - 4\\right)y + \\left(0.2 - \\frac{x_k}{t}\\right) = 0\n$$\nAs shown in Part 1, for $t < 0.25$, the function underlying this equation is strictly increasing, guaranteeing a unique root. This root can be found efficiently using a numerical method like Newton's method, for which the derivative is $g'(y; t) = 12y^2 + \\frac{1}{t} - 4$. The algorithm proceeds by iterating this root-finding step until the distance between successive iterates, $|x_{k+1} - x_k|$, is below a tolerance $\\varepsilon = 10^{-10}$ or a maximum number of iterations $N=100$ is reached. The final step is to compute the distance from the final iterate to the nearest of the three critical points. The Python code in the final answer implements this procedure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import newton\n\ndef solve():\n    \"\"\"\n    Solves the proximal point algorithm problem for the given function and test cases.\n    \"\"\"\n    mu = 0.2\n\n    # Find and classify critical points of f(x)\n    # f'(x) = 4x^3 - 4x + mu = 0\n    coeffs = [4, 0, -4, mu]\n    critical_points = np.roots(coeffs)\n    critical_points.sort()\n\n    test_cases = [\n        {'t': 0.2, 'x0': -2.0, 'N': 100, 'eps': 1e-10}, # Case 1\n        {'t': 0.25, 'x0': 0.0, 'N': 100, 'eps': 1e-10}, # Case 2\n        {'t': 0.1, 'x0': 2.0, 'N': 100, 'eps': 1e-10},  # Case 3\n        {'t': 0.05, 'x0': 0.5, 'N': 100, 'eps': 1e-10}, # Case 4\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        t = case['t']\n        x_k = case['x0']\n        N = case['N']\n        eps = case['eps']\n\n        for _ in range(N):\n            x_prev = x_k\n            \n            # For t=0.25, a special case where the equation has an analytical solution.\n            if t == 0.25:\n                # 4y^3 + (4-4)y + (mu - 4*x_prev) = 0\n                # 4y^3 = 4*x_prev - mu\n                # y^3 = x_prev - mu/4\n                # y = (x_prev - 0.05)^(1/3)\n                arg = x_prev - mu / 4.0\n                x_k = np.sign(arg) * np.power(np.abs(arg), 1.0/3.0)\n            else:\n                # Solve g(y) = f'(y) + (1/t)*(y - x_k) = 0 for y\n                # g(y) = 4y^3 + (1/t - 4)y + (mu - x_prev/t)\n                def g(y, x_k_val, t_val, mu_val):\n                    return 4 * y**3 + (1/t_val - 4) * y + (mu_val - x_k_val / t_val)\n\n                # Derivative of g(y) for Newton's method\n                # g'(y) = 12y^2 + (1/t - 4)\n                def g_prime(y, x_k_val, t_val, mu_val):\n                    return 12 * y**2 + (1/t_val - 4)\n                \n                # Use Newton's method to find the root y, which is x_{k+1}\n                # Use x_prev as the initial guess for the root.\n                x_k = newton(g, x_prev, fprime=g_prime, args=(x_prev, t, mu))\n            \n            if np.abs(x_k - x_prev) < eps:\n                break\n        \n        # Calculate the distance to the nearest critical point\n        distance = np.min(np.abs(x_k - critical_points))\n        results.append(distance)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}