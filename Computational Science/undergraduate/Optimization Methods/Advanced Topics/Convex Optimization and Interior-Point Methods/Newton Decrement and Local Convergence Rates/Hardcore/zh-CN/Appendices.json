{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握牛顿递减量，一个有效的方法是在一个简单、可控的环境中对其进行剖析。本练习采用一个可分离的二次函数，让我们可以清晰地看到每个变量对总体递减量的贡献。通过这个练习，你将阐明函数局部曲率与不同方向收敛速度之间的关系。",
            "id": "3156868",
            "problem": "考虑在 $\\mathbb{R}^{n}$ 中一个可分离二次函数的无约束最小化问题，该函数由 $f(x) = \\sum_{i=1}^{n} \\tfrac{1}{2} q_{i} x_{i}^{2} - b_{i} x_{i}$ 给出，其中对于所有 $i \\in \\{1,\\dots,n\\}$，$q_{i} \\in \\mathbb{R}$ 满足 $q_{i}  0$ 且 $b_{i} \\in \\mathbb{R}$。令 $\\nabla f(x)$ 表示梯度，$\\nabla^{2} f(x)$ 表示Hessian矩阵。牛顿减量 $\\lambda(x)$ 由基本表达式 $\\lambda(x)^{2} = \\nabla f(x)^{\\mathsf{T}} \\left[\\nabla^{2} f(x)\\right]^{-1} \\nabla f(x)$ 定义。\n\n任务 A. 从给定函数 $f(x)$ 的梯度 $\\nabla f(x)$ 和Hessian矩阵 $\\nabla^{2} f(x)$ 的定义出发，推导 $\\lambda(x)^{2}$ 的一个分量表达式，该表达式能揭示每个坐标对牛顿减量的贡献。\n\n任务 B. 考虑一个源于标准信赖域代理的阻尼牛顿步，其中搜索方向 $p(x)$ 通过求解 $\\left(\\nabla^{2} f(x) + \\mu I\\right) p(x) = -\\nabla f(x)$ 来定义，其中 $\\mu  0$ 是一个固定的阻尼参数，更新规则为 $x^{+} = x + p(x)$。令 $x^{\\star}$ 表示 $f(x)$ 的唯一极小值点。表达误差 $e = x - x^{\\star}$，并根据 $e$、$q_{i}$ 和 $\\mu$ 推导出按坐标的误差更新 $e^{+}$。然后，根据 $q_{i}$ 和 $q_{j}$，陈述在此阻尼牛顿更新下，坐标 $i$ 比坐标 $j$ 表现出严格更快的局部线性收敛率的精确条件。\n\n任务 C. 对于 $n = 3$，$q = (2, \\tfrac{1}{2}, 8)$，$b = (1, -2, 4)$ 和 $x = (0.2, -3, 0.5)$ 的具体实例，计算牛顿减量 $\\lambda(x)$。将您的答案四舍五入到四位有效数字。以无单位的单个数字形式提供最终答案。",
            "solution": "问题陈述经核实具有科学依据、问题适定、客观且完整。这是数值优化领域的一个标准问题。\n\n该问题由三个任务组成。我们将按顺序解决每个任务。\n\n**任务 A：牛顿减量的分量表达式**\n\n目标函数是 $\\mathbb{R}^{n}$ 上的一个可分离二次函数：\n$$f(x) = \\sum_{i=1}^{n} \\left(\\frac{1}{2} q_{i} x_{i}^{2} - b_{i} x_{i}\\right)$$\n其中对于所有 $i \\in \\{1,\\dots,n\\}$，$q_{i}  0$。\n\n首先，我们计算梯度向量 $\\nabla f(x)$。梯度的第 $i$ 个分量 $(\\nabla f(x))_{i}$ 是 $f(x)$ 关于 $x_i$ 的偏导数：\n$$(\\nabla f(x))_{i} = \\frac{\\partial f}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left(\\frac{1}{2} q_{i} x_{i}^{2} - b_{i} x_{i}\\right) = q_i x_i - b_i$$\n因此，梯度向量为 $\\nabla f(x) = [q_1 x_1 - b_1, \\dots, q_n x_n - b_n]^{\\mathsf{T}}$。\n\n接下来，我们计算Hessian矩阵 $\\nabla^{2} f(x)$。第 $j$ 行第 $k$ 列的元素 $(\\nabla^{2} f(x))_{jk}$ 是二阶偏导数 $\\frac{\\partial^2 f}{\\partial x_j \\partial x_k}$。\n对于 $j \\neq k$，导数是关于不同变量的。由于函数是可分离的，我们有：\n$$(\\nabla^{2} f(x))_{jk} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_k} = \\frac{\\partial}{\\partial x_j} (q_k x_k - b_k) = 0$$\n对于 $j = k$，我们有：\n$$(\\nabla^{2} f(x))_{jj} = \\frac{\\partial^2 f}{\\partial x_j^2} = \\frac{\\partial}{\\partial x_j} (q_j x_j - b_j) = q_j$$\n因此，Hessian矩阵是一个常对角矩阵，我们可以用 $Q$ 表示：\n$$\\nabla^{2} f(x) = Q = \\text{diag}(q_1, q_2, \\dots, q_n)$$\n因为所有 $q_i  0$，Hessian矩阵 $Q$ 是正定的。\n\nHessian矩阵的逆 $[\\nabla^{2} f(x)]^{-1} = Q^{-1}$ 也是一个对角矩阵，其对角线元素是 $Q$ 元素的倒数：\n$$[\\nabla^{2} f(x)]^{-1} = \\text{diag}\\left(\\frac{1}{q_1}, \\frac{1}{q_2}, \\dots, \\frac{1}{q_n}\\right)$$\n\n现在，我们将这些表达式代入牛顿减量平方 $\\lambda(x)^{2}$ 的定义中：\n$$\\lambda(x)^{2} = \\nabla f(x)^{\\mathsf{T}} \\left[\\nabla^{2} f(x)\\right]^{-1} \\nabla f(x)$$\n这是一个二次型。由于矩阵 $[\\nabla^{2} f(x)]^{-1}$ 是对角矩阵，该表达式简化为一个和式：\n$$\\lambda(x)^{2} = \\sum_{i=1}^{n} (\\nabla f(x))_{i} \\left([\\nabla^{2} f(x)]^{-1}\\right)_{ii} (\\nabla f(x))_{i}$$\n$$\\lambda(x)^{2} = \\sum_{i=1}^{n} (q_i x_i - b_i) \\left(\\frac{1}{q_i}\\right) (q_i x_i - b_i)$$\n这就得出了 $\\lambda(x)^{2}$ 的分量表达式：\n$$\\lambda(x)^{2} = \\sum_{i=1}^{n} \\frac{(q_i x_i - b_i)^2}{q_i}$$\n该表达式表明，总的牛顿减量平方是来自每个坐标贡献的总和。\n\n**任务 B：误差更新与收敛率比较**\n\n首先，我们求 $f(x)$ 的唯一极小值点 $x^{\\star}$。由于 $f(x)$ 是严格凸的（$q_i  0$），通过将梯度设为零来找到极小值点：\n$$\\nabla f(x^{\\star}) = 0 \\implies q_i x_i^{\\star} - b_i = 0 \\text{ for all } i$$\n解出 $x_i^{\\star}$ 得 $x_i^{\\star} = \\frac{b_i}{q_i}$。\n\n阻尼牛顿步定义为 $x^{+} = x + p(x)$，其中 $p(x)$ 是以下方程的解：\n$$\\left(\\nabla^{2} f(x) + \\mu I\\right) p(x) = -\\nabla f(x)$$\n代入梯度和Hessian矩阵的表达式：\n$$\\left(\\text{diag}(q_1, \\dots, q_n) + \\mu \\text{diag}(1, \\dots, 1)\\right) p(x) = -[q_1 x_1 - b_1, \\dots, q_n x_n - b_n]^{\\mathsf{T}}$$\n$$\\text{diag}(q_1+\\mu, \\dots, q_n+\\mu) p(x) = -[q_1 x_1 - b_1, \\dots, q_n x_n - b_n]^{\\mathsf{T}}$$\n这个对角系统可以按分量求解 $p_i(x)$：\n$$(q_i+\\mu) p_i(x) = -(q_i x_i - b_i) \\implies p_i(x) = -\\frac{q_i x_i - b_i}{q_i + \\mu}$$\n第 $i$ 个坐标的更新为 $x_i^{+} = x_i + p_i(x) = x_i - \\frac{q_i x_i - b_i}{q_i + \\mu}$。\n\n坐标 $i$ 的误差为 $e_i = x_i - x_i^{\\star}$。更新后的误差为 $e_i^{+} = x_i^{+} - x_i^{\\star}$。\n我们用 $e_i$ 来表示 $e_i^{+}$：\n$$e_i^{+} = \\left(x_i - \\frac{q_i x_i - b_i}{q_i + \\mu}\\right) - x_i^{\\star}$$\n对 $x_i$ 的项进行重排：\n$$e_i^{+} = \\frac{(q_i+\\mu)x_i - (q_i x_i - b_i)}{q_i+\\mu} - x_i^{\\star} = \\frac{\\mu x_i + b_i}{q_i + \\mu} - x_i^{\\star}$$\n代入 $x_i = e_i + x_i^{\\star}$：\n$$e_i^{+} = \\frac{\\mu (e_i + x_i^{\\star}) + b_i}{q_i + \\mu} - x_i^{\\star}$$\n$$e_i^{+} = \\frac{\\mu e_i}{q_i + \\mu} + \\frac{\\mu x_i^{\\star} + b_i}{q_i + \\mu} - x_i^{\\star}$$\n使用 $x_i^{\\star} = b_i/q_i$，第二项变为：\n$$\\frac{\\mu (b_i/q_i) + b_i}{q_i + \\mu} = \\frac{b_i (\\mu/q_i + 1)}{q_i + \\mu} = \\frac{b_i (\\frac{\\mu+q_i}{q_i})}{q_i + \\mu} = \\frac{b_i}{q_i} = x_i^{\\star}$$\n将此结果代回 $e_i^{+}$ 的表达式中：\n$$e_i^{+} = \\frac{\\mu e_i}{q_i + \\mu} + x_i^{\\star} - x_i^{\\star} = \\frac{\\mu}{q_i + \\mu} e_i$$\n这就是按坐标的误差更新：$e_i^{+} = \\left(\\frac{\\mu}{q_i + \\mu}\\right) e_i$。\n\n第 $i$ 个坐标的局部线性收敛率由因子 $\\rho_i = \\left|\\frac{\\mu}{q_i+\\mu}\\right|$ 控制。因为 $\\mu  0$ 且 $q_i  0$，我们有 $0  \\mu  q_i + \\mu$，这意味着 $0  \\rho_i  1$。所以，$\\rho_i = \\frac{\\mu}{q_i+\\mu}$。\n更快的收敛率对应于更小的收敛因子 $\\rho$。如果 $\\rho_i  \\rho_j$，则坐标 $i$ 的收敛严格快于坐标 $j$。\n$$\\frac{\\mu}{q_i + \\mu}  \\frac{\\mu}{q_j + \\mu}$$\n因为 $\\mu  0$，我们可以将两边同除以 $\\mu$：\n$$\\frac{1}{q_i + \\mu}  \\frac{1}{q_j + \\mu}$$\n因为两个分母都为正，我们可以对两边取倒数并反转不等号：\n$$q_i + \\mu > q_j + \\mu$$\n两边减去 $\\mu$ 得到条件：\n$$q_i > q_j$$\n因此，坐标 $i$ 表现出比坐标 $j$ 严格更快的局部线性收敛率，当且仅当其对应的二次项系数 $q_i$ 严格大于 $q_j$。\n\n**任务 C：数值计算**\n\n给定 $n=3$ 的具体实例及：\n- $q = (q_1, q_2, q_3) = (2, \\frac{1}{2}, 8)$\n- $b = (b_1, b_2, b_3) = (1, -2, 4)$\n- $x = (x_1, x_2, x_3) = (0.2, -3, 0.5)$\n\n我们使用任务 A 中推导出的公式：$\\lambda(x)^2 = \\sum_{i=1}^{3} \\frac{(q_i x_i - b_i)^2}{q_i}$。\n\n我们计算每个坐标的贡献：\n- 对于 $i=1$：\n$$\\frac{(q_1 x_1 - b_1)^2}{q_1} = \\frac{(2 \\times 0.2 - 1)^2}{2} = \\frac{(0.4 - 1)^2}{2} = \\frac{(-0.6)^2}{2} = \\frac{0.36}{2} = 0.18$$\n- 对于 $i=2$：\n$$\\frac{(q_2 x_2 - b_2)^2}{q_2} = \\frac{(\\frac{1}{2} \\times (-3) - (-2))^2}{\\frac{1}{2}} = \\frac{(-1.5 + 2)^2}{0.5} = \\frac{(0.5)^2}{0.5} = \\frac{0.25}{0.5} = 0.5$$\n- 对于 $i=3$：\n$$\\frac{(q_3 x_3 - b_3)^2}{q_3} = \\frac{(8 \\times 0.5 - 4)^2}{8} = \\frac{(4 - 4)^2}{8} = \\frac{0^2}{8} = 0$$\n\n总和给出 $\\lambda(x)^2$：\n$$\\lambda(x)^2 = 0.18 + 0.5 + 0 = 0.68$$\n牛顿减量 $\\lambda(x)$ 是该值的平方根：\n$$\\lambda(x) = \\sqrt{0.68} \\approx 0.824621125\\dots$$\n四舍五入到四位有效数字，我们得到 $0.8246$。",
            "answer": "$$\\boxed{0.8246}$$"
        },
        {
            "introduction": "在理想的解析求导世界之外，我们常常面临实际的实现挑战。本练习将探讨当无法获得精确Hessian矩阵而必须使用近似值时的情况，这在现实世界的问题中非常普遍。通过比较精确Hessian和有限差分近似Hessian下牛顿递减量的表现，你将深入了解数值误差对收敛速率分析的影响。",
            "id": "3156861",
            "problem": "考虑一个二次连续可微函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$，该函数是严格凸的，并且有唯一的极小化子 $x^\\star$。用于极小化 $f$ 的牛顿法是基于点 $x$ 处的一阶和二阶导数构建的，其局部行为通常用牛顿减量来概括，牛顿减量是一个从这些导数中导出的标量。您的任务是研究当Hessian矩阵被精确计算与用有限差分近似时，牛顿减量轨迹如何变化，并量化为获得可靠的局部收敛速率预测所需的有限差分步长的精度。\n\n从以下基本基础开始：\n- 梯度 $\\nabla f(x)$ 由 $f$ 的一阶偏导数定义。\n- Hessian矩阵 $\\nabla^2 f(x)$ 由 $f$ 的二阶偏导数定义，对于二次连续可微的 $f$，该矩阵是对称的。\n- 在点 $x$ 处的牛顿步长通过求解一个由Hessian矩阵和梯度构成的线性系统得到，该系统源于 $f$ 在 $x$ 附近的二阶泰勒近似。\n\n使用维度 $n=2$ 下的特定光滑、严格凸函数：\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\, x^\\top Q\\, x \\;+\\; \\mu \\sum_{j=1}^{2} \\cosh(x_j),\n$$\n其中 $x=(x_1,x_2)^\\top$，$Q\\in\\mathbb{R}^{2\\times 2}$ 是对称正定矩阵，且 $\\mu0$。在本问题中，使用\n$$\nQ \\;=\\; \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}, \\quad \\mu \\;=\\; 0.2, \\quad x_0 \\;=\\; \\begin{bmatrix} 0.4 \\\\ -0.4 \\end{bmatrix}.\n$$\n\n任务：\n- 对于给定的 $f(x)$，从第一性原理出发解析推导 $\\nabla f(x)$ 和 $\\nabla^2 f(x)$，并使用精确的Hessian矩阵实现牛顿法，以生成从 $x_0$ 开始的轨迹 $\\{x_k\\}$。在每个迭代点 $x_k$，计算相关的牛顿减量。\n- 通过对梯度应用中心有限差分来近似点 $x$ 处的Hessian矩阵：对于给定的步长 $h0$，通过沿每个坐标方向扰动 $x$ 并对得到的梯度进行差分，构造 $\\nabla^2 f(x)$ 的近似。确保该近似被对称化，并且如果检测到非正特征值，则使用所需的最小对角移位对其进行稳定化，以使牛顿步长的线性系统是适定的。\n- 实现第二条从 $x_0$ 开始的牛顿轨迹 $\\{x_k^{\\mathrm{fd}}\\}$，但使用步长为 $h$ 的有限差分Hessian矩阵（梯度保持精确）。在每个迭代点，根据有限差分Hessian矩阵计算相关的减量。\n- 为了分析局部收敛速率预测，请考虑沿两条轨迹的减量序列值 $\\{\\lambda_k\\}$ 和 $\\{\\lambda_k^{\\mathrm{fd}}\\}$。在精确减量很小的局部邻域中，牛顿法表现出特征性的加速。通过比较由每条轨迹构建的序列 $\\left\\{ \\lambda_{k+1} / \\lambda_k^2 \\right\\}$ 来量化预测的局部速率。仅使用满足以下条件的索引 $k$：在第 $k$ 次迭代时，精确减量满足 $\\lambda_k \\le 10^{-3}$，并且两条轨迹都具有所需的下一个迭代点；如果不存在这样的索引，则改用两条轨迹共有的最后3个可用的连续索引，如果少于3个，则使用所有可用的索引。\n- 对于每个测试的 $h$，计算一个单一的标量度量 $E(h)$，定义为在所选索引集上两个局部速率预测器序列之间的最大相对偏差：\n$$\nE(h) \\;=\\; \\max_k \\frac{\\left| \\left( \\lambda_{k+1}^{\\mathrm{fd}} / \\left(\\lambda_k^{\\mathrm{fd}}\\right)^2 \\right) - \\left( \\lambda_{k+1} / \\lambda_k^2 \\right) \\right|}{\\max\\!\\left(10^{-12}, \\left| \\lambda_{k+1} / \\lambda_k^2 \\right| \\right)}.\n$$\n该度量捕捉了有限差分Hessian矩阵对精确Hessian矩阵所预测的局部速率的再现程度。\n\n程序中需遵循的实现细节：\n- 对精确轨迹使用精确解析的 $\\nabla f(x)$ 和 $\\nabla^2 f(x)$。\n- 通过对梯度的列使用步长 $h$ 进行中心差分来构建点 $x$ 处的有限差分Hessian近似：对于每个坐标 $j$，将第 $j$ 列设置为 $\\left( \\nabla f(x + h e_j) - \\nabla f(x - h e_j) \\right)/(2h)$，其中 $e_j$ 是 $\\mathbb{R}^2$ 中的第 $j$ 个标准基向量，然后通过与其转置求平均来进行对称化。\n- 如果需要，通过添加对角移位 $\\alpha I$ 来稳定化有限差分Hessian矩阵 $H_{\\mathrm{fd}}$，其中 $\\alpha\\ge 0$ 是使最小特征值严格大于 $10^{-12}$ 所需的最小值。\n- 运行两个牛顿轨迹，使用不带线搜索的完整步长，最多迭代50次，或直到精确减量达到 $10^{-12}$，以先发生者为准。\n\n测试套件：\n- 使用四个有限差分步长 $h \\in \\{10^{-1}, 10^{-4}, 10^{-6}, 10^{-8}\\}$，以及上面指定的固定 $Q$、$\\mu$ 和 $x_0$。这些选择涵盖了一般情况、两个高精度区域以及一个接近双精度极限的边界条件。\n- 对每个 $h$，按所述计算 $E(h)$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含按 $h = 10^{-1}, 10^{-4}, 10^{-6}, 10^{-8}$ 顺序排列的四个计算结果，形式为用方括号括起来的逗号分隔列表，例如 $[r_1,r_2,r_3,r_4]$。每个 $r_i$ 必须是实数（浮点数）。本问题不涉及物理单位或角度，因此不需要进行单位转换。",
            "solution": "所陈述的问题已经过正式分析和验证。\n\n### 步骤 1：提取给定信息\n- **目标函数**：$f(x) = \\tfrac{1}{2}\\, x^\\top Q\\, x + \\mu \\sum_{j=1}^{2} \\cosh(x_j)$，对于 $x \\in \\mathbb{R}^2$。\n- **参数**：\n  - $Q = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}$\n  - $\\mu = 0.2$\n  - 初始点：$x_0 = \\begin{bmatrix} 0.4 \\\\ -0.4 \\end{bmatrix}$\n- **牛顿法（精确）**：基于解析梯度 $\\nabla f(x)$ 和 Hessian矩阵 $\\nabla^2 f(x)$。\n- **牛顿法（有限差分）**：基于解析梯度 $\\nabla f(x)$ 和一个近似的Hessian矩阵 $H_{\\mathrm{fd}}(x)$。\n- **Hessian矩阵近似**：\n  - 第 $j$ 列通过对梯度进行中心差分计算：$(\\nabla f(x + h e_j) - \\nabla f(x - h e_j)) / (2h)$。\n  - 所得矩阵被对称化。\n  - 如果对称化矩阵的最小特征值不严格大于 $10^{-12}$，则通过添加 $\\alpha I$ 进行稳定化，其中 $\\alpha \\ge 0$ 是所需的最小移位量。\n- **牛顿减量**：$\\lambda(x) = \\sqrt{\\nabla f(x)^\\top H(x)^{-1} \\nabla f(x)}$，其中 $H$ 是精确或近似的Hessian矩阵。\n- **轨迹生成**：\n  - 精确轨迹和有限差分轨迹都最多运行50次迭代。\n  - 两者的停止条件都是当精确减量 $\\lambda_k$ 降至 $10^{-12}$ 以下时。\n- **局部速率预测器**：对于精确轨迹，该量为 $\\lambda_{k+1}/\\lambda_k^2$；对于有限差分轨迹，该量为 $\\lambda_{k+1}^{\\mathrm{fd}}/(\\lambda_k^{\\mathrm{fd}})^2$。\n- **用于分析的索引集**：用于比较速率预测器的索引 $k$ 的集合由以下规则确定：\n  1. 主要规则：所有满足精确减量 $\\lambda_k \\le 10^{-3}$ 且两条轨迹的第 $(k+1)$ 个迭代点都可用的 $k$。\n  2. 备用规则：如果主要集合为空，则使用最后3个可用的连续索引，如果不足3个，则使用所有可用的索引。\n- **误差度量**：$E(h) = \\max_k \\frac{\\left| \\left( \\lambda_{k+1}^{\\mathrm{fd}} / \\left(\\lambda_k^{\\mathrm{fd}}\\right)^2 \\right) - \\left( \\lambda_{k+1} / \\lambda_k^2 \\right) \\right|}{\\max\\!\\left(10^{-12}, \\left| \\lambda_{k+1} / \\lambda_k^2 \\right| \\right)}$，在所选索引集上取最大值。\n- **测试套件**：有限差分步长 $h \\in \\{10^{-1}, 10^{-4}, 10^{-6}, 10^{-8}\\}$。\n\n### 步骤 2：使用提取的给定信息进行验证\n根据验证标准对问题进行评估。\n- **科学依据**：该问题是数值优化中的一个标准练习，这是应用数学和科学计算中的一个核心课题。函数 $f(x)$ 是严格凸的，因为 $Q$ 是正定的（其特征值为 $\\frac{7 \\pm \\sqrt{(4-3)^2 + 4(1)^2}}{2} = \\frac{7 \\pm \\sqrt{17}}{2}$，两者均为正），并且项 $\\mu \\sum \\cosh(x_j)$ 也是凸的（其Hessian矩阵是一个对角矩阵，对角元素 $\\mu\\cosh(x_j)0$ 为正）。因此，存在唯一的极小化子。牛顿法、牛顿减量和有限差分近似都是基础且完善的概念。\n- **适定性**：该问题是适定的。所有参数和程序都已明确定义。$f$ 的凸性保证了极小化问题解的存在性和唯一性。近似Hessian矩阵的稳定化程序确保了牛顿步长的线性系统总是适定的（即矩阵可逆）。误差度量是无歧义地定义的。\n- **客观性**：该问题使用精确的数学语言陈述，不含任何主观或基于意见的声明。\n- **其他缺陷**：该问题是自洽、一致的，并且没有其他列出的缺陷（例如，不完整、矛盾、过于简单）。\n\n### 步骤 3：结论与行动\n该问题是**有效的**。将提供一个完整的、有理有据的解答。\n\n**目标函数的解析导数**\n目标函数由 $f(x) = \\tfrac{1}{2}\\, x^\\top Q\\, x + \\mu \\sum_{j=1}^{2} \\cosh(x_j)$ 给出，其中 $x = (x_1, x_2)^\\top$。\n$f(x)$ 的梯度是其一阶偏导数的向量。二次项 $\\tfrac{1}{2} x^\\top Q x$ 的梯度是 $Qx$，因为 $Q$ 是对称的。双曲余弦项的梯度是一个元素为 $\\mu \\sinh(x_j)$ 的向量。\n因此，梯度为：\n$$\n\\nabla f(x) = Qx + \\mu \\begin{pmatrix} \\sinh(x_1) \\\\ \\sinh(x_2) \\end{pmatrix} = \\begin{pmatrix} 4x_1 + x_2 + \\mu \\sinh(x_1) \\\\ x_1 + 3x_2 + \\mu \\sinh(x_2) \\end{pmatrix}\n$$\n$f(x)$ 的Hessian矩阵是其二阶偏导数的矩阵。二次项的Hessian矩阵是常数矩阵 $Q$。双曲余弦项的Hessian矩阵是一个对角矩阵，其对角元素为 $\\mu \\cosh(x_j)$。\n因此，Hessian矩阵为：\n$$\n\\nabla^2 f(x) = Q + \\mu \\begin{pmatrix} \\cosh(x_1)  0 \\\\ 0  \\cosh(x_2) \\end{pmatrix} = \\begin{pmatrix} 4 + \\mu \\cosh(x_1)  1 \\\\ 1  3 + \\mu \\cosh(x_2) \\end{pmatrix}\n$$\n由于 $Q$ 是正定的且对于所有 $x_j$ 都有 $\\mu\\cosh(x_j)  0$，因此Hessian矩阵 $\\nabla^2 f(x)$ 对于所有 $x \\in \\mathbb{R}^2$ 都是正定的。\n\n**牛顿法轨迹**\n对于两条轨迹，我们都从 $x_0 = (0.4, -0.4)^\\top$ 开始。在每次迭代 $k$ 中，我们计算一个搜索方向并更新当前点。\n对于精确牛顿法，搜索方向 $\\Delta x_k$ 通过求解以下线性系统得到：\n$$\n\\nabla^2 f(x_k) \\Delta x_k = -\\nabla f(x_k)\n$$\n对于有限差分牛顿法，搜索方向 $\\Delta x_k^{\\mathrm{fd}}$ 通过求解以下方程得到：\n$$\nH_{\\mathrm{stab}}(x_k^{\\mathrm{fd}}) \\Delta x_k^{\\mathrm{fd}} = -\\nabla f(x_k^{\\mathrm{fd}})\n$$\n其中 $H_{\\mathrm{stab}}$ 是稳定化、对称化的Hessian矩阵的有限差分近似。初始近似矩阵 $H_{\\mathrm{approx}}$ 的第 $j$ 列由下式给出：\n$$\n(H_{\\mathrm{approx}})_{\\cdot, j} = \\frac{\\nabla f(x + h e_j) - \\nabla f(x - h e_j)}{2h}\n$$\n然后该矩阵被对称化：$H_{\\mathrm{fd}} = \\frac{1}{2}(H_{\\mathrm{approx}} + H_{\\mathrm{approx}}^\\top)$。为确保正定性，我们计算 $H_{\\mathrm{fd}}$ 的最小特征值 $\\lambda_{\\min}$。如果 $\\lambda_{\\min} \\le 10^{-12}$，我们计算一个移位量 $\\alpha = \\max(0, 1.1 \\times 10^{-12} - \\lambda_{\\min})$ 并设置 $H_{\\mathrm{stab}} = H_{\\mathrm{fd}} + \\alpha I$。否则，$H_{\\mathrm{stab}} = H_{\\mathrm{fd}}$。\n迭代点通过一个完整步长进行更新：$x_{k+1} = x_k + \\Delta x_k$ 和 $x_{k+1}^{\\mathrm{fd}} = x_k^{\\mathrm{fd}} + \\Delta x_k^{\\mathrm{fd}}$。\n\n**牛顿减量与收敛性分析**\n精确方法在第 $k$ 步的牛顿减量为 $\\lambda_k = \\sqrt{-\\nabla f(x_k)^\\top \\Delta x_k}$。类似地，对于有限差分方法，$\\lambda_k^{\\mathrm{fd}} = \\sqrt{-\\nabla f(x_k^{\\mathrm{fd}})^\\top \\Delta x_k^{\\mathrm{fd}}}$。\n轨迹同步生成，最多迭代50次，当 $\\lambda_k \\le 10^{-12}$ 时终止。\n\n为了评估局部收敛速率预测的质量，我们在一个特定的索引集上比较速率预测器序列 $R_k = \\lambda_{k+1}/\\lambda_k^2$ 和 $R_k^{\\mathrm{fd}} = \\lambda_{k+1}^{\\mathrm{fd}}/(\\lambda_k^{\\mathrm{fd}})^2$。索引 $k$ 的选择标准是该方法处于二次收敛区域，即对于精确方法，$\\lambda_k \\le 10^{-3}$。如果没有产生这样的迭代点，则使用最后三个可用的连续索引作为备用方案。误差度量 $E(h)$ 是这两个序列在所选索引上的最大相对偏差，它提供了一个定量度量，说明有限差分近似如何影响极小化子附近的预测动态行为。算法通过遍历每个指定的 $h$ 值，生成两条轨迹，计算速率预测器序列，最后计算误差度量 $E(h)$ 来进行。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the error metric E(h) for different finite-difference step sizes h.\n    \"\"\"\n    Q = np.array([[4.0, 1.0], [1.0, 3.0]])\n    mu = 0.2\n    x0 = np.array([0.4, -0.4])\n    h_values = [1e-1, 1e-4, 1e-6, 1e-8]\n    \n    max_iter = 50\n    tol_decrement = 1e-12\n    n_dim = 2\n\n    def grad_f(x):\n        \"\"\"Computes the analytical gradient of f(x).\"\"\"\n        return Q @ x + mu * np.sinh(x)\n\n    def hess_f(x):\n        \"\"\"Computes the analytical Hessian of f(x).\"\"\"\n        return Q + mu * np.diag(np.cosh(x))\n\n    results = []\n    \n    for h in h_values:\n        # Initialize trajectories\n        x_exact = x0.copy()\n        x_fd = x0.copy()\n        \n        lams_exact = []\n        lams_fd = []\n        \n        for _ in range(max_iter):\n            # --- Exact Newton Step ---\n            g_exact = grad_f(x_exact)\n            H_exact = hess_f(x_exact)\n            \n            # Solve for Newton step\n            dx_exact = np.linalg.solve(H_exact, -g_exact)\n            \n            # Compute Newton decrement\n            lam_sq_exact = -g_exact.T @ dx_exact\n            lam_exact = np.sqrt(max(0, lam_sq_exact))\n            lams_exact.append(lam_exact)\n            \n            # --- Finite-Difference Newton Step ---\n            g_fd_pt = grad_f(x_fd)\n            \n            # Construct FD Hessian\n            H_approx = np.zeros_like(Q)\n            for j in range(n_dim):\n                e_j = np.zeros(n_dim)\n                e_j[j] = 1.0\n                g_plus = grad_f(x_fd + h * e_j)\n                g_minus = grad_f(x_fd - h * e_j)\n                H_approx[:, j] = (g_plus - g_minus) / (2 * h)\n            \n            H_fd = 0.5 * (H_approx + H_approx.T)\n            \n            # Stabilize FD Hessian\n            min_eig = np.min(np.linalg.eigvalsh(H_fd))\n            alpha = 0.0\n            # Target is strictly > 1e-12. We target 1.1e-12 for robustness.\n            target_min_eig = 1.1e-12\n            if min_eig  target_min_eig:\n                alpha = max(0.0, target_min_eig - min_eig)\n            \n            H_fd_stab = H_fd + alpha * np.eye(n_dim)\n\n            # Solve for Newton step\n            dx_fd = np.linalg.solve(H_fd_stab, -g_fd_pt)\n            \n            # Compute FD Newton decrement\n            lam_sq_fd = -g_fd_pt.T @ dx_fd\n            lam_fd = np.sqrt(max(0, lam_sq_fd))\n            lams_fd.append(lam_fd)\n\n            # Check for termination\n            if lam_exact = tol_decrement:\n                break\n                \n            # Update points\n            x_exact += dx_exact\n            x_fd += dx_fd\n\n        # --- Analyze Convergence Rates ---\n        num_decrements = len(lams_exact)\n        if num_decrements  2:\n            results.append(0.0)\n            continue\n            \n        rate_k_indices = []\n        # Primary rule for index selection\n        for k in range(num_decrements - 1):\n            if lams_exact[k] = 1e-3:\n                rate_k_indices.append(k)\n\n        # Fallback rule if primary set is empty\n        if not rate_k_indices:\n            num_computable_rates = num_decrements - 1\n            start_k = max(0, num_computable_rates - 3)\n            rate_k_indices = list(range(start_k, num_computable_rates))\n            \n        deviations = []\n        if rate_k_indices:\n            for k in rate_k_indices:\n                # Handle potential division by zero for decrement^2, although unlikely here\n                if lams_exact[k]  1e-15 or lams_fd[k]  1e-15:\n                    continue\n\n                rate_exact = lams_exact[k+1] / (lams_exact[k]**2)\n                rate_fd = lams_fd[k+1] / (lams_fd[k]**2)\n                \n                numerator = abs(rate_fd - rate_exact)\n                denominator = max(1e-12, abs(rate_exact))\n                \n                deviations.append(numerator / denominator)\n\n        E_h = max(deviations) if deviations else 0.0\n        results.append(E_h)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后，本练习将展示牛顿递减量在一类重要的现代优化算法——内点法中的强大作用。你将为线性规划问题实现一个对数障碍函数法，其中牛顿递减量将作为一个具有坚实理论基础且在实践中行之有效的停止准则。你将推导并凭经验验证一个著名的结论，该结论将牛顿递减量与自洽函数的最优性差距的二次界联系起来。",
            "id": "3156797",
            "problem": "考虑一个二阶连续可微的自协调凸函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$。自协调的概念由以下不等式定义\n$$\n\\left|D^3 f(x)[h,h,h]\\right| \\le 2 \\left(h^\\top \\nabla^2 f(x)\\, h\\right)^{3/2}\n$$\n对于 $f$ 定义域中的所有 $x$ 和所有方向 $h \\in \\mathbb{R}^n$，其中 $D^3 f(x)[h,h,h]$ 表示 $f$ 在 $x$ 点沿方向 $h$ 的三阶方向导数。对于此类函数，牛顿法使用牛顿步长 $p(x) = -\\nabla^2 f(x)^{-1}\\nabla f(x)$ 和牛顿减量，其定义为\n$$\n\\lambda(x) = \\sqrt{\\nabla f(x)^\\top \\left[\\nabla^2 f(x)\\right]^{-1} \\nabla f(x)}.\n$$\n从自协调的基本定义和凸二阶可微函数的基本性质出发，推导出一个纯粹用牛顿减量表示的、有理论依据的停止准则，该准则能保证在 $f$ 的最小化点处的次优性差距具有二次界。您的推导必须从上述自协调不等式和牛顿法的定义开始，不能假设任何已知的收敛速率公式。\n\n然后，通过对数障碍函数，在一个小规模线性规划问题上实现并测试此停止准则。考虑以下形式的线性规划问题\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; c^\\top x \\quad \\text{subject to} \\quad A x \\le b, \\quad x \\ge 0,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，$c \\in \\mathbb{R}^n$。对于 $t  0$，引入带有障碍项的目标函数，\n$$\nf_t(x) = t\\, c^\\top x \\;-\\; \\sum_{i=1}^{n} \\log x_i \\;-\\; \\sum_{j=1}^{m} \\log\\!\\left(b_j - a_j^\\top x\\right),\n$$\n其中 $a_j^\\top$ 表示 $A$ 的第 $j$ 行，定义域是由 $x_i  0$ (对所有 $i$) 和 $a_j^\\top x  b_j$ (对所有 $j$) 定义的严格内部。从多元微积分的基本原理出发，推导梯度 $\\nabla f_t(x)$ 和海森矩阵 $\\nabla^2 f_t(x)$ 的表达式。\n\n您必须实现一个带有回溯线搜索的阻尼牛顿法，该方法需：\n- 在每次迭代中保持严格可行性 $x_i  0$ 和 $a_j^\\top x  b_j$，\n- 在每次迭代中计算牛顿减量 $\\lambda(x)$，\n- 当以 $\\lambda(x)$ 表示的停止准则成立时停止，\n- 并通过将所达到的目标值与相同障碍参数 $t$ 下最小化点的高精度近似值进行比较，来验证所承诺的次优性差距的二次界。\n\n测试套件：\n为以下两个小型线性规划问题实现上述算法，每个问题有两个决策变量（$n = 2$）和三个不等式约束（$m = 3$）。所有情况均使用指定的初始可行点 $x^{(0)}$。\n\n- 线性规划 $1$：\n  - $A = \\begin{bmatrix} 1  2 \\\\ 1  0 \\\\ 0  1 \\end{bmatrix}$,\n  - $b = \\begin{bmatrix} 3 \\\\ 2 \\\\ 2 \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$,\n  - $x^{(0)} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}$。\n\n- 线性规划 $2$：\n  - $A = \\begin{bmatrix} 2  1 \\\\ 1  3 \\\\ 1  0.5 \\end{bmatrix}$,\n  - $b = \\begin{bmatrix} 4 \\\\ 5 \\\\ 2 \\end{bmatrix}$,\n  - $c = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$,\n  - $x^{(0)} = \\begin{bmatrix} 0.4 \\\\ 0.3 \\end{bmatrix}$。\n\n对于每个线性规划问题，使用指定的 $t$ 值和目标容差运行障碍法，并通过使用一个非常严格的容差，用同样的阻尼牛顿法计算最小化点 $x^\\star_t$ 的高精度近似值，来检验停止准则界的正确性。对于每个测试用例，程序必须返回一个布尔值，指示不等式\n$$\nf_t(x_{\\text{stop}}) - f_t(x^\\star_t) \\le \\frac{\\varepsilon^2}{2}\n$$\n是否成立，其中 $x_{\\text{stop}}$ 是停止准则首次触发时的迭代点，$\\varepsilon$ 是停止准则中使用的容差。\n\n需要实现的测试用例：\n- 用例 $1$：线性规划 $1$，$t = 1$，$\\varepsilon = 10^{-2}$。\n- 用例 $2$：线性规划 $1$，$t = 5$，$\\varepsilon = 5 \\times 10^{-3}$。\n- 用例 $3$：线性规划 $2$，$t = 1$，$\\varepsilon = 10^{-2}$。\n- 用例 $4$：线性规划 $2$，$t = 10$，$\\varepsilon = 10^{-3}$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表（例如，$[result_1,result_2,result_3,result_4]$），其中每个 $result_k$是为上述测试用例 $k$ 指定的布尔值。不涉及物理单位。不涉及角度。不涉及百分比。",
            "solution": "该问题要求完成两项主要任务。首先，基于牛顿减量为自协调函数上的牛顿法推导一个停止准则，该准则保证了次优性差距的二次界。其次，在线性规划的对数障碍函数上实现并验证此规则。\n\n### 第一部分：次优性界与停止准则的推导\n\n设 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 是一个二阶连续可微的凸自协调函数。自协调的定义属性由以下不等式给出：\n$$\n\\left|D^3 f(x)[h,h,h]\\right| \\le 2 \\left(h^\\top \\nabla^2 f(x)\\, h\\right)^{3/2}\n$$\n对于 $f$ 定义域中的所有 $x$ 和所有 $h \\in \\mathbb{R}^n$。我们将 $x$ 点的局部范数定义为 $\\|u\\|_x = \\sqrt{u^\\top \\nabla^2 f(x) u}$。该不等式可以写成 $|D^3 f(x)[h,h,h]| \\le 2 \\|h\\|_x^3$。\n\n我们的目标是为次优性差距 $f(x) - f(x^\\star)$ 提供一个界，其中 $x^\\star$ 是 $f$ 的唯一最小化点，即 $\\nabla f(x^\\star) = 0$。该界应以牛顿减量 $\\lambda(x) = \\sqrt{\\nabla f(x)^\\top [\\nabla^2 f(x)]^{-1} \\nabla f(x)}$ 来表示。\n\n推导过程如下：\n1.  **为海森矩阵定界：** 自协调不等式的一个关键推论是关于海森矩阵变化幅度的界。通过分析函数在 $x$ 和 $y=x+h$ 之间的线段上的行为，可以为任何满足 $\\|y-x\\|_x  1$ 的 $y$ 推导出以下不等式：\n    $$\n    f(y) \\ge f(x) + \\nabla f(x)^\\top(y-x) + \\omega\\left(\\|y-x\\|_x\\right)\n    $$\n    其中 $\\omega(u) = u - \\log(1+u)$。这是一个基于 $x$ 点局部信息的强大的 $f$ 函数全局下界。推导过程涉及对三阶导数不等式进行两次积分。\n\n2.  **为最优值定下界：** 上述不等式对任何可行点 $y$ 都成立。让我们将其应用于最小化点 $y = x^\\star$。\n    $$\n    f(x^\\star) \\ge f(x) + \\nabla f(x)^\\top(x^\\star-x) + \\omega\\left(\\|x^\\star-x\\|_x\\right)\n    $$\n    这为最优值 $f(x^\\star)$ 提供了一个下界。\n\n3.  **引入牛顿减量：** 我们可以找到右侧的一个不依赖于未知 $x^\\star$ 的下界。令 $h = x^\\star-x$。通过在局部范数内积中应用柯西-施瓦茨不等式，我们得到：\n    $$\n    \\nabla f(x)^\\top h = (\\nabla f(x)^\\top[\\nabla^2 f(x)]^{-1/2}) ([\\nabla^2 f(x)]^{1/2} h) \\ge -\\|\\nabla f(x)\\|_{x,*} \\|h\\|_x\n    $$\n    其中 $\\|\\cdot\\|_{x,*}$ 是 $\\|\\cdot\\|_x$ 的对偶范数。梯度在该对偶范数下的范数正是牛顿减量，$\\|\\nabla f(x)\\|_{x,*} = \\lambda(x)$。因此，$\\nabla f(x)^\\top h \\ge -\\lambda(x)\\|h\\|_x$。\n\n4.  **最小化下界：** 将此代入 $f(x^\\star)$ 的不等式中：\n    $$\n    f(x^\\star) \\ge f(x) - \\lambda(x)\\|x^\\star-x\\|_x + \\omega\\left(\\|x^\\star-x\\|_x\\right)\n    $$\n    令 $u = \\|x^\\star-x\\|_x$。表达式变为：\n    $$\n    f(x^\\star) \\ge f(x) + (1-\\lambda(x))u - \\log(1+u)\n    $$\n    该不等式对于特定的值 $u = \\|x^\\star - x\\|_x$ 成立。然而，由于 $f(x^\\star)$ 是全局最小值，它必须大于或等于右侧表达式在所有 $u \\ge 0$ 上的最小值。令 $g(u) = (1-\\lambda(x))u - \\log(1+u)$。\n    为了找到 $g(u)$ 的最小值，我们计算其导数：$g'(u) = 1-\\lambda(x) - \\frac{1}{1+u}$。\n    假设 $\\lambda(x)  1$，令 $g'(u)=0$ 得到 $1+u = \\frac{1}{1-\\lambda(x)}$，所以 $u = \\frac{\\lambda(x)}{1-\\lambda(x)}$。\n    将这个 $u$ 值代回 $g(u)$ 得到最小值：\n    $$\n    \\min_{u \\ge 0} g(u) = (1-\\lambda(x))\\frac{\\lambda(x)}{1-\\lambda(x)} - \\log\\left(1 + \\frac{\\lambda(x)}{1-\\lambda(x)}\\right) = \\lambda(x) - \\log\\left(\\frac{1}{1-\\lambda(x)}\\right) = \\lambda(x) + \\log(1-\\lambda(x))\n    $$\n    因此，我们得到了 $f(x^\\star)$ 的一个下界：\n    $$\n    f(x^\\star) \\ge f(x) + \\lambda(x) + \\log(1-\\lambda(x))\n    $$\n\n5.  **构建停止准则：** 重新整理不等式可得到次优性差距的一个上界：\n    $$\n    f(x) - f(x^\\star) \\le - \\left( \\lambda(x) + \\log(1-\\lambda(x)) \\right)\n    $$\n    这个界在 $\\lambda(x)  1$ 时成立。对于较小的 $\\lambda(x)$ 值，我们可以使用 $\\log(1-z)$ 在 $z=0$ 附近的泰勒级数展开，即 $\\log(1-z) = -z - \\frac{z^2}{2} - O(z^3)$。\n    代入 $z = \\lambda(x)$:\n    $$\n    f(x) - f(x^\\star) \\le - \\left( \\lambda(x) + \\left(-\\lambda(x) - \\frac{\\lambda(x)^2}{2} - O(\\lambda(x)^3)\\right) \\right) = \\frac{\\lambda(x)^2}{2} + O(\\lambda(x)^3)\n    $$\n    这表明对于较小的 $\\lambda(x)$，次优性差距由 $\\lambda(x)$ 呈二次界定。一个有理论依据的停止准则是当牛顿减量 $\\lambda(x)$ 小于预设容差 $\\varepsilon$ 时终止。如果我们停止在 $\\lambda(x) \\le \\varepsilon$ 时，我们可以确信次优性差距大约由 $\\varepsilon^2/2$ 界定。问题要求验证不等式 $f_t(x_{\\text{stop}}) - f_t(x^\\star_t) \\le \\frac{\\varepsilon^2}{2}$。\n\n### 第二部分：障碍函数的梯度与海森矩阵\n\n对于一个线性规划问题，带有障碍项的目标函数为：\n$$\nf_t(x) = t c^\\top x - \\sum_{i=1}^{n} \\log x_i - \\sum_{j=1}^{m} \\log(b_j - a_j^\\top x)\n$$\n我们使用标准多元微积分推导其梯度 $\\nabla f_t(x)$ 和海森矩阵 $\\nabla^2 f_t(x)$。\n\n**梯度：** 梯度的第 $k$ 个分量是关于 $x_k$ 的偏导数：\n$$\n\\frac{\\partial f_t}{\\partial x_k} = \\frac{\\partial}{\\partial x_k}(t c^\\top x) - \\sum_{i=1}^{n}\\frac{\\partial}{\\partial x_k}(\\log x_i) - \\sum_{j=1}^{m}\\frac{\\partial}{\\partial x_k}(\\log(b_j - a_j^\\top x))\n$$\n$$\n\\frac{\\partial f_t}{\\partial x_k} = t c_k - \\frac{1}{x_k} - \\sum_{j=1}^{m} \\frac{1}{b_j - a_j^\\top x} (-a_{jk}) = t c_k - \\frac{1}{x_k} + \\sum_{j=1}^{m} \\frac{a_{jk}}{b_j - a_j^\\top x}\n$$\n在向量形式下，使用 $A_{j,:}=a_j^\\top$，表达式为：\n$$\n\\nabla f_t(x) = t c - (x^{\\circ -1}) + A^\\top ( (b-Ax)^{\\circ -1} )\n$$\n其中 $^{\\circ -1}$ 表示逐元素求逆。\n\n**海森矩阵：** 海森矩阵的 $(k,l)$ 项是 $\\frac{\\partial^2 f_t}{\\partial x_l \\partial x_k}$:\n$$\n\\frac{\\partial^2 f_t}{\\partial x_l \\partial x_k} = \\frac{\\partial}{\\partial x_l}\\left(t c_k - \\frac{1}{x_k} + \\sum_{j=1}^{m} \\frac{a_{jk}}{b_j - a_j^\\top x}\\right)\n$$\n$$\n\\frac{\\partial^2 f_t}{\\partial x_l \\partial x_k} = 0 + \\frac{\\delta_{kl}}{x_k^2} + \\sum_{j=1}^{m} a_{jk} \\frac{-\\frac{\\partial}{\\partial x_l}(b_j - a_j^\\top x)}{(b_j - a_j^\\top x)^2} = \\frac{\\delta_{kl}}{x_k^2} + \\sum_{j=1}^{m} \\frac{a_{jk} a_{jl}}{(b_j - a_j^\\top x)^2}\n$$\n其中 $\\delta_{kl}$ 是克罗内克δ。在矩阵形式下，表达式为：\n$$\n\\nabla^2 f_t(x) = \\text{diag}(x_1^{-2}, \\dots, x_n^{-2}) + \\sum_{j=1}^{m} \\frac{1}{(b_j - a_j^\\top x)^2} a_j a_j^\\top = \\text{diag}(x^{\\circ -2}) + A^\\top \\text{diag}((b-Ax)^{\\circ -2}) A\n$$\n\n### 第三部分：实现与验证\n\n采用带有回溯线搜索的阻尼牛顿法进行实现。\n1.  **初始化**：从一个严格可行点 $x^{(0)}$ 开始。\n2.  **迭代**：在第 $k$ 步：\n    a. 计算梯度 $g_k = \\nabla f_t(x^{(k)})$ 和海森矩阵 $H_k = \\nabla^2 f_t(x^{(k)})$。\n    b. 计算牛顿步长 $p_k = -H_k^{-1} g_k$。\n    c. 计算牛顿减量的平方 $\\lambda_k^2 = -g_k^\\top p_k$。\n    d. **停止准则**：若 $\\lambda_k^2 \\le \\varepsilon^2$，则终止并返回 $x_{\\text{stop}} = x^{(k)}$。\n    e. **线搜索**：找到一个步长 $s_k  0$，既能保持严格可行性（$x^{(k)}+s_k p_k  0$ 和 $A(x^{(k)}+s_k p_k)  b$），又满足Armijo条件：$f_t(x^{(k)}+s_k p_k) \\le f_t(x^{(k)}) + \\alpha s_k g_k^\\top p_k$，其中参数 $\\alpha \\in (0, 0.5)$。\n    f. **更新**：设置 $x^{(k+1)} = x^{(k)} + s_k p_k$。\n\n对于每个测试用例，我们首先使用指定的容差 $\\varepsilon$ 运行此算法以找到 $x_{\\text{stop}}$。然后我们再次使用一个非常高的精度容差（例如 $\\varepsilon_{\\text{acc}} = 10^{-12}$）运行它，以找到一个接近精确解的最小化点 $x^\\star_t$。最后，我们验证推导出的次优性界是否成立：$f_t(x_{\\text{stop}}) - f_t(x^\\star_t) \\le \\varepsilon^2/2$。",
            "answer": "```python\nimport numpy as np\n\ndef barrier_function(x, A, b, c, t):\n    \"\"\"\n    Computes the value of the barrier-augmented objective function.\n    Returns np.inf if x is not strictly feasible.\n    \"\"\"\n    if np.any(x = 0) or np.any(A @ x >= b):\n        return np.inf\n    log_x = np.sum(np.log(x))\n    log_slack = np.sum(np.log(b - A @ x))\n    return t * (c @ x) - log_x - log_slack\n\ndef gradient(x, A, b, c, t):\n    \"\"\"Computes the gradient of the barrier function.\"\"\"\n    inv_x = 1.0 / x\n    slack = b - A @ x\n    inv_slack = 1.0 / slack\n    return t * c - inv_x + A.T @ inv_slack\n\ndef hessian(x, A, b, c, t):\n    \"\"\"Computes the Hessian of the barrier function.\"\"\"\n    inv_x_sq = 1.0 / (x**2)\n    H_log_x = np.diag(inv_x_sq)\n    \n    slack = b - A @ x\n    inv_slack_sq = 1.0 / (slack**2)\n    D_inv_slack_sq = np.diag(inv_slack_sq)\n    \n    H_log_slack = A.T @ D_inv_slack_sq @ A\n    \n    return H_log_x + H_log_slack\n\ndef damped_newton_method(A, b, c, t, x0, epsilon, alpha=0.25, beta=0.5):\n    \"\"\"\n    Performs damped Newton method with backtracking line search to minimize the barrier function.\n    \"\"\"\n    x = x0.copy()\n    max_iter = 100\n    \n    for _ in range(max_iter):\n        grad = gradient(x, A, b, c, t)\n        hess = hessian(x, A, b, c, t)\n        \n        try:\n            p = np.linalg.solve(hess, -grad)\n        except np.linalg.LinAlgError:\n            # This should ideally not happen if the problem is well-posed and x is feasible\n            # A pseudo-inverse could be a robust fallback, but is not needed here.\n            break\n\n        lambda_sq = -grad.T @ p\n        \n        if lambda_sq = epsilon**2:\n            break\n            \n        # Backtracking line search\n        s = 1.0\n        \n        # Ensure step stays within feasible region\n        s_max = np.inf\n        # Constraint x_i > 0\n        neg_p_indices = p  0\n        if np.any(neg_p_indices):\n            s_max = min(s_max, np.min(-x[neg_p_indices] / p[neg_p_indices]))\n        \n        # Constraint Ax  b\n        ap = A @ p\n        pos_ap_indices = ap > 0\n        if np.any(pos_ap_indices):\n            slack = b - A @ x\n            s_max = min(s_max, np.min(slack[pos_ap_indices] / ap[pos_ap_indices]))\n        \n        # Use a fudge factor to ensure strict feasibility\n        if s_max  np.inf:\n            s = min(s, 0.99 * s_max)\n        \n        # Armijo condition\n        f_x = barrier_function(x, A, b, c, t)\n        val_armijo = alpha * s * grad.T @ p\n        while True:\n            x_new = x + s * p\n            # Check feasibility explicitly, as s can become very small\n            if np.all(x_new > 0) and np.all(A @ x_new  b):\n                f_x_new = barrier_function(x_new, A, b, c, t)\n                if f_x_new = f_x + val_armijo:\n                    break\n            s *= beta\n            val_armijo *= beta\n            if s  1e-15: # Prevent infinite loop if something goes wrong\n                break\n        \n        x = x + s * p\n        \n    return x\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define problem data\n    lp1 = {\n        'A': np.array([[1, 2], [1, 0], [0, 1]], dtype=float),\n        'b': np.array([3, 2, 2], dtype=float),\n        'c': np.array([1, 2], dtype=float),\n        'x0': np.array([0.5, 0.5], dtype=float)\n    }\n    \n    lp2 = {\n        'A': np.array([[2, 1], [1, 3], [1, 0.5]], dtype=float),\n        'b': np.array([4, 5, 2], dtype=float),\n        'c': np.array([2, 1], dtype=float),\n        'x0': np.array([0.4, 0.3], dtype=float)\n    }\n\n    test_cases = [\n        # (LP data, t, epsilon)\n        (lp1, 1.0, 1e-2),\n        (lp1, 5.0, 5e-3),\n        (lp2, 1.0, 1e-2),\n        (lp2, 10.0, 1e-3),\n    ]\n\n    results = []\n    high_accuracy_epsilon = 1e-12\n\n    for lp_data, t, epsilon in test_cases:\n        A, b, c, x0 = lp_data['A'], lp_data['b'], lp_data['c'], lp_data['x0']\n        \n        # Find the point where the stopping rule is met\n        x_stop = damped_newton_method(A, b, c, t, x0, epsilon)\n        \n        # Find a high-accuracy approximation of the minimizer\n        x_star_t = damped_newton_method(A, b, c, t, x0, high_accuracy_epsilon)\n        \n        # Calculate the function values\n        f_stop = barrier_function(x_stop, A, b, c, t)\n        f_star_t = barrier_function(x_star_t, A, b, c, t)\n        \n        # Check the inequality\n        suboptimality_gap = f_stop - f_star_t\n        bound = epsilon**2 / 2.0\n        \n        is_bound_satisfied = suboptimality_gap = bound\n        results.append(is_bound_satisfied)\n\n    # Format the final output as a comma-separated list of booleans\n    print(f\"[{','.join(map(lambda b: str(b), results))}]\")\n\nsolve()\n```"
        }
    ]
}