{
    "hands_on_practices": [
        {
            "introduction": "The Newton decrement, $\\lambda(x)$, is a pivotal quantity in optimization that measures the proximity of a point $x$ to the optimal solution. To build a solid intuition for this concept, we will first dissect it in a well-behaved setting. This practice  invites you to analyze a separable quadratic function, where you can break down the total decrement into a sum of contributions from each coordinate. By further examining a damped Newton step, you will derive a precise relationship between the function's curvature in each direction and the corresponding local convergence rate, revealing the deep connection between problem structure and algorithmic performance.",
            "id": "3156868",
            "problem": "Consider the unconstrained minimization of a separable quadratic function in $\\mathbb{R}^{n}$ given by $f(x) = \\sum_{i=1}^{n} \\tfrac{1}{2} q_{i} x_{i}^{2} - b_{i} x_{i}$, where $q_{i} \\in \\mathbb{R}$ satisfy $q_{i}  0$ for all $i \\in \\{1,\\dots,n\\}$ and $b_{i} \\in \\mathbb{R}$. Let $\\nabla f(x)$ denote the gradient and $\\nabla^{2} f(x)$ denote the Hessian. The Newton decrement $\\lambda(x)$ is defined by the foundational expression $\\lambda(x)^{2} = \\nabla f(x)^{\\mathsf{T}} \\left[\\nabla^{2} f(x)\\right]^{-1} \\nabla f(x)$.\n\nTask A. Starting from the definitions of $\\nabla f(x)$ and $\\nabla^{2} f(x)$ for the given $f(x)$, derive a componentwise expression for $\\lambda(x)^{2}$ that reveals the contribution of each coordinate to the Newton decrement.\n\nTask B. Consider a damped Newton step arising from a standard trust-region surrogate in which the search direction $p(x)$ is defined by solving $\\left(\\nabla^{2} f(x) + \\mu I\\right) p(x) = -\\nabla f(x)$ for a fixed damping parameter $\\mu  0$, and the update is $x^{+} = x + p(x)$. Let $x^{\\star}$ denote the unique minimizer of $f(x)$. Express the error $e = x - x^{\\star}$ and derive the per-coordinate error update $e^{+}$ in terms of $e$, $q_{i}$, and $\\mu$. Then state precise conditions, in terms of $q_{i}$ and $q_{j}$, under which coordinate $i$ exhibits a strictly faster local linear convergence rate than coordinate $j$ under this damped Newton update.\n\nTask C. For the concrete instance with $n = 3$, $q = (2, \\tfrac{1}{2}, 8)$, $b = (1, -2, 4)$, and $x = (0.2, -3, 0.5)$, compute the Newton decrement $\\lambda(x)$. Round your answer to four significant figures. Provide your final answer as a single number with no units.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. It is a standard problem in the field of numerical optimization.\n\nThe problem is composed of three tasks. We will address each in sequence.\n\n**Task A: Componentwise Expression for the Newton Decrement**\n\nThe objective function is a separable quadratic function in $\\mathbb{R}^{n}$:\n$$f(x) = \\sum_{i=1}^{n} \\left(\\frac{1}{2} q_{i} x_{i}^{2} - b_{i} x_{i}\\right)$$\nwhere $q_{i}  0$ for all $i \\in \\{1,\\dots,n\\}$.\n\nFirst, we compute the gradient vector $\\nabla f(x)$. The $i$-th component of the gradient, $(\\nabla f(x))_{i}$, is the partial derivative of $f(x)$ with respect to $x_i$:\n$$(\\nabla f(x))_{i} = \\frac{\\partial f}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left(\\frac{1}{2} q_{i} x_{i}^{2} - b_{i} x_{i}\\right) = q_i x_i - b_i$$\nThus, the gradient vector is $\\nabla f(x) = [q_1 x_1 - b_1, \\dots, q_n x_n - b_n]^{\\mathsf{T}}$.\n\nNext, we compute the Hessian matrix $\\nabla^{2} f(x)$. The entry at row $j$ and column $k$, $(\\nabla^{2} f(x))_{jk}$, is the second partial derivative $\\frac{\\partial^2 f}{\\partial x_j \\partial x_k}$.\nFor $j \\neq k$, the derivatives are with respect to different variables. Since the function is separable, we have:\n$$(\\nabla^{2} f(x))_{jk} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_k} = \\frac{\\partial}{\\partial x_j} (q_k x_k - b_k) = 0$$\nFor $j = k$, we have:\n$$(\\nabla^{2} f(x))_{jj} = \\frac{\\partial^2 f}{\\partial x_j^2} = \\frac{\\partial}{\\partial x_j} (q_j x_j - b_j) = q_j$$\nThe Hessian is therefore a constant diagonal matrix, which we can denote by $Q$:\n$$\\nabla^{2} f(x) = Q = \\text{diag}(q_1, q_2, \\dots, q_n)$$\nSince all $q_i  0$, the Hessian $Q$ is positive definite.\n\nThe inverse of the Hessian, $[\\nabla^{2} f(x)]^{-1} = Q^{-1}$, is also a diagonal matrix whose diagonal entries are the reciprocals of the entries of $Q$:\n$$[\\nabla^{2} f(x)]^{-1} = \\text{diag}\\left(\\frac{1}{q_1}, \\frac{1}{q_2}, \\dots, \\frac{1}{q_n}\\right)$$\n\nNow, we substitute these expressions into the definition of the Newton decrement squared, $\\lambda(x)^{2}$:\n$$\\lambda(x)^{2} = \\nabla f(x)^{\\mathsf{T}} \\left[\\nabla^{2} f(x)\\right]^{-1} \\nabla f(x)$$\nThis is a quadratic form. Since the matrix $[\\nabla^{2} f(x)]^{-1}$ is diagonal, the expression simplifies to a sum:\n$$\\lambda(x)^{2} = \\sum_{i=1}^{n} (\\nabla f(x))_{i} \\left([\\nabla^{2} f(x)]^{-1}\\right)_{ii} (\\nabla f(x))_{i}$$\n$$\\lambda(x)^{2} = \\sum_{i=1}^{n} (q_i x_i - b_i) \\left(\\frac{1}{q_i}\\right) (q_i x_i - b_i)$$\nThis yields the componentwise expression for $\\lambda(x)^{2}$:\n$$\\lambda(x)^{2} = \\sum_{i=1}^{n} \\frac{(q_i x_i - b_i)^2}{q_i}$$\nThis expression shows that the total squared Newton decrement is the sum of contributions from each coordinate.\n\n**Task B: Error Update and Convergence Rate Comparison**\n\nFirst, we find the unique minimizer $x^{\\star}$ of $f(x)$. Since $f(x)$ is strictly convex ($q_i  0$), the minimizer is found by setting the gradient to zero:\n$$\\nabla f(x^{\\star}) = 0 \\implies q_i x_i^{\\star} - b_i = 0 \\text{ for all } i$$\nSolving for $x_i^{\\star}$ gives $x_i^{\\star} = \\frac{b_i}{q_i}$.\n\nThe damped Newton step is defined by $x^{+} = x + p(x)$, where $p(x)$ is the solution to:\n$$\\left(\\nabla^{2} f(x) + \\mu I\\right) p(x) = -\\nabla f(x)$$\nSubstituting the expressions for the gradient and Hessian:\n$$\\left(\\text{diag}(q_1, \\dots, q_n) + \\mu \\text{diag}(1, \\dots, 1)\\right) p(x) = -[q_1 x_1 - b_1, \\dots, q_n x_n - b_n]^{\\mathsf{T}}$$\n$$\\text{diag}(q_1+\\mu, \\dots, q_n+\\mu) p(x) = -[q_1 x_1 - b_1, \\dots, q_n x_n - b_n]^{\\mathsf{T}}$$\nThis diagonal system can be solved componentwise for $p_i(x)$:\n$$(q_i+\\mu) p_i(x) = -(q_i x_i - b_i) \\implies p_i(x) = -\\frac{q_i x_i - b_i}{q_i + \\mu}$$\nThe update for the $i$-th coordinate is $x_i^{+} = x_i + p_i(x) = x_i - \\frac{q_i x_i - b_i}{q_i + \\mu}$.\n\nThe error in coordinate $i$ is $e_i = x_i - x_i^{\\star}$. The updated error is $e_i^{+} = x_i^{+} - x_i^{\\star}$.\nWe express $e_i^{+}$ in terms of $e_i$:\n$$e_i^{+} = \\left(x_i - \\frac{q_i x_i - b_i}{q_i + \\mu}\\right) - x_i^{\\star}$$\nRearranging terms for $x_i$:\n$$e_i^{+} = \\frac{(q_i+\\mu)x_i - (q_i x_i - b_i)}{q_i+\\mu} - x_i^{\\star} = \\frac{\\mu x_i + b_i}{q_i + \\mu} - x_i^{\\star}$$\nSubstitute $x_i = e_i + x_i^{\\star}$:\n$$e_i^{+} = \\frac{\\mu (e_i + x_i^{\\star}) + b_i}{q_i + \\mu} - x_i^{\\star}$$\n$$e_i^{+} = \\frac{\\mu e_i}{q_i + \\mu} + \\frac{\\mu x_i^{\\star} + b_i}{q_i + \\mu} - x_i^{\\star}$$\nUsing $x_i^{\\star} = b_i/q_i$, the second term becomes:\n$$\\frac{\\mu (b_i/q_i) + b_i}{q_i + \\mu} = \\frac{b_i (\\mu/q_i + 1)}{q_i + \\mu} = \\frac{b_i (\\frac{\\mu+q_i}{q_i})}{q_i + \\mu} = \\frac{b_i}{q_i} = x_i^{\\star}$$\nSubstituting this back into the expression for $e_i^{+}$:\n$$e_i^{+} = \\frac{\\mu e_i}{q_i + \\mu} + x_i^{\\star} - x_i^{\\star} = \\frac{\\mu}{q_i + \\mu} e_i$$\nThis is the per-coordinate error update: $e_i^{+} = \\left(\\frac{\\mu}{q_i + \\mu}\\right) e_i$.\n\nThe local linear convergence rate for the $i$-th coordinate is controlled by the factor $\\rho_i = \\left|\\frac{\\mu}{q_i+\\mu}\\right|$. Since $\\mu  0$ and $q_i  0$, we have $0  \\mu  q_i + \\mu$, which implies $0  \\rho_i  1$. So, $\\rho_i = \\frac{\\mu}{q_i+\\mu}$.\nA faster convergence rate corresponds to a smaller value of the rate factor $\\rho$. Coordinate $i$ converges strictly faster than coordinate $j$ if $\\rho_i  \\rho_j$.\n$$\\frac{\\mu}{q_i + \\mu}  \\frac{\\mu}{q_j + \\mu}$$\nSince $\\mu  0$, we can divide both sides by $\\mu$:\n$$\\frac{1}{q_i + \\mu}  \\frac{1}{q_j + \\mu}$$\nBecause both denominators are positive, we can take the reciprocal of both sides and reverse the inequality:\n$$q_i + \\mu  q_j + \\mu$$\nSubtracting $\\mu$ from both sides gives the condition:\n$$q_i  q_j$$\nTherefore, coordinate $i$ exhibits a strictly faster local linear convergence rate than coordinate $j$ if and only if its corresponding quadratic-term coefficient $q_i$ is strictly greater than $q_j$.\n\n**Task C: Numerical Computation**\n\nWe are given the concrete instance with $n=3$ and:\n- $q = (q_1, q_2, q_3) = (2, \\frac{1}{2}, 8)$\n- $b = (b_1, b_2, b_3) = (1, -2, 4)$\n- $x = (x_1, x_2, x_3) = (0.2, -3, 0.5)$\n\nWe use the formula derived in Task A: $\\lambda(x)^2 = \\sum_{i=1}^{3} \\frac{(q_i x_i - b_i)^2}{q_i}$.\n\nWe compute the contribution from each coordinate:\n- For $i=1$:\n$$\\frac{(q_1 x_1 - b_1)^2}{q_1} = \\frac{(2 \\times 0.2 - 1)^2}{2} = \\frac{(0.4 - 1)^2}{2} = \\frac{(-0.6)^2}{2} = \\frac{0.36}{2} = 0.18$$\n- For $i=2$:\n$$\\frac{(q_2 x_2 - b_2)^2}{q_2} = \\frac{(\\frac{1}{2} \\times (-3) - (-2))^2}{\\frac{1}{2}} = \\frac{(-1.5 + 2)^2}{0.5} = \\frac{(0.5)^2}{0.5} = \\frac{0.25}{0.5} = 0.5$$\n- For $i=3$:\n$$\\frac{(q_3 x_3 - b_3)^2}{q_3} = \\frac{(8 \\times 0.5 - 4)^2}{8} = \\frac{(4 - 4)^2}{8} = \\frac{0^2}{8} = 0$$\n\nThe sum gives $\\lambda(x)^2$:\n$$\\lambda(x)^2 = 0.18 + 0.5 + 0 = 0.68$$\nThe Newton decrement $\\lambda(x)$ is the square root of this value:\n$$\\lambda(x) = \\sqrt{0.68} \\approx 0.824621125\\dots$$\nRounding to four significant figures, we get $0.8246$.",
            "answer": "$$\\boxed{0.8246}$$"
        },
        {
            "introduction": "The promise of Newton's method lies in its remarkably fast local quadratic convergence. This rapid convergence, however, is contingent on using the exact Hessian matrix, which can be computationally expensive or inaccessible in many real-world applications. This thought experiment  illuminates the consequences of this practical challenge by having you analyze an iteration with a fixed, incorrect Hessian. You will discover how this modeling error degrades the convergence rate from quadratic to linear and then explore a suite of corrective strategies, providing insight into the design of powerful modern algorithms like quasi-Newton and inexact Newton methods.",
            "id": "3156836",
            "problem": "Consider a twice continuously differentiable function $f:\\mathbb{R}^2\\to\\mathbb{R}$ with a unique minimizer at $x^\\star$, and define the Newton decrement at a point $x$ by $\\lambda(x)=\\sqrt{\\nabla f(x)^\\top\\left[\\nabla^2 f(x)\\right]^{-1}\\nabla f(x)}$. Assume the Hessian is locally Lipschitz continuous near $x^\\star$. You will construct an explicit example in which the Newton decrement stagnates due to a persistent modeling error in the Hessian, then identify remedies that restore quadratic local convergence.\n\nLet\n$$\nf(x_1,x_2)=\\frac{1}{2}x_1^2+2x_2^2,\n$$\nso that $\\nabla f(x_1,x_2)=\\begin{bmatrix}x_1\\\\4x_2\\end{bmatrix}$ and $\\nabla^2 f(x_1,x_2)=\\mathbf{H}^\\star=\\operatorname{diag}(1,4)$ for all $(x_1,x_2)\\in\\mathbb{R}^2$. Consider a Hessian approximation $\\mathbf{B}=\\operatorname{diag}(1,8)$ that is kept fixed and used to generate steps $p_k$ via the model system $\\mathbf{B}\\,p_k=-\\nabla f(x_k)$ with the unit step update $x_{k+1}=x_k+p_k$. Take the initial point $x_0=(0,1)^\\top$.\n\nBased on first principles for $\\nabla f$, $\\nabla^2 f$, and $\\lambda(x)$, analyze how $\\lambda(x_k)$ evolves under this approximate-Hessian iteration and determine whether it stagnates (i.e., decreases only linearly rather than quadratically). Then, select all actions below that would restore quadratic local convergence of $\\lambda(x_k)$ for functions like this under the stated smoothness assumptions.\n\nA. Replace the approximate Hessian $\\mathbf{B}$ by the exact Hessian $\\nabla^2 f(x_k)$ once $\\lambda(x_k)$ is sufficiently small, and take full Newton steps; more generally ensure $\\mathbf{B}_k\\to\\nabla^2 f(x^\\star)$ as $k\\to\\infty$.\n\nB. Keep $\\mathbf{B}=\\operatorname{diag}(1,8)$ fixed and use backtracking line search with a constant reduction factor to enforce descent; this will restore quadratic convergence of $\\lambda(x_k)$.\n\nC. Use the exact Hessian $\\nabla^2 f(x_k)$ but solve the Newton linear system $\\nabla^2 f(x_k)\\,p_k=-\\nabla f(x_k)$ approximately with a forcing term $\\eta_k$ that vanishes, for example enforce $\\|\\nabla^2 f(x_k)\\,p_k+\\nabla f(x_k)\\|\\le \\eta_k\\|\\nabla f(x_k)\\|$ with $\\eta_k\\to 0$ as $k\\to\\infty$; then take full steps.\n\nD. Add a constant Tikhonov regularization $\\mu\\mathbf{I}$ with $\\mu0$ to the fixed approximate Hessian and keep it unchanged, i.e., use $(\\mathbf{B}+\\mu\\mathbf{I})\\,p_k=-\\nabla f(x_k)$ and unit steps; this will restore quadratic convergence of $\\lambda(x_k)$ provided $\\mu$ is large enough.\n\nE. Update $\\mathbf{B}_k$ by the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method with accurate line searches so that $\\mathbf{B}_k\\to\\nabla^2 f(x^\\star)$, and switch to exact Newton when $\\lambda(x_k)$ is small; this restores quadratic local convergence.\n\nSelect all correct options.",
            "solution": "The problem statement is analyzed for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- The function to be minimized is $f:\\mathbb{R}^2\\to\\mathbb{R}$, which is twice continuously differentiable.\n- There is a unique minimizer at $x^\\star$.\n- The Newton decrement is defined as $\\lambda(x)=\\sqrt{\\nabla f(x)^\\top\\left[\\nabla^2 f(x)\\right]^{-1}\\nabla f(x)}$.\n- The Hessian $\\nabla^2 f(x)$ is assumed to be locally Lipschitz continuous near $x^\\star$.\n- The specific function is $f(x_1,x_2)=\\frac{1}{2}x_1^2+2x_2^2$.\n- The gradient is $\\nabla f(x_1,x_2)=\\begin{bmatrix}x_1\\\\4x_2\\end{bmatrix}$.\n- The exact Hessian is constant: $\\nabla^2 f(x_1,x_2)=\\mathbf{H}^\\star=\\operatorname{diag}(1,4)$.\n- A fixed approximate Hessian is used: $\\mathbf{B}=\\operatorname{diag}(1,8)$.\n- The iteration is defined by the linear system $\\mathbf{B}\\,p_k=-\\nabla f(x_k)$ with a unit step update $x_{k+1}=x_k+p_k$.\n- The initial point is $x_0=(0,1)^\\top$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded (Critical)**: The problem is a standard exercise in numerical optimization, dealing with the convergence properties of Newton-like methods. All concepts—gradient, Hessian, Newton decrement, quadratic/linear convergence—are fundamental to the field.\n- **Well-Posed**: The problem is well-posed. The function is a simple convex quadratic, for which a unique minimizer exists. The iterative scheme is deterministic, generating a unique sequence of points. The questions asked are precise and have definite answers based on established optimization theory.\n- **Objective (Critical)**: The problem is stated using precise mathematical language and definitions. There are no subjective or ambiguous terms.\n- The problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or infeasibility.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be derived.\n\n### Derivation and Analysis\n\nFirst, we analyze the behavior of the given iteration. The minimizer of $f(x_1,x_2)=\\frac{1}{2}x_1^2+2x_2^2$ is found by setting the gradient to zero, $\\nabla f(x) = \\begin{bmatrix}x_1\\\\4x_2\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\end{bmatrix}$, which gives the unique minimizer $x^\\star=(0,0)^\\top$. The exact Hessian is $\\mathbf{H}^\\star = \\nabla^2 f(x^\\star) = \\operatorname{diag}(1,4)$.\n\nThe iteration uses the fixed approximate Hessian $\\mathbf{B}=\\operatorname{diag}(1,8)$. The update rule is $x_{k+1}=x_k+p_k$, where $\\mathbf{B}\\,p_k=-\\nabla f(x_k)$. This can be written as:\n$$\nx_{k+1} = x_k - \\mathbf{B}^{-1} \\nabla f(x_k)\n$$\nWith $\\mathbf{B}^{-1} = \\operatorname{diag}(1, 1/8)$, the iteration for a point $x_k = (x_{k,1}, x_{k,2})^\\top$ is:\n$$\n\\begin{bmatrix} x_{k+1,1} \\\\ x_{k+1,2} \\end{bmatrix} = \\begin{bmatrix} x_{k,1} \\\\ x_{k,2} \\end{bmatrix} - \\begin{bmatrix} 1  0 \\\\ 0  1/8 \\end{bmatrix} \\begin{bmatrix} x_{k,1} \\\\ 4x_{k,2} \\end{bmatrix} = \\begin{bmatrix} x_{k,1} \\\\ x_{k,2} \\end{bmatrix} - \\begin{bmatrix} x_{k,1} \\\\ \\frac{1}{2}x_{k,2} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\frac{1}{2}x_{k,2} \\end{bmatrix}\n$$\nStarting with $x_0=(0,1)^\\top$, we can generate the sequence of iterates:\n$x_0 = (0,1)^\\top$\n$x_1 = (0, 1/2)^\\top$\n$x_2 = (0, 1/4)^\\top$\nIn general, $x_k = (0, (1/2)^k)^\\top$. The sequence converges to the minimizer $x^\\star=(0,0)^\\top$.\n\nNext, we analyze the Newton decrement $\\lambda(x_k)$. The definition uses the *exact* Hessian $\\mathbf{H}^\\star$:\n$$\n\\lambda(x)^2 = \\nabla f(x)^\\top [\\mathbf{H}^\\star]^{-1} \\nabla f(x)\n$$\nWe have $\\nabla f(x_k) = \\begin{bmatrix} 0 \\\\ 4x_{k,2} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 4(1/2)^k \\end{bmatrix}$ and $[\\mathbf{H}^\\star]^{-1} = \\operatorname{diag}(1, 1/4)$.\n$$\n\\lambda(x_k)^2 = \\begin{bmatrix} 0  4(1/2)^k \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 0  1/4 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 4(1/2)^k \\end{bmatrix} = \\begin{bmatrix} 0  (1/2)^k \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 4(1/2)^k \\end{bmatrix} = 4 \\left( \\frac{1}{2} \\right)^{2k} = 4 \\left( \\frac{1}{4} \\right)^k\n$$\nTherefore, the Newton decrement at iterate $k$ is $\\lambda(x_k) = \\sqrt{4 (1/4)^k} = 2 (1/2)^k$.\n\nTo determine the convergence rate, we examine the ratio of successive terms:\n$$\n\\frac{\\lambda(x_{k+1})}{\\lambda(x_k)} = \\frac{2(1/2)^{k+1}}{2(1/2)^k} = \\frac{1}{2}\n$$\nSince this ratio is a constant less than $1$, the convergence of $\\lambda(x_k)$ is linear with a rate of $1/2$. It is not quadratic. For quadratic convergence, we would need $\\lambda(x_{k+1}) = O(\\lambda(x_k)^2)$. Here, $\\frac{\\lambda(x_{k+1})}{\\lambda(x_k)^2} = \\frac{2(1/2)^{k+1}}{(2(1/2)^k)^2} = \\frac{(1/2)^k}{4(1/4)^k} = \\frac{(1/2)^k}{4(1/2)^{2k}} = \\frac{1}{4}2^k$, which diverges. Thus, the Newton decrement stagnates, exhibiting only linear convergence.\n\nThe general reason for this is that the method is a stationary iterative method $x_{k+1} = g(x_k)$ with $g(x) = x - \\mathbf{B}^{-1}\\nabla f(x)$. The rate of convergence is determined by the spectral radius of the Jacobian of $g(x)$ at the solution $x^\\star$:\n$$\nJ_g(x^\\star) = \\mathbf{I} - \\mathbf{B}^{-1} \\nabla^2 f(x^\\star) = \\mathbf{I} - \\mathbf{B}^{-1} \\mathbf{H}^\\star\n$$\nFor quadratic convergence, this Jacobian must be the zero matrix, which requires $\\mathbf{B}=\\mathbf{H}^\\star$. Since $\\mathbf{B} \\neq \\mathbf{H}^\\star$, the convergence is at best linear.\n\n### Evaluation of Options\n\nWe now evaluate the proposed actions to restore quadratic local convergence. Quadratic convergence of Newton-type methods, whether for the iterates $x_k$ or the decrement $\\lambda(x_k)$, is achieved when the search direction is a sufficiently close approximation to the exact Newton direction, and a step of length $1$ is taken.\n\n**A. Replace the approximate Hessian $\\mathbf{B}$ by the exact Hessian $\\nabla^2 f(x_k)$ once $\\lambda(x_k)$ is sufficiently small, and take full Newton steps; more generally ensure $\\mathbf{B}_k\\to\\nabla^2 f(x^\\star)$ as $k\\to\\infty$.**\n\nThis describes switching to the pure Newton method, $x_{k+1} = x_k - [\\nabla^2 f(x_k)]^{-1}\\nabla f(x_k)$. Under the stated assumptions (twice continuously differentiable $f$ and locally Lipschitz Hessian), the pure Newton method is guaranteed to converge quadratically provided the starting point is sufficiently close to the solution $x^\\star$. A small value of the Newton decrement $\\lambda(x_k)$ is a standard and effective criterion for being \"sufficiently close\". The second part of the statement, $\\mathbf{B}_k\\to\\nabla^2 f(x^\\star)$, is a condition for superlinear convergence. Quadratic convergence requires the approximation to be more accurate, e.g., $\\|\\mathbf{B}_k - \\nabla^2 f(x_k)\\| = O(\\|x_k - x^\\star\\|)$. However, the primary action proposed—switching to the exact Hessian—is the canonical method to achieve quadratic convergence.\n- Verdict: **Correct**.\n\n**B. Keep $\\mathbf{B}=\\operatorname{diag}(1,8)$ fixed and use backtracking line search with a constant reduction factor to enforce descent; this will restore quadratic convergence of $\\lambda(x_k)$.**\n\nThe fundamental reason for the loss of quadratic convergence is the use of an incorrect Hessian approximation $\\mathbf{B} \\neq \\mathbf{H}^\\star$, which results in a search direction $p_k$ that is not the true Newton direction. A line search algorithm, like backtracking, only adjusts the step length $\\alpha_k$ along this suboptimal direction. It cannot correct the direction itself. While a line search can improve the global convergence properties of the algorithm, it cannot change the local convergence rate from linear to quadratic. The rate is fundamentally limited by the quality of the search direction, which is fixed and flawed in this proposal.\n- Verdict: **Incorrect**.\n\n**C. Use the exact Hessian $\\nabla^2 f(x_k)$ but solve the Newton linear system $\\nabla^2 f(x_k)\\,p_k=-\\nabla f(x_k)$ approximately with a forcing term $\\eta_k$ that vanishes, for example enforce $\\|\\nabla^2 f(x_k)\\,p_k+\\nabla f(x_k)\\|\\le \\eta_k\\|\\nabla f(x_k)\\|$ with $\\eta_k\\to 0$ as $k\\to\\infty$; then take full steps.**\n\nThis describes the inexact Newton method. The theory of inexact Newton methods states that if the forcing sequence $\\eta_k$ is chosen appropriately, various convergence rates can be achieved. If $\\eta_k \\to 0$, the convergence is superlinear. To achieve quadratic convergence, the forcing term must vanish at a rate proportional to the gradient norm, i.e., $\\eta_k = O(\\|\\nabla f(x_k)\\|)$. Since the framework of inexact Newton methods allows for such a choice of $\\eta_k$, it represents a valid action that can restore quadratic convergence. The example given, $\\eta_k \\to 0$, is the general condition for superlinear convergence, and the action of choosing a faster rate within this framework leads to quadratic convergence.\n- Verdict: **Correct**.\n\n**D. Add a constant Tikhonov regularization $\\mu\\mathbf{I}$ with $\\mu0$ to the fixed approximate Hessian and keep it unchanged, i.e., use $(\\mathbf{B}+\\mu\\mathbf{I})\\,p_k=-\\nabla f(x_k)$ and unit steps; this will restore quadratic convergence of $\\lambda(x_k)$ provided $\\mu$ is large enough.**\n\nThis approach uses a new fixed Hessian approximation $\\mathbf{B}' = \\mathbf{B} + \\mu\\mathbf{I}$. For the same reason the original iteration was linear, this modified iteration will also be linear. The convergence rate is determined by the Jacobian $\\mathbf{I} - (\\mathbf{B}+\\mu\\mathbf{I})^{-1}\\mathbf{H}^\\star$. For quadratic convergence, this Jacobian would need to be the zero matrix, which means we need $\\mathbf{B}+\\mu\\mathbf{I} = \\mathbf{H}^\\star$. This is generally impossible for a fixed $\\mathbf{B} \\neq \\mathbf{H}^\\star$ and a scalar $\\mu$. For our specific problem, this would require $\\operatorname{diag}(1+\\mu, 8+\\mu) = \\operatorname{diag}(1,4)$, which is impossible. Simply adding regularization to a fixed incorrect Hessian does not restore quadratic convergence.\n- Verdict: **Incorrect**.\n\n**E. Update $\\mathbf{B}_k$ by the Broyden-Fletcher-Goldfarb-Shanno (BFGS) method with accurate line searches so that $\\mathbf{B}_k\\to\\nabla^2 f(x^\\star)$, and switch to exact Newton when $\\lambda(x_k)$ is small; this restores quadratic local convergence.**\n\nThis describes a hybrid quasi-Newton/Newton strategy. The BFGS method is a quasi-Newton method that builds up an approximation to the Hessian. Under standard assumptions, it achieves superlinear convergence. While BFGS itself is not quadratically convergent, it is very effective at bringing the iterate into the neighborhood of the solution where the pure Newton method converges quadratically. The proposed strategy is to use BFGS to get \"close\" and then switch to the pure Newton method (using the exact Hessian) to finish with quadratic convergence. This is a very common and effective practical implementation. The action of implementing this hybrid algorithm does indeed restore quadratic local convergence.\n- Verdict: **Correct**.",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "Theory provides the foundation, but implementation reveals the practical nuances. This coding exercise  bridges the gap between the abstract concepts of convergence rates and the realities of numerical computation. You will implement Newton's method twice: once with the exact analytical Hessian and again with a finite-difference approximation, a common technique used when analytical derivatives are unavailable. By comparing the resulting trajectories of the Newton decrement, you will directly observe how numerical approximation errors affect the algorithm's local convergence behavior, gaining hands-on experience with the critical trade-offs involved in choosing a finite-difference step size.",
            "id": "3156861",
            "problem": "Consider a twice continuously differentiable function $f:\\mathbb{R}^n\\to\\mathbb{R}$ that is strictly convex and has a unique minimizer $x^\\star$. The Newton method for minimizing $f$ is built from the first and second derivatives at $x$, and its local behavior is often summarized using the Newton decrement, which is a scalar quantity derived from these derivatives. Your task is to study how the Newton decrement trajectory changes when the Hessian is computed exactly versus when it is approximated by finite differences, and to quantify the accuracy needed in the finite-difference step to obtain reliable local convergence rate predictions.\n\nStart from the following fundamental base:\n- The gradient $\\nabla f(x)$ is defined by the first-order partial derivatives of $f$.\n- The Hessian $\\nabla^2 f(x)$ is defined by the second-order partial derivatives of $f$ and is symmetric for twice continuously differentiable $f$.\n- The Newton step at $x$ solves a linear system formed by the Hessian and the gradient, derived from the second-order Taylor approximation of $f$ around $x$.\n\nWork with the specific, smooth, strictly convex function in dimension $n=2$:\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\, x^\\top Q\\, x \\;+\\; \\mu \\sum_{j=1}^{2} \\cosh(x_j),\n$$\nwhere $x=(x_1,x_2)^\\top$, $Q\\in\\mathbb{R}^{2\\times 2}$ is symmetric positive definite, and $\\mu0$. In this problem, use\n$$\nQ \\;=\\; \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}, \\quad \\mu \\;=\\; 0.2, \\quad x_0 \\;=\\; \\begin{bmatrix} 0.4 \\\\ -0.4 \\end{bmatrix}.\n$$\n\nTasks:\n- Derive $\\nabla f(x)$ and $\\nabla^2 f(x)$ analytically from first principles for the given $f(x)$, and implement the Newton method using the exact Hessian to generate a trajectory $\\{x_k\\}$ starting at $x_0$. At each iterate $x_k$, compute the associated Newton decrement.\n- Approximate the Hessian at $x$ via central finite differences applied to the gradient: for a given step size $h0$, construct an approximation of $\\nabla^2 f(x)$ by perturbing $x$ along each coordinate direction and differencing the resulting gradients. Ensure the approximation is symmetrized, and stabilize it with the minimal diagonal shift required if a nonpositive eigenvalue is detected, so that the linear system for the Newton step is well-posed.\n- Implement a second Newton trajectory $\\{x_k^{\\mathrm{fd}}\\}$ starting at $x_0$ but using the finite-difference Hessian with step size $h$ (the gradient remains exact). At each iterate, compute the associated decrement from the finite-difference Hessian.\n- To analyze local convergence rate predictions, consider the sequence values of the decrements $\\{\\lambda_k\\}$ and $\\{\\lambda_k^{\\mathrm{fd}}\\}$ along the two trajectories. In the local neighborhood where the exact decrement is small, the Newton method exhibits a characteristic acceleration. Quantify the predicted local rate by comparing the sequences $\\left\\{ \\lambda_{k+1} / \\lambda_k^2 \\right\\}$ built from each trajectory. Use only indices $k$ for which the exact decrement at iteration $k$ satisfies $\\lambda_k \\le 10^{-3}$ and for which both trajectories have the needed next iterate available; if no such indices exist, instead use the last available $3$ consecutive indices common to both trajectories or as many as available if fewer than $3$ exist.\n- For each tested $h$, compute a single scalar metric $E(h)$ defined as the maximum relative deviation between the two sequences of local rate predictors over the chosen index set:\n$$\nE(h) \\;=\\; \\max_k \\frac{\\left| \\left( \\lambda_{k+1}^{\\mathrm{fd}} / \\left(\\lambda_k^{\\mathrm{fd}}\\right)^2 \\right) - \\left( \\lambda_{k+1} / \\lambda_k^2 \\right) \\right|}{\\max\\!\\left(10^{-12}, \\left| \\lambda_{k+1} / \\lambda_k^2 \\right| \\right)}.\n$$\nThis metric captures how well the finite-difference Hessian reproduces the local rate predicted by the exact Hessian.\n\nImplementation details to follow in your program:\n- Use the exact analytical $\\nabla f(x)$ and $\\nabla^2 f(x)$ for the exact trajectory.\n- Build the finite-difference Hessian approximation at $x$ by central differencing columns of the gradient with step size $h$: for each coordinate $j$, set the $j$-th column to $\\left( \\nabla f(x + h e_j) - \\nabla f(x - h e_j) \\right)/(2h)$, where $e_j$ is the $j$-th standard basis vector in $\\mathbb{R}^2$, then symmetrize by averaging with its transpose.\n- Stabilize the finite-difference Hessian $H_{\\mathrm{fd}}$ by adding a diagonal shift $\\alpha I$ with the smallest $\\alpha\\ge 0$ that makes the minimal eigenvalue strictly greater than $10^{-12}$ if needed.\n- Run both Newton trajectories using full steps without line search for up to $50$ iterations or until the exact decrement reaches $10^{-12}$, whichever occurs first.\n\nTest suite:\n- Use the four finite-difference step sizes $h \\in \\{10^{-1}, 10^{-4}, 10^{-6}, 10^{-8}\\}$ with the fixed $Q$, $\\mu$, and $x_0$ specified above. These choices cover a general case, two high-accuracy regimes, and a boundary condition near double-precision limits.\n- For each $h$, compute $E(h)$ as described.\n\nRequired final output format:\n- Your program should produce a single line of output containing the four computed results in the order of $h = 10^{-1}, 10^{-4}, 10^{-6}, 10^{-8}$ as a comma-separated list enclosed in square brackets, for example $[r_1,r_2,r_3,r_4]$. Each $r_i$ must be a real number (float). No physical units or angles are involved in this problem, so no unit conversions are required.",
            "solution": "The problem as stated is formally analyzed and validated.\n\n### Step 1: Extract Givens\n- **Objective function**: $f(x) = \\tfrac{1}{2}\\, x^\\top Q\\, x + \\mu \\sum_{j=1}^{2} \\cosh(x_j)$, for $x \\in \\mathbb{R}^2$.\n- **Parameters**:\n  - $Q = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}$\n  - $\\mu = 0.2$\n  - Initial point: $x_0 = \\begin{bmatrix} 0.4 \\\\ -0.4 \\end{bmatrix}$\n- **Newton Method (Exact)**: Based on analytical gradient $\\nabla f(x)$ and Hessian $\\nabla^2 f(x)$.\n- **Newton Method (Finite-Difference)**: Based on analytical gradient $\\nabla f(x)$ and an approximated Hessian $H_{\\mathrm{fd}}(x)$.\n- **Hessian Approximation**:\n  - The $j$-th column is computed via central differences on the gradient: $(\\nabla f(x + h e_j) - \\nabla f(x - h e_j)) / (2h)$.\n  - The resulting matrix is symmetrized.\n  - The symmetrized matrix is stabilized by adding $\\alpha I$ if its minimal eigenvalue is not strictly greater than $10^{-12}$, where $\\alpha \\ge 0$ is the minimum required shift.\n- **Newton Decrement**: $\\lambda(x) = \\sqrt{\\nabla f(x)^\\top H(x)^{-1} \\nabla f(x)}$, where $H$ is the exact or approximate Hessian.\n- **Trajectory Generation**:\n  - Both exact and finite-difference trajectories are run for up to $50$ iterations.\n  - The stopping condition for both is when the exact decrement $\\lambda_k$ falls below $10^{-12}$.\n- **Local Rate Predictor**: The quantity $\\lambda_{k+1}/\\lambda_k^2$ for the exact trajectory, and $\\lambda_{k+1}^{\\mathrm{fd}}/(\\lambda_k^{\\mathrm{fd}})^2$ for the finite-difference trajectory.\n- **Index Set for Analysis**: The set of indices $k$ over which the rate predictors are compared is determined by the rule:\n  1. Primary: All $k$ such that the exact decrement $\\lambda_k \\le 10^{-3}$ and the $(k+1)$-th iterate is available for both trajectories.\n  2. Fallback: If the primary set is empty, use the last $3$ available consecutive indices, or fewer if not all $3$ are available.\n- **Error Metric**: $E(h) = \\max_k \\frac{\\left| \\left( \\lambda_{k+1}^{\\mathrm{fd}} / \\left(\\lambda_k^{\\mathrm{fd}}\\right)^2 \\right) - \\left( \\lambda_{k+1} / \\lambda_k^2 \\right) \\right|}{\\max\\!\\left(10^{-12}, \\left| \\lambda_{k+1} / \\lambda_k^2 \\right| \\right)}$, maximized over the selected index set.\n- **Test Suite**: Finite-difference step sizes $h \\in \\{10^{-1}, 10^{-4}, 10^{-6}, 10^{-8}\\}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is a standard exercise in numerical optimization, a core topic in applied mathematics and scientific computing. The function $f(x)$ is strictly convex because $Q$ is positive definite (its eigenvalues are $\\frac{7 \\pm \\sqrt{(4-3)^2 + 4(1)^2}}{2} = \\frac{7 \\pm \\sqrt{17}}{2}$, which are both positive) and the term $\\mu \\sum \\cosh(x_j)$ is also convex (its Hessian is a diagonal matrix with positive entries $\\mu\\cosh(x_j)0$). Therefore, a unique minimizer exists. The Newton method, Newton decrement, and finite-difference approximations are all fundamental, well-established concepts.\n- **Well-Posed**: The problem is well-posed. All parameters and procedures are explicitly defined. The convexity of $f$ guarantees the existence and uniqueness of a solution to the minimization problem. The stabilization procedure for the approximate Hessian ensures that the linear system for the Newton step is always well-posed (i.e., the matrix is invertible). The error metric is unambiguously defined.\n- **Objective**: The problem is stated using precise mathematical language, free from any subjective or opinion-based claims.\n- **Other Flaws**: The problem is self-contained, consistent, and does not suffer from any of the other listed flaws (e.g., incompleteness, contradiction, triviality).\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete, reasoned solution will be provided.\n\n**Analytical Derivatives of the Objective Function**\nThe objective function is given by $f(x) = \\tfrac{1}{2}\\, x^\\top Q\\, x + \\mu \\sum_{j=1}^{2} \\cosh(x_j)$, where $x = (x_1, x_2)^\\top$.\nThe gradient of $f(x)$ is the vector of its first partial derivatives. The gradient of the quadratic term $\\tfrac{1}{2} x^\\top Q x$ is $Qx$ since $Q$ is symmetric. The gradient of the hyperbolic cosine term is a vector with elements $\\mu \\sinh(x_j)$.\nThus, the gradient is:\n$$\n\\nabla f(x) = Qx + \\mu \\begin{pmatrix} \\sinh(x_1) \\\\ \\sinh(x_2) \\end{pmatrix} = \\begin{pmatrix} 4x_1 + x_2 + \\mu \\sinh(x_1) \\\\ x_1 + 3x_2 + \\mu \\sinh(x_2) \\end{pmatrix}\n$$\nThe Hessian of $f(x)$ is the matrix of its second partial derivatives. The Hessian of the quadratic term is the constant matrix $Q$. The Hessian of the hyperbolic cosine term is a diagonal matrix with elements $\\mu \\cosh(x_j)$.\nThus, the Hessian is:\n$$\n\\nabla^2 f(x) = Q + \\mu \\begin{pmatrix} \\cosh(x_1)  0 \\\\ 0  \\cosh(x_2) \\end{pmatrix} = \\begin{pmatrix} 4 + \\mu \\cosh(x_1)  1 \\\\ 1  3 + \\mu \\cosh(x_2) \\end{pmatrix}\n$$\nSince $Q$ is positive definite and $\\mu\\cosh(x_j)  0$ for all $x_j$, the Hessian $\\nabla^2 f(x)$ is positive definite for all $x \\in \\mathbb{R}^2$.\n\n**Newton's Method Trajectories**\nFor both trajectories, we start at $x_0 = (0.4, -0.4)^\\top$. At each iteration $k$, we compute a search direction and update the current point.\nFor the exact Newton method, the search direction $\\Delta x_k$ is found by solving the linear system:\n$$\n\\nabla^2 f(x_k) \\Delta x_k = -\\nabla f(x_k)\n$$\nFor the finite-difference Newton method, the search direction $\\Delta x_k^{\\mathrm{fd}}$ is found by solving:\n$$\nH_{\\mathrm{stab}}(x_k^{\\mathrm{fd}}) \\Delta x_k^{\\mathrm{fd}} = -\\nabla f(x_k^{\\mathrm{fd}})\n$$\nwhere $H_{\\mathrm{stab}}$ is the stabilized, symmetrized, finite-difference approximation of the Hessian. The $j$-th column of the initial approximation matrix $H_{\\mathrm{approx}}$ is given by:\n$$\n(H_{\\mathrm{approx}})_{\\cdot, j} = \\frac{\\nabla f(x + h e_j) - \\nabla f(x - h e_j)}{2h}\n$$\nThis matrix is then symmetrized: $H_{\\mathrm{fd}} = \\frac{1}{2}(H_{\\mathrm{approx}} + H_{\\mathrm{approx}}^\\top)$. To ensure positive definiteness, we compute the smallest eigenvalue $\\lambda_{\\min}$ of $H_{\\mathrm{fd}}$. If $\\lambda_{\\min} \\le 10^{-12}$, we compute a shift $\\alpha = \\max(0, 1.1 \\times 10^{-12} - \\lambda_{\\min})$ and set $H_{\\mathrm{stab}} = H_{\\mathrm{fd}} + \\alpha I$. Otherwise, $H_{\\mathrm{stab}} = H_{\\mathrm{fd}}$.\nThe iterates are updated with a full step: $x_{k+1} = x_k + \\Delta x_k$ and $x_{k+1}^{\\mathrm{fd}} = x_k^{\\mathrm{fd}} + \\Delta x_k^{\\mathrm{fd}}$.\n\n**Newton Decrement and Convergence Analysis**\nThe Newton decrement at step $k$ for the exact method is $\\lambda_k = \\sqrt{-\\nabla f(x_k)^\\top \\Delta x_k}$. Similarly, for the finite-difference method, $\\lambda_k^{\\mathrm{fd}} = \\sqrt{-\\nabla f(x_k^{\\mathrm{fd}})^\\top \\Delta x_k^{\\mathrm{fd}}}$.\nThe trajectories are generated in lock-step for a maximum of $50$ iterations, terminating when $\\lambda_k \\le 10^{-12}$.\n\nTo assess the quality of the local convergence rate prediction, we compare the sequences of rate predictors, $R_k = \\lambda_{k+1}/\\lambda_k^2$ and $R_k^{\\mathrm{fd}} = \\lambda_{k+1}^{\\mathrm{fd}}/(\\lambda_k^{\\mathrm{fd}})^2$, over a specific set of indices. The indices $k$ are chosen where the method is in the region of quadratic convergence, identified by $\\lambda_k \\le 10^{-3}$ for the exact method. If no such iterates are produced, a fallback of the last three available consecutive indices is used. The error metric $E(h)$ is the maximum relative deviation between these two sequences over the chosen indices, providing a quantitative measure of how the finite-difference approximation affects the predicted dynamics near the minimizer. The algorithm proceeds by iterating through each specified value of $h$, generating both trajectories, computing the rate predictor sequences, and finally calculating the error metric $E(h)$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the error metric E(h) for different finite-difference step sizes h.\n    \"\"\"\n    Q = np.array([[4.0, 1.0], [1.0, 3.0]])\n    mu = 0.2\n    x0 = np.array([0.4, -0.4])\n    h_values = [1e-1, 1e-4, 1e-6, 1e-8]\n    \n    max_iter = 50\n    tol_decrement = 1e-12\n    n_dim = 2\n\n    def grad_f(x):\n        \"\"\"Computes the analytical gradient of f(x).\"\"\"\n        return Q @ x + mu * np.sinh(x)\n\n    def hess_f(x):\n        \"\"\"Computes the analytical Hessian of f(x).\"\"\"\n        return Q + mu * np.diag(np.cosh(x))\n\n    results = []\n    \n    for h in h_values:\n        # Initialize trajectories\n        x_exact = x0.copy()\n        x_fd = x0.copy()\n        \n        lams_exact = []\n        lams_fd = []\n        \n        for _ in range(max_iter):\n            # --- Exact Newton Step ---\n            g_exact = grad_f(x_exact)\n            H_exact = hess_f(x_exact)\n            \n            # Solve for Newton step\n            dx_exact = np.linalg.solve(H_exact, -g_exact)\n            \n            # Compute Newton decrement\n            lam_sq_exact = -g_exact.T @ dx_exact\n            lam_exact = np.sqrt(max(0, lam_sq_exact))\n            lams_exact.append(lam_exact)\n            \n            # --- Finite-Difference Newton Step ---\n            g_fd_pt = grad_f(x_fd)\n            \n            # Construct FD Hessian\n            H_approx = np.zeros_like(Q)\n            for j in range(n_dim):\n                e_j = np.zeros(n_dim)\n                e_j[j] = 1.0\n                g_plus = grad_f(x_fd + h * e_j)\n                g_minus = grad_f(x_fd - h * e_j)\n                H_approx[:, j] = (g_plus - g_minus) / (2 * h)\n            \n            H_fd = 0.5 * (H_approx + H_approx.T)\n            \n            # Stabilize FD Hessian\n            min_eig = np.min(np.linalg.eigvalsh(H_fd))\n            alpha = 0.0\n            # Target is strictly  1e-12. We target 1.1e-12 for robustness.\n            target_min_eig = 1.1e-12\n            if min_eig  target_min_eig:\n                alpha = max(0.0, target_min_eig - min_eig)\n            \n            H_fd_stab = H_fd + alpha * np.eye(n_dim)\n\n            # Solve for Newton step\n            dx_fd = np.linalg.solve(H_fd_stab, -g_fd_pt)\n            \n            # Compute FD Newton decrement\n            lam_sq_fd = -g_fd_pt.T @ dx_fd\n            lam_fd = np.sqrt(max(0, lam_sq_fd))\n            lams_fd.append(lam_fd)\n\n            # Check for termination\n            if lam_exact = tol_decrement:\n                break\n                \n            # Update points\n            x_exact += dx_exact\n            x_fd += dx_fd\n\n        # --- Analyze Convergence Rates ---\n        num_decrements = len(lams_exact)\n        if num_decrements  2:\n            results.append(0.0)\n            continue\n            \n        rate_k_indices = []\n        # Primary rule for index selection\n        for k in range(num_decrements - 1):\n            if lams_exact[k] = 1e-3:\n                rate_k_indices.append(k)\n\n        # Fallback rule if primary set is empty\n        if not rate_k_indices:\n            num_computable_rates = num_decrements - 1\n            start_k = max(0, num_computable_rates - 3)\n            rate_k_indices = list(range(start_k, num_computable_rates))\n            \n        deviations = []\n        if rate_k_indices:\n            for k in rate_k_indices:\n                # Handle potential division by zero for decrement^2, although unlikely here\n                if lams_exact[k]  1e-15 or lams_fd[k]  1e-15:\n                    continue\n\n                rate_exact = lams_exact[k+1] / (lams_exact[k]**2)\n                rate_fd = lams_fd[k+1] / (lams_fd[k]**2)\n                \n                numerator = abs(rate_fd - rate_exact)\n                denominator = max(1e-12, abs(rate_exact))\n                \n                deviations.append(numerator / denominator)\n\n        E_h = max(deviations) if deviations else 0.0\n        results.append(E_h)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}