{
    "hands_on_practices": [
        {
            "introduction": "This problem provides a foundational exercise in applying the logarithmic barrier method. By working through a simple, two-dimensional linear program from start to finish, you will see exactly how the barrier term transforms a constrained problem into an unconstrained one. This practice will solidify your understanding of how to derive the central path and verify its convergence to the true optimal solution, bridging the gap between abstract theory and a concrete calculation .",
            "id": "3145963",
            "problem": "Consider the constrained optimization problem in two variables $x$ and $y$:\nminimize the linear objective $f(x,y) = x + y$ subject to the inequality constraints $x \\geq 0$, $y \\geq 0$, and $x + y \\leq 1$. The logarithmic barrier method replaces the inequality constraints with a barrier term and considers, for a parameter $\\mu  0$, the barrier subproblem \n$$\n\\min_{x,y} \\;\\; F_{\\mu}(x,y) := x + y - \\mu\\left(\\ln x + \\ln y + \\ln(1 - x - y)\\right)\n$$\nover the open domain $\\{(x,y) \\in \\mathbb{R}^{2} : x  0,\\; y  0,\\; x + y  1\\}$.\n\nUsing only foundational facts from multivariable calculus and convex analysis—namely, that the natural logarithm $\\ln(\\cdot)$ is strictly concave on $(0,\\infty)$, that a nonnegative weighted sum of strictly concave functions is strictly concave, and that adding a linear function preserves strict convexity when the sign is reversed in a barrier—carry out the following:\n\n- Establish that $F_{\\mu}(x,y)$ is strictly convex on its domain, so its unique minimizer is characterized by the vanishing of its gradient.\n- Derive, in closed form, the unique minimizer $(x_{\\mu}, y_{\\mu})$ of the barrier subproblem as an explicit function of $\\mu$.\n- Determine the constrained optimum $(x^{\\star}, y^{\\star})$ of the original problem and verify, by taking the limit as $\\mu \\to 0$, that $(x_{\\mu}, y_{\\mu})$ approaches $(x^{\\star}, y^{\\star})$.\n\nProvide your final answer as the single expression for $(x_{\\mu}, y_{\\mu})$ in terms of $\\mu$. No rounding is required, and no units are involved.",
            "solution": "The problem is mathematically well-posed, self-contained, and scientifically sound within the domain of convex optimization. All provided information is consistent and sufficient for deriving a unique solution. The problem is therefore valid.\n\nThe task is to analyze a logarithmic barrier subproblem for a given linear program. The original constrained problem is:\n$$\n\\text{minimize } f(x,y) = x + y \\\\\n\\text{subject to } x \\geq 0, y \\geq 0, x + y \\leq 1\n$$\nThe barrier subproblem for a parameter $\\mu  0$ is to minimize the function:\n$$\nF_{\\mu}(x,y) = x + y - \\mu\\left(\\ln x + \\ln y + \\ln(1 - x - y)\\right)\n$$\nover the open convex set $\\mathcal{D} = \\{(x,y) \\in \\mathbb{R}^{2} : x  0, y  0, x + y  1\\}$.\n\nFirst, we establish that $F_{\\mu}(x,y)$ is strictly convex on its domain $\\mathcal{D}$. The function $F_{\\mu}(x,y)$ is a sum of four component functions: $f_0(x,y) = x+y$, $f_1(x) = -\\mu \\ln x$, $f_2(y) = -\\mu \\ln y$, and $f_3(x,y) = -\\mu \\ln(1-x-y)$.\n1.  The function $f_0(x,y) = x+y$ is linear, and is therefore convex.\n2.  The function $f_1(x) = -\\mu \\ln x$ is a function of $x$ alone. Its second derivative is $f_1''(x) = \\frac{\\mu}{x^2}$. Since $\\mu  0$ and $x  0$ on $\\mathcal{D}$, we have $f_1''(x)  0$. Thus, $f_1(x)$ is a strictly convex function of $x$. As a function of $(x,y)$, it is strictly convex.\n3.  Similarly, $f_2(y) = -\\mu \\ln y$ is a strictly convex function of $y$, and hence a strictly convex function of $(x,y)$.\n4.  The function $f_3(x,y) = -\\mu \\ln(1-x-y)$ is the composition of the convex function $h(z) = -\\mu \\ln z$ and the affine mapping $g(x,y) = 1-x-y$. A composition of a convex function with an affine mapping is convex.\nThe function $F_{\\mu}(x,y)$ is the sum of these four functions. The sum of several convex functions and at least one strictly convex function is strictly convex. Since $F_{\\mu}(x,y)$ includes the strictly convex functions $f_1(x)$ and $f_2(y)$, it is strictly convex on the convex domain $\\mathcal{D}$.\n\nA strictly convex function over a convex domain has at most one minimizer. Since $F_{\\mu}(x,y)$ is differentiable, its unique minimizer $(x_{\\mu}, y_{\\mu})$ must satisfy the first-order optimality condition, where the gradient vanishes: $\\nabla F_{\\mu}(x,y) = 0$.\nThe partial derivatives of $F_{\\mu}(x,y)$ are:\n$$\n\\frac{\\partial F_{\\mu}}{\\partial x} = 1 - \\frac{\\mu}{x} + \\frac{\\mu}{1 - x - y}\n$$\n$$\n\\frac{\\partial F_{\\mu}}{\\partial y} = 1 - \\frac{\\mu}{y} + \\frac{\\mu}{1 - x - y}\n$$\nSetting the gradient to zero gives the system of equations:\n$$\n1 - \\frac{\\mu}{x} + \\frac{\\mu}{1 - x - y} = 0 \\quad (1)\n$$\n$$\n1 - \\frac{\\mu}{y} + \\frac{\\mu}{1 - x - y} = 0 \\quad (2)\n$$\nSubtracting equation $(2)$ from $(1)$ gives:\n$$\n\\left(1 - \\frac{\\mu}{x}\\right) - \\left(1 - \\frac{\\mu}{y}\\right) = 0 \\implies -\\frac{\\mu}{x} + \\frac{\\mu}{y} = 0\n$$\nSince $\\mu  0$, this implies $\\frac{1}{x} = \\frac{1}{y}$, which means $x = y$. This is expected from the symmetry of the problem. Substituting $y=x$ into equation $(1)$:\n$$\n1 - \\frac{\\mu}{x} + \\frac{\\mu}{1 - 2x} = 0\n$$\nTo solve for $x$, we rearrange the equation:\n$$\n1 = \\frac{\\mu}{x} - \\frac{\\mu}{1 - 2x} = \\frac{\\mu(1 - 2x) - \\mu x}{x(1 - 2x)} = \\frac{\\mu(1 - 3x)}{x(1 - 2x)}\n$$\nThis leads to the quadratic equation:\n$$\nx(1 - 2x) = \\mu(1 - 3x) \\implies x - 2x^2 = \\mu - 3\\mu x\n$$\n$$\n2x^2 - (1 + 3\\mu)x + \\mu = 0\n$$\nUsing the quadratic formula, the solutions for $x$ are:\n$$\nx = \\frac{(1 + 3\\mu) \\pm \\sqrt{(1 + 3\\mu)^2 - 4(2)\\mu}}{2(2)} = \\frac{1 + 3\\mu \\pm \\sqrt{1 + 6\\mu + 9\\mu^2 - 8\\mu}}{4} = \\frac{1 + 3\\mu \\pm \\sqrt{9\\mu^2 - 2\\mu + 1}}{4}\n$$\nThe domain $\\mathcal{D}$ requires $x  0$, $y  0$, and $x+y  1$. With $x=y$, this simplifies to $x  0$ and $2x  1$, i.e., $0  x  \\frac{1}{2}$. We must select the root that lies in this interval.\nLet's analyze the two roots. For $\\mu  0$, we have $\\sqrt{9\\mu^2 - 2\\mu + 1}  \\sqrt{9\\mu^2 - 6\\mu + 1} = \\sqrt{(3\\mu-1)^2} = |3\\mu-1|$. For $\\mu \\in (0, 1/3)$, this is $1-3\\mu$.\nThe root with the plus sign is $x_+ = \\frac{1 + 3\\mu + \\sqrt{9\\mu^2 - 2\\mu + 1}}{4}$. For $\\mu \\in (0, 1/3)$, $x_+  \\frac{1+3\\mu + (1-3\\mu)}{4} = \\frac{2}{4} = \\frac{1}{2}$. This root is outside the required interval.\nThe root with the minus sign is $x_- = \\frac{1 + 3\\mu - \\sqrt{9\\mu^2 - 2\\mu + 1}}{4}$. We check its bounds. Since $1+3\\mu  \\sqrt{(1+3\\mu)^2} = \\sqrt{9\\mu^2+6\\mu+1}  \\sqrt{9\\mu^2-2\\mu+1}$ for $\\mu0$, the numerator is positive, so $x_-0$. As established, $x_-$ is also less than $1/2$. Thus, this is the correct solution.\nSo, the unique minimizer is $(x_{\\mu}, y_{\\mu})$ where $x_{\\mu} = y_{\\mu} = \\frac{1 + 3\\mu - \\sqrt{9\\mu^2 - 2\\mu + 1}}{4}$.\nTo obtain a more convenient form, we can rationalize the numerator:\n$$\nx_{\\mu} = \\frac{(1 + 3\\mu - \\sqrt{9\\mu^2 - 2\\mu + 1})(1 + 3\\mu + \\sqrt{9\\mu^2 - 2\\mu + 1})}{4(1 + 3\\mu + \\sqrt{9\\mu^2 - 2\\mu + 1})}\n$$\n$$\nx_{\\mu} = \\frac{(1+3\\mu)^2 - (9\\mu^2 - 2\\mu + 1)}{4(1 + 3\\mu + \\sqrt{9\\mu^2 - 2\\mu + 1})} = \\frac{1+6\\mu+9\\mu^2 - 9\\mu^2 + 2\\mu - 1}{4(1 + 3\\mu + \\sqrt{9\\mu^2 - 2\\mu + 1})}\n$$\n$$\nx_{\\mu} = \\frac{8\\mu}{4(1 + 3\\mu + \\sqrt{9\\mu^2 - 2\\mu + 1})} = \\frac{2\\mu}{1 + 3\\mu + \\sqrt{9\\mu^2 - 2\\mu + 1}}\n$$\nThus, the unique minimizer is $(x_{\\mu}, y_{\\mu})$ with $x_{\\mu} = y_{\\mu} = \\frac{2\\mu}{1 + 3\\mu + \\sqrt{9\\mu^2 - 2\\mu + 1}}$.\n\nFinally, we find the optimum of the original problem and verify the limit. The original problem is to minimize $f(x,y)=x+y$ over the feasible set, which is a triangle with vertices $(0,0)$, $(1,0)$, and $(0,1)$. Since $x \\geq 0$ and $y \\geq 0$, the minimum value of $x+y$ is $0$, which is achieved uniquely at the point $(x^{\\star}, y^{\\star}) = (0,0)$.\nNow we take the limit of our solution as $\\mu \\to 0^+$:\n$$\n\\lim_{\\mu \\to 0^+} x_{\\mu} = \\lim_{\\mu \\to 0^+} \\frac{2\\mu}{1 + 3\\mu + \\sqrt{9\\mu^2 - 2\\mu + 1}} = \\frac{2(0)}{1 + 3(0) + \\sqrt{9(0)^2 - 2(0) + 1}} = \\frac{0}{1+1} = 0\n$$\nSince $y_{\\mu} = x_{\\mu}$, we also have $\\lim_{\\mu \\to 0^+} y_{\\mu} = 0$. Therefore, $\\lim_{\\mu \\to 0^+} (x_{\\mu}, y_{\\mu}) = (0,0) = (x^{\\star}, y^{\\star})$, which verifies that the central path converges to the optimal solution of the original problem.\n\nThe problem asks for the closed-form expression for the unique minimizer $(x_{\\mu}, y_{\\mu})$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2\\mu}{1 + 3\\mu + \\sqrt{9\\mu^2 - 2\\mu + 1}}  \\frac{2\\mu}{1 + 3\\mu + \\sqrt{9\\mu^2 - 2\\mu + 1}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having seen how to solve a barrier subproblem, we now delve deeper into the geometry that makes the method so powerful. This exercise examines the curvature of the barrier function near the boundary, as captured by its Hessian matrix. By analyzing the Hessian's eigenvalues for a circular constraint, you will uncover the anisotropic \"repelling force\" of the barrier, which is fundamental to the behavior and efficiency of interior-point algorithms .",
            "id": "3145933",
            "problem": "Consider the inequality constraint defined by the Euclidean norm (the two-norm) $g(x)=\\|x\\|_{2}-1\\leq 0$ in $\\mathbb{R}^{n}$ with $n\\geq 2$, and the corresponding logarithmic barrier function $\\phi(x)=-\\ln\\!\\big(1-\\|x\\|_{2}\\big)$ defined for all $x$ with $\\|x\\|_{2}1$. Let $r=\\|x\\|_{2}$ and suppose $x\\neq 0$ so that $r\\in(0,1)$ and the unit vector $u=x/r$ is well-defined. Using only fundamental definitions and the chain rule, derive an explicit expression for the Hessian $\\nabla^{2}\\phi(x)$ in terms of $r$ and $u$, and determine its eigenvalues associated with directions parallel to $u$ (radial) and orthogonal to $u$ (tangential). Then, rigorously analyze the asymptotic behavior of these eigenvalues as $r\\to 1^{-}$ by computing the two limits\n$$\n\\lim_{r\\to 1^{-}}(1-r)^{2}\\,\\lambda_{\\mathrm{radial}}(r)\n\\quad\\text{and}\\quad\n\\lim_{r\\to 1^{-}}(1-r)\\,\\lambda_{\\mathrm{tangential}}(r),\n$$\nwhere $\\lambda_{\\mathrm{radial}}(r)$ is the eigenvalue of $\\nabla^{2}\\phi(x)$ in the direction $u$ and $\\lambda_{\\mathrm{tangential}}(r)$ is the eigenvalue in any direction orthogonal to $u$. Express your final answer as a single row matrix containing the two limits in the order shown. No rounding is required.",
            "solution": "The user wants me to solve the following problem:\n\"\"\nConsider the inequality constraint defined by the Euclidean norm (the two-norm) $g(x)=\\|x\\|_{2}-1\\leq 0$ in $\\mathbb{R}^{n}$ with $n\\geq 2$, and the corresponding logarithmic barrier function $\\phi(x)=-\\ln\\!\\big(1-\\|x\\|_{2}\\big)$ defined for all $x$ with $\\|x\\|_{2}1$. Let $r=\\|x\\|_{2}$ and suppose $x\\neq 0$ so that $r\\in(0,1)$ and the unit vector $u=x/r$ is well-defined. Using only fundamental definitions and the chain rule, derive an explicit expression for the Hessian $\\nabla^{2}\\phi(x)$ in terms of $r$ and $u$, and determine its eigenvalues associated with directions parallel to $u$ (radial) and orthogonal to $u$ (tangential). Then, rigorously analyze the asymptotic behavior of these eigenvalues as $r\\to 1^{-}$ by computing the two limits\n$$\n\\lim_{r\\to 1^{-}}(1-r)^{2}\\,\\lambda_{\\mathrm{radial}}(r)\n\\quad\\text{and}\\quad\n\\lim_{r\\to 1^{-}}(1-r)\\,\\lambda_{\\mathrm{tangential}}(r),\n$$\nwhere $\\lambda_{\\mathrm{radial}}(r)$ is the eigenvalue of $\\nabla^{2}\\phi(x)$ in the direction $u$ and $\\lambda_{\\mathrm{tangential}}(r)$ is the eigenvalue in any direction orthogonal to $u$. Express your final answer as a single row matrix containing the two limits in the order shown. No rounding is required.\n\"\"\n\n### Step 1: Extract Givens\n- Constraint: $g(x) = \\|x\\|_{2} - 1 \\leq 0$ in $\\mathbb{R}^{n}$ with $n \\geq 2$.\n- Logarithmic barrier function: $\\phi(x) = -\\ln(1 - \\|x\\|_{2})$ for $\\|x\\|_{2}  1$.\n- Notation: $r = \\|x\\|_{2}$. The domain of $\\phi$ is $r  1$.\n- Condition: $x \\neq 0$, so $r \\in (0,1)$.\n- Unit vector: $u = x/r$.\n- Task 1: Derive an expression for the Hessian $\\nabla^{2}\\phi(x)$ in terms of $r$ and $u$.\n- Task 2: Determine the eigenvalues of $\\nabla^{2}\\phi(x)$ for directions parallel to $u$ ($\\lambda_{\\mathrm{radial}}$) and orthogonal to $u$ ($\\lambda_{\\mathrm{tangential}}$).\n- Task 3: Compute the limits $\\lim_{r\\to 1^{-}}(1-r)^{2}\\,\\lambda_{\\mathrm{radial}}(r)$ and $\\lim_{r\\to 1^{-}}(1-r)\\,\\lambda_{\\mathrm{tangential}}(r)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is mathematically well-posed and scientifically grounded. It concerns the derivation of the Hessian of a standard logarithmic barrier function used in interior-point methods for optimization, specifically for problems involving second-order cone constraints. The function $\\phi(x)$, its domain, and the variables $r$ and $u$ are all clearly and unambiguously defined. The tasks are specific, objective, and solvable using standard methods of vector calculus. All necessary information is provided. The premises are not contradictory, and the problem structure leads to a unique solution.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\nThe logarithmic barrier function is $\\phi(x) = -\\ln(1-r(x))$, where $r(x) = \\|x\\|_2 = (x^T x)^{1/2}$. We are given $x \\neq 0$, so $r  0$.\n\nFirst, we compute the gradient of $\\phi(x)$, denoted by $\\nabla \\phi(x)$. By the chain rule, we have:\n$$\n\\nabla \\phi(x) = \\frac{d\\phi}{dr} \\nabla r(x)\n$$\nThe derivative of $\\phi$ with respect to $r$ is:\n$$\n\\frac{d\\phi}{dr} = \\frac{d}{dr} \\big(-\\ln(1-r)\\big) = - \\frac{1}{1-r} (-1) = \\frac{1}{1-r}\n$$\nThe gradient of the Euclidean norm $r(x)$ is a standard result. For $x \\neq 0$, its $i$-th component is:\n$$\n(\\nabla r(x))_i = \\frac{\\partial}{\\partial x_i} \\left(\\sum_{j=1}^{n} x_j^2\\right)^{1/2} = \\frac{1}{2} \\left(\\sum_{j=1}^{n} x_j^2\\right)^{-1/2} (2x_i) = \\frac{x_i}{\\|x\\|_2} = \\frac{x_i}{r}\n$$\nIn vector form, this is $\\nabla r(x) = \\frac{x}{r} = u$.\nSubstituting these results into the expression for $\\nabla \\phi(x)$:\n$$\n\\nabla \\phi(x) = \\frac{1}{1-r} u\n$$\n\nNext, we compute the Hessian of $\\phi(x)$, which is the Jacobian matrix of the gradient, $\\nabla^2 \\phi(x) = \\nabla(\\nabla \\phi(x))^T$. We apply the product rule for the gradient of a scalar function times a vector function, $\\nabla(f(x) \\mathbf{v}(x)) = (\\nabla f(x)) \\mathbf{v}(x)^T + f(x) \\nabla \\mathbf{v}(x)$, where column vector convention is used for gradients. Let $f(x) = \\frac{1}{1-r(x)}$ and $\\mathbf{v}(x) = u(x)$.\n\nFirst, we find the gradient of $f(x) = (1-r(x))^{-1}$:\n$$\n\\nabla f(x) = \\frac{df}{dr} \\nabla r(x) = \\left(- (1-r)^{-2} (-1)\\right) u = \\frac{1}{(1-r)^2} u\n$$\nNext, we find the Jacobian of the unit vector $u(x) = x/r(x)$. Its $(i,j)$-th entry is $\\frac{\\partial u_i}{\\partial x_j}$:\n$$\n\\frac{\\partial u_i}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left(\\frac{x_i}{r}\\right) = \\frac{\\delta_{ij} r - x_i \\frac{\\partial r}{\\partial x_j}}{r^2}\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. Since $\\frac{\\partial r}{\\partial x_j} = \\frac{x_j}{r} = u_j$, we have:\n$$\n\\frac{\\partial u_i}{\\partial x_j} = \\frac{\\delta_{ij} r - x_i u_j}{r^2} = \\frac{\\delta_{ij}}{r} - \\frac{x_i u_j}{r^2} = \\frac{1}{r} \\left(\\delta_{ij} - \\frac{x_i}{r} u_j\\right) = \\frac{1}{r}(\\delta_{ij} - u_i u_j)\n$$\nIn matrix form, the Jacobian of $u$ is $\\nabla u(x) = \\frac{1}{r}(I - u u^T)$, where $I$ is the $n \\times n$ identity matrix.\n\nNow we assemble the Hessian $\\nabla^2 \\phi(x)$:\n$$\n\\nabla^2 \\phi(x) = (\\nabla f(x)) u(x)^T + f(x) \\nabla u(x) = \\left(\\frac{1}{(1-r)^2} u\\right) u^T + \\frac{1}{1-r} \\left(\\frac{1}{r}(I - u u^T)\\right)\n$$\nThis gives the explicit expression for the Hessian in terms of $r$ and $u$:\n$$\n\\nabla^{2}\\phi(x) = \\frac{1}{(1-r)^2} u u^T + \\frac{1}{r(1-r)} (I - u u^T)\n$$\n\nThe next step is to find the eigenvalues of this Hessian. The matrix is a linear combination of the identity matrix $I$ and the rank-one projection matrix $u u^T$. Its eigenvectors are aligned with $u$ or are orthogonal to $u$.\n\nCase 1: Eigenvalue for the radial direction. Let the eigenvector be $v = u$.\n$$\n\\nabla^{2}\\phi(x) u = \\left( \\frac{1}{(1-r)^2} u u^T + \\frac{1}{r(1-r)} (I - u u^T) \\right) u\n$$\nUsing $u^T u = \\|u\\|_2^2=1$, we get:\n$$\n\\nabla^{2}\\phi(x) u = \\frac{1}{(1-r)^2} u(u^T u) + \\frac{1}{r(1-r)} (u - u(u^T u)) = \\frac{1}{(1-r)^2} u + \\frac{1}{r(1-r)} (u - u) = \\frac{1}{(1-r)^2} u\n$$\nThe eigenvalue for the direction parallel to $u$ (radial direction) is:\n$$\n\\lambda_{\\mathrm{radial}}(r) = \\frac{1}{(1-r)^2}\n$$\n\nCase 2: Eigenvalues for tangential directions. Let the eigenvector be $v$ such that $v$ is orthogonal to $u$, i.e., $u^T v = 0$. Since $n \\geq 2$, such vectors exist.\n$$\n\\nabla^{2}\\phi(x) v = \\left( \\frac{1}{(1-r)^2} u u^T + \\frac{1}{r(1-r)} (I - u u^T) \\right) v\n$$\nUsing $u^T v = 0$, we get:\n$$\n\\nabla^{2}\\phi(x) v = \\frac{1}{(1-r)^2} u(u^T v) + \\frac{1}{r(1-r)} (v - u(u^T v)) = 0 + \\frac{1}{r(1-r)} (v - 0) = \\frac{1}{r(1-r)} v\n$$\nThe eigenvalue for any direction orthogonal to $u$ (tangential direction) is:\n$$\n\\lambda_{\\mathrm{tangential}}(r) = \\frac{1}{r(1-r)}\n$$\nNote that there is an $(n-1)$-dimensional eigenspace associated with this eigenvalue.\n\nFinally, we analyze the asymptotic behavior by computing the two specified limits as $r \\to 1^{-}$.\n\nThe first limit is:\n$$\n\\lim_{r\\to 1^{-}}(1-r)^{2}\\,\\lambda_{\\mathrm{radial}}(r) = \\lim_{r\\to 1^{-}}(1-r)^{2} \\cdot \\frac{1}{(1-r)^2} = \\lim_{r\\to 1^{-}} 1 = 1\n$$\nThe second limit is:\n$$\n\\lim_{r\\to 1^{-}}(1-r)\\,\\lambda_{\\mathrm{tangential}}(r) = \\lim_{r\\to 1^{-}}(1-r) \\cdot \\frac{1}{r(1-r)} = \\lim_{r\\to 1^{-}} \\frac{1}{r} = \\frac{1}{1} = 1\n$$\nBoth limits evaluate to $1$. The final answer is the row matrix containing these two values.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1  1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Theoretical understanding is the first step, but implementation brings its own challenges. This practice focuses on a critical component of any interior-point solver: the line search. You will derive a rule to determine the maximum step size $\\alpha$ that can be taken along a search direction without violating the constraints, ensuring the algorithm always remains strictly feasible. This exercise connects the abstract concept of the barrier domain to the practical logic needed to build a robust optimization algorithm .",
            "id": "3145965",
            "problem": "Consider an inequality-constrained optimization problem with constraints $a_i^{\\top} x \\leq b_i$ for $i \\in \\{1, \\dots, m\\}$, where $a_i \\in \\mathbb{R}^{n}$ and $b_i \\in \\mathbb{R}$. A logarithmic barrier subproblem introduces the barrier term $-\\mu \\sum_{i=1}^{m} \\ln\\!\\big(b_i - a_i^{\\top} x\\big)$ with parameter $\\mu  0$, so the domain of the barrier objective is the strictly feasible region where the slacks $s_i(x) = b_i - a_i^{\\top} x$ satisfy $s_i(x)  0$ for all $i$. In an Interior-Point Method (IPM), a Newton update of the form $x^{+} = x + \\alpha \\Delta x$ must preserve strict feasibility, meaning $b_i - a_i^{\\top} (x + \\alpha \\Delta x)  0$ for all $i$. \n\nStarting from the barrier domain condition and the inequality constraints, derive a step-size rule that guarantees strict feasibility along the Newton update by characterizing the set of $\\alpha  0$ that preserve positivity of all slacks $b_i - a_i^{\\top} (x + \\alpha \\Delta x)$. Then define the maximal feasible step bound $\\alpha^{\\star}$ as the supremum of such $\\alpha$ values and express $\\alpha^{\\star}$ in terms of $x$, $\\Delta x$, $\\{a_i\\}_{i=1}^{m}$, and $\\{b_i\\}_{i=1}^{m}$.\n\nFinally, evaluate $\\alpha^{\\star}$ for the concrete data:\n- $n = 2$, $m = 3$,\n- $a_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $a_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $a_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$,\n- $b_1 = 4$, $b_2 = 3$, $b_3 = 5$,\n- current point $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$,\n- Newton direction $\\Delta x = \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\end{pmatrix}$.\n\nProvide the final answer as the exact value of the supremum $\\alpha^{\\star}$ (a single real number). No rounding is required.",
            "solution": "The problem requires the derivation of a rule for the step size $\\alpha$ in an Interior-Point Method to maintain strict feasibility, and the calculation of the maximal feasible step bound $\\alpha^{\\star}$ for a specific instance.\n\nFirst, we perform a formal validation of the problem statement.\n\n**Step 1: Extract Givens**\n- Inequality constraints: $a_i^{\\top} x \\leq b_i$ for $i \\in \\{1, \\dots, m\\}$, with $a_i \\in \\mathbb{R}^{n}$ and $b_i \\in \\mathbb{R}$.\n- Barrier term: $-\\mu \\sum_{i=1}^{m} \\ln(b_i - a_i^{\\top} x)$ with parameter $\\mu  0$.\n- Strict feasibility at the current point $x$: $b_i - a_i^{\\top} x  0$ for all $i$.\n- Newton update: $x^{+} = x + \\alpha \\Delta x$.\n- Strict feasibility condition for the updated point $x^{+}$: $b_i - a_i^{\\top} (x + \\alpha \\Delta x)  0$ for all $i$, given $\\alpha  0$.\n- Objective: Derive the maximal step bound $\\alpha^{\\star} = \\sup\\{\\alpha  0 \\mid b_i - a_i^{\\top}(x + \\alpha \\Delta x)  0, \\forall i \\in \\{1, \\dots, m\\}\\}$.\n- Concrete data for evaluation: $n = 2$, $m = 3$; $a_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $a_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $a_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$; $b_1 = 4$, $b_2 = 3$, $b_3 = 5$; current point $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$; Newton direction $\\Delta x = \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\end{pmatrix}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, rooted in the standard theory of Interior-Point Methods for constrained optimization. It is well-posed, providing all necessary information for a unique solution. The initial point $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ is strictly feasible:\n- $b_1 - a_1^{\\top}x = 4 - (1)(1) - (0)(1) = 3  0$.\n- $b_2 - a_2^{\\top}x = 3 - (0)(1) - (1)(1) = 2  0$.\n- $b_3 - a_3^{\\top}x = 5 - (1)(1) - (1)(1) = 3  0$.\nSince the initial point satisfies the domain condition of the barrier function, the problem setup is consistent and complete. It does not violate any scientific principles, is mathematically formalizable, and is objective.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the solution.\n\n**Derivation of the Maximal Feasible Step Bound $\\alpha^{\\star}$**\n\nThe condition for the updated point $x^{+} = x + \\alpha \\Delta x$ to remain strictly feasible is that for every constraint $i \\in \\{1, \\dots, m\\}$, the inequality $b_i - a_i^{\\top} x^{+}  0$ must hold. For $\\alpha  0$, we have:\n$$b_i - a_i^{\\top}(x + \\alpha \\Delta x)  0$$\nBy linearity of the dot product, this can be expanded to:\n$$b_i - a_i^{\\top}x - \\alpha (a_i^{\\top} \\Delta x)  0$$\nLet $s_i(x) = b_i - a_i^{\\top}x$ denote the slack for the $i$-th constraint at the current point $x$. By the premise of an interior-point iteration, $s_i(x)  0$ for all $i$. Substituting this definition, the inequality becomes:\n$$s_i(x) - \\alpha (a_i^{\\top} \\Delta x)  0$$\nTo find the valid range for $\\alpha$, we analyze this inequality based on the sign of the term $a_i^{\\top} \\Delta x$.\n\nCase 1: $a_i^{\\top} \\Delta x \\leq 0$.\nIn this case, for any $\\alpha  0$, the term $-\\alpha (a_i^{\\top} \\Delta x)$ is non-negative. Since $s_i(x)  0$ by assumption, the inequality $s_i(x) - \\alpha (a_i^{\\top} \\Delta x) \\geq s_i(x)  0$ is satisfied for all $\\alpha  0$. This means that if the search direction $\\Delta x$ moves parallel to or away from the $i$-th constraint boundary, strict feasibility of that constraint is maintained for any positive step size. Thus, such a constraint imposes no upper bound on $\\alpha$.\n\nCase 2: $a_i^{\\top} \\Delta x  0$.\nThis case corresponds to the search direction pointing towards the $i$-th constraint boundary. We can rearrange the inequality to solve for $\\alpha$:\n$$s_i(x)  \\alpha (a_i^{\\top} \\Delta x)$$\n$$\\alpha  \\frac{s_i(x)}{a_i^{\\top} \\Delta x}$$\nThis establishes an upper bound on $\\alpha$ for each constraint $i$ for which $a_i^{\\top} \\Delta x  0$.\n\nTo ensure that the updated point $x^{+}$ is strictly feasible with respect to *all* constraints simultaneously, the step size $\\alpha$ must satisfy the most restrictive of these upper bounds. Therefore, $\\alpha$ must be strictly less than the minimum of all such ratios. The set of valid step sizes $\\alpha  0$ is characterized by the interval $(0, \\alpha^{\\star})$, where $\\alpha^{\\star}$ is the supremum of this set. The supremum is given by:\n$$\\alpha^{\\star} = \\min_{i \\,:\\, a_i^{\\top} \\Delta x  0} \\left\\{ \\frac{s_i(x)}{a_i^{\\top} \\Delta x} \\right\\} = \\min_{i \\,:\\, a_i^{\\top} \\Delta x  0} \\left\\{ \\frac{b_i - a_i^{\\top} x}{a_i^{\\top} \\Delta x} \\right\\}$$\nIf the set $\\{i \\mid a_i^{\\top} \\Delta x  0\\}$ is empty, then no constraint imposes an upper bound on $\\alpha$, and we define $\\alpha^{\\star} = +\\infty$.\n\n**Evaluation for the Concrete Data**\n\nWe now apply this formula to the provided data:\n- $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, $\\Delta x = \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\end{pmatrix}$.\n- Constraints: $a_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, b_1 = 4$; $a_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, b_2 = 3$; $a_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, b_3 = 5$.\n\nFor each constraint $i \\in \\{1, 2, 3\\}$, we calculate the numerator $b_i - a_i^{\\top}x$ and the denominator $a_i^{\\top} \\Delta x$.\n\nConstraint $i=1$:\n- Numerator: $b_1 - a_1^{\\top}x = 4 - \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 4 - 1 = 3$.\n- Denominator: $a_1^{\\top} \\Delta x = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\end{pmatrix} = \\frac{1}{2}$.\n- Since the denominator is positive ($a_1^{\\top} \\Delta x = \\frac{1}{2}  0$), this constraint imposes a bound. The ratio is $\\frac{3}{\\frac{1}{2}} = 6$.\n\nConstraint $i=2$:\n- Numerator: $b_2 - a_2^{\\top}x = 3 - \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 3 - 1 = 2$.\n- Denominator: $a_2^{\\top} \\Delta x = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\end{pmatrix} = 1$.\n- Since the denominator is positive ($a_2^{\\top} \\Delta x = 1  0$), this constraint imposes a bound. The ratio is $\\frac{2}{1} = 2$.\n\nConstraint $i=3$:\n- Numerator: $b_3 - a_3^{\\top}x = 5 - \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 5 - (1+1) = 3$.\n- Denominator: $a_3^{\\top} \\Delta x = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\end{pmatrix} = \\frac{1}{2} + 1 = \\frac{3}{2}$.\n- Since the denominator is positive ($a_3^{\\top} \\Delta x = \\frac{3}{2}  0$), this constraint imposes a bound. The ratio is $\\frac{3}{\\frac{3}{2}} = 2$.\n\nAll three constraints have $a_i^{\\top} \\Delta x  0$. The maximal feasible step bound $\\alpha^{\\star}$ is the minimum of the calculated ratios:\n$$\\alpha^{\\star} = \\min \\{6, 2, 2\\}$$\n$$\\alpha^{\\star} = 2$$\nThus, the supremum of the set of feasible step sizes is $2$.",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}