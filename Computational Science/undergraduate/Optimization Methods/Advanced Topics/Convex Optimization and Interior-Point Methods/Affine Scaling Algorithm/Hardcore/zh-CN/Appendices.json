{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握仿射缩放算法，最好的方法莫过于亲自动手。这个练习将引导你完成一次完整的仿射缩放迭代计算，帮助你揭开算法中每个组成部分（如拉格朗日乘子、搜索方向和步长）的神秘面纱，并理解它们各自扮演的角色。通过这个具体的数值示例，你将能够巩固对算法核心机制的理解 。",
            "id": "3095992",
            "problem": "考虑以下线性规划 (LP) 问题：最小化线性目标函数 $c^{\\top} x$，约束条件为仿射等式约束 $A x = b$ 和非负约束 $x \\ge 0$。数据为\n$$\nA = \\begin{pmatrix} 1  1  1 \\end{pmatrix}, \\quad b = 3, \\quad c = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix},\n$$\n且严格可行起始点为\n$$\nx^{(0)} = \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{3}{2} \\\\ 1 \\end{pmatrix}.\n$$\n执行一次完整的原始仿射尺度 (AS) 方法迭代，从第一性原理出发：将搜索方向解释为线性化目标函数 $c^{\\top} d$ 在由 $X^{-1}$（其中 $X = \\mathrm{diag}(x^{(0)})$）导出的局部范数下，限制在仿射可行集 $A d = 0$ 内的最速下降方向。利用此解释，确定满足一阶可行性的拉格朗日乘子 $y$，然后构造相应的搜索方向 $p$，最后使用到边界距离比例规则（参数 $\\beta = \\tfrac{9}{10}$）选择步长 $\\alpha$。计算更新后的点 $x^{(1)} = x^{(0)} + \\alpha p$。提供 $x^{(1)}$ 各分量的精确值；不要四舍五入。将最终答案表示为单个行向量。",
            "solution": "该问题是适定的、有科学依据的，并为得到唯一解提供了所有必要信息。所提供的起始点 $x^{(0)}$ 被验证为严格可行：\n$A x^{(0)} = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{3}{2} \\\\ 1 \\end{pmatrix} = \\frac{1}{2} + \\frac{3}{2} + 1 = 3 = b$。\n$x^{(0)}$ 的所有分量均为严格正数。因此，该问题有效，我们继续求解。\n\n原始仿射尺度方法通过考虑问题的尺度变换版本，在当前严格可行点 $x$ 处生成一个搜索方向 $p$。如上所述，我们在可行子空间 $Ad=0$ 内，寻找目标函数 $c^{\\top}d$ 的最速下降方向，该方向由局部范数 $\\|d\\|_{X^{-1}} = \\sqrt{d^{\\top}X^{-2}d}$ 度量，其中 $X = \\mathrm{diag}(x)$。这可以表述为以下优化问题：\n$$\n\\begin{array}{ll}\n\\text{minimize}  c^{\\top} d \\\\\n\\text{subject to}  Ad = 0 \\\\\n                  d^{\\top}X^{-2}d = \\text{constant}\n\\end{array}\n$$\n该问题的拉格朗日函数为 $L(d, y) = c^{\\top}d - y^{\\top}(Ad)$，其中我们只考虑方向上的等式约束。关于 $d$ 在尺度变换范数下的最优性条件意味着，目标函数关于尺度变换坐标的梯度必须与尺度变换后的可行子空间正交。搜索方向 $p$ 由负的尺度变换梯度在尺度变换约束矩阵的零空间上的投影给出。\n\n令尺度变换变量为 $\\tilde{d} = X^{-1} d$，尺度变换数据为 $\\tilde{A} = AX$ 和 $\\tilde{c} = Xc$。在尺度变换空间中的搜索方向 $\\tilde{p}$ 是 $-\\tilde{c}$ 在 $\\tilde{A}$ 的零空间上的投影：\n$$\n\\tilde{p} = -\\left(I - \\tilde{A}^{\\top}(\\tilde{A}\\tilde{A}^{\\top})^{-1}\\tilde{A}\\right)\\tilde{c}\n$$\n那么在原始空间中的搜索方向为 $p = X\\tilde{p}$。\n\n或者，更直接地，原始线性规划的一阶最优性条件意味着存在一个对偶变量估计 $y$ 和一个简约成本向量 $s = c - A^{\\top}y$。仿射尺度搜索方向 $p$ 由 $p = -X^2 s$ 给出。选择对偶估计 $y$（拉格朗日乘子）以确保搜索方向位于 $A$ 的零空间中，即 $Ap=0$。\n$A(-X^2(c-A^{\\top}y)) = 0 \\implies -AX^2c + AX^2A^{\\top}y = 0 \\implies y = (AX^2A^{\\top})^{-1}AX^2c$。这与问题中寻找拉格朗日乘子 $y$ 的要求相符。\n\n给定起始点 $x^{(0)} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix}$。\n尺度变换矩阵 $X$ 及其平方为：\n$$\nX = \\mathrm{diag}(x^{(0)}) = \\begin{pmatrix} \\frac{1}{2}  0  0 \\\\ 0  \\frac{3}{2}  0 \\\\ 0  0  1 \\end{pmatrix}, \\quad X^2 = \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{9}{4}  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n给定数据：$A = \\begin{pmatrix} 1  1  1 \\end{pmatrix}$ 和 $c = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix}$。\n\n首先，我们计算拉格朗日乘子 $y$ 所需的各分量：\n$$\nAX^2 = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{9}{4}  0 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}  \\frac{9}{4}  1 \\end{pmatrix}\n$$\n$$\nAX^2A^{\\top} = \\begin{pmatrix} \\frac{1}{4}  \\frac{9}{4}  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{4} + \\frac{9}{4} + 1 = \\frac{10}{4} + 1 = \\frac{5}{2} + 1 = \\frac{7}{2}\n$$\n$$\nAX^2c = \\begin{pmatrix} \\frac{1}{4}  \\frac{9}{4}  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\frac{1}{4} + \\frac{18}{4} + 0 = \\frac{19}{4}\n$$\n现在，我们计算拉格朗日乘子 $y$：\n$$\ny = (AX^2A^{\\top})^{-1}AX^2c = \\left(\\frac{7}{2}\\right)^{-1} \\left(\\frac{19}{4}\\right) = \\frac{2}{7} \\cdot \\frac{19}{4} = \\frac{19}{14}\n$$\n接下来，我们确定简约成本向量 $s = c - A^{\\top}y$：\n$$\ns = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\left(\\frac{19}{14}\\right) = \\begin{pmatrix} 1 - \\frac{19}{14} \\\\ 2 - \\frac{19}{14} \\\\ 0 - \\frac{19}{14} \\end{pmatrix} = \\begin{pmatrix} \\frac{14-19}{14} \\\\ \\frac{28-19}{14} \\\\ -\\frac{19}{14} \\end{pmatrix} = \\begin{pmatrix} -\\frac{5}{14} \\\\ \\frac{9}{14} \\\\ -\\frac{19}{14} \\end{pmatrix}\n$$\n搜索方向 $p$ 为 $p = -X^2 s$：\n$$\np = - \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{9}{4}  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} -\\frac{5}{14} \\\\ \\frac{9}{14} \\\\ -\\frac{19}{14} \\end{pmatrix} = - \\begin{pmatrix} -\\frac{5}{56} \\\\ \\frac{81}{56} \\\\ -\\frac{19}{14} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{56} \\\\ -\\frac{81}{56} \\\\ \\frac{19}{14} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{56} \\\\ -\\frac{81}{56} \\\\ \\frac{76}{56} \\end{pmatrix}\n$$\n为了确定步长 $\\alpha$，我们使用到边界距离比例规则，$\\alpha = \\beta \\alpha_{\\max}$。首先，我们找到保持非负性的最大步长 $\\alpha_{\\max}$，即 $x^{(0)} + \\alpha p \\ge 0$。这受到 $p_i  0$ 的分量的限制。\n$$\n\\alpha_{\\max} = \\min_{i: p_i  0} \\left\\{ \\frac{x_i^{(0)}}{-p_i} \\right\\}\n$$\n在我们的例子中，只有 $p_2 = -\\frac{81}{56}$ 是负数。\n$$\n\\alpha_{\\max} = \\frac{x_2^{(0)}}{-p_2} = \\frac{\\frac{3}{2}}{-(-\\frac{81}{56})} = \\frac{3}{2} \\cdot \\frac{56}{81} = \\frac{3 \\cdot 28}{81} = \\frac{28}{27}\n$$\n步长参数为 $\\beta = \\frac{9}{10}$。所以步长为：\n$$\n\\alpha = \\beta \\alpha_{\\max} = \\frac{9}{10} \\cdot \\frac{28}{27} = \\frac{1}{10} \\cdot \\frac{28}{3} = \\frac{28}{30} = \\frac{14}{15}\n$$\n最后，我们计算新的迭代点 $x^{(1)} = x^{(0)} + \\alpha p$：\n$$\nx^{(1)} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} + \\frac{14}{15} \\begin{pmatrix} \\frac{5}{56} \\\\ -\\frac{81}{56} \\\\ \\frac{76}{56} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{14 \\cdot 5}{15 \\cdot 56} \\\\ \\frac{14 \\cdot (-81)}{15 \\cdot 56} \\\\ \\frac{14 \\cdot 76}{15 \\cdot 56} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{5}{15 \\cdot 4} \\\\ \\frac{-81}{15 \\cdot 4} \\\\ \\frac{76}{15 \\cdot 4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{5}{60} \\\\ -\\frac{81}{60} \\\\ \\frac{76}{60} \\end{pmatrix}\n$$\n$$\nx_1^{(1)} = \\frac{1}{2} + \\frac{5}{60} = \\frac{30}{60} + \\frac{5}{60} = \\frac{35}{60} = \\frac{7}{12}\n$$\n$$\nx_2^{(1)} = \\frac{3}{2} - \\frac{81}{60} = \\frac{90}{60} - \\frac{81}{60} = \\frac{9}{60} = \\frac{3}{20}\n$$\n$$\nx_3^{(1)} = 1 + \\frac{76}{60} = \\frac{60}{60} + \\frac{76}{60} = \\frac{136}{60} = \\frac{34}{15}\n$$\n更新后的点是 $x^{(1)} = \\begin{pmatrix} \\frac{7}{12} \\\\ \\frac{3}{20} \\\\ \\frac{34}{15} \\end{pmatrix}$。\n作为检验，我们确认 $A x^{(1)} = b$：\n$$\n\\frac{7}{12} + \\frac{3}{20} + \\frac{34}{15} = \\frac{35}{60} + \\frac{9}{60} + \\frac{136}{60} = \\frac{35+9+136}{60} = \\frac{180}{60} = 3\n$$\n约束条件得到满足。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{7}{12}  \\frac{3}{20}  \\frac{34}{15} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在掌握了单步计算之后，我们来探讨一个更宏观的问题：算法的收敛行为。通过对比唯一最优解和存在多个最优解这两种情形，本练习旨在帮助你建立关于仿射缩放轨迹如何逼近最优解的直观理解。这种定性分析对于预测算法在不同类型问题上的表现至关重要 。",
            "id": "3095998",
            "problem": "考虑标准形式的线性规划 (LP) 问题：最小化 $c^{\\mathsf{T}} x$，约束条件为 $A x = b$ 和 $x > 0$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是行满秩矩阵，$x \\in \\mathbb{R}^n$ 位于正象限。原始仿射尺度算法 (ASA) 在一个严格可行点 $x$ 处，通过在可行方向集 $\\{ d : A d = 0 \\}$ 上近似求解一个局部信赖域问题来选择一个下降方向 $d$，使用 Dikin 尺度矩阵 $D(x) = \\mathrm{diag}(x)$ 和一个步长 $0  \\alpha  \\alpha_{\\max}(x)$ 来维持 $x + \\alpha d > 0$。轨迹 $\\{ x^{(k)} \\}_{k \\ge 0}$ 由 $x^{(k+1)} = x^{(k)} + \\alpha_k d^{(k)}$ 定义，其中 $x^{(0)}$ 是严格可行的，且 $\\alpha_k$ 足够小以防止离开正象限。\n\n通过比较以下两个线性规划问题，研究 ASA 轨迹的定性行为。这两个问题中，$A \\in \\mathbb{R}^{1 \\times 2}$，$A = \\begin{pmatrix} 1  1 \\end{pmatrix}$，$b = 1$，以及一个严格可行的起始点 $x^{(0)} = \\begin{pmatrix} p \\\\ 1 - p \\end{pmatrix}$，其中 $p \\in (0,1)$：\n\n- 情况 $\\mathrm{I}$（唯一最优极点）：$c = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，因此唯一最优解是 $x^\\star = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。\n- 情况 $\\mathrm{II}$（多个最优极点）：$c = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$，因此整个面 $\\{ x \\in \\mathbb{R}^2 : x_1 + x_2 = 1, x \\ge 0 \\}$ 都是最优的，两个极点 $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 和 $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ 都是最优解。\n\n关于 ASA 轨迹，以下哪个陈述是正确的？选择所有适用的选项。\n\nA. 在情况 $\\mathrm{I}$ 中，对于足够小的步长，ASA 的迭代点 $x^{(k)}$ 满足 $A x^{(k)} = b$ 并收敛到唯一最优极点 $x^\\star = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$，其方向 $d^{(k)}$ 位于仿射子空间 $\\{ d : A d = 0 \\}$ 中。\n\nB. 在情况 $\\mathrm{II}$ 中，因为 $c \\in \\mathrm{range}(A^{\\mathsf{T}})$，所以在任何严格可行点 $x$ 处的 ASA 投影下降方向都是零向量；因此，轨迹立即在 $x^{(0)}$ 处停滞，该点位于最优面的相对内部。\n\nC. 在情况 $\\mathrm{II}$ 中，ASA 通常会无限循环，并且永远不会到达最优面。\n\nD. 在情况 $\\mathrm{I}$ 和情况 $\\mathrm{II}$ 中，ASA 轨迹都收敛到可行域 $\\{ x : A x = b, x > 0 \\}$ 的解析中心，而与 $c$ 的选择无关。\n\nE. 在情况 $\\mathrm{I}$ 中，通过在每次迭代中选择保持可行性的最大步长（即 $\\alpha_k = \\alpha_{\\max}(x^{(k)})$），ASA 在有限次迭代内达到 $x^\\star = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。",
            "solution": "## 问题验证\n\n### 步骤 1：提取已知条件\n\n问题陈述提供了以下信息：\n\n*   **问题类型**：标准形式的线性规划 (LP)。\n*   **目标**：最小化 $c^{\\mathsf{T}} x$。\n*   **约束条件**：$A x = b$ 和 $x > 0$。\n*   **矩阵和向量维度**：$A \\in \\mathbb{R}^{m \\times n}$，$x \\in \\mathbb{R}^n$。$A$ 是行满秩的。\n*   **算法**：原始仿射尺度算法 (ASA)。\n*   **起始点**：一个严格可行点 $x^{(0)}$（$A x^{(0)} = b$，$x^{(0)} > 0$）。\n*   **下降方向 $d$**：在可行方向集 $\\{ d : A d = 0 \\}$ 上选择。\n*   **尺度矩阵**：Dikin 尺度矩阵 $D(x) = \\mathrm{diag}(x)$。\n*   **步长**：$0  \\alpha  \\alpha_{\\max}(x)$ 以维持严格可行性 $x + \\alpha d > 0$。\n*   **更新规则**：$x^{(k+1)} = x^{(k)} + \\alpha_k d^{(k)}$。\n\n问题随后具体化为一类特定的 LP：\n*   $A = \\begin{pmatrix} 1  1 \\end{pmatrix} \\in \\mathbb{R}^{1 \\times 2}$。\n*   $b = 1$。\n*   严格可行的起始点 $x^{(0)} = \\begin{pmatrix} p \\\\ 1 - p \\end{pmatrix}$，其中 $p \\in (0,1)$。\n\n给出了成本向量 $c$ 的两种情况：\n*   **情况 I**：$c = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。唯一最优解是 $x^\\star = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。\n*   **情况 II**：$c = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。最优集是整个面 $\\{ x \\in \\mathbb{R}^2 : x_1 + x_2 = 1, x \\ge 0 \\}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n\n问题陈述描述了原始仿射尺度算法，这是数值优化领域中用于求解线性规划的一个成熟方法。所给的设置是一个标准的、典型的例子，用来说明该算法的行为。\n\n*   **科学依据**：该问题根植于优化和线性规划的数学理论。仿射尺度算法是一种经典的内点法。所有概念都是标准的。\n*   **适定性**：问题定义清晰。参数 $A$、$b$ 和 $c$ 在两种不同情况下都已明确给出。问题要求分析 ASA 轨迹的定性行为，这可以通过分析算法的机制来确定。\n*   **客观性**：问题以精确、客观的数学语言陈述，没有歧义或主观内容。\n*   **完整性和一致性**：问题是自包含的。矩阵 $A = \\begin{pmatrix} 1  1 \\end{pmatrix}$ 的秩为 $1$，与其行维度 $m=1$ 相匹配，因此它具有规定的行满秩。起始点 $x^{(0)}$ 被确认为严格可行：$A x^{(0)} = p + (1-p) = 1 = b$，且对于 $p \\in (0,1)$，$x^{(0)}$ 的两个分量都严格为正。两种情况下最优解的描述都是正确的。\n\n### 步骤 3：结论和行动\n\n问题陈述是有效的。它在科学上是合理的、适定的且内部一致。我们可以继续进行求解。\n\n## 解的推导\n\n原始仿射尺度算法通过求解一个缩放的信赖域子问题，在严格可行点 $x$ 处生成一个搜索方向 $d$。该方向的标准显式公式为：\n$$ d = -D(x)^2 (c - A^{\\mathsf{T}}w) $$\n其中 $D(x) = \\mathrm{diag}(x)$ 是尺度矩阵，$w$ 是对偶变量估计的向量，由下式给出：\n$$ w = (A D(x)^2 A^{\\mathsf{T}})^{-1} (A D(x)^2 c) $$\n我们首先计算特定问题参数下方向 $d$ 的一般形式：\n$A = \\begin{pmatrix} 1  1 \\end{pmatrix}$，所以 $m=1$ 且 $n=2$。$A^{\\mathsf{T}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。\n$x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$，所以 $D(x) = \\begin{pmatrix} x_1  0 \\\\ 0  x_2 \\end{pmatrix}$ 且 $D(x)^2 = \\begin{pmatrix} x_1^2  0 \\\\ 0  x_2^2 \\end{pmatrix}$。\n\n表达式 $w$ 中的各项是：\n$A D(x)^2 = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} x_1^2  0 \\\\ 0  x_2^2 \\end{pmatrix} = \\begin{pmatrix} x_1^2  x_2^2 \\end{pmatrix}$。\n$A D(x)^2 A^{\\mathsf{T}} = \\begin{pmatrix} x_1^2  x_2^2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = x_1^2 + x_2^2$。这是一个标量。\n$(A D(x)^2 A^{\\mathsf{T}})^{-1} = \\frac{1}{x_1^2 + x_2^2}$。\n$A D(x)^2 c = \\begin{pmatrix} x_1^2  x_2^2 \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = c_1 x_1^2 + c_2 x_2^2$。\n\n所以，对偶估计 $w$（在这种情况下是一个标量）是：\n$$ w = \\frac{c_1 x_1^2 + c_2 x_2^2}{x_1^2 + x_2^2} $$\n搜索方向 $d$ 是：\n$$ d = - \\begin{pmatrix} x_1^2  0 \\\\ 0  x_2^2 \\end{pmatrix} \\left( \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} w \\right) = - \\begin{pmatrix} x_1^2(c_1 - w) \\\\ x_2^2(c_2 - w) \\end{pmatrix} $$\n我们现在分析这两种情况。\n\n**情况 I 的分析**\n这里，$c = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，所以 $c_1 = 1$，$c_2 = 0$。\n对偶估计是 $w = \\frac{1 \\cdot x_1^2 + 0 \\cdot x_2^2}{x_1^2 + x_2^2} = \\frac{x_1^2}{x_1^2 + x_2^2}$。\n方向向量 $d = \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix}$ 的分量是：\n$d_1 = -x_1^2(c_1 - w) = -x_1^2 \\left( 1 - \\frac{x_1^2}{x_1^2 + x_2^2} \\right) = -x_1^2 \\left( \\frac{x_2^2}{x_1^2 + x_2^2} \\right) = -\\frac{x_1^2 x_2^2}{x_1^2 + x_2^2}$。\n$d_2 = -x_2^2(c_2 - w) = -x_2^2 \\left( 0 - \\frac{x_1^2}{x_1^2 + x_2^2} \\right) = \\frac{x_1^2 x_2^2}{x_1^2 + x_2^2}$。\n所以，$d = \\frac{x_1^2 x_2^2}{x_1^2 + x_2^2} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$。\n更新规则是 $x^{(k+1)} = x^{(k)} + \\alpha_k d^{(k)}$。由于 $x_1, x_2 > 0$ 且 $\\alpha_k > 0$，第一个分量 $x_1$ 将总是减小，而第二个分量 $x_2$ 将总是增加。迭代点 $x^{(k)}$ 的轨迹沿着可行线段 $x_1 + x_2 = 1$ 向顶点 $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ 移动，该顶点是唯一最优解 $x^\\star$。对于任何步长规则，只要 $\\alpha_k$ 是最大可能步长的一部分，迭代点就会收敛到 $x^\\star$。\n\n**情况 II 的分析**\n这里，$c = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$，所以 $c_1 = 1$，$c_2 = 1$。\n这个成本向量可以写成 $c = A^{\\mathsf{T}}$，这意味着 $c$ 在 $A^{\\mathsf{T}}$ 的值域中。\n对偶估计是 $w = \\frac{1 \\cdot x_1^2 + 1 \\cdot x_2^2}{x_1^2 + x_2^2} = \\frac{x_1^2 + x_2^2}{x_1^2 + x_2^2} = 1$。\n搜索方向 $d$ 是：\n$d = - \\begin{pmatrix} x_1^2(1 - 1) \\\\ x_2^2(1 - 1) \\end{pmatrix} = - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n搜索方向是零向量。因此，算法无法取得任何进展。\n$x^{(1)} = x^{(0)} + \\alpha_0 d^{(0)} = x^{(0)}$。轨迹在起始点 $x^{(0)}$ 处立即停滞。\n对于任何可行点，目标函数都是 $c^{\\mathsf{T}}x = x_1+x_2=1$。因此，整个可行域就是最优面。任何严格可行点 $x^{(0)}$ 都位于这个最优面的相对内部。\n\n## 逐项分析\n\n**A. 在情况 I 中，对于足够小的步长，ASA 的迭代点 $x^{(k)}$ 满足 $A x^{(k)} = b$ 并收敛到唯一最优极点 $x^\\star = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$，其方向 $d^{(k)}$ 位于仿射子空间 $\\{ d : A d = 0 \\}$ 中。**\n这个陈述是正确的。\n1.  方向 $d$ 被构造为在 $A$ 的零空间中，因此 $Ad=0$。如果 $Ax^{(k)}=b$，那么 $A x^{(k+1)} = A(x^{(k)} + \\alpha_k d^{(k)}) = Ax^{(k)} + \\alpha_k Ad^{(k)} = b + 0 = b$。可行性得以保持。集合 $\\{d : Ad=0\\}$ 是 $\\mathbb{R}^n$ 的一个线性子空间，这是一种仿射子空间。\n2.  我们对情况 I 的分析表明，迭代点单调地向 $x^\\star = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ 移动。短步长原始仿射尺度算法收敛到唯一最优解是一个标准的理论结果。\n**结论：正确。**\n\n**B. 在情况 II 中，因为 $c \\in \\mathrm{range}(A^{\\mathsf{T}})$，所以在任何严格可行点 $x$ 处的 ASA 投影下降方向都是零向量；因此，轨迹立即在 $x^{(0)}$ 处停滞，该点位于最优面的相对内部。**\n这个陈述是正确的。\n1.  在情况 II 中，$c = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ 且 $A^{\\mathsf{T}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$，所以确实有 $c = A^{\\mathsf{T}}$，这意味着 $c \\in \\mathrm{range}(A^{\\mathsf{T}})$。\n2.  我们对情况 II 的分析表明，这个条件导致简约成本向量为零，因此对于任何严格可行点 $x$，下降方向 $d$ 都是零向量。\n3.  因此，算法在初始点 $x^{(0)}$ 处停滞。\n4.  最优面是集合 $\\{x: x_1+x_2=1, x \\ge 0\\}$。起始点 $x^{(0)}$ 的分量在 $(0,1)$ 内，它位于该集合的相对内部。\n**结论：正确。**\n\n**C. 在情况 II 中，ASA 通常会无限循环，并且永远不会到达最优面。**\n这个陈述是不正确的。根据对选项 B 的分析，算法从一个已经位于最优面上的点 $x^{(0)}$ 开始。轨迹在该点停滞；它既不循环，也不会无法到达最优面。\n**结论：不正确。**\n\n**D. 在情况 I 和情况 II 中，ASA 轨迹都收敛到可行域 $\\{ x : A x = b, x > 0 \\}$ 的解析中心，而与 $c$ 的选择无关。**\n这个陈述是不正确的。ASA 的轨迹从根本上依赖于成本向量 $c$。\n1.  可行域 $\\{x: x_1+x_2=1, x_1>0, x_2>0\\}$ 的解析中心是使 $\\ln(x_1) + \\ln(x_2)$ 最大化的点，即 $x_{ac} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix}$。\n2.  在情况 I 中，轨迹收敛到最优顶点 $x^\\star = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$，这并非解析中心。\n3.  在情况 II 中，轨迹在起始点 $x^{(0)} = \\begin{pmatrix} p \\\\ 1-p \\end{pmatrix}$ 处停滞，这仅在 $p=1/2$ 的特定情况下才是解析中心。\nASA 是在缩放空间中的最速下降法；不应将其与明确以解析中心为目标的中心路径跟踪法相混淆。\n**结论：不正确。**\n\n**E. 在情况 I 中，通过在每次迭代中选择保持可行性的最大步长（即 $\\alpha_k = \\alpha_{\\max}(x^{(k)})$），ASA 在有限次迭代内达到 $x^\\star = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。**\n在问题对算法的定义背景下，这个陈述是不正确的。问题陈述明确定义步长为 $0  \\alpha  \\alpha_{\\max}(x)$，这是一个严格不等式，确保迭代点保持严格可行（即，在正象限的内部）。这是算法的“短步长”或“阻尼步长”版本。在此规则下，一个分量为零的边界点只能作为极限点达到，而不能在有限步数内达到。虽然一个*不同*的算法，即采用完整步长到达边界（$\\alpha = \\alpha_{\\max}$）的“长步长”变体，在这个特定问题中可以在单次迭代中达到最优解，但选项 E 描述的过程违反了问题陈述中给出的前提。问题是关于*所定义的* ASA 的行为，它要求 $\\alpha_k  \\alpha_{\\max}$。因此，无法实现有限步收敛。\n**结论：不正确。**",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "理论和手动计算是基础，但将算法付诸实践会遇到新的挑战，尤其是数值稳定性问题。这个编程练习将模拟一个病态线性规划问题，其中约束矩阵的列尺度差异巨大。通过对比标准仿射缩放算法和列归一化变体的性能，你将学会一种重要的预处理技术，以增强算法的鲁棒性和效率 。",
            "id": "3095999",
            "problem": "您将使用原始仿射尺度算法构建并求解一个线性规划族，并在约束矩阵的列存在数量级差异的实例上比较两种变体。目标是量化列归一化如何影响达到固定的一阶最优性容差所需的迭代次数。您的程序的最终输出必须是单行文本，汇总预定测试套件上的结果。\n\n考虑等式形式的线性规划：最小化 $c^\\top x$，约束条件为 $A x = b$，其中 $x \\in \\mathbb{R}^n$，$x > 0$。原始仿射尺度算法通过计算在由 $D^{-1}$ 导出的加权范数下最陡峭的可行下降方向，来迭代更新一个严格可行点 $x$，其中 $D = \\mathrm{diag}(x)$，然后沿此方向前进一步，同时保持严格正性。您应依赖的相关定义和属性包括：\n- 线性规划的定义，线性目标梯度 $c$ 的作用，以及可行下降方向的概念。\n- 在由正定矩阵（此处为 $D^{-1}$ 导出的范数）定义的范数下的最速下降方向。\n- 向量到矩阵零空间上的投影，以及构造满足 $A p = 0$ 的可行方向。\n- 线性规划的一阶最优性条件，涉及简约成本向量和与等式约束相关的拉格朗日乘子。\n- 需要选择一个步长以确保 $x + \\alpha p > 0$，并在合适的最优性残差足够小时终止。\n\n您将实现两种变体：\n- 标准原始仿射尺度算法，直接应用于 $(A,b,c)$。\n- 列归一化变体，通过变量变换对 $A$ 的列进行预处理，使其具有单位欧几里得范数。设 $S = \\mathrm{diag}(\\|a_1\\|_2,\\dots,\\|a_n\\|_2)$，其中 $a_j$ 表示 $A$ 的第 $j$ 列。定义变换后的变量 $y = S x$，变换后的约束矩阵 $A' = A S^{-1}$，以及变换后的目标向量 $c' = S^{-1} c$。从 $y^{(0)} = S x^{(0)}$ 开始，对 $(A', b, c')$ 在 $y$ 变量上应用仿射尺度算法。为进行公平比较，在映射回 $x = S^{-1} y$ 后，使用在原始变量中定义的相同最优性残差来检查收敛性。\n\n两种变体的停止规则：在当前点 $x$ 处，构建 $D = \\mathrm{diag}(x)$ 并计算拉格朗日乘子 $\\lambda$，其解为 $m \\times m$ 线性系统 $(A D^2 A^\\top)\\, \\lambda = A D^2 c$。定义简约成本向量 $r = c - A^\\top \\lambda$。当 $\\| D r \\|_\\infty \\le 10^{-6}$ 时终止。使用步长保护参数 $\\beta = 0.9$，并在每次迭代中将步长 $\\alpha$ 设置为 $\\beta \\cdot \\min(1, \\alpha_{\\max})$，其中 $\\alpha_{\\max}$ 是保持 $x + \\alpha' p > 0$ 的最大步长 $\\alpha'$。设置 $10{,}000$ 次迭代的硬上限；如果方法在上限内未达到容差，则报告上限作为迭代次数。\n\n实例构建：\n- 将随机数生成器种子固定为 $2025$。\n- 设置 $m = 2$ 和 $n = 30$。\n- 从 $[0.5, 1.5]$ 的均匀分布中抽取独立条目，生成一个基矩阵 $A_{\\text{base}} \\in \\mathbb{R}^{m \\times n}$。\n- 对于一个散度参数 $p \\in \\{0, 3, 6\\}$，定义一个列缩放向量 $s \\in \\mathbb{R}^n$，其元素为 $s_j = 10^{\\ell_j}$，其中 $\\ell_j$ 对于 $j = 1, \\dots, n$ 在 $[-p, p]$ 上均匀分布。构建 $A = A_{\\text{base}} \\, \\mathrm{diag}(s)$，使得 $A$ 的列存在数量级差异。\n- 从 $[0.5, 1.5]$ 的均匀分布中抽取独立条目，生成一个严格正的初始点 $x^{(0)} \\in \\mathbb{R}^n$。\n- 设置 $b = A x^{(0)}$，从而使 $x^{(0)}$ 成为严格可行点。\n- 从 $[0.5, 1.5]$ 的均匀分布中抽取独立条目，生成目标向量 $c \\in \\mathbb{R}^n$。\n\n测试套件：\n- 对相同的 $A_{\\text{base}}$、$x^{(0)}$ 和 $c$ 使用三个散度参数 $p \\in \\{0, 3, 6\\}$。对于每个 $p$，按上述规定构建 $A$ 和 $b$，然后使用相同的容差、步长保护参数和迭代上限运行两种算法变体。\n- 对于每个 $p$，记录标准变体所用的迭代次数和列归一化变体所用的迭代次数，两者均根据原始变量中的相同停止规则进行测量。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例（每个 $p$），结果是一个包含两个整数元素的列表 $[\\text{iters\\_standard}, \\text{iters\\_normalized}]$。因此，最终输出应类似于 $[[k_0^{\\text{std}},k_0^{\\text{norm}}],[k_1^{\\text{std}},k_1^{\\text{norm}}],[k_2^{\\text{std}},k_2^{\\text{norm}}]]$，其中每个 $k_i^{\\text{std}}$ 和 $k_i^{\\text{norm}}$ 都是整数迭代次数。\n\n角度单位不适用。不存在物理单位。所有报告的值都是无单位的整数。确保您的代码在指定的种子和参数下是确定性的，并且不需要任何外部输入。",
            "solution": "该问题是有效的。它提出了一个定义明确的计算实验，用于评估列缩放对原始仿射尺度算法在线性规划问题上性能的影响。所有参数、方法和评估标准都已明确指定。\n\n任务是求解以下形式的线性规划 (LP) 问题：\n$$ \\text{minimize } c^\\top x \\quad \\text{subject to} \\quad Ax = b, \\quad x > 0 $$\n其中 $c, x \\in \\mathbb{R}^n$，$A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$。条件 $x > 0$ 意味着向量 $x$ 的每个分量 $x_j$ 都必须是严格正数。这种问题结构适合采用内点法，例如原始仿射尺度算法。\n\n原始仿射尺度算法是一种迭代方法，它从一个严格可行点 $x^{(0)}$（即 $A x^{(0)} = b$ 且 $x^{(0)} > 0$）开始，生成一个迭代序列 $x^{(k)}$，这些迭代点保持严格可行，并旨在降低目标函数值。\n\n在每次迭代 $k$ 中，给定当前迭代点 $x^{(k)}$，算法的核心是找到一个合适的搜索方向 $p^{(k)}$ 和步长 $\\alpha^{(k)}$。然后，下一个迭代点计算为 $x^{(k+1)} = x^{(k)} + \\alpha^{(k)} p^{(k)}$。\n\n搜索方向 $p^{(k)}$ 是通过在缩放坐标系中考虑问题而导出的。令 $D_k = \\mathrm{diag}(x^{(k)})$ 为对角矩阵，其对角线元素是当前迭代点 $x^{(k)}$ 的分量。我们定义变量变换 $x = D_k y$。当前点 $x^{(k)}$ 映射到 $y^{(k)} = (D_k)^{-1} x^{(k)} = e$，其中 $e$ 是全一向量。线性规划的约束变为 $(AD_k)y = b$。原始空间中的可行方向 $p^{(k)}$ 必须满足 $A p^{(k)} = 0$。这对应于缩放空间中满足 $(AD_k) d_y = 0$ 的方向 $d_y$。\n\n仿射尺度方向是缩放后目标函数的负梯度在缩放后约束矩阵 $AD_k$ 的零空间上的投影。缩放后的目标是 $(D_k c)^\\top y$，因此其梯度为 $D_k c$。投影到矩阵 $M$ 的零空间上的投影矩阵是 $I - M^\\top(MM^\\top)^{-1}M$。因此，缩放空间中的方向为：\n$$ d_y = - \\left( I - (AD_k)^\\top(AD_k(AD_k)^\\top)^{-1}AD_k \\right) D_k c $$\n通过 $p^{(k)} = D_k d_y$ 将其映射回原始空间，我们得到搜索方向：\n$$ p^{(k)} = -D_k^2 \\left( c - A^\\top (AD_k^2A^\\top)^{-1}A D_k^2 c \\right) $$\n我们可以识别出拉格朗日乘子 $\\lambda^{(k)} = (AD_k^2A^\\top)^{-1}A D_k^2 c$ 和简约成本向量 $r^{(k)} = c - A^\\top \\lambda^{(k)}$。然后，方向可以简化为：\n$$ p^{(k)} = -D_k^2 r^{(k)} $$\n\n停止准则是基于一阶最优性条件，该条件要求解的简约成本向量为非负。内点法的一个常见条件是检查缩放后简约成本的范数。根据规定，当 $\\|D_k r^{(k)}\\|_\\infty \\le \\epsilon$ 时算法终止，容差为 $\\epsilon = 10^{-6}$。\n\n必须选择步长 $\\alpha^{(k)}$ 以确保下一个迭代点保持严格可行，即 $x^{(k+1)} = x^{(k)} + \\alpha^{(k)} p^{(k)} > 0$。对于任何 $p_j^{(k)}  0$ 的分量 $j$，必须有 $\\alpha^{(k)}  -x_j^{(k)} / p_j^{(k)}$。这导致了最大可能步长 $\\alpha_{\\text{max_to_boundary}} = \\min_{j: p_j^{(k)}0} (-x_j^{(k)} / p_j^{(k)})$。问题指定了一个特定规则：将步长设为 $\\alpha^{(k)} = \\beta \\cdot \\min(1, \\alpha_{\\text{max_to_boundary}})$，其中保护参数 $\\beta = 0.9$。\n\n我们实现并比较该算法的两种变体：\n\n1.  **标准变体**：此变体将前述的仿射尺度算法直接应用于给定的问题数据 $(A, b, c)$，从 $x^{(0)}$ 开始。\n\n2.  **列归一化变体**：此变体旨在减轻因矩阵 $A$ 的列范数存在较大差异而导致的潜在病态问题。它通过变量变换引入了一个预处理步骤。\n    - 首先，我们定义一个缩放矩阵 $S = \\mathrm{diag}(\\|a_1\\|_2, \\|a_2\\|_2, \\dots, \\|a_n\\|_2)$，其中 $a_j$ 是 $A$ 的第 $j$ 列。\n    - 我们引入新变量 $y = Sx$，这意味着 $x = S^{-1}y$。\n    - 原始 LP 被转换为关于 $y$ 变量的等价 LP：\n      $$ \\text{minimize } (S^{-1}c)^\\top y \\quad \\text{subject to} \\quad (AS^{-1})y = b, \\quad y > 0 $$\n    - 令 $c' = S^{-1}c$ 和 $A' = AS^{-1}$。现在 $A'$ 的列具有单位欧几里得范数。\n    - 然后，将标准仿射尺度算法应用于变换后的问题 $(A', b, c')$，从变换后的初始点 $y^{(0)} = S x^{(0)}$ 开始。\n    - 为进行公平比较，停止准则在每次迭代时都在原始问题空间中评估。迭代点 $y^{(k)}$ 被映射回 $x^{(k)} = S^{-1} y^{(k)}$，并使用原始数据 $(A, b, c)$ 和当前点 $x^{(k)}$ 检查终止规则 $\\|D_k r^{(k)}\\|_\\infty \\le 10^{-6}$。\n\n该实验通过从一个基矩阵 $A_{\\text{base}}$ 开始，并使用散度参数 $p \\in \\{0, 3, 6\\}$ 在其列中引入尺度差异来构建实例。对于每个 $p$，两种算法变体都会运行，并记录收敛所需的迭代次数，最大上限为 $10,000$。这使得我们能够定量比较在面对尺度不佳的数据时，列归一化如何影响算法的收敛速度。",
            "answer": "```python\nimport numpy as np\n\ndef run_affine_scaling(A, b, c, x0, tol, max_iter, beta):\n    \"\"\"\n    Implements the standard primal affine scaling algorithm.\n    \"\"\"\n    m, n = A.shape\n    x = x0.copy()\n\n    for k in range(max_iter):\n        # 1. Check stopping criterion\n        # Using element-wise operations with x vector is more efficient than\n        # explicitly forming the diagonal matrix D. D^2 is diag(x**2).\n        x_sq = x * x\n        AD2 = A * x_sq  # Equivalent to A @ diag(x_sq)\n        M = AD2 @ A.T\n\n        try:\n            # Check for singularity.\n            if np.linalg.cond(M) > 1 / np.finfo(M.dtype).eps:\n                # The matrix is numerically singular, likely due to ill-conditioning.\n                # This can happen with large p, algorithm fails to proceed.\n                return max_iter\n            \n            # Solve the m x m system for Lagrange multipliers\n            lambda_ = np.linalg.solve(M, AD2 @ c)\n        except np.linalg.LinAlgError:\n            # If solver fails, indicates breakdown of the method\n            return max_iter\n\n        r = c - A.T @ lambda_  # Reduced cost\n        \n        # Norm of scaled reduced cost: ||D r||_inf\n        Dr_norm_inf = np.linalg.norm(x * r, ord=np.inf)\n        if Dr_norm_inf = tol:\n            return k\n\n        # 2. Compute search direction\n        p = -x_sq * r\n\n        # 3. Compute step size\n        neg_p_indices = np.where(p  0)[0]\n        if len(neg_p_indices) == 0:\n            # Direction is non-negative, objective is unbounded below.\n            # This should not happen in this problem's setup.\n            return max_iter \n        \n        # Max step to the boundary\n        alpha_max_to_bnd = np.min(-x[neg_p_indices] / p[neg_p_indices])\n\n        # Per problem spec: alpha = beta * min(1, alpha_max)\n        alpha_cand = min(1.0, alpha_max_to_bnd)\n        \n        # Final step size with safeguard\n        alpha = beta * alpha_cand\n        \n        # 4. Update iterate\n        x += alpha * p\n\n    return max_iter\n\ndef run_affine_scaling_normalized(A_orig, b_orig, c_orig, x0_orig, tol, max_iter, beta):\n    \"\"\"\n    Implements the column-normalized primal affine scaling algorithm.\n    \"\"\"\n    m, n = A_orig.shape\n\n    # 1. Preprocessing: Create the transformed problem\n    col_norms = np.linalg.norm(A_orig, axis=0)\n    # Avoid division by zero if a column is zero, though not expected here.\n    col_norms[col_norms == 0] = 1.0  \n    \n    A_prime = A_orig / col_norms\n    c_prime = c_orig / col_norms\n    b_prime = b_orig # b is unchanged\n    y0 = x0_orig * col_norms\n\n    y = y0.copy()\n\n    for k in range(max_iter):\n        # 2. Check stopping criterion in the original space\n        x = y / col_norms\n        x_sq = x * x\n        \n        AD2_orig = A_orig * x_sq\n        M_orig = AD2_orig @ A_orig.T\n\n        try:\n            if np.linalg.cond(M_orig) > 1 / np.finfo(M_orig.dtype).eps:\n                return max_iter\n            lambda_orig = np.linalg.solve(M_orig, AD2_orig @ c_orig)\n        except np.linalg.LinAlgError:\n            return max_iter\n            \n        r_orig = c_orig - A_orig.T @ lambda_orig\n        \n        Dr_norm_inf = np.linalg.norm(x * r_orig, ord=np.inf)\n        if Dr_norm_inf = tol:\n            return k\n\n        # 3. Perform one iteration in the transformed (y) space\n        y_sq = y * y\n        A_prime_D2y = A_prime * y_sq\n        M_y = A_prime_D2y @ A_prime.T\n        \n        try:\n            if np.linalg.cond(M_y) > 1 / np.finfo(M_y.dtype).eps:\n                return max_iter\n            lambda_y = np.linalg.solve(M_y, A_prime_D2y @ c_prime)\n        except np.linalg.LinAlgError:\n            return max_iter\n\n        r_y = c_prime - A_prime.T @ lambda_y\n        p_y = -y_sq * r_y\n\n        neg_py_indices = np.where(p_y  0)[0]\n        if len(neg_py_indices) == 0:\n            return max_iter\n        \n        alpha_max_to_bnd_y = np.min(-y[neg_py_indices] / p_y[neg_py_indices])\n        alpha_cand_y = min(1.0, alpha_max_to_bnd_y)\n        alpha_y = beta * alpha_cand_y\n        \n        y += alpha_y * p_y\n\n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to set up the problem, run the simulations, and format the output.\n    \"\"\"\n    # Fixed parameters\n    seed = 2025\n    m, n = 2, 30\n    tol = 1e-6\n    beta = 0.9\n    max_iter = 10000\n    spread_params = [0, 3, 6]\n\n    # Generate base data using the fixed seed\n    rng = np.random.default_rng(seed)\n    A_base = rng.uniform(0.5, 1.5, size=(m, n))\n    x0 = rng.uniform(0.5, 1.5, size=n)\n    c = rng.uniform(0.5, 1.5, size=n)\n\n    results = []\n\n    for p in spread_params:\n        # Construct the A matrix and b vector for the current spread parameter\n        if p == 0:\n            s_vec = np.ones(n)\n        else:\n            # Use uniform distribution for exponents as per problem description\n            exponents = rng.uniform(-p, p, size=n)\n            s_vec = 10.0**exponents\n        \n        # A = A_base * s_vec is equivalent to A_base @ diag(s_vec)\n        A = A_base * s_vec\n        b = A @ x0\n\n        # Run both algorithm variants\n        iters_standard = run_affine_scaling(A, b, c, x0, tol, max_iter, beta)\n        iters_normalized = run_affine_scaling_normalized(A, b, c, x0, tol, max_iter, beta)\n        \n        # Store results for this test case\n        results.append([iters_standard, iters_normalized])\n\n    # Format the final output string exactly as required\n    output_parts = [f\"[{std},{norm}]\" for std, norm in results]\n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```"
        }
    ]
}