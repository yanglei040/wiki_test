{
    "hands_on_practices": [
        {
            "introduction": "To truly understand an algorithm, there is no substitute for working through its mechanics by hand. This first practice provides a concrete exercise to demystify the Affine Scaling algorithm. By performing a single, complete iteration on a small-scale linear program, you will calculate each key component—from the dual variable estimate to the final step—and see firsthand how they fit together to move towards an optimal solution .",
            "id": "3095992",
            "problem": "Consider the following Linear Program (LP): minimize the linear objective $c^{\\top} x$ subject to the affine equality constraints $A x = b$ and the nonnegativity constraints $x \\ge 0$. The data are\n$$\nA = \\begin{pmatrix} 1  1  1 \\end{pmatrix}, \\quad b = 3, \\quad c = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix},\n$$\nand the strictly feasible starting point is\n$$\nx^{(0)} = \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{3}{2} \\\\ 1 \\end{pmatrix}.\n$$\nPerform one full iteration of the primal Affine Scaling (AS) method, beginning from first principles: interpret the search direction as the steepest descent direction for the linearized objective $c^{\\top} d$ restricted to the affine feasible set $A d = 0$ under the local norm induced by $X^{-1}$, where $X = \\mathrm{diag}(x^{(0)})$. Using this interpretation, determine the Lagrange multiplier $y$ that enforces first-order feasibility, then form the corresponding search direction $p$, and finally choose a step size $\\alpha$ using the fraction-to-the-boundary rule with parameter $\\beta = \\tfrac{9}{10}$. Compute the updated point $x^{(1)} = x^{(0)} + \\alpha p$. Provide the exact values for the components of $x^{(1)}$; do not round. Express your final answer as a single row vector.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. The provided starting point $x^{(0)}$ is verified to be strictly feasible:\n$A x^{(0)} = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\tfrac{1}{2} \\\\ \\tfrac{3}{2} \\\\ 1 \\end{pmatrix} = \\frac{1}{2} + \\frac{3}{2} + 1 = 3 = b$.\nAll components of $x^{(0)}$ are strictly positive. Thus, the problem is valid and we proceed with the solution.\n\nThe primal affine scaling method generates a search direction $p$ at a current strictly feasible point $x$ by considering a scaled version of the problem. As described, we seek the steepest descent direction for the objective $c^{\\top}d$ within the feasible subspace $Ad=0$, measured by the local norm $\\|d\\|_{X^{-1}} = \\sqrt{d^{\\top}X^{-2}d}$, where $X = \\mathrm{diag}(x)$. This is formulated as the optimization problem:\n$$\n\\begin{array}{ll}\n\\text{minimize}  c^{\\top} d \\\\\n\\text{subject to}  Ad = 0 \\\\\n                  d^{\\top}X^{-2}d = \\text{constant}\n\\end{array}\n$$\nThe Lagrangian for this problem is $L(d, y) = c^{\\top}d - y^{\\top}(Ad)$, where we consider only the equality constraint on the direction. The optimality condition with respect to $d$ in the scaled norm implies that the gradient of the objective with respect to the scaled coordinates must be orthogonal to the scaled feasible subspace. The search direction $p$ is given by the projection of the negative scaled gradient onto the null space of the scaled constraint matrix.\n\nLet the scaled variables be $\\tilde{d} = X^{-1} d$ and the scaled data be $\\tilde{A} = AX$ and $\\tilde{c} = Xc$. The search direction in the scaled space, $\\tilde{p}$, is the projection of $-\\tilde{c}$ onto the null space of $\\tilde{A}$:\n$$\n\\tilde{p} = -\\left(I - \\tilde{A}^{\\top}(\\tilde{A}\\tilde{A}^{\\top})^{-1}\\tilde{A}\\right)\\tilde{c}\n$$\nThe search direction in the original space is then $p = X\\tilde{p}$.\n\nAlternatively, and more directly, the first-order optimality conditions for the original LP imply the existence of a dual variable estimate $y$ and a reduced cost vector $s = c - A^{\\top}y$. The affine scaling search direction $p$ is given by $p = -X^2 s$. The dual estimate $y$ (the Lagrange multiplier) is chosen to ensure that the search direction lies in the null space of $A$, i.e., $Ap=0$.\n$A(-X^2(c-A^{\\top}y)) = 0 \\implies -AX^2c + AX^2A^{\\top}y = 0 \\implies y = (AX^2A^{\\top})^{-1}AX^2c$. This matches the problem's request to find the Lagrange multiplier $y$.\n\nWe are given the starting point $x^{(0)} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix}$.\nThe scaling matrix $X$ and its square are:\n$$\nX = \\mathrm{diag}(x^{(0)}) = \\begin{pmatrix} \\frac{1}{2}  0  0 \\\\ 0  \\frac{3}{2}  0 \\\\ 0  0  1 \\end{pmatrix}, \\quad X^2 = \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{9}{4}  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\nGiven data: $A = \\begin{pmatrix} 1  1  1 \\end{pmatrix}$ and $c = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix}$.\n\nFirst, we compute the components needed for the Lagrange multiplier $y$:\n$$\nAX^2 = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{9}{4}  0 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}  \\frac{9}{4}  1 \\end{pmatrix}\n$$\n$$\nAX^2A^{\\top} = \\begin{pmatrix} \\frac{1}{4}  \\frac{9}{4}  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{4} + \\frac{9}{4} + 1 = \\frac{10}{4} + 1 = \\frac{5}{2} + 1 = \\frac{7}{2}\n$$\n$$\nAX^2c = \\begin{pmatrix} \\frac{1}{4}  \\frac{9}{4}  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\frac{1}{4} + \\frac{18}{4} + 0 = \\frac{19}{4}\n$$\nNow, we compute the Lagrange multiplier $y$:\n$$\ny = (AX^2A^{\\top})^{-1}AX^2c = \\left(\\frac{7}{2}\\right)^{-1} \\left(\\frac{19}{4}\\right) = \\frac{2}{7} \\cdot \\frac{19}{4} = \\frac{19}{14}\n$$\nNext, we determine the reduced cost vector $s = c - A^{\\top}y$:\n$$\ns = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\left(\\frac{19}{14}\\right) = \\begin{pmatrix} 1 - \\frac{19}{14} \\\\ 2 - \\frac{19}{14} \\\\ 0 - \\frac{19}{14} \\end{pmatrix} = \\begin{pmatrix} \\frac{14-19}{14} \\\\ \\frac{28-19}{14} \\\\ -\\frac{19}{14} \\end{pmatrix} = \\begin{pmatrix} -\\frac{5}{14} \\\\ \\frac{9}{14} \\\\ -\\frac{19}{14} \\end{pmatrix}\n$$\nThe search direction $p$ is $p = -X^2 s$:\n$$\np = - \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{9}{4}  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} -\\frac{5}{14} \\\\ \\frac{9}{14} \\\\ -\\frac{19}{14} \\end{pmatrix} = - \\begin{pmatrix} -\\frac{5}{56} \\\\ \\frac{81}{56} \\\\ -\\frac{19}{14} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{56} \\\\ -\\frac{81}{56} \\\\ \\frac{19}{14} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{56} \\\\ -\\frac{81}{56} \\\\ \\frac{76}{56} \\end{pmatrix}\n$$\nTo determine the step size $\\alpha$, we use the fraction-to-the-boundary rule, $\\alpha = \\beta \\alpha_{\\max}$. First, we find the maximum step $\\alpha_{\\max}$ that preserves nonnegativity, $x^{(0)} + \\alpha p \\ge 0$. This is limited by components where $p_i  0$.\n$$\n\\alpha_{\\max} = \\min_{i: p_i  0} \\left\\{ \\frac{x_i^{(0)}}{-p_i} \\right\\}\n$$\nIn our case, only $p_2 = -\\frac{81}{56}$ is negative.\n$$\n\\alpha_{\\max} = \\frac{x_2^{(0)}}{-p_2} = \\frac{\\frac{3}{2}}{-(-\\frac{81}{56})} = \\frac{3}{2} \\cdot \\frac{56}{81} = \\frac{3 \\cdot 28}{81} = \\frac{28}{27}\n$$\nThe step size parameter is $\\beta = \\frac{9}{10}$. So the step size is:\n$$\n\\alpha = \\beta \\alpha_{\\max} = \\frac{9}{10} \\cdot \\frac{28}{27} = \\frac{1}{10} \\cdot \\frac{28}{3} = \\frac{28}{30} = \\frac{14}{15}\n$$\nFinally, we compute the new iterate $x^{(1)} = x^{(0)} + \\alpha p$:\n$$\nx^{(1)} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} + \\frac{14}{15} \\begin{pmatrix} \\frac{5}{56} \\\\ -\\frac{81}{56} \\\\ \\frac{76}{56} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{14 \\cdot 5}{15 \\cdot 56} \\\\ \\frac{14 \\cdot (-81)}{15 \\cdot 56} \\\\ \\frac{14 \\cdot 76}{15 \\cdot 56} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{5}{15 \\cdot 4} \\\\ \\frac{-81}{15 \\cdot 4} \\\\ \\frac{76}{15 \\cdot 4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{5}{60} \\\\ -\\frac{81}{60} \\\\ \\frac{76}{60} \\end{pmatrix}\n$$\n$$\nx_1^{(1)} = \\frac{1}{2} + \\frac{5}{60} = \\frac{30}{60} + \\frac{5}{60} = \\frac{35}{60} = \\frac{7}{12}\n$$\n$$\nx_2^{(1)} = \\frac{3}{2} - \\frac{81}{60} = \\frac{90}{60} - \\frac{81}{60} = \\frac{9}{60} = \\frac{3}{20}\n$$\n$$\nx_3^{(1)} = 1 + \\frac{76}{60} = \\frac{60}{60} + \\frac{76}{60} = \\frac{136}{60} = \\frac{34}{15}\n$$\nThe updated point is $x^{(1)} = \\begin{pmatrix} \\frac{7}{12} \\\\ \\frac{3}{20} \\\\ \\frac{34}{15} \\end{pmatrix}$.\nAs a check, we confirm $A x^{(1)} = b$:\n$$\n\\frac{7}{12} + \\frac{3}{20} + \\frac{34}{15} = \\frac{35}{60} + \\frac{9}{60} + \\frac{136}{60} = \\frac{35+9+136}{60} = \\frac{180}{60} = 3\n$$\nThe constraint is satisfied.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{7}{12}  \\frac{3}{20}  \\frac{34}{15} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "After mastering the mechanics of a single step, we now zoom out to consider the algorithm's entire trajectory. This exercise explores the qualitative behavior of the affine scaling iterates under two different objective functions, revealing how the algorithm navigates the feasible region. You will analyze a case with a unique optimal solution and contrast it with a case where the entire feasible set is optimal, uncovering a critical condition under which the algorithm can stall .",
            "id": "3095998",
            "problem": "Consider the Linear Programming (LP) problem in standard form: minimize $c^{\\mathsf{T}} x$ subject to $A x = b$ and $x > 0$, where $A \\in \\mathbb{R}^{m \\times n}$ has full row rank and $x \\in \\mathbb{R}^n$ lies in the positive orthant. The primal affine scaling algorithm (ASA) chooses, at a strictly feasible point $x$, a descent direction $d$ by approximately solving a local trust-region problem over the set of feasible directions $\\{ d : A d = 0 \\}$, using the Dikin scaling matrix $D(x) = \\mathrm{diag}(x)$ and a step size $0  \\alpha  \\alpha_{\\max}(x)$ to maintain $x + \\alpha d > 0$. The trajectory $\\{ x^{(k)} \\}_{k \\ge 0}$ is defined by $x^{(k+1)} = x^{(k)} + \\alpha_k d^{(k)}$ with $x^{(0)}$ strictly feasible and $\\alpha_k$ sufficiently small to prevent leaving the positive orthant.\n\nInvestigate the qualitative behavior of the ASA trajectory by comparing the following two LPs with $A \\in \\mathbb{R}^{1 \\times 2}$, $A = \\begin{pmatrix} 1  1 \\end{pmatrix}$, $b = 1$, and a strictly feasible starting point $x^{(0)} = \\begin{pmatrix} p \\\\ 1 - p \\end{pmatrix}$ with $p \\in (0,1)$:\n\n- Case $\\mathrm{I}$ (unique optimal extreme point): $c = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, so the unique optimal solution is $x^\\star = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n- Case $\\mathrm{II}$ (multiple optimal extreme points): $c = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, so the entire face $\\{ x \\in \\mathbb{R}^2 : x_1 + x_2 = 1, x \\ge 0 \\}$ is optimal, and both extreme points $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ are optimal.\n\nWhich of the following statements about the ASA trajectory is correct? Select all that apply.\n\nA. In Case $\\mathrm{I}$, for sufficiently small step sizes, the ASA iterates $x^{(k)}$ satisfy $A x^{(k)} = b$ and converge to the unique optimal extreme point $x^\\star = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, with directions $d^{(k)}$ lying in the affine subspace $\\{ d : A d = 0 \\}$.\n\nB. In Case $\\mathrm{II}$, because $c \\in \\mathrm{range}(A^{\\mathsf{T}})$, the ASA projected descent direction at any strictly feasible $x$ is the zero vector; hence, the trajectory stalls immediately at $x^{(0)}$, which lies in the relative interior of the optimal face.\n\nC. In Case $\\mathrm{II}$, the ASA generically cycles indefinitely and never reaches the optimal face.\n\nD. In both Case $\\mathrm{I}$ and Case $\\mathrm{II}$, the ASA trajectory converges to the analytic center of the feasible region $\\{ x : A x = b, x > 0 \\}$, independent of the choice of $c$.\n\nE. In Case $\\mathrm{I}$, by choosing the step size at each iteration as the largest that preserves feasibility (i.e., $\\alpha_k = \\alpha_{\\max}(x^{(k)})$), the ASA reaches $x^\\star = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ in finitely many iterations.",
            "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n\nThe problem statement provides the following information:\n\n*   **Problem Type**: Linear Programming (LP) in standard form.\n*   **Objective**: Minimize $c^{\\mathsf{T}} x$.\n*   **Constraints**: $A x = b$ and $x > 0$.\n*   **Matrix and Vector Dimensions**: $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^n$. $A$ has full row rank.\n*   **Algorithm**: Primal affine scaling algorithm (ASA).\n*   **Starting Point**: A strictly feasible point $x^{(0)}$ ($A x^{(0)} = b$, $x^{(0)} > 0$).\n*   **Descent Direction $d$**: Chosen over the feasible directions $\\{ d : A d = 0 \\}$.\n*   **Scaling Matrix**: Dikin scaling matrix $D(x) = \\mathrm{diag}(x)$.\n*   **Step Size**: $0  \\alpha  \\alpha_{\\max}(x)$ to maintain strict feasibility $x + \\alpha d > 0$.\n*   **Update Rule**: $x^{(k+1)} = x^{(k)} + \\alpha_k d^{(k)}$.\n\nThe problem then specializes to a specific family of LPs:\n*   $A = \\begin{pmatrix} 1  1 \\end{pmatrix} \\in \\mathbb{R}^{1 \\times 2}$.\n*   $b = 1$.\n*   Strictly feasible starting point $x^{(0)} = \\begin{pmatrix} p \\\\ 1 - p \\end{pmatrix}$ with $p \\in (0,1)$.\n\nTwo cases for the cost vector $c$ are given:\n*   **Case I**: $c = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. The unique optimal solution is $x^\\star = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n*   **Case II**: $c = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. The optimal set is the entire face $\\{ x \\in \\mathbb{R}^2 : x_1 + x_2 = 1, x \\ge 0 \\}$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement describes the primal affine scaling algorithm, a well-established method in the field of numerical optimization for solving linear programs. The setup is a standard, canonical example used to illustrate the algorithm's behavior.\n\n*   **Scientifically Grounded**: The problem is rooted in the mathematical theory of optimization and linear programming. The affine scaling algorithm is a classical interior-point method. All concepts are standard.\n*   **Well-Posed**: The problem is clearly defined. The parameters $A$, $b$, and $c$ are explicitly given for two distinct cases. The question asks for the qualitative behavior of the ASA trajectory, which can be determined by analyzing the algorithm's mechanics.\n*   **Objective**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective content.\n*   **Completeness and Consistency**: The problem is self-contained. The matrix $A = \\begin{pmatrix} 1  1 \\end{pmatrix}$ has a rank of $1$, which matches its row dimension $m=1$, so it has full row rank as stipulated. The starting point $x^{(0)}$ is confirmed to be strictly feasible: $A x^{(0)} = p + (1-p) = 1 = b$, and for $p \\in (0,1)$, both components of $x^{(0)}$ are strictly positive. The descriptions of the optimal solutions in both cases are correct.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. It is scientifically sound, well-posed, and internally consistent. We may proceed to the solution.\n\n## Derivation of the Solution\n\nThe primal affine scaling algorithm generates a search direction $d$ at a strictly feasible point $x$ by solving a scaled trust-region subproblem. The standard explicit formula for this direction is:\n$$ d = -D(x)^2 (c - A^{\\mathsf{T}}w) $$\nwhere $D(x) = \\mathrm{diag}(x)$ is the scaling matrix and $w$ is the vector of dual variable estimates, given by:\n$$ w = (A D(x)^2 A^{\\mathsf{T}})^{-1} (A D(x)^2 c) $$\nWe first compute the general form of the direction $d$ for the specific problem parameters:\n$A = \\begin{pmatrix} 1  1 \\end{pmatrix}$, so $m=1$ and $n=2$. $A^{\\mathsf{T}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n$x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$, so $D(x) = \\begin{pmatrix} x_1  0 \\\\ 0  x_2 \\end{pmatrix}$ and $D(x)^2 = \\begin{pmatrix} x_1^2  0 \\\\ 0  x_2^2 \\end{pmatrix}$.\n\nThe terms in the expression for $w$ are:\n$A D(x)^2 = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} x_1^2  0 \\\\ 0  x_2^2 \\end{pmatrix} = \\begin{pmatrix} x_1^2  x_2^2 \\end{pmatrix}$.\n$A D(x)^2 A^{\\mathsf{T}} = \\begin{pmatrix} x_1^2  x_2^2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = x_1^2 + x_2^2$. This is a scalar.\n$(A D(x)^2 A^{\\mathsf{T}})^{-1} = \\frac{1}{x_1^2 + x_2^2}$.\n$A D(x)^2 c = \\begin{pmatrix} x_1^2  x_2^2 \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = c_1 x_1^2 + c_2 x_2^2$.\n\nSo, the dual estimate $w$ (a scalar in this case) is:\n$$ w = \\frac{c_1 x_1^2 + c_2 x_2^2}{x_1^2 + x_2^2} $$\nThe search direction $d$ is:\n$$ d = - \\begin{pmatrix} x_1^2  0 \\\\ 0  x_2^2 \\end{pmatrix} \\left( \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} w \\right) = - \\begin{pmatrix} x_1^2(c_1 - w) \\\\ x_2^2(c_2 - w) \\end{pmatrix} $$\nWe now analyze the two cases.\n\n**Analysis of Case I**\nHere, $c = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, so $c_1 = 1$, $c_2 = 0$.\nThe dual estimate is $w = \\frac{1 \\cdot x_1^2 + 0 \\cdot x_2^2}{x_1^2 + x_2^2} = \\frac{x_1^2}{x_1^2 + x_2^2}$.\nThe components of the direction vector $d = \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix}$ are:\n$d_1 = -x_1^2(c_1 - w) = -x_1^2 \\left( 1 - \\frac{x_1^2}{x_1^2 + x_2^2} \\right) = -x_1^2 \\left( \\frac{x_2^2}{x_1^2 + x_2^2} \\right) = -\\frac{x_1^2 x_2^2}{x_1^2 + x_2^2}$.\n$d_2 = -x_2^2(c_2 - w) = -x_2^2 \\left( 0 - \\frac{x_1^2}{x_1^2 + x_2^2} \\right) = \\frac{x_1^2 x_2^2}{x_1^2 + x_2^2}$.\nSo, $d = \\frac{x_1^2 x_2^2}{x_1^2 + x_2^2} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\nThe update is $x^{(k+1)} = x^{(k)} + \\alpha_k d^{(k)}$. Since $x_1, x_2  0$ and $\\alpha_k  0$, the first component $x_1$ will always decrease, and the second component $x_2$ will always increase. The trajectory of iterates $x^{(k)}$ moves along the feasible line segment $x_1 + x_2 = 1$ towards the vertex $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, which is the unique optimal solution $x^\\star$. For any step size rule where $\\alpha_k$ is a fraction of the maximum possible step, the iterates converge to $x^\\star$.\n\n**Analysis of Case II**\nHere, $c = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, so $c_1 = 1$, $c_2 = 1$.\nThis cost vector can be written as $c = A^{\\mathsf{T}}$, which means $c$ is in the range of $A^{\\mathsf{T}}$.\nThe dual estimate is $w = \\frac{1 \\cdot x_1^2 + 1 \\cdot x_2^2}{x_1^2 + x_2^2} = \\frac{x_1^2 + x_2^2}{x_1^2 + x_2^2} = 1$.\nThe search direction $d$ is:\n$d = - \\begin{pmatrix} x_1^2(1 - 1) \\\\ x_2^2(1 - 1) \\end{pmatrix} = - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe search direction is the zero vector. Therefore, the algorithm cannot make any progress.\n$x^{(1)} = x^{(0)} + \\alpha_0 d^{(0)} = x^{(0)}$. The trajectory stalls immediately at the starting point $x^{(0)}$.\nThe objective function is $c^{\\mathsf{T}}x = x_1+x_2=1$ for any feasible point. Thus, the entire feasible region is the optimal face. Any strictly feasible point $x^{(0)}$ is in the relative interior of this optimal face.\n\n## Option-by-Option Analysis\n\n**A. In Case I, for sufficiently small step sizes, the ASA iterates $x^{(k)}$ satisfy $A x^{(k)} = b$ and converge to the unique optimal extreme point $x^\\star = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, with directions $d^{(k)}$ lying in the affine subspace $\\{ d : A d = 0 \\}$.**\nThis statement is correct.\n1.  The direction $d$ is constructed to be in the null space of $A$, so $Ad=0$. If $Ax^{(k)}=b$, then $A x^{(k+1)} = A(x^{(k)} + \\alpha_k d^{(k)}) = Ax^{(k)} + \\alpha_k Ad^{(k)} = b + 0 = b$. Feasibility is maintained. The set $\\{d : Ad=0\\}$ is a linear subspace of $\\mathbb{R}^n$, which is a type of affine subspace.\n2.  Our analysis of Case I showed that the iterates move monotonically towards $x^\\star = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. The convergence of the short-step primal affine scaling algorithm to a unique optimum is a standard theoretical result.\n**Verdict: Correct.**\n\n**B. In Case II, because $c \\in \\mathrm{range}(A^{\\mathsf{T}})$, the ASA projected descent direction at any strictly feasible $x$ is the zero vector; hence, the trajectory stalls immediately at $x^{(0)}$, which lies in the relative interior of the optimal face.**\nThis statement is correct.\n1.  In Case II, $c = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $A^{\\mathsf{T}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, so indeed $c = A^{\\mathsf{T}}$, meaning $c \\in \\mathrm{range}(A^{\\mathsf{T}})$.\n2.  Our analysis of Case II showed that this condition leads to a reduced cost vector of zero, and thus the descent direction $d$ is the zero vector for any strictly feasible point $x$.\n3.  Consequently, the algorithm stalls at the initial point $x^{(0)}$.\n4.  The optimal face is the set $\\{x: x_1+x_2=1, x \\ge 0\\}$. The starting point $x^{(0)}$ with components in $(0,1)$ is in the relative interior of this set.\n**Verdict: Correct.**\n\n**C. In Case II, the ASA generically cycles indefinitely and never reaches the optimal face.**\nThis statement is incorrect. As established in the analysis of option B, the algorithm begins at a point $x^{(0)}$ which is already on the optimal face. The trajectory stalls at this point; it does not cycle, nor does it fail to reach the optimal face.\n**Verdict: Incorrect.**\n\n**D. In both Case I and Case II, the ASA trajectory converges to the analytic center of the feasible region $\\{ x : A x = b, x > 0 \\}$, independent of the choice of $c$.**\nThis statement is incorrect. The trajectory of the ASA is fundamentally dependent on the cost vector $c$.\n1.  The analytic center of the feasible region $\\{x: x_1+x_2=1, x_10, x_20\\}$ is the point that maximizes $\\ln(x_1) + \\ln(x_2)$, which is $x_{ac} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix}$.\n2.  In Case I, the trajectory converges to the optimal vertex $x^\\star = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, which is not the analytic center.\n3.  In Case II, the trajectory stalls at the starting point $x^{(0)} = \\begin{pmatrix} p \\\\ 1-p \\end{pmatrix}$, which is only the analytic center in the specific case where $p=1/2$.\nThe ASA is a steepest descent method in a scaled space; it should not be confused with central-path-following methods which explicitly target the analytic center.\n**Verdict: Incorrect.**\n\n**E. In Case I, by choosing the step size at each iteration as the largest that preserves feasibility (i.e., $\\alpha_k = \\alpha_{\\max}(x^{(k)})$), the ASA reaches $x^\\star = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ in finitely many iterations.**\nThis statement is incorrect within the context of the problem's definition of the algorithm. The problem statement explicitly defines the step size as $0  \\alpha  \\alpha_{\\max}(x)$, which is a strict inequality ensuring the iterates remain strictly feasible (i.e., in the interior of the positive orthant). This is a \"short-step\" or \"damped-step\" version of the algorithm. Under this rule, a boundary point where a component is zero can only be reached as a limit point, not in a finite number of steps. While it is true that a *different* algorithm, the \"long-step\" variant that takes a full step to the boundary ($\\alpha = \\alpha_{\\max}$), would reach the optimum in a single iteration in this specific problem, Option E describes a procedure that violates the premises given in the problem statement. The question is about the behavior of the ASA *as defined*, which requires $\\alpha_k  \\alpha_{\\max}$. Therefore, finite convergence is not achieved.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "Theory and small examples provide the foundation, but a deeper understanding of an algorithm's practical performance comes from implementation and experimentation. This final practice challenges you to code the Affine Scaling Algorithm and design a numerical experiment. By comparing the convergence behavior from two different starting points—the geometrically significant analytic center versus a random point—you will investigate the practical impact of initialization on the algorithm's efficiency .",
            "id": "3096001",
            "problem": "Consider the problem class of Linear Programming (LP): minimize a linear objective under equality constraints with strict positivity, stated as: find $x \\in \\mathbb{R}^n$ that solves\n$$\n\\text{minimize } c^\\top x \\quad \\text{subject to } A x = b,\\ \\ x > 0,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $c \\in \\mathbb{R}^n$ are given data, and $x > 0$ denotes strict componentwise positivity. The Affine Scaling Algorithm (ASA) is an interior method that iteratively updates a strictly positive feasible point to decrease the objective while maintaining equality feasibility, using a descent direction derived from first principles and a step size rule that preserves positivity.\n\nYour task is to design and implement a numerical experiment that compares convergence behavior of the Affine Scaling Algorithm (ASA) when initialized at two different interior points for the same LP: the analytic center versus a random interior point. Use the following foundational bases in your design:\n\n- The LP feasibility set is the intersection of an affine subspace $\\{x \\mid A x = b\\}$ with the open positive orthant $\\{x \\mid x > 0\\}$.\n- The analytic center of a polytope with strictly positive variables (under equality constraints) is the maximizer of the barrier function $\\sum_{i=1}^n \\log(x_i)$ over the feasible set. For the special case $A = \\mathbf{1}^\\top$ (a single equality $\\sum_{i=1}^n x_i = 1$), symmetry implies the analytic center is the uniform point with all coordinates equal, i.e., $x_i = 1/n$ for all $i$.\n- A random interior point for the simplex constraint $\\sum_{i=1}^n x_i = 1$ with $x > 0$ can be sampled from a Dirichlet distribution with strictly positive parameters.\n\nImplement the standard primal Affine Scaling Algorithm (ASA) from first principles, enforcing the following requirements:\n\n- At each iteration, form a strictly interior, equality-feasible update $x \\leftarrow x + \\alpha d$ with a direction $d$ that respects equality feasibility and a step size $\\alpha \\in (0,1)$ scaled to ensure the updated iterate remains strictly positive.\n- Use a stopping criterion based on a scientifically sound measure of stationarity and progress that is derivable from the LP optimality conditions and the ASA construction. Your criterion should terminate when the iterates are sufficiently close to stationarity or when no significant progress can be made within positivity constraints.\n\nExperimental design:\n\n- For each test case, run ASA twice on the same LP data: once initialized at the analytic center (uniform point) and once at a random interior point (Dirichlet sample with all parameters equal to $1$ and a specified random seed). Record the number of iterations taken to terminate and the final objective value $c^\\top x$ at termination for both initializations.\n\nTest suite:\n\n- Common constraint for all cases: $m = 1$, $A = \\mathbf{1}^\\top \\in \\mathbb{R}^{1 \\times n}$, $b = [1]$, so the feasible set is the standard open simplex $\\{x \\in \\mathbb{R}^n \\mid \\sum_{i=1}^n x_i = 1,\\ x_i > 0\\}$.\n- Use the following cases to exercise different behaviors:\n  - Case $1$ (happy path): $n = 4$, $c = [1, 2, 3, 4]$, random seed $42$, step fraction $\\theta = 0.9$, tolerance $\\varepsilon = 10^{-10}$, maximum iterations $10000$.\n  - Case $2$ (boundary condition with flat objective): $n = 4$, $c = [1, 1, 1, 1]$, random seed $7$, step fraction $\\theta = 0.9$, tolerance $\\varepsilon = 10^{-10}$, maximum iterations $10000$.\n  - Case $3$ (larger dimension and highly nonuniform costs): $n = 8$, $c = [4, 1, 3, 2, 5, 0.5, 7, 0.1]$, random seed $123$, step fraction $\\theta = 0.9$, tolerance $\\varepsilon = 10^{-10}$, maximum iterations $10000$.\n\nInitializations per case:\n\n- Analytic center: $x_{\\text{ac}} = \\left[\\frac{1}{n}, \\ldots, \\frac{1}{n}\\right]$.\n- Random interior: $x_{\\text{rand}}$ drawn from a Dirichlet distribution with all parameters equal to $1$ using the specified random seed.\n\nOutput specification:\n\n- For each test case, produce the tuple of results $[\\text{iter}_{\\text{ac}}, \\text{iter}_{\\text{rand}}, \\text{obj}_{\\text{ac}}, \\text{obj}_{\\text{rand}}]$, where $\\text{iter}_{\\text{ac}}$ and $\\text{iter}_{\\text{rand}}$ are integers giving the iteration counts until termination, and $\\text{obj}_{\\text{ac}}$ and $\\text{obj}_{\\text{rand}}$ are floats giving the final objective values $c^\\top x$ at termination.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets, with one four-element list per test case, in the order of the test suite above. For example, the format must be of the form\n$$\n\\left[[\\text{iter}_1^{\\text{ac}}, \\text{iter}_1^{\\text{rand}}, \\text{obj}_1^{\\text{ac}}, \\text{obj}_1^{\\text{rand}}],\\ [\\text{iter}_2^{\\text{ac}}, \\text{iter}_2^{\\text{rand}}, \\text{obj}_2^{\\text{ac}}, \\text{obj}_2^{\\text{rand}}],\\ [\\text{iter}_3^{\\text{ac}}, \\text{iter}_3^{\\text{rand}}, \\text{obj}_3^{\\text{ac}}, \\text{obj}_3^{\\text{rand}}]\\right].\n$$\n\nNo physical units are involved, and angles are not applicable. All numerical outputs must be plain floats or integers as specified.",
            "solution": "The problem requires the implementation and comparison of the Primal Affine Scaling Algorithm (ASA) for a Linear Program (LP) under two different initializations. The validation procedure confirms that the problem is well-posed, scientifically sound, and contains all necessary information for a unique, verifiable solution.\n\nThe LP is formulated as:\n$$\n\\text{minimize } c^\\top x \\quad \\text{subject to } A x = b,\\ \\ x > 0\n$$\nwhere $x, c \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{m \\times n}$, and $b \\in \\mathbb{R}^m$. The condition $x > 0$ signifies that each component $x_i$ must be strictly positive, making this an interior-point method. The algorithm generates a sequence of strictly feasible points $\\{x_k\\}$ that converge to the optimal solution.\n\nThe core of the Affine Scaling Algorithm is the computation of a descent direction at each strictly feasible iterate $x_k$. Let $X_k = \\text{diag}(x_k)$ be the diagonal matrix whose diagonal entries are the components of $x_k$. The algorithm employs a scaling transformation $y = X_k^{-1}x$, which maps the current iterate $x_k$ to the vector of all ones, $e = [1, \\dots, 1]^\\top$. In this scaled space, the feasibility constraints become $A X_k y = b$ and $y > 0$.\n\nThe goal is to find an update $x_{k+1} = x_k + d_k$ that remains feasible and decreases the objective function. In the scaled space, this corresponds to finding a direction $d_y$ such that $y_{k+1} = e + d_y$. For the new point to satisfy the equality constraints, we must have $A X_k (e + d_y) = b$. Since $A X_k e = A x_k = b$, this implies that the search direction in the scaled space must lie in the null space of the scaled constraint matrix, i.e., $A X_k d_y = 0$.\n\nTo find the best descent direction, we seek to minimize the change in the objective function, which in the scaled space is $(c^\\top X_k) d_y$. To ensure the problem is well-defined, we constrain the search direction to a unit sphere, $\\|d_y\\|_2 = 1$. The subproblem at each iteration is thus:\n$$\n\\text{minimize } (X_k c)^\\top d_y \\quad \\text{subject to } A X_k d_y = 0, \\quad \\|d_y\\|_2 = 1\n$$\nThe solution to this subproblem is the negative projection of the scaled cost vector, $c_s = X_k c$, onto the null space of $M = A X_k$. The projection matrix onto this null space is $P = I - M^\\top(M M^\\top)^{-1}M$. The optimal scaled direction is thus $d_y \\propto -P c_s$.\n\nThis leads to the primal affine scaling search direction in the original unscaled space. Let $p_k$ be the estimate of the dual variables, given by:\n$$\np_k = (A X_k^2 A^\\top)^{-1} A X_k^2 c\n$$\nThe dual slack, or reduced cost vector, is $s_k = c - A^\\top p_k$. The search direction $d_k$ is then computed as:\n$$\nd_k = -X_k^2 s_k = -X_k^2(c - A^\\top p_k)\n$$\nThis direction is guaranteed to be a feasible descent direction, as $A d_k = 0$ and $c^\\top d_k = -s_k^\\top X_k^2 s_k = -\\|X_k s_k\\|_2^2 \\le 0$.\n\nFor the specific test cases provided, the constraints are simplified to the standard simplex: $m = 1$, $A = \\mathbf{1}^\\top$, and $b=[1]$. The calculations become:\n- The matrix $A X_k^2 A^\\top$ becomes a scalar: $\\mathbf{1}^\\top X_k^2 \\mathbf{1} = \\sum_{i=1}^n x_{k,i}^2$.\n- The dual variable estimate $p_k$ is also a scalar:\n$$\np_k = \\frac{\\mathbf{1}^\\top X_k^2 c}{\\mathbf{1}^\\top X_k^2 \\mathbf{1}} = \\frac{\\sum_{i=1}^n x_{k,i}^2 c_i}{\\sum_{i=1}^n x_{k,i}^2}\n$$\n- The dual slack is $s_k = c - \\mathbf{1} p_k$.\n- The search direction components are $d_{k,i} = -x_{k,i}^2(c_i - p_k)$.\n\nOnce the direction $d_k$ is found, the next iterate is $x_{k+1} = x_k + \\alpha d_k$. The step size $\\alpha$ must be chosen to ensure strict feasibility, $x_{k+1}  0$. This requires $x_{k,i} + \\alpha d_{k,i}  0$ for all $i$. If $d_{k,i}  0$, we must have $\\alpha  -x_{k,i}/d_{k,i}$. The maximum possible step size is therefore:\n$$\n\\alpha_{\\max} = \\min_{i: d_{k,i}  0} \\left( \\frac{-x_{k,i}}{d_{k,i}} \\right)\n$$\nTo avoid stepping exactly to the boundary, a fraction $\\theta \\in (0,1)$ is used, yielding the final step size $\\alpha = \\theta \\alpha_{\\max}$. For this problem, $\\theta = 0.9$.\n\nThe algorithm must terminate when a suitable stopping criterion is met. A robust criterion, derivable from the Karush-Kuhn-Tucker (KKT) optimality conditions, is based on the stationarity of the iterates. The objective value ceases to change when $c^\\top d_k = -\\|X_k s_k\\|_2^2 = 0$. Since $x_k  0$, this implies $s_k=0$. The search direction $d_k = -X_k^2 s_k$ would also be zero. Therefore, the algorithm can be terminated when the norm of the search direction becomes smaller than a given tolerance $\\varepsilon$:\n$$\n\\|d_k\\|_2  \\varepsilon\n$$\nThis condition signifies that no significant feasible descent can be made, indicating that the current point is near optimal.\n\nThe experiment will execute this algorithm for each test case, starting from two initial points:\n1.  The analytic center of the simplex, $x_{\\text{ac}} = [\\frac{1}{n}, \\dots, \\frac{1}{n}]^\\top$.\n2.  A random interior point, $x_{\\text{rand}}$, sampled from a Dirichlet distribution with parameters $\\alpha_i=1$ for all $i=1, \\dots, n$, using a specified random seed for reproducibility.\n\nThe number of iterations and the final objective value $c^\\top x$ are recorded for both runs in each case, allowing for a comparison of convergence behavior based on the starting point.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef affine_scaling(\n    A: np.ndarray,\n    b: np.ndarray,\n    c: np.ndarray,\n    x0: np.ndarray,\n    theta: float,\n    epsilon: float,\n    max_iter: int,\n):\n    \"\"\"\n    Implements the Primal Affine Scaling Algorithm for Linear Programming.\n\n    Args:\n        A (np.ndarray): Constraint matrix (m x n).\n        b (np.ndarray): Constraint vector (m x 1).\n        c (np.ndarray): Cost vector (n x 1).\n        x0 (np.ndarray): Initial strictly feasible point (n x 1).\n        theta (float): Step size fraction (0  theta  1).\n        epsilon (float): Convergence tolerance.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        tuple: (iteration_count, final_objective_value)\n    \"\"\"\n    x = x0.copy()\n    n = len(c)\n\n    for k in range(max_iter):\n        # 1. Check for positivity\n        if np.any(x = 0):\n            # This should not happen with proper step size control\n            raise ValueError(\"Iterate x is not strictly positive.\")\n\n        # 2. Compute dual variables, slack, and search direction\n        x_sq = x * x\n        \n        # Specialized for A = 1^T (scalar calculations)\n        p_num = np.dot(x_sq, c)\n        p_den = np.sum(x_sq)\n\n        # Handle case where denominator is zero (can only happen if x=0)\n        if p_den  1e-20:\n             p = 0.0\n        else:\n             p = p_num / p_den\n        \n        s = c - p\n        d = -x_sq * s\n        \n        # 3. Check stopping criterion\n        norm_d = np.linalg.norm(d)\n        if norm_d  epsilon:\n            return k, np.dot(c, x)\n\n        # 4. Compute step size\n        d_neg_indices = np.where(d  0)[0]\n\n        if len(d_neg_indices) == 0:\n            # This implies d = 0. Since Ad = 0, we have sum(d_i) = 0.\n            # If all d_i = 0, this means d must be the zero vector.\n            # This case is handled by the stopping criterion norm_d  epsilon.\n            # If we reach here, it's a numerical precision issue.\n            return k, np.dot(c, x)\n\n        alpha_max = np.min(-x[d_neg_indices] / d[d_neg_indices])\n        alpha = theta * alpha_max\n\n        # 5. Update x\n        x = x + alpha * d\n    \n    # Max iterations reached\n    return max_iter, np.dot(c, x)\n\n\ndef solve():\n    \"\"\"\n    Runs the experimental comparison of the Affine Scaling Algorithm\n    for the specified test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: n=4, c=[1, 2, 3, 4], seed=42\n        {\n            \"n\": 4,\n            \"c\": np.array([1.0, 2.0, 3.0, 4.0]),\n            \"seed\": 42,\n            \"theta\": 0.9,\n            \"epsilon\": 1e-10,\n            \"max_iter\": 10000,\n        },\n        # Case 2: n=4, c=[1, 1, 1, 1], seed=7\n        {\n            \"n\": 4,\n            \"c\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"seed\": 7,\n            \"theta\": 0.9,\n            \"epsilon\": 1e-10,\n            \"max_iter\": 10000,\n        },\n        # Case 3: n=8, c=[4, 1, 3, 2, 5, 0.5, 7, 0.1], seed=123\n        {\n            \"n\": 8,\n            \"c\": np.array([4.0, 1.0, 3.0, 2.0, 5.0, 0.5, 7.0, 0.1]),\n            \"seed\": 123,\n            \"theta\": 0.9,\n            \"epsilon\": 1e-10,\n            \"max_iter\": 10000,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case[\"n\"]\n        c = case[\"c\"]\n        seed = case[\"seed\"]\n        theta = case[\"theta\"]\n        epsilon = case[\"epsilon\"]\n        max_iter = case[\"max_iter\"]\n\n        # Common constraints for all cases\n        A = np.ones((1, n))\n        b = np.array([1.0])\n\n        # Initialization 1: Analytic Center\n        x_ac_init = np.full(n, 1.0 / n)\n        \n        iter_ac, obj_ac = affine_scaling(A, b, c, x_ac_init, theta, epsilon, max_iter)\n\n        # Initialization 2: Random Interior Point from Dirichlet distribution\n        rng = np.random.default_rng(seed)\n        # Using numpy.random.Generator.dirichlet, available in numpy = 1.17\n        x_rand_init = rng.dirichlet(np.ones(n))\n\n        iter_rand, obj_rand = affine_scaling(A, b, c, x_rand_init, theta, epsilon, max_iter)\n\n        results.append([iter_ac, iter_rand, obj_ac, obj_rand])\n\n    # Final print statement must match the exact specified format.\n    # The str() of a list of lists in Python is very close.\n    # We remove spaces to be certain.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}