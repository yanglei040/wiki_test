{
    "hands_on_practices": [
        {
            "introduction": "To truly understand an algorithm, it's invaluable to perform a single iteration by hand. This exercise focuses on a Quadratic Program (QP) with simple box constraints, a common and important problem structure in fields ranging from engineering to finance. This practice will demystify the core of an interior-point method by having you form the barrier-augmented objective and calculate the Newton direction. By computing the gradient and Hessian yourself, you will gain a concrete understanding of how the logarithmic barrier function shapes the search landscape to guide the optimization process toward a solution .",
            "id": "3139232",
            "problem": "Consider the box-constrained Quadratic Program (QP), defined as minimizing the quadratic objective subject to simple bounds, $$\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2} x^{\\top} Q x + q^{\\top} x \\quad \\text{subject to} \\quad \\ell \\leq x \\leq u,$$ where $Q \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, $q \\in \\mathbb{R}^{n}$, and the bounds satisfy $\\ell_{i}  u_{i}$ for all $i \\in \\{1,\\dots,n\\}$. Interior-point methods convert inequality constraints into a barrier-augmented unconstrained problem using a strictly convex barrier function that diverges on the boundary of the feasible region.\n\n(a) Starting from the core definition of a logarithmic barrier that penalizes proximity to each bound, derive a barrier formulation for the box constraints that uses separate logarithmic terms for the lower and upper bounds. Then write the barrier-augmented objective $F_{t}(x)$ that combines the original objective with the barrier, using a positive scalar parameter $t  0$.\n\n(b) For the three-dimensional case ($n = 3$) with \n$$Q = \\begin{pmatrix} 4  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{pmatrix}, \\quad q = \\begin{pmatrix} -2 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\quad \\ell = \\begin{pmatrix} 0 \\\\ -1 \\\\ \\frac{1}{2} \\end{pmatrix}, \\quad u = \\begin{pmatrix} 3 \\\\ 2 \\\\ \\frac{5}{2} \\end{pmatrix},$$ \ntake the barrier parameter $t = 1$ and the strictly feasible point \n$$x = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}.$$\nCompute, by first principles, the gradient $\\nabla F_{t}(x)$ and Hessian $\\nabla^{2} F_{t}(x)$ of the barrier-augmented objective at the given $x$. Using exact arithmetic, perform one Newton step $p$ defined by solving the linear system \n$$\\nabla^{2} F_{t}(x) \\, p = - \\nabla F_{t}(x),$$ \nand then report the Newton decrement squared, defined by \n$$\\lambda(x)^{2} = \\nabla F_{t}(x)^{\\top} \\left(\\nabla^{2} F_{t}(x)\\right)^{-1} \\nabla F_{t}(x).$$\nGive the final answer for $\\lambda(x)^{2}$ as a single exact rational number. Do not round.",
            "solution": "The user wants me to solve a two-part problem related to interior-point methods for a box-constrained quadratic program.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- The problem is a box-constrained quadratic program (QP):\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2} x^{\\top} Q x + q^{\\top} x \\quad \\text{subject to} \\quad \\ell \\leq x \\leq u $$\n- $Q \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite matrix.\n- $q \\in \\mathbb{R}^{n}$, $\\ell \\in \\mathbb{R}^{n}$, $u \\in \\mathbb{R}^{n}$.\n- The bounds satisfy $\\ell_{i}  u_{i}$ for all $i \\in \\{1,\\dots,n\\}$.\n- Part (a) asks for the derivation of the logarithmic barrier formulation for the box constraints and the corresponding barrier-augmented objective function $F_{t}(x)$ with a parameter $t  0$.\n- Part (b) provides specific data for the $n=3$ case:\n  - $Q = \\begin{pmatrix} 4  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{pmatrix}$\n  - $q = \\begin{pmatrix} -2 \\\\ 0 \\\\ 1 \\end{pmatrix}$\n  - $\\ell = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1/2 \\end{pmatrix}$\n  - $u = \\begin{pmatrix} 3 \\\\ 2 \\\\ 5/2 \\end{pmatrix}$\n  - Barrier parameter $t = 1$.\n  - Strictly feasible point $x = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n- Part (b) requires the computation of:\n  1. The gradient $\\nabla F_{t}(x)$ and Hessian $\\nabla^{2} F_{t}(x)$ at the given point $x$.\n  2. One Newton step $p$ found by solving $\\nabla^{2} F_{t}(x) \\, p = - \\nabla F_{t}(x)$.\n  3. The Newton decrement squared, $\\lambda(x)^{2} = \\nabla F_{t}(x)^{\\top} \\left(\\nabla^{2} F_{t}(x)\\right)^{-1} \\nabla F_{t}(x)$.\n- The final answer for $\\lambda(x)^2$ must be a single exact rational number.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is a standard exercise in the field of convex optimization, applying the logarithmic barrier interior-point method to a quadratic program. This is a well-established and scientifically sound topic.\n- **Well-Posedness**:\n    - The matrix $Q$ is stated to be symmetric positive definite. Let's verify this using Sylvester's criterion on its leading principal minors:\n        - $D_1 = |4| = 4  0$.\n        - $D_2 = \\det\\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} = 4(3) - 1(1) = 11  0$.\n        - $D_3 = \\det(Q) = 4(3 \\cdot 2 - 1 \\cdot 1) - 1(1 \\cdot 2 - 1 \\cdot 0) = 4(5) - 1(2) = 18  0$.\n    Since all leading principal minors are positive, $Q$ is positive definite. This makes the original objective function strictly convex.\n    - The point $x$ must be strictly feasible for the logarithmic barrier to be defined. Let's check if $\\ell  x  u$:\n        - For $i=1$: $\\ell_1=0  x_1=1  u_1=3$. True.\n        - For $i=2$: $\\ell_2=-1  x_2=0  u_2=2$. True.\n        - For $i=3$: $\\ell_3=1/2  x_3=1  u_3=5/2$. True.\n    The point $x$ is strictly feasible. The barrier function and its derivatives are well-defined at this point.\n- **Completeness and Consistency**: The problem is self-contained. All necessary data and definitions are provided. There are no contradictions. The given data is numerically consistent.\n- **Conclusion**: The problem is scientifically grounded, well-posed, and all necessary information is provided. It is a valid problem.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will now proceed with the solution.\n\n**Part (a): Barrier Formulation**\n\nThe box constraint $\\ell \\leq x \\leq u$ can be written as $n$ pairs of inequality constraints:\n$$ x_i - \\ell_i \\geq 0 \\quad \\text{and} \\quad u_i - x_i \\geq 0 \\quad \\text{for } i = 1, \\dots, n $$\nThe logarithmic barrier function for a set of inequalities $g_j(x) \\geq 0$ is $\\phi(x) = -\\sum_j \\ln(g_j(x))$. Applying this to our constraints, the barrier function for the box constraints is:\n$$ \\phi(x) = -\\sum_{i=1}^{n} \\ln(x_i - \\ell_i) - \\sum_{i=1}^{n} \\ln(u_i - x_i) $$\nThis function is strictly convex and smooth in the interior of the feasible region, and it approaches $+\\infty$ as $x$ approaches the boundary.\n\nInterior-point methods solve a sequence of unconstrained problems where the objective is an augmentation of the original objective with the barrier function. A common formulation for the barrier-augmented objective is:\n$$ F_t(x) = f(x) + \\frac{1}{t} \\phi(x) $$\nwhere $f(x) = \\frac{1}{2} x^{\\top} Q x + q^{\\top} x$ is the original objective function and $t  0$ is the barrier parameter. As $t \\to \\infty$, the solutions to $\\min F_t(x)$ trace the central path towards the solution of the original constrained problem.\n\nSubstituting the expressions for $f(x)$ and $\\phi(x)$, we get:\n$$ F_t(x) = \\frac{1}{2} x^{\\top} Q x + q^{\\top} x - \\frac{1}{t} \\sum_{i=1}^{n} \\left[ \\ln(x_i - \\ell_i) + \\ln(u_i - x_i) \\right] $$\n\n**Part (b): Computation for the 3D Case**\n\nFor this part, we are given $n=3$, $t=1$, and specific values for $Q, q, \\ell, u,$ and $x$. With $t=1$, the objective function becomes:\n$$ F_1(x) = \\frac{1}{2} x^{\\top} Q x + q^{\\top} x - \\sum_{i=1}^{3} \\left[ \\ln(x_i - \\ell_i) + \\ln(u_i - x_i) \\right] $$\n\n**1. Gradient and Hessian Computation**\n\nThe gradient of $F_1(x)$ is:\n$$ \\nabla F_1(x) = Q x + q - \\begin{pmatrix} \\frac{1}{x_1 - \\ell_1} \\\\ \\frac{1}{x_2 - \\ell_2} \\\\ \\frac{1}{x_3 - \\ell_3} \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{u_1 - x_1} \\\\ \\frac{1}{u_2 - x_2} \\\\ \\frac{1}{u_3 - x_3} \\end{pmatrix} $$\nThe Hessian of $F_1(x)$ is:\n$$ \\nabla^2 F_1(x) = Q + \\text{diag}\\left(\\frac{1}{(x_i - \\ell_i)^2}\\right) + \\text{diag}\\left(\\frac{1}{(u_i - x_i)^2}\\right) $$\nWe evaluate these at the given point $x = (1, 0, 1)^{\\top}$. First, we compute the vectors $x-\\ell$ and $u-x$:\n$$ x - \\ell = \\begin{pmatrix} 1 - 0 \\\\ 0 - (-1) \\\\ 1 - 1/2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1/2 \\end{pmatrix} $$\n$$ u - x = \\begin{pmatrix} 3 - 1 \\\\ 2 - 0 \\\\ 5/2 - 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 3/2 \\end{pmatrix} $$\n\nLet's compute the gradient, which we denote by $g$:\n$$ Qx = \\begin{pmatrix} 4  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\end{pmatrix} $$\n$$ Qx + q = \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} -2 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 3 \\end{pmatrix} $$\nThe barrier gradient terms are:\n$$ \\begin{pmatrix} 1/1 \\\\ 1/1 \\\\ 1/(1/2) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix} \\quad \\text{and} \\quad \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/(3/2) \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 2/3 \\end{pmatrix} $$\nSo, the gradient is:\n$$ g = \\nabla F_1(x) = \\begin{pmatrix} 2 \\\\ 2 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 2/3 \\end{pmatrix} = \\begin{pmatrix} 1 + 1/2 \\\\ 1 + 1/2 \\\\ 1 + 2/3 \\end{pmatrix} = \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 5/3 \\end{pmatrix} $$\n\nNext, we compute the Hessian, which we denote by $H$:\nThe diagonal matrices from the barrier term are:\n$$ \\text{diag}\\left(\\frac{1}{1^2}, \\frac{1}{1^2}, \\frac{1}{(1/2)^2}\\right) = \\text{diag}(1, 1, 4) $$\n$$ \\text{diag}\\left(\\frac{1}{2^2}, \\frac{1}{2^2}, \\frac{1}{(3/2)^2}\\right) = \\text{diag}(1/4, 1/4, 4/9) $$\nThe Hessian is the sum of $Q$ and these two diagonal matrices:\n$$ H = \\nabla^2 F_1(x) = \\begin{pmatrix} 4  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{pmatrix} + \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  4 \\end{pmatrix} + \\begin{pmatrix} 1/4  0  0 \\\\ 0  1/4  0 \\\\ 0  0  4/9 \\end{pmatrix} $$\n$$ H = \\begin{pmatrix} 4+1+1/4  1  0 \\\\ 1  3+1+1/4  1 \\\\ 0  1  2+4+4/9 \\end{pmatrix} = \\begin{pmatrix} 21/4  1  0 \\\\ 1  17/4  1 \\\\ 0  1  58/9 \\end{pmatrix} $$\n\n**2. Newton Step and Newton Decrement Squared**\n\nThe Newton step $p$ is the solution to $H p = -g$. The Newton decrement squared is $\\lambda(x)^2 = -g^{\\top}p = g^{\\top}H^{-1}g$. We calculate $\\lambda(x)^2$ using the formula $\\lambda(x)^2 = \\frac{g^{\\top} (\\text{adj}(H)) g}{\\det(H)}$.\n\nFirst, we compute the determinant of $H$:\n$$ \\det(H) = \\frac{21}{4}\\left(\\frac{17}{4} \\cdot \\frac{58}{9} - 1\\right) - 1\\left(1 \\cdot \\frac{58}{9} - 0\\right) = \\frac{21}{4}\\left(\\frac{986}{36} - \\frac{36}{36}\\right) - \\frac{58}{9} $$\n$$ \\det(H) = \\frac{21}{4}\\left(\\frac{950}{36}\\right) - \\frac{58}{9} = \\frac{19950}{144} - \\frac{58}{9} = \\frac{3325}{24} - \\frac{58}{9} = \\frac{3 \\cdot 3325 - 8 \\cdot 58}{72} = \\frac{9975 - 464}{72} = \\frac{9511}{72} $$\n\nNext, we compute the adjugate of $H$. Since $H$ is symmetric, its adjugate (transpose of the cofactor matrix) is also symmetric.\n$C_{11} = \\frac{17}{4}\\frac{58}{9}-1 = \\frac{950}{36} = \\frac{475}{18}$.\n$C_{12} = -\\left(1 \\cdot \\frac{58}{9}\\right) = -\\frac{58}{9}$.\n$C_{13} = 1 \\cdot 1 = 1$.\n$C_{22} = \\frac{21}{4}\\frac{58}{9} = \\frac{1218}{36} = \\frac{203}{6}$.\n$C_{23} = -\\left(\\frac{21}{4} \\cdot 1\\right) = -\\frac{21}{4}$.\n$C_{33} = \\frac{21}{4}\\frac{17}{4}-1 = \\frac{357}{16}-1 = \\frac{341}{16}$.\n$$ \\text{adj}(H) = \\begin{pmatrix} 475/18  -58/9  1 \\\\ -58/9  203/6  -21/4 \\\\ 1  -21/4  341/16 \\end{pmatrix} $$\nLet $y = \\text{adj}(H) g$:\n$$ y_1 = (\\frac{475}{18})(\\frac{3}{2}) + (-\\frac{58}{9})(\\frac{3}{2}) + (1)(\\frac{5}{3}) = \\frac{475}{12} - \\frac{58}{6} + \\frac{5}{3} = \\frac{475 - 116 + 20}{12} = \\frac{379}{12} $$\n$$ y_2 = (-\\frac{58}{9})(\\frac{3}{2}) + (\\frac{203}{6})(\\frac{3}{2}) + (-\\frac{21}{4})(\\frac{5}{3}) = -\\frac{58}{6} + \\frac{203}{4} - \\frac{35}{4} = -\\frac{29}{3} + \\frac{168}{4} = -\\frac{29}{3} + 42 = \\frac{97}{3} $$\n$$ y_3 = (1)(\\frac{3}{2}) + (-\\frac{21}{4})(\\frac{3}{2}) + (\\frac{341}{16})(\\frac{5}{3}) = \\frac{3}{2} - \\frac{63}{8} + \\frac{1705}{48} = \\frac{72 - 378 + 1705}{48} = \\frac{1399}{48} $$\nNow we compute $g^{\\top}y = g^{\\top} \\text{adj}(H) g$:\n$$ g^{\\top}y = (\\frac{3}{2})(\\frac{379}{12}) + (\\frac{3}{2})(\\frac{97}{3}) + (\\frac{5}{3})(\\frac{1399}{48}) = \\frac{1137}{24} + \\frac{97}{2} + \\frac{6995}{144} $$\nThe least common denominator is $144$.\n$$ g^{\\top}y = \\frac{6 \\cdot 1137}{144} + \\frac{72 \\cdot 97}{144} + \\frac{6995}{144} = \\frac{6822 + 6984 + 6995}{144} = \\frac{20801}{144} $$\nFinally, we compute the Newton decrement squared:\n$$ \\lambda(x)^2 = \\frac{g^{\\top} \\text{adj}(H) g}{\\det(H)} = \\frac{20801/144}{9511/72} = \\frac{20801}{144} \\cdot \\frac{72}{9511} = \\frac{20801}{2 \\cdot 9511} = \\frac{20801}{19022} $$\nThe Newton step $p$ is given by $p = -H^{-1}g = -\\frac{y}{\\det(H)}$:\n$$ p = - \\frac{1}{9511/72} \\begin{pmatrix} 379/12 \\\\ 97/3 \\\\ 1399/48 \\end{pmatrix} = -\\frac{72}{9511} \\begin{pmatrix} 379/12 \\\\ 97/3 \\\\ 1399/48 \\end{pmatrix} = -\\begin{pmatrix} 6 \\cdot 379 / 9511 \\\\ 24 \\cdot 97 / 9511 \\\\ (3/2) \\cdot 1399 / 9511 \\end{pmatrix} = \\begin{pmatrix} -2274/9511 \\\\ -2328/9511 \\\\ -4197/19022 \\end{pmatrix} $$\nThe problem asks for the Newton decrement squared as the final answer.\n$$ \\lambda(x)^2 = \\frac{20801}{19022} $$",
            "answer": "$$\\boxed{\\frac{20801}{19022}}$$"
        },
        {
            "introduction": "Interior-point methods are not limited to problems with vector variables; their framework extends powerfully to optimization over cones of matrices, such as in Semidefinite Programs (SDPs). This exercise explores this generalization by introducing the logarithmic determinant (`log-det`) barrier, the natural counterpart to the standard logarithm for the cone of positive semidefinite matrices. By deriving the Newton direction for a small SDP, you will see how the fundamental principles of IPMs adapt to this more complex geometry, highlighting the method's versatility and introducing key tools for optimization in matrix spaces .",
            "id": "3139151",
            "problem": "Consider the Semidefinite Programming (SDP) problem of minimizing the linear objective over the positive semidefinite cone with linear equality constraints. Specifically, consider the $2 \\times 2$ symmetric variable matrix $X$ and the problem\n$$\\min\\ \\mathrm{tr}(C X)\\quad \\text{subject to}\\quad \\mathrm{tr}(A_1 X) = b_1,\\ \\mathrm{tr}(A_2 X) = b_2,\\ X \\succeq 0,$$\nwhere $C = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix}$, $A_1 = \\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix}$, $A_2 = \\begin{pmatrix}0  0 \\\\ 0  1\\end{pmatrix}$, $b_1 = 2$, and $b_2 = 1$. Using an interior-point method with the logarithmic determinant barrier $-\\mu \\ln \\det(X)$, form the barrier-augmented objective. Then, at the feasible interior point\n$$X^{(k)} = \\begin{pmatrix}2  0.2 \\\\ 0.2  1\\end{pmatrix},$$\nwith barrier parameter $\\mu = 1$, derive from first principles the gradient and Hessian of the barrier term with respect to $X$ using the Frobenius inner product, set up the equality-constrained Newton system, and compute the Newton direction $\\Delta X$ that preserves the linearized equality constraints at $X^{(k)}$.\n\nReport the Newton direction as the ordered triple $(\\Delta x_{11}, \\Delta x_{12}, \\Delta x_{22})$, where $X = \\begin{pmatrix}x_{11}  x_{12} \\\\ x_{12}  x_{22}\\end{pmatrix}$. Your final answer must be a single row matrix containing these three entries. No rounding is required.",
            "solution": "The user wants to find the Newton direction for a given Semidefinite Programming (SDP) problem at a specific interior point.\n\nThe problem is stated as:\n$$ \\min_{X} \\ \\mathrm{tr}(C X) $$\n$$ \\text{subject to} \\quad \\mathrm{tr}(A_1 X) = b_1, \\ \\mathrm{tr}(A_2 X) = b_2, \\ X \\succeq 0 $$\nwhere $X$ is a $2 \\times 2$ symmetric matrix, $X = \\begin{pmatrix} x_{11}  x_{12} \\\\ x_{12}  x_{22} \\end{pmatrix}$.\n\nThe given data are:\n$C = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix}$, $A_1 = \\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix}$, $A_2 = \\begin{pmatrix}0  0 \\\\ 0  1\\end{pmatrix}$, $b_1 = 2$, $b_2 = 1$.\nThe current iterate is $X^{(k)} = \\begin{pmatrix}2  0.2 \\\\ 0.2  1\\end{pmatrix}$ and the barrier parameter is $\\mu=1$.\n\nFirst, we verify that $X^{(k)}$ is a strictly feasible point.\n1.  Symmetry: $X^{(k)}$ is symmetric.\n2.  Equality constraints:\n    $\\mathrm{tr}(A_1 X^{(k)}) = \\mathrm{tr}\\left(\\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix} \\begin{pmatrix}2  0.2 \\\\ 0.2  1\\end{pmatrix}\\right) = 2 = b_1$.\n    $\\mathrm{tr}(A_2 X^{(k)}) = \\mathrm{tr}\\left(\\begin{pmatrix}0  0 \\\\ 0  1\\end{pmatrix} \\begin{pmatrix}2  0.2 \\\\ 0.2  1\\end{pmatrix}\\right) = 1 = b_2$.\n    The constraints are satisfied.\n3.  Strict positivity: $X^{(k)} \\succ 0$.\n    The leading principal minors of $X^{(k)}$ are $2  0$ and $\\det(X^{(k)}) = 2(1) - (0.2)^2 = 2 - 0.04 = 1.96  0$.\n    Thus, $X^{(k)}$ is a strictly feasible point (an interior point).\n\nThe interior-point method uses a barrier function to handle the $X \\succeq 0$ constraint. The barrier-augmented objective function is:\n$$ f(X) = \\mathrm{tr}(C X) - \\mu \\ln \\det(X) $$\nThe Newton step $\\Delta X$ is found by applying Newton's method to the KKT conditions of the constrained problem, or equivalently, by solving a quadratically approximated problem subject to linearized constraints.\n\nThe objective function of this quadratic subproblem is:\n$$ \\min_{\\Delta X} \\ \\mathrm{tr}(\\nabla f(X^{(k)})^T \\Delta X) + \\frac{1}{2} \\mathrm{tr}(\\Delta X^T \\nabla^2 f(X^{(k)})[\\Delta X]) $$\nThe constraints on the Newton step $\\Delta X$ are derived from the linearization of the original equality constraints $\\mathrm{tr}(A_i(X^{(k)}+\\Delta X)) = b_i$. As $X^{(k)}$ is feasible, this simplifies to:\n$$ \\mathrm{tr}(A_i \\Delta X) = 0 \\quad \\text{for } i=1, 2 $$\nGiven $A_1 = \\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix}$ and $A_2 = \\begin{pmatrix}0  0 \\\\ 0  1\\end{pmatrix}$, and $\\Delta X = \\begin{pmatrix} \\Delta x_{11}  \\Delta x_{12} \\\\ \\Delta x_{12}  \\Delta x_{22} \\end{pmatrix}$, the constraints are:\n$\\mathrm{tr}(A_1 \\Delta X) = \\Delta x_{11} = 0$.\n$\\mathrm{tr}(A_2 \\Delta X) = \\Delta x_{22} = 0$.\nThis is a crucial simplification. The Newton direction must have zero diagonal entries. Thus, $\\Delta X$ has the form:\n$$ \\Delta X = \\begin{pmatrix} 0  \\Delta x_{12} \\\\ \\Delta x_{12}  0 \\end{pmatrix} $$\n\nNext, we derive the gradient and Hessian of the objective function $f(X)$. The Frobenius inner product for symmetric matrices is $\\langle U, V \\rangle = \\mathrm{tr}(UV)$.\nThe gradient of $\\mathrm{tr}(CX)$ is $C$.\nThe gradient of the barrier term, $\\phi(X) = -\\ln \\det(X)$, is $\\nabla \\phi(X) = -X^{-1}$.\nThe gradient of the full objective is $\\nabla f(X) = C - \\mu X^{-1}$. At $X^{(k)}$, this is $G = C - \\mu (X^{(k)})^{-1}$.\n\nThe Hessian of $\\mathrm{tr}(CX)$ is zero. The Hessian of $\\phi(X)$ acting on a direction $\\Delta X$ is $\\nabla^2 \\phi(X)[\\Delta X] = X^{-1} \\Delta X X^{-1}$.\nThe Hessian of the full objective is $\\nabla^2 f(X)[\\Delta X] = \\mu X^{-1} \\Delta X X^{-1}$.\n\nThe Newton step $\\Delta X$ is found by solving the KKT system for the quadratically approximated problem. The Lagrangian is:\n$$ L(\\Delta X, y_1, y_2) = \\mathrm{tr}(G^T \\Delta X) + \\frac{1}{2} \\mu \\mathrm{tr}(\\Delta X (X^{(k)})^{-1} \\Delta X (X^{(k)})^{-1}) + y_1 \\mathrm{tr}(A_1 \\Delta X) + y_2 \\mathrm{tr}(A_2 \\Delta X) $$\nThe optimality condition $\\nabla_{\\Delta X} L = 0$ gives the Newton system equation:\n$$ G + \\mu (X^{(k)})^{-1} \\Delta X (X^{(k)})^{-1} + y_1 A_1 + y_2 A_2 = 0 $$\nSubstituting $G = C - \\mu (X^{(k)})^{-1}$:\n$$ C - \\mu (X^{(k)})^{-1} + \\mu (X^{(k)})^{-1} \\Delta X (X^{(k)})^{-1} + y_1 A_1 + y_2 A_2 = 0 $$\nThis is a matrix equation, which we can analyze component-wise. The matrices $A_1$ and $A_2$ are diagonal, so their $(1,2)$ components are zero. This means the Lagrange multiplier terms $y_1 A_1 + y_2 A_2$ have no influence on the $(1,2)$ entry of the equation.\n\nTaking the $(1,2)$ component of the matrix equation gives:\n$$ C_{12} - \\mu (X^{(k)-1})_{12} + \\mu ((X^{(k)})^{-1} \\Delta X (X^{(k)})^{-1})_{12} + 0 = 0 $$\nWe have $C_{12} = 0$ and $\\mu=1$, so this simplifies to:\n$$ ((X^{(k)})^{-1} \\Delta X (X^{(k)})^{-1})_{12} = (X^{(k)-1})_{12} $$\nLet's compute the inverse of $X^{(k)}$. We use fractional representation $0.2 = 1/5$.\n$X^{(k)} = \\begin{pmatrix}2  1/5 \\\\ 1/5  1\\end{pmatrix}$.\n$\\det(X^{(k)}) = 2(1) - (1/5)^2 = 2 - 1/25 = 49/25$.\n$X^{(k)-1} = \\frac{1}{49/25} \\begin{pmatrix} 1  -1/5 \\\\ -1/5  2 \\end{pmatrix} = \\frac{25}{49} \\begin{pmatrix} 1  -1/5 \\\\ -1/5  2 \\end{pmatrix} = \\begin{pmatrix} 25/49  -5/49 \\\\ -5/49  50/49 \\end{pmatrix}$.\nSo, $(X^{(k)-1})_{12} = -5/49$.\n\nLet $V = X^{(k)-1}$. Let $\\delta = \\Delta x_{12}$. We have $\\Delta X = \\begin{pmatrix} 0  \\delta \\\\ \\delta  0 \\end{pmatrix}$.\nWe need to compute the $(1,2)$ component of $V \\Delta X V$.\n$V \\Delta X = \\begin{pmatrix} V_{11}  V_{12} \\\\ V_{12}  V_{22} \\end{pmatrix} \\begin{pmatrix} 0  \\delta \\\\ \\delta  0 \\end{pmatrix} = \\begin{pmatrix} \\delta V_{12}  \\delta V_{11} \\\\ \\delta V_{22}  \\delta V_{12} \\end{pmatrix}$.\nThe $(1,2)$ component of $(V \\Delta X) V$ is $(\\delta V_{12})V_{12} + (\\delta V_{11})V_{22} = \\delta (V_{12}^2 + V_{11}V_{22})$.\n\nSubstituting this into our equation: $\\delta (V_{12}^2 + V_{11}V_{22}) = V_{12}$.\nWe have the values for the components of $V = X^{(k)-1}$:\n$V_{11} = 25/49$, $V_{12} = -5/49$, $V_{22} = 50/49$.\n$\\delta \\left( \\left(-\\frac{5}{49}\\right)^2 + \\left(\\frac{25}{49}\\right)\\left(\\frac{50}{49}\\right) \\right) = -\\frac{5}{49}$.\n$\\delta \\left( \\frac{25}{49^2} + \\frac{1250}{49^2} \\right) = -\\frac{5}{49}$.\n$\\delta \\left( \\frac{1275}{2401} \\right) = -\\frac{5}{49}$.\n$\\delta = -\\frac{5}{49} \\cdot \\frac{2401}{1275}$.\nSince $2401 = 49^2$, this simplifies to:\n$\\delta = -5 \\cdot \\frac{49}{1275}$.\nFactoring the denominator, $1275 = 5 \\times 255 = 5 \\times 5 \\times 51 = 25 \\times 51$.\n$\\delta = -5 \\cdot \\frac{49}{25 \\cdot 51} = -\\frac{49}{5 \\cdot 51} = -\\frac{49}{255}$.\n\nSo, $\\Delta x_{12} = -49/255$.\nThe full Newton direction is specified by the triple $(\\Delta x_{11}, \\Delta x_{12}, \\Delta x_{22})$.\nWe have found $\\Delta x_{11}=0$, $\\Delta x_{12}=-49/255$, and $\\Delta x_{22}=0$.\nThe Newton direction is $(0, -49/255, 0)$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 0  -\\frac{49}{255}  0 \\end{pmatrix} } $$"
        },
        {
            "introduction": "Moving from manual calculations to a full implementation reveals the practical challenges and algorithmic structure of a complete solver. This problem guides you through building a primal-dual IPM for a realistic portfolio optimization task, bridging the gap between theory and application. A key challenge in large-scale optimization is avoiding the direct computation and storage of large matrix inverses. This practice mandates an \"inversion-free\" approach, compelling you to use an iterative solver like the Conjugate Gradient method for the Newton system, a standard and efficient technique in high-performance scientific computing .",
            "id": "2402731",
            "problem": "You are asked to write a complete, runnable program that computes optimal long-only portfolio weights for a set of quadratic programs defined by mean-variance objectives with a single full-investment constraint. The program must solve each quadratic program by only using products of the Hessian matrix with vectors and must not compute or use any explicit matrix inverse of the Hessian.\n\nThe optimization problem for a given test case is: minimize the objective\n$$\n\\frac{1}{2}\\,\\boldsymbol{x}^{\\top} Q\\,\\boldsymbol{x} - \\boldsymbol{\\mu}^{\\top}\\boldsymbol{x}\n$$\nsubject to the constraints\n$$\n\\boldsymbol{1}^{\\top}\\boldsymbol{x} = 1,\\quad \\boldsymbol{x} \\succeq \\boldsymbol{0},\n$$\nwhere $Q \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, $\\boldsymbol{\\mu} \\in \\mathbb{R}^{n}$ is the expected return vector, $\\boldsymbol{1}$ is the all-ones vector in $\\mathbb{R}^{n}$, and $\\boldsymbol{0}$ is the zero vector in $\\mathbb{R}^{n}$. The decision variable is $\\boldsymbol{x} \\in \\mathbb{R}^{n}$. All computations are to be done in purely numerical terms; no physical units apply.\n\nYour program must produce, for each test case, the optimal weight vector $\\boldsymbol{x}^{\\star}$ as a list of real numbers, each rounded to six decimal places. The constraint $\\boldsymbol{x} \\succeq \\boldsymbol{0}$ enforces long-only positions. The full-investment constraint is $\\boldsymbol{1}^{\\top}\\boldsymbol{x} = 1$.\n\nThe test suite consists of the following three cases. In each case, $A = \\boldsymbol{1}^{\\top}$ and $b = 1$ define the single equality constraint. All numeric entries below are exact.\n\n- Test case $1$ ($n=4$):\n  - Hessian\n    $$\n    Q_1 =\n    \\begin{bmatrix}\n    0.10  0.02  0.04  0.00 \\\\\n    0.02  0.08  0.01  0.00 \\\\\n    0.04  0.01  0.09  0.00 \\\\\n    0.00  0.00  0.00  0.05\n    \\end{bmatrix}\n    $$\n  - Expected returns\n    $$\n    \\boldsymbol{\\mu}_1 = \\begin{bmatrix} 0.12 \\\\ 0.10 \\\\ 0.07 \\\\ 0.03 \\end{bmatrix}\n    $$\n\n- Test case $2$ ($n=3$):\n  - Hessian\n    $$\n    Q_2 =\n    \\begin{bmatrix}\n    0.20  0.15  0.15 \\\\\n    0.15  0.30  0.25 \\\\\n    0.15  0.25  0.50\n    \\end{bmatrix}\n    $$\n  - Expected returns\n    $$\n    \\boldsymbol{\\mu}_2 = \\begin{bmatrix} 0.18 \\\\ 0.02 \\\\ 0.01 \\end{bmatrix}\n    $$\n\n- Test case $3$ ($n=3$):\n  - Hessian\n    $$\n    Q_3 = 0.02 \\cdot\n    \\begin{bmatrix}\n    1.0  0.999  0.9995 \\\\\n    0.999  1.0  0.9992 \\\\\n    0.9995  0.9992  1.0\n    \\end{bmatrix}\n    +\n    10^{-6}\\, I_3\n    $$\n    where $I_3$ is the $3 \\times 3$ identity matrix.\n  - Expected returns\n    $$\n    \\boldsymbol{\\mu}_3 = \\begin{bmatrix} 0.050 \\\\ 0.051 \\\\ 0.049 \\end{bmatrix}\n    $$\n\nYour program must:\n- Enforce the constraints exactly within numerical tolerance.\n- Use only matrix-vector products involving $Q$; do not form or use any explicit inverse of $Q$ or of any derived matrix.\n- Return the optimizer $\\boldsymbol{x}^{\\star}$ for each test case, rounded to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of the rounded weights for the corresponding test case, for example,\n$$\n[ [x_{1,1},x_{1,2},\\dots], [x_{2,1},x_{2,2},\\dots], [x_{3,1},x_{3,2},\\dots] ].\n$$\nNo spaces are required, and each entry must be printed with exactly six digits after the decimal point.\n\nDesign for coverage:\n- The first case is a standard instance with moderate correlations.\n- The second case encourages boundary solutions where some optimal weights may be numerically zero.\n- The third case is nearly collinear and tests numerical stability.\n\nThe expected answer for each test case is the list of optimal weights as real numbers. Each list must consist of exactly $n$ floating-point numbers.",
            "solution": "The problem presented is a convex quadratic program (QP) concerning optimal portfolio allocation under a mean-variance framework. The objective is to minimize a function representing a trade-off between portfolio risk (variance) and return, subject to a full-investment constraint and a long-only (non-negativity) constraint on the portfolio weights $\\boldsymbol{x}$.\n\nThe optimization problem is formally stated as:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad  f(\\boldsymbol{x}) = \\frac{1}{2}\\,\\boldsymbol{x}^{\\top} Q\\,\\boldsymbol{x} - \\boldsymbol{\\mu}^{\\top}\\boldsymbol{x} \\\\\n\\text{subject to} \\quad  \\boldsymbol{1}^{\\top}\\boldsymbol{x} = 1 \\\\\n \\boldsymbol{x} \\succeq \\boldsymbol{0}\n\\end{aligned}\n$$\nwhere $Q$ is a symmetric positive definite covariance matrix, $\\boldsymbol{\\mu}$ is the vector of expected returns, and $\\boldsymbol{x}$ is the vector of portfolio weights. The condition that $Q$ is positive definite ensures that the objective function $f(\\boldsymbol{x})$ is strictly convex. The feasible region, defined by the linear equality and non-negativity constraints, is a simplex, which is a closed and convex set. The minimization of a strictly convex function over a non-empty, closed, convex set guarantees the existence of a unique optimal solution $\\boldsymbol{x}^{\\star}$.\n\nThe optimality of a solution is fully characterized by the Karush-Kuhn-Tucker (KKT) conditions. The Lagrangian for this problem is:\n$$\n\\mathcal{L}(\\boldsymbol{x}, \\lambda, \\boldsymbol{s}) = \\frac{1}{2}\\boldsymbol{x}^{\\top} Q \\boldsymbol{x} - \\boldsymbol{\\mu}^{\\top} \\boldsymbol{x} - \\lambda(\\boldsymbol{1}^{\\top}\\boldsymbol{x} - 1) - \\boldsymbol{s}^{\\top}\\boldsymbol{x}\n$$\nwhere $\\lambda \\in \\mathbb{R}$ is the Lagrange multiplier for the equality constraint and $\\boldsymbol{s} \\in \\mathbb{R}^n$ is the vector of Lagrange multipliers for the non-negativity constraints. The KKT conditions for optimality are:\n1.  **Stationarity:** $\\nabla_{\\boldsymbol{x}} \\mathcal{L} = Q\\boldsymbol{x} - \\boldsymbol{\\mu} - \\lambda\\boldsymbol{1} - \\boldsymbol{s} = \\boldsymbol{0}$\n2.  **Primal Feasibility:** $\\boldsymbol{1}^{\\top}\\boldsymbol{x} = 1$, $\\boldsymbol{x} \\succeq \\boldsymbol{0}$\n3.  **Dual Feasibility:** $\\boldsymbol{s} \\succeq \\boldsymbol{0}$\n4.  **Complementary Slackness:** $x_i s_i = 0$ for all $i \\in \\{1, \\dots, n\\}$\n\nTo solve this system of equations and inequalities, we employ a primal-dual interior-point method (IPM). This class of algorithms is highly effective for QPs and is well-suited to the problem's constraints, particularly the restriction against forming or using explicit matrix inverses. IPMs iteratively solve a sequence of perturbed KKT systems, approaching the optimal solution from the interior of the feasible region.\n\nThe complementary slackness conditions $x_i s_i = 0$ are replaced by a perturbed version, $x_i s_i = \\tau$, where $\\tau  0$ is a barrier parameter that is driven to zero as the iterations proceed. The core of the IPM is to apply Newton's method to solve this perturbed KKT system for the search directions $(\\Delta\\boldsymbol{x}, \\Delta\\lambda, \\Delta\\boldsymbol{s})$. This leads to a linear system at each iteration.\n\nLet the current iterate be $(\\boldsymbol{x}, \\lambda, \\boldsymbol{s})$. The primal and dual residuals are:\n$$\n\\begin{aligned}\n\\boldsymbol{r}_p = \\boldsymbol{1}^{\\top}\\boldsymbol{x} - 1 \\\\\n\\boldsymbol{r}_d = Q\\boldsymbol{x} - \\boldsymbol{\\mu} - \\lambda\\boldsymbol{1} - \\boldsymbol{s}\n\\end{aligned}\n$$\nThe Newton system for the search directions $(\\Delta\\boldsymbol{x}, \\Delta\\lambda, \\Delta\\boldsymbol{s})$ is:\n$$\n\\begin{bmatrix}\nQ  \\boldsymbol{1}  I \\\\\n\\boldsymbol{1}^{\\top}  0  \\boldsymbol{0}^{\\top} \\\\\nS  \\boldsymbol{0}  X\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta\\boldsymbol{x} \\\\\n-\\Delta\\lambda \\\\\n-\\Delta\\boldsymbol{s}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{r}_d \\\\\nr_p \\\\\nX\\boldsymbol{s} - \\sigma \\mu_{gap} \\boldsymbol{1}\n\\end{bmatrix}\n$$\nwhere $X = \\text{diag}(\\boldsymbol{x})$, $S = \\text{diag}(\\boldsymbol{s})$, $\\mu_{gap} = (\\boldsymbol{x}^{\\top}\\boldsymbol{s})/n$ is the duality gap, and $\\sigma \\in [0, 1]$ is a centering parameter.\n\nTo adhere to the constraint of not forming or inverting matrices, we do not solve this system directly. Instead, we eliminate $\\Delta\\boldsymbol{s}$ to obtain a smaller, equivalent system known as the augmented system or normal equations:\n$$\n\\begin{bmatrix}\nQ + X^{-1}S  \\boldsymbol{1} \\\\\n\\boldsymbol{1}^{\\top}  0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta\\boldsymbol{x} \\\\\n-\\Delta\\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{b}_1 \\\\\nb_2\n\\end{bmatrix}\n$$\nHere, the matrix $H_{bar} = Q + X^{-1}S$ is symmetric and positive definite, as $Q$ is positive definite and $X^{-1}S$ is a diagonal matrix with positive entries (since $\\boldsymbol{x}  \\boldsymbol{0}$ and $\\boldsymbol{s}  \\boldsymbol{0}$ in the interior).\n\nTo solve this system for $(\\Delta\\boldsymbol{x}, \\Delta\\lambda)$ without inverting $H_{bar}$, we use the Schur complement method. We solve for $\\Delta\\boldsymbol{x}$ in terms of $\\Delta\\lambda$ from the first block equation: $\\Delta\\boldsymbol{x} = H_{bar}^{-1}(\\boldsymbol{b}_1 - \\Delta\\lambda\\boldsymbol{1})$. Substituting this into the second block equation gives a scalar equation for $\\Delta\\lambda$:\n$$\n\\Delta\\lambda = \\frac{\\boldsymbol{1}^{\\top}H_{bar}^{-1}\\boldsymbol{b}_1 - b_2}{\\boldsymbol{1}^{\\top}H_{bar}^{-1}\\boldsymbol{1}}\n$$\nComputing $\\Delta\\lambda$ requires the vectors $H_{bar}^{-1}\\boldsymbol{b}_1$ and $H_{bar}^{-1}\\boldsymbol{1}$. These are obtained by solving the linear systems $H_{bar}\\boldsymbol{y}_1 = \\boldsymbol{b}_1$ and $H_{bar}\\boldsymbol{y}_2 = \\boldsymbol{1}$. Crucially, we use the Conjugate Gradient (CG) method to solve these systems. CG is an iterative algorithm that finds the solution using only matrix-vector products of the form $H_{bar}\\boldsymbol{v} = Q\\boldsymbol{v} + (X^{-1}S)\\boldsymbol{v}$. This operation relies only on matrix-vector products with the original Hessian $Q$, thereby satisfying all problem constraints.\n\nOnce $\\Delta\\lambda$ and $\\Delta\\boldsymbol{x}$ are found, $\\Delta\\boldsymbol{s}$ is recovered. A line search is then performed to determine a step size $\\alpha \\in (0, 1]$ that maintains the positivity of $\\boldsymbol{x}$ and $\\boldsymbol{s}$. The iterates are updated:\n$$\n\\boldsymbol{x} \\leftarrow \\boldsymbol{x} + \\alpha \\Delta\\boldsymbol{x}, \\quad \\lambda \\leftarrow \\lambda + \\alpha \\Delta\\lambda, \\quad \\boldsymbol{s} \\leftarrow \\boldsymbol{s} + \\alpha \\Delta\\boldsymbol{s}\n$$\nThis process is repeated until the primal residuals, dual residuals, and the duality gap are all below a specified tolerance, at which point the algorithm has converged to the optimal solution $\\boldsymbol{x}^{\\star}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef conjugate_gradient(matvec_func, b, x0=None, tol=1e-10, max_iter=1000):\n    \"\"\"\n    Solves the symmetric positive-definite system Ax = b using the Conjugate Gradient method.\n    'matvec_func' is a function that computes the product A@v.\n    \"\"\"\n    n = len(b)\n    if x0 is None:\n        x = np.zeros(n)\n    else:\n        x = x0.copy()\n\n    r = b - matvec_func(x)\n    p = r.copy()\n    rs_old = np.dot(r, r)\n\n    if np.sqrt(rs_old)  tol:\n        return x\n\n    for _ in range(max_iter):\n        Ap = matvec_func(p)\n        alpha = rs_old / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rs_new = np.dot(r, r)\n\n        if np.sqrt(rs_new)  tol:\n            break\n\n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n\n    return x\n\ndef solve_portfolio_optimization(Q, mu, max_iter=50, tol_outer=1e-9, tol_inner=1e-12, eta=0.995, sigma=0.1):\n    \"\"\"\n    Solves the quadratic program using a primal-dual interior-point method.\n    The method iteratively solves the KKT conditions using Newton's method,\n    with an inner loop using Conjugate Gradient to solve linear systems\n    without explicit matrix inversion.\n    \"\"\"\n    n = len(mu)\n    \n    # Initialization\n    x = np.ones(n) / n\n    lam = 0.0\n    s = np.ones(n)\n\n    ones_n = np.ones(n)\n\n    for _ in range(max_iter):\n        # Calculate residuals and duality gap\n        res_p = np.sum(x) - 1.0\n        res_d = Q @ x - mu - lam * ones_n - s\n        gap = np.dot(s, x) / n\n\n        # Convergence check\n        norm_res_d = np.linalg.norm(res_d)\n        if norm_res_d  tol_outer and abs(res_p)  tol_outer and gap  tol_outer:\n            break\n\n        # Define matrix-vector product for the CG solver's system matrix H_bar = Q + X^-1 * S\n        s_over_x = s / x\n        def h_bar_mv(v):\n            return Q @ v + s_over_x * v\n\n        # Set up the right-hand sides for the reduced Newton system\n        rhs_dx_eq = -res_d - s + (sigma * gap) / x\n        rhs_lam_eq = -res_p\n\n        # Solve the linear systems using Conjugate Gradient\n        # System 1: H_bar * y1 = rhs_dx_eq\n        # System 2: H_bar * y2 = 1\n        y1 = conjugate_gradient(h_bar_mv, rhs_dx_eq, tol=tol_inner, max_iter=2*n)\n        y2 = conjugate_gradient(h_bar_mv, ones_n, tol=tol_inner, max_iter=2*n)\n\n        # Compute search directions for lambda, x, and s\n        dlam = (rhs_lam_eq - np.sum(y1)) / np.sum(y2)\n        dx = y1 + dlam * y2\n        ds = (-s * x - s * dx + sigma * gap) / x\n        \n        # Line search for step sizes to maintain positivity\n        alpha_p = 1.0\n        if np.any(dx  0):\n            alpha_p = min(1.0, np.min(-x[dx  0] / dx[dx  0]))\n            \n        alpha_s = 1.0\n        if np.any(ds  0):\n            alpha_s = min(1.0, np.min(-s[ds  0] / ds[ds  0]))\n        \n        alpha = min(alpha_p, alpha_s) * eta\n\n        # Update iterates\n        x += alpha * dx\n        s += alpha * ds\n        lam += alpha * dlam\n\n    # Post-processing for clean output\n    x[x  tol_outer] = 0.0\n    x /= np.sum(x)\n    \n    return x\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the solver.\n    \"\"\"\n    # Test case 1\n    Q1 = np.array([\n        [0.10, 0.02, 0.04, 0.00],\n        [0.02, 0.08, 0.01, 0.00],\n        [0.04, 0.01, 0.09, 0.00],\n        [0.00, 0.00, 0.00, 0.05]\n    ])\n    mu1 = np.array([0.12, 0.10, 0.07, 0.03])\n\n    # Test case 2\n    Q2 = np.array([\n        [0.20, 0.15, 0.15],\n        [0.15, 0.30, 0.25],\n        [0.15, 0.25, 0.50]\n    ])\n    mu2 = np.array([0.18, 0.02, 0.01])\n\n    # Test case 3\n    Q3_base = 0.02 * np.array([\n        [1.0,    0.999,  0.9995],\n        [0.999,  1.0,    0.9992],\n        [0.9995, 0.9992, 1.0]\n    ])\n    Q3 = Q3_base + 1e-6 * np.identity(3)\n    mu3 = np.array([0.050, 0.051, 0.049])\n\n    test_cases = [\n        (Q1, mu1),\n        (Q2, mu2),\n        (Q3, mu3),\n    ]\n\n    results = []\n    for Q, mu in test_cases:\n        x_star = solve_portfolio_optimization(Q, mu)\n        results.append(x_star)\n\n    # Format the final output string exactly as required\n    def format_list(arr):\n        return '[' + ','.join([f'{x:.6f}' for x in arr]) + ']'\n        \n    formatted_strings = [format_list(res) for res in results]\n    print(f\"[{','.join(formatted_strings)}]\")\n\nsolve()\n```"
        }
    ]
}