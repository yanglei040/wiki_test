{
    "hands_on_practices": [
        {
            "introduction": "第一个练习将重点关注DC规划中两个最基本的技能。我们将练习如何将一个非凸函数分解为两个凸函数的差，然后计算其中凸函数部分的次微分，这是DCA算法中的关键步骤。通过这个练习，你将掌握构建和分析DC规划问题的基础。",
            "id": "3119906",
            "problem": "考虑由下式定义的函数 $f:\\mathbb{R}^3 \\to \\mathbb{R}$\n$$\nf(x) \\;=\\; \\max_{i \\in \\{1,2,3\\}} \\, a_i^\\top x \\;-\\; \\mu \\,\\|x\\|_2^2,\n$$\n其中 $a_1=\\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix}$，$a_2=\\begin{pmatrix}0\\\\2\\\\-1\\end{pmatrix}$，$a_3=\\begin{pmatrix}-1\\\\0\\\\2\\end{pmatrix}$，且 $\\mu>0$ 是一个给定的标量（取 $\\mu=2$）。目标是从差分凸 (Difference-of-Convex, DC) 规划的角度分析这个目标函数。\n\n任务：\n1. 仅使用凸分析和优化的基本定义和事实，推导出一个差分凸 (DC) 分解 $f(x)=g(x)-h(x)$，其中 $g$ 和 $h$ 都是 $\\mathbb{R}^3$ 上的凸函数。解释为什么每个分量都是凸的。\n2. 令 $x_0=\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}$。刻画凸函数 $g$ 在 $x_0$ 处的次微分，并找出在 $x_0$ 处的所有激活仿射函数。然后，在 $\\partial g(x_0)$ 中的所有次梯度中，确定具有最小欧几里得范数的唯一子梯度，并将其以行向量的形式报告。\n\n你最终报告的答案应该是 $g$ 在 $x_0$ 处的最小欧几里得范数次梯度，以行向量形式表示，使用精确值。无需四舍五入。",
            "solution": "该问题被评估为有效。它在科学上基于凸优化领域，特别是差分凸 (DC) 规划。该问题提法良好、客观且自成一体，提供了推导指定任务唯一解所需的所有数据和定义。\n\n待分析的函数是 $f:\\mathbb{R}^3 \\to \\mathbb{R}$，定义为\n$$\nf(x) \\;=\\; \\max_{i \\in \\{1,2,3\\}} \\, a_i^\\top x \\;-\\; \\mu \\,\\|x\\|_2^2\n$$\n其中 $a_1=\\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix}$，$a_2=\\begin{pmatrix}0\\\\2\\\\-1\\end{pmatrix}$，$a_3=\\begin{pmatrix}-1\\\\0\\\\2\\end{pmatrix}$，且 $\\mu=2$。\n\n任务1：推导差分凸 (DC) 分解 $f(x)=g(x)-h(x)$。\n\n我们可以通过定义两个分量函数 $g(x)$ 和 $h(x)$ 来分解函数 $f(x)$。一个自然的选择是：\n$$\ng(x) = \\max_{i \\in \\{1,2,3\\}} \\, a_i^\\top x\n$$\n$$\nh(x) = \\mu \\,\\|x\\|_2^2 = 2 \\,\\|x\\|_2^2\n$$\n通过这个选择，很明显 $f(x) = g(x) - h(x)$。我们现在必须证明 $g(x)$ 和 $h(x)$ 都是 $\\mathbb{R}^3$ 上的凸函数。\n\n对于 $g(x)$：\n对于每个索引 $i \\in \\{1, 2, 3\\}$，函数 $g_i(x) = a_i^\\top x$ 是 $\\mathbb{R}^3$ 上的一个线性泛函。任何线性函数也是仿射函数，而所有仿射函数都是凸函数（也是凹函数）。函数 $g(x)$ 被定义为这三个凸函数的逐点最大值，即 $g(x) = \\max\\{g_1(x), g_2(x), g_3(x)\\}$。凸分析中的一个基本性质指出，一组凸函数的逐点最大值本身也是一个凸函数。因此，$g(x)$ 是 $\\mathbb{R}^3$ 上的一个凸函数。\n\n对于 $h(x)$：\n函数 $h(x)$ 由 $h(x) = 2\\|x\\|_2^2$ 给出。在坐标系中，对于 $x = \\begin{pmatrix}x_1  x_2  x_3\\end{pmatrix}^\\top \\in \\mathbb{R}^3$，这表示为 $h(x) = 2(x_1^2 + x_2^2 + x_3^2)$。这是一个关于 $x$ 的二次函数。一个二阶可微函数是凸函数的充要条件是其海森矩阵在其整个定义域上是半正定的。$h(x)$ 的梯度是：\n$$\n\\nabla h(x) = \\nabla \\left( 2(x_1^2 + x_2^2 + x_3^2) \\right) = 2 \\begin{pmatrix} 2x_1 \\\\ 2x_2 \\\\ 2x_3 \\end{pmatrix} = 4x\n$$\n$h(x)$ 的海森矩阵是其梯度的雅可比矩阵：\n$$\n\\nabla^2 h(x) = \\nabla(4x) = 4I_3 = \\begin{pmatrix} 4  0  0 \\\\ 0  4  0 \\\\ 0  0  4 \\end{pmatrix}\n$$\n其中 $I_3$ 是 $3 \\times 3$ 的单位矩阵。该矩阵的特征值都等于 $4$，是严格正的。因此，海森矩阵 $\\nabla^2 h(x)$ 对所有 $x \\in \\mathbb{R}^3$ 都是正定的。这证明了 $h(x)$ 是 $\\mathbb{R}^3$ 上的一个严格凸函数。\n\n由于 $g(x)$ 和 $h(x)$ 都是凸函数，因此分解 $f(x) = g(x) - h(x)$ 是一个有效的 DC 分解。\n\n任务2：刻画在 $x_0 = \\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}$ 处的 $\\partial g(x_0)$ 并找到最小范数次梯度。\n\n首先，我们必须确定在 $x_0$ 处的激活仿射函数。我们对每个 $i \\in \\{1, 2, 3\\}$ 计算 $a_i^\\top x_0$：\n$$\na_1^\\top x_0 = \\begin{pmatrix}1  -1  0\\end{pmatrix} \\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix} = 1(1) - 1(1) + 0(1) = 0\n$$\n$$\na_2^\\top x_0 = \\begin{pmatrix}0  2  -1\\end{pmatrix} \\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix} = 0(1) + 2(1) - 1(1) = 1\n$$\n$$\na_3^\\top x_0 = \\begin{pmatrix}-1  0  2\\end{pmatrix} \\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix} = -1(1) + 0(1) + 2(1) = 1\n$$\n$g(x_0)$ 的值是这些值的最大值：$g(x_0) = \\max\\{0, 1, 1\\} = 1$。\n在 $x_0$ 处的激活索引集，记为 $I(x_0)$，包含使 $a_i^\\top x_0 = g(x_0)$ 成立的索引。在这种情况下，$I(x_0) = \\{2, 3\\}$。激活的仿射函数是 $a_2^\\top x$ 和 $a_3^\\top x$。\n\n$g(x)$ 在点 $x_0$ 处的次微分由该点处激活函数的梯度（或次梯度）的凸包给出。由于函数 $g_i(x)=a_i^\\top x$ 是可微的，它们的次微分是包含其梯度的单点集，即 $\\partial g_i(x_0) = \\{\\nabla g_i(x_0)\\} = \\{a_i\\}$。\n$g$ 在 $x_0$ 处的次微分因此是：\n$$\n\\partial g(x_0) = \\text{conv} \\left( \\bigcup_{i \\in I(x_0)} \\{a_i\\} \\right) = \\text{conv}\\{a_2, a_3\\}\n$$\n这是在 $\\mathbb{R}^3$ 中连接向量 $a_2$ 和 $a_3$ 的线段。任何次梯度 $v \\in \\partial g(x_0)$ 都可以表示为 $a_2$ 和 $a_3$ 的凸组合：\n$$\nv = \\lambda a_2 + (1-\\lambda) a_3, \\quad \\text{for } \\lambda \\in [0, 1]\n$$\n代入向量 $a_2$ 和 $a_3$：\n$$\nv(\\lambda) = \\lambda \\begin{pmatrix}0\\\\2\\\\-1\\end{pmatrix} + (1-\\lambda) \\begin{pmatrix}-1\\\\0\\\\2\\end{pmatrix} = \\begin{pmatrix} -(1-\\lambda) \\\\ 2\\lambda \\\\ -\\lambda + 2(1-\\lambda) \\end{pmatrix} = \\begin{pmatrix} \\lambda-1 \\\\ 2\\lambda \\\\ 2-3\\lambda \\end{pmatrix}\n$$\n我们需要找到具有最小欧几里得范数的次梯度。这等价于在线段 $[a_2, a_3]$ 上找到离原点最近的向量 $v$。我们可以通过最小化关于 $\\lambda \\in [0, 1]$ 的欧几里得范数的平方 $\\|v(\\lambda)\\|_2^2$ 来实现这一点。\n$$\n\\|v(\\lambda)\\|_2^2 = (\\lambda-1)^2 + (2\\lambda)^2 + (2-3\\lambda)^2\n$$\n展开各项：\n$$\n\\|v(\\lambda)\\|_2^2 = (\\lambda^2 - 2\\lambda + 1) + 4\\lambda^2 + (4 - 12\\lambda + 9\\lambda^2) = 14\\lambda^2 - 14\\lambda + 5\n$$\n这是一个关于 $\\lambda$ 的二次函数，代表一个开口向上的抛物线。其最小值位于顶点。我们通过将导数设为零来找到最小化 $\\lambda$ 的值：\n$$\n\\frac{d}{d\\lambda} (14\\lambda^2 - 14\\lambda + 5) = 28\\lambda - 14 = 0\n$$\n解出 $\\lambda$ 得到 $\\lambda = \\frac{14}{28} = \\frac{1}{2}$。\n因为这个值 $\\lambda = \\frac{1}{2}$ 位于区间 $[0, 1]$ 内，所以它就是真正的最小值点。\n\n我们将 $\\lambda = \\frac{1}{2}$ 代入 $v(\\lambda)$ 的表达式中，找到具有最小欧几里得范数的唯一子梯度：\n$$\nv_{\\min} = v\\left(\\frac{1}{2}\\right) = \\begin{pmatrix} \\frac{1}{2}-1 \\\\ 2\\left(\\frac{1}{2}\\right) \\\\ 2-3\\left(\\frac{1}{2}\\right) \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ 1 \\\\ \\frac{4}{2}-\\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ 1 \\\\ \\frac{1}{2} \\end{pmatrix}\n$$\n题目要求将此向量以行向量的形式报告。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{1}{2}  1  \\frac{1}{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在掌握了DC分解的基础上，这个练习将带你深入了解差分凸算法(DCA)的内部机制。你将推导单次DCA迭代的更新规则，该规则通过线性化凹函数部分，构建出一个凸子问题，并最终优雅地转化为一个线性规划问题来求解。这个练习将理论与具体实现联系起来，是理解算法核心的关键一步。",
            "id": "3119896",
            "problem": "考虑函数 $f(x)=\\max_{i\\in\\{1,\\dots,m\\}} a_i^\\top x - \\alpha\\|x\\|_2^2$，其中 $x\\in\\mathbb{R}^n$，$\\alpha>0$，且对于 $i\\in\\{1,\\dots,m\\}$ 有 $a_i\\in\\mathbb{R}^n$。从凸性 (convexity) 和差分凸 (difference-of-convex, DC) 分解的基本定义出发，完成以下任务。\n\n1. 仅使用“仿射函数的逐点最大值是凸函数”以及“范数平方的正标量倍是凸函数”这两个核心定义，为 $f(x)$ 推导出一个显式的 DC 分解 $f(x)=g(x)-h(x)$，其中 $g$ 和 $h$ 均为凸函数。\n\n2. 根据差分凸算法 (Difference-of-Convex Algorithm, DCA) 的定义，从第一性原理出发，推导单次迭代的更新规则，即给定当前迭代点 $x^{(0)}\\in\\mathbb{R}^n$，通过在 $x^{(0)}$ 处对凸部分 $h(x)$ 进行线性化，并在一个紧凸集上最小化所得到的凸模型来计算 $x^{(1)}$。为确保子问题是适定的 (well-posed)，将最小化限制在轴对齐的盒子 (axis-aligned box) $C=\\{x\\in\\mathbb{R}^n:\\ -R\\le x_j\\le R\\ \\text{for all } j\\in\\{1,\\dots,n\\}\\}$ 内，其中给定 $R>0$。证明 DCA 子问题\n$$x^{(1)}\\in\\arg\\min_{y\\in C}\\ \\ g(y)-\\langle\\nabla h(x^{(0)}),y\\rangle$$\n等价于关于变量 $(y,t)\\in\\mathbb{R}^n\\times\\mathbb{R}$ 的线性规划：\n$$\\min_{y\\in\\mathbb{R}^n,\\ t\\in\\mathbb{R}}\\ \\ t\\quad\\text{subject to}\\quad (a_i-2\\alpha x^{(0)})^\\top y \\le t\\ \\ \\text{for all } i\\in\\{1,\\dots,m\\},\\ \\ -R\\le y_j\\le R\\ \\ \\text{for all } j\\in\\{1,\\dots,n\\}.$$\n\n3. 实现一个完整、可运行的程序，该程序根据第 2 项中的推导精确执行一次 DCA 迭代，求解相应的线性规划，并为下面指定的每个测试用例返回 $x^{(1)}$。该程序必须确定性地求解线性规划，且不需任何用户输入。所有答案必须表示为纯粹的无量纲数。您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。每个案例的 $x^{(1)}$ 向量应呈现为由方括号括起来的逗号分隔列表，并且每个标量都四舍五入到六位小数（例如，$[[0.123456,0.000000],[1.000000]]$）。\n\n测试套件：\n- 案例 1：$n=2$, $m=3$, $a_1=[1,0]$, $a_2=[0,1]$, $a_3=[-1,-1]$, $\\alpha=0.5$, $R=1.0$, $x^{(0)}=[0.5,-0.3]$。\n- 案例 2：$n=3$, $m=4$, $a_1=[1,2,0]$, $a_2=[-2,1,1]$, $a_3=[0,-1,1]$, $a_4=[1,1,1]$, $\\alpha=0.3$, $R=2.0$, $x^{(0)}=[0,0,0]$。\n- 案例 3：$n=1$, $m=2$, $a_1=[2]$, $a_2=[-3]$, $\\alpha=1.0$, $R=0.5$, $x^{(0)}=[0.4]$。\n- 案例 4：$n=2$, $m=2$, $a_1=[0.1,0.1]$, $a_2=[-0.1,0.2]$, $\\alpha=0.01$, $R=0.1$, $x^{(0)}=[0.0,0.0]$。\n\n您的程序必须输出单行，包含 $[x^{(1)}_{\\text{案例 1}},x^{(1)}_{\\text{案例 2}},x^{(1)}_{\\text{案例 3}},x^{(1)}_{\\text{案例 4}}]$，其中每个 $x^{(1)}_{\\text{案例 k}}$ 是一个浮点数列表，四舍五入到六位小数，逗号后没有空格。",
            "solution": "该问题要求一个三部分的回应：为给定函数推导一个差分凸 (DC) 分解，推导相应的差分凸算法 (DCA) 子问题及其与线性规划 (LP) 的等价性，并实现一个程序来为一组测试用例求解此 LP。\n\n### 第 1 部分：DC 分解的推导\n\n待分析的函数是 $f(x) = \\max_{i \\in \\{1, \\dots, m\\}} a_i^\\top x - \\alpha\\|x\\|_2^2$，其中 $x \\in \\mathbb{R}^n$，$a_i \\in \\mathbb{R}^n$，且 $\\alpha > 0$。一个 DC 分解将 $f(x)$ 表示为两个凸函数之差，$f(x) = g(x) - h(x)$。\n\n我们遵循凸分析的两个基本原则：\n1.  一组凸函数的逐点最大值本身也是一个凸函数。\n2.  一个非负标量与一个凸函数的乘积是一个凸函数。\n\n我们将函数 $f(x)$ 分解为两部分。\n第一部分是 $\\max_{i \\in \\{1, \\dots, m\\}} a_i^\\top x$。对于每个 $i \\in \\{1, \\dots, m\\}$，函数 $l_i(x) = a_i^\\top x$ 是关于 $x$ 的仿射函数（因此也是凸函数）。根据第一个原则，这些仿射函数的逐点最大值是凸的。我们将这部分定义为 $g(x)$：\n$$g(x) = \\max_{i \\in \\{1, \\dots, m\\}} a_i^\\top x$$\n根据定义，$g(x)$ 是一个凸函数。\n\n$f(x)$ 的第二部分是 $-\\alpha\\|x\\|_2^2$。这是函数 $\\alpha\\|x\\|_2^2$ 的负值。平方欧几里得范数 $\\|x\\|_2^2$ 是一个著名的凸函数。根据第二个原则，由于 $\\alpha > 0$，函数 $h(x) = \\alpha\\|x\\|_2^2$ 也是一个凸函数。\n\n通过这些定义，原始函数 $f(x)$ 可以写成：\n$$f(x) = g(x) - h(x) = \\left( \\max_{i \\in \\{1, \\dots, m\\}} a_i^\\top x \\right) - \\left( \\alpha\\|x\\|_2^2 \\right)$$\n这为 $f(x)$ 提供了一个显式的 DC 分解，其中 $g(x) = \\max_{i \\in \\{1, \\dots, m\\}} a_i^\\top x$ 和 $h(x) = \\alpha\\|x\\|_2^2$ 均为凸函数。\n\n### 第 2 部分：DCA 子问题和 LP 等价性的推导\n\nDCA 是一种用于最小化 DC 函数 $f(x) = g(x) - h(x)$ 的迭代方法。在每次迭代 $k$ 中，给定当前点 $x^{(k)}$，该算法通过将函数 $h(x)$ 替换为它在 $x^{(k)}$ 附近的一阶泰勒近似来构建 $f(x)$ 的一个凸模型。下一个迭代点 $x^{(k+1)}$ 通过最小化这个凸模型得到。\n\nDCA 的通用单步更新规则由下式给出：\n$$x^{(k+1)} \\in \\arg\\min_{y} \\left( g(y) - (h(x^{(k)}) + \\langle \\nabla h(x^{(k)}), y - x^{(k)} \\rangle) \\right)$$\n由于 $h(x^{(k)})$ 和 $\\langle \\nabla h(x^{(k)}), x^{(k)} \\rangle$ 相对于优化变量 $y$ 是常数项，最小化上述表达式等价于最小化：\n$$x^{(k+1)} \\in \\arg\\min_{y} \\left( g(y) - \\langle \\nabla h(x^{(k)}), y \\rangle \\right)$$\n问题规定，该最小化应在紧凸集 $C = \\{x \\in \\mathbb{R}^n : -R \\le x_j \\le R \\text{ for all } j \\in \\{1, \\dots, n\\}\\}$ 上进行。从初始点 $x^{(0)}$ 开始，第一次迭代是通过求解以下问题来找到 $x^{(1)}$：\n$$x^{(1)} \\in \\arg\\min_{y \\in C} \\left( g(y) - \\langle \\nabla h(x^{(0)}), y \\rangle \\right)$$\n从第 1 部分可知，$g(y) = \\max_{i \\in \\{1, \\dots, m\\}} a_i^\\top y$ 且 $h(x) = \\alpha\\|x\\|_2^2 = \\alpha \\sum_{j=1}^n x_j^2$。$h(x)$ 的梯度是 $\\nabla h(x) = 2\\alpha x$。在点 $x^{(0)}$ 处，梯度为 $\\nabla h(x^{(0)}) = 2\\alpha x^{(0)}$。\n\n将这些代入子问题的目标函数中得到：\n$$ g(y) - \\langle \\nabla h(x^{(0)}), y \\rangle = \\max_{i \\in \\{1, \\dots, m\\}} a_i^\\top y - \\langle 2\\alpha x^{(0)}, y \\rangle $$\n利用内积的线性性质，我们可以将最大值内的项合并：\n$$ \\max_{i \\in \\{1, \\dots, m\\}} a_i^\\top y - (2\\alpha x^{(0)})^\\top y = \\max_{i \\in \\{1, \\dots, m\\}} (a_i - 2\\alpha x^{(0)})^\\top y $$\n因此，DCA 子问题是：\n$$ x^{(1)} \\in \\arg\\min_{y \\in C} \\left( \\max_{i \\in \\{1, \\dots, m\\}} (a_i - 2\\alpha x^{(0)})^\\top y \\right) $$\n这是一个极小化极大问题 (minimax problem)，可以重构为一个线性规划 (LP)。我们引入一个辅助标量变量 $t \\in \\mathbb{R}$。如果我们最小化 $t$ 并满足条件 $t \\ge \\max_i (\\dots)$，那么目标函数 $\\max_i (\\dots)$ 就被最小化了。这等价于以下线性不等式组：\n$$ t \\ge (a_i - 2\\alpha x^{(0)})^\\top y \\quad \\text{for all } i \\in \\{1, \\dots, m\\} $$\n将域约束 $y \\in C$ 包含进来，完整的优化问题就变成了关于变量 $(y, t) \\in \\mathbb{R}^n \\times \\mathbb{R}$ 的一个 LP：\n$$ \\min_{y \\in \\mathbb{R}^n, t \\in \\mathbb{R}} \\quad t $$\n$$ \\text{subject to:} $$\n$$ (a_i - 2\\alpha x^{(0)})^\\top y \\le t \\quad \\text{for all } i \\in \\{1, \\dots, m\\} $$\n$$ -R \\le y_j \\le R \\quad \\text{for all } j \\in \\{1, \\dots, n\\} $$\n这个 LP 公式与问题陈述中提供的公式相同。从此 LP 中得到的最优向量 $y$ 就是解 $x^{(1)}$。\n\n### 第 3 部分：实现\n\n提供的 Python 代码实现了针对每个测试用例求解所推导出的 LP 的方案。它使用了 `scipy.optimize.linprog` 函数，一个用于线性规划的标准数值求解器。对于每个测试用例，代码首先构建 LP 的参数：成本向量 `c`、不等式约束矩阵 `A_ub`、不等式约束边界 `b_ub` 和变量边界。然后，它调用 `linprog` 并提取最优向量 $y$，它对应于 $x^{(1)}$。最终结果按要求格式化并打印出来。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Solves for the first DCA iterate x^(1) for a series of test cases.\n    Each subproblem is a linear program derived from the DCA framework.\n    \"\"\"\n    \n    # Test Suite:\n    # Each tuple contains (n, m, a_vectors, alpha, R, x0)\n    test_cases = [\n        (2, 3, np.array([[1, 0], [0, 1], [-1, -1]]), 0.5, 1.0, np.array([0.5, -0.3])),\n        (3, 4, np.array([[1, 2, 0], [-2, 1, 1], [0, -1, 1], [1, 1, 1]]), 0.3, 2.0, np.array([0, 0, 0])),\n        (1, 2, np.array([[2], [-3]]), 1.0, 0.5, np.array([0.4])),\n        (2, 2, np.array([[0.1, 0.1], [-0.1, 0.2]]), 0.01, 0.1, np.array([0.0, 0.0]))\n    ]\n\n    results_vectors = []\n\n    for case in test_cases:\n        n, m, a_vectors, alpha, R, x0 = case\n\n        # The optimization variable for linprog is z = [y_1, ..., y_n, t].\n        # The size of z is n + 1.\n\n        # 1. Define the objective function vector 'c'.\n        # We want to minimize t, so c is [0, ..., 0, 1].\n        c = np.zeros(n + 1)\n        c[n] = 1.0\n\n        # 2. Define the inequality constraints A_ub * z = b_ub.\n        # These correspond to (a_i - 2*alpha*x0)^T * y - t = 0.\n        \n        # Calculate the vectors b_i = a_i - 2*alpha*x0\n        grad_h_x0 = 2 * alpha * x0\n        B = a_vectors - grad_h_x0\n        \n        # A_ub is an m x (n+1) matrix.\n        # The i-th row is [b_i^T, -1].\n        A_ub = np.hstack([B, -np.ones((m, 1))])\n        \n        # b_ub is a vector of zeros of size m.\n        b_ub = np.zeros(m)\n\n        # 3. Define the bounds for each variable in z.\n        # -R = y_j = R for j=1..n\n        # t is unbounded.\n        bounds = [(-R, R)] * n + [(None, None)]\n\n        # 4. Solve the linear program.\n        # The 'highs' method is deterministic and robust.\n        res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n        \n        # 5. Extract the solution for y (which is x^(1)).\n        # The solution vector res.x contains [y_1, ..., y_n, t].\n        # We need the first n components.\n        x1 = res.x[:n]\n        \n        results_vectors.append(x1)\n\n    # Format the output as specified: [[r1_1,r1_2,...],[r2_1,r2_2,...],...]\n    # with each number rounded to six decimal places.\n    formatted_results = []\n    for vec in results_vectors:\n        formatted_vec = \",\".join([f\"{v:.6f}\" for v in vec])\n        formatted_results.append(f\"[{formatted_vec}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后的这个练习将展示DC规划在一个真实的机器学习问题——从数据中学习稀疏图——中的威力。你将使用带帽$\\ell_1$惩罚项来构建问题，这是另一个重要的非凸函数，然后将其表达为一个DC规划问题，并实现完整的DCA算法来求解。这个综合性问题将整合你学到的所有概念和技能，从头到尾解决一个有意义的优化任务。",
            "id": "3119825",
            "problem": "您的任务是使用差分凸 (DC) 框架和差分凸算法 (DCA) 来构建并求解一个带帽 $\\ell_1$ 惩罚的图学习问题。目标是在学习到的边权重中引入稀疏性，同时保持图的连通性。您必须生成一个完整、可运行的程序，该程序对指定的测试用例执行 DCA，并以如下所述的精确格式输出汇总指标。\n\n使用的基本原理：\n- 如果一个函数 $F$ 可以写成 $F(\\mathbf{x}) = G(\\mathbf{x}) - H(\\mathbf{x})$ 的形式，其中 $G$ 和 $H$ 是凸函数，则该函数被称为差分凸 (DC) 函数。\n- 差分凸算法 (DCA) 通过求解在 $\\mathbf{x}^k$ 处对凹部进行线性化得到的凸子问题来迭代构造序列 $\\{\\mathbf{x}^k\\}$，即 $\\mathbf{x}^{k+1} \\in \\arg\\min_{\\mathbf{x} \\in \\mathcal{X}} \\left\\{ G(\\mathbf{x}) - \\langle \\mathbf{s}^k, \\mathbf{x} \\rangle \\right\\}$，其中 $\\mathbf{s}^k \\in \\partial H(\\mathbf{x}^k)$ 且 $\\partial H$ 表示 $H$ 的次微分。\n- 带参数 $\\lambda > 0$ 和 $\\theta > 0$ 的标量 $x$ 上的带帽 $\\ell_1$ 惩罚定义为 $p_{\\lambda,\\theta}(x) = \\min\\{\\lambda |x|, \\theta\\}$，并且是非凸的。\n- 一个包含 $n$ 个节点的连通无向图，可以通过对一个固定生成树（例如，最小生成树）中所有边的权重强制施加严格为正的下界来保证其保持连通性，因为一个具有严格正权重的生成树形成一个连通子图。\n\n问题设置：\n- 考虑一个无向图，它有 $n$ 个节点，索引为 $i \\in \\{1,\\dots,n\\}$，嵌入在 $\\mathbb{R}^2$ 中的给定坐标处。设无向边的集合为所有满足 $1 \\le i  j \\le n$ 的节点对 $(i,j)$，边权重向量为 $\\mathbf{w} \\in \\mathbb{R}^m$，其中 $m = n(n-1)/2$，且所有边的权重 $w_{ij} \\ge 0$。\n- 设节点 $i$ 和 $j$ 之间的基准亲和度为 $w^0_{ij} = \\exp\\left(-\\frac{\\| \\mathbf{x}_i - \\mathbf{x}_j \\|_2^2}{\\sigma^2}\\right)$，其中给定了 $\\sigma > 0$，$\\mathbf{x}_i \\in \\mathbb{R}^2$ 是节点 $i$ 的坐标。\n- 学习目标是估计一个与 $\\mathbf{w}^0$ 相近的 $\\mathbf{w}$，同时通过带帽 $\\ell_1$ 正则化来促进稀疏性。优化问题为：\n$$\n\\min_{\\mathbf{w} \\in \\mathbb{R}^m} \\; F(\\mathbf{w}) = \\frac{1}{2} \\| \\mathbf{w} - \\mathbf{w}^0 \\|_2^2 + \\sum_{e=1}^m p_{\\lambda,\\theta}(w_e)\n\\quad \\text{约束条件为} \\quad \\mathbf{w} \\ge \\mathbf{l},\n$$\n其中不等式是逐元素的，$\\mathbf{l} \\in \\mathbb{R}^m_{\\ge 0}$ 是一个下界向量。为了保持连通性，$\\mathbf{l}$ 必须对一个固定生成树 $T$ 中的所有边 $e$ 强制施加 $w_e \\ge \\tau$（其中指定了 $\\tau > 0$），并对所有其他边施加 $w_e \\ge 0$。\n- 您必须将带帽 $\\ell_1$ 惩罚表示为 DC 函数（即，找出凸函数 $G$ 和 $H$ 使得 $p_{\\lambda,\\theta}(x) = G(x) - H(x)$），并应用 DCA。在每次 DCA 迭代中，您将：\n    1. 计算一个次梯度 $\\mathbf{s}^k \\in \\partial H(\\mathbf{w}^k)$。\n    2. 求解凸子问题以生成 $\\mathbf{w}^{k+1}$：\n    $$\n    \\mathbf{w}^{k+1} \\in \\arg\\min_{\\mathbf{w} \\ge \\mathbf{l}} \\left\\{ \\frac{1}{2} \\| \\mathbf{w} - \\mathbf{w}^0 \\|_2^2 + \\sum_{e=1}^m G(w_e) - \\langle \\mathbf{s}^k, \\mathbf{w} \\rangle \\right\\}.\n    $$\n- 在所述的目标和箱式约束下，该凸子问题在坐标上是可分的。您必须为这个凸子问题实现一个精确的逐坐标求解器，该求解器需满足非负性和生成树下界约束。\n\n实现要求：\n- 实现 DCA，其停止准则基于 $\\mathbf{w}$ 的相对变化，即当 $\\frac{\\| \\mathbf{w}^{k+1} - \\mathbf{w}^{k} \\|_2}{\\max\\{1, \\| \\mathbf{w}^{k} \\|_2\\}} \\le \\varepsilon$ 时或达到迭代上限时停止，其中指定了 $\\varepsilon > 0$。\n- 将生成树 $T$ 构建为完全图的最小生成树，使用节点间的成对欧几里得距离作为边成本。对所有 $e \\in T$ 强制施加 $w_e \\ge \\tau$，否则 $w_e \\ge 0$。\n- 收敛后，构建学习到的图的无向邻接矩阵，并使用遍历算法验证其连通性，其中如果边的权重严格大于 $0$，则认为该边存在。\n\n测试套件：\n对于所有测试用例，参数包括节点坐标、$\\sigma$、$\\lambda$、$\\theta$、$\\tau$、最大迭代次数 $K_{\\max}$ 和容差 $\\varepsilon$。您的程序必须执行以下三个测试用例，并按规定输出结果。\n\n- 测试用例 1（正常路径）：\n    - $n = 5$ 个节点，坐标为：\n      $\\mathbf{x}_1 = (0,0)$、$\\mathbf{x}_2 = (1,0)$、$\\mathbf{x}_3 = (1,1)$、$\\mathbf{x}_4 = (0,1)$、$\\mathbf{x}_5 = (0.5,0.2)$。\n    - 参数：$\\sigma = 0.8$, $\\lambda = 0.3$, $\\theta = 0.15$, $\\tau = 0.05$, $K_{\\max} = 50$, $\\varepsilon = 10^{-8}$。\n- 测试用例 2（强稀疏性压力和共线几何结构）：\n    - $n = 4$ 个节点，坐标为：\n      $\\mathbf{x}_1 = (0,0)$、$\\mathbf{x}_2 = (0.5,0)$、$\\mathbf{x}_3 = (1.0,0)$、$\\mathbf{x}_4 = (1.5,0)$。\n    - 参数：$\\sigma = 0.5$, $\\lambda = 0.5$, $\\theta = 0.05$, $\\tau = 0.05$, $K_{\\max} = 50$, $\\varepsilon = 10^{-8}$。\n- 测试用例 3（因上限值较大导致的弱稀疏性）：\n    - $n = 6$ 个节点，坐标为：\n      $\\mathbf{x}_1 = (0,0)$、$\\mathbf{x}_2 = (0.3,0.1)$、$\\mathbf{x}_3 = (0.6,0)$、$\\mathbf{x}_4 = (0.0,0.6)$、$\\mathbf{x}_5 = (0.6,0.6)$、$\\mathbf{x}_6 = (0.3,0.3)$。\n    - 参数：$\\sigma = 1.0$, $\\lambda = 0.2$, $\\theta = 0.5$, $\\tau = 0.03$, $K_{\\max} = 50$, $\\varepsilon = 10^{-8}$。\n\n输出规格：\n- 对于每个测试用例，计算并返回一个包含三个条目的列表：$[c, s, b]$，其中 $c$ 是学习到的权重严格大于 $0$ 的边的整数数量，$s$ 是所有学习到的边权重的浮点数总和，$b$ 是一个布尔值，指示学习到的图是否连通（如果连通则为 $\\text{True}$；否则为 $\\text{False}$）。\n- 您的程序应生成单行输出，其中包含测试用例 1、2 和 3 的三个结果，格式为用方括号括起来的逗号分隔列表，即 $[[c_1,s_1,b_1],[c_2,s_2,b_2],[c_3,s_3,b_3]]$。",
            "solution": "用户提供的问题是图学习领域中一个适定的优化任务，适合使用差分凸算法 (DCA) 解决。该问题具有科学依据，内容自洽，并且所有参数都已明确指定。所需的方法论在非凸优化领域是标准方法。我现在将提供一个完整的解决方案。\n\n问题的核心是解决以下优化问题：\n$$\n\\min_{\\mathbf{w} \\in \\mathbb{R}^m} \\; F(\\mathbf{w}) = \\frac{1}{2} \\| \\mathbf{w} - \\mathbf{w}^0 \\|_2^2 + \\sum_{e=1}^m p_{\\lambda,\\theta}(w_e)\n\\quad \\text{约束条件为} \\quad \\mathbf{w} \\ge \\mathbf{l}\n$$\n其中 $p_{\\lambda,\\theta}(x) = \\min\\{\\lambda |x|, \\theta\\}$ 是带帽 $\\ell_1$ 惩罚，约束 $\\mathbf{w} \\ge \\mathbf{l}$ 确保了非负性并强制保持连通性，其中对于预先计算的生成树中的边 $e$，有 $l_e \\ge \\tau > 0$，对于其他边有 $l_e \\ge 0$。由于约束强制 $w_e \\ge 0$，惩罚项简化为 $p_{\\lambda,\\theta}(w_e) = \\min\\{\\lambda w_e, \\theta\\}$。\n\n### 1. 差分凸 (DC) 公式化\n\nDCA 要求目标函数表示为两个凸函数的差。目标函数 $F(\\mathbf{w})$ 是一个凸二次项和一组非凸带帽 $\\ell_1$ 惩罚项的总和。我们可以将整个目标函数公式化为 DC 函数。\n\n对于非负变量 $x \\ge 0$，带帽 $\\ell_1$ 惩罚可以写成：\n$$\np_{\\lambda,\\theta}(x) = \\min\\{\\lambda x, \\theta\\}\n$$\n这是一个标准的非凸函数，可以分解为两个凸函数的差。一个常见的分解是：\n$$\np_{\\lambda,\\theta}(x) = \\lambda x - \\max\\{0, \\lambda x - \\theta\\}\n$$\n我们定义 $g(x) = \\lambda x$ 和 $h(x) = \\max\\{0, \\lambda x - \\theta\\}$。\n- $g(x)$ 是一个线性函数，因此是凸的。\n- $h(x)$ 是两个凸函数（$0$ 和 $\\lambda x - \\theta$）的最大值，因此也是凸的。\n因此，$p_{\\lambda,\\theta}(x) = g(x) - h(x)$ 是一个有效的 DC 分解。\n\n现在，我们可以将整个目标函数 $F(\\mathbf{w})$ 重写为 $G(\\mathbf{w}) - H(\\mathbf{w})$ 的形式：\n$$\nF(\\mathbf{w}) = \\left( \\frac{1}{2} \\| \\mathbf{w} - \\mathbf{w}^0 \\|_2^2 + \\sum_{e=1}^m g(w_e) \\right) - \\sum_{e=1}^m h(w_e)\n$$\n我们定义两个凸分量：\n1.  $G(\\mathbf{w}) = \\frac{1}{2} \\| \\mathbf{w} - \\mathbf{w}^0 \\|_2^2 + \\sum_{e=1}^m \\lambda w_e = \\frac{1}{2} \\| \\mathbf{w} - \\mathbf{w}^0 \\|_2^2 + \\lambda \\mathbf{1}^T \\mathbf{w}$。这个函数是严格凸的，因为它是严格凸的二次函数和线性函数的和。\n2.  $H(\\mathbf{w}) = \\sum_{e=1}^m \\max\\{0, \\lambda w_e - \\theta\\}$。这个函数是凸的，因为它是凸函数的和。\n\n### 2. 差分凸算法 (DCA)\n\nDCA 是一种迭代算法，在每一步 $k$ 中，它围绕当前迭代点 $\\mathbf{w}^k$ 对凹部 $-H(\\mathbf{w})$ 进行线性化，并求解得到的凸子问题。更新规则为：\n$$\n\\mathbf{w}^{k+1} \\in \\arg\\min_{\\mathbf{w} \\ge \\mathbf{l}} \\left\\{ G(\\mathbf{w}) - \\langle \\mathbf{s}^k, \\mathbf{w} \\rangle \\right\\}\n$$\n其中 $\\mathbf{s}^k$ 是 $H$ 在 $\\mathbf{w}^k$ 处的次梯度，即 $\\mathbf{s}^k \\in \\partial H(\\mathbf{w}^k)$。\n\n**次梯度计算：**\n由于 $H(\\mathbf{w}) = \\sum_{e=1}^m h(w_e)$，其微分是其各分量次微分的笛卡尔积。我们逐分量计算次梯度 $\\mathbf{s}^k$：$s_e^k \\in \\partial h(w_e^k)$。$h(x) = \\max\\{0, \\lambda x - \\theta\\}$ 的次微分是：\n$$\n\\partial h(x) = \\begin{cases} \\{0\\}  \\text{若 } \\lambda x - \\theta  0 \\iff x  \\theta/\\lambda \\\\ [0, \\lambda]  \\text{若 } \\lambda x - \\theta = 0 \\iff x = \\theta/\\lambda \\\\ \\{\\lambda\\}  \\text{若 } \\lambda x - \\theta > 0 \\iff x > \\theta/\\lambda \\end{cases}\n$$\n为了获得一个特定的次梯度向量 $\\mathbf{s}^k$，我们可以在不可微点处做出确定性的选择。一个常见的选择是取区间的端点之一，例如 $\\lambda$。因此我们设定：\n$$\ns_e^k = \\begin{cases} 0  \\text{若 } w_e^k  \\theta/\\lambda \\\\ \\lambda  \\text{若 } w_e^k \\ge \\theta/\\lambda \\end{cases}\n$$\n\n**求解凸子问题：**\n$\\mathbf{w}^{k+1}$ 的子问题是：\n$$\n\\mathbf{w}^{k+1} = \\arg\\min_{\\mathbf{w} \\ge \\mathbf{l}} \\left\\{ \\frac{1}{2} \\| \\mathbf{w} - \\mathbf{w}^0 \\|_2^2 + \\lambda \\sum_{e=1}^m w_e - \\sum_{e=1}^m s_e^k w_e \\right\\}\n$$\n这个优化问题是可分的，意味着我们可以独立求解每个分量 $w_e$：\n$$\nw_e^{k+1} = \\arg\\min_{w_e \\ge l_e} \\left\\{ \\frac{1}{2} (w_e - w_e^0)^2 + (\\lambda - s_e^k) w_e \\right\\}\n$$\n这是一个带下界约束的一维二次最小化问题。无约束最小化点 $w_e^*$ 可以通过将目标函数对 $w_e$ 的导数设为零来找到：\n$$\n(w_e - w_e^0) + (\\lambda - s_e^k) = 0 \\implies w_e^* = w_e^0 - (\\lambda - s_e^k)\n$$\n有约束问题的解是 $w_e^*$ 在可行集 $[l_e, \\infty)$ 上的投影：\n$$\nw_e^{k+1} = \\max\\{l_e, w_e^*\\} = \\max\\{l_e, w_e^0 - (\\lambda - s_e^k)\\}\n$$\n代入 $s_e^k$ 的表达式，我们得到每个分量的闭式更新规则：\n- 若 $w_e^k  \\theta/\\lambda$，则 $s_e^k=0$，且 $w_e^{k+1} = \\max\\{l_e, w_e^0 - \\lambda\\}$。\n- 若 $w_e^k \\ge \\theta/\\lambda$，则 $s_e^k=\\lambda$，且 $w_e^{k+1} = \\max\\{l_e, w_e^0\\}$。\n\n### 3. 算法流程\n\n每个测试用例的完整算法如下：\n\n1.  **初始化**：\n    a. 给定节点坐标 $\\{\\mathbf{x}_i\\}$，计算成对欧几里得距离矩阵。\n    b. 构建完全图，并使用距离作为边成本计算其最小生成树 (MST)。\n    c. 定义下界向量 $\\mathbf{l} \\in \\mathbb{R}^m$：对于每条边 $e$，如果 $e$ 在 MST 中，则设 $l_e = \\tau$，否则设 $l_e = 0$。\n    d. 计算基准亲和度向量 $\\mathbf{w}^0$，其中 $w^0_{ij} = \\exp(-\\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2^2 / \\sigma^2)$。\n    e. 初始化权重向量 $\\mathbf{w}^0_{\\text{iter}} = \\max\\{\\mathbf{w}^0, \\mathbf{l}\\}$ 以确保起始点可行。为简单起见，我们将迭代序列表示为 $\\mathbf{w}^k$。\n\n2.  **DCA 迭代**：对于 $k=0, 1, ..., K_{\\max}-1$：\n    a. 使用上面推导的逐分量更新规则计算下一个迭代点 $\\mathbf{w}^{k+1}$。\n    b. 检查停止准则：如果 $\\frac{\\|\\mathbf{w}^{k+1} - \\mathbf{w}^k\\|_2}{\\max\\{1, \\|\\mathbf{w}^k\\|_2\\}} \\le \\varepsilon$，则终止循环。\n\n3.  **结果分析**：\n    a. 设最终权重向量为 $\\mathbf{w}^*$。\n    b. 统计权重严格为正的边的数量，$c = |\\{e \\mid w^*_e > 0\\}|$。\n    c. 计算所有学习到的权重的总和，$s = \\sum_e w^*_e$。\n    d. 验证连通性：构建一个邻接矩阵，其中如果边的权重 $w^*_e > 0$，则该边存在。使用图遍历算法（如 BFS 或 DFS）或连通分量算法来确定图是否连通。这将得到布尔值 $b$。由于 MST 中的边权重至少为 $\\tau>0$，该构造保证了 $b=\\text{True}$。\n\n该流程已为提供的测试用例实现，以生成所需的输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.sparse.csgraph import minimum_spanning_tree, connected_components\n\ndef run_dca_for_case(node_coords, sigma, lam, theta, tau, K_max, eps):\n    \"\"\"\n    Solves the graph learning problem for a single test case using DCA.\n    \"\"\"\n    n = len(node_coords)\n    m = n * (n - 1) // 2\n\n    # Generate an ordered list of edges (i, j) with i  j\n    edges = []\n    edge_map = {}\n    idx = 0\n    for i in range(n):\n        for j in range(i + 1, n):\n            edges.append((i, j))\n            edge_map[(i, j)] = idx\n            idx += 1\n\n    # 1. Pre-computation\n    # 1a. Compute baseline affinity vector w0\n    coords = np.array(node_coords)\n    w0 = np.zeros(m)\n    for i in range(m):\n        u, v = edges[i]\n        dist_sq = np.sum((coords[u] - coords[v])**2)\n        w0[i] = np.exp(-dist_sq / sigma**2)\n\n    # 1b. Compute MST to define lower bounds l\n    dist_matrix = squareform(pdist(coords))\n    mst_sparse = minimum_spanning_tree(dist_matrix)\n    mst_rows, mst_cols = mst_sparse.nonzero()\n    mst_edges = set(zip(mst_rows, mst_cols))\n\n    l_vec = np.zeros(m)\n    for i in range(m):\n        u, v = edges[i]\n        if (u, v) in mst_edges:\n            l_vec[i] = tau\n\n    # 2. DCA Iteration\n    # 2a. Initialize w\n    w_k = np.maximum(w0, l_vec) # Start with a feasible point\n    \n    threshold = theta / lam\n\n    for k in range(K_max):\n        w_prev = w_k.copy()\n\n        # Compute subgradient s_k is not explicitly needed. The logic is embedded in the update.\n        # if w_prev[e]  threshold -> s_k[e]=0 -> w_k[e] = max(l_vec[e], w0[e] - lam)\n        # if w_prev[e] >= threshold -> s_k[e]=lam -> w_k[e] = max(l_vec[e], w0[e])\n        \n        w_k_case1 = np.maximum(l_vec, w0 - lam)\n        w_k_case2 = np.maximum(l_vec, w0)\n        \n        w_k = np.where(w_prev  threshold, w_k_case1, w_k_case2)\n\n        # Check stopping criterion\n        norm_w_prev = np.linalg.norm(w_prev)\n        rel_change = np.linalg.norm(w_k - w_prev) / max(1.0, norm_w_prev)\n        \n        if rel_change = eps:\n            break\n\n    w_final = w_k\n\n    # 3. Post-processing and Result Analysis\n    # 3a. Count edges with weight > 0\n    c = np.sum(w_final > 1e-9) # Use small tolerance for float comparison\n\n    # 3b. Sum of all weights\n    s = np.sum(w_final)\n\n    # 3c. Check connectivity\n    adj_matrix = np.zeros((n, n))\n    for i in range(m):\n        if w_final[i] > 1e-9:\n            u, v = edges[i]\n            adj_matrix[u, v] = 1\n            adj_matrix[v, u] = 1\n    \n    n_components, _ = connected_components(adj_matrix, directed=False)\n    b = n_components == 1\n\n    return [int(c), float(s), bool(b)]\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        {\n            \"node_coords\": [(0, 0), (1, 0), (1, 1), (0, 1), (0.5, 0.2)],\n            \"sigma\": 0.8, \"lam\": 0.3, \"theta\": 0.15, \"tau\": 0.05,\n            \"K_max\": 50, \"eps\": 1e-8\n        },\n        {\n            \"node_coords\": [(0, 0), (0.5, 0), (1.0, 0), (1.5, 0)],\n            \"sigma\": 0.5, \"lam\": 0.5, \"theta\": 0.05, \"tau\": 0.05,\n            \"K_max\": 50, \"eps\": 1e-8\n        },\n        {\n            \"node_coords\": [(0, 0), (0.3, 0.1), (0.6, 0), (0.0, 0.6), (0.6, 0.6), (0.3, 0.3)],\n            \"sigma\": 1.0, \"lam\": 0.2, \"theta\": 0.5, \"tau\": 0.03,\n            \"K_max\": 50, \"eps\": 1e-8\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_dca_for_case(**case)\n        results.append(result)\n\n    # Convert boolean True to 'True' to match typical list string representation\n    # This involves a custom string conversion for the nested lists.\n    results_str = []\n    for res in results:\n        # Format [c, s, b] into a string representation\n        # str(True) -> 'True', so direct mapping is fine.\n        res_str = f\"[{res[0]},{res[1]},{str(res[2])}]\"\n        results_str.append(res_str)\n        \n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        }
    ]
}