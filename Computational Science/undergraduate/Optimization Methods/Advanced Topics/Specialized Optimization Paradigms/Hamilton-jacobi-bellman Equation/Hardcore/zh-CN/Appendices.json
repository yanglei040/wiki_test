{
    "hands_on_practices": [
        {
            "introduction": "哈密尔顿-雅可比-贝尔曼（HJB）方程的核心是哈密尔顿量，它将系统的动态、运行成本和价值函数的梯度封装在一个表达式中。这个练习将带你动手实践HJB方程的第一步：为给定的随机控制问题构建哈密尔顿量。通过对控制变量进行最小化，你将推导出最优反馈控制以及最小化的哈密尔顿量，这是求解整个HJB方程的关键所在。",
            "id": "3080765",
            "problem": "考虑一维受控 Itô 随机微分方程\n$$\n\\mathrm{d}X_{t} = b(X_{t},u_{t})\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t},\n$$\n其漂移项为 $b(x,u) = u$，扩散系数为常数 $\\sigma > 0$，控制过程 $u_{t}$ 在 $\\mathbb{R}$ 中取值。设运行成本为\n$$\nf(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2},\n$$\n其中 $r>0$ 且 $q>0$。对于具有有界连续终端成本 $g(x)$ 的有限时间域问题，将价值函数 $V(t,x)$ 定义为\n$$\nV(t,x) = \\inf_{u} \\,\\mathbb{E}\\!\\left[ \\int_{t}^{T} f(X_{s},u_{s})\\,\\mathrm{d}s + g(X_{T}) \\,\\middle|\\, X_{t}=x \\right],\n$$\n其中下确界取自所有在 $\\mathbb{R}$ 中取值的循序可测控制。根据动态规划原理以及 Hamilton-Jacobi-Bellman (HJB) 方程中出现的哈密顿量的定义，点 $(x,p,M)$ 处的哈密顿量由受控生成元作用于一个 $C^{2}$ 测试函数与运行成本之和关于控制 $u \\in \\mathbb{R}$ 的下确界给出。特别地，如果 $p$ 表示一阶空间导数，$M$ 表示二阶空间导数，请计算：\n- 作为 $x$、$p$、$M$、$\\sigma$、$r$ 和 $q$ 的显式函数的最小化哈密顿量；\n- 作为 $p$ 和 $r$ 的显式函数的相应最小化状态反馈控制。\n\n请将您的最终答案表示为一个双元行矩阵，第一个元素是最小化哈密顿量，第二个元素是最小化反馈控制。不需要数值近似，也不涉及单位。",
            "solution": "用户要求我解决一个与随机最优控制背景下的 Hamilton-Jacobi-Bellman (HJB) 方程相关的问题。\n\n### 第一步：提取已知条件\n- 受控 Itô 随机微分方程 (SDE)：$\\mathrm{d}X_{t} = b(X_{t},u_{t})\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}$。\n- 漂移函数：$b(x,u) = u$。\n- 扩散系数：$\\sigma > 0$ 是一个常数。\n- 控制过程 $u_{t}$ 在 $\\mathbb{R}$ 中取值。\n- 运行成本函数：$f(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2}$。\n- 成本函数中的常数满足：$r>0$ 和 $q>0$。\n- 价值函数在有限时间域 $[t, T]$ 上定义为：$V(t,x) = \\inf_{u} \\,\\mathbb{E}\\!\\left[ \\int_{t}^{T} f(X_{s},u_{s})\\,\\mathrm{d}s + g(X_{T}) \\,\\middle|\\, X_{t}=x \\right]$。\n- 终端成本 $g(x)$ 是一个有界连续函数。\n- 点 $(x,p,M)$ 处的哈密顿量定义为，受控生成元作用于一个 $C^{2}$ 测试函数与运行成本之和关于控制 $u \\in \\mathbb{R}$ 的下确界。\n- 变量 $p$ 表示测试函数的一阶空间导数。\n- 变量 $M$ 表示测试函数的二阶空间导数。\n\n### 第二步：使用提取的已知条件进行验证\n对问题进行验证。\n\n- **科学依据**：该问题是随机线性二次调节器 (LQR) 问题的标准表述，这是最优控制理论的基石。Hamilton-Jacobi-Bellman (HJB) 方程是通过动态规划解决此类问题的核心工具。所有概念在随机微分方程和控制理论领域都是标准且成熟的。\n- **适定性**：该问题是适定的。它要求计算两个明确、定义良好的量：最小化哈密顿量和最优反馈控制。条件 $r>0$ 确保成本函数在控制变量 $u$ 上是严格凸的，这保证了最小值的存在性和唯一性。\n- **客观性**：该问题以精确、客观的数学语言陈述，没有任何主观性或歧义。\n\n该问题没有指令中列出的任何缺陷。它在科学上是合理的、适定的、客观的、完整的和可形式化的。\n\n### 第三步：结论与行动\n该问题是 **有效的**。将提供完整的解决方案。\n\n### 解\n该问题要求计算给定随机控制问题的最小化哈密顿量和相应的最优控制。在 Hamilton-Jacobi-Bellman (HJB) 方程的背景下，哈密顿量定义为：\n$$\nH(x,p,M) = \\inf_{u \\in \\mathbb{R}} \\left\\{ \\mathcal{L}^{u} + f(x,u) \\right\\}\n$$\n其中 $\\mathcal{L}^{u}$ 是受控随机过程的生成元，作用于一个测试函数（其一阶和二阶空间导数分别用 $p$ 和 $M$ 表示），而 $f(x,u)$ 是运行成本。\n\n首先，我们确定生成元 $\\mathcal{L}^{u}$。对于由 $\\mathrm{d}X_{t} = \\mu(t, X_{t})\\,\\mathrm{d}t + \\Sigma(t, X_{t})\\,\\mathrm{d}W_{t}$ 给出的一般一维 Itô 过程，作用于 $C^{2}$ 函数 $\\phi(x)$ 的生成元是：\n$$\n\\mathcal{L}\\phi(x) = \\mu(t,x) \\frac{\\partial \\phi}{\\partial x} + \\frac{1}{2} \\Sigma(t,x)^{2} \\frac{\\partial^{2} \\phi}{\\partial x^{2}}\n$$\n在我们的具体问题中，SDE 是 $\\mathrm{d}X_{t} = u_{t}\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_{t}$。漂移项是 $\\mu(t,x,u) = u$，扩散项是 $\\Sigma(t,x,u) = \\sigma$。设 $\\phi(x)$ 为一个通用的 $C^2$ 测试函数。根据问题陈述，我们将其导数表示为 $ p = \\frac{\\mathrm{d}\\phi}{\\mathrm{d}x}$ 和 $M = \\frac{\\mathrm{d}^2\\phi}{\\mathrm{d}x^2}$。\n因此，受控过程的生成元 $\\mathcal{L}^{u}$ 为：\n$$\n\\mathcal{L}^{u} = u \\cdot p + \\frac{1}{2}\\sigma^{2} \\cdot M\n$$\n接下来，我们加上运行成本 $f(x,u) = \\frac{1}{2} r\\,u^{2} + q\\,x^{2}$。需要关于控制 $u \\in \\mathbb{R}$ 最小化的表达式是：\n$$\nJ(u) = \\mathcal{L}^{u} + f(x,u) = \\left( u p + \\frac{1}{2}\\sigma^{2}M \\right) + \\left( \\frac{1}{2} r\\,u^{2} + q\\,x^{2} \\right)\n$$\n我们可以将此表达式重新整理为 $u$ 的函数：\n$$\nJ(u) = \\frac{1}{2} r\\,u^{2} + p\\,u + \\left( q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M \\right)\n$$\n这是关于 $u$ 的二次函数。由于问题陈述 $r>0$，该抛物线开口向上，函数有唯一的全局最小值。为了找到使 $J(u)$ 最小化的 $u$ 值，我们计算其关于 $u$ 的一阶导数并令其为零。\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}u} = \\frac{\\mathrm{d}}{\\mathrm{d}u} \\left( \\frac{1}{2} r\\,u^{2} + p\\,u + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M \\right) = r\\,u + p\n$$\n将导数设为零，得到最优控制，我们将其表示为 $u^{\\star}$：\n$$\nr\\,u^{\\star} + p = 0 \\implies u^{\\star} = -\\frac{p}{r}\n$$\n这是作为 $p$ 和 $r$ 的函数的最小化状态反馈控制。这是所要求答案的第二部分。\n\n为了找到最小化的哈密顿量 $H(x,p,M)$，我们将 $u^{\\star}$ 代回 $J(u)$ 的表达式中：\n$$\nH(x,p,M) = J(u^{\\star}) = \\frac{1}{2} r\\left(-\\frac{p}{r}\\right)^{2} + p\\left(-\\frac{p}{r}\\right) + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n$$\nH(x,p,M) = \\frac{1}{2} r\\left(\\frac{p^{2}}{r^{2}}\\right) - \\frac{p^{2}}{r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n$$\nH(x,p,M) = \\frac{p^{2}}{2r} - \\frac{p^{2}}{r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n合并包含 $p^{2}$ 的项：\n$$\nH(x,p,M) = -\\frac{p^{2}}{2r} + q\\,x^{2} + \\frac{1}{2}\\sigma^{2}M\n$$\n这是作为 $x$、$p$、$M$、$\\sigma$、$r$ 和 $q$ 的显式函数的最小化哈密顿量。这是所要求答案的第一部分。\n\n最终答案以一个双元行矩阵的形式呈现。\n第一个元素：最小化哈密顿量 $H(x,p,M) = q\\,x^{2} - \\frac{p^{2}}{2r} + \\frac{1}{2}\\sigma^{2}M$。\n第二个元素：最小化控制 $u^{\\star} = -\\frac{p}{r}$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} q\\,x^{2} - \\frac{p^{2}}{2r} + \\frac{1}{2}\\sigma^{2}M  -\\frac{p}{r} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "掌握了哈密顿量之后，我们便可以着手求解完整的HJB方程。这个练习将引导你解决一个无限时域、折扣成本的线性二次调节器（LQR）问题，这类问题在经济学和工程学中非常普遍。你将学习如何使用二次价值函数“猜想”($V(x) = Px^2 + v_0$)，并将HJB偏微分方程转化为一个代数黎卡提方程（Algebraic Riccati Equation），从而求得稳态解。",
            "id": "3080759",
            "problem": "考虑一维受控随机微分方程（SDE）\n$$\n\\mathrm{d}x_{t}=\\big(a\\,x_{t}+b\\,u_{t}\\big)\\,\\mathrm{d}t+\\sigma\\,\\mathrm{d}W_{t},\n$$\n其中 $a=1$，$b=2$，$\\sigma=1$，且 $\\{W_{t}\\}_{t\\ge 0}$ 是标准维纳过程。令控制 $\\{u_{t}\\}_{t\\ge 0}$ 关于由 $\\{W_{t}\\}$ 生成的滤子是渐进可测的，并满足确保唯一强解存在的标准可积性条件。无限时域折扣成本为\n$$\nJ(u)=\\mathbb{E}\\left[\\int_{0}^{\\infty}\\exp(-\\rho t)\\,\\big(q\\,x_{t}^{2}+r\\,u_{t}^{2}\\big)\\,\\mathrm{d}t\\right],\n$$\n其中 $\\rho=3$，$q=1$，$r=1$。假设时间同质性，并考虑平稳马尔可夫反馈控制 $u_{t}=u(x_{t})$。\n\n从动态规划原理和扩散的无穷小生成元出发，推导在无限时域折扣设置下价值函数 $V(x)$ 的 Hamilton-Jacobi-Bellman (HJB) 方程。在二次拟设 $V(x)=P\\,x^{2}+v_{0}$（其中常数 $P0$ 且 $v_{0}\\in\\mathbb{R}$）下，证明 $P$ 必须满足一个代数 Riccati 方程，显式求解 $P$，并确定平稳最优反馈 $u^{\\star}(x)$ 和相应的价值函数 $V(x)$。\n\n以单行矩阵的形式提供你的最终答案，包含三个表达式，顺序为 $(P,\\;u^{\\star}(x),\\;V(x))$。最终答案中不应出现推导过程。最终答案必须是精确的闭式表达式。不要四舍五入或近似。不需要单位。",
            "solution": "该问题是有效的。这是一个适定的、有科学依据的随机最优控制问题，具体来说是一个连续时间线性二次调节器。所有必要的数据和条件都已提供，没有矛盾或含糊之处。\n\n无限时域折扣最优控制问题的价值函数定义为\n$$\nV(x) = \\inf_{u} J(u) = \\inf_{u} \\mathbb{E}\\left[\\int_{0}^{\\infty}\\exp(-\\rho t)\\,\\big(q\\,x_{t}^{2}+r\\,u_{t}^{2}\\big)\\,\\mathrm{d}t \\, \\bigg| \\, x_0=x \\right]\n$$\n其中下确界取遍所有容许控制 $\\{u_t\\}$。动态规划原理导出 Hamilton-Jacobi-Bellman (HJB) 方程。对于一个时间同质的无限时域折扣问题，HJB 方程由下式给出\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ L(x, u) + \\mathcal{A}^u V(x) \\right\\}\n$$\n其中 $\\rho$ 是折扣率，$L(x, u) = q\\,x^{2}+r\\,u^{2}$ 是运行成本，而 $\\mathcal{A}^u$ 是受控扩散过程 $x_t$ 的无穷小生成元。\n\n受控随机微分方程（SDE）为\n$$\n\\mathrm{d}x_{t}=\\big(a\\,x_{t}+b\\,u_{t}\\big)\\,\\mathrm{d}t+\\sigma\\,\\mathrm{d}W_{t}\n$$\n漂移系数为 $\\mu(x, u) = a\\,x+b\\,u$，扩散系数为 $\\eta(x, u) = \\sigma$。作用于二次可微函数 $V(x)$ 的无穷小生成元 $\\mathcal{A}^u$ 是\n$$\n\\mathcal{A}^u V(x) = \\mu(x,u) \\frac{\\mathrm{d}V}{\\mathrm{d}x} + \\frac{1}{2} \\eta(x,u)^2 \\frac{\\mathrm{d}^2V}{\\mathrm{d}x^2} = (a\\,x+b\\,u)V'(x) + \\frac{1}{2}\\sigma^2 V''(x)\n$$\n将 $L(x, u)$ 和 $\\mathcal{A}^u V(x)$ 代入 HJB 方程，得到\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ q\\,x^2 + r\\,u^2 + (a\\,x+b\\,u)V'(x) + \\frac{1}{2}\\sigma^2 V''(x) \\right\\}\n$$\n为了找到最优控制 $u^{\\star}$，我们将大括号内的表达式关于 $u$ 最小化。该表达式是 $u$ 的二次函数。最小化的一阶条件是通过将关于 $u$ 的偏导数设为零来找到的：\n$$\n\\frac{\\partial}{\\partial u} \\left[ q\\,x^2 + r\\,u^2 + (a\\,x+b\\,u)V'(x) + \\frac{1}{2}\\sigma^2 V''(x) \\right] = 2\\,r\\,u + b\\,V'(x) = 0\n$$\n求解 $u$ 得到平稳最优反馈控制，作为价值函数导数的函数：\n$$\nu^{\\star}(x) = -\\frac{b}{2r}V'(x)\n$$\n二阶导数是 $2r$，由于 $r=1  0$ 为正，这确认了这确实是一个最小值点。\n\n将 $u^{\\star}(x)$ 代回 HJB 方程消除了下确界：\n$$\n\\rho V(x) = q\\,x^2 + r\\left(-\\frac{b}{2r}V'(x)\\right)^2 + \\left(a\\,x+b\\left(-\\frac{b}{2r}V'(x)\\right)\\right)V'(x) + \\frac{1}{2}\\sigma^2 V''(x)\n$$\n简化各项：\n$$\n\\rho V(x) = q\\,x^2 + r\\frac{b^2}{4r^2}(V'(x))^2 + a\\,xV'(x) - \\frac{b^2}{2r}(V'(x))^2 + \\frac{1}{2}\\sigma^2 V''(x)\n$$\n$$\n\\rho V(x) = q\\,x^2 + a\\,xV'(x) - \\frac{b^2}{4r}(V'(x))^2 + \\frac{1}{2}\\sigma^2 V''(x)\n$$\n这是一个关于价值函数 $V(x)$ 的非线性常微分方程。\n\n我们现在使用二次拟设 $V(x) = P\\,x^2 + v_0$，其中 $P0$ 和 $v_0$ 是待定常数。导数是：\n$$\nV'(x) = 2\\,P\\,x\n$$\n$$\nV''(x) = 2\\,P\n$$\n将这些代入 HJB 方程：\n$$\n\\rho(P\\,x^2+v_0) = q\\,x^2 + a\\,x(2\\,P\\,x) - \\frac{b^2}{4r}(2\\,P\\,x)^2 + \\frac{1}{2}\\sigma^2(2\\,P)\n$$\n$$\n\\rho\\,P\\,x^2 + \\rho\\,v_0 = q\\,x^2 + 2\\,a\\,P\\,x^2 - \\frac{b^2}{4r}(4\\,P^2\\,x^2) + \\sigma^2\\,P\n$$\n$$\n\\rho\\,P\\,x^2 + \\rho\\,v_0 = \\left( q + 2\\,a\\,P - \\frac{b^2}{r}P^2 \\right)x^2 + \\sigma^2\\,P\n$$\n为了使该方程对所有 $x \\in \\mathbb{R}$ 成立，我们必须令等式两边 $x$ 的各次幂的系数相等。\n令 $x^2$ 的系数相等：\n$$\n\\rho\\,P = q + 2\\,a\\,P - \\frac{b^2}{r}P^2\n$$\n整理后得到关于 $P$ 的代数 Riccati 方程 (ARE)：\n$$\n\\frac{b^2}{r}P^2 + (\\rho - 2\\,a)P - q = 0\n$$\n令常数项（$x^0$ 的系数）相等：\n$$\n\\rho\\,v_0 = \\sigma^2\\,P \\quad \\implies \\quad v_0 = \\frac{\\sigma^2\\,P}{\\rho}\n$$\n给定的参数值为：$a=1$，$b=2$，$\\sigma=1$，$\\rho=3$，$q=1$，$r=1$。将这些值代入 ARE：\n$$\n\\frac{2^2}{1}P^2 + (3 - 2 \\cdot 1)P - 1 = 0\n$$\n$$\n4P^2 + P - 1 = 0\n$$\n我们使用二次公式求解这个关于 $P$ 的二次方程：\n$$\nP = \\frac{-1 \\pm \\sqrt{1^2 - 4(4)(-1)}}{2(4)} = \\frac{-1 \\pm \\sqrt{1 + 16}}{8} = \\frac{-1 \\pm \\sqrt{17}}{8}\n$$\n价值函数必须是凸的，这对应于 $P  0$。因此，我们必须选择正根：\n$$\nP = \\frac{-1 + \\sqrt{17}}{8}\n$$\n现在我们求常数项 $v_0$：\n$$\nv_0 = \\frac{\\sigma^2\\,P}{\\rho} = \\frac{1^2}{3}P = \\frac{1}{3} \\left( \\frac{\\sqrt{17}-1}{8} \\right) = \\frac{\\sqrt{17}-1}{24}\n$$\n因此，价值函数为：\n$$\nV(x) = P\\,x^2 + v_0 = \\left(\\frac{\\sqrt{17}-1}{8}\\right)x^2 + \\frac{\\sqrt{17}-1}{24}\n$$\n最后，我们确定最优反馈控制 $u^{\\star}(x)$：\n$$\nu^{\\star}(x) = -\\frac{b}{2r}V'(x) = -\\frac{b}{2r}(2\\,P\\,x) = -\\frac{b}{r}Px\n$$\n代入 $b$，$r$ 和已解出的 $P$ 的值：\n$$\nu^{\\star}(x) = -\\frac{2}{1} \\left( \\frac{\\sqrt{17}-1}{8} \\right) x = -\\frac{\\sqrt{17}-1}{4}x\n$$\n所求的三个量是：\n$P = \\frac{\\sqrt{17}-1}{8}$\n$u^{\\star}(x) = -\\frac{\\sqrt{17}-1}{4}x$\n$V(x) = \\frac{\\sqrt{17}-1}{8}x^2 + \\frac{\\sqrt{17}-1}{24}$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\sqrt{17}-1}{8}  -\\frac{\\sqrt{17}-1}{4}x  \\frac{\\sqrt{17}-1}{8}x^2 + \\frac{\\sqrt{17}-1}{24} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "解析解为我们提供了深刻的理论洞见，但在许多实际应用中，我们必须依赖数值方法。这个练习是一个编程任务，旨在将理论付诸实践，通过实现策略迭代算法来数值求解HJB方程。你将学习如何离散化状态和控制空间，并交替进行“策略评估”和“策略改进”两个步骤，最终收敛到最优策略和价值函数，体验从连续理论到离散计算的完整过程。",
            "id": "3001638",
            "problem": "请从第一性原理出发，为一个源于受控随机微分方程的一维平稳折扣 Hamilton-Jacobi-Bellman 方程实现一个完整的策略迭代方案。受控状态过程由随机微分方程 $dX_t = \\left(a X_t + b u_t\\right) dt + \\sigma dW_t$ 指定，折扣率为 $\\rho  0$，其中 $a$、$b$、$\\sigma$ 和 $\\rho$ 是固定的实数参数，$u_t \\in \\mathbb{R}$ 是一个控制，$W_t$ 是一个标准维纳过程。瞬时成本为 $\\ell(x,u) = q x^2 + r u^2$，其中 $q  0$ 且 $r  0$。从初始状态 $x$ 出发，目标是在所有容许控制下，最小化折扣成本 $J^u(x) = \\mathbb{E}\\left[\\int_0^\\infty e^{-\\rho t} \\ell\\left(X_t^u, u_t\\right) dt\\right]$。令 $V(x)$ 表示价值函数。\n\n您的任务是：\n- 在有界区间 $\\left[-x_{\\max}, x_{\\max}\\right]$ 上，推导 $V(x)$ 满足的平稳折扣 Hamilton-Jacobi-Bellman 方程，其狄利克雷边界条件选择为与该问题的精确无限时域折扣线性二次解 $V^{\\mathrm{ref}}(x)$ 相匹配。已知参考解是二次形式 $V^{\\mathrm{ref}}(x) = P x^2 + C$，其中系数 $P$ 由 Hamilton-Jacobi-Bellman 方程所蕴含的代数 Riccati 关系确定，而 $C$ 的选择是为了满足常数扩散项的平稳折扣平衡。\n- 在 $\\left[-x_{\\max}, x_{\\max}\\right]$ 上用 $N$ 个网格点均匀离散化状态空间，网格间距为 $h = 2 x_{\\max}/(N-1)$。在内部节点处，用中心差分算子近似一阶导数，用标准三点中心差分算子近似二阶导数。施加狄利克雷边界条件 $V(-x_{\\max}) = V^{\\mathrm{ref}}(-x_{\\max})$ 和 $V(x_{\\max}) = V^{\\mathrm{ref}}(x_{\\max})$。\n- 将动作空间离散化为一个有限的对称集合 $\\mathcal{U}_M = \\left\\{u_j\\right\\}_{j=1}^M$，该集合包含 $M$ 个在 $\\left[-u_{\\max}, u_{\\max}\\right]$ 内均匀间隔的值，其中 $u_{\\max}$ 选择为 $u_{\\max} = \\kappa \\left| b P x_{\\max} / r \\right|$，$\\kappa = 1.25$，以使离散动作网格能覆盖整个计算域上的连续优化器。\n- 实现策略迭代：\n  - 策略评估：对于每个内部网格点 $x_i$ 的固定离散策略 $u(x_i) \\in \\mathcal{U}_M$，求解由离散化的平稳折扣 Hamilton-Jacobi-Bellman 方程产生的线性系统，以获得网格上的价值向量。\n  - 策略改进：在每个内部网格点 $x_i$，使用当前的离散梯度近似 $\\left(D_x V\\right)(x_i)$，计算离散哈密顿最小化子 $\\arg\\min_{u \\in \\mathcal{U}_M} \\left\\{ r u^2 + b u \\left(D_x V\\right)(x_i) \\right\\}$，并更新策略。\n  - 当价值向量的上确界范数变化和策略的上确界范数变化均小于容差 $\\varepsilon$ 时，或当策略迭代次数达到最大值 $K_{\\max}$ 时终止。\n- 对于下述每个测试用例，计算网格上的均匀误差 $\\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty}$，其中 $V_{\\mathrm{num}}$ 是收敛的数值价值函数，$V^{\\mathrm{ref}}$ 是精确的二次参考解。\n\n使用以下参数集测试套件，每个参数集以元组 $\\left(a, b, q, r, \\sigma, \\rho, x_{\\max}, N, M\\right)$ 的形式提供：\n- 测试 1：$\\left(-0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 3.0, 161, 61\\right)$。\n- 测试 2：$\\left(0.2, 1.0, 1.0, 0.5, 0.1, 1.0, 2.0, 161, 61\\right)$。\n- 测试 3：$\\left(0.3, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 161, 61\\right)$。\n\n实现细节和要求：\n- 从内部节点上的零策略 $u \\equiv 0$ 开始进行策略迭代。\n- 使用容差 $\\varepsilon = 10^{-6}$ 和最大迭代次数 $K_{\\max} = 100$。\n- 所有计算都是无量纲的；不需要物理单位。\n- 您的程序必须是一个完整的单一脚本，能够对所有三个测试执行整个计算过程，无需用户输入，并打印包含结果的单行输出。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，顺序与测试用例一致，例如 $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$，其中每个条目是对应测试在状态网格上计算出的 $\\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty}$ 的浮点值。",
            "solution": "用户提供的问题是随机最优控制领域中一个适定且科学上合理的问题。它要求使用策略迭代方案来数值求解一个平稳折扣 Hamilton-Jacobi-Bellman (HJB) 方程。该问题是自洽的，提供了所有必要的参数和方法论规范。它基于成熟的数学理论，是计算金融和控制工程中的一个标准问题。因此，该问题被认为是有效的，下面提供了完整的解决方案。\n\n### 1. Hamilton-Jacobi-Bellman 方程与精确解\n\n状态过程由线性随机微分方程控制：\n$$\ndX_t = \\left(a X_t + b u_t\\right) dt + \\sigma dW_t\n$$\n其中 $a$、$b$ 和 $\\sigma$ 是实常数，$u_t \\in \\mathbb{R}$ 是控制，$W_t$ 是一个维纳过程。目标是最小化折扣成本泛函，其折扣率为 $\\rho  0$，瞬时成本为二次形式 $\\ell(x, u) = q x^2 + r u^2$：\n$$\nJ^u(x) = \\mathbb{E}\\left[\\int_0^\\infty e^{-\\rho t} \\ell\\left(X_t^u, u_t\\right) dt \\;\\middle|\\; X_0 = x \\right]\n$$\n价值函数 $V(x) = \\inf_u J^u(x)$ 满足平稳 Hamilton-Jacobi-Bellman (HJB) 方程：\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ \\mathcal{L}^u V(x) + \\ell(x, u) \\right\\}\n$$\n其中 $\\mathcal{L}^u$ 是过程 $X_t$ 的无穷小生成元，由 $\\mathcal{L}^u V(x) = (a x + b u) V'(x) + \\frac{1}{2} \\sigma^2 V''(x)$ 给出。代入生成元和瞬时成本的表达式，HJB 方程变为：\n$$\n\\rho V(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ (a x + b u) V'(x) + \\frac{1}{2} \\sigma^2 V''(x) + q x^2 + r u^2 \\right\\}\n$$\n通过将花括号中项对 $u$ 的偏导数设为零来求得关于 $u$ 的下确界：\n$$\nb V'(x) + 2 r u = 0 \\implies u^*(x) = -\\frac{b}{2r} V'(x)\n$$\n将这个最优控制 $u^*(x)$ 代回 HJB 方程，得到一个关于 $V(x)$ 的非线性常微分方程：\n$$\n\\rho V(x) = a x V'(x) - \\frac{b^2}{4r} (V'(x))^2 + \\frac{1}{2} \\sigma^2 V''(x) + q x^2\n$$\n对于这个线性二次问题，已知价值函数是状态的二次函数，$V(x) = P x^2 + C$。其导数为 $V'(x) = 2 P x$ 和 $V''(x) = 2 P$。将这些代入 HJB 方程得到：\n$$\n\\rho (P x^2 + C) = a x (2 P x) - \\frac{b^2}{4r} (2 P x)^2 + \\frac{1}{2} \\sigma^2 (2 P) + q x^2\n$$\n$$\n\\rho P x^2 + \\rho C = (2aP - \\frac{b^2}{r}P^2 + q) x^2 + \\sigma^2 P\n$$\n令 $x$ 的同次幂系数相等，我们得到两个关于未知常数 $P$ 和 $C$ 的代数方程。$x^2$ 的系数产生了连续时间代数 Riccati 方程 (ARE)：\n$$\n\\frac{b^2}{r} P^2 - (2a - \\rho) P - q = 0\n$$\n由于成本必须是正定的，我们要求 $P  0$。ARE 是一个关于 $P$ 的二次方程，并存在唯一的正解，由下式给出：\n$$\nP = \\frac{(2a - \\rho) + \\sqrt{(2a - \\rho)^2 + 4(b^2/r)q}}{2(b^2/r)}\n$$\n令常数项相等，得到 $C$ 的方程：\n$$\n\\rho C = \\sigma^2 P \\implies C = \\frac{\\sigma^2 P}{\\rho}\n$$\n这定义了在无限域上的精确参考解 $V^{\\mathrm{ref}}(x) = Px^2 + C$。\n\n### 2. 离散化\n\n问题在有界域 $x \\in [-x_{\\max}, x_{\\max}]$ 上求解，该域被离散化为一个包含 $N$ 个点的均匀网格 $\\{x_i\\}_{i=0}^{N-1}$，网格间距为 $h = 2x_{\\max}/(N-1)$。令 $V_i$ 为 $V(x_i)$ 的数值近似。在内部网格点 $x_i$（对于 $i \\in \\{1, \\dots, N-2\\}$），导数使用中心有限差分进行近似：\n$$\nV'(x_i) \\approx \\frac{V_{i+1} - V_{i-1}}{2h}, \\qquad V''(x_i) \\approx \\frac{V_{i+1} - 2V_i + V_{i-1}}{h^2}\n$$\n对于一个固定的策略 $u(x)$（离散表示为 $u_i = u(x_i)$），HJB 方程在 $V$ 中是线性的。在每个内部点 $x_i$ 的离散形式为：\n$$\n\\rho V_i = (a x_i + b u_i) \\left(\\frac{V_{i+1} - V_{i-1}}{2h}\\right) + \\frac{1}{2}\\sigma^2 \\left(\\frac{V_{i+1} - 2V_i + V_{i-1}}{h^2}\\right) + q x_i^2 + r u_i^2\n$$\n整理各项，我们得到一个关于 $V_{i-1}, V_i, V_{i+1}$ 的线性方程：\n$$\n\\left( \\frac{a x_i + b u_i}{2h} - \\frac{\\sigma^2}{2h^2} \\right) V_{i-1} + \\left( \\rho + \\frac{\\sigma^2}{h^2} \\right) V_i - \\left( \\frac{a x_i + b u_i}{2h} + \\frac{\\sigma^2}{2h^2} \\right) V_{i+1} = q x_i^2 + r u_i^2\n$$\n边界条件为狄利克雷类型，固定为参考解的值：$V_0 = V^{\\mathrm{ref}}(-x_{\\max})$ 和 $V_{N-1} = V^{\\mathrm{ref}}(x_{\\max})$。\n\n### 3. 策略迭代算法\n\n策略迭代是一种迭代方法，它在策略评估和策略改进两个步骤之间交替进行。\n\n**初始化**：算法从一个初始策略开始，设为零策略，即对所有内部网格点 $x_i$，有 $u^{(0)}(x_i) = 0$。初始价值函数 $V^{(0)}$ 初始化为零，边界值从参考解设定。\n\n**策略评估**：对于给定的策略 $u^{(k)}$，我们求解相应的价值函数 $V^{(k+1)}$。所有内部点 $i \\in \\{1, \\dots, N-2\\}$ 的离散 HJB 方程集合构成一个关于未知值 $\\{V_i^{(k+1)}\\}_{i=1}^{N-2}$ 的三对角线性方程组。这个形如 $\\mathbf{A}\\mathbf{V} = \\mathbf{d}$ 的系统使用标准的线性代数求解器求解。矩阵 $\\mathbf{A}$ 和向量 $\\mathbf{d}$ 依赖于当前策略 $u^{(k)}$。\n\n**策略改进**：计算出新的价值函数 $V^{(k+1)}$ 后，通过在每个内部网格点 $x_i$ 最小化离散哈密顿量来更新策略为 $u^{(k+1)}$：\n$$\nu_i^{(k+1)} = \\arg\\min_{u \\in \\mathcal{U}_M} \\left\\{ (a x_i + b u) \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h} + \\frac{1}{2}\\sigma^2 (\\dots) + q x_i^2 + r u^2 \\right\\}\n$$\n这等价于在离散动作集 $\\mathcal{U}_M$ 上最小化 $r u^2 + b u \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h}$。无约束最小化子为 $u_i^* = -\\frac{b}{2r} \\frac{V_{i+1}^{(k+1)} - V_{i-1}^{(k+1)}}{2h}$。然后，新的策略动作 $u_i^{(k+1)}$ 被选为离散动作集 $\\mathcal{U}_M$ 中最接近 $u_i^*$ 的元素。\n\n**终止**：评估和改进之间的迭代持续进行，直到策略和价值函数收敛。当价值向量的最大绝对变化 $\\|V^{(k+1)} - V^{(k)}\\|_{\\infty}$ 和策略向量的最大绝对变化 $\\|u^{(k+1)} - u^{(k)}\\|_{\\infty}$ 都低于指定的容差 $\\varepsilon = 10^{-6}$，或达到最大迭代次数 $K_{\\max} = 100$ 时，过程终止。\n\n### 4. 误差计算\n\n策略迭代收敛到最终数值解 $V_{\\mathrm{num}}$ 后，通过计算其与精确参考解 $V^{\\mathrm{ref}}$ 在计算网格上的误差的一致范数来评估其准确性：\n$$\n\\text{误差} = \\| V_{\\mathrm{num}} - V^{\\mathrm{ref}} \\|_{\\infty} = \\max_{i \\in \\{0, \\dots, N-1\\}} | V_{\\mathrm{num}}(x_i) - V^{\\mathrm{ref}}(x_i) |\n$$\n为每个提供的测试用例计算此量。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the policy iteration for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (a, b, q, r, sigma, rho, x_max, N, M)\n        (-0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 3.0, 161, 61),\n        (0.2, 1.0, 1.0, 0.5, 0.1, 1.0, 2.0, 161, 61),\n        (0.3, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 161, 61),\n    ]\n\n    results = []\n    for params in test_cases:\n        error = run_policy_iteration(params)\n        results.append(error)\n\n    # Format and print the final output as a single line\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef run_policy_iteration(params):\n    \"\"\"\n    Solves the HJB equation for a single set of parameters using policy iteration.\n    \"\"\"\n    a, b, q, r, sigma, rho, x_max, N, M = params\n    \n    # --- Step A: Pre-computation and Setup ---\n\n    # 1. Solve the Algebraic Riccati Equation for P\n    # The ARE is (b^2/r)P^2 - (2a - rho)P - q = 0\n    A_ric = b**2 / r\n    B_ric = -(2.0*a - rho)\n    C_ric = -q\n    discriminant = B_ric**2 - 4.0 * A_ric * C_ric\n    # We need the positive root for P\n    P = (-B_ric + np.sqrt(discriminant)) / (2.0 * A_ric)\n\n    # 2. Calculate the constant C\n    C = (sigma**2 * P) / rho\n\n    # 3. Define the reference (exact) value function\n    def v_ref(x):\n        return P * x**2 + C\n\n    # 4. Set up the state and action grids\n    x_grid = np.linspace(-x_max, x_max, N)\n    h = x_grid[1] - x_grid[0]\n    v_ref_grid = v_ref(x_grid)\n\n    kappa = 1.25\n    u_max_val = kappa * abs(b * P * x_max / r)\n    \n    if M > 1 and u_max_val > 0:\n        u_grid = np.linspace(-u_max_val, u_max_val, M)\n    else:\n        u_grid = np.zeros(M)\n    u_step = u_grid[1] - u_grid[0] if M > 1 else 0\n\n    # 5. Set up iteration parameters\n    tol = 1e-6\n    max_iter = 100\n    \n    # --- Step B: Policy Iteration Loop ---\n\n    # Initialization\n    # Policy for interior points (size N-2)\n    current_policy = np.zeros(N - 2)\n    # Value function on the full grid (size N)\n    current_V = np.zeros(N)\n    # Set Dirichlet boundary conditions from the exact solution\n    current_V[0] = v_ref_grid[0]\n    current_V[-1] = v_ref_grid[-1]\n    \n    x_interior = x_grid[1:-1]\n    \n    for k in range(max_iter):\n        \n        # --- 1. Policy Evaluation ---\n        # Solve the linear system A * V_interior = d for V\n        \n        drift_coeff = a * x_interior + b * current_policy\n        \n        # Coefficients of the tridiagonal system\n        L = drift_coeff / (2.0 * h) - sigma**2 / (2.0 * h**2)\n        D = rho + sigma**2 / h**2\n        U = -drift_coeff / (2.0 * h) - sigma**2 / (2.0 * h**2)\n        \n        # Construct the (N-2) x (N-2) system matrix A\n        A = np.diag(D * np.ones(N-2)) + np.diag(U[:-1], k=1) + np.diag(L[1:], k=-1)\n        \n        # Construct the right-hand side vector d\n        d = q * x_interior**2 + r * current_policy**2\n        \n        # Adjust d for boundary conditions\n        d[0] -= L[0] * v_ref_grid[0]\n        d[-1] -= U[-1] * v_ref_grid[-1]\n        \n        # Solve for the new interior values of V\n        try:\n            V_interior_new = np.linalg.solve(A, d)\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrix, although not expected here\n            return np.inf\n\n        # Form the full new value function vector\n        V_new = np.concatenate(([v_ref_grid[0]], V_interior_new, [v_ref_grid[-1]]))\n        \n        # --- 2. Policy Improvement ---\n        # Update the policy by minimizing the Hamiltonian\n        \n        # Approximate V'(x) at interior points\n        V_prime_interior = (V_new[2:] - V_new[:-2]) / (2.0 * h)\n        \n        # Compute the unconstrained optimal control\n        u_star_interior = -b / (2.0 * r) * V_prime_interior\n        \n        # Find the closest control in the discrete action space\n        new_policy = np.zeros_like(current_policy)\n        if M > 1 and u_step > 0:\n            indices = np.round((u_star_interior - u_grid[0]) / u_step)\n            indices = np.clip(indices, 0, M - 1).astype(int)\n            new_policy = u_grid[indices]\n            \n        # --- 3. Termination Check ---\n        \n        val_change = np.max(np.abs(V_new - current_V))\n        pol_change = np.max(np.abs(new_policy - current_policy))\n        \n        # Update for the next iteration\n        current_V = V_new\n        current_policy = new_policy\n        \n        if val_change  tol and pol_change  tol:\n            break\n            \n    # --- Step C: Final Error Calculation ---\n    \n    # The converged numerical solution is the last computed value function\n    V_num = current_V\n    error = np.max(np.abs(V_num - v_ref_grid))\n    \n    return error\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}