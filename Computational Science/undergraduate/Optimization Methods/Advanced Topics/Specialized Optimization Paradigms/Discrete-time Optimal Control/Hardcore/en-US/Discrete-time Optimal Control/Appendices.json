{
    "hands_on_practices": [
        {
            "introduction": "What is the fastest way to guide a system to a desired state? This fundamental question lies at the heart of time-optimal control. This exercise challenges you to derive the control strategy for a simple system to reach its target in the minimum number of steps, revealing the structure of a \"bang-bang\" policy where the control pushes the system to its limits. By solving this problem analytically, you will gain deep intuition for how control constraints and the objective function dictate the nature of the optimal solution. ",
            "id": "3121152",
            "problem": "Consider the discrete-time control system with scalar state and control given by $x_{k+1} = x_k + u_k$, where the control satisfies the bound $u_k \\in [-1,1]$ for all integer time indices $k \\geq 0$. The initial state $x_0 \\in \\mathbb{R}$ is given, and the terminal condition is $x_N = 0$ at a free final time $N \\in \\mathbb{N}$. The objective is to minimize the time-to-go, that is, to find a control policy and a corresponding minimal integer $N$ such that $x_N = 0$ and the number of steps $N$ is as small as possible, under the constraint $u_k \\in [-1,1]$. Using the Bellman optimality principle for dynamic programming (DP), derive the structure of the time-optimal control and determine the minimal time $N^{\\star}$ as an explicit function of $x_0$. Identify the switching time index $k_s$ at which the control departs from the bang-bang regime to exactly satisfy the terminal condition. Then verify that your control structure is consistent with necessary conditions analogous to the discrete-time Pontryagin Maximum Principle (PMP). Provide your final answer as the explicit analytic expression for $N^{\\star}$ in terms of $x_0$. No numerical rounding is required, and no physical units are involved.",
            "solution": "The problem of finding the minimal time $N$ to drive the state of a discrete-time system to a target is a classic optimal control problem. We will first validate the problem, then solve it using a direct approach based on the system dynamics and constraints, corroborate the result using the principle of optimality from dynamic programming, and finally verify its consistency with the discrete-time Pontryagin's Maximum Principle (PMP).\n\n### Step 1: Extract Givens\n- System dynamics: $x_{k+1} = x_k + u_k$, where $x_k, u_k \\in \\mathbb{R}$.\n- Control constraint: $u_k \\in [-1, 1]$ for $k \\ge 0$.\n- Initial condition: $x_0 \\in \\mathbb{R}$ is given.\n- Terminal condition: $x_N = 0$.\n- Final time: $N \\in \\mathbb{N}$ is free and to be minimized.\n- Objective: Minimize the time-to-go, $N$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard, well-defined minimum-time optimal control problem for a discrete-time single integrator.\n- **Scientifically Grounded (Critical)**: The problem is a fundamental topic in control theory and is mathematically sound.\n- **Well-Posed**: The problem is well-posed. The state space is $\\mathbb{R}$, the control set is compact, the dynamics are linear, and the objective is clear. A unique, stable, and meaningful solution exists.\n- **Objective (Critical)**: The objective is clear, precise, and free of any subjective elements.\nThe problem possesses no flaws from the checklist. It is a valid, formalizable problem in discrete-time optimal control.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed to the solution.\n\n### Derivation of the Minimal Time $N^{\\star}$\n\nThe system dynamics are given by $x_{k+1} = x_k + u_k$. By iterating this equation from $k=0$ to $k=N-1$, we can express the final state $x_N$ in terms of the initial state $x_0$ and the control sequence $\\{u_k\\}_{k=0}^{N-1}$:\n$$x_N = x_{N-1} + u_{N-1} = (x_{N-2} + u_{N-2}) + u_{N-1} = \\dots = x_0 + \\sum_{k=0}^{N-1} u_k$$\nThe terminal condition is $x_N=0$. Therefore, the sum of the control inputs must satisfy:\n$$\\sum_{k=0}^{N-1} u_k = -x_0$$\nThe control at each step is bounded: $u_k \\in [-1, 1]$, which implies $|u_k| \\le 1$. We can use the triangle inequality to bound the sum of the controls:\n$$\\left| \\sum_{k=0}^{N-1} u_k \\right| \\le \\sum_{k=0}^{N-1} |u_k| \\le \\sum_{k=0}^{N-1} 1 = N$$\nCombining these two results, we have:\n$$|-x_0| \\le N \\implies |x_0| \\le N$$\nSince the number of time steps $N$ must be an integer, the smallest possible integer value for $N$ that satisfies this condition is the smallest integer that is greater than or equal to $|x_0|$. This is the definition of the ceiling function.\nTherefore, the minimal time $N^{\\star}$ is given by:\n$$N^{\\star} = \\lceil |x_0| \\rceil$$\nFor this minimal time to be achievable, we must demonstrate that there exists a control sequence $\\{u_k\\}_{k=0}^{N^\\star-1}$ satisfying $|u_k| \\le 1$ that drives the system from $x_0$ to $x_{N^\\star}=0$.\n\n### Structure of the Optimal Control\n\nThe optimal strategy to minimize time is to change the state as much as possible at each step, in the direction of the origin. This suggests a \"bang-bang\" control strategy, where the control is set to its extremal values, i.e., $u_k = \\pm 1$.\n\nCase 1: $x_0  0$. To drive $x_k$ towards $0$, we must decrease its value. The most effective way to do this is to choose $u_k = -1$.\nLet's apply this control for the first $N^{\\star}-1$ steps, from $k=0$ to $k=N^{\\star}-2$.\n$$u_k^{\\star} = -1 \\quad \\text{for } k=0, 1, \\dots, N^{\\star}-2$$\nThe state at time $k=N^{\\star}-1$ is:\n$$x_{N^{\\star}-1} = x_0 + \\sum_{k=0}^{N^{\\star}-2} (-1) = x_0 - (N^{\\star}-1)$$\nAt the final step $k=N^{\\star}-1$, we must choose $u_{N^{\\star}-1}$ to satisfy the terminal condition $x_{N^{\\star}}=0$.\n$$x_{N^{\\star}} = x_{N^{\\star}-1} + u_{N^{\\star}-1} = 0 \\implies u_{N^{\\star}-1}^{\\star} = -x_{N^{\\star}-1} = -(x_0 - (N^{\\star}-1)) = -(x_0 - \\lceil x_0 \\rceil + 1)$$\nWe must verify that this final control is valid, i.e., $|u_{N^{\\star}-1}^{\\star}| \\le 1$.\nFrom the definition of the ceiling function, we have $\\lceil x_0 \\rceil - 1  x_0 \\le \\lceil x_0 \\rceil$.\nLet $N^{\\star} = \\lceil x_0 \\rceil$. Then $N^{\\star} - 1  x_0 \\le N^{\\star}$.\nThis implies $0  x_0 - (N^{\\star}-1) \\le 1$.\nThus, $x_{N^{\\star}-1} \\in (0, 1]$, and the final control $u_{N^{\\star}-1}^{\\star} = -x_{N^{\\star}-1}$ is in the range $[-1, 0)$. This is a valid control.\nIf $x_0$ is an integer, $x_0=N^\\star$, then $x_{N^\\star-1}=1$ and $u_{N^\\star-1}=-1$. In this case, the control is bang-bang for all steps. If $x_0$ is not an integer, the final control is not on the boundary.\n\nCase 2: $x_0  0$. To drive $x_k$ towards $0$, we must increase its value. The optimal choice is $u_k=1$.\n$$u_k^{\\star} = 1 \\quad \\text{for } k=0, 1, \\dots, N^{\\star}-2$$\nThe state at time $k=N^{\\star}-1$ is:\n$$x_{N^{\\star}-1} = x_0 + \\sum_{k=0}^{N^{\\star}-2} (1) = x_0 + (N^{\\star}-1)$$\nThe final control is $u_{N^{\\star}-1}^{\\star} = -x_{N^{\\star}-1} = -(x_0 + N^{\\star}-1) = -(x_0 + \\lceil -x_0 \\rceil - 1)$.\nLet $y_0 = -x_0  0$. Then $N^\\star = \\lceil y_0 \\rceil$. From Case 1, we know $0  y_0 - (\\lceil y_0 \\rceil - 1) \\le 1$.\nSubstituting back, $0  -x_0 - (N^\\star - 1) \\le 1$, which means $-1 \\le x_0 + N^\\star - 1  0$.\nThus, $x_{N^{\\star}-1} \\in [-1, 0)$, and the final control $u_{N^{\\star}-1}^{\\star} = -x_{N^{\\star}-1}$ is in the range $(0, 1]$. This is a valid control.\n\nGeneral Control Structure: Using the signum function $\\text{sgn}(x)$, we can summarize the control law. For $x_0 \\ne 0$:\n$$u_k^{\\star} = -\\text{sgn}(x_0), \\quad k=0, \\dots, N^{\\star}-2$$\n$$u_{N^{\\star}-1}^{\\star} = -(x_0 - (N^{\\star}-1)\\text{sgn}(x_0))$$\nNote that for $x_0=0$, $N^{\\star}=0$ and no control is applied.\n\n### Switching Time Index $k_s$\nThe problem asks for the time index $k_s$ at which the control departs from the bang-bang regime ($|u_k|=1$). This departure occurs at the final time step, if at all. It happens whenever the control value is not equal to $\\pm 1$. This is the case if and only if $x_0$ is not an integer.\nThe index of this final control action is $k = N^{\\star}-1$. Thus, the switching time index is:\n$$k_s = N^{\\star}-1 = \\lceil |x_0| \\rceil - 1$$\nFor $|x_0| \\le 1$, $N^\\star=1$, so $k_s=0$. The first (and only) control is $u_0 = -x_0$, which is generally not bang-bang. For $|x_0|1$, the trajectory has a bang-bang portion followed by this final adjustment.\n\n### Verification with Discrete Pontryagin's Maximum Principle (PMP)\nThe objective is to minimize $J=N = \\sum_{k=0}^{N-1} 1$. The running cost is $L(x_k, u_k) = 1$.\nThe discrete-time Hamiltonian is $H_k = L_k + p_{k+1} f(x_k, u_k) = 1 + p_{k+1}(x_k+u_k)$.\nThe necessary conditions from PMP are:\n1. State equation: $x_{k+1} = \\frac{\\partial H_k}{\\partial p_{k+1}} = x_k+u_k$. (Satisfied by definition).\n2. Costate equation: $p_k = \\frac{\\partial H_k}{\\partial x_k} = p_{k+1}$. This implies the costate is constant: $p_k = p$ for all $k$.\n3. Minimization of Hamiltonian: The optimal control $u_k^{\\star}$ must minimize $H_k$ over $u_k \\in [-1, 1]$. This is equivalent to minimizing $p u_k$.\n$$u_k^{\\star} = \\begin{cases} -1  \\text{if } p  0 \\\\ 1  \\text{if } p  0 \\\\ \\text{any in } [-1,1]  \\text{if } p = 0 \\end{cases}$$\nThis can be written as $u_k^{\\star} = -\\text{sgn}(p)$ for $p \\neq 0$.\n\nThe derived optimal control consists of a bang-bang part for $k=0, \\dots, N^{\\star}-2$, and a final control $u_{N^{\\star}-1}^{\\star}$ determined by the terminal state constraint $x_{N^{\\star}}=0$.\nThe bang-bang portion of the control, $u_k^* = -\\text{sgn}(x_0)$, is consistent with the PMP necessary condition if we choose a constant costate $p$ such that $\\text{sgn}(p) = \\text{sgn}(x_0)$. Such a costate exists (e.g., $p=\\text{sgn}(x_0)$ for $x_0 \\ne 0$).\nThe final control $u_{N^{\\star}-1}^{\\star}$ is not determined by the minimization of the Hamiltonian because it is uniquely fixed by the state $x_{N^{\\star}-1}$ and the requirement to reach $x_{N^{\\star}}=0$. The PMP framework applies to the selection of \"free\" controls.\nIf $|u_{N^{\\star}-1}^{\\star}|  1$ (which occurs when $x_0$ is not an integer), this is an interior control. In some PMP formulations, an interior control implies that the switching function ($p$ in this case) is zero. If $p=0$, all controls would be singular, which contradicts the bang-bang portion of the trajectory for $N^{\\star} \\ge 2$.\nThis apparent conflict is a known subtlety of the discrete-time PMP. A consistent view is that the PMP conditions determine the control trajectory up to the penultimate step, and the final control is determined by the boundary condition. Our derived control structure is therefore consistent with the necessary conditions from PMP.\n\nThe final result for the minimal time $N^{\\star}$ depends only on the initial state $x_0$.",
            "answer": "$$\n\\boxed{\\lceil |x_0| \\rceil}\n$$"
        },
        {
            "introduction": "In the real world, delays are everywhereâ€”from network latency to fluid transport. This practice demonstrates how to handle such challenges within the powerful Linear Quadratic Regulator (LQR) framework, which is the cornerstone of modern control. You will learn the indispensable technique of state augmentation, which elegantly converts a system with input delay into an equivalent, standard state-space form. Mastering this method allows you to apply the systematic and robust LQR design process to a much wider array of practical engineering systems. ",
            "id": "3121144",
            "problem": "Consider the discrete-time linear system with one-step input delay given by $x_{k+1} = a x_{k} + b u_{k-1}$, where $x_{k} \\in \\mathbb{R}$ is the state and $u_{k} \\in \\mathbb{R}$ is the control input. The delay is $d=1$. You are asked to convert this delayed system into an equivalent non-delayed augmented system and derive the optimal control via the Riccati recursion.\n\nLet the finite-horizon performance index be\n$$J = \\sum_{k=0}^{N-1} \\left( x_{k}^{2} q + u_{k}^{2} r \\right) + x_{N}^{2} q_{f},$$\nwith nonnegative scalars $q$, $r$, and $q_{f}$, and horizon length $N \\in \\mathbb{N}$. Assume the optimal control is computed using the Linear Quadratic Regulator (LQR), derived by the principle of dynamic programming.\n\nTasks:\n1. Introduce the augmented state $z_{k} \\triangleq \\begin{pmatrix} x_{k} \\\\ u_{k-1} \\end{pmatrix}$ and write the non-delayed augmented dynamics in the form $z_{k+1} = \\bar{A} z_{k} + \\bar{B} u_{k}$, identifying $\\bar{A}$ and $\\bar{B}$ in terms of $a$ and $b$.\n2. Starting from the dynamic programming principle, show that the optimal cost-to-go is quadratic in the augmented state, $J_{k}^{\\star}(z_{k}) = z_{k}^{\\top} P_{k} z_{k}$, and derive the Riccati recursion for $P_{k}$ together with the corresponding optimal state-feedback control law $u_{k}^{\\star} = -K_{k} z_{k}$ in terms of $\\bar{A}$, $\\bar{B}$, $Q_{z}$, and $R$, where $Q_{z}$ is the augmented-state penalty matrix consistent with the given performance index.\n3. For the specific system parameters $a = 2$, $b = 1$, $q = 1$, $r = 1$, $q_{f} = 1$, horizon $N = 2$, and initial conditions $x_{0} = 2$, $u_{-1} = 1$, compute the optimal control input $u_{0}^{\\star}$. Your final answer must be a single real number. No rounding is required.",
            "solution": "The problem statement is a well-defined and standard problem in discrete-time optimal control theory. It asks for the derivation and application of the Linear Quadratic Regulator (LQR) for a system with a single-step input delay. The method of state augmentation is a standard technique for handling such delays. The problem is scientifically grounded, self-contained, and all necessary parameters for a unique solution are provided. Therefore, the problem is valid.\n\n### Task 1: Augmented System Formulation\n\nThe given discrete-time linear system with input delay is:\n$$x_{k+1} = a x_{k} + b u_{k-1}$$\nwhere $x_k \\in \\mathbb{R}$ is the state and $u_k \\in \\mathbb{R}$ is the control input at time step $k$. The augmented state vector is defined as $z_{k} \\triangleq \\begin{pmatrix} x_{k} \\\\ u_{k-1} \\end{pmatrix}$.\n\nWe want to find the augmented system dynamics in the form $z_{k+1} = \\bar{A} z_{k} + \\bar{B} u_{k}$. Let's construct the state vector at time $k+1$:\n$$z_{k+1} = \\begin{pmatrix} x_{k+1} \\\\ u_{(k+1)-1} \\end{pmatrix} = \\begin{pmatrix} x_{k+1} \\\\ u_{k} \\end{pmatrix}$$\nThe first component, $x_{k+1}$, is given by the system dynamics: $x_{k+1} = a x_{k} + b u_{k-1}$. In terms of the components of $z_k$, this is $x_{k+1} = a x_{k} + b u_{k-1} + 0 \\cdot u_k$.\nThe second component, $u_k$, can be written as a trivial identity: $u_k = 0 \\cdot x_k + 0 \\cdot u_{k-1} + 1 \\cdot u_k$.\n\nCombining these two equations in matrix form, we have:\n$$z_{k+1} = \\begin{pmatrix} x_{k+1} \\\\ u_{k} \\end{pmatrix} = \\begin{pmatrix} a  b \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} x_{k} \\\\ u_{k-1} \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} u_{k}$$\nThis matches the desired form $z_{k+1} = \\bar{A} z_{k} + \\bar{B} u_{k}$ with the augmented system matrices:\n$$\\bar{A} = \\begin{pmatrix} a  b \\\\ 0  0 \\end{pmatrix}, \\quad \\bar{B} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$\n\n### Task 2: Riccati Recursion Derivation\n\nThe performance index is given by:\n$$J = \\sum_{k=0}^{N-1} \\left( x_{k}^{2} q + u_{k}^{2} r \\right) + x_{N}^{2} q_{f}$$\nWe rewrite this cost function in terms of the augmented state $z_k$.\nThe stage cost is $L(x_k, u_k) = x_k^2 q + u_k^2 r$.\nSince $x_k = \\begin{pmatrix} 1  0 \\end{pmatrix} z_k$, we have $x_k^2 q = \\left(\\begin{pmatrix} 1  0 \\end{pmatrix} z_k\\right)^2 q = z_k^\\top \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} q \\begin{pmatrix} 1  0 \\end{pmatrix} z_k = z_k^\\top \\begin{pmatrix} q  0 \\\\ 0  0 \\end{pmatrix} z_k$.\nThus, the stage cost can be written as $L(z_k, u_k) = z_k^\\top Q_z z_k + u_k^\\top R u_k$, where $Q_z = \\begin{pmatrix} q  0 \\\\ 0  0 \\end{pmatrix}$ and $R = r$.\n\nThe terminal cost is $x_N^2 q_f$. Similarly, this becomes $z_N^\\top P_N z_N$ with $P_N = \\begin{pmatrix} q_f  0 \\\\ 0  0 \\end{pmatrix}$.\nThe augmented LQR problem is to minimize $J = \\sum_{k=0}^{N-1} (z_k^\\top Q_z z_k + u_k^\\top R u_k) + z_N^\\top P_N z_N$ subject to $z_{k+1} = \\bar{A} z_k + \\bar{B} u_k$.\n\nWe use dynamic programming. Let $J_k^\\star(z_k)$ be the optimal cost-to-go from state $z_k$ at time $k$. The Bellman equation is:\n$$J_k^\\star(z_k) = \\min_{u_k} \\left[ z_k^\\top Q_z z_k + u_k^\\top R u_k + J_{k+1}^\\star(z_{k+1}) \\right]$$\nWe hypothesize a quadratic form for the cost-to-go: $J_k^\\star(z_k) = z_k^\\top P_k z_k$, where $P_k$ is a symmetric positive semi-definite matrix. The terminal condition is $J_N^\\star(z_N) = z_N^\\top P_N z_N$, so we have $P_N = \\begin{pmatrix} q_f  0 \\\\ 0  0 \\end{pmatrix}$.\n\nSubstituting into the Bellman equation:\n$$z_k^\\top P_k z_k = \\min_{u_k} \\left[ z_k^\\top Q_z z_k + u_k^\\top R u_k + (\\bar{A} z_k + \\bar{B} u_k)^\\top P_{k+1} (\\bar{A} z_k + \\bar{B} u_k) \\right]$$\nLet's expand the term inside the minimization, which we call $\\mathcal{L}_k(z_k, u_k)$:\n$$\\mathcal{L}_k(z_k, u_k) = z_k^\\top Q_z z_k + u_k^\\top R u_k + z_k^\\top \\bar{A}^\\top P_{k+1} \\bar{A} z_k + 2 u_k^\\top \\bar{B}^\\top P_{k+1} \\bar{A} z_k + u_k^\\top \\bar{B}^\\top P_{k+1} \\bar{B} u_k$$\nTo find the optimal control $u_k^\\star$, we take the gradient of $\\mathcal{L}_k$ with respect to $u_k$ and set it to zero:\n$$\\frac{\\partial \\mathcal{L}_k}{\\partial u_k} = 2 R u_k + 2 \\bar{B}^\\top P_{k+1} \\bar{A} z_k + 2 \\bar{B}^\\top P_{k+1} \\bar{B} u_k = 0$$\nSolving for $u_k$:\n$$(R + \\bar{B}^\\top P_{k+1} \\bar{B}) u_k = - \\bar{B}^\\top P_{k+1} \\bar{A} z_k$$\n$$u_k^\\star = - (R + \\bar{B}^\\top P_{k+1} \\bar{B})^{-1} (\\bar{B}^\\top P_{k+1} \\bar{A}) z_k$$\nThis is a state-feedback control law $u_k^\\star = -K_k z_k$, with the time-varying gain matrix:\n$$K_k = (R + \\bar{B}^\\top P_{k+1} \\bar{B})^{-1} \\bar{B}^\\top P_{k+1} \\bar{A}$$\nSubstituting $u_k^\\star$ back into the Bellman equation yields the Riccati recursion for $P_k$:\n$$P_k = Q_z + \\bar{A}^\\top P_{k+1} \\bar{A} - (\\bar{A}^\\top P_{k+1} \\bar{B}) (R + \\bar{B}^\\top P_{k+1} \\bar{B})^{-1} (\\bar{B}^\\top P_{k+1} \\bar{A})$$\nThis equation is solved backwards in time from the terminal condition $P_N$. An equivalent form is $P_k = Q_z + \\bar{A}^\\top P_{k+1} \\bar{A} - K_k^\\top (R + \\bar{B}^\\top P_{k+1} \\bar{B}) K_k$.\n\n### Task 3: Numerical Computation\n\nThe given parameters are $a = 2$, $b = 1$, $q = 1$, $r = 1$, $q_f = 1$, and $N = 2$.\nThe augmented system matrices are:\n$$\\bar{A} = \\begin{pmatrix} 2  1 \\\\ 0  0 \\end{pmatrix}, \\quad \\bar{B} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$\nThe cost matrices are:\n$$Q_z = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}, \\quad R = 1$$\nWe need to find $u_0^\\star$, which requires computing $K_0$. This requires $P_1$, which in turn is computed from $P_2$.\n\n**Step 1: Compute $P_2$ (at $k=N=2$)**\nThe terminal cost matrix is:\n$$P_2 = P_N = \\begin{pmatrix} q_f  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}$$\n\n**Step 2: Compute $P_1$ (at $k=N-1=1$)**\nWe use the Riccati recursion for $k=1$:\n$$P_1 = Q_z + \\bar{A}^\\top P_2 \\bar{A} - (\\bar{A}^\\top P_2 \\bar{B}) (R + \\bar{B}^\\top P_2 \\bar{B})^{-1} (\\bar{B}^\\top P_2 \\bar{A})$$\nLet's compute the terms:\n$$\\bar{A}^\\top P_2 \\bar{A} = \\begin{pmatrix} 2  0 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 4  2 \\\\ 2  1 \\end{pmatrix}$$\n$$\\bar{B}^\\top P_2 \\bar{B} = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 0$$\n$$\\bar{A}^\\top P_2 \\bar{B} = \\begin{pmatrix} 2  0 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nThe correction term is zero. Therefore:\n$$P_1 = Q_z + \\bar{A}^\\top P_2 \\bar{A} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 4  2 \\\\ 2  1 \\end{pmatrix} = \\begin{pmatrix} 5  2 \\\\ 2  1 \\end{pmatrix}$$\n\n**Step 3: Compute $K_0$ (at $k=0$)**\nThe gain $K_0$ is given by:\n$$K_0 = (R + \\bar{B}^\\top P_1 \\bar{B})^{-1} \\bar{B}^\\top P_1 \\bar{A}$$\nLet's compute the terms:\n$$\\bar{B}^\\top P_1 \\bar{B} = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 5  2 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 1$$\n$$R + \\bar{B}^\\top P_1 \\bar{B} = 1 + 1 = 2$$\n$$\\bar{B}^\\top P_1 \\bar{A} = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 5  2 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 4  2 \\end{pmatrix}$$\nNow, we can compute $K_0$:\n$$K_0 = (2)^{-1} \\begin{pmatrix} 4  2 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 4  2 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\end{pmatrix}$$\n\n**Step 4: Compute the optimal control $u_0^\\star$**\nThe optimal control is $u_0^\\star = -K_0 z_0$.\nThe initial conditions are $x_0 = 2$ and $u_{-1} = 1$. The initial augmented state is:\n$$z_0 = \\begin{pmatrix} x_0 \\\\ u_{-1} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\nFinally, we compute $u_0^\\star$:\n$$u_0^\\star = -K_0 z_0 = -\\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -(2 \\cdot 2 + 1 \\cdot 1) = -(4+1) = -5$$\nThe optimal control input at $k=0$ is $-5$.",
            "answer": "$$\\boxed{-5}$$"
        },
        {
            "introduction": "While the LQR framework is powerful, many real-world problems feature complex, non-convex cost functions that render it inapplicable. This hands-on coding challenge guides you through implementing the dynamic programming algorithm, a universal tool for solving such intricate optimal control problems. By building the solver yourself, you will see how this method systematically explores the state space to find the true optimal policy, cleverly avoiding the pitfalls of short-sighted, greedy approaches that get stuck in local minima. ",
            "id": "3121214",
            "problem": "Consider a deterministic, finite-horizon, discrete-time optimal control problem with scalar state and control. The system evolves according to the dynamics $x_{k+1} = x_k + u_k$ for integer time indices $k = 0, 1, \\dots, N-1$, with given initial state $x_0 \\in \\mathbb{R}$ and control $u_k$ constrained to a fixed, finite grid of admissible values $\\mathcal{U} \\subset \\mathbb{R}$. The stage cost is $c(x_k,u_k) = q \\, x_k^2 + \\ell(u_k)$, where the nonconvex control cost is\n$$\n\\ell(u) = \\min\\{(u - a)^2,\\,(u + b)^2\\},\n$$\nwith parameters $a  0$ and $b  0$. The terminal cost is $\\phi(x_N) = p \\, x_N^2$, with $p  0$. The total cost to be minimized is the sum of stage costs and the terminal cost over the horizon.\n\nStarting only from the definition of the control problem and the principle of optimality for dynamic programming, derive how to compute the optimal cost-to-go and policy. Implement a complete algorithm that:\n- Constructs a uniform grid $\\mathcal{X}$ over the state space sufficient to contain all reachable states from $x_0$ under $\\mathcal{U}$ across the horizon $N$.\n- Computes the optimal policy over the state grid by backward dynamic programming and applies it forward from $x_0$ to produce the realized optimal control sequence $(u_0^{\\star},\\dots,u_{N-1}^{\\star})$ and state trajectory $(x_0,\\dots,x_N)$.\n- Computes the realized optimal total cost $J^{\\star}$ along this trajectory.\n- Computes a greedy myopic policy that, at each time step, chooses a control $u_k$ minimizing $\\ell(u_k)$ alone (ties must be broken by choosing the control with the smallest absolute value, and if still tied, the smallest numerical value), and applies it forward from $x_0$ to produce the greedy total cost $J^{\\mathrm{greedy}}$.\n- Identifies policy switching by counting how many times the chosen active branch of $\\ell(u)$ changes along the realized optimal control sequence. For a control value $u$, define the active branch index as $0$ if $(u-a)^2 \\le (u+b)^2$ and $1$ otherwise. The number of switching events is the count of indices $k$ where the branch index changes from $k-1$ to $k$.\n- Detects a local-minimum trap indicator by the following rule: return a boolean that is true if and only if the greedy policy uses a single branch for all time steps (no switching), the dynamic programming policy has at least one switch, and $J^{\\mathrm{greedy}}  J^{\\star}$.\n\nYour implementation must treat state evolution on the grid by mapping $x_{k+1}$ to the nearest state grid point. If $x_{k+1}$ is outside the grid, it must be clipped to the nearest boundary grid point. You must define a tie-breaking rule for dynamic programming when multiple controls attain the same minimal value: choose the control with the smallest absolute value, and if still tied, the smallest numerical value.\n\nTest Suite. For each of the following parameter sets, run the algorithm and collect the four-tuple of results for that test case: $[J^{\\star},\\,S^{\\star},\\,\\text{is\\_greedy\\_suboptimal},\\,\\text{is\\_trap}]$, where $J^{\\star}$ is the realized optimal total cost as a float, $S^{\\star}$ is the integer number of branch switches along the realized optimal policy, $\\text{is\\_greedy\\_suboptimal}$ is a boolean indicating whether $J^{\\mathrm{greedy}}  J^{\\star}$, and $\\text{is\\_trap}$ is the boolean local-minimum trap indicator defined above. Use a state-grid spacing of $\\Delta x = 0.25$.\n\n- Case 1 (general case): $N = 6$, $q = 1.0$, $p = 5.0$, $a = 1.0$, $b = 1.0$, $x_0 = 2.5$, $\\mathcal{U} = \\{-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0\\}$.\n- Case 2 (boundary horizon): $N = 1$, $q = 1.0$, $p = 2.0$, $a = 1.0$, $b = 1.0$, $x_0 = -0.75$, $\\mathcal{U} = \\{-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0\\}$.\n- Case 3 (asymmetric wells): $N = 5$, $q = 0.5$, $p = 10.0$, $a = 2.0$, $b = 0.5$, $x_0 = -3.0$, $\\mathcal{U} = \\{-3.0,-2.5,-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0,2.5,3.0\\}$.\n- Case 4 (strong terminal penalty): $N = 8$, $q = 0.1$, $p = 20.0$, $a = 1.5$, $b = 1.5$, $x_0 = 3.0$, $\\mathcal{U} = \\{-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0\\}$.\n\nFinal Output Format. Your program should produce a single line of output containing the results for all test cases aggregated as a comma-separated list enclosed in square brackets, where each element itself is the comma-separated four-tuple for a test case enclosed in square brackets. For example:\n$[ [J_1^{\\star},S_1^{\\star},\\text{flag}_{1},\\text{flag}_{1}], [J_2^{\\star},S_2^{\\star},\\text{flag}_{2},\\text{flag}_{2}], \\dots ]$.\nNo physical units or angle units are involved. All booleans must be printed as capitalized Python booleans ($\\text{True}$ or $\\text{False}$). All floats and integers must be printed in standard Python notation.",
            "solution": "The user-provided problem is a valid deterministic, finite-horizon, discrete-time optimal control problem. It is well-posed, scientifically grounded in the principles of optimal control theory, and all parameters, constraints, and objectives are clearly defined. The core challenge lies in the non-convex nature of the control cost function $\\ell(u)$, which makes dynamic programming (DP) a suitable solution method. The solution will be derived from the Principle of Optimality.\n\n**1. Problem Formulation and Principle of Optimality**\n\nThe objective is to minimize the total cost function $J$ given by:\n$$J(x_0, u_0, \\dots, u_{N-1}) = \\sum_{k=0}^{N-1} c(x_k, u_k) + \\phi(x_N)$$\nwhere the stage cost is $c(x_k, u_k) = qx_k^2 + \\ell(u_k)$ with $\\ell(u) = \\min\\{(u - a)^2, (u + b)^2\\}$, and the terminal cost is $\\phi(x_N) = px_N^2$. The system dynamics are $x_{k+1} = x_k + u_k$, with a given initial state $x_0$. The control $u_k$ must be chosen from a finite set of admissible controls $\\mathcal{U}$.\n\nAccording to Bellman's Principle of Optimality, an optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. This principle gives rise to the dynamic programming recursion.\n\nLet $J_k^{\\star}(x)$ be the optimal cost-to-go from state $x$ at time $k$ to the end of the horizon. The recursion begins at the terminal time $N$:\n$$J_N^{\\star}(x) = \\phi(x) = p x^2$$\nFor the preceding time steps, $k = N-1, N-2, \\dots, 0$, the Bellman equation relates the cost-to-go at time $k$ to the cost-to-go at time $k+1$:\n$$J_k^{\\star}(x) = \\min_{u \\in \\mathcal{U}} \\left\\{ c(x, u) + J_{k+1}^{\\star}(x + u) \\right\\}$$\nThe term inside the minimum represents the cost incurred by choosing control $u$ at state $x$ and time $k$, and then proceeding optimally thereafter.\n\n**2. Numerical Solution via Discretization**\n\nSince the state $x$ is continuous, we cannot directly tabulate $J_k^{\\star}(x)$. The problem specifies a numerical approach using state-space discretization.\n\nFirst, we construct a uniform state grid $\\mathcal{X}$ that is guaranteed to contain all possible trajectories. The minimum and maximum reachable states are $x_{\\min} = x_0 + N \\cdot \\min(\\mathcal{U})$ and $x_{\\max} = x_0 + N \\cdot \\max(\\mathcal{U})$, respectively. The grid $\\mathcal{X}$ is constructed over the interval $[x_{\\min}, x_{\\max}]$ with a specified spacing $\\Delta x$. Let the grid points be $\\{x_i\\}$.\n\nThe DP algorithm then proceeds backward in time on this grid:\n\n**Step 1: Backward Pass (Policy and Value Function Computation)**\n- **Initialization ($k=N$):** For each state $x_i \\in \\mathcal{X}$, compute and store the terminal cost: $J_N(x_i) = p x_i^2$.\n- **Backward Iteration ($k=N-1, \\dots, 0$):** For each state $x_i \\in \\mathcal{X}$:\n  - For each admissible control $u_j \\in \\mathcal{U}$, calculate the cost-to-go for that specific choice:\n    1. Compute the next state: $x_{\\text{next}} = x_i + u_j$.\n    2. Map this next state to the grid. As per the problem, this involves clipping $x_{\\text{next}}$ to the grid boundaries and then finding the nearest grid point, let's call it $x'_{\\text{next}}$.\n    3. Retrieve the already computed optimal cost-to-go from this next grid state: $J_{k+1}(x'_{\\text{next}})$.\n    4. The total cost for this $(x_i, u_j)$ pair is $V(u_j) = q x_i^2 + \\ell(u_j) + J_{k+1}(x'_{\\text{next}})$.\n  - Find the minimum cost over all controls: $J_k(x_i) = \\min_{u_j \\in \\mathcal{U}} V(u_j)$.\n  - Identify the set of controls that achieve this minimum. Apply the specified tie-breaking rule (smallest absolute value, then smallest numerical value) to select the unique optimal control $\\mu_k^{\\star}(x_i)$.\n  - Store the optimal cost-to-go $J_k(x_i)$ and the optimal control $\\mu_k^{\\star}(x_i)$.\n\nAfter this pass, we have the optimal policy $\\mu_k^{\\star}(x_i)$ and the optimal value function $J_k(x_i)$ tabulated for all grid states $x_i$ and time steps $k$.\n\n**Step 2: Forward Pass (Trajectory Realization)**\nWith the optimal policy $\\mu_k^{\\star}$ computed, we simulate the system forward from the initial state $x_0$ to find the realized optimal trajectory and calculate the associated costs and metrics.\n\n- Initialize the state trajectory with $x_0^{\\star} = x_0$.\n- For $k=0, \\dots, N-1$:\n  1. Take the current true state $x_k^{\\star}$.\n  2. Map $x_k^{\\star}$ to its nearest grid point, $x'_k = \\text{nearest}(x_k^{\\star}, \\mathcal{X})$.\n  3. Look up the optimal control from the policy table: $u_k^{\\star} = \\mu_k^{\\star}(x'_k)$.\n  4. Evolve the true state: $x_{k+1}^{\\star} = x_k^{\\star} + u_k^{\\star}$.\n  5. Store $u_k^{\\star}$ and $x_{k+1}^{\\star}$.\n\n**3. Calculation of Required Metrics**\n\n- **Optimal Cost ($J^{\\star}$):** Using the realized optimal state trajectory $(x_0^{\\star}, \\dots, x_N^{\\star})$ and control sequence $(u_0^{\\star}, \\dots, u_{N-1}^{\\star})$, the total cost is calculated as:\n  $$J^{\\star} = \\sum_{k=0}^{N-1} (q(x_k^{\\star})^2 + \\ell(u_k^{\\star})) + p(x_N^{\\star})^2$$\n\n- **Greedy Policy Cost ($J^{\\mathrm{greedy}}$):** A myopic greedy policy is one that minimizes the immediate control cost $\\ell(u)$ at every step, ignoring the state cost and future consequences. The greedy control $u^{\\mathrm{greedy}}$ is found by $\\arg\\min_{u \\in \\mathcal{U}} \\ell(u)$, with ties broken as specified. Since $\\ell(u)$ is independent of state and time, this control is constant for all $k$. The greedy trajectory is simulated starting from $x_0$, and $J^{\\mathrm{greedy}}$ is calculated similarly.\n\n- **Policy Switching ($S^{\\star}$):** The control cost $\\ell(u) = \\min\\{(u-a)^2, (u+b)^2\\}$ has two branches. The active branch for a control $u$ is determined by whether $(u-a)^2 \\le (u+b)^2$. This simplifies to checking if $u \\ge (a-b)/2$. We define a branch index function:\n  $$B(u) = \\begin{cases} 0  \\text{if } u \\ge (a-b)/2 \\\\ 1  \\text{if } u  (a-b)/2 \\end{cases}$$\n  We apply this function to the optimal control sequence $u_0^{\\star}, \\dots, u_{N-1}^{\\star}$ to get a sequence of branch indices. The number of switches $S^{\\star}$ is the count of times the branch index changes between consecutive time steps.\n\n- **Trap Indicators:** The boolean flags `is_greedy_suboptimal` ($J^{\\mathrm{greedy}}  J^{\\star}$) and `is_trap` (greedy has no switches, DP has at least one, and greedy is suboptimal) are computed based on these results. The \"trap\" signifies a situation where a simple greedy approach gets stuck in a locally attractive control mode, while the optimal DP policy correctly plans ahead and switches modes to achieve a lower overall cost.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the algorithm for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {'N': 6, 'q': 1.0, 'p': 5.0, 'a': 1.0, 'b': 1.0, 'x0': 2.5, 'U': [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]},\n        {'N': 1, 'q': 1.0, 'p': 2.0, 'a': 1.0, 'b': 1.0, 'x0': -0.75, 'U': [-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0]},\n        {'N': 5, 'q': 0.5, 'p': 10.0, 'a': 2.0, 'b': 0.5, 'x0': -3.0, 'U': [-3.0, -2.5, -2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]},\n        {'N': 8, 'q': 0.1, 'p': 20.0, 'a': 1.5, 'b': 1.5, 'x0': 3.0, 'U': [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]},\n    ]\n    dx = 0.25\n\n    results = []\n    for params in test_cases:\n        result = _solve_case(_dx=dx, **params)\n        results.append(result)\n    \n    # Format the final output string\n    s_results = [f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]\n    print(f\"[{','.join(s_results)}]\")\n\ndef _solve_case(N, q, p, a, b, x0, U, _dx):\n    \"\"\"\n    Solves a single optimal control problem instance.\n    \"\"\"\n\n    # --- Helper Functions ---\n    def l_cost(u, a_param, b_param):\n        return min((u - a_param)**2, (u + b_param)**2)\n\n    def branch_index(u, a_param, b_param):\n        # branch 0 if (u-a)^2 = (u+b)^2, else 1. This simplifies to u = (a-b)/2.\n        return 0 if u = (a_param - b_param) / 2.0 else 1\n\n    def solve_tiebreak(controls):\n        # Sort by |u|, then by u\n        return sorted(controls, key=lambda u_val: (abs(u_val), u_val))[0]\n\n    # --- 1. State Grid Construction ---\n    u_min_val, u_max_val = min(U), max(U)\n    x_min_reach = x0 + N * u_min_val\n    x_max_reach = x0 + N * u_max_val\n    # Use arange; add a small tolerance to include the endpoint\n    X = np.arange(x_min_reach, x_max_reach + _dx / 2.0, _dx)\n    num_states = len(X)\n    x_grid_min_val, x_grid_max_val = X[0], X[-1]\n    \n    def map_to_index(x):\n        clipped_x = np.clip(x, x_grid_min_val, x_grid_max_val)\n        return np.argmin(np.abs(X - clipped_x))\n\n    # --- 2. Dynamic Programming Backward Pass ---\n    J = np.zeros((N + 1, num_states))\n    policy = np.zeros((N, num_states))\n\n    # Terminal cost\n    J[N, :] = p * X**2\n\n    # Iterate backwards from k = N-1 to 0\n    for k in range(N - 1, -1, -1):\n        for i in range(num_states):\n            x = X[i]\n            stage_cost_x = q * x**2\n            \n            costs_for_u = []\n            for u in U:\n                control_cost_l = l_cost(u, a, b)\n                x_next_raw = x + u\n                next_idx = map_to_index(x_next_raw)\n                cost_to_go_next = J[k + 1, next_idx]\n                total_cost_for_u = stage_cost_x + control_cost_l + cost_to_go_next\n                costs_for_u.append(total_cost_for_u)\n                \n            min_cost = min(costs_for_u)\n            J[k, i] = min_cost\n            \n            candidate_us = [u for u, cost in zip(U, costs_for_u) if np.isclose(cost, min_cost)]\n            policy[k, i] = solve_tiebreak(candidate_us)\n\n    # --- 3. Forward Pass (Optimal Trajectory) ---\n    x_traj_opt = np.zeros(N + 1)\n    u_traj_opt = np.zeros(N)\n    x_traj_opt[0] = x0\n\n    for k in range(N):\n        current_x = x_traj_opt[k]\n        current_idx = map_to_index(current_x)\n        u_star_k = policy[k, current_idx]\n        u_traj_opt[k] = u_star_k\n        x_traj_opt[k + 1] = current_x + u_star_k\n    \n    # --- 4. Calculate Optimal Cost (J_star) ---\n    J_star = 0.0\n    for k in range(N):\n        J_star += q * x_traj_opt[k]**2 + l_cost(u_traj_opt[k], a, b)\n    J_star += p * x_traj_opt[N]**2\n\n    # --- 5. Calculate Policy Switches (S_star) ---\n    branch_indices_opt = [branch_index(u, a, b) for u in u_traj_opt]\n    S_star = 0\n    if N  0:\n        for k in range(1, N):\n            if branch_indices_opt[k] != branch_indices_opt[k-1]:\n                S_star += 1\n\n    # --- 6. Greedy Policy Simulation ---\n    l_vals = [l_cost(u, a, b) for u in U]\n    min_l = min(l_vals)\n    candidate_greedy_us = [u for u, l_val in zip(U, l_vals) if np.isclose(l_val, min_l)]\n    u_greedy = solve_tiebreak(candidate_greedy_us)\n\n    x_traj_greedy = np.zeros(N + 1)\n    x_traj_greedy[0] = x0\n    for k in range(N):\n        x_traj_greedy[k + 1] = x_traj_greedy[k] + u_greedy\n    \n    J_greedy = 0.0\n    for k in range(N):\n        J_greedy += q * x_traj_greedy[k]**2 + l_cost(u_greedy, a, b)\n    J_greedy += p * x_traj_greedy[N]**2\n        \n    # --- 7. Final Indicators ---\n    is_greedy_suboptimal = J_greedy  J_star\n    \n    # Greedy policy uses constant control, so it has 0 switches.\n    greedy_switches = 0\n    \n    # A trap occurs if greedy stays on one branch, DP switches, and DP is better.\n    is_trap = (greedy_switches == 0) and (S_star = 1) and is_greedy_suboptimal\n\n    return [J_star, S_star, is_greedy_suboptimal, is_trap]\n\nif __name__ == '__main__':\n    solve()\n\n```"
        }
    ]
}