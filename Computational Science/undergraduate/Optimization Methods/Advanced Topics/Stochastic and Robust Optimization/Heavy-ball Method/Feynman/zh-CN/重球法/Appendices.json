{
    "hands_on_practices": [
        {
            "introduction": "理论是指导，实践是检验。要真正掌握重球法，我们必须亲手实现它，并观察其在不同场景下的行为。这个练习将引导我们回到一个理想化的场景——二次目标函数——来剖析重球法的核心权衡。通过在一个具有不同曲率（特征值）的二次函数上比较重球法和标准梯度下降法，我们将量化动量如何在加速收敛的同时，也可能在某些维度上引发不必要的振荡。",
            "id": "3135479",
            "problem": "要求您实现一个程序，用于在具有等比数列特征值的可分凸二次目标函数上，比较重球动量法和经典梯度下降法。目标函数为 $$f(x)=\\frac{1}{2}\\sum_{i=1}^{d}\\lambda_i x_i^2,$$ 其中 $$\\lambda_i=\\lambda_{\\min}\\,q^{i-1},$$ 维度 $d\\geq 2$ 为整数，基数 $\\lambda_{\\min}>0$ 为正数，公比 $q>1$。该比较必须量化两种效应：在具有较大 $\\lambda_i$ 的坐标上的更快衰减，以及在具有较小 $\\lambda_i$ 的坐标上的振荡。\n\n从二次目标的离散时间最速下降法的基本原理以及源于经典力学类比的重球动量概念出发，推导适用于该二次函数的迭代规则，并确定能在区间 $[\\lambda_{\\min},\\lambda_{\\max}]$（其中 $\\lambda_{\\max}=\\lambda_{\\min}q^{d-1}$）上最小化最坏情况线性收敛率的恒定步长。对于梯度下降法，确定一个单一的步长 $\\alpha_{\\mathrm{GD}}$，以最小化所有坐标上的最坏情况收缩因子。对于重球法，确定 $\\alpha_{\\mathrm{HB}}$ 和 $\\beta_{\\mathrm{HB}}$，以最小化该区间上两步线性递推的最坏情况谱半径。在您的模拟中使用这些参数，但有一个边界条件测试用例例外，其中重球动量参数 $\\beta_{\\mathrm{HB}}$ 被强制设为 $0$，以明确测试无动量极限。\n\n按如下方式实现这两种方法，初始条件为对所有 $i$ 都有 $x_{0,i}=1$，对于重球法有 $x_{-1}=x_0$。每种方法运行 $T$ 次迭代。对于梯度下降法，使用适用于此二次函数的单步迭代。对于重球法，使用适用于此二次函数动量的两步迭代。根据每个坐标的 $\\lambda_i$ 独立处理每个坐标。\n\n定义两个定量指标来捕捉这种权衡关系：\n- 大特征值衰减比：经过 $T$ 次迭代后，计算 $$R_{\\mathrm{large}}:=\\frac{\\sum_{i\\in\\mathcal{I}_{\\mathrm{large}}}\\left|x^{\\mathrm{HB}}_{T,i}\\right|}{\\sum_{i\\in\\mathcal{I}_{\\mathrm{large}}}\\left|x^{\\mathrm{GD}}_{T,i}\\right|},$$ 其中 $\\mathcal{I}_{\\mathrm{large}}$ 是按 $\\lambda_i$ 排序的前半部分坐标（即 $\\lambda_i$ 值最大的那些坐标）的索引集。如果分母在数值上为零，则添加一个小的正项 $10^{-12}$ 以避免除以零。$R_{\\mathrm{large}}  1$ 的值表示相对于梯度下降法，重球动量法在大特征值坐标上的衰减更快。\n- 小特征值振荡率差异：对于按 $\\lambda_i$ 排序的后半部分坐标，计算符号翻转的迭代步数的平均比例，即对于小特征值一半中的每个坐标 $i$，计算满足 $x_{k+1,i}\\cdot x_{k,i}  0$ 的 $k\\in\\{0,1,\\dots,T-1\\}$ 的数量，除以 $T$，然后对这些坐标求平均。对重球法和梯度下降法各计算一次，并定义 $\\Delta_{\\mathrm{osc,small}}:=\\text{fraction}^{\\mathrm{HB}}-\\text{fraction}^{\\mathrm{GD}}$。正值表示相对于梯度下降法，重球动量法在小特征值坐标上表现出更剧烈的振荡。\n\n您的程序必须生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，每个测试用例包含两个浮点数，顺序为 $[R_{\\mathrm{large}},\\Delta_{\\mathrm{osc,small}}]$，并将所有测试用例的结果平铺展开。例如，输出应类似于 `[r1,delta1,r2,delta2,...]`。\n\n使用以下测试套件以确保覆盖典型和边缘行为。在所有情况下，对所有 $i$ 初始化 $x_{0,i}=1$，并为重球法设置 $x_{-1}=x_0$：\n- 情况 $1$ (正常路径)：$d=10$, $\\lambda_{\\min}=1.0$, $q=1.5$, $T=50$，在 $[\\lambda_{\\min},\\lambda_{\\max}]$ 上使用最优重球参数和最优梯度下降步长。\n- 情况 $2$ (差异悬殊的特征值)：$d=12$, $\\lambda_{\\min}=0.1$, $q=3.0$, $T=80$，使用最优重球参数和最优梯度下降步长。\n- 情况 $3$ (近乎均匀的谱)：$d=8$, $\\lambda_{\\min}=1.0$, $q=1.01$, $T=60$，使用最优重球参数和最优梯度下降步长。\n- 情况 $4$ (边界情况，无动量)：$d=10$, $\\lambda_{\\min}=1.0$, $q=2.0$, $T=50$，设置 $\\beta_{\\mathrm{HB}}=0$，并为重球法和梯度下降法使用相同的 $\\alpha$，其值等于在 $[\\lambda_{\\min},\\lambda_{\\max}]$ 上的最优梯度下降步长。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，顺序为 $[R_{\\mathrm{large}}^{(1)},\\Delta_{\\mathrm{osc,small}}^{(1)},R_{\\mathrm{large}}^{(2)},\\Delta_{\\mathrm{osc,small}}^{(2)},R_{\\mathrm{large}}^{(3)},\\Delta_{\\mathrm{osc,small}}^{(3)},R_{\\mathrm{large}}^{(4)},\\Delta_{\\mathrm{osc,small}}^{(4)}]$，其中上标 $^{(j)}$ 表示测试用例 $j\\in\\{1,2,3,4\\}$。所有输出都是无单位的实数。不应打印百分比；所有量都应以小数形式表示。最终答案必须是可执行代码，完全按照指定执行这些计算，并且只打印所述的单行输出。",
            "solution": "用户提供的问题陈述经过严格评估，被认为是有效的。这是一个在数值优化领域内适定的、有科学依据的问题。因此，我们将着手提供一个完整的解决方案。\n\n该问题要求对经典梯度下降（GD）法和重球（HB）动量法在最小化一个特定的凸二次目标函数 $f(x)$ 上的表现进行比较。\n\n目标函数由下式给出：\n$$f(x) = \\frac{1}{2}\\sum_{i=1}^{d}\\lambda_i x_i^2 = \\frac{1}{2} x^T \\Lambda x$$\n其中 $x \\in \\mathbb{R}^d$ 且 $\\Lambda$ 是一个对角矩阵，其元素为 $\\Lambda_{ii} = \\lambda_i$。特征值呈等比数列结构：\n$$\\lambda_i = \\lambda_{\\min}\\,q^{i-1} \\quad \\text{对于 } i=1, \\dots, d$$\n其中给定了常数 $d \\geq 2$, $\\lambda_{\\min}  0$, 和 $q  1$。\n\n该函数的海森矩阵是 $\\nabla^2 f(x) = \\Lambda$，它是一个常对角矩阵。这一结构特性使得 $d$ 维优化问题可以解耦为 $d$ 个独立的一维问题，每个坐标 $x_i$ 对应一个：\n$$\\min_{x_i} f_i(x_i) = \\frac{1}{2}\\lambda_i x_i^2$$\n第 $i$ 个分量的梯度是 $\\nabla f_i(x_i) = \\lambda_i x_i$。我们可以独立地分析两种算法在每个坐标上的收敛性。\n\n### 梯度下降（GD）分析\n梯度下降的更新规则是 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$。对于第 $i$ 个坐标，这变为：\n$$x_{k+1,i} = x_{k,i} - \\alpha_{\\mathrm{GD}} (\\lambda_i x_{k,i}) = (1 - \\alpha_{\\mathrm{GD}} \\lambda_i) x_{k,i}$$\n这是一个一阶线性递推。状态 $x_{k,i}$ 的大小在每一步都乘以一个因子 $|1 - \\alpha_{\\mathrm{GD}} \\lambda_i|$。为确保所有坐标都收敛，对于海森矩阵谱中的所有 $\\lambda_i$，该收缩因子必须小于 1。谱是特征值集合 $\\{\\lambda_1, \\dots, \\lambda_d\\}$，位于区间 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 内，其中 $\\lambda_{\\max} = \\lambda_{\\min}q^{d-1}$。\n\n问题要求找到步长 $\\alpha_{\\mathrm{GD}}$，以最小化在该区间上的最坏情况（即最大）收缩因子：\n$$\\min_{\\alpha} \\max_{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]} |1 - \\alpha \\lambda|$$\n这是优化中的一个经典结果。最优步长是使函数 $g(\\lambda) = 1 - \\alpha \\lambda$ 在区间端点处具有相等大小的步长：$|1 - \\alpha \\lambda_{\\min}| = |-(1 - \\alpha \\lambda_{\\max})|$。这得出：\n$$1 - \\alpha \\lambda_{\\min} = \\alpha \\lambda_{\\max} - 1 \\implies 2 = \\alpha (\\lambda_{\\min} + \\lambda_{\\max})$$\n因此，最优恒定步长是：\n$$\\alpha_{\\mathrm{GD}}^* = \\frac{2}{\\lambda_{\\min} + \\lambda_{\\max}}$$\n相应的最坏情况收缩因子是 $\\rho_{\\mathrm{GD}}^* = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}} = \\frac{\\kappa - 1}{\\kappa + 1}$，其中 $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$ 是海森矩阵的条件数。\n\n### 重球（HB）法分析\n重球法在更新规则中引入了一个动量项：\n$$x_{k+1} = x_k - \\alpha \\nabla f(x_k) + \\beta(x_k - x_{k-1})$$\n对于第 $i$ 个坐标，更新变为一个两步线性递推：\n$$x_{k+1,i} = x_{k,i} - \\alpha_{\\mathrm{HB}}(\\lambda_i x_{k,i}) + \\beta_{\\mathrm{HB}}(x_{k,i} - x_{k-1,i})$$\n$$x_{k+1,i} = (1 - \\alpha_{\\mathrm{HB}}\\lambda_i + \\beta_{\\mathrm{HB}})x_{k,i} - \\beta_{\\mathrm{HB}}x_{k-1,i}$$\n我们可以通过检查其特征多项式来分析此递推的稳定性和收敛性：\n$$z^2 - (1 - \\alpha_{\\mathrm{HB}}\\lambda_i + \\beta_{\\mathrm{HB}})z + \\beta_{\\mathrm{HB}} = 0$$\n坐标 $i$ 的收敛速率由谱半径 $\\rho_{\\mathrm{HB}}(\\lambda_i)$ 决定，即该多项式根的最大模。\n\n问题要求找到参数 $\\alpha_{\\mathrm{HB}}$ 和 $\\beta_{\\mathrm{HB}}$，以最小化在区间 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 上的最坏情况谱半径。这个最小-最大问题的解也是一个标准结果，由 Polyak 建立。最优参数由下式给出：\n$$\\alpha_{\\mathrm{HB}}^* = \\left(\\frac{2}{\\sqrt{\\lambda_{\\max}} + \\sqrt{\\lambda_{\\min}}}\\right)^2 = \\frac{4}{(\\sqrt{\\lambda_{\\max}} + \\sqrt{\\lambda_{\\min}})^2}$$\n$$\\beta_{\\mathrm{HB}}^* = \\left(\\frac{\\sqrt{\\lambda_{\\max}} - \\sqrt{\\lambda_{\\min}}}{\\sqrt{\\lambda_{\\max}} + \\sqrt{\\lambda_{\\min}}}\\right)^2 = \\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right)^2$$\n使用这些参数，与梯度下降法相比，最坏情况收敛因子得到显著改善：$\\rho_{\\mathrm{HB}}^* = \\sqrt{\\beta_{\\mathrm{HB}}^*} = \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}$。\n\n### 模拟与指标\n对于每个测试用例，我们执行以下步骤：\n1.  计算 $\\lambda_{\\max} = \\lambda_{\\min}q^{d-1}$ 以及对于 $i=1, \\dots, d$ 的全套特征值 $\\lambda_i$。\n2.  根据上述公式计算最优参数 $\\alpha_{\\mathrm{GD}}^*$, $\\alpha_{\\mathrm{HB}}^*$, 和 $\\beta_{\\mathrm{HB}}^*$，但情况4除外。\n3.  初始化轨迹：对所有 $i$，$x_{0,i}=1$。对于HB， $x_{-1}=x_0$。\n4.  对两种算法进行 $T$ 次迭代模拟。HB 从 $x_0$ 到 $x_1$ 的第一步使用 $x_{-1}=x_0$：$x_{1,i} = (1 - \\alpha_{\\mathrm{HB}}\\lambda_i + \\beta_{\\mathrm{HB}})x_{0,i} - \\beta_{\\mathrm{HB}}x_{-1,i} = (1 - \\alpha_{\\mathrm{HB}}\\lambda_i)x_{0,i}$。这是一个简单的梯度步。后续步骤使用完整的递推。\n5.  $T$ 次迭代后，我们计算两个指定的指标。由于 $q1$，特征值自然有序，因此“后半部分”是坐标 $i=1, \\dots, d/2$，“前半部分”是坐标 $i=d/2+1, \\dots, d$。\n    -   **大特征值衰减比, $R_{\\mathrm{large}}$**：该指标比较 HB 与 GD 在对应大特征值的最终状态向量分量的大小。小于 1 的值表示 HB 在这些分量上的收敛速度更快。\n        $$R_{\\mathrm{large}}=\\frac{\\sum_{i=d/2+1}^{d}\\left|x^{\\mathrm{HB}}_{T,i}\\right|}{\\sum_{i=d/2+1}^{d}\\left|x^{\\mathrm{GD}}_{T,i}\\right|}$$\n        如果分母为零，则按规定添加一个小数 $10^{-12}$。\n    -   **小特征值振荡率差异, $\\Delta_{\\mathrm{osc,small}}$**：该指标量化了在小特征值分量上振荡行为的差异。\n        $$\\Delta_{\\mathrm{osc,small}} = \\left(\\frac{1}{d/2} \\sum_{i=1}^{d/2} \\frac{1}{T} \\sum_{k=0}^{T-1} \\mathbb{I}(x^{\\mathrm{HB}}_{k+1,i} \\cdot x^{\\mathrm{HB}}_{k,i}  0)\\right) - \\left(\\frac{1}{d/2} \\sum_{i=1}^{d/2} \\frac{1}{T} \\sum_{k=0}^{T-1} \\mathbb{I}(x^{\\mathrm{GD}}_{k+1,i} \\cdot x^{\\mathrm{GD}}_{k,i}  0)\\right)$$\n        其中 $\\mathbb{I}(\\cdot)$ 是指示函数。正值意味着与 GD 相比，HB 方法在这些分量上表现出更多的符号翻转（振荡）。\n\n在情况4中，我们给定 $\\beta_{\\mathrm{HB}}=0$ 和 $\\alpha_{\\mathrm{HB}}=\\alpha_{\\mathrm{GD}}^*$。HB 的更新规则简化为 $x_{k+1,i} = (1 - \\alpha_{\\mathrm{GD}}^*\\lambda_i)x_{k,i}$，这与 GD 的更新规则完全相同。因此，我们预期轨迹将是相同的，从而导致 $R_{\\mathrm{large}}=1$ 和 $\\Delta_{\\mathrm{osc,small}}=0$。这可作为对实现方案的合理性检查。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by simulating and comparing Gradient Descent (GD)\n    and the Heavy-Ball (HB) method on a specified quadratic objective.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1: happy path\n        {'d': 10, 'lambda_min': 1.0, 'q': 1.5, 'T': 50, 'beta_override': None},\n        # Case 2: strongly disparate eigenvalues\n        {'d': 12, 'lambda_min': 0.1, 'q': 3.0, 'T': 80, 'beta_override': None},\n        # Case 3: near-uniform spectrum\n        {'d': 8, 'lambda_min': 1.0, 'q': 1.01, 'T': 60, 'beta_override': None},\n        # Case 4: boundary, no momentum\n        {'d': 10, 'lambda_min': 1.0, 'q': 2.0, 'T': 50, 'beta_override': 0.0},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        d = case['d']\n        lambda_min = case['lambda_min']\n        q = case['q']\n        T = case['T']\n        beta_override = case['beta_override']\n\n        # 1. Define eigenvalues and spectral interval\n        # Using 0-based indexing for arrays: i = 0 to d-1\n        indices = np.arange(d)\n        lambdas = lambda_min * q**indices\n        lambda_max = lambdas[-1]\n\n        # 2. Calculate optimal parameters for GD and HB\n        alpha_gd_opt = 2.0 / (lambda_min + lambda_max)\n        \n        # For Cases 1, 2, 3: use optimal HB parameters\n        if beta_override is None:\n            sqrt_l_min = np.sqrt(lambda_min)\n            sqrt_l_max = np.sqrt(lambda_max)\n            alpha_hb_opt = (2.0 / (sqrt_l_min + sqrt_l_max))**2\n            beta_hb_opt = ((sqrt_l_max - sqrt_l_min) / (sqrt_l_max + sqrt_l_min))**2\n        # For Case 4: use specified parameters\n        else:\n            alpha_hb_opt = alpha_gd_opt\n            beta_hb_opt = beta_override\n\n        # 3. Initialize trajectories\n        x_gd = np.zeros((d, T + 1))\n        x_hb = np.zeros((d, T + 1))\n        \n        x_gd[:, 0] = 1.0\n        x_hb[:, 0] = 1.0\n        \n        #\n        # 4. Run simulations\n        #\n        \n        # Gradient Descent simulation\n        gd_contraction = 1.0 - alpha_gd_opt * lambdas\n        for k in range(T):\n            x_gd[:, k + 1] = gd_contraction * x_gd[:, k]\n            \n        # Heavy-Ball simulation\n        # First step as per x_{-1} = x_0\n        x_hb[:, 1] = (1.0 - alpha_hb_opt * lambdas) * x_hb[:, 0]\n        # Subsequent steps\n        hb_c1 = 1.0 - alpha_hb_opt * lambdas + beta_hb_opt\n        hb_c2 = -beta_hb_opt\n        for k in range(1, T):\n            x_hb[:, k + 1] = hb_c1 * x_hb[:, k] + hb_c2 * x_hb[:, k - 1]\n\n        # 5. Calculate metrics\n        split_idx = d // 2\n\n        # Metric 1: Large-eigenvalue decay ratio (R_large)\n        large_indices = slice(split_idx, d)\n        \n        num = np.sum(np.abs(x_hb[large_indices, T]))\n        den = np.sum(np.abs(x_gd[large_indices, T]))\n        \n        if den == 0.0:\n            den = 1e-12\n        \n        r_large = num / den\n\n        # Metric 2: Small-eigenvalue oscillation rate difference (Delta_osc,small)\n        small_indices = slice(0, split_idx)\n        \n        # Calculate sign flips for trajectory from k=0 to T-1 (T steps)\n        # x[:, 1:T+1] is x_1...x_T\n        # x[:, 0:T] is x_0...x_{T-1}\n        num_coords_small = split_idx\n        \n        sign_flips_hb = np.sum((x_hb[small_indices, 1:T+1] * x_hb[small_indices, 0:T])  0)\n        fraction_hb = sign_flips_hb / (num_coords_small * T)\n        \n        sign_flips_gd = np.sum((x_gd[small_indices, 1:T+1] * x_gd[small_indices, 0:T])  0)\n        fraction_gd = sign_flips_gd / (num_coords_small * T)\n                                 \n        delta_osc_small = fraction_hb - fraction_gd\n\n        results.extend([r_large, delta_osc_small])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在理想的光滑函数上观察到的振荡，在现实世界中常见的非光滑问题（如L1正则化）中会演变成一个更棘手的问题：“Z字形振荡” (zig-zagging)。本练习将我们带入一个带“尖角”的分段线性函数世界，直观地展示标准重球法的动量项如何导致算法在最优解附近反复穿越，从而降低收敛效率。更重要的是，这个练习挑战我们设计并实现一种“各向异性动量”作为补救措施，通过动态调整动量来抑制这种振荡，这让我们初步体验到设计高级优化算法的思路。",
            "id": "3135497",
            "problem": "考虑一个在二维空间中定义的凸分段线性目标函数 $f(\\mathbf{x}) = |x_1| + s\\,|x_2|$，其中 $\\mathbf{x} = (x_1,x_2)$ 且 $s$ 是一个正标量。令 $\\partial f(\\mathbf{x})$ 表示 $f$ 在 $\\mathbf{x}$ 处的次微分。使用如下约定 $$\\mathrm{sign}(u)=\\begin{cases}1,  u  0, \\\\ 0,  u = 0, \\\\ -1,  u  0, \\end{cases}$$ 并且，对于此 $f$，选择次梯度法则 $\\mathbf{g}(\\mathbf{x})=\\big(\\mathrm{sign}(x_1),\\,s\\,\\mathrm{sign}(x_2)\\big)\\in \\partial f(\\mathbf{x})$。\n\n从一个有效的物理-数学基础出发：牛顿第二运动定律以及从势能推导出的保守力的定义。一个单位质量的粒子，在由 $f(\\mathbf{x})$ 给出的势能景观中并受到线性粘性阻尼作用，其运动遵循 $\\ddot{\\mathbf{x}}(t) = -\\nabla f(\\mathbf{x}(t)) - c\\,\\dot{\\mathbf{x}}(t)$，其中 $c0$ 是阻尼系数，$\\dot{\\mathbf{x}}(t)$ 是速度，$\\ddot{\\mathbf{x}}(t)$ 是加速度。通过将 $\\nabla f$ 替换为一个选定的次梯度 $\\mathbf{g}(\\mathbf{x})\\in\\partial f(\\mathbf{x})$，并使用一致的第一性原理时间离散化方法，为此非光滑函数 $f$ 从该定律推导出一个离散时间优化迭代式。解释当动量项在次梯度符号变化时仍然持续存在的情况下，所得到的方法如何在非光滑点（如坐标轴）附近表现出“之字形”震荡。\n\n然后，设计一个使用各向异性动量的修正方案：一个能对次梯度符号一致性做出反应的、按坐标的动量系数。具体来说，令在第 $t$ 次迭代时，按坐标的动量系数为 $$\\beta_i^{(t)}=\\begin{cases}\\beta_{\\mathrm{same}},  \\mathrm{sign}\\big(g_i^{(t)}\\big)=\\mathrm{sign}\\big(g_i^{(t-1)}\\big),\\\\ \\beta_{\\mathrm{flip}},  \\text{otherwise,}\\end{cases}$$ 对于 $i\\in\\{1,2\\}$，其中 $0\\le \\beta_{\\mathrm{flip}} \\le \\beta_{\\mathrm{same}} \\le 1$，且 $g_i^{(t)}$ 是在第 $t$ 步选定的次梯度的第 $i$ 个分量。您的程序必须在上述目标函数 $f(\\mathbf{x})$ 上，使用给定的次梯度法则，同时实现各向同性动量版本（单个标量动量系数）和各向异性动量修正方案（按坐标的动量系数）。\n\n如下定义轨迹 $\\{\\mathbf{x}^{(t)}\\}_{t\\ge 0}$ 的之字形计数。对于每个坐标 $i\\in\\{1,2\\}$，考虑带符号步长序列 $\\Delta x_i^{(t)}=x_i^{(t+1)}-x_i^{(t)}$。从 $t=0$ 扫描此序列至最后一次迭代，跳过任何等于 $0$ 的 $\\Delta x_i^{(t)}$，并在连续非零步长的符号不同时计为一个之字形。总之字形计数是两个按坐标计数的总和。使用此定义来衡量之字形震荡。\n\n您的程序必须通过模拟推导出的离散时间迭代，为以下测试套件生成结果。在所有情况下，初始的“前一迭代点”必须设为与初始迭代点相同，以表示零初始速度。对次梯度使用上述符号法则。\n\n- 测试用例 $\\mathbf{A}$ (正常路径，展示之字形震荡及其修正方案):\n  - 参数: $s=10$, 步长 $\\alpha=0.1$, 各向同性动量系数 $\\beta=0.9$, 迭代次数 $T=200$, 初始点 $\\mathbf{x}^{(0)}=(5,-5)$。\n  - 输出: 两个整数，各向同性动量的之字形计数和使用 $\\beta_{\\mathrm{same}}=0.9$ 及 $\\beta_{\\mathrm{flip}}=0.1$ 的各向异性动量的之字形计数。\n\n- 测试用例 $\\mathbf{B}$ (边界条件，零动量):\n  - 参数: $s=10$, 步长 $\\alpha=0.1$, 各向同性动量系数 $\\beta=0$, 迭代次数 $T=200$, 初始点 $\\mathbf{x}^{(0)}=(5,-5)$。\n  - 输出: 两个整数，各向同性动量的之字形计数和使用 $\\beta_{\\mathrm{same}}=0$ 及 $\\beta_{\\mathrm{flip}}=0$ 的各向异性动量的之字形计数。\n\n- 测试用例 $\\mathbf{C}$ (边缘情况，从最小值点开始):\n  - 参数: $s=10$, 步长 $\\alpha=0.3$, 各向同性动量系数 $\\beta=0.9$, 迭代次数 $T=50$, 初始点 $\\mathbf{x}^{(0)}=(0,0)$。\n  - 输出: 两个浮点数，各向同性动量的最终欧几里得范数 $\\|\\mathbf{x}^{(T)}\\|_2$ 和使用 $\\beta_{\\mathrm{same}}=0.9$ 及 $\\beta_{\\mathrm{flip}}=0.1$ 的各向异性动量的最终欧几里得范数 $\\|\\mathbf{x}^{(T)}\\|_2$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表（例如，“[$\\text{result}_1,\\text{result}_2,\\dots$]”）。结果必须按以下顺序排列：测试用例 $\\mathbf{A}$ 各向同性动量之字形计数，测试用例 $\\mathbf{A}$ 各向异性动量之字形计数，测试用例 $\\mathbf{B}$ 各向同性动量之字形计数，测试用例 $\\mathbf{B}$ 各向异性动量之字形计数，测试用例 $\\mathbf{C}$ 各向同性动量最终范数，测试用例 $\\mathbf{C}$ 各向异性动量最终范数。",
            "solution": "该问题是有效的，因为它在科学上基于优化理论和经典力学，定义明确，提供了所有必要的参数和定义，并以客观、正式的语言表述。我们将首先推导离散时间优化算法，然后解释之字形震荡的机理，最后实现指定的算法以计算所需的结果。\n\n### 1. 从物理原理推导重球法\n\n问题始于一个描述单位质量粒子在势能景观 $f(\\mathbf{x})$ 中受线性粘性阻尼作用下运动的二阶常微分方程 (ODE)：\n$$ \\ddot{\\mathbf{x}}(t) + c\\,\\dot{\\mathbf{x}}(t) + \\nabla f(\\mathbf{x}(t)) = \\mathbf{0} $$\n此处，$\\ddot{\\mathbf{x}}(t)$ 是加速度，$\\dot{\\mathbf{x}}(t)$ 是速度，$c  0$ 是阻尼系数，而 $-\\nabla f(\\mathbf{x})$ 是从势能 $f$ 推导出的保守力。\n\n对于非光滑目标函数 $f(\\mathbf{x}) = |x_1| + s\\,|x_2|$，梯度 $\\nabla f$ 并非处处有定义。我们将其替换为一个有效的次梯度 $\\mathbf{g}(\\mathbf{x}) \\in \\partial f(\\mathbf{x})$，具体规则为 $\\mathbf{g}(\\mathbf{x}) = (\\mathrm{sign}(x_1),\\,s\\,\\mathrm{sign}(x_2))$。连续时间动态系统变为：\n$$ \\ddot{\\mathbf{x}}(t) + c\\,\\dot{\\mathbf{x}}(t) + \\mathbf{g}(\\mathbf{x}(t)) = \\mathbf{0} $$\n为了推导离散时间迭代，我们用步长 $\\Delta t  0$ 将时间离散化，令 $\\mathbf{x}^{(k)} \\approx \\mathbf{x}(k \\Delta t)$。我们对时间 $t_k = k \\Delta t$ 处的导数采用有限差分近似：\n- 加速度（中心差分）：$\\ddot{\\mathbf{x}}(t_k) \\approx \\frac{\\mathbf{x}^{(k+1)} - 2\\mathbf{x}^{(k)} + \\mathbf{x}^{(k-1)}}{(\\Delta t)^2}$\n- 速度（后向差分）：$\\dot{\\mathbf{x}}(t_k) \\approx \\frac{\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}}{\\Delta t}$\n\n将这些近似代入常微分方程得到：\n$$ \\frac{\\mathbf{x}^{(k+1)} - 2\\mathbf{x}^{(k)} + \\mathbf{x}^{(k-1)}}{(\\Delta t)^2} + c \\left( \\frac{\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}}{\\Delta t} \\right) + \\mathbf{g}(\\mathbf{x}^{(k)}) = \\mathbf{0} $$\n我们重排方程以求解下一个迭代点 $\\mathbf{x}^{(k+1)}$：\n$$ \\mathbf{x}^{(k+1)} = 2\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)} - c \\Delta t (\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) - (\\Delta t)^2 \\mathbf{g}(\\mathbf{x}^{(k)}) $$\n$$ \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + (\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) - c \\Delta t (\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) - (\\Delta t)^2 \\mathbf{g}(\\mathbf{x}^{(k)}) $$\n$$ \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - (\\Delta t)^2 \\mathbf{g}(\\mathbf{x}^{(k)}) + (1 - c \\Delta t)(\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) $$\n此方程具有重球法的标准形式：\n$$ \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\alpha \\mathbf{g}^{(k)} + \\beta(\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) $$\n其中 $\\mathbf{g}^{(k)} = \\mathbf{g}(\\mathbf{x}^{(k)})$，步长被确定为 $\\alpha = (\\Delta t)^2$，动量系数为 $\\beta = 1 - c \\Delta t$。$\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}$ 项代表动量。零初始速度的条件通过将“前一”迭代点设为与初始迭代点相同来建模，即 $\\mathbf{x}^{(-1)} = \\mathbf{x}^{(0)}$，这使得初始动量项为零。\n\n### 2. 非光滑优化中的之字形震荡现象\n\n目标函数 $f(\\mathbf{x}) = |x_1| + s\\,|x_2|$ 沿坐标轴有不可微的“折痕”，在这些地方次梯度是不连续的。重球法，由于其各向同性的动量系数 $\\beta$，在穿过这些折痕时容易出现震荡行为，即“之字形”震荡。\n\n考虑一个迭代点 $\\mathbf{x}^{(k)}$ 正在逼近位于 $\\mathbf{0}$ 的最小值点。当它穿过一个坐标轴，例如 $x_2$-轴，其第一个坐标 $x_1^{(k)}$ 的符号会翻转。这导致次梯度的第一个分量 $g_1^{(k)} = \\mathrm{sign}(x_1^{(k)})$ 发生突然的符号变化。更新中基于梯度的部分 $-\\alpha \\mathbf{g}^{(k)}$ 会立即改变方向以修正过冲。\n\n然而，动量项 $\\beta(\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)})$ 保留了先前步骤的惯性。这个累积的动量是在次梯度指向前一个方向时建立的。穿过折痕后，这个动量与新的、修正性的次梯度方向相反。如果动量系数 $\\beta$ 很大（例如 $\\beta=0.9$），动量项可能会主导次梯度项，导致下一个迭代点 $\\mathbf{x}^{(k+1)}$ 再次过冲坐标轴，但方向相反。这个过程重复发生，导致轨迹在折痕上之字形穿梭，从而减慢了收敛速度。\n\n### 3. 作为修正方案的各向异性动量\n\n所提出的各向异性动量方案旨在减轻这种之字形震荡。按坐标的动量系数 $\\beta_i^{(t)}$ 根据次梯度符号的历史进行自适应调整：\n$$ \\beta_i^{(t)}=\\begin{cases}\\beta_{\\mathrm{same}},  \\mathrm{sign}\\big(g_i^{(t)}\\big)=\\mathrm{sign}\\big(g_i^{(t-1)}\\big)\\\\ \\beta_{\\mathrm{flip}},  \\text{otherwise}\\end{cases} $$\n其中 $0 \\le \\beta_{\\mathrm{flip}} \\le \\beta_{\\mathrm{same}} \\le 1$。\n\n当一个坐标 $x_i$ 穿过一个轴时，相应的次梯度分量 $g_i$ 的符号会翻转。该规则检测到这种不一致，$\\mathrm{sign}(g_i^{(t)}) \\neq \\mathrm{sign}(g_i^{(t-1)})$，并通过将其系数设置为较小的值 $\\beta_{\\mathrm{flip}}$ 来大幅减小该特定坐标的动量。这一行为有效地“抑制”了导致过冲的有害动量。通过阻尼问题坐标上的震荡，算法可以在新次梯度的引导下采取更受控的步伐。在次梯度符号一致的区域，系数保持在较高的值 $\\beta_{\\mathrm{same}}$，从而保留了动量的加速效益。更新法则变为：\n$$ \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\alpha \\mathbf{g}^{(k)} + \\mathbf{\\beta}^{(k)} \\odot (\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) $$\n其中 $\\mathbf{\\beta}^{(k)}$ 是一个按坐标的动量系数向量，$\\odot$ 表示逐元素乘法。这种仅在需要时对动量进行有针对性的阻尼，是抑制之字形震荡和改善在非光滑目标函数上收敛性的关键。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n\n    def get_subgradient(x, s):\n        \"\"\"Computes the subgradient for f(x) = |x1| + s*|x2|.\"\"\"\n        return np.array([np.sign(x[0]), s * np.sign(x[1])])\n\n    def count_zigzags(trajectory):\n        \"\"\"\n        Counts zig-zags as per the problem definition.\n        The trajectory is a numpy array of shape (T+1, 2).\n        \"\"\"\n        total_zigzag_count = 0\n        num_points = trajectory.shape[0]\n        if num_points  2:\n            return 0\n        \n        #\n        # For each coordinate\n        for i in range(2):\n            steps = trajectory[1:, i] - trajectory[:-1, i]\n            nonzero_steps = steps[steps != 0]\n\n            if len(nonzero_steps)  2:\n                continue\n\n            coord_zigzag_count = 0\n            last_sign = np.sign(nonzero_steps[0])\n            for j in range(1, len(nonzero_steps)):\n                current_sign = np.sign(nonzero_steps[j])\n                if current_sign != last_sign:\n                    coord_zigzag_count += 1\n                last_sign = current_sign\n            total_zigzag_count += coord_zigzag_count\n        \n        return total_zigzag_count\n\n    def run_simulation(s, alpha, T, x0, mode, beta_iso=0.9, beta_same=0.9, beta_flip=0.1):\n        \"\"\"\n        Runs the optimization simulation for either isotropic or anisotropic momentum.\n        \"\"\"\n        x_curr = np.array(x0, dtype=float)\n        x_prev = np.array(x0, dtype=float)\n        \n        trajectory = [x_curr.copy()]\n        \n        # Initialize previous gradient for anisotropic mode check\n        g_prev = get_subgradient(x_curr, s)\n\n        for _ in range(T):\n            g_curr = get_subgradient(x_curr, s)\n            \n            if mode == 'isotropic':\n                beta = beta_iso\n            elif mode == 'anisotropic':\n                signs_g_curr = np.sign(g_curr)\n                signs_g_prev = np.sign(g_prev)\n                # Element-wise check for sign agreement\n                beta = np.where(signs_g_curr == signs_g_prev, beta_same, beta_flip)\n            else:\n                raise ValueError(\"Invalid mode specified.\")\n\n            momentum_term = beta * (x_curr - x_prev)\n            x_next = x_curr - alpha * g_curr + momentum_term\n            \n            trajectory.append(x_next.copy())\n            \n            # Update state for the next iteration\n            x_prev = x_curr\n            x_curr = x_next\n            g_prev = g_curr\n            \n        return np.array(trajectory)\n\n    results = []\n\n    # Test Case A\n    s_A = 10.0\n    alpha_A = 0.1\n    beta_iso_A = 0.9\n    T_A = 200\n    x0_A = (5.0, -5.0)\n    beta_same_A = 0.9\n    beta_flip_A = 0.1\n    \n    traj_A_iso = run_simulation(s_A, alpha_A, T_A, x0_A, 'isotropic', beta_iso=beta_iso_A)\n    zigzag_A_iso = count_zigzags(traj_A_iso)\n    results.append(zigzag_A_iso)\n    \n    traj_A_aniso = run_simulation(s_A, alpha_A, T_A, x0_A, 'anisotropic', beta_same=beta_same_A, beta_flip=beta_flip_A)\n    zigzag_A_aniso = count_zigzags(traj_A_aniso)\n    results.append(zigzag_A_aniso)\n\n    # Test Case B\n    s_B = 10.0\n    alpha_B = 0.1\n    beta_iso_B = 0.0\n    T_B = 200\n    x0_B = (5.0, -5.0)\n    beta_same_B = 0.0\n    beta_flip_B = 0.0\n\n    traj_B_iso = run_simulation(s_B, alpha_B, T_B, x0_B, 'isotropic', beta_iso=beta_iso_B)\n    zigzag_B_iso = count_zigzags(traj_B_iso)\n    results.append(zigzag_B_iso)\n\n    traj_B_aniso = run_simulation(s_B, alpha_B, T_B, x0_B, 'anisotropic', beta_same=beta_same_B, beta_flip=beta_flip_B)\n    zigzag_B_aniso = count_zigzags(traj_B_aniso)\n    results.append(zigzag_B_aniso)\n\n    # Test Case C\n    s_C = 10.0\n    alpha_C = 0.3\n    beta_iso_C = 0.9\n    T_C = 50\n    x0_C = (0.0, 0.0)\n    beta_same_C = 0.9\n    beta_flip_C = 0.1\n\n    traj_C_iso = run_simulation(s_C, alpha_C, T_C, x0_C, 'isotropic', beta_iso=beta_iso_C)\n    norm_C_iso = np.linalg.norm(traj_C_iso[-1])\n    results.append(norm_C_iso)\n\n    traj_C_aniso = run_simulation(s_C, alpha_C, T_C, x0_C, 'anisotropic', beta_same=beta_same_C, beta_flip=beta_flip_C)\n    norm_C_aniso = np.linalg.norm(traj_C_aniso[-1])\n    results.append(norm_C_aniso)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "除了函数的光滑性，现实中的优化问题还常常面临另一个挑战：梯度噪声。例如，在训练神经网络时，梯度通常是在一小批数据上计算的，这使其成为真实梯度的一个含噪估计。本练习将通过精确的数学推导而非蒙特卡洛模拟，来研究梯度噪声对重球法性能的影响。我们将分析并找出一个临界点，在该点之后，重球法因噪声累积而导致的均方误差（Mean Squared Error, MSE）反而会超过更简单的梯度下降法，从而深刻揭示动量在随机优化环境下的双刃剑效应。",
            "id": "3135505",
            "problem": "你需要比较在一维强凸二次目标函数和加性梯度噪声条件下，重球法和梯度下降法 (GD) 的行为。考虑目标函数 $f(x) = \\tfrac{1}{2} a x^2$，其曲率 $a \\in \\mathbb{R}_{0}$。在迭代 $k \\in \\mathbb{N}$ 时，可用梯度受到独立同分布的高斯噪声的干扰：$\\widehat{\\nabla f}(x_k) = a x_k + \\xi_k$，其中 $\\xi_k \\sim \\mathcal{N}(0,\\sigma^2)$，并且在不同的 $k$ 之间是独立的。假设初始化 $x_0 = x_{\\mathrm{init}}$ 是确定性的。对于重球法，使用标准初始化 $x_{-1} = x_0$（零初始速度）。\n\n- 梯度下降法 (GD) 更新，步长为 $\\alpha \\in \\mathbb{R}_{0}$：\n  $$\n  x_{k+1}^{\\mathrm{GD}} = x_k^{\\mathrm{GD}} - \\alpha\\,\\widehat{\\nabla f}(x_k^{\\mathrm{GD}})\\,.\n  $$\n\n- 重球法 (HB) 更新，步长为 $\\alpha \\in \\mathbb{R}_{0}$，动量为 $\\beta \\in [0,1)$：\n  $$\n  x_{k+1}^{\\mathrm{HB}} = x_k^{\\mathrm{HB}} - \\alpha\\,\\widehat{\\nabla f}(x_k^{\\mathrm{HB}}) + \\beta\\left(x_k^{\\mathrm{HB}} - x_{k-1}^{\\mathrm{HB}}\\right), \\quad x_{-1}^{\\mathrm{HB}} = x_0^{\\mathrm{HB}}\\,.\n  $$\n\n请基于精确期望进行计算，而不是通过蒙特卡洛模拟。令 $\\operatorname{MSE}_k^{\\mathrm{GD}} = \\mathbb{E}\\!\\left[(x_k^{\\mathrm{GD}})^2\\right]$ 和 $\\operatorname{MSE}_k^{\\mathrm{HB}} = \\mathbb{E}\\!\\left[(x_k^{\\mathrm{HB}})^2\\right]$。仅使用基本线性系统推理、高斯随机变量的期望和方差性质以及给定的更新规则，推导出 $x_k^{\\mathrm{GD}}$ 和 $x_k^{\\mathrm{HB}}$ 的均值和二阶矩（或等价地，均值和方差）的递推关系。然后，对于下面的每个测试用例，计算 GD 在均方误差上超过 HB 的最小迭代指数 $k^\\star \\in \\{1,2,\\dots,T_{\\max}\\}$，即满足以下条件的最小 $k \\ge 1$：\n$$\n\\operatorname{MSE}_k^{\\mathrm{GD}} \\le \\operatorname{MSE}_k^{\\mathrm{HB}}\\,.\n$$\n如果在 $1 \\le k \\le T_{\\max}$ 范围内不存在这样的 $k$，则该测试用例输出 -1。\n\n要求和说明：\n- 使用 $x_{-1}^{\\mathrm{HB}} = x_0^{\\mathrm{HB}}$。\n- 假设所选参数能确保两种方法都是均方稳定的（二阶矩不发散）。\n- 所有计算必须通过对期望和协方差的精确递推进行解析计算；不要模拟随机样本。\n- 你的程序的最终输出必须是单行文本，将所有测试用例的答案汇总为一个用方括号括起来的逗号分隔列表。\n\n测试套件（每个测试用例是一个元组 $(a,\\alpha,\\beta,\\sigma,x_0,T_{\\max})$）：\n1. $(1.0, 0.2, 0.4, 0.4, 5.0, 500)$\n2. $(1.0, 0.2, 0.4, 1.2, 5.0, 200)$\n3. $(1.0, 0.2, 0.4, 10^{-20}, 5.0, 200)$\n4. $(1.0, 1.5, 0.8, 0.2, 5.0, 100)$\n\n覆盖范围解读：\n- 第一个用例的构造是这样的：在无噪声极限下，重球法初始收敛更快，但在非零 $\\sigma$ 下，其最终的稳态均方误差比 GD 更大；因此应该存在一个有限的交叉点指数。\n- 第二个用例增大了 $\\sigma$，以突显重球法优势的早期丧失。\n- 第三个用例使用极小的 $\\sigma$，以将任何可能的交叉点延迟到给定范围之外。\n- 第四个用例使用的参数使得重球法即使在确定性情况下，在第一步之后也没有优势，从而突显了 GD 的早期主导地位。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含四个交叉点指数（每个测试用例一个），格式为用方括号括起来的逗号分隔列表，例如 `[k1,k2,k3,k4]`。",
            "solution": "用户的请求是随机优化算法分析领域中一个有效的、适定的问题。我将首先推导梯度下降法 (GD) 和重球法 (HB) 的均方误差 (MSE) 的精确期望递推关系。随后，我将分析这些递推关系，以确定问题中定义的交叉点。\n\n目标函数为 $f(x) = \\frac{1}{2} a x^2$，其唯一最小化点是 $x^*=0$。因此，相对于最小化点的均方误差为 $\\operatorname{MSE}_k = \\mathbb{E}[(x_k - x^*)^2] = \\mathbb{E}[x_k^2]$。带噪声的梯度为 $\\widehat{\\nabla f}(x_k) = a x_k + \\xi_k$，其中 $\\xi_k \\sim \\mathcal{N}(0, \\sigma^2)$ 是独立同分布的噪声项。初始状态 $x_0$ 是确定性的。\n\n### 梯度下降法 (GD) 分析\n\nGD 更新规则为：\n$$\nx_{k+1}^{\\mathrm{GD}} = x_k^{\\mathrm{GD}} - \\alpha \\widehat{\\nabla f}(x_k^{\\mathrm{GD}}) = x_k^{\\mathrm{GD}} - \\alpha (a x_k^{\\mathrm{GD}} + \\xi_k)\n$$\n$$\nx_{k+1}^{\\mathrm{GD}} = (1 - a\\alpha) x_k^{\\mathrm{GD}} - \\alpha \\xi_k\n$$\n令 $\\mu_k^{\\mathrm{GD}} = \\mathbb{E}[x_k^{\\mathrm{GD}}]$ 且 $S_k^{\\mathrm{GD}} = \\operatorname{MSE}_k^{\\mathrm{GD}} = \\mathbb{E}[(x_k^{\\mathrm{GD}})^2]$。\n\n**一阶矩（均值）：**\n对更新规则取期望，并使用 $\\mathbb{E}[\\xi_k] = 0$：\n$$\n\\mu_{k+1}^{\\mathrm{GD}} = \\mathbb{E}[(1 - a\\alpha) x_k^{\\mathrm{GD}} - \\alpha \\xi_k] = (1 - a\\alpha) \\mu_k^{\\mathrm{GD}}\n$$\n由于初始条件 $x_0$ 是确定性的，我们有 $\\mu_0^{\\mathrm{GD}} = x_0$。\n\n**二阶矩 (MSE)：**\n将更新规则平方可得：\n$$\n(x_{k+1}^{\\mathrm{GD}})^2 = (1 - a\\alpha)^2 (x_k^{\\mathrm{GD}})^2 - 2 \\alpha (1 - a\\alpha) x_k^{\\mathrm{GD}} \\xi_k + \\alpha^2 \\xi_k^2\n$$\n取期望时，我们注意到 $x_k^{\\mathrm{GD}}$ 是噪声项 $\\{\\xi_0, \\dots, \\xi_{k-1}\\}$ 的函数，因此与 $\\xi_k$ 独立。这意味着 $\\mathbb{E}[x_k^{\\mathrm{GD}} \\xi_k] = \\mathbb{E}[x_k^{\\mathrm{GD}}] \\mathbb{E}[\\xi_k] = \\mu_k^{\\mathrm{GD}} \\cdot 0 = 0$。我们还有 $\\mathbb{E}[\\xi_k^2] = \\operatorname{Var}(\\xi_k) + (\\mathbb{E}[\\xi_k])^2 = \\sigma^2$。\n$$\nS_{k+1}^{\\mathrm{GD}} = \\mathbb{E}[(x_{k+1}^{\\mathrm{GD}})^2] = (1 - a\\alpha)^2 \\mathbb{E}[(x_k^{\\mathrm{GD}})^2] + \\alpha^2 \\mathbb{E}[\\xi_k^2]\n$$\n$$\nS_{k+1}^{\\mathrm{GD}} = (1 - a\\alpha)^2 S_k^{\\mathrm{GD}} + \\alpha^2 \\sigma^2\n$$\n由于 $x_0$ 是确定性的，初始 MSE 为 $S_0^{\\mathrm{GD}} = \\mathbb{E}[x_0^2] = x_0^2$。\n\n### 重球法 (HB) 分析\n\nHB 更新规则为：\n$$\nx_{k+1}^{\\mathrm{HB}} = x_k^{\\mathrm{HB}} - \\alpha \\widehat{\\nabla f}(x_k^{\\mathrm{HB}}) + \\beta (x_k^{\\mathrm{HB}} - x_{k-1}^{\\mathrm{HB}})\n$$\n$$\nx_{k+1}^{\\mathrm{HB}} = (1 - a\\alpha + \\beta) x_k^{\\mathrm{HB}} - \\beta x_{k-1}^{\\mathrm{HB}} - \\alpha \\xi_k\n$$\n这是一个二阶线性递推关系。我们可以通过将其提升到二维状态空间中的一阶系统来分析其动态。令状态向量为 $z_k = \\begin{pmatrix} x_k^{\\mathrm{HB}} \\\\ x_{k-1}^{\\mathrm{HB}} \\end{pmatrix}$。系统演化如下：\n$$\nz_{k+1} = \\begin{pmatrix} x_{k+1}^{\\mathrm{HB}} \\\\ x_k^{\\mathrm{HB}} \\end{pmatrix} = \\begin{pmatrix} 1 - a\\alpha + \\beta  -\\beta \\\\ 1  0 \\end{pmatrix} z_k - \\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix} \\xi_k\n$$\n我们将其表示为 $z_{k+1} = M z_k - L \\xi_k$。我们关心的是二阶矩矩阵 $C_k = \\mathbb{E}[z_k z_k^T]$ 的演化。所求的 MSE 是该矩阵的左上角元素，即 $S_k^{\\mathrm{HB}} = \\mathbb{E}[(x_k^{\\mathrm{HB}})^2] = [C_k]_{11}$。\n\n**二阶矩矩阵：**\n将状态更新与其自身做外积：\n$$\nz_{k+1} z_{k+1}^T = (M z_k - L \\xi_k)(M z_k - L \\xi_k)^T = M z_k z_k^T M^T - \\xi_k (M z_k L^T + L z_k^T M^T) + \\xi_k^2 L L^T\n$$\n取期望，我们利用 $z_k$ 和 $\\xi_k$ 的独立性，发现交叉项消失，即 $\\mathbb{E}[\\xi_k M z_k L^T] = M \\mathbb{E}[z_k] \\mathbb{E}[\\xi_k] L^T = 0$。这得到了二阶矩矩阵的递推关系，称为离散时间李雅普诺夫方程 (discrete-time Lyapunov equation)：\n$$\nC_{k+1} = M C_k M^T + \\sigma^2 L L^T\n$$\n其中 $L L^T = \\begin{pmatrix} \\alpha^2  0 \\\\ 0  0 \\end{pmatrix}$。初始状态由 $x_0^{\\mathrm{HB}} = x_0$ 和指定条件 $x_{-1}^{\\mathrm{HB}} = x_0$ 确定。由于这些都是确定性的，初始状态向量为 $z_0 = \\begin{pmatrix} x_0 \\\\ x_0 \\end{pmatrix}$。初始二阶矩矩阵为：\n$$\nC_0 = \\mathbb{E}[z_0 z_0^T] = z_0 z_0^T = \\begin{pmatrix} x_0 \\\\ x_0 \\end{pmatrix} \\begin{pmatrix} x_0  x_0 \\end{pmatrix} = \\begin{pmatrix} x_0^2  x_0^2 \\\\ x_0^2  x_0^2 \\end{pmatrix}\n$$\n\n### 交叉点分析\n\n问题要求找到最小的整数 $k^\\star \\ge 1$ 使得 $\\operatorname{MSE}_k^{\\mathrm{GD}} \\le \\operatorname{MSE}_k^{\\mathrm{HB}}$。我们来评估在第一步，$k=1$ 时的这个条件。\n\n**GD 在 k=1 时的 MSE：**\n使用初始条件 $S_0^{\\mathrm{GD}} = x_0^2$ 的递推关系：\n$$\nS_1^{\\mathrm{GD}} = (1 - a\\alpha)^2 S_0^{\\mathrm{GD}} + \\alpha^2 \\sigma^2 = (1 - a\\alpha)^2 x_0^2 + \\alpha^2 \\sigma^2\n$$\n\n**HB 在 k=1 时的 MSE：**\n使用初始条件为 $C_0$ 的矩阵递推关系：\n$$\nC_1 = M C_0 M^T + \\sigma^2 L L^T\n$$\n我们来计算 $M C_0 M^T$ 这一项：\n$$\nM C_0 = \\begin{pmatrix} 1 - a\\alpha + \\beta  -\\beta \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} x_0^2  x_0^2 \\\\ x_0^2  x_0^2 \\end{pmatrix} = \\begin{pmatrix} (1 - a\\alpha)x_0^2  (1 - a\\alpha)x_0^2 \\\\ x_0^2  x_0^2 \\end{pmatrix}\n$$\n$$\nM C_0 M^T = \\begin{pmatrix} (1 - a\\alpha)x_0^2  (1 - a\\alpha)x_0^2 \\\\ x_0^2  x_0^2 \\end{pmatrix} \\begin{pmatrix} 1 - a\\alpha + \\beta  1 \\\\ -\\beta  0 \\end{pmatrix} = \\begin{pmatrix} (1 - a\\alpha)^2 x_0^2  (1 - a\\alpha)x_0^2 \\\\ (1 - a\\alpha)x_0^2  x_0^2 \\end{pmatrix}\n$$\n现在，我们加上噪声的贡献：\n$$\nC_1 = \\begin{pmatrix} (1 - a\\alpha)^2 x_0^2  (1 - a\\alpha)x_0^2 \\\\ (1 - a\\alpha)x_0^2  x_0^2 \\end{pmatrix} + \\begin{pmatrix} \\alpha^2\\sigma^2  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} (1 - a\\alpha)^2 x_0^2 + \\alpha^2 \\sigma^2  (1 - a\\alpha)x_0^2 \\\\ (1 - a\\alpha)x_0^2  x_0^2 \\end{pmatrix}\n$$\nHB 在 $k=1$ 时的 MSE 是左上角元素：\n$$\nS_1^{\\mathrm{HB}} = [C_1]_{11} = (1 - a\\alpha)^2 x_0^2 + \\alpha^2 \\sigma^2\n$$\n\n**结论：**\n通过直接计算，我们发现 $\\operatorname{MSE}_1^{\\mathrm{GD}} = \\operatorname{MSE}_1^{\\mathrm{HB}}$。这个结果是 HB 初始化 $x_{-1}^{\\mathrm{HB}} = x_0^{\\mathrm{HB}}$ 的直接推论，该初始化使得动量项 $\\beta(x_0^{\\mathrm{HB}} - x_{-1}^{\\mathrm{HB}})$ 在第一步中为零。因此，HB 的第一步与 GD 的第一步是完全相同的。\n\n问题要求找到最小的整数 $k^\\star \\in \\{1, 2, \\ldots, T_{\\max}\\}$，使得 $\\operatorname{MSE}_k^{\\mathrm{GD}} \\le \\operatorname{MSE}_k^{\\mathrm{HB}}$。由于我们已经证明了在 $k=1$ 时等式成立，所以在搜索范围的第一步就满足了该条件。因此，对于任何有效的参数集 $(a, \\alpha, \\beta, \\sigma, x_0)$，答案都是 $k^\\star = 1$。这些测试用例虽然被构造成在 $k1$ 时展现出不同的动态特性，但它们都在 $k=1$ 时满足交叉条件。所提供的实现将形式上执行推导出的递推关系。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the smallest iteration k >= 1 where MSE_GD = MSE_HB for given test cases.\n    \"\"\"\n    test_cases = [\n        (1.0, 0.2, 0.4, 0.4, 5.0, 500),\n        (1.0, 0.2, 0.4, 1.2, 5.0, 200),\n        (1.0, 0.2, 0.4, 1e-20, 5.0, 200),\n        (1.0, 1.5, 0.8, 0.2, 5.0, 100)\n    ]\n\n    results = []\n    for a, alpha, beta, sigma, x0, T_max in test_cases:\n        k_star = find_crossover_index(a, alpha, beta, sigma, x0, T_max)\n        results.append(k_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef find_crossover_index(a: float, alpha: float, beta: float, sigma: float, x0: float, T_max: int) -> int:\n    \"\"\"\n    Derives the MSE for GD and HB and finds the first k >= 1 where MSE_GD = MSE_HB.\n\n    Args:\n        a: Curvature of the objective function.\n        alpha: Stepsize for both methods.\n        beta: Momentum parameter for the heavy-ball method.\n        sigma: Standard deviation of the gradient noise.\n        x0: Deterministic initial point.\n        T_max: Maximum number of iterations to check.\n\n    Returns:\n        The smallest iteration index k >= 1 where the crossover occurs, or -1 if not found.\n    \"\"\"\n    # Initialize Mean Squared Error (MSE) for Gradient Descent (GD)\n    # S_k = E[x_k^2]\n    s_gd_k = x0**2\n    \n    # Initialize the 2x2 second-moment matrix C_k = E[z_k z_k^T] for Heavy-Ball (HB),\n    # where z_k = [x_k, x_{k-1}]^T. The initial condition x_{-1}=x_0 makes z_0 deterministic.\n    C_k = np.array([\n        [x0**2, x0**2],\n        [x0**2, x0**2]\n    ], dtype=np.float64)\n\n    # Pre-compute constant matrices for HB update\n    # State transition matrix M for z_{k+1} = M z_k + noise\n    M = np.array([\n        [1 - a * alpha + beta, -beta],\n        [1.0, 0.0]\n    ], dtype=np.float64)\n    \n    # Noise covariance term for the state-space update\n    noise_term_hb = np.array([\n        [alpha**2 * sigma**2, 0.0],\n        [0.0, 0.0]\n    ], dtype=np.float64)\n\n    # Iterate from k=1 to T_max to find the crossover point\n    for k in range(1, T_max + 1):\n        # Update GD MSE for step k\n        s_gd_k_prev = s_gd_k\n        s_gd_k = (1 - a * alpha)**2 * s_gd_k_prev + alpha**2 * sigma**2\n        \n        # Update HB second-moment matrix for step k using the Lyapunov recursion\n        C_k = M @ C_k @ M.T + noise_term_hb\n        s_hb_k = C_k[0, 0]\n\n        # Check for the crossover condition\n        if s_gd_k = s_hb_k:\n            return k\n            \n    return -1\n\nsolve()\n```"
        }
    ]
}