{
    "hands_on_practices": [
        {
            "introduction": "理论知识只有通过实践才能真正内化。本节的第一个练习将带领你深入探索重球法的核心权衡。你将通过编程比较重球法和经典梯度下降法在二次函数上的表现，并亲手验证动量如何在曲率大的方向上加速收敛，同时又可能在曲率小的方向上引发振荡。这个练习旨在通过量化分析，让你对重球法的基本行为特性建立起直观且深刻的理解 。",
            "id": "3135479",
            "problem": "要求您实现一个程序，用于比较重球动量法和经典梯度下降法在一个特征值成等比数列的可分离凸二次目标函数上的表现。该目标函数为 $$f(x)=\\frac{1}{2}\\sum_{i=1}^{d}\\lambda_i x_i^2,$$ 其中，对于整数维度 $$d\\geq 2$$、正基数 $$\\lambda_{\\min}0$$ 和公比 $$q1$$，有 $$\\lambda_i=\\lambda_{\\min}\\,q^{i-1}$$。该比较必须量化两种效应：在较大 $$\\lambda_i$$ 对应的坐标上的更快衰减，以及在较小 $$\\lambda_i$$ 对应的坐标上的振荡。\n\n从二次目标的离散时间最速下降法的基本原理以及源于经典力学类比的重球动量概念出发，推导出适用于该二次函数的迭代规则，并确定恒定的步长，以最小化在区间 $$[\\lambda_{\\min},\\lambda_{\\max}]$$ 上的最坏情况线性收敛率，其中 $$\\lambda_{\\max}=\\lambda_{\\min}q^{d-1}$$。对于梯度下降法，确定一个单一的步长 $$\\alpha_{\\mathrm{GD}}$$，以最小化所有坐标上的最坏情况收缩因子。对于重球法，确定 $$\\alpha_{\\mathrm{HB}}$$ 和 $$\\beta_{\\mathrm{HB}}$$，以最小化此区间上两步线性递推的最坏情况谱半径。在您的模拟中使用这些参数，但有一个边界条件测试用例除外，在该用例中，重球动量参数 $$\\beta_{\\mathrm{HB}}$$ 被强制设为 $$0$$，以明确测试无动量的极限情况。\n\n按如下方式实现这两种方法，初始条件为所有 $$i$$ 都有 $$x_{0,i}=1$$，对于重球法，$$x_{-1}=x_0$$。每种方法运行 $$T$$ 次迭代。对于梯度下降法，使用适用于该二次函数的单步迭代。对于重球法，使用适用于该二次函数的动量的两步迭代。根据每个坐标的 $$\\lambda_i$$ 独立处理它们。\n\n定义两个定量指标来捕捉这种权衡：\n- 大特征值衰减比：经过 $$T$$ 次迭代后，计算 $$R_{\\mathrm{large}}:=\\frac{\\sum_{i\\in\\mathcal{I}_{\\mathrm{large}}}\\left|x^{\\mathrm{HB}}_{T,i}\\right|}{\\sum_{i\\in\\mathcal{I}_{\\mathrm{large}}}\\left|x^{\\mathrm{GD}}_{T,i}\\right|},$$ 其中 $$\\mathcal{I}_{\\mathrm{large}}$$ 是按 $$\\lambda_i$$ 值排名的前一半坐标的索引集（即那些具有最大 $$\\lambda_i$$ 值的坐标）。如果分母在数值上为零，则添加一个小的正常数 $$10^{-12}$$ 以避免除以零。$R_{\\mathrm{large}}  1$ 的值表明，相对于梯度下降法，重球动量法在大特征值坐标上衰减得更快。\n- 小特征值振荡率差异：对于按 $$\\lambda_i$$ 值排名的后一半坐标，计算符号翻转的迭代步数的平均比例，即，对于小的一半中的每个坐标 $$i$$，计算满足 $x_{k+1,i}\\cdot x_{k,i}  0$ 的 $$k\\in\\{0,1,\\dots,T-1\\}$$ 的数量，除以 $$T$$，然后在这些坐标上取平均。对重球法和梯度下降法各计算一次，并定义 $$\\Delta_{\\mathrm{osc,small}}:=\\text{fraction}^{\\mathrm{HB}}-\\text{fraction}^{\\mathrm{GD}}$$。正值表明，相对于梯度下降法，重球动量法在小特征值坐标上表现出更剧烈的振荡。\n\n您的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例包含两个浮点数，顺序为 $$[R_{\\mathrm{large}},\\Delta_{\\mathrm{osc,small}}]$$，并将所有测试用例的结果平铺展开。例如，输出应类似于 $$[r_1,\\delta_1,r_2,\\delta_2,\\dots]$$。\n\n使用以下测试套件以确保覆盖典型和边缘行为。在所有情况下，初始化所有 $$i$$ 的 $$x_{0,i}=1$$，并为重球法设置 $$x_{-1}=x_0$$：\n- 情况 1（正常路径）：$$d=10$$, $$\\lambda_{\\min}=1.0$$, $$q=1.5$$, $$T=50$$，在 $$[\\lambda_{\\min},\\lambda_{\\max}]$$ 上使用最优重球参数和在相同区间上的最优梯度下降步长。\n- 情况 2（强差异特征值）：$$d=12$$, $$\\lambda_{\\min}=0.1$$, $$q=3.0$$, $$T=80$$，使用最优重球参数和最优梯度下降步长。\n- 情况 3（近似均匀谱）：$$d=8$$, $$\\lambda_{\\min}=1.0$$, $$q=1.01$$, $$T=60$$，使用最优重球参数和最优梯度下降步长。\n- 情况 4（边界，无动量）：$$d=10$$, $$\\lambda_{\\min}=1.0$$, $$q=2.0$$, $$T=50$$，设置 $$\\beta_{\\mathrm{HB}}=0$$，并为重球法和梯度下降法使用相同的 $$\\alpha$$，其值等于在 $$[\\lambda_{\\min},\\lambda_{\\max}]$$ 上的最优梯度下降步长。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，顺序为 $$[R_{\\mathrm{large}}^{(1)},\\Delta_{\\mathrm{osc,small}}^{(1)},R_{\\mathrm{large}}^{(2)},\\Delta_{\\mathrm{osc,small}}^{(2)},R_{\\mathrm{large}}^{(3)},\\Delta_{\\mathrm{osc,small}}^{(3)},R_{\\mathrm{large}}^{(4)},\\Delta_{\\mathrm{osc,small}}^{(4)}]$$，其中上标 $$^{(j)}$$ 索引测试用例 $$j\\in\\{1,2,3,4\\}$$。所有输出都是无单位的实数。不应打印百分比；所有量都以小数表示。最终答案必须是可执行代码，完全按照规定执行这些计算，并且只打印所描述的单行输出。",
            "solution": "用户提供的问题陈述经过严格评估，被认为是有效的。这是一个在数值优化领域内定义明确、有科学依据的问题。因此，我们将提供一个完整的解决方案。\n\n该问题要求比较经典梯度下降（GD）法和重球（HB）动量法在最小化一个特定的凸二次目标函数 $f(x)$ 上的表现。\n\n目标函数由下式给出：\n$$f(x) = \\frac{1}{2}\\sum_{i=1}^{d}\\lambda_i x_i^2 = \\frac{1}{2} x^T \\Lambda x$$\n其中 $x \\in \\mathbb{R}^d$，$\\Lambda$ 是一个对角矩阵，其对角线元素为 $\\Lambda_{ii} = \\lambda_i$。特征值按等比数列结构排列：\n$$\\lambda_i = \\lambda_{\\min}\\,q^{i-1} \\quad \\text{for } i=1, \\dots, d$$\n其中给定了常数 $d \\geq 2$、$\\lambda_{\\min} > 0$ 和 $q > 1$。\n\n该函数的 Hessian 矩阵为 $\\nabla^2 f(x) = \\Lambda$，它是一个常对角矩阵。这一结构特性使得 $d$ 维优化问题可以解耦为 $d$ 个独立的一维问题，每个坐标 $x_i$ 对应一个：\n$$\\min_{x_i} f_i(x_i) = \\frac{1}{2}\\lambda_i x_i^2$$\n第 $i$ 个分量的梯度是 $\\nabla f_i(x_i) = \\lambda_i x_i$。我们可以独立分析两种算法在每个坐标上的收敛性。\n\n### 梯度下降（GD）分析\n梯度下降的更新规则是 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$。对于第 $i$ 个坐标，这变为：\n$$x_{k+1,i} = x_{k,i} - \\alpha_{\\mathrm{GD}} (\\lambda_i x_{k,i}) = (1 - \\alpha_{\\mathrm{GD}} \\lambda_i) x_{k,i}$$\n这是一个一阶线性递推。状态 $x_{k,i}$ 的大小在每一步都乘以一个因子 $|1 - \\alpha_{\\mathrm{GD}} \\lambda_i|$。为确保所有坐标都收敛，对于 Hessian 矩阵谱中的所有 $\\lambda_i$，该收缩因子必须小于 $1$。谱是特征值的集合 $\\{\\lambda_1, \\dots, \\lambda_d\\}$，它位于区间 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 内，其中 $\\lambda_{\\max} = \\lambda_{\\min}q^{d-1}$。\n\n问题要求找到步长 $\\alpha_{\\mathrm{GD}}$，以最小化该区间上的最坏情况（即最大）收缩因子：\n$$\\min_{\\alpha} \\max_{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]} |1 - \\alpha \\lambda|$$\n这是一个优化中的经典结果。最优步长是使得函数 $g(\\lambda) = 1 - \\alpha \\lambda$ 在区间端点处具有相等的大小：$|1 - \\alpha \\lambda_{\\min}| = |-(1 - \\alpha \\lambda_{\\max})|$。这得到：\n$$1 - \\alpha \\lambda_{\\min} = \\alpha \\lambda_{\\max} - 1 \\implies 2 = \\alpha (\\lambda_{\\min} + \\lambda_{\\max})$$\n因此，最优的恒定步长是：\n$$\\alpha_{\\mathrm{GD}}^* = \\frac{2}{\\lambda_{\\min} + \\lambda_{\\max}}$$\n相应的最坏情况收缩因子为 $\\rho_{\\mathrm{GD}}^* = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}} = \\frac{\\kappa - 1}{\\kappa + 1}$，其中 $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$ 是 Hessian 矩阵的条件数。\n\n### 重球（HB）法分析\n重球法在更新规则中引入了一个动量项：\n$$x_{k+1} = x_k - \\alpha \\nabla f(x_k) + \\beta(x_k - x_{k-1})$$\n对于第 $i$ 个坐标，更新变为一个两步线性递推：\n$$x_{k+1,i} = x_{k,i} - \\alpha_{\\mathrm{HB}}(\\lambda_i x_{k,i}) + \\beta_{\\mathrm{HB}}(x_{k,i} - x_{k-1,i})$$\n$$x_{k+1,i} = (1 - \\alpha_{\\mathrm{HB}}\\lambda_i + \\beta_{\\mathrm{HB}})x_{k,i} - \\beta_{\\mathrm{HB}}x_{k-1,i}$$\n我们可以通过检查其特征多项式来分析此递推的稳定性和收敛性：\n$$z^2 - (1 - \\alpha_{\\mathrm{HB}}\\lambda_i + \\beta_{\\mathrm{HB}})z + \\beta_{\\mathrm{HB}} = 0$$\n坐标 $i$ 的收敛速率由谱半径 $\\rho_{\\mathrm{HB}}(\\lambda_i)$ 决定，它是该多项式根的最大模。\n\n问题要求找到参数 $\\alpha_{\\mathrm{HB}}$ 和 $\\beta_{\\mathrm{HB}}$，以最小化在区间 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 上的最坏情况谱半径。这个最小-最大问题的解也是一个标准结果，由 Polyak 建立。最优参数由下式给出：\n$$\\alpha_{\\mathrm{HB}}^* = \\left(\\frac{2}{\\sqrt{\\lambda_{\\max}} + \\sqrt{\\lambda_{\\min}}}\\right)^2 = \\frac{4}{(\\sqrt{\\lambda_{\\max}} + \\sqrt{\\lambda_{\\min}})^2}$$\n$$\\beta_{\\mathrm{HB}}^* = \\left(\\frac{\\sqrt{\\lambda_{\\max}} - \\sqrt{\\lambda_{\\min}}}{\\sqrt{\\lambda_{\\max}} + \\sqrt{\\lambda_{\\min}}}\\right)^2 = \\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right)^2$$\n使用这些参数，最坏情况收敛因子相比梯度下降有显著改善：$\\rho_{\\mathrm{HB}}^* = \\sqrt{\\beta_{\\mathrm{HB}}^*} = \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}$。\n\n### 模拟与指标\n对于每个测试用例，我们执行以下步骤：\n1.  计算 $\\lambda_{\\max} = \\lambda_{\\min}q^{d-1}$ 和 $i=1, \\dots, d$ 的全套特征值 $\\lambda_i$。\n2.  根据上述公式计算最优参数 $\\alpha_{\\mathrm{GD}}^*$、$\\alpha_{\\mathrm{HB}}^*$ 和 $\\beta_{\\mathrm{HB}}^*$，但情况 4 除外，按其规定处理。\n3.  初始化轨迹：对所有 $i$，有 $x_{0,i}=1$。对于 HB，有 $x_{-1}=x_0$。\n4.  对两种算法模拟 $T$ 次迭代。从 $x_0$ 到 $x_1$ 的初始 HB 步骤使用 $x_{-1}=x_0$：\n$x_{1,i} = (1 - \\alpha_{\\mathrm{HB}}\\lambda_i + \\beta_{\\mathrm{HB}})x_{0,i} - \\beta_{\\mathrm{HB}}x_{-1,i} = (1 - \\alpha_{\\mathrm{HB}}\\lambda_i)x_{0,i}$。这是一个简单的梯度步骤。后续步骤使用完整的递推关系。\n5.  在 $T$ 次迭代后，我们计算两个指定的指标。由于 $q>1$，特征值是自然排序的，所以“后一半”是坐标 $i=1, \\dots, d/2$，“前一半”是坐标 $i=d/2+1, \\dots, d$。\n    -   **大特征值衰减比, $R_{\\mathrm{large}}$**：该指标比较了 HB 和 GD 最终状态向量中对应于大特征值的分量的大小。小于 1 的值表示 HB 在这些分量上收敛更快。\n        $$R_{\\mathrm{large}}=\\frac{\\sum_{i=d/2+1}^{d}\\left|x^{\\mathrm{HB}}_{T,i}\\right|}{\\sum_{i=d/2+1}^{d}\\left|x^{\\mathrm{GD}}_{T,i}\\right|}$$\n        如果分母为零，则按规定加上一个小值 $10^{-12}$。\n    -   **小特征值振荡率差异, $\\Delta_{\\mathrm{osc,small}}$**：该指标量化了在小特征值分量上振荡行为的差异。\n        $$\\Delta_{\\mathrm{osc,small}} = \\left(\\frac{1}{d/2} \\sum_{i=1}^{d/2} \\frac{1}{T} \\sum_{k=0}^{T-1} \\mathbb{I}(x^{\\mathrm{HB}}_{k+1,i} \\cdot x^{\\mathrm{HB}}_{k,i}  0)\\right) - \\left(\\frac{1}{d/2} \\sum_{i=1}^{d/2} \\frac{1}{T} \\sum_{k=0}^{T-1} \\mathbb{I}(x^{\\mathrm{GD}}_{k+1,i} \\cdot x^{\\mathrm{GD}}_{k,i}  0)\\right)$$\n        其中 $\\mathbb{I}(\\cdot)$ 是指示函数。正值意味着与 GD 相比，HB 方法在这些分量上表现出更多的符号翻转（振荡）。\n\n在情况 4 中，我们给定 $\\beta_{\\mathrm{HB}}=0$ 和 $\\alpha_{\\mathrm{HB}}=\\alpha_{\\mathrm{GD}}^*$。HB 的更新规则简化为 $x_{k+1,i} = (1 - \\alpha_{\\mathrm{GD}}^*\\lambda_i)x_{k,i}$，这与 GD 的更新规则完全相同。因此，我们预期轨迹将是相同的，从而导致 $R_{\\mathrm{large}}=1$ 和 $\\Delta_{\\mathrm{osc,small}}=0$。这可以作为实现的合理性检查。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by simulating and comparing Gradient Descent (GD)\n    and the Heavy-Ball (HB) method on a specified quadratic objective.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1: happy path\n        {'d': 10, 'lambda_min': 1.0, 'q': 1.5, 'T': 50, 'beta_override': None},\n        # Case 2: strongly disparate eigenvalues\n        {'d': 12, 'lambda_min': 0.1, 'q': 3.0, 'T': 80, 'beta_override': None},\n        # Case 3: near-uniform spectrum\n        {'d': 8, 'lambda_min': 1.0, 'q': 1.01, 'T': 60, 'beta_override': None},\n        # Case 4: boundary, no momentum\n        {'d': 10, 'lambda_min': 1.0, 'q': 2.0, 'T': 50, 'beta_override': 0.0},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        d = case['d']\n        lambda_min = case['lambda_min']\n        q = case['q']\n        T = case['T']\n        beta_override = case['beta_override']\n\n        # 1. Define eigenvalues and spectral interval\n        # Using 0-based indexing for arrays: i = 0 to d-1\n        indices = np.arange(d)\n        lambdas = lambda_min * q**indices\n        lambda_max = lambdas[-1]\n\n        # 2. Calculate optimal parameters for GD and HB\n        alpha_gd_opt = 2.0 / (lambda_min + lambda_max)\n        \n        # For Cases 1, 2, 3: use optimal HB parameters\n        if beta_override is None:\n            sqrt_l_min = np.sqrt(lambda_min)\n            sqrt_l_max = np.sqrt(lambda_max)\n            alpha_hb_opt = (2.0 / (sqrt_l_min + sqrt_l_max))**2\n            beta_hb_opt = ((sqrt_l_max - sqrt_l_min) / (sqrt_l_max + sqrt_l_min))**2\n        # For Case 4: use specified parameters\n        else:\n            alpha_hb_opt = alpha_gd_opt\n            beta_hb_opt = beta_override\n\n        # 3. Initialize trajectories\n        x_gd = np.zeros((d, T + 1))\n        x_hb = np.zeros((d, T + 1))\n        \n        x_gd[:, 0] = 1.0\n        x_hb[:, 0] = 1.0\n        \n        #\n        # 4. Run simulations\n        #\n        \n        # Gradient Descent simulation\n        gd_contraction = 1.0 - alpha_gd_opt * lambdas\n        for k in range(T):\n            x_gd[:, k + 1] = gd_contraction * x_gd[:, k]\n            \n        # Heavy-Ball simulation\n        # First step as per x_{-1} = x_0\n        x_hb[:, 1] = (1.0 - alpha_hb_opt * lambdas) * x_hb[:, 0]\n        # Subsequent steps\n        hb_c1 = 1.0 - alpha_hb_opt * lambdas + beta_hb_opt\n        hb_c2 = -beta_hb_opt\n        for k in range(1, T):\n            x_hb[:, k + 1] = hb_c1 * x_hb[:, k] + hb_c2 * x_hb[:, k - 1]\n\n        # 5. Calculate metrics\n        split_idx = d // 2\n\n        # Metric 1: Large-eigenvalue decay ratio (R_large)\n        large_indices = slice(split_idx, d)\n        \n        num = np.sum(np.abs(x_hb[large_indices, T]))\n        den = np.sum(np.abs(x_gd[large_indices, T]))\n        \n        if den == 0.0:\n            den = 1e-12\n        \n        r_large = num / den\n\n        # Metric 2: Small-eigenvalue oscillation rate difference (Delta_osc,small)\n        small_indices = slice(0, split_idx)\n        \n        # Calculate sign flips for trajectory from k=0 to T-1 (T steps)\n        # x[:, 1:T+1] is x_1...x_T\n        # x[:, 0:T] is x_0...x_{T-1}\n        num_coords_small = split_idx\n        \n        sign_flips_hb = np.sum((x_hb[small_indices, 1:T+1] * x_hb[small_indices, 0:T])  0)\n        fraction_hb = sign_flips_hb / (num_coords_small * T)\n        \n        sign_flips_gd = np.sum((x_gd[small_indices, 1:T+1] * x_gd[small_indices, 0:T])  0)\n        fraction_gd = sign_flips_gd / (num_coords_small * T)\n                                 \n        delta_osc_small = fraction_hb - fraction_gd\n\n        results.extend([r_large, delta_osc_small])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "优化问题并非总是光滑的。当目标函数存在“尖点”或“折痕”时，标准重球法的表现会如何呢？这个练习将向你展示，在处理非光滑的凸函数（如绝对值函数之和）时，动量项可能导致迭代点在最优解附近反复“之字形”穿越，从而严重拖慢收敛速度。更进一步，你将被要求设计并实现一种“各向异性动量”策略作为补救措施，这不仅能让你认识到标准方法的局限性，还能锻炼你针对特定问题结构改进算法的能力 。",
            "id": "3135497",
            "problem": "考虑一个在二维空间中定义的凸分段线性目标函数 $$f(\\mathbf{x}) = |x_1| + s\\,|x_2|,$$ 其中 $\\mathbf{x} = (x_1,x_2)$ 且 $s$ 是一个正标量。令 $\\partial f(\\mathbf{x})$ 表示 $f$ 在 $\\mathbf{x}$ 处的次微分。使用如下约定 $$\\mathrm{sign}(u)=\\begin{cases}1,  u > 0 \\\\ 0,  u = 0 \\\\ -1,  u  0 \\end{cases}$$ 并且，对于该函数 $f$，选择次梯度法则 $$\\mathbf{g}(\\mathbf{x})=\\big(\\mathrm{sign}(x_1),\\,s\\,\\mathrm{sign}(x_2)\\big)\\in \\partial f(\\mathbf{x}).$$\n\n从一个有效的物理-数学基础出发：牛顿第二运动定律以及从势能推导出的保守力的定义。在由 $f(\\mathbf{x})$ 给出的势能景观中，一个带有线性粘性阻尼的单位质量粒子遵循 $$\\ddot{\\mathbf{x}}(t) = -\\nabla f(\\mathbf{x}(t)) - c\\,\\dot{\\mathbf{x}}(t),$$ 其中 $c>0$ 是一个阻尼系数，$\\dot{\\mathbf{x}}(t)$ 是速度，$\\ddot{\\mathbf{x}}(t)$ 是加速度。通过将 $\\nabla f$ 替换为选定的次梯度 $\\mathbf{g}(\\mathbf{x})\\in\\partial f(\\mathbf{x})$ 并使用一致的第一性原理时间离散化，为此非光滑函数 $f$ 从该定律推导出一个离散时间优化迭代。解释当动量项在次梯度符号变化时仍然存在的情况下，所得方法如何在非光滑点（例如坐标轴）附近表现出“Z字形振荡”。\n\n然后，设计一种使用各向异性动量的纠正措施：一个对次梯度符号一致性作出反应的逐坐标动量系数。具体来说，令第 $t$ 次迭代中每个坐标的动量系数为 $$\\beta_i^{(t)}=\\begin{cases}\\beta_{\\mathrm{same}},  \\mathrm{sign}\\big(g_i^{(t)}\\big)=\\mathrm{sign}\\big(g_i^{(t-1)}\\big)\\\\ \\beta_{\\mathrm{flip}},  \\text{otherwise,}\\end{cases}$$ 对于 $i\\in\\{1,2\\}$，其中 $0\\le \\beta_{\\mathrm{flip}} \\le \\beta_{\\mathrm{same}}  1$，且 $g_i^{(t)}$ 是第 $t$ 步所选次梯度的第 $i$ 个分量。你的程序必须在上述目标函数 $f(\\mathbf{x})$上，使用给定的次梯度法则，实现各向同性动量版本（单一标量动量系数）和各向异性动量纠正措施（逐坐标动量系数）。\n\n定义轨迹 $\\{\\mathbf{x}^{(t)}\\}_{t\\ge 0}$ 的Z字形振荡计数如下。对于每个坐标 $i\\in\\{1,2\\}$，考虑带符号的步长序列 $$\\Delta x_i^{(t)}=x_i^{(t+1)}-x_i^{(t)}.$$ 从 $t=0$ 扫描此序列至最终迭代，跳过任何等于 $0$ 的 $\\Delta x_i^{(t)}$，并在连续的非零步长符号不同时计为一个Z字形振荡。Z字形振荡总数是两个坐标计数的总和。使用此定义来衡量Z字形振荡。\n\n你的程序必须通过模拟推导出的离散时间迭代来为以下测试套件生成结果。在所有情况下，初始的“前一迭代值”必须设置为等于初始迭代值，以表示零初始速度。对次梯度使用上述符号规则。\n\n- 测试用例 $\\mathbf{A}$ (“理想路径”，展示Z字形振荡和纠正措施)：\n  - 参数：$s=10$，步长 $\\alpha=0.1$，各向同性动量系数 $\\beta=0.9$，迭代次数 $T=200$，初始点 $\\mathbf{x}^{(0)}=(5,-5)$。\n  - 输出：两个整数，分别为使用各向同性动量的Z字形振荡计数和使用 $\\beta_{\\mathrm{same}}=0.9$ 与 $\\beta_{\\mathrm{flip}}=0.1$ 的各向异性动量的Z字形振荡计数。\n\n- 测试用例 $\\mathbf{B}$ (边界条件，零动量)：\n  - 参数：$s=10$，步长 $\\alpha=0.1$，各向同性动量系数 $\\beta=0$，迭代次数 $T=200$，初始点 $\\mathbf{x}^{(0)}=(5,-5)$。\n  - 输出：两个整数，分别为使用各向同性动量的Z字形振荡计数和使用 $\\beta_{\\mathrm{same}}=0$ 与 $\\beta_{\\mathrm{flip}}=0$ 的各向异性动量的Z字形振荡计数。\n\n- 测试用例 $\\mathbf{C}$ (边缘情况，从最小值点开始)：\n  - 参数：$s=10$，步长 $\\alpha=0.3$，各向同性动量系数 $\\beta=0.9$，迭代次数 $T=50$，初始点 $\\mathbf{x}^{(0)}=(0,0)$。\n  - 输出：两个浮点数，分别为使用各向同性动量的最终欧几里得范数 $\\|\\mathbf{x}^{(T)}\\|_2$ 和使用 $\\beta_{\\mathrm{same}}=0.9$ 与 $\\beta_{\\mathrm{flip}}=0.1$ 的各向异性动量的最终欧几里得范数 $\\|\\mathbf{x}^{(T)}\\|_2$。\n\n您的程序应生成单行输出，其中包含以逗号分隔并用方括号括起来的结果列表（例如，“[$\\text{result}_1,\\text{result}_2,\\dots$]”）。结果必须按以下顺序排列：测试用例 $\\mathbf{A}$ 各向同性动量Z字形振荡计数、测试用例 $\\mathbf{A}$ 各向异性动量Z字形振荡计数、测试用例 $\\mathbf{B}$ 各向同性动量Z字形振荡计数、测试用例 $\\mathbf{B}$ 各向异性动量Z字形振荡计数、测试用例 $\\mathbf{C}$ 各向同性动量最终范数、测试用例 $\\mathbf{C}$ 各向异性动量最终范数。",
            "solution": "该问题是有效的，因为它在科学上基于优化理论和经典力学，提供了所有必要的参数和定义，是良定的，并以客观、正式的语言表述。我们将首先推导离散时间优化算法，然后解释Z字形振荡的机制，最后实现指定的算法来计算所需的结果。\n\n### 1. 从物理原理推导重球法\n\n问题始于描述一个单位质量粒子在势能景观 $f(\\mathbf{x})$ 中受线性粘性阻尼影响的运动的二阶常微分方程 (ODE)：\n$$ \\ddot{\\mathbf{x}}(t) + c\\,\\dot{\\mathbf{x}}(t) + \\nabla f(\\mathbf{x}(t)) = \\mathbf{0} $$\n这里，$\\ddot{\\mathbf{x}}(t)$ 是加速度，$\\dot{\\mathbf{x}}(t)$ 是速度，$c > 0$ 是阻尼系数，且 $-\\nabla f(\\mathbf{x})$ 是从势能 $f$ 推导出的保守力。\n\n对于非光滑目标函数 $f(\\mathbf{x}) = |x_1| + s\\,|x_2|$，梯度 $\\nabla f$ 并非处处有定义。我们根据法则 $\\mathbf{g}(\\mathbf{x}) = (\\mathrm{sign}(x_1),\\,s\\,\\mathrm{sign}(x_2))$，用一个有效的次梯度 $\\mathbf{g}(\\mathbf{x}) \\in \\partial f(\\mathbf{x})$ 替换它。连续时间动力学变为：\n$$ \\ddot{\\mathbf{x}}(t) + c\\,\\dot{\\mathbf{x}}(t) + \\mathbf{g}(\\mathbf{x}(t)) = \\mathbf{0} $$\n为推导离散时间迭代，我们用步长 $\\Delta t > 0$ 将时间离散化，令 $\\mathbf{x}^{(k)} \\approx \\mathbf{x}(k \\Delta t)$。我们对时间 $t_k = k \\Delta t$ 处的导数采用有限差分近似：\n- 加速度（中心差分）：$\\ddot{\\mathbf{x}}(t_k) \\approx \\frac{\\mathbf{x}^{(k+1)} - 2\\mathbf{x}^{(k)} + \\mathbf{x}^{(k-1)}}{(\\Delta t)^2}$\n- 速度（后向差分）：$\\dot{\\mathbf{x}}(t_k) \\approx \\frac{\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}}{\\Delta t}$\n\n将这些近似值代入ODE得到：\n$$ \\frac{\\mathbf{x}^{(k+1)} - 2\\mathbf{x}^{(k)} + \\mathbf{x}^{(k-1)}}{(\\Delta t)^2} + c \\left( \\frac{\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}}{\\Delta t} \\right) + \\mathbf{g}(\\mathbf{x}^{(k)}) = \\mathbf{0} $$\n我们重新整理方程以解出下一个迭代值 $\\mathbf{x}^{(k+1)}$：\n$$ \\mathbf{x}^{(k+1)} = 2\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)} - c \\Delta t (\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) - (\\Delta t)^2 \\mathbf{g}(\\mathbf{x}^{(k)}) $$\n$$ \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + (\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) - c \\Delta t (\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) - (\\Delta t)^2 \\mathbf{g}(\\mathbf{x}^{(k)}) $$\n$$ \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - (\\Delta t)^2 \\mathbf{g}(\\mathbf{x}^{(k)}) + (1 - c \\Delta t)(\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) $$\n该方程具有重球法的规范形式：\n$$ \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\alpha \\mathbf{g}^{(k)} + \\beta(\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) $$\n其中 $\\mathbf{g}^{(k)} = \\mathbf{g}(\\mathbf{x}^{(k)})$，步长被确定为 $\\alpha = (\\Delta t)^2$，动量系数为 $\\beta = 1 - c \\Delta t$。项 $\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}$ 代表动量。零初始速度的初始条件通过将“前一”迭代值设置为等于初始迭代值 $\\mathbf{x}^{(-1)} = \\mathbf{x}^{(0)}$ 来建模，这使得初始动量项为零。\n\n### 2. 非光滑优化中的Z字形振荡现象\n\n目标函数 $f(\\mathbf{x}) = |x_1| + s\\,|x_2|$ 沿着坐标轴有不可微的“折痕”，在这些地方次梯度是不连续的。重球法，由于其各向同性的动量系数 $\\beta$，在穿过这些折痕时易于产生振荡行为，即“Z字形振荡”。\n\n考虑一个迭代值 $\\mathbf{x}^{(k)}$ 正在接近位于 $\\mathbf{0}$ 的最小值点。当它穿过一个轴，例如 $x_2$ 轴时，其第一个坐标 $x_1^{(k)}$ 的符号会翻转。这导致次梯度的第一个分量 $g_1^{(k)} = \\mathrm{sign}(x_1^{(k)})$ 发生突然的符号变化。更新中基于梯度的部分 $-\\alpha \\mathbf{g}^{(k)}$ 会立即改变方向以纠正过冲。\n\n然而，动量项 $\\beta(\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)})$ 保留了前面步骤的惯性。这个累积的动量是在次梯度指向先前方向时建立的。穿过折痕后，这个动量与新的、修正性的次梯度方向相反。如果动量系数 $\\beta$ 很大（例如 $\\beta=0.9$），动量项可能会主导次梯度项，导致下一个迭代值 $\\mathbf{x}^{(k+1)}$ 再次过冲坐标轴，但方向相反。这个过程重复进行，导致跨越折痕的Z字形振荡轨迹，从而减慢了收敛速度。\n\n### 3. 作为纠正措施的各向异性动量\n\n所提出的各向异性动量方案旨在减轻这种Z字形振荡。逐坐标的动量系数 $\\beta_i^{(t)}$ 根据次梯度符号的历史进行调整：\n$$ \\beta_i^{(t)}=\\begin{cases}\\beta_{\\mathrm{same}},  \\mathrm{sign}\\big(g_i^{(t)}\\big)=\\mathrm{sign}\\big(g_i^{(t-1)}\\big)\\\\ \\beta_{\\mathrm{flip}},  \\text{otherwise}\\end{cases} $$\n其中 $0 \\le \\beta_{\\mathrm{flip}} \\le \\beta_{\\mathrm{same}}  1$。\n\n当一个坐标 $x_i$ 穿过一个轴时，相应的次梯度分量 $g_i$ 的符号会翻转。该规则检测到这种不一致，$\\mathrm{sign}(g_i^{(t)}) \\neq \\mathrm{sign}(g_i^{(t-1)})$，并通过将其系数设置为较小的值 $\\beta_{\\mathrm{flip}}$ 来大幅减少该特定坐标的动量。这一操作有效地“抑制”了导致过冲的有害动量。通过在有问题的坐标上阻尼振荡，算法可以在新次梯度的引导下采取更受控的步骤。在次梯度符号一致的区域，系数保持在较高的值 $\\beta_{\\mathrm{same}}$，保留了动量的加速好处。更新法则变为：\n$$ \\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\alpha \\mathbf{g}^{(k)} + \\mathbf{\\beta}^{(k)} \\odot (\\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}) $$\n其中 $\\mathbf{\\beta}^{(k)}$ 是逐坐标动量系数的向量，$\\odot$ 表示逐元素乘法。这种仅在需要时对动量进行定向阻尼是抑制Z字形振荡和改善非光滑目标函数收敛性的关键。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n\n    def get_subgradient(x, s):\n        \"\"\"Computes the subgradient for f(x) = |x1| + s*|x2|.\"\"\"\n        return np.array([np.sign(x[0]), s * np.sign(x[1])])\n\n    def count_zigzags(trajectory):\n        \"\"\"\n        Counts zig-zags as per the problem definition.\n        The trajectory is a numpy array of shape (T+1, 2).\n        \"\"\"\n        total_zigzag_count = 0\n        num_points = trajectory.shape[0]\n        if num_points  2:\n            return 0\n        \n        #\n        # For each coordinate\n        for i in range(2):\n            steps = trajectory[1:, i] - trajectory[:-1, i]\n            nonzero_steps = steps[steps != 0]\n\n            if len(nonzero_steps)  2:\n                continue\n\n            coord_zigzag_count = 0\n            last_sign = np.sign(nonzero_steps[0])\n            for j in range(1, len(nonzero_steps)):\n                current_sign = np.sign(nonzero_steps[j])\n                if current_sign != last_sign:\n                    coord_zigzag_count += 1\n                last_sign = current_sign\n            total_zigzag_count += coord_zigzag_count\n        \n        return total_zigzag_count\n\n    def run_simulation(s, alpha, T, x0, mode, beta_iso=0.9, beta_same=0.9, beta_flip=0.1):\n        \"\"\"\n        Runs the optimization simulation for either isotropic or anisotropic momentum.\n        \"\"\"\n        x_curr = np.array(x0, dtype=float)\n        x_prev = np.array(x0, dtype=float)\n        \n        trajectory = [x_curr.copy()]\n        \n        # Initialize previous gradient for anisotropic mode check\n        g_prev = get_subgradient(x_curr, s)\n\n        for _ in range(T):\n            g_curr = get_subgradient(x_curr, s)\n            \n            if mode == 'isotropic':\n                beta = beta_iso\n            elif mode == 'anisotropic':\n                signs_g_curr = np.sign(g_curr)\n                signs_g_prev = np.sign(g_prev)\n                # Element-wise check for sign agreement\n                beta = np.where(signs_g_curr == signs_g_prev, beta_same, beta_flip)\n            else:\n                raise ValueError(\"Invalid mode specified.\")\n\n            momentum_term = beta * (x_curr - x_prev)\n            x_next = x_curr - alpha * g_curr + momentum_term\n            \n            trajectory.append(x_next.copy())\n            \n            # Update state for the next iteration\n            x_prev = x_curr\n            x_curr = x_next\n            g_prev = g_curr\n            \n        return np.array(trajectory)\n\n    results = []\n\n    # Test Case A\n    s_A = 10.0\n    alpha_A = 0.1\n    beta_iso_A = 0.9\n    T_A = 200\n    x0_A = (5.0, -5.0)\n    beta_same_A = 0.9\n    beta_flip_A = 0.1\n    \n    traj_A_iso = run_simulation(s_A, alpha_A, T_A, x0_A, 'isotropic', beta_iso=beta_iso_A)\n    zigzag_A_iso = count_zigzags(traj_A_iso)\n    results.append(zigzag_A_iso)\n    \n    traj_A_aniso = run_simulation(s_A, alpha_A, T_A, x0_A, 'anisotropic', beta_same=beta_same_A, beta_flip=beta_flip_A)\n    zigzag_A_aniso = count_zigzags(traj_A_aniso)\n    results.append(zigzag_A_aniso)\n\n    # Test Case B\n    s_B = 10.0\n    alpha_B = 0.1\n    beta_iso_B = 0.0\n    T_B = 200\n    x0_B = (5.0, -5.0)\n    beta_same_B = 0.0\n    beta_flip_B = 0.0\n\n    traj_B_iso = run_simulation(s_B, alpha_B, T_B, x0_B, 'isotropic', beta_iso=beta_iso_B)\n    zigzag_B_iso = count_zigzags(traj_B_iso)\n    results.append(zigzag_B_iso)\n\n    traj_B_aniso = run_simulation(s_B, alpha_B, T_B, x0_B, 'anisotropic', beta_same=beta_same_B, beta_flip=beta_flip_B)\n    zigzag_B_aniso = count_zigzags(traj_B_aniso)\n    results.append(zigzag_B_aniso)\n\n    # Test Case C\n    s_C = 10.0\n    alpha_C = 0.3\n    beta_iso_C = 0.9\n    T_C = 50\n    x0_C = (0.0, 0.0)\n    beta_same_C = 0.9\n    beta_flip_C = 0.1\n\n    traj_C_iso = run_simulation(s_C, alpha_C, T_C, x0_C, 'isotropic', beta_iso=beta_iso_C)\n    norm_C_iso = np.linalg.norm(traj_C_iso[-1])\n    results.append(norm_C_iso)\n\n    traj_C_aniso = run_simulation(s_C, alpha_C, T_C, x0_C, 'anisotropic', beta_same=beta_same_C, beta_flip=beta_flip_C)\n    norm_C_aniso = np.linalg.norm(traj_C_aniso[-1])\n    results.append(norm_C_aniso)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在机器学习的许多应用中，我们只能获得带有噪声的梯度估计。动量在这样的随机环境下表现如何？这个练习将引导你从理论上精确分析梯度噪声对重球法和梯度下降法的影响。通过推导并计算均方误差（MSE）的演化，你将发现一个关键现象：虽然动量在确定性环境中能加速收敛，但它可能会放大梯度噪声，导致其最终的收敛误差高于无动量的梯度下降法。这个练习揭示了重球法在随机优化中的一个重要局限性，为理解更先进的优化算法（如Adam）提供了背景 。",
            "id": "3135505",
            "problem": "您需要比较重球法和梯度下降 (GD) 在加性梯度噪声下，于一维强凸二次目标函数上的行为。考虑目标函数 $f(x) = \\tfrac{1}{2} a x^2$，其曲率 $a \\in \\mathbb{R}_{0}$。在第 $k \\in \\mathbb{N}$ 次迭代中，可用梯度被独立同分布的高斯噪声所污染：$\\widehat{\\nabla f}(x_k) = a x_k + \\xi_k$，其中 $\\xi_k \\sim \\mathcal{N}(0,\\sigma^2)$，并且在不同 $k$ 之间相互独立。假设初始化 $x_0 = x_{\\mathrm{init}}$ 是确定性的。对于重球法，使用标准初始化 $x_{-1} = x_0$（零初始速度）。\n\n- 梯度下降 (GD) 更新，步长为 $\\alpha \\in \\mathbb{R}_{0}$：\n  $$\n  x_{k+1}^{\\mathrm{GD}} \\;=\\; x_k^{\\mathrm{GD}} \\;-\\; \\alpha\\,\\widehat{\\nabla f}(x_k^{\\mathrm{GD}})\\,.\n  $$\n\n- 重球法 (HB) 更新，步长为 $\\alpha \\in \\mathbb{R}_{0}$，动量为 $\\beta \\in [0,1)$：\n  $$\n  x_{k+1}^{\\mathrm{HB}} \\;=\\; x_k^{\\mathrm{HB}} \\;-\\; \\alpha\\,\\widehat{\\nabla f}(x_k^{\\mathrm{HB}}) \\;+\\; \\beta\\left(x_k^{\\mathrm{HB}} - x_{k-1}^{\\mathrm{HB}}\\right), \\quad x_{-1}^{\\mathrm{HB}} = x_0^{\\mathrm{HB}}\\,.\n  $$\n\n在精确期望下进行计算，而不是通过蒙特卡洛方法。令 $\\operatorname{MSE}_k^{\\mathrm{GD}} = \\mathbb{E}\\!\\left[(x_k^{\\mathrm{GD}})^2\\right]$ 和 $\\operatorname{MSE}_k^{\\mathrm{HB}} = \\mathbb{E}\\!\\left[(x_k^{\\mathrm{HB}})^2\\right]$。仅使用基本线性系统推理、高斯随机变量的期望和方差性质以及给定的更新规则，推导出 $x_k^{\\mathrm{GD}}$ 和 $x_k^{\\mathrm{HB}}$ 的均值和二阶矩（或等价地，均值和方差）的递推关系。然后，对于下方的每个测试用例，计算 GD 的均方误差首次优于 HB 的最小迭代索引 $k^\\star \\in \\{1,2,\\dots,T_{\\max}\\}$，即满足以下条件的最小 $k \\ge 1$：\n$$\n\\operatorname{MSE}_k^{\\mathrm{GD}} \\;\\le\\; \\operatorname{MSE}_k^{\\mathrm{HB}}\\,.\n$$\n如果在 $1 \\le k \\le T_{\\max}$ 内不存在这样的 $k$，则该测试用例输出 $-1$。\n\n要求与说明：\n- 使用 $x_{-1}^{\\mathrm{HB}} = x_0^{\\mathrm{HB}}$。\n- 假设所选参数能确保两种方法都是均方稳定的（二阶矩不发散）。\n- 所有计算必须通过关于期望和协方差的精确递推关系进行解析计算；不要模拟随机样本。\n- 您的程序的最终输出必须是单行文本，其中包含所有测试用例的答案，形式为用方括号括起来的逗号分隔列表。\n\n测试套件（每个测试用例是一个元组 $(a,\\alpha,\\beta,\\sigma,x_0,T_{\\max})$）：\n1. $(1.0,\\;0.2,\\;0.4,\\;0.4,\\;5.0,\\;500)$\n2. $(1.0,\\;0.2,\\;0.4,\\;1.2,\\;5.0,\\;200)$\n3. $(1.0,\\;0.2,\\;0.4,\\;10^{-20},\\;5.0,\\;200)$\n4. $(1.0,\\;1.5,\\;0.8,\\;0.2,\\;5.0,\\;100)$\n\n测试覆盖范围说明：\n- 第一个用例的构造旨在说明，在无噪声极限情况下，重球法初始收敛更快，但在非零 $\\sigma$ 下，其最终的稳态均方误差比 GD 更大；应该存在一个有限的交叉索引。\n- 第二个用例增大了 $\\sigma$ 以突显重球法优势的提早丧失。\n- 第三个用例使用极小的 $\\sigma$ 以将任何可能的交叉点延迟到给定范围之外。\n- 第四个用例使用的参数使得重球法即使在确定性情况下，在第一步之后也不具备优势，从而突显 GD 的早期主导地位。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含四个交叉索引（每个测试用例一个），格式为用方括号括起来的逗号分隔列表，例如 $\\texttt{[k1,k2,k3,k4]}$。",
            "solution": "用户的请求是随机优化算法分析中一个有效的、良定的问题。我将首先推导梯度下降 (GD) 和重球法 (HB) 的均方误差 (MSE) 在精确期望下的递推关系。随后，我将分析这些递推关系，以确定问题中定义的交叉点。\n\n目标函数是 $f(x) = \\frac{1}{2} a x^2$，其唯一最小值点为 $x^*=0$。因此，相对于最小值点的均方误差为 $\\operatorname{MSE}_k = \\mathbb{E}[(x_k - x^*)^2] = \\mathbb{E}[x_k^2]$。带噪声的梯度为 $\\widehat{\\nabla f}(x_k) = a x_k + \\xi_k$，其中 $\\xi_k \\sim \\mathcal{N}(0, \\sigma^2)$ 是独立同分布的噪声项。初始状态 $x_0$ 是确定性的。\n\n### 梯度下降 (GD) 分析\n\nGD 的更新规则是：\n$$\nx_{k+1}^{\\mathrm{GD}} = x_k^{\\mathrm{GD}} - \\alpha \\widehat{\\nabla f}(x_k^{\\mathrm{GD}}) = x_k^{\\mathrm{GD}} - \\alpha (a x_k^{\\mathrm{GD}} + \\xi_k)\n$$\n$$\nx_{k+1}^{\\mathrm{GD}} = (1 - a\\alpha) x_k^{\\mathrm{GD}} - \\alpha \\xi_k\n$$\n令 $\\mu_k^{\\mathrm{GD}} = \\mathbb{E}[x_k^{\\mathrm{GD}}]$ 且 $S_k^{\\mathrm{GD}} = \\operatorname{MSE}_k^{\\mathrm{GD}} = \\mathbb{E}[(x_k^{\\mathrm{GD}})^2]$。\n\n**一阶矩（均值）：**\n对更新规则取期望，并使用 $\\mathbb{E}[\\xi_k] = 0$：\n$$\n\\mu_{k+1}^{\\mathrm{GD}} = \\mathbb{E}[(1 - a\\alpha) x_k^{\\mathrm{GD}} - \\alpha \\xi_k] = (1 - a\\alpha) \\mu_k^{\\mathrm{GD}}\n$$\n由于初始条件 $x_0$ 是确定性的，我们有 $\\mu_0^{\\mathrm{GD}} = x_0$。\n\n**二阶矩（均方误差）：**\n将更新规则平方得到：\n$$\n(x_{k+1}^{\\mathrm{GD}})^2 = (1 - a\\alpha)^2 (x_k^{\\mathrm{GD}})^2 - 2 \\alpha (1 - a\\alpha) x_k^{\\mathrm{GD}} \\xi_k + \\alpha^2 \\xi_k^2\n$$\n取期望时，我们注意到 $x_k^{\\mathrm{GD}}$ 是噪声项 $\\{\\xi_0, \\dots, \\xi_{k-1}\\}$ 的函数，因此与 $\\xi_k$ 独立。这意味着 $\\mathbb{E}[x_k^{\\mathrm{GD}} \\xi_k] = \\mathbb{E}[x_k^{\\mathrm{GD}}] \\mathbb{E}[\\xi_k] = \\mu_k^{\\mathrm{GD}} \\cdot 0 = 0$。我们还有 $\\mathbb{E}[\\xi_k^2] = \\operatorname{Var}(\\xi_k) + (\\mathbb{E}[\\xi_k])^2 = \\sigma^2$。\n$$\nS_{k+1}^{\\mathrm{GD}} = \\mathbb{E}[(x_{k+1}^{\\mathrm{GD}})^2] = (1 - a\\alpha)^2 \\mathbb{E}[(x_k^{\\mathrm{GD}})^2] + \\alpha^2 \\mathbb{E}[\\xi_k^2]\n$$\n$$\nS_{k+1}^{\\mathrm{GD}} = (1 - a\\alpha)^2 S_k^{\\mathrm{GD}} + \\alpha^2 \\sigma^2\n$$\n由于 $x_0$ 是确定性的，初始均方误差为 $S_0^{\\mathrm{GD}} = \\mathbb{E}[x_0^2] = x_0^2$。\n\n### 重球法 (HB) 分析\n\nHB 的更新规则是：\n$$\nx_{k+1}^{\\mathrm{HB}} = x_k^{\\mathrm{HB}} - \\alpha \\widehat{\\nabla f}(x_k^{\\mathrm{HB}}) + \\beta (x_k^{\\mathrm{HB}} - x_{k-1}^{\\mathrm{HB}})\n$$\n$$\nx_{k+1}^{\\mathrm{HB}} = (1 - a\\alpha + \\beta) x_k^{\\mathrm{HB}} - \\beta x_{k-1}^{\\mathrm{HB}} - \\alpha \\xi_k\n$$\n这是一个二阶线性递推关系。我们可以通过将其提升到二维状态空间中的一阶系统来分析其动态。令状态向量为 $z_k = \\begin{pmatrix} x_k^{\\mathrm{HB}} \\\\ x_{k-1}^{\\mathrm{HB}} \\end{pmatrix}$。系统演化如下：\n$$\nz_{k+1} = \\begin{pmatrix} x_{k+1}^{\\mathrm{HB}} \\\\ x_k^{\\mathrm{HB}} \\end{pmatrix} = \\begin{pmatrix} 1 - a\\alpha + \\beta  -\\beta \\\\ 1  0 \\end{pmatrix} z_k - \\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix} \\xi_k\n$$\n我们将其表示为 $z_{k+1} = M z_k - L \\xi_k$。我们关心的是二阶矩矩阵 $C_k = \\mathbb{E}[z_k z_k^T]$ 的演化。所需的均方误差是该矩阵的左上角元素，$S_k^{\\mathrm{HB}} = \\mathbb{E}[(x_k^{\\mathrm{HB}})^2] = [C_k]_{11}$。\n\n**二阶矩矩阵：**\n将状态更新与其自身做外积：\n$$\nz_{k+1} z_{k+1}^T = (M z_k - L \\xi_k)(M z_k - L \\xi_k)^T = M z_k z_k^T M^T - \\xi_k (M z_k L^T + L z_k^T M^T) + \\xi_k^2 L L^T\n$$\n取期望时，我们利用 $z_k$ 和 $\\xi_k$ 的独立性，发现交叉项为零，即 $\\mathbb{E}[\\xi_k M z_k L^T] = M \\mathbb{E}[z_k] \\mathbb{E}[\\xi_k] L^T = 0$。这产生二阶矩矩阵的递推关系，即所谓的离散时间李雅普诺夫方程：\n$$\nC_{k+1} = M C_k M^T + \\sigma^2 L L^T\n$$\n其中 $L L^T = \\begin{pmatrix} \\alpha^2  0 \\\\ 0  0 \\end{pmatrix}$。初始状态由 $x_0^{\\mathrm{HB}} = x_0$ 和指定条件 $x_{-1}^{\\mathrm{HB}} = x_0$ 确定。由于这些都是确定性的，初始状态向量为 $z_0 = \\begin{pmatrix} x_0 \\\\ x_0 \\end{pmatrix}$。初始二阶矩矩阵是：\n$$\nC_0 = \\mathbb{E}[z_0 z_0^T] = z_0 z_0^T = \\begin{pmatrix} x_0 \\\\ x_0 \\end{pmatrix} \\begin{pmatrix} x_0  x_0 \\end{pmatrix} = \\begin{pmatrix} x_0^2  x_0^2 \\\\ x_0^2  x_0^2 \\end{pmatrix}\n$$\n\n### 交叉点分析\n\n问题要求找到满足 $\\operatorname{MSE}_k^{\\mathrm{GD}} \\le \\operatorname{MSE}_k^{\\mathrm{HB}}$ 的最小整数 $k^\\star \\ge 1$。让我们在第一步，$k=1$ 时评估此条件。\n\n**GD 在 $k=1$ 时的均方误差：**\n使用初始条件 $S_0^{\\mathrm{GD}} = x_0^2$ 的递推关系：\n$$\nS_1^{\\mathrm{GD}} = (1 - a\\alpha)^2 S_0^{\\mathrm{GD}} + \\alpha^2 \\sigma^2 = (1 - a\\alpha)^2 x_0^2 + \\alpha^2 \\sigma^2\n$$\n\n**HB 在 $k=1$ 时的均方误差：**\n使用初始条件为 $C_0$ 的矩阵递推关系：\n$$\nC_1 = M C_0 M^T + \\sigma^2 L L^T\n$$\n我们来计算 $M C_0 M^T$ 项：\n$$\nM C_0 = \\begin{pmatrix} 1 - a\\alpha + \\beta  -\\beta \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} x_0^2  x_0^2 \\\\ x_0^2  x_0^2 \\end{pmatrix} = \\begin{pmatrix} (1 - a\\alpha)x_0^2  (1 - a\\alpha)x_0^2 \\\\ x_0^2  x_0^2 \\end{pmatrix}\n$$\n$$\nM C_0 M^T = \\begin{pmatrix} (1 - a\\alpha)x_0^2  (1 - a\\alpha)x_0^2 \\\\ x_0^2  x_0^2 \\end{pmatrix} \\begin{pmatrix} 1 - a\\alpha + \\beta  1 \\\\ -\\beta  0 \\end{pmatrix} = \\begin{pmatrix} (1 - a\\alpha)^2 x_0^2  (1 - a\\alpha)x_0^2 \\\\ (1 - a\\alpha)x_0^2  x_0^2 \\end{pmatrix}\n$$\n现在，我们加上噪声的贡献：\n$$\nC_1 = \\begin{pmatrix} (1 - a\\alpha)^2 x_0^2  (1 - a\\alpha)x_0^2 \\\\ (1 - a\\alpha)x_0^2  x_0^2 \\end{pmatrix} + \\begin{pmatrix} \\alpha^2\\sigma^2  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} (1 - a\\alpha)^2 x_0^2 + \\alpha^2 \\sigma^2  (1 - a\\alpha)x_0^2 \\\\ (1 - a\\alpha)x_0^2  x_0^2 \\end{pmatrix}\n$$\nHB 在 $k=1$ 时的均方误差是左上角元素：\n$$\nS_1^{\\mathrm{HB}} = [C_1]_{11} = (1 - a\\alpha)^2 x_0^2 + \\alpha^2 \\sigma^2\n$$\n\n**结论：**\n通过直接计算，我们发现 $\\operatorname{MSE}_1^{\\mathrm{GD}} = \\operatorname{MSE}_1^{\\mathrm{HB}}$。这个结果是 HB 初始化条件 $x_{-1}^{\\mathrm{HB}} = x_0^{\\mathrm{HB}}$ 的直接推论，该条件使得动量项 $\\beta(x_0^{\\mathrm{HB}} - x_{-1}^{\\mathrm{HB}})$ 在第一步中为零。因此，HB 的第一步与 GD 的第一步完全相同。\n\n问题要求找到满足 $\\operatorname{MSE}_k^{\\mathrm{GD}} \\le \\operatorname{MSE}_k^{\\mathrm{HB}}$ 的最小整数 $k^\\star \\in \\{1, 2, \\ldots, T_{\\max}\\}$。既然我们已经证明在 $k=1$ 时等式成立，那么在搜索范围的第一步就满足了条件。因此，对于任何有效的参数集 $(a, \\alpha, \\beta, \\sigma, x_0)$，答案都是 $k^\\star = 1$。这些测试用例，尽管被构造成在 $k>1$ 时表现出不同的动态行为，但都在 $k=1$ 时满足交叉条件。所提供的实现将形式上执行推导出的递推关系。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the smallest iteration k >= 1 where MSE_GD = MSE_HB for given test cases.\n    \"\"\"\n    test_cases = [\n        (1.0, 0.2, 0.4, 0.4, 5.0, 500),\n        (1.0, 0.2, 0.4, 1.2, 5.0, 200),\n        (1.0, 0.2, 0.4, 1e-20, 5.0, 200),\n        (1.0, 1.5, 0.8, 0.2, 5.0, 100)\n    ]\n\n    results = []\n    for a, alpha, beta, sigma, x0, T_max in test_cases:\n        k_star = find_crossover_index(a, alpha, beta, sigma, x0, T_max)\n        results.append(k_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef find_crossover_index(a: float, alpha: float, beta: float, sigma: float, x0: float, T_max: int) -> int:\n    \"\"\"\n    Derives the MSE for GD and HB and finds the first k >= 1 where MSE_GD = MSE_HB.\n\n    Args:\n        a: Curvature of the objective function.\n        alpha: Stepsize for both methods.\n        beta: Momentum parameter for the heavy-ball method.\n        sigma: Standard deviation of the gradient noise.\n        x0: Deterministic initial point.\n        T_max: Maximum number of iterations to check.\n\n    Returns:\n        The smallest iteration index k >= 1 where the crossover occurs, or -1 if not found.\n    \"\"\"\n    # Initialize Mean Squared Error (MSE) for Gradient Descent (GD)\n    # S_k = E[x_k^2]\n    s_gd_k = x0**2\n    \n    # Initialize the 2x2 second-moment matrix C_k = E[z_k z_k^T] for Heavy-Ball (HB),\n    # where z_k = [x_k, x_{k-1}]^T. The initial condition x_{-1}=x_0 makes z_0 deterministic.\n    C_k = np.array([\n        [x0**2, x0**2],\n        [x0**2, x0**2]\n    ], dtype=np.float64)\n\n    # Pre-compute constant matrices for HB update\n    # State transition matrix M for z_{k+1} = M z_k + noise\n    M = np.array([\n        [1 - a * alpha + beta, -beta],\n        [1.0, 0.0]\n    ], dtype=np.float64)\n    \n    # Noise covariance term for the state-space update\n    noise_term_hb = np.array([\n        [alpha**2 * sigma**2, 0.0],\n        [0.0, 0.0]\n    ], dtype=np.float64)\n\n    # Iterate from k=1 to T_max to find the crossover point\n    for k in range(1, T_max + 1):\n        # Update GD MSE for step k\n        s_gd_k = (1 - a * alpha)**2 * s_gd_k + alpha**2 * sigma**2\n        \n        # Update HB second-moment matrix for step k using the Lyapunov recursion\n        C_k = M @ C_k @ M.T + noise_term_hb\n        s_hb_k = C_k[0, 0]\n\n        # Check for the crossover condition\n        if s_gd_k = s_hb_k:\n            return k\n            \n    return -1\n\nsolve()\n```"
        }
    ]
}