{
    "hands_on_practices": [
        {
            "introduction": "How do we guarantee a design specification is met when our implementation is imperfect? This exercise  explores this fundamental question by deriving the robust counterpart for a norm constraint subject to additive errors. By applying foundational principles like the triangle inequality, you will see how uncertainty translates into a tangible \"inflation factor,\" providing a clear safety margin for your design.",
            "id": "3195353",
            "problem": "Consider a design requirement on a true but unobserved vector $x \\in \\mathbb{R}^{n}$ given by the Euclidean norm constraint $\\|x\\|_{2} \\le r$, where $r > 0$ is specified. In implementation, $x$ is not directly available; instead, a nominal vector $z \\in \\mathbb{R}^{n}$ is chosen, and the implemented (true) vector is affected by an additive measurement/implementation error $\\delta \\in \\mathbb{R}^{n}$ so that $x = z + \\delta$. The error is known to lie in an ellipsoidal uncertainty set constructed from a linear image of the Euclidean unit ball:\n$$\n\\mathcal{U} \\;=\\; \\{\\, \\delta \\in \\mathbb{R}^{n} : \\delta = \\eta\\,P\\,u,\\;\\|u\\|_{2} \\le 1 \\,\\},\n$$\nwhere $\\eta > 0$ is a known scalar and $P \\in \\mathbb{R}^{n \\times n}$ is a known matrix. The robust counterpart of the requirement demands that the inequality $\\|x\\|_{2} \\le r$ hold for all $\\delta \\in \\mathcal{U}$.\n\nStarting only from the definitions of the Euclidean norm, the spectral norm (largest singular value) of a matrix, and the robust counterpart as the worst-case satisfaction of the constraint over the uncertainty set, derive an explicit deterministic inequality on $z$ that guarantees $\\|x\\|_{2} \\le r$ for all $\\delta \\in \\mathcal{U}$. Interpret this deterministic inequality as an “inflation” of the radius on the left-hand side by a nonnegative quantity $\\alpha$ (i.e., of the form $\\|z\\|_{2} + \\alpha \\le r$), and determine the exact closed-form expression for the inflation factor $\\alpha$ in terms of $\\eta$ and $P$.\n\nFinally, evaluate this expression numerically for\n$$\nP \\;=\\; \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & \\tfrac{1}{2} & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix},\n\\qquad\n\\eta \\;=\\; 0.3,\n$$\nand report the value of $\\alpha$. Round your answer to four significant figures. No units are required.",
            "solution": "The problem statement is evaluated to be scientifically grounded, well-posed, objective, and self-contained. It is a standard problem in robust optimization theory concerning ellipsoidal uncertainty. We may therefore proceed with a solution.\n\nThe design requirement is given by the inequality $\\|x\\|_{2} \\le r$, where $x \\in \\mathbb{R}^{n}$ is the true vector. The implementation model is $x = z + \\delta$, where $z \\in \\mathbb{R}^{n}$ is the nominal design vector and $\\delta \\in \\mathbb{R}^{n}$ is an additive error. The error $\\delta$ is known to belong to the uncertainty set $\\mathcal{U} = \\{\\, \\delta \\in \\mathbb{R}^{n} : \\delta = \\eta\\,P\\,u,\\;\\|u\\|_{2} \\le 1 \\,\\}$, where $\\eta > 0$ and $P \\in \\mathbb{R}^{n \\times n}$ are known.\n\nThe robust counterpart of the requirement demands that the inequality $\\|x\\|_{2} \\le r$ holds for all possible realizations of the error $\\delta$ from the set $\\mathcal{U}$. This can be formulated as a worst-case requirement:\n$$\n\\sup_{\\delta \\in \\mathcal{U}} \\|z + \\delta\\|_{2} \\le r\n$$\nSubstituting the definition of the uncertainty set $\\mathcal{U}$, we can express the error vector $\\delta$ in terms of the matrix $P$, the scalar $\\eta$, and a vector $u$ from the unit Euclidean ball. The robust counterpart becomes:\n$$\n\\sup_{\\|u\\|_{2} \\le 1} \\|z + \\eta\\,P\\,u\\|_{2} \\le r\n$$\nThis is the exact deterministic inequality on the design vector $z$. The problem asks to interpret this inequality in the form $\\|z\\|_{2} + \\alpha \\le r$ and find the expression for the inflation factor $\\alpha$. This a new constraint which must be a sufficient condition for the original robust constraint to hold. That is, if $\\|z\\|_{2} + \\alpha \\le r$ is satisfied, then $\\sup_{\\|u\\|_{2} \\le 1} \\|z + \\eta\\,P\\,u\\|_{2} \\le r$ must also be satisfied. This will be true if we can establish the inequality:\n$$\n\\sup_{\\|u\\|_{2} \\le 1} \\|z + \\eta\\,P\\,u\\|_{2} \\le \\|z\\|_{2} + \\alpha\n$$\nfor all $z \\in \\mathbb{R}^n$. To find the tightest such bound, we should seek the smallest non-negative $\\alpha$ that satisfies this for all $z$. This leads to defining $\\alpha$ as:\n$$\n\\alpha = \\sup_{z \\in \\mathbb{R}^n} \\left( \\sup_{\\|u\\|_{2} \\le 1} \\|z + \\eta\\,P\\,u\\|_{2} - \\|z\\|_{2} \\right)\n$$\nWe can find an upper bound for the expression within the supremum over $z$ by applying the triangle inequality for the Euclidean norm:\n$$\n\\|z + \\eta\\,P\\,u\\|_{2} \\le \\|z\\|_{2} + \\|\\eta\\,P\\,u\\|_{2}\n$$\nThis inequality holds for any specific $u$. Taking the supremum over all $u$ with $\\|u\\|_{2} \\le 1$ on both sides:\n$$\n\\sup_{\\|u\\|_{2} \\le 1} \\|z + \\eta\\,P\\,u\\|_{2} \\le \\sup_{\\|u\\|_{2} \\le 1} \\left( \\|z\\|_{2} + \\|\\eta\\,P\\,u\\|_{2} \\right) = \\|z\\|_{2} + \\sup_{\\|u\\|_{2} \\le 1} \\|\\eta\\,P\\,u\\|_{2}\n$$\nThe last term can be simplified:\n$$\n\\sup_{\\|u\\|_{2} \\le 1} \\|\\eta\\,P\\,u\\|_{2} = \\eta \\sup_{\\|u\\|_{2} \\le 1} \\|P\\,u\\|_{2}\n$$\nThe expression $\\sup_{\\|u\\|_{2} \\le 1} \\|P\\,u\\|_{2}$ is the definition of the spectral norm (or induced $2$-norm) of the matrix $P$, denoted by $\\|P\\|_{2}$. The spectral norm is equal to the largest singular value of $P$, denoted $\\sigma_{\\max}(P)$.\nThus, we have established an upper bound:\n$$\n\\sup_{\\|u\\|_{2} \\le 1} \\|z + \\eta\\,P\\,u\\|_{2} \\le \\|z\\|_{2} + \\eta\\,\\|P\\|_{2}\n$$\nFrom this, it follows that $\\alpha \\le \\eta\\,\\|P\\|_{2}$. To show that this bound is tight, we must demonstrate that there exists a choice of $z$ for which the equality is met. Let $\\sigma_{\\max}(P)$ be the largest singular value of $P$, and let $u_1 \\in \\mathbb{R}^n$ and $v_1 \\in \\mathbb{R}^n$ be the corresponding left and right singular vectors, respectively. By definition, they are unit vectors satisfying $P\\,v_1 = \\sigma_{\\max}(P)\\,u_1$.\n\nLet us choose the nominal vector $z$ to be aligned with the left singular vector $u_1$. Specifically, let $z = c\\,u_1$ for some scalar $c > 0$. For this $z$, we have $\\|z\\|_{2} = \\|c\\,u_1\\|_{2} = c\\,\\|u_1\\|_{2} = c$.\n\nNow, let's evaluate the worst-case norm for this specific $z$:\n$$\n\\sup_{\\|u\\|_{2} \\le 1} \\|z + \\eta\\,P\\,u\\|_{2} = \\sup_{\\|u\\|_{2} \\le 1} \\|c\\,u_1 + \\eta\\,P\\,u\\|_{2}\n$$\nSince the supremum must be greater than or equal to the value for any particular choice of $u$ with $\\|u\\|_{2} \\le 1$, let's choose $u = v_1$. As $\\|v_1\\|_{2} = 1$, this choice is valid.\nFor $u=v_1$, the norm becomes:\n$$\n\\|c\\,u_1 + \\eta\\,P\\,v_1\\|_{2} = \\|c\\,u_1 + \\eta\\,\\sigma_{\\max}(P)\\,u_1\\|_{2} = \\|(c + \\eta\\,\\sigma_{\\max}(P))\\,u_1\\|_{2}\n$$\nSince $c > 0$, $\\eta > 0$, and $\\sigma_{\\max}(P) \\ge 0$, the scalar factor is positive.\n$$\n\\|(c + \\eta\\,\\sigma_{\\max}(P))\\,u_1\\|_{2} = (c + \\eta\\,\\sigma_{\\max}(P))\\,\\|u_1\\|_{2} = c + \\eta\\,\\sigma_{\\max}(P)\n$$\nSubstituting back $c = \\|z\\|_{2}$ and $\\sigma_{\\max}(P) = \\|P\\|_{2}$, we get:\n$$\nc + \\eta\\,\\sigma_{\\max}(P) = \\|z\\|_{2} + \\eta\\,\\|P\\|_{2}\n$$\nWe have found a specific $z$ and a specific $u$ for which $\\|z + \\eta\\,P\\,u\\|_{2} = \\|z\\|_{2} + \\eta\\,\\|P\\|_{2}$. This implies that for this $z$:\n$$\n\\sup_{\\|u\\|_{2} \\le 1} \\|z + \\eta\\,P\\,u\\|_{2} \\ge \\|z\\|_{2} + \\eta\\,\\|P\\|_{2}\n$$\nCombining this with our earlier finding that $\\sup_{\\|u\\|_{2} \\le 1} \\|z + \\eta\\,P\\,u\\|_{2} \\le \\|z\\|_{2} + \\eta\\,\\|P\\|_{2}$, we must have equality for this choice of $z$.\nThis confirms that the supremum in the definition of $\\alpha$ can be achieved and is equal to $\\eta\\,\\|P\\|_{2}$.\nThe exact closed-form expression for the inflation factor is therefore:\n$$\n\\alpha = \\eta\\,\\|P\\|_{2} = \\eta\\,\\sigma_{\\max}(P)\n$$\n\nFinally, we evaluate this expression for the given numerical values:\n$$\nP \\;=\\; \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & \\tfrac{1}{2} & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix},\n\\qquad\n\\eta \\;=\\; 0.3\n$$\nThe matrix $P$ is a real diagonal matrix. Its singular values are the absolute values of its diagonal entries. The diagonal entries are $2$, $\\frac{1}{2}$, and $1$. The singular values are therefore $\\sigma_1=2$, $\\sigma_2=1$, and $\\sigma_3=0.5$.\nThe largest singular value is $\\sigma_{\\max}(P) = 2$. The spectral norm is $\\|P\\|_{2} = 2$.\nThe inflation factor $\\alpha$ is:\n$$\n\\alpha = \\eta\\,\\|P\\|_{2} = 0.3 \\times 2 = 0.6\n$$\nRounding to four significant figures, the value is $0.6000$.",
            "answer": "$$\\boxed{0.6000}$$"
        },
        {
            "introduction": "Ellipsoidal uncertainty sets are common in practice, but their geometry can seem complex. This practice  introduces the \"whitening transformation,\" a powerful technique that simplifies an ellipsoid into a sphere through a change of variables. Implementing this method will not only demystify the process of finding the worst-case scenario but also provide a concrete computational tool for handling correlated uncertainties.",
            "id": "3195373",
            "problem": "Consider the robust linear inequality in the presence of uncertain coefficients. Let the decision vector be $x \\in \\mathbb{R}^{n}$, the nominal coefficient vector be $\\bar{a} \\in \\mathbb{R}^{n}$, and the uncertainty in the coefficients be modeled by an ellipsoidal set\n$$\n\\mathcal{E} = \\left\\{ a \\in \\mathbb{R}^{n} \\;\\middle|\\; (a - \\bar{a})^{\\top} \\Sigma^{-1} (a - \\bar{a}) \\le \\rho^{2} \\right\\},\n$$\nwhere $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and $\\rho > 0$ is a radius parameter. The robust constraint requires that $a^{\\top} x \\le b$ holds for all $a \\in \\mathcal{E}$, with $b \\in \\mathbb{R}$ given.\n\nStarting from the definitions of an ellipsoidal uncertainty set and robust satisfaction of linear inequalities, and using only well-tested inequalities such as the Cauchy–Schwarz inequality, derive how the change of variables $z = \\Sigma^{-1/2} (a - \\bar{a})$ transforms the ellipsoidal set $\\mathcal{E}$ into a spherical (Euclidean ball) set in the whitened coordinates. Explain how this transformation enables the computation of the worst-case value of $a^{\\top} x$ over $\\mathcal{E}$ and hence simplifies the robust counterpart of the inequality. Show the reasoning step-by-step, beginning from first principles.\n\nThen, implement a program that, for each test case below, computes the worst-case left-hand side value of $a^{\\top} x$ over the ellipsoid $\\mathcal{E}$ using the whitening transformation described above, and returns this value. Your implementation must:\n- Compute a symmetric matrix square root $\\Sigma^{1/2}$ and its inverse $\\Sigma^{-1/2}$ using an eigenvalue decomposition suitable for symmetric positive definite matrices.\n- Use the whitening variable $z = \\Sigma^{-1/2} (a - \\bar{a})$ to justify and implement the computation of the worst-case value of $a^{\\top} x$.\n- Handle the edge case $x = 0$ properly.\n- For internal verification, also reconstruct at least one worst-case maximizer $a^{\\star} \\in \\mathcal{E}$ that attains the computed supremum, using the whitened-space characterization. This reconstruction is part of the implementation but does not need to be printed.\n\nFor all test cases, express the final outputs as real numbers (floats), rounded to six decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n\nUse the following test suite, which covers a general case, a boundary condition, an ill-conditioned covariance, and a larger-radius case:\n\n- Test Case 1 (general symmetric positive definite covariance):\n  - Dimension: $n = 3$\n  - $\\bar{a} = [0.8,\\,-1.2,\\,0.5]$\n  - $$ \\Sigma = \\begin{bmatrix} 3.0 & 0.5 & 0.0 \\\\ 0.5 & 2.0 & 0.4 \\\\ 0.0 & 0.4 & 1.5 \\end{bmatrix} $$\n  - $\\rho = 1.0$\n  - $x = [0.5,\\,1.0,\\,-0.5]$\n  - $b = 1.0$\n\n- Test Case 2 (boundary case $x = 0$):\n  - Dimension: $n = 3$\n  - $\\bar{a} = [2.0,\\,-1.0,\\,0.0]$\n  - $$ \\Sigma = \\operatorname{diag}(1.0,\\,2.0,\\,3.0) $$\n  - $\\rho = 0.75$\n  - $x = [0.0,\\,0.0,\\,0.0]$\n  - $b = 0.0$\n\n- Test Case 3 (ill-conditioned covariance, small eigenvalue aligned with $x$):\n  - Dimension: $n = 3$\n  - $\\bar{a} = [0.0,\\,0.0,\\,0.0]$\n  - $$ \\Sigma = \\operatorname{diag}(10^{-6},\\,1.0,\\,2.0) $$\n  - $\\rho = 1.0$\n  - $x = [1.0,\\,0.0,\\,0.0]$\n  - $b = 0.001$\n\n- Test Case 4 (larger radius, off-diagonal covariance):\n  - Dimension: $n = 3$\n  - $\\bar{a} = [1.0,\\,0.5,\\,-0.5]$\n  - $$ \\Sigma = \\begin{bmatrix} 2.5 & 0.8 & 0.3 \\\\ 0.8 & 1.8 & 0.6 \\\\ 0.3 & 0.6 & 1.2 \\end{bmatrix} $$\n  - $\\rho = 2.5$\n  - $x = [-0.2,\\,0.7,\\,0.1]$\n  - $b = 0.0$\n\nYour program should produce a single line of output containing the robust worst-case values $w$ for each test case, in the exact format:\n- A string of the form $[w_1,w_2,w_3,w_4]$, where each $w_i$ is rounded to six decimal places.",
            "solution": "The problem requires the derivation of the robust counterpart for a linear inequality $a^{\\top} x \\le b$ where the coefficient vector $a$ is uncertain and belongs to an ellipsoidal set $\\mathcal{E}$. Subsequently, we must compute the worst-case value of the expression $a^\\top x$ for several specific instances.\n\nThe robust counterpart of the inequality $a^{\\top} x \\le b$ requires the condition to hold for all possible realizations of $a \\in \\mathcal{E}$. This is equivalent to ensuring that the maximum possible value of $a^{\\top} x$ over the set $\\mathcal{E}$ is less than or equal to $b$. The problem thus reduces to solving the following maximization problem:\n$$\n\\max_{a \\in \\mathcal{E}} a^{\\top} x\n$$\nThe value of this maximum is the worst-case left-hand side of the inequality. Let us denote this value by $w$.\n\nThe uncertainty set is defined as:\n$$\n\\mathcal{E} = \\left\\{ a \\in \\mathbb{R}^{n} \\;\\middle|\\; (a - \\bar{a})^{\\top} \\Sigma^{-1} (a - \\bar{a}) \\le \\rho^{2} \\right\\}\n$$\nwhere $\\bar{a}$ is the nominal coefficient vector, $\\Sigma$ is a symmetric positive definite (SPD) matrix, and $\\rho > 0$ is a scalar radius.\n\nTo analyze the maximization problem, we first express the uncertain vector $a$ as a sum of its nominal value $\\bar{a}$ and a deviation vector $\\delta = a - \\bar{a}$. Substituting $a = \\bar{a} + \\delta$ into the objective function $a^{\\top} x$ gives:\n$$\na^{\\top} x = (\\bar{a} + \\delta)^{\\top} x = \\bar{a}^{\\top} x + \\delta^{\\top} x\n$$\nThe constraint defining the set $\\mathcal{E}$ can be rewritten in terms of $\\delta$:\n$$\n\\delta^{\\top} \\Sigma^{-1} \\delta \\le \\rho^{2}\n$$\nThe maximization problem for $w$ can now be expressed as:\n$$\nw = \\max_{\\delta: \\delta^{\\top} \\Sigma^{-1} \\delta \\le \\rho^{2}} (\\bar{a}^{\\top} x + \\delta^{\\top} x)\n$$\nSince $\\bar{a}^{\\top} x$ is a constant term with respect to the optimization variable $\\delta$, we can separate it from the maximization:\n$$\nw = \\bar{a}^{\\top} x + \\max_{\\delta: \\delta^{\\top} \\Sigma^{-1} \\delta \\le \\rho^{2}} \\delta^{\\top} x\n$$\nOur task now is to solve the subproblem $\\max_{\\delta} \\delta^{\\top} x$.\n\nThe problem suggests a change of variables, which is a standard technique known as whitening. Let us define the whitening variable $z$ as:\n$$\nz = \\Sigma^{-1/2} \\delta\n$$\nHere, $\\Sigma^{-1/2}$ is the inverse of the symmetric square root of $\\Sigma$. Since $\\Sigma$ is symmetric positive definite, its eigenvalue decomposition is $\\Sigma = U D U^{\\top}$, where $U$ is an orthogonal matrix of eigenvectors and $D$ is a diagonal matrix of strictly positive eigenvalues. The symmetric square root is then $\\Sigma^{1/2} = U D^{1/2} U^{\\top}$, where $D^{1/2}$ is the diagonal matrix of the square roots of the eigenvalues. $\\Sigma^{1/2}$ is also symmetric and positive definite. Its inverse is $\\Sigma^{-1/2} = U D^{-1/2} U^{\\top}$.\n\nWe now transform the constraint on $\\delta$ into a constraint on $z$.\n$$\n\\delta^{\\top} \\Sigma^{-1} \\delta = \\delta^{\\top} (\\Sigma^{-1/2}\\Sigma^{-1/2}) \\delta = (\\Sigma^{-1/2} \\delta)^{\\top} (\\Sigma^{-1/2} \\delta) = z^{\\top} z = \\|z\\|_{2}^{2}\n$$\nThe constraint $\\delta^{\\top} \\Sigma^{-1} \\delta \\le \\rho^{2}$ is therefore equivalent to $z^{\\top} z \\le \\rho^{2}$. This inequality defines a Euclidean ball (a filled sphere) of radius $\\rho$ centered at the origin in the $z$-space. The ellipsoidal uncertainty set in the $a$-space is transformed into a simple spherical set in the $z$-space.\n\nNext, we transform the objective function of the subproblem, $\\delta^{\\top} x$, into the new coordinates. From $z = \\Sigma^{-1/2} \\delta$, we find $\\delta = \\Sigma^{1/2} z$. Substituting this into the objective gives:\n$$\n\\delta^{\\top} x = (\\Sigma^{1/2} z)^{\\top} x = z^{\\top} (\\Sigma^{1/2})^{\\top} x\n$$\nSince $\\Sigma^{1/2}$ is symmetric, $(\\Sigma^{1/2})^{\\top} = \\Sigma^{1/2}$, and we have:\n$$\n\\delta^{\\top} x = z^{\\top} (\\Sigma^{1/2} x)\n$$\nThe subproblem becomes:\n$$\n\\max_{z: \\|z\\|_{2} \\le \\rho} z^{\\top} (\\Sigma^{1/2} x)\n$$\nThis is the problem of maximizing the dot product of the vector $z$, which lies within a ball of radius $\\rho$, with the fixed vector $v = \\Sigma^{1/2} x$. By the Cauchy-Schwarz inequality, $u^{\\top} v \\le \\|u\\|_{2} \\|v\\|_{2}$. Applying this here with $u=z$:\n$$\nz^{\\top} (\\Sigma^{1/2} x) \\le \\|z\\|_{2} \\|\\Sigma^{1/2} x\\|_{2}\n$$\nThe maximum value of $\\|z\\|_{2}$ is $\\rho$. Therefore, the maximum value of the expression is $\\rho \\|\\Sigma^{1/2} x\\|_{2}$. This maximum is attained when $z$ is collinear with and points in the same direction as $\\Sigma^{1/2} x$, and has the maximum possible length $\\rho$. The optimal $z$ is:\n$$\nz^{\\star} = \\rho \\frac{\\Sigma^{1/2} x}{\\|\\Sigma^{1/2} x\\|_{2}} \\quad (\\text{if } \\Sigma^{1/2} x \\neq 0)\n$$\nIf $x=0$, then $\\Sigma^{1/2}x = 0$, and the objective $z^\\top 0$ is always $0$. The maximum is thus $0$.\n\nCombining the results, the worst-case value $w$ is:\n$$\nw = \\bar{a}^{\\top} x + \\rho \\|\\Sigma^{1/2} x\\|_{2}\n$$\nThis formula holds even for $x=0$, since in that case $\\|\\Sigma^{1/2} x\\|_{2} = 0$, giving $w = 0$.\n\nFor computational purposes, the term $\\|\\Sigma^{1/2} x\\|_{2}$ can be calculated more directly without explicitly forming $\\Sigma^{1/2}$. We note that:\n$$\n\\|\\Sigma^{1/2} x\\|_{2}^{2} = (\\Sigma^{1/2} x)^{\\top}(\\Sigma^{1/2} x) = x^{\\top}(\\Sigma^{1/2})^{\\top}\\Sigma^{1/2} x = x^{\\top}\\Sigma^{1/2}\\Sigma^{1/2} x = x^{\\top}\\Sigma x\n$$\nTherefore, $\\|\\Sigma^{1/2} x\\|_{2} = \\sqrt{x^{\\top}\\Sigma x}$. This gives the final, computationally convenient formula for the worst-case value:\n$$\nw = \\bar{a}^{\\top} x + \\rho \\sqrt{x^{\\top}\\Sigma x}\n$$\nThe robust counterpart of the original inequality $a^{\\top} x \\le b$ is the deterministic second-order cone constraint:\n$$\n\\bar{a}^{\\top} x + \\rho \\sqrt{x^{\\top}\\Sigma x} \\le b\n$$\nThe program implementation will calculate $w$ using this final expression. For internal verification, as requested, the worst-case coefficient vector $a^{\\star}$ can be reconstructed by transforming $z^{\\star}$ back to the original space. If $x \\ne 0$:\n$$\n\\delta^{\\star} = \\Sigma^{1/2} z^{\\star} = \\Sigma^{1/2} \\left( \\rho \\frac{\\Sigma^{1/2} x}{\\|\\Sigma^{1/2} x\\|_{2}} \\right) = \\rho \\frac{\\Sigma x}{\\sqrt{x^{\\top}\\Sigma x}}\n$$\n$$\na^{\\star} = \\bar{a} + \\delta^{\\star} = \\bar{a} + \\rho \\frac{\\Sigma x}{\\sqrt{x^{\\top}\\Sigma x}}\n$$\nThis demonstrates that the worst-case uncertainty occurs when the deviation $\\delta$ is aligned with $\\Sigma x$ and is scaled to the boundary of the ellipsoid.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the worst-case value of a^T*x for a robust linear inequality\n    with ellipsoidal uncertainty.\n    \"\"\"\n    test_cases = [\n        # Test Case 1 (general symmetric positive definite covariance)\n        {\n            \"a_bar\": np.array([0.8, -1.2, 0.5]),\n            \"Sigma\": np.array([[3.0, 0.5, 0.0], [0.5, 2.0, 0.4], [0.0, 0.4, 1.5]]),\n            \"rho\": 1.0,\n            \"x\": np.array([0.5, 1.0, -0.5]),\n        },\n        # Test Case 2 (boundary case x = 0)\n        {\n            \"a_bar\": np.array([2.0, -1.0, 0.0]),\n            \"Sigma\": np.diag([1.0, 2.0, 3.0]),\n            \"rho\": 0.75,\n            \"x\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Test Case 3 (ill-conditioned covariance)\n        {\n            \"a_bar\": np.array([0.0, 0.0, 0.0]),\n            \"Sigma\": np.diag([1e-6, 1.0, 2.0]),\n            \"rho\": 1.0,\n            \"x\": np.array([1.0, 0.0, 0.0]),\n        },\n        # Test Case 4 (larger radius, off-diagonal covariance)\n        {\n            \"a_bar\": np.array([1.0, 0.5, -0.5]),\n            \"Sigma\": np.array([[2.5, 0.8, 0.3], [0.8, 1.8, 0.6], [0.3, 0.6, 1.2]]),\n            \"rho\": 2.5,\n            \"x\": np.array([-0.2, 0.7, 0.1]),\n        }\n    ]\n\n    results = []\n\n    for i, case in enumerate(test_cases):\n        a_bar = case[\"a_bar\"]\n        Sigma = case[\"Sigma\"]\n        rho = case[\"rho\"]\n        x = case[\"x\"]\n\n        # The worst-case value of a^T*x is given by:\n        # w = a_bar^T * x + rho * sqrt(x^T * Sigma * x)\n        \n        # Calculate the nominal part\n        nominal_value = a_bar.T @ x\n\n        # Calculate the uncertainty part. Handle the x=0 case.\n        # If x is the zero vector, then x^T*Sigma*x is 0.\n        if np.all(x == 0):\n            uncertainty_term_value = 0.0\n        else:\n            # The derivation uses a whitening transformation and results in rho * ||Sigma^(1/2) * x||_2.\n            # This is equivalent to rho * sqrt(x^T * Sigma * x), which is more direct to compute.\n            # We follow the derived formula for implementation.\n            quadratic_form_val = x.T @ Sigma @ x\n            uncertainty_term_value = rho * np.sqrt(quadratic_form_val)\n\n        # Total worst-case value\n        worst_case_value = nominal_value + uncertainty_term_value\n        results.append(worst_case_value)\n\n        # ------ Internal Verification (as requested, not for output) ------\n        # Reconstruct the worst-case maximizer a_star to verify the solution.\n        # a_star = a_bar + rho * (Sigma * x) / sqrt(x^T * Sigma * x)\n        \n        # Handle the x=0 case where a_star is not unique.\n        if np.all(x == 0):\n            # Any a in the ellipsoid gives a^T*x = 0. Choose a_star = a_bar.\n            a_star = a_bar\n        else:\n            sqrt_x_Sigma_x = np.sqrt(x.T @ Sigma @ x)\n            # Avoid division by zero, though if x!=0 and Sigma is SPD, this should be > 0.\n            if sqrt_x_Sigma_x > 1e-12:\n                delta_star = rho * (Sigma @ x) / sqrt_x_Sigma_x\n                a_star = a_bar + delta_star\n            else:\n                # This case should not be reached with valid inputs\n                a_star = a_bar\n\n        # Check that a_star achieves the worst-case value\n        achieved_value = a_star.T @ x\n        assert np.isclose(achieved_value, worst_case_value), f\"Case {i+1}: Verification failed for value\"\n\n        # Check that a_star is on the boundary of the ellipsoid\n        if not np.all(x == 0):\n            dev = a_star - a_bar\n            Sigma_inv = np.linalg.inv(Sigma)\n            ellipsoid_val_sq = dev.T @ Sigma_inv @ dev\n            assert np.isclose(ellipsoid_val_sq, rho**2), f\"Case {i+1}: Verification failed for boundary\"\n        # ---------------- End Internal Verification -----------------\n\n    # Format the output string as a comma-separated list of floats rounded to 6 decimal places,\n    # enclosed in square brackets.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "While some robust problems have elegant closed-form solutions, many require iterative algorithms to solve. This hands-on coding challenge  guides you through implementing the cutting-plane method, a general and powerful approach for robust optimization. By building a solver that iteratively adds the \"most violated\" constraint, you will gain practical insight into how complex robust problems are tackled and compare the process for different types of uncertainty sets.",
            "id": "3195342",
            "problem": "Consider the robust linear inequality $a^{\\top} x \\le b$ that must hold for all uncertain coefficient vectors $a$ belonging to an uncertainty set $\\mathcal{U}$. The decision vector is $x \\in \\mathbb{R}^n$ and the scalar is $b \\in \\mathbb{R}$. Two families of uncertainty sets are considered:\n\n- Ellipsoidal: $\\mathcal{U}_{\\mathrm{ell}} = \\{ a \\in \\mathbb{R}^n \\mid a = a_0 + P u, \\ \\|u\\|_2 \\le 1 \\}$, where $a_0 \\in \\mathbb{R}^n$ and $P \\in \\mathbb{R}^{n \\times n}$.\n- Polyhedral (hypercube image): $\\mathcal{U}_{\\mathrm{poly}} = \\{ a \\in \\mathbb{R}^n \\mid a = a_0 + G z, \\ \\|z\\|_{\\infty} \\le 1 \\}$, where $a_0 \\in \\mathbb{R}^n$ and $G \\in \\mathbb{R}^{n \\times m}$.\n\nA robust optimization approach requires that the worst-case value $\\sup_{a \\in \\mathcal{U}} a^{\\top} x$ is below $b$. This supremum is the support function of $\\mathcal{U}$ evaluated at $x$. The cutting-plane method constructs a finite relaxation of the robust constraint by iteratively adding linear constraints of the form $a_k^{\\top} x \\le b$ where $a_k \\in \\mathcal{U}$ is a maximizer of $a^{\\top} x$ for the current $x$. Each iteration consists of:\n1. Solving the Linear Programming (LP) subproblem $\\min c^{\\top} x$ subject to $0 \\le x_i \\le 1$ for all $i$, and all accumulated cuts $a_k^{\\top} x \\le b$.\n2. Computing a maximizer $a^{\\star} \\in \\mathcal{U}$ of $a^{\\top} x$ at the current $x$ using valid foundational inequalities from linear algebra and convex analysis (such as the Cauchy–Schwarz inequality and properties of dual norms), and checking if $a^{\\star \\top} x \\le b$ within a fixed tolerance.\n3. If violated, adding the cut $a^{\\star \\top} x \\le b$ and repeating; otherwise, terminating.\n\nStart from the core definitions of support functions and dual norms, and derive the maximizers $a^{\\star}$ for both ellipsoidal and polyhedral uncertainty sets using only these fundamental bases. Then implement the cutting-plane method exactly as described. The LP subproblems must be solved with bounds $0 \\le x_i \\le 1$ for all $i$; no other base constraints are present besides the cuts accumulated from uncertainty. Use a termination tolerance of $\\varepsilon = 10^{-8}$ and a maximum of $100$ iterations per run. If the LP solver returns an infeasible status at any iteration, terminate and return the current iteration count.\n\nTest suite:\n- Case 1 (happy path): $n=3$, $c = [-1.0,-0.8,-1.2]$, $b=1.0$, $a_0 = [0.6,0.2,0.3]$, $P = \\mathrm{diag}([0.3,0.1,0.2])$, $G = \\mathrm{diag}([0.2,0.15,0.05])$.\n- Case 2 (boundary condition: zero-iteration possibility): $n=4$, $c = [0.5,0.4,0.7,0.6]$, $b=0.2$, $a_0 = [1.0,0.5,0.2,0.1]$, $P = \\mathrm{diag}([0.4,0.3,0.2,0.1])$, $G = \\mathrm{diag}([0.2,0.1,0.05,0.03])$.\n- Case 3 (edge case: degenerate uncertainty): $n=3$, $c = [-0.9,-1.1,-0.7]$, $b=0.5$, $a_0 = [0.3,0.2,0.25]$, $P = 0_{3 \\times 3}$ (the zero matrix), $G = 0_{3 \\times 3}$.\n\nFor each case, run the cutting-plane method twice: once with $\\mathcal{U}_{\\mathrm{poly}}$ and once with $\\mathcal{U}_{\\mathrm{ell}}$. Record the number of iterations taken to satisfy the robust constraint (i.e., the number of cuts added). Your program should produce a single line of output containing these six iteration counts as a comma-separated list enclosed in square brackets, in the order $[\\text{Case1-poly},\\text{Case1-ell},\\text{Case2-poly},\\text{Case2-ell},\\text{Case3-poly},\\text{Case3-ell}]$. No physical units, angle units, or percentage formats apply to this problem; all outputs are integers.",
            "solution": "The user has provided a well-defined problem in robust linear optimization.\n\n### Step 1: Extract Givens\n- **Robust Linear Inequality**: $a^{\\top} x \\le b$ must hold for all $a \\in \\mathcal{U}$.\n- **Decision Vector**: $x \\in \\mathbb{R}^n$.\n- **Scalar Right-Hand Side**: $b \\in \\mathbb{R}$.\n- **Ellipsoidal Uncertainty Set**: $\\mathcal{U}_{\\mathrm{ell}} = \\{ a \\in \\mathbb{R}^n \\mid a = a_0 + P u, \\ \\|u\\|_2 \\le 1 \\}$, with center $a_0 \\in \\mathbb{R}^n$ and shape matrix $P \\in \\mathbb{R}^{n \\times n}$.\n- **Polyhedral Uncertainty Set**: $\\mathcal{U}_{\\mathrm{poly}} = \\{ a \\in \\mathbb{R}^n \\mid a = a_0 + G z, \\ \\|z\\|_{\\infty} \\le 1 \\}$, with center $a_0 \\in \\mathbb{R}^n$ and generator matrix $G \\in \\mathbb{R}^{n \\times m}$.\n- **Optimization Problem**: The cutting-plane method is applied to solve $\\min c^{\\top} x$ subject to the robust constraint and box constraints $0 \\le x_i \\le 1$.\n- **Cutting-Plane Algorithm**:\n    1.  Solve an LP subproblem with current cuts: $\\min c^{\\top} x$ subject to $0 \\le x_i \\le 1$ and $a_k^{\\top} x \\le b$ for all accumulated cuts $k$.\n    2.  Find a maximizer $a^{\\star} = \\arg\\max_{a \\in \\mathcal{U}} a^{\\top} x$ for the current solution $x$.\n    3.  If $a^{\\star \\top} x \\le b$ within tolerance $\\varepsilon$, terminate. Otherwise, add $a^{\\star \\top} x \\le b$ as a new cut and repeat.\n- **Parameters**: Termination tolerance $\\varepsilon = 10^{-8}$, maximum iterations $100$.\n- **Termination on Infeasibility**: If the LP subproblem is infeasible at any step, the algorithm terminates, returning the current number of accumulated cuts.\n- **Test Suite**: Three cases with specified parameters for $n$, $c$, $b$, $a_0$, $P$, and $G$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a standard application of the cutting-plane (or Kelley's) method to a robust linear program with common uncertainty sets. All concepts, including support functions, dual norms, and the algorithm itself, are fundamental topics in convex and robust optimization. The problem is scientifically sound.\n- **Well-Posed**: The problem is well-posed. Each LP subproblem is feasible (initially, the feasible set is the non-empty hypercube $[0,1]^n$) or becomes infeasible due to accumulating constraints. The objective function is linear and the domain is compact (intersection of the hypercube with linear inequalities), so unboundedness is not an issue. The overall cutting-plane algorithm is a standard procedure for this class of problems.\n- **Objective**: All terms and procedures are defined with mathematical precision. There is no subjective or ambiguous language.\n- **Completeness**: The problem provides all necessary data for the test cases and all parameters for the algorithm. The setup is self-contained.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be developed.\n\n### Derivation of the Maximizer $a^{\\star}$\n\nThe core of the cutting-plane method is to find the \"most violated\" constraint at each iteration. This corresponds to finding the vector $a^{\\star} \\in \\mathcal{U}$ that maximizes the expression $a^{\\top}x$ for a given decision vector $x$. The value of this maximum is the support function of the set $\\mathcal{U}$ at $x$, denoted $\\sigma_{\\mathcal{U}}(x) = \\sup_{a \\in \\mathcal{U}} a^{\\top}x$. The robust constraint $a^{\\top}x \\le b$ for all $a \\in \\mathcal{U}$ is equivalent to the single constraint $\\sigma_{\\mathcal{U}}(x) \\le b$.\n\n#### 1. Ellipsoidal Uncertainty Set $\\mathcal{U}_{\\mathrm{ell}}$\nFor the set $\\mathcal{U}_{\\mathrm{ell}} = \\{ a_0 + P u \\mid \\|u\\|_2 \\le 1 \\}$, the maximization problem is:\n$$ \\max_{a \\in \\mathcal{U}_{\\mathrm{ell}}} a^{\\top}x = \\max_{\\|u\\|_2 \\le 1} (a_0 + Pu)^{\\top}x $$\nWe can expand the term inside the maximization:\n$$ (a_0 + Pu)^{\\top}x = a_0^{\\top}x + (Pu)^{\\top}x = a_0^{\\top}x + u^{\\top}P^{\\top}x $$\nThe term $a_0^{\\top}x$ is constant with respect to $u$. The problem reduces to maximizing $u^{\\top}(P^{\\top}x)$ subject to the constraint $\\|u\\|_2 \\le 1$.\n\nBy the Cauchy-Schwarz inequality, for any two vectors $v, w \\in \\mathbb{R}^n$, we have $|v^{\\top}w| \\le \\|v\\|_2 \\|w\\|_2$. Applying this with $v=u$ and $w=P^{\\top}x$:\n$$ u^{\\top}(P^{\\top}x) \\le \\|u\\|_2 \\|P^{\\top}x\\|_2 $$\nSince $\\|u\\|_2 \\le 1$, we have:\n$$ u^{\\top}(P^{\\top}x) \\le \\|P^{\\top}x\\|_2 $$\nThe maximum value of $u^{\\top}(P^{\\top}x)$ is $\\|P^{\\top}x\\|_2$. This maximum is achieved when $u$ is a vector of norm $1$ that is collinear and co-directional with $P^{\\top}x$. Thus, the maximizing vector $u^{\\star}$ is:\n$$ u^{\\star} = \\begin{cases} \\frac{P^{\\top}x}{\\|P^{\\top}x\\|_2} & \\text{if } P^{\\top}x \\neq 0 \\\\ 0 & \\text{if } P^{\\top}x = 0 \\end{cases} $$\nThe maximizer $a^{\\star}$ is then found by substituting $u^{\\star}$ back into the definition of $a$:\n$$ a^{\\star} = a_0 + Pu^{\\star} = a_0 + P \\frac{P^{\\top}x}{\\|P^{\\top}x\\|_2} $$\nThe support function evaluates to $\\sigma_{\\mathcal{U}_{\\mathrm{ell}}}(x) = a_0^{\\top}x + \\|P^{\\top}x\\|_2$.\n\n#### 2. Polyhedral Uncertainty Set $\\mathcal{U}_{\\mathrm{poly}}$\nFor the set $\\mathcal{U}_{\\mathrm{poly}} = \\{ a_0 + G z \\mid \\|z\\|_{\\infty} \\le 1 \\}$, where $G \\in \\mathbb{R}^{n \\times m}$ and $z \\in \\mathbb{R}^m$, the maximization problem is:\n$$ \\max_{a \\in \\mathcal{U}_{\\mathrm{poly}}} a^{\\top}x = \\max_{\\|z\\|_{\\infty} \\le 1} (a_0 + Gz)^{\\top}x $$\nExpanding the expression:\n$$ (a_0 + Gz)^{\\top}x = a_0^{\\top}x + (Gz)^{\\top}x = a_0^{\\top}x + z^{\\top}G^{\\top}x $$\nAgain, $a_0^{\\top}x$ is constant. We need to maximize $z^{\\top}(G^{\\top}x)$ subject to $\\|z\\|_{\\infty} \\le 1$, which is equivalent to $-1 \\le z_i \\le 1$ for all $i=1, \\dots, m$.\nLet $v = G^{\\top}x \\in \\mathbb{R}^m$. The expression to maximize is $z^{\\top}v = \\sum_{i=1}^m z_i v_i$. To maximize this sum subject to the box constraints on $z_i$, we should choose each $z_i$ independently to have the largest possible value in the direction of $v_i$.\n- If $v_i > 0$, we choose $z_i=1$.\n- If $v_i < 0$, we choose $z_i=-1$.\n- If $v_i = 0$, any $z_i \\in [-1, 1]$ results in $z_i v_i = 0$, so we can simply choose $z_i=0$.\n\nThis logic is captured precisely by the sign function, $\\mathrm{sgn}(\\cdot)$. The maximizing vector $z^{\\star}$ is:\n$$ z^{\\star} = \\mathrm{sgn}(G^{\\top}x) $$\nwhere the sign function is applied element-wise. The maximizer $a^{\\star}$ is then:\n$$ a^{\\star} = a_0 + G z^{\\star} = a_0 + G \\, \\mathrm{sgn}(G^{\\top}x) $$\nThis result can also be understood through the concept of dual norms. The maximum value of $z^{\\top}v$ over the unit ball of the $\\|\\cdot\\|_{\\infty}$ norm is, by definition, the dual norm of $v$. The dual norm of the $\\|\\cdot\\|_{\\infty}$ norm is the $\\|\\cdot\\|_1$ norm. Thus, $\\sup_{\\|z\\|_{\\infty} \\le 1} z^{\\top}v = \\|v\\|_1 = \\sum_{i=1}^m |v_i|$. The support function is $\\sigma_{\\mathcal{U}_{\\mathrm{poly}}}(x) = a_0^{\\top}x + \\|G^{\\top}x\\|_1$.\n\n### Algorithm Implementation\nThe cutting-plane method is implemented as an iterative loop.\n1.  Initialize with an empty set of cuts. Let the number of cuts be `num_cuts` $= 0$.\n2.  Begin a loop that runs for a maximum of $100$ iterations.\n3.  In each iteration, solve the current LP subproblem: $\\min c^{\\top}x$ subject to the box constraints $0 \\le x_i \\le 1$ and the `num_cuts` accumulated cuts. This is performed using `scipy.optimize.linprog`.\n4.  If the LP solver reports infeasibility, the algorithm terminates, returning the current value of `num_cuts`.\n5.  With the optimal solution $x$ from the LP, calculate the corresponding maximizer $a^{\\star}$ using the derived formulas for the specified uncertainty set ($\\mathcal{U}_{\\mathrm{ell}}$ or $\\mathcal{U}_{\\mathrm{poly}}$).\n6.  Calculate the violation as $v = a^{\\star\\top}x - b$.\n7.  If $v \\le \\varepsilon = 10^{-8}$, the robust constraint is satisfied for the current $x$. The algorithm has converged and terminates, returning `num_cuts`.\n8.  If $v > \\varepsilon$, the constraint is violated. Add the new cut $a^{\\star\\top}x \\le b$ to the set of constraints, increment `num_cuts` by $1$, and continue to the next iteration.\n9.  If the loop completes without convergence, the maximum number of iterations ($100$) is returned.\n\nThis procedure is applied to each test case for both polyhedral and ellipsoidal uncertainty sets.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the cutting-plane algorithm.\n    \"\"\"\n\n    def run_cutting_plane(c, b, a_0, M, uncertainty_type, n, epsilon, max_iter):\n        \"\"\"\n        Implements the cutting-plane algorithm for a robust linear inequality.\n\n        Args:\n            c (np.ndarray): Cost vector for the LP objective.\n            b (float): RHS of the robust inequality.\n            a_0 (np.ndarray): Center of the uncertainty set.\n            M (np.ndarray): Shape matrix (P for ellipsoidal, G for polyhedral).\n            uncertainty_type (str): Either 'ell' for ellipsoidal or 'poly' for polyhedral.\n            n (int): Dimension of the decision vector x.\n            epsilon (float): Termination tolerance.\n            max_iter (int): Maximum number of iterations.\n\n        Returns:\n            int: The number of cuts added before termination.\n        \"\"\"\n        A_cuts = np.empty((0, n))\n        b_cuts = np.empty(0)\n        \n        num_cuts = 0\n        while num_cuts < max_iter:\n            # Step 1: Solve the LP subproblem\n            # Bounds are 0 <= x_i <= 1 for all i\n            res = linprog(c, A_ub=A_cuts, b_ub=b_cuts, bounds=(0, 1), method='highs')\n\n            # Handle infeasible LP subproblem\n            if res.status == 2:  # Status 2 for 'highs' method indicates infeasibility\n                return num_cuts\n            \n            # This case should ideally not be hit with the given problem structure (bounded domain)\n            if not res.success:\n                # Return max_iter to signify failure to solve the subproblem\n                return max_iter\n\n            x = res.x\n            \n            # Step 2: Compute the maximizer a_star (worst-case 'a')\n            if uncertainty_type == 'ell':\n                P = M\n                Pt_x = P.T @ x\n                norm_Pt_x = np.linalg.norm(Pt_x, 2)\n                \n                if norm_Pt_x < 1e-12: # Numerical stability for division by zero\n                    u_star = np.zeros(n)\n                else:\n                    u_star = Pt_x / norm_Pt_x\n                a_star = a_0 + P @ u_star\n            elif uncertainty_type == 'poly':\n                G = M\n                Gt_x = G.T @ x\n                z_star = np.sign(Gt_x)\n                a_star = a_0 + G @ z_star\n            else:\n                # This case should not be reached with valid inputs\n                raise ValueError(\"Invalid uncertainty type specified.\")\n\n            # Step 3: Check for violation and terminate or add cut\n            violation = a_star.T @ x - b\n            \n            if violation <= epsilon:\n                return num_cuts\n            \n            # Add the new cut\n            A_cuts = np.vstack([A_cuts, a_star])\n            b_cuts = np.append(b_cuts, b)\n            num_cuts += 1\n            \n        return max_iter\n\n    # Define common algorithm parameters\n    epsilon = 1e-8\n    max_iterations = 100\n\n    # Define test cases\n    test_cases = [\n        {\n            \"n\": 3, \"m\": 3,\n            \"c\": np.array([-1.0, -0.8, -1.2]),\n            \"b\": 1.0,\n            \"a_0\": np.array([0.6, 0.2, 0.3]),\n            \"P\": np.diag([0.3, 0.1, 0.2]),\n            \"G\": np.diag([0.2, 0.15, 0.05])\n        },\n        {\n            \"n\": 4, \"m\": 4,\n            \"c\": np.array([0.5, 0.4, 0.7, 0.6]),\n            \"b\": 0.2,\n            \"a_0\": np.array([1.0, 0.5, 0.2, 0.1]),\n            \"P\": np.diag([0.4, 0.3, 0.2, 0.1]),\n            \"G\": np.diag([0.2, 0.1, 0.05, 0.03])\n        },\n        {\n            \"n\": 3, \"m\": 3,\n            \"c\": np.array([-0.9, -1.1, -0.7]),\n            \"b\": 0.5,\n            \"a_0\": np.array([0.3, 0.2, 0.25]),\n            \"P\": np.zeros((3, 3)),\n            \"G\": np.zeros((3, 3))\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Run for Polyhedral Uncertainty\n        iters_poly = run_cutting_plane(\n            case[\"c\"], case[\"b\"], case[\"a_0\"], case[\"G\"], 'poly', case[\"n\"], epsilon, max_iterations\n        )\n        results.append(iters_poly)\n\n        # Run for Ellipsoidal Uncertainty\n        iters_ell = run_cutting_plane(\n            case[\"c\"], case[\"b\"], case[\"a_0\"], case[\"P\"], 'ell', case[\"n\"], epsilon, max_iterations\n        )\n        results.append(iters_ell)\n\n    # Print the results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}