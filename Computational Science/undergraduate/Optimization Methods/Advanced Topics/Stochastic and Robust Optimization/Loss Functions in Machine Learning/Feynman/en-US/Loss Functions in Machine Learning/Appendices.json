{
    "hands_on_practices": [
        {
            "introduction": "The properties of a loss function directly influence the nature of a model's optimal solution. This first practice explores the classic Least Absolute Deviations (LAD) regression, which uses the $\\ell_1$ norm, a robust alternative to the more common squared error loss. By analyzing a carefully constructed scenario from first principles, you will discover how the non-differentiability of the absolute value function leads to a set of optimal solutions directly related to the statistical median, providing a foundational insight into the geometry of non-smooth optimization. ",
            "id": "3146362",
            "problem": "You are given a linear regression model with absolute loss, also known as Least Absolute Deviations (LAD). The objective is to minimize the function $f(\\beta) = \\sum_{i=1}^{n} \\lvert y_i - x_i^\\top \\beta \\rvert$ over the parameter vector $\\beta \\in \\mathbb{R}^p$. Starting only from the core definitions of convex functions and subgradients, and the first-order optimality condition for convex nonsmooth optimization ($0 \\in \\partial f(\\beta^\\star)$), construct and analyze a concrete scenario where the LAD objective admits multiple minimizers. Then, implement a program to compute and report quantitative characteristics of the minimizer set for a small test suite.\n\nFundamental base to use:\n- Definition of convexity and subgradient: for a convex function $g:\\mathbb{R}^d \\to \\mathbb{R}$, the subdifferential at $z$ is $\\partial g(z) = \\{ s \\in \\mathbb{R}^d : g(w) \\ge g(z) + s^\\top (w - z) \\text{ for all } w \\}$.\n- For the absolute value function, $\\partial \\lvert r \\rvert = \\{ \\operatorname{sign}(r) \\}$ if $r \\ne 0$ and $\\partial \\lvert 0 \\rvert = [-1,1]$.\n- First-order optimality for convex nonsmooth minimization: a point $\\beta^\\star$ is optimal if and only if $0 \\in \\partial f(\\beta^\\star)$.\n\nScenario construction requirement:\n- Consider data matrices $X \\in \\mathbb{R}^{n \\times p}$ whose rows are identical, $x_i = x_0$ for all $i$, so predictions are constant across $i$, $x_i^\\top \\beta = x_0^\\top \\beta = \\gamma$. This reduces the LAD regression to choosing a scalar $\\gamma$ that minimizes $\\sum_{i=1}^n \\lvert y_i - \\gamma \\rvert$.\n- Use an even number $n$ of samples and choose a vector $y$ such that the two middle order statistics differ, yielding an interval of optimal $\\gamma$ values and hence multiple minimizers. Also include cases where $n$ is odd and where all $y_i$ are equal to probe boundary behavior. Finally, include a case with $p = 2$ and linearly dependent columns (nontrivial nullspace), to illustrate multiple minimizers in parameter space even when the optimal $\\gamma$ is unique.\n\nTasks:\n1. Derive, from the subgradient definition and the first-order optimality condition, the characterization of the optimal $\\gamma$ for the constant-prediction case. Show that the optimality condition $0 \\in \\partial f(\\beta^\\star)$ reduces to $\\sum_{i=1}^n s_i = 0$ where $s_i \\in \\partial \\lvert y_i - \\gamma \\rvert$, and conclude that any $\\gamma$ in the closed interval between the lower median and upper median of $\\{y_i\\}_{i=1}^n$ is optimal when $n$ is even; when $n$ is odd, the unique optimal $\\gamma$ is the median. Explain how the presence of a nontrivial nullspace of $X$ (that is, $p - \\operatorname{rank}(X) > 0$) creates infinitely many parameter-space minimizers that map to the same optimal predictions.\n2. For each test case below, compute:\n   - The interval $[\\gamma_{\\mathrm{low}}, \\gamma_{\\mathrm{high}}]$ of optimal constant predictions. For odd $n$, this collapses to a single point with $\\gamma_{\\mathrm{low}} = \\gamma_{\\mathrm{high}}$.\n   - The boolean $u_\\gamma$ indicating whether the optimal $\\gamma$ is unique ($u_\\gamma = \\text{True}$ if and only if $\\gamma_{\\mathrm{low}} = \\gamma_{\\mathrm{high}}$).\n   - The integer nullspace dimension $d_{\\mathcal{N}} = p - \\operatorname{rank}(X)$.\n   - The boolean $m_\\beta$ indicating whether the set of parameter-space minimizers is non-unique, defined as $m_\\beta = \\text{True}$ if and only if $(\\gamma_{\\mathrm{high}} - \\gamma_{\\mathrm{low}}) > 0$ or $d_{\\mathcal{N}} > 0$.\n   - The minimal absolute loss value $L_\\star = \\min_{\\beta} \\sum_{i=1}^{n} \\lvert y_i - x_i^\\top \\beta \\rvert$, which, in the constant-prediction scenario, equals $\\min_{\\gamma} \\sum_{i=1}^{n} \\lvert y_i - \\gamma \\rvert$.\n\nTest suite:\n- Case $1$ (even $n$, multiple optimal $\\gamma$ values, $p = 1$):\n  - $X = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $y = [0,1,3,10]$.\n- Case $2$ (odd $n$, unique optimal $\\gamma$, $p = 1$):\n  - $X = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $y = [0,2,2,5,9]$.\n- Case $3$ (even $n$, multiple optimal $\\gamma$, $p = 2$ with linearly dependent columns, nontrivial nullspace):\n  - $X = \\begin{bmatrix} 1  2 \\\\ 1  2 \\\\ 1  2 \\\\ 1  2 \\end{bmatrix}$, $y = [0,1,3,10]$.\n- Case $4$ (all $y_i$ equal, boundary behavior, $p = 1$):\n  - $X = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $y = [2,2,2,2]$.\n\nOutput specification:\n- Your program should produce a single line of output containing a list of per-case results. For each case, output the list $[\\gamma_{\\mathrm{low}}, \\gamma_{\\mathrm{high}}, u_\\gamma, d_{\\mathcal{N}}, m_\\beta, L_\\star]$. Aggregate the four case lists into one list and print it as a comma-separated list enclosed in square brackets (for example, $[ [\\dots], [\\dots], [\\dots], [\\dots] ]$). All numbers must be printed as plain decimal numbers and booleans as plain $\\text{True}$ or $\\text{False}$. No physical units or angle units are involved. Percentages are not used.",
            "solution": "The problem requires an analysis of the conditions leading to multiple minimizers for the Least Absolute Deviations (LAD) regression objective function and a computational implementation for specific test cases. The validation confirms the problem is well-posed, scientifically sound, and all necessary information is provided.\n\n### Part 1: Theoretical Derivation\n\nThe LAD objective function is given by:\n$$f(\\beta) = \\sum_{i=1}^{n} \\lvert y_i - x_i^\\top \\beta \\rvert$$\nwhere $y_i \\in \\mathbb{R}$ are the observed responses, $x_i \\in \\mathbb{R}^p$ are the predictor vectors, and $\\beta \\in \\mathbb{R}^p$ is the parameter vector to be optimized. The function $f(\\beta)$ is a sum of absolute value functions composed with affine functions of $\\beta$. Since the absolute value function is convex and the composition with an affine function preserves convexity, and the sum of convex functions is convex, $f(\\beta)$ is a convex function.\n\nThe first-order necessary and sufficient condition for a point $\\beta^\\star$ to be a minimizer of the convex function $f(\\beta)$ is that the zero vector must be an element of the subdifferential of $f$ at $\\beta^\\star$:\n$$0 \\in \\partial f(\\beta^\\star)$$\n\nThe subdifferential of $f(\\beta)$ can be computed using the sum rule for subdifferentials. Let $f_i(\\beta) = \\lvert y_i - x_i^\\top \\beta \\rvert$. Then $\\partial f(\\beta) = \\sum_{i=1}^n \\partial f_i(\\beta)$, where the sum is the Minkowski sum of sets. An element of $\\partial f(\\beta)$ is a vector of the form $\\sum_{i=1}^n v_i$, where each $v_i \\in \\partial f_i(\\beta)$.\n\nTo find $\\partial f_i(\\beta)$, we use the chain rule for subgradients. Let $h(u) = \\lvert u \\rvert$ and $g_i(\\beta) = y_i - x_i^\\top \\beta$. Then $f_i(\\beta) = h(g_i(\\beta))$. The chain rule states that $\\partial f_i(\\beta) = (\\partial g_i(\\beta))^\\top \\partial h(g_i(\\beta))$. The subgradient of the affine function $g_i(\\beta)$ is just the vector of coefficients, $\\partial g_i(\\beta) = \\{-x_i\\}$. The subdifferential of the absolute value function is $\\partial \\lvert u \\rvert = \\{\\operatorname{sign}(u)\\}$ for $u \\neq 0$ and $\\partial \\lvert 0 \\rvert = [-1, 1]$.\nLetting $r_i = y_i - x_i^\\top\\beta$ be the residual, an element of $\\partial f_i(\\beta)$ is a vector of the form $-s_i x_i$, where $s_i \\in \\partial \\lvert r_i \\rvert$. That is, $s_i = \\operatorname{sign}(y_i - x_i^\\top \\beta)$ if $y_i \\neq x_i^\\top \\beta$, and $s_i \\in [-1, 1]$ if $y_i = x_i^\\top \\beta$.\n\nThe optimality condition $0 \\in \\partial f(\\beta^\\star)$ is therefore equivalent to the existence of scalars $s_1, \\dots, s_n$ satisfying the conditions above, such that:\n$$\\sum_{i=1}^{n} (-s_i x_i) = 0 \\quad \\implies \\quad \\sum_{i=1}^{n} s_i x_i = 0$$\n\nNow, we introduce the specific scenario required by the problem: all predictor vectors are identical, i.e., $x_i = x_0$ for all $i=1, \\dots, n$. The optimality condition becomes:\n$$\\sum_{i=1}^{n} s_i x_0 = 0 \\quad \\implies \\quad x_0 \\left(\\sum_{i=1}^{n} s_i\\right) = 0$$\nAssuming $x_0 \\neq 0$ (as is true in the provided test cases), this vector equation reduces to a scalar equation:\n$$\\sum_{i=1}^{n} s_i = 0$$\nIn this scenario, the prediction $x_i^\\top \\beta = x_0^\\top \\beta$ is a constant value for all $i$. Let's denote this constant prediction by $\\gamma = x_0^\\top\\beta$. The scalars $s_i$ must be elements of the subdifferentials $\\partial \\lvert y_i - \\gamma^\\star \\rvert$ for some optimal prediction value $\\gamma^\\star$.\n\nThe problem of minimizing $f(\\beta)$ is thus reduced to finding the set of optimal prediction values $\\gamma^\\star$ that minimize $g(\\gamma) = \\sum_{i=1}^n \\lvert y_i - \\gamma \\rvert$, and then finding all $\\beta$ such that $x_0^\\top\\beta$ is in this set.\nLet's analyze the condition $\\sum_{i=1}^n s_i = 0$, where $s_i \\in \\partial \\lvert y_i - \\gamma^\\star \\rvert$. We partition the indices based on the sign of the residual:\n$I_ = \\{i \\mid y_i  \\gamma^\\star\\}$, $I_ = \\{i \\mid y_i  \\gamma^\\star\\}$, and $I_0 = \\{i \\mid y_i = \\gamma^\\star\\}$.\nThe condition becomes:\n$$\\sum_{i \\in I_} (1) + \\sum_{i \\in I_} (-1) + \\sum_{i \\in I_0} s_i = 0$$\nwhich simplifies to:\n$$|I_| - |I_| = -\\sum_{i \\in I_0} s_i$$\nSince each $s_i \\in [-1, 1]$, their sum must lie in the interval $[-|I_0|, |I_0|]$. Thus, a value $\\gamma^\\star$ is optimal if and only if $|I_| - |I_| \\in [ -|I_0|, |I_0| ]$, or equivalently, $\\left| |I_| - |I_| \\right| \\leq |I_0|$.\n\nThis condition precisely characterizes the median. Let $y_{(1)} \\le y_{(2)} \\le \\dots \\le y_{(n)}$ be the sorted response values (order statistics).\n- **If $n$ is odd**, let $n = 2k+1$. The median is unique, $y_{(k+1)}$. Let's test $\\gamma^\\star = y_{(k+1)}$. At most $k$ values are smaller and at most $k$ values are larger. $\\left| |I_| - |I_| \\right| \\le |I_0|$ will hold. For any $\\gamma  y_{(k+1)}$, $|I_|$ decreases and $|I_|$ increases, making $|I_|  |I_|$. If $\\gamma$ is not equal to any $y_i$, then $|I_0|=0$, and the condition requires $|I_| = |I_|$, which is impossible for odd $n$. Thus, the unique optimal $\\gamma$ is the median, $\\gamma^\\star = y_{((n+1)/2)}$.\n\n- **If $n$ is even**, let $n=2k$. Any value in the interval $[y_{(k)}, y_{(k+1)}]$ is a median.\n  - If we choose $\\gamma^\\star \\in (y_{(k)}, y_{(k+1)})$, then $|I_| = k$, $|I_| = k$, and $|I_0| = 0$. The condition $\\left| k - k \\right| \\le 0$ holds. So, any such $\\gamma^\\star$ is optimal.\n  - At the endpoints $\\gamma^\\star = y_{(k)}$ and $\\gamma^\\star = y_{(k+1)}$, the condition also holds. For example, at $\\gamma^\\star = y_{(k)}$, we have $|I_|  k$, $|I_| \\ge k$. The condition allows for an imbalance between $|I_| and $|I_|$ that is \"absorbed\" by the points in $I_0$.\nThus, for an even number of samples, any value $\\gamma^\\star$ in the closed interval formed by the two central order statistics, $[y_{(n/2)}, y_{(n/2)+1}]$, is an optimal prediction. This interval is what the problem refers to as $[\\gamma_{\\mathrm{low}}, \\gamma_{\\mathrm{high}}]$. It collapses to a single point if $y_{(n/2)} = y_{(n/2)+1}$.\n\nFinally, we analyze the non-uniqueness of the parameter vector $\\beta^\\star$. The set of minimizers is $S_\\beta = \\{\\beta \\in \\mathbb{R}^p \\mid x_0^\\top \\beta \\in [\\gamma_{\\mathrm{low}}, \\gamma_{\\mathrm{high}}]\\}$.\n1.  If the interval of optimal predictions is non-degenerate (i.e., $\\gamma_{\\mathrm{low}}  \\gamma_{\\mathrm{high}}$), there are infinitely many optimal values for $\\gamma = x_0^\\top \\beta$. This immediately implies there are infinitely many solutions for $\\beta$.\n2.  Even if the optimal prediction $\\gamma^\\star$ is unique ($\\gamma_{\\mathrm{low}} = \\gamma_{\\mathrm{high}}$), the set of minimizers is $S_\\beta = \\{\\beta \\in \\mathbb{R}^p \\mid x_0^\\top \\beta = \\gamma^\\star\\}$. This is a linear equation for $\\beta$. If $\\beta_p$ is a particular solution, the general solution is $\\beta = \\beta_p + v$, where $v$ is any vector in the nullspace of the linear map defined by $x_0^\\top$, i.e., $x_0^\\top v = 0$. The dimension of this nullspace is $p - \\operatorname{rank}(x_0^\\top)$. Since all rows of $X$ are $x_0$, $\\operatorname{rank}(X) = \\operatorname{rank}(x_0^\\top) = 1$ (for $x_0 \\neq 0$). The nullspace dimension is therefore $d_{\\mathcal{N}} = p - \\operatorname{rank}(X) = p - 1$. If $p1$, $d_{\\mathcal{N}}  0$, and there are infinitely many solutions for $\\beta$ even if $\\gamma^\\star$ is unique.\n\nIn summary, the set of minimizers $\\beta^\\star$ is non-unique ($m_\\beta = \\text{True}$) if either the interval of optimal predictions has positive length ($(\\gamma_{\\mathrm{high}} - \\gamma_{\\mathrm{low}})  0$) or the nullspace of $X$ is non-trivial ($d_{\\mathcal{N}}  0$).\n\n### Part 2: Computation for Test Suite\n\nThe following Python code implements the logic derived above to calculate the required quantities for each test case.\n- Medians are found by sorting the `y` vector.\n- Nullspace dimension is computed as $p - \\operatorname{rank}(X)$.\n- The minimal loss $L_\\star$ is calculated using an optimal $\\gamma$ (e.g., $\\gamma_{\\mathrm{low}}$), as the loss is constant over the optimal interval.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the LAD regression problem for multiple test cases, analyzing\n    the uniqueness of minimizers.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: even n, multiple optimal gamma, p=1\n        (np.array([[1], [1], [1], [1]], dtype=float), \n         np.array([0, 1, 3, 10], dtype=float)),\n        # Case 2: odd n, unique optimal gamma, p=1\n        (np.array([[1], [1], [1], [1], [1]], dtype=float), \n         np.array([0, 2, 2, 5, 9], dtype=float)),\n        # Case 3: even n, multiple optimal gamma, p=2 with dependent columns\n        (np.array([[1, 2], [1, 2], [1, 2], [1, 2]], dtype=float), \n         np.array([0, 1, 3, 10], dtype=float)),\n        # Case 4: all y_i equal, boundary behavior\n        (np.array([[1], [1], [1], [1]], dtype=float), \n         np.array([2, 2, 2, 2], dtype=float)),\n    ]\n\n    all_results = []\n    for X, y in test_cases:\n        n, p = X.shape\n        y_sorted = np.sort(y)\n\n        # 1. Compute the interval of optimal constant predictions [gamma_low, gamma_high]\n        if n % 2 == 1:\n            # For odd n, the median is unique\n            median_idx = (n - 1) // 2\n            gamma_low = y_sorted[median_idx]\n            gamma_high = y_sorted[median_idx]\n        else:\n            # For even n, the optimal interval is between the two middle elements\n            upper_median_idx = n // 2\n            lower_median_idx = upper_median_idx - 1\n            gamma_low = y_sorted[lower_median_idx]\n            gamma_high = y_sorted[upper_median_idx]\n            \n        # 2. Compute u_gamma: uniqueness of optimal gamma\n        u_gamma = (gamma_low == gamma_high)\n        \n        # 3. Compute d_N: nullspace dimension\n        rank_X = np.linalg.matrix_rank(X)\n        d_N = p - rank_X\n        \n        # 4. Compute m_beta: non-uniqueness of parameter-space minimizers\n        m_beta = (gamma_high  gamma_low) or (d_N  0)\n        \n        # 5. Compute L_star: minimal absolute loss value\n        # The loss is constant on the interval [gamma_low, gamma_high].\n        # We can evaluate it at any point in the interval, e.g., gamma_low.\n        L_star = np.sum(np.abs(y - gamma_low))\n        \n        # Collect results for the current case\n        case_result = [\n            float(gamma_low),\n            float(gamma_high),\n            u_gamma,\n            int(d_N),\n            m_beta,\n            float(L_star)\n        ]\n        all_results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # Format each inner list as a string \"[v1,v2,...]\"\n    result_strings = [\n        f\"[{','.join(map(str, res))}]\"\n        for res in all_results\n    ]\n    \n    # Join the inner list strings into a final string \"[[...],[...],...]\"\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the static optimality conditions explored previously, this exercise transitions to implementing a dynamic optimization algorithm. You will work with the quantile or \"pinball\" loss, which generalizes the absolute value loss to estimate any quantile of a distribution, not just the median. This hands-on task requires you to implement a subgradient descent algorithm from scratch, forcing you to confront the practical challenge of selecting a single subgradient from the subdifferential set at points of non-differentiability. ",
            "id": "3146402",
            "problem": "Consider the quantile (pinball) loss defined for any residual $r \\in \\mathbb{R}$ and quantile parameter $\\tau \\in (0,1)$ by\n$$\n\\ell_{\\tau}(r) = \\max\\{\\tau\\, r, (\\tau - 1)\\, r\\}.\n$$\nYou will address two tasks from first principles and implement a subgradient method that is testable on a small suite of cases.\n\nTask A (first principles subdifferential). Using only the definition of the convex subdifferential and basic properties of linear and convex functions, derive the complete subdifferential $\\partial \\ell_{\\tau}(0)$ at $r=0$. Your derivation must start from the definition: a scalar $g$ is in $\\partial \\ell_{\\tau}(0)$ if and only if, for all $r \\in \\mathbb{R}$,\n$$\n\\ell_{\\tau}(r) \\ge \\ell_{\\tau}(0) + g\\,(r - 0).\n$$\nClearly identify all $g$ that satisfy this inequality, and provide the set $\\partial \\ell_{\\tau}(0)$ in interval form.\n\nTask B (subgradient method with tie-breaking). Consider one-dimensional empirical quantile regression with objective\n$$\nf(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell_{\\tau}(y_i - \\theta),\n$$\nwhere $\\theta \\in \\mathbb{R}$ is the decision variable and $y_1,\\dots,y_n \\in \\mathbb{R}$ are the given observations. Design a subgradient method to minimize $f(\\theta)$ that:\n- uses the step-size schedule $\\alpha_k = \\dfrac{a_0}{\\sqrt{k+1}}$ for iteration index $k \\in \\{0,1,2,\\dots\\}$, with a given positive constant $a_0$,\n- forms a subgradient $\\hat{g}_k \\in \\partial f(\\theta_k)$ by averaging elementwise subgradients computed at residuals $r_i(\\theta_k) = y_i - \\theta_k$,\n- resolves the set-valued subgradient at any zero residual $r_i(\\theta_k) = 0$ by a deterministic tie-breaking rule that selects the midpoint of the subdifferential interval you derived in Task A, and\n- updates by $\\theta_{k+1} = \\theta_k - \\alpha_k\\, \\hat{g}_k$.\n\nYour implementation must be purely numerical and must not rely on any symbolic manipulation. For each test case below, your program must compute:\n- the subdifferential interval endpoints $(\\tau - 1, \\tau)$ at $r=0$,\n- the final iterate $\\theta_T$ after exactly $T$ iterations of the subgradient method described above.\n\nTest suite. Your program must run the method on the following cases, in this exact order:\n- Case $1$: $\\tau = 0.5$, data $y = (-1.0, 0.0, 2.0)$, initial $\\theta_0 = 1.0$, $a_0 = 0.7$, number of iterations $T = 60$.\n- Case $2$: $\\tau = 0.1$, data $y = (-3.0, -2.0, 0.0, 5.0)$, initial $\\theta_0 = 0.0$, $a_0 = 0.6$, number of iterations $T = 80$.\n- Case $3$: $\\tau = 0.5$, data $y = (0.0, 0.0, 0.0, 0.0)$, initial $\\theta_0 = 1.0$, $a_0 = 0.8$, number of iterations $T = 40$.\n- Case $4$: $\\tau = 0.9$, data $y = (-1.0, 0.0, 3.0, 10.0)$, initial $\\theta_0 = 0.0$, $a_0 = 0.5$, number of iterations $T = 100$.\n\nFinal output format. Your program should produce a single line of output containing a list of results, one per case, where each result is a list of three floating-point numbers $[\\tau - 1, \\tau, \\theta_T]$. The overall output must be a single list of these per-case lists printed on one line, for example:\n$[[x_{1},y_{1},z_{1}],[x_{2},y_{2},z_{2}],\\dots]$.",
            "solution": "The problem presents two tasks: first, to derive the subdifferential of the quantile loss function at the origin from first principles, and second, to design and implement a subgradient method for a simple one-dimensional quantile regression problem. The problem is well-posed, scientifically sound, and contains all necessary information for a deterministic solution.\n\n### Task A: Derivation of the Subdifferential $\\partial \\ell_{\\tau}(0)$\n\nThe quantile loss function, or pinball loss, is given by\n$$\n\\ell_{\\tau}(r) = \\max\\{\\tau r, (\\tau - 1) r\\}\n$$\nfor a residual $r \\in \\mathbb{R}$ and a quantile parameter $\\tau \\in (0, 1)$. This function is convex, as it is the maximum of two linear (and therefore convex) functions of $r$.\n\nThe subdifferential of a convex function $h:\\mathbb{R} \\to \\mathbb{R}$ at a point $x_0$ is the set of all slopes $g$ of lines that pass through $(x_0, h(x_0))$ and lie on or below the graph of $h(x)$. Formally, the subdifferential $\\partial h(x_0)$ is the set of all scalars $g \\in \\mathbb{R}$, called subgradients, such that for all $x \\in \\mathbb{R}$:\n$$\nh(x) \\ge h(x_0) + g(x - x_0)\n$$\n\nWe are asked to find the subdifferential $\\partial \\ell_{\\tau}(0)$ at the point $r=0$. First, we evaluate the function at this point:\n$$\n\\ell_{\\tau}(0) = \\max\\{\\tau \\cdot 0, (\\tau - 1) \\cdot 0\\} = \\max\\{0, 0\\} = 0.\n$$\nSubstituting $h = \\ell_{\\tau}$ and $x_0 = 0$ into the definition, a scalar $g$ is in $\\partial \\ell_{\\tau}(0)$ if and only if for all $r \\in \\mathbb{R}$:\n$$\n\\ell_{\\tau}(r) \\ge \\ell_{\\tau}(0) + g(r - 0)\n$$\n$$\n\\ell_{\\tau}(r) \\ge g r\n$$\nThis inequality must hold for all possible values of $r$. We analyze it by considering two cases for the sign of $r$.\n\n**Case 1: $r  0$**\nWhen $r$ is positive, we evaluate $\\ell_{\\tau}(r)$. Since $\\tau \\in (0, 1)$, $\\tau$ is positive and $\\tau - 1$ is negative. Therefore, $\\tau r  0$ and $(\\tau - 1) r  0$.\nThe maximum is thus $\\tau r$:\n$$\n\\ell_{\\tau}(r) = \\tau r \\quad \\text{for } r  0.\n$$\nSubstituting this into the subgradient inequality, we get:\n$$\n\\tau r \\ge g r\n$$\nSince $r  0$, we can divide by $r$ without changing the direction of the inequality:\n$$\n\\tau \\ge g\n$$\n\n**Case 2: $r  0$**\nWhen $r$ is negative, $\\tau r$ is negative and $(\\tau - 1) r$ is positive.\nThe maximum is thus $(\\tau - 1) r$:\n$$\n\\ell_{\\tau}(r) = (\\tau - 1) r \\quad \\text{for } r  0.\n$$\nSubstituting this into the subgradient inequality:\n$$\n(\\tau - 1) r \\ge g r\n$$\nSince $r  0$, dividing by $r$ reverses the direction of the inequality:\n$$\n\\tau - 1 \\le g\n$$\n\n**Conclusion\n**To be a valid subgradient $g$ at $r=0$, a scalar $g$ must satisfy the conditions derived from both cases simultaneously. That is, $g$ must satisfy both $g \\le \\tau$ and $g \\ge \\tau - 1$.\nCombining these, we have:\n$$\n\\tau - 1 \\le g \\le \\tau\n$$\nThe set of all such $g$ is the closed interval $[\\tau - 1, \\tau]$. Therefore, the subdifferential of the quantile loss at $r=0$ is:\n$$\n\\partial \\ell_{\\tau}(0) = [\\tau - 1, \\tau]\n$$\n\n### Task B: Subgradient Method Design and Implementation\n\nThe objective function to minimize is the empirical risk for one-dimensional quantile regression:\n$$\nf(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell_{\\tau}(y_i - \\theta)\n$$\nThis function is a finite sum of convex functions, so it is itself convex. We can minimize it using a subgradient method. The update rule for the parameter $\\theta$ at iteration $k$ is given as:\n$$\n\\theta_{k+1} = \\theta_k - \\alpha_k \\hat{g}_k\n$$\nwhere $\\alpha_k$ is the step size and $\\hat{g}_k \\in \\partial f(\\theta_k)$ is a subgradient of the objective function at the current iterate $\\theta_k$.\n\n**1. Subgradient of the Objective Function $f(\\theta)$**\nThe subdifferential of a sum of convex functions is the Minkowski sum of their individual subdifferentials. Applying this and the chain rule for subdifferentials:\n$$\n\\partial f(\\theta) = \\partial \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\ell_{\\tau}(y_i - \\theta) \\right) = \\frac{1}{n} \\sum_{i=1}^{n} \\partial_{\\theta} \\left( \\ell_{\\tau}(y_i - \\theta) \\right)\n$$\nLet $r_i(\\theta) = y_i - \\theta$. The derivative of $r_i$ with respect to $\\theta$ is $\\frac{d r_i}{d \\theta} = -1$.\nUsing the chain rule, the subdifferential of the $i$-th term with respect to $\\theta$ is:\n$$\n\\partial_{\\theta} \\ell_{\\tau}(y_i - \\theta) = \\partial_{r} \\ell_{\\tau}(r_i(\\theta)) \\cdot \\frac{d r_i}{d \\theta} = \\partial_{r} \\ell_{\\tau}(y_i - \\theta) \\cdot (-1)\n$$\nwhere $\\partial_{r} \\ell_{\\tau}$ is the subdifferential of $\\ell_{\\tau}$ with respect to its argument $r$.\nThus, a subgradient $\\hat{g}_k \\in \\partial f(\\theta_k)$ can be constructed by selecting a subgradient $s_{i,k}$ from each $\\partial_{r} \\ell_{\\tau}(y_i - \\theta_k)$ and forming the sum:\n$$\n\\hat{g}_k = \\frac{1}{n} \\sum_{i=1}^{n} (-s_{i,k}) = -\\frac{1}{n} \\sum_{i=1}^{n} s_{i,k}\n$$\n\n**2. Elementwise Subgradient Selection ($s_{i,k}$)**\nThe problem requires a specific deterministic rule for selecting $s_{i,k} \\in \\partial_{r} \\ell_{\\tau}(y_i - \\theta_k)$. Let $r_i = y_i - \\theta_k$.\n- If $r_i  0$, $\\ell_{\\tau}(r_i)$ is differentiable with derivative $\\tau$. The subdifferential is a singleton set: $\\partial_{r} \\ell_{\\tau}(r_i) = \\{\\tau\\}$. We must select $s_{i,k} = \\tau$.\n- If $r_i  0$, $\\ell_{\\tau}(r_i)$ is differentiable with derivative $\\tau-1$. The subdifferential is a singleton set: $\\partial_{r} \\ell_{\\tau}(r_i) = \\{\\tau-1\\}$. We must select $s_{i,k} = \\tau-1$.\n- If $r_i = 0$, the subdifferential is the interval $[\\tau-1, \\tau]$, as derived in Task A. The problem specifies a tie-breaking rule to select the midpoint of this interval: $s_{i,k} = \\frac{(\\tau-1) + \\tau}{2} = \\tau - 0.5$.\n\n**3. Update Rule**\nSubstituting the expression for $\\hat{g}_k$ into the update equation:\n$$\n\\theta_{k+1} = \\theta_k - \\alpha_k \\left(-\\frac{1}{n} \\sum_{i=1}^{n} s_{i,k}\\right) = \\theta_k + \\frac{\\alpha_k}{n} \\sum_{i=1}^{n} s_{i,k}\n$$\nThe step-size schedule is given as $\\alpha_k = \\frac{a_0}{\\sqrt{k+1}}$.\n\n**4. Algorithm Summary**\nThe complete algorithm is as follows:\n1. Initialize $\\theta_0$.\n2. For $k = 0, 1, 2, \\dots, T-1$:\n   a. Compute the step size: $\\alpha_k = \\frac{a_0}{\\sqrt{k+1}}$.\n   b. Initialize a sum for the elementwise subgradients: $S_k = 0$.\n   c. For each observation $y_i$ from $i=1$ to $n$:\n      i. Compute the residual: $r_i = y_i - \\theta_k$.\n      ii. Select the subgradient $s_{i,k}$ based on the sign of $r_i$:\n         - If $r_i  0$, $s_{i,k} = \\tau$.\n         - If $r_i  0$, $s_{i,k} = \\tau - 1$.\n         - If $r_i = 0$, $s_{i,k} = \\tau - 0.5$.\n      iii. Add to the sum: $S_k = S_k + s_{i,k}$.\n   d. Update the parameter: $\\theta_{k+1} = \\theta_k + \\frac{\\alpha_k}{n} S_k$.\n3. The final result is the iterate $\\theta_T$.\n\nThis fully specified, deterministic algorithm will be implemented to solve the given test cases. The required outputs for each case are the endpoints of the subdifferential interval, $(\\tau - 1, \\tau)$, and the final iterate $\\theta_T$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the quantile regression problem using a subgradient method\n    for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: tau, y, theta_0, a_0, T\n        (0.5, np.array([-1.0, 0.0, 2.0]), 1.0, 0.7, 60),\n        # Case 2\n        (0.1, np.array([-3.0, -2.0, 0.0, 5.0]), 0.0, 0.6, 80),\n        # Case 3\n        (0.5, np.array([0.0, 0.0, 0.0, 0.0]), 1.0, 0.8, 40),\n        # Case 4\n        (0.9, np.array([-1.0, 0.0, 3.0, 10.0]), 0.0, 0.5, 100),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        tau, y, theta_0, a_0, T = case\n        n = len(y)\n        \n        # Initialize theta\n        theta_k = theta_0\n        \n        # Perform T iterations of the subgradient method\n        for k in range(T):\n            # Calculate step size\n            alpha_k = a_0 / np.sqrt(k + 1)\n            \n            # Calculate the sum of elementwise subgradients\n            s_sum = 0.0\n            for y_i in y:\n                residual = y_i - theta_k\n                \n                # Select subgradient s_i based on the sign of the residual\n                if residual  0:\n                    s_i = tau\n                elif residual  0:\n                    s_i = tau - 1\n                else:  # residual == 0, apply tie-breaking rule\n                    s_i = tau - 0.5\n                \n                s_sum += s_i\n            \n            # Update theta\n            # The subgradient of the objective f(theta) is -(1/n) * s_sum.\n            # The update is theta_{k+1} = theta_k - alpha_k * g_k\n            # = theta_k - alpha_k * (-(1/n) * s_sum)\n            # = theta_k + (alpha_k / n) * s_sum\n            theta_k = theta_k + (alpha_k / n) * s_sum\n        \n        # The final iterate is theta_T\n        theta_T = theta_k\n        \n        # The subdifferential interval at r=0 is [tau - 1, tau]\n        lower_bound = tau - 1\n        upper_bound = tau\n        \n        # Store the results for the current case\n        results.append([lower_bound, upper_bound, theta_T])\n\n    # Format the final output string\n    # e.g., [[-0.5,0.5,-0.02109...],[-0.9,0.1,-0.1654...],...]\n    result_str = \"[\" + \",\".join(f\"[{res[0]},{res[1]},{res[2]}]\" for res in results) + \"]\"\n    \n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Many modern machine learning models are optimized by minimizing a composite objective function, typically a sum of a smooth data-fitting term and a non-smooth regularizer. This practice introduces the powerful proximal gradient method, an efficient algorithm tailored for such problems. You will derive the components for and implement this method on a problem featuring a smooth squared hinge loss and a non-smooth $\\ell_1$ regularizer, a core combination found in linear Support Vector Machines (SVMs), to encourage sparse solutions. ",
            "id": "3146371",
            "problem": "Consider binary classification with labels $y_i \\in \\{-1,+1\\}$ and features $x_i \\in \\mathbb{R}^d$, and the linear separator $w \\in \\mathbb{R}^d$. The hinge loss for one example $(x_i,y_i)$ is the function $\\ell_i(w) = \\max(0, 1 - y_i x_i^\\top w)$. Your tasks are the following, grounded in first principles of convex analysis and the definition of subgradients.\n\nTask $1$ (derivation): Starting from the definition of the hinge loss and the definition of the subdifferential for a pointwise maximum of convex functions, derive the subgradient set of $\\ell_i(w)$ with respect to $w$. Then extend your result to the empirical hinge loss $\\sum_{i=1}^n \\ell_i(w)$.\n\nTask $2$ (algorithm design): Consider the squared hinge loss, defined by the function $h(z) = \\max(0, 1 - z)^2$ for a scalar $z \\in \\mathbb{R}$. Define the smooth empirical loss\n$$\nf(w) = \\frac{1}{n} \\sum_{i=1}^n h(y_i x_i^\\top w),\n$$\nand the regularizer $g(w) = \\lambda \\|w\\|_1$ where $\\lambda \\ge 0$. Starting from the chain rule and the definition of $h(z)$, derive the gradient $\\nabla f(w)$. Using the property that $f(w)$ has a Lipschitz-continuous gradient, derive a computable upper bound $L$ on the Lipschitz constant of $\\nabla f(w)$ in terms of $\\{x_i\\}_{i=1}^n$ (do not assume any special structure on the data). Then, from the definition of the proximal operator for the $\\ell_1$ norm, derive the iterative update of the Proximal Gradient (PG) method for minimizing $F(w) = f(w) + g(w)$ with a fixed step size $\\alpha = 1/L$.\n\nTask $3$ (implementation and testing): Implement a complete program that:\n- Constructs the following three test cases (each case specifies $n$, $d$, $\\{(x_i,y_i)\\}_{i=1}^n$, $\\lambda$, and the number of iterations $K$):\n    - Case A (linearly separable): $n=4$, $d=2$, with\n        $x_1 = (2,0)$, $y_1 = +1$;\n        $x_2 = (0,2)$, $y_2 = +1$;\n        $x_3 = (-2,0)$, $y_3 = -1$;\n        $x_4 = (0,-2)$, $y_4 = -1$;\n        $\\lambda = 0.1$, $K = 200$.\n    - Case B (partially conflicting): $n=4$, $d=2$, with\n        $x_1 = (0.5,0.5)$, $y_1 = +1$;\n        $x_2 = (0.5,-0.5)$, $y_2 = +1$;\n        $x_3 = (-0.5,0.5)$, $y_3 = -1$;\n        $x_4 = (-0.5,-0.5)$, $y_4 = -1$;\n        $\\lambda = 0.5$, $K = 200$.\n    - Case C (degenerate features): $n=3$, $d=2$, with\n        $x_1 = (0,0)$, $y_1 = +1$;\n        $x_2 = (0,0)$, $y_2 = -1$;\n        $x_3 = (0,0)$, $y_3 = +1$;\n        $\\lambda = 0.3$, $K = 200$.\n- Initializes $w^{(0)} = 0 \\in \\mathbb{R}^d$, computes the step size $\\alpha = 1/L$ from your bound, and runs $K$ iterations of the PG algorithm for each case to produce $w^{(K)}$.\n- Computes the final objective value $F(w^{(K)}) = \\frac{1}{n} \\sum_{i=1}^n \\max(0, 1 - y_i x_i^\\top w^{(K)})^2 + \\lambda \\|w^{(K)}\\|_1$ as a real number for each case.\n\nYour program should produce a single line of output containing the three final objective values, as a comma-separated list enclosed in square brackets. For example, the output format must be exactly like $[v_A,v_B,v_C]$, where $v_A$, $v_B$, and $v_C$ are the floating-point values corresponding to Cases A, B, and C, respectively.\n\nIn addition, in your derivation explain why, under the assumptions that $f$ is convex with a Lipschitz-continuous gradient and $g$ is convex with a computable proximal operator, the Proximal Gradient (PG) method with a fixed step size $\\alpha \\in (0, 1/L]$ achieves the sublinear convergence rate $O(1/k)$ in objective value, and state how this applies to the squared hinge loss setting defined above.",
            "solution": "This problem requires the derivation of subgradients and gradients for hinge-based losses, the design and analysis of a proximal gradient algorithm for a regularized classification problem, and its implementation. We shall proceed by addressing each task in sequence, adhering to rigorous mathematical formalism.\n\n### Task 1: Subgradient of the Hinge Loss\n\nThe hinge loss for a single data point $(x_i, y_i)$, where $y_i \\in \\{-1, +1\\}$ and $x_i, w \\in \\mathbb{R}^d$, is defined as:\n$$\n\\ell_i(w) = \\max(0, 1 - y_i x_i^\\top w)\n$$\nThis function is a pointwise maximum of two convex, differentiable functions of $w$: $f_1(w) = 0$ and $f_2(w) = 1 - y_i x_i^\\top w$. The gradients of these functions are $\\nabla f_1(w) = 0$ and $\\nabla f_2(w) = -y_i x_i$.\n\nThe subdifferential of a pointwise maximum of a finite number of convex functions, $f(x) = \\max_{j=1,\\dots,m} f_j(x)$, at a point $x$ is the convex hull of the union of the subdifferentials of the \"active\" functions (those for which $f_j(x) = f(x)$):\n$$\n\\partial f(x) = \\text{conv} \\left( \\bigcup_{j: f_j(x) = f(x)} \\partial f_j(x) \\right)\n$$\nIn our case, since $f_1(w)$ and $f_2(w)$ are differentiable, their subdifferentials are singletons containing their gradients. We analyze the subdifferential $\\partial \\ell_i(w)$ by considering three cases for the margin term $z_i = y_i x_i^\\top w$:\n\n1.  **Correctly classified with margin ($1 - y_i x_i^\\top w  0$ or $y_i x_i^\\top w  1$):**\n    In this case, $\\ell_i(w) = 0$. The only active function is $f_1(w) = 0$. Thus, the subdifferential is the singleton set containing the gradient of $f_1(w)$:\n    $$\n    \\partial \\ell_i(w) = \\{0\\}\n    $$\n\n2.  **Incorrectly classified or on the wrong side of the margin ($1 - y_i x_i^\\top w  0$ or $y_i x_i^\\top w  1$):**\n    Here, $\\ell_i(w) = 1 - y_i x_i^\\top w$. The only active function is $f_2(w)$. The subdifferential is the singleton set containing the gradient of $f_2(w)$:\n    $$\n    \\partial \\ell_i(w) = \\{-y_i x_i\\}\n    $$\n\n3.  **Exactly on the margin ($1 - y_i x_i^\\top w = 0$ or $y_i x_i^\\top w = 1$):**\n    At this non-differentiable point, both functions are active: $\\ell_i(w) = f_1(w) = f_2(w) = 0$. The subdifferential is the convex hull of their gradients:\n    $$\n    \\partial \\ell_i(w) = \\text{conv}(\\{\\nabla f_1(w), \\nabla f_2(w)\\}) = \\text{conv}(\\{0, -y_i x_i\\})\n    $$\n    This is the line segment connecting $0$ and $-y_i x_i$:\n    $$\n    \\partial \\ell_i(w) = \\{ -\\theta y_i x_i \\mid \\theta \\in [0, 1] \\}\n    $$\n\nThe subdifferential for the total empirical hinge loss, $\\mathcal{L}(w) = \\sum_{i=1}^n \\ell_i(w)$, is given by the sum rule for subdifferentials (Minkowski sum), as the sum of convex functions:\n$$\n\\partial \\mathcal{L}(w) = \\sum_{i=1}^n \\partial \\ell_i(w) = \\left\\{ \\sum_{i=1}^n g_i \\mid g_i \\in \\partial \\ell_i(w) \\text{ for each } i \\right\\}\n$$\nwhere each $\\partial \\ell_i(w)$ is determined by the three cases above.\n\n### Task 2: Proximal Gradient Method for Squared Hinge Loss\n\nWe consider the objective function $F(w) = f(w) + g(w)$, with the smooth loss $f(w)$ and the non-smooth regularizer $g(w)$ defined as:\n$$\nf(w) = \\frac{1}{n} \\sum_{i=1}^n \\max(0, 1 - y_i x_i^\\top w)^2, \\quad g(w) = \\lambda \\|w\\|_1\n$$\n\n**Gradient of $f(w)$:**\nLet $h(z) = \\max(0, 1-z)^2$. The function $f(w)$ can be written as $f(w) = \\frac{1}{n} \\sum_{i=1}^n h(z_i(w))$, where $z_i(w) = y_i x_i^\\top w$.\nFirst, we find the derivative of $h(z)$.\n- If $z  1$, $h(z) = (1-z)^2$, so $h'(z) = 2(1-z)(-1) = -2(1-z)$.\n- If $z  1$, $h(z) = 0$, so $h'(z) = 0$.\n- If $z = 1$, the left-hand derivative is $\\lim_{z \\to 1^-} -2(1-z) = 0$, and the right-hand derivative is $0$. Thus, $h(z)$ is continuously differentiable everywhere.\nThe derivative can be compactly written as $h'(z) = -2\\max(0, 1-z)$.\n\nUsing the chain rule, the gradient of $f(w)$ is:\n$$\n\\nabla f(w) = \\frac{1}{n} \\sum_{i=1}^n h'(y_i x_i^\\top w) \\nabla_w(y_i x_i^\\top w)\n$$\nSince $\\nabla_w(y_i x_i^\\top w) = y_i x_i$, we have:\n$$\n\\nabla f(w) = \\frac{1}{n} \\sum_{i=1}^n \\left( -2 \\max(0, 1 - y_i x_i^\\top w) \\right) y_i x_i = -\\frac{2}{n} \\sum_{i=1}^n \\max(0, 1 - y_i x_i^\\top w) y_i x_i\n$$\n\n**Lipschitz Constant of $\\nabla f(w)$:**\nA function's gradient is $L$-Lipschitz continuous if its Hessian has a spectral norm bounded by $L$. The Hessian of $f(w)$ is $\\nabla^2 f(w) = \\frac{1}{n} \\sum_{i=1}^n \\nabla_w^2 [h(y_i x_i^\\top w)]$.\nThe second derivative of $h(z)$ is $h''(z) = 2$ if $z1$ and $h''(z)=0$ if $z1$. This can be written as $h''(z) = 2 \\cdot \\mathbb{I}(z  1)$, where $\\mathbb{I}(\\cdot)$ is the indicator function. The Hessian of the term for the $i$-th sample is obtained via the chain rule:\n$$\n\\nabla_w^2 [h(y_i x_i^\\top w)] = h''(y_i x_i^\\top w) (y_i x_i) (y_i x_i)^\\top = h''(y_i x_i^\\top w) x_i x_i^\\top\n$$\nSince $h''(z) \\ge 0$, the Hessian of each term is a positive semidefinite matrix. The Hessian of $f(w)$ is:\n$$\n\\nabla^2 f(w) = \\frac{1}{n} \\sum_{i=1}^n h''(y_i x_i^\\top w) x_i x_i^\\top\n$$\nSince $0 \\le h''(z) \\le 2$, we can bound the Hessian matrix:\n$$\n0 \\preceq \\nabla^2 f(w) \\preceq \\frac{2}{n} \\sum_{i=1}^n x_i x_i^\\top\n$$\nwhere $\\preceq$ denotes the Loewner order. The Lipschitz constant $L$ can be taken as an upper bound on the spectral norm of the Hessian. Let $X$ be the $n \\times d$ data matrix with rows $x_i^\\top$. Then $\\sum_{i=1}^n x_i x_i^\\top = X^\\top X$.\n$$\nL = \\sup_w \\|\\nabla^2 f(w)\\|_2 \\le \\left\\| \\frac{2}{n} \\sum_{i=1}^n x_i x_i^\\top \\right\\|_2 = \\frac{2}{n} \\|X^\\top X\\|_2\n$$\nThis provides a computable upper bound for the Lipschitz constant.\n\n**Proximal Gradient (PG) Update:**\nThe PG algorithm generates a sequence $w^{(k)}$ via the iterative update:\n$$\nw^{(k+1)} = \\text{prox}_{\\alpha g}(w^{(k)} - \\alpha \\nabla f(w^{(k)}))\n$$\nwhere $\\text{prox}_{\\alpha g}(z) = \\arg\\min_u \\left( g(u) + \\frac{1}{2\\alpha} \\|u - z\\|_2^2 \\right)$. For $g(w) = \\lambda \\|w\\|_1$, the proximal operator is the soft-thresholding operator, $\\mathcal{S}_{\\alpha\\lambda}(\\cdot)$:\n$$\n[\\text{prox}_{\\alpha\\lambda\\|\\cdot\\|_1}(z)]_j = \\mathcal{S}_{\\alpha\\lambda}(z_j) = \\text{sign}(z_j) \\max(0, |z_j| - \\alpha\\lambda)\n$$\nWe set the step size $\\alpha = 1/L$. The iterative update is:\n1.  **Gradient computation:** $\\nabla f(w^{(k)}) = -\\frac{2}{n} \\sum_{i=1}^n \\max(0, 1 - y_i x_i^\\top w^{(k)}) y_i x_i$.\n2.  **Gradient step:** $u^{(k)} = w^{(k)} - \\alpha \\nabla f(w^{(k)})$.\n3.  **Proximal step:** $w_j^{(k+1)} = \\text{sign}(u_j^{(k)}) \\max(0, |u_j^{(k)}| - \\alpha \\lambda)$ for each component $j \\in \\{1, \\dots, d\\}$.\n\n**Convergence Rate of Proximal Gradient:**\nFor an objective function $F(w) = f(w) + g(w)$, the PG method with step size $\\alpha \\in (0, 1/L]$ is guaranteed to converge under the following assumptions:\n1.  $f$ is a convex, differentiable function with an $L$-Lipschitz continuous gradient ($\\nabla f$ is $L$-smooth).\n2.  $g$ is a convex, possibly non-differentiable function, for which the proximal operator is efficiently computable.\n\nIn our problem, $f(w)$ is convex because $h(z) = \\max(0, 1-z)^2$ is a convex function (its second derivative $h''(z)$ is non-negative), composition with a linear map preserves convexity, and the sum of convex functions is convex. We have shown that $\\nabla f$ is $L$-smooth. The regularizer $g(w) = \\lambda \\|w\\|_1$ is convex, and its proximal operator is the well-known soft-thresholding operator. All assumptions are satisfied.\n\nThe standard convergence analysis for PG establishes that the sequence of objective values converges to the optimal value $F(w^*)$. Specifically, for any $k \\ge 1$:\n$$\nF(w^{(k)}) - F(w^*) \\le \\frac{\\|w^{(0)} - w^*\\|_2^2}{2\\alpha k}\n$$\nwhere $w^*$ is a minimizer of $F(w)$. This demonstrates a sublinear convergence rate of $O(1/k)$ for the objective function error. The proof relies on combining the descent lemma for the smooth part $f$, the convexity property of the non-smooth part $g$, and the definition of the proximal update step. This guarantees that the iterates make steady progress towards the minimum.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the proximal gradient optimization problem for three test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"n\": 4,\n            \"d\": 2,\n            \"X\": np.array([[2., 0.], [0., 2.], [-2., 0.], [0., -2.]]),\n            \"y\": np.array([1., 1., -1., -1.]),\n            \"lambda_val\": 0.1,\n            \"K\": 200,\n        },\n        {\n            \"name\": \"Case B\",\n            \"n\": 4,\n            \"d\": 2,\n            \"X\": np.array([[0.5, 0.5], [0.5, -0.5], [-0.5, 0.5], [-0.5, -0.5]]),\n            \"y\": np.array([1., 1., -1., -1.]),\n            \"lambda_val\": 0.5,\n            \"K\": 200,\n        },\n        {\n            \"name\": \"Case C\",\n            \"n\": 3,\n            \"d\": 2,\n            \"X\": np.array([[0., 0.], [0., 0.], [0., 0.]]),\n            \"y\": np.array([1., -1., 1.]),\n            \"lambda_val\": 0.3,\n            \"K\": 200,\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X = case[\"X\"]\n        y = case[\"y\"]\n        n = case[\"n\"]\n        d = case[\"d\"]\n        lambda_val = case[\"lambda_val\"]\n        K = case[\"K\"]\n\n        # Calculate Lipschitz constant L\n        # L = (2/n) * ||X^T X||_2\n        X_T_X = X.T @ X\n        # The spectral norm (ord=2) of a matrix is its largest singular value.\n        # For a positive semi-definite matrix like X^T X, this is the largest eigenvalue.\n        spectral_norm = np.linalg.norm(X_T_X, ord=2)\n        L = (2.0 / n) * spectral_norm\n\n        # Initialize weights\n        w = np.zeros(d)\n\n        # Handle the degenerate case where L=0\n        if L == 0:\n            # If L=0, it means all X_i are zero.\n            # The gradient of f(w) is always zero.\n            # The objective is f(w) + g(w) = 1 + lambda * ||w||_1.\n            # With w_init=0, PG update does not move w.\n            # So w_final = 0.\n            w_final = w\n        else:\n            # Set fixed step size\n            alpha = 1.0 / L\n\n            # Proximal Gradient iterations\n            for _ in range(K):\n                # 1. Compute gradient of f(w)\n                margins = X @ w * y\n                \n                # nabla_f(w) = (-2/n) * sum_i [ max(0, 1-m_i) * y_i * x_i ]\n                coeffs = np.maximum(0, 1 - margins) * y # Shape (n,)\n                grad_f = (-2.0 / n) * (X.T @ coeffs) # Shape (d,)\n            \n                # 2. Gradient descent step\n                u = w - alpha * grad_f\n\n                # 3. Proximal step (soft-thresholding)\n                prox_arg = alpha * lambda_val\n                w = np.sign(u) * np.maximum(0, np.abs(u) - prox_arg)\n            \n            w_final = w\n            \n        # Compute final objective value F(w_final)\n        # f(w) = (1/n) * sum(max(0, 1 - y_i * x_i^T w)^2)\n        final_margins = X @ w_final * y\n        f_val = np.mean(np.maximum(0, 1 - final_margins)**2)\n        \n        # g(w) = lambda * ||w||_1\n        g_val = lambda_val * np.linalg.norm(w_final, ord=1)\n        \n        final_objective = f_val + g_val\n        results.append(final_objective)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}