{
    "hands_on_practices": [
        {
            "introduction": "本练习探讨了机器学习中一个基础而重要的问题：损失函数的选择如何影响解的特性。通过分析最小绝对偏差（LAD）回归，我们将看到与平滑的平方损失不同，使用非平滑的绝对值损失会导致解可能不唯一 。这个实践将帮助你从第一性原理出发，运用次梯度的概念来刻画优化问题的解集，并理解解的非唯一性可能源于目标函数本身或数据特征的结构。",
            "id": "3146362",
            "problem": "给定一个带有绝对损失的线性回归模型，也称为最小绝对偏差（LAD）。目标是最小化函数 $f(\\beta) = \\sum_{i=1}^{n} \\lvert y_i - x_i^\\top \\beta \\rvert$，其中参数向量 $\\beta \\in \\mathbb{R}^p$。仅从凸函数和次梯度的核心定义，以及凸非光滑优化的一阶最优性条件（$0 \\in \\partial f(\\beta^\\star)$）出发，构造并分析一个具体的场景，在该场景中 LAD 目标函数存在多个最小化子。然后，实现一个程序来计算并报告一个小型测试集上最小化子集的量化特征。\n\n使用的基本原理：\n- 凸性和次梯度的定义：对于一个凸函数 $g:\\mathbb{R}^d \\to \\mathbb{R}$，其在点 $z$ 处的次微分为 $\\partial g(z) = \\{ s \\in \\mathbb{R}^d : g(w) \\ge g(z) + s^\\top (w - z) \\text{ for all } w \\}$。\n- 对于绝对值函数，$\\partial \\lvert r \\rvert = \\{ \\operatorname{sign}(r) \\}$ 如果 $r \\ne 0$，且 $\\partial \\lvert 0 \\rvert = [-1,1]$。\n- 凸非光滑最小化的一阶最优性条件：一个点 $\\beta^\\star$ 是最优的，当且仅当 $0 \\in \\partial f(\\beta^\\star)$。\n\n场景构造要求：\n- 考虑数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$，其所有行都相同，即对于所有 $i$，$x_i = x_0$。因此，所有预测值在 $i$ 上都是常数，$x_i^\\top \\beta = x_0^\\top \\beta = \\gamma$。这将 LAD 回归问题简化为选择一个标量 $\\gamma$ 来最小化 $\\sum_{i=1}^n \\lvert y_i - \\gamma \\rvert$。\n- 使用偶数个样本 $n$，并选择一个向量 $y$，使得两个中间的顺序统计量不同，从而产生一个最优 $\\gamma$ 值的区间，进而得到多个最小化子。同时，也包括 $n$ 为奇数和所有 $y_i$ 相等的情况，以探究边界行为。最后，包括一个 $p = 2$ 且列向量线性相关（非平凡零空间）的情况，以说明即使最优 $\\gamma$ 是唯一的，参数空间中也可能存在多个最小化子。\n\n任务：\n1. 从次梯度的定义和一阶最优性条件出发，推导在常数预测情况下最优 $\\gamma$ 的特征。证明最优性条件 $0 \\in \\partial f(\\beta^\\star)$ 可简化为 $\\sum_{i=1}^n s_i = 0$，其中 $s_i \\in \\partial \\lvert y_i - \\gamma \\rvert$。并由此得出结论：当 $n$ 为偶数时，任何位于 $\\{y_i\\}_{i=1}^n$ 的下中位数和上中位数之间的闭区间内的 $\\gamma$ 都是最优的；当 $n$ 为奇数时，唯一的最优 $\\gamma$ 是中位数。解释 $X$ 的非平凡零空间（即 $p - \\operatorname{rank}(X) > 0$）如何导致参数空间中存在无限多个最小化子，而这些最小化子都映射到相同的最优预测值。\n2. 对于下方的每个测试用例，计算：\n   - 最优常数预测值的区间 $[\\gamma_{\\mathrm{low}}, \\gamma_{\\mathrm{high}}]$。对于奇数 $n$，该区间退化为单个点，即 $\\gamma_{\\mathrm{low}} = \\gamma_{\\mathrm{high}}$。\n   - 布尔值 $u_\\gamma$，表示最优 $\\gamma$ 是否唯一（$u_\\gamma = \\text{True}$ 当且仅当 $\\gamma_{\\mathrm{low}} = \\gamma_{\\mathrm{high}}$）。\n   - 整数零空间维度 $d_{\\mathcal{N}} = p - \\operatorname{rank}(X)$。\n   - 布尔值 $m_\\beta$，表示参数空间最小化子集是否非唯一，定义为 $m_\\beta = \\text{True}$ 当且仅当 $(\\gamma_{\\mathrm{high}} - \\gamma_{\\mathrm{low}}) > 0$ 或 $d_{\\mathcal{N}} > 0$。\n   - 最小绝对损失值 $L_\\star = \\min_{\\beta} \\sum_{i=1}^{n} \\lvert y_i - x_i^\\top \\beta \\rvert$，在常数预测场景下，它等于 $\\min_{\\gamma} \\sum_{i=1}^{n} \\lvert y_i - \\gamma \\rvert$。\n\n测试集：\n- 用例 1（偶数 $n$，多个最优 $\\gamma$ 值，$p = 1$）：\n  - $X = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$，$y = [0,1,3,10]$。\n- 用例 2（奇数 $n$，唯一最优 $\\gamma$，$p = 1$）：\n  - $X = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$，$y = [0,2,2,5,9]$。\n- 用例 3（偶数 $n$，多个最优 $\\gamma$，$p = 2$ 且列线性相关，非平凡零空间）：\n  - $X = \\begin{bmatrix} 1  2 \\\\ 1  2 \\\\ 1  2 \\\\ 1  2 \\end{bmatrix}$，$y = [0,1,3,10]$。\n- 用例 4（所有 $y_i$ 相等，边界行为，$p = 1$）：\n  - $X = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$，$y = [2,2,2,2]$。\n\n输出规范：\n- 你的程序应生成单行输出，包含一个按用例划分的结果列表。对于每个用例，输出列表 $[\\gamma_{\\mathrm{low}}, \\gamma_{\\mathrm{high}}, u_\\gamma, d_{\\mathcal{N}}, m_\\beta, L_\\star]$。将四个用例的列表聚合成一个列表，并以方括号括起来的逗号分隔列表形式打印（例如，$[[…], […], […], […]] $）。所有数字必须打印为普通十进制数，布尔值必须打印为纯文本的 $\\text{True}$ 或 $\\text{False}$。不涉及物理单位或角度单位。不使用百分比。",
            "solution": "该问题要求分析导致最小绝对偏差（LAD）回归目标函数存在多个最小化子的条件，并对特定测试用例进行计算实现。验证证实了该问题是适定的、科学上合理的，并且提供了所有必要的信息。\n\n### 第1部分：理论推导\n\nLAD 目标函数由以下公式给出：\n$$f(\\beta) = \\sum_{i=1}^{n} \\lvert y_i - x_i^\\top \\beta \\rvert$$\n其中 $y_i \\in \\mathbb{R}$ 是观测响应值，$x_i \\in \\mathbb{R}^p$ 是预测向量，$\\beta \\in \\mathbb{R}^p$ 是待优化的参数向量。函数 $f(\\beta)$ 是绝对值函数与 $\\beta$ 的仿射函数复合后求和得到的。由于绝对值函数是凸的，与仿射函数的复合保留了凸性，且凸函数的和仍然是凸的，因此 $f(\\beta)$ 是一个凸函数。\n\n对于一个点 $\\beta^\\star$ 是凸函数 $f(\\beta)$ 的最小化子，其一阶充要条件是零向量必须是 $f$ 在 $\\beta^\\star$ 处的次微分的元素：\n$$0 \\in \\partial f(\\beta^\\star)$$\n\n$f(\\beta)$ 的次微分可以使用次微分的求和法则来计算。令 $f_i(\\beta) = \\lvert y_i - x_i^\\top \\beta \\rvert$。那么 $\\partial f(\\beta) = \\sum_{i=1}^n \\partial f_i(\\beta)$，这里的和是集合的闵可夫斯基和。$\\partial f(\\beta)$ 中的一个元素是形如 $\\sum_{i=1}^n v_i$ 的向量，其中每个 $v_i \\in \\partial f_i(\\beta)$。\n\n为了找到 $\\partial f_i(\\beta)$，我们使用次梯度的链式法则。令 $h(u) = \\lvert u \\rvert$ 和 $g_i(\\beta) = y_i - x_i^\\top \\beta$。那么 $f_i(\\beta) = h(g_i(\\beta))$。链式法则表明 $\\partial f_i(\\beta) = (\\partial g_i(\\beta))^\\top \\partial h(g_i(\\beta))$。仿射函数 $g_i(\\beta)$ 的次梯度就是系数向量，即 $\\partial g_i(\\beta) = \\{-x_i\\}$。绝对值函数的次微分为 $\\partial \\lvert u \\rvert = \\{\\operatorname{sign}(u)\\}$（对于 $u \\neq 0$）和 $\\partial \\lvert 0 \\rvert = [-1, 1]$。\n令残差为 $r_i = y_i - x_i^\\top\\beta$，$\\partial f_i(\\beta)$ 中的一个元素是形如 $-s_i x_i$ 的向量，其中 $s_i \\in \\partial \\lvert r_i \\rvert$。也就是说，如果 $y_i \\neq x_i^\\top \\beta$，则 $s_i = \\operatorname{sign}(y_i - x_i^\\top \\beta)$；如果 $y_i = x_i^\\top \\beta$，则 $s_i \\in [-1, 1]$。\n\n因此，最优性条件 $0 \\in \\partial f(\\beta^\\star)$ 等价于存在满足上述条件的标量 $s_1, \\dots, s_n$，使得：\n$$\\sum_{i=1}^{n} (-s_i x_i) = 0 \\quad \\implies \\quad \\sum_{i=1}^{n} s_i x_i = 0$$\n\n现在，我们引入问题所要求的特定场景：所有预测向量都相同，即对于所有 $i=1, \\dots, n$，$x_i = x_0$。最优性条件变为：\n$$\\sum_{i=1}^{n} s_i x_0 = 0 \\quad \\implies \\quad x_0 \\left(\\sum_{i=1}^{n} s_i\\right) = 0$$\n假设 $x_0 \\neq 0$（在所提供的测试用例中是成立的），这个向量方程简化为一个标量方程：\n$$\\sum_{i=1}^{n} s_i = 0$$\n在这种场景下，预测值 $x_i^\\top \\beta = x_0^\\top \\beta$ 对所有 $i$ 都是一个常数值。我们用 $\\gamma = x_0^\\top\\beta$ 表示这个常数预测值。标量 $s_i$ 必须是某个最优预测值 $\\gamma^\\star$ 对应的次微分 $\\partial \\lvert y_i - \\gamma^\\star \\rvert$ 中的元素。\n\n因此，最小化 $f(\\beta)$ 的问题被简化为找到最优预测值 $\\gamma^\\star$ 的集合，该集合最小化 $g(\\gamma) = \\sum_{i=1}^n \\lvert y_i - \\gamma \\rvert$，然后找到所有满足 $x_0^\\top\\beta$ 在这个集合中的 $\\beta$。\n让我们分析条件 $\\sum_{i=1}^n s_i = 0$，其中 $s_i \\in \\partial \\lvert y_i - \\gamma^\\star \\rvert$。我们根据残差的符号对索引进行划分：\n$I_> = \\{i \\mid y_i > \\gamma^\\star\\}$，$I_ = \\{i \\mid y_i  \\gamma^\\star\\}$，以及 $I_0 = \\{i \\mid y_i = \\gamma^\\star\\}$。\n条件变为：\n$$\\sum_{i \\in I_>} (1) + \\sum_{i \\in I_} (-1) + \\sum_{i \\in I_0} s_i = 0$$\n这可以简化为：\n$$|I_>| - |I_| = -\\sum_{i \\in I_0} s_i$$\n由于每个 $s_i \\in [-1, 1]$，它们的和必须在区间 $[-|I_0|, |I_0|]$ 内。因此，一个值 $\\gamma^\\star$ 是最优的，当且仅当 $|I_>| - |I_| \\in [ -|I_0|, |I_0| ]$，或者等价地，$\\left| |I_>| - |I_| \\right| \\leq |I_0|$。\n\n这个条件精确地描述了中位数。令 $y_{(1)} \\le y_{(2)} \\le \\dots \\le y_{(n)}$ 为排序后的响应值（顺序统计量）。\n- **如果 $n$ 是奇数**，令 $n = 2k+1$。中位数是唯一的，$y_{(k+1)}$。我们测试 $\\gamma^\\star = y_{(k+1)}$。此时，最多有 $k$ 个值比它小，最多有 $k$ 个值比它大。$\\left| |I_>| - |I_| \\right| \\le |I_0|$ 将会成立。对于任何 $\\gamma  y_{(k+1)}$， $|I_|$ 减小， $|I_>|$ 增加，使得 $|I_>| > |I_|$。如果 $\\gamma$ 不等于任何 $y_i$，那么 $|I_0|=0$，条件要求 $|I_>| = |I_|$，这对于奇数 $n$ 是不可能的。因此，唯一的最优 $\\gamma$ 是中位数，$\\gamma^\\star = y_{((n+1)/2)}$。\n\n- **如果 $n$ 是偶数**，令 $n=2k$。在区间 $[y_{(k)}, y_{(k+1)}]$ 内的任何值都是中位数。\n  - 如果我们选择 $\\gamma^\\star \\in (y_{(k)}, y_{(k+1)})$，那么 $|I_>| = k$， $|I_| = k$，且 $|I_0| = 0$。条件 $\\left| k - k \\right| \\le 0$ 成立。所以，任何这样的 $\\gamma^\\star$ 都是最优的。\n  - 在端点 $\\gamma^\\star = y_{(k)}$ 和 $\\gamma^\\star = y_{(k+1)}$ 处，条件也成立。例如，在 $\\gamma^\\star = y_{(k)}$ 处，我们有 $|I_|  k$， $|I_>| \\ge k$。该条件允许 $|I_>|$ 和 $|I_|$ 之间存在不平衡，这种不平衡被 $I_0$ 中的点“吸收”了。\n因此，对于偶数个样本，由两个中心顺序统计量构成的闭区间 $[y_{(n/2)}, y_{((n/2)+1)}]$ 内的任何值 $\\gamma^\\star$ 都是最优预测。这个区间就是问题中提到的 $[\\gamma_{\\mathrm{low}}, \\gamma_{\\mathrm{high}}]$。如果 $y_{(n/2)} = y_{((n/2)+1)}$，它就退化为一个单点。\n\n最后，我们分析参数向量 $\\beta^\\star$ 的非唯一性。最小化子的集合是 $S_\\beta = \\{\\beta \\in \\mathbb{R}^p \\mid x_0^\\top \\beta \\in [\\gamma_{\\mathrm{low}}, \\gamma_{\\mathrm{high}}]\\}$.\n1.  如果最优预测值的区间非退化（即 $\\gamma_{\\mathrm{low}}  \\gamma_{\\mathrm{high}}$），那么对于 $\\gamma = x_0^\\top \\beta$ 就有无限多个最优值。这直接意味着 $\\beta$ 有无限多个解。\n2.  即使最优预测 $\\gamma^\\star$ 是唯一的（$\\gamma_{\\mathrm{low}} = \\gamma_{\\mathrm{high}}$），最小化子集是 $S_\\beta = \\{\\beta \\in \\mathbb{R}^p \\mid x_0^\\top \\beta = \\gamma^\\star\\}$。这是一个关于 $\\beta$ 的线性方程。如果 $\\beta_p$ 是一个特解，那么通解是 $\\beta = \\beta_p + v$，其中 $v$ 是由 $x_0^\\top$ 定义的线性映射的零空间中的任意向量，即 $x_0^\\top v = 0$。这个零空间的维度是 $p - \\operatorname{rank}(x_0^\\top)$。由于 $X$ 的所有行都是 $x_0$，$\\operatorname{rank}(X) = \\operatorname{rank}(x_0^\\top) = 1$（对于 $x_0 \\neq 0$）。因此零空间的维度是 $d_{\\mathcal{N}} = p - \\operatorname{rank}(X) = p - 1$。如果 $p>1$，则 $d_{\\mathcal{N}} > 0$，即使 $\\gamma^\\star$ 是唯一的，$\\beta$ 也有无限多个解。\n\n总结来说，最小化子 $\\beta^\\star$ 的集合是非唯一的（$m_\\beta = \\text{True}$），如果最优预测区间的长度为正（$(\\gamma_{\\mathrm{high}} - \\gamma_{\\mathrm{low}}) > 0$），或者 $X$ 的零空间是非平凡的（$d_{\\mathcal{N}} > 0$）。\n\n### 第2部分：测试集的计算\n\n下面的 Python 代码实现了上述推导的逻辑，用于为每个测试用例计算所需的量。\n- 通过对 `y` 向量排序来找到中位数。\n- 零空间维度计算为 $p - \\operatorname{rank}(X)$。\n- 最小损失 $L_\\star$ 使用一个最优的 $\\gamma$（例如 $\\gamma_{\\mathrm{low}}$）来计算，因为在最优区间内损失是恒定的。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the LAD regression problem for multiple test cases, analyzing\n    the uniqueness of minimizers.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: even n, multiple optimal gamma, p=1\n        (np.array([[1], [1], [1], [1]], dtype=float), \n         np.array([0, 1, 3, 10], dtype=float)),\n        # Case 2: odd n, unique optimal gamma, p=1\n        (np.array([[1], [1], [1], [1], [1]], dtype=float), \n         np.array([0, 2, 2, 5, 9], dtype=float)),\n        # Case 3: even n, multiple optimal gamma, p=2 with dependent columns\n        (np.array([[1, 2], [1, 2], [1, 2], [1, 2]], dtype=float), \n         np.array([0, 1, 3, 10], dtype=float)),\n        # Case 4: all y_i equal, boundary behavior\n        (np.array([[1], [1], [1], [1]], dtype=float), \n         np.array([2, 2, 2, 2], dtype=float)),\n    ]\n\n    all_results = []\n    for X, y in test_cases:\n        n, p = X.shape\n        y_sorted = np.sort(y)\n\n        # 1. Compute the interval of optimal constant predictions [gamma_low, gamma_high]\n        if n % 2 == 1:\n            # For odd n, the median is unique\n            median_idx = (n - 1) // 2\n            gamma_low = y_sorted[median_idx]\n            gamma_high = y_sorted[median_idx]\n        else:\n            # For even n, the optimal interval is between the two middle elements\n            upper_median_idx = n // 2\n            lower_median_idx = upper_median_idx - 1\n            gamma_low = y_sorted[lower_median_idx]\n            gamma_high = y_sorted[upper_median_idx]\n            \n        # 2. Compute u_gamma: uniqueness of optimal gamma\n        u_gamma = (gamma_low == gamma_high)\n        \n        # 3. Compute d_N: nullspace dimension\n        rank_X = np.linalg.matrix_rank(X)\n        d_N = p - rank_X\n        \n        # 4. Compute m_beta: non-uniqueness of parameter-space minimizers\n        m_beta = (gamma_high > gamma_low) or (d_N > 0)\n        \n        # 5. Compute L_star: minimal absolute loss value\n        # The loss is constant on the interval [gamma_low, gamma_high].\n        # We can evaluate it at any point in the interval, e.g., gamma_low.\n        L_star = np.sum(np.abs(y - gamma_low))\n        \n        # Collect results for the current case\n        case_result = [\n            float(gamma_low),\n            float(gamma_high),\n            u_gamma,\n            int(d_N),\n            m_beta,\n            float(L_star)\n        ]\n        all_results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # Format each inner list as a string \"[v1,v2,...]\"\n    result_strings = [\n        f\"[{','.join(map(str, res))}]\"\n        for res in all_results\n    ]\n    \n    # Join the inner list strings into a final string \"[[...],[...],...]\"\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在许多现实世界的机器学习任务中，我们最终关心的性能指标（如目标检测中的交并比 $IoU$）往往是高度非凸和非平滑的，这使得直接优化它们变得异常困难。本练习  旨在揭示这一核心挑战，通过一个具体的例子展示 $IoU$ 损失的非凸性。然后，它引导我们转向实践中广泛使用的“代理损失”（surrogate losses），如Hinge损失和逻辑损失，并验证它们的凸性，从而理解为什么这些代理损失是构建可有效优化模型的基础。",
            "id": "3146363",
            "problem": "给定一个实数轴上的一维集合预测场景。真实集合 (ground-truth set) 是闭区间 $B = [0,1]$。一个模型预测的集合也是一个固定宽度 $w = 1$ 的闭区间，由其左端点 $\\theta \\in \\mathbb{R}$ 参数化为 $A_\\theta = [\\theta, \\theta + 1]$。交并比（Intersection over Union, IoU）损失定义为\n$$\n\\mathcal{L}_{\\mathrm{IoU}}(\\theta) \\;=\\; 1 \\;-\\; \\frac{|A_\\theta \\cap B|}{|A_\\theta \\cup B|},\n$$\n其中 $|\\cdot|$ 表示一维 Lebesgue 测度（长度）。对于 $w=1$ 和 $B=[0,1]$，这是一个关于 $\\theta$ 的精确的分段定义函数。\n\n你将分析 $\\mathcal{L}_{\\mathrm{IoU}}(\\theta)$ 的非凸性，然后为一个固定网格上的逐点分类问题构建凸代理损失，并最终展示凸性带来的优化优势。\n\n你可以依赖的基础知识：\n- 凸性的定义：一个函数 $f$ 是凸函数，当且仅当对于所有 $x, y$ 和所有 $t \\in [0,1]$，都有 $f(tx+(1-t)y) \\le t f(x) + (1-t) f(y)$。\n- 二元分类中合页损失（hinge loss）和逻辑斯谛损失（logistic loss）的标准定义，以及它们在线性模型参数下的凸性。\n- 牛顿法（Newton's method）的基本性质，以及线性模型中逻辑斯谛损失的海森矩阵（Hessian）的半正定性。\n\n需要实现和计算的任务：\n1) 通过显式计算交集长度 $|A_\\theta \\cap B|$ 和并集长度 $|A_\\theta \\cup B|$ 来定义 IoU 损失 $\\mathcal{L}_{\\mathrm{IoU}}(\\theta)$，计算公式为\n$$\n|A_\\theta \\cap B| \\;=\\; \\max\\bigl\\{0, \\min(\\theta+1,1) - \\max(\\theta,0)\\bigr\\}, \\qquad |A_\\theta \\cup B| \\;=\\; 1 + 1 - |A_\\theta \\cap B|.\n$$\n使用这个结果来为特定的测试用例 $(\\theta_1,\\theta_2,t) = (0,1,0.5)$ 评估凸性不等式。此测试所需的布尔值输出是\n$$\n\\mathcal{L}_{\\mathrm{IoU}}(t \\theta_1 + (1-t)\\theta_2) \\;\\le\\; t \\,\\mathcal{L}_{\\mathrm{IoU}}(\\theta_1) + (1-t)\\,\\mathcal{L}_{\\mathrm{IoU}}(\\theta_2)\n$$\n是否成立。该测试旨在通过展示一个特定的违例来揭示非凸性。\n\n2) 通过将区间 $[-1.5,1.5]$ 离散化为 $N=121$ 个等距点 $\\{x_i\\}_{i=1}^{N}$ 并分配标签\n$$\ny_i \\;=\\; \\begin{cases}\n+1,  \\text{if } x_i \\in [0,1],\\\\\n-1,  \\text{otherwise.}\n\\end{cases}\n$$\n来构建一个逐点的二元分类数据集。考虑一个带有分数 $s_{w,b}(x) = w x + b$ 的线性分类器，并定义经验合页损失\n$$\n\\mathcal{L}_{\\mathrm{hinge}}(w,b) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} \\max\\bigl\\{0,\\, 1 - y_i\\, (w x_i + b)\\bigr\\}。\n$$\n对于参数 $(w_1,b_1)=(4.0,-1.0)$，$(w_2,b_2)=(-2.0,0.5)$ 和 $t=0.5$，评估\n$$\n\\mathcal{L}_{\\mathrm{hinge}}(t w_1+(1-t)w_2,\\; t b_1+(1-t)b_2) \\;\\le\\; t \\,\\mathcal{L}_{\\mathrm{hinge}}(w_1,b_1) + (1-t)\\,\\mathcal{L}_{\\mathrm{hinge}}(w_2,b_2)\n$$\n是否成立。输出结果布尔值。\n\n3) 使用相同的数据集和线性分类器，定义经验逻辑斯谛损失\n$$\n\\mathcal{L}_{\\mathrm{log}}(w,b) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} \\log\\!\\bigl(1 + \\exp\\bigl(-y_i \\,(w x_i + b)\\bigr)\\bigr)。\n$$\n对于相同的参数对和 $t=0.5$，评估\n$$\n\\mathcal{L}_{\\mathrm{log}}(t w_1+(1-t)w_2,\\; t b_1+(1-t)b_2) \\;\\le\\; t \\,\\mathcal{L}_{\\mathrm{log}}(w_1,b_1) + (1-t)\\,\\mathcal{L}_{\\mathrm{log}}(w_2,b_2)\n$$\n是否成立。输出结果布尔值。\n\n4) 展示凸代理的优化优势：使用带回溯线搜索的牛顿法最小化 $\\mathcal{L}_{\\mathrm{log}}(w,b)$，其中 $(w,b) \\in \\mathbb{R}^2$，直到步长的欧几里得范数小于 $10^{-10}$ 或达到 $100$ 次迭代。从两个不同的初始值 $(w,b)=(0.0,0.0)$ 和 $(w,b)=(10.0,-10.0)$ 开始运行，并返回一个布尔值，指示两次运行是否收敛到本质上相同的解，即同时满足\n$$\n\\|\\,(w^{\\star}_A, b^{\\star}_A) - (w^{\\star}_B, b^{\\star}_B)\\,\\|_2   10^{-6}\n\\quad\\text{and}\\quad\n\\bigl|\\,\\mathcal{L}_{\\mathrm{log}}(w^{\\star}_A,b^{\\star}_A) - \\mathcal{L}_{\\mathrm{log}}(w^{\\star}_B,b^{\\star}_B)\\,\\bigr|   10^{-8}。\n$$\n\n5) 令 $(w^{\\star},b^{\\star})$ 为任务4中得到的最小化器。计算 $\\mathcal{L}_{\\mathrm{log}}(w,b)$ 在 $(w^{\\star},b^{\\star})$ 处的海森矩阵，并将其最小特征值作为一个浮点数返回。对于带有线性模型的逻辑斯谛损失，海森矩阵的形式为\n$$\nH \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} \\sigma\\!\\bigl(y_i\\, s_{w,b}(x_i)\\bigr)\\,\\sigma\\!\\bigl(-y_i\\, s_{w,b}(x_i)\\bigr)\\,\n\\begin{bmatrix}\nx_i^2  x_i\\\\\nx_i  1\n\\end{bmatrix},\n$$\n其中 $\\sigma(z)=\\frac{1}{1+\\exp(-z)}$ 表示逻辑斯谛 sigmoid 函数。一个非负的最小特征值在数值上证实了半正定性。\n\n测试套件和要求的输出：\n- 测试1：$(\\theta_1,\\theta_2,t)=(0,1,0.5)$ 用于 $\\mathcal{L}_{\\mathrm{IoU}}$ 凸性检查；输出一个布尔值。\n- 测试2：$(w_1,b_1)=(4.0,-1.0)$，$(w_2,b_2)=(-2.0,0.5)$，$t=0.5$ 用于 $\\mathcal{L}_{\\mathrm{hinge}}$ 凸性检查；输出一个布尔值。\n- 测试3：与测试2相同的参数，用于 $\\mathcal{L}_{\\mathrm{log}}$ 凸性检查；输出一个布尔值。\n- 测试4：从 $(0.0,0.0)$ 和 $(10.0,-10.0)$ 开始的牛顿法优化；输出一个布尔值，指示是否在指定容差内收敛到相同的解。\n- 测试5：任务4中优化器处 $\\mathcal{L}_{\\mathrm{log}}$ 的海森矩阵的最小特征值；输出一个浮点数。\n\n最终输出格式：\n你的程序应该生成单行输出，包含一个用方括号括起来的逗号分隔列表的结果（例如，\"[result1,result2,result3,result4,result5]\"），其中 result1 到 result4 是布尔值，result5 是一个浮点数。不应打印任何其他文本。",
            "solution": "问题陈述已经过仔细验证，并被确定为有效。它在科学上植根于优化和机器学习领域，问题定义良好、客观且内部一致。所有必要的定义、公式和参数都已提供，从而可以得出一个完整且无歧义的计算解。\n\n该问题要求进行一个多部分分析，比较非凸的交并比（$\\mathcal{L}_{\\mathrm{IoU}}$）损失与用于分类任务的凸代理损失（合页损失和逻辑斯谛损失），最终通过使用牛顿法展示凸性的优化优势。\n\n以下是每个任务的逐步推导和推理。\n\n### 任务1：IoU损失的凸性分析\n\n交并比损失定义为 $\\mathcal{L}_{\\mathrm{IoU}}(\\theta) = 1 - \\frac{|A_\\theta \\cap B|}{|A_\\theta \\cup B|}$，其中真实集合是 $B = [0,1]$，预测集合是 $A_\\theta = [\\theta, \\theta+1]$。交集的长度（Lebesgue 测度）由 $|A_\\theta \\cap B| = \\max\\{0, \\min(\\theta+1,1) - \\max(\\theta,0)\\}$ 给出，并集的长度是 $|A_\\theta \\cup B| = |A_\\theta| + |B| - |A_\\theta \\cap B| = 1 + 1 - |A_\\theta \\cap B| = 2 - |A_\\theta \\cap B|$。\n\n我们针对特定情况 $(\\theta_1, \\theta_2, t) = (0, 1, 0.5)$ 测试凸性不等式 $f(t x + (1-t) y) \\le t f(x) + (1-t) f(y)$。\n\n用于凸性检查的区间端点是 $\\theta_1 = 0$ 和 $\\theta_2 = 1$。中点是 $\\theta_m = t \\theta_1 + (1-t) \\theta_2 = 0.5 \\times 0 + 0.5 \\times 1 = 0.5$。\n\n我们在这三个点上评估损失：\n1.  对于 $\\theta_1 = 0$，预测集合是 $A_0 = [0,1]$。\n    - 交集: $|A_0 \\cap B| = |[0,1] \\cap [0,1]| = 1$。\n    - 并集: $|A_0 \\cup B| = |[0,1] \\cup [0,1]| = 1$。\n    - 损失: $\\mathcal{L}_{\\mathrm{IoU}}(0) = 1 - 1/1 = 0$。\n\n2.  对于 $\\theta_2 = 1$，预测集合是 $A_1 = [1,2]$。\n    - 交集: $|A_1 \\cap B| = |[1,2] \\cap [0,1]| = |\\{1\\}| = 0$。\n    - 并集: $|A_1 \\cup B| = |[0,1] \\cup [1,2]| = |[0,2]| = 2$。\n    - 损失: $\\mathcal{L}_{\\mathrm{IoU}}(1) = 1 - 0/2 = 1$。\n\n3.  对于中点 $\\theta_m = 0.5$，预测集合是 $A_{0.5} = [0.5, 1.5]$。\n    - 交集: $|A_{0.5} \\cap B| = |[0.5, 1.5] \\cap [0,1]| = |[0.5, 1]| = 0.5$。\n    - 并集: $|A_{0.5} \\cup B| = |[0, 1.5]| = 1.5$。\n    - 损失: $\\mathcal{L}_{\\mathrm{IoU}}(0.5) = 1 - 0.5 / 1.5 = 1 - 1/3 = 2/3$。\n\n现在，我们检查凸性不等式：\n$\\mathcal{L}_{\\mathrm{IoU}}(\\theta_m) \\le t \\mathcal{L}_{\\mathrm{IoU}}(\\theta_1) + (1-t) \\mathcal{L}_{\\mathrm{IoU}}(\\theta_2)$\n$2/3 \\le 0.5 \\times 0 + 0.5 \\times 1$\n$0.666... \\le 0.5$\n\n这个不等式是错误的。这表明 $\\mathcal{L}_{\\mathrm{IoU}}(\\theta)$ 不是一个凸函数，因为我们找到了一个具体的反例。要求的输出是 `False`。\n\n### 任务2和3：合页损失和逻辑斯谛损失的凸性\n\n对于这些任务，我们通过在 $[-1.5, 1.5]$ 上采样 $N=121$ 个点 $\\{x_i\\}$ 并分配标签 $y_i = +1$（对于 $x_i \\in [0,1]$）和 $y_i = -1$（其他情况）来构建一个逐点的二元分类数据集。我们考虑一个带有分数 $s_{w,b}(x) = w x + b$ 的线性分类器。\n\n经验合页损失 $\\mathcal{L}_{\\mathrm{hinge}}(w,b)$ 和逻辑斯谛损失 $\\mathcal{L}_{\\mathrm{log}}(w,b)$ 被定义为数据集上的总和：\n$$ \\mathcal{L}_{\\mathrm{hinge}}(w,b) = \\frac{1}{N}\\sum_{i=1}^{N} \\max\\bigl\\{0, 1 - y_i (w x_i + b)\\bigr\\} $$\n$$ \\mathcal{L}_{\\mathrm{log}}(w,b) = \\frac{1}{N}\\sum_{i=1}^{N} \\log\\bigl(1 + \\exp\\bigl(-y_i (w x_i + b)\\bigr)\\bigr) $$\n\n合页损失函数 $f(z) = \\max(0, 1-z)$ 和对数损失函数 $f(z) = \\log(1+\\exp(-z))$ 都是关于其参数 $z$ 的凸函数。当参数 $z$ 是参数 $(w,b)$ 的线性函数时，如 $z_i = y_i (w x_i + b)$，得到的损失函数 $\\mathcal{L}_{\\mathrm{hinge}}(w,b)$ 和 $\\mathcal{L}_{\\mathrm{log}}(w,b)$ 关于 $(w,b)$ 是凸的。这是因为与仿射映射的复合保留了凸性，并且凸函数的和也是凸的。\n\n因此，根据凸性的定义，不等式 $L(t \\theta_1 + (1-t)\\theta_2) \\le t L(\\theta_1) + (1-t)L(\\theta_2)$ 对于任何参数 $\\theta_1=(w_1, b_1)$、$\\theta_2=(w_2, b_2)$ 和任何 $t \\in [0,1]$ 的选择都必须成立。问题要求对特定参数 $(w_1, b_1) = (4.0, -1.0)$、$(w_2, b_2) = (-2.0, 0.5)$ 和 $t=0.5$ 进行验证。由于已知这两个损失都是凸的，所以在两种情况下不等式都将满足。任务2和任务3的要求输出都是 `True`。\n\n### 任务4：通过牛顿法进行优化\n\n这个任务展示了凸优化的一个关键优势：无论从哪个起始点开始，都能收敛到唯一的全局最小值。我们被要求使用牛顿法从两个不同的初始值 $(w,b)=(0.0,0.0)$ 和 $(w,b)=(10.0,-10.0)$ 开始，最小化严格凸的逻辑斯谛损失 $\\mathcal{L}_{\\mathrm{log}}(w,b)$。\n\n牛顿法通过使用二阶泰勒近似来迭代地寻找函数的最小值。参数 $\\mathbf{p} = [w,b]^T$ 的更新步骤是：\n$$ \\mathbf{p}_{k+1} = \\mathbf{p}_k - \\lambda_k H_k^{-1} g_k $$\n其中 $g_k = \\nabla \\mathcal{L}_{\\mathrm{log}}(\\mathbf{p}_k)$ 是梯度， $H_k = \\nabla^2 \\mathcal{L}_{\\mathrm{log}}(\\mathbf{p}_k)$ 是海森矩阵，$\\lambda_k$ 是通过回溯线搜索确定的步长，以确保损失充分下降。\n\n因为对于这个数据集（特征 $[x_i, 1]^T$ 不是共线的），$\\mathcal{L}_{\\mathrm{log}}(w,b)$ 是严格凸的，所以它拥有一个唯一的全局最小值 $(w^\\star, b^\\star)$。牛顿法在最小值附近具有二次收敛速度，是一种强大的算法，保证能从任何合理的起始点收敛到这个唯一的优化解。因此，我们预期从 $(0.0,0.0)$ 和 $(10.0,-10.0)$ 开始的优化运行将产生在数值精度内相同的解 $(w_A^\\star, b_A^\\star)$ 和 $(w_B^\\star, b_B^\\star)$。根据给定的参数和损失值容差定义的两次运行是否收敛到相同解的测试应该会通过。要求的输出是 `True`。\n\n### 任务5：最优点处的海森矩阵分析\n\n最后一个任务是在找到的最小化器 $(w^\\star,b^\\star)$ 处计算 $\\mathcal{L}_{\\mathrm{log}}(w,b)$ 的海森矩阵，并确定其最小特征值。海森矩阵的公式是：\n$$ H = \\frac{1}{N}\\sum_{i=1}^{N} \\sigma(z_i)\\sigma(-z_i) \\begin{bmatrix} x_i^2  x_i \\\\ x_i  1 \\end{bmatrix}, \\quad \\text{where } z_i = y_i(w x_i + b) $$\n且 $\\sigma(z) = (1+\\exp(-z))^{-1}$ 是 sigmoid 函数。项 $\\sigma(z_i)\\sigma(-z_i)$ 是 sigmoid 函数的导数，并且总是正的。矩阵 $\\begin{psmallmatrix} x_i^2  x_i \\\\ x_i  1 \\end{psmallmatrix}$ 是外积 $\\mathbf{v}_i \\mathbf{v}_i^T$（其中 $\\mathbf{v}_i = [x_i, 1]^T$），它是半正定的。\n\n由于数据点 $x_i$ 不完全相同，向量 $\\mathbf{v}_i$ 张成了整个二维参数空间。海森矩阵是半正定矩阵的和，并且因为向量 $\\mathbf{v}_i$ 张成了该空间且系数 $\\sigma(z_i)\\sigma(-z_i)$ 是正的，所以最终的海森矩阵 $H$ 是正定的。\n\n正定矩阵的一个基本性质是其所有特征值都严格为正。因此，计算最小化器处海森矩阵的最小特征值，可以作为该点处损失函数严格凸性的数值确认。我们预期结果是一个正的浮点数。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Executes all tasks described in the problem statement and prints the results.\n    \"\"\"\n\n    results = []\n\n    # --- Task 1: Convexity Analysis of IoU Loss ---\n    def iou_loss(theta):\n        # Ground-truth set B = [0,1], Predicted set A = [theta, theta+1]\n        intersection = max(0.0, min(theta + 1.0, 1.0) - max(theta, 0.0))\n        # Union = |A| + |B| - Intersection = 1 + 1 - intersection\n        union = 2.0 - intersection\n        if union == 0.0:  # This happens only if intersection is 2.0, impossible.\n            return 0.0 if intersection == 1.0 else 1.0 # Perfect match - loss 0\n        return 1.0 - intersection / union\n\n    theta1, theta2, t = 0.0, 1.0, 0.5\n    theta_m = t * theta1 + (1.0 - t) * theta2\n    \n    loss_m = iou_loss(theta_m)\n    loss_1 = iou_loss(theta1)\n    loss_2 = iou_loss(theta2)\n    \n    is_convex_iou = loss_m = t * loss_1 + (1.0 - t) * loss_2\n    results.append(is_convex_iou)\n\n    # --- Setup for Tasks 2-5 ---\n    N = 121\n    x_data = np.linspace(-1.5, 1.5, N)\n    y_data = np.ones(N)\n    y_data[(x_data  0) | (x_data  1)] = -1.0\n\n    # --- Task 2: Convexity of Hinge Loss ---\n    def hinge_loss(w, b, x, y):\n        margins = y * (w * x + b)\n        losses = np.maximum(0.0, 1.0 - margins)\n        return np.mean(losses)\n\n    w1, b1 = 4.0, -1.0\n    w2, b2 = -2.0, 0.5\n    wm = t * w1 + (1.0 - t) * w2\n    bm = t * b1 + (1.0 - t) * b2\n\n    hinge_m = hinge_loss(wm, bm, x_data, y_data)\n    hinge_1 = hinge_loss(w1, b1, x_data, y_data)\n    hinge_2 = hinge_loss(w2, b2, x_data, y_data)\n    \n    is_convex_hinge = hinge_m = t * hinge_1 + (1.0 - t) * hinge_2\n    results.append(is_convex_hinge)\n\n    # --- Task 3: Convexity of Logistic Loss ---\n    def logistic_loss(w, b, x, y):\n        # Use numerically stable log(1+exp(z)) = logaddexp(0, z)\n        z = -y * (w * x + b)\n        losses = np.logaddexp(0, z)\n        return np.mean(losses)\n\n    log_m = logistic_loss(wm, bm, x_data, y_data)\n    log_1 = logistic_loss(w1, b1, x_data, y_data)\n    log_2 = logistic_loss(w2, b2, x_data, y_data)\n    \n    is_convex_log = log_m = t * log_1 + (1.0 - t) * log_2\n    results.append(is_convex_log)\n\n    # --- Functions for Task 4  5 ---\n    def log_loss_grad_hess(w, b, x, y):\n        n_points = len(x)\n        scores = w * x + b\n        z = y * scores\n        \n        # Gradient\n        sigma_neg_z = expit(-z)\n        grad_w = np.sum(sigma_neg_z * (-y * x)) / n_points\n        grad_b = np.sum(sigma_neg_z * (-y)) / n_points\n        grad = np.array([grad_w, grad_b])\n        \n        # Hessian\n        sigma_z = expit(z)\n        coeffs = sigma_z * sigma_neg_z\n        H_ww = np.sum(coeffs * x**2) / n_points\n        H_wb = np.sum(coeffs * x) / n_points\n        H_bb = np.sum(coeffs) / n_points\n        H = np.array([[H_ww, H_wb], [H_wb, H_bb]])\n        \n        return grad, H\n        \n    def newton_minimize(initial_params, x, y):\n        params = np.array(initial_params, dtype=float)\n        max_iter = 100\n        step_norm_tol = 1e-10\n        alpha, beta = 0.25, 0.5\n      \n        for _ in range(max_iter):\n            w, b = params\n            grad, H = log_loss_grad_hess(w, b, x, y)\n            \n            try:\n                # Solve H * step = -grad for the Newton step\n                step = np.linalg.solve(H, -grad)\n            except np.linalg.LinAlgError:\n                # Hessian is singular, should not happen for this problem\n                break\n\n            # Backtracking line search\n            line_search_t = 1.0\n            current_loss = logistic_loss(w, b, x, y)\n            armijo_term = alpha * np.dot(grad, step)\n            \n            while True:\n                next_w, next_b = params + line_search_t * step\n                if logistic_loss(next_w, next_b, x, y) = current_loss + line_search_t * armijo_term:\n                    break\n                line_search_t *= beta\n                if line_search_t  1e-15:  # Failsafe\n                    break\n            \n            final_step = line_search_t * step\n            params += final_step\n            \n            if np.linalg.norm(final_step)  step_norm_tol:\n                break\n                \n        return params\n\n    # --- Task 4: Newton's Method Convergence ---\n    init_A = (0.0, 0.0)\n    init_B = (10.0, -10.0)\n    \n    sol_A = newton_minimize(init_A, x_data, y_data)\n    sol_B = newton_minimize(init_B, x_data, y_data)\n    \n    wA, bA = sol_A\n    wB, bB = sol_B\n    loss_A = logistic_loss(wA, bA, x_data, y_data)\n    loss_B = logistic_loss(wB, bB, x_data, y_data)\n    \n    param_dist = np.linalg.norm(sol_A - sol_B)\n    loss_diff = abs(loss_A - loss_B)\n    \n    converged_to_same = (param_dist  1e-6) and (loss_diff  1e-8)\n    results.append(converged_to_same)\n\n    # --- Task 5: Hessian Eigenvalue at Minimum ---\n    w_star, b_star = sol_A  # Both solutions should be the same\n    _, H_star = log_loss_grad_hess(w_star, b_star, x_data, y_data)\n    eigenvalues = np.linalg.eigvalsh(H_star)\n    min_eigenvalue = np.min(eigenvalues)\n    results.append(min_eigenvalue)\n    \n    # --- Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在构建强大的机器学习模型时，我们常常需要将拟合数据的损失函数（如逻辑损失）与旨在防止过拟合并鼓励稀疏性的正则化项（如弹性网络）结合起来。这种组合目标函数包含一个平滑部分和一个非平滑部分，需要比标准梯度下降更强大的优化算法 。本练习将指导你推导并实现弹性网络正则化项的“邻近算子”（proximal operator），并将其应用于求解逻辑回归问题的邻近梯度法，这是解决此类复合优化问题的核心技术。",
            "id": "3146352",
            "problem": "您需要推导一个邻近映射，并为带有弹性网络正则化的二元逻辑回归中的一个复合目标实现邻近梯度法。核心目标函数是\n$$\nF(\\boldsymbol{\\theta}) \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\log\\!\\big(1+\\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\theta})\\big) - y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right)\\;+\\;\\lambda_1 \\|\\boldsymbol{\\theta}\\|_1\\;+\\;\\frac{\\lambda_2}{2}\\|\\boldsymbol{\\theta}\\|_2^2,\n$$\n其中 $\\boldsymbol{\\theta}\\in\\mathbb{R}^d$，$\\mathbf{x}_i\\in\\mathbb{R}^d$，$y_i\\in\\{0,1\\}$，$\\lambda_1\\ge 0$，$\\lambda_2\\ge 0$，$n$ 是样本数量。\n\n任务：\n1) 仅从邻近算子的定义以及凸函数和次梯度的基本性质出发，推导邻近映射的闭式表达式\n$$\n\\operatorname{prox}_{t g}(\\mathbf{v})\\;=\\;\\underset{\\boldsymbol{\\theta}\\in\\mathbb{R}^d}{\\arg\\min}\\;\\left\\{t\\left(\\lambda_1\\|\\boldsymbol{\\theta}\\|_1+\\frac{\\lambda_2}{2}\\|\\boldsymbol{\\theta}\\|_2^2\\right)+\\frac{1}{2}\\|\\boldsymbol{\\theta}-\\mathbf{v}\\|_2^2\\right\\},\n$$\n对于任意步长 $t0$ 和任意 $\\mathbf{v}\\in\\mathbb{R}^d$。您的推导必须从邻近算子的定义开始，并且只使用基本的凸分析（例如，可分性和次梯度最优性条件）。不要假设公式；相反，要证明它为何成立。\n\n2) 实现邻近梯度法来最小化 $F(\\boldsymbol{\\theta})$，将逻辑损失视为光滑部分，将弹性网络项视为由邻近映射处理的非光滑部分。使用以下原则作为您的出发点：\n- 光滑部分 $f(\\boldsymbol{\\theta})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\log(1+\\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\theta})) - y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right)$ 的梯度是\n$$\n\\nabla f(\\boldsymbol{\\theta})=\\frac{1}{n}\\mathbf{X}^\\top\\left(\\boldsymbol{\\sigma}(\\mathbf{X}\\boldsymbol{\\theta})-\\mathbf{y}\\right),\n$$\n其中 $\\boldsymbol{\\sigma}(\\mathbf{z})$ 将 $\\sigma(z)=\\frac{1}{1+\\exp(-z)}$ 逐元素应用。\n- $\\nabla f$ 的一个有效的全局 Lipschitz 常数是 $L\\le \\frac{1}{4n}\\|\\mathbf{X}\\|_2^2$，其中 $\\|\\mathbf{X}\\|_2$ 是 $\\mathbf{X}$ 的谱范数。当 $\\|\\mathbf{X}\\|_20$ 时，使用步长 $t=\\frac{1}{L}$。如果 $\\|\\mathbf{X}\\|_2=0$，则使用 $t=1$。\n\n您的实现必须执行 $K$ 次固定的邻近梯度迭代：\n$$\n\\boldsymbol{\\theta}^{k+1}=\\operatorname{prox}_{t g}\\!\\left(\\boldsymbol{\\theta}^k - t\\,\\nabla f(\\boldsymbol{\\theta}^k)\\right),\n$$\n从一个指定的初始向量 $\\boldsymbol{\\theta}^0$ 开始（如果未另外指定，则默认为零向量）。\n\n测试套件和所需输出：\n实现您的程序以计算以下测试用例的输出。在所有情况下，当您被要求报告一个浮点数或一个浮点数列表时，请将每个值四舍五入到恰好 $6$ 位小数。\n\n- 测试用例 A（邻近映射检查）：\n  - 参数：$\\mathbf{v}=[-0.5,\\,0.2,\\,3.0]$，$t=0.5$，$\\lambda_1=0.3$，$\\lambda_2=0.4$。\n  - 输出：向量 $\\operatorname{prox}_{t g}(\\mathbf{v})$，以包含 $3$ 个浮点数的列表形式。\n\n- 测试用例 B（正常路径优化）：\n  - 数据：\n    $$\n    \\mathbf{X}=\\begin{bmatrix}\n    0.5  1.0\\\\\n    1.5  2.0\\\\\n    -1.0  -0.5\\\\\n    -1.5  -1.0\\\\\n    0.3  -0.2\\\\\n    -0.3  0.2\n    \\end{bmatrix},\\quad\n    \\mathbf{y}=\\begin{bmatrix}1\\\\1\\\\0\\\\0\\\\1\\\\0\\end{bmatrix}.\n    $$\n  - 正则化：$\\lambda_1=0.1$，$\\lambda_2=0.2$。\n  - 迭代次数：$K=300$。\n  - 初始化：$\\boldsymbol{\\theta}^0=\\mathbf{0}$。\n  - 输出：最终目标值 $F(\\boldsymbol{\\theta}^K)$，作为一个浮点数。\n\n- 测试用例 C（仅岭回归边界）：\n  - 与测试用例 B 中相同的 $\\mathbf{X}$ 和 $\\mathbf{y}$。\n  - 正则化：$\\lambda_1=0$，$\\lambda_2=0.5$。\n  - 迭代次数：$K=300$。\n  - 初始化：$\\boldsymbol{\\theta}^0=\\mathbf{0}$。\n  - 输出：最终目标值 $F(\\boldsymbol{\\theta}^K)$，作为一个浮点数。\n\n- 测试用例 D（仅 LASSO 边界）：\n  - 与测试用例 B 中相同的 $\\mathbf{X}$ 和 $\\mathbf{y}$。\n  - 正则化：$\\lambda_1=0.5$，$\\lambda_2=0$。\n  - 迭代次数：$K=400$。\n  - 初始化：$\\boldsymbol{\\theta}^0=\\mathbf{0}$。\n  - 输出：最终目标值 $F(\\boldsymbol{\\theta}^K)$，作为一个浮点数。\n\n- 测试用例 E（零设计矩阵和非零初始化的边界情况）：\n  - 数据：\n    $$\n    \\mathbf{X}=\\begin{bmatrix}\n    0  0\\\\\n    0  0\\\\\n    0  0\n    \\end{bmatrix},\\quad\n    \\mathbf{y}=\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}.\n    $$\n  - 正则化：$\\lambda_1=0.2$，$\\lambda_2=0.1$。\n  - 迭代次数：$K=5$。\n  - 初始化：$\\boldsymbol{\\theta}^0=\\begin{bmatrix}1.0\\\\-1.0\\end{bmatrix}$。\n  - 对于此情况，使用上述步长规则；由于 $\\|\\mathbf{X}\\|_2=0$，您必须使用 $t=1$。\n  - 输出：最终参数向量 $\\boldsymbol{\\theta}^K$，以包含 $2$ 个浮点数的列表形式。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含按以下顺序排列的逗号分隔列表，并用方括号括起来：$\\left[\\text{A},\\text{B},\\text{C},\\text{D},\\text{E}\\right]$，其中 $\\text{A}$ 是测试用例 A 的列表，$\\text{B}$ 是测试用例 B 的浮点数，依此类推。例如：$[\\text{list},\\text{float},\\text{float},\\text{float},\\text{list}]$。所有浮点数必须按上述规定四舍五入到恰好 $6$ 位小数。",
            "solution": "我们从邻近算子的定义开始。对于一个正常、闭、凸函数 $g:\\mathbb{R}^d\\to\\mathbb{R}\\cup\\{+\\infty\\}$ 以及任意 $t0$，\n$$\n\\operatorname{prox}_{t g}(\\mathbf{v}) \\;=\\; \\underset{\\boldsymbol{\\theta}\\in\\mathbb{R}^d}{\\arg\\min}\\;\\left\\{t\\,g(\\boldsymbol{\\theta}) + \\frac{1}{2}\\|\\boldsymbol{\\theta}-\\mathbf{v}\\|_2^2\\right\\}.\n$$\n这里 $g(\\boldsymbol{\\theta})=\\lambda_1\\|\\boldsymbol{\\theta}\\|_1+\\frac{\\lambda_2}{2}\\|\\boldsymbol{\\theta}\\|_2^2$ 且 $\\lambda_1\\ge 0, \\lambda_2\\ge 0$。邻近定义中的目标函数是\n$$\nQ(\\boldsymbol{\\theta}) \\;=\\; t\\lambda_1\\|\\boldsymbol{\\theta}\\|_1 + \\frac{t\\lambda_2}{2}\\|\\boldsymbol{\\theta}\\|_2^2 + \\frac{1}{2}\\|\\boldsymbol{\\theta}-\\mathbf{v}\\|_2^2.\n$$\n根据可分性，$Q(\\boldsymbol{\\theta})$ 可以按坐标分解：$Q(\\boldsymbol{\\theta})=\\sum_{j=1}^{d} q_j(\\theta_j)$，其中\n$$\nq_j(u)= t\\lambda_1 |u| + \\frac{t\\lambda_2}{2} u^2 + \\frac{1}{2}(u - v_j)^2.\n$$\n因此，$\\operatorname{prox}_{t g}(\\mathbf{v})$ 可以通过独立最小化每个 $q_j(u)$ 来获得。我们求解标量 $u$。\n\n考虑凸函数次梯度的最优性条件：$0\\in \\partial q_j(u^\\star)$。$q_j$ 的次梯度是\n$$\n\\partial q_j(u) \\;=\\; t\\lambda_1\\,\\partial |u| + t\\lambda_2 u + (u - v_j),\n$$\n其中，当 $u\\neq 0$ 时 $\\partial |u|=\\{\\operatorname{sign}(u)\\}$，当 $u=0$ 时 $\\partial |u|=[-1,1]$。\n\n情况 $u^\\star\\neq 0$：此时 $0 = t\\lambda_1 \\operatorname{sign}(u^\\star) + (t\\lambda_2+1)u^\\star - v_j$，得到\n$$\nu^\\star \\;=\\; \\frac{v_j - t\\lambda_1 \\operatorname{sign}(u^\\star)}{1 + t\\lambda_2}.\n$$\n一致性要求 $\\operatorname{sign}(u^\\star)=\\operatorname{sign}(v_j - t\\lambda_1 \\operatorname{sign}(u^\\star))$，这在 $|v_j|t\\lambda_1$ 且 $u^\\star$ 与 $v_j$ 符号相同时成立。\n\n情况 $u^\\star=0$：此时次梯度条件要求 $0\\in t\\lambda_1 [-1,1] - v_j$，等价于 $|v_j|\\le t\\lambda_1$。\n\n结合这两种情况可以得到软阈值结构。定义软阈值算子 $S_\\alpha:\\mathbb{R}\\to\\mathbb{R}$ 为\n$$\nS_\\alpha(v) \\;=\\; \\operatorname{sign}(v)\\,\\max\\{|v|-\\alpha,\\,0\\}.\n$$\n根据以上情况，解为\n$$\nu^\\star \\;=\\; \\frac{1}{1+t\\lambda_2}\\,S_{t\\lambda_1}(v_j),\n$$\n这可以逐元素地推广到向量：\n$$\n\\operatorname{prox}_{t g}(\\mathbf{v}) \\;=\\; \\frac{1}{1+t\\lambda_2}\\,S_{t\\lambda_1}(\\mathbf{v}),\n$$\n其中 $S_{t\\lambda_1}$ 是逐分量应用的。当 $\\lambda_2=0$ 时，此表达式简化为标准软阈值；当 $\\lambda_1=0$ 时，简化为简单的缩放 $\\frac{1}{1+t\\lambda_2}\\mathbf{v}$。\n\n接下来，我们实现邻近梯度法来最小化 $F(\\boldsymbol{\\theta})=f(\\boldsymbol{\\theta})+g(\\boldsymbol{\\theta})$，其中\n$$\nf(\\boldsymbol{\\theta})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\log(1+\\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\theta})) - y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right),\\quad\ng(\\boldsymbol{\\theta})=\\lambda_1\\|\\boldsymbol{\\theta}\\|_1+\\frac{\\lambda_2}{2}\\|\\boldsymbol{\\theta}\\|_2^2.\n$$\n$f$ 的梯度由下式给出\n$$\n\\nabla f(\\boldsymbol{\\theta})=\\frac{1}{n}\\mathbf{X}^\\top(\\boldsymbol{\\sigma}(\\mathbf{X}\\boldsymbol{\\theta})-\\mathbf{y}),\n$$\n其中 $\\sigma(z)=\\frac{1}{1+\\exp(-z)}$。$f$ 的 Hessian 矩阵是\n$$\n\\nabla^2 f(\\boldsymbol{\\theta})=\\frac{1}{n}\\mathbf{X}^\\top \\mathbf{D}(\\boldsymbol{\\theta}) \\mathbf{X},\n$$\n其中 $\\mathbf{D}(\\boldsymbol{\\theta})=\\operatorname{diag}(\\sigma(z_i)(1-\\sigma(z_i)))$ 且 $z_i=\\mathbf{x}_i^\\top\\boldsymbol{\\theta}$。由于 $0\\le \\sigma(z)(1-\\sigma(z))\\le \\frac{1}{4}$，我们有一个统一的上界\n$$\n\\nabla^2 f(\\boldsymbol{\\theta}) \\preceq \\frac{1}{4n}\\mathbf{X}^\\top \\mathbf{X},\n$$\n这意味着 $\\nabla f$ 是 Lipschitz 连续的，其 Lipschitz 常数 $L\\le \\frac{1}{4n}\\|\\mathbf{X}\\|_2^2$。因此，当 $\\|\\mathbf{X}\\|_20$ 时，一个有效的固定步长是 $t=\\frac{1}{L}=\\frac{4n}{\\|\\mathbf{X}\\|_2^2}$。如果 $\\|\\mathbf{X}\\|_2=0$，那么对所有 $\\boldsymbol{\\theta}$ 都有 $\\nabla f(\\boldsymbol{\\theta})=\\mathbf{0}$，任何正步长都是可接受的；为具体起见，我们指定 $t=1$。\n\n邻近梯度迭代步骤则为\n$$\n\\boldsymbol{\\theta}^{k+1}=\\operatorname{prox}_{t g}\\!\\left(\\boldsymbol{\\theta}^k - t\\,\\nabla f(\\boldsymbol{\\theta}^k)\\right)\n= \\frac{1}{1+t\\lambda_2}\\,S_{t\\lambda_1}\\!\\left(\\boldsymbol{\\theta}^k - t\\,\\nabla f(\\boldsymbol{\\theta}^k)\\right).\n$$\n\n算法步骤：\n- 计算 $n$ 和 $d$，并初始化 $\\boldsymbol{\\theta}^0$。\n- 计算 $\\|\\mathbf{X}\\|_2$（谱范数），如果 $\\|\\mathbf{X}\\|_20$，则设置 $t=\\frac{4n}{\\|\\mathbf{X}\\|_2^2}$，否则 $t=1$。\n- 对于 $k=0,1,\\dots,K-1$：\n  - 计算 $\\nabla f(\\boldsymbol{\\theta}^k)=\\frac{1}{n}\\mathbf{X}^\\top(\\boldsymbol{\\sigma}(\\mathbf{X}\\boldsymbol{\\theta}^k)-\\mathbf{y})$。\n  - 形成梯度步 $\\mathbf{v}^k=\\boldsymbol{\\theta}^k - t\\,\\nabla f(\\boldsymbol{\\theta}^k)$。\n  - 应用邻近算子：$\\boldsymbol{\\theta}^{k+1}=\\frac{1}{1+t\\lambda_2}\\,S_{t\\lambda_1}(\\mathbf{v}^k)$。\n\n为了报告结果，我们评估目标函数\n$$\nF(\\boldsymbol{\\theta})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\log(1+\\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\theta})) - y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right)+\\lambda_1\\|\\boldsymbol{\\theta}\\|_1+\\frac{\\lambda_2}{2}\\|\\boldsymbol{\\theta}\\|_2^2.\n$$\n\n测试套件包括：\n- 一次邻近映射检查，以数值方式验证推导的公式。\n- 一个 $\\lambda_10$ 和 $\\lambda_20$ 的通用情况。\n- 边界情况 $\\lambda_1=0$（仅岭回归）和 $\\lambda_2=0$（仅 LASSO）。\n- 一个 $\\mathbf{X}=\\mathbf{0}$ 且非零初始化的边界情况，以检验在光滑梯度为零时邻近算子的动态。\n\n所有输出都四舍五入到恰好 $6$ 位小数，并汇总成所需的单行列表格式。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigmoid(z):\n    # Numerically stable sigmoid\n    # For large negative values, exp(-z) may overflow; using standard implementation is fine for small test sizes\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef soft_threshold(v, alpha):\n    # Elementwise soft-thresholding: sign(v) * max(|v| - alpha, 0)\n    return np.sign(v) * np.maximum(np.abs(v) - alpha, 0.0)\n\ndef prox_elastic_net(v, t, lam1, lam2):\n    # prox_{t (lam1 ||.||_1 + (lam2/2)||.||_2^2)}(v) = (1 / (1 + t*lam2)) * S_{t*lam1}(v)\n    if t  0:\n        raise ValueError(\"Step size t must be nonnegative\")\n    if lam1  0 or lam2  0:\n        raise ValueError(\"Regularization parameters must be nonnegative\")\n    if t == 0:\n        return v.copy()\n    shrink = soft_threshold(v, t * lam1)\n    if lam2 == 0.0:\n        return shrink\n    return shrink / (1.0 + t * lam2)\n\ndef logistic_objective(X, y, theta, lam1, lam2):\n    n = X.shape[0]\n    z = X @ theta\n    # softplus: log(1 + exp(z))\n    # Use stable computation\n    # softplus(z) = max(z,0) + log(1 + exp(-|z|))\n    softplus = np.maximum(z, 0.0) + np.log1p(np.exp(-np.abs(z)))\n    data_term = (softplus - y * z).mean()\n    reg = lam1 * np.linalg.norm(theta, 1) + 0.5 * lam2 * np.dot(theta, theta)\n    return data_term + reg\n\ndef grad_logistic(X, y, theta):\n    n = X.shape[0]\n    z = X @ theta\n    s = sigmoid(z)\n    grad = (X.T @ (s - y)) / n\n    return grad\n\ndef spectral_norm(X):\n    # Compute spectral norm (2-norm). For small matrices, SVD is fine.\n    if X.size == 0:\n        return 0.0\n    return np.linalg.norm(X, 2)\n\ndef prox_gradient(X, y, lam1, lam2, K, theta0=None, t=None):\n    n, d = X.shape\n    if theta0 is None:\n        theta = np.zeros(d)\n    else:\n        theta = np.array(theta0, dtype=float).copy()\n    if t is None:\n        norm2 = spectral_norm(X)\n        if norm2 > 0:\n            L = (norm2 ** 2) / (4.0 * n)\n            t = 1.0 / L\n        else:\n            t = 1.0\n    # Iterate\n    for _ in range(K):\n        g = grad_logistic(X, y, theta)\n        v = theta - t * g\n        theta = prox_elastic_net(v, t, lam1, lam2)\n    return theta\n\ndef round_list(vals, decimals=6):\n    return [float(f\"{v:.{decimals}f}\") for v in vals]\n\ndef solve():\n    results = []\n\n    # Test case A: proximal mapping check\n    v_A = np.array([-0.5, 0.2, 3.0], dtype=float)\n    t_A = 0.5\n    lam1_A = 0.3\n    lam2_A = 0.4\n    prox_A = prox_elastic_net(v_A, t_A, lam1_A, lam2_A)\n    results.append(str(round_list(prox_A.tolist(), 6)))\n\n    # Shared dataset for B, C, D\n    X_shared = np.array([\n        [0.5, 1.0],\n        [1.5, 2.0],\n        [-1.0, -0.5],\n        [-1.5, -1.0],\n        [0.3, -0.2],\n        [-0.3, 0.2],\n    ], dtype=float)\n    y_shared = np.array([1, 1, 0, 0, 1, 0], dtype=float)\n\n    # Test case B: happy path\n    lam1_B, lam2_B = 0.1, 0.2\n    K_B = 300\n    theta_B = prox_gradient(X_shared, y_shared, lam1_B, lam2_B, K_B, theta0=np.zeros(2))\n    obj_B = logistic_objective(X_shared, y_shared, theta_B, lam1_B, lam2_B)\n    results.append(f\"{obj_B:.6f}\")\n\n    # Test case C: ridge-only boundary\n    lam1_C, lam2_C = 0.0, 0.5\n    K_C = 300\n    theta_C = prox_gradient(X_shared, y_shared, lam1_C, lam2_C, K_C, theta0=np.zeros(2))\n    obj_C = logistic_objective(X_shared, y_shared, theta_C, lam1_C, lam2_C)\n    results.append(f\"{obj_C:.6f}\")\n\n    # Test case D: lasso-only boundary\n    lam1_D, lam2_D = 0.5, 0.0\n    K_D = 400\n    theta_D = prox_gradient(X_shared, y_shared, lam1_D, lam2_D, K_D, theta0=np.zeros(2))\n    obj_D = logistic_objective(X_shared, y_shared, theta_D, lam1_D, lam2_D)\n    results.append(f\"{obj_D:.6f}\")\n\n    # Test case E: zero design matrix and nonzero initialization\n    X_E = np.zeros((3, 2), dtype=float)\n    y_E = np.array([0.0, 1.0, 0.0], dtype=float)\n    lam1_E, lam2_E = 0.2, 0.1\n    K_E = 5\n    theta0_E = np.array([1.0, -1.0], dtype=float)\n    # For zero matrix, step size defaults to t=1 inside prox_gradient\n    theta_E = prox_gradient(X_E, y_E, lam1_E, lam2_E, K_E, theta0=theta0_E)\n    results.append(str(round_list(theta_E.tolist(), 6)))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}