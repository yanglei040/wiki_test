{
    "hands_on_practices": [
        {
            "introduction": "While squared error loss is common in regression, the absolute error loss, or Least Absolute Deviations (LAD), offers robustness to outliers. This practice delves into the fascinating consequences of using a nonsmooth loss function, revealing that the set of optimal solutions can be surprisingly rich. By analyzing the subgradient optimality conditions, you will discover a fundamental connection between LAD regression and the statistical median, and explore how this can lead to an entire interval of optimal predictions .",
            "id": "3146362",
            "problem": "You are given a linear regression model with absolute loss, also known as Least Absolute Deviations (LAD). The objective is to minimize the function $f(\\beta) = \\sum_{i=1}^{n} \\lvert y_i - x_i^\\top \\beta \\rvert$ over the parameter vector $\\beta \\in \\mathbb{R}^p$. Starting only from the core definitions of convex functions and subgradients, and the first-order optimality condition for convex nonsmooth optimization ($0 \\in \\partial f(\\beta^\\star)$), construct and analyze a concrete scenario where the LAD objective admits multiple minimizers. Then, implement a program to compute and report quantitative characteristics of the minimizer set for a small test suite.\n\nFundamental base to use:\n- Definition of convexity and subgradient: for a convex function $g:\\mathbb{R}^d \\to \\mathbb{R}$, the subdifferential at $z$ is $\\partial g(z) = \\{ s \\in \\mathbb{R}^d : g(w) \\ge g(z) + s^\\top (w - z) \\text{ for all } w \\}$.\n- For the absolute value function, $\\partial \\lvert r \\rvert = \\{ \\operatorname{sign}(r) \\}$ if $r \\ne 0$ and $\\partial \\lvert 0 \\rvert = [-1,1]$.\n- First-order optimality for convex nonsmooth minimization: a point $\\beta^\\star$ is optimal if and only if $0 \\in \\partial f(\\beta^\\star)$.\n\nScenario construction requirement:\n- Consider data matrices $X \\in \\mathbb{R}^{n \\times p}$ whose rows are identical, $x_i = x_0$ for all $i$, so predictions are constant across $i$, $x_i^\\top \\beta = x_0^\\top \\beta = \\gamma$. This reduces the LAD regression to choosing a scalar $\\gamma$ that minimizes $\\sum_{i=1}^n \\lvert y_i - \\gamma \\rvert$.\n- Use an even number $n$ of samples and choose a vector $y$ such that the two middle order statistics differ, yielding an interval of optimal $\\gamma$ values and hence multiple minimizers. Also include cases where $n$ is odd and where all $y_i$ are equal to probe boundary behavior. Finally, include a case with $p = 2$ and linearly dependent columns (nontrivial nullspace), to illustrate multiple minimizers in parameter space even when the optimal $\\gamma$ is unique.\n\nTasks:\n1. Derive, from the subgradient definition and the first-order optimality condition, the characterization of the optimal $\\gamma$ for the constant-prediction case. Show that the optimality condition $0 \\in \\partial f(\\beta^\\star)$ reduces to $\\sum_{i=1}^n s_i = 0$ where $s_i \\in \\partial \\lvert y_i - \\gamma \\rvert$, and conclude that any $\\gamma$ in the closed interval between the lower median and upper median of $\\{y_i\\}_{i=1}^n$ is optimal when $n$ is even; when $n$ is odd, the unique optimal $\\gamma$ is the median. Explain how the presence of a nontrivial nullspace of $X$ (that is, $p - \\operatorname{rank}(X)  0$) creates infinitely many parameter-space minimizers that map to the same optimal predictions.\n2. For each test case below, compute:\n   - The interval $[\\gamma_{\\mathrm{low}}, \\gamma_{\\mathrm{high}}]$ of optimal constant predictions. For odd $n$, this collapses to a single point with $\\gamma_{\\mathrm{low}} = \\gamma_{\\mathrm{high}}$.\n   - The boolean $u_\\gamma$ indicating whether the optimal $\\gamma$ is unique ($u_\\gamma = \\text{True}$ if and only if $\\gamma_{\\mathrm{low}} = \\gamma_{\\mathrm{high}}$).\n   - The integer nullspace dimension $d_{\\mathcal{N}} = p - \\operatorname{rank}(X)$.\n   - The boolean $m_\\beta$ indicating whether the set of parameter-space minimizers is non-unique, defined as $m_\\beta = \\text{True}$ if and only if $(\\gamma_{\\mathrm{high}} - \\gamma_{\\mathrm{low}})  0$ or $d_{\\mathcal{N}}  0$.\n   - The minimal absolute loss value $L_\\star = \\min_{\\beta} \\sum_{i=1}^{n} \\lvert y_i - x_i^\\top \\beta \\rvert$, which, in the constant-prediction scenario, equals $\\min_{\\gamma} \\sum_{i=1}^{n} \\lvert y_i - \\gamma \\rvert$.\n\nTest suite:\n- Case $1$ (even $n$, multiple optimal $\\gamma$ values, $p = 1$):\n  - $X = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $y = [0,1,3,10]$.\n- Case $2$ (odd $n$, unique optimal $\\gamma$, $p = 1$):\n  - $X = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $y = [0,2,2,5,9]$.\n- Case $3$ (even $n$, multiple optimal $\\gamma$, $p = 2$ with linearly dependent columns, nontrivial nullspace):\n  - $X = \\begin{bmatrix} 1  2 \\\\ 1  2 \\\\ 1  2 \\\\ 1  2 \\end{bmatrix}$, $y = [0,1,3,10]$.\n- Case $4$ (all $y_i$ equal, boundary behavior, $p = 1$):\n  - $X = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $y = [2,2,2,2]$.\n\nOutput specification:\n- Your program should produce a single line of output containing a list of per-case results. For each case, output the list $[\\gamma_{\\mathrm{low}}, \\gamma_{\\mathrm{high}}, u_\\gamma, d_{\\mathcal{N}}, m_\\beta, L_\\star]$. Aggregate the four case lists into one list and print it as a comma-separated list enclosed in square brackets (for example, $[ [\\dots], [\\dots], [\\dots], [\\dots] ]$). All numbers must be printed as plain decimal numbers and booleans as plain $\\text{True}$ or $\\text{False}$. No physical units or angle units are involved. Percentages are not used.",
            "solution": "The problem requires an analysis of the conditions leading to multiple minimizers for the Least Absolute Deviations (LAD) regression objective function and a computational implementation for specific test cases. The validation confirms the problem is well-posed, scientifically sound, and all necessary information is provided.\n\n### Part 1: Theoretical Derivation\n\nThe LAD objective function is given by:\n$$f(\\beta) = \\sum_{i=1}^{n} \\lvert y_i - x_i^\\top \\beta \\rvert$$\nwhere $y_i \\in \\mathbb{R}$ are the observed responses, $x_i \\in \\mathbb{R}^p$ are the predictor vectors, and $\\beta \\in \\mathbb{R}^p$ is the parameter vector to be optimized. The function $f(\\beta)$ is a sum of absolute value functions composed with affine functions of $\\beta$. Since the absolute value function is convex and the composition with an affine function preserves convexity, and the sum of convex functions is convex, $f(\\beta)$ is a convex function.\n\nThe first-order necessary and sufficient condition for a point $\\beta^\\star$ to be a minimizer of the convex function $f(\\beta)$ is that the zero vector must be an element of the subdifferential of $f$ at $\\beta^\\star$:\n$$0 \\in \\partial f(\\beta^\\star)$$\n\nThe subdifferential of $f(\\beta)$ can be computed using the sum rule for subdifferentials. Let $f_i(\\beta) = \\lvert y_i - x_i^\\top \\beta \\rvert$. Then $\\partial f(\\beta) = \\sum_{i=1}^n \\partial f_i(\\beta)$, where the sum is the Minkowski sum of sets. An element of $\\partial f(\\beta)$ is a vector of the form $\\sum_{i=1}^n v_i$, where each $v_i \\in \\partial f_i(\\beta)$.\n\nTo find $\\partial f_i(\\beta)$, we use the chain rule for subgradients. Let $h(u) = \\lvert u \\rvert$ and $g_i(\\beta) = y_i - x_i^\\top \\beta$. Then $f_i(\\beta) = h(g_i(\\beta))$. The chain rule states that $\\partial f_i(\\beta) = (\\partial g_i(\\beta))^\\top \\partial h(g_i(\\beta))$. The subgradient of the affine function $g_i(\\beta)$ is just the vector of coefficients, $\\partial g_i(\\beta) = \\{-x_i\\}$. The subdifferential of the absolute value function is $\\partial \\lvert u \\rvert = \\{\\operatorname{sign}(u)\\}$ for $u \\neq 0$ and $\\partial \\lvert 0 \\rvert = [-1, 1]$.\nLetting $r_i = y_i - x_i^\\top\\beta$ be the residual, an element of $\\partial f_i(\\beta)$ is a vector of the form $-s_i x_i$, where $s_i \\in \\partial \\lvert r_i \\rvert$. That is, $s_i = \\operatorname{sign}(y_i - x_i^\\top \\beta)$ if $y_i \\neq x_i^\\top \\beta$, and $s_i \\in [-1, 1]$ if $y_i = x_i^\\top \\beta$.\n\nThe optimality condition $0 \\in \\partial f(\\beta^\\star)$ is therefore equivalent to the existence of scalars $s_1, \\dots, s_n$ satisfying the conditions above, such that:\n$$\\sum_{i=1}^{n} (-s_i x_i) = 0 \\quad \\implies \\quad \\sum_{i=1}^{n} s_i x_i = 0$$\n\nNow, we introduce the specific scenario required by the problem: all predictor vectors are identical, i.e., $x_i = x_0$ for all $i=1, \\dots, n$. The optimality condition becomes:\n$$\\sum_{i=1}^{n} s_i x_0 = 0 \\quad \\implies \\quad x_0 \\left(\\sum_{i=1}^{n} s_i\\right) = 0$$\nAssuming $x_0 \\neq 0$ (as is true in the provided test cases), this vector equation reduces to a scalar equation:\n$$\\sum_{i=1}^{n} s_i = 0$$\nIn this scenario, the prediction $x_i^\\top \\beta = x_0^\\top \\beta$ is a constant value for all $i$. Let's denote this constant prediction by $\\gamma = x_0^\\top\\beta$. The scalars $s_i$ must be elements of the subdifferentials $\\partial \\lvert y_i - \\gamma^\\star \\rvert$ for some optimal prediction value $\\gamma^\\star$.\n\nThe problem of minimizing $f(\\beta)$ is thus reduced to finding the set of optimal prediction values $\\gamma^\\star$ that minimize $g(\\gamma) = \\sum_{i=1}^n \\lvert y_i - \\gamma \\rvert$, and then finding all $\\beta$ such that $x_0^\\top\\beta$ is in this set.\nLet's analyze the condition $\\sum_{i=1}^n s_i = 0$, where $s_i \\in \\partial \\lvert y_i - \\gamma^\\star \\rvert$. We partition the indices based on the sign of the residual:\n$I_ = \\{i \\mid y_i  \\gamma^\\star\\}$, $I_ = \\{i \\mid y_i  \\gamma^\\star\\}$, and $I_0 = \\{i \\mid y_i = \\gamma^\\star\\}$.\nThe condition becomes:\n$$\\sum_{i \\in I_} (1) + \\sum_{i \\in I_} (-1) + \\sum_{i \\in I_0} s_i = 0$$\nwhich simplifies to:\n$$|I_| - |I_| = -\\sum_{i \\in I_0} s_i$$\nSince each $s_i \\in [-1, 1]$, their sum must lie in the interval $[-|I_0|, |I_0|]$. Thus, a value $\\gamma^\\star$ is optimal if and only if $|I_| - |I_| \\in [ -|I_0|, |I_0| ]$, or equivalently, $\\left| |I_| - |I_| \\right| \\leq |I_0|$.\n\nThis condition precisely characterizes the median. Let $y_{(1)} \\le y_{(2)} \\le \\dots \\le y_{(n)}$ be the sorted response values (order statistics).\n- **If $n$ is odd**, let $n = 2k+1$. The median is unique, $y_{(k+1)}$. Let's test $\\gamma^\\star = y_{(k+1)}$. At most $k$ values are smaller and at most $k$ values are larger. $\\left| |I_| - |I_| \\right| \\le |I_0|$ will hold. For any $\\gamma  y_{(k+1)}$, $|I_|$ decreases and $|I_|$ increases, making $|I_|  |I_|$. If $\\gamma$ is not equal to any $y_i$, then $|I_0|=0$, and the condition requires $|I_| = |I_|$, which is impossible for odd $n$. Thus, the unique optimal $\\gamma$ is the median, $\\gamma^\\star = y_{((n+1)/2)}$.\n\n- **If $n$ is even**, let $n=2k$. Any value in the interval $[y_{(k)}, y_{(k+1)}]$ is a median.\n  - If we choose $\\gamma^\\star \\in (y_{(k)}, y_{(k+1)})$, then $|I_| = k$, $|I_| = k$, and $|I_0| = 0$. The condition $\\left| k - k \\right| \\le 0$ holds. So, any such $\\gamma^\\star$ is optimal.\n  - At the endpoints $\\gamma^\\star = y_{(k)}$ and $\\gamma^\\star = y_{(k+1)}$, the condition also holds. For example, at $\\gamma^\\star = y_{(k)}$, we have $|I_|  k$, $|I_| \\ge k$. The condition allows for an imbalance between $|I_|$ and $|I_|$ that is \"absorbed\" by the points in $I_0$.\nThus, for an even number of samples, any value $\\gamma^\\star$ in the closed interval formed by the two central order statistics, $[y_{(n/2)}, y_{(n/2)+1}]$, is an optimal prediction. This interval is what the problem refers to as $[\\gamma_{\\mathrm{low}}, \\gamma_{\\mathrm{high}}]$. It collapses to a single point if $y_{(n/2)} = y_{(n/2)+1}$.\n\nFinally, we analyze the non-uniqueness of the parameter vector $\\beta^\\star$. The set of minimizers is $S_\\beta = \\{\\beta \\in \\mathbb{R}^p \\mid x_0^\\top \\beta \\in [\\gamma_{\\mathrm{low}}, \\gamma_{\\mathrm{high}}]\\}$.\n1.  If the interval of optimal predictions is non-degenerate (i.e., $\\gamma_{\\mathrm{low}}  \\gamma_{\\mathrm{high}}$), there are infinitely many optimal values for $\\gamma = x_0^\\top \\beta$. This immediately implies there are infinitely many solutions for $\\beta$.\n2.  Even if the optimal prediction $\\gamma^\\star$ is unique ($\\gamma_{\\mathrm{low}} = \\gamma_{\\mathrm{high}}$), the set of minimizers is $S_\\beta = \\{\\beta \\in \\mathbb{R}^p \\mid x_0^\\top \\beta = \\gamma^\\star\\}$. This is a linear equation for $\\beta$. If $\\beta_p$ is a particular solution, the general solution is $\\beta = \\beta_p + v$, where $v$ is any vector in the nullspace of the linear map defined by $x_0^\\top$, i.e., $x_0^\\top v = 0$. The dimension of this nullspace is $p - \\operatorname{rank}(x_0^\\top)$. Since all rows of $X$ are $x_0$, $\\operatorname{rank}(X) = \\operatorname{rank}(x_0^\\top) = 1$ (for $x_0 \\neq 0$). The nullspace dimension is therefore $d_{\\mathcal{N}} = p - \\operatorname{rank}(X) = p - 1$. If $p1$, $d_{\\mathcal{N}}  0$, and there are infinitely many solutions for $\\beta$ even if $\\gamma^\\star$ is unique.\n\nIn summary, the set of minimizers $\\beta^\\star$ is non-unique ($m_\\beta = \\text{True}$) if either the interval of optimal predictions has positive length ($(\\gamma_{\\mathrm{high}} - \\gamma_{\\mathrm{low}})  0$) or the nullspace of $X$ is non-trivial ($d_{\\mathcal{N}}  0$).\n\n### Part 2: Computation for Test Suite\n\nThe following Python code implements the logic derived above to calculate the required quantities for each test case.\n- Medians are found by sorting the `y` vector.\n- Nullspace dimension is computed as $p - \\operatorname{rank}(X)$.\n- The minimal loss $L_\\star$ is calculated using an optimal $\\gamma$ (e.g., $\\gamma_{\\mathrm{low}}$), as the loss is constant over the optimal interval.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the LAD regression problem for multiple test cases, analyzing\n    the uniqueness of minimizers.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: even n, multiple optimal gamma, p=1\n        (np.array([[1], [1], [1], [1]], dtype=float), \n         np.array([0, 1, 3, 10], dtype=float)),\n        # Case 2: odd n, unique optimal gamma, p=1\n        (np.array([[1], [1], [1], [1], [1]], dtype=float), \n         np.array([0, 2, 2, 5, 9], dtype=float)),\n        # Case 3: even n, multiple optimal gamma, p=2 with dependent columns\n        (np.array([[1, 2], [1, 2], [1, 2], [1, 2]], dtype=float), \n         np.array([0, 1, 3, 10], dtype=float)),\n        # Case 4: all y_i equal, boundary behavior\n        (np.array([[1], [1], [1], [1]], dtype=float), \n         np.array([2, 2, 2, 2], dtype=float)),\n    ]\n\n    all_results = []\n    for X, y in test_cases:\n        n, p = X.shape\n        y_sorted = np.sort(y)\n\n        # 1. Compute the interval of optimal constant predictions [gamma_low, gamma_high]\n        if n % 2 == 1:\n            # For odd n, the median is unique\n            median_idx = (n - 1) // 2\n            gamma_low = y_sorted[median_idx]\n            gamma_high = y_sorted[median_idx]\n        else:\n            # For even n, the optimal interval is between the two middle elements\n            upper_median_idx = n // 2\n            lower_median_idx = upper_median_idx - 1\n            gamma_low = y_sorted[lower_median_idx]\n            gamma_high = y_sorted[upper_median_idx]\n            \n        # 2. Compute u_gamma: uniqueness of optimal gamma\n        u_gamma = (gamma_low == gamma_high)\n        \n        # 3. Compute d_N: nullspace dimension\n        rank_X = np.linalg.matrix_rank(X)\n        d_N = p - rank_X\n        \n        # 4. Compute m_beta: non-uniqueness of parameter-space minimizers\n        m_beta = (gamma_high  gamma_low) or (d_N  0)\n        \n        # 5. Compute L_star: minimal absolute loss value\n        # The loss is constant on the interval [gamma_low, gamma_high].\n        # We can evaluate it at any point in the interval, e.g., gamma_low.\n        L_star = np.sum(np.abs(y - gamma_low))\n        \n        # Collect results for the current case\n        case_result = [\n            float(gamma_low),\n            float(gamma_high),\n            u_gamma,\n            int(d_N),\n            m_beta,\n            float(L_star)\n        ]\n        all_results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # Format each inner list as a string \"[v1,v2,...]\"\n    result_strings = [\n        f\"[{','.join(map(str, res))}]\"\n        for res in all_results\n    ]\n    \n    # Join the inner list strings into a final string \"[[...],[...],...]\"\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many modern machine learning models combine a smooth data-fitting loss with a nonsmooth regularizer to enforce properties like sparsity. This hands-on problem tackles such a composite objective, using the squared hinge loss for classification and the $\\ell_1$ norm for regularization. You will derive the necessary components from first principles and implement the powerful Proximal Gradient method, an algorithm designed to efficiently handle this smooth-plus-nonsmooth structure .",
            "id": "3146371",
            "problem": "Consider binary classification with labels $y_i \\in \\{-1,+1\\}$ and features $x_i \\in \\mathbb{R}^d$, and the linear separator $w \\in \\mathbb{R}^d$. The hinge loss for one example $(x_i,y_i)$ is the function $\\ell_i(w) = \\max(0, 1 - y_i x_i^\\top w)$. Your tasks are the following, grounded in first principles of convex analysis and the definition of subgradients.\n\nTask $1$ (derivation): Starting from the definition of the hinge loss and the definition of the subdifferential for a pointwise maximum of convex functions, derive the subgradient set of $\\ell_i(w)$ with respect to $w$. Then extend your result to the empirical hinge loss $\\sum_{i=1}^n \\ell_i(w)$.\n\nTask $2$ (algorithm design): Consider the squared hinge loss, defined by the function $h(z) = \\max(0, 1 - z)^2$ for a scalar $z \\in \\mathbb{R}$. Define the smooth empirical loss\n$$\nf(w) = \\frac{1}{n} \\sum_{i=1}^n h(y_i x_i^\\top w),\n$$\nand the regularizer $g(w) = \\lambda \\|w\\|_1$ where $\\lambda \\ge 0$. Starting from the chain rule and the definition of $h(z)$, derive the gradient $\\nabla f(w)$. Using the property that $f(w)$ has a Lipschitz-continuous gradient, derive a computable upper bound $L$ on the Lipschitz constant of $\\nabla f(w)$ in terms of $\\{x_i\\}_{i=1}^n$ (do not assume any special structure on the data). Then, from the definition of the proximal operator for the $\\ell_1$ norm, derive the iterative update of the Proximal Gradient (PG) method for minimizing $F(w) = f(w) + g(w)$ with a fixed step size $\\alpha = 1/L$.\n\nTask $3$ (implementation and testing): Implement a complete program that:\n- Constructs the following three test cases (each case specifies $n$, $d$, $\\{(x_i,y_i)\\}_{i=1}^n$, $\\lambda$, and the number of iterations $K$):\n    - Case A (linearly separable): $n=4$, $d=2$, with\n        $x_1 = (2,0)$, $y_1 = +1$;\n        $x_2 = (0,2)$, $y_2 = +1$;\n        $x_3 = (-2,0)$, $y_3 = -1$;\n        $x_4 = (0,-2)$, $y_4 = -1$;\n        $\\lambda = 0.1$, $K = 200$.\n    - Case B (partially conflicting): $n=4$, $d=2$, with\n        $x_1 = (0.5,0.5)$, $y_1 = +1$;\n        $x_2 = (0.5,-0.5)$, $y_2 = +1$;\n        $x_3 = (-0.5,0.5)$, $y_3 = -1$;\n        $x_4 = (-0.5,-0.5)$, $y_4 = -1$;\n        $\\lambda = 0.5$, $K = 200$.\n    - Case C (degenerate features): $n=3$, $d=2$, with\n        $x_1 = (0,0)$, $y_1 = +1$;\n        $x_2 = (0,0)$, $y_2 = -1$;\n        $x_3 = (0,0)$, $y_3 = +1$;\n        $\\lambda = 0.3$, $K = 200$.\n- Initializes $w^{(0)} = 0 \\in \\mathbb{R}^d$, computes the step size $\\alpha = 1/L$ from your bound, and runs $K$ iterations of the PG algorithm for each case to produce $w^{(K)}$.\n- Computes the final objective value $F(w^{(K)}) = \\frac{1}{n} \\sum_{i=1}^n \\max(0, 1 - y_i x_i^\\top w^{(K)})^2 + \\lambda \\|w^{(K)}\\|_1$ as a real number for each case.\n\nYour program should produce a single line of output containing the three final objective values, as a comma-separated list enclosed in square brackets. For example, the output format must be exactly like $[v_A,v_B,v_C]$, where $v_A$, $v_B$, and $v_C$ are the floating-point values corresponding to Cases A, B, and C, respectively.\n\nIn addition, in your derivation explain why, under the assumptions that $f$ is convex with a Lipschitz-continuous gradient and $g$ is convex with a computable proximal operator, the Proximal Gradient (PG) method with a fixed step size $\\alpha \\in (0, 1/L]$ achieves the sublinear convergence rate $O(1/k)$ in objective value, and state how this applies to the squared hinge loss setting defined above.",
            "solution": "This problem requires the derivation of subgradients and gradients for hinge-based losses, the design and analysis of a proximal gradient algorithm for a regularized classification problem, and its implementation. We shall proceed by addressing each task in sequence, adhering to rigorous mathematical formalism.\n\n### Task 1: Subgradient of the Hinge Loss\n\nThe hinge loss for a single data point $(x_i, y_i)$, where $y_i \\in \\{-1, +1\\}$ and $x_i, w \\in \\mathbb{R}^d$, is defined as:\n$$\n\\ell_i(w) = \\max(0, 1 - y_i x_i^\\top w)\n$$\nThis function is a pointwise maximum of two convex, differentiable functions of $w$: $f_1(w) = 0$ and $f_2(w) = 1 - y_i x_i^\\top w$. The gradients of these functions are $\\nabla f_1(w) = 0$ and $\\nabla f_2(w) = -y_i x_i$.\n\nThe subdifferential of a pointwise maximum of a finite number of convex functions, $f(x) = \\max_{j=1,\\dots,m} f_j(x)$, at a point $x$ is the convex hull of the union of the subdifferentials of the \"active\" functions (those for which $f_j(x) = f(x)$):\n$$\n\\partial f(x) = \\text{conv} \\left( \\bigcup_{j: f_j(x) = f(x)} \\partial f_j(x) \\right)\n$$\nIn our case, since $f_1(w)$ and $f_2(w)$ are differentiable, their subdifferentials are singletons containing their gradients. We analyze the subdifferential $\\partial \\ell_i(w)$ by considering three cases for the margin term $z_i = y_i x_i^\\top w$:\n\n1.  **Correctly classified with margin ($1 - y_i x_i^\\top w  0$ or $y_i x_i^\\top w  1$):**\n    In this case, $\\ell_i(w) = 0$. The only active function is $f_1(w) = 0$. Thus, the subdifferential is the singleton set containing the gradient of $f_1(w)$:\n    $$\n    \\partial \\ell_i(w) = \\{ \\nabla f_1(w) \\} = \\{0\\}\n    $$\n\n2.  **Incorrectly classified or on the wrong side of the margin ($1 - y_i x_i^\\top w  0$ or $y_i x_i^\\top w  1$):**\n    Here, $\\ell_i(w) = 1 - y_i x_i^\\top w$. The only active function is $f_2(w)$. The subdifferential is the singleton set containing the gradient of $f_2(w)$:\n    $$\n    \\partial \\ell_i(w) = \\{ \\nabla f_2(w) \\} = \\{-y_i x_i\\}\n    $$\n\n3.  **Exactly on the margin ($1 - y_i x_i^\\top w = 0$ or $y_i x_i^\\top w = 1$):**\n    At this non-differentiable point, both functions are active: $\\ell_i(w) = f_1(w) = f_2(w) = 0$. The subdifferential is the convex hull of their gradients:\n    $$\n    \\partial \\ell_i(w) = \\text{conv}(\\{\\nabla f_1(w), \\nabla f_2(w)\\}) = \\text{conv}(\\{0, -y_i x_i\\})\n    $$\n    This is the line segment connecting $0$ and $-y_i x_i$:\n    $$\n    \\partial \\ell_i(w) = \\{ -\\theta y_i x_i \\mid \\theta \\in [0, 1] \\}\n    $$\n\nThe subdifferential for the total empirical hinge loss, $\\mathcal{L}(w) = \\sum_{i=1}^n \\ell_i(w)$, is given by the sum rule for subdifferentials (Minkowski sum), as the sum of convex functions:\n$$\n\\partial \\mathcal{L}(w) = \\sum_{i=1}^n \\partial \\ell_i(w) = \\left\\{ \\sum_{i=1}^n g_i \\mid g_i \\in \\partial \\ell_i(w) \\text{ for each } i \\right\\}\n$$\nwhere each $\\partial \\ell_i(w)$ is determined by the three cases above.\n\n### Task 2: Proximal Gradient Method for Squared Hinge Loss\n\nWe consider the objective function $F(w) = f(w) + g(w)$, with the smooth loss $f(w)$ and the non-smooth regularizer $g(w)$ defined as:\n$$\nf(w) = \\frac{1}{n} \\sum_{i=1}^n \\max(0, 1 - y_i x_i^\\top w)^2, \\quad g(w) = \\lambda \\|w\\|_1\n$$\n\n**Gradient of $f(w)$:**\nLet $h(z) = \\max(0, 1-z)^2$. The function $f(w)$ can be written as $f(w) = \\frac{1}{n} \\sum_{i=1}^n h(z_i(w))$, where $z_i(w) = y_i x_i^\\top w$.\nFirst, we find the derivative of $h(z)$.\n- If $z  1$, $h(z) = (1-z)^2$, so $h'(z) = 2(1-z)(-1) = -2(1-z)$.\n- If $z  1$, $h(z) = 0$, so $h'(z) = 0$.\n- If $z = 1$, the left-hand derivative is $\\lim_{z \\to 1^-} -2(1-z) = 0$, and the right-hand derivative is $0$. Thus, $h(z)$ is continuously differentiable everywhere.\nThe derivative can be compactly written as $h'(z) = -2\\max(0, 1-z)$.\n\nUsing the chain rule, the gradient of $f(w)$ is:\n$$\n\\nabla f(w) = \\frac{1}{n} \\sum_{i=1}^n h'(y_i x_i^\\top w) \\nabla_w(y_i x_i^\\top w)\n$$\nSince $\\nabla_w(y_i x_i^\\top w) = y_i x_i$, we have:\n$$\n\\nabla f(w) = \\frac{1}{n} \\sum_{i=1}^n \\left( -2 \\max(0, 1 - y_i x_i^\\top w) \\right) y_i x_i = -\\frac{2}{n} \\sum_{i=1}^n \\max(0, 1 - y_i x_i^\\top w) y_i x_i\n$$\n\n**Lipschitz Constant of $\\nabla f(w)$:**\nA function's gradient is $L$-Lipschitz continuous if its Hessian has a spectral norm bounded by $L$. The Hessian of $f(w)$ is $\\nabla^2 f(w) = \\frac{1}{n} \\sum_{i=1}^n \\nabla_w^2 [h(y_i x_i^\\top w)]$.\nThe second derivative of $h(z)$ is $h''(z) = 2$ if $z1$ and $h''(z)=0$ if $z1$. This can be written as $h''(z) = 2 \\cdot \\mathbb{I}(z  1)$, where $\\mathbb{I}(\\cdot)$ is the indicator function. The Hessian of the term for the $i$-th sample is obtained via the chain rule:\n$$\n\\nabla_w^2 [h(y_i x_i^\\top w)] = h''(y_i x_i^\\top w) (y_i x_i) (y_i x_i)^\\top = h''(y_i x_i^\\top w) x_i x_i^\\top\n$$\nSince $h''(z) \\ge 0$, the Hessian of each term is a positive semidefinite matrix. The Hessian of $f(w)$ is:\n$$\n\\nabla^2 f(w) = \\frac{1}{n} \\sum_{i=1}^n h''(y_i x_i^\\top w) x_i x_i^\\top\n$$\nSince $0 \\le h''(z) \\le 2$, we can bound the Hessian matrix:\n$$\n0 \\preceq \\nabla^2 f(w) \\preceq \\frac{2}{n} \\sum_{i=1}^n x_i x_i^\\top\n$$\nwhere $\\preceq$ denotes the Loewner order. The Lipschitz constant $L$ can be taken as an upper bound on the spectral norm of the Hessian. Let $X$ be the $n \\times d$ data matrix with rows $x_i^\\top$. Then $\\sum_{i=1}^n x_i x_i^\\top = X^\\top X$.\n$$\nL = \\sup_w \\|\\nabla^2 f(w)\\|_2 \\le \\left\\| \\frac{2}{n} \\sum_{i=1}^n x_i x_i^\\top \\right\\|_2 = \\frac{2}{n} \\|X^\\top X\\|_2\n$$\nThis provides a computable upper bound for the Lipschitz constant.\n\n**Proximal Gradient (PG) Update:**\nThe PG algorithm generates a sequence $w^{(k)}$ via the iterative update:\n$$\nw^{(k+1)} = \\text{prox}_{\\alpha g}(w^{(k)} - \\alpha \\nabla f(w^{(k)}))\n$$\nwhere $\\text{prox}_{\\alpha g}(z) = \\arg\\min_u \\left( g(u) + \\frac{1}{2\\alpha} \\|u - z\\|_2^2 \\right)$. For $g(w) = \\lambda \\|w\\|_1$, the proximal operator is the soft-thresholding operator, $\\mathcal{S}_{\\alpha\\lambda}(\\cdot)$:\n$$\n[\\text{prox}_{\\alpha\\lambda\\|\\cdot\\|_1}(z)]_j = \\mathcal{S}_{\\alpha\\lambda}(z_j) = \\text{sign}(z_j) \\max(0, |z_j| - \\alpha\\lambda)\n$$\nWe set the step size $\\alpha = 1/L$. The iterative update is:\n1.  **Gradient computation:** $\\nabla f(w^{(k)}) = -\\frac{2}{n} \\sum_{i=1}^n \\max(0, 1 - y_i x_i^\\top w^{(k)}) y_i x_i$.\n2.  **Gradient step:** $u^{(k)} = w^{(k)} - \\alpha \\nabla f(w^{(k)})$.\n3.  **Proximal step:** $w_j^{(k+1)} = \\text{sign}(u_j^{(k)}) \\max(0, |u_j^{(k)}| - \\alpha \\lambda)$ for each component $j \\in \\{1, \\dots, d\\}$.\n\n**Convergence Rate of Proximal Gradient:**\nFor an objective function $F(w) = f(w) + g(w)$, the PG method with step size $\\alpha \\in (0, 1/L]$ is guaranteed to converge under the following assumptions:\n1.  $f$ is a convex, differentiable function with an $L$-Lipschitz continuous gradient ($\\nabla f$ is $L$-smooth).\n2.  $g$ is a convex, possibly non-differentiable function, for which the proximal operator is efficiently computable.\n\nIn our problem, $f(w)$ is convex because $h(z) = \\max(0, 1-z)^2$ is a convex function (its second derivative $h''(z)$ is non-negative), composition with a linear map preserves convexity, and the sum of convex functions is convex. We have shown that $\\nabla f$ is $L$-smooth. The regularizer $g(w) = \\lambda \\|w\\|_1$ is convex, and its proximal operator is the well-known soft-thresholding operator. All assumptions are satisfied.\n\nThe standard convergence analysis for PG establishes that the sequence of objective values converges to the optimal value $F(w^*)$. Specifically, for any $k \\ge 1$:\n$$\nF(w^{(k)}) - F(w^*) \\le \\frac{\\|w^{(0)} - w^*\\|_2^2}{2\\alpha k}\n$$\nwhere $w^*$ is a minimizer of $F(w)$. This demonstrates a sublinear convergence rate of $O(1/k)$ for the objective function error. The proof relies on combining the descent lemma for the smooth part $f$, the convexity property of the non-smooth part $g$, and the definition of the proximal update step. This guarantees that the iterates make steady progress towards the minimum.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the proximal gradient optimization problem for three test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"n\": 4,\n            \"d\": 2,\n            \"X\": np.array([[2., 0.], [0., 2.], [-2., 0.], [0., -2.]]),\n            \"y\": np.array([1., 1., -1., -1.]),\n            \"lambda_val\": 0.1,\n            \"K\": 200,\n        },\n        {\n            \"name\": \"Case B\",\n            \"n\": 4,\n            \"d\": 2,\n            \"X\": np.array([[0.5, 0.5], [0.5, -0.5], [-0.5, 0.5], [-0.5, -0.5]]),\n            \"y\": np.array([1., 1., -1., -1.]),\n            \"lambda_val\": 0.5,\n            \"K\": 200,\n        },\n        {\n            \"name\": \"Case C\",\n            \"n\": 3,\n            \"d\": 2,\n            \"X\": np.array([[0., 0.], [0., 0.], [0., 0.]]),\n            \"y\": np.array([1., -1., 1.]),\n            \"lambda_val\": 0.3,\n            \"K\": 200,\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X = case[\"X\"]\n        y = case[\"y\"]\n        n = case[\"n\"]\n        d = case[\"d\"]\n        lambda_val = case[\"lambda_val\"]\n        K = case[\"K\"]\n\n        # Calculate Lipschitz constant L\n        # L = (2/n) * ||X^T X||_2\n        X_T_X = X.T @ X\n        # The spectral norm (ord=2) of a matrix is its largest singular value.\n        # For a positive semi-definite matrix like X^T X, this is the largest eigenvalue.\n        spectral_norm = np.linalg.norm(X_T_X, ord=2)\n        L = (2.0 / n) * spectral_norm\n\n        # Initialize weights\n        w = np.zeros(d)\n\n        # Handle the degenerate case where L=0\n        if L == 0:\n            # If L=0, it means all X_i are zero.\n            # The gradient of f(w) is always zero.\n            # The objective is f(w) + g(w) = 1 + lambda * ||w||_1.\n            # With w_init=0, PG update does not move w.\n            # So w_final = 0.\n            w_final = w\n        else:\n            # Set fixed step size\n            alpha = 1.0 / L\n\n            # Proximal Gradient iterations\n            for _ in range(K):\n                # 1. Compute gradient of f(w)\n                #margins = y * (X @ w) # Shape: (n,)\n                margins = X @ w * y\n                \n                # Coeffs for the sum: -2 * max(0, 1 - y_i x_i^T w) * y_i\n                # This form has a factor of 'n' already in the definition of f(w).\n                # nabla_f(w) = (1/n) * sum_i [ -2 * max(0, 1 - y_i x_i^T w) * y_i * x_i ]\n                \n                # Simplified: nabla_f(w) = (-2/n) * sum_i [ max(0, 1-m_i) * y_i * x_i ]\n                # Let's write this using matrix operations\n                coeffs = np.maximum(0, 1 - margins) * y # Shape (n,)\n                # The gradient is the weighted sum of (-2/n) * x_i vectors\n                grad_f = (-2.0 / n) * (X.T @ coeffs) # Shape (d,)\n            \n                # 2. Gradient descent step\n                u = w - alpha * grad_f\n\n                # 3. Proximal step (soft-thresholding)\n                prox_arg = alpha * lambda_val\n                w = np.sign(u) * np.maximum(0, np.abs(u) - prox_arg)\n            \n            w_final = w\n            \n        # Compute final objective value F(w_final)\n        # f(w) = (1/n) * sum(max(0, 1 - y_i * x_i^T w)^2)\n        final_margins = X @ w_final * y\n        f_val = np.mean(np.maximum(0, 1 - final_margins)**2)\n        \n        # g(w) = lambda * ||w||_1\n        g_val = lambda_val * np.linalg.norm(w_final, ord=1)\n        \n        final_objective = f_val + g_val\n        results.append(final_objective)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "In many applications, the ultimate performance metric, like Intersection over Union (IoU) for object detection, is not itself a convex function, making direct optimization difficult and unreliable. This exercise demonstrates this challenge by showing the non-convexity of the IoU loss and then introduces the concept of convex surrogates, like hinge and logistic loss . By comparing the difficult optimization landscape of IoU with the well-behaved nature of its surrogates, you will gain a crucial insight into why convex loss functions are a cornerstone of practical machine learning.",
            "id": "3146363",
            "problem": "You are given a one-dimensional set prediction scenario on the real line. The ground-truth set is the closed interval $B = [0,1]$. A model predicts a set that is also a closed interval of fixed width $w = 1$ parameterized by its left endpoint $\\theta \\in \\mathbb{R}$ as $A_\\theta = [\\theta, \\theta + 1]$. The Intersection over Union (IoU) loss is defined by\n$$\n\\mathcal{L}_{\\mathrm{IoU}}(\\theta) \\;=\\; 1 \\;-\\; \\frac{|A_\\theta \\cap B|}{|A_\\theta \\cup B|},\n$$\nwhere $|\\cdot|$ denotes Lebesgue measure (length) in one dimension. For $w=1$ and $B=[0,1]$, this is a precise, piecewise-defined function of $\\theta$.\n\nYou will analyze the non-convexity of $\\mathcal{L}_{\\mathrm{IoU}}(\\theta)$ and then construct convex surrogate losses for pointwise classification on a fixed grid, finally demonstrating optimization benefits of convexity.\n\nFundamental base you may rely on:\n- The definition of convexity: a function $f$ is convex if and only if for all $x,y$ and all $t \\in [0,1]$, $f(tx+(1-t)y) \\le t f(x) + (1-t) f(y)$.\n- Standard definitions of hinge loss and logistic loss for binary classification, and their convexity in linear model parameters.\n- Basic properties of Newton's method and positive semidefiniteness of the Hessian of logistic loss in linear models.\n\nTasks to implement and compute:\n1) Define the IoU loss $\\mathcal{L}_{\\mathrm{IoU}}(\\theta)$ for $w=1$ and $B=[0,1]$ by explicitly computing the intersection length $|A_\\theta \\cap B|$ and union length $|A_\\theta \\cup B|$ using\n$$\n|A_\\theta \\cap B| \\;=\\; \\max\\bigl\\{0, \\min(\\theta+1,1) - \\max(\\theta,0)\\bigr\\}, \\qquad |A_\\theta \\cup B| \\;=\\; 1 + 1 - |A_\\theta \\cap B|.\n$$\nUse this to evaluate the convexity inequality for the specific test case $(\\theta_1,\\theta_2,t) = (0,1,0.5)$. The required boolean output for this test is whether\n$$\n\\mathcal{L}_{\\mathrm{IoU}}(t \\theta_1 + (1-t)\\theta_2) \\;\\le\\; t \\,\\mathcal{L}_{\\mathrm{IoU}}(\\theta_1) + (1-t)\\,\\mathcal{L}_{\\mathrm{IoU}}(\\theta_2)\n$$\nholds. This test is designed to reveal non-convexity by exhibiting a specific violation.\n\n2) Construct a pointwise binary classification dataset by discretizing the interval $[-1.5,1.5]$ into $N=121$ equally spaced points $\\{x_i\\}_{i=1}^{N}$ and assigning labels\n$$\ny_i \\;=\\; \\begin{cases}\n+1,  \\text{if } x_i \\in [0,1],\\\\\n-1,  \\text{otherwise.}\n\\end{cases}\n$$\nConsider a linear classifier with score $s_{w,b}(x) = w x + b$ and define the empirical hinge loss\n$$\n\\mathcal{L}_{\\mathrm{hinge}}(w,b) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} \\max\\bigl\\{0,\\, 1 - y_i\\, (w x_i + b)\\bigr\\}.\n$$\nFor parameters $(w_1,b_1)=(4.0,-1.0)$, $(w_2,b_2)=(-2.0,0.5)$, and $t=0.5$, evaluate whether\n$$\n\\mathcal{L}_{\\mathrm{hinge}}(t w_1+(1-t)w_2,\\; t b_1+(1-t)b_2) \\;\\le\\; t \\,\\mathcal{L}_{\\mathrm{hinge}}(w_1,b_1) + (1-t)\\,\\mathcal{L}_{\\mathrm{hinge}}(w_2,b_2)\n$$\nholds. Output the resulting boolean.\n\n3) Using the same dataset and linear classifier, define the empirical logistic loss\n$$\n\\mathcal{L}_{\\mathrm{log}}(w,b) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} \\log\\!\\bigl(1 + \\exp\\bigl(-y_i \\,(w x_i + b)\\bigr)\\bigr).\n$$\nFor the same parameter pairs and $t=0.5$, evaluate whether\n$$\n\\mathcal{L}_{\\mathrm{log}}(t w_1+(1-t)w_2,\\; t b_1+(1-t)b_2) \\;\\le\\; t \\,\\mathcal{L}_{\\mathrm{log}}(w_1,b_1) + (1-t)\\,\\mathcal{L}_{\\mathrm{log}}(w_2,b_2)\n$$\nholds. Output the resulting boolean.\n\n4) Demonstrate an optimization benefit of convex surrogates: minimize $\\mathcal{L}_{\\mathrm{log}}(w,b)$ over $(w,b) \\in \\mathbb{R}^2$ using Newton's method with backtracking line search until the Euclidean norm of the step is less than $10^{-10}$ or $100$ iterations are reached. Run from two different initializations $(w,b)=(0.0,0.0)$ and $(w,b)=(10.0,-10.0)$, and return a boolean indicating whether both runs converge to essentially the same solution by simultaneously satisfying\n$$\n\\|\\,(w^{\\star}_A, b^{\\star}_A) - (w^{\\star}_B, b^{\\star}_B)\\,\\|_2  10^{-6}\n\\quad\\text{and}\\quad\n\\bigl|\\,\\mathcal{L}_{\\mathrm{log}}(w^{\\star}_A,b^{\\star}_A) - \\mathcal{L}_{\\mathrm{log}}(w^{\\star}_B,b^{\\star}_B)\\,\\bigr|  10^{-8}.\n$$\n\n5) Let $(w^{\\star},b^{\\star})$ be the minimizer obtained in Task 4. Compute the Hessian matrix of $\\mathcal{L}_{\\mathrm{log}}(w,b)$ at $(w^{\\star},b^{\\star})$ and return its smallest eigenvalue as a floating-point number. For the logistic loss with linear models, the Hessian takes the form\n$$\nH \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} \\sigma\\!\\bigl(y_i\\, s_{w,b}(x_i)\\bigr)\\,\\sigma\\!\\bigl(-y_i\\, s_{w,b}(x_i)\\bigr)\\,\n\\begin{bmatrix}\nx_i^2  x_i\\\\\nx_i  1\n\\end{bmatrix},\n$$\nwhere $\\sigma(z)=\\frac{1}{1+\\exp(-z)}$ denotes the logistic sigmoid. A nonnegative smallest eigenvalue numerically confirms positive semidefiniteness.\n\nTest suite and required outputs:\n- Test 1: $(\\theta_1,\\theta_2,t)=(0,1,0.5)$ for $\\mathcal{L}_{\\mathrm{IoU}}$ convexity check; output a boolean.\n- Test 2: $(w_1,b_1)=(4.0,-1.0)$, $(w_2,b_2)=(-2.0,0.5)$, $t=0.5$ for $\\mathcal{L}_{\\mathrm{hinge}}$ convexity check; output a boolean.\n- Test 3: Same parameters as Test 2 for $\\mathcal{L}_{\\mathrm{log}}$ convexity check; output a boolean.\n- Test 4: Newton optimization starting from $(0.0,0.0)$ and $(10.0,-10.0)$; output a boolean indicating convergence to the same solution within the specified tolerances.\n- Test 5: Smallest eigenvalue of the Hessian of $\\mathcal{L}_{\\mathrm{log}}$ at the optimizer from Test 4; output a float.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where result1 through result4 are booleans and result5 is a float. No other text should be printed.",
            "solution": "The problem statement has been meticulously validated and is determined to be valid. It is scientifically grounded in the fields of optimization and machine learning, well-posed, objective, and internally consistent. All necessary definitions, formulas, and parameters are provided, allowing for a complete and unambiguous computational solution.\n\nThe problem requires a multi-part analysis comparing the non-convex Intersection over Union ($\\mathcal{L}_{\\mathrm{IoU}}$) loss with convex surrogate losses (hinge and logistic) for a classification task, culminating in a demonstration of the optimization benefits of convexity using Newton's method.\n\nHere is a step-by-step derivation and reasoning for each task.\n\n### Task 1: Convexity Analysis of IoU Loss\n\nThe Intersection over Union loss is defined as $\\mathcal{L}_{\\mathrm{IoU}}(\\theta) = 1 - \\frac{|A_\\theta \\cap B|}{|A_\\theta \\cup B|}$, where the ground-truth set is $B = [0,1]$ and the predicted set is $A_\\theta = [\\theta, \\theta+1]$. The length (Lebesgue measure) of the intersection is given by $|A_\\theta \\cap B| = \\max\\{0, \\min(\\theta+1,1) - \\max(\\theta,0)\\}$, and the union length is $|A_\\theta \\cup B| = |A_\\theta| + |B| - |A_\\theta \\cap B| = 1 + 1 - |A_\\theta \\cap B| = 2 - |A_\\theta \\cap B|$.\n\nWe test the convexity inequality, $f(t x + (1-t) y) \\le t f(x) + (1-t) f(y)$, for the specific case $(\\theta_1, \\theta_2, t) = (0, 1, 0.5)$.\n\nThe endpoints of the interval for the convexity check are $\\theta_1 = 0$ and $\\theta_2 = 1$. The midpoint is $\\theta_m = t \\theta_1 + (1-t) \\theta_2 = 0.5 \\times 0 + 0.5 \\times 1 = 0.5$.\n\nWe evaluate the loss at these three points:\n1.  For $\\theta_1 = 0$, the predicted set is $A_0 = [0,1]$.\n    - Intersection: $|A_0 \\cap B| = |[0,1] \\cap [0,1]| = 1$.\n    - Union: $|A_0 \\cup B| = |[0,1] \\cup [0,1]| = 1$.\n    - Loss: $\\mathcal{L}_{\\mathrm{IoU}}(0) = 1 - 1/1 = 0$.\n\n2.  For $\\theta_2 = 1$, the predicted set is $A_1 = [1,2]$.\n    - Intersection: $|A_1 \\cap B| = |[1,2] \\cap [0,1]| = |\\{1\\}| = 0$.\n    - Union: $|A_1 \\cup B| = |[0,1] \\cup [1,2]| = |[0,2]| = 2$.\n    - Loss: $\\mathcal{L}_{\\mathrm{IoU}}(1) = 1 - 0/2 = 1$.\n\n3.  For the midpoint $\\theta_m = 0.5$, the predicted set is $A_{0.5} = [0.5, 1.5]$.\n    - Intersection: $|A_{0.5} \\cap B| = |[0.5, 1.5] \\cap [0,1]| = |[0.5, 1]| = 0.5$.\n    - Union: $|A_{0.5} \\cup B| = |[0, 1.5]| = 1.5$.\n    - Loss: $\\mathcal{L}_{\\mathrm{IoU}}(0.5) = 1 - 0.5 / 1.5 = 1 - 1/3 = 2/3$.\n\nNow, we check the convexity inequality:\n$\\mathcal{L}_{\\mathrm{IoU}}(\\theta_m) \\le t \\mathcal{L}_{\\mathrm{IoU}}(\\theta_1) + (1-t) \\mathcal{L}_{\\mathrm{IoU}}(\\theta_2)$\n$2/3 \\le 0.5 \\times 0 + 0.5 \\times 1$\n$2/3 \\le 0.5$\n$0.666... \\le 0.5$\n\nThis inequality is false. This demonstrates that $\\mathcal{L}_{\\mathrm{IoU}}(\\theta)$ is not a convex function, as we have found a specific counterexample. The required output is `False`.\n\n### Task 2  3: Convexity of Hinge and Logistic Losses\n\nFor these tasks, a pointwise binary classification dataset is constructed by sampling $N=121$ points $\\{x_i\\}$ on $[-1.5, 1.5]$ and assigning labels $y_i = +1$ for $x_i \\in [0,1]$ and $y_i = -1$ otherwise. We consider a linear classifier with score $s_{w,b}(x) = w x + b$.\n\nThe empirical hinge loss $\\mathcal{L}_{\\mathrm{hinge}}(w,b)$ and logistic loss $\\mathcal{L}_{\\mathrm{log}}(w,b)$ are defined as sums over the dataset:\n$$ \\mathcal{L}_{\\mathrm{hinge}}(w,b) = \\frac{1}{N}\\sum_{i=1}^{N} \\max\\bigl\\{0, 1 - y_i (w x_i + b)\\bigr\\} $$\n$$ \\mathcal{L}_{\\mathrm{log}}(w,b) = \\frac{1}{N}\\sum_{i=1}^{N} \\log\\bigl(1 + \\exp\\bigl(-y_i (w x_i + b)\\bigr)\\bigr) $$\n\nBoth the hinge loss function, $f(z) = \\max(0, 1-z)$, and the log-loss function, $f(z) = \\log(1+\\exp(-z))$, are convex functions of their argument $z$. When the argument $z$ is a linear function of the parameters $(w,b)$, as in $z_i = y_i (w x_i + b)$, the resulting loss functions $\\mathcal{L}_{\\mathrm{hinge}}(w,b)$ and $\\mathcal{L}_{\\mathrm{log}}(w,b)$ are convex with respect to $(w,b)$. This is because composition with an affine mapping preserves convexity, and the sum of convex functions is convex.\n\nTherefore, by the definition of convexity, the inequality $L(t \\theta_1 + (1-t)\\theta_2) \\le t L(\\theta_1) + (1-t)L(\\theta_2)$ must hold for any choice of parameters $\\theta_1=(w_1, b_1)$, $\\theta_2=(w_2, b_2)$ and any $t \\in [0,1]$. The problem asks to verify this for the specific parameters $(w_1, b_1) = (4.0, -1.0)$, $(w_2, b_2) = (-2.0, 0.5)$, and $t=0.5$. Since both losses are known to be convex, the inequality will be satisfied in both cases. The required outputs are `True` for both Task 2 and Task 3.\n\n### Task 4: Optimization via Newton's Method\n\nThis task demonstrates a key advantage of convex optimization: convergence to a unique global minimum regardless of the starting point. We are asked to minimize the strictly convex logistic loss $\\mathcal{L}_{\\mathrm{log}}(w,b)$ using Newton's method from two different initializations: $(w,b)=(0.0,0.0)$ and $(w,b)=(10.0,-10.0)$.\n\nNewton's method iteratively finds the minimum of a function by using a second-order Taylor approximation. The update step for parameters $\\mathbf{p} = [w,b]^T$ is:\n$$ \\mathbf{p}_{k+1} = \\mathbf{p}_k - \\lambda_k H_k^{-1} g_k $$\nwhere $g_k = \\nabla \\mathcal{L}_{\\mathrm{log}}(\\mathbf{p}_k)$ is the gradient, $H_k = \\nabla^2 \\mathcal{L}_{\\mathrm{log}}(\\mathbf{p}_k)$ is the Hessian matrix, and $\\lambda_k$ is a step size determined by backtracking line search to ensure sufficient decrease in the loss.\n\nBecause $\\mathcal{L}_{\\mathrm{log}}(w,b)$ is strictly convex for this dataset (the features $[x_i, 1]^T$ are not collinear), it possesses a unique global minimum $(w^\\star, b^\\star)$. Newton's method, with its quadratic convergence rate near the minimum, is a powerful algorithm that is guaranteed to converge to this unique optimum from any reasonable starting point. Therefore, we expect the optimization runs starting from $(0.0,0.0)$ and $(10.0,-10.0)$ to yield solutions $(w_A^\\star, b_A^\\star)$ and $(w_B^\\star, b_B^\\star)$ that are identical within numerical precision. The test of whether both runs converge to the same solution, as defined by the given tolerances on the parameters and the loss value, should pass. The required output is `True`.\n\n### Task 5: Hessian Analysis at the Optimum\n\nThe final task is to compute the Hessian matrix of $\\mathcal{L}_{\\mathrm{log}}(w,b)$ at the found minimizer $(w^\\star,b^\\star)$ and determine its smallest eigenvalue. The formula for the Hessian is:\n$$ H = \\frac{1}{N}\\sum_{i=1}^{N} \\sigma(z_i)\\sigma(-z_i) \\begin{bmatrix} x_i^2  x_i \\\\ x_i  1 \\end{bmatrix}, \\quad \\text{where } z_i = y_i(w x_i + b) $$\nand $\\sigma(z) = (1+\\exp(-z))^{-1}$ is the sigmoid function. The term $\\sigma(z_i)\\sigma(-z_i)$ is the derivative of the sigmoid function and is always positive. The matrix $\\begin{psmallmatrix} x_i^2  x_i \\\\ x_i  1 \\end{psmallmatrix}$ is the outer product $\\mathbf{v}_i \\mathbf{v}_i^T$ where $\\mathbf{v}_i = [x_i, 1]^T$, which is positive semidefinite.\n\nSince the data points $x_i$ are not all identical, the vectors $\\mathbf{v}_i$ span the full two-dimensional parameter space. The Hessian is a sum of positive semidefinite matrices, and because the vectors $\\mathbf{v}_i$ span the space and the coefficients $\\sigma(z_i)\\sigma(-z_i)$ are positive, the resulting Hessian $H$ is positive definite.\n\nA fundamental property of a positive definite matrix is that all its eigenvalues are strictly positive. Therefore, calculating the smallest eigenvalue of the Hessian at the minimizer serves as a numerical confirmation of the strict convexity of the loss function at that point. We expect the result to be a positive floating-point number.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Executes all tasks described in the problem statement and prints the results.\n    \"\"\"\n\n    results = []\n\n    # --- Task 1: Convexity Analysis of IoU Loss ---\n    def iou_loss(theta):\n        # Ground-truth set B = [0,1], Predicted set A = [theta, theta+1]\n        intersection = max(0.0, min(theta + 1.0, 1.0) - max(theta, 0.0))\n        # Union = |A| + |B| - Intersection = 1 + 1 - intersection\n        union = 2.0 - intersection\n        if union == 0.0:  # This happens only if intersection is 2.0, impossible.\n            return 0.0 if intersection == 1.0 else 1.0 # Perfect match - loss 0\n        return 1.0 - intersection / union\n\n    theta1, theta2, t = 0.0, 1.0, 0.5\n    theta_m = t * theta1 + (1.0 - t) * theta2\n    \n    loss_m = iou_loss(theta_m)\n    loss_1 = iou_loss(theta1)\n    loss_2 = iou_loss(theta2)\n    \n    is_convex_iou = loss_m = t * loss_1 + (1.0 - t) * loss_2\n    results.append(is_convex_iou)\n\n    # --- Setup for Tasks 2-5 ---\n    N = 121\n    x_data = np.linspace(-1.5, 1.5, N)\n    y_data = np.ones(N)\n    y_data[(x_data  0) | (x_data  1)] = -1.0\n\n    # --- Task 2: Convexity of Hinge Loss ---\n    def hinge_loss(w, b, x, y):\n        margins = y * (w * x + b)\n        losses = np.maximum(0.0, 1.0 - margins)\n        return np.mean(losses)\n\n    w1, b1 = 4.0, -1.0\n    w2, b2 = -2.0, 0.5\n    wm = t * w1 + (1.0 - t) * w2\n    bm = t * b1 + (1.0 - t) * b2\n\n    hinge_m = hinge_loss(wm, bm, x_data, y_data)\n    hinge_1 = hinge_loss(w1, b1, x_data, y_data)\n    hinge_2 = hinge_loss(w2, b2, x_data, y_data)\n    \n    is_convex_hinge = hinge_m = t * hinge_1 + (1.0 - t) * hinge_2\n    results.append(is_convex_hinge)\n\n    # --- Task 3: Convexity of Logistic Loss ---\n    def logistic_loss(w, b, x, y):\n        # Use numerically stable log(1+exp(z)) = logaddexp(0, z)\n        z = -y * (w * x + b)\n        losses = np.logaddexp(0, z)\n        return np.mean(losses)\n\n    log_m = logistic_loss(wm, bm, x_data, y_data)\n    log_1 = logistic_loss(w1, b1, x_data, y_data)\n    log_2 = logistic_loss(w2, b2, x_data, y_data)\n    \n    is_convex_log = log_m = t * log_1 + (1.0 - t) * log_2\n    results.append(is_convex_log)\n\n    # --- Functions for Task 4  5 ---\n    def log_loss_grad_hess(w, b, x, y):\n        n_points = len(x)\n        scores = w * x + b\n        z = y * scores\n        \n        # Gradient\n        sigma_neg_z = expit(-z)\n        grad_w = np.sum(sigma_neg_z * (-y * x)) / n_points\n        grad_b = np.sum(sigma_neg_z * (-y)) / n_points\n        grad = np.array([grad_w, grad_b])\n        \n        # Hessian\n        sigma_z = expit(z)\n        coeffs = sigma_z * sigma_neg_z\n        H_ww = np.sum(coeffs * x**2) / n_points\n        H_wb = np.sum(coeffs * x) / n_points\n        H_bb = np.sum(coeffs) / n_points\n        H = np.array([[H_ww, H_wb], [H_wb, H_bb]])\n        \n        return grad, H\n        \n    def newton_minimize(initial_params, x, y):\n        params = np.array(initial_params, dtype=float)\n        max_iter = 100\n        step_norm_tol = 1e-10\n        alpha, beta = 0.25, 0.5\n      \n        for _ in range(max_iter):\n            w, b = params\n            grad, H = log_loss_grad_hess(w, b, x, y)\n            \n            try:\n                # Solve H * step = -grad for the Newton step\n                step = np.linalg.solve(H, -grad)\n            except np.linalg.LinAlgError:\n                # Hessian is singular, should not happen for this problem\n                break\n\n            # Backtracking line search\n            line_search_t = 1.0\n            current_loss = logistic_loss(w, b, x, y)\n            armijo_term = alpha * np.dot(grad, step)\n            \n            while True:\n                next_w, next_b = params + line_search_t * step\n                if logistic_loss(next_w, next_b, x, y) = current_loss + line_search_t * armijo_term:\n                    break\n                line_search_t *= beta\n                if line_search_t  1e-15:  # Failsafe\n                    break\n            \n            final_step = line_search_t * step\n            params += final_step\n            \n            if np.linalg.norm(final_step)  step_norm_tol:\n                break\n                \n        return params\n\n    # --- Task 4: Newton's Method Convergence ---\n    init_A = (0.0, 0.0)\n    init_B = (10.0, -10.0)\n    \n    sol_A = newton_minimize(init_A, x_data, y_data)\n    sol_B = newton_minimize(init_B, x_data, y_data)\n    \n    wA, bA = sol_A\n    wB, bB = sol_B\n    loss_A = logistic_loss(wA, bA, x_data, y_data)\n    loss_B = logistic_loss(wB, bB, x_data, y_data)\n    \n    param_dist = np.linalg.norm(sol_A - sol_B)\n    loss_diff = abs(loss_A - loss_B)\n    \n    converged_to_same = (param_dist  1e-6) and (loss_diff  1e-8)\n    results.append(converged_to_same)\n\n    # --- Task 5: Hessian Eigenvalue at Minimum ---\n    w_star, b_star = sol_A  # Both solutions should be the same\n    _, H_star = log_loss_grad_hess(w_star, b_star, x_data, y_data)\n    eigenvalues = np.linalg.eigvalsh(H_star)\n    min_eigenvalue = np.min(eigenvalues)\n    results.append(min_eigenvalue)\n    \n    # --- Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}