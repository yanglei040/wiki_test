{
    "hands_on_practices": [
        {
            "introduction": "许多重要的损失函数，例如用于稳健回归的分位数（或弹球）损失函数，并非处处可微。本练习将介绍次梯度这一基本概念，它是梯度的推广，并指导你通过实现次梯度法来最小化这类损失函数。通过从第一性原理推导次梯度并构建优化算法，你将深入理解如何处理机器学习中的非光滑凸优化问题。",
            "id": "3146402",
            "problem": "考虑为任意残差 $r \\in \\mathbb{R}$ 和分位数参数 $\\tau \\in (0,1)$ 定义的分位数（弹球）损失\n$$\n\\ell_{\\tau}(r) = \\max\\{\\tau\\, r, (\\tau - 1)\\, r\\}。\n$$\n您将从第一性原理出发完成两项任务，并实现一个可在小规模测试用例上进行测试的次梯度方法。\n\n任务 A（第一性原理次微分）。仅使用凸次微分的定义以及线性和凸函数的基本性质，推导出在 $r=0$ 处的完整次微分 $\\partial \\ell_{\\tau}(0)$。您的推导必须从以下定义开始：一个标量 $g$ 属于 $\\partial \\ell_{\\tau}(0)$ 当且仅当对于所有 $r \\in \\mathbb{R}$，\n$$\n\\ell_{\\tau}(r) \\ge \\ell_{\\tau}(0) + g\\,(r - 0)。\n$$\n清晰地指出所有满足此不等式的 $g$，并以区间形式给出集合 $\\partial \\ell_{\\tau}(0)$。\n\n任务 B（带平局决胜规则的次梯度方法）。考虑带目标函数的一维经验分位数回归\n$$\nf(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell_{\\tau}(y_i - \\theta)，\n$$\n其中 $\\theta \\in \\mathbb{R}$ 是决策变量，$y_1,\\dots,y_n \\in \\mathbb{R}$ 是给定的观测值。设计一个次梯度方法来最小化 $f(\\theta)$，该方法：\n- 对迭代索引 $k \\in \\{0,1,2,\\dots\\}$，使用步长方案 $\\alpha_k = \\dfrac{a_0}{\\sqrt{k+1}}$，其中 $a_0$ 是给定的正常数，\n- 通过对在残差 $r_i(\\theta_k) = y_i - \\theta_k$ 处计算的逐元素次梯度进行平均，来构成一个次梯度 $\\hat{g}_k \\in \\partial f(\\theta_k)$，\n- 对于任何零残差 $r_i(\\theta_k) = 0$ 处的多值次梯度，通过一个确定性的平局决胜规则来解决，该规则选择您在任务 A 中推导出的次微分区间的中点，以及\n- 通过 $\\theta_{k+1} = \\theta_k - \\alpha_k\\, \\hat{g}_k$ 进行更新。\n\n您的实现必须是纯数值的，并且不得依赖任何符号操作。对于下面的每个测试用例，您的程序必须计算：\n- 在 $r=0$ 处的次微分区间端点 $(\\tau - 1, \\tau)$，\n- 在恰好 $T$ 次迭代后，次梯度方法的最终迭代值 $\\theta_T$。\n\n测试套件。您的程序必须按此确切顺序在以下用例上运行该方法：\n- 用例 1：$\\tau = 0.5$，数据 $y = (-1.0, 0.0, 2.0)$，初始值 $\\theta_0 = 1.0$， $a_0 = 0.7$，迭代次数 $T = 60$。\n- 用例 2：$\\tau = 0.1$，数据 $y = (-3.0, -2.0, 0.0, 5.0)$，初始值 $\\theta_0 = 0.0$， $a_0 = 0.6$，迭代次数 $T = 80$。\n- 用例 3：$\\tau = 0.5$，数据 $y = (0.0, 0.0, 0.0, 0.0)$，初始值 $\\theta_0 = 1.0$， $a_0 = 0.8$，迭代次数 $T = 40$。\n- 用例 4：$\\tau = 0.9$，数据 $y = (-1.0, 0.0, 3.0, 10.0)$，初始值 $\\theta_0 = 0.0$， $a_0 = 0.5$，迭代次数 $T = 100$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个结果列表，每个用例一个结果，每个结果是一个包含三个浮点数 $[\\tau - 1, \\tau, \\theta_T]$ 的列表。总体输出必须是这些按用例排列的列表组成的单个列表，打印在一行上，例如：\n$[[x_{1},y_{1},z_{1}],[x_{2},y_{2},z_{2}],\\dots]$。",
            "solution": "该问题提出了两个任务：首先，从第一性原理出发推导分位数损失函数在原点处的次微分；其次，为一维分位数回归问题设计并实现一个次梯度方法。该问题表述清晰，科学上合理，并包含了确定性解所需的所有信息。\n\n### 任务 A：次微分 $\\partial \\ell_{\\tau}(0)$ 的推导\n\n分位数损失函数，或称弹球损失，由下式给出\n$$\n\\ell_{\\tau}(r) = \\max\\{\\tau r, (\\tau - 1) r\\}\n$$\n其中残差 $r \\in \\mathbb{R}$，分位数参数 $\\tau \\in (0, 1)$。该函数是凸函数，因为它是两个线性（因此也是凸）函数 $r$ 的最大值。\n\n凸函数 $h:\\mathbb{R} \\to \\mathbb{R}$ 在点 $x_0$ 处的次微分是指所有穿过点 $(x_0, h(x_0))$ 并且位于 $h(x)$ 图像之上或之下的直线的斜率 $g$ 的集合。形式上，次微分 $\\partial h(x_0)$ 是所有标量 $g \\in \\mathbb{R}$（称为次梯度）的集合，使得对于所有 $x \\in \\mathbb{R}$：\n$$\nh(x) \\ge h(x_0) + g(x - x_0)\n$$\n\n我们被要求找出在点 $r=0$ 处的次微分 $\\partial \\ell_{\\tau}(0)$。首先，我们计算函数在该点的值：\n$$\n\\ell_{\\tau}(0) = \\max\\{\\tau \\cdot 0, (\\tau - 1) \\cdot 0\\} = \\max\\{0, 0\\} = 0.\n$$\n将 $h = \\ell_{\\tau}$ 和 $x_0 = 0$ 代入定义中，一个标量 $g$ 属于 $\\partial \\ell_{\\tau}(0)$ 当且仅当对于所有 $r \\in \\mathbb{R}$：\n$$\n\\ell_{\\tau}(r) \\ge \\ell_{\\tau}(0) + g(r - 0)\n$$\n$$\n\\ell_{\\tau}(r) \\ge g r\n$$\n这个不等式必须对所有可能的 $r$ 值都成立。我们通过考虑 $r$ 的两种符号情况来分析它。\n\n**情况 1：$r > 0$**\n当 $r$ 为正时，我们计算 $\\ell_{\\tau}(r)$。因为 $\\tau \\in (0, 1)$，所以 $\\tau$ 是正的，而 $\\tau - 1$ 是负的。因此，$\\tau r > 0$ 且 $(\\tau - 1) r  0$。\n所以最大值为 $\\tau r$：\n$$\n\\ell_{\\tau}(r) = \\tau r \\quad \\text{for } r > 0.\n$$\n将此代入次梯度不等式，我们得到：\n$$\n\\tau r \\ge g r\n$$\n因为 $r > 0$，我们可以用 $r$ 除两边而不改变不等式的方向：\n$$\n\\tau \\ge g\n$$\n\n**情况 2：$r  0$**\n当 $r$ 为负时，$\\tau r$ 是负的，而 $(\\tau - 1) r$ 是正的。\n所以最大值为 $(\\tau - 1) r$：\n$$\n\\ell_{\\tau}(r) = (\\tau - 1) r \\quad \\text{for } r  0.\n$$\n将此代入次梯度不等式：\n$$\n(\\tau - 1) r \\ge g r\n$$\n因为 $r  0$，用 $r$ 除两边会反转不等式的方向：\n$$\n\\tau - 1 \\le g\n$$\n\n**结论\n**要成为 $r=0$ 处的有效次梯度 $g$，标量 $g$ 必须同时满足从两种情况推导出的条件。也就是说，$g$ 必须同时满足 $g \\le \\tau$ 和 $g \\ge \\tau - 1$。\n综合这些条件，我们有：\n$$\n\\tau - 1 \\le g \\le \\tau\n$$\n所有这样的 $g$ 的集合是闭区间 $[\\tau - 1, \\tau]$。因此，分位数损失在 $r=0$ 处的次微分是：\n$$\n\\partial \\ell_{\\tau}(0) = [\\tau - 1, \\tau]\n$$\n\n### 任务 B：次梯度方法的设计与实现\n\n要最小化的目标函数是一维分位数回归的经验风险：\n$$\nf(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell_{\\tau}(y_i - \\theta)\n$$\n这个函数是有限个凸函数的和，因此它本身也是凸的。我们可以使用次梯度方法来最小化它。参数 $\\theta$ 在第 $k$ 次迭代的更新规则如下：\n$$\n\\theta_{k+1} = \\theta_k - \\alpha_k \\hat{g}_k\n$$\n其中 $\\alpha_k$ 是步长，$\\hat{g}_k \\in \\partial f(\\theta_k)$ 是目标函数在当前迭代值 $\\theta_k$ 处的一个次梯度。\n\n**1. 目标函数 $f(\\theta)$ 的次梯度**\n多个凸函数之和的次微分是它们各自次微分的闵可夫斯基和。应用此性质和次微分的链式法则：\n$$\n\\partial f(\\theta) = \\partial \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\ell_{\\tau}(y_i - \\theta) \\right) = \\frac{1}{n} \\sum_{i=1}^{n} \\partial_{\\theta} \\left( \\ell_{\\tau}(y_i - \\theta) \\right)\n$$\n令 $r_i(\\theta) = y_i - \\theta$。$r_i$ 关于 $\\theta$ 的导数是 $\\frac{d r_i}{d \\theta} = -1$。\n使用链式法则，第 $i$ 项关于 $\\theta$ 的次微分是：\n$$\n\\partial_{\\theta} \\ell_{\\tau}(y_i - \\theta) = \\partial_{r} \\ell_{\\tau}(r_i(\\theta)) \\cdot \\frac{d r_i}{d \\theta} = \\partial_{r} \\ell_{\\tau}(y_i - \\theta) \\cdot (-1)\n$$\n其中 $\\partial_{r} \\ell_{\\tau}$ 是 $\\ell_{\\tau}$ 关于其参数 $r$ 的次微分。\n因此，可以通过从每个 $\\partial_{r} \\ell_{\\tau}(y_i - \\theta_k)$ 中选择一个次梯度 $s_{i,k}$ 并求和来构造一个次梯度 $\\hat{g}_k \\in \\partial f(\\theta_k)$：\n$$\n\\hat{g}_k = \\frac{1}{n} \\sum_{i=1}^{n} (-s_{i,k}) = -\\frac{1}{n} \\sum_{i=1}^{n} s_{i,k}\n$$\n\n**2. 逐元素次梯度选择 ($s_{i,k}$)**\n问题要求一个特定的确定性规则来选择 $s_{i,k} \\in \\partial_{r} \\ell_{\\tau}(y_i - \\theta_k)$。令 $r_i = y_i - \\theta_k$。\n- 如果 $r_i > 0$，$\\ell_{\\tau}(r_i)$ 可微，导数为 $\\tau$。次微分是一个单元素集：$\\partial_{r} \\ell_{\\tau}(r_i) = \\{\\tau\\}$。我们必须选择 $s_{i,k} = \\tau$。\n- 如果 $r_i  0$，$\\ell_{\\tau}(r_i)$ 可微，导数为 $\\tau-1$。次微分是一个单元素集：$\\partial_{r} \\ell_{\\tau}(r_i) = \\{\\tau-1\\}$。我们必须选择 $s_{i,k} = \\tau-1$。\n- 如果 $r_i = 0$，次微分是区间 $[\\tau-1, \\tau]$，如任务 A 所推导。问题指定了一个平局决胜规则，即选择该区间的中点：$s_{i,k} = \\frac{(\\tau-1) + \\tau}{2} = \\tau - 0.5$。\n\n**3. 更新规则**\n将 $\\hat{g}_k$ 的表达式代入更新方程：\n$$\n\\theta_{k+1} = \\theta_k - \\alpha_k \\left(-\\frac{1}{n} \\sum_{i=1}^{n} s_{i,k}\\right) = \\theta_k + \\frac{\\alpha_k}{n} \\sum_{i=1}^{n} s_{i,k}\n$$\n步长方案给定为 $\\alpha_k = \\frac{a_0}{\\sqrt{k+1}}$。\n\n**4. 算法摘要**\n完整的算法如下：\n1. 初始化 $\\theta_0$。\n2. 对于 $k = 0, 1, 2, \\dots, T-1$：\n   a. 计算步长：$\\alpha_k = \\frac{a_0}{\\sqrt{k+1}}$。\n   b. 初始化逐元素次梯度的和：$S_k = 0$。\n   c. 对于从 $i=1$ 到 $n$ 的每个观测值 $y_i$：\n      i. 计算残差：$r_i = y_i - \\theta_k$。\n      ii. 根据 $r_i$ 的符号选择次梯度 $s_{i,k}$：\n         - 如果 $r_i > 0$，$s_{i,k} = \\tau$。\n         - 如果 $r_i  0$，$s_{i,k} = \\tau - 1$。\n         - 如果 $r_i = 0$，$s_{i,k} = \\tau - 0.5$。\n      iii. 加到总和中：$S_k = S_k + s_{i,k}$。\n   d. 更新参数：$\\theta_{k+1} = \\theta_k + \\frac{\\alpha_k}{n} S_k$。\n3. 最终结果是迭代值 $\\theta_T$。\n\n这个完全指定、确定性的算法将被实现以解决给定的测试用例。每个用例所需的输出是次微分区间的端点 $(\\tau - 1, \\tau)$ 和最终迭代值 $\\theta_T$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the quantile regression problem using a subgradient method\n    for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: tau, y, theta_0, a_0, T\n        (0.5, np.array([-1.0, 0.0, 2.0]), 1.0, 0.7, 60),\n        # Case 2\n        (0.1, np.array([-3.0, -2.0, 0.0, 5.0]), 0.0, 0.6, 80),\n        # Case 3\n        (0.5, np.array([0.0, 0.0, 0.0, 0.0]), 1.0, 0.8, 40),\n        # Case 4\n        (0.9, np.array([-1.0, 0.0, 3.0, 10.0]), 0.0, 0.5, 100),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        tau, y, theta_0, a_0, T = case\n        n = len(y)\n        \n        # Initialize theta\n        theta_k = theta_0\n        \n        # Perform T iterations of the subgradient method\n        for k in range(T):\n            # Calculate step size\n            alpha_k = a_0 / np.sqrt(k + 1)\n            \n            # Calculate the sum of elementwise subgradients\n            s_sum = 0.0\n            for y_i in y:\n                residual = y_i - theta_k\n                \n                # Select subgradient s_i based on the sign of the residual\n                if residual > 0:\n                    s_i = tau\n                elif residual  0:\n                    s_i = tau - 1\n                else:  # residual == 0, apply tie-breaking rule\n                    s_i = tau - 0.5\n                \n                s_sum += s_i\n            \n            # Update theta\n            # The subgradient of the objective f(theta) is -(1/n) * s_sum.\n            # The update is theta_{k+1} = theta_k - alpha_k * g_k\n            # = theta_k - alpha_k * (-(1/n) * s_sum)\n            # = theta_k + (alpha_k / n) * s_sum\n            theta_k = theta_k + (alpha_k / n) * s_sum\n        \n        # The final iterate is theta_T\n        theta_T = theta_k\n        \n        # The subdifferential interval at r=0 is [tau - 1, tau]\n        lower_bound = tau - 1\n        upper_bound = tau\n        \n        # Store the results for the current case\n        results.append([lower_bound, upper_bound, theta_T])\n\n    # Format the final output string\n    # e.g., [[-0.5,0.5,-0.02109...],[-0.9,0.1,-0.1654...],...]\n    result_str = \"[\" + \",\".join(f\"[{res[0]},{res[1]},{res[2]}]\" for res in results) + \"]\"\n    \n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "现实世界的机器学习模型通常将一个光滑的数据拟合项（如逻辑损失）与一个非光滑的正则化项（如 $L_1$ 或弹性网络惩罚）相结合，以防止过拟合并鼓励稀疏性。本练习将演示如何使用近端梯度法高效地解决此类复合问题。你将亲自推导并实现一个名为近端算子的关键组件，从而掌握处理现代机器学习中常见的结构化优化问题的强大技术。",
            "id": "3146352",
            "problem": "要求您推导一个邻近映射（proximal mapping），并实现一个邻近梯度法（proximal gradient method），用于处理带有弹性网络正则化（elastic-net regularization）的二元逻辑回归中的复合目标。核心目标函数为\n$$\nF(\\boldsymbol{\\theta}) \\;=\\; \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\log\\!\\big(1+\\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\theta})\\big) - y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right)\\;+\\;\\lambda_1 \\|\\boldsymbol{\\theta}\\|_1\\;+\\;\\frac{\\lambda_2}{2}\\|\\boldsymbol{\\theta}\\|_2^2,\n$$\n其中 $\\boldsymbol{\\theta}\\in\\mathbb{R}^d$，$\\mathbf{x}_i\\in\\mathbb{R}^d$，$y_i\\in\\{0,1\\}$，$\\lambda_1\\ge 0$，$\\lambda_2\\ge 0$，$n$ 是样本数量。\n\n任务：\n1) 仅从邻近算子（proximal operator）的定义以及凸函数和次梯度（subgradient）的基本性质出发，推导邻近映射的闭式表达式（closed-form expression）\n$$\n\\operatorname{prox}_{t g}(\\mathbf{v})\\;=\\;\\underset{\\boldsymbol{\\theta}\\in\\mathbb{R}^d}{\\arg\\min}\\;\\left\\{t\\left(\\lambda_1\\|\\boldsymbol{\\theta}\\|_1+\\frac{\\lambda_2}{2}\\|\\boldsymbol{\\theta}\\|_2^2\\right)+\\frac{1}{2}\\|\\boldsymbol{\\theta}-\\mathbf{v}\\|_2^2\\right\\},\n$$\n对于任意步长 $t>0$ 和任意 $\\mathbf{v}\\in\\mathbb{R}^d$。您的推导必须从邻近算子的定义开始，并且只使用基本的凸分析知识（例如，可分离性（separability）和次梯度最优性条件（subgradient optimality conditions））。不要直接假设公式成立；相反，应展示其成立的原因。\n\n2) 实现邻近梯度法以最小化 $F(\\boldsymbol{\\theta})$，将逻辑损失（logistic loss）视为光滑部分，弹性网络项视为由邻近映射处理的非光滑部分。使用以下原则作为您的出发点：\n- 光滑部分 $f(\\boldsymbol{\\theta})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\log(1+\\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\theta})) - y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right)$ 的梯度为\n$$\n\\nabla f(\\boldsymbol{\\theta})=\\frac{1}{n}\\mathbf{X}^\\top\\left(\\boldsymbol{\\sigma}(\\mathbf{X}\\boldsymbol{\\theta})-\\mathbf{y}\\right),\n$$\n其中 $\\boldsymbol{\\sigma}(\\mathbf{z})$ 将 $\\sigma(z)=\\frac{1}{1+\\exp(-z)}$ 按元素（elementwise）应用。\n- $\\nabla f$ 的一个有效的全局利普希茨常数（global Lipschitz constant）是 $L\\le \\frac{1}{4n}\\|\\mathbf{X}\\|_2^2$，其中 $\\|\\mathbf{X}\\|_2$ 是 $\\mathbf{X}$ 的谱范数（spectral norm）。当 $\\|\\mathbf{X}\\|_2>0$ 时，使用步长 $t=\\frac{1}{L}$。如果 $\\|\\mathbf{X}\\|_2=0$，则使用 $t=1$。\n\n您的实现必须执行 $K$ 次固定的邻近梯度迭代：\n$$\n\\boldsymbol{\\theta}^{k+1}=\\operatorname{prox}_{t g}\\!\\left(\\boldsymbol{\\theta}^k - t\\,\\nabla f(\\boldsymbol{\\theta}^k)\\right),\n$$\n从指定的初始向量 $\\boldsymbol{\\theta}^0$ 开始（如果未另外指定，则默认为零向量）。\n\n测试套件和要求的输出：\n实现您的程序，为以下测试用例计算输出。在所有情况下，当要求报告一个浮点数或一个浮点数列表时，请将每个值四舍五入到恰好 $6$ 位小数。\n\n- 测试用例 A (邻近映射检查):\n  - 参数: $\\mathbf{v}=[-0.5,\\,0.2,\\,3.0]$, $t=0.5$, $\\lambda_1=0.3$, $\\lambda_2=0.4$。\n  - 输出: 向量 $\\operatorname{prox}_{t g}(\\mathbf{v})$，以一个包含 $3$ 个浮点数的列表形式。\n\n- 测试用例 B (正常路径优化):\n  - 数据: \n    $$\n    \\mathbf{X}=\\begin{bmatrix}\n    0.5  1.0\\\\\n    1.5  2.0\\\\\n    -1.0  -0.5\\\\\n    -1.5  -1.0\\\\\n    0.3  -0.2\\\\\n    -0.3  0.2\n    \\end{bmatrix},\\quad\n    \\mathbf{y}=\\begin{bmatrix}1\\\\1\\\\0\\\\0\\\\1\\\\0\\end{bmatrix}.\n    $$\n  - 正则化: $\\lambda_1=0.1$, $\\lambda_2=0.2$。\n  - 迭代次数: $K=300$。\n  - 初始化: $\\boldsymbol{\\theta}^0=\\mathbf{0}$。\n  - 输出: 最终的目标值 $F(\\boldsymbol{\\theta}^K)$，以单个浮点数形式。\n\n- 测试用例 C (仅岭回归边界情况):\n  - $\\mathbf{X}$ 和 $\\mathbf{y}$ 与测试用例 B 中相同。\n  - 正则化: $\\lambda_1=0$, $\\lambda_2=0.5$。\n  - 迭代次数: $K=300$。\n  - 初始化: $\\boldsymbol{\\theta}^0=\\mathbf{0}$。\n  - 输出: 最终的目标值 $F(\\boldsymbol{\\theta}^K)$，以单个浮点数形式。\n\n- 测试用例 D (仅Lasso回归边界情况):\n  - $\\mathbf{X}$ 和 $\\mathbf{y}$ 与测试用例 B 中相同。\n  - 正则化: $\\lambda_1=0.5$, $\\lambda_2=0$。\n  - 迭代次数: $K=400$。\n  - 初始化: $\\boldsymbol{\\theta}^0=\\mathbf{0}$。\n  - 输出: 最终的目标值 $F(\\boldsymbol{\\theta}^K)$，以单个浮点数形式。\n\n- 测试用例 E (零设计矩阵和非零初始化的边缘情况):\n  - 数据:\n    $$\n    \\mathbf{X}=\\begin{bmatrix}\n    0  0\\\\\n    0  0\\\\\n    0  0\n    \\end{bmatrix},\\quad\n    \\mathbf{y}=\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}.\n    $$\n  - 正则化: $\\lambda_1=0.2$, $\\lambda_2=0.1$。\n  - 迭代次数: $K=5$。\n  - 初始化: $\\boldsymbol{\\theta}^0=\\begin{bmatrix}1.0\\\\-1.0\\end{bmatrix}$。\n  - 对于此情况，使用上述步长规则；由于 $\\|\\mathbf{X}\\|_2=0$，您必须使用 $t=1$。\n  - 输出: 最终的参数向量 $\\boldsymbol{\\theta}^K$，以一个包含 $2$ 个浮点数的列表形式。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含按顺序排列的结果，格式为方括号括起来的逗号分隔列表：$\\left[\\text{A},\\text{B},\\text{C},\\text{D},\\text{E}\\right]$，其中 $\\text{A}$ 是测试用例 A 的列表，$\\text{B}$ 是测试用例 B 的浮点数，依此类推。例如：$[\\text{list},\\text{float},\\text{float},\\text{float},\\text{list}]$。所有浮点数都必须如上所述四舍五入到恰好 $6$ 位小数。",
            "solution": "我们从邻近算子的定义开始。对于一个真、闭、凸函数（proper, closed, convex function）$g:\\mathbb{R}^d\\to\\mathbb{R}\\cup\\{+\\infty\\}$ 和任意 $t>0$，\n$$\n\\operatorname{prox}_{t g}(\\mathbf{v}) \\;=\\; \\underset{\\boldsymbol{\\theta}\\in\\mathbb{R}^d}{\\arg\\min}\\;\\left\\{t\\,g(\\boldsymbol{\\theta}) + \\frac{1}{2}\\|\\boldsymbol{\\theta}-\\mathbf{v}\\|_2^2\\right\\}.\n$$\n这里 $g(\\boldsymbol{\\theta})=\\lambda_1\\|\\boldsymbol{\\theta}\\|_1+\\frac{\\lambda_2}{2}\\|\\boldsymbol{\\theta}\\|_2^2$，其中 $\\lambda_1\\ge 0$ 且 $\\lambda_2\\ge 0$。邻近定义中的目标是\n$$\nQ(\\boldsymbol{\\theta}) \\;=\\; t\\lambda_1\\|\\boldsymbol{\\theta}\\|_1 + \\frac{t\\lambda_2}{2}\\|\\boldsymbol{\\theta}\\|_2^2 + \\frac{1}{2}\\|\\boldsymbol{\\theta}-\\mathbf{v}\\|_2^2.\n$$\n根据可分离性（separability），$Q(\\boldsymbol{\\theta})$ 可以按坐标分解：$Q(\\boldsymbol{\\theta})=\\sum_{j=1}^{d} q_j(\\theta_j)$，其中\n$$\nq_j(u)= t\\lambda_1 |u| + \\frac{t\\lambda_2}{2} u^2 + \\frac{1}{2}(u - v_j)^2.\n$$\n因此，$\\operatorname{prox}_{t g}(\\mathbf{v})$ 可以通过独立地最小化每个 $q_j(u)$ 来获得。我们求解标量 $u$。\n\n考虑凸函数的最优性次梯度条件：$0\\in \\partial q_j(u^\\star)$。$q_j$ 的次梯度是\n$$\n\\partial q_j(u) \\;=\\; t\\lambda_1\\,\\partial |u| + t\\lambda_2 u + (u - v_j),\n$$\n其中，当 $u\\neq 0$ 时 $\\partial |u|=\\{\\operatorname{sign}(u)\\}$，当 $u=0$ 时 $\\partial |u|=[-1,1]$。\n\n情况 $u^\\star\\neq 0$：则 $0 = t\\lambda_1 \\operatorname{sign}(u^\\star) + (t\\lambda_2+1)u^\\star - v_j$，这可以得出\n$$\nu^\\star \\;=\\; \\frac{v_j - t\\lambda_1 \\operatorname{sign}(u^\\star)}{1 + t\\lambda_2}.\n$$\n一致性要求 $\\operatorname{sign}(u^\\star)=\\operatorname{sign}(v_j - t\\lambda_1 \\operatorname{sign}(u^\\star))$，这在 $|v_j|>t\\lambda_1$ 且 $u^\\star$ 与 $v_j$ 同号时成立。\n\n情况 $u^\\star=0$：则次梯度条件要求 $0\\in t\\lambda_1 [-1,1] - v_j$，等价于 $|v_j|\\le t\\lambda_1$。\n\n结合这两种情况，我们得到软阈值（soft-thresholding）结构。定义软阈值算子 $S_\\alpha:\\mathbb{R}\\to\\mathbb{R}$ 为\n$$\nS_\\alpha(v) \\;=\\; \\operatorname{sign}(v)\\,\\max\\{|v|-\\alpha,\\,0\\}.\n$$\n根据以上情况，解为\n$$\nu^\\star \\;=\\; \\frac{1}{1+t\\lambda_2}\\,S_{t\\lambda_1}(v_j),\n$$\n并且这可以按元素扩展到向量：\n$$\n\\operatorname{prox}_{t g}(\\mathbf{v}) \\;=\\; \\frac{1}{1+t\\lambda_2}\\,S_{t\\lambda_1}(\\mathbf{v}),\n$$\n其中 $S_{t\\lambda_1}$ 是按分量（componentwise）应用的。当 $\\lambda_2=0$ 时，该表达式简化为标准的软阈值；当 $\\lambda_1=0$ 时，简化为一个简单的收缩（shrinkage）$\\frac{1}{1+t\\lambda_2}\\mathbf{v}$。\n\n接下来，我们实现邻近梯度法来最小化 $F(\\boldsymbol{\\theta})=f(\\boldsymbol{\\theta})+g(\\boldsymbol{\\theta})$，其中\n$$\nf(\\boldsymbol{\\theta})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\log(1+\\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\theta})) - y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right),\\quad\ng(\\boldsymbol{\\theta})=\\lambda_1\\|\\boldsymbol{\\theta}\\|_1+\\frac{\\lambda_2}{2}\\|\\boldsymbol{\\theta}\\|_2^2.\n$$\n$f$ 的梯度由下式给出\n$$\n\\nabla f(\\boldsymbol{\\theta})=\\frac{1}{n}\\mathbf{X}^\\top(\\boldsymbol{\\sigma}(\\mathbf{X}\\boldsymbol{\\theta})-\\mathbf{y}),\n$$\n其中 $\\sigma(z)=\\frac{1}{1+\\exp(-z)}$。$f$ 的海森矩阵（Hessian）是\n$$\n\\nabla^2 f(\\boldsymbol{\\theta})=\\frac{1}{n}\\mathbf{X}^\\top \\mathbf{D}(\\boldsymbol{\\theta}) \\mathbf{X},\n$$\n其中 $\\mathbf{D}(\\boldsymbol{\\theta})=\\operatorname{diag}(\\sigma(z_i)(1-\\sigma(z_i)))$ 且 $z_i=\\mathbf{x}_i^\\top\\boldsymbol{\\theta}$。由于 $0\\le \\sigma(z)(1-\\sigma(z))\\le \\frac{1}{4}$，我们有一个一致的上界\n$$\n\\nabla^2 f(\\boldsymbol{\\theta}) \\preceq \\frac{1}{4n}\\mathbf{X}^\\top \\mathbf{X},\n$$\n这意味着 $\\nabla f$ 是利普希茨连续的（Lipschitz continuous），利普希茨常数 $L\\le \\frac{1}{4n}\\|\\mathbf{X}\\|_2^2$。因此，当 $\\|\\mathbf{X}\\|_2>0$ 时，一个有效的固定步长是 $t=\\frac{1}{L}=\\frac{4n}{\\|\\mathbf{X}\\|_2^2}$。如果 $\\|\\mathbf{X}\\|_2=0$，那么对所有 $\\boldsymbol{\\theta}$ 都有 $\\nabla f(\\boldsymbol{\\theta})=\\mathbf{0}$，任何正步长都是可接受的；为具体起见，我们指定 $t=1$。\n\n邻近梯度迭代则为\n$$\n\\boldsymbol{\\theta}^{k+1}=\\operatorname{prox}_{t g}\\!\\left(\\boldsymbol{\\theta}^k - t\\,\\nabla f(\\boldsymbol{\\theta}^k)\\right)\n= \\frac{1}{1+t\\lambda_2}\\,S_{t\\lambda_1}\\!\\left(\\boldsymbol{\\theta}^k - t\\,\\nabla f(\\boldsymbol{\\theta}^k)\\right).\n$$\n\n算法步骤：\n- 计算 $n$ 和 $d$，并初始化 $\\boldsymbol{\\theta}^0$。\n- 计算 $\\|\\mathbf{X}\\|_2$（谱范数）并设置 $t=\\frac{4n}{\\|\\mathbf{X}\\|_2^2}$（如果 $\\|\\mathbf{X}\\|_2>0$），否则设置 $t=1$。\n- 对于 $k=0,1,\\dots,K-1$：\n  - 计算 $\\nabla f(\\boldsymbol{\\theta}^k)=\\frac{1}{n}\\mathbf{X}^\\top(\\boldsymbol{\\sigma}(\\mathbf{X}\\boldsymbol{\\theta}^k)-\\mathbf{y})$。\n  - 形成梯度步 $\\mathbf{v}^k=\\boldsymbol{\\theta}^k - t\\,\\nabla f(\\boldsymbol{\\theta}^k)$。\n  - 应用邻近算子：$\\boldsymbol{\\theta}^{k+1}=\\frac{1}{1+t\\lambda_2}\\,S_{t\\lambda_1}(\\mathbf{v}^k)$。\n\n为了报告结果，我们评估目标函数\n$$\nF(\\boldsymbol{\\theta})=\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\log(1+\\exp(\\mathbf{x}_i^\\top \\boldsymbol{\\theta})) - y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right)+\\lambda_1\\|\\boldsymbol{\\theta}\\|_1+\\frac{\\lambda_2}{2}\\|\\boldsymbol{\\theta}\\|_2^2.\n$$\n\n测试套件涵盖：\n- 一个邻近映射检查，以数值方式验证推导出的公式。\n- 一个 $\\lambda_1>0$ 和 $\\lambda_2>0$ 的通用情况。\n- 边界情况 $\\lambda_1=0$（仅岭回归）和 $\\lambda_2=0$（仅Lasso回归）。\n- 一个 $\\mathbf{X}=\\mathbf{0}$ 且非零初始化的边缘情况，用于检验光滑梯度为零时邻近算子的动态。\n\n所有输出都四舍五入到恰好 $6$ 位小数，并汇总成所要求的单行列表格式。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigmoid(z):\n    # Numerically stable sigmoid\n    # For large negative values, exp(-z) may overflow; using standard implementation is fine for small test sizes\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef soft_threshold(v, alpha):\n    # Elementwise soft-thresholding: sign(v) * max(|v| - alpha, 0)\n    return np.sign(v) * np.maximum(np.abs(v) - alpha, 0.0)\n\ndef prox_elastic_net(v, t, lam1, lam2):\n    # prox_{t (lam1 ||.||_1 + (lam2/2)||.||_2^2)}(v) = (1 / (1 + t*lam2)) * S_{t*lam1}(v)\n    if t  0:\n        raise ValueError(\"Step size t must be nonnegative\")\n    if lam1  0 or lam2  0:\n        raise ValueError(\"Regularization parameters must be nonnegative\")\n    if t == 0:\n        return v.copy()\n    shrink = soft_threshold(v, t * lam1)\n    if lam2 == 0.0:\n        return shrink\n    return shrink / (1.0 + t * lam2)\n\ndef logistic_objective(X, y, theta, lam1, lam2):\n    n = X.shape[0]\n    z = X @ theta\n    # softplus: log(1 + exp(z))\n    # Use stable computation\n    # softplus(z) = max(z,0) + log(1 + exp(-|z|))\n    softplus = np.maximum(z, 0.0) + np.log1p(np.exp(-np.abs(z)))\n    data_term = (softplus - y * z).mean()\n    reg = lam1 * np.linalg.norm(theta, 1) + 0.5 * lam2 * np.dot(theta, theta)\n    return data_term + reg\n\ndef grad_logistic(X, y, theta):\n    n = X.shape[0]\n    z = X @ theta\n    s = sigmoid(z)\n    grad = (X.T @ (s - y)) / n\n    return grad\n\ndef spectral_norm(X):\n    # Compute spectral norm (2-norm). For small matrices, SVD is fine.\n    if X.size == 0:\n        return 0.0\n    return np.linalg.norm(X, 2)\n\ndef prox_gradient(X, y, lam1, lam2, K, theta0=None, t=None):\n    n, d = X.shape\n    if theta0 is None:\n        theta = np.zeros(d)\n    else:\n        theta = np.array(theta0, dtype=float).copy()\n    if t is None:\n        norm2 = spectral_norm(X)\n        if norm2 > 0:\n            L = (norm2 ** 2) / (4.0 * n)\n            t = 1.0 / L\n        else:\n            t = 1.0\n    # Iterate\n    for _ in range(K):\n        g = grad_logistic(X, y, theta)\n        v = theta - t * g\n        theta = prox_elastic_net(v, t, lam1, lam2)\n    return theta\n\ndef round_list(vals, decimals=6):\n    return [float(f\"{v:.{decimals}f}\") for v in vals]\n\ndef solve():\n    results = []\n\n    # Test case A: proximal mapping check\n    v_A = np.array([-0.5, 0.2, 3.0], dtype=float)\n    t_A = 0.5\n    lam1_A = 0.3\n    lam2_A = 0.4\n    prox_A = prox_elastic_net(v_A, t_A, lam1_A, lam2_A)\n    results.append(str(round_list(prox_A.tolist(), 6)))\n\n    # Shared dataset for B, C, D\n    X_shared = np.array([\n        [0.5, 1.0],\n        [1.5, 2.0],\n        [-1.0, -0.5],\n        [-1.5, -1.0],\n        [0.3, -0.2],\n        [-0.3, 0.2],\n    ], dtype=float)\n    y_shared = np.array([1, 1, 0, 0, 1, 0], dtype=float)\n\n    # Test case B: happy path\n    lam1_B, lam2_B = 0.1, 0.2\n    K_B = 300\n    theta_B = prox_gradient(X_shared, y_shared, lam1_B, lam2_B, K_B, theta0=np.zeros(2))\n    obj_B = logistic_objective(X_shared, y_shared, theta_B, lam1_B, lam2_B)\n    results.append(f\"{obj_B:.6f}\")\n\n    # Test case C: ridge-only boundary\n    lam1_C, lam2_C = 0.0, 0.5\n    K_C = 300\n    theta_C = prox_gradient(X_shared, y_shared, lam1_C, lam2_C, K_C, theta0=np.zeros(2))\n    obj_C = logistic_objective(X_shared, y_shared, theta_C, lam1_C, lam2_C)\n    results.append(f\"{obj_C:.6f}\")\n\n    # Test case D: lasso-only boundary\n    lam1_D, lam2_D = 0.5, 0.0\n    K_D = 400\n    theta_D = prox_gradient(X_shared, y_shared, lam1_D, lam2_D, K_D, theta0=np.zeros(2))\n    obj_D = logistic_objective(X_shared, y_shared, theta_D, lam1_D, lam2_D)\n    results.append(f\"{obj_D:.6f}\")\n\n    # Test case E: zero design matrix and nonzero initialization\n    X_E = np.zeros((3, 2), dtype=float)\n    y_E = np.array([0.0, 1.0, 0.0], dtype=float)\n    lam1_E, lam2_E = 0.2, 0.1\n    K_E = 5\n    theta0_E = np.array([1.0, -1.0], dtype=float)\n    # For zero matrix, step size defaults to t=1 inside prox_gradient\n    theta_E = prox_gradient(X_E, y_E, lam1_E, lam2_E, K_E, theta0=theta0_E)\n    results.append(str(round_list(theta_E.tolist(), 6)))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "虽然机器学习中的许多性能指标（例如物体检测中的交并比 $IoU$）非常直观，但它们通常是非凸的，这使得优化过程困难且不可靠。本练习将深入探讨这一挑战，首先通过一个具体的例子展示 $IoU$ 损失的非凸性。随后，本练习将阐明为何要使用诸如逻辑损失等凸代理函数，并展示凸性如何为我们提供通向可靠优化的途径，保证算法能够收敛到全局最优解。",
            "id": "3146363",
            "problem": "给定一个实数轴上的一维集合预测场景。真实集合（ground-truth set）是闭区间 $B = [0,1]$。一个模型预测的集合也是一个固定宽度 $w=1$ 的闭区间，由其左端点 $\\theta \\in \\mathbb{R}$ 参数化为 $A_\\theta = [\\theta, \\theta + 1]$。交并比（Intersection over Union, IoU）损失定义为\n$$\n\\mathcal{L}_{\\mathrm{IoU}}(\\theta) \\;=\\; 1 \\;-\\; \\frac{|A_\\theta \\cap B|}{|A_\\theta \\cup B|},\n$$\n其中 $|\\cdot|$ 表示一维空间中的勒贝格测度（长度）。对于 $w=1$ 和 $B=[0,1]$，这是一个关于 $\\theta$ 的精确的分段定义函数。\n\n您将分析 $\\mathcal{L}_{\\mathrm{IoU}}(\\theta)$ 的非凸性，然后为一个固定网格上的逐点分类问题构建凸代理损失，并最终展示凸性带来的优化优势。\n\n您可以依赖的基础知识：\n- 凸函数的定义：一个函数 $f$ 是凸的，当且仅当对于所有 $x, y$ 以及所有 $t \\in [0,1]$，都有 $f(tx+(1-t)y) \\le t f(x) + (1-t) f(y)$。\n- 二元分类中合页损失（hinge loss）和逻辑损失（logistic loss）的标准定义，以及它们在线性模型参数下的凸性。\n- Newton 法的基本性质，以及线性模型中逻辑损失的海森矩阵（Hessian）的正半定性。\n\n需要实现和计算的任务：\n1) 对于 $w=1$ 和 $B=[0,1]$，通过使用以下公式显式计算交集长度 $|A_\\theta \\cap B|$ 和并集长度 $|A_\\theta \\cup B|$ 来定义 IoU 损失 $\\mathcal{L}_{\\mathrm{IoU}}(\\theta)$：\n$$\n|A_\\theta \\cap B| \\;=\\; \\max\\bigl\\{0, \\min(\\theta+1,1) - \\max(\\theta,0)\\bigr\\}, \\qquad |A_\\theta \\cup B| \\;=\\; 1 + 1 - |A_\\theta \\cap B|.\n$$\n使用此定义来评估特定测试用例 $(\\theta_1,\\theta_2,t) = (0,1,0.5)$ 的凸性不等式。此测试所需的布尔值输出是判断以下不等式是否成立：\n$$\n\\mathcal{L}_{\\mathrm{IoU}}(t \\theta_1 + (1-t)\\theta_2) \\;\\le\\; t \\,\\mathcal{L}_{\\mathrm{IoU}}(\\theta_1) + (1-t)\\,\\mathcal{L}_{\\mathrm{IoU}}(\\theta_2)\n$$\n该测试旨在通过展示一个具体的违例来揭示其非凸性。\n\n2) 通过将区间 $[-1.5,1.5]$ 离散化为 $N=121$ 个等距点 $\\{x_i\\}_{i=1}^{N}$ 并分配标签来构建一个逐点的二元分类数据集：\n$$\ny_i \\;=\\; \\begin{cases}\n+1,  \\text{if } x_i \\in [0,1],\\\\\n-1,  \\text{otherwise.}\n\\end{cases}\n$$\n考虑一个带有分数函数 $s_{w,b}(x) = w x + b$ 的线性分类器，并定义经验合页损失：\n$$\n\\mathcal{L}_{\\mathrm{hinge}}(w,b) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} \\max\\bigl\\{0,\\, 1 - y_i\\, (w x_i + b)\\bigr\\}.\n$$\n对于参数 $(w_1,b_1)=(4.0,-1.0)$、$(w_2,b_2)=(-2.0,0.5)$ 和 $t=0.5$，评估以下不等式是否成立：\n$$\n\\mathcal{L}_{\\mathrm{hinge}}(t w_1+(1-t)w_2,\\; t b_1+(1-t)b_2) \\;\\le\\; t \\,\\mathcal{L}_{\\mathrm{hinge}}(w_1,b_1) + (1-t)\\,\\mathcal{L}_{\\mathrm{hinge}}(w_2,b_2)\n$$\n输出结果布尔值。\n\n3) 使用相同的数据集和线性分类器，定义经验逻辑损失：\n$$\n\\mathcal{L}_{\\mathrm{log}}(w,b) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} \\log\\!\\bigl(1 + \\exp\\bigl(-y_i \\,(w x_i + b)\\bigr)\\bigr).\n$$\n对于相同的参数对和 $t=0.5$，评估以下不等式是否成立：\n$$\n\\mathcal{L}_{\\mathrm{log}}(t w_1+(1-t)w_2,\\; t b_1+(1-t)b_2) \\;\\le\\; t \\,\\mathcal{L}_{\\mathrm{log}}(w_1,b_1) + (1-t)\\,\\mathcal{L}_{\\mathrm{log}}(w_2,b_2)\n$$\n输出结果布尔值。\n\n4) 展示凸代理的优化优势：使用带有回溯线搜索的 Newton 法最小化 $\\mathcal{L}_{\\mathrm{log}}(w,b)$（其中 $(w,b) \\in \\mathbb{R}^2$），直到步长的欧几里得范数小于 $10^{-10}$ 或达到 $100$ 次迭代。从两个不同的初始点 $(w,b)=(0.0,0.0)$ 和 $(w,b)=(10.0,-10.0)$ 开始运行，并返回一个布尔值，指示两次运行是否收敛到本质上相同的解，即同时满足：\n$$\n\\|\\,(w^{\\star}_A, b^{\\star}_A) - (w^{\\star}_B, b^{\\star}_B)\\,\\|_2  10^{-6}\n\\quad\\text{and}\\quad\n\\bigl|\\,\\mathcal{L}_{\\mathrm{log}}(w^{\\star}_A,b^{\\star}_A) - \\mathcal{L}_{\\mathrm{log}}(w^{\\star}_B,b^{\\star}_B)\\,\\bigr|  10^{-8}.\n$$\n\n5) 设 $(w^{\\star},b^{\\star})$ 为任务 4 中得到的最小化器。计算 $\\mathcal{L}_{\\mathrm{log}}(w,b)$ 在 $(w^{\\star},b^{\\star})$ 处的海森矩阵，并以浮点数形式返回其最小特征值。对于线性模型中的逻辑损失，海森矩阵具有以下形式：\n$$\nH \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} \\sigma\\!\\bigl(y_i\\, s_{w,b}(x_i)\\bigr)\\,\\sigma\\!\\bigl(-y_i\\, s_{w,b}(x_i)\\bigr)\\,\n\\begin{bmatrix}\nx_i^2  x_i\\\\\nx_i  1\n\\end{bmatrix},\n$$\n其中 $\\sigma(z)=\\frac{1}{1+\\exp(-z)}$ 表示逻辑 sigmoid 函数。一个非负的最小特征值在数值上证实了其正半定性。\n\n测试套件和要求的输出：\n- 测试 1：使用 $(\\theta_1,\\theta_2,t)=(0,1,0.5)$ 对 $\\mathcal{L}_{\\mathrm{IoU}}$ 进行凸性检查；输出一个布尔值。\n- 测试 2：使用 $(w_1,b_1)=(4.0,-1.0)$、$(w_2,b_2)=(-2.0,0.5)$、$t=0.5$ 对 $\\mathcal{L}_{\\mathrm{hinge}}$ 进行凸性检查；输出一个布尔值。\n- 测试 3：使用与测试 2 相同的参数对 $\\mathcal{L}_{\\mathrm{log}}$ 进行凸性检查；输出一个布尔值。\n- 测试 4：从 $(0.0,0.0)$ 和 $(10.0,-10.0)$ 开始进行 Newton 优化；输出一个布尔值，指示是否在指定容差内收敛到相同的解。\n- 测试 5：在测试 4 得到的优化器处，$\\mathcal{L}_{\\mathrm{log}}$ 的海森矩阵的最小特征值；输出一个浮点数。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，`[result1,result2,result3,result4,result5]`），其中 result1 到 result4 是布尔值，result5 是一个浮点数。不应打印其他任何文本。",
            "solution": "问题陈述经过了仔细验证，并被确定为有效。它在优化和机器学习领域有科学依据，问题定义良好、客观且内部一致。所有必要的定义、公式和参数都已提供，可以得出一个完整且明确的计算解。\n\n该问题要求进行多部分分析，比较非凸的交并比（$\\mathcal{L}_{\\mathrm{IoU}}$）损失与用于分类任务的凸代理损失（合页损失和逻辑损失），最终使用 Newton 法展示凸性的优化优势。\n\n以下是每个任务的逐步推导和推理。\n\n### 任务 1：IoU 损失的凸性分析\n\n交并比损失定义为 $\\mathcal{L}_{\\mathrm{IoU}}(\\theta) = 1 - \\frac{|A_\\theta \\cap B|}{|A_\\theta \\cup B|}$，其中真实集合是 $B = [0,1]$，预测集合是 $A_\\theta = [\\theta, \\theta+1]$。交集的长度（勒贝格测度）由 $|A_\\theta \\cap B| = \\max\\{0, \\min(\\theta+1,1) - \\max(\\theta,0)\\}$ 给出，并集的长度为 $|A_\\theta \\cup B| = |A_\\theta| + |B| - |A_\\theta \\cap B| = 1 + 1 - |A_\\theta \\cap B| = 2 - |A_\\theta \\cap B|$。\n\n我们针对特定情况 $(\\theta_1, \\theta_2, t) = (0, 1, 0.5)$ 测试凸性不等式 $f(t x + (1-t) y) \\le t f(x) + (1-t) f(y)$。\n\n用于凸性检查的区间端点是 $\\theta_1 = 0$ 和 $\\theta_2 = 1$。中点是 $\\theta_m = t \\theta_1 + (1-t) \\theta_2 = 0.5 \\times 0 + 0.5 \\times 1 = 0.5$。\n\n我们计算这三点处的损失：\n1.  对于 $\\theta_1 = 0$，预测集合是 $A_0 = [0,1]$。\n    - 交集：$|A_0 \\cap B| = |[0,1] \\cap [0,1]| = 1$。\n    - 并集：$|A_0 \\cup B| = |[0,1] \\cup [0,1]| = 1$。\n    - 损失：$\\mathcal{L}_{\\mathrm{IoU}}(0) = 1 - 1/1 = 0$。\n\n2.  对于 $\\theta_2 = 1$，预测集合是 $A_1 = [1,2]$。\n    - 交集：$|A_1 \\cap B| = |[1,2] \\cap [0,1]| = |\\{1\\}| = 0$。\n    - 并集：$|A_1 \\cup B| = |[0,1] \\cup [1,2]| = |[0,2]| = 2$。\n    - 损失：$\\mathcal{L}_{\\mathrm{IoU}}(1) = 1 - 0/2 = 1$。\n\n3.  对于中点 $\\theta_m = 0.5$，预测集合是 $A_{0.5} = [0.5, 1.5]$。\n    - 交集：$|A_{0.5} \\cap B| = |[0.5, 1.5] \\cap [0,1]| = |[0.5, 1]| = 0.5$。\n    - 并集：$|A_{0.5} \\cup B| = |[0, 1.5]| = 1.5$。\n    - 损失：$\\mathcal{L}_{\\mathrm{IoU}}(0.5) = 1 - 0.5 / 1.5 = 1 - 1/3 = 2/3$。\n\n现在，我们检查凸性不等式：\n$\\mathcal{L}_{\\mathrm{IoU}}(\\theta_m) \\le t \\mathcal{L}_{\\mathrm{IoU}}(\\theta_1) + (1-t) \\mathcal{L}_{\\mathrm{IoU}}(\\theta_2)$\n$2/3 \\le 0.5 \\times 0 + 0.5 \\times 1$\n$2/3 \\le 0.5$\n$0.666... \\le 0.5$\n\n这个不等式是错误的。这表明 $\\mathcal{L}_{\\mathrm{IoU}}(\\theta)$ 不是一个凸函数，因为我们找到了一个具体的反例。要求的输出是 `False`。\n\n### 任务 2 和 3：合页损失和逻辑损失的凸性\n\n对于这些任务，通过在 $[-1.5, 1.5]$ 上采样 $N=121$ 个点 $\\{x_i\\}$ 并为 $x_i \\in [0,1]$ 的点分配标签 $y_i = +1$、否则分配 $y_i = -1$ 来构建一个逐点二元分类数据集。我们考虑一个带有分数函数 $s_{w,b}(x) = w x + b$ 的线性分类器。\n\n经验合页损失 $\\mathcal{L}_{\\mathrm{hinge}}(w,b)$ 和经验逻辑损失 $\\mathcal{L}_{\\mathrm{log}}(w,b)$ 定义为在数据集上的总和：\n$$ \\mathcal{L}_{\\mathrm{hinge}}(w,b) = \\frac{1}{N}\\sum_{i=1}^{N} \\max\\bigl\\{0, 1 - y_i (w x_i + b)\\bigr\\} $$\n$$ \\mathcal{L}_{\\mathrm{log}}(w,b) = \\frac{1}{N}\\sum_{i=1}^{N} \\log\\bigl(1 + \\exp\\bigl(-y_i (w x_i + b)\\bigr)\\bigr) $$\n\n合页损失函数 $f(z) = \\max(0, 1-z)$ 和对数损失函数 $f(z) = \\log(1+\\exp(-z))$ 都是关于其参数 $z$ 的凸函数。当参数 $z$ 是参数 $(w,b)$ 的线性函数时，如 $z_i = y_i (w x_i + b)$，所得到的损失函数 $\\mathcal{L}_{\\mathrm{hinge}}(w,b)$ 和 $\\mathcal{L}_{\\mathrm{log}}(w,b)$ 都是关于 $(w,b)$ 的凸函数。这是因为与仿射映射的复合会保持凸性，并且凸函数的和也是凸的。\n\n因此，根据凸性的定义，不等式 $L(t \\theta_1 + (1-t)\\theta_2) \\le t L(\\theta_1) + (1-t)L(\\theta_2)$ 对任意参数选择 $\\theta_1=(w_1, b_1)$、$\\theta_2=(w_2, b_2)$ 和任意 $t \\in [0,1]$ 都必须成立。问题要求对特定参数 $(w_1, b_1) = (4.0, -1.0)$、$(w_2, b_2) = (-2.0, 0.5)$ 和 $t=0.5$ 验证这一点。由于已知这两种损失都是凸的，因此在两种情况下不等式都会成立。任务 2 和任务 3 要求的输出都是 `True`。\n\n### 任务 4：通过 Newton 法进行优化\n\n这个任务展示了凸优化的一个关键优势：无论从哪个起始点开始，都能收敛到唯一的全局最小值。我们被要求使用 Newton 法从两个不同的初始点 $(w,b)=(0.0,0.0)$ 和 $(w,b)=(10.0,-10.0)$ 来最小化严格凸的逻辑损失 $\\mathcal{L}_{\\mathrm{log}}(w,b)$。\n\nNewton 法通过使用二阶泰勒近似来迭代地寻找函数的最小值。参数 $\\mathbf{p} = [w,b]^T$ 的更新步骤是：\n$$ \\mathbf{p}_{k+1} = \\mathbf{p}_k - \\lambda_k H_k^{-1} g_k $$\n其中 $g_k = \\nabla \\mathcal{L}_{\\mathrm{log}}(\\mathbf{p}_k)$ 是梯度，$H_k = \\nabla^2 \\mathcal{L}_{\\mathrm{log}}(\\mathbf{p}_k)$ 是海森矩阵，而 $\\lambda_k$ 是由回溯线搜索确定的步长，以确保损失充分下降。\n\n因为对于这个数据集，$\\mathcal{L}_{\\mathrm{log}}(w,b)$ 是严格凸的（特征 $[x_i, 1]^T$ 不是共线的），所以它拥有唯一的全局最小值 $(w^\\star, b^\\star)$。Newton 法在最小值附近具有二次收敛速度，是一种强大的算法，保证可以从任何合理的起始点收敛到这个唯一的最优解。因此，我们预期从 $(0.0,0.0)$ 和 $(10.0,-10.0)$ 开始的优化运行将产生在数值精度内相同的解 $(w_A^\\star, b_A^\\star)$ 和 $(w_B^\\star, b_B^\\star)$。根据给定的参数和损失值容差，检验两次运行是否收敛到相同解的测试应该会通过。要求的输出是 `True`。\n\n### 任务 5：最优解处的海森矩阵分析\n\n最后一个任务是计算在找到的最小化器 $(w^\\star,b^\\star)$ 处的 $\\mathcal{L}_{\\mathrm{log}}(w,b)$ 的海森矩阵，并确定其最小特征值。海森矩阵的公式是：\n$$ H = \\frac{1}{N}\\sum_{i=1}^{N} \\sigma(z_i)\\sigma(-z_i) \\begin{bmatrix} x_i^2  x_i \\\\ x_i  1 \\end{bmatrix}, \\quad \\text{where } z_i = y_i(w x_i + b) $$\n并且 $\\sigma(z) = (1+\\exp(-z))^{-1}$ 是 sigmoid 函数。项 $\\sigma(z_i)\\sigma(-z_i)$ 是 sigmoid 函数的导数，并且总是正的。矩阵 $\\begin{psmallmatrix} x_i^2  x_i \\\\ x_i  1 \\end{psmallmatrix}$ 是外积 $\\mathbf{v}_i \\mathbf{v}_i^T$，其中 $\\mathbf{v}_i = [x_i, 1]^T$，这是一个正半定矩阵。\n\n由于数据点 $x_i$ 不完全相同，向量 $\\mathbf{v}_i$ 张成了整个二维参数空间。海森矩阵是正半定矩阵的和，并且因为向量 $\\mathbf{v}_i$ 张成了整个空间且系数 $\\sigma(z_i)\\sigma(-z_i)$ 是正的，所以最终的海森矩阵 $H$ 是正定的。\n\n正定矩阵的一个基本性质是其所有特征值都严格为正。因此，计算最小化器处的海森矩阵的最小特征值，可以作为损失函数在该点严格凸性的数值验证。我们预期结果是一个正的浮点数。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Executes all tasks described in the problem statement and prints the results.\n    \"\"\"\n\n    results = []\n\n    # --- Task 1: Convexity Analysis of IoU Loss ---\n    def iou_loss(theta):\n        # Ground-truth set B = [0,1], Predicted set A = [theta, theta+1]\n        intersection = max(0.0, min(theta + 1.0, 1.0) - max(theta, 0.0))\n        # Union = |A| + |B| - Intersection = 1 + 1 - intersection\n        union = 2.0 - intersection\n        if union == 0.0:\n            return 0.0 if intersection == 1.0 else 1.0\n        return 1.0 - intersection / union\n\n    theta1, theta2, t = 0.0, 1.0, 0.5\n    theta_m = t * theta1 + (1.0 - t) * theta2\n    \n    loss_m = iou_loss(theta_m)\n    loss_1 = iou_loss(theta1)\n    loss_2 = iou_loss(theta2)\n    \n    is_convex_iou = loss_m = t * loss_1 + (1.0 - t) * loss_2\n    results.append(is_convex_iou)\n\n    # --- Setup for Tasks 2-5 ---\n    N = 121\n    x_data = np.linspace(-1.5, 1.5, N)\n    y_data = np.ones(N)\n    y_data[(x_data  0) | (x_data > 1)] = -1.0\n\n    # --- Task 2: Convexity of Hinge Loss ---\n    def hinge_loss(w, b, x, y):\n        margins = y * (w * x + b)\n        losses = np.maximum(0.0, 1.0 - margins)\n        return np.mean(losses)\n\n    w1, b1 = 4.0, -1.0\n    w2, b2 = -2.0, 0.5\n    wm = t * w1 + (1.0 - t) * w2\n    bm = t * b1 + (1.0 - t) * b2\n\n    hinge_m = hinge_loss(wm, bm, x_data, y_data)\n    hinge_1 = hinge_loss(w1, b1, x_data, y_data)\n    hinge_2 = hinge_loss(w2, b2, x_data, y_data)\n    \n    is_convex_hinge = hinge_m = t * hinge_1 + (1.0 - t) * hinge_2\n    results.append(is_convex_hinge)\n\n    # --- Task 3: Convexity of Logistic Loss ---\n    def logistic_loss(w, b, x, y):\n        # Use numerically stable log(1+exp(z)) = logaddexp(0, z)\n        z = -y * (w * x + b)\n        losses = np.logaddexp(0, z)\n        return np.mean(losses)\n\n    log_m = logistic_loss(wm, bm, x_data, y_data)\n    log_1 = logistic_loss(w1, b1, x_data, y_data)\n    log_2 = logistic_loss(w2, b2, x_data, y_data)\n    \n    is_convex_log = log_m = t * log_1 + (1.0 - t) * log_2\n    results.append(is_convex_log)\n\n    # --- Functions for Task 4  5 ---\n    def log_loss_grad_hess(w, b, x, y):\n        n_points = len(x)\n        scores = w * x + b\n        z = y * scores\n        \n        # Gradient\n        sigma_neg_z = expit(-z)\n        grad_w = np.sum(sigma_neg_z * (-y * x)) / n_points\n        grad_b = np.sum(sigma_neg_z * (-y)) / n_points\n        grad = np.array([grad_w, grad_b])\n        \n        # Hessian\n        sigma_z = expit(z)\n        coeffs = sigma_z * sigma_neg_z\n        H_ww = np.sum(coeffs * x**2) / n_points\n        H_wb = np.sum(coeffs * x) / n_points\n        H_bb = np.sum(coeffs) / n_points\n        H = np.array([[H_ww, H_wb], [H_wb, H_bb]])\n        \n        return grad, H\n        \n    def newton_minimize(initial_params, x, y):\n        params = np.array(initial_params, dtype=float)\n        max_iter = 100\n        step_norm_tol = 1e-10\n        alpha, beta = 0.25, 0.5\n      \n        for _ in range(max_iter):\n            w, b = params\n            grad, H = log_loss_grad_hess(w, b, x, y)\n            \n            try:\n                # Solve H * step = -grad for the Newton step\n                step = np.linalg.solve(H, -grad)\n            except np.linalg.LinAlgError:\n                # Hessian is singular, should not happen for this problem\n                break\n\n            # Backtracking line search\n            line_search_t = 1.0\n            current_loss = logistic_loss(w, b, x, y)\n            armijo_term = alpha * np.dot(grad, step)\n            \n            while True:\n                next_w, next_b = params + line_search_t * step\n                if logistic_loss(next_w, next_b, x, y) = current_loss + line_search_t * armijo_term:\n                    break\n                line_search_t *= beta\n                if line_search_t  1e-15:  # Failsafe\n                    break\n            \n            final_step = line_search_t * step\n            params += final_step\n            \n            if np.linalg.norm(final_step)  step_norm_tol:\n                break\n                \n        return params\n\n    # --- Task 4: Newton's Method Convergence ---\n    init_A = (0.0, 0.0)\n    init_B = (10.0, -10.0)\n    \n    sol_A = newton_minimize(init_A, x_data, y_data)\n    sol_B = newton_minimize(init_B, x_data, y_data)\n    \n    wA, bA = sol_A\n    wB, bB = sol_B\n    loss_A = logistic_loss(wA, bA, x_data, y_data)\n    loss_B = logistic_loss(wB, bB, x_data, y_data)\n    \n    param_dist = np.linalg.norm(sol_A - sol_B)\n    loss_diff = abs(loss_A - loss_B)\n    \n    converged_to_same = (param_dist  1e-6) and (loss_diff  1e-8)\n    results.append(converged_to_same)\n\n    # --- Task 5: Hessian Eigenvalue at Minimum ---\n    w_star, b_star = sol_A  # Both solutions should be the same\n    _, H_star = log_loss_grad_hess(w_star, b_star, x_data, y_data)\n    eigenvalues = np.linalg.eigvalsh(H_star)\n    min_eigenvalue = np.min(eigenvalues)\n    results.append(min_eigenvalue)\n    \n    # --- Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}