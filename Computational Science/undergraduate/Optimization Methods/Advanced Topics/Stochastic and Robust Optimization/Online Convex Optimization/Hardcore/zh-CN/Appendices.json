{
    "hands_on_practices": [
        {
            "introduction": "跟随正则化领导者 (Follow-The-Regularized-Leader, FTRL) 是在线凸优化中一个强大而通用的框架。本练习旨在通过一个具体且重要的例子——在概率单纯形上使用熵正则化——来帮助你深入理解这一抽象概念。这个场景之所以著名，是因为它与经典的乘法权重更新算法（Multiplicative Weights Update）紧密相关，通过亲手推导，你将掌握 FTRL 的核心原理并熟练运用相关的优化技巧。",
            "id": "3159427",
            "problem": "考虑一个在$3$维概率单纯形$\\Delta_{3}=\\{x\\in\\mathbb{R}^{3}: x_{i}\\geq 0,\\ \\sum_{i=1}^{3}x_{i}=1\\}$上的在线凸优化（Online Convex Optimization, OCO）问题。在每一轮$t\\in\\{1,2,3,4,5\\}$，学习者选择一个决策$x_{t}\\in\\Delta_{3}$，然后对手揭示一个线性损失函数$f_{t}(x)=\\ell_{t}\\cdot x$，其中$\\ell_{t}\\in\\mathbb{R}^{3}$。学习者使用带有熵正则化的“跟随正则化领导者”（Follow-The-Regularized-Leader, FTRL）算法，具体来说，正则化项为负香农熵$R(x)=\\sum_{i=1}^{3} x_{i}\\ln x_{i}$，学习率参数为$\\eta0$。在第$t+1$轮的FTRL决策是以下凸规划问题的解：\n$$\nx_{t+1}\\in\\arg\\min_{x\\in\\Delta_{3}}\\left\\{\\eta\\sum_{s=1}^{t}(\\ell_{s}\\cdot x)+R(x)\\right\\}.\n$$\n假设对手选择一系列与域的顶点对齐的损失，方式如下：对于一个固定的量值$M0$，\n$$\n\\ell_{1}=(M,0,0),\\quad \\ell_{2}=(M,0,0),\\quad \\ell_{3}=(0,M,0),\\quad \\ell_{4}=(0,M,0),\\quad \\ell_{5}=(0,0,M).\n$$\n从没有先前损失开始，使用上述OCO和FTRL的定义以及凸优化的原理，推导经过这五轮后，由熵FTRL更新产生的学习者决策$x_{6}$的显式解析表达式。你的最终答案必须是$x_{6}$作为$\\Delta_{3}$中一个点的单一闭式表达式，用$\\eta$和$M$表示。不需要四舍五入。",
            "solution": "用户想要找到一个在线凸优化问题在5轮之后，学习者的决策向量$x_{6}$。学习者使用带有负香农熵正则化项的“跟随正则化领导者”（FTRL）算法。\n\n决策$x_{t+1}$通过求解以下优化问题来确定：\n$$\nx_{t+1} = \\arg\\min_{x\\in\\Delta_{3}}\\left\\{\\eta\\sum_{s=1}^{t}(\\ell_{s}\\cdot x)+R(x)\\right\\}\n$$\n其中$\\Delta_{3}=\\{x\\in\\mathbb{R}^{3}: x_{i}\\geq 0,\\ \\sum_{i=1}^{3}x_{i}=1\\}$是$3$维概率单纯形，$R(x)=\\sum_{i=1}^{3} x_{i}\\ln x_{i}$是正则化项，$\\eta > 0$是学习率。\n\n我们需要计算$x_{6}$，这意味着我们需要对从$t=1$到$t=5$的损失进行求和。设$L_{5}$为累积损失向量，$L_{5} = \\sum_{s=1}^{5} \\ell_{s}$。给定的损失向量是：\n$$\n\\ell_{1}=(M,0,0) \\\\\n\\ell_{2}=(M,0,0) \\\\\n\\ell_{3}=(0,M,0) \\\\\n\\ell_{4}=(0,M,0) \\\\\n\\ell_{5}=(0,0,M)\n$$\n按分量对这些向量求和，我们得到：\n$$\nL_{5} = (M+M, M+M, M) = (2M, 2M, M)\n$$\n关于$x_{6}$的优化问题是：\n$$\nx_{6} = \\arg\\min_{x\\in\\Delta_{3}}\\left\\{\\eta (L_{5} \\cdot x) + R(x)\\right\\}\n$$\n我们写出目标函数，记为$F(x)$：\n$$\nF(x) = \\eta (2Mx_{1} + 2Mx_{2} + Mx_{3}) + x_{1}\\ln x_{1} + x_{2}\\ln x_{2} + x_{3}\\ln x_{3}\n$$\n这是一个带有约束$x_{1}+x_{2}+x_{3}=1$和$x_{i} \\geq 0$（对于$i \\in \\{1, 2, 3\\}$）的凸优化问题。我们使用拉格朗日乘数法来找到解。拉格朗日函数$\\mathcal{L}$是：\n$$\n\\mathcal{L}(x, \\nu, \\lambda_{1}, \\lambda_{2}, \\lambda_{3}) = F(x) + \\nu\\left(\\sum_{i=1}^{3}x_{i} - 1\\right) - \\sum_{i=1}^{3}\\lambda_{i}x_{i}\n$$\n其中$\\nu$是等式约束的拉格朗日乘数，$\\lambda_i$是非负约束$x_i \\geq 0$的乘数。\n\n在最小值点必须满足卡罗需-库恩-塔克（KKT）条件。平稳性条件要求将拉格朗日函数关于$x$的梯度设为零。对于每个分量$x_{i}$：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{i}} = \\eta (L_{5})_{i} + (\\ln x_{i} + 1) + \\nu - \\lambda_{i} = 0\n$$\n其中$(L_{5})_{i}$是向量$L_{5}$的第$i$个分量。\n\n正则化项$R(x)$是严格凸的，并且当任何$x_i \\to 0^+$时，其导数$\\ln x_i + 1 \\to -\\infty$。这起到了一个屏障的作用，确保对于有限的损失值，最优解将位于单纯形的内部，即对于所有$i$都有$x_{i} > 0$。\n根据KKT互补松弛条件，$\\lambda_{i}x_{i} = 0$。由于$x_{i} > 0$，我们必须有$\\lambda_{i} = 0$（对于$i=1, 2, 3$）。\n\n因此，平稳性条件简化为：\n$$\n\\eta (L_{5})_{i} + \\ln x_{i} + 1 + \\nu = 0\n$$\n求解$x_{i}$：\n$$\n\\ln x_{i} = -\\eta (L_{5})_{i} - 1 - \\nu\n$$\n$$\nx_{i} = \\exp(-\\eta (L_{5})_{i} - 1 - \\nu) = \\exp(-\\eta (L_{5})_{i}) \\exp(-1-\\nu)\n$$\n项$\\exp(-1-\\nu)$对所有$i$都是一个常数。我们可以使用单纯形约束$\\sum_{i=1}^{3}x_{i} = 1$来找到它的值：\n$$\n\\sum_{i=1}^{3} \\exp(-\\eta (L_{5})_{i}) \\exp(-1-\\nu) = 1\n$$\n$$\n\\exp(-1-\\nu) \\left( \\sum_{j=1}^{3} \\exp(-\\eta (L_{5})_{j}) \\right) = 1\n$$\n$$\n\\exp(-1-\\nu) = \\frac{1}{\\sum_{j=1}^{3} \\exp(-\\eta (L_{5})_{j})}\n$$\n将此代回到$x_{i}$的表达式中，我们得到通解，这对应于应用于负累积损失的softmax函数：\n$$\nx_{i} = \\frac{\\exp(-\\eta (L_{5})_{i})}{\\sum_{j=1}^{3} \\exp(-\\eta (L_{5})_{j})}\n$$\n现在，我们代入累积损失向量$L_{5} = (2M, 2M, M)$的具体分量来找到$x_{6} = (x_{6,1}, x_{6,2}, x_{6,3})$的分量。分母是：\n$$\nD = \\sum_{j=1}^{3} \\exp(-\\eta (L_{5})_{j}) = \\exp(-\\eta (2M)) + \\exp(-\\eta (2M)) + \\exp(-\\eta M) = 2\\exp(-2\\eta M) + \\exp(-\\eta M)\n$$\n$x_{6}$的分量是：\n$$\nx_{6,1} = \\frac{\\exp(-\\eta (L_{5})_{1})}{D} = \\frac{\\exp(-2\\eta M)}{2\\exp(-2\\eta M) + \\exp(-\\eta M)}\n$$\n$$\nx_{6,2} = \\frac{\\exp(-\\eta (L_{5})_{2})}{D} = \\frac{\\exp(-2\\eta M)}{2\\exp(-2\\eta M) + \\exp(-\\eta M)}\n$$\n$$\nx_{6,3} = \\frac{\\exp(-\\eta (L_{5})_{3})}{D} = \\frac{\\exp(-\\eta M)}{2\\exp(-2\\eta M) + \\exp(-\\eta M)}\n$$\n为了简化这些表达式，我们可以将每个分数的分子和分母都乘以$\\exp(2\\eta M)$：\n$$\nx_{6,1} = \\frac{\\exp(-2\\eta M) \\cdot \\exp(2\\eta M)}{(2\\exp(-2\\eta M) + \\exp(-\\eta M)) \\cdot \\exp(2\\eta M)} = \\frac{1}{2 + \\exp(\\eta M)}\n$$\n$$\nx_{6,2} = \\frac{\\exp(-2\\eta M) \\cdot \\exp(2\\eta M)}{(2\\exp(-2\\eta M) + \\exp(-\\eta M)) \\cdot \\exp(2\\eta M)} = \\frac{1}{2 + \\exp(\\eta M)}\n$$\n$$\nx_{6,3} = \\frac{\\exp(-\\eta M) \\cdot \\exp(2\\eta M)}{(2\\exp(-2\\eta M) + \\exp(-\\eta M)) \\cdot \\exp(2\\eta M)} = \\frac{\\exp(\\eta M)}{2 + \\exp(\\eta M)}\n$$\n因此，学习者的决策向量$x_{6}$是：\n$$\nx_{6} = \\left( \\frac{1}{2 + \\exp(\\eta M)}, \\frac{1}{2 + \\exp(\\eta M)}, \\frac{\\exp(\\eta M)}{2 + \\exp(\\eta M)} \\right)\n$$",
            "answer": "$$\n\\boxed{x_{6} = \\left( \\frac{1}{2 + \\exp(\\eta M)}, \\frac{1}{2 + \\exp(\\eta M)}, \\frac{\\exp(\\eta M)}{2 + \\exp(\\eta M)} \\right)}\n$$"
        },
        {
            "introduction": "在掌握了核心算法之后，我们来探讨一项在现代机器学习中无处不在的实用技术：梯度裁剪。虽然它常被用作防止梯度爆炸的“黑盒”工具，但其理论后果值得深入研究。本练习将引导你从第一性原理出发，推导一个包含梯度裁剪的在线梯度下降算法的遗憾上界，从而定量地揭示裁剪操作在引入偏差（bias）与保证梯度有界性之间所做的权衡。",
            "id": "3159395",
            "problem": "考虑在闭凸集 $\\mathcal{K} \\subset \\mathbb{R}^{d}$ 上的在线凸优化问题，使用在线梯度下降 (OGD) 算法。该集合的欧几里得直径为 $D$，即对于所有 $\\mathbf{x}, \\mathbf{y} \\in \\mathcal{K}$，有 $\\|\\mathbf{x} - \\mathbf{y}\\| \\le D$。在每一轮 $t \\in \\{1,2,\\dots,T\\}$，在选择 $\\mathbf{x}_{t} \\in \\mathcal{K}$ 之后，一个凸损失函数 $f_{t} : \\mathcal{K} \\to \\mathbb{R}$ 会被揭示。我们定义相对于固定比较器 $\\mathbf{x}^{\\star} \\in \\mathcal{K}$ 的遗憾为\n$$\nR_{T} \\triangleq \\sum_{t=1}^{T} \\big( f_{t}(\\mathbf{x}_{t}) - f_{t}(\\mathbf{x}^{\\star}) \\big).\n$$\n假设可以获取梯度 $\\mathbf{g}_{t} \\triangleq \\nabla f_{t}(\\mathbf{x}_{t})$，但为了强制执行遗憾证明中使用的有界梯度假设，您在阈值 $C > 0$ 处应用梯度裁剪，定义裁剪后的梯度为\n$$\n\\mathbf{u}_{t} \\triangleq \\begin{cases}\n\\mathbf{g}_{t},  \\text{若 } \\|\\mathbf{g}_{t}\\| \\le C, \\\\\nC \\, \\dfrac{\\mathbf{g}_{t}}{\\|\\mathbf{g}_{t}\\|},  \\text{若 } \\|\\mathbf{g}_{t}\\|  C.\n\\end{cases}\n$$\nOGD 更新使用裁剪后的梯度：\n$$\n\\mathbf{x}_{t+1} \\triangleq \\Pi_{\\mathcal{K}}\\big( \\mathbf{x}_{t} - \\eta \\mathbf{u}_{t} \\big),\n$$\n其中 $\\Pi_{\\mathcal{K}}$ 表示到集合 $\\mathcal{K}$ 上的欧几里得投影，$\\eta  0$ 是步长。\n\n从基本定义和性质（凸性：$f_{t}(\\mathbf{x}_{t}) - f_{t}(\\mathbf{x}^{\\star}) \\le \\langle \\nabla f_{t}(\\mathbf{x}_{t}), \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle$、欧几里得投影的非扩张性以及柯西-施瓦茨不等式）出发，推导 $R_{T}$ 的一个上界，该上界明确地体现了由裁剪强制执行的有界梯度项与裁剪引入的偏差之间的权衡。为了量化偏差，假设梯度的以下数据模型：恰好有 $k$ 轮的梯度范数 $\\|\\mathbf{g}_{t}\\| = G$（其中 $G  C$），而在所有其他轮中 $\\|\\mathbf{g}_{t}\\| \\le C$。对于这 $k$ 轮，裁剪导致的差异为 $\\|\\mathbf{g}_{t} - \\mathbf{u}_{t}\\| = G - C$，你可以使用 $\\mathcal{K}$ 的直径将每轮裁剪偏差的内积贡献上界限定为 $D (G - C)$。\n\n选择 $\\eta$ 以最小化你推导出的关于 $\\eta$ 的上界，并将 $R_{T}$ 的最小化上界报告为关于 $D$、$C$、$T$、$k$ 和 $G$ 的单一闭式解析表达式。不需要进行数值评估，也不涉及单位。最终答案必须仅为该单一表达式。",
            "solution": "该问题要求推导带梯度裁剪的在线梯度下降 (OGD) 算法的遗憾 $R_{T}$ 的一个上界。推导必须从基本原理出发，并考虑到裁剪引入的偏差。最终的界必须相对于步长 $\\eta$ 进行最小化。\n\n遗憾定义为 $R_{T} \\triangleq \\sum_{t=1}^{T} \\big( f_{t}(\\mathbf{x}_{t}) - f_{t}(\\mathbf{x}^{\\star}) \\big)$。\n由于每个损失函数 $f_t$ 都是凸的，我们可以应用凸性的一阶条件，该条件表明对于定义域中的任意 $\\mathbf{x}, \\mathbf{y}$，有 $f_t(\\mathbf{x}) - f_t(\\mathbf{y}) \\le \\langle \\nabla f_t(\\mathbf{x}), \\mathbf{x} - \\mathbf{y} \\rangle$。将此应用于我们的情境中，其中 $\\mathbf{g}_{t} \\triangleq \\nabla f_{t}(\\mathbf{x}_{t})$：\n$$\nf_{t}(\\mathbf{x}_{t}) - f_{t}(\\mathbf{x}^{\\star}) \\le \\langle \\mathbf{g}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle.\n$$\n对所有从 $t=1$ 到 $T$ 的轮次求和：\n$$\nR_{T} \\le \\sum_{t=1}^{T} \\langle \\mathbf{g}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle.\n$$\nOGD 更新规则使用的是裁剪后的梯度 $\\mathbf{u}_t$，而不是真实的梯度 $\\mathbf{g}_t$。为了分析算法的性能，我们必须将 $\\mathbf{u}_t$ 引入表达式中。我们将内积分解如下：\n$$\n\\langle \\mathbf{g}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle = \\langle \\mathbf{u}_{t} + (\\mathbf{g}_{t} - \\mathbf{u}_{t}), \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle = \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle + \\langle \\mathbf{g}_{t} - \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle.\n$$\n第一项与算法的更新有关，而第二项是裁剪引入的偏差。因此，遗憾受两个分量之和的限制：\n$$\nR_{T} \\le \\sum_{t=1}^{T} \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle + \\sum_{t=1}^{T} \\langle \\mathbf{g}_{t} - \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle.\n$$\n我们分别分析每个分量。\n\n首先，考虑包含偏差的项，$\\sum_{t=1}^{T} \\langle \\mathbf{g}_{t} - \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle$。\n根据问题的数据模型，有两种类型的轮次。对于 $\\|\\mathbf{g}_{t}\\| \\le C$ 的 $T-k$ 轮，裁剪未被激活，因此 $\\mathbf{u}_{t} = \\mathbf{g}_{t}$，这意味着 $\\mathbf{g}_{t} - \\mathbf{u}_{t} = \\mathbf{0}$。这些轮次的贡献为零。\n对于 $\\|\\mathbf{g}_{t}\\| = G  C$ 的 $k$ 轮，问题给出了内积的一个明确上界：$\\langle \\mathbf{g}_{t} - \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle \\le D (G - C)$。\n对所有 $T$ 轮求和，偏差项的总贡献的界为：\n$$\n\\sum_{t=1}^{T} \\langle \\mathbf{g}_{t} - \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle \\le k D (G - C).\n$$\n\n接下来，我们分析标准的 OGD 项，$\\sum_{t=1}^{T} \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle$。我们使用基于到比较器的平方欧几里得距离 $\\|\\mathbf{x}_{t} - \\mathbf{x}^{\\star}\\|^{2}$ 的势函数论证。更新规则是 $\\mathbf{x}_{t+1} = \\Pi_{\\mathcal{K}}(\\mathbf{x}_{t} - \\eta \\mathbf{u}_{t})$。\n我们来分析第 $t+1$ 步的距离：\n$$\n\\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} = \\|\\Pi_{\\mathcal{K}}(\\mathbf{x}_{t} - \\eta \\mathbf{u}_{t}) - \\mathbf{x}^{\\star}\\|^{2}.\n$$\n由于 $\\mathbf{x}^{\\star} \\in \\mathcal{K}$，我们可以使用欧几里得投影 $\\Pi_{\\mathcal{K}}$ 的非扩张性质，该性质表明对于任意向量 $\\mathbf{z}$ 和任意点 $\\mathbf{y} \\in \\mathcal{K}$，都有 $\\|\\Pi_{\\mathcal{K}}(\\mathbf{z}) - \\mathbf{y}\\| \\le \\|\\mathbf{z} - \\mathbf{y}\\|$。将此应用于 $\\mathbf{z} = \\mathbf{x}_{t} - \\eta \\mathbf{u}_{t}$ 和 $\\mathbf{y} = \\mathbf{x}^{\\star}$：\n$$\n\\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} \\le \\|\\mathbf{x}_{t} - \\eta \\mathbf{u}_{t} - \\mathbf{x}^{\\star}\\|^{2} = \\|(\\mathbf{x}_{t} - \\mathbf{x}^{\\star}) - \\eta \\mathbf{u}_{t}\\|^{2}.\n$$\n展开平方范数：\n$$\n\\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} \\le \\|\\mathbf{x}_{t} - \\mathbf{x}^{\\star}\\|^{2} - 2\\eta \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle + \\eta^{2} \\|\\mathbf{u}_{t}\\|^{2}.\n$$\n重新整理这个不等式以分离出内积项，我们得到：\n$$\n\\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle \\le \\frac{1}{2\\eta} \\left( \\|\\mathbf{x}_{t} - \\mathbf{x}^{\\star}\\|^{2} - \\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} \\right) + \\frac{\\eta}{2} \\|\\mathbf{u}_{t}\\|^{2}.\n$$\n将此不等式从 $t=1$ 到 $T$ 求和：\n$$\n\\sum_{t=1}^{T} \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle \\le \\frac{1}{2\\eta} \\sum_{t=1}^{T} \\left( \\|\\mathbf{x}_{t} - \\mathbf{x}^{\\star}\\|^{2} - \\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} \\right) + \\frac{\\eta}{2} \\sum_{t=1}^{T} \\|\\mathbf{u}_{t}\\|^{2}.\n$$\n右边的第一个和是一个伸缩级数：\n$$\n\\sum_{t=1}^{T} \\left( \\|\\mathbf{x}_{t} - \\mathbf{x}^{\\star}\\|^{2} - \\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} \\right) = \\|\\mathbf{x}_{1} - \\mathbf{x}^{\\star}\\|^{2} - \\|\\mathbfx_{T+1} - \\mathbf{x}^{\\star}\\|^{2}.\n$$\n由于 $\\mathbf{x}_{1}, \\mathbf{x}^{\\star} \\in \\mathcal{K}$，距离 $\\|\\mathbf{x}_{1} - \\mathbf{x}^{\\star}\\|$ 受集合 $\\mathcal{K}$ 的直径 $D$ 的限制，因此 $\\|\\mathbf{x}_{1} - \\mathbf{x}^{\\star}\\|^{2} \\le D^{2}$。此外，$\\|\\mathbf{x}_{T+1} - \\mathbf{x}^{\\star}\\|^{2} \\ge 0$。因此，该伸缩级数的上界为 $D^{2}$。\n对于第二个和，我们使用裁剪后梯度的性质：根据构造，对所有 $t$ 都有 $\\|\\mathbf{u}_{t}\\| \\le C$。因此，$\\|\\mathbf{u}_{t}\\|^{2} \\le C^{2}$，并且\n$$\n\\sum_{t=1}^{T} \\|\\mathbf{u}_{t}\\|^{2} \\le \\sum_{t=1}^{T} C^{2} = T C^{2}.\n$$\n代入这些界，我们得到：\n$$\n\\sum_{t=1}^{T} \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle \\le \\frac{D^{2}}{2\\eta} + \\frac{\\eta T C^{2}}{2}.\n$$\n结合遗憾的两个分量的界：\n$$\nR_{T} \\le \\left( \\frac{D^{2}}{2\\eta} + \\frac{\\eta T C^{2}}{2} \\right) + k D (G - C).\n$$\n这个表达式是我们的遗憾上界，它是步长 $\\eta$ 的函数。为了找到最紧的界，我们必须选择 $\\eta$ 来最小化这个表达式。项 $k D (G-C)$ 相对于 $\\eta$ 是常数。我们需要最小化 $f(\\eta) = \\frac{D^{2}}{2\\eta} + \\frac{\\eta T C^{2}}{2}$。\n我们对 $\\eta$ 求导并令其为 $0$：\n$$\n\\frac{d f}{d \\eta} = -\\frac{D^{2}}{2\\eta^{2}} + \\frac{T C^{2}}{2} = 0.\n$$\n求解 $\\eta$：\n$$\n\\frac{T C^{2}}{2} = \\frac{D^{2}}{2\\eta^{2}} \\implies \\eta^{2} = \\frac{D^{2}}{T C^{2}} \\implies \\eta^{\\star} = \\frac{D}{C\\sqrt{T}}.\n$$\n我们选择正根，因为 $\\eta  0$。二阶导数 $\\frac{d^{2}f}{d\\eta^{2}} = \\frac{D^{2}}{\\eta^{3}}$ 对于 $\\eta0$ 是正的，这证实了这是一个最小值。\n最后，我们将最优步长 $\\eta^{\\star}$ 代回遗憾界中：\n$$\nR_{T} \\le \\frac{D^{2}}{2 \\left(\\frac{D}{C\\sqrt{T}}\\right)} + \\frac{\\left(\\frac{D}{C\\sqrt{T}}\\right) T C^{2}}{2} + k D (G - C)\n$$\n$$\nR_{T} \\le \\frac{D^{2} C \\sqrt{T}}{2D} + \\frac{D T C^{2}}{2 C \\sqrt{T}} + k D (G - C)\n$$\n$$\nR_{T} \\le \\frac{D C \\sqrt{T}}{2} + \\frac{D C \\sqrt{T}}{2} + k D (G - C)\n$$\n$$\nR_{T} \\le D C \\sqrt{T} + k D (G - C).\n$$\n这就是遗憾 $R_T$ 的最小化上界。",
            "answer": "$$\\boxed{D C \\sqrt{T} + k D (G - C)}$$"
        },
        {
            "introduction": "理论分析固然重要，但构建并测试新算法是推动领域发展的关键。本练习将带你从分析转向综合与实验，探索一种能根据数据流动态调整行为的自适应算法。你将实现一个基于梯度变化的“智能重启”机制，并在不同类型的环境中测试其有效性，亲身体验算法设计中直觉与验证相结合的过程。",
            "id": "3159468",
            "problem": "你将实现并评估一个在欧几里得球上的在线凸优化设置中，镜像下降的自适应重启变体。目标是测试在表现出趋势与纯噪声的梯度序列下，基于梯度内积的重启是否能改善遗憾。\n\n开始的基础由在线凸优化中以下经过充分检验的定义和事实组成。在凸决策集 $\\mathcal{K}$ 上的在线凸优化协议中，在每一轮 $t \\in \\{1,\\dots,T\\}$：玩家选择一个点 $x_t \\in \\mathcal{K}$，然后揭示一个凸损失函数 $f_t$，玩家产生损失 $f_t(x_t)$。相对于事后最佳固定比较器的遗憾为\n$$\n\\mathrm{Regret}_T \\triangleq \\sum_{t=1}^T f_t(x_t) - \\min_{x \\in \\mathcal{K}} \\sum_{t=1}^T f_t(x).\n$$\n使用可微的严格凸镜像映射 $\\psi$ 的镜像下降产生如下更新\n$$\nx_{t+1} \\in \\arg\\min_{x \\in \\mathcal{K}} \\left\\{ \\eta \\langle g_t, x \\rangle + D_{\\psi}(x, x_t) \\right\\},\n$$\n其中 $g_t \\in \\partial f_t(x_t)$ 是一个次梯度（对于我们的可微线性损失，这就是梯度），$\\eta  0$ 是步长，而 $D_{\\psi}(x,y) \\triangleq \\psi(x) - \\psi(y) - \\langle \\nabla \\psi(y), x - y \\rangle$ 是Bregman散度。对于欧几里得镜像映射 $\\psi(x) = \\tfrac{1}{2}\\|x\\|_2^2$，我们有 $D_{\\psi}(x,y) = \\tfrac{1}{2}\\|x - y\\|_2^2$。你的推导必须从这些定义和事实出发，不得假设任何目标公式。\n\n你将专注于形式为 $f_t(x) = \\langle g_t, x \\rangle$ 的线性损失，其定义在半径为 $R  0$ 的欧几里得球 $\\mathcal{K} = \\{ x \\in \\mathbb{R}^d : \\|x\\|_2 \\le R \\}$ 上。在这种情况下，事后最佳固定比较器是 $\\mathcal{K}$ 上的一个线性规划的解，你应该在你的解中根据累积梯度 $G_T = \\sum_{t=1}^T g_t$ 明确地推导出它。\n\n你将实现两种算法：\n\n- 基线镜像下降（欧几里得）：从 $x_1 = 0$ 开始，对于 $t \\in \\{1,\\dots,T\\}$，产生损失 $f_t(x_t) = \\langle g_t, x_t \\rangle$，并更新\n  $$\n  x_{t+1} = \\Pi_{\\mathcal{K}}(x_t - \\eta g_t),\n  $$\n  其中 $\\Pi_{\\mathcal{K}}$ 是到 $\\mathcal{K}$ 上的欧几里得投影。\n\n- 自适应重启镜像下降：如下定义重启锚点 $a_t$。对于 $t = 1$，设置 $a_1 = x_1$。对于 $t \\ge 2$，基于连续梯度的内积和阈值 $\\alpha \\in [0,1]$ 计算余弦条件：\n  $$\n  \\langle g_t, g_{t-1} \\rangle \\le - \\alpha \\, \\|g_t\\|_2 \\, \\|g_{t-1}\\|_2.\n  $$\n  如果条件成立，设置 $a_t = 0$（重启到 $\\mathcal{K}$ 的中心）；否则设置 $a_t = x_t$。然后更新\n  $$\n  x_{t+1} = \\Pi_{\\mathcal{K}}(a_t - \\eta g_t).\n  $$\n  第 $t$ 轮的损失仍然在 $x_t$ 处产生；重启决策仅影响 $x_{t+1}$。\n\n你必须在相同的梯度序列 $g_t$ 上评估这两种算法，并报告带符号的遗憾差\n$$\n\\Delta \\mathrm{Regret} \\triangleq \\mathrm{Regret}_T^{\\text{restart}} - \\mathrm{Regret}_T^{\\text{baseline}}.\n$$\n负值表示重启改善了遗憾。\n\n你的实现必须遵守以下场景和测试套件。所有向量和矩阵都是实值的，所有运算都是欧几里得运算。不涉及物理单位。角度（如果相关）通过余弦关系隐式地以弧度表示。所有随机性必须通过固定的随机种子来确定。在所有情况下，使用 $R = 1$ 并初始化 $x_1 = 0$。对于每种情况，按照规定构造梯度序列 $(g_t)_{t=1}^T$，然后使用给定的参数运行两种算法。\n\n测试套件。对于以下每种情况，你必须严格使用所陈述的参数。所有标量数值都必须解释为实数。\n\n- 情况1（低噪声趋势）：\n  - 维度 $d = 5$，轮数 $T = 200$。\n  - 基本趋势 $v = 0.3 \\cdot \\mathbf{1}_d$。\n  - 噪声标准差 $\\sigma = 0.02$。\n  - 用于噪声的随机种子为 $11$。\n  - 步长 $\\eta = 0.2$，重启阈值 $\\alpha = 0.0$。\n  - 梯度序列：$g_t = v + \\sigma \\cdot \\xi_t$，其中 $\\xi_t \\sim \\mathcal{N}(0, I_d)$ 独立同分布，使用指定的种子生成。\n\n- 情况2（纯噪声）：\n  - 维度 $d = 10$，轮数 $T = 400$。\n  - 基本趋势 $v = 0$。\n  - 噪声标准差 $\\sigma = 0.2$。\n  - 用于噪声的随机种子为 $22$。\n  - 步长 $\\eta = 0.2$，重启阈值 $\\alpha = 0.0$。\n  - 梯度序列：$g_t = \\sigma \\cdot \\xi_t$，其中 $\\xi_t$ 如上所述。\n\n- 情况3（小趋势伴随较高噪声）：\n  - 维度 $d = 20$，轮数 $T = 400$。\n  - 基本趋势幅度 $\\|v\\|_2 = 0.05$，其方向通过使用随机种子 $33$ 采样 $\\tilde{v} \\sim \\mathcal{N}(0, I_d)$ 并设置 $v = 0.05 \\cdot \\tilde{v} / \\|\\tilde{v}\\|_2$ 得出。\n  - 噪声标准差 $\\sigma = 0.3$，噪声序列使用相同的随机种子 $33$。\n  - 步长 $\\eta = 0.15$，重启阈值 $\\alpha = 0.5$。\n  - 梯度序列：$g_t = v + \\sigma \\cdot \\xi_t$。\n\n- 情况4（单轮边界情况）：\n  - 维度 $d = 3$，轮数 $T = 1$。\n  - 基本趋势 $v = 0.5 \\cdot \\mathbf{1}_d$。\n  - 噪声标准差 $\\sigma = 0$。\n  - 步长 $\\eta = 0.5$，重启阈值 $\\alpha = 0.0$。\n  - 梯度序列：$g_1 = v$。\n\n- 情况5（对抗性交替梯度）：\n  - 维度 $d = 2$，轮数 $T = 50$。\n  - 确定性梯度在第一个坐标上交替：对于奇数 $t$，$g_t = (1, 0)$；对于偶数 $t$，$g_t = (-1, 0)$。\n  - 步长 $\\eta = 0.4$，重启阈值 $\\alpha = 0.9$。\n\n对于每种情况，计算上面定义的带符号遗憾差 $\\Delta \\mathrm{Regret}$。你的程序应该生成单行输出，包含按情况1到5顺序排列的结果，形式为方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3,r_4,r_5]$）。每个 $r_i$ 都必须是浮点数。不允许有其他输出。",
            "solution": "该问题是有效的。它在科学上基于既定的在线凸优化理论，对于每个测试用例，问题都是适定的（well-posed），具有唯一且可计算的解，并且其表述是客观的，带有精确的数学定义和参数。\n\n### 步骤1：提取给定信息\n\n**协议：** 在凸集 $\\mathcal{K}$ 上的在线凸优化。在每一轮 $t \\in \\{1,\\dots,T\\}$，玩家选择一个点 $x_t \\in \\mathcal{K}$，一个凸损失函数 $f_t$ 被揭示，玩家产生损失 $f_t(x_t)$。\n\n**定义：**\n- **遗憾：** $\\mathrm{Regret}_T \\triangleq \\sum_{t=1}^T f_t(x_t) - \\min_{x \\in \\mathcal{K}} \\sum_{t=1}^T f_t(x)$。\n- **决策集：** $\\mathcal{K} = \\{ x \\in \\mathbb{R}^d : \\|x\\|_2 \\le R \\}$，其中 $R  0$。\n- **损失函数：** $f_t(x) = \\langle g_t, x \\rangle$，对于梯度向量序列 $(g_t)_{t=1}^T$。\n- **目标度量：** $\\Delta \\mathrm{Regret} \\triangleq \\mathrm{Regret}_T^{\\text{restart}} - \\mathrm{Regret}_T^{\\text{baseline}}$。\n\n**算法：**\n- **初始条件：** 两种算法均为 $x_1 = 0$。半径为 $R=1$。\n- **基线镜像下降：** $x_{t+1} = \\Pi_{\\mathcal{K}}(x_t - \\eta g_t)$，其中 $\\Pi_{\\mathcal{K}}$ 是到 $\\mathcal{K}$ 上的欧几里得投影。\n- **自适应重启镜像下降：**\n  - 锚点 $a_t$：\n    - 对于 $t=1$，$a_1 = x_1$。\n    - 对于 $t \\ge 2$，如果 $\\langle g_t, g_{t-1} \\rangle \\le - \\alpha \\, \\|g_t\\|_2 \\, \\|g_{t-1}\\|_2$，则 $a_t = 0$；否则，$a_t = x_t$。\n  - 更新：$x_{t+1} = \\Pi_{\\mathcal{K}}(a_t - \\eta g_t)$。\n- **产生的损失：** 在第 $t$ 轮，两种算法的损失均为 $f_t(x_t) = \\langle g_t, x_t \\rangle$。\n\n### 步骤2：解决方案推导与算法设计\n\n任务是计算基线镜像下降和自适应重启版本镜像下降之间的遗憾差 $\\Delta \\mathrm{Regret}$。\n\n**1. 事后比较器与遗憾简化**\n遗憾是相对于事后最佳固定决策定义的。对于线性损失 $f_t(x) = \\langle g_t, x \\rangle$，一个固定点 $x \\in \\mathcal{K}$ 的累积损失为：\n$$\n\\sum_{t=1}^T f_t(x) = \\sum_{t=1}^T \\langle g_t, x \\rangle = \\left\\langle \\sum_{t=1}^T g_t, x \\right\\rangle = \\langle G_T, x \\rangle,\n$$\n其中 $G_T = \\sum_{t=1}^T g_t$ 是累积梯度。事后最佳固定决策是以下优化问题的解：\n$$\nx^* = \\arg\\min_{x \\in \\mathcal{K}} \\langle G_T, x \\rangle.\n$$\n集合 $\\mathcal{K}$ 是欧几里得球 $\\{x \\in \\mathbb{R}^d : \\|x\\|_2 \\le R\\}$。根据柯西-施瓦茨不等式，$\\langle G_T, x \\rangle \\ge -\\|G_T\\|_2 \\|x\\|_2$。由于 $\\|x\\|_2 \\le R$，当 $x$ 与 $G_T$ 反平行且具有最大范数（即 $\\|x\\|_2 = R$）时，达到最小值。因此，最优比较器是：\n$$\nx^* = -R \\frac{G_T}{\\|G_T\\|_2} \\quad (\\text{如果 } G_T \\neq 0).\n$$\n两种算法的遗憾为：\n$$\n\\mathrm{Regret}_T^{\\text{baseline}} = \\sum_{t=1}^T f_t(x_t^{\\text{baseline}}) - \\langle G_T, x^* \\rangle\n$$\n$$\n\\mathrm{Regret}_T^{\\text{restart}} = \\sum_{t=1}^T f_t(x_t^{\\text{restart}}) - \\langle G_T, x^* \\rangle\n$$\n目标度量 $\\Delta \\mathrm{Regret}$ 简化为总产生损失的差值，因为事后项对两者是共同的，所以相互抵消：\n$$\n\\Delta \\mathrm{Regret} = \\mathrm{Regret}_T^{\\text{restart}} - \\mathrm{Regret}_T^{\\text{baseline}} = \\sum_{t=1}^T f_t(x_t^{\\text{restart}}) - \\sum_{t=1}^T f_t(x_t^{\\text{baseline}}).\n$$\n这就是我们将要计算的量。\n\n**2. 算法实现**\n两种算法都是投影梯度下降的变体。欧几里得投影算子 $\\Pi_{\\mathcal{K}}$ 将一个点 $y \\in \\mathbb{R}^d$ 映射到球 $\\mathcal{K}$ 中的最近点：\n$$\n\\Pi_{\\mathcal{K}}(y) = \\begin{cases} y,  \\text{if } \\|y\\|_2 \\le R \\\\ R \\frac{y}{\\|y\\|_2},  \\text{if } \\|y\\|_2  R \\end{cases}\n$$\n对于每个测试用例，我们首先生成完整的梯度序列。然后，我们在此序列上运行两个独立的模拟，每个算法一个，并计算总损失。最后，我们计算它们的差值。\n\n**Python 实现**\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates baseline and adaptive-restart mirror descent\n    for online convex optimization over a Euclidean ball.\n    \"\"\"\n\n    def run_algorithm(gradients, d, R, eta, alpha, mode):\n        \"\"\"\n        Runs a single online optimization algorithm.\n        \"\"\"\n        T = len(gradients)\n        # x_1 is initialized to 0. In our 0-indexed loop, this means the\n        # point for the first round (t=0) is 0.\n        x = np.zeros(d)\n        total_loss = 0.0\n\n        for t in range(T):\n            # g is g_{t+1} from the problem's 1-based indexing.\n            # x is x_{t+1} from the problem's 1-based indexing.\n            g = gradients[t]\n            \n            # Incur loss at the current point x.\n            total_loss += np.dot(g, x)\n            \n            # Determine the point from which to update, to compute x_{t+2}.\n            # The anchor a_{t+1} is used to compute x_{t+2}.\n            update_point = x\n            if mode == 'restart':\n                # The restart rule for round t+1 (t>=1) uses g_{t+1} and g_t.\n                # In our 0-indexed loop at step `t`, this corresponds to\n                # checking gradients[t] and gradients[t-1].\n                if t >= 1:\n                    g_current = gradients[t]\n                    g_prev = gradients[t-1]\n                    norm_g_curr = np.linalg.norm(g_current)\n                    norm_g_prev = np.linalg.norm(g_prev)\n                    \n                    if norm_g_curr > 1e-9 and norm_g_prev > 1e-9:\n                        dot_prod = np.dot(g_current, g_prev)\n                        if dot_prod = -alpha * norm_g_curr * norm_g_prev:\n                            update_point = np.zeros(d) # Restart\n            \n            # Compute x for the next iteration (x_{t+2}).\n            y_next = update_point - eta * g\n            norm_y = np.linalg.norm(y_next)\n\n            if norm_y > R:\n                x = R * y_next / norm_y\n            else:\n                x = y_next\n                \n        return total_loss\n\n    test_cases_params = [\n        # Case 1\n        {'d': 5, 'T': 200, 'sigma': 0.02, 'seed': 11, 'eta': 0.2, 'alpha': 0.0, 'case_type': 'trend'},\n        # Case 2\n        {'d': 10, 'T': 400, 'sigma': 0.2, 'seed': 22, 'eta': 0.2, 'alpha': 0.0, 'case_type': 'noise'},\n        # Case 3\n        {'d': 20, 'T': 400, 'v_norm': 0.05, 'sigma': 0.3, 'seed': 33, 'eta': 0.15, 'alpha': 0.5, 'case_type': 'mixed'},\n        # Case 4\n        {'d': 3, 'T': 1, 'sigma': 0.0, 'seed': None, 'eta': 0.5, 'alpha': 0.0, 'case_type': 'boundary'},\n        # Case 5\n        {'d': 2, 'T': 50, 'sigma': 0.0, 'seed': None, 'eta': 0.4, 'alpha': 0.9, 'case_type': 'adversarial'},\n    ]\n\n    R = 1.0\n    results = []\n\n    for params in test_cases_params:\n        d = params['d']\n        T = params['T']\n        eta = params['eta']\n        alpha = params['alpha']\n        sigma = params['sigma']\n        \n        # Generate gradients\n        gradients = []\n        if params['case_type'] == 'trend':\n            rng = np.random.default_rng(params['seed'])\n            v = 0.3 * np.ones(d)\n            for _ in range(T):\n                noise = rng.normal(scale=sigma, size=d)\n                gradients.append(v + noise)\n        elif params['case_type'] == 'noise':\n            rng = np.random.default_rng(params['seed'])\n            for _ in range(T):\n                gradients.append(rng.normal(scale=sigma, size=d))\n        elif params['case_type'] == 'mixed':\n            rng = np.random.default_rng(params['seed'])\n            v_tilde = rng.normal(size=d)\n            v = params['v_norm'] * v_tilde / np.linalg.norm(v_tilde)\n            # Re-seed for noise generation is not specified, so we continue with the same rng state\n            for _ in range(T):\n                noise = rng.normal(scale=sigma, size=d)\n                gradients.append(v + noise)\n        elif params['case_type'] == 'boundary':\n            v = 0.5 * np.ones(d)\n            gradients.append(v)\n        elif params['case_type'] == 'adversarial':\n            g_odd = np.zeros(d); g_odd[0] = 1.0\n            g_even = np.zeros(d); g_even[0] = -1.0\n            for t in range(1, T + 1):\n                if t % 2 != 0:\n                    gradients.append(g_odd)\n                else:\n                    gradients.append(g_even)\n\n        # Run algorithms and compute delta regret\n        loss_baseline = run_algorithm(gradients, d, R, eta, alpha, mode='baseline')\n        loss_restart = run_algorithm(gradients, d, R, eta, alpha, mode='restart')\n        delta_regret = loss_restart - loss_baseline\n        results.append(delta_regret)\n\n    return f\"[{','.join(f'{r:.12f}' for r in results)}]\"\n\n# The final answer is the output of this function.\n# print(solve())\n```",
            "answer": "[0.443900985220,0.584144670054,-0.573902300067,0.000000000000,9.600000000000]"
        }
    ]
}