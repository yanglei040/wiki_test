{
    "hands_on_practices": [
        {
            "introduction": "LASSO (最小绝对收缩和选择算子) 是执行稀疏回归和特征选择的关键工具。然而，其选择的特征集合（即解的“支撑集”）可能对输入数据的微小变化非常敏感，尤其是在特征间存在相关性（即设计矩阵是病态的）时。本练习将让你亲手实现求解LASSO问题的近端梯度法 (ISTA)，并实证地探究其特征选择的稳定性，从而将矩阵条件数等理论概念与实际的算法行为联系起来 。",
            "id": "3183732",
            "problem": "考虑诱导稀疏性的最小二乘回归问题，称为最小绝对收缩和选择算子 (LASSO)，其中决策变量是系数向量 $x \\in \\mathbb{R}^n$，给定的数据是矩阵-向量对 $(A, b)$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 且 $b \\in \\mathbb{R}^m$。目标是最小化凸函数\n$$\nF(x) = \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\n其中 $\\lambda > 0$ 是一个正则化参数，$\\|\\cdot\\|_1$ 表示 1-范数。该模型在最优解 $x^\\star$ 中诱导稀疏性，其支撑集 $\\mathrm{supp}(x^\\star) = \\{ i \\in \\{1,\\dots,n\\} : x^\\star_i \\neq 0 \\}$ 可能对 $A$ 中的微小扰动高度敏感，特别是当 $A$ 是病态矩阵时。\n\n从凸优化、1-范数的次梯度和 Karush-Kuhn-Tucker (KKT) 条件的基本定义出发，推导一个算法，通过迭代一个近端梯度步来计算近似最小化子 $x^\\star$。该步骤的步长应根据 $F(x)$ 光滑部分梯度的 Lipschitz 常数来适当选择。然后，量化支撑集在 $A$ 的微小扰动下的敏感度，并将其与条件数以及 KKT 符号向量的变化联系起来。\n\n定义与使用要求：\n- 光滑项 $f(x) = \\frac{1}{2}\\|A x - b\\|_2^2$ 的梯度是 $\\nabla f(x) = A^\\top (A x - b)$。\n- 在 $x$ 处 $\\|x\\|_1$ 的次梯度是向量集合 $s \\in \\mathbb{R}^n$，其中如果 $x_i \\neq 0$ 则 $s_i = \\mathrm{sign}(x_i)$，如果 $x_i = 0$ 则 $s_i \\in [-1,1]$。\n- 最小化子 $x^\\star$ 的 KKT 最优性条件是 $A^\\top (A x^\\star - b) + \\lambda s^\\star = 0$，其中 $s^\\star$ 属于 $\\|x^\\star\\|_1$ 的次梯度。等价地，KKT 符号向量可以逐点定义为 $s^\\star = -\\frac{1}{\\lambda} A^\\top (A x^\\star - b)$，它必须满足 $|s^\\star_i| \\leq 1$ 并且当 $x^\\star_i \\neq 0$ 时等于 $\\mathrm{sign}(x^\\star_i)$。\n- 1-范数的近端算子是软阈值映射 $S_\\alpha(u)_i = \\mathrm{sign}(u_i)\\max\\{|u_i| - \\alpha, 0\\}$。\n\n实现一个程序，对下面定义的每个测试用例执行以下步骤：\n1. 使用迭代更新 $x^{k+1} = S_{\\lambda t}\\left(x^k - t A^\\top(A x^k - b)\\right)$ 计算 $F(x)$ 的一个近似最小化子 $x^\\star$，其中 $t \\in (0, 1/L]$ 且 $L$ 是 $\\nabla f(x)$ 的 Lipschitz 常数，由 $L = \\|A\\|_2^2$ 给出，即 $A$ 的谱范数（最大奇异值）的平方。使用相同的过程计算扰动矩阵 $A + \\Delta$ 的 $x^\\star$，保持 $b$ 和 $\\lambda$ 不变。\n2. 使用阈值 $10^{-6}$ 确定扰动前后的支撑集 $\\mathrm{supp}(x^\\star)$，即如果 $|x^\\star_i| > 10^{-6}$，则索引 $i$ 属于支撑集。\n3. 计算以下量化指标：\n   - $A$ 的条件数，定义为 $\\kappa(A) = \\sigma_{\\max}(A) / \\sigma_{\\min}(A)$，其中 $\\sigma_{\\max}(A)$ 和 $\\sigma_{\\min}(A)$ 分别是 $A$ 的最大和最小严格正奇异值。如果没有严格正奇异值存在，或者如果数值下溢导致 $\\sigma_{\\min}(A)$ 实际上为零，则设置 $\\kappa(A) = 10^{16}$ 作为一个较大的代理值。对 $A + \\Delta$ 计算相同的值，记为 $\\kappa(A+\\Delta)$。\n   - 扰动的 Frobenius 范数，$\\|\\Delta\\|_F$。\n   - 翻转指示符，如果扰动前后的支撑集不同则为 $1$，否则为 $0$。\n   - 汉明支撑集差异，等于扰动前后支撑集对称差集的大小，即只存在于两个支撑集之一中的索引数量。\n   - KKT 符号变化数，通过比较 KKT 符号向量 $s^\\star = -\\frac{1}{\\lambda}A^\\top(Ax^\\star - b)$ 和 $s^\\star_\\Delta = -\\frac{1}{\\lambda}(A+\\Delta)^\\top((A+\\Delta)x^\\star_\\Delta - b)$ 的符号来计算，统计符号分类发生变化的索引数量。使用分类规则：如果分量大于 $10^{-8}$，符号为 $+1$；如果小于 $-10^{-8}$，符号为 $-1$；否则为 $0$。\n\n测试套件：\n- 案例 1 (可能翻转支撑集的对抗性近共线性)：\n  - $A = \\begin{bmatrix} 1  1 + 10^{-3}  0 \\\\ 0  0  1 \\\\ 0  0  0.5 \\end{bmatrix}$，\n  - $\\Delta = \\begin{bmatrix} 0  -2 \\cdot 10^{-3}  0 \\\\ 0  0  0 \\\\ 0  0  0 \\end{bmatrix}$，\n  - $b = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$，\n  - $\\lambda = 0.05$。\n- 案例 2 (预期稳定的良态基线)：\n  - $A = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{bmatrix}$，\n  - $\\Delta = \\begin{bmatrix} 10^{-4}  -10^{-4}  0 \\\\ 0  10^{-4}  -10^{-4} \\\\ 0  0  10^{-4} \\end{bmatrix}$，\n  - $b = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$，\n  - $\\lambda = 0.05$。\n- 案例 3 (极端病态的近退化列)：\n  - $A = \\begin{bmatrix} 1  1 + 10^{-6}  0 \\\\ 0  10^{-6}  0 \\\\ 0  0  1 \\end{bmatrix}$，\n  - $\\Delta = \\begin{bmatrix} 0  -2 \\cdot 10^{-6}  0 \\\\ 0  0  0 \\\\ 0  0  0 \\end{bmatrix}$，\n  - $b = \\begin{bmatrix} 1 \\\\ 10^{-6} \\\\ 0 \\end{bmatrix}$，\n  - $\\lambda = 0.02$。\n\n输出规范：\n- 对于每个案例，输出一个包含六个条目的列表：$[\\kappa(A), \\kappa(A+\\Delta), \\|\\Delta\\|_F, \\text{flip}, \\text{hamming}, \\text{kkt\\_sign\\_changes}]$，其中前三个是浮点数值，后三个是整数。\n- 您的程序应生成单行输出，其中包含所有案例的结果，格式为方括号内由逗号分隔的列表，每个案例的结果本身也是如上所述的列表。例如，格式应类似于 $[[a_1,a_2,a_3,a_4,a_5,a_6],[b_1,\\dots,b_6],[c_1,\\dots,c_6]]$，其中包含数值。在最终输出中将浮点值四舍五入到六位小数。",
            "solution": "该问题要求实现并分析 LASSO (最小绝对收缩和选择算子) 优化问题的解。目标是最小化函数 $F(x) = f(x) + g(x)$，其中 $f(x) = \\frac{1}{2}\\|A x - b\\|_2^2$ 是一个光滑、凸、可微的损失项，而 $g(x) = \\lambda \\|x\\|_1$ 是一个凸、不可微的正则化项，它能促进解向量 $x$ 的稀疏性。\n\n用于寻找近似最小化子 $x^\\star$ 的算法是近端梯度法 (Proximal Gradient Method)，也称为迭代收缩阈值算法 (Iterative Shrinkage-Thresholding Algorithm, ISTA)。该方法非常适用于形式为 $f(x) + g(x)$ 的复合目标函数。其核心思想是，对光滑部分 $f(x)$ 迭代执行梯度下降步，然后应用非光滑部分 $g(x)$ 的近端算子来校正该步骤。\n\n近端梯度法的迭代更新规则由下式给出：\n$$\nx^{k+1} = \\mathrm{prox}_{t g}(x^k - t \\nabla f(x^k))\n$$\n其中 $t > 0$ 是步长，$\\nabla f(x)$ 是光滑项的梯度，$\\mathrm{prox}_{t g}$ 是函数 $t g(x)$ 的近端算子。\n\n首先，我们确定我们特定问题的组成部分：\n1.  光滑项是 $f(x) = \\frac{1}{2}\\|A x - b\\|_2^2$。其梯度被正确地给出为 $\\nabla f(x) = A^\\top (A x - b)$。\n2.  非光滑项是 $g(x) = \\lambda \\|x\\|_1$。映射 $u \\mapsto \\alpha \\|u\\|_1$ 的近端算子是逐元素的软阈值算子，定义为 $S_\\alpha(u)_i = \\mathrm{sign}(u_i)\\max\\{|u_i| - \\alpha, 0\\}$。因此，我们问题的近端算子是 $\\mathrm{prox}_{t g}(u) = S_{\\lambda t}(u)$。\n\n将这些组件代入通用更新规则，我们得到 LASSO 的特定迭代算法：\n$$\nx^{k+1} = S_{\\lambda t}\\left(x^k - t A^\\top(A x^k - b)\\right)\n$$\n这与问题描述中提供的更新规则相匹配。\n\n如果步长 $t$ 满足条件 $0 < t \\leq 1/L$，该算法的收敛性就能得到保证，其中 $L$ 是梯度 $\\nabla f(x)$ 的 Lipschitz 常数。Lipschitz 常数是满足对所有 $x, y$ 都有 $\\|\\nabla f(x) - \\nabla f(y)\\|_2 \\leq L \\|x - y\\|_2$ 的最小标量 $L$。我们可以如下推导 $L$：\n$$\n\\|\\nabla f(x) - \\nabla f(y)\\|_2 = \\|A^\\top(A x - b) - A^\\top(A y - b)\\|_2 = \\|A^\\top A (x - y)\\|_2\n$$\n根据诱导矩阵范数的定义，我们有：\n$$\n\\|A^\\top A (x - y)\\|_2 \\leq \\|A^\\top A\\|_2 \\|x - y\\|_2\n$$\n因此，Lipschitz 常数是 $L = \\|A^\\top A\\|_2$。对于任何矩阵 $M$，$\\|M^\\top M\\|_2$ 是 $M^\\top M$ 的最大特征值，它等于 $M$ 的最大奇异值 $\\sigma_{\\max}(M)$ 的平方。$M$ 的谱范数 $\\|M\\|_2$ 根据定义就是 $\\sigma_{\\max}(M)$。因此，$L = \\sigma_{\\max}(A)^2 = \\|A\\|_2^2$，这证实了问题描述中给出的值。一个安全且标准的步长选择是 $t = 1/L$。\n\n实现过程首先初始化 $x^0$ (例如，作为零向量)，然后迭代更新规则，直到变化量 $\\|x^{k+1} - x^k\\|_2$ 小于一个微小容差，或达到最大迭代次数。此过程对原始矩阵 $A$ 和扰动矩阵 $A+\\Delta$ 都执行，以获得各自的解 $x^\\star$ 和 $x^\\star_\\Delta$。\n\n一旦计算出解，就计算以下指标来量化解的支撑集对扰动 $\\Delta$ 的敏感度：\n1.  **支撑集**：通过识别系数绝对值 $|x_i^\\star|$ 超过阈值 $10^{-6}$ 的索引 $i$ 来确定 $x^\\star$ 和 $x^\\star_\\Delta$ 的支撑集。\n2.  **条件数**：$\\kappa(A)$ 和 $\\kappa(A+\\Delta)$ 计算为最大与最小严格正奇异值之比。如果矩阵是奇异的或数值上接近奇异，则使用一个较大的代理值 $10^{16}$。该指标表明矩阵距离秩亏的程度，这是解稳定性的一个关键因素。\n3.  **扰动范数**：扰动的大小由 Frobenius 范数 $\\|\\Delta\\|_F = \\sqrt{\\sum_{i,j} |\\Delta_{ij}|^2}$ 衡量。\n4.  **翻转指示符**：一个二进制值，指示原始解和扰动解的支撑集是否相同。值为 $1$ 表示有效预测变量集合发生了变化。\n5.  **汉明支撑集差异**：两个支撑集之间对称差集的大小，它计算了由于扰动而被添加或从模型中移除的特征数量。\n6.  **KKT 符号变化**：LASSO 的 Karush-Kuhn-Tucker (KKT) 最优性条件指出，在最小化子 $x^\\star$ 处，必须存在一个次梯度向量 $s^\\star \\in \\partial \\|x^\\star\\|_1$，使得 $\\nabla f(x^\\star) + \\lambda s^\\star = 0$。这意味着 $s^\\star = -\\frac{1}{\\lambda} \\nabla f(x^\\star) = -\\frac{1}{\\lambda} A^\\top (A x^\\star - b)$。向量 $s^\\star$ 必须满足对所有 $i$ 都有 $|s^\\star_i| \\le 1$，并且对于 $i \\in \\mathrm{supp}(x^\\star)$ 有 $s^\\star_i = \\mathrm{sign}(x^\\star_i)$。原始问题和扰动问题之间此 KKT 向量符号模式的变化，表明最优性条件发生了根本性转变，这通常先于或伴随着支撑集的变化。符号是使用 $10^{-8}$ 的数值容差确定的。\n\n该程序实现了这整个流程，并将其应用于每个测试用例，以展示矩阵条件、扰动与稀疏解稳定性之间的关系。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([[1.0, 1.0 + 1e-3, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 0.5]]),\n            \"Delta\": np.array([[0.0, -2e-3, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]),\n            \"b\": np.array([1.0, 0.0, 0.0]),\n            \"lambda_reg\": 0.05\n        },\n        {\n            \"A\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),\n            \"Delta\": np.array([[1e-4, -1e-4, 0.0], [0.0, 1e-4, -1e-4], [0.0, 0.0, 1e-4]]),\n            \"b\": np.array([1.0, 1.0, 1.0]),\n            \"lambda_reg\": 0.05\n        },\n        {\n            \"A\": np.array([[1.0, 1.0 + 1e-6, 0.0], [0.0, 1e-6, 0.0], [0.0, 0.0, 1.0]]),\n            \"Delta\": np.array([[0.0, -2e-6, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]),\n            \"b\": np.array([1.0, 1e-6, 0.0]),\n            \"lambda_reg\": 0.02\n        }\n    ]\n\n    def solve_lasso(A, b, lambda_reg, max_iter=20000, tol=1e-9):\n        \"\"\"\n        Solves the LASSO problem using the Proximal Gradient Method (ISTA).\n        \"\"\"\n        m, n = A.shape\n        x = np.zeros(n)\n\n        s = np.linalg.svd(A, compute_uv=False)\n        L = s[0]**2 if s.size > 0 else 0.0\n\n        if L == 0:\n            return x # A is zero matrix, x=0 is a solution for b=0\n\n        t = 1.0 / L\n        alpha = lambda_reg * t\n\n        for _ in range(max_iter):\n            x_old = x.copy()\n            \n            grad = A.T @ (A @ x - b)\n            u = x - t * grad\n            \n            x = np.sign(u) * np.maximum(np.abs(u) - alpha, 0.0)\n            \n            if np.linalg.norm(x - x_old)  tol:\n                break\n                \n        return x\n\n    def process_case(A, Delta, b, lambda_reg):\n        \"\"\"\n        Computes all required metrics for a single test case.\n        \"\"\"\n        A_orig = A\n        A_pert = A + Delta\n\n        # 1. Compute solutions\n        x_star_orig = solve_lasso(A_orig, b, lambda_reg)\n        x_star_pert = solve_lasso(A_pert, b, lambda_reg)\n\n        # 2. Determine supports\n        support_threshold = 1e-6\n        support_orig = {i for i, val in enumerate(x_star_orig) if abs(val) > support_threshold}\n        support_pert = {i for i, val in enumerate(x_star_pert) if abs(val) > support_threshold}\n\n        # 3. Compute metrics\n        \n        # Condition numbers\n        def get_kappa(M):\n            s = np.linalg.svd(M, compute_uv=False)\n            s_pos = s[s > 1e-12]\n            if len(s_pos) == 0:\n                return 1e16\n            kappa = s_pos[0] / s_pos[-1]\n            return kappa if np.isfinite(kappa) and kappa  1e16 else 1e16\n\n        kappa_A = get_kappa(A_orig)\n        kappa_A_delta = get_kappa(A_pert)\n\n        # Frobenius norm of perturbation\n        norm_delta_F = np.linalg.norm(Delta, 'fro')\n\n        # Flip indicator\n        flip = 1 if support_orig != support_pert else 0\n\n        # Hamming support difference\n        hamming = len(support_orig.symmetric_difference(support_pert))\n\n        # KKT sign changes\n        s_star_orig = -(1.0/lambda_reg) * A_orig.T @ (A_orig @ x_star_orig - b)\n        s_star_pert = -(1.0/lambda_reg) * A_pert.T @ (A_pert @ x_star_pert - b)\n\n        def get_sign_vector(v, tol=1e-8):\n            return np.where(v > tol, 1, np.where(v  -tol, -1, 0))\n\n        signs_orig = get_sign_vector(s_star_orig)\n        signs_pert = get_sign_vector(s_star_pert)\n\n        kkt_sign_changes = int(np.sum(signs_orig != signs_pert))\n        \n        return [kappa_A, kappa_A_delta, norm_delta_F, flip, hamming, kkt_sign_changes]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(case[\"A\"], case[\"Delta\"], case[\"b\"], case[\"lambda_reg\"])\n        all_results.append(result)\n    \n    # Format the final output string\n    output_parts = []\n    for res in all_results:\n        # Round float values to 6 decimal places\n        r_kappa_A = f\"{res[0]:.6f}\"\n        r_kappa_Ad = f\"{res[1]:.6f}\"\n        r_norm_d = f\"{res[2]:.6f}\"\n        \n        # Integers are formatted as is\n        flip = res[3]\n        hamming = res[4]\n        kkt_changes = res[5]\n        \n        case_str = f\"[{r_kappa_A},{r_kappa_Ad},{r_norm_d},{flip},{hamming},{kkt_changes}]\"\n        output_parts.append(case_str)\n        \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "许多先进的优化算法，其核心思想是在每次迭代中高效地解决一个更小、更具体的子问题。将一个点投影到凸集上，例如 $\\ell_1$ 范数球，就是稀疏优化中一个常见且基础的子问题。通过为这个投影问题实现一个高效算法，你不仅能为更复杂的求解器开发一个关键模块，还能更深入地理解稀疏性范数的几何结构 。",
            "id": "3183719",
            "problem": "给定一个向量 $v \\in \\mathbb{R}^n$ 和一个半径 $\\tau \\geq 0$。考虑 $v$ 到 $\\ell_1$ 球 $\\{x \\in \\mathbb{R}^n : \\lVert x \\rVert_1 \\leq \\tau\\}$ 上的欧几里得投影，该投影被定义为以下凸优化问题的唯一解 $x^\\star$：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\ \\frac{1}{2}\\lVert x - v \\rVert_2^2 \\quad \\text{subject to} \\quad \\lVert x \\rVert_1 \\leq \\tau .\n$$\n你的任务是实现一个程序，该程序使用排序和阈值法计算此投影，并在适用时利用一种特殊的线性时间方法。这项工作必须基于基本定义：范数定义、到闭合凸集上的欧几里得投影，以及针对凸问题的 Karush–Kuhn–Tucker 最优性条件。\n\n算法要求：\n- 如果 $\\lVert v \\rVert_1 \\leq \\tau$，则投影为 $v$ 本身；必须通过单次遍历在 $O(n)$ 时间内检测到这种情况。\n- 否则，实现标准的排序和阈值法，该方法计算一个阈值并应用逐分量收缩。你的实现必须使用排序来获得阈值，并且在一般情况下必须在 $O(n \\log n)$ 时间内运行。\n- 为一种特殊情况提出并实现一个线性时间变体：当 $v$ 的分量的绝对值已经按非增序排列时，即 $\\lvert v_1 \\rvert \\geq \\lvert v_2 \\rvert \\geq \\cdots \\geq \\lvert v_n \\rvert$，你必须避免排序，并使用前缀和通过单次扫描在 $O(n)$ 时间内计算投影以定位收缩阈值。\n\n数值输出规范：\n- 对于每个测试用例，你的程序必须输出一个包含以下两项的对：\n  1. 投影向量 $x^\\star$，每个分量四舍五入到 $6$ 位小数。\n  2. 一个整数标志 $f \\in \\{0,1\\}$，指示是否使用了特殊情况的线性时间分支（$f = 1$）或未使用（$f = 0$）。如果 $\\lVert v \\rVert_1 \\leq \\tau$ 且你直接返回 $v$，则设置 $f = 0$。\n- 你的程序必须将所有测试用例的结果汇总到一行输出中，该行包含一个用方括号括起来的逗号分隔列表（例如，$[ \\text{case1}, \\text{case2}, \\ldots ]$）。每个用例必须以 $[\\text{projected\\_list}, f]$ 的形式出现。\n\n测试套件：\n- 使用以下测试用例，其中每一对是 $(v,\\tau)$：\n  1. $([3.0, -1.0, 2.0, -4.0], 5.0)$，一个需要收缩的一般情况。\n  2. $([0.2, -0.1, 0.3], 0.7)$，一个边界情况，其中 $\\lVert v \\rVert_1 \\leq \\tau$，投影应为 $v$。\n  3. $([-5.0, 4.0, -3.0], 0.0)$，一个 $\\tau = 0$ 的边缘情况。\n  4. $([4.0, -3.0, 2.0, -1.0, 0.5], 5.5)$，一个特殊情况，其中 $\\lvert v_i \\rvert$ 已经按非增序排列，应使用线性时间路径。\n\n最终输出格式：\n- 你的程序必须只打印一行：一个包含四个条目的列表，每个条目对应一个测试用例，并且每个条目必须是 $[\\text{projected\\_list}, f]$。每个 projected\\_list 必须包含四舍五入到 $6$ 位小数的实数。整数标志必须是 $0$ 或 $1$。不得打印任何额外文本。",
            "solution": "问题是计算向量 $v \\in \\mathbb{R}^n$ 到半径为 $\\tau \\geq 0$ 的 $\\ell_1$ 球上的欧几里得投影。这是凸优化中的一个标准问题，其形式化表述为：\n$$\nx^\\star = \\arg\\min_{x \\in \\mathbb{R}^n} \\ \\frac{1}{2}\\lVert x - v \\rVert_2^2 \\quad \\text{subject to} \\quad \\lVert x \\rVert_1 \\leq \\tau .\n$$\n目标函数 $f(x) = \\frac{1}{2}\\lVert x - v \\rVert_2^2$ 是严格凸的，约束集 $\\mathcal{C} = \\{x \\in \\mathbb{R}^n : \\lVert x \\rVert_1 \\leq \\tau\\}$ 是一个闭合凸集。因此，保证存在唯一解 $x^\\star$。\n\n我们可以通过分析 Karush-Kuhn-Tucker (KKT) 条件来解决这个问题。该问题的拉格朗日函数是：\n$$\n\\mathcal{L}(x, \\lambda) = \\frac{1}{2}\\lVert x - v \\rVert_2^2 + \\lambda(\\lVert x \\rVert_1 - \\tau),\n$$\n其中 $\\lambda \\geq 0$ 是与不等式约束相关的拉格朗日乘子。最优解 $(x^\\star, \\lambda^\\star)$ 的 KKT 条件是：\n1.  **平稳性 (Stationarity)**：$\\nabla_x \\mathcal{L}(x^\\star, \\lambda^\\star) = 0$。\n2.  **原始可行性 (Primal Feasibility)**：$\\lVert x^\\star \\rVert_1 \\leq \\tau$。\n3.  **对偶可行性 (Dual Feasibility)**：$\\lambda^\\star \\geq 0$。\n4.  **互补松弛性 (Complementary Slackness)**：$\\lambda^\\star(\\lVert x^\\star \\rVert_1 - \\tau) = 0$。\n\n平稳性条件涉及 $\\ell_1$ 范数的次梯度。拉格朗日函数关于 $x$ 的梯度由 $\\nabla_x \\mathcal{L}(x, \\lambda) = x - v + \\lambda \\partial \\lVert x \\rVert_1$ 给出，其中 $\\partial \\lVert x \\rVert_1$ 是 $\\ell_1$ 范数的次梯度。平稳性条件 $0 \\in x^\\star - v + \\lambda^\\star \\partial \\lVert x^\\star \\rVert_1$ 可以按分量进行分析：\n$$\nv_i - x_i^\\star \\in \\lambda^\\star \\partial \\lvert x_i^\\star \\rvert \\quad \\text{for } i=1, \\dots, n.\n$$\n绝对值函数的次梯度为：当 $z \\neq 0$ 时，$\\partial \\lvert z \\rvert = \\text{sign}(z)$；当 $z=0$ 时，$\\partial \\lvert 0 \\rvert = [-1, 1]$。分析 $x_i^\\star$ 的各种情况：\n-   如果 $x_i^\\star  0$，则 $v_i - x_i^\\star = \\lambda^\\star$，所以 $x_i^\\star = v_i - \\lambda^\\star$。这意味着 $v_i  \\lambda^\\star$。\n-   如果 $x_i^\\star  0$，则 $v_i - x_i^\\star = -\\lambda^\\star$，所以 $x_i^\\star = v_i + \\lambda^\\star$。这意味着 $v_i  -\\lambda^\\star$。\n-   如果 $x_i^\\star = 0$，则 $v_i \\in [-\\lambda^\\star, \\lambda^\\star]$，所以 $\\lvert v_i \\rvert \\leq \\lambda^\\star$。\n\n这三种情况可以统一为一个表达式，即软阈值算子：\n$$\nx_i^\\star = \\text{sign}(v_i) \\max(0, \\lvert v_i \\rvert - \\lambda^\\star).\n$$\n现在问题简化为找到拉格朗日乘子 $\\lambda^\\star \\geq 0$ 的正确值。我们使用互补松弛性条件来确定 $\\lambda^\\star$。\n\n**情况 1：$v$ 在 $\\ell_1$ 球内部**\n如果 $\\lVert v \\rVert_1 \\leq \\tau$，我们可以测试 $\\lambda^\\star = 0$ 是否为有效选择。在软阈值公式中设置 $\\lambda^\\star = 0$ 会得到 $x_i^\\star = \\text{sign}(v_i)\\max(0, \\lvert v_i \\rvert) = v_i$。因此，$x^\\star = v$。该解是原始可行的，因为 $\\lVert x^\\star \\rVert_1 = \\lVert v \\rVert_1 \\leq \\tau$。对偶可行性 $\\lambda^\\star=0 \\geq 0$ 也满足。互补松弛性条件 $\\lambda^\\star(\\lVert v \\rVert_1 - \\tau) = 0 \\cdot (\\lVert v \\rVert_1 - \\tau) = 0$ 也满足。因此，如果 $\\lVert v \\rVert_1 \\leq \\tau$，投影就是 $v$ 本身。此检查可在 $O(n)$ 时间内完成。\n\n**情况 2：$v$ 在 $\\ell_1$ 球外部**\n如果 $\\lVert v \\rVert_1  \\tau$，那么 $x^\\star \\neq v$，这要求 $\\lambda^\\star  0$。根据互补松弛性，约束必须是激活的，即 $\\lVert x^\\star \\rVert_1 = \\tau$。代入软阈值表达式，我们得到：\n$$\n\\sum_{i=1}^n \\lvert x_i^\\star \\rvert = \\sum_{i=1}^n \\max(0, \\lvert v_i \\rvert - \\lambda^\\star) = \\tau.\n$$\n我们必须找到方程 $h(\\lambda) = \\sum_{i=1}^n \\max(0, \\lvert v_i \\rvert - \\lambda) = \\tau$ 的根 $\\lambda^\\star  0$。函数 $h(\\lambda)$ 是连续、分段线性的，并且随 $\\lambda$ 单调递减。它的断点位于 $\\lvert v_i \\rvert$ 的值处。这种结构使得可以高效地搜索 $\\lambda^\\star$。\n\n**通用算法 ($O(n \\log n)$)**\n令 $u_i = \\lvert v_i \\rvert$。我们将这些值按降序排序：$u_{(1)} \\geq u_{(2)} \\geq \\cdots \\geq u_{(n)}$。假设正确的阈值 $\\lambda^\\star$ 位于某个 $\\rho \\in \\{1, \\ldots, n\\}$ 的区间 $(u_{(\\rho+1)}, u_{(\\rho)}]$ 内（其中 $u_{(n+1)}:=0$）。那么投影向量 $x^\\star$ 中恰好有 $\\rho$ 个分量是非零的。方程变为：\n$$\n\\sum_{i=1}^{\\rho} (u_{(i)} - \\lambda^\\star) = \\tau \\implies \\lambda^\\star = \\frac{1}{\\rho}\\left(\\sum_{i=1}^{\\rho} u_{(i)} - \\tau\\right).\n$$\n我们需要找到正确的 $\\rho$。正确的 $\\rho$ 是满足条件的最大索引 $j \\in \\{1, \\ldots, n\\}$，使得得到的阈值小于 $u_{(j)}$，从而确保 $x_{(j)}^\\star$ 为正。该条件为 $u_{(j)}  \\frac{1}{j}\\left(\\sum_{i=1}^{j} u_{(i)} - \\tau\\right)$，可简化为 $j \\cdot u_{(j)}  \\sum_{i=1}^{j} u_{(i)} - \\tau$。\n\n算法如下：\n1.  计算 $u_i = \\lvert v_i \\rvert$，其中 $i=1, \\dots, n$。\n2.  如果 $\\sum_{i=1}^n u_i \\leq \\tau$，返回 $v$。此外，对于特殊情况 $\\tau = 0$，唯一的可行点是 $x=0$，因此我们返回零向量。\n3.  将 $u$ 按降序排序得到 $u_{(1)}, \\ldots, u_{(n)}$。这需要 $O(n \\log n)$ 时间。\n4.  计算累积和 $S_j = \\sum_{i=1}^j u_{(i)}$。\n5.  找到满足 $j \\cdot u_{(j)}  S_j - \\tau$ 的最大索引 $\\rho \\in \\{1, \\ldots, n\\}$。这可以在排序后用 $O(n)$ 时间找到。由于我们假设 $\\lVert v \\rVert_1  \\tau  0$，因此保证存在这样的 $\\rho$。\n6.  计算阈值 $\\lambda^\\star = \\frac{1}{\\rho}(S_{\\rho} - \\tau)$。\n7.  使用 $x_i^\\star = \\text{sign}(v_i) \\max(0, \\lvert v_i \\rvert - \\lambda^\\star)$ 计算投影 $x^\\star$。这需要 $O(n)$ 时间。\n\n主要步骤是排序，导致总复杂度为 $O(n \\log n)$。\n\n**特殊情况线性时间算法 ($O(n)$)**\n如果 $v$ 的分量的绝对值已经按非增序排列，即 $\\lvert v_1 \\rvert \\geq \\lvert v_2 \\rvert \\geq \\cdots \\geq \\lvert v_n \\rvert$，则可以跳过排序步骤（步骤 3）。向量 $u$ 已经是 $u_{(i)} = \\lvert v_i \\rvert$。我们可以通过计算前缀和并在每个索引处检查条件，在单次遍历中以 $O(n)$ 时间找到 $\\rho$。算法的其余部分保持不变，从而使总时间复杂度为 $O(n)$。这种特殊情况通过对输入向量进行 $O(n)$ 的扫描来检测。标志 $f=1$ 用于指示何时采用此线性时间路径；否则，$f=0$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef project_l1_ball(v, tau):\n    \"\"\"\n    Computes the Euclidean projection of a vector v onto the L1 ball of radius tau.\n    \n    The projection is the solution to the optimization problem:\n        min_x ||x - v||_2^2\n        s.t.  ||x||_1 = tau\n\n    Args:\n        v (list or np.ndarray): The vector to project.\n        tau (float): The radius of the L1 ball. Must be non-negative.\n\n    Returns:\n        tuple: A tuple containing:\n            - list: The projected vector, with components rounded to 6 decimal places.\n            - int: A flag (1 if the special linear-time case was used, 0 otherwise).\n    \"\"\"\n    v_np = np.asarray(v, dtype=float)\n    n = len(v_np)\n    flag = 0\n\n    # Handle the edge case where tau = 0. The projection must be the zero vector.\n    if tau == 0.0:\n        return [0.0] * n, 0\n\n    # Handle the trivial case where v is already in the L1 ball.\n    l1_norm_v = np.sum(np.abs(v_np))\n    if l1_norm_v = tau:\n        return [round(c, 6) for c in v_np], 0\n\n    # The projection algorithm requires operating on absolute values.\n    v_abs = np.abs(v_np)\n    \n    # Check if a special-case linear-time algorithm can be used.\n    # This is applicable if the absolute values of v's components are already sorted.\n    is_sorted_desc = np.all(v_abs[:-1] >= v_abs[1:])\n    \n    theta = 0.0  # The shrinkage threshold\n\n    if is_sorted_desc:\n        flag = 1\n        # O(n) path: No sorting needed.\n        # Compute cumulative sums of the already sorted absolute values.\n        cssv = np.cumsum(v_abs)\n        # Find the number of non-zero elements in the solution, rho.\n        # This is the largest index j such that u_j > (cumsum(u)_j - tau) / j.\n        # Note: Using 1-based indexing for rho, so j = index + 1\n        j_vals = np.arange(1, n + 1)\n        # Since ||v||_1 > tau > 0, the set of indices satisfying the condition is non-empty.\n        rho_idx = np.where(v_abs > (cssv - tau) / j_vals)[0][-1]\n        rho = rho_idx + 1\n        theta = (cssv[rho_idx] - tau) / rho\n    else:\n        flag = 0\n        # O(n log n) path: General case requires sorting.\n        # Get indices that would sort v_abs in descending order.\n        desc_indices = np.argsort(v_abs)[::-1]\n        v_abs_sorted = v_abs[desc_indices]\n        \n        cssv = np.cumsum(v_abs_sorted)\n        j_vals = np.arange(1, n + 1)\n        rho_idx = np.where(v_abs_sorted > (cssv - tau) / j_vals)[0][-1]\n        rho = rho_idx + 1\n        theta = (cssv[rho_idx] - tau) / rho\n    \n    # Apply the soft-thresholding operator with the calculated threshold theta.\n    projection = np.sign(v_np) * np.maximum(v_abs - theta, 0)\n    \n    # Round components to 6 decimal places as specified.\n    rounded_projection = [round(c, 6) for c in projection]\n\n    return rounded_projection, flag\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the L1 ball projection problem.\n    \"\"\"\n    test_cases = [\n        # Case 1: General case requiring shrinkage.\n        ([3.0, -1.0, 2.0, -4.0], 5.0),\n        # Case 2: Trivial case, point is already in the ball.\n        ([0.2, -0.1, 0.3], 0.7),\n        # Case 3: Edge case with tau = 0.\n        ([-5.0, 4.0, -3.0], 0.0),\n        # Case 4: Special case where |v_i| are pre-sorted for linear-time solution.\n        ([4.0, -3.0, 2.0, -1.0, 0.5], 5.5),\n    ]\n\n    results = []\n    for v, tau in test_cases:\n        projection, flag = project_l1_ball(v, tau)\n        \n        # Format the projected list into a string without spaces\n        proj_str = '[' + ','.join(map(str, projection)) + ']'\n        # Format the full result for one case, e.g., \"[[1.0,2.0],0]\"\n        case_str = f\"[{proj_str},{flag}]\"\n        results.append(case_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}