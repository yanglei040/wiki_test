{
    "hands_on_practices": [
        {
            "introduction": "The effectiveness of a mini-batch method hinges on the mini-batch gradient being a reliable proxy for the true gradient. However, a poorly chosen batch can yield a highly inaccurate estimate, a risk we can quantify by considering a worst-case scenario. This exercise makes the abstract concept of gradient variance tangible by asking you to think like an adversary and construct the mini-batch that maximizes the estimation error, then explore how the simple, practical technique of gradient clipping can robustly mitigate this risk .",
            "id": "3150590",
            "problem": "Consider a dataset of per-sample gradients in one dimension, given by the multiset $\\{-4,-1,0,0,1,5\\}$, and suppose a mini-batch of size $b=3$ is used to form a mini-batch gradient estimator. Let the full-data gradient be the arithmetic mean $\\bar{g}$ of all per-sample gradients, and the mini-batch gradient estimator be the arithmetic mean $\\hat{g}(B)$ of the gradients in the selected mini-batch $B$. In standard mini-batch stochastic methods, $\\hat{g}(B)$ is intended to be a low-variance estimate of $\\bar{g}$ when $B$ is sampled randomly. In adversarial mini-batch construction, an adversary selects the mini-batch $B$ of fixed size $b=3$ to maximize the worst-case squared deviation of the estimator from the true gradient, defined by the mean squared error (MSE) measure $R(B)=(\\hat{g}(B)-\\bar{g})^{2}$.\n\nStarting from the fundamental definitions of the arithmetic mean and basic properties of averages, derive the form of the adversarial selection that maximizes $R(B)$, and compute the exact worst-case value\n$$\nR^{\\star}=\\max_{\\substack{B\\subset\\{-4,-1,0,0,1,5\\}\\\\|B|=3}}(\\hat{g}(B)-\\bar{g})^{2}.\n$$\n\nAs a mitigation strategy, consider scalar gradient clipping of each per-sample gradient to the interval $[-\\tau,\\tau]$ with threshold $\\tau=3$. Define the clipped gradient mapping $\\phi(g)=\\max\\{-\\tau,\\min\\{g,\\tau\\}\\}$ and the clipped dataset as $\\{\\phi(g):g\\in\\{-4,-1,0,0,1,5\\}\\}$. Let $\\bar{g}_{\\tau}$ be the mean of the clipped gradients and $\\hat{g}_{\\tau}(B)$ be the mean over a selected mini-batch of clipped gradients. Compute the exact worst-case clipped value\n$$\nR^{\\star}_{\\tau}=\\max_{\\substack{B\\subset\\{\\phi(g):g\\in\\{-4,-1,0,0,1,5\\}\\}\\\\|B|=3}}(\\hat{g}_{\\tau}(B)-\\bar{g}_{\\tau})^{2}.\n$$\n\nReport your final answer as a single row matrix containing $R^{\\star}$ and $R^{\\star}_{\\tau}$, in exact form. No rounding is required.",
            "solution": "The problem requires calculating the worst-case squared deviation of a mini-batch gradient estimator from the true full-data gradient, both with and without gradient clipping.\n\n**1. Unclipped Case ($R^{\\star}$)**\n\nThe dataset of per-sample gradients is $S = \\{-4, -1, 0, 0, 1, 5\\}$.\nThe full-data gradient, $\\bar{g}$, is the arithmetic mean of these values:\n$$\n\\bar{g} = \\frac{1}{6} (-4 - 1 + 0 + 0 + 1 + 5) = \\frac{1}{6}\n$$\nThe mini-batch gradient $\\hat{g}(B)$ is the mean of a mini-batch $B$ of size $b=3$. To maximize the squared deviation $(\\hat{g}(B) - \\bar{g})^2$, we must find the mini-batch $B$ that makes its mean $\\hat{g}(B)$ as far as possible from $\\bar{g}$. This occurs when the sum of elements in $B$ is either maximized or minimized.\n\n-   **Maximum mean**: Select the 3 largest values $\\{0, 1, 5\\}$. The sum is $6$. The mean is $\\hat{g}_{max} = \\frac{6}{3} = 2$.\n-   **Minimum mean**: Select the 3 smallest values $\\{-4, -1, 0\\}$. The sum is $-5$. The mean is $\\hat{g}_{min} = \\frac{-5}{3}$.\n\nNow we find the squared deviations from $\\bar{g} = 1/6$:\n-   For $\\hat{g}_{max}$: $(2 - \\frac{1}{6})^2 = (\\frac{11}{6})^2 = \\frac{121}{36}$.\n-   For $\\hat{g}_{min}$: $(-\\frac{5}{3} - \\frac{1}{6})^2 = (-\\frac{10}{6} - \\frac{1}{6})^2 = (-\\frac{11}{6})^2 = \\frac{121}{36}$.\n\nThe worst-case squared deviation is therefore $R^{\\star} = \\frac{121}{36}$.\n\n**2. Clipped Case ($R^{\\star}_{\\tau}$)**\n\nThe clipping function is $\\phi(g)=\\max\\{-3,\\min\\{g,3\\}\\}$. Applying this to the dataset $S$:\n$\\phi(-4) = -3$, $\\phi(-1) = -1$, $\\phi(0)=0$, $\\phi(1)=1$, $\\phi(5)=3$.\nThe clipped dataset is $S_{\\tau} = \\{-3, -1, 0, 0, 1, 3\\}$.\nThe mean of the clipped gradients, $\\bar{g}_{\\tau}$, is:\n$$\n\\bar{g}_{\\tau} = \\frac{1}{6}(-3 - 1 + 0 + 0 + 1 + 3) = \\frac{0}{6} = 0\n$$\nWe seek to maximize $(\\hat{g}_{\\tau}(B) - \\bar{g}_{\\tau})^2 = (\\hat{g}_{\\tau}(B))^2$ over mini-batches of size 3 from $S_{\\tau}$. This is equivalent to maximizing $|\\hat{g}_{\\tau}(B)|$.\n\n-   **Maximum mean**: Select $\\{0, 1, 3\\}$. The sum is $4$. The mean is $\\hat{g}_{\\tau,max} = \\frac{4}{3}$.\n-   **Minimum mean**: Select $\\{-3, -1, 0\\}$. The sum is $-4$. The mean is $\\hat{g}_{\\tau,min} = -\\frac{4}{3}$.\n\nThe maximum absolute mean is $|\\pm \\frac{4}{3}| = \\frac{4}{3}$. The worst-case clipped squared deviation is:\n$$\nR^{\\star}_{\\tau} = \\left(\\frac{4}{3}\\right)^2 = \\frac{16}{9}\n$$\nThe final result is the matrix of $R^{\\star}$ and $R^{\\star}_{\\tau}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{121}{36} & \\frac{16}{9}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While gradient clipping offers a robust guardrail, we can often do better than simply treating all data points equally. If we have information about the reliability, or variance, of individual gradient samples, we can construct a more precise estimator by giving more weight to the more reliable samples. This practice will guide you through the formal optimization process of deriving these optimal weights, revealing the powerful principle of inverse-variance weighting that is fundamental to signal processing and advanced optimization .",
            "id": "3150573",
            "problem": "Consider Stochastic Gradient Descent (SGD) in the mini-batch setting for a single-parameter model. Let a mini-batch be denoted by the index set $B=\\{1,2,3\\}$, and for each $i \\in B$ let $X_i$ be a scalar stochastic gradient estimate that satisfies $\\mathbb{E}[X_i]=\\mu$ (unbiasedness) and $\\operatorname{Var}(X_i)=\\sigma_i^{2}$. Assume the random variables $X_i$ are mutually independent. A weighted mini-batch gradient estimator is formed as $\\widehat{g}=\\sum_{i \\in B} w_i X_i$, where the per-sample weights satisfy the constraint $\\sum_{i \\in B} w_i=1$ to preserve unbiasedness, i.e., $\\mathbb{E}[\\widehat{g}]=\\mu$. \n\nStarting from the definitions of expectation and variance and using only standard properties of independence and linearity, derive the weights that minimize the variance $\\operatorname{Var}(\\widehat{g})$ subject to $\\sum_{i \\in B} w_i=1$. Then, evaluate the optimal weights for a mini-batch with variances $\\sigma_1^2=1$, $\\sigma_2^2=4$, and $\\sigma_3^2=9$. \n\nExpress your final answer as a row vector of the three optimal weights in the order $(w_1, w_2, w_3)$. No rounding is required.",
            "solution": "The problem is to find weights $(w_1, w_2, w_3)$ that minimize the variance of the weighted estimator $\\widehat{g} = \\sum_{i=1}^3 w_i X_i$, subject to the constraint $\\sum_{i=1}^3 w_i = 1$.\n\nGiven that the individual estimators $X_i$ are independent, the variance of the weighted sum is the weighted sum of their variances:\n$$\n\\operatorname{Var}(\\widehat{g}) = \\operatorname{Var}\\left(\\sum_{i=1}^3 w_i X_i\\right) = \\sum_{i=1}^3 w_i^2 \\operatorname{Var}(X_i) = \\sum_{i=1}^3 w_i^2 \\sigma_i^2\n$$\nWe must minimize this objective function subject to the linear constraint. This is a classic constrained optimization problem, solvable with Lagrange multipliers. The Lagrangian is:\n$$\n\\mathcal{L}(w_1, w_2, w_3, \\lambda) = \\left(\\sum_{i=1}^3 w_i^2 \\sigma_i^2\\right) - \\lambda\\left(\\sum_{i=1}^3 w_i - 1\\right)\n$$\nTo find the minimum, we set the partial derivative with respect to each $w_i$ to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w_i} = 2 w_i \\sigma_i^2 - \\lambda = 0 \\implies w_i = \\frac{\\lambda}{2 \\sigma_i^2}\n$$\nThis shows the optimal weights are inversely proportional to their corresponding variances. We substitute this result into the constraint equation to find $\\lambda$:\n$$\n\\sum_{i=1}^3 \\frac{\\lambda}{2 \\sigma_i^2} = 1 \\implies \\lambda = \\frac{2}{\\sum_{j=1}^3 1/\\sigma_j^2}\n$$\nSubstituting $\\lambda$ back into the expression for $w_i$ yields the general formula for inverse-variance weighting:\n$$\nw_i = \\frac{1/\\sigma_i^2}{\\sum_{j=1}^3 1/\\sigma_j^2}\n$$\nNow, we use the given variances $\\sigma_1^2=1$, $\\sigma_2^2=4$, and $\\sigma_3^2=9$. First, we compute the sum of the inverse variances:\n$$\n\\sum_{j=1}^3 \\frac{1}{\\sigma_j^2} = \\frac{1}{1} + \\frac{1}{4} + \\frac{1}{9} = \\frac{36}{36} + \\frac{9}{36} + \\frac{4}{36} = \\frac{49}{36}\n$$\nFinally, we compute each optimal weight:\n$$\nw_1 = \\frac{1/1}{49/36} = \\frac{36}{49}\n$$\n$$\nw_2 = \\frac{1/4}{49/36} = \\frac{1}{4} \\cdot \\frac{36}{49} = \\frac{9}{49}\n$$\n$$\nw_3 = \\frac{1/9}{49/36} = \\frac{1}{9} \\cdot \\frac{36}{49} = \\frac{4}{49}\n$$\nThe optimal weights are $(w_1, w_2, w_3) = (\\frac{36}{49}, \\frac{9}{49}, \\frac{4}{49})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{36}{49} & \\frac{9}{49} & \\frac{4}{49}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from statistical theory to practical implementation, we often face hardware constraints, most notably memory limits. When our desired batch size exceeds what our machine can handle at once, a common strategy is to process smaller \"micro-batches\" and accumulate their gradients. While this solves the memory issue, this practice will guide you through an analysis of a subtle but critical side effect: the accumulation of numerical rounding errors in finite-precision arithmetic, revealing a hidden trade-off in large-scale model training .",
            "id": "3150664",
            "problem": "You are minimizing an empirical risk with Stochastic Gradient Descent (SGD), estimating the gradient at each iteration by an average over a mini-batch of size $B$. Due to a strict memory limit, you can only process at most $b$ samples at once, with $b \\ll B$. To emulate the $B$-sample gradient, you adopt micro-batching with gradient accumulation: you split the $B$ samples into $m$ micro-batches of size $b$, where $m = B/b$ is an integer, compute each micro-batch average gradient, and then average those $m$ micro-averages.\n\nAssume arithmetic is performed in a floating-point system with rounding to nearest and unit roundoff $u$, following the standard model that each elementary floating-point operation introduces a multiplicative error factor of the form $(1+\\delta)$ with $|\\delta| \\leq u$. Focus on a single fixed component of the gradient vector at a given iteration, and assume all $B$ exact component values being averaged are nonnegative real numbers bounded above by a constant $G$, and that the exact batch mean of this component is strictly positive. Ignore stochastic sampling variance and treat the inputs as exact reals prior to floating-point rounding.\n\nConsider two schemes for computing the batch mean of this component:\n- Scheme A (hypothetical direct $B$-batch): compute the mean by naively summing $B$ values in sequence and then multiplying by $1/B$ once.\n- Scheme M (micro-batching with accumulation): for each of the $m$ micro-batches, naively sum $b$ values in sequence and multiply by $1/b$ once to form a micro-mean; then naively sum the $m$ micro-means in sequence and multiply by $1/m$ once to form the final mean.\n\nAssume naive summation order as written, and no compensated summation or special fused operations are used. Work to first order in $u$ (i.e., neglect terms of order $u^{2}$ and higher).\n\nLet $E_{A}$ denote a first-order worst-case bound on the relative rounding error in the mean computed by Scheme A, and let $E_{M}$ denote the analogous bound for Scheme M. What is the ratio $R = E_{M}/E_{A}$, expressed in closed form in terms of $b$ (and constants), under the stated assumptions? Your final answer must be a single closed-form analytic expression. No rounding is required.",
            "solution": "This problem requires a first-order analysis of rounding errors. We use the standard result that for non-negative inputs, computing a mean of $N$ values via naive summation followed by one division has a worst-case relative error bound of $Nu$, where $u$ is the unit roundoff.\n\n**Scheme A: Direct B-batch Mean**\nThis scheme computes the mean of $B$ samples directly. Applying the standard result with $N=B$, the worst-case relative error bound is:\n$$\nE_A = Bu\n$$\n\n**Scheme M: Micro-batching with Accumulation**\nThis is a two-stage process. The total error is the sum of the error propagated from the first stage and the new error generated in the second.\n\n1.  **Stage 1: Computing micro-means.** For each of the $m$ micro-batches, a mean of $b$ samples is computed. Applying the standard result with $N=b$, the worst-case relative error for each micro-mean is bounded by $bu$. This is the error that propagates to the next stage.\n2.  **Stage 2: Averaging the micro-means.** The final mean is computed by averaging the $m$ micro-means from the previous stage. New rounding error is generated in this process. Applying the standard result with $N=m$, the generated relative error is bounded by $mu$.\n\nThe total first-order worst-case relative error for Scheme M is the sum of the propagated and generated error bounds:\n$$\nE_M = (\\text{propagated error bound}) + (\\text{generated error bound}) = bu + mu\n$$\n\n**Calculating the Ratio R**\nThe ratio of the worst-case error bounds is:\n$$\nR = \\frac{E_M}{E_A} = \\frac{(b+m)u}{Bu} = \\frac{b+m}{B}\n$$\nUsing the given relation $m = B/b$, we substitute for $m$:\n$$\nR = \\frac{b + (B/b)}{B} = \\frac{b}{B} + \\frac{B/b}{B} = \\frac{b}{B} + \\frac{1}{b}\n$$",
            "answer": "$$\\boxed{\\frac{b}{B} + \\frac{1}{b}}$$"
        }
    ]
}