{
    "hands_on_practices": [
        {
            "introduction": "在求解像 LASSO 这样的高维优化问题时，一个直观且有效的方法是坐标下降法。该方法将一个复杂的多变量问题分解为一系列简单的一维优化子问题，即每次只沿着一个坐标方向进行优化，同时固定其他所有坐标。这个练习将引导你从第一性原理出发，推导 LASSO 问题中单个坐标的精确更新规则，并揭示其解的核心结构——软阈值算子，从而为理解正则化问题的求解过程奠定坚实的基础。",
            "id": "3172091",
            "problem": "考虑最小绝对收缩和选择算子 (LASSO) 问题，其目标是最小化目标函数\n$$\nF(w) \\;=\\; \\frac{1}{2}\\,\\|y - X w\\|_{2}^{2} \\;+\\; \\lambda\\,\\|w\\|_{1},\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$ 是一个固定的数据矩阵，$y \\in \\mathbb{R}^{n}$ 是一个固定的响应向量，$w \\in \\mathbb{R}^{p}$ 是决策变量，$\\lambda > 0$ 是一个正则化参数。令 $X_{j} \\in \\mathbb{R}^{n}$ 表示 $X$ 的第 $j$ 列，$w_{j}$ 表示 $w$ 的第 $j$ 个分量。定义软阈值算子 $S_{\\alpha}(z)$ 为\n$$\nS_{\\alpha}(z) \\;=\\; \\operatorname{sign}(z)\\,\\max\\big(|z| - \\alpha,\\,0\\big),\n$$\n其中 $\\alpha \\ge 0$ 且 $z \\in \\mathbb{R}$。\n\n从 $F(w)$ 的定义和绝对值函数的次梯度出发，推导在保持所有其他坐标固定的情况下，对 $w_{j}$ 的精确坐标级最小化器。你的推导必须从通过分离坐标 $j$ 的贡献而获得的一维简化问题开始，并且只使用第一性原理（凸性和 $\\ell_1$ 范数的次梯度最优性）。然后，使用你推导出的更新规则，为以下实例计算一个具体的坐标下降更新：\n$$\nX \\;=\\; \\begin{pmatrix}\n2  -1  0 \\\\\n0  1  3\n\\end{pmatrix},\\qquad\ny \\;=\\; \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix},\\qquad\nw \\;=\\; \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.1 \\end{pmatrix},\\qquad\n\\lambda \\;=\\; 1.2,\n$$\n针对坐标 $j = 2$。请提供经过一次精确坐标级最小化步骤后 $w_{2}$ 的更新数值。\n\n最后，基于 $F(w)$ 的结构和坐标下降的性质，简要解释为什么循环坐标下降对于这个凸问题能够全局收敛，以及为什么在实践中 $X$ 的稀疏性可以加速收敛。你的解释应依赖于基本性质（凸性、$\\ell_1$ 惩罚项的可分离性、以及平滑部分的坐标级 Lipschitz 连续性），而不应引用未经证明的论断或快捷公式。\n\n将更新后的 $w_{2}$ 的最终数值答案表示为单个实数。如果需要四舍五入，请保留四位有效数字；否则，提供精确值。",
            "solution": "所述问题是有效的。它在科学上基于凸优化理论，对于指定的任务是适定的，并且具有唯一解，所有定义和数据都是完整和一致的。因此，我们可以开始求解。\n\n该问题需要一个三部分的回应：首先，推导 LASSO 目标函数的坐标级最小化器；其次，对该结果进行数值应用；第三，解释坐标下降法对此问题的收敛性质。\n\n**第一部分：坐标级最小化器的推导**\n\nLASSO 目标函数由下式给出\n$$\nF(w) = \\frac{1}{2}\\|y - X w\\|_{2}^{2} + \\lambda\\|w\\|_{1}\n$$\n其中 $w \\in \\mathbb{R}^{p}$。我们希望找到 $F(w)$ 关于单个坐标 $w_j$ 的最小化器，同时保持所有其他坐标 $w_k$ (其中 $k \\neq j$) 固定。\n\n我们可以将函数 $F(w)$ 分解为依赖于 $w_j$ 的项和相对于 $w_j$ 为常数的项。平滑项是 $\\|y - X w\\|_{2}^{2}$。我们可以如下分离 $X$ 的第 $j$ 列的贡献：\n$$\nXw = \\sum_{k=1}^{p} X_k w_k = X_j w_j + \\sum_{k \\neq j} X_k w_k\n$$\n我们定义部分残差向量 $r_{(-j)} \\in \\mathbb{R}^{n}$ 为\n$$\nr_{(-j)} = y - \\sum_{k \\neq j} X_k w_k\n$$\n将此代入目标函数，我们得到\n$$\nF(w) = \\frac{1}{2}\\|r_{(-j)} - X_j w_j\\|_{2}^{2} + \\lambda \\sum_{k=1}^{p} |w_k| = \\frac{1}{2}\\|r_{(-j)} - X_j w_j\\|_{2}^{2} + \\lambda |w_j| + \\lambda \\sum_{k \\neq j} |w_k|\n$$\n为了最小化 $F(w)$ 关于 $w_j$ 的值，我们可以忽略所有不依赖于 $w_j$ 的项。这给我们一个一维优化问题：\n$$\n\\min_{w_j \\in \\mathbb{R}} f_j(w_j) \\quad \\text{其中} \\quad f_j(w_j) = \\frac{1}{2}\\|r_{(-j)} - X_j w_j\\|_{2}^{2} + \\lambda|w_j|\n$$\n我们展开平方 $\\ell_2$ 范数：\n$$\n\\|r_{(-j)} - X_j w_j\\|_{2}^{2} = (r_{(-j)} - X_j w_j)^T(r_{(-j)} - X_j w_j) = \\|r_{(-j)}\\|_2^2 - 2 w_j X_j^T r_{(-j)} + w_j^2 \\|X_j\\|_2^2\n$$\n函数 $f_j(w_j)$ 在相差一个加法常数的情况下可以写成\n$$\nf_j(w_j) = \\frac{1}{2}\\|X_j\\|_2^2 w_j^2 - (X_j^T r_{(-j)}) w_j + \\lambda|w_j|\n$$\n该函数是凸函数，因为它是一个二次函数（假设 $X_j \\neq 0$，则为凸函数）与一个乘以 $\\lambda > 0$ 的绝对值函数（也是凸函数）的和。一个点 $w_j^*$ 是 $f_j(w_j)$ 的全局最小化点，当且仅当 $0$ 属于 $f_j$ 在 $w_j^*$ 处的次微分，记为 $\\partial f_j(w_j^*)$。\n\n$f_j(w_j)$ 的次微分由下式给出：\n$$\n\\partial f_j(w_j) = \\|X_j\\|_2^2 w_j - X_j^T r_{(-j)} + \\lambda \\partial|w_j|\n$$\n其中绝对值函数的次微分是\n$$\n\\partial|z| = \\begin{cases} \\{\\operatorname{sign}(z)\\}  \\text{if } z \\neq 0 \\\\ [-1, 1]  \\text{if } z = 0 \\end{cases}\n$$\n因此，最优性条件 $0 \\in \\partial f_j(w_j^*)$ 变为：\n$$\nX_j^T r_{(-j)} - \\|X_j\\|_2^2 w_j^* \\in \\lambda \\partial|w_j^*|\n$$\n我们根据 $w_j^*$ 的值分情况分析这个条件：\n1.  如果 $w_j^* > 0$，则 $\\partial|w_j^*| = \\{1\\}$。该条件变为 $X_j^T r_{(-j)} - \\|X_j\\|_2^2 w_j^* = \\lambda$，解得 $w_j^* = \\frac{X_j^T r_{(-j)} - \\lambda}{\\|X_j\\|_2^2}$。为了使该解与初始假设 $w_j^* > 0$ 一致，我们必须有 $X_j^T r_{(-j)} > \\lambda$。\n2.  如果 $w_j^*  0$，则 $\\partial|w_j^*| = \\{-1\\}$。该条件变为 $X_j^T r_{(-j)} - \\|X_j\\|_2^2 w_j^* = -\\lambda$，解得 $w_j^* = \\frac{X_j^T r_{(-j)} + \\lambda}{\\|X_j\\|_2^2}$。为了与 $w_j^*  0$ 一致，我们必须有 $X_j^T r_{(-j)}  -\\lambda$。\n3.  如果 $w_j^* = 0$，则 $\\partial|w_j^*| = [-1, 1]$。该条件变为 $X_j^T r_{(-j)} \\in [-\\lambda, \\lambda]$，即 $|X_j^T r_{(-j)}| \\le \\lambda$。\n\n让我们整合这些结果。令 $z = X_j^T r_{(-j)}$ 和 $\\alpha = \\|X_j\\|_2^2$。最优的 $w_j^*$ 是：\n$$\nw_j^* = \\begin{cases} (z - \\lambda)/\\alpha  \\text{if } z  \\lambda \\\\ (z + \\lambda)/\\alpha  \\text{if } z  -\\lambda \\\\ 0  \\text{if } |z| \\le \\lambda \\end{cases}\n$$\n这个表达式恰好是软阈值算子 $S_{\\lambda/\\alpha}(z/\\alpha)$。我们来验证一下：\n$$\nS_{\\lambda/\\alpha}\\left(\\frac{z}{\\alpha}\\right) = \\operatorname{sign}\\left(\\frac{z}{\\alpha}\\right) \\max\\left(\\left|\\frac{z}{\\alpha}\\right| - \\frac{\\lambda}{\\alpha}, 0\\right) = \\frac{1}{\\alpha} \\operatorname{sign}(z) \\max\\left(|z| - \\lambda, 0\\right)\n$$\n- 如果 $z  \\lambda$，表达式给出 $\\frac{1}{\\alpha}(1)(z - \\lambda) = (z-\\lambda)/\\alpha$。\n- 如果 $z  -\\lambda$，表达式给出 $\\frac{1}{\\alpha}(-1)(-z - \\lambda) = (z+\\lambda)/\\alpha$。\n- 如果 $|z| \\le \\lambda$，表达式给出 $\\frac{1}{\\alpha}\\operatorname{sign}(z)(0) = 0$。\n推导完成。$w_j$ 的精确坐标级最小化器由 $w_j^* = S_{\\lambda/\\|X_j\\|_2^2}\\left(\\frac{X_j^T r_{(-j)}}{\\|X_j\\|_2^2}\\right)$ 给出。\n\n**第二部分：数值计算**\n\n我们给定以下实例：\n$$\nX = \\begin{pmatrix} 2  -1  0 \\\\ 0  1  3 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}, \\quad w_{\\text{initial}} = \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.1 \\end{pmatrix}, \\quad \\lambda = 1.2\n$$\n我们需要更新坐标 $j=2$，它对应于 $w_2$。其他坐标固定在它们当前的值上：$w_1 = 0.5$ 和 $w_3 = 0.1$。\n\n首先，我们确定 $X$ 的相关列：$X_2 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$。\n该列的平方范数为 $\\|X_2\\|_2^2 = (-1)^2 + 1^2 = 2$。\n\n接下来，我们计算部分残差 $r_{(-2)}$：\n$$\nr_{(-2)} = y - X_1 w_1 - X_3 w_3 = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}(0.5) - \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}(0.1)\n$$\n$$\nr_{(-2)} = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 1 - 1 - 0 \\\\ 4 - 0 - 0.3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3.7 \\end{pmatrix}\n$$\n现在，我们计算项 $X_2^T r_{(-2)}$：\n$$\nX_2^T r_{(-2)} = \\begin{pmatrix} -1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3.7 \\end{pmatrix} = (-1)(0) + (1)(3.7) = 3.7\n$$\n现在我们可以应用软阈值算子。更新后的值 $w_2^{\\text{new}}$ 是\n$$\nw_2^{\\text{new}} = S_{\\lambda/\\|X_2\\|_2^2}\\left(\\frac{X_2^T r_{(-2)}}{\\|X_2\\|_2^2}\\right)\n$$\n阈值参数是 $\\lambda/\\|X_2\\|_2^2 = 1.2/2 = 0.6$。算子的参数是 $X_2^T r_{(-2)}/\\|X_2\\|_2^2 = 3.7/2 = 1.85$。\n所以，我们计算：\n$$\nw_2^{\\text{new}} = S_{0.6}(1.85)\n$$\n使用定义 $S_{\\alpha}(z) = \\operatorname{sign}(z) \\max(|z|-\\alpha, 0)$：\n$$\nw_2^{\\text{new}} = \\operatorname{sign}(1.85) \\max(|1.85| - 0.6, 0) = 1 \\cdot \\max(1.85 - 0.6, 0) = \\max(1.25, 0) = 1.25\n$$\n$w_2$ 的更新值恰好是 $1.25$。\n\n**第三部分：收敛性解释**\n\n循环坐标下降法对 LASSO 问题的全局收敛性取决于目标函数 $F(w)$ 的特定结构。\n函数 $F(w) = \\frac{1}{2}\\|y - Xw\\|_2^2 + \\lambda \\|w\\|_1$ 是两个分量的和：\n1.  一个平滑、可微的凸函数 $f(w) = \\frac{1}{2}\\|y - Xw\\|_2^2$。其梯度为 $\\nabla f(w) = X^T(Xw - y)$。关于单个坐标的梯度 $\\nabla_j f(w) = X_j^T(Xw - y)$ 是 Lipschitz 连续的，其常数为 $L_j = \\|X_j\\|_2^2$。\n2.  一个非平滑的凸函数 $g(w) = \\lambda \\|w\\|_1$。关键在于，这个函数是**可分离的**，意味着它可以写成关于单个坐标的函数之和：$g(w) = \\sum_{j=1}^p \\lambda|w_j|$。\n\n这些性质的结合——一个由具有坐标级 Lipschitz 梯度的平滑部分和一个可分离的非平滑部分组成的凸目标函数——足以保证循环坐标下降法收敛到全局最小值。在每一步中，算法对一个坐标执行精确最小化。因为函数是凸的，任何局部最小值都是全局最小值。由坐标下降生成的迭代序列 $\\{w^{(k)}\\}$ 保证有极限点，并且任何这样的极限点都是 $F(w)$ 的全局最小化点。这是非平滑凸优化理论中的一个经典结果。\n\n数据矩阵 $X$ 的稀疏性可以在计算时间上显著加速收敛。对 $w_j$ 的每次坐标更新中的核心计算是项 $X_j^T r_{(-j)}$。这可以计算为 $X_j^T y - \\sum_{k \\neq j} (X_j^T X_k) w_k$。如果 $X$ 是稀疏的，它的许多列 $X_k$ 将是稀疏向量。内积 $X_j^T X_k$ 的计算可以快得多，因为所需的乘法次数与两个向量都具有非零元素的位置数量成正比，而不是它们的完整维度 $n$。在许多情况下，这个内积将恰好为零。这降低了求和的成本。另外，如果维护完整的残差 $r=y-Xw$，则更新 $w_j \\to w_j^{\\text{new}}$ 需要更新残差 $r \\to r - X_j (w_j^{\\text{new}} - w_j^{\\text{old}})$。如果向量 $X_j$ 是稀疏的（几乎没有非零项），这个更新操作的计算成本很低。因此，坐标下降的每一步完成得更快，从而导致达到收敛的总时间更快。",
            "answer": "$$\n\\boxed{1.25}\n$$"
        },
        {
            "introduction": "在坐标下降法之外，近端梯度法（proximal gradient methods）为求解如 LASSO 这类复合优化问题提供了另一套强大的框架。迭代软阈值算法（ISTA）是该方法族的典型代表，它将光滑部分的梯度下降步骤与非光滑部分的近端映射（在此即为软阈值化）相结合。这个练习要求你动手实现带回溯线搜索的 ISTA 算法，这不仅能让你掌握一种核心的一阶优化算法，还能让你理解如何通过自适应步长选择来保证算法的收敛性，将理论与实际应用紧密联系起来。",
            "id": "3172033",
            "problem": "考虑一个复合优化问题，其包含一个 $\\ell_2$ 数据保真项和一个 $\\ell_1$ 正则化项。设 $A \\in \\mathbb{R}^{m \\times n}$ 且 $b \\in \\mathbb{R}^{m}$。定义光滑凸函数 $g:\\mathbb{R}^{n}\\to\\mathbb{R}$ 为 $g(w) = \\frac{1}{2}\\lVert A w - b\\rVert_2^2$，以及完整的目标函数 $\\phi:\\mathbb{R}^{n}\\to\\mathbb{R}$ 为 $\\phi(w) = g(w) + \\lambda \\lVert w\\rVert_1$，其中 $\\lambda  0$ 且 $\\lVert \\cdot \\rVert_1$ 表示 $\\ell_1$ 范数。梯度 $\\nabla g$ 是利普希茨连续的，其利普希茨常数为 $L$，即对于所有 $u,w \\in \\mathbb{R}^n$，满足 $g(u) \\le g(w) + \\langle \\nabla g(w), u-w \\rangle + \\frac{L}{2}\\lVert u-w \\rVert_2^2$。\n\n您的任务是为这个复合目标函数实现带有回溯线搜索（Backtracking Line Search, BLS）的迭代软阈值算法（Iterative Soft-Thresholding Algorithm, ISTA）。在第 $k$ 次迭代时，给定当前迭代点 $w^k$ 和当前步长 $\\eta  0$，定义二次代理函数\n$$\nQ_{\\eta}(w^k, u) = g(w^k) + \\langle \\nabla g(w^k), u - w^k \\rangle + \\frac{1}{2\\eta}\\lVert u - w^k \\rVert_2^2 + \\lambda \\lVert u \\rVert_1.\n$$\n回溯线搜索过程如下：从给定的 $\\eta$ 开始，构造候选点 $u$ 作为 $Q_{\\eta}(w^k, \\cdot)$ 的唯一最小化子，并检查接受准则 $\\phi(u) \\le Q_{\\eta}(w^k, u)$。如果准则不满足，则以 $0  \\beta  1$ 缩小步长 $\\eta \\leftarrow \\beta \\eta$，为新的 $\\eta$ 重新计算 $u$，并重复此过程直至准则成立。然后设置 $w^{k+1} \\leftarrow u$ 并继续。该机制与估计 $g$ 的利普希茨常数（LC）$L$ 相关，因为只要接受准则被满足，它就保证了 $\\frac{1}{\\eta}$ 是 $L$ 的一个经验上界。\n\n在一个独立的程序中实现上述过程。使用以下基本定义和事实：\n- 对于 $g(w) = \\frac{1}{2}\\lVert A w - b\\rVert_2^2$，其梯度 $\\nabla g(w)$ 为 $\\nabla g(w) = A^\\top(Aw - b)$。\n- 与 $\\ell_1$ 范数相关的邻近映射（proximal mapping）用于获得 $Q_{\\eta}(w^k, \\cdot)$ 的最小化子。\n- $\\nabla g$ 的真实利普希茨常数 $L$ 等于 $A^\\top A$ 的最大特征值，也就是 $A$ 的最大奇异值的平方。\n\n算法在 $w^0 = 0$ 处初始化。在所有测试用例中，使用收缩因子 $\\beta = 0.5$ 并运行 $K = 60$ 次迭代。为了进行数值比较，定义经验上界 $\\hat{L}$ 为在所有迭代中接受的步长所对应的 $\\frac{1}{\\eta}$ 的最大值。令 $L_{\\text{true}}$ 为从 $A$ 计算出的真实利普希茨常数。对于每个测试用例，报告绝对差 $\\lvert \\hat{L} - L_{\\text{true}} \\rvert$。\n\n测试套件：\n实现四个测试用例，每个用例指定 $(m,n,\\text{seed},s,\\lambda,\\eta_{\\text{init}})$，其中 $s$ 是应用于 $A$ 的一个缩放因子：\n- 用例 1：$(m,n,\\text{seed},s,\\lambda,\\eta_{\\text{init}}) = (40, 20, 17, 1.0, 0.05, \\eta_{\\text{init}})$，其中 $\\eta_{\\text{init}}$ 对于生成的 $A$ 选择为 $\\eta_{\\text{init}} = \\frac{0.9}{L_{\\text{true}}}$。\n- 用例 2：$(40, 20, 23, 1.0, 0.05, 10.0)$。\n- 用例 3：$(50, 30, 42, 30.0, 0.10, 1.0)$。\n- 用例 4：$(50, 30, 99, 0.0, 1.00, 1.0)$，其中由于 $s=0.0$， $A$ 是零矩阵。\n\n数据生成：\n- 对于每个用例，生成具有独立标准正态分布条目的矩阵 $A$，然后按 $s$ 进行缩放（即，设置 $A \\leftarrow s \\cdot A$）。\n- 生成具有独立标准正态分布条目的向量 $b$。\n- 使用给定的 $\\text{seed}$ 初始化伪随机数生成器以确保可复现性。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $[r_1,r_2,r_3,r_4]$），其中每个 $r_i$ 是上述第 $i$ 个测试用例的浮点数 $\\lvert \\hat{L} - L_{\\text{true}} \\rvert$。不应打印任何其他文本。此问题不使用角度和物理单位，因此不需要提供。",
            "solution": "该问题要求实现带有回溯线搜索（BLS）的迭代软阈值算法（ISTA），以解决复合凸优化问题：\n$$\n\\min_{w \\in \\mathbb{R}^n} \\phi(w) \\quad \\text{where} \\quad \\phi(w) = g(w) + \\lambda \\lVert w \\rVert_1.\n$$\n函数 $g(w)$ 是光滑的凸数据保真项，定义为 $g(w) = \\frac{1}{2}\\lVert Aw - b \\rVert_2^2$。第二项是 $\\ell_1$ 范数正则化项，由参数 $\\lambda  0$ 加权。\n\nISTA的核心是邻近梯度法。在每次迭代 $k$ 中，通过对光滑部分 $g(w)$ 进行梯度下降步，然后应用非光滑部分的邻近算子，来计算下一个迭代点 $w^{k+1}$。更新规则是：\n$$\nw^{k+1} = \\text{prox}_{\\eta_k \\lambda, \\lVert\\cdot\\rVert_1}(w^k - \\eta_k \\nabla g(w^k)),\n$$\n其中 $\\eta_k  0$ 是第 $k$ 次迭代的步长。$g(w)$ 的梯度由 $\\nabla g(w) = A^\\top(Aw - b)$ 给出。\n\n由因子 $\\alpha  0$ 缩放的 $\\ell_1$ 范数的邻近算子是软阈值算子 $S_{\\alpha}$。对于向量 $z \\in \\mathbb{R}^n$，它是按分量应用的：\n$$\n(\\text{prox}_{\\alpha, \\lVert\\cdot\\rVert_1}(z))_i = S_{\\alpha}(z_i) = \\text{sign}(z_i) \\max(|z_i| - \\alpha, 0).\n$$\n在我们的例子中，$\\alpha = \\eta_k \\lambda$。因此，候选更新 $u$ 是通过最小化二次代理函数 $Q_{\\eta}(w^k, u)$ 找到的，这可以被证明等价于邻近梯度步：\n$$\nu = \\arg\\min_{u'} Q_{\\eta}(w^k, u') = \\arg\\min_{u'} \\left( \\frac{1}{2\\eta} \\lVert u' - (w^k - \\eta \\nabla g(w^k)) \\rVert_2^2 + \\lambda \\lVert u' \\rVert_1 \\right) = \\text{prox}_{\\eta\\lambda, \\lVert\\cdot\\rVert_1}(w^k - \\eta \\nabla g(w^k)).\n$$\n\n算法的一个关键部分是选择步长 $\\eta_k$。如果 $\\eta_k  1/L$（其中 $L$ 是 $\\nabla g(w)$ 的利普希茨常数），则收敛性得到保证。回溯线搜索（BLS）是一种在每次迭代中无需预先知道 $L$ 即可找到合适 $\\eta_k$ 的策略。该方法过程如下：\n\n在每次迭代 $k$ 中，从一个初始步长猜测 $\\eta$ 开始：\n1. 计算候选点 $u = \\text{prox}_{\\eta\\lambda, \\lVert\\cdot\\rVert_1}(w^k - \\eta \\nabla g(w^k))$。\n2. 检查接受准则：$\\phi(u) \\le Q_{\\eta}(w^k, u)$。代入 $\\phi$ 和 $Q_{\\eta}$ 的定义，该不等式简化为：\n$$\ng(u) \\le g(w^k) + \\langle \\nabla g(w^k), u - w^k \\rangle + \\frac{1}{2\\eta} \\lVert u - w^k \\rVert_2^2.\n$$\n这个条件确保了目标函数有足够的下降。\n3. 如果准则满足，则接受步长 $\\eta_k = \\eta$，并执行更新：$w^{k+1} = u$。\n4. 如果准则不满足，则减小步长 $\\eta \\leftarrow \\beta \\eta$（其中 $0  \\beta  1$），并重复步骤1-2，直到条件满足为止。\n\n问题指定了初始迭代点 $w^0=0$、收缩因子 $\\beta=0.5$ 以及总共 $K=60$ 次迭代。对于每次迭代，BLS 从给定的初始步长 $\\eta_{\\text{init}}$ 开始。\n\n$\\nabla g(w)$ 的真实利普希茨常数，表示为 $L_{\\text{true}}$，是其海森矩阵 $\\nabla^2 g(w) = A^\\top A$ 的最大特征值。这等价于矩阵 $A$ 的最大奇异值的平方：\n$$\nL_{\\text{true}} = \\lambda_{\\max}(A^\\top A) = \\sigma_{\\max}(A)^2.\n$$\n这可以使用奇异值分解（SVD）计算。\n\n接受准则确保了 $1/\\eta_k$ 作为 $g(w)$ 曲率的局部上界。利普希茨常数的经验上界 $\\hat{L}$ 定义为在所有迭代中这些上界的最大值：\n$$\n\\hat{L} = \\max_{k=0, \\dots, K-1} \\left\\{ \\frac{1}{\\eta_k} \\right\\},\n$$\n其中 $\\eta_k$ 是第 $k$ 次迭代时接受的步长。最终任务是为每个测试用例计算绝对差 $|\\hat{L} - L_{\\text{true}}|$。\n\n对于缩放因子 $s=0$ 的特殊情况，矩阵 $A$ 变为零矩阵。因此，$g(w) = \\frac{1}{2}\\lVert b \\rVert_2^2$ 是一个常数，其梯度 $\\nabla g(w)$ 始终是零向量，其真实利普希茨常数 $L_{\\text{true}} = 0$。算法的行为变得简化，但过程仍然是良定义的。\n\n实现将按以下步骤进行：设置每个测试用例，根据指定的种子和缩放因子生成数据 $A$ 和 $b$，计算 $L_{\\text{true}}$，然后执行带有 BLS 的 ISTA，共进行 $K=60$ 次迭代。在执行期间，记录接受的步长 $\\eta_k$，以便在最后计算 $\\hat{L}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the Iterative Soft-Thresholding Algorithm with Backtracking\n    Line Search for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        # (m, n, seed, s, lambda, eta_init)\n        (40, 20, 17, 1.0, 0.05, 'dynamic'),\n        (40, 20, 23, 1.0, 0.05, 10.0),\n        (50, 30, 42, 30.0, 0.10, 1.0),\n        (50, 30, 99, 0.0, 1.00, 1.0),\n    ]\n\n    results = []\n\n    def soft_threshold(z, alpha):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - alpha, 0)\n\n    def g(A, b, w):\n        \"\"\"Computes the value of the smooth function g(w).\"\"\"\n        return 0.5 * np.linalg.norm(A @ w - b)**2\n\n    def grad_g(A, b, w):\n        \"\"\"Computes the gradient of g(w).\"\"\"\n        return A.T @ (A @ w - b)\n\n    for case_params in test_cases:\n        m, n, seed, s, lam, eta_init_val = case_params\n\n        # Data generation\n        rng = np.random.default_rng(seed)\n        A_raw = rng.standard_normal((m, n))\n        b = rng.standard_normal(m)\n        A = s * A_raw\n\n        # Calculate true Lipschitz constant\n        if s == 0.0:\n            L_true = 0.0\n        else:\n            # L_true = sigma_max(A)^2\n            singular_values = np.linalg.svd(A, compute_uv=False)\n            L_true = singular_values[0]**2\n        \n        # Determine initial step size eta_init\n        if eta_init_val == 'dynamic':\n            # Case 1 specific initialization\n            # As s=1.0, A is not the zero matrix, so L_true > 0 with probability 1\n            eta_init = 0.9 / L_true\n        else:\n            eta_init = eta_init_val\n\n        # Algorithm parameters\n        K = 60\n        beta = 0.5\n        w = np.zeros(n)\n        \n        one_over_etas = []\n\n        # Main ISTA loop\n        for _ in range(K):\n            current_grad_g = grad_g(A, b, w)\n            current_g_w = g(A, b, w)\n            \n            eta = eta_init\n\n            # Backtracking Line Search loop\n            while True:\n                # Proximal gradient step\n                z = w - eta * current_grad_g\n                u = soft_threshold(z, eta * lam)\n                \n                # Check acceptance criterion\n                g_u = g(A, b, u)\n                \n                # RHS of the acceptance criterion inequality\n                surrogate_rhs = current_g_w + np.dot(current_grad_g, u - w) + (0.5 / eta) * np.linalg.norm(u - w)**2\n                \n                if g_u = surrogate_rhs:\n                    break  # Step size accepted\n                else:\n                    eta *= beta # Shrink step size\n            \n            # Store the accepted 1/eta\n            if eta > 0: # Avoid division by zero if eta somehow becomes zero\n                one_over_etas.append(1.0 / eta)\n            \n            # Update w\n            w = u\n            \n        # Calculate empirical Lipschitz constant\n        if not one_over_etas:\n            # This case happens if K=0, but given K=60, the list will not be empty.\n            L_hat = 0.0\n        else:\n            L_hat = max(one_over_etas)\n            \n        # Calculate and store the absolute difference\n        diff = abs(L_hat - L_true)\n        results.append(diff)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "作为一种更为先进和通用的优化工具，交替方向乘子法（ADMM）能够高效地求解各类含约束或复杂正则项的问题。通过引入“分裂”变量，ADMM 将原问题分解为多个更容易处理的子问题，并通过对偶上升来协调它们的解。本练习将指导你为 LASSO 问题推导并实现 ADMM 算法，让你亲身体验如何将一个问题重构为适合 ADMM 的形式，并掌握这一在机器学习和信号处理领域应用广泛的强大算法。",
            "id": "3172064",
            "problem": "要求您推导并实现用于最小绝对收缩和选择算子 (Lasso) 的交替方向乘子法 (ADMM)，其中使用分裂变量 $z$ 和等式约束 $w=z$。从约束公式以及凸优化和增广拉格朗日方法的基本定义开始。仅基于第一性原理推导变量 $w$、$z$ 和缩放对偶变量 $u$ 的显式更新公式，这些原理包括：等式约束凸问题的增广拉格朗日量的定义、可微子问题的平稳性条件，以及 $\\ell_{1}$-范数的近端算子定义。然后，实现您推导出的算法，并在提供的测试套件上进行评估。\n\n该优化问题是最小化目标函数\n$$\n\\frac{1}{2}\\lVert y - X w \\rVert_{2}^{2} + \\lambda \\lVert z \\rVert_{1}\n$$\n服从约束条件\n$$\nw = z,\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$ 是一个数据矩阵，$y \\in \\mathbb{R}^{n}$ 是一个响应向量，$w \\in \\mathbb{R}^{p}$ 是系数向量，$\\lambda \\in \\mathbb{R}_{+}$ 是正则化参数，$\\lVert \\cdot \\rVert_{1}$ 和 $\\lVert \\cdot \\rVert_{2}$ 分别表示 $\\ell_{1}$-范数和欧几里得范数。\n\n要求：\n- 从增广拉格朗日量开始，推导 $w$、$z$ 和缩放对偶变量 $u$ 的 ADMM 更新。您的推导必须仅使用等式约束的增广拉格朗日量的核心定义以及凸函数和近端算子的性质。不要使用预先记忆的快捷公式。\n- 实现算法，其停止标准基于原始残差和对偶残差的范数，并与绝对和相对容差进行比较。\n\n数值实现细节：\n- 使用带有惩罚参数 $\\rho \\in \\mathbb{R}_{+}$ 的缩放形式 ADMM。\n- 对原始残差 $r^{k} = w^{k} - z^{k}$ 和对偶残差 $s^{k} = \\rho (z^{k} - z^{k-1})$ 使用停止标准：\n  - 当 $\\lVert r^{k} \\rVert_{2} \\le \\varepsilon_{\\mathrm{pri}}$ 且 $\\lVert s^{k} \\rVert_{2} \\le \\varepsilon_{\\mathrm{dual}}$ 时停止。\n  - 使用 $\\varepsilon_{\\mathrm{pri}} = \\sqrt{p}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\max\\{\\lVert w^{k} \\rVert_{2}, \\lVert z^{k} \\rVert_{2}\\}$ 和 $\\varepsilon_{\\mathrm{dual}} = \\sqrt{p}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\rho \\lVert u^{k} \\rVert_{2}$，其中 $p$ 是 $X$ 的列数。\n- 使用 $\\varepsilon_{\\mathrm{abs}} = 10^{-6}$、$\\varepsilon_{\\mathrm{rel}} = 10^{-4}$ 和最大迭代次数 $K_{\\max} = 2000$，除非提前收敛。\n- 初始化 $w^{0} = 0$, $z^{0} = 0$, $u^{0} = 0$。\n\n测试套件：\n在以下四种情况下运行您的实现。对于每种情况，计算最终的目标值\n$$\nF(w) = \\frac{1}{2}\\lVert y - X w \\rVert_{2}^{2} + \\lambda \\lVert w \\rVert_{1}\n$$\n在收敛解 $w$ 处，并将结果四舍五入到 $6$ 位小数。\n\n- 情况 A（正交设计，检查软阈值行为）：\n  - $X = I_{3}$，即 $3 \\times 3$ 单位矩阵。\n  - $y = [3.0, -1.0, 0.2]^{\\top}$。\n  - $\\lambda = 1.0$。\n  - $\\rho = 1.0$。\n- 情况 B（极大正则化，将 $w$ 推向零的边界情况）：\n  - $X = \\begin{bmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  1 \\\\ 2  -1  0 \\end{bmatrix}$。\n  - $y = [1.0, -2.0, 0.5, 3.0]^{\\top}$。\n  - $\\lambda = 100.0$。\n  - $\\rho = 1.0$。\n- 情况 C（秩亏设计，通过 $\\rho I$ 项测试数值稳定性）：\n  - $X = \\begin{bmatrix} 1  2 \\\\ 2  4 \\\\ 3  6 \\end{bmatrix}$。\n  - $y = [1.0, 0.0, 1.0]^{\\top}$。\n  - $\\lambda = 0.1$。\n  - $\\rho = 1.0$。\n- 情况 D（微小正则化，接近最小二乘）：\n  - $X = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\\\ 2  1 \\end{bmatrix}$。\n  - $y = [1.0, 2.0, 2.5, 4.0]^{\\top}$。\n  - $\\lambda = 10^{-6}$。\n  - $\\rho = 1.0$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含按 A、B、C、D 顺序排列的 4 个四舍五入后的目标值，形式为用方括号括起来的逗号分隔列表（例如，$[v_{A},v_{B},v_{C},v_{D}]$）。不应打印任何额外文本。不涉及物理单位或角度单位，所有数值答案都是无单位的实数。唯一可接受的输出是四舍五入到 $6$ 位小数的实数。",
            "solution": "用户提供的问题是一个有效请求，要求推导并实现用于 Lasso 优化问题的交替方向乘子法 (ADMM)。所有定义、约束和数据都具有科学依据，内部一致且适定。该任务完全属于凸优化领域。\n\nLasso 优化问题使用分裂变量 $z$ 表述为：\n$$\n\\text{minimize} \\quad f(w) + g(z) \\quad \\text{subject to} \\quad w - z = 0\n$$\n其中 $f(w) = \\frac{1}{2}\\lVert y - X w \\rVert_{2}^{2}$ 且 $g(z) = \\lambda \\lVert z \\rVert_{1}$。这是一个等式约束的凸优化问题。\n\nADMM 算法通过最小化增广拉格朗日量来解决这个问题，该拉格朗日量结合了目标函数和等式约束。对于形式为 $\\min_{w,z} f(w) + g(z)$ 且服从 $Aw + Bz = c$ 的一般问题，增广拉格朗日量为：\n$$\nL_{\\rho}(w, z, \\nu) = f(w) + g(z) + \\nu^\\top(Aw + Bz - c) + \\frac{\\rho}{2}\\lVert Aw + Bz - c \\rVert_{2}^{2}\n$$\n其中 $\\nu$ 是对偶变量，$\\rho  0$ 是惩罚参数。\n\n对于我们的特定问题，$A = I$，$B = -I$（其中 $I$ 是适当维度的单位矩阵），且 $c = 0$。将这些代入一般形式可得：\n$$\nL_{\\rho}(w, z, \\nu) = \\frac{1}{2}\\lVert y - Xw \\rVert_{2}^{2} + \\lambda \\lVert z \\rVert_{1} + \\nu^\\top(w - z) + \\frac{\\rho}{2}\\lVert w - z \\rVert_{2}^{2}\n$$\n\nADMM 算法以缩放形式运行，其中对偶变量按 $\\rho$ 缩放。令 $u = (1/\\rho)\\nu$，则 $\\nu = \\rho u$。项 $\\nu^\\top(w-z) + \\frac{\\rho}{2}\\lVert w-z \\rVert_2^2$ 可以通过配方法重写：\n$$\n\\rho u^\\top(w-z) + \\frac{\\rho}{2}\\lVert w-z \\rVert_2^2 = \\frac{\\rho}{2} (2u^\\top(w-z) + \\lVert w-z \\rVert_2^2) = \\frac{\\rho}{2}(\\lVert w - z + u \\rVert_{2}^{2} - \\lVert u \\rVert_{2}^{2})\n$$\n忽略相对于 $w$ 和 $z$ 的常数项，缩放后的增广拉格朗日量为：\n$$\nL_{\\rho}(w, z, u) = \\frac{1}{2}\\lVert y - Xw \\rVert_{2}^{2} + \\lambda \\lVert z \\rVert_{1} + \\frac{\\rho}{2}\\lVert w - z + u \\rVert_{2}^{2}\n$$\n\nADMM 是一种迭代方法，在每次迭代 $k+1$ 中，按顺序执行三个更新：\n1. 关于 $w$ 最小化 $L_{\\rho}$，将 $z$ 和 $u$ 固定在第 $k$ 次迭代的值。\n2. 关于 $z$ 最小化 $L_{\\rho}$，使用新计算出的 $w^{k+1}$ 和固定的 $u^k$。\n3. 更新对偶变量 $u$。\n\n每个更新步骤的显式推导如下：\n\n**1. $w$-更新**\n\n$w$ 的更新通过求解以下无约束最小化问题找到：\n$$\nw^{k+1} = \\arg\\min_{w} \\left( \\frac{1}{2}\\lVert y - Xw \\rVert_{2}^{2} + \\frac{\\rho}{2}\\lVert w - z^{k} + u^{k} \\rVert_{2}^{2} \\right)\n$$\n目标函数是关于 $w$ 的二次函数且可微。通过将其关于 $w$ 的梯度设为零来找到最小值。设目标为 $J(w)$。\n$$\n\\nabla_{w} J(w) = \\nabla_{w} \\left( \\frac{1}{2}(y - Xw)^\\top(y - Xw) + \\frac{\\rho}{2}(w - (z^k-u^k))^\\top(w - (z^k-u^k)) \\right)\n$$\n$$\n\\nabla_{w} J(w) = -X^\\top(y - Xw) + \\rho(w - (z^k - u^k))\n$$\n设置 $\\nabla_{w} J(w) = 0$：\n$$\n-X^\\top y + X^\\top X w + \\rho w - \\rho(z^k - u^k) = 0\n$$\n$$\n(X^\\top X + \\rho I) w = X^\\top y + \\rho(z^k - u^k)\n$$\n对于任何 $\\rho  0$，矩阵 $(X^\\top X + \\rho I)$ 都是正定的，因此是可逆的，因为 $X^\\top X$ 是半正定的。$w$ 的更新是这个线性系统的解：\n$$\nw^{k+1} = (X^\\top X + \\rho I)^{-1} (X^\\top y + \\rho(z^k - u^k))\n$$\n\n**2. $z$-更新**\n\n$z$ 的更新通过求解以下问题找到：\n$$\nz^{k+1} = \\arg\\min_{z} \\left( \\lambda \\lVert z \\rVert_{1} + \\frac{\\rho}{2}\\lVert w^{k+1} - z + u^{k} \\rVert_{2}^{2} \\right)\n$$\n我们可以将二次项重写为 $\\frac{\\rho}{2}\\lVert z - (w^{k+1} + u^{k}) \\rVert_{2}^{2}$。问题变为：\n$$\nz^{k+1} = \\arg\\min_{z} \\left( \\lambda \\lVert z \\rVert_{1} + \\frac{\\rho}{2}\\lVert z - (w^{k+1} + u^{k}) \\rVert_{2}^{2} \\right)\n$$\n这是缩放的 $\\ell_1$-范数函数 $h(z) = \\lambda \\lVert z \\rVert_{1}$ 的近端算子的定义，在点 $v = w^{k+1} + u^{k}$ 处求值，缩放参数为 $1/\\rho$：\n$$\nz^{k+1} = \\text{prox}_{\\lambda\\lVert \\cdot \\rVert_1, 1/\\rho}(w^{k+1} + u^k)\n$$\n这个问题的解由软阈值算子 $S_{\\kappa}(a)$ 给出，其按元素定义为 $S_{\\kappa}(a_i) = \\text{sign}(a_i)\\max(|a_i|-\\kappa, 0)$。在我们的例子中，阈值是 $\\kappa = \\lambda/\\rho$。更新公式为：\n$$\nz^{k+1} = S_{\\lambda/\\rho}(w^{k+1} + u^k)\n$$\n\n**3. $u$-更新**\n\n缩放对偶变量的更新是：\n$$\nu^{k+1} = u^k + r^{k+1}\n$$\n其中 $r^{k+1} = w^{k+1} - z^{k+1}$ 是第 $k+1$ 次迭代的原始残差。此更新规则确保如果原始残差不为零，则调整对偶变量以在后续迭代中更强地强制执行约束。\n\n**停止标准**\n\n当原始残差和对偶残差的范数低于指定的容差时，算法终止。在第 $k$ 次迭代时：\n- 原始残差为 $r^k = w^k - z^k$。\n- 对偶残差为 $s^k = \\rho (z^k - z^{k-1})$。\n- 容差为 $\\varepsilon_{\\mathrm{pri}} = \\sqrt{p}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\max\\{\\lVert w^{k} \\rVert_{2}, \\lVert z^{k} \\rVert_{2}\\}$ 和 $\\varepsilon_{\\mathrm{dual}} = \\sqrt{p}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\rho \\lVert u^{k} \\rVert_{2}$。\n- 当 $\\lVert r^{k} \\rVert_{2} \\le \\varepsilon_{\\mathrm{pri}}$ 且 $\\lVert s^{k} \\rVert_{2} \\le \\varepsilon_{\\mathrm{dual}}$ 时，循环终止。\n\n数值实现将使用这些推导出的更新和停止标准。对于 $w$-更新，通过使用预先计算的 $(X^\\top X + \\rho I)$ 的 Cholesky 分解来高效地求解线性系统。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Sets up and runs the ADMM for Lasso on the four test cases specified\n    in the problem statement.\n    \"\"\"\n\n    def solve_admm(X, y, lambda_, rho, eps_abs=1e-6, eps_rel=1e-4, max_iter=2000):\n        \"\"\"\n        Solves the Lasso problem using ADMM.\n\n        The problem is to minimize:\n        0.5 * ||y - Xw||_2^2 + lambda * ||w||_1\n\n        Args:\n            X (np.ndarray): Data matrix (n x p).\n            y (np.ndarray): Response vector (n,).\n            lambda_ (float): L1 regularization parameter.\n            rho (float): ADMM penalty parameter.\n            eps_abs (float): Absolute tolerance for stopping criteria.\n            eps_rel (float): Relative tolerance for stopping criteria.\n            max_iter (int): Maximum number of iterations.\n\n        Returns:\n            np.ndarray: The converged coefficient vector w.\n        \"\"\"\n        n, p = X.shape\n\n        # Initialize variables\n        w = np.zeros(p)\n        z = np.zeros(p)\n        u = np.zeros(p)\n\n        # Pre-computation for w-update\n        X_T_X = X.T @ X\n        X_T_y = X.T @ y\n        # Use Cholesky factorization to solve the linear system in the w-update efficiently\n        # (X^T X + rho * I)w = X^T y + rho * (z - u)\n        L_and_lower = linalg.cho_factor(X_T_X + rho * np.eye(p))\n\n        for k in range(max_iter):\n            # Store z from previous iteration for dual residual calculation\n            z_prev = np.copy(z)\n\n            # 1. w-update\n            rhs = X_T_y + rho * (z - u)\n            w = linalg.cho_solve(L_and_lower, rhs)\n\n            # 2. z-update (soft-thresholding)\n            z_tilde = w + u\n            threshold = lambda_ / rho\n            z = np.sign(z_tilde) * np.maximum(np.abs(z_tilde) - threshold, 0)\n            \n            # 3. u-update\n            u = u + w - z\n\n            # Stopping criteria check\n            # Primal residual\n            r_k = w - z\n            norm_r = np.linalg.norm(r_k)\n            \n            # Dual residual\n            s_k = rho * (z - z_prev)\n            norm_s = np.linalg.norm(s_k)\n\n            # Tolerances\n            eps_pri = np.sqrt(p) * eps_abs + eps_rel * np.maximum(np.linalg.norm(w), np.linalg.norm(z))\n            eps_dual = np.sqrt(p) * eps_abs + eps_rel * rho * np.linalg.norm(u)\n\n            if norm_r = eps_pri and norm_s = eps_dual:\n                break\n        \n        return w\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Orthonormal design\n        {\n            'X': np.eye(3),\n            'y': np.array([3.0, -1.0, 0.2]),\n            'lambda': 1.0,\n            'rho': 1.0\n        },\n        # Case B: Very large regularization\n        {\n            'X': np.array([[1, 0, 2], [0, 1, -1], [1, 1, 1], [2, -1, 0]]),\n            'y': np.array([1.0, -2.0, 0.5, 3.0]),\n            'lambda': 100.0,\n            'rho': 1.0\n        },\n        # Case C: Rank-deficient design\n        {\n            'X': np.array([[1, 2], [2, 4], [3, 6]]),\n            'y': np.array([1.0, 0.0, 1.0]),\n            'lambda': 0.1,\n            'rho': 1.0\n        },\n        # Case D: Tiny regularization\n        {\n            'X': np.array([[1, 0], [0, 1], [1, 1], [2, 1]]),\n            'y': np.array([1.0, 2.0, 2.5, 4.0]),\n            'lambda': 1e-6,\n            'rho': 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X, y, lambda_, rho = case['X'], case['y'], case['lambda'], case['rho']\n        \n        w_final = solve_admm(X, y, lambda_, rho)\n        \n        # Calculate the final objective value F(w)\n        objective_value = 0.5 * np.linalg.norm(y - X @ w_final)**2 + lambda_ * np.linalg.norm(w_final, 1)\n        \n        results.append(round(objective_value, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}