{
    "hands_on_practices": [
        {
            "introduction": "Before applying any algorithm, it's crucial to understand its fundamental structure. This first practice  challenges you to analyze a piece of pseudocode and identify the underlying optimization method. This exercise will sharpen your ability to recognize the key distinguishing feature of the Nesterov Accelerated Gradient (NAG) method: the evaluation of the gradient at a \"look-ahead\" position.",
            "id": "2187801",
            "problem": "In the field of machine learning, iterative optimization algorithms are used to minimize a loss function $L(w)$ by updating a parameter vector $w$. Two popular momentum-based methods are Classical Momentum and Nesterov Accelerated Gradient (NAG).\n\nThe core idea of momentum is to accelerate movement in persistent directions of descent and to dampen oscillations. This is achieved by adding a \"velocity\" term, $v$, to the update rule. The update depends on a learning rate, $\\eta$, and a momentum coefficient, $\\gamma$.\n\nThe key difference between the two methods lies in *where* the gradient of the loss function is evaluated.\n- **Classical Momentum**: Computes the gradient at the current parameter position, $w_t$.\n- **Nesterov Accelerated Gradient (NAG)**: First, it makes a \"look-ahead\" step in the direction of the previous velocity. It then computes the gradient at this look-ahead position to make a more informed correction.\n\nConsider the following pseudocode for a single update step of an optimization algorithm. The function `compute_gradient_at(point)` calculates the gradient of the loss function $L$ at the given `point`.\n\n```\nfunction update_step(w, v, eta, gamma):\n    // w: current parameter vector\n    // v: current velocity vector\n    // eta: learning rate\n    // gamma: momentum coefficient\n\n    // Step 1: Calculate a temporary, projected position\n    w_projected = w - gamma * v\n\n    // Step 2: Compute the gradient at the projected position\n    grad = compute_gradient_at(w_projected)\n\n    // Step 3: Update the velocity vector\n    v_new = gamma * v + eta * grad\n\n    // Step 4: Update the parameter vector\n    w_new = w - v_new\n\n    // Return the updated parameter and velocity vectors\n    return w_new, v_new\n```\n\nWhich optimization algorithm does this pseudocode implement?\n\nA. Simple Gradient Descent with Momentum (Classical Momentum)\nB. Nesterov Accelerated Gradient (NAG)\nC. Simple Gradient Descent\nD. Adagrad\nE. RMSprop",
            "solution": "We are given an update scheme with parameters $w$ and velocity $v$, learning rate $\\eta$, and momentum coefficient $\\gamma$. The pseudocode performs the following steps:\n1) Projected position:\n$$w_{\\text{proj}}=w-\\gamma v.$$\n2) Gradient at the projected position:\n$$g=\\nabla L(w_{\\text{proj}})=\\nabla L(w-\\gamma v).$$\n3) Velocity update:\n$$v_{\\text{new}}=\\gamma v+\\eta g.$$\n4) Parameter update:\n$$w_{\\text{new}}=w-v_{\\text{new}}=w-(\\gamma v+\\eta g).$$\n\nClassical Momentum uses\n$$v_{t+1}=\\gamma v_{t}+\\eta \\nabla L(w_{t}),\\qquad w_{t+1}=w_{t}-v_{t+1},$$\nthat is, the gradient is evaluated at the current point $w_{t}$.\n\nNesterov Accelerated Gradient first computes a look-ahead point\n$$\\tilde{w}_{t}=w_{t}-\\gamma v_{t},$$\nthen evaluates the gradient there,\n$$g_{t}=\\nabla L(\\tilde{w}_{t})=\\nabla L(w_{t}-\\gamma v_{t}),$$\nand updates\n$$v_{t+1}=\\gamma v_{t}+\\eta g_{t},\\qquad w_{t+1}=w_{t}-v_{t+1}.$$\n\nComparing these formulas with the pseudocode, we see exact agreement with the Nesterov procedure: the gradient is computed at the look-ahead position $w-\\gamma v$, then the standard momentum-style velocity and parameter updates are applied. Therefore, the algorithm implemented by the pseudocode is Nesterov Accelerated Gradient.\n\nIt is not Simple Gradient Descent because there is a velocity term; it is not Classical Momentum because the gradient is not evaluated at $w$; and it is neither Adagrad nor RMSprop because there is no per-parameter adaptive scaling using accumulated squared gradients.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "With the conceptual framework in place, the next step is to apply the NAG update rules mechanically. This problem  provides a simple one-dimensional function and asks you to compute the exact value of the look-ahead point in an early iteration. Performing this calculation by hand reinforces the sequence of operations and makes the abstract formulas of the algorithm concrete and tangible.",
            "id": "2187811",
            "problem": "An engineer is applying the Nesterov Accelerated Gradient (NAG) algorithm to minimize a one-dimensional objective function. The function is given by $f(x) = 2x^2$. The algorithm iteratively updates a position parameter $x$ and a velocity term $v$ according to the following set of rules, starting from an initial position $x_0$ and an initial velocity $v_0=0$:\n$$ v_{t} = \\gamma v_{t-1} + \\eta \\nabla f(x_{t-1} - \\gamma v_{t-1}) $$\n$$ x_{t} = x_{t-1} - v_{t} $$\nIn these equations, a subscript denotes the iteration number, so $t=1, 2, 3, \\ldots$. The constant $\\gamma$ is the momentum parameter, $\\eta$ is the learning rate, and $\\nabla f$ is the gradient of the function $f(x)$. The evaluation of the gradient occurs at the \"look-ahead\" point, given by the term $x_{t-1} - \\gamma v_{t-1}$.\n\nFor an initial position of $x_0 = 10$, a momentum parameter of $\\gamma = 0.9$, and a learning rate of $\\eta = 0.1$, calculate the numerical value of the look-ahead point that is used during the second iteration of the algorithm (i.e., for $t=2$).",
            "solution": "We are given the Nesterov Accelerated Gradient updates:\n$$v_{t}=\\gamma v_{t-1}+\\eta \\nabla f\\!\\left(x_{t-1}-\\gamma v_{t-1}\\right), \\quad x_{t}=x_{t-1}-v_{t}.$$\nThe objective is $f(x)=2x^{2}$, so its gradient is\n$$\\nabla f(x)=\\frac{d}{dx}(2x^{2})=4x.$$\n\nGiven $x_{0}=10$, $v_{0}=0$, $\\gamma=0.9$, and $\\eta=0.1$, first compute the look-ahead point for $t=1$:\n$$x_{0}-\\gamma v_{0}=10-0.9\\cdot 0=10.$$\nThen update the velocity at $t=1$:\n$$v_{1}=\\gamma v_{0}+\\eta \\nabla f(10)=0.9\\cdot 0+0.1\\cdot 4\\cdot 10=4.$$\nUpdate the position:\n$$x_{1}=x_{0}-v_{1}=10-4=6.$$\n\nFor the second iteration ($t=2$), the look-ahead point is\n$$x_{1}-\\gamma v_{1}=6-0.9\\cdot 4=6-3.6=2.4.$$\nTherefore, the numerical value of the look-ahead point used during the second iteration is $2.4$.",
            "answer": "$$\\boxed{2.4}$$"
        },
        {
            "introduction": "Now that you can recognize and execute the NAG algorithm, let's build intuition for *why* it is so effective. This exercise  presents a hypothetical but common scenario where an optimizer has overshot a minimum. By analyzing the update step under these specific conditions, you will gain a deeper insight into how NAG's look-ahead mechanism provides a valuable \"correction\" to the momentum, a feature that enhances its performance over classical momentum methods.",
            "id": "2187789",
            "problem": "Consider a machine learning model whose parameters are being updated using an iterative optimization algorithm. At step $t$, the state of the optimizer is described by the parameter vector $\\theta_t$ and an accumulated momentum vector $v_t$. The optimizer's goal is to minimize a loss function $J(\\theta)$. The update is governed by a learning rate $\\eta > 0$ and a momentum coefficient $\\gamma \\in (0, 1)$.\n\nThe update rules for two common algorithms, Standard Momentum (SM) and Nesterov Accelerated Gradient (NAG), are given as follows:\n\n- **Standard Momentum (SM):**\n  1. Compute gradient: $g_t = \\nabla J(\\theta_t)$\n  2. Update momentum: $v_{t+1} = \\gamma v_t + \\eta g_t$\n  3. Update parameters: $\\theta_{t+1} = \\theta_t - v_{t+1}$\n\n- **Nesterov Accelerated Gradient (NAG):**\n  1. Compute lookahead position: $\\theta_{lookahead} = \\theta_t - \\gamma v_t$\n  2. Compute gradient at lookahead position: $g_{lookahead} = \\nabla J(\\theta_{lookahead})$\n  3. Update momentum: $v_{t+1} = \\gamma v_t + \\eta g_{lookahead}$\n  4. Update parameters: $\\theta_{t+1} = \\theta_t - v_{t+1}$\n\nNow, consider a specific situation where the optimizer has likely overshot a local minimum. The gradient at the current position $\\theta_t$ points in the exact opposite direction of the momentum vector, such that $\\nabla J(\\theta_t) = -c v_t$ for some positive constant $c$.\n\nTo analyze the behavior of NAG, assume that in the local vicinity of $\\theta_t$, the gradient of the loss function changes approximately linearly. That is, for a small displacement vector $d$, the gradient can be approximated as $\\nabla J(\\theta_t + d) \\approx \\nabla J(\\theta_t) + H d$, where the Hessian matrix $H$ is a constant, positive definite matrix. For this analysis, assume this relationship is exact and simplify the Hessian to $H = \\lambda I$, where $\\lambda$ is a positive scalar constant representing the local curvature and $I$ is the identity matrix. Thus, the approximation becomes an equality: $\\nabla J(\\theta_t + d) = \\nabla J(\\theta_t) + \\lambda d$.\n\nYour task is to determine the parameter update vector, $\\Delta \\theta = \\theta_{t+1} - \\theta_t$, for a single step of the Nesterov Accelerated Gradient (NAG) algorithm under these conditions. Express your answer as a symbolic expression in terms of $\\eta$, $\\gamma$, $c$, $\\lambda$, and the momentum vector $v_t$.",
            "solution": "We are given the Nesterov Accelerated Gradient (NAG) update:\n1) Compute lookahead position using the current momentum: $\\theta_{\\text{lookahead}} = \\theta_{t} - \\gamma v_{t}$.\n2) Compute the gradient at the lookahead position: $g_{\\text{lookahead}} = \\nabla J(\\theta_{\\text{lookahead}})$.\n3) Update the momentum: $v_{t+1} = \\gamma v_{t} + \\eta g_{\\text{lookahead}}$.\n4) Update the parameters: $\\theta_{t+1} = \\theta_{t} - v_{t+1}$.\n\nWe assume the local linear model for the gradient with Hessian $H = \\lambda I$, namely $\\nabla J(\\theta_{t} + d) = \\nabla J(\\theta_{t}) + \\lambda d$, and we are given that the current gradient is opposite to the momentum, $\\nabla J(\\theta_{t}) = - c v_{t}$ with $c > 0$.\n\nFirst, compute the gradient at the lookahead point. The displacement from $\\theta_{t}$ to the lookahead point is $d = \\theta_{\\text{lookahead}} - \\theta_{t} = - \\gamma v_{t}$. Using the linear gradient model,\n$$\ng_{\\text{lookahead}} = \\nabla J(\\theta_{\\text{lookahead}}) = \\nabla J(\\theta_{t}) + \\lambda d = - c v_{t} + \\lambda(- \\gamma v_{t}) = -\\left(c + \\gamma \\lambda\\right) v_{t}.\n$$\nUpdate the momentum using the NAG rule:\n$$\nv_{t+1} = \\gamma v_{t} + \\eta g_{\\text{lookahead}} = \\gamma v_{t} + \\eta \\left[-\\left(c + \\gamma \\lambda\\right) v_{t}\\right] = \\left[\\gamma - \\eta \\left(c + \\gamma \\lambda\\right)\\right] v_{t}.\n$$\nUpdate the parameters and form the parameter update vector $\\Delta \\theta = \\theta_{t+1} - \\theta_{t}$:\n$$\n\\theta_{t+1} = \\theta_{t} - v_{t+1} \\quad \\Longrightarrow \\quad \\Delta \\theta = - v_{t+1} = - \\left[\\gamma - \\eta \\left(c + \\gamma \\lambda\\right)\\right] v_{t} = \\left[\\eta \\left(c + \\gamma \\lambda\\right) - \\gamma\\right] v_{t}.\n$$\nThus, under the stated assumptions, the NAG one-step parameter update is\n$$\n\\Delta \\theta = \\left[\\eta \\left(c + \\gamma \\lambda\\right) - \\gamma\\right] v_{t}.\n$$",
            "answer": "$$\\boxed{\\left[\\eta\\left(c+\\gamma\\lambda\\right)-\\gamma\\right]v_{t}}$$"
        }
    ]
}