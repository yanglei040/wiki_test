{
    "hands_on_practices": [
        {
            "introduction": "理解一个算法的第一步是能够在其代码实现中识别出其核心结构。这个练习  旨在检验你是否能根据其标志性的“前瞻”梯度计算步骤，将Nesterov加速梯度（NAG）与其他动量方法区分开来。这对于在实际应用中正确解读和实现优化器至关重要。",
            "id": "2187801",
            "problem": "在机器学习领域，迭代优化算法通过更新参数矢量 $w$ 来最小化损失函数 $L(w)$。两种流行的基于动量的方法是 Classical Momentum 和 Nesterov Accelerated Gradient (NAG)。\n\n动量的核心思想是加速在持续下降方向上的移动，并抑制振荡。这是通过在更新规则中加入一个“速度”项 $v$ 来实现的。更新依赖于学习率 $\\eta$ 和动量系数 $\\gamma$。\n\n这两种方法的关键区别在于*何处*计算损失函数的梯度。\n- **Classical Momentum**：在当前参数位置 $w_t$ 处计算梯度。\n- **Nesterov Accelerated Gradient (NAG)**：首先，它沿着先前速度的方向进行一次“前瞻”步骤。然后，它在这个前瞻位置计算梯度，以进行更明智的修正。\n\n考虑以下用于优化算法单次更新步骤的伪代码。函数 `compute_gradient_at(point)` 计算损失函数 $L$ 在给定 `point` 处的梯度。\n\n```\nfunction update_step(w, v, eta, gamma):\n    // w: current parameter vector\n    // v: current velocity vector\n    // eta: learning rate\n    // gamma: momentum coefficient\n\n    // Step 1: Calculate a temporary, projected position\n    w_projected = w - gamma * v\n\n    // Step 2: Compute the gradient at the projected position\n    grad = compute_gradient_at(w_projected)\n\n    // Step 3: Update the velocity vector\n    v_new = gamma * v + eta * grad\n\n    // Step 4: Update the parameter vector\n    w_new = w - v_new\n\n    // Return the updated parameter and velocity vectors\n    return w_new, v_new\n```\n\n该伪代码实现了哪种优化算法？\n\nA. Simple Gradient Descent with Momentum (Classical Momentum)\n\nB. Nesterov Accelerated Gradient (NAG)\n\nC. Simple Gradient Descent\n\nD. Adagrad\n\nE. RMSprop",
            "solution": "给定一个更新方案，其参数为 $w$ 和速度 $v$，学习率为 $\\eta$，动量系数为 $\\gamma$。该伪代码执行以下步骤：\n1) 投影位置：\n$$w_{\\text{proj}}=w-\\gamma v$$\n2) 在投影位置的梯度：\n$$g=\\nabla L(w_{\\text{proj}})=\\nabla L(w-\\gamma v)$$\n3) 速度更新：\n$$v_{\\text{new}}=\\gamma v+\\eta g$$\n4) 参数更新：\n$$w_{\\text{new}}=w-v_{\\text{new}}=w-(\\gamma v+\\eta g)$$\n\nClassical Momentum 使用\n$$v_{t+1}=\\gamma v_{t}+\\eta \\nabla L(w_{t}),\\qquad w_{t+1}=w_{t}-v_{t+1},$$\n也就是说，梯度是在当前点 $w_{t}$ 处计算的。\n\nNesterov Accelerated Gradient 首先计算一个前瞻点\n$$\\tilde{w}_{t}=w_{t}-\\gamma v_{t},$$\n然后在此处计算梯度，\n$$g_{t}=\\nabla L(\\tilde{w}_{t})=\\nabla L(w_{t}-\\gamma v_{t}),$$\n并更新\n$$v_{t+1}=\\gamma v_{t}+\\eta g_{t},\\qquad w_{t+1}=w_{t}-v_{t+1}。$$\n\n将这些公式与伪代码进行比较，我们发现它与 Nesterov 过程完全一致：梯度是在前瞻位置 $w-\\gamma v$ 计算的，然后应用标准的动量式速度和参数更新。因此，伪代码实现的算法是 Nesterov Accelerated Gradient。\n\n它不是 Simple Gradient Descent，因为存在速度项；它不是 Classical Momentum，因为梯度不是在 $w$ 处计算的；它也不是 Adagrad 或 RMSprop，因为没有使用累积平方梯度进行逐参数的自适应缩放。",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "在识别出算法的结构之后，下一步是亲手执行它。这个练习  提供了一个动手机会，让你追踪NAG算法最初几个迭代步骤的计算过程。通过计算关键的“前瞻”点，你可以具体地看到NAG是如何运作的，并理解其与标准动量法的区别所在。",
            "id": "2187811",
            "problem": "一位工程师正在应用涅斯捷罗夫加速梯度 (Nesterov Accelerated Gradient, NAG) 算法来最小化一个一维目标函数。该函数为 $f(x) = 2x^2$。该算法根据以下规则集，从初始位置 $x_0$ 和初始速度 $v_0=0$ 开始，迭代地更新位置参数 $x$ 和速度项 $v$：\n$$ v_{t} = \\gamma v_{t-1} + \\eta \\nabla f(x_{t-1} - \\gamma v_{t-1}) $$\n$$ x_{t} = x_{t-1} - v_{t} $$\n在这些方程中，下标表示迭代次数，因此 $t=1, 2, 3, \\ldots$。常数 $\\gamma$ 是动量参数，$\\eta$ 是学习率，而 $\\nabla f$ 是函数 $f(x)$ 的梯度。梯度的计算发生在“前瞻”点，该点由项 $x_{t-1} - \\gamma v_{t-1}$ 给出。\n\n对于初始位置 $x_0 = 10$，动量参数 $\\gamma = 0.9$，以及学习率 $\\eta = 0.1$，计算在算法的第二次迭代（即 $t=2$）中使用的前瞻点的数值。",
            "solution": "给定涅斯捷罗夫加速梯度更新公式：\n$$v_{t}=\\gamma v_{t-1}+\\eta \\nabla f\\!\\left(x_{t-1}-\\gamma v_{t-1}\\right), \\quad x_{t}=x_{t-1}-v_{t}。$$\n目标函数为 $f(x)=2x^{2}$，所以其梯度为\n$$\\nabla f(x)=\\frac{d}{dx}(2x^{2})=4x。$$\n\n给定 $x_{0}=10$, $v_{0}=0$, $\\gamma=0.9$ 和 $\\eta=0.1$，首先计算 $t=1$ 时的前瞻点：\n$$x_{0}-\\gamma v_{0}=10-0.9\\cdot 0=10。$$\n然后更新 $t=1$ 时的速度：\n$$v_{1}=\\gamma v_{0}+\\eta \\nabla f(10)=0.9\\cdot 0+0.1\\cdot 4\\cdot 10=4。$$\n更新位置：\n$$x_{1}=x_{0}-v_{1}=10-4=6。$$\n\n对于第二次迭代（$t=2$），前瞻点为\n$$x_{1}-\\gamma v_{1}=6-0.9\\cdot 4=6-3.6=2.4。$$\n因此，在第二次迭代中使用的前瞻点的数值是 $2.4$。",
            "answer": "$$\\boxed{2.4}$$"
        },
        {
            "introduction": "为什么“前瞻”梯度如此有效？这个思想实验  探究了一个优化器可能“越过”最小值的场景。通过分析在这种特定条件下NAG的更新步骤，我们可以更深入、更直观地理解NAG是如何利用前瞻梯度来修正其路径并抑制振荡的。",
            "id": "2187789",
            "problem": "考虑一个机器学习模型，其参数正在通过一个迭代优化算法进行更新。在第 $t$ 步，优化器的状态由参数向量 $\\theta_t$ 和一个累积动量向量 $v_t$ 来描述。优化器的目标是最小化损失函数 $J(\\theta)$。更新过程由学习率 $\\eta > 0$ 和动量系数 $\\gamma \\in (0, 1)$ 控制。\n\n两种常见算法，即标准动量（SM）和 Nesterov 加速梯度（NAG）的更新规则如下：\n\n- **标准动量 (SM):**\n  1. 计算梯度：$g_t = \\nabla J(\\theta_t)$\n  2. 更新动量：$v_{t+1} = \\gamma v_t + \\eta g_t$\n  3. 更新参数：$\\theta_{t+1} = \\theta_t - v_{t+1}$\n\n- **Nesterov 加速梯度 (NAG):**\n  1. 计算前瞻位置：$\\theta_{lookahead} = \\theta_t - \\gamma v_t$\n  2. 计算在前瞻位置的梯度：$g_{lookahead} = \\nabla J(\\theta_{lookahead})$\n  3. 更新动量：$v_{t+1} = \\gamma v_t + \\eta g_{lookahead}$\n  4. 更新参数：$\\theta_{t+1} = \\theta_t - v_{t+1}$\n\n现在，考虑一种特殊情况，即优化器可能已经越过了一个局部最小值。在当前位置 $\\theta_t$ 的梯度与动量向量的方向完全相反，即 $\\nabla J(\\theta_t) = -c v_t$，其中 $c$ 为某个正的常数。\n\n为了分析 NAG 的行为，我们假设在 $\\theta_t$ 的局部邻域内，损失函数的梯度近似线性变化。也就是说，对于一个小的位移向量 $d$，梯度可以近似为 $\\nabla J(\\theta_t + d) \\approx \\nabla J(\\theta_t) + H d$，其中 Hessian 矩阵 $H$ 是一个常数正定矩阵。为了进行此分析，假设这种关系是精确的，并将 Hessian 矩阵简化为 $H = \\lambda I$，其中 $\\lambda$ 是一个表示局部曲率的正的常数标量，$I$ 是单位矩阵。因此，该近似变为一个等式：$\\nabla J(\\theta_t + d) = \\nabla J(\\theta_t) + \\lambda d$。\n\n你的任务是在这些条件下，确定 Nesterov 加速梯度（NAG）算法单步的参数更新向量 $\\Delta \\theta = \\theta_{t+1} - \\theta_t$。请用一个包含 $\\eta$、$\\gamma$、$c$、$\\lambda$ 和动量向量 $v_t$ 的符号表达式来表示你的答案。",
            "solution": "我们已知 Nesterov 加速梯度（NAG）的更新步骤：\n1) 使用当前动量计算前瞻位置：$\\theta_{\\text{lookahead}} = \\theta_{t} - \\gamma v_{t}$。\n2) 计算在前瞻位置的梯度：$g_{\\text{lookahead}} = \\nabla J(\\theta_{\\text{lookahead}})$。\n3) 更新动量：$v_{t+1} = \\gamma v_{t} + \\eta g_{\\text{lookahead}}$。\n4) 更新参数：$\\theta_{t+1} = \\theta_{t} - v_{t+1}$。\n\n我们假设梯度的局部线性模型为 $\\nabla J(\\theta_{t} + d) = \\nabla J(\\theta_{t}) + \\lambda d$（其中 Hessian 矩阵 $H = \\lambda I$），并且已知当前梯度与动量方向相反，即 $\\nabla J(\\theta_{t}) = - c v_{t}$，其中 $c > 0$。\n\n首先，计算前瞻点的梯度。从 $\\theta_{t}$ 到前瞻点的位移是 $d = \\theta_{\\text{lookahead}} - \\theta_{t} = - \\gamma v_{t}$。使用线性梯度模型，\n$$\ng_{\\text{lookahead}} = \\nabla J(\\theta_{\\text{lookahead}}) = \\nabla J(\\theta_{t}) + \\lambda d = - c v_{t} + \\lambda(- \\gamma v_{t}) = -\\left(c + \\gamma \\lambda\\right) v_{t}。\n$$\n使用 NAG 规则更新动量：\n$$\nv_{t+1} = \\gamma v_{t} + \\eta g_{\\text{lookahead}} = \\gamma v_{t} + \\eta \\left[-\\left(c + \\gamma \\lambda\\right) v_{t}\\right] = \\left[\\gamma - \\eta \\left(c + \\gamma \\lambda\\right)\\right] v_{t}。\n$$\n更新参数并构成参数更新向量 $\\Delta \\theta = \\theta_{t+1} - \\theta_{t}$：\n$$\n\\theta_{t+1} = \\theta_{t} - v_{t+1} \\quad \\Longrightarrow \\quad \\Delta \\theta = - v_{t+1} = - \\left[\\gamma - \\eta \\left(c + \\gamma \\lambda\\right)\\right] v_{t} = \\left[\\eta \\left(c + \\gamma \\lambda\\right) - \\gamma\\right] v_{t}。\n$$\n因此，在所述假设下，NAG 的单步参数更新为\n$$\n\\Delta \\theta = \\left[\\eta \\left(c + \\gamma \\lambda\\right) - \\gamma\\right] v_{t}。\n$$",
            "answer": "$$\\boxed{\\left[\\eta\\left(c+\\gamma\\lambda\\right)-\\gamma\\right]v_{t}}$$"
        }
    ]
}