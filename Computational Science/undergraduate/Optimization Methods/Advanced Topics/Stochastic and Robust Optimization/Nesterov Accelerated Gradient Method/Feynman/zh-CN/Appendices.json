{
    "hands_on_practices": [
        {
            "introduction": "在深入研究数学细节之前，识别 Nesterov 加速梯度（NAG）在实际代码中的标志性特征至关重要。本练习将要求你分析一段伪代码，并根据梯度的计算位置将其与经典动量法区分开来。这项技能对于在实践中实现和调试优化算法至关重要。",
            "id": "2187801",
            "problem": "在机器学习领域，迭代优化算法通过更新参数矢量 $w$ 来最小化损失函数 $L(w)$。两种流行的基于动量的方法是经典动量法（Classical Momentum）和 Nesterov 加速梯度（Nesterov Accelerated Gradient, NAG）。\n\n动量的核心思想是加速在持续下降方向上的运动，并抑制振荡。这是通过在更新规则中添加一个“速度”项 $v$ 来实现的。更新依赖于学习率 $\\eta$ 和动量系数 $\\gamma$。\n\n这两种方法之间的关键区别在于计算损失函数梯度的*位置*。\n- **经典动量法（Classical Momentum）**：在当前参数位置 $w_t$ 计算梯度。\n- **Nesterov 加速梯度 (NAG)**：首先，它沿着先前速度的方向进行一次“前瞻”步骤。然后，它在这个前瞻位置计算梯度，以做出更明智的修正。\n\n考虑以下用于优化算法单个更新步骤的伪代码。函数 `compute_gradient_at(point)` 计算损失函数 $L$ 在给定 `point` 处的梯度。\n\n```\nfunction update_step(w, v, eta, gamma):\n    // w: current parameter vector\n    // v: current velocity vector\n    // eta: learning rate\n    // gamma: momentum coefficient\n\n    // Step 1: Calculate a temporary, projected position\n    w_projected = w - gamma * v\n\n    // Step 2: Compute the gradient at the projected position\n    grad = compute_gradient_at(w_projected)\n\n    // Step 3: Update the velocity vector\n    v_new = gamma * v + eta * grad\n\n    // Step 4: Update the parameter vector\n    w_new = w - v_new\n\n    // Return the updated parameter and velocity vectors\n    return w_new, v_new\n```\n\n该伪代码实现了哪种优化算法？\n\nA. 带动量的梯度下降法（经典动量法）\n\nB. Nesterov 加速梯度 (NAG)\n\nC. 简单梯度下降法\n\nD. Adagrad\n\nE. RMSprop",
            "solution": "给定一个更新方案，其参数为 $w$ 和速度 $v$，学习率为 $\\eta$，动量系数为 $\\gamma$。该伪代码执行以下步骤：\n1) 投影位置：\n$$w_{\\text{proj}}=w-\\gamma v.$$\n2) 在投影位置的梯度：\n$$g=\\nabla L(w_{\\text{proj}})=\\nabla L(w-\\gamma v).$$\n3) 速度更新：\n$$v_{\\text{new}}=\\gamma v+\\eta g.$$\n4) 参数更新：\n$$w_{\\text{new}}=w-v_{\\text{new}}=w-(\\gamma v+\\eta g).$$\n\n经典动量法使用\n$$v_{t+1}=\\gamma v_{t}+\\eta \\nabla L(w_{t}),\\qquad w_{t+1}=w_{t}-v_{t+1},$$\n也就是说，梯度是在当前点 $w_{t}$ 处计算的。\n\nNesterov 加速梯度首先计算一个前瞻点\n$$\\tilde{w}_{t}=w_{t}-\\gamma v_{t},$$\n然后在该点计算梯度，\n$$g_{t}=\\nabla L(\\tilde{w}_{t})=\\nabla L(w_{t}-\\gamma v_{t}),$$\n并进行更新\n$$v_{t+1}=\\gamma v_{t}+\\eta g_{t},\\qquad w_{t+1}=w_{t}-v_{t+1}.$$\n\n将这些公式与伪代码进行比较，我们发现它与 Nesterov 的过程完全一致：梯度是在前瞻位置 $w-\\gamma v$ 计算的，然后应用标准的动量式速度和参数更新。因此，该伪代码实现的算法是 Nesterov 加速梯度。\n\n它不是简单梯度下降法，因为存在速度项；它不是经典动量法，因为梯度不是在 $w$ 处计算的；它也不是 Adagrad 或 RMSprop，因为没有使用累积平方梯度进行逐参数的自适应缩放。",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "现在你已经能够从概念上识别 NAG 算法，让我们来亲手实践它的运算过程。这个问题要求你在一个简单的一维二次函数上，一步步地执行 NAG 的迭代计算。通过亲手计算“前瞻”点和随后的参数更新，你将对该算法如何探索优化曲面获得具体而深刻的理解。",
            "id": "2187811",
            "problem": "一位工程师正在应用 Nesterov 加速梯度 (Nesterov Accelerated Gradient, NAG) 算法来最小化一个一维目标函数。该函数为 $f(x) = 2x^2$。该算法根据以下规则集，从初始位置 $x_0$ 和初始速度 $v_0=0$ 开始，迭代地更新位置参数 $x$ 和速度项 $v$：\n$$ v_{t} = \\gamma v_{t-1} + \\eta \\nabla f(x_{t-1} - \\gamma v_{t-1}) $$\n$$ x_{t} = x_{t-1} - v_{t} $$\n在这些方程中，下标表示迭代次数，因此 $t=1, 2, 3, \\ldots$。常数 $\\gamma$ 是动量参数，$\\eta$ 是学习率，$\\nabla f$ 是函数 $f(x)$ 的梯度。梯度的计算发生在“前瞻”点，由项 $x_{t-1} - \\gamma v_{t-1}$ 给出。\n\n对于初始位置 $x_0 = 10$，动量参数 $\\gamma = 0.9$，学习率 $\\eta = 0.1$，计算在算法的第二次迭代（即 $t=2$）中使用的前瞻点的数值。",
            "solution": "我们已知 Nesterov 加速梯度更新公式：\n$$v_{t}=\\gamma v_{t-1}+\\eta \\nabla f\\!\\left(x_{t-1}-\\gamma v_{t-1}\\right), \\quad x_{t}=x_{t-1}-v_{t}.$$\n目标函数为 $f(x)=2x^{2}$，所以其梯度是\n$$\\nabla f(x)=\\frac{d}{dx}(2x^{2})=4x.$$\n\n给定 $x_{0}=10$，$v_{0}=0$，$\\gamma=0.9$ 和 $\\eta=0.1$，首先计算 $t=1$ 时的前瞻点：\n$$x_{0}-\\gamma v_{0}=10-0.9\\cdot 0=10.$$\n然后更新 $t=1$ 时的速度：\n$$v_{1}=\\gamma v_{0}+\\eta \\nabla f(10)=0.9\\cdot 0+0.1\\cdot 4\\cdot 10=4.$$\n更新位置：\n$$x_{1}=x_{0}-v_{1}=10-4=6.$$\n\n对于第二次迭代（$t=2$），前瞻点是\n$$x_{1}-\\gamma v_{1}=6-0.9\\cdot 4=6-3.6=2.4.$$\n因此，在第二次迭代中使用的前瞻点的数值是 $2.4$。",
            "answer": "$$\\boxed{2.4}$$"
        },
        {
            "introduction": "NAG 的真正威力在于其“前瞻”梯度所提供的校正能力。本练习设定了一个优化器可能已经“过冲”了最小值的假设情景。你的任务是分析在此情况下 NAG 的更新规则如何响应，从而揭示它相比于标准动量法，在抑制振荡和智能修正路径方面的优势。",
            "id": "2187789",
            "problem": "考虑一个机器学习模型，其参数正通过一个迭代优化算法进行更新。在第 $t$ 步，优化器的状态由参数向量 $\\theta_t$ 和一个累积动量向量 $v_t$ 描述。优化器的目标是最小化损失函数 $J(\\theta)$。更新过程由学习率 $\\eta > 0$ 和动量系数 $\\gamma \\in (0, 1)$ 控制。\n\n两种常见算法，即标准动量法 (Standard Momentum, SM) 和 Nesterov 加速梯度 (Nesterov Accelerated Gradient, NAG) 的更新规则如下：\n\n- **标准动量法 (SM):**\n  1. 计算梯度: $g_t = \\nabla J(\\theta_t)$\n  2. 更新动量: $v_{t+1} = \\gamma v_t + \\eta g_t$\n  3. 更新参数: $\\theta_{t+1} = \\theta_t - v_{t+1}$\n\n- **Nesterov 加速梯度 (NAG):**\n  1. 计算前瞻位置: $\\theta_{lookahead} = \\theta_t - \\gamma v_t$\n  2. 计算前瞻位置的梯度: $g_{lookahead} = \\nabla J(\\theta_{lookahead})$\n  3. 更新动量: $v_{t+1} = \\gamma v_t + \\eta g_{lookahead}$\n  4. 更新参数: $\\theta_{t+1} = \\theta_t - v_{t+1}$\n\n现在，考虑一个特定情况，即优化器很可能已经越过了一个局部最小值。在当前位置 $\\theta_t$ 处的梯度指向与动量向量完全相反的方向，即 $\\nabla J(\\theta_t) = -c v_t$，其中 $c$ 为某个正常数。\n\n为了分析 NAG 的行为，假设在 $\\theta_t$ 的局部邻域内，损失函数的梯度近似线性变化。也就是说，对于一个小的位移向量 $d$，梯度可以近似为 $\\nabla J(\\theta_t + d) \\approx \\nabla J(\\theta_t) + H d$，其中 Hessian 矩阵 $H$ 是一个常数正定矩阵。为进行此分析，假设此关系是精确的，并将 Hessian 矩阵简化为 $H = \\lambda I$，其中 $\\lambda$ 是一个代表局部曲率的正常数标量，$I$ 是单位矩阵。因此，该近似变为一个等式：$\\nabla J(\\theta_t + d) = \\nabla J(\\theta_t) + \\lambda d$。\n\n你的任务是，在这些条件下，确定 Nesterov 加速梯度 (NAG) 算法单步的参数更新向量 $\\Delta \\theta = \\theta_{t+1} - \\theta_t$。将你的答案表示为关于 $\\eta$、$\\gamma$、$c$、$\\lambda$ 和动量向量 $v_t$ 的符号表达式。",
            "solution": "给定 Nesterov 加速梯度 (NAG) 更新规则：\n1) 使用当前动量计算前瞻位置：$\\theta_{\\text{lookahead}} = \\theta_{t} - \\gamma v_{t}$。\n2) 计算前瞻位置的梯度：$g_{\\text{lookahead}} = \\nabla J(\\theta_{\\text{lookahead}})$。\n3) 更新动量：$v_{t+1} = \\gamma v_{t} + \\eta g_{\\text{lookahead}}$。\n4) 更新参数：$\\theta_{t+1} = \\theta_{t} - v_{t+1}$。\n\n我们假设梯度的局部线性模型 Hessian $H = \\lambda I$，即 $\\nabla J(\\theta_{t} + d) = \\nabla J(\\theta_{t}) + \\lambda d$，并且给定当前梯度与动量方向相反，$\\nabla J(\\theta_{t}) = - c v_{t}$ 且 $c > 0$。\n\n首先，计算前瞻点的梯度。从 $\\theta_{t}$ 到前瞻点的位移是 $d = \\theta_{\\text{lookahead}} - \\theta_{t} = - \\gamma v_{t}$。使用线性梯度模型，\n$$\ng_{\\text{lookahead}} = \\nabla J(\\theta_{\\text{lookahead}}) = \\nabla J(\\theta_{t}) + \\lambda d = - c v_{t} + \\lambda(- \\gamma v_{t}) = -\\left(c + \\gamma \\lambda\\right) v_{t}.\n$$\n使用 NAG 规则更新动量：\n$$\nv_{t+1} = \\gamma v_{t} + \\eta g_{\\text{lookahead}} = \\gamma v_{t} + \\eta \\left[-\\left(c + \\gamma \\lambda\\right) v_{t}\\right] = \\left[\\gamma - \\eta \\left(c + \\gamma \\lambda\\right)\\right] v_{t}.\n$$\n更新参数并形成参数更新向量 $\\Delta \\theta = \\theta_{t+1} - \\theta_{t}$：\n$$\n\\theta_{t+1} = \\theta_{t} - v_{t+1} \\quad \\Longrightarrow \\quad \\Delta \\theta = - v_{t+1} = - \\left[\\gamma - \\eta \\left(c + \\gamma \\lambda\\right)\\right] v_{t} = \\left[\\eta \\left(c + \\gamma \\lambda\\right) - \\gamma\\right] v_{t}.\n$$\n因此，在所述假设下，NAG 的单步参数更新为\n$$\n\\Delta \\theta = \\left[\\eta \\left(c + \\gamma \\lambda\\right) - \\gamma\\right] v_{t}.\n$$",
            "answer": "$$\\boxed{\\left[\\eta\\left(c+\\gamma\\lambda\\right)-\\gamma\\right]v_{t}}$$"
        }
    ]
}