{
    "hands_on_practices": [
        {
            "introduction": "The effectiveness of any local search heuristic hinges on the design of its neighborhood function, which defines the set of moves the algorithm can make from a given solution. This exercise  challenges you to think like an algorithm designer, quantitatively comparing a generic, problem-agnostic neighborhood with a \"smarter,\" domain-specific one. By analyzing their respective costs and success rates, you will gain a deeper appreciation for the crucial trade-off between the computational effort of generating a move and the quality of that move, a fundamental principle in creating efficient heuristics.",
            "id": "3136481",
            "problem": "A university course timetabling problem assigns $n$ events to $t$ time slots and $r$ rooms. A solution $x$ is an assignment satisfying hard constraints such as teacher non-overlap, room capacity, and one event per room-time. Soft constraints, such as spreading events across days or minimizing late slots, contribute a penalty. Let the objective be a weighted sum $f(x)$ of soft-constraint penalties, with hard constraints enforced by rejecting infeasible neighbors during search. A local search method uses a neighborhood function $N(x)$ that generates candidate moves; a move from $x$ to $y$ is accepted if $y$ is feasible and $f(y) < f(x)$.\n\nConsider two neighborhood designs:\n- A generic neighborhood $N_g(x)$ consisting of swap and insert moves that disregard domain structure: any two events may swap their time slots, and any single event may be moved to any time slot-room pair, subject to post-generation feasibility checks.\n- A domain-specific neighborhood $N_d(x)$ that uses domain knowledge to pre-filter moves based on the conflict graph and capacity constraints: swaps are proposed only if they preserve teacher non-overlap and capacity, and inserts are proposed only to time slot-room pairs that admit capacity and do not create conflicts.\n\nAssume the following empirical statistics for a particular instance under the same random neighbor sampling scheme and evaluation budget:\n- Under $N_g(x)$, a randomly generated neighbor is feasible with probability $q_g = 0.2$, and, conditional on feasibility, is improving with probability $a_g = 0.25$. The per-neighbor generation and evaluation cost is $c_g = 1$ time unit.\n- Under $N_d(x)$, a randomly generated neighbor is feasible with probability $q_d = 0.8$, and, conditional on feasibility, is improving with probability $a_d = 0.5$. The per-neighbor generation and evaluation cost is $c_d = 1.5$ time units.\n\nSuppose the local search employs simple hill-climbing: draw one neighbor $y \\in N(x)$ uniformly at random and accept $y$ if $y$ is feasible and $f(y) < f(x)$; otherwise stay at $x$ and draw another neighbor. Additionally, consider a variant using Simulated Annealing (SA), where a non-improving feasible move with objective increase $\\Delta f = f(y) - f(x) > 0$ is accepted with probability $p_{\\text{SA}} = \\exp(-\\Delta f / T)$ at temperature $T$.\n\nEvaluate the following statements and select all that are correct:\n\nA. Under hill-climbing with rejection of infeasible and non-improving moves, the expected time per accepted improvement step is smaller for $N_d(x)$ than for $N_g(x)$ for the given $q_g$, $a_g$, $c_g$, $q_d$, $a_d$, $c_d$.\n\nB. Under Simulated Annealing at any temperature $T$, restricting the neighborhood from $N_g(x)$ to $N_d(x)$ necessarily decreases the probability of escaping local minima in the feasible region, so SA will converge worse with $N_d(x)$ regardless of $T$.\n\nC. If $N_d(x)$ induces a connected graph over the feasible solution set (that is, for any two feasible assignments $x$ and $x'$ there exists a path of moves in $N_d$ from $x$ to $x'$), then restricting to $N_d(x)$ preserves reachability of globally optimal feasible solutions while potentially improving per-iteration efficiency.\n\nD. In tabu search, generic neighborhoods $N_g(x)$ always dominate domain-specific neighborhoods $N_d(x)$ because the tabu mechanism and penalties compensate for the high rate of infeasible moves, making domain knowledge redundant.\n\nChoose all correct options.",
            "solution": "The user wants me to act as a meticulous STEM professor and solve a problem about heuristic optimization methods.\n\n### Problem Validation\n\nFirst, I will validate the problem statement.\n\n**Step 1: Extract Givens**\n\n- **Problem context**: University course timetabling.\n- **Variables**: $n$ events, $t$ time slots, $r$ rooms.\n- **Solution representation**: A solution $x$ is an assignment of events.\n- **Constraints**:\n    - Hard constraints: Teacher non-overlap, room capacity, one event per room-time. Must be satisfied.\n    - Soft constraints: Spreading events, minimizing late slots. Incur a penalty.\n- **Objective function**: $f(x)$ is a weighted sum of soft-constraint penalties; the goal is minimization.\n- **Search paradigm**: Local search with a neighborhood function $N(x)$. Infeasible neighbors are rejected.\n- **Neighborhood definitions**:\n    - **Generic neighborhood $N_g(x)$**:\n        - Moves: Swap any two events' time slots, or insert any event to any time slot-room pair.\n        - Feasibility check: Post-generation.\n        - Statistics:\n            - Feasibility probability: $q_g = 0.2$.\n            - Conditional improvement probability: $a_g = 0.25$.\n            - Per-neighbor cost: $c_g = 1$ time unit.\n    - **Domain-specific neighborhood $N_d(x)$**:\n        - Moves: Pre-filtered based on conflict graph and capacity constraints.\n        - Statistics:\n            - Feasibility probability: $q_d = 0.8$.\n            - Conditional improvement probability: $a_d = 0.5$.\n            - Per-neighbor cost: $c_d = 1.5$ time units.\n- **Algorithms**:\n    - **Hill-climbing**: Accept a randomly drawn neighbor $y$ if it is feasible and $f(y) < f(x)$.\n    - **Simulated Annealing (SA)**: A non-improving feasible move with $\\Delta f = f(y) - f(x) > 0$ is accepted with probability $p_{\\text{SA}} = \\exp(-\\Delta f / T)$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientific Grounding**: The problem is well-grounded in the established field of combinatorial optimization and metaheuristics. All concepts, including timetabling, local search, neighborhood design, hill-climbing, and simulated annealing, are standard and correctly defined.\n- **Well-Posed**: The problem provides sufficient quantitative data to evaluate the statements. The question posed in each option is answerable through logical deduction and calculation based on the given model.\n- **Objectivity**: The language is precise and objective.\n- **Consistency**: The problem statement is internally consistent. A potential ambiguity exists in the description of $N_d(x)$, which \"pre-filter[s] moves based on the conflict graph and capacity constraints\", which might imply $100\\%$ feasibility. However, the given feasibility is $q_d = 0.8$. This is not a contradiction; it implies that the pre-filtering addresses some, but not all, hard constraints (e.g., it might handle teacher conflicts and capacity but not the \"one event per room-time\" constraint, which is checked post-generation), a realistic scenario in complex problem domains.\n- **Realism**: The provided numerical statistics are plausible. It is common for more sophisticated, domain-specific neighborhood generators to have a higher computational cost ($c_d > c_g$) but yield a much higher proportion of high-quality moves ($q_d > q_g$, $a_d > a_g$).\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid, scientifically sound, well-posed, and internally consistent. I will proceed with the solution derivation.\n\n### Solution Derivation\n\nI will now evaluate each of the four statements.\n\n**A. Under hill-climbing with rejection of infeasible and non-improving moves, the expected time per accepted improvement step is smaller for $N_d(x)$ than for $N_g(x)$ for the given $q_g$, $a_g$, $c_g$, $q_d$, $a_d$, $c_d$.**\n\nTo evaluate this statement, we must calculate the expected time to find and accept an improving move for each neighborhood.\nThe process of finding an accepted improvement is a sequence of Bernoulli trials, where a \"success\" is generating a neighbor that is both feasible and improving. The number of trials to achieve the first success follows a geometric distribution.\n\nLet $p$ be the probability of success in a single trial. The expected number of trials to the first success is $1/p$.\nLet $c$ be the cost (time) per trial.\nThe expected time per accepted improvement step is $E[\\text{Time}] = c \\times (1/p)$.\n\nThe probability of a single trial being a success (generating a feasible and improving neighbor) is the product of the probability of the neighbor being feasible ($q$) and the conditional probability of it being improving given it is feasible ($a$). Thus, $p = q \\times a$.\n\nFor the generic neighborhood $N_g(x)$:\n- Probability of a feasible and improving neighbor: $p_g = q_g \\times a_g = 0.2 \\times 0.25 = 0.05$.\n- Cost per neighbor generation/evaluation: $c_g = 1$ time unit.\n- Expected time per improvement: $E_g = \\frac{c_g}{p_g} = \\frac{1}{0.05} = 20$ time units.\n\nFor the domain-specific neighborhood $N_d(x)$:\n- Probability of a feasible and improving neighbor: $p_d = q_d \\times a_d = 0.8 \\times 0.5 = 0.4$.\n- Cost per neighbor generation/evaluation: $c_d = 1.5$ time units.\n- Expected time per improvement: $E_d = \\frac{c_d}{p_d} = \\frac{1.5}{0.4} = \\frac{15}{4} = 3.75$ time units.\n\nComparing the expected times: $E_d = 3.75$ and $E_g = 20$.\nSince $3.75 < 20$, the expected time per accepted improvement step is indeed smaller for $N_d(x)$ than for $N_g(x)$.\n\nVerdict for A: **Correct**.\n\n**B. Under Simulated Annealing at any temperature $T$, restricting the neighborhood from $N_g(x)$ to $N_d(x)$ necessarily decreases the probability of escaping local minima in the feasible region, so SA will converge worse with $N_d(x)$ regardless of $T$.**\n\nThis statement makes a strong claim about escaping local minima. An escape from a local minimum in SA requires making a non-improving move, which has probability $p_{\\text{SA}} = \\exp(-\\Delta f / T)$. The ability to escape depends on the availability of such non-improving feasible neighbors in the neighborhood.\n\nLet's calculate the probability of generating a feasible, non-improving neighbor in a single trial. This is given by $p_{\\text{escape\\_candidate}} = q \\times (1-a)$.\n\nFor the generic neighborhood $N_g(x)$:\n- Probability of generating a feasible, non-improving neighbor:\n$p_{g, \\text{esc}} = q_g \\times (1 - a_g) = 0.2 \\times (1 - 0.25) = 0.2 \\times 0.75 = 0.15$.\n\nFor the domain-specific neighborhood $N_d(x)$:\n- Probability of generating a feasible, non-improving neighbor:\n$p_{d, \\text{esc}} = q_d \\times (1 - a_d) = 0.8 \\times (1 - 0.5) = 0.8 \\times 0.5 = 0.4$.\n\nThe probability of generating a potential escape move is higher for $N_d(x)$ ($0.4$) than for $N_g(x)$ ($0.15$). Therefore, the premise that restricting to $N_d(x)$ \"necessarily decreases the probability of escaping local minima\" is false. With $N_d(x)$, the SA algorithm is more likely to find a non-improving candidate move to consider for an escape.\n\nFurthermore, the conclusion that SA \"will converge worse... regardless of $T$\" is an unsupported generalization. The overall performance of SA depends on the balance between exploration (non-improving moves) and exploitation (improving moves), and the cooling schedule for $T$. A neighborhood like $N_d(x)$ that is more efficient at finding both improving moves (as shown in A) and non-improving moves could lead to much better convergence, potentially with a faster cooling schedule. The statement is fundamentally flawed.\n\nVerdict for B: **Incorrect**.\n\n**C. If $N_d(x)$ induces a connected graph over the feasible solution set (that is, for any two feasible assignments $x$ and $x'$ there exists a path of moves in $N_d$ from $x$ to $x'$), then restricting to $N_d(x)$ preserves reachability of globally optimal feasible solutions while potentially improving per-iteration efficiency.**\n\nThis statement consists of two claims.\n\n1.  **\"...preserves reachability of globally optimal feasible solutions\"**: The premise is that the neighborhood $N_d(x)$ induces a connected graph on the set of all feasible solutions. By definition, a connected graph means that any node (feasible solution) is reachable from any other node via a path of edges (moves from the neighborhood). A globally optimal solution is a member of this set of feasible solutions. Therefore, if the graph is connected, any global optimum is reachable from any starting feasible solution. This part of the statement is a direct and correct consequence of the definition of connectivity in a search space.\n\n2.  **\"...while potentially improving per-iteration efficiency\"**: This claim suggests that using $N_d(x)$ can be more efficient. Our analysis for statement A confirms this for the given data. We calculated the expected time to find an improving move (a measure of per-iteration efficiency for hill-climbing) to be $E_d = 3.75$ for $N_d(x)$ and $E_g = 20$ for $N_g(x)$. The improvement is substantial. The use of \"potentially\" is appropriate, as this efficiency gain is dependent on the specific problem instance and neighborhood statistics, not a universal law.\n\nThe statement correctly links a crucial theoretical property (connectivity, guaranteeing reachability) with a practical benefit (improved efficiency), which is demonstrated by the problem's data. This represents a desirable goal in heuristic design: creating a \"smarter\" neighborhood that is efficient without sacrificing the ability to explore the entire feasible space.\n\nVerdict for C: **Correct**.\n\n**D. In tabu search, generic neighborhoods $N_g(x)$ always dominate domain-specific neighborhoods $N_d(x)$ because the tabu mechanism and penalties compensate for the high rate of infeasible moves, making domain knowledge redundant.**\n\nThis statement makes an absolute claim (\"always dominate\") and a faulty conjecture (\"making domain knowledge redundant\").\n\nThe efficiency of any local search method, including Tabu Search (TS), is critically dependent on the cost and quality of the neighbors it evaluates. The neighborhood $N_g(x)$ generates infeasible moves $80\\%$ of the time ($1-q_g$). In the described problem setup, these are simply rejected, meaning $80\\%$ of the computational effort per trial with $N_g(x)$ is wasted. The tabu list, which is the core of TS, operates on *accepted moves*. It cannot \"compensate\" for the vast number of generated moves that are not even candidates for acceptance because they are infeasible.\n\nThe assertion that domain knowledge becomes \"redundant\" is contrary to one of the most fundamental principles of designing effective algorithms for hard problems. The \"No Free Lunch\" theorems for optimization imply that an algorithm's performance is elevated precisely when it incorporates knowledge about the problem's structure. The domain-specific neighborhood $N_d(x)$ is an example of embodying such knowledge to guide the search more effectively. Discarding this knowledge in favor of a brute-force, generic neighborhood is almost invariably a poor strategy. A more efficient neighborhood allows TS to perform more meaningful search steps in the same amount of time, leading to better solutions.\n\nThe word \"always\" makes the statement definitively false. It is trivial to construct scenarios (such as the one given in this problem) where a well-designed domain-specific neighborhood vastly outperforms a generic one within a TS framework.\n\nVerdict for D: **Incorrect**.",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "An iterative heuristic is only as good as its stopping rule; run it too long, and you waste resources, but stop too soon, and you miss better solutions. This practice  delves into this dilemma by asking you to implement and evaluate an adaptive stopping criterion based on a moving average of recent improvements. Through a Monte Carlo simulation, you will compare your practical rule against a theoretical \"oracle,\" allowing you to quantify the real-world risks of stopping prematurely versus stopping too late.",
            "id": "3136544",
            "problem": "You are asked to design, justify, and evaluate an adaptive stopping rule for a stochastic heuristic optimization process using only fundamental definitions from probability and statistics and standard algorithmic constructs. The goal is to quantify the trade-off between stopping prematurely and stopping too late.\n\nAssume an iterative heuristic optimizer produces a nonnegative scalar improvement sequence $\\{\\Delta_t\\}_{t=1}^{\\infty}$, where $\\Delta_t$ is the improvement in the objective value achieved at iteration $t$. Model the expected improvement by the exponentially decaying curve\n$$\n\\mathbb{E}[\\Delta_t] = A e^{-\\beta t},\n$$\nwith $A \\gt 0$ and $\\beta \\gt 0$. The observed improvement is\n$$\n\\Delta_t = \\max\\{0,\\, A e^{-\\beta t} + \\eta_t\\},\n$$\nwhere $\\{\\eta_t\\}$ are independent and identically distributed Gaussian noise terms with $\\eta_t \\sim \\mathcal{N}(0,\\sigma^2)$, and negative values are truncated at $0$ to enforce nonnegativity.\n\nDefine the following two stopping times.\n\n- Oracle stopping time. For a reference tolerance $\\epsilon_{\\text{true}} \\gt 0$, the oracle stop index is the smallest integer $t_{\\star}$ such that $\\mathbb{E}[\\Delta_t] \\le \\epsilon_{\\text{true}}$, that is,\n$$\nt_{\\star} = \\min\\{t \\in \\mathbb{N} : A e^{-\\beta t} \\le \\epsilon_{\\text{true}}\\}.\n$$\n\n- Adaptive moving-average rule. For a window length $w \\in \\mathbb{N}$ and an estimated tolerance $\\epsilon_{\\text{est}} \\gt 0$, define the moving average\n$$\n\\overline{\\Delta}_t = \\frac{1}{w} \\sum_{i=t-w+1}^{t} \\Delta_i \\quad \\text{for} \\quad t \\ge w.\n$$\nThe adaptive stop index is\n$$\n\\tau = \\min\\left\\{t \\in \\mathbb{N}: t \\ge w \\ \\text{and} \\ \\overline{\\Delta}_t \\le \\epsilon_{\\text{est}}\\right\\},\n$$\nwith the convention that if the set is empty up to a maximum allowed iteration $T_{\\max}$, then $\\tau$ is set to $T_{\\max}$.\n\nFor each parameter set below, perform a Monte Carlo experiment with $M$ independent runs. In each run, simulate the improvement sequence up to $T_{\\max}$ and record the adaptive stop index $\\tau$ and the oracle index $t_{\\star}$. Quantify:\n\n- Premature stopping risk as the fraction of runs where $\\tau \\lt t_{\\star}$.\n- Delayed stopping risk as the fraction of runs where $\\tau \\gt t_{\\star}$.\n\nBoth risks must be reported as decimal fractions in $[0,1]$ rounded to three decimals.\n\nUse the following test suite. Each tuple lists $(A,\\ \\beta,\\ \\sigma,\\ w,\\ \\epsilon_{\\text{est}},\\ \\epsilon_{\\text{true}},\\ T_{\\max},\\ M,\\ \\text{seed})$:\n\n- Test $1$: $(0.5, 0.05, 0.02, 10, 0.01, 0.01, 400, 4000, 123)$.\n- Test $2$: $(0.5, 0.05, 0.02, 5, 0.02, 0.01, 400, 4000, 456)$.\n- Test $3$: $(0.5, 0.05, 0.02, 20, 0.005, 0.01, 400, 4000, 789)$.\n- Test $4$: $(0.5, 0.05, 0.08, 3, 0.01, 0.01, 400, 4000, 42)$.\n- Test $5$: $(0.02, 0.1, 0.005, 5, 0.01, 0.01, 200, 4000, 321)$.\n\nRequirements and clarifications:\n\n- Angles do not appear in this problem; there is no angle unit to specify.\n- No physical units appear; all quantities are unitless scalars.\n- For each test, compute $t_{\\star}$ from the model definition of the oracle and ensure $t_{\\star} \\le T_{\\max}$ by the chosen parameters.\n- Your program must output a single line containing a list of floating-point numbers: for each test in the order given, output the premature risk followed by the delayed risk, both rounded to three decimals. Aggregate all tests into a single list. For example, the output format must be exactly like $[r_{1,\\text{prem}},r_{1,\\text{delay}},r_{2,\\text{prem}},\\dots,r_{5,\\text{delay}}]$ with no spaces.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3]$).",
            "solution": "The problem requires the design and evaluation of an adaptive stopping rule for a stochastic optimization process. This will be accomplished by comparing the performance of a practical, data-driven rule against a theoretical \"oracle\" rule through Monte Carlo simulation.\n\n### Fundamental Concepts\n\nThe core of the problem lies in the trade-off between exploration and exploitation, a classic theme in optimization. An iterative optimizer seeks to improve a solution over time. The improvement per iteration, denoted by $\\Delta_t$, is modeled as a stochastic process. We assume the expected improvement follows an exponential decay, a common model for processes with diminishing returns:\n$$\n\\mathbb{E}[\\Delta_t] = A e^{-\\beta t}\n$$\nHere, $A > 0$ represents the initial scale of improvement and $\\beta > 0$ is the decay rate. The observed improvement $\\Delta_t$ is a noisy version of this expectation:\n$$\n\\Delta_t = \\max\\{0,\\, A e^{-\\beta t} + \\eta_t\\}\n$$\nwhere $\\eta_t$ is a random noise term, modeled as an independent and identically distributed (i.i.d.) Gaussian variable, $\\eta_t \\sim \\mathcal{N}(0, \\sigma^2)$. The $\\max\\{0, \\cdot\\}$ operation ensures the observed improvement is nonnegative, which is physically meaningful.\n\n### The Oracle Stopping Rule\n\nAn ideal stopping rule would terminate the optimization process precisely when the *expected* future improvement drops below a meaningful threshold, $\\epsilon_{\\text{true}}$. This defines the oracle stopping time, $t_{\\star}$, which is inaccessible in a real-world scenario as it requires knowledge of the underlying model parameters $A$ and $\\beta$.\nThe oracle stop index $t_{\\star}$ is the smallest integer time $t$ for which the expected improvement is no greater than the true tolerance $\\epsilon_{\\text{true}}$:\n$$\nt_{\\star} = \\min\\{t \\in \\mathbb{N} : A e^{-\\beta t} \\le \\epsilon_{\\text{true}}\\}\n$$\nThis inequality can be solved for $t$:\n$$\nA e^{-\\beta t} \\le \\epsilon_{\\text{true}} \\implies e^{-\\beta t} \\le \\frac{\\epsilon_{\\text{true}}}{A} \\implies -\\beta t \\le \\ln\\left(\\frac{\\epsilon_{\\text{true}}}{A}\\right)\n$$\n$$\nt \\ge -\\frac{1}{\\beta} \\ln\\left(\\frac{\\epsilon_{\\text{true}}}{A}\\right) \\implies t \\ge \\frac{1}{\\beta} \\ln\\left(\\frac{A}{\\epsilon_{\\text{true}}}\\right)\n$$\nSince $t$ must be an integer, $t_{\\star}$ is the smallest integer satisfying this condition, which is found by taking the ceiling of the right-hand side. As $t$ must be a positive integer ($t \\in \\mathbb{N}$), we have:\n$$\nt_{\\star} = \\max\\left(1, \\left\\lceil \\frac{1}{\\beta} \\ln\\left(\\frac{A}{\\epsilon_{\\text{true}}}\\right) \\right\\rceil\\right)\n$$\nFor each set of parameters, $t_{\\star}$ is a deterministic, constant value that serves as our benchmark.\n\n### The Adaptive Stopping Rule\n\nA practical stopping rule cannot access $\\mathbb{E}[\\Delta_t]$ and must rely on the observable, noisy sequence $\\{\\Delta_t\\}$. The proposed adaptive rule uses a simple moving average to filter the noise and estimate the recent trend in improvement. The moving average over a window of size $w$ is defined as:\n$$\n\\overline{\\Delta}_t = \\frac{1}{w} \\sum_{i=t-w+1}^{t} \\Delta_i \\quad \\text{for} \\quad t \\ge w\n$$\nThe adaptive stop index, $\\tau$, is the first time $t$ (at or after the first full window is available at $t=w$) that this moving average drops below a specified estimated tolerance, $\\epsilon_{\\text{est}}$:\n$$\n\\tau = \\min\\left\\{t \\in \\mathbb{N}: t \\ge w \\ \\text{and} \\ \\overline{\\Delta}_t \\le \\epsilon_{\\text{est}}\\right\\}\n$$\nIf this condition is not met by a maximum iteration count $T_{\\max}$, we set $\\tau = T_{\\max}$.\n\nThe behavior of this rule is governed by the interplay of the window size $w$, the noise level $\\sigma$, and the chosen tolerance $\\epsilon_{\\text{est}}$.\n- A small $w$ makes the rule highly responsive but susceptible to noise, potentially causing premature stopping on a random dip or delayed stopping on a random spike.\n- A large $w$ provides better smoothing of the noise but introduces lag. The average $\\overline{\\Delta}_t$ will include older, larger values of $\\Delta_i$, potentially keeping the average above $\\epsilon_{\\text{est}}$ long after the true expected improvement has fallen below the threshold, leading to delayed stopping.\n\n### Monte Carlo Simulation and Risk Assessment\n\nSince $\\Delta_t$ is a random variable, the adaptive stopping time $\\tau$ is also a random variable. To evaluate the performance of the adaptive rule, we must analyze the distribution of $\\tau$ in relation to the fixed oracle time $t_{\\star}$. A Monte Carlo simulation is the appropriate tool for this analysis.\n\nThe procedure is as follows:\n1.  For a given set of parameters $(A, \\beta, \\sigma, w, \\epsilon_{\\text{est}}, \\epsilon_{\\text{true}}, T_{\\max}, M, \\text{seed})$, we first calculate the deterministic oracle time $t_{\\star}$.\n2.  We then conduct $M$ independent simulation runs.\n3.  In each run:\n    a. A sequence of random noise $\\{\\eta_t\\}_{t=1}^{T_{\\max}}$ is generated.\n    b. The sequence of observed improvements $\\{\\Delta_t\\}_{t=1}^{T_{\\max}}$ is computed.\n    c. The moving average sequence $\\{\\overline{\\Delta}_t\\}_{t=w}^{T_{\\max}}$ is calculated.\n    d. The adaptive stopping time $\\tau$ for that specific run is determined.\n4.  After $M$ runs, we have a sample of $M$ values for $\\tau$. We can then estimate the risks by counting outcomes:\n    - **Premature Stopping Risk**: The fraction of runs where the process stopped too early, i.e., $P(\\tau < t_{\\star}) \\approx \\frac{\\text{count}(\\tau < t_{\\star})}{M}$. This represents the risk of failing to achieve the full potential of the optimization.\n    - **Delayed Stopping Risk**: The fraction of runs where the process stopped too late, i.e., $P(\\tau > t_{\\star}) \\approx \\frac{\\text{count}(\\tau > t_{\\star})}{M}$. This represents the risk of wasting computational resources.\n\nThe Python implementation will carry out this simulation for each test case provided. The use of NumPy allows for efficient, vectorized operations to generate the sequences and compute the moving averages. Random number generation is controlled by a seed for reproducibility.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the Monte Carlo simulation for all test cases and print the results.\n    \"\"\"\n    # Test suite: (A, beta, sigma, w, eps_est, eps_true, T_max, M, seed)\n    test_cases = [\n        (0.5, 0.05, 0.02, 10, 0.01, 0.01, 400, 4000, 123),\n        (0.5, 0.05, 0.02, 5, 0.02, 0.01, 400, 4000, 456),\n        (0.5, 0.05, 0.02, 20, 0.005, 0.01, 400, 4000, 789),\n        (0.5, 0.05, 0.08, 3, 0.01, 0.01, 400, 4000, 42),\n        (0.02, 0.1, 0.005, 5, 0.01, 0.01, 200, 4000, 321),\n    ]\n\n    results = []\n    for params in test_cases:\n        premature_risk, delayed_risk = run_simulation(*params)\n        results.extend([round(premature_risk, 3), round(delayed_risk, 3)])\n\n    # Format output according to the problem specification\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(A, beta, sigma, w, eps_est, eps_true, T_max, M, seed):\n    \"\"\"\n    Performs a Monte Carlo simulation for a single set of parameters.\n\n    Args:\n        A (float): Initial improvement scale.\n        beta (float): Decay rate.\n        sigma (float): Standard deviation of Gaussian noise.\n        w (int): Moving average window size.\n        eps_est (float): Estimated tolerance for the adaptive rule.\n        eps_true (float): True tolerance for the oracle rule.\n        T_max (int): Maximum number of iterations.\n        M (int): Number of Monte Carlo runs.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple[float, float]: A tuple containing the premature stopping risk and\n                             the delayed stopping risk.\n    \"\"\"\n    # Step 1: Calculate the deterministic oracle stopping time t_star\n    # t_star must be an integer t >= 1.\n    # The condition is t >= (1/beta) * ln(A / eps_true).\n    if A <= eps_true:\n        t_star = 1\n    else:\n        t_star_val = (1 / beta) * np.log(A / eps_true)\n        t_star = int(np.ceil(t_star_val))\n        if t_star < 1:\n            t_star = 1\n\n    # Step 2: Set up Monte Carlo simulation\n    rng = np.random.default_rng(seed)\n    premature_count = 0\n    delayed_count = 0\n    \n    # Generate time steps array (1-based index)\n    t_steps = np.arange(1, T_max + 1)\n    \n    # Pre-calculate the expected improvement curve\n    expected_delta = A * np.exp(-beta * t_steps)\n\n    for _ in range(M):\n        # Step 3a: Generate noise and observed improvements for one run\n        eta = rng.normal(loc=0, scale=sigma, size=T_max)\n        delta_t = np.maximum(0, expected_delta + eta)\n\n        # Step 3b: Calculate moving averages efficiently using cumulative sums\n        # This creates a sliding window of size w over delta_t\n        cumsum_delta = np.cumsum(delta_t)\n        # Pad with 0 for the t-w calculation\n        cumsum_delta_padded = np.concatenate(([0], cumsum_delta))\n        moving_sums = cumsum_delta_padded[w:] - cumsum_delta_padded[:-w]\n        moving_averages = moving_sums / w\n\n        # moving_averages[i] corresponds to the average at time t = i + w\n        \n        # Step 3c: Find the adaptive stopping time tau\n        # Find indices where the condition is met\n        below_threshold_indices = np.where(moving_averages <= eps_est)[0]\n        \n        if below_threshold_indices.size > 0:\n            # The first index 'k' where the condition is met\n            first_idx = below_threshold_indices[0]\n            # Convert index back to time t. Index k corresponds to t = k + w.\n            tau = first_idx + w\n        else:\n            # If condition is never met, stop at T_max\n            tau = T_max\n\n        # Step 3d: Compare tau with t_star and update counters\n        if tau < t_star:\n            premature_count += 1\n        elif tau > t_star:\n            delayed_count += 1\n\n    # Step 4: Calculate risks\n    premature_risk = premature_count / M\n    delayed_risk = delayed_count / M\n\n    return premature_risk, delayed_risk\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        },
        {
            "introduction": "This final practice moves beyond simple local search to the implementation of a complete, model-based metaheuristic: the Cross-Entropy (CE) method. In this exercise , you will apply the CE method to the classic $0$-$1$ knapsack problem, learning how to iteratively refine a probabilistic model of the solution space based on high-performing \"elite\" samples. This hands-on project will solidify your understanding of how advanced heuristics learn from experience to guide the search toward optimal regions and how their performance is sensitive to key hyperparameters like the elite fraction $p$.",
            "id": "3136467",
            "problem": "You will implement the Cross-Entropy method for a binary sampling distribution to solve a single-instance, $0$-$1$ knapsack problem and analyze sensitivity to the elite fraction. The Cross-Entropy method is a model-based stochastic search that iteratively updates a parameterized sampling distribution by minimizing the Kullback–Leibler divergence (also called cross-entropy) to a target distribution that concentrates on high-performing solutions. The sampling distribution family here is the product of independent Bernoulli distributions, one per item. The update is derived from first principles by maximizing the likelihood of an elite set of solutions under the model. You must implement the full method and return quantitative results for a specified test suite of elite fractions.\n\nKnapsack instance: Let $n$ denote the number of items, $\\mathbf{w} \\in \\mathbb{N}^{n}$ the weights, $\\mathbf{v} \\in \\mathbb{N}^{n}$ the values, and $C \\in \\mathbb{N}$ the capacity. Consider $n=15$, capacity $C=50$, with\n$\\mathbf{w} = [12,7,11,8,9,13,4,6,10,3,14,5,2,1,15]$ and\n$\\mathbf{v} = [36,14,22,21,19,32,9,17,25,8,28,12,5,3,34]$.\nA candidate solution is a vector $\\mathbf{x} \\in \\{0,1\\}^{n}$, with objective $f(\\mathbf{x}) = \\sum_{i=1}^{n} v_{i} x_{i}$ and feasibility constraint $\\sum_{i=1}^{n} w_{i} x_{i} \\le C$.\n\nSampling distribution: Use a vector of independent Bernoulli parameters $\\boldsymbol{\\theta} \\in (0,1)^{n}$, with the $i$-th bit sampled as $x_{i} \\sim \\text{Bernoulli}(\\theta_{i})$. Initialize $\\theta_{i} = 0.5$ for all $i$.\n\nFeasibility handling: To ensure feasibility, apply a deterministic repair operator to every sampled $\\mathbf{x}$ that violates the capacity. If $\\sum_{i} w_{i} x_{i} > C$, iteratively set $x_{j} \\leftarrow 0$ for items $j$ in increasing order of the ratio $r_{j} = v_{j} / w_{j}$ (that is, remove items with the smallest $v_{j}/w_{j}$ first) until $\\sum_{i} w_{i} x_{i} \\le C$. This operator must terminate in finitely many steps and produce a feasible solution.\n\nCross-Entropy iteration: At each iteration $t = 1,2,\\dots,T$, independently sample $N$ solutions from the current $\\boldsymbol{\\theta}$, repair them if needed, evaluate $f(\\cdot)$, and select the set of elites $\\mathcal{E}$ as the top $m$ solutions by objective value, where $m = \\max(1, \\lceil pN \\rceil)$ for a given elite fraction $p \\in (0,1]$. Update $\\boldsymbol{\\theta}$ by maximizing the likelihood of the elite set under the Bernoulli product model, using an exponential smoothing step with parameter $\\alpha \\in (0,1]$. After the update, project each $\\theta_{i}$ to the interval $[\\varepsilon, 1 - \\varepsilon]$ with $\\varepsilon \\in (0, 0.5)$ to avoid degeneracy. Maintain and return the best feasible objective value found across all samples and all iterations.\n\nFundamental base you must use and explicitly reason from in your design and derivation:\n- The definition of the $0$-$1$ knapsack problem: maximize $f(\\mathbf{x}) = \\sum_{i=1}^{n} v_{i} x_{i}$ subject to $\\sum_{i=1}^{n} w_{i} x_{i} \\le C$, with $\\mathbf{x} \\in \\{0,1\\}^{n}$.\n- The independent Bernoulli product family: for $\\mathbf{x} \\in \\{0,1\\}^{n}$, $p_{\\boldsymbol{\\theta}}(\\mathbf{x}) = \\prod_{i=1}^{n} \\theta_{i}^{x_{i}} (1-\\theta_{i})^{1 - x_{i}}$.\n- Maximum likelihood estimation for independent Bernoulli variables, applied to a finite elite dataset.\n- The definition of the Kullback–Leibler divergence and the equivalence between minimizing cross-entropy to a target distribution supported on elites and maximizing likelihood over the elite set.\n\nHyperparameters to use in your implementation:\n- Number of samples per iteration $N = 300$.\n- Number of iterations $T = 60$.\n- Smoothing parameter $\\alpha = 0.7$.\n- Probability floor $\\varepsilon = 0.05$.\n- Random number generator seed $s = 314159$ for all test cases.\n\nTest suite: You must evaluate the best objective value returned by the algorithm for each of the following elite fractions $p$:\n- Case $1$: $p = 0.05$.\n- Case $2$: $p = 0.20$.\n- Case $3$: $p = 0.50$.\n- Case $4$: $p = 0.90$.\n\nDesign for coverage: The test suite includes a small elite fraction $p = 0.05$ (aggressive updates), a moderate fraction $p = 0.20$, a balanced fraction $p = 0.50$, and a very large fraction $p = 0.90$ (conservative updates), to probe the bias–variance trade-off in the Cross-Entropy method.\n\nAnswer specification:\n- For each case, return the single integer equal to the best feasible objective value found by the algorithm after $T$ iterations.\n- Aggregate the results for the four cases into a single output line formatted exactly as a comma-separated list enclosed in square brackets, for example, $[r_{1},r_{2},r_{3},r_{4}]$, where each $r_{k}$ is an integer.\n- No physical units apply. Angles are not involved. Percentages must be input as decimals as provided.\n- Your program must produce exactly one line of output in the specified format and must not require any user input or external files.",
            "solution": "The problem requires the implementation of the Cross-Entropy (CE) method to find a high-quality solution for a specific instance of the $0$-$1$ knapsack problem. The sensitivity of the algorithm's performance to the elite fraction parameter, $p$, will be analyzed across a provided test suite.\n\nThe approach begins with a formal validation of the problem statement, which is found to be scientifically sound, well-posed, and complete. Following validation, a detailed, principle-based solution is derived and explained.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Problem**: $0$-$1$ knapsack problem.\n*   **Instance**:\n    *   Number of items: $n=15$.\n    *   Capacity: $C=50$.\n    *   Weights: $\\mathbf{w} = [12,7,11,8,9,13,4,6,10,3,14,5,2,1,15]$.\n    *   Values: $\\mathbf{v} = [36,14,22,21,19,32,9,17,25,8,28,12,5,3,34]$.\n*   **Solution Vector**: $\\mathbf{x} \\in \\{0,1\\}^{n}$.\n*   **Objective**: Maximize $f(\\mathbf{x}) = \\sum_{i=1}^{n} v_{i} x_{i}$.\n*   **Constraint**: $\\sum_{i=1}^{n} w_{i} x_{i} \\le C$.\n*   **Sampling Distribution**: Product of independent Bernoulli distributions with parameters $\\boldsymbol{\\theta} \\in (0,1)^{n}$.\n*   **Initialization**: $\\theta_{i} = 0.5$ for all $i$.\n*   **Feasibility Handling**: A deterministic repair operator that iteratively removes items from an overweight knapsack in increasing order of their value-to-weight ratio $r_j = v_j/w_j$.\n*   **CE Iteration**: Based on sampling, repair, evaluation, elite selection, and parameter update.\n*   **Fundamental Principles**: $0$-$1$ knapsack definition, Bernoulli product model, Maximum Likelihood Estimation (MLE) for Bernoulli variables, equivalence of KL-divergence minimization and MLE.\n*   **Hyperparameters**:\n    *   Samples per iteration: $N = 300$.\n    *   Iterations: $T = 60$.\n    *   Smoothing parameter: $\\alpha = 0.7$.\n    *   Probability floor: $\\varepsilon = 0.05$.\n    *   Random seed: $s = 314159$.\n*   **Test Suite**: Elite fractions $p \\in \\{0.05, 0.20, 0.50, 0.90\\}$.\n*   **Output**: Best feasible objective value found for each $p$, formatted as a list.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the validation criteria:\n*   **Scientifically Grounded**: The problem is well-grounded in the established fields of optimization (Cross-Entropy method) and computer science (knapsack problem). The mathematical principles cited are correct.\n*   **Well-Posed**: All necessary data, parameters, and algorithmic steps are explicitly defined. The use of a fixed random seed ensures a unique and reproducible result for each test case.\n*   **Objective**: The problem is stated using precise, quantitative, and unbiased language.\n*   The problem exhibits no flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity. It is a standard, non-trivial computational problem that is perfectly verifiable.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A complete solution will be provided.\n\n### Solution Derivation and Algorithmic Design\n\nThe Cross-Entropy method is a stochastic optimization algorithm that works by iteratively updating the parameters of a probability distribution over the solution space. The goal is to evolve the distribution so that it generates samples in regions of high objective value.\n\n**1. The Knapsack Problem and Probabilistic Model**\n\nThe problem is to select a subset of $n=15$ items to maximize total value without exceeding a weight capacity of $C=50$. A solution is a binary vector $\\mathbf{x} \\in \\{0,1\\}^{n}$, where $x_i=1$ if item $i$ is included and $x_i=0$ otherwise.\nThe objective function is $f(\\mathbf{x}) = \\mathbf{v}^T \\mathbf{x} = \\sum_{i=1}^{n} v_{i} x_{i}$, subject to the constraint $\\mathbf{w}^T \\mathbf{x} = \\sum_{i=1}^{n} w_{i} x_{i} \\le C$.\n\nWe define a parameterized probability distribution over the space of solutions $\\{0,1\\}^n$. As specified, this is a product of independent Bernoulli distributions:\n$$p_{\\boldsymbol{\\theta}}(\\mathbf{x}) = \\prod_{i=1}^{n} p(x_i | \\theta_i) = \\prod_{i=1}^{n} \\theta_{i}^{x_{i}} (1-\\theta_{i})^{1 - x_{i}}$$\nHere, $\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_n)$ is the vector of parameters, where $\\theta_i$ is the probability that item $i$ is included in a solution ($x_i=1$). Initially, all items have an equal chance of being selected: $\\theta_i^{(0)} = 0.5$ for all $i=1, \\dots, n$.\n\n**2. Parameter Update via Maximum Likelihood Estimation**\n\nThe core of the CE method is the update rule for $\\boldsymbol{\\theta}$. At each iteration $t$, we generate a population of $N$ candidate solutions from the current distribution $p_{\\boldsymbol{\\theta}^{(t-1)}}(\\mathbf{x})$. After repairing and evaluating these solutions, we select an \"elite set\" $\\mathcal{E}$ containing the $m$ best-performing solutions. The distribution for the next iteration, $\\boldsymbol{\\theta}^{(t)}$, is chosen to be \"closer\" to an ideal distribution that would only generate solutions from this elite set.\n\nMinimizing the Kullback-Leibler divergence (or cross-entropy) between $p_{\\boldsymbol{\\theta}}(\\mathbf{x})$ and this ideal target distribution is equivalent to maximizing the likelihood of the elite samples under our model $p_{\\boldsymbol{\\theta}}(\\mathbf{x})$.\n\nLet the elite set be $\\mathcal{E} = \\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(m)}\\}$. The likelihood function is:\n$$L(\\boldsymbol{\\theta}; \\mathcal{E}) = \\prod_{j=1}^{m} p_{\\boldsymbol{\\theta}}(\\mathbf{x}^{(j)}) = \\prod_{j=1}^{m} \\prod_{i=1}^{n} \\theta_{i}^{x_{i}^{(j)}} (1-\\theta_{i})^{1 - x_{i}^{(j)}}$$\nFor easier maximization, we use the log-likelihood:\n$$\\log L(\\boldsymbol{\\theta}; \\mathcal{E}) = \\sum_{j=1}^{m} \\sum_{i=1}^{n} \\left[ x_{i}^{(j)} \\log(\\theta_i) + (1-x_{i}^{(j)}) \\log(1-\\theta_i) \\right]$$\nTo find the parameters $\\boldsymbol{\\theta}$ that maximize this function, we take the partial derivative with respect to each $\\theta_k$ and set it to zero:\n$$\\frac{\\partial \\log L}{\\partial \\theta_k} = \\sum_{j=1}^{m} \\left[ \\frac{x_{k}^{(j)}}{\\theta_k} - \\frac{1-x_{k}^{(j)}}{1-\\theta_k} \\right] = 0$$\n$$\\frac{1}{\\theta_k} \\sum_{j=1}^{m} x_{k}^{(j)} = \\frac{1}{1-\\theta_k} \\sum_{j=1}^{m} (1-x_{k}^{(j)})$$\nSolving for $\\theta_k$ yields the maximum likelihood estimate, which we denote as $\\hat{\\theta}_k$:\n$$\\hat{\\theta}_k = \\frac{\\sum_{j=1}^{m} x_{k}^{(j)}}{m}$$\nThis elegant result shows that the optimal parameter $\\hat{\\theta}_k$ for the $k$-th item is simply the frequency of that item's inclusion across all solutions in the elite set.\n\n**3. Smoothed Update and Parameter Projection**\n\nTo prevent premature convergence and provide stability, the parameter update is smoothed using an exponential moving average with smoothing parameter $\\alpha$:\n$$\\boldsymbol{\\theta}^{(t)} = \\alpha \\hat{\\boldsymbol{\\theta}} + (1-\\alpha) \\boldsymbol{\\theta}^{(t-1)}$$\nA larger $\\alpha$ (like the specified $\\alpha=0.7$) gives more weight to the most recent elite set, leading to faster convergence, while a smaller $\\alpha$ leads to more conservative updates.\n\nFurthermore, to ensure that the search does not stagnate by having probabilities collapse to exactly $0$ or $1$, the updated parameters are projected into the interval $[\\varepsilon, 1-\\varepsilon]$. For each component $i$:\n$$\\theta_i^{(t)} \\leftarrow \\max(\\varepsilon, \\min(1-\\varepsilon, \\theta_i^{(t)}))$$\nWith $\\varepsilon=0.05$, every item always has at least a $5\\%$ chance of being sampled (or not sampled), maintaining some level of exploration.\n\n**4. Feasibility Repair Operator**\n\nSince sampling from the Bernoulli distribution does not respect the knapsack capacity constraint, a repair step is necessary. For any generated solution $\\mathbf{x}$ where $\\mathbf{w}^T\\mathbf{x} > C$, we must remove items until the constraint is met. The specified strategy is to remove items with the lowest value-to-weight ratio, $r_j = v_j/w_j$, first. This is a greedy choice that aims to preserve as much value as possible while reducing weight. This operator is applied to every generated sample before evaluation.\n\n**5. Complete Algorithm**\n\nThe full algorithm for a given elite fraction $p$ is as follows:\n\n1.  **Initialization**:\n    *   Set iteration counter $t \\leftarrow 0$.\n    *   Initialize Bernoulli parameters $\\boldsymbol{\\theta}^{(0)} \\leftarrow (0.5, 0.5, \\dots, 0.5)$.\n    *   Initialize best-found objective value $f_{best} \\leftarrow -\\infty$.\n    *   Pre-compute item indices sorted by $v_j/w_j$.\n    *   Set elite sample count $m = \\max(1, \\lceil pN \\rceil)$.\n\n2.  **Iteration Loop**: For $t = 1, \\dots, T$:\n    *   **Sampling**: Generate $N$ samples $\\{\\mathbf{x}^{(1)}, \\dots, \\mathbf{x}^{(N)}\\}$ by drawing $x_i^{(j)} \\sim \\text{Bernoulli}(\\theta_i^{(t-1)})$.\n    *   **Repair**: For each sample $\\mathbf{x}^{(j)}$: if $\\mathbf{w}^T\\mathbf{x}^{(j)} > C$, apply the repair operator until $\\mathbf{w}^T\\mathbf{x}^{(j)} \\le C$. Let the repaired samples be $\\{\\mathbf{x'}^{(1)}, \\dots, \\mathbf{x'}^{(N)}\\}$.\n    *   **Evaluation**: Compute the objective values $f(\\mathbf{x'}^{(j)})$ for all $j=1, \\dots, N$.\n    *   **Update Best**: $f_{best} \\leftarrow \\max(f_{best}, \\max_{j} f(\\mathbf{x'}^{(j)}))$.\n    *   **Elite Selection**: Identify the set $\\mathcal{E}$ of the $m$ repaired samples with the highest objective values.\n    *   **Parameter Update**:\n        a. Calculate MLE: $\\hat{\\theta}_i = \\frac{1}{m} \\sum_{\\mathbf{x'} \\in \\mathcal{E}} x'_i$ for all $i=1, \\dots, n$.\n        b. Smooth: $\\boldsymbol{\\theta}^{(t)} = \\alpha \\hat{\\boldsymbol{\\theta}} + (1-\\alpha) \\boldsymbol{\\theta}^{(t-1)}$.\n        c. Project: $\\theta_i^{(t)} \\leftarrow \\max(\\varepsilon, \\min(1-\\varepsilon, \\theta_i^{(t)}))$.\n\n3.  **Termination**: After $T$ iterations, return $f_{best}$.\n\nThis procedure is repeated for each value of $p$ in the test suite $\\{0.05, 0.20, 0.50, 0.90\\}$, with the random number generator re-initialized for each run to ensure fair, independent trials.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Implements the Cross-Entropy method for the 0-1 knapsack problem\n    and evaluates it for a suite of elite fraction parameters.\n    \"\"\"\n    \n    # Knapsack problem instance\n    n = 15\n    capacity = 50\n    weights = np.array([12, 7, 11, 8, 9, 13, 4, 6, 10, 3, 14, 5, 2, 1, 15])\n    values = np.array([36, 14, 22, 21, 19, 32, 9, 17, 25, 8, 28, 12, 5, 3, 34])\n\n    # Hyperparameters\n    N_SAMPLES = 300\n    N_ITERATIONS = 60\n    ALPHA = 0.7  # Smoothing parameter\n    EPSILON = 0.05  # Probability floor\n    SEED = 314159\n\n    # Test suite for elite fraction p\n    test_cases = [0.05, 0.20, 0.50, 0.90]\n    \n    # Pre-calculate value-to-weight ratios and sorted indices for repair\n    # Handle division by zero if a weight is 0, though not present in this data\n    with np.errstate(divide='ignore', invalid='ignore'):\n        ratios = values / weights\n    ratios[np.isinf(ratios)] = np.finfo(np.float64).max # If w=0, v>0, ratio is infinite\n    ratios[np.isnan(ratios)] = -1.0 # If w=0, v=0\n    \n    # Indices of items sorted by increasing v/w ratio\n    # Items with lower ratios will be removed first\n    sorted_indices_for_repair = np.argsort(ratios)\n    \n    final_results = []\n\n    for p_elite in test_cases:\n        # Initialize RNG for reproducibility for each test case\n        rng = np.random.default_rng(SEED)\n        \n        # Initialize Bernoulli parameters\n        theta = np.full(n, 0.5)\n        \n        # Track the best objective value found so far\n        best_overall_value = 0\n        \n        # Calculate number of elite samples\n        # m = max(1, ceil(p*N))\n        m_elite = max(1, int(math.ceil(p_elite * N_SAMPLES)))\n\n        for _ in range(N_ITERATIONS):\n            # 1. Sampling: Generate N samples from the Bernoulli distribution\n            # samples[i, j] is 1 if item j is in sample i, 0 otherwise\n            random_values = rng.random((N_SAMPLES, n))\n            samples = (random_values < theta).astype(int)\n            \n            repaired_samples = np.copy(samples)\n            \n            # 2. Feasibility Repair\n            for i in range(N_SAMPLES):\n                current_weight = np.dot(repaired_samples[i], weights)\n                if current_weight > capacity:\n                    # The sample is overweight, repair it\n                    # Iterate through items sorted by low v/w ratio\n                    for item_idx in sorted_indices_for_repair:\n                        if repaired_samples[i, item_idx] == 1:\n                            # Remove this item from the knapsack\n                            repaired_samples[i, item_idx] = 0\n                            current_weight -= weights[item_idx]\n                            if current_weight <= capacity:\n                                # Repair complete for this sample\n                                break\n            \n            # 3. Evaluation\n            objective_values = np.dot(repaired_samples, values)\n\n            # 4. Update Best-So-Far\n            current_best_value = np.max(objective_values)\n            if current_best_value > best_overall_value:\n                best_overall_value = current_best_value\n            \n            # 5. Elite Selection\n            # Get indices of the top m_elite samples\n            elite_indices = np.argsort(objective_values)[-m_elite:]\n            elite_samples = repaired_samples[elite_indices]\n\n            # 6. Parameter Update\n            # a. MLE estimate\n            theta_hat = np.mean(elite_samples, axis=0)\n            \n            # b. Smoothing\n            theta = ALPHA * theta_hat + (1 - ALPHA) * theta\n            \n            # c. Projection\n            theta = np.clip(theta, EPSILON, 1 - EPSILON)\n\n        final_results.append(int(best_overall_value))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n\n```"
        }
    ]
}