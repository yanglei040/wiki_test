{
    "hands_on_practices": [
        {
            "introduction": "The fundamental challenge in global optimization is distinguishing a true global minimum from numerous \"deceptive\" local minima. Before we can deploy complex algorithms, it's crucial to build an intuition for what these landscapes look like. This exercise  asks you to analytically identify a function where a local minimum is not the global one, reinforcing the very reason global optimization methods are necessary.",
            "id": "2176795",
            "problem": "In the study of optimization, a critical distinction is made between local and global optima. A function $f(x)$ has a local minimum at a point $c$ if $f(c) \\leq f(x)$ for all $x$ in some open interval containing $c$. A function has a global minimum on a given domain at a point $d$ if $f(d) \\leq f(x)$ for all $x$ in that domain.\n\nConsider the task of finding a simple, non-constant polynomial function $p(x)$ which, when analyzed over the closed interval $[-5, 5]$, possesses at least one local minimum that is *not* a global minimum on that same interval.\n\nWhich of the following polynomial functions satisfies this condition on the interval $[-5, 5]$?\n\nA. $p(x) = x^{2} - 10$\n\nB. $p(x) = x^{3} - 12x$\n\nC. $p(x) = 5x + 2$\n\nD. $p(x) = x^{4} - 8x^{2} + 10$\n\nE. $p(x) = -x^{2} - 3x$",
            "solution": "We are asked to find, among the given polynomials, one that has at least one local minimum in the interval $[-5,5]$ that is not a global minimum on the same interval. By the Closed Interval Method, a continuous function on a closed interval attains its absolute extrema at critical points (where the derivative is zero or undefined) and at the endpoints. For polynomials, derivatives are defined everywhere, so we check critical points (where $p'(x)=0$) and endpoints. To classify local extrema, we use the second derivative test: if $p''(c)>0$, then $c$ is a local minimum; if $p''(c)<0$, then $c$ is a local maximum.\n\nAnalyze each option:\n\nOption A: $p(x)=x^{2}-10$. Then\n$$\np'(x)=2x,\\quad p''(x)=2.\n$$\nThe only critical point is $x=0$, and since $p''(0)=2>0$, $x=0$ is a local minimum. Its value is\n$$\np(0)=-10.\n$$\nAt the endpoints,\n$$\np(-5)=25-10=15,\\quad p(5)=25-10=15.\n$$\nThus the global minimum on $[-5,5]$ is at $x=0$ with value $-10$, which is also the (only) local minimum. There is no local minimum that is not global. This option does not satisfy the condition.\n\nOption B: $p(x)=x^{3}-12x$. Then\n$$\np'(x)=3x^{2}-12=3(x^{2}-4),\\quad p''(x)=6x.\n$$\nCritical points occur at $x=\\pm 2$. The second derivative test gives\n$$\np''(-2)=-12<0 \\Rightarrow x=-2 \\text{ is a local maximum},\\quad p''(2)=12>0 \\Rightarrow x=2 \\text{ is a local minimum}.\n$$\nEvaluate $p$ at critical points and endpoints:\n$$\np(2)=8-24=-16,\\quad p(-2)=-8+24=16,\n$$\n$$\np(-5)=-125+60=-65,\\quad p(5)=125-60=65.\n$$\nThe smallest value among these is $-65$ at $x=-5$, so the global minimum on $[-5,5]$ is at $x=-5$ with value $-65$. The point $x=2$ is a local minimum with value $-16$, which is strictly greater than $-65$, hence it is not a global minimum. Therefore, this option satisfies the condition.\n\nOption C: $p(x)=5x+2$. Then\n$$\np'(x)=5\\neq 0 \\text{ for all } x,\n$$\nso there are no critical points and no local minima in the interior. The function is strictly increasing, so the minimum on $[-5,5]$ occurs at $x=-5$ and is global. There is no local minimum that is not global. This option does not satisfy the condition.\n\nOption D: $p(x)=x^{4}-8x^{2}+10$. Then\n$$\np'(x)=4x^{3}-16x=4x(x^{2}-4),\\quad p''(x)=12x^{2}-16.\n$$\nCritical points are $x=0,\\pm 2$. The second derivative test yields\n$$\np''(0)=-16<0 \\Rightarrow x=0 \\text{ is a local maximum},\\quad p''(\\pm 2)=32>0 \\Rightarrow x=\\pm 2 \\text{ are local minima}.\n$$\nEvaluate:\n$$\np(\\pm 2)=16-32+10=-6,\\quad p(0)=10,\n$$\n$$\np(\\pm 5)=625-200+10=435.\n$$\nThe smallest values are $-6$ at $x=\\pm 2$, and these are also the global minima on $[-5,5]$. Hence the local minima are global; there is no local minimum that is not global. This option does not satisfy the condition.\n\nOption E: $p(x)=-x^{2}-3x$. Then\n$$\np'(x)=-2x-3,\\quad p''(x)=-2<0,\n$$\nso the vertex is a local maximum; there is no local minimum in the interior. On $[-5,5]$, minima occur at endpoints and are global. There is no local minimum that is not global. This option does not satisfy the condition.\n\nTherefore, the unique option that has at least one local minimum that is not a global minimum on $[-5,5]$ is Option B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Particle Swarm Optimization (PSO) is a powerful method that mimics the collective intelligence of a flock of birds. The success of the swarm depends on a delicate balance between individual exploration (cognitive influence) and group consensus (social influence). This problem  is a thought experiment that explores how tuning the cognitive ($c_1$) and social ($c_2$) parameters can dramatically alter the swarm's behavior, leading to either productive exploration or premature convergence to a suboptimal solution.",
            "id": "2176755",
            "problem": "An engineer is using a Particle Swarm Optimization (PSO) algorithm to find the minimum of a complex, high-dimensional, non-convex function that is known to have many local minima. The motion of each particle in the swarm is governed by the following standard update rules for its position $x_i$ and velocity $v_i$ at discrete time step $t$:\n\n$$v_{i}(t+1) = w v_{i}(t) + c_1 r_1 (p_{i} - x_i(t)) + c_2 r_2 (p_{g} - x_i(t))$$\n$$x_{i}(t+1) = x_{i}(t) + v_{i}(t+1)$$\n\nHere, $w$ is the inertia weight, $p_i$ is the best position found so far by particle $i$ (its personal best), and $p_g$ is the best position found so far by any particle in the entire swarm (the global best). The parameters $c_1$ and $c_2$ are positive constants known as the cognitive and social coefficients, respectively, which control the influence of the personal and global best positions. The terms $r_1$ and $r_2$ are random numbers independently drawn from a uniform distribution on $[0, 1]$ at each update step for each particle.\n\nThe engineer experiments with two different parameter configurations, keeping all other parameters ($w$, swarm size, number of iterations) constant.\n\n*   **Parameter Set A:** $c_1 = 2.5$ and $c_2 = 0.1$.\n*   **Parameter Set B:** $c_1 = 0.1$ and $c_2 = 2.5$.\n\nBased on the principles of the PSO algorithm, what is the most likely search behavior to be observed for Parameter Set A and Parameter Set B, respectively?\n\nA. Premature convergence to a single suboptimal local minimum; Stagnation, with particles clustering at multiple distinct local optima.\n\nB. Stagnation, with particles clustering at multiple distinct local optima; Premature convergence to a single suboptimal local minimum.\n\nC. Uncontrolled divergence where particles leave the bounds of the search space; Rapid and efficient convergence to the true global minimum.\n\nD. Aimless wandering of particles without any clear convergence pattern; Stagnation, with particles clustering at multiple distinct local optima.\n\nE. Rapid and efficient convergence to the true global minimum; Premature convergence to a single suboptimal local minimum.",
            "solution": "We analyze the standard PSO updates\n$$v_{i}(t+1)=w\\,v_{i}(t)+c_{1}r_{1}\\big(p_{i}-x_{i}(t)\\big)+c_{2}r_{2}\\big(p_{g}-x_{i}(t)\\big),$$\n$$x_{i}(t+1)=x_{i}(t)+v_{i}(t+1).$$\nAt each update, $r_{1},r_{2}\\in[0,1]$. Taking norms and using the triangle inequality gives\n$$\\|v_{i}(t+1)\\|\\leq w\\|v_{i}(t)\\|+c_{1}r_{1}\\|p_{i}-x_{i}(t)\\|+c_{2}r_{2}\\|p_{g}-x_{i}(t)\\|.$$\nSince $r_{1},r_{2}\\in[0,1]$, the cognitive and social contributions are bounded above by $c_{1}\\|p_{i}-x_{i}(t)\\|$ and $c_{2}\\|p_{g}-x_{i}(t)\\|$, respectively. Thus, the relative influence of the personal-best attraction versus the global-best attraction is controlled directly by the magnitudes of $c_{1}$ and $c_{2}$.\n\nFor Parameter Set A, $c_{1}\\gg c_{2}$. Therefore, for each particle $i$, the dominant pull in the velocity update is toward its own best $p_{i}$, since\n$$c_{1}r_{1}\\|p_{i}-x_{i}(t)\\|\\gg c_{2}r_{2}\\|p_{g}-x_{i}(t)\\|$$\nwhenever the distances are comparable, due to the coefficient disparity. Consequently, particles tend to exploit around their personal bests with relatively weak coupling to the swarmâ€™s global best. This promotes formation of multiple clusters, each around different $p_{i}$, and limits information sharing, a known cause of stagnation where particles settle near various local optima without coordinated convergence.\n\nFor Parameter Set B, $c_{2}\\gg c_{1}$. Then the social term dominates, and for each particle\n$$c_{2}r_{2}\\|p_{g}-x_{i}(t)\\|\\gg c_{1}r_{1}\\|p_{i}-x_{i}(t)\\|.$$\nThe strong attraction toward $p_{g}$ causes rapid contraction of the swarm toward the current global best. In non-convex landscapes with many local minima, this strong social pull frequently leads to the entire swarm collapsing prematurely around a single suboptimal local minimum (premature convergence), since exploration driven by the cognitive term is weak.\n\nTherefore, under Set A (high $c_{1}$, low $c_{2}$) the likely behavior is stagnation with clustering at multiple distinct local optima, and under Set B (low $c_{1}$, high $c_{2}$) the likely behavior is premature convergence to a single suboptimal local minimum. Among the options, this corresponds to choice B.\n\nNo other listed behavior aligns with these parameter regimes: uncontrolled divergence (option C) is not implied by increasing either $c_{1}$ or $c_{2}$ alone under fixed $w$, aimless wandering (option D) contradicts the strong attraction terms, and rapid efficient convergence to the true global minimum (option E) is not generally guaranteed in high-dimensional, multimodal landscapes with such imbalanced coefficients.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "While PSO relies on a velocity-based update, other metaheuristics like Genetic Algorithms (GA) and Differential Evolution (DE) use different mechanisms to generate new candidate solutions. This practice  provides a concrete, step-by-step calculation comparing how DE's vector-based mutation and GA's crossover-based approach perform on a tricky, \"deceptive\" function. By working through the mechanics, you will gain a deeper appreciation for how different algorithmic strategies navigate complex search spaces.",
            "id": "2176751",
            "problem": "An optimization task is designed to find the minimum of a two-dimensional \"deceptive\" function, $f(x, y)$, defined over the domain $x, y \\in [-10, 10]$. The function has a very narrow, deep global minimum and a wide, shallow local minimum, making it challenging for many optimization algorithms. The function is given by:\n$$f(x, y) = -5 \\exp\\left(-\\frac{(x - 4)^2 + (y - 4)^2}{0.5}\\right) - 2 \\exp\\left(-\\frac{(x + 2)^2 + (y + 2)^2}{10}\\right)$$\nThe global minimum of this function is located at the point $\\mathbf{x}_g = (4, 4)$.\n\nWe are analyzing the behavior of two population-based optimization algorithms: Differential Evolution (DE) and a real-coded Genetic Algorithm (GA). At a particular generation, we focus on a specific \"target\" individual in the population represented by the vector $\\mathbf{x}_i = (-2.1, -1.9)$. We want to compare how a single step of each algorithm updates this individual's position.\n\nFor the **Differential Evolution** algorithm, a trial vector $\\mathbf{v}_i$ is generated using the `DE/rand/1` strategy, defined as $\\mathbf{v}_i = \\mathbf{x}_{r1} + F (\\mathbf{x}_{r2} - \\mathbf{x}_{r3})$, where $\\mathbf{x}_{r1}, \\mathbf{x}_{r2}, \\mathbf{x}_{r3}$ are distinct individuals randomly chosen from the population, and $F$ is the mutation factor. For this specific step, the chosen individuals are:\n- $\\mathbf{x}_{r1} = (-1.8, -2.2)$\n- $\\mathbf{x}_{r2} = (3.9, 4.1)$\n- $\\mathbf{x}_{r3} = (-2.0, -2.3)$\nThe mutation factor is $F = 0.8$.\n\nFor the **Genetic Algorithm**, a new candidate individual $\\mathbf{c}'$ is created from the target $\\mathbf{x}_i$ through crossover and mutation.\n- **Crossover**: Arithmetic crossover is performed between the target $\\mathbf{x}_i$ and a selected parent $\\mathbf{x}_p = (-1.9, -2.0)$. The child vector $\\mathbf{c}$ is a weighted average: $\\mathbf{c} = \\alpha \\mathbf{x}_i + (1-\\alpha) \\mathbf{x}_p$, with a weight $\\alpha = 0.5$.\n- **Mutation**: The resulting child vector $\\mathbf{c}$ is then mutated by adding a mutation vector $\\mathbf{m} = (0.1, -0.2)$. The final candidate is $\\mathbf{c}' = \\mathbf{c} + \\mathbf{m}$.\n\nTo compare the exploratory capability of these two updates, calculate the ratio of the Euclidean distance from the DE trial vector to the global optimum, to the Euclidean distance from the GA candidate vector to the global optimum. That is, compute the value of $\\frac{\\|\\mathbf{v}_i - \\mathbf{x}_g\\|}{\\|\\mathbf{c}' - \\mathbf{x}_g\\|}$.\n\nRound your final answer to three significant figures.",
            "solution": "We are asked to compute the ratio of Euclidean distances from the global optimum $\\mathbf{x}_{g}=(4,4)$ for two updated candidates produced by DE and GA, respectively.\n\nFor Differential Evolution with the DE/rand/1 strategy, the trial vector is\n$$\n\\mathbf{v}_{i}=\\mathbf{x}_{r1}+F\\left(\\mathbf{x}_{r2}-\\mathbf{x}_{r3}\\right),\n$$\nwith $\\mathbf{x}_{r1}=(-1.8,-2.2)$, $\\mathbf{x}_{r2}=(3.9,4.1)$, $\\mathbf{x}_{r3}=(-2.0,-2.3)$, and $F=0.8$. First compute the difference:\n$$\n\\mathbf{x}_{r2}-\\mathbf{x}_{r3}=(3.9-(-2.0),\\,4.1-(-2.3))=(5.9,\\,6.4).\n$$\nScale by $F$:\n$$\nF(\\mathbf{x}_{r2}-\\mathbf{x}_{r3})=0.8\\cdot(5.9,\\,6.4)=(4.72,\\,5.12).\n$$\nAdd $\\mathbf{x}_{r1}$:\n$$\n\\mathbf{v}_{i}=(-1.8,-2.2)+(4.72,5.12)=(2.92,\\,2.92).\n$$\nThe Euclidean distance from $\\mathbf{v}_{i}$ to $\\mathbf{x}_{g}$ is\n$$\n\\|\\mathbf{v}_{i}-\\mathbf{x}_{g}\\|=\\|(2.92-4,\\,2.92-4)\\|=\\|(-1.08,\\,-1.08)\\|=\\sqrt{(-1.08)^{2}+(-1.08)^{2}}=1.08\\sqrt{2}.\n$$\n\nFor the Genetic Algorithm, arithmetic crossover with weight $\\alpha=0.5$ between $\\mathbf{x}_{i}=(-2.1,-1.9)$ and $\\mathbf{x}_{p}=(-1.9,-2.0)$ yields\n$$\n\\mathbf{c}=\\alpha\\mathbf{x}_{i}+(1-\\alpha)\\mathbf{x}_{p}=0.5(-2.1,-1.9)+0.5(-1.9,-2.0)=(-2.0,\\,-1.95).\n$$\nMutation adds $\\mathbf{m}=(0.1,-0.2)$, giving\n$$\n\\mathbf{c}'=\\mathbf{c}+\\mathbf{m}=(-2.0+0.1,\\,-1.95-0.2)=(-1.9,\\,-2.15).\n$$\nThe Euclidean distance from $\\mathbf{c}'$ to $\\mathbf{x}_{g}$ is\n$$\n\\|\\mathbf{c}'-\\mathbf{x}_{g}\\|=\\|(-1.9-4,\\,-2.15-4)\\|=\\|(-5.9,\\,-6.15)\\|=\\sqrt{(5.9)^{2}+(6.15)^{2}}=\\sqrt{34.81+37.8225}=\\sqrt{72.6325}.\n$$\n\nTherefore, the required ratio is\n$$\nR=\\frac{\\|\\mathbf{v}_{i}-\\mathbf{x}_{g}\\|}{\\|\\mathbf{c}'-\\mathbf{x}_{g}\\|}=\\frac{1.08\\sqrt{2}}{\\sqrt{72.6325}}.\n$$\nNumerically,\n$$\n1.08\\sqrt{2}\\approx 1.527350647,\\quad \\sqrt{72.6325}\\approx 8.5224703,\\quad R\\approx \\frac{1.527350647}{8.5224703}\\approx 0.179.\n$$\nRounded to three significant figures, the ratio is $0.179$.",
            "answer": "$$\\boxed{0.179}$$"
        }
    ]
}