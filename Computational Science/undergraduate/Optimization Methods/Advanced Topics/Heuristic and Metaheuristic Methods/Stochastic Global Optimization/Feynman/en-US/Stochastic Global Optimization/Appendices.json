{
    "hands_on_practices": [
        {
            "introduction": "The multistart method is a cornerstone of global optimization, but a crucial question always arises: how many restarts are sufficient? This exercise guides you through the fundamental probabilistic calculation needed to answer this question. By treating each restart as an independent trial, you will derive a formula that connects the desired probability of success to the required number of attempts, providing a solid theoretical foundation for designing stochastic search strategies. ",
            "id": "3186484",
            "problem": "Consider the $d$-dimensional Rastrigin function defined by $f(x)=\\sum_{i=1}^{d}\\big(x_{i}^{2}-10\\cos(2\\pi x_{i})+10\\big)$ on the hyper-rectangle (box) domain $[-5.12,5.12]^{d}$. A stochastic multistart strategy is used: each restart independently samples an initial point $x^{(0)}$ uniformly at random from the domain and then applies a deterministic local optimization method that converges to a local minimum whose basin of attraction contains $x^{(0)}$. Assume the basin of attraction of the global minimum has a volume fraction $p$ with $0p1$, meaning a uniformly sampled initial point lies in this basin with probability $p$. Using only fundamental probability principles for independent trials, derive a closed-form expression for the smallest integer number of restarts $m$ such that the probability of having reached the global minimum at least once across $m$ independent restarts is at least $0.95$. Express your final answer as a function of $p$ and $0.95$, and provide it as a single closed-form expression. No rounding is required.",
            "solution": "The problem is to find the smallest integer number of restarts, $m$, for a stochastic multistart optimization method to ensure that the probability of finding the global minimum at least once is at least $0.95$.\n\nFirst, we formalize the problem using probability theory. The problem statement provides the following key information:\n1.  Each restart is an independent trial.\n2.  An initial point for a restart is sampled uniformly at random from the domain.\n3.  The probability that a single, randomly initiated restart converges to the global minimum is given by the volume fraction of its basin of attraction, which is denoted by $p$.\n\nLet $S$ be the event that a single restart successfully finds the global minimum. The probability of this event is given as:\n$$P(S) = p$$\nThe problem states that $0  p  1$.\n\nLet $F$ be the event that a single restart fails to find the global minimum. This is the complement of event $S$. The probability of failure is:\n$$P(F) = 1 - P(S) = 1 - p$$\n\nWe are performing $m$ independent restarts. Let $A$ be the event that the global minimum is found at least once across these $m$ restarts. The problem requires us to find the smallest integer $m$ such that:\n$$P(A) \\ge 0.95$$\n\nIt is computationally more straightforward to first calculate the probability of the complement event, $A^c$. The event $A^c$ is the scenario where none of the $m$ restarts find the global minimum, meaning all $m$ restarts fail.\n$$A^c = \\text{failure on all } m \\text{ restarts}$$\n\nSince the restarts are independent events, the probability of all $m$ failing is the product of their individual probabilities of failure:\n$$P(A^c) = \\underbrace{P(F) \\times P(F) \\times \\dots \\times P(F)}_{m \\text{ times}} = (P(F))^m = (1 - p)^m$$\n\nThe probability of event $A$ (at least one success) is related to the probability of its complement $A^c$ (no successes) by the formula $P(A) = 1 - P(A^c)$. Therefore:\n$$P(A) = 1 - (1 - p)^m$$\n\nNow, we apply the condition specified in the problem:\n$$1 - (1 - p)^m \\ge 0.95$$\n\nWe need to solve this inequality for $m$. Rearranging the terms, we get:\n$$1 - 0.95 \\ge (1 - p)^m$$\n$$0.05 \\ge (1 - p)^m$$\n\nTo solve for the exponent $m$, we take the natural logarithm of both sides. Since the natural logarithm function, $\\ln(x)$, is a strictly increasing function, applying it to both sides of an inequality preserves the direction of the inequality:\n$$\\ln(0.05) \\ge \\ln((1 - p)^m)$$\n\nUsing the logarithm property $\\ln(x^y) = y \\ln(x)$, we have:\n$$\\ln(0.05) \\ge m \\ln(1 - p)$$\n\nTo isolate $m$, we must divide by $\\ln(1 - p)$. The direction of the inequality upon division depends on the sign of $\\ln(1 - p)$. The problem states that $0  p  1$, which implies that $0  1 - p  1$. The natural logarithm of any number between $0$ and $1$ is negative. Thus, $\\ln(1 - p)  0$.\n\nWhen dividing an inequality by a negative number, the direction of the inequality sign must be reversed:\n$$\\frac{\\ln(0.05)}{\\ln(1 - p)} \\le m$$\nThis can be written as:\n$$m \\ge \\frac{\\ln(0.05)}{\\ln(1 - p)}$$\nNote that $\\ln(0.05)$ is also negative, so the ratio $\\frac{\\ln(0.05)}{\\ln(1 - p)}$ is a positive number, as expected for a number of trials.\n\nThe problem requires the smallest *integer* value of $m$ that satisfies this condition. The smallest integer greater than or equal to a given real number $x$ is given by the ceiling function, $\\lceil x \\rceil$. Therefore, the smallest integer $m$ is:\n$$m = \\left\\lceil \\frac{\\ln(0.05)}{\\ln(1 - p)} \\right\\rceil$$\n\nThis expression provides the required closed-form for the smallest integer number of restarts $m$ as a function of the probability $p$ and the desired success rate of $0.95$.",
            "answer": "$$\\boxed{\\left\\lceil \\frac{\\ln(0.05)}{\\ln(1 - p)} \\right\\rceil}$$"
        },
        {
            "introduction": "The previous exercise assumed we knew the probability $p$ of finding the global minimum in a single attempt. In practice, this value is unknown and must be estimated. This hands-on coding problem challenges you to do just that by empirically probing the landscape of a function to estimate the size of a basin of attraction. You will use Monte Carlo sampling and a local solver to connect the abstract concept of a basin to a concrete numerical estimate, which you will then use to determine a practical seeding density for a multistart search. ",
            "id": "3186406",
            "problem": "You are asked to design and implement a complete, runnable program that empirically estimates an attraction basin for a specified local solver and uses this estimate to determine a suitable multistart seeding density for stochastic global optimization. The exercise must start from core definitions and well-tested facts in optimization and probability and avoid relying on any shortcut formulas beyond those derivable from such fundamentals.\n\nDefinitions and principles to use:\n- A deterministic local solver applied to a differentiable objective function $f:\\mathbb{R}^n \\to \\mathbb{R}$ attempts to produce a point $\\hat{x}$ such that $\\nabla f(\\hat{x}) \\approx 0$ and $f(\\hat{x}) \\le f(x_0)$ for a given initial point $x_0$. The set of initial points whose solver trajectories converge to a particular local minimum $x^{\\star}$ is called the attraction basin of $x^{\\star}$ for that solver.\n- An empirical estimate of attraction behavior can be obtained by Monte Carlo sampling. If $S$ independent initial points are sampled from a region, and $K$ solver runs converge to $x^{\\star}$ by a specified proximity criterion, then the Monte Carlo estimator $\\hat{p} = K/S$ is an unbiased estimator of the probability $p$ that a random point drawn from that region leads to convergence to $x^{\\star}$.\n- For multistart, with independent uniformly distributed starting points over a bounded domain $D \\subset \\mathbb{R}^n$ of finite area $A(D)$, if a particular subregion $B \\subset D$ has measure fraction $p_B = A(B)/A(D)$, then the probability that at least one of $N$ independent starts falls in $B$ is $1 - (1 - p_B)^N$. Solving $1 - (1 - p_B)^N \\ge q$ for $N$ gives the minimal number of starts to achieve coverage probability at least $q$.\n\nYour program must implement the following procedure in two dimensions $n=2$.\n\n1. Given a twice continuously differentiable objective $f:\\mathbb{R}^2\\to \\mathbb{R}$, a candidate local minimum $m \\in \\mathbb{R}^2$, a bounded rectangular domain $D = [\\ell_x,u_x] \\times [\\ell_y,u_y]$ with $A(D) = (u_x - \\ell_x)(u_y - \\ell_y)$, a list of radii $\\mathcal{R} = \\{r_1,\\dots,r_k\\}$ with each $r_i  0$, a sample size $S \\in \\mathbb{N}$, a success threshold $\\tau \\in (0,1)$, a target multistart coverage probability $q \\in (0,1)$, and a Euclidean proximity tolerance $\\varepsilon  0$, estimate an empirical attraction basin radius for a specified local solver as follows:\n   - For each radius $r \\in \\mathcal{R}$, independently sample $S$ points uniformly from the disk $B(m,r) = \\{x \\in \\mathbb{R}^2 : \\|x - m\\|_2 \\le r\\}$ intersected with $D$. For each sampled point $x_0$, run the local solver on $f$ starting at $x_0$. Declare a success if the solver reports successful termination and returns $\\hat{x}$ with $\\|\\hat{x} - m\\|_2 \\le \\varepsilon$. Compute the empirical success rate $\\hat{p}(r) = K(r)/S$, where $K(r)$ is the success count at radius $r$.\n   - Define the empirical attraction radius $r^{\\star}$ as the smallest $r \\in \\mathcal{R}$ such that $\\hat{p}(r) \\ge \\tau$. If no such $r$ exists, define $r^{\\star}$ to be the $r \\in \\mathcal{R}$ that maximizes $\\hat{p}(r)$, breaking ties by choosing the smallest such $r$. Let $p^{\\star} = \\hat{p}(r^{\\star})$.\n   - Approximate the basin area by $\\pi (r^{\\star})^2$ and the corresponding domain fraction by $p_{\\mathrm{area}} = \\min\\{1, \\pi (r^{\\star})^2 / A(D)\\}$. Using independence of multistart seeds drawn uniformly from $D$, the minimal number of starts $N$ to ensure at least one start falls in the approximate basin with probability at least $q$ satisfies $1 - (1 - p_{\\mathrm{area}})^N \\ge q$. Solve for $N$ as the smallest integer $N$ with this property.\n\n2. Implement the above using standard local solvers:\n   - Use the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method where specified, supplying an analytic gradient when available.\n   - Use the Nelder–Mead simplex method where specified, without a gradient.\n\n3. Sampling uniform points in a disk must be unbiased: if $U \\sim \\mathrm{Uniform}[0,1]$ and $\\Theta \\sim \\mathrm{Uniform}[0,2\\pi)$ are independent, then a point offset with polar coordinates $(\\rho,\\theta)$ where $\\rho = r \\sqrt{U}$ and $\\theta = \\Theta$ is uniformly distributed over the disk of radius $r$.\n\nQuantities to compute and report:\n- For each test case, compute the triple $[N, r^{\\star}, p^{\\star}]$. The number $N$ must be the smallest integer satisfying the multistart coverage condition. The value $r^{\\star}$ must be the empirically selected radius, and $p^{\\star}$ must be the corresponding empirical success rate. Express probabilities as decimals (not percentages). No physical units are involved.\n\nTest suite:\nYour program must run the following three test cases and aggregate their outputs.\n\n- Test case $1$ (Himmelblau function, Broyden–Fletcher–Goldfarb–Shanno method):\n  - Objective: Himmelblau’s function $f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$.\n  - Candidate minimum: $m = (3,2)$.\n  - Domain: $D = [-6,6] \\times [-6,6]$.\n  - Radii: $\\mathcal{R} = \\{0.15, 0.30, 0.60, 1.20\\}$.\n  - Samples per radius: $S = 120$.\n  - Success threshold: $\\tau = 0.9$.\n  - Target coverage: $q = 0.95$.\n  - Proximity tolerance: $\\varepsilon = 0.01$.\n\n- Test case $2$ (Himmelblau function, Nelder–Mead simplex method):\n  - Objective: same as test case $1$.\n  - Candidate minimum: $m = (-2.805118, 3.131312)$.\n  - Domain: $D = [-6,6] \\times [-6,6]$.\n  - Radii: $\\mathcal{R} = \\{0.10, 0.30, 0.60, 1.00\\}$.\n  - Samples per radius: $S = 100$.\n  - Success threshold: $\\tau = 0.8$.\n  - Target coverage: $q = 0.90$.\n  - Proximity tolerance: $\\varepsilon = 0.02$.\n\n- Test case $3$ (Rosenbrock function, Broyden–Fletcher–Goldfarb–Shanno method):\n  - Objective: Rosenbrock’s function $f(x,y) = (1-x)^2 + 100(y - x^2)^2$.\n  - Candidate minimum: $m = (1,1)$.\n  - Domain: $D = [-2,2] \\times [-2,2]$.\n  - Radii: $\\mathcal{R} = \\{0.05, 0.10, 0.20, 0.40\\}$.\n  - Samples per radius: $S = 150$.\n  - Success threshold: $\\tau = 0.95$.\n  - Target coverage: $q = 0.99$.\n  - Proximity tolerance: $\\varepsilon = 0.01$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, where each test case contributes its own bracketed triple. For example, an output with three test cases must appear as\n\"[[N1,r1,p1],[N2,r2,p2],[N3,r3,p3]]\"\nwhere each $N_i$ is an integer and each $r_i$ and $p_i$ are decimal numbers. Round $r_i$ and $p_i$ to six digits after the decimal point in the printed output. No other text should be printed.",
            "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to proceed with a solution. The task is to implement a procedure to empirically estimate the attraction basin for a local minimum of a function and then determine the necessary number of starting points for a multistart optimization strategy to find that minimum with a certain probability.\n\nThe procedure is executed in two main stages for an objective function $f: \\mathbb{R}^2 \\to \\mathbb{R}$ and a candidate local minimum $m \\in \\mathbb{R}^2$.\n\nFirst, we empirically estimate the attraction radius. For a set of test radii $\\mathcal{R} = \\{r_1, \\dots, r_k\\}$, we perform a Monte Carlo simulation. For each radius $r \\in \\mathcal{R}$, we generate $S$ initial points $\\{x_0^{(1)}, \\dots, x_0^{(S)}\\}$ sampled uniformly from the disk $B(m, r)$ of radius $r$ centered at $m$. The problem specifies that for the given test cases, these disks are fully contained within the search domain $D$, simplifying sampling. Uniform sampling in a disk is achieved using inverse transform sampling in polar coordinates. A point's radial coordinate $\\rho$ and angular coordinate $\\theta$ are generated via $\\rho = r \\sqrt{U_1}$ and $\\theta = 2\\pi U_2$, where $U_1, U_2$ are independent random variables uniformly distributed on $[0,1]$. This ensures that generated points have a uniform probability density over the area of the disk.\n\nFor each sampled point $x_0$, a local optimization solver (either BFGS or Nelder-Mead, as specified) is initiated. A trial is deemed a \"success\" if two conditions are met: the solver converges successfully, and the found minimum $\\hat{x}$ is within a Euclidean distance $\\varepsilon$ of the target minimum $m$, i.e., $\\|\\hat{x} - m\\|_2 \\le \\varepsilon$. The number of successes $K(r)$ for a given radius $r$ is counted. The empirical probability of convergence, or success rate, is then estimated as $\\hat{p}(r) = K(r)/S$.\n\nAfter computing $\\hat{p}(r)$ for all $r \\in \\mathcal{R}$, the empirical attraction radius $r^{\\star}$ is selected according to the specified rule. We identify the set of radii for which the success rate meets or exceeds a given threshold $\\tau$, i.e., $\\{r \\in \\mathcal{R} \\mid \\hat{p}(r) \\ge \\tau\\}$. If this set is non-empty, $r^{\\star}$ is chosen as the minimum radius in this set. If no radius meets the threshold, $r^{\\star}$ is chosen as the radius that yields the maximum observed success rate, with ties broken by selecting the smallest such radius. The success rate corresponding to $r^{\\star}$ is denoted $p^{\\star} = \\hat{p}(r^{\\star})$.\n\nSecond, using this empirical radius $r^{\\star}$, we calculate the required number of starting points $N$ for a multistart method. The basin of attraction for the minimum $m$ is approximated as a disk of radius $r^{\\star}$, with an area of $A_{\\text{basin}} = \\pi (r^{\\star})^2$. The total search domain $D$ is a rectangle $[\\ell_x, u_x] \\times [\\ell_y, u_y]$ with area $A(D) = (u_x - \\ell_x)(u_y - \\ell_y)$. The probability that a single point chosen uniformly at random from $D$ falls into the approximated basin is the ratio of their areas, denoted $p_{\\text{area}} = \\min(1, A_{\\text{basin}} / A(D))$.\n\nFor a multistart method with $N$ independent starting points drawn uniformly from $D$, the probability that all $N$ points miss the basin is $(1 - p_{\\text{area}})^N$. Consequently, the probability that at least one point falls within the basin is $1 - (1 - p_{\\text{area}})^N$. We must find the smallest integer $N$ that satisfies the target coverage probability $q$:\n$$1 - (1 - p_{\\text{area}})^N \\ge q$$\nRearranging the inequality, we get $(1 - p_{\\text{area}})^N \\le 1 - q$. Taking the natural logarithm of both sides gives $N \\log(1 - p_{\\text{area}}) \\le \\log(1 - q)$. Since $p_{\\text{area}} \\in (0, 1)$, $\\log(1 - p_{\\text{area}})$ is negative. Dividing by it reverses the inequality sign:\n$$N \\ge \\frac{\\log(1-q)}{\\log(1-p_{\\text{area}})}$$\nThe smallest integer $N$ satisfying this is obtained by taking the ceiling of the right-hand side, $N = \\left\\lceil \\frac{\\log(1-q)}{\\log(1-p_{\\text{area}})} \\right\\rceil$. In the special case where $p_{\\text{area}} = 1$, a single starting point guarantees coverage, so $N=1$.\n\nThe implementation will use `numpy` for numerical computations and random number generation, and `scipy.optimize.minimize` for the local search algorithms. The analytic gradients for the BFGS method are required, as specified.\n\nFor Himmelblau's function $f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$, the partial derivatives are:\n$$ \\frac{\\partial f}{\\partial x} = 4x(x^2 + y - 11) + 2(x + y^2 - 7) $$\n$$ \\frac{\\partial f}{\\partial y} = 2(x^2 + y - 11) + 4y(x + y^2 - 7) $$\nFor Rosenbrock's function $f(x,y) = (1-x)^2 + 100(y - x^2)^2$, the partial derivatives are:\n$$ \\frac{\\partial f}{\\partial x} = -2(1-x) - 400x(y-x^2) $$\n$$ \\frac{\\partial f}{\\partial y} = 200(y-x^2) $$\nThese functions and their gradients are implemented to solve the test cases. A fixed seed for the random number generator ensures the reproducibility of the Monte Carlo simulation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    # Set a fixed seed for the random number generator for reproducibility.\n    np.random.seed(0)\n\n    # Define objective functions and their gradients.\n    def himmelblau(p):\n        x, y = p\n        return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n\n    def grad_himmelblau(p):\n        x, y = p\n        df_dx = 4 * x * (x**2 + y - 11) + 2 * (x + y**2 - 7)\n        df_dy = 2 * (x**2 + y - 11) + 4 * y * (x + y**2 - 7)\n        return np.array([df_dx, df_dy])\n\n    def rosenbrock(p):\n        x, y = p\n        return (1 - x)**2 + 100 * (y - x**2)**2\n\n    def grad_rosenbrock(p):\n        x, y = p\n        df_dx = -2 * (1 - x) - 400 * x * (y - x**2)\n        df_dy = 200 * (y - x**2)\n        return np.array([df_dx, df_dy])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": himmelblau,\n            \"grad_f\": grad_himmelblau,\n            \"method\": \"BFGS\",\n            \"m\": np.array([3.0, 2.0]),\n            \"D\": [-6.0, 6.0, -6.0, 6.0],\n            \"R\": [0.15, 0.30, 0.60, 1.20],\n            \"S\": 120,\n            \"tau\": 0.9,\n            \"q\": 0.95,\n            \"eps\": 0.01,\n        },\n        {\n            \"f\": himmelblau,\n            \"grad_f\": None,\n            \"method\": \"Nelder-Mead\",\n            \"m\": np.array([-2.805118, 3.131312]),\n            \"D\": [-6.0, 6.0, -6.0, 6.0],\n            \"R\": [0.10, 0.30, 0.60, 1.00],\n            \"S\": 100,\n            \"tau\": 0.8,\n            \"q\": 0.90,\n            \"eps\": 0.02,\n        },\n        {\n            \"f\": rosenbrock,\n            \"grad_f\": grad_rosenbrock,\n            \"method\": \"BFGS\",\n            \"m\": np.array([1.0, 1.0]),\n            \"D\": [-2.0, 2.0, -2.0, 2.0],\n            \"R\": [0.05, 0.10, 0.20, 0.40],\n            \"S\": 150,\n            \"tau\": 0.95,\n            \"q\": 0.99,\n            \"eps\": 0.01,\n        },\n    ]\n\n    def sample_in_disk(center, radius, n_samples):\n        # Sample points uniformly from a disk of given center and radius.\n        r_vals = radius * np.sqrt(np.random.uniform(0, 1, size=n_samples))\n        theta_vals = 2 * np.pi * np.random.uniform(0, 1, size=n_samples)\n        x_coords = center[0] + r_vals * np.cos(theta_vals)\n        y_coords = center[1] + r_vals * np.sin(theta_vals)\n        return np.stack((x_coords, y_coords), axis=-1)\n\n    def run_case(case):\n        # Execute the full procedure for a single test case.\n        f, grad_f, method, m, domain, radii, S, tau, q, eps = (\n            case[\"f\"], case[\"grad_f\"], case[\"method\"], case[\"m\"], case[\"D\"],\n            case[\"R\"], case[\"S\"], case[\"tau\"], case[\"q\"], case[\"eps\"]\n        )\n\n        phat_results = {}\n        for r_test in radii:\n            initial_points = sample_in_disk(m, r_test, S)\n            success_count = 0\n            for x0 in initial_points:\n                res = minimize(f, x0, method=method, jac=grad_f, options={'maxiter': 1000})\n                if res.success and np.linalg.norm(res.x - m) = eps:\n                    success_count += 1\n            phat_results[r_test] = success_count / S\n        \n        # Determine r_star and p_star based on the specified rules.\n        successful_radii = [r for r, p in phat_results.items() if p = tau]\n        \n        if successful_radii:\n            r_star = min(successful_radii)\n        else:\n            max_p = -1.0\n            # Find max probability among all radii\n            for p in phat_results.values():\n                if p > max_p:\n                    max_p = p\n            # Find radii that achieve max_p, then choose the smallest\n            best_radii = [r for r, p in phat_results.items() if p == max_p]\n            r_star = min(best_radii)\n            \n        p_star = phat_results[r_star]\n\n        # Calculate the required number of multistarts, N.\n        lx, ux, ly, uy = domain\n        A_D = (ux - lx) * (uy - ly)\n        A_basin = np.pi * (r_star**2)\n        p_area = min(1.0, A_basin / A_D)\n\n        if p_area >= 1.0:\n            N = 1\n        elif p_area = 0.0:\n            N = float('inf') # Should not happen with r_star > 0\n        else:\n            N = math.ceil(math.log(1 - q) / math.log(1 - p_area))\n        \n        return int(N), r_star, p_star\n\n    results = []\n    for case in test_cases:\n        result = run_case(case)\n        results.append(result)\n\n    # Format the results into the required string format.\n    formatted_results = []\n    for N, r_star, p_star in results:\n        formatted_results.append(f\"[{int(N)},{r_star:.6f},{p_star:.6f}]\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While increasing the number of random starts improves the chance of finding a global minimum, smarter sampling strategies can achieve the same goal with less computational effort. This exercise explores antithetic variates, a variance reduction technique that exploits problem symmetry to improve search efficiency. By analyzing how paired, symmetric starting points explore the search space, you will quantify the benefits of this structured approach compared to purely independent sampling, gaining insight into why the structure of your samples matters. ",
            "id": "3186487",
            "problem": "Consider a nonconvex objective function $f:\\mathbb{R}\\to\\mathbb{R}$ that is symmetric in the sense that $f(x)=f(-x)$, with two symmetric global minimizers at $x=\\pm m$ attaining the objective value $f_{g}$ and a central local minimizer at $x=0$ attaining the objective value $f_{\\ell}$, where $f_{\\ell}f_{g}$. A deterministic local search map $T:\\mathbb{R}\\to\\{\\pm m,0\\}$ sends a seed $x$ to the point of attraction it converges to, with the following property: there exists a threshold $a0$ such that $T(x)\\in\\{\\pm m\\}$ if and only if $|x|\\ge a$, and $T(x)=0$ if and only if $|x|a$. Seeds are drawn independently from a continuous distribution that is symmetric about $0$; let $p=\\mathbb{P}(|X|\\ge a)$ denote the probability that a single random seed $X$ lies in the global basin of attraction, and let $1-p=\\mathbb{P}(|X|a)$ denote the probability that it lies in the local basin.\n\nA multistart routine allocates two seeds per run, and records the best-found objective $B$ in that run as the minimum of the two objective values returned by local search from the two seeds. Two allocation schemes are considered:\n\n(i) Independent allocation: two independent seeds $X_{1}$ and $X_{2}$ are drawn, and the local search outputs the pair of objective values $\\{f(T(X_{1})),f(T(X_{2}))\\}$.\n\n(ii) Antithetic allocation: one seed $X$ is drawn and paired antithetically as $(X,-X)$, and the local search outputs the pair of objective values $\\{f(T(X)),f(T(-X))\\}$.\n\nStarting from the definitions of expectation and variance and the given symmetric setup, derive the distribution of $B$ under each allocation scheme and obtain the variance of $B$ in each case in terms of $p$ and $\\Delta=f_{\\ell}-f_{g}0$. Then, compute the variance reduction factor $R(p)$ defined as the ratio of the variance under antithetic allocation to the variance under independent allocation. Provide your final answer as a single closed-form expression for $R(p)$ in terms of $p$ only. No numerical evaluation is required.",
            "solution": "Let $Y$ be the random variable for the objective value obtained from a single random seed $X$. According to the problem description:\n- If $|X| \\ge a$ (an event with probability $p$), the local search converges to a global minimum, so the resulting objective value is $Y = f_g$.\n- If $|X|  a$ (an event with probability $1-p$), the local search converges to the local minimum, so the resulting objective value is $Y = f_\\ell$.\n\nWe now analyze the best-found objective value, $B$, for each of the two allocation schemes.\n\n**(ii) Antithetic Allocation**\n\nIn this scheme, the two seeds are $X$ and $-X$. The best-found objective is $B_A = \\min\\{f(T(X)), f(T(-X))\\}$.\nThe basin of attraction for a seed $x$ is determined by its magnitude $|x|$. Since $|X| = |-X|$, both seeds will always land in the same type of basin.\n- If $|X| \\ge a$, then $f(T(X)) = f_g$ and $f(T(-X)) = f_g$. Therefore, $B_A = \\min\\{f_g, f_g\\} = f_g$. This occurs with probability $p$.\n- If $|X|  a$, then $f(T(X)) = f_\\ell$ and $f(T(-X)) = f_\\ell$. Therefore, $B_A = \\min\\{f_\\ell, f_\\ell\\} = f_\\ell$. This occurs with probability $1-p$.\nThe distribution of $B_A$ is identical to that of a single trial's outcome, $Y$. The variance is:\n$$ \\text{Var}(B_A) = (f_\\ell - f_g)^2 \\mathbb{P}(B_A=f_g)\\mathbb{P}(B_A=f_\\ell) = \\Delta^2 p(1-p) $$\nwhere $\\Delta = f_{\\ell} - f_g$.\n\n**(i) Independent Allocation**\n\nIn this scheme, two independent seeds $X_1$ and $X_2$ are drawn. Let $Y_1 = f(T(X_1))$ and $Y_2 = f(T(X_2))$ be the corresponding objective values. $Y_1$ and $Y_2$ are independent random variables, each with the same distribution as $Y$. The best-found objective is $B_I = \\min\\{Y_1, Y_2\\}$.\nSince $f_g  f_\\ell$, the value of $B_I$ will be $f_g$ if at least one of the trials is successful. $B_I$ is $f_\\ell$ only if both trials fail to find the global minimum.\nThe probability of $B_I$ being $f_\\ell$ is:\n$$ \\mathbb{P}(B_I = f_{\\ell}) = \\mathbb{P}(Y_1 = f_{\\ell} \\text{ and } Y_2 = f_{\\ell}) = \\mathbb{P}(Y_1 = f_{\\ell}) \\times \\mathbb{P}(Y_2 = f_{\\ell}) = (1-p)^2 $$\nThe probability of $B_I$ being $f_g$ is the complementary probability:\n$$ \\mathbb{P}(B_I = f_g) = 1 - \\mathbb{P}(B_I = f_{\\ell}) = 1 - (1-p)^2 = 2p - p^2 = p(2-p) $$\nThe variance of $B_I$ is:\n$$ \\text{Var}(B_I) = (f_\\ell - f_g)^2 \\mathbb{P}(B_I=f_g)\\mathbb{P}(B_I=f_\\ell) = \\Delta^2 [p(2-p)][(1-p)^2] $$\n\n**Variance Reduction Factor**\n\nFinally, the variance reduction factor $R(p)$ is the ratio of the two variances:\n$$ R(p) = \\frac{\\text{Var}(B_A)}{\\text{Var}(B_I)} = \\frac{\\Delta^2 p(1-p)}{\\Delta^2 p(2-p)(1-p)^2} $$\nFor $p \\in (0,1)$, we can cancel the common factors $\\Delta^2$, $p$, and $(1-p)$ to get:\n$$ R(p) = \\frac{1}{(2-p)(1-p)} $$",
            "answer": "$$\\boxed{\\frac{1}{(2-p)(1-p)}}$$"
        }
    ]
}