{
    "hands_on_practices": [
        {
            "introduction": "The multistart method is a cornerstone of stochastic global optimization, but its success hinges on a critical question: how many random trials are sufficient? This exercise tackles this fundamental problem by guiding you through a derivation based on core probability principles. By working through this practice , you will establish the theoretical relationship between the number of restarts, the likelihood of finding the global optimum's basin of attraction, and the overall confidence in your search.",
            "id": "3186484",
            "problem": "Consider the $d$-dimensional Rastrigin function defined by $f(x)=\\sum_{i=1}^{d}\\big(x_{i}^{2}-10\\cos(2\\pi x_{i})+10\\big)$ on the hyper-rectangle (box) domain $[-5.12,5.12]^{d}$. A stochastic multistart strategy is used: each restart independently samples an initial point $x^{(0)}$ uniformly at random from the domain and then applies a deterministic local optimization method that converges to a local minimum whose basin of attraction contains $x^{(0)}$. Assume the basin of attraction of the global minimum has a volume fraction $p$ with $0p1$, meaning a uniformly sampled initial point lies in this basin with probability $p$. Using only fundamental probability principles for independent trials, derive a closed-form expression for the smallest integer number of restarts $m$ such that the probability of having reached the global minimum at least once across $m$ independent restarts is at least $0.95$. Express your final answer as a function of $p$ and $0.95$, and provide it as a single closed-form expression. No rounding is required.",
            "solution": "The problem is to find the smallest integer number of restarts, $m$, for a stochastic multistart optimization method to ensure that the probability of finding the global minimum at least once is at least $0.95$.\n\nFirst, we formalize the problem using probability theory. The problem statement provides the following key information:\n1.  Each restart is an independent trial.\n2.  An initial point for a restart is sampled uniformly at random from the domain.\n3.  The probability that a single, randomly initiated restart converges to the global minimum is given by the volume fraction of its basin of attraction, which is denoted by $p$.\n\nLet $S$ be the event that a single restart successfully finds the global minimum. The probability of this event is given as:\n$$P(S) = p$$\nThe problem states that $0  p  1$.\n\nLet $F$ be the event that a single restart fails to find the global minimum. This is the complement of event $S$. The probability of failure is:\n$$P(F) = 1 - P(S) = 1 - p$$\n\nWe are performing $m$ independent restarts. Let $A$ be the event that the global minimum is found at least once across these $m$ restarts. The problem requires us to find the smallest integer $m$ such that:\n$$P(A) \\ge 0.95$$\n\nIt is computationally more straightforward to first calculate the probability of the complement event, $A^c$. The event $A^c$ is the scenario where none of the $m$ restarts find the global minimum, meaning all $m$ restarts fail.\n$$A^c = \\text{failure on all } m \\text{ restarts}$$\n\nSince the restarts are independent events, the probability of all $m$ failing is the product of their individual probabilities of failure:\n$$P(A^c) = \\underbrace{P(F) \\times P(F) \\times \\dots \\times P(F)}_{m \\text{ times}} = (P(F))^m = (1 - p)^m$$\n\nThe probability of event $A$ (at least one success) is related to the probability of its complement $A^c$ (no successes) by the formula $P(A) = 1 - P(A^c)$. Therefore:\n$$P(A) = 1 - (1 - p)^m$$\n\nNow, we apply the condition specified in the problem:\n$$1 - (1 - p)^m \\ge 0.95$$\n\nWe need to solve this inequality for $m$. Rearranging the terms, we get:\n$$1 - 0.95 \\ge (1 - p)^m$$\n$$0.05 \\ge (1 - p)^m$$\n\nTo solve for the exponent $m$, we take the natural logarithm of both sides. Since the natural logarithm function, $\\ln(x)$, is a strictly increasing function, applying it to both sides of an inequality preserves the direction of the inequality:\n$$\\ln(0.05) \\ge \\ln((1 - p)^m)$$\n\nUsing the logarithm property $\\ln(x^y) = y \\ln(x)$, we have:\n$$\\ln(0.05) \\ge m \\ln(1 - p)$$\n\nTo isolate $m$, we must divide by $\\ln(1 - p)$. The direction of the inequality upon division depends on the sign of $\\ln(1 - p)$. The problem states that $0  p  1$, which implies that $0  1 - p  1$. The natural logarithm of any number between $0$ and $1$ is negative. Thus, $\\ln(1 - p)  0$.\n\nWhen dividing an inequality by a negative number, the direction of the inequality sign must be reversed:\n$$\\frac{\\ln(0.05)}{\\ln(1 - p)} \\le m$$\nThis can be written as:\n$$m \\ge \\frac{\\ln(0.05)}{\\ln(1 - p)}$$\nNote that $\\ln(0.05)$ is also negative, so the ratio $\\frac{\\ln(0.05)}{\\ln(1 - p)}$ is a positive number, as expected for a number of trials.\n\nThe problem requires the smallest *integer* value of $m$ that satisfies this condition. The smallest integer greater than or equal to a given real number $x$ is given by the ceiling function, $\\lceil x \\rceil$. Therefore, the smallest integer $m$ is:\n$$m = \\left\\lceil \\frac{\\ln(0.05)}{\\ln(1 - p)} \\right\\rceil$$\n\nThis expression provides the required closed-form for the smallest integer number of restarts $m$ as a function of the probability $p$ and the desired success rate of $0.95$.",
            "answer": "$$\\boxed{\\left\\lceil \\frac{\\ln(0.05)}{\\ln(1 - p)} \\right\\rceil}$$"
        },
        {
            "introduction": "Theoretical models often rely on knowing the probability of sampling within a basin of attraction, a parameter that is rarely known in real-world scenarios. This hands-on coding exercise  demonstrates how to bridge this gap by empirically estimating this probability. You will implement a Monte Carlo simulation to probe the landscape around a known minimum, providing a practical method for informing the design of an effective multistart strategy.",
            "id": "3186406",
            "problem": "You are asked to design and implement a complete, runnable program that empirically estimates an attraction basin for a specified local solver and uses this estimate to determine a suitable multistart seeding density for stochastic global optimization. The exercise must start from core definitions and well-tested facts in optimization and probability and avoid relying on any shortcut formulas beyond those derivable from such fundamentals.\n\nDefinitions and principles to use:\n- A deterministic local solver applied to a differentiable objective function $f:\\mathbb{R}^n \\to \\mathbb{R}$ attempts to produce a point $\\hat{x}$ such that $\\nabla f(\\hat{x}) \\approx 0$ and $f(\\hat{x}) \\le f(x_0)$ for a given initial point $x_0$. The set of initial points whose solver trajectories converge to a particular local minimum $x^{\\star}$ is called the attraction basin of $x^{\\star}$ for that solver.\n- An empirical estimate of attraction behavior can be obtained by Monte Carlo sampling. If $S$ independent initial points are sampled from a region, and $K$ solver runs converge to $x^{\\star}$ by a specified proximity criterion, then the Monte Carlo estimator $\\hat{p} = K/S$ is an unbiased estimator of the probability $p$ that a random point drawn from that region leads to convergence to $x^{\\star}$.\n- For multistart, with independent uniformly distributed starting points over a bounded domain $D \\subset \\mathbb{R}^n$ of finite area $A(D)$, if a particular subregion $B \\subset D$ has measure fraction $p_B = A(B)/A(D)$, then the probability that at least one of $N$ independent starts falls in $B$ is $1 - (1 - p_B)^N$. Solving $1 - (1 - p_B)^N \\ge q$ for $N$ gives the minimal number of starts to achieve coverage probability at least $q$.\n\nYour program must implement the following procedure in two dimensions $n=2$.\n\n1. Given a twice continuously differentiable objective $f:\\mathbb{R}^2\\to \\mathbb{R}$, a candidate local minimum $m \\in \\mathbb{R}^2$, a bounded rectangular domain $D = [\\ell_x,u_x] \\times [\\ell_y,u_y]$ with $A(D) = (u_x - \\ell_x)(u_y - \\ell_y)$, a list of radii $\\mathcal{R} = \\{r_1,\\dots,r_k\\}$ with each $r_i  0$, a sample size $S \\in \\mathbb{N}$, a success threshold $\\tau \\in (0,1)$, a target multistart coverage probability $q \\in (0,1)$, and a Euclidean proximity tolerance $\\varepsilon  0$, estimate an empirical attraction basin radius for a specified local solver as follows:\n   - For each radius $r \\in \\mathcal{R}$, independently sample $S$ points uniformly from the disk $B(m,r) = \\{x \\in \\mathbb{R}^2 : \\|x - m\\|_2 \\le r\\}$ intersected with $D$. For each sampled point $x_0$, run the local solver on $f$ starting at $x_0$. Declare a success if the solver reports successful termination and returns $\\hat{x}$ with $\\|\\hat{x} - m\\|_2 \\le \\varepsilon$. Compute the empirical success rate $\\hat{p}(r) = K(r)/S$, where $K(r)$ is the success count at radius $r$.\n   - Define the empirical attraction radius $r^{\\star}$ as the smallest $r \\in \\mathcal{R}$ such that $\\hat{p}(r) \\ge \\tau$. If no such $r$ exists, define $r^{\\star}$ to be the $r \\in \\mathcal{R}$ that maximizes $\\hat{p}(r)$, breaking ties by choosing the smallest such $r$. Let $p^{\\star} = \\hat{p}(r^{\\star})$.\n   - Approximate the basin area by $\\pi (r^{\\star})^2$ and the corresponding domain fraction by $p_{\\mathrm{area}} = \\min\\{1, \\pi (r^{\\star})^2 / A(D)\\}$. Using independence of multistart seeds drawn uniformly from $D$, the minimal number of starts $N$ to ensure at least one start falls in the approximate basin with probability at least $q$ satisfies $1 - (1 - p_{\\mathrm{area}})^N \\ge q$. Solve for $N$ as the smallest integer $N$ with this property.\n\n2. Implement the above using standard local solvers:\n   - Use the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method where specified, supplying an analytic gradient when available.\n   - Use the Nelder–Mead simplex method where specified, without a gradient.\n\n3. Sampling uniform points in a disk must be unbiased: if $U \\sim \\mathrm{Uniform}[0,1]$ and $\\Theta \\sim \\mathrm{Uniform}[0,2\\pi)$ are independent, then a point offset with polar coordinates $(\\rho,\\theta)$ where $\\rho = r \\sqrt{U}$ and $\\theta = \\Theta$ is uniformly distributed over the disk of radius $r$.\n\nQuantities to compute and report:\n- For each test case, compute the triple $[N, r^{\\star}, p^{\\star}]$. The number $N$ must be the smallest integer satisfying the multistart coverage condition. The value $r^{\\star}$ must be the empirically selected radius, and $p^{\\star}$ must be the corresponding empirical success rate. Express probabilities as decimals (not percentages). No physical units are involved.\n\nTest suite:\nYour program must run the following three test cases and aggregate their outputs.\n\n- Test case $1$ (Himmelblau function, Broyden–Fletcher–Goldfarb–Shanno method):\n  - Objective: Himmelblau’s function $f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$.\n  - Candidate minimum: $m = (3,2)$.\n  - Domain: $D = [-6,6] \\times [-6,6]$.\n  - Radii: $\\mathcal{R} = \\{0.15, 0.30, 0.60, 1.20\\}$.\n  - Samples per radius: $S = 120$.\n  - Success threshold: $\\tau = 0.9$.\n  - Target coverage: $q = 0.95$.\n  - Proximity tolerance: $\\varepsilon = 0.01$.\n\n- Test case $2$ (Himmelblau function, Nelder–Mead simplex method):\n  - Objective: same as test case $1$.\n  - Candidate minimum: $m = (-2.805118, 3.131312)$.\n  - Domain: $D = [-6,6] \\times [-6,6]$.\n  - Radii: $\\mathcal{R} = \\{0.10, 0.30, 0.60, 1.00\\}$.\n  - Samples per radius: $S = 100$.\n  - Success threshold: $\\tau = 0.8$.\n  - Target coverage: $q = 0.90$.\n  - Proximity tolerance: $\\varepsilon = 0.02$.\n\n- Test case $3$ (Rosenbrock function, Broyden–Fletcher–Goldfarb–Shanno method):\n  - Objective: Rosenbrock’s function $f(x,y) = (1-x)^2 + 100(y - x^2)^2$.\n  - Candidate minimum: $m = (1,1)$.\n  - Domain: $D = [-2,2] \\times [-2,2]$.\n  - Radii: $\\mathcal{R} = \\{0.05, 0.10, 0.20, 0.40\\}$.\n  - Samples per radius: $S = 150$.\n  - Success threshold: $\\tau = 0.95$.\n  - Target coverage: $q = 0.99$.\n  - Proximity tolerance: $\\varepsilon = 0.01$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, where each test case contributes its own bracketed triple. For example, an output with three test cases must appear as\n\"[[N1,r1,p1],[N2,r2,p2],[N3,r3,p3]]\"\nwhere each $N_i$ is an integer and each $r_i$ and $p_i$ are decimal numbers. Round $r_i$ and $p_i$ to six digits after the decimal point in the printed output. No other text should be printed.",
            "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to proceed with a solution. The task is to implement a procedure to empirically estimate the attraction basin for a local minimum of a function and then determine the necessary number of starting points for a multistart optimization strategy to find that minimum with a certain probability.\n\nThe procedure is executed in two main stages for an objective function $f: \\mathbb{R}^2 \\to \\mathbb{R}$ and a candidate local minimum $m \\in \\mathbb{R}^2$.\n\nFirst, we empirically estimate the attraction radius. For a set of test radii $\\mathcal{R} = \\{r_1, \\dots, r_k\\}$, we perform a Monte Carlo simulation. For each radius $r \\in \\mathcal{R}$, we generate $S$ initial points $\\{x_0^{(1)}, \\dots, x_0^{(S)}\\}$ sampled uniformly from the disk $B(m, r)$ of radius $r$ centered at $m$. The problem specifies that for the given test cases, these disks are fully contained within the search domain $D$, simplifying sampling. Uniform sampling in a disk is achieved using inverse transform sampling in polar coordinates. A point's radial coordinate $\\rho$ and angular coordinate $\\theta$ are generated via $\\rho = r \\sqrt{U_1}$ and $\\theta = 2\\pi U_2$, where $U_1, U_2$ are independent random variables uniformly distributed on $[0,1]$. This ensures that generated points have a uniform probability density over the area of the disk.\n\nFor each sampled point $x_0$, a local optimization solver (either BFGS or Nelder-Mead, as specified) is initiated. A trial is deemed a \"success\" if two conditions are met: the solver converges successfully, and the found minimum $\\hat{x}$ is within a Euclidean distance $\\varepsilon$ of the target minimum $m$, i.e., $\\|\\hat{x} - m\\|_2 \\le \\varepsilon$. The number of successes $K(r)$ for a given radius $r$ is counted. The empirical probability of convergence, or success rate, is then estimated as $\\hat{p}(r) = K(r)/S$.\n\nAfter computing $\\hat{p}(r)$ for all $r \\in \\mathcal{R}$, the empirical attraction radius $r^{\\star}$ is selected according to the specified rule. We identify the set of radii for which the success rate meets or exceeds a given threshold $\\tau$, i.e., $\\{r \\in \\mathcal{R} \\mid \\hat{p}(r) \\ge \\tau\\}$. If this set is non-empty, $r^{\\star}$ is chosen as the minimum radius in this set. If no radius meets the threshold, $r^{\\star}$ is chosen as the radius that yields the maximum observed success rate, with ties broken by selecting the smallest such radius. The success rate corresponding to $r^{\\star}$ is denoted $p^{\\star} = \\hat{p}(r^{\\star})$.\n\nSecond, using this empirical radius $r^{\\star}$, we calculate the required number of starting points $N$ for a multistart method. The basin of attraction for the minimum $m$ is approximated as a disk of radius $r^{\\star}$, with an area of $A_{\\text{basin}} = \\pi (r^{\\star})^2$. The total search domain $D$ is a rectangle $[\\ell_x, u_x] \\times [\\ell_y, u_y]$ with area $A(D) = (u_x - \\ell_x)(u_y - \\ell_y)$. The probability that a single point chosen uniformly at random from $D$ falls into the approximated basin is the ratio of their areas, denoted $p_{\\text{area}} = \\min(1, A_{\\text{basin}} / A(D))$.\n\nFor a multistart method with $N$ independent starting points drawn uniformly from $D$, the probability that all $N$ points miss the basin is $(1 - p_{\\text{area}})^N$. Consequently, the probability that at least one point falls within the basin is $1 - (1 - p_{\\text{area}})^N$. We must find the smallest integer $N$ that satisfies the target coverage probability $q$:\n$$1 - (1 - p_{\\text{area}})^N \\ge q$$\nRearranging the inequality, we get $(1 - p_{\\text{area}})^N \\le 1 - q$. Taking the natural logarithm of both sides gives $N \\log(1 - p_{\\text{area}}) \\le \\log(1 - q)$. Since $p_{\\text{area}} \\in (0, 1)$, $\\log(1 - p_{\\text{area}})$ is negative. Dividing by it reverses the inequality sign:\n$$N \\ge \\frac{\\log(1-q)}{\\log(1-p_{\\text{area}})}$$\nThe smallest integer $N$ satisfying this is obtained by taking the ceiling of the right-hand side, $N = \\left\\lceil \\frac{\\log(1-q)}{\\log(1-p_{\\text{area}})} \\right\\rceil$. In the special case where $p_{\\text{area}} = 1$, a single starting point guarantees coverage, so $N=1$.\n\nThe implementation will use `numpy` for numerical computations and random number generation, and `scipy.optimize.minimize` for the local search algorithms. The analytic gradients for the BFGS method are required, as specified.\n\nFor Himmelblau's function $f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$, the partial derivatives are:\n$$ \\frac{\\partial f}{\\partial x} = 4x(x^2 + y - 11) + 2(x + y^2 - 7) $$\n$$ \\frac{\\partial f}{\\partial y} = 2(x^2 + y - 11) + 4y(x + y^2 - 7) $$\nFor Rosenbrock's function $f(x,y) = (1-x)^2 + 100(y - x^2)^2$, the partial derivatives are:\n$$ \\frac{\\partial f}{\\partial x} = -2(1-x) - 400x(y-x^2) $$\n$$ \\frac{\\partial f}{\\partial y} = 200(y-x^2) $$\nThese functions and their gradients are implemented to solve the test cases. A fixed seed for the random number generator ensures the reproducibility of the Monte Carlo simulation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    # Set a fixed seed for the random number generator for reproducibility.\n    np.random.seed(0)\n\n    # Define objective functions and their gradients.\n    def himmelblau(p):\n        x, y = p\n        return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n\n    def grad_himmelblau(p):\n        x, y = p\n        df_dx = 4 * x * (x**2 + y - 11) + 2 * (x + y**2 - 7)\n        df_dy = 2 * (x**2 + y - 11) + 4 * y * (x + y**2 - 7)\n        return np.array([df_dx, df_dy])\n\n    def rosenbrock(p):\n        x, y = p\n        return (1 - x)**2 + 100 * (y - x**2)**2\n\n    def grad_rosenbrock(p):\n        x, y = p\n        df_dx = -2 * (1 - x) - 400 * x * (y - x**2)\n        df_dy = 200 * (y - x**2)\n        return np.array([df_dx, df_dy])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": himmelblau,\n            \"grad_f\": grad_himmelblau,\n            \"method\": \"BFGS\",\n            \"m\": np.array([3.0, 2.0]),\n            \"D\": [-6.0, 6.0, -6.0, 6.0],\n            \"R\": [0.15, 0.30, 0.60, 1.20],\n            \"S\": 120,\n            \"tau\": 0.9,\n            \"q\": 0.95,\n            \"eps\": 0.01,\n        },\n        {\n            \"f\": himmelblau,\n            \"grad_f\": None,\n            \"method\": \"Nelder-Mead\",\n            \"m\": np.array([-2.805118, 3.131312]),\n            \"D\": [-6.0, 6.0, -6.0, 6.0],\n            \"R\": [0.10, 0.30, 0.60, 1.00],\n            \"S\": 100,\n            \"tau\": 0.8,\n            \"q\": 0.90,\n            \"eps\": 0.02,\n        },\n        {\n            \"f\": rosenbrock,\n            \"grad_f\": grad_rosenbrock,\n            \"method\": \"BFGS\",\n            \"m\": np.array([1.0, 1.0]),\n            \"D\": [-2.0, 2.0, -2.0, 2.0],\n            \"R\": [0.05, 0.10, 0.20, 0.40],\n            \"S\": 150,\n            \"tau\": 0.95,\n            \"q\": 0.99,\n            \"eps\": 0.01,\n        },\n    ]\n\n    def sample_in_disk(center, radius, n_samples):\n        # Sample points uniformly from a disk of given center and radius.\n        r_vals = radius * np.sqrt(np.random.uniform(0, 1, size=n_samples))\n        theta_vals = 2 * np.pi * np.random.uniform(0, 1, size=n_samples)\n        x_coords = center[0] + r_vals * np.cos(theta_vals)\n        y_coords = center[1] + r_vals * np.sin(theta_vals)\n        return np.stack((x_coords, y_coords), axis=-1)\n\n    def run_case(case):\n        # Execute the full procedure for a single test case.\n        f, grad_f, method, m, domain, radii, S, tau, q, eps = (\n            case[\"f\"], case[\"grad_f\"], case[\"method\"], case[\"m\"], case[\"D\"],\n            case[\"R\"], case[\"S\"], case[\"tau\"], case[\"q\"], case[\"eps\"]\n        )\n\n        phat_results = {}\n        for r_test in radii:\n            initial_points = sample_in_disk(m, r_test, S)\n            success_count = 0\n            for x0 in initial_points:\n                res = minimize(f, x0, method=method, jac=grad_f, options={'maxiter': 1000})\n                if res.success and np.linalg.norm(res.x - m) = eps:\n                    success_count += 1\n            phat_results[r_test] = success_count / S\n        \n        # Determine r_star and p_star based on the specified rules.\n        successful_radii = [r for r, p in phat_results.items() if p >= tau]\n        \n        if successful_radii:\n            r_star = min(successful_radii)\n        else:\n            max_p = -1.0\n            # Find max probability among all radii\n            for p in phat_results.values():\n                if p > max_p:\n                    max_p = p\n            # Find radii that achieve max_p, then choose the smallest\n            best_radii = [r for r, p in phat_results.items() if p == max_p]\n            r_star = min(best_radii)\n            \n        p_star = phat_results[r_star]\n\n        # Calculate the required number of multistarts, N.\n        lx, ux, ly, uy = domain\n        A_D = (ux - lx) * (uy - ly)\n        A_basin = np.pi * (r_star**2)\n        p_area = min(1.0, A_basin / A_D)\n\n        if p_area >= 1.0:\n            N = 1\n        elif p_area = 0.0:\n            # This case should not be reached with r_star > 0, but as a safeguard:\n            N = float('inf') \n        else:\n            N = math.ceil(math.log(1 - q) / math.log(1 - p_area))\n        \n        return int(N), r_star, p_star\n\n    results = []\n    for case in test_cases:\n        result = run_case(case)\n        results.append(result)\n\n    # Format the results into the required string format.\n    formatted_results = []\n    for N, r_star, p_star in results:\n        formatted_results.append(f\"[{int(N)},{r_star:.6f},{p_star:.6f}]\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond simply finding a good solution, we can analyze the set of local minima found during a multistart search to make statistical predictions about future performance. This advanced practice introduces the powerful combination of order statistics and nonparametric bootstrapping. You will first derive the expected value of the best-found minimum and then implement a bootstrap procedure to estimate this quantity from observed data, showcasing how to leverage past results to forecast future success .",
            "id": "3186375",
            "problem": "Consider a multistart global optimization setting where each independent restart yields a local minimum depth modeled as an independent and identically distributed random variable $X$ with cumulative distribution function $F$. Let $X_1,\\dots,X_m$ be $m$ independent draws from $F$, and define the best-found depth after $m$ restarts as $M_m = \\min\\{X_1,\\dots,X_m\\}$. \n\nTask 1 (derivation): Starting only from the definitions of cumulative distribution function, independence, and order statistics, and from the tail integral identity for nonnegative random variables, derive a general expression for the expected best depth $\\,\\mathbb{E}[M_m]\\,$ in terms of $F$ and $m$. Your derivation must not assume any particular parametric form for $F$ beyond the stated definitions.\n\nTask 2 (closed forms for specific distributions): Using your result from Task 1, derive closed-form expressions for $\\,\\mathbb{E}[M_m]\\,$ in each of the following distribution families where $X$ is supported on $[0,\\infty)$ or an interval thereof:\n- Uniform distribution on $[a,b]$ with parameters $a$ and $b$ where $ab$.\n- Exponential distribution with rate parameter $\\lambda0$.\n- Weibull distribution with shape parameter $k0$ and scale parameter $\\lambda0$.\n\nTask 3 (online estimation via bootstrapping): Suppose that after $r$ preliminary restarts you have an observed list $\\{x^{\\mathrm{obs}}_1,\\dots,x^{\\mathrm{obs}}_r\\}$ of realized local minimum depths. Propose an online method to estimate $\\,\\mathbb{E}[M_m]\\,$ for a future budget of $m$ restarts using nonparametric bootstrapping of the observed minima values. Your proposal should specify:\n- How to form the empirical estimator of the distribution using the observed values.\n- How to compute an estimator of $\\,\\mathbb{E}[M_m]\\,$ by resampling with replacement.\n- How to make the estimator reproducible by fixing a pseudorandom seed.\n\nProgramming task: Implement a complete program that:\n- Computes the theoretical value of $\\,\\mathbb{E}[M_m]\\,$ for the three families in Task 2 using closed forms derived from Task 1.\n- Computes a bootstrap estimate of $\\,\\mathbb{E}[M_m]\\,$ using the nonparametric bootstrap you proposed in Task 3, with a fixed number of bootstrap replicates $B$ and a fixed pseudorandom seed.\n- Applies both computations to the test suite below and outputs the results in the required format.\n\nUse $B=20000$ bootstrap replicates and the fixed seed $12345$ in all bootstrap computations. Round all reported floating-point results to exactly $6$ decimal places.\n\nTest suite (four cases):\n- Case $1$ (happy path, uniform): $F$ is Uniform on $[0,1]$, with $m=5$ and observed minima list $\\{0.12,\\,0.34,\\,0.07,\\,0.56,\\,0.91,\\,0.22,\\,0.44,\\,0.03\\}$.\n- Case $2$ (happy path, exponential): $F$ is Exponential with rate $\\lambda=2.0$, with $m=10$ and observed minima list $\\{0.10,\\,0.40,\\,0.01,\\,0.25,\\,0.02,\\,0.33,\\,0.12,\\,0.05\\}$.\n- Case $3$ (coverage, Weibull): $F$ is Weibull with shape $k=2.0$ and scale $\\lambda=1.5$, with $m=3$ and observed minima list $\\{0.20,\\,1.00,\\,1.35,\\,0.50,\\,2.10,\\,0.95,\\,0.75,\\,0.30\\}$.\n- Case $4$ (boundary condition $m=1$): $F$ is Uniform on $[0,1]$, with $m=1$ and observed minima list $\\{0.80,\\,0.40,\\,0.10,\\,0.60,\\,0.30\\}$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case in order $1$ through $4$, include first the theoretical value and then the bootstrap estimate, both rounded to exactly $6$ decimal places. Thus the output should have $8$ numbers in total in the order $[\\text{case1\\_theory},\\text{case1\\_bootstrap},\\text{case2\\_theory},\\text{case2\\_bootstrap},\\text{case3\\_theory},\\text{case3\\_bootstrap},\\text{case4\\_theory},\\text{case4\\_bootstrap}]$.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard, non-trivial problem in probability theory and computational statistics applied to optimization. All necessary data and definitions for a unique solution are provided. We proceed with the derivation and solution.\n\n### Task 1: Derivation of the General Expression for $\\mathbb{E}[M_m]$\n\nLet $X$ be a non-negative random variable representing the depth of a local minimum, with cumulative distribution function (CDF) $F(x) = P(X \\le x)$. We are given $m$ independent and identically distributed (i.i.d.) random variables $X_1, \\dots, X_m$, each with CDF $F$. The best-found depth is defined as the first order statistic $M_m = \\min\\{X_1, \\dots, X_m\\}$. Our objective is to derive a general expression for the expected value of $M_m$, denoted $\\mathbb{E}[M_m]$.\n\nFirst, we determine the CDF of $M_m$, which we denote by $F_{M_m}(x)$. By definition, $F_{M_m}(x) = P(M_m \\le x)$. It is often easier to work with the complementary event:\n$$F_{M_m}(x) = 1 - P(M_m  x)$$\nThe event $M_m  x$ is equivalent to the event that all $X_i$ are greater than $x$:\n$$P(M_m  x) = P(\\min\\{X_1, \\dots, X_m\\}  x) = P(X_1  x, X_2  x, \\dots, X_m  x)$$\nSince the random variables $X_1, \\dots, X_m$ are independent, the joint probability is the product of the individual probabilities:\n$$P(M_m  x) = \\prod_{i=1}^{m} P(X_i  x)$$\nFurthermore, because the variables are identically distributed, $P(X_i  x)$ is the same for all $i$. This probability, known as the survival function $S(x)$, is given by $S(x) = 1 - F(x)$. Thus:\n$$P(M_m  x) = (P(X  x))^m = (1 - F(x))^m$$\nSubstituting this back into the expression for the CDF of $M_m$, we get:\n$$F_{M_m}(x) = 1 - (1 - F(x))^m$$\nThe problem specifies that $X$ is a non-negative random variable, meaning its support is on $[0, \\infty)$ or a subinterval thereof. Consequently, $M_m$ is also a non-negative random variable. For any non-negative random variable $Y$, its expected value can be computed using the tail integral identity:\n$$\\mathbb{E}[Y] = \\int_0^\\infty P(Y  y) \\, dy = \\int_0^\\infty (1 - F_Y(y)) \\, dy$$\nApplying this identity to $M_m$, we have:\n$$\\mathbb{E}[M_m] = \\int_0^\\infty P(M_m  x) \\, dx$$\nUsing our previously derived expression for $P(M_m  x)$, we arrive at the general formula for the expected best depth:\n$$\\mathbb{E}[M_m] = \\int_0^\\infty (1 - F(x))^m \\, dx$$\nThis expression is valid for any non-negative random variable $X$ with CDF $F$.\n\n### Task 2: Closed-Form Expressions for Specific Distributions\n\nWe now apply the general formula to derive closed-form expressions for $\\mathbb{E}[M_m]$ for three specific distribution families, assuming their support is non-negative.\n\n**1. Uniform distribution on $[a, b]$**\nFor this case, we assume $0 \\le a  b$. The CDF of $X$ is:\n$$F(x) = \\begin{cases} 0  x  a \\\\ \\frac{x-a}{b-a}  a \\le x \\le b \\\\ 1  x  b \\end{cases}$$\nThe survival function $S(x) = 1 - F(x)$ is:\n$$1 - F(x) = \\begin{cases} 1  x  a \\\\ 1 - \\frac{x-a}{b-a} = \\frac{b-x}{b-a}  a \\le x \\le b \\\\ 0  x  b \\end{cases}$$\nThe integral for $\\mathbb{E}[M_m]$ must be split according to the support of the integrand:\n$$\\mathbb{E}[M_m] = \\int_0^a (1)^m \\, dx + \\int_a^b \\left(\\frac{b-x}{b-a}\\right)^m \\, dx + \\int_b^\\infty (0)^m \\, dx$$\n$$\\mathbb{E}[M_m] = [x]_0^a + \\frac{1}{(b-a)^m} \\int_a^b (b-x)^m \\, dx = a + \\frac{1}{(b-a)^m} \\left[ -\\frac{(b-x)^{m+1}}{m+1} \\right]_a^b$$\n$$\\mathbb{E}[M_m] = a + \\frac{1}{(b-a)^m} \\left( 0 - \\left( -\\frac{(b-a)^{m+1}}{m+1} \\right) \\right) = a + \\frac{(b-a)^{m+1}}{(b-a)^m(m+1)}$$\nThis simplifies to the closed-form expression:\n$$\\mathbb{E}[M_m] = a + \\frac{b-a}{m+1}$$\n\n**2. Exponential distribution with rate parameter $\\lambda  0$**\nThe support is $[0, \\infty)$. The CDF is $F(x) = 1 - e^{-\\lambda x}$ for $x \\ge 0$.\nThe survival function is $1 - F(x) = e^{-\\lambda x}$.\nPlugging this into the general formula:\n$$\\mathbb{E}[M_m] = \\int_0^\\infty (e^{-\\lambda x})^m \\, dx = \\int_0^\\infty e^{-m\\lambda x} \\, dx$$\nThis is a standard integral:\n$$\\mathbb{E}[M_m] = \\left[ -\\frac{1}{m\\lambda} e^{-m\\lambda x} \\right]_0^\\infty = 0 - \\left( -\\frac{1}{m\\lambda} e^0 \\right) = \\frac{1}{m\\lambda}$$\nThe closed-form expression is:\n$$\\mathbb{E}[M_m] = \\frac{1}{m\\lambda}$$\nThis aligns with the known result that the minimum of $m$ i.i.d. exponential($\\lambda$) variables is an exponential($m\\lambda$) variable, whose mean is $1/(m\\lambda)$.\n\n**3. Weibull distribution with shape $k  0$ and scale $\\lambda  0$**\nThe support is $[0, \\infty)$. The CDF is $F(x) = 1 - e^{-(x/\\lambda)^k}$ for $x \\ge 0$.\nThe survival function is $1 - F(x) = e^{-(x/\\lambda)^k}$.\nApplying the general formula:\n$$\\mathbb{E}[M_m] = \\int_0^\\infty \\left(e^{-(x/\\lambda)^k}\\right)^m \\, dx = \\int_0^\\infty e^{-m(x/\\lambda)^k} \\, dx$$\nThe integrand corresponds to the survival function of a Weibull variable with shape $k$ and a modified scale parameter. Specifically, $m(x/\\lambda)^k = (x/(\\lambda m^{-1/k}))^k$. This shows that $M_m$ is also a Weibull-distributed random variable with shape parameter $k$ and scale parameter $\\lambda_m = \\lambda m^{-1/k}$.\nThe expected value of a Weibull variable with shape $k'$ and scale $\\lambda'$ is given by $\\lambda' \\Gamma(1 + 1/k')$, where $\\Gamma$ is the Gamma function.\nFor $M_m$, the parameters are $k' = k$ and $\\lambda' = \\lambda m^{-1/k}$. Therefore, the expected value is:\n$$\\mathbb{E}[M_m] = (\\lambda m^{-1/k}) \\Gamma\\left(1 + \\frac{1}{k}\\right)$$\nThis is the closed-form expression for the Weibull distribution case.\n\n### Task 3: Online Estimation via Nonparametric Bootstrapping\n\nGiven a set of $r$ observed local minimum depths $\\{x^{\\mathrm{obs}}_1, \\dots, x^{\\mathrm{obs}}_r\\}$, we can estimate $\\mathbb{E}[M_m]$ for a future budget of $m$ restarts without assuming a parametric form for the underlying distribution $F$. This is achieved using nonparametric bootstrapping.\n\n**1. Empirical Estimator of the Distribution:**\nThe unknown true CDF $F$ is approximated by the empirical CDF (ECDF), denoted $\\hat{F}_r(x)$. The ECDF is constructed from the $r$ observations and is defined as the fraction of observations less than or equal to $x$:\n$$\\hat{F}_r(x) = \\frac{1}{r} \\sum_{i=1}^r \\mathbb{I}(x^{\\mathrm{obs}}_i \\le x)$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. This function represents a discrete probability distribution that places a probability mass of $1/r$ on each of the observed values $x^{\\mathrm{obs}}_i$.\n\n**2. Computation of the Estimator:**\nThe bootstrap principle involves replacing the true distribution $F$ with its empirical estimate $\\hat{F}_r$. The target quantity $\\mathbb{E}[M_m] = \\mathbb{E}_F[\\min\\{X_1, \\dots, X_m\\}]$ is estimated by $\\hat{\\mathbb{E}}[M_m] = \\mathbb{E}_{\\hat{F}_r}[\\min\\{X^*_1, \\dots, X^*_m\\}]$, where each $X^*_j$ is a random variable drawn from the empirical distribution $\\hat{F}_r$. Drawing from $\\hat{F}_r$ is equivalent to sampling with replacement from the original set of observations $\\{x^{\\mathrm{obs}}_1, \\dots, x^{\\mathrm{obs}}_r\\}$.\n\nThe expected value $\\mathbb{E}_{\\hat{F}_r}$ is typically approximated using Monte Carlo simulation. The full procedure is as follows:\n- Set a number of bootstrap replicates, $B$ (e.g., $B=20000$).\n- For each replicate $j$ from $1$ to $B$:\n    - Generate a \"bootstrap sample\" of size $m$ by drawing $m$ values with replacement from the observed data $\\{x^{\\mathrm{obs}}_1, \\dots, x^{\\mathrm{obs}}_r\\}$. Let this sample be $\\{x^*_{j,1}, \\dots, x^*_{j,m}\\}$.\n    - Compute the minimum of this bootstrap sample: $M^*_{m,j} = \\min\\{x^*_{j,1}, \\dots, x^*_{j,m}\\}$.\n- The bootstrap estimate of $\\mathbb{E}[M_m]$ is the arithmetic mean of the $B$ bootstrap minimums:\n$$\\hat{\\mathbb{E}}[M_m] = \\frac{1}{B} \\sum_{j=1}^B M^*_{m,j}$$\n\n**3. Reproducibility:**\nThe resampling process relies on a pseudorandom number generator (PRNG). To ensure that the bootstrap estimate is deterministic and reproducible, the PRNG must be initialized with a fixed seed before the resampling loop begins. For this problem, the seed is specified as $12345$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import gamma\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing theoretical and bootstrap estimates for the expected minimum.\n    \"\"\"\n\n    def theory_uniform(a, b, m):\n        \"\"\"\n        Computes the theoretical expected minimum for a Uniform distribution.\n        Formula: E[M_m] = a + (b-a)/(m+1)\n        \"\"\"\n        return a + (b - a) / (m + 1)\n\n    def theory_exponential(rate, m):\n        \"\"\"\n        Computes the theoretical expected minimum for an Exponential distribution.\n        Formula: E[M_m] = 1 / (m * rate)\n        \"\"\"\n        return 1 / (m * rate)\n\n    def theory_weibull(shape_k, scale_lambda, m):\n        \"\"\"\n        Computes the theoretical expected minimum for a Weibull distribution.\n        Formula: E[M_m] = scale * (m**(-1/k)) * Gamma(1 + 1/k)\n        \"\"\"\n        return scale_lambda * (m**(-1 / shape_k)) * gamma(1 + 1 / shape_k)\n\n    def bootstrap_estimate(observed_data, m, B, seed):\n        \"\"\"\n        Computes the nonparametric bootstrap estimate of the expected minimum.\n        \n        Args:\n            observed_data (list or np.ndarray): The observed minima values.\n            m (int): The number of restarts for which to estimate the expected minimum.\n            B (int): The number of bootstrap replicates.\n            seed (int): The seed for the pseudorandom number generator.\n        \n        Returns:\n            float: The bootstrap estimate of E[M_m].\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        bootstrap_mins = []\n        \n        # Convert to numpy array for efficiency\n        observed_array = np.array(observed_data)\n        \n        for _ in range(B):\n            # Draw a sample of size m with replacement from the observed data\n            bootstrap_sample = rng.choice(observed_array, size=m, replace=True)\n            # Compute the minimum of the bootstrap sample\n            current_min = np.min(bootstrap_sample)\n            bootstrap_mins.append(current_min)\n            \n        # The estimate is the mean of the bootstrap minimums\n        return np.mean(bootstrap_mins)\n\n    # Problem parameters\n    B_REPLICATES = 20000\n    SEED = 12345\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"dist\": \"uniform\",\n            \"params\": {\"a\": 0, \"b\": 1},\n            \"m\": 5,\n            \"observed_minima\": [0.12, 0.34, 0.07, 0.56, 0.91, 0.22, 0.44, 0.03]\n        },\n        {\n            \"dist\": \"exponential\",\n            \"params\": {\"rate\": 2.0},\n            \"m\": 10,\n            \"observed_minima\": [0.10, 0.40, 0.01, 0.25, 0.02, 0.33, 0.12, 0.05]\n        },\n        {\n            \"dist\": \"weibull\",\n            \"params\": {\"shape_k\": 2.0, \"scale_lambda\": 1.5},\n            \"m\": 3,\n            \"observed_minima\": [0.20, 1.00, 1.35, 0.50, 2.10, 0.95, 0.75, 0.30]\n        },\n        {\n            \"dist\": \"uniform\",\n            \"params\": {\"a\": 0, \"b\": 1},\n            \"m\": 1,\n            \"observed_minima\": [0.80, 0.40, 0.10, 0.60, 0.30]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        m = case[\"m\"]\n        observed = case[\"observed_minima\"]\n        \n        # Calculate theoretical value\n        if case[\"dist\"] == \"uniform\":\n            theory_val = theory_uniform(case[\"params\"][\"a\"], case[\"params\"][\"b\"], m)\n        elif case[\"dist\"] == \"exponential\":\n            theory_val = theory_exponential(case[\"params\"][\"rate\"], m)\n        elif case[\"dist\"] == \"weibull\":\n            theory_val = theory_weibull(case[\"params\"][\"shape_k\"], case[\"params\"][\"scale_lambda\"], m)\n        \n        # Calculate bootstrap estimate\n        bootstrap_val = bootstrap_estimate(observed, m, B_REPLICATES, SEED)\n        \n        # Format results to 6 decimal places and append\n        results.append(f\"{theory_val:.6f}\")\n        results.append(f\"{bootstrap_val:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}