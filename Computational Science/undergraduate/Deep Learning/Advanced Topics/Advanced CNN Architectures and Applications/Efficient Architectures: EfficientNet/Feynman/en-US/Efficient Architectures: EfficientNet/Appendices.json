{
    "hands_on_practices": [
        {
            "introduction": "To master the principle of compound scaling, one must first understand its direct impact on a model's computational budget. This foundational exercise guides you through building a cost model for the Mobile Inverted Bottleneck Convolution (MBConv) block, the core component of EfficientNet. You will then implement the compound scaling rule to see how systematically adjusting depth, width, and resolution affects the total Multiply-Accumulate operations (MACs), providing a concrete understanding of how efficiency is managed at scale .",
            "id": "3119662",
            "problem": "You are tasked with implementing a principled cost model and scaling procedure for a family of efficient convolutional neural network architectures, focusing on the EfficientNet compound scaling idea and the Mobile Inverted Bottleneck Convolution (MBConv) with Squeeze-and-Excitation (SE). Your program must be fully self-contained and produce quantifiable outputs for a predefined test suite. The goal is to evaluate accuracy recovery under compound scaling from EfficientNet-B0 through EfficientNet-B3 analogs and to quantify deviations attributable to training hyperparameters.\n\nFoundational base you must use:\n\n- For a two-dimensional convolution with kernel size $k \\times k$, input channels $C_{\\text{in}}$, output channels $C_{\\text{out}}$, and spatial resolution $H \\times W$, the multiplyâ€“accumulate count (MAC) is $H W C_{\\text{in}} k^2 C_{\\text{out}}$; the parameter count is $C_{\\text{in}} k^2 C_{\\text{out}}$.\n- For a depthwise convolution on $C$ channels with kernel size $k \\times k$ and spatial resolution $H \\times W$, the MAC is $H W C k^2$; the parameter count is $C k^2$.\n- A pointwise ($1 \\times 1$) convolution from $C_{\\text{in}}$ to $C_{\\text{out}}$ at resolution $H \\times W$ has MAC $H W C_{\\text{in}} C_{\\text{out}}$ and parameters $C_{\\text{in}} C_{\\text{out}}$.\n- The Mobile Inverted Bottleneck Convolution (MBConv) block with expansion factor $t$ and kernel size $k$ consists of: an expansion pointwise convolution from $C_{\\text{in}}$ to $t C_{\\text{in}}$, a depthwise convolution with kernel size $k \\times k$ on $t C_{\\text{in}}$ channels, an SE module that reduces channels by a ratio $\\rho_{\\text{se}}$ and then re-expands, and a projection pointwise convolution back to $C_{\\text{out}}$.\n- The Squeeze-and-Excitation (SE) module uses two fully connected or $1 \\times 1$ convolutional transforms: one from $t C_{\\text{in}}$ to $t C_{\\text{in}} \\rho_{\\text{se}}$ and one back from $t C_{\\text{in}} \\rho_{\\text{se}}$ to $t C_{\\text{in}}$. Its parameter count and MACs for these linear transforms are $t C_{\\text{in}} \\cdot (t C_{\\text{in}} \\rho_{\\text{se}}) + (t C_{\\text{in}} \\rho_{\\text{se}}) \\cdot t C_{\\text{in}}$. Global average pooling and elementwise nonlinearities are ignored in MACs.\n- Rounding rule: any channel count must be rounded to the nearest integer that is divisible by $8$.\n- Compound scaling principle: the total compute of a stage with width $w$, depth $d$ repeats, and square resolution $s \\times s$ scales approximately as a product in the form $s^2 w^2 d$ multiplied by constants from the block structure. Imposing a per-increment scaling coefficient $\\phi$ and fixed multipliers $(\\alpha, \\beta, \\gamma)$ that act on $(d, w, s)$ respectively, the doubling-of-compute constraint requires that increasing $\\phi$ by $1$ approximately doubles the compute, which yields a constraint of the form $\\alpha \\beta^2 \\gamma^2 \\approx 2$.\n\nYour program must:\n\n1. Implement an MBConv-with-SE cost model:\n   - Inputs: $C_{\\text{in}}$, $C_{\\text{out}}$, $t$, $k$, $s$, $\\rho_{\\text{se}}$.\n   - Outputs: per-block MACs and parameter count, using the foundational formulas above.\n   - For a stage with $d$ repeats, assume $C_{\\text{in}} = C_{\\text{out}} = w$ for all repeats and compute stage MACs as $d$ times the per-block MACs, applying the rounding rule for all channel counts to be divisible by $8$.\n\n2. Implement compound scaling:\n   - Inputs: baseline $(w_0, d_0, s_0)$, scaling multipliers $(\\alpha, \\beta, \\gamma)$, and scaling coefficient $\\phi$.\n   - Compute scaled $(w, d, s)$ by exponentially scaling each dimension by its respective multiplier for the given $\\phi$ under the doubling-of-compute constraint as described. Apply rounding to $w$ and $t w$ to be divisible by $8$, round $d$ to the nearest positive integer, and round $s$ to the nearest integer.\n\n3. Define a baseline stage (EfficientNet-B0 analog) and compute its stage MACs $N_0$ with MBConv settings:\n   - Baseline: $w_0 = 32$, $d_0 = 4$, $s_0 = 224$, $t = 6$, $k = 3$, $\\rho_{\\text{se}} = 0.25$, channel divisor $8$.\n\n4. For a scaled stage at coefficient $\\phi$, compute:\n   - Scaled stage MACs $N$ using the scaled $(w, d, s)$ from step $2$ and the MBConv settings from step $3$ unless otherwise specified in the test case.\n   - Target doubled-compute $N_{\\text{target}} = N_0 \\cdot 2^{\\phi}$.\n\n5. Define an accuracy proxy from widely observed neural scaling behavior:\n   - Use a normalized compute $x = N / N_0$.\n   - A proxy loss $L(x) = c_0 x^{-p} + c_1$, where $c_0$, $p$, and $c_1$ are positive constants, and the proxy accuracy $A(x) = 1 - L(x)$.\n   - Training hyperparameters induce an effective utilization factor $u = \\min(\\eta / \\eta^{\\ast}, 1) \\cdot \\min(B / B^{\\ast}, 1) \\cdot \\min(E / E^{\\ast}, 1)$, where $(\\eta, B, E)$ are the learning rate, batch size, and epoch count, and $(\\eta^{\\ast}, B^{\\ast}, E^{\\ast})$ are their respective optimal values.\n   - The realized accuracy is $A_{\\text{eff}} = A(x) \\cdot u$. The ideal target accuracy at coefficient $\\phi$ is $A_{\\text{target}} = 1 - \\left(c_0 \\left(2^{\\phi}\\right)^{-p} + c_1\\right)$.\n   - Define deviation $\\Delta = A_{\\text{eff}} - A_{\\text{target}}$.\n\n6. Use the following fixed constants and scaling multipliers:\n   - Scaling multipliers: $\\alpha = 1.2$, $\\beta = 1.1$, $\\gamma = 1.15$.\n   - Accuracy proxy constants: $c_0 = 0.4$, $p = 0.2$, $c_1 = 0.1$.\n   - Optimal hyperparameters: $\\eta^{\\ast} = 0.2$, $B^{\\ast} = 128$, $E^{\\ast} = 350$.\n\n7. Test suite:\n   - Case $1$: $\\phi = 0$, $t = 6$, $\\rho_{\\text{se}} = 0.25$, $B = 128$, $\\eta = 0.2$, $E = 350$.\n   - Case $2$: $\\phi = 1$, $t = 6$, $\\rho_{\\text{se}} = 0.25$, $B = 128$, $\\eta = 0.2$, $E = 350$.\n   - Case $3$: $\\phi = 2$, $t = 6$, $\\rho_{\\text{se}} = 0.25$, $B = 128$, $\\eta = 0.05$, $E = 350$.\n   - Case $4$: $\\phi = 3$, $t = 6$, $\\rho_{\\text{se}} = 0.25$, $B = 32$, $\\eta = 0.2$, $E = 200$.\n   - Case $5$: $\\phi = 1$, $t = 6$, $\\rho_{\\text{se}} = 0.5$, $B = 128$, $\\eta = 0.2$, $E = 350$.\n\nYour program should produce a single line of output containing the deviations $\\Delta$ for the cases in the order above, as a comma-separated list enclosed in square brackets, for example, $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5]$. No additional text should be printed.",
            "solution": "The problem requires the implementation of a computational model to analyze the compound scaling of a convolutional neural network (CNN) stage, based on the principles articulated for the EfficientNet family of models. The analysis involves calculating the computational cost (in Multiply-Accumulate operations, or MACs) of a stage, scaling its dimensions, and evaluating the resulting performance against an idealized target using a defined accuracy proxy. This process allows for the quantification of deviations from ideal scaling due to both architectural rounding and suboptimal training hyperparameters.\n\nThe solution is structured upon three foundational pillars: the cost model for the Mobile Inverted Bottleneck Convolution (MBConv) block, the compound scaling law for network dimensions, and an explicit model for accuracy and its deviation from an ideal target.\n\nFirst, we establish the cost model for a single MBConv block within a stage. A stage is defined by its width (number of channels, $w$), depth (number of block repetitions, $d$), and a square spatial resolution of side length $s$. For a block where input and output channels are both $w$, with an expansion factor $t$, a depthwise convolution kernel of size $k \\times k$, and a Squeeze-and-Excitation (SE) module with a reduction ratio $\\rho_{\\text{se}}$, the MAC count is the sum of its constituent operations. The problem specifies a rounding rule where all channel counts must be rounded to the nearest integer divisible by a divisor, given as $8$. Let $\\text{round}_m(x)$ denote the function that rounds $x$ to the nearest multiple of $m$. The channel counts used in the calculation are:\n- Expansion channels: $C_{\\text{exp}} = \\text{round}_8(t \\cdot w)$\n- SE-reduced channels: $C_{\\text{se}} = \\text{round}_8(C_{\\text{exp}} \\cdot \\rho_{\\text{se}})$\n\nThe total MACs for one MBConv block are the sum of the MACs for each component:\n1.  Expansion ($1 \\times 1$ conv): $s^2 \\cdot w \\cdot C_{\\text{exp}}$\n2.  Depthwise ($k \\times k$ conv): $s^2 \\cdot C_{\\text{exp}} \\cdot k^2$\n3.  Squeeze-and-Excitation (two linear layers on pooled features): $2 \\cdot C_{\\text{exp}} \\cdot C_{\\text{se}}$\n4.  Projection ($1 \\times 1$ conv): $s^2 \\cdot C_{\\text{exp}} \\cdot w$\n\nThe total MACs for a single block, $\\text{MACs}_{\\text{block}}$, is the sum of these four terms. The total MACs for the stage, $N$, is then $d \\cdot \\text{MACs}_{\\text{block}}$.\n\nSecond, we implement the compound scaling principle. Given a baseline architecture with depth $d_0$, width $w_0$, and resolution $s_0$, the dimensions are scaled by a coefficient $\\phi$ using specific multipliers $\\alpha$, $\\beta$, and $\\gamma$ for depth, width, and resolution, respectively. The scaled dimensions are given by:\n- $d(\\phi) = d_0 \\cdot \\alpha^{\\phi}$\n- $w(\\phi) = w_0 \\cdot \\beta^{\\phi}$\n- $s(\\phi) = s_0 \\cdot \\gamma^{\\phi}$\n\nThese raw scaled values are then rounded according to the problem's rules: $d$ is rounded to the nearest positive integer, $w$ is rounded to the nearest integer divisible by $8$, and $s$ is rounded to the nearest integer. The constraint $\\alpha \\beta^2 \\gamma^2 \\approx 2$ ensures that increasing $\\phi$ by $1$ approximately doubles the total computational cost, which is proportional to $d \\cdot w^2 \\cdot s^2$.\n\nThird, we define the accuracy and deviation models. The performance of a model is related to its computational budget through a power-law relationship. The proxy accuracy $A(x)$ is defined as $A(x) = 1 - L(x)$, where $L(x) = c_0 x^{-p} + c_1$ is the proxy loss. Here, $x = N/N_0$ is the computational budget of the scaled model normalized by the baseline model's budget $N_0$. The constants $c_0$, $c_1$, and $p$ are given. The ideal or target accuracy for a scaling level $\\phi$, $A_{\\text{target}}$, assumes perfect computational scaling, i.e., $N = N_0 \\cdot 2^\\phi$. Thus, $A_{\\text{target}}(\\phi) = 1 - (c_0 (2^\\phi)^{-p} + c_1)$.\nThe realized accuracy, $A_{\\text{eff}}$, is affected by training hyperparameters. This is modeled by a utilization factor $u$, defined as $u = \\min(\\eta / \\eta^{\\ast}, 1) \\cdot \\min(B / B^{\\ast}, 1) \\cdot \\min(E / E^{\\ast}, 1)$, where $(\\eta, B, E)$ are the learning rate, batch size, and epoch count, and $(\\eta^{\\ast}, B^{\\ast}, E^{\\ast})$ are their optimal values. The effective accuracy is then $A_{\\text{eff}} = A(x) \\cdot u$. The final quantity of interest is the deviation $\\Delta = A_{\\text{eff}} - A_{\\text{target}}$, which measures the difference between the model's realized accuracy and its ideal target.\n\nThe computational procedure for each test case is as follows:\n1.  Calculate the baseline MAC count $N_0$ using the given baseline parameters for $\\phi=0$.\n2.  For each test case with a specific set of $(\\phi, t, \\rho_{\\text{se}}, B, \\eta, E)$:\n    a. Compute the scaled dimensions $(w, d, s)$ using $\\phi$ and the specified rounding rules.\n    b. Calculate the stage MACs, $N$, using these scaled dimensions and the given MBConv parameters $(t, k, \\rho_{\\text{se}})$.\n    c. Compute the normalized compute $x = N/N_0$.\n    d. Calculate the utilization factor $u$ from the training hyperparameters.\n    e. Compute the effective accuracy $A_{\\text{eff}} = (1 - (c_0 x^{-p} + c_1)) \\cdot u$.\n    f. Compute the target accuracy $A_{\\text{target}} = 1 - (c_0 (2^\\phi)^{-p} + c_1)$.\n    g. Calculate and record the deviation $\\Delta = A_{\\text{eff}} - A_{\\text{target}}$.\n\nThe following program implements this logic to compute the deviations for the provided test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a cost and accuracy model for a CNN stage based on EfficientNet principles,\n    calculates performance deviations for a set of test cases.\n    \"\"\"\n\n    # --- Constants and Baseline Definitions ---\n\n    # Scaling multipliers\n    ALPHA = 1.2\n    BETA = 1.1\n    GAMMA = 1.15\n\n    # Accuracy proxy constants\n    C0 = 0.4\n    P = 0.2\n    C1 = 0.1\n\n    # Optimal hyperparameters\n    ETA_STAR = 0.2\n    B_STAR = 128\n    E_STAR = 350\n\n    # Baseline architecture and MBConv settings\n    W0 = 32\n    D0 = 4\n    S0 = 224\n    K_BASE = 3\n    CHANNEL_DIVISOR = 8\n\n    # Test suite\n    test_cases = [\n        # (phi, t, rho_se, B, eta, E)\n        (0, 6, 0.25, 128, 0.2, 350),\n        (1, 6, 0.25, 128, 0.2, 350),\n        (2, 6, 0.25, 128, 0.05, 350),\n        (3, 6, 0.25, 32, 0.2, 200),\n        (1, 6, 0.5, 128, 0.2, 350),\n    ]\n\n    # --- Helper Functions ---\n\n    def round_divisible(n, divisor):\n        \"\"\"Rounds n to the nearest integer divisible by divisor.\"\"\"\n        if divisor == 0:\n            return int(n)\n        return int(np.round(n / divisor) * divisor)\n\n    def calculate_stage_macs(w, d, s, t, k, rho_se, divisor):\n        \"\"\"Calculates total MACs for an MBConv stage.\"\"\"\n        \n        # Apply rounding to all channel counts\n        c_in = w # w is assumed pre-rounded\n        c_expand = round_divisible(t * c_in, divisor)\n        c_se = round_divisible(c_expand * rho_se, divisor)\n        # Ensure c_se is at least divisor if c_expand is not zero to avoid div by zero in some real cases,\n        # but problem says \"round\" not \"round up\". For n < divisor/2, this can become 0.\n        if c_se == 0 and c_expand > 0:\n             # As per strict problem spec, a 0 is possible. In a real net this would be an issue.\n             # e.g., if t*w*rho_se < 4. We will follow spec.\n             pass\n        c_out = w\n\n        s_squared = s * s\n        \n        # MACs for each part of a single MBConv block\n        macs_expansion = s_squared * c_in * c_expand\n        macs_depthwise = s_squared * c_expand * (k * k)\n        macs_se = 2 * c_expand * c_se\n        macs_projection = s_squared * c_expand * c_out\n        \n        macs_per_block = macs_expansion + macs_depthwise + macs_se + macs_projection\n        \n        total_macs = d * macs_per_block\n        return total_macs\n\n    def scale_dimensions(phi, w0, d0, s0, alpha, beta, gamma, divisor):\n        \"\"\"Scales network dimensions based on phi and rounds them.\"\"\"\n        d_scaled = d0 * (alpha ** phi)\n        w_scaled = w0 * (beta ** phi)\n        s_scaled = s0 * (gamma ** phi)\n        \n        # Rounding as per problem specification\n        d = max(1, int(np.round(d_scaled))) # nearest positive integer\n        w = round_divisible(w_scaled, divisor)\n        s = int(np.round(s_scaled))\n        \n        return w, d, s\n\n    # --- Main Calculation Logic ---\n\n    results = []\n\n    # 1. Calculate baseline MACs (N0)\n    w_base, d_base, s_base = scale_dimensions(0, W0, D0, S0, ALPHA, BETA, GAMMA, CHANNEL_DIVISOR)\n    base_case = test_cases[0]\n    n0 = calculate_stage_macs(\n        w=w_base, \n        d=d_base, \n        s=s_base, \n        t=base_case[1], \n        k=K_BASE, \n        rho_se=base_case[2], \n        divisor=CHANNEL_DIVISOR\n    )\n\n    if n0 == 0:\n        raise ValueError(\"Baseline MAC count (N0) is zero, preventing normalization.\")\n\n    # 2. Iterate through test cases\n    for case in test_cases:\n        phi, t, rho_se, b, eta, e = case\n        \n        # a. Compute scaled dimensions\n        w, d, s = scale_dimensions(phi, W0, D0, S0, ALPHA, BETA, GAMMA, CHANNEL_DIVISOR)\n        \n        # b. Calculate actual stage MACs (N)\n        n = calculate_stage_macs(w, d, s, t, K_BASE, rho_se, CHANNEL_DIVISOR)\n        \n        # c. Compute normalized compute (x)\n        x = n / n0\n        \n        # d. Calculate utilization factor (u)\n        u = min(eta / ETA_STAR, 1) * min(b / B_STAR, 1) * min(e / E_STAR, 1)\n        \n        # e. Compute effective accuracy (A_eff)\n        loss_x = C0 * (x ** -P) + C1\n        a_x = 1 - loss_x\n        a_eff = a_x * u\n        \n        # f. Compute target accuracy (A_target)\n        loss_target = C0 * ((2 ** phi) ** -P) + C1\n        a_target = 1 - loss_target\n        \n        # g. Calculate deviation (Delta)\n        delta = a_eff - a_target\n        results.append(delta)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Scaling a network's input resolution is a powerful tool, but it comes with practical challenges that go beyond simple computational cost. This practice delves into a subtle but critical consequence: the mismatch in Batch Normalization statistics when a model is trained at one resolution and evaluated at another. You will model this statistical drift and explore a post-hoc recalibration strategy, gaining insight into the practical steps required to maintain model performance after scaling .",
            "id": "3119502",
            "problem": "Consider an image classification network following the EfficientNet family, which uses compound scaling to adjust width, depth, and input resolution. Focus on the effect of evaluating at a different input resolution while keeping the network weights fixed. The network contains Batch Normalization (BN), that is, for a pre-BN scalar activation $x$, the BN transform is $y = a \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^{2} + \\varepsilon}} + b$, where $a$ and $b$ are the learned affine parameters, and $(\\mu, \\sigma^{2})$ are the running estimates collected during training at a fixed training resolution $r$. Suppose training is done at a fixed resolution $r$ and evaluation is done at $r' = \\gamma \\cdot r$ with $\\gamma > 0$.\n\nModel the single-channel pre-BN activation distribution as follows. At training resolution, assume $x \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$. At evaluation resolution $r' = \\gamma \\cdot r$, due to changes in effective receptive-field statistics, assume the pre-BN variance scales according to a power law,\n$$\n\\sigma^{2}(\\gamma) = \\sigma_{0}^{2} \\cdot \\gamma^{-\\beta},\n$$\nfor some exponent $\\beta > 0$, while the mean remains unchanged at $\\mu_{0}$. This is a stylized but plausible model for local averaging effects with increased resolution. When evaluating without any recalibration of BN statistics, the normalized activation variance after BN becomes\n$$\nv_{\\text{no-recal}}(\\gamma) = \\frac{\\sigma^{2}(\\gamma)}{\\sigma_{0}^{2}} = \\gamma^{-\\beta}.\n$$\n\nNow consider post-hoc BN recalibration: given $n$ independent calibration samples at resolution $r'$, recompute the BN variance using the unbiased sample variance $s^{2}$. Use the well-tested fact from classical statistics that if $x$ is normally distributed with true variance $\\sigma^{2}(\\gamma)$, then $(n-1)\\,s^{2}/\\sigma^{2}(\\gamma)$ follows a chi-square distribution with $n-1$ degrees of freedom. Define the recalibrated normalized variance as $v_{\\text{recal}} = \\sigma^{2}(\\gamma)/s^{2}$. For $n \\leq 3$, this expectation is not finite; therefore, in such cases treat the calibration as insufficient and default to using $v_{\\text{no-recal}}(\\gamma)$.\n\nDefine a smooth proxy for top-$1$ accuracy as a function of the normalized variance mismatch,\n$$\n\\text{Acc}(v) = A_{0} \\cdot \\exp\\!\\big(-k \\cdot |1 - v|\\big),\n$$\nwhere $A_{0} \\in (0,1]$ is the baseline accuracy at matched statistics and $k > 0$ is a sensitivity coefficient. All accuracies must be expressed as a decimal in $[0,1]$.\n\nYour task is to write a program that, for each test case, computes the predicted accuracy under:\n- no recalibration, using $v_{\\text{no-recal}}(\\gamma)$, and\n- post-hoc BN recalibration of the variance using $n$ samples, where for $n > 3$ you must derive and use $\\mathbb{E}[v_{\\text{recal}}]$ implied by the chi-square property above, and for $n \\leq 3$ you must use $v_{\\text{no-recal}}(\\gamma)$.\n\nUse the following fixed constants in all computations: $A_{0} = 0.82$, $k = 1.25$, $\\beta = 0.6$. For each test case, the program should output the recalibrated-accuracy prediction (using $n$ as specified by the test case). No physical units or angles are involved; report all accuracies as decimals.\n\nTest suite:\n- Case $1$: $(\\gamma, n) = (1.0, 0)$.\n- Case $2$: $(\\gamma, n) = (1.4, 0)$.\n- Case $3$: $(\\gamma, n) = (1.4, 32)$.\n- Case $4$: $(\\gamma, n) = (0.7, 32)$.\n- Case $5$: $(\\gamma, n) = (2.0, 3)$.\n- Case $6$: $(\\gamma, n) = (1.4, 1000)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, \"[result1,result2,...]\"), in the same order as the test suite. Each result must be a single floating-point number equal to the predicted accuracy under the specified calibration setting.",
            "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded in statistical theory and deep learning principles, well-posed with a unique and derivable solution, and internally consistent.\n\nThe problem asks for the computation of a predicted accuracy metric for an image classification network under different input resolutions and Batch Normalization (BN) recalibration scenarios. The core of the problem involves determining the effective normalized variance mismatch, denoted by $v$, and substituting it into a given accuracy model.\n\nFirst, let us establish the two scenarios for determining the value of $v$.\nThe problem specifies that for a given resolution scaling factor $\\gamma$, the true variance of pre-BN activations is $\\sigma^2(\\gamma) = \\sigma_0^2 \\cdot \\gamma^{-\\beta}$, where $\\sigma_0^2$ is the variance at the training resolution. The network's stored BN statistics use $\\sigma_0^2$.\n\n1.  **No Recalibration or Insufficient Calibration ($n \\le 3$):**\n    In this case, the network uses its original BN statistics. The mismatch between the true variance at the evaluation resolution, $\\sigma^2(\\gamma)$, and the stored variance from training, $\\sigma_0^2$, is directly used. The normalized variance mismatch is defined as:\n    $$\n    v = v_{\\text{no-recal}}(\\gamma) = \\frac{\\sigma^{2}(\\gamma)}{\\sigma_{0}^{2}} = \\gamma^{-\\beta}\n    $$\n    This applies when no recalibration is performed ($n=0$) or when the number of calibration samples $n$ is less than or equal to $3$.\n\n2.  **Post-Hoc Recalibration ($n > 3$):**\n    Here, the BN variance is re-estimated using an unbiased sample variance $s^2$ from $n$ samples. The problem asks us to use the expectation of the recalibrated normalized variance, $v_{\\text{recal}} = \\sigma^2(\\gamma)/s^2$, as the effective mismatch $v$. We must derive $\\mathbb{E}[v_{\\text{recal}}]$.\n\n    We are given that the quantity $Y = \\frac{(n-1)s^2}{\\sigma^2(\\gamma)}$ follows a chi-square distribution with $\\nu = n-1$ degrees of freedom, i.e., $Y \\sim \\chi^2_{n-1}$. We need to compute:\n    $$\n    \\mathbb{E}[v_{\\text{recal}}] = \\mathbb{E}\\left[\\frac{\\sigma^2(\\gamma)}{s^2}\\right]\n    $$\n    From the definition of $Y$, we can express $s^2$ as $s^2 = \\frac{\\sigma^2(\\gamma)}{n-1} Y$. Substituting this into the expectation gives:\n    $$\n    \\mathbb{E}[v_{\\text{recal}}] = \\mathbb{E}\\left[\\frac{\\sigma^2(\\gamma)}{\\frac{\\sigma^2(\\gamma)}{n-1} Y}\\right] = \\mathbb{E}\\left[\\frac{n-1}{Y}\\right] = (n-1) \\mathbb{E}\\left[\\frac{1}{Y}\\right]\n    $$\n    To find $\\mathbb{E}[1/Y]$, we use the definition of expectation for a continuous random variable with probability density function $f_Y(y)$. For $Y \\sim \\chi^2_{\\nu}$, the PDF is $f_Y(y) = \\frac{1}{2^{\\nu/2}\\Gamma(\\nu/2)} y^{\\nu/2-1}e^{-y/2}$ for $y > 0$.\n    $$\n    \\mathbb{E}\\left[\\frac{1}{Y}\\right] = \\int_{0}^{\\infty} \\frac{1}{y} f_Y(y) \\, dy = \\int_{0}^{\\infty} \\frac{1}{y} \\frac{1}{2^{\\nu/2}\\Gamma(\\nu/2)} y^{\\nu/2-1}e^{-y/2} \\, dy\n    $$\n    $$\n    \\mathbb{E}\\left[\\frac{1}{Y}\\right] = \\frac{1}{2^{\\nu/2}\\Gamma(\\nu/2)} \\int_{0}^{\\infty} y^{(\\nu/2-1)-1}e^{-y/2} \\, dy\n    $$\n    The integral is related to the Gamma function, $\\Gamma(z) = \\int_0^\\infty t^{z-1}e^{-t}dt$. Let $t = y/2$, so $y=2t$ and $dy=2dt$.\n    $$\n    \\int_{0}^{\\infty} (2t)^{(\\nu/2-1)-1}e^{-t} (2dt) = 2^{\\nu/2-2} \\cdot 2 \\int_{0}^{\\infty} t^{\\nu/2-2}e^{-t} dt = 2^{\\nu/2-1} \\Gamma(\\nu/2 - 1)\n    $$\n    Substituting this back:\n    $$\n    \\mathbb{E}\\left[\\frac{1}{Y}\\right] = \\frac{1}{2^{\\nu/2}\\Gamma(\\nu/2)} \\cdot 2^{\\nu/2-1} \\Gamma(\\nu/2 - 1) = \\frac{1}{2} \\frac{\\Gamma(\\nu/2 - 1)}{\\Gamma(\\nu/2)}\n    $$\n    Using the property $\\Gamma(z) = (z-1)\\Gamma(z-1)$, we get $\\Gamma(\\nu/2) = (\\nu/2-1)\\Gamma(\\nu/2-1)$.\n    $$\n    \\mathbb{E}\\left[\\frac{1}{Y}\\right] = \\frac{1}{2} \\frac{1}{\\nu/2 - 1} = \\frac{1}{\\nu-2}\n    $$\n    This expectation is finite only if the argument to the Gamma function, $\\nu/2 - 1$, is positive, which means $\\nu > 2$. Since $\\nu = n-1$, this requires $n-1 > 2$, or $n > 3$. This confirms the problem's assertion that the expectation is not finite for $n \\le 3$.\n\n    Finally, we find the expectation of the recalibrated normalized variance for $n > 3$:\n    $$\n    v = \\mathbb{E}[v_{\\text{recal}}] = (n-1) \\mathbb{E}\\left[\\frac{1}{Y}\\right] = (n-1) \\frac{1}{(n-1)-2} = \\frac{n-1}{n-3}\n    $$\n    Notably, this expected mismatch value depends only on the number of calibration samples $n$ and not on the resolution scaling factor $\\gamma$.\n\nWith the derivation of $v$ for all cases, we can compute the predicted accuracy using the provided model:\n$$\n\\text{Acc}(v) = A_{0} \\cdot \\exp(-k \\cdot |1 - v|)\n$$\nThe constants are given as $A_0 = 0.82$, $k = 1.25$, and $\\beta = 0.6$.\n\nThe overall algorithm for each test case $(\\gamma, n)$ is:\n1.  Read the input values for $\\gamma$ and $n$.\n2.  If $n \\le 3$, calculate the normalized variance mismatch as $v = \\gamma^{-\\beta}$.\n3.  If $n > 3$, calculate the normalized variance mismatch as $v = \\frac{n-1}{n-3}$.\n4.  Compute the accuracy using $\\text{Acc}(v) = 0.82 \\cdot \\exp(-1.25 \\cdot |1 - v|)$.\n5.  The final program will execute this logic for each test case in the provided suite and format the results as specified.",
            "answer": "```python\nimport numpy as np\n# The problem can be solved with numpy alone; scipy is not required for the final computation.\n\ndef solve():\n    \"\"\"\n    Computes the predicted accuracy for a series of test cases based on a model\n    of Batch Normalization statistics mismatch in a neural network.\n    \"\"\"\n    # Define fixed constants from the problem statement.\n    A0 = 0.82\n    k = 1.25\n    beta = 0.6\n\n    # Define the test cases from the problem statement as a list of tuples (gamma, n).\n    test_cases = [\n        (1.0, 0),   # Case 1\n        (1.4, 0),   # Case 2\n        (1.4, 32),  # Case 3\n        (0.7, 32),  # Case 4\n        (2.0, 3),   # Case 5\n        (1.4, 1000) # Case 6\n    ]\n\n    results = []\n    for gamma, n in test_cases:\n        # Determine the effective normalized variance mismatch, v.\n        # The logic is split based on the number of calibration samples, n.\n\n        if n <= 3:\n            # For n <= 3, calibration is insufficient. Default to the no-recalibration case.\n            # The variance mismatch is calculated based on the resolution scaling factor gamma.\n            # v = v_no-recal(gamma) = gamma^(-beta)\n            v = gamma ** (-beta)\n        else:\n            # For n > 3, post-hoc BN recalibration is performed.\n            # The effective variance mismatch is the expectation of the recalibrated normalized variance.\n            # This was derived as E[v_recal] = (n-1) / (n-3).\n            v = (n - 1) / (n - 3)\n\n        # Calculate the predicted accuracy using the provided model.\n        # Acc(v) = A0 * exp(-k * |1 - v|)\n        accuracy = A0 * np.exp(-k * np.abs(1 - v))\n        results.append(accuracy)\n\n    # Format the final output as a comma-separated list of floating-point numbers\n    # enclosed in square brackets, as per the problem specification.\n    print(f\"[{','.join(f'{res:.12f}' for res in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "The elegance of EfficientNet lies in its principled approach, but where do the specific scaling coefficients for depth, width, and resolution originate? This capstone exercise places you in the role of a network architect, tasking you with deriving these coefficients from first principles. By setting up and solving a multi-objective optimization problem that balances model accuracy against hardware-specific latency and memory costs, you will uncover the fundamental trade-offs that drive the design of efficient neural architectures .",
            "id": "3119675",
            "problem": "You are tasked with implementing a self-contained program that computes optimal compound scaling coefficients for an EfficientNet-style Convolutional Neural Network (CNN). The optimization must balance model accuracy against latency and memory costs, under a realistic hardware budget constraint. Your program must solve a multi-objective optimization by scalarization.\n\nStart from the following empirically grounded and widely used scaling relationships in convolutional architectures:\n\n1. For a stack of convolutional layers in a CNN, the floating-point compute required scales proportionally to the product of depth, squared width, and squared input resolution. Let the scaling multipliers for depth, width, and resolution be denoted by $\\alpha$, $\\beta$, and $\\gamma$ respectively. Then an idealized latency measure $L$ (in normalized, dimensionless units) is modeled as\n$$\nL(\\alpha,\\beta,\\gamma) = k_L \\, \\alpha \\, \\beta^2 \\, \\gamma^2,\n$$\nwhere $k_L$ is a positive constant capturing baseline compute-to-latency proportionality.\n\n2. The total memory footprint $M$ (in normalized, dimensionless units), combining parameter memory and activation memory, is modeled as\n$$\nM(\\alpha,\\beta,\\gamma) = k_P \\, \\alpha \\, \\beta^2 \\;+\\; k_A \\, \\alpha \\, \\beta \\, \\gamma^2,\n$$\nwhere $k_P$ represents proportionality for parameter memory and $k_A$ for activation memory. These forms follow from parameters scaling roughly with the product of depth and squared width, while activations scale with depth, width, and squared resolution.\n\n3. Accuracy $A$ (in an abstract accuracy score unit that is dimensionless) exhibits diminishing returns with increasing scale. A concave surrogate consistent with empirical scaling laws is\n$$\nA(\\alpha,\\beta,\\gamma) = s_d \\, \\ln(\\alpha) \\;+\\; s_w \\, \\ln(\\beta) \\;+\\; s_r \\, \\ln(\\gamma),\n$$\nwhere $s_d$, $s_w$, and $s_r$ are positive coefficients encoding the relative accuracy sensitivity to depth, width, and resolution.\n\nThe multi-objective optimization is scalarized into\n$$\nJ(\\alpha,\\beta,\\gamma;\\lambda_1,\\lambda_2) \\;=\\; A(\\alpha,\\beta,\\gamma) \\;-\\; \\lambda_1 \\, L(\\alpha,\\beta,\\gamma) \\;-\\; \\lambda_2 \\, M(\\alpha,\\beta,\\gamma),\n$$\nwhere $\\lambda_1$ and $\\lambda_2$ are nonnegative hardware-specific weights penalizing latency and memory respectively, both in dimensionless normalized units.\n\nOptimization constraints:\n- Compound compute budget:\n$$\n\\alpha \\, \\beta^2 \\, \\gamma^2 \\;\\le\\; B,\n$$\nwith $B \\ge 1$ a dimensionless budget. This inequality reflects that available compute may be underutilized if penalties are high; it must never be exceeded.\n- Lower bounds:\n$$\n\\alpha \\ge 1, \\quad \\beta \\ge 1, \\quad \\gamma \\ge 1.\n$$\nThese lower bounds enforce non-shrinking scaling (no downscaling below baseline).\n- Upper bounds:\n$$\n\\alpha \\le U, \\quad \\beta \\le U, \\quad \\gamma \\le U,\n$$\nwhere $U$ is a fixed upper limit to ensure well-posedness and reflect practical upper bounds on scaling.\n\nConstants to use in all computations:\n- $k_L = 1.0$, $k_P = 1.0$, $k_A = 0.5$,\n- $s_d = 0.20$, $s_w = 0.30$, $s_r = 0.50$,\n- $U = 5.0$.\n\nYour program must numerically maximize $J(\\alpha,\\beta,\\gamma;\\lambda_1,\\lambda_2)$ subject to the constraints. You may solve this by any correct numerical method; a constrained optimizer for differentiable objectives is acceptable. All quantities $L$, $M$, $A$, $J$, $\\alpha$, $\\beta$, $\\gamma$, $\\lambda_1$, $\\lambda_2$, $B$, $k_L$, $k_P$, $k_A$, $s_d$, $s_w$, $s_r$, and $U$ are dimensionless and must be treated as such.\n\nTest Suite:\nRun your solver on the following hardware-specific penalty weights and budgets $(\\lambda_1,\\lambda_2,B)$:\n- Case 1 (balanced, happy path): $(0.02,\\, 0.01,\\, 2.0)$\n- Case 2 (memory-dominated penalty): $(0.0,\\, 0.03,\\, 3.0)$\n- Case 3 (latency-dominated penalty): $(0.05,\\, 0.0,\\, 3.0)$\n- Case 4 (very high penalties, likely minimal scaling): $(0.20,\\, 0.10,\\, 2.0)$\n- Case 5 (boundary budget): $(0.01,\\, 0.005,\\, 1.0)$\n\nRequired final output:\n- For each test case, compute the optimal triple $(\\alpha^\\star,\\beta^\\star,\\gamma^\\star)$.\n- Round each scalar in the triple to $4$ decimal places.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets, where each element is the rounded triple as a Python-style list. For example, the output should look like\n$$\n[\\,[\\alpha_1,\\beta_1,\\gamma_1],\\,[\\alpha_2,\\beta_2,\\gamma_2],\\,\\dots\\,]\n$$\nwith each $\\alpha_i$, $\\beta_i$, and $\\gamma_i$ rounded to $4$ decimals. No additional text should be printed.",
            "solution": "The user has provided a well-defined multi-objective optimization problem, which I will first validate and then solve.\n\n### Step 1: Extract Givens\n\n**Models:**\n- Latency: $L(\\alpha,\\beta,\\gamma) = k_L \\, \\alpha \\, \\beta^2 \\, \\gamma^2$\n- Memory: $M(\\alpha,\\beta,\\gamma) = k_P \\, \\alpha \\, \\beta^2 \\;+\\; k_A \\, \\alpha \\, \\beta \\, \\gamma^2$\n- Accuracy: $A(\\alpha,\\beta,\\gamma) = s_d \\, \\ln(\\alpha) \\;+\\; s_w \\, \\ln(\\beta) \\;+\\; s_r \\, \\ln(\\gamma)$\n\n**Objective Function:**\n- Scalarized objective to maximize: $J(\\alpha,\\beta,\\gamma;\\lambda_1,\\lambda_2) \\;=\\; A(\\alpha,\\beta,\\gamma) \\;-\\; \\lambda_1 \\, L(\\alpha,\\beta,\\gamma) \\;-\\; \\lambda_2 \\, M(\\alpha,\\beta,\\gamma)$\n\n**Constraints:**\n- Compute budget: $\\alpha \\, \\beta^2 \\, \\gamma^2 \\;\\le\\; B$, with $B \\ge 1$\n- Lower bounds: $\\alpha \\ge 1, \\quad \\beta \\ge 1, \\quad \\gamma \\ge 1$\n- Upper bounds: $\\alpha \\le U, \\quad \\beta \\le U, \\quad \\gamma \\le U$\n\n**Constants:**\n- $k_L = 1.0$\n- $k_P = 1.0$\n- $k_A = 0.5$\n- $s_d = 0.20$\n- $s_w = 0.30$\n- $s_r = 0.50$\n- $U = 5.0$\n\n**Test Suite:**\n- Case 1: $(\\lambda_1, \\lambda_2, B) = (0.02, 0.01, 2.0)$\n- Case 2: $(\\lambda_1, \\lambda_2, B) = (0.0, 0.03, 3.0)$\n- Case 3: $(\\lambda_1, \\lambda_2, B) = (0.05, 0.0, 3.0)$\n- Case 4: $(\\lambda_1, \\lambda_2, B) = (0.20, 0.10, 2.0)$\n- Case 5: $(\\lambda_1, \\lambda_2, B) = (0.01, 0.005, 1.0)$\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientifically Grounded:** The problem is grounded in the principles of neural network architecture design, specifically relating to compound scaling as popularized by EfficientNet. The functional forms for latency ($L$), memory ($M$), and accuracy ($A$) are simplified but plausible models. The scaling of compute with depth ($\\alpha$), width squared ($\\beta^2$), and resolution squared ($\\gamma^2$) is standard. The memory model correctly distinguishes between parameters (scaling with $\\alpha \\beta^2$) and activations (scaling with $\\alpha \\beta \\gamma^2$). The logarithmic accuracy model reflects the principle of diminishing returns. The formulation is a valid abstraction of a real-world engineering problem in deep learning.\n\n2.  **Well-Posed:** The problem is to maximize a continuous, differentiable function $J$ over a compact (closed and bounded) subset of $\\mathbb{R}^3$. The feasible region is defined by the inequalities $1 \\le \\alpha \\le U$, $1 \\le \\beta \\le U$, $1 \\le \\gamma \\le U$, and $\\alpha \\beta^2 \\gamma^2 \\le B$. By the Extreme Value Theorem, a continuous function on a compact set must attain a maximum. Therefore, a solution is guaranteed to exist. The problem is well-posed.\n\n3.  **Objective:** The problem is stated using precise mathematical definitions and objective, formal language. All constants and variables are clearly defined. There is no ambiguity.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. It is a well-posed, scientifically grounded, and objective constrained non-linear optimization problem. I will proceed with the solution.\n\n### Solution Derivation\n\nThe task is to find the values of $(\\alpha, \\beta, \\gamma)$ that maximize the objective function $J$ subject to the given constraints. This is a constrained non-linear programming problem. Since numerical optimization libraries are typically designed to find minima, we will equivalently minimize the negative of the objective function, $-J$.\n\nLet the vector of variables be $\\mathbf{x} = [\\alpha, \\beta, \\gamma]$. The objective function to minimize is:\n$$\n-J(\\mathbf{x}; \\lambda_1, \\lambda_2) = -A(\\mathbf{x}) + \\lambda_1 L(\\mathbf{x}) + \\lambda_2 M(\\mathbf{x})\n$$\nSubstituting the given expressions and constants:\n$$\n-J(\\alpha, \\beta, \\gamma) = -[s_d \\ln(\\alpha) + s_w \\ln(\\beta) + s_r \\ln(\\gamma)] + \\lambda_1 [k_L \\alpha \\beta^2 \\gamma^2] + \\lambda_2 [k_P \\alpha \\beta^2 + k_A \\alpha \\beta \\gamma^2]\n$$\nGrouping terms by the variables $\\alpha, \\beta, \\gamma$:\n$$\n-J(\\alpha, \\beta, \\gamma) = -s_d \\ln(\\alpha) - s_w \\ln(\\beta) - s_r \\ln(\\gamma) + (\\lambda_1 k_L + \\lambda_2 k_A) \\alpha \\beta^2 \\gamma^2 + \\lambda_2 k_P \\alpha \\beta^2\n$$\nSubstituting the known constant values ($k_L=1.0, k_P=1.0, k_A=0.5, s_d=0.2, s_w=0.3, s_r=0.5$):\n$$\n-J(\\alpha, \\beta, \\gamma) = -0.2 \\ln(\\alpha) - 0.3 \\ln(\\beta) - 0.5 \\ln(\\gamma) + (\\lambda_1 + 0.5 \\lambda_2) \\alpha \\beta^2 \\gamma^2 + \\lambda_2 \\alpha \\beta^2\n$$\nThis is the function we will provide to a numerical optimizer.\n\nThe constraints are:\n1.  **Inequality Constraint:** $\\alpha \\beta^2 \\gamma^2 \\le B$. For standard solvers that require constraints of the form $g(\\mathbf{x}) \\ge 0$, this is written as $B - \\alpha \\beta^2 \\gamma^2 \\ge 0$.\n2.  **Bound Constraints:**\n    $1 \\le \\alpha \\le U$\n    $1 \\le \\beta \\le U$\n    $1 \\le \\gamma \\le U$\n    with $U=5.0$.\n\nWe will use the Sequential Least Squares Programming (`SLSQP`) method from the `scipy.optimize` library, as it is well-suited for non-linear optimization problems with both bound and inequality constraints.\n\nWe will iterate through each test case $(\\lambda_1, \\lambda_2, B)$, defining the objective function and constraints accordingly. A robust initial guess for the optimizer is the baseline model $(\\alpha, \\beta, \\gamma) = (1, 1, 1)$, which is guaranteed to be in the feasible region for any $B \\ge 1$.\n\nFor the special case where $B=1.0$ (Test Case 5), the constraints $\\alpha \\ge 1, \\beta \\ge 1, \\gamma \\ge 1$ and $\\alpha \\beta^2 \\gamma^2 \\le 1$ together force the only possible solution to be $\\alpha=1, \\beta=1, \\gamma=1$. The optimizer should converge to this point.\n\nThe final result for each test case will be the optimal triple $(\\alpha^\\star, \\beta^\\star, \\gamma^\\star)$, with each component rounded to $4$ decimal places, formatted as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Computes optimal compound scaling coefficients for a CNN model by solving\n    a constrained multi-objective optimization problem.\n    \"\"\"\n    # Define constants as specified in the problem statement.\n    # All quantities are dimensionless.\n    k_L = 1.0  # Latency proportionality\n    k_P = 1.0  # Parameter memory proportionality\n    k_A = 0.5  # Activation memory proportionality\n    s_d = 0.20 # Accuracy sensitivity to depth\n    s_w = 0.30 # Accuracy sensitivity to width\n    s_r = 0.50 # Accuracy sensitivity to resolution\n    U = 5.0    # Upper bound for scaling factors\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (lambda1, lambda2, B)\n    test_cases = [\n        (0.02, 0.01, 2.0),   # Case 1: balanced, happy path\n        (0.0, 0.03, 3.0),    # Case 2: memory-dominated penalty\n        (0.05, 0.0, 3.0),    # Case 3: latency-dominated penalty\n        (0.20, 0.10, 2.0),   # Case 4: very high penalties\n        (0.01, 0.005, 1.0),  # Case 5: boundary budget\n    ]\n\n    results = []\n\n    # Define the objective function to be minimized (-J)\n    def objective_function(x, lambda1, lambda2):\n        \"\"\"\n        Calculates the negative of the scalarized objective function J.\n        We minimize -J to maximize J.\n        \n        x: numpy array [alpha, beta, gamma]\n        lambda1: latency penalty weight\n        lambda2: memory penalty weight\n        \"\"\"\n        alpha, beta, gamma = x[0], x[1], x[2]\n        \n        # Guard against log(x) for x<=0, though bounds should prevent this.\n        if alpha <= 0 or beta <= 0 or gamma <= 0:\n            return np.inf\n\n        accuracy_term = s_d * np.log(alpha) + s_w * np.log(beta) + s_r * np.log(gamma)\n        \n        latency_term = k_L * alpha * beta**2 * gamma**2\n        \n        memory_term = k_P * alpha * beta**2 + k_A * alpha * beta * gamma**2\n        \n        penalty = lambda1 * latency_term + lambda2 * memory_term\n        \n        # Return negative J for minimization\n        return -(accuracy_term - penalty)\n\n    for case in test_cases:\n        lambda1, lambda2, B = case\n\n        # Define the inequality constraint: alpha * beta^2 * gamma^2 <= B\n        # Scipy's SLSQP expects constraints in the form g(x) >= 0\n        constraint = {\n            'type': 'ineq',\n            'fun': lambda x: B - x[0] * x[1]**2 * x[2]**2\n        }\n\n        # Define the bounds for alpha, beta, gamma: 1 <= x_i <= U\n        bounds = ((1.0, U), (1.0, U), (1.0, U))\n\n        # Initial guess: the baseline model, which is always feasible.\n        x0 = np.array([1.0, 1.0, 1.0])\n\n        # Perform the constrained optimization\n        opt_result = minimize(\n            fun=objective_function,\n            x0=x0,\n            args=(lambda1, lambda2),\n            method='SLSQP',\n            bounds=bounds,\n            constraints=[constraint],\n            tol=1e-9\n        )\n\n        # Extract the optimal scaling factors\n        optimal_coeffs = opt_result.x\n\n        # Round the results to 4 decimal places\n        rounded_coeffs = np.round(optimal_coeffs, 4).tolist()\n        results.append(rounded_coeffs)\n    \n    # Format the final output string exactly as required\n    # e.g., [[1.0, 1.0, 1.0],[...]] without extra spaces in the numbers.\n    # The default str() representation of a list is what's needed.\n    # Example: str([1.23, 4.56]) -> '[1.23, 4.56]'\n    output_str = f\"[{','.join(map(str, results))}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}