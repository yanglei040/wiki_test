{
    "hands_on_practices": [
        {
            "introduction": "To truly master a concept, it's essential to understand its inner workings. This first practice takes you under the hood of automatic differentiation frameworks by tasking you with deriving the gradients for a $1 \\times 1$ convolution from first principles . By working through the calculus and then verifying your results against a numerical gradient check, you will build a foundational understanding of how these layers learn and gain a critical debugging skill for your own future implementations.",
            "id": "3094339",
            "problem": "A $1 \\times 1$ convolution applies the same per-location linear map across channels. Consider an input tensor $X \\in \\mathbb{R}^{N \\times H \\times W \\times C_{\\text{in}}}$, weights $W \\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}}}$, and bias $b \\in \\mathbb{R}^{C_{\\text{out}}}$. The $1 \\times 1$ convolution output $Y \\in \\mathbb{R}^{N \\times H \\times W \\times C_{\\text{out}}}$ is defined componentwise by\n$$\nY[n,h,w,c] \\;=\\; \\sum_{k=1}^{C_{\\text{in}}} W[c,k]\\,X[n,h,w,k] \\;+\\; b[c],\n$$\nfor all $n \\in \\{1,\\dots,N\\}$, $h \\in \\{1,\\dots,H\\}$, $w \\in \\{1,\\dots,W\\}$, and $c \\in \\{1,\\dots,C_{\\text{out}}\\}$. Given a target tensor $T \\in \\mathbb{R}^{N \\times H \\times W \\times C_{\\text{out}}}$, define the mean-squared error loss\n$$\nL \\;=\\; \\frac{1}{2 N H W} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} \\sum_{c=1}^{C_{\\text{out}}} \\big(Y[n,h,w,c] - T[n,h,w,c]\\big)^{2}.\n$$\nWork from first principles of multivariate calculus (the chain rule and linearity) and the above definitions of the $1 \\times 1$ convolution and loss. Do not assume any pre-derived gradient formulas.\n\nConsider the concrete setting with $N=1$, $H=1$, $W=2$, $C_{\\text{in}}=2$, $C_{\\text{out}}=2$. A simple pseudorandom generator produced the following realized tensors (treat these as fixed values for this exercise):\n- Input $X$ has two spatial positions $(h,w)=(1,1)$ and $(h,w)=(1,2)$ with channel vectors\n$$\nx_{1} \\;=\\; \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad\nx_{2} \\;=\\; \\begin{pmatrix} 2 \\\\ 0.5 \\end{pmatrix}.\n$$\n- Weights and bias are\n$$\nW \\;=\\; \\begin{pmatrix} 1 & 2 \\\\ -1 & 0.5 \\end{pmatrix}, \\qquad\nb \\;=\\; \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix}.\n$$\n- Targets at the two spatial positions are\n$$\nt_{1} \\;=\\; \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}, \\quad\nt_{2} \\;=\\; \\begin{pmatrix} 3 \\\\ -2.5 \\end{pmatrix}.\n$$\n\nTasks:\n1. Starting from the definitions above, derive symbolic formulas for the gradients $\\nabla_{W} L$, $\\nabla_{b} L$, and $\\nabla_{X} L$ for general $X$, $W$, $b$, and $T$.\n2. Evaluate your analytical expressions for the specific numerical tensors given to obtain a concrete numerical matrix for $\\nabla_{W} L$.\n3. Implement, conceptually, a central-difference numerical gradient for $W$ by defining, for each index pair $(i,j)$, the perturbation matrix $E_{ij}$ with $(E_{ij})_{pq} = 1$ if $p=i,q=j$ and $0$ otherwise, and computing\n$$\n\\big(\\nabla_{W} L\\big)^{\\text{num}}_{ij} \\;=\\; \\frac{L\\big(W + \\epsilon E_{ij}\\big) - L\\big(W - \\epsilon E_{ij}\\big)}{2 \\epsilon},\n$$\nwith $\\epsilon = 10^{-3}$, holding $X$, $b$, and $T$ fixed.\n4. Using your results, compute the Frobenius norm of the difference between the analytical and numerical gradients with respect to $W$,\n$$\n\\Delta \\;=\\; \\left\\| \\nabla_{W} L \\;-\\; \\big(\\nabla_{W} L\\big)^{\\text{num}} \\right\\|_{F}.\n$$\n\nReport the single real-valued number $\\Delta$. No rounding is required. Express the answer as a plain number without units.",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. We proceed with the tasks as outlined.\n\nThe problem asks for a four-part solution involving the derivation and computation of gradients for a $1 \\times 1$ convolution layer. We will address each task sequentially.\n\n**Task 1: Symbolic Gradient Derivation**\n\nWe are given the mean-squared error loss function:\n$$\nL = \\frac{1}{2NHW} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} \\sum_{c=1}^{C_{\\text{out}}} \\left( Y[n,h,w,c] - T[n,h,w,c] \\right)^{2}\n$$\nand the $1 \\times 1$ convolution operation:\n$$\nY[n,h,w,c] = \\sum_{k=1}^{C_{\\text{in}}} W[c,k] X[n,h,w,k] + b[c]\n$$\n\nWe will use the chain rule of multivariate calculus. Let us denote the error term as $E[n,h,w,c] = Y[n,h,w,c] - T[n,h,w,c]$. The loss is $L = \\frac{1}{2NHW} \\sum_{n,h,w,c} E[n,h,w,c]^2$.\n\nThe derivative of the loss with respect to an arbitrary parameter $\\theta$ is:\n$$\n\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{2NHW} \\sum_{n,h,w,c} 2 E[n,h,w,c] \\frac{\\partial E[n,h,w,c]}{\\partial \\theta} = \\frac{1}{NHW} \\sum_{n,h,w,c} E[n,h,w,c] \\frac{\\partial Y[n,h,w,c]}{\\partial \\theta}\n$$\nsince $T$ is a constant tensor.\n\n1.  **Gradient with respect to weights, $\\nabla_W L$**:\n    We compute the partial derivative with respect to a single weight element $W[i,j]$, where $i \\in \\{1,\\dots,C_{\\text{out}}\\}$ and $j \\in \\{1,\\dots,C_{\\text{in}}\\}$.\n    First, we find the derivative of the output $Y$ with respect to $W[i,j]$:\n    $$\n    \\frac{\\partial Y[n,h,w,c]}{\\partial W[i,j]} = \\frac{\\partial}{\\partial W[i,j]} \\left( \\sum_{k=1}^{C_{\\text{in}}} W[c,k] X[n,h,w,k] + b[c] \\right)\n    $$\n    This derivative is non-zero only when $c=i$ and $k=j$. Using the Kronecker delta, this can be written as:\n    $$\n    \\frac{\\partial Y[n,h,w,c]}{\\partial W[i,j]} = \\delta_{ci} X[n,h,w,j]\n    $$\n    Substituting this into the general gradient formula:\n    $$\n    (\\nabla_W L)_{ij} = \\frac{\\partial L}{\\partial W[i,j]} = \\frac{1}{NHW} \\sum_{n,h,w,c} E[n,h,w,c] \\left( \\delta_{ci} X[n,h,w,j] \\right)\n    $$\n    The sum over $c$ collapses due to the delta function, leaving only the term where $c=i$:\n    $$\n    (\\nabla_W L)_{ij} = \\frac{1}{NHW} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} E[n,h,w,i] X[n,h,w,j]\n    $$\n    Replacing $E[n,h,w,i]$ with its definition, we get the final symbolic formula for the gradient with respect to $W$:\n    $$\n    (\\nabla_W L)_{ij} = \\frac{1}{NHW} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (Y[n,h,w,i] - T[n,h,w,i]) X[n,h,w,j]\n    $$\n\n2.  **Gradient with respect to bias, $\\nabla_b L$**:\n    We compute the partial derivative with respect to a bias element $b[i]$, where $i \\in \\{1,\\dots,C_{\\text{out}}\\}$.\n    $$\n    \\frac{\\partial Y[n,h,w,c]}{\\partial b[i]} = \\frac{\\partial}{\\partial b[i]} \\left( \\sum_{k=1}^{C_{\\text{in}}} W[c,k] X[n,h,w,k] + b[c] \\right) = \\delta_{ci}\n    $$\n    Substituting this into the general gradient formula:\n    $$\n    (\\nabla_b L)_i = \\frac{\\partial L}{\\partial b[i]} = \\frac{1}{NHW} \\sum_{n,h,w,c} E[n,h,w,c] \\delta_{ci} = \\frac{1}{NHW} \\sum_{n,h,w} E[n,h,w,i]\n    $$\n    $$\n    (\\nabla_b L)_i = \\frac{1}{NHW} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (Y[n,h,w,i] - T[n,h,w,i])\n    $$\n\n3.  **Gradient with respect to input, $\\nabla_X L$**:\n    We compute the partial derivative with respect to an input element $X[n',h',w',j]$, where $(n',h',w')$ is a specific location and $j \\in \\{1,\\dots,C_{\\text{in}}\\}$.\n    $$\n    \\frac{\\partial Y[n,h,w,c]}{\\partial X[n',h',w',j]} = \\frac{\\partial}{\\partial X[n',h',w',j]} \\left( \\sum_{k=1}^{C_{\\text{in}}} W[c,k] X[n,h,w,k] \\right)\n    $$\n    This is non-zero only if $n=n'$, $h=h'$, $w=w'$, and $k=j$.\n    $$\n    \\frac{\\partial Y[n,h,w,c]}{\\partial X[n',h',w',j]} = \\delta_{nn'} \\delta_{hh'} \\delta_{ww'} W[c,j]\n    $$\n    Substituting into the general gradient formula:\n    $$\n    (\\nabla_X L)_{n'h'w'j} = \\frac{\\partial L}{\\partial X[n',h',w',j]} = \\frac{1}{NHW} \\sum_{n,h,w,c} E[n,h,w,c] (\\delta_{nn'} \\delta_{hh'} \\delta_{ww'} W[c,j])\n    $$\n    The sums over $n,h,w$ collapse, leaving:\n    $$\n    (\\nabla_X L)_{n'h'w'j} = \\frac{1}{NHW} \\sum_{c=1}^{C_{\\text{out}}} E[n',h',w',c] W[c,j]\n    $$\n    $$\n    (\\nabla_X L)_{n'h'w'j} = \\frac{1}{NHW} \\sum_{c=1}^{C_{\\text{out}}} (Y[n',h',w',c] - T[n',h',w',c]) W[c,j]\n    $$\n\n**Task 2: Numerical Evaluation of the Analytical Gradient $\\nabla_W L$**\n\nThe dimensions are $N=1$, $H=1$, $W=2$, $C_{\\text{in}}=2$, $C_{\\text{out}}=2$. The normalization factor is $\\frac{1}{NHW} = \\frac{1}{2}$. We are given two spatial positions, which we index by $w=1$ and $w=2$.\n\nFirst, compute the output vectors $y_1, y_2$ at each position using $y_w = W x_w + b$:\n$$\nx_1 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad x_2 = \\begin{pmatrix} 2 \\\\ 0.5 \\end{pmatrix}, \\quad W = \\begin{pmatrix} 1 & 2 \\\\ -1 & 0.5 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix}\n$$\n$$\ny_1 = \\begin{pmatrix} 1 & 2 \\\\ -1 & 0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 2 \\cdot (-1) \\\\ -1 \\cdot 1 + 0.5 \\cdot (-1) \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -1.5 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ -2 \\end{pmatrix}\n$$\n$$\ny_2 = \\begin{pmatrix} 1 & 2 \\\\ -1 & 0.5 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0.5 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 2 + 2 \\cdot 0.5 \\\\ -1 \\cdot 2 + 0.5 \\cdot 0.5 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1.75 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} 3.5 \\\\ -2.25 \\end{pmatrix}\n$$\nNext, compute the error vectors $E_1 = y_1-t_1$ and $E_2 = y_2-t_2$:\n$$\nt_1 = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}, \\quad t_2 = \\begin{pmatrix} 3 \\\\ -2.5 \\end{pmatrix}\n$$\n$$\nE_1 = y_1 - t_1 = \\begin{pmatrix} -0.5 \\\\ -2 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ -1 \\end{pmatrix}\n$$\n$$\nE_2 = y_2 - t_2 = \\begin{pmatrix} 3.5 \\\\ -2.25 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ -2.5 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0.25 \\end{pmatrix}\n$$\nNow, apply the symbolic formula for $\\nabla_W L$. For a single batch and height, this simplifies to summing over the width dimension:\n$$\n\\nabla_W L = \\frac{1}{2} \\sum_{w=1}^{2} E_w x_w^T = \\frac{1}{2} (E_1 x_1^T + E_2 x_2^T)\n$$\nwhere $E_w x_w^T$ is an outer product.\n$$\nE_1 x_1^T = \\begin{pmatrix} -0.5 \\\\ -1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\end{pmatrix} = \\begin{pmatrix} -0.5 & 0.5 \\\\ -1 & 1 \\end{pmatrix}\n$$\n$$\nE_2 x_2^T = \\begin{pmatrix} 0.5 \\\\ 0.25 \\end{pmatrix} \\begin{pmatrix} 2 & 0.5 \\end{pmatrix} = \\begin{pmatrix} 1 & 0.25 \\\\ 0.5 & 0.125 \\end{pmatrix}\n$$\nSumming these and multiplying by the normalization factor:\n$$\n\\nabla_W L = \\frac{1}{2} \\left( \\begin{pmatrix} -0.5 & 0.5 \\\\ -1 & 1 \\end{pmatrix} + \\begin{pmatrix} 1 & 0.25 \\\\ 0.5 & 0.125 \\end{pmatrix} \\right) = \\frac{1}{2} \\begin{pmatrix} 0.5 & 0.75 \\\\ -0.5 & 1.125 \\end{pmatrix}\n$$\n$$\n\\nabla_W L = \\begin{pmatrix} 0.25 & 0.375 \\\\ -0.25 & 0.5625 \\end{pmatrix}\n$$\n\n**Task 3: Conceptual Implementation of the Numerical Gradient**\n\nThe numerical gradient for a component $W_{ij}$ is given by the central-difference formula:\n$$\n\\big(\\nabla_{W} L\\big)^{\\text{num}}_{ij} = \\frac{L(W + \\epsilon E_{ij}) - L(W - \\epsilon E_{ij})}{2 \\epsilon}\n$$\nwhere $\\epsilon = 10^{-3}$ and $E_{ij}$ is the standard basis matrix.\nThe loss $L$ is a quadratic function of the output $Y$. The output $Y$ is a linear function of the weights $W$. Therefore, $L$ is a quadratic polynomial in the components of $W$.\nFor any one-dimensional quadratic polynomial $P(w) = aw^2 + bw + c$, the central-difference formula for the derivative is exact:\n$$\n\\frac{P(w + \\epsilon) - P(w - \\epsilon)}{2\\epsilon} = \\frac{[a(w+\\epsilon)^2 + b(w+\\epsilon) + c] - [a(w-\\epsilon)^2 + b(w-\\epsilon) + c]}{2\\epsilon}\n$$\n$$\n= \\frac{a(w^2+2w\\epsilon+\\epsilon^2) - a(w^2-2w\\epsilon+\\epsilon^2) + b(w+\\epsilon) - b(w-\\epsilon)}{2\\epsilon} = \\frac{4aw\\epsilon + 2b\\epsilon}{2\\epsilon} = 2aw+b = P'(w)\n$$\nThis result holds for any multivariate quadratic function when computing partial derivatives. Since $L(W)$ is quadratic in the entries of $W$, the central-difference approximation is exact and does not depend on the specific value of $\\epsilon$ (for $\\epsilon \\neq 0$).\nTherefore, the numerical gradient must be identical to the analytical gradient.\n$$\n\\big(\\nabla_{W} L\\big)^{\\text{num}} = \\nabla_{W} L = \\begin{pmatrix} 0.25 & 0.375 \\\\ -0.25 & 0.5625 \\end{pmatrix}\n$$\n\n**Task 4: Frobenius Norm of the Difference**\n\nWe are asked to compute $\\Delta = \\left\\| \\nabla_{W} L - \\big(\\nabla_{W} L\\big)^{\\text{num}} \\right\\|_{F}$.\nBased on the analysis in Task 3, the difference between the analytical and numerical gradients is the zero matrix:\n$$\n\\nabla_{W} L - \\big(\\nabla_{W} L\\big)^{\\text{num}} = \\begin{pmatrix} 0.25 & 0.375 \\\\ -0.25 & 0.5625 \\end{pmatrix} - \\begin{pmatrix} 0.25 & 0.375 \\\\ -0.25 & 0.5625 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThe Frobenius norm of a matrix $A$ is $\\|A\\|_F = \\sqrt{\\sum_{i,j} |A_{ij}|^2}$. For the zero matrix, this is:\n$$\n\\Delta = \\left\\| \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} \\right\\|_{F} = \\sqrt{0^2 + 0^2 + 0^2 + 0^2} = 0\n$$",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Beyond the mechanics of a single layer, the power of $1 \\times 1$ convolutions shines in their architectural applications. This exercise challenges you to think like a model designer, balancing computational efficiency with predictive accuracy . You will formulate and solve a constrained optimization problem to determine the ideal \"bottleneck\" width, a core concept in modern efficient architectures like ResNet, revealing how $1 \\times 1$ layers are strategically used to manage model complexity.",
            "id": "3094430",
            "problem": "You are analyzing a bottleneck block inspired by the Network in Network concept in deep learning: a sequence of three convolutions consisting of a $1 \\times 1$ convolution that reduces channels from $C_{\\text{in}}$ to $C_{\\text{mid}}$, followed by a spatial $k \\times k$ convolution that maps $C_{\\text{mid}}$ to $C_{\\text{mid}}$, followed by a $1 \\times 1$ convolution that expands channels from $C_{\\text{mid}}$ to $C_{\\text{out}}$. The input feature map has spatial dimensions $H \\times W$ and $C_{\\text{in}}$ channels, and the output feature map has $C_{\\text{out}}$ channels. All quantities $H$, $W$, $C_{\\text{in}}$, $C_{\\text{out}}$, $k$ are positive integers.\n\nYou will select the bottleneck width $C_{\\text{mid}}$ to balance computational cost and accuracy. The computational cost is measured in Floating Point Operations (FLOPs), which refers to the number of multiply-add computations per forward pass through the block. The validation accuracy contribution of the block is modeled by a strictly increasing, concave function with diminishing returns, given by $A(C_{\\text{mid}}) = A_{0} + \\alpha \\ln(C_{\\text{mid}})$, where $A_{0}$ and $\\alpha$ are positive constants derived from empirical calibration on a development set. You are required to meet a target validation accuracy $A_{\\text{t}}$, with $A_{\\text{t}} > A_{0}$.\n\nStarting from the definitions of convolutional computation on discrete grids and the counting of multiplications for $1 \\times 1$ and $k \\times k$ convolutions over all spatial locations and channels, derive the total FLOPs of the three-layer bottleneck block as a function of $C_{\\text{mid}}$ and establish whether it is monotone in $C_{\\text{mid}}$ for positive $H$, $W$, $C_{\\text{in}}$, $C_{\\text{out}}$, $k$. Then, formulate and solve the constrained optimization problem that minimizes the total FLOPs subject to the accuracy constraint $A(C_{\\text{mid}}) \\geq A_{\\text{t}}$. Provide the optimal $C_{\\text{mid}}$ in closed form as a single analytic expression. If any approximation is needed, state and justify it. No rounding is required.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of deep learning, specifically concerning convolutional neural network architectures and computational cost analysis. It is well-posed as a constrained optimization problem with clearly defined objective and constraint functions. The language is objective and the setup is self-contained and consistent, with the implicit assumption of spatial dimension preservation via padding being a standard convention in this context.\n\nThe first step is to derive the total computational cost, measured in Floating Point Operations (FLOPs), for the three-layer bottleneck block. A single multiply-add operation is counted as one FLOP. The spatial dimensions of the feature map, $H \\times W$, are assumed to be preserved throughout the block, which is standard practice achieved using appropriate padding in the convolutional layers.\n\nThe computational cost of a single convolutional layer producing an output map of size $H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{out}}$ from an input map of size $H_{\\text{in}} \\times W_{\\text{in}} \\times C_{\\text{in}}$ with a kernel of size $k \\times k$ is given by the product of the number of output positions, the number of output channels, and the number of operations per output value. Each output value is the result of a dot product of size $k \\times k \\times C_{\\text{in}}$. Thus, the FLOPs are $H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{out}} \\times C_{\\text{in}} \\times k^2$.\n\nWe apply this formula to each layer in the bottleneck block:\n\n1.  **First layer:** A $1 \\times 1$ convolution that reduces channels from $C_{\\text{in}}$ to $C_{\\text{mid}}$.\n    *   Input channels: $C_{\\text{in}}$\n    *   Output channels: $C_{\\text{mid}}$\n    *   Kernel size: $1 \\times 1$ (i.e., $k=1$)\n    *   Output map size: $H \\times W$\n    *   FLOPs for layer 1, $F_1$: $H \\times W \\times C_{\\text{mid}} \\times C_{\\text{in}} \\times 1^2 = HW C_{\\text{in}} C_{\\text{mid}}$.\n\n2.  **Second layer:** A $k \\times k$ spatial convolution.\n    *   Input channels: $C_{\\text{mid}}$\n    *   Output channels: $C_{\\text{mid}}$\n    *   Kernel size: $k \\times k$\n    *   Output map size: $H \\times W$\n    *   FLOPs for layer 2, $F_2$: $H \\times W \\times C_{\\text{mid}} \\times C_{\\text{mid}} \\times k^2 = HW k^2 C_{\\text{mid}}^2$.\n\n3.  **Third layer:** A $1 \\times 1$ convolution that expands channels from $C_{\\text{mid}}$ to $C_{\\text{out}}$.\n    *   Input channels: $C_{\\text{mid}}$\n    *   Output channels: $C_{\\text{out}}$\n    *   Kernel size: $1 \\times 1$\n    *   Output map size: $H \\times W$\n    *   FLOPs for layer 3, $F_3$: $H \\times W \\times C_{\\text{out}} \\times C_{\\text{mid}} \\times 1^2 = HW C_{\\text{out}} C_{\\text{mid}}$.\n\nThe total FLOPs, $F(C_{\\text{mid}})$, is the sum of the FLOPs for the three layers:\n$$ F(C_{\\text{mid}}) = F_1 + F_2 + F_3 = HW C_{\\text{in}} C_{\\text{mid}} + HW k^2 C_{\\text{mid}}^2 + HW C_{\\text{out}} C_{\\text{mid}} $$\nFactoring out common terms, we get the total FLOPs as a function of $C_{\\text{mid}}$:\n$$ F(C_{\\text{mid}}) = HW \\left( k^2 C_{\\text{mid}}^2 + (C_{\\text{in}} + C_{\\text{out}})C_{\\text{mid}} \\right) $$\nThis is a quadratic function of $C_{\\text{mid}}$.\n\nTo establish whether this function is monotone in $C_{\\text{mid}}$, we examine its derivative. Treating $C_{\\text{mid}}$ as a continuous positive variable for analysis:\n$$ \\frac{dF}{dC_{\\text{mid}}} = HW \\left( 2k^2 C_{\\text{mid}} + (C_{\\text{in}} + C_{\\text{out}}) \\right) $$\nAccording to the problem statement, $H, W, k, C_{\\text{in}}, C_{\\text{out}}$ are all positive integers. The bottleneck width $C_{\\text{mid}}$ must also be a positive quantity. Therefore, every term in the derivative expression is positive: $H>0$, $W>0$, $2k^2>0$, $C_{\\text{mid}}>0$, and $(C_{\\text{in}}+C_{\\text{out}})>0$. Consequently, $\\frac{dF}{dC_{\\text{mid}}} > 0$ for all $C_{\\text{mid}} > 0$. This proves that the total FLOPs function $F(C_{\\text{mid}})$ is strictly increasing for positive $C_{\\text{mid}}$.\n\nNext, we formulate and solve the constrained optimization problem. We want to minimize the computational cost $F(C_{\\text{mid}})$ subject to the validation accuracy constraint.\nThe optimization problem is:\n$$ \\min_{C_{\\text{mid}} > 0} F(C_{\\text{mid}}) = HW \\left( k^2 C_{\\text{mid}}^2 + (C_{\\text{in}} + C_{\\text{out}})C_{\\text{mid}} \\right) $$\nSubject to:\n$$ A(C_{\\text{mid}}) \\geq A_{\\text{t}} $$\nwhere $A(C_{\\text{mid}}) = A_{0} + \\alpha \\ln(C_{\\text{mid}})$.\n\nWe first solve the constraint for $C_{\\text{mid}}$.\n$$ A_{0} + \\alpha \\ln(C_{\\text{mid}}) \\geq A_{\\text{t}} $$\nGiven that $\\alpha > 0$ and $A_{\\text{t}} > A_{0}$:\n$$ \\alpha \\ln(C_{\\text{mid}}) \\geq A_{\\text{t}} - A_{0} $$\n$$ \\ln(C_{\\text{mid}}) \\geq \\frac{A_{\\text{t}} - A_{0}}{\\alpha} $$\nSince the exponential function is strictly increasing, we can exponentiate both sides without changing the inequality direction:\n$$ C_{\\text{mid}} \\geq \\exp\\left(\\frac{A_{\\text{t}} - A_{0}}{\\alpha}\\right) $$\nThis inequality defines the feasible region for $C_{\\text{mid}}$.\n\nWe have already established that the objective function, $F(C_{\\text{mid}})$, is strictly increasing for $C_{\\text{mid}} > 0$. To minimize a strictly increasing function, one must choose the smallest possible value of its argument from the feasible set. The minimum value of $F(C_{\\text{mid}})$ will therefore occur at the minimum value of $C_{\\text{mid}}$ that satisfies the constraint.\n\nThe smallest value of $C_{\\text{mid}}$ satisfying $C_{\\text{mid}} \\geq \\exp\\left(\\frac{A_{\\text{t}} - A_{0}}{\\alpha}\\right)$ is precisely at the boundary of the feasible region. Although $C_{\\text{mid}}$ must be an integer in a practical implementation, the problem asks for a closed-form analytic expression, and no rounding is required. This implies that we should solve the problem in the continuous domain, which is a standard relaxation for such problems. The optimal value is thus:\n$$ C_{\\text{mid, opt}} = \\exp\\left(\\frac{A_{\\text{t}} - A_{0}}{\\alpha}\\right) $$\nThis expression gives the minimal bottleneck width required to meet the target accuracy $A_{\\text{t}}$, and since the cost function is monotonic, this choice of $C_{\\text{mid}}$ also minimizes the computational cost.",
            "answer": "$$\\boxed{\\exp\\left(\\frac{A_{\\text{t}} - A_{0}}{\\alpha}\\right)}$$"
        },
        {
            "introduction": "The true power of $1 \\times 1$ convolutions is unleashed when they are stacked to create more complex computations. This practice guides you through the implementation of the seminal \"Network in Network\" (NiN) concept, where a miniature multi-layer perceptron (MLP) is applied at every spatial location using only $1 \\times 1$ convolutions . By building and training this pixel-wise MLP, you will gain hands-on experience with how these layers introduce rich, non-linear interactions across channels, a technique that has become a cornerstone of many state-of-the-art architectures.",
            "id": "3094438",
            "problem": "You are asked to implement and analyze the pixel-wise multilayer perceptron convolution (MLPConv) idea from Network in Network (NiN) using only pointwise convolutions of spatial size $1\\times 1$ and to test classification accuracy on synthetic datasets. The implementation must be a complete, runnable program. Begin from the following fundamental bases:\n\n- Discrete convolution definition: for an input tensor $X\\in\\mathbb{R}^{H\\times W\\times C}$ and a convolutional kernel $K\\in\\mathbb{R}^{k_h\\times k_w\\times C\\times F}$, the output at spatial location $(i,j)$ in channel $f$ is\n$$\nY_{i,j,f}=\\sum_{u=1}^{k_h}\\sum_{v=1}^{k_w}\\sum_{c=1}^{C}K_{u,v,c,f}\\,X_{i+u',j+v',c}\n$$\nwhere $(u',v')$ indexes the appropriate spatial offsets. A kernel of spatial size $1\\times 1$ reduces this to a per-pixel, across-channel linear projection:\n$$\nY_{i,j,f}=\\sum_{c=1}^{C}W_{c,f}\\,X_{i,j,c}+b_f\n$$\nwith shared weights $W\\in\\mathbb{R}^{C\\times F}$ and biases $b\\in\\mathbb{R}^{F}$ across all $(i,j)$.\n\n- Rectified Linear Unit (ReLU) activation: for a scalar $z$, define $\\operatorname{ReLU}(z)=\\max(0,z)$ applied componentwise to vectors.\n\n- Softmax function: for logits $z\\in\\mathbb{R}^{K}$, define $\\operatorname{softmax}(z)_k=\\exp(z_k)\\big/\\sum_{t=1}^{K}\\exp(z_t)$.\n\n- Cross-entropy loss for multi-class classification: given a one-hot label vector $y\\in\\{0,1\\}^{K}$, the loss is\n$$\n\\ell(z,y)=-\\sum_{k=1}^{K}y_k\\log\\left(\\operatorname{softmax}(z)_k\\right).\n$$\n\nYour task is to:\n\n- Construct a three-layer pixel-wise MLP using only $1\\times 1$ convolutions. For each pixel $(i,j)$, the network computes\n$$\nz^{(1)}_{i,j} = W_1\\,x_{i,j} + b_1,\\quad a^{(1)}_{i,j}=\\operatorname{ReLU}\\!\\left(z^{(1)}_{i,j}\\right),\n$$\n$$\nz^{(2)}_{i,j} = W_2\\,a^{(1)}_{i,j} + b_2,\\quad a^{(2)}_{i,j}=\\operatorname{ReLU}\\!\\left(z^{(2)}_{i,j}\\right),\n$$\n$$\nz^{(3)}_{i,j} = W_3\\,a^{(2)}_{i,j} + b_3,\\quad p_{i,j}=\\operatorname{softmax}\\!\\left(z^{(3)}_{i,j}\\right),\n$$\nwhere $x_{i,j}\\in\\mathbb{R}^{C}$ is the input at pixel $(i,j)$ and $p_{i,j}\\in\\mathbb{R}^{K}$ are the predicted class probabilities. No spatial kernels of size $k>1$ are permitted; only $1\\times 1$ kernels are allowed.\n\n- Train this network from scratch using full-batch gradient descent starting from random initialization, with shared weights across all pixels (convolutional weight sharing). Use the cross-entropy loss averaged over all pixels:\n$$\n\\mathcal{L}=\\frac{1}{N}\\sum_{i=1}^{H}\\sum_{j=1}^{W}\\ell\\!\\left(z^{(3)}_{i,j},y_{i,j}\\right),\n$$\nwhere $N=H\\cdot W$ and $y_{i,j}\\in\\{0,\\dots,K-1\\}$ are ground-truth labels. No $k>1$ kernels may appear anywhere in the computation.\n\n- Evaluate the classification accuracy after training, defined as\n$$\n\\operatorname{acc}=\\frac{1}{N}\\sum_{i=1}^{H}\\sum_{j=1}^{W}\\mathbf{1}\\!\\left[\\arg\\max_{k}p_{i,j,k}=y_{i,j}\\right],\n$$\nwhere $\\mathbf{1}[\\cdot]$ denotes the indicator function.\n\nImplement a single program that trains and evaluates on the following synthetic test suite. All randomness must be controlled by the specified seed values for reproducibility. No external data or user input is allowed. There are no physical units involved.\n\nTest suite (each case specifies $H$, $W$, $C$, $K$, hidden widths $F_1$, $F_2$, number of optimization steps, learning rate $\\eta$, and seed; and a deterministic data generation rule):\n\n- Case $1$ (general case):\n  - $H=4$, $W=4$, $C=3$, $K=2$, $F_1=5$, $F_2=3$, $\\text{steps}=800$, $\\eta=0.2$, $\\text{seed}=123$.\n  - Data generation for features: for pixel coordinates $(i,j)$ with $i\\in\\{0,\\dots,H-1\\}$ and $j\\in\\{0,\\dots,W-1\\}$,\n    $x_{i,j,0}=\\frac{i+1}{H}$, $x_{i,j,1}=\\frac{j+1}{W}$, $x_{i,j,2}=\\frac{(i+1)+(j+1)}{H+W}$.\n    Labels: define a score $s=0.7\\,x_{i,j,0}-0.5\\,x_{i,j,1}+0.4\\,x_{i,j,2}-0.3$; set $y_{i,j}=1$ if $s>0$ and $y_{i,j}=0$ otherwise.\n\n- Case $2$ (multi-class case):\n  - $H=3$, $W=3$, $C=2$, $K=3$, $F_1=4$, $F_2=3$, $\\text{steps}=1500$, $\\eta=0.2$, $\\text{seed}=456$.\n  - Data generation for features: $x_{i,j,0}=\\frac{i+1}{H}$, $x_{i,j,1}=\\frac{j+1}{W}-0.5$.\n    Ground-truth linear classifier: $W^{\\star}\\in\\mathbb{R}^{3\\times 2}$ with rows $\\left[1.0,-1.2\\right]$, $\\left[-0.6,0.8\\right]$, $\\left[0.2,0.1\\right]$ and bias $b^{\\star}=\\left[0.0,0.1,-0.2\\right]$. Labels: $y_{i,j}=\\arg\\max_{k}\\left((W^{\\star}x_{i,j}+b^{\\star})_k\\right)$.\n\n- Case $3$ (single-pixel boundary):\n  - $H=1$, $W=1$, $C=4$, $K=2$, $F_1=3$, $F_2=2$, $\\text{steps}=500$, $\\eta=0.2$, $\\text{seed}=789$.\n  - Data generation: $x_{0,0}=\\left[0.2,-0.4,0.6,-0.8\\right]$. Label via threshold: let $v=\\left[1.0,1.0,-0.5,-0.5\\right]$ and $b=-0.1$, set $y_{0,0}=1$ if $v^{\\top}x_{0,0}+b>0$, otherwise $y_{0,0}=0$.\n\n- Case $4$ (degenerate single-class edge case):\n  - $H=2$, $W=2$, $C=1$, $K=1$, $F_1=2$, $F_2=2$, $\\text{steps}=0$, $\\eta=0.1$, $\\text{seed}=42$.\n  - Data generation: all features zero, $x_{i,j,0}=0$, and all labels $y_{i,j}=0$.\n\nYour program must implement training by full-batch gradient descent on $\\mathcal{L}$ with the above architecture and constraints, evaluate the classification accuracy for each case, and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each accuracy expressed as a float rounded to four decimal places (for example, $\\left[\\text{acc}_1,\\text{acc}_2,\\text{acc}_3,\\text{acc}_4\\right]$ printed as $\\left[0.9375,1.0000,1.0000,1.0000\\right]$).",
            "solution": "The user's problem statement is determined to be valid. It is scientifically grounded, well-posed, objective, and self-contained. The problem requires the implementation of a specific neural network architecture, known as a pixel-wise multilayer perceptron (MLP) or a Network-in-Network (NiN) block, using only $1\\times 1$ convolutions. The training is to be performed with full-batch gradient descent. All required mathematical definitions, architectural specifications, training parameters, and reproducible data generation rules are provided. The problem is a standard, albeit detailed, implementation task in computational machine learning.\n\nThe solution proceeds by first implementing the specified network architecture and the associated training algorithm. The core idea is that a $1\\times 1$ convolution, which operates on each pixel's feature vector independently but shares weights across all spatial locations, is equivalent to applying a standard fully-connected layer to each pixel's feature vector.\n\nFollowing the common machine learning library convention, we treat the input tensor $X \\in \\mathbb{R}^{H \\times W \\times C}$ by reshaping it into a matrix of \"flattened\" pixel vectors $X_{\\text{flat}} \\in \\mathbb{R}^{N \\times C}$, where $N=H \\cdot W$ is the total number of pixels. Each row of this matrix corresponds to a single pixel's feature vector $x_{i,j} \\in \\mathbb{R}^{C}$.\n\nThe three-layer pixel-wise MLP is defined by the following forward propagation equations:\nThe first layer computes the pre-activations $Z^{(1)} \\in \\mathbb{R}^{N \\times F_1}$ and activations $A^{(1)} \\in \\mathbb{R}^{N \\times F_1}$:\n$$ Z^{(1)} = X_{\\text{flat}} W_1 + b_1 $$\n$$ A^{(1)} = \\operatorname{ReLU}(Z^{(1)}) $$\nHere, $W_1 \\in \\mathbb{R}^{C \\times F_1}$ and $b_1 \\in \\mathbb{R}^{F_1}$ are the weights and biases of the first layer, and the $\\operatorname{ReLU}$ function is applied element-wise.\n\nThe second layer similarly computes pre-activations $Z^{(2)} \\in \\mathbb{R}^{N \\times F_2}$ and activations $A^{(2)} \\in \\mathbb{R}^{N \\times F_2}$:\n$$ Z^{(2)} = A^{(1)} W_2 + b_2 $$\n$$ A^{(2)} = \\operatorname{ReLU}(Z^{(2)}) $$\nwith weights $W_2 \\in \\mathbb{R}^{F_1 \\times F_2}$ and biases $b_2 \\in \\mathbb{R}^{F_2}$.\n\nThe final output layer produces the logits $Z^{(3)} \\in \\mathbb{R}^{N \\times K}$ for the $K$ classes:\n$$ Z^{(3)} = A^{(2)} W_3 + b_3 $$\nwith weights $W_3 \\in \\mathbb{R}^{F_2 \\times K}$ and biases $b_3 \\in \\mathbb{R}^{K}$.\n\nThe class probabilities $P \\in \\mathbb{R}^{N \\times K}$ are obtained by applying the $\\operatorname{softmax}$ function to the logits for each pixel:\n$$ P_{i,k} = \\frac{\\exp((Z^{(3)})_{i,k})}{\\sum_{t=1}^{K}\\exp((Z^{(3)})_{i,t})} $$\nFor numerical stability, this is implemented as $\\operatorname{softmax}(z) = \\operatorname{softmax}(z - \\max(z))$.\n\nThe model is trained by minimizing the average cross-entropy loss $\\mathcal{L}$ over all $N$ pixels:\n$$ \\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{k=1}^{K}(Y_{\\text{oh}})_{i,k} \\log(P_{i,k}) $$\nwhere $Y_{\\text{oh}} \\in \\{0,1\\}^{N \\times K}$ is the one-hot encoded matrix of ground-truth labels.\n\nTraining is performed using full-batch gradient descent, which requires computing the gradients of the loss $\\mathcal{L}$ with respect to all model parameters ($W_1, b_1, W_2, b_2, W_3, b_3$). This is achieved via the backpropagation algorithm, which applies the chain rule recursively.\n\nThe gradient of the loss with respect to the output logits $Z^{(3)}$, denoted as the error signal $\\delta^{(3)}$, is the starting point:\n$$ \\delta^{(3)} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{(3)}} = \\frac{1}{N}(P - Y_{\\text{oh}}) $$\nThis error is then propagated backward through the network.\n\nThe gradients for the third layer's parameters are:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial W_3} = (A^{(2)})^T \\delta^{(3)} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b_3} = \\sum_{i=1}^{N} (\\delta^{(3)})_i $$\n\nThe error is propagated to the second layer's activations, $\\frac{\\partial \\mathcal{L}}{\\partial A^{(2)}} = \\delta^{(3)} W_3^T$, and then through the ReLU non-linearity, whose derivative is $\\operatorname{ReLU}'(z) = \\mathbf{1}[z>0]$. The error signal for the second layer's pre-activations is:\n$$ \\delta^{(2)} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{(2)}} = (\\delta^{(3)} W_3^T) \\odot \\operatorname{ReLU}'(Z^{(2)}) $$\nwhere $\\odot$ denotes element-wise multiplication.\n\nThe gradients for the second layer's parameters are:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial W_2} = (A^{(1)})^T \\delta^{(2)} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b_2} = \\sum_{i=1}^{N} (\\delta^{(2)})_i $$\n\nSimilarly, the error signal for the first layer's pre-activations is:\n$$ \\delta^{(1)} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{(1)}} = (\\delta^{(2)} W_2^T) \\odot \\operatorname{ReLU}'(Z^{(1)}) $$\n\nAnd the gradients for the first layer's parameters are:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial W_1} = (X_{\\text{flat}})^T \\delta^{(1)} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b_1} = \\sum_{i=1}^{N} (\\delta^{(1)})_i $$\n\nFinally, the parameters are updated in each step of the gradient descent algorithm using the learning rate $\\eta$:\n$$ W \\leftarrow W - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W}, \\quad b \\leftarrow b - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b} $$\nfor each layer. After the specified number of training steps, the model's accuracy is evaluated on the same data by comparing the predicted class (the index of the maximal logit) with the true class for each pixel.\n\nThis entire procedure is implemented in the following program and applied to each of the four test cases defined in the problem statement.",
            "answer": "```python\nimport numpy as np\n\ndef train_and_evaluate(X, Y, K, F1, F2, steps, eta, seed):\n    \"\"\"\n    Trains and evaluates the pixel-wise MLP model.\n\n    This function encapsulates the entire process:\n    1. Data preparation.\n    2. Weight initialization.\n    3. The training loop with forward and backward passes.\n    4. Final accuracy evaluation.\n    \"\"\"\n    H, W, C = X.shape\n    N = H * W\n    \n    rng = np.random.default_rng(seed)\n\n    # Reshape data for matrix operations\n    X_flat = X.reshape(N, C)\n    Y_flat = Y.flatten()\n    \n    if K > 1:\n        Y_oh = np.zeros((N, K))\n        Y_oh[np.arange(N), Y_flat] = 1\n    else: # K=1\n        Y_oh = np.ones((N, 1))\n\n    # Initialize weights and biases\n    W1 = rng.standard_normal((C, F1), dtype=np.float64) * 0.1\n    b1 = np.zeros(F1, dtype=np.float64)\n    W2 = rng.standard_normal((F1, F2), dtype=np.float64) * 0.1\n    b2 = np.zeros(F2, dtype=np.float64)\n    W3 = rng.standard_normal((F2, K), dtype=np.float64) * 0.1\n    b3 = np.zeros(K, dtype=np.float64)\n\n    # ReLU activation and its derivative\n    def relu(z):\n        return np.maximum(0, z)\n\n    def relu_derivative(z):\n        return (z > 0).astype(z.dtype)\n\n    # Softmax function (numerically stable)\n    def softmax(z):\n        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    # Training loop\n    for _ in range(steps):\n        # --- Forward Pass ---\n        Z1 = X_flat @ W1 + b1\n        A1 = relu(Z1)\n        Z2 = A1 @ W2 + b2\n        A2 = relu(Z2)\n        Z3 = A2 @ W3 + b3\n\n        if K > 1:\n            P = softmax(Z3)\n        else: # K=1, softmax is just 1\n            P = np.ones_like(Z3)\n        \n        # --- Backward Pass ---\n        # Gradient of loss w.r.t. Z3\n        delta3 = (P - Y_oh) / N\n\n        # Gradients for layer 3\n        grad_W3 = A2.T @ delta3\n        grad_b3 = np.sum(delta3, axis=0)\n\n        # Propagate error to A2\n        delta_A2 = delta3 @ W3.T\n        # Propagate through ReLU\n        delta2 = delta_A2 * relu_derivative(Z2)\n\n        # Gradients for layer 2\n        grad_W2 = A1.T @ delta2\n        grad_b2 = np.sum(delta2, axis=0)\n\n        # Propagate error to A1\n        delta_A1 = delta2 @ W2.T\n        # Propagate through ReLU\n        delta1 = delta_A1 * relu_derivative(Z1)\n\n        # Gradients for layer 1\n        grad_W1 = X_flat.T @ delta1\n        grad_b1 = np.sum(delta1, axis=0)\n\n        # --- Gradient Descent Update ---\n        W1 -= eta * grad_W1\n        b1 -= eta * grad_b1\n        W2 -= eta * grad_W2\n        b2 -= eta * grad_b2\n        W3 -= eta * grad_W3\n        b3 -= eta * grad_b3\n\n    # --- Final Evaluation ---\n    # Forward pass with trained weights\n    Z1 = X_flat @ W1 + b1\n    A1 = relu(Z1)\n    Z2 = A1 @ W2 + b2\n    A2 = relu(Z2)\n    Z3 = A2 @ W3 + b3\n    \n    # Predictions are the argmax of the logits\n    predictions = np.argmax(Z3, axis=1)\n    \n    # Calculate accuracy\n    accuracy = np.mean(predictions == Y_flat)\n    \n    return accuracy\n\n\ndef solve():\n    test_cases = [\n        {'id': 1, 'H': 4, 'W': 4, 'C': 3, 'K': 2, 'F1': 5, 'F2': 3, 'steps': 800, 'eta': 0.2, 'seed': 123},\n        {'id': 2, 'H': 3, 'W': 3, 'C': 2, 'K': 3, 'F1': 4, 'F2': 3, 'steps': 1500, 'eta': 0.2, 'seed': 456},\n        {'id': 3, 'H': 1, 'W': 1, 'C': 4, 'K': 2, 'F1': 3, 'F2': 2, 'steps': 500, 'eta': 0.2, 'seed': 789},\n        {'id': 4, 'H': 2, 'W': 2, 'C': 1, 'K': 1, 'F1': 2, 'F2': 2, 'steps': 0, 'eta': 0.1, 'seed': 42},\n    ]\n\n    results = []\n    for params in test_cases:\n        H, W, C = params['H'], params['W'], params['C']\n        X = np.zeros((H, W, C), dtype=np.float64)\n        Y = np.zeros((H, W), dtype=int)\n\n        if params['id'] == 1:\n            for i in range(H):\n                for j in range(W):\n                    x0 = (i + 1) / H\n                    x1 = (j + 1) / W\n                    x2 = ((i + 1) + (j + 1)) / (H + W)\n                    X[i, j, :] = [x0, x1, x2]\n                    s = 0.7 * x0 - 0.5 * x1 + 0.4 * x2 - 0.3\n                    Y[i, j] = 1 if s > 0 else 0\n        \n        elif params['id'] == 2:\n            W_star = np.array([[1.0, -1.2], [-0.6, 0.8], [0.2, 0.1]], dtype=np.float64)\n            b_star = np.array([0.0, 0.1, -0.2], dtype=np.float64)\n            for i in range(H):\n                for j in range(W):\n                    x0 = (i + 1) / H\n                    x1 = (j + 1) / W - 0.5\n                    x_ij = np.array([x0, x1], dtype=np.float64)\n                    X[i, j, :] = x_ij\n                    logits = W_star @ x_ij + b_star\n                    Y[i, j] = np.argmax(logits)\n\n        elif params['id'] == 3:\n            x_00 = np.array([0.2, -0.4, 0.6, -0.8], dtype=np.float64)\n            v = np.array([1.0, 1.0, -0.5, -0.5], dtype=np.float64)\n            b = -0.1\n            X[0, 0, :] = x_00\n            Y[0, 0] = 1 if v @ x_00 + b > 0 else 0\n            \n        elif params['id'] == 4:\n            # X and Y are already initialized to zeros.\n            pass\n\n        accuracy = train_and_evaluate(X, Y, params['K'], params['F1'], params['F2'], \n                                      params['steps'], params['eta'], params['seed'])\n        \n        results.append(f\"{accuracy:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}