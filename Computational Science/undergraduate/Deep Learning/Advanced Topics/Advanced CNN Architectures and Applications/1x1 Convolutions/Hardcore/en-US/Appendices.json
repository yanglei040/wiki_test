{
    "hands_on_practices": [
        {
            "introduction": "This first practice will ground your understanding of 1x1 convolutions in the fundamental mechanics of neural network training. By deriving the gradients from first principles and verifying your results with numerical methods, you will see precisely how a 1x1 convolutional layer learns by adjusting its weights during backpropagation . This exercise is crucial for demystifying the training process and appreciating the 1x1 convolution as a per-pixel linear transformation that is fully integrated into the gradient-based learning framework.",
            "id": "3094339",
            "problem": "A $1 \\times 1$ convolution applies the same per-location linear map across channels. Consider an input tensor $X \\in \\mathbb{R}^{N \\times H \\times W \\times C_{\\text{in}}}$, weights $W \\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}}}$, and bias $b \\in \\mathbb{R}^{C_{\\text{out}}}$. The $1 \\times 1$ convolution output $Y \\in \\mathbb{R}^{N \\times H \\times W \\times C_{\\text{out}}}$ is defined componentwise by\n$$\nY[n,h,w,c] \\;=\\; \\sum_{k=1}^{C_{\\text{in}}} W[c,k]\\,X[n,h,w,k] \\;+\\; b[c],\n$$\nfor all $n \\in \\{1,\\dots,N\\}$, $h \\in \\{1,\\dots,H\\}$, $w \\in \\{1,\\dots,W\\}$, and $c \\in \\{1,\\dots,C_{\\text{out}}\\}$. Given a target tensor $T \\in \\mathbb{R}^{N \\times H \\times W \\times C_{\\text{out}}}$, define the mean-squared error loss\n$$\nL \\;=\\; \\frac{1}{2 N H W} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} \\sum_{c=1}^{C_{\\text{out}}} \\big(Y[n,h,w,c] - T[n,h,w,c]\\big)^{2}.\n$$\nWork from first principles of multivariate calculus (the chain rule and linearity) and the above definitions of the $1 \\times 1$ convolution and loss. Do not assume any pre-derived gradient formulas.\n\nConsider the concrete setting with $N=1$, $H=1$, $W=2$, $C_{\\text{in}}=2$, $C_{\\text{out}}=2$. A simple pseudorandom generator produced the following realized tensors (treat these as fixed values for this exercise):\n- Input $X$ has two spatial positions $(h,w)=(1,1)$ and $(h,w)=(1,2)$ with channel vectors\n$$\nx_{1} \\;=\\; \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad\nx_{2} \\;=\\; \\begin{pmatrix} 2 \\\\ 0.5 \\end{pmatrix}.\n$$\n- Weights and bias are\n$$\nW \\;=\\; \\begin{pmatrix} 1  2 \\\\ -1  0.5 \\end{pmatrix}, \\qquad\nb \\;=\\; \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix}.\n$$\n- Targets at the two spatial positions are\n$$\nt_{1} \\;=\\; \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}, \\quad\nt_{2} \\;=\\; \\begin{pmatrix} 3 \\\\ -2.5 \\end{pmatrix}.\n$$\n\nTasks:\n1. Starting from the definitions above, derive symbolic formulas for the gradients $\\nabla_{W} L$, $\\nabla_{b} L$, and $\\nabla_{X} L$ for general $X$, $W$, $b$, and $T$.\n2. Evaluate your analytical expressions for the specific numerical tensors given to obtain a concrete numerical matrix for $\\nabla_{W} L$.\n3. Implement, conceptually, a central-difference numerical gradient for $W$ by defining, for each index pair $(i,j)$, the perturbation matrix $E_{ij}$ with $(E_{ij})_{pq} = 1$ if $p=i,q=j$ and $0$ otherwise, and computing\n$$\n\\big(\\nabla_{W} L\\big)^{\\text{num}}_{ij} \\;=\\; \\frac{L\\big(W + \\epsilon E_{ij}\\big) - L\\big(W - \\epsilon E_{ij}\\big)}{2 \\epsilon},\n$$\nwith $\\epsilon = 10^{-3}$, holding $X$, $b$, and $T$ fixed.\n4. Using your results, compute the Frobenius norm of the difference between the analytical and numerical gradients with respect to $W$,\n$$\n\\Delta \\;=\\; \\left\\| \\nabla_{W} L \\;-\\; \\big(\\nabla_{W} L\\big)^{\\text{num}} \\right\\|_{F}.\n$$\n\nReport the single real-valued number $\\Delta$. No rounding is required. Express the answer as a plain number without units.",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. We proceed with the tasks as outlined.\n\nThe problem asks for a four-part solution involving the derivation and computation of gradients for a $1 \\times 1$ convolution layer. We will address each task sequentially.\n\n**Task 1: Symbolic Gradient Derivation**\n\nWe are given the mean-squared error loss function:\n$$\nL = \\frac{1}{2NHW} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} \\sum_{c=1}^{C_{\\text{out}}} \\left( Y[n,h,w,c] - T[n,h,w,c] \\right)^{2}\n$$\nand the $1 \\times 1$ convolution operation:\n$$\nY[n,h,w,c] = \\sum_{k=1}^{C_{\\text{in}}} W[c,k] X[n,h,w,k] + b[c]\n$$\n\nWe will use the chain rule of multivariate calculus. Let us denote the error term as $E[n,h,w,c] = Y[n,h,w,c] - T[n,h,w,c]$. The loss is $L = \\frac{1}{2NHW} \\sum_{n,h,w,c} E[n,h,w,c]^2$.\n\nThe derivative of the loss with respect to an arbitrary parameter $\\theta$ is:\n$$\n\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{2NHW} \\sum_{n,h,w,c} 2 E[n,h,w,c] \\frac{\\partial E[n,h,w,c]}{\\partial \\theta} = \\frac{1}{NHW} \\sum_{n,h,w,c} E[n,h,w,c] \\frac{\\partial Y[n,h,w,c]}{\\partial \\theta}\n$$\nsince $T$ is a constant tensor.\n\n1.  **Gradient with respect to weights, $\\nabla_W L$**:\n    We compute the partial derivative with respect to a single weight element $W[i,j]$, where $i \\in \\{1,\\dots,C_{\\text{out}}\\}$ and $j \\in \\{1,\\dots,C_{\\text{in}}\\}$.\n    First, we find the derivative of the output $Y$ with respect to $W[i,j]$:\n    $$\n    \\frac{\\partial Y[n,h,w,c]}{\\partial W[i,j]} = \\frac{\\partial}{\\partial W[i,j]} \\left( \\sum_{k=1}^{C_{\\text{in}}} W[c,k] X[n,h,w,k] + b[c] \\right)\n    $$\n    This derivative is non-zero only when $c=i$ and $k=j$. Using the Kronecker delta, this can be written as:\n    $$\n    \\frac{\\partial Y[n,h,w,c]}{\\partial W[i,j]} = \\delta_{ci} X[n,h,w,j]\n    $$\n    Substituting this into the general gradient formula:\n    $$\n    (\\nabla_W L)_{ij} = \\frac{\\partial L}{\\partial W[i,j]} = \\frac{1}{NHW} \\sum_{n,h,w,c} E[n,h,w,c] \\left( \\delta_{ci} X[n,h,w,j] \\right)\n    $$\n    The sum over $c$ collapses due to the delta function, leaving only the term where $c=i$:\n    $$\n    (\\nabla_W L)_{ij} = \\frac{1}{NHW} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} E[n,h,w,i] X[n,h,w,j]\n    $$\n    Replacing $E[n,h,w,i]$ with its definition, we get the final symbolic formula for the gradient with respect to $W$:\n    $$\n    (\\nabla_W L)_{ij} = \\frac{1}{NHW} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (Y[n,h,w,i] - T[n,h,w,i]) X[n,h,w,j]\n    $$\n\n2.  **Gradient with respect to bias, $\\nabla_b L$**:\n    We compute the partial derivative with respect to a bias element $b[i]$, where $i \\in \\{1,\\dots,C_{\\text{out}}\\}$.\n    $$\n    \\frac{\\partial Y[n,h,w,c]}{\\partial b[i]} = \\frac{\\partial}{\\partial b[i]} \\left( \\sum_{k=1}^{C_{\\text{in}}} W[c,k] X[n,h,w,k] + b[c] \\right) = \\delta_{ci}\n    $$\n    Substituting this into the general gradient formula:\n    $$\n    (\\nabla_b L)_i = \\frac{\\partial L}{\\partial b[i]} = \\frac{1}{NHW} \\sum_{n,h,w,c} E[n,h,w,c] \\delta_{ci} = \\frac{1}{NHW} \\sum_{n,h,w} E[n,h,w,i]\n    $$\n    $$\n    (\\nabla_b L)_i = \\frac{1}{NHW} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (Y[n,h,w,i] - T[n,h,w,i])\n    $$\n\n3.  **Gradient with respect to input, $\\nabla_X L$**:\n    We compute the partial derivative with respect to an input element $X[n',h',w',j]$, where $(n',h',w')$ is a specific location and $j \\in \\{1,\\dots,C_{\\text{in}}\\}$.\n    $$\n    \\frac{\\partial Y[n,h,w,c]}{\\partial X[n',h',w',j]} = \\frac{\\partial}{\\partial X[n',h',w',j]} \\left( \\sum_{k=1}^{C_{\\text{in}}} W[c,k] X[n,h,w,k] \\right)\n    $$\n    This is non-zero only if $n=n'$, $h=h'$, $w=w'$, and $k=j$.\n    $$\n    \\frac{\\partial Y[n,h,w,c]}{\\partial X[n',h',w',j]} = \\delta_{nn'} \\delta_{hh'} \\delta_{ww'} W[c,j]\n    $$\n    Substituting into the general gradient formula:\n    $$\n    (\\nabla_X L)_{n'h'w'j} = \\frac{\\partial L}{\\partial X[n',h',w',j]} = \\frac{1}{NHW} \\sum_{n,h,w,c} E[n,h,w,c] (\\delta_{nn'} \\delta_{hh'} \\delta_{ww'} W[c,j])\n    $$\n    The sums over $n,h,w$ collapse, leaving:\n    $$\n    (\\nabla_X L)_{n'h'w'j} = \\frac{1}{NHW} \\sum_{c=1}^{C_{\\text{out}}} E[n',h',w',c] W[c,j]\n    $$\n    $$\n    (\\nabla_X L)_{n'h'w'j} = \\frac{1}{NHW} \\sum_{c=1}^{C_{\\text{out}}} (Y[n',h',w',c] - T[n',h',w',c]) W[c,j]\n    $$\n\n**Task 2: Numerical Evaluation of the Analytical Gradient $\\nabla_W L$**\n\nThe dimensions are $N=1$, $H=1$, $W=2$, $C_{\\text{in}}=2$, $C_{\\text{out}}=2$. The normalization factor is $\\frac{1}{NHW} = \\frac{1}{2}$. We are given two spatial positions, which we index by $w=1$ and $w=2$.\n\nFirst, compute the output vectors $y_1, y_2$ at each position using $y_w = W x_w + b$:\n$$\nx_1 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad x_2 = \\begin{pmatrix} 2 \\\\ 0.5 \\end{pmatrix}, \\quad W = \\begin{pmatrix} 1  2 \\\\ -1  0.5 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix}\n$$\n$$\ny_1 = \\begin{pmatrix} 1  2 \\\\ -1  0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 2 \\cdot (-1) \\\\ -1 \\cdot 1 + 0.5 \\cdot (-1) \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -1.5 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ -2 \\end{pmatrix}\n$$\n$$\ny_2 = \\begin{pmatrix} 1  2 \\\\ -1  0.5 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0.5 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 2 + 2 \\cdot 0.5 \\\\ -1 \\cdot 2 + 0.5 \\cdot 0.5 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1.75 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} 3.5 \\\\ -2.25 \\end{pmatrix}\n$$\nNext, compute the error vectors $E_1 = y_1-t_1$ and $E_2 = y_2-t_2$:\n$$\nt_1 = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}, \\quad t_2 = \\begin{pmatrix} 3 \\\\ -2.5 \\end{pmatrix}\n$$\n$$\nE_1 = y_1 - t_1 = \\begin{pmatrix} -0.5 \\\\ -2 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ -1 \\end{pmatrix}\n$$\n$$\nE_2 = y_2 - t_2 = \\begin{pmatrix} 3.5 \\\\ -2.25 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ -2.5 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0.25 \\end{pmatrix}\n$$\nNow, apply the symbolic formula for $\\nabla_W L$. For a single batch and height, this simplifies to summing over the width dimension:\n$$\n\\nabla_W L = \\frac{1}{2} \\sum_{w=1}^{2} E_w x_w^T = \\frac{1}{2} (E_1 x_1^T + E_2 x_2^T)\n$$\nwhere $E_w x_w^T$ is an outer product.\n$$\nE_1 x_1^T = \\begin{pmatrix} -0.5 \\\\ -1 \\end{pmatrix} \\begin{pmatrix} 1  -1 \\end{pmatrix} = \\begin{pmatrix} -0.5  0.5 \\\\ -1  1 \\end{pmatrix}\n$$\n$$\nE_2 x_2^T = \\begin{pmatrix} 0.5 \\\\ 0.25 \\end{pmatrix} \\begin{pmatrix} 2  0.5 \\end{pmatrix} = \\begin{pmatrix} 1  0.25 \\\\ 0.5  0.125 \\end{pmatrix}\n$$\nSumming these and multiplying by the normalization factor:\n$$\n\\nabla_W L = \\frac{1}{2} \\left( \\begin{pmatrix} -0.5  0.5 \\\\ -1  1 \\end{pmatrix} + \\begin{pmatrix} 1  0.25 \\\\ 0.5  0.125 \\end{pmatrix} \\right) = \\frac{1}{2} \\begin{pmatrix} 0.5  0.75 \\\\ -0.5  1.125 \\end{pmatrix}\n$$\n$$\n\\nabla_W L = \\begin{pmatrix} 0.25  0.375 \\\\ -0.25  0.5625 \\end{pmatrix}\n$$\n\n**Task 3: Conceptual Implementation of the Numerical Gradient**\n\nThe numerical gradient for a component $W_{ij}$ is given by the central-difference formula:\n$$\n\\big(\\nabla_{W} L\\big)^{\\text{num}}_{ij} = \\frac{L(W + \\epsilon E_{ij}) - L(W - \\epsilon E_{ij})}{2 \\epsilon}\n$$\nwhere $\\epsilon = 10^{-3}$ and $E_{ij}$ is the standard basis matrix.\nThe loss $L$ is a quadratic function of the output $Y$. The output $Y$ is a linear function of the weights $W$. Therefore, $L$ is a quadratic polynomial in the components of $W$.\nFor any one-dimensional quadratic polynomial $P(w) = aw^2 + bw + c$, the central-difference formula for the derivative is exact:\n$$\n\\frac{P(w + \\epsilon) - P(w - \\epsilon)}{2\\epsilon} = \\frac{[a(w+\\epsilon)^2 + b(w+\\epsilon) + c] - [a(w-\\epsilon)^2 + b(w-\\epsilon) + c]}{2\\epsilon}\n$$\n$$\n= \\frac{a(w^2+2w\\epsilon+\\epsilon^2) - a(w^2-2w\\epsilon+\\epsilon^2) + b(w+\\epsilon) - b(w-\\epsilon)}{2\\epsilon} = \\frac{4aw\\epsilon + 2b\\epsilon}{2\\epsilon} = 2aw+b = P'(w)\n$$\nThis result holds for any multivariate quadratic function when computing partial derivatives. Since $L(W)$ is quadratic in the entries of $W$, the central-difference approximation is exact and does not depend on the specific value of $\\epsilon$ (for $\\epsilon \\neq 0$).\nTherefore, the numerical gradient must be identical to the analytical gradient.\n$$\n\\big(\\nabla_{W} L\\big)^{\\text{num}} = \\nabla_{W} L = \\begin{pmatrix} 0.25  0.375 \\\\ -0.25  0.5625 \\end{pmatrix}\n$$\n\n**Task 4: Frobenius Norm of the Difference**\n\nWe are asked to compute $\\Delta = \\left\\| \\nabla_{W} L - \\big(\\nabla_{W} L\\big)^{\\text{num}} \\right\\|_{F}$.\nBased on the analysis in Task 3, the difference between the analytical and numerical gradients is the zero matrix:\n$$\n\\nabla_{W} L - \\big(\\nabla_{W} L\\big)^{\\text{num}} = \\begin{pmatrix} 0.25  0.375 \\\\ -0.25  0.5625 \\end{pmatrix} - \\begin{pmatrix} 0.25  0.375 \\\\ -0.25  0.5625 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}\n$$\nThe Frobenius norm of a matrix $A$ is $\\|A\\|_F = \\sqrt{\\sum_{i,j} |A_{ij}|^2}$. For the zero matrix, this is:\n$$\n\\Delta = \\left\\| \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix} \\right\\|_{F} = \\sqrt{0^2 + 0^2 + 0^2 + 0^2} = 0\n$$",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "To truly master a tool, one must understand both its strengths and its limitations. This exercise uses a pair of cleverly designed synthetic datasets to explore the core competency of 1x1 convolutions—powerful channel mixing—while also exposing their inherent spatial blindness . By comparing a model that is spatially unaware with one that incorporates simple spatial context, you will gain a deeper intuition for why 1x1 convolutions are typically combined with other spatially-aware layers in modern CNNs.",
            "id": "3094354",
            "problem": "Consider image tensors and classification rules in the setting of Convolutional Neural Networks (CNN). A $1 \\times 1$ convolution in a CNN is a linear channel mixer that applies the same linear mapping at each spatial location. Specifically, for an input tensor $X \\in \\mathbb{R}^{H \\times W \\times C}$, a $1 \\times 1$ convolution with weight vector $w \\in \\mathbb{R}^{C}$ and scalar bias $b \\in \\mathbb{R}$ produces a single-channel output $Z \\in \\mathbb{R}^{H \\times W}$, where $Z_{i,j} = b + w^\\top X_{i,j,:}$ for all spatial indices $(i,j)$. Global Average Pooling (GAP) computes $z = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} Z_{i,j}$. The Network In Network (NIN) perspective emphasizes using such local linear mappings (possibly followed by nonlinearities) to increase modeling power via channel mixing.\n\nYour task is to implement and evaluate two classification models on two synthetic datasets:\n\n- Model $\\mathcal{M}_1$ (channel-mixing only): a single $1 \\times 1$ convolution with a fixed weight $w \\in \\mathbb{R}^{C}$ and bias $b = 0$, followed by Global Average Pooling and a threshold at $0$ to classify. That is, compute $Z_{i,j} = w^\\top X_{i,j,:}$, then $z = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} Z_{i,j}$, and predict $\\hat{y} = 1$ if $z \\ge 0$, else $\\hat{y} = 0$.\n\n- Model $\\mathcal{M}_2$ (spatial-context aware linear pooling): first apply the same $1 \\times 1$ convolution $Z_{i,j} = w^\\top X_{i,j,:}$, then perform a spatially non-uniform linear pooling using a fixed mask $M \\in \\mathbb{R}^{H \\times W}$ defined by $M_{i,j} = +1$ for the left half (all columns $j$ with $j \\le W/2$) and $M_{i,j} = -1$ for the right half (all columns $j$ with $j  W/2$). Compute $z = \\sum_{i=1}^{H} \\sum_{j=1}^{W} M_{i,j} Z_{i,j}$ and classify with the threshold $\\hat{y} = 1$ if $z \\ge 0$, else $\\hat{y} = 0$.\n\nDefine two datasets:\n\n- Dataset A (separable by channel mixing): For each image $X \\in \\mathbb{R}^{H \\times W \\times C}$, fix a nonzero vector $\\alpha \\in \\mathbb{R}^{C}$. Let the binary label $y \\in \\{0,1\\}$ be determined by an underlying sign $s \\in \\{-1,+1\\}$, where $s = +1$ corresponds to $y=1$ and $s = -1$ corresponds to $y=0$. Construct each image by setting every spatial location to the same channel vector $X_{i,j,:} = s \\cdot \\alpha$ for all $(i,j)$. This dataset is linearly separable by a channel mixer because the class depends only on the sign of a fixed linear form in channels and does not depend on spatial arrangement. Assume $b = 0$ and use $w = \\alpha$.\n\n- Dataset B (spatially entangled variant): For each image $X \\in \\mathbb{R}^{H \\times W \\times C}$ with even width $W$, use the same $\\alpha \\in \\mathbb{R}^{C}$ and the same binary label $y$ via $s \\in \\{-1,+1\\}$. Set the left half pixels to $X_{i,j,:} = s \\cdot \\alpha$ for all $j \\le W/2$ and the right half pixels to $X_{i,j,:} = -s \\cdot \\alpha$ for all $j  W/2$. The global average over all spatial positions is zero, so any spatially uniform channel mixer followed by Global Average Pooling loses the spatial information needed to classify correctly. Assume $b = 0$ and use $w = \\alpha$.\n\nYou must implement a program that, for each specified test case, deterministically constructs both datasets with alternating labels $s$ across the $N$ samples (i.e., sample index $k$ has $s = +1$ when $k$ is even and $s = -1$ when $k$ is odd), evaluates classification accuracy (fraction correct) for:\n\n- $\\mathcal{M}_1$ on Dataset A,\n- $\\mathcal{M}_1$ on Dataset B,\n- $\\mathcal{M}_2$ on Dataset B,\n\nand outputs the results for all test cases as floats.\n\nTest suite parameter values:\n\n- Case $1$ (happy path): $H=4$, $W=4$, $C=3$, $N=200$, $\\alpha = [1,-2,3]$.\n- Case $2$ (boundary size): $H=2$, $W=2$, $C=2$, $N=20$, $\\alpha = [1,1]$.\n- Case $3$ (increased channels): $H=4$, $W=6$, $C=5$, $N=50$, $\\alpha = [1,0,-1,2,-2]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be ordered by concatenating the three accuracies per case in the sequence above, across all cases, yielding nine floats in total: $[\\text{acc}_{\\mathcal{M}_1\\text{ on A, case 1}}, \\text{acc}_{\\mathcal{M}_1\\text{ on B, case 1}}, \\text{acc}_{\\mathcal{M}_2\\text{ on B, case 1}}, \\text{acc}_{\\mathcal{M}_1\\text{ on A, case 2}}, \\text{acc}_{\\mathcal{M}_1\\text{ on B, case 2}}, \\text{acc}_{\\mathcal{M}_2\\text{ on B, case 2}}, \\text{acc}_{\\mathcal{M}_1\\text{ on A, case 3}}, \\text{acc}_{\\mathcal{M}_1\\text{ on B, case 3}}, \\text{acc}_{\\mathcal{M}_2\\text{ on B, case 3}}]$. Each accuracy must be represented as a float.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of convolutional neural networks, well-posed with clear definitions and constraints, and objective in its formulation. It presents a solvable task with a unique, meaningful outcome.\n\nThe core of the problem is to analyze the capabilities and limitations of two simple classification models, $\\mathcal{M}_1$ and $\\mathcal{M}_2$, on two synthetic datasets, A and B. Model $\\mathcal{M}_1$ relies on channel-mixing followed by spatially-invariant Global Average Pooling (GAP), while $\\mathcal{M}_2$ incorporates a spatially-aware pooling mechanism. The datasets are designed to probe this difference. Dataset A is spatially uniform, while Dataset B has spatially-varying content designed to be neutralized by GAP.\n\nA critical point of interpretation is the definition of \"left half\" and \"right half\" of an image of width $W$. The problem states this corresponds to columns with index $j$ satisfying $j \\le W/2$ and $j  W/2$, respectively. Coupled with the hint that for Dataset B, \"The global average over all spatial positions is zero,\" it is clear that a 50/50 split of the columns is intended. For an even width $W$ and standard 0-based array indexing ($j \\in \\{0, 1, \\dots, W-1\\}$), this split is achieved by defining the left half as columns with indices $j  W/2$ and the right half as columns with indices $j \\ge W/2$. This ensures an equal number of columns, $W/2$, in each half, which makes the global average for Dataset B zero as stated. This interpretation will be used for all calculations.\n\nLet the vector $w = \\alpha$. The dot product $w^\\top \\alpha = \\alpha^\\top \\alpha = \\sum_k \\alpha_k^2 = ||\\alpha||_2^2$. Since $\\alpha$ is a non-zero vector, this quantity is a positive constant, which we denote by $K = ||\\alpha||_2^2  0$. The bias $b$ is given as $0$.\n\n### Analysis of Model $\\mathcal{M}_1$ on Dataset A\n\n1.  **Dataset A Construction**: For a given sample with sign $s \\in \\{-1, +1\\}$, every pixel vector in the image tensor $X \\in \\mathbb{R}^{H \\times W \\times C}$ is identical: $X_{i,j,:} = s \\cdot \\alpha$ for all spatial locations $(i,j)$. The true label is $y=1$ for $s=+1$ and $y=0$ for $s=-1$.\n\n2.  **$1 \\times 1$ Convolution**: The model first computes $Z \\in \\mathbb{R}^{H \\times W}$ where each element $Z_{i,j}$ is given by the channel-wise dot product:\n    $$Z_{i,j} = w^\\top X_{i,j,:} = \\alpha^\\top (s \\cdot \\alpha) = s \\cdot (\\alpha^\\top \\alpha) = s \\cdot K$$\n    This value is constant across all spatial locations $(i,j)$.\n\n3.  **Global Average Pooling (GAP)**: The next step is to compute the mean of all elements in $Z$:\n    $$z = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} Z_{i,j} = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (s \\cdot K) = \\frac{1}{HW} (HW \\cdot s \\cdot K) = s \\cdot K$$\n\n4.  **Classification**: The prediction $\\hat{y}$ is $1$ if $z \\ge 0$ and $0$ otherwise. Since $K  0$, the sign of $z$ is determined entirely by $s$.\n    -   If $s = +1$, then $z = K  0$. The model predicts $\\hat{y}=1$. The true label is $y=1$. This is a correct classification.\n    -   If $s = -1$, then $z = -K  0$. The model predicts $\\hat{y}=0$. The true label is $y=0$. This is a correct classification.\n\n5.  **Accuracy**: Model $\\mathcal{M}_1$ classifies every sample in Dataset A correctly. Therefore, the accuracy is $1.0$.\n\n### Analysis of Model $\\mathcal{M}_1$ on Dataset B\n\n1.  **Dataset B Construction**: For a sample with sign $s$, the image $X$ is split spatially. Let $W_{\\text{half}} = W/2$.\n    -   Left half ($j  W_{\\text{half}}$): $X_{i,j,:} = s \\cdot \\alpha$.\n    -   Right half ($j \\ge W_{\\text{half}}$): $X_{i,j,:} = -s \\cdot \\alpha$.\n\n2.  **$1 \\times 1$ Convolution**: The output $Z$ reflects this spatial split:\n    -   Left half ($j  W_{\\text{half}}$): $Z_{i,j} = w^\\top (s \\cdot \\alpha) = s \\cdot K$.\n    -   Right half ($j \\ge W_{\\text{half}}$): $Z_{i,j} = w^\\top (-s \\cdot \\alpha) = -s \\cdot K$.\n\n3.  **Global Average Pooling (GAP)**: The pooled value $z$ averages over the two halves. There are $H \\cdot W_{\\text{half}}$ pixels in each half.\n    $$z = \\frac{1}{HW} \\left( \\sum_{\\text{left}} Z_{i,j} + \\sum_{\\text{right}} Z_{i,j} \\right) = \\frac{1}{HW} \\left( (H \\cdot W_{\\text{half}}) \\cdot (s \\cdot K) + (H \\cdot W_{\\text{half}}) \\cdot (-s \\cdot K) \\right)$$\n    $$z = \\frac{H W_{\\text{half}}}{HW} (sK - sK) = 0$$\n    The pooled output is $0$ for every sample, regardless of its class sign $s$.\n\n4.  **Classification**: The decision rule is $\\hat{y}=1$ if $z \\ge 0$. Since $z=0$ for all samples, the model consistently predicts $\\hat{y}=1$.\n    -   For samples with $s = +1$, the true label is $y=1$. The prediction $\\hat{y}=1$ is correct.\n    -   For samples with $s = -1$, the true label is $y=0$. The prediction $\\hat{y}=1$ is incorrect.\n\n5.  **Accuracy**: The dataset is constructed with an equal number of samples for each class ($N$ is even in all test cases, with alternating signs). Therefore, the model is correct for exactly half of the dataset. The accuracy is $0.5$.\n\n### Analysis of Model $\\mathcal{M}_2$ on Dataset B\n\n1.  **Dataset B and $1 \\times 1$ Convolution**: The setup is identical to the previous analysis, yielding $Z_{i,j} = s \\cdot K$ on the left half and $Z_{i,j} = -s \\cdot K$ on the right half.\n\n2.  **Spatially Non-uniform Linear Pooling**: Model $\\mathcal{M}_2$ replaces GAP with a weighted sum using a mask $M \\in \\mathbb{R}^{H \\times W}$. The mask is defined as $M_{i,j} = +1$ for the left half and $M_{i,j} = -1$ for the right half.\n    $$z = \\sum_{i=1}^{H} \\sum_{j=1}^{W} M_{i,j} Z_{i,j} = \\sum_{\\text{left}} (+1) \\cdot Z_{i,j} + \\sum_{\\text{right}} (-1) \\cdot Z_{i,j}$$\n    $$z = (H \\cdot W_{\\text{half}}) \\cdot (s \\cdot K) - (H \\cdot W_{\\text{half}}) \\cdot (-s \\cdot K)$$\n    $$z = (H \\cdot W_{\\text{half}}) \\cdot sK + (H \\cdot W_{\\text{half}}) \\cdot sK = 2 \\cdot (H \\cdot W_{\\text{half}}) \\cdot sK = HWsK$$\n\n3.  **Classification**: The pooled value is $z = HWsK$. Since $H, W, K$ are all positive constants, the sign of $z$ is the sign of $s$.\n    -   If $s = +1$, then $z = HWK  0$. The model predicts $\\hat{y}=1$. The true label is $y=1$. This is correct.\n    -   If $s = -1$, then $z = -HWK  0$. The model predicts $\\hat{y}=0$. The true label is $y=0$. This is correct.\n\n4.  **Accuracy**: Model $\\mathcal{M}_2$ correctly classifies all samples in Dataset B. The accuracy is $1.0$.\n\n### Summary and Implementation\n\nThe analytical results are consistent and independent of the specific parameters $H, C, N$ and the choice of non-zero $\\alpha$ provided in the test cases. For each case, the expected accuracies are:\n-   $\\mathcal{M}_1$ on Dataset A: $1.0$\n-   $\\mathcal{M}_1$ on Dataset B: $0.5$\n-   $\\mathcal{M}_2$ on Dataset B: $1.0$\n\nThe program will simulate these processes for each test case to generate the required output. It will construct the datasets sample by sample, apply the models as defined, and compute the classification accuracies. This confirms the theoretical analysis and adheres to the implementation aspect of the task.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates two classification models on two synthetic datasets\n    as per the problem description.\n    \"\"\"\n    test_cases = [\n        {'H': 4, 'W': 4, 'C': 3, 'N': 200, 'alpha': np.array([1, -2, 3], dtype=np.float64)},\n        {'H': 2, 'W': 2, 'C': 2, 'N': 20, 'alpha': np.array([1, 1], dtype=np.float64)},\n        {'H': 4, 'W': 6, 'C': 5, 'N': 50, 'alpha': np.array([1, 0, -1, 2, -2], dtype=np.float64)},\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        H, W, C, N = case['H'], case['W'], case['C'], case['N']\n        alpha = case['alpha']\n        w = alpha\n\n        # --- Evaluation 1: Model M1 on Dataset A ---\n        correct_count_m1a = 0\n        for k in range(N):\n            s = 1.0 if k % 2 == 0 else -1.0\n            y_true = 1 if s > 0 else 0\n            \n            # Construct image X for Dataset A\n            X = np.full((H, W, C), s * alpha)\n            \n            # Apply Model M1\n            # 1x1 convolution\n            Z = np.tensordot(X, w, axes=([-1], [0]))\n            # Global Average Pooling\n            z = np.mean(Z)\n            # Classification\n            y_pred = 1 if z >= 0 else 0\n            \n            if y_pred == y_true:\n                correct_count_m1a += 1\n                \n        all_results.append(correct_count_m1a / N)\n\n        # --- Evaluation 2: Model M1 on Dataset B ---\n        # --- and Evaluation 3: Model M2 on Dataset B ---\n        \n        # We can evaluate both models on Dataset B in a single loop\n        correct_count_m1b = 0\n        correct_count_m2b = 0\n        \n        W_half = W // 2\n        \n        # Pre-build the mask for Model M2\n        M = np.ones((H, W))\n        M[:, W_half:] = -1.0\n\n        for k in range(N):\n            s = 1.0 if k % 2 == 0 else -1.0\n            y_true = 1 if s > 0 else 0\n\n            # Construct image X for Dataset B\n            X = np.zeros((H, W, C))\n            X[:, :W_half, :] = s * alpha\n            X[:, W_half:, :] = -s * alpha\n\n            # After 1x1 convolution, Z is common for both models\n            Z = np.tensordot(X, w, axes=([-1], [0]))\n            \n            # Apply Model M1 on Z\n            z_m1 = np.mean(Z)\n            y_pred_m1 = 1 if z_m1 >= 0 else 0\n            if y_pred_m1 == y_true:\n                correct_count_m1b += 1\n\n            # Apply Model M2 on Z\n            z_m2 = np.sum(M * Z)\n            y_pred_m2 = 1 if z_m2 >= 0 else 0\n            if y_pred_m2 == y_true:\n                correct_count_m2b += 1\n\n        all_results.append(correct_count_m1b / N)\n        all_results.append(correct_count_m2b / N)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Moving from mechanics to architecture, this problem places you in the role of a network designer facing a classic trade-off between computational cost and model accuracy. You will analyze a 'bottleneck' block, a common architectural pattern where 1x1 convolutions are used to efficiently manage the number of channels before and after a more expensive spatial convolution . By formulating and solving an optimization problem, you'll see how these layers provide a critical knob for building deep, yet efficient, neural networks.",
            "id": "3094430",
            "problem": "You are analyzing a bottleneck block inspired by the Network in Network concept in deep learning: a sequence of three convolutions consisting of a $1 \\times 1$ convolution that reduces channels from $C_{\\text{in}}$ to $C_{\\text{mid}}$, followed by a spatial $k \\times k$ convolution that maps $C_{\\text{mid}}$ to $C_{\\text{mid}}$, followed by a $1 \\times 1$ convolution that expands channels from $C_{\\text{mid}}$ to $C_{\\text{out}}$. The input feature map has spatial dimensions $H \\times W$ and $C_{\\text{in}}$ channels, and the output feature map has $C_{\\text{out}}$ channels. All quantities $H, W, C_{\\text{in}}, C_{\\text{out}}, k$ are positive integers.\n\nYou will select the bottleneck width $C_{\\text{mid}}$ to balance computational cost and accuracy. The computational cost is measured in Floating Point Operations (FLOPs), where Floating Point Operations (FLOPs) refers to the number of multiply-add computations per forward pass through the block. The validation accuracy contribution of the block is modeled by a strictly increasing, concave function with diminishing returns, given by $A(C_{\\text{mid}}) = A_{0} + \\alpha \\ln(C_{\\text{mid}})$, where $A_{0}$ and $\\alpha$ are positive constants derived from empirical calibration on a development set. You are required to meet a target validation accuracy $A_{\\text{t}}$, with $A_{\\text{t}}  A_{0}$.\n\nStarting from the definitions of convolutional computation on discrete grids and the counting of multiplications for $1 \\times 1$ and $k \\times k$ convolutions over all spatial locations and channels, derive the total FLOPs of the three-layer bottleneck block as a function of $C_{\\text{mid}}$ and establish whether it is monotone in $C_{\\text{mid}}$ for positive $H, W, C_{\\text{in}}, C_{\\text{out}}, k$. Then, formulate and solve the constrained optimization problem that minimizes the total FLOPs subject to the accuracy constraint $A(C_{\\text{mid}}) \\geq A_{\\text{t}}$. Provide the optimal $C_{\\text{mid}}$ in closed form as a single analytic expression. If any approximation is needed, state and justify it. No rounding is required.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of deep learning, specifically concerning convolutional neural network architectures and computational cost analysis. It is well-posed as a constrained optimization problem with clearly defined objective and constraint functions. The language is objective and the setup is self-contained and consistent, with the implicit assumption of spatial dimension preservation via padding being a standard convention in this context.\n\nThe first step is to derive the total computational cost, measured in Floating Point Operations (FLOPs), for the three-layer bottleneck block. A single multiply-add operation is counted as one FLOP. The spatial dimensions of the feature map, $H \\times W$, are assumed to be preserved throughout the block, which is standard practice achieved using appropriate padding in the convolutional layers.\n\nThe computational cost of a single convolutional layer producing an output map of size $H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{out}}$ from an input map of size $H_{\\text{in}} \\times W_{\\text{in}} \\times C_{\\text{in}}$ with a kernel of size $k \\times k$ is given by the product of the number of output positions, the number of output channels, and the number of operations per output value. Each output value is the result of a dot product of size $k \\times k \\times C_{\\text{in}}$. Thus, the FLOPs are $H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{out}} \\times C_{\\text{in}} \\times k^2$.\n\nWe apply this formula to each layer in the bottleneck block:\n\n1.  **First layer:** A $1 \\times 1$ convolution that reduces channels from $C_{\\text{in}}$ to $C_{\\text{mid}}$.\n    *   Input channels: $C_{\\text{in}}$\n    *   Output channels: $C_{\\text{mid}}$\n    *   Kernel size: $1 \\times 1$ (i.e., $k=1$)\n    *   Output map size: $H \\times W$\n    *   FLOPs for layer 1, $F_1$: $H \\times W \\times C_{\\text{mid}} \\times C_{\\text{in}} \\times 1^2 = HW C_{\\text{in}} C_{\\text{mid}}$.\n\n2.  **Second layer:** A $k \\times k$ spatial convolution.\n    *   Input channels: $C_{\\text{mid}}$\n    *   Output channels: $C_{\\text{mid}}$\n    *   Kernel size: $k \\times k$\n    *   Output map size: $H \\times W$\n    *   FLOPs for layer 2, $F_2$: $H \\times W \\times C_{\\text{mid}} \\times C_{\\text{mid}} \\times k^2 = HW k^2 C_{\\text{mid}}^2$.\n\n3.  **Third layer:** A $1 \\times 1$ convolution that expands channels from $C_{\\text{mid}}$ to $C_{\\text{out}}$.\n    *   Input channels: $C_{\\text{mid}}$\n    *   Output channels: $C_{\\text{out}}$\n    *   Kernel size: $1 \\times 1$\n    *   Output map size: $H \\times W$\n    *   FLOPs for layer 3, $F_3$: $H \\times W \\times C_{\\text{out}} \\times C_{\\text{mid}} \\times 1^2 = HW C_{\\text{out}} C_{\\text{mid}}$.\n\nThe total FLOPs, $F(C_{\\text{mid}})$, is the sum of the FLOPs for the three layers:\n$$ F(C_{\\text{mid}}) = F_1 + F_2 + F_3 = HW C_{\\text{in}} C_{\\text{mid}} + HW k^2 C_{\\text{mid}}^2 + HW C_{\\text{out}} C_{\\text{mid}} $$\nFactoring out common terms, we get the total FLOPs as a function of $C_{\\text{mid}}$:\n$$ F(C_{\\text{mid}}) = HW \\left( k^2 C_{\\text{mid}}^2 + (C_{\\text{in}} + C_{\\text{out}})C_{\\text{mid}} \\right) $$\nThis is a quadratic function of $C_{\\text{mid}}$.\n\nTo establish whether this function is monotone in $C_{\\text{mid}}$, we examine its derivative. Treating $C_{\\text{mid}}$ as a continuous positive variable for analysis:\n$$ \\frac{dF}{dC_{\\text{mid}}} = HW \\left( 2k^2 C_{\\text{mid}} + (C_{\\text{in}} + C_{\\text{out}}) \\right) $$\nAccording to the problem statement, $H, W, k, C_{\\text{in}}, C_{\\text{out}}$ are all positive integers. The bottleneck width $C_{\\text{mid}}$ must also be a positive quantity. Therefore, every term in the derivative expression is positive: $H0$, $W0$, $2k^20$, $C_{\\text{mid}}0$, and $(C_{\\text{in}}+C_{\\text{out}})0$. Consequently, $\\frac{dF}{dC_{\\text{mid}}}  0$ for all $C_{\\text{mid}}  0$. This proves that the total FLOPs function $F(C_{\\text{mid}})$ is strictly increasing for positive $C_{\\text{mid}}$.\n\nNext, we formulate and solve the constrained optimization problem. We want to minimize the computational cost $F(C_{\\text{mid}})$ subject to the validation accuracy constraint.\nThe optimization problem is:\n$$ \\min_{C_{\\text{mid}}  0} F(C_{\\text{mid}}) = HW \\left( k^2 C_{\\text{mid}}^2 + (C_{\\text{in}} + C_{\\text{out}})C_{\\text{mid}} \\right) $$\nSubject to:\n$$ A(C_{\\text{mid}}) \\geq A_{\\text{t}} $$\nwhere $A(C_{\\text{mid}}) = A_{0} + \\alpha \\ln(C_{\\text{mid}})$.\n\nWe first solve the constraint for $C_{\\text{mid}}$.\n$$ A_{0} + \\alpha \\ln(C_{\\text{mid}}) \\geq A_{\\text{t}} $$\nGiven that $\\alpha  0$ and $A_{\\text{t}}  A_{0}$:\n$$ \\alpha \\ln(C_{\\text{mid}}) \\geq A_{\\text{t}} - A_{0} $$\n$$ \\ln(C_{\\text{mid}}) \\geq \\frac{A_{\\text{t}} - A_{0}}{\\alpha} $$\nSince the exponential function is strictly increasing, we can exponentiate both sides without changing the inequality direction:\n$$ C_{\\text{mid}} \\geq \\exp\\left(\\frac{A_{\\text{t}} - A_{0}}{\\alpha}\\right) $$\nThis inequality defines the feasible region for $C_{\\text{mid}}$.\n\nWe have already established that the objective function, $F(C_{\\text{mid}})$, is strictly increasing for $C_{\\text{mid}}  0$. To minimize a strictly increasing function, one must choose the smallest possible value of its argument from the feasible set. The minimum value of $F(C_{\\text{mid}})$ will therefore occur at the minimum value of $C_{\\text{mid}}$ that satisfies the constraint.\n\nThe smallest value of $C_{\\text{mid}}$ satisfying $C_{\\text{mid}} \\geq \\exp\\left(\\frac{A_{\\text{t}} - A_{0}}{\\alpha}\\right)$ is precisely at the boundary of the feasible region. Although $C_{\\text{mid}}$ must be an integer in a practical implementation, the problem asks for a closed-form analytic expression, and no rounding is required. This implies that we should solve the problem in the continuous domain, which is a standard relaxation for such problems. The optimal value is thus:\n$$ C_{\\text{mid, opt}} = \\exp\\left(\\frac{A_{\\text{t}} - A_{0}}{\\alpha}\\right) $$\nThis expression gives the minimal bottleneck width required to meet the target accuracy $A_{\\text{t}}$, and since the cost function is monotonic, this choice of $C_{\\text{mid}}$ also minimizes the computational cost.",
            "answer": "$$\\boxed{\\exp\\left(\\frac{A_{\\text{t}} - A_{0}}{\\alpha}\\right)}$$"
        }
    ]
}