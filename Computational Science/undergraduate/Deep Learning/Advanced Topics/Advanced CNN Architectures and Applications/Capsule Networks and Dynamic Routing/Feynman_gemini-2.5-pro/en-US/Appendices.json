{
    "hands_on_practices": [
        {
            "introduction": "Capsule Networks were designed to overcome a key limitation of traditional Convolutional Neural Networks (CNNs): the lack of rotational equivariance. This practice provides a direct, hands-on comparison of these two architectures in a controlled setting . By implementing a simplified dynamic routing mechanism and a CNN-like baseline, you will numerically verify how CapsNets preserve pose information under rotation, whereas CNNs discard it, making the abstract concept of equivariance concrete.",
            "id": "3104851",
            "problem": "You are asked to implement a self-contained numerical experiment that contrasts the equivariance of Capsule Networks (CapsNets) with the invariance of Convolutional Neural Networks (CNNs) using a synthetic setup that mimics the effect of an affine transformation on a pose. The experiment must be conducted entirely in vector space with explicit matrices and vectors, without loading any external datasets. You will implement a single higher-level capsule with dynamic routing and a simple CNN-like baseline. The goal is to measure and compare the pose vector alignment error under a rotation transformation and to analyze how it depends on the number of dynamic routing iterations.\n\nFundamental base and definitions to use:\n- A rotation transformation in two dimensions is represented by the matrix\n$$\nT(\\theta) \\;=\\; \\begin{bmatrix}\n\\cos(\\theta) & -\\sin(\\theta)\\\\\n\\sin(\\theta) & \\cos(\\theta)\n\\end{bmatrix},\n$$\nwhich belongs to the special orthogonal group $SO(2)$ and preserves vector norms.\n- The dynamic routing procedure between capsules uses coupling coefficients computed by a softmax over routing logits and the \"squash\" nonlinearity. For a set of \"prediction\" vectors $\\{ \\mathbf{u}_i \\in \\mathbb{R}^2 \\}_{i=1}^m$ and routing logits $\\{ b_i \\in \\mathbb{R} \\}_{i=1}^m$, the coupling coefficients are\n$$\nc_i \\;=\\; \\frac{\\exp(b_i)}{\\sum_{k=1}^{m} \\exp(b_k)},\n$$\nand the pre-activation is\n$$\n\\mathbf{s} \\;=\\; \\sum_{i=1}^{m} c_i \\, \\mathbf{u}_i.\n$$\nThe squash nonlinearity maps $\\mathbf{s}$ to an output pose vector $\\mathbf{v} \\in \\mathbb{R}^2$ via\n$$\n\\mathbf{v} \\;=\\; \\frac{\\lVert \\mathbf{s} \\rVert^2}{1 + \\lVert \\mathbf{s} \\rVert^2} \\cdot \\frac{\\mathbf{s}}{\\lVert \\mathbf{s} \\rVert},\n$$\nwith the convention that if $\\lVert \\mathbf{s} \\rVert = 0$ then $\\mathbf{v} = \\mathbf{0}$.\n- The routing logits are updated by agreement for a specified number of iterations $r \\in \\mathbb{N}_0$ (nonnegative integers) using the dot product (agreement) between each prediction and the current output:\n$$\nb_i \\leftarrow b_i + \\mathbf{u}_i \\cdot \\mathbf{v}.\n$$\n- Equivariance of a representation $\\mathbf{v}$ with respect to $T(\\theta)$ means that if the input is transformed by $T(\\theta)$, then the output pose transforms by the same $T(\\theta)$. In formulas, if $\\mathbf{v}(\\theta)$ is the output pose for a transformed input, equivariance implies $\\mathbf{v}(\\theta) \\approx T(\\theta)\\,\\mathbf{v}(0)$.\n- We measure pose vector alignment error between two nonzero vectors $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^2$ as the angle between them in radians,\n$$\n\\operatorname{ang}(\\mathbf{a}, \\mathbf{b}) \\;=\\; \\arccos\\!\\left( \\frac{\\mathbf{a}^\\top \\mathbf{b}}{\\lVert \\mathbf{a} \\rVert \\,\\lVert \\mathbf{b} \\rVert} \\right).\n$$\n\nSynthetic setup:\n- Let the \"true\" canonical pose be $\\mathbf{p}_0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$.\n- For any rotation angle $\\theta \\in \\mathbb{R}$ (in radians), the transformed pose is $\\mathbf{p}(\\theta) = T(\\theta)\\,\\mathbf{p}_0$.\n- There are $m = 5$ primary capsules. Each produces a $2$-dimensional \"vote\" vector that is a scaled and slightly misaligned observation of the transformed pose:\n$$\n\\mathbf{u}_i(\\theta) \\;=\\; s_i \\, R(\\varphi_i)\\, \\mathbf{p}(\\theta),\n$$\nwhere $R(\\varphi)$ is the rotation matrix $T(\\varphi)$ and the constants are\n$$\n[s_1, s_2, s_3, s_4, s_5] \\;=\\; [1.2, 0.9, 1.1, 0.8, 1.0],\n$$\n$$\n[\\varphi_1, \\varphi_2, \\varphi_3, \\varphi_4, \\varphi_5] \\;=\\; [-0.2, 0.1, 0.0, 0.15, -0.1].\n$$\n- The higher-level capsule uses identity transforms (that is, prediction-to-output transforms are the identity in $\\mathbb{R}^2$), so the $\\mathbf{u}_i(\\theta)$ directly participate in routing.\n- The Convolutional Neural Network (CNN)-like baseline discards pose direction by aggregating only magnitudes. Define its representation for angle $\\theta$ as the $2$-vector\n$$\n\\mathbf{v}_{\\mathrm{cnn}}(\\theta) \\;=\\; \\begin{bmatrix} g(\\theta) \\\\ 0 \\end{bmatrix}, \\quad \\text{where} \\quad g(\\theta) \\;=\\; \\sum_{i=1}^{m} \\lVert \\mathbf{u}_i(\\theta) \\rVert.\n$$\nThis baseline is intentionally invariant to $\\theta$ in direction because it aligns along the fixed $x$-axis.\n\nYour tasks:\n1. Implement the dynamic routing algorithm described above for a single higher-level capsule, parameterized by the number of routing iterations $r \\in \\mathbb{N}_0$. Use the initial logits $b_i = 0$ for all $i$. In each iteration, compute the coupling coefficients $c_i$ by the softmax over $\\{b_i\\}$, compute $\\mathbf{s}$ and $\\mathbf{v}$ by the formulas above, and then update $b_i \\leftarrow b_i + \\mathbf{u}_i(\\theta) \\cdot \\mathbf{v}$. The output pose $\\mathbf{v}_{\\mathrm{caps}}(\\theta; r)$ for a given $\\theta$ and $r$ is the $\\mathbf{v}$ computed in the final iteration. For the boundary case $r = 0$, define $\\mathbf{v}_{\\mathrm{caps}}(\\theta; 0)$ as the squash of the uniform average, that is, use $c_i = 1/m$ without any updates.\n2. For the CNN-like baseline, compute $\\mathbf{v}_{\\mathrm{cnn}}(\\theta)$ as defined above.\n3. For a given model that outputs $\\mathbf{v}(\\theta)$, define the equivariance alignment error at angle $\\theta$ as\n$$\n\\varepsilon(\\theta) \\;=\\; \\operatorname{ang}\\!\\left( \\mathbf{v}(\\theta), \\; T(\\theta)\\,\\mathbf{v}(0) \\right),\n$$\nmeasured in radians.\n\nTest suite:\nEvaluate the pair of errors $\\left[ \\varepsilon_{\\mathrm{caps}}(\\theta; r), \\; \\varepsilon_{\\mathrm{cnn}}(\\theta) \\right]$ on the following cases:\n- Case $1$: $\\theta = 0.0$, $r = 0$.\n- Case $2$: $\\theta = 0.5$, $r = 0$.\n- Case $3$: $\\theta = 0.5$, $r = 5$.\n- Case $4$: $\\theta = 1.0$, $r = 0$.\n- Case $5$: $\\theta = 1.0$, $r = 5$.\n- Case $6$: $\\theta = 1.0$, $r = 10$.\n\nAngle units:\n- All angles $\\theta$, $\\varphi_i$, and errors $\\varepsilon$ must be handled and reported in radians.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the six cases as a comma-separated list of six inner lists, each inner list containing two floating-point numbers rounded to exactly six digits after the decimal point, in the order specified above. The output must be enclosed in a single pair of square brackets. For example, the printed structure must look like\n$$\n[ [a_1, b_1], [a_2, b_2], \\ldots, [a_6, b_6] ],\n$$\nbut printed without any spaces after commas to match the exact requirement\n$$\n[[a_1,b_1],[a_2,b_2],\\ldots,[a_6,b_6]].\n$$",
            "solution": "The problem statement is evaluated to be scientifically grounded, well-posed, objective, and internally consistent. It provides a clear and formalizable numerical experiment to contrast the properties of Capsule Networks (CapsNets) and a simplified Convolutional Neural Network (CNN) baseline with respect to affine transformations, specifically $SO(2)$ rotations. All necessary constants, definitions, and procedures are explicitly provided. We may therefore proceed with a complete solution.\n\nThe core of this problem lies in understanding and quantifying equivariance. A function $f$ is equivariant with respect to a transformation group $G$ if, for any transformation $T \\in G$, applying the transformation to the input and then passing it through the function yields the same result as passing the original input through the function and then applying the transformation to the output. In this context, we are investigating equivariance with respect to the rotation group $SO(2)$, and the condition is $\\mathbf{v}(\\theta) = T(\\theta)\\mathbf{v}(0)$, where $\\mathbf{v}(\\theta)$ is the model's output for an input rotated by $\\theta$. The deviation from this ideal is measured by the equivariance alignment error, $\\varepsilon(\\theta) = \\operatorname{ang}(\\mathbf{v}(\\theta), T(\\theta)\\mathbf{v}(0))$.\n\nWe will first analyze the theoretical behavior of both the CapsNet and CNN models under the problem's synthetic setup. Then, we will implement the numerical computation to verify this analysis.\n\n### Theoretical Analysis\n\n**1. CNN-like Baseline Model**\n\nThe CNN-like baseline model computes a representation $\\mathbf{v}_{\\mathrm{cnn}}(\\theta) = [g(\\theta), 0]^\\top$, where $g(\\theta) = \\sum_{i=1}^{m} \\lVert \\mathbf{u}_i(\\theta) \\rVert$. The prediction vectors are defined as $\\mathbf{u}_i(\\theta) = s_i R(\\varphi_i) \\mathbf{p}(\\theta)$, with $\\mathbf{p}(\\theta) = T(\\theta)\\mathbf{p}_0$. Here, $s_i$ are scalar constants, and $R(\\varphi_i)$ and $T(\\theta)$ are rotation matrices from $SO(2)$. Since rotations are orthogonal transformations, they preserve vector norms.\nThe norm of a prediction vector is:\n$$\n\\lVert \\mathbf{u}_i(\\theta) \\rVert = \\lVert s_i R(\\varphi_i) T(\\theta) \\mathbf{p}_0 \\rVert = |s_i| \\cdot \\lVert R(\\varphi_i) \\rVert \\cdot \\lVert T(\\theta) \\rVert \\cdot \\lVert \\mathbf{p}_0 \\rVert\n$$\nGiven that for any rotation matrix $A$, $\\lVert A\\mathbf{x} \\rVert = \\lVert\\mathbf{x}\\rVert$, and all $s_i > 0$, this simplifies to:\n$$\n\\lVert \\mathbf{u}_i(\\theta) \\rVert = s_i \\lVert \\mathbf{p}_0 \\rVert\n$$\nThis result is independent of the rotation angle $\\theta$. Consequently, the aggregated magnitude $g(\\theta)$ is also a constant, independent of $\\theta$:\n$$\ng(\\theta) = \\sum_{i=1}^{m} s_i \\lVert \\mathbf{p}_0 \\rVert = \\left( \\sum_{i=1}^{m} s_i \\right) \\lVert \\mathbf{p}_0 \\rVert = G\n$$\nThe CNN output is therefore $\\mathbf{v}_{\\mathrm{cnn}}(\\theta) = [G, 0]^\\top$ for any $\\theta$. This model is fully invariant; its output never changes.\n\nTo compute the equivariance error $\\varepsilon_{\\mathrm{cnn}}(\\theta)$, we need $\\mathbf{v}_{\\mathrm{cnn}}(0) = [G, 0]^\\top$. The target equivariant vector is $T(\\theta)\\mathbf{v}_{\\mathrm{cnn}}(0)$:\n$$\nT(\\theta)\\mathbf{v}_{\\mathrm{cnn}}(0) = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix} \\begin{bmatrix} G \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} G\\cos(\\theta) \\\\ G\\sin(\\theta) \\end{bmatrix}\n$$\nThe error is the angle between the actual output $\\mathbf{v}_{\\mathrm{cnn}}(\\theta) = [G, 0]^\\top$ and the target vector.\n$$\n\\varepsilon_{\\mathrm{cnn}}(\\theta) = \\operatorname{ang}\\left( \\begin{bmatrix} G \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} G\\cos(\\theta) \\\\ G\\sin(\\theta) \\end{bmatrix} \\right) = \\arccos\\left( \\frac{[G, 0] \\cdot [G\\cos(\\theta), G\\sin(\\theta)]}{G \\cdot G} \\right) = \\arccos(\\cos(\\theta))\n$$\nFor $\\theta$ in the primary range $[0, \\pi]$, which includes all test cases, this simplifies to $\\varepsilon_{\\mathrm{cnn}}(\\theta) = \\theta$. The CNN baseline's failure to rotate results in an error equal to the rotation angle, demonstrating a complete lack of equivariance.\n\n**2. Capsule Network Model**\n\nThe CapsNet model's behavior is fundamentally different. Let's analyze the prediction vectors $\\mathbf{u}_i(\\theta)$:\n$$\n\\mathbf{u}_i(\\theta) = s_i R(\\varphi_i) \\mathbf{p}(\\theta) = s_i T(\\varphi_i) T(\\theta) \\mathbf{p}_0\n$$\nSince rotations in $SO(2)$ commute, $T(\\varphi_i)T(\\theta) = T(\\theta)T(\\varphi_i)$. Thus:\n$$\n\\mathbf{u}_i(\\theta) = s_i T(\\theta) T(\\varphi_i) \\mathbf{p}_0\n$$\nThe prediction vector at $\\theta=0$ is $\\mathbf{u}_i(0) = s_i T(\\varphi_i) \\mathbf{p}_0$. By substitution, we find a direct relationship:\n$$\n\\mathbf{u}_i(\\theta) = T(\\theta) \\mathbf{u}_i(0)\n$$\nEach individual prediction vector is perfectly equivariant with respect to the global rotation $\\theta$.\n\nNow we analyze the dynamic routing process. Let the state of the model (vectors $\\mathbf{s}, \\mathbf{v}$ and scalars $b_i, c_i$) at angle $\\theta$ after $k$ routing iterations be denoted with a superscript $(\\theta, k)$.\n\n**Base Case ($r=0$):**\nThe output is computed using uniform coupling coefficients $c_i = 1/m$. The pre-activation vector $\\mathbf{s}^{(\\theta, 0)}$ is:\n$$\n\\mathbf{s}^{(\\theta, 0)} = \\sum_{i=1}^{m} \\frac{1}{m} \\mathbf{u}_i(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} T(\\theta) \\mathbf{u}_i(0) = T(\\theta) \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\mathbf{u}_i(0) \\right) = T(\\theta) \\mathbf{s}^{(0, 0)}\n$$\nThe pre-activation vector $\\mathbf{s}$ is perfectly equivariant. Now we apply the squash function. A key property of the squash function is its interaction with rotations. Since $\\lVert T(\\theta)\\mathbf{s} \\rVert = \\lVert \\mathbf{s} \\rVert$, we have:\n$$\n\\mathbf{v}^{(\\theta, 0)} = \\operatorname{squash}(\\mathbf{s}^{(\\theta, 0)}) = \\operatorname{squash}(T(\\theta)\\mathbf{s}^{(0, 0)}) = T(\\theta)\\operatorname{squash}(\\mathbf{s}^{(0, 0)}) = T(\\theta)\\mathbf{v}^{(0, 0)}\n$$\nThis demonstrates that for $r=0$, the capsule output is perfectly equivariant. The error $\\varepsilon_{\\mathrm{caps}}(\\theta; 0)$ must be $0$.\n\n**Inductive Step ($r>0$):**\nAssume that after $k-1$ iterations, the output is perfectly equivariant: $\\mathbf{v}^{(\\theta, k-1)} = T(\\theta)\\mathbf{v}^{(0, k-1)}$. The routing logits are updated as $b_i \\leftarrow b_i + \\mathbf{u}_i \\cdot \\mathbf{v}$. Let's examine the update term:\n$$\n\\mathbf{u}_i(\\theta) \\cdot \\mathbf{v}^{(\\theta, k-1)} = (T(\\theta)\\mathbf{u}_i(0)) \\cdot (T(\\theta)\\mathbf{v}^{(0, k-1)})\n$$\nThe dot product is invariant under rotations, i.e., $(Ra)\\cdot(Rb) = a \\cdot b$. Therefore:\n$$\n\\mathbf{u}_i(\\theta) \\cdot \\mathbf{v}^{(\\theta, k-1)} = \\mathbf{u}_i(0) \\cdot \\mathbf{v}^{(0, k-1)}\n$$\nThe logit update amount is independent of $\\theta$. By induction, if the initial logits are the same for all $\\theta$ (which they are, $b_i=0$), then the logits $b_i^{(\\theta, k)}$ at any iteration $k$ are independent of $\\theta$: $b_i^{(\\theta, k)} = b_i^{(0, k)}$.\nThis implies the coupling coefficients $c_i^{(\\theta, k)}$ are also independent of $\\theta$.\nThe pre-activation $\\mathbf{s}^{(\\theta, k)}$ for the next iteration is:\n$$\n\\mathbf{s}^{(\\theta, k)} = \\sum_{i=1}^{m} c_i^{(\\theta, k)} \\mathbf{u}_i(\\theta) = \\sum_{i=1}^{m} c_i^{(0, k)} T(\\theta)\\mathbf{u}_i(0) = T(\\theta) \\sum_{i=1}^{m} c_i^{(0, k)} \\mathbf{u}_i(0) = T(\\theta)\\mathbf{s}^{(0, k)}\n$$\nFollowing the same logic as the base case, applying the squash function yields $\\mathbf{v}^{(\\theta, k)} = T(\\theta)\\mathbf{v}^{(0, k)}$.\nThe induction holds: the capsule network output is perfectly equivariant for any number of routing iterations $r$ in this idealized setup. The error $\\varepsilon_{\\mathrm{caps}}(\\theta; r)$ is theoretically $0$ for all test cases. The analysis of its dependence on $r$ is that there is no dependence; the system is perfectly equivariant from the start.\n\n### Numerical Implementation\n\nWe will now implement the algorithms as described to confirm our theoretical analysis. The implementation will follow the specified procedures for both models and calculate the errors for the given test suite. We expect the CapsNet error to be $0$ (or a value on the order of machine epsilon) and the CNN error to be $\\theta$.\n\nThe overall process for each test case $(\\theta, r)$ will be:\n1.  Calculate $\\mathbf{v}_{\\mathrm{caps}}(\\theta; r)$ and $\\mathbf{v}_{\\mathrm{cnn}}(\\theta)$.\n2.  Calculate the reference vectors $\\mathbf{v}_{\\mathrm{caps}}(0; r)$ and $\\mathbf{v}_{\\mathrm{cnn}}(0)$.\n3.  Compute the target vectors $T(\\theta)\\mathbf{v}_{\\mathrm{caps}}(0; r)$ and $T(\\theta)\\mathbf{v}_{\\mathrm{cnn}}(0)$.\n4.  Compute the angles between the actual and target vectors to find $\\varepsilon_{\\mathrm{caps}}$ and $\\varepsilon_{\\mathrm{cnn}}$.\n\nAll calculations will be performed using floating-point arithmetic in Python with the `numpy` library.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a numerical experiment to contrast Capsule Network equivariance\n    with CNN invariance under 2D rotation.\n    \"\"\"\n\n    # --- Problem Constants ---\n    P0 = np.array([2.0, 1.0])\n    S_VALS = np.array([1.2, 0.9, 1.1, 0.8, 1.0])\n    PHI_VALS = np.array([-0.2, 0.1, 0.0, 0.15, -0.1])\n    M = 5\n\n    # --- Helper Functions ---\n    def rotation_matrix(theta: float) -> np.ndarray:\n        \"\"\"Computes the 2D rotation matrix T(theta).\"\"\"\n        c, s = np.cos(theta), np.sin(theta)\n        return np.array([[c, -s], [s, c]])\n\n    def squash(s: np.ndarray, epsilon: float = 1e-9) -> np.ndarray:\n        \"\"\"Applies the squashing non-linearity to a vector s.\"\"\"\n        s_norm_sq = np.sum(s**2)\n        s_norm = np.sqrt(s_norm_sq)\n        if s_norm < epsilon:\n            return np.zeros_like(s)\n        scale = s_norm_sq / (1.0 + s_norm_sq)\n        return scale * s / s_norm\n\n    def get_votes(theta: float) -> np.ndarray:\n        \"\"\"Generates the set of prediction vectors u_i(theta).\"\"\"\n        p_theta = rotation_matrix(theta) @ P0\n        votes = np.zeros((M, 2))\n        for i in range(M):\n            # u_i(theta) = s_i * R(phi_i) * p(theta)\n            votes[i, :] = S_VALS[i] * (rotation_matrix(PHI_VALS[i]) @ p_theta)\n        return votes\n\n    def angle_between(v1: np.ndarray, v2: np.ndarray, epsilon: float = 1e-9) -> float:\n        \"\"\"Calculates the angle in radians between two 2D vectors.\"\"\"\n        norm1 = np.linalg.norm(v1)\n        norm2 = np.linalg.norm(v2)\n        if norm1 < epsilon or norm2 < epsilon:\n            return 0.0\n        \n        dot_product = np.dot(v1, v2)\n        cos_angle = dot_product / (norm1 * norm2)\n        # Clip to handle potential floating point inaccuracies\n        cos_angle = np.clip(cos_angle, -1.0, 1.0)\n        return np.arccos(cos_angle)\n\n    # --- Model Implementations ---\n    def capsule_model(theta: float, r: int) -> np.ndarray:\n        \"\"\"\n        Computes the output pose vector v_caps(theta; r) using dynamic routing.\n        \"\"\"\n        votes = get_votes(theta)\n        \n        if r == 0:\n            # Uniform averaging, no routing iterations\n            s = np.mean(votes, axis=0)\n            return squash(s)\n\n        # Dynamic routing for r > 0\n        b = np.zeros(M)\n        v = np.zeros(2) # v is initialized implicitly in loop\n        for _ in range(r):\n            # Softmax to get coupling coefficients\n            c = np.exp(b) / np.sum(np.exp(b))\n            # Weighted sum of votes\n            s = np.sum(c[:, np.newaxis] * votes, axis=0)\n            # Squash to get output pose vector\n            v = squash(s)\n            # Update logits by agreement\n            agreement = np.dot(votes, v) # shape (M,)\n            b += agreement\n            \n        return v\n\n    def cnn_model(theta: float) -> np.ndarray:\n        \"\"\"Computes the output representation v_cnn(theta).\"\"\"\n        votes = get_votes(theta)\n        g_theta = np.sum(np.linalg.norm(votes, axis=1))\n        return np.array([g_theta, 0.0])\n\n    # --- Test Suite ---\n    test_cases = [\n        # (theta, r)\n        (0.0, 0),  # Case 1\n        (0.5, 0),  # Case 2\n        (0.5, 5),  # Case 3\n        (1.0, 0),  # Case 4\n        (1.0, 5),  # Case 5\n        (1.0, 10), # Case 6\n    ]\n\n    results = []\n    for theta, r in test_cases:\n        # 1. Calculate CapsNet error\n        v_caps_theta = capsule_model(theta, r)\n        v_caps_0 = capsule_model(0.0, r)\n        target_caps = rotation_matrix(theta) @ v_caps_0\n        eps_caps = angle_between(v_caps_theta, target_caps)\n\n        # 2. Calculate CNN error\n        v_cnn_theta = cnn_model(theta)\n        v_cnn_0 = cnn_model(0.0)\n        target_cnn = rotation_matrix(theta) @ v_cnn_0\n        eps_cnn = angle_between(v_cnn_theta, target_cnn)\n\n        results.append(f\"[{eps_caps:.6f},{eps_cnn:.6f}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While powerful, the 'routing-by-agreement' algorithm can be indecisive when faced with symmetric or ambiguous inputs, a scenario where parts could plausibly belong to multiple wholes. This exercise guides you through a classic failure case of dynamic routing known as symmetric ambiguity . You will first observe the routing mechanism fail to make a clear choice and then implement and test simple but effective priors to break the symmetry and guide the network to the correct part-whole assignment.",
            "id": "3104796",
            "problem": "Consider a simplified vector-based Capsule Network (CapsNet) routing scenario with two layers: a set of primary capsules (parts) indexed by $i \\in \\{0,1\\}$ and a set of higher-level capsules (wholes) indexed by $j \\in \\{0,1\\}$. Each primary capsule $i$ emits a prediction vector $u_{\\hat{i}j} \\in \\mathbb{R}^d$ for each higher-level capsule $j$. The pose dimensionality is $d = 2$. The routing algorithm to be implemented is the routing-by-agreement scheme defined by the following base components.\n\n- Let $b_{ij}$ be the routing logits. Initialize $b_{ij}$ as needed by any prior you include. At each routing iteration, compute the coupling coefficients $c_{ij}$ by applying the softmax across $j$ for each fixed $i$, namely $c_{ij} = \\exp(b_{ij}) \\big/ \\sum_{j'} \\exp(b_{ij'})$.\n- Compute the pre-activation of each higher-level capsule $j$ as $\\mathbf{s}_j = \\sum_i c_{ij} u_{\\hat{i}j}$.\n- Apply the standard squash nonlinearity for vector capsules to produce $\\mathbf{v}_j \\in \\mathbb{R}^d$ via\n$$\nv_j = \\frac{\\lVert s_j \\rVert^2}{1 + \\lVert s_j \\rVert^2} \\cdot \\frac{s_j}{\\lVert s_j \\rVert},\n$$\nwith the understanding that a numerically stable implementation must handle the case $\\lVert s_j \\rVert = 0$.\n- Update the routing logits by agreement: $b_{ij} \\leftarrow b_{ij} + u_{\\hat{i}j} \\cdot v_j$ where $\\cdot$ denotes the Euclidean inner product.\n\nYou will investigate symmetric ambiguity in routing, in which parts can be assigned to incorrect wholes due to symmetry, and then add symmetry-breaking priors to improve routing.\n\nYour task is to implement the above routing algorithm for a fixed number of iterations $r = 3$ and evaluate the sum of coupling mass directed to the ground-truth whole. For a given ground-truth index $j^\\star \\in \\{0,1\\}$, define the evaluation metric\n$$\n\\mathcal{S} = \\sum_{i \\in \\{0,1\\}} c_{i j^\\star}.\n$$\n\nYou must implement two symmetry-breaking priors:\n\n- Parent logit bias prior: a parent-level bias vector $\\beta = (\\beta_0, \\beta_1)$ that is added to the initial logits $b_{ij}$ identically for both parts $i$ (that is, $b_{ij} \\leftarrow b_{ij} + \\beta_j$ before the first softmax).\n- Vote perturbation prior: a small vector $\\Delta = (\\delta_x, \\delta_y)$ that perturbs prediction vectors so that predictions toward the ground-truth whole are shifted as $u_{\\hat{i} j^\\star} \\leftarrow u_{\\hat{i} j^\\star} + \\Delta$, while predictions toward the other whole are shifted oppositely as $u_{\\hat{i} j} \\leftarrow u_{\\hat{i} j} - \\Delta$ for $j \\neq j^\\star$.\n\nImplement the routing and evaluate the metric $\\mathcal{S}$ for each of the following test cases. In all cases use $n = 2$ parts, $m = 2$ wholes, $d = 2$, and $r = 3$ iterations. The ground-truth whole is $j^\\star = 0$.\n\n- Test case A (perfect symmetry, no prior):\n  - Predictions: $u_{\\hat{00}} = (1, 0)$, $u_{\\hat{10}} = (1, 0)$, $u_{\\hat{01}} = (0, 1)$, $u_{\\hat{11}} = (0, 1)$.\n  - Priors: $\\beta = (0, 0)$, $\\Delta = (0, 0)$.\n\n- Test case B (perfect symmetry, logit bias prior):\n  - Predictions: as in Test case A.\n  - Priors: $\\beta = (0.05, -0.05)$, $\\Delta = (0, 0)$.\n\n- Test case C (conflicting votes, no prior):\n  - Predictions: $u_{\\hat{00}} = (1, 0)$, $u_{\\hat{10}} = (0, 1)$, $u_{\\hat{01}} = (0, 1)$, $u_{\\hat{11}} = (1, 0)$.\n  - Priors: $\\beta = (0, 0)$, $\\Delta = (0, 0)$.\n\n- Test case D (conflicting votes, logit bias prior):\n  - Predictions: as in Test case C.\n  - Priors: $\\beta = (0.2, -0.2)$, $\\Delta = (0, 0)$.\n\n- Test case E (perfect symmetry, vote perturbation prior):\n  - Predictions: as in Test case A.\n  - Priors: $\\beta = (0, 0)$, $\\Delta = (0.05, 0.00)$.\n\nYour program must compute the metric $\\mathcal{S}$ for each test case in the order A, B, C, D, E, each as a real number. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each real number rounded to exactly $6$ decimal places (e.g., $[1.000000,1.937500,1.000000,1.750000,1.875000]$). No other output should be produced.",
            "solution": "The user has provided a problem that requires the implementation and evaluation of a simplified dynamic routing algorithm, a core component of Capsule Networks (CapsNets). The task is to compute a specific metric, $\\mathcal{S}$, under five different scenarios designed to test the algorithm's behavior in the presence of symmetry and the effect of symmetry-breaking priors.\n\nFirst, the validity of the problem statement is confirmed. The problem is scientifically grounded in the principles of deep learning, specifically the routing-by-agreement mechanism detailed in the original CapsNet literature. All provided parameters, equations, and conditions are self-contained, mathematically consistent, and well-defined, constituting a well-posed problem. There are no factual errors, ambiguities, or subjective elements.\n\nThe solution involves implementing the iterative routing algorithm as specified. The state of the system is stored in several matrices. The prediction vectors from primary capsules $i$ to higher-level capsules $j$ are given by $u_{\\hat{i}j}$. These are stored in a tensor of size $n \\times m \\times d$, where $n=2$ is the number of primary capsules, $m=2$ is the number of higher-level capsules, and $d=2$ is the vector dimensionality. The routing logits, $b_{ij}$, which dynamically adjust to control the routing, are stored in an $n \\times m$ matrix.\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization**:\n    The routing logits $b_{ij}$ are initialized to $0$. The problem defines two types of priors that modify this initial state.\n    - **Vote Perturbation Prior**: Before the routing process begins, the prediction vectors $u_{\\hat{i}j}$ are perturbed. For the ground-truth capsule $j^\\star=0$, prediction vectors are shifted by a small vector $\\Delta$, i.e., $u_{\\hat{i}0} \\leftarrow u_{\\hat{i}0} + \\Delta$. For the other capsule ($j=1$), they are shifted by $-\\Delta$, i.e., $u_{\\hat{i}1} \\leftarrow u_{\\hat{i}1} - \\Delta$. This is applied to all primary capsules $i \\in \\{0,1\\}$.\n    - **Parent Logit Bias Prior**: A bias vector $\\beta = (\\beta_0, \\beta_1)$ is added to the initial logits. For each primary capsule $i$, the logit for higher-level capsule $j$ is updated as $b_{ij} \\leftarrow b_{ij} + \\beta_j$. This operation is performed before the first iteration of the routing loop.\n\n2.  **Iterative Routing**: The core of the algorithm is a loop that runs for a fixed number of iterations, $r = 3$. Each iteration consists of four steps:\n    a.  **Coupling Coefficients ($c_{ij}$)**: The routing logits $b_{ij}$ are converted into coupling coefficients $c_{ij}$ using the softmax function. The softmax is applied for each primary capsule $i$ across all higher-level capsules $j$:\n        $$c_{ij} = \\frac{\\exp(b_{ij})}{\\sum_{j' \\in \\{0,1\\}} \\exp(b_{ij'})}$$\n        The value $c_{ij}$ represents the probability that primary capsule $i$ should be routed to higher-level capsule $j$.\n\n    b.  **Pre-activation ($s_j$)**: The pre-activation vector $s_j$ for each higher-level capsule $j$ is calculated as a weighted sum of the prediction vectors from all primary capsules. The weights are the coupling coefficients $c_{ij}$:\n        $$s_j = \\sum_{i \\in \\{0,1\\}} c_{ij} u_{\\hat{i}j}$$\n\n    c.  **Activation ($v_j$)**: The pre-activation vector $s_j$ is passed through a non-linear \"squash\" function to produce the final output vector $v_j$ for the higher-level capsule. The squash function scales the vector's magnitude to be between $0$ and $1$, preserving its direction.\n        $$v_j = \\frac{\\lVert s_j \\rVert^2}{1 + \\lVert s_j \\rVert^2} \\frac{s_j}{\\lVert s_j \\rVert}$$\n        A numerically stable implementation is essential, particularly for the case where $\\lVert s_j \\rVert = 0$. In this scenario, $v_j$ must be the zero vector. This is handled by adding a small epsilon to the denominator term $\\lVert s_j \\rVert$ before division.\n\n    d.  **Logit Update ($b_{ij}$)**: The routing logits are updated based on the agreement between the prediction vectors $u_{\\hat{i}j}$ and the resulting higher-level capsule activations $v_j$. The agreement is measured by the Euclidean inner product (dot product).\n        $$b_{ij} \\leftarrow b_{ij} + u_{\\hat{i}j} \\cdot v_j$$\n        This update implements the \"routing-by-agreement\" principle: if a primary capsule's prediction aligns well with the aggregated output of a higher-level capsule, the logit connecting them is increased, reinforcing that routing path in the next iteration.\n\n3.  **Final Evaluation**:\n    After $r=3$ iterations, the final routing logits $b_{ij}$ are obtained. From these, a final set of coupling coefficients $c_{ij}$ is computed using the same softmax function as in step 2a. The evaluation metric $\\mathcal{S}$ is the sum of the coupling coefficients directed to the ground-truth capsule $j^\\star=0$ from all primary capsules:\n    $$\\mathcal{S} = \\sum_{i \\in \\{0,1\\}} c_{i, j^\\star} = c_{00} + c_{10}$$\n    This metric quantifies the total \"routing mass\" assigned to the correct whole, with a maximum possible value of $2$ (if both primary capsules route fully to $j^\\star=0$) and a minimum of $0$. A value of $1$ indicates that, on average, the routing is perfectly ambiguous.\n\nThe implementation will process each of the five test cases by setting the appropriate prediction vectors and prior parameters and executing the described algorithm to calculate the metric $\\mathcal{S}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import softmax\n\ndef squash(s, epsilon=1e-9):\n    \"\"\"\n    Applies the squash nonlinearity to a set of vectors.\n    s: A NumPy array of shape (m, d) representing pre-activation vectors.\n    Returns: A NumPy array of shape (m, d) representing activated vectors.\n    \"\"\"\n    # s_norm_sq has shape (m, 1) to enable broadcasting\n    s_norm_sq = np.sum(s**2, axis=-1, keepdims=True)\n    # scale factor also has shape (m, 1)\n    scale = s_norm_sq / (1.0 + s_norm_sq)\n    # s_norm has shape (m, 1)\n    s_norm = np.sqrt(s_norm_sq)\n    # unit_s is s normalized, with protection against division by zero\n    unit_s = s / (s_norm + epsilon)\n    # The result v is the scaled unit vector\n    v = scale * unit_s\n    return v\n\ndef run_routing(u_in, beta, delta, j_star, r, n, m, d):\n    \"\"\"\n    Executes the dynamic routing algorithm for a given configuration.\n    \n    Args:\n        u_in (np.ndarray): Prediction vectors, shape (n, m, d).\n        beta (np.ndarray): Parent logit bias prior, shape (m,).\n        delta (np.ndarray): Vote perturbation prior, shape (d,).\n        j_star (int): Ground-truth whole index.\n        r (int): Number of routing iterations.\n        n (int): Number of primary capsules.\n        m (int): Number of higher-level capsules.\n        d (int): Dimensionality of pose vectors.\n\n    Returns:\n        float: The evaluation metric S.\n    \"\"\"\n    # Make a copy to avoid modifying the global test case data\n    u = u_in.copy()\n    \n    # 1. Apply vote perturbation prior\n    if np.any(delta):\n        other_j = 1 - j_star\n        u[:, j_star, :] += delta\n        u[:, other_j, :] -= delta\n\n    # 2. Initialize routing logits b_ij\n    b = np.zeros((n, m))\n    \n    # 3. Apply parent logit bias prior\n    if np.any(beta):\n        b += beta # Broadcasting adds beta to each row of b\n\n    # 4. Routing loop for r iterations\n    for _ in range(r):\n        # a. Compute coupling coefficients c_ij by softmax over logits\n        c = softmax(b, axis=1)\n        \n        # b. Compute pre-activations s_j as a weighted sum of predictions\n        s = np.einsum('ij,ijd->jd', c, u)\n        \n        # c. Compute activations v_j using the squash function\n        v = squash(s)\n        \n        # d. Update logits b_ij based on agreement (dot product)\n        agreement = np.einsum('ijd,jd->ij', u, v)\n        b += agreement\n        \n    # 5. Compute final coupling coefficients after all iterations\n    final_c = softmax(b, axis=1)\n    \n    # 6. Compute the metric S: sum of coupling mass to the ground-truth whole\n    metric_s = np.sum(final_c[:, j_star])\n    \n    return metric_s\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the routing algorithm for each, printing the results.\n    \"\"\"\n    # Define constants for all test cases\n    n, m, d = 2, 2, 2\n    r = 3\n    j_star = 0\n    \n    # Define prediction vectors for the test cases\n    u_A = np.array([[[1.0, 0.0], [0.0, 1.0]], \n                    [[1.0, 0.0], [0.0, 1.0]]])\n    \n    u_C = np.array([[[1.0, 0.0], [0.0, 1.0]], \n                    [[0.0, 1.0], [1.0, 0.0]]])\n\n    # Define the 5 test cases from the problem statement\n    test_cases = [\n        # Case A: Perfect symmetry, no prior\n        {'u': u_A, 'beta': np.array([0.0, 0.0]), 'delta': np.array([0.0, 0.0])},\n        # Case B: Perfect symmetry, logit bias prior\n        {'u': u_A, 'beta': np.array([0.05, -0.05]), 'delta': np.array([0.0, 0.0])},\n        # Case C: Conflicting votes, no prior\n        {'u': u_C, 'beta': np.array([0.0, 0.0]), 'delta': np.array([0.0, 0.0])},\n        # Case D: Conflicting votes, logit bias prior\n        {'u': u_C, 'beta': np.array([0.2, -0.2]), 'delta': np.array([0.0, 0.0])},\n        # Case E: Perfect symmetry, vote perturbation prior\n        {'u': u_A, 'beta': np.array([0.0, 0.0]), 'delta': np.array([0.05, 0.0])},\n    ]\n\n    results = []\n    for case in test_cases:\n        s_metric = run_routing(\n            u_in=case['u'],\n            beta=case['beta'],\n            delta=case['delta'],\n            j_star=j_star,\n            r=r, n=n, m=m, d=d\n        )\n        results.append(s_metric)\n\n    # Format the output as a comma-separated list with 6 decimal places, enclosed in brackets.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The dynamic routing process is an iterative optimization whose behavior is highly sensitive to its hyperparameters. This practice focuses on temperature, a critical parameter that controls the sharpness of the softmax function used to calculate coupling coefficients . You will explore how a temperature annealing schedule—starting high and decreasing over time—can help the routing algorithm first explore various part-whole hypotheses and then confidently converge to the most coherent one, preventing premature decisions.",
            "id": "3104863",
            "problem": "Consider Capsule Networks (CapsNets) with Dynamic Routing. At routing iteration $t \\in \\{0,1,\\dots,T-1\\}$, for each lower-level capsule index $i \\in \\{0,\\dots,I-1\\}$ and each parent capsule index $j \\in \\{0,\\dots,J-1\\}$, define routing logits $b_{ij}^{(t)} \\in \\mathbb{R}$. The coupling coefficients $c_{ij}^{(t)}$ are computed via a temperature-scaled softmax over parent capsules:\n$$\nc_{ij}^{(t)} \\triangleq \\frac{\\exp\\left(b_{ij}^{(t)}/\\tau_t\\right)}{\\sum_{k=0}^{J-1}\\exp\\left(b_{ik}^{(t)}/\\tau_t\\right)},\n$$\nwhere $\\tau_t > 0$ is a temperature at iteration $t$. The parent capsule outputs use the squash nonlinearity. Let predicted votes $\\mathbf{\\hat{u}}_{j|i} \\in \\mathbb{R}^d$ be fixed. The parent capsule pre-activation at iteration $t$ is\n$$\n\\mathbf{s}_j^{(t)} \\triangleq \\sum_{i=0}^{I-1} c_{ij}^{(t)} \\,\\mathbf{\\hat{u}}_{j|i},\n$$\nand the parent capsule output is the squashed vector\n$$\n\\mathbf{v}_j^{(t)} \\triangleq \\mathrm{squash}\\!\\left(\\mathbf{s}_j^{(t)}\\right) = \\frac{\\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert^2}{1 + \\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert^2} \\cdot \\frac{\\mathbf{s}_j^{(t)}}{\\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert},\n$$\nwith the convention that the result is the zero vector if $\\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert = 0$. Agreement updates the routing logits as\n$$\nb_{ij}^{(t+1)} \\triangleq b_{ij}^{(t)} + \\mathbf{\\hat{u}}_{j|i} \\cdot \\mathbf{v}_j^{(t)}.\n$$\nDefine the Shannon entropy of the coupling distribution for each lower-level capsule $i$ at iteration $t$ as\n$$\nH_i^{(t)} \\triangleq -\\sum_{j=0}^{J-1} c_{ij}^{(t)} \\log c_{ij}^{(t)}.\n$$\nYou must investigate temperature annealing schedules $\\tau_t$ that prevent early collapse (premature assignment of all coupling mass to a single parent capsule) while ensuring final selectivity (confident assignment at the last iteration).\n\nStarting from the fundamental definitions above, design a program that:\n- Implements the temperature-scaled softmax routing, squash nonlinearity, and agreement updates precisely as stated.\n- Simulates routing for a fixed, small scenario with $I=3$ lower-level capsules, $J=2$ parent capsules, vector dimension $d=2$, number of routing iterations $T=6$, and initial logits $b_{ij}^{(0)} = 0$ for all $i,j$.\n- Uses the following fixed predicted votes (each entry is a two-dimensional vector):\n  - For lower-level capsule $i=0$: $\\mathbf{\\hat{u}}_{0|0} = [1.2, 0.0]$, $\\mathbf{\\hat{u}}_{1|0} = [0.0, 1.1]$.\n  - For lower-level capsule $i=1$: $\\mathbf{\\hat{u}}_{0|1} = [1.0, 0.1]$, $\\mathbf{\\hat{u}}_{1|1} = [0.1, 0.9]$.\n  - For lower-level capsule $i=2$: $\\mathbf{\\hat{u}}_{0|2} = [0.0, 1.0]$, $\\mathbf{\\hat{u}}_{1|2} = [1.0, 0.0]$.\n\nYour program must evaluate five candidate temperature schedules, each specified by parameters but without prescribing any explicit formula in this problem statement. For each schedule, your program must decide whether both of the following constraints are met:\n- Early anti-collapse: the mean entropy across lower-level capsules at iterations $t=0$ and $t=1$ is at least $H_{\\mathrm{early,min}} = 0.55$.\n- Final selectivity: the mean entropy across lower-level capsules at iteration $t=T-1$ is at most $H_{\\mathrm{final,max}} = 0.20$.\n\nThe five schedules and their parameters to be evaluated are:\n- Case A (fast-decaying, non-increasing): initial temperature $3.0$, decay factor parameter $0.5$.\n- Case B (always high): constant temperature $3.0$.\n- Case C (always low): constant temperature $0.25$.\n- Case D (smooth decay with floor across the $T$ iterations): lower bound $0.2$, upper bound $3.0$.\n- Case E (entropy-triggered two-phase scheme): high phase temperature $3.0$, low phase temperature $0.2$, switch threshold on mean entropy $0.60$, forced low temperature starting from iteration $3$.\n\nYour program must implement these schedules, run the routing simulation for each, and produce a boolean for each case indicating whether both constraints are satisfied. The final output format must be a single line containing the five booleans for Cases A–E, as a comma-separated list enclosed in square brackets, for example $[\\mathrm{True},\\mathrm{False},\\dots]$.\n\nYour program must be self-contained, require no input, and use the specified scenario and parameters. No physical units or angles are involved. The test suite is the five schedules above, which cover a general case, boundary conditions, and an edge case. The answers are booleans.",
            "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in the principles of Capsule Networks, is well-posed with a complete and consistent set of definitions and parameters, and poses a verifiable, non-trivial computational task. The minor ambiguity in the definition of certain temperature schedules is resolved by selecting standard functional forms that are consistent with the provided descriptions, a common practice in formalizing algorithmic problems.\n\nThe task is to simulate the dynamic routing process in a Capsule Network for a specific small-scale configuration and evaluate five different temperature annealing schedules against two performance constraints: preventing premature convergence (early anti-collapse) and ensuring final decisiveness (final selectivity).\n\nThe core of the simulation is an iterative process over $t \\in \\{0, 1, \\dots, T-1\\}$, where $T=6$. The state of the system at each iteration is defined by the routing logits $b_{ij}^{(t)}$. We start with $b_{ij}^{(0)} = 0$ for all $i \\in \\{0, \\dots, 2\\}$ and $j \\in \\{0, \\dots, 1\\}$.\n\n**Step 1: Coupling Coefficient Calculation**\nAt each iteration $t$, the coupling coefficients $c_{ij}^{(t)}$ are computed from the logits $b_{ij}^{(t)}$ using a temperature-scaled softmax function. For each lower-level capsule $i$, the coefficients are a probability distribution over the parent capsules $j$:\n$$c_{ij}^{(t)} = \\frac{\\exp\\left(b_{ij}^{(t)}/\\tau_t\\right)}{\\sum_{k=0}^{J-1}\\exp\\left(b_{ik}^{(t)}/\\tau_t\\right)}$$\nHere, $\\tau_t > 0$ is the temperature at iteration $t$. A high temperature flattens the distribution, encouraging exploration, while a low temperature sharpens it, encouraging exploitation. For numerical stability, a common technique is to subtract the maximum logit value from all logits before exponentiation, which does not change the result of the softmax.\n\n**Step 2: Entropy Calculation**\nThe Shannon entropy of the coupling distribution for each capsule $i$ is calculated to measure the degree of certainty in routing:\n$$H_i^{(t)} = -\\sum_{j=0}^{J-1} c_{ij}^{(t)} \\log c_{ij}^{(t)}$$\nThe natural logarithm is used, as is standard. The mean entropy across all $I$ lower-level capsules, $\\bar{H}^{(t)} = \\frac{1}{I}\\sum_{i=0}^{I-1}H_i^{(t)}$, is the primary metric for our evaluation constraints.\n\n**Step 3: Parent Capsule Activation**\nThe pre-activation vector $\\mathbf{s}_j^{(t)}$ for each parent capsule $j$ is a weighted sum of the prediction vectors $\\mathbf{\\hat{u}}_{j|i}$ from all lower-level capsules $i$:\n$$\\mathbf{s}_j^{(t)} = \\sum_{i=0}^{I-1} c_{ij}^{(t)} \\,\\mathbf{\\hat{u}}_{j|i}$$\nThis vector aggregates the \"votes\" from the layer below. The contribution of each vote is weighted by the corresponding coupling coefficient.\n\n**Step 4: Squash Nonlinearity**\nThe pre-activation vector $\\mathbf{s}_j^{(t)}$ is passed through a non-linear \"squashing\" function to produce the parent capsule's output vector $\\mathbf{v}_j^{(t)}$. This function normalizes the vector's length to be between $0$ and $1$, preserving its orientation:\n$$\\mathbf{v}_j^{(t)} = \\frac{\\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert^2}{1 + \\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert^2} \\cdot \\frac{\\mathbf{s}_j^{(t)}}{\\left\\lVert \\mathbf{s}_j^{(t)}\\right\\rVert}$$\nIf $\\mathbf{s}_j^{(t)}$ is the zero vector, its norm is $0$, and the output $\\mathbf{v}_j^{(t)}$ is also the zero vector, a case that must be handled to prevent division by zero.\n\n**Step 5: Routing Logit Update**\nThe logits for the next iteration, $b_{ij}^{(t+1)}$, are updated based on the agreement between the prediction vectors $\\mathbf{\\hat{u}}_{j|i}$ and the parent capsule outputs $\\mathbf{v}_j^{(t)}$. The agreement is measured by the scalar product:\n$$b_{ij}^{(t+1)} = b_{ij}^{(t)} + \\mathbf{\\hat{u}}_{j|i} \\cdot \\mathbf{v}_j^{(t)}$$\nThis \"routing-by-agreement\" mechanism reinforces couplings that lead to strong agreement.\n\n**Temperature Schedules**\nThe five candidate schedules are implemented as follows:\n- **Case A (Fast Decay):** An exponential decay schedule is used: $\\tau_t = \\tau_{init} \\cdot \\gamma^t$, with $\\tau_{init} = 3.0$ and $\\gamma = 0.5$.\n- **Case B (Always High):** A constant schedule: $\\tau_t = 3.0$.\n- **Case C (Always Low):** A constant schedule: $\\tau_t = 0.25$.\n- **Case D (Smooth Decay):** A linear decay schedule from an upper to a lower bound: $\\tau_t = \\tau_{upper} - (\\tau_{upper} - \\tau_{lower}) \\frac{t}{T-1}$, with $\\tau_{upper} = 3.0$ and $\\tau_{lower} = 0.2$.\n- **Case E (Entropy-Triggered):** A dynamic schedule. For iteration $t$:\n  _If_ $t \\ge 3$, $\\tau_t=0.2$.\n  _Else if_ $t=0$, $\\tau_t=3.0$.\n  _Else_ ($t \\in \\{1, 2\\}$), $\\tau_t=3.0$ if $\\bar{H}^{(t-1)} \\ge 0.60$, and $\\tau_t=0.2$ otherwise.\n\n**Evaluation and Program Design**\nA function is designed to take a schedule's parameters, run the complete simulation for $T=6$ iterations, and record the mean entropy $\\bar{H}^{(t)}$ at each step. After the simulation, it checks the two constraints:\n1.  **Early anti-collapse**: $\\bar{H}^{(0)} \\ge 0.55$ and $\\bar{H}^{(1)} \\ge 0.55$.\n2.  **Final selectivity**: $\\bar{H}^{(5)} \\le 0.20$.\n\nThe function returns `True` if both constraints are met, and `False` otherwise. The main program executes this evaluation for each of the five schedules and formats the boolean results as specified. A vectorized implementation using NumPy is employed for efficiency, particularly for the agreement calculation which can be expressed as an Einstein summation: `agreement = np.einsum('ijd,jd->ij', u_hat, v)`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to evaluate five temperature schedules for dynamic routing\n    in a Capsule Network and print the results.\n    \"\"\"\n\n    def evaluate_schedule(schedule_type, params):\n        \"\"\"\n        Simulates dynamic routing for a given temperature schedule and\n        evaluates it against specified entropy constraints.\n\n        Args:\n            schedule_type (str): An identifier for the schedule ('A', 'B', 'C', 'D', 'E').\n            params (tuple): Parameters for the schedule.\n\n        Returns:\n            bool: True if both early anti-collapse and final selectivity\n                  constraints are met, False otherwise.\n        \"\"\"\n        # --- Fixed Simulation Parameters ---\n        I, J, d, T = 3, 2, 2, 6\n        H_early_min, H_final_max = 0.55, 0.20\n\n        # --- Fixed Predicted Votes (u_hat[i, j, :] is u_j|i) ---\n        u_hat = np.zeros((I, J, d))\n        u_hat[0, 0, :] = [1.2, 0.0]\n        u_hat[0, 1, :] = [0.0, 1.1]\n        u_hat[1, 0, :] = [1.0, 0.1]\n        u_hat[1, 1, :] = [0.1, 0.9]\n        u_hat[2, 0, :] = [0.0, 1.0]\n        u_hat[2, 1, :] = [1.0, 0.0]\n\n        # --- Initialization ---\n        b = np.zeros((I, J))  # Routing logits\n        mean_entropies = []\n\n        # --- Dynamic Routing Simulation Loop ---\n        for t in range(T):\n            # 1. Determine current temperature tau_t\n            current_tau = 0.0\n            if schedule_type == 'A':\n                initial_temp, decay_factor = params\n                current_tau = initial_temp * (decay_factor ** t)\n            elif schedule_type == 'B' or schedule_type == 'C':\n                const_temp, = params\n                current_tau = const_temp\n            elif schedule_type == 'D':\n                lower, upper = params\n                current_tau = upper - (upper - lower) * t / (T - 1)\n            elif schedule_type == 'E':\n                high_temp, low_temp, switch_thresh, forced_low_iter = params\n                if t >= forced_low_iter:\n                    current_tau = low_temp\n                elif t == 0:\n                    current_tau = high_temp\n                else:  # t in {1, 2}\n                    if mean_entropies[t - 1] < switch_thresh:\n                        current_tau = low_temp\n                    else:\n                        current_tau = high_temp\n\n            # 2. Calculate coupling coefficients c_ij\n            # Using a numerically stable softmax implementation\n            scaled_b = b / current_tau\n            max_b = np.max(scaled_b, axis=1, keepdims=True)\n            exp_b = np.exp(scaled_b - max_b)\n            c = exp_b / np.sum(exp_b, axis=1, keepdims=True)\n\n            # 3. Calculate and store mean entropy\n            # c_ij * log(c_ij) -> 0 as c_ij -> 0. Add a small epsilon for safety,\n            # though softmax output is strictly positive with float precision.\n            entropies_i = -np.sum(c * np.log(c + 1e-12), axis=1)\n            mean_entropies.append(np.mean(entropies_i))\n\n            # 4. Calculate parent capsule pre-activations s_j\n            s = np.zeros((J, d))\n            for j in range(J):\n                s[j, :] = np.sum(c[:, j:j + 1] * u_hat[:, j, :], axis=0)\n\n            # 5. Calculate parent capsule outputs v_j (squash nonlinearity)\n            v = np.zeros((J, d))\n            for j in range(J):\n                s_norm_sq = np.sum(np.square(s[j]))\n                if s_norm_sq > 0:\n                    s_norm = np.sqrt(s_norm_sq)\n                    scale = s_norm_sq / (1.0 + s_norm_sq)\n                    v[j, :] = scale * s[j] / s_norm\n            \n            # 6. Update routing logits b_ij\n            agreement = np.einsum('ijd,jd->ij', u_hat, v)\n            b = b + agreement\n\n        # --- Check Constraints ---\n        early_ok = (mean_entropies[0] >= H_early_min) and \\\n                   (mean_entropies[1] >= H_early_min)\n        final_ok = (mean_entropies[T - 1] <= H_final_max)\n\n        return early_ok and final_ok\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ('A', (3.0, 0.5)),      # fast-decaying\n        ('B', (3.0,)),          # always high\n        ('C', (0.25,)),         # always low\n        ('D', (0.2, 3.0)),      # smooth decay\n        ('E', (3.0, 0.2, 0.60, 3)), # entropy-triggered\n    ]\n\n    results = []\n    for case_type, case_params in test_cases:\n        result = evaluate_schedule(case_type, case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}