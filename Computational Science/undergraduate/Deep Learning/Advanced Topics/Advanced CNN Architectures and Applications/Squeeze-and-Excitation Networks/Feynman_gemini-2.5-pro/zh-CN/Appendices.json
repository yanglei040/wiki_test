{
    "hands_on_practices": [
        {
            "introduction": "要真正理解Squeeze-and-Excitation (SE) 模块的工作原理，最好的方法莫过于从零开始亲手实现它。本练习将引导你基于其核心数学定义，逐步构建SE模块的三个关键部分：用于聚合全局信息的“Squeeze”（压缩）操作，学习通道间相互依赖关系的“Excitation”（激励）操作，以及最终执行特征重标定的“Scale”（缩放）操作。通过这个过程，你不仅能掌握其内部机制，还能学会量化其引入的参数开销，这是评估模型效率的重要一步。",
            "id": "3139403",
            "problem": "要求您实现一个通道级Squeeze-and-Excitation (SE) 模块，并从第一性原理出发量化其参数开销。SE模块作用于一个具有通道、高度和宽度轴的三维输入张量。您必须实现的操作仅使用线性映射和逐点非线性的核心定义在下文给出。\n\n给定一个输入张量 $x \\in \\mathbb{R}^{C \\times H \\times W}$，Squeeze（压缩）操作定义为通道级全局平均：\n$$\ns_c \\;=\\; \\frac{1}{H\\,W}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} x_{cij}, \\quad \\text{for } c \\in \\{0,\\dots,C-1\\}.\n$$\n将 $s_c$ 堆叠起来，得到描述符向量 $s \\in \\mathbb{R}^{C}$。Excitation（激励）操作定义为一个两层的多层感知机 (MLP)，其缩减率为 $r$，其中 $r$ 是一个能整除 $C$ 的正整数。令 $\\mathrm{ReLU}(u) = \\max\\{0,u\\}$ 表示修正线性单元（Rectified Linear Unit），$\\sigma(u)=\\frac{1}{1+e^{-u}}$ 表示逻辑S型函数（logistic sigmoid）。通过学习参数 $W_1 \\in \\mathbb{R}^{(C/r)\\times C}$、$b_1 \\in \\mathbb{R}^{C/r}$、$W_2 \\in \\mathbb{R}^{C \\times (C/r)}$ 和 $b_2 \\in \\mathbb{R}^{C}$，激励输出 $z \\in \\mathbb{R}^{C}$ 计算如下：\n$$\nt \\;=\\; \\mathrm{ReLU}(W_1 s + b_1), \\qquad z \\;=\\; \\sigma(W_2 t + b_2).\n$$\n最后，用 $z$ 对输入张量进行通道级缩放，\n$$\ny_{cij} \\;=\\; x_{cij}\\, z_c, \\quad \\text{for all } c,i,j,\n$$\n以获得经SE增强的张量 $y \\in \\mathbb{R}^{C \\times H \\times W}$。\n\n参数开销定义为相对于不含SE模块的基线模型，由激励MLP引入的额外参数数量。对于两个非共享权重的线性层，权重数量为：\n$$\n\\#\\text{weights}_{\\text{untied}} \\;=\\; C \\cdot \\frac{C}{r} \\;+\\; \\frac{C}{r} \\cdot C \\;=\\; \\frac{2C^2}{r},\n$$\n偏置数量为：\n$$\n\\#\\text{biases} \\;=\\; \\frac{C}{r} \\;+\\; C.\n$$\n在权重共享的变体中（即 $W_2 = W_1^{\\top}$），独立权重数量减少为：\n$$\n\\#\\text{weights}_{\\text{tied}} \\;=\\; C \\cdot \\frac{C}{r} \\;=\\; \\frac{C^2}{r}.\n$$\n\n请完全按照规定实现SE模块，并为每个测试用例计算以下输出：\n- 压缩描述符之和，$\\sum_{c=0}^{C-1} s_c$。\n- 缩放后输出的所有元素之和，$\\sum_{c,i,j} y_{cij}$，四舍五入到六位小数。\n- 非共享权重的开销 $\\#\\text{weights}_{\\text{untied}}$。\n- 共享权重的开销 $\\#\\text{weights}_{\\text{tied}}$。\n- 非共享权重加偏置的开销 $\\#\\text{weights}_{\\text{untied}} + \\#\\text{biases}$。\n\n您的程序必须使用以下测试套件。在所有情况下，$C$ 都能被 $r$ 整除，且以下所有定义都必须被精确实现。\n\n测试用例A（正常路径）：\n- $C = 4$, $H = 2$, $W = 2$, $r = 2$。\n- 输入由 $x_{cij} = c - i + j$ 定义，其中 $c \\in \\{0,1,2,3\\}$，$i \\in \\{0,1\\}$，$j \\in \\{0,1\\}$。\n- 参数：\n$$\nW_1 \\;=\\; \\begin{bmatrix}\n1  -1  0.5  0 \\\\\n0.25  0.5  -0.5  1\n\\end{bmatrix}, \\quad\nb_1 \\;=\\; \\begin{bmatrix} -0.5 \\\\ 0 \\end{bmatrix},\n$$\n$$\nW_2 \\;=\\; \\begin{bmatrix}\n1  0.5 \\\\\n-0.5  0.25 \\\\\n0  1 \\\\\n0.75  -1\n\\end{bmatrix}, \\quad\nb_2 \\;=\\; \\begin{bmatrix} 0 \\\\ 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}.\n$$\n\n测试用例B（空间维度的边界情况，$H W = 1$）：\n- $C = 8$, $H = 1$, $W = 1$, $r = 2$。\n- 输入由 $x_{c00} = c + 1$ 定义，其中 $c \\in \\{0,1,\\dots,7\\}$。\n- 参数由基于索引的公式逐元素定义。令 $a$ 为行索引，$b$ 为列索引，两者均从零开始：\n$$\nW_1[a,b] \\;=\\; \\frac{a\\cdot 8 + b - 12}{16}, \\quad \\text{for } a \\in \\{0,1,2,3\\}, \\; b \\in \\{0,1,\\dots,7\\},\n$$\n$$\nb_1 \\;=\\; \\begin{bmatrix} -0.25 \\\\ 0 \\\\ 0.1 \\\\ -0.1 \\end{bmatrix}, \\quad\nW_2[a,b] \\;=\\; \\frac{a\\cdot 4 + b - 8}{8}, \\quad \\text{for } a \\in \\{0,1,\\dots,7\\}, \\; b \\in \\{0,1,2,3\\},\n$$\n$$\nb_2 \\;=\\; \\mathbf{0} \\in \\mathbb{R}^{8}.\n$$\n\n测试用例C（边缘情况 $r = 1$ 且通道符号交替）：\n- $C = 6$, $H = 3$, $W = 1$, $r = 1$。\n- 输入由 $x_{ci0} = (-1)^c \\cdot (i+1)$ 定义，其中 $c \\in \\{0,1,\\dots,5\\}$ 且 $i \\in \\{0,1,2\\}$。\n- 参数：\n$$\nW_1 \\;=\\; 0.5\\, I_6 \\;-\\; 0.25\\,(\\mathbf{1}_6 \\mathbf{1}_6^{\\top} - I_6), \\quad b_1 \\;=\\; \\mathbf{0} \\in \\mathbb{R}^{6},\n$$\n$$\nW_2 \\;=\\; 0.3\\, I_6 \\;-\\; 0.05\\, \\mathbf{1}_6 \\mathbf{1}_6^{\\top}, \\quad b_2 \\;=\\; \\begin{bmatrix} -0.1 \\\\ -0.05 \\\\ 0 \\\\ 0.05 \\\\ 0.1 \\\\ 0.15 \\end{bmatrix},\n$$\n其中 $I_6$ 是 $6 \\times 6$ 的单位矩阵，$\\mathbf{1}_6$ 是长度为6的全1向量。\n\n此问题不涉及角度单位和物理单位。所有数值输出都应为标准十进制形式的实数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含所有三个测试用例的结果，格式为列表的列表。每个内部列表必须按以下顺序排列：\n$$\n\\left[ \\sum_{c} s_c,\\; \\mathrm{round}\\!\\left(\\sum_{c,i,j} y_{cij},\\,6\\right),\\; \\#\\text{weights}_{\\text{untied}},\\; \\#\\text{weights}_{\\text{tied}},\\; \\#\\text{weights}_{\\text{untied}} + \\#\\text{biases} \\right].\n$$\n例如，打印的结构必须如下所示：\n$$\n\\big[\\,[\\dots],\\,[\\dots],\\,[\\dots]\\,\\big].\n$$",
            "solution": "该问题要求基于其基本数学定义，实现并分析Squeeze-and-Excitation (SE) 模块，这是深度学习中一种常见的架构组件。解决方案包括对三个不同的测试用例，逐步执行所定义的操作。其核心原理是通过数据驱动机制进行通道级特征重校准。\n\nSE模块作用于一个输入张量 $x \\in \\mathbb{R}^{C \\times H \\times W}$，其中 $C$ 是通道数，$H$ 和 $W$ 分别是空间高度和宽度。该过程包括三个主要阶段：Squeeze（压缩）、Excitation（激励）和Scale（缩放）。\n\n**1. Squeeze（压缩）：全局信息嵌入**\n第一步，“Squeeze”（压缩），将特征图沿其空间维度（$H \\times W$）进行聚合，以生成一个通道描述符。这是通过全局平均池化实现的，该操作计算每个通道的平均值。得到的向量 $s \\in \\mathbb{R}^{C}$ 为每个通道嵌入了全局感受野。$s$ 的第 $c$ 个元素的公式为：\n$$\ns_c = \\frac{1}{H \\cdot W}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} x_{cij}\n$$\n在实现中，这对应于沿输入张量 $x$ 的空间轴（轴1和轴2）取平均值。每个测试用例的第一个要求输出是这个压缩向量的元素之和，即 $\\sum_{c=0}^{C-1} s_c$。\n\n**2. Excitation（激励）：自适应重校准**\n“Excitation”（激励）阶段从压缩描述符 $s$ 生成一组逐通道的调制权重，或称为“激活值”。这是通过一个小型神经网络，具体来说是一个两层的多层感知机 (MLP) 来完成的。该MLP的结构是先降低通道维度然后再恢复，从而创建一个计算瓶颈，有助于捕获非线性的通道间相互依赖关系。\n\n该MLP由以下部分组成：\n- 一个降维线性层，其权重为 $W_1 \\in \\mathbb{R}^{(C/r)\\times C}$，偏置为 $b_1 \\in \\mathbb{R}^{C/r}$，其中 $r$ 是缩减率。\n- 一个修正线性单元（$\\mathrm{ReLU}$）激活函数，定义为 $\\mathrm{ReLU}(u) = \\max\\{0,u\\}$。\n- 一个升维线性层，其权重为 $W_2 \\in \\mathbb{R}^{C \\times (C/r)}$，偏置为 $b_2 \\in \\mathbb{R}^{C}$。\n- 一个逻辑S型激活函数，$\\sigma(u)=\\frac{1}{1+e^{-u}}$，它将输出归一化到 $(0, 1)$ 范围内。\n\n计算过程如下：\n$$\nt = \\mathrm{ReLU}(W_1 s + b_1)\n$$\n$$\nz = \\sigma(W_2 t + b_2)\n$$\n得到的向量 $z \\in \\mathbb{R}^{C}$ 包含通道级的缩放因子。\n\n**3. Scale（缩放）：特征重校准**\n最后阶段将从激励步骤中学到的缩放因子应用于原始输入张量 $x$。输入张量的每个通道都乘以向量 $z$ 中对应的缩放因子。这将重校准特征图，放大信息丰富的通道并抑制作用较小的通道。输出张量 $y \\in \\mathbb{R}^{C \\times H \\times W}$ 由下式给出：\n$$\ny_{cij} = x_{cij} \\cdot z_c\n$$\n在实现中，这是通过将缩放向量 $z$（重塑为维度 $C \\times 1 \\times 1$）广播到输入张量 $x$上来实现的。第二个要求的输出是最终缩放后张量中所有元素的和，即 $\\sum_{c,i,j} y_{cij}$，其计算方式为 $\\sum_c (z_c \\cdot \\sum_{i,j} x_{cij}) = H \\cdot W \\cdot \\sum_c (z_c \\cdot s_c)$。\n\n**4. 参数开销计算**\n该问题还要求量化SE模块的MLP所引入的额外可学习参数的数量。这些参数包括两个线性层的权重和偏置。\n- **非共享权重：** 当 $W_1$ 和 $W_2$ 独立时，总权重数量是两个矩阵中元素的总和：$(\\frac{C}{r} \\times C) + (C \\times \\frac{C}{r}) = \\frac{2C^2}{r}$。\n- **共享权重：** 在一个 $W_2 = W_1^{\\top}$ 的变体中，独立权重数量减少为 $W_1$ 的大小，即 $\\frac{C^2}{r}$。\n- **偏置：** 偏置参数的数量是 $b_1$ 和 $b_2$ 中元素的总和，即 $\\frac{C}{r} + C$。\n- **总非共享开销：** 非共享权重情况下的总参数数量是共享权重和偏置的总和：$\\frac{2C^2}{r} + \\frac{C}{r} + C$。\n\n实现部分将使用指定的输入张量和MLP参数，为所提供的三个测试用例中的每一个计算这五个量。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and format results for all test cases.\n    \"\"\"\n\n    def se_block_and_overhead(C, H, W, r, x, W1, b1, W2, b2):\n        \"\"\"\n        Implements the SE block operations and calculates parameter overhead.\n\n        Args:\n            C (int): Number of channels.\n            H (int): Height of the input tensor.\n            W (int): Width of the input tensor.\n            r (int): Reduction ratio.\n            x (np.ndarray): Input tensor of shape (C, H, W).\n            W1 (np.ndarray): Weights of the first linear layer.\n            b1 (np.ndarray): Biases of the first linear layer.\n            W2 (np.ndarray): Weights of the second linear layer.\n            b2 (np.ndarray): Biases of the second linear layer.\n\n        Returns:\n            list: A list containing the five required output values.\n        \"\"\"\n        # 1. Squeeze Operation: Global Average Pooling\n        # s_c = (1/(H*W)) * sum(x_cij for i,j)\n        s = np.mean(x, axis=(1, 2))\n        sum_s = np.sum(s)\n\n        # 2. Excitation Operation: MLP\n        # t = ReLU(W1*s + b1)\n        t = np.maximum(0, W1 @ s + b1)\n        \n        # z = sigmoid(W2*t + b2)\n        z_raw = W2 @ t + b2\n        z = 1 / (1 + np.exp(-z_raw))\n\n        # 3. Scale Operation\n        # y_cij = x_cij * z_c\n        # Reshape z to (C, 1, 1) for broadcasting\n        y = x * z.reshape((C, 1, 1))\n        sum_y = np.sum(y)\n\n        # 4. Parameter Overhead Calculation\n        weights_untied = (2 * C**2) // r\n        weights_tied = (C**2) // r\n        biases = (C // r) + C\n        total_untied_with_biases = weights_untied + biases\n        \n        return [\n            sum_s,\n            round(sum_y, 6),\n            weights_untied,\n            weights_tied,\n            total_untied_with_biases\n        ]\n\n    results = []\n\n    # --- Test Case A ---\n    C_A, H_A, W_A, r_A = 4, 2, 2, 2\n    x_A = np.fromfunction(lambda c, i, j: c - i + j, (C_A, H_A, W_A))\n    W1_A = np.array([\n        [1, -1, 0.5, 0],\n        [0.25, 0.5, -0.5, 1]\n    ])\n    b1_A = np.array([-0.5, 0])\n    W2_A = np.array([\n        [1, 0.5],\n        [-0.5, 0.25],\n        [0, 1],\n        [0.75, -1]\n    ])\n    b2_A = np.array([0, 0.1, -0.2, 0.3])\n    results.append(se_block_and_overhead(C_A, H_A, W_A, r_A, x_A, W1_A, b1_A, W2_A, b2_A))\n\n    # --- Test Case B ---\n    C_B, H_B, W_B, r_B = 8, 1, 1, 2\n    x_B = (np.arange(C_B) + 1).reshape(C_B, H_B, W_B)\n    W1_B = np.fromfunction(lambda a, b: (a * 8 + b - 12) / 16, (C_B // r_B, C_B))\n    b1_B = np.array([-0.25, 0, 0.1, -0.1])\n    W2_B = np.fromfunction(lambda a, b: (a * 4 + b - 8) / 8, (C_B, C_B // r_B))\n    b2_B = np.zeros(C_B)\n    results.append(se_block_and_overhead(C_B, H_B, W_B, r_B, x_B, W1_B, b1_B, W2_B, b2_B))\n\n    # --- Test Case C ---\n    C_C, H_C, W_C, r_C = 6, 3, 1, 1\n    c_vec_C = (-1)**np.arange(C_C)\n    i_vec_C = np.arange(1, H_C + 1)\n    x_C = (c_vec_C[:, np.newaxis, np.newaxis]\n           * i_vec_C[np.newaxis, :, np.newaxis])\n    \n    I6 = np.identity(C_C)\n    J6 = np.ones((C_C, C_C))\n    \n    W1_C = 0.5 * I6 - 0.25 * (J6 - I6)\n    b1_C = np.zeros(C_C)\n    W2_C = 0.3 * I6 - 0.05 * J6\n    b2_C = np.array([-0.1, -0.05, 0, 0.05, 0.1, 0.15])\n    results.append(se_block_and_overhead(C_C, H_C, W_C, r_C, x_C, W1_C, b1_C, W2_C, b2_C))\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to a string and join them with commas.\n    # The outer brackets are added by the f-string.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了SE模块的基本构造后，我们可以深入探索其核心的设计选择。标准的SE模块使用Sigmoid函数来生成门控信号，这允许每个通道的权重被独立地缩放。本练习将探讨一种替代方案：使用Softmax函数，它引入了一种竞争机制，即所有通道必须在一个固定的“注意力预算”内争夺重要性。通过对比这两种机制，你将能更深刻地理解门控激活函数的选择对模型行为产生的细微而重要的影响。",
            "id": "3175710",
            "problem": "您必须在卷积特征图中实现两种用于“挤压-激励”（squeeze-and-excitation）的通道注意力机制，并计算能揭示其行为差异的定量指标。这两种机制分别是：通过 softmax 映射位于概率单纯形中的约束挤压-激励门，以及通过 logistic sigmoid 映射的独立逐元素门。该任务必须通过编写一个独立的程序来完成，该程序为给定的测试套件生成指定的输出。\n\n从以下基本定义开始：\n- 一个卷积张量表示为 $X \\in \\mathbb{R}^{C \\times H \\times W}$，其中 $C$ 是通道数，$H$ 是高度，$W$ 是宽度。\n- 全局平均池化描述符为 $s \\in \\mathbb{R}^{C}$，其分量为 $s_c = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} X_{c, i, j}$。\n- 挤压网络是一个两层变换，其缩减率为 $r = \\max(1, \\lfloor C/2 \\rfloor)$，使用修正线性单元映射 $\\mathrm{ReLU}(x) = \\max(0, x)$，并具有如下定义的确定性权重。令 $z \\in \\mathbb{R}^{C}$ 为门前激活值：\n$$\nz = W_2 \\, \\mathrm{ReLU}(W_1 s + b_1) + b_2,\n$$\n其中 $W_1 \\in \\mathbb{R}^{r \\times C}$，$b_1 \\in \\mathbb{R}^{r}$，$W_2 \\in \\mathbb{R}^{C \\times r}$，以及 $b_2 \\in \\mathbb{R}^{C}$。\n\n使用基本三角函数定义确定性权重，以使程序无需训练即可复现：\n- 对于 $i \\in \\{1, \\dots, r\\}$ 和 $j \\in \\{1, \\dots, C\\}$，\n$$\n(W_1)_{i, j} = 0.1 \\, \\sin\\big((i)(j)\\big), \\quad (W_2)_{j, i} = 0.1 \\, \\cos\\big((j)(i)\\big).\n$$\n- 对于 $i \\in \\{1, \\dots, r\\}$ 和 $j \\in \\{1, \\dots, C\\}$，\n$$\n(b_1)_i = 0.05 \\, \\cos(i), \\quad (b_2)_j = -0.02 \\, \\sin(j).\n$$\n\n构建两种通道门：\n- 约束门 $g^{\\mathrm{soft}} \\in \\mathbb{R}^{C}$ 通过将 softmax 映射应用于 $z$ 计算得出：\n$$\ng^{\\mathrm{soft}}_c = \\frac{\\exp(z_c)}{\\sum_{k=1}^{C} \\exp(z_k)}, \\quad \\text{因此 } \\sum_{c=1}^{C} g^{\\mathrm{soft}}_c = 1 \\text{ 且 } g^{\\mathrm{soft}}_c > 0.\n$$\n- 独立门 $g^{\\mathrm{sig}} \\in \\mathbb{R}^{C}$ 通过将 logistic sigmoid 映射逐元素应用于 $z$ 计算得出：\n$$\ng^{\\mathrm{sig}}_c = \\frac{1}{1 + \\exp(-z_c)}, \\quad \\text{因此对于每个 } c \\text{，} 0 < g^{\\mathrm{sig}}_c < 1 \\text{ 独立成立}.\n$$\n\n将门逐通道应用于原始特征图：\n- 重加权张量为 $Y^{\\mathrm{soft}} \\in \\mathbb{R}^{C \\times H \\times W}$ 和 $Y^{\\mathrm{sig}} \\in \\mathbb{R}^{C \\times H \\times W}$，其定义为\n$$\nY^{\\mathrm{soft}}_{c, i, j} = g^{\\mathrm{soft}}_c \\, X_{c, i, j}, \\quad Y^{\\mathrm{sig}}_{c, i, j} = g^{\\mathrm{sig}}_c \\, X_{c, i, j}.\n$$\n\n对于每个测试用例，计算以下指标：\n- $M_1$：一个布尔值，如果 $\\left|\\sum_{c=1}^{C} g^{\\mathrm{soft}}_c - 1\\right| \\leq \\varepsilon$（容差 $\\varepsilon = 10^{-12}$）成立，则为 $\\mathrm{True}$，否则为 $\\mathrm{False}$。\n- $M_2$：$g^{\\mathrm{sig}}$ 的 $\\ell_1$ 范数，等于 $\\sum_{c=1}^{C} g^{\\mathrm{sig}}_c$。\n- $M_3$：约束门的强调比率，定义为 $\\frac{\\max_c g^{\\mathrm{soft}}_c}{\\frac{1}{C}\\sum_{c=1}^{C} g^{\\mathrm{soft}}_c}$。\n- $M_4$：独立门的强调比率，定义为 $\\frac{\\max_c g^{\\mathrm{sig}}_c}{\\frac{1}{C}\\sum_{c=1}^{C} g^{\\mathrm{sig}}_c}$。\n- $M_5$：约束门的方差，$\\mathrm{Var}(g^{\\mathrm{soft}})$。\n- $M_6$：独立门的方差，$\\mathrm{Var}(g^{\\mathrm{sig}})$。\n- $M_7$：约束门的能量缩放比率，定义为 $\\frac{\\|Y^{\\mathrm{soft}}\\|_1}{\\|X\\|_1}$，其中 $\\|T\\|_1 = \\sum_{c=1}^{C}\\sum_{i=1}^{H}\\sum_{j=1}^{W} |T_{c, i, j}|$。如果 $\\|X\\|_1 = 0$，则该比率定义为 $0$。\n- $M_8$：独立门的能量缩放比率，定义为 $\\frac{\\|Y^{\\mathrm{sig}}\\|_1}{\\|X\\|_1}$，当 $\\|X\\|_1 = 0$ 时同样遵循定义为 $0$ 的约定。\n\n为以下测试套件实现上述计算。每个测试用例都明确指定了 $(C, H, W)$ 和张量 $X$。\n\n测试用例 1 (通用混合值):\n- $C = 4$, $H = 2$, $W = 3$。\n- $X$ 通道：\n$$\nX_{0,:,:} = \\begin{bmatrix} 0.5  0.2  -0.1 \\\\ 0.3  -0.2  0.0 \\end{bmatrix}, \\quad\nX_{1,:,:} = \\begin{bmatrix} -0.4  0.1  0.2 \\\\ -0.1  -0.05  0.3 \\end{bmatrix},\n$$\n$$\nX_{2,:,:} = \\begin{bmatrix} 1.2  0.8  0.5 \\\\ 0.4  0.0  -0.3 \\end{bmatrix}, \\quad\nX_{3,:,:} = \\begin{bmatrix} 0.0  -0.1  -0.2 \\\\ 0.3  0.1  0.0 \\end{bmatrix}.\n$$\n\n测试用例 2 (单通道边界情况):\n- $C = 1$, $H = 3$, $W = 3$。\n- $X$ 通道：\n$$\nX_{0,:,:} = \\begin{bmatrix} 0.1  0.2  0.3 \\\\ 0.4  0.5  0.6 \\\\ 0.7  0.8  0.9 \\end{bmatrix}.\n$$\n\n测试用例 3 (零张量边缘情况):\n- $C = 3$, $H = 2$, $W = 2$。\n- $X$ 通道：\n$$\nX_{0,:,:} = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}, \\quad\nX_{1,:,:} = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}, \\quad\nX_{2,:,:} = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}.\n$$\n\n测试用例 4 (主导通道和混合符号):\n- $C = 5$, $H = 2$, $W = 2$。\n- $X$ 通道：\n$$\nX_{0,:,:} = \\begin{bmatrix} 10.0  9.0 \\\\ 8.0  7.0 \\end{bmatrix}, \\quad\nX_{1,:,:} = \\begin{bmatrix} -5.0  -4.0 \\\\ -3.0  -2.0 \\end{bmatrix},\n$$\n$$\nX_{2,:,:} = \\begin{bmatrix} 0.1  -0.1 \\\\ 0.05  -0.05 \\end{bmatrix}, \\quad\nX_{3,:,:} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  -1.0 \\end{bmatrix}, \\quad\nX_{4,:,:} = \\begin{bmatrix} 0.3  0.2 \\\\ 0.1  0.0 \\end{bmatrix}.\n$$\n\n您的程序必须为每个测试用例计算 $(M_1, M_2, M_3, M_4, M_5, M_6, M_7, M_8)$，并生成一行输出，其中包含所有测试用例展平后的结果，格式为方括号内以逗号分隔的列表，例如 $[\\text{结果}_1,\\text{结果}_2,\\dots,\\text{结果}_{32}]$。不涉及物理单位或角度，因此不需要。输出必须是布尔值或浮点数类型，并且必须完全按照定义进行计算。",
            "solution": "该问题要求实现并对比分析两种源自 Squeeze-and-Excitation (SE) 架构的不同通道注意力机制。第一种机制是约束门 $g^{\\mathrm{soft}}$，它使用 softmax 函数将总计为 $1$ 的注意力预算分配到各个通道，从而强制产生竞争。第二种是独立门 $g^{\\mathrm{sig}}$，它使用逐元素的 logistic sigmoid 函数，允许每个通道的注意力权重独立于其他通道确定。该过程是确定性的，所有网络权重和偏置都由三角函数定义。对于每个输入张量 $X$，我们计算一组八个指标（$M_1$ 到 $M_8$），以定量评估这两种门控策略之间的行为差异。\n\n该解决方案通过对每个测试用例遵循一系列定义明确的计算步骤来实现。\n\n**步骤 1：全局平均池化**\n该过程从“挤压”（squeeze）操作开始，该操作将每个通道在高度 $H$ 和宽度 $W$ 上的空间信息聚合成一个单一的描述符值。对于输入张量 $X \\in \\mathbb{R}^{C \\times H \\times W}$，我们计算一个全局平均池化描述符向量 $s \\in \\mathbb{R}^{C}$。该向量的每个分量 $s_c$ 代表第 $c$ 个通道的平均激活值：\n$$\ns_c = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} X_{c, i, j} \\quad \\text{for } c \\in \\{1, \\dots, C\\}.\n$$\n\n**步骤 2：挤压网络变换**\n描述符向量 $s$ 随后被一个两层全连接网络（即“激励”（excitation）块）处理，以生成特定于通道的门前激活值。该网络由一个降维层和一个升维层组成。\n\n首先，缩减率 $r$ 由通道数 $C$ 决定：\n$$\nr = \\max(1, \\lfloor C/2 \\rfloor).\n$$\n网络权重 $W_1 \\in \\mathbb{R}^{r \\times C}$，$b_1 \\in \\mathbb{R}^{r}$，$W_2 \\in \\mathbb{R}^{C \\times r}$ 和 $b_2 \\in \\mathbb{R}^{C}$ 是确定性定义的。按照规定，对矩阵和向量使用 1-based 索引：\n- 对于 $i \\in \\{1, \\dots, r\\}$ 和 $j \\in \\{1, \\dots, C\\}$：\n  $$\n  (W_1)_{i, j} = 0.1 \\, \\sin(ij), \\quad (W_2)_{j, i} = 0.1 \\, \\cos(ji).\n  $$\n- 对于 $i \\in \\{1, \\dots, r\\}$ 和 $j \\in \\{1, \\dots, C\\}$：\n  $$\n  (b_1)_i = 0.05 \\, \\cos(i), \\quad (b_2)_j = -0.02 \\, \\sin(j).\n  $$\n然后，使用修正线性单元（$\\mathrm{ReLU}$）激活函数（定义为 $\\mathrm{ReLU}(x) = \\max(0, x)$）计算门前激活向量 $z \\in \\mathbb{R}^{C}$：\n$$\nz = W_2 \\, \\mathrm{ReLU}(W_1 s + b_1) + b_2.\n$$\n\n**步骤 3：门生成**\n根据门前激活向量 $z$，我们计算两种类型的通道门：\n\n- **约束门 ($g^{\\mathrm{soft}}$)**：将 softmax 函数应用于 $z$ 以生成一个门向量，其分量之和为 $1$ 且均为正数。这创造了一种竞争动态，即增加一个通道的权重必然会减少其他通道的权重。\n$$\ng^{\\mathrm{soft}}_c = \\frac{\\exp(z_c)}{\\sum_{k=1}^{C} \\exp(z_k)}.\n$$\n- **独立门 ($g^{\\mathrm{sig}}$)**：将 logistic sigmoid 函数逐元素应用于 $z$。每个门权重 $g^{\\mathrm{sig}}_c$ 都被独立地映射到 $(0, 1)$ 区间。\n$$\ng^{\\mathrm{sig}}_c = \\frac{1}{1 + \\exp(-z_c)}.\n$$\n\n**步骤 4：通道重加权与指标计算**\n计算出的门用于重新缩放原始输入张量 $X$ 的通道。重加权后的张量为 $Y^{\\mathrm{soft}}$ 和 $Y^{\\mathrm{sig}}$：\n$$\nY^{\\mathrm{soft}}_{c, i, j} = g^{\\mathrm{soft}}_c \\, X_{c, i, j}, \\quad Y^{\\mathrm{sig}}_{c, i, j} = g^{\\mathrm{sig}}_c \\, X_{c, i, j}.\n$$\n最后，计算八个解析指标：\n\n- $M_1$：对 softmax 函数基本属性的布尔检查，确保 $\\sum_{c=1}^{C} g^{\\mathrm{soft}}_c$ 在数值上接近 $1$。条件为 $|\\sum_{c=1}^{C} g^{\\mathrm{soft}}_c - 1| \\leq \\varepsilon$，容差 $\\varepsilon = 10^{-12}$。\n\n- $M_2$：独立门的 $\\ell_1$ 范数，即 $\\sum_{c=1}^{C} g^{\\mathrm{sig}}_c$。该指标衡量由 sigmoid 门分配的总“注意力能量”，其不受固定预算的约束。\n\n- $M_3$：约束门的强调比率，即 $\\frac{\\max_c g^{\\mathrm{soft}}_c}{\\frac{1}{C}\\sum_{c=1}^{C} g^{\\mathrm{soft}}_c}$。由于 $g^{\\mathrm{soft}}_c$ 的总和为 $1$，其平均值为 $1/C$，因此该比率可简化为 $C \\cdot \\max_c g^{\\mathrm{soft}}_c$。它衡量最重要的通道所获得的注意力相对于平均水平高出多少。更高的值表示一个更“峰值化”或更具选择性的注意力分布。\n\n- $M_4$：独立门的强调比率，即 $\\frac{\\max_c g^{\\mathrm{sig}}_c}{\\frac{1}{C}\\sum_{c=1}^{C} g^{\\mathrm{sig}}_c}$。该指标衡量 sigmoid 门的选择性这一相同概念。\n\n- $M_5$：约束门的方差，即 $\\mathrm{Var}(g^{\\mathrm{soft}})$，衡量注意力权重的分散程度。较高的方差表明注意力在各通道间的分布更不均匀。\n\n- $M_6$：独立门的方差，即 $\\mathrm{Var}(g^{\\mathrm{sig}})$。\n\n- $M_7$：约束门的能量缩放比率，即 $\\frac{\\|Y^{\\mathrm{soft}}\\|_1}{\\|X\\|_1}$，其中 $\\|T\\|_1 = \\sum_{c,i,j} |T_{c,i,j}|$。由于 $g^{\\mathrm{soft}}_c > 0$，该式可简化为 $\\frac{\\sum_c g^{\\mathrm{soft}}_c \\|X_c\\|_1}{\\sum_c \\|X_c\\|_1}$，这是由注意力分数加权的逐通道 $\\ell_1$ 范数的加权平均值。它量化了门对张量量级的整体缩放效应。如果 $\\|X\\|_1=0$，则定义为 $0$。\n\n- $M_8$：独立门的能量缩放比率，即 $\\frac{\\|Y^{\\mathrm{sig}}\\|_1}{\\|X\\|_1}$，计算方法与 $M_7$ 类似。\n\n将此完整流程应用于每个测试用例，以生成所需的数值结果。",
            "answer": "```python\nimport numpy as np\n\ndef compute_metrics(C, H, W, X):\n    \"\"\"\n    Computes squeeze-and-excitation gates and derived metrics for a given tensor.\n    \"\"\"\n    # Step 1: Global Average Pooling\n    s = np.mean(X, axis=(1, 2)) if H * W > 0 else np.zeros(C)\n\n    # Step 2: Squeeze Network\n    r = max(1, C // 2)\n\n    # Generate deterministic weights (using 1-based indexing from problem)\n    i_vec_r = np.arange(1, r + 1, dtype=float).reshape(-1, 1)\n    j_vec_C = np.arange(1, C + 1, dtype=float).reshape(1, -1)\n\n    W1 = 0.1 * np.sin(i_vec_r * j_vec_C)\n    b1 = 0.05 * np.cos(np.arange(1, r + 1, dtype=float))\n\n    j_vec_C_T = j_vec_C.T\n    i_vec_r_T = i_vec_r.T\n    W2 = 0.1 * np.cos(j_vec_C_T * i_vec_r_T)\n    b2 = -0.02 * np.sin(np.arange(1, C + 1, dtype=float))\n    \n    # Compute pre-gate activation z\n    z = W2 @ np.maximum(0, W1 @ s + b1) + b2\n\n    # Step 3: Gate Generation\n    # Constrained gate (softmax) with numerical stability\n    if z.size > 0:\n        stable_z = z - np.max(z)\n        exp_z = np.exp(stable_z)\n        g_soft = exp_z / np.sum(exp_z)\n    else: # Edge case for C=0, though not in tests\n        g_soft = np.array([])\n\n    # Independent gate (sigmoid)\n    g_sig = 1 / (1 + np.exp(-z))\n\n    # Step 4: Metric Computations\n    # M1: Softmax sum-to-one check\n    M1 = np.abs(np.sum(g_soft) - 1.0) = 1e-12 if g_soft.size > 0 else True\n    \n    # M2: Sigmoid l1-norm\n    M2 = np.sum(g_sig)\n    \n    # M3, M4: Emphasis Ratios\n    # Handle C=1 case where mean=max\n    mean_g_soft = np.mean(g_soft) if g_soft.size > 0 else 0.0\n    M3 = np.max(g_soft) / mean_g_soft if mean_g_soft > 0 else 1.0\n    \n    mean_g_sig = np.mean(g_sig) if g_sig.size > 0 else 0.0\n    M4 = np.max(g_sig) / mean_g_sig if mean_g_sig > 0 else 1.0\n\n    # M5, M6: Variances\n    M5 = np.var(g_soft)\n    M6 = np.var(g_sig)\n\n    # M7, M8: Energy Scaling Ratios\n    l1_norm_X = np.sum(np.abs(X))\n    if l1_norm_X == 0:\n        M7 = 0.0\n        M8 = 0.0\n    else:\n        channel_l1_norms = np.sum(np.abs(X), axis=(1, 2))\n        M7 = np.sum(g_soft * channel_l1_norms) / l1_norm_X\n        M8 = np.sum(g_sig * channel_l1_norms) / l1_norm_X\n\n    return (M1, M2, M3, M4, M5, M6, M7, M8)\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run computations, and print results.\n    \"\"\"\n    test_cases = [\n        (4, 2, 3, np.array([\n            [[0.5, 0.2, -0.1], [0.3, -0.2, 0.0]],\n            [[-0.4, 0.1, 0.2], [-0.1, -0.05, 0.3]],\n            [[1.2, 0.8, 0.5], [0.4, 0.0, -0.3]],\n            [[0.0, -0.1, -0.2], [0.3, 0.1, 0.0]]\n        ])),\n        (1, 3, 3, np.array([\n            [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]]\n        ])),\n        (3, 2, 2, np.zeros((3, 2, 2))),\n        (5, 2, 2, np.array([\n            [[10.0, 9.0], [8.0, 7.0]],\n            [[-5.0, -4.0], [-3.0, -2.0]],\n            [[0.1, -0.1], [0.05, -0.05]],\n            [[1.0, 0.0], [0.0, -1.0]],\n            [[0.3, 0.2], [0.1, 0.0]]\n        ]))\n    ]\n\n    all_results = []\n    for C, H, W, X in test_cases:\n        metrics = compute_metrics(C, H, W, X)\n        all_results.extend(metrics)\n    \n    # Format according to spec: [val1,val2,...]\n    # Python's str(True) is 'True', which is the required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后的这个实践将我们从组件设计带入一个更系统化的分析层面。我们将构建一个合成的对抗性环境，其中部分“干扰”（clutter）通道被特意设计用来误导模型。你的挑战是训练SE模块，使其学会在这种环境下识别并抑制这些对抗性通道，同时利用谱范数（spectral norm）正则化来约束模型复杂度，以找到一个更鲁棒的解决方案。这个练习生动地展示了SE模块在实际应用中滤除无关或有害信息的强大能力。",
            "id": "3175797",
            "problem": "要求您在对抗性生成的杂波通道上，实现并研究一种简化的、带有谱范数正则化的 Squeeze-and-Excitation (SE) 机制。所有工作均以向量形式进行，处理逐通道特征，并假设为二元分类问题。\n\n使用的基本原理：\n- 带有交叉熵损失的二元逻辑斯蒂模型：对于一个 logit $a$，预测值为 $\\hat{y} = \\sigma(a)$，其中 $\\sigma(\\cdot)$ 是逻辑斯蒂 sigmoid 函数，损失为 $L = -y \\log(\\hat{y}) - (1-y)\\log(1-\\hat{y})$。损失相对于 logit 的导数为 $\\frac{\\partial L}{\\partial a} = \\hat{y} - y$。\n- 通过一个带有修正线性单元 (ReLU) 和 sigmoid 激活函数的双层瓶颈结构来计算逐通道的 SE 门控。\n- 矩阵 $W$ 的谱范数定义为 $\\|W\\|_{2} = \\max_{\\|v\\|=1} \\|W v\\|$，它等于最大的奇异值。最大奇异值相对于 $W$ 的梯度由顶层左奇异向量和右奇异向量的外积给出。\n\n设置：\n- 共有 $C$ 个通道，其中 $C = 8$。一个输入样本是向量 $x \\in \\mathbb{R}^{C}$，表示通道激活值（空间维度被压缩；因此 squeeze 操作是恒等变换）。\n- SE 模块使用两个学习到的矩阵 $W_1 \\in \\mathbb{R}^{C/r \\times C}$ 和 $W_2 \\in \\mathbb{R}^{C \\times C/r}$ 从 $x$ 计算门控向量 $g \\in \\mathbb{R}^{C}$，其中缩减比 $r \\in \\{2,4\\}$。计算过程为：$z_1 = W_1 x$，$a_1 = \\max(0, z_1)$ (逐元素)，$z_2 = W_2 a_1$，以及 $g = \\sigma(z_2)$ (逐元素)。\n- 调制后的特征为 $x^{\\mathrm{tilde}} = g \\odot x$，其中 $\\odot$ 表示逐元素乘积。分类器是固定的，作为一个均匀聚合器 $a = \\sum_{i=1}^{C} x^{\\mathrm{tilde}}_i$，并且 $\\hat{y} = \\sigma(a)$。\n\n训练目标：\n- 最小化数据集上的平均交叉熵，加上对 $W_1$ 和 $W_2$ 的谱范数正则化惩罚项。总损失为\n$$\n\\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^{N} \\left( -y_n \\log(\\hat{y}_n) - (1 - y_n)\\log(1 - \\hat{y}_n) \\right) + \\lambda\\left(\\|W_1\\|_2 + \\|W_2\\|_2\\right),\n$$\n其中 $N$ 是样本数量，$y_n \\in \\{0,1\\}$，并且 $\\lambda  0$ 是谱正则化强度。\n\n带有对抗性杂波的数据集设计：\n- 设信号通道为索引 $\\{0,1\\}$，杂波通道为索引 $\\{2,3,4,5,6,7\\}$。对于每个样本 $n$：\n  1. 均匀抽取标签 $y_n \\in \\{0,1\\}$。\n  2. 将信号通道设置为 $x_{n,i} = s \\cdot \\mu + \\varepsilon_{n,i}$，对于 $i \\in \\{0,1\\}$，其中如果 $y_n = 1$ 则 $s = +1$，如果 $y_n = 0$ 则 $s = -1$；$\\mu \\ge 0$ 是信号幅度，$\\varepsilon_{n,i} \\sim \\mathcal{N}(0, \\sigma^2)$ 是标准差为 $\\sigma$ 的高斯噪声。\n  3. 计算 $S_n = \\sum_{i \\in \\{0,1\\}} x_{n,i}$ 并定义一个对抗性符号 $c_n = -\\operatorname{sign}(S_n)$（如果 $S_n \\neq 0$）；如果 $\\mu = 0$ 或 $S_n = 0$，则将 $c_n$ 设置为一个与 $y_n$ 无关的随机 $\\pm 1$。\n  4. 对于每个杂波通道 $j \\in \\{2,3,4,5,6,7\\}$，设置 $x_{n,j} = \\gamma \\cdot c_n \\cdot (\\mu + |\\eta_{n,j}|)$，其中 $\\eta_{n,j} \\sim \\mathcal{N}(0, \\sigma^2)$ 并且 $\\gamma  0$ 控制杂波的幅度。这样生成的杂波其符号与信号总和相反，且幅度较大。\n\n训练方法：\n- 仅对 $W_1$ 和 $W_2$ 使用批量梯度下降法；分类器按前述方式固定。通过链式法则推导梯度：从交叉熵导数 $\\frac{\\partial L}{\\partial a} = \\hat{y} - y$ 开始，经由 $x^{\\mathrm{tilde}} = g \\odot x$、$g = \\sigma(z_2)$、$z_2 = W_2 a_1$、$a_1 = \\max(0, z_1)$ 和 $z_1 = W_1 x$ 进行传播。对于谱惩罚项，通过幂迭代法近似顶层奇异向量，并将 $\\lambda \\, u_1 v_1^{\\top}$ 加到 $\\frac{\\partial \\mathcal{L}}{\\partial W_1}$ 上，将 $\\lambda \\, u_2 v_2^{\\top}$ 加到 $\\frac{\\partial \\mathcal{L}}{\\partial W_2}$ 上，其中 $u_k, v_k$ 是 $W_k$ 的顶层左、右奇异向量。\n- 用小的随机值初始化 $W_1$ 和 $W_2$。使用恒定的学习率和固定的 epoch 数。\n\n评估与决策规则：\n- 训练结束后，计算每个通道的平均门控值 $\\bar{g}_i = \\frac{1}{N} \\sum_{n=1}^{N} g_{n,i}$。令 $\\bar{g}_{\\mathrm{sig}}$ 为信号通道上 $\\bar{g}_i$ 的均值，$\\bar{g}_{\\mathrm{clutter}}$ 为杂波通道上 $\\bar{g}_i$ 的均值。\n- 如果 $\\bar{g}_{\\mathrm{sig}} - \\bar{g}_{\\mathrm{clutter}}  \\delta$，则宣告杂波已被抑制，其中容差 $\\delta = 0.1$（门控值为无量纲量）。\n\n不涉及角度和物理单位；所有量均为无量纲量。\n\n测试套件：\n在以下参数集上运行您的程序，每个参数集根据上述决策规则产生一个布尔值：\n- 案例 1：$C = 8$, $r = 2$, $N = 240$, $\\mu = 1.0$, $\\sigma = 0.3$, $\\gamma = 3.0$, $\\lambda = 0.1$, 随机种子 $0$。\n- 案例 2：$C = 8$, $r = 2$, $N = 240$, $\\mu = 1.0$, $\\sigma = 0.3$, $\\gamma = 10.0$, $\\lambda = 0.2$, 随机种子 $1$。\n- 案例 3：$C = 8$, $r = 4$, $N = 240$, $\\mu = 1.0$, $\\sigma = 0.5$, $\\gamma = 15.0$, $\\lambda = 0.5$, 随机种子 $2$。\n- 案例 4 (边界情况)：$C = 8$, $r = 2$, $N = 240$, $\\mu = 0.0$, $\\sigma = 0.5$, $\\gamma = 10.0$, $\\lambda = 0.2$, 随机种子 $3$。\n\n要求的最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[result1,result2,result3,result4]”），其中每个结果是对应测试用例的布尔值，指示 SE 模块是否根据规则 $\\bar{g}_{\\mathrm{sig}} - \\bar{g}_{\\mathrm{clutter}}  \\delta$ 在谱范数正则化下抑制了杂波。",
            "solution": "该问题要求实现一个带有谱范数正则化的Squeeze-and-Excitation (SE) 模块，并在一个特制的、含有对抗性杂波的数据集上进行训练。其目标是检验SE模块在学习识别并抑制误导性特征通道方面的能力。解决方案涉及以下几个关键步骤：\n\n**1. 模型与前向传播**\n模型由一个SE模块和一个固定的线性分类器组成。\n- **SE模块**：输入向量 $x \\in \\mathbb{R}^{C}$ 首先通过一个双层全连接网络计算门控向量 $g$。该网络包含一个带有ReLU激活的降维层（权重 $W_1$）和一个带有Sigmoid激活的升维层（权重 $W_2$）。计算过程为 $g = \\sigma(W_2 \\cdot \\text{ReLU}(W_1 x))$。\n- **特征调制**：通过将门控向量 $g$ 与原始输入 $x$ 进行逐元素相乘，得到调制后的特征 $x^{\\mathrm{tilde}} = g \\odot x$。\n- **分类器**：一个固定的聚合层对调制后的特征求和，得到logit $a = \\sum_i x^{\\mathrm{tilde}}_i$，最终预测值由 $\\hat{y} = \\sigma(a)$ 给出。\n\n**2. 损失函数与梯度下降**\n训练目标是最小化总损失 $\\mathcal{L}$，它由两部分组成：\n- **交叉熵损失**：在整个数据集上计算的平均二元交叉熵损失。\n- **谱范数正则化**：对权重矩阵 $W_1$ 和 $W_2$ 的谱范数（最大奇异值）施加的惩罚项，由正则化强度 $\\lambda$ 控制。\n$$\n\\mathcal{L} = \\text{Avg}(\\text{CrossEntropy}) + \\lambda\\left(\\|W_1\\|_2 + \\|W_2\\|_2\\right)\n$$\n使用批量梯度下降法更新权重。通过反向传播计算交叉熵损失的梯度。谱范数正则化项的梯度由其顶层左、右奇异向量的外积给出，即 $\\nabla \\|W\\|_2 = u v^\\top$。这些奇异向量通过幂迭代法进行数值近似。\n\n**3. 数据集生成与训练**\n根据问题描述生成一个合成数据集。其中，部分通道（信号通道）包含与标签相关的信号，而其余通道（杂波通道）则被设计为与信号总和的符号相反，旨在对抗性地误导分类器。\n在训练过程中，模型（即可学习的权重 $W_1$ 和 $W_2$）必须学会区分信号和杂波。一个成功的模型会为信号通道分配较高的门控值，同时为杂波通道分配较低的门控值，从而有效地将其从最终的分类决策中“静音”。\n\n**4. 评估**\n训练完成后，通过计算整个数据集上每个通道的平均门控值 $\\bar{g}_i$ 来评估模型的行为。比较信号通道的平均门控值 $\\bar{g}_{\\mathrm{sig}}$ 和杂波通道的平均门控值 $\\bar{g}_{\\mathrm{clutter}}$。如果它们的差值超过预设阈值 $\\delta$，则判定模型成功抑制了杂波。",
            "answer": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Computes the sigmoid function element-wise.\"\"\"\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef relu(z):\n    \"\"\"Computes the Rectified Linear Unit function element-wise.\"\"\"\n    return np.maximum(0, z)\n\ndef power_iteration(W, rng, num_iterations=20):\n    \"\"\"\n    Approximates top left and right singular vectors of a matrix W via power iteration.\n    \n    Args:\n        W (np.ndarray): The matrix.\n        rng (np.random.Generator): Random number generator for initialization.\n        num_iterations (int): Number of iterations to run.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: The top left singular vector u and top right singular vector v.\n    \"\"\"\n    m, n = W.shape\n    v = rng.normal(size=n)\n    v = v / np.linalg.norm(v)\n\n    for _ in range(num_iterations):\n        u_un = W @ v\n        if np.linalg.norm(u_un) == 0:\n            return np.zeros(m), np.zeros(n)\n        u = u_un / np.linalg.norm(u_un)\n        \n        v_un = W.T @ u\n        if np.linalg.norm(v_un) == 0:\n            return np.zeros(m), np.zeros(n)\n        v = v_un / np.linalg.norm(v_un)\n    \n    # Final normalization for u\n    u = W @ v\n    if np.linalg.norm(u) == 0:\n        return np.zeros(m), v\n    u = u / np.linalg.norm(u)\n\n    return u, v\n\ndef generate_data(N, C, mu, sigma, gamma, rng):\n    \"\"\"\n    Generates the dataset with adversarial clutter.\n    \"\"\"\n    X = np.zeros((N, C))\n    y = rng.integers(0, 2, size=N)\n\n    for n in range(N):\n        s = 1.0 if y[n] == 1 else -1.0\n        \n        # Signal channels\n        eps = rng.normal(0, sigma, size=2)\n        X[n, 0:2] = s * mu + eps\n        \n        S_n = np.sum(X[n, 0:2])\n        \n        # Adversarial sign\n        if mu == 0 or S_n == 0:\n            c_n = rng.choice([-1.0, 1.0])\n        else:\n            c_n = -np.sign(S_n)\n            \n        # Clutter channels\n        eta = rng.normal(0, sigma, size=C - 2)\n        X[n, 2:] = gamma * c_n * (mu + np.abs(eta))\n        \n    return X, y\n\ndef run_case(C, r, N, mu, sigma, gamma, lambda_reg, seed):\n    \"\"\"\n    Runs a single test case from the problem statement.\n    \"\"\"\n    # Hyperparameters\n    learning_rate = 0.01\n    num_epochs = 100\n    delta = 0.1\n    init_weight_scale = 0.1\n\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate Data\n    X, y = generate_data(N, C, mu, sigma, gamma, rng)\n\n    # 2. Initialize Weights\n    hidden_dim = C // r\n    W1 = rng.uniform(-init_weight_scale, init_weight_scale, size=(hidden_dim, C))\n    W2 = rng.uniform(-init_weight_scale, init_weight_scale, size=(C, hidden_dim))\n\n    # 3. Training Loop (Batch Gradient Descent)\n    y_reshaped = y.reshape(-1, 1)\n\n    for epoch in range(num_epochs):\n        # -- Forward Pass --\n        z1 = X @ W1.T\n        a1 = relu(z1)\n        z2 = a1 @ W2.T\n        g = sigmoid(z2)\n        x_tilde = g * X\n        a = np.sum(x_tilde, axis=1, keepdims=True)\n        y_hat = sigmoid(a)\n        \n        # -- Backward Pass --\n        # Gradient of cross-entropy loss\n        dL_da = (y_hat - y_reshaped) / N\n        dL_dx_tilde = dL_da\n        dL_dg = dL_dx_tilde * X\n        dL_dz2 = dL_dg * (g * (1 - g))\n        \n        grad_L_W2 = dL_dz2.T @ a1\n        \n        dL_da1 = dL_dz2 @ W2\n        d_relu_dz1 = (z1 > 0).astype(float)\n        dL_dz1 = dL_da1 * d_relu_dz1\n        \n        grad_L_W1 = dL_dz1.T @ X\n        \n        # Gradient of spectral norm regularization\n        u1, v1 = power_iteration(W1, rng)\n        grad_reg_W1 = lambda_reg * np.outer(u1, v1)\n        \n        u2, v2 = power_iteration(W2, rng)\n        grad_reg_W2 = lambda_reg * np.outer(u2, v2)\n\n        # Total gradient\n        grad_W1 = grad_L_W1 + grad_reg_W1\n        grad_W2 = grad_L_W2 + grad_reg_W2\n        \n        # -- Weight Update --\n        W1 -= learning_rate * grad_W1\n        W2 -= learning_rate * grad_W2\n\n    # 4. Evaluation\n    z1_final = X @ W1.T\n    a1_final = relu(z1_final)\n    z2_final = a1_final @ W2.T\n    g_final = sigmoid(z2_final)\n\n    g_bar = np.mean(g_final, axis=0)\n    g_bar_sig = np.mean(g_bar[0:2])\n    g_bar_clutter = np.mean(g_bar[2:])\n    \n    return g_bar_sig - g_bar_clutter > delta\n\ndef solve():\n    \"\"\"\n    Main function to define and run all test cases.\n    \"\"\"\n    test_cases = [\n        # C, r, N, mu, sigma, gamma, lambda, seed\n        (8, 2, 240, 1.0, 0.3, 3.0, 0.1, 0),\n        (8, 2, 240, 1.0, 0.3, 10.0, 0.2, 1),\n        (8, 4, 240, 1.0, 0.5, 15.0, 0.5, 2),\n        (8, 2, 240, 0.0, 0.5, 10.0, 0.2, 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        C, r, N, mu, sigma, gamma, lambda_reg, seed = case\n        result = run_case(C, r, N, mu, sigma, gamma, lambda_reg, seed)\n        results.append(str(result))\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        }
    ]
}