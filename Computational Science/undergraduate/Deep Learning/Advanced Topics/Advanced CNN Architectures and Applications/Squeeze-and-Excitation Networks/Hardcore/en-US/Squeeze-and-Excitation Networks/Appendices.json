{
    "hands_on_practices": [
        {
            "introduction": "To truly understand how Squeeze-and-Excitation networks enhance feature maps, we must first build one from the ground up. This exercise guides you through implementing the three core stages—Squeeze, Excitation, and Scale—directly from their mathematical definitions . By translating these principles into code, you will gain a concrete understanding of the mechanism and learn to quantify its computational cost in terms of parameter overhead.",
            "id": "3139403",
            "problem": "You are asked to implement a channel-wise Squeeze-and-Excitation (SE) block and to quantify its parameter overhead from first principles. The SE block operates on a three-dimensional input tensor with channel, height, and width axes. The operations you must implement are defined below using only core definitions of linear maps and pointwise nonlinearities.\n\nGiven an input tensor $x \\in \\mathbb{R}^{C \\times H \\times W}$, define the squeeze operation by the channel-wise global average\n$$\ns_c \\;=\\; \\frac{1}{H\\,W}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} x_{cij}, \\quad \\text{for } c \\in \\{0,\\dots,C-1\\}.\n$$\nStacking $s_c$ yields the descriptor vector $s \\in \\mathbb{R}^{C}$. Define the excitation as a two-layer Multilayer Perceptron (MLP) with reduction ratio $r$, where $r$ is a positive integer that divides $C$. Let $\\mathrm{ReLU}(u) = \\max\\{0,u\\}$ denote the Rectified Linear Unit and $\\sigma(u)=\\frac{1}{1+e^{-u}}$ denote the logistic sigmoid. With learned parameters $W_1 \\in \\mathbb{R}^{(C/r)\\times C}$, $b_1 \\in \\mathbb{R}^{C/r}$, $W_2 \\in \\mathbb{R}^{C \\times (C/r)}$, and $b_2 \\in \\mathbb{R}^{C}$, the excitation output $z \\in \\mathbb{R}^{C}$ is\n$$\nt \\;=\\; \\mathrm{ReLU}(W_1 s + b_1), \\qquad z \\;=\\; \\sigma(W_2 t + b_2).\n$$\nFinally, scale the input tensor channel-wise by $z$,\n$$\ny_{cij} \\;=\\; x_{cij}\\, z_c, \\quad \\text{for all } c,i,j,\n$$\nto obtain the SE-augmented tensor $y \\in \\mathbb{R}^{C \\times H \\times W}$.\n\nParameter overhead is defined as the number of additional parameters introduced by the excitation MLP relative to a baseline without the SE block. For two untied linear layers, the weight count is\n$$\n\\text{\\#weights}_{\\text{untied}} \\;=\\; C \\cdot \\frac{C}{r} \\;+\\; \\frac{C}{r} \\cdot C \\;=\\; \\frac{2C^2}{r},\n$$\nand the bias count is\n$$\n\\text{\\#biases} \\;=\\; \\frac{C}{r} \\;+\\; C.\n$$\nIn a tied-weights variant where $W_2 = W_1^{\\top}$, the independent weights count reduces to\n$$\n\\text{\\#weights}_{\\text{tied}} \\;=\\; C \\cdot \\frac{C}{r} \\;=\\; \\frac{C^2}{r}.\n$$\n\nImplement the SE block exactly as specified and compute the following outputs for each test case:\n- The sum of the squeezed descriptor, $\\sum_{c=0}^{C-1} s_c$.\n- The sum of all elements of the scaled output, $\\sum_{c,i,j} y_{cij}$, rounded to six decimal places.\n- The untied-weights overhead $\\text{\\#weights}_{\\text{untied}}$.\n- The tied-weights overhead $\\text{\\#weights}_{\\text{tied}}$.\n- The untied-weights-plus-biases overhead $\\text{\\#weights}_{\\text{untied}} + \\text{\\#biases}$.\n\nYour program must use the following test suite. In all cases, $C$ is divisible by $r$ and all definitions below must be implemented exactly.\n\nTest case A (happy path):\n- $C = 4$, $H = 2$, $W = 2$, $r = 2$.\n- Input defined by $x_{cij} = c - i + j$ for $c \\in \\{0,1,2,3\\}$, $i \\in \\{0,1\\}$, $j \\in \\{0,1\\}$.\n- Parameters:\n$$\nW_1 \\;=\\; \\begin{bmatrix}\n1  -1  0.5  0 \\\\\n0.25  0.5  -0.5  1\n\\end{bmatrix}, \\quad\nb_1 \\;=\\; \\begin{bmatrix} -0.5 \\\\ 0 \\end{bmatrix},\n$$\n$$\nW_2 \\;=\\; \\begin{bmatrix}\n1  0.5 \\\\\n-0.5  0.25 \\\\\n0  1 \\\\\n0.75  -1\n\\end{bmatrix}, \\quad\nb_2 \\;=\\; \\begin{bmatrix} 0 \\\\ 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}.\n$$\n\nTest case B (boundary on spatial extent, $H W = 1$):\n- $C = 8$, $H = 1$, $W = 1$, $r = 2$.\n- Input defined by $x_{c00} = c + 1$ for $c \\in \\{0,1,\\dots,7\\}$.\n- Parameters defined elementwise by index-based formulas. Let $a$ index rows and $b$ index columns, both zero-based:\n$$\nW_1[a,b] \\;=\\; \\frac{a\\cdot 8 + b - 12}{16}, \\quad \\text{for } a \\in \\{0,1,2,3\\}, \\; b \\in \\{0,1,\\dots,7\\},\n$$\n$$\nb_1 \\;=\\; \\begin{bmatrix} -0.25 \\\\ 0 \\\\ 0.1 \\\\ -0.1 \\end{bmatrix}, \\quad\nW_2[a,b] \\;=\\; \\frac{a\\cdot 4 + b - 8}{8}, \\quad \\text{for } a \\in \\{0,1,\\dots,7\\}, \\; b \\in \\{0,1,2,3\\},\n$$\n$$\nb_2 \\;=\\; \\mathbf{0} \\in \\mathbb{R}^{8}.\n$$\n\nTest case C (edge case $r = 1$ with sign-alternating channels):\n- $C = 6$, $H = 3$, $W = 1$, $r = 1$.\n- Input defined by $x_{ci0} = (-1)^c \\cdot (i+1)$ for $c \\in \\{0,1,\\dots,5\\}$ and $i \\in \\{0,1,2\\}$.\n- Parameters:\n$$\nW_1 \\;=\\; 0.5\\, I_6 \\;-\\; 0.25\\,(\\mathbf{1}_6 \\mathbf{1}_6^{\\top} - I_6), \\quad b_1 \\;=\\; \\mathbf{0} \\in \\mathbb{R}^{6},\n$$\n$$\nW_2 \\;=\\; 0.3\\, I_6 \\;-\\; 0.05\\, \\mathbf{1}_6 \\mathbf{1}_6^{\\top}, \\quad b_2 \\;=\\; \\begin{bmatrix} -0.1 \\\\ -0.05 \\\\ 0 \\\\ 0.05 \\\\ 0.1 \\\\ 0.15 \\end{bmatrix},\n$$\nwhere $I_6$ is the $6 \\times 6$ identity matrix and $\\mathbf{1}_6$ is the $6$-vector of ones.\n\nAngle units and physical units do not apply here. All numeric outputs should be real numbers in standard decimal form.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all three test cases as a list of lists. Each inner list must be ordered as\n$$\n\\left[ \\sum_{c} s_c,\\; \\mathrm{round}\\!\\left(\\sum_{c,i,j} y_{cij},\\,6\\right),\\; \\text{\\#weights}_{\\text{untied}},\\; \\text{\\#weights}_{\\text{tied}},\\; \\text{\\#weights}_{\\text{untied}} + \\text{\\#biases} \\right].\n$$\nFor example, the printed structure must look like\n$$\n\\big[\\,[\\dots],\\,[\\dots],\\,[\\dots]\\,\\big].\n$$",
            "solution": "The problem requires the implementation and analysis of a Squeeze-and-Excitation (SE) block, a common architectural component in deep learning, based on its fundamental mathematical definition. The solution involves a step-by-step execution of the defined operations for three distinct test cases. The core principles are channel-wise feature recalibration through a data-driven mechanism.\n\nThe SE block operates on an input tensor $x \\in \\mathbb{R}^{C \\times H \\times W}$, where $C$ is the number of channels, and $H$ and $W$ are the spatial height and width, respectively. The process consists of three main stages: Squeeze, Excitation, and Scale.\n\n**1. Squeeze: Global Information Embedding**\nThe first step, \"Squeeze,\" aggregates feature maps across their spatial dimensions ($H \\times W$) to produce a channel descriptor. This is achieved using global average pooling, which calculates the mean value for each channel. The resulting vector $s \\in \\mathbb{R}^{C}$ embeds a global receptive field for each channel. The formula for the $c$-th element of $s$ is:\n$$\ns_c = \\frac{1}{H \\cdot W}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} x_{cij}\n$$\nIn implementation, this corresponds to taking the mean of the input tensor $x$ along its spatial axes (axes $1$ and $2$). The first required output for each test case is the sum of the elements of this squeezed vector, $\\sum_{c=0}^{C-1} s_c$.\n\n**2. Excitation: Adaptive Recalibration**\nThe \"Excitation\" stage generates a set of per-channel modulation weights, or \"activations,\" from the squeezed descriptor $s$. This is accomplished by a small neural network, specifically a two-layer Multilayer Perceptron (MLP). The MLP is structured to first reduce the channel dimensionality and then restore it, creating a computational bottleneck that helps capture non-linear channel interdependencies.\n\nThe MLP consists of:\n- A dimensionality-reduction linear layer with weights $W_1 \\in \\mathbb{R}^{(C/r)\\times C}$ and biases $b_1 \\in \\mathbb{R}^{C/r}$, where $r$ is the reduction ratio.\n- A Rectified Linear Unit ($\\mathrm{ReLU}$) activation function, defined as $\\mathrm{ReLU}(u) = \\max\\{0,u\\}$.\n- A dimensionality-increasing linear layer with weights $W_2 \\in \\mathbb{R}^{C \\times (C/r)}$ and biases $b_2 \\in \\mathbb{R}^{C}$.\n- A logistic sigmoid activation function, $\\sigma(u)=\\frac{1}{1+e^{-u}}$, which normalizes the output to the range $(0, 1)$.\n\nThe computation proceeds as follows:\n$$\nt = \\mathrm{ReLU}(W_1 s + b_1)\n$$\n$$\nz = \\sigma(W_2 t + b_2)\n$$\nThe resulting vector $z \\in \\mathbb{R}^{C}$ contains the channel-wise scaling factors.\n\n**3. Scale: Feature Recalibration**\nThe final stage applies the learned scaling factors from the excitation step to the original input tensor $x$. Each channel of the input tensor is multiplied by its corresponding scaling factor from the vector $z$. This recalibrates the feature maps, amplifying informative channels and suppressing less useful ones. The output tensor $y \\in \\mathbb{R}^{C \\times H \\times W}$ is given by:\n$$\ny_{cij} = x_{cij} \\cdot z_c\n$$\nIn implementation, this is achieved by broadcasting the scaling vector $z$ (reshaped to have dimensions $C \\times 1 \\times 1$) over the input tensor $x$. The second required output is the sum of all elements in the final scaled tensor, $\\sum_{c,i,j} y_{cij}$, which is calculated as $\\sum_c (z_c \\cdot \\sum_{i,j} x_{cij}) = H \\cdot W \\cdot \\sum_c (z_c \\cdot s_c)$.\n\n**4. Parameter Overhead Calculation**\nThe problem also requires quantifying the number of additional learnable parameters introduced by the SE block's MLP. The parameters consist of weights and biases from the two linear layers.\n- **Untied Weights:** The total number of weights when $W_1$ and $W_2$ are independent is the sum of the elements in both matrices: $(\\frac{C}{r} \\times C) + (C \\times \\frac{C}{r}) = \\frac{2C^2}{r}$.\n- **Tied Weights:** In a variant where $W_2 = W_1^{\\top}$, the number of independent weights reduces to the size of $W_1$, which is $\\frac{C^2}{r}$.\n- **Biases:** The number of bias parameters is the sum of the elements in $b_1$ and $b_2$, which is $\\frac{C}{r} + C$.\n- **Total Untied Overhead:** The total number of parameters for the untied-weights case is the sum of untied weights and biases: $\\frac{2C^2}{r} + \\frac{C}{r} + C$.\n\nThe implementation will compute these five quantities for each of the three test cases provided, using the specified input tensors and MLP parameters.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and format results for all test cases.\n    \"\"\"\n\n    def se_block_and_overhead(C, H, W, r, x, W1, b1, W2, b2):\n        \"\"\"\n        Implements the SE block operations and calculates parameter overhead.\n\n        Args:\n            C (int): Number of channels.\n            H (int): Height of the input tensor.\n            W (int): Width of the input tensor.\n            r (int): Reduction ratio.\n            x (np.ndarray): Input tensor of shape (C, H, W).\n            W1 (np.ndarray): Weights of the first linear layer.\n            b1 (np.ndarray): Biases of the first linear layer.\n            W2 (np.ndarray): Weights of the second linear layer.\n            b2 (np.ndarray): Biases of the second linear layer.\n\n        Returns:\n            list: A list containing the five required output values.\n        \"\"\"\n        # 1. Squeeze Operation: Global Average Pooling\n        # s_c = (1/(H*W)) * sum(x_cij for i,j)\n        s = np.mean(x, axis=(1, 2))\n        sum_s = np.sum(s)\n\n        # 2. Excitation Operation: MLP\n        # t = ReLU(W1*s + b1)\n        t = np.maximum(0, W1 @ s + b1)\n        \n        # z = sigmoid(W2*t + b2)\n        z_raw = W2 @ t + b2\n        z = 1 / (1 + np.exp(-z_raw))\n\n        # 3. Scale Operation\n        # y_cij = x_cij * z_c\n        # Reshape z to (C, 1, 1) for broadcasting\n        y = x * z.reshape((C, 1, 1))\n        sum_y = np.sum(y)\n\n        # 4. Parameter Overhead Calculation\n        weights_untied = (2 * C**2) // r\n        weights_tied = (C**2) // r\n        biases = (C // r) + C\n        total_untied_with_biases = weights_untied + biases\n        \n        return [\n            sum_s,\n            round(sum_y, 6),\n            weights_untied,\n            weights_tied,\n            total_untied_with_biases\n        ]\n\n    results = []\n\n    # --- Test Case A ---\n    C_A, H_A, W_A, r_A = 4, 2, 2, 2\n    x_A = np.fromfunction(lambda c, i, j: c - i + j, (C_A, H_A, W_A))\n    W1_A = np.array([\n        [1, -1, 0.5, 0],\n        [0.25, 0.5, -0.5, 1]\n    ])\n    b1_A = np.array([-0.5, 0])\n    W2_A = np.array([\n        [1, 0.5],\n        [-0.5, 0.25],\n        [0, 1],\n        [0.75, -1]\n    ])\n    b2_A = np.array([0, 0.1, -0.2, 0.3])\n    results.append(se_block_and_overhead(C_A, H_A, W_A, r_A, x_A, W1_A, b1_A, W2_A, b2_A))\n\n    # --- Test Case B ---\n    C_B, H_B, W_B, r_B = 8, 1, 1, 2\n    x_B = (np.arange(C_B) + 1).reshape(C_B, H_B, W_B)\n    W1_B = np.fromfunction(lambda a, b: (a * 8 + b - 12) / 16, (C_B // r_B, C_B))\n    b1_B = np.array([-0.25, 0, 0.1, -0.1])\n    W2_B = np.fromfunction(lambda a, b: (a * 4 + b - 8) / 8, (C_B, C_B // r_B))\n    b2_B = np.zeros(C_B)\n    results.append(se_block_and_overhead(C_B, H_B, W_B, r_B, x_B, W1_B, b1_B, W2_B, b2_B))\n\n    # --- Test Case C ---\n    C_C, H_C, W_C, r_C = 6, 3, 1, 1\n    c_vec_C = (-1)**np.arange(C_C)\n    i_vec_C = np.arange(1, H_C + 1)\n    x_C = (c_vec_C[:, np.newaxis, np.newaxis]\n           * i_vec_C[np.newaxis, :, np.newaxis])\n    \n    I6 = np.identity(C_C)\n    J6 = np.ones((C_C, C_C))\n    \n    W1_C = 0.5 * I6 - 0.25 * (J6 - I6)\n    b1_C = np.zeros(C_C)\n    W2_C = 0.3 * I6 - 0.05 * J6\n    b2_C = np.array([-0.1, -0.05, 0, 0.05, 0.1, 0.15])\n    results.append(se_block_and_overhead(C_C, H_C, W_C, r_C, x_C, W1_C, b1_C, W2_C, b2_C))\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to a string and join them with commas.\n    # The outer brackets are added by the f-string.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The standard SE block uses a sigmoid function to generate channel gates, allowing each channel's importance to be scaled independently. But what if the channels had to compete for attention? This practice  challenges you to implement and compare the standard sigmoid gate against a constrained softmax gate, exploring how this fundamental design choice alters the network's behavior through quantitative analysis.",
            "id": "3175710",
            "problem": "You must implement two channel-attention mechanisms for squeeze-and-excitation within a convolutional feature map and compute quantitative metrics that reveal their behavioral differences. The mechanisms are: a constrained squeeze-and-excitation gate that lies in the probability simplex via the softmax mapping, and an independent element-wise gate via the logistic sigmoid mapping. The task must be completed by writing a single, self-contained program that produces the specified outputs for the given test suite.\n\nStart from the following base definitions:\n- A convolutional tensor is represented as $X \\in \\mathbb{R}^{C \\times H \\times W}$, where $C$ is the number of channels, $H$ is the height, and $W$ is the width.\n- The global average pooling descriptor is $s \\in \\mathbb{R}^{C}$ with components $s_c = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} X_{c, i, j}$.\n- The squeeze network is a two-layer transformation with a reduction ratio $r = \\max(1, \\lfloor C/2 \\rfloor)$, a rectified linear unit mapping $\\mathrm{ReLU}(x) = \\max(0, x)$, and deterministic weights defined below. Let $z \\in \\mathbb{R}^{C}$ be the pre-gate activation:\n$$\nz = W_2 \\, \\mathrm{ReLU}(W_1 s + b_1) + b_2,\n$$\nwhere $W_1 \\in \\mathbb{R}^{r \\times C}$, $b_1 \\in \\mathbb{R}^{r}$, $W_2 \\in \\mathbb{R}^{C \\times r}$, and $b_2 \\in \\mathbb{R}^{C}$.\n\nDefine the deterministic weights using elementary trigonometric functions so that the program is reproducible without training:\n- For $i \\in \\{1, \\dots, r\\}$ and $j \\in \\{1, \\dots, C\\}$,\n$$\n(W_1)_{i, j} = 0.1 \\, \\sin\\big((i)(j)\\big), \\quad (W_2)_{j, i} = 0.1 \\, \\cos\\big((j)(i)\\big).\n$$\n- For $i \\in \\{1, \\dots, r\\}$ and $j \\in \\{1, \\dots, C\\}$,\n$$\n(b_1)_i = 0.05 \\, \\cos(i), \\quad (b_2)_j = -0.02 \\, \\sin(j).\n$$\n\nConstruct two channel gates:\n- The constrained gate $g^{\\mathrm{soft}} \\in \\mathbb{R}^{C}$ is computed by the softmax mapping applied to $z$:\n$$\ng^{\\mathrm{soft}}_c = \\frac{\\exp(z_c)}{\\sum_{k=1}^{C} \\exp(z_k)}, \\quad \\text{so } \\sum_{c=1}^{C} g^{\\mathrm{soft}}_c = 1 \\text{ and } g^{\\mathrm{soft}}_c  0.\n$$\n- The independent gate $g^{\\mathrm{sig}} \\in \\mathbb{R}^{C}$ is computed by the logistic sigmoid mapping applied element-wise to $z$:\n$$\ng^{\\mathrm{sig}}_c = \\frac{1}{1 + \\exp(-z_c)}, \\quad \\text{so } 0  g^{\\mathrm{sig}}_c  1 \\text{ independently for each } c.\n$$\n\nApply the gates channel-wise to the original feature map:\n- The reweighted tensors are $Y^{\\mathrm{soft}} \\in \\mathbb{R}^{C \\times H \\times W}$ and $Y^{\\mathrm{sig}} \\in \\mathbb{R}^{C \\times H \\times W}$ with\n$$\nY^{\\mathrm{soft}}_{c, i, j} = g^{\\mathrm{soft}}_c \\, X_{c, i, j}, \\quad Y^{\\mathrm{sig}}_{c, i, j} = g^{\\mathrm{sig}}_c \\, X_{c, i, j}.\n$$\n\nFor each test case, compute the following metrics:\n- $M_1$: A boolean that is $\\mathrm{True}$ if $|\\sum_{c=1}^{C} g^{\\mathrm{soft}}_c - 1| \\leq \\varepsilon$ with tolerance $\\varepsilon = 10^{-12}$, and $\\mathrm{False}$ otherwise.\n- $M_2$: The $\\ell_1$-norm of $g^{\\mathrm{sig}}$, which equals $\\sum_{c=1}^{C} g^{\\mathrm{sig}}_c$.\n- $M_3$: The emphasis ratio for the constrained gate, defined as $\\frac{\\max_c g^{\\mathrm{soft}}_c}{\\frac{1}{C}\\sum_{c=1}^{C} g^{\\mathrm{soft}}_c}$.\n- $M_4$: The emphasis ratio for the independent gate, defined as $\\frac{\\max_c g^{\\mathrm{sig}}_c}{\\frac{1}{C}\\sum_{c=1}^{C} g^{\\mathrm{sig}}_c}$.\n- $M_5$: The variance of the constrained gate, $\\mathrm{Var}(g^{\\mathrm{soft}})$.\n- $M_6$: The variance of the independent gate, $\\mathrm{Var}(g^{\\mathrm{sig}})$.\n- $M_7$: The energy scaling ratio for the constrained gate, defined as $\\frac{\\|Y^{\\mathrm{soft}}\\|_1}{\\|X\\|_1}$, where $\\|T\\|_1 = \\sum_{c=1}^{C}\\sum_{i=1}^{H}\\sum_{j=1}^{W} |T_{c, i, j}|$. If $\\|X\\|_1 = 0$, define the ratio to be $0$.\n- $M_8$: The energy scaling ratio for the independent gate, defined as $\\frac{\\|Y^{\\mathrm{sig}}\\|_1}{\\|X\\|_1}$, with the same $0$ convention when $\\|X\\|_1 = 0$.\n\nImplement the above for the following test suite. Each test case specifies $(C, H, W)$ and the tensor $X$ explicitly.\n\nTest case $1$ (general mixed values):\n- $C = 4$, $H = 2$, $W = 3$.\n- $X$ channels:\n$$\nX_{0,:,:} = \\begin{bmatrix} 0.5  0.2  -0.1 \\\\ 0.3  -0.2  0.0 \\end{bmatrix}, \\quad\nX_{1,:,:} = \\begin{bmatrix} -0.4  0.1  0.2 \\\\ -0.1  -0.05  0.3 \\end{bmatrix},\n$$\n$$\nX_{2,:,:} = \\begin{bmatrix} 1.2  0.8  0.5 \\\\ 0.4  0.0  -0.3 \\end{bmatrix}, \\quad\nX_{3,:,:} = \\begin{bmatrix} 0.0  -0.1  -0.2 \\\\ 0.3  0.1  0.0 \\end{bmatrix}.\n$$\n\nTest case $2$ (single-channel boundary):\n- $C = 1$, $H = 3$, $W = 3$.\n- $X$ channel:\n$$\nX_{0,:,:} = \\begin{bmatrix} 0.1  0.2  0.3 \\\\ 0.4  0.5  0.6 \\\\ 0.7  0.8  0.9 \\end{bmatrix}.\n$$\n\nTest case $3$ (zero tensor edge case):\n- $C = 3$, $H = 2$, $W = 2$.\n- $X$ channels:\n$$\nX_{0,:,:} = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}, \\quad\nX_{1,:,:} = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}, \\quad\nX_{2,:,:} = \\begin{bmatrix} 0.0  0.0 \\\\ 0.0  0.0 \\end{bmatrix}.\n$$\n\nTest case $4$ (dominant channel and mixed signs):\n- $C = 5$, $H = 2$, $W = 2$.\n- $X$ channels:\n$$\nX_{0,:,:} = \\begin{bmatrix} 10.0  9.0 \\\\ 8.0  7.0 \\end{bmatrix}, \\quad\nX_{1,:,:} = \\begin{bmatrix} -5.0  -4.0 \\\\ -3.0  -2.0 \\end{bmatrix},\n$$\n$$\nX_{2,:,:} = \\begin{bmatrix} 0.1  -0.1 \\\\ 0.05  -0.05 \\end{bmatrix}, \\quad\nX_{3,:,:} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  -1.0 \\end{bmatrix}, \\quad\nX_{4,:,:} = \\begin{bmatrix} 0.3  0.2 \\\\ 0.1  0.0 \\end{bmatrix}.\n$$\n\nYour program must compute $(M_1, M_2, M_3, M_4, M_5, M_6, M_7, M_8)$ for each test case and produce a single line of output containing the results flattened across all test cases as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_{32}]$. No physical units or angles are involved, so none are required. The outputs must be of type boolean or floating-point numbers and must be computed exactly as defined.",
            "solution": "The problem requires the implementation and comparative analysis of two distinct channel-attention mechanisms derived from the Squeeze-and-Excitation (SE) architecture. The first mechanism, a constrained gate $g^{\\mathrm{soft}}$, uses the softmax function to distribute a total attention budget of $1$ across channels, enforcing competition. The second, an independent gate $g^{\\mathrm{sig}}$, uses the element-wise logistic sigmoid function, allowing each channel's attention weight to be determined independently of the others. The process is deterministic, with all network weights and biases defined by trigonometric functions. For each input tensor $X$, we compute a set of eight metrics ($M_1$ through $M_8$) to quantitatively assess the behavioral differences between these two gating strategies.\n\nThe solution is implemented by following a sequence of well-defined computational steps for each test case.\n\n**Step 1: Global Average Pooling**\nThe process begins with the \"squeeze\" operation, which aggregates spatial information across the height $H$ and width $W$ of each channel into a single descriptor value. For an input tensor $X \\in \\mathbb{R}^{C \\times H \\times W}$, we compute a global average pooling descriptor vector $s \\in \\mathbb{R}^{C}$. Each component $s_c$ of this vector represents the average activation for the $c$-th channel:\n$$\ns_c = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} X_{c, i, j} \\quad \\text{for } c \\in \\{1, \\dots, C\\}.\n$$\n\n**Step 2: Squeeze Network Transformation**\nThe descriptor vector $s$ is then processed by a two-layer fully connected network, the \"excitation\" block, to generate channel-specific pre-gate activations. This network consists of a dimensionality-reduction layer followed by a dimensionality-expansion layer.\n\nFirst, the reduction ratio $r$ is determined by the number of channels $C$:\n$$\nr = \\max(1, \\lfloor C/2 \\rfloor).\n$$\nThe network weights $W_1 \\in \\mathbb{R}^{r \\times C}$, $b_1 \\in \\mathbb{R}^{r}$, $W_2 \\in \\mathbb{R}^{C \\times r}$, and $b_2 \\in \\mathbb{R}^{C}$ are defined deterministically. Using $1$-based indexing for matrices and vectors as specified:\n- For $i \\in \\{1, \\dots, r\\}$ and $j \\in \\{1, \\dots, C\\}$:\n  $$\n  (W_1)_{i, j} = 0.1 \\, \\sin(ij), \\quad (W_2)_{j, i} = 0.1 \\, \\cos(ji).\n  $$\n- For $i \\in \\{1, \\dots, r\\}$ and $j \\in \\{1, \\dots, C\\}$:\n  $$\n  (b_1)_i = 0.05 \\, \\cos(i), \\quad (b_2)_j = -0.02 \\, \\sin(j).\n  $$\nThe pre-gate activation vector $z \\in \\mathbb{R}^{C}$ is then calculated using the Rectified Linear Unit ($\\mathrm{ReLU}$) activation function, defined as $\\mathrm{ReLU}(x) = \\max(0, x)$:\n$$\nz = W_2 \\, \\mathrm{ReLU}(W_1 s + b_1) + b_2.\n$$\n\n**Step 3: Gate Generation**\nFrom the pre-gate activation vector $z$, we compute the two types of channel gates:\n\n- **Constrained Gate ($g^{\\mathrm{soft}}$)**: The softmax function is applied to $z$ to produce a gate vector where the components sum to $1$ and are all positive. This creates a competitive dynamic where increasing the weight of one channel necessarily decreases the weights of others.\n$$\ng^{\\mathrm{soft}}_c = \\frac{\\exp(z_c)}{\\sum_{k=1}^{C} \\exp(z_k)}.\n$$\n- **Independent Gate ($g^{\\mathrm{sig}}$)**: The logistic sigmoid function is applied element-wise to $z$. Each gate weight $g^{\\mathrm{sig}}_c$ is mapped to the range $(0, 1)$ independently.\n$$\ng^{\\mathrm{sig}}_c = \\frac{1}{1 + \\exp(-z_c)}.\n$$\n\n**Step 4: Channel Reweighting and Metric Computation**\nThe computed gates are used to rescale the channels of the original input tensor $X$. The reweighted tensors are $Y^{\\mathrm{soft}}$ and $Y^{\\mathrm{sig}}$:\n$$\nY^{\\mathrm{soft}}_{c, i, j} = g^{\\mathrm{soft}}_c \\, X_{c, i, j}, \\quad Y^{\\mathrm{sig}}_{c, i, j} = g^{\\mathrm{sig}}_c \\, X_{c, i, j}.\n$$\nFinally, the eight analytical metrics are computed:\n\n- $M_1$: A boolean check on the fundamental property of the softmax function, ensuring $\\sum_{c=1}^{C} g^{\\mathrm{soft}}_c$ is numerically close to $1$. The condition is $|\\sum_{c=1}^{C} g^{\\mathrm{soft}}_c - 1| \\leq \\varepsilon$, with tolerance $\\varepsilon = 10^{-12}$.\n\n- $M_2$: The $\\ell_1$-norm of the independent gate, $\\sum_{c=1}^{C} g^{\\mathrm{sig}}_c$. This metric measures the total \"attentional energy\" allocated by the sigmoid gate, which is not constrained to a fixed budget.\n\n- $M_3$: The emphasis ratio for the constrained gate, $\\frac{\\max_c g^{\\mathrm{soft}}_c}{\\frac{1}{C}\\sum_{c=1}^{C} g^{\\mathrm{soft}}_c}$. Because the sum of $g^{\\mathrm{soft}}_c$ is $1$, the average is $1/C$, and the ratio simplifies to $C \\cdot \\max_c g^{\\mathrm{soft}}_c$. It measures how much more attention the most important channel receives compared to the average. A higher value indicates a more \"peaked\" or selective attention distribution.\n\n- $M_4$: The emphasis ratio for the independent gate, $\\frac{\\max_c g^{\\mathrm{sig}}_c}{\\frac{1}{C}\\sum_{c=1}^{C} g^{\\mathrm{sig}}_c}$. This measures the same concept of selectivity for the sigmoid gate.\n\n- $M_5$: The variance of the constrained gate, $\\mathrm{Var}(g^{\\mathrm{soft}})$, measures the spread of the attention weights. A higher variance indicates that attention is more unevenly distributed across channels.\n\n- $M_6$: The variance of the independent gate, $\\mathrm{Var}(g^{\\mathrm{sig}})$.\n\n- $M_7$: The energy scaling ratio for the constrained gate, $\\frac{\\|Y^{\\mathrm{soft}}\\|_1}{\\|X\\|_1}$, where $\\|T\\|_1 = \\sum_{c,i,j} |T_{c,i,j}|$. Since $g^{\\mathrm{soft}}_c > 0$, this simplifies to $\\frac{\\sum_c g^{\\mathrm{soft}}_c \\|X_c\\|_1}{\\sum_c \\|X_c\\|_1}$, which is a weighted average of the channel-wise $\\ell_1$-norms, weighted by the attention scores. It quantifies the overall scaling effect of the gate on the tensor's magnitude. If $\\|X\\|_1=0$, this is defined as $0$.\n\n- $M_8$: The energy scaling ratio for the independent gate, $\\frac{\\|Y^{\\mathrm{sig}}\\|_1}{\\|X\\|_1}$, calculated analogously to $M_7$.\n\nThis complete pipeline is applied to each test case to generate the required numerical results.",
            "answer": "```python\nimport numpy as np\n\ndef compute_metrics(C, H, W, X):\n    \"\"\"\n    Computes squeeze-and-excitation gates and derived metrics for a given tensor.\n    \"\"\"\n    # Step 1: Global Average Pooling\n    s = np.mean(X, axis=(1, 2)) if H * W  0 else np.zeros(C)\n\n    # Step 2: Squeeze Network\n    r = max(1, C // 2)\n\n    # Generate deterministic weights (using 1-based indexing from problem)\n    i_vec_r = np.arange(1, r + 1, dtype=float).reshape(-1, 1)\n    j_vec_C = np.arange(1, C + 1, dtype=float).reshape(1, -1)\n\n    W1 = 0.1 * np.sin(i_vec_r * j_vec_C)\n    b1 = 0.05 * np.cos(np.arange(1, r + 1, dtype=float))\n\n    j_vec_C_T = j_vec_C.T\n    i_vec_r_T = i_vec_r.T\n    W2 = 0.1 * np.cos(j_vec_C_T * i_vec_r_T)\n    b2 = -0.02 * np.sin(np.arange(1, C + 1, dtype=float))\n    \n    # Compute pre-gate activation z\n    z = W2 @ np.maximum(0, W1 @ s + b1) + b2\n\n    # Step 3: Gate Generation\n    # Constrained gate (softmax) with numerical stability\n    if z.size  0:\n        stable_z = z - np.max(z)\n        exp_z = np.exp(stable_z)\n        g_soft = exp_z / np.sum(exp_z)\n    else: # Edge case for C=0, though not in tests\n        g_soft = np.array([])\n\n    # Independent gate (sigmoid)\n    g_sig = 1 / (1 + np.exp(-z))\n\n    # Step 4: Metric Computations\n    # M1: Softmax sum-to-one check\n    M1 = np.abs(np.sum(g_soft) - 1.0) = 1e-12 if g_soft.size  0 else True\n    \n    # M2: Sigmoid l1-norm\n    M2 = np.sum(g_sig)\n    \n    # M3, M4: Emphasis Ratios\n    # Handle C=1 case where mean=max\n    mean_g_soft = np.mean(g_soft) if g_soft.size  0 else 0.0\n    M3 = np.max(g_soft) / mean_g_soft if mean_g_soft  0 else 1.0\n    \n    mean_g_sig = np.mean(g_sig) if g_sig.size  0 else 0.0\n    M4 = np.max(g_sig) / mean_g_sig if mean_g_sig  0 else 1.0\n\n    # M5, M6: Variances\n    M5 = np.var(g_soft)\n    M6 = np.var(g_sig)\n\n    # M7, M8: Energy Scaling Ratios\n    l1_norm_X = np.sum(np.abs(X))\n    if l1_norm_X == 0:\n        M7 = 0.0\n        M8 = 0.0\n    else:\n        channel_l1_norms = np.sum(np.abs(X), axis=(1, 2))\n        M7 = np.sum(g_soft * channel_l1_norms) / l1_norm_X\n        M8 = np.sum(g_sig * channel_l1_norms) / l1_norm_X\n\n    return (M1, M2, M3, M4, M5, M6, M7, M8)\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run computations, and print results.\n    \"\"\"\n    test_cases = [\n        (4, 2, 3, np.array([\n            [[0.5, 0.2, -0.1], [0.3, -0.2, 0.0]],\n            [[-0.4, 0.1, 0.2], [-0.1, -0.05, 0.3]],\n            [[1.2, 0.8, 0.5], [0.4, 0.0, -0.3]],\n            [[0.0, -0.1, -0.2], [0.3, 0.1, 0.0]]\n        ])),\n        (1, 3, 3, np.array([\n            [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]]\n        ])),\n        (3, 2, 2, np.zeros((3, 2, 2))),\n        (5, 2, 2, np.array([\n            [[10.0, 9.0], [8.0, 7.0]],\n            [[-5.0, -4.0], [-3.0, -2.0]],\n            [[0.1, -0.1], [0.05, -0.05]],\n            [[1.0, 0.0], [0.0, -1.0]],\n            [[0.3, 0.2], [0.1, 0.0]]\n        ]))\n    ]\n\n    all_results = []\n    for C, H, W, X in test_cases:\n        metrics = compute_metrics(C, H, W, X)\n        all_results.extend(metrics)\n    \n    # Format according to spec: [val1,val2,...]\n    # Python's str(True) is 'True', which is the required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The reduction ratio, $r$, is a critical hyperparameter in an SE block, governing the capacity of the gating mechanism. Choosing $r$ involves a classic trade-off: a small $r$ creates a complex model that can overfit, while a large $r$ creates a simple model that may underfit. This exercise  provides a framework grounded in statistical learning theory to find the optimal balance, allowing you to select $r$ not by guesswork, but by minimizing a defined structural risk.",
            "id": "3175715",
            "problem": "You will implement a program that evaluates the effect of the reduction ratio in Squeeze-and-Excitation (SE) networks on a simplified surrogate of expected risk, and outputs the optimal reduction ratio for each of several test cases. Your analysis must strictly follow a principle-based derivation grounded in core definitions of Empirical Risk Minimization (ERM), complexity control via pseudo-dimension, and best low-rank approximation from linear algebra.\n\nContext and definitions:\n- In a Squeeze-and-Excitation (SE) block with $C$ channels, the squeeze operation computes a channel descriptor $z \\in \\mathbb{R}^{C}$ via global average pooling, and the excitation applies a bottlenecked two-layer multilayer perceptron (MLP) with reduction ratio $r \\in \\mathbb{N}$ that maps $z$ to a gating vector $g(z) \\in (0,1)^{C}$, which scales the channels. The bottleneck dimension is $m(r) = C / r$ when $r$ divides $C$.\n- The total parameter count in the excitation MLP, counting weights and biases, is modeled as\n$$\nP(r) = \\frac{2 C^{2}}{r} + \\frac{C}{r} + C,\n$$\nsince the two weight matrices have shapes $(\\frac{C}{r} \\times C)$ and $(C \\times \\frac{C}{r})$, and there are biases of sizes $\\frac{C}{r}$ and $C$.\n- We study a regression surrogate for the gating map by linearizing the excitation mapping. Let the ground-truth linear operator be $A \\in \\mathbb{R}^{C \\times C}$, and assume isotropic inputs $z \\sim \\mathcal{N}(0, I_{C})$. Training uses Empirical Risk Minimization with squared loss. The model class is constrained by the bottleneck to realize matrices of rank at most $m(r)$, since in the linear regime one can model the excitation as a product of two linear layers with an intermediate dimension $m(r)$.\n- The best achievable approximation error of $A$ by a rank-$m(r)$ matrix under expected squared error equals the squared Frobenius norm residual of the best rank-$m(r)$ approximation:\n$$\n\\mathcal{E}_{\\text{approx}}(r; A) = \\sum_{i=m(r)+1}^{C} \\sigma_{i}(A)^{2},\n$$\nwhere $\\{\\sigma_{i}(A)\\}_{i=1}^{C}$ are the singular values of $A$ in nonincreasing order. This follows from the Eckart–Young–Mirsky theorem and the fact that $\\mathbb{E}\\left[\\lVert (A - \\hat{A}) z \\rVert_{2}^{2}\\right] = \\lVert A - \\hat{A} \\rVert_{F}^{2}$ when $z \\sim \\mathcal{N}(0, I_{C})$.\n- To control overfitting, use a classical structural risk perspective grounded in statistical learning theory. For real-valued neural networks with piecewise linear or sigmoid activations, the pseudo-dimension scales on the order of $P \\log P$. We use the surrogate\n$$\nd(r) = P(r) \\cdot \\ln\\big(\\max\\{P(r), e\\}\\big),\n$$\nand the generalization penalty\n$$\n\\mathcal{C}(r; N) = \\sqrt{\\frac{d(r) \\cdot \\ln\\!\\left(\\max\\left\\{\\frac{eN}{d(r)}, 1\\right\\}\\right)}{N}},\n$$\nwhich mirrors standard generalization-gap bounds where the complexity grows with pseudo-dimension and shrinks with sample size $N$. All logarithms are natural logarithms.\n- Define the structural risk criterion\n$$\nJ(r; N, A) = \\mathcal{E}_{\\text{approx}}(r; A) + \\mathcal{C}(r; N).\n$$\nFor a fixed problem $(N, A)$ and candidate set $\\mathcal{R}$ of valid reduction ratios that divide $C$, the preferred reduction ratio is the minimizer\n$$\nr^{\\star} = \\arg\\min_{r \\in \\mathcal{R}} J(r; N, A).\n$$\nIn case of ties, choose the largest $r$ (smallest capacity) among minimizers to stay conservative against overfitting.\n\nYour program must:\n- Implement the above definitions exactly.\n- For each test case, compute $r^{\\star}$ over the candidate set $\\mathcal{R} = \\{1, 2, 4, 8, 16\\}$ with $C = 16$.\n- Treat $\\{\\sigma_{i}(A)\\}_{i=1}^{C}$ as given; you do not need to construct $A$ explicitly because $\\mathcal{E}_{\\text{approx}}$ depends only on the singular values.\n\nTest suite:\n- Use $C = 16$ and the candidate set $\\mathcal{R} = \\{1, 2, 4, 8, 16\\}$ for all cases. For the first three test cases, use the same ground-truth singular values, and vary only the sample size $N$ to probe the capacity–overfitting trade-off and the curriculum idea “start small then increase capacity.”\n- Case 1 (small sample): $N = 100$, with singular values\n$$\n\\sigma(A) = \\left[3.0, 2.7, 2.4, 2.1, 1.9, 1.7, 1.5, 1.3, 1.1, 0.9, 0.7, 0.5, 0.4, 0.3, 0.2, 0.1\\right].\n$$\n- Case 2 (moderate sample): $N = 500$, same $\\sigma(A)$ as Case 1.\n- Case 3 (large sample): $N = 5000$, same $\\sigma(A)$ as Case 1.\n- Case 4 (low-rank target): $N = 100$, with a low-rank ground-truth given by\n$$\n\\sigma(A) = \\left[3.0, 2.5, 2.0, 1.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\\right].\n$$\n\nFinal output format:\n- Your program should produce a single line of output containing the four chosen reduction ratios $[r^{\\star}_{1}, r^{\\star}_{2}, r^{\\star}_{3}, r^{\\star}_{4}]$ corresponding to Cases $1$ through $4$, respectively, as a comma-separated list enclosed in square brackets. For example, a valid output would look like $[8,4,2,4]$.\n\nNotes:\n- Use $m(r) = C / r$ exactly (all $r$ in $\\mathcal{R}$ divide $C$).\n- Use natural logarithms in all occurrences of $\\ln(\\cdot)$.\n- If $d(r) = 0$, treat $\\ln\\!\\left(\\max\\left\\{\\frac{eN}{d(r)}, 1\\right\\}\\right)$ as $\\ln(eN)$ for numerical stability. If $\\frac{eN}{d(r)}  1$, clamp the argument of the logarithm to $1$ (so that the logarithm contributes $0$ to the penalty), as specified above.\n- The intended interpretation of the results relates to curriculum on the reduction ratio: as $N$ grows across Cases $1 \\rightarrow 2 \\rightarrow 3$, the optimal $r^{\\star}$ should not decrease strictly in all instances, but the tendency to prefer smaller $r$ (larger capacity) is expected to increase because the complexity penalty shrinks with $N$, reflecting a principled trade-off between approximation and overfitting.",
            "solution": "The problem requires the implementation of a program to determine the optimal reduction ratio, $r^{\\star}$, for a Squeeze-and-Excitation (SE) network block by minimizing a defined structural risk criterion, $J(r; N, A)$. The solution is derived by strictly adhering to the provided theoretical framework, which integrates principles from statistical learning theory and linear algebra.\n\nThe overall objective is to find $r^{\\star} = \\arg\\min_{r \\in \\mathcal{R}} J(r; N, A)$ for a given number of channels $C=16$, a set of candidate reduction ratios $\\mathcal{R} = \\{1, 2, 4, 8, 16\\}$, and specific test cases defined by sample size $N$ and the singular value spectrum $\\sigma(A)$ of a ground-truth linear operator $A$. The structural risk $J(r; N, A)$ comprises two competing terms: an approximation error $\\mathcal{E}_{\\text{approx}}(r; A)$ and a complexity penalty $\\mathcal{C}(r; N)$.\n\nThe solution proceeds by implementing each component of the structural risk criterion as defined.\n\n1.  **Model Complexity Quantification**: The complexity of the model associated with a reduction ratio $r$ is captured through a hierarchy of definitions.\n    -   The bottleneck dimension of the SE block's internal MLP is $m(r) = C/r$. For $C=16$ and $r \\in \\mathcal{R}$, $m(r)$ takes values in $\\{16, 8, 4, 2, 1\\}$.\n    -   The total number of parameters (weights and biases) in the excitation MLP is given by $P(r) = \\frac{2 C^{2}}{r} + \\frac{C}{r} + C$. A smaller $r$ corresponds to a larger bottleneck dimension $m(r)$ and thus a higher parameter count $P(r)$, representing a more complex model.\n    -   The model's complexity is formally measured by a surrogate for the pseudo-dimension, $d(r) = P(r) \\cdot \\ln\\big(\\max\\{P(r), e\\}\\big)$, where $e$ is Euler's number and $\\ln$ is the natural logarithm. This function grows with $P(r)$, mapping parameter count to a complexity measure used in generalization bounds.\n\n2.  **Approximation Error Calculation**: The approximation error, $\\mathcal{E}_{\\text{approx}}(r; A)$, quantifies the best possible performance of a model with capacity constrained by $r$.\n    -   According to the Eckart–Young–Mirsky theorem, the best rank-$k$ approximation to a matrix $A$ under the Frobenius norm is obtained by truncating its singular value decomposition. For the specified regression task with isotropic inputs, this translates to an expected squared error.\n    -   The approximation error is thus the sum of the squares of the singular values that are discarded by the rank-$m(r)$ approximation:\n        $$\n        \\mathcal{E}_{\\text{approx}}(r; A) = \\sum_{i=m(r)+1}^{C} \\sigma_{i}(A)^{2}\n        $$\n    -   This term represents the irreducible error due to the model's limited capacity. A larger $r$ (smaller capacity, smaller $m(r)$) leads to a larger approximation error, as more singular values contribute to the sum. Conversely, for $r=1$, $m(1)=C=16$, the sum is empty and $\\mathcal{E}_{\\text{approx}}(1; A)=0$, as a full-rank model can perfectly represent any matrix $A$.\n\n3.  **Generalization Penalty Calculation**: The complexity penalty, $\\mathcal{C}(r; N)$, accounts for the risk of overfitting.\n    -   It is defined by a standard form from statistical learning theory:\n        $$\n        \\mathcal{C}(r; N) = \\sqrt{\\frac{d(r) \\cdot \\ln\\!\\left(\\max\\left\\{\\frac{eN}{d(r)}, 1\\right\\}\\right)}{N}}\n        $$\n    -   This term increases with model complexity $d(r)$ and decreases with sample size $N$. The logarithmic factor reflects how the generalization gap closes with more data.\n    -   A crucial feature of this formula is the clamping of the logarithm's argument at $1$. If $\\frac{eN}{d(r)}  1$ (i.e., if complexity $d(r)$ is large relative to sample size $N$), the logarithm becomes $\\ln(1)=0$, and the penalty term $\\mathcal{C}(r;N)$ vanishes. This models a scenario where the generalization bound is vacuous, failing to penalize overly complex models when data is scarce.\n\n4.  **Optimization and Selection**: For each test case, the optimal reduction ratio $r^{\\star}$ is found by executing the following procedure:\n    -   For each candidate $r \\in \\{1, 2, 4, 8, 16\\}$, the total risk $J(r) = \\mathcal{E}_{\\text{approx}}(r; A) + \\mathcal{C}(r; N)$ is computed.\n    -   The minimum risk value, $J_{\\min} = \\min_{r \\in \\mathcal{R}} J(r)$, is identified.\n    -   The set of minimizers, $\\{r | J(r) = J_{\\min}\\}$, is determined. A small numerical tolerance (e.g., $10^{-9}$) is used to compare floating-point risk values robustly.\n    -   The final optimal ratio $r^{\\star}$ is selected as the largest value in the set of minimizers. This tie-breaking rule enforces a preference for the simplest model (largest $r$, smallest capacity) that achieves the minimum risk, embodying the principle of Occam's razor.\n\nThis structured, principle-based process is implemented for each of the four test cases to yield the final list of optimal reduction ratios.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal reduction ratio for Squeeze-and-Excitation networks\n    based on a structural risk minimization framework for several test cases.\n    \"\"\"\n    # Define constants and test cases as per the problem statement.\n    C = 16\n    R_CANDIDATES = [1, 2, 4, 8, 16]\n    \n    test_cases = [\n        {\n            \"N\": 100,\n            \"sigmas\": np.array([3.0, 2.7, 2.4, 2.1, 1.9, 1.7, 1.5, 1.3, 1.1, 0.9, 0.7, 0.5, 0.4, 0.3, 0.2, 0.1])\n        },\n        {\n            \"N\": 500,\n            \"sigmas\": np.array([3.0, 2.7, 2.4, 2.1, 1.9, 1.7, 1.5, 1.3, 1.1, 0.9, 0.7, 0.5, 0.4, 0.3, 0.2, 0.1])\n        },\n        {\n            \"N\": 5000,\n            \"sigmas\": np.array([3.0, 2.7, 2.4, 2.1, 1.9, 1.7, 1.5, 1.3, 1.1, 0.9, 0.7, 0.5, 0.4, 0.3, 0.2, 0.1])\n        },\n        {\n            \"N\": 100,\n            \"sigmas\": np.array([3.0, 2.5, 2.0, 1.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n        }\n    ]\n\n    # Helper functions implementing the provided mathematical definitions.\n    def calculate_P(r, C_val):\n        \"\"\"Calculates the parameter count P(r).\"\"\"\n        return (2 * C_val**2 / r) + (C_val / r) + C_val\n\n    def calculate_d(r, C_val):\n        \"\"\"Calculates the pseudo-dimension surrogate d(r).\"\"\"\n        p_val = calculate_P(r, C_val)\n        return p_val * np.log(max(p_val, np.e))\n\n    def calculate_E_approx(r, C_val, sigmas_val):\n        \"\"\"Calculates the approximation error E_approx(r).\"\"\"\n        m_r = C_val // r\n        # The sum is over singular values with index  m(r).\n        # In 0-based indexing, this corresponds to slicing from index m_r.\n        if m_r = len(sigmas_val):\n            return 0.0\n        return np.sum(np.square(sigmas_val[m_r:]))\n\n    def calculate_C_penalty(r, C_val, N_val):\n        \"\"\"Calculates the generalization penalty C(r, N).\"\"\"\n        d_val = calculate_d(r, C_val)\n        # As per the problem, clamp the argument of the logarithm to be at least 1.\n        log_arg = max((np.e * N_val) / d_val, 1.0)\n        log_term = np.log(log_arg)\n        # The penalty is sqrt( (d * log_term) / N ).\n        penalty = np.sqrt((d_val * log_term) / N_val)\n        return penalty\n\n    optimal_ratios = []\n    \n    for case in test_cases:\n        N = case[\"N\"]\n        sigmas = case[\"sigmas\"]\n        \n        risk_results = []\n        for r in R_CANDIDATES:\n            # Calculate the two components of the structural risk.\n            e_approx = calculate_E_approx(r, C, sigmas)\n            c_penalty = calculate_C_penalty(r, C, N)\n            \n            # The total risk J(r).\n            j_risk = e_approx + c_penalty\n            risk_results.append((j_risk, r))\n            \n        # Find the minimum risk value.\n        min_risk = min(res[0] for res in risk_results)\n        \n        # Identify all reduction ratios that achieve this minimum risk (using a tolerance for float comparison).\n        minimizers = [r for risk, r in risk_results if np.isclose(risk, min_risk, atol=1e-9)]\n        \n        # Apply the tie-breaking rule: choose the largest r among the minimizers.\n        optimal_r = max(minimizers)\n        optimal_ratios.append(optimal_r)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, optimal_ratios))}]\")\n\nsolve()\n```"
        }
    ]
}