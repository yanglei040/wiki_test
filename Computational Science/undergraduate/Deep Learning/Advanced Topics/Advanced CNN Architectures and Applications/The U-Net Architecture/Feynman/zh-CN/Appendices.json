{
    "hands_on_practices": [
        {
            "introduction": "在设计或选择一个深度学习模型时，评估其计算成本是至关重要的一步。本练习将引导你从第一性原理出发，推导U-Net架构中每像素所需的浮点运算次数（FLOPs）。通过这种方式，你不仅能深化对U-Net逐层操作的理解，还能掌握量化和比较不同模型复杂度的核心技能。",
            "id": "3193868",
            "problem": "考虑一个用于图像到图像映射的二维U形卷积神经网络（U-Net），其具有以下架构和约定。输入是一个具有任意空间分辨率的正方形图像，分析是在每个输入像素的基础上进行的，因此结果与绝对输入尺寸无关。编码器（下采样路径）有 $L$ 个分辨率级别，索引为 $l \\in \\{0,\\dots,L-1\\}$，然后是级别 $L$ 的瓶颈层，再然后是解码器（上采样路径），有 $L$ 个分辨率级别，索引为 $l \\in \\{L-1,\\dots,0\\}$。设级别 $l$ 的特征通道数为 $2^{l} c$，其中 $c$ 是给定的基础通道宽度。在每个编码器级别 $l \\in \\{0,\\dots,L-1\\}$，有两个卷积层，每个都是标准二维卷积，卷积核大小为 $k \\times k$，步幅为 $1$，并使用“相同”填充（same padding）以保持该级别的空间分辨率；这两个卷积都接收 $2^{l} c$ 个输入通道，并产生 $2^{l} c$ 个输出通道。在编码器级别之间，通过在每个空间维度上进行因子为 $2$ 的 $2 \\times 2$ 非参数化下采样。级别 $L$ 的瓶颈层包括两个 $k \\times k$ 卷积，从 $2^{L} c$ 个通道到 $2^{L} c$ 个通道。在解码器中，在每个级别 $l \\in \\{L-1,\\dots,0\\}$，首先是一个转置卷积（也称为反卷积），其卷积核大小为 $2 \\times 2$，步幅为 $2$，在每个维度上进行空间上采样因子为 $2$，并将通道数从 $2^{l+1} c$ 减半到 $2^{l} c$。上采样后的特征图与来自编码器相同分辨率的跳跃连接（具有 $2^{l} c$ 个通道）进行拼接（沿通道方向），产生 $2^{l+1} c$ 个通道。之后是两个标准的 $k \\times k$ 卷积：第一个将通道数从 $2^{l+1} c$ 减少到 $2^{l} c$，第二个将通道数保持在 $2^{l} c$。忽略池化、拼接、偏置加法和非线性激活的成本。不包括任何最终的分类层。按照一个乘法计为 $1$ 次浮点运算、一个加法计为 $1$ 次浮点运算的惯例来计算浮点运算次数，因此一次乘加运算贡献 $2$ 次浮点运算。\n\n任务：\n- 仅从以下定义出发：一个产生空间面积为 $A$、卷积核大小为 $k \\times k$、$C_{\\text{in}}$ 个输入通道和 $C_{\\text{out}}$ 个输出通道的标准二维卷积，执行 $2 A k^{2} C_{\\text{in}} C_{\\text{out}}$ 次浮点运算，推导出该U-Net每个输入像素的总理论浮点运算次数的封闭形式表达式 $F(k,c,L)$，纯粹用 $k$、$c$ 和 $L$ 表示。你的推导必须涵盖所有编码器模块、瓶颈层和所有解码器模块（包括转置卷积），并且必须使用级别 $l$ 的空间面积相对于输入减少了 $4^{l}$ 倍这一事实。\n- 一个假设的性能分析器运行报告称，对于 $k=3$，$c=64$ 和 $L=4$，所有转置卷积合计占总浮点运算的比例约为 $0.14$。使用你推导出的 $F(k,c,L)$ 来计算归因于所有转置卷积的理论比例，并评论性能分析器的报告是否与理论一致。无需四舍五入。\n- 提出一项修改，将每个 $2 \\times 2$ 转置卷积替换为非学习的双线性上采样，后跟一个从 $2^{l+1} c$ 通道到 $2^{l} c$ 通道的 $1 \\times 1$ 卷积。使用相同的计数惯例，推导此替换所隐含的每个输入像素浮点运算的变化量，并指出它改变了 $F(k,c,L)$ 中的哪一项。无需四舍五入。\n\n你最终报告的答案必须仅为第一个任务中指定的单个封闭形式表达式 $F(k,c,L)$。不包括任何单位。不要四舍五入。",
            "solution": "问题陈述已经过严格验证，并被确定为有效。它具有科学依据，问题提出得当，客观，并包含足够的信息以获得唯一解。其定义和架构与深度学习领域的标准实践一致。\n\n解题过程通过推导所需表达式来进行。\n\n**任务1：推导每个输入像素的总浮点运算次数 $F(k,c,L)$**\n\n我们计算U-Net架构中每个组件的浮点运算（FLOPs）。最终结果通过将总FLOPs除以输入图像面积（记为 $A_0$）进行归一化，表示为每个输入像素的运算量。\n\n分辨率级别 $l$ 的特征通道数由 $C_l = 2^{l} c$ 给出。级别 $l$ 的空间面积为 $A_l = \\frac{A_0}{4^{l}}$。对于一个输出面积为 $A$、卷积核大小为 $k \\times k$、$C_{\\text{in}}$ 个输入通道和 $C_{\\text{out}}$ 个输出通道的标准卷积，其FLOPs的计算公式为 $2 A k^{2} C_{\\text{in}} C_{\\text{out}}$。\n\n**1. 编码器路径FLOPs**\n对于每个级别 $l \\in \\{0, \\dots, L-1\\}$，有两个相同的 $k \\times k$ 卷积。对于这些卷积，输入和输出通道数均为 $C_{\\text{in}} = C_{\\text{out}} = C_l = 2^{l} c$。输出空间面积为 $A_l = \\frac{A_0}{4^{l}}$。该级别单个卷积的FLOPs为：\n$$F_{\\text{enc,conv},l} = 2 A_l k^2 C_l^2 = 2 \\left(\\frac{A_0}{4^l}\\right) k^2 (2^l c)^2 = 2 \\left(\\frac{A_0}{4^l}\\right) k^2 (4^l c^2) = 2 A_0 k^2 c^2$$\n由于在 $L$ 个编码器级别中的每一个都有两个这样的卷积，编码器路径的总FLOPs为：\n$$F_{\\text{enc}} = \\sum_{l=0}^{L-1} 2 \\times (2 A_0 k^2 c^2) = L \\times (4 A_0 k^2 c^2) = 4 L A_0 k^2 c^2$$\n编码器每个输入像素的FLOPs为 $f_{\\text{enc}} = \\frac{F_{\\text{enc}}}{A_0} = 4 L k^2 c^2$。\n\n**2. 瓶颈层FLOPs**\n在级别 $L$，有两个 $k \\times k$ 卷积，其 $C_{\\text{in}} = C_{\\text{out}} = C_L = 2^{L} c$，输出面积为 $A_L = \\frac{A_0}{4^{L}}$。其中一个卷积的FLOPs为：\n$$F_{\\text{bottle,conv}} = 2 A_L k^2 C_L^2 = 2 \\left(\\frac{A_0}{4^L}\\right) k^2 (2^L c)^2 = 2 A_0 k^2 c^2$$\n瓶颈层中有两个卷积，因此总瓶颈层FLOPs为 $F_{\\text{bottle}} = 2 \\times (2 A_0 k^2 c^2) = 4 A_0 k^2 c^2$。\n瓶颈层每个输入像素的FLOPs为 $f_{\\text{bottle}} = \\frac{F_{\\text{bottle}}}{A_0} = 4 k^2 c^2$。\n\n**3. 解码器路径FLOPs**\n对于每个级别 $l \\in \\{L-1, \\dots, 0\\}$，这对应于 $L$ 个不同的级别，我们计算转置卷积和两个标准卷积的FLOPs。\n\n- **转置卷积：** 这是一个步幅为2的 $2 \\times 2$ 操作，它将特征从级别 $l+1$ 上采样到级别 $l$。\n输入通道数：$C_{\\text{in}} = C_{l+1} = 2^{l+1} c$。\n输出通道数：$C_{\\text{out}} = C_l = 2^l c$。\n输出面积：$A_l = \\frac{A_0}{4^l}$。\n该操作的FLOPs为：\n$$F_{\\text{deconv},l} = 2 A_l (\\text{kernel\\_size})^2 C_{\\text{in}} C_{\\text{out}} = 2 \\left(\\frac{A_0}{4^l}\\right) (2^2) (2^{l+1} c)(2^l c)$$\n$$F_{\\text{deconv},l} = 8 \\left(\\frac{A_0}{4^l}\\right) (2 \\cdot 2^l c)(2^l c) = 16 A_0 c^2 \\frac{4^l}{4^l} = 16 A_0 c^2$$\n对 $L$ 个解码器级别求和，所有转置卷积的总FLOPs为 $F_{\\text{deconv}} = L \\times (16 A_0 c^2) = 16 L A_0 c^2$。每个输入像素的FLOPs为 $f_{\\text{deconv}} = 16 L c^2$。\n\n- **标准卷积：** 与跳跃连接拼接后，输入通道数为 $2^{l}c + 2^{l}c = 2^{l+1}c$。\n第一个 $k \\times k$ 卷积将通道数从 $2^{l+1} c$ 减少到 $2^l c$：\n$$F_{\\text{dec,conv1},l} = 2 A_l k^2 (2^{l+1}c)(2^l c) = 2 \\left(\\frac{A_0}{4^l}\\right) k^2 (2 \\cdot 2^l c)(2^l c) = 4 A_0 k^2 c^2 \\frac{4^l}{4^l} = 4 A_0 k^2 c^2$$\n第二个 $k \\times k$ 卷积将通道数从 $2^l c$ 映射到 $2^l c$：\n$$F_{\\text{dec,conv2},l} = 2 A_l k^2 (2^l c)(2^l c) = 2 \\left(\\frac{A_0}{4^l}\\right) k^2 (2^l c)^2 = 2 A_0 k^2 c^2 \\frac{4^l}{4^l} = 2 A_0 k^2 c^2$$\n每个解码器级别 $l$ 的标准卷积总FLOPs为 $F_{\\text{dec,convs},l} = 4 A_0 k^2 c^2 + 2 A_0 k^2 c^2 = 6 A_0 k^2 c^2$。\n对 $L$ 个级别求和得到 $F_{\\text{dec,convs}} = L \\times (6 A_0 k^2 c^2) = 6 L A_0 k^2 c^2$。每个输入像素的FLOPs为 $f_{\\text{dec,convs}} = 6 L k^2 c^2$。\n\n**每个输入像素的总FLOPs**\n每个输入像素的总FLOPs，$F(k,c,L)$，是所有部分贡献的总和：\n$$F(k,c,L) = f_{\\text{enc}} + f_{\\text{bottle}} + f_{\\text{deconv}} + f_{\\text{dec,convs}}$$\n$$F(k,c,L) = 4 L k^2 c^2 + 4 k^2 c^2 + 16 L c^2 + 6 L k^2 c^2$$\n合并同类项：\n$$F(k,c,L) = (4L + 4 + 6L) k^2 c^2 + 16 L c^2$$\n$$F(k,c,L) = (10L + 4) k^2 c^2 + 16 L c^2$$\n对表达式进行因式分解，得到最终的封闭形式：\n$$F(k,c,L) = 2 k^2 c^2 (5L + 2) + 16 L c^2$$\n\n**任务2：与性能分析器报告的一致性**\n归因于转置卷积的总FLOPs分数 $\\eta$ 为：\n$$\\eta = \\frac{f_{\\text{deconv}}}{F(k,c,L)} = \\frac{16 L c^2}{2 k^2 c^2 (5L + 2) + 16 L c^2} = \\frac{16 L}{2 k^2 (5L + 2) + 16 L}$$\n代入给定值 $k=3$ 和 $L=4$：\n$$\\eta = \\frac{16 \\times 4}{2 \\times 3^2 (5 \\times 4 + 2) + 16 \\times 4} = \\frac{64}{2 \\times 9 \\times 22 + 64} = \\frac{64}{396 + 64} = \\frac{64}{460}$$\n简化并计算该分数：\n$$\\eta = \\frac{16}{115} \\approx 0.13913$$\n约 $0.139$ 的理论值与性能分析器报告的约 $0.14$ 的值是一致的。\n\n**任务3：上采样路径的修改**\n提议的修改将每个 $2 \\times 2$ 转置卷积替换为非学习的双线性上采样，后跟一个 $1 \\times 1$ 卷积。非参数化上采样的成本被忽略。新的 $1 \\times 1$ 卷积的成本计算如下。\n在每个解码器级别 $l$，该卷积的卷积核大小为 $k=1$，输入通道数为 $C_{\\text{in}}=2^{l+1}c$，输出通道数为 $C_{\\text{out}}=2^l c$，输出面积为 $A_l = \\frac{A_0}{4^l}$。这个新操作的每个输入像素的FLOPs，$f'_{\\text{upsample},l}$，为：\n$$f'_{\\text{upsample},l} = \\frac{2 A_l (1^2) C_{\\text{in}} C_{\\text{out}}}{A_0} = \\frac{2 (\\frac{A_0}{4^l}) (1) (2^{l+1}c)(2^l c)}{A_0} = 2 \\left(\\frac{1}{4^l}\\right) (2 \\cdot 2^l c)(2^l c) = 4 c^2$$\n这取代了原来每个级别的转置卷积成本，即 $f_{\\text{deconv},l} = 16 c^2$。每个解码器级别每像素FLOPs的变化量为 $\\Delta f_l = 4 c^2 - 16 c^2 = -12 c^2$。\n由于此修改应用于所有 $L$ 个解码器级别，每个输入像素FLOPs的总变化量为：\n$$\\Delta F = \\sum_{l=0}^{L-1} (-12 c^2) = -12 L c^2$$\n此修改改变了 $F(k,c,L)$ 中代表转置卷积总成本的项。原始项 $16 L c^2$ 被新的上采样操作总成本 $L \\times (4c^2) = 4Lc^2$ 所取代。变化是每个输入像素减少了 $12 L c^2$ 的FLOPs。",
            "answer": "$$\\boxed{2 k^2 c^2 (5L + 2) + 16 L c^2}$$"
        },
        {
            "introduction": "理论上的计算成本只是故事的一部分；在实践中，模型训练往往受限于硬件的内存容量。本练习聚焦于一种强大的内存优化技术——梯度检查点（gradient checkpointing），并让你通过编程实现来量化其核心权衡。你将推导并计算启用该技术后所节省的内存比例，以及由此带来的额外计算时间开销，从而深刻理解在资源受限环境下内存与计算之间的博弈。",
            "id": "3193838",
            "problem": "给定一个U-Net架构的简化但有原则的计算模型，用于研究梯度检查点（gradient checkpointing）对激活内存使用和训练时间计算量的影响。目标是从第一性原理推导激活内存和计算时间的公式，然后实现一个程序，根据给定的配置参数测试套件，计算内存节省分数和训练时间开销。\n\n假设一个U-Net具有$L$个下采样步骤和$L$个上采样步骤，并在深度$L$处有一个瓶颈层。在每个层级$l \\in \\{0,\\dots,L\\}$，空间分辨率为$H_l = H_0 / 2^l$和$W_l = W_0 / 2^l$，通道数为$C_l = C_0 \\cdot 2^l$。每个层级（包括$l = L$处的瓶颈层）在编码器中包含$B$个卷积块，在解码器中也包含$B$个卷积块，但瓶颈层除外，它只有一次$B$个块。因此，卷积块的总数为$N = B \\cdot (2L + 1)$。假设每个元素存储为$b$字节（对于$32$位浮点数，$b = 4$字节）。每个激活张量被建模为一个大小为$H_l \\cdot W_l \\cdot C_l$个元素的数组。\n\n基本原理：\n- 微分链式法则意味着训练步骤需要访问中间激活以进行反向传播。在实践中，实现方式是在前向传播期间存储这些激活，以避免重新计算。\n- 梯度检查点只存储激活的一个子集，并在反向传播期间动态地重新计算省略的激活，从而用额外的计算换取内存。\n- 对于非检查点基线，前向传播阶段的峰值激活内存被建模为所有必须存储到反向传播时的中间激活张量的总和，这相当于存储每个卷积块的输出。\n- 使用仅边界检查点策略，我们在前向传播期间只持久化存储每个层级末端的编码器激活（也作为跳跃连接）和瓶颈层的输出；其他激活在反向传播期间重新计算。此模型将前向阶段的峰值近似为这些检查点的总和。\n- 计算时间建模使用每个块的前向成本$c_f$和后向成本$c_b$（单位为任意但一致的时间单位）。基线训练执行$N$个前向块和$N$个后向块。在边界检查点下，每个层级的段长度为$B$，反向传播需要为每个段额外进行$(B-1)$次前向块计算。共有$(2L + 1)$个这样的段。\n\n您的任务：\n1) 推导基线峰值激活内存$M_0$（以字节为单位），作为$H_0$、$W_0$、$C_0$、$L$、$B$和$b$的函数。使用$H_l$、$W_l$和$C_l$的定义。设$S_l = H_l \\cdot W_l \\cdot C_l$。在基线（无检查点）中，每个卷积块的输出都被存储。将$M_0$表示为对$l$的求和，其中包含适当计数的$S_l$乘以$b$。\n\n2) 根据所述的仅边界检查点策略，推导检查点化的前向阶段峰值激活内存$M_1$（以字节为单位）。只持久化存储$L$个编码器层级末端的激活（层级$l \\in \\{0,\\dots,L-1\\}$）加上瓶颈层激活（层级$L$）。用$S_l$和$b$表示$M_1$。\n\n3) 将内存节省分数$\\eta$定义为\n$$\n\\eta = \\frac{M_0 - M_1}{M_0}。\n$$\n清楚地说明$\\eta$是$[0,1]$范围内的十进制数。\n\n4) 推导基线每次迭代的计算时间$T_0$和检查点化的每次迭代的计算时间$T_1$。使用基线执行$N$个前向块计算和$N$个后向块计算的模型，因此$T_0 = N \\cdot c_f + N \\cdot c_b$。对于检查点化，加上重新计算的成本，该成本等于$(2L + 1) \\cdot (B - 1) \\cdot c_f$（因为有$(2L + 1)$个长度为$B$的段）。因此，\n$$\nT_1 = N \\cdot c_f + N \\cdot c_b + (2L + 1) \\cdot (B - 1) \\cdot c_f。\n$$\n\n5) 将训练时间乘法开销因子$\\gamma$定义为\n$$\n\\gamma = \\frac{T_1}{T_0}，\n$$\n并将分数开销定义为$\\gamma - 1$。您应该报告$\\gamma$。\n\n实现要求：\n- 实现一个程序，对于以下测试套件中的每个测试用例，仅使用推导出的公式计算$M_0$（以字节为单位）、$M_1$（以字节为单位）、$\\eta$和$\\gamma$。所有空间维度都是可以被$2^L$整除的整数；假设是精确的整数除法。使用$b = 4$字节。\n- 输出格式：对于整个测试套件，打印一个单行，包含一个用方括号括起来的逗号分隔列表。这个顶层列表中的每个元素对应一个测试用例，并且本身是一个包含四个值的列表，顺序如下：$[\\eta, \\gamma, M_0, M_1]$。前两个条目必须是精确到$6$位小数的十进制浮点数，后两个条目必须是表示字节的整数。例如：$[[0.123456,1.234567,123,45],[\\dots],\\dots]$。输出字符串中不允许有任何空格。\n\n测试套件：\n- 情况1：$H_0 = 256$, $W_0 = 256$, $C_0 = 64$, $L = 4$, $B = 2$, $c_f = 1.0$, $c_b = 2.0$, $b = 4$。\n- 情况2：$H_0 = 64$, $W_0 = 64$, $C_0 = 16$, $L = 2$, $B = 1$, $c_f = 1.0$, $c_b = 2.0$, $b = 4$。\n- 情况3：$H_0 = 512$, $W_0 = 512$, $C_0 = 32$, $L = 3$, $B = 3$, $c_f = 1.0$, $c_b = 2.0$, $b = 4$。\n- 情况4：$H_0 = 128$, $W_0 = 128$, $C_0 = 32$, $L = 5$, $B = 3$, $c_f = 1.0$, $c_b = 2.0$, $b = 4$。\n\n角度单位不适用。所有内存量必须以字节报告。所有分数答案必须是十进制数，而不是百分比。您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表的结果（例如，$ [result_1,result_2,\\dots]$），其中每个$result_i$是指定的四元素列表$[\\eta,\\gamma,M_0,M_1]$。",
            "solution": "该问题是有效的，因为它在科学上基于深度学习的原理，问题提出得很好，信息充分且一致，并且陈述客观。所提供的模型虽然经过简化，但是一个用于分析梯度检查点权衡的一致且可形式化的框架。我们继续进行推导和实现。\n\n按照要求，解答分五步推导，建立了指定U-Net模型下内存使用和计算时间的数学公式。\n\n1) 基线峰值激活内存 $M_0$ 的推导。\n\n基线策略要求存储每个卷积块的输出激活张量，直到反向传播。U-Net架构包括一个编码器路径、一个瓶颈层和一个解码器路径。\n- 编码器有$L$个层级，索引为$l \\in \\{0, \\dots, L-1\\}$，每个层级有$B$个块。\n- 瓶颈层在$l=L$层级，有$B$个块。\n- 解码器有$L$个层级，索引为$l \\in \\{L-1, \\dots, 0\\}$，每个层级有$B$个块。\n\n在层级$l$的激活张量大小为$S_l = H_l \\cdot W_l \\cdot C_l$。使用给定的定义，$H_l = H_0 / 2^l$，$W_l = W_0 / 2^l$和$C_l = C_0 \\cdot 2^l$，我们可以将$S_l$表示为：\n$$S_l = \\left(\\frac{H_0}{2^l}\\right) \\left(\\frac{W_0}{2^l}\\right) (C_0 \\cdot 2^l) = H_0 W_0 C_0 \\frac{1}{2^l}$$\n假设所有空间维度在除法后都是整数。\n\n总内存$M_0$是所有块输出的内存总和。由于在编码器、解码器和瓶颈层的每个层级都有$B$个块，并且每个元素需要$b$个字节：\n- 编码器块的内存：$\\sum_{l=0}^{L-1} B \\cdot S_l \\cdot b$。\n- 瓶颈层块的内存：$B \\cdot S_L \\cdot b$。\n- 解码器块的内存：$\\sum_{l=0}^{L-1} B \\cdot S_l \\cdot b$。注意，解码器层级在空间分辨率和通道数方面与编码器层级镜像对应，因此求和是基于相同的$S_l$项。\n\n将这些贡献相加得到总基线内存$M_0$：\n$$M_0 = \\left(\\sum_{l=0}^{L-1} B \\cdot S_l \\cdot b\\right) + (B \\cdot S_L \\cdot b) + \\left(\\sum_{l=0}^{L-1} B \\cdot S_l \\cdot b\\right)$$\n$$M_0 = 2B b \\sum_{l=0}^{L-1} S_l + B b S_L$$\n提取公因式，我们得到$M_0$的公式：\n$$M_0 = b \\cdot B \\left( S_L + 2\\sum_{l=0}^{L-1} S_l \\right)$$\n\n2) 检查点化的前向阶段峰值激活内存$M_1$的推导。\n\n在仅边界检查点策略下，我们只存储$L$个编码器层级中每一个的单个输出激活张量以及瓶颈层的单个输出激活张量。\n- 对于每个编码器层级$l \\in \\{0, \\dots, L-1\\}$，我们存储一个大小为$S_l$的激活。\n- 对于瓶颈层级$l=L$，我们存储一个大小为$S_L$的激活。\n\n总检查点内存$M_1$是这$L+1$个张量大小的总和，乘以字节大小$b$：\n$$M_1 = \\left(\\sum_{l=0}^{L-1} S_l \\cdot b\\right) + (S_L \\cdot b)$$\n这可以简化为：\n$$M_1 = b \\sum_{l=0}^{L} S_l$$\n\n3) 内存节省分数$\\eta$的定义。\n\n内存节省分数$\\eta$定义为相对于基线的内存减少量，并由基线内存归一化。它是一个无量纲的量，表示节省的内存部分。\n$$\\eta = \\frac{M_0 - M_1}{M_0} = 1 - \\frac{M_1}{M_0}$$\n由于$M_1 \\le M_0$，$\\eta$的值是范围在$[0, 1]$内的十进制数。\n\n4) 基线计算时间$T_0$和检查点化计算时间$T_1$的推导。\n\n问题提供了计算时间的模型。卷积块的总数为$N = B(2L+1)$。每个块的前向计算成本为$c_f$，后向计算成本为$c_b$。\n基线训练迭代包括对所有$N$个块进行一次前向传播和一次后向传播。\n$$T_0 = N \\cdot c_f + N \\cdot c_b = N(c_f + c_b) = B(2L+1)(c_f + c_b)$$\n对于检查点化的情况，反向传播需要重新计算未存储的激活。模型规定，这会为$(2L+1)$个段中的每一个（编码器、瓶颈层和解码器中每个层级一个段）带来$(B-1)$次前向计算的额外成本。\n总的检查点化计算时间$T_1$是基线时间加上这个重新计算的开销：\n$$T_1 = T_0 + (2L+1)(B-1)c_f$$\n代入$T_0$的表达式：\n$$T_1 = B(2L+1)(c_f+c_b) + (2L+1)(B-1)c_f$$\n\n5) 训练时间乘法开销因子$\\gamma$的定义。\n\n开销因子$\\gamma$是检查点化训练时间与基线训练时间的比率。\n$$\\gamma = \\frac{T_1}{T_0}$$\n代入$T_1$和$T_0$的表达式：\n$$\\gamma = \\frac{B(2L+1)(c_f+c_b) + (2L+1)(B-1)c_f}{B(2L+1)(c_f+c_b)}$$\n项$(2L+1)$可以消掉，得到一个与网络深度$L$无关的$\\gamma$的简化表达式：\n$$\\gamma = \\frac{B(c_f+c_b) + (B-1)c_f}{B(c_f+c_b)} = 1 + \\frac{(B-1)c_f}{B(c_f+c_b)}$$\n该因子表示由于检查点化导致的计算时间的乘法增加。当$B=1$时，段内没有重新计算，$\\gamma=1$，表示没有开销。当$B>1$时，$\\gamma>1$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Calculates activation memory and compute overhead for U-Net with gradient checkpointing.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (H0, W0, C0, L, B, cf, cb, b)\n        (256, 256, 64, 4, 2, 1.0, 2.0, 4),\n        (64, 64, 16, 2, 1, 1.0, 2.0, 4),\n        (512, 512, 32, 3, 3, 1.0, 2.0, 4),\n        (128, 128, 32, 5, 3, 1.0, 2.0, 4),\n    ]\n\n    results_str = []\n    \n    for case in test_cases:\n        H0, W0, C0, L, B, cf, cb, b = case\n\n        # Calculate S_l for l = 0 to L\n        s_values = []\n        for l in range(L + 1):\n            # As per problem, assume exact integer division for spatial dimensions\n            H_l = H0 // (2**l)\n            W_l = W0 // (2**l)\n            C_l = C0 * (2**l)\n            s_l = H_l * W_l * C_l\n            s_values.append(s_l)\n\n        # 1. Calculate M0 (Baseline Memory)\n        # M0 = b * B * (S_L + 2 * sum_{l=0}^{L-1} S_l)\n        sum_s_0_to_L_minus_1 = sum(s_values[0:L])\n        s_L = s_values[L]\n        M0 = b * B * (s_L + 2 * sum_s_0_to_L_minus_1)\n        M0 = int(M0)\n\n        # 2. Calculate M1 (Checkpointed Memory)\n        # M1 = b * sum_{l=0}^{L} S_l\n        sum_s_0_to_L = sum(s_values)\n        M1 = b * sum_s_0_to_L\n        M1 = int(M1)\n\n        # 3. Calculate eta (Memory Saved Fraction)\n        if M0 == 0:\n            # Avoid division by zero, though not expected in these test cases\n            eta = 0.0\n        else:\n            eta = (M0 - M1) / M0\n\n        # 4. Calculate gamma (Compute Overhead Factor)\n        # Simplified formula: gamma = 1 + ((B-1)*cf) / (B*(cf+cb))\n        if B == 0 or (cf + cb) == 0:\n             # Avoid division by zero, though B >= 1 in all tests\n            gamma = 1.0\n        else:\n            gamma = 1.0 + ((B - 1) * cf) / (B * (cf + cb))\n\n        # Format the results for the current test case as a string\n        # eta and gamma are rounded to 6 decimal places via f-string formatting.\n        # M0 and M1 are integers.\n        case_result_str = f\"[{eta:.6f},{gamma:.6f},{M0},{M1}]\"\n        results_str.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    # e.g., [[eta1,gamma1,M0_1,M1_1],[eta2,gamma2,M0_2,M1_2]]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个强大的架构必须与一个合适的目标函数相结合才能有效学习。对于U-Net的主要应用——语义分割，尤其是在处理类别不平衡的数据时，损失函数的选择至关重要。本练习将引导你从梯度的角度分析像素级损失（如二元交叉熵）和区域级损失（如Dice损失）的根本差异，并据此构建一个能够根据类别不平衡程度自适应调整的混合损失策略。",
            "id": "3193860",
            "problem": "一个二元语义分割任务通过U-Net架构来解决，该架构包含一个压缩空间分辨率的编码器和一个使用跳跃连接来桥接相应编码器和解码器阶段的解码器，以扩展分辨率。U-Net通过对解码器的logits进行logistic sigmoid变换，为每个像素索引$i$输出一个概率$\\hat{y}_i \\in (0,1)$。考虑使用逐像素损失进行训练，其中真实标签$y_i \\in \\{0,1\\}$，并且在给定数据集中，前景（正）类的流行率为$p = \\mathbb{P}(y=1)$。假设批次足够大，以至于经验比例能紧密地追踪$p$。两种常见的损失函数族是二元交叉熵（BCE）损失和基于重叠的损失（例如，Soft Dice损失），后者通过预测值和标签的总和进行归一化，以强调集合级别的重叠。假设使用标准的、未加权的BCE和不带类别权重的标准Soft Dice损失。我们考虑的混合损失是$L = \\lambda L_{\\text{Dice}} + (1 - \\lambda) L_{\\text{BCE}}$，其中混合参数$\\lambda \\in [0,1]$，在多类别情况下，逐通道应用。对于评估，关注交并比（IoU），它与Dice系数密切相关。\n\n从基本原理出发：\n- BCE在像素级别是一个适当的评分规则，并产生与预测概率和目标标签之间差异成正比的逐像素梯度，因此它在所有像素上进行聚合，而没有显式的类别大小归一化。\n- Soft Dice损失可微地近似Dice重叠度，并包含通过预测和标签的总和进行的归一化，这倾向于平衡来自前景和背景的贡献，而与$p$无关。\n\n基于这些原理进行推理，比较未加权的sigmoid + BCE与Dice + BCE混合损失在不同$p$值下的行为。特别地，推理在初始化附近（通常$\\hat{y} \\approx 0.5$）BCE下负样本与正样本的预期总梯度贡献，以及由于归一化导致的Dice损失的近似$p$-不变性。利用这一点来描绘IoU的优势区域：对于哪些$p$的范围，您预期混合损失会优于纯BCE，反之亦然？然后，为每个类别通道提出一个有原则的、特定于类别的规则来选择作为$p$的函数的$\\lambda$，旨在将总梯度不平衡保持在选定的容差范围内，并解释其基本原理。\n\n选择与上述第一性原理推理最一致的陈述。\n\n选项：\n\nA. 当前景流行率$p$很小（$p \\ll 0.5$）时，一个混合损失$L = \\lambda L_{\\text{Dice}} + (1 - \\lambda) L_{\\text{BCE}}$，其中$\\lambda$随$\\lvert p - 0.5 \\rvert$增加，通常会产生比纯$L_{\\text{BCE}}$更优的IoU。一个方便的、特定于类别的规则是$\\lambda(p) = 1 - 2 \\lvert p - 0.5 \\rvert$，裁剪到$[0,1]$范围内，并应用于每个类别通道；在$p \\approx 1$附近，倾向于使用较小的$\\lambda$。\n\nB. 纯$L_{\\text{BCE}}$直接优化IoU，并且对任何$p$值的类别不平衡都不敏感，因此它对所有$p$值都是最优的；基于重叠的归一化损失会增加不必要的偏差。\n\nC. 如果$p$非常小，应始终设置$\\lambda = 1$（纯$L_{\\text{Dice}}$），如果$p$非常大，则始终设置$\\lambda = 0$（纯$L_{\\text{BCE}}$）；混合对校准或稳定性从来没有帮助。\n\nD. 一个有原则的混合方法是$\\lambda(p) = p$：当前景变得普遍（$p$大）时，更多地依赖$L_{\\text{Dice}}$，当$p$小时，更多地依赖$L_{\\text{BCE}}$。\n\nE. 在初始化附近，未加权$L_{\\text{BCE}}$下负样本与正样本的总梯度量级的预期比率约为$(1-p)/p$，而$L_{\\text{Dice}}$由于其归一化而对$p$近似不变。因此，选择一个容差$k > 1$作为可接受的不平衡度，并设置混合比例，以使有效比率不超过$k$：当$\\max\\{(1-p)/p,\\, p/(1-p)\\} > k$时，要求$\\lambda \\ge \\dfrac{\\max\\left\\{\\frac{1-p}{p}, \\frac{p}{1-p}\\right\\} - k}{\\max\\left\\{\\frac{1-p}{p}, \\frac{p}{1-p}\\right\\} - 1}$；否则使用一个较小的$\\lambda$（例如$\\lambda \\le 0.5$）。这将优势区域映射如下：当$p$极端时（接近0或接近1），Dice + BCE混合损失在IoU上占主导地位，而当$p \\approx 0.5$时，纯BCE或低$\\lambda$的混合损失就足够了。通过估计每个类别的流行率$p_c$，将相同的逐类别$\\lambda(p)$规则应用于多类别U-Net。",
            "solution": "用户要求对问题陈述进行批判性验证，然后根据第一性原理进行推导并评估给定的选项。\n\n### 问题验证\n\n#### 第1步：提取已知条件\n\n*   **任务**：二元语义分割。\n*   **架构**：U-Net。\n*   **输出**：通过logistic sigmoid变换得到的每个像素的概率$\\hat{y}_i \\in (0,1)$。\n*   **真实标签**：每个像素的标签$y_i \\in \\{0,1\\}$。\n*   **类别流行率**：前景（正）类的流行率为$p = \\mathbb{P}(y=1)$。\n*   **假设**：批次足够大，以至于经验比例近似于$p$。\n*   **损失函数**：\n    *   $L_{\\text{BCE}}$：未加权的二元交叉熵。\n    *   $L_{\\text{Dice}}$：标准的Soft Dice损失，一种基于重叠的损失。\n*   **混合损失**：$L = \\lambda L_{\\text{Dice}} + (1 - \\lambda) L_{\\text{BCE}}$，其中$\\lambda \\in [0,1]$。\n*   **多类别扩展**：损失函数逐通道应用。\n*   **评估指标**：交并比（IoU）。\n*   **前提1 (BCE)**：$L_{\\text{BCE}}$是一个适当的评分规则；像素级梯度与$\\hat{y}_i - y_i$成正比；聚合时没有类别大小归一化。\n*   **前提2 (Dice)**：$L_{\\text{Dice}}$近似Dice重叠度；包含通过预测和标签的总和进行的归一化；倾向于平衡前景和背景的贡献，而不管$p$如何。\n*   **推理假设**：分析在初始化附近进行，此时$\\hat{y} \\approx 0.5$是常见情况。\n\n#### 第2步：使用提取的已知条件进行验证\n\n*   **科学依据**：该问题牢固地植根于计算机视觉深度学习的成熟领域。U-Net、BCE损失、Dice损失、IoU以及类别不平衡的挑战都是标准和基本的概念。对BCE和Dice损失行为的描述是对其数学性质的准确提炼。\n*   **问题定义良好**：问题定义清晰。它要求在不同条件（$p$）下对损失函数的行为进行合乎逻辑的比较，并选择与此推理最一致的选项。所提供的前提足以推导出逻辑结论。\n*   **客观性**：问题以精确、客观和技术性的语言陈述，没有主观性或歧义。\n\n问题陈述通过了所有验证标准。它没有表现出任何列出的缺陷（例如，科学上不健全、不完整、有歧义）。\n\n#### 第3步：结论与行动\n\n*   **结论**：问题是**有效的**。\n*   **行动**：继续进行解题推导。\n\n### 解题推导\n\n问题的核心是理解类别不平衡（由前景流行率$p$量化）如何影响U-Net在不同损失函数下的训练动态，并因此影响由IoU衡量的最终性能。\n\n#### 1. $L_{\\text{BCE}}$与类别不平衡分析\n\n单个像素$i$的二元交叉熵损失由下式给出：\n$$L_{\\text{BCE}, i} = -[y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]$$\n该损失相对于logit $z_i$（其中$\\hat{y}_i = \\sigma(z_i) = 1/(1+e^{-z_i})$）的梯度为：\n$$\\frac{\\partial L_{\\text{BCE}, i}}{\\partial z_i} = \\hat{y}_i - y_i$$\n总损失是批次中所有$N$个像素的平均值，$L_{\\text{BCE}} = \\frac{1}{N} \\sum_{i=1}^N L_{\\text{BCE}, i}$。相对于任何网络参数的总梯度是这些逐像素贡献的总和。\n\n根据问题的设定，我们来分析初始化附近的情况，此时对所有$i$都有$\\hat{y}_i \\approx 0.5$。\n正样本像素（前景，$y_i=1$）的期望数量是$N \\cdot p$。\n负样本像素（背景，$y_i=0$）的期望数量是$N \\cdot (1-p)$。\n\n来自正样本像素的总梯度贡献与以下成正比：\n$$\\sum_{i \\text{ s.t. } y_i=1} (\\hat{y}_i - y_i) \\approx \\sum_{i \\text{ s.t. } y_i=1} (0.5 - 1) = -0.5 \\cdot (N \\cdot p)$$\n来自负样本像素的总梯度贡献与以下成正比：\n$$\\sum_{i \\text{ s.t. } y_i=0} (\\hat{y}_i - y_i) \\approx \\sum_{i \\text{ s.t. } y_i=0} (0.5 - 0) = 0.5 \\cdot (N \\cdot (1-p))$$\n\n负样本总梯度量级与正样本总梯度量级的比率为：\n$$\\frac{|0.5 \\cdot N \\cdot (1-p)|}{|-0.5 \\cdot N \\cdot p|} = \\frac{1-p}{p}$$\n如果$p$很小（例如，$p=0.01$），这个比率是$\\frac{0.99}{0.01} = 99$。来自背景像素的梯度以99倍的优势压倒了来自前景像素的梯度。因此，网络被激励在任何地方都预测为背景，导致前景类的IoU非常低。反之，如果$p$很大（例如，$p=0.99$），比率是$\\frac{0.01}{0.99} \\approx 0.01$，前景梯度占主导。\n因此，未加权的$L_{\\text{BCE}}$对类别不平衡高度敏感。当$p$远非0.5时，它在像IoU这样的重叠度量上的表现会显著下降。\n\n#### 2. $L_{\\text{Dice}}$与类别不平衡分析\n\nSoft Dice损失通常表示为$L_{\\text{Dice}} = 1 - D_s$，其中Soft Dice系数$D_s$是：\n$$D_s = \\frac{2 \\sum_i y_i \\hat{y}_i + \\epsilon}{\\sum_i y_i^2 + \\sum_i \\hat{y}_i^2 + \\epsilon} \\quad \\text{或类似的变体，如} \\quad D_s = \\frac{2 \\sum_i y_i \\hat{y}_i + \\epsilon}{\\sum_i y_i + \\sum_i \\hat{y}_i + \\epsilon}$$\n正如问题陈述中指出的，关键特征是分母，它涉及对所有预测和标签的求和。该项起到归一化因子的作用。损失是基于整个批次/图像的聚合统计数据计算的，而不是简单平均的逐像素基础。这种结构使得损失关注于预测集合与真实集合之间的重叠，而这也是IoU所度量的。通过按预测集和真实集的大小进行归一化，它内在地平衡了前景和背景的重要性。如果一个小的前景目标被漏掉，它会对损失产生显著贡献，无论有多少背景像素被正确分类。因此，$L_{\\text{Dice}}$对类别流行率$p$相对不敏感。\n\n#### 3. 混合损失与最优混合\n\n基于以上性质：\n*   当$p \\approx 0.5$（类别平衡）时，$L_{\\text{BCE}}$是有效的。其梯度表现良好，并且作为一个适当的评分规则，它鼓励概率校准良好。\n*   当$p \\to 0$或$p \\to 1$（极端不平衡）时，$L_{\\text{BCE}}$被多数类主导，导致IoU较差。在这些情况下，$L_{\\text{Dice}}$因其内在的平衡性而更优。\n\n混合损失$L = \\lambda L_{\\text{Dice}} + (1 - \\lambda) L_{\\text{BCE}}$允许进行权衡。混合参数$\\lambda$应根据不平衡的程度来选择。\n*   对于低不平衡度（$p \\approx 0.5$），$\\lambda$应该较小。\n*   对于高不平衡度（$p \\to 0$或$p \\to 1$），$\\lambda$应该较大。\n\n不平衡的程度可以由$\\max\\{\\frac{1-p}{p}, \\frac{p}{1-p}\\}$量化。一个有原则的选择$\\lambda$的规则应该使$\\lambda$成为这个不平衡因子的增函数。\n\n### 逐项分析\n\n*   **A. 当前景流行率$p$很小（$p \\ll 0.5$）时，一个混合...其中$\\lambda$随$\\lvert p - 0.5 \\rvert$增加，通常会产生更优的IoU...一个方便的、特定于类别的规则是$\\lambda(p) = 1 - 2 \\lvert p - 0.5 \\rvert$，裁剪到$[0,1]$范围内...**\n    *   陈述的第一部分是正确的，即一个$\\lambda$随不平衡度（由$\\lvert p - 0.5 \\rvert$衡量）增加的混合损失更优。然而，所提出的规则$\\lambda(p) = 1 - 2 \\lvert p - 0.5 \\rvert$根本上是错误的。对于极端不平衡（$p=0$或$p=1$），此规则给出$\\lambda = 1 - 2(0.5) = 0$，推荐纯BCE。对于完美平衡（$p=0.5$），它给出$\\lambda = 1 - 2(0) = 1$，推荐纯Dice。这与期望的行为完全相反。\n    *   **结论**：不正确。\n\n*   **B. 纯$L_{\\text{BCE}}$直接优化IoU，并且对任何$p$值的类别不平衡都不敏感，因此它对所有$p$值都是最优的；基于重叠的归一化损失会增加不必要的偏差。**\n    *   这个陈述包含几个错误的论断。$L_{\\text{BCE}}$并不直接优化IoU；它优化的是逐像素的对数似然。如上所述，它对类别不平衡高度敏感。基于重叠的归一化损失是对抗由不平衡引入的偏差的主要工具，而不是增加偏差。\n    *   **结论**：不正确。\n\n*   **C. 如果$p$非常小，应始终设置$\\lambda = 1$（纯$L_{\\text{Dice}}$），如果$p$非常大，则始终设置$\\lambda = 0$（纯$L_{\\text{BCE}}$）；混合对校准或稳定性从来没有帮助。**\n    *   对小$p$的建议是合理的。然而，对大$p$（例如，$p=0.99$）的建议是有缺陷的。这也是一个极端不平衡的情况，其中背景类是稀有的，同样的逻辑也适用：$L_{\\text{BCE}}$会有偏差，而$L_{\\text{Dice}}$会有帮助。声称混合“从来没有帮助”是一个过于绝对和错误的泛化；混合损失被广泛使用，正是因为它们通常能提高性能和训练稳定性。\n    *   **结论**：不正确。\n\n*   **D. 一个有原则的混合方法是$\\lambda(p) = p$：当前景变得普遍（$p$大）时，更多地依赖$L_{\\text{Dice}}$，当$p$小时，更多地依赖$L_{\\text{BCE}}$。**\n    *   这个规则建议，对于一个稀有的前景类（$p \\to 0$），应该使用$\\lambda \\to 0$，即纯$L_{\\text{BCE}}$。这恰恰是$L_{\\text{BCE}}$因不平衡而表现不佳的场景。这个规则是反直觉且有缺陷的。\n    *   **结论**：不正确。\n\n*   **E. 在初始化附近，未加权$L_{\\text{BCE}}$下负样本与正样本的总梯度量级的预期比率约为$(1-p)/p$，而$L_{\\text{Dice}}$由于其归一化而对$p$近似不变。因此，选择一个容差$k > 1$作为可接受的不平衡度，并设置混合比例，以使有效比率不超过$k$：当$\\max\\{(1-p)/p,\\, p/(1-p)\\} > k$时，要求$\\lambda \\ge \\dfrac{\\max\\left\\{\\frac{1-p}{p}, \\frac{p}{1-p}\\right\\} - k}{\\max\\left\\{\\frac{1-p}{p}, \\frac{p}{1-p}\\right\\} - 1}$；否则使用一个较小的$\\lambda$（例如$\\lambda \\le 0.5$）。这将优势区域映射如下：当$p$极端时（接近0或接近1），Dice + BCE混合损失在IoU上占主导地位，而当$p \\approx 0.5$时，纯BCE或低$\\lambda$的混合损失就足够了。通过估计每个类别的流行率$p_c$，将相同的逐类别$\\lambda(p)$规则应用于多类别U-Net。**\n    *   这个陈述正确地指出了BCE梯度不平衡比为$\\frac{1-p}{p}$（或其倒数）。它正确地指出$L_{\\text{Dice}}$近似不变。然后，它提出了一个有原则的方法来控制整体梯度不平衡。设不平衡因子为$I(p) = \\max\\{\\frac{1-p}{p}, \\frac{p}{1-p}\\}$。混合损失的有效不平衡度可以近似为$(1-\\lambda)I(p) + \\lambda$。将其设置为小于或等于容差$k$得到$(1-\\lambda)I(p) + \\lambda \\le k$。当$I(p) > k$时，解出$\\lambda$得到$\\lambda \\ge \\frac{I(p) - k}{I(p) - 1}$。所提出的公式是正确的。由此产生的优势区域映射（极端$p$时使用混合损失，在$p \\approx 0.5$时使用BCE主导的损失）与我们的第一性原理分析一致。对多类别情况的扩展是合乎逻辑的。这个选项完整并正确地概括了问题所要求的推理过程。\n    *   **结论**：正确。",
            "answer": "$$\\boxed{E}$$"
        }
    ]
}