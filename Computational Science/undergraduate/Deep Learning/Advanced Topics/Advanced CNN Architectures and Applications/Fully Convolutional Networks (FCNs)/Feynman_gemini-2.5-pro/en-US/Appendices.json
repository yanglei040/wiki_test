{
    "hands_on_practices": [
        {
            "introduction": "Architectures like U-Net are powerful tools for semantic segmentation, but their skip connections require precise spatial alignment between the encoder and decoder pathways. This exercise guides you through the fundamental calculations needed to ensure feature maps from different scales can be correctly concatenated . By mastering this, you will gain a core skill for building and debugging your own encoder-decoder networks without introducing spatial distortions.",
            "id": "3126516",
            "problem": "Consider a Fully Convolutional Network (FCN) with a U-Net-like encoder-decoder structure for two-dimensional images. The encoder has three scales. At each encoder scale, the operation sequence is: two convolutions with kernel size $3 \\times 3$, stride $1$, and zero padding, each followed by a Rectified Linear Unit (ReLU), and then a downsampling by max pooling with kernel size $s \\times s$ and stride $s$. The decoder inverts the spatial reductions by using at each decoder scale one transposed convolution (also called a deconvolution) with kernel size $s \\times s$, stride $s$, zero padding, and an integer output padding $o$ to be chosen so that spatial alignment occurs without distortion; this upsampling is followed by a skip connection concatenation and then two convolutions of kernel size $3 \\times 3$, stride $1$, and zero padding. Assume all convolutions and transposed convolutions use unit dilation, and all operations are applied equally in the height and width dimensions.\n\nThe input image is square with size $N \\times N$, where $N = 256$. The downsampling stride is $s = 2$ at every pooling layer, and the upsampling stride is also $s = 2$ at every transposed convolution. The skip connections concatenate the encoder feature map produced immediately before the pooling operation at the corresponding scale with the decoder feature map produced immediately after the transposed convolution at that scale. To ensure concatenation without any spatial distortion, the encoder feature map is symmetrically cropped in height and width (by the same number of pixels on each side) so that its spatial size exactly matches the decoder feature map at the moment of concatenation.\n\nUsing only fundamental definitions of convolutional output sizing and transposed convolutional output sizing, compute:\n- the required symmetric cropping (in pixels per side) for the encoder feature maps at scale $2$ and scale $1$ so that they align with their corresponding upsampled decoder feature maps at the moment of concatenation, and\n- the minimal integer output padding $o$ to use in each transposed convolution so that the upsampling achieves exact doubling without spatial distortion at the two decoder stages.\n\nNo rounding is required. Express the final answer as a row matrix containing, in order, the cropping per side at scale $2$, the cropping per side at scale $1$, the output padding for the transposed convolution at the decoder stage aligned with scale $2$, and the output padding for the transposed convolution at the decoder stage aligned with scale $1$.",
            "solution": "The user wants to determine the necessary cropping and output padding values for a U-Net-like architecture to ensure spatial alignment during skip connections.\n\nFirst, we establish the formulas for calculating the output spatial dimension ($H_{out}$) from an input dimension ($H_{in}$) for the relevant operations. All operations are symmetric in height and width, so we only need to analyze one dimension.\n\n1.  **Convolution**: For a convolution with kernel size $k$, stride $u$, and padding $p$, the output size is $H_{out} = \\lfloor \\frac{H_{in} + 2p - k}{u} \\rfloor + 1$. The problem specifies convolutions with $k=3$, $u=1$, and $p=0$.\n    $$H_{out} = \\left\\lfloor \\frac{H_{in} + 2(0) - 3}{1} \\right\\rfloor + 1 = H_{in} - 3 + 1 = H_{in} - 2$$\n    Each such convolution reduces the spatial dimension by $2$.\n\n2.  **Max Pooling**: For max pooling with kernel size $s$ and stride $s$, the output size is $H_{out} = \\lfloor \\frac{H_{in} - s}{s} \\rfloor + 1 = \\lfloor \\frac{H_{in}}{s} \\rfloor$. The problem specifies $s=2$.\n    $$H_{out} = \\left\\lfloor \\frac{H_{in}}{2} \\right\\rfloor$$\n\n3.  **Transposed Convolution**: For a transposed convolution with kernel size $k_t$, stride $u_t$, padding $p_t$, and output padding $o$, the general formula is $H_{out} = (H_{in} - 1)u_t - 2p_t + k_t + o$ (assuming unit dilation). The problem specifies $k_t=s$, $u_t=s$, and $p_t=0$, with $s=2$.\n    $$H_{out} = (H_{in} - 1)s - 2(0) + s + o = s \\cdot H_{in} + o = 2H_{in} + o$$\n\nNext, we trace the spatial dimensions through the network's encoder and decoder paths.\n\n**Encoder Path**\n\nThe input image has size $N=256$. Let $H_i$ be the input size to encoder scale $i$, and $H_{skip,i}$ be the size of the feature map produced at scale $i$ before pooling, which is used for the skip connection.\n\n*   **Input**: The initial input size is $H_0 = 256$.\n\n*   **Encoder Scale 1**:\n    *   Input to this scale is $H_0 = 256$.\n    *   The operation sequence is two convolutions. After the first convolution: $256 - 2 = 254$.\n    *   After the second convolution: $254 - 2 = 252$.\n    *   This is the feature map for the skip connection corresponding to this scale: $H_{skip1} = 252$.\n    *   A max pooling operation follows: $\\lfloor 252 / 2 \\rfloor = 126$. This is the input to the next scale.\n\n*   **Encoder Scale 2**:\n    *   Input to this scale is $126$.\n    *   After the first convolution: $126 - 2 = 124$.\n    *   After the second convolution: $124 - 2 = 122$.\n    *   This is the feature map for the skip connection: $H_{skip2} = 122$.\n    *   A max pooling operation follows: $\\lfloor 122 / 2 \\rfloor = 61$. This is the input to the next scale.\n\n*   **Encoder Scale 3 (Bottleneck)**:\n    *   Input to this scale is $61$.\n    *   After the first convolution: $61 - 2 = 59$.\n    *   After the second convolution: $59 - 2 = 57$.\n    *   This is the bottleneck feature map, $H_{bottleneck}=57$, which is the input to the decoder path.\n\n**Decoder Path**\n\nThe decoder path reconstructs the spatial dimensions. Let $c_i$ be the symmetric cropping per side and $o_i$ be the output padding for the decoder stage corresponding to encoder scale $i$.\n\n*   **First Decoder Stage (corresponding to Encoder Scale 2)**:\n    *   The input from the bottleneck is $H_{bottleneck} = 57$.\n    *   A transposed convolution with stride $s=2$ is applied. The output size is $D_{up2} = 2 \\cdot 57 + o_2 = 114 + o_2$.\n    *   This upsampled map must be concatenated with the feature map from the corresponding encoder scale, $H_{skip2} = 122$.\n    *   To match the sizes, $H_{skip2}$ is symmetrically cropped. The size after cropping is $H_{skip2} - 2c_2$.\n    *   The alignment condition is: $122 - 2c_2 = 114 + o_2$.\n    *   Rearranging gives: $2c_2 + o_2 = 8$.\n    *   We must find the minimal non-negative integer $o_2$ that yields an integer solution for $c_2$.\n        *   If $o_2 = 0$, then $2c_2 = 8 \\implies c_2 = 4$. This is a valid integer.\n        *   If $o_2 = 1$, then $2c_2 = 7 \\implies c_2 = 3.5$. This is not an integer crop.\n        *   If $o_2 = 2$, then $2c_2 = 6 \\implies c_2 = 3$. This is valid, but $o_2=2$ is not minimal.\n    *   The minimal integer output padding is $o_2=0$, which gives a cropping of $c_2 = 4$ pixels per side.\n    *   After concatenation, the feature map size is $114$. This map undergoes two convolutions:\n        *   After the first convolution: $114 - 2 = 112$.\n        *   After the second convolution: $112 - 2 = 110$. This is the input to the next decoder stage.\n\n*   **Second Decoder Stage (corresponding to Encoder Scale 1)**:\n    *   The input from the previous stage is $110$.\n    *   A transposed convolution with stride $s=2$ is applied. The output size is $D_{up1} = 2 \\cdot 110 + o_1 = 220 + o_1$.\n    *   This map is concatenated with the feature map from encoder scale 1, $H_{skip1} = 252$.\n    *   The alignment condition after cropping $H_{skip1}$ is: $252 - 2c_1 = 220 + o_1$.\n    *   Rearranging gives: $2c_1 + o_1 = 32$.\n    *   We find the minimal non-negative integer $o_1$ that yields an integer $c_1$.\n        *   If $o_1 = 0$, then $2c_1 = 32 \\implies c_1 = 16$. This is a valid integer.\n        *   If $o_1 = 1$, then $2c_1 = 31 \\implies c_1 = 15.5$. Not an integer.\n    *   The minimal integer output padding is $o_1=0$, which gives a cropping of $c_1 = 16$ pixels per side.\n\nThe computed values are:\n-   Cropping per side at scale 2: $c_2 = 4$.\n-   Cropping per side at scale 1: $c_1 = 16$.\n-   Output padding for the decoder stage aligned with scale 2: $o_2=0$.\n-   Output padding for the decoder stage aligned with scale 1: $o_1=0$.\n\nThe final answer is presented as a row matrix in the order `[c_2, c_1, o_2, o_1]`.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4 & 16 & 0 & 0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Modern deep learning often involves a trade-off between model accuracy, computational cost, and memory footprint. This practice challenges you to quantitatively analyze three different types of convolutional layers: standard, depthwise, and depthwise-separable . By calculating their parameter counts and theoretical latency, you will learn to make informed design decisions that balance predictive performance with computational efficiency, a crucial skill for deploying models on resource-constrained devices.",
            "id": "3126599",
            "problem": "Consider a semantic segmentation model built as a Fully Convolutional Network (FCN). Within one encoder block, you are evaluating three alternative convolutional layer designs that map an input tensor of spatial size $H \\times W$ with $M$ channels to an output with $N$ channels, all using stride $1$ and \"same\" padding so that $H_{\\text{out}} = H$ and $W_{\\text{out}} = W$. The three designs are: a standard $k \\times k$ convolution, a depthwise $k \\times k$ convolution with depth multiplier $\\gamma$ chosen so that the number of output channels equals $N$ (i.e., $\\gamma = \\frac{N}{M}$), and a depthwise-separable convolution consisting of a depthwise $k \\times k$ convolution (with depth multiplier $1$) followed by a pointwise $1 \\times 1$ convolution producing $N$ output channels. Bias terms are ignored in parameter counts. Assume each multiply-add pair counts as $2$ Floating-Point Operations (FLOPs), and all computations are dense.\n\nStart from the definition of discrete convolution and the notion that a convolution computes, per output position and per output channel, a weighted sum (dot product) of $k \\times k$ spatial neighborhoods across input channels. Derive expressions for parameter counts and FLOPs for each of the three designs based on these principles. Then, for the following setting:\n- $H = W = 128$,\n- $M = 64$,\n- $N = 128$,\n- $k = 3$,\n- hardware throughput $R = 5.0 \\times 10^{11}$ FLOPs per second,\n\ncompute the total parameter count and the total FLOPs for a single forward pass of each design on one image. Convert the FLOPs to latency by $L = \\frac{\\text{FLOPs}}{R}$ seconds and then to milliseconds. On a held-out validation set of the same dataset, the measured mean Intersection-over-Union (mIoU) for the three designs is:\n- standard convolution: $0.78$,\n- depthwise-only convolution (with $\\gamma = \\frac{N}{M}$): $0.74$,\n- depthwise-separable convolution: $0.77$.\n\nAmong these three, select the design that achieves at least $0.75$ mIoU and minimizes latency. Report the resulting latency in milliseconds. Round your answer to four significant figures. Express the final latency in milliseconds.",
            "solution": "The problem requires the derivation of parameter counts and computational costs (FLOPs) for three convolutional layer designs, followed by a numerical evaluation and a selection based on performance and latency criteria.\n\nLet the input tensor have dimensions $H \\times W \\times M$ and the output tensor have dimensions $H_{\\text{out}} \\times W_{\\text{out}} \\times N$. The problem specifies a stride of $s=1$ and \"same\" padding, which ensures the output spatial dimensions are identical to the input, i.e., $H_{\\text{out}} = H$ and $W_{\\text{out}} = W$. We are given that a multiply-add operation pair counts as $2$ Floating-Point Operations (FLOPs). Bias terms are to be ignored.\n\nThe fundamental operation in a convolutional layer is the computation of a weighted sum over a spatial neighborhood across all input channels. For an output at a specific location and for a specific output channel, this is a dot product between a filter and a corresponding patch of the input tensor.\n\n**Design 1: Standard $k \\times k$ Convolution**\n\nA standard convolution uses $N$ distinct filters to produce $N$ output channels. Each filter must span the entire depth of the input, so each filter has dimensions $k \\times k \\times M$.\n\n*   **Parameter Count ($P_1$):**\n    There are $N$ filters, and each filter contains $k \\times k \\times M$ weight parameters.\n    $$P_1 = N \\times (k \\times k \\times M) = k^2 M N$$\n\n*   **FLOPs ($F_1$):**\n    For each of the $H \\times W$ output positions, and for each of the $N$ output channels, we compute one dot product. The dot product is between a $k \\times k \\times M$ filter and a $k \\times k \\times M$ input patch. This involves $k^2 M$ multiplications and approximately $k^2 M$ additions, constituting $k^2 M$ multiply-add pairs. Since each pair is $2$ FLOPs, the computation for one output value is $2 k^2 M$ FLOPs.\n    $$F_1 = (H \\times W) \\times N \\times (2 k^2 M) = 2 H W k^2 M N$$\n\n**Design 2: Depthwise $k \\times k$ Convolution with Depth Multiplier $\\gamma = N/M$**\n\nA depthwise convolution applies filters to each input channel independently. A depth multiplier $\\gamma$ means that for each of the $M$ input channels, we apply $\\gamma$ separate $k \\times k$ filters, producing $\\gamma$ output channels. The total number of output channels is therefore $M\\gamma$. The problem specifies $\\gamma = N/M$, so the total output channels are $M \\times (N/M) = N$, as required.\n\n*   **Parameter Count ($P_2$):**\n    For each of the $M$ input channels, there are $\\gamma$ filters of size $k \\times k$. The total number of filters is $M\\gamma$.\n    $$P_2 = (M \\gamma) \\times (k \\times k) = M \\left(\\frac{N}{M}\\right) k^2 = N k^2$$\n\n*   **FLOPs ($F_2$):**\n    For each of the $H \\times W$ output positions, we compute $M\\gamma = N$ output values. Each computation involves a dot product over a $k \\times k$ spatial region of a single input channel. This is $k^2$ multiply-add pairs, or $2 k^2$ FLOPs.\n    $$F_2 = (H \\times W) \\times (M \\gamma) \\times (2 k^2) = (H \\times W) \\times N \\times (2 k^2) = 2 H W k^2 N$$\n\n**Design 3: Depthwise-Separable Convolution**\n\nThis design consists of two sequential steps: a depthwise convolution followed by a pointwise ($1 \\times 1$) convolution.\n\n*   **Step A: Depthwise Convolution**\n    This step uses a depth multiplier of $1$. It applies one $k \\times k$ filter to each of the $M$ input channels. The output is a tensor of size $H \\times W \\times M$.\n    *   Parameters ($P_{3A}$): There are $M$ filters of size $k \\times k$.\n        $$P_{3A} = M \\times k^2$$\n    *   FLOPs ($F_{3A}$): Using the logic from Design 2 with $\\gamma=1$ (and thus $N=M$ in that formula's context).\n        $$F_{3A} = (H \\times W) \\times M \\times (2 k^2) = 2 H W k^2 M$$\n\n*   **Step B: Pointwise Convolution**\n    This step is a standard $1 \\times 1$ convolution that maps the $M$-channel output from Step A to the final $N$ output channels.\n    *   Parameters ($P_{3B}$): Using the formula for a standard convolution (Design 1) with $k=1$.\n        $$P_{3B} = N \\times (1^2 \\times M) = M N$$\n    *   FLOPs ($F_{3B}$): Using the FLOPs formula for a standard convolution with $k=1$.\n        $$F_{3B} = (H \\times W) \\times N \\times (2 \\times 1^2 \\times M) = 2 H W M N$$\n\n*   **Total for Design 3:**\n    The total parameters and FLOPs are the sum of the two steps.\n    *   Total Parameters ($P_3$):\n        $$P_3 = P_{3A} + P_{3B} = M k^2 + M N = M(k^2 + N)$$\n    *   Total FLOPs ($F_3$):\n        $$F_3 = F_{3A} + F_{3B} = 2 H W k^2 M + 2 H W M N = 2 H W M (k^2 + N)$$\n\n**Numerical Evaluation and Selection**\n\nWe are given the following values:\n$H = 128$, $W = 128$, $M = 64$, $N = 128$, $k = 3$, and hardware throughput $R = 5.0 \\times 10^{11}$ FLOPs/s.\n\n*   **Design 1 (Standard):**\n    $P_1 = k^2 M N = 3^2 \\times 64 \\times 128 = 9 \\times 8192 = 73728$.\n    $F_1 = 2 H W k^2 M N = 2 \\times 128^2 \\times 73728 = 2 \\times 16384 \\times 73728 = 2415919104$ FLOPs.\n    $L_1 = \\frac{F_1}{R} = \\frac{2415919104}{5.0 \\times 10^{11}} \\approx 4.8318 \\times 10^{-3}~\\text{s} = 4.8318~\\text{ms}$.\n\n*   **Design 2 (Depthwise-only):**\n    $P_2 = N k^2 = 128 \\times 3^2 = 128 \\times 9 = 1152$.\n    $F_2 = 2 H W k^2 N = 2 \\times 128^2 \\times 3^2 \\times 128 = 2 \\times 16384 \\times 9 \\times 128 = 37748736$ FLOPs.\n    $L_2 = \\frac{F_2}{R} = \\frac{37748736}{5.0 \\times 10^{11}} \\approx 7.5497 \\times 10^{-5}~\\text{s} = 0.075497~\\text{ms}$.\n\n*   **Design 3 (Depthwise-Separable):**\n    $P_3 = M(k^2 + N) = 64 \\times (3^2 + 128) = 64 \\times (9 + 128) = 64 \\times 137 = 8768$.\n    $F_3 = 2 H W M(k^2 + N) = 2 \\times 128^2 \\times 64 \\times (3^2+128) = 2 \\times 16384 \\times 8768 = 287309824$ FLOPs.\n    $L_3 = \\frac{F_3}{R} = \\frac{287309824}{5.0 \\times 10^{11}} \\approx 5.7462 \\times 10^{-4}~\\text{s} = 0.57462~\\text{ms}$.\n\n**Selection:**\n\nThe selection criteria are:\n1.  Mean Intersection-over-Union (mIoU) must be at least $0.75$.\n2.  Among the designs that satisfy criterion 1, choose the one with the minimum latency.\n\nThe given mIoU values are:\n*   Standard convolution: $0.78$\n*   Depthwise-only convolution: $0.74$\n*   Depthwise-separable convolution: $0.77$\n\nApplying criterion 1:\n*   Standard convolution: $0.78 \\ge 0.75$. This design is a candidate.\n*   Depthwise-only convolution: $0.74 < 0.75$. This design is disqualified.\n*   Depthwise-separable convolution: $0.77 \\ge 0.75$. This design is a candidate.\n\nApplying criterion 2 to the remaining candidates (Standard and Depthwise-Separable):\n*   Latency of Standard convolution ($L_1$) is $4.8318~\\text{ms}$.\n*   Latency of Depthwise-separable convolution ($L_3$) is $0.57462~\\text{ms}$.\n\nComparing the latencies, we find that $0.57462~\\text{ms} < 4.8318~\\text{ms}$. Therefore, the depthwise-separable convolution design is the optimal choice as it meets the accuracy threshold while providing the lower latency.\n\nThe question asks for the resulting latency in milliseconds, rounded to four significant figures.\n$L_3 = 0.57462~\\text{ms}$.\nRounding to four significant figures gives $0.5746~\\text{ms}$.",
            "answer": "$$\\boxed{0.5746}$$"
        },
        {
            "introduction": "Fully Convolutional Networks are not limited to a single prediction task; they can be trained to solve multiple tasks simultaneously, such as predicting semantic classes and object boundaries from the same input image. This exercise delves into the principles of multi-task learning by having you derive and implement a combined loss function . You will also analyze the interaction between tasks by measuring the alignment of their gradients, providing insight into how shared features are influenced by competing objectives.",
            "id": "3126589",
            "problem": "You are given a simplified abstraction of a Fully Convolutional Network (FCN) for per-pixel multi-task prediction. A Fully Convolutional Network (FCN) produces a shared per-pixel feature vector and applies task-specific heads that are equivalent to $1\\times1$ convolutions, which can be modeled as linear maps. In this setting, for each pixel with shared feature vector $\\mathbf{f}\\in\\mathbb{R}^D$, the semantic class head produces a vector of logits $\\mathbf{z}^{(c)}\\in\\mathbb{R}^K$ via $\\mathbf{z}^{(c)}=\\mathbf{W}^{(c)}\\mathbf{f}$, and the boundary head produces a scalar logit $z^{(b)}\\in\\mathbb{R}$ via $z^{(b)}=\\mathbf{w}^{(b)}\\mathbf{f}$. The semantic class label is modeled as a categorical variable over $K$ classes, and the boundary label is modeled as a Bernoulli variable indicating boundary presence.\n\nUse the following foundational base for derivations and implementation:\n- The categorical and Bernoulli distributions are standard probabilistic models for multiclass classification and binary classification, respectively.\n- Maximum Likelihood Estimation (MLE) seeks to maximize the likelihood of observed labels under the model, which is equivalent to minimizing the negative log-likelihood.\n- The chain rule of calculus applies to differentiable mappings of model parameters and intermediate variables.\n\nYour tasks are:\n- Derive a principled multi-task objective starting from the negative log-likelihood of the categorical and Bernoulli models applied to the per-pixel predictions, assigned weights $\\alpha$ and $\\beta$ for the semantic and boundary terms, respectively. The logits should be transformed into probabilities using the standard softmax for the categorical head and the standard sigmoid for the boundary head.\n- Implement the objective and compute its value for each test case. For the boundary term, use an inverse-frequency positive-class weight computed per test case as $w_{+}=N_{0}/\\max(N_{1},1)$, where $N_{1}$ is the count of pixels with boundary label $1$ and $N_{0}$ is the count with boundary label $0$. If $N_{1}=0$, define $w_{+}=1$ to avoid division by zero. Apply $w_{+}$ only to pixels with boundary label $1$.\n- Compute, for each pixel, the gradient of each taskâ€™s loss term with respect to the shared feature vector $\\mathbf{f}$ using only the task-specific term before applying the task weights $\\alpha$ and $\\beta$. Then, compute the cosine similarity between the semantic-task gradient and the boundary-task gradient for that pixel. Average these cosine similarities across pixels to obtain a mean gradient alignment. If either gradient vector has zero norm at a pixel, define the cosine similarity for that pixel to be $0$.\n- For numerical stability, when evaluating logarithms of probabilities, clip any probability $p$ to the interval $[\\epsilon,1-\\epsilon]$ with $\\epsilon=10^{-12}$.\n\nThe program must return, for each test case, a list with three values:\n- The total multi-task loss as a float (with at least $6$ significant digits), computed as $\\alpha$ times the sum of the semantic per-pixel negative log-likelihood plus $\\beta$ times the sum of the weighted boundary per-pixel negative log-likelihood.\n- The mean gradient alignment (cosine similarity averaged over pixels) as a float.\n- The computed positive-class weight $w_{+}$ as a float.\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list, e.g., $[\\,[\\text{loss}_1,\\text{align}_1,w_{+,1}],\\,[\\text{loss}_2,\\text{align}_2,w_{+,2}]\\,]$.\n\nTest suite and parameters:\n\n- Test case $1$:\n    - Image size: $H=2$, $W=2$, feature dimension $D=3$, classes $K=3$.\n    - Shared features $\\mathbf{F}\\in\\mathbb{R}^{H\\times W\\times D}$ (row-major order):\n      $$\n      \\begin{aligned}\n      \\mathbf{f}_{0,0}&=[\\,0.2,\\,-0.1,\\,0.3\\,],\\quad\n      \\mathbf{f}_{0,1}=[\\,0.0,\\,0.1,\\,-0.2\\,],\\\\\n      \\mathbf{f}_{1,0}&=[\\,0.5,\\,0.2,\\,0.0\\,],\\quad\n      \\mathbf{f}_{1,1}=[\\,-0.3,\\,0.4,\\,0.1\\,].\n      \\end{aligned}\n      $$\n    - Semantic head weights $\\mathbf{W}^{(c)}\\in\\mathbb{R}^{K\\times D}$:\n      $$\n      \\mathbf{W}^{(c)}=\n      \\begin{bmatrix}\n      0.1 & -0.2 & 0.0\\\\\n      0.0 & 0.3 & -0.1\\\\\n      -0.1 & 0.0 & 0.2\n      \\end{bmatrix}.\n      $$\n    - Boundary head weights $\\mathbf{w}^{(b)}\\in\\mathbb{R}^{1\\times D}$:\n      $$\n      \\mathbf{w}^{(b)}=[\\,0.2,\\,-0.1,\\,0.3\\,].\n      $$\n    - Semantic labels $\\mathbf{Y}\\in\\{0,1,2\\}^{H\\times W}$:\n      $$\n      \\mathbf{Y}=\n      \\begin{bmatrix}\n      0 & 1\\\\\n      2 & 0\n      \\end{bmatrix}.\n      $$\n    - Boundary labels $\\mathbf{B}\\in\\{0,1\\}^{H\\times W}$:\n      $$\n      \\mathbf{B}=\n      \\begin{bmatrix}\n      0 & 1\\\\\n      1 & 0\n      \\end{bmatrix}.\n      $$\n    - Task weights: $\\alpha=1.0$, $\\beta=0.5$.\n\n- Test case $2$:\n    - Image size: $H=3$, $W=1$, feature dimension $D=2$, classes $K=2$.\n    - Shared features:\n      $$\n      \\mathbf{f}_{0,0}=[\\,0.0,\\,0.0\\,],\\quad\n      \\mathbf{f}_{1,0}=[\\,0.1,\\,-0.1\\,],\\quad\n      \\mathbf{f}_{2,0}=[\\,-0.2,\\,0.3\\,].\n      $$\n    - Semantic head weights:\n      $$\n      \\mathbf{W}^{(c)}=\n      \\begin{bmatrix}\n      0.05 & 0.04\\\\\n      -0.03 & 0.02\n      \\end{bmatrix}.\n      $$\n    - Boundary head weights:\n      $$\n      \\mathbf{w}^{(b)}=[\\,0.1,\\,0.0\\,].\n      $$\n    - Semantic labels:\n      $$\n      \\mathbf{Y}=\n      \\begin{bmatrix}\n      1\\\\\n      0\\\\\n      1\n      \\end{bmatrix}.\n      $$\n    - Boundary labels:\n      $$\n      \\mathbf{B}=\n      \\begin{bmatrix}\n      0\\\\\n      0\\\\\n      0\n      \\end{bmatrix}.\n      $$\n    - Task weights: $\\alpha=0.8$, $\\beta=1.2$.\n\n- Test case $3$:\n    - Image size: $H=2$, $W=3$, feature dimension $D=2$, classes $K=3$.\n    - Shared features:\n      $$\n      \\begin{aligned}\n      \\mathbf{f}_{0,0}&=[\\,0.3,\\,-0.2\\,],\\quad \\mathbf{f}_{0,1}=[\\,0.0,\\,0.1\\,],\\quad \\mathbf{f}_{0,2}=[\\,-0.1,\\,0.4\\,],\\\\\n      \\mathbf{f}_{1,0}&=[\\,0.2,\\,0.2\\,],\\quad \\mathbf{f}_{1,1}=[\\,-0.3,\\,-0.1\\,],\\quad \\mathbf{f}_{1,2}=[\\,0.5,\\,-0.4\\,].\n      \\end{aligned}\n      $$\n    - Semantic head weights:\n      $$\n      \\mathbf{W}^{(c)}=\n      \\begin{bmatrix}\n      0.2 & -0.1\\\\\n      -0.2 & 0.3\\\\\n      0.1 & 0.0\n      \\end{bmatrix}.\n      $$\n    - Boundary head weights:\n      $$\n      \\mathbf{w}^{(b)}=[\\,-0.1,\\,0.2\\,].\n      $$\n    - Semantic labels:\n      $$\n      \\mathbf{Y}=\n      \\begin{bmatrix}\n      0 & 2 & 1\\\\\n      1 & 0 & 2\n      \\end{bmatrix}.\n      $$\n    - Boundary labels:\n      $$\n      \\mathbf{B}=\n      \\begin{bmatrix}\n      1 & 1 & 0\\\\\n      0 & 1 & 1\n      \\end{bmatrix}.\n      $$\n    - Task weights: $\\alpha=0.7$, $\\beta=1.3$.",
            "solution": "First, we derive the mathematical expressions for the required quantities. We consider a single pixel with index $i$. The shared feature vector is $\\mathbf{f}_i \\in \\mathbb{R}^D$.\n\n**Multi-Task Objective Function Derivation**\n\nThe total loss $L_{total}$ is a weighted sum of the semantic segmentation loss and the boundary detection loss, aggregated over all $N=H \\times W$ pixels.\n$$\nL_{total} = \\alpha L_{sem} + \\beta L_{bnd} = \\alpha \\sum_{i=1}^{N} L_{sem, i} + \\beta \\sum_{i=1}^{N} L_{bnd, i}^{\\text{weighted}}\n$$\n\n**Semantic Segmentation Loss ($L_{sem}$)**\n\nThe semantic head computes class logits $\\mathbf{z}_i^{(c)} \\in \\mathbb{R}^K$ for pixel $i$ via the linear map:\n$$\n\\mathbf{z}_i^{(c)} = \\mathbf{W}^{(c)} \\mathbf{f}_i\n$$\nThese logits are converted into a probability distribution over the $K$ classes using the softmax function:\n$$\np_{i,k}^{(c)} = \\text{softmax}(\\mathbf{z}_i^{(c)})_k = \\frac{\\exp(z_{i,k}^{(c)})}{\\sum_{j=0}^{K-1} \\exp(z_{i,j}^{(c)})}\n$$\nwhere $p_{i,k}^{(c)}$ is the predicted probability that pixel $i$ belongs to class $k$. Given the ground-truth integer label $y_i \\in \\{0, 1, \\dots, K-1\\}$, the semantic loss for pixel $i$ is the negative log-likelihood (NLL) of the true class, also known as the cross-entropy loss:\n$$\nL_{sem, i} = -\\log(p_{i, y_i}^{(c)})\n$$\nTo ensure numerical stability, the probability $p_{i, y_i}^{(c)}$ is clipped to the interval $[\\epsilon, 1-\\epsilon]$, with $\\epsilon = 10^{-12}$, before taking the logarithm.\n\n**Boundary Detection Loss ($L_{bnd}$)**\n\nThe boundary head computes a scalar logit $z_i^{(b)} \\in \\mathbb{R}$ for pixel $i$:\n$$\nz_i^{(b)} = \\mathbf{w}^{(b)} \\mathbf{f}_i\n$$\nThe logit is converted to a probability of the pixel being a boundary ($b_i=1$) using the sigmoid function:\n$$\np_i^{(b)} = \\sigma(z_i^{(b)}) = \\frac{1}{1 + \\exp(-z_i^{(b)})}\n$$\nThe boundary loss is a weighted binary cross-entropy. The weight $w_+$ for the positive class (boundary present, $b_i=1$) is defined as:\n$$\nw_{+} = \\begin{cases} 1 & \\text{if } N_1 = 0 \\\\ N_0 / N_1 & \\text{if } N_1 > 0 \\end{cases}\n$$\nwhere $N_1$ is the total count of pixels with label $1$ and $N_0$ is the count of pixels with label $0$. This weight is applied only to positive samples. The weighted NLL for pixel $i$ with binary label $b_i \\in \\{0, 1\\}$ is:\n$$\nL_{bnd, i}^{\\text{weighted}} = -[w_+ b_i \\log(p_i^{(b)}) + (1 - b_i) \\log(1 - p_i^{(b)})]\n$$\nFor numerical stability, probabilities are clipped to $[\\epsilon, 1-\\epsilon]$ before the logarithm is applied.\n\n**Gradient Alignment Derivation**\n\nWe require the gradients of the unweighted, per-pixel loss terms with respect to the shared feature vector $\\mathbf{f}_i$.\n\n**Gradient of Semantic Loss ($\\nabla_{\\mathbf{f}_i} L_{sem, i}$)**\n\nThe gradient of the semantic loss $L_{sem, i} = -\\log(p_{i, y_i}^{(c)})$ with respect to $\\mathbf{f}_i$ is found using the chain rule. First, we find the gradient with respect to the logits $\\mathbf{z}_i^{(c)}$. The derivative of the cross-entropy loss with respect to the pre-softmax logits $z_{i,j}^{(c)}$ is well-known:\n$$\n\\frac{\\partial L_{sem, i}}{\\partial z_{i,j}^{(c)}} = p_{i,j}^{(c)} - \\delta_{j, y_i}\n$$\nwhere $\\delta_{j, y_i}$ is the Kronecker delta. Let $\\mathbf{e}_{y_i}$ be the one-hot column vector for the true class $y_i$. The gradient with respect to the logit vector $\\mathbf{z}_i^{(c)}$ is $\\mathbf{p}_i^{(c)} - \\mathbf{e}_{y_i}$.\nUsing the chain rule, $\\nabla_{\\mathbf{f}_i} L_{sem, i} = (\\nabla_{\\mathbf{f}_i} \\mathbf{z}_i^{(c)})^T \\nabla_{\\mathbf{z}_i^{(c)}} L_{sem, i}$. Since $\\mathbf{z}_i^{(c)} = \\mathbf{W}^{(c)} \\mathbf{f}_i$, the Jacobian matrix $\\nabla_{\\mathbf{f}_i} \\mathbf{z}_i^{(c)}$ is $\\mathbf{W}^{(c)}$. Thus, the gradient is:\n$$\n\\mathbf{g}_i^{(c)} = \\nabla_{\\mathbf{f}_i} L_{sem, i} = (\\mathbf{W}^{(c)})^T (\\mathbf{p}_i^{(c)} - \\mathbf{e}_{y_i})\n$$\n\n**Gradient of Boundary Loss ($\\nabla_{\\mathbf{f}_i} L_{bnd, i}^{\\text{unweighted}}$)**\n\nFor gradient alignment, we consider the unweighted boundary loss ($w_+=1$):\n$$\nL_{bnd, i}^{\\text{unweighted}} = -[b_i \\log(p_i^{(b)}) + (1-b_i) \\log(1 - p_i^{(b)})]\n$$\nThe gradient with respect to the logit $z_i^{(b)}$ is a standard result for binary cross-entropy with a sigmoid activation:\n$$\n\\frac{\\partial L_{bnd, i}^{\\text{unweighted}}}{\\partial z_i^{(b)}} = p_i^{(b)} - b_i\n$$\nApplying the chain rule, and since $z_i^{(b)} = \\mathbf{w}^{(b)} \\mathbf{f}_i$, we have $\\nabla_{\\mathbf{f}_i} z_i^{(b)} = (\\mathbf{w}^{(b)})^T$. The gradient becomes:\n$$\n\\mathbf{g}_i^{(b)} = \\nabla_{\\mathbf{f}_i} L_{bnd, i}^{\\text{unweighted}} = (p_i^{(b)} - b_i) (\\mathbf{w}^{(b)})^T\n$$\n\n**Mean Gradient Alignment**\n\nFor each pixel $i$, the cosine similarity between the two task gradients $\\mathbf{g}_i^{(c)}$ and $\\mathbf{g}_i^{(b)}$ is calculated:\n$$\n\\text{sim}_i = \\frac{\\mathbf{g}_i^{(c)} \\cdot \\mathbf{g}_i^{(b)}}{\\|\\mathbf{g}_i^{(c)}\\|_2 \\|\\mathbf{g}_i^{(b)}\\|_2}\n$$\nIf either gradient has a norm of zero, $\\text{sim}_i$ is defined as $0$. The mean gradient alignment is the average of these similarities over all $N$ pixels:\n$$\n\\text{Alignment} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{sim}_i\n$$\n\nThe implementation will perform these calculations for each test case using vectorized operations in NumPy for efficiency.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n5.174154 & 0.068369 & 1.0 \\\\\n2.565452 & 0.0 & 1.0 \\\\\n7.070625 & -0.053748 & 0.5\n\\end{pmatrix}\n}\n$$"
        }
    ]
}