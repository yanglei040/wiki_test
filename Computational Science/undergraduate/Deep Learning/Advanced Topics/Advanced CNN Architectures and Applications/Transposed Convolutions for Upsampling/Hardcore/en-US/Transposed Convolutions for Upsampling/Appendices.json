{
    "hands_on_practices": [
        {
            "introduction": "Before building complex networks, it is essential to master the fundamentals of each layer. This exercise challenges you to derive the output size formula for a transposed convolution, starting from the more familiar standard convolution. By working from first principles, you will solidify your understanding of the inverse relationship between these two operations, a crucial skill for designing and debugging custom model architectures .",
            "id": "3196147",
            "problem": "Consider a one-dimensional transposed convolution (also known as fractionally strided convolution) used for upsampling in a Convolutional Neural Network (CNN). Let the transposed convolution take an input of length $n$ and use a kernel of size $k$, stride $s$, padding $p$, and output padding $op$. The stride $s$ spaces the positions where the kernel acts, the padding $p$ represents the number of zero elements conceptually added to each end of the signal on the forward pass, and the output padding $op$ is an integer satisfying $0 \\leq op  s$ that selects one of the $s$ possible spatial alignments in the upsampled output by increasing its length by $op$.\n\nAs a fundamental base, use the well-tested formula for the output length $m$ of a standard (forward) one-dimensional convolution with input length $L$, kernel size $k$, stride $s$, and padding $p$:\n$$\nm \\;=\\; \\left\\lfloor \\frac{L + 2p - k}{s} \\right\\rfloor + 1,\n$$\nand the fact that the transposed convolution implements the transpose of the linear operator corresponding to this forward convolution.\n\nDerive, from these principles and without appealing to any pre-memorized target formulas, a closed-form expression for the exact output length of the transposed convolution in terms of $n$, $k$, $s$, $p$, and $op$. In your derivation, state any conditions under which the floor operation in the forward formula can be eliminated to yield an exact equality, and explain the role of $op$ in determining the final length.\n\nFinally, evaluate the derived expression numerically for the case $n=7$, $k=3$, $s=2$, $p=1$, and $op=1$. No rounding is required; report the exact integer output length. The final answer must be a single number.",
            "solution": "The problem requires the derivation of the output length of a one-dimensional transposed convolution from first principles. The starting point is the formula for the output length of a standard forward convolution and the fact that a transposed convolution corresponds to the transpose of the forward convolution's linear operator.\n\nLet the standard forward convolution be a linear operator represented by a matrix $C$. This operator maps an input vector of length $L$ to an output vector of length $m$. The problem provides the formula for $m$ as:\n$$m = \\left\\lfloor \\frac{L + 2p - k}{s} \\right\\rfloor + 1$$\nwhere $k$ is the kernel size, $s$ is the stride, and $p$ is the padding.\n\nA transposed convolution performs the transpose operation, which we can denote by $C^T$. This operator maps an input of size $m$ to an output of size $L$. The problem specifies that for our transposed convolution, the input has a length of $n$. This means we must associate $n$ with the output size of the forward convolution, i.e., $m=n$. The goal is to find the output length of the transposed convolution, which we will denote as $L_{out}$. This length corresponds to the input size $L$ of the forward convolution.\n\nBy substituting $m=n$ and $L=L_{out}$ into the forward convolution formula, we establish the fundamental relationship governing the dimensions:\n$$n = \\left\\lfloor \\frac{L_{out} + 2p - k}{s} \\right\\rfloor + 1$$\n\nOur task is to solve this equation for $L_{out}$. First, we isolate the floor function:\n$$n - 1 = \\left\\lfloor \\frac{L_{out} + 2p - k}{s} \\right\\rfloor$$\nThe definition of the floor function states that for any real number $x$ and integer $z$, the equality $z = \\lfloor x \\rfloor$ is equivalent to the inequality $z \\leq x  z + 1$. Applying this property, we get:\n$$n - 1 \\leq \\frac{L_{out} + 2p - k}{s}  (n - 1) + 1$$\n$$n - 1 \\leq \\frac{L_{out} + 2p - k}{s}  n$$\nSince the stride $s$ is a positive integer, we can multiply through the inequality by $s$ without changing the direction of the inequalities:\n$$s(n - 1) \\leq L_{out} + 2p - k  sn$$\nFinally, we isolate $L_{out}$ by adding $(k - 2p)$ to all parts of the inequality:\n$$s(n - 1) + k - 2p \\leq L_{out}  sn + k - 2p$$\nThis inequality reveals that there is not a single unique solution for $L_{out}$. Instead, there is a range of possible integer values that $L_{out}$ can take. This ambiguity arises because the forward convolution is a many-to-one mapping; multiple input lengths $L$ can produce the same output length $m$ due to the nature of striding and the floor function. The number of possible integer values for $L_{out}$ in this range is $(sn + k - 2p) - (s(n - 1) + k - 2p) = s$.\n\nThe problem introduces output padding, $op$, as an integer satisfying $0 \\leq op  s$. Its role is to resolve this ambiguity by selecting a specific output length from the $s$ possible options. The problem states that $op$ increases the output length. A standard convention is to define a base output length (the minimum possible value) and add $op$ as an offset. The minimum possible integer value for $L_{out}$ from our derived inequality is:\n$$L_{min} = s(n - 1) + k - 2p$$\nThe final output length, $L_{out}$, is then determined by adding the output padding $op$ to this minimum length:\n$$L_{out} = L_{min} + op = s(n - 1) + k - 2p + op$$\nThis formula provides a determinate output length for the transposed convolution for any valid set of parameters $n, k, s, p,$ and $op$.\n\nThe problem also asks for the conditions under which the floor operation in the forward formula can be eliminated. The expression $m = \\lfloor \\frac{L + 2p - k}{s} \\rfloor + 1$ simplifies to an exact equality without the floor function if and only if its argument is an integer. This condition is:\n$$\\frac{L + 2p - k}{s} \\in \\mathbb{Z}$$\nThis is equivalent to stating that $(L + 2p - k)$ is perfectly divisible by the stride $s$, or mathematically:\n$$(L + 2p - k) \\pmod s = 0$$\nWhen this condition holds, the relationship between $L$ and $m$ becomes one-to-one. We can solve for $L$ directly:\n$$m = \\frac{L + 2p - k}{s} + 1 \\implies s(m-1) = L + 2p - k \\implies L = s(m-1) + k - 2p$$\nComparing this to our derived formula for $L_{out}$ (with $L=L_{out}$ and $m=n$), we see that this corresponds to the case where the output padding $op=0$. Thus, the role of $op$ is to handle the general case where the relationship between input and output sizes in the forward pass is not one-to-one, allowing the user to select one of the $s$ possible output sizes for the transposed operation.\n\nFinally, we evaluate the derived expression for the given numerical case: $n=7$, $k=3$, $s=2$, $p=1$, and $op=1$.\nSubstituting these values into the formula for $L_{out}$:\n$$L_{out} = s(n - 1) + k - 2p + op$$\n$$L_{out} = 2(7 - 1) + 3 - 2(1) + 1$$\n$$L_{out} = 2(6) + 3 - 2 + 1$$\n$$L_{out} = 12 + 3 - 2 + 1$$\n$$L_{out} = 15 - 2 + 1$$\n$$L_{out} = 13 + 1$$\n$$L_{out} = 14$$\nThe resulting output length is $14$.",
            "answer": "$$\\boxed{14}$$"
        },
        {
            "introduction": "A deep learning practitioner must often choose between multiple methods to achieve the same goal, each with different trade-offs. This practice guides you through a quantitative comparison of transposed convolution against other popular upsampling techniques like pixel shuffle and upsample-then-convolve. By analyzing their computational cost, parameter count, and memory usage, you will learn to make informed architectural decisions that balance model performance with efficiency .",
            "id": "3196153",
            "problem": "Consider three upsampling strategies in a two-dimensional deep neural network layer that maps an input tensor of spatial size $H_{\\text{in}} \\times W_{\\text{in}}$ and channels $C_{\\text{in}}$ to an output tensor of spatial size $H_{\\text{out}} \\times W_{\\text{out}}$ and channels $C_{\\text{out}}$, with stride $s$ satisfying $H_{\\text{out}} = s H_{\\text{in}}$ and $W_{\\text{out}} = s W_{\\text{in}}$. The three strategies are: (i) transposed convolution with kernel size $k \\times k$, stride $s$, and parameter tensor of shape $C_{\\text{out}} \\times C_{\\text{in}} \\times k \\times k$; (ii) pixel shuffle (also called sub-pixel convolution), which first applies a standard convolution at the low-resolution scale to produce $C_{\\text{out}} s^{2}$ channels and then rearranges by pixel shuffle into the high-resolution output; and (iii) nearest-neighbor upsampling by factor $s$ followed by a standard convolution at the high-resolution scale with kernel size $k \\times k$ that maps $C_{\\text{in}}$ to $C_{\\text{out}}$.\n\nStarting from the definition of discrete convolution as a sum of weighted neighborhood products across input channels and spatial offsets, and using the convention that one multiply-add operation counts as $2$ floating-point operations (FLOPs), derive symbolic expressions for the total forward-pass FLOPs and the parameter counts for each of the three strategies as functions of $H_{\\text{out}}$, $W_{\\text{out}}$, $C_{\\text{in}}$, $C_{\\text{out}}$, $k$, and $s$. In addition, define the peak intermediate activation element count (excluding the given input and final output tensors) as the maximum number of elements materialized in any intermediate tensor specific to the strategy, and derive expressions for these counts for each strategy.\n\nThen, evaluate these expressions for $C_{\\text{in}} = 64$, $C_{\\text{out}} = 64$, $s = 2$, and $k = 3$. Express FLOPs as functions of $H_{\\text{out}}$ and $W_{\\text{out}}$, and express peak intermediate activation element counts as functions of $H_{\\text{out}}$ and $W_{\\text{out}}$ as well. No rounding is required.\n\nProvide your final answer as a single row matrix with $9$ entries in the following order: $\\big[$ FLOPs for transposed convolution, parameters for transposed convolution, peak intermediate elements for transposed convolution, FLOPs for pixel shuffle, parameters for pixel shuffle, peak intermediate elements for pixel shuffle, FLOPs for nearest-neighbor-plus-convolution, parameters for nearest-neighbor-plus-convolution, peak intermediate elements for nearest-neighbor-plus-convolution $\\big]$.",
            "solution": "The problem statement is a valid, well-posed, and scientifically grounded question within the domain of deep learning. It asks for a quantitative comparison of three standard upsampling techniques. All givens are clearly stated and consistent. We will proceed with the derivation.\n\nThe fundamental operation is the two-dimensional discrete convolution. For a standard convolution mapping an input tensor of shape $C_{\\text{in}} \\times H_{\\text{in}} \\times W_{\\text{in}}$ to an output tensor of shape $C_{\\text{out}} \\times H_{\\text{out}} \\times W_{\\text{out}}$ with a kernel of size $k \\times k$, the number of floating-point operations (FLOPs) is calculated. For each output pixel in the $H_{\\text{out}} \\times W_{\\text{out}}$ spatial grid and for each of the $C_{\\text{out}}$ output channels, the computation involves a sum over the $k \\times k$ neighborhood and all $C_{\\text{in}}$ input channels. This consists of $C_{\\text{in}} \\times k \\times k$ multiplications and a similar number of additions. With one multiply-add operation defined as $2$ FLOPs, the total FLOPs are:\n$$\n\\text{FLOPs} \\approx 2 \\times H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{out}} \\times C_{\\text{in}} \\times k \\times k\n$$\nWe analyze each of the three upsampling strategies based on this principle. The input spatial size is $H_{\\text{in}} \\times W_{\\text{in}}$ and the output is $H_{\\text{out}} \\times W_{\\text{out}}$, with $H_{\\text{out}} = s H_{\\text{in}}$ and $W_{\\text{out}} = s W_{\\text{in}}$.\n\n### Strategy (i): Transposed Convolution\n\nA transposed convolution (also known as a deconvolution or fractionally-strided convolution) with stride $s$ maps a low-resolution input grid to a high-resolution output grid. The computational cost is defined by the number of multiply-add operations, which occur for each pixel in the input grid.\n\n- **FLOPs**: The multiply-add operations are performed with respect to the input tensor of size $H_{\\text{in}} \\times W_{\\text{in}}$. For each of the $H_{\\text{in}} \\times W_{\\text{in}}$ input pixels, and for each of the $C_{\\text{in}}$ input channels and $C_{\\text{out}}$ output channels, the kernel weights are applied. This is equivalent to a standard convolution producing an output of size $H_{\\text{in}} \\times W_{\\text{in}}$.\n$$\n\\text{FLOPs}_{\\text{TC}} = 2 \\times H_{\\text{in}} \\times W_{\\text{in}} \\times C_{\\text{out}} \\times C_{\\text{in}} \\times k^2\n$$\nSubstituting $H_{\\text{in}} = H_{\\text{out}}/s$ and $W_{\\text{in}} = W_{\\text{out}}/s$, we get:\n$$\n\\text{FLOPs}_{\\text{TC}} = 2 \\times \\frac{H_{\\text{out}}}{s} \\times \\frac{W_{\\text{out}}}{s} \\times C_{\\text{out}} \\times C_{\\text{in}} \\times k^2 = \\frac{2 H_{\\text{out}} W_{\\text{out}} C_{\\text{in}} C_{\\text{out}} k^2}{s^2}\n$$\n\n- **Parameters**: The problem states the parameter tensor has shape $C_{\\text{out}} \\times C_{\\text{in}} \\times k \\times k$. The total number of parameters (weights) is the product of these dimensions. Biases are not mentioned and are thus excluded.\n$$\n\\text{Params}_{\\text{TC}} = C_{\\text{in}} C_{\\text{out}} k^2\n$$\n\n- **Peak Intermediate Activation Element Count**: A common implementation of transposed convolution involves upsampling the input tensor by inserting $s-1$ rows and columns of zeros between input elements, and then applying a standard convolution. The intermediate tensor is this zero-interspersed tensor. An input of spatial size $H_{\\text{in}} \\times W_{\\text{in}}$ becomes an intermediate tensor of size $((H_{\\text{in}}-1)s + 1) \\times ((W_{\\text{in}}-1)s + 1)$ with $C_{\\text{in}}$ channels.\n$$\n\\text{Interm}_{\\text{TC}} = C_{\\text{in}} \\times ((H_{\\text{in}}-1)s + 1) \\times ((W_{\\text{in}}-1)s + 1)\n$$\nIn terms of output dimensions:\n$$\n\\text{Interm}_{\\text{TC}} = C_{\\text{in}} \\times \\left(\\left(\\frac{H_{\\text{out}}}{s}-1\\right)s + 1\\right) \\times \\left(\\left(\\frac{W_{\\text{out}}}{s}-1\\right)s + 1\\right) = C_{\\text{in}} (H_{\\text{out}} - s + 1)(W_{\\text{out}} - s + 1)\n$$\n\n### Strategy (ii): Pixel Shuffle\n\nThis strategy consists of a standard convolution at low resolution followed by a depth-to-space rearrangement.\n\n- **FLOPs**: The first step is a standard convolution on the $H_{\\text{in}} \\times W_{\\text{in}}$ input, mapping $C_{\\text{in}}$ channels to $C_{\\text{out}}s^2$ channels. The kernel size is assumed to be $k \\times k$ for a fair comparison. The second step (pixel shuffle) is a rearrangement and has zero FLOPs.\n$$\n\\text{FLOPs}_{\\text{PS}} = 2 \\times H_{\\text{in}} \\times W_{\\text{in}} \\times (C_{\\text{out}}s^2) \\times C_{\\text{in}} \\times k^2\n$$\nSubstituting for $H_{\\text{in}}$ and $W_{\\text{in}}$:\n$$\n\\text{FLOPs}_{\\text{PS}} = 2 \\times \\frac{H_{\\text{out}}}{s} \\times \\frac{W_{\\text{out}}}{s} \\times C_{\\text{out}}s^2 \\times C_{\\text{in}} \\times k^2 = 2 H_{\\text{out}} W_{\\text{out}} C_{\\text{in}} C_{\\text{out}} k^2\n$$\n\n- **Parameters**: The parameters belong to the initial convolution, which has input channels $C_{\\text{in}}$, output channels $C_{\\text{out}}s^2$, and kernel size $k \\times k$.\n$$\n\\text{Params}_{\\text{PS}} = C_{\\text{in}} (C_{\\text{out}}s^2) k^2 = C_{\\text{in}} C_{\\text{out}} s^2 k^2\n$$\n\n- **Peak Intermediate Activation Element Count**: The intermediate tensor is the output of the low-resolution convolution, before the pixel shuffle. It has shape $(C_{\\text{out}}s^2) \\times H_{\\text{in}} \\times W_{\\text{in}}$.\n$$\n\\text{Interm}_{\\text{PS}} = (C_{\\text{out}}s^2) \\times H_{\\text{in}} \\times W_{\\text{in}} = C_{\\text{out}}s^2 \\times \\frac{H_{\\text{out}}}{s} \\times \\frac{W_{\\text{out}}}{s} = C_{\\text{out}} H_{\\text{out}} W_{\\text{out}}\n$$\n\n### Strategy (iii): Nearest-Neighbor Upsampling + Convolution\n\nThis strategy first expands the spatial resolution and then applies a convolution.\n\n- **FLOPs**: The first step, nearest-neighbor upsampling, involves no floating-point arithmetic. All FLOPs come from the subsequent standard convolution at the high-resolution scale. This convolution maps the upsampled tensor of shape $C_{\\text{in}} \\times H_{\\text{out}} \\times W_{\\text{out}}$ to the output tensor of shape $C_{\\text{out}} \\times H_{\\text{out}} \\times W_{\\text{out}}$ using a $k \\times k$ kernel.\n$$\n\\text{FLOPs}_{\\text{Up+C}} = 2 \\times H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{out}} \\times C_{\\text{in}} \\times k^2\n$$\n\n- **Parameters**: The parameters are those of the high-resolution convolution.\n$$\n\\text{Params}_{\\text{Up+C}} = C_{\\text{in}} C_{\\text{out}} k^2\n$$\n\n- **Peak Intermediate Activation Element Count**: The intermediate tensor is the output of the nearest-neighbor upsampling step, which has shape $C_{\\text{in}} \\times H_{\\text{out}} \\times W_{\\text{out}}$.\n$$\n\\text{Interm}_{\\text{Up+C}} = C_{\\text{in}} H_{\\text{out}} W_{\\text{out}}\n$$\n\n### Numerical Evaluation\nWe now substitute the given values: $C_{\\text{in}} = 64$, $C_{\\text{out}} = 64$, $s = 2$, and $k = 3$.\n\n**For Transposed Convolution (i):**\n- $\\text{FLOPs}_{\\text{TC}} = \\frac{2 H_{\\text{out}} W_{\\text{out}} (64)(64) (3^2)}{2^2} = \\frac{2 \\times 4096 \\times 9}{4} H_{\\text{out}} W_{\\text{out}} = 18432 H_{\\text{out}} W_{\\text{out}}$.\n- $\\text{Params}_{\\text{TC}} = (64)(64)(3^2) = 4096 \\times 9 = 36864$.\n- $\\text{Interm}_{\\text{TC}} = 64 (H_{\\text{out}} - 2 + 1)(W_{\\text{out}} - 2 + 1) = 64(H_{\\text{out}} - 1)(W_{\\text{out}} - 1)$.\n\n**For Pixel Shuffle (ii):**\n- $\\text{FLOPs}_{\\text{PS}} = 2 H_{\\text{out}} W_{\\text{out}} (64)(64) (3^2) = 2 \\times 4096 \\times 9 H_{\\text{out}} W_{\\text{out}} = 73728 H_{\\text{out}} W_{\\text{out}}$.\n- $\\text{Params}_{\\text{PS}} = (64)(64)(2^2)(3^2) = 4096 \\times 4 \\times 9 = 147456$.\n- $\\text{Interm}_{\\text{PS}} = 64 H_{\\text{out}} W_{\\text{out}}$.\n\n**For Upsampling + Convolution (iii):**\n- $\\text{FLOPs}_{\\text{Up+C}} = 2 H_{\\text{out}} W_{\\text{out}} (64)(64) (3^2) = 73728 H_{\\text{out}} W_{\\text{out}}$.\n- $\\text{Params}_{\\text{Up+C}} = (64)(64)(3^2) = 4096 \\times 9 = 36864$.\n- $\\text{Interm}_{\\text{Up+C}} = 64 H_{\\text{out}} W_{\\text{out}}$.\n\nThe final answer is a row matrix containing these nine results in the specified order.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 18432 H_{\\text{out}} W_{\\text{out}}  36864  64(H_{\\text{out}} - 1)(W_{\\text{out}} - 1)  73728 H_{\\text{out}} W_{\\text{out}}  147456  64 H_{\\text{out}} W_{\\text{out}}  73728 H_{\\text{out}} W_{\\text{out}}  36864  64 H_{\\text{out}} W_{\\text{out}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Transposed convolutions are powerful but are known to sometimes produce \"checkerboard\" artifacts that can degrade model performance. This hands-on coding challenge delves into the root cause of this issue by exploring the concept of kernel \"phase\". By implementing a transposed convolution from scratch and observing how kernel alignment affects the output, you will gain a deep, intuitive understanding of how these artifacts form and why precise kernel design is so important .",
            "id": "3196158",
            "problem": "You are given a purely mathematical and computational challenge about two-dimensional transposed convolution in the context of upsampling a binary chessboard pattern. The goal is to reason from first principles about discrete convolution and linear operators to construct a correct implementation of two-dimensional transposed convolution, to define a rigorous notion of kernel phase, and to demonstrate quantitatively how phase tuning can reduce checkerboard artifacts (ghost grids) when reconstructing a high-resolution chessboard from a low-resolution one.\n\nStart from the following foundational base:\n- The discrete convolution of a one-dimensional signal $x[n]$ with a kernel $h[m]$ is defined by $y[n] = \\sum_{m \\in \\mathbb{Z}} x[m] \\, h[n-m]$. By linearity, its two-dimensional analogue for $x[i,j]$ and $h[u,v]$ is $y[p,q] = \\sum_{i,j} x[i,j] \\, h[p-i, q-j]$.\n- A strided convolution with stride $s$ can be seen as applying convolution and then sampling every $s$-th output index. The transpose of that linear operator corresponds to first inserting $s-1$ zeros between samples (upsampling by factor $s$) and then convolving with a flipped kernel. This yields a concrete, implementable expression for transposed convolution.\n- A two-dimensional transposed convolution with stride $s$ and a finite kernel of size $k \\times k$ produces an output whose natural, uncropped size is $(H-1)\\cdot s + k$ by $(W-1)\\cdot s + k$ when applied to an input of size $H \\times W$.\n\nTasks:\n1. Derive from the above base an explicit summation formula for a two-dimensional transposed convolution with integer stride $s$, a finite kernel $K$ of size $k \\times k$, and a kernel phase parameter $\\phi = (\\phi_y,\\phi_x)$ where $\\phi_y \\in \\{0,1,\\dots,s-1\\}$ and $\\phi_x \\in \\{0,1,\\dots,s-1\\}$. The kernel phase determines which integer tap of the kernel is aligned to each upsampled lattice point. Implement this derivation as a correct numerical procedure.\n2. Construct a low-resolution binary chessboard $X \\in \\{0,1\\}^{H \\times W}$ defined by $X[i,j] = ((i + j) \\bmod 2)$, using real-valued arithmetic. Construct the target high-resolution image $Y_{\\text{target}}$ by nearest-neighbor replication by factor $s$ in both dimensions, i.e., each pixel is expanded into an $s \\times s$ constant block.\n3. For a given stride $s$ and kernel size $k$, define a separable, symmetric, normalized kernel $K$ of size $k \\times k$ as the outer product of the one-dimensional binomial weights $b[r] = \\binom{k-1}{r} / 2^{\\,k-1}$ for $r \\in \\{0,1,\\dots,k-1\\}$, so that $K[u,v] = b[u]\\cdot b[v]$. This kernel is a discrete triangular (tent) kernel that is commonly used to approximate bilinear interpolation.\n4. Implement the two-dimensional transposed convolution using the derived summation. Use zero padding outside the output domain. For each phase $\\phi$ in the grid $\\{0,1,\\dots,s-1\\} \\times \\{0,1,\\dots,s-1\\}$, compute the uncropped output $Y_{\\phi}$ of shape $((H-1)\\cdot s + k) \\times ((W-1)\\cdot s + k)$ and then center-crop it to the exact target shape $sH \\times sW$ to form $\\widehat{Y}_{\\phi}$. The center-crop is defined by removing $\\lfloor (k-s)/2 \\rfloor$ pixels on the top and left and $\\lceil (k-s)/2 \\rceil$ pixels on the bottom and right along each axis to match $sH \\times sW$ (equivalently, crop a centered window of size $sH \\times sW$).\n5. Quantify ghost grids by the Mean Squared Error (MSE) between the reconstructed output and the target, defined as\n   $$ \\operatorname{MSE}(A,B) = \\frac{1}{N} \\sum_{n=1}^{N} \\left( A_n - B_n \\right)^2, $$\n   where $N = sH \\cdot sW$, and $A$ and $B$ are flattened arrays of size $N$.\n6. For each test case, define the improvement ratio\n   $$ r = \\frac{\\min_{\\phi} \\operatorname{MSE}(\\widehat{Y}_{\\phi}, Y_{\\text{target}})}{\\max_{\\phi} \\operatorname{MSE}(\\widehat{Y}_{\\phi}, Y_{\\text{target}})}. $$\n   A value of $r  1$ indicates that phase tuning reduces ghost grids relative to the worst phase.\n7. Your program must compute $r$ for each test case and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each $r$ rounded to exactly $6$ decimal places. For example, print as $[0.123456,0.654321,1.000000]$.\n\nTest suite:\n- Case $1$ (general, mismatch $k \\neq s$): $H = 5$, $W = 6$, $s = 2$, $k = 3$.\n- Case $2$ (boundary, $k = s$): $H = 5$, $W = 6$, $s = 2$, $k = 2$.\n- Case $3$ (edge, mismatch $k \\neq s$ and larger stride): $H = 5$, $W = 6$, $s = 3$, $k = 4$.\n\nAll quantities are unitless. Your final output must be one line in the exact format described above. No user input is allowed, and no files or network access may be used. The program must be complete and runnable as specified. The only accepted output types are floating point numbers in the list. Round each $r$ to $6$ decimal places before printing.",
            "solution": "The problem requires the derivation of a phased two-dimensional transposed convolution, its implementation to upsample a binary chessboard pattern, and an analysis of how kernel phase affects checkerboard artifacts. The solution proceeds by first establishing a formal mathematical model for the operation, then describes its numerical implementation and application to the specific test cases.\n\n### Step 1: Derivation of the Phased Transposed Convolution Formula\n\nA two-dimensional convolution with stride $s$ is a linear operator. Its transpose, the transposed convolution, is also a linear operator. A constructive way to understand the transposed convolution is as an operation that maps each point in the input grid to a scaled, shifted copy of the kernel in the output grid.\n\nLet the input be a discrete two-dimensional signal $X[i,j]$ of size $H \\times W$. Let the convolution kernel be $K[u,v]$ of size $k \\times k$. For a stride of $s$, the transposed convolution operation effectively places a copy of the kernel $K$, scaled by the input value $X[i,j]$, at a location in the output grid corresponding to the input coordinate $(i,j)$. Without any phase shift, the top-left corner $(0,0)$ of the kernel copy is aligned with the upscaled coordinate $(is, js)$ in the output grid.\n\nThe problem introduces a kernel phase parameter $\\phi = (\\phi_y, \\phi_x)$, where $\\phi_y, \\phi_x \\in \\{0, 1, \\dots, s-1\\}$. This phase determines which tap of the kernel, $(\\phi_y, \\phi_x)$, is aligned to the upscaled lattice point $(is, js)$. If tap $(\\phi_y, \\phi_x)$ of the kernel is positioned at $(is, js)$, then the kernel's origin (tap $(0,0)$) is effectively shifted to position $(is - \\phi_y, js - \\phi_x)$.\n\nConsequently, the contribution of a single input pixel $X[i,j]$ to the output pixel $Y[p,q]$ is non-zero only if $(p,q)$ falls within the footprint of the shifted kernel. The specific kernel tap that lands on $(p,q)$ is $(u,v)$, where:\n$p = (is - \\phi_y) + u \\implies u = p - is + \\phi_y$\n$q = (js - \\phi_x) + v \\implies v = q - js + \\phi_x$\n\nThe total value of an output pixel $Y_{\\phi}[p,q]$ is the sum of contributions from all input pixels. This leads to the explicit summation formula for the phased two-dimensional transposed convolution:\n$$\nY_{\\phi}[p,q] = \\sum_{i=0}^{H-1} \\sum_{j=0}^{W-1} X[i,j] \\cdot K[p - is + \\phi_y, q - js + \\phi_x]\n$$\nThis formula is valid for all output coordinates $(p,q)$. The kernel $K$ is defined to be zero outside its finite support, i.e., for indices not in the range $[0, k-1] \\times [0, k-1]$. This formula provides a direct basis for numerical implementation.\n\n### Step 2: Algorithmic Design and Implementation\n\nThe solution is structured to solve each test case by following the tasks specified.\n\n**Input, Target, and Kernel Generation (Tasks 2  3):**\nFirst, for a given test case $(H, W, s, k)$, the necessary components are constructed.\n- The low-resolution input $X$ is an $H \\times W$ matrix defined by the binary chessboard pattern $X[i,j] = ((i+j) \\pmod 2)$.\n- The high-resolution target $Y_{\\text{target}}$ is an $sH \\times sW$ matrix generated by upsampling $X$ via nearest-neighbor replication. This is equivalent to expanding each pixel of $X$ into a constant $s \\times s$ block and is efficiently implemented using a Kronecker product.\n- The $k \\times k$ kernel $K$ is constructed as the outer product of a one-dimensional binomial filter vector $b$. The elements of $b$ are given by $b[r] = \\binom{k-1}{r} / 2^{k-1}$ for $r \\in \\{0, \\dots, k-1\\}$. The resulting kernel $K[u,v] = b[u]b[v]$ is symmetric and its elements sum to $1$.\n\n**Transposed Convolution and Cropping (Task 4):**\nThe derived summation formula is implemented directly. An output grid $Y_{\\phi}$ of the required uncropped size, $((H-1)s+k) \\times ((W-1)s+k)$, is initialized to zeros. The value of each pixel $Y_{\\phi}[p,q]$ is computed by iterating through all input pixels $X[i,j]$ and summing their contributions, as dictated by the formula. This procedure is repeated for every possible phase $\\phi$ in the grid $\\{0, \\dots, s-1\\} \\times \\{0, \\dots, s-1\\}$.\n\nAfter computing the uncropped output $Y_{\\phi}$, it is center-cropped to the target dimensions $sH \\times sW$. The total number of pixels to remove along each axis is $k-s$. These are removed by trimming $\\lfloor (k-s)/2 \\rfloor$ pixels from the start (top/left) and $\\lceil (k-s)/2 \\rceil$ pixels from the end (bottom/right). This produces the final reconstructed image $\\widehat{Y}_{\\phi}$.\n\n**Error Quantification and Improvement Ratio (Tasks 5  6):**\nCheckerboard artifacts, which manifest as an undesirable high-frequency grid pattern in the output, arise from uneven overlap of the kernel responses. The severity of these artifacts is quantified by the Mean Squared Error (MSE) between the reconstructed image $\\widehat{Y}_{\\phi}$ and the ideal nearest-neighbor target $Y_{\\text{target}}$:\n$$\n\\operatorname{MSE}(\\widehat{Y}_{\\phi}, Y_{\\text{target}}) = \\frac{1}{sH \\cdot sW} \\sum_{p=0}^{sH-1} \\sum_{q=0}^{sW-1} \\left( \\widehat{Y}_{\\phi}[p,q] - Y_{\\text{target}}[p,q] \\right)^2\n$$\nFor each test case, the MSE is calculated for all possible phases $\\phi$. The minimum and maximum MSE values are identified. The final improvement ratio $r$ is calculated as:\n$$\nr = \\frac{\\min_{\\phi} \\operatorname{MSE}(\\widehat{Y}_{\\phi}, Y_{\\text{target}})}{\\max_{\\phi} \\operatorname{MSE}(\\widehat{Y}_{\\phi}, Y_{\\text{target}})}\n$$\nThis ratio measures the effectiveness of phase tuning. A value of $r  1$ demonstrates that an optimal choice of kernel phase can significantly reduce reconstruction errors compared to the worst-case phase. If the maximum MSE is zero (perfect reconstruction for all phases), the ratio is taken to be $1$. The computed value of $r$ for each test case is rounded to six decimal places for the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import comb\n\ndef transposed_conv(X, K, s, phi):\n    \"\"\"\n    Computes the 2D transposed convolution using the derived summation formula.\n\n    Args:\n        X (np.ndarray): Input matrix of shape (H, W).\n        K (np.ndarray): Kernel matrix of shape (k, k).\n        s (int): The stride.\n        phi (tuple): The phase (phi_y, phi_x).\n\n    Returns:\n        np.ndarray: The uncropped output matrix.\n    \"\"\"\n    H, W = X.shape\n    k, _ = K.shape\n    phi_y, phi_x = phi\n    \n    out_H = (H - 1) * s + k\n    out_W = (W - 1) * s + k\n    Y = np.zeros((out_H, out_W), dtype=float)\n\n    # This direct implementation of the derived formula is clear and sufficient\n    # for the problem's constraints.\n    for p in range(out_H):\n        for q in range(out_W):\n            val = 0.0\n            for i in range(H):\n                for j in range(W):\n                    u = p - i * s + phi_y\n                    v = q - j * s + phi_x\n                    if 0 = u  k and 0 = v  k:\n                        val += X[i, j] * K[u, v]\n            Y[p, q] = val\n            \n    return Y\n\ndef compute_improvement_ratio(H, W, s, k):\n    \"\"\"\n    Computes the improvement ratio 'r' for a single test case.\n    \"\"\"\n    # Task 3: Construct the separable, symmetric, normalized kernel K\n    b = np.array([comb(k - 1, r, exact=False) for r in range(k)], dtype=float) / (2**(k - 1))\n    K = np.outer(b, b)\n\n    # Task 2: Construct the low-resolution chessboard X and high-resolution target Y_target\n    I, J = np.ogrid[:H, :W]\n    X = ((I + J) % 2).astype(float)\n    Y_target = np.kron(X, np.ones((s, s)))\n    \n    target_H, target_W = Y_target.shape\n\n    # Task 4: Define center-cropping parameters\n    pad_amount = k - s\n    crop_y_start = int(np.floor(pad_amount / 2))\n    crop_x_start = int(np.floor(pad_amount / 2))\n\n    mses = []\n    # Iterate over all possible phases\n    for phi_y in range(s):\n        for phi_x in range(s):\n            # Task 4 (Part 1): Compute the uncropped transposed convolution output\n            Y_phi = transposed_conv(X, K, s, (phi_y, phi_x))\n\n            # Task 4 (Part 2): Center-crop the output to the target shape\n            Y_hat_phi = Y_phi[crop_y_start : crop_y_start + target_H, crop_x_start : crop_x_start + target_W]\n\n            # Task 5: Quantify ghost grids by Mean Squared Error (MSE)\n            mse = np.mean((Y_hat_phi - Y_target)**2)\n            mses.append(mse)\n\n    # Task 6: Compute the improvement ratio r\n    min_mse = np.min(mses)\n    max_mse = np.max(mses)\n\n    if max_mse == 0:\n        return 1.0\n    else:\n        return min_mse / max_mse\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (5, 6, 2, 3),  # Case 1\n        (5, 6, 2, 2),  # Case 2\n        (5, 6, 3, 4),  # Case 3\n    ]\n\n    results = []\n    for H, W, s, k in test_cases:\n        r = compute_improvement_ratio(H, W, s, k)\n        # Round to 6 decimal places and format to ensure 6 places are shown\n        results.append(f\"{round(r, 6):.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}