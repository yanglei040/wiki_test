{
    "hands_on_practices": [
        {
            "introduction": "To effectively use depthwise separable convolutions (DSC), one must first master their operational mechanics. This practice grounds the abstract concept in concrete calculations by having you first derive the general formula for a convolution's output dimensions and then apply it to compute the total computational cost of a DSC block . Mastering these skills is essential for designing efficient neural network architectures and for reasoning about the performance and efficiency trade-offs of different layer types.",
            "id": "3115192",
            "problem": "A Depthwise Separable Convolution (DSC) consists of a depthwise convolution followed by a pointwise convolution. The depthwise convolution applies one spatial filter per input channel, and the pointwise convolution uses filters of spatial size $1 \\times 1$ to mix channels. Consider a two-dimensional discrete convolution and use the following fundamental bases:\n\n- The two-dimensional discrete convolution of a single-channel input $x$ with a kernel $w$ at spatial location $(u,v)$ computes a weighted sum over a local receptive field of $x$.\n- Zero-padding of size $p$ extends the input by $p$ zeros on each spatial boundary, and stride $s$ advances the convolution window by $s$ pixels along each spatial axis.\n\nFrom these bases, derive the counting argument that determines how many valid spatial positions the depthwise convolution can produce along height and width when applied to an input of spatial size $(H,W)$ with a square kernel of size $k \\times k$, stride $s$, and symmetric zero-padding $p$.\n\nThen, apply your derivation to the following DSC block:\n\n- Input tensor has height $H = 128$, width $W = 96$, and number of channels $C_{\\text{in}} = 32$.\n- Depthwise convolution uses a square kernel of size $k = 5$, stride $s = 2$, and symmetric zero-padding $p = 1$ on both height and width.\n- Pointwise convolution uses $C_{\\text{out}} = 64$ output channels, stride $1$, and zero-padding $0$.\n\nAssume no dilation, and that the pointwise convolution preserves the spatial dimensions produced by the depthwise convolution. Define one multiply-add operation as one multiplication followed immediately by one addition, counted once per weight-activation pair contributing to a single output value. Bias terms are not counted.\n\nCompute the total number of multiply-add operations required to process one input image through the entire DSC block described above (depthwise stage plus pointwise stage). Express your final answer as a single integer with no units and no rounding. Only provide this total count as your final answer.",
            "solution": "The problem asks for two tasks: first, to derive the formula for the spatial output dimensions of a 2D convolution, and second, to apply this to calculate the total number of multiply-add operations for a specific Depthwise Separable Convolution (DSC) block.\n\nFirst, we derive the formula for the output dimensions. Consider a single spatial dimension of an input tensor, with size $D_{in}$. When symmetric zero-padding of size $p$ is applied, $p$ zeros are added to each of the two ends of this dimension. The effective size of the padded dimension becomes $D_{padded} = D_{in} + 2p$. A kernel of size $k$ is convolved over this padded dimension with a stride of $s$.\n\nLet the positions of the kernel be indexed by $i$, starting from $i=0$. The receptive field covered by the kernel at position $i$ starts at index $i \\times s$ and ends at index $i \\times s + k - 1$ (using $0$-based indexing). For the kernel to be placed at a valid position, its entire receptive field must lie within the padded dimension. Therefore, the last index covered must be less than the padded dimension size:\n$$i \\times s + k - 1  D_{padded}$$\n$$i \\times s \\le D_{padded} - k$$\n$$i \\le \\frac{D_{padded} - k}{s}$$\nSince $i$ must be an integer, the maximum value for the index $i$ is $\\lfloor \\frac{D_{padded} - k}{s} \\rfloor$. The number of valid positions is the count of possible integer values for $i$, which range from $0$ to $\\lfloor \\frac{D_{padded} - k}{s} \\rfloor$. The total number of such positions, which corresponds to the output dimension $D_{out}$, is:\n$$D_{out} = \\left\\lfloor \\frac{D_{padded} - k}{s} \\right\\rfloor + 1$$\nSubstituting $D_{padded} = D_{in} + 2p$, we obtain the general formula for an output spatial dimension:\n$$D_{out} = \\left\\lfloor \\frac{D_{in} + 2p - k}{s} \\right\\rfloor + 1$$\nApplying this to the height ($H$) and width ($W$) dimensions, we get:\n$$H_{out} = \\left\\lfloor \\frac{H + 2p - k}{s} \\right\\rfloor + 1$$\n$$W_{out} = \\left\\lfloor \\frac{W + 2p - k}{s} \\right\\rfloor + 1$$\nThis completes the first part of the problem.\n\nNext, we compute the total number of multiply-add operations for the given DSC block. The total cost, $\\text{Ops}_{\\text{total}}$, is the sum of the costs of the depthwise convolution stage, $\\text{Ops}_{\\text{dw}}$, and the pointwise convolution stage, $\\text{Ops}_{\\text{pw}}$.\n\n**1. Depthwise Convolution Stage**\nThe input tensor has dimensions $H = 128$, $W = 96$, and $C_{\\text{in}} = 32$.\nThe depthwise convolution parameters are: kernel size $k = 5$, stride $s = 2$, and padding $p = 1$.\nFirst, we compute the spatial dimensions of the output feature map from this stage, which we denote as $(H_{\\text{dw}}, W_{\\text{dw}})$:\n$$H_{\\text{dw}} = \\left\\lfloor \\frac{128 + 2(1) - 5}{2} \\right\\rfloor + 1 = \\left\\lfloor \\frac{125}{2} \\right\\rfloor + 1 = 62 + 1 = 63$$\n$$W_{\\text{dw}} = \\left\\lfloor \\frac{96 + 2(1) - 5}{2} \\right\\rfloor + 1 = \\left\\lfloor \\frac{93}{2} \\right\\rfloor + 1 = 46 + 1 = 47$$\nThe depthwise convolution applies a distinct $k \\times k$ filter to each of the $C_{\\text{in}}$ input channels. To compute a single value in one output channel's feature map requires $k \\times k$ multiply-add operations. This is repeated for all $H_{\\text{dw}} \\times W_{\\text{dw}}$ spatial locations and for all $C_{\\text{in}}$ channels.\nThe total number of operations for the depthwise stage is:\n$$\\text{Ops}_{\\text{dw}} = C_{\\text{in}} \\times k \\times k \\times H_{\\text{dw}} \\times W_{\\text{dw}}$$\nSubstituting the given values:\n$$\\text{Ops}_{\\text{dw}} = 32 \\times 5 \\times 5 \\times 63 \\times 47$$\n$$\\text{Ops}_{\\text{dw}} = 32 \\times 25 \\times 2961$$\n$$\\text{Ops}_{\\text{dw}} = 800 \\times 2961 = 2,368,800$$\n\n**2. Pointwise Convolution Stage**\nThe input to this stage is the output of the depthwise stage, with dimensions $(H_{\\text{in}}, W_{\\text{in}}, C_{\\text{in}}) = (63, 47, 32)$.\nThe pointwise convolution is a standard convolution with $1 \\times 1$ filters. Its parameters are: kernel size $k_{pw} = 1$, stride $s_{pw} = 1$, padding $p_{pw} = 0$, and number of output channels $C_{\\text{out}} = 64$. The spatial dimensions are preserved, which is consistent with these parameters.\nTo calculate one value at a single spatial position in one of the $C_{\\text{out}}$ output channels, a dot product is performed across the $C_{\\text{in}}$ input channels. This requires $1 \\times 1 \\times C_{\\text{in}} = C_{\\text{in}}$ multiply-add operations. This computation is repeated for all spatial locations in the output map ($H_{\\text{dw}} \\times W_{\\text{dw}}$) and for every output channel ($C_{\\text{out}}$).\nThe total number of operations for the pointwise stage is:\n$$\\text{Ops}_{\\text{pw}} = C_{\\text{in}} \\times C_{\\text{out}} \\times H_{\\text{dw}} \\times W_{\\text{dw}}$$\nSubstituting the values:\n$$\\text{Ops}_{\\text{pw}} = 32 \\times 64 \\times 63 \\times 47$$\n$$\\text{Ops}_{\\text{pw}} = 2048 \\times 2961 = 6,064,128$$\n\n**Total Operations**\nThe total number of multiply-add operations for the entire DSC block is the sum of the operations from both stages:\n$$\\text{Ops}_{\\text{total}} = \\text{Ops}_{\\text{dw}} + \\text{Ops}_{\\text{pw}} = 2,368,800 + 6,064,128 = 8,432,928$$",
            "answer": "$$\\boxed{8432928}$$"
        },
        {
            "introduction": "A convolutional block's true power emerges from the interplay between linear filtering and nonlinear activation functions. This hands-on exercise explores a critical but subtle design choice: where to place the nonlinearity, such as a Rectified Linear Unit (ReLU), within a DSC block . By deriving and empirically testing the functional power of different arrangements, you will discover how this architectural decision fundamentally alters what the layer can learn, transforming it from a simple linear model into a powerful, non-linear function approximator.",
            "id": "3115159",
            "problem": "Consider one-dimensional depthwise separable convolution under the following simplifying but scientifically sound regime: the spatial kernel size is $1$, the depthwise filter is the identity per channel, and the operation reduces to per-channel bias followed by cross-channel mixing. Begin from the core definitions: convolution as linear shift-invariant mapping, the depthwise separable structure as per-channel convolution followed by a $1 \\times 1$ pointwise mixing across channels, and the Rectified Linear Unit (ReLU) nonlinearity defined as $\\mathrm{ReLU}(t) = \\max\\{0,t\\}$. Under these constraints, there are two orderings of nonlinearity placement to compare: (A) ReLU after depthwise and before pointwise, and (B) ReLU after pointwise. Your tasks are to derive the functional class each ordering represents and to empirically test their representational consequences on a synthetic separability benchmark. Work in purely mathematical terms with an input at a single spatial location of $C=2$ channels, denoted $x=(x_1, x_2) \\in \\mathbb{R}^2$.\n\nDerivation task:\n- From the core definitions above, derive the functional form of ordering (A) and ordering (B) when the depthwise filter is the identity and the kernel size is $1$. Carefully identify the linear maps and the placement of the ReLU. Do not assume any shortcut formula; start from the linear per-channel transformation plus bias, then the cross-channel linear mixing, and place the ReLU according to each ordering.\n\nBenchmark task:\n- Define classification by threshold at $0$: a model predicts class $1$ for an input $x$ if its scalar output is greater than or equal to $0$, and class $0$ otherwise. Use three synthetic test cases that probe the role of nonlinearity placement:\n    1. Case $1$ (mixing-needed halfspace): Inputs are all pairs $(x_1,x_2)$ from the grid $S=\\{-1.0,-0.5,0.0,0.5,1.0\\}$, yielding $25$ points. The target class is $1$ if $x_1 - x_2 \\ge 0$ and $0$ otherwise. This target requires mixing before thresholding.\n    2. Case $2$ (sum of positive parts): Inputs are the same $25$ points from the grid $S$. The target class is $1$ if $\\mathrm{ReLU}(x_1) + 2\\,\\mathrm{ReLU}(x_2) - 1.0 \\ge 0$ and $0$ otherwise. This target requires per-channel rectification before mixing.\n    3. Case $3$ (nonnegative boundary equivalence): Inputs are all pairs $(x_1,x_2)$ from the grid $S_+=\\{0.0,0.5,1.0\\}$, yielding $9$ points. The target class is $1$ if $x_1 + 2\\,x_2 - 1.0 \\ge 0$ and $0$ otherwise. With nonnegative inputs, per-channel rectification has no effect, so the two orderings should be equivalent in representational capacity for this halfspace.\n\nEvaluation protocol:\n- For each case and each ordering, search over an integer parameter grid to find the best possible classification accuracy on the given inputs under the classification rule described above. The parameterization is:\n    - Depthwise biases $b_{d,1}, b_{d,2} \\in \\{-1,0,1\\}$.\n    - Pointwise mixing weights $w_1, w_2 \\in \\{-2,-1,0,1,2\\}$.\n    - Output bias $b_p \\in \\{-2,-1,0,1,2\\}$.\n- For ordering (A), the scalar output is the cross-channel linear mixing applied to the per-channel rectified and biased inputs, plus the output bias, with no further nonlinearity. Predict class $1$ if the output is $\\ge 0$ and class $0$ otherwise.\n- For ordering (B), the scalar output is the rectified (via ReLU) result of the cross-channel linear mixing of biased inputs plus the output bias. Predict class $1$ if the output is $\\ge 0$ and class $0$ otherwise, which is equivalent to checking whether the pre-ReLU sum is $\\ge 0$.\n\nTest suite and final output specification:\n- Implement both orderings and evaluate the best achievable classification accuracy over the specified parameter grid for each of the three cases.\n- Your program should produce a single line of output containing the six accuracies in the following order: $[\\text{A1},\\text{B1},\\text{A2},\\text{B2},\\text{A3},\\text{B3}]$, where $\\text{A}k$ is the best accuracy for ordering (A) on case $k$, and $\\text{B}k$ is the best accuracy for ordering (B) on case $k$.\n- The final outputs are floats in $[0,1]$, each equal to the fraction of inputs correctly classified under the best parameters found for that ordering and case. There are no physical units or angle units involved.\n\nYour solution must start from the foundational definitions of convolution, linear mixing, and the Rectified Linear Unit (ReLU), derive the functional classes for the two orderings, and implement the described benchmark and search protocol to produce the specified output.",
            "solution": "The problem requires a derivation of the functional forms for two variants of a simplified depthwise separable convolution layer, followed by an empirical evaluation on synthetic classification tasks. The validation confirms the problem is scientifically sound, well-posed, and all necessary information for a complete solution is provided.\n\n### Derivation of Functional Forms\n\nThe context is a depthwise separable convolution with a spatial kernel size of $1$, performed on an input at a single spatial location with $C=2$ channels, denoted by the vector $x = (x_1, x_2)^T \\in \\mathbb{R}^2$. The operation consists of a depthwise stage followed by a pointwise stage.\n\n**1. Stage Definitions**\n\n*   **Depthwise Stage**: The problem specifies that the depthwise filter is the identity and the kernel size is $1$. This simplifies the per-channel convolution to a function that applies a bias to each channel independently. Let the depthwise biases be $b_d = (b_{d,1}, b_{d,2})^T$. The output of this stage, $x'$, is given by:\n    $$\n    x' = \\begin{pmatrix} x'_1 \\\\ x'_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot x_1 + b_{d,1} \\\\ 1 \\cdot x_2 + b_{d,2} \\end{pmatrix} = \\begin{pmatrix} x_1 + b_{d,1} \\\\ x_2 + b_{d,2} \\end{pmatrix}\n    $$\n\n*   **Pointwise Stage**: This stage performs a $1 \\times 1$ convolution, which reduces to a linear combination of the input channels to produce a single scalar output. Let the input to this stage be a vector $z = (z_1, z_2)^T$. The pointwise weights are $w = (w_1, w_2)$ and there is a single output bias $b_p$. The output is:\n    $$\n    y_{\\text{out}} = w_1 z_1 + w_2 z_2 + b_p\n    $$\n\n*   **Nonlinearity**: The Rectified Linear Unit (ReLU) is defined as $\\mathrm{ReLU}(t) = \\max\\{0, t\\}$.\n\nWe now derive the functional forms for the two specified orderings of nonlinearity placement.\n\n**2. Ordering (A): ReLU after Depthwise, before Pointwise**\n\nIn this configuration, the ReLU activation is applied to the output of the depthwise stage, and the result is then fed into the pointwise stage.\n\n1.  The input $x=(x_1, x_2)^T$ passes through the depthwise stage, resulting in $x'=(x_1+b_{d,1}, x_2+b_{d,2})^T$.\n2.  The ReLU function is applied element-wise to $x'$:\n    $$\n    x'' = \\begin{pmatrix} \\mathrm{ReLU}(x_1 + b_{d,1}) \\\\ \\mathrm{ReLU}(x_2 + b_{d,2}) \\end{pmatrix}\n    $$\n3.  The vector $x''$ is the input to the pointwise stage ($z=x''$). The final scalar output, which we denote $f_A(x)$, is:\n    $$\n    f_A(x; b_d, w, b_p) = w_1 \\mathrm{ReLU}(x_1 + b_{d,1}) + w_2 \\mathrm{ReLU}(x_2 + b_{d,2}) + b_p\n    $$\nThe functional class represented by ordering (A) is a linear combination of shifted and rectified inputs. This is the form of a simple two-layer neural network with a hidden layer of two ReLU neurons and a linear output unit. The decision boundary defined by $f_A(x) = 0$ is piecewise linear, allowing it to approximate non-linear and non-convex decision regions.\n\n**3. Ordering (B): ReLU after Pointwise**\n\nIn this configuration, the depthwise and pointwise stages are composed linearly first, and the ReLU activation is applied only to the final scalar output.\n\n1.  The input $x=(x_1, x_2)^T$ passes through the depthwise stage, resulting in $x'=(x_1+b_{d,1}, x_2+b_{d,2})^T$.\n2.  The vector $x'$ is the input to the pointwise stage ($z=x'$). The pre-activation output is:\n    $$\n    y_{\\text{pre}} = w_1(x_1 + b_{d,1}) + w_2(x_2 + b_{d,2}) + b_p\n    $$\n    We can rearrange this into the standard form of a linear function of $x_1$ and $x_2$:\n    $$\n    y_{\\text{pre}} = w_1 x_1 + w_2 x_2 + (w_1 b_{d,1} + w_2 b_{d,2} + b_p)\n    $$\n3.  The ReLU function is applied to this scalar value. The final output, $f_B(x)$, is:\n    $$\n    f_B(x; b_d, w, b_p) = \\mathrm{ReLU}(y_{\\text{pre}}) = \\mathrm{ReLU}(w_1 x_1 + w_2 x_2 + (w_1 b_{d,1} + w_2 b_{d,2} + b_p))\n    $$\nThe classification rule is to predict class $1$ if $f_B(x) \\ge 0$. As $\\mathrm{ReLU}(t) \\ge 0$ if and only if $t \\ge 0$, this is equivalent to checking if $y_{\\text{pre}} \\ge 0$. The decision boundary is therefore $y_{\\text{pre}}=0$, which is the equation of a line in the $(x_1, x_2)$ plane. Thus, for any choice of parameters, ordering (B) acts as a linear classifier, capable of learning only hyperplane decision boundaries.\n\n### Benchmark Task Analysis and Implementation Strategy\n\nThe benchmark tasks are designed to probe the differing representational capacities of these two functional forms.\n\n*   **Case 1 (mixing-needed halfspace):** The target is $x_1 - x_2 \\ge 0$. This is a linear decision boundary. As derived, ordering (B) is inherently a linear classifier and should be able to solve this perfectly. Ordering (A) can also represent this linear function, for example, by choosing depthwise biases $b_{d,1}, b_{d,2}$ large enough such that for all inputs on the grid, the arguments to the ReLUs are always positive. In this case, $\\mathrm{ReLU}(x_i+b_{d,i}) = x_i+b_{d,i}$, and the model becomes linear. Both models are expected to achieve perfect accuracy.\n\n*   **Case 2 (sum of positive parts):** The target is $\\mathrm{ReLU}(x_1) + 2\\mathrm{ReLU}(x_2) - 1.0 \\ge 0$. The form of this target function matches the structure of ordering (A) precisely. By selecting parameters $b_{d,1}=0, b_{d,2}=0, w_1=1, w_2=2, b_p=-1$, all of which are available in the specified search grid, ordering (A) can perfectly represent the target function. In contrast, this decision boundary is non-linear, so the linear classifier represented by ordering (B) will be unable to achieve perfect accuracy.\n\n*   **Case 3 (nonnegative boundary equivalence):** The target is $x_1 + 2x_2 - 1.0 \\ge 0$, and all inputs $(x_1, x_2)$ have $x_i \\ge 0$. For ordering (A), if we select $b_{d,1}=0$ and $b_{d,2}=0$, the function becomes $f_A(x) = w_1\\mathrm{ReLU}(x_1) + w_2\\mathrm{ReLU}(x_2) + b_p$. Since $x_i \\ge 0$, $\\mathrm{ReLU}(x_i) = x_i$, so the function simplifies to $f_A(x) = w_1 x_1 + w_2 x_2 + b_p$. This is identical to the pre-activation function of ordering (B) (with $b_{d,1}=b_{d,2}=0$). Since ordering (A)'s parameter search space contains the parameters that make it equivalent to ordering (B), and the target is a linear separator, both models have the capacity to solve this task perfectly.\n\nThe implementation will proceed by exhaustively searching the discrete parameter grid for each of the six scenarios (3 cases $\\times$ 2 orderings). For each combination of parameters, the model's accuracy is calculated on the corresponding dataset. The maximum accuracy found across all parameter combinations is reported for each scenario.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Derives and empirically tests two orderings of nonlinearity placement\n    in a simplified depthwise separable convolution.\n    \"\"\"\n\n    # Define parameter grids\n    b_d_vals = [-1, 0, 1]\n    w_vals = [-2, -1, 0, 1, 2]\n    b_p_vals = [-2, -1, 0, 1, 2]\n    \n    b_d_pairs = list(itertools.product(b_d_vals, repeat=2))\n    w_pairs = list(itertools.product(w_vals, repeat=2))\n\n    # Define input grids for test cases\n    S = np.array([-1.0, -0.5, 0.0, 0.5, 1.0])\n    S_plus = np.array([0.0, 0.5, 1.0])\n\n    inputs_1_2 = np.array(list(itertools.product(S, repeat=2)))\n    inputs_3 = np.array(list(itertools.product(S_plus, repeat=2)))\n    \n    # Define ground truth labels for test cases\n    # Case 1: mixing-needed halfspace\n    x1, x2 = inputs_1_2[:, 0], inputs_1_2[:, 1]\n    y_true_1 = (x1 - x2 >= 0).astype(int)\n\n    # Case 2: sum of positive parts\n    y_true_2 = (np.maximum(0, x1) + 2 * np.maximum(0, x2) - 1.0 >= 0).astype(int)\n\n    # Case 3: nonnegative boundary equivalence\n    x1_3, x2_3 = inputs_3[:, 0], inputs_3[:, 1]\n    y_true_3 = (x1_3 + 2 * x2_3 - 1.0 >= 0).astype(int)\n\n    test_cases = [\n        (inputs_1_2, y_true_1),\n        (inputs_1_2, y_true_2),\n        (inputs_3, y_true_3)\n    ]\n\n    final_results = []\n    \n    # ReLU function\n    def relu(t):\n        return np.maximum(0, t)\n\n    for case_inputs, y_true in test_cases:\n        max_acc_A = 0.0\n        max_acc_B = 0.0\n        \n        num_points = len(case_inputs)\n        x1_data, x2_data = case_inputs[:, 0], case_inputs[:, 1]\n\n        # Exhaustive search over the parameter grid\n        for bd1, bd2 in b_d_pairs:\n            for w1, w2 in w_pairs:\n                for a_bp in b_p_vals: # 'a_bp' to distinguish from the var name in the loop\n                    bp = a_bp\n                    \n                    # Ordering (A): ReLU after depthwise, before pointwise\n                    output_A = w1 * relu(x1_data + bd1) + w2 * relu(x2_data + bd2) + bp\n                    preds_A = (output_A >= 0).astype(int)\n                    acc_A = np.sum(preds_A == y_true) / num_points\n                    if acc_A > max_acc_A:\n                        max_acc_A = acc_A\n\n                    # Ordering (B): ReLU after pointwise\n                    # Classification is based on the pre-ReLU value being >= 0\n                    output_B_pre = w1 * (x1_data + bd1) + w2 * (x2_data + bd2) + bp\n                    preds_B = (output_B_pre >= 0).astype(int)\n                    acc_B = np.sum(preds_B == y_true) / num_points\n                    if acc_B > max_acc_B:\n                        max_acc_B = acc_B\n        \n        final_results.append(max_acc_A)\n        final_results.append(max_acc_B)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The remarkable efficiency of Depthwise Separable Convolutions stems from the strong assumption that spatial and cross-channel correlations can be factorized. This advanced practice equips you with a powerful, quantitative method rooted in linear algebra to test this very assumption . By analyzing a standard convolutional kernel's singular value decomposition, you will learn to rigorously measure when the DSC approximation is poor and to quantify the potential benefit of augmenting the model to capture these non-separable features.",
            "id": "3115201",
            "problem": "You are given the task of designing a computational experiment to detect when Depthwise Separable Convolution (DSC) hurts performance due to non-separable cross-channel spatial features and to quantify the gains from adding a small full convolutional branch. The experiment must be expressed purely in mathematical and algorithmic terms and implemented as a complete, runnable program. The problem concerns Convolutional Neural Networks (CNN), standard convolution, and DSC.\n\nConsider a standard convolutional kernel represented as a four-dimensional tensor $K \\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}} \\times k_x \\times k_y}$, where $C_{\\text{out}}$ is the number of output channels, $C_{\\text{in}}$ is the number of input channels, and $k_x, k_y$ are the spatial extents of the kernel. Treat convolution as a linear operator acting on input tensors, and measure approximation fidelity using the Frobenius norm.\n\nDefine Depthwise Separable Convolution (DSC) as a composition of a depthwise spatial convolution and a pointwise $(1 \\times 1)$ convolution. In kernel form, a DSC expresses an effective kernel $\\widehat{K}$ under the factorized constraint\n$$\n\\widehat{K}[o,i,x,y] = P[o,i] \\cdot D[i,x,y],\n$$\nwhere $P \\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}}}$ is the pointwise mixing matrix, and $D \\in \\mathbb{R}^{C_{\\text{in}} \\times k_x \\times k_y}$ contains the per-input-channel spatial kernels. This factorization imposes that, for each input channel $i$, all output channels share a single spatial kernel $D[i,\\cdot,\\cdot]$, scaled by $P[o,i]$. When the true kernel $K$ cannot be represented in this form, DSC is said to be “hurt” by non-separable cross-channel spatial features. \n\nTo quantify “when DSC hurts,” use the following principle-based detection method rooted in linear algebra. For each input channel $i$, define the matrix $A^{(i)} \\in \\mathbb{R}^{C_{\\text{out}} \\times (k_x k_y)}$ by flattening the spatial dimensions of $K$ for that input channel:\n$$\nA^{(i)}[o, s] = K[o,i,x_s,y_s], \\quad \\text{for } s \\in \\{1,\\ldots,k_x k_y\\} \\text{ indexing the pair } (x_s,y_s).\n$$\nThe DSC constraint implies that $A^{(i)}$ admits a rank-$1$ approximation of the form $p^{(i)} d^{(i)^\\top}$ with $p^{(i)} \\in \\mathbb{R}^{C_{\\text{out}}}$ and $d^{(i)} \\in \\mathbb{R}^{k_x k_y}$. By the Singular Value Decomposition (SVD), which is a well-tested linear algebraic tool, and the Eckart–Young theorem, the best rank-$r$ approximation of a matrix in the Frobenius norm retains its top $r$ singular values. Therefore, the minimal squared error of the rank-$1$ DSC approximation for $A^{(i)}$ equals\n$$\nE_{\\text{DSC}}^{(i)} = \\sum_{j=2}^{m_i} \\sigma_j^{(i)2},\n$$\nwhere $\\{\\sigma_j^{(i)}\\}_{j=1}^{m_i}$ are the singular values of $A^{(i)}$, and $m_i = \\min(C_{\\text{out}}, k_x k_y)$. The total energy is \n$$\n\\|A^{(i)}\\|_F^2 = \\sum_{j=1}^{m_i} \\sigma_j^{(i)2}.\n$$\nAggregating across input channels, the global relative error of the best DSC approximation is\n$$\n\\varepsilon_{\\text{DSC}} = \\frac{\\sum_{i=1}^{C_{\\text{in}}} E_{\\text{DSC}}^{(i)}}{\\sum_{i=1}^{C_{\\text{in}}} \\|A^{(i)}\\|_F^2}.\n$$\n\nTo propose and quantify a compensation, consider adding a “small full convolution” branch that contributes an additional rank-$r$ component per input channel to the approximation of $A^{(i)}$. That is, approximate $A^{(i)}$ by a rank-$(1+r)$ matrix, which is the best possible approximation of that rank in Frobenius norm. By the same SVD principle, the minimal squared error with this augmentation is\n$$\nE_{\\text{aug}}^{(i)}(r) = \\sum_{j=r+2}^{m_i} \\sigma_j^{(i)2}, \\quad \\text{with } 1+r \\le m_i \\text{ and zero if } 1+r \\ge m_i.\n$$\nHence, the global relative error for the augmented approximation is\n$$\n\\varepsilon_{\\text{aug}}(r) = \\frac{\\sum_{i=1}^{C_{\\text{in}}} E_{\\text{aug}}^{(i)}(r)}{\\sum_{i=1}^{C_{\\text{in}}} \\|A^{(i)}\\|_F^2}.\n$$\nInterpreting these results, a large $\\varepsilon_{\\text{DSC}}$ signals that DSC is hurt by non-separable cross-channel spatial features, and the reduction $\\varepsilon_{\\text{DSC}} - \\varepsilon_{\\text{aug}}(r)$ quantifies the gain from the added small full convolution branch.\n\nImplement a program to compute $\\varepsilon_{\\text{DSC}}$ and $\\varepsilon_{\\text{aug}}(r)$ for several synthetic kernels $K$ with different structures, using the following test suite. All random values must be generated from a normal distribution with mean $0$ and variance $1$. The program must implement the kernels exactly as specified, using the provided random seeds to ensure reproducibility.\n\nTest Suite:\n- Case 1 (perfectly DSC-separable): $C_{\\text{out}} = 4$, $C_{\\text{in}} = 3$, $k_x = k_y = 3$, seed $0$. Construct $P \\in \\mathbb{R}^{4 \\times 3}$ and $D \\in \\mathbb{R}^{3 \\times 3 \\times 3}$ from the seed, and set $K[o,i,x,y] = P[o,i] \\cdot D[i,x,y]$. Use $r = 1$.\n- Case 2 (moderately non-separable): $C_{\\text{out}} = 5$, $C_{\\text{in}} = 4$, $k_x = k_y = 3$, seed $1$. Construct a DSC base as in Case 1, then add a residual $R \\in \\mathbb{R}^{5 \\times 4 \\times 3 \\times 3}$ drawn from the seed, and set $K = K_{\\text{base}} + \\alpha R$ with $\\alpha = 0.3$. Use $r = 1$.\n- Case 3 (highly non-separable): $C_{\\text{out}} = 6$, $C_{\\text{in}} = 4$, $k_x = k_y = 5$, seed $2$. Set $K$ to be fully random (no DSC structure). Use $r = 2$.\n- Case 4 (boundary, single output channel): $C_{\\text{out}} = 1$, $C_{\\text{in}} = 3$, $k_x = k_y = 3$, seed $3$. Set $K$ to be fully random. Use $r = 1$.\n- Case 5 (boundary, single input channel with diverse outputs): $C_{\\text{out}} = 5$, $C_{\\text{in}} = 1$, $k_x = k_y = 3$, seed $4$. Construct distinct per-output spatial kernels $S[o,x,y]$ and set $K[o,0,x,y] = S[o,x,y]$. Use $r = 2$.\n\nYour program must:\n- For each case, compute $\\varepsilon_{\\text{DSC}}$ and $\\varepsilon_{\\text{aug}}(r)$ exactly as defined.\n- Aggregate the results into a single line of output containing a comma-separated list of five elements (one per case), where each element is a list of two floating-point numbers $[\\varepsilon_{\\text{DSC}}, \\varepsilon_{\\text{aug}}(r)]$.\n- The final output must be formatted as a single line with no additional text and must be enclosed in square brackets. For example: $[[e_{1,\\text{DSC}},e_{1,\\text{aug}}],[e_{2,\\text{DSC}},e_{2,\\text{aug}}],\\ldots]$.\n\nNo physical units are involved, no angles are required, and all reported quantities must be floats. The program must be deterministic and require no user input.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of linear algebra and deep learning, well-posed with a clear and complete set of definitions and constraints, and objective in its formulation. We can therefore proceed to a solution.\n\nThe task is to design and implement a computational experiment to measure the degree to which a general convolutional kernel deviates from the structure imposed by Depthwise Separable Convolution (DSC), and to quantify the benefit of a proposed compensatory mechanism. The methodology is rooted in the linear algebraic properties of the convolution operation, specifically using the Singular Value Decomposition (SVD) to analyze the rank structure of the kernel.\n\nA standard convolutional layer is parameterized by a $4$-dimensional kernel tensor $K \\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}} \\times k_x \\times k_y}$, where $C_{\\text{in}}$ and $C_{\\text{out}}$ are the number of input and output channels, respectively, and $k_x, k_y$ define the spatial size of the kernel. This kernel transforms an input tensor with $C_{\\text{in}}$ channels into an output tensor with $C_{\\text{out}}$ channels.\n\nDepthwise Separable Convolution (DSC) factorizes this operation into two simpler steps: a depthwise convolution, which applies a single spatial filter per input channel, followed by a pointwise convolution, which mixes the resulting channels. This imposes a strict structural constraint on the effective kernel, which can be expressed as:\n$$\n\\widehat{K}[o,i,x,y] = P[o,i] \\cdot D[i,x,y]\n$$\nHere, $D \\in \\mathbb{R}^{C_{\\text{in}} \\times k_x \\times k_y}$ contains the $C_{\\text{in}}$ independent spatial filters, and $P \\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}}}$ is the pointwise mixing matrix. This factorization implies that for a given input channel $i$, the spatial filtering pattern $D[i, \\cdot, \\cdot]$ is identical for all output channels $o$, merely scaled by the coefficients $P[o,i]$. A general kernel $K$ may not respect this property, leading to a loss of performance when approximated by a DSC kernel $\\widehat{K}$.\n\nTo quantify this loss, we analyze the structure of $K$ on a per-input-channel basis. For each input channel $i \\in \\{1, \\ldots, C_{\\text{in}}\\}$, we can extract a slice of the kernel tensor, $K[:, i, :, :]$, which has dimensions $C_{\\text{out}} \\times k_x \\times k_y$. By flattening the two spatial dimensions, we obtain a matrix $A^{(i)} \\in \\mathbb{R}^{C_{\\text{out}} \\times (k_x k_y)}$. The elements of this matrix are given by $A^{(i)}[o, s] = K[o, i, x_s,y_s]$, where $s$ is a flattened spatial index.\n\nThe DSC constraint, $\\widehat{K}[o,i,x,y] = P[o,i] \\cdot D[i,x,y]$, implies that the corresponding matrix for the DSC kernel, $\\widehat{A}^{(i)}$, can be written as the outer product of two vectors: a column vector $p^{(i)} \\in \\mathbb{R}^{C_{\\text{out}}}$ (the $i$-th column of $P$) and the transpose of a row vector $d^{(i)\\top} \\in \\mathbb{R}^{k_x k_y}$ (the flattened $i$-th spatial filter from $D$). A matrix that can be expressed as an outer product is, by definition, a rank-$1$ matrix.\n\nThus, the core idea is to measure how well each matrix $A^{(i)}$ can be approximated by a rank-$1$ matrix. The Eckart-Young-Mirsky theorem states that the best rank-$k$ approximation of a matrix in the Frobenius norm is obtained by retaining the top $k$ singular values from its SVD. For our case, the best rank-$1$ approximation of $A^{(i)}$ uses its largest singular value, $\\sigma_1^{(i)}$. The squared Frobenius norm of the error of this approximation is the sum of the squares of all other singular values:\n$$\nE_{\\text{DSC}}^{(i)} = \\sum_{j=2}^{m_i} (\\sigma_j^{(i)})^2\n$$\nwhere $\\{\\sigma_j^{(i)}\\}_{j=1}^{m_i}$ are the singular values of $A^{(i)}$ in descending order, and $m_i = \\min(C_{\\text{out}}, k_x k_y)$ is the maximum possible rank. The total \"energy\" of the kernel for this input channel is the squared Frobenius norm of the matrix, which equals the sum of squares of all its singular values: $\\|A^{(i)}\\|_F^2 = \\sum_{j=1}^{m_i} (\\sigma_j^{(i)})^2$.\n\nTo obtain a global measure of non-separability, we aggregate these quantities across all input channels. The global relative error of the best DSC approximation is:\n$$\n\\varepsilon_{\\text{DSC}} = \\frac{\\sum_{i=1}^{C_{\\text{in}}} E_{\\text{DSC}}^{(i)}}{\\sum_{i=1}^{C_{\\text{in}}} \\|A^{(i)}\\|_F^2} = \\frac{\\sum_{i=1}^{C_{\\text{in}}} \\sum_{j=2}^{m_i} (\\sigma_j^{(i)})^2}{\\sum_{i=1}^{C_{\\text{in}}} \\sum_{j=1}^{m_i} (\\sigma_j^{(i)})^2}\n$$\nA value of $\\varepsilon_{\\text{DSC}} = 0$ indicates a perfectly separable kernel, while a larger value signifies a greater deviation from the DSC structure.\n\nThe problem then proposes to compensate for this deviation by augmenting the model. Instead of a rank-$1$ approximation for each $A^{(i)}$, we consider a rank-$(1+r)$ approximation. This corresponds to a hybrid convolutional block where a DSC layer is paralleled by a small, full convolutional layer capable of capturing a rank-$r$ component of the residual. Again, by the Eckart-Young theorem, the best rank-$(1+r)$ approximation is found by keeping the top $1+r$ singular values. The squared error of this augmented approximation is:\n$$\nE_{\\text{aug}}^{(i)}(r) = \\sum_{j=r+2}^{m_i} (\\sigma_j^{(i)})^2\n$$\nThe corresponding global relative error for this augmented model is:\n$$\n\\varepsilon_{\\text{aug}}(r) = \\frac{\\sum_{i=1}^{C_{\\text{in}}} E_{\\text{aug}}^{(i)}(r)}{\\sum_{i=1}^{C_{\\text{in}}} \\|A^{(i)}\\|_F^2}\n$$\nThe reduction in error, $\\varepsilon_{\\text{DSC}} - \\varepsilon_{\\text{aug}}(r)$, quantifies the representational gain achieved by adding the rank-$r$ compensatory branch.\n\nThe implementation will proceed by defining a function that takes the kernel parameters ($C_{\\text{out}}, C_{\\text{in}}, k_x, k_y$), the augmentation rank $r$, and a random seed. This function will:\n1.  Construct the kernel $K$ according to the specifications of one of the five test cases.\n2.  Initialize cumulative error and energy sums to $0$.\n3.  Loop through each input channel $i$ from $0$ to $C_{\\text{in}}-1$.\n    a. Extract the $C_{\\text{out}} \\times k_x \\times k_y$ slice and reshape it into the $C_{\\text{out}} \\times (k_x k_y)$ matrix $A^{(i)}$.\n    b. Compute the singular values $\\sigma_j^{(i)}$ of $A^{(i)}$ using `numpy.linalg.svd`.\n    c. Calculate the squared singular values, $(\\sigma_j^{(i)})^2$.\n    d. Compute the channel's total energy, DSC error $E_{\\text{DSC}}^{(i)}$, and augmented error $E_{\\text{aug}}^{(i)}(r)$ by summing the appropriate squared singular values.\n    e. Add these values to the respective cumulative sums.\n4.  After the loop, calculate the final relative errors $\\varepsilon_{\\text{DSC}}$ and $\\varepsilon_{\\text{aug}}(r)$ by dividing the total error sums by the total energy sum. This function is then called for each test case to produce the final results.",
            "answer": "```python\nimport numpy as np\n# from scipy import linalg # numpy.linalg is sufficient and used here\n\ndef compute_metrics(C_out, C_in, k_x, k_y, r, seed, case_id, alpha=0.0):\n    \"\"\"\n    Computes the DSC and augmented approximation errors for a given kernel.\n\n    Args:\n        C_out (int): Number of output channels.\n        C_in (int): Number of input channels.\n        k_x (int): Kernel width.\n        k_y (int): Kernel height.\n        r (int): Rank of the augmenting full convolution.\n        seed (int): Random seed for reproducibility.\n        case_id (int): Identifier for the test case to determine kernel construction.\n        alpha (float): Mixing coefficient for the non-separable component in Case 2.\n\n    Returns:\n        list: A list containing two floats: [epsilon_dsc, epsilon_aug].\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct the Kernel K based on the test case\n    K = None\n    if case_id == 1:  # Perfectly DSC-separable\n        P = rng.normal(size=(C_out, C_in))\n        D = rng.normal(size=(C_in, k_x, k_y))\n        # K[o,i,x,y] = P[o,i] * D[i,x,y] via broadcasting\n        K = P[:, :, np.newaxis, np.newaxis] * D[np.newaxis, :, :, :]\n    elif case_id == 2:  # Moderately non-separable\n        P_base = rng.normal(size=(C_out, C_in))\n        D_base = rng.normal(size=(C_in, k_x, k_y))\n        K_base = P_base[:, :, np.newaxis, np.newaxis] * D_base[np.newaxis, :, :, :]\n        R = rng.normal(size=(C_out, C_in, k_x, k_y))\n        K = K_base + alpha * R\n    elif case_id == 3:  # Highly non-separable (fully random)\n        K = rng.normal(size=(C_out, C_in, k_x, k_y))\n    elif case_id == 4:  # Boundary: single output channel (fully random)\n        K = rng.normal(size=(C_out, C_in, k_x, k_y))\n    elif case_id == 5:  # Boundary: single input channel, structured\n        S = rng.normal(size=(C_out, k_x, k_y))\n        # Add a singleton dimension for the input channel\n        K = S[:, np.newaxis, :, :]\n\n    # 2. Calculate errors by analyzing each input channel's corresponding matrix\n    total_dsc_sq_error = 0.0\n    total_aug_sq_error = 0.0\n    total_energy = 0.0\n    k_spatial_dim = k_x * k_y\n\n    for i in range(C_in):\n        # Reshape the kernel slice for input channel i into matrix A^(i)\n        A_i = K[:, i, :, :].reshape((C_out, k_spatial_dim))\n\n        # Compute singular values; they are sorted in descending order\n        s = np.linalg.svd(A_i, compute_uv=False)\n        s_sq = s**2\n        \n        channel_energy = np.sum(s_sq)\n        \n        # A channel with zero energy contributes nothing to the error or total energy\n        if channel_energy == 0:\n            continue\n            \n        total_energy += channel_energy\n\n        # E_DSC: Error of rank-1 approximation. Sum of squared singular values from the 2nd one.\n        # In 0-based indexing, this is s_sq[1:].\n        if len(s_sq) > 1:\n            total_dsc_sq_error += np.sum(s_sq[1:])\n\n        # E_aug: Error of rank-(1+r) approximation. Sum from the (1+r)+1-th one.\n        # In 0-based indexing, this is s_sq[1+r:].\n        if len(s_sq) > 1 + r:\n            total_aug_sq_error += np.sum(s_sq[1+r:])\n\n    # 3. Compute final global relative errors\n    # Add a small epsilon to prevent division by zero in pathological cases\n    epsilon_denominator = 1e-12\n    denominator = total_energy + epsilon_denominator\n    \n    eps_dsc = total_dsc_sq_error / denominator\n    eps_aug = total_aug_sq_error / denominator\n    \n    return [eps_dsc, eps_aug]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Test suite parameters:\n    # (C_out, C_in, k_x, k_y, r, seed, case_id, alpha)\n    test_cases = [\n        (4, 3, 3, 3, 1, 0, 1, 0.0),   # Case 1\n        (5, 4, 3, 3, 1, 1, 2, 0.3),   # Case 2\n        (6, 4, 5, 5, 2, 2, 3, 0.0),   # Case 3\n        (1, 3, 3, 3, 1, 3, 4, 0.0),   # Case 4\n        (5, 1, 3, 3, 2, 4, 5, 0.0),   # Case 5\n    ]\n\n    results = []\n    for case_params in test_cases:\n        C_out, C_in, k_x, k_y, r, seed, case_id, alpha = case_params\n        result = compute_metrics(C_out, C_in, k_x, k_y, r, seed, case_id, alpha)\n        results.append(result)\n\n    # Format the final output string exactly as required\n    result_str = \",\".join(f\"[{res[0]},{res[1]}]\" for res in results)\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        }
    ]
}