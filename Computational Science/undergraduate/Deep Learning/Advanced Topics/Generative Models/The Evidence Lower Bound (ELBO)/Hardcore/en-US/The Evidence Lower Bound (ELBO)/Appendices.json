{
    "hands_on_practices": [
        {
            "introduction": "This first exercise is foundational to understanding Variational Autoencoders (VAEs). Starting from first principles, you will derive the complete Evidence Lower Bound (ELBO) for a VAE with a Gaussian posterior and a deterministic decoder, a common setup in practice. This hands-on derivation  will clarify exactly how the final objective function, with its reconstruction and regularization terms, emerges from the core ideas of variational inference.",
            "id": "3184516",
            "problem": "Consider a Variational Autoencoder (VAE) with a deterministic decoder under Gaussian observation noise for a single data point $x \\in \\mathbb{R}^{d}$ and a latent variable $z \\in \\mathbb{R}^{k}$. The generative model is specified by a standard normal prior $p(z)$ and a Gaussian likelihood $p(x \\mid z)$ with fixed, isotropic noise variance. Concretely, assume\n- $p(z) = \\mathcal{N}(0, I_{k})$,\n- $p(x \\mid z) = \\mathcal{N}(W z + b, \\sigma^{2} I_{d})$, where $W \\in \\mathbb{R}^{d \\times k}$ and $b \\in \\mathbb{R}^{d}$ are decoder parameters and $\\sigma^{2} > 0$ is the observation noise variance,\n- a mean-field Gaussian variational posterior $q(z \\mid x) = \\mathcal{N}(m, \\operatorname{diag}(s^2))$, where $m \\in \\mathbb{R}^{k}$ and $s^2 \\in \\mathbb{R}_{>0}^{k}$.\n\nStarting from first principles, namely Bayes’ rule, the definition of Kullback–Leibler (KL) divergence, and Jensen’s inequality, derive an explicit Evidence Lower Bound (ELBO) on $\\log p(x)$ for this model that is fully expressed in terms of $x, W, b, m, s^2, \\sigma^2, d$, and $k$. Your derivation should explicitly evaluate the expectation over $q(z \\mid x)$ for the Gaussian likelihood and should reduce the KL divergence between the two Gaussians to a closed form that depends only on $m$ and $s^2$. Then, analyze how the observation noise variance $\\sigma^2$ balances the contribution of the reconstruction quality against the KL divergence by identifying the coefficient that scales the expected squared reconstruction error and by explaining qualitatively how changing $\\sigma^2$ affects this balance.\n\nYour final answer must be a single, closed-form analytic expression for the ELBO for the single data point $x$ in terms of $x, W, b, m, s^2, \\sigma^2, d$, and $k$. Do not include any intermediate steps or explanations in the final answer. No rounding is required.",
            "solution": "The objective is to derive the Evidence Lower Bound (ELBO) on the log-marginal likelihood $\\log p(x)$ for a specified Variational Autoencoder (VAE) model. The derivation will proceed from first principles.\n\nLet $x \\in \\mathbb{R}^{d}$ be a single data point and $z \\in \\mathbb{R}^{k}$ be a latent variable. The log-marginal likelihood $\\log p(x)$ can be rewritten by introducing an arbitrary variational distribution $q(z \\mid x)$:\n$$\n\\log p(x) = \\log \\int p(x, z) dz = \\log \\int p(x, z) \\frac{q(z \\mid x)}{q(z \\mid x)} dz = \\log \\left( \\mathbb{E}_{q(z \\mid x)} \\left[ \\frac{p(x, z)}{q(z \\mid x)} \\right] \\right)\n$$\nThe logarithm is a concave function, so by applying Jensen's inequality, $\\log(\\mathbb{E}[Y]) \\ge \\mathbb{E}[\\log Y]$, we obtain a lower bound on $\\log p(x)$:\n$$\n\\log p(x) \\ge \\mathbb{E}_{q(z \\mid x)} \\left[ \\log \\left( \\frac{p(x, z)}{q(z \\mid x)} \\right) \\right] \\equiv \\mathcal{L}(x)\n$$\nThis lower bound $\\mathcal{L}(x)$ is the ELBO. Using the product rule of probability, $p(x, z) = p(x \\mid z) p(z)$, we can expand the ELBO:\n$$\n\\mathcal{L}(x) = \\mathbb{E}_{q(z \\mid x)} [\\log p(x \\mid z) + \\log p(z) - \\log q(z \\mid x)]\n$$\nRearranging the terms allows us to express the ELBO as the difference between an expected log-likelihood (reconstruction fidelity) term and a Kullback-Leibler (KL) divergence (regularization) term:\n$$\n\\mathcal{L}(x) = \\mathbb{E}_{q(z \\mid x)} [\\log p(x \\mid z)] - (\\mathbb{E}_{q(z \\mid x)} [\\log q(z \\mid x)] - \\mathbb{E}_{q(z \\mid x)} [\\log p(z)]) = \\mathbb{E}_{q(z \\mid x)} [\\log p(x \\mid z)] - D_{KL}(q(z \\mid x) \\| p(z))\n$$\nWe now proceed to derive the closed-form expressions for each of these two terms based on the given model specifications.\n\n**1. Evaluation of the Expected Log-Likelihood Term**\n\nThe likelihood distribution is given as $p(x \\mid z) = \\mathcal{N}(x; W z + b, \\sigma^{2} I_{d})$. The log-likelihood is:\n$$\n\\log p(x \\mid z) = \\log \\left( (2\\pi\\sigma^2)^{-d/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\|x - (Wz + b)\\|_2^2\\right) \\right) = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\|x - Wz - b\\|_2^2\n$$\nWe take the expectation of this expression with respect to the variational posterior $q(z \\mid x) = \\mathcal{N}(z; m, S)$, where $S = \\operatorname{diag}(s^2)$.\n$$\n\\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] = \\mathbb{E}_{q} \\left[ -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\|x - Wz - b\\|_2^2 \\right] = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\mathbb{E}_{q} [\\|x - Wz - b\\|_2^2]\n$$\nThe expected squared norm can be expanded. Let $\\mathbb{E}_q[z] = m$ and $\\operatorname{Cov}_q[z] = \\mathbb{E}_q[(z-m)(z-m)^T] = S$.\n$$\n\\mathbb{E}_{q} [\\|x - Wz - b\\|_2^2] = \\mathbb{E}_{q} [\\|(x - Wm - b) - W(z - m)\\|_2^2]\n$$\nExpanding the square and noting that the cross-term expectation $\\mathbb{E}_{q}[-2(x-Wm-b)^T W (z-m)]$ is zero because $\\mathbb{E}_{q}[z-m] = 0$:\n$$\n\\mathbb{E}_{q}[ \\cdots ] = \\|x - Wm - b\\|_2^2 + \\mathbb{E}_{q} [(z-m)^T W^T W (z-m)]\n$$\nThe expectation of a quadratic form is $\\mathbb{E}[v^T A v] = \\operatorname{Tr}(A \\operatorname{Cov}[v]) + \\mathbb{E}[v]^T A \\mathbb{E}[v]$. Here, $v=z-m$, so $\\mathbb{E}[v]=0$. Thus:\n$$\n\\mathbb{E}_{q} [(z-m)^T W^T W (z-m)] = \\operatorname{Tr}(W^T W S) = \\operatorname{Tr}(W^T W \\operatorname{diag}(s^2))\n$$\nSo, the full expected squared norm is:\n$$\n\\mathbb{E}_{q} [\\|x - Wz - b\\|_2^2] = \\|x - Wm - b\\|_2^2 + \\operatorname{Tr}(W^T W \\operatorname{diag}(s^2))\n$$\nSubstituting this back, the expected log-likelihood becomes:\n$$\n\\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\left( \\|x - Wm - b\\|_2^2 + \\operatorname{Tr}(W^T W \\operatorname{diag}(s^2)) \\right)\n$$\n\n**2. Evaluation of the KL Divergence Term**\n\nThe KL divergence is between the variational posterior $q(z \\mid x) = \\mathcal{N}(z; m, \\operatorname{diag}(s^2))$ and the prior $p(z) = \\mathcal{N}(z; 0, I_k)$. The general formula for the KL divergence between two $k$-dimensional Gaussians $q = \\mathcal{N}(\\mu_1, \\Sigma_1)$ and $p = \\mathcal{N}(\\mu_2, \\Sigma_2)$ is:\n$$\nD_{KL}(q \\| p) = \\frac{1}{2} \\left( \\operatorname{Tr}(\\Sigma_2^{-1}\\Sigma_1) + (\\mu_2 - \\mu_1)^T \\Sigma_2^{-1} (\\mu_2 - \\mu_1) - k + \\log\\frac{\\det \\Sigma_2}{\\det \\Sigma_1} \\right)\n$$\nWith our parameters $\\mu_1 = m$, $\\Sigma_1 = \\operatorname{diag}(s^2)$, $\\mu_2 = 0$, and $\\Sigma_2 = I_k$, we get:\n-   Trace term: $\\operatorname{Tr}(I_k^{-1} \\operatorname{diag}(s^2)) = \\operatorname{Tr}(\\operatorname{diag}(s^2)) = \\sum_{j=1}^k s_j^2$.\n-   Quadratic term: $(0 - m)^T I_k^{-1} (0 - m) = m^T m = \\|m\\|_2^2$.\n-   Log-determinant term: $\\log\\frac{\\det I_k}{\\det(\\operatorname{diag}(s^2))} = \\log(1) - \\log(\\prod_{j=1}^k s_j^2) = -\\sum_{j=1}^k \\log(s_j^2)$.\n\nSubstituting these into the formula provides the closed-form expression for the KL divergence:\n$$\nD_{KL}(q(z \\mid x) \\| p(z)) = \\frac{1}{2} \\left( \\sum_{j=1}^k s_j^2 + \\|m\\|_2^2 - k - \\sum_{j=1}^k \\log(s_j^2) \\right)\n$$\n\n**3. Complete ELBO Expression**\n\nBy combining the two derived components, $\\mathcal{L}(x) = \\mathbb{E}_{q}[\\log p(x \\mid z)] - D_{KL}(q \\| p)$, we obtain the final explicit ELBO for the given VAE model:\n$$\n\\mathcal{L}(x) = -\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\left( \\|x - Wm - b\\|_2^2 + \\operatorname{Tr}(W^T W \\operatorname{diag}(s^2)) \\right) - \\frac{1}{2} \\left( \\|m\\|_2^2 + \\sum_{j=1}^k s_j^2 - k - \\sum_{j=1}^k \\log(s_j^2) \\right)\n$$\n\n**4. Analysis of the Role of Observation Noise Variance $\\sigma^2$**\n\nIn the VAE framework, the parameters of the model (including $W, b$ and the parameters of the encoder that produces $m, s^2$) are optimized to maximize the ELBO, which is equivalent to minimizing the negative ELBO, $L(x) = -\\mathcal{L}(x)$. The term in the ELBO related to reconstruction is the expected log-likelihood, $\\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)]$. The part of this term that corresponds to the reconstruction error is $-\\frac{1}{2\\sigma^2} \\mathbb{E}_{q(z \\mid x)}[\\|x - (Wz+b)\\|_2^2]$.\n\nThe coefficient that scales the expected squared reconstruction error, $\\mathbb{E}_{q(z \\mid x)}[\\|x - (Wz+b)\\|_2^2]$, is $\\frac{1}{2\\sigma^2}$. This coefficient governs the trade-off between the reconstruction quality and the KL divergence term, which acts as a regularizer.\n\n-   When $\\sigma^2$ is small, the coefficient $\\frac{1}{2\\sigma^2}$ is large. This places a high penalty on reconstruction errors in the loss function $L(x)$. The optimization procedure is driven to make the model's reconstructions very accurate, i.e., to minimize $\\|x - (Wz+b)\\|_2^2$. This prioritizes data fidelity, potentially at the cost of forcing the variational posterior $q(z \\mid x)$ to deviate far from the prior $p(z)$, thus increasing the KL divergence.\n\n-   When $\\sigma^2$ is large, the coefficient $\\frac{1}{2\\sigma^2}$ is small. This lessens the penalty on reconstruction errors, allowing for less precise, or \"blurrier,\" reconstructions. The optimization then places greater relative importance on minimizing the KL divergence term, which encourages the latent representations encoded by $q(z \\mid x)$ to conform more closely to the structure of the standard normal prior $p(z)$.\n\nTherefore, the observation noise variance $\\sigma^2$ acts as a crucial hyperparameter that balances the two primary objectives of the VAE: achieving high-quality reconstructions and maintaining a regularized, structured latent space.",
            "answer": "$$\n\\boxed{-\\frac{d}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\left( \\|x - Wm - b\\|_2^2 + \\operatorname{Tr}(W^T W \\operatorname{diag}(s^2)) \\right) - \\frac{1}{2} \\left( \\|m\\|_2^2 + \\sum_{j=1}^k s_j^2 - k - \\sum_{j=1}^k \\log(s_j^2) \\right)}\n$$"
        },
        {
            "introduction": "While the ELBO provides a powerful objective, its two terms—reconstruction fidelity and KL regularization—can sometimes be at odds during training, leading to a failure mode known as posterior collapse. In this practical coding exercise , you will implement a simple VAE, create a scenario where posterior collapse occurs, and then use KL annealing to successfully train a meaningful generative model. This practice provides crucial insight into the dynamics of VAE training and the importance of carefully balancing the ELBO's components.",
            "id": "3184506",
            "problem": "You are asked to formalize and empirically investigate posterior collapse in a Variational Autoencoder (VAE) using the Evidence Lower Bound (ELBO) in a fully reproducible, programmatic setting. Work entirely in purely mathematical and algorithmic terms. The starting point must be core definitions only: the marginal log-likelihood of observed data and the definition of the Kullback–Leibler divergence. Avoid introducing any shortcut formulas in the problem statement.\n\nConstruct a one-dimensional latent-variable generative model with a one-dimensional observation as follows. The prior is a standard Gaussian over the latent variable $z$, denoted $p_{\\theta}(z)$. The likelihood is a Gaussian over $x$ with fixed known variance and mean given by a linear function of $z$. The approximate posterior is a Gaussian with mean and variance given by linear functions of $x$ composed with a smooth positive mapping. Use a single-sample reparameterization that defaults to the mean of the approximate posterior (that is, take the reparameterization noise to be deterministically zero) to remove stochastic variance in the training loop.\n\nUse only the following foundational base:\n- The definition of the marginal likelihood $p_{\\theta}(x)$ as an integral over latent variables.\n- Jensen’s inequality.\n- The definition of Kullback–Leibler divergence (KL) between two continuous distributions.\n\nTasks:\n1) From these base definitions, obtain a computable lower bound on $\\log p_{\\theta}(x)$ that decomposes into an expectation term under the approximate posterior and a divergence term. Clearly state what the two terms are conceptually, without quoting any standard “shortcut” formulas. Your derivation must start from $\\log p_{\\theta}(x)$ and use only the allowed foundations.\n2) Specify a minimal VAE with the following structure:\n   - Latent prior $p_{\\theta}(z)$ is standard normal in one dimension.\n   - Likelihood $p_{\\theta}(x \\mid z)$ is Gaussian with fixed known variance $\\sigma_x^2$ and mean given by a linear map of $z$.\n   - Approximate posterior $q_{\\phi}(z \\mid x)$ is Gaussian with mean and variance parameterized as linear functions of $x$, where the variance mapping uses a smooth function to ensure positivity.\n   - Training uses the single-sample reparameterization estimator with the reparameterization noise fixed to zero at every step and for every data point, so that the latent sample equals the posterior mean.\n3) Construct a synthetic dataset in one dimension that is prone to posterior collapse when the divergence penalty is strong. Use a balanced mixture of two Gaussians with separated means and small within-component variance. The dataset must be created from first principles, not loaded.\n4) Implement a training loop that performs full-batch stochastic gradient descent on the negative of the lower bound derived in Task $1$, with learning rate $\\eta$ and a specified number of steps $T$. The decoder variance $\\sigma_x^2$ is fixed and known; all other parameters are learned. Compute the expected data-fit term and the divergence term exactly or by the single-sample estimator described above. During training, modulate the contribution of the divergence term with a nonnegative scalar weight $\\beta_t$ that can vary with the iteration index $t$.\n5) Examine posterior collapse by comparing the magnitude of the divergence term and the data-fit term at the end of training for different schedules of $\\beta_t$. Posterior collapse is operationally defined here as the learned approximate posterior being close to the prior so that the divergence term is small while the data-fit term fails to improve substantially beyond a decoder that ignores $z$.\n6) Propose and implement a schedule for $\\beta_t$ (for example, a linear annealing from zero to a target value) that demonstrably reduces collapse on your toy dataset, relative to a strong constant penalty. Justify, in words, why this should help, based on the derivation in Task $1$.\n\nTest Suite and required program output:\n- Use the following fixed and explicit settings across all tests, unless specified otherwise. The observation variance is $\\sigma_x^2 = 0.25$. The dataset has $N = 400$ points, drawn as an equally weighted mixture of two Gaussians with means $-m$ and $+m$, where $m = 2.5$, and component standard deviation $s = 0.3$. The training uses full-batch updates, learning rate $\\eta = 0.05$, number of steps $T = 400$, and deterministic reparameterization noise fixed to zero. Initialize all learnable parameters to $0$.\n- Define three training regimes (each defines a test case):\n  1) Case A (strong constant divergence penalty): $\\beta_t = 4.0$ for all $t \\in \\{1,\\dots,T\\}$.\n  2) Case B (linear annealing): $\\beta_t = 4.0 \\cdot \\min\\left(1, \\frac{t}{T_{\\text{warm}}}\\right)$ with $T_{\\text{warm}} = 200$.\n  3) Case C (no divergence penalty): $\\beta_t = 0$ for all $t \\in \\{1,\\dots,T\\}$.\n- For each case, after training, compute:\n  - The average divergence term across the dataset.\n  - The average negative expected data-fit term across the dataset, computed with the same deterministic single-sample estimator used in training.\n- Compute the following booleans:\n  - collapse_A: true if and only if the average divergence in Case A is below the threshold $t_1 = 0.1$.\n  - prevented_B: true if and only if the average divergence in Case B exceeds $t_2 = 0.2$ and the average negative expected data-fit in Case B is at least $d = 0.2$ smaller than in Case A.\n  - highKL_C: true if and only if the average divergence in Case C exceeds $t_3 = 1.0$.\n- Final Output Format: Your program should produce a single line of output containing the three boolean results as a comma-separated list enclosed in square brackets (for example, \"[True,False,True]\"). No other output is permitted.\n\nAngle units are not applicable. There are no physical units. All numeric thresholds must be interpreted as real numbers without percentage signs. The code must be self-contained, take no input, and rely only on the specified scientific libraries.",
            "solution": "The problem requires the derivation of the Evidence Lower Bound (ELBO), the specification and implementation of a Variational Autoencoder (VAE) for a one-dimensional dataset, and an empirical investigation of posterior collapse under different training schemes. The entire process must be grounded in foundational definitions.\n\n### Task 1: Derivation of the Evidence Lower Bound (ELBO)\n\nThe goal is to find a computable lower bound on the marginal log-likelihood, $\\log p_{\\theta}(x)$, of an observed data point $x$. The marginal likelihood is defined by integrating over the latent variable $z$:\n$$\np_{\\theta}(x) = \\int p_{\\theta}(x, z) dz\n$$\nwhere $p_{\\theta}(x, z) = p_{\\theta}(x|z)p_{\\theta}(z)$ is the joint probability distribution.\n\nThe derivation proceeds as follows:\n1.  We begin with the logarithm of the marginal likelihood, $\\log p_{\\theta}(x)$. We introduce an arbitrary distribution over the latent variable, $q_{\\phi}(z|x)$, which will serve as our approximate posterior. We can multiply and divide by this term inside the integral without changing the value:\n    $$\n    \\log p_{\\theta}(x) = \\log \\left( \\int p_{\\theta}(x, z) \\frac{q_{\\phi}(z|x)}{q_{\\phi}(z|x)} dz \\right)\n    $$\n2.  The integral can be rewritten as an expectation with respect to the distribution $q_{\\phi}(z|x)$:\n    $$\n    \\log p_{\\theta}(x) = \\log \\left( \\mathbb{E}_{z \\sim q_{\\phi}(z|x)} \\left[ \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z|x)} \\right] \\right)\n    $$\n3.  We now apply Jensen's inequality. Since the logarithm function $\\log(\\cdot)$ is concave, we have $\\log(\\mathbb{E}[Y]) \\ge \\mathbb{E}[\\log(Y)]$. Applying this to the expression above yields a lower bound:\n    $$\n    \\log p_{\\theta}(x) \\ge \\mathbb{E}_{z \\sim q_{\\phi}(z|x)} \\left[ \\log \\left( \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z|x)} \\right) \\right]\n    $$\n    This inequality defines the Evidence Lower Bound, denoted $\\mathcal{L}(\\theta, \\phi; x)$.\n\n4.  To reveal its structure, we decompose the term inside the expectation using the definition of the joint probability, $p_{\\theta}(x, z) = p_{\\theta}(x|z)p_{\\theta}(z)$:\n    $$\n    \\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{z \\sim q_{\\phi}(z|x)} [ \\log p_{\\theta}(x|z) + \\log p_{\\theta}(z) - \\log q_{\\phi}(z|x) ]\n    $$\n5.  Rearranging the terms, we can separate it into two main components:\n    $$\n    \\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{z \\sim q_{\\phi}(z|x)} [ \\log p_{\\theta}(x|z) ] - \\mathbb{E}_{z \\sim q_{\\phi}(z|x)} \\left[ \\log \\frac{q_{\\phi}(z|x)}{p_{\\theta}(z)} \\right]\n    $$\n6.  The second term is, by definition, the Kullback–Leibler (KL) divergence between $q_{\\phi}(z|x)$ and $p_{\\theta}(z)$:\n    $$\n    D_{KL}(q_{\\phi}(z|x) || p_{\\theta}(z)) = \\int q_{\\phi}(z|x) \\log \\frac{q_{\\phi}(z|x)}{p_{\\theta}(z)} dz = \\mathbb{E}_{z \\sim q_{\\phi}(z|x)} \\left[ \\log \\frac{q_{\\phi}(z|x)}{p_{\\theta}(z)} \\right]\n    $$\n    Substituting this definition gives the final form of the ELBO:\n    $$\n    \\mathcal{L}(\\theta, \\phi; x) = \\underbrace{\\mathbb{E}_{z \\sim q_{\\phi}(z|x)} [ \\log p_{\\theta}(x|z) ]}_{\\text{Expected Data-Fit Term}} - \\underbrace{D_{KL}(q_{\\phi}(z|x) || p_{\\theta}(z))}_{\\text{Divergence Term}}\n    $$\n    Conceptually, the first term is the expected log-likelihood of the data $x$ under the generative model's likelihood, where the expectation is taken over latent codes $z$ produced by the approximate posterior for that data point. This term encourages the decoder to accurately reconstruct the input data from its latent representation. The second term is the KL divergence, which acts as a regularizer, penalizing the approximate posterior for deviating from the prior distribution over latent variables. Maximizing the ELBO thus involves a trade-off between data reconstruction and regularization.\n\n### Task 2-4: Model Specification, Dataset, and Training\n\nThe problem specifies a VAE with a one-dimensional latent space and one-dimensional observations.\n\n-   **Latent Prior**: The prior over the latent variable $z$ is a standard normal distribution:\n    $$p_{\\theta}(z) = \\mathcal{N}(z | 0, 1)$$\n-   **Approximate Posterior (Encoder)**: The approximate posterior $q_{\\phi}(z|x)$ is a Gaussian distribution whose mean and variance are functions of the input $x$. The parameters $\\phi = \\{w_e, b_e, w_v, b_v\\}$ are learnable.\n    $$q_{\\phi}(z | x) = \\mathcal{N}(z | \\mu_z(x), \\sigma_z^2(x))$$\n    where the mean is a linear function of $x$, $\\mu_z(x) = w_e x + b_e$, and the variance is given by a linear function passed through a smooth positive mapping to ensure positivity: $\\sigma_z^2(x) = \\text{softplus}(w_v x + b_v)$, using the function $\\text{softplus}(y) = \\log(1+e^y)$.\n-   **Likelihood (Decoder)**: The likelihood $p_{\\theta}(x|z)$ is a Gaussian with a fixed variance $\\sigma_x^2 = 0.25$ and a mean that is a linear function of the latent variable $z$. The parameters $\\theta = \\{w_d, b_d\\}$ are learnable.\n    $$p_{\\theta}(x | z) = \\mathcal{N}(x | \\mu_x(z), \\sigma_x^2)$$\n    where $\\mu_x(z) = w_d z + b_d$.\n-   **Dataset**: The synthetic dataset consists of $N=400$ points drawn from an equal mixture of two Gaussians, $\\frac{1}{2}\\mathcal{N}(-2.5, 0.3^2) + \\frac{1}{2}\\mathcal{N}(2.5, 0.3^2)$. This bimodal structure is challenging for a simple model and serves to highlight the effects of posterior collapse.\n-   **Training Objective**: We maximize the ELBO, which is equivalent to minimizing its negative. A weighting factor $\\beta_t$ is applied to the KL divergence term, leading to the loss function for a single sample $x_i$ at training step $t$:\n    $$L_t(x_i) = - \\mathbb{E}_{z \\sim q_{\\phi}(z|x_i)} [ \\log p_{\\theta}(x_i|z) ] + \\beta_t D_{KL}(q_{\\phi}(z|x_i) || p_{\\theta}(z))$$\n    As specified, a single-sample estimator with zero reparameterization noise is used, meaning for each $x_i$, we set the latent sample to be the mean of the approximate posterior: $z_i = \\mu_z(x_i)$. The expectation is thus replaced by a direct evaluation at this sample.\n    The two loss components are:\n    1.  **Negative Expected Data-Fit Term (Reconstruction Loss)**:\n        $$-\\log p_{\\theta}(x_i|z_i=\\mu_z(x_i)) = \\frac{1}{2}\\log(2\\pi\\sigma_x^2) + \\frac{(x_i - \\mu_x(\\mu_z(x_i)))^2}{2\\sigma_x^2}$$\n    2.  **Divergence Term**: The KL divergence between two one-dimensional Gaussians $q(z|x_i) = \\mathcal{N}(\\mu_z(x_i), \\sigma_z^2(x_i))$ and $p(z) = \\mathcal{N}(0, 1)$ has a closed-form analytical solution:\n        $$D_{KL}(q_{\\phi}(z|x_i) || p_{\\theta}(z)) = \\frac{1}{2} \\left( \\mu_z(x_i)^2 + \\sigma_z^2(x_i) - \\log(\\sigma_z^2(x_i)) - 1 \\right)$$\n-   **Algorithm**: Training proceeds via full-batch gradient descent for $T=400$ steps with a learning rate $\\eta=0.05$. All learnable parameters $\\{w_d, b_d, w_e, b_e, w_v, b_v\\}$ are initialized to $0$. In each step, the gradients of the total batch loss $L_t = \\frac{1}{N}\\sum_i L_t(x_i)$ with respect to all parameters are computed and the parameters are updated.\n\n### Task 5-6: Posterior Collapse and Mitigation\n\n**Posterior collapse** is a phenomenon where the approximate posterior $q_{\\phi}(z|x)$ becomes nearly identical to the prior $p(z)$ for all inputs $x$. In our model, this corresponds to the learned parameters driving $\\mu_z(x) \\to 0$ and $\\sigma_z^2(x) \\to 1$, which forces the divergence term $D_{KL}(q_{\\phi}(z|x) || p_{\\theta}(z)) \\to 0$. When this happens, the latent code $z$ contains no information about the input $x$, and the decoder is forced to model the data distribution unconditionally. For our bimodal dataset, a single Gaussian decoder cannot model the data well, leading to a high reconstruction error.\n\nA large, constant divergence penalty $\\beta_t$ (Case A: $\\beta_t = 4.0$) can cause posterior collapse. At the start of training, the encoder is weak and produces an uninformative latent code. The large $\\beta_t$ heavily penalizes any deviation from the prior, and the gradient descent process finds it easiest to reduce the total loss by collapsing the posterior to the prior, even at the cost of poor reconstruction.\n\nThe proposed mitigation strategy is **KL annealing** (Case B), where the weight $\\beta_t$ starts at $0$ and is gradually increased to its target value. The schedule is $\\beta_t = 4.0 \\cdot \\min(1, \\frac{t}{200})$. This approach should help for the following reason: by starting with $\\beta_t=0$, the training initially focuses exclusively on minimizing the reconstruction error. This forces the encoder to learn to pass meaningful information about $x$ through the latent code $z$ so that the decoder can effectively reconstruct the bimodal data. Once the model has learned a useful, non-trivial latent representation, the KL penalty is gradually introduced. At this stage, the model is in a better region of the parameter space and is less likely to revert to the trivial \"collapsed\" solution. It is more likely to find a desirable local minimum that balances good reconstruction with a regularized posterior.\n\nThe implementation will compare these two cases along with a baseline where $\\beta_t=0$ (Case C), which removes the regularization entirely, to empirically verify these effects.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the VAE posterior collapse problem by implementing and training\n    a one-dimensional VAE under three different beta schedules.\n    \"\"\"\n\n    # --- Problem Constants ---\n    SIGMA_X_SQ = 0.25\n    N_POINTS = 400\n    DATASET_MEAN_SEP = 2.5\n    DATASET_STD = 0.3\n    LEARNING_RATE = 0.05\n    N_STEPS = 400\n    RANDOM_SEED = 42\n\n    # --- Evaluation Thresholds ---\n    T1_COLLAPSE = 0.1\n    T2_PREVENTED_DIV = 0.2\n    D_PREVENTED_FIT = 0.2\n    T3_HIGH_KL = 1.0\n\n    # --- Helper Functions ---\n    def softplus(x):\n        return np.log1p(np.exp(x))\n\n    def sigmoid(x):\n        return 1.0 / (1.0 + np.exp(-x))\n\n    # --- Dataset Generation ---\n    np.random.seed(RANDOM_SEED)\n    half_n = N_POINTS // 2\n    data_neg = np.random.normal(loc=-DATASET_MEAN_SEP, scale=DATASET_STD, size=half_n)\n    data_pos = np.random.normal(loc=DATASET_MEAN_SEP, scale=DATASET_STD, size=half_n)\n    X = np.concatenate([data_neg, data_pos]).reshape(-1, 1)\n\n    def train_vae(beta_schedule_fn):\n        \"\"\"\n        Trains a VAE with a given beta schedule.\n        \"\"\"\n        # Learnable parameters, initialized to 0\n        params = {\n            'w_e': 0.0, 'b_e': 0.0,  # Encoder mean\n            'w_v': 0.0, 'b_v': 0.0,  # Encoder variance\n            'w_d': 0.0, 'b_d': 0.0   # Decoder mean\n        }\n\n        for t in range(1, N_STEPS + 1):\n            beta_t = beta_schedule_fn(t)\n\n            # --- Forward Pass ---\n            # Encoder\n            mu_z = params['w_e'] * X + params['b_e']\n            pre_sigma_z = params['w_v'] * X + params['b_v']\n            sigma_z_sq = softplus(pre_sigma_z)\n\n            # Deterministic latent sample\n            z = mu_z\n\n            # Decoder\n            mu_x_recon = params['w_d'] * z + params['b_d']\n\n            # --- Loss Calculation ---\n            # Negative expected data-fit term (Reconstruction Loss)\n            # The constant term is kept for correctness, though it doesn't affect gradients.\n            recon_loss = 0.5 * np.log(2 * np.pi * SIGMA_X_SQ) + (X - mu_x_recon)**2 / (2 * SIGMA_X_SQ)\n            \n            # Divergence term (KL Divergence)\n            kl_div = 0.5 * (-np.log(sigma_z_sq) + sigma_z_sq + mu_z**2 - 1)\n\n            # Total loss (we minimize this)\n            # loss_val = np.mean(recon_loss + beta_t * kl_div)\n\n            # --- Backward Pass (Gradients) ---\n            # Gradient of recon_loss with respect to decoder mean\n            grad_mu_x_recon = (mu_x_recon - X) / SIGMA_X_SQ\n            \n            # Gradients for decoder parameters\n            grad_w_d = np.mean(grad_mu_x_recon * z)\n            grad_b_d = np.mean(grad_mu_x_recon)\n\n            # Backpropagate gradient to z\n            grad_z = grad_mu_x_recon * params['w_d']\n            \n            # Gradient from KL term wrt mu_z\n            grad_kl_mu_z = mu_z\n\n            # Total gradient wrt mu_z (from recon and KL terms)\n            grad_mu_z = grad_z + beta_t * grad_kl_mu_z\n\n            # Gradients for encoder mean parameters\n            grad_w_e = np.mean(grad_mu_z * X)\n            grad_b_e = np.mean(grad_mu_z)\n\n            # Gradient from KL term wrt sigma_z_sq\n            grad_kl_sigma_z_sq = 0.5 * (1 - 1 / sigma_z_sq)\n            \n            # Total gradient wrt sigma_z_sq\n            grad_sigma_z_sq = beta_t * grad_kl_sigma_z_sq\n\n            # Backpropagate gradient to pre_sigma_z\n            grad_pre_sigma_z = grad_sigma_z_sq * sigmoid(pre_sigma_z)\n\n            # Gradients for encoder variance parameters\n            grad_w_v = np.mean(grad_pre_sigma_z * X)\n            grad_b_v = np.mean(grad_pre_sigma_z)\n\n            # --- Parameter Update ---\n            params['w_d'] -= LEARNING_RATE * grad_w_d\n            params['b_d'] -= LEARNING_RATE * grad_b_d\n            params['w_e'] -= LEARNING_RATE * grad_w_e\n            params['b_e'] -= LEARNING_RATE * grad_b_e\n            params['w_v'] -= LEARNING_RATE * grad_w_v\n            params['b_v'] -= LEARNING_RATE * grad_b_v\n        \n        # --- Final Evaluation ---\n        # Re-compute final loss terms with final parameters\n        mu_z = params['w_e'] * X + params['b_e']\n        pre_sigma_z = params['w_v'] * X + params['b_v']\n        sigma_z_sq = softplus(pre_sigma_z)\n        z = mu_z\n        mu_x_recon = params['w_d'] * z + params['b_d']\n        \n        final_recon_loss = 0.5 * np.log(2 * np.pi * SIGMA_X_SQ) + (X - mu_x_recon)**2 / (2 * SIGMA_X_SQ)\n        final_kl_div = 0.5 * (-np.log(sigma_z_sq) + sigma_z_sq + mu_z**2 - 1)\n        \n        avg_neg_data_fit = np.mean(final_recon_loss)\n        avg_divergence = np.mean(final_kl_div)\n        \n        return avg_divergence, avg_neg_data_fit\n\n    # --- Test Cases ---\n    test_cases = [\n        # Case A: Strong constant divergence penalty\n        (lambda t: 4.0),\n        # Case B: Linear annealing\n        (lambda t: 4.0 * min(1.0, t / 200.0)),\n        # Case C: No divergence penalty\n        (lambda t: 0.0),\n    ]\n\n    results = []\n    for beta_fn in test_cases:\n        div, fit = train_vae(beta_fn)\n        results.append({'divergence': div, 'neg_fit': fit})\n\n    # --- Compute Booleans ---\n    # Case A results\n    avg_div_A = results[0]['divergence']\n    avg_neg_fit_A = results[0]['neg_fit']\n    \n    # Case B results\n    avg_div_B = results[1]['divergence']\n    avg_neg_fit_B = results[1]['neg_fit']\n\n    # Case C results\n    avg_div_C = results[2]['divergence']\n\n    collapse_A = avg_div_A < T1_COLLAPSE\n    prevented_B = (avg_div_B > T2_PREVENTED_DIV) and (avg_neg_fit_B < avg_neg_fit_A - D_PREVENTED_FIT)\n    highKL_C = avg_div_C > T3_HIGH_KL\n    \n    final_booleans = [collapse_A, prevented_B, highKL_C]\n\n    # --- Final Output ---\n    print(f\"[{','.join(map(str, final_booleans))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The ELBO is more than just a training objective for standard autoencoders; it is a general framework for performing approximate inference. This advanced exercise  demonstrates this flexibility by tasking you with deriving and implementing the ELBO for a model with incomplete or missing data. By correctly marginalizing out the unobserved components, you will see how the ELBO provides a principled way to perform tasks like image inpainting and reinforce the fundamental connection between the ELBO and the true log-marginal likelihood.",
            "id": "3184447",
            "problem": "Consider a latent-variable generative model used in deep learning for images, and suppose a subset of pixels is missing due to occlusion or corruption. The goal is to derive a principled treatment of missing data in the Evidence Lower Bound (ELBO) and to verify the derivation numerically on small Modified National Institute of Standards and Technology (MNIST) dataset-inspired toy images with masks that remove some pixels.\n\nStart from the following fundamental definitions and facts:\n- Bayes' rule: for random variables $x$ and $z$, $p(z \\mid x) = \\dfrac{p(x \\mid z) p(z)}{p(x)}$.\n- The marginal likelihood integrates out the latent variables: $p(x) = \\int p(x \\mid z) p(z) \\, dz$.\n- Jensen's inequality: for any integrable function $f$ and random variable $Z$, $\\log \\mathbb{E}[f(Z)] \\geq \\mathbb{E}[\\log f(Z)]$.\n\nAssume a linear-Gaussian latent-variable model with the following specifications:\n- Latent dimension $k = 2$ and data dimension $d = 16$.\n- Prior $p(z) = \\mathcal{N}(0, I_k)$.\n- Decoder (likelihood) $p(x \\mid z) = \\mathcal{N}(W z + b, \\sigma_x^2 I_d)$, where $W \\in \\mathbb{R}^{d \\times k}$ and $b \\in \\mathbb{R}^{d}$ are fixed parameters, and $\\sigma_x^2 > 0$ is a known noise variance.\n\nLet $m \\in \\{0,1\\}^d$ be a binary mask that indicates which components of $x$ are observed: for index $i \\in \\{1,\\dots,d\\}$, $m_i = 1$ means pixel $x_i$ is observed, and $m_i = 0$ means pixel $x_i$ is missing. Denote the observed subset as $x_{\\mathrm{obs}} = \\{x_i : m_i = 1\\}$ and the corresponding submatrices $W_{\\mathrm{obs}}$ (rows of $W$ where $m_i = 1$) and $b_{\\mathrm{obs}}$ (entries of $b$ where $m_i = 1$). Missing components $x_{\\mathrm{miss}}$ are to be integrated out in any likelihood term.\n\nTasks:\n1. Derive, starting only from the fundamental definitions and facts listed above, an ELBO for the partially observed data case, that is, a lower bound on $\\log p(x_{\\mathrm{obs}})$ that explicitly integrates over missing components and does not condition on them. Your derivation should make precise how the mask $m$ changes the likelihood term and how the missing pixels $x_{\\mathrm{miss}}$ are integrated out.\n2. Specialize your derivation to the linear-Gaussian model given above and derive the closed-form posterior distribution $q(z \\mid x_{\\mathrm{obs}})$ that optimizes the ELBO within the family of Gaussian distributions. Your derivation should yield a Gaussian with mean and covariance expressed in terms of $W_{\\mathrm{obs}}$, $b_{\\mathrm{obs}}$, $x_{\\mathrm{obs}}$, and $\\sigma_x^2$.\n3. Implement a program that, for each test case below, computes:\n   - the exact log marginal density $\\log p(x_{\\mathrm{obs}})$ under the model by analytically integrating out $z$ and $x_{\\mathrm{miss}}$, and\n   - the masked ELBO evaluated at the exact posterior $q(z \\mid x_{\\mathrm{obs}})$ you derived.\n   Return the absolute difference between these two quantities for each test case. Each result must be a real-valued float.\n\nModel parameters to use in your implementation:\n- $d = 16$, arranged as a $4 \\times 4$ image and flattened in row-major order.\n- $k = 2$.\n- The decoder matrix $W \\in \\mathbb{R}^{16 \\times 2}$ is defined by two stroke-like basis columns on the $4 \\times 4$ grid:\n  - Column $1$ (vertical stroke along the second column of the image): entries $W_{i,1} = 0.8$ at flattened indices $\\{1, 5, 9, 13\\}$ and $W_{i,1} = 0$ elsewhere.\n  - Column $2$ (horizontal stroke along the third row of the image): entries $W_{i,2} = 0.7$ at flattened indices $\\{8, 9, 10, 11\\}$ and $W_{i,2} = 0$ elsewhere.\n- The bias vector $b \\in \\mathbb{R}^{16}$ has entries $b_i = 0.1$ for all $i \\in \\{1,\\dots,16\\}$.\n\nTest suite of images and masks (all images are $4 \\times 4$ grids flattened in row-major order; intensities are in $[0,1]$):\n- Test case $1$ (\"zero-like\" ring image, moderate noise, scattered missing pixels):\n  - Image $x^{(1)}$ as a ring: border pixels have intensity $0.9$ and interior pixels have intensity $0.1$. Concretely, the $4 \\times 4$ matrix is\n    $\n    \\begin{pmatrix}\n    0.9 & 0.9 & 0.9 & 0.9 \\\\\n    0.9 & 0.1 & 0.1 & 0.9 \\\\\n    0.9 & 0.1 & 0.1 & 0.9 \\\\\n    0.9 & 0.9 & 0.9 & 0.9\n    \\end{pmatrix}\n    $.\n  - Mask $m^{(1)}$ with missing pixels at flattened indices $\\{2, 3, 10, 11\\}$ and observed elsewhere. As a $4 \\times 4$ binary matrix:\n    $\n    \\begin{pmatrix}\n    1 & 1 & 0 & 0 \\\\\n    1 & 1 & 1 & 1 \\\\\n    1 & 1 & 0 & 0 \\\\\n    1 & 1 & 1 & 1\n    \\end{pmatrix}\n    $.\n  - Noise variance $\\sigma_{x,1}^2 = 0.05$.\n- Test case $2$ (same \"zero-like\" image, higher noise, contiguous missing block):\n  - Image $x^{(2)}$ equal to $x^{(1)}$.\n  - Mask $m^{(2)}$ with missing top-left block at flattened indices $\\{0, 1, 4, 5\\}$ and observed elsewhere. As a $4 \\times 4$ binary matrix:\n    $\n    \\begin{pmatrix}\n    0 & 0 & 1 & 1 \\\\\n    0 & 0 & 1 & 1 \\\\\n    1 & 1 & 1 & 1 \\\\\n    1 & 1 & 1 & 1\n    \\end{pmatrix}\n    $.\n  - Noise variance $\\sigma_{x,2}^2 = 0.2$.\n- Test case $3$ (\"one-like\" vertical line image, very high noise, entire last row missing):\n  - Image $x^{(3)}$ with a vertical line on the second column: pixels on the second column have intensity $0.9$ and all other pixels have intensity $0.1$. Concretely, the $4 \\times 4$ matrix is\n    $\n    \\begin{pmatrix}\n    0.1 & 0.9 & 0.1 & 0.1 \\\\\n    0.1 & 0.9 & 0.1 & 0.1 \\\\\n    0.1 & 0.9 & 0.1 & 0.1 \\\\\n    0.1 & 0.9 & 0.1 & 0.1\n    \\end{pmatrix}\n    $.\n  - Mask $m^{(3)}$ with missing entire last row at flattened indices $\\{12, 13, 14, 15\\}$ and observed elsewhere. As a $4 \\times 4$ binary matrix:\n    $\n    \\begin{pmatrix}\n    1 & 1 & 1 & 1 \\\\\n    1 & 1 & 1 & 1 \\\\\n    1 & 1 & 1 & 1 \\\\\n    0 & 0 & 0 & 0\n    \\end{pmatrix}\n    $.\n  - Noise variance $\\sigma_{x,3}^2 = 1.0$.\n\nImplementation requirements:\n- For each test case, flatten the $4 \\times 4$ image into a length-$16$ vector in row-major order, and construct the observed index set from the mask.\n- Compute the exact log marginal $\\log p(x_{\\mathrm{obs}})$ by analytically integrating out $z$ under the linear-Gaussian model and restricting to the observed entries via $W_{\\mathrm{obs}}$ and $b_{\\mathrm{obs}}$.\n- Derive and implement the masked ELBO evaluated at the exact posterior $q(z \\mid x_{\\mathrm{obs}})$ for the linear-Gaussian model.\n- Return the absolute differences for all three test cases as a single line of output containing the results as a comma-separated list enclosed in square brackets, with each float rounded to $10$ decimal places, for example, $[r_1,r_2,r_3]$.\n\nNotes:\n- Angles do not appear in this problem, so no angle unit specification is needed.\n- There are no physical units involved; all quantities are dimensionless.\n- The answer for each test case must be a float.",
            "solution": "The problem requires a three-part analysis: (1) a derivation of the Evidence Lower Bound (ELBO) for a generative model with missing data, (2) the specialization of this result to a linear-Gaussian model to find the optimal variational posterior, and (3) a numerical implementation to verify the theoretical results. The core of the problem lies in demonstrating the relationship between the log-marginal likelihood and the ELBO when the variational distribution matches the true posterior.\n\n### Part 1: Derivation of the ELBO for Partially Observed Data\n\nWe are given a latent variable model $p(x, z) = p(x \\mid z)p(z)$. The data vector $x \\in \\mathbb{R}^d$ is partially observed. Let $m \\in \\{0, 1\\}^d$ be a binary mask where $m_i = 1$ indicates that $x_i$ is observed and $m_i = 0$ indicates it is missing. We denote the set of observed data as $x_{\\mathrm{obs}}$ and missing data as $x_{\\mathrm{miss}}$. Our goal is to find a lower bound on the log-marginal likelihood of the observed data, $\\log p(x_{\\mathrm{obs}})$.\n\nThe marginal likelihood $p(x_{\\mathrm{obs}})$ is obtained by integrating over both the latent variables $z$ and the missing data $x_{\\mathrm{miss}}$:\n$$\np(x_{\\mathrm{obs}}) = \\int \\int p(x_{\\mathrm{obs}}, x_{\\mathrm{miss}}, z) \\, dx_{\\mathrm{miss}} \\, dz\n$$\nUsing the chain rule, $p(x, z) = p(x \\mid z) p(z)$, we can write:\n$$\np(x_{\\mathrm{obs}}) = \\int \\left( \\int p(x_{\\mathrm{obs}}, x_{\\mathrm{miss}} \\mid z) \\, dx_{\\mathrm{miss}} \\right) p(z) \\, dz\n$$\nLet's define the likelihood of the observed data given $z$ as $p(x_{\\mathrm{obs}} \\mid z) = \\int p(x \\mid z) \\, dx_{\\mathrm{miss}}$. Then the marginal likelihood becomes:\n$$\np(x_{\\mathrm{obs}}) = \\int p(x_{\\mathrm{obs}} \\mid z) p(z) \\, dz\n$$\nTo derive the ELBO, we introduce an arbitrary variational distribution $q(z)$ over the latent variables. We can write the log-marginal likelihood as:\n$$\n\\log p(x_{\\mathrm{obs}}) = \\log \\int p(x_{\\mathrm{obs}} \\mid z) p(z) \\frac{q(z)}{q(z)} \\, dz = \\log \\mathbb{E}_{q(z)} \\left[ \\frac{p(x_{\\mathrm{obs}} \\mid z) p(z)}{q(z)} \\right]\n$$\nUsing Jensen's inequality, which states that for a concave function like the logarithm, $\\log \\mathbb{E}[Y] \\ge \\mathbb{E}[\\log Y]$, we obtain the lower bound:\n$$\n\\log p(x_{\\mathrm{obs}}) \\ge \\mathbb{E}_{q(z)} \\left[ \\log \\left( \\frac{p(x_{\\mathrm{obs}} \\mid z) p(z)}{q(z)} \\right) \\right]\n$$\nThis inequality defines the Evidence Lower Bound, $\\mathcal{L}(q)$:\n$$\n\\mathcal{L}(q) = \\mathbb{E}_{q(z)} [ \\log p(x_{\\mathrm{obs}} \\mid z) + \\log p(z) - \\log q(z) ]\n$$\nThis can be rewritten in a more familiar form:\n$$\n\\mathcal{L}(q) = \\mathbb{E}_{q(z)}[\\log p(x_{\\mathrm{obs}} \\mid z)] - D_{KL}(q(z) || p(z))\n$$\nwhere the first term is the expected log-likelihood of the observed data under the variational posterior, and the second term is the Kullback-Leibler (KL) divergence between the variational posterior $q(z)$ and the prior $p(z)$. This expression correctly accounts for missing data by marginalizing them out from the likelihood term $p(x \\mid z)$.\n\n### Part 2: Optimal Gaussian Posterior for the Linear-Gaussian Model\n\nWe are given a linear-Gaussian model:\n- Prior: $p(z) = \\mathcal{N}(z; 0, I_k)$\n- Likelihood: $p(x \\mid z) = \\mathcal{N}(x; Wz + b, \\sigma_x^2 I_d)$\n\nThe full likelihood is a product of independent Gaussians for each dimension. Therefore, integrating out the missing dimensions $x_{\\mathrm{miss}}$ simply removes the corresponding terms from the product:\n$$\np(x_{\\mathrm{obs}} \\mid z) = \\int \\prod_{i=1}^d p(x_i \\mid z) \\, dx_{\\mathrm{miss}} = \\prod_{i: m_i=1} p(x_i \\mid z)\n$$\nThis implies that the distribution of $x_{\\mathrm{obs}}$ conditioned on $z$ is also Gaussian:\n$$\np(x_{\\mathrm{obs}} \\mid z) = \\mathcal{N}(x_{\\mathrm{obs}}; W_{\\mathrm{obs}}z + b_{\\mathrm{obs}}, \\sigma_x^2 I_{d_{\\mathrm{obs}}})\n$$\nwhere $W_{\\mathrm{obs}}$, $b_{\\mathrm{obs}}$ are sub-blocks corresponding to the observed dimensions, and $d_{\\mathrm{obs}}$ is the number of observed dimensions.\n\nThe ELBO is maximized when the variational distribution $q(z)$ is equal to the true posterior $p(z \\mid x_{\\mathrm{obs}})$. Since the prior and likelihood are Gaussian, the posterior is also Gaussian. We derive its parameters using Bayes' rule:\n$$\np(z \\mid x_{\\mathrm{obs}}) \\propto p(x_{\\mathrm{obs}} \\mid z) p(z)\n$$\nTaking the logarithm and ignoring constants:\n$$\n\\log p(z \\mid x_{\\mathrm{obs}}) \\propto -\\frac{1}{2\\sigma_x^2} (x_{\\mathrm{obs}} - W_{\\mathrm{obs}}z - b_{\\mathrm{obs}})^T(x_{\\mathrm{obs}} - W_{\\mathrm{obs}}z - b_{\\mathrm{obs}}) - \\frac{1}{2} z^T z\n$$\nExpanding and collecting terms in $z$:\n$$\n\\log p(z \\mid x_{\\mathrm{obs}}) \\propto -\\frac{1}{2} \\left[ z^T \\left(\\frac{1}{\\sigma_x^2} W_{\\mathrm{obs}}^T W_{\\mathrm{obs}} + I_k\\right) z - 2 z^T \\left(\\frac{1}{\\sigma_x^2} W_{\\mathrm{obs}}^T (x_{\\mathrm{obs}} - b_{\\mathrm{obs}})\\right) \\right]\n$$\nBy completing the square, we can identify this as the log-density of a Gaussian distribution $\\mathcal{N}(z; \\mu_{z|x}, \\Sigma_{z|x})$. By matching the quadratic and linear terms of $z$ with the general form $\\log \\mathcal{N}(z; \\mu, \\Sigma) \\propto -\\frac{1}{2}(z^T\\Sigma^{-1}z - 2z^T\\Sigma^{-1}\\mu)$, we find the posterior parameters:\n- Posterior Precision (Inverse Covariance): $\\Sigma_{z|x}^{-1} = \\frac{1}{\\sigma_x^2} W_{\\mathrm{obs}}^T W_{\\mathrm{obs}} + I_k$\n- Posterior Covariance: $\\Sigma_{z|x} = \\left(\\frac{1}{\\sigma_x^2} W_{\\mathrm{obs}}^T W_{\\mathrm{obs}} + I_k\\right)^{-1}$\n- Posterior Mean: $\\mu_{z|x} = \\Sigma_{z|x} \\left( \\frac{1}{\\sigma_x^2} W_{\\mathrm{obs}}^T (x_{\\mathrm{obs}} - b_{\\mathrm{obs}}) \\right)$\n\nThe optimal Gaussian variational distribution $q(z \\mid x_{\\mathrm{obs}})$ is this exact posterior, $p(z \\mid x_{\\mathrm{obs}})$.\n\n### Part 3: Equivalence of ELBO and Log-Marginal Likelihood\n\nThe relationship between the log-marginal likelihood and the ELBO is given by:\n$$\n\\log p(x_{\\mathrm{obs}}) = \\mathcal{L}(q) + D_{KL}(q(z) || p(z \\mid x_{\\mathrm{obs}}))\n$$\nWhen the variational distribution $q(z)$ is set to be the true posterior $p(z \\mid x_{\\mathrm{obs}})$, the KL divergence term becomes $D_{KL}(p(z \\mid x_{\\mathrm{obs}}) || p(z \\mid x_{\\mathrm{obs}})) = 0$. Consequently, the ELBO becomes equal to the log-marginal likelihood:\n$$\n\\log p(x_{\\mathrm{obs}}) = \\mathcal{L}(q(z) = p(z \\mid x_{\\mathrm{obs}}))\n$$\nThe problem asks to compute the absolute difference between these two quantities. Theoretically, this difference is zero. The numerical implementation will verify this up to floating-point precision.\n\n### Part 4: Computational Strategy\n\nWe will compute both quantities separately and then find their absolute difference.\n\n1.  **Exact Log-Marginal Likelihood $\\log p(x_{\\mathrm{obs}})$**: Since the model is linear-Gaussian, we can analytically integrate out $z$.\n    - $z \\sim \\mathcal{N}(0, I_k)$\n    - $x_{\\mathrm{obs}} \\mid z \\sim \\mathcal{N}(W_{\\mathrm{obs}}z + b_{\\mathrm{obs}}, \\sigma_x^2 I_{d_{\\mathrm{obs}}})$\n    The marginal distribution of $x_{\\mathrm{obs}}$ is obtained by marginalizing over $z$, which results in:\n    $$\n    p(x_{\\mathrm{obs}}) = \\mathcal{N}(x_{\\mathrm{obs}}; b_{\\mathrm{obs}}, W_{\\mathrm{obs}}W_{\\mathrm{obs}}^T + \\sigma_x^2 I_{d_{\\mathrm{obs}}})\n    $$\n    We can compute $\\log p(x_{\\mathrm{obs}})$ by evaluating the log-probability density function of this multivariate Gaussian at the given vector $x_{\\mathrm{obs}}$.\n\n2.  **ELBO at the Optimal Posterior**: We compute $\\mathcal{L}(q)$ with $q(z) = p(z \\mid x_{\\mathrm{obs}}) = \\mathcal{N}(z; \\mu_{z|x}, \\Sigma_{z|x})$.\n    $$\n    \\mathcal{L}(q) = \\mathbb{E}_{q(z)}[\\log p(x_{\\mathrm{obs}} \\mid z)] - D_{KL}(q(z) || p(z))\n    $$\n    The terms are calculated as follows:\n    - **Expected Log-Likelihood**:\n    $$\n    \\mathbb{E}_{q(z)}[\\log p(x_{\\mathrm{obs}} \\mid z)] = -\\frac{d_{\\mathrm{obs}}}{2} \\log(2\\pi\\sigma_x^2) -\\frac{1}{2\\sigma_x^2} \\mathbb{E}_{q(z)}[\\|x_{\\mathrm{obs}} - (W_{\\mathrm{obs}}z + b_{\\mathrm{obs}})\\|^2]\n    $$\n    The expectation of the squared norm is $\\mathbb{E}_{q(z)}[\\| (x_{\\mathrm{obs}} - b_{\\mathrm{obs}} - W_{\\mathrm{obs}}\\mu_{z|x}) - W_{\\mathrm{obs}}(z-\\mu_{z|x}) \\|^2] = \\|x_{\\mathrm{obs}} - b_{\\mathrm{obs}} - W_{\\mathrm{obs}}\\mu_{z|x}\\|^2 + \\mathrm{Tr}(W_{\\mathrm{obs}}^T W_{\\mathrm{obs}}\\Sigma_{z|x})$.\n    - **KL Divergence to Prior**:\n    For $q(z) = \\mathcal{N}(\\mu_{z|x}, \\Sigma_{z|x})$ and $p(z) = \\mathcal{N}(0, I_k)$, the KL divergence is:\n    $$\n    D_{KL}(q(z) || p(z)) = \\frac{1}{2} \\left( \\mathrm{Tr}(\\Sigma_{z|x}) + \\mu_{z|x}^T \\mu_{z|x} - k - \\log\\det(\\Sigma_{z|x}) \\right)\n    $$\n    These expressions allow for a direct numerical computation of the ELBO. The absolute difference between $\\log p(x_{\\mathrm{obs}})$ and $\\mathcal{L}(q)$ will then be calculated for each test case.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import multivariate_normal\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    \n    # Define model parameters\n    d = 16\n    k = 2\n    \n    # Define decoder weight matrix W\n    W = np.zeros((d, k), dtype=np.float64)\n    # Column 1: vertical stroke\n    W[[1, 5, 9, 13], 0] = 0.8\n    # Column 2: horizontal stroke\n    W[[8, 9, 10, 11], 1] = 0.7\n    \n    # Define decoder bias vector b\n    b = np.full(d, 0.1, dtype=np.float64)\n\n    # Define test cases\n    test_cases = [\n        # Case 1: \"zero-like\" ring, scattered missing pixels\n        {\n            \"x_matrix\": np.array([\n                [0.9, 0.9, 0.9, 0.9],\n                [0.9, 0.1, 0.1, 0.9],\n                [0.9, 0.1, 0.1, 0.9],\n                [0.9, 0.9, 0.9, 0.9]\n            ], dtype=np.float64),\n            \"m_matrix\": np.array([\n                [1, 1, 0, 0],\n                [1, 1, 1, 1],\n                [1, 1, 0, 0],\n                [1, 1, 1, 1]\n            ], dtype=np.float64),\n            \"sigma_sq\": 0.05\n        },\n        # Case 2: \"zero-like\" ring, top-left block missing\n        {\n            \"x_matrix\": np.array([\n                [0.9, 0.9, 0.9, 0.9],\n                [0.9, 0.1, 0.1, 0.9],\n                [0.9, 0.1, 0.1, 0.9],\n                [0.9, 0.9, 0.9, 0.9]\n            ], dtype=np.float64),\n            \"m_matrix\": np.array([\n                [0, 0, 1, 1],\n                [0, 0, 1, 1],\n                [1, 1, 1, 1],\n                [1, 1, 1, 1]\n            ], dtype=np.float64),\n            \"sigma_sq\": 0.2\n        },\n        # Case 3: \"one-like\" line, last row missing\n        {\n            \"x_matrix\": np.array([\n                [0.1, 0.9, 0.1, 0.1],\n                [0.1, 0.9, 0.1, 0.1],\n                [0.1, 0.9, 0.1, 0.1],\n                [0.1, 0.9, 0.1, 0.1]\n            ], dtype=np.float64),\n            \"m_matrix\": np.array([\n                [1, 1, 1, 1],\n                [1, 1, 1, 1],\n                [1, 1, 1, 1],\n                [0, 0, 0, 0]\n            ], dtype=np.float64),\n            \"sigma_sq\": 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        diff = compute_difference(\n            case[\"x_matrix\"], \n            case[\"m_matrix\"], \n            case[\"sigma_sq\"], \n            W, b, k, d\n        )\n        results.append(diff)\n        \n    # Format the final output string as specified.\n    # The rounding to 10 decimal places is handled by the f-string.\n    formatted_results = [f'{r:.10f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef compute_difference(x_matrix, m_matrix, sigma_sq, W, b, k, d):\n    \"\"\"\n    Computes the absolute difference between the exact log marginal likelihood\n    and the ELBO evaluated at the true posterior.\n    \"\"\"\n    # Flatten images and masks to vectors (row-major order)\n    x = x_matrix.flatten()\n    m = m_matrix.flatten()\n    \n    # Create boolean mask for observed pixels\n    mask = m.astype(bool)\n    \n    # Extract observed data and corresponding model parameters\n    x_obs = x[mask]\n    W_obs = W[mask, :]\n    b_obs = b[mask]\n    d_obs = len(x_obs)\n\n    # --- 1. Compute exact log marginal likelihood log p(x_obs) ---\n    # The marginal p(x_obs) is Gaussian with:\n    # mean = b_obs\n    # cov = W_obs @ W_obs.T + sigma_sq * I\n    mu_x_obs = b_obs\n    cov_x_obs = W_obs @ W_obs.T + sigma_sq * np.identity(d_obs, dtype=np.float64)\n    \n    # Use scipy's multivariate_normal for stable calculation\n    log_p_x_obs = multivariate_normal.logpdf(x_obs, mean=mu_x_obs, cov=cov_x_obs)\n\n    # --- 2. Compute the ELBO at the exact posterior q(z) = p(z|x_obs) ---\n    \n    # First, calculate parameters of the posterior q(z) = N(mu_z_x, Sigma_z_x)\n    Sigma_z_x_inv = (1 / sigma_sq) * (W_obs.T @ W_obs) + np.identity(k, dtype=np.float64)\n    Sigma_z_x = np.linalg.inv(Sigma_z_x_inv)\n    \n    mu_z_x = Sigma_z_x @ ((1 / sigma_sq) * W_obs.T @ (x_obs - b_obs))\n\n    # Then, calculate the two terms of the ELBO: E[log p(x_obs|z)] - KL(q||p)\n\n    # Term 1: Expected log-likelihood E_{q(z)}[log p(x_obs|z)]\n    # E[log p(x_obs|z)] = -d_obs/2*log(2*pi*sigma_sq) - 1/(2*sigma_sq) * E[||x_obs - (W_obs*z + b_obs)||^2]\n    # The expectation of the squared norm expands to:\n    # ||x_obs - b_obs - W_obs*mu_z_x||^2 + Tr(W_obs.T @ W_obs @ Sigma_z_x)\n    recon_error_mean = np.sum((x_obs - b_obs - W_obs @ mu_z_x)**2)\n    recon_error_var = np.trace(W_obs.T @ W_obs @ Sigma_z_x)\n    \n    expected_log_likelihood = \\\n        -0.5 * d_obs * np.log(2 * np.pi * sigma_sq) \\\n        - (1 / (2 * sigma_sq)) * (recon_error_mean + recon_error_var)\n\n    # Term 2: KL-divergence D_KL(q(z)||p(z)) where p(z) = N(0, I)\n    # D_KL = 0.5 * (Tr(Sigma_z_x) + mu_z_x.T @ mu_z_x - k - log_det(Sigma_z_x))\n    sign, log_det_Sigma_z_x = np.linalg.slogdet(Sigma_z_x)\n    if sign <= 0: # Covariance must be positive definite\n        log_det_Sigma_z_x = -np.inf\n        \n    kl_divergence = 0.5 * (\n        np.trace(Sigma_z_x) + \n        mu_z_x.T @ mu_z_x - \n        k - \n        log_det_Sigma_z_x\n    )\n\n    # ELBO = E[log p(x_obs|z)] - D_KL\n    elbo = expected_log_likelihood - kl_divergence\n    \n    # --- 3. Compute the absolute difference ---\n    # Theoretically, this difference is 0. Numerically, it will be close to 0.\n    abs_difference = np.abs(log_p_x_obs - elbo)\n    \n    return abs_difference\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}