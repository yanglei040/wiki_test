{
    "hands_on_practices": [
        {
            "introduction": "理论知识需要通过实践来巩固。本节的第一个练习将探讨卷积自编码器的物理结构（即滤波器尺寸）如何直接影响其重建能力。通过这个动手实践，你将具体地观察到感受野大小如何在保留细节和平滑信号之间进行权衡，这是理解卷积网络工作原理的关键一步。",
            "id": "3099855",
            "problem": "要求您实现并分析一个简化的一维卷积自编码器，以研究在两种不同的重建损失（均方误差和平均绝对误差）下，感受野大小如何影响精细细节的重建质量。您的程序必须是一个完整、可运行的脚本，用于计算指定的量并以要求的格式打印最终结果。\n\n从以下基本设定开始：\n\n- 一个一维自编码器通过编码器将输入信号 $x \\in \\mathbb{R}^N$ 映射到潜层表示，并通过解码器进行重建。在本问题中，编码器和解码器都是线性的、移位不变的算子，实现为与有限脉冲响应核的离散互相关。\n- 编码器计算互相关 $y = x \\star w$，其中，对于一个长度为奇数 $K \\ge 1$ 的核 $w \\in \\mathbb{R}^K$，带零填充的离散互相关定义为\n$$\ny[n] = \\sum_{j=0}^{K-1} w[j]\\; x[n + j - c], \\quad c = \\left\\lfloor \\frac{K}{2} \\right\\rfloor,\n$$\n其中对于索引 $m$ 在 $[0, N-1]$ 之外的情况，$x[m] = 0$。这将产生一个与 $x$ 长度相同的输出 $y$。\n- 解码器使用反转的核 $\\tilde{w}[j] = w[K-1-j]$ 计算另一次互相关，得到重建信号 $\\hat{x} = y \\star \\tilde{w}$。这对应于一个权重绑定、步幅为 1 且带零填充的线性卷积自编码器，因此整体映射是一个线性的、时不变的平滑算子。\n- 重建损失是按样本定义并沿信号进行平均的：\n  - 均方误差 (MSE)：$L_2(\\hat{x}, x) = \\frac{1}{N} \\sum_{n=0}^{N-1} \\left( \\hat{x}[n] - x[n] \\right)^2$。\n  - 平均绝对误差 (L1)：$L_1(\\hat{x}, x) = \\frac{1}{N} \\sum_{n=0}^{N-1} \\left| \\hat{x}[n] - x[n] \\right|$。\n- 感受野是核的长度 $K$，核的条目取自一个归一化盒式滤波器：对于 $j \\in \\{0,1,\\dots,K-1\\}$，$w[j] = \\frac{1}{K}$。\n\n您的任务是使用指定的架构（不执行训练；权重由定义固定），量化在 $L_2$ 和 $L_1$ 两种损失下，$K$ 如何影响精细细节的重建。为了分离感受野对精细细节内容的影响，您将评估三个长度为 $N$ 的合成输入：\n\n- $x^{(1)}$：中心的单位脉冲，$x^{(1)}[n] = 1$ 当 $n = \\frac{N}{2}$ 时，否则 $x^{(1)}[n] = 0$。\n- $x^{(2)}$：高频交替余弦，$x^{(2)}[n] = \\cos(\\pi n)$。\n- $x^{(3)}$：低频余弦，$x^{(3)}[n] = \\cos\\!\\left( \\frac{2\\pi f_0 n}{N} \\right)$。\n\n使用 $N = 128$ 和 $f_0 = 4$。\n\n对于下面测试套件中的每个核大小 $K$，计算以下内容：\n\n1. 对于每个信号 $x^{(i)}$（其中 $i \\in \\{1,2,3\\}$），使用上面定义的编码器-解码器生成 $\\hat{x}^{(i)}$。\n2. 计算 $L_2\\!\\left(\\hat{x}^{(i)}, x^{(i)}\\right)$ 和 $L_1\\!\\left(\\hat{x}^{(i)}, x^{(i)}\\right)$。\n3. 对三个信号的损失进行平均以获得\n$$\n\\overline{L}_2(K) = \\frac{1}{3} \\sum_{i=1}^3 L_2\\!\\left(\\hat{x}^{(i)}, x^{(i)}\\right), \\quad\n\\overline{L}_1(K) = \\frac{1}{3} \\sum_{i=1}^3 L_1\\!\\left(\\hat{x}^{(i)}, x^{(i)}\\right).\n$$\n4. 使用脉冲信号量化精细细节的保留程度：计算比率\n$$\nr(K) = \\frac{\\hat{x}^{(1)}\\!\\left[\\frac{N}{2}\\right]}{x^{(1)}\\!\\left[\\frac{N}{2}\\right]}.\n$$\n由于 $x^{(1)}\\!\\left[\\frac{N}{2}\\right] = 1$，这可以简化为 $r(K) = \\hat{x}^{(1)}\\!\\left[\\frac{N}{2}\\right]$。\n\n测试套件：\n\n- 使用核大小 $K \\in \\{1, 3, 7, 15\\}$。\n\n答案规格：\n\n- 对于每个 $K$，按升序生成一个结果列表 $[K, \\overline{L}_2(K), \\overline{L}_1(K), r(K)]$。\n- 您的程序应生成单行输出，其中包含这些列表的逗号分隔列表，并用方括号括起来。值 $\\overline{L}_2(K)$、$\\overline{L}_1(K)$ 和 $r(K)$ 必须四舍五入到六位小数。例如，一个有效的输出格式是\n$[[1,0.000000,0.000000,1.000000],[3, \\dots],[7, \\dots],[15, \\dots]]$\n（打印的行中没有空格）。",
            "solution": "该问题要求对一个简化的一维线性卷积自编码器进行分析。任务的核心是实现指定的信号处理链，并量化由核长度 $K$ 决定的感受野大小如何影响具有不同频率特性的信号的重建。自编码器的权重被固定为归一化盒式滤波器，这意味着不涉及训练。该分析纯粹基于所定义架构的信号处理属性。\n\n输入是一个一维信号 $x \\in \\mathbb{R}^N$。编码器通过与核 $w \\in \\mathbb{R}^K$ 的离散互相关将输入 $x$ 映射到潜层表示 $y$：$y = x \\star w$。解码器通过将反转的核 $\\tilde{w}$ 应用于 $y$ 进行另一次互相关来重建信号 $\\hat{x}$：$\\hat{x} = y \\star \\tilde{w}$。核大小 $K$ 必须是奇数。互相关定义了零填充以保持信号长度为 $N$：\n$$y[n] = \\sum_{j=0}^{K-1} w[j]\\; x[n + j - c], \\quad c = \\left\\lfloor \\frac{K}{2} \\right\\rfloor$$\n核权重由归一化盒式滤波器给出，$w[j] = 1/K$ 对于所有 $j \\in \\{0, \\dots, K-1\\}$。该核的一个关键属性是其对称性，即 $w[j] = w[K-1-j]$。因此，反转的核 $\\tilde{w}$ 与原始核 $w$ 相同。从输入 $x$ 到重建 $\\hat{x}$ 的整体变换因此是两次相同互相关操作的级联：\n$$\\hat{x} = (x \\star w) \\star w$$\n此操作等效于输入信号 $x$ 与一个有效核的单次卷积，该有效核是 $w$ 的自卷积。此变换构成一个线性时不变 (LTI) 系统。\n\n核 $w$ 作为一个移动平均滤波器，是低通滤波器的一种基本形式。连续两次应用此滤波器会产生更强的低通滤波效果。整个自编码器系统的有效脉冲响应是一个长度为 $2K-1$ 的三角滤波器。参数 $K$ 代表感受野大小，控制此滤波器的宽度。较大的 $K$ 会导致更宽的有效滤波器，从而对信号进行更激进的平滑处理。这种增强的平滑会更严重地衰减高频分量和精细细节。该问题使用三个特定信号来探究此行为：\n1.  $x^{(1)}$：单位脉冲。重建信号 $\\hat{x}^{(1)}$ 是系统的脉冲响应，直接揭示了有效滤波器的形状。\n2.  $x^{(2)}$：高频余弦 $\\cos(\\pi n)$。该信号将被低通滤波器强烈衰减，特别是对于较大的 $K$。\n3.  $x^{(3)}$：低频余弦。预计该信号会比 $x^{(2)}$ 更好地保留下来，作为参考。\n\n计算过程如下。对核大小 $K \\in \\{1, 3, 7, 15\\}$ 进行分析。对于每个 $K$ 值，执行以下步骤：\n1.  生成长度为 $N = 128$、频率参数为 $f_0 = 4$ 的三个输入信号 $x^{(1)}$、$x^{(2)}$ 和 $x^{(3)}$。\n    -   $x^{(1)}[n] = \\delta[n - N/2]$，其中 $N/2 = 64$。\n    -   $x^{(2)}[n] = \\cos(\\pi n)$。\n    -   $x^{(3)}[n] = \\cos(2\\pi f_0 n / N)$。\n2.  创建大小为 $K$ 的核 $w$，其条目为 $w[j] = 1/K$。\n3.  对于每个信号 $x^{(i)}$，通过应用指定的互相关操作两次来计算重建信号 $\\hat{x}^{(i)}$。\n4.  使用均方误差 ($L_2$) 和平均绝对误差 ($L_1$) 两个损失函数来衡量重建质量，其定义如下：\n    $$L_2(\\hat{x}, x) = \\frac{1}{N} \\sum_{n=0}^{N-1} (\\hat{x}[n] - x[n])^2$$\n    $$L_1(\\hat{x}, x) = \\frac{1}{N} \\sum_{n=0}^{N-1} |\\hat{x}[n] - x[n]|$$\n5.  将这些单独的损失在三个信号上平均，以产生最终的度量指标 $\\overline{L}_2(K)$ 和 $\\overline{L}_1(K)$。\n6.  精细细节的保留程度通过比率 $r(K) = \\hat{x}^{(1)}[N/2] / x^{(1)}[N/2]$ 来量化。由于 $x^{(1)}[N/2] = 1$，该式简化为 $r(K) = \\hat{x}^{(1)}[N/2]$。基于系统脉冲响应的解析推导证实，该值恰好为 $1/K^2$，这可以作为对实现的一个有价值的检验。\n最终输出聚合了来自测试套件的每个 $K$ 所计算出的这些值。",
            "answer": "```python\nimport numpy as np\n\ndef cross_correlate(x, w):\n    \"\"\"\n    Computes the 1D cross-correlation with zero-padding as defined in the problem.\n    y[n] = sum_{j=0}^{K-1} w[j] * x[n + j - c], where c = floor(K/2).\n    \"\"\"\n    N = len(x)\n    K = len(w)\n    # For odd K, floor(K/2) is equivalent to (K-1)//2.\n    c = (K - 1) // 2\n    y = np.zeros(N, dtype=np.float64)\n    \n    for n in range(N):\n        sum_val = 0.0\n        for j in range(K):\n            m = n + j - c\n            if 0 = m  N:\n                sum_val += w[j] * x[m]\n        y[n] = sum_val\n    return y\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem as specified.\n    \"\"\"\n    # Define problem parameters\n    N = 128\n    f0 = 4\n    kernel_sizes = [1, 3, 7, 15]\n\n    # Generate input signals\n    n_indices = np.arange(N)\n    \n    # x^(1): Unit impulse at the center\n    x1 = np.zeros(N, dtype=np.float64)\n    center_idx = N // 2\n    x1[center_idx] = 1.0\n\n    # x^(2): High-frequency alternating cosine\n    x2 = np.cos(np.pi * n_indices)\n\n    # x^(3): Low-frequency cosine\n    x3 = np.cos(2 * np.pi * f0 * n_indices / N)\n    \n    signals = [x1, x2, x3]\n    \n    all_results = []\n\n    for K in kernel_sizes:\n        # Create the normalized box filter kernel\n        w = np.full(K, 1.0 / K, dtype=np.float64)\n        \n        # The reversed kernel w_tilde is the same as w because w is symmetric\n        w_tilde = w\n        \n        total_l2_loss = 0.0\n        total_l1_loss = 0.0\n        r_K = 0.0\n        \n        # Process each signal\n        for i, x in enumerate(signals):\n            # Form reconstruction x_hat by applying the operation twice\n            # Encoder pass\n            y = cross_correlate(x, w)\n            # Decoder pass\n            x_hat = cross_correlate(y, w_tilde)\n            \n            # Compute losses\n            l2_loss = np.mean((x_hat - x)**2)\n            l1_loss = np.mean(np.abs(x_hat - x))\n            \n            # Accumulate losses for averaging\n            total_l2_loss += l2_loss\n            total_l1_loss += l1_loss\n            \n            # For signal x1, compute the retention ratio r(K)\n            if i == 0:  # Signal x1 is at index 0\n                # r(K) = x_hat[N/2] / x[N/2]. Since x[N/2]=1, it's just x_hat[N/2]\n                r_K = x_hat[center_idx]\n\n        # Average the losses over the three signals\n        avg_l2 = total_l2_loss / 3.0\n        avg_l1 = total_l1_loss / 3.0\n        \n        # Store raw float values for this K\n        result_tuple = [K, avg_l2, avg_l1, r_K]\n        all_results.append(result_tuple)\n        \n    # Format the final output string as required\n    # The f-string formatting with ':.6f' handles rounding to 6 decimal places\n    formatted_results = [f\"[{k},{l2:.6f},{l1:.6f},{r:.6f}]\" for k, l2, l1, r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the solver\nsolve()\n```"
        },
        {
            "introduction": "选择了模型结构后，下一个关键问题是：我们应该如何衡量重建的好坏？这个练习将深入探讨损失函数的选择，特别是为什么在处理二元数据时，二元交叉熵（$L_{\\mathrm{BCE}}$）通常优于均方误差（$L_{\\mathrm{MSE}}$）。通过对梯度的数学推导，你将揭示当$L_{\\mathrm{MSE}}$与sigmoid激活函数结合使用时可能出现的梯度消失问题，这是训练神经网络时的一个核心实践考量。",
            "id": "3099815",
            "problem": "考虑一个单输出自编码器解码器，其输出激活函数为 sigmoid 函数 $\\sigma(z)$，其中 $z$ 是标量预激活（logit）。重建目标是二元的，$x \\in \\{0,1\\}$，预测的重建结果是 $y = \\sigma(z)$。下面定义了两种常见的重建损失：\n\n- 二元交叉熵（BCE）：$L_{\\mathrm{BCE}}(y;x) = -\\left[x \\ln(y) + (1 - x)\\ln(1 - y)\\right]$。\n- 均方误差（MSE）：$L_{\\mathrm{MSE}}(y;x) = \\frac{1}{2}(y - x)^{2}$。\n\n仅从以上定义、sigmoid 的定义 $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$ 以及基础微积分（链式法则）出发，完成以下任务：\n\n1. 对每种损失，推导关于 logit $z$ 的梯度，即 $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}$ 和 $\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}$，并将每个梯度表示为 $y$ 和 $x$ 的函数。\n2. 专门针对二元目标 $x \\in \\{0,1\\}$ 的情况，将梯度幅值 $\\left|\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}\\right|$ 和 $\\left|\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}\\right|$ 简化为仅含 $y$ 的函数。\n3. 推导梯度幅值之比 $r(y)$ 的闭式表达式，\n   $$r(y) = \\frac{\\left|\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}\\right|}{\\left|\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}\\right|},$$\n   对于 $x \\in \\{0,1\\}$，并将其完全简化为 $y$ 的函数。\n4. 根据 $r(y)$ 的表达式简要解释，为什么当 sigmoid 输出在 $y \\approx 0$ 或 $y \\approx 1$ 附近饱和时，二元交叉熵能比均方误差引导更快的学习。\n5. 通过对以下两种情况进行数值计算 $r(y)$，以经验性地验证您的符号结果：$x=1$ 且 $y = 10^{-4}$，以及 $x=0$ 且 $y = 1 - 10^{-4}$。将您的数值四舍五入到四位有效数字。\n\n你的最终答案应该仅为第 3 部分中 $r(y)$ 的简化闭式表达式。",
            "solution": "目标是分析二元交叉熵（BCE）和均方误差（MSE）损失函数关于 logit $z$ 的梯度，对于一个具有 sigmoid 激活函数的单输出单元。此分析将阐明为什么对于二元重建任务，BCE 通常优于 MSE。\n\n推导的核心依赖于微分的链式法则。给定一个损失函数 $L$，它依赖于模型输出 $y$，而 $y$ 又是 logit $z$ 的函数（即 $y = \\sigma(z)$），则损失关于 logit 的梯度为：\n$$\n\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z}\n$$\n首先，我们推导公共项 $\\frac{\\partial y}{\\partial z}$，也就是 sigmoid 函数 $\\sigma(z)$ 的导数。\n给定 $y = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$，其关于 $z$ 的导数为：\n$$\n\\frac{\\partial y}{\\partial z} = \\frac{d}{dz} (1 + \\exp(-z))^{-1} = -1 \\cdot (1 + \\exp(-z))^{-2} \\cdot (-\\exp(-z)) = \\frac{\\exp(-z)}{(1 + \\exp(-z))^2}\n$$\n这个表达式可以通过用 $y = \\sigma(z)$ 来表示进行简化：\n$$\n\\frac{\\partial y}{\\partial z} = \\left(\\frac{1}{1 + \\exp(-z)}\\right) \\left(\\frac{\\exp(-z)}{1 + \\exp(-z)}\\right) = \\left(\\frac{1}{1 + \\exp(-z)}\\right) \\left(\\frac{1 + \\exp(-z) - 1}{1 + \\exp(-z)}\\right) = y (1 - y)\n$$\n这是 sigmoid 函数的一个标准且有用的恒等式。\n\n**1. 梯度推导**\n\n我们现在计算每种损失函数的梯度。\n\n**二元交叉熵（BCE）梯度：**\nBCE 损失定义为 $L_{\\mathrm{BCE}}(y;x) = -\\left[x \\ln(y) + (1 - x)\\ln(1 - y)\\right]$。\n首先，我们求关于 $y$ 的偏导数：\n$$\n\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial y} = -\\left[ \\frac{x}{y} - \\frac{1 - x}{1 - y} \\right] = -\\frac{x(1 - y) - (1 - x)y}{y(1 - y)} = -\\frac{x - xy - y + xy}{y(1 - y)} = -\\frac{x - y}{y(1 - y)} = \\frac{y - x}{y(1 - y)}\n$$\n使用链式法则，我们求得关于 $z$ 的梯度：\n$$\n\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = \\frac{\\partial L_{\\mathrm{BCE}}}{\\partial y} \\frac{\\partial y}{\\partial z} = \\left( \\frac{y - x}{y(1 - y)} \\right) \\cdot (y(1 - y)) = y - x\n$$\n\n**均方误差（MSE）梯度：**\nMSE 损失定义为 $L_{\\mathrm{MSE}}(y;x) = \\frac{1}{2}(y - x)^{2}$。\n首先，我们求关于 $y$ 的偏导数：\n$$\n\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial y} = \\frac{1}{2} \\cdot 2(y - x) \\cdot 1 = y - x\n$$\n使用链式法则，我们求得关于 $z$ 的梯度：\n$$\n\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z} = \\frac{\\partial L_{\\mathrm{MSE}}}{\\partial y} \\frac{\\partial y}{\\partial z} = (y - x) \\cdot (y(1 - y))\n$$\n\n**2. 二元目标的梯度幅值**\n\n我们现在将这些结果专门用于二元目标值 $x \\in \\{0, 1\\}$。sigmoid 函数的输出严格满足 $y \\in (0, 1)$。\n\n**BCE 梯度幅值：**\n梯度为 $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = y - x$。\n- 如果 $x = 0$：$\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = y$。因为 $y  0$，幅值为 $\\left|\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}\\right| = y$。\n- 如果 $x = 1$：$\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = y - 1$。因为 $y  1$，项 $y-1$ 是负的。幅值为 $\\left|\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}\\right| = -(y - 1) = 1 - y$。\n\n**MSE 梯度幅值：**\n梯度为 $\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z} = (y - x)y(1 - y)$。对于 $y \\in (0, 1)$，项 $y(1-y)$ 总是正的。\n- 如果 $x = 0$：$\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z} = y \\cdot y(1 - y) = y^2(1 - y)$。此项为正。幅值为 $\\left|\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}\\right| = y^2(1 - y)$。\n- 如果 $x = 1$：$\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z} = (y - 1)y(1 - y) = -y(1 - y)^2$。此项为负。幅值为 $\\left|\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}\\right| = -(-y(1 - y)^2) = y(1 - y)^2$。\n\n**3. 梯度幅值的比率**\n\n我们计算比率 $r(y) = \\frac{\\left|\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z}\\right|}{\\left|\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z}\\right|}$，针对二元目标 $x$ 的两种情况。\n\n- 如果 $x = 0$：\n$$\nr(y) = \\frac{y}{y^2(1 - y)} = \\frac{1}{y(1 - y)}\n$$\n- 如果 $x = 1$：\n$$\nr(y) = \\frac{1 - y}{y(1 - y)^2} = \\frac{1}{y(1 - y)}\n$$\n在两种情况下，比率的表达式是相同的。简化的闭式表达式为：\n$$\nr(y) = \\frac{1}{y(1 - y)}\n$$\n\n**4. 学习速度的解释**\n\n梯度 $\\frac{\\partial L}{\\partial z}$ 是反向传播以更新输出单元前一层权重的误差信号。更大的梯度幅值通常导致更大的权重更新，从而实现更快的学习，尤其是在预测不正确时。\n\nMSE 梯度 $\\frac{\\partial L_{\\mathrm{MSE}}}{\\partial z} = (y-x)y(1-y)$ 包含因子 $y(1-y) = \\sigma'(z)$。当 sigmoid 输出 $y$ 饱和时，即 $y \\to 0$ 或 $y \\to 1$ 时，此项 $\\sigma'(z)$ 趋近于 $0$。如果模型做出了一个自信但错误的预测（例如，当目标 $x=1$ 时 $y \\approx 0$），$y(1-y)$ 项会变得非常小，导致整个梯度“消失”。这会导致学习极其缓慢，因为模型几乎收不到任何信号来纠正其巨大的错误。\n\n相比之下，BCE 梯度 $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} = y-x$ 没有这个有问题的 $y(1-y)$ 项。对数损失函数的使用恰好抵消了来自链式法则的 $\\sigma'(z)$ 因子。因此，如果模型做出了一个自信但错误的预测，梯度的幅值仍然很大。例如，如果 $x=1$ 且 $y \\approx 0$，梯度 $\\frac{\\partial L_{\\mathrm{BCE}}}{\\partial z} \\approx 0-1 = -1$，这是一个强烈的修正信号。\n\n比率 $r(y) = \\frac{1}{y(1-y)}$ 正式地捕捉了这种行为。当输出 $y$ 在 $0$ 或 $1$ 附近饱和时，分母 $y(1-y)$ 趋近于 $0$，导致比率 $r(y)$ 趋近于无穷大。这表明，在这些饱和区域，BCE 梯度的幅值变得比 MSE 梯度的幅值大得不可限量，从而引导快得多的学习，并帮助模型逃离不良的局部最小值。\n\n**5. 数值验证**\n\n我们为给定情况计算 $r(y) = \\frac{1}{y(1 - y)}$。\n\n- 情况 1：$x=1$ 且 $y = 10^{-4}$。\n  这里，$y = 0.0001$。预测是高度自信且不正确的。\n  $$\n  r(10^{-4}) = \\frac{1}{0.0001 \\times (1 - 0.0001)} = \\frac{1}{0.0001 \\times 0.9999} = \\frac{1}{0.00009999} \\approx 10001.0001\n  $$\n  四舍五入到四位有效数字，结果是 $1.000 \\times 10^4$。\n\n- 情况 2：$x=0$ 且 $y = 1 - 10^{-4}$。\n  这里，$y = 0.9999$。预测同样是高度自信且不正确的。\n  $$\n  r(1 - 10^{-4}) = \\frac{1}{0.9999 \\times (1 - 0.9999)} = \\frac{1}{0.9999 \\times 0.0001} = \\frac{1}{0.00009999} \\approx 10001.0001\n  $$\n  四舍五入到四位有效数字，结果是 $1.000 \\times 10^4$。\n\n数值结果证实，对于 sigmoid 输出饱和的自信且错误的预测，BCE 梯度大约是 MSE 梯度的 $10000$ 倍，从而验证了理论分析。",
            "answer": "$$\n\\boxed{\\frac{1}{y(1 - y)}}\n$$"
        },
        {
            "introduction": "标准的重建损失平等地对待所有像素或数据点的误差，但这在现实世界中往往不成立。本次高级练习将引导你构建一个更智能的模型，它不仅能重建输入，还能预测自己重建结果的不确定性。通过从最大似然估计原理出发推导损失函数，你将学会如何设计一个异方差自编码器，这是迈向构建更强大的概率深度学习模型的重要一步。",
            "id": "3099841",
            "problem": "考虑一个自编码器，其编码器为 $f_{\\phi}$，解码器为 $g_{\\theta}$，通过 $\\hat{x} = g_{\\theta}(f_{\\phi}(x))$ 来重构输入向量 $x \\in \\mathbb{R}^{d}$。你将把重构噪声建模为异方差的：在给定潜码 $z = f_{\\phi}(x)$ 的条件下，坐标 $x_{i}$ 是条件独立的，并服从高斯密度分布，其均值为 $\\mu_{i}(z) = \\hat{x}_{i}$，方差为 $\\sigma_{i}^{2}(x)$，该方差依赖于观测到的输入 $x$。假设目标 $t$ 的一元高斯密度函数为 $p(t \\mid \\mu,\\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\!\\big(-\\frac{(t-\\mu)^{2}}{2\\sigma^{2}}\\big)$，其中均值为 $\\mu$，方差为 $\\sigma^{2}$，并假设在给定 $z$ 的情况下，各维度之间条件独立。\n\n任务 A。基于这些基础，推导出单样本负对数似然 (NLL) $-\\ln p(x \\mid z)$，将其表示为残差 $x_{i}-\\hat{x}_{i}$ 和方差 $\\sigma_{i}^{2}(x)$ 的函数。然后将其重写，以明确一个形式为 $\\sum_{i} w_{i}(x)\\,(x_{i}-\\hat{x}_{i})^{2}$ 的加权均方误差 (MSE) 项，外加一个与方差相关的惩罚项，并根据 $\\sigma_{i}^{2}(x)$ 确定 $w_{i}(x)$。\n\n任务 B。对于一个三维输入 $x \\in \\mathbb{R}^{3}$，其坐标为 $x = (x_{1},x_{2},x_{3})$，假设异方差函数已知，并由 $\\sigma_{i}^{2}(x) = \\alpha_{i} + \\beta_{i} x_{i}^{2}$ 给出，其中 $\\alpha_{i}$ 为严格正数，$\\beta_{i}$ 为非负数。考虑具体实例，其中\n- $x = (\\,1.0,\\,-2.0,\\,0.5\\,)$,\n- $\\hat{x} = (\\,0.8,\\,-1.5,\\,0.4\\,)$,\n- $\\alpha = (\\,0.2,\\,0.5,\\,0.1\\,)$,\n- $\\beta = (\\,0.0,\\,0.1,\\,0.2\\,)$.\n\n使用你推导出的 NLL，为给定的 $x$ 和 $\\hat{x}$ 数值计算完整的单样本 NLL $-\\ln p(x \\mid z)$（包括加权 MSE 项和涉及对数的方差惩罚项）。使用自然对数。将你的最终数值答案四舍五入到四位有效数字。",
            "solution": "### 任务 A：负对数似然的推导\n\n问题陈述，在给定潜码 $z = f_{\\phi}(x)$ 的条件下，输入向量 $x$ 的坐标 $x_i$ 是条件独立的。因此，向量 $x$ 的似然函数是其各分量独立概率密度的乘积：\n$$p(x \\mid z) = \\prod_{i=1}^{d} p(x_i \\mid z)$$\n每个分量 $x_i$ 服从高斯概率密度分布，其均值为 $\\mu_i(z) = \\hat{x}_i$，方差为 $\\sigma_i^2(x)$：\n$$p(x_i \\mid z) = \\frac{1}{\\sqrt{2\\pi\\sigma_{i}^{2}(x)}}\\exp\\!\\left(-\\frac{(x_{i}-\\hat{x}_{i})^{2}}{2\\sigma_{i}^{2}(x)}\\right)$$\n对数似然 $\\ln p(x \\mid z)$ 是通过对联合概率密度取自然对数得到的：\n$$\\ln p(x \\mid z) = \\ln \\left(\\prod_{i=1}^{d} p(x_i \\mid z)\\right) = \\sum_{i=1}^{d} \\ln p(x_i \\mid z)$$\n代入高斯密度的表达式，我们得到：\n$$\\ln p(x \\mid z) = \\sum_{i=1}^{d} \\ln\\left(\\frac{1}{\\sqrt{2\\pi\\sigma_{i}^{2}(x)}}\\exp\\!\\left(-\\frac{(x_{i}-\\hat{x}_{i})^{2}}{2\\sigma_{i}^{2}(x)}\\right)\\right)$$\n利用对数的性质 $\\ln(ab) = \\ln(a) + \\ln(b)$ 和 $\\ln(e^c) = c$：\n$$\\ln p(x \\mid z) = \\sum_{i=1}^{d} \\left( \\ln\\left((2\\pi\\sigma_{i}^{2}(x))^{-1/2}\\right) - \\frac{(x_{i}-\\hat{x}_{i})^{2}}{2\\sigma_{i}^{2}(x)} \\right)$$\n使用 $\\ln(a^b) = b\\ln(a)$ 进一步简化第一项：\n$$\\ln p(x \\mid z) = \\sum_{i=1}^{d} \\left( -\\frac{1}{2}\\ln(2\\pi\\sigma_{i}^{2}(x)) - \\frac{(x_{i}-\\hat{x}_{i})^{2}}{2\\sigma_{i}^{2}(x)} \\right)$$\n$$\\ln p(x \\mid z) = \\sum_{i=1}^{d} \\left( -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\sigma_{i}^{2}(x)) - \\frac{(x_{i}-\\hat{x}_{i})^{2}}{2\\sigma_{i}^{2}(x)} \\right)$$\n作为损失函数的单样本负对数似然 (NLL) 是 $-\\ln p(x \\mid z)$：\n$$-\\ln p(x \\mid z) = -\\sum_{i=1}^{d} \\left( -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\sigma_{i}^{2}(x)) - \\frac{(x_{i}-\\hat{x}_{i})^{2}}{2\\sigma_{i}^{2}(x)} \\right)$$\n$$-\\ln p(x \\mid z) = \\sum_{i=1}^{d} \\left( \\frac{(x_{i}-\\hat{x}_{i})^{2}}{2\\sigma_{i}^{2}(x)} + \\frac{1}{2}\\ln(\\sigma_{i}^{2}(x)) + \\frac{1}{2}\\ln(2\\pi) \\right)$$\n这个表达式就是以残差 $x_{i}-\\hat{x}_{i}$ 和方差 $\\sigma_{i}^{2}(x)$ 为函数的 NLL。为了将其重写为加权均方误差加惩罚项的形式，我们可以对各项进行分组：\n$$-\\ln p(x \\mid z) = \\sum_{i=1}^{d} \\frac{1}{2\\sigma_{i}^{2}(x)}(x_{i}-\\hat{x}_{i})^{2} + \\left( \\sum_{i=1}^{d} \\frac{1}{2}\\ln(\\sigma_{i}^{2}(x)) + \\frac{d}{2}\\ln(2\\pi) \\right)$$\n将其与指定形式 $\\sum_{i} w_{i}(x)\\,(x_{i}-\\hat{x}_{i})^{2} + \\text{penalty}$ 进行比较，我们可以确定权重 $w_i(x)$ 为：\n$$w_{i}(x) = \\frac{1}{2\\sigma_{i}^{2}(x)}$$\n与方差相关的惩罚项是表达式的其余部分，其中包括一个正则化方差以防止其塌缩到零的项，以及一个常数项：\n$$\\text{Penalty} = \\sum_{i=1}^{d} \\frac{1}{2}\\ln(\\sigma_{i}^{2}(x)) + \\frac{d}{2}\\ln(2\\pi)$$\n\n### 任务 B：数值评估\n\n我们给定一个三维输入，$d=3$。具体数值如下：\n- $x = (1.0, -2.0, 0.5)$\n- $\\hat{x} = (0.8, -1.5, 0.4)$\n- $\\alpha = (0.2, 0.5, 0.1)$\n- $\\beta = (0.0, 0.1, 0.2)$\n\n首先，我们计算坐标级别的方差 $\\sigma_{i}^{2}(x) = \\alpha_{i} + \\beta_{i} x_{i}^{2}$：\n- 对于 $i=1$: $\\sigma_{1}^{2}(x) = 0.2 + 0.0 \\times (1.0)^{2} = 0.2$\n- 对于 $i=2$: $\\sigma_{2}^{2}(x) = 0.5 + 0.1 \\times (-2.0)^{2} = 0.5 + 0.1 \\times 4.0 = 0.9$\n- 对于 $i=3$: $\\sigma_{3}^{2}(x) = 0.1 + 0.2 \\times (0.5)^{2} = 0.1 + 0.2 \\times 0.25 = 0.15$\n\n接下来，我们计算平方残差 $(x_i - \\hat{x}_i)^2$：\n- 对于 $i=1$: $(1.0 - 0.8)^2 = (0.2)^2 = 0.04$\n- 对于 $i=2$: $(-2.0 - (-1.5))^2 = (-0.5)^2 = 0.25$\n- 对于 $i=3$: $(0.5 - 0.4)^2 = (0.1)^2 = 0.01$\n\n现在，我们使用 A 部分推导出的公式来评估完整的单样本 NLL：\n$$-\\ln p(x \\mid z) = \\sum_{i=1}^{3} \\left( \\frac{(x_{i}-\\hat{x}_{i})^{2}}{2\\sigma_{i}^{2}(x)} + \\frac{1}{2}\\ln(\\sigma_{i}^{2}(x)) \\right) + \\frac{3}{2}\\ln(2\\pi)$$\n我们可以计算和中的每一项：\n- 对于 $i=1$: $\\frac{0.04}{2 \\times 0.2} + \\frac{1}{2}\\ln(0.2) = \\frac{0.04}{0.4} + \\frac{1}{2}\\ln(0.2) = 0.1 - 0.804719... = -0.704719...$\n- 对于 $i=2$: $\\frac{0.25}{2 \\times 0.9} + \\frac{1}{2}\\ln(0.9) = \\frac{0.25}{1.8} + \\frac{1}{2}\\ln(0.9) = 0.138888... - 0.052680... = 0.086208...$\n- 对于 $i=3$: $\\frac{0.01}{2 \\times 0.15} + \\frac{1}{2}\\ln(0.15) = \\frac{0.01}{0.3} + \\frac{1}{2}\\ln(0.15) = 0.033333... - 0.948560... = -0.915227...$\n\n这三项的和是：\n$$\\sum_{i=1}^{3} (\\dots) = -0.704719... + 0.086208... - 0.915227... = -1.533738...$$\n常数项是：\n$$\\frac{3}{2}\\ln(2\\pi) \\approx 1.5 \\times 1.837877... = 2.756816...$$\n最后，我们将和与常数项相加，得到总的 NLL：\n$$-\\ln p(x \\mid z) = -1.533738... + 2.756816... = 1.223078...$$\n将结果四舍五入到四位有效数字，得到 $1.223$。\n\n或者，使用分组的表达式：\n- 加权平方误差之和： $0.1 + 0.138888... + 0.033333... = 0.272222...$\n- 对数方差惩罚项之和： $\\frac{1}{2}(\\ln(0.2) + \\ln(0.9) + \\ln(0.15)) = \\frac{1}{2}\\ln(0.027) \\approx -1.805959...$\n- 总 NLL： $0.272222... - 1.805959... + 2.756816... = 1.223079...$\n四舍五入到四位有效数字，我们得到 $1.223$。",
            "answer": "$$\\boxed{1.223}$$"
        }
    ]
}