{
    "hands_on_practices": [
        {
            "introduction": "A primary challenge in training Generative Adversarial Networks is maintaining a delicate balance between the generator and the discriminator. If the discriminator becomes too powerful and its gradients too erratic, the generator receives no useful learning signal. This exercise  provides a hands-on comparison of two critical regularization techniques, R1 and WGAN-GP, designed to stabilize training by constraining the discriminator's gradients. By implementing them in a simplified one-dimensional setting, you will gain a concrete understanding of how they differently enforce the crucial 1-Lipschitz condition.",
            "id": "3098187",
            "problem": "Consider the style-based Generative Adversarial Network version $2$ (StyleGAN2) and its discriminator regularization, contrasted with the Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP). The goal is to isolate and compare their impact on input-space Lipschitzness and stability on a controlled toy dataset. Work in one dimension, define a differentiable scalar discriminator function $D(x;\\theta)$ with parameters $\\theta = (\\alpha,\\beta,\\gamma)$ as\n$$\nD(x;\\theta) = \\alpha x + \\beta \\sin(\\gamma x),\n$$\nand its input gradient\n$$\ng(x;\\theta) = \\frac{\\partial D(x;\\theta)}{\\partial x} = \\alpha + \\beta \\gamma \\cos(\\gamma x).\n$$\n\nUse the following foundational definitions and facts as your starting point:\n- The Lipschitz constant of a differentiable function $f$ over a domain $\\mathcal{X}$ satisfies $L(f) \\le \\sup_{x \\in \\mathcal{X}} \\|\\nabla f(x)\\|_2$. In one dimension with scalar output, this reduces to $L(f) \\le \\sup_{x \\in \\mathcal{X}} |f'(x)|$.\n- The WGAN-GP regularization aims to enforce approximately $1$-Lipschitzness by penalizing deviations of the gradient norm from $1$. The penalty is\n$$\nP_{\\mathrm{GP}}(\\theta) = \\lambda \\,\\mathbb{E}_{\\hat{x}} \\left(\\left|g(\\hat{x};\\theta)\\right| - 1\\right)^2,\n$$\nwhere $\\hat{x} = t\\,x_r + (1-t)\\,x_f$, $t \\sim \\mathrm{Uniform}[0,1]$, $x_r \\sim p_r$ is a real sample, and $x_f \\sim p_f$ is a fake sample.\n- The R1 regularization used in StyleGAN2 penalizes the squared gradient norm on real data:\n$$\nP_{\\mathrm{R1}}(\\theta) = \\lambda \\,\\mathbb{E}_{x_r \\sim p_r} \\left(g(x_r;\\theta)^2\\right).\n$$\n\nYou are given controlled univariate Gaussian data distributions for real and fake:\n- Real data: $x_r \\sim \\mathcal{N}(\\mu_r,\\sigma_r^2)$.\n- Fake data: $x_f \\sim \\mathcal{N}(\\mu_f,\\sigma_f^2)$.\n\nConsider a single gradient descent step on the regularizer alone (i.e., ignore the adversarial loss) with learning rate $\\eta$,\n$$\n\\theta_{\\mathrm{new}} = \\theta_{\\mathrm{old}} - \\eta \\,\\nabla_{\\theta} P(\\theta_{\\mathrm{old}}),\n$$\napplied separately for $P_{\\mathrm{R1}}$ and for $P_{\\mathrm{GP}}$ (two separate updates starting from the same $\\theta_{\\mathrm{old}}$). After each update, estimate:\n- An empirical Lipschitz constant\n$$\n\\widehat{L}(\\theta) = \\max_{x \\in \\mathcal{S}} |g(x;\\theta)|,\n$$\nover a finite evaluation set $\\mathcal{S}$ defined as the union of:\n    - A uniform grid of $M$ points on the interval $[a,b]$, where $a = \\min(\\mu_r - 4\\sigma_r,\\, \\mu_f - 4\\sigma_f)$ and $b = \\max(\\mu_r + 4\\sigma_r,\\, \\mu_f + 4\\sigma_f)$;\n    - A batch of $N$ real samples $x_r$;\n    - A batch of $N$ fake samples $x_f$;\n    - A batch of $N$ interpolated samples $\\hat{x}$ defined as above.\n- A stability index on real data, defined as the sample variance of $g(x_r;\\theta)$:\n$$\nS(\\theta) = \\mathrm{Var}\\left[g(x_r;\\theta)\\right],\n$$\nestimated over the same batch of $N$ real samples.\n\nImplement a program that:\n1. Constructs the toy dataset batches using the specified distributions and a fixed random seed per test case.\n2. Computes the parameter gradients $\\nabla_{\\theta} P_{\\mathrm{R1}}$ and $\\nabla_{\\theta} P_{\\mathrm{GP}}$ from first principles using the chain rule and the definitions above.\n3. Applies one gradient descent update step for each regularizer separately, yielding $\\theta_{\\mathrm{R1}}$ and $\\theta_{\\mathrm{GP}}$.\n4. Evaluates $\\widehat{L}(\\theta_{\\mathrm{R1}})$, $\\widehat{L}(\\theta_{\\mathrm{GP}})$, $S(\\theta_{\\mathrm{R1}})$, and $S(\\theta_{\\mathrm{GP}})$.\n\nYour program must use Monte Carlo estimates with sample size $N$ for expectations, and a uniform grid with $M$ points for the Lipschitz estimate. All computations are real-valued floats. There are no physical units or angles involved. All random draws must be deterministic by setting the specified seed before generating any samples.\n\nTest Suite:\nProvide results for the following four test cases, each given as a tuple of parameters $(\\alpha_0, \\beta_0, \\gamma_0, \\mu_r, \\sigma_r, \\mu_f, \\sigma_f, \\lambda, \\eta, N, M, \\mathrm{seed})$:\n- Case 1 (general nonlinearity): $(\\alpha_0=0.8, \\beta_0=0.5, \\gamma_0=1.5, \\mu_r=0.0, \\sigma_r=1.0, \\mu_f=1.0, \\sigma_f=1.0, \\lambda=10.0, \\eta=0.01, N=4000, M=400, \\mathrm{seed}=42)$.\n- Case 2 (linear discriminator boundary): $(\\alpha_0=2.0, \\beta_0=0.0, \\gamma_0=1.0, \\mu_r=-0.5, \\sigma_r=0.5, \\mu_f=0.5, \\sigma_f=0.5, \\lambda=5.0, \\eta=0.02, N=4000, M=400, \\mathrm{seed}=0)$.\n- Case 3 (low-frequency nonlinearity): $(\\alpha_0=0.5, \\beta_0=1.0, \\gamma_0=0.1, \\mu_r=0.0, \\sigma_r=1.5, \\mu_f=0.0, \\sigma_f=1.5, \\lambda=2.0, \\eta=0.05, N=4000, M=400, \\mathrm{seed}=123)$.\n- Case 4 (high-frequency nonlinearity): $(\\alpha_0=0.3, \\beta_0=1.0, \\gamma_0=5.0, \\mu_r=0.0, \\sigma_r=1.0, \\mu_f=-1.0, \\sigma_f=1.0, \\lambda=10.0, \\eta=0.01, N=4000, M=400, \\mathrm{seed}=999)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing a list of results for the four test cases. For each case, output a list with four floats in the order $[\\widehat{L}(\\theta_{\\mathrm{R1}}),\\widehat{L}(\\theta_{\\mathrm{GP}}),S(\\theta_{\\mathrm{R1}}),S(\\theta_{\\mathrm{GP}})]$. Aggregate these four per-case lists into a single outer list. The final printed string must look like a Python list of lists, for example $[[\\cdots],[\\cdots],[\\cdots],[\\cdots]]$.",
            "solution": "The problem statement has been meticulously reviewed and is determined to be valid. It is scientifically grounded in the principles of deep learning and optimization, specifically concerning the regularization techniques in Generative Adversarial Networks (GANs). The problem is well-posed, providing a complete and unambiguous set of definitions, parameters, and procedures. It is an objective, formalizable task that requires the derivation and implementation of a computational experiment.\n\nThe core of the problem is to compute the effect of a single gradient descent step on the parameters $\\theta = (\\alpha, \\beta, \\gamma)$ of a toy discriminator, $D(x;\\theta)$, for two different regularizers: R1 and WGAN-GP. We must first derive the gradients of these regularization penalties with respect to $\\theta$.\n\nThe discriminator's gradient with respect to its input $x$ is given as:\n$$\ng(x;\\theta) = \\frac{\\partial D(x;\\theta)}{\\partial x} = \\alpha + \\beta \\gamma \\cos(\\gamma x)\n$$\n\nTo find the gradients of the regularizers, $\\nabla_{\\theta} P(\\theta)$, we first require the partial derivatives of $g(x;\\theta)$ with respect to each parameter in $\\theta$:\n$$\n\\frac{\\partial g}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left( \\alpha + \\beta \\gamma \\cos(\\gamma x) \\right) = 1\n$$\n$$\n\\frac{\\partial g}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} \\left( \\alpha + \\beta \\gamma \\cos(\\gamma x) \\right) = \\gamma \\cos(\\gamma x)\n$$\nUsing the product rule for the derivative with respect to $\\gamma$:\n$$\n\\frac{\\partial g}{\\partial \\gamma} = \\frac{\\partial}{\\partial \\gamma} \\left( \\alpha + \\beta \\gamma \\cos(\\gamma x) \\right) = \\beta \\frac{\\partial}{\\partial \\gamma} \\left( \\gamma \\cos(\\gamma x) \\right) = \\beta \\left( 1 \\cdot \\cos(\\gamma x) + \\gamma \\cdot (-\\sin(\\gamma x) \\cdot x) \\right) = \\beta (\\cos(\\gamma x) - \\gamma x \\sin(\\gamma x))\n$$\nWe collect these into a vector $\\nabla_{\\theta} g(x;\\theta) = \\left[ \\frac{\\partial g}{\\partial \\alpha}, \\frac{\\partial g}{\\partial \\beta}, \\frac{\\partial g}{\\partial \\gamma} \\right]^T$.\n\n### Gradient of R1 Regularization\nThe R1 regularizer is defined as $P_{\\mathrm{R1}}(\\theta) = \\lambda \\,\\mathbb{E}_{x_r \\sim p_r} \\left( g(x_r;\\theta)^2 \\right)$.\nWe can interchange the gradient and expectation operators. Applying the chain rule, the gradient with respect to $\\theta$ is:\n$$\n\\nabla_{\\theta} P_{\\mathrm{R1}}(\\theta) = \\lambda \\,\\mathbb{E}_{x_r} \\left[ \\nabla_{\\theta} \\left( g(x_r;\\theta)^2 \\right) \\right] = \\lambda \\,\\mathbb{E}_{x_r} \\left[ 2 g(x_r;\\theta) \\nabla_{\\theta} g(x_r;\\theta) \\right]\n$$\nThe expectation $\\mathbb{E}_{x_r}[\\cdot]$ is approximated using a Monte Carlo estimate over a batch of $N$ real samples $\\{x_{r,i}\\}_{i=1}^N$:\n$$\n\\nabla_{\\theta} P_{\\mathrm{R1}}(\\theta) \\approx \\frac{2\\lambda}{N} \\sum_{i=1}^{N} g(x_{r,i};\\theta) \\nabla_{\\theta} g(x_{r,i};\\theta)\n$$\n\n### Gradient of WGAN-GP Regularization\nThe WGAN-GP regularizer is defined as $P_{\\mathrm{GP}}(\\theta) = \\lambda \\,\\mathbb{E}_{\\hat{x}} \\left(\\left|g(\\hat{x};\\theta)\\right| - 1\\right)^2$, where samples $\\hat{x}$ are drawn from interpolations between real and fake data.\nIts gradient, again using the chain rule, is:\n$$\n\\nabla_{\\theta} P_{\\mathrm{GP}}(\\theta) = \\lambda \\,\\mathbb{E}_{\\hat{x}} \\left[ \\nabla_{\\theta} \\left( |g(\\hat{x};\\theta)| - 1 \\right)^2 \\right] = \\lambda \\,\\mathbb{E}_{\\hat{x}} \\left[ 2 \\left( |g(\\hat{x};\\theta)| - 1 \\right) \\nabla_{\\theta} |g(\\hat{x};\\theta)| \\right]\n$$\nUsing the identity $\\nabla |u| = \\mathrm{sgn}(u) \\nabla u$, for $u \\neq 0$:\n$$\n\\nabla_{\\theta} P_{\\mathrm{GP}}(\\theta) = 2\\lambda \\,\\mathbb{E}_{\\hat{x}} \\left[ \\left( |g(\\hat{x};\\theta)| - 1 \\right) \\mathrm{sgn}(g(\\hat{x};\\theta)) \\nabla_{\\theta} g(\\hat{x};\\theta) \\right]\n$$\nThis expectation $\\mathbb{E}_{\\hat{x}}[\\cdot]$ is approximated by a Monte Carlo estimate over a batch of $N$ interpolated samples $\\{\\hat{x}_i\\}_{i=1}^N$:\n$$\n\\nabla_{\\theta} P_{\\mathrm{GP}}(\\theta) \\approx \\frac{2\\lambda}{N} \\sum_{i=1}^{N} \\left( \\left|g(\\hat{x}_i;\\theta)\\right| - 1 \\right) \\mathrm{sgn}(g(\\hat{x}_i;\\theta)) \\nabla_{\\theta} g(\\hat{x}_i;\\theta)\n$$\n\n### Algorithmic Procedure\nThe overall procedure for each test case is as follows:\n1.  Initialize parameters $\\theta_{\\mathrm{old}} = (\\alpha_0, \\beta_0, \\gamma_0)$ and set the random seed for reproducibility.\n2.  Generate data batches: $N$ real samples $x_r \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)$, $N$ fake samples $x_f \\sim \\mathcal{N}(\\mu_f, \\sigma_f^2)$, and $N$ interpolation coefficients $t \\sim \\mathrm{Uniform}[0,1]$. Construct $N$ interpolated samples $\\hat{x}$.\n3.  Compute the gradient $\\nabla_{\\theta} P_{\\mathrm{R1}}(\\theta_{\\mathrm{old}})$ via Monte Carlo estimation on the real samples $x_r$.\n4.  Perform the R1 gradient descent update: $\\theta_{\\mathrm{R1}} = \\theta_{\\mathrm{old}} - \\eta \\nabla_{\\theta} P_{\\mathrm{R1}}(\\theta_{\\mathrm{old}})$.\n5.  Compute the gradient $\\nabla_{\\theta} P_{\\mathrm{GP}}(\\theta_{\\mathrm{old}})$ via Monte Carlo estimation on the interpolated samples $\\hat{x}$.\n6.  Perform the GP gradient descent update: $\\theta_{\\mathrm{GP}} = \\theta_{\\mathrm{old}} - \\eta \\nabla_{\\theta} P_{\\mathrm{GP}}(\\theta_{\\mathrm{old}})$.\n7.  Construct the evaluation set $\\mathcal{S}$ as the union of a uniform grid over the data range, and the generated sample batches ($x_r, x_f, \\hat{x}$).\n8.  Estimate the Lipschitz constant for each updated parameter set by finding the maximum of $|g(x;\\theta)|$ over the evaluation set $\\mathcal{S}$:\n    $$\n    \\widehat{L}(\\theta_{\\mathrm{R1}}) = \\max_{x \\in \\mathcal{S}} |g(x;\\theta_{\\mathrm{R1}})| \\quad \\text{and} \\quad \\widehat{L}(\\theta_{\\mathrm{GP}}) = \\max_{x \\in \\mathcal{S}} |g(x;\\theta_{\\mathrm{GP}})|\n    $$\n9.  Estimate the stability index for each updated parameter set by computing the sample variance of $g(x_r;\\theta)$ over the real data batch:\n    $$\n    S(\\theta_{\\mathrm{R1}}) = \\mathrm{Var}[g(x_r;\\theta_{\\mathrm{R1}})] \\quad \\text{and} \\quad S(\\theta_{\\mathrm{GP}}) = \\mathrm{Var}[g(x_r;\\theta_{\\mathrm{GP}})]\n    $$\n10. Collate the four resulting metrics $[\\widehat{L}(\\theta_{\\mathrm{R1}}), \\widehat{L}(\\theta_{\\mathrm{GP}}), S(\\theta_{\\mathrm{R1}}), S(\\theta_{\\mathrm{GP}})]$.\n\nThis procedure is implemented for each provided test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases specified.\n    \"\"\"\n    test_cases = [\n        # (alpha0, beta0, gamma0, mu_r, sigma_r, mu_f, sigma_f, lambda, eta, N, M, seed)\n        (0.8, 0.5, 1.5, 0.0, 1.0, 1.0, 1.0, 10.0, 0.01, 4000, 400, 42),\n        (2.0, 0.0, 1.0, -0.5, 0.5, 0.5, 0.5, 5.0, 0.02, 4000, 400, 0),\n        (0.5, 1.0, 0.1, 0.0, 1.5, 0.0, 1.5, 2.0, 0.05, 4000, 400, 123),\n        (0.3, 1.0, 5.0, 0.0, 1.0, -1.0, 1.0, 10.0, 0.01, 4000, 400, 999),\n    ]\n\n    # Helper functions for discriminator gradient g(x; theta) and its partials\n    def g(x, theta):\n        alpha, beta, gamma = theta\n        return alpha + beta * gamma * np.cos(gamma * x)\n\n    def dg_dalpha(x, theta):\n        return np.ones_like(x)\n\n    def dg_dbeta(x, theta):\n        alpha, beta, gamma = theta\n        return gamma * np.cos(gamma * x)\n\n    def dg_dgamma(x, theta):\n        alpha, beta, gamma = theta\n        return beta * (np.cos(gamma * x) - gamma * x * np.sin(gamma * x))\n\n    final_results = []\n    for case in test_cases:\n        alpha0, beta0, gamma0, mu_r, sigma_r, mu_f, sigma_f, lam, eta, N, M, seed = case\n        theta_old = np.array([alpha0, beta0, gamma0])\n        \n        # 1. Set seed and generate data\n        rng = np.random.default_rng(seed)\n        x_r = rng.normal(mu_r, sigma_r, N)\n        x_f = rng.normal(mu_f, sigma_f, N)\n        t = rng.uniform(0, 1, N)\n        x_hat = t * x_r + (1 - t) * x_f\n        \n        # 2. Compute R1 update\n        g_xr = g(x_r, theta_old)\n        grad_r1_alpha = (2 * lam * np.mean(g_xr * dg_dalpha(x_r, theta_old)))\n        grad_r1_beta = (2 * lam * np.mean(g_xr * dg_dbeta(x_r, theta_old)))\n        grad_r1_gamma = (2 * lam * np.mean(g_xr * dg_dgamma(x_r, theta_old)))\n        grad_r1 = np.array([grad_r1_alpha, grad_r1_beta, grad_r1_gamma])\n        theta_r1 = theta_old - eta * grad_r1\n        \n        # 3. Compute GP update\n        g_xhat = g(x_hat, theta_old)\n        gp_term = (np.abs(g_xhat) - 1) * np.sign(g_xhat)\n        grad_gp_alpha = (2 * lam * np.mean(gp_term * dg_dalpha(x_hat, theta_old)))\n        grad_gp_beta = (2 * lam * np.mean(gp_term * dg_dbeta(x_hat, theta_old)))\n        grad_gp_gamma = (2 * lam * np.mean(gp_term * dg_dgamma(x_hat, theta_old)))\n        grad_gp = np.array([grad_gp_alpha, grad_gp_beta, grad_gp_gamma])\n        theta_gp = theta_old - eta * grad_gp\n        \n        # 4. Evaluate metrics\n        # Construct evaluation set S\n        a = min(mu_r - 4 * sigma_r, mu_f - 4 * sigma_f)\n        b = max(mu_r + 4 * sigma_r, mu_f + 4 * sigma_f)\n        grid_points = np.linspace(a, b, M)\n        eval_set = np.concatenate([grid_points, x_r, x_f, x_hat])\n        \n        # Empirical Lipschitz constant\n        L_hat_r1 = np.max(np.abs(g(eval_set, theta_r1)))\n        L_hat_gp = np.max(np.abs(g(eval_set, theta_gp)))\n        \n        # Stability index\n        S_r1 = np.var(g(x_r, theta_r1))\n        S_gp = np.var(g(x_r, theta_gp))\n        \n        case_result = [L_hat_r1, L_hat_gp, S_r1, S_gp]\n        final_results.append([f\"{v:.8f}\" for v in case_result])\n\n    # Final print statement in the exact required format\n    # The format requires string representation of list of lists.\n    # e.g., [[1.0, 2.0], [3.0, 4.0]] - '[[1.0, 2.0],[3.0, 4.0]]'\n    # The map(str,...) and join is a robust way to achieve this.\n    print(f\"[{','.join(map(str, final_results))}]\".replace(\"'\", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "The remarkable realism of models like StyleGAN stems from architectural innovations that control the synthesis process across different scales. One of the most elegant of these is the per-resolution injection of random noise, which adds fine, stochastic details like hair texture or skin pores. This practice  simulates this mechanism in a 1D signal-processing context, allowing you to directly ablate noise at different resolutions and quantitatively measure the trade-off between preserving underlying structure and synthesizing realistic high-frequency \"texture\".",
            "id": "3098258",
            "problem": "You will implement and analyze a simplified per-resolution noise injection mechanism inspired by Style-Based Generative Adversarial Network (StyleGAN). The goal is to ablate noise injection at different resolutions and quantitatively measure its effect on two quantities: texture realism and structural stability. You will implement a purely mathematical, programmatic experiment using a one-dimensional signal, using the principles of linear time-invariant systems and frequency-domain analysis.\n\nStart from the following fundamental base:\n- Additive white Gaussian noise is a random process with independent samples drawn from a Gaussian distribution with zero mean and variance $\\sigma^2$. When linearly filtered by a finite-impulse response kernel, the output remains a Gaussian process whose power spectral density is the input spectral density multiplied by the squared magnitude of the filter's frequency response.\n- The discrete Fourier transform relates time-domain convolution to multiplication in the frequency domain. Let $\\mathcal{F}\\{x\\}[k]$ denote the discrete Fourier transform of $x[n]$; then $\\mathcal{F}\\{x \\ast h\\}[k] = \\mathcal{F}\\{x\\}[k] \\cdot \\mathcal{F}\\{h\\}[k]$, where $\\ast$ denotes circular convolution.\n- The Pearson correlation coefficient between two finite-length signals $x[n]$ and $y[n]$ is defined as $\\rho(x,y) = \\frac{\\sum_{n=0}^{N-1} (x[n]-\\bar{x})(y[n]-\\bar{y})}{\\sqrt{\\sum_{n=0}^{N-1} (x[n]-\\bar{x})^2}\\sqrt{\\sum_{n=0}^{N-1} (y[n]-\\bar{y})^2}}$, where $\\bar{x}$ and $\\bar{y}$ are sample means.\n- The Fast Fourier Transform (FFT) efficiently computes the discrete Fourier transform. High-frequency energy in a discrete signal is captured by the squared magnitude of FFT coefficients at large frequency indices.\n\nConstruct a toy generator as follows.\n\n1. Domain and base structure:\n   - Use signal length $N = 512$.\n   - Define a structural signal $s[n]$ that encodes low-frequency \"shape\" as a sum of sinusoids in radians:\n     $$ s[n] = 0.6 \\sin\\left(\\frac{2\\pi \\cdot 2 \\cdot n}{N}\\right) + 0.4 \\sin\\left(\\frac{2\\pi \\cdot 5 \\cdot n}{N}\\right), \\quad n = 0,1,\\dots,N-1. $$\n     Angles are in radians.\n\n2. Multi-resolution noise injection:\n   - Consider $L = 4$ noise resolutions with strides $s_\\ell \\in \\{64, 32, 16, 8\\}$ for levels $\\ell = 0,1,2,3$ (from coarsest to finest). At each level $\\ell$, draw a low-resolution white Gaussian noise vector $u_\\ell[m]$ of length $N / s_\\ell$, with $u_\\ell[m] \\sim \\mathcal{N}(0, 1)$.\n   - Upsample $u_\\ell[m]$ to length $N$ by nearest-neighbor repetition by a factor of $s_\\ell$, producing $v_\\ell[n]$.\n   - Low-pass filter $v_\\ell[n]$ by a length-$s_\\ell$ box filter (discrete averaging filter) with circular boundary conditions to avoid introducing spurious high frequencies. Let $h_\\ell[n]$ be the kernel defined by $h_\\ell[n] = 1/s_\\ell$ for $n \\in \\{0,1,\\dots,s_\\ell-1\\}$ and $h_\\ell[n]=0$ otherwise. The filtered noise is $w_\\ell = v_\\ell \\ast h_\\ell$ (circular convolution).\n   - Inject scaled noise at each level with amplitudes $a_\\ell \\ge 0$, yielding the generator output\n     $$ x[n] = s[n] + \\sum_{\\ell=0}^{3} a_\\ell \\, w_\\ell[n]. $$\n   - Use a fixed random seed of $0$ for reproducibility of all Gaussian draws.\n\n3. Metrics:\n   - Structural stability score $\\mathrm{S}(x)$ is the Pearson correlation coefficient $\\rho(x, s)$.\n   - Texture realism score $\\mathrm{R}(x)$ is defined as follows. Compute the one-sided real FFT of $x[n]$, excluding the direct current component. Let $X[k]$ denote the one-sided FFT for indices $k = 0, 1, \\dots, N/2$. Define power $P[k] = |X[k]|^2$. Let $k_c = 32$. Define the high-frequency power fraction\n     $$ p_{\\mathrm{high}}(x) = \\frac{\\sum_{k=k_c}^{N/2} P[k]}{\\sum_{k=1}^{N/2} P[k]}. $$\n     Let $p^\\star = 0.25$ be the target high-frequency fraction typical of moderately textured signals in this toy setup. The realism score is\n     $$ \\mathrm{R}(x) = \\max\\left(0, \\, 1 - \\frac{|p_{\\mathrm{high}}(x) - p^\\star|}{p^\\star} \\right). $$\n\n4. Task:\n   - For each specified noise ablation setting (amplitude vector), generate $x[n]$ and compute $\\mathrm{R}(x)$ and $\\mathrm{S}(x)$.\n   - The output for each case is the pair $[\\mathrm{R}(x), \\mathrm{S}(x)]$, with each value rounded to exactly $4$ decimal places.\n\n5. Test suite:\n   - Case A (no noise): $[a_0, a_1, a_2, a_3] = [0.0, 0.0, 0.0, 0.0]$.\n   - Case B (strong noise at all scales): $[a_0, a_1, a_2, a_3] = [0.6, 0.6, 0.6, 0.6]$.\n   - Case C (low-resolution only): $[a_0, a_1, a_2, a_3] = [0.6, 0.3, 0.0, 0.0]$.\n   - Case D (high-resolution only): $[a_0, a_1, a_2, a_3] = [0.0, 0.0, 0.3, 0.6]$.\n   - Case E (balanced mild): $[a_0, a_1, a_2, a_3] = [0.2, 0.1, 0.1, 0.2]$.\n\n6. Required final output format:\n   - Your program should produce a single line of output containing a list of five two-element lists, in the order A, B, C, D, E. Each inner list contains the two rounded floating-point scores $[\\mathrm{R}(x), \\mathrm{S}(x)]$ for that case. The outer list and all inner lists must be enclosed in square brackets, with elements separated by commas and no spaces. For example: \n     $$ [[r_A,s_A],[r_B,s_B],[r_C,s_C],[r_D,s_D],[r_E,s_E]] $$\n     where each $r_\\cdot$ and $s_\\cdot$ is shown with exactly $4$ decimal places.\n\nYour program must be a complete, runnable script that performs all computations deterministically using the fixed random seed and prints exactly one line in the specified format. No physical units are involved. Angles used in sinusoids are in radians. Percentages, if any, must be expressed as decimals as already defined above rather than using a percent sign.",
            "solution": "The user's request is to perform a computational experiment simulating a simplified per-resolution noise injection mechanism, a concept inspired by StyleGAN. The task requires generating a one-dimensional signal by combining a deterministic low-frequency structural component with stochastic noise components at different resolutions. The effects of ablating these noise components are to be quantified using two custom metrics: a Structural Stability score and a Texture Realism score.\n\nThe problem is determined to be **valid**. It is a well-posed, scientifically grounded computational exercise based on principles of digital signal processing and statistics. All parameters, equations, and procedures are explicitly defined, ensuring a unique and verifiable solution.\n\nThe solution proceeds as follows:\n\n1.  **Initialization**: We define the global parameters as specified. The signal length is $N = 512$. The random number generator is seeded with $0$ for reproducibility. The strides for the four noise levels are $s_\\ell = [64, 32, 16, 8]$. The parameters for the Texture Realism score are the high-frequency cutoff index $k_c = 32$ and the target high-frequency power fraction $p^\\star = 0.25$.\n\n2.  **Structural Signal Generation**: The base structural signal, $s[n]$, which represents the low-frequency content of our generated signal, is synthesized according to the given formula:\n    $$\n    s[n] = 0.6 \\sin\\left(\\frac{4\\pi n}{N}\\right) + 0.4 \\sin\\left(\\frac{10\\pi n}{N}\\right)\n    $$\n    for $n = 0, \\dots, N-1$. This signal is deterministic and serves as the ground truth for measuring structural integrity.\n\n3.  **Multi-Resolution Noise Synthesis**: For each of the $L=4$ resolution levels, a corresponding filtered noise signal $w_\\ell[n]$ is generated. This process is independent of the test cases and can be pre-computed. For each level $\\ell$:\n    a. A low-resolution noise vector $u_\\ell$ of length $N/s_\\ell$ is drawn from a standard normal distribution $\\mathcal{N}(0, 1)$.\n    b. This vector is upsampled to length $N$ via nearest-neighbor repetition with a factor of $s_\\ell$, producing $v_\\ell[n]$.\n    c. To smooth the blocky upsampled noise, $v_\\ell[n]$ is circularly convolved with a box filter $h_\\ell$ of length $s_\\ell$. The kernel $h_\\ell$ is an averaging filter, with $h_\\ell[n] = 1/s_\\ell$ for its first $s_\\ell$ samples and zero otherwise. The circular convolution $w_\\ell = v_\\ell \\ast h_\\ell$ is efficiently computed in the frequency domain using the Fast Fourier Transform (FFT) via the convolution theorem: $\\mathcal{F}\\{x \\ast h\\} = \\mathcal{F}\\{x\\} \\cdot \\mathcal{F}\\{h\\}$.\n    $$\n    w_\\ell[n] = \\mathcal{F}^{-1}\\left\\{ \\mathcal{F}\\{v_\\ell\\}[k] \\cdot \\mathcal{F}\\{h_\\ell\\}[k] \\right\\}\n    $$\n\n4.  **Signal Generation for Test Cases**: For each of the five test cases defined by an amplitude vector $[a_0, a_1, a_2, a_3]$, the final output signal $x[n]$ is synthesized by a weighted sum of the structural signal and the pre-computed noise signals:\n    $$\n    x[n] = s[n] + \\sum_{\\ell=0}^{3} a_\\ell \\, w_\\ell[n]\n    $$\n\n5.  **Metric Computation**: For each generated signal $x[n]$, two metrics are computed.\n    a. **Structural Stability $\\mathrm{S}(x)$**: This is the Pearson correlation coefficient between the generated signal $x[n]$ and the original structural signal $s[n]$. It quantifies how much of the original low-frequency structure is preserved after noise injection. A value of $1$ indicates perfect preservation.\n    $$\n    \\mathrm{S}(x) = \\rho(x, s) = \\frac{\\sum_{n=0}^{N-1} (x[n]-\\bar{x})(s[n]-\\bar{s})}{\\sqrt{\\sum_{n=0}^{N-1} (x[n]-\\bar{x})^2}\\sqrt{\\sum_{n=0}^{N-1} (s[n]-\\bar{s})^2}}\n    $$\n    b. **Texture Realism $\\mathrm{R}(x)$**: This metric quantifies the amount of high-frequency content, or \"texture,\" in the signal relative to a target value. First, the one-sided Real FFT of $x[n]$ is computed to get the frequency-domain representation $X[k]$. The power spectrum is $P[k] = |X[k]|^2$. The high-frequency power fraction $p_{\\mathrm{high}}(x)$ is the ratio of power in frequencies $k \\ge k_c$ to the total power in all non-DC frequencies ($k \\ge 1$).\n    $$\n    p_{\\mathrm{high}}(x) = \\frac{\\sum_{k=k_c}^{N/2} P[k]}{\\sum_{k=1}^{N/2} P[k]}\n    $$\n    The realism score $\\mathrm{R}(x)$ measures how close $p_{\\mathrm{high}}(x)$ is to the target fraction $p^\\star$, scaled to a range between $0$ and $1$.\n    $$\n    \\mathrm{R}(x) = \\max\\left(0, \\, 1 - \\frac{|p_{\\mathrm{high}}(x) - p^\\star|}{p^\\star} \\right)\n    $$\n\n6.  **Output Formatting**: The computed scores $[\\mathrm{R}(x), \\mathrm{S}(x)]$ for each test case are rounded to $4$ decimal places and formatted into a single-line list of lists as required.\n\nThe implementation consolidates these steps into a single script that deterministically computes the results for all test cases and prints them in the specified format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes a simplified per-resolution noise injection mechanism\n    inspired by StyleGAN.\n    \"\"\"\n    # 1. Define constants and initialization\n    N = 512\n    strides = [64, 32, 16, 8]\n    L = len(strides)\n    seed = 0\n    rng = np.random.default_rng(seed)\n    # Metric parameters\n    p_star = 0.25\n    k_c = 32\n\n    # 2. Generate the base structural signal s[n]\n    n = np.arange(N)\n    s = 0.6 * np.sin(2 * np.pi * 2 * n / N) + 0.4 * np.sin(2 * np.pi * 5 * n / N)\n\n    # 3. Pre-compute the four filtered noise signals w_ell[n]\n    w_list = []\n    for s_ell in strides:\n        # a. Generate low-resolution Gaussian noise u_ell\n        noise_len = N // s_ell\n        u_ell = rng.standard_normal(size=noise_len)\n\n        # b. Upsample u_ell to v_ell using nearest-neighbor repetition\n        v_ell = np.repeat(u_ell, s_ell)\n\n        # c. Create box filter kernel h_ell and perform circular convolution\n        # using the FFT-based convolution theorem.\n        h_ell = np.zeros(N)\n        h_ell[:s_ell] = 1.0 / s_ell\n        \n        fft_v = np.fft.fft(v_ell)\n        fft_h = np.fft.fft(h_ell)\n        \n        # The result of IFFT on real-signal convolution should be real.\n        # We take .real to discard negligible imaginary parts from numerical error.\n        w_ell = np.fft.ifft(fft_v * fft_h).real\n        w_list.append(w_ell)\n    \n    # 4. Define the test suite\n    test_cases = {\n        'A': [0.0, 0.0, 0.0, 0.0],\n        'B': [0.6, 0.6, 0.6, 0.6],\n        'C': [0.6, 0.3, 0.0, 0.0],\n        'D': [0.0, 0.0, 0.3, 0.6],\n        'E': [0.2, 0.1, 0.1, 0.2]\n    }\n    \n    all_results = []\n    # Process cases in specified order A, B, C, D, E\n    for case_id in sorted(test_cases.keys()):\n        amplitudes = test_cases[case_id]\n        \n        # 5. Generate the final signal x[n] for the current case\n        noise_component = np.zeros(N)\n        for i in range(L):\n            noise_component += amplitudes[i] * w_list[i]\n        x = s + noise_component\n\n        # 6. Compute Structural Stability S(x)\n        # np.corrcoef calculates the Pearson correlation coefficient matrix.\n        # The off-diagonal element [0, 1] is rho(x, s).\n        s_score = np.corrcoef(x, s)[0, 1]\n\n        # 7. Compute Texture Realism R(x)\n        # Use one-sided real FFT for efficiency and correctness.\n        X_fft = np.fft.rfft(x)  # Produces N/2 + 1 complex coefficients\n        \n        # Power spectrum P[k] = |X[k]|^2\n        P = np.abs(X_fft)**2\n        \n        # Calculate high-frequency power and total non-DC power\n        # P[k_c:] sums power from frequency k_c to N/2\n        # P[1:] sums power from frequency 1 to N/2\n        sum_high = np.sum(P[k_c:])\n        sum_total_no_dc = np.sum(P[1:])\n        \n        # Avoid division by zero, though unlikely in this problem setup\n        if sum_total_no_dc == 0:\n            p_high = 0.0\n        else:\n            p_high = sum_high / sum_total_no_dc\n            \n        r_score = max(0.0, 1.0 - abs(p_high - p_star) / p_star)\n\n        # 8. Store the rounded results for the current case\n        all_results.append([round(r_score, 4), round(s_score, 4)])\n\n    # 9. Format and print the final output string exactly as specified\n    output_str_parts = []\n    for r, s in all_results:\n        output_str_parts.append(f\"[{r:.4f},{s:.4f}]\")\n    \n    final_output = \"[\" + \",\".join(output_str_parts) + \"]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond generating random images, a key goal of advanced GANs is to control their output, for instance, by specifying a desired object class. This practice  delves into this topic of conditional generation by comparing two influential architectural patterns: the Auxiliary Classifier GAN (ACGAN) head and the Projection Discriminator used in BigGAN. By mathematically analyzing their behavior under the practical challenge of symmetric label noise, you will uncover a fundamental difference in their robustness and gain insight into why the projection-based approach has become a cornerstone of modern, large-scale conditional models.",
            "id": "3098265",
            "problem": "You are asked to quantitatively compare the robustness of the Large Scale Generative Adversarial Network (BigGAN) discriminator under symmetric label noise when using two different class-conditioning mechanisms: the projection discriminator versus an Auxiliary Classifier Generative Adversarial Network (ACGAN) classifier head. Your program must compute, for each specified test case, the expected cosine similarity between the noisy-label gradient and the clean-label gradient for both mechanisms, under a simplified but scientifically consistent model, using only fundamental definitions from probability and loss functions. The final program must be a complete, runnable program and must produce the specified outputs without any external inputs.\n\nThe foundational base consists of the following definitions:\n- Generative Adversarial Network (GAN): the discriminator maps an input to a scalar score and is trained by a classification loss that distinguishes real versus generated data.\n- Symmetric label noise: for a $K$-class problem, the observed label $\\tilde{y}$ equals the true label $y$ with probability $1 - \\varepsilon$, and otherwise is replaced by a label sampled uniformly from the remaining $K - 1$ classes with probability $\\varepsilon$. Formally, for $j \\in \\{1, \\dots, K\\}$, letting $t$ denote the index of the true class and $e_j$ denote the one-hot basis vector for class $j$,\n$$\n\\mathbb{P}(\\tilde{y} = y) = 1 - \\varepsilon, \\quad\n\\mathbb{P}(\\tilde{y} = j \\neq y) = \\frac{\\varepsilon}{K - 1}.\n$$\n- Softmax cross-entropy: for logits $z \\in \\mathbb{R}^K$, the predicted class probabilities are $p_j = \\frac{\\exp(z_j)}{\\sum_{k=1}^K \\exp(z_k)}$, and the cross-entropy loss with observed one-hot label $\\hat{y}$ is $\\ell(z, \\hat{y}) = -\\sum_{j=1}^K \\hat{y}_j \\log p_j$. Its gradient with respect to the logits is $\\nabla_z \\ell = p - \\hat{y}$.\n\nYou must compute two cosine similarities:\n1. Projection discriminator cosine similarity. Consider the BigGAN projection discriminator class-conditional score $s(x, y) = f(x) + \\langle v(y), \\phi(x) \\rangle$, where $v(y) \\in \\mathbb{R}^K$ is the class embedding and $\\phi(x) \\in \\mathbb{R}^K$ is the discriminator feature. Under the assumption that $\\{v(1), \\dots, v(K)\\}$ form an orthonormal set of unit vectors and using a standard logistic adversarial loss, the gradient with respect to $\\phi(x)$ contributed by the projection term is proportional to $v(\\tilde{y})$. Treat the clean-label gradient direction as $g_{\\mathrm{proj,clean}} \\propto v(y)$ and the noisy-label gradient direction as $g_{\\mathrm{proj,noisy}} \\propto v(\\tilde{y})$. Define the expected noisy gradient as the expectation over the symmetric label noise model. Your task is to compute the cosine similarity\n$$\n\\cos_{\\mathrm{proj}} = \\frac{\\langle \\mathbb{E}[g_{\\mathrm{proj,noisy}}], g_{\\mathrm{proj,clean}} \\rangle}{\\|\\mathbb{E}[g_{\\mathrm{proj,noisy}}]\\| \\, \\|g_{\\mathrm{proj,clean}}\\|}.\n$$\n\n2. Auxiliary classifier (ACGAN) cosine similarity. The ACGAN classifier head produces logits $z \\in \\mathbb{R}^K$ and predictions $p \\in \\Delta^{K-1}$, and is trained with softmax cross-entropy on the observed noisy labels. Let $g_{\\mathrm{ACGAN,clean}} = p - y$ denote the clean-label gradient with respect to logits and $g_{\\mathrm{ACGAN,noisy}} = p - \\tilde{y}$ the noisy-label gradient. Define the expected noisy gradient as $\\mathbb{E}[g_{\\mathrm{ACGAN,noisy}}] = p - \\mathbb{E}[\\tilde{y}]$ under the symmetric label noise model. Compute the cosine similarity\n$$\n\\cos_{\\mathrm{ACGAN}} = \\frac{\\langle \\mathbb{E}[g_{\\mathrm{ACGAN,noisy}}], g_{\\mathrm{ACGAN,clean}} \\rangle}{\\|\\mathbb{E}[g_{\\mathrm{ACGAN,noisy}}]\\| \\, \\|g_{\\mathrm{ACGAN,clean}}\\|}.\n$$\n\nIn both cases, the cosine similarity is a dimensionless quantity in the range $[-1, 1]$.\n\nImplementation requirements:\n- Use only the definitions above. For the projection discriminator, assume unit-norm orthonormal class embeddings $v(j)$ for $j \\in \\{1, \\dots, K\\}$ and treat gradient scalars from the adversarial loss as cancelling in the cosine similarity. For the ACGAN head, use $g_{\\mathrm{ACGAN,clean}} = p - y$ and $\\mathbb{E}[g_{\\mathrm{ACGAN,noisy}}] = p - \\mathbb{E}[\\tilde{y}]$, with $\\mathbb{E}[\\tilde{y}]$ implied by the symmetric noise model.\n- All computations must be deterministic given the parameters specified in each test case.\n- No physical units are involved.\n- Your program must compute both cosine similarities for each test case, outputting a two-element list $[\\cos_{\\mathrm{proj}}, \\cos_{\\mathrm{ACGAN}}]$ per test case, with each float rounded to six decimal places.\n\nTest suite:\nCompute results for the following parameter sets, where $K$ is the number of classes, $\\varepsilon$ is the noise rate, $t$ is the true class index (zero-based), and $p$ is the predicted probability vector:\n1. $K = 10$, $\\varepsilon = 0.2$, $t = 3$, $p_t = 0.9$, and $p_j = \\frac{0.1}{9}$ for all $j \\neq t$.\n2. $K = 10$, $\\varepsilon = 0.0$, $t = 5$, $p_t = 0.7$, and $p_j = \\frac{0.3}{9}$ for all $j \\neq t$.\n3. $K = 10$, $\\varepsilon = 0.9$, $t = 0$, $p_j = 0.1$ for all $j \\in \\{0, \\dots, 9\\}$.\n4. $K = 2$, $\\varepsilon = 0.5$, $t = 1$, $p_0 = 0.3$, $p_1 = 0.7$.\n5. $K = 100$, $\\varepsilon = 0.3$, $t = 42$, $p_t = 0.6$, and $p_j = \\frac{0.4}{99}$ for all $j \\neq t$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list $[\\cos_{\\mathrm{proj}}, \\cos_{\\mathrm{ACGAN}}]$. For example, the printed line must look like\n$$\n[\\,[c_1^{\\mathrm{proj}}, c_1^{\\mathrm{ACGAN}}],\\,[c_2^{\\mathrm{proj}}, c_2^{\\mathrm{ACGAN}}],\\,\\dots\\,]\n$$\nwith each cosine value rounded to six decimal places and no spaces in the string.",
            "solution": "The problem requires a quantitative comparison of the robustness of two class-conditioning mechanisms in a Generative Adversarial Network (GAN) discriminator: the projection discriminator and an Auxiliary Classifier GAN (ACGAN) head. The metric for comparison is the cosine similarity between the clean-label gradient and the expected noisy-label gradient, under a symmetric label noise model. The problem is valid as it is scientifically grounded in established machine learning principles, well-posed with all necessary information provided, and objective in its formulation.\n\nLet $K$ be the number of classes, $\\varepsilon$ be the symmetric label noise rate, $y$ be the one-hot vector for the true class, and $\\tilde{y}$ be the one-hot vector for the observed (potentially noisy) label. The true class index is denoted by $t$. The noise model is given by:\n$$\n\\mathbb{P}(\\tilde{y} = y) = 1 - \\varepsilon\n$$\n$$\n\\mathbb{P}(\\tilde{y} = e_j) = \\frac{\\varepsilon}{K - 1} \\quad \\text{for } j \\neq t\n$$\nwhere $e_j$ is the one-hot basis vector for class $j$.\n\nWe will derive the analytical expression for the cosine similarity for each mechanism separately.\n\n### 1. Projection Discriminator\n\nThe class-conditional score is $s(x, y) = f(x) + \\langle v(y), \\phi(x) \\rangle$, where $\\{v(1), \\dots, v(K)\\}$ is an orthonormal set of unit vectors representing the class embeddings. The gradient of the class-conditional term with respect to the feature vector $\\phi(x)$ is proportional to the class embedding of the label used.\nThe clean-label gradient direction is $g_{\\mathrm{proj,clean}} \\propto v(y) = v_t$, where $v_t$ is the embedding for the true class $t$.\nThe noisy-label gradient direction is $g_{\\mathrm{proj,noisy}} \\propto v(\\tilde{y})$.\n\nWe are asked to compute $\\cos_{\\mathrm{proj}} = \\frac{\\langle \\mathbb{E}[g_{\\mathrm{proj,noisy}}], g_{\\mathrm{proj,clean}} \\rangle}{\\|\\mathbb{E}[g_{\\mathrm{proj,noisy}}]\\| \\, \\|g_{\\mathrm{proj,clean}}\\|}$.\nLet's set the proportionality constant to $1$ without loss of generality, as it cancels out.\n$g_{\\mathrm{proj,clean}} = v_t$.\n$g_{\\mathrm{proj,noisy}} = v(\\tilde{y})$.\n\nFirst, we compute the expected noisy gradient, $\\mathbb{E}[g_{\\mathrm{proj,noisy}}]$, by taking the expectation over the distribution of $\\tilde{y}$:\n$$\n\\mathbb{E}[g_{\\mathrm{proj,noisy}}] = \\mathbb{E}[v(\\tilde{y})] = \\sum_{j=1}^K \\mathbb{P}(\\text{observed class is } j) \\cdot v_j\n$$\nUsing the symmetric noise model, with true class $t$:\n$$\n\\mathbb{E}[v(\\tilde{y})] = \\mathbb{P}(\\text{observed class is } t) \\cdot v_t + \\sum_{j \\neq t} \\mathbb{P}(\\text{observed class is } j) \\cdot v_j\n$$\n$$\n\\mathbb{E}[v(\\tilde{y})] = (1 - \\varepsilon) v_t + \\sum_{j \\neq t} \\frac{\\varepsilon}{K-1} v_j\n$$\n\nNow we compute the terms for the cosine similarity formula:\nThe inner product in the numerator is:\n$$\n\\langle \\mathbb{E}[g_{\\mathrm{proj,noisy}}], g_{\\mathrm{proj,clean}} \\rangle = \\left\\langle (1 - \\varepsilon) v_t + \\sum_{j \\neq t} \\frac{\\varepsilon}{K-1} v_j, v_t \\right\\rangle\n$$\nUsing the orthonormality property of the embeddings, $\\langle v_j, v_k \\rangle = \\delta_{jk}$ (Kronecker delta):\n$$\n= (1 - \\varepsilon) \\langle v_t, v_t \\rangle + \\sum_{j \\neq t} \\frac{\\varepsilon}{K-1} \\langle v_j, v_t \\rangle = (1 - \\varepsilon) \\cdot 1 + \\sum_{j \\neq t} \\frac{\\varepsilon}{K-1} \\cdot 0 = 1 - \\varepsilon\n$$\n\nThe norms in the denominator are:\n$\\|g_{\\mathrm{proj,clean}}\\| = \\|v_t\\| = 1$, as the embeddings are unit vectors.\nThe norm of the expected noisy gradient is $\\|\\mathbb{E}[g_{\\mathrm{proj,noisy}}]\\| = \\|\\mathbb{E}[v(\\tilde{y})]\\|$. We compute its squared value:\n$$\n\\|\\mathbb{E}[v(\\tilde{y})]\\|^2 = \\left\\langle (1 - \\varepsilon) v_t + \\sum_{j \\neq t} \\frac{\\varepsilon}{K-1} v_j, (1 - \\varepsilon) v_t + \\sum_{k \\neq t} \\frac{\\varepsilon}{K-1} v_k \\right\\rangle\n$$\nExpanding using orthonormality:\n$$\n= (1 - \\varepsilon)^2 \\langle v_t, v_t \\rangle + \\sum_{j \\neq t} \\sum_{k \\neq t} \\left(\\frac{\\varepsilon}{K-1}\\right)^2 \\langle v_j, v_k \\rangle = (1 - \\varepsilon)^2 + \\sum_{j \\neq t} \\left(\\frac{\\varepsilon}{K-1}\\right)^2 \\langle v_j, v_j \\rangle\n$$\nSince there are $K-1$ terms in the sum where $j=k$:\n$$\n= (1 - \\varepsilon)^2 + (K - 1) \\left(\\frac{\\varepsilon}{K-1}\\right)^2 = (1 - \\varepsilon)^2 + \\frac{\\varepsilon^2}{K-1}\n$$\nSo, $\\|\\mathbb{E}[g_{\\mathrm{proj,noisy}}]\\| = \\sqrt{(1 - \\varepsilon)^2 + \\frac{\\varepsilon^2}{K-1}}$.\n\nCombining these parts, the cosine similarity for the projection discriminator is:\n$$\n\\cos_{\\mathrm{proj}} = \\frac{1 - \\varepsilon}{\\sqrt{(1 - \\varepsilon)^2 + \\frac{\\varepsilon^2}{K-1}}}\n$$\nThis expression depends only on the number of classes $K$ and the noise rate $\\varepsilon$.\n\n### 2. Auxiliary Classifier (ACGAN)\n\nThe ACGAN head is trained using softmax cross-entropy. The gradient of the loss with respect to the logits $z$ is $p - \\hat{y}$, where $p$ is the softmax probability vector and $\\hat{y}$ is the one-hot label vector.\nThe clean-label gradient is $g_{\\mathrm{ACGAN,clean}} = p - y = p - e_t$.\nThe noisy-label gradient is $g_{\\mathrm{ACGAN,noisy}} = p - \\tilde{y}$.\nThe expected noisy gradient is $\\mathbb{E}[g_{\\mathrm{ACGAN,noisy}}] = \\mathbb{E}[p - \\tilde{y}] = p - \\mathbb{E}[\\tilde{y}]$.\n\nFirst, we find the vector $\\mathbb{E}[\\tilde{y}]$. Let's denote this vector as $\\bar{y}$. The $j$-th component of this vector is $\\bar{y}_j = \\mathbb{E}[\\tilde{y}_j] = \\mathbb{P}(\\text{observed class is } j)$.\nFrom the symmetric noise model, where $t$ is the true class:\n$$\n\\bar{y}_t = \\mathbb{P}(\\text{observed class is } t) = 1 - \\varepsilon\n$$\n$$\n\\bar{y}_j = \\mathbb{P}(\\text{observed class is } j) = \\frac{\\varepsilon}{K-1} \\quad \\text{for } j \\neq t\n$$\nSo, $\\mathbb{E}[g_{\\mathrm{ACGAN,noisy}}] = p - \\bar{y}$.\n\nThe cosine similarity is thus:\n$$\n\\cos_{\\mathrm{ACGAN}} = \\frac{\\langle \\mathbb{E}[g_{\\mathrm{ACGAN,noisy}}], g_{\\mathrm{ACGAN,clean}} \\rangle}{\\|\\mathbb{E}[g_{\\mathrm{ACGAN,noisy}}]\\| \\, \\|g_{\\mathrm{ACGAN,clean}}\\|} = \\frac{\\langle p - \\bar{y}, p - e_t \\rangle}{\\|p - \\bar{y}\\| \\, \\|p - e_t\\|}\n$$\nThis expression requires vector computation for each test case, using the provided parameters for $K$, $\\varepsilon$, $t$, and the probability vector $p$. The vectors are:\n- $p$: The given probability vector.\n- $e_t$: A $K$-dimensional one-hot vector with a $1$ at index $t$ and $0$s elsewhere.\n- $\\bar{y}$: A $K$-dimensional vector with $1 - \\varepsilon$ at index $t$ and $\\frac{\\varepsilon}{K-1}$ elsewhere.\n\nIn the case where either norm in the denominator is zero, the cosine similarity is undefined. We define it to be $0$ in such cases, as a zero-norm expected gradient signifies a complete loss of directional signal from the label.\n\nThe implementation will directly compute these derived formulas for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the cosine similarities for the specified test cases.\n    \"\"\"\n    \n    # Test suite: (K, epsilon, true_class_idx, p_params)\n    # p_params is either a tuple (p_t, \"distributed\") for the common case,\n    # or a numpy array for special cases.\n    test_cases = [\n        (10, 0.2, 3, (0.9, \"distributed\")),\n        (10, 0.0, 5, (0.7, \"distributed\")),\n        (10, 0.9, 0, np.full(10, 0.1)),\n        (2, 0.5, 1, np.array([0.3, 0.7])),\n        (100, 0.3, 42, (0.6, \"distributed\")),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        K, epsilon, t_idx, p_config = case\n\n        # 1. Projection Discriminator Calculation\n        # Formula: cos_proj = (1 - eps) / sqrt((1 - eps)^2 + eps^2 / (K - 1))\n        # Handle K=1 case to avoid division by zero, although not in test suite.\n        if K == 1:\n            # With K=1, there is no noise possible so eps must be 0.\n            # cos_proj is trivially 1.\n            cos_proj = 1.0 if epsilon == 0 else np.nan\n        else:\n            numerator_proj = 1 - epsilon\n            denominator_proj = np.sqrt((1 - epsilon)**2 + epsilon**2 / (K - 1))\n            if denominator_proj == 0:\n                # This only happens if 1-epsilon=0 and epsilon=0, which is impossible.\n                # However, for robustness, we handle it.\n                cos_proj = 0.0\n            else:\n                cos_proj = numerator_proj / denominator_proj\n\n        # 2. ACGAN Calculation\n        # Formula: cos_acgan = p - E[y_tilde], p - y_true / (||p - E[y_tilde]|| * ||p - y_true||)\n        \n        # Construct probability vector p\n        if isinstance(p_config, np.ndarray):\n            p_vec = p_config\n        else:\n            p_t, _ = p_config\n            if K  1:\n                p_vec = np.full(K, (1 - p_t) / (K - 1))\n            else:\n                p_vec = np.array([1.0])\n            p_vec[t_idx] = p_t\n\n        # Construct true label vector y_true (one-hot)\n        y_true_one_hot = np.zeros(K)\n        y_true_one_hot[t_idx] = 1.0\n\n        # Construct expected noisy label vector E[y_tilde]\n        if K  1:\n            y_tilde_expected = np.full(K, epsilon / (K - 1))\n        else:\n            y_tilde_expected = np.array([1.0]) # No noise possible\n        y_tilde_expected[t_idx] = 1 - epsilon\n        \n        # Calculate clean and expected noisy gradients\n        grad_clean = p_vec - y_true_one_hot\n        grad_noisy_expected = p_vec - y_tilde_expected\n\n        # Calculate norms\n        norm_clean = np.linalg.norm(grad_clean)\n        norm_noisy_expected = np.linalg.norm(grad_noisy_expected)\n\n        # Calculate cosine similarity\n        if norm_clean == 0 or norm_noisy_expected == 0:\n            # If either gradient vector is zero, the directional information is lost.\n            # We define the similarity as 0 in this edge case.\n            cos_acgan = 0.0\n        else:\n            dot_product = np.dot(grad_clean, grad_noisy_expected)\n            cos_acgan = dot_product / (norm_clean * norm_noisy_expected)\n\n        # Append formatted results for the current test case\n        results.append(f\"[{cos_proj:.6f},{cos_acgan:.6f}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}