{
    "hands_on_practices": [
        {
            "introduction": "The heart of a Wasserstein GAN is the replacement of the standard GAN's loss function with the Wasserstein distance, a more powerful metric for comparing probability distributions. This first exercise bridges the gap between the formal mathematics of the $1$-Wasserstein distance and the practical methods used to estimate it from data. By deriving a closed-form solution for simple Gaussian distributions and comparing it to an empirical estimate, you will build a solid intuition for why the WGAN's critic objective is a valid and robust measure of distributional difference .",
            "id": "3137294",
            "problem": "You are asked to connect the mathematical foundations of the Earth Mover’s distance and the practical estimation approach used in Wasserstein Generative Adversarial Networks (WGANs), in a controlled setting where both the real and generated distributions are one-dimensional Gaussians. Start from foundational definitions of the $1$-Wasserstein distance and derive an exact expression for the distance between a standard normal distribution and an arbitrary normal distribution. Then implement a numerical estimator that mirrors the empirical critic behavior in one dimension, and compare the two across several parameter regimes.\n\nBackground and definitions:\n- A Generative Adversarial Network (GAN) frames learning as a two-player game between a generator and a critic (or discriminator). A Wasserstein Generative Adversarial Network (WGAN) replaces the Jensen–Shannon divergence with the $1$-Wasserstein distance to improve training stability.\n- The $1$-Wasserstein distance $W_1$ between two probability distributions $p_r$ and $p_g$ on the real line is defined by the optimal transport cost with the cost function $c(x,y) = |x-y|$. Equivalently, via the Kantorovich–Rubinstein (KR) duality, it can be represented as a supremum over $1$-Lipschitz functions $f$: $$W_1(p_r, p_g) = \\sup_{\\|f\\|_{\\mathrm{Lip}} \\le 1} \\mathbb{E}_{X \\sim p_r}[f(X)] - \\mathbb{E}_{Y \\sim p_g}[f(Y)].$$\n- In one dimension, a fundamental and well-tested fact is that the optimal coupling is monotone, which yields the quantile coupling identity $$W_1(p_r, p_g) = \\int_0^1 \\left| F_r^{-1}(u) - F_g^{-1}(u) \\right| \\, du,$$ where $F^{-1}$ denotes the quantile function (the inverse cumulative distribution function (CDF)).\n\nProblem setup:\n- Let $p_r = \\mathcal{N}(0, 1)$ be the real data distribution and $p_g = \\mathcal{N}(\\mu, \\sigma^2)$ be the generator distribution, with parameters $\\mu \\in \\mathbb{R}$ and $\\sigma \\in \\mathbb{R}_{>0}$.\n- Your tasks are:\n  1. Derive from the above principles a closed-form expression for $W_1(\\mathcal{N}(0,1), \\mathcal{N}(\\mu,\\sigma^2))$, expressed only in terms of $\\mu$ and $\\sigma$, without using any shortcut formulas that assume the answer.\n  2. Implement a numerical estimator for the empirical $1$-Wasserstein distance between $p_r$ and $p_g$ using $n$ independent samples from each distribution, based on the monotone transport in one dimension. Concretely, for equal-weight empirical distributions with sample size $n$, the empirical $1$-Wasserstein distance equals the mean absolute difference between the order statistics of the two sample sets.\n  3. Compare the closed-form value and the empirical estimate for several $(\\mu, \\sigma)$ pairs.\n\nYour program must:\n- Compute the closed-form value derived in Task $1$.\n- Estimate the empirical value in Task $2$ by drawing $n$ samples from each distribution, sorting both sets, and averaging the absolute differences between the sorted samples.\n- Use a fixed pseudorandom seed for reproducibility.\n- For each test case, output a triplet of floats $[w_{\\mathrm{closed}}, w_{\\mathrm{empirical}}, \\Delta]$, where $w_{\\mathrm{closed}}$ is the closed-form value, $w_{\\mathrm{empirical}}$ is the empirical estimate, and $\\Delta$ is the absolute difference $|w_{\\mathrm{closed}} - w_{\\mathrm{empirical}}|$.\n\nTest suite:\n- Use the following parameter sets $(\\mu, \\sigma)$ with $n$ samples per distribution for each case:\n  1. $(\\mu, \\sigma) = (0, 1)$, baseline identical distributions.\n  2. $(\\mu, \\sigma) = (1, 1)$, pure mean shift.\n  3. $(\\mu, \\sigma) = (0, 2)$, pure scale change.\n  4. $(\\mu, \\sigma) = (-2, 1.5)$, simultaneous mean and scale change.\n  5. $(\\mu, \\sigma) = (0.5, 0.5)$, mean positive and scale smaller than $1$.\n  6. $(\\mu, \\sigma) = (3, 0.1)$, large mean and very small variance.\n- Use $n = 100000$ samples for each empirical estimate and a fixed seed.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each item is itself a comma-separated list for one test case. For example: $$[[w_1^{\\mathrm{closed}}, w_1^{\\mathrm{emp}}, \\Delta_1],[w_2^{\\mathrm{closed}}, w_2^{\\mathrm{emp}}, \\Delta_2],\\dots].$$\n- All outputs must be floats. No physical units or angle units apply. Do not use percentage signs; if any fractional values arise, they must be represented as decimals.",
            "solution": "The problem requires a two-part analysis of the $1$-Wasserstein distance between two one-dimensional Gaussian distributions. First, we must derive a closed-form expression for this distance starting from the quantile-based definition. Second, we are to implement a numerical estimator based on sorted empirical samples and compare its results to the theoretical values.\n\n**1. Derivation of the Closed-Form Expression for $W_1(\\mathcal{N}(0,1), \\mathcal{N}(\\mu,\\sigma^2))$**\n\nThe $1$-Wasserstein distance, $W_1$, between two probability distributions $p_r$ and $p_g$ on the real line can be computed using their quantile functions, $F_r^{-1}$ and $F_g^{-1}$ (inverse CDFs). The formula is given as:\n$$W_1(p_r, p_g) = \\int_0^1 \\left| F_r^{-1}(u) - F_g^{-1}(u) \\right| \\, du$$\n\nThe problem specifies the real distribution $p_r$ as the standard normal distribution, $p_r = \\mathcal{N}(0,1)$, and the generated distribution $p_g$ as a general normal distribution, $p_g = \\mathcal{N}(\\mu, \\sigma^2)$, where $\\mu$ is the mean and $\\sigma > 0$ is the standard deviation.\n\nFirst, we determine the quantile functions for these distributions. Let $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0,1)$.\nThe quantile function is the inverse of the CDF.\n- For $p_r = \\mathcal{N}(0,1)$, the CDF is $F_r(x) = \\Phi(x)$. Thus, the quantile function is $F_r^{-1}(u) = \\Phi^{-1}(u)$.\n- For $p_g = \\mathcal{N}(\\mu, \\sigma^2)$, a random variable $Y \\sim p_g$ can be represented as $Y = \\mu + \\sigma Z$, where $Z \\sim \\mathcal{N}(0,1)$. The CDF is $F_g(y) = P(Y \\le y) = P(\\mu + \\sigma Z \\le y) = P\\left(Z \\le \\frac{y-\\mu}{\\sigma}\\right) = \\Phi\\left(\\frac{y-\\mu}{\\sigma}\\right)$. To find the quantile function $F_g^{-1}(u)$, we set $F_g(y) = u$ and solve for $y$:\n$u = \\Phi\\left(\\frac{y-\\mu}{\\sigma}\\right) \\implies \\Phi^{-1}(u) = \\frac{y-\\mu}{\\sigma} \\implies y = \\mu + \\sigma \\Phi^{-1}(u)$.\nSo, the quantile function is $F_g^{-1}(u) = \\mu + \\sigma \\Phi^{-1}(u)$.\n\nSubstituting these quantile functions into the integral for $W_1$:\n$$W_1(p_r, p_g) = \\int_0^1 \\left| \\Phi^{-1}(u) - (\\mu + \\sigma \\Phi^{-1}(u)) \\right| \\, du$$\n$$W_1(p_r, p_g) = \\int_0^1 \\left| (1-\\sigma)\\Phi^{-1}(u) - \\mu \\right| \\, du$$\n\nTo evaluate this integral, we perform a change of variables. Let $z = \\Phi^{-1}(u)$, which implies $u = \\Phi(z)$. The differential is $du = \\phi(z) dz$, where $\\phi(z)$ is the probability density function (PDF) of the standard normal distribution. As $u$ ranges from $0$ to $1$, $z$ ranges from $-\\infty$ to $\\infty$. The integral becomes:\n$$W_1(p_r, p_g) = \\int_{-\\infty}^{\\infty} \\left| (1-\\sigma)z - \\mu \\right| \\phi(z) \\, dz$$\nThis expression is the definition of the expectation $\\mathbb{E}[|(1-\\sigma)Z - \\mu|]$, where $Z$ is a random variable with distribution $\\mathcal{N}(0,1)$.\n\nLet $W = (1-\\sigma)Z - \\mu$. Since $Z$ is a normal random variable, $W$ is also normally distributed. Its mean and variance are:\n- $\\mathbb{E}[W] = \\mathbb{E}[(1-\\sigma)Z - \\mu] = (1-\\sigma)\\mathbb{E}[Z] - \\mu = (1-\\sigma) \\cdot 0 - \\mu = -\\mu$.\n- $\\text{Var}(W) = \\text{Var}[(1-\\sigma)Z - \\mu] = (1-\\sigma)^2 \\text{Var}(Z) = (1-\\sigma)^2 \\cdot 1 = (1-\\sigma)^2$.\nThus, $W \\sim \\mathcal{N}(-\\mu, (1-\\sigma)^2)$. The problem is now to compute $\\mathbb{E}[|W|]$. This is the first moment of a folded normal distribution.\n\nWe handle two cases for $\\sigma$:\nCase 1: $\\sigma = 1$. The random variable $W$ becomes a constant: $W = (1-1)Z - \\mu = -\\mu$. The expectation is $\\mathbb{E}[|-\\mu|] = |\\mu|$.\n\nCase 2: $\\sigma \\neq 1$. The mean of the folded normal distribution for a variable $X \\sim \\mathcal{N}(\\mu_X, \\sigma_X^2)$ is given by the formula:\n$$\\mathbb{E}[|X|] = \\mu_X \\left(1 - 2\\Phi(-\\mu_X/\\sigma_X)\\right) + \\sigma_X \\sqrt{2/\\pi} \\exp\\left(-\\frac{\\mu_X^2}{2\\sigma_X^2}\\right)$$\nIn our context, $X = W$, so we substitute $\\mu_X = -\\mu$ and $\\sigma_X = \\sqrt{(1-\\sigma)^2} = |1-\\sigma|$.\n$$W_1 = (-\\mu) \\left(1 - 2\\Phi\\left(-\\frac{-\\mu}{|1-\\sigma|}\\right)\\right) + |1-\\sigma| \\sqrt{\\frac{2}{\\pi}} \\exp\\left(-\\frac{(-\\mu)^2}{2|1-\\sigma|^2}\\right)$$\n$$W_1 = -\\mu \\left(1 - 2\\Phi\\left(\\frac{\\mu}{|1-\\sigma|}\\right)\\right) + |1-\\sigma| \\sqrt{\\frac{2}{\\pi}} \\exp\\left(-\\frac{\\mu^2}{2(1-\\sigma)^2}\\right)$$\nUsing the relationship between the error function $\\text{erf}(x)$ and $\\Phi(x)$, specifically $1 - 2\\Phi(z) = -\\text{erf}(z/\\sqrt{2})$, the expression simplifies to:\n$$W_1 = -\\mu \\left(-\\text{erf}\\left(\\frac{\\mu}{|1-\\sigma|\\sqrt{2}}\\right)\\right) + |1-\\sigma| \\sqrt{\\frac{2}{\\pi}} \\exp\\left(-\\frac{\\mu^2}{2(1-\\sigma)^2}\\right)$$\n$$W_1 = \\mu \\cdot \\text{erf}\\left(\\frac{\\mu}{|1-\\sigma|\\sqrt{2}}\\right) + |1-\\sigma| \\sqrt{\\frac{2}{\\pi}} \\exp\\left(-\\frac{\\mu^2}{2(1-\\sigma)^2}\\right)$$\nThis is the closed-form solution for $\\sigma \\neq 1$.\n\nIn summary, the closed-form expression for the $1$-Wasserstein distance is:\n$$ W_1(\\mathcal{N}(0,1), \\mathcal{N}(\\mu,\\sigma^2)) = \\begin{cases} |\\mu| & \\text{if } \\sigma = 1 \\\\ \\mu \\cdot \\text{erf}\\left(\\frac{\\mu}{|1-\\sigma|\\sqrt{2}}\\right) + |1-\\sigma| \\sqrt{\\frac{2}{\\pi}} \\exp\\left(-\\frac{\\mu^2}{2(1-\\sigma)^2}\\right) & \\text{if } \\sigma \\neq 1 \\end{cases} $$\n\n**2. Numerical Estimation of the Empirical $W_1$**\n\nThe problem provides the methodology for estimating the $1$-Wasserstein distance empirically in one dimension. For two sets of samples of equal size $n$, one from $p_r$ and one from $p_g$, the empirical distance is the mean absolute difference between their order statistics (sorted samples). This is a direct consequence of the quantile identity applied to the empirical cumulative distribution functions.\n\nThe algorithm is as follows:\n1. Generate $n$ independent samples, $\\{x_1, \\dots, x_n\\}$, from the real distribution $p_r = \\mathcal{N}(0,1)$.\n2. Generate $n$ independent samples, $\\{y_1, \\dots, y_n\\}$, from the generator distribution $p_g = \\mathcal{N}(\\mu, \\sigma^2)$.\n3. Sort both sets of samples to obtain the order statistics: $x_{(1)} \\le x_{(2)} \\le \\dots \\le x_{(n)}$ and $y_{(1)} \\le y_{(2)} \\le \\dots \\le y_{(n)}$.\n4. The empirical $1$-Wasserstein distance, $\\hat{W}_1$, is computed as the average of the absolute differences between corresponding sorted samples:\n$$\\hat{W}_1(p_r, p_g) = \\frac{1}{n} \\sum_{i=1}^n |x_{(i)} - y_{(i)}|$$\nA fixed pseudorandom seed is used to ensure the reproducibility of the random samples and thus the empirical estimate.\n\nThe final program will implement the derived closed-form calculation and the empirical estimation procedure for each of the specified $(\\mu, \\sigma)$ pairs, and report the theoretical value, the empirical estimate, and their absolute difference.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf\n\ndef calculate_closed_form(mu: float, sigma: float) -> float:\n    \"\"\"\n    Computes the closed-form 1-Wasserstein distance between\n    N(0, 1) and N(mu, sigma^2).\n    \"\"\"\n    # The derived formula has a special case for sigma = 1\n    if sigma == 1.0:\n        return np.abs(mu)\n    else:\n        # General formula for sigma != 1\n        abs_one_minus_sigma = np.abs(1.0 - sigma)\n        \n        # Argument for the error function\n        erf_arg = mu / (abs_one_minus_sigma * np.sqrt(2.0))\n        \n        # First term of the sum\n        mu_term = mu * erf(erf_arg)\n        \n        # Second term of the sum\n        sigma_term_factor = abs_one_minus_sigma * np.sqrt(2.0 / np.pi)\n        exp_term = np.exp(-mu**2 / (2.0 * abs_one_minus_sigma**2))\n        sigma_term = sigma_term_factor * exp_term\n        \n        return mu_term + sigma_term\n\ndef calculate_empirical(mu: float, sigma: float, n: int, rng: np.random.Generator) -> float:\n    \"\"\"\n    Computes the empirical 1-Wasserstein distance by sampling.\n    \"\"\"\n    # Generate n samples from the real distribution N(0, 1)\n    samples_r = rng.standard_normal(n)\n    \n    # Generate n samples from the generator distribution N(mu, sigma^2)\n    # Note: rng.standard_normal generates from N(0,1). We scale by sigma and shift by mu.\n    samples_g = rng.standard_normal(n) * sigma + mu\n    \n    # Sort both sets of samples to get the order statistics\n    sorted_r = np.sort(samples_r)\n    sorted_g = np.sort(samples_g)\n    \n    # The empirical distance is the mean absolute difference of the order statistics\n    w_empirical = np.mean(np.abs(sorted_r - sorted_g))\n    \n    return w_empirical\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (mu, sigma)\n    test_cases = [\n        (0.0, 1.0),\n        (1.0, 1.0),\n        (0.0, 2.0),\n        (-2.0, 1.5),\n        (0.5, 0.5),\n        (3.0, 0.1)\n    ]\n    \n    # Sample size for empirical estimation\n    n_samples = 100000\n    \n    # Fixed seed for reproducibility. Use a modern Generator object.\n    seed = 42\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for mu, sigma in test_cases:\n        # Task 1: Compute the closed-form value\n        w_closed = calculate_closed_form(mu, sigma)\n        \n        # Task 2: Compute the empirical estimate\n        w_empirical = calculate_empirical(mu, sigma, n_samples, rng)\n        \n        # Task 3: Compute the absolute difference\n        delta = np.abs(w_closed - w_empirical)\n        \n        # Store the triplet of floats\n        results.append([w_closed, w_empirical, delta])\n\n    # Final print statement in the exact required format.\n    # The format requires a string representation of a list of lists.\n    # e.g., [[val1, val2, val3],[...]]\n    # map(str, results) converts each inner list to its string form \"[v1, v2, v3]\"\n    # ','.join(...) combines them with commas in between\n    # f\"[{...}]\" wraps the final string in the outer list brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A key advantage of WGANs is their ability to provide meaningful gradients even when the real and generated distributions have disjoint supports, a common cause of training failure in standard GANs. This practice allows you to investigate the behavior of the critic function that makes this possible, revealing how it learns to optimally span the \"gaps\" between distributions. Through a hands-on simulation using projected gradient ascent, you will visualize the structure of the optimal critic and gain a deeper understanding of the source of WGAN stability .",
            "id": "3137255",
            "problem": "Construct a complete, runnable program that empirically investigates how support mismatch between the real data distribution $p_r$ and the generator distribution $p_g$ affects the training behavior of the critic function $f$ in a Wasserstein Generative Adversarial Network (WGAN). Use the Kantorovich–Rubinstein dual characterization of the Wasserstein-$1$ distance as the fundamental base: the Wasserstein-$1$ distance equals the supremum of the objective $\\,\\mathbb{E}_{p_r}[f] - \\mathbb{E}_{p_g}[f]\\,$ taken over all $1$-Lipschitz functions $f$. Work in one spatial dimension and approximate $f$ on a uniform discrete grid.\n\nYour program must implement the following purely mathematical and algorithmic procedure:\n\n- Define a uniform grid on a bounded interval $[x_{\\min}, x_{\\max}]$ with $N$ points and spacing $\\Delta x$. Represent $f$ as values $\\{f(x_i)\\}_{i=1}^N$ on this grid.\n- Define $p_r$ and $p_g$ as uniform distributions over unions of closed intervals in $[x_{\\min}, x_{\\max}]$. On the grid, approximate $\\mathbb{E}_{p_r}[f]$ and $\\mathbb{E}_{p_g}[f]$ by assigning equal weights to each grid point that lies within the support of $p_r$ (respectively $p_g$), normalized so that the weights for each distribution sum to $1$.\n- Optimize the critic by projected gradient ascent on the discrete objective $L(f) = \\sum_{i=1}^N f(x_i)\\,w_i$ with $w_i = w^{(r)}_i - w^{(g)}_i$, where $w^{(r)}_i$ and $w^{(g)}_i$ are the real and generator weights at $x_i$. Use updates $f \\leftarrow f + \\eta \\, w$ with step size $\\eta$, followed by a projection onto the set of $1$-Lipschitz functions by enforcing the bounded-difference constraints $|f(x_i) - f(x_{i-1})| \\le \\Delta x$ for all adjacent grid points and anchoring $f(x_1) = 0$ to fix the additive gauge. The projection must proceed by clipping the adjacent differences and reconstructing $f$ via cumulative summation, ensuring $|f(x_i) - f(x_{i-1})| \\le \\Delta x$ holds for all $i$.\n- After optimization, compute three quantitative metrics for each test case:\n    $1.$ The approximate Wasserstein-$1$ objective value $\\,\\mathbb{E}_{p_r}[f] - \\mathbb{E}_{p_g}[f] = \\sum_{i=1}^N f(x_i)\\,w_i\\,$, reported as a float.\n    $2.$ The normalized mean absolute slope of $f$ on grid edges lying strictly in gap regions where neither $p_r$ nor $p_g$ has support. Specifically, compute adjacent differences $s_i = f(x_{i+1}) - f(x_i)$, restrict to indices $i$ for which both $x_i$ and $x_{i+1}$ lie outside the supports of $p_r$ and $p_g$, take $\\frac{1}{M}\\sum |s_i|/\\Delta x$ over these edges (with $M$ the count of such edges). If there are no such edges, define this metric to be $0$. Report this normalized mean as a float in $[0,1]$.\n    $3.$ The critic contrast between the midpoints of the leftmost real and generator intervals, defined as $f(m_r) - f(m_g)$ where $m_r$ is the midpoint of the leftmost interval of $p_r$ and $m_g$ is the midpoint of the leftmost interval of $p_g$. Evaluate $f$ at the nearest grid points to $m_r$ and $m_g$. Report this as a float.\n\nDesign a test suite that probes different support relationships between $p_r$ and $p_g$. Use the following cases:\n\n- Case $1$ (fully disjoint supports with a single gap): domain $[0,3]$ with $N=401$; $p_r$ support $[0,1]$; $p_g$ support $[2,3]$.\n- Case $2$ (two-component supports with alternating disjoint components): domain $[0,5]$ with $N=401$; $p_r$ supports $[0,1]$ and $[3,4]$; $p_g$ supports $[1,2]$ and $[4,5]$.\n- Case $3$ (complete overlap): domain $[0,2]$ with $N=401$; $p_r$ support $[0.5,1.5]$; $p_g$ support $[0.5,1.5]$.\n- Case $4$ (small disjoint intervals near boundaries): domain $[0,1]$ with $N=401$; $p_r$ support $[0,0.2]$; $p_g$ support $[0.8,1.0]$.\n- Case $5$ (interleaved multi-component supports): domain $[0,3]$ with $N=401$; $p_r$ supports $[0,0.5]$ and $[1.5,2.0]$; $p_g$ supports $[0.5,1.0]$ and $[2.5,3.0]$.\n\nFor optimization, use $T=800$ ascent-projection iterations and step size $\\eta=5.0$. These values fix the computational procedure and ensure reproducibility.\n\nYour program must produce a single line of output containing the results aggregated across all test cases in order, as a comma-separated list of lists enclosed in square brackets. Each inner list must contain the three floats in the order described above and must be rounded to $4$ decimal places. For example, the output format should be exactly like\n$[[a_1,b_1,c_1],[a_2,b_2,c_2],\\ldots,[a_5,b_5,c_5]]$,\nwith no spaces anywhere in the line.\n\nThere are no physical units involved in this problem, and angles are not used. All results must be reported as floats or lists of floats as specified. The program must be self-contained and require no user input or external files.",
            "solution": "The user requests a computational investigation into the behavior of the critic function in a one-dimensional Wasserstein Generative Adversarial Network (WGAN). The problem is valid, scientifically grounded in the theory of optimal transport and constrained optimization, and provides a complete, well-posed specification for an algorithm and its test cases. We will proceed with a full solution.\n\nThe foundational principle of this problem is the Kantorovich-Rubinstein duality, which provides a formulation for the Wasserstein-$1$ distance, also known as the Earth-Mover's distance, between two probability distributions $p_r$ (real data) and $p_g$ (generator). The duality states:\n$$\nW_1(p_r, p_g) = \\sup_{\\|f\\|_L \\le 1} \\left( \\mathbb{E}_{x \\sim p_r}[f(x)] - \\mathbb{E}_{x \\sim p_g}[f(x)] \\right)\n$$\nHere, the supremum is taken over all $1$-Lipschitz functions $f: \\mathcal{X} \\to \\mathbb{R}$, which are functions satisfying $|f(x) - f(y)| \\le |x - y|$ for all $x, y$ in the domain $\\mathcal{X}$. In the context of WGANs, this function $f$ is called the critic. The training process for the critic involves finding a function $f$ that maximizes the objective $\\mathbb{E}_{p_r}[f] - \\mathbb{E}_{p_g}[f]$. This problem investigates this optimization process on a discrete domain.\n\nWe first discretize the problem. The continuous domain, a closed interval $[x_{\\min}, x_{\\max}]$, is replaced by a uniform grid of $N$ points $\\{x_i\\}_{i=1}^N$, where $x_i = x_{\\min} + (i-1)\\Delta x$ and the grid spacing is $\\Delta x = (x_{\\max} - x_{\\min})/(N-1)$. The critic function $f$ is represented by a vector of its values on this grid, denoted by $\\mathbf{f} = (f(x_1), f(x_2), \\dots, f(x_N))$.\n\nThe distributions $p_r$ and $p_g$ are defined as uniform over specified sub-intervals of $[x_{\\min}, x_{\\max}]$. The expectations are approximated by weighted sums over the grid points. Let $S_r$ be the set of indices $i$ such that $x_i$ lies within the support of $p_r$, and let $N_r = |S_r|$. The weights for $p_r$ are $w^{(r)}_i = 1/N_r$ if $i \\in S_r$ and $w^{(r)}_i = 0$ otherwise. The weights $w^{(g)}_i$ for $p_g$ are defined analogously. The discrete objective function to be maximized is:\n$$\nL(\\mathbf{f}) = \\sum_{i=1}^N f(x_i) w^{(r)}_i - \\sum_{i=1}^N f(x_i) w^{(g)}_i = \\sum_{i=1}^N f(x_i) (w^{(r)}_i - w^{(g)}_i) = \\mathbf{f} \\cdot \\mathbf{w}\n$$\nwhere $\\mathbf{w}$ is the vector with components $w_i = w^{(r)}_i - w^{(g)}_i$.\n\nThe optimization is performed using projected gradient ascent. The gradient of the linear objective $L(\\mathbf{f})$ with respect to the vector $\\mathbf{f}$ is simply the weight vector $\\mathbf{w}$. The update rule consists of a gradient ascent step followed by a projection step to enforce the constraints.\n1.  **Gradient Ascent Step**: An intermediate function $\\mathbf{f}_{\\text{new}}$ is computed by moving in the direction of the gradient:\n    $$\n    \\mathbf{f}_{\\text{new}} \\leftarrow \\mathbf{f} + \\eta \\mathbf{w}\n    $$\n    where $\\eta$ is the step size.\n\n2.  **Projection Step**: The function $\\mathbf{f}_{\\text{new}}$ is projected back onto the set of discrete $1$-Lipschitz functions that are anchored at the origin. The discrete $1$-Lipschitz condition is $|f(x_i) - f(x_{i-1})| \\le \\Delta x$. The anchoring condition is $f(x_1) = 0$. The specified projection algorithm is:\n    a. Compute the forward differences of $\\mathbf{f}_{\\text{new}}$: $\\Delta_i = f_{\\text{new}}(x_{i+1}) - f_{\\text{new}}(x_i)$ for $i=1, \\dots, N-1$.\n    b. Clip these differences to the allowable range: $\\Delta'_i = \\text{clip}(\\Delta_i, -\\Delta x, \\Delta x)$.\n    c. Reconstruct the final updated function $\\mathbf{f}_{\\text{next}}$ by setting its first component to zero and then performing a cumulative sum of the clipped differences:\n       $$\n       f_{\\text{next}}(x_1) = 0\n       $$\n       $$\n       f_{\\text{next}}(x_i) = \\sum_{j=1}^{i-1} \\Delta'_j \\quad \\text{for } i > 1\n       $$\n    This procedure is repeated for a fixed number of iterations, $T=800$, with a step size $\\eta=5.0$.\n\nAfter the optimization converges to the final critic function $\\mathbf{f}^*$, we compute three metrics to analyze its properties:\n\n1.  **Approximate Wasserstein-$1$ Objective**: This is the value of the objective function for the optimized critic, $\\sum_{i=1}^N f^*(x_i) w_i$. It provides an estimate of the Wasserstein distance between the discrete distributions.\n\n2.  **Normalized Mean Absolute Slope in Gaps**: This metric quantifies the critic's behavior in regions where neither $p_r$ nor $p_g$ has support. We identify all adjacent grid point pairs $(x_i, x_{i+1})$ that fall within such a \"gap\". The metric is the mean of the normalized absolute slopes, $|f^*(x_{i+1}) - f^*(x_i)|/\\Delta x$, over all such pairs. A value close to $1$ indicates that the critic is using its maximum allowable slope to span the distance between the supports, which is characteristic behavior for WGANs with disjoint supports. If no such gap edges exist, the metric is defined as $0$.\n\n3.  **Critic Contrast**: This is defined as $f^*(m_r) - f^*(m_g)$, where $m_r$ and $m_g$ are the midpoints of the leftmost support intervals of $p_r$ and $p_g$, respectively. The function values are taken at the grid points nearest to these midpoints. This metric measures the critic's ability to distinguish the cores of the two distributions.\n\nThe provided test cases are designed to explore different scenarios of support overlap and separation, revealing how the critic's structure adapts to the geometry of the distributions. The implementation will systematically process each case and compute the specified metrics.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the WGAN critic optimization simulation for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: fully disjoint supports with a single gap\n        {'domain': (0.0, 3.0), 'N': 401, 'pr_supports': [[0.0, 1.0]], 'pg_supports': [[2.0, 3.0]]},\n        # Case 2: two-component supports with alternating disjoint components\n        {'domain': (0.0, 5.0), 'N': 401, 'pr_supports': [[0.0, 1.0], [3.0, 4.0]], 'pg_supports': [[1.0, 2.0], [4.0, 5.0]]},\n        # Case 3: complete overlap\n        {'domain': (0.0, 2.0), 'N': 401, 'pr_supports': [[0.5, 1.5]], 'pg_supports': [[0.5, 1.5]]},\n        # Case 4: small disjoint intervals near boundaries\n        {'domain': (0.0, 1.0), 'N': 401, 'pr_supports': [[0.0, 0.2]], 'pg_supports': [[0.8, 1.0]]},\n        # Case 5: interleaved multi-component supports\n        {'domain': (0.0, 3.0), 'N': 401, 'pr_supports': [[0.0, 0.5], [1.5, 2.0]], 'pg_supports': [[0.5, 1.0], [2.5, 3.0]]},\n    ]\n\n    T = 800  # Number of iterations\n    eta = 5.0  # Step size\n\n    all_results = []\n\n    for case in test_cases:\n        # 1. Setup grid and distributions\n        xmin, xmax = case['domain']\n        N = case['N']\n        x, dx = np.linspace(xmin, xmax, N, retstep=True)\n\n        pr_supports = case['pr_supports']\n        pg_supports = case['pg_supports']\n\n        # Create masks for supports\n        pr_mask = np.zeros(N, dtype=bool)\n        for start, end in pr_supports:\n            pr_mask |= (x >= start) & (x <= end)\n\n        pg_mask = np.zeros(N, dtype=bool)\n        for start, end in pg_supports:\n            pg_mask |= (x >= start) & (x <= end)\n        \n        # Calculate weights\n        w_r = np.zeros(N)\n        num_pr_points = np.sum(pr_mask)\n        if num_pr_points > 0:\n            w_r[pr_mask] = 1.0 / num_pr_points\n\n        w_g = np.zeros(N)\n        num_pg_points = np.sum(pg_mask)\n        if num_pg_points > 0:\n            w_g[pg_mask] = 1.0 / num_pg_points\n        \n        w = w_r - w_g\n\n        # 2. Optimize the critic function f\n        f = np.zeros(N)\n        for _ in range(T):\n            # Gradient ascent step\n            f_new = f + eta * w\n            \n            # Projection step\n            diffs = f_new[1:] - f_new[:-1]\n            clipped_diffs = np.clip(diffs, -dx, dx)\n            \n            f = np.zeros(N)\n            f[1:] = np.cumsum(clipped_diffs)\n\n        # 3. Compute metrics\n        # Metric 1: Wasserstein Objective\n        metric1 = np.sum(f * w)\n\n        # Metric 2: Normalized Mean Absolute Slope in Gaps\n        gap_mask = ~pr_mask & ~pg_mask\n        gap_edge_mask = gap_mask[:-1] & gap_mask[1:]\n        \n        if np.any(gap_edge_mask):\n            diffs_f = f[1:] - f[:-1]\n            gap_diffs = diffs_f[gap_edge_mask]\n            metric2 = np.mean(np.abs(gap_diffs) / dx)\n        else:\n            metric2 = 0.0\n\n        # Metric 3: Critic Contrast\n        m_r = (pr_supports[0][0] + pr_supports[0][1]) / 2.0\n        m_g = (pg_supports[0][0] + pg_supports[0][1]) / 2.0\n\n        idx_r = np.abs(x - m_r).argmin()\n        idx_g = np.abs(x - m_g).argmin()\n        \n        metric3 = f[idx_r] - f[idx_g]\n        \n        all_results.append([metric1, metric2, metric3])\n\n    # 4. Format and print output\n    case_strs = []\n    for result_case in all_results:\n        inner_str = f\"[{result_case[0]:.4f},{result_case[1]:.4f},{result_case[2]:.4f}]\"\n        case_strs.append(inner_str)\n    \n    output_str = f\"[{','.join(case_strs)}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "In practice, training a WGAN involves a delicate dance between the generator and the critic. A common and effective heuristic is to update the critic several times for each single update of the generator, but why is this necessary? This simulation models the core training dynamics to explore how the critic-to-generator update ratio, $n_D:n_G$, impacts convergence. By observing how different update schedules affect the generator's ability to learn the true data distribution, you will understand the critical importance of a well-converged critic for providing stable and informative gradients .",
            "id": "3137290",
            "problem": "Consider the training stability of a Wasserstein Generative Adversarial Network (WGAN), a specific formulation of a Generative Adversarial Network (GAN), under the two time-scale update rule. Let the real data distribution be a one-dimensional, discrete, uniform mixture of modes located at positions $y_1, y_2, \\dots, y_m$, each with equal mass. Let the generator produce a one-dimensional, discrete, uniform mixture of points located at positions $x_1, x_2, \\dots, x_m$, each with equal mass. Assume $m$ is fixed and identical for both distributions. The Wasserstein-$1$ distance (also called Earth Mover distance) between the real distribution $p_r$ and the generator distribution $p_g$ is denoted by $W_1(p_r, p_g)$ and can be defined by the Kantorovich–Rubinstein duality as\n$$\nW_1(p_r, p_g) = \\sup_{\\|f\\|_{\\text{Lip}} \\le 1} \\left( \\mathbb{E}_{x \\sim p_r}[f(x)] - \\mathbb{E}_{x \\sim p_g}[f(x)] \\right),\n$$\nwhere the supremum is over all $1$-Lipschitz functions $f$. The two time-scale update rule prescribes performing $n_D$ critic (discriminator) updates for every $n_G$ generator updates.\n\nIn one dimension, consider the following principled simulation model that reflects the WGAN training dynamics without parameterizing $f$ explicitly:\n- The critic is represented by its derivative field evaluated at generator support points, $d_i \\approx f'(x_i)$, which must satisfy the $1$-Lipschitz constraint almost everywhere. At each critic update, the field $d_i$ is relaxed toward the optimal subgradient direction using an exponential moving average with relaxation rate $\\alpha \\in (0,1)$:\n$$\nd_i \\leftarrow (1 - \\alpha) \\, d_i + \\alpha \\, s_i,\n$$\nwhere $s_i = \\operatorname{sgn}\\big(F_r(x_i) - F_g(x_i)\\big)$, and $F_r$ and $F_g$ are the cumulative distribution functions (CDFs) of $p_r$ and $p_g$, respectively. Here, $\\operatorname{sgn}$ denotes the sign function with $\\operatorname{sgn}(0) = 0$.\n- At each generator update, each $x_i$ is updated by a simple gradient descent step with step size $\\eta > 0$ using the current critic derivative $d_i$:\n$$\nx_i \\leftarrow x_i - \\eta \\, d_i.\n$$\n\nThe training proceeds in cycles. In each cycle, apply $n_D$ critic updates followed by $n_G$ generator updates. Repeat cycles until a fixed total number of generator updates $T$ has been performed. After training, measure:\n1. Convergence in $W_1$: compute $W_1(p_r, p_g)$ exactly in one dimension by using the monotone transport principle (in one dimension, the optimal transport matches the sorted supports).\n2. Mode coverage: for a tolerance $\\varepsilon > 0$, declare a real mode $y_j$ as covered if there exists some generator point $x_i$ such that $|x_i - y_j| \\le \\varepsilon$. Mode coverage is the fraction of covered modes, expressed as a decimal between $0$ and $1$.\n\nUse the following fixed parameters for all test cases:\n- Real modes: $y = [-3, 0, 3]$ (that is, $m = 3$).\n- Initial generator positions: $x^{(0)} = [-6, 1, 6]$.\n- Critic relaxation rate: $\\alpha = 0.2$.\n- Generator step size: $\\eta = 0.05$.\n- Tolerance for mode coverage: $\\varepsilon = 0.25$.\n- Total number of generator updates: $T = 200$.\n\nDefine convergence as a boolean that is true if and only if $W_1(p_r, p_g) \\le 0.1$ and the mode coverage equals $1$.\n\nImplement a program that simulates the above training dynamics and outputs, for each test case, the final $W_1$ as a float, the mode coverage as a decimal fraction, and the convergence boolean encoded as an integer ($1$ for true, $0$ for false).\n\nTest suite:\n- Case $1$ (boundary, no critic learning): $(n_D, n_G) = (0, 1)$.\n- Case $2$ (balanced): $(n_D, n_G) = (1, 1)$.\n- Case $3$ (typical WGAN-like): $(n_D, n_G) = (5, 1)$.\n- Case $4$ (heavy critic): $(n_D, n_G) = (20, 1)$.\n- Case $5$ (fast generator relative to critic): $(n_D, n_G) = (1, 5)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, aggregating the triplets for all test cases in order, flattened into a single list. For example, the output format must be:\n$[w1_1, \\text{coverage}_1, \\text{converged}_1, w1_2, \\text{coverage}_2, \\text{converged}_2, \\dots]$,\nwhere each $w1_i$ is a float, each coverage is a decimal fraction, and each converged is $0$ or $1$.",
            "solution": "The problem has been validated and is deemed **valid**. It provides a self-contained, scientifically grounded, and well-posed simulation task based on established principles of Wasserstein Generative Adversarial Networks (WGANs) and optimal transport theory. All necessary parameters and initial conditions are specified, allowing for a deterministic and verifiable solution. The initial state of the critic's derivative field, $d_i$, is not explicitly given; however, a standard and logical choice for such iterative algorithms is to initialize it to zero, i.e., $d_i^{(0)} = 0$ for all $i$. This assumption is made in the following solution.\n\nThe objective is to simulate the training dynamics of a simplified WGAN for five different hyperparameter settings of $(n_D, n_G)$ and evaluate the outcome using three metrics: the final Wasserstein-1 distance, the mode coverage, and a binary convergence flag.\n\nThe simulation adheres to the following sequence of steps.\n\n**1. System Initialization**\n\nThe simulation is initialized with the specified parameters.\nThe real data distribution $p_r$ is a discrete uniform distribution over the set of modes $y = \\{-3, 0, 3\\}$. The number of modes is $m=3$.\nThe generator distribution $p_g$ is a discrete uniform distribution over a set of $m=3$ points, $x = \\{x_1, x_2, x_3\\}$. Their initial positions are given by $x^{(0)} = [-6, 1, 6]$.\nThe critic's derivative field, represented by the vector $d = [d_1, d_2, d_3]$, is initialized to $d^{(0)} = [0, 0, 0]$.\nOther fixed parameters are:\n- Critic relaxation rate: $\\alpha = 0.2$\n- Generator step size: $\\eta = 0.05$\n- Mode coverage tolerance: $\\varepsilon = 0.25$\n- Total number of generator updates: $T = 200$\n\n**2. Cumulative Distribution Functions (CDFs)**\n\nThe critic update rule depends on the subgradient of the Wasserstein distance, which in one dimension is related to the difference between the CDFs of the real and generated distributions. For a discrete distribution $P$ with support points $\\{z_1, \\dots, z_m\\}$ and uniform weights $1/m$, the CDF is given by:\n$$\nF_P(z) = \\frac{1}{m} \\sum_{i=1}^m \\mathbf{1}_{z_i \\le z}\n$$\nwhere $\\mathbf{1}$ is the indicator function.\nThus, for our specific problem, the CDFs for the real and generator distributions, evaluated at a point $z$, are:\n$$\nF_r(z) = \\frac{1}{3} \\sum_{j=1}^3 \\mathbf{1}_{y_j \\le z}\n$$\n$$\nF_g(z) = \\frac{1}{3} \\sum_{i=1}^3 \\mathbf{1}_{x_i \\le z}\n$$\n\n**3. Training Loop**\n\nThe training proceeds in cycles. For each test case defined by a pair $(n_D, n_G)$, the total number of cycles is calculated as $N_{\\text{cycles}} = T / n_G$. Since $T=200$ and $n_G$ is either $1$ or $5$ in the test suite, $N_{\\text{cycles}}$ is always an integer. Each cycle consists of two phases: critic updates and generator updates.\n\n**3.1. Critic Update Phase**\n\nFor each of the $n_D$ steps in this phase, the critic's derivative field $d$ is updated. The update for each component $d_i$ is an exponential moving average towards a target direction $s_i$:\n$$\nd_i \\leftarrow (1 - \\alpha) \\, d_i + \\alpha \\, s_i \\quad \\text{for } i = 1, 2, 3\n$$\nThe target direction $s_i$ is the sign of the difference between the CDFs evaluated at the generator's current support point $x_i$:\n$$\ns_i = \\operatorname{sgn}\\big(F_r(x_i) - F_g(x_i)\\big)\n$$\nwhere the sign function is defined as $\\operatorname{sgn}(u) = 1$ if $u > 0$, $-1$ if $u < 0$, and $0$ if $u = 0$. These updates are performed for each of the $m=3$ components of $d$.\n\n**3.2. Generator Update Phase**\n\nFor each of the $n_G$ steps in this phase, the generator's support points $x$ are updated via gradient descent. The gradient is provided by the critic's derivative field $d$.\n$$\nx_i \\leftarrow x_i - \\eta \\, d_i \\quad \\text{for } i = 1, 2, 3\n$$\nThis update moves each point $x_i$ in the direction that is expected to reduce the Wasserstein distance.\n\n**4. Post-Training Evaluation**\n\nAfter completing all $N_{\\text{cycles}}$, the final state of the generator points $x$ is used to compute the performance metrics.\n\n**4.1. Wasserstein-1 Distance ($W_1$)**\n\nFor one-dimensional discrete distributions with an equal number of points and uniform weights, the $W_1$ distance has a simple closed-form solution. It is the mean absolute difference between the sorted support points of the two distributions. Let $x_{(1)}, x_{(2)}, x_{(3)}$ be the sorted generator points and $y_{(1)}, y_{(2)}, y_{(3)}$ be the sorted real modes. The real modes are already sorted: $y = [-3, 0, 3]$.\n$$\nW_1(p_r, p_g) = \\frac{1}{m} \\sum_{i=1}^m |x_{(i)} - y_{(i)}| = \\frac{1}{3} \\big( |x_{(1)} - (-3)| + |x_{(2)} - 0| + |x_{(3)} - 3| \\big)\n$$\n\n**4.2. Mode Coverage**\n\nA real mode $y_j$ is considered \"covered\" if at least one final generator point $x_i$ is within a distance of $\\varepsilon=0.25$ from it.\n$$\n\\text{covered}(y_j) = \\bigvee_{i=1}^3 \\big( |x_i - y_j| \\le \\varepsilon \\big)\n$$\nThe mode coverage is the fraction of real modes that are covered:\n$$\n\\text{Coverage} = \\frac{1}{m} \\sum_{j=1}^m \\text{covered}(y_j)\n$$\n\n**4.3. Convergence Criterion**\n\nA boolean convergence flag is set to true (represented as $1$) if and only if both of the following conditions are met; otherwise, it is false (represented as $0$).\n1. The final Wasserstein-1 distance is sufficiently small: $W_1(p_r, p_g) \\le 0.1$.\n2. All real modes are covered: Mode Coverage $= 1.0$.\n\nThis entire process is repeated for each of the five $(n_D, n_G)$ test cases, and the results are aggregated into a single flat list for the final output.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates WGAN training dynamics and evaluates performance for multiple test cases.\n    \"\"\"\n\n    def run_simulation(nD, nG):\n        \"\"\"\n        Runs a single simulation for a given (nD, nG) pair.\n\n        Args:\n            nD (int): Number of critic updates per cycle.\n            nG (int): Number of generator updates per cycle.\n\n        Returns:\n            tuple: A triplet containing the final W1 distance (float),\n                   mode coverage (float), and convergence status (int).\n        \"\"\"\n        # Fixed parameters\n        y_real = np.array([-3.0, 0.0, 3.0])\n        m = len(y_real)\n        x = np.array([-6.0, 1.0, 6.0])\n        alpha = 0.2\n        eta = 0.05\n        epsilon = 0.25\n        T = 200\n\n        # Initial state\n        d = np.zeros(m)\n\n        # The problem statement implies T is a multiple of nG for all test cases.\n        if T % nG != 0:\n            # This case is not expected based on the problem description.\n            # We enforce the total number of generator updates to be exactly T.\n            num_cycles = T // nG\n            remaining_gen_updates = T % nG\n        else:\n            num_cycles = T // nG\n            remaining_gen_updates = 0\n\n        for _ in range(num_cycles):\n            # Critic update phase\n            for _ in range(nD):\n                # Calculate CDFs F_r(x_i) and F_g(x_i)\n                fr_x = np.array([np.sum(y_real <= xi) for xi in x]) / m\n                fg_x = np.array([np.sum(x <= xi) for xi in x]) / m\n\n                # Calculate target direction s\n                s = np.sign(fr_x - fg_x)\n\n                # Update critic's derivative field d\n                d = (1 - alpha) * d + alpha * s\n\n            # Generator update phase\n            for _ in range(nG):\n                # Update generator points x\n                x = x - eta * d\n        \n        # Handle any remaining generator updates if T is not a multiple of nG\n        # (Not needed for the given test cases, but robust)\n        if remaining_gen_updates > 0:\n             # Perform one more cycle of critic updates before final generator steps\n            for _ in range(nD):\n                fr_x = np.array([np.sum(y_real <= xi) for xi in x]) / m\n                fg_x = np.array([np.sum(x <= xi) for xi in x]) / m\n                s = np.sign(fr_x - fg_x)\n                d = (1 - alpha) * d + alpha * s\n            for _ in range(remaining_gen_updates):\n                x = x - eta * d\n\n\n        # --- Post-training evaluation ---\n\n        # 1. Wasserstein-1 distance\n        x_sorted = np.sort(x)\n        w1_distance = np.mean(np.abs(x_sorted - y_real))\n\n        # 2. Mode coverage\n        covered_count = 0\n        for yj in y_real:\n            is_covered = np.any(np.abs(x - yj) <= epsilon)\n            if is_covered:\n                covered_count += 1\n        mode_coverage = covered_count / m\n\n        # 3. Convergence criterion\n        converged = (w1_distance <= 0.1) and (mode_coverage == 1.0)\n        converged_int = 1 if converged else 0\n\n        return w1_distance, mode_coverage, converged_int\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0, 1),   # Case 1: no critic learning\n        (1, 1),   # Case 2: balanced\n        (5, 1),   # Case 3: typical WGAN-like\n        (20, 1),  # Case 4: heavy critic\n        (1, 5),   # Case 5: fast generator relative to critic\n    ]\n\n    results = []\n    for nD, nG in test_cases:\n        w1, coverage, converged = run_simulation(nD, nG)\n        results.extend([w1, coverage, converged])\n\n    # Final print statement in the exact required format.\n    formatted_results = []\n    for i, res in enumerate(results):\n        # Format floats, keep ints as is\n        if i % 3 in [0, 1]:  # w1 and coverage\n            formatted_results.append(f\"{res:.7f}\")\n        else: # converged\n            formatted_results.append(str(res))\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}