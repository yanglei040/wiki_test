{
    "hands_on_practices": [
        {
            "introduction": "在生成器网络中，转置卷积（或称反卷积）是实现上采样的常用技术，但它容易在生成的图像中引入棋盘状的伪影。本练习旨在通过量化分析，帮助你深入理解这一常见问题，你将定义一个名为“周期性子网格方差”($\\mathrm{PSV}$)的指标来精确衡量这些伪影，并将其与更先进的“先调整大小后卷积”策略进行比较。通过这个实践，你将学会如何识别并量化生成对抗网络中的架构缺陷，并掌握一种在实践中提升生成图像质量的关键方法。",
            "id": "3127615",
            "problem": "您有一项任务，需要研究在用于图像到图像翻译的生成对抗网络 (GANs) 中，由转置卷积（也称为反卷积）可能引起的棋盘格伪影，并将其与一种先缩放后卷积的替代方案进行比较。目标是定义基于数学的伪影度量和一个 PatchGAN（生成对抗网络中基于块的判别器）对此类伪影敏感度的代理指标，并在一个小型合成测试套件上以确定性方式运行的程序中实现它们。\n\n首先从离散卷积和转置卷积的核心定义开始。设图像为一个二维实数数组。设图像与卷积核的卷积定义为离散和\n$$\n(y \\star k)[i,j] \\equiv \\sum_{u}\\sum_{v} y[i-u,j-v]\\,k[u,v],\n$$\n在需要时隐式使用零填充。设图像 $x$ 的因子为 $s$ 的最近邻上采样表示为 $U_{s}^{\\mathrm{NN}}(x)$，其定义为将每个像素复制成一个 $s \\times s$ 的块。设步幅为 $s$ 的转置卷积通过在相邻像素之间沿两个空间轴插入 $(s-1)$ 个零，形成一个形状扩大了 $s$ 倍的稀疏图像 $Z_{s}(x)$，然后与卷积核 $k$ 进行标准卷积来实现。先缩放后卷积的替代方案是 $(U_{s}^{\\mathrm{NN}}(x)) \\star k$ 的复合操作，使用零填充以保持尺寸。\n\n定义以下伪影度量，称为周期性子网格方差 (PSV)。给定一个图像 $I$ 和步幅 $s$，对于每个剩余类 $g=(a,b)$，其中 $a \\in \\{0,\\dots,s-1\\}$ 且 $b \\in \\{0,\\dots,s-1\\}$，设\n$$\n\\mu_g \\equiv \\frac{1}{|\\{(i,j): i \\equiv a \\bmod s,\\; j \\equiv b \\bmod s\\}|}\\sum_{\\substack{i\\equiv a \\bmod s\\\\ j\\equiv b \\bmod s}} I[i,j],\n$$\n并设全局均值为 $\\mu \\equiv \\frac{1}{HW}\\sum_{i,j} I[i,j]$，其中 $H$ 和 $W$ 是高度和宽度。则周期性子网格方差为\n$$\n\\mathrm{PSV}_s(I) \\equiv \\frac{1}{s^2}\\sum_{a=0}^{s-1}\\sum_{b=0}^{s-1}\\left(\\mu_{(a,b)}-\\mu\\right)^2.\n$$\n直观地看，如果存在与步幅为 $s$ 的网格对齐的棋盘格模式，那么 $\\mathrm{PSV}_s(I)$ 会增加，而对于常数图像，它为 $0$。\n\n为了与 PatchGAN 敏感度关联，定义一个代理敏感度泛函，它计算大小为 $p \\times p$ 的非重叠块内的 PSV。对于一个可被 $s$ 整除的固定 $p$，将图像划分为 $N$ 个不相交的块 $\\{I^{(n)}\\}_{n=1}^N$。定义每块的 PSV 为 $\\mathrm{PSV}_s(I^{(n)})$，全局方差为 $\\sigma^2 \\equiv \\frac{1}{HW}\\sum_{i,j}\\left(I[i,j]-\\mu\\right)^2$。对于一个固定的阈值 $\\tau \\equiv \\beta \\,\\sigma^2$，其中 $\\beta$ 是一个小的常数 $\\beta \\in (0,1)$，定义敏感度代理指标\n$$\nS_{\\text{patch}}(I; s,p,\\beta)\\equiv \\frac{1}{N}\\sum_{n=1}^N \\mathbf{1}\\left\\{\\mathrm{PSV}_s\\left(I^{(n)}\\right)>\\tau\\right\\}.\n$$\n此数量近似于被基于块的判别器标记为伪影主导的块的比例。\n\n您的程序必须为每个测试用例实现以下处理流程：\n- 输入：一个形状为 $L \\times L$、值在 $[0,1]$ 范围内的低分辨率图像 $x$，一个上采样步幅 $s$，以及一个卷积核大小 $k$，其预定义的、由一维向量 $v$ 通过 $K = v v^\\top$ 构建的可分离核 $K$ 被归一化以使 $\\sum_{u,v} K[u,v] = 1$。\n- 转置卷积输出：$I_{\\text{deconv}} = Z_s(x) \\star K$，使用零填充，输出尺寸恰好为 $(sL) \\times (sL)$。\n- 先缩放后卷积输出：$I_{\\text{resize}} = (U_{s}^{\\mathrm{NN}}(x)) \\star K$，使用零填充，输出尺寸恰好为 $(sL) \\times (sL)$。\n- 计算 $G_{\\text{deconv}} \\equiv \\mathrm{PSV}_s(I_{\\text{deconv}})$ 和 $G_{\\text{resize}} \\equiv \\mathrm{PSV}_s(I_{\\text{resize}})$。\n- 计算差值 $R \\equiv G_{\\text{deconv}} - G_{\\text{resize}}$。\n- 计算 $S_{\\text{deconv}} \\equiv S_{\\text{patch}}(I_{\\text{deconv}}; s,p,\\beta)$ 和 $S_{\\text{resize}} \\equiv S_{\\text{patch}}(I_{\\text{resize}}; s,p,\\beta)$，以及差值 $P \\equiv S_{\\text{deconv}} - S_{\\text{resize}}$。\n\n对所有测试用例使用以下固定参数：\n- 步幅 $s = 2$。\n- 块大小 $p = 8$。\n- 敏感度阈值比例 $\\beta = 0.1$。\n- 卷积核：\n  - 如果 $k=3$，使用 $v = [1,2,1]$。\n  - 如果 $k=4$，使用 $v = [1,3,3,1]$。\n  在两种情况下，都使用 $K = \\frac{1}{\\sum_{u} \\sum_{v} v[u]v[v]} \\, v v^\\top$ 以使 $\\sum K = 1$。\n- 卷积是二维的，带零填充和“相同”输出尺寸。\n\n测试套件：\n- 案例1（预期出现棋盘格的顺利路径）：$L=16$，$k=3$，$x$ 是一个中心为1的正方形的二值图像。具体来说，如果 $i \\in [L/4, 3L/4)$ 且 $j \\in [L/4, 3L/4)$，则 $x[i,j] = 1$，否则为 $0$。\n- 案例2（卷积核大小可被步幅整除，伪影减少）：$L=16$，$k=4$，$x$ 与案例1中的中心正方形相同。\n- 案例3（边缘情况：常数图像）：$L=16$，$k=3$，对于所有 $i,j$，$x[i,j] = 1$。\n- 案例4（随机纹理）：$L=16$，$k=3$，$x$ 是一个伯努利随机图像，$\\mathbb{P}(x[i,j]=1)=0.5$，使用固定的随机种子 $42$ 生成。\n\n要求的最终输出：\n- 对于每个案例，计算如上定义的对 $(R,P)$。\n- 您的程序应产生单行输出，包含一个逗号分隔的扁平列表形式的结果，顺序为 $[R_1,P_1,R_2,P_2,R_3,P_3,R_4,P_4]$，四舍五入到恰好 $6$ 位小数，并用方括号括起来，例如 $\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$。\n\n所有数组索引都从零开始。没有物理单位。不出现角度。您的实现必须在给定上述测试套件和常量的情况下完全确定。仅使用指定的环境和库。您解决方案的正确性由测试用例产生的数值来评判。",
            "solution": "用户提供的问题是有效的，因为它具有科学依据、定义明确且客观。它基于深度学习和信号处理中的既有概念，专门解决生成对抗网络 (GANs) 中转置卷积的棋盘格伪影问题。所有数学定义、参数和测试用例都以足够的精度指定，以允许一个唯一且可验证的解。\n\n问题的核心是实现两种上采样-卷积过程，为其输出计算指定的伪影度量，并比较结果。这两种过程是：\n1.  **转置卷积**：表示为插入零后进行标准卷积。一个大小为 $L \\times L$ 的输入图像 $x$ 首先通过在相邻像素间插入 $s-1$ 个零被转换为一个大小为 $(sL-s+1) \\times (sL-s+1)$ 的稀疏图像 $Z_s(x)$。然后这个稀疏图像与一个卷积核 $K$ 进行卷积。卷积操作必须进行填充以产生一个大小为 $(sL) \\times (sL)$ 的最终输出 $I_{\\text{deconv}}$。\n2.  **先缩放后卷积**：一个概念上更简单的替代方案，其中输入图像 $x$ 首先使用最近邻复制将图像放大 $s$ 倍，得到一个大小为 $(sL) \\times (sL)$ 的图像 $U_{s}^{\\mathrm{NN}}(x)$。然后这个放大的图像与卷积核 $K$ 进行卷积，使用保持图像尺寸的填充（`'same'` 卷积），得到输出 $I_{\\text{resize}}$。\n\n该问题引入了两个度量来量化伪影：\n-   **周期性子网格方差 ($\\mathrm{PSV}_s(I)$)**：该度量衡量图像 $I$ 中 $s \\times s$ 周期性子网格上平均像素值的方差。一个高的 $\\mathrm{PSV}_s$ 值表示存在一个与上采样步幅 $s$ 匹配的强烈周期性模式，这是棋盘格伪影的特征。它定义为 $\\mathrm{PSV}_s(I) \\equiv \\frac{1}{s^2}\\sum_{g}\\left(\\mu_{g}-\\mu\\right)^2$，其中 $\\mu$ 是全局均值，$\\mu_g$ 是 $s^2$ 个子网格的均值。\n-   **基于块的敏感度 ($S_{\\text{patch}}(I; s, p, \\beta)$)**：该度量作为 PatchGAN 判别器可能如何对伪影作出反应的代理指标。它计算图像中非重叠的 $p \\times p$ 块的比例，这些块的局部 $\\mathrm{PSV}_s$ 超过了一个动态阈值 $\\tau = \\beta \\sigma^2$，其中 $\\sigma^2$ 是图像的全局方差，$\\beta$ 是一个小的常数。\n\n对于四个测试用例中的每一个，解决方案按以下步骤进行：\n1.  **输入生成**：根据测试用例参数构建大小为 $L \\times L$ 的低分辨率输入图像 $x$ 和大小为 $k \\times k$ 的卷积核 $K$。可分离核 $K$ 从向量 $v$ 导出为 $K = vv^\\top$ 并归一化使其总和为 $1$。\n2.  **图像生成**：根据其定义生成高分辨率图像 $I_{\\text{deconv}}$ 和 $I_{\\text{resize}}$。$I_{\\text{deconv}}$ 的卷积操作通过一次 `'full'` 卷积，然后进行对称裁剪来实现，以达到指定的 $(sL) \\times (sL)$ 输出维度。$I_{\\text{resize}}$ 的卷积使用 `'same'` 卷积实现，它自然会产生一个与其输入大小相同的输出，即 $(sL) \\times (sL)$。\n3.  **度量计算**：\n    -   计算 PSV 值，$G_{\\text{deconv}} = \\mathrm{PSV}_s(I_{\\text{deconv}})$ 和 $G_{\\text{resize}} = \\mathrm{PSV}_s(I_{\\text{resize}})$。它们的差值为 $R = G_{\\text{deconv}} - G_{\\text{resize}}$。一个正的 $R$ 值表示转置卷积方法比先缩放后卷积方法产生更多的棋盘格伪影。\n    -   使用指定的参数 $s=2$、$p=8$ 和 $\\beta=0.1$ 计算基于块的敏感度值，$S_{\\text{deconv}} = S_{\\text{patch}}(I_{\\text{deconv}})$ 和 $S_{\\text{resize}} = S_{\\text{patch}}(I_{\\text{resize}})$。它们的差值为 $P = S_{\\text{deconv}} - S_{\\text{resize}}$。一个正的 $P$ 值表明 PatchGAN 更可能在转置卷积输出中识别出伪影。\n4.  **结果聚合**：收集所有四个测试用例计算出的对 $(R, P)$，并格式化为单个列表作为最终输出。使用固定的随机种子确保了随机图像案例的结果是确定性的。",
            "answer": "```python\nimport numpy as np\nfrom scipy.signal import convolve2d\n\ndef get_kernel(k_size):\n    \"\"\"Constructs a normalized separable 2D kernel.\"\"\"\n    if k_size == 3:\n        v = np.array([1, 2, 1], dtype=float)\n    elif k_size == 4:\n        v = np.array([1, 3, 3, 1], dtype=float)\n    else:\n        raise ValueError(\"Unsupported kernel size\")\n    \n    kernel = np.outer(v, v)\n    kernel /= kernel.sum()\n    return kernel\n\ndef get_input_image(L, image_type, seed=42):\n    \"\"\"Generates the low-resolution input image for a test case.\"\"\"\n    if image_type == 'centered_square':\n        x = np.zeros((L, L), dtype=float)\n        start = L // 4\n        end = 3 * L // 4\n        x[start:end, start:end] = 1.0\n        return x\n    elif image_type == 'constant':\n        return np.ones((L, L), dtype=float)\n    elif image_type == 'random':\n        rng = np.random.default_rng(seed)\n        return rng.choice([0.0, 1.0], size=(L, L), p=[0.5, 0.5])\n    else:\n        raise ValueError(\"Unsupported image type\")\n\ndef upsample_zeros(x, s):\n    \"\"\"Upsamples by inserting s-1 zeros between pixels.\"\"\"\n    L = x.shape[0]\n    up_L = s * L - s + 1\n    z = np.zeros((up_L, up_L), dtype=float)\n    z[::s, ::s] = x\n    return z\n\ndef upsample_nn(x, s):\n    \"\"\"Upsamples using nearest-neighbor replication.\"\"\"\n    return np.kron(x, np.ones((s, s), dtype=float))\n\ndef custom_convolve(img, kernel, output_shape):\n    \"\"\"Performs 2D convolution with padding to achieve a target output size.\"\"\"\n    full_conv = convolve2d(img, kernel, mode='full')\n    \n    crop_total_h = full_conv.shape[0] - output_shape[0]\n    crop_total_w = full_conv.shape[1] - output_shape[1]\n\n    crop_start_h = crop_total_h // 2\n    crop_start_w = crop_total_w // 2\n    \n    crop_end_h = crop_start_h + output_shape[0]\n    crop_end_w = crop_start_w + output_shape[1]\n    \n    return full_conv[crop_start_h:crop_end_h, crop_start_w:crop_end_w]\n\ndef psv(I, s):\n    \"\"\"Computes the Periodic Subgrid Variance (PSV).\"\"\"\n    if I.size == 0:\n        return 0.0\n    mu = I.mean()\n    subgrid_means = []\n    for a in range(s):\n        for b in range(s):\n            subgrid = I[a::s, b::s]\n            if subgrid.size > 0:\n                subgrid_means.append(subgrid.mean())\n            else:\n                subgrid_means.append(mu) \n    \n    psv_val = np.mean((np.array(subgrid_means) - mu)**2)\n    return psv_val\n\ndef s_patch(I, s, p, beta):\n    \"\"\"Computes the patch-based sensitivity proxy.\"\"\"\n    H, W = I.shape\n    num_patches_h = H // p\n    num_patches_w = W // p\n    N = num_patches_h * num_patches_w\n\n    if N == 0:\n        return 0.0\n\n    sigma2 = np.var(I)\n    tau = beta * sigma2\n\n    flagged_patches = 0\n    for i in range(num_patches_h):\n        for j in range(num_patches_w):\n            patch = I[i*p : (i+1)*p, j*p : (j+1)*p]\n            psv_patch = psv(patch, s)\n            if psv_patch > tau:\n                flagged_patches += 1\n    \n    return float(flagged_patches) / N\n\ndef process_case(L, k, image_type, s, p, beta):\n    \"\"\"Processes a single test case and computes (R, P).\"\"\"\n    output_L = s * L\n    \n    x = get_input_image(L, image_type)\n    kernel = get_kernel(k)\n\n    # Transposed convolution path\n    z_x = upsample_zeros(x, s)\n    I_deconv = custom_convolve(z_x, kernel, output_shape=(output_L, output_L))\n    \n    # Resize-then-convolution path\n    u_x = upsample_nn(x, s)\n    I_resize = convolve2d(u_x, kernel, mode='same')\n    \n    # Compute R\n    G_deconv = psv(I_deconv, s)\n    G_resize = psv(I_resize, s)\n    R = G_deconv - G_resize\n    \n    # Compute P\n    S_deconv = s_patch(I_deconv, s, p, beta)\n    S_resize = s_patch(I_resize, s, p, beta)\n    P = S_deconv - S_resize\n    \n    return R, P\n\ndef solve():\n    \"\"\"Main function to run the test suite and print results.\"\"\"\n    # Fixed parameters\n    s = 2\n    p = 8\n    beta = 0.1\n    \n    test_cases = [\n        {'L': 16, 'k': 3, 'image_type': 'centered_square'},\n        {'L': 16, 'k': 4, 'image_type': 'centered_square'},\n        {'L': 16, 'k': 3, 'image_type': 'constant'},\n        {'L': 16, 'k': 3, 'image_type': 'random'},\n    ]\n\n    results = []\n    for case in test_cases:\n        R, P = process_case(case['L'], case['k'], case['image_type'], s, p, beta)\n        results.append(R)\n        results.append(P)\n\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "训练生成对抗网络(GAN)是出了名的不稳定，时常面临模式崩溃或梯度发散的挑战。本练习将引导你通过一个简化的线性模型，在一个可控的环境中直观地探索GAN的训练动态。你将亲手实现并比较三种主流的稳定性技术——梯度裁剪、谱归一化和权重裁剪——看看它们如何影响模型的收敛过程。这项实践将为你建立关于GAN训练稳定性的深刻直觉，并揭示不同正则化方法背后的数学原理。",
            "id": "3127717",
            "problem": "你将实现并分析一个简化的线性化最小-最大博弈中判别器的梯度裁剪，该博弈近似了用于图像到图像翻译任务（如 pix2pix 和 Cycle-Consistent Generative Adversarial Network (CycleGAN)）的生成对抗网络 (GANs) 的局部训练动态。你将比较其对稳定性的影响，并与两种受 Wasserstein 生成对抗网络 (WGAN) 启发的替代约束方法进行对比：谱归一化和权重裁剪。你的程序必须在一个纯数学环境中模拟离散时间梯度下降-上升动态，并报告在一组预定测试用例下的量化稳定性指标。\n\n起点和定义：\n- 考虑一个双线性目标函数，它模拟了生成器-判别器博弈的局部行为，由 $L(\\mathbf{d}, \\mathbf{g}) = \\mathbf{d}^{\\top} \\mathbf{A} \\mathbf{g}$ 给出，其中 $\\mathbf{d} \\in \\mathbb{R}^{n}$ 是判别器参数，$\\mathbf{g} \\in \\mathbb{R}^{n}$ 是生成器参数，$\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ 是一个表示线性化交互的固定矩阵。判别器最大化该目标，而生成器最小化该目标。\n- 从状态 $(\\mathbf{d}_{t}, \\mathbf{g}_{t})$ 出发，使用步长 $\\eta > 0$ 的无约束同时梯度下降-上升更新，使用梯度 $\\nabla_{\\mathbf{d}} L(\\mathbf{d}_{t}, \\mathbf{g}_{t}) = \\mathbf{A} \\mathbf{g}_{t}$ 和 $\\nabla_{\\mathbf{g}} L(\\mathbf{d}_{t}, \\mathbf{g}_{t}) = \\mathbf{A}^{\\top} \\mathbf{d}_{t}$，通过仅依赖于 $(\\mathbf{d}_{t}, \\mathbf{g}_{t})$ 的同时更新来产生下一个状态 $(\\mathbf{d}_{t+1}, \\mathbf{g}_{t+1})$。\n- 你将实现三种判别器约束：\n  1) 梯度裁剪（仅应用于判别器梯度）：将 $\\nabla_{\\mathbf{d}} L$ 替换为 $\\operatorname{clip}_{c}(\\nabla_{\\mathbf{d}} L)$，其中 $\\operatorname{clip}_{c}(\\mathbf{v}) = \\mathbf{v} \\cdot \\min\\{1, c / \\lVert \\mathbf{v} \\rVert_{2}\\}$ 且 $c \\ge 0$ 是一个阈值。\n  2) 谱归一化：将 $\\mathbf{A}$ 重新缩放为 $\\mathbf{A}_{\\text{SN}} = \\alpha \\cdot \\mathbf{A} / \\sigma_{\\max}(\\mathbf{A})$，其中 $\\sigma_{\\max}(\\mathbf{A})$ 是谱范数（最大奇异值），$\\alpha \\ge 0$ 是一个目标值。\n  3) 权重裁剪：通过应用边界为 $w \\ge 0$ 的逐元素裁剪来形成 $\\mathbf{A}_{\\text{WC}}$，即 $(\\mathbf{A}_{\\text{WC}})_{ij} = \\max\\{-w, \\min\\{A_{ij}, w\\}\\}$。\n- 稳定性将通过经验增长率 $r = \\lVert [\\mathbf{d}_{T}; \\mathbf{g}_{T}] \\rVert_{2} / \\lVert [\\mathbf{d}_{0}; \\mathbf{g}_{0}] \\rVert_{2}$ 来量化，该值是在 $T$ 步之后，基于一个共同的随机初始化计算得出的。此外，报告与该方法使用的有效交互矩阵相关的线性化无约束更新算子的谱半径 $\\rho$（对于梯度裁剪，报告由未修改的 $\\mathbf{A}$ 导出的谱半径；对于谱归一化和权重裁剪，报告修改后矩阵的谱半径）。最后，报告判别器梯度裁剪被激活的步数所占的比例，定义为满足 $\\lVert \\nabla_{\\mathbf{d}} L(\\mathbf{d}_{t}, \\mathbf{g}_{t}) \\rVert_{2} > c$ 的迭代次数的比例。对于不使用梯度裁剪的方法，此比例定义为 $0$。\n\n程序要求：\n- 维度为 $n = 5$。使用固定的种子 $s_{A} = 7$ 抽取独立的标准正态分布项来初始化 $\\mathbf{A}$。使用固定的种子 $s_{0} = 11$ 抽取独立的标准正态分布项来初始化 $(\\mathbf{d}_{0}, \\mathbf{g}_{0})$。在所有测试用例中都使用相同的 $\\mathbf{A}$ 和 $(\\mathbf{d}_{0}, \\mathbf{g}_{0})$。\n- 使用同时更新：在每一步 $t$，从 $(\\mathbf{d}_{t}, \\mathbf{g}_{t})$ 计算梯度，并仅使用这些梯度生成 $(\\mathbf{d}_{t+1}, \\mathbf{g}_{t+1})$，而不使用同一次迭代中的中间更新值。\n- 对于梯度裁剪方法，仅裁剪判别器的梯度；生成器的梯度永远不被裁剪。\n- 所有测试用例的模拟时域为 $T = 200$ 步。使用欧几里得范数 $\\lVert \\cdot \\rVert_{2}$。\n- 谱范数 $\\sigma_{\\max}(\\mathbf{A})$ 定义为 $\\mathbf{A}$ 的最大奇异值。谱半径 $\\rho(\\mathbf{M})$ 定义为 $\\mathbf{M}$ 的特征值中的最大绝对值。\n\n要比较的判别器约束：\n- 方法 $0$ (无)：无裁剪，使用原始的 $\\mathbf{A}$。\n- 方法 $1$ (梯度裁剪)：仅对判别器梯度应用阈值为 $c \\ge 0$ 的裁剪。\n- 方法 $2$ (谱归一化)：使用目标值为 $\\alpha \\ge 0$ 的 $\\mathbf{A}_{\\text{SN}}$。\n- 方法 $3$ (权重裁剪)：使用边界为 $w \\ge 0$ 的 $\\mathbf{A}_{\\text{WC}}$。\n\n为给定 $(\\mathbf{d}_{t}, \\mathbf{g}_{t})$ 的一步骤需要实现的模拟细节：\n- 对于方法 $0, 2, 3$，每个测试用例形成一个有效矩阵 $\\mathbf{A}_{\\text{eff}}$：对于方法 $0$，$\\mathbf{A}_{\\text{eff}} = \\mathbf{A}$；对于方法 $2$，$\\mathbf{A}_{\\text{eff}} = \\alpha \\cdot \\mathbf{A} / \\sigma_{\\max}(\\mathbf{A})$；对于方法 $3$，$\\mathbf{A}_{\\text{eff}}$ 是使用边界 $w$ 逐元素裁剪后的 $\\mathbf{A}$。然后更新\n  $\\mathbf{d}_{t+1} = \\mathbf{d}_{t} + \\eta \\, \\mathbf{A}_{\\text{eff}} \\mathbf{g}_{t}$ 和 $\\mathbf{g}_{t+1} = \\mathbf{g}_{t} - \\eta \\, \\mathbf{A}_{\\text{eff}}^{\\top} \\mathbf{d}_{t}$。\n- 对于方法 $1$，使用 $\\mathbf{A}$ 并计算判别器梯度 $\\mathbf{g}^{(d)}_{t} = \\mathbf{A} \\mathbf{g}_{t}$，将其裁剪到阈值 $c$，并更新 $\\mathbf{d}_{t+1} = \\mathbf{d}_{t} + \\eta \\, \\operatorname{clip}_{c}(\\mathbf{g}^{(d)}_{t})$。生成器的更新使用未裁剪的生成器梯度：$\\mathbf{g}_{t+1} = \\mathbf{g}_{t} - \\eta \\, \\mathbf{A}^{\\top} \\mathbf{d}_{t}$。\n\n线性化算子和谱半径：\n- 对于方法 $0, 2, 3$，同时更新定义了在堆叠状态 $\\mathbf{z}_{t} = [\\mathbf{d}_{t}; \\mathbf{g}_{t}]$ 上的一个线性映射，形式为 $\\mathbf{z}_{t+1} = \\mathbf{M} \\mathbf{z}_{t}$，其中 $\\mathbf{M}$ 取决于 $\\eta$ 和 $\\mathbf{A}_{\\text{eff}}$。计算并报告这些方法的谱半径 $\\rho(\\mathbf{M})$。\n- 对于方法 $1$，报告如同未应用裁剪一样计算出的谱半径（使用 $\\mathbf{A}_{\\text{eff}} = \\mathbf{A}$），以提供一个共同的线性参考。\n\n每个测试用例的输出指标：\n- 增长率 $r = \\lVert [\\mathbf{d}_{T}; \\mathbf{g}_{T}] \\rVert_{2} / \\lVert [\\mathbf{d}_{0}; \\mathbf{g}_{0}] \\rVert_{2}$，为一个浮点数。\n- 相应线性更新矩阵 $\\mathbf{M}$ 的谱半径 $\\rho$，为一个浮点数。\n- 裁剪比例 $q \\in [0, 1]$，定义为判别器梯度裁剪被激活的步数所占的比例；对于不使用梯度裁剪的方法，$q = 0$。\n\n测试套件：\n使用 $n = 5$, $T = 200$, $s_{A} = 7$, $s_{0} = 11$，以及以下五个测试用例，每个用例指定为一个元组 $(\\text{method}, \\eta, c, \\alpha, w)$:\n1) $(0, 0.05, 0.0, 0.0, 0.0)$: 无约束，中等步长。\n2) $(1, 0.20, 0.50, 0.0, 0.0)$: 判别器梯度裁剪，阈值 $c = 0.50$，较大步长。\n3) $(2, 0.20, 0.0, 0.50, 0.0)$: 谱归一化至 $\\alpha = 0.50$，较大步长。\n4) $(3, 0.20, 0.0, 0.0, 0.10)$: 权重裁剪，边界 $w = 0.10$，较大步长。\n5) $(1, 0.20, 0.00, 0.0, 0.0)$: 梯度裁剪的边界情况，其中 $c = 0.00$。\n\n你的程序应该生成单行输出，包含一个列表的列表形式的结果 $[\\,[r_{1}, \\rho_{1}, q_{1}],\\,[r_{2}, \\rho_{2}, q_{2}],\\,[r_{3}, \\rho_{3}, q_{3}],\\,[r_{4}, \\rho_{4}, q_{4}],\\,[r_{5}, \\rho_{5}, q_{5}]\\,]$，不含任何额外文本。\n\n所有数值结果都应为标准实数；不涉及物理单位。不使用角度。不使用百分比；裁剪比例是一个在 $[0, 1]$ 区间内的实数。",
            "solution": "该问题要求对一个线性化的最小-最大博弈进行模拟和分析，该博弈可作为生成对抗网络 (GANs) 训练动态的简化模型。我们将比较无约束系统的稳定性与三种应用于判别器的常见正则化技术：梯度裁剪、谱归一化和权重裁剪。\n\n模型的核心是双线性目标函数 $L(\\mathbf{d}, \\mathbf{g}) = \\mathbf{d}^{\\top} \\mathbf{A} \\mathbf{g}$，其中 $\\mathbf{d}, \\mathbf{g} \\in \\mathbb{R}^{n}$ 分别是判别器和生成器的参数向量，$\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ 是一个固定的交互矩阵。判别器旨在最大化 $L$，而生成器旨在最小化它。这是一个零和博弈。\n\n动态由同时梯度下降-上升来建模。梯度为 $\\nabla_{\\mathbf{d}} L = \\mathbf{A} \\mathbf{g}$ 和 $\\nabla_{\\mathbf{g}} L = \\mathbf{A}^{\\top} \\mathbf{d}$。使用学习率 $\\eta > 0$，从状态 $(\\mathbf{d}_{t}, \\mathbf{g}_{t})$ 到 $(\\mathbf{d}_{t+1}, \\mathbf{g}_{t+1})$ 的更新规则是：\n$$\n\\mathbf{d}_{t+1} = \\mathbf{d}_{t} + \\eta (\\nabla_{\\mathbf{d}} L)|_{(\\mathbf{d}_{t}, \\mathbf{g}_{t})}\n$$\n$$\n\\mathbf{g}_{t+1} = \\mathbf{g}_{t} - \\eta (\\nabla_{\\mathbf{g}} L)|_{(\\mathbf{d}_{t}, \\mathbf{g}_{t})}\n$$\n\n对于四种方法中的三种（无约束、谱归一化、权重裁剪），动态是线性的。这些方法可以通过使用一个有效交互矩阵 $\\mathbf{A}_{\\text{eff}}$ 来描述。更新变为：\n$$\n\\mathbf{d}_{t+1} = \\mathbf{d}_{t} + \\eta \\mathbf{A}_{\\text{eff}} \\mathbf{g}_{t}\n$$\n$$\n\\mathbf{g}_{t+1} = \\mathbf{g}_{t} - \\eta \\mathbf{A}_{\\text{eff}}^{\\top} \\mathbf{d}_{t}\n$$\n我们可以将完整状态表示为一个堆叠向量 $\\mathbf{z}_{t} = [\\mathbf{d}_{t}^{\\top}, \\mathbf{g}_{t}^{\\top}]^{\\top} \\in \\mathbb{R}^{2n}$。更新可以写成一个线性变换 $\\mathbf{z}_{t+1} = \\mathbf{M} \\mathbf{z}_{t}$，其中更新矩阵 $\\mathbf{M}$ 为：\n$$\n\\mathbf{M} = \\begin{pmatrix} \\mathbf{I}_{n}  \\eta \\mathbf{A}_{\\text{eff}} \\\\ -\\eta \\mathbf{A}_{\\text{eff}}^{\\top}  \\mathbf{I}_{n} \\end{pmatrix}\n$$\n这里，$\\mathbf{I}_{n}$ 是 $n \\times n$ 的单位矩阵。这个线性动力系统的稳定性由矩阵 $\\mathbf{M}$ 的谱半径 $\\rho(\\mathbf{M})$ 决定，即其特征值的最大绝对值。如果 $\\rho(\\mathbf{M}) > 1$，状态向量的范数 $\\lVert \\mathbf{z}_{t} \\rVert$ 通常会指数级增长，表示不稳定。如果 $\\rho(\\mathbf{M}) \\le 1$，则动态是稳定的。\n\n$\\mathbf{M}$ 的特征值可以通过将它们与 $\\mathbf{A}_{\\text{eff}}$ 的奇异值关联起来找到。矩阵 $\\mathbf{M}$ 可以写成 $\\mathbf{M} = \\mathbf{I}_{2n} + \\mathbf{K}$，其中 $\\mathbf{K} = \\begin{pmatrix} \\mathbf{0}  \\eta \\mathbf{A}_{\\text{eff}} \\\\ -\\eta \\mathbf{A}_{\\text{eff}}^{\\top}  \\mathbf{0} \\end{pmatrix}$。$\\mathbf{K}$ 的特征值是纯虚数，由 $\\lambda_{\\mathbf{K}} = \\pm i \\eta s_{k}$ 给出，其中 $s_{k}$ 是 $\\mathbf{A}_{\\text{eff}}$ 的奇异值。那么 $\\mathbf{M}$ 的特征值为 $\\lambda_{\\mathbf{M}} = 1 + \\lambda_{\\mathbf{K}} = 1 \\pm i \\eta s_{k}$。这些特征值的模为 $|\\lambda_{\\mathbf{M}}| = \\sqrt{1^2 + (\\eta s_{k})^2} = \\sqrt{1 + \\eta^2 s_{k}^2}$。谱半径 $\\rho(\\mathbf{M})$ 是这些模的最大值，它在最大奇异值 $\\sigma_{\\max}(\\mathbf{A}_{\\text{eff}})$ 处取得：\n$$\n\\rho(\\mathbf{M}) = \\sqrt{1 + \\eta^2 \\sigma_{\\max}(\\mathbf{A}_{\\text{eff}})^2}\n$$\n这个解析结果清晰地揭示了步长 $\\eta$、交互矩阵 $\\mathbf{A}_{\\text{eff}}$ 的谱特性与线性系统稳定性之间的联系。\n\n我们现在分析每种方法：\n1.  **方法 0 (无)**: $\\mathbf{A}_{\\text{eff}} = \\mathbf{A}$。稳定性由 $\\rho(\\mathbf{M}) = \\sqrt{1 + \\eta^2 \\sigma_{\\max}(\\mathbf{A})^2}$ 决定。对于任何 $\\eta > 0$，$\\rho(\\mathbf{M}) > 1$，所以无约束系统预计是不稳定的。\n2.  **方法 2 (谱归一化)**: $\\mathbf{A}_{\\text{eff}} = \\alpha \\cdot \\mathbf{A} / \\sigma_{\\max}(\\mathbf{A})$。这个过程直接设置了有效矩阵的谱范数，$\\sigma_{\\max}(\\mathbf{A}_{\\text{eff}}) = \\alpha$。谱半径则由 $\\rho(\\mathbf{M}) = \\sqrt{1 + (\\eta\\alpha)^2}$ 给出。该方法提供了对博弈线性稳定性的显式控制。\n3.  **方法 3 (权重裁剪)**: $\\mathbf{A}_{\\text{eff}}$ 通过逐元素裁剪形成：$(\\mathbf{A}_{\\text{eff}})_{ij} = \\max\\{-w, \\min\\{A_{ij}, w\\}\\}$。此操作通常会减小矩阵元素的大小，这通常会导致更小的谱范数，即 $\\sigma_{\\max}(\\mathbf{A}_{\\text{eff}}) \\le \\sigma_{\\max}(\\mathbf{A})$。这可以通过减小 $\\rho(\\mathbf{M})$ 来稳定系统。\n4.  **方法 1 (梯度裁剪)**: 该方法引入了一个非线性。判别器的更新是 $\\mathbf{d}_{t+1} = \\mathbf{d}_{t} + \\eta \\cdot \\operatorname{clip}_{c}(\\mathbf{A} \\mathbf{g}_{t})$，其中 $\\operatorname{clip}_{c}(\\mathbf{v}) = \\mathbf{v} \\cdot \\min\\{1, c / \\lVert \\mathbf{v} \\rVert_{2}\\}$。这防止了更新向量 $\\eta \\nabla_{\\mathbf{d}} L$ 的范数超过 $\\eta c$。系统不再是线性的，因此对 $\\mathbf{M}$ 的谱半径分析不能描述真实的动态。我们使用 $\\mathbf{A}_{\\text{eff}}=\\mathbf{A}$ 计算 $\\rho$，为该方法必须对抗的潜在线性不稳定性提供一个参考。裁剪被激活的步数比例 $q$ 衡量了这种非线性干预的程度。$c=0$ 的特殊情况导致对任何 $\\mathbf{v}$ 都有 $\\operatorname{clip}_{0}(\\mathbf{v})=\\mathbf{0}$，这将判别器参数 $\\mathbf{d}$ 冻结在它们的初始值。\n\n实现遵循以下原则。首先，使用固定的种子生成矩阵 $\\mathbf{A}$ 和初始状态 $(\\mathbf{d}_0, \\mathbf{g}_0)$。对于每个测试用例，为线性方法确定适当的 $\\mathbf{A}_{\\text{eff}}$。计算谱半径 $\\rho$。然后，一个模拟循环运行 $T=200$ 步，应用所选方法的特定更新规则。对于梯度裁剪，一个计数器跟踪判别器梯度范数 $\\lVert \\mathbf{A} \\mathbf{g}_t \\rVert_2$ 何时超过阈值 $c$。最后，计算增长率 $r = \\lVert [\\mathbf{d}_T; \\mathbf{g}_T] \\rVert_2 / \\lVert [\\mathbf{d}_0; \\mathbf{g}_0] \\rVert_2$ 和裁剪比例 $q$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the GAN dynamics simulation and analysis.\n    \"\"\"\n    # Global parameters\n    n = 5\n    T = 200\n    s_A = 7\n    s_0 = 11\n\n    # Initialize matrix A and initial state vectors d0, g0 using specified seeds\n    rng_A = np.random.default_rng(seed=s_A)\n    A = rng_A.standard_normal((n, n))\n\n    rng_0 = np.random.default_rng(seed=s_0)\n    d0 = rng_0.standard_normal(n)\n    g0 = rng_0.standard_normal(n)\n\n    # Test suite: (method, eta, c, alpha, w)\n    test_cases = [\n        (0, 0.05, 0.0, 0.0, 0.0),  # Method 0: No constraint\n        (1, 0.20, 0.50, 0.0, 0.0),  # Method 1: Gradient clipping\n        (2, 0.20, 0.0, 0.50, 0.0),  # Method 2: Spectral normalization\n        (3, 0.20, 0.0, 0.0, 0.10),  # Method 3: Weight clipping\n        (1, 0.20, 0.00, 0.0, 0.0),  # Method 1: Gradient clipping with c=0\n    ]\n\n    results = []\n    for case in test_cases:\n        method, eta, c, alpha, w = case\n        \n        # --- 1. Calculate Spectral Radius (rho) ---\n        # For methods 0, 2, 3, A_eff is determined by the method's parameters.\n        # For method 1, rho is calculated for the underlying unconstrained linear system.\n        A_eff_for_rho = A\n        if method == 2:\n            s = np.linalg.svd(A, compute_uv=False)\n            sigma_max_A = s[0] if s.size > 0 else 0.0\n            A_eff_for_rho = alpha * A / sigma_max_A if sigma_max_A > 0 else np.zeros_like(A)\n        elif method == 3:\n            A_eff_for_rho = np.clip(A, -w, w)\n        \n        I_n = np.identity(n)\n        M = np.block([\n            [I_n, eta * A_eff_for_rho],\n            [-eta * A_eff_for_rho.T, I_n]\n        ])\n        eigvals = np.linalg.eigvals(M)\n        rho = np.max(np.abs(eigvals))\n\n        # --- 2. Run Simulation ---\n        d, g = d0.copy(), g0.copy()\n        clipped_steps_count = 0\n\n        # Determine the effective matrix for simulation (for linear methods)\n        A_eff_sim = None\n        if method == 0:\n            A_eff_sim = A\n        elif method == 2:\n            A_eff_sim = A_eff_for_rho\n        elif method == 3:\n            A_eff_sim = A_eff_for_rho\n        \n        for _ in range(T):\n            if method == 1:  # Non-linear update for gradient clipping\n                grad_d = A @ g\n                norm_grad_d = np.linalg.norm(grad_d)\n                \n                d_update = grad_d\n                if norm_grad_d > c:\n                    clipped_steps_count += 1\n                    if norm_grad_d > 1e-9: # Avoid division by zero\n                        d_update = grad_d * (c / norm_grad_d)\n                    else: # if norm is zero, gradient is zero vector\n                        d_update = np.zeros_like(grad_d)\n\n                d_next = d + eta * d_update\n                g_next = g - eta * (A.T @ d)\n                d, g = d_next, g_next\n            else:  # Linear updates for methods 0, 2, 3\n                d_next = d + eta * (A_eff_sim @ g)\n                g_next = g - eta * (A_eff_sim.T @ d)\n                d, g = d_next, g_next\n\n        # --- 3. Calculate Final Metrics (r, q) ---\n        z0 = np.concatenate((d0, g0))\n        norm_z0 = np.linalg.norm(z0)\n        \n        zT = np.concatenate((d, g))\n        norm_zT = np.linalg.norm(zT)\n\n        r = norm_zT / norm_z0 if norm_z0 > 1e-9 else 0.0\n        \n        q = 0.0\n        if method == 1:\n            q = clipped_steps_count / T if T > 0 else 0.0\n\n        results.append([r, rho, q])\n\n    # Format the final output string exactly as required\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "CycleGAN通过巧妙的循环一致性损失，在非成对数据集上实现了令人惊艳的图像翻译，但这种设计也并非没有漏洞，模型有时会学会以意想不到的方式“欺骗”损失函数。本练习将引导你揭示CycleGAN的一个经典失败模式：模型可能放弃学习有意义的语义映射，转而学习一种“隐写术”，将源图像信息隐藏在目标域风格的图像中，以此来满足循环一致性约束。我们将运用信息论中的互信息概念来量化这种信息的“泄露”，并探讨如何设计相应的惩罚项来缓解这一问题。",
            "id": "3127696",
            "problem": "您的任务是构建并分析一个合成场景，该场景展示了用于图像到图像翻译的循环一致性生成对抗网络 (CycleGAN) 的一种失效模式，并设计一个有原则的惩罚项来缓解该问题。背景如下：一个生成对抗网络 (GAN) 由一个生成器和一个判别器组成，通过对抗训练来建模目标分布。循环一致性生成对抗网络 (CycleGAN) 在此基础上增加了两个生成器，表示为 $F$ 和 $G$，它们在两个域 $X$ 和 $Y$ 之间进行映射，同时附加一个循环一致性约束，鼓励对于 $x \\sim p_X$ 有 $G(F(x)) \\approx x$ 以及对于 $y \\sim p_Y$ 有 $F(G(y)) \\approx y$。考虑一个对抗性数据集，其中分布 $p_X$ 包含的图像 $x$ 是内容和机器可读代码的配对：每个 $x$ 都有一个内容区域和一个嵌入的类似“快速响应 (QR)”码的区域，该区域确定性地编码了 $x$ 本身的内容。这就产生了一种潜在的失效模式：生成器 $F$ 可以将 $x$ 的内容隐藏在与 $p_Y$ 的边缘统计数据相匹配的高频微观纹理中，而生成器 $G$ 可以解码隐藏的代码以重构 $x$，从而在不执行有意义翻译的情况下实现较低的循环一致性误差。\n\n请为这种隐藏和解码信道建立一个数学上精确的玩具模型，并针对各种参数设置计算两个量。您的模型必须满足以下条件：\n\n1. 设 $x \\in \\{0,1\\}^{m \\times m}$ 表示内容区域的比特，视为参数为 $1/2$ 的独立同分布的伯努利随机变量。设 $k = m^2$ 表示内容比特的数量。定义一个隐藏机制，其中 $F$ 使用幅度编码 $s = a \\cdot (2b - 1)$ 将每个内容比特 $b \\in \\{0,1\\}$ 编码为一个实值微观纹理符号 $s \\in \\mathbb{R}$，其中 $a > 0$ 是一个设计振幅。将 $F$ 和 $G$ 之间的损坏建模为独立应用于每个符号的加性高斯噪声 $n \\sim \\mathcal{N}(0, \\sigma^2)$，得到 $z = s + n$。$G$ 中的解码器使用一个对称擦除阈值策略，阈值为 $\\tau \\ge 0$：如果 $|z| < \\tau$，则该比特被视为已擦除；否则，该比特通过 $z$ 的符号进行解码，即对于 $|z| \\ge \\tau$，有 $\\hat{b} = \\mathbb{I}[z \\ge 0]$。在擦除的情况下，让 $G$ 默认解码为 $\\hat{b} = 0$。\n2. 在此信道下，使用标准正态累积分布函数（表示为 $\\Phi$），推导以下每个比特的概率表达式（用 $a$、$\\sigma$ 和 $\\tau$ 表示）：\n   a. 擦除概率 $p_{\\mathrm{erase}}$，即 $|z| < \\tau$ 的概率。\n   b. 当真实比特为 $1$ 时的错分概率，表示为 $p_{\\mathrm{mis1}}$，即在 $b=1$ 的条件下 $z \\le -\\tau$ 的概率。\n   c. 当真实比特为 $0$ 时的错分概率，表示为 $p_{\\mathrm{mis0}}$，即在 $b=0$ 的条件下 $z \\ge \\tau$ 的概率。\n3. 使用第 2 项中的概率，计算每比特的期望循环一致性误差，定义为期望绝对差 $\\mathbb{E}[|b - \\hat{b}|]$，其中解码策略如上所述，并假设 $b$ 在 $\\{0,1\\}$上均匀分布。将此期望循环一致性误差用 $p_{\\mathrm{erase}}$、$p_{\\mathrm{mis1}}$ 和 $p_{\\mathrm{mis0}}$ 表示，并乘以 $k$ 以获得内容区域所有 $k$ 个比特的期望循环一致性误差。\n4. 使用输入内容 $x$ 和解码输出 $G(F(x))$ 之间的互信息 (MI)（表示为 $I(x; G(F(x)))$）设计一个信息瓶颈惩罚项。为了分析上的易处理性，将微观纹理信道近似为一个二元擦除信道 (BEC)，其擦除概率等于第 2 项中计算的每比特擦除概率 $p_{\\mathrm{erase}}$，并且其输出符号在未擦除时保留一比特信息。在此 BEC 近似和独立比特的假设下，计算 MI 的估计值（以比特为单位），作为 $k$ 和 $p_{\\mathrm{erase}}$ 的函数。\n\n实现一个程序，对于每个指定的参数设置 $(a, \\sigma, \\tau, k)$，计算：\n- $k$ 个比特上的期望循环一致性误差，以浮点数表示。\n- 在 BEC 近似下的信息瓶颈惩罚项 $I(x;G(F(x)))$（以比特为单位），以浮点数表示。\n\n您的程序必须使用以下测试套件，并以要求的格式精确生成最终输出。对于每个测试用例，按上述顺序计算两个浮点数，并将它们聚合到一个扁平列表中。测试套件参数为：\n- 用例 1（正常路径）：$a = 1.0$, $\\sigma = 0.1$, $\\tau = 0.2$, $k = 256$。\n- 用例 2（中等噪声）：$a = 0.3$, $\\sigma = 0.7$, $\\tau = 0.3$, $k = 256$。\n- 用例 3（边界阈值）：$a = 0.5$, $\\sigma = 0.5$, $\\tau = 0.0$, $k = 256$。\n- 用例 4（边缘情况，高噪声和小振幅）：$a = 0.1$, $\\sigma = 1.2$, $\\tau = 0.4$, $k = 256$。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，顺序如下：\n$[\\text{error\\_case1}, I\\_\\text{case1}, \\text{error\\_case2}, I\\_\\text{case2}, \\text{error\\_case3}, I\\_\\text{case3}, \\text{error\\_case4}, I\\_\\text{case4}]$。",
            "solution": "用户提供了一个关于 CycleGAN 失效模式的问题，该问题将使用一个简化的、数学上易于处理的模型进行分析。任务是为几组参数推导并计算两个关键指标：期望循环一致性误差和一个信息论惩罚项。该问题是适定的，在概率论和信息论方面具有科学依据，并且为得出唯一解提供了所有必要的组成部分。\n\n解决方案分三个阶段进行。首先，推导所定义的比特级通信信道中关键事件（擦除和错分）的概率。其次，使用这些概率来构建期望循环一致性误差的表达式。第三，使用基于二元擦除信道的近似来构建信息瓶颈惩罚项。\n\n**1. 每比特信道概率的推导**\n\n问题为单个比特 $b \\in \\{0,1\\}$ 定义了一个通信信道模型。该比特通过幅度调制编码成信号 $s = a(2b-1)$，其中 $a>0$。这意味着如果 $b=1$，信号为 $s=+a$；如果 $b=0$，信号为 $s=-a$。该信号受到加性高斯白噪声 $n \\sim \\mathcal{N}(0, \\sigma^2)$ 的干扰，得到的接收信号为 $z=s+n$。\n\n设 $\\Phi(\\cdot)$ 表示标准正态分布 $\\mathcal{N}(0,1)$ 的累积分布函数 (CDF)。一般正态分布 $\\mathcal{N}(\\mu, \\sigma^2)$ 在点 $x$ 处的 CDF 由 $\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)$ 给出。\n\n当 $b=1$ 时，接收信号为 $z_1 = a+n$，服从分布 $\\mathcal{N}(a, \\sigma^2)$。\n当 $b=0$ 时，接收信号为 $z_0 = -a+n$，服从分布 $\\mathcal{N}(-a, \\sigma^2)$。\n\na. **擦除概率 ($p_{\\mathrm{erase}}$)**：对于给定的阈值 $\\tau \\ge 0$，擦除事件定义为 $|z| < \\tau$。输入比特 $b$ 是一个参数为 $1/2$ 的伯努利随机变量。总擦除概率根据全概率定律求得：\n$$p_{\\mathrm{erase}} = P(|z| < \\tau) = P(|z| < \\tau | b=0)P(b=0) + P(|z| < \\tau | b=1)P(b=1)$$\n由于问题的对称性（$b=0$ 和 $b=1$ 的分布关于 0 相互反射，且擦除区间 $(-\\tau, \\tau)$ 是对称的），条件概率相等：$P(|z_0| < \\tau) = P(|z_1| < \\tau)$。\n因此，我们只需使用其中一个条件概率来计算 $p_{\\mathrm{erase}}$：\n$$p_{\\mathrm{erase}} = P(|z_1| < \\tau) = P(-\\tau < z_1 < \\tau)$$\n$$p_{\\mathrm{erase}} = P(z_1 < \\tau) - P(z_1 \\le -\\tau)$$\n使用 $\\mathcal{N}(a, \\sigma^2)$ 的 CDF，这变为：\n$$p_{\\mathrm{erase}} = \\Phi\\left(\\frac{\\tau - a}{\\sigma}\\right) - \\Phi\\left(\\frac{-\\tau - a}{\\sigma}\\right)$$\n\nb. **错分概率 ($p_{\\mathrm{mis1}}, p_{\\mathrm{mis0}}$)**：当接收信号落入错误的决策区域（不包括擦除区域）时，会发生错分。\n对于 $b=1$，真实比特为 $1$。如果 $z_1 \\le -\\tau$，解码器输出 $\\hat{b}=0$。错分概率 $p_{\\mathrm{mis1}}$ 是：\n$$p_{\\mathrm{mis1}} = P(z_1 \\le -\\tau) = \\Phi\\left(\\frac{-\\tau - a}{\\sigma}\\right)$$\n对于 $b=0$，真实比特为 $0$。如果 $z_0 \\ge \\tau$，解码器输出 $\\hat{b}=1$。错分概率 $p_{\\mathrm{mis0}}$ 是：\n$$p_{\\mathrm{mis0}} = P(z_0 \\ge \\tau) = 1 - P(z_0 < \\tau) = 1 - \\Phi\\left(\\frac{\\tau - (-a)}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{\\tau+a}{\\sigma}\\right)$$\n利用性质 $\\Phi(-x) = 1 - \\Phi(x)$，这可以简化为：\n$$p_{\\mathrm{mis0}} = \\Phi\\left(-\\frac{a+\\tau}{\\sigma}\\right) = \\Phi\\left(\\frac{-a-\\tau}{\\sigma}\\right)$$\n正如问题对称性所预期的， $p_{\\mathrm{mis0}} = p_{\\mathrm{mis1}}$。我们用 $p_{\\mathrm{mis}}$ 表示这个共同的概率。\n\n**2. 期望循环一致性误差的推导**\n\n单个比特的循环一致性误差定义为 $\\mathbb{E}[|b - \\hat{b}|]$。由于误差大小 $|b-\\hat{b}|$ 要么是 $0$ 要么是 $1$，这个期望值等于总错误概率 $P(b \\neq \\hat{b})$。\n$$P(b \\neq \\hat{b}) = P(\\hat{b} \\neq b | b=0)P(b=0) + P(\\hat{b} \\neq b | b=1)P(b=1)$$\n由于 $P(b=0) = P(b=1) = 1/2$：\n$$\\mathbb{E}[|b - \\hat{b}|] = \\frac{1}{2} P(\\hat{b}=1|b=0) + \\frac{1}{2} P(\\hat{b}=0|b=1)$$\n- 如果 $b=0$，当 $\\hat{b}=1$ 时发生错误。这发生在 $z_0 \\ge \\tau$ 的情况下，概率为 $p_{\\mathrm{mis0}} = p_{\\mathrm{mis}}$。\n- 如果 $b=1$，当 $\\hat{b}=0$ 时发生错误。这可能以两种互斥的方式发生：要么比特被错分 ($z_1 \\le -\\tau$)，要么它被擦除 ($|z_1|<\\tau$) 并默认解码为 $\\hat{b}=0$。\n$$P(\\hat{b}=0|b=1) = P(z_1 \\le -\\tau) + P(|z_1| < \\tau) = p_{\\mathrm{mis1}} + p_{\\mathrm{erase}} = p_{\\mathrm{mis}} + p_{\\mathrm{erase}}$$\n综合这些，每比特的期望误差为：\n$$\\mathbb{E}[|b - \\hat{b}|] = \\frac{1}{2} p_{\\mathrm{mis}} + \\frac{1}{2} (p_{\\mathrm{mis}} + p_{\\mathrm{erase}}) = p_{\\mathrm{mis}} + \\frac{1}{2} p_{\\mathrm{erase}}$$\n代入推导出的 $p_{\\mathrm{mis}}$ 和 $p_{\\mathrm{erase}}$ 的表达式：\n$$\\mathbb{E}[|b - \\hat{b}|] = \\Phi\\left(\\frac{-a-\\tau}{\\sigma}\\right) + \\frac{1}{2}\\left[ \\Phi\\left(\\frac{\\tau-a}{\\sigma}\\right) - \\Phi\\left(\\frac{-a-\\tau}{\\sigma}\\right) \\right]$$\n$$\\mathbb{E}[|b - \\hat{b}|] = \\frac{1}{2}\\left[ \\Phi\\left(\\frac{-a-\\tau}{\\sigma}\\right) + \\Phi\\left(\\frac{\\tau-a}{\\sigma}\\right) \\right]$$\n在 $k$ 个独立比特上的总期望循环一致性误差就是每比特误差的 $k$ 倍。\n\n**3. 信息瓶颈惩罚项的推导**\n\n惩罚项定义为互信息 (MI) $I(x; G(F(x)))$，其中 $x$ 是输入内容，$G(F(x))$ 是重构的内容。由于输入比特是独立同分布的，并且信道对每个比特独立作用，总互信息是每比特互信息的 $k$ 倍：$I(x; G(F(x))) = k \\cdot I(b; \\hat{b})$。\n\n问题指定将信道近似为擦除概率为 $p_e$ 的二元擦除信道 (BEC)，其中 $p_e$ 等于我们推导出的 $p_{\\mathrm{erase}}$。在 BEC 中，一个比特要么以概率 $1-p_e$ 被正确传输，要么以概率 $p_e$ 被擦除。对于均匀二元输入 $b$，BEC 的互信息由 $I(b; \\hat{b}) = 1 - p_e$ 比特给出。\n\n因此，信息瓶颈惩罚项近似为：\n$$I(x; G(F(x))) \\approx k \\cdot (1 - p_{\\mathrm{erase}})$$\n代入 $p_{\\mathrm{erase}}$ 的表达式：\n$$I(x; G(F(x))) \\approx k \\cdot \\left(1 - \\left[ \\Phi\\left(\\frac{\\tau - a}{\\sigma}\\right) - \\Phi\\left(\\frac{-\\tau - a}{\\sigma}\\right) \\right]\\right)$$\n\n**用于实现的公式摘要**\n对于每个参数集 $(a, \\sigma, \\tau, k)$：\n1. 计算参数值：$v_1 = \\frac{-a-\\tau}{\\sigma}$ 和 $v_2 = \\frac{\\tau-a}{\\sigma}$。\n2. 计算 CDF 值：$\\phi_1 = \\Phi(v_1)$ 和 $\\phi_2 = \\Phi(v_2)$。\n3. 计算总期望误差：$E = k \\cdot \\frac{1}{2}(\\phi_1 + \\phi_2)$。\n4. 计算擦除概率：$p_{\\mathrm{erase}} = \\phi_2 - \\phi_1$。\n5. 计算信息惩罚项：$I = k \\cdot (1 - p_{\\mathrm{erase}})$。\n这些公式将在最终的程序中实现。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the expected cycle-consistency error and mutual information penalty\n    for a toy model of a CycleGAN failure mode.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (a, sigma, tau, k)\n    test_cases = [\n        (1.0, 0.1, 0.2, 256),  # Case 1 (happy path)\n        (0.3, 0.7, 0.3, 256),  # Case 2 (moderate noise)\n        (0.5, 0.5, 0.0, 256),  # Case 3 (boundary threshold)\n        (0.1, 1.2, 0.4, 256),  # Case 4 (edge case, high noise and small amplitude)\n    ]\n\n    results = []\n    for case in test_cases:\n        a, sigma, tau, k = case\n\n        # 1. Calculate argument values for the standard normal CDF (Phi).\n        # These correspond to the normalized thresholds.\n        # arg_mis: argument for p_mis calculation\n        # arg_erase_upper: upper bound for p_erase calculation\n        arg_mis = (-a - tau) / sigma\n        arg_erase_upper = (tau - a) / sigma\n\n        # 2. Compute the required CDF values using scipy.stats.norm.cdf.\n        phi_mis = norm.cdf(arg_mis)\n        phi_erase_upper = norm.cdf(arg_erase_upper)\n\n        # 3. Compute the per-bit probabilities.\n        # p_mis is the probability of misclassifying a bit (0->1 or 1->0) without erasure.\n        # The problem shows p_mis0 = p_mis1, so we use a single p_mis.\n        p_mis = phi_mis\n        \n        # p_erase is the probability of a bit being erased (decoded value defaults to 0).\n        # p_erase = Phi((tau-a)/sigma) - Phi((-tau-a)/sigma)\n        p_erase = phi_erase_upper - phi_mis\n\n        # 4. Compute the expected cycle-consistency error over k bits.\n        # Per-bit error is p_mis + 0.5 * p_erase, which simplifies to\n        # 0.5 * [Phi((-a-tau)/sigma) + Phi((tau-a)/sigma)].\n        total_expected_error = k * 0.5 * (phi_mis + phi_erase_upper)\n\n        # 5. Compute the information bottleneck penalty (Mutual Information) in bits.\n        # This is approximated by modeling the channel as a Binary Erasure Channel (BEC)\n        # with erasure probability p_erase. The MI for k bits is k * (1 - p_erase).\n        mi_penalty = k * (1.0 - p_erase)\n\n        results.append(total_expected_error)\n        results.append(mi_penalty)\n\n    # Final print statement in the exact required format.\n    # Produces a single line of output: [error1,I1,error2,I2,...]\n    formatted_results = [f\"{r:.16f}\".rstrip('0').rstrip('.') if 'e' not in f\"{r:.16f}\" else f\"{r:.16e}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}