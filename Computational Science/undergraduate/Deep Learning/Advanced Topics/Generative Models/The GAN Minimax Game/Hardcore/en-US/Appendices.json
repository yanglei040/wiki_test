{
    "hands_on_practices": [
        {
            "introduction": "To truly understand the goal of the GAN minimax game, it is instructive to start with an idealized scenario. This exercise guides you through the analytical solution for a simple case where the data follows a Gaussian distribution and the generator has a compatible structure. By deriving the optimal discriminator and solving for the generator's best strategy, you will reveal the fundamental connection between the GAN objective and minimizing the divergence between the real and generated data distributions .",
            "id": "3185804",
            "problem": "Consider the standard Generative Adversarial Network (GAN) minimax game with value function\n$$\nV(D,G) \\;=\\; \\mathbb{E}_{x\\sim p_{\\text{data}}}\\big[\\ln D(x)\\big] \\;+\\; \\mathbb{E}_{z\\sim p_{z}}\\big[\\ln\\big(1 - D(G(z))\\big)\\big],\n$$\nwhere $p_{\\text{data}}=\\mathcal{N}(\\mu,\\sigma^{2})$ is a one-dimensional Gaussian data distribution with unknown parameters $\\mu\\in\\mathbb{R}$ and $\\sigma0$, the latent distribution is $p_{z}=\\mathcal{N}(0,1)$, the generator is an affine map $G(z)=a z + b$ with parameters $a\\in\\mathbb{R}$ and $b\\in\\mathbb{R}$, and the discriminator $D:\\mathbb{R}\\to(0,1)$ is an arbitrary measurable function. Assume the parameterization enforces the identifiability constraint $a\\ge 0$.\n\nStarting from the definition of the minimax game above and using only first principles of probability, optimization, and information measures, do the following:\n- For a fixed generator $G$ (equivalently, a fixed model distribution), derive the optimal discriminator $D^{\\star}$ by maximizing the value function with respect to $D$.\n- Substitute this optimal discriminator back into the value function to obtain the generator’s induced objective. Using fundamental properties of well-known information measures, argue where this induced objective is minimized.\n- From this, determine the equilibrium generator parameters $(a^{\\star}, b^{\\star})$ as functions of $\\mu$ and $\\sigma$.\n\nProvide your final answer as the row matrix $\\big(a^{\\star}\\;\\; b^{\\star}\\big)$. No numerical rounding is required.",
            "solution": "The problem is to find the equilibrium parameters $(a^{\\star}, b^{\\star})$ of a generative model in a standard Generative Adversarial Network (GAN) setup. The process involves three main steps: finding the optimal discriminator for a fixed generator, deriving the generator's objective function, and minimizing this objective to find the optimal generator parameters.\n\nFirst, let's identify the distributions involved. The data distribution is given as $p_{\\text{data}} = \\mathcal{N}(\\mu, \\sigma^2)$. The generator is an affine transformation $G(z) = az + b$ applied to a latent variable $z$ drawn from a standard normal distribution, $p_z = \\mathcal{N}(0,1)$. The distribution of the generated data, $p_g$, is therefore also a normal distribution. The mean of the generated data is $\\mathbb{E}[G(z)] = \\mathbb{E}[az+b] = a\\mathbb{E}[z]+b = a \\cdot 0 + b = b$. The variance is $\\text{Var}(G(z)) = \\text{Var}(az+b) = a^2\\text{Var}(z) = a^2 \\cdot 1^2 = a^2$. Thus, the generated data distribution is $p_g = \\mathcal{N}(b, a^2)$. The identifiability constraint is $a \\ge 0$.\n\nThe minimax game is defined by the value function $V(D, G)$:\n$$V(D,G) \\;=\\; \\mathbb{E}_{x\\sim p_{\\text{data}}}\\big[\\ln D(x)\\big] \\;+\\; \\mathbb{E}_{z\\sim p_{z}}\\big[\\ln\\big(1 - D(G(z))\\big)\\big]$$\nBy a change of variables, the second expectation can be taken over the distribution of generated data, $p_g$. This allows us to write the value function as an integral over the data space $\\mathbb{R}$:\n$$V(D,G) \\;=\\; \\int_{x \\in \\mathbb{R}} p_{\\text{data}}(x) \\ln D(x) \\,dx \\;+\\; \\int_{x \\in \\mathbb{R}} p_g(x) \\ln(1 - D(x)) \\,dx$$\n$$V(D,G) \\;=\\; \\int_{x \\in \\mathbb{R}} \\big[ p_{\\text{data}}(x) \\ln D(x) + p_g(x) \\ln(1 - D(x)) \\big] \\,dx$$\nFor a fixed generator $G$ (and thus a fixed $p_g$), we find the optimal discriminator $D^{\\star}(x)$ by maximizing $V(D,G)$ with respect to the function $D(x)$. Since the integral is maximized when the integrand is maximized for each value of $x$, we consider the function $f(y) = c_1 \\ln y + c_2 \\ln(1 - y)$ for a fixed $x$, where $y = D(x)$, $c_1 = p_{\\text{data}}(x)$, and $c_2 = p_g(x)$. The domain for $y$ is $(0,1)$. To find the maximum, we compute the derivative with respect to $y$ and set it to zero:\n$$\\frac{df}{dy} = \\frac{c_1}{y} - \\frac{c_2}{1-y} = 0$$\n$$\\frac{p_{\\text{data}}(x)}{D(x)} - \\frac{p_g(x)}{1 - D(x)} = 0$$\n$$p_{\\text{data}}(x)(1 - D(x)) = p_g(x)D(x)$$\n$$p_{\\text{data}}(x) = D(x) \\big(p_{\\text{data}}(x) + p_g(x)\\big)$$\nSolving for $D(x)$ gives the optimal discriminator $D^{\\star}$:\n$$D^{\\star}(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}$$\n\nNext, we substitute this optimal discriminator $D^{\\star}(x)$ back into the value function $V(D,G)$ to find the objective function for the generator, $C(G) = \\max_D V(D,G) = V(D^{\\star}, G)$.\n$$C(G) = \\mathbb{E}_{x\\sim p_{\\text{data}}}\\left[\\ln\\left(\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\\right)\\right] + \\mathbb{E}_{x\\sim p_g}\\left[\\ln\\left(1 - \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\\right)\\right]$$\nThe term inside the second logarithm simplifies to:\n$$1 - \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)} = \\frac{p_{\\text{data}}(x) + p_g(x) - p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)} = \\frac{p_g(x)}{p_{\\text{data}}(x) + p_g(x)}$$\nSo the generator's objective becomes:\n$$C(G) = \\mathbb{E}_{x\\sim p_{\\text{data}}}\\left[\\ln\\left(\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\\right)\\right] + \\mathbb{E}_{x\\sim p_g}\\left[\\ln\\left(\\frac{p_g(x)}{p_{\\text{data}}(x) + p_g(x)}\\right)\\right]$$\nWe can relate this expression to well-known information measures. By adding and subtracting $\\ln 2$ inside each logarithm, we can express $C(G)$ in terms of the Kullback-Leibler (KL) divergence. The KL divergence between two distributions $P$ and $Q$ is $D_{KL}(P \\| Q) = \\mathbb{E}_{x \\sim P}[\\ln(P(x)/Q(x))]$.\nLet's rewrite the terms in $C(G)$:\n$$ \\ln\\left(\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\\right) = \\ln\\left(\\frac{p_{\\text{data}}(x)}{\\frac{p_{\\text{data}}(x) + p_g(x)}{2}}\\right) - \\ln(2) $$\n$$ \\ln\\left(\\frac{p_g(x)}{p_{\\text{data}}(x) + p_g(x)}\\right) = \\ln\\left(\\frac{p_g(x)}{\\frac{p_{\\text{data}}(x) + p_g(x)}{2}}\\right) - \\ln(2) $$\nSubstituting these back into the expression for $C(G)$:\n$$C(G) = \\mathbb{E}_{x\\sim p_{\\text{data}}}\\left[\\ln\\left(\\frac{p_{\\text{data}}(x)}{\\frac{p_{\\text{data}}(x) + p_g(x)}{2}}\\right)\\right] - \\ln(2) \\;+\\; \\mathbb{E}_{x\\sim p_g}\\left[\\ln\\left(\\frac{p_g(x)}{\\frac{p_{\\text{data}}(x) + p_g(x)}{2}}\\right)\\right] - \\ln(2)$$\nRecognizing the KL divergence terms, we have:\n$$C(G) = D_{KL}\\left(p_{\\text{data}} \\left\\| \\frac{p_{\\text{data}} + p_g}{2}\\right.\\right) + D_{KL}\\left(p_g \\left\\| \\frac{p_{\\text{data}} + p_g}{2}\\right.\\right) - 2\\ln(2)$$\nThe sum of these two KL divergences is twice the Jensen-Shannon Divergence (JSD) between $p_{\\text{data}}$ and $p_g$, where $JSD(P \\| Q) = \\frac{1}{2} D_{KL}(P \\| M) + \\frac{1}{2} D_{KL}(Q \\| M)$ with $M = \\frac{P+Q}{2}$.\nTherefore, the generator's objective is:\n$$C(G) = 2 \\cdot JSD(p_{\\text{data}} \\| p_g) - 2\\ln(2)$$\nThe generator's goal is to minimize $C(G)$. Since $2$ and $-2\\ln(2)$ are constants, this is equivalent to minimizing the Jensen-Shannon Divergence, $JSD(p_{\\text{data}} \\| p_g)$. A fundamental property of the JSD is that it is always non-negative, and $JSD(P \\| Q) = 0$ if and only if $P=Q$. Thus, the minimum of $C(G)$ is achieved when the generated distribution $p_g$ is identical to the data distribution $p_{\\text{data}}$.\n\nFinally, we determine the optimal generator parameters $(a^{\\star}, b^{\\star})$ by setting $p_g = p_{\\text{data}}$.\nWe have the data distribution $p_{\\text{data}} = \\mathcal{N}(\\mu, \\sigma^2)$ and the generated distribution $p_g = \\mathcal{N}(b, a^2)$. For these two normal distributions to be identical, their parameters (mean and variance) must be equal.\nEquating the means:\n$$b = \\mu$$\nEquating the variances:\n$$a^2 = \\sigma^2$$\nThis implies $a = \\pm \\sigma$. The problem specifies an identifiability constraint $a \\ge 0$. Since the standard deviation $\\sigma$ is given to be positive ($\\sigma  0$), the only solution that satisfies the constraint is $a = \\sigma$.\nTherefore, the equilibrium generator parameters are $a^{\\star} = \\sigma$ and $b^{\\star} = \\mu$.\nThe final answer is the row matrix $(a^{\\star}, b^{\\star})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sigma  \\mu\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the ideal outcome of GAN training is for the generator to capture the full diversity of the data, this is often not what happens in practice. This problem provides a tangible, analytical look at the phenomenon of 'mode collapse,' where the generator learns to produce only a limited subset of the data types. By working through this discrete two-mode example, you will explore the conditions under which a generator might find a stable equilibrium by ignoring parts of the data distribution, a key challenge in adversarial training .",
            "id": "3185859",
            "problem": "Consider the original Generative Adversarial Network (GAN) minimax game, where the discriminator is a function $D:\\mathcal{X}\\to(0,1)$ and the generator induces a model distribution $p_{g}$ over the data space $\\mathcal{X}$. The objective is\n$$\nV(D,G)\\;=\\;\\mathbb{E}_{x\\sim p_{\\text{data}}}\\!\\left[\\ln D(x)\\right]\\;+\\;\\mathbb{E}_{x\\sim p_{g}}\\!\\left[\\ln\\!\\big(1-D(x)\\big)\\right].\n$$\nConstruct a two-mode data distribution on the discrete space $\\mathcal{X}=\\{-1,+1\\}$ by setting\n$$\np_{\\text{data}}(-1)\\;=\\;w,\\qquad p_{\\text{data}}(+1)\\;=\\;1-w,\\qquad \\text{with } w=\\frac{3}{5}.\n$$\nLet the generator family be the set of all mixtures supported on the same two points, parameterized by a scalar $\\alpha\\in[0,1]$ as\n$$\np_{g,\\alpha}(-1)\\;=\\;\\alpha,\\qquad p_{g,\\alpha}(+1)\\;=\\;1-\\alpha.\n$$\nAssume the discriminator is unconstrained except for the codomain $(0,1)$ and can assign independent values to the two atoms, i.e., $D(-1)=d_{-}$ and $D(+1)=d_{+}$.\n\nStarting from the objective definition above and standard calculus, do the following:\n\n- For a fixed generator parameter $\\alpha\\in[0,1]$, derive the optimal discriminator values $d_{-}^{\\star}$ and $d_{+}^{\\star}$ that maximize $V(D,G_{\\alpha})$.\n- Substitute these optimal discriminator values into $V$ to obtain the optimized generator objective $L(\\alpha)\\equiv\\max_{D}V(D,G_{\\alpha})$.\n- Determine the set of Nash equilibria for the generator when $\\alpha$ is allowed to vary over the full interval $[0,1]$.\n- Now restrict the generator to place all its mass on a single mode, i.e., $\\alpha\\in\\{0,1\\}$. Compute the restricted minimax value\n$$\nV_{\\text{single}}^{\\star}\\;\\equiv\\;\\min_{\\alpha\\in\\{0,1\\}}\\;\\max_{D}V(D,G_{\\alpha})\n$$\nexactly in terms of natural logarithms.\n\nReport as your final answer the exact analytic value of $V_{\\text{single}}^{\\star}$ (no approximation or rounding).",
            "solution": "The problem is valid as it is scientifically grounded in the theory of Generative Adversarial Networks, is well-posed with all necessary information provided, and is formulated using objective, mathematical language. We proceed with the solution.\n\nThe objective function for the GAN minimax game is given by\n$$V(D,G) = \\mathbb{E}_{x\\sim p_{\\text{data}}}\\!\\left[\\ln D(x)\\right] + \\mathbb{E}_{x\\sim p_{g}}\\!\\left[\\ln\\!\\big(1-D(x)\\big)\\right]$$\nThe data space is the discrete set $\\mathcal{X}=\\{-1,+1\\}$. The expectations can be written as sums. Let $D(-1)=d_{-}$ and $D(+1)=d_{+}$. Let $p_{\\text{data}}(-1)=w$, $p_{\\text{data}}(+1)=1-w$, $p_{g,\\alpha}(-1)=\\alpha$, and $p_{g,\\alpha}(+1)=1-\\alpha$. The objective function becomes:\n$$V(d_{-}, d_{+}, \\alpha) = w \\ln(d_{-}) + (1-w) \\ln(d_{+}) + \\alpha \\ln(1 - d_{-}) + (1-\\alpha) \\ln(1 - d_{+})$$\n\n**1. Derive the optimal discriminator $D^{\\star}$**\n\nFor a fixed generator parameter $\\alpha$, we find the optimal discriminator by maximizing $V$ with respect to $d_{-}$ and $d_{+}$. Since the objective function is separable in $d_{-}$ and $d_{+}$, we can optimize them independently.\nTo find the optimal $d_{-}$, we take the partial derivative of $V$ with respect to $d_{-}$ and set it to zero:\n$$\\frac{\\partial V}{\\partial d_{-}} = \\frac{w}{d_{-}} - \\frac{\\alpha}{1 - d_{-}} = 0$$\nSolving for $d_{-}$ gives:\n$$w(1 - d_{-}) = \\alpha d_{-} \\implies w = (\\alpha + w)d_{-} \\implies d_{-}^{\\star}(\\alpha) = \\frac{w}{w + \\alpha}$$\nSimilarly, for $d_{+}$:\n$$\\frac{\\partial V}{\\partial d_{+}} = \\frac{1-w}{d_{+}} - \\frac{1-\\alpha}{1 - d_{+}} = 0$$\nSolving for $d_{+}$ gives:\n$$(1-w)(1 - d_{+}) = (1-\\alpha)d_{+} \\implies 1-w = ((1-\\alpha)+(1-w))d_{+} \\implies d_{+}^{\\star}(\\alpha) = \\frac{1-w}{1-w + 1-\\alpha}$$\nThese are the optimal discriminator values for a given $\\alpha$. In general, the optimal discriminator is $D^{\\star}(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x)+p_{g,\\alpha}(x)}$.\n\n**2. Derive the optimized generator objective $L(\\alpha)$**\n\nWe substitute the optimal discriminator values $d_{-}^{\\star}$ and $d_{+}^{\\star}$ back into the objective function $V$ to obtain $L(\\alpha) = \\max_{D}V(D,G_{\\alpha})$.\n$$L(\\alpha) = w \\ln(d_{-}^{\\star}) + (1-w) \\ln(d_{+}^{\\star}) + \\alpha \\ln(1 - d_{-}^{\\star}) + (1-\\alpha) \\ln(1 - d_{+}^{\\star})$$\nWe also need the terms $1-d_{-}^{\\star}$ and $1-d_{+}^{\\star}$:\n$$1 - d_{-}^{\\star}(\\alpha) = 1 - \\frac{w}{w+\\alpha} = \\frac{\\alpha}{w+\\alpha}$$\n$$1 - d_{+}^{\\star}(\\alpha) = 1 - \\frac{1-w}{1-w+1-\\alpha} = \\frac{1-\\alpha}{1-w+1-\\alpha}$$\nSubstituting these into the expression for $L(\\alpha)$:\n$$L(\\alpha) = w \\ln\\left(\\frac{w}{w+\\alpha}\\right) + (1-w) \\ln\\left(\\frac{1-w}{2-w-\\alpha}\\right) + \\alpha \\ln\\left(\\frac{\\alpha}{w+\\alpha}\\right) + (1-\\alpha) \\ln\\left(\\frac{1-\\alpha}{2-w-\\alpha}\\right)$$\nThis expression represents the generator's objective function after the discriminator has been optimized.\n\n**3. Determine the Nash equilibria**\n\nA Nash equilibrium occurs when the generator chooses $\\alpha$ to minimize its objective $L(\\alpha)$. This corresponds to finding $\\alpha^{\\star} = \\arg\\min_{\\alpha \\in [0,1]} L(\\alpha)$. The global minimum of $L(\\alpha)$ is achieved when the generator distribution matches the data distribution, i.e., $p_{g,\\alpha} = p_{\\text{data}}$, if possible.\nThis occurs when $\\alpha = p_{g,\\alpha}(-1) = p_{\\text{data}}(-1) = w$.\nGiven $w=\\frac{3}{5}$, which is in the interval $[0,1]$, the generator can perfectly match the data distribution. Thus, a Nash equilibrium exists at $\\alpha = w = \\frac{3}{5}$.\nTo verify this, we can differentiate $L(\\alpha)$ with respect to $\\alpha$ and find the critical points. Using the envelope theorem:\n$$\\frac{dL}{d\\alpha} = \\frac{\\partial}{\\partial\\alpha} V(D^{\\star}, G_{\\alpha}) = \\ln(1-d_{-}^{\\star}) - \\ln(1-d_{+}^{\\star})$$\n$$\\frac{dL}{d\\alpha} = \\ln\\left(\\frac{\\alpha}{w+\\alpha}\\right) - \\ln\\left(\\frac{1-\\alpha}{2-w-\\alpha}\\right) = \\ln\\left(\\frac{\\alpha(2-w-\\alpha)}{(w+\\alpha)(1-\\alpha)}\\right)$$\nSetting the derivative to zero implies the argument of the logarithm is $1$:\n$$\\frac{\\alpha(2-w-\\alpha)}{(w+\\alpha)(1-\\alpha)} = 1 \\implies 2\\alpha - w\\alpha - \\alpha^2 = w - w\\alpha + \\alpha - \\alpha^2 \\implies \\alpha=w$$\nThe second derivative is $L''(\\alpha) = \\frac{1}{2w(1-w)}$ at $\\alpha=w$, which is positive for $w \\in (0,1)$, confirming a minimum.\nThe set of Nash equilibria for the generator is therefore $\\{\\frac{3}{5}\\}$.\n\n**4. Compute the restricted minimax value $V_{\\text{single}}^{\\star}$**\n\nWe need to compute $V_{\\text{single}}^{\\star} \\equiv \\min_{\\alpha\\in\\{0,1\\}} L(\\alpha)$. This requires evaluating $L(0)$ and $L(1)$.\n\nCase 1: $\\alpha=0$.\nThe generator distribution is $p_{g,0}(-1)=0, p_{g,0}(+1)=1$.\nThe optimal discriminator values are:\n$d_{-}^{\\star}(0) = \\frac{w}{w+0} = 1$.\n$d_{+}^{\\star}(0) = \\frac{1-w}{1-w+1-0} = \\frac{1-w}{2-w}$.\nThe definition of the discriminator's codomain is $(0,1)$, so $d_{-}^{\\star}(0)=1$ is a boundary point. The maximum is a supremum.\n$L(0) = \\sup_{d_{-}\\to 1} \\{w\\ln(d_{-})\\} + (1-w)\\ln(d_{+}^{\\star}(0)) + 0\\cdot\\ln(1-d_{-}) + 1\\cdot\\ln(1-d_{+}^{\\star}(0))$.\nThe first term evaluates to $w\\ln(1)=0$. The third term is zero because of the $\\alpha=0$ factor (or $\\lim_{x\\to 0} x\\ln x = 0$).\n$$L(0) = (1-w)\\ln\\left(\\frac{1-w}{2-w}\\right) + \\ln\\left(1-\\frac{1-w}{2-w}\\right) = (1-w)\\ln\\left(\\frac{1-w}{2-w}\\right) + \\ln\\left(\\frac{1}{2-w}\\right)$$\n$$L(0) = (1-w)\\ln(1-w) - (1-w)\\ln(2-w) - \\ln(2-w) = (1-w)\\ln(1-w) - (2-w)\\ln(2-w)$$\n\nCase 2: $\\alpha=1$.\nThe generator distribution is $p_{g,1}(-1)=1, p_{g,1}(+1)=0$.\nBy symmetry with the previous case (swapping the roles of $-1$ and $+1$, which corresponds to replacing $w$ with $1-w$ and $\\alpha$ with $1-\\alpha$), we can deduce the result. However, for clarity, we compute it directly.\n$d_{-}^{\\star}(1) = \\frac{w}{w+1}$.\n$d_{+}^{\\star}(1) = \\frac{1-w}{1-w+1-1} = 1$.\nSimilarly, $d_{+}^{\\star}(1)$ is a boundary value.\n$L(1) = w\\ln(d_{-}^{\\star}(1)) + \\sup_{d_{+}\\to 1}\\{(1-w)\\ln(d_{+})\\} + 1\\cdot\\ln(1-d_{-}^{\\star}(1)) + 0\\cdot\\ln(1-d_{+})$.\nThe second and fourth terms are zero.\n$$L(1) = w\\ln\\left(\\frac{w}{w+1}\\right) + \\ln\\left(1-\\frac{w}{w+1}\\right) = w\\ln\\left(\\frac{w}{w+1}\\right) + \\ln\\left(\\frac{1}{w+1}\\right)$$\n$$L(1) = w\\ln(w) - w\\ln(w+1) - \\ln(w+1) = w\\ln(w) - (1+w)\\ln(1+w)$$\n\nNow, we compare $L(0)$ and $L(1)$ for $w=\\frac{3}{5}$. Let $f(x) = x\\ln(x) - (1+x)\\ln(1+x)$. Then $L(1) = f(w)$ and $L(0) = f(1-w)$.\nConsider the derivative of $f(x)$ for $x\\in(0,1)$:\n$$f'(x) = (\\ln x + 1) - (\\ln(1+x) + 1) = \\ln x - \\ln(1+x) = \\ln\\left(\\frac{x}{1+x}\\right)$$\nSince $x0$, we have $1+x  x$, so $\\frac{x}{1+x}  1$. This implies $\\ln(\\frac{x}{1+x})  0$, so $f(x)$ is a strictly decreasing function on $(0,1)$.\nWe are given $w=\\frac{3}{5}$, so $1-w=\\frac{2}{5}$. Since $\\frac{3}{5}  \\frac{2}{5}$, and $f(x)$ is decreasing, it follows that $f(\\frac{3}{5})  f(\\frac{2}{5})$.\nTherefore, $L(1)  L(0)$.\n\nThe required value is $V_{\\text{single}}^{\\star} = \\min\\{L(0), L(1)\\} = L(1)$.\nWe now compute the value of $L(1)$ with $w=\\frac{3}{5}$.\n$$V_{\\text{single}}^{\\star} = \\frac{3}{5}\\ln\\left(\\frac{3}{5}\\right) - \\left(1+\\frac{3}{5}\\right)\\ln\\left(1+\\frac{3}{5}\\right)$$\n$$V_{\\text{single}}^{\\star} = \\frac{3}{5}\\ln\\left(\\frac{3}{5}\\right) - \\frac{8}{5}\\ln\\left(\\frac{8}{5}\\right)$$\n$$V_{\\text{single}}^{\\star} = \\frac{3}{5}(\\ln 3 - \\ln 5) - \\frac{8}{5}(\\ln 8 - \\ln 5)$$\n$$V_{\\text{single}}^{\\star} = \\frac{3}{5}\\ln 3 - \\frac{3}{5}\\ln 5 - \\frac{8}{5}\\ln(2^3) + \\frac{8}{5}\\ln 5$$\n$$V_{\\text{single}}^{\\star} = \\frac{3}{5}\\ln 3 + \\frac{5}{5}\\ln 5 - \\frac{8 \\times 3}{5}\\ln 2$$\n$$V_{\\text{single}}^{\\star} = \\frac{3\\ln 3 + 5\\ln 5 - 24\\ln 2}{5}$$",
            "answer": "$$\\boxed{\\frac{3\\ln(3) - 24\\ln(2) + 5\\ln(5)}{5}}$$"
        },
        {
            "introduction": "Theory provides the foundation, but simulation reveals the dynamics of training in action. This hands-on coding exercise challenges you to build a simple GAN from the ground up, deriving the gradient updates and implementing the training loop. You will then conduct a set of controlled experiments to investigate a critical question in GAN design: how does the structure of the latent space $p_z$ influence the generator's ability to cover all the modes of a complex data distribution ?",
            "id": "3185781",
            "problem": "You are asked to design and implement a complete, runnable program that simulates the minimax training dynamics of a Generative Adversarial Network (GAN), and then uses this simulation to study how a multimodal latent prior distribution interacts with the game to encourage or discourage mode coverage in the generator distribution.\n\nDefinitions and setup that you must use:\n- A Generative Adversarial Network (GAN) consists of a generator and a discriminator. The generator is a function $g_{\\theta}$ with parameters $\\theta$ that maps latent variables $z$ to data space $x$, and the discriminator is a classifier $D_{\\phi}$ with parameters $\\phi$ that outputs the probability that an input $x$ is real data rather than generated.\n- The minimax value function is\n$$\nV(D_{\\phi}, G_{\\theta}) \\;=\\; \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D_{\\phi}(x)] \\;+\\; \\mathbb{E}_{z \\sim p_{z}}[\\log(1 - D_{\\phi}(g_{\\theta}(z)))].\n$$\n- In this task, you must use a one-dimensional data domain and parametric families:\n  - Generator: $g_{\\theta}(z) \\;=\\; a z + b$, where $\\theta = (a,b)$ and $a,b \\in \\mathbb{R}$.\n  - Discriminator: $D_{\\phi}(x) \\;=\\; \\sigma(w x + c)$, where $\\phi = (w,c)$, $w,c \\in \\mathbb{R}$, and $\\sigma(u) = \\frac{1}{1 + e^{-u}}$ is the logistic sigmoid function.\n\nData distribution and latent priors to be used:\n- The target data distribution $p_{\\text{data}}$ is a balanced mixture of two Gaussians on the real line:\n$$\np_{\\text{data}} \\;=\\; \\tfrac{1}{2}\\,\\mathcal{N}(-\\mu,\\sigma_{\\text{data}}^{2}) \\;+\\; \\tfrac{1}{2}\\,\\mathcal{N}(\\mu,\\sigma_{\\text{data}}^{2}),\n$$\nwith $\\mu = 2.0$ and $\\sigma_{\\text{data}} = 0.2$.\n- The latent prior $p_{z}$ will be a mixture of two Gaussians\n$$\np_{z} \\;=\\; \\alpha\\,\\mathcal{N}(m_{1},\\sigma_{z}^{2}) \\;+\\; (1-\\alpha)\\,\\mathcal{N}(m_{2},\\sigma_{z}^{2}),\n$$\nwith parameters $(m_{1},m_{2},\\alpha,\\sigma_{z})$ specified by each test case below.\n\nTraining protocol you must implement:\n- Use the minimax objective $V(D_{\\phi}, G_{\\theta})$ as stated above.\n- Use Monte Carlo to approximate expectations at each step by independent and identically distributed samples from $p_{\\text{data}}$ and $p_{z}$.\n- Derive the parameter update rules from first principles by taking gradients of $V(D_{\\phi}, G_{\\theta})$ with respect to $(w,c)$ for the discriminator and with respect to $(a,b)$ for the generator. Use gradient ascent on $(w,c)$ and gradient descent on $(a,b)$.\n- Use the following hyperparameters exactly:\n  - Number of training iterations $T = 2000$.\n  - At each iteration, perform $1$ discriminator update followed by $1$ generator update.\n  - Batch size $B = 512$ for both real and latent samples per update.\n  - Learning rate for discriminator $\\eta_{D} = 0.01$.\n  - Learning rate for generator $\\eta_{G} = 0.05$.\n  - Initialization: $a_{0} = 0.1$, $b_{0} = 0.0$, $w_{0} = 0.0$, $c_{0} = 0.0$.\n  - Random seed set to a fixed value to ensure reproducibility.\n\nExperimental metric for mode coverage:\n- After training, compute the generator’s images of the latent component means: $m'_{1} = a\\,m_{1} + b$ and $m'_{2} = a\\,m_{2} + b$.\n- Define the set of data means $\\{-\\mu, \\mu\\}$ and the set of generated means $\\{m'_{1}, m'_{2}\\}$.\n- A data mode at mean $\\mu_{d} \\in \\{-\\mu, \\mu\\}$ is considered “covered” if $\\min\\{|m'_{1} - \\mu_{d}|, |m'_{2} - \\mu_{d}|\\} \\le \\tau$, where the coverage tolerance is $\\tau = 0.6$.\n- The coverage score is the integer count in $\\{0,1,2\\}$ equal to the number of data modes that are covered by the generator in this sense.\n\nTest suite you must implement and run:\n- Use the following five cases for $(m_{1},m_{2},\\alpha,\\sigma_{z})$:\n  - Case $1$: $(m_{1},m_{2},\\alpha,\\sigma_{z}) = (-1.0,\\,1.0,\\,0.5,\\,0.3)$.\n  - Case $2$: $(m_{1},m_{2},\\alpha,\\sigma_{z}) = (0.0,\\,0.0,\\,0.5,\\,0.3)$.\n  - Case $3$: $(m_{1},m_{2},\\alpha,\\sigma_{z}) = (-1.0,\\,1.0,\\,0.95,\\,0.3)$.\n  - Case $4$: $(m_{1},m_{2},\\alpha,\\sigma_{z}) = (-0.2,\\,0.2,\\,0.5,\\,0.3)$.\n  - Case $5$: $(m_{1},m_{2},\\alpha,\\sigma_{z}) = (-3.0,\\,3.0,\\,0.5,\\,0.3)$.\n\nWhat to output:\n- Your program must run all five cases in the test suite, train the GAN for each case as specified, compute the final coverage score for each case, and aggregate the five integer results into a single list printed on one line.\n- The final output format must be exactly one line containing the list of results as a comma-separated list enclosed in square brackets, for example: $[r_{1},r_{2},r_{3},r_{4},r_{5}]$ with each $r_{i}$ an integer in $\\{0,1,2\\}$ and no extra text.\n\nNotes:\n- All quantities are dimensionless; there are no physical units involved.\n- Angles are not used; no angle units are required.\n- Percentages, when present (e.g., $\\alpha$), are parameters in $[0,1]$ and must be treated as decimals, not as percentages.",
            "solution": "The problem requires the design and implementation of a simulation for the training dynamics of a simple Generative Adversarial Network (GAN). The goal is to investigate how the structure of a multimodal latent prior distribution, $p_z$, influences the generator's ability to capture the modes of a bimodal data distribution, $p_{\\text{data}}$. We will first derive the necessary mathematical framework and then specify the algorithmic implementation based on these principles.\n\nThe GAN framework consists of a generator $G_{\\theta}$ with parameters $\\theta=(a,b)$ and a discriminator $D_{\\phi}$ with parameters $\\phi=(w,c)$. The generator is a linear function $g_{\\theta}(z) = az + b$. The discriminator is a logistic classifier $D_{\\phi}(x) = \\sigma(wx + c)$, where $\\sigma(u)=(1+e^{-u})^{-1}$ is the logistic sigmoid function. The training is governed by the minimax game on the value function $V(D,G)$:\n$$\nV(D_{\\phi}, G_{\\theta}) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D_{\\phi}(x)] + \\mathbb{E}_{z \\sim p_{z}}[\\log(1 - D_{\\phi}(g_{\\theta}(z)))]\n$$\nThe discriminator aims to maximize this value, while the generator aims to minimize it.\n\n### Gradient Derivation for Parameter Updates\n\nTo implement the training, we use gradient-based optimization. The discriminator parameters $\\phi=(w,c)$ are updated using gradient ascent, and the generator parameters $\\theta=(a,b)$ are updated using gradient descent. We must therefore derive the gradients of $V$ with respect to these parameters.\n\n**1. Discriminator Gradient ($\\nabla_{\\phi} V$)**\n\nThe update rule for the discriminator is $\\phi \\leftarrow \\phi + \\eta_D \\nabla_{\\phi} V$. We compute the partial derivatives of $V$ with respect to $w$ and $c$. Using the chain rule and the property $\\frac{d}{du}\\log\\sigma(u) = 1-\\sigma(u)$:\n$$\n\\frac{\\partial}{\\partial w} \\log D_{\\phi}(x) = \\frac{\\partial}{\\partial w} \\log \\sigma(wx+c) = (1 - \\sigma(wx+c)) \\cdot x = (1 - D_{\\phi}(x))x\n$$\n$$\n\\frac{\\partial}{\\partial c} \\log D_{\\phi}(x) = \\frac{\\partial}{\\partial c} \\log \\sigma(wx+c) = (1 - \\sigma(wx+c)) \\cdot 1 = 1 - D_{\\phi}(x)\n$$\nSimilarly, using the property $\\frac{d}{du}\\log(1-\\sigma(u)) = -\\sigma(u)$:\n$$\n\\frac{\\partial}{\\partial w} \\log(1 - D_{\\phi}(g_{\\theta}(z))) = -\\sigma(wg_{\\theta}(z)+c) \\cdot g_{\\theta}(z) = -D_{\\phi}(g_{\\theta}(z))g_{\\theta}(z)\n$$\n$$\n\\frac{\\partial}{\\partial c} \\log(1 - D_{\\phi}(g_{\\theta}(z))) = -\\sigma(wg_{\\theta}(z)+c) \\cdot 1 = -D_{\\phi}(g_{\\theta}(z))\n$$\nTaking the expectation over the respective distributions, we obtain the gradients:\n$$\n\\frac{\\partial V}{\\partial w} = \\mathbb{E}_{x \\sim p_{\\text{data}}}[(1 - D_{\\phi}(x))x] + \\mathbb{E}_{z \\sim p_{z}}[-D_{\\phi}(g_{\\theta}(z))g_{\\theta}(z)]\n$$\n$$\n\\frac{\\partial V}{\\partial c} = \\mathbb{E}_{x \\sim p_{\\text{data}}}[1 - D_{\\phi}(x)] + \\mathbb{E}_{z \\sim p_{z}}[-D_{\\phi}(g_{\\theta}(z))]\n$$\n\n**2. Generator Gradient ($\\nabla_{\\theta} V$)**\n\nThe update rule for the generator is $\\theta \\leftarrow \\theta - \\eta_G \\nabla_{\\theta} V$. The first term in $V$ is independent of $\\theta$. We compute the partial derivatives of the second term with respect to $a$ and $b$:\n$$\n\\nabla_{\\theta} V = \\mathbb{E}_{z \\sim p_{z}}[\\nabla_{\\theta} \\log(1 - D_{\\phi}(g_{\\theta}(z)))]\n$$\nUsing the chain rule, $\\nabla_{\\theta}[\\cdot] = \\frac{\\partial[\\cdot]}{\\partial g_{\\theta}} \\nabla_{\\theta}g_{\\theta}(z)$. The gradient with respect to the generator's output $g$ is:\n$$\n\\frac{\\partial}{\\partial g} \\log(1 - D_{\\phi}(g)) = \\frac{\\partial}{\\partial g} \\log(1 - \\sigma(wg+c)) = -\\sigma(wg+c) \\cdot w = -w D_{\\phi}(g)\n$$\nThe gradients of the generator function $g_{\\theta}(z) = az+b$ are $\\frac{\\partial g_{\\theta}}{\\partial a} = z$ and $\\frac{\\partial g_{\\theta}}{\\partial b} = 1$. Combining these gives:\n$$\n\\frac{\\partial V}{\\partial a} = \\mathbb{E}_{z \\sim p_{z}} \\left[ (-w D_{\\phi}(g_{\\theta}(z))) \\cdot z \\right] = -\\mathbb{E}_{z \\sim p_{z}} [w D_{\\phi}(g_{\\theta}(z)) z]\n$$\n$$\n\\frac{\\partial V}{\\partial b} = \\mathbb{E}_{z \\sim p_{z}} \\left[ (-w D_{\\phi}(g_{\\theta}(z))) \\cdot 1 \\right] = -\\mathbb{E}_{z \\sim p_{z}} [w D_{\\phi}(g_{\\theta}(z))]\n$$\n\n### Algorithmic Implementation\n\nThe training process is simulated for $T=2000$ iterations. At each iteration, we perform one discriminator update followed by one generator update. The expectations are approximated using Monte Carlo estimation with a batch size of $B=512$.\n\n**1. Sampling**\n-   **Data Distribution $p_{\\text{data}}$**: To draw a sample from $p_{\\text{data}} = \\frac{1}{2}\\,\\mathcal{N}(-\\mu,\\sigma_{\\text{data}}^{2}) + \\frac{1}{2}\\,\\mathcal{N}(\\mu,\\sigma_{\\text{data}}^{2})$, we first select one of the two Gaussian components with equal probability ($0.5$) and then draw a sample from the selected component. For a batch of size $B$, approximately $B/2$ samples will be drawn from each component.\n-   **Latent Distribution $p_z$**: To draw a sample from $p_{z} = \\alpha\\,\\mathcal{N}(m_{1},\\sigma_{z}^{2}) + (1-\\alpha)\\,\\mathcal{N}(m_{2},\\sigma_{z}^{2})$, we select the first component with probability $\\alpha$ and the second with probability $1-\\alpha$, then sample from the chosen Gaussian.\n\n**2. Training Loop**\nAll parameters are initialized as specified: $a_0=0.1, b_0=0.0, w_0=0.0, c_0=0.0$. For each iteration $t=0, \\dots, T-1$:\n\n-   **Discriminator Update**:\n    1.  Draw a batch of $B$ real samples $\\{x_i\\}_{i=1}^B \\sim p_{\\text{data}}$.\n    2.  Draw a batch of $B$ latent samples $\\{z_i\\}_{i=1}^B \\sim p_{z}$.\n    3.  Generate a batch of fake samples $\\{x_{g,i}\\}_{i=1}^B$ where $x_{g,i} = g_{\\theta_t}(z_i) = a_t z_i + b_t$.\n    4.  Compute gradients $\\nabla_{w} V_t$ and $\\nabla_{c} V_t$ using the current parameters $\\phi_t, \\theta_t$ and the batches:\n        $$\n        \\nabla_{w} V_t \\approx \\frac{1}{B}\\sum_{i=1}^B \\left[ (1 - D_{\\phi_t}(x_i))x_i - D_{\\phi_t}(x_{g,i})x_{g,i} \\right] \\\\\n        \\nabla_{c} V_t \\approx \\frac{1}{B}\\sum_{i=1}^B \\left[ (1 - D_{\\phi_t}(x_i)) - D_{\\phi_t}(x_{g,i}) \\right]\n        $$\n    5.  Update discriminator parameters: $w_{t+1} = w_t + \\eta_D \\nabla_{w} V_t$, $c_{t+1} = c_t + \\eta_D \\nabla_{c} V_t$.\n\n-   **Generator Update**:\n    1.  Draw a new batch of $B$ latent samples $\\{z'_i\\}_{i=1}^B \\sim p_{z}$.\n    2.  Generate a new batch of fake samples $\\{x'_{g,i}\\}_{i=1}^B$ with the old generator parameters: $x'_{g,i} = g_{\\theta_t}(z'_i) = a_t z'_i + b_t$.\n    3.  Compute gradients $\\nabla_{a} V_t$ and $\\nabla_{b} V_t$ using the old generator parameters $\\theta_t$ but the *newly updated* discriminator parameters $\\phi_{t+1}$:\n        $$\n        \\nabla_{a} V_t \\approx -\\frac{1}{B}\\sum_{i=1}^B w_{t+1} D_{\\phi_{t+1}}(x'_{g,i}) z'_i \\\\\n        \\nabla_{b} V_t \\approx -\\frac{1}{B}\\sum_{i=1}^B w_{t+1} D_{\\phi_{t+1}}(x'_{g,i})\n        $$\n    4.  Update generator parameters: $a_{t+1} = a_t - \\eta_G \\nabla_{a} V_t$, $b_{t+1} = b_t - \\eta_G \\nabla_{b} V_t$.\n\n### Evaluation of Mode Coverage\n\nAfter $T$ training iterations, the final generator parameters $(a_T, b_T)$ are used to evaluate mode coverage. The latent prior has component means $m_1$ and $m_2$. The generator maps these to $m'_1 = a_T m_1 + b_T$ and $m'_2 = a_T m_2 + b_T$. The target data distribution has means at $-\\mu$ and $\\mu$ (where $\\mu=2.0$).\n\nA data mode at mean $\\mu_d \\in \\{-\\mu, \\mu\\}$ is considered \"covered\" if at least one of the generated means is close to it, i.e., $\\min\\{|m'_{1} - \\mu_{d}|, |m'_{2} - \\mu_{d}|\\} \\le \\tau$, where the tolerance is $\\tau = 0.6$. The final score for a given test case is the total number of covered data modes, which can be $0$, $1$, or $2$. This procedure is repeated for each of the five test cases defined in the problem statement.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\ndef solve():\n    \"\"\"\n    Main function to run the GAN simulation for all test cases and print results.\n    \"\"\"\n\n    # --- Fixed Parameters and Hyperparameters ---\n    fixed_params = {\n        \"mu\": 2.0,\n        \"sigma_data\": 0.2,\n    }\n    hyperparams = {\n        \"T\": 2000,\n        \"B\": 512,\n        \"eta_D\": 0.01,\n        \"eta_G\": 0.05,\n        \"a0\": 0.1,\n        \"b0\": 0.0,\n        \"w0\": 0.0,\n        \"c0\": 0.0,\n        \"seed\": 1234, # Fixed seed for reproducibility\n        \"tau\": 0.6,\n    }\n\n    # --- Test Suite ---\n    test_cases = [\n        # (m1, m2, alpha, sigma_z)\n        (-1.0, 1.0, 0.5, 0.3),   # Case 1\n        (0.0, 0.0, 0.5, 0.3),    # Case 2\n        (-1.0, 1.0, 0.95, 0.3),  # Case 3\n        (-0.2, 0.2, 0.5, 0.3),   # Case 4\n        (-3.0, 3.0, 0.5, 0.3),   # Case 5\n    ]\n    \n    results = []\n    for case_params_tuple in test_cases:\n        case_params = {\n            \"m1\": case_params_tuple[0],\n            \"m2\": case_params_tuple[1],\n            \"alpha\": case_params_tuple[2],\n            \"sigma_z\": case_params_tuple[3],\n        }\n        score = _run_simulation_for_case(case_params, fixed_params, hyperparams)\n        results.append(score)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef _sample_p_data(rng, B, mu, sigma_data):\n    \"\"\"Samples a batch from the real data distribution p_data.\"\"\"\n    # Choose which Gaussian component to sample from for each point\n    # 0 for N(-mu, sigma_data^2), 1 for N(mu, sigma_data^2)\n    choices = rng.binomial(1, 0.5, size=B)\n    \n    # Sample from N(-mu, sigma_data^2)\n    n_neg = B - np.sum(choices)\n    samples_neg = rng.normal(loc=-mu, scale=sigma_data, size=n_neg)\n    \n    # Sample from N(mu, sigma_data^2)\n    n_pos = np.sum(choices)\n    samples_pos = rng.normal(loc=mu, scale=sigma_data, size=n_pos)\n    \n    return np.concatenate((samples_neg, samples_pos))\n\ndef _sample_p_z(rng, B, m1, m2, alpha, sigma_z):\n    \"\"\"Samples a batch from the latent distribution p_z.\"\"\"\n    # Choose which Gaussian component to sample from for each point\n    # 0 for N(m1, sigma_z^2), 1 for N(m2, sigma_z^2)\n    choices = rng.binomial(1, 1 - alpha, size=B)\n\n    # Sample from N(m1, sigma_z^2)\n    n_1 = B - np.sum(choices)\n    samples_1 = rng.normal(loc=m1, scale=sigma_z, size=n_1)\n\n    # Sample from N(m2, sigma_z^2)\n    n_2 = np.sum(choices)\n    samples_2 = rng.normal(loc=m2, scale=sigma_z, size=n_2)\n\n    return np.concatenate((samples_1, samples_2))\n    \n\ndef _run_simulation_for_case(case_params, fixed_params, hyperparams):\n    \"\"\"\n    Trains a GAN for a single case and computes the coverage score.\n    \"\"\"\n    # Unpack parameters for convenience\n    mu, sigma_data = fixed_params[\"mu\"], fixed_params[\"sigma_data\"]\n    m1, m2, alpha, sigma_z = case_params[\"m1\"], case_params[\"m2\"], case_params[\"alpha\"], case_params[\"sigma_z\"]\n    T, B, eta_D, eta_G, tau = hyperparams[\"T\"], hyperparams[\"B\"], hyperparams[\"eta_D\"], hyperparams[\"eta_G\"], hyperparams[\"tau\"]\n    \n    # Initialize random number generator\n    rng = np.random.default_rng(hyperparams[\"seed\"])\n\n    # Initialize parameters\n    a, b = hyperparams[\"a0\"], hyperparams[\"b0\"]\n    w, c = hyperparams[\"w0\"], hyperparams[\"c0\"]\n\n    # Training loop\n    for _ in range(T):\n        # --- Discriminator Update ---\n        x_real = _sample_p_data(rng, B, mu, sigma_data)\n        z_d = _sample_p_z(rng, B, m1, m2, alpha, sigma_z)\n        x_fake = a * z_d + b\n\n        D_real = sigmoid(w * x_real + c)\n        D_fake = sigmoid(w * x_fake + c)\n\n        grad_w_V = np.mean((1 - D_real) * x_real) - np.mean(D_fake * x_fake)\n        grad_c_V = np.mean(1 - D_real) - np.mean(D_fake)\n\n        w += eta_D * grad_w_V\n        c += eta_D * grad_c_V\n\n        # --- Generator Update ---\n        z_g = _sample_p_z(rng, B, m1, m2, alpha, sigma_z)\n        x_fake_g = a * z_g + b\n\n        # D output for fake samples using updated D parameters\n        D_fake_g = sigmoid(w * x_fake_g + c)\n        \n        # Gradients of V w.r.t a and b\n        grad_a_V = -np.mean(w * D_fake_g * z_g)\n        grad_b_V = -np.mean(w * D_fake_g)\n\n        # Gradient descent update\n        a -= eta_G * grad_a_V\n        b -= eta_G * grad_b_V\n\n    # --- Evaluation ---\n    m_prime_1 = a * m1 + b\n    m_prime_2 = a * m2 + b\n    \n    data_means = [-mu, mu]\n    gen_means = [m_prime_1, m_prime_2]\n    coverage_score = 0\n    \n    for data_mean in data_means:\n        min_dist = min(abs(gm - data_mean) for gm in gen_means)\n        if min_dist = tau:\n            coverage_score += 1\n            \n    return coverage_score\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        }
    ]
}