## 引言
[扩散](@entry_id:141445)[概率模型](@entry_id:265150)（Diffusion Probabilistic Models, DPMs）作为一类新兴的[深度生成模型](@entry_id:748264)，已在高质量图像生成等领域取得了革命性的突破，其能力甚至超越了[生成对抗网络](@entry_id:634268)（GANs）。这些模型的核心思想出人意料地优雅：它们通过一个精心设计的“[前向过程](@entry_id:634012)”系统性地破坏数据结构直至其变为纯粹的噪声，然后学习一个“反向过程”来精确地逆转这一步骤，从而学会从噪声中创造出全新的、逼真的数据。这种方法不仅生成效果惊人，其稳健的训练过程和深厚的理论基础也为研究者们所青睐。

本文旨在系统性地揭开[扩散模型](@entry_id:142185)的神秘面纱。我们将深入探讨一个核心问题：机器如何能学会从看似无序的混沌中重建秩序？为了回答这个问题，我们将带领读者进行一次从理论到实践的深度探索。
- 在“原理与机制”一章中，我们将剖析模型的核心数学框架，包括前向加噪与反向去噪过程的精确定义、简化的训练目标、与[分数匹配](@entry_id:635640)的等价性，以及[U-Net架构](@entry_id:635581)和时间编码等关键设计选择。
- 接着，在“应用与跨学科连接”一章中，我们将视野扩展到模型的多功能性，展示其如何与[数学物理](@entry_id:265403)、[控制论](@entry_id:262536)等领域建立理论桥梁，并应用于[材料科学](@entry_id:152226)、[强化学习](@entry_id:141144)和公平性约束等前沿问题。
- 最后，“动手实践”部分将通过精心设计的练习，帮助读者将理论知识转化为实践技能，亲身体验模型训练中的关键动态和高级采样技术的微妙之处。

通过这次学习之旅，你将不仅掌握扩散模型的工作原理，更能理解其作为一种强大而灵活的[概率建模](@entry_id:168598)工具，在现代人工智能领域所扮演的重要角色。

## 原理与机制

本章深入探讨构成[扩散](@entry_id:141445)概率模型（DPM）核心的数学原理和运行机制。我们将从定义其基本过程（前向加噪过程和反向去噪过程）开始，逐步揭示这些模型如何能够从纯噪声中生成高度结构化的数据。我们将剖析模型训练的目标函数、不同的网络参数化选择及其等价性，并探讨模型架构和[采样策略](@entry_id:188482)中的关键设计考量。

### [前向过程](@entry_id:634012)：从有序到混沌的确定性路径

扩散模型的起点是一个精心设计的、逐步破坏[数据结构](@entry_id:262134)的前向加噪过程。这个过程是一个马尔可夫链，它在 $T$ 个离散的时间步中，逐渐将来自复杂数据[分布](@entry_id:182848) $p(x_0)$ 的一个样本 $x_0$ 转化为一个来自简单[先验分布](@entry_id:141376)（通常是标准正态分布）的样本 $x_T$。

#### 离散时间下的马尔可夫加噪

对于任意时间步 $t \in \{1, 2, \dots, T\}$，从 $x_{t-1}$ 到 $x_t$ 的转移被定义为一个高斯扰动。具体而言，转移核 $q(x_t | x_{t-1})$ 是一个均值和[方差](@entry_id:200758)由预定义噪声调度（noise schedule）$\{\beta_t\}_{t=1}^T$ 控制的高斯分布：

$$
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I)
$$

这里，$I$ 是[单位矩阵](@entry_id:156724)，$\beta_t \in (0,1)$ 是一个在第 $t$ 步加入的噪声[方差](@entry_id:200758)。通常，$\beta_t$ 会随着 $t$ 的增大而增大，意味着越往后的步骤加入的噪声越多。为了方便数学推导，我们定义 $\alpha_t = 1 - \beta_t$。因此，[前向过程](@entry_id:634012)的一步可以写作：

$$
x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1-\alpha_t} z_t
$$

其中 $z_t \sim \mathcal{N}(0, I)$ 是一个标准正态噪声。这个公式直观地表明，$x_t$ 是对 $x_{t-1}$ 的轻微缩放并叠加一个小的噪声项构成的。

#### [边际分布](@entry_id:264862)的便捷性质

尽管[前向过程](@entry_id:634012)是逐步定义的，但它有一个极其重要的性质：我们可以直接计算任意时刻 $t$ 的样本 $x_t$ 在给定初始样本 $x_0$ 下的[分布](@entry_id:182848) $q(x_t|x_0)$。通过递归地展开上述方程，我们可以得到 $x_t$ 与 $x_0$ 的直接关系。

令 $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$ 为累积乘积。通过对方程的递归代入，可以证明：

$$
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \bar{z}_t
$$

其中 $\bar{z}_t$ 是多个独立高斯噪声的[线性组合](@entry_id:154743)，其本身也服从标准正态分布 $\mathcal{N}(0, I)$。这意味着从 $x_0$ 到 $x_t$ 的整个加噪过程可以一步完成，其条件分布为一个简单的正态分布：

$$
q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t) I)
$$

这个性质是扩散模型能够被有效训练的基石。它允许我们从数据集中随机抽取一个 $x_0$，并直接采样任意时刻 $t$ 的噪声版本 $x_t$ 用于训练，而无需模拟整个马尔可夫链。

随着 $t$ 从 $0$ 增加到 $T$，$\bar{\alpha}_t$ 从 $1$ 单调递减到接近 $0$。因此，在 $x_t$ 的表达式中，来自 $x_0$ 的“信号”项 $\sqrt{\bar{\alpha}_t} x_0$ 逐渐衰减，而“噪声”项 $\sqrt{1-\bar{\alpha}_t} \bar{z}_t$ 的权重则逐渐增加。当 $T$ 足够大且噪声调度设计得当时，$\bar{\alpha}_T \approx 0$，此时 $x_T$ 的[分布](@entry_id:182848)近似于 $\mathcal{N}(0, I)$，几乎完全抹去了关于 $x_0$ 的所有信息。这个过程将一个结构复杂的初始[分布](@entry_id:182848)，如一个多模态的[高斯混合模型](@entry_id:634640)（GMM），平滑地转变为一个简单的单峰高斯分布，其中原始的模态特征被完全“模糊”掉 。

#### 连续时间下的SDE视角

离散时间的[前向过程](@entry_id:634012)可以被看作是一个[随机微分方程](@entry_id:146618)（SDE）的离散化。当我们考虑时间步长 $\Delta t \to 0$ 的极限时，离散的更新规则收敛到一个连续时间的[随机过程](@entry_id:159502) 。若将噪声调度 $\beta_k$ 定义为 $\beta(t_{k-1})\Delta t$，则[前向过程](@entry_id:634012)的SDE形式为：

$$
\mathrm{d}x_t = -\frac{1}{2}\beta(t)x_t\,\mathrm{d}t + \sqrt{\beta(t)}\,\mathrm{d}w_t
$$

其中 $w_t$ 是一个标准的维纳过程。这个SDE是一个[Ornstein-Uhlenbeck过程](@entry_id:140047)，它描述了一个粒子在向原点漂移的同时经历随机布朗运动。这种连续时间视角不仅为[扩散模型](@entry_id:142185)提供了更深刻的理论框架，还将其与物理学和[随机过程](@entry_id:159502)理论中的其他模型联系起来。这种SDE表述也引出了其对应的时间反向SDE，这对理解反向生成过程至关重要。

### 反向过程：从混沌中重建秩序

生成数据的过程，即反向过程，旨在逆转前向加噪的步骤。我们的目标是学习一个模型 $p_\theta(x_{t-1} | x_t)$，它能够从 $t$ 时刻的噪声样本 $x_t$ 中估计出 $t-1$ 时刻的样本 $x_{t-1}$。从 $t=T$ 开始，通过反复应用这个去噪步骤，我们最终可以从一个纯噪声样本 $x_T \sim \mathcal{N}(0, I)$ 中恢复出一个近似来自原始数据[分布](@entry_id:182848)的样本 $x_0$。

#### 反向过程的棘手之处与关键洞见

反向过程的真实[条件概率](@entry_id:151013) $q(x_{t-1} | x_t)$ 是棘手的。要计算它，需要对所有可能的 $x_0$ 进行积分，这在计算上是不可行的。然而，一个关键的洞见是，如果我们额外**给定初始样本 $x_0$**，那么这个反向条件概率 $q(x_{t-1} | x_t, x_0)$ 就变得易于处理了 。

利用贝叶斯定理，我们可以推导出：
$$
q(x_{t-1} | x_t, x_0) \propto q(x_t | x_{t-1}, x_0) q(x_{t-1} | x_0)
$$

由于[前向过程](@entry_id:634012)的[马尔可夫性质](@entry_id:139474)， $q(x_t | x_{t-1}, x_0) = q(x_t | x_{t-1})$。这两个[分布](@entry_id:182848)都是高斯分布，它们的乘积仍然是[高斯分布](@entry_id:154414)。经过严谨的代数推导，可以得到 $q(x_{t-1} | x_t, x_0)$ 的精确形式：它是一个均值 $\tilde{\mu}_t(x_t, x_0)$ 和[方差](@entry_id:200758) $\tilde{\beta}_t I$ 的[正态分布](@entry_id:154414)，其中：

$$
\tilde{\mu}_t(x_t, x_0) = \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} x_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} x_0
$$

$$
\tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \beta_t
$$

这个结果至关重要，它揭示了理想的去噪步骤（即[后验均值](@entry_id:173826) $\tilde{\mu}_t$）是 $x_t$ 和 $x_0$ 的一个[线性组合](@entry_id:154743)。[方差](@entry_id:200758) $\tilde{\beta}_t$ 仅依赖于已知的噪声调度，不依赖于数据。

#### [神经网](@entry_id:276355)络的角色与[参数化](@entry_id:272587)选择

既然理想的去噪步骤依赖于未知的 $x_0$，我们就需要一个[神经网](@entry_id:276355)络来从当前的噪声样本 $x_t$ 中预测出这个缺失的信息。这个[神经网](@entry_id:276355)络 $p_\theta(x_{t-1} | x_t)$ 通常被[参数化](@entry_id:272587)为一个高斯分布，其均值由网络预测，[方差](@entry_id:200758)则可以是固定的或可学习的。

$$
p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)
$$

核心任务就是让 $\mu_\theta(x_t, t)$ 尽可能地逼近真实的[后验均值](@entry_id:173826) $\tilde{\mu}_t(x_t, x_0)$。观察 $\tilde{\mu}_t$ 的表达式，我们发现网络本质上需要从 $x_t$ 中估计出 $x_0$。这引出了一个问题：网络应该直接预测 $x_0$，还是预测其他等价的量？

实践中，有三种流行的参数化选择 ：

1.  **预测 $x_0$**：网络直接输出对原始图像的估计 $\hat{x}_0(x_t, t)$。
2.  **预测噪声 $\epsilon$**：网络预测用于生成 $x_t$ 的标准正态噪声 $\hat{\epsilon}(x_t, t)$。这是最常见的选择。
3.  **预测速度 $v$**：网络预测 $x_0$ 和 $\epsilon$ 的一个特定[线性组合](@entry_id:154743)，通常定义为 $v = \sqrt{\bar{\alpha}_t}\epsilon - \sqrt{1-\bar{\alpha}_t}x_0$。

这三种[参数化](@entry_id:272587)是完[全等](@entry_id:273198)价的。给定 $x_t$ 和其中任意一个预测量（例如 $\hat{x}_0$），我们都可以通过简单的代数变换精确地计算出另外两个量。例如，从[前向过程](@entry_id:634012)的定义 $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$ 出发，可以推导出：

$$
\hat{\epsilon}(\hat{x}_0, x_t) = \frac{x_t - \sqrt{\bar{\alpha}_t} \hat{x}_0}{\sqrt{1-\bar{\alpha}_t}} \quad \text{以及} \quad \hat{x}_0(\hat{\epsilon}, x_t) = \frac{x_t - \sqrt{1-\bar{\alpha}_t} \hat{\epsilon}}{\sqrt{\bar{\alpha}_t}}
$$

更进一步，可以证明，对于一个给定的[预测误差](@entry_id:753692)，这三种参数化下的均方误差（MSE）损失之间存在一个仅与 $\bar{\alpha}_t$ 相关的确定性比例关系。例如，$\mathrm{MSE}_\epsilon = \frac{\bar{\alpha}_t}{1-\bar{\alpha}_t} \mathrm{MSE}_{x_0}$。这意味着在一种[参数化](@entry_id:272587)下优化[损失函数](@entry_id:634569)，等价于在另一种参数化下优化一个加权的损失函数。因此，从理论上讲，模型的最优解是相同的，学习动态也是等价的。

### 训练目标：学习去噪的艺术

[扩散模型](@entry_id:142185)的训练目标是最大化数据的[对数似然](@entry_id:273783)的[证据下界](@entry_id:634110)（ELBO），这与[变分自编码器](@entry_id:177996)（VAE）的目标类似。经过简化，这个目标可以近似为在所有时间步 $t$ 上，让模型预测的[分布](@entry_id:182848) $p_\theta(x_{t-1}|x_t)$ 与真实[后验分布](@entry_id:145605) $q(x_{t-1}|x_t, x_0)$ 之间的[KL散度](@entry_id:140001)最小化。

#### 简化为均方误差损失

选择预测噪声 $\epsilon$ 的参数化后，训练目标可以被进一步简化为一个直观的[均方误差损失函数](@entry_id:634102)：

$$
\mathcal{L}(\theta) = \mathbb{E}_{t, x_0, \epsilon} \left[ w(t) \|\epsilon - \hat{\epsilon}_\theta(x_t, t)\|^2 \right]
$$

其中，$t$ 从 $\{1, \dots, T\}$ 中均匀采样，$x_0 \sim p(x_0)$，$\epsilon \sim \mathcal{N}(0, I)$，$x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$。$w(t)$ 是一个时间依赖的权重函数。

这个[损失函数](@entry_id:634569)鼓励[神经网](@entry_id:276355)络 $\hat{\epsilon}_\theta$ 在给定噪声图像 $x_t$ 和时间步 $t$ 的情况下，准确地预测出所添加的噪声 $\epsilon$。

#### 从贝叶斯[去噪](@entry_id:165626)视角理解

为何预测噪声 $\epsilon$ 是一个合理的选择？我们可以从[贝叶斯推断](@entry_id:146958)和信号处理的角度找到答案 。考虑一个简单的[线性高斯模型](@entry_id:268963)，其中观测值 $x_t$ 是“干净信号” $x_0$ 和“噪声” $\epsilon$ 的线性组合。在该模型下，给定观测值 $x_t$ 后，噪声 $\epsilon$ 的后验期望（即最小均方误差估计）可以被精确计算出来：

$$
\mathbb{E}[\epsilon | x_t] = \frac{\sqrt{1-\bar{\alpha}_t}}{\sigma_0^2\bar{\alpha}_t + 1 - \bar{\alpha}_t} x_t
$$

其中 $\sigma_0^2$ 是 $x_0$ 的先验[方差](@entry_id:200758)。这个结果表明，在理想的（高斯）情况下，最优的噪声估计器是 $x_t$ 的一个线性函数。这为使用[神经网](@entry_id:276355)络从 $x_t$ 预测 $\epsilon$ 提供了强有力的理论依据，[神经网](@entry_id:276355)络可以被看作是学习这个（通常是[非线性](@entry_id:637147)的）最优[贝叶斯估计](@entry_id:137133)器。

#### 与[分数匹配](@entry_id:635640)的联系

扩散模型的训练目标与另一类生成模型——基于分数的模型（Score-Based Models）——有着深刻的联系。基于分数的模型旨在学习数据[分布](@entry_id:182848)在不同噪声水平下的**[分数函数](@entry_id:164520)**（score function），即对数[概率密度](@entry_id:175496)的梯度 $\nabla_x \log p_t(x)$。

可以证明，DDPM的 $\epsilon$-预测损失函数等价于一个加权的去噪[分数匹配](@entry_id:635640)（Denoising Score Matching）目标。具体来说，预测噪声 $\epsilon$ 等价于预测[分数函数](@entry_id:164520)，它们之间存在一个简单的换算关系：$\nabla_x \log p_t(x) = -\frac{1}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x, t)$。

因此，DDPM可以被看作是训练分数模型的一种特定实现。不同的训练目标，如切片[分数匹配](@entry_id:635640)（Sliced Score Matching, SSM），虽然在单样本梯度[方差](@entry_id:200758)和收敛效率上可能与标准的 $\epsilon$-回归有所不同，但它们都旨在学习相同的底层分数场 。在连续时间极限下，学习到的[分数函数](@entry_id:164520)可以被用来定义一个时间反向SDE，该SDE能够从噪声生成数据 ：

$$
\mathrm{d}x_t = \left[-\frac{1}{2}\beta(t)x_t - \beta(t) \nabla_x \log p_t(x_t)\right]\mathrm{d}t + \sqrt{\beta(t)}\,\mathrm{d}\bar{w}_t
$$

这个方程明确地将[分数函数](@entry_id:164520) $\nabla_x \log p_t(x_t)$ 与生成过程的“漂移项”联系起来，为生成过程提供了坚实的理论基础。

#### 损失权重 $w(t)$ 的作用

损失函数中的权重项 $w(t)$ 对模型的性能至关重要。由于一个有限容量的[神经网](@entry_id:276355)络无法在所有时间步 $t$ 上都完美地逼近最优的去噪函数，权重 $w(t)$ 决定了模型应该优先在哪些噪声水平上表现得更好 。

*   **常数权重 ($w(t)=1$)**：这种选择平等地对待所有时间步的误差。
*   **基于信噪比的权重**：一些研究发现，对[信噪比](@entry_id:185071)较低的时间步（即噪声较多、t较大时）给予更高的权重，可以提高生成样本的质量。例如，DDPM论文中隐式使用的权重与 $\frac{1}{\sigma_t^2}$ 相关。

不同的加权方案会导致学习到的全局去噪函数 $\hat{\epsilon}_\theta$ 在不同时间步 $t$ 上与该时刻的最优函数 $\hat{\epsilon}_t^*$ 产生不同的偏差。选择合适的权重方案是一个重要的实践考量，它影响着模型最终的生成质量。

### [去噪](@entry_id:165626)网络的设计与采样

最后，我们将探讨去噪[神经网](@entry_id:276355)络本身的一些设计细节以及如何从训练好的模型中进行采样。

#### 模型架构与时间编码

[去噪](@entry_id:165626)网络通常采用[U-Net架构](@entry_id:635581)，这种架构在图像到图像的转换任务中表现出色。一个关键的设计是如何将时间信息 $t$ 输入到网络中。由于不同时间步的[去噪](@entry_id:165626)任务难度和特性不同，模型必须能够根据 $t$ 调整其行为。

*   **[参数共享](@entry_id:634285)与时间嵌入**：一种高效的方法是让整个网络的大部分参数在所有时间步之间共享，然后通过**时间嵌入**（time embedding）将时间信息 $t$ 注入到网络的中间层。通常，时间 $t$ 首先被转换成一个高维向量（例如，使用[正弦位置编码](@entry_id:637792)），然后通过线性层映射并加到[U-Net](@entry_id:635895)的[残差块](@entry_id:637094)中。这种[参数共享](@entry_id:634285)机制极大地提高了模型的参数效率 。相比之下，为每个时间步训练一个独立的模型头（separate heads）虽然在数据充足时可能获得稍好的性能，但其参数量巨大，且无法泛化到训练时未见过的时间步。

#### 反向过程[方差](@entry_id:200758)的学习

在我们的模型 $p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)$ 中，[方差](@entry_id:200758) $\sigma_t^2$ 的选择是一个重要问题。

*   **固定[方差](@entry_id:200758)**：一个简单的选择是将其固定为真实后验[方差](@entry_id:200758) $\tilde{\beta}_t$。这在理论上是合理的，因为它匹配了[目标分布](@entry_id:634522)的[方差](@entry_id:200758)。
*   **学习[方差](@entry_id:200758)**：另一个选择是让[方差](@entry_id:200758)也成为网络的一个输出，或者通过一个插值方案在 $\beta_t$ 和 $\tilde{\beta}_t$ 之间选择。研究表明，学习[方差](@entry_id:200758)可以进一步收紧ELBO，从而可能提高样本的对数似然 。学习[方差](@entry_id:200758)允许模型在均值预测不准时，通过增大生成过程的随机性来弥补，这在实践中可以带来微小的性能提升。

#### 采样过程

一旦模型训练完成，我们就可以通过模拟反向过程来生成样本。这个过程被称为**祖先采样**（ancestral sampling）。

1.  从[先验分布](@entry_id:141376)中采样一个初始噪声 $x_T \sim \mathcal{N}(0, I)$。
2.  对于 $t = T, T-1, \dots, 1$：
    a. 使用[神经网](@entry_id:276355)络计算[去噪](@entry_id:165626)步的均值 $\mu_\theta(x_t, t)$。
    b. 计算[方差](@entry_id:200758) $\sigma_t^2$（固定或学习得到）。
    c. 采样 $x_{t-1} \sim \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)$。
3.  最终得到的 $x_0$ 就是一个生成的样本。

这个过程需要 $T$ 次网络[前向传播](@entry_id:193086)，通常比较耗时。为了加速采样，研究者们提出了多种策略，例如通过跳步来减少采样总步数。这些加速算法（如DDIM）利用了扩散过程的性质，即可以在一个更粗糙的时间网格上定义一个等效的生成过程，而这个过程的[边际分布](@entry_id:264862)与原始的精细过程相匹配 。这允许在生成质量和采样速度之间进行权衡。