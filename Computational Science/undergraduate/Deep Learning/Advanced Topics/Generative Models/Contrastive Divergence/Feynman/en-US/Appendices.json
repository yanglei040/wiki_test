{
    "hands_on_practices": [
        {
            "introduction": "Before we start coding, it's vital to solidify our theoretical grasp of the Contrastive Divergence (CD) algorithm. This practice  challenges you to analyze the behavior of the CD gradient estimator in illustrative cases, particularly when the number of Gibbs steps, $k$, is very small. By working through these scenarios from first principles, you will build a strong intuition for why CD-$k$ is a biased estimator and how this bias can sometimes lead the learning process astray.",
            "id": "3112328",
            "problem": "You are pretraining a Deep Belief Network (DBN) layer by layer using Restricted Boltzmann Machines (RBMs). Each RBM has one visible binary unit $v \\in \\{0,1\\}$, one hidden binary unit $h \\in \\{0,1\\}$, a weight $W \\in \\mathbb{R}$, and biases $b, c \\in \\mathbb{R}$. Training uses Contrastive Divergence (CD) with $k$ Gibbs steps, where the “negative” sample is obtained by running $k$ alternating conditional samplings starting from data $v_0$.\n\nFrom first principles about energy-based models, conditional distributions in binary-binary RBMs, and the definition of Contrastive Divergence, answer the following multiple-choice question. Select all options that are correct. You must justify your choices by reasoning from the definitions (not by citing ready-made training rules).\n\nConsider two toy setups for illustration:\n- Setup S$1$: $b=0$, $c=0$, $W=2$, and a single data case $v=1$.\n- Setup S$2$: $b=0$, $c=-3$, $W=4$, and a data distribution with $P_{\\text{data}}(v=1)=0.5$.\n\nWhich statements are correct?\n\nA. For an RBM trained by Contrastive Divergence (CD), using $k=0$ yields a zero parameter update for $W$ because the contrastive (negative) statistic is computed at the same data point as the data (positive) statistic, so the two sides cancel exactly.\n\nB. In Setup S$1$, CD with $k=0$ produces a nonzero positive update for $W$ strictly proportional to approximately $0.1192$.\n\nC. In Setup S$2$, the true maximum-likelihood gradient with respect to $W$ (averaged over the data distribution and the model) is negative, yet a one-step Contrastive Divergence estimate starting from $v_0=1$ yields a positive update of approximately $0.108$, illustrating that too-small $k$ can point in the wrong direction and, if repeated, can drive parameters to diverge.\n\nD. Using too-small $k$ in CD cannot cause divergence because each CD step strictly increases the log-likelihood.\n\nE. In the limit $k \\to \\infty$, CD-$k$ recovers the exact negative-phase model statistic (under standard mixing assumptions), so the estimator becomes unbiased for the maximum-likelihood gradient in RBMs.",
            "solution": "The problem asks for an evaluation of several statements regarding the training of a Restricted Boltzmann Machine (RBM) using Contrastive Divergence (CD). I will first establish the necessary theoretical background from first principles and then analyze each option.\n\n### Theoretical Foundation\n\nAn RBM with one binary visible unit $v \\in \\{0, 1\\}$ and one binary hidden unit $h \\in \\{0, 1\\}$ is an energy-based model with the energy function:\n$$E(v, h) = -vWh - bv - ch$$\nwhere $W \\in \\mathbb{R}$ is the weight, and $b, c \\in \\mathbb{R}$ are the biases for the visible and hidden units, respectively.\n\nThe joint probability distribution is given by $P(v, h) = \\frac{1}{Z} e^{-E(v, h)}$, where $Z = \\sum_{v', h'} e^{-E(v', h')}$ is the partition function.\n\nThe conditional probabilities, which are essential for Gibbs sampling, are given by the sigmoid function $\\sigma(x) = (1 + e^{-x})^{-1}$:\n$$P(h=1|v) = \\frac{e^{-E(v,h=1)}}{e^{-E(v,h=0)} + e^{-E(v,h=1)}} = \\frac{e^{Wv+c}}{e^0 + e^{Wv+c}} = \\sigma(Wv + c)$$\n$$P(v=1|h) = \\frac{e^{-E(v=1,h)}}{e^{-E(v=0,h)} + e^{-E(v=1,h)}} = \\frac{e^{Wh+b}}{e^0 + e^{Wh+b}} = \\sigma(Wh + b)$$\n\nThe gradient of the log-likelihood of a single data point $v_0$ with respect to the weight $W$ is:\n$$\\frac{\\partial \\log P(v_0)}{\\partial W} = \\mathbb{E}_{h|v_0}\\left[-\\frac{\\partial E(v_0, h)}{\\partial W}\\right] - \\mathbb{E}_{v,h \\sim P}\\left[-\\frac{\\partial E(v, h)}{\\partial W}\\right]$$\nSince $-\\frac{\\partial E}{\\partial W} = vh$, this becomes:\n$$\\frac{\\partial \\log P(v_0)}{\\partial W} = \\mathbb{E}_{h|v_0}[v_0 h] - \\mathbb{E}_{v,h}[vh]$$\nThe first term is the \"positive phase\" statistic, $\\langle vh \\rangle_{\\text{data}}$, and the second is the \"negative phase\" statistic, $\\langle vh \\rangle_{\\text{model}}$. The positive phase can be computed as:\n$$\\langle vh \\rangle_{\\text{data}} = v_0 \\cdot P(h=1|v_0)$$\nThe negative phase is generally intractable. Contrastive Divergence with $k$ steps (CD-$k$) approximates this gradient. It starts a Gibbs chain from the data point $v_0$, runs for $k$ steps ($v_0 \\to h_0 \\to v_1 \\to \\dots \\to v_k$), and uses the final visible state $v_k$ to compute the negative statistic. The CD-$k$ update for $W$ is proportional to:\n$$\\Delta W \\propto \\langle vh \\rangle_{\\text{data}} - \\langle vh \\rangle_{\\text{model}, k} = v_0 P(h=1|v_0) - v_k P(h=1|v_k)$$\nwhere the term involving $v_k$ is an approximation of the negative phase.\n\n### Option-by-Option Analysis\n\n**A. For an RBM trained by Contrastive Divergence (CD), using $k=0$ yields a zero parameter update for $W$ because the contrastive (negative) statistic is computed at the same data point as the data (positive) statistic, so the two sides cancel exactly.**\n\nTo analyze CD with $k=0$, we follow the problem description: \"the 'negative' sample is obtained by running $k$ alternating conditional samplings starting from data $v_0$\". If $k=0$, no sampling steps are performed. Therefore, the state of the Gibbs chain after $0$ steps is simply its initial state, i.e., $v_k = v_0$.\n\nThe CD-$0$ update for $W$ is:\n$$\\Delta W \\propto v_0 P(h=1|v_0) - v_k P(h=1|v_k)$$\nSubstituting $v_k = v_0$:\n$$\\Delta W \\propto v_0 P(h=1|v_0) - v_0 P(h=1|v_0) = 0$$\nThe positive statistic ($v_0 P(h=1|v_0)$) and the negative statistic approximation ($v_0 P(h=1|v_0)$) are identical, resulting in a zero gradient estimate. This holds for any parameters $W, b, c$ and any data point $v_0$. The reasoning provided in the option is correct.\n\n**Verdict on A: Correct**\n\n**B. In Setup S$1$, CD with $k=0$ produces a nonzero positive update for $W$ strictly proportional to approximately $0.1192$.**\n\nThis statement directly contradicts the conclusion from our analysis of Option A. As we have shown, the CD-$0$ update is mathematically guaranteed to be zero for any setup.\nSetup S$1$ has parameters $b=0$, $c=0$, $W=2$, and a data case $v=1$.\nThe CD-$0$ update is:\n$$\\Delta W \\propto v_0 P(h=1|v_0) - v_0 P(h=1|v_0) = 1 \\cdot P(h=1|v=1) - 1 \\cdot P(h=1|v=1) = 0$$\nThe value $0.1192$ is approximately $1 - \\sigma(2) \\approx 1 - 0.8808 = 0.1192$. This value may arise from a misunderstanding of the CD update rule, but under the standard definition provided and used in the field, the update is zero. The statement is therefore false.\n\n**Verdict on B: Incorrect**\n\n**C. In Setup S$2$, the true maximum-likelihood gradient with respect to $W$ (averaged over the data distribution and the model) is negative, yet a one-step Contrastive Divergence estimate starting from $v_0=1$ yields a positive update of approximately $0.108$, illustrating that too-small $k$ can point in the wrong direction and, if repeated, can drive parameters to diverge.**\n\nThis is a two-part claim that requires detailed calculation.\nSetup S$2$: $b=0$, $c=-3$, $W=4$, and $P_{\\text{data}}(v=1)=0.5$.\n\n**Part 1: The true maximum-likelihood (ML) gradient is negative.**\nThe true gradient is $\\frac{\\partial \\mathcal{L}}{\\partial W} = \\mathbb{E}_{v \\sim P_{\\text{data}}}[\\langle vh \\rangle] - \\mathbb{E}_{v,h \\sim P_{\\text{model}}}[\\langle vh \\rangle]$.\n\nPositive Phase Statistic: $\\mathbb{E}_{v \\sim P_{\\text{data}}}[v \\cdot P(h=1|v)]$\n- For $v=1$: $1 \\cdot P(h=1|v=1) = \\sigma(W \\cdot 1 + c) = \\sigma(4-3) = \\sigma(1) \\approx 0.7311$.\n- For $v=0$: $0 \\cdot P(h=1|v=0) = 0$.\nAveraging over $P_{\\text{data}}$: $\\langle vh \\rangle_{\\text{data}} = 0.5 \\cdot \\sigma(1) + 0.5 \\cdot 0 \\approx 0.3655$.\n\nNegative Phase Statistic: $\\mathbb{E}_{v,h \\sim P_{\\text{model}}}[vh] = P_{\\text{model}}(v=1, h=1)$.\nWe need the partition function $Z = \\sum_{v,h} e^{-E(v,h)} = \\sum_{v,h} e^{vWh+bv+ch}$. With $b=0,c=-3,W=4$:\n- $e^{-E(0,0)} = e^0 = 1$.\n- $e^{-E(0,1)} = e^{c} = e^{-3}$.\n- $e^{-E(1,0)} = e^0 = 1$.\n- $e^{-E(1,1)} = e^{W+c} = e^{4-3} = e^{1}$.\nSo, $Z = 1 + e^{-3} + 1 + e^{1} = 2 + e^{-3} + e \\approx 2 + 0.0498 + 2.7183 = 4.7681$.\n$\\langle vh \\rangle_{\\text{model}} = P(v=1, h=1) = \\frac{e^1}{Z} \\approx \\frac{2.7183}{4.7681} \\approx 0.5701$.\n\nTrue ML Gradient: $\\frac{\\partial \\mathcal{L}}{\\partial W} \\approx 0.3655 - 0.5701 = -0.2046$.\nThe gradient is indeed negative.\n\n**Part 2: The CD-$1$ update from $v_0=1$ is positive and $\\approx 0.108$.**\nFor a single data point $v_0=1$, the update is $\\Delta W \\propto v_0 P(h=1|v_0) - \\mathbb{E}_{v_1}[v_1 P(h=1|v_1)]$.\nThe positive statistic is $1 \\cdot P(h=1|v=1) = \\sigma(1) \\approx 0.7311$.\nFor the negative statistic, we perform one Gibbs step from $v_0=1$:\n1. Sample $h_0 \\sim P(h|v_0=1)$. $P(h_0=1|v_0=1) = \\sigma(W \\cdot 1 + c) = \\sigma(1)$.\n2. Sample $v_1 \\sim P(v|h_0)$.\nThe expectation for the negative statistic is taken over this sampling process:\n$\\langle vh \\rangle_{\\text{model},1} = \\mathbb{E}_{h_0 \\sim P(h|v_0=1)} \\left[ \\mathbb{E}_{v_1 \\sim P(v|h_0)}[v_1 P(h=1|v_1)] \\right]$\n- If $h_0=1$ (prob. $\\sigma(1)$): $P(v_1=1|h_0=1) = \\sigma(W \\cdot 1 + b) = \\sigma(4)$. The term is $\\sigma(4) \\cdot (1 \\cdot P(h=1|v=1)) + (1-\\sigma(4)) \\cdot (0) = \\sigma(4)\\sigma(1)$.\n- If $h_0=0$ (prob. $1-\\sigma(1)$): $P(v_1=1|h_0=0) = \\sigma(W \\cdot 0 + b) = \\sigma(0)=0.5$. The term is $0.5 \\cdot (1 \\cdot P(h=1|v=1)) + 0.5 \\cdot (0) = 0.5\\sigma(1)$.\nCombining these:\n$\\langle vh \\rangle_{\\text{model},1} = \\sigma(1) \\cdot [\\sigma(4)\\sigma(1)] + (1-\\sigma(1)) \\cdot [0.5\\sigma(1)]$\n$= \\sigma(1)[\\sigma(1)\\sigma(4) + 0.5(1-\\sigma(1))]$\nUsing $\\sigma(1) \\approx 0.7311$, $\\sigma(4) \\approx 0.9820$, $1-\\sigma(1) \\approx 0.2689$:\n$\\langle vh \\rangle_{\\text{model},1} \\approx 0.7311 \\cdot [0.7311 \\cdot 0.9820 + 0.5 \\cdot 0.2689] = 0.7311 \\cdot [0.7179 + 0.1345] \\approx 0.7311 \\cdot 0.8524 \\approx 0.6232$.\nThe CD-$1$ gradient estimate is: $\\Delta W \\propto \\sigma(1) - 0.6232 \\approx 0.7311 - 0.6232 = 0.1079$.\nThis is approximately $0.108$. The estimate is positive, while the true gradient is negative. This illustrates that a biased, small-$k$ CD update can indeed point in the wrong direction. Such behavior can lead to unstable training or failure to converge to a good solution.\n\n**Verdict on C: Correct**\n\n**D. Using too-small $k$ in CD cannot cause divergence because each CD step strictly increases the log-likelihood.**\n\nThis statement is fundamentally incorrect. The CD-$k$ update vector is not the gradient of the log-likelihood $\\mathcal{L}$. It is an approximation. Taking a step along an approximate gradient direction does not guarantee an increase in the original objective function. As explicitly shown in the analysis of Option C, the CD-1 gradient can point in a direction that has a negative dot product with the true ML gradient. Taking a step in such a direction would *decrease* the log-likelihood. It has been formally shown that CD is not guaranteed to be an ascent direction for the log-likelihood. Therefore, it can lead to oscillatory behavior or divergence, especially for small $k$.\n\n**Verdict on D: Incorrect**\n\n**E. In the limit $k \\to \\infty$, CD-$k$ recovers the exact negative-phase model statistic (under standard mixing assumptions), so the estimator becomes unbiased for the maximum-likelihood gradient in RBMs.**\n\nThe CD-$k$ procedure involves running a Gibbs sampling Markov chain for $k$ steps. Under standard assumptions (ergodicity), as $k \\to \\infty$, the distribution of the state $v_k$ converges to the model's stationary (equilibrium) distribution, $P_{\\text{model}}(v)$, regardless of the starting state $v_0$.\nThe negative phase of the CD-$k$ gradient is computed using samples $v_k$. As $k \\to \\infty$, the expectation of this statistic converges to the expectation under the true model distribution:\n$$\\lim_{k \\to \\infty} \\mathbb{E}[\\langle vh \\rangle_{\\text{model}, k}] = \\mathbb{E}_{v \\sim P_{\\text{model}}}[\\langle vh|v \\rangle] = \\langle vh \\rangle_{\\text{model}}$$\nThe positive phase in CD-$k$ is the exact positive phase of the ML gradient. Since the negative phase converges to the exact negative phase, the full CD-$k$ gradient estimator converges to the true ML gradient:\n$$\\lim_{k \\to \\infty} \\left( \\langle vh \\rangle_{\\text{data}} - \\langle vh \\rangle_{\\text{model}, k} \\right) = \\langle vh \\rangle_{\\text{data}} - \\langle vh \\rangle_{\\text{model}} = \\frac{\\partial \\mathcal{L}}{\\partial W}$$\nAn estimator whose expected value equals the true value of the parameter being estimated is, by definition, an unbiased estimator. Therefore, in the limit $k \\to \\infty$, the CD-$k$ gradient estimator becomes an unbiased estimator of the true maximum-likelihood gradient. The statement is a correct description of the theoretical underpinning of Contrastive Divergence.\n\n**Verdict on E: Correct**",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "The previous exercise highlighted theoretically how a small number of Gibbs steps can produce a misleading gradient. This practice  brings that theory to life through a hands-on coding simulation. You will implement a Restricted Boltzmann Machine (RBM) and train it on a carefully designed dataset, creating a scenario where the simplest approximation, CD-1, demonstrably fails to learn the true data distribution. You will then see how more sophisticated sampling methods, like using a larger $k$ or employing Persistent Contrastive Divergence (PCD), can rescue the model from this failure mode.",
            "id": "3109758",
            "problem": "You are asked to design and implement a complete, runnable program that constructs a synthetic learning scenario for a Restricted Boltzmann Machine (RBM) and demonstrates how the behavior of Contrastive Divergence (CD) depends on the number of Gibbs steps and on the use of Persistent Contrastive Divergence (PCD) with multiple chains. Your implementation must be driven by first principles.\n\nStart from the following fundamental base:\n\n- A Restricted Boltzmann Machine (RBM) defines a joint distribution over binary visible units and binary hidden units via the energy function\n$$\nE(\\mathbf{v},\\mathbf{h}) \\;=\\; -\\mathbf{b}_v^\\top\\mathbf{v} \\;-\\; \\mathbf{b}_h^\\top\\mathbf{h} \\;-\\; \\mathbf{v}^\\top \\mathbf{W}\\,\\mathbf{h},\n$$\nwhere $\\mathbf{v}\\in\\{0,1\\}^{D}$ are visible units, $\\mathbf{h}\\in\\{0,1\\}^{H}$ are hidden units, $\\mathbf{b}_v\\in\\mathbb{R}^{D}$ and $\\mathbf{b}_h\\in\\mathbb{R}^{H}$ are biases, and $\\mathbf{W}\\in\\mathbb{R}^{D\\times H}$ are weights.\n\n- The model distribution is\n$$\np(\\mathbf{v},\\mathbf{h}) \\;=\\; \\frac{1}{Z}\\,\\exp\\!\\big(-E(\\mathbf{v},\\mathbf{h})\\big),\\quad Z \\;=\\; \\sum_{\\mathbf{v}\\in\\{0,1\\}^{D}}\\sum_{\\mathbf{h}\\in\\{0,1\\}^{H}}\\exp\\!\\big(-E(\\mathbf{v},\\mathbf{h})\\big).\n$$\n\n- The conditional distributions factorize as\n$$\np(\\mathbf{h}\\mid \\mathbf{v}) \\;=\\; \\prod_{j=1}^{H}\\sigma\\!\\big(b_{h,j} + \\mathbf{W}_{:,j}^\\top \\mathbf{v}\\big)^{h_j}\\,\\big(1-\\sigma(\\cdot)\\big)^{1-h_j}, \\quad\np(\\mathbf{v}\\mid \\mathbf{h}) \\;=\\; \\prod_{i=1}^{D}\\sigma\\!\\big(b_{v,i} + \\mathbf{W}_{i,:}\\mathbf{h}\\big)^{v_i}\\,\\big(1-\\sigma(\\cdot)\\big)^{1-v_i},\n$$\nwhere $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$ is the logistic sigmoid.\n\n- The gradient of the log-likelihood under an energy-based model equals a data-dependent expectation minus a model-dependent expectation.\n\nYour task is to:\n\n1) Derive the stochastic update rules for $\\mathbf{W}$, $\\mathbf{b}_v$, and $\\mathbf{b}_h$ using the above base and implement training with CD-$k$, where $k$ denotes the number of blocked Gibbs sampling steps initialized at a data point, and Persistent Contrastive Divergence (PCD), where a set of persistent chains are advanced across updates.\n\n2) Construct a synthetic data distribution over $D$ binary visible units that is the equal-probability mixture of two isolated modes. Concretely, set $D=6$ and define two patterns\n$$\n\\mathbf{v}^{(A)} = [1,1,1,0,0,0]^\\top,\\qquad \\mathbf{v}^{(B)} = [0,0,0,1,1,1]^\\top,\n$$\nand a data distribution that places probability $\\frac{1}{2}$ on $\\mathbf{v}^{(A)}$ and $\\frac{1}{2}$ on $\\mathbf{v}^{(B)}$.\n\n3) Implement an RBM with $H=2$ binary hidden units. Initialize parameters from a reproducible small random Gaussian with zero mean. For evaluation, exactly enumerate all $2^{D}$ visible configurations to compute the model distribution $p_\\theta(\\mathbf{v})$ via the free energy\n$$\nF(\\mathbf{v}) \\;=\\; -\\mathbf{b}_v^\\top \\mathbf{v} \\;-\\; \\sum_{j=1}^{H}\\log\\!\\big(1+\\exp\\big(b_{h,j} + \\mathbf{W}_{:,j}^\\top \\mathbf{v}\\big)\\big),\n$$\nand $p_\\theta(\\mathbf{v}) \\propto \\exp\\!\\big(-F(\\mathbf{v})\\big)$ with an exactly computed partition function over $\\mathbf{v}\\in\\{0,1\\}^{D}$.\n\n4) Construct a training schedule that reveals a failure mode of CD-$1$ by using a skewed data curriculum: for a fraction $\\alpha$ of the total updates, present only $\\mathbf{v}^{(A)}$ as the positive phase input, and for the remaining fraction $(1-\\alpha)$ alternate deterministically between $\\mathbf{v}^{(A)}$ and $\\mathbf{v}^{(B)}$. This simulates path dependence and poor mixing conditions. Keep the dataset and curriculum fixed across methods to isolate the effect of the negative phase. Then show that either increasing $k$ (e.g., larger CD-$k$) or using PCD with multiple persistent chains mitigates the failure.\n\n5) Define a quantitative success criterion for the bimodal data: the trained model must allocate non-negligible mass to both true modes. Use the criterion\n$$\np_\\theta\\!\\big(\\mathbf{v}^{(A)}\\big) > \\tau \\quad \\text{and} \\quad p_\\theta\\!\\big(\\mathbf{v}^{(B)}\\big) > \\tau,\n$$\nwith threshold $\\tau = 0.2$. For a unimodal control case (data equal to $\\mathbf{v}^{(A)}$ only), define success as $p_\\theta\\!\\big(\\mathbf{v}^{(A)}\\big) > 0.5$.\n\n6) Implement the following test suite of parameter sets. Each test case trains from the same kind of initialization distribution but with its own random seed for parameters and sampling; the curriculum skew $\\alpha$ is given. For all cases, use $D=6$, $H=2$, a constant learning rate $\\eta$, and exactly one positive sample per update. No physical units apply.\n\n- Case $1$ (boundary/failure construction): method CD-$1$, $k=1$, $\\alpha=0.8$, $\\eta=0.05$, updates $T=400$, initialization seed $s=0$, target data is bimodal.\n- Case $2$ (rescue by larger $k$): method CD-$10$, $k=10$, $\\alpha=0.8$, $\\eta=0.05$, updates $T=400$, initialization seed $s=0$, target data is bimodal.\n- Case $3$ (rescue by PCD): method PCD with $M=10$ persistent chains, one blocked Gibbs step per update, $\\alpha=0.8$, $\\eta=0.05$, updates $T=400$, initialization seed $s=0$, target data is bimodal.\n- Case $4$ (control): method CD-$1$, $k=1$, $\\alpha=1.0$, $\\eta=0.05$, updates $T=400$, initialization seed $s=1$, target data is unimodal at $\\mathbf{v}^{(A)}$.\n\n7) After training each case, compute $p_\\theta\\!\\big(\\mathbf{v}^{(A)}\\big)$ and $p_\\theta\\!\\big(\\mathbf{v}^{(B)}\\big)$ by exact enumeration as specified above, and evaluate the success criterion for that case, yielding a boolean result for each case.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4].\n$$\nEach $\\text{result}_i$ must be the boolean value of the success criterion defined above for that test case. No angles, physical units, or percentages are involved in this problem; all values are dimensionless.",
            "solution": "The problem requires the design and implementation of a computational experiment to demonstrate the behavior of Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) for training a Restricted Boltzmann Machine (RBM). The solution involves deriving the learning rules from first principles, implementing the RBM and training algorithms, and evaluating them on a synthetic dataset designed to highlight specific failure modes.\n\n**1. Theoretical Foundation: RBM Gradient Derivation**\n\nA Restricted Boltzmann Machine is an energy-based model for a joint distribution over visible units $\\mathbf{v} \\in \\{0, 1\\}^D$ and hidden units $\\mathbf{h} \\in \\{0, 1\\}^H$. The energy function is given by:\n$$\nE(\\mathbf{v}, \\mathbf{h}; \\theta) = -\\mathbf{b}_v^\\top\\mathbf{v} - \\mathbf{b}_h^\\top\\mathbf{h} - \\mathbf{v}^\\top\\mathbf{W}\\mathbf{h}\n$$\nwhere the model parameters are $\\theta = \\{\\mathbf{W}, \\mathbf{b}_v, \\mathbf{b}_h\\}$. The joint probability is $p(\\mathbf{v}, \\mathbf{h}) = Z^{-1} \\exp(-E(\\mathbf{v}, \\mathbf{h}))$, where $Z$ is the partition function. The marginal probability of the visible units is $p(\\mathbf{v}) = \\sum_{\\mathbf{h}} p(\\mathbf{v}, \\mathbf{h})$.\n\nThe goal of training is to maximize the log-likelihood of the observed data. For a single data sample $\\mathbf{v}^{(d)}$, the log-likelihood is:\n$$\n\\mathcal{L}(\\theta) = \\log p(\\mathbf{v}^{(d)}) = \\log \\sum_{\\mathbf{h}} p(\\mathbf{v}^{(d)}, \\mathbf{h}) = \\log \\sum_{\\mathbf{h}} \\frac{\\exp(-E(\\mathbf{v}^{(d)}, \\mathbf{h}))}{Z}\n$$\nThis can be expressed using the free energy, $F(\\mathbf{v}) = -\\log \\sum_{\\mathbf{h}} \\exp(-E(\\mathbf{v}, \\mathbf{h}))$, as:\n$$\n\\mathcal{L}(\\theta) = -F(\\mathbf{v}^{(d)}) - \\log Z\n$$\nThe gradient with respect to any parameter $\\phi \\in \\theta$ is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\phi} = -\\frac{\\partial F(\\mathbf{v}^{(d)})}{\\partial \\phi} - \\frac{\\partial \\log Z}{\\partial \\phi}\n$$\nThe two terms can be expressed as expectations. The first term, related to the data, is:\n$$\n-\\frac{\\partial F(\\mathbf{v}^{(d)})}{\\partial \\phi} = \\frac{\\partial}{\\partial \\phi} \\log \\sum_{\\mathbf{h}} e^{-E(\\mathbf{v}^{(d)}, \\mathbf{h})} = \\sum_{\\mathbf{h}} p(\\mathbf{h} | \\mathbf{v}^{(d)}) \\left( -\\frac{\\partial E(\\mathbf{v}^{(d)}, \\mathbf{h})}{\\partial \\phi} \\right) = \\mathbb{E}_{\\mathbf{h} \\sim p(\\cdot|\\mathbf{v}^{(d)})} \\left[ -\\frac{\\partial E(\\mathbf{v}^{(d)}, \\mathbf{h})}{\\partial \\phi} \\right]\n$$\nThis is the \"positive phase\" expectation, taken over the hidden units conditioned on the data.\nThe second term, related to the model, is:\n$$\n-\\frac{\\partial \\log Z}{\\partial \\phi} = -\\frac{1}{Z} \\frac{\\partial Z}{\\partial \\phi} = -\\frac{1}{Z} \\sum_{\\mathbf{v}, \\mathbf{h}} \\frac{\\partial e^{-E(\\mathbf{v}, \\mathbf{h})}}{\\partial \\phi} = \\sum_{\\mathbf{v}, \\mathbf{h}} p(\\mathbf{v}, \\mathbf{h}) \\left( -\\frac{\\partial E(\\mathbf{v}, \\mathbf{h})}{\\partial \\phi} \\right) = \\mathbb{E}_{\\mathbf{v}, \\mathbf{h} \\sim p(\\cdot, \\cdot)} \\left[ -\\frac{\\partial E(\\mathbf{v}, \\mathbf{h})}{\\partial \\phi} \\right]\n$$\nThis is the \"negative phase\" expectation, taken over the full model distribution. Combining them, the gradient is the difference between two expectations:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\phi} = \\mathbb{E}_{\\mathbf{h} \\sim p(\\cdot|\\mathbf{v}^{(d)})} \\left[ -\\frac{\\partial E(\\mathbf{v}^{(d)}, \\mathbf{h})}{\\partial \\phi} \\right] - \\mathbb{E}_{\\mathbf{v}, \\mathbf{h} \\sim p(\\cdot, \\cdot)} \\left[ -\\frac{\\partial E(\\mathbf{v}, \\mathbf{h})}{\\partial \\phi} \\right]\n$$\nThe derivatives of the energy function are simple:\n$\\frac{\\partial E}{\\partial W_{ij}} = -v_i h_j$, $\\frac{\\partial E}{\\partial b_{v,i}} = -v_i$, and $\\frac{\\partial E}{\\partial b_{h,j}} = -h_j$.\nThis yields the exact gradients:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W_{ij}} = \\mathbb{E}_{\\mathbf{h} \\sim p(\\cdot|\\mathbf{v}^{(d)})}[v_i^{(d)} h_j] - \\mathbb{E}_{\\mathbf{v}, \\mathbf{h} \\sim p(\\cdot, \\cdot)}[v_i h_j]\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b_{v,i}} = v_i^{(d)} - \\mathbb{E}_{\\mathbf{v} \\sim p(\\cdot)}[v_i]\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b_{h,j}} = \\mathbb{E}_{\\mathbf{h} \\sim p(\\cdot|\\mathbf{v}^{(d)})}[h_j] - \\mathbb{E}_{\\mathbf{h} \\sim p(\\cdot)}[h_j]\n$$\nThe positive phase expectations are tractable due to the RBM's structure: $p(h_j=1|\\mathbf{v}) = \\sigma(b_{h,j} + \\sum_i W_{ij}v_i)$. The negative phase expectations, however, are intractable as they require summing over all $2^{D+H}$ states.\n\n**2. Algorithmic Design: CD and PCD Approximations**\n\nContrastive Divergence approximates the intractable negative phase expectation.\n\n**Contrastive Divergence (CD-k):**\nThe CD-$k$ algorithm approximates the model expectation by running a Gibbs sampler for $k$ full steps (v-to-h, h-to-v), initialized from a data point $\\mathbf{v}^{(d)}$. Let $\\mathbf{v}^{(0)} = \\mathbf{v}^{(d)}$.\nThe Gibbs chain is:\n$\\mathbf{h}^{(t)} \\sim p(\\mathbf{h} | \\mathbf{v}^{(t)})$\n$\\mathbf{v}^{(t+1)} \\sim p(\\mathbf{v} | \\mathbf{h}^{(t)})$\nAfter $k$ steps, we obtain a sample $\\mathbf{v}^{(k)}$. The pair $(\\mathbf{v}^{(k)}, \\mathbb{E}[\\mathbf{h}|\\mathbf{v}^{(k)}])$ is used to approximate the negative phase statistics. The stochastic gradient update rules with learning rate $\\eta$ become:\n$$\n\\Delta W_{ij} = \\eta \\left( v_i^{(0)} p(h_j=1|\\mathbf{v}^{(0)}) - v_i^{(k)} p(h_j=1|\\mathbf{v}^{(k)}) \\right)\n$$\n$$\n\\Delta b_{v,i} = \\eta \\left( v_i^{(0)} - v_i^{(k)} \\right)\n$$\n$$\n\\Delta b_{h,j} = \\eta \\left( p(h_j=1|\\mathbf{v}^{(0)}) - p(h_j=1|\\mathbf{v}^{(k)}) \\right)\n$$\nwhere $\\mathbf{v}^{(0)}$ is the data sample and $\\mathbf{v}^{(k)}$ is the sample after $k$ Gibbs steps. For small $k$ (e.g., $k=1$), the chain may not have mixed well, and $\\mathbf{v}^{(k)}$ can be close to $\\mathbf{v}^{(0)}$, leading to a biased gradient estimate. This is especially problematic if the modes of the data distribution are far apart.\n\n**Persistent Contrastive Divergence (PCD):**\nPCD addresses the poor mixing of short CD chains. It maintains a set of $M$ persistent \"fantasy\" particles or chains $\\{\\mathbf{v}^{(m)}_{chain}\\}_{m=1}^M$ that are treated as samples from the model distribution. At each training step:\n1. A data sample $\\mathbf{v}^{(d)}$ provides the positive phase statistics.\n2. Each persistent chain $\\mathbf{v}^{(m)}_{chain}$ is updated with one (or more) Gibbs sampling step to yield a new state $\\mathbf{v'}^{(m)}_{chain}$.\n3. The negative phase statistics are computed by averaging over these new states $\\{\\mathbf{v'}^{(m)}_{chain}\\}$.\n4. The parameters are updated using the difference between positive and negative statistics.\n5. The persistent chains are replaced by the new states: $\\mathbf{v}^{(m)}_{chain} \\leftarrow \\mathbf{v'}^{(m)}_{chain}$.\nBecause the chains are not re-initialized from data at each step, they can explore the state space more freely, leading to a better approximation of the model's equilibrium distribution, especially when modes are separated.\n\n**3. Experimental Setup and Evaluation**\n\nThe experiment is designed to create a scenario where CD-$1$ is likely to fail and to show how CD-$k$ (with larger $k$) and PCD can succeed.\n\n- **Data:** A bimodal distribution over $D=6$ visible units. The two modes are $\\mathbf{v}^{(A)} = [1,1,1,0,0,0]^\\top$ and $\\mathbf{v}^{(B)} = [0,0,0,1,1,1]^\\top$, each with probability $1/2$. These vectors are distant in Hamming space, making it difficult for a Gibbs chain to move between them.\n- **Curriculum:** The training is deliberately skewed. For the first $80\\%$ of updates ($\\alpha=0.8$), only $\\mathbf{v}^{(A)}$ is presented. This will cause the RBM to strongly learn the first mode. The remaining $20\\%$ of updates alternate between $\\mathbf{v}^{(A)}$ and $\\mathbf{v}^{(B)}$. For CD-$1$, the Gibbs chain started at $\\mathbf{v}^{(A)}$ or $\\mathbf{v}^{(B)}$ is very unlikely to cross over to the other mode in one step. The model will tend to forget the mode it is not currently seeing, or fail to learn the second mode properly.\n- **Evaluation:** Since $D=6$ is small, we can evaluate the model precisely. We compute the free energy $F(\\mathbf{v})$ for all $2^6=64$ possible visible vectors $\\mathbf{v}$.\n$$\nF(\\mathbf{v}) = -\\mathbf{b}_v^\\top \\mathbf{v} - \\sum_{j=1}^{H} \\log\\left(1 + \\exp\\left(b_{h,j} + \\mathbf{W}_{:,j}^\\top \\mathbf{v}\\right)\\right)\n$$\nThe marginal probability is $p(\\mathbf{v}) = \\frac{\\exp(-F(\\mathbf{v}))}{Z_v}$, where the visible-space partition function $Z_v = \\sum_{\\mathbf{v}'} \\exp(-F(\\mathbf{v}'))$. This $Z_v$ is computed exactly by summing over all $64$ configurations.\n- **Success Criterion:** A trained model is considered successful on the bimodal task if it assigns significant probability to both modes: $p_\\theta(\\mathbf{v}^{(A)}) > 0.2$ and $p_\\theta(\\mathbf{v}^{(B)}) > 0.2$. For the unimodal control case, success is defined as $p_\\theta(\\mathbf{v}^{(A)}) > 0.5$.\n\nThis setup allows for a clear, quantitative comparison between CD-$1$, CD-$10$, and PCD, isolating the effect of the negative phase sampling strategy on the final learned distribution.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Main function to run the RBM training and evaluation experiments.\n    \"\"\"\n    \n    # Define problem constants\n    D = 6  # Number of visible units\n    H = 2  # Number of hidden units\n    V_A = np.array([1, 1, 1, 0, 0, 0])\n    V_B = np.array([0, 0, 0, 1, 1, 1])\n    \n    # Test case parameters\n    test_cases = [\n        {'method': 'cd', 'k': 1, 'alpha': 0.8, 'eta': 0.05, 'T': 400, 'seed': 0, 'data': 'bimodal'},\n        {'method': 'cd', 'k': 10, 'alpha': 0.8, 'eta': 0.05, 'T': 400, 'seed': 0, 'data': 'bimodal'},\n        {'method': 'pcd', 'M': 10, 'k': 1, 'alpha': 0.8, 'eta': 0.05, 'T': 400, 'seed': 0, 'data': 'bimodal'},\n        {'method': 'cd', 'k': 1, 'alpha': 1.0, 'eta': 0.05, 'T': 400, 'seed': 1, 'data': 'unimodal'},\n    ]\n\n    class RBM:\n        def __init__(self, D, H, seed):\n            self.D = D\n            self.H = H\n            self.rng = np.random.default_rng(seed)\n            # Initialize parameters from a small random Gaussian\n            self.W = self.rng.normal(loc=0.0, scale=0.01, size=(D, H))\n            self.b_v = np.zeros(D)\n            self.b_h = np.zeros(H)\n\n        def _sigmoid(self, x):\n            return 1.0 / (1.0 + np.exp(-x))\n\n        def _sample_h_given_v(self, v):\n            \"\"\"Sample hidden units given visible units.\"\"\"\n            h_probs = self._sigmoid(self.b_h + v @ self.W)\n            h_samples = (self.rng.random(self.H)  h_probs).astype(np.float64)\n            return h_probs, h_samples\n\n        def _sample_v_given_h(self, h):\n            \"\"\"Sample visible units given hidden units.\"\"\"\n            v_probs = self._sigmoid(self.b_v + h @ self.W.T)\n            v_samples = (self.rng.random(self.D)  v_probs).astype(np.float64)\n            return v_probs, v_samples\n        \n        def _gibbs_step(self, v):\n            \"\"\"Perform one full Gibbs step.\"\"\"\n            _, h_sample = self._sample_h_given_v(v)\n            _, v_sample = self._sample_v_given_h(h_sample)\n            return v_sample\n\n        def train_cd(self, curriculum, k, eta):\n            \"\"\"Train the RBM using CD-k.\"\"\"\n            for v_pos in curriculum:\n                # Positive phase\n                pos_h_probs, _ = self._sample_h_given_v(v_pos)\n\n                # Negative phase (Gibbs sampling)\n                v_neg = v_pos.copy()\n                for _ in range(k):\n                    v_neg = self._gibbs_step(v_neg)\n                \n                neg_h_probs, _ = self._sample_h_given_v(v_neg)\n\n                # Update parameters\n                self.W += eta * (np.outer(v_pos, pos_h_probs) - np.outer(v_neg, neg_h_probs))\n                self.b_v += eta * (v_pos - v_neg)\n                self.b_h += eta * (pos_h_probs - neg_h_probs)\n\n        def train_pcd(self, curriculum, M, k, eta):\n            \"\"\"Train the RBM using PCD with M persistent chains.\"\"\"\n            # Initialize persistent chains (fantasy particles)\n            chains = self.rng.integers(0, 2, size=(M, self.D)).astype(np.float64)\n\n            for v_pos in curriculum:\n                # Positive phase\n                pos_h_probs, _ = self._sample_h_given_v(v_pos)\n\n                # Negative phase (update persistent chains)\n                for i in range(M):\n                    for _ in range(k): # k steps per update for each chain\n                        chains[i] = self._gibbs_step(chains[i])\n                \n                v_negs = chains\n                neg_h_probs_batch = self._sigmoid(self.b_h + v_negs @ self.W)\n\n                # Update parameters\n                self.W += eta * (np.outer(v_pos, pos_h_probs) - np.mean([np.outer(v_negs[i], neg_h_probs_batch[i]) for i in range(M)], axis=0))\n                self.b_v += eta * (v_pos - np.mean(v_negs, axis=0))\n                self.b_h += eta * (pos_h_probs - np.mean(neg_h_probs_batch, axis=0))\n\n        def evaluate_prob(self, v_target):\n            \"\"\"Compute the exact probability of a visible vector v_target.\"\"\"\n            # Generate all 2^D possible visible vectors\n            num_v_states = 2**self.D\n            all_v = np.zeros((num_v_states, self.D), dtype=int)\n            for i in range(num_v_states):\n                binary_str = format(i, f'0{self.D}b')\n                all_v[i, :] = [int(b) for b in binary_str]\n\n            # Compute free energy for all visible states\n            # F(v) = -b_v^T v - sum_j log(1 + exp(b_h_j + W_j^T v))\n            log_1_plus_exp_term = np.log(1 + np.exp(self.b_h + all_v @ self.W))\n            free_energies = -all_v @ self.b_v - np.sum(log_1_plus_exp_term, axis=1)\n\n            # Compute log partition function using log-sum-exp for stability\n            log_Z_v = logsumexp(-free_energies)\n\n            # Compute log probability of the target vector\n            log_1_plus_exp_target = np.log(1 + np.exp(self.b_h + v_target @ self.W))\n            F_target = -v_target @ self.b_v - np.sum(log_1_plus_exp_target)\n            log_prob_target = -F_target - log_Z_v\n\n            return np.exp(log_prob_target)\n\n    results = []\n    \n    for case in test_cases:\n        # 1. Initialize RBM with the specified seed\n        rbm = RBM(D, H, seed=case['seed'])\n        \n        # 2. Construct the training curriculum\n        T = case['T']\n        alpha = case['alpha']\n        num_skewed_updates = int(alpha * T)\n        num_alternating_updates = T - num_skewed_updates\n        \n        curriculum = []\n        if case['data'] == 'unimodal' or num_skewed_updates > 0:\n            curriculum.extend([V_A] * num_skewed_updates)\n            \n        if case['data'] == 'bimodal' and num_alternating_updates > 0:\n            for i in range(num_alternating_updates):\n                curriculum.append(V_A if i % 2 == 0 else V_B)\n        \n        # 3. Train the RBM\n        if case['method'] == 'cd':\n            rbm.train_cd(curriculum, k=case['k'], eta=case['eta'])\n        elif case['method'] == 'pcd':\n            rbm.train_pcd(curriculum, M=case['M'], k=case['k'], eta=case['eta'])\n            \n        # 4. Evaluate the trained model\n        prob_A = rbm.evaluate_prob(V_A)\n        prob_B = rbm.evaluate_prob(V_B)\n        \n        # 5. Check success criterion\n        success = False\n        if case['data'] == 'bimodal':\n            # Bimodal success: both modes have significant probability\n            if prob_A > 0.2 and prob_B > 0.2:\n                success = True\n        elif case['data'] == 'unimodal':\n            # Unimodal success: the single mode has high probability\n            if prob_A > 0.5:\n                success = True\n                \n        results.append(success)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "We have established that the choice of $k$, the number of Gibbs steps, is critical for successful RBM training. While using a large, fixed $k$ can be effective, it is also computationally expensive. This advanced practice  guides you in developing a more intelligent, adaptive strategy where the algorithm adjusts $k$ on the fly. You will implement a control policy that monitors the Gibbs sampling chain for signs of poor mixing and automatically increases $k$, offering a practical solution to a key challenge in training energy-based models.",
            "id": "3109677",
            "problem": "You are asked to design and implement an adaptive Contrastive Divergence (CD) control policy for training a Restricted Boltzmann Machine (RBM), and to test the stability of this policy under different parameter settings.\n\nThe fundamental base for this problem is the definition of a Restricted Boltzmann Machine (RBM) with binary visible and binary hidden units. An RBM defines a joint distribution over a visible vector $v \\in \\{0,1\\}^{n_v}$ and a hidden vector $h \\in \\{0,1\\}^{n_h}$ via the energy function\n$$\nE(v,h) = - v^\\top W h - b^\\top v - c^\\top h,\n$$\nwhere $W \\in \\mathbb{R}^{n_v \\times n_h}$ are the weights, $b \\in \\mathbb{R}^{n_v}$ are visible biases, and $c \\in \\mathbb{R}^{n_h}$ are hidden biases. The distribution is given by\n$$\np(v,h) = \\frac{1}{Z} \\exp\\left(-E(v,h)\\right),\n$$\nwhere $Z$ is the partition function. The gradient of the log-likelihood with respect to parameters involves a difference of expectations under the data distribution and the model distribution. Contrastive Divergence (CD) approximates the intractable model expectation using a $k$-step Gibbs sampling chain that starts at the data, where $k \\in \\mathbb{N}$ is the number of Gibbs steps.\n\nYou must design an adaptive control policy for Contrastive Divergence where the number of Gibbs steps $k$ is increased when the estimated lag-$1$ autocorrelation $\\hat{\\rho}$ of the sequence of negative-phase visible samples $v^{(t)}$ exceeds a threshold. Concretely, at training iteration $t$, let $v^{(t)}$ denote the negative-phase visible sample obtained after running the current $k$-step Gibbs chain. Define a scalar summary $s_t$ as the mean activity of the visible sample,\n$$\ns_t = \\frac{1}{n_v \\cdot n_b} \\sum_{i=1}^{n_b} \\sum_{j=1}^{n_v} v^{(t)}_{i,j},\n$$\nwhere $n_b$ is the batch size and $n_v$ is the number of visible units. Maintain a sliding window of the last $W$ values $\\{s_{t-W+1}, \\dots, s_t\\}$ and estimate the lag-$1$ autocorrelation as\n$$\n\\hat{\\rho} = \\frac{\\sum_{\\ell=2}^{L} (s_\\ell - \\bar{s})(s_{\\ell-1} - \\bar{s})}{\\sum_{\\ell=1}^{L} (s_\\ell - \\bar{s})^2},\n$$\nwhere $L = \\min(W, t)$ and $\\bar{s}$ is the average of the window values. If the denominator is $0$, define $\\hat{\\rho} = 0$.\n\nThe control policy is:\n- Initialize $k$ to $k_0$.\n- At each iteration $t$, after computing $v^{(t)}$ and $\\hat{\\rho}$, if $\\hat{\\rho}  \\tau$, set $k \\leftarrow \\min(k+1, \\bar{k})$, otherwise leave $k$ unchanged. Here $\\tau$ is the autocorrelation threshold and $\\bar{k}$ is a maximum cap on $k$.\n\nThe training loop uses standard CD updates for the RBM parameters with a fixed learning rate and batch size. You must evaluate the stability of the control policy defined above. A policy is considered stable if, over the final $M$ iterations of training, the value of $k$ remains constant. Formally, let $\\{k^{(1)}, k^{(2)}, \\dots, k^{(T)}\\}$ denote the sequence of $k$ values across $T$ iterations. Define stability as:\n$$\n\\text{stable} = \\left(\\max\\{k^{(T-M+1)}, \\dots, k^{(T)}\\} - \\min\\{k^{(T-M+1)}, \\dots, k^{(T)}\\} = 0\\right).\n$$\n\nImplement a program that:\n- Constructs a binary RBM with $n_v = 6$ visible units and $n_h = 4$ hidden units.\n- Trains it on a synthetic binary dataset generated from a mixture of two Bernoulli product distributions for $N = 500$ samples, with mixing weight $0.5$ and distinct, fixed probability vectors for the two modes. Use a mini-batch size of $n_b = 50$, learning rate $\\alpha = 0.05$, and initialize weights and biases to small random values or zeros where appropriate.\n- Runs the adaptive CD policy as described for $T$ iterations per test case.\n- Computes the lag-$1$ autocorrelation estimate $\\hat{\\rho}$ over the last $W$ values of $s_t$ at each iteration and adjusts $k$ according to the policy.\n\nTest suite:\n- Use the following four parameter sets $(\\tau, W, \\bar{k}, T)$ with $k_0 = 1$, $M = 20$ and minimum $k$ fixed at $1$ for all cases:\n    1. $(0.40, 20, 10, 250)$ representing a typical case with moderate threshold.\n    2. $(0.05, 5, 8, 250)$ representing a low threshold and small window that should trigger frequent increases but likely saturate early.\n    3. $(0.80, 50, 6, 250)$ representing a high threshold and large window that should rarely trigger increases.\n    4. $(0.05, 5, 100, 80)$ representing a low threshold with a large maximum $\\bar{k}$ and shorter training horizon, potentially preventing settlement.\n\nFor each test case, your program must output a boolean indicating stability as defined above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[result_1,result_2,result_3,result_4]$. There are no physical units or angle units involved in this problem, and numerical outputs should be boolean values only.",
            "solution": "The user's request is to design, implement, and test an adaptive control policy for the number of Gibbs sampling steps, $k$, in the Contrastive Divergence (CD-k) algorithm used for training a Restricted Boltzmann Machine (RBM). The stability of this policy will be evaluated under several parameter configurations.\n\nFirst, we formalize the components of the problem. A Restricted Boltzmann Machine with binary visible units $v \\in \\{0,1\\}^{n_v}$ and binary hidden units $h \\in \\{0,1\\}^{n_h}$ defines a probability distribution through an energy function $E(v,h) = -v^\\top W h - b^\\top v - c^\\top h$, where $W$, $b$, and $c$ are the weight matrix, visible biases, and hidden biases, respectively. The joint probability is $p(v,h) = Z^{-1} \\exp(-E(v,h))$, where $Z$ is the partition function. Training an RBM involves updating its parameters to maximize the log-likelihood of the data. The gradient of the log-likelihood for a data sample $v_{data}$ is $\\Delta\\theta \\propto \\mathbb{E}_{h|v_{data}}[\\nabla_\\theta(-E(v_{data},h))] - \\mathbb{E}_{v,h}[\\nabla_\\theta(-E(v,h))]$. The second term, the model expectation, is intractable. The CD-k algorithm approximates this term by running a $k$-step Gibbs sampling chain, starting from a data sample $v^{(0)}=v_{data}$, to produce a \"negative phase\" sample $v^{(k)}$. The parameter updates are then based on the difference between statistics computed on the data (the \"positive phase\") and the sample $v^{(k)}$.\n\nThe core of the problem is the adaptive control policy for $k$. This policy aims to ensure the Gibbs chain mixes well by monitoring the autocorrelation of its samples. At each training iteration $t$, we compute a scalar summary of the negative-phase visible samples $v^{(t)}$, defined as their mean activity $s_t = (n_v n_b)^{-1} \\sum_{i,j} v^{(t)}_{i,j}$, where $n_b$ is the batch size. Using a sliding window of the last $W$ values of these summaries, $\\{s_{t-W+1}, \\dots, s_t\\}$, we estimate the lag-$1$ autocorrelation $\\hat{\\rho}$. The formula provided is $\\hat{\\rho} = (\\sum_{\\ell=2}^{L} (s_\\ell - \\bar{s})(s_{\\ell-1} - \\bar{s})) / (\\sum_{\\ell=1}^{L} (s_\\ell - \\bar{s})^2)$, where $L$ is the number of points in the window and $\\bar{s}$ is their mean. A high value of $\\hat{\\rho}$ indicates poor mixing of the Gibbs chain. The control policy is as follows: if $\\hat{\\rho}$ exceeds a threshold $\\tau$, the number of Gibbs steps is incremented, $k \\leftarrow \\min(k+1, \\bar{k})$, where $\\bar{k}$ is a predefined maximum. Otherwise, $k$ remains unchanged. The initial value is $k=k_0$.\n\nThe implementation will begin by generating the synthetic dataset as specified. A total of $N=500$ samples for an RBM with $n_v=6$ visible units are drawn from a mixture of two Bernoulli product distributions with a mixing weight of $0.5$. We define two distinct probability vectors, for example $p_1 = [0.9, 0.9, 0.9, 0.1, 0.1, 0.1]$ and $p_2 = [0.1, 0.1, 0.1, 0.9, 0.9, 0.9]$, to create two well-separated modes in the data distribution. This dataset will be fixed across all test cases to ensure a fair comparison.\n\nThe simulation of the RBM training proceeds for each test case. For each case, the RBM parameters ($W, b, c$) are re-initialized using a fixed random seed to ensure reproducibility. The weights $W$ are drawn from a zero-mean normal distribution with a small standard deviation (e.g., $0.01$), and biases $b, c$ are initialized to zero. The training loop runs for $T$ iterations. In each iteration, a mini-batch of $n_b=50$ samples is drawn from the dataset. The CD-k update is performed using the current value of $k$. This involves:\n1.  **Positive Phase**: For the data batch $v_{data}$, compute the conditional probabilities of the hidden units, $p(h|v_{data})$.\n2.  **Negative Phase**: Starting from $v_{data}$, run a Gibbs sampling chain for $k$ steps. Each step consists of sampling hidden units given visible, $h \\sim p(h|v)$, followed by sampling visible units given hidden, $v \\sim p(v|h)$. This produces the negative-phase samples $v_{neg}$.\n3.  **Parameter Update**: The parameters $W, b, c$ are updated using the gradients approximated from the positive and negative phase statistics, scaled by a learning rate $\\alpha=0.05$.\n\nAfter the parameter update, the adaptive policy for $k$ is executed. The mean activity $s_t$ of the new negative-phase samples $v_{neg}$ is calculated and stored. The autocorrelation $\\hat{\\rho}$ is computed from the history of $s_t$. Based on the comparison of $\\hat{\\rho}$ with the threshold $\\tau$, the value of $k$ for the next iteration is determined and recorded.\n\nFinally, after all $T$ iterations for a given test case are complete, we evaluate the stability of the policy. The policy is deemed stable if the value of $k$ did not change during the final $M=20$ iterations of training. This is formally checked by verifying if the maximum and minimum values of $k$ in the recorded history for iterations $\\{T-M+1, \\dots, T\\}$ are equal. The boolean result (stable or not stable) is recorded for each of the four specified test cases. The entire process is encapsulated in a program that outputs these boolean results as a comma-separated list.",
            "answer": "```python\nimport numpy as np\n\ndef sigmoid(x):\n    \"\"\"Numerically stable sigmoid function.\"\"\"\n    # Clip to avoid overflow in exp for very negative x\n    # which can happen with unstable parameter updates.\n    # While numpy handles it, explicit clipping is safer.\n    x = np.clip(x, -500, 500)\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef solve():\n    \"\"\"\n    Designs, implements, and tests an adaptive Contrastive Divergence (CD) control policy\n    for training a Restricted Boltzmann Machine (RBM).\n    \"\"\"\n\n    # Test cases parameters: (tau, W_window, k_bar, T)\n    test_cases = [\n        (0.40, 20, 10, 250),\n        (0.05, 5, 8, 250),\n        (0.80, 50, 6, 250),\n        (0.05, 5, 100, 80)\n    ]\n\n    # RBM and Training constants\n    n_v = 6\n    n_h = 4\n    N = 500\n    n_b = 50\n    alpha = 0.05\n    k_0 = 1\n    M = 20\n\n    # Use fixed seeds for reproducibility of data generation and model training\n    DATA_SEED = 123\n    MODEL_SEED = 456\n\n    # --- Step 1: Generate Synthetic Data ---\n    # This is done once for all test cases.\n    rng_data = np.random.default_rng(DATA_SEED)\n    p1 = np.array([0.9, 0.9, 0.9, 0.1, 0.1, 0.1])\n    p2 = np.array([0.1, 0.1, 0.1, 0.9, 0.9, 0.9])\n    data1 = rng_data.binomial(1, p1, size=(N // 2, n_v))\n    data2 = rng_data.binomial(1, p2, size=(N - (N // 2), n_v))\n    data = np.vstack([data1, data2]).astype(np.float64)\n\n    results = []\n\n    # --- Step 2: Run simulation for each test case ---\n    for case in test_cases:\n        tau, W_window, k_bar, T = case\n\n        # Reset model and training state for each case to ensure fair comparison\n        rng_model = np.random.default_rng(MODEL_SEED)\n        \n        # Shuffle a copy of the data for this run\n        current_data = data.copy()\n        rng_model.shuffle(current_data)\n\n        # RBM parameters\n        W_weights = rng_model.normal(loc=0.0, scale=0.01, size=(n_v, n_h))\n        b_bias = np.zeros(n_v)\n        c_bias = np.zeros(n_h)\n\n        # Policy state\n        k = k_0\n        s_history = []\n        k_history = []\n        \n        num_batches = N // n_b\n\n        # --- Step 3: Training Loop ---\n        for t in range(1, T + 1):\n            # Mini-batch selection\n            batch_idx = (t - 1) % num_batches\n            v_data = current_data[batch_idx * n_b : (batch_idx + 1) * n_b]\n\n            # --- CD-k step ---\n            # Positive phase\n            prob_h_pos = sigmoid(v_data @ W_weights + c_bias)\n            \n            # Negative phase (Gibbs sampling)\n            v_neg = v_data\n            for _ in range(k):\n                # sample h from v\n                prob_h_neg_gibbs = sigmoid(v_neg @ W_weights + c_bias)\n                h_neg_gibbs = (rng_model.random(size=prob_h_neg_gibbs.shape)  prob_h_neg_gibbs).astype(np.float64)\n                # sample v from h\n                prob_v_neg_gibbs = sigmoid(h_neg_gibbs @ W_weights.T + b_bias)\n                v_neg = (rng_model.random(size=prob_v_neg_gibbs.shape)  prob_v_neg_gibbs).astype(np.float64)\n            \n            prob_h_neg = sigmoid(v_neg @ W_weights + c_bias)\n            \n            # Parameter updates\n            grad_W = (v_data.T @ prob_h_pos - v_neg.T @ prob_h_neg) / n_b\n            grad_b = np.mean(v_data - v_neg, axis=0)\n            grad_c = np.mean(prob_h_pos - prob_h_neg, axis=0)\n            \n            W_weights += alpha * grad_W\n            b_bias += alpha * grad_b\n            c_bias += alpha * grad_c\n            \n            # Log current k *before* updating it for the next step\n            k_history.append(k)\n\n            # --- Adaptive policy logic ---\n            s_t = np.mean(v_neg)\n            s_history.append(s_t)\n            if len(s_history) > W_window:\n                s_history.pop(0)\n            \n            # Compute autocorrelation\n            rho_hat = 0.0\n            current_window = np.array(s_history)\n            L = len(current_window)\n            if L > 1:\n                mean_s = np.mean(current_window)\n                devs = current_window - mean_s\n                denominator = np.sum(devs**2)\n                if denominator > 1e-9: # Avoid division by zero for constant sequence\n                    numerator = np.sum(devs[1:] * devs[:-1])\n                    rho_hat = numerator / denominator\n            \n            # Update k for the *next* iteration\n            if rho_hat > tau:\n                k = min(k + 1, k_bar)\n\n        # --- Step 4: Stability Check ---\n        if T >= M:\n            last_M_ks = k_history[-M:]\n            is_stable = (np.max(last_M_ks) - np.min(last_M_ks) == 0)\n        else:\n            # If total iterations T is less than M, stability cannot be properly evaluated\n            # as the window of size M doesn't exist. The problem sets T >= M.\n            is_stable = False\n            \n        results.append(is_stable)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(lambda r: str(r).lower(), results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}