{
    "hands_on_practices": [
        {
            "introduction": "在设计任何复杂的深度学习模型时，理解其架构和参数构成是第一步。这个练习将带你深入探究条件生成对抗网络（cGAN）的一个关键组件：条件批量归一化（Conditional Batch Normalization）。通过推导生成器中用于条件化的参数总数，你将掌握分析模型容量和计算复杂度的基本技能。这个看似简单的计算练习  是连接理论与实践的桥梁，有助于你更深刻地理解条件信息是如何注入并贯穿整个生成网络的。",
            "id": "3108910",
            "problem": "考虑一个条件生成对抗网络（cGAN）中的生成器，其中每个归一化操作都使用条件批量归一化（BN）。对于 $L$ 个层中的每一层（索引为 $\\ell \\in \\{1,2,\\dots,L\\}$），特征图有 $C_{\\ell}$ 个通道，且BN仿射参数不是固定的；相反，它们是条件变量 $y$ 的函数。具体来说，对于每一层 $\\ell$，缩放和平移分别是向量 $\\gamma_{\\ell}(y) \\in \\mathbb{R}^{C_{\\ell}}$ 和 $\\beta_{\\ell}(y) \\in \\mathbb{R}^{C_{\\ell}}$。条件变量 $y$ 通过某种嵌入机制映射到一个嵌入 $e(y) \\in \\mathbb{R}^{d}$（不计算嵌入本身的参数）。\n\n假设对于每一层 $\\ell$，$\\gamma_{\\ell}(y)$ 和 $\\beta_{\\ell}(y)$ 都被参数化为嵌入 $e(y)$ 的仿射函数，并由独立的全连接层实现。也就是说，$\\gamma_{\\ell}(y)$ 和 $\\beta_{\\ell}(y)$ 中的每一个都是由一个从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{C_{\\ell}}$ 的学习到的仿射映射生成的。\n\n从一个从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{m}$ 的仿射映射的参数数量等于其权重矩阵中的条目数加上其偏置向量中的条目数这一定义出发，推导出一个闭式解析表达式，用于表示在所有 $L$ 层中用于生成条件BN参数 $\\{\\gamma_{\\ell}(y), \\beta_{\\ell}(y)\\}_{\\ell=1}^{L}$ 的总学习参数数量。请用 $L$, $\\{C_{\\ell}\\}_{\\ell=1}^{L}$ 和 $d$ 将您的最终答案表示为单个解析表达式。",
            "solution": "问题要求推导一个闭式解析表达式，用于表示在生成器网络的所有 $L$ 层中，生成条件批量归一化（BN）参数（特别是缩放参数 $\\gamma_{\\ell}(y)$ 和平移参数 $\\beta_{\\ell}(y)$）所使用的总学习参数数量。\n\n首先，我们必须根据问题中的定义，确定单个仿射映射的参数数量。一个从向量空间 $\\mathbb{R}^{d}$ 到另一个向量空间 $\\mathbb{R}^{m}$ 的仿射映射是形如 $f(x) = Wx + b$ 的函数，其中 $x \\in \\mathbb{R}^{d}$，$W$ 是一个权重矩阵，$b \\in \\mathbb{R}^{m}$ 是一个偏置向量。为了维度一致，矩阵 $W$ 的大小必须是 $m \\times d$。此映射中的参数数量是 $W$ 中元素数量和 $b$ 中元素数量的总和。\n权重矩阵 $W$ 中的元素数量是 $m \\times d$。\n偏置向量 $b$ 中的元素数量是 $m$。\n因此，从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{m}$ 的单个仿射映射的总参数数量为 $md + m$，可以因式分解为 $m(d+1)$。\n\n问题陈述，对于每一层 $\\ell \\in \\{1, 2, \\dots, L\\}$，生成器有一个包含 $C_{\\ell}$ 个通道的特征图。该层的条件BN参数 $\\gamma_{\\ell}(y) \\in \\mathbb{R}^{C_{\\ell}}$ 和 $\\beta_{\\ell}(y) \\in \\mathbb{R}^{C_{\\ell}}$ 是由条件嵌入 $e(y) \\in \\mathbb{R}^{d}$ 生成的。\n\n我们来分析单个层 $\\ell$ 的参数数量。\n\n1.  **缩放向量 $\\gamma_{\\ell}(y)$ 的参数：**\n    问题明确指出，$\\gamma_{\\ell}(y)$ 是由一个从嵌入空间 $\\mathbb{R}^{d}$ 到通道空间 $\\mathbb{R}^{C_{\\ell}}$ 的学习到的仿射映射生成的。\n    在这里，输入维度是 $d$（来自 $e(y)$），输出维度是 $m = C_{\\ell}$（因为 $\\gamma_{\\ell}(y)$ 是一个长度为 $C_{\\ell}$ 的向量）。\n    使用我们关于仿射映射参数数量的公式，生成 $\\gamma_{\\ell}(y)$ 的参数数量（我们称之为 $N_{\\gamma, \\ell}$）是：\n    $$N_{\\gamma, \\ell} = C_{\\ell}d + C_{\\ell}$$\n\n2.  **平移向量 $\\beta_{\\ell}(y)$ 的参数：**\n    类似地，$\\beta_{\\ell}(y)$ 是由一个从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{C_{\\ell}}$ 的独立学习到的仿射映射生成的。其结构与 $\\gamma_{\\ell}(y)$ 的结构相同。\n    输入维度是 $d$，输出维度是 $m = C_{\\ell}$。\n    生成 $\\beta_{\\ell}(y)$ 的参数数量（我们称之为 $N_{\\beta, \\ell}$）是：\n    $$N_{\\beta, \\ell} = C_{\\ell}d + C_{\\ell}$$\n\n单个层 $\\ell$ 中条件BN的总参数数量（记为 $N_{\\ell}$）是 $\\gamma_{\\ell}(y)$ 和 $\\beta_{\\ell}(y)$ 的参数之和。问题陈述这些是由*独立*的全连接层实现的，所以我们将它们的参数数量相加。\n$$N_{\\ell} = N_{\\gamma, \\ell} + N_{\\beta, \\ell} = (C_{\\ell}d + C_{\\ell}) + (C_{\\ell}d + C_{\\ell})$$\n$$N_{\\ell} = 2C_{\\ell}d + 2C_{\\ell}$$\n该表达式可以因式分解为：\n$$N_{\\ell} = 2C_{\\ell}(d+1)$$\n\n最后，为了求出生成器所有 $L$ 层中的总学习参数数量 $N_{\\text{total}}$，我们必须将每一层（从 $\\ell=1$ 到 $L$）的参数 $N_{\\ell}$ 相加。问题陈述我们不应计算嵌入机制 $e(y)$ 本身的参数。\n$$N_{\\text{total}} = \\sum_{\\ell=1}^{L} N_{\\ell}$$\n代入 $N_{\\ell}$ 的表达式：\n$$N_{\\text{total}} = \\sum_{\\ell=1}^{L} 2C_{\\ell}(d+1)$$\n项 $2(d+1)$ 相对于求和索引 $\\ell$ 是一个常数，可以从求和中提出来。\n$$N_{\\text{total}} = 2(d+1) \\sum_{\\ell=1}^{L} C_{\\ell}$$\n这就是在所有 $L$ 层中用于生成条件BN参数的总学习参数数量的闭式解析表达式。它用给定的量 $L$、$\\{C_{\\ell}\\}_{\\ell=1}^{L}$ 和 $d$ 表示。",
            "answer": "$$\n\\boxed{2(d+1) \\sum_{\\ell=1}^{L} C_{\\ell}}\n$$"
        },
        {
            "introduction": "一个训练好的生成器不仅要能生成逼真的数据，还应能理解条件变量的内在结构。本练习  引导我们超越简单的对抗性损失，通过引入平滑性正则化项来主动控制生成器的行为。你将学习如何通过修改损失函数，来约束生成器在连续条件（如角度）下的输出流形，从而提高其泛化和外推能力。这个实践将理论（正则化）、数学（最小二乘法）和编程实现紧密结合，让你亲手打造一个行为更可控的生成器。",
            "id": "3108842",
            "problem": "要求您设计并分析一个简化的条件生成对抗网络 (cGAN)，其条件为连续的角度变量。目标是基于第一性原理进行设计，形式化生成器关于条件变量的平滑度正则化，并实证检验在训练范围之外的外推能力。所有角度均须以弧度为单位。\n\n考虑一个条件生成对抗网络 (cGAN)，其生成器定义为函数 $G:\\mathbb{R}^d\\times\\mathbb{R}\\to\\mathbb{R}^2$，其中条件变量 $y\\in\\mathbb{R}$ 被解释为角度，噪声 $z\\in\\mathbb{R}^d$ 是标准正态分布。真实数据分布是一个条件分布 $p_{\\text{data}}(x\\mid y)$，其条件均值为 $m(y)=[\\cos(y),\\sin(y)]^\\top$，条件协方差为 $\\sigma^2 I_2$，其中 $\\sigma>0$ 为固定值。角度是连续的，条件空间是实数轴。\n\n您将分析一种受限的生成器类别，其形式为\n$$\nG(z,y;\\theta)=u(y)+s\\,z,\\quad z\\sim\\mathcal{N}(0,I_2),\n$$\n其中 $u:\\mathbb{R}\\to\\mathbb{R}^2$ 是一个确定性均值映射，$s\\ge 0$ 是一个标量。假设 $u(y)$ 由一个关于 $y$ 的 2 次多项式基进行参数化，即对于 $\\phi(y)=[1,\\,y,\\,y^2]^\\top\\in\\mathbb{R}^3$，存在一个矩阵 $W\\in\\mathbb{R}^{2\\times 3}$ 使得\n$$\nu(y)=W\\,\\phi(y).\n$$\n令平滑度惩罚项为生成器雅可比矩阵关于 $y$ 的确定性均值部分的 $\\ell_2$ 范数的平方：\n$$\n\\mathcal{S}(W)=\\mathbb{E}_{y\\sim p_{\\text{train}}}\\left[\\left\\|\\frac{\\partial u}{\\partial y}(y)\\right\\|_2^2\\right],\n$$\n其中 $\\frac{\\partial u}{\\partial y}(y)=W\\,\\phi'(y)$ 且 $\\phi'(y)=\\frac{d}{dy}\\phi(y)=[0,\\,1,\\,2y]^\\top$。\n\n您将使用训练角度的均匀网格上的经验平均来近似期望。定义一个经验目标函数，该函数平衡了均值匹配、协方差匹配和平滑度：\n$$\n\\mathcal{L}(W,s)=\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi(y_i)-m(y_i)\\right\\|_2^2\\;+\\;\\lambda_{\\text{smooth}}\\cdot\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi'(y_i)\\right\\|_2^2\\;+\\;\\lambda_{\\text{cov}}\\cdot\\left\\|s^2 I_2-\\sigma^2 I_2\\right\\|_F^2,\n$$\n其中 $y_i$ 是训练角度，$N$ 是训练角度的数量，$\\lambda_{\\text{smooth}}\\ge 0$ 是平滑度权重，$\\lambda_{\\text{cov}}\\ge 0$ 是协方差匹配权重。\n\n任务：\n- 从条件生成对抗网络 (cGAN)、经验风险最小化以及上述平滑度惩罚项的定义出发，推导必要的优化条件，并提供一种计算方法来获得最小化 $\\mathcal{L}(W,s)$ 的最优参数 $W^\\star$ 和 $s^\\star$。\n- 实现一个程序，该程序：\n  - 在指定范围内构建均匀网格作为训练角度集。\n  - 基于带导数正则化的最小二乘法第一性原理计算 $W^\\star$ 和 $s^\\star$。\n  - 在指定的测试角度上评估拟合的均值映射 $u^\\star(y)=W^\\star\\phi(y)$，并报告与目标 $m(y)$ 相比的均方误差 (MSE)，该误差在所有测试角度和两个输出维度上取平均。\n- 使用 $\\sigma=0.2$，$\\lambda_{\\text{cov}}=1$，以及在每个指定范围内（包含端点）的 $N=101$ 个等距分布的训练角度。所有角度均以弧度为单位。\n\n测试套件：\n- 情况 1（一般情况）：训练范围 $[-\\pi/2,\\;\\pi/2]$，$\\lambda_{\\text{smooth}}=0.1$，测试角度 $\\{-\\pi/2,\\;0,\\;\\pi/2,\\;3\\pi/4\\}$。\n- 情况 2（无平滑度）：训练范围 $[-\\pi/2,\\;\\pi/2]$，$\\lambda_{\\text{smooth}}=0$，测试角度 $\\{-\\pi/2,\\;0,\\;\\pi/2,\\;3\\pi/4\\}$。\n- 情况 3（强平滑度）：训练范围 $[-\\pi/2,\\;\\pi/2]$，$\\lambda_{\\text{smooth}}=10$，测试角度 $\\{-\\pi/2,\\;-\\pi/4,\\;\\pi/2,\\;\\pi\\}$。\n- 情况 4（窄训练范围）：训练范围 $[-\\pi/4,\\;\\pi/4]$，$\\lambda_{\\text{smooth}}=0.1$，测试角度 $\\{-\\pi/4,\\;0,\\;\\pi/4,\\;3\\pi/4\\}$。\n\n您的程序必须输出单行，其中包含四个得到的 MSE 值（按此顺序），形式为逗号分隔的列表并用方括号括起，例如：“[v1,v2,v3,v4]”。这些值必须是浮点数。除了所有角度以弧度为单位的规定外，不需要其他单位。程序不得读取任何输入。",
            "solution": "用户提供了一个有效且定义明确的问题，该问题基于统计机器学习和生成模型的原理。任务是为一个简化的条件生成对抗网络 (cGAN) 参数估计问题推导并实现一个解决方案，该问题被形式化为一个带平滑度惩罚项的经验风险最小化任务。\n\n问题是要找到参数 $W^\\star \\in \\mathbb{R}^{2\\times 3}$ 和 $s^\\star \\ge 0$，以最小化目标函数：\n$$\n\\mathcal{L}(W,s)=\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi(y_i)-m(y_i)\\right\\|_2^2\\;+\\;\\lambda_{\\text{smooth}}\\cdot\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi'(y_i)\\right\\|_2^2\\;+\\;\\lambda_{\\text{cov}}\\cdot\\left\\|s^2 I_2-\\sigma^2 I_2\\right\\|_F^2\n$$\n该目标函数体现了经验风险最小化的原则。第一项是均方误差损失，它促使生成器的条件均值 $u(y) = W\\phi(y)$ 与真实数据的条件均值 $m(y)$ 匹配。第三项使生成器的条件协方差 $s^2 I_2$ 与真实数据的条件协方差 $\\sigma^2 I_2$ 匹配。第二项是正则化惩罚项，用于对学习到的均值映射 $u(y)$ 关于条件变量 $y$ 强制施加平滑性。\n\n目标函数 $\\mathcal{L}(W,s)$ 可以分解为两个独立的部分：一个只依赖于 $W$，另一个只依赖于 $s$。\n$$\n\\mathcal{L}(W,s) = \\mathcal{L}_W(W) + \\mathcal{L}_s(s)\n$$\n其中\n$$\n\\mathcal{L}_W(W) = \\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi(y_i)-m(y_i)\\right\\|_2^2 + \\lambda_{\\text{smooth}}\\cdot\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi'(y_i)\\right\\|_2^2\n$$\n以及\n$$\n\\mathcal{L}_s(s) = \\lambda_{\\text{cov}}\\cdot\\left\\|s^2 I_2-\\sigma^2 I_2\\right\\|_F^2\n$$\n我们可以独立地最小化这两个部分，以找到最优参数 $W^\\star$ 和 $s^\\star$。\n\n**尺度参数 $s$ 的优化**\n\n我们必须对 $s \\ge 0$ 最小化 $\\mathcal{L}_s(s)$。弗罗贝尼乌斯范数的平方是矩阵元素平方和。矩阵 $s^2 I_2-\\sigma^2 I_2$ 是一个对角矩阵：\n$$\ns^2 I_2-\\sigma^2 I_2 = \\begin{pmatrix} s^2 - \\sigma^2  0 \\\\ 0  s^2 - \\sigma^2 \\end{pmatrix}\n$$\n其弗罗贝尼乌斯范数的平方为：\n$$\n\\left\\|s^2 I_2-\\sigma^2 I_2\\right\\|_F^2 = (s^2 - \\sigma^2)^2 + 0^2 + 0^2 + (s^2 - \\sigma^2)^2 = 2(s^2 - \\sigma^2)^2\n$$\n目标函数变为 $\\mathcal{L}_s(s) = 2 \\lambda_{\\text{cov}} (s^2 - \\sigma^2)^2$。由于 $\\lambda_{\\text{cov}} > 0$，此表达式是一个非负量，当其底数为零时最小化，即 $s^2 - \\sigma^2 = 0$。这意味着 $s^2 = \\sigma^2$。给定约束 $s \\ge 0$ 以及 $\\sigma > 0$ 的事实，唯一的最优尺度参数是 $s^\\star = \\sigma$。对于给定的问题，$\\sigma = 0.2$，所以 $s^\\star = 0.2$。\n\n**权重矩阵 $W$ 的优化**\n\n关于 $W$ 的目标是一个正则化最小二乘问题。设 $W$ 由两个行向量 $w_1^\\top$ 和 $w_2^\\top$ 组成，因此 $W = \\begin{pmatrix} w_1^\\top \\\\ w_2^\\top \\end{pmatrix}$，其中 $w_1, w_2 \\in \\mathbb{R}^3$。目标均值为 $m(y) = [\\cos(y), \\sin(y)]^\\top = [m_1(y), m_2(y)]^\\top$。\n目标函数 $\\mathcal{L}_W(W)$ 可以对 $W$ 的每一行进行分离，因为欧几里得范数的平方 $\\|a-b\\|_2^2$ 可以按分量分解。忽略常数因子 $1/N$，关于 $W$ 的总代价是关于 $w_1$ 和 $w_2$ 的代价之和：\n$$\nJ(w_1, w_2) = \\sum_{i=1}^{N} \\left[ (w_1^\\top\\phi(y_i) - m_1(y_i))^2 + (w_2^\\top\\phi(y_i) - m_2(y_i))^2 \\right] + \\lambda_{\\text{smooth}}\\sum_{i=1}^{N} \\left[ (w_1^\\top\\phi'(y_i))^2 + (w_2^\\top\\phi'(y_i))^2 \\right]\n$$\n这可以解耦为两个独立的优化问题，一个针对 $w_1$，另一个针对 $w_2$。让我们为通用权重向量 $w_k \\in \\mathbb{R}^3$ 和目标函数 $m_k(y)$（对于 $k \\in \\{1, 2\\}$）构建问题。目标是最小化：\n$$\nJ(w_k) = \\sum_{i=1}^{N} (w_k^\\top \\phi(y_i) - m_k(y_i))^2 + \\lambda_{\\text{smooth}} \\sum_{i=1}^{N} (w_k^\\top \\phi'(y_i))^2\n$$\n这是一个标准的正则化最小二乘问题。为了解决它，我们用矩阵形式表示。设训练角度为 $\\{y_i\\}_{i=1}^N$。定义设计矩阵 $\\Phi \\in \\mathbb{R}^{N\\times 3}$，其第 $i$ 行为 $\\phi(y_i)^\\top = [1, y_i, y_i^2]$。定义导数基矩阵 $\\Phi' \\in \\mathbb{R}^{N\\times 3}$，其行为 $\\phi'(y_i)^\\top = [0, 1, 2y_i]$。设 $M_k \\in \\mathbb{R}^N$ 为目标值向量 $[m_k(y_1), \\dots, m_k(y_N)]^\\top$。\n目标函数可以写成：\n$$\nJ(w_k) = \\| \\Phi w_k - M_k \\|_2^2 + \\lambda_{\\text{smooth}} \\| \\Phi' w_k \\|_2^2\n$$\n为了找到最小值，我们对 $w_k$ 求梯度并令其为零：\n$$\n\\nabla_{w_k} J(w_k) = 2 \\Phi^\\top (\\Phi w_k - M_k) + 2 \\lambda_{\\text{smooth}} \\Phi'^\\top \\Phi' w_k = 0\n$$\n整理各项，我们得到这个正则化问题的正规方程：\n$$\n(\\Phi^\\top \\Phi) w_k - \\Phi^\\top M_k + \\lambda_{\\text{smooth}} (\\Phi'^\\top \\Phi') w_k = 0\n$$\n$$\n(\\Phi^\\top \\Phi + \\lambda_{\\text{smooth}} \\Phi'^\\top \\Phi') w_k = \\Phi^\\top M_k\n$$\n这是一个 $A w_k = b_k$ 形式的线性系统，其中矩阵 $A = \\Phi^\\top\\Phi + \\lambda_{\\text{smooth}}\\Phi'^\\top\\Phi'$ 是一个 $3 \\times 3$ 矩阵，向量 $b_k = \\Phi^\\top M_k$ 是一个 $3 \\times 1$ 向量。矩阵 $A$ 是对称正定的（对于 $\\lambda_{\\text{smooth}} > 0$ 和非共线的训练点），保证了唯一解的存在。\n我们对 $k=1$ 和 $k=2$ 求解该系统，以获得最优权重向量 $w_1^\\star$ 和 $w_2^\\star$：\n- 对于 $w_1^\\star$，我们使用 $M_1 = [\\cos(y_1), \\dots, \\cos(y_N)]^\\top$。\n- 对于 $w_2^\\star$，我们使用 $M_2 = [\\sin(y_1), \\dots, \\sin(y_N)]^\\top$。\n然后，最优权重矩阵为 $W^\\star = \\begin{pmatrix} w_1^{\\star\\top} \\\\ w_2^{\\star\\top} \\end{pmatrix}$。\n\n**计算方法与评估**\n\n计算步骤如下：\n1.  对于每个测试案例，在指定范围内生成 $N=101$ 个训练角度 $y_i$ 的网格。\n2.  根据训练数据构建矩阵 $\\Phi$ 和 $\\Phi'$ 以及目标向量 $M_1$ 和 $M_2$。\n3.  计算矩阵 $A = (\\Phi^\\top \\Phi) + \\lambda_{\\text{smooth}} (\\Phi'^\\top \\Phi')$ 和右端向量 $b_1 = \\Phi^\\top M_1$ 及 $b_2 = \\Phi^\\top M_2$。\n4.  求解两个 $3 \\times 3$ 线性系统 $A w_1 = b_1$ 和 $A w_2 = b_2$ 以找到 $w_1^\\star$ 和 $w_2^\\star$。\n5.  组装最优权重矩阵 $W^\\star$。\n6.  对于测试角度集 $\\{y_j^{\\text{test}}\\}_{j=1}^{K}$，计算预测的均值映射值 $u^\\star(y_j^{\\text{test}}) = W^\\star \\phi(y_j^{\\text{test}})$。\n7.  按规定计算最终的均方误差 (MSE)，该误差在 $K$ 个测试点和两个输出维度上取平均：\n$$\n\\text{MSE} = \\frac{1}{K} \\sum_{j=1}^{K} \\frac{1}{2} \\left\\| u^\\star(y_j^{\\text{test}}) - m(y_j^{\\text{test}}) \\right\\|_2^2\n$$\n\n该步骤基于正则化经验风险最小化的原理，为问题提供了完整的解决方案。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It orchestrates the calculation for each case and prints the final result.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"train_range\": [-np.pi/2, np.pi/2],\n            \"lambda_smooth\": 0.1,\n            \"test_angles\": [-np.pi/2, 0, np.pi/2, 3*np.pi/4],\n        },\n        # Case 2 (no smoothness)\n        {\n            \"train_range\": [-np.pi/2, np.pi/2],\n            \"lambda_smooth\": 0.0,\n            \"test_angles\": [-np.pi/2, 0, np.pi/2, 3*np.pi/4],\n        },\n        # Case 3 (strong smoothness)\n        {\n            \"train_range\": [-np.pi/2, np.pi/2],\n            \"lambda_smooth\": 10.0,\n            \"test_angles\": [-np.pi/2, -np.pi/4, np.pi/2, np.pi],\n        },\n        # Case 4 (narrow training range)\n        {\n            \"train_range\": [-np.pi/4, np.pi/4],\n            \"lambda_smooth\": 0.1,\n            \"test_angles\": [-np.pi/4, 0, np.pi/4, 3*np.pi/4],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        mse = solve_case(\n            train_range=case[\"train_range\"],\n            lambda_smooth=case[\"lambda_smooth\"],\n            test_angles=case[\"test_angles\"]\n        )\n        results.append(mse)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.7f}' for r in results)}]\")\n\ndef solve_case(train_range, lambda_smooth, test_angles):\n    \"\"\"\n    Solves for the optimal parameters W* and evaluates the MSE for a single case.\n\n    Args:\n        train_range (list): A list [min_angle, max_angle] for training.\n        lambda_smooth (float): The smoothness regularization weight.\n        test_angles (list): A list of angles for evaluation.\n\n    Returns:\n        float: The calculated Mean Squared Error.\n    \"\"\"\n    N = 101 # Number of training samples\n\n    # 1. Construct training data\n    y_train = np.linspace(train_range[0], train_range[1], N)\n\n    # 2. Construct design matrices and target vectors\n    # Basis function matrix Phi, where Phi[i,:] = [1, y_i, y_i^2]\n    phi_train = np.c_[np.ones(N), y_train, y_train**2]\n    \n    # Derivative basis matrix Phi', where Phi'[i,:] = [0, 1, 2*y_i]\n    phi_prime_train = np.c_[np.zeros(N), np.ones(N), 2 * y_train]\n\n    # Target vectors M1 and M2 for cos(y) and sin(y)\n    m1_train = np.cos(y_train)\n    m2_train = np.sin(y_train)\n\n    # 3. Formulate and solve the normal equations\n    # A = (Phi^T * Phi + lambda * Phi'^T * Phi')\n    A = phi_train.T @ phi_train + lambda_smooth * (phi_prime_train.T @ phi_prime_train)\n\n    # b1 = Phi^T * M1\n    b1 = phi_train.T @ m1_train\n    # b2 = Phi^T * M2\n    b2 = phi_train.T @ m2_train\n\n    # Solve the linear systems A*w1 = b1 and A*w2 = b2\n    w1_star = np.linalg.solve(A, b1)\n    w2_star = np.linalg.solve(A, b2)\n\n    # The optimal weight matrix W*\n    W_star = np.array([w1_star, w2_star])\n\n    # 4. Evaluate the model on test angles\n    y_test = np.array(test_angles)\n    N_test = len(y_test)\n\n    # Construct basis matrix for test angles\n    phi_test = np.c_[np.ones(N_test), y_test, y_test**2]\n\n    # Predicted mean values u*(y) = W* * phi(y)\n    # phi_test has shape (N_test, 3), W_star has shape (2, 3)\n    # We want result shape (N_test, 2), so we compute (phi_test @ W_star.T)\n    u_star_test = phi_test @ W_star.T\n\n    # True mean values m(y)\n    m_test = np.c_[np.cos(y_test), np.sin(y_test)]\n\n    # 5. Calculate MSE\n    # Squared L2 norm of the error for each test point\n    squared_errors = np.sum((u_star_test - m_test)**2, axis=1)\n\n    # Average over test points and the two output dimensions\n    mse = np.mean(squared_errors) / 2.0\n    \n    return mse\n\nsolve()\n\n```"
        },
        {
            "introduction": "生成模型的最终目标是产生既高质量又忠实于给定条件的数据。我们如何客观地衡量一个cGAN生成器是否做到了“名副其实”？本练习  提出了一个关键的评估指标——“$y$-忠实度”，并提供了一种基于经典统计学（线性判别分析）的严谨方法来量化它。通过在一个固定的“真实世界”分类器上评估生成样本的分类准确率，你将学会如何检验生成数据与标签之间的一致性，这是评估和诊断cGAN性能的核心技能。",
            "id": "3108919",
            "problem": "给定一个条件生成对抗网络（cGAN）的数学模型，其生成器能够根据标签生成特征。您的任务是通过训练一个冻结的分类器并计算期望 $\\mathbb{E}_{z,y}[\\mathbb{1}\\{C(G(z,y))=y\\}]$ 的经验近似值，来量化生成器的“$y$-忠实度”，然后将其与判别器的辅助标签头在相同生成数据上的准确率进行比较。所有计算都必须使用高斯生成模型和贝叶斯决策理论从第一性原理推导，并进行数值实现。\n\n定义和假设：\n- 潜变量 $z \\in \\mathbb{R}^{d}$ 从具有独立分量的标准多元正态分布中抽取，即 $z \\sim \\mathcal{N}(0, I_{d})$。\n- 标签 $y \\in \\{0,1,\\dots,K-1\\}$ 从指定的先验分布 $\\pi(y)$ 中抽取。\n- 生成器由生成的数据 $x \\in \\mathbb{R}^{d}$ 的类条件分布定义，给定为\n  $$x \\mid y \\sim \\mathcal{N}(\\mu^{\\mathrm{gen}}_{y}, \\Sigma_{\\mathrm{gen}}), \\quad \\Sigma_{\\mathrm{gen}} = B B^{\\top},$$\n  其中 $B \\in \\mathbb{R}^{d \\times d}$ 是一个给定的满秩矩阵，$(\\mu^{\\mathrm{gen}}_{y})_{y=0}^{K-1}$ 是给定的类均值。\n- 用于训练冻结分类器 $C$ 的“真实”数据分布是具有共享协方差的高斯分布：\n  $$x \\mid y \\sim \\mathcal{N}(\\mu^{\\mathrm{real}}_{y}, \\Sigma_{\\mathrm{real}}),$$\n  其中共享协方差矩阵 $\\Sigma_{\\mathrm{real}} \\in \\mathbb{R}^{d \\times d}$ 和类均值 $(\\mu^{\\mathrm{real}}_{y})_{y=0}^{K-1}$ 是已知的。\n\n训练协议：\n- 冻结分类器 $C$ 使用线性判别分析（LDA）进行训练，这是在类条件高斯分布和共享协方差假设下的贝叶斯最优分类器。您必须从真实数据分布中每个类别抽取的 $n_{C}$ 个样本中，通过最大似然估计来估算类均值和共享协方差；然后将 $C$ 冻结。\n- 判别器的标签头，记为 $D_{y}$，使用从生成器的类条件分布中每个类别抽取的 $n_{D}$ 个样本（即，在生成的数据上）单独训练为一个LDA分类器。这模拟了在cGAN设置中训练的、用于从生成数据中识别 $y$ 的判别器辅助分类器。\n\n评估指标：\n- 将生成器关于冻结分类器 $C$ 的 $y$-忠实度定义为\n  $$\\mathbb{E}_{z,y}[\\mathbb{1}\\{C(G(z,y))=y\\}],$$\n  其中期望是关于 $y \\sim \\pi(y)$ 和 $z \\sim \\mathcal{N}(0, I_{d})$ 计算的。您必须通过蒙特卡洛方法使用 $n_{\\mathrm{eval}}$ 个独立样本来近似此期望。\n- 在相同的生成评估集上，计算 $D_{y}$ 的分类准确率：\n  $$\\mathbb{E}_{z,y}[\\mathbb{1}\\{D_{y}(G(z,y))=y\\}],$$\n  同样使用相同的 $n_{\\mathrm{eval}}$ 个样本通过蒙特卡洛方法进行近似。\n\n您必须使用的基础理论：\n- 在具有共享协方差的高斯类条件模型下，贝叶斯决策规则导出线性判别函数。必须使用类均值和合并共享协方差的最大似然估计来实例化LDA分类器。不允许使用其他分类“捷径”。\n\n蒙特卡洛近似细节：\n- 为了近似 $\\mathbb{E}_{z,y}[\\cdot]$，对于 $i \\in \\{1,\\dots,n_{\\mathrm{eval}}\\}$，独立地抽取标签 $y_{i} \\sim \\pi(y)$ 和潜变量 $z_{i} \\sim \\mathcal{N}(0, I_{d})$，然后形成 $x_{i} = G(z_{i}, y_{i})$，其中 $x_{i} \\sim \\mathcal{N}(\\mu^{\\mathrm{gen}}_{y_{i}}, \\Sigma_{\\mathrm{gen}})$，并计算指示函数的经验平均值。为了可复现性，您必须使用固定的随机种子 $0$。\n\n测试套件：\n对于下面的每种情况，您必须：\n- 在每个类别有 $n_{C}$ 个样本的真实数据上训练冻结分类器 $C$。\n- 在每个类别有 $n_{D}$ 个样本的生成数据上训练 $D_{y}$。\n- 在一个包含 $n_{\\mathrm{eval}}$ 个、根据指定的 $\\pi(y)$ 抽取的生成样本的评估集上评估两者。\n\n所有情况都使用维度 $d=2$。在所有情况中，共享的真实协方差是 $\\Sigma_{\\mathrm{real}} = \\sigma_{\\mathrm{real}}^{2} I_{2}$，生成器的协方差是 $\\Sigma_{\\mathrm{gen}} = \\sigma_{\\mathrm{gen}}^{2} I_{2}$，其中 $B=\\sigma_{\\mathrm{gen}} I_{2}$。\n\n- 情况 1 (理想情况, $K=3$):\n  - $\\mu^{\\mathrm{real}}_{0} = (0,0)$, $\\mu^{\\mathrm{real}}_{1} = (3,0)$, $\\mu^{\\mathrm{real}}_{2} = (0,3)$。\n  - $\\mu^{\\mathrm{gen}}_{0} = (0.1,-0.1)$, $\\mu^{\\mathrm{gen}}_{1} = (3.1,0.2)$, $\\mu^{\\mathrm{gen}}_{2} = (-0.1,3.2)$。\n  - $\\sigma_{\\mathrm{real}} = 0.2$, $\\sigma_{\\mathrm{gen}} = 0.25$。\n  - $n_{C} = 50$, $n_{D} = 30$, $n_{\\mathrm{eval}} = 3000$。\n  - $\\pi(y) = (1/3, 1/3, 1/3)$。\n\n- 情况 2 (高度重叠, $K=3$):\n  - $\\mu^{\\mathrm{real}}_{0} = (0,0)$, $\\mu^{\\mathrm{real}}_{1} = (1.0,0.2)$, $\\mu^{\\mathrm{real}}_{2} = (0.2,1.0)$。\n  - $\\mu^{\\mathrm{gen}}_{0} = (0,0)$, $\\mu^{\\mathrm{gen}}_{1} = (1.0,0.2)$, $\\mu^{\\mathrm{gen}}_{2} = (0.2,1.0)$。\n  - $\\sigma_{\\mathrm{real}} = 0.6$, $\\sigma_{\\mathrm{gen}} = 0.7$。\n  - $n_{C} = 200$, $n_{D} = 50$, $n_{\\mathrm{eval}} = 3000$。\n  - $\\pi(y) = (1/3, 1/3, 1/3)$。\n\n- 情况 3 (非均匀标签先验, $K=3$):\n  - $\\mu^{\\mathrm{real}}_{0} = (0,0)$, $\\mu^{\\mathrm{real}}_{1} = (3,0)$, $\\mu^{\\mathrm{real}}_{2} = (0,3)$。\n  - $\\mu^{\\mathrm{gen}}_{0} = (0.1,-0.1)$, $\\mu^{\\mathrm{gen}}_{1} = (3.1,0.2)$, $\\mu^{\\mathrm{gen}}_{2} = (-0.1,3.2)$。\n  - $\\sigma_{\\mathrm{real}} = 0.2$, $\\sigma_{\\mathrm{gen}} = 0.25$。\n  - $n_{C} = 50$, $n_{D} = 30$, $n_{\\mathrm{eval}} = 4000$。\n  - $\\pi(y) = (0.8, 0.1, 0.1)$。\n\n- 情况 4 (判别器训练样本极少, $K=2$):\n  - $\\mu^{\\mathrm{real}}_{0} = (0,0)$, $\\mu^{\\mathrm{real}}_{1} = (2.5,0)$。\n  - $\\mu^{\\mathrm{gen}}_{0} = (0.2,-0.1)$, $\\mu^{\\mathrm{gen}}_{1} = (2.6,0.1)$。\n  - $\\sigma_{\\mathrm{real}} = 0.3$, $\\sigma_{\\mathrm{gen}} = 0.3$。\n  - $n_{C} = 40$, $n_{D} = 3$, $n_{\\mathrm{eval}} = 2000$。\n  - $\\pi(y) = (0.5, 0.5)$。\n\n- 情况 5 (近乎完美分离, $K=3$):\n  - $\\mu^{\\mathrm{real}}_{0} = (0,0)$, $\\mu^{\\mathrm{real}}_{1} = (5,5)$, $\\mu^{\\mathrm{real}}_{2} = (-5,5)$。\n  - $\\mu^{\\mathrm{gen}}_{0} = (0,0)$, $\\mu^{\\mathrm{gen}}_{1} = (5,5)$, $\\mu^{\\mathrm{gen}}_{2} = (-5,5)$。\n  - $\\sigma_{\\mathrm{real}} = 0.2$, $\\sigma_{\\mathrm{gen}} = 0.1$。\n  - $n_{C} = 20$, $n_{D} = 20$, $n_{\\mathrm{eval}} = 3000$。\n  - $\\pi(y) = (1/3, 1/3, 1/3)$。\n\n算法要求：\n- 从第一性原理实现LDA：从训练数据估计类均值 $\\hat{\\mu}_{k}$ 和合并协方差 $\\hat{\\Sigma}$，添加一个小的岭项 $\\lambda I_{d}$（其中 $\\lambda = 10^{-6}$）以确保可逆性，计算判别分数进行预测，并使用从训练频率估计的类先验。\n- 使用相同的生成评估集来计算两种准确率，以便进行公平比较。\n- 将随机种子固定为 $0$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，包含一个由数对组成的列表，其中每个数对是 $[\\text{acc}_{C}, \\text{acc}_{D}]$，对应于一种情况，并按上述情况的顺序排列。例如：\"[[0.95,0.93],[...],...]\"。条目必须是浮点数。\n\n不涉及物理单位或角度单位。所有数值答案必须是十进制形式的浮点数。程序必须完整且可按规定运行，并且不得需要任何用户输入。",
            "solution": "该问题是有效的，因为它提出了一个在统计机器学习领域中定义明确、具有科学依据的计算任务。它要求在一个模拟的条件生成对抗网络（cGAN）框架内，基于线性判别分析（LDA）实现和比较两个分类器。所有参数、模型和评估过程都得到了完整且一致的规定。\n\n解决方案首先建立LDA作为给定问题背景下贝叶斯最优分类器的理论基础。然后，我们详细说明训练和评估过程的数值实现。\n\n**1. 理论基础：线性判别分析**\n\n问题假设“真实”数据和“生成”数据都遵循类条件高斯分布，并且在各自的域（真实和生成）内具有共享的协方差矩阵。对于一个有 $K$ 个类别的分类问题，其中类别 $k \\in \\{0, \\dots, K-1\\}$ 的数据 $x \\in \\mathbb{R}^d$ 来自正态分布 $\\mathcal{N}(\\mu_k, \\Sigma)$，贝叶斯最优决策规则将 $x$ 分配给最大化后验概率 $P(y=k|x)$ 的类别。使用贝叶斯定理，这等价于最大化 $p(x|y=k)P(y=k)$。取对数并舍去对所有类别都相同的常数项后，我们寻求最大化判别分数 $\\delta_k(x)$：\n$$ \\delta_k(x) = \\log p(x|y=k) + \\log P(y=k) $$\n代入多元正态分布的概率密度函数：\n$$ \\log p(x|y=k) = -\\frac{1}{2}(x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k) - \\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\Sigma| $$\n展开二次项并舍去与 $k$ 无关的项（即 $x^T\\Sigma^{-1}x$, $\\frac{d}{2}\\log(2\\pi)$, $\\frac{1}{2}\\log|\\Sigma|$），决策规则简化为最大化以下关于 $x$ 的线性函数：\n$$ \\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2}\\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k $$\n其中 $\\pi_k = P(y=k)$ 是类别 $k$ 的先验概率。任意两个类别 $i$ 和 $j$ 之间的决策边界是满足 $\\delta_i(x) = \\delta_j(x)$ 的点集，这定义了一个超平面。因此，该分类器是线性的。\n\n**2. 从训练数据进行参数估计**\n\n在实践中，真实参数 $\\mu_k$、$\\Sigma$ 和 $\\pi_k$ 是未知的，必须从训练集 $\\{(x_i, y_i)\\}_{i=1}^N$ 中估计。我们使用最大似然估计（MLE），得到以下估计值：\n- **类先验 ($\\hat{\\pi}_k$)：** 类别 $k$ 中训练样本的比例。对于一个有 $N_k$ 个来自类别 $k$ 的样本和总共 $N$ 个样本的训练集，$\\hat{\\pi}_k = N_k / N$。\n- **类均值 ($\\hat{\\mu}_k$)：** 属于类别 $k$ 的所有训练数据的样本均值。\n$$ \\hat{\\mu}_k = \\frac{1}{N_k} \\sum_{i: y_i=k} x_i $$\n- **合并协方差 ($\\hat{\\Sigma}$):** 各个类协方差矩阵的加权平均，权重为其自由度。无偏估计量为：\n$$ \\hat{\\Sigma} = \\frac{1}{N-K} \\sum_{k=0}^{K-1} S_k = \\frac{1}{N-K} \\sum_{k=0}^{K-1} \\sum_{i: y_i=k} (x_i - \\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^T $$\n为了数值稳定性，尤其是在样本数量不远大于维度 $d$ 的情况下，估计的协方差矩阵在求逆之前会通过添加一个小的单位矩阵倍数 $\\lambda I_d$ 进行正则化。\n\n**3. 模拟和评估过程**\n\n每个测试用例的总体流程如下：\n\n1.  **实例化分类器：** 我们定义两个LDA分类器，$C$ 和 $D_y$。\n\n2.  **训练冻结分类器 $C$：**\n    - 通过从“真实”数据分布 $x \\mid y \\sim \\mathcal{N}(\\mu^{\\mathrm{real}}_{y}, \\Sigma_{\\mathrm{real}})$ 中为每个类别抽取 $n_C$ 个样本来生成一个训练集。\n    - 在此数据集上训练分类器 $C$ 以学习参数 $(\\hat{\\pi}^{\\mathrm{real}}, \\hat{\\mu}^{\\mathrm{real}}, \\hat{\\Sigma}^{\\mathrm{real}})$。由于训练数据是平衡的，所以 $\\hat{\\pi}_k^{\\mathrm{real}} = 1/K$。\n\n3.  **训练判别器头 $D_y$：**\n    - 通过从生成器的分布 $x \\mid y \\sim \\mathcal{N}(\\mu^{\\mathrm{gen}}_{y}, \\Sigma_{\\mathrm{gen}})$ 中为每个类别抽取 $n_D$ 个样本来生成一个单独的训练集。\n    - 在此生成的数据集上训练分类器 $D_y$ 以学习其自己的一套参数 $(\\hat{\\pi}^{\\mathrm{gen}}, \\hat{\\mu}^{\\mathrm{gen}}, \\hat{\\Sigma}^{\\mathrm{gen}})$。同样，$\\hat{\\pi}_k^{\\mathrm{gen}} = 1/K$。\n\n4.  **蒙特卡洛评估：**\n    - 创建一个包含 $n_{\\mathrm{eval}}$ 个样本的单一公共评估集。这是通过首先从指定的先验分布 $\\pi(y)$ 中抽样 $n_{\\mathrm{eval}}$ 个标签 $\\{y_i\\}_{i=1}^{n_{\\mathrm{eval}}}$ 来完成的。然后，对于每个标签 $y_i$，从生成器的分布 $\\mathcal{N}(\\mu^{\\mathrm{gen}}_{y_i}, \\Sigma_{\\mathrm{gen}})$ 中抽样一个相应的数据点 $x_i$。\n    - “$y$-忠实度”是分类器 $C$ 在此评估集上的准确率：$ \\mathrm{acc}_C = \\frac{1}{n_{\\mathrm{eval}}} \\sum_{i=1}^{n_{\\mathrm{eval}}} \\mathbb{1}\\{C(x_i)=y_i\\} $。\n    - 判别器的准确率是 $D_y$ 在同一评估集上的准确率：$ \\mathrm{acc}_{D_y} = \\frac{1}{n_{\\mathrm{eval}}} \\sum_{i=1}^{n_{\\mathrm{eval}}} \\mathbb{1}\\{D_y(x_i)=y_i\\} $。\n\n此过程允许直接比较一个在真实数据上训练的分类器如何看待生成器的输出，与一个在生成器自己的输出上训练的辅助分类器的表现如何。它们准确率的差异凸显了真实数据分布和生成数据分布之间的差异。整个模拟使用固定的随机种子执行，以确保可复现性。",
            "answer": "```python\nimport numpy as np\n\n# A global random number generator to be used throughout the simulation for reproducibility.\nRNG = np.random.default_rng(0)\n\nclass LDAClassifier:\n    \"\"\"\n    A Linear Discriminant Analysis (LDA) classifier implemented from first principles.\n    Assumes Gaussian class-conditional distributions with a shared covariance matrix.\n    \"\"\"\n    def __init__(self, ridge_lambda=1e-6):\n        self.ridge_lambda = ridge_lambda\n        self.means = None\n        self.priors = None\n        self.inv_cov = None\n        self.discriminant_constants = None\n        self.n_classes = 0\n\n    def fit(self, X, y):\n        \"\"\"\n        Estimates LDA parameters from training data (X, y).\n        - Class priors from training frequencies.\n        - Class means.\n        - Pooled covariance matrix.\n        \"\"\"\n        n_samples, n_features = X.shape\n        unique_classes = np.unique(y)\n        self.n_classes = len(unique_classes)\n\n        # Estimate class priors from training frequencies\n        class_counts = np.array([np.sum(y == k) for k in range(self.n_classes)])\n        self.priors = class_counts / n_samples\n\n        # Estimate class means\n        self.means = np.array([X[y == k].mean(axis=0) for k in range(self.n_classes)])\n\n        # Estimate pooled covariance matrix\n        pooled_cov = np.zeros((n_features, n_features))\n        if n_samples > self.n_classes:\n            for k in range(self.n_classes):\n                class_samples = X[y == k]\n                if class_counts[k] > 0:\n                    residuals = class_samples - self.means[k]\n                    # Sum of squares matrix S_k = sum (x_i - mu_k)(x_i - mu_k)^T\n                    pooled_cov += residuals.T @ residuals\n            # Unbiased estimator denominator is N-K\n            pooled_cov /= (n_samples - self.n_classes)\n        else:\n            # Fallback for insufficient data (N = K)\n            pooled_cov = np.identity(n_features)\n\n        # Add ridge regularization and compute inverse for numerical stability\n        reg_cov = pooled_cov + self.ridge_lambda * np.identity(n_features)\n        self.inv_cov = np.linalg.inv(reg_cov)\n        \n        # Pre-compute parts of the discriminant function for efficient prediction\n        # delta_k(x) = x.T @ inv_cov @ mu_k - 0.5 * mu_k.T @ inv_cov @ mu_k + log(pi_k)\n        log_priors = np.log(self.priors + 1e-12)  # Epsilon for log(0)\n        self.discriminant_constants = -0.5 * np.sum((self.means @ self.inv_cov) * self.means, axis=1) + log_priors\n\n    def predict(self, X):\n        \"\"\"\n        Predicts class labels for new data X.\n        \"\"\"\n        # Linear part of discriminant: X @ inv_cov @ means.T\n        scores = X @ self.inv_cov @ self.means.T\n        # Add constant part\n        scores += self.discriminant_constants\n        return np.argmax(scores, axis=1)\n\ndef generate_balanced_data(means, cov, n_samples_per_class, K):\n    \"\"\"Generates a balanced dataset with n_samples_per_class for each class.\"\"\"\n    d = cov.shape[0]\n    X = np.empty((n_samples_per_class * K, d))\n    y = np.empty(n_samples_per_class * K, dtype=int)\n    for k in range(K):\n        start_idx = k * n_samples_per_class\n        end_idx = (k + 1) * n_samples_per_class\n        X[start_idx:end_idx] = RNG.multivariate_normal(means[k], cov, size=n_samples_per_class)\n        y[start_idx:end_idx] = k\n    return X, y\n\ndef generate_eval_data(means, cov, n_eval, priors, K):\n    \"\"\"Generates an evaluation dataset with labels sampled from the prior distribution.\"\"\"\n    d = cov.shape[0]\n    # Sample labels from the prior distribution\n    y_eval = RNG.choice(K, size=n_eval, p=priors)\n    X_eval = np.empty((n_eval, d))\n    for k in range(K):\n        # Generate data for all samples of class k at once\n        class_mask = (y_eval == k)\n        n_k = np.sum(class_mask)\n        if n_k > 0:\n            X_eval[class_mask] = RNG.multivariate_normal(means[k], cov, size=n_k)\n    return X_eval, y_eval\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, K=3)\n        {\n            'K': 3, 'd': 2,\n            'mu_real': np.array([[0,0], [3,0], [0,3]]),\n            'mu_gen': np.array([[0.1,-0.1], [3.1,0.2], [-0.1,3.2]]),\n            'sigma_real': 0.2, 'sigma_gen': 0.25,\n            'n_C': 50, 'n_D': 30, 'n_eval': 3000,\n            'pi_y': np.array([1/3, 1/3, 1/3])\n        },\n        # Case 2 (high overlap, K=3)\n        {\n            'K': 3, 'd': 2,\n            'mu_real': np.array([[0,0], [1.0,0.2], [0.2,1.0]]),\n            'mu_gen': np.array([[0,0], [1.0,0.2], [0.2,1.0]]),\n            'sigma_real': 0.6, 'sigma_gen': 0.7,\n            'n_C': 200, 'n_D': 50, 'n_eval': 3000,\n            'pi_y': np.array([1/3, 1/3, 1/3])\n        },\n        # Case 3 (non-uniform label prior, K=3)\n        {\n            'K': 3, 'd': 2,\n            'mu_real': np.array([[0,0], [3,0], [0,3]]),\n            'mu_gen': np.array([[0.1,-0.1], [3.1,0.2], [-0.1,3.2]]),\n            'sigma_real': 0.2, 'sigma_gen': 0.25,\n            'n_C': 50, 'n_D': 30, 'n_eval': 4000,\n            'pi_y': np.array([0.8, 0.1, 0.1])\n        },\n        # Case 4 (very small discriminator training, K=2)\n        {\n            'K': 2, 'd': 2,\n            'mu_real': np.array([[0,0], [2.5,0]]),\n            'mu_gen': np.array([[0.2,-0.1], [2.6,0.1]]),\n            'sigma_real': 0.3, 'sigma_gen': 0.3,\n            'n_C': 40, 'n_D': 3, 'n_eval': 2000,\n            'pi_y': np.array([0.5, 0.5])\n        },\n        # Case 5 (near-perfect separation, K=3)\n        {\n            'K': 3, 'd': 2,\n            'mu_real': np.array([[0,0], [5,5], [-5,5]]),\n            'mu_gen': np.array([[0,0], [5,5], [-5,5]]),\n            'sigma_real': 0.2, 'sigma_gen': 0.1,\n            'n_C': 20, 'n_D': 20, 'n_eval': 3000,\n            'pi_y': np.array([1/3, 1/3, 1/3])\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        K, d = params['K'], params['d']\n        cov_real = (params['sigma_real']**2) * np.identity(d)\n        cov_gen = (params['sigma_gen']**2) * np.identity(d)\n\n        # 1. Train Classifier C on \"real\" data\n        X_real_train, y_real_train = generate_balanced_data(\n            params['mu_real'], cov_real, params['n_C'], K\n        )\n        classifier_C = LDAClassifier()\n        classifier_C.fit(X_real_train, y_real_train)\n        \n        # 2. Train Classifier D_y on \"generated\" data\n        X_gen_train, y_gen_train = generate_balanced_data(\n            params['mu_gen'], cov_gen, params['n_D'], K\n        )\n        classifier_Dy = LDAClassifier()\n        classifier_Dy.fit(X_gen_train, y_gen_train)\n        \n        # 3. Create common evaluation set from the generator\n        X_eval, y_eval = generate_eval_data(\n            params['mu_gen'], cov_gen, params['n_eval'], params['pi_y'], K\n        )\n        \n        # 4. Evaluate both classifiers on the same evaluation set\n        # Accuracy of C (y-faithfulness)\n        y_pred_C = classifier_C.predict(X_eval)\n        acc_C = np.mean(y_pred_C == y_eval)\n        \n        # Accuracy of Dy\n        y_pred_Dy = classifier_Dy.predict(X_eval)\n        acc_Dy = np.mean(y_pred_Dy == y_eval)\n        \n        results.append([acc_C, acc_Dy])\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}