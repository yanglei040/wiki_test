{
    "hands_on_practices": [
        {
            "introduction": "cGAN生成器的核心任务是学习一个从条件 $y$ 到数据分布的函数。这个练习将通过一个简化的模型，让你亲手实践如何有效地学习这个映射，其中条件是一个连续的角度变量。你还将探索正则化这一关键概念，它如何促使模型平moothly地泛化，尤其是在训练数据范围之外进行外推时，这对提升模型的鲁棒性至关重要。",
            "id": "3108842",
            "problem": "你的任务是设计并分析一个简化的条件生成对抗网络（cGAN），该网络在角度上进行连续条件化。目标是基于第一性原理进行设计，对生成器关于条件变量的平滑度进行正则化形式化，并凭经验测试其在训练范围之外的外推能力。所有角度都必须以弧度为单位。\n\n考虑一个条件生成对抗网络（cGAN），其生成器定义为函数 $G:\\mathbb{R}^d\\times\\mathbb{R}\\to\\mathbb{R}^2$，其中条件变量为 $y\\in\\mathbb{R}$（解释为角度），噪声 $z\\in\\mathbb{R}^d$ 是标准正态分布。真实数据分布是一个条件分布 $p_{\\text{data}}(x\\mid y)$，其条件均值为 $m(y)=[\\cos(y),\\sin(y)]^\\top$，条件协方差为 $\\sigma^2 I_2$，其中 $\\sigma0$ 是一个固定值。角度是连续的，条件空间是实数线。\n\n你将分析一个受限的生成器类，其形式为\n$$\nG(z,y;\\theta)=u(y)+s\\,z,\\quad z\\sim\\mathcal{N}(0,I_2),\n$$\n其中 $u:\\mathbb{R}\\to\\mathbb{R}^2$ 是一个确定性均值映射，$s\\ge 0$ 是一个标量。假设 $u(y)$ 由一个关于 $y$ 的 2 次多项式基参数化，即对于 $\\phi(y)=[1,\\,y,\\,y^2]^\\top\\in\\mathbb{R}^3$，存在一个矩阵 $W\\in\\mathbb{R}^{2\\times 3}$ 使得\n$$\nu(y)=W\\,\\phi(y).\n$$\n设平滑度惩罚为生成器确定性均值部分关于 $y$ 的雅可比矩阵的 $\\ell_2$ 范数的平方：\n$$\n\\mathcal{S}(W)=\\mathbb{E}_{y\\sim p_{\\text{train}}}\\left[\\left\\|\\frac{\\partial u}{\\partial y}(y)\\right\\|_2^2\\right],\n$$\n其中 $\\frac{\\partial u}{\\partial y}(y)=W\\,\\phi'(y)$ 且 $\\phi'(y)=\\frac{d}{dy}\\phi(y)=[0,\\,1,\\,2y]^\\top$。\n\n你将使用训练角度的均匀网格上的经验平均来近似期望。定义一个平衡均值匹配、协方差匹配和平滑度的经验目标：\n$$\n\\mathcal{L}(W,s)=\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi(y_i)-m(y_i)\\right\\|_2^2\\;+\\;\\lambda_{\\text{smooth}}\\cdot\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi'(y_i)\\right\\|_2^2\\;+\\;\\lambda_{\\text{cov}}\\cdot\\left\\|s^2 I_2-\\sigma^2 I_2\\right\\|_F^2,\n$$\n其中 $y_i$ 是训练角度，$N$ 是训练角度的数量，$\\lambda_{\\text{smooth}}\\ge 0$ 是平滑度权重，$\\lambda_{\\text{cov}}\\ge 0$ 是协方差匹配权重。\n\n任务：\n- 从条件生成对抗网络（cGANs）、经验风险最小化以及上述平滑度惩罚的核心定义出发，推导出必要的最优性条件，并提供一种计算方法来获得最小化 $\\mathcal{L}(W,s)$ 的最优参数 $W^\\star$ 和 $s^\\star$。\n- 实现一个程序，该程序：\n  - 在指定范围内构建作为均匀网格的训练角度集。\n  - 从带有导数正则化的最小二乘法的第一性原理计算 $W^\\star$ 和 $s^\\star$。\n  - 在指定的测试角度上评估拟合的均值映射 $u^\\star(y)=W^\\star\\phi(y)$，并报告与目标 $m(y)$ 相比的均方误差（MSE），该误差在所有测试角度和两个输出维度上取平均值。\n- 使用 $\\sigma=0.2$，$\\lambda_{\\text{cov}}=1$，以及在每个指定范围内（含端点）均匀间隔的 $N=101$ 个训练角度。所有角度均以弧度为单位。\n\n测试套件：\n- 情况 1（一般情况）：训练范围 $[-\\pi/2,\\;\\pi/2]$，$\\lambda_{\\text{smooth}}=0.1$，测试角度 $\\{-\\pi/2,\\;0,\\;\\pi/2,\\;3\\pi/4\\}$。\n- 情况 2（无平滑度）：训练范围 $[-\\pi/2,\\;\\pi/2]$，$\\lambda_{\\text{smooth}}=0$，测试角度 $\\{-\\pi/2,\\;0,\\;\\pi/2,\\;3\\pi/4\\}$。\n- 情况 3（强平滑度）：训练范围 $[-\\pi/2,\\;\\pi/2]$，$\\lambda_{\\text{smooth}}=10$，测试角度 $\\{-\\pi/2,\\;-\\pi/4,\\;\\pi/2,\\;\\pi\\}$。\n- 情况 4（窄训练范围）：训练范围 $[-\\pi/4,\\;\\pi/4]$，$\\lambda_{\\text{smooth}}=0.1$，测试角度 $\\{-\\pi/4,\\;0,\\;\\pi/4,\\;3\\pi/4\\}$。\n\n你的程序必须输出一行，其中包含四个得到的 MSE 值（按此顺序），格式为逗号分隔的列表，并用方括号括起来，例如：“[v1,v2,v3,v4]”。这些值必须是浮点数。除了所有角度都以弧度为单位的规定外，不需要其他单位。程序不得读取任何输入。",
            "solution": "此问题提出了一个基于统计机器学习和生成模型原理的有效且定义明确的任务。任务是为一个简化的条件生成对抗网络（cGAN）参数估计问题推导并实现一个解决方案，该问题被表述为一个带有平滑度惩罚的经验风险最小化任务。\n\n问题是找到参数 $W^\\star \\in \\mathbb{R}^{2\\times 3}$ 和 $s^\\star \\ge 0$ 来最小化目标函数：\n$$\n\\mathcal{L}(W,s)=\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi(y_i)-m(y_i)\\right\\|_2^2\\;+\\;\\lambda_{\\text{smooth}}\\cdot\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi'(y_i)\\right\\|_2^2\\;+\\;\\lambda_{\\text{cov}}\\cdot\\left\\|s^2 I_2-\\sigma^2 I_2\\right\\|_F^2\n$$\n这个目标体现了经验风险最小化的原则。第一项是均方误差损失，它鼓励生成器的条件均值 $u(y) = W\\phi(y)$ 与真实数据的条件均值 $m(y)$ 相匹配。第三项使生成器的条件协方差 $s^2 I_2$ 与真实数据的条件协方差 $\\sigma^2 I_2$ 相匹配。第二项是正则化惩罚，用于在学习到的均值映射 $u(y)$ 上强制施加关于条件变量 $y$ 的平滑性。\n\n目标函数 $\\mathcal{L}(W,s)$ 可以分解为两个独立的部分：一个只依赖于 $W$，另一个只依赖于 $s$。\n$$\n\\mathcal{L}(W,s) = \\mathcal{L}_W(W) + \\mathcal{L}_s(s)\n$$\n其中\n$$\n\\mathcal{L}_W(W) = \\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi(y_i)-m(y_i)\\right\\|_2^2 + \\lambda_{\\text{smooth}}\\cdot\\frac{1}{N}\\sum_{i=1}^{N}\\left\\|W\\,\\phi'(y_i)\\right\\|_2^2\n$$\n且\n$$\n\\mathcal{L}_s(s) = \\lambda_{\\text{cov}}\\cdot\\left\\|s^2 I_2-\\sigma^2 I_2\\right\\|_F^2\n$$\n我们可以独立地最小化这两部分来找到最优参数 $W^\\star$ 和 $s^\\star$。\n\n**尺度参数 $s$ 的优化**\n\n我们必须最小化关于 $s \\ge 0$ 的 $\\mathcal{L}_s(s)$。弗罗贝尼乌斯范数的平方是矩阵元素平方的和。矩阵 $s^2 I_2-\\sigma^2 I_2$ 是一个对角矩阵：\n$$\ns^2 I_2-\\sigma^2 I_2 = \\begin{pmatrix} s^2 - \\sigma^2  0 \\\\ 0  s^2 - \\sigma^2 \\end{pmatrix}\n$$\n其弗罗贝尼乌斯范数的平方是：\n$$\n\\left\\|s^2 I_2-\\sigma^2 I_2\\right\\|_F^2 = (s^2 - \\sigma^2)^2 + 0^2 + 0^2 + (s^2 - \\sigma^2)^2 = 2(s^2 - \\sigma^2)^2\n$$\n目标函数变为 $\\mathcal{L}_s(s) = 2 \\lambda_{\\text{cov}} (s^2 - \\sigma^2)^2$。由于 $\\lambda_{\\text{cov}}  0$，这个表达式是一个非负量，当其底数为零时达到最小值，即 $s^2 - \\sigma^2 = 0$。这意味着 $s^2 = \\sigma^2$。考虑到约束 $s \\ge 0$ 和 $\\sigma  0$ 的事实，唯一的最优尺度参数是 $s^\\star = \\sigma$。对于给定的问题，$\\sigma = 0.2$，所以 $s^\\star = 0.2$。\n\n**权重矩阵 $W$ 的优化**\n\n关于 $W$ 的目标是一个正则化最小二乘问题。设 $W$ 由两个行向量 $w_1^\\top$ 和 $w_2^\\top$ 组成，所以 $W = \\begin{pmatrix} w_1^\\top \\\\ w_2^\\top \\end{pmatrix}$，其中 $w_1, w_2 \\in \\mathbb{R}^3$。目标均值为 $m(y) = [\\cos(y), \\sin(y)]^\\top = [m_1(y), m_2(y)]^\\top$。\n目标 $\\mathcal{L}_W(W)$ 可以对 $W$ 的每一行进行分离，因为欧几里得范数的平方 $\\|a-b\\|_2^2$ 可以按分量分解。忽略常数因子 $1/N$， $W$ 的总成本是 $w_1$ 和 $w_2$ 成本的总和：\n$$\nJ(w_1, w_2) = \\sum_{i=1}^{N} \\left[ (w_1^\\top\\phi(y_i) - m_1(y_i))^2 + (w_2^\\top\\phi(y_i) - m_2(y_i))^2 \\right] + \\lambda_{\\text{smooth}}\\sum_{i=1}^{N} \\left[ (w_1^\\top\\phi'(y_i))^2 + (w_2^\\top\\phi'(y_i))^2 \\right]\n$$\n这可以解耦为两个独立的优化问题，一个针对 $w_1$，一个针对 $w_2$。让我们为通用权重向量 $w_k \\in \\mathbb{R}^3$ 和目标函数 $m_k(y)$（其中 $k \\in \\{1, 2\\}$）构建问题。目标是最小化：\n$$\nJ(w_k) = \\sum_{i=1}^{N} (w_k^\\top \\phi(y_i) - m_k(y_i))^2 + \\lambda_{\\text{smooth}} \\sum_{i=1}^{N} (w_k^\\top \\phi'(y_i))^2\n$$\n这是一个标准的正则化最小二乘问题。为了解决它，我们将其表示为矩阵形式。设训练角度为 $\\{y_i\\}_{i=1}^N$。定义设计矩阵 $\\Phi \\in \\mathbb{R}^{N\\times 3}$，其第 $i$ 行为 $\\phi(y_i)^\\top = [1, y_i, y_i^2]$。定义导数基矩阵 $\\Phi' \\in \\mathbb{R}^{N\\times 3}$，其行为 $\\phi'(y_i)^\\top = [0, 1, 2y_i]$。设 $M_k \\in \\mathbb{R}^N$ 为目标值向量 $[m_k(y_1), \\dots, m_k(y_N)]^\\top$。\n目标可以写成：\n$$\nJ(w_k) = \\| \\Phi w_k - M_k \\|_2^2 + \\lambda_{\\text{smooth}} \\| \\Phi' w_k \\|_2^2\n$$\n为了找到最小值，我们对 $w_k$ 求梯度并将其设为零：\n$$\n\\nabla_{w_k} J(w_k) = 2 \\Phi^\\top (\\Phi w_k - M_k) + 2 \\lambda_{\\text{smooth}} \\Phi'^\\top \\Phi' w_k = 0\n$$\n整理各项，我们得到这个正则化问题的正规方程：\n$$\n(\\Phi^\\top \\Phi) w_k - \\Phi^\\top M_k + \\lambda_{\\text{smooth}} (\\Phi'^\\top \\Phi') w_k = 0\n$$\n$$\n(\\Phi^\\top \\Phi + \\lambda_{\\text{smooth}} \\Phi'^\\top \\Phi') w_k = \\Phi^\\top M_k\n$$\n这是一个形式为 $A w_k = b_k$ 的线性系统，其中矩阵 $A = \\Phi^\\top\\Phi + \\lambda_{\\text{smooth}}\\Phi'^\\top\\Phi'$ 是一个 $3 \\times 3$ 的矩阵，向量 $b_k = \\Phi^\\top M_k$ 是一个 $3 \\times 1$ 的向量。矩阵 $A$ 是对称正定的（对于 $\\lambda_{\\text{smooth}}  0$ 和非共线的训练点），保证了唯一解。\n我们对 $k=1$ 和 $k=2$ 解这个系统，以获得最优权重向量 $w_1^\\star$ 和 $w_2^\\star$：\n- 对于 $w_1^\\star$，我们使用 $M_1 = [\\cos(y_1), \\dots, \\cos(y_N)]^\\top$。\n- 对于 $w_2^\\star$，我们使用 $M_2 = [\\sin(y_1), \\dots, \\sin(y_N)]^\\top$。\n最优权重矩阵则为 $W^\\star = \\begin{pmatrix} w_1^{\\star\\top} \\\\ w_2^{\\star\\top} \\end{pmatrix}$。\n\n**计算方法与评估**\n\n计算过程如下：\n1.  对于每个测试用例，在指定范围内生成 $N=101$ 个训练角度 $y_i$ 的网格。\n2.  从训练数据构建矩阵 $\\Phi$ 和 $\\Phi'$ 以及目标向量 $M_1$ 和 $M_2$。\n3.  计算矩阵 $A = (\\Phi^\\top \\Phi) + \\lambda_{\\text{smooth}} (\\Phi'^\\top \\Phi')$ 和右侧向量 $b_1 = \\Phi^\\top M_1$ 与 $b_2 = \\Phi^\\top M_2$。\n4.  解两个 $3 \\times 3$ 的线性系统 $A w_1 = b_1$ 和 $A w_2 = b_2$ 来找到 $w_1^\\star$ 和 $w_2^\\star$。\n5.  组装最优权重矩阵 $W^\\star$。\n6.  对于测试角度集 $\\{y_j^{\\text{test}}\\}_{j=1}^{K}$，计算预测的均值映射值 $u^\\star(y_j^{\\text{test}}) = W^\\star \\phi(y_j^{\\text{test}})$。\n7.  按规定计算最终的均方误差（MSE），该误差在 $K$ 个测试点和两个输出维度上取平均值：\n$$\n\\text{MSE} = \\frac{1}{K} \\sum_{j=1}^{K} \\frac{1}{2} \\left\\| u^\\star(y_j^{\\text{test}}) - m(y_j^{\\text{test}}) \\right\\|_2^2\n$$\n这个过程基于正则化经验风险最小化的原理，为问题提供了完整的解决方案。\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It orchestrates the calculation for each case and prints the final result.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general case)\n        {\n            \"train_range\": [-np.pi/2, np.pi/2],\n            \"lambda_smooth\": 0.1,\n            \"test_angles\": [-np.pi/2, 0, np.pi/2, 3*np.pi/4],\n        },\n        # Case 2 (no smoothness)\n        {\n            \"train_range\": [-np.pi/2, np.pi/2],\n            \"lambda_smooth\": 0.0,\n            \"test_angles\": [-np.pi/2, 0, np.pi/2, 3*np.pi/4],\n        },\n        # Case 3 (strong smoothness)\n        {\n            \"train_range\": [-np.pi/2, np.pi/2],\n            \"lambda_smooth\": 10.0,\n            \"test_angles\": [-np.pi/2, -np.pi/4, np.pi/2, np.pi],\n        },\n        # Case 4 (narrow training range)\n        {\n            \"train_range\": [-np.pi/4, np.pi/4],\n            \"lambda_smooth\": 0.1,\n            \"test_angles\": [-np.pi/4, 0, np.pi/4, 3*np.pi/4],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        mse = solve_case(\n            train_range=case[\"train_range\"],\n            lambda_smooth=case[\"lambda_smooth\"],\n            test_angles=case[\"test_angles\"]\n        )\n        results.append(mse)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.7f}' for r in results)}]\")\n\ndef solve_case(train_range, lambda_smooth, test_angles):\n    \"\"\"\n    Solves for the optimal parameters W* and evaluates the MSE for a single case.\n\n    Args:\n        train_range (list): A list [min_angle, max_angle] for training.\n        lambda_smooth (float): The smoothness regularization weight.\n        test_angles (list): A list of angles for evaluation.\n\n    Returns:\n        float: The calculated Mean Squared Error.\n    \"\"\"\n    N = 101 # Number of training samples\n\n    # 1. Construct training data\n    y_train = np.linspace(train_range[0], train_range[1], N)\n\n    # 2. Construct design matrices and target vectors\n    # Basis function matrix Phi, where Phi[i,:] = [1, y_i, y_i^2]\n    phi_train = np.c_[np.ones(N), y_train, y_train**2]\n    \n    # Derivative basis matrix Phi', where Phi'[i,:] = [0, 1, 2*y_i]\n    phi_prime_train = np.c_[np.zeros(N), np.ones(N), 2 * y_train]\n\n    # Target vectors M1 and M2 for cos(y) and sin(y)\n    m1_train = np.cos(y_train)\n    m2_train = np.sin(y_train)\n\n    # 3. Formulate and solve the normal equations\n    # A = (Phi^T * Phi + lambda * Phi'^T * Phi')\n    A = phi_train.T @ phi_train + lambda_smooth * (phi_prime_train.T @ phi_prime_train)\n\n    # b1 = Phi^T * M1\n    b1 = phi_train.T @ m1_train\n    # b2 = Phi^T * M2\n    b2 = phi_train.T @ m2_train\n\n    # Solve the linear systems A*w1 = b1 and A*w2 = b2\n    w1_star = np.linalg.solve(A, b1)\n    w2_star = np.linalg.solve(A, b2)\n\n    # The optimal weight matrix W*\n    W_star = np.array([w1_star, w2_star])\n\n    # 4. Evaluate the model on test angles\n    y_test = np.array(test_angles)\n    N_test = len(y_test)\n\n    # Construct basis matrix for test angles\n    phi_test = np.c_[np.ones(N_test), y_test, y_test**2]\n\n    # Predicted mean values u*(y) = W* * phi(y)\n    # phi_test has shape (N_test, 3), W_star has shape (2, 3)\n    # We want result shape (N_test, 2), so we compute (phi_test @ W_star.T)\n    u_star_test = phi_test @ W_star.T\n\n    # True mean values m(y)\n    m_test = np.c_[np.cos(y_test), np.sin(y_test)]\n\n    # 5. Calculate MSE\n    # Squared L2 norm of the error for each test point\n    squared_errors = np.sum((u_star_test - m_test)**2, axis=1)\n\n    # Average over test points and the two output dimensions\n    mse = np.mean(squared_errors) / 2.0\n    \n    return mse\n\nsolve()\n```",
            "answer": "[0.0003058,0.0028206,0.0024467,0.1252994]"
        },
        {
            "introduction": "训练好一个生成器后，我们如何确定它生成的数据真正符合给定的条件？这个实践介绍了一种量化方法来衡量这种“条件忠实度”($y$-faithfulness)。你将使用真实数据训练一个分类器，并用它来测试生成样本能否被正确识别，从而为你提供一种评估生成器性能的原则性方法。",
            "id": "3108919",
            "problem": "给定一个条件生成对抗网络 (cGAN) 的数学模型，其生成器根据标签生成特征。您的任务是通过训练一个冻结的分类器并计算期望 $\\mathbb{E}_{z,y}[\\mathbb{1}\\{C(G(z,y))=y\\}]$ 的经验近似值，来量化生成器的“$y$-忠实度”，然后将其与判别器的辅助标签头在相同生成数据上的准确率进行比较。所有计算都必须使用高斯生成模型和贝叶斯决策理论从第一性原理推导，并进行数值实现。\n\n定义和假设：\n- 潜变量 $z \\in \\mathbb{R}^{d}$ 从具有独立分量的标准多变量正态分布中抽取，即 $z \\sim \\mathcal{N}(0, I_{d})$。\n- 标签 $y \\in \\{0,1,\\dots,K-1\\}$ 从指定的先验分布 $\\pi(y)$ 中抽取。\n- 生成器由生成数据 $x \\in \\mathbb{R}^{d}$ 的类条件分布定义，给定为\n  $$x \\mid y \\sim \\mathcal{N}(\\mu^{\\mathrm{gen}}_{y}, \\Sigma_{\\mathrm{gen}}), \\quad \\Sigma_{\\mathrm{gen}} = B B^{\\top},$$\n  其中 $B \\in \\mathbb{R}^{d \\times d}$ 是一个给定的满秩矩阵，$(\\mu^{\\mathrm{gen}}_{y})_{y=0}^{K-1}$ 是给定的类别均值。\n- 用于训练冻结分类器 $C$ 的“真实”数据分布是具有共享协方差的高斯分布：\n  $$x \\mid y \\sim \\mathcal{N}(\\mu^{\\mathrm{real}}_{y}, \\Sigma_{\\mathrm{real}}),$$\n  其中共享协方差矩阵 $\\Sigma_{\\mathrm{real}} \\in \\mathbb{R}^{d \\times d}$ 和类别均值 $(\\mu^{\\mathrm{real}}_{y})_{y=0}^{K-1}$ 已知。\n\n训练协议：\n- 冻结分类器 $C$ 使用线性判别分析 (LDA) 进行训练，这是在类条件高斯分布和共享协方差假设下的贝叶斯最优分类器。您必须从真实数据分布中为每个类别抽取 $n_{C}$ 个样本，通过最大似然估计来估计类别均值和共享协方差；然后冻结 $C$。\n- 判别器的标签头，记作 $D_{y}$，使用从生成器的类条件分布（即，在生成数据上）为每个类别抽取的 $n_{D}$ 个样本，单独训练为一个 LDA 分类器。这模拟了在 cGAN 设置中训练的、用于从生成数据中识别 $y$ 的判别器辅助分类器。\n\n评估指标：\n- 定义生成器相对于冻结分类器 $C$ 的 $y$-忠实度为\n  $$\\mathbb{E}_{z,y}[\\mathbb{1}\\{C(G(z,y))=y\\}],$$\n  其中期望是在 $y \\sim \\pi(y)$ 和 $z \\sim \\mathcal{N}(0, I_{d})$ 上计算的。您必须使用 $n_{\\mathrm{eval}}$ 个独立样本通过蒙特卡洛方法来近似这个期望。\n- 在相同的生成评估集上，计算 $D_{y}$ 的分类准确率：\n  $$\\mathbb{E}_{z,y}[\\mathbb{1}\\{D_{y}(G(z,y))=y\\}],$$\n  同样使用相同的 $n_{\\mathrm{eval}}$ 个样本通过蒙特卡洛方法来近似。\n\n您必须使用的基础理论：\n- 在具有共享协方差的高斯类条件模型下的贝叶斯决策规则会导出线性判别函数。必须使用类别均值和合并共享协方差的最大似然估计来实例化 LDA 分类器。不允许使用其他分类“捷径”。\n\n蒙特卡洛近似细节：\n- 为了近似 $\\mathbb{E}_{z,y}[\\cdot]$，对于 $i \\in \\{1,\\dots,n_{\\mathrm{eval}}\\}$，独立地抽样标签 $y_{i} \\sim \\pi(y)$ 和潜变量 $z_{i} \\sim \\mathcal{N}(0, I_{d})$，然后生成 $x_{i} = G(z_{i}, y_{i})$，其中 $x_{i} \\sim \\mathcal{N}(\\mu^{\\mathrm{gen}}_{y_{i}}, \\Sigma_{\\mathrm{gen}})$，并计算指示函数的经验平均值。您必须使用固定的随机种子 $0$ 以保证可复现性。\n\n测试套件：\n对于以下每种情况，您必须：\n- 在真实数据上使用每个类别 $n_{C}$ 个样本训练冻结分类器 $C$。\n- 在生成数据上使用每个类别 $n_{D}$ 个样本训练 $D_{y}$。\n- 在一个包含 $n_{\\mathrm{eval}}$ 个、按指定 $\\pi(y)$ 抽取的生成样本的评估集上评估两者。\n\n所有情况均使用维度 $d=2$。在所有情况下，共享的真实协方差为 $\\Sigma_{\\mathrm{real}} = \\sigma_{\\mathrm{real}}^{2} I_{2}$，生成器协方差为 $\\Sigma_{\\mathrm{gen}} = \\sigma_{\\mathrm{gen}}^{2} I_{2}$，其中 $B=\\sigma_{\\mathrm{gen}} I_{2}$。\n\n- 情况 1 (理想情况, $K=3$):\n  - $\\mu^{\\mathrm{real}}_{0} = (0,0)$, $\\mu^{\\mathrm{real}}_{1} = (3,0)$, $\\mu^{\\mathrm{real}}_{2} = (0,3)$。\n  - $\\mu^{\\mathrm{gen}}_{0} = (0.1,-0.1)$, $\\mu^{\\mathrm{gen}}_{1} = (3.1,0.2)$, $\\mu^{\\mathrm{gen}}_{2} = (-0.1,3.2)$。\n  - $\\sigma_{\\mathrm{real}} = 0.2$, $\\sigma_{\\mathrm{gen}} = 0.25$。\n  - $n_{C} = 50$, $n_{D} = 30$, $n_{\\mathrm{eval}} = 3000$。\n  - $\\pi(y) = (1/3, 1/3, 1/3)$。\n\n- 情况 2 (高重叠度, $K=3$):\n  - $\\mu^{\\mathrm{real}}_{0} = (0,0)$, $\\mu^{\\mathrm{real}}_{1} = (1.0,0.2)$, $\\mu^{\\mathrm{real}}_{2} = (0.2,1.0)$。\n  - $\\mu^{\\mathrm{gen}}_{0} = (0,0)$, $\\mu^{\\mathrm{gen}}_{1} = (1.0,0.2)$, $\\mu^{\\mathrm{gen}}_{2} = (0.2,1.0)$。\n  - $\\sigma_{\\mathrm{real}} = 0.6$, $\\sigma_{\\mathrm{gen}} = 0.7$。\n  - $n_{C} = 200$, $n_{D} = 50$, $n_{\\mathrm{eval}} = 3000$。\n  - $\\pi(y) = (1/3, 1/3, 1/3)$。\n\n- 情况 3 (非均匀标签先验, $K=3$):\n  - $\\mu^{\\mathrm{real}}_{0} = (0,0)$, $\\mu^{\\mathrm{real}}_{1} = (3,0)$, $\\mu^{\\mathrm{real}}_{2} = (0,3)$。\n  - $\\mu^{\\mathrm{gen}}_{0} = (0.1,-0.1)$, $\\mu^{\\mathrm{gen}}_{1} = (3.1,0.2)$, $\\mu^{\\mathrm{gen}}_{2} = (-0.1,3.2)$。\n  - $\\sigma_{\\mathrm{real}} = 0.2$, $\\sigma_{\\mathrm{gen}} = 0.25$。\n  - $n_{C} = 50$, $n_{D} = 30$, $n_{\\mathrm{eval}} = 4000$。\n  - $\\pi(y) = (0.8, 0.1, 0.1)$。\n\n- 情况 4 (判别器训练样本非常少, $K=2$):\n  - $\\mu^{\\mathrm{real}}_{0} = (0,0)$, $\\mu^{\\mathrm{real}}_{1} = (2.5,0)$。\n  - $\\mu^{\\mathrm{gen}}_{0} = (0.2,-0.1)$, $\\mu^{\\mathrm{gen}}_{1} = (2.6,0.1)$。\n  - $\\sigma_{\\mathrm{real}} = 0.3$, $\\sigma_{\\mathrm{gen}} = 0.3$。\n  - $n_{C} = 40$, $n_{D} = 3$, $n_{\\mathrm{eval}} = 2000$。\n  - $\\pi(y) = (0.5, 0.5)$。\n\n- 情况 5 (近乎完美分离, $K=3$):\n  - $\\mu^{\\mathrm{real}}_{0} = (0,0)$, $\\mu^{\\mathrm{real}}_{1} = (5,5)$, $\\mu^{\\mathrm{real}}_{2} = (-5,5)$。\n  - $\\mu^{\\mathrm{gen}}_{0} = (0,0)$, $\\mu^{\\mathrm{gen}}_{1} = (5,5)$, $\\mu^{\\mathrm{gen}}_{2} = (-5,5)$。\n  - $\\sigma_{\\mathrm{real}} = 0.2$, $\\sigma_{\\mathrm{gen}} = 0.1$。\n  - $n_{C} = 20$, $n_{D} = 20$, $n_{\\mathrm{eval}} = 3000$。\n  - $\\pi(y) = (1/3, 1/3, 1/3)$。\n\n算法要求：\n- 从第一性原理实现 LDA：从训练数据中估计类别均值 $\\hat{\\mu}_{k}$ 和合并协方差 $\\hat{\\Sigma}$，添加一个小的脊项 $\\lambda I_{d}$（其中 $\\lambda = 10^{-6}$）以确保可逆性，计算判别分数进行预测，并使用从训练频率估计的类别先验。\n- 使用相同的生成评估集计算两种准确率，以进行公平比较。\n- 将随机种子固定为 $0$。\n\n要求的最终输出格式：\n- 您的程序应生成一行输出，包含一个由数对组成的列表，其中每个数对是某个情况下的 $[\\text{acc}_{C}, \\text{acc}_{D}]$，顺序与上述情况一致。例如：\"[[0.95,0.93],[...],...]\"。列表中的条目必须是浮点数。\n\n不涉及物理单位或角度单位。所有数值答案必须是十进制形式的浮点数。程序必须是完整的、可按规定运行的，且不得需要任何用户输入。",
            "solution": "此问题提出了一个在统计机器学习领域中定义明确、有科学依据的计算任务。它要求在一个模拟的条件生成对抗网络 (cGAN) 框架内，实现并比较两种基于线性判别分析 (LDA) 的分类器。所有参数、模型和评估程序都得到了完整且一致的规定。\n\n解决方案首先建立 LDA 作为给定问题背景下的贝叶斯最优分类器的理论基础。然后，我们详细说明训练和评估过程的数值实现。\n\n**1. 理论基础：线性判别分析**\n\n该问题假设“真实”数据和“生成”数据在各自的域（真实和生成）内都遵循具有共享协方差矩阵的类条件高斯分布。对于一个有 $K$ 个类别的分类问题，其中类别 $k \\in \\{0, \\dots, K-1\\}$ 的数据 $x \\in \\mathbb{R}^d$ 从正态分布 $\\mathcal{N}(\\mu_k, \\Sigma)$ 中抽取，贝叶斯最优决策规则将 $x$ 分配给能最大化后验概率 $P(y=k|x)$ 的类别。使用贝叶斯定理，这等价于最大化 $p(x|y=k)P(y=k)$。取对数并舍去对所有类别都为常数的项后，我们寻求最大化判别分数 $\\delta_k(x)$：\n$$ \\delta_k(x) = \\log p(x|y=k) + \\log P(y=k) $$\n代入多变量正态分布的概率密度函数：\n$$ \\log p(x|y=k) = -\\frac{1}{2}(x - \\mu_k)^T \\Sigma^{-1} (x - \\mu_k) - \\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\Sigma| $$\n展开二次项并舍去与 $k$ 无关的项（即 $x^T\\Sigma^{-1}x$、$\\frac{d}{2}\\log(2\\pi)$、$\\frac{1}{2}\\log|\\Sigma|$），决策规则简化为最大化以下关于 $x$ 的线性函数：\n$$ \\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2}\\mu_k^T \\Sigma^{-1} \\mu_k + \\log \\pi_k $$\n其中 $\\pi_k = P(y=k)$ 是类别 $k$ 的先验概率。任意两个类别 $i$ 和 $j$ 之间的决策边界是满足 $\\delta_i(x) = \\delta_j(x)$ 的点集，这定义了一个超平面。因此，该分类器是一个线性分类器。\n\n**2. 从训练数据中进行参数估计**\n\n在实践中，真实参数 $\\mu_k$、$\\Sigma$ 和 $\\pi_k$ 是未知的，必须从训练集 $\\{(x_i, y_i)\\}_{i=1}^N$ 中估计。我们使用最大似然估计 (MLE)，得到以下估计值：\n- **类别先验 ($\\hat{\\pi}_k$):** 训练样本中类别 $k$ 的比例。对于一个有 $N_k$ 个类别 $k$ 样本和总共 $N$ 个样本的训练集，$\\hat{\\pi}_k = N_k / N$。\n- **类别均值 ($\\hat{\\mu}_k$):** 所有属于类别 $k$ 的训练数据的样本均值。\n$$ \\hat{\\mu}_k = \\frac{1}{N_k} \\sum_{i: y_i=k} x_i $$\n- **合并协方差 ($\\hat{\\Sigma}$):** 各个类别协方差矩阵的加权平均，权重为其自由度。无偏估计量为：\n$$ \\hat{\\Sigma} = \\frac{1}{N-K} \\sum_{k=0}^{K-1} S_k = \\frac{1}{N-K} \\sum_{k=0}^{K-1} \\sum_{i: y_i=k} (x_i - \\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^T $$\n为了数值稳定性，特别是当样本数量不远大于维度 $d$ 时，估计的协方差矩阵在求逆前通过加上一个小的单位矩阵倍数 $\\lambda I_d$ 来进行正则化。\n\n**3. 模拟与评估程序**\n\n每个测试用例的总体流程如下：\n\n1.  **实例化分类器：** 我们定义两个 LDA 分类器，$C$ 和 $D_y$。\n\n2.  **训练冻结分类器 $C$：**\n    - 通过从“真实”数据分布 $x \\mid y \\sim \\mathcal{N}(\\mu^{\\mathrm{real}}_{y}, \\Sigma_{\\mathrm{real}})$ 为每个类别抽取 $n_C$ 个样本来生成一个训练集。\n    - 在此数据集上训练分类器 $C$ 以学习参数 $(\\hat{\\pi}^{\\mathrm{real}}, \\hat{\\mu}^{\\mathrm{real}}, \\hat{\\Sigma}^{\\mathrm{real}})$。由于训练数据是平衡的，$\\hat{\\pi}_k^{\\mathrm{real}} = 1/K$。\n\n3.  **训练判别器头 $D_y$：**\n    - 通过从生成器分布 $x \\mid y \\sim \\mathcal{N}(\\mu^{\\mathrm{gen}}_{y}, \\Sigma_{\\mathrm{gen}})$ 为每个类别抽取 $n_D$ 个样本来生成一个单独的训练集。\n    - 在此生成的数据集上训练分类器 $D_y$ 以学习其自己的一套参数 $(\\hat{\\pi}^{\\mathrm{gen}}, \\hat{\\mu}^{\\mathrm{gen}}, \\hat{\\Sigma}^{\\mathrm{gen}})$。同样，$\\hat{\\pi}_k^{\\mathrm{gen}} = 1/K$。\n\n4.  **蒙特卡洛评估：**\n    - 创建一个包含 $n_{\\mathrm{eval}}$ 个样本的单一公共评估集。首先从指定的先验分布 $\\pi(y)$ 中抽样 $n_{\\mathrm{eval}}$ 个标签 $\\{y_i\\}_{i=1}^{n_{\\mathrm{eval}}}$。然后，对于每个标签 $y_i$，从生成器分布 $\\mathcal{N}(\\mu^{\\mathrm{gen}}_{y_i}, \\Sigma_{\\mathrm{gen}})$ 中抽样一个对应的数据点 $x_i$。\n    - “$y$-忠实度”是分类器 $C$ 在此评估集上的准确率：$ \\mathrm{acc}_C = \\frac{1}{n_{\\mathrm{eval}}} \\sum_{i=1}^{n_{\\mathrm{eval}}} \\mathbb{1}\\{C(x_i)=y_i\\} $。\n    - 判别器的准确率是 $D_y$ 在同一评估集上的准确率：$ \\mathrm{acc}_{D_y} = \\frac{1}{n_{\\mathrm{eval}}} \\sum_{i=1}^{n_{\\mathrm{eval}}} \\mathbb{1}\\{D_y(x_i)=y_i\\} $。\n\n该程序允许直接比较一个在真实数据上训练的分类器如何看待生成器的输出，与一个在生成器自身输出上训练的辅助分类器的表现。它们准确率的差异凸显了真实数据分布与生成数据分布之间的差异。整个模拟过程使用固定的随机种子执行，以确保可复现性。\n```python\nimport numpy as np\n\n# A global random number generator to be used throughout the simulation for reproducibility.\nRNG = np.random.default_rng(0)\n\nclass LDAClassifier:\n    \"\"\"\n    A Linear Discriminant Analysis (LDA) classifier implemented from first principles.\n    Assumes Gaussian class-conditional distributions with a shared covariance matrix.\n    \"\"\"\n    def __init__(self, ridge_lambda=1e-6):\n        self.ridge_lambda = ridge_lambda\n        self.means = None\n        self.priors = None\n        self.inv_cov = None\n        self.discriminant_constants = None\n        self.n_classes = 0\n\n    def fit(self, X, y):\n        \"\"\"\n        Estimates LDA parameters from training data (X, y).\n        - Class priors from training frequencies.\n        - Class means.\n        - Pooled covariance matrix.\n        \"\"\"\n        n_samples, n_features = X.shape\n        unique_classes = np.unique(y)\n        self.n_classes = len(unique_classes)\n\n        # Estimate class priors from training frequencies\n        class_counts = np.array([np.sum(y == k) for k in range(self.n_classes)])\n        self.priors = class_counts / n_samples\n\n        # Estimate class means\n        self.means = np.array([X[y == k].mean(axis=0) for k in range(self.n_classes)])\n\n        # Estimate pooled covariance matrix\n        pooled_cov = np.zeros((n_features, n_features))\n        if n_samples > self.n_classes:\n            for k in range(self.n_classes):\n                class_samples = X[y == k]\n                if class_counts[k] > 0:\n                    residuals = class_samples - self.means[k]\n                    # Sum of squares matrix S_k = sum (x_i - mu_k)(x_i - mu_k)^T\n                    pooled_cov += residuals.T @ residuals\n            # Unbiased estimator denominator is N-K\n            pooled_cov /= (n_samples - self.n_classes)\n        else:\n            # Fallback for insufficient data (N = K)\n            pooled_cov = np.identity(n_features)\n\n        # Add ridge regularization and compute inverse for numerical stability\n        reg_cov = pooled_cov + self.ridge_lambda * np.identity(n_features)\n        self.inv_cov = np.linalg.inv(reg_cov)\n        \n        # Pre-compute parts of the discriminant function for efficient prediction\n        # delta_k(x) = x.T @ inv_cov @ mu_k - 0.5 * mu_k.T @ inv_cov @ mu_k + log(pi_k)\n        log_priors = np.log(self.priors + 1e-12)  # Epsilon for log(0)\n        self.discriminant_constants = -0.5 * np.sum((self.means @ self.inv_cov) * self.means, axis=1) + log_priors\n\n    def predict(self, X):\n        \"\"\"\n        Predicts class labels for new data X.\n        \"\"\"\n        # Linear part of discriminant: X @ inv_cov @ means.T\n        scores = X @ self.inv_cov @ self.means.T\n        # Add constant part\n        scores += self.discriminant_constants\n        return np.argmax(scores, axis=1)\n\ndef generate_balanced_data(means, cov, n_samples_per_class, K):\n    \"\"\"Generates a balanced dataset with n_samples_per_class for each class.\"\"\"\n    d = cov.shape[0]\n    X = np.empty((n_samples_per_class * K, d))\n    y = np.empty(n_samples_per_class * K, dtype=int)\n    for k in range(K):\n        start_idx = k * n_samples_per_class\n        end_idx = (k + 1) * n_samples_per_class\n        X[start_idx:end_idx] = RNG.multivariate_normal(means[k], cov, size=n_samples_per_class)\n        y[start_idx:end_idx] = k\n    return X, y\n\ndef generate_eval_data(means, cov, n_eval, priors, K):\n    \"\"\"Generates an evaluation dataset with labels sampled from the prior distribution.\"\"\"\n    d = cov.shape[0]\n    # Sample labels from the prior distribution\n    y_eval = RNG.choice(K, size=n_eval, p=priors)\n    X_eval = np.empty((n_eval, d))\n    for k in range(K):\n        # Generate data for all samples of class k at once\n        class_mask = (y_eval == k)\n        n_k = np.sum(class_mask)\n        if n_k > 0:\n            X_eval[class_mask] = RNG.multivariate_normal(means[k], cov, size=n_k)\n    return X_eval, y_eval\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, K=3)\n        {\n            'K': 3, 'd': 2,\n            'mu_real': np.array([[0,0], [3,0], [0,3]]),\n            'mu_gen': np.array([[0.1,-0.1], [3.1,0.2], [-0.1,3.2]]),\n            'sigma_real': 0.2, 'sigma_gen': 0.25,\n            'n_C': 50, 'n_D': 30, 'n_eval': 3000,\n            'pi_y': np.array([1/3, 1/3, 1/3])\n        },\n        # Case 2 (high overlap, K=3)\n        {\n            'K': 3, 'd': 2,\n            'mu_real': np.array([[0,0], [1.0,0.2], [0.2,1.0]]),\n            'mu_gen': np.array([[0,0], [1.0,0.2], [0.2,1.0]]),\n            'sigma_real': 0.6, 'sigma_gen': 0.7,\n            'n_C': 200, 'n_D': 50, 'n_eval': 3000,\n            'pi_y': np.array([1/3, 1/3, 1/3])\n        },\n        # Case 3 (non-uniform label prior, K=3)\n        {\n            'K': 3, 'd': 2,\n            'mu_real': np.array([[0,0], [3,0], [0,3]]),\n            'mu_gen': np.array([[0.1,-0.1], [3.1,0.2], [-0.1,3.2]]),\n            'sigma_real': 0.2, 'sigma_gen': 0.25,\n            'n_C': 50, 'n_D': 30, 'n_eval': 4000,\n            'pi_y': np.array([0.8, 0.1, 0.1])\n        },\n        # Case 4 (very small discriminator training, K=2)\n        {\n            'K': 2, 'd': 2,\n            'mu_real': np.array([[0,0], [2.5,0]]),\n            'mu_gen': np.array([[0.2,-0.1], [2.6,0.1]]),\n            'sigma_real': 0.3, 'sigma_gen': 0.3,\n            'n_C': 40, 'n_D': 3, 'n_eval': 2000,\n            'pi_y': np.array([0.5, 0.5])\n        },\n        # Case 5 (near-perfect separation, K=3)\n        {\n            'K': 3, 'd': 2,\n            'mu_real': np.array([[0,0], [5,5], [-5,5]]),\n            'mu_gen': np.array([[0,0], [5,5], [-5,5]]),\n            'sigma_real': 0.2, 'sigma_gen': 0.1,\n            'n_C': 20, 'n_D': 20, 'n_eval': 3000,\n            'pi_y': np.array([1/3, 1/3, 1/3])\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        K, d = params['K'], params['d']\n        cov_real = (params['sigma_real']**2) * np.identity(d)\n        cov_gen = (params['sigma_gen']**2) * np.identity(d)\n\n        # 1. Train Classifier C on \"real\" data\n        X_real_train, y_real_train = generate_balanced_data(\n            params['mu_real'], cov_real, params['n_C'], K\n        )\n        classifier_C = LDAClassifier()\n        classifier_C.fit(X_real_train, y_real_train)\n        \n        # 2. Train Classifier D_y on \"generated\" data\n        X_gen_train, y_gen_train = generate_balanced_data(\n            params['mu_gen'], cov_gen, params['n_D'], K\n        )\n        classifier_Dy = LDAClassifier()\n        classifier_Dy.fit(X_gen_train, y_gen_train)\n        \n        # 3. Create common evaluation set from the generator\n        X_eval, y_eval = generate_eval_data(\n            params['mu_gen'], cov_gen, params['n_eval'], params['pi_y'], K\n        )\n        \n        # 4. Evaluate both classifiers on the same evaluation set\n        # Accuracy of C (y-faithfulness)\n        y_pred_C = classifier_C.predict(X_eval)\n        acc_C = np.mean(y_pred_C == y_eval)\n        \n        # Accuracy of Dy\n        y_pred_Dy = classifier_Dy.predict(X_eval)\n        acc_Dy = np.mean(y_pred_Dy == y_eval)\n        \n        results.append([acc_C, acc_Dy])\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```",
            "answer": "[[0.9986666666666667,0.9986666666666667],[0.7303333333333333,0.7163333333333334],[0.9995,0.9995],[0.998,0.9915],[1.0,1.0]]"
        },
        {
            "introduction": "生成模型最令人兴奋的应用之一是创造不同输出之间的平滑过渡或“变形”。本练习通过“混合”条件向量来探索这一特性。通过比较混合条件下的生成器输出与原始条件输出的混合结果，你将深入理解生成器的架构如何影响其产生线性插值的能力。",
            "id": "3108852",
            "problem": "您正在研究一个条件生成对抗网络（conditional Generative Adversarial Network (cGAN)）的生成器，当使用凸组合混合条件时，是否可以在两个条件向量之间实现线性渐变。考虑一个确定性生成器，其输出维度为 $d_x$，噪声维度为 $d_z$，条件维度为 $d_y$。对于任意噪声向量 $z \\in \\mathbb{R}^{d_z}$ 和条件向量 $y \\in \\mathbb{R}^{d_y}$，生成器产生一个输出 $G(z,y) \\in \\mathbb{R}^{d_x}$。假设生成器是一个单一的仿射层，后跟一个按元素激活函数，即对于参数 $A \\in \\mathbb{R}^{d_x \\times (d_z + d_y)}$ 和 $b \\in \\mathbb{R}^{d_x}$，\n$$\nG(z,y) \\triangleq \\phi\\left(A \\begin{bmatrix} z \\\\ y \\end{bmatrix} + b\\right),\n$$\n其中 $\\phi$ 是恒等函数（表示为 $\\mathrm{id}$）或双曲正切函数（表示为 $\\tanh$），按元素应用。拼接操作 $\\begin{bmatrix} z \\\\ y \\end{bmatrix}$ 将 $z$ 和 $y$ 堆叠成 $\\mathbb{R}^{d_z + d_y}$ 中的单个向量。\n\n在条件混合下，两个标签 $y_1, y_2 \\in \\mathbb{R}^{d_y}$ 通过凸权重 $\\lambda \\in [0,1]$ 组合成 $\\tilde{y} = \\lambda y_1 + (1-\\lambda) y_2$。输出空间中预期的线性渐变是凸组合 $\\lambda G(z,y_1) + (1-\\lambda) G(z,y_2)$。您的目标是通过比较 $G(z,\\tilde{y})$ 与端点输出的凸组合来衡量与完美线性的偏差。\n\n从上述定义以及仿射映射和凸组合的线性性质出发，推导一个标量误差泛函，该泛函通过输出坐标上平方差的平均值来量化在给定 $\\lambda$ 下的线性偏差。然后，实现一个程序，为每个指定的测试用例和每个指定的 $\\lambda$ 计算此误差，并将所有结果输出到一个单一的扁平列表中。\n\n将给定 $\\lambda$ 下的线性误差定义如下：首先计算混合条件下生成器输出与端点输出的凸组合之间的差分向量，然后计算该差分向量各分量平方的均值。因此，对于每个 $\\lambda$，您的最终误差必须是一个实数 $\\mathbb{R}$，通过对 $d_x$ 个输出坐标的平方值求平均得到。\n\n使用以下测试套件。每个用例指定了 $(A,b,z,y_1,y_2,\\phi,\\Lambda)$，其中 $\\Lambda$ 是要评估的 $\\lambda$ 值的列表。\n\n- 用例 $1$ (理想情况；线性生成器，因此对于所有 $\\lambda$ 预期误差为零):\n  - $d_z = 1$, $d_y = 2$, $d_x = 2$\n  - $A = \\begin{bmatrix} 1.0  0.5  -0.25 \\\\ 0.2  -0.1  0.3 \\end{bmatrix}$, $b = \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix}$\n  - $z = \\begin{bmatrix} 0.7 \\end{bmatrix}$, $y_1 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$, $y_2 = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$\n  - $\\phi = \\mathrm{id}$\n  - $\\Lambda = [\\,0.0,\\,0.25,\\,0.5,\\,0.75,\\,1.0\\,]$\n\n- 用例 $2$ (非线性生成器；内部的 $\\lambda$ 值通常会产生正误差):\n  - $d_z = 1$, $d_y = 2$, $d_x = 2$\n  - $A = \\begin{bmatrix} 0.8  1.2  -0.9 \\\\ -0.5  0.3  0.7 \\end{bmatrix}$, $b = \\begin{bmatrix} 0.05 \\\\ -0.1 \\end{bmatrix}$\n  - $z = \\begin{bmatrix} -0.3 \\end{bmatrix}$, $y_1 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$, $y_2 = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$\n  - $\\phi = \\tanh$\n  - $\\Lambda = [\\,0.0,\\,0.25,\\,0.5,\\,0.75,\\,1.0\\,]$\n\n- 用例 $3$ (边界条件；任何生成器在 $\\lambda \\in \\{0,1\\}$ 时都必须产生零误差):\n  - $d_z = 1$, $d_y = 2$, $d_x = 2$\n  - $A = \\begin{bmatrix} -0.6  0.4  0.9 \\\\ 0.3  -0.8  0.2 \\end{bmatrix}$, $b = \\begin{bmatrix} -0.05 \\\\ 0.2 \\end{bmatrix}$\n  - $z = \\begin{bmatrix} 1.1 \\end{bmatrix}$, $y_1 = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$, $y_2 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$\n  - $\\phi = \\tanh$\n  - $\\Lambda = [\\,0.0,\\,1.0\\,]$\n\n- 用例 $4$ (退化条件；相同的标签意味着对于所有 $\\lambda$ 误差都为零):\n  - $d_z = 1$, $d_y = 2$, $d_x = 2$\n  - $A = \\begin{bmatrix} 0.8  1.2  -0.9 \\\\ -0.5  0.3  0.7 \\end{bmatrix}$, $b = \\begin{bmatrix} 0.05 \\\\ -0.1 \\end{bmatrix}$\n  - $z = \\begin{bmatrix} 0.2 \\end{bmatrix}$, $y_1 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$, $y_2 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$\n  - $\\phi = \\tanh$\n  - $\\Lambda = [\\,0.2,\\,0.4,\\,0.6,\\,0.8\\,]$\n\n实现要求：\n\n- 对于每个用例和 $\\Lambda$ 中的每个 $\\lambda$，计算上述标量误差。\n- 最终输出必须是单行，其中包含按上述顺序列出的、跨所有用例和所有 $\\Lambda$ 值的全部误差的拼接，形式为用方括号括起来的逗号分隔列表。例如，如果一个用例产生三个误差 $e_1, e_2, e_3$，它们必须显示为 $[e_1,e_2,e_3,\\dots]$，不含空格。每个误差必须打印为浮点数。为了可读性，您可以四舍五入到固定的小数位数，但对于指定的定义和参数，这些值在数值上必须是正确的。\n\n本问题不涉及物理单位或角度。所有数值输出都必须是实数。",
            "solution": "此问题在科学上基于线性代数和深度学习的数学原理，提法明确，提供了所有必要信息。因此，可以进行推导和求解。\n\n该问题要求我们量化一个简化的条件GAN生成器模型在条件输入的凸组合下的线性偏差。生成器 $G$ 定义为\n$$\nG(z,y) \\triangleq \\phi\\left(A \\begin{bmatrix} z \\\\ y \\end{bmatrix} + b\\right)\n$$\n其中 $z \\in \\mathbb{R}^{d_z}$ 是噪声向量，$y \\in \\mathbb{R}^{d_y}$ 是条件向量，$A \\in \\mathbb{R}^{d_x \\times (d_z + d_y)}$ 是权重矩阵，$b \\in \\mathbb{R}^{d_x}$ 是偏置向量，$\\phi$ 是一个按元素激活函数。生成器的输出是 $\\mathbb{R}^{d_x}$ 中的一个向量。\n\n给定两个条件向量 $y_1$ 和 $y_2$，它们通过一个凸权重 $\\lambda \\in [0,1]$ 进行混合，形成一个新的条件向量 $\\tilde{y} = \\lambda y_1 + (1-\\lambda) y_2$。目标是将此混合输入的生成器输出 $G(z,\\tilde{y})$ 与对应于原始条件向量的输出的理想线性插值 $\\lambda G(z,y_1) + (1-\\lambda) G(z,y_2)$ 进行比较。\n\n误差定义为这两个量之间的均方差。让我们首先推导误差泛函 $E(\\lambda)$ 的一个更显式的表达式。\n\n令仿射变换表示为 $T(v) = Av+b$，其中 $v = \\begin{bmatrix} z \\\\ y \\end{bmatrix}$。我们可以将矩阵 $A$ 分割成两个子矩阵，$A_z \\in \\mathbb{R}^{d_x \\times d_z}$ 和 $A_y \\in \\mathbb{R}^{d_x \\times d_y}$，分别对应于噪声和条件输入，使得 $A = [A_z \\mid A_y]$。该变换可以写成：\n$$\nA \\begin{bmatrix} z \\\\ y \\end{bmatrix} + b = A_z z + A_y y + b\n$$\n我们将条件输入 $y_1$ 和 $y_2$ 的预激活向量定义为 $u_1$ 和 $u_2$：\n$$\nu_1 \\triangleq A_z z + A_y y_1 + b\n$$\n$$\nu_2 \\triangleq A_z z + A_y y_2 + b\n$$\n对应的生成器输出为 $G(z,y_1) = \\phi(u_1)$ 和 $G(z,y_2) = \\phi(u_2)$。\n\n现在，考虑混合条件向量 $\\tilde{y} = \\lambda y_1 + (1-\\lambda) y_2$。该输入的预激活（我们表示为 $\\tilde{u}$）是：\n$$\n\\tilde{u} \\triangleq A_z z + A_y \\tilde{y} + b = A_z z + A_y (\\lambda y_1 + (1-\\lambda) y_2) + b\n$$\n根据矩阵乘法的线性性质，我们分配 $A_y$：\n$$\n\\tilde{u} = A_z z + \\lambda A_y y_1 + (1-\\lambda) A_y y_2 + b\n$$\n由于 $b = \\lambda b + (1-\\lambda)b$，我们可以通过重组含 $\\lambda$ 的项来重写 $\\tilde{u}$：\n$$\n\\tilde{u} = \\lambda (A_z z + A_y y_1 + b) + (1-\\lambda) (A_z z + A_y y_2 + b)\n$$\n这简化为原始预激活向量的凸组合：\n$$\n\\tilde{u} = \\lambda u_1 + (1-\\lambda) u_2\n$$\n因此，混合条件的生成器输出为 $G(z,\\tilde{y}) = \\phi(\\tilde{u}) = \\phi(\\lambda u_1 + (1-\\lambda) u_2)$。\n\n衡量与完美线性偏差的差分向量 $\\Delta$ 由下式给出：\n$$\n\\Delta(\\lambda) = G(z, \\tilde{y}) - \\left[ \\lambda G(z, y_1) + (1-\\lambda) G(z, y_2) \\right]\n$$\n代入我们的表达式，我们得到：\n$$\n\\Delta(\\lambda) = \\phi(\\lambda u_1 + (1-\\lambda) u_2) - \\left[ \\lambda \\phi(u_1) + (1-\\lambda) \\phi(u_2) \\right]\n$$\n该表达式表明，线性偏差完全取决于激活函数 $\\phi$ 是否对向量 $x, y$ 满足属性 $f(\\lambda x + (1-\\lambda)y) = \\lambda f(x) + (1-\\lambda)f(y)$。\n\n标量误差 $E(\\lambda)$ 定义为该差分向量 $\\Delta(\\lambda) \\in \\mathbb{R}^{d_x}$ 各分量平方的均值：\n$$\nE(\\lambda) = \\frac{1}{d_x} \\sum_{i=1}^{d_x} \\left( \\Delta_i(\\lambda) \\right)^2 = \\frac{1}{d_x} \\| \\Delta(\\lambda) \\|_2^2\n$$\n\n我们可以分析在测试用例指定的条件下该误差的行为：\n1.  如果 $\\phi = \\mathrm{id}$（恒等函数），那么 $\\phi(x) = x$。差分向量变为：\n    $$\n    \\Delta(\\lambda) = (\\lambda u_1 + (1-\\lambda) u_2) - [ \\lambda u_1 + (1-\\lambda) u_2 ] = \\mathbf{0}\n    $$\n    因此，对于线性生成器，误差 $E(\\lambda)$ 对所有 $\\lambda \\in [0,1]$ 恒等于零，正如在用例1中所预期的。\n\n2.  如果激活函数 $\\phi$ 是非线性的，例如 $\\phi = \\tanh$，对于 $\\lambda \\in (0,1)$，误差通常为非零。$\\tanh$ 函数既非全局凸也非全局凹，因此 $\\Delta(\\lambda)$ 中元素的符号将取决于 $u_1$ 和 $u_2$ 的值。这在用例2中进行了测试。\n\n3.  在边界处，当 $\\lambda = 1$ 或 $\\lambda = 0$ 时，无论激活函数如何，误差都必须为零。\n    - 如果 $\\lambda = 1$：$\\tilde{u} = u_1$。$\\Delta(1) = \\phi(u_1) - [1 \\cdot \\phi(u_1) + 0 \\cdot \\phi(u_2)] = \\mathbf{0}$。 所以，$E(1) = 0$。\n    - 如果 $\\lambda = 0$：$\\tilde{u} = u_2$。$\\Delta(0) = \\phi(u_2) - [0 \\cdot \\phi(u_1) + 1 \\cdot \\phi(u_2)] = \\mathbf{0}$。 所以，$E(0) = 0$。\n    这与用例1、2和3的预期一致。\n\n4.  如果条件向量相同，即 $y_1 = y_2$，那么 $u_1 = u_2$。因此，$\\tilde{u} = \\lambda u_1 + (1-\\lambda) u_1 = u_1$。差分向量变为：\n    $$\n    \\Delta(\\lambda) = \\phi(u_1) - \\left[ \\lambda \\phi(u_1) + (1-\\lambda) \\phi(u_1) \\right] = \\phi(u_1) - \\phi(u_1) = \\mathbf{0}\n    $$\n    当 $y_1=y_2$ 时，误差 $E(\\lambda)$ 对所有 $\\lambda$ 均为零，正如在用例4中所预期的。\n\n下面的 Python 实现通过首先计算 $u_1$ 和 $u_2$，然后推导 $\\tilde{u}$，最后评估如上推导的均方差，来为每个指定的测试用例计算误差 $E(\\lambda)$。\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the cGAN linearity error problem for a suite of test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            # Case 1 (happy path; linear generator so zero error is expected for all lambda)\n            'A': np.array([[1.0, 0.5, -0.25], [0.2, -0.1, 0.3]]),\n            'b': np.array([[0.1], [-0.2]]),\n            'z': np.array([[0.7]]),\n            'y1': np.array([[1.0], [0.0]]),\n            'y2': np.array([[0.0], [1.0]]),\n            'phi': 'id',\n            'Lambda': [0.0, 0.25, 0.5, 0.75, 1.0],\n        },\n        {\n            # Case 2 (nonlinear generator; interior lambda values generally produce positive error)\n            'A': np.array([[0.8, 1.2, -0.9], [-0.5, 0.3, 0.7]]),\n            'b': np.array([[0.05], [-0.1]]),\n            'z': np.array([[-0.3]]),\n            'y1': np.array([[1.0], [0.0]]),\n            'y2': np.array([[0.0], [1.0]]),\n            'phi': 'tanh',\n            'Lambda': [0.0, 0.25, 0.5, 0.75, 1.0],\n        },\n        {\n            # Case 3 (boundary condition; any generator must yield zero error at lambda in {0,1})\n            'A': np.array([[-0.6, 0.4, 0.9], [0.3, -0.8, 0.2]]),\n            'b': np.array([[-0.05], [0.2]]),\n            'z': np.array([[1.1]]),\n            'y1': np.array([[0.0], [1.0]]),\n            'y2': np.array([[1.0], [0.0]]),\n            'phi': 'tanh',\n            'Lambda': [0.0, 1.0],\n        },\n        {\n            # Case 4 (degenerate conditioning; identical labels imply zero error for all lambda)\n            'A': np.array([[0.8, 1.2, -0.9], [-0.5, 0.3, 0.7]]),\n            'b': np.array([[0.05], [-0.1]]),\n            'z': np.array([[0.2]]),\n            'y1': np.array([[1.0], [0.0]]),\n            'y2': np.array([[1.0], [0.0]]),\n            'phi': 'tanh',\n            'Lambda': [0.2, 0.4, 0.6, 0.8],\n        }\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        A, b, z, y1, y2 = case['A'], case['b'], case['z'], case['y1'], case['y2']\n        phi_str, lambdas = case['phi'], case['Lambda']\n        \n        if phi_str == 'id':\n            phi = lambda x: x\n        elif phi_str == 'tanh':\n            phi = np.tanh\n        else:\n            raise ValueError(f\"Unknown activation function: {phi_str}\")\n        \n        # Form full input vectors\n        v1 = np.vstack((z, y1))\n        v2 = np.vstack((z, y2))\n        \n        # Calculate pre-activations\n        u1 = A @ v1 + b\n        u2 = A @ v2 + b\n        \n        # Pre-calculate generator outputs for endpoints\n        G_y1 = phi(u1)\n        G_y2 = phi(u2)\n\n        for lambda_val in lambdas:\n            # Calculate output under mixed conditioning\n            u_tilde = lambda_val * u1 + (1 - lambda_val) * u2\n            G_y_tilde = phi(u_tilde)\n            \n            # Calculate the ideal linear morph in the output space\n            G_morph = lambda_val * G_y1 + (1 - lambda_val) * G_y2\n            \n            # Calculate the difference vector\n            diff_vector = G_y_tilde - G_morph\n            \n            # Calculate the mean squared error\n            error = np.mean(np.square(diff_vector))\n            \n            # Ensure negative zero is formatted as positive zero for consistency\n            if np.isclose(error, 0.0):\n                error = 0.0\n\n            results.append(error)\n            \n    # Format and print the final output string\n    print(f\"[{','.join(f'{r:.17f}'.rstrip('0').rstrip('.') for r in results)}]\")\n\nsolve()\n```",
            "answer": "[0,0,0,0,0,0,0.01358988698188185,0.01948834909187687,0.006859341334964648,0,0,0,0,0,0,0,0]"
        }
    ]
}