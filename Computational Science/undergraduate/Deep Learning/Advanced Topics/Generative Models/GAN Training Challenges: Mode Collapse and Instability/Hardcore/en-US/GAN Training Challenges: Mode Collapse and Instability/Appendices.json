{
    "hands_on_practices": [
        {
            "introduction": "The core of Generative Adversarial Network (GAN) training is a dynamic game, and the stability of this game hinges on how the two players—the generator and the discriminator—are updated. This exercise models the GAN updates as a linear dynamical system around a saddle point to explore this very issue. By analyzing the spectral radius of the update matrix, you can rigorously determine why simultaneous gradient updates often lead to divergence and how an alternating Two Time-Scale Update Rule (TTUR) can achieve local stability, providing a foundational insight into GAN training dynamics .",
            "id": "3127265",
            "problem": "Consider a zero-sum Generative Adversarial Network (GAN) game with a bilinear value function given by $$V(\\theta,\\phi)=\\theta^{\\top}A\\phi,$$ where $\\theta\\in\\mathbb{R}^{n}$, $\\phi\\in\\mathbb{R}^{m}$, and $A\\in\\mathbb{R}^{n\\times m}$. The generator minimizes $V$ and the discriminator maximizes $V$. Let the generator learning rate be $\\eta_{G}>0$ and the discriminator learning rate be $\\eta_{D}>0$, and define the ratio $$r=\\frac{\\eta_{D}}{\\eta_{G}}>0.$$\n\nAnalyze two discrete-time training schemes linearized around the saddle point $(\\theta,\\phi)=(0,0)$:\n1. One-time-scale simultaneous gradient descent-ascent:\n$$\\theta_{k+1}=\\theta_{k}-\\eta_{G}\\nabla_{\\theta}V(\\theta_{k},\\phi_{k})=\\theta_{k}-\\eta_{G}A\\phi_{k},\\quad \\phi_{k+1}=\\phi_{k}+\\eta_{D}\\nabla_{\\phi}V(\\theta_{k},\\phi_{k})=\\phi_{k}+\\eta_{D}A^{\\top}\\theta_{k}.$$\n2. Two Time-Scale Update Rule (TTUR; alternating updates): first update the discriminator, then the generator,\n$$\\phi_{k+1}=\\phi_{k}+\\eta_{D}A^{\\top}\\theta_{k},\\quad \\theta_{k+1}=\\theta_{k}-\\eta_{G}A\\phi_{k+1}.$$\n\nStarting from the definitions of gradient descent-ascent and the spectral radius of a linear operator, use the singular value decomposition of $A$ to analyze the linearized dynamics along the right-left singular vector pairs of $A$. Derive the iteration matrices for both schemes and obtain the spectral radius in terms of the singular values of $A$ and the step sizes. From first principles, determine whether there exists any positive ratio $r$ that yields local stabilization in the sense of non-expansive linear dynamics (spectral radius less than or equal to $1$) for the alternating TTUR scheme, and compare this with the simultaneous scheme. Report the minimal achievable spectral radius (over all $r>0$) for the alternating TTUR scheme as a single real number. No rounding is required.",
            "solution": "The problem requires an analysis of the local stability of two discrete-time training schemes for a Generative Adversarial Network (GAN) with a bilinear value function $V(\\theta,\\phi)=\\theta^{\\top}A\\phi$. The stability is determined by the spectral radius of the iteration matrix for the dynamics linearized around the saddle point $(\\theta,\\phi)=(0,0)$.\n\nLet the state of the system at step $k$ be the concatenated vector $z_k = \\begin{pmatrix} \\theta_k \\\\ \\phi_k \\end{pmatrix}$. The updates are linear, so they can be expressed in the form $z_{k+1} = M z_k$ for an appropriate iteration matrix $M$. A linear system is non-expansive (a form of stability) if the spectral radius of its iteration matrix, $\\rho(M)$, is less than or equal to $1$.\n\nTo analyze the matrices, we use the Singular Value Decomposition (SVD) of the matrix $A$, which is $A = U\\Sigma V^{\\top}$. Here, $U \\in \\mathbb{R}^{n\\times n}$ and $V \\in \\mathbb{R}^{m\\times m}$ are orthogonal matrices whose columns are the left-singular vectors ($u_i$) and right-singular vectors ($v_i$) of $A$, respectively. $\\Sigma \\in \\mathbb{R}^{n\\times m}$ is a rectangular diagonal matrix containing the non-negative singular values $\\sigma_i$. The singular vectors satisfy $Av_i = \\sigma_i u_i$ and $A^{\\top}u_i = \\sigma_i v_i$. By projecting the dynamics onto the basis of singular vectors, the system decouples into a set of independent $2 \\times 2$ systems, one for each singular value $\\sigma_i$. We analyze the dynamics of the coefficients of $\\theta_k$ and $\\phi_k$ in these bases.\n\n**1. Simultaneous Gradient Descent-Ascent (SGDA)**\n\nThe update rules are:\n$$ \\theta_{k+1}=\\theta_{k}-\\eta_{G}A\\phi_{k} $$\n$$ \\phi_{k+1}=\\phi_{k}+\\eta_{D}A^{\\top}\\theta_{k} $$\nIn matrix form, this is $z_{k+1} = M_{\\text{sim}} z_k$, with the iteration matrix:\n$$ M_{\\text{sim}} = \\begin{pmatrix} I & -\\eta_{G}A \\\\ \\eta_{D}A^{\\top} & I \\end{pmatrix} $$\nLet's analyze the dynamics for a single mode corresponding to a singular value $\\sigma_i$. We express $\\theta_k = c_k u_i$ and $\\phi_k = d_k v_i$. The updates for the coefficients $(c_k, d_k)$ become:\n$$ c_{k+1}u_i = c_k u_i - \\eta_{G} A (d_k v_i) = c_k u_i - \\eta_{G} d_k (\\sigma_i u_i) \\implies c_{k+1} = c_k - \\eta_{G}\\sigma_i d_k $$\n$$ d_{k+1}v_i = d_k v_i + \\eta_{D} A^{\\top} (c_k u_i) = d_k v_i + \\eta_{D} c_k (\\sigma_i v_i) \\implies d_{k+1} = d_k + \\eta_{D}\\sigma_i c_k $$\nThis yields a $2 \\times 2$ system for each mode:\n$$ \\begin{pmatrix} c_{k+1} \\\\ d_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1 & -\\eta_{G}\\sigma_i \\\\ \\eta_{D}\\sigma_i & 1 \\end{pmatrix} \\begin{pmatrix} c_k \\\\ d_k \\end{pmatrix} $$\nThe eigenvalues $\\lambda$ of the $2 \\times 2$ matrix are given by the characteristic equation $(\\lambda-1)^2 - (-\\eta_{G}\\sigma_i)(\\eta_{D}\\sigma_i) = 0$, which simplifies to $(\\lambda-1)^2 = -\\eta_{G}\\eta_{D}\\sigma_i^2$. The eigenvalues are $\\lambda = 1 \\pm i\\sigma_i\\sqrt{\\eta_{G}\\eta_{D}}$.\nThe magnitude of these eigenvalues is $|\\lambda| = \\sqrt{1^2 + (\\sigma_i\\sqrt{\\eta_{G}\\eta_{D}})^2} = \\sqrt{1 + \\eta_{G}\\eta_{D}\\sigma_i^2}$.\nFor any non-zero singular value $\\sigma_i > 0$ and positive learning rates $\\eta_G, \\eta_D > 0$, we have $|\\lambda| > 1$. The spectral radius of the full system matrix $M_{\\text{sim}}$ is the maximum of these magnitudes over all singular values:\n$$ \\rho(M_{\\text{sim}}) = \\max_i \\sqrt{1 + \\eta_{G}\\eta_{D}\\sigma_i^2} = \\sqrt{1 + \\eta_{G}\\eta_{D}\\sigma_{\\max}^2} $$\nwhere $\\sigma_{\\max}$ is the largest singular value of $A$. Since $\\rho(M_{\\text{sim}}) > 1$ for any choice of positive learning rates, the simultaneous SGDA scheme is always locally expansive and thus unstable.\n\n**2. Two Time-Scale Update Rule (TTUR, alternating)**\n\nThe update rules are:\n$$ \\phi_{k+1}=\\phi_{k}+\\eta_{D}A^{\\top}\\theta_{k} $$\n$$ \\theta_{k+1}=\\theta_{k}-\\eta_{G}A\\phi_{k+1} $$\nSubstituting the first equation into the second gives the full update for $\\theta_k$:\n$$ \\theta_{k+1} = \\theta_k - \\eta_G A (\\phi_k + \\eta_D A^{\\top}\\theta_k) = (I - \\eta_G \\eta_D A A^{\\top})\\theta_k - \\eta_G A \\phi_k $$\nThe iteration matrix $M_{\\text{alt}}$ is:\n$$ M_{\\text{alt}} = \\begin{pmatrix} I - \\eta_{G}\\eta_{D}AA^{\\top} & -\\eta_{G}A \\\\ \\eta_{D}A^{\\top} & I \\end{pmatrix} $$\nAgain, we analyze the dynamics in the basis of singular vectors. Let $\\theta_k = c_k u_i$ and $\\phi_k = d_k v_i$. The coefficient updates are:\n$$ d_{k+1} = d_k + \\eta_{D}\\sigma_i c_k $$\n$$ c_{k+1} = c_k - \\eta_G \\sigma_i d_{k+1} = c_k - \\eta_G \\sigma_i (d_k + \\eta_D \\sigma_i c_k) = (1 - \\eta_G \\eta_D \\sigma_i^2)c_k - \\eta_G \\sigma_i d_k $$\nThe $2 \\times 2$ system for the coefficients is:\n$$ \\begin{pmatrix} c_{k+1} \\\\ d_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1-\\eta_{G}\\eta_{D}\\sigma_i^2 & -\\eta_{G}\\sigma_i \\\\ \\eta_{D}\\sigma_i & 1 \\end{pmatrix} \\begin{pmatrix} c_k \\\\ d_k \\end{pmatrix} $$\nLet $x_i = \\eta_G \\eta_D \\sigma_i^2$. The characteristic equation for the eigenvalues $\\lambda$ is $(1-x_i-\\lambda)(1-\\lambda) - (-\\eta_G \\sigma_i)(\\eta_D \\sigma_i) = 0$, which simplifies to $\\lambda^2 - (2-x_i)\\lambda + 1 = 0$.\nThe eigenvalues are $\\lambda = \\frac{(2-x_i) \\pm \\sqrt{(2-x_i)^2 - 4}}{2} = 1 - \\frac{x_i}{2} \\pm \\frac{\\sqrt{x_i^2 - 4x_i}}{2}$.\n\nWe analyze the magnitude $|\\lambda|$ based on the value of $x_i$:\n- **Case $0 \\le x_i \\le 4$**: The term under the square root is non-positive, so the eigenvalues are a complex conjugate pair: $\\lambda = 1 - \\frac{x_i}{2} \\pm i\\frac{\\sqrt{4x_i - x_i^2}}{2}$.\nThe squared magnitude is $|\\lambda|^2 = (1-\\frac{x_i}{2})^2 + (\\frac{\\sqrt{4x_i - x_i^2}}{2})^2 = 1 - x_i + \\frac{x_i^2}{4} + \\frac{4x_i - x_i^2}{4} = 1$. Thus, $|\\lambda|=1$.\n- **Case $x_i > 4$**: The eigenvalues are real. From Vieta's formulas, their product is $1$. This means if one has magnitude greater than $1$, the other must have magnitude less than $1$. The larger magnitude is $|\\lambda| = |\\frac{x_i}{2}-1 + \\frac{\\sqrt{x_i^2-4x_i}}{2}|$. Since for $x_i>4$, $\\frac{x_i}{2}-1>1$, this magnitude is strictly greater than $1$.\n\nFor the alternating TTUR scheme to be non-expansive, its spectral radius $\\rho(M_{\\text{alt}})$ must be less than or equal to $1$. This requires that $|\\lambda| \\le 1$ for all modes, which in turn requires $x_i \\le 4$ for all $i$. This condition must hold for the largest singular value, $\\sigma_{\\max}$.\nThe condition for non-expansive dynamics is therefore $\\eta_{G}\\eta_{D}\\sigma_{\\max}^2 \\le 4$.\nWith $r = \\eta_D/\\eta_G$, this is $r\\eta_G^2\\sigma_{\\max}^2 \\le 4$. For any positive ratio $r>0$, one can always choose the learning rate $\\eta_G$ to be sufficiently small, e.g., $\\eta_G \\le \\frac{2}{\\sigma_{\\max}\\sqrt{r}}$, to satisfy this condition. Thus, unlike the simultaneous scheme, the alternating TTUR scheme can be stabilized for any positive ratio $r$.\n\nThe problem asks for the minimal achievable spectral radius over all $r>0$. This is equivalent to finding $\\inf_{\\eta_G>0, \\eta_D>0} \\rho(M_{\\text{alt}})$.\nThe magnitude of the eigenvalues for any mode is $|\\lambda_i| \\ge 1$. The minimum value of $|\\lambda_i|=1$ is achieved when $0 \\le x_i \\le 4$. To achieve a system-wide spectral radius of $1$, we require $x_i \\le 4$ for all $i$, which is guaranteed if $x_{\\max} = \\eta_G \\eta_D \\sigma_{\\max}^2 \\le 4$. As shown, this condition is achievable for any $r>0$ by selecting small enough learning rates. When this condition is met, every mode has eigenvalues of magnitude $1$, so the spectral radius of the entire system is $\\rho(M_{\\text{alt}}) = \\max_i|\\lambda_i|=1$. Since the magnitude can never be less than $1$, the minimal achievable spectral radius is $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Instability in GANs can arise not just from the update algorithm but also from subtle interactions within the network architecture. This practice investigates how a common component, Batch Normalization, can inadvertently create a pathological link between real and fake data when used in the discriminator, leading to training oscillations. By comparing this to modern alternatives like Spectral Normalization and Layer Normalization , you will understand how thoughtful architectural design is crucial for taming the discriminator and stabilizing the adversarial training process.",
            "id": "3127207",
            "problem": "Consider a Generative Adversarial Network (GAN) defined by the minimax objective\n$$\\min_{G} \\max_{D} \\ \\mathbb{E}_{\\mathbf{x} \\sim p_{\\mathrm{data}}} \\big[ \\log D(\\mathbf{x}) \\big] + \\mathbb{E}_{\\mathbf{z} \\sim p_{\\mathbf{z}}} \\big[ \\log (1 - D(G(\\mathbf{z}))) \\big],$$\nwhere $G$ is the generator, $D$ is the discriminator, $p_{\\mathrm{data}}$ is the data distribution, and $p_{\\mathbf{z}}$ is a simple latent distribution. Suppose the discriminator $D$ uses batch normalization (BN) at an intermediate layer with pre-activation vector $\\mathbf{a} \\in \\mathbb{R}^{d}$ and per-feature normalization\n$$\\hat{a}_{i} = \\frac{a_{i} - \\mu_{\\mathrm{batch}}}{\\sqrt{\\sigma_{\\mathrm{batch}}^{2} + \\epsilon}}, \\quad y_{i} = \\gamma \\hat{a}_{i} + \\beta,$$\nfor each feature index $i \\in \\{1, \\dots, d\\}$, where $\\mu_{\\mathrm{batch}}$ and $\\sigma_{\\mathrm{batch}}^{2}$ are the mean and variance computed across a mixed minibatch containing both real samples $\\mathbf{x}$ and fake samples $\\tilde{\\mathbf{x}} = G(\\mathbf{z})$, $\\gamma$ and $\\beta$ are learnable affine parameters, and $\\epsilon > 0$ is a small constant. Assume $D$ is updated using such mixed minibatches so that $\\mu_{\\mathrm{batch}}$ and $\\sigma_{\\mathrm{batch}}^{2}$ depend jointly on the features from real and fake examples.\n\nUse the following fundamental bases:\n- The GAN objective above and the fact that training alternates gradient-based updates to $D$ and $G$ that depend on the discriminator’s output $D(\\cdot)$ and its internal transformations.\n- The batch normalization transformation depends on batch statistics, so the forward and backward signals for any example in the batch depend on other examples in the same batch via $\\mu_{\\mathrm{batch}}$ and $\\sigma_{\\mathrm{batch}}^{2}$.\n- Spectral normalization (SN) rescales each weight matrix $\\mathbf{W}$ as $\\mathbf{W}_{\\mathrm{SN}} = \\mathbf{W} / s(\\mathbf{W})$, where $s(\\mathbf{W})$ is the spectral norm (largest singular value), thereby bounding the layer’s operator norm and the network’s Lipschitz constant.\n- Layer normalization (LN) computes per-example statistics across features: for a given example with activation vector $\\mathbf{a}$, LN uses $\\mu_{\\mathrm{layer}} = \\frac{1}{d} \\sum_{i=1}^{d} a_{i}$ and $\\sigma_{\\mathrm{layer}}^{2} = \\frac{1}{d} \\sum_{i=1}^{d} (a_{i} - \\mu_{\\mathrm{layer}})^{2}$, normalizing $\\mathbf{a}$ without coupling different examples in the batch.\n\nStarting from these bases, reason about how mixing real and fake examples in BN creates coupling in $D$’s internal normalization that can produce oscillatory training dynamics: when $G$ changes its output distribution, $\\mu_{\\mathrm{batch}}$ and $\\sigma_{\\mathrm{batch}}^{2}$ shift, altering $D$’s scaling for both real and fake examples, which in turn changes gradient magnitudes and directions fed back to $G$; such feedback can yield non-monotonic trajectories and oscillations. Then consider replacing BN in $D$ with either spectral normalization or layer normalization, and predict their effects on stability and mode collapse under the same training protocol.\n\nWhich option best captures the mechanism and the predicted effects?\n\nA. Replacing batch normalization in $D$ with spectral normalization reduces oscillations by bounding the discriminator’s Lipschitz constant, thereby attenuating abrupt changes in gradient magnitude induced by batch-statistics shifts; this tends to stabilize training and can reduce the incidence of mode collapse by providing smoother generator updates.\n\nB. Replacing batch normalization in $D$ with layer normalization removes inter-example coupling because statistics are computed per example, thus decoupling real and fake samples; this reduces oscillations driven by batch composition, typically improves stability, and may modestly slow convergence due to the absence of batch-level variance reduction.\n\nC. Spectral normalization amplifies high-frequency features and increases oscillations because it relies on batch statistics, strengthening coupling between real and fake; therefore it is expected to destabilize training and exacerbate mode collapse compared to batch normalization.\n\nD. Layer normalization has the same coupling effect as batch normalization since it computes mean and variance across the entire minibatch; consequently, it does not change oscillatory behavior relative to using batch normalization in $D$.",
            "solution": "The core issue stems from the use of Batch Normalization (BN) within the discriminator $D$ on minibatches containing both real samples $\\mathbf{x}$ and fake samples $\\tilde{\\mathbf{x}}=G(\\mathbf{z})$.\n\n1.  **Analysis of the BN-induced Instability:**\n    The normalization of a pre-activation vector $\\mathbf{a}$ in a BN layer is given by $\\hat{a}_{i} = (a_{i} - \\mu_{\\mathrm{batch}}) / \\sqrt{\\sigma_{\\mathrm{batch}}^{2} + \\epsilon}$. The crucial point is that the batch mean $\\mu_{\\mathrm{batch}}$ and variance $\\sigma_{\\mathrm{batch}}^{2}$ are computed over the activations from all samples in the minibatch. When the batch is mixed, the statistics are a function of both the real and fake data activations.\n    This creates an artificial coupling: the normalized representation of a real sample inside $D$ becomes dependent on the fake samples present in the same batch, and vice versa.\n    The generator's loss depends on the discriminator's output, $D(G(\\mathbf{z}))$. The gradient of this loss with respect to the generator's parameters is computed via backpropagation through $D$. Because of the BN coupling, the gradient signal $\\frac{\\partial D(G(\\mathbf{z}))}{\\partial G}$ is not only a function of the fake data but also implicitly a function of the real data in the batch.\n    This leads to pathological training dynamics. An update to the generator $G$ changes the distribution of fake samples. This change alters the batch statistics ($\\mu_{\\mathrm{batch}}$, $\\sigma_{\\mathrm{batch}}^{2}$) in the next iteration. This shift in statistics modifies the internal representations and, consequently, the critic's output for *both* real and fake samples. The resulting gradient landscape for $G$ can become noisy and non-stationary, causing the training to oscillate and struggle to converge. This unstable feedback loop can also exacerbate mode collapse, where $G$ learns to produce outputs that exploit this statistical coupling rather than learning the full data distribution.\n\n2.  **Effect of Replacing BN with Spectral Normalization (SN):**\n    SN is applied to the weight matrices $\\mathbf{W}$ of the discriminator's layers, not to the activations. It enforces a Lipschitz constraint on the discriminator function $D$ by ensuring that each layer has an operator norm of at most $1$. Specifically, $\\mathbf{W}_{\\mathrm{SN}} = \\mathbf{W} / s(\\mathbf{W})$, where $s(\\mathbf{W})$ is the spectral norm (largest singular value) of $\\mathbf{W}$. The calculation of $s(\\mathbf{W})$ is independent of the input data batch.\n    By enforcing a Lipschitz constraint (e.g., $L=1$), SN guarantees that $\\|D(\\mathbf{u}) - D(\\mathbf{v})\\| \\leq \\|\\mathbf{u} - \\mathbf{v}\\|$ for any inputs $\\mathbf{u}$ and $\\mathbf{v}$ (assuming $1$-Lipschitz activations). This property directly stabilizes training by preventing the discriminator's gradients from becoming excessively large or changing too abruptly.\n    The gradients passed to the generator become smoother and more reliable. This stabilization directly counters the oscillatory behavior described above, not by changing the normalization of activations, but by regularizing the function space of $D$ itself. This improved gradient flow helps the generator to converge more steadily and can reduce the likelihood of mode collapse. The mechanism is the removal of batch dependency *and* the control of the global function properties of $D$.\n\n3.  **Effect of Replacing BN with Layer Normalization (LN):**\n    LN computes normalization statistics on a per-example basis. For a given activation vector $\\mathbf{a} \\in \\mathbb{R}^{d}$, the mean $\\mu_{\\mathrm{layer}}$ and variance $\\sigma_{\\mathrm{layer}}^{2}$ are computed across the $d$ feature channels of that single example.\n    The normalization of an activation vector from a real sample is therefore completely independent of any other sample, real or fake, in the minibatch. LN directly severs the inter-example coupling that is the root cause of the BN-induced instability.\n    By decoupling the processing of real and fake samples, the generator's updates no longer cause spurious shifts in the discriminator's representation of real data. The gradient feedback to the generator becomes cleaner and more directly related to the quality of its output, thus reducing oscillations and improving training stability. The claim about potentially slower convergence is a secondary effect sometimes observed, as the batch-level regularization effect of BN is lost, but the primary and most significant effect in this context is the stabilization.\n\n### Option-by-Option Analysis\n\n**A. Replacing batch normalization in D with spectral normalization reduces oscillations by bounding the discriminator’s Lipschitz constant, thereby attenuating abrupt changes in gradient magnitude induced by batch-statistics shifts; this tends to stabilize training and can reduce the incidence of mode collapse by providing smoother generator updates.**\n-   This statement correctly identifies that SN bounds the discriminator's Lipschitz constant. It correctly links this to the attenuation of abrupt gradient changes, which is the mechanism for reducing oscillations. While SN's primary effect is to regularize the function $D$ itself, doing so implicitly solves the problem of unstable gradients, which in the original problem were \"induced by batch-statistics shifts\". The predicted effects—stabilized training, reduced mode collapse, and smoother generator updates—are all accurate consequences of using SN in GANs.\n-   **Verdict: Correct.**\n\n**B. Replacing batch normalization in D with layer normalization removes inter-example coupling because statistics are computed per example, thus decoupling real and fake samples; this reduces oscillations driven by batch composition, typically improves stability, and may modestly slow convergence due to the absence of batch-level variance reduction.**\n-   This statement accurately describes the mechanism of LN: its statistics are computed per-example, which \"removes inter-example coupling\" and \"decoupling real and fake samples\". It correctly attributes the reduction in oscillations and improvement in stability to this decoupling. The final clause about a modest slowdown in convergence is a plausible and often discussed nuance, but the core analysis of stability is entirely correct.\n-   **Verdict: Correct.**\n\n**C. Spectral normalization amplifies high-frequency features and increases oscillations because it relies on batch statistics, strengthening coupling between real and fake; therefore it is expected to destabilize training and exacerbate mode collapse compared to batch normalization.**\n-   This statement is fundamentally flawed. SN does *not* rely on batch statistics; it operates on the network's weight matrices independently of the input data. Its purpose and effect are to stabilize, not \"increase oscillations\". It does not strengthen coupling; it removes the source of BN's coupling. The predicted effects are the opposite of what is observed and intended.\n-   **Verdict: Incorrect.**\n\n**D. Layer normalization has the same coupling effect as batch normalization since it computes mean and variance across the entire minibatch; consequently, it does not change oscillatory behavior relative to using batch normalization in D.**\n-   This statement is factually incorrect. It wrongly claims that LN computes statistics \"across the entire minibatch\". As defined in the problem statement and in the literature, LN computes statistics per-example, across features. Therefore, it does *not* have the same coupling effect as BN. The conclusion is based on a false premise.\n-   **Verdict: Incorrect.**\n\nBoth options A and B provide accurate, distinct mechanisms for stabilizing GAN training by replacing batch normalization. Both capture the core reasoning required by the problem prompt.",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "A primary challenge in GAN training is mode collapse, where the generator produces only a limited variety of outputs. To address this, we can explicitly modify the training objective to reward diversity. This exercise introduces Minibatch Discrimination, a technique that enables the discriminator to assess the similarity of samples within a batch and penalize the generator for a lack of variety. By deriving the gradient for this mechanism , you will gain a concrete understanding of how to engineer a \"repulsive force\" between generated samples, directly encouraging the generator to capture the full diversity of the data distribution.",
            "id": "3127206",
            "problem": "Consider a Generative Adversarial Network (GAN), defined as a two-player game between a generator and a discriminator, where the generator $G$ maps a latent variable $z$ to a data point $x$, and the discriminator $D$ estimates the probability that an input comes from the real data distribution rather than from $G$. Let the generator be the scalar linear map $G_{w}(z) = w z$ with parameter $w \\in \\mathbb{R}$ and $z \\in \\mathbb{R}$. Let the discriminator output be $D(x, s) = \\sigma(h(x, s))$, where $\\sigma(u) = \\frac{1}{1 + \\exp(-u)}$ is the logistic sigmoid, and the discriminator logit is $h(x, s) = a x + b - \\lambda s$, with $a, b, \\lambda \\in \\mathbb{R}$. To encourage diversity in the generator outputs within a minibatch, augment the discriminator with a minibatch discrimination similarity term\n$$\nc(x, x') = \\exp\\!\\big(-\\beta (x - x')^{2}\\big),\n$$\nwhere $\\beta > 0$, and define for a minibatch $\\{z_{i}\\}_{i=1}^{n}$ the per-sample similarity summary\n$$\ns_{i} = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} c(x_{i}, x_{j}), \\quad x_{i} = G_{w}(z_{i}).\n$$\nAssume the generator uses the non-saturating objective\n$$\nL_{G}(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell\\big(D(x_{i}, s_{i})\\big), \\quad \\ell(p) = -\\ln(p).\n$$\nTasks:\n1. Starting only from these definitions, derive a closed-form expression for $\\frac{d L_{G}}{d w}$ in terms of $a$, $b$, $\\lambda$, $\\beta$, $w$, and $\\{z_{i}\\}_{i=1}^{n}$.\n2. Numerically evaluate the derivative for the minibatch size $n = 3$ with $z_{1} = -1$, $z_{2} = 0$, $z_{3} = 1$, parameters $a = 1.5$, $b = 0$, $\\lambda = 2$, $\\beta = 0.5$, at $w = 1$. Round your final numerical answer to four significant figures. Express the final result as a unitless scalar.\n3. Explain qualitatively, based on your derived gradient, how the minibatch discrimination contribution encourages output diversity across the minibatch, and discuss pitfalls if the minibatch size $n$ is small relative to the count of distinct modes in the data distribution. No additional calculation is required for this part.",
            "solution": "The problem is divided into three tasks: 1) deriving the gradient of the generator's loss function, 2) numerically evaluating this gradient for a specific case, and 3) qualitatively explaining the role of the minibatch discrimination term.\n\n### Task 1: Derivation of the Gradient $\\frac{d L_{G}}{d w}$\n\nThe generator's loss function is given by:\n$$L_{G}(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell\\big(D(x_{i}, s_{i})\\big)$$\nwhere $\\ell(p) = -\\ln(p)$, $x_i = G_w(z_i) = w z_i$, and $D(x, s) = \\sigma(h(x,s))$ is the discriminator output.\nThe derivative of the loss with respect to the generator parameter $w$ is:\n$$\\frac{d L_{G}}{d w} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{d}{dw} \\left[ -\\ln\\left(D(x_{i}, s_{i})\\right) \\right]$$\nLet's analyze a single term in the summation. Using the chain rule, we have:\n$$\\frac{d}{dw} \\left[ -\\ln(D_i) \\right] = -\\frac{1}{D_i} \\frac{dD_i}{dw}$$\nwhere we use the shorthand $D_i = D(x_i, s_i)$.\n\nThe discriminator output is $D_i = \\sigma(h_i)$, where $h_i = h(x_i, s_i) = a x_i + b - \\lambda s_i$. The derivative of the sigmoid function $\\sigma(u)$ is $\\sigma'(u) = \\sigma(u)(1-\\sigma(u))$. Applying the chain rule again:\n$$\\frac{dD_i}{dw} = \\frac{d}{dw} \\sigma(h_i) = \\sigma'(h_i) \\frac{dh_i}{dw} = \\sigma(h_i)(1-\\sigma(h_i)) \\frac{dh_i}{dw} = D_i(1 - D_i) \\frac{dh_i}{dw}$$\nSubstituting this back into the derivative of the single loss term:\n$$\\frac{d}{dw} \\left[ -\\ln(D_i) \\right] = -\\frac{1}{D_i} \\left[ D_i(1 - D_i) \\frac{dh_i}{dw} \\right] = -(1 - D_i) \\frac{dh_i}{dw}$$\nThis is a standard result for the non-saturating generator loss.\n\nNext, we need to find the derivative of the discriminator logit $h_i$ with respect to $w$:\n$$h_i = a x_i + b - \\lambda s_i$$\n$$\\frac{dh_i}{dw} = \\frac{d}{dw} (a x_i + b - \\lambda s_i) = a \\frac{dx_i}{dw} - \\lambda \\frac{ds_i}{dw}$$\nGiven $x_i = w z_i$, its derivative is $\\frac{dx_i}{dw} = z_i$.\n\nNow, we compute the derivative of the similarity summary term $s_i$:\n$$s_i = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} c(x_{i}, x_{j})$$\n$$\\frac{ds_i}{dw} = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} \\frac{d}{dw} c(x_i, x_j)$$\nThe similarity function is $c(x, x') = \\exp(-\\beta (x - x')^2)$. Its derivative with respect to $w$ is found using the chain rule:\n$$\\frac{d}{dw} c(x_i, x_j) = \\exp(-\\beta (x_i - x_j)^2) \\cdot \\frac{d}{dw} (-\\beta (x_i - x_j)^2)$$\n$$\\frac{d}{dw} c(x_i, x_j) = c(x_i, x_j) \\cdot (-2\\beta (x_i - x_j)) \\cdot \\frac{d}{dw}(x_i - x_j)$$\nWith $x_i = w z_i$ and $x_j = w z_j$, we have $x_i - x_j = w(z_i - z_j)$, so $\\frac{d}{dw}(x_i - x_j) = z_i - z_j$.\nSubstituting this in, we get:\n$$\\frac{d}{dw} c(x_i, x_j) = c(x_i, x_j) \\cdot (-2\\beta w(z_i - z_j)) \\cdot (z_i - z_j) = -2\\beta w (z_i - z_j)^2 c(x_i, x_j)$$\nTherefore, the derivative of $s_i$ is:\n$$\\frac{ds_i}{dw} = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} \\left[ -2\\beta w (z_i - z_j)^2 c(x_i, x_j) \\right] = -2\\beta w \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} (z_i - z_j)^2 c(x_i, x_j)$$\nNow we substitute this into the expression for $\\frac{dh_i}{dw}$:\n$$\\frac{dh_i}{dw} = a z_i - \\lambda \\left[ -2\\beta w \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} (z_i - z_j)^2 c(x_i, x_j) \\right] = a z_i + 2\\lambda\\beta w \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} (z_i - z_j)^2 c(x_i, x_j)$$\nFinally, we assemble the complete expression for $\\frac{d L_{G}}{d w}$:\n$$\\frac{d L_{G}}{d w} = \\frac{1}{n} \\sum_{i=1}^{n} -(1 - D(x_i, s_i)) \\left[ a z_i + 2\\lambda\\beta w \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} (z_i - z_j)^2 c(x_i, x_j) \\right]$$\nwhere $x_i = w z_i$, $s_i = \\sum_{j \\neq i} c(x_i, x_j)$, $c(x_i, x_j) = \\exp(-\\beta(x_i-x_j)^2)$, and $D(x_i, s_i) = \\sigma(a x_i + b - \\lambda s_i)$. This is the desired closed-form expression.\n\n### Task 2: Numerical Evaluation of the Gradient\n\nWe are given the following values:\n$n = 3$, $z_1 = -1$, $z_2 = 0$, $z_3 = 1$.\n$a = 1.5$, $b = 0$, $\\lambda = 2$, $\\beta = 0.5$, and we evaluate at $w = 1$.\n\nAt $w = 1$, the generated samples are $x_i = 1 \\cdot z_i = z_i$, so $x_1 = -1$, $x_2 = 0$, $x_3 = 1$.\n\nFirst, compute the similarity terms $c(x_i, x_j) = \\exp(-0.5(x_i - x_j)^2)$:\n$c(x_1, x_2) = c(x_2, x_1) = \\exp(-0.5(-1 - 0)^2) = \\exp(-0.5)$.\n$c(x_1, x_3) = c(x_3, x_1) = \\exp(-0.5(-1 - 1)^2) = \\exp(-0.5(-2)^2) = \\exp(-2)$.\n$c(x_2, x_3) = c(x_3, x_2) = \\exp(-0.5(0 - 1)^2) = \\exp(-0.5)$.\n\nNext, compute the per-sample similarity summaries $s_i = \\sum_{j \\neq i} c(x_i, x_j)$:\n$s_1 = c(x_1, x_2) + c(x_1, x_3) = \\exp(-0.5) + \\exp(-2)$.\n$s_2 = c(x_2, x_1) + c(x_2, x_3) = \\exp(-0.5) + \\exp(-0.5) = 2\\exp(-0.5)$.\n$s_3 = c(x_3, x_1) + c(x_3, x_2) = \\exp(-2) + \\exp(-0.5) = s_1$.\n\nNow compute the discriminator logits $h_i = a x_i + b - \\lambda s_i = 1.5 x_i - 2 s_i$:\n$h_1 = 1.5(-1) - 2(\\exp(-0.5) + \\exp(-2))$.\n$h_2 = 1.5(0) - 2(2\\exp(-0.5)) = -4\\exp(-0.5)$.\n$h_3 = 1.5(1) - 2(\\exp(-0.5) + \\exp(-2))$.\n\nWe also need the terms in the square brackets from the Task 1 result, let's call it $K_i$:\n$K_i = a z_i + 2\\lambda\\beta w \\sum_{j \\neq i} (z_i - z_j)^2 c(x_i, x_j) = 1.5 z_i + 2(2)(0.5)(1) \\sum_{j \\neq i} (z_i - z_j)^2 c(x_i, x_j) = 1.5 z_i + 2 \\sum_{j \\neq i} (z_i - z_j)^2 c(x_i, x_j)$.\n$K_1 = 1.5(-1) + 2\\left[(z_1-z_2)^2 c(x_1,x_2) + (z_1-z_3)^2 c(x_1,x_3)\\right] = -1.5 + 2\\left[(-1)^2 \\exp(-0.5) + (-2)^2 \\exp(-2)\\right] = -1.5 + 2\\exp(-0.5) + 8\\exp(-2)$.\n$K_2 = 1.5(0) + 2\\left[(z_2-z_1)^2 c(x_2,x_1) + (z_2-z_3)^2 c(x_2,x_3)\\right] = 2\\left[(1)^2 \\exp(-0.5) + (-1)^2 \\exp(-0.5)\\right] = 4\\exp(-0.5)$.\n$K_3 = 1.5(1) + 2\\left[(z_3-z_1)^2 c(x_3,x_1) + (z_3-z_2)^2 c(x_3,x_2)\\right] = 1.5 + 2\\left[(2)^2 \\exp(-2) + (1)^2 \\exp(-0.5)\\right] = 1.5 + 8\\exp(-2) + 2\\exp(-0.5)$.\n\nNow, we perform numerical evaluation:\n$\\exp(-0.5) \\approx 0.606531$\n$\\exp(-2) \\approx 0.135335$\n\n$s_1 = s_3 \\approx 0.606531 + 0.135335 = 0.741866$.\n$s_2 \\approx 2 \\times 0.606531 = 1.213062$.\n\n$h_1 \\approx -1.5 - 2(0.741866) = -2.983732$.\n$h_2 \\approx -4(0.606531) = -2.426124$.\n$h_3 \\approx 1.5 - 2(0.741866) = 0.016268$.\n\n$K_1 \\approx -1.5 + 2(0.606531) + 8(0.135335) = -1.5 + 1.213062 + 1.08268 = 0.795742$.\n$K_2 \\approx 4(0.606531) = 2.426124$.\n$K_3 \\approx 1.5 + 8(0.135335) + 2(0.606531) = 1.5 + 1.08268 + 1.213062 = 3.795742$.\n\nThe gradient is $\\frac{d L_G}{d w} = \\frac{1}{3} \\sum_{i=1}^3 -(1 - \\sigma(h_i)) K_i = \\frac{1}{3} \\sum_{i=1}^3 -\\sigma(-h_i) K_i$.\n$\\sigma(-h_1) = \\sigma(2.983732) = \\frac{1}{1 + \\exp(-2.983732)} \\approx 0.951825$.\n$\\sigma(-h_2) = \\sigma(2.426124) = \\frac{1}{1 + \\exp(-2.426124)} \\approx 0.918790$.\n$\\sigma(-h_3) = \\sigma(-0.016268) = \\frac{1}{1 + \\exp(0.016268)} \\approx 0.495935$.\n\nThe terms in the sum are:\nTerm 1: $-\\sigma(-h_1) K_1 \\approx -0.951825 \\times 0.795742 \\approx -0.75747$.\nTerm 2: $-\\sigma(-h_2) K_2 \\approx -0.918790 \\times 2.426124 \\approx -2.22940$.\nTerm 3: $-\\sigma(-h_3) K_3 \\approx -0.495935 \\times 3.795742 \\approx -1.88251$.\n\nSum $= -0.75747 - 2.22940 - 1.88251 = -4.86938$.\n$\\frac{d L_G}{d w} = \\frac{-4.86938}{3} \\approx -1.623127$.\n\nRounding to four significant figures, the result is $-1.623$.\n\n### Task 3: Qualitative Explanation\n\nThe goal of the generator is to produce samples that the discriminator classifies as real. For the non-saturating loss, this means the generator aims to maximize the discriminator's output probability $D(x_i, s_i) = \\sigma(h_i)$, which is equivalent to maximizing the logit $h_i = a x_i + b - \\lambda s_i$.\n\nThe minibatch discrimination term enters the logit as $-\\lambda s_i$, where $s_i = \\sum_{j \\neq i} \\exp(-\\beta (x_i-x_j)^2)$. The term $s_i$ is a measure of similarity of sample $x_i$ to other samples in the minibatch. If the generator produces samples that are very close to each other (low diversity, potential mode collapse), the distances $|x_i-x_j|$ will be small, making the exponential term $\\exp(-\\beta(x_i-x_j)^2)$ close to $1$. This results in a large $s_i$, which, due to the $-\\lambda$ coefficient (with $\\lambda > 0$), significantly penalizes the logit $h_i$. A lower logit means the discriminator is more likely to classify the sample as fake. To counteract this and fool the discriminator, the generator is forced to produce samples that are far apart from each other, i.e., to increase the distances $|x_i-x_j|$. This directly encourages diversity within the minibatch.\n\nThe gradient derived in Task 1 reflects this. The gradient update for $w$ is driven by terms like $(1 - D_i) \\lambda (-\\frac{ds_i}{dw})$. Since $\\frac{ds_i}{dw} = -2\\beta w \\sum_{j \\neq i} (z_i - z_j)^2 c(x_i, x_j)$ is generally negative (for $w>0$), this part of the total gradient is negative. In gradient descent ($w \\leftarrow w - \\eta \\frac{dL_G}{dw}$), a negative gradient causes an increase in $w$. Increasing $w$ scales all distances $|x_i-x_j|=|w(z_i-z_j)|$ up, effectively pushing samples apart and promoting diversity.\n\nA significant pitfall arises when the minibatch size $n$ is small compared to the number of distinct modes in the true data distribution. Minibatch discrimination enforces diversity *locally*, within each batch. If $n$ is too small, a single batch may only contain samples from one or a few of the true modes. The mechanism will still try to force these samples apart, which can be detrimental. It might prevent the generator from learning the true (often small) variance within a single mode, effectively distorting the learned distribution. Furthermore, it does not prevent inter-batch mode collapse. The generator could learn to produce diverse samples for one mode in one batch, and diverse samples for the *same* mode in the next batch, never discovering the other modes of the data distribution. Thus, while it is effective at preventing complete collapse to a single point, its ability to ensure coverage of all data modes is limited by the minibatch size.",
            "answer": "$$\\boxed{-1.623}$$"
        }
    ]
}