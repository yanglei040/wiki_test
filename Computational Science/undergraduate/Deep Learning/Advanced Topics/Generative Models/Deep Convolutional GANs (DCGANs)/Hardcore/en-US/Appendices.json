{
    "hands_on_practices": [
        {
            "introduction": "While GANs can learn complex data distributions, they often produce lower-quality or unrealistic images when sampling from the sparse, low-density tails of the latent distribution (e.g., a standard normal distribution). A widely used practical technique, known as the 'truncation trick,' addresses this by sampling from a narrower, truncated latent distribution. In this exercise (), we will mathematically formalize this concept using a simplified linear generator to precisely quantify the trade-off between sample fidelity (how realistic the images are) and diversity (the variety of outputs).",
            "id": "3112763",
            "problem": "You are studying the latent truncation trick in the context of a Deep Convolutional Generative Adversarial Network (DCGAN). The core question is how sampling the latent vector with reduced variance affects both the diversity and the fidelity of generated outputs. To make the analysis mathematically precise and computationally testable, you will work with a linearized generator near the latent origin that approximates the first upsampling and convolutional blocks.\n\nAssume the latent vector $z \\in \\mathbb{R}^m$ is sampled from a multivariate normal distribution with zero mean and isotropic covariance, specifically $z \\sim \\mathcal{N}(0, \\sigma^2 I_m)$, where $\\sigma  0$ parametrizes the truncation level. Consider a linear generator approximation $G(z) = A z + \\mu$, where $A \\in \\mathbb{R}^{d \\times m}$ has full row rank, $\\mu \\in \\mathbb{R}^d$, and $d \\le m$. The generated random vector is $X = G(z) \\in \\mathbb{R}^d$, which induces a generated distribution $p_g(x)$ that is Gaussian under this approximation.\n\nYour tasks are:\n\n$1.$ Starting only from core definitions and well-tested formulas, derive the following as functions of $\\sigma$:\n- The differential entropy of the generated distribution, $h(p_g)$.\n- A diversity measure defined as $D(\\sigma) = \\mathbb{E}\\left[\\lVert X - \\mu \\rVert_2^2\\right]$.\n- A fidelity score defined as the expected log-likelihood of generated samples under a fixed data model $p_{\\text{data}}(x) = \\mathcal{N}(\\mu_{\\text{data}}, \\Sigma_{\\text{data}})$, namely $F(\\sigma) = \\mathbb{E}_{X \\sim p_g}\\left[\\log p_{\\text{data}}(X)\\right]$.\n\nAssume $\\Sigma_{\\text{data}} \\in \\mathbb{R}^{d \\times d}$ is symmetric positive definite. Express your derivations in terms of $A$, $\\mu$, $\\mu_{\\text{data}}$, $\\Sigma_{\\text{data}}$, and $\\sigma$ only. All steps must begin from the fundamental definitions of a multivariate normal distribution, covariance propagation through linear maps, and the definition of differential entropy for continuous distributions.\n\n$2.$ Implement a program that, for each test case in the suite below, computes the triple $[h(p_g), D(\\sigma), F(\\sigma)]$ using your derived formulas. Use natural logarithms. The results must be printed as floating-point numbers. Aggregate the triples for all cases into a single list.\n\nTest suite. For each case, $d = 3$ and $m = 4$; all matrices and vectors are given explicitly:\n- Case $1$:\n  - $A = \\begin{bmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  1  1 \\end{bmatrix}$,\n  - $\\mu = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\mu_{\\text{data}} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\Sigma_{\\text{data}} = \\operatorname{diag}(1, 1, 1)$,\n  - $\\sigma = 1.0$.\n- Case $2$:\n  - Same $A$, $\\mu$, $\\mu_{\\text{data}}$, $\\Sigma_{\\text{data}}$ as Case $1$,\n  - $\\sigma = 0.2$.\n- Case $3$:\n  - $A = \\begin{bmatrix} 1  2  0  0 \\\\ 0  1  1  0 \\\\ 0  0  1  2 \\end{bmatrix}$,\n  - $\\mu = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.0 \\end{bmatrix}$,\n  - $\\mu_{\\text{data}} = \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ 0.1 \\end{bmatrix}$,\n  - $\\Sigma_{\\text{data}} = \\operatorname{diag}(0.8, 1.2, 1.5)$,\n  - $\\sigma = 0.5$.\n- Case $4$:\n  - $A = \\begin{bmatrix} 2  0  0  0 \\\\ 0  1  0  1 \\\\ 0  0  1  0 \\end{bmatrix}$,\n  - $\\mu = \\begin{bmatrix} 0.0 \\\\ 0.1 \\\\ -0.2 \\end{bmatrix}$,\n  - $\\mu_{\\text{data}} = \\begin{bmatrix} 0.1 \\\\ 0.0 \\\\ -0.1 \\end{bmatrix}$,\n  - $\\Sigma_{\\text{data}} = \\begin{bmatrix} 1.5  0.1  0.0 \\\\ 0.1  0.7  0.0 \\\\ 0.0  0.0  0.9 \\end{bmatrix}$,\n  - $\\sigma = 0.01$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of lists in the order of the cases, where each inner list is $[h, D, F]$ with each value formatted as a floating-point number. For example, a valid output would be of the form $[[h_1,D_1,F_1],[h_2,D_2,F_2],[h_3,D_3,F_3],[h_4,D_4,F_4]]$.",
            "solution": "The problem requires the derivation of three quantities related to a linearized generative model and their subsequent computation for a set of test cases. The model is defined by a latent variable $z \\in \\mathbb{R}^m$ drawn from $z \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ and a linear generator $G(z) = A z + \\mu$, producing an output $X = G(z) \\in \\mathbb{R}^d$.\n\nFirst, we characterize the distribution of the generated random vector $X$. Since $X$ is an affine transformation of a Gaussian random vector $z$, $X$ itself is a Gaussian random vector. Its mean $\\mu_g$ and covariance $\\Sigma_g$ are determined as follows:\nThe mean is $\\mu_g = \\mathbb{E}[X] = \\mathbb{E}[A z + \\mu] = A \\mathbb{E}[z] + \\mu$. Since $\\mathbb{E}[z] = 0$, we have $\\mu_g = \\mu$.\nThe covariance is $\\Sigma_g = \\text{Cov}(X) = \\text{Cov}(A z + \\mu) = A \\text{Cov}(z) A^T$. Given $\\text{Cov}(z) = \\sigma^2 I_m$, the covariance of $X$ is $\\Sigma_g = A (\\sigma^2 I_m) A^T = \\sigma^2 A A^T$.\nTherefore, the generated distribution $p_g$ is a multivariate normal distribution with parameters $(\\mu, \\sigma^2 A A^T)$, i.e., $X \\sim \\mathcal{N}(\\mu, \\sigma^2 A A^T)$. The problem states that $A$ has full row rank and $d \\le m$, which ensures that the $d \\times d$ matrix $A A^T$ is positive definite, and thus $\\Sigma_g$ is a valid non-singular covariance matrix.\n\nWith the distribution $p_g$ established, we can derive the three required quantities. All logarithms are natural logarithms.\n\n$1.$ Differential Entropy $h(p_g)$:\nThe differential entropy of a $d$-dimensional multivariate normal distribution $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ is given by the standard formula $h = \\frac{1}{2} \\log \\det(2\\pi e \\boldsymbol{\\Sigma})$.\nSubstituting the parameters of $p_g$, we have $\\boldsymbol{\\Sigma} = \\Sigma_g = \\sigma^2 A A^T$.\n$$\nh(p_g) = \\frac{1}{2} \\log \\det(2\\pi e (\\sigma^2 A A^T))\n$$\nUsing properties of the determinant and logarithm:\n$$\nh(p_g) = \\frac{1}{2} \\log((2\\pi e)^d \\det(\\sigma^2 A A^T)) = \\frac{1}{2} [ \\log((2\\pi e)^d) + \\log(\\sigma^{2d} \\det(A A^T)) ]\n$$\n$$\nh(p_g) = \\frac{1}{2} [ d \\log(2\\pi e) + 2d \\log(\\sigma) + \\log(\\det(A A^T)) ]\n$$\n$$\nh(p_g) = \\frac{d}{2} (\\log(2\\pi) + \\log(e)) + d \\log(\\sigma) + \\frac{1}{2} \\log(\\det(A A^T))\n$$\nSince $\\log(e) = 1$, the final expression for the differential entropy is:\n$$\nh(p_g) = d \\log(\\sigma) + \\frac{1}{2} \\log(\\det(A A^T)) + \\frac{d}{2} (1 + \\log(2\\pi))\n$$\n\n$2.$ Diversity Measure $D(\\sigma)$:\nThe diversity measure is defined as $D(\\sigma) = \\mathbb{E}\\left[\\lVert X - \\mu \\rVert_2^2\\right]$.\nThis is the expected squared Euclidean distance of a sample $X$ from its mean $\\mu_g = \\mu$. This quantity is also known as the trace of the covariance matrix of $X$.\nLet $Y = X - \\mu$. Then $\\mathbb{E}[Y]=0$ and $\\text{Cov}(Y) = \\Sigma_g$.\n$$\nD(\\sigma) = \\mathbb{E}\\left[ \\lVert Y \\rVert_2^2 \\right] = \\mathbb{E}[Y^T Y]\n$$\nUsing the cyclic property of the trace, $Y^T Y = \\text{Tr}(Y^T Y) = \\text{Tr}(Y Y^T)$.\n$$\nD(\\sigma) = \\mathbb{E}[\\text{Tr}(Y Y^T)] = \\text{Tr}(\\mathbb{E}[Y Y^T])\n$$\nSince $\\mathbb{E}[Y] = 0$, the definition of covariance gives $\\text{Cov}(Y) = \\mathbb{E}[(Y-\\mathbb{E}[Y])(Y-\\mathbb{E}[Y])^T] = \\mathbb{E}[Y Y^T]$. Thus:\n$$\nD(\\sigma) = \\text{Tr}(\\text{Cov}(Y)) = \\text{Tr}(\\Sigma_g) = \\text{Tr}(\\sigma^2 A A^T)\n$$\nBy linearity of the trace operator, the final expression for the diversity is:\n$$\nD(\\sigma) = \\sigma^2 \\text{Tr}(A A^T)\n$$\n\n$3.$ Fidelity Score $F(\\sigma)$:\nThe fidelity score is defined as the expected log-likelihood of a generated sample $X \\sim p_g$ under a given data distribution $p_{\\text{data}}(x) = \\mathcal{N}(\\mu_{\\text{data}}, \\Sigma_{\\text{data}})$.\n$$\nF(\\sigma) = \\mathbb{E}_{X \\sim p_g}[\\log p_{\\text{data}}(X)]\n$$\nThe log-probability density function for $p_{\\text{data}}$ is:\n$$\n\\log p_{\\text{data}}(x) = -\\frac{1}{2} (x - \\mu_{\\text{data}})^T \\Sigma_{\\text{data}}^{-1} (x - \\mu_{\\text{data}}) - \\frac{1}{2} \\log \\det(2\\pi \\Sigma_{\\text{data}})\n$$\nTaking the expectation with respect to $X \\sim p_g$:\n$$\nF(\\sigma) = \\mathbb{E}_{X \\sim p_g}\\left[ -\\frac{1}{2} (X - \\mu_{\\text{data}})^T \\Sigma_{\\text{data}}^{-1} (X - \\mu_{\\text{data}}) \\right] - \\frac{1}{2} \\log((2\\pi)^d \\det(\\Sigma_{\\text{data}}))\n$$\nThe second term is a constant with respect to the expectation. Let's focus on the expected value of the quadratic form. Using the trace trick, $\\mathbb{E}[(X - \\mu_{\\text{data}})^T \\Sigma_{\\text{data}}^{-1} (X - \\mu_{\\text{data}})] = \\mathbb{E}[\\text{Tr}(\\Sigma_{\\text{data}}^{-1} (X - \\mu_{\\text{data}})(X - \\mu_{\\text{data}})^T)]$.\nBy linearity of trace and expectation, this becomes $\\text{Tr}(\\Sigma_{\\text{data}}^{-1} \\mathbb{E}[(X - \\mu_{\\text{data}})(X - \\mu_{\\text{data}})^T])$.\nThe matrix $\\mathbb{E}[(X - \\mu_{\\text{data}})(X - \\mu_{\\text{data}})^T]$ is the second moment matrix of $X$ about $\\mu_{\\text{data}}$. It can be expressed in terms of the covariance $\\Sigma_g$ and mean $\\mu_g=\\mu$ of $X$ as:\n$$\n\\mathbb{E}[(X - \\mu_{\\text{data}})(X - \\mu_{\\text{data}})^T] = \\Sigma_g + (\\mu - \\mu_{\\text{data}})(\\mu - \\mu_{\\text{data}})^T\n$$\nSubstituting this back, the expectation of the quadratic form is:\n$$\n\\text{Tr}(\\Sigma_{\\text{data}}^{-1} (\\Sigma_g + (\\mu - \\mu_{\\text{data}})(\\mu - \\mu_{\\text{data}})^T)) = \\text{Tr}(\\Sigma_{\\text{data}}^{-1} \\Sigma_g) + \\text{Tr}(\\Sigma_{\\text{data}}^{-1} (\\mu - \\mu_{\\text{data}})(\\mu - \\mu_{\\text{data}})^T)\n$$\nThe second trace term simplifies to the scalar quadratic form $(\\mu - \\mu_{\\text{data}})^T \\Sigma_{\\text{data}}^{-1} (\\mu - \\mu_{\\text{data}})$.\nSubstituting $\\Sigma_g = \\sigma^2 A A^T$, the expectation becomes:\n$$\n\\sigma^2 \\text{Tr}(\\Sigma_{\\text{data}}^{-1} A A^T) + (\\mu - \\mu_{\\text{data}})^T \\Sigma_{\\text{data}}^{-1} (\\mu - \\mu_{\\text{data}})\n$$\nPutting everything together, the fidelity score is:\n$$\nF(\\sigma) = -\\frac{1}{2} \\left[ \\sigma^2 \\text{Tr}(\\Sigma_{\\text{data}}^{-1} A A^T) + (\\mu - \\mu_{\\text{data}})^T \\Sigma_{\\text{data}}^{-1} (\\mu - \\mu_{\\text{data}}) \\right] - \\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log(\\det(\\Sigma_{\\text{data}}))\n$$\nThese three derived formulas are implemented to compute the required values for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the [entropy, diversity, fidelity] triples for the\n    test cases provided in the problem description.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        ( # Case 1\n            np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 1]]),\n            np.array([0, 0, 0]),\n            np.array([0, 0, 0]),\n            np.identity(3),\n            1.0\n        ),\n        ( # Case 2\n            np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 1]]),\n            np.array([0, 0, 0]),\n            np.array([0, 0, 0]),\n            np.identity(3),\n            0.2\n        ),\n        ( # Case 3\n            np.array([[1, 2, 0, 0], [0, 1, 1, 0], [0, 0, 1, 2]]),\n            np.array([0.5, -0.5, 0.0]),\n            np.array([0.2, -0.1, 0.1]),\n            np.diag([0.8, 1.2, 1.5]),\n            0.5\n        ),\n        ( # Case 4\n            np.array([[2, 0, 0, 0], [0, 1, 0, 1], [0, 0, 1, 0]]),\n            np.array([0.0, 0.1, -0.2]),\n            np.array([0.1, 0.0, -0.1]),\n            np.array([[1.5, 0.1, 0.0], [0.1, 0.7, 0.0], [0.0, 0.0, 0.9]]),\n            0.01\n        )\n    ]\n\n    def calculate_metrics(A, mu, mu_data, Sigma_data, sigma):\n        \"\"\"\n        Calculates the entropy h, diversity D, and fidelity F using the derived formulas.\n        \"\"\"\n        # Ensure inputs are floating point numbers for precision.\n        A = A.astype(float)\n        mu = mu.astype(float)\n        mu_data = mu_data.astype(float)\n        Sigma_data = Sigma_data.astype(float)\n        sigma = float(sigma)\n        \n        d = float(A.shape[0])\n\n        # Intermediate computation: A @ A.T\n        A_AT = A @ A.T\n        \n        # --- 1. Differential Entropy h(p_g) ---\n        # h = d*log(sigma) + 0.5*log(det(A_AT)) + 0.5*d*(1 + log(2*pi))\n        # Use slogdet for numerical stability, which returns (sign, log(abs(det))).\n        # Since A_AT is positive definite, sign is 1.\n        _sign, log_det_A_AT = np.linalg.slogdet(A_AT)\n        h = d * np.log(sigma) + 0.5 * log_det_A_AT + 0.5 * d * (1.0 + np.log(2 * np.pi))\n\n        # --- 2. Diversity D(sigma) ---\n        # D = sigma^2 * tr(A_AT)\n        D = sigma**2 * np.trace(A_AT)\n\n        # --- 3. Fidelity F(sigma) ---\n        # F = -0.5*[sigma^2*Tr(inv(Sigma_d)*A_AT) + (mu-mu_d)^T*inv(Sigma_d)*(mu-mu_d)]\n        #     - 0.5*d*log(2*pi) - 0.5*log(det(Sigma_d))\n        inv_Sigma_data = np.linalg.inv(Sigma_data)\n        _sign, log_det_Sigma_data = np.linalg.slogdet(Sigma_data)\n\n        tr_term = sigma**2 * np.trace(inv_Sigma_data @ A_AT)\n        \n        delta_mu = mu - mu_data\n        mahalanobis_term = delta_mu.T @ inv_Sigma_data @ delta_mu\n\n        const_term = 0.5 * d * np.log(2 * np.pi) + 0.5 * log_det_Sigma_data\n\n        F = -0.5 * (tr_term + mahalanobis_term) - const_term\n        \n        return [h, D, F]\n\n    results = []\n    for case in test_cases:\n        A, mu, mu_data, Sigma_data, sigma = case\n        metrics = calculate_metrics(A, mu, mu_data, Sigma_data, sigma)\n        results.append(metrics)\n    \n    # Format the output string as a list of lists, with floating point numbers\n    # and no spaces.\n    output_str = \"[\" + \",\".join([f\"[{h},{D},{F}]\" for h, D, F in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The generator in a DCGAN learns a complex, high-dimensional mapping from a simple latent space to the intricate space of images. A key question is understanding the structure of this learned mapping. In this practice (), we explore this by performing linear interpolations between two points in the latent space and observing the path in the output image space. You will implement simplified generator models to see firsthand how nonlinearities can 'entangle' features, causing a straight-line path in the latent space to become a complex, curved path in the image space, a phenomenon we can quantify by measuring the smoothness of the output.",
            "id": "3112803",
            "problem": "Consider Deep Convolutional Generative Adversarial Networks (DCGANs), where the generator maps a latent vector to an image through convolutional computations and pointwise nonlinearities. Let the latent space be a real vector space of dimension $d$, and denote latent vectors by $\\mathbf{z} \\in \\mathbb{R}^d$. A generator is a function $G: \\mathbb{R}^d \\to \\mathbb{R}^{S \\times S}$ that produces an image of spatial resolution $S \\times S$ from a latent vector. The goal is to assess whether the generator exhibits linear disentanglement along a linear interpolation in latent space, or whether the generator induces an entangled manifold, by measuring output-path smoothness via the $L_2$ differences between consecutive generated images along the path.\n\nFundamental base and definitions:\n- Generative Adversarial Networks (GANs) define a generator $G$ that transforms a latent vector $\\mathbf{z}$ into a data sample; Deep Convolutional Generative Adversarial Networks (DCGANs) specialize $G$ through convolutional layers.\n- A linear interpolation path in latent space is $ \\mathbf{z}(t) = (1-t)\\,\\mathbf{z}_1 + t\\,\\mathbf{z}_2$ for $t \\in [0,1]$.\n- The $L_2$ norm of a vector $\\mathbf{x} \\in \\mathbb{R}^n$ is $ \\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^{n} x_i^2}$.\n- The path smoothness along a discretized interpolation $t_i$ is assessed by the sequence of $L_2$ differences $d_i = \\|G(\\mathbf{z}(t_{i+1})) - G(\\mathbf{z}(t_i))\\|_2$.\n- Angles in trigonometric functions are to be interpreted in radians.\n\nYou will implement three generator prototypes, each constructed from principled convolutional operations and upsampling, with fixed, deterministic kernels and basis maps. All constructions must use only linear convolution and pointwise nonlinearities; convolution must be two-dimensional and applied channel-wise or after channel mixing as specified. The following constants are used throughout:\n- Latent dimension $d = 8$.\n- Base feature grid size $s = 4$ and upsampled output spatial size $S = 8$.\n- Two intermediate channels, indexed by $c \\in \\{0,1\\}$.\n\nDefine the basis functions for constructing channel feature maps from a latent vector $\\mathbf{z} = (z_1,\\dots,z_d)$ as follows. For each channel $c \\in \\{0,1\\}$ and each spatial index $(i,j)$ with $i \\in \\{0,\\dots,s-1\\}$ and $j \\in \\{0,\\dots,s-1\\}$:\n- For channel $c=0$, define\n$$\na_k^{(0)}(i,j) = \\sin\\!\\left(\\frac{\\pi\\,(k+1)\\,(i+1)}{s}\\right) + \\cos\\!\\left(\\frac{\\pi\\,(k+1)\\,(j+1)}{s}\\right).\n$$\n- For channel $c=1$, define\n$$\na_k^{(1)}(i,j) = \\cos\\!\\left(\\frac{\\pi\\,(k+1)\\,(i+1)}{s}\\right) - \\sin\\!\\left(\\frac{\\pi\\,(k+1)\\,(j+1)}{s}\\right).\n$$\nConstruct the base feature maps $F_c \\in \\mathbb{R}^{s \\times s}$ by\n$$\nF_c(i,j) = \\frac{1}{d}\\sum_{k=1}^{d} z_k \\, a_k^{(c)}(i,j).\n$$\nUpsample each $F_c$ to $U_c \\in \\mathbb{R}^{S \\times S}$ by nearest-neighbor replication, that is, each element of $F_c$ is duplicated into a $2 \\times 2$ block.\n\nDefine two fixed convolution kernels of size $3 \\times 3$ with indices $p,q \\in \\{0,1,2\\}$:\n$$\nK_0(p,q) = \\frac{1}{1 + p + q}, \\quad K_1(p,q) = \\frac{(-1)^{p+q}}{1 + p + q}.\n$$\nConvolution is performed as two-dimensional convolution with stride $1$, padding handled symmetrically, and output shape equal to input shape (standard \"same\" convolution). The following three generator prototypes must be implemented:\n1. Linear generator $G_{\\mathrm{lin}}$: compute\n$$\nH_0 = \\mathrm{conv2d}(U_0, K_0), \\quad H_1 = \\mathrm{conv2d}(U_1, K_1), \\quad G_{\\mathrm{lin}}(\\mathbf{z}) = H_0 + H_1.\n$$\n2. Nonlinear gating generator $G_{\\mathrm{gate}}$: compute\n$$\nH_0 = \\mathrm{conv2d}(U_0, K_0), \\quad H_1 = \\mathrm{conv2d}(U_1, K_1),\n$$\nthen apply pointwise nonlinearities and multiplicative interaction,\n$$\nG_{\\mathrm{gate}}(\\mathbf{z}) = \\tanh(H_0) \\odot \\max(H_1, 0) + \\frac{1}{2}\\,\\mathrm{conv2d}(\\tanh(H_0), K_0),\n$$\nwhere $\\odot$ denotes elementwise multiplication.\n3. Saturating tanh generator $G_{\\tanh}$: compute\n$$\nG_{\\tanh}(\\mathbf{z}) = \\tanh\\!\\big(\\mathrm{conv2d}(U_0 + U_1, K_0)\\big).\n$$\n\nFor a given generator $G$, latent endpoints $\\mathbf{z}_1$ and $\\mathbf{z}_2$, a number of discrete steps $n \\in \\mathbb{N}$, and a threshold $\\varepsilon  0$, define a uniform discretization $t_i = \\frac{i}{n}$ for $i \\in \\{0,1,\\dots,n\\}$ and compute differences\n$$\nd_i = \\left\\| \\mathrm{vec}\\!\\big(G(\\mathbf{z}(t_{i+1}))\\big) - \\mathrm{vec}\\!\\big(G(\\mathbf{z}(t_i))\\big) \\right\\|_2,\\quad i \\in \\{0,1,\\dots,n-1\\},\n$$\nwhere $\\mathrm{vec}(\\cdot)$ denotes vectorization of the $S \\times S$ image into $\\mathbb{R}^{S^2}$. Let the mean and standard deviation of $\\{d_i\\}_{i=0}^{n-1}$ be $m$ and $s$, respectively, and define the coefficient of variation\n$$\n\\mathrm{CV} = \\begin{cases}\n\\frac{s}{m},  \\text{if } m > 0 \\\\\n0,  \\text{if } m = 0\n\\end{cases}\n$$\nClassify the generator behavior as \"linear disentanglement\" if $\\mathrm{CV} \\le \\varepsilon$, and \"entangled manifolds\" otherwise. Return a boolean for each test case, where $\\mathrm{True}$ denotes \"linear disentanglement\" and $\\mathrm{False}$ denotes \"entangled manifolds\".\n\nYour program must implement the above definitions and compute the classification for each of the following test cases (all angles in the trigonometric functions are in radians):\n\n- Test case $1$ (happy path, linear generator):\n  - Generator: $G_{\\mathrm{lin}}$.\n  - $\\mathbf{z}_1 = [0.1, -0.2, 0.3, -0.4, 0.5, -0.6, 0.7, -0.8]$.\n  - $\\mathbf{z}_2 = [-0.5, 0.4, -0.3, 0.2, -0.1, 0.0, 0.1, 0.2]$.\n  - Steps: $n = 20$.\n  - Threshold: $\\varepsilon = 10^{-6}$.\n\n- Test case $2$ (nonlinear gating, expected entanglement):\n  - Generator: $G_{\\mathrm{gate}}$.\n  - $\\mathbf{z}_1 = [-1.5, 0.0, 0.5, -0.2, 1.0, -1.2, 0.3, 0.7]$.\n  - $\\mathbf{z}_2 = [2.0, -0.5, 0.8, -1.0, 1.5, 0.0, -0.3, 0.1]$.\n  - Steps: $n = 20$.\n  - Threshold: $\\varepsilon = 0.02$.\n\n- Test case $3$ (boundary condition, identical endpoints):\n  - Generator: $G_{\\tanh}$.\n  - $\\mathbf{z}_1 = [0.3, -0.1, 0.05, 0.0, -0.2, 0.4, -0.3, 0.2]$.\n  - $\\mathbf{z}_2 = [0.3, -0.1, 0.05, 0.0, -0.2, 0.4, -0.3, 0.2]$.\n  - Steps: $n = 10$.\n  - Threshold: $\\varepsilon = 10^{-6}$.\n\n- Test case $4$ (saturation edge case):\n  - Generator: $G_{\\tanh}$.\n  - $\\mathbf{z}_1 = [3.0, -2.5, 2.2, -1.8, 1.6, -1.4, 1.2, -1.0]$.\n  - $\\mathbf{z}_2 = [-3.0, 2.4, -2.1, 1.9, -1.7, 1.5, -1.3, 1.1]$.\n  - Steps: $n = 20$.\n  - Threshold: $\\varepsilon = 0.02$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$), where each $result_i$ is a boolean as defined above.",
            "solution": "The user's problem statement has been analyzed and validated. It is scientifically grounded, well-posed, objective, and internally consistent. All definitions and parameters are provided, enabling a direct and unambiguous implementation. The problem is a stylized but conceptually sound exploration of feature entanglement in a simplified Deep Convolutional Generative Adversarial Network (DCGAN) generator, which is a relevant topic in deep learning.\n\nThe solution will be constructed by implementing the specified components in a step-by-step manner, corresponding to the mathematical definitions provided.\n\n### 1. Preliminaries and Constants\nThe problem defines several constants: latent dimension $d=8$, base grid size $s=4$, and output grid size $S=8$. The analysis involves two channels, indexed by $c \\in \\{0,1\\}$. All angles in trigonometric functions are specified to be in radians.\n\n### 2. Convolution Kernels\nTwo fixed $3 \\times 3$ convolution kernels, $K_0$ and $K_1$, are defined. For indices $p,q \\in \\{0,1,2\\}$:\n$$\nK_0(p,q) = \\frac{1}{1 + p + q}\n$$\n$$\nK_1(p,q) = \\frac{(-1)^{p+q}}{1 + p + q}\n$$\nThese kernels will be pre-computed as $3 \\times 3$ matrices. The convolution operation is a standard 2D convolution with stride $1$ and symmetric padding such that the output dimensions match the input dimensions ('same' convolution). For a $3 \\times 3$ kernel, this requires a padding of width $1$ on all sides of the input feature map.\n\n### 3. Basis Map Construction\nThe initial feature maps are constructed from a latent vector $\\mathbf{z} \\in \\mathbb{R}^d$ using a set of basis functions. For each channel $c \\in \\{0,1\\}$, a set of $d=8$ basis maps $\\{ a_k^{(c)} \\}_{k=1}^d$ are defined. For spatial indices $i \\in \\{0, \\dots, s-1\\}$ and $j \\in \\{0, \\dots, s-1\\}$, and basis index $k \\in \\{1, \\dots, d\\}$:\n- Channel $c=0$: $a_k^{(0)}(i,j) = \\sin\\!\\left(\\frac{\\pi\\,(k+1)\\,(i+1)}{s}\\right) + \\cos\\!\\left(\\frac{\\pi\\,(k+1)\\,(j+1)}{s}\\right)$\n- Channel $c=1$: $a_k^{(1)}(i,j) = \\cos\\!\\left(\\frac{\\pi\\,(k+1)\\,(i+1)}{s}\\right) - \\sin\\!\\left(\\frac{\\pi\\,(k+1)\\,(j+1)}{s}\\right)$\n\nThese basis maps of size $s \\times s$ are fixed and can be pre-computed.\n\n### 4. Base Feature Map Generation\nGiven a latent vector $\\mathbf{z} = (z_1, \\dots, z_d)$, the base feature maps $F_c \\in \\mathbb{R}^{s \\times s}$ are computed as a linear combination of the basis maps:\n$$\nF_c(i,j) = \\frac{1}{d}\\sum_{k=1}^{d} z_k \\, a_k^{(c)}(i,j)\n$$\nThis operation is linear with respect to the input latent vector $\\mathbf{z}$.\n\n### 5. Upsampling\nThe base feature maps $F_c$ of size $s \\times s = 4 \\times 4$ are upsampled to $U_c$ of size $S \\times S = 8 \\times 8$. The specified method is nearest-neighbor replication, where each value in $F_c$ is duplicated to form a $2 \\times 2$ block in $U_c$. This operation is also linear.\n\n### 6. Generator Architectures\nThree distinct generator functions, $G_{\\mathrm{lin}}$, $G_{\\mathrm{gate}}$, and $G_{\\tanh}$, are implemented. Each maps a latent vector $\\mathbf{z}$ to an $S \\times S$ output image.\n\n1.  **Linear Generator $G_{\\mathrm{lin}}$**: This generator is a purely linear transformation.\n    $$\n    H_0 = \\mathrm{conv2d}(U_0, K_0), \\quad H_1 = \\mathrm{conv2d}(U_1, K_1)\n    $$\n    $$\n    G_{\\mathrm{lin}}(\\mathbf{z}) = H_0 + H_1\n    $$\n    Since all constituent operations (basis map combination, upsampling, convolution, addition) are linear, the entire mapping from $\\mathbf{z}$ to $G_{\\mathrm{lin}}(\\mathbf{z})$ is linear.\n\n2.  **Nonlinear Gating Generator $G_{\\mathrm{gate}}$**: This generator introduces nonlinearities and channel interaction.\n    $$\n    H_0 = \\mathrm{conv2d}(U_0, K_0), \\quad H_1 = \\mathrm{conv2d}(U_1, K_1)\n    $$\n    $$\n    G_{\\mathrm{gate}}(\\mathbf{z}) = \\tanh(H_0) \\odot \\max(H_1, 0) + \\frac{1}{2}\\,\\mathrm{conv2d}(\\tanh(H_0), K_0)\n    $$\n    The pointwise hyperbolic tangent ($\\tanh$), ReLU-like gating ($\\max(H_1, 0)$), and elementwise multiplication ($\\odot$) make this a highly nonlinear function of $\\mathbf{z}$.\n\n3.  **Saturating Tanh Generator $G_{\\tanh}$**: This generator applies a final saturating nonlinearity.\n    $$\n    G_{\\tanh}(\\mathbf{z}) = \\tanh\\!\\big(\\mathrm{conv2d}(U_0 + U_1, K_0)\\big)\n    $$\n    Here, the channels are mixed by addition before convolution. The final $\\tanh$ function will cause saturation for large input values, a common feature in GANs.\n\n### 7. Path Smoothness Analysis\nThe core of the problem is to analyze the smoothness of the output path generated by a linear interpolation in the latent space.\n- The latent path is $\\mathbf{z}(t) = (1-t)\\,\\mathbf{z}_1 + t\\,\\mathbf{z}_2$ for $t \\in [0,1]$.\n- This path is discretized using $t_i = \\frac{i}{n}$ for $i \\in \\{0, 1, \\dots, n\\}$.\n- For each segment of the path, the $L_2$ distance between consecutive generated images is computed:\n$$\nd_i = \\left\\| \\mathrm{vec}\\!\\big(G(\\mathbf{z}(t_{i+1}))\\big) - \\mathrm{vec}\\!\\big(G(\\mathbf{z}(t_i))\\big) \\right\\|_2, \\quad \\text{for } i \\in \\{0, 1, \\dots, n-1\\}\n$$\nThis gives a sequence of $n$ distance values $\\{d_i\\}_{i=0}^{n-1}$.\n\n### 8. Classification\nThe behavior of the generator is classified based on the uniformity of these step sizes.\n- The mean $m$ and standard deviation $s$ of the sequence $\\{d_i\\}$ are calculated.\n- The coefficient of variation, $\\mathrm{CV} = s/m$ (or $0$ if $m=0$), is computed. A low $\\mathrm{CV}$ indicates that the step sizes $d_i$ are nearly constant, suggesting a linear or near-linear mapping along the path. A high $\\mathrm{CV}$ indicates varying step sizes, characteristic of a curved, entangled manifold.\n- The generator's behavior is classified as \"linear disentanglement\" (returning $\\mathrm{True}$) if $\\mathrm{CV} \\le \\varepsilon$, and \"entangled manifolds\" (returning $\\mathrm{False}$) otherwise.\n\n### 9. Test Case Execution\nThe implemented functions are applied to the four test cases provided.\n- For $G_{\\mathrm{lin}}$, we expect all $d_i$ values to be virtually identical because the generator is a linear transformation. This will result in $s \\approx 0$ and $\\mathrm{CV} \\approx 0$, leading to a `True` classification.\n- For the case where $\\mathbf{z}_1 = \\mathbf{z}_2$, the path is static. Thus, all $d_i=0$, leading to $m=0$, $s=0$, and $\\mathrm{CV}=0$, which results in a `True` classification.\n- For the nonlinear generators $G_{\\mathrm{gate}}$ and $G_{\\tanh}$ applied to distinct endpoints, the nonlinearities are expected to warp the latent path into a curve in the output space. The rate of travel along this curve will not be constant, leading to a significant standard deviation $s$ in the step sizes $d_i$, a non-negligible $\\mathrm{CV}$, and thus a `False` classification.\nThe final program calculates the boolean classification for each test case and prints them in the specified format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import convolve2d\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing the DCGAN generator prototypes and\n    analyzing their path smoothness.\n    \"\"\"\n\n    # Define constants from the problem statement\n    D_LATENT = 8\n    S_BASE = 4\n    S_OUTPUT = 8\n\n    # Pre-compute convolution kernels\n    p, q = np.mgrid[0:3, 0:3]\n    K0 = 1 / (1 + p + q)\n    K1 = ((-1)**(p + q)) / (1 + p + q)\n\n    # Pre-compute basis maps\n    k = np.arange(1, D_LATENT + 1)\n    i = np.arange(S_BASE)\n    j = np.arange(S_BASE)\n    \n    arg_i = np.pi * (k[:, None, None] + 1) * (i[None, :, None] + 1) / S_BASE\n    arg_j = np.pi * (k[:, None, None] + 1) * (j[None, None, :] + 1) / S_BASE\n    \n    BASIS_MAPS_0 = np.sin(arg_i) + np.cos(arg_j)\n    BASIS_MAPS_1 = np.cos(arg_i) - np.sin(arg_j)\n\n    def generate_base_maps(z: np.ndarray) - tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Generates the base feature maps F_0 and F_1 from a latent vector z.\n        \"\"\"\n        z_reshaped = z[:, np.newaxis, np.newaxis]\n        \n        weighted_maps_0 = z_reshaped * BASIS_MAPS_0\n        F0 = (1 / D_LATENT) * np.sum(weighted_maps_0, axis=0)\n        \n        weighted_maps_1 = z_reshaped * BASIS_MAPS_1\n        F1 = (1 / D_LATENT) * np.sum(weighted_maps_1, axis=0)\n        \n        return F0, F1\n\n    def upsample(F: np.ndarray) - np.ndarray:\n        \"\"\"\n        Upsamples a feature map using nearest-neighbor replication.\n        \"\"\"\n        return np.kron(F, np.ones((2, 2)))\n\n    # Define Generator Prototypes\n    def g_lin(z: np.ndarray) - np.ndarray:\n        F0, F1 = generate_base_maps(z)\n        U0, U1 = upsample(F0), upsample(F1)\n        H0 = convolve2d(U0, K0, mode='same', boundary='fill', fillvalue=0)\n        H1 = convolve2d(U1, K1, mode='same', boundary='fill', fillvalue=0)\n        return H0 + H1\n\n    def g_gate(z: np.ndarray) - np.ndarray:\n        F0, F1 = generate_base_maps(z)\n        U0, U1 = upsample(F0), upsample(F1)\n        H0 = convolve2d(U0, K0, mode='same', boundary='fill', fillvalue=0)\n        H1 = convolve2d(U1, K1, mode='same', boundary='fill', fillvalue=0)\n        tanh_H0 = np.tanh(H0)\n        relu_H1 = np.maximum(H1, 0)\n        term1 = tanh_H0 * relu_H1\n        term2 = 0.5 * convolve2d(tanh_H0, K0, mode='same', boundary='fill', fillvalue=0)\n        return term1 + term2\n\n    def g_tanh(z: np.ndarray) - np.ndarray:\n        F0, F1 = generate_base_maps(z)\n        U0, U1 = upsample(F0), upsample(F1)\n        U_sum = U0 + U1\n        H = convolve2d(U_sum, K0, mode='same', boundary='fill', fillvalue=0)\n        return np.tanh(H)\n\n    generator_map = {\n        \"G_lin\": g_lin,\n        \"G_gate\": g_gate,\n        \"G_tanh\": g_tanh,\n    }\n\n    def analyze_path(gen_name, z1, z2, n, epsilon):\n        \"\"\"\n        Performs the path analysis and returns the classification.\n        \"\"\"\n        gen_func = generator_map[gen_name]\n        z1_np = np.array(z1)\n        z2_np = np.array(z2)\n        \n        d_values = []\n        for i in range(n):\n            t_curr = i / n\n            t_next = (i + 1) / n\n            \n            z_curr = (1 - t_curr) * z1_np + t_curr * z2_np\n            z_next = (1 - t_next) * z1_np + t_next * z2_np\n            \n            img_curr = gen_func(z_curr)\n            img_next = gen_func(z_next)\n            \n            diff = np.linalg.norm(img_curr.flatten() - img_next.flatten())\n            d_values.append(diff)\n            \n        d_values_np = np.array(d_values)\n        \n        mean_d = np.mean(d_values_np)\n        std_d = np.std(d_values_np)\n        \n        if mean_d  0:\n            cv = std_d / mean_d\n        else:\n            cv = 0.0\n            \n        return cv = epsilon\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\"G_lin\", [0.1, -0.2, 0.3, -0.4, 0.5, -0.6, 0.7, -0.8], [-0.5, 0.4, -0.3, 0.2, -0.1, 0.0, 0.1, 0.2], 20, 1e-6),\n        (\"G_gate\", [-1.5, 0.0, 0.5, -0.2, 1.0, -1.2, 0.3, 0.7], [2.0, -0.5, 0.8, -1.0, 1.5, 0.0, -0.3, 0.1], 20, 0.02),\n        (\"G_tanh\", [0.3, -0.1, 0.05, 0.0, -0.2, 0.4, -0.3, 0.2], [0.3, -0.1, 0.05, 0.0, -0.2, 0.4, -0.3, 0.2], 10, 1e-6),\n        (\"G_tanh\", [3.0, -2.5, 2.2, -1.8, 1.6, -1.4, 1.2, -1.0], [-3.0, 2.4, -2.1, 1.9, -1.7, 1.5, -1.3, 1.1], 20, 0.02)\n    ]\n\n    results = []\n    for case in test_cases:\n        gen_name, z1, z2, n, epsilon = case\n        result = analyze_path(gen_name, z1, z2, n, epsilon)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "To gain an even deeper, more localized understanding of the generator's mapping, we can turn to the tools of calculus. The Jacobian matrix, which contains all the first-order partial derivatives of the generator's output with respect to its latent input, provides the best linear approximation of the generator at a single point. This exercise () will guide you through constructing the Jacobian for a simple DCGAN generator and using its singular value decomposition (SVD) to analyze local properties. This powerful technique reveals which directions in the latent space cause the most significant changes in the output image and which dimensions might be locally redundant.",
            "id": "3112792",
            "problem": "Consider a Deep Convolutional Generative Adversarial Network (DCGAN), where the generator $G$ maps a latent vector $z \\in \\mathbb{R}^d$ to a spatially arranged output through a sequence of operations that are standard in convolutional neural networks: a fully connected affine transformation, a Rectified Linear Unit (ReLU) nonlinearity, a two-dimensional transposed convolution, and a hyperbolic tangent ($\\tanh$) output nonlinearity. The goal is to derive, from first principles, the Jacobian $J = \\partial G(z)/\\partial z$, interpret its singular value spectrum to assess the local sensitivity of spatial features to latent coordinates, and evaluate the redundancy of latent dimensions.\n\nStart from the following fundamental base:\n- The chain rule for Jacobians of compositions, which states that for a composition of functions $G = f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1$, the Jacobian is $J_G(z) = J_{f_L}(f_{L-1}(\\cdots)) \\cdots J_{f_1}(z)$.\n- The Jacobian of a linear map $x \\mapsto Ax$ is the matrix $A$.\n- The derivative of the Rectified Linear Unit (ReLU) applied elementwise is $1$ when the pre-activation is strictly positive and $0$ otherwise.\n- The derivative of the hyperbolic tangent is $\\mathrm{sech}^2(y) = 1 - \\tanh^2(y)$ applied elementwise at the pre-$\\tanh$ activation $y$.\n- The two-dimensional transposed convolution is a linear operator from input feature maps to output feature maps that can be represented as a matrix $T$ acting on a flattened input.\n\nIn all test cases below, the generator is defined by the following generic composition:\n1. A fully connected linear map $a_0 = W z + b$ where $W \\in \\mathbb{R}^{N_{\\text{in}} \\times d}$ and $b \\in \\mathbb{R}^{N_{\\text{in}}}$, producing $N_{\\text{in}}$ pre-activation feature values.\n2. An elementwise Rectified Linear Unit $h_0 = \\max(a_0, 0)$ reshaped into $C_{\\text{in}}$ input channels of spatial size $H_0 \\times W_0$ in channel-major, row-major order.\n3. A two-dimensional transposed convolution with stride $s = 1$ and no padding, using kernels $K$ to produce a single output channel of spatial size $H_{\\text{out}} \\times W_{\\text{out}}$, where $H_{\\text{out}} = H_0 + k_h - 1$ and $W_{\\text{out}} = W_0 + k_w - 1$ for kernel height $k_h$ and width $k_w$. The transposed convolution is defined by\n$$\ny[u, v] = \\sum_{c=0}^{C_{\\text{in}}-1} \\sum_{i=0}^{H_0-1} \\sum_{j=0}^{W_0-1} h_0[c, i, j] \\cdot K[c, u-i, v-j],\n$$\nwith valid index ranges such that $0 \\le u-i  k_h$ and $0 \\le v-j  k_w$; $y$ is the pre-$\\tanh$ output.\n4. An elementwise hyperbolic tangent $o = \\tanh(y)$.\n\nUnder this model, the Jacobian with respect to $z$ at a given point $z$ is\n$$\nJ(z) = D_{\\tanh}(y) \\, T \\, D_{\\mathrm{ReLU}}(a_0) \\, W,\n$$\nwhere $D_{\\mathrm{ReLU}}(a_0)$ is a diagonal matrix with entries $1$ if the corresponding $a_0$ component is strictly positive and $0$ otherwise, $T$ is the matrix representation of the transposed convolution that maps the flattened $h_0$ into the flattened $y$, and $D_{\\tanh}(y)$ is a diagonal matrix with entries $1 - \\tanh^2(y)$ evaluated at the flattened $y$.\n\nYou must implement this mapping and compute the singular values of $J(z)$ using the Singular Value Decomposition (SVD). Interpret large singular values as high sensitivity of the spatial features to certain latent directions and very small singular values (relative to a numerical threshold) as locally redundant or suppressed latent directions. For numerical rank determination, use a threshold $\\epsilon$ where a singular value $s$ contributes to the rank if $s \\ge \\epsilon$.\n\nYour program must implement the generator exactly as specified in each test case and must compute, for each test case, the following outputs:\n- The largest singular value of $J(z)$, denoted $s_{\\max}$.\n- The smallest singular value of $J(z)$ among all singular values returned by the SVD (which equals $0$ if $J(z)$ is rank-deficient), denoted $s_{\\min}$.\n- The fraction of latent dimensions that are effectively active, defined as the numerical rank divided by $d$, that is,\n$$\n\\text{rank fraction} = \\frac{\\#\\{s_i \\mid s_i \\ge \\epsilon\\}}{d}.\n$$\nExpress all three quantities as decimal numbers.\n\nThe final output format must be a single line containing a list of per-test-case results. Each per-test-case result is a list of three decimal numbers in the order $[s_{\\max}, s_{\\min}, \\text{rank fraction}]$. The overall output must be a single list of these per-test-case lists, printed as a single line with no additional text, for example, \n$[\\,[s_{\\max}^{(1)}, s_{\\min}^{(1)}, r^{(1)}],\\, [s_{\\max}^{(2)}, s_{\\min}^{(2)}, r^{(2)}],\\, \\ldots\\,]$.\n\nTest Suite Specification:\n- Test Case $1$ (happy path):\n    - Latent dimension $d = 4$.\n    - Input channels $C_{\\text{in}} = 1$, output channels $C_{\\text{out}} = 1$.\n    - Spatial size $H_0 = 2$, $W_0 = 2$.\n    - Fully connected weights $W \\in \\mathbb{R}^{4 \\times 4}$ equal to $0.5 I_4$ (where $I_4$ is the $4 \\times 4$ identity), bias $b = 0$.\n    - ReLU applied elementwise.\n    - Transposed convolution kernel $K \\in \\mathbb{R}^{1 \\times 1 \\times 2 \\times 2}$ with the single input channel kernel \n    $$\n    K^{(0)} = \\begin{bmatrix} 0.5  -0.25 \\\\ 0.75  0.25 \\end{bmatrix}.\n    $$\n    - Output nonlinearity is $\\tanh$ applied elementwise.\n    - Latent vector \n    $$\n    z^{(1)} = \\begin{bmatrix} 0.2 \\\\ -0.4 \\\\ 0.1 \\\\ 0.3 \\end{bmatrix}.\n    $$\n    - Numerical threshold $\\epsilon = 10^{-9}$.\n\n- Test Case $2$ (boundary gating case):\n    - Same architecture and parameters as Test Case $1$.\n    - Latent vector \n    $$\n    z^{(2)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n    $$\n    - Numerical threshold $\\epsilon = 10^{-9}$.\n    - Note: With the convention that the derivative of ReLU at $0$ is $0$, the Jacobian should collapse to the zero matrix in this case.\n\n- Test Case $3$ (multi-channel edge case):\n    - Latent dimension $d = 4$.\n    - Input channels $C_{\\text{in}} = 2$, output channels $C_{\\text{out}} = 1$.\n    - Spatial size $H_0 = 2$, $W_0 = 2$.\n    - Fully connected weights $W \\in \\mathbb{R}^{8 \\times 4}$ with rows specified in channel-major, row-major order for the $2$ input channels, each of size $2 \\times 2$. Explicitly,\n    $$\n    W = \\begin{bmatrix}\n    0.4  -0.3  0.1  0.0 \\\\\n    0.0  0.2  -0.1  0.5 \\\\\n    -0.2  0.1  0.3  -0.4 \\\\\n    0.3  0.0  0.2  0.1 \\\\\n    -0.1  0.4  0.0  -0.2 \\\\\n    0.2  -0.5  0.3  0.0 \\\\\n    0.0  0.1  -0.3  0.4 \\\\\n    0.5  0.2  0.1  -0.1\n    \\end{bmatrix}, \\quad b = 0.\n    $$\n    - ReLU applied elementwise.\n    - Transposed convolution kernels $K \\in \\mathbb{R}^{2 \\times 1 \\times 2 \\times 2}$, one per input channel (aggregated to a single output channel), given by\n    $$\n    K^{(0)} = \\begin{bmatrix} 0.3  -0.1 \\\\ 0.2  0.4 \\end{bmatrix}, \\quad\n    K^{(1)} = \\begin{bmatrix} -0.2  0.5 \\\\ 0.1  -0.3 \\end{bmatrix}.\n    $$\n    - Output nonlinearity is $\\tanh$ applied elementwise.\n    - Latent vector \n    $$\n    z^{(3)} = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\\\ 0.2 \\end{bmatrix}.\n    $$\n    - Numerical threshold $\\epsilon = 10^{-9}$.\n\nImplementation and Output Requirements:\n- Implement the above generator mathematically and algorithmically, using the chain rule to construct $J(z)$ as the product of the diagonal Jacobians and linear operators, with $T$ constructed explicitly by its action on basis inputs.\n- Compute the singular values of $J(z)$ via Singular Value Decomposition.\n- For each test case, output a list $[s_{\\max}, s_{\\min}, \\text{rank fraction}]$ as decimal numbers.\n- The program must print a single line containing a list of the per-test-case lists, e.g., \n$[\\,[s_{\\max}^{(1)}, s_{\\min}^{(1)}, r^{(1)}],\\, [s_{\\max}^{(2)}, s_{\\min}^{(2)}, r^{(2)}],\\, [s_{\\max}^{(3)}, s_{\\min}^{(3)}, r^{(3)}]\\,]$.",
            "solution": "The user-provided problem is valid. It is a well-defined and scientifically grounded exercise in applying differential calculus to analyze the local properties of a deep convolutional generative adversarial network (DCGAN) generator. All parameters and architectural details are specified, allowing for a unique and verifiable solution.\n\nThe core of the problem is to compute the Jacobian matrix $J(z) = \\frac{\\partial G(z)}{\\partial z}$ for a given generator model $G$ and a latent vector $z$, and then to analyze this Jacobian using its singular value decomposition (SVD). The generator $G$ is a composition of four standard neural network layers:\n$1$. A fully connected (affine) layer: $a_0 = W z + b$\n$2$. An elementwise Rectified Linear Unit (ReLU) activation: $h_0 = \\max(a_0, 0)$\n$3$. A two-dimensional transposed convolution, which is a linear operator we can represent by a matrix $T$: $y = T h_{0,\\text{flat}}$\n$4$. An elementwise hyperbolic tangent ($\\tanh$) activation: $o = \\tanh(y)$\n\nThe generator is thus the function composition $G(z) = \\tanh(T \\cdot \\mathrm{ReLU}(Wz+b))$. By the chain rule for derivatives of vector functions, the Jacobian of $G$ is the product of the Jacobians of its constituent layers, evaluated at the appropriate intermediate values. The problem provides the resulting formula:\n$$\nJ(z) = D_{\\tanh}(y) \\, T \\, D_{\\mathrm{ReLU}}(a_0) \\, W\n$$\nHere, $D_{\\mathrm{ReLU}}(a_0)$ and $D_{\\tanh}(y)$ are diagonal matrices representing the elementwise derivatives of the ReLU and $\\tanh$ functions, respectively. The matrices $W$ and $T$ represent the linear transformations of the fully connected and transposed convolution layers.\n\nTo implement the solution, we follow these steps for each test case:\n\n**Step 1: Forward Pass**\nFirst, we must perform a forward pass through the generator network with the given latent vector $z$ to compute the intermediate pre-activation values needed for the Jacobians.\n- The pre-ReLU activations are computed as $a_0 = W z + b$.\n- The post-ReLU activations are $h_0 = \\max(a_0, 0)$. This vector is then reshaped into a tensor of shape $(C_{\\text{in}}, H_0, W_0)$.\n- The pre-$\\tanh$ activations $y$ are computed by applying the transposed convolution to the $h_0$ tensor.\n- The final output is $o=\\tanh(y)$, although this is not strictly needed for the Jacobian calculation itself.\n\n**Step 2: Jacobian Matrix Construction**\nWith the intermediate values from the forward pass, we construct each matrix in the Jacobian product:\n- $W$: The weight matrix of the initial fully-connected layer is given directly in each test case.\n- $D_{\\mathrm{ReLU}}(a_0)$: This is a diagonal matrix of size $N_{\\text{in}} \\times N_{\\text{in}}$, where $N_{\\text{in}} = C_{\\text{in}} \\times H_0 \\times W_0$. The $i$-th diagonal element is $1$ if the corresponding pre-activation $a_{0,i}  0$, and $0$ otherwise, as per the problem's specified derivative for ReLU.\n- $T$: This matrix represents the linear operation of the transposed convolution. It has dimensions $N_{\\text{out}} \\times N_{\\text{in}}$, where $N_{\\text{out}} = H_{\\text{out}} \\times W_{\\text{out}}$. An element $T_{pq}$ of this matrix corresponds to the influence of the $q$-th input unit (in flattened $h_0$) on the $p$-th output unit (in flattened $y$). This can be determined by considering the transposed convolution formula. The element of $T$ at row-index `out_idx` (corresponding to spatial location $(u,v)$ in the output) and column-index `in_idx` (corresponding to input channel $c$ and location $(i,j)$) is given by the value of the kernel that connects them: $K[c, u-i, v-j]$, provided the kernel indices are valid, and $0$ otherwise.\n- $D_{\\tanh}(y)$: This is a diagonal matrix of size $N_{\\text{out}} \\times N_{\\text{out}}$. The $j$-th diagonal element is given by the derivative of the hyperbolic tangent, $1 - \\tanh^2(y_j)$, evaluated at the pre-activation $y_j$.\n\n**Step 3: Jacobian Computation and SVD**\nThe full Jacobian matrix $J(z)$ is computed as the matrix product of these four components. Once $J(z)$ is obtained, we perform a Singular Value Decomposition (SVD), which decomposes $J(z)$ into $U S V^T$, where $S$ is a diagonal matrix of singular values $s_i \\ge 0$. These singular values quantify how much the mapping $G$ stretches space along different orthogonal directions in the input and output spaces.\n\n**Step 4: Analysis of Singular Values**\nThe computed singular values are used to determine the required metrics:\n- The largest singular value, $s_{\\max} = \\max_i s_i$, indicates the maximum local sensitivity of the output to changes in the latent vector.\n- The smallest singular value, $s_{\\min} = \\min_i s_i$, indicates the minimum local sensitivity. If $s_{\\min}$ is zero or close to it, some latent directions are locally \"collapsed\" or have no effect on the output.\n- The rank fraction is computed as the numerical rank of $J(z)$ divided by the latent dimension $d$. The numerical rank is the count of singular values $s_i$ that are greater than or equal to a small threshold $\\epsilon = 10^{-9}$. This fraction measures the proportion of latent dimensions that are locally active or non-redundant.\n\nThis comprehensive procedure is encapsulated in the provided Python code, which systematically constructs the matrices, computes the Jacobian, performs the SVD, and extracts the specified metrics for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for a suite of test cases.\n    It orchestrates the setup of each case and the final printing of results.\n    \"\"\"\n\n    def _compute_analysis(z, d, W, b, K, C_in, H0, W0, k_h, k_w, epsilon):\n        \"\"\"\n        Helper function to compute the Jacobian and its analysis for a single test case.\n\n        This function performs the forward pass to get intermediate activations,\n        constructs the component matrices of the Jacobian, computes the full\n        Jacobian via matrix multiplication, and finally analyzes its singular\n        value spectrum.\n        \"\"\"\n        # --- 1. Forward Pass to compute intermediate activations ---\n        N_in = C_in * H0 * W0\n\n        # Layer 1: Fully connected\n        a0 = W @ z + b\n\n        # Layer 2: ReLU\n        h0 = np.maximum(a0, 0)\n        h0_tensor = h0.reshape((C_in, H0, W0))\n\n        # Layer 3: Transposed Convolution\n        H_out = H0 + k_h - 1\n        W_out = W0 + k_w - 1\n        N_out = H_out * W_out\n        \n        # Pre-tanh output y\n        y = np.zeros((H_out, W_out))\n        for c_idx in range(C_in):\n            # This check accelerates computation if a whole channel in h0 is zero\n            if np.any(h0_tensor[c_idx, :, :]):\n                for i in range(H0):\n                    for j in range(W0):\n                        # This check avoids the inner loops if the specific activation is zero\n                        if h0_tensor[c_idx, i, j]  0:\n                            for u in range(H_out):\n                                for v in range(W_out):\n                                    ki = u - i\n                                    kj = v - j\n                                    if 0 = ki  k_h and 0 = kj  k_w:\n                                        y[u, v] += h0_tensor[c_idx, i, j] * K[c_idx, 0, ki, kj]\n        \n        y_flat = y.flatten()\n\n        # --- 2. Jacobian Matrix Construction ---\n        \n        # W is given.\n\n        # Diagonal Jacobian of ReLU, D_relu\n        d_relu_diag = (a0  0).astype(float)\n        D_relu = np.diag(d_relu_diag)\n\n        # Matrix for Transposed Convolution, T\n        T = np.zeros((N_out, N_in))\n        for c_idx in range(C_in):\n            for i in range(H0):\n                for j in range(W0):\n                    in_idx = c_idx * (H0 * W0) + i * W0 + j\n                    for u in range(H_out):\n                        for v in range(W_out):\n                            out_idx = u * W_out + v\n                            ki = u - i\n                            kj = v - j\n                            if 0 = ki  k_h and 0 = kj  k_w:\n                                T[out_idx, in_idx] = K[c_idx, 0, ki, kj]\n\n        # Diagonal Jacobian of tanh, D_tanh\n        d_tanh_diag = 1 - np.tanh(y_flat)**2\n        D_tanh = np.diag(d_tanh_diag)\n        \n        # Full Jacobian J(z) using the chain rule: J = D_tanh @ T @ D_relu @ W\n        # Dimensions: (N_out x N_out) @ (N_out x N_in) @ (N_in x N_in) @ (N_in x d) - (N_out x d)\n        J = D_tanh @ T @ D_relu @ W\n\n        # --- 3. Singular Value Decomposition (SVD) and Analysis ---\n        if J.size == 0:\n            s = np.array([])\n        else:\n            s = np.linalg.svd(J, compute_uv=False)\n\n        s_max = np.max(s) if s.size  0 else 0.0\n        \n        # The number of singular values is min(N_out, d), which is d=4 in all cases.\n        s_min = np.min(s) if s.size  0 else 0.0\n        \n        numerical_rank = np.sum(s = epsilon)\n        rank_fraction = numerical_rank / d\n        \n        return [float(s_max), float(s_min), float(rank_fraction)]\n\n    # --- Test Suite Specification ---\n\n    # Test Case 1: Happy path\n    d1 = 4\n    z1 = np.array([0.2, -0.4, 0.1, 0.3])\n    W1 = 0.5 * np.identity(4)\n    b1 = np.zeros(4)\n    K1_mat = np.array([[0.5, -0.25], [0.75, 0.25]])\n    K1 = K1_mat.reshape(1, 1, 2, 2)\n    case1_params = {\n        \"z\": z1, \"d\": d1, \"W\": W1, \"b\": b1, \"K\": K1,\n        \"C_in\": 1, \"H0\": 2, \"W0\": 2, \"k_h\": 2, \"k_w\": 2, \"epsilon\": 1e-9\n    }\n    result1 = _compute_analysis(**case1_params)\n\n    # Test Case 2: Boundary gating case (z=0)\n    z2 = np.zeros(4)\n    case2_params = {**case1_params, \"z\": z2}\n    result2 = _compute_analysis(**case2_params)\n\n    # Test Case 3: Multi-channel edge case\n    d3 = 4\n    z3 = np.array([0.1, -0.2, 0.05, 0.2])\n    W3 = np.array([\n        [0.4, -0.3, 0.1, 0.0], [0.0, 0.2, -0.1, 0.5],\n        [-0.2, 0.1, 0.3, -0.4], [0.3, 0.0, 0.2, 0.1],\n        [-0.1, 0.4, 0.0, -0.2], [0.2, -0.5, 0.3, 0.0],\n        [0.0, 0.1, -0.3, 0.4], [0.5, 0.2, 0.1, -0.1]\n    ])\n    b3 = np.zeros(8)\n    K3_c0 = np.array([[0.3, -0.1], [0.2, 0.4]])\n    K3_c1 = np.array([[-0.2, 0.5], [0.1, -0.3]])\n    K3 = np.stack([K3_c0, K3_c1]).reshape(2, 1, 2, 2)\n    case3_params = {\n        \"z\": z3, \"d\": d3, \"W\": W3, \"b\": b3, \"K\": K3,\n        \"C_in\": 2, \"H0\": 2, \"W0\": 2, \"k_h\": 2, \"k_w\": 2, \"epsilon\": 1e-9\n    }\n    result3 = _compute_analysis(**case3_params)\n\n    results = [result1, result2, result3]\n    \n    # Final print statement in the exact required format.\n    # str() on a list of floats includes spaces, and join adds commas.\n    # The final string is a valid representation of a list of lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}