{
    "hands_on_practices": [
        {
            "introduction": "自注意力机制的核心是 $softmax$ 函数，它将得分转换为概率分布。然而，当输入值非常大或非常小时，直接计算 $softmax$ 可能会导致数值溢出或下溢，从而破坏计算的稳定性。本练习将指导你实现一个关键的技巧——“log-sum-exp”——来稳定 $softmax$ 的计算，并量化不同浮点精度下的数值误差，这是任何从头开始实现注意力机制的人都必须掌握的基本技能。",
            "id": "3192585",
            "problem": "考虑一个自注意力评分设置，其中为索引为 $i$ 的查询（query）和索引为 $j$ 的键（key）计算一组能量 $e_{ij}$，并通过对能量逐行应用 softmax 函数来获得注意力权重。对于行向量 $\\mathbf{e}_i$，softmax 函数的基本定义为 $softmax(\\mathbf{e}_i)_j = \\exp(e_{ij}) \\big/ \\sum_{k} \\exp(e_{ik})$，其中 $\\exp$ 表示指数函数，$\\sum$ 表示有限求和。$\\log \\sum_{j} \\exp(e_{ij})$ 的计算是 softmax 数值分析的核心，因为 $\\log$ 是 $\\exp$ 的反函数，并将乘积转换为和。电气与电子工程师协会 (IEEE) 754 标准的浮点格式 fp16 (binary 16) 和 fp32 (binary 32) 在精度和动态范围上有所不同，这在评估 $\\exp$、$\\log$ 和 $softmax$ 时会影响数值稳定性和舍入误差。\n\n仅从指数函数 $\\exp$、自然对数 $\\log$ 和 softmax 函数 $softmax$ 的定义出发，为每行 $i$ 推导一个用于计算 $\\log \\sum_{j} \\exp(e_{ij})$ 的数值稳定变换，并解释为什么该稳定变换能在不改变数学值的情况下减少上溢和下溢。然后，使用该稳定变换，设计一个算法，以指定的浮点精度逐行计算 $softmax(\\mathbf{e}_i)$，同时减轻灾难性的数值问题。\n\n实现该算法，通过将 fp16 和 fp32 稳定化 softmax 的输出与高精度 fp64 (binary 64) 稳定化参考值进行比较，来量化不同浮点精度下的数值误差。对于下方的每个测试用例，生成以下四个量：\n- fp16 稳定化 softmax 与 fp64 稳定化参考值在测试矩阵所有条目上的最大绝对差（一个实数）。\n- fp32 稳定化 softmax 与 fp64 稳定化参考值在测试矩阵所有条目上的最大绝对差（一个实数）。\n- 在 fp64 精度下，朴素直接计算 $\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right)$ 与相同数量的稳定化计算之间的逐行最大绝对差（一个实数；如果某行的朴素计算值不是有限的，则其对该最大值的贡献视为 $nan$）。\n- 一个布尔值，指示是否有任何行的朴素计算 $\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right)$ 在 fp64 中不是有限的（即上溢到 $+\\infty$ 或 $nan$ 的传播）。\n\n使用以下能量矩阵测试套件，每个矩阵都被理解为 softmax 的逐行输入 $\\mathbf{e}_i$。所有条目均为无单位实数值。在计算中，您必须独立处理每个矩阵行。这些矩阵是：\n- 案例 $1$（中等值）：$$E^{(1)} = \\begin{bmatrix} -1  0  1  2 \\\\ 0.5  -0.5  3  -3 \\end{bmatrix}.$$\n- 案例 $2$（极端范围）：$$E^{(2)} = \\begin{bmatrix} 1000  -1000  0  1 \\\\ 88  87  86  85 \\end{bmatrix}.$$\n- 案例 $3$（均匀输入）：$$E^{(3)} = \\begin{bmatrix} 0  0  0  0 \\\\ 0  0  0  0 \\end{bmatrix}.$$\n- 案例 $4$（极大负值）：$$E^{(4)} = \\begin{bmatrix} -1000  -1001  -999  -1200 \\\\ -50  -60  -70  -80 \\end{bmatrix}.$$\n\n算法要求：\n- 对于每个矩阵 $E^{(k)}$，根据 $softmax$ 的定义和您推导的稳定变换，逐行计算一个稳定化的 fp64 参考 softmax。\n- 对于每个矩阵 $E^{(k)}$，使用相同的算法在 fp16 和 fp32 中逐行计算稳定化的 $softmax$，但算术运算在指定的精度下执行。\n- 对于每个矩阵 $E^{(k)}$，直接根据定义逐行计算朴素的 fp64 值 $\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right)$，并与相同数量的稳定化 fp64 值进行比较。\n\n答案格式规范：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中第 $k$ 个元素是对应于案例 $k$ 的四个项目的列表，顺序如上所述。例如，最终输出应类似于 `[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3],[a_4,b_4,c_4,d_4]]`，除了分隔数字和布尔值所必需的空格外，不插入任何多余空格。\n\n无需外部输入。所有计算都是无单位实数。不涉及角度。将所有差异量表示为实数。最后一行必须是唯一的输出。",
            "solution": "目标是通过对能量 $e_{ij}$ 应用 softmax 函数来逐行计算注意力权重，同时保持数值稳定性，然后测量与精度相关的数值误差。\n\n我们从核心定义和属性开始：\n\n$1.$ 对于行向量 $\\mathbf{e}_i$ 的 softmax 定义：\n$$softmax(\\mathbf{e}_i)_j = \\frac{\\exp(e_{ij})}{\\sum_{k} \\exp(e_{ik})}.$$\nSoftmax 将实数值分数 $e_{ij}$ 映射到总和为 $1$ 的非负权重，这是自注意力中注意力分布的一个要求。\n\n$2.$ 指数和对数的基本性质：\n指数函数 $\\exp$ 是严格递增的，并将 $\\mathbb{R}$ 映射到 $(0,\\infty)$。自然对数 $\\log$ 是它在 $(0,\\infty)$ 上的反函数，并且对于 $a>0$ 和 $b>0$ 满足 $\\log(ab) = \\log(a) + \\log(b)$。这些性质允许对指数和进行受控的操作。\n\n$3.$ 数值问题：\n如果任何 $e_{ij}$ 非常大（导致 $\\exp(e_{ij})$ 超出可表示范围），直接计算 $\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right)$ 可能会上溢；如果 $e_{ij}$ 是非常大的负数（导致 $\\exp(e_{ij})$ 四舍五入为 $0$），则可能会下溢。这种上溢或下溢会损害 softmax 分母的正确性，从而影响 $softmax(\\mathbf{e}_i)$ 的正确性。\n\n为了推导 $\\log \\sum_{j} \\exp(e_{ij})$ 的数值稳定变换，我们使用基于对数-指数关系的因式分解。设 $m_i$ 为行 $\\mathbf{e}_i$ 的最大元素，即 $m_i = \\max_j e_{ij}$。那么：\n\n$$\n\\sum_{j} \\exp(e_{ij}) = \\sum_{j} \\exp\\left((e_{ij} - m_i) + m_i\\right) = \\sum_{j} \\left[\\exp(e_{ij} - m_i)\\cdot \\exp(m_i)\\right] = \\exp(m_i)\\cdot \\sum_{j} \\exp(e_{ij} - m_i).\n$$\n\n应用 $\\log$ 和属性 $\\log(ab) = \\log(a) + \\log(b)$ 得出\n\n$$\n\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right) = \\log\\left( \\exp(m_i)\\cdot \\sum_{j} \\exp(e_{ij} - m_i) \\right) = m_i + \\log \\left( \\sum_{j} \\exp(e_{ij} - m_i) \\right).\n$$\n\n这个变换是数值稳定的，因为：\n- 对于所有 $j$，项 $e_{ij} - m_i \\le 0$，所以最大的移位值是 $0$，其指数恰好是 $1$，即使 $m_i$ 很大也能避免上溢。\n- 极大的负值 $e_{ij}$ 会产生 $e_{ij} - m_i \\ll 0$，其指数非常小；将这些贡献下溢到 $0$ 对总和的影响很小，因为相对于最大项，它们已经可以忽略不计。\n- 整体数学值通过代数恒等式得以保留；该变换不改变 $\\log \\sum_{j} \\exp(e_{ij})$ 的值。\n\n对于稳定化的 softmax，我们使用相同的移位。对于行 $\\mathbf{e}_i$，定义 $m_i = \\max_j e_{ij}$ 并计算\n\n$$\nsoftmax(\\mathbf{e}_i)_j = \\frac{\\exp(e_{ij} - m_i)}{\\sum_{k} \\exp(e_{ik} - m_i)}.\n$$\n\n这在数学上保持了 softmax 值不变，因为公共乘法因子 $\\exp(m_i)$ 在分子和分母中被抵消了，但它通过防止分子和分母中的上溢，极大地提高了数值稳定性。\n\n浮点数考虑：\n- 在电气与电子工程师协会 (IEEE) 754 标准下，fp16 (binary 16) 比 fp32 (binary 32) 具有更小的动态范围和更少的小数位数，而 fp32 又比 fp64 (binary 64) 精度低。较低的精度会增加舍入误差，并增加在未移位的情况下，极小指数发生下溢和极大指数发生上溢的可能性。\n- 通过使用稳定化算法计算 softmax 并在 fp16、fp32 和 fp64 中执行算术运算，我们可以通过测量与高精度 fp64 参考值的最大绝对差来量化精度对最终注意力权重的影响。\n\n算法设计：\n- 对于每个测试矩阵 $E^{(k)}$，使用上述按最大值移位的技术逐行计算一个稳定化的 fp64 参考 softmax。\n- 对于相同的输入，通过在指定的 dtype 中对数组执行相同的操作，在 fp16 和 fp32 中计算稳定化的 softmax。将输出转换为 fp64 以进行比较。\n- 单独地，对于每一行，直接计算朴素的 fp64 值 $\\log\\left( \\sum_{j} \\exp(e_{ij}) \\right)$。当 $\\exp(e_{ij})$ 发生上溢时，这可能产生非有限值（例如 $+\\infty$），特别是对于非常大的正 $e_{ij}$。将此朴素值与稳定化的 fp64 值 $m_i + \\log\\left( \\sum_{j} \\exp(e_{ij} - m_i) \\right)$ 进行比较，并报告各行之间的最大绝对差。如果某行的朴素计算不是有限的，则该行对最大值的贡献为 $nan$，并且如果出现任何此类非有限值，则布尔上溢指示符设置为 true。\n\n测试套件覆盖范围基本原理：\n- 案例 1 使用中等值，代表了朴素计算安全且舍入误差较小的典型情况。\n- 案例 2 包含一行，其中有一个极大的正值 $1000$ 与极大的负值 $-1000$ 以及一个中等范围 $0$ 到 $1$ 混合，这会在朴素的 $\\exp$ 计算中导致上溢，并测试了稳定化的必要性；第二行使用 $88$ 到 $85$ 的值，这些值很大但在 fp64 中是有限的。\n- 案例 3 使用均匀的零，产生完全均匀的 softmax 权重，并测试精度对对称性的影响。\n- 案例 4 使用极大的负值来研究较低精度格式中的下溢行为以及 softmax 对微小指数的敏感性。\n\n输出规范：\n- 对于每个矩阵 $E^{(k)}$，输出一个列表 $[a_k, b_k, c_k, d_k]$，其中 $a_k$ 是 fp16 稳定化 softmax 与 fp64 参考值的最大绝对差，$b_k$ 是 fp32 的相应数量，$c_k$ 是在 fp64 中朴素计算和稳定化计算的 $\\log \\sum_{j} \\exp(e_{ij})$ 在各行上聚合的最大绝对差（非有限的朴素值贡献为 $nan$），$d_k$ 是一个布尔值，指示是否有任何行的朴素计算不是有限的。将四个案例的结果聚合到一个用方括号括起来的逗号分隔的列表中。\n\n这种有原则的设计将 $\\exp$ 和 $\\log$ 的数学恒等式与自注意力 softmax 计算中使用的算法选择联系起来，展示了理论上的稳定性以及经验性的、与精度相关的误差特性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef logsumexp_naive(row: np.ndarray) -> float:\n    \"\"\"Compute naive log(sum(exp(row))) in float64.\"\"\"\n    row64 = row.astype(np.float64)\n    s = np.sum(np.exp(row64))\n    return float(np.log(s))\n\ndef logsumexp_stable(row: np.ndarray) -> float:\n    \"\"\"Compute stable logsumexp in float64: m + log(sum(exp(row - m))).\"\"\"\n    row64 = row.astype(np.float64)\n    m = np.max(row64)\n    shifted = row64 - m\n    s = np.sum(np.exp(shifted))\n    return float(m + np.log(s))\n\ndef softmax_stable(matrix: np.ndarray, dtype: np.dtype) -> np.ndarray:\n    \"\"\"\n    Compute row-wise stabilized softmax in the specified dtype.\n    Returns results as float64 for comparison.\n    \"\"\"\n    x = matrix.astype(dtype)\n    # Row-wise max\n    m = np.max(x, axis=1, keepdims=True)\n    # Shift and exponentiate in dtype; cast back to dtype explicitly\n    shifted = (x - m).astype(dtype)\n    exps = np.exp(shifted).astype(dtype)\n    sums = np.sum(exps, axis=1, keepdims=True).astype(dtype)\n    # Avoid division by zero: if sum is zero (underflow), result remains zero\n    soft = exps / sums\n    return soft.astype(np.float64)\n\ndef max_abs_diff(a: np.ndarray, b: np.ndarray) -> float:\n    \"\"\"Maximum absolute difference between two arrays.\"\"\"\n    return float(np.max(np.abs(a - b)))\n\ndef any_nonfinite(arr: np.ndarray) -> bool:\n    \"\"\"Check if any element is non-finite.\"\"\"\n    return bool(np.any(~np.isfinite(arr)))\n\ndef format_item(item):\n    \"\"\"Format item without spaces, recursively for lists.\"\"\"\n    if isinstance(item, list):\n        return \"[\" + \",\".join(format_item(x) for x in item) + \"]\"\n    elif isinstance(item, float):\n        # Ensure Python's default float string is used (includes 'nan' or 'inf' if present)\n        return str(item)\n    elif isinstance(item, (int, np.integer)):\n        return str(int(item))\n    elif isinstance(item, (bool, np.bool_)):\n        return \"True\" if bool(item) else \"False\"\n    else:\n        return str(item)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([[-1.0, 0.0, 1.0, 2.0],\n                  [0.5, -0.5, 3.0, -3.0]], dtype=np.float64),\n        np.array([[1000.0, -1000.0, 0.0, 1.0],\n                  [88.0, 87.0, 86.0, 85.0]], dtype=np.float64),\n        np.array([[0.0, 0.0, 0.0, 0.0],\n                  [0.0, 0.0, 0.0, 0.0]], dtype=np.float64),\n        np.array([[-1000.0, -1001.0, -999.0, -1200.0],\n                  [-50.0, -60.0, -70.0, -80.0]], dtype=np.float64),\n    ]\n\n    results = []\n    for E in test_cases:\n        # Reference stabilized softmax in float64\n        soft_ref = softmax_stable(E, np.float64)\n\n        # Stabilized softmax in fp16 and fp32\n        soft16 = softmax_stable(E, np.float16)\n        soft32 = softmax_stable(E, np.float32)\n\n        # Error metrics (max absolute difference w.r.t. fp64 reference)\n        err16 = max_abs_diff(soft16, soft_ref)\n        err32 = max_abs_diff(soft32, soft_ref)\n\n        # Naive vs stabilized logsumexp in float64, row-wise\n        naive_vals = []\n        stable_vals = []\n        nonfinite_flag = False\n        diffs = []\n        for i in range(E.shape[0]):\n            row = E[i, :]\n            naive = logsumexp_naive(row)\n            stable = logsumexp_stable(row)\n            naive_vals.append(naive)\n            stable_vals.append(stable)\n            if not np.isfinite(naive):\n                nonfinite_flag = True\n                diffs.append(np.nan)\n            else:\n                diffs.append(abs(naive - stable))\n\n        # Max absolute difference (nan propagated if present)\n        # If any nan present, np.nanmax ignores nan and returns max of finite values.\n        # If all are nan, result is nan.\n        diffs_arr = np.array(diffs, dtype=np.float64)\n        if np.all(np.isnan(diffs_arr)):\n            max_logsumexp_diff = float(np.nan)\n        else:\n            max_logsumexp_diff = float(np.nanmax(diffs_arr))\n\n        results.append([err16, err32, max_logsumexp_diff, nonfinite_flag])\n\n    # Final print statement in the exact required format: nested list without extra spaces.\n    print(format_item(results))\n\nsolve()\n```"
        },
        {
            "introduction": "在确保了数值稳定性之后，下一个关键问题是计算效率。对于长序列，标准注意力计算公式 $(QK^{\\top})V$ 会创建一个巨大的 $n \\times n$ 注意力矩阵，这在计算和内存上都可能非常昂贵。本练习将探讨一种利用矩阵乘法结合律的替代计算顺序 $Q(K^{\\top}V)$，并使用一个简化的性能模型来分析其在特定条件下带来的显著加速，这有助于我们理解现代高效注意力实现（如 FlashAttention）背后的核心思想。",
            "id": "3192615",
            "problem": "在一个单头缩放点积自注意力模块中，任何非线性变换之前的核心线性代数链可以写成三个维度兼容的矩阵的乘积。考虑一个批处理、多头的设置，其中查询（query）、键（key）和值（value）存储为形状为 $B \\times H \\times n \\times d_h$ 的张量，其中 $B$ 是批量大小，$H$ 是头数，$n$ 是序列长度，$d_h$ 是每个头的嵌入维度。对于每个批次-头对，定义矩阵 $Q, K, V \\in \\mathbb{R}^{n \\times d_h}$。忽略任何缩放因子和非线性变换，仅关注三个矩阵的结合律乘积。\n\n你的任务是分析批处理乘积的两种数学上等价的求值顺序：\n- 基准顺序：$\\left(QK^{\\top}\\right)V$。\n- 重排序（结合律）顺序：$Q\\left(K^{\\top}V\\right)$。\n\n假设以下基本事实：\n- 形状为 $(m \\times k)$ 和 $(k \\times n)$ 的通用矩阵-矩阵乘法 (GEMM) 执行 $2mkn$ 次浮点运算。\n- 对于每个 GEMM，每个输入元素被精确读取一次，每个输出元素被精确写入一次。由一个 GEMM 生成并由另一个 GEMM 消耗的任何中间结果都必须精确地写入主存一次，然后再从主存读取一次。不假设任何进一步的缓存或融合。\n- 计算的 roofline 执行时间是计算时间和内存时间中的较大者，其中计算时间等于总浮点运算次数除以峰值浮点速率，内存时间等于总移动字节数除以持续内存带宽。\n\n考虑元素大小为 $s = 2$ 字节的半精度浮点 (fp16) 张量。设硬件为图形处理单元 (GPU)，其峰值 fp16 吞吐量为 $P = 1.0 \\times 10^{14}$ 次浮点运算/秒，持续内存带宽为 $W = 1.0 \\times 10^{12}$ 字节/秒。取 $B = 2$，$H = 8$，$n = 4096$，$d_h = 64$。\n\n仅使用上述原则和标准矩阵维度，对所有 $B \\times H$ 个头执行以下操作：\n- 推导每种顺序的总浮点运算次数。\n- 推导每种顺序的总内存流量（以字节为单位）。\n- 计算每种顺序的 roofline 预测执行时间，即其计算受限时间和内存受限时间的最大值。\n- 最后，计算加速因子 $S$，定义为基准时间除以重排序时间，结果为一个纯数。\n\n将您的最终答案 $S$ 四舍五入到四位有效数字。将最终答案表示为无单位的纯数。",
            "solution": "问题陈述已经过验证，被认为是具有科学依据、良构且客观的。它使用清晰定义的参数和明确的计算与内存访问模型，对自注意力机制的两种计算策略进行了标准的 roofline 模型分析。该问题是有效的，并将推导出解决方案。\n\n核心任务是比较批处理、多头自注意力计算的两种求值顺序的性能，具体是 `$(QK^{\\top})V$`（基准）和 `$Q(K^{\\top}V)$`（重排序）。我们将使用提供的 roofline 模型，该模型将执行时间定义为计算时间和内存时间的最大值。\n\n首先，我们用符号定义给定的参数：\n- 批量大小：`$B = 2$`\n- 头数：`$H = 8$`\n- 序列长度：`$n = 4096$`\n- 每个头的嵌入维度：`$d_h = 64$`\n- fp16 的元素大小：`$s = 2$` 字节\n- 峰值 fp16 吞吐量：`$P = 1.0 \\times 10^{14}$` FLOPS\n- 持续内存带宽：`$W = 1.0 \\times 10^{12}$` 字节/秒\n\n独立注意力计算的总数是 `$N_{ops} = B \\times H = 2 \\times 8 = 16$`。\n对于每次计算，矩阵 `$Q, K, V$` 的维度为 `$\\mathbb{R}^{n \\times d_h}$`。\n一个形状为 `$(m \\times k)$` 的矩阵与一个形状为 `$(k \\times p)$` 的矩阵进行通用矩阵-矩阵乘法 (GEMM)，需要 `$2mkp$` 次浮点运算 (FLOPs)。此类操作的总内存流量，计及读取两个输入矩阵和写入一个输出矩阵，为 `$s(mk + kp + mp)$` 字节。\n\n### 基准顺序分析：`$(QK^{\\top})V$`\n\n此求值过程在所有 `$N_{ops}$` 个头上分两步进行。\n\n**第 1 步：计算注意力矩阵 `$A = QK^{\\top}$`**\n对于每个头，这是 `$Q \\in \\mathbb{R}^{n \\times d_h}$` 和 `$K^{\\top} \\in \\mathbb{R}^{d_h \\times n}$` 的乘积。结果矩阵 `$A$` 的形状为 `$(n \\times n)`。\n- 每个头的 FLOPs：当 `$m=n, k=d_h, p=n$` 时，FLOPs 为 `$2 \\times n \\times d_h \\times n = 2n^2d_h$`。\n- 每个头的内存流量：流量为 `$s(nd_h + d_h n + n^2) = s(2nd_h + n^2)$` 字节。\n\n**第 2 步：计算输出 `$C = AV$`**\n对于每个头，这是 `$A \\in \\mathbb{R}^{n \\times n}$` 和 `$V \\in \\mathbb{R}^{n \\times d_h}$` 的乘积。结果矩阵 `$C$` 的形状为 `$(n \\times d_h)`。\n- 每个头的 FLOPs：当 `$m=n, k=n, p=d_h$` 时，FLOPs 为 `$2 \\times n \\times n \\times d_h = 2n^2d_h$`。\n- 每个头的内存流量：流量为 `$s(n^2 + nd_h + nd_h) = s(n^2 + 2nd_h)$` 字节。\n\n**基准顺序的总计**\n总 FLOPs 和内存流量是这两个步骤的总和，再乘以头数 `$N_{ops}$`。\n- 总 FLOPs, `$F_{base}`$：\n$$F_{base} = N_{ops} (2n^2d_h + 2n^2d_h) = 4BHn^2d_h$$\n$$F_{base} = 4 \\times 2 \\times 8 \\times (4096)^2 \\times 64 = 64 \\times (2^{12})^2 \\times 2^6 = 2^6 \\times 2^{24} \\times 2^6 = 2^{36} \\approx 6.872 \\times 10^{10} \\text{ FLOPs}$$\n- 总内存流量, `$M_{base}`$：\n$$M_{base} = N_{ops} [s(2nd_h + n^2) + s(n^2 + 2nd_h)] = BHs(4nd_h + 2n^2)$$\n$$M_{base} = 16 \\times 2 \\times [4(4096)(64) + 2(4096)^2] = 32 [4 \\cdot 2^{12} \\cdot 2^6 + 2 \\cdot (2^{12})^2]$$\n$$M_{base} = 2^5 [2^2 \\cdot 2^{18} + 2 \\cdot 2^{24}] = 2^5 [2^{20} + 2^{25}] = 2^{25}(1+2^5) = 33 \\times 2^{25} \\approx 1.107 \\times 10^9 \\text{ bytes}$$\n\n**基准 Roofline 时间, `$T_{base}$`**\n- 计算时间：`$T_{compute, base} = F_{base} / P = (2^{36}) / (10^{14}) \\approx 6.872 \\times 10^{-4}$` 秒。\n- 内存时间：`$T_{memory, base} = M_{base} / W = (33 \\times 2^{25}) / (10^{12}) \\approx 1.107 \\times 10^{-3}$` 秒。\n- 执行时间：`$T_{base} = \\max(T_{compute, base}, T_{memory, base}) = T_{memory, base} \\approx 1.107 \\times 10^{-3}$` 秒。\n\n### 重排序顺序分析：`$Q(K^{\\top}V)$`\n\n此求值过程同样在所有 `$N_{ops}$` 个头上分两步进行。\n\n**第 1 步：计算上下文矩阵 `$B = K^{\\top}V$`**\n对于每个头，这是 `$K^{\\top} \\in \\mathbb{R}^{d_h \\times n}$` 和 `$V \\in \\mathbb{R}^{n \\times d_h}$` 的乘积。结果矩阵 `$B$` 的形状为 `$(d_h \\times d_h)`。\n- 每个头的 FLOPs：当 `$m=d_h, k=n, p=d_h$` 时，FLOPs 为 `$2 \\times d_h \\times n \\times d_h = 2nd_h^2$`。\n- 每个头的内存流量：流量为 `$s(d_h n + nd_h + d_h^2) = s(2nd_h + d_h^2)$` 字节。\n\n**第 2 步：计算输出 `$C = QB$`**\n对于每个头，这是 `$Q \\in \\mathbb{R}^{n \\times d_h}$` 和 `$B \\in \\mathbb{R}^{d_h \\times d_h}$` 的乘积。结果矩阵 `$C$` 的形状为 `$(n \\times d_h)`。\n- 每个头的 FLOPs：当 `$m=n, k=d_h, p=d_h$` 时，FLOPs 为 `$2 \\times n \\times d_h \\times d_h = 2nd_h^2$`。\n- 每个头的内存流量：流量为 `$s(nd_h + d_h^2 + nd_h) = s(2nd_h + d_h^2)$` 字节。\n\n**重排序顺序的总计**\n- 总 FLOPs, `$F_{reord}`$：\n$$F_{reord} = N_{ops}(2nd_h^2 + 2nd_h^2) = 4BHnd_h^2$$\n$$F_{reord} = 4 \\times 2 \\times 8 \\times 4096 \\times (64)^2 = 64 \\times 2^{12} \\times (2^6)^2 = 2^6 \\times 2^{12} \\times 2^{12} = 2^{30} \\approx 1.074 \\times 10^9 \\text{ FLOPs}$$\n- 总内存流量, `$M_{reord}`$：\n$$M_{reord} = N_{ops} [s(2nd_h + d_h^2) + s(2nd_h + d_h^2)] = BHs(4nd_h + 2d_h^2)$$\n$$M_{reord} = 16 \\times 2 \\times [4(4096)(64) + 2(64)^2] = 32 [4 \\cdot 2^{12} \\cdot 2^6 + 2 \\cdot (2^6)^2]$$\n$$M_{reord} = 2^5 [2^2 \\cdot 2^{18} + 2 \\cdot 2^{12}] = 2^5 [2^{20} + 2^{13}] = 2^{18}(2^7+1) = 129 \\times 2^{18} \\approx 3.382 \\times 10^7 \\text{ bytes}$$\n\n**重排序 Roofline 时间, `$T_{reord}`**\n- 计算时间：`$T_{compute, reord} = F_{reord} / P = (2^{30}) / (10^{14}) \\approx 1.074 \\times 10^{-5}$` 秒。\n- 内存时间：`$T_{memory, reord} = M_{reord} / W = (129 \\times 2^{18}) / (10^{12}) \\approx 3.382 \\times 10^{-5}$` 秒。\n- 执行时间：`$T_{reord} = \\max(T_{compute, reord}, T_{memory, reord}) = T_{memory, reord} \\approx 3.382 \\times 10^{-5}$` 秒。\n\n### 加速因子 `$S$`\n\n两种求值顺序都是内存受限的，因为两种情况下都有 `$T_{memory} > T_{compute}$`。加速因子 `$S$` 是它们执行时间的比率。\n$$S = \\frac{T_{base}}{T_{reord}} = \\frac{T_{memory, base}}{T_{memory, reord}} = \\frac{M_{base} / W}{M_{reord} / W} = \\frac{M_{base}}{M_{reord}}$$\n代入内存流量的符号表达式：\n$$S = \\frac{BHs(4nd_h + 2n^2)}{BHs(4nd_h + 2d_h^2)} = \\frac{2n(2d_h + n)}{2d_h(2n + d_h)}$$\n使用计算出的值：\n$$S = \\frac{33 \\times 2^{25}}{129 \\times 2^{18}} = \\frac{33}{129} \\times 2^{25-18} = \\frac{3 \\times 11}{3 \\times 43} \\times 2^7 = \\frac{11}{43} \\times 128 = \\frac{1408}{43}$$\n$$S \\approx 32.744186...$$\n四舍五入到四位有效数字，加速因子为 `$32.74$`。",
            "answer": "$$\\boxed{32.74}$$"
        },
        {
            "introduction": "除了实现细节，理解自注意力机制的表达能力也至关重要。本练习将展示如何通过精心设计查询（Query）、键（Key）和值（Value）的投影矩阵，使自注意力机制能够执行特定的算法任务，例如计算序列的奇偶校验。这个过程将揭示注意力与位置编码之间的关键相互作用，阐明模型是如何学习基于位置的复杂关系的。",
            "id": "3192596",
            "problem": "要求您设计并实现一个完整的、可运行的程序。该程序构建一个玩具数据集，其中一个单独的缩放点积自注意力头使用正弦位置编码在不同位置上交替其焦点，然后针对一个奇偶校验任务，定量分析由此产生的注意力模式。\n\n您必须使用的基本依据是：\n- 单头缩放点积自注意力机制的定义。对于一个长度为 T 的序列，给定查询 $Q \\in \\mathbb{R}^{T \\times d_k}$、键 $K \\in \\mathbb{R}^{T \\times d_k}$ 和值 $V \\in \\mathbb{R}^{T \\times d_v}$，在查询索引 $t$ 处的注意力输出为\n$$\n\\mathrm{Attn}(t) = \\sum_{s=0}^{T-1} \\alpha_{t,s} V_s,\n\\quad\n\\alpha_{t,s} = \\frac{\\exp\\left(z_{t,s}\\right)}{\\sum_{u=0}^{T-1} \\exp\\left(z_{t,u}\\right)},\n\\quad\nz_{t,s} = \\frac{\\langle Q_t, K_s \\rangle}{\\sqrt{d_k}},\n$$\n其中 $\\langle \\cdot , \\cdot \\rangle$ 表示标准欧几里得点积。\n\n- 由一个以弧度为单位的角度 $\\theta_p$ 参数化的正弦位置编码。对于位置索引 $p \\in \\{0,1,\\dots,T-1\\}$，您必须使用 $\\theta_p = \\pi p$。位置编码是二维向量 $[\\sin(\\theta_p), \\cos(\\theta_p)]$。\n\n- 对于所有实数角 a 和 b 的三角恒等式：\n$$\n\\cos(a-b) = \\cos(a)\\cos(b) + \\sin(a)\\sin(b).\n$$\n\n您的构建必须满足以下规范：\n\n- 输入是二进制序列 $x \\in \\{0,1\\}^T$。对于每个位置 $p$，通过拼接词元值及其位置编码来定义一个嵌入向量 $e_p \\in \\mathbb{R}^3$：\n$$\ne_p = [x_p, \\sin(\\theta_p), \\cos(\\theta_p)].\n$$\n所有的正弦和余弦都应以弧度计算。\n\n- 对于一个键-查询维度 $d_k = 2$ 和值维度 $d_v = 1$ 的单个注意力头，您必须构建线性投影 $W_Q \\in \\mathbb{R}^{3 \\times d_k}$、$W_K \\in \\mathbb{R}^{3 \\times d_k}$ 和 $W_V \\in \\mathbb{R}^{3 \\times 1}$，使得：\n  - 查询 $Q_t \\in \\mathbb{R}^{2}$ 和键 $K_s \\in \\mathbb{R}^{2}$ 仅依赖于 $e_t$ 和 $e_s$ 的位置分量。\n  - 值 $V_s \\in \\mathbb{R}$ 仅依赖于 $e_s$ 的词元分量 $x_s$。\n  - 因此，未归一化的注意力分数 $z_{t,s}$ 仅依赖于 $t$ 和 $s$ 之间的位置关系。\n\n- 引入一个非负标量温度 $\\beta \\in \\mathbb{R}_{\\ge 0}$，在 softmax 操作之前乘以分数 $z_{t,s}$，即使用分数 $z_{t,s}^{(\\beta)} = \\beta \\cdot \\langle Q_t, K_s \\rangle$ 代替 $z_{t,s}$，并且不要在其他地方更改 $d_k$。这将点积大小的影响与 softmax 的锐度分离开来。\n\n- 将您的注意力分析集中在查询位置 $t=0$。当 $\\theta_p = \\pi p$ 时，偶数索引和奇数索引的位置在 $\\cos(\\theta_p)$ 中具有相反的相位，这在 $z_{t,s}^{(\\beta)}$ 依赖于 $\\theta_t - \\theta_s$ 时，会导致跨位置的交替注意力模式。通过保真度度量来量化这种交替：\n$$\nF = \\sum_{\\substack{s \\in \\{0,\\dots,T-1\\}\\\\ s \\text{ 是偶数}}} \\alpha_{0,s} \\;-\\; \\sum_{\\substack{s \\in \\{0,\\dots,T-1\\}\\\\ s \\text{ 是奇数}}} \\alpha_{0,s}.\n$$\n\n- 将每个序列 $x$ 的目标标签定义为奇偶性：\n$$\ny(x) = \\left(\\sum_{p=0}^{T-1} x_p\\right) \\bmod 2.\n$$\n\n- 仅使用上面描述的单个注意力头和查询 $t=0$ 的注意力权重，按如下方式恢复精确的奇偶性。因为您的设计必须使 $\\alpha_{0,s}$ 在两个奇偶性分组 $\\{s \\,:\\, s \\text{ 是偶数}\\}$ 和 $\\{s \\,:\\, s \\text{ 是奇数}\\}$ 内为常数，所以您可以计算词元值的组内加权平均值，然后按组的大小重新缩放以获得精确的组内总和：\n  - 令 $E = \\{s \\in \\{0,\\dots,T-1\\} \\,:\\, s \\text{ 是偶数}\\}$ 且 $O = \\{s \\in \\{0,\\dots,T-1\\} \\,:\\, s \\text{ 是奇数}\\}$。定义\n  $$\n  \\mu_E(x) = \\frac{\\sum_{s \\in E} \\alpha_{0,s} \\, x_s}{\\sum_{s \\in E} \\alpha_{0,s}}, \\quad\n  \\mu_O(x) = \\frac{\\sum_{s \\in O} \\alpha_{0,s} \\, x_s}{\\sum_{s \\in O} \\alpha_{0,s}},\n  $$\n  约定如果分母因为对应集合为空而为零，则对应的平均值定义为 $0$，并且组的大小视为 $0$。令 $k_E = |E|$ 和 $k_O = |O|$。构成预测的总和\n  $$\n  \\widehat{S}(x) = k_E \\cdot \\mu_E(x) + k_O \\cdot \\mu_O(x),\n  $$\n  和预测的奇偶性\n  $$\n  \\widehat{y}(x) = \\lfloor \\widehat{S}(x) + \\tfrac{1}{2} \\rfloor \\bmod 2.\n  $$\n  这个过程必须以数值方式实现；除了计算准确率外，在推断过程中不得使用真实标签。\n\n- 对于下面的每个测试用例，在长度为 T 的所有二进制序列的穷尽数据集上，评估交替保真度 $F$ 和分类准确率：\n$$\n\\mathrm{Acc} = \\frac{1}{2^T} \\sum_{x \\in \\{0,1\\}^T} \\mathbf{1}\\{\\widehat{y}(x) = y(x)\\},\n$$\n\n角度单位要求：所有三角函数必须使用弧度。\n\n测试套件和程序输出：\n\n- 使用以下测试用例，每个用例指定为一对 $(T,\\beta)$：\n  - 用例 1：$(T,\\beta) = (8, 2.0)$。\n  - 用例 2：$(T,\\beta) = (1, 3.0)$。\n  - 用例 3：$(T,\\beta) = (7, 0.0)$。\n  - 用例 4：$(T,\\beta) = (10, 5.0)$。\n\n- 对于每个用例，计算两个浮点数：查询 $t=0$ 时的交替保真度 $F$，以及准确率 $\\mathrm{Acc}$（一个在 $[0,1]$ 区间内的小数）。在最终输出中将这两个数字都四舍五入到小数点后6位。\n\n- 您的程序应生成单行输出，其中包含所有按顺序排列的结果，形式为方括号括起来的逗号分隔列表，即 `[F_1,Acc_1,F_2,Acc_2,F_3,Acc_3,F_4,Acc_4]`，每个浮点数都四舍五入到小数点后6位，并且行中任何地方都没有空格。\n\n程序必须是自包含的，不需要任何输入，并且必须仅使用上述定义和指定的测试用例来确定性地计算所需的输出。不允许任何随机化。所有三角计算都必须使用弧度。结果类型必须是指定的浮点数。",
            "solution": "用户提供的问题陈述已被分析和验证，是科学上合理的、适定的且内部一致的。\n\n该问题要求设计一种特定的单头自注意力机制来解决二进制序列上的奇偶性任务。该设计必须利用正弦位置编码来创建一种交替的注意力模式。我们将首先构建所需的投影矩阵，然后推导注意力权重和保真度度量 $F$ 的解析形式，最后分析奇偶性预测机制的准确性。\n\n### 步骤 1：构建投影矩阵\n\n在位置 $p$ 处的词元 $x_p \\in \\{0, 1\\}$ 的输入嵌入由 $e_p = [x_p, \\sin(\\theta_p), \\cos(\\theta_p)] \\in \\mathbb{R}^3$ 给出，其中位置角为 $\\theta_p = \\pi p$ 弧度。我们需要构建权重矩阵 $W_Q \\in \\mathbb{R}^{3 \\times 2}$、$W_K \\in \\mathbb{R}^{3 \\times 2}$ 和 $W_V \\in \\mathbb{R}^{3 \\times 1}$（因为 $d_k=2, d_v=1$），并受限于几个约束。\n\n1.  查询 $Q_t = e_t W_Q$ 和键 $K_s = e_s W_K$ 必须仅依赖于嵌入的位置分量。这意味着 $W_Q$ 和 $W_K$ 的第一行必须都为零。\n2.  值 $V_s = e_s W_V$ 必须仅依赖于词元分量 $x_s$。这意味着 $W_V$ 的第二和第三行必须都为零。\n3.  未归一化的注意力分数是 $\\langle Q_t, K_s \\rangle$ 的函数，应依赖于位置差 $\\theta_t - \\theta_s$。提供的三角恒等式 $\\cos(a-b) = \\cos(a)\\cos(b) + \\sin(a)\\sin(b)$ 提供了一条思路。\n\n令嵌入的位置部分为向量 $\\vec{p}_p = [\\sin(\\theta_p), \\cos(\\theta_p)]$。点积 $\\langle\\vec{p}_t, \\vec{p}_s\\rangle$ 恰好是 $\\cos(\\theta_t - \\theta_s)$。如果查询和键就是嵌入的位置部分，我们就可以实现这一点。我们选择作用于位置分量的子矩阵为单位矩阵。一个满足所有约束的权重矩阵的简单选择是：\n$$\nW_Q = \\begin{pmatrix} 0  0 \\\\ 1  0 \\\\ 0  1 \\end{pmatrix}, \\quad W_K = \\begin{pmatrix} 0  0 \\\\ 1  0 \\\\ 0  1 \\end{pmatrix}, \\quad W_V = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n使用这些矩阵，我们有：\n-   位置 $t$ 处的查询：$Q_t = e_t W_Q = [\\sin(\\theta_t), \\cos(\\theta_t)]$。\n-   位置 $s$ 处的键：$K_s = e_s W_K = [\\sin(\\theta_s), \\cos(\\theta_s)]$。\n-   位置 $s$ 处的值：$V_s = e_s W_V = [x_s]$。\n\n查询和键的点积为 $\\langle Q_t, K_s \\rangle = \\sin(\\theta_t)\\sin(\\theta_s) + \\cos(\\theta_t)\\cos(\\theta_s) = \\cos(\\theta_t - \\theta_s)$。当 $\\theta_p=\\pi p$ 时，这变为 $\\langle Q_t, K_s \\rangle = \\cos(\\pi(t-s))$。\n\n### 步骤 2：推导注意力权重和保真度\n\n问题指定在 softmax 函数中使用温度缩放的分数 $z_{t,s}^{(\\beta)} = \\beta \\cdot \\langle Q_t, K_s \\rangle$，取代标准的缩放点积公式。因此，未归一化的分数为：\n$$\nz_{t,s}^{(\\beta)} = \\beta \\cos(\\pi(t-s))\n$$\n注意力权重 $\\alpha_{t,s}$ 由应用于这些分数的 softmax 函数给出。我们感兴趣的是位置 $t=0$ 处的查询：\n$$\n\\alpha_{0,s} = \\frac{\\exp\\left(z_{0,s}^{(\\beta)}\\right)}{\\sum_{u=0}^{T-1} \\exp\\left(z_{0,u}^{(\\beta)}\\right)}\n$$\n对于 $t=0$，分数简化为：$z_{0,s}^{(\\beta)} = \\beta \\cos(-\\pi s) = \\beta \\cos(\\pi s)$。$\\cos(\\pi s)$ 的值在 $s$ 为偶数时为 $1$，在 $s$ 为奇数时为 $-1$。\n$$\nz_{0,s}^{(\\beta)} = \\begin{cases} \\beta  \\text{若 } s \\text{ 是偶数} \\\\ -\\beta  \\text{若 } s \\text{ 是奇数} \\end{cases}\n$$\n令 $E = \\{s \\in \\{0, \\dots, T-1\\} : s \\text{ 是偶数}\\}$ 和 $O = \\{s \\in \\{0, \\dots, T-1\\} : s \\text{ 是奇数}\\}$。令它们的大小为 $k_E = |E| = \\lceil T/2 \\rceil$ 和 $k_O = |O| = \\lfloor T/2 \\rfloor$。注意力权重 $\\alpha_{0,s}$ 将取两个值之一：\n-   对于 $s \\in E$: $\\alpha_{0,s} = \\alpha_{\\text{even}} = \\frac{e^{\\beta}}{k_E e^{\\beta} + k_O e^{-\\beta}}$\n-   对于 $s \\in O$: $\\alpha_{0,s} = \\alpha_{\\text{odd}} = \\frac{e^{-\\beta}}{k_E e^{\\beta} + k_O e^{-\\beta}}$\n\n在 $t=0$ 处的交替保真度度量 $F$ 定义为 $F = \\sum_{s \\in E} \\alpha_{0,s} - \\sum_{s \\in O} \\alpha_{0,s}$。代入常数权重：\n$$\nF = k_E \\alpha_{\\text{even}} - k_O \\alpha_{\\text{odd}} = \\frac{k_E e^{\\beta} - k_O e^{-\\beta}}{k_E e^{\\beta} + k_O e^{-\\beta}}\n$$\n这个公式可以直接为每个测试用例 $(T, \\beta)$ 计算。\n\n### 步骤 3：分析奇偶性预测和准确率\n\n问题定义了一个预测序列 $x \\in \\{0,1\\}^T$ 奇偶性的过程。真实奇偶性为 $y(x) = (\\sum_{p=0}^{T-1} x_p) \\pmod 2$。预测机制使用偶数和奇数位置上词元值的加权平均值。\n$$\n\\mu_E(x) = \\frac{\\sum_{s \\in E} \\alpha_{0,s} \\, x_s}{\\sum_{s \\in E} \\alpha_{0,s}}, \\quad \\mu_O(x) = \\frac{\\sum_{s \\in O} \\alpha_{0,s} \\, x_s}{\\sum_{s \\in O} \\alpha_{0,s}}\n$$\n由于 $\\alpha_{0,s}$ 在每个组（偶数/奇数）内是常数，我们可以简化这些表达式。对于 $T>0$，$k_E > 0$ 且 $\\alpha_{\\text{even}} > 0$，所以 $\\mu_E(x)$ 的分母非零。\n$$\n\\mu_E(x) = \\frac{\\alpha_{\\text{even}} \\sum_{s \\in E} x_s}{k_E \\alpha_{\\text{even}}} = \\frac{1}{k_E} \\sum_{s \\in E} x_s\n$$\n类似地，如果 $k_O > 0$:\n$$\n\\mu_O(x) = \\frac{\\alpha_{\\text{odd}} \\sum_{s \\in O} x_s}{k_O \\alpha_{\\text{odd}}} = \\frac{1}{k_O} \\sum_{s \\in O} x_s\n$$\n如果 $k_O = 0$（当 $T=1$ 时发生），问题约定设置 $\\mu_O(x) = 0$。\n\n预测的总和为 $\\widehat{S}(x) = k_E \\mu_E(x) + k_O \\mu_O(x)$。代入均值的表达式：\n-   如果 $k_E > 0$ 且 $k_O > 0$:\n    $$ \\widehat{S}(x) = k_E \\left(\\frac{1}{k_E} \\sum_{s \\in E} x_s\\right) + k_O \\left(\\frac{1}{k_O} \\sum_{s \\in O} x_s\\right) = \\sum_{s \\in E} x_s + \\sum_{s \\in O} x_s = \\sum_{p=0}^{T-1} x_p $$\n-   如果 $k_O = 0$ (即 $T=1$):\n    $$ \\widehat{S}(x) = k_E \\mu_E(x) + 0 \\cdot \\mu_O(x) = 1 \\cdot \\left(\\frac{1}{1} \\sum_{s \\in E} x_s\\right) + 0 = x_0 = \\sum_{p=0}^{0} x_p $$\n在所有 $T>0$ 的情况下，预测的总和 $\\widehat{S}(x)$ 精确等于词元的真实总和 $S(x) = \\sum_{p=0}^{T-1} x_p$。\n\n预测的奇偶性为 $\\widehat{y}(x) = \\lfloor \\widehat{S}(x) + 0.5 \\rfloor \\pmod 2$。由于 $S(x)$ 始终是整数（因为它是二进制值的和），我们有 $\\lfloor S(x) + 0.5 \\rfloor = S(x)$。因此：\n$$\n\\widehat{y}(x) = S(x) \\pmod 2 = y(x)\n$$\n预测的奇偶性与每个序列 $x$ 的真实奇偶性相同。这对于任何序列长度 $T>0$ 和温度 $\\beta$ 都成立。因此，在整个数据集 $\\{0,1\\}^T$ 上的准确率 $\\mathrm{Acc}$ 必须精确为 $1.0$。\n\n问题要求对此过程进行数值实现。由于精确的代数抵消，数值浮点误差是唯一的潜在偏差来源。然而，对于标准的双精度算术，这些误差将是可忽略不计的，并且四舍五入操作 $\\lfloor \\cdot + 0.5 \\rfloor$ 确保了鲁棒性，导致计算出的准确率为 $1.0$。\n\n### 步骤 4：数值评估\n\n我们现在将这些公式应用于指定的测试用例。\n\n-   **用例 1:** $(T, \\beta) = (8, 2.0)$。 $k_E=4$, $k_O=4$。\n    $F = \\frac{4e^2 - 4e^{-2}}{4e^2 + 4e^{-2}} = \\frac{e^2 - e^{-2}}{e^2 + e^{-2}} = \\tanh(2.0) \\approx 0.964028$。 $\\mathrm{Acc} = 1.0$。\n-   **用例 2:** $(T, \\beta) = (1, 3.0)$。 $k_E=1$, $k_O=0$。\n    $F = \\frac{1e^3 - 0}{1e^3 + 0} = 1.0$。 $\\mathrm{Acc} = 1.0$。\n-   **用例 3:** $(T, \\beta) = (7, 0.0)$。 $k_E=4$, $k_O=3$。\n    $F = \\frac{4e^0 - 3e^{0}}{4e^0 + 3e^{0}} = \\frac{4-3}{4+3} = \\frac{1}{7} \\approx 0.142857$。 $\\mathrm{Acc} = 1.0$。\n-   **用例 4:** $(T, \\beta) = (10, 5.0)$。 $k_E=5$, $k_O=5$。\n    $F = \\frac{5e^5 - 5e^{-5}}{5e^5 + 5e^{-5}} = \\frac{e^5 - e^{-5}}{e^5 + e^{-5}} = \\tanh(5.0) \\approx 0.999909$。 $\\mathrm{Acc} = 1.0$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating alternation fidelity and parity prediction accuracy\n    for a series of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement as (T, beta) pairs.\n    test_cases = [\n        (8, 2.0),\n        (1, 3.0),\n        (7, 0.0),\n        (10, 5.0),\n    ]\n\n    results = []\n    \n    for T, beta in test_cases:\n        # Step 1: Calculate alternation fidelity F\n        \n        # Determine the number of even and odd indices in the sequence of length T.\n        # k_E is the size of the set E = {s in {0,...,T-1} : s is even}\n        # k_O is the size of the set O = {s in {0,...,T-1} : s is odd}\n        k_E = int(np.ceil(T / 2.0))\n        k_O = int(np.floor(T / 2.0))\n        \n        # Calculate the numerator and denominator for the fidelity metric F.\n        # F = (k_E * exp(beta) - k_O * exp(-beta)) / (k_E * exp(beta) + k_O * exp(-beta))\n        # This formula is robust. If T=1, k_O=0, denominator is exp(beta) != 0.\n        # If beta=0, denominator is k_E + k_O = T != 0 for T>=1.\n        numerator_F = k_E * np.exp(beta) - k_O * np.exp(-beta)\n        denominator_F = k_E * np.exp(beta) + k_O * np.exp(-beta)\n        \n        fidelity = 0.0\n        if denominator_F != 0:\n            fidelity = numerator_F / denominator_F\n\n        results.append(f\"{fidelity:.6f}\")\n        \n        # Step 2: Calculate classification accuracy Acc\n        \n        # The problem requires a numerical implementation of the accuracy calculation.\n        # We iterate through all 2^T binary sequences.\n        num_sequences = 2**T\n        correct_predictions = 0\n        \n        # Generate indices for even and odd positions\n        even_indices = [i for i in range(T) if i % 2 == 0]\n        odd_indices = [i for i in range(T) if i % 2 != 0]\n\n        for i in range(num_sequences):\n            # Generate the binary sequence x of length T from integer i\n            x = np.array([(i >> p)  1 for p in range(T)])\n            \n            # True parity y(x)\n            true_sum = np.sum(x)\n            true_parity = true_sum % 2\n            \n            # Predicted parity y_hat(x)\n            # The calculation simplifies as shown in the analytical solution.\n            # Here we implement the full procedure as requested.\n            \n            # Calculate means mu_E and mu_O.\n            # alpha_{0,s} are constant within even/odd groups, so they cancel.\n            sum_x_even = np.sum(x[even_indices]) if len(even_indices) > 0 else 0\n            sum_x_odd = np.sum(x[odd_indices]) if len(odd_indices) > 0 else 0\n            \n            mu_E = sum_x_even / k_E if k_E > 0 else 0.0\n            mu_O = sum_x_odd / k_O if k_O > 0 else 0.0\n            \n            # Calculate predicted total sum S_hat(x)\n            pred_sum = k_E * mu_E + k_O * mu_O\n            \n            # Calculate predicted parity y_hat(x)\n            # Use floor(val + 0.5) for rounding to nearest integer.\n            pred_parity = int(np.floor(pred_sum + 0.5)) % 2\n            \n            if pred_parity == true_parity:\n                correct_predictions += 1\n                \n        accuracy = correct_predictions / num_sequences\n        results.append(f\"{accuracy:.6f}\")\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}