{
    "hands_on_practices": [
        {
            "introduction": "A core feature of Vision Transformers (ViTs) is the use of positional embeddings to encode the spatial arrangement of image patches. But what happens when an image at test time has a different resolution than the one used for training? This practice  challenges you to explore methods for adapting these fixed embeddings to new image grids, a critical step for making ViTs flexible. You will compare a simple nearest-neighbor approach with a more principled bilinear interpolation strategy to quantify the resulting \"positional drift\" and understand why robust interpolation is essential.",
            "id": "3199247",
            "problem": "You are given a family of models based on the Vision Transformer (ViT). A Vision Transformer tokenizes an input image into a grid of non-overlapping patches and augments each patch embedding with an absolute positional embedding. Let the pretraining image resolution yield a grid of $G_{0}^{(h)} \\times G_{0}^{(w)}$ patch centers, where $G_{0}^{(h)} \\in \\mathbb{N}$ and $G_{0}^{(w)} \\in \\mathbb{N}$ denote the number of patches along height and width, respectively. At test time, the image may be resized, producing a different grid of $G_{1}^{(h)} \\times G_{1}^{(w)}$ patch centers. Absolute positional embeddings are defined only on the pretraining grid; therefore, at test time, one must map the test-time grid locations to the pretraining grid in order to construct positional embeddings for the new tokens. Two strategies are considered:\n\n- Nearest-neighbor selection (no interpolation): for each test-time token at grid index $(i,j)$ with $i \\in \\{0,\\dots,G_{1}^{(h)}-1\\}$ and $j \\in \\{0,\\dots,G_{1}^{(w)}-1\\}$, choose a single pretraining grid index $(u,v)$ based on rounding a continuous mapping from the test grid to the pretraining grid. This yields a single pretraining center whose absolute embedding is reused directly.\n- Bilinear interpolation with border clamping: for each test-time token at $(i,j)$, compute a continuous coordinate in the pretraining grid and then interpolate bilinearly using the four nearest pretraining centers. When a neighbor index lies outside the valid pretraining index range, clamp it to the nearest valid boundary index.\n\nDefine the normalized center coordinate of a grid index $(i,j)$ in a grid of size $G^{(h)} \\times G^{(w)}$ as $c(i,j;G^{(h)},G^{(w)}) = \\left(\\frac{i+\\frac{1}{2}}{G^{(h)}}, \\frac{j+\\frac{1}{2}}{G^{(w)}}\\right) \\in [0,1]^{2}$. Define the continuous pretraining-grid coordinates corresponding to a test-time index $(i,j)$ as $(\\tilde{u},\\tilde{v}) \\in \\mathbb{R}^{2}$ that satisfy the center-alignment constraint $\\left(\\frac{\\tilde{u}+\\frac{1}{2}}{G_{0}^{(h)}}, \\frac{\\tilde{v}+\\frac{1}{2}}{G_{0}^{(w)}}\\right) = \\left(\\frac{i+\\frac{1}{2}}{G_{1}^{(h)}}, \\frac{j+\\frac{1}{2}}{G_{1}^{(w)}}\\right)$. This mapping preserves the relative center location in $[0,1]^{2}$ when moving between grids.\n\nFor a given strategy, define the token positional drift for a test-time index $(i,j)$ as the Euclidean distance in $[0,1]^{2}$ between the test-time center $c(i,j;G_{1}^{(h)},G_{1}^{(w)})$ and the center implied by the pretraining positions used by that strategy. For nearest-neighbor selection, the implied center is the single selected pretraining center. For bilinear interpolation, the implied center is the bilinear barycenter of the contributing pretraining centers under border clamping. Aggregate the drift over all test-time tokens by the root-mean-square drift, namely the square root of the mean of the squared per-token distances.\n\nStarting from first principles of coordinate normalization and grid resampling, derive a computable procedure for the root-mean-square drift under both strategies. Then, implement it as a program that, for each test case below, computes two floats: the root-mean-square drift for nearest-neighbor selection and for bilinear interpolation with border clamping, in this order.\n\nUse the following test suite covering nominal, upscaling, downscaling, rectangular change, and extreme collapse cases. Each tuple lists $(G_{0}^{(h)}, G_{0}^{(w)}, G_{1}^{(h)}, G_{1}^{(w)})$:\n- Case $1$: $(14, 14, 14, 14)$.\n- Case $2$: $(14, 14, 16, 16)$.\n- Case $3$: $(14, 14, 10, 10)$.\n- Case $4$: $(14, 20, 16, 10)$.\n- Case $5$: $(14, 14, 1, 1)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the numbers are in the order $[\\text{nn}_{1}, \\text{bi}_{1}, \\text{nn}_{2}, \\text{bi}_{2}, \\dots, \\text{nn}_{5}, \\text{bi}_{5}]$, with each value being a floating-point number. No physical units are involved. There are no angles or percentages in this task. The program must not read any input and must run as-is.",
            "solution": "The problem is well-posed, scientifically grounded, and contains sufficient information to derive a unique, verifiable solution. The concepts of grid resampling, coordinate normalization, and interpolation are standard in computer vision and numerical analysis. The problem is a direct application of these principles to a specific scenario in deep learning, namely the adaptation of Vision Transformer positional embeddings. We shall proceed with a formal derivation.\n\n### 1. Coordinate Systems and Mapping\n\nLet a grid be of size $G^{(h)} \\times G^{(w)}$. The normalized center coordinate for a token at integer index $(k, l)$, where $k \\in \\{0, \\dots, G^{(h)}-1\\}$ and $l \\in \\{0, \\dots, G^{(w)}-1\\}$, is defined as:\n$$\nc(k,l; G^{(h)}, G^{(w)}) = \\left( \\frac{k+0.5}{G^{(h)}}, \\frac{l+0.5}{G^{(w)}} \\right)\n$$\nA test-time token at index $(i,j)$ in the $G_{1}^{(h)} \\times G_{1}^{(w)}$ grid has a normalized center:\n$$\nc_{test}(i,j) = c(i,j; G_{1}^{(h)}, G_{1}^{(w)}) = \\left( \\frac{i+0.5}{G_{1}^{(h)}}, \\frac{j+0.5}{G_{1}^{(w)}} \\right)\n$$\nThe problem defines a continuous mapping to pretraining grid coordinates $(\\tilde{u}, \\tilde{v}) \\in \\mathbb{R}^2$ by preserving this normalized center. This gives the constraint:\n$$\n\\left( \\frac{\\tilde{u}+0.5}{G_{0}^{(h)}}, \\frac{\\tilde{v}+0.5}{G_{0}^{(w)}} \\right) = \\left( \\frac{i+0.5}{G_{1}^{(h)}}, \\frac{j+0.5}{G_{1}^{(w)}} \\right)\n$$\nSolving for $\\tilde{u}$ and $\\tilde{v}$ yields the explicit mapping:\n$$\n\\tilde{u}(i) = G_{0}^{(h)} \\left( \\frac{i+0.5}{G_{1}^{(h)}} \\right) - 0.5\n$$\n$$\n\\tilde{v}(j) = G_{0}^{(w)} \\left( \\frac{j+0.5}{G_{1}^{(w)}} \\right) - 0.5\n$$\nThese continuous coordinates form the basis for both resampling strategies.\n\n### 2. Strategy 1: Nearest-Neighbor (NN) Selection\n\nIn this strategy, the continuous coordinates $(\\tilde{u}, \\tilde{v})$ are rounded to the nearest integer to select a single pretraining grid index $(u, v)$:\n$$\nu = \\text{round}(\\tilde{u})\n$$\n$$\nv = \\text{round}(\\tilde{v})\n$$\nThe implied center is the normalized center of this single pretraining token:\n$$\nc_{implied, NN}(i,j) = c(u,v; G_{0}^{(h)}, G_{0}^{(w)}) = \\left( \\frac{u+0.5}{G_{0}^{(h)}}, \\frac{v+0.5}{G_{0}^{(w)}} \\right)\n$$\nThe squared positional drift for the token $(i,j)$ is the squared Euclidean distance between the test center and the implied center:\n$$\nd_{NN}^2(i,j) = || c_{test}(i,j) - c_{implied, NN}(i,j) ||_2^2\n$$\nUsing the center-alignment constraint, we can simplify this expression. The difference in the first coordinate is:\n$$\nc_{test}^{(h)}(i,j) - c_{implied, NN}^{(h)}(i,j) = \\frac{\\tilde{u}+0.5}{G_{0}^{(h)}} - \\frac{u+0.5}{G_{0}^{(h)}} = \\frac{\\tilde{u}-u}{G_{0}^{(h)}}\n$$\nA similar relation holds for the second coordinate. The squared drift is thus:\n$$\nd_{NN}^2(i,j) = \\left(\\frac{\\tilde{u}-u}{G_{0}^{(h)}}\\right)^2 + \\left(\\frac{\\tilde{v}-v}{G_{0}^{(w)}}\\right)^2\n$$\nThe total RMSD is the square root of the mean of these squared drifts over all $G_{1}^{(h)} \\times G_{1}^{(w)}$ test tokens:\n$$\n\\text{RMSD}_{NN} = \\sqrt{ \\frac{1}{G_{1}^{(h)} G_{1}^{(w)}} \\sum_{i=0}^{G_{1}^{(h)}-1} \\sum_{j=0}^{G_{1}^{(w)}-1} d_{NN}^2(i,j) }\n$$\n\n### 3. Strategy 2: Bilinear Interpolation (BI) with Border Clamping\n\nThis strategy uses the $4$ pretraining grid centers surrounding the continuous coordinate $(\\tilde{u}, \\tilde{v})$. Let $u_0 = \\lfloor \\tilde{u} \\rfloor$, $u_1 = u_0+1$, $v_0 = \\lfloor \\tilde{v} \\rfloor$, and $v_1 = v_0+1$. Let the fractional parts be $\\Delta u = \\tilde{u} - u_0$ and $\\Delta v = \\tilde{v} - v_0$.\n\nThe crucial step is border clamping. The pretraining indices must lie in $[0, G_{0}^{(h)}-1]$ and $[0, G_{0}^{(w)}-1]$. The $4$ corner indices are clamped as follows:\n$$\nu'_0 = \\text{clamp}(u_0, 0, G_0^{(h)}-1) \\quad u'_1 = \\text{clamp}(u_1, 0, G_0^{(h)}-1)\n$$\n$$\nv'_0 = \\text{clamp}(v_0, 0, G_0^{(w)}-1) \\quad v'_1 = \\text{clamp}(v_1, 0, G_0^{(w)}-1)\n$$\nThe implied center is the bilinear barycenter of the normalized coordinates of these $4$ clamped pretraining centers. The weights are determined by $\\Delta u$ and $\\Delta v$. The height component of the implied center is:\n$$\nc_{implied, BI}^{(h)}(i,j) = \\frac{(1-\\Delta u)(u'_0+0.5) + \\Delta u(u'_1+0.5)}{G_0^{(h)}}\n$$\nAnd similarly for the width component:\n$$\nc_{implied, BI}^{(w)}(i,j) = \\frac{(1-\\Delta v)(v'_0+0.5) + \\Delta v(v'_1+0.5)}{G_0^{(w)}}\n$$\nIf clamping is not active (i.e., $u'_0=u_0$, $u'_1=u_1$, etc.), the numerator of the height component simplifies to $\\tilde{u}+0.5$. In this case, $c_{implied, BI}^{(h)}(i,j) = \\frac{\\tilde{u}+0.5}{G_0^{(h)}} = c_{test}^{(h)}(i,j)$, resulting in zero drift. Therefore, positional drift under this strategy arises exclusively from the non-linear effects of border clamping. This occurs when upscaling ($G_1 > G_0$), as the range of $(\\tilde{u}, \\tilde{v})$ extends beyond the valid index range $[0, G_0-1]$.\n\nThe squared positional drift for the token $(i,j)$ is:\n$$\nd_{BI}^2(i,j) = || c_{test}(i,j) - c_{implied, BI}(i,j) ||_2^2\n$$\n$$\nd_{BI}^2(i,j) = \\left( c_{test}^{(h)}(i,j) - c_{implied, BI}^{(h)}(i,j) \\right)^2 + \\left( c_{test}^{(w)}(i,j) - c_{implied, BI}^{(w)}(i,j) \\right)^2\n$$\nThe total RMSD is calculated by averaging over all test tokens:\n$$\n\\text{RMSD}_{BI} = \\sqrt{ \\frac{1}{G_{1}^{(h)} G_{1}^{(w)}} \\sum_{i=0}^{G_{1}^{(h)}-1} \\sum_{j=0}^{G_{1}^{(w)}-1} d_{BI}^2(i,j) }\n$$\n\n### 4. Computational Procedure\n\nA vectorized computation using NumPy is efficient. For each test case $(G_{0}^{(h)}, G_{0}^{(w)}, G_{1}^{(h)}, G_{1}^{(w)})$:\n1.  Create $2$D arrays of test indices $i$ and $j$ of shape $(G_{1}^{(h)}, G_{1}^{(w)})$.\n2.  Compute the continuous pretraining coordinates $(\\tilde{u}, \\tilde{v})$ for all test tokens in a vectorized manner.\n3.  For the NN strategy, round $(\\tilde{u}, \\tilde{v})$ to get $(u, v)$, compute the implied centers, calculate the squared distances, and find the square root of their mean.\n4.  For the BI strategy, compute $\\lfloor \\tilde{u} \\rfloor$, $\\lfloor \\tilde{v} \\rfloor$ and the fractional parts $\\Delta u$, $\\Delta v$. Clamp the corner indices, compute the implied barycentric centers, calculate the squared distances, and find the square root of their mean.\n5.  Store the two resulting RMSD values. This process is repeated for all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Vision Transformer positional drift problem for a suite of test cases.\n    \"\"\"\n\n    # Test suite: each tuple is (G0h, G0w, G1h, G1w)\n    test_cases = [\n        (14, 14, 14, 14),\n        (14, 14, 16, 16),\n        (14, 14, 10, 10),\n        (14, 20, 16, 10),\n        (14, 14, 1, 1),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        G0h, G0w, G1h, G1w = case\n\n        if G1h == 0 or G1w == 0:\n            # Handle trivial case with no tokens to avoid division by zero.\n            results.extend([0.0, 0.0])\n            continue\n            \n        # 1. Create grids of test indices\n        # Generate 1D arrays for i and j coordinates\n        i_coords = np.arange(G1h)\n        j_coords = np.arange(G1w)\n        # Create 2D grids for vectorized computation\n        ii, jj = np.meshgrid(i_coords, j_coords, indexing='ij')\n\n        # 2. Coordinate Mapping\n        # Calculate continuous pretraining coordinates (u_tilde, v_tilde)\n        u_tilde = G0h * (ii + 0.5) / G1h - 0.5\n        v_tilde = G0w * (jj + 0.5) / G1w - 0.5\n\n        # Normalized coordinates of the test tokens\n        c_test_h = (ii + 0.5) / G1h\n        c_test_w = (jj + 0.5) / G1w\n\n        # -------- Strategy 1: Nearest-Neighbor (NN) Selection --------\n        \n        # Round to get nearest pretraining indices. np.round rounds .5 to nearest even.\n        u_nn = np.round(u_tilde)\n        v_nn = np.round(v_tilde)\n\n        # Implied centers from the selected pretraining indices\n        c_implied_nn_h = (u_nn + 0.5) / G0h\n        c_implied_nn_w = (v_nn + 0.5) / G0w\n\n        # Squared Euclidean distance for each token\n        dist_sq_nn = (c_test_h - c_implied_nn_h)**2 + (c_test_w - c_implied_nn_w)**2\n\n        # Root-mean-square drift\n        rmsd_nn = np.sqrt(np.mean(dist_sq_nn))\n        results.append(rmsd_nn)\n\n        # -------- Strategy 2: Bilinear Interpolation (BI) with Border Clamping --------\n\n        # Get floor indices for interpolation\n        u0 = np.floor(u_tilde)\n        v0 = np.floor(v_tilde)\n\n        # Get fractional parts for interpolation weights\n        delta_u = u_tilde - u0\n        delta_v = v_tilde - v0\n\n        # Clamp the four corner indices to the valid pretraining index range\n        u0_clamped = np.clip(u0, 0, G0h - 1)\n        u1_clamped = np.clip(u0 + 1, 0, G0h - 1)\n        v0_clamped = np.clip(v0, 0, G0w - 1)\n        v1_clamped = np.clip(v0 + 1, 0, G0w - 1)\n        \n        # Compute the implied center coordinates (barycenter of clamped indices)\n        # This is a linear interpolation between the centers of the clamped indices\n        implied_center_coord_h = (1 - delta_u) * (u0_clamped + 0.5) + delta_u * (u1_clamped + 0.5)\n        implied_center_coord_w = (1 - delta_v) * (v0_clamped + 0.5) + delta_v * (v1_clamped + 0.5)\n\n        # Normalize the implied center for BI\n        c_implied_bi_h = implied_center_coord_h / G0h\n        c_implied_bi_w = implied_center_coord_w / G0w\n\n        # Squared Euclidean distance for each token\n        dist_sq_bi = (c_test_h - c_implied_bi_h)**2 + (c_test_w - c_implied_bi_w)**2\n\n        # Root-mean-square drift\n        rmsd_bi = np.sqrt(np.mean(dist_sq_bi))\n        results.append(rmsd_bi)\n\n    # Format the final output string as a list of floats\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Training large models like ViTs from scratch is immensely resource-intensive, which led to the development of techniques like knowledge distillation. In this method, a smaller \"student\" ViT learns from a larger, pre-trained \"teacher\" model. This exercise  guides you through the fundamental principles of distillation, requiring you to derive the composite loss function that combines fidelity to the ground truth with mimicry of the teacher. You will also practice analyzing the student's learned attention mechanism, providing insight into both efficient training and model interpretability.",
            "id": "3199218",
            "problem": "You are asked to reason from first principles about Data-Efficient Image Transformers (DeiT) distilled from a Convolutional Neural Network (CNN) teacher, and to implement a program that computes a principled distillation loss and analyzes learned attention patterns. The focus is on Vision Transformers (ViT), Kullback–Leibler divergence (KL), Cross-Entropy (CE), and multi-head self-attention. Start from fundamental definitions and well-tested facts in probability and optimization, and do not rely on shortcut formulas beyond those definitions.\n\nThe scenario is as follows. A Vision Transformer (ViT) student produces class logits, a Convolutional Neural Network (CNN) teacher produces class logits, and there is a ground-truth one-hot label. Distillation combines a term that aligns the student’s predictive distribution with the teacher’s predictive distribution, and a term that enforces fidelity to the ground-truth label. Attention patterns are represented as row-stochastic matrices over tokens per head and layer, from which you must compute information-theoretic and alignment metrics that characterize how attention is distributed and how it aligns to a teacher-provided spatial guidance distribution.\n\nYour tasks are:\n- From first principles, derive the composite distillation loss defined by a weighted sum of two objectives built from foundational definitions: the Kullback–Leibler divergence and the Cross-Entropy. Show that this yields a scalar objective of the form $\\mathcal{L} = \\lambda \\, \\mathrm{KL}(p_{\\text{ViT}} \\| p_{\\text{teacher}}) + (1-\\lambda) \\, \\mathcal{L}_{\\text{CE}}$, where $p_{\\text{ViT}}$ and $p_{\\text{teacher}}$ are the predictive distributions obtained by applying the softmax function to the respective logits, $\\lambda \\in [0,1]$, and $\\mathcal{L}_{\\text{CE}}$ is the Cross-Entropy with the ground-truth one-hot label.\n- Implement the following computations:\n  1. Compute the softmax distribution $p_i = \\exp(z_i) / \\sum_j \\exp(z_j)$ for both student and teacher logits.\n  2. Compute the Kullback–Leibler divergence $\\mathrm{KL}(p \\| q) = \\sum_i p_i \\big( \\log p_i - \\log q_i \\big)$ using $p = p_{\\text{ViT}}$ and $q = p_{\\text{teacher}}$.\n  3. Compute the Cross-Entropy $\\mathcal{L}_{\\text{CE}} = - \\sum_i y_i \\log p_i$ using the ground-truth one-hot vector $y$ and $p = p_{\\text{ViT}}$.\n  4. Compute the distillation loss $\\mathcal{L}$ as specified above.\n  5. Attention analysis:\n     - You are given multi-head self-attention matrices $A^{(l,h)} \\in \\mathbb{R}^{N \\times N}$ for each layer $l$ and head $h$, where $N$ is the number of tokens and each row is a probability distribution over target tokens. Extract the class-token attention row (row index $0$) and restrict it to the patch tokens (column indices from $1$ to $N-1$). Renormalize this restricted vector to sum to $1$.\n     - Compute the average Shannon entropy across layers and heads of these restricted class-token attention distributions: $H = - \\sum_{i} a_i \\log a_i$, averaged over all heads and layers.\n     - Compute a cosine similarity alignment between the mean class-token patch attention vector (averaged over layers and heads, then renormalized to sum to $1$) and the teacher-provided patch-level guidance vector $g$ (renormalized to sum to $1$): $\\cos(\\theta) = \\frac{\\langle a, g \\rangle}{\\|a\\|_2 \\, \\|g\\|_2}$.\n\nAngle units are not applicable. There are no physical units. All outputs must be real numbers.\n\nTest Suite:\nProvide the following four test cases. In each case, the number of classes is $3$, the number of tokens is $N=5$ (one class token plus four patch tokens), and attention matrices have $L=2$ layers and $H=2$ heads. For rows other than the class-token row, you may assume uniform distributions for simplicity.\n\n- Test Case $1$ (happy path):\n  - Student logits $z_{\\text{ViT}} = [\\, \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; 2.0, 0.5, -1.0 \\, ]$\n  - Teacher logits $z_{\\text{teacher}} = [\\, \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; 3.0, -0.5, -2.0 \\, ]$\n  - Ground truth one-hot $y = [\\, \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; 1, 0, 0 \\, ]$\n  - Weight $\\lambda = $ $0.5$\n  - Teacher patch guidance $g = [\\, \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; \\; 0.4, 0.4, 0.1, 0.1 \\, ]$\n  - Attention matrices $A^{(l,h)}$ with $L=2$, $H=2$, $N=5$:\n    - Layer $1$, Head $1$, rows:\n      - Row $0$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.1, 0.5, 0.3, 0.05, 0.05 \\, ]$\n      - Row $1$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.2, 0.2, 0.2, 0.2, 0.2 \\, ]$\n      - Row $2$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.2, 0.2, 0.2, 0.2, 0.2 \\, ]$\n      - Row $3$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.2, 0.2, 0.2, 0.2, 0.2 \\, ]$\n      - Row $4$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.2, 0.2, 0.2, 0.2, 0.2 \\, ]$\n    - Layer $1$, Head $2$, rows:\n      - Row $0$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.15, 0.45, 0.25, 0.10, 0.05 \\, ]$\n      - Other rows are the same uniform $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.2, 0.2, 0.2, 0.2, 0.2 \\, ]$\n    - Layer $2$, Head $1$, rows:\n      - Row $0$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.12, 0.55, 0.20, 0.08, 0.05 \\, ]$\n      - Other rows: uniform $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.2, 0.2, 0.2, 0.2, 0.2 \\, ]$\n    - Layer $2$, Head $2$, rows:\n      - Row $0$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.10, 0.50, 0.25, 0.10, 0.05 \\, ]$\n      - Other rows: uniform $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.2, 0.2, 0.2, 0.2, 0.2 \\, ]$\n\n- Test Case $2$ (boundary $\\lambda = 1$ and identical predictions):\n  - Student logits $z_{\\text{ViT}} = [\\, \\; \\; \\; \\; \\; \\; \\; 1.2, -0.1, 0.0 \\, ]$\n  - Teacher logits $z_{\\text{teacher}} = [\\, \\; \\; \\; \\; \\; \\; \\; 1.2, -0.1, 0.0 \\, ]$\n  - Ground truth one-hot $y = [\\, \\; \\; \\; \\; \\; \\; \\; 0, 1, 0 \\, ]$\n  - Weight $\\lambda = $ $1.0$\n  - Teacher patch guidance $g = [\\, \\; \\; \\; \\; \\; \\; \\; 0.7, 0.1, 0.1, 0.1 \\, ]$\n  - Attention matrices $A^{(l,h)}$ with diffuse class-token attention (all class-token rows uniform):\n    - For all $l \\in \\{1,2\\}$ and $h \\in \\{1,2\\}$, Row $0$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.2, 0.2, 0.2, 0.2, 0.2 \\, ]$\n    - Other rows: uniform $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.2, 0.2, 0.2, 0.2, 0.2 \\, ]$\n\n- Test Case $3$ (boundary $\\lambda = 0$ and a uniform student):\n  - Student logits $z_{\\text{ViT}} = [\\, \\; \\; \\; \\; \\; \\; \\; 0.0, 0.0, 0.0 \\, ]$\n  - Teacher logits $z_{\\text{teacher}} = [\\, \\; \\; \\; \\; \\; \\; \\; 0.5, -0.2, 0.1 \\, ]$\n  - Ground truth one-hot $y = [\\, \\; \\; \\; \\; \\; \\; \\; 0, 0, 1 \\, ]$\n  - Weight $\\lambda = $ $0.0$\n  - Teacher patch guidance $g = [\\, \\; \\; \\; \\; \\; \\; \\; 0.2, 0.3, 0.3, 0.2 \\, ]$\n  - Attention matrices $A^{(l,h)}$ with concentrated class-token attention on one patch:\n    - Layer $1$, Head $1$, Row $0$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.05, 0.10, 0.80, 0.03, 0.02 \\, ]$\n    - Layer $1$, Head $2$, Row $0$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.04, 0.12, 0.78, 0.04, 0.02 \\, ]$\n    - Layer $2$, Head $1$, Row $0$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.06, 0.08, 0.80, 0.04, 0.02 \\, ]$\n    - Layer $2$, Head $2$, Row $0$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.05, 0.10, 0.75, 0.06, 0.04 \\, ]$\n    - Other rows: uniform $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.2, 0.2, 0.2, 0.2, 0.2 \\, ]$\n\n- Test Case $4$ (edge case with misaligned spatial guidance):\n  - Student logits $z_{\\text{ViT}} = [\\, \\; \\; \\; \\; \\; \\; \\; 0.3, 1.0, -0.7 \\, ]$\n  - Teacher logits $z_{\\text{teacher}} = [\\, \\; \\; \\; \\; \\; \\; \\; -0.2, 2.5, -1.0 \\, ]$\n  - Ground truth one-hot $y = [\\, \\; \\; \\; \\; \\; \\; \\; 0, 1, 0 \\, ]$\n  - Weight $\\lambda = $ $0.8$\n  - Teacher patch guidance $g = [\\, \\; \\; \\; \\; \\; \\; \\; 0.50, 0.20, 0.20, 0.10 \\, ]$\n  - Attention matrices $A^{(l,h)}$ with class-token attention biased to later patches:\n    - Layer $1$, Head $1$, Row $0$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.05, 0.10, 0.20, 0.30, 0.35 \\, ]$\n    - Layer $1$, Head $2$, Row $0$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.05, 0.15, 0.20, 0.25, 0.35 \\, ]$\n    - Layer $2$, Head $1$, Row $0$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.04, 0.12, 0.18, 0.30, 0.36 \\, ]$\n    - Layer $2$, Head $2$, Row $0$: $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.06, 0.10, 0.22, 0.28, 0.34 \\, ]$\n    - Other rows: uniform $[\\, \\; \\; \\; \\; \\; \\; \\; \\; 0.2, 0.2, 0.2, 0.2, 0.2 \\, ]$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each entry must be a three-element list in the order $[\\, \\mathcal{L}, H, \\cos(\\theta)\\, ]$, where $\\mathcal{L}$ is the distillation loss, $H$ is the average attention entropy over patch tokens for the class token, and $\\cos(\\theta)$ is the cosine similarity alignment to $g$. For example, the output format must look like $[\\,[l_1,h_1,c_1],\\,[l_2,h_2,c_2],\\,[l_3,h_3,c_3],\\,[l_4,h_4,c_4]\\,]$, with numeric values for each test case in order $1$ through $4$.",
            "solution": "The user-provided problem is valid as it is scientifically grounded, well-posed, and objective. It is rooted in established principles of deep learning, particularly knowledge distillation in Vision Transformers (ViT), and uses standard information-theoretic metrics. All necessary data and definitions are provided for a unique and meaningful solution.\n\n### Part 1: First-Principles Derivation of the Distillation Loss\n\nThe objective is to derive the composite loss function $\\mathcal{L} = \\lambda \\, \\mathrm{KL}(p_{\\text{ViT}} \\| p_{\\text{teacher}}) + (1-\\lambda) \\, \\mathcal{L}_{\\text{CE}}$ from fundamental principles. This loss function is designed to train a student model (a ViT) by combining two goals: matching the predictions of a teacher model (a CNN) and correctly classifying the input based on ground-truth labels.\n\n1.  **Probability Distributions from Logits**:\n    A neural network for a $K$-class classification problem typically outputs a vector of raw scores, or logits, denoted by $z \\in \\mathbb{R}^K$. To interpret these scores as a probability distribution over the classes, the softmax function is applied. For a logit vector $z = [z_1, z_2, \\ldots, z_K]$, the probability of the $i$-th class is given by:\n    $$\n    p_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^K \\exp(z_j)}\n    $$\n    Let $p_{\\text{ViT}}$ be the probability distribution produced by the student ViT from its logits $z_{\\text{ViT}}$, and $p_{\\text{teacher}}$ be the distribution from the teacher CNN's logits $z_{\\text{teacher}}$.\n\n2.  **Objective 1: Fidelity to Ground Truth (Cross-Entropy Loss)**:\n    The primary goal of a classification model is to accurately predict the true class. For a given input, the ground-truth label is represented as a one-hot vector $y \\in \\{0, 1\\}^K$, where $y_k=1$ for the true class $k$ and $y_i=0$ for $i \\neq k$. The training objective is to maximize the probability assigned to the true class, $(p_{\\text{ViT}})_k$. This is equivalent to maximizing the log-likelihood $\\log((p_{\\text{ViT}})_k)$, or, more commonly, minimizing the negative log-likelihood $-\\log((p_{\\text{ViT}})_k)$.\n    Generalizing this for the one-hot vector $y$, the loss is:\n    $$\n    \\mathcal{L}_{\\text{CE}} = - \\sum_{i=1}^K y_i \\log((p_{\\text{ViT}})_i)\n    $$\n    This is the definition of the Cross-Entropy loss between the true distribution $y$ and the predicted distribution $p_{\\text{ViT}}$. It measures the dissimilarity between the model's prediction and the \"hard\" ground-truth label.\n\n3.  **Objective 2: Knowledge Distillation (Kullback-Leibler Divergence)**:\n    Knowledge distillation aims to transfer the \"knowledge\" from a larger, pre-trained teacher model to a smaller student model. The teacher's knowledge is encapsulated in its output distribution $p_{\\text{teacher}}$, which provides richer information than a one-hot label by indicating relative probabilities for incorrect classes (e.g., \"a cat is more like a dog than a car\"). The student is trained to mimic this \"soft\" target distribution.\n    A principled measure of the difference between two probability distributions $P$ and $Q$ is the Kullback-Leibler (KL) divergence. The problem specifies minimizing the KL divergence from the student's distribution to the teacher's, which is defined as:\n    $$\n    \\mathrm{KL}(p_{\\text{ViT}} \\| p_{\\text{teacher}}) = \\sum_{i=1}^K (p_{\\text{ViT}})_i \\log\\left(\\frac{(p_{\\text{ViT}})_i}{(p_{\\text{teacher}})_i}\\right) = \\sum_{i=1}^K (p_{\\text{ViT}})_i \\left( \\log((p_{\\text{ViT}})_i) - \\log((p_{\\text{teacher}})_i) \\right)\n    $$\n    Minimizing this term encourages $p_{\\text{ViT}}$ to become similar to $p_{\\text{teacher}}$. The divergence is zero if and only if the distributions are identical.\n\n4.  **Composite Distillation Loss**:\n    The final loss function is a convex combination of the two objectives, balanced by a hyperparameter $\\lambda \\in [0, 1]$. The weight $\\lambda$ controls the trade-off between learning from the teacher's soft targets and learning from the hard ground-truth labels.\n    The composite loss $\\mathcal{L}$ is formulated as:\n    $$\n    \\mathcal{L} = \\lambda \\cdot (\\text{Distillation Objective}) + (1-\\lambda) \\cdot (\\text{Fidelity Objective})\n    $$\n    Substituting the derived expressions for each objective yields the final form:\n    $$\n    \\mathcal{L} = \\lambda \\, \\mathrm{KL}(p_{\\text{ViT}} \\| p_{\\text{teacher}}) + (1-\\lambda) \\, \\mathcal{L}_{\\text{CE}}\n    $$\n    This completes the derivation.\n\n### Part 2: Implementation of Computations\n\nThe implementation will follow the derived principles and the steps outlined in the problem statement.\n\n1.  **Softmax**: Computes $p_i = \\exp(z_i) / \\sum_j \\exp(z_j)$ for given logits $z$. A numerically stable version is used.\n2.  **Kullback-Leibler Divergence**: Computes $\\mathrm{KL}(p \\| q) = \\sum_i p_i (\\log p_i - \\log q_i)$.\n3.  **Cross-Entropy Loss**: Computes $\\mathcal{L}_{\\text{CE}} = - \\sum_i y_i \\log p_i$. Given that $y$ is one-hot, this simplifies to $-\\log p_k$ where $y_k=1$.\n4.  **Distillation Loss**: Combines the above as $\\mathcal{L} = \\lambda \\cdot \\mathrm{KL}(p_{\\text{ViT}} \\| p_{\\text{teacher}}) + (1-\\lambda) \\cdot \\mathcal{L}_{\\text{CE}}$.\n5.  **Attention Analysis**:\n    - **Class-Token Attention**: For each attention matrix $A^{(l,h)}$, the first row (index $0$) is extracted. This row represents the attention from the class token to all other tokens.\n    - **Patch Attention**: This vector is restricted to its components corresponding to patch tokens (indices $1$ to $N-1$).\n    - **Renormalization**: The restricted vector is renormalized to sum to $1$, yielding a probability distribution $a^{(l,h)}$ over the patch tokens.\n    - **Average Entropy**: The Shannon entropy $H(a) = -\\sum_i a_i \\log a_i$ (using the natural logarithm) is calculated for each of the $L \\times H$ renormalized patch attention vectors. The final metric $H$ is the average of these individual entropies.\n    - **Alignment**: The mean patch attention vector $\\bar{a}$ is computed by element-wise averaging of all $a^{(l,h)}$ vectors and then renormalized. The teacher guidance vector $g$ is also renormalized to a probability distribution $g'$. The cosine similarity $\\mathrm{cos}(\\theta) = \\frac{\\langle \\bar{a}, g' \\rangle}{\\|\\bar{a}\\|_2 \\, \\|g'\\|_2}$ is computed to measure the alignment between the student's average spatial attention focus and the teacher's guidance.\n\nThese computations will be performed for each of the four test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the DeiT distillation and attention analysis problem for the given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1 (happy path)\n        {\n            \"z_vit\": np.array([2.0, 0.5, -1.0]),\n            \"z_teacher\": np.array([3.0, -0.5, -2.0]),\n            \"y_true\": np.array([1, 0, 0]),\n            \"lambda_val\": 0.5,\n            \"g_guidance\": np.array([0.4, 0.4, 0.1, 0.1]),\n            \"attention_matrices\": [\n                # Layer 1\n                np.array([ # Head 1\n                    [0.1, 0.5, 0.3, 0.05, 0.05],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2]\n                ]),\n                np.array([ # Head 2\n                    [0.15, 0.45, 0.25, 0.10, 0.05],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2]\n                ]),\n                # Layer 2\n                np.array([ # Head 1\n                    [0.12, 0.55, 0.20, 0.08, 0.05],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2]\n                ]),\n                np.array([ # Head 2\n                    [0.10, 0.50, 0.25, 0.10, 0.05],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2],\n                    [0.2, 0.2, 0.2, 0.2, 0.2]\n                ]),\n            ]\n        },\n        # Test Case 2 (boundary lambda=1 and identical predictions)\n        {\n            \"z_vit\": np.array([1.2, -0.1, 0.0]),\n            \"z_teacher\": np.array([1.2, -0.1, 0.0]),\n            \"y_true\": np.array([0, 1, 0]),\n            \"lambda_val\": 1.0,\n            \"g_guidance\": np.array([0.7, 0.1, 0.1, 0.1]),\n            \"attention_matrices\": [\n                # Uniform class-token attention for all L=2, H=2\n                np.array([[0.2, 0.2, 0.2, 0.2, 0.2]] * 5)\n            ] * 4\n        },\n        # Test Case 3 (boundary lambda=0 and a uniform student)\n        {\n            \"z_vit\": np.array([0.0, 0.0, 0.0]),\n            \"z_teacher\": np.array([0.5, -0.2, 0.1]),\n            \"y_true\": np.array([0, 0, 1]),\n            \"lambda_val\": 0.0,\n            \"g_guidance\": np.array([0.2, 0.3, 0.3, 0.2]),\n            \"attention_matrices\": [\n                # L1H1\n                 np.array([[0.05, 0.10, 0.80, 0.03, 0.02], [0.2]*5, [0.2]*5, [0.2]*5, [0.2]*5]),\n                # L1H2\n                 np.array([[0.04, 0.12, 0.78, 0.04, 0.02], [0.2]*5, [0.2]*5, [0.2]*5, [0.2]*5]),\n                # L2H1\n                 np.array([[0.06, 0.08, 0.80, 0.04, 0.02], [0.2]*5, [0.2]*5, [0.2]*5, [0.2]*5]),\n                # L2H2\n                 np.array([[0.05, 0.10, 0.75, 0.06, 0.04], [0.2]*5, [0.2]*5, [0.2]*5, [0.2]*5]),\n            ]\n        },\n        # Test Case 4 (edge case with misaligned spatial guidance)\n        {\n            \"z_vit\": np.array([0.3, 1.0, -0.7]),\n            \"z_teacher\": np.array([-0.2, 2.5, -1.0]),\n            \"y_true\": np.array([0, 1, 0]),\n            \"lambda_val\": 0.8,\n            \"g_guidance\": np.array([0.50, 0.20, 0.20, 0.10]),\n            \"attention_matrices\": [\n                # L1H1\n                np.array([[0.05, 0.10, 0.20, 0.30, 0.35], [0.2]*5, [0.2]*5, [0.2]*5, [0.2]*5]),\n                # L1H2\n                np.array([[0.05, 0.15, 0.20, 0.25, 0.35], [0.2]*5, [0.2]*5, [0.2]*5, [0.2]*5]),\n                # L2H1\n                np.array([[0.04, 0.12, 0.18, 0.30, 0.36], [0.2]*5, [0.2]*5, [0.2]*5, [0.2]*5]),\n                # L2H2\n                np.array([[0.06, 0.10, 0.22, 0.28, 0.34], [0.2]*5, [0.2]*5, [0.2]*5, [0.2]*5]),\n            ]\n        }\n    ]\n\n    def softmax(z):\n        \"\"\"Computes a numerically stable softmax distribution.\"\"\"\n        e_z = np.exp(z - np.max(z))\n        return e_z / e_z.sum()\n\n    def kl_divergence(p, q):\n        \"\"\"Computes KL-divergence KL(p || q).\"\"\"\n        return np.sum(p * (np.log(p) - np.log(q)))\n\n    def cross_entropy(y_true, p_pred):\n        \"\"\"Computes cross-entropy loss.\"\"\"\n        # Since y_true is one-hot, find the index of the true class\n        true_class_idx = np.argmax(y_true)\n        # Loss is the negative log probability of the true class\n        return -np.log(p_pred[true_class_idx])\n\n    def shannon_entropy(p):\n        \"\"\"Computes Shannon entropy H(p) using natural logarithm.\"\"\"\n        # Filter out zero probabilities to avoid log(0)\n        p_nz = p[p > 0]\n        return -np.sum(p_nz * np.log(p_nz))\n\n    def cosine_similarity(v1, v2):\n        \"\"\"Computes cosine similarity between two vectors.\"\"\"\n        dot_product = np.dot(v1, v2)\n        norm_v1 = np.linalg.norm(v1)\n        norm_v2 = np.linalg.norm(v2)\n        if norm_v1 == 0 or norm_v2 == 0:\n            return 0.0\n        return dot_product / (norm_v1 * norm_v2)\n\n    results = []\n    # Process each test case\n    for case in test_cases:\n        # --- Loss Calculation ---\n        p_vit = softmax(case[\"z_vit\"])\n        p_teacher = softmax(case[\"z_teacher\"])\n        \n        l_kl = kl_divergence(p_vit, p_teacher)\n        l_ce = cross_entropy(case[\"y_true\"], p_vit)\n        \n        distillation_loss = case[\"lambda_val\"] * l_kl + (1 - case[\"lambda_val\"]) * l_ce\n\n        # --- Attention Analysis ---\n        entropies = []\n        renormalized_attentions = []\n        \n        for A in case[\"attention_matrices\"]:\n            # Extract class token attention row (index 0)\n            class_token_attn = A[0, :]\n            # Restrict to patch tokens (indices 1 to N-1)\n            patch_attn_raw = class_token_attn[1:]\n            \n            # Renormalize to sum to 1\n            patch_attn_sum = patch_attn_raw.sum()\n            if patch_attn_sum > 0:\n                renormalized_patch_attn = patch_attn_raw / patch_attn_sum\n            else: # Handle case of zero sum to avoid division by zero\n                # If all patch attentions are 0, create a uniform distribution\n                n_patches = len(patch_attn_raw)\n                renormalized_patch_attn = np.ones(n_patches) / n_patches\n                \n            renormalized_attentions.append(renormalized_patch_attn)\n            entropies.append(shannon_entropy(renormalized_patch_attn))\n\n        # Calculate average entropy\n        avg_entropy = np.mean(entropies)\n        \n        # Calculate mean patch attention vector\n        # This is already a probability distribution as it's a convex combination\n        # of probability distributions. Renormalizing again for formal correctness.\n        mean_patch_attn_unnorm = np.mean(renormalized_attentions, axis=0)\n        mean_patch_attn = mean_patch_attn_unnorm / mean_patch_attn_unnorm.sum()\n        \n        # Renormalize teacher guidance vector\n        g_guidance = case[\"g_guidance\"]\n        g_norm = g_guidance / g_guidance.sum()\n        \n        # Calculate cosine similarity\n        alignment = cosine_similarity(mean_patch_attn, g_norm)\n        \n        results.append([distillation_loss, avg_entropy, alignment])\n        \n    # Final print statement in the exact required format.\n    # The str() representation of a list includes spaces after commas, which matches\n    # the implicit formatting in the problem description's LaTeX examples.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "In practical applications, the power of ViTs is most often unlocked through transfer learning, where a pre-trained model is adapted for a new, specific task. A key decision in this process is determining how much of the model to update: should you retrain all the weights (full fine-tuning) or only train a new classification head (linear probing)? This hands-on practice  investigates the trade-offs between these two strategies. By correlating the performance gain $ \\Delta \\mathrm{acc} $ with a metric for feature quality, you will develop a deeper intuition for when each adaptation strategy is most effective.",
            "id": "3199207",
            "problem": "A Vision Transformer (ViT) is a neural architecture that maps an image to a representation through patch embedding and transformer blocks. Consider two training strategies for downstream classification on a target dataset: full fine-tuning (updating all parameters of the model) and linear probing (training only a linear classifier on top of frozen embeddings). Suppose the same test set is used to evaluate both strategies. The objective is to compute the accuracy gain and relate it to a measure of frozen embedding transferability.\n\nUse the following fundamental definitions as the base:\n1. Accuracy is defined as $ \\mathrm{acc} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}\\{ \\hat{y}_i = y_i \\} $, where $n$ is the total number of test examples, $y_i$ is the ground-truth label, and $ \\hat{y}_i $ is the predicted label by a classifier. The accuracy gain between full fine-tuning and linear probing is defined as $ \\Delta \\mathrm{acc} = \\mathrm{acc}_{\\mathrm{full}} - \\mathrm{acc}_{\\mathrm{probe}} $.\n2. To quantify the transferability of frozen embeddings, define for $K$ classes with class mean embeddings $ \\mu_k \\in \\mathbb{R}^d $ and isotropic within-class covariance matrices $ S_k = s_k^2 I_d $, the transferability score\n$$\nT = \\frac{2}{K(K-1)} \\sum_{1 \\le i  j \\le K} \\frac{\\| \\mu_i - \\mu_j \\|_2}{\\sqrt{\\operatorname{tr}(S_i)} + \\sqrt{\\operatorname{tr}(S_j)}} ,\n$$\nwhere $ \\operatorname{tr}(S_k) = d s_k^2 $.\n3. To relate $ \\Delta \\mathrm{acc} $ to embedding transferability, compute the Spearman rank correlation $ \\rho $ between the list of $ \\Delta \\mathrm{acc} $ values across test cases and the list of inverse transferability scores $ 1/T $ across the same test cases. Spearman correlation is the Pearson correlation between rank-transformed variables, with average ranks for ties:\n$$\n\\rho = \\frac{\\sum_{i=1}^{m} (R_x(i) - \\bar{R_x})(R_y(i) - \\bar{R_y})}{\\sqrt{\\sum_{i=1}^{m} (R_x(i) - \\bar{R_x})^2} \\sqrt{\\sum_{i=1}^{m} (R_y(i) - \\bar{R_y})^2}} ,\n$$\nwhere $m$ is the number of test cases, $R_x(i)$ is the rank of case $i$ by $ \\Delta \\mathrm{acc} $, $R_y(i)$ is the rank of case $i$ by $ 1/T $, and $ \\bar{R_x}, \\bar{R_y} $ are mean ranks.\n\nYour task is to implement a program that, for each test case, computes $ \\Delta \\mathrm{acc} $ and $ T $, and then computes the Spearman rank correlation $ \\rho $ between the list $ [\\Delta \\mathrm{acc}_1, \\dots, \\Delta \\mathrm{acc}_m] $ and the list $ [1/T_1, \\dots, 1/T_m] $. Express all accuracies and correlation values as decimal floats. No physical units or angle units are involved.\n\nTest suite:\nAll cases use $ K = 3 $ classes and embedding dimension $ d = 4 $.\n\n- Case $1$:\n    - $ n_{\\mathrm{test}} = 200 $, $ c_{\\mathrm{full}} = 182 $, $ c_{\\mathrm{probe}} = 176 $.\n    - $ \\mu_1 = [2.0, -1.0, 0.5, 1.5] $, $ \\mu_2 = [-1.5, 2.2, -0.3, 0.7] $, $ \\mu_3 = [0.0, 0.0, 2.5, -1.0] $.\n    - $ s_1 = 0.50 $, $ s_2 = 0.60 $, $ s_3 = 0.55 $.\n\n- Case $2$:\n    - $ n_{\\mathrm{test}} = 200 $, $ c_{\\mathrm{full}} = 158 $, $ c_{\\mathrm{probe}} = 130 $.\n    - $ \\mu_1 = [0.5, 0.6, 0.4, 0.5] $, $ \\mu_2 = [0.6, 0.5, 0.5, 0.6] $, $ \\mu_3 = [0.4, 0.7, 0.5, 0.4] $.\n    - $ s_1 = 1.20 $, $ s_2 = 1.00 $, $ s_3 = 1.10 $.\n\n- Case $3$:\n    - $ n_{\\mathrm{test}} = 100 $, $ c_{\\mathrm{full}} = 98 $, $ c_{\\mathrm{probe}} = 98 $.\n    - $ \\mu_1 = [3.0, 0.0, 0.0, 0.0] $, $ \\mu_2 = [0.0, 3.0, 0.0, 0.0] $, $ \\mu_3 = [0.0, 0.0, 3.0, 0.0] $.\n    - $ s_1 = 0.30 $, $ s_2 = 0.30 $, $ s_3 = 0.30 $.\n\n- Case $4$:\n    - $ n_{\\mathrm{test}} = 50 $, $ c_{\\mathrm{full}} = 41 $, $ c_{\\mathrm{probe}} = 35 $.\n    - $ \\mu_1 = [1.0, -0.5, 0.0, 0.5] $, $ \\mu_2 = [-0.5, 1.0, 0.5, 0.0] $, $ \\mu_3 = [0.5, 0.5, -0.5, 1.0] $.\n    - $ s_1 = 0.90 $, $ s_2 = 0.80 $, $ s_3 = 1.00 $.\n\n- Case $5$:\n    - $ n_{\\mathrm{test}} = 500 $, $ c_{\\mathrm{full}} = 450 $, $ c_{\\mathrm{probe}} = 420 $.\n    - $ \\mu_1 = [1.5, 1.5, -0.5, 0.0] $, $ \\mu_2 = [-1.0, -1.2, 0.5, 0.0] $, $ \\mu_3 = [0.0, 2.0, 1.0, -1.0] $.\n    - $ s_1 = 0.40 $, $ s_2 = 0.45 $, $ s_3 = 0.50 $.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain $6$ floats: the five $ \\Delta \\mathrm{acc} $ values for cases $1$ through $5$ in order, followed by the Spearman rank correlation $ \\rho $ between $ [\\Delta \\mathrm{acc}_1, \\dots, \\Delta \\mathrm{acc}_5] $ and $ [1/T_1, \\dots, 1/T_5] $. Each float must be formatted to exactly six decimal places, for example $ [0.030000,0.140000,0.000000,0.120000,0.060000,1.000000] $.",
            "solution": "The objective is to compute the accuracy gain, $ \\Delta \\mathrm{acc} $, and a transferability score, $ T $, for five distinct test cases involving a Vision Transformer (ViT). Subsequently, we must determine the Spearman rank correlation, $ \\rho $, between the series of accuracy gains and the series of inverse transferability scores, $ 1/T $.\n\nThe problem provides a comprehensive set of definitions and data. All test cases share a common setup with $ K=3 $ classes and an embedding dimension of $ d=4 $. The overall analysis comprises three main stages:\n1.  For each of the $ m=5 $ test cases, compute the accuracy gain $ \\Delta \\mathrm{acc} $.\n2.  For each test case, compute the transferability score $ T $.\n3.  Compute the Spearman rank correlation $ \\rho $ between the list of $ \\Delta \\mathrm{acc} $ values and the list of $ 1/T $ values.\n\nLet's detail the calculation for each stage.\n\n**1. Accuracy Gain ( $ \\Delta \\mathrm{acc} $ ) Calculation**\n\nThe accuracy is given by $ \\mathrm{acc} = c/n $, where $ c $ is the number of correctly classified examples and $ n $ is the total number of test examples. The accuracy gain is defined as:\n$$\n\\Delta \\mathrm{acc} = \\mathrm{acc}_{\\mathrm{full}} - \\mathrm{acc}_{\\mathrm{probe}} = \\frac{c_{\\mathrm{full}}}{n_{\\mathrm{test}}} - \\frac{c_{\\mathrm{probe}}}{n_{\\mathrm{test}}}\n$$\n\n**2. Transferability Score ( $ T $ ) Calculation**\n\nThe transferability score $ T $ is defined as:\n$$\nT = \\frac{2}{K(K-1)} \\sum_{1 \\le i  j \\le K} \\frac{\\| \\mu_i - \\mu_j \\|_2}{\\sqrt{\\operatorname{tr}(S_i)} + \\sqrt{\\operatorname{tr}(S_j)}}\n$$\nGiven that the covariance matrices are isotropic, $ S_k = s_k^2 I_d $, their trace is $ \\operatorname{tr}(S_k) = \\operatorname{tr}(s_k^2 I_d) = d s_k^2 $. The denominator term becomes $ \\sqrt{d s_i^2} + \\sqrt{d s_j^2} = (s_i + s_j)\\sqrt{d} $.\nFor the given parameters $ K=3 $ and $ d=4 $, the normalization constant is $ \\frac{2}{K(K-1)\\sqrt{d}} = \\frac{2}{3(2)\\sqrt{4}} = \\frac{2}{12} = \\frac{1}{6} $.\nThe summation runs over the class pairs $ (1, 2) $, $ (1, 3) $, and $ (2, 3) $. The specific formula for our cases is:\n$$\nT = \\frac{1}{6} \\left( \\frac{\\|\\mu_1 - \\mu_2\\|_2}{s_1 + s_2} + \\frac{\\|\\mu_1 - \\mu_3\\|_2}{s_1 + s_3} + \\frac{\\|\\mu_2 - \\mu_3\\|_2}{s_2 + s_3} \\right)\n$$\nwhere $ \\| \\cdot \\|_2 $ denotes the Euclidean norm.\n\n**Case-by-Case Calculations**\n\n**Case 1:**\n-   $ n_{\\mathrm{test}} = 200 $, $ c_{\\mathrm{full}} = 182 $, $ c_{\\mathrm{probe}} = 176 $.\n-   $ \\Delta \\mathrm{acc}_1 = \\frac{182}{200} - \\frac{176}{200} = 0.91 - 0.88 = 0.03 $.\n-   $ s_1 = 0.50, s_2 = 0.60, s_3 = 0.55 $.\n-   $ \\mu_1 - \\mu_2 = [3.5, -3.2, 0.8, 0.8] $, $ \\| \\mu_1 - \\mu_2 \\|_2 = \\sqrt{23.77} \\approx 4.8754 $.\n-   $ \\mu_1 - \\mu_3 = [2.0, -1.0, -2.0, 2.5] $, $ \\| \\mu_1 - \\mu_3 \\|_2 = \\sqrt{15.25} \\approx 3.9051 $.\n-   $ \\mu_2 - \\mu_3 = [-1.5, 2.2, -2.8, 1.7] $, $ \\| \\mu_2 - \\mu_3 \\|_2 = \\sqrt{17.82} \\approx 4.2214 $.\n-   $ T_1 = \\frac{1}{6} \\left(\\frac{4.8754}{0.5+0.6} + \\frac{3.9051}{0.5+0.55} + \\frac{4.2214}{0.6+0.55}\\right) = \\frac{1}{6} (4.4322 + 3.7192 + 3.6708) \\approx 1.9704 $.\n-   $ 1/T_1 \\approx 0.507522 $.\n\n**Case 2:**\n-   $ n_{\\mathrm{test}} = 200 $, $ c_{\\mathrm{full}} = 158 $, $ c_{\\mathrm{probe}} = 130 $.\n-   $ \\Delta \\mathrm{acc}_2 = \\frac{158 - 130}{200} = \\frac{28}{200} = 0.14 $.\n-   $ s_1 = 1.20, s_2 = 1.00, s_3 = 1.10 $.\n-   $ \\mu_1 - \\mu_2 = [-0.1, 0.1, -0.1, -0.1] $, $ \\| \\mu_1 - \\mu_2 \\|_2 = \\sqrt{0.04} = 0.2 $.\n-   $ \\mu_1 - \\mu_3 = [0.1, -0.1, -0.1, 0.1] $, $ \\| \\mu_1 - \\mu_3 \\|_2 = \\sqrt{0.04} = 0.2 $.\n-   $ \\mu_2 - \\mu_3 = [0.2, -0.2, 0.0, 0.2] $, $ \\| \\mu_2 - \\mu_3 \\|_2 = \\sqrt{0.12} \\approx 0.3464 $.\n-   $ T_2 = \\frac{1}{6} \\left(\\frac{0.2}{1.2+1.0} + \\frac{0.2}{1.2+1.1} + \\frac{0.3464}{1.0+1.1}\\right) = \\frac{1}{6} (0.0909 + 0.0870 + 0.1650) \\approx 0.0571 $.\n-   $ 1/T_2 \\approx 17.502035 $.\n\n**Case 3:**\n-   $ n_{\\mathrm{test}} = 100 $, $ c_{\\mathrm{full}} = 98 $, $ c_{\\mathrm{probe}} = 98 $.\n-   $ \\Delta \\mathrm{acc}_3 = \\frac{98 - 98}{100} = 0.0 $.\n-   $ s_1 = 0.30, s_2 = 0.30, s_3 = 0.30 $.\n-   $ \\| \\mu_1 - \\mu_2 \\|_2 = \\sqrt{3^2+(-3)^2} = \\sqrt{18} $, $ \\| \\mu_1 - \\mu_3 \\|_2 = \\sqrt{18} $, $ \\| \\mu_2 - \\mu_3 \\|_2 = \\sqrt{18} $.\n-   $ T_3 = \\frac{1}{6} \\left(\\frac{\\sqrt{18}}{0.3+0.3} + \\frac{\\sqrt{18}}{0.3+0.3} + \\frac{\\sqrt{18}}{0.3+0.3}\\right) = \\frac{1}{2} \\frac{\\sqrt{18}}{0.6} \\approx 3.5355 $.\n-   $ 1/T_3 \\approx 0.282843 $.\n\n**Case 4:**\n-   $ n_{\\mathrm{test}} = 50 $, $ c_{\\mathrm{full}} = 41 $, $ c_{\\mathrm{probe}} = 35 $.\n-   $ \\Delta \\mathrm{acc}_4 = \\frac{41 - 35}{50} = \\frac{6}{50} = 0.12 $.\n-   $ s_1 = 0.90, s_2 = 0.80, s_3 = 1.00 $.\n-   $ \\| \\mu_1 - \\mu_2 \\|_2 = \\sqrt{1.5^2+(-1.5)^2+(-0.5)^2+0.5^2} = \\sqrt{5} \\approx 2.2361 $.\n-   $ \\| \\mu_1 - \\mu_3 \\|_2 = \\sqrt{0.5^2+(-1)^2+0.5^2+(-0.5)^2} = \\sqrt{1.75} \\approx 1.3229 $.\n-   $ \\| \\mu_2 - \\mu_3 \\|_2 = \\sqrt{(-1)^2+0.5^2+1^2+(-1)^2} = \\sqrt{3.25} \\approx 1.8028 $.\n-   $ T_4 = \\frac{1}{6} \\left(\\frac{\\sqrt{5}}{0.9+0.8} + \\frac{\\sqrt{1.75}}{0.9+1.0} + \\frac{\\sqrt{3.25}}{0.8+1.0}\\right) = \\frac{1}{6} (1.3153 + 0.6963 + 1.0015) \\approx 0.5022 $.\n-   $ 1/T_4 \\approx 1.991286 $.\n\n**Case 5:**\n-   $ n_{\\mathrm{test}} = 500 $, $ c_{\\mathrm{full}} = 450 $, $ c_{\\mathrm{probe}} = 420 $.\n-   $ \\Delta \\mathrm{acc}_5 = \\frac{450 - 420}{500} = \\frac{30}{500} = 0.06 $.\n-   $ s_1 = 0.40, s_2 = 0.45, s_3 = 0.50 $.\n-   $ \\| \\mu_1 - \\mu_2 \\|_2 = \\sqrt{2.5^2+2.7^2+(-1)^2} = \\sqrt{14.54} \\approx 3.8131 $.\n-   $ \\| \\mu_1 - \\mu_3 \\|_2 = \\sqrt{1.5^2+(-0.5)^2+(-1.5)^2+1^2} = \\sqrt{5.75} \\approx 2.3979 $.\n-   $ \\| \\mu_2 - \\mu_3 \\|_2 = \\sqrt{(-1)^2+(-3.2)^2+(-0.5)^2+1^2} = \\sqrt{12.49} \\approx 3.5341 $.\n-   $ T_5 = \\frac{1}{6} \\left(\\frac{3.8131}{0.4+0.45} + \\frac{2.3979}{0.4+0.5} + \\frac{3.5341}{0.45+0.5}\\right) = \\frac{1}{6} (4.4860 + 2.6644 + 3.7201) \\approx 1.8118 $.\n-   $ 1/T_5 \\approx 0.551954 $.\n\n**3. Spearman Rank Correlation ( $ \\rho $ ) Calculation**\n\nWe compute the correlation between the list of accuracy gains, $ X = [\\Delta \\mathrm{acc}_i] $, and the list of inverse transferability scores, $ Y = [1/T_i] $.\n$ X = [0.03, 0.14, 0.00, 0.12, 0.06] $\n$ Y = [0.507522, 17.502035, 0.282843, 1.991286, 0.551954] $\n\nFirst, we must rank the data in each list.\n-   Ranking $ X $:\n    -   $ 0.00 $ (case $3$) ranks $1$\n    -   $ 0.03 $ (case $1$) ranks $2$\n    -   $ 0.06 $ (case $5$) ranks $3$\n    -   $ 0.12 $ (case $4$) ranks $4$\n    -   $ 0.14 $ (case $2$) ranks $5$\n    The rank vector for $ X $, ordered by case number ($1$ to $5$), is $ R_x = [2, 5, 1, 4, 3] $.\n\n-   Ranking $ Y $:\n    -   $ 0.282843 $ (case $3$) ranks $1$\n    -   $ 0.507522 $ (case $1$) ranks $2$\n    -   $ 0.551954 $ (case $5$) ranks $3$\n    -   $ 1.991286 $ (case $4$) ranks $4$\n    -   $ 17.502035 $ (case $2$) ranks $5$\n    The rank vector for $ Y $, ordered by case number ($1$ to $5$), is $ R_y = [2, 5, 1, 4, 3] $.\n\nSince there are no ties in either list, the ranks are permutations of $ \\{1, 2, 3, 4, 5\\} $.\nThe mean rank for both lists is $ \\bar{R} = (1+2+3+4+5)/5 = 3 $.\nThe rank vectors are identical: $ R_x = R_y $. This indicates a perfect monotonic relationship. The Spearman correlation $ \\rho $ is the Pearson correlation of these ranks.\n$$\n\\rho = \\frac{\\sum_{i=1}^{m} (R_x(i) - \\bar{R_x})(R_y(i) - \\bar{R_y})}{\\sqrt{\\sum_{i=1}^{m} (R_x(i) - \\bar{R_x})^2} \\sqrt{\\sum_{i=1}^{m} (R_y(i) - \\bar{R_y})^2}}\n$$\nSince $ R_x(i) = R_y(i) $ for all $ i=1, \\dots, 5 $, the numerator becomes $ \\sum_{i=1}^{5} (R_x(i) - \\bar{R_x})^2 $, which is identical to the term inside the square root in the denominator.\n-   Deviations from mean for ranks: $ R - \\bar{R} = [-1, 2, -2, 1, 0] $.\n-   Sum of squared deviations: $ \\sum (R(i) - \\bar{R})^2 = (-1)^2 + 2^2 + (-2)^2 + 1^2 + 0^2 = 1+4+4+1+0 = 10 $.\nTherefore,\n$$\n\\rho = \\frac{10}{\\sqrt{10}\\sqrt{10}} = \\frac{10}{10} = 1.0\n$$\nA correlation of $ 1.0 $ signifies a perfect positive monotonic relationship between $ \\Delta \\mathrm{acc} $ and $ 1/T $. This supports the hypothesis that a larger accuracy gain from full fine-tuning over linear probing is associated with poorer transferability of the frozen embeddings.\n\nFinal results, formatted to six decimal places:\n-   $ \\Delta \\mathrm{acc}_1 = 0.030000 $\n-   $ \\Delta \\mathrm{acc}_2 = 0.140000 $\n-   $ \\Delta \\mathrm{acc}_3 = 0.000000 $\n-   $ \\Delta \\mathrm{acc}_4 = 0.120000 $\n-   $ \\Delta \\mathrm{acc}_5 = 0.060000 $\n-   $ \\rho = 1.000000 $\n\nThe final output is the list $ [0.030000, 0.140000, 0.000000, 0.120000, 0.060000, 1.000000] $.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes accuracy gains and transferability scores for several Vision Transformer\n    training scenarios, then calculates the Spearman rank correlation between them.\n    \"\"\"\n    \n    # Common parameters for all test cases as specified in the problem statement\n    K_classes = 3\n    d_embedding = 4\n\n    test_cases = [\n        {\n            \"n_test\": 200, \"c_full\": 182, \"c_probe\": 176,\n            \"mu\": np.array([\n                [2.0, -1.0, 0.5, 1.5],\n                [-1.5, 2.2, -0.3, 0.7],\n                [0.0, 0.0, 2.5, -1.0]\n            ]),\n            \"s\": np.array([0.50, 0.60, 0.55]),\n        },\n        {\n            \"n_test\": 200, \"c_full\": 158, \"c_probe\": 130,\n            \"mu\": np.array([\n                [0.5, 0.6, 0.4, 0.5],\n                [0.6, 0.5, 0.5, 0.6],\n                [0.4, 0.7, 0.5, 0.4]\n            ]),\n            \"s\": np.array([1.20, 1.00, 1.10]),\n        },\n        {\n            \"n_test\": 100, \"c_full\": 98, \"c_probe\": 98,\n            \"mu\": np.array([\n                [3.0, 0.0, 0.0, 0.0],\n                [0.0, 3.0, 0.0, 0.0],\n                [0.0, 0.0, 3.0, 0.0]\n            ]),\n            \"s\": np.array([0.30, 0.30, 0.30]),\n        },\n        {\n            \"n_test\": 50, \"c_full\": 41, \"c_probe\": 35,\n            \"mu\": np.array([\n                [1.0, -0.5, 0.0, 0.5],\n                [-0.5, 1.0, 0.5, 0.0],\n                [0.5, 0.5, -0.5, 1.0]\n            ]),\n            \"s\": np.array([0.90, 0.80, 1.00]),\n        },\n        {\n            \"n_test\": 500, \"c_full\": 450, \"c_probe\": 420,\n            \"mu\": np.array([\n                [1.5, 1.5, -0.5, 0.0],\n                [-1.0, -1.2, 0.5, 0.0],\n                [0.0, 2.0, 1.0, -1.0]\n            ]),\n            \"s\": np.array([0.40, 0.45, 0.50]),\n        }\n    ]\n\n    delta_accs = []\n    inv_Ts = []\n\n    for case in test_cases:\n        # 1. Compute Accuracy Gain (delta_acc)\n        acc_full = case[\"c_full\"] / case[\"n_test\"]\n        acc_probe = case[\"c_probe\"] / case[\"n_test\"]\n        delta_acc = acc_full - acc_probe\n        delta_accs.append(delta_acc)\n\n        # 2. Compute Transferability Score (T)\n        mu = case[\"mu\"]\n        s = case[\"s\"]\n        \n        # The transferability score formula can be simplified for the given K and d.\n        # Constant factor: 2 / (K*(K-1)*sqrt(d)) = 2 / (3*2*sqrt(4)) = 1/6\n        constant_factor = 1.0 / (K_classes * (K_classes - 1) * np.sqrt(d_embedding) / 2.0)\n        \n        sum_term = 0\n        for i in range(K_classes):\n            for j in range(i + 1, K_classes):\n                norm_diff = np.linalg.norm(mu[i] - mu[j])\n                sum_s = s[i] + s[j]\n                sum_term += norm_diff / sum_s\n        \n        T = constant_factor * sum_term\n        # The problem requires correlation with 1/T\n        inv_Ts.append(1.0 / T)\n\n    # 3. Compute Spearman Rank Correlation (rho)\n    def rank_data(data):\n        \"\"\"\n        Assigns ranks to data, using average rank for ties.\n        \"\"\"\n        indexed_data = sorted([(val, i) for i, val in enumerate(data)])\n        ranks = [0] * len(data)\n        i = 0\n        while i  len(data):\n            j = i\n            # Find group of ties\n            while j  len(data) - 1 and indexed_data[j][0] == indexed_data[j+1][0]:\n                j += 1\n            # Ranks for this group are from i+1 to j+1\n            avg_rank = sum(range(i + 1, j + 2)) / (j - i + 1)\n            # Assign average rank to all tied elements\n            for k in range(i, j + 1):\n                original_index = indexed_data[k][1]\n                ranks[original_index] = avg_rank\n            i = j + 1\n        return np.array(ranks)\n\n    Rx = rank_data(delta_accs)\n    Ry = rank_data(inv_Ts)\n    \n    # Using the formula for Pearson correlation on the ranks\n    # which is the definition of Spearman correlation given in the problem\n    Rx_dev = Rx - np.mean(Rx)\n    Ry_dev = Ry - np.mean(Ry)\n    \n    numerator = np.sum(Rx_dev * Ry_dev)\n    denominator_x = np.sqrt(np.sum(Rx_dev**2))\n    denominator_y = np.sqrt(np.sum(Ry_dev**2))\n    \n    # Handle case where standard deviation is zero (all ranks are the same)\n    if denominator_x == 0 or denominator_y == 0:\n        rho = 0.0  # Or undefined, but 0 is a common convention\n    else:\n        rho = numerator / (denominator_x * denominator_y)\n\n    # Format the results for the final output\n    results_to_print = [f\"{val:.6f}\" for val in delta_accs]\n    results_to_print.append(f\"{rho:.6f}\")\n    \n    print(f\"[{','.join(results_to_print)}]\")\n\nsolve()\n\n```"
        }
    ]
}