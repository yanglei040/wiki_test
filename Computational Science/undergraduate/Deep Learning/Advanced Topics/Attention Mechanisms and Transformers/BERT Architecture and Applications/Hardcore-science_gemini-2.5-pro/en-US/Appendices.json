{
    "hands_on_practices": [
        {
            "introduction": "To truly appreciate the scale and complexity of a model like BERT, we must move beyond high-level diagrams and get our hands dirty with its architectural details. This first practice invites you to do just that by calculating the number of learnable parameters within a single BERT encoder block from first principles. By deriving the formula and applying it to a standard configuration, you will gain a concrete understanding of what contributes to the model's size and how architectural choices, like the number of attention heads, impact its parameter count .",
            "id": "3102535",
            "problem": "Consider a single encoder block in Bidirectional Encoder Representations from Transformers (BERT). The block contains Multi-Head Self-Attention (MHSA) followed by a two-layer position-wise feed-forward network, with residual connections and two Layer Normalization (LN) modules. Assume the following canonical design choices, which are consistent with the original architecture: \n- The hidden size is $d$, the number of attention heads is $h$, and the feed-forward inner dimension is $d_{ff}$. \n- Query, key, and value projections are implemented as learned affine maps from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{d}$, each with a weight matrix in $\\mathbb{R}^{d \\times d}$ and a bias in $\\mathbb{R}^{d}$, and the attention output projection is likewise a learned affine map with weight in $\\mathbb{R}^{d \\times d}$ and bias in $\\mathbb{R}^{d}$. \n- The feed-forward network consists of two learned affine maps: the first from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{d_{ff}}$ and the second from $\\mathbb{R}^{d_{ff}}$ back to $\\mathbb{R}^{d}$, each with a weight matrix and a bias vector. The nonlinearity in between is Gaussian Error Linear Unit (GELU) and has no learnable parameters. \n- Layer Normalization is applied twice per block, each LN having a learned scale $\\gamma \\in \\mathbb{R}^{d}$ and shift $\\beta \\in \\mathbb{R}^{d}$. There are no other learned parameters in the block. \n- Token embeddings and positional encodings are part of the embedding layer outside the block and must not be counted here.\n\nStarting only from the core definition that a learned affine map from $\\mathbb{R}^{m}$ to $\\mathbb{R}^{n}$ has $n \\times m$ weight parameters and $n$ bias parameters, and that Layer Normalization contributes $2 \\times d$ parameters per application (scale and shift), derive a closed-form expression for the total learnable parameter count of this encoder block in terms of $d$, $d_{ff}$, and $h$. Then, propose a variant that increases the number of heads from $h$ to $\\alpha h$ while keeping the total compute of the MHSA projections and attention score multiplications fixed for a given hidden size $d$ and sequence length (you may assume that the per-head dimension is $d/h$ and that keeping $d$ constant fixes the dominant MHSA compute). For this variant with $\\alpha = 2$, give the resulting per-head dimension.\n\nFinally, evaluate your expression numerically for the BERT-Base configuration $d = 768$, $d_{ff} = 3072$, $h = 12$, and $\\alpha = 2$. Provide your final answer as a two-entry row matrix containing:\n- The total parameter count of the encoder block, and\n- The new per-head dimension after increasing the number of heads by the factor $\\alpha$.\n\nNo rounding is required; report exact integers. Express your final answer as a row matrix in the form $\\begin{pmatrix} \\text{param\\_count} & \\text{new\\_head\\_dim} \\end{pmatrix}$.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the established architecture of Transformer models, is well-posed with sufficient information for a unique solution, and is expressed in objective, formal language. The provided data are consistent and realistic, corresponding to the well-known BERT-Base configuration. The problem is a standard, non-trivial exercise in analyzing the parameterization of a deep learning model.\n\nThe solution is approached in three parts: first, deriving a general symbolic expression for the number of learnable parameters in a BERT encoder block; second, analyzing the proposed architectural variant; and third, evaluating the derived expressions numerically for the given configuration.\n\n**Part 1: Derivation of the Total Learnable Parameter Count**\n\nThe total number of learnable parameters, $P_{\\text{total}}$, is the sum of the parameters from its constituent components: the Multi-Head Self-Attention (MHSA) module, the position-wise Feed-Forward Network (FFN), and the two Layer Normalization (LN) modules. We will calculate the parameters for each component based on the provided definitions.\n\nA learned affine map from $\\mathbb{R}^{m}$ to $\\mathbb{R}^{n}$ has a weight matrix of size $n \\times m$ and a bias vector of size $n$, for a total of $n \\times m + n$ parameters.\n\n1.  **Multi-Head Self-Attention (MHSA) Parameters ($P_{\\text{MHSA}}$)**\n    The MHSA mechanism consists of four learned affine projections as defined in the problem statement.\n    *   **Query projection (Q):** An affine map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{d}$.\n        - Weight matrix $W_Q \\in \\mathbb{R}^{d \\times d}$: $d \\times d = d^2$ parameters.\n        - Bias vector $b_Q \\in \\mathbb{R}^{d}$: $d$ parameters.\n        - Total for Q: $d^2 + d$.\n    *   **Key projection (K):** An affine map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{d}$.\n        - Weight matrix $W_K \\in \\mathbb{R}^{d \\times d}$: $d^2$ parameters.\n        - Bias vector $b_K \\in \\mathbb{R}^{d}$: $d$ parameters.\n        - Total for K: $d^2 + d$.\n    *   **Value projection (V):** An affine map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{d}$.\n        - Weight matrix $W_V \\in \\mathbb{R}^{d \\times d}$: $d^2$ parameters.\n        - Bias vector $b_V \\in \\mathbb{R}^{d}$: $d$ parameters.\n        - Total for V: $d^2 + d$.\n    *   **Output projection (O):** An affine map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{d}$.\n        - Weight matrix $W_O \\in \\mathbb{R}^{d \\times d}$: $d^2$ parameters.\n        - Bias vector $b_O \\in \\mathbb{R}^{d}$: $d$ parameters.\n        - Total for O: $d^2 + d$.\n    The total number of parameters in the MHSA module is the sum of these four components. Note that the number of heads, $h$, does not explicitly appear in this formulation, as the projections are defined as single transformations on the entire hidden state of dimension $d$.\n    $$P_{\\text{MHSA}} = (d^2 + d) + (d^2 + d) + (d^2 + d) + (d^2 + d) = 4(d^2 + d) = 4d^2 + 4d$$\n\n2.  **Position-wise Feed-Forward Network (FFN) Parameters ($P_{\\text{FFN}}$)**\n    The FFN consists of two learned affine maps with a GELU non-linearity, which has no parameters.\n    *   **First layer:** An affine map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{d_{ff}}$.\n        - Weight matrix $W_1 \\in \\mathbb{R}^{d_{ff} \\times d}$: $d_{ff} \\times d$ parameters.\n        - Bias vector $b_1 \\in \\mathbb{R}^{d_{ff}}$: $d_{ff}$ parameters.\n        - Total for the first layer: $d \\cdot d_{ff} + d_{ff}$.\n    *   **Second layer:** An affine map from $\\mathbb{R}^{d_{ff}}$ to $\\mathbb{R}^{d}$.\n        - Weight matrix $W_2 \\in \\mathbb{R}^{d \\times d_{ff}}$: $d \\times d_{ff}$ parameters.\n        - Bias vector $b_2 \\in \\mathbb{R}^{d}$: $d$ parameters.\n        - Total for the second layer: $d \\cdot d_{ff} + d$.\n    The total number of parameters in the FFN is the sum of these two layers.\n    $$P_{\\text{FFN}} = (d \\cdot d_{ff} + d_{ff}) + (d \\cdot d_{ff} + d) = 2d \\cdot d_{ff} + d_{ff} + d$$\n\n3.  **Layer Normalization (LN) Parameters ($P_{\\text{LN}}$)**\n    The block contains two Layer Normalization modules. Each module has a learnable scale vector $\\gamma \\in \\mathbb{R}^{d}$ and a learnable shift vector $\\beta \\in \\mathbb{R}^{d}$.\n    - Parameters per LN module: $d + d = 2d$.\n    - Total parameters for two LN modules:\n    $$P_{\\text{LN}} = 2 \\times (2d) = 4d$$\n\n**Total Parameters ($P_{\\text{total}}$)**\nThe total number of learnable parameters in the encoder block is the sum of the parameters from all components.\n$$P_{\\text{total}} = P_{\\text{MHSA}} + P_{\\text{FFN}} + P_{\\text{LN}}$$\n$$P_{\\text{total}} = (4d^2 + 4d) + (2d \\cdot d_{ff} + d_{ff} + d) + (4d)$$\n$$P_{\\text{total}} = 4d^2 + (4d + d + 4d) + 2d \\cdot d_{ff} + d_{ff}$$\n$$P_{\\text{total}} = 4d^2 + 9d + d_{ff}(2d + 1)$$\nThis is the closed-form expression for the total parameter count.\n\n**Part 2: Architectural Variant and New Per-Head Dimension**\n\nThe problem proposes a variant where the number of attention heads is changed from $h$ to $h' = \\alpha h$, while the model's hidden size $d$ is kept constant. In the standard Transformer architecture, the total dimensionality across all heads equals the model's hidden size. That is, if $d_k$ is the dimension of each head's query, key, and value vectors, then $h \\cdot d_k = d$.\n\nThe original per-head dimension is $d_k = \\frac{d}{h}$.\nThe new number of heads is $h' = \\alpha h$.\nTo maintain the relationship $h' \\cdot d'_k = d$, where $d'_k$ is the new per-head dimension, we can solve for $d'_k$:\n$$d'_k = \\frac{d}{h'} = \\frac{d}{\\alpha h} = \\frac{1}{\\alpha} \\left(\\frac{d}{h}\\right)$$\nThe new per-head dimension is the original per-head dimension scaled by $1/\\alpha$.\n\n**Part 3: Numerical Evaluation**\n\nWe are given the BERT-Base configuration: $d = 768$, $d_{ff} = 3072$, $h = 12$, and the modification factor $\\alpha = 2$.\n\n*   **Total Parameter Count:**\n    We substitute the given values into the derived formula for $P_{\\text{total}}$.\n    $$P_{\\text{total}} = 4(768)^2 + 9(768) + 3072(2 \\cdot 768 + 1)$$\n    $$P_{\\text{total}} = 4(589824) + 6912 + 3072(1536 + 1)$$\n    $$P_{\\text{total}} = 2359296 + 6912 + 3072(1537)$$\n    $$P_{\\text{total}} = 2359296 + 6912 + 4721664$$\n    $$P_{\\text{total}} = 7087872$$\n\n*   **New Per-Head Dimension:**\n    The original number of heads is $h=12$. The new number of heads is $h' = \\alpha h = 2 \\times 12 = 24$.\n    The hidden size is $d = 768$. The new per-head dimension $d'_k$ is:\n    $$d'_k = \\frac{d}{h'} = \\frac{768}{24}$$\n    To compute this, we can divide by $12$ then by $2$:\n    $$d'_k = \\frac{768/12}{2} = \\frac{64}{2} = 32$$\n    The new per-head dimension is $32$.\n\nThe final answer requires a row matrix containing the total parameter count and the new per-head dimension.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} 7087872 & 32 \\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Having examined the static structure of a BERT encoder, our next step is to explore its dynamic behavior. The magic of BERT lies in its self-attention mechanism, which allows the model to weigh the importance of different words in a sequence when processing any given word. This exercise provides a hands-on simulation of attention attribution, a technique used to interpret how the model resolves linguistic phenomena like coreference by tracing the connections a pronoun makes to its potential antecedents .",
            "id": "3102501",
            "problem": "You are given a simplified single-layer, multi-head self-attention setting inspired by Bidirectional Encoder Representations from Transformers (BERT). The goal is to compute attention-based attribution from a pronoun token to its candidate antecedents in three coreference examples, and to identify which attention heads specialize in long-range dependencies.\n\nFundamental base:\n- In scaled dot-product self-attention, queries, keys, and values are computed for each token. The attention weight from token $i$ to token $j$ is $a_{ij} = \\mathrm{softmax}\\left(\\frac{q_i \\cdot k_j}{\\sqrt{d_k}}\\right)$, which is row-stochastic due to the softmax, so $\\sum_{j=0}^{N-1} a_{ij} = 1$ for each row $i$.\n- In multi-head attention with $H$ heads, each head $h$ yields a row-stochastic matrix $A^{(h)} \\in \\mathbb{R}^{N \\times N}$. A simple uniform aggregation over heads yields $A = \\frac{1}{H} \\sum_{h=0}^{H-1} A^{(h)}$, which is also row-stochastic.\n- A residual connection can be approximated at the attention attribution level by a convex combination $\\tilde{A} = \\lambda I + (1 - \\lambda) A$, where $I$ is the identity matrix and $\\lambda \\in [0,1]$ controls how much of the token’s original representation is preserved.\n- Attention attribution from a source index $s$ to all tokens is computed as the row vector $e_s^\\top \\tilde{A}$, where $e_s$ is the one-hot row vector with a $1$ at position $s$ and $0$ elsewhere. The attribution score for a candidate antecedent index $k$ is the $k$-th entry of $e_s^\\top \\tilde{A}$.\n\nLong-range specialization definition:\n- Fix a distance threshold $D \\in \\mathbb{N}$ and a specialization threshold $T \\in [0,1]$. For head $h$, the long-range mass at source index $s$ is $r^{(h)}(s) = \\sum_{k \\,:\\, |k-s| \\ge D} A^{(h)}_{s,k}$. Head $h$ is deemed “long-range specialized” at $s$ if $r^{(h)}(s) \\ge T$.\n\nAssumptions and construction of full attention matrices:\n- For each test case, the pronoun row (query at the pronoun index) is explicitly specified for every head as a probability distribution over all token indices. For all non-pronoun rows, assume a simple, scientifically plausible local-context distribution: let $w_{\\text{self}} = 0.7$, and distribute the remaining mass $1 - w_{\\text{self}} = 0.3$ equally to the existing immediate neighbors (left and right), with boundary adjustments as needed so that each non-pronoun row sums to $1$.\n\nIndexing rule:\n- Use zero-based indexing. Tokens are indexed from $0$ to $N-1$.\n- Candidate antecedent indices are provided per test case. If two candidates receive equal maximal attribution (within machine precision), break ties by choosing the smallest index.\n\nGlobal parameters:\n- Number of layers $L = 1$.\n- Number of heads $H = 3$.\n- Residual mixing parameter $\\lambda = 0.2$.\n- Long-range distance threshold $D = 2$.\n- Specialization threshold $T = 0.6$.\n\nTest suite:\n- Test Case 1 (short-range vs medium-range coreference):\n  - Sequence length $N = 7$.\n  - Pronoun index $s = 5$.\n  - Candidate antecedents $\\{1, 3\\}$.\n  - Pronoun row distributions for each head:\n    - Head $0$: $[0.05, 0.30, 0.05, 0.45, 0.05, 0.00, 0.10]$.\n    - Head $1$: $[0.10, 0.20, 0.10, 0.40, 0.10, 0.00, 0.10]$.\n    - Head $2$: $[0.05, 0.20, 0.05, 0.55, 0.05, 0.00, 0.10]$.\n- Test Case 2 (long-range antecedent further back):\n  - Sequence length $N = 12$.\n  - Pronoun index $s = 11$.\n  - Candidate antecedents $\\{6, 9\\}$.\n  - Pronoun row distributions for each head:\n    - Head $0$: $[0.02, 0.02, 0.02, 0.02, 0.02, 0.05, 0.55, 0.05, 0.15, 0.05, 0.03, 0.02]$.\n    - Head $1$: $[0.02, 0.02, 0.02, 0.02, 0.02, 0.05, 0.30, 0.05, 0.10, 0.35, 0.03, 0.02]$.\n    - Head $2$: $[0.02, 0.02, 0.02, 0.02, 0.02, 0.05, 0.45, 0.05, 0.20, 0.10, 0.03, 0.02]$.\n- Test Case 3 (tie between two candidates):\n  - Sequence length $N = 6$.\n  - Pronoun index $s = 5$.\n  - Candidate antecedents $\\{1, 4\\}$.\n  - Pronoun row distributions for each head:\n    - Head $0$: $[0.05, 0.40, 0.05, 0.15, 0.30, 0.05]$.\n    - Head $1$: $[0.05, 0.20, 0.05, 0.30, 0.40, 0.00]$.\n    - Head $2$: $[0.05, 0.30, 0.05, 0.20, 0.20, 0.20]$.\n\nRequired computations for each test case:\n1. Construct each head’s full attention matrix $A^{(h)} \\in \\mathbb{R}^{N \\times N}$ by placing the given pronoun row at index $s$ and filling all other rows with the local-context rule described above. Verify that each row sums to $1$.\n2. Compute the uniform head aggregation $A = \\frac{1}{H} \\sum_{h=0}^{H-1} A^{(h)}$, then the residual-mixed matrix $\\tilde{A} = \\lambda I + (1 - \\lambda) A$.\n3. Compute the attribution vector $u = e_s^\\top \\tilde{A}$ and extract the attribution scores at the candidate indices. Choose the antecedent index $k^\\star$ with maximal attribution; break ties by the smallest index.\n4. For each head $h$, compute the long-range mass $r^{(h)}(s) = \\sum_{k \\,:\\, |k-s| \\ge D} A^{(h)}_{s,k}$. Report the list of head indices that satisfy $r^{(h)}(s) \\ge T$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list with one entry per test case. Each entry must itself be a list of two elements: the chosen antecedent index $k^\\star$ (an integer) and the list of long-range specialized head indices (a list of integers). The entire output must be printed as a single comma-separated list enclosed in square brackets with no spaces, for example, $[\\,[k_1^\\star,[h_{1,1},\\dots]],[k_2^\\star,[\\dots]],\\dots\\,]$.",
            "solution": "The user's request is a valid, well-posed problem that requires a step-by-step application of principles from the self-attention mechanism, as abstracted from the Transformer architecture. I will proceed with a detailed solution.\n\nThe problem asks for an analysis of a simplified single-layer, multi-head self-attention model across three test cases. For each case, we must determine the most likely antecedent for a pronoun from a set of candidates and identify which attention heads are specialized for long-range dependencies.\n\nThe analysis is based on the following global parameters:\n- Number of attention heads: $H = 3$\n- Residual mixing parameter: $\\lambda = 0.2$\n- Long-range distance threshold: $D = 2$\n- Long-range specialization threshold: $T = 0.6$\n- Self-attention weight for non-pronoun rows: $w_{\\text{self}} = 0.7$\n\nThe solution for each test case involves four sequential computational steps.\n\n**Step 1: Construction of Per-Head Attention Matrices ($A^{(h)}$)**\nFor each head $h \\in \\{0, 1, 2\\}$, we are required to construct a full $N \\times N$ attention matrix $A^{(h)}$. The row corresponding to the pronoun at index $s$, denoted $A^{(h)}_{s,:}$, is explicitly provided as a probability distribution. All other rows $i \\neq s$ are populated according to a local-context heuristic: the self-attention weight is $A^{(h)}_{i,i} = w_{\\text{self}} = 0.7$. The residual probability, $1 - w_{\\text{self}} = 0.3$, is distributed equally among its immediate neighbors. This results in the following assignments for $i \\neq s$:\n- For an interior token $i \\in \\{1, \\dots, N-2\\}$: $A^{(h)}_{i, i-1} = 0.15$ and $A^{(h)}_{i, i+1} = 0.15$.\n- For the first token $i=0$: $A^{(h)}_{0, 1} = 0.3$.\n- For the last token $i=N-1$: $A^{(h)}_{N-1, N-2} = 0.3$.\nAll other entries in rows $i \\neq s$ are zero. This construction ensures each matrix $A^{(h)}$ is row-stochastic.\n\n**Step 2: Head Aggregation and Residual Mixing**\nThe individual head matrices are averaged to form a single aggregated attention matrix $A$:\n$$ A = \\frac{1}{H} \\sum_{h=0}^{H-1} A^{(h)} $$\nNext, a residual connection is modeled by taking a convex combination of $A$ and the identity matrix $I$:\n$$ \\tilde{A} = \\lambda I + (1 - \\lambda) A $$\nWith $\\lambda = 0.2$, this becomes $\\tilde{A} = 0.2 I + 0.8 A$.\n\n**Step 3: Antecedent Identification via Attention Attribution**\nThe attribution from the pronoun at source $s$ to all other tokens is given by the $s$-th row of $\\tilde{A}$, which we denote as the vector $u = e_s^\\top \\tilde{A}$. The score for a candidate antecedent at index $k$ is the $k$-th element of this vector, $u_k = (\\tilde{A})_{s,k}$.\nThe antecedent $k^\\star$ is selected from the given set of candidates by finding the one with the maximum attribution score:\n$$ k^\\star = \\arg\\max_{k \\in \\text{Candidates}} u_k $$\nIf a tie occurs, the candidate with the smallest index is chosen. It is important to note that for any candidate $k \\neq s$, the score is $u_k = (1-\\lambda)A_{s,k}$. Since $(1-\\lambda)$ is a positive constant, maximizing $u_k$ is equivalent to maximizing $A_{s,k}$.\n\n**Step 4: Identification of Long-Range Specialized Heads**\nA head $h$ is deemed \"long-range specialized\" at a source index $s$ if the total attention it pays to tokens at or beyond a distance $D$ meets or exceeds the threshold $T$. The long-range mass is calculated as:\n$$ r^{(h)}(s) = \\sum_{k \\,:\\, |k-s| \\ge D} A^{(h)}_{s,k} $$\nThe head is specialized if $r^{(h)}(s) \\ge T$.\n\nA critical observation is that the outcomes of Step 3 (antecedent identification) and Step 4 (specialization) depend solely on the pronoun rows $A^{(h)}_{s,:}$. The construction of non-pronoun rows is thus extraneous to the final required outputs. However, the procedure is followed as specified for formal adherence to the problem statement.\n\n**Calculations and Results for Each Test Case:**\n\n**Test Case 1:**\n- Parameters: $N=7$, $s=5$, Candidates $=\\{1, 3\\}$.\n- **Antecedent Identification**: We compare the aggregated attention scores for candidates $k=1$ and $k=3$.\n  - $A_{5,1} = \\frac{1}{3} (A^{(0)}_{5,1} + A^{(1)}_{5,1} + A^{(2)}_{5,1}) = \\frac{1}{3} (0.30 + 0.20 + 0.20) = \\frac{0.70}{3} \\approx 0.2333$\n  - $A_{5,3} = \\frac{1}{3} (A^{(0)}_{5,3} + A^{(1)}_{5,3} + A^{(2)}_{5,3}) = \\frac{1}{3} (0.45 + 0.40 + 0.55) = \\frac{1.40}{3} \\approx 0.4667$\n  Since $A_{5,3} > A_{5,1}$, the chosen antecedent is $k^\\star = 3$.\n- **Long-Range Specialization**: For $s=5$ and $D=2$, the long-range indices are $k \\in \\{0, 1, 2, 3\\}$.\n  - $r^{(0)}(5) = A^{(0)}_{5,0}+A^{(0)}_{5,1}+A^{(0)}_{5,2}+A^{(0)}_{5,3} = 0.05 + 0.30 + 0.05 + 0.45 = 0.85 \\ge 0.6$. Head $0$ is specialized.\n  - $r^{(1)}(5) = A^{(1)}_{5,0}+A^{(1)}_{5,1}+A^{(1)}_{5,2}+A^{(1)}_{5,3} = 0.10 + 0.20 + 0.10 + 0.40 = 0.80 \\ge 0.6$. Head $1$ is specialized.\n  - $r^{(2)}(5) = A^{(2)}_{5,0}+A^{(2)}_{5,1}+A^{(2)}_{5,2}+A^{(2)}_{5,3} = 0.05 + 0.20 + 0.05 + 0.55 = 0.85 \\ge 0.6$. Head $2$ is specialized.\n- Result: $k^\\star=3$; Specialized heads: $[0, 1, 2]$.\n\n**Test Case 2:**\n- Parameters: $N=12$, $s=11$, Candidates $=\\{6, 9\\}$.\n- **Antecedent Identification**:\n  - $A_{11,6} = \\frac{1}{3}(0.55 + 0.30 + 0.45) = \\frac{1.30}{3} \\approx 0.4333$\n  - $A_{11,9} = \\frac{1}{3}(0.15 + 0.35 + 0.10) = \\frac{0.60}{3} = 0.2000$\n  Since $A_{11,6} > A_{11,9}$, the chosen antecedent is $k^\\star = 6$.\n- **Long-Range Specialization**: For $s=11$ and $D=2$, the long-range indices are $k \\in \\{0, \\dots, 9\\}$. The long-range mass can be computed as $r^{(h)}(11) = 1 - (A^{(h)}_{11,10} + A^{(h)}_{11,11})$.\n  - For all heads $h \\in \\{0,1,2\\}$, we have $A^{(h)}_{11,10}=0.03$ and $A^{(h)}_{11,11}=0.02$.\n  - Thus, for each head, $r^{(h)}(11) = 1 - (0.03 + 0.02) = 0.95 \\ge 0.6$. All three heads are specialized.\n- Result: $k^\\star=6$; Specialized heads: $[0, 1, 2]$.\n\n**Test Case 3:**\n- Parameters: $N=6$, $s=5$, Candidates $=\\{1, 4\\}$.\n- **Antecedent Identification**:\n  - $A_{5,1} = \\frac{1}{3}(0.40 + 0.20 + 0.30) = \\frac{0.90}{3} = 0.3000$\n  - $A_{5,4} = \\frac{1}{3}(0.30 + 0.40 + 0.20) = \\frac{0.90}{3} = 0.3000$\n  The scores are identical. The tie-breaking rule dictates selecting the smallest index, so $k^\\star = 1$.\n- **Long-Range Specialization**: For $s=5$ and $D=2$, the long-range indices are $k \\in \\{0, 1, 2, 3\\}$.\n  - $r^{(0)}(5) = 0.05 + 0.40 + 0.05 + 0.15 = 0.65 \\ge 0.6$. Head $0$ is specialized.\n  - $r^{(1)}(5) = 0.05 + 0.20 + 0.05 + 0.30 = 0.60 \\ge 0.6$. Head $1$ is specialized.\n  - $r^{(2)}(5) = 0.05 + 0.30 + 0.05 + 0.20 = 0.60 \\ge 0.6$. Head $2$ is specialized.\n- Result: $k^\\star=1$; Specialized heads: $[0, 1, 2]$.",
            "answer": "[[3,[0,1,2]],[6,[0,1,2]],[1,[0,1,2]]]"
        },
        {
            "introduction": "Even the most powerful models have weaknesses, and understanding their vulnerabilities is crucial for building robust AI systems. This final practice introduces you to the world of adversarial attacks, where we intentionally craft inputs to fool a model. You will implement a simplified version of the \"HotFlip\" attack, using gradient information from a surrogate model to greedily change single tokens in a sentence until the model's sentiment prediction is reversed, offering a fascinating glimpse into the brittleness of neural networks .",
            "id": "3102527",
            "problem": "You are given a first-order surrogate of a fine-tuned Bidirectional Encoder Representations from Transformers (BERT) sentiment classifier. The surrogate models the final classification head operating on an aggregated token representation. Let the vocabulary size be $V$, the embedding dimension be $d$, and a sequence of length $L$ be represented by token indices $\\{t_1, t_2, \\dots, t_L\\}$, where each $t_i \\in \\{0,1,\\dots,V-1\\}$. The embedding matrix is $E \\in \\mathbb{R}^{V \\times d}$, with rows $E_k \\in \\mathbb{R}^d$ for token index $k$. The sequence representation is the average of its token embeddings,\n$$\nh = \\frac{1}{L} \\sum_{i=1}^L E_{t_i}.\n$$\nA logistic regression head with weights $W \\in \\mathbb{R}^d$ and bias $b \\in \\mathbb{R}$ computes the logit\n$$\nz = W^\\top h + b,\n$$\nthe probability of the positive class\n$$\np = \\sigma(z) = \\frac{1}{1 + e^{-z}},\n$$\nand the predicted label $\\hat{y}$ is defined by the threshold rule: $\\hat{y} = 1$ if $p \\ge 0.5$ and $\\hat{y} = 0$ otherwise. The binary cross-entropy loss for a label $y \\in \\{0,1\\}$ is\n$$\n\\mathcal{L}(y,p) = -\\left(y\\log(p) + (1-y)\\log(1-p)\\right).\n$$\n\nThe HotFlip attack constructs adversarial examples by flipping tokens to maximize the increase in loss while minimally changing the input. Using a first-order Taylor approximation, flipping a single token at position $j$ from $a$ to $b$ changes the loss by approximately\n$$\n\\Delta \\mathcal{L}_{j,a \\rightarrow b} \\approx \\nabla_{E_{t_j}} \\mathcal{L}(y,p)^\\top \\left(E_b - E_a\\right).\n$$\nUnder the surrogate model above, the chain rule gives\n$$\n\\nabla_{E_{t_j}} \\mathcal{L}(y,p) = \\frac{1}{L} \\left(p - y\\right) W,\n$$\nso that\n$$\n\\Delta \\mathcal{L}_{j,a \\rightarrow b} \\approx \\frac{p - y}{L} \\, W^\\top \\left(E_b - E_a\\right).\n$$\n\nYour task is to implement a greedy HotFlip attack that, for each input sequence, iteratively applies the single flip $(j,a \\rightarrow b)$ that maximizes $\\Delta \\mathcal{L}$ with $y$ fixed to the current predicted label $\\hat{y}$ (that is, you always maximize the loss of the currently predicted class). After each flip, recompute $p$ and $\\hat{y}$ and continue until the predicted label flips to the opposite class. When multiple flips yield the same $\\Delta \\mathcal{L}$, break ties first by choosing the smallest token index $b$ and then by choosing the smallest position index $j$. If no flip is possible (which is not the case in this test suite), the sequence remains unchanged. The goal is to change sentiment while minimally altering tokens, so the greedy process must stop immediately when the prediction flips to the opposite class.\n\nUse the following fixed parameters, which define the surrogate of the BERT classification head and vocabulary:\n\n- Vocabulary size $V = 6$ and embedding dimension $d = 2$.\n- Embedding matrix $E \\in \\mathbb{R}^{6 \\times 2}$ defined row-wise as\n  - $E_0 = [\\,0.0,\\, 0.0\\,]$,\n  - $E_1 = [\\,2.0,\\, 2.0\\,]$,\n  - $E_2 = [\\,-2.0,\\, -2.0\\,]$,\n  - $E_3 = [\\,1.0,\\, 0.5\\,]$,\n  - $E_4 = [\\,-1.0,\\, -0.5\\,]$,\n  - $E_5 = [\\,-1.0,\\, -0.5\\,]$.\n- Classifier weights $W = [\\,1.0,\\, 1.0\\,]$ and bias $b = 0.0$.\n\nDesign and implement an algorithm that performs the greedy HotFlip attack described above. You must apply it to the following test suite of sequences (each sequence is a list of token indices):\n\n- Test case $1$ (general case): $[\\,3,\\,1,\\,0,\\,3\\,]$.\n- Test case $2$ (requires multiple flips): $[\\,4,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4\\,]$.\n- Test case $3$ (tie-breaking across candidate tokens): $[\\,3,\\,3\\,]$.\n\nFor each test case, the program must output the final adversarial sequence of token indices obtained by the greedy HotFlip procedure when the predicted label flips to the opposite class. The answer for each test case must be a list of integers.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each inner list formatted without spaces. For example, the format must be\n$$\n[\\,[a_1,a_2,\\dots],\\,[b_1,b_2,\\dots],\\,[c_1,c_2,\\dots]\\,].\n$$",
            "solution": "The user wants to implement a greedy HotFlip adversarial attack on a simplified surrogate model of a sentiment classifier.\n\n### Step 1: Problem Validation\n\n**1. Extract Givens**\n\n- **Model Definition:**\n  - Vocabulary size $V = 6$, embedding dimension $d = 2$.\n  - Sequence of length $L$: $\\{t_1, t_2, \\dots, t_L\\}$, where $t_i \\in \\{0, \\dots, 5\\}$.\n  - Embedding matrix $E \\in \\mathbb{R}^{6 \\times 2}$ given by:\n    $E_0 = [\\,0.0,\\, 0.0\\,]$, $E_1 = [\\,2.0,\\, 2.0\\,]$, $E_2 = [\\,-2.0,\\, -2.0\\,]$, $E_3 = [\\,1.0,\\, 0.5\\,]$, $E_4 = [\\,-1.0,\\, -0.5\\,]$, $E_5 = [\\,-1.0,\\, -0.5\\,]$.\n  - Sequence representation: $h = \\frac{1}{L} \\sum_{i=1}^L E_{t_i}$.\n  - Classifier weights $W = [\\,1.0,\\, 1.0\\,]$ and bias $b = 0.0$.\n  - Logit: $z = W^\\top h + b$.\n  - Positive class probability: $p = \\sigma(z)$.\n  - Prediction rule: $\\hat{y} = 1$ if $p \\ge 0.5$ (i.e., $z \\ge 0$), and $\\hat{y} = 0$ otherwise.\n\n- **Attack Definition:**\n  - **Greedy Iterative Process:** At each step, apply the single token flip $(j, a \\rightarrow b)$ that maximizes the approximate change in loss, $\\Delta \\mathcal{L}$.\n  - **Objective Function:** Maximize $\\Delta \\mathcal{L}_{j,a \\rightarrow b} \\approx \\frac{p - \\hat{y}}{L} \\, W^\\top \\left(E_b - E_a\\right)$, where $\\hat{y}$ is the current predicted label.\n  - **Termination Condition:** Stop immediately when the predicted label $\\hat{y}$ flips to the opposite of the initial prediction.\n  - **Tie-Breaking Rule:** If multiple flips yield the same maximum $\\Delta \\mathcal{L}$, choose the one with the smallest new token index $b$. If a tie persists, choose the one with the smallest position index $j$.\n\n- **Test Cases:**\n  1. $[\\,3,\\,1,\\,0,\\,3\\,]$\n  2. $[\\,4,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4\\,]$\n  3. $[\\,3,\\,3\\,]$\n\n- **Output Format:** A single line `[[a1,a2,...],[b1,b2,...],[c1,c2,...]]`.\n\n**2. Validate Using Extracted Givens**\n\n- **Scientific Groundedness:** The problem is well-grounded in the field of deep learning, specifically concerning adversarial attacks on neural networks. The model is a standard logistic regression classifier operating on averaged embeddings. The attack method, HotFlip, is a known technique, and the use of a first-order Taylor approximation to guide the search is a standard approach. The mathematics (vector calculus, linear algebra) is correct.\n- **Well-Posedness:** The problem is well-posed. The model, parameters, and inputs are fully specified. The greedy algorithm is explicitly defined, including a clear objective function, a termination condition, and a comprehensive tie-breaking rule. This ensures that a unique solution exists for each test case.\n- **Objectivity:** The problem is stated in precise, objective, and mathematical language, free from ambiguity or subjective elements.\n\n**3. Verdict and Action**\n\nThe problem is valid. It is scientifically sound, well-posed, and objective. I will proceed with providing a solution.\n\n### Step 2: Algorithmic Design and Principles\n\nThe core of the task is to implement the specified greedy iterative attack. For each test case, we begin with the initial sequence and its prediction. Then, we loop, and in each iteration, we find and apply the single best token flip until the prediction is reversed.\n\n**1. SImplifying the Objective Function**\nThe objective is to maximize $\\Delta \\mathcal{L} \\approx \\frac{p - \\hat{y}}{L} W^\\top(E_b - E_a)$. Let's analyze the term $\\frac{p - \\hat{y}}{L}$:\n- If the current prediction is $\\hat{y} = 1$, then $p \\ge 0.5$, so $p-1 \\le 0$. Maximizing $\\Delta \\mathcal{L}$ requires minimizing the term $S_{j,b} = W^\\top(E_b - E_a)$.\n- If the current prediction is $\\hat{y} = 0$, then $p < 0.5$, so $p-0 > 0$. Maximizing $\\Delta \\mathcal{L}$ requires maximizing the term $S_{j,b} = W^\\top(E_b - E_a)$.\n\nThe term $W^\\top E_k$ for each token $k$ is constant throughout the process. We can pre-compute these values to speed up the search for the best flip. Let's define `wte_scores` as a vector where `wte_scores[k]` $= W^\\top E_k$.\n\n**2. Greedy Search Strategy**\nAt each step of the attack, we must find the optimal pair $(j, b)$ according to the objective and the tie-breaking rules. A robust way to implement this is a two-stage process:\n1.  **Find Best Score:** Iterate through all possible flips (all positions $j$ and all potential new tokens $b \\neq t_j$) and find the best possible score (minimum or maximum of $S_{j,b}$).\n2.  **Collect and Select:** Collect all flips $(j, b)$ that achieve this best score. From this list of optimal candidates, apply the tie-breaking rule: sort the candidates first by the new token index $b$ and then by the position index $j$, and select the first one.\n\n**3. Efficient Updates**\nCalculating the sequence representation $h = \\frac{1}{L} \\sum E_{t_i}$ from scratch after every flip is inefficient. A more efficient method is to update the sum of embeddings. If we flip token $a$ at position $j$ to token $b$, the new sum of embeddings is:\n$$ \\sum_{\\text{new}} = \\sum_{\\text{old}} - E_a + E_b $$\nWe can maintain this sum and compute $h$ by dividing by $L$ only when needed to calculate the prediction.\n\n**4. Algorithm**\nFor each test case:\n1.  Initialize `tokens` with the input sequence.\n2.  Compute the initial sum of embeddings $\\sum E = \\sum_{i=1}^L E_{t_i}$.\n3.  Compute the initial prediction $\\hat{y}_{\\text{initial}}$ from $\\sum E$.\n4.  Enter a `while` loop that runs until the prediction flips.\n    a. Inside the loop, determine the current prediction $\\hat{y}_{\\text{current}}$.\n    b. If $\\hat{y}_{\\text{current}} \\neq \\hat{y}_{\\text{initial}}$, break the loop.\n    c. Find the set of best candidate flips:\n        i. Initialize an empty list `candidate_flips` and `best_score` to $+\\infty$ or $-\\infty$.\n        ii. Loop through each position $j \\in [0, L-1]$ and each potential new token $b \\in [0, V-1]$.\n        iii. If $b$ is the same as the current token at $j$, skip.\n        iv. Calculate the score $S_{j,b} = \\text{wte\\_scores}[b] - \\text{wte\\_scores}[t_j]$.\n        v. Compare `score` with `best_score`. If it's better, reset `candidate_flips` to $[(j, b)]$ and update `best_score`. If it's equal, append $(j, b)$ to `candidate_flips`.\n    d. Apply tie-breaking: sort `candidate_flips` by $b$ then by $j$. Take the first element as the `(best_j, best_b)`.\n    e. Update the state:\n        i. Let $a = tokens[\\text{best\\_j}]$.\n        ii. Update `tokens[best_j] = best_b`.\n        iii. Update $\\sum E = \\sum E - E_a + E_b$.\n5.  Add the final `tokens` list to the results.\n6.  Format and print all results.",
            "answer": "[[3,2,0,3],[1,1,1,1,1,4,4,4],[2,3]]"
        }
    ]
}