{
    "hands_on_practices": [
        {
            "introduction": "The Transformer architecture is famously split into an encoder and a decoder stack. This distinction is not arbitrary; it reflects a fundamental difference in how information is processed. This practice  makes this difference tangible by challenging you to construct a task that requires bidirectional context, proving from first principles why a causally-masked decoder fails while an encoder succeeds.",
            "id": "3195539",
            "problem": "You will construct, analyze, and algorithmically decide a synthetic sequence classification task that fundamentally requires bidirectional context, and you will compare what can be computed by a stack of encoder layers versus a stack of decoder layers with a causal mask. You will then implement a program that, given a small test suite of parameter values, determines which architecture can solve the task under those parameters. All mathematical entities must be written in LaTeX.\n\nTask definition. Consider binary sequences of odd length $n \\in \\mathbb{N}$ with a special query token at the middle position. Write the sequence as\n$$\nx = \\big(a_{1}, a_{2}, \\dots, a_{m-1}, \\mathrm{[Q]}, b_{1}, b_{2}, \\dots, b_{m-1}\\big),\n$$\nwhere $n = 2m-1$, $m = \\frac{n+1}{2}$, and each $a_{k}, b_{k} \\in \\{0,1\\}$ for $k \\in \\{1,\\dots,m-1\\}$. Define the classification function $f:\\{0,1\\}^{n-1}\\to \\{0,1\\}$ by\n$$\nf(x) = 1 \\;\\;\\text{if and only if}\\;\\; \\forall d \\in \\{1,\\dots,m-1\\},\\; a_{d} = b_{d},\n$$\nand $f(x)=0$ otherwise. In words, the token at distance $d$ to the left of $\\mathrm{[Q]}$ must equal the token at distance $d$ to the right of $\\mathrm{[Q]}$; this is a center-palindrome check around $\\mathrm{[Q]}$.\n\nArchitectural model. Consider two architectures built from $L \\in \\mathbb{N}$ identical layers of Multi-Head Attention (MHA) with a per-layer, per-head windowed self-attention constraint of width $w \\in \\mathbb{N}$, followed by position-wise feed-forward networks. The attention graph abstraction for one layer is:\n- Encoder stack: from each position $i$, there are directed edges to all positions $j$ with $|i-j| \\le w$ (bidirectional within the window).\n- Decoder stack with a causal mask: from each position $i$, there are directed edges to all positions $j$ with $0 \\le i-j \\le w$ (only to the left within the window, including self).\n\nInformation-propagation model. Model each layer as enabling one-step message passing along edges. Over $L$ layers, information can propagate along any path of at most $L$ edges in the directed graph defined by the layer connectivity. The decision for this task must be produced at the middle query position $\\mathrm{[Q]}$ after exactly $L$ layers.\n\nYour goals.\n1. From first principles of directed graph reachability induced by the attention windows, prove that for the decoder with a causal mask, the token at $\\mathrm{[Q]}$ cannot access any information from the right half $\\{b_{1},\\dots,b_{m-1}\\}$ for any $n>1$, regardless of $w$ and $L$. Conclude that no decoder of this form can compute $f$ exactly for all inputs when $n>1$.\n2. From the same principles, derive a necessary and sufficient condition on $n$, $w$, and $L$ under which an encoder stack can compute $f$ exactly at $\\mathrm{[Q]}$. Your derivation must begin from the directed graph reachability view and yield a closed-form condition involving $n$, $w$, and $L$, justified by a constructive scheme that aggregates evidence to $\\mathrm{[Q]}$.\n3. Design a decision rule in terms of $n$, $w$, and $L$ that returns two booleans: one indicating whether an encoder stack can solve the task at $\\mathrm{[Q]}$ exactly for all inputs of length $n$, and one indicating whether a decoder with a causal mask can do so. Your rule must correctly handle the degenerate case $n=1$.\n\nProgram specification. Implement a complete, runnable program that evaluates your decision rule on the following test suite of $(n,w,L)$ triplets:\n- $(n,w,L) = (21,3,4)$,\n- $(n,w,L) = (17,4,2)$,\n- $(n,w,L) = (31,3,4)$,\n- $(n,w,L) = (1,1,1)$,\n- $(n,w,L) = (19,2,5)$.\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output an integer code $c \\in \\{0,1,2,3\\}$ defined by\n$$\nc \\;=\\; 2\\cdot \\mathbf{1}\\{\\text{encoder can solve}\\}\\;+\\; \\mathbf{1}\\{\\text{decoder can solve}\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function that returns $1$ if the statement is true and $0$ otherwise. For example, $c=2$ means the encoder can solve and the decoder cannot, $c=3$ means both can, $c=0$ means neither can, and $c=1$ means the decoder can but the encoder cannot. The final output format must be exactly like $[c_{1},c_{2},c_{3},c_{4},c_{5}]$ for the five cases listed above.",
            "solution": "The problem statement is assessed to be valid. It is a well-posed, scientifically grounded, and objective problem that explores the computational limits of idealized transformer architectures. The problem is based on a standard and useful abstraction of information flow in neural networks as graph reachability, a concept central to the theoretical analysis of deep learning models. All definitions and constraints are clear and self-consistent.\n\nWe now proceed to the solution, which is divided into three parts as requested.\n\nFirst, we formalize the problem's information-flow model. The sequence has length $n$, with positions indexed from $1$ to $n$. The central query token $\\mathrm{[Q]}$ is at position $m = (n+1)/2$. The task is to verify if $a_d = b_d$ for all $d \\in \\{1, \\dots, m-1\\}$, where $a_d$ is at position $m-d$ and $b_d$ is at position $m+d$. The decision is made at position $m$. Following the standard interpretation of transformer mechanics, a layer's computation at a position $i$ gathers information *from* a set of positions $\\{j\\}$. Therefore, the problem's description of \"directed edges from each position $i$ to all positions $j$\" is interpreted as the set of positions $\\{j\\}$ that position $i$ attends to, implying information flows from $j$ to $i$. The directed edges of the information-flow graph are thus $(j, i)$.\n\n### 1. Analysis of the Decoder Stack with Causal Mask\n\nThe decoder architecture is defined by a causal attention mask. Under our formal model, an edge $(j, i)$ exists in the information-flow graph if and only if position $i$ attends to position $j$. The rule is given by $0 \\le i-j \\le w$. This implies that $i-w \\le j \\le i$.\n\nA crucial property of this rule is that for any edge $(j, i)$, the source index $j$ must be less than or equal to the destination index $i$ (i.e., $j \\le i$). Now, consider a path of information propagation, which is a sequence of connected nodes $(p_0, p_1, \\dots, p_k)$ in the graph, where $p_0$ is the original source of information and $p_k$ is the final destination. For this to be a valid path, an edge $(p_s, p_{s+1})$ must exist for each $s \\in \\{0, \\dots, k-1\\}$. Applying the edge rule, this requires $p_s \\le p_{s+1}$ for all steps $s$. Consequently, any path in the decoder's information-flow graph must have non-decreasing position indices: $p_0 \\le p_1 \\le \\dots \\le p_k$.\n\nThe classification task requires checking if $a_d = b_d$, where $b_d$ is the token at position $m+d$. For any $d \\in \\{1, \\dots, m-1\\}$, the position $m+d$ is strictly greater than $m$. For the decision to be made at position $m$, information about the token $b_d$ must propagate from its source position $j_{src} = m+d$ to the destination position $i_{dst} = m$.\n\nThis would require the existence of a path from $p_0 = m+d$ to $p_k = m$. However, as proven above, any such path must satisfy $p_0 \\le p_k$, which would mean $m+d \\le m$. This inequality is false for any $d \\ge 1$. Therefore, no path exists from any position in the right half of the sequence (positions greater than $m$) to the central query position $m$.\n\nSince the value of $f(x)$ depends on tokens $\\{b_1, \\dots, b_{m-1}\\}$, and information from these tokens cannot reach position $m$, a decoder stack with a causal mask cannot compute $f(x)$ for any input sequence where $n > 1$.\n\nThe degenerate case is $n=1$. Here, $m = (1+1)/2 = 1$, and the range of $d$ is $\\{1, \\dots, m-1\\} = \\emptyset$. The condition $\\forall d \\in \\emptyset, a_d=b_d$ is vacuously true. Thus, for $n=1$, $f(x)=1$ for all inputs. A decoder can trivially learn to output this constant value.\n\nConclusion: A decoder with a causal mask can solve the task if and only if $n=1$.\n\n### 2. Analysis of the Encoder Stack\n\nThe encoder architecture has bidirectional attention. An edge $(j,i)$ exists if $|i-j| \\le w$. This condition is symmetric, meaning an edge $(i,j)$ also exists. Information can flow in both directions between any two positions within the window $w$.\n\nTo compute $f(x)$ at position $m$, the model must have access to information from all other positions in the sequence. The model consists of $L$ layers. In one layer, information at position $j$ can propagate to any position $i$ in the interval $[j-w, j+w]$. After $L$ layers, information from an initial position $j$ has propagated to cover the interval $[j-Lw, j+Lw]$. This is the effective receptive field.\n\nFor the decision at position $m$ to be fully informed, information from every position in the sequence $\\{1, \\dots, n\\}$ must be able to reach $m$ within $L$ layers. This means that for any position $j \\in \\{1, \\dots, n\\}$, $m$ must be within the receptive field of $j$ after $L$ layers. That is, $m \\in [j-Lw, j+Lw]$, which is equivalent to $|m-j| \\le Lw$.\n\nWe must ensure this condition holds for all $j$. The most stringent requirement comes from the positions most distant from the center $m$. These are the endpoints of the sequence, $j=1$ and $j=n$.\nThe distance from the center $m = (n+1)/2$ to the endpoints is:\n- To position $1$: $|m-1| = |\\frac{n+1}{2} - 1| = |\\frac{n-1}{2}| = \\frac{n-1}{2}$.\n- To position $n$: $|m-n| = |\\frac{n+1}{2} - n| = |\\frac{1-n}{2}| = \\frac{n-1}{2}$.\n\nSince these are the maximum distances, the condition simplifies to ensuring that information can travel this far. The total distance information can propagate in $L$ layers is $Lw$. Therefore, a necessary and sufficient condition for all information to reach the center is:\n$$\nLw \\ge \\frac{n-1}{2}\n$$\nIf this condition is met, information from all pairs $(a_d, b_d)$ can be aggregated at position $m$, allowing a sufficiently powerful model (which we assume) to compute the function $f(x)$. If the condition is not met, there is at least one token (at an endpoint) whose information cannot reach the center, making the computation impossible for certain inputs. Note that since $n$ is odd, $n-1$ is even, and $(n-1)/2$ is always an integer.\n\nFor the special case $n=1$, the condition becomes $Lw \\ge (1-1)/2$, which is $Lw \\ge 0$. As $L \\in \\mathbb{N}$ and $w \\in \\mathbb{N}$, we have $L \\ge 1$ and $w \\ge 1$, so this is always true. This aligns with the fact that the task is trivial for $n=1$.\n\nConclusion: An encoder stack can solve the task if and only if $Lw \\ge (n-1)/2$.\n\n### 3. Final Decision Rule\n\nBased on the analysis above, we can formulate a decision rule for a given triplet $(n, w, L)$.\n\n- **Encoder Solvability**: An encoder stack can solve the task if and only if the cumulative information propagation distance $Lw$ is sufficient to cover the distance from the sequence endpoints to the center.\n  $$ \\mathbf{1}\\{\\text{encoder can solve}\\} = \\mathbf{1}\\left\\{ Lw \\ge \\frac{n-1}{2} \\right\\} $$\n\n- **Decoder Solvability**: A decoder stack with a causal mask can solve the task if and only if the sequence is of length $n=1$, where the classification task is trivial.\n  $$ \\mathbf{1}\\{\\text{decoder can solve}\\} = \\mathbf{1}\\{ n = 1 \\} $$\n\nThe integer code $c$ for each test case is computed as specified:\n$$ c = 2\\cdot \\mathbf{1}\\{\\text{encoder can solve}\\} + \\mathbf{1}\\{\\text{decoder can solve}\\} $$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the transformer architecture problem by applying the derived decision rules\n    to a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (21, 3, 4),  # (n, w, L) triplet 1\n        (17, 4, 2),  # (n, w, L) triplet 2\n        (31, 3, 4),  # (n, w, L) triplet 3\n        (1, 1, 1),   # (n, w, L) triplet 4\n        (19, 2, 5),  # (n, w, L) triplet 5\n    ]\n\n    results = []\n    for n, w, l in test_cases:\n        # A. Decision rule for the decoder stack with a causal mask.\n        # The task is a center-palindrome check.\n        # A decoder with a causal mask prevents information flow from the future (right side)\n        # to the past (left side or center).\n        # When n > 1, the decision at the center position m = (n+1)/2 cannot access\n        # information from any position j > m.\n        # Thus, the comparison required by the task is impossible.\n        # The only exception is the trivial case n=1, where the palindrome condition is\n        # vacuously true, and the model only needs to output a constant 1.\n        decoder_can_solve = (n == 1)\n\n        # B. Decision rule for the encoder stack.\n        # An encoder has bidirectional attention. Information can flow from any position j to\n        # any position i, provided enough layers L and a large enough window w.\n        # The key condition is that information from the sequence endpoints (positions 1 and n)\n        # must be able to reach the center position m = (n+1)/2.\n        # The distance from either endpoint to the center is (n-1)/2.\n        # In L layers, with an attention window of width w, information can propagate\n        # a maximum distance of L * w.\n        # The task is solvable if and only if this reach is sufficient to cover the distance.\n        # This condition also correctly handles the n=1 case (l * w >= 0, which is always true).\n        encoder_can_solve = (l * w >= (n - 1) / 2)\n\n        # C. Calculate the integer code c as per the problem specification.\n        # c = 2 * I{encoder can solve} + 1 * I{decoder can solve}\n        # where I{.} is the indicator function (1 if true, 0 if false).\n        c = 2 * int(encoder_can_solve) + 1 * int(decoder_can_solve)\n        results.append(c)\n\n    # Final print statement in the exact required format: [c1,c2,c3,c4,c5]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While encoders can see the entire input sequence, how does this change if we restrict each layer's attention to a local window for efficiency? This exercise  explores the power of stacking layers, asking you to derive the 'effective receptive field' of such a network. By precisely calculating how information propagates layer by layer, you will discover the direct relationship between network depth $L$, window size $w$, and the model's ability to capture long-range dependencies.",
            "id": "3195573",
            "problem": "Consider a Transformer encoder stack of $L$ identical layers operating on a one-dimensional sequence of length $N$ (assume $N$ is sufficiently large so that edge effects can be ignored for the token of interest). Each layer consists of Multi-Head Attention (MHA), which for the purposes of this problem is modeled as self-attention constrained to a fixed, symmetric local window, followed by a Positionwise Feed-Forward Network (FFN). Formally, at layer $\\ell \\in \\{1, 2, \\dots, L\\}$ and position $i \\in \\{1, 2, \\dots, N\\}$, the attention operation updates the representation at position $i$ by taking a weighted sum of representations from positions $j$ such that $|j - i| \\leq r$, where $r \\in \\mathbb{N}$ is a fixed radius (identical across layers); the FFN acts independently on each position and therefore does not introduce new positional dependencies. Define the window size $w$ by $w = 2r + 1$.\n\nDefine the dependency set at depth $L$ for a target position $i$, denoted $D^{(L)}(i)$, as the set of input positions (layer $0$) whose representations can influence the representation at position $i$ in layer $L$ via some path through the computation graph induced by the local self-attention edges and positionwise feed-forward operations. Treat influence in purely structural terms: a position $j$ is in $D^{(L)}(i)$ if and only if there exists a path of allowed edges from $(j, 0)$ to $(i, L)$ in the layered computation graph consistent with the local window constraint $|j_{\\ell} - j_{\\ell-1}| \\leq r$ at each attention layer $\\ell$, and with the FFN contributing no cross-position edges.\n\nStarting from these definitions and first principles about how information can propagate across layers under a bounded local neighborhood constraint, derive tight upper and lower bounds for $|D^{(L)}(i)|$ as a function of $L$ and $w$ (or equivalently $r$), for positions $i$ that are at least $Lr$ indices away from the sequence boundaries so that edge effects are negligible. Then, by matching these bounds, provide the exact closed-form expression for $|D^{(L)}(i)|$ as a function of $L$ and $w$. Express your final answer as a single analytical expression in terms of $L$ and $w$ only. No numerical rounding is required.",
            "solution": "The problem asks for the exact size of the dependency set, $|D^{(L)}(i)|$, for a target position $i$ at layer $L$ in a Transformer encoder stack with local attention. The size is to be expressed as a function of the number of layers, $L$, and the attention window size, $w$. The analysis assumes the target position $i$ is sufficiently far from the sequence boundaries to ignore edge effects.\n\nFirst, let us formalize the given definitions. The encoder stack has $L$ layers. The attention mechanism in each layer is local with a fixed radius $r \\in \\mathbb{N}$. The window size is defined as $w = 2r + 1$. The representation at position $j_\\ell$ in layer $\\ell$ is influenced by representations from the previous layer, $\\ell-1$, at positions $j_{\\ell-1}$ such that $|j_\\ell - j_{\\ell-1}| \\leq r$. The Positionwise Feed-Forward Network (FFN) operates on each position independently and does not expand the dependency set.\n\nThe dependency set $D^{(L)}(i)$ is the set of all input positions $j$ (at layer $0$) from which there exists a computational path to position $i$ at layer $L$. A path is a sequence of positions $(j_0, j_1, \\dots, j_L)$ where $j_0 = j$ and $j_L = i$, satisfying the local attention constraint $|j_\\ell - j_{\\ell-1}| \\leq r$ for all $\\ell \\in \\{1, 2, \\dots, L\\}$. The problem asks for $|D^{(L)}(i)|$, the number of such valid starting positions $j_0$.\n\nWe will first establish a tight upper bound on $|D^{(L)}(i)|$ and then a tight lower bound. By showing these bounds are equal, we will determine the exact size of the set.\n\n**1. Derivation of the Upper Bound**\n\nLet $j \\in D^{(L)}(i)$. By definition, there exists a path of positions $(j_0, j_1, \\dots, j_L)$ with $j_0=j$ and $j_L=i$, such that for each layer transition from $\\ell-1$ to $\\ell$, the positional shift is bounded: $|j_\\ell - j_{\\ell-1}| \\leq r$.\n\nThe total displacement from the input position $j$ to the output position $i$ can be expressed as a sum of single-layer displacements:\n$$i - j = j_L - j_0 = \\sum_{\\ell=1}^{L} (j_\\ell - j_{\\ell-1})$$\nLet $\\Delta_\\ell = j_\\ell - j_{\\ell-1}$ be the displacement at layer $\\ell$. The constraint is $|\\Delta_\\ell| \\leq r$ for $\\ell = 1, \\dots, L$.\n\nUsing the triangle inequality for absolute values, we can bound the magnitude of the total displacement:\n$$|i - j| = \\left| \\sum_{\\ell=1}^{L} \\Delta_\\ell \\right| \\leq \\sum_{\\ell=1}^{L} |\\Delta_\\ell|$$\nSince each individual displacement is bounded by $r$, we have:\n$$\\sum_{\\ell=1}^{L} |\\Delta_\\ell| \\leq \\sum_{\\ell=1}^{L} r = Lr$$\nTherefore, any input position $j$ that can influence the output at position $i$ after $L$ layers must satisfy the condition:\n$$|i - j| \\leq Lr$$\nThis inequality defines the maximum possible range of input positions that can contribute to the output at position $i$. The set of all integers $j$ satisfying this condition is $\\{j \\in \\mathbb{Z} : i - Lr \\leq j \\leq i + Lr\\}$.\nThe dependency set $D^{(L)}(i)$ must be a subset of this set:\n$$D^{(L)}(i) \\subseteq \\{j \\in \\mathbb{Z} : |i - j| \\leq Lr\\}$$\nSince we are given that edge effects are negligible, this interval is not truncated. The number of integer points in this interval is $(i+Lr) - (i-Lr) + 1 = 2Lr + 1$.\nThus, we have an upper bound on the size of the dependency set:\n$$|D^{(L)}(i)| \\leq 2Lr + 1$$\n\n**2. Derivation of the Lower Bound**\n\nTo establish the lower bound, we must show that every integer position $j$ satisfying $|i-j| \\leq Lr$ is indeed a member of $D^{(L)}(i)$. This is equivalent to showing that for any such $j$, a valid path from $(j, 0)$ to $(i, L)$ exists.\n\nLet $j$ be an integer position such that $|i-j| \\leq Lr$. Let $d = i - j$ be the required total displacement. We have $|d| \\leq Lr$. We need to construct a sequence of integer displacements $(\\Delta_1, \\Delta_2, \\dots, \\Delta_L)$ such that:\n1.  $\\sum_{\\ell=1}^{L} \\Delta_\\ell = d$\n2.  $|\\Delta_\\ell| \\leq r$ for all $\\ell \\in \\{1, 2, \\dots, L\\}$\n\nWe can construct such a sequence. Without loss of generality, assume $d \\geq 0$. The case $d < 0$ follows symmetrically by reversing the signs of the displacements.\nSince $0 \\leq d \\leq Lr$ and $r \\geq 1$ (as $r \\in \\mathbb{N}$), we can use integer division to write $d$ as $d = qr + k$, where $q = \\lfloor d/r \\rfloor$ is the quotient and $k = d \\pmod r$ is the remainder. By definition of integer division, $q$ and $k$ are integers and $0 \\leq k < r$.\nFrom $d \\leq Lr$, we have $qr+k \\leq Lr$. Since $k \\geq 0$, it follows that $qr \\leq Lr$, which implies $q \\leq L$.\n\nWe can now define the sequence of displacements:\n- Set $\\Delta_\\ell = r$ for $\\ell = 1, \\dots, q$. These steps are valid as $|\\Delta_\\ell|=r \\leq r$.\n- If $q < L$, set $\\Delta_{q+1} = k$. This is valid since $0 \\leq k < r$ implies $|k| \\leq r$.\n- Set the remaining displacements $\\Delta_\\ell = 0$ for $\\ell = q+2, \\dots, L$. These are valid as $|0| \\leq r$.\n\nThe sum of these displacements is:\n$$\\sum_{\\ell=1}^{L} \\Delta_\\ell = \\sum_{\\ell=1}^{q} r + k + \\sum_{\\ell=q+2}^{L} 0 = qr + k = d$$\nThis construction provides a valid path for any $d$ where $q = \\lfloor d/r \\rfloor < L$.\n\nThe only special case is when $q=L$. This occurs if $Lr \\leq d < (L+1)r$. Combined with the condition $d \\leq Lr$, this implies $d=Lr$. In this case, $q=L$ and $k=0$. We can set $\\Delta_\\ell = r$ for all $\\ell = 1, \\dots, L$. The sum is $\\sum_{\\ell=1}^L r = Lr = d$, and each step is valid.\n\nThus, for any integer displacement $d$ with $|d| \\leq Lr$, a valid sequence of single-layer displacements exists. This proves that every position $j$ satisfying $|i-j| \\leq Lr$ is reachable. Therefore:\n$$\\{j \\in \\mathbb{Z} : |i - j| \\leq Lr\\} \\subseteq D^{(L)}(i)$$\nThe size of this set gives us the lower bound:\n$$|D^{(L)}(i)| \\geq 2Lr + 1$$\n\n**3. Conclusion and Final Expression**\n\nWe have shown that $2Lr + 1 \\leq |D^{(L)}(i)| \\leq 2Lr + 1$. The upper and lower bounds match, so the size of the dependency set is exactly:\n$$|D^{(L)}(i)| = 2Lr + 1$$\nThe problem requires the answer to be expressed in terms of $L$ and the window size $w$. We are given the relation $w = 2r + 1$. We can solve for the radius $r$:\n$$r = \\frac{w-1}{2}$$\nSubstituting this expression for $r$ into our result for $|D^{(L)}(i)|$:\n$$|D^{(L)}(i)| = 2L \\left( \\frac{w-1}{2} \\right) + 1 = L(w-1) + 1$$\nThis is the final closed-form expression for the size of the dependency set for a position far from the sequence edges.",
            "answer": "$$\\boxed{L(w-1) + 1}$$"
        },
        {
            "introduction": "At its core, what is the attention mechanism really doing? This practice  provides a powerful intuition by framing attention as a form of content-addressable memory. By implementing a 'toy cipher' task, you will see how queries (ciphertext) can retrieve values (plaintext) by matching against keys, solidifying a key-value retrieval model that demystifies how attention associates and recalls information.",
            "id": "3195550",
            "problem": "You will design and analyze a toy cipher task that demonstrates how a transformer encoder can perform decryption via attention-based key matching and how this mechanism functions as content-addressable memory. The core of the task is a retrieval process in which a set of key vectors and corresponding value vectors store plaintext symbols, and a set of query vectors represent ciphertext that must be decrypted by matching queries to keys using a similarity-based weighting, followed by a normalized weighting scheme and a weighted aggregation of values. All mathematical entities must be treated rigorously as vectors and matrices over the real numbers. Your program must implement the retrieval mechanism and quantify decryption success using a well-defined accuracy metric for a set of test cases.\n\nFundamental base to use:\n- Vector spaces over the real numbers, with vectors in $\\mathbb{R}^{d}$ and matrices in $\\mathbb{R}^{n \\times d}$.\n- The Euclidean dot product, for $x, y \\in \\mathbb{R}^{d}$, defined by $x \\cdot y = \\sum_{i=1}^{d} x_i y_i$.\n- The principle that larger dot products indicate greater similarity under a fixed norm constraint.\n- A normalization procedure that transforms a set of real-valued scores into a probability distribution over keys by exponentiation and normalization by the sum of exponentials.\n- A scaling principle that counteracts dimensionality growth in the dot product when $d$ increases, implemented by dividing the dot product scores by a dimension-dependent factor before exponentiation.\n\nCipher-memory setup:\n- Keys $K \\in \\mathbb{R}^{N \\times d_k}$ store addresses, values $V \\in \\mathbb{R}^{N \\times d_v}$ store plaintext symbols in one-hot form, and queries $Q \\in \\mathbb{R}^{N \\times d_k}$ represent ciphertext that must be matched to keys to recover plaintext. Use $N = 5$, $d_k = 4$, and $d_v = 5$.\n\n- Define the keys $K$ by rows $k_0, k_1, k_2, k_3, k_4 \\in \\mathbb{R}^{4}$ as follows, with each written as a unit-norm vector:\n  $k_0 = (1, 0, 0, 0)$,\n  $k_1 = (0, 1, 0, 0)$,\n  $k_2 = (0, 0, 1, 0)$,\n  $k_3 = (0, 0, 0, 1)$,\n  $k_4 = \\left(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0, 0\\right)$.\n  Collect these into $K$ as $K = \\begin{bmatrix} k_0^\\top \\\\ k_1^\\top \\\\ k_2^\\top \\\\ k_3^\\top \\\\ k_4^\\top \\end{bmatrix}$.\n\n- Define the values $V \\in \\mathbb{R}^{5 \\times 5}$ as the $5 \\times 5$ identity matrix, so that the plaintext label of $k_i$ is the index $i \\in \\{0, 1, 2, 3, 4\\}$ and the associated value vector is the one-hot vector $e_i \\in \\mathbb{R}^{5}$ with a $1$ at position $i$ and $0$ elsewhere.\n\n- The predicted plaintext symbol for query $q \\in \\mathbb{R}^{4}$ is obtained by:\n  1. Computing similarity scores against all keys using the dot product.\n  2. Dividing scores by a dimension-dependent scale factor to maintain stability across $d_k$.\n  3. Applying exponentiation and normalization to obtain a probability distribution over keys.\n  4. Taking a weighted sum of the value vectors using these probabilities to produce an output vector in $\\mathbb{R}^{5}$.\n  5. Choosing the symbol whose index is the position of the largest component of the output vector (top-$1$ prediction).\n\nNormalization details:\n- All queries must be normalized to unit $\\ell_2$ norm after any additive noise is introduced.\n\nDecryption accuracy:\n- For a sequence of $N$ queries, the top-$1$ decryption accuracy is the fraction of positions for which the predicted symbol index equals the ground-truth label associated with the queryâ€™s intended key.\n\nTest suite:\nImplement three test cases that exercise different facets of attention-based decryption.\n\n- Test Case A (happy path, exact matches):\n  Use the permutation $\\pi_A = [2, 0, 4, 1, 3]$ and define $Q_A$ by permuting the keys: $Q_A = \\begin{bmatrix} k_{\\pi_A(0)}^\\top \\\\ k_{\\pi_A(1)}^\\top \\\\ k_{\\pi_A(2)}^\\top \\\\ k_{\\pi_A(3)}^\\top \\\\ k_{\\pi_A(4)}^\\top \\end{bmatrix}$. The ground-truth plaintext sequence is the label list $[ \\pi_A(0), \\pi_A(1), \\pi_A(2), \\pi_A(3), \\pi_A(4) ]$.\n\n- Test Case B (noisy queries, stability under perturbations):\n  Start from $Q_A$ and add independent Gaussian noise with standard deviation $\\sigma = 0.30$ to each component, then renormalize each query to unit norm to obtain $Q_B$. Use the same ground-truth labels as in Test Case A.\n\n- Test Case C (key collision, ambiguity in content addressing):\n  Create a colliding key set $K_C$ by taking $K$ and replacing $k_1$ with $k_3$, i.e., set $k_1 := k_3$. Use the permutation $\\pi_C = [1, 3, 0, 4, 2]$ and define $Q_C$ by permuting the rows of $K_C$ accordingly. Use the ground-truth plaintext labels $[ \\pi_C(0), \\pi_C(1), \\pi_C(2), \\pi_C(3), \\pi_C(4) ]$. The values $V$ remain the $5 \\times 5$ identity, meaning the two identical keys still correspond to different plaintext symbols.\n\nRequired output:\n- Your program must compute the top-$1$ decryption accuracy for each of the three test cases in the order A, B, C.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[a_A,a_B,a_C]$, where each $a_\\cdot$ is a floating-point number in decimal form.",
            "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded in the principles of linear algebra and the concepts of attention mechanisms from deep learning. The problem is well-posed, self-contained, and formalizable into a computational task. There are no contradictions, ambiguities, or factual inaccuracies.\n\nThe task is to design and analyze a toy cipher based on the scaled dot-product attention mechanism, which serves as a model for content-addressable memory. This involves retrieving plaintext symbols (values) by matching ciphertext (queries) against a set of stored addresses (keys). The success of this decryption process is quantified by a top-$1$ accuracy metric.\n\nThe core of the decryption mechanism is the attention function, which computes an output for a set of queries $Q \\in \\mathbb{R}^{N \\times d_k}$ given a set of keys $K \\in \\mathbb{R}^{N \\times d_k}$ and values $V \\in \\mathbb{R}^{N \\times d_v}$. The dimensions are given as $N=5$, $d_k=4$, and $d_v=5$. The process for a matrix of queries $Q$ is defined as:\n$1$. Compute similarity scores between each query and all keys. This is performed via matrix multiplication: $S = QK^\\top$. The resulting score matrix $S \\in \\mathbb{R}^{N \\times N}$ contains the dot product of each query vector with each key vector.\n$2$. Scale the scores to counteract the growth of dot product variance with dimension. Each score is divided by $\\sqrt{d_k}$. The scaled score matrix is $S' = \\frac{QK^\\top}{\\sqrt{d_k}}$.\n$3$. Normalize the scaled scores for each query to form a probability distribution over the keys. This is achieved by applying the softmax function row-wise to $S'$: $A = \\text{softmax}(S')$. The element $A_{ij}$ of the resulting attention matrix $A \\in \\mathbb{R}^{N \\times N}$ represents the weight of key $j$ for query $i$.\n$4$. Compute the output vectors as a weighted sum of the value vectors, where the weights are the attention probabilities. This is computed as $O = AV$. The resulting output matrix $O \\in \\mathbb{R}^{N \\times d_v}$ contains the retrieved (decrypted) representations.\n$5$. For each output vector $o_i$ (a row in $O$), the predicted plaintext symbol index $\\hat{y}_i$ is the index of its largest component: $\\hat{y}_i = \\arg\\max_j (o_i)_j$.\n\nThe decryption accuracy is the fraction of queries for which the predicted symbol index matches the ground-truth symbol index: $\\text{Accuracy} = \\frac{1}{N}\\sum_{i=0}^{N-1} \\mathbb{I}(\\hat{y}_i = y_i)$, where $\\mathbb{I}$ is the indicator function.\n\nThe key and value matrices are defined as follows. The key vectors are:\n$k_0 = (1, 0, 0, 0)$\n$k_1 = (0, 1, 0, 0)$\n$k_2 = (0, 0, 1, 0)$\n$k_3 = (0, 0, 0, 1)$\n$k_4 = \\left(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0, 0\\right)$\nThese are collected as rows into the key matrix $K \\in \\mathbb{R}^{5 \\times 4}$. The value matrix $V \\in \\mathbb{R}^{5 \\times 5}$ is the identity matrix, $V=I_5$. This means the value vector $v_i$ associated with key $k_i$ is the one-hot vector $e_i$ corresponding to the label $i$.\n\nWe now analyze the three specified test cases.\n\n**Test Case A: Exact Matches**\nIn this case, the query vectors are perfect copies of the key vectors, albeit in a permuted order. The permutation is $\\pi_A = [2, 0, 4, 1, 3]$, so the query matrix is $Q_A$, where the $i$-th row is $k_{\\pi_A(i)}$. The ground-truth labels are $Y_A = \\pi_A$.\nFor any query $q_i = k_{\\pi_A(i)}$, its dot product with the matching key $k_{\\pi_A(i)}$ will be $1$ (since all keys are unit-norm), while its dot product with any other orthogonal key (e.g., $k_0 \\cdot k_1 = 0$) will be $0$. The dot product with the non-orthogonal key $k_4$ will be less than $1$. For example, for $q_0 = k_2$, the scores will be $[k_2 \\cdot k_0, k_2 \\cdot k_1, k_2 \\cdot k_2, k_2 \\cdot k_3, k_2 \\cdot k_4] = [0, 0, 1, 0, 0]$. The softmax function will concentrate all probability mass on the index of the matching key. Therefore, the attention mechanism will perfectly retrieve the correct value vector $v_{\\pi_A(i)}$, and the predicted label will be $\\pi_A(i)$. This should hold for all queries, resulting in a decryption accuracy of $1.0$.\n\n**Test Case B: Noisy Queries**\nHere, we start with $Q_A$ and add Gaussian noise with standard deviation $\\sigma = 0.30$ to each component. The resulting noisy query vectors are then renormalized to unit $\\ell_2$ norm. This simulates a more realistic scenario where ciphertext may be corrupted.\nThe noise perturbs the query vectors. A noisy query $q_i'$ derived from $k_{\\pi_A(i)}$ will still be most similar to its original key $k_{\\pi_A(i)}$, but its dot product scores with other keys will no longer be exactly zero. As long as the noise is not excessively large, the score $q_i' \\cdot k_{\\pi_A(i)}$ will remain the largest score. The softmax function will thus still assign the highest attention weight to the correct key. However, if the noise is large enough to make a query more similar to an incorrect key, a decryption error will occur. With $\\sigma=0.30$, we expect high but not perfect accuracy.\n\n**Test Case C: Key Collision**\nThis case demonstrates a failure mode of content-addressable memory: ambiguity from non-unique keys. The key matrix $K_C$ is created by replacing key $k_1$ in the original matrix $K$ with key $k_3$. So, $K_C$ has two identical rows at indices $1$ and $3$, both equal to the vector $k_3 = (0, 0, 0, 1)$. The value matrix $V$ remains the identity matrix, meaning the key at index $1$ is associated with plaintext label $1$ (value $v_1=e_1$) and the key at index $3$ is associated with label $3$ (value $v_3=e_3$).\nThe queries $Q_C$ are generated by permuting the rows of this new key matrix $K_C$ with $\\pi_C = [1, 3, 0, 4, 2]$. The ground-truth labels are $Y_C = \\pi_C$.\nLet's analyze the query $q_0 = k_{C, \\pi_C(0)} = k_{C,1}$. Since row $1$ of $K_C$ is $k_3$, this query is $q_0 = k_3$. The ground-truth label is $y_0 = \\pi_C(0) = 1$. When we compute the attention scores for this query against $K_C$, the dot product $q_0 \\cdot k_{C,j}$ will be maximal and equal for both $j=1$ and $j=3$, as both $k_{C,1}$ and $k_{C,3}$ are identical to $q_0$. The softmax function will assign equal, high attention weights to keys $1$ and $3$. The resulting output vector will be a sum of value vectors, dominated by $w_1 v_1 + w_3 v_3$, where $w_1=w_3$. This means the output vector will have equal large components at indices $1$ and $3$. The $\\arg\\max$ function, by convention (e.g., in NumPy), breaks ties by returning the first index of the maximum value. Thus, the prediction for $q_0$ will be $1$, which matches the ground truth $y_0=1$.\nNow consider the query $q_1 = k_{C, \\pi_C(1)} = k_{C,3}$. This query is also the vector $k_3$, identical to $q_0$. The prediction process is identical, so the predicted label will again be $1$. However, the ground-truth label is $y_1 = \\pi_C(1) = 3$. This is a decryption error.\nFor the other queries ($q_2, q_3, q_4$), they match unique keys in $K_C$ and are expected to be decrypted correctly. In total, we expect $1$ error out of $5$ queries, leading to an accuracy of $4/5 = 0.8$.\nThis case highlights that when keys are not unique, the attention mechanism cannot distinguish between them, and the retrieval becomes ambiguous, with the outcome potentially depending on arbitrary tie-breaking rules.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the toy cipher problem by implementing and testing a simplified\n    attention mechanism.\n    \"\"\"\n    N = 5\n    d_k = 4\n    d_v = 5\n\n    def softmax(x, axis=-1):\n        \"\"\"Numerically stable softmax function.\"\"\"\n        e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n        return e_x / np.sum(e_x, axis=axis, keepdims=True)\n\n    def attention_decrypt(Q, K, V):\n        \"\"\"\n        Performs decryption using the scaled dot-product attention mechanism.\n        Returns the predicted symbol indices.\n        \"\"\"\n        # 1. Compute similarity scores and scale them\n        scores = (Q @ K.T) / np.sqrt(d_k)\n        \n        # 2. Compute attention weights\n        attention_weights = softmax(scores, axis=1)\n        \n        # 3. Compute output vectors (weighted sum of values)\n        output = attention_weights @ V\n        \n        # 4. Predict symbols by taking the argmax\n        predictions = np.argmax(output, axis=1)\n        \n        return predictions\n\n    def calculate_accuracy(predictions, ground_truth):\n        \"\"\"Calculates the top-1 decryption accuracy.\"\"\"\n        return np.mean(predictions == ground_truth)\n\n    # --- Common Setup ---\n    # Define the base key matrix K\n    K = np.array([\n        [1.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0],\n        [1/np.sqrt(2), 1/np.sqrt(2), 0.0, 0.0]\n    ])\n\n    # Define the value matrix V as the identity matrix\n    V = np.identity(N)\n\n    results = []\n\n    # --- Test Case A: Happy path, exact matches ---\n    pi_A = np.array([2, 0, 4, 1, 3])\n    Q_A = K[pi_A]\n    Y_A = pi_A\n    \n    predictions_A = attention_decrypt(Q_A, K, V)\n    accuracy_A = calculate_accuracy(predictions_A, Y_A)\n    results.append(accuracy_A)\n\n    # --- Test Case B: Noisy queries, stability under perturbations ---\n    # Use a fixed seed for reproducibility\n    np.random.seed(42)\n    sigma = 0.30\n    \n    noise = np.random.normal(scale=sigma, size=Q_A.shape)\n    Q_B_noisy = Q_A + noise\n    \n    # Renormalize each query to unit norm\n    norms = np.linalg.norm(Q_B_noisy, axis=1, keepdims=True)\n    Q_B = Q_B_noisy / norms\n    \n    # Ground truth remains the same\n    Y_B = Y_A\n    \n    predictions_B = attention_decrypt(Q_B, K, V)\n    accuracy_B = calculate_accuracy(predictions_B, Y_B)\n    results.append(accuracy_B)\n\n    # --- Test Case C: Key collision, ambiguity in content addressing ---\n    # Create the colliding key set K_C\n    K_C = K.copy()\n    K_C[1] = K[3]  # k_1 is replaced by k_3\n    \n    pi_C = np.array([1, 3, 0, 4, 2])\n    \n    # Queries Q_C are permuted rows of K_C\n    Q_C = K_C[pi_C]\n    \n    # Ground truth labels\n    Y_C = pi_C\n    \n    predictions_C = attention_decrypt(Q_C, K_C, V)\n    accuracy_C = calculate_accuracy(predictions_C, Y_C)\n    results.append(accuracy_C)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}