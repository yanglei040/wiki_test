{
    "hands_on_practices": [
        {
            "introduction": "Transformer模型的核心是其注意力机制，但$Q$、$K$、$V$矩阵的交互可能显得抽象。为了揭开其神秘面纱，我们可以将注意力机制看作一个可微的内容可寻址存储系统。在这个实践中，你将通过构建一个玩具密码任务，亲手实现注意力机制，其中“密文”（查询）通过匹配“钥匙”（键）来检索“明文”（值），从而深刻理解注意力是如何基于内容相似性来检索和加权信息的。",
            "id": "3195550",
            "problem": "你将设计并分析一个玩具密码任务，该任务演示 transformer 编码器如何通过基于注意力的密钥匹配来执行解密，以及此机制如何作为内容可寻址存储器运作。该任务的核心是一个检索过程，其中一组键向量和相应的值向量存储明文符号，一组查询向量代表必须通过使用基于相似度的加权将查询与密钥匹配来解密的密文，随后是一个归一化加权方案和值的加权聚合。所有数学实体都必须严格地视为实数域上的向量和矩阵。你的程序必须实现该检索机制，并使用一个为一组测试用例明确定义的准确率度量来量化解密成功率。\n\n使用的基础原理：\n- 实数域上的向量空间，其中向量在 $\\mathbb{R}^{d}$ 中，矩阵在 $\\mathbb{R}^{n \\times d}$ 中。\n- Euclidean 点积，对于 $x, y \\in \\mathbb{R}^{d}$，定义为 $x \\cdot y = \\sum_{i=1}^{d} x_i y_i$。\n- 在固定范数约束下，点积越大表示相似度越高的原则。\n- 一个归一化过程，通过指数化和除以指数和进行归一化，将一组实值分数转换为关于密钥的概率分布。\n- 一个缩放原则，通过在指数化前将点积得分除以一个与维度相关的因子，来抵消当 $d$ 增加时点积中的维度增长效应。\n\n密码-存储器设置：\n- 密钥 $K \\in \\mathbb{R}^{N \\times d_k}$ 存储地址，值 $V \\in \\mathbb{R}^{N \\times d_v}$ 以独热（one-hot）形式存储明文符号，查询 $Q \\in \\mathbb{R}^{N \\times d_k}$ 代表必须与密钥匹配以恢复明文的密文。使用 $N = 5$，$d_k = 4$ 和 $d_v = 5$。\n\n- 将密钥 $K$ 按行 $k_0, k_1, k_2, k_3, k_4 \\in \\mathbb{R}^{4}$ 定义如下，每个都写成单位范数向量：\n  $k_0 = (1, 0, 0, 0)$,\n  $k_1 = (0, 1, 0, 0)$,\n  $k_2 = (0, 0, 1, 0)$,\n  $k_3 = (0, 0, 0, 1)$,\n  $k_4 = \\left(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0, 0\\right)$。\n  将它们收集到 $K$ 中，形式为 $K = \\begin{bmatrix} k_0^\\top \\\\ k_1^\\top \\\\ k_2^\\top \\\\ k_3^\\top \\\\ k_4^\\top \\end{bmatrix}$。\n\n- 将值 $V \\in \\mathbb{R}^{5 \\times 5}$ 定义为 $5 \\times 5$ 的单位矩阵，因此 $k_i$ 的明文标签是索引 $i \\in \\{0, 1, 2, 3, 4\\}$，并且相关的值向量是独热向量 $e_i \\in \\mathbb{R}^{5}$，其在位置 $i$ 处为 $1$，其他位置为 $0$。\n\n- 查询 $q \\in \\mathbb{R}^{4}$ 的预测明文符号通过以下步骤获得：\n  1. 使用点积计算与所有密钥的相似度得分。\n  2. 将得分除以一个与维度相关的缩放因子，以在 $d_k$ 变化时保持稳定性。\n  3. 应用指数化和归一化以获得关于密钥的概率分布。\n  4. 使用这些概率对值向量进行加权求和，以产生一个在 $\\mathbb{R}^{5}$ 中的输出向量。\n  5. 选择其索引为输出向量最大分量位置的符号（top-1 预测）。\n\n归一化细节：\n- 在引入任何加性噪声后，所有查询都必须归一化为单位 $\\ell_2$ 范数。\n\n解密准确率：\n- 对于一个包含 $N$ 个查询的序列，top-1 解密准确率是预测的符号索引等于查询的目标密钥所关联的真实标签的位置所占的比例。\n\n测试套件：\n实现三个测试用例，以测试基于注意力的解密的不同方面。\n\n- 测试用例 A（理想路径，精确匹配）：\n  使用排列 $\\pi_A = [2, 0, 4, 1, 3]$ 并通过排列密钥来定义 $Q_A$：$Q_A = \\begin{bmatrix} k_{\\pi_A(0)}^\\top \\\\ k_{\\pi_A(1)}^\\top \\\\ k_{\\pi_A(2)}^\\top \\\\ k_{\\pi_A(3)}^\\top \\\\ k_{\\pi_A(4)}^\\top \\end{bmatrix}$。真实明文序列是标签列表 $[ \\pi_A(0), \\pi_A(1), \\pi_A(2), \\pi_A(3), \\pi_A(4) ]$。\n\n- 测试用例 B（带噪声的查询，扰动下的稳定性）：\n  从 $Q_A$ 开始，向每个分量添加标准差为 $\\sigma = 0.30$ 的独立高斯噪声，然后将每个查询重新归一化为单位范数以获得 $Q_B$。使用与测试用例 A 中相同的真实标签。\n\n- 测试用例 C（密钥冲突，内容寻址中的歧义性）：\n  创建一个冲突密钥集 $K_C$，方法是取 $K$ 并将 $k_1$ 替换为 $k_3$，即设置 $k_1 := k_3$。使用排列 $\\pi_C = [1, 3, 0, 4, 2]$ 并通过相应地排列 $K_C$ 的行来定义 $Q_C$。使用真实明文标签 $[ \\pi_C(0), \\pi_C(1), \\pi_C(2), \\pi_C(3), \\pi_C(4) ]$。值 $V$ 保持为 $5 \\times 5$ 的单位矩阵，这意味着两个相同的密钥仍然对应不同的明文符号。\n\n要求输出：\n- 你的程序必须按 A、B、C 的顺序计算这三个测试用例的 top-1 解密准确率。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表，例如 $[a_A,a_B,a_C]$，其中每个 $a_\\cdot$ 是一个十进制形式的浮点数。",
            "solution": "问题陈述已经过分析，被认为是有效的。它在科学上基于线性代数的原理和深度学习中的注意力机制概念。该问题定义明确、自成一体，并且可以形式化为一个计算任务。其中没有矛盾、歧义或事实错误。\n\n任务是设计和分析一个基于缩放点积注意力机制的玩具密码，它可作为内容可寻址存储器的模型。这涉及通过将密文（查询）与一组存储的地址（密钥）进行匹配来检索明文符号（值）。这个解密过程的成功率通过 top-1 准确率度量来量化。\n\n解密机制的核心是注意力函数，它为一组查询 $Q \\in \\mathbb{R}^{N \\times d_k}$ 计算输出，给定一组密钥 $K \\in \\mathbb{R}^{N \\times d_k}$ 和值 $V \\in \\mathbb{R}^{N \\times d_v}$。维度给定为 $N=5$，$d_k=4$，和 $d_v=5$。对于查询矩阵 $Q$ 的处理过程定义如下：\n$1$. 计算每个查询与所有密钥之间的相似度得分。这通过矩阵乘法执行：$S = QK^\\top$。得到的得分矩阵 $S \\in \\mathbb{R}^{N \\times N}$ 包含每个查询向量与每个键向量的点积。\n$2$. 缩放得分以抵消点积方差随维度增长的影响。每个得分都除以 $\\sqrt{d_k}$。缩放后的得分矩阵是 $S' = \\frac{QK^\\top}{\\sqrt{d_k}}$。\n$3$. 对每个查询的缩放后得分进行归一化，以形成关于密钥的概率分布。这是通过对 $S'$ 逐行应用 softmax 函数来实现的：$A = \\text{softmax}(S')$。得到的注意力矩阵 $A \\in \\mathbb{R}^{N \\times N}$ 的元素 $A_{ij}$ 代表密钥 $j$ 对于查询 $i$ 的权重。\n$4$. 将输出向量计算为值向量的加权和，其中权重是注意力概率。这计算为 $O = AV$。得到的输出矩阵 $O \\in \\mathbb{R}^{N \\times d_v}$ 包含检索到的（解密的）表示。\n$5$. 对于每个输出向量 $o_i$（$O$ 中的一行），预测的明文符号索引 $\\hat{y}_i$ 是其最大分量的索引：$\\hat{y}_i = \\arg\\max_j (o_i)_j$。\n\n解密准确率是预测的符号索引与真实符号索引匹配的查询所占的比例：$\\text{Accuracy} = \\frac{1}{N}\\sum_{i=0}^{N-1} \\mathbb{I}(\\hat{y}_i = y_i)$，其中 $\\mathbb{I}$ 是指示函数。\n\n密钥和值矩阵定义如下。密钥向量为：\n$k_0 = (1, 0, 0, 0)$\n$k_1 = (0, 1, 0, 0)$\n$k_2 = (0, 0, 1, 0)$\n$k_3 = (0, 0, 0, 1)$\n$k_4 = \\left(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0, 0\\right)$\n这些作为行被收集到密钥矩阵 $K \\in \\mathbb{R}^{5 \\times 4}$ 中。值矩阵 $V \\in \\mathbb{R}^{5 \\times 5}$ 是单位矩阵，$V=I_5$。这意味着与密钥 $k_i$ 相关联的值向量 $v_i$ 是对应于标签 $i$ 的独热向量 $e_i$。\n\n我们现在分析三个指定的测试用例。\n\n**测试用例 A：精确匹配**\n在这种情况下，查询向量是密钥向量的完美副本，尽管顺序是打乱的。排列是 $\\pi_A = [2, 0, 4, 1, 3]$，所以查询矩阵是 $Q_A$，其中第 $i$ 行是 $k_{\\pi_A(i)}$。真实标签是 $Y_A = \\pi_A$。\n对于任何查询 $q_i = k_{\\pi_A(i)}$，它与匹配密钥 $k_{\\pi_A(i)}$ 的点积将为 $1$（因为所有密钥都是单位范数），而它与任何其他正交密钥（例如，$k_0 \\cdot k_1 = 0$）的点积将为 $0$。与非正交密钥 $k_4$ 的点积将小于 $1$。例如，对于 $q_0 = k_2$，得分将是 $[k_2 \\cdot k_0, k_2 \\cdot k_1, k_2 \\cdot k_2, k_2 \\cdot k_3, k_2 \\cdot k_4] = [0, 0, 1, 0, 0]$。softmax 函数会将所有概率质量集中在匹配密钥的索引上。因此，注意力机制将完美地检索到正确的值向量 $v_{\\pi_A(i)}$，并且预测的标签将是 $\\pi_A(i)$。这应该对所有查询都成立，从而得到 1.0 的解密准确率。\n\n**测试用例 B：带噪声的查询**\n在这里，我们从 $Q_A$ 开始，向每个分量添加标准差为 $\\sigma = 0.30$ 的高斯噪声。然后将得到的带噪声的查询向量重新归一化为单位 $\\ell_2$ 范数。这模拟了一个更真实的场景，即密文可能被破坏。\n噪声扰动了查询向量。从 $k_{\\pi_A(i)}$ 派生的带噪声的查询 $q_i'$ 仍然会与其原始密钥 $k_{\\pi_A(i)}$ 最相似，但它与其他密钥的点积得分将不再精确为零。只要噪声不是过大，得分 $q_i' \\cdot k_{\\pi_A(i)}$ 仍然会是最大的得分。因此，softmax 函数仍然会为正确的密钥分配最高的注意力权重。然而，如果噪声足够大，使得一个查询与一个不正确的密钥更相似，就会发生解密错误。对于 $\\sigma=0.30$，我们预期准确率会很高但并非完美。\n\n**测试用例 C：密钥冲突**\n这个案例演示了内容可寻址存储器的一种失败模式：由非唯一密钥引起的歧义。密钥矩阵 $K_C$ 是通过将原始矩阵 $K$ 中的密钥 $k_1$ 替换为密钥 $k_3$ 来创建的。所以，$K_C$ 在索引 1 和 3 处有两行相同的行，都等于向量 $k_3 = (0, 0, 0, 1)$。值矩阵 $V$ 保持为单位矩阵，这意味着索引 1 处的密钥与明文标签 1（值 $v_1=e_1$）相关联，而索引 3 处的密钥与标签 3（值 $v_3=e_3$）相关联。\n查询 $Q_C$ 是通过用 $\\pi_C = [1, 3, 0, 4, 2]$ 排列这个新密钥矩阵 $K_C$ 的行来生成的。真实标签是 $Y_C = \\pi_C$。\n让我们分析查询 $q_0 = k_{C, \\pi_C(0)} = k_{C,1}$。由于 $K_C$ 的第 1 行是 $k_3$，这个查询是 $q_0 = k_3$。真实标签是 $y_0 = \\pi_C(0) = 1$。当我们为这个查询计算对 $K_C$ 的注意力得分时，点积 $q_0 \\cdot k_{C,j}$ 对于 $j=1$ 和 $j=3$ 将是最大且相等的，因为 $k_{C,1}$ 和 $k_{C,3}$ 都与 $q_0$ 相同。softmax 函数将为密钥 1 和 3 分配相等的高注意力权重。得到的输出向量将是值向量的和，主要由 $w_1 v_1 + w_3 v_3$ 主导，其中 $w_1=w_3$。这意味着输出向量在索引 1 和 3 处将有相等的较大分量。`arg\\max` 函数按照惯例（例如，在 NumPy 中），通过返回最大值的第一个索引来打破平局。因此，对 $q_0$ 的预测将是 1，这与真实值 $y_0=1$ 相符。\n现在考虑查询 $q_1 = k_{C, \\pi_C(1)} = k_{C,3}$。这个查询也是向量 $k_3$，与 $q_0$ 相同。预测过程是相同的，所以预测的标签将再次是 1。然而，真实标签是 $y_1 = \\pi_C(1) = 3$。这是一个解密错误。\n对于其他查询（$q_2, q_3, q_4$），它们与 $K_C$ 中的唯一密钥匹配，预期会被正确解密。总共，我们预期 5 个查询中会有 1 个错误，从而得到 4/5 = 0.8 的准确率。\n这个案例凸显了当密钥不唯一时，注意力机制无法区分它们，检索变得有歧义，其结果可能取决于任意的平局打破规则。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the toy cipher problem by implementing and testing a simplified\n    attention mechanism.\n    \"\"\"\n    N = 5\n    d_k = 4\n    d_v = 5\n\n    def softmax(x, axis=-1):\n        \"\"\"Numerically stable softmax function.\"\"\"\n        e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n        return e_x / np.sum(e_x, axis=axis, keepdims=True)\n\n    def attention_decrypt(Q, K, V):\n        \"\"\"\n        Performs decryption using the scaled dot-product attention mechanism.\n        Returns the predicted symbol indices.\n        \"\"\"\n        # 1. Compute similarity scores and scale them\n        scores = (Q @ K.T) / np.sqrt(d_k)\n        \n        # 2. Compute attention weights\n        attention_weights = softmax(scores, axis=1)\n        \n        # 3. Compute output vectors (weighted sum of values)\n        output = attention_weights @ V\n        \n        # 4. Predict symbols by taking the argmax\n        predictions = np.argmax(output, axis=1)\n        \n        return predictions\n\n    def calculate_accuracy(predictions, ground_truth):\n        \"\"\"Calculates the top-1 decryption accuracy.\"\"\"\n        return np.mean(predictions == ground_truth)\n\n    # --- Common Setup ---\n    # Define the base key matrix K\n    K = np.array([\n        [1.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0],\n        [1/np.sqrt(2), 1/np.sqrt(2), 0.0, 0.0]\n    ])\n\n    # Define the value matrix V as the identity matrix\n    V = np.identity(N)\n\n    results = []\n\n    # --- Test Case A: Happy path, exact matches ---\n    pi_A = np.array([2, 0, 4, 1, 3])\n    Q_A = K[pi_A]\n    Y_A = pi_A\n    \n    predictions_A = attention_decrypt(Q_A, K, V)\n    accuracy_A = calculate_accuracy(predictions_A, Y_A)\n    results.append(accuracy_A)\n\n    # --- Test Case B: Noisy queries, stability under perturbations ---\n    # Use a fixed seed for reproducibility\n    np.random.seed(42)\n    sigma = 0.30\n    \n    noise = np.random.normal(scale=sigma, size=Q_A.shape)\n    Q_B_noisy = Q_A + noise\n    \n    # Renormalize each query to unit norm\n    norms = np.linalg.norm(Q_B_noisy, axis=1, keepdims=True)\n    Q_B = Q_B_noisy / norms\n    \n    # Ground truth remains the same\n    Y_B = Y_A\n    \n    predictions_B = attention_decrypt(Q_B, K, V)\n    accuracy_B = calculate_accuracy(predictions_B, Y_B)\n    results.append(accuracy_B)\n\n    # --- Test Case C: Key collision, ambiguity in content addressing ---\n    # Create the colliding key set K_C\n    K_C = K.copy()\n    K_C[1] = K[3]  # k_1 is replaced by k_3\n    \n    pi_C = np.array([1, 3, 0, 4, 2])\n    \n    # Queries Q_C are permuted rows of K_C\n    Q_C = K_C[pi_C]\n    \n    # Ground truth labels\n    Y_C = pi_C\n    \n    predictions_C = attention_decrypt(Q_C, K_C, V)\n    accuracy_C = calculate_accuracy(predictions_C, Y_C)\n    results.append(accuracy_C)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "理解了注意力的核心原理后，下一步是探究它在Transformer架构中的两种主要应用方式：编码器堆栈和解码器堆栈。这两种结构的关键区别在于信息流动的方向——编码器允许双向流动，而解码器则通过因果掩码强制单向流动。本练习将通过一个精心设计的“回文检测”任务，让你从根本上理解这种差异所带来的计算能力的不同，并推导出一个模型能否解决该任务的充要条件。",
            "id": "3195539",
            "problem": "你将构建、分析并从算法上判定一个从根本上需要双向上下文的合成序列分类任务，并且你将比较一个编码器层堆栈和一个带有因果掩码的解码器层堆栈所能计算的内容。然后，你将实现一个程序，该程序在给定一组小的参数值测试套件的情况下，确定哪种架构可以在这些参数下解决该任务。所有数学实体必须用 LaTeX 书写。\n\n任务定义。考虑长度为奇数 $n \\in \\mathbb{N}$ 的二元序列，其在中间位置有一个特殊的查询标记。将序列写为\n$$\nx = \\big(a_{1}, a_{2}, \\dots, a_{m-1}, \\mathrm{[Q]}, b_{1}, b_{2}, \\dots, b_{m-1}\\big),\n$$\n其中 $n = 2m-1$，$m = \\frac{n+1}{2}$，并且对于 $k \\in \\{1,\\dots,m-1\\}$，每个 $a_{k}, b_{k} \\in \\{0,1\\}$。定义分类函数 $f:\\{0,1\\}^{n-1}\\to \\{0,1\\}$ 为\n$$\nf(x) = 1 \\;\\;\\text{当且仅当}\\;\\; \\forall d \\in \\{1,\\dots,m-1\\},\\; a_{d} = b_{d},\n$$\n否则 $f(x)=0$。换言之，在 $\\mathrm{[Q]}$ 左侧距离为 $d$ 的标记必须等于在 $\\mathrm{[Q]}$ 右侧距离为 $d$ 的标记；这是一个围绕 $\\mathrm{[Q]}$ 的中心回文检查。\n\n架构模型。考虑两种架构，它们由 $L \\in \\mathbb{N}$ 个相同的多头注意力（MHA）层构成，每层、每头都有一个宽度为 $w \\in \\mathbb{N}$ 的窗口化自注意力约束，其后是按位置的前馈网络。单层注意力的图抽象为：\n- 编码器堆栈：从每个位置 $i$，存在指向所有满足 $|i-j| \\le w$ 的位置 $j$ 的有向边（在窗口内是双向的）。\n- 带有因果掩码的解码器堆栈：从每个位置 $i$，存在指向所有满足 $0 \\le i-j \\le w$ 的位置 $j$ 的有向边（仅指向窗口内的左侧，包括自身）。\n\n信息传播模型。将每一层建模为沿着边进行一步消息传递。经过 $L$ 层，信息可以沿着由层连通性定义的有向图中的任何长度最多为 $L$ 的路径传播。对于此任务，决策必须在经过恰好 $L$ 层后，在中间的查询位置 $\\mathrm{[Q]}$ 处产生。\n\n你的目标。\n1. 从由注意力窗口引起的有向图可达性的基本原理出发，证明对于带有因果掩码的解码器，对于任何 $n>1$，无论 $w$ 和 $L$ 为何，$\\mathrm{[Q]}$ 处的标记都无法访问来自右半部分 $\\{b_{1},\\dots,b_{m-1}\\}$ 的任何信息。由此得出结论，当 $n>1$ 时，没有任何这种形式的解码器可以为所有输入精确计算 $f$。\n2. 基于相同的原理，推导出一个关于 $n$、$w$ 和 $L$ 的充要条件，在该条件下，编码器堆栈可以在 $\\mathrm{[Q]}$ 处精确计算 $f$。你的推导必须从有向图可达性的角度开始，并得出一个包含 $n$、$w$ 和 $L$ 的闭式条件，并由一个将证据汇集到 $\\mathrm{[Q]}$ 的构造性方案来证明其合理性。\n3. 设计一个关于 $n$、$w$ 和 $L$ 的决策规则，该规则返回两个布尔值：一个表示编码器堆栈是否能为所有长度为 $n$ 的输入在 $\\mathrm{[Q]}$ 处精确解决该任务，另一个表示带有因果掩码的解码器是否能做到。你的规则必须正确处理 $n=1$ 的退化情况。\n\n程序规范。实现一个完整的、可运行的程序，该程序对以下 $(n,w,L)$ 三元组的测试套件评估你的决策规则：\n- $(n,w,L) = (21,3,4)$,\n- $(n,w,L) = (17,4,2)$,\n- $(n,w,L) = (31,3,4)$,\n- $(n,w,L) = (1,1,1)$,\n- $(n,w,L) = (19,2,5)$.\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表。对于每个测试用例，输出一个整数代码 $c \\in \\{0,1,2,3\\}$，定义为\n$$\nc \\;=\\; 2\\cdot \\mathbf{1}\\{\\text{编码器可解}\\}\\;+\\; \\mathbf{1}\\{\\text{解码器可解}\\},\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数，如果语句为真则返回 $1$，否则返回 $0$。例如，$c=2$ 表示编码器可解而解码器不可解，$c=3$ 表示两者都可解，$c=0$ 表示两者都不可解，$c=1$ 表示解码器可解但编码器不可解。最终输出格式必须与上面列出的五个案例的 $[c_{1},c_{2},c_{3},c_{4},c_{5}]$ 完全一样。",
            "solution": "问题陈述经评估有效。这是一个适定的、有科学依据的、客观的问题，它探讨了理想化 Transformer 架构的计算极限。该问题基于一个标准且有用的抽象，即将神经网络中的信息流视为图可达性，这是深度学习模型理论分析的核心概念。所有定义和约束都是清晰且自洽的。\n\n我们现在按要求分三部分进行解答。\n\n首先，我们对问题的信息流模型进行形式化。序列长度为 $n$，位置索引从 $1$ 到 $n$。中心查询标记 $\\mathrm{[Q]}$ 位于位置 $m = (n+1)/2$。任务是验证对于所有 $d \\in \\{1, \\dots, m-1\\}$，$a_d = b_d$ 是否成立，其中 $a_d$ 位于位置 $m-d$，$b_d$ 位于位置 $m+d$。决策在位置 $m$ 做出。遵循 Transformer 机制的标准解释，一个层在位置 $i$ 的计算会从一组位置 $\\{j\\}$ 收集信息。因此，问题描述中的“从每个位置 $i$ 到所有位置 $j$ 的有向边”被解释为位置 $i$ 所关注的一组位置 $\\{j\\}$，这意味着信息从 $j$ 流向 $i$。因此，信息流图的有向边是 $(j, i)$。\n\n### 1. 对带有因果掩码的解码器堆栈的分析\n\n解码器架构由一个因果注意力掩码定义。在我们的形式模型下，信息流图中存在一条边 $(j, i)$ 当且仅当位置 $i$ 关注位置 $j$。该规则由 $0 \\le i-j \\le w$ 给出。这意味着 $i-w \\le j \\le i$。\n\n这个规则的一个关键属性是，对于任何边 $(j, i)$，源索引 $j$ 必须小于或等于目标索引 $i$（即 $j \\le i$）。现在，考虑一条信息传播路径，它是图中一系列连接的节点 $(p_0, p_1, \\dots, p_k)$，其中 $p_0$ 是信息的原始来源，$p_k$ 是最终目的地。要使这是一条有效路径，对于每个 $s \\in \\{0, \\dots, k-1\\}$，必须存在一条边 $(p_s, p_{s+1})$。应用边规则，这要求在所有步骤 $s$ 中都有 $p_s \\le p_{s+1}$。因此，解码器信息流图中的任何路径都必须具有非递减的位置索引：$p_0 \\le p_1 \\le \\dots \\le p_k$。\n\n分类任务要求检查 $a_d = b_d$，其中 $b_d$ 是位置 $m+d$ 处的标记。对于任何 $d \\in \\{1, \\dots, m-1\\}$，位置 $m+d$ 严格大于 $m$。为了在位置 $m$ 做出决策，关于标记 $b_d$ 的信息必须从其源位置 $j_{src} = m+d$ 传播到目标位置 $i_{dst} = m$。\n\n这将要求存在一条从 $p_0 = m+d$ 到 $p_k = m$ 的路径。然而，如上所证，任何这样的路径都必须满足 $p_0 \\le p_k$，这意味着 $m+d \\le m$。对于任何 $d \\ge 1$，这个不等式都是错误的。因此，不存在从序列右半部分（大于 $m$ 的位置）的任何位置到中心查询位置 $m$ 的路径。\n\n由于 $f(x)$ 的值依赖于标记 $\\{b_1, \\dots, b_{m-1}\\}$，而来自这些标记的信息无法到达位置 $m$，因此带有因果掩码的解码器堆栈无法为任何 $n > 1$ 的输入序列计算 $f(x)$。\n\n退化情况是 $n=1$。此时，$m = (1+1)/2 = 1$，$d$ 的范围是 $\\{1, \\dots, m-1\\} = \\emptyset$。条件 $\\forall d \\in \\emptyset, a_d=b_d$ 是虚真的。因此，对于 $n=1$，$f(x)=1$ 对所有输入都成立。解码器可以轻易地学会输出这个恒定值。\n\n结论：带有因果掩码的解码器当且仅当 $n=1$ 时可以解决该任务。\n\n### 2. 对编码器堆栈的分析\n\n编码器架构具有双向注意力。如果 $|i-j| \\le w$，则存在一条边 $(j,i)$。这个条件是对称的，意味着边 $(i,j)$ 也存在。信息可以在窗口 $w$ 内的任意两个位置之间双向流动。\n\n为了在位置 $m$ 计算 $f(x)$，模型必须能够访问序列中所有其他位置的信息。该模型由 $L$ 层组成。在一层中，位置 $j$ 的信息可以传播到区间 $[j-w, j+w]$ 中的任何位置 $i$。经过 $L$ 层后，来自初始位置 $j$ 的信息已经传播覆盖了区间 $[j-Lw, j+Lw]$。这就是有效的感受野。\n\n为了使位置 $m$ 的决策能够充分知情，来自序列 $\\{1, \\dots, n\\}$ 中每个位置的信息都必须能够在 $L$ 层内到达 $m$。这意味着对于任何位置 $j \\in \\{1, \\dots, n\\}$，$m$ 必须在 $L$ 层后位于 $j$ 的感受野内。也就是说，$m \\in [j-Lw, j+Lw]$，这等价于 $|m-j| \\le Lw$。\n\n我们必须确保这个条件对所有 $j$ 都成立。最严格的要求来自于离中心 $m$ 最远的位置。这些是序列的端点，$j=1$ 和 $j=n$。\n从中心 $m = (n+1)/2$ 到端点的距离是：\n- 到位置 $1$：$|m-1| = |\\frac{n+1}{2} - 1| = |\\frac{n-1}{2}| = \\frac{n-1}{2}$。\n- 到位置 $n$：$|m-n| = |\\frac{n+1}{2} - n| = |\\frac{1-n}{2}| = \\frac{n-1}{2}$。\n\n由于这些是最大距离，条件简化为确保信息能够传播这么远。信息在 $L$ 层中可以传播的总距离是 $Lw$。因此，所有信息能够到达中心的充要条件是：\n$$\nLw \\ge \\frac{n-1}{2}\n$$\n如果满足此条件，来自所有配对 $(a_d, b_d)$ 的信息都可以在位置 $m$ 处汇集，从而允许一个足够强大的模型（我们假设是这样）计算函数 $f(x)$。如果不满足该条件，则至少有一个标记（在端点处）的信息无法到达中心，使得对于某些输入无法进行计算。注意，由于 $n$ 是奇数，$n-1$ 是偶数，因此 $(n-1)/2$ 总是一个整数。\n\n对于特殊情况 $n=1$，条件变为 $Lw \\ge (1-1)/2$，即 $Lw \\ge 0$。由于 $L \\in \\mathbb{N}$ 和 $w \\in \\mathbb{N}$，我们有 $L \\ge 1$ 和 $w \\ge 1$，所以这总是成立的。这与 $n=1$ 时任务是平凡的事实相符。\n\n结论：编码器堆栈当且仅当 $Lw \\ge (n-1)/2$ 时可以解决该任务。\n\n### 3. 最终决策规则\n\n基于以上分析，我们可以为给定的三元组 $(n, w, L)$ 制定一个决策规则。\n\n- **编码器可解性**：编码器堆栈能够解决任务，当且仅当累积信息传播距离 $Lw$足以覆盖从序列端点到中心的距离。\n  $$ \\mathbf{1}\\{\\text{编码器可解}\\} = \\mathbf{1}\\left\\{ Lw \\ge \\frac{n-1}{2} \\right\\} $$\n\n- **解码器可解性**：带有因果掩码的解码器堆栈能够解决任务，当且仅当序列长度为 $n=1$，此时分类任务是平凡的。\n  $$ \\mathbf{1}\\{\\text{解码器可解}\\} = \\mathbf{1}\\{ n = 1 \\} $$\n\n每个测试用例的整数代码 $c$ 按规定计算：\n$$ c = 2\\cdot \\mathbf{1}\\{\\text{编码器可解}\\} + \\mathbf{1}\\{\\text{解码器可解}\\} $$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the transformer architecture problem by applying the derived decision rules\n    to a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (21, 3, 4),  # (n, w, L) triplet 1\n        (17, 4, 2),  # (n, w, L) triplet 2\n        (31, 3, 4),  # (n, w, L) triplet 3\n        (1, 1, 1),   # (n, w, L) triplet 4\n        (19, 2, 5),  # (n, w, L) triplet 5\n    ]\n\n    results = []\n    for n, w, l in test_cases:\n        # A. Decision rule for the decoder stack with a causal mask.\n        # The task is a center-palindrome check.\n        # A decoder with a causal mask prevents information flow from the future (right side)\n        # to the past (left side or center).\n        # When n > 1, the decision at the center position m = (n+1)/2 cannot access\n        # information from any position j > m.\n        # Thus, the comparison required by the task is impossible.\n        # The only exception is the trivial case n=1, where the palindrome condition is\n        # vacuously true, and the model only needs to output a constant 1.\n        decoder_can_solve = (n == 1)\n\n        # B. Decision rule for the encoder stack.\n        # An encoder has bidirectional attention. Information can flow from any position j to\n        # any position i, provided enough layers L and a large enough window w.\n        # The key condition is that information from the sequence endpoints (positions 1 and n)\n        # must be able to reach the center position m = (n+1)/2.\n        # The distance from either endpoint to the center is (n-1)/2.\n        # In L layers, with an attention window of width w, information can propagate\n        # a maximum distance of L * w.\n        # The task is solvable if and only if this reach is sufficient to cover the distance.\n        # This condition also correctly handles the n=1 case (l * w >= 0, which is always true).\n        encoder_can_solve = (l * w >= (n - 1) / 2)\n\n        # C. Calculate the integer code c as per the problem specification.\n        # c = 2 * I{encoder can solve} + 1 * I{decoder can solve}\n        # where I{.} is the indicator function (1 if true, 0 if false).\n        c = 2 * int(encoder_can_solve) + 1 * int(decoder_can_solve)\n        results.append(c)\n\n    # Final print statement in the exact required format: [c1,c2,c3,c4,c5]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "我们已经学习了注意力机制和堆栈结构，但还有一个基本问题：Transformer本身不具备序列顺序感知能力，那么它是如何处理位置信息的呢？答案是位置编码，但它并非万能药，尤其是在处理比训练时更长的序列时会面临“外推失败”的挑战。在这个高级实践中，你将通过分析正弦位置编码的“混叠”现象，深入探究Transformer在长度泛化方面的内在局限性，并量化地预测模型何时会因为位置信息的模糊性而失效。",
            "id": "3195577",
            "problem": "考虑一个 Transformer 编码器-解码器堆栈，其注意力机制为缩放点积注意力。缩放点积注意力的基本定义是 $A(Q,K,V)=\\mathrm{softmax}\\left(\\frac{QK^{\\top}}{\\sqrt{d_k}}\\right)V$，其中 $Q$ 表示查询矩阵，$K$ 表示键矩阵，$V$ 表示值矩阵，$d_k$ 表示键维度。在词元嵌入被线性投影到 $Q$ 和 $K$ 之前，会向其添加位置编码。在本问题中，您将构建一个关于序列长度 $n$ 的理想化课程，分析随着所需注意力范围的增长而产生的训练动态，并推导因位置编码的限制导致外推（至更长序列）失败的边界。\n\n假设一个简化的单头场景，其中内容对 $Q$ 和 $K$ 的贡献是平衡的，并且在决定远程注意力时不会主导位置分量，因此位置之间的相似性由位置嵌入点积决定。设位置编码为正弦形式，具有 $2M$ 个维度，组织为 $M$ 对 $\\big(\\sin(w_i n),\\cos(w_i n)\\big)$，$i\\in\\{0,1,\\dots,M-1\\}$，其中 $n$ 是位置索引，用于三角函数参数时以弧度为单位。对于两个位置 $n$ 和 $n+d$，定义一个归一化位置相似度 $s(d)$，即 $M$ 个余弦项的平均值，并定义一个混叠阈值 $\\tau\\in(0,1)$，使得如果对于某个 $d\\ne 0$ 有 $s(d)\\ge \\tau$，则在尝试泛化到训练范围之外时，注意力机制有很大风险混淆位置 $n$ 和 $n+d$。\n\n您需要编写一个完整、可运行的程序，该程序：\n1. 构建一个由递增的最大注意力范围（序列长度）$\\{n_1,n_2,\\dots\\}$ 组成的课程。\n2. 对于给定的正弦频率方案 $w_i=w_0 r^i$（其中基频为 $w_0$，几何比率为 $r$），计算最小的正整数混叠距离 $d_{\\mathrm{alias}}$（如果在有界搜索中存在），使得归一化位置相似度 $s(d)$ 至少为 $\\tau$。\n3. 确定由于位置编码模糊性导致外推失败的第一个训练阶段索引（按课程顺序）。失败意味着该阶段的最大范围 $n_j$ 大于或等于 $d_{\\mathrm{alias}}$。如果在提供的课程中不存在这样的阶段，则阶段索引返回 $-1$。\n4. 确定对指定测试长度 $n_{\\mathrm{test}}$ 的泛化是否因 $n_{\\mathrm{test}}\\ge d_{\\mathrm{alias}}$ 而失败。\n5. 生成单行输出，包含每个测试用例的结果。每个结果是一个列表 $[d_{\\mathrm{alias}},j_{\\mathrm{fail}},b_{\\mathrm{test}}]$，其中 $d_{\\mathrm{alias}}$ 是一个整数（如果在搜索边界内未找到混叠，则使用 $-1$），$j_{\\mathrm{fail}}$ 是一个从 1 开始计数的整数阶段索引（如果没有，则使用 $-1$），$b_{\\mathrm{test}}$ 是一个布尔值，指示测试长度是否失败。该行必须是这些逐例列表的逗号分隔列表，并用方括号括起来，其中角度以弧度指定和计算。\n\n使用以下测试套件。对于每个用例，角度以弧度为单位，位置 $n$ 为离散整数。对于每个用例，在整数范围 $d\\in\\{1,2,\\dots,\\max(\\{n_1,n_2,\\dots\\}\\cup\\{n_{\\mathrm{test}}\\})\\}$ 内搜索 $d_{\\mathrm{alias}}$：\n- 用例 1：$M=3$，$w_0=\\pi/6$，$r=2$，$\\tau=0.95$，课程 $\\{8,10,12\\}$，$n_{\\mathrm{test}}=20$。\n- 用例 2：$M=1$，$w_0=\\pi/4$，$r=1$，$\\tau=0.999$，课程 $\\{7,8\\}$，$n_{\\mathrm{test}}=9$。\n- 用例 3：$M=4$，$w_0=\\pi/10$，$r=3$，$\\tau=0.98$，课程 $\\{5,10,15\\}$，$n_{\\mathrm{test}}=18$。\n\n您必须遵循以下解释：\n- 距离 $d$ 处的位置相似度为 $s(d)=\\frac{1}{M}\\sum_{i=0}^{M-1}\\cos(w_i d)$，其中余弦函数的所有参数都以弧度为单位。\n- 混叠距离 $d_{\\mathrm{alias}}$ 是指定搜索范围内的最小正整数 $d$，使得 $s(d)\\ge \\tau$；如果不存在，则使用 $-1$。\n- 如果 $n_j\\ge d_{\\mathrm{alias}}$，则外推在课程阶段 $j$ 失败，此时报告最小的此类 $j$，否则报告 $-1$。\n- 如果 $n_{\\mathrm{test}}\\ge d_{\\mathrm{alias}}$，则对测试长度的外推失败，否则不失败。\n\n最终输出格式要求：\n您的程序应生成单行输出，其中包含三个用例的结果，格式为方括号括起来的逗号分隔列表，每个用例的结果本身是一个列表，格式为 $[d_{\\mathrm{alias}},j_{\\mathrm{fail}},b_{\\mathrm{test}}]$，不含空格（例如，$[[12,3,True],[8,2,True],[20,-1,False]]$）。不允许外部输入，并且在整个过程中角度必须以弧度处理。",
            "solution": "该问题要求分析 Transformer 模型中正弦位置编码的局限性，特别是在其泛化到训练期间未见过的序列长度方面的能力。问题的核心是确定位置混叠（不同相对位置之间的混淆）何时发生并导致外推失败。我们将通过首先建立位置相似性的数学框架，然后定义混叠条件，最后将该框架应用于给定的测试用例来解决此问题。\n\n### 位置相似性原理\n\nTransformer 架构的自注意力机制本身缺乏序列感知能力，因此它依赖位置编码来提供关于词元顺序的信息。该问题指定了一种正弦位置编码方案。对于由整数 $n$ 索引的给定位置，其位置编码是一个 $P_n \\in \\mathbb{R}^{2M}$ 的向量，由 $M$ 对具有不同频率的正弦和余弦函数组成：\n$$P_n = \\left( \\sin(w_0 n), \\cos(w_0 n), \\sin(w_1 n), \\cos(w_1 n), \\dots, \\sin(w_{M-1} n), \\cos(w_{M-1} n) \\right)$$\n其中频率 $\\{w_i\\}$ 构成一个几何级数 $w_i = w_0 r^i$，$i \\in \\{0, 1, \\dots, M-1\\}$。三角函数的参数以弧度为单位。\n\n自注意力机制基于词元的查询向量和键向量的点积来计算它们之间的相似度分数。根据问题中的简化假设，即对于远程交互，位置信息主导该分数，两个位置 $n$ 和 $n+d$ 之间的相似性可以近似为其位置编码向量的点积 $P_n \\cdot P_{n+d}$。\n\n让我们计算这个点积：\n$$P_n \\cdot P_{n+d} = \\sum_{i=0}^{M-1} \\left[ \\sin(w_i n) \\sin(w_i (n+d)) + \\cos(w_i n) \\cos(w_i (n+d)) \\right]$$\n使用三角恒等式 $\\cos(A-B) = \\cos A \\cos B + \\sin A \\sin B$，我们设 $A = w_i(n+d)$ 和 $B = w_i n$，则求和内的项简化为：\n$$\\cos(w_i n) \\cos(w_i (n+d)) + \\sin(w_i n) \\sin(w_i (n+d)) = \\cos(w_i(n+d) - w_i n) = \\cos(w_i d)$$\n因此，点积变为：\n$$P_n \\cdot P_{n+d} = \\sum_{i=0}^{M-1} \\cos(w_i d)$$\n此推导揭示的一个关键属性是，两个位置之间的相似性仅取决于它们的相对距离 $d$，而不取决于它们的绝对位置 $n$。问题通过对 $M$ 个频率分量的和进行平均来定义归一化位置相似度 $s(d)$：\n$$s(d) = \\frac{1}{M} \\sum_{i=0}^{M-1} \\cos(w_i d)$$\n\n### 位置混叠与外推失败\n\n如果两个相对位置 $d_1$ 和 $d_2$ 对应的相似度特征（由 $s(d_1)$ 和 $s(d_2)$ 决定）是不同的，那么模型就能够区分它们。对于零偏移，$s(0)$ 的相似度始终是最大的：$s(0) = \\frac{1}{M} \\sum_{i=0}^{M-1} \\cos(0) = 1$。\n\n当对于某个非零距离 $d \\neq 0$，其相似度 $s(d)$ 非常接近 $1$ 时，就会发生位置混叠。这意味着模型将相对位置 $d$ 感知为与相对位置 $0$ 几乎相同，从而产生歧义。当对于所有频率 $w_i$，项 $\\cos(w_i d)$ 都接近 $1$ 时，就会发生这种情况，而这又发生在每个 $w_i d$ 都接近 $\\pi$ 的偶数倍时，即对于某些整数 $k_i$ 有 $w_i d \\approx 2k_i \\pi$。\n\n问题通过引入一个混叠阈值 $\\tau \\in (0,1)$ 来形式化这一点。混叠距离 $d_{\\mathrm{alias}}$ 定义为使得相似度达到或超过此阈值的最小正整数 $d$：\n$$d_{\\mathrm{alias}} = \\min \\{ d \\in \\mathbb{Z}^+ \\mid s(d) \\ge \\tau \\}$$\n\n外推失败随后与这个混叠距离联系在一起。\n1.  **训练课程失败**：模型在逐渐增大的序列长度上进行训练，这由一个课程 $\\{n_1, n_2, \\dots\\}$ 定义。如果课程中的阶段 $j$ 使用的序列长度 $n_j$ 满足 $n_j \\ge d_{\\mathrm{alias}}$，这意味着训练数据本身包含了被模糊编码的位置。这可能阻碍鲁棒位置表示的学习。第一个发生这种情况的阶段 $j_{\\mathrm{fail}}$ 标志着这个问题的开始。如果在搜索边界内没有找到 $d_{\\mathrm{alias}}$，或者所有的 $n_j  d_{\\mathrm{alias}}$，则课程中没有发生失败，我们设 $j_{\\mathrm{fail}} = -1$。\n2.  **测试泛化失败**：当模型在长度为 $n_{\\mathrm{test}}$ 的序列上进行测试时，它必须泛化其学到的位置理解。如果 $n_{\\mathrm{test}} \\ge d_{\\mathrm{alias}}$，模型需要在包含混叠距离的相对距离范围内操作，这不可避免地导致混淆。布尔值 $b_{\\mathrm{test}}$ 捕捉此失败条件是否满足。如果未找到 $d_{\\mathrm{alias}}$，则此失败模式不会发生。\n\n### 算法解决方案\n\n对于每个由参数 $(M, w_0, r, \\tau, \\text{curriculum}, n_{\\mathrm{test}})$ 指定的测试用例，我们执行以下过程：\n\n1.  **确定搜索边界**：在整数范围 $d \\in \\{1, 2, \\dots, d_{\\max}\\}$ 内搜索 $d_{\\mathrm{alias}}$，其中 $d_{\\max} = \\max(\\max(\\text{curriculum}), n_{\\mathrm{test}})$。\n\n2.  **计算混叠距离 ($d_{\\mathrm{alias}}$)**：\n    - 初始化 $d_{\\mathrm{alias}} = -1$。\n    - 预计算频率 $w_i = w_0 r^i$，$i \\in \\{0, \\dots, M-1\\}$。\n    - 从 $1$ 到 $d_{\\max}$ 迭代 $d$。\n    - 对于每个 $d$，计算 $s(d) = \\frac{1}{M}\\sum_{i=0}^{M-1} \\cos(w_i d)$。\n    - 如果 $s(d) \\ge \\tau$，则设置 $d_{\\mathrm{alias}} = d$ 并终止搜索，因为我们已经找到了最小的这样的 $d$。\n\n3.  **找到课程失败阶段 ($j_{\\mathrm{fail}}$)**：\n    - 初始化 $j_{\\mathrm{fail}} = -1$。\n    - 如果找到了一个有效的 $d_{\\mathrm{alias}}$（即 $d_{\\mathrm{alias}} \\neq -1$）：\n        - 使用从 1 开始的索引 $j$ 遍历课程长度 $\\{n_j\\}$。\n        - 如果 $n_j \\ge d_{\\mathrm{alias}}$，则设置 $j_{\\mathrm{fail}} = j$ 并终止搜索，因为我们已经找到了第一个失败的阶段。\n\n4.  **确定测试失败 ($b_{\\mathrm{test}}$)**：\n    - 初始化 $b_{\\mathrm{test}} = \\text{False}$。\n    - 如果找到了一个有效的 $d_{\\mathrm{alias}}$：\n        - 如果 $n_{\\mathrm{test}} \\ge d_{\\mathrm{alias}}$，则设置 $b_{\\mathrm{test}} = \\text{True}$。\n\n5.  **编译结果**：该测试用例的最终结果是列表 $[d_{\\mathrm{alias}}, j_{\\mathrm{fail}}, b_{\\mathrm{test}}]$。对所有测试用例重复此过程。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the positional encoding aliasing problem for a given set of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: M=3, w0=pi/6, r=2, tau=0.95, curriculum={8,10,12}, n_test=20\n        {\n            'M': 3, 'w0': np.pi / 6, 'r': 2, 'tau': 0.95,\n            'curriculum': [8, 10, 12], 'n_test': 20\n        },\n        # Case 2: M=1, w0=pi/4, r=1, tau=0.999, curriculum={7,8}, n_test=9\n        {\n            'M': 1, 'w0': np.pi / 4, 'r': 1, 'tau': 0.999,\n            'curriculum': [7, 8], 'n_test': 9\n        },\n        # Case 3: M=4, w0=pi/10, r=3, tau=0.98, curriculum={5,10,15}, n_test=18\n        {\n            'M': 4, 'w0': np.pi / 10, 'r': 3, 'tau': 0.98,\n            'curriculum': [5, 10, 15], 'n_test': 18\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        M = case['M']\n        w0 = case['w0']\n        r = case['r']\n        tau = case['tau']\n        curriculum = case['curriculum']\n        n_test = case['n_test']\n        \n        # 1. Determine the search bound for d\n        d_max = max(max(curriculum), n_test)\n\n        # 2. Compute the minimal positive integer alias distance d_alias\n        d_alias = -1\n        \n        # Pre-calculate frequencies w_i\n        i_s = np.arange(M)\n        w_is = w0 * np.power(r, i_s)\n\n        for d in range(1, d_max + 1):\n            # Calculate positional similarity s(d)\n            # s(d) = (1/M) * sum(cos(w_i * d)) for i=0 to M-1\n            s_d = np.mean(np.cos(w_is * d))\n            \n            if s_d >= tau:\n                d_alias = d\n                break # Found the minimal d, so we can stop searching\n\n        # 3. Determine the first failing training stage index j_fail\n        j_fail = -1\n        if d_alias != -1:\n            for j, n_j in enumerate(curriculum, 1):\n                if n_j >= d_alias:\n                    j_fail = j\n                    break\n\n        # 4. Determine if generalization to n_test fails (b_test)\n        b_test = False\n        if d_alias != -1:\n            if n_test >= d_alias:\n                b_test = True\n\n        all_results.append([d_alias, j_fail, b_test])\n\n    # Format the final output string\n    case_results_str = []\n    for d_alias, j_fail, b_test in all_results:\n        case_results_str.append(f\"[{d_alias},{j_fail},{'True' if b_test else 'False'}]\")\n\n    print(f\"[{','.join(case_results_str)}]\")\n\nsolve()\n```"
        }
    ]
}