{
    "hands_on_practices": [
        {
            "introduction": "Dynamic masking, where different tokens are chosen for masking in each training epoch, is a cornerstone of modern pre-training strategies. This exercise  challenges you to analyze this process through the lens of probability theory. By deriving the expected number of times a single token is used for a gradient update and the probability it is masked at least once, you will gain a quantitative and deeper understanding of how the masking rate ($p$) and training duration ($E$) collectively determine the learning signal's distribution across the entire dataset.",
            "id": "3164748",
            "problem": "A Transformer encoder is pre-trained using Masked Language Modeling (MLM), where, during each pass over the data, every token position independently becomes a prediction target with probability $p$, regardless of whether it is replaced by a special mask symbol, a random token, or left unchanged. The training corpus contains a fixed set of token positions, and each epoch iterates over every token position exactly once. Masking is dynamically re-sampled at each epoch so that the selection decision for a given token position in one epoch is independent of its selection in any other epoch. Training runs for $E$ epochs. Stochastic Gradient Descent (SGD) is used, but for this problem, focus only on per-token contributions: each time a token position is selected as a prediction target, it contributes a single loss term and thus one gradient-contributing event for that token position during that epoch.\n\nFor a fixed token position $i$, define the random variable $S$ as the total number of epochs in which this position is selected as a prediction target. Using only the axioms of probability for independent Bernoulli trials and linearity of expectation as the fundamental base, derive, in closed form, the following quantities in terms of $E$ and $p$:\n- The expected number of gradient-contributing events per token position, $\\mathbb{E}[S]$.\n- The coverage probability that the token position is selected at least once over the $E$ epochs, $\\Pr(S \\geq 1)$.\n\nExpress your final answer as a single row vector $\\begin{pmatrix} \\mathbb{E}[S] & \\Pr(S \\geq 1) \\end{pmatrix}$. No rounding is required, and no physical units are involved.",
            "solution": "The problem statement is evaluated for validity before attempting a solution.\n\n### Step 1: Extract Givens\n- A Transformer encoder is pre-trained using Masked Language Modeling (MLM).\n- During each pass (epoch), every token position independently becomes a prediction target with probability $p$.\n- The selection decision for a given token position in one epoch is independent of its selection in any other epoch.\n- The total number of epochs for training is $E$.\n- Each time a token position is selected as a prediction target, it contributes a single gradient-contributing event.\n- For a fixed token position $i$, the random variable $S$ is defined as the total number of epochs in which this position is selected as a prediction target.\n- The derivation must use only the axioms of probability for independent Bernoulli trials and linearity of expectation.\n- The required quantities to be derived are the expected number of gradient-contributing events per token position, $\\mathbb{E}[S]$, and the coverage probability that the token position is selected at least once, $\\Pr(S \\geq 1)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n- **Scientifically Grounded:** The problem describes a simplified but fundamentally correct probabilistic model of the Masked Language Modeling (MLM) pre-training objective, a cornerstone of modern natural language processing models like BERT. The use of Bernoulli trials to model the independent masking events is standard and mathematically sound.\n- **Well-Posed:** The problem is clearly defined with all necessary parameters ($E$, $p$) and explicit definitions of the random variable $S$ and the target quantities ($\\mathbb{E}[S]$, $\\Pr(S \\geq 1)$). The conditions lead to a unique and meaningful solution.\n- **Objective:** The language is formal, precise, and devoid of any subjective or ambiguous terminology.\n- **Flaw Checklist:**\n    1.  **Scientific or Factual Unsoundness:** None. The setup is a valid abstraction of a real-world machine learning process.\n    2.  **Non-Formalizable or Irrelevant:** The problem is entirely formalizable within probability theory and is directly relevant to the field of deep learning.\n    3.  **Incomplete or Contradictory Setup:** The problem is self-contained and consistent. The independence of events across epochs is explicitly stated.\n    4.  **Unrealistic or Infeasible:** The scenario is a standard theoretical model used in the analysis of such algorithms. It is not unrealistic.\n    5.  **Ill-Posed or Poorly Structured:** The problem is well-structured and leads to a unique solution.\n    6.  **Pseudo-Profound, Trivial, or Tautological:** The problem requires the correct application of fundamental principles of probability theory (linearity of expectation, properties of independent events) and is a non-trivial exercise in formal derivation.\n    7.  **Outside Scientific Verifiability:** The derivation is mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid** and well-posed. A solution will be derived.\n\n### Derivation\n\nLet us consider a single, fixed token position. The training process runs for $E$ epochs.\n\nFirst, we define a set of indicator random variables, $X_j$, for each epoch $j \\in \\{1, 2, \\dots, E\\}$.\nLet $X_j = 1$ if the token position is selected as a prediction target in epoch $j$, and $X_j = 0$ otherwise.\n\nAccording to the problem statement, the selection in any given epoch is an independent event with probability $p$. Therefore, each $X_j$ is an independent Bernoulli random variable with parameter $p$. The probability mass function for each $X_j$ is:\n$$ \\Pr(X_j = 1) = p $$\n$$ \\Pr(X_j = 0) = 1 - p $$\n\nThe random variable $S$ represents the total number of epochs in which the position is selected. This is the sum of the indicator variables over all epochs:\n$$ S = \\sum_{j=1}^{E} X_j $$\n\n**1. Derivation of the Expected Value $\\mathbb{E}[S]$**\n\nWe need to find the expected total number of gradient-contributing events, which is $\\mathbb{E}[S]$. The problem requires using the principle of linearity of expectation.\n\nThe expectation of a single Bernoulli random variable $X_j$ is:\n$$ \\mathbb{E}[X_j] = 1 \\cdot \\Pr(X_j=1) + 0 \\cdot \\Pr(X_j=0) = 1 \\cdot p + 0 \\cdot (1-p) = p $$\n\nUsing the linearity of expectation, which states that the expectation of a sum of random variables is the sum of their individual expectations, we have:\n$$ \\mathbb{E}[S] = \\mathbb{E}\\left[\\sum_{j=1}^{E} X_j\\right] = \\sum_{j=1}^{E} \\mathbb{E}[X_j] $$\n\nSince $\\mathbb{E}[X_j] = p$ for every epoch $j$, the sum becomes:\n$$ \\mathbb{E}[S] = \\sum_{j=1}^{E} p = Ep $$\n\n**2. Derivation of the Coverage Probability $\\Pr(S \\geq 1)$**\n\nWe need to find the probability that the token position is selected at least once over the $E$ epochs. This is the probability $\\Pr(S \\geq 1)$.\n\nIt is more direct to calculate this probability using the complement rule. The event \"$S \\geq 1$\" (the token is selected at least once) is the complement of the event \"$S = 0$\" (the token is never selected).\n$$ \\Pr(S \\geq 1) = 1 - \\Pr(S = 0) $$\n\nThe event \"$S=0$\" occurs if and only if the token is not selected in any of the $E$ epochs. This means $X_1=0$ AND $X_2=0$ AND ... AND $X_E=0$.\n$$ \\Pr(S=0) = \\Pr(X_1=0, X_2=0, \\dots, X_E=0) $$\n\nThe problem states that selection decisions across epochs are independent. Therefore, the probability of the joint event is the product of the probabilities of the individual events:\n$$ \\Pr(S=0) = \\prod_{j=1}^{E} \\Pr(X_j=0) $$\n\nFor each epoch $j$, the probability of the token not being selected is $\\Pr(X_j=0) = 1-p$.\nSubstituting this into the product, we get:\n$$ \\Pr(S=0) = \\prod_{j=1}^{E} (1-p) = (1-p)^E $$\n\nFinally, substituting this result back into the complement rule equation gives the coverage probability:\n$$ \\Pr(S \\geq 1) = 1 - (1-p)^E $$\n\nThe derived quantities are $\\mathbb{E}[S] = Ep$ and $\\Pr(S \\geq 1) = 1 - (1-p)^E$. These are expressed in closed form in terms of $E$ and $p$ as required.",
            "answer": "$$\\boxed{\\begin{pmatrix} Ep & 1 - (1-p)^{E} \\end{{pmatrix}}$$"
        },
        {
            "introduction": "The engine of learning in neural networks is backpropagation, a process driven by the gradient of the loss function with respect to the model's parameters. This exercise  guides you through the fundamental calculus of deriving the gradient for the Masked Language Modeling (MLM) objective, starting from the cross-entropy loss and the softmax function. By working through the chain rule to find the update signal for different masking strategies, you will build an intuition for how architectural choices, such as using a shared versus a span-specific mask embedding, directly shape the learning dynamics of a Transformer.",
            "id": "3164800",
            "problem": "Consider a transformer-based sequence model trained with Masked Language Modeling (MLM), where Masked Language Modeling (MLM) is defined as minimizing the negative log-likelihood of the true tokens at masked positions. Let the vocabulary size be $V$, the embedding dimension be $d$, and the output classification layer be a linear map $U \\in \\mathbb{R}^{V \\times d}$ followed by a softmax. For a given training example, suppose there are $K$ disjoint masked spans indexed by $k \\in \\{1,\\dots,K\\}$, with the set of masked positions in span $k$ denoted $S^{(k)}$.\n\nFor each masked position $i \\in S^{(k)}$, the transformer produces a hidden state $h_{i} \\in \\mathbb{R}^{d}$ which is fed to the output head to produce logits $z_{i} = U h_{i} \\in \\mathbb{R}^{V}$ and probabilities $p_{i} = \\mathrm{softmax}(z_{i}) \\in \\mathbb{R}^{V}$. Let the ground-truth index at position $i$ be $y_{i} \\in \\{1,\\dots,V\\}$, and let $e_{y_{i}} \\in \\mathbb{R}^{V}$ be the one-hot vector for $y_{i}$. Define the per-example MLM loss as\n$$\n\\mathcal{L}_{\\mathrm{MLM}} = \\sum_{k=1}^{K} \\sum_{i \\in S^{(k)}} \\left( - \\ln p_{i}[y_{i}] \\right).\n$$\n\nAssume the following first-order local linearization of the transformer around the masked inputs:\n- In the shared mask token setting, a single global parameter vector $m \\in \\mathbb{R}^{d}$ is used as the input embedding for every masked token in every span, and the hidden states satisfy\n$$\nh_{i} = A_{i} m + b_{i},\n$$\nfor each masked position $i$, where $A_{i} \\in \\mathbb{R}^{d \\times d}$ and $b_{i} \\in \\mathbb{R}^{d}$ do not depend on $m$.\n\n- In the learned span-level mask embeddings setting, each span $k$ has its own parameter vector $s^{(k)} \\in \\mathbb{R}^{d}$ used as the input embedding for every masked token in that span, and the hidden states satisfy\n$$\nh_{i} = A_{i} s^{(k)} + b_{i},\n$$\nfor each $i \\in S^{(k)}$, with the same $A_{i}$ and $b_{i}$ as above that do not depend on $s^{(k)}$.\n\nStarting from core definitions of the softmax function and cross-entropy, and using only standard calculus (including the chain rule), derive closed-form analytic expressions for the gradient of $\\mathcal{L}_{\\mathrm{MLM}}$ with respect to:\n- the shared mask token $m$;\n- the learned span-level mask embedding $s^{(k)}$ for an arbitrary fixed span index $k$.\n\nExpress your final answer as a single row matrix whose first entry is $\\nabla_{m} \\mathcal{L}_{\\mathrm{MLM}}$ and whose second entry is $\\nabla_{s^{(k)}} \\mathcal{L}_{\\mathrm{MLM}}$. No numerical approximation or rounding is required, and no physical units are involved. Your derivation must be self-contained and must not invoke any shortcut formulas beyond the stated definitions.",
            "solution": "The problem requires the derivation of the gradients of the Masked Language Modeling (MLM) loss, $\\mathcal{L}_{\\mathrm{MLM}}$, with respect to two sets of parameters: a shared mask token embedding $m$ and a set of span-level mask embeddings $s^{(k)}$. The derivation will be built from first principles using the chain rule of calculus.\n\nThe MLM loss is defined as the sum of negative log-likelihoods over all masked positions:\n$$\n\\mathcal{L}_{\\mathrm{MLM}} = \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} \\mathcal{L}_i\n$$\nwhere $\\mathcal{L}_i = - \\ln p_{i}[y_{i}]$ is the loss for a single token at position $i$. The index $k'$ is used as a dummy summation variable over the spans to distinguish it from the fixed index $k$ in the second part of the problem.\n\nThe sequence of computations for a single position $i$ is:\n$1$. The input embedding ($m$ or $s^{(k)}$) is used to compute the hidden state $h_i$.\n$2$. The hidden state $h_i \\in \\mathbb{R}^d$ is used to compute the logits $z_i \\in \\mathbb{R}^V$ via $z_i = U h_i$.\n$3$. The logits $z_i$ are converted to probabilities $p_i \\in \\mathbb{R}^V$ via $p_i = \\mathrm{softmax}(z_i)$.\n$4$. The loss $\\mathcal{L}_i$ is calculated from $p_i$ and the ground-truth index $y_i$.\n\nWe will compute the gradient by applying the chain rule backward through this computation graph.\n\n**Step 1: Gradient of $\\mathcal{L}_i$ with respect to the logits $z_i$**\n\nThe loss for position $i$ is $\\mathcal{L}_i = -\\ln p_i[y_i]$. The probability $p_i[j]$ for any class index $j \\in \\{1, \\dots, V\\}$ is given by the softmax function:\n$$\np_i[j] = \\frac{\\exp(z_i[j])}{\\sum_{l=1}^{V} \\exp(z_i[l])}\n$$\nwhere $z_i[j]$ is the $j$-th component of the logit vector $z_i$.\n\nSubstituting this into the loss function for the true class $y_i$:\n$$\n\\mathcal{L}_i = - \\ln \\left( \\frac{\\exp(z_i[y_i])}{\\sum_{l=1}^{V} \\exp(z_i[l])} \\right) = -z_i[y_i] + \\ln\\left(\\sum_{l=1}^{V} \\exp(z_i[l])\\right)\n$$\nWe now compute the partial derivative of $\\mathcal{L}_i$ with respect to an arbitrary component $z_i[j]$ of the logit vector $z_i$.\n\nIf $j = y_i$:\n$$\n\\frac{\\partial \\mathcal{L}_i}{\\partial z_i[y_i]} = -1 + \\frac{1}{\\sum_{l=1}^{V} \\exp(z_i[l])} \\cdot \\frac{\\partial}{\\partial z_i[y_i]} \\left( \\sum_{l=1}^{V} \\exp(z_i[l]) \\right) = -1 + \\frac{\\exp(z_i[y_i])}{\\sum_{l=1}^{V} \\exp(z_i[l])} = p_i[y_i] - 1\n$$\nIf $j \\neq y_i$:\n$$\n\\frac{\\partial \\mathcal{L}_i}{\\partial z_i[j]} = 0 + \\frac{1}{\\sum_{l=1}^{V} \\exp(z_i[l])} \\cdot \\frac{\\partial}{\\partial z_i[j]} \\left( \\sum_{l=1}^{V} \\exp(z_i[l]) \\right) = \\frac{\\exp(z_i[j])}{\\sum_{l=1}^{V} \\exp(z_i[l])} = p_i[j]\n$$\nThese two cases can be combined into a single vector expression. Let $e_{y_i}$ be the one-hot vector where the component corresponding to index $y_i$ is $1$ and all others are $0$. The gradient of the scalar $\\mathcal{L}_i$ with respect to the vector $z_i$ is:\n$$\n\\nabla_{z_i} \\mathcal{L}_i = p_i - e_{y_i}\n$$\nThis vector $\\nabla_{z_i} \\mathcal{L}_i \\in \\mathbb{R}^V$ represents the error signal at the logit level.\n\n**Step 2: Gradient of $\\mathcal{L}_i$ with respect to the hidden state $h_i$**\n\nThe logits are a linear function of the hidden state: $z_i = U h_i$, where $U \\in \\mathbb{R}^{V \\times d}$ and $h_i \\in \\mathbb{R}^d$. We use the chain rule for vector-valued functions. The gradient of the scalar $\\mathcal{L}_i$ with respect to the vector $h_i$ is:\n$$\n\\nabla_{h_i} \\mathcal{L}_i = \\left(\\frac{\\partial z_i}{\\partial h_i}\\right)^T (\\nabla_{z_i} \\mathcal{L}_i)\n$$\nThe term $\\frac{\\partial z_i}{\\partial h_i}$ is the Jacobian matrix of the function $z_i(h_i)$. Since $z_i = U h_i$, this Jacobian is simply the matrix $U$. Therefore:\n$$\n\\nabla_{h_i} \\mathcal{L}_i = U^T (\\nabla_{z_i} \\mathcal{L}_i) = U^T (p_i - e_{y_i})\n$$\nThis gradient, which we denote as $g_i = \\nabla_{h_i} \\mathcal{L}_i$, is a vector in $\\mathbb{R}^d$.\n\n**Step 3: Derivation of $\\nabla_{m} \\mathcal{L}_{\\mathrm{MLM}}$**\n\nIn the shared mask token setting, the hidden state $h_i$ for every masked position $i$ is a linear function of a single shared parameter vector $m \\in \\mathbb{R}^d$:\n$$\nh_i = A_i m + b_i\n$$\nThe total loss $\\mathcal{L}_{\\mathrm{MLM}}$ is a function of $m$ through its dependence on every $h_i$. By linearity of the gradient operator and the chain rule:\n$$\n\\nabla_{m} \\mathcal{L}_{\\mathrm{MLM}} = \\nabla_{m} \\left( \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} \\mathcal{L}_i(h_i(m)) \\right) = \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} \\nabla_{m} \\mathcal{L}_i\n$$\nFor each term $\\mathcal{L}_i$, we apply the chain rule again:\n$$\n\\nabla_{m} \\mathcal{L}_i = \\left(\\frac{\\partial h_i}{\\partial m}\\right)^T (\\nabla_{h_i} \\mathcal{L}_i)\n$$\nThe Jacobian of the function $h_i(m)$ is the matrix $A_i \\in \\mathbb{R}^{d \\times d}$. Substituting this and the expression for $\\nabla_{h_i} \\mathcal{L}_i = g_i$:\n$$\n\\nabla_{m} \\mathcal{L}_i = A_i^T g_i = A_i^T U^T (p_i - e_{y_i})\n$$\nSumming over all masked positions gives the final gradient with respect to $m$:\n$$\n\\nabla_{m} \\mathcal{L}_{\\mathrm{MLM}} = \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} A_i^T U^T (p_i - e_{y_i})\n$$\n\n**Step 4: Derivation of $\\nabla_{s^{(k)}} \\mathcal{L}_{\\mathrm{MLM}}$**\n\nIn the span-level mask embeddings setting, for a fixed span index $k$, the parameter vector $s^{(k)} \\in \\mathbb{R}^d$ influences only the hidden states $h_i$ where $i \\in S^{(k)}$. For these positions, the relationship is:\n$$\nh_i = A_i s^{(k)} + b_i \\quad \\text{for } i \\in S^{(k)}\n$$\nFor any position $j$ in a different span, $j \\in S^{(k')}$ with $k' \\neq k$, the hidden state $h_j$ depends on $s^{(k')}$, not $s^{(k)}$. Therefore, $\\nabla_{s^{(k)}} \\mathcal{L}_j = 0$ for $j \\notin S^{(k)}$.\n\nThe gradient of the total loss with respect to $s^{(k)}$ is:\n$$\n\\nabla_{s^{(k)}} \\mathcal{L}_{\\mathrm{MLM}} = \\nabla_{s^{(k)}} \\left( \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} \\mathcal{L}_i \\right) = \\sum_{i \\in S^{(k)}} \\nabla_{s^{(k)}} \\mathcal{L}_i\n$$\nThe sum is only over the positions in span $k$. For each term in this sum, we apply the chain rule:\n$$\n\\nabla_{s^{(k)}} \\mathcal{L}_i = \\left(\\frac{\\partial h_i}{\\partial s^{(k)}}\\right)^T (\\nabla_{h_i} \\mathcal{L}_i) \\quad \\text{for } i \\in S^{(k)}\n$$\nThe Jacobian $\\frac{\\partial h_i}{\\partial s^{(k)}}$ is the matrix $A_i$. Substituting this and the expression for $g_i = \\nabla_{h_i} \\mathcal{L}_i$:\n$$\n\\nabla_{s^{(k)}} \\mathcal{L}_i = A_i^T g_i = A_i^T U^T (p_i - e_{y_i})\n$$\nSumming over all positions within span $k$ gives the final gradient with respect to $s^{(k)}$:\n$$\n\\nabla_{s^{(k)}} \\mathcal{L}_{\\mathrm{MLM}} = \\sum_{i \\in S^{(k)}} A_i^T U^T (p_i - e_{y_i})\n$$\nThe results are the closed-form analytic expressions for the requested gradients.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} A_i^T U^T (p_i - e_{y_i}) & \\sum_{i \\in S^{(k)}} A_i^T U^T (p_i - e_{y_i}) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The power of pre-training objectives extends beyond natural language to structured domains like mathematics and computer code, where syntax and semantics are rigorously defined. This hands-on coding practice  asks you to build a complete evaluation framework to test a model's grasp of arithmetic structure. By implementing a parser that correctly handles operator precedence and using it to assess a model's ability to fill in masked operators and parentheses, you will experience first-hand how to design and apply rigorous, domain-specific evaluations for pre-trained models.",
            "id": "3164749",
            "problem": "You are asked to implement an evaluation routine for Masked Language Modeling (MLM) in the context of mathematical expressions, focusing on operator precedence and parentheses recovery. The goal is to measure how well a pre-training objective for transformer models recovers masked operators and parentheses in arithmetic expressions.\n\nFundamental base:\n- Masked Language Modeling (MLM) objective seeks to maximize the likelihood of the correct masked tokens given the unmasked context. Formally, for a token sequence $x$ with masked positions indexed by the set $\\mathcal{M}$, ground-truth tokens $\\{t_i\\}_{i \\in \\mathcal{M}}$, and model predictive distributions $\\{p_i(\\cdot)\\}_{i \\in \\mathcal{M}}$ over a vocabulary $\\mathcal{V}$, the Maximum Likelihood Estimation (MLE) criterion is equivalent to minimizing the cross-entropy loss:\n$$\n\\mathcal{L} \\;=\\; -\\sum_{i \\in \\mathcal{M}} \\log p_i(t_i).\n$$\n- Arithmetic expressions follow standard precedence: parentheses first, then multiplication and division, then addition and subtraction, all operators being left-associative. In this problem, the operator vocabulary is $\\mathcal{V} = \\{+, -, *, /, (, )\\}$, where $($ and $)$ denote parentheses. All numbers are non-negative integers and division is real-valued with the usual rule that division by zero is undefined.\n\nDefinitions and evaluation metrics to implement:\n- Token-level top-$1$ accuracy: For each masked position $i \\in \\mathcal{M}$, let $\\hat{t}_i = \\arg\\max_{v \\in \\mathcal{V}} p_i(v)$. The token-level accuracy is\n$$\n\\text{Accuracy} \\;=\\; \\frac{1}{|\\mathcal{M}|}\\sum_{i \\in \\mathcal{M}} \\mathbf{1}\\{\\hat{t}_i = t_i\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n- Negative log-likelihood (NLL): Compute\n$$\n\\text{NLL} \\;=\\; -\\sum_{i \\in \\mathcal{M}} \\log p_i(t_i),\n$$\nusing the natural logarithm (base $e$).\n- Expression-level correctness: Let $E$ denote the tokenized expression with masked positions. Define $E_{\\text{gt}}$ as the expression obtained by substituting each masked position with its ground-truth token $t_i$ and $E_{\\text{pred}}$ as the expression obtained by substituting each masked position with the top-$1$ prediction $\\hat{t}_i$. Evaluate both $E_{\\text{gt}}$ and $E_{\\text{pred}}$ numerically under standard precedence and left-associativity. If either expression is syntactically invalid (for example, mismatched parentheses or invalid operator placement) or evaluates to an undefined quantity (for example, division by zero), treat it as incorrect. Otherwise, declare expression-level correctness to be true if\n$$\n| \\text{val}(E_{\\text{pred}}) - \\text{val}(E_{\\text{gt}}) | < 10^{-9}.\n$$\n\nYour task:\n- Implement a parser and evaluator for arithmetic expressions over the vocabulary $\\mathcal{V}$ that respects parentheses and operator precedence. You must not rely on external input; instead, use the fixed test suite below.\n- For each test case, compute the token-level top-$1$ accuracy, the NLL, and the expression-level correctness boolean as defined above.\n\nAngle units are not applicable; there are no physical units required. All probabilities must be treated as real numbers. Use the natural logarithm (base $e$) for the NLL.\n\nTest suite:\nFor each test case, the expression is given as a token list containing the literal symbol $\\text{MASK}$ in positions to be recovered, the ground-truth masked tokens in left-to-right order, and the model's predicted probability distributions over $\\mathcal{V}$ for each masked position. The vocabulary is fixed as $\\mathcal{V} = \\{+, -, *, /, (, )\\}$.\n\n- Test case $1$:\n  - Tokens: $\\left[3, *, (, 2, \\text{MASK}, 4, ), -, 5\\right]$\n  - Ground truth masked tokens: $\\left[+\\right]$\n  - Predicted probabilities at the masked position: $\\{+\\!\\mapsto\\!0.7,\\; -\\!\\mapsto\\!0.2,\\; *\\!\\mapsto\\!0.05,\\; /\\!\\mapsto\\!0.05,\\; (\\!\\mapsto\\!0.0,\\; )\\!\\mapsto\\!0.0\\}$\n- Test case $2$:\n  - Tokens: $\\left[10, \\text{MASK}, 3, *, (, 1, +, 2, )\\right]$\n  - Ground truth masked tokens: $\\left[/\\right]$\n  - Predicted probabilities: $\\{/\\!\\mapsto\\!0.4,\\; *\\!\\mapsto\\!0.5,\\; +\\!\\mapsto\\!0.05,\\; -\\!\\mapsto\\!0.05,\\; (\\!\\mapsto\\!0.0,\\; )\\!\\mapsto\\!0.0\\}$\n- Test case $3$:\n  - Tokens: $\\left[8, *, \\text{MASK}, 3, +, 1, \\text{MASK}\\right]$\n  - Ground truth masked tokens: $\\left[(, )\\right]$\n  - Predicted probabilities (first mask): $\\{(\\!\\mapsto\\!0.6,\\; )\\!\\mapsto\\!0.1,\\; *\\!\\mapsto\\!0.1,\\; +\\!\\mapsto\\!0.1,\\; -\\!\\mapsto\\!0.05,\\; /\\!\\mapsto\\!0.05\\}$\n  - Predicted probabilities (second mask): $\\{)\\!\\mapsto\\!0.55,\\; (\\!\\mapsto\\!0.1,\\; +\\!\\mapsto\\!0.1,\\; -\\!\\mapsto\\!0.1,\\; *\\!\\mapsto\\!0.075,\\; /\\!\\mapsto\\!0.075\\}$\n- Test case $4$:\n  - Tokens: $\\left[12, \\text{MASK}, (, 3, -, 3, )\\right]$\n  - Ground truth masked tokens: $\\left[+\\right]$\n  - Predicted probabilities: $\\{/\\!\\mapsto\\!0.45,\\; +\\!\\mapsto\\!0.3,\\; *\\!\\mapsto\\!0.1,\\; -\\!\\mapsto\\!0.1,\\; (\\!\\mapsto\\!0.025,\\; )\\!\\mapsto\\!0.025\\}$\n- Test case $5$:\n  - Tokens: $\\left[\\text{MASK}, 5, -, 2, \\text{MASK}\\right]$\n  - Ground truth masked tokens: $\\left[(, )\\right]$\n  - Predicted probabilities (first mask): $\\{(\\!\\mapsto\\!0.51,\\; )\\!\\mapsto\\!0.2,\\; +\\!\\mapsto\\!0.08,\\; -\\!\\mapsto\\!0.08,\\; *\\!\\mapsto\\!0.065,\\; /\\!\\mapsto\\!0.065\\}$\n  - Predicted probabilities (second mask): $\\{(\\!\\mapsto\\!0.52,\\; )\\!\\mapsto\\!0.48,\\; +\\!\\mapsto\\!0.0,\\; -\\!\\mapsto\\!0.0,\\; *\\!\\mapsto\\!0.0,\\; /\\!\\mapsto\\!0.0\\}$\n\nProgram output specification:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets. Each test case result must be a list of the form $[\\text{accuracy}, \\text{NLL}, \\text{expression\\_correct}]$, where $\\text{accuracy}$ and $\\text{NLL}$ are real numbers and $\\text{expression\\_correct}$ is a boolean. For example, an output shape like [[a_1, l_1, b_1],[a_2, l_2, b_2],...] is expected.",
            "solution": "The solution is grounded in the Masked Language Modeling (MLM) objective and arithmetic evaluation rules.\n\nStarting point and principles:\n1. Masked Language Modeling (MLM) defines, for each masked position $i \\in \\mathcal{M}$, a categorical distribution $p_i(\\cdot)$ over the vocabulary $\\mathcal{V}$. The Maximum Likelihood Estimation (MLE) objective minimizes the cross-entropy loss:\n$$\n\\mathcal{L} = -\\sum_{i \\in \\mathcal{M}} \\log p_i(t_i),\n$$\nwhere $t_i$ is the ground-truth token. This yields a per-sample Negative Log-Likelihood (NLL) computed by summing $-\\log p_i(t_i)$ over all masked positions in the sample.\n\n2. Token-level top-$1$ accuracy is a direct observable from the model outputs: at each masked position $i$, compute the top-$1$ prediction $\\hat{t}_i = \\arg\\max_{v \\in \\mathcal{V}} p_i(v)$ and average the indicator of correctness over all masks:\n$$\n\\text{Accuracy} \\;=\\; \\frac{1}{|\\mathcal{M}|}\\sum_{i \\in \\mathcal{M}} \\mathbf{1}\\{\\hat{t}_i = t_i\\}.\n$$\n\n3. Arithmetic expression evaluation must obey operator precedence and associativity. Specifically:\n   - Parentheses $($ and $)$ override precedence by grouping.\n   - Multiplication $*$ and division $/$ have higher precedence than addition $+$ and subtraction $-$.\n   - All operators are left-associative.\nA robust algorithm to enforce these rules is the \"shunting-yard\" algorithm, which transforms infix expressions into Reverse Polish Notation (RPN), followed by evaluation of the RPN. Syntactic validity must be checked (balanced parentheses, proper operator/operand ordering), and division by zero must be detected and treated as invalid.\n\nAlgorithmic steps:\n- Token replacement:\n  - Given a token list with $\\text{MASK}$ positions, and either the ground-truth tokens $\\{t_i\\}$ or top-$1$ predicted tokens $\\{\\hat{t}_i\\}$, substitute left-to-right to produce a fully specified expression.\n- Parsing and evaluation:\n  - Use the shunting-yard algorithm to convert infix tokens to RPN, respecting precedence $\\pi(*)=\\pi(/)=2$, $\\pi(+)=\\pi(-)=1$, treating parentheses specially.\n  - Evaluate the RPN by a numeric stack; each binary operator pops two operands $a$ and $b$ and pushes back the result $a + b$, $a - b$, $a * b$, or $a / b$ with the constraint $b \\neq 0$ for division.\n  - If any syntactic error or division by zero occurs, mark the expression as invalid.\n- Metrics computation:\n  - Accuracy: count matches between $\\hat{t}_i$ and $t_i$ and divide by $|\\mathcal{M}|$.\n  - NLL: sum $-\\log p_i(t_i)$ over masked positions, using the natural logarithm.\n  - Expression-level correctness: evaluate $E_{\\text{gt}}$ to get $\\text{val}(E_{\\text{gt}})$, and $E_{\\text{pred}}$ to get $\\text{val}(E_{\\text{pred}})$. If either is invalid, return false. Otherwise, return true if $|\\text{val}(E_{\\text{pred}}) - \\text{val}(E_{\\text{gt}})| < 10^{-9}$.\n\nManual computation for the provided test suite:\n- Test case $1$:\n  - Ground truth substitution yields $3 * ( 2 + 4 ) - 5 = 3 * 6 - 5 = 13$.\n  - Top-$1$ prediction at the mask is $+$, matching ground truth.\n  - Accuracy $= 1.0$.\n  - NLL $= -\\log(0.7) \\approx 0.35667494393873245$.\n  - Predicted expression equals ground truth; correctness is true.\n- Test case $2$:\n  - Ground truth substitution yields $10 / 3 * (1 + 2) = \\frac{10}{3} * 3 = 10$.\n  - Top-$1$ prediction is $*$, not matching ground truth $/$.\n  - Accuracy $= 0.0$.\n  - NLL $= -\\log(0.4) \\approx 0.9162907318741551$.\n  - Predicted expression evaluates to $10 * 3 * 3 = 90$; correctness is false.\n- Test case $3$:\n  - Ground truth substitution yields $8 * ( 3 + 1 ) = 8 * 4 = 32$.\n  - Top-$1$ predictions are $(, )$, both matching ground truth.\n  - Accuracy $= 1.0$.\n  - NLL $= -\\log(0.6) + -\\log(0.55) \\approx 0.5108256237659907 + 0.5978370007556204 = 1.108662624521611$.\n  - Predicted expression equals ground truth; correctness is true.\n- Test case $4$:\n  - Ground truth substitution yields $12 + (3 - 3) = 12 + 0 = 12$.\n  - Top-$1$ prediction is $/$, not matching ground truth $+$.\n  - Accuracy $= 0.0$.\n  - NLL $= -\\log(0.3) \\approx 1.2039728043259361$.\n  - Predicted expression attempts $12 / (3 - 3) = 12 / 0$; invalid due to division by zero; correctness is false.\n- Test case $5$:\n  - Ground truth substitution yields $( 5 - 2 ) = 3$.\n  - Top-$1$ predictions are first mask $($ (correct), second mask $($ (incorrect; ground truth is $)$).\n  - Accuracy $= \\frac{1}{2} = 0.5$.\n  - NLL $= -\\log(0.51) + -\\log(0.48) \\approx 0.6733445532637656 + 0.7339691750802004 = 1.407313728343966$.\n  - Predicted expression has mismatched parentheses; invalid; correctness is false.\n\nThe program will implement the shunting-yard parser and evaluator, compute the metrics for each case, and print a single line: a list of per-case results of the form $[\\text{accuracy}, \\text{NLL}, \\text{boolean}]$.",
            "answer": "```python\n# Python 3.12 program to evaluate MLM on masked arithmetic expressions\n# respecting operator precedence and parentheses.\n# Libraries allowed: numpy 1.23.5, scipy 1.11.4 (not used), and Python standard library.\n\nimport numpy as np\nimport math\nfrom typing import List, Dict, Tuple, Optional\n\nVOCAB = ['+', '-', '*', '/', '(', ')']\n\ndef top1_prediction(prob_dist: Dict[str, float]) -> str:\n    # Return the token with highest probability. Ties broken by lexicographic order of token keys.\n    # To ensure deterministic behavior, sort by (-probability, token).\n    return sorted(prob_dist.items(), key=lambda kv: (-kv[1], kv[0]))[0][0]\n\ndef replace_masks(tokens: List[str], fills: List[str]) -> List[str]:\n    # Replace 'MASK' tokens left-to-right with corresponding fills.\n    result = []\n    fill_iter = iter(fills)\n    for tok in tokens:\n        if tok == 'MASK':\n            result.append(next(fill_iter))\n        else:\n            result.append(tok)\n    return result\n\ndef is_number(tok: str) -> bool:\n    # A simplified check for non-negative integers as per problem spec.\n    try:\n        int(tok)\n        return True\n    except (ValueError, TypeError):\n        return False\n\ndef apply_op(op: str, a: float, b: float) -> Optional[float]:\n    # Applies binary operator to operands a (left) and b (right).\n    if op == '+':\n        return a + b\n    elif op == '-':\n        return a - b\n    elif op == '*':\n        return a * b\n    elif op == '/':\n        if abs(b) < 1e-18: # Avoid division by zero\n            return None\n        return a / b\n    else:\n        return None\n\ndef evaluate_expression(tokens: List[str]) -> Optional[float]:\n    # Evaluate infix expression tokens using shunting-yard algorithm and evaluation.\n    # Returns None if invalid syntax or division by zero.\n    # Precedence: * and / (2) > + and - (1), left-associative.\n    precedence = {'+': 1, '-': 1, '*': 2, '/': 2}\n    output_queue: List[str] = []\n    op_stack: List[str] = []\n\n    # Validate token sequence on the fly (basic checks).\n    prev_type = None  # 'num', 'op', '(', ')'\n    \n    # Check for empty token list\n    if not tokens:\n        return None\n\n    for i, tok in enumerate(tokens):\n        current_token_str = str(tok) # Ensure token is string for checks\n        if is_number(current_token_str):\n            if prev_type in ('num', ')'): return None # number cannot follow number or ')'\n            output_queue.append(current_token_str)\n            prev_type = 'num'\n        elif current_token_str in precedence:\n            if prev_type in (None, 'op', '('): return None # op cannot be at start or follow op or '('\n            while (op_stack and op_stack[-1] in precedence and\n                   precedence.get(op_stack[-1], 0) >= precedence.get(current_token_str, 0)):\n                output_queue.append(op_stack.pop())\n            op_stack.append(current_token_str)\n            prev_type = 'op'\n        elif current_token_str == '(':\n            if prev_type in ('num', ')'): return None # '(' cannot follow number or ')'\n            op_stack.append(current_token_str)\n            prev_type = '('\n        elif current_token_str == ')':\n            if prev_type in (None, 'op', '('): return None # ')' cannot be at start or follow op or '('\n            found_open = False\n            while op_stack:\n                top = op_stack.pop()\n                if top == '(':\n                    found_open = True\n                    break\n                else:\n                    output_queue.append(top)\n            if not found_open:\n                return None # Mismatched parentheses\n            prev_type = ')'\n        else:\n            return None # Unknown token\n\n    # Finalize: pop remaining operators; check for final syntax validity\n    if prev_type in ('op', '('): return None # Expression cannot end with an operator or '('\n\n    while op_stack:\n        top = op_stack.pop()\n        if top in ('(', ')'):\n            return None # Mismatched parentheses\n        output_queue.append(top)\n\n    # Evaluate RPN\n    stack: List[float] = []\n    for tok in output_queue:\n        if is_number(tok):\n            stack.append(float(tok))\n        elif tok in precedence:\n            if len(stack) < 2:\n                return None\n            b = stack.pop()\n            a = stack.pop()\n            res = apply_op(tok, a, b)\n            if res is None:\n                return None\n            stack.append(res)\n        else:\n            return None\n\n    return stack[0] if len(stack) == 1 else None\n\ndef compute_metrics(tokens: List[str],\n                    gt_fills: List[str],\n                    prob_dists: List[Dict[str, float]]) -> Tuple[float, float, bool]:\n    # Compute accuracy, NLL, and expression-level correctness.\n    # Top-1 predictions\n    top_preds = [top1_prediction(d) for d in prob_dists]\n\n    # Accuracy\n    correct = sum(1 for p, g in zip(top_preds, gt_fills) if p == g)\n    accuracy = correct / len(gt_fills) if gt_fills else 0.0\n\n    # NLL\n    nll = 0.0\n    for g, d in zip(gt_fills, prob_dists):\n        p = d.get(g, 0.0)\n        if p <= 1e-9: # Use a small epsilon for robustness against log(0)\n            nll = float('inf')\n            break\n        nll += -math.log(p)\n\n    # Expression-level correctness\n    gt_expr_tokens = replace_masks([str(t) for t in tokens], gt_fills)\n    pred_expr_tokens = replace_masks([str(t) for t in tokens], top_preds)\n\n    gt_val = evaluate_expression(gt_expr_tokens)\n    pred_val = evaluate_expression(pred_expr_tokens)\n\n    expr_correct = (gt_val is not None) and (pred_val is not None) and (abs(pred_val - gt_val) < 1e-9)\n\n    return accuracy, nll, expr_correct\n\ndef solve():\n    # Define test cases consistent with the problem statement.\n    test_cases = [\n        # Test case 1\n        (\n            [3, \"*\", \"(\", 2, \"MASK\", 4, \")\", \"-\", 5],\n            [\"+\"],\n            [\n                {\"+\": 0.7, \"-\": 0.2, \"*\": 0.05, \"/\": 0.05, \"(\": 0.0, \")\": 0.0}\n            ]\n        ),\n        # Test case 2\n        (\n            [10, \"MASK\", 3, \"*\", \"(\", 1, \"+\", 2, \")\"],\n            [\"/\"],\n            [\n                {\"/\": 0.4, \"*\": 0.5, \"+\": 0.05, \"-\": 0.05, \"(\": 0.0, \")\": 0.0}\n            ]\n        ),\n        # Test case 3\n        (\n            [8, \"*\", \"MASK\", 3, \"+\", 1, \"MASK\"],\n            [\"(\", \")\"],\n            [\n                {\"(\": 0.6, \")\": 0.1, \"*\": 0.1, \"+\": 0.1, \"-\": 0.05, \"/\": 0.05},\n                {\")\": 0.55, \"(\": 0.1, \"+\": 0.1, \"-\": 0.1, \"*\": 0.075, \"/\": 0.075}\n            ]\n        ),\n        # Test case 4\n        (\n            [12, \"MASK\", \"(\", 3, \"-\", 3, \")\"],\n            [\"+\"],\n            [\n                {\"/\": 0.45, \"+\": 0.3, \"*\": 0.1, \"-\": 0.1, \"(\": 0.025, \")\": 0.025}\n            ]\n        ),\n        # Test case 5\n        (\n            [\"MASK\", 5, \"-\", 2, \"MASK\"],\n            [\"(\", \")\"],\n            [\n                {\"(\": 0.51, \")\": 0.2, \"+\": 0.08, \"-\": 0.08, \"*\": 0.065, \"/\": 0.065},\n                {\"(\": 0.52, \")\": 0.48, \"+\": 0.0, \"-\": 0.0, \"*\": 0.0, \"/\": 0.0}\n            ]\n        ),\n    ]\n\n    results = []\n    for tokens, gt_fills, prob_dists in test_cases:\n        acc, nll, expr_ok = compute_metrics(tokens, gt_fills, prob_dists)\n        results.append([acc, nll, expr_ok])\n\n    # Convert boolean to lowercase 'true'/'false' for consistent output\n    # but the problem spec asks for a boolean, so Python's default repr is fine.\n    print(str(results).replace(\" \", \"\"))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}