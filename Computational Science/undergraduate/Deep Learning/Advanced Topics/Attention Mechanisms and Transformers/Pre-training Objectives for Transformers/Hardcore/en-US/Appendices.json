{
    "hands_on_practices": [
        {
            "introduction": "To understand the training dynamics of Masked Language Modeling (MLM), we must first analyze the masking process itself. This initial exercise models dynamic masking as a simple probabilistic process, allowing you to quantify how frequently each token provides a learning signal over the course of training. By deriving key statistical properties from first principles, you will build a foundational intuition for how training data is utilized in large-scale pre-training .",
            "id": "3164748",
            "problem": "A Transformer encoder is pre-trained using Masked Language Modeling (MLM), where, during each pass over the data, every token position independently becomes a prediction target with probability $p$, regardless of whether it is replaced by a special mask symbol, a random token, or left unchanged. The training corpus contains a fixed set of token positions, and each epoch iterates over every token position exactly once. Masking is dynamically re-sampled at each epoch so that the selection decision for a given token position in one epoch is independent of its selection in any other epoch. Training runs for $E$ epochs. Stochastic Gradient Descent (SGD) is used, but for this problem, focus only on per-token contributions: each time a token position is selected as a prediction target, it contributes a single loss term and thus one gradient-contributing event for that token position during that epoch.\n\nFor a fixed token position $i$, define the random variable $S$ as the total number of epochs in which this position is selected as a prediction target. Using only the axioms of probability for independent Bernoulli trials and linearity of expectation as the fundamental base, derive, in closed form, the following quantities in terms of $E$ and $p$:\n- The expected number of gradient-contributing events per token position, $\\mathbb{E}[S]$.\n- The coverage probability that the token position is selected at least once over the $E$ epochs, $\\Pr(S \\geq 1)$.\n\nExpress your final answer as a single row vector $\\begin{pmatrix} \\mathbb{E}[S] & \\Pr(S \\geq 1) \\end{pmatrix}$. No rounding is required, and no physical units are involved.",
            "solution": "The problem statement is evaluated for validity before attempting a solution.\n\n### Step 1: Extract Givens\n- A Transformer encoder is pre-trained using Masked Language Modeling (MLM).\n- During each pass (epoch), every token position independently becomes a prediction target with probability $p$.\n- The selection decision for a given token position in one epoch is independent of its selection in any other epoch.\n- The total number of epochs for training is $E$.\n- Each time a token position is selected as a prediction target, it contributes a single gradient-contributing event.\n- For a fixed token position $i$, the random variable $S$ is defined as the total number of epochs in which this position is selected as a prediction target.\n- The derivation must use only the axioms of probability for independent Bernoulli trials and linearity of expectation.\n- The required quantities to be derived are the expected number of gradient-contributing events per token position, $\\mathbb{E}[S]$, and the coverage probability that the token position is selected at least once, $\\Pr(S \\geq 1)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n- **Scientifically Grounded:** The problem describes a simplified but fundamentally correct probabilistic model of the Masked Language Modeling (MLM) pre-training objective, a cornerstone of modern natural language processing models like BERT. The use of Bernoulli trials to model the independent masking events is standard and mathematically sound.\n- **Well-Posed:** The problem is clearly defined with all necessary parameters ($E$, $p$) and explicit definitions of the random variable $S$ and the target quantities ($\\mathbb{E}[S]$, $\\Pr(S \\geq 1)$). The conditions lead to a unique and meaningful solution.\n- **Objective:** The language is formal, precise, and devoid of any subjective or ambiguous terminology.\n- **Flaw Checklist:**\n    1.  **Scientific or Factual Unsoundness:** None. The setup is a valid abstraction of a real-world machine learning process.\n    2.  **Non-Formalizable or Irrelevant:** The problem is entirely formalizable within probability theory and is directly relevant to the field of deep learning.\n    3.  **Incomplete or Contradictory Setup:** The problem is self-contained and consistent. The independence of events across epochs is explicitly stated.\n    4.  **Unrealistic or Infeasible:** The scenario is a standard theoretical model used in the analysis of such algorithms. It is not unrealistic.\n    5.  **Ill-Posed or Poorly Structured:** The problem is well-structured and leads to a unique solution.\n    6.  **Pseudo-Profound, Trivial, or Tautological:** The problem requires the correct application of fundamental principles of probability theory (linearity of expectation, properties of independent events) and is a non-trivial exercise in formal derivation.\n    7.  **Outside Scientific Verifiability:** The derivation is mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid** and well-posed. A solution will be derived.\n\n### Derivation\n\nLet us consider a single, fixed token position. The training process runs for $E$ epochs.\n\nFirst, we define a set of indicator random variables, $X_j$, for each epoch $j \\in \\{1, 2, \\dots, E\\}$.\nLet $X_j = 1$ if the token position is selected as a prediction target in epoch $j$, and $X_j = 0$ otherwise.\n\nAccording to the problem statement, the selection in any given epoch is an independent event with probability $p$. Therefore, each $X_j$ is an independent Bernoulli random variable with parameter $p$. The probability mass function for each $X_j$ is:\n$$ \\Pr(X_j = 1) = p $$\n$$ \\Pr(X_j = 0) = 1 - p $$\n\nThe random variable $S$ represents the total number of epochs in which the position is selected. This is the sum of the indicator variables over all epochs:\n$$ S = \\sum_{j=1}^{E} X_j $$\n\n**1. Derivation of the Expected Value $\\mathbb{E}[S]$**\n\nWe need to find the expected total number of gradient-contributing events, which is $\\mathbb{E}[S]$. The problem requires using the principle of linearity of expectation.\n\nThe expectation of a single Bernoulli random variable $X_j$ is:\n$$ \\mathbb{E}[X_j] = 1 \\cdot \\Pr(X_j=1) + 0 \\cdot \\Pr(X_j=0) = 1 \\cdot p + 0 \\cdot (1-p) = p $$\n\nUsing the linearity of expectation, which states that the expectation of a sum of random variables is the sum of their individual expectations, we have:\n$$ \\mathbb{E}[S] = \\mathbb{E}\\left[\\sum_{j=1}^{E} X_j\\right] = \\sum_{j=1}^{E} \\mathbb{E}[X_j] $$\n\nSince $\\mathbb{E}[X_j] = p$ for every epoch $j$, the sum becomes:\n$$ \\mathbb{E}[S] = \\sum_{j=1}^{E} p = Ep $$\n\n**2. Derivation of the Coverage Probability $\\Pr(S \\geq 1)$**\n\nWe need to find the probability that the token position is selected at least once over the $E$ epochs. This is the probability $\\Pr(S \\geq 1)$.\n\nIt is more direct to calculate this probability using the complement rule. The event \"$S \\geq 1$\" (the token is selected at least once) is the complement of the event \"$S = 0$\" (the token is never selected).\n$$ \\Pr(S \\geq 1) = 1 - \\Pr(S = 0) $$\n\nThe event \"$S=0$\" occurs if and only if the token is not selected in any of the $E$ epochs. This means $X_1=0$ AND $X_2=0$ AND ... AND $X_E=0$.\n$$ \\Pr(S=0) = \\Pr(X_1=0, X_2=0, \\dots, X_E=0) $$\n\nThe problem states that selection decisions across epochs are independent. Therefore, the probability of the joint event is the product of the probabilities of the individual events:\n$$ \\Pr(S=0) = \\prod_{j=1}^{E} \\Pr(X_j=0) $$\n\nFor each epoch $j$, the probability of the token not being selected is $\\Pr(X_j=0) = 1-p$.\nSubstituting this into the product, we get:\n$$ \\Pr(S=0) = \\prod_{j=1}^{E} (1-p) = (1-p)^E $$\n\nFinally, substituting this result back into the complement rule equation gives the coverage probability:\n$$ \\Pr(S \\geq 1) = 1 - (1-p)^E $$\n\nThe derived quantities are $\\mathbb{E}[S] = Ep$ and $\\Pr(S \\geq 1) = 1 - (1-p)^E$. These are expressed in closed form in terms of $E$ and $p$ as required.",
            "answer": "$$\\boxed{\\begin{pmatrix} Ep & 1 - (1-p)^{E} \\end{pmatrix}}$$"
        },
        {
            "introduction": "The core of learning in a neural network lies in how gradients flow from the loss function back to the model's parameters. This practice delves into the calculus of MLM, asking you to derive the gradient with respect to different types of mask embeddings . Completing this derivation will illuminate how architectural decisions, such as using a shared versus specialized mask token, concretely impact the parameter updates during training.",
            "id": "3164800",
            "problem": "Consider a transformer-based sequence model trained with Masked Language Modeling (MLM), which is defined as minimizing the negative log-likelihood of the true tokens at masked positions. Let the vocabulary size be $V$, the embedding dimension be $d$, and the output classification layer be a linear map $U \\in \\mathbb{R}^{V \\times d}$ followed by a softmax. For a given training example, suppose there are $K$ disjoint masked spans indexed by $k \\in \\{1,\\dots,K\\}$, with the set of masked positions in span $k$ denoted $S^{(k)}$.\n\nFor each masked position $i \\in S^{(k)}$, the transformer produces a hidden state $h_{i} \\in \\mathbb{R}^{d}$ which is fed to the output head to produce logits $z_{i} = U h_{i} \\in \\mathbb{R}^{V}$ and probabilities $p_{i} = \\mathrm{softmax}(z_{i}) \\in \\mathbb{R}^{V}$. Let the ground-truth index at position $i$ be $y_{i} \\in \\{1,\\dots,V\\}$, and let $e_{y_{i}} \\in \\mathbb{R}^{V}$ be the one-hot vector for $y_{i}$. Define the per-example MLM loss as\n$$\n\\mathcal{L}_{\\mathrm{MLM}} = \\sum_{k=1}^{K} \\sum_{i \\in S^{(k)}} \\left( - \\ln p_{i}[y_{i}] \\right).\n$$\n\nAssume the following first-order local linearization of the transformer around the masked inputs:\n- In the shared mask token setting, a single global parameter vector $m \\in \\mathbb{R}^{d}$ is used as the input embedding for every masked token in every span, and the hidden states satisfy\n$$\nh_{i} = A_{i} m + b_{i},\n$$\nfor each masked position $i$, where $A_{i} \\in \\mathbb{R}^{d \\times d}$ and $b_{i} \\in \\mathbb{R}^{d}$ do not depend on $m$.\n\n- In the learned span-level mask embeddings setting, each span $k$ has its own parameter vector $s^{(k)} \\in \\mathbb{R}^{d}$ used as the input embedding for every masked token in that span, and the hidden states satisfy\n$$\nh_{i} = A_{i} s^{(k)} + b_{i},\n$$\nfor each $i \\in S^{(k)}$, with the same $A_{i}$ and $b_{i}$ as above that do not depend on $s^{(k)}$.\n\nStarting from core definitions of the softmax function and cross-entropy, and using only standard calculus (including the chain rule), derive closed-form analytic expressions for the gradient of $\\mathcal{L}_{\\mathrm{MLM}}$ with respect to:\n- the shared mask token $m$;\n- the learned span-level mask embedding $s^{(k)}$ for an arbitrary fixed span index $k$.\n\nExpress your final answer as a single row matrix whose first entry is $\\nabla_{m} \\mathcal{L}_{\\mathrm{MLM}}$ and whose second entry is $\\nabla_{s^{(k)}} \\mathcal{L}_{\\mathrm{MLM}}$. No numerical approximation or rounding is required, and no physical units are involved. Your derivation must be self-contained and must not invoke any shortcut formulas beyond the stated definitions.",
            "solution": "The problem requires the derivation of the gradients of the Masked Language Modeling (MLM) loss, $\\mathcal{L}_{\\mathrm{MLM}}$, with respect to two sets of parameters: a shared mask token embedding $m$ and a set of span-level mask embeddings $s^{(k)}$. The derivation will be built from first principles using the chain rule of calculus.\n\nThe MLM loss is defined as the sum of negative log-likelihoods over all masked positions:\n$$\n\\mathcal{L}_{\\mathrm{MLM}} = \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} \\mathcal{L}_i\n$$\nwhere $\\mathcal{L}_i = - \\ln p_{i}[y_{i}]$ is the loss for a single token at position $i$. The index $k'$ is used as a dummy summation variable over the spans to distinguish it from the fixed index $k$ in the second part of the problem.\n\nThe sequence of computations for a single position $i$ is:\n$1$. The input embedding ($m$ or $s^{(k)}$) is used to compute the hidden state $h_i$.\n$2$. The hidden state $h_i \\in \\mathbb{R}^d$ is used to compute the logits $z_i \\in \\mathbb{R}^V$ via $z_i = U h_i$.\n$3$. The logits $z_i$ are converted to probabilities $p_i \\in \\mathbb{R}^V$ via $p_i = \\mathrm{softmax}(z_i)$.\n$4$. The loss $\\mathcal{L}_i$ is calculated from $p_i$ and the ground-truth index $y_i$.\n\nWe will compute the gradient by applying the chain rule backward through this computation graph.\n\n**Step 1: Gradient of $\\mathcal{L}_i$ with respect to the logits $z_i$**\n\nThe loss for position $i$ is $\\mathcal{L}_i = -\\ln p_i[y_i]$. The probability $p_i[j]$ for any class index $j \\in \\{1, \\dots, V\\}$ is given by the softmax function:\n$$\np_i[j] = \\frac{\\exp(z_i[j])}{\\sum_{l=1}^{V} \\exp(z_i[l])}\n$$\nwhere $z_i[j]$ is the $j$-th component of the logit vector $z_i$.\n\nSubstituting this into the loss function for the true class $y_i$:\n$$\n\\mathcal{L}_i = - \\ln \\left( \\frac{\\exp(z_i[y_i])}{\\sum_{l=1}^{V} \\exp(z_i[l])} \\right) = -z_i[y_i] + \\ln\\left(\\sum_{l=1}^{V} \\exp(z_i[l])\\right)\n$$\nWe now compute the partial derivative of $\\mathcal{L}_i$ with respect to an arbitrary component $z_i[j]$ of the logit vector $z_i$.\n\nIf $j = y_i$:\n$$\n\\frac{\\partial \\mathcal{L}_i}{\\partial z_i[y_i]} = -1 + \\frac{1}{\\sum_{l=1}^{V} \\exp(z_i[l])} \\cdot \\frac{\\partial}{\\partial z_i[y_i]} \\left( \\sum_{l=1}^{V} \\exp(z_i[l]) \\right) = -1 + \\frac{\\exp(z_i[y_i])}{\\sum_{l=1}^{V} \\exp(z_i[l])} = p_i[y_i] - 1\n$$\nIf $j \\neq y_i$:\n$$\n\\frac{\\partial \\mathcal{L}_i}{\\partial z_i[j]} = 0 + \\frac{1}{\\sum_{l=1}^{V} \\exp(z_i[l])} \\cdot \\frac{\\partial}{\\partial z_i[j]} \\left( \\sum_{l=1}^{V} \\exp(z_i[l]) \\right) = \\frac{\\exp(z_i[j])}{\\sum_{l=1}^{V} \\exp(z_i[l])} = p_i[j]\n$$\nThese two cases can be combined into a single vector expression. Let $e_{y_i}$ be the one-hot vector where the component corresponding to index $y_i$ is $1$ and all others are $0$. The gradient of the scalar $\\mathcal{L}_i$ with respect to the vector $z_i$ is:\n$$\n\\nabla_{z_i} \\mathcal{L}_i = p_i - e_{y_i}\n$$\nThis vector $\\nabla_{z_i} \\mathcal{L}_i \\in \\mathbb{R}^V$ represents the error signal at the logit level.\n\n**Step 2: Gradient of $\\mathcal{L}_i$ with respect to the hidden state $h_i$**\n\nThe logits are a linear function of the hidden state: $z_i = U h_i$, where $U \\in \\mathbb{R}^{V \\times d}$ and $h_i \\in \\mathbb{R}^d$. We use the chain rule for vector-valued functions. The gradient of the scalar $\\mathcal{L}_i$ with respect to the vector $h_i$ is:\n$$\n\\nabla_{h_i} \\mathcal{L}_i = \\left(\\frac{\\partial z_i}{\\partial h_i}\\right)^T (\\nabla_{z_i} \\mathcal{L}_i)\n$$\nThe term $\\frac{\\partial z_i}{\\partial h_i}$ is the Jacobian matrix of the function $z_i(h_i)$. Since $z_i = U h_i$, this Jacobian is simply the matrix $U$. Therefore:\n$$\n\\nabla_{h_i} \\mathcal{L}_i = U^T (\\nabla_{z_i} \\mathcal{L}_i) = U^T (p_i - e_{y_i})\n$$\nThis gradient, which we denote as $g_i = \\nabla_{h_i} \\mathcal{L}_i$, is a vector in $\\mathbb{R}^d$.\n\n**Step 3: Derivation of $\\nabla_{m} \\mathcal{L}_{\\mathrm{MLM}}$**\n\nIn the shared mask token setting, the hidden state $h_i$ for every masked position $i$ is a linear function of a single shared parameter vector $m \\in \\mathbb{R}^d$:\n$$\nh_i = A_i m + b_i\n$$\nThe total loss $\\mathcal{L}_{\\mathrm{MLM}}$ is a function of $m$ through its dependence on every $h_i$. By linearity of the gradient operator and the chain rule:\n$$\n\\nabla_{m} \\mathcal{L}_{\\mathrm{MLM}} = \\nabla_{m} \\left( \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} \\mathcal{L}_i(h_i(m)) \\right) = \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} \\nabla_{m} \\mathcal{L}_i\n$$\nFor each term $\\mathcal{L}_i$, we apply the chain rule again:\n$$\n\\nabla_{m} \\mathcal{L}_i = \\left(\\frac{\\partial h_i}{\\partial m}\\right)^T (\\nabla_{h_i} \\mathcal{L}_i)\n$$\nThe Jacobian of the function $h_i(m)$ is the matrix $A_i \\in \\mathbb{R}^{d \\times d}$. Substituting this and the expression for $\\nabla_{h_i} \\mathcal{L}_i = g_i$:\n$$\n\\nabla_{m} \\mathcal{L}_i = A_i^T g_i = A_i^T U^T (p_i - e_{y_i})\n$$\nSumming over all masked positions gives the final gradient with respect to $m$:\n$$\n\\nabla_{m} \\mathcal{L}_{\\mathrm{MLM}} = \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} A_i^T U^T (p_i - e_{y_i})\n$$\n\n**Step 4: Derivation of $\\nabla_{s^{(k)}} \\mathcal{L}_{\\mathrm{MLM}}$**\n\nIn the span-level mask embeddings setting, for a fixed span index $k$, the parameter vector $s^{(k)} \\in \\mathbb{R}^d$ influences only the hidden states $h_i$ where $i \\in S^{(k)}$. For these positions, the relationship is:\n$$\nh_i = A_i s^{(k)} + b_i \\quad \\text{for } i \\in S^{(k)}\n$$\nFor any position $j$ in a different span, $j \\in S^{(k')}$ with $k' \\neq k$, the hidden state $h_j$ depends on $s^{(k')}$, not $s^{(k)}$. Therefore, $\\nabla_{s^{(k)}} \\mathcal{L}_j = 0$ for $j \\notin S^{(k)}$.\n\nThe gradient of the total loss with respect to $s^{(k)}$ is:\n$$\n\\nabla_{s^{(k)}} \\mathcal{L}_{\\mathrm{MLM}} = \\nabla_{s^{(k)}} \\left( \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} \\mathcal{L}_i \\right) = \\sum_{i \\in S^{(k)}} \\nabla_{s^{(k)}} \\mathcal{L}_i\n$$\nThe sum is only over the positions in span $k$. For each term in this sum, we apply the chain rule:\n$$\n\\nabla_{s^{(k)}} \\mathcal{L}_i = \\left(\\frac{\\partial h_i}{\\partial s^{(k)}}\\right)^T (\\nabla_{h_i} \\mathcal{L}_i) \\quad \\text{for } i \\in S^{(k)}\n$$\nThe Jacobian $\\frac{\\partial h_i}{\\partial s^{(k)}}$ is the matrix $A_i$. Substituting this and the expression for $g_i = \\nabla_{h_i} \\mathcal{L}_i$:\n$$\n\\nabla_{s^{(k)}} \\mathcal{L}_i = A_i^T g_i = A_i^T U^T (p_i - e_{y_i})\n$$\nSumming over all positions within span $k$ gives the final gradient with respect to $s^{(k)}$:\n$$\n\\nabla_{s^{(k)}} \\mathcal{L}_{\\mathrm{MLM}} = \\sum_{i \\in S^{(k)}} A_i^T U^T (p_i - e_{y_i})\n$$\nThe results are the closed-form analytic expressions for the requested gradients.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} A_i^T U^T (p_i - e_{y_i}) & \\sum_{i \\in S^{(k)}} A_i^T U^T (p_i - e_{y_i}) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A powerful way to reason about predictive models is through the lens of information theory, where the goal is to minimize uncertainty. This final exercise connects the MLM objective to the concept of conditional entropy, exploring how providing the model with extra side-information can reduce the ambiguity of its prediction task . By quantifying the optimal loss reduction, you will gain a deeper appreciation for how different masking strategies can be formally analyzed in terms of the information they provide or conceal.",
            "id": "3164769",
            "problem": "Consider a Transformer trained with the Masked Language Modeling (MLM) objective. Let the context be a random variable $C$, the masked word be a random variable $W$, and the part-of-speech be a random variable $P$ taking values in a finite set $\\{p_{1}, p_{2}, \\dots, p_{k}\\}$. The MLM training minimizes the expected Negative Log-Likelihood (NLL), which, under an optimal model with sufficient capacity and data, equals the cross-entropy between the true data-generating distribution and the model distribution. Assume the following data-generating process at masked positions:\n- Given $C$, first sample $P$ according to $p(P \\mid C)$.\n- Given $P$, sample $W$ uniformly from a disjoint vocabulary subset $\\mathcal{V}_{P}$ of size $V_{P}$.\n\nTwo masking schemes are considered:\n1. A single generic mask embedding $m$ that conveys no explicit information about $P$ beyond what is inferable from $C$.\n2. A specialized mask embedding $m_{p}$ per part-of-speech that deterministically reveals the true $P$ at the masked position to the model.\n\nStarting only from core definitions of cross-entropy and conditional entropy, derive a closed-form expression for the difference in the optimal expected NLL between the generic mask and the specialized mask schemes for a fixed context $C$. Then, for a specific context where the part-of-speech distribution is $p(P=\\text{noun} \\mid C)=0.6$, $p(P=\\text{verb} \\mid C)=0.3$, and $p(P=\\text{adjective} \\mid C)=0.1$, compute the numerical value of the reduction in the optimal expected NLL achieved by the specialized mask relative to the generic mask. Use natural logarithms and express your final numerical reduction in nats. Round your answer to four significant figures.",
            "solution": "The supplied problem is subjected to rigorous validation.\n\n**Step 1: Extract Givens**\n- Let $C$ be a random variable for the context.\n- Let $W$ be a random variable for the masked word.\n- Let $P$ be a random variable for the part-of-speech, with a finite state space $\\{p_{1}, p_{2}, \\dots, p_{k}\\}$.\n- The training objective is minimizing the expected Negative Log-Likelihood (NLL).\n- For an optimal model, the expected NLL is equal to the cross-entropy, which simplifies to the entropy of the true data-generating distribution.\n- The data-generating process is modeled as:\n    1. Given a context $C$, the part-of-speech $P$ is sampled according to the conditional probability distribution $p(P \\mid C)$.\n    2. Given a part-of-speech $P$, the word $W$ is sampled uniformly from a disjoint vocabulary subset $\\mathcal{V}_{P}$ of size $V_{P}$. This implies the Markov chain $C \\rightarrow P \\rightarrow W$.\n- Masking Scheme 1 (Generic): A single mask embedding $m$ is used. The model input is the context $C$.\n- Masking Scheme 2 (Specialized): A specific mask embedding $m_{p}$ is used for each part-of-speech $p$, deterministically revealing the true $P$ of the masked word. The model input is the context $C$ and the part-of-speech $P$.\n- The task is to derive the difference in optimal expected NLL between the two schemes for a fixed context $C$, and then compute a numerical value for a specific case.\n- For the numerical calculation, the specific conditional distribution is $p(P=\\text{noun} \\mid C)=0.6$, $p(P=\\text{verb} \\mid C)=0.3$, and $p(P=\\text{adjective} \\mid C)=0.1$.\n- Natural logarithms (base $e$) are to be used, with the result in nats.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in information theory and its application to deep learning, specifically transformer-based language models. It is well-posed, providing a clear data-generating model and sufficient constraints to arrive at a unique, meaningful solution. The terminology is precise and objective. The problem does not violate any of the invalidity criteria. The fact that the vocabulary subset sizes $V_P$ are not provided for the numerical part is not a flaw, as a thorough derivation demonstrates they cancel out from the final expression for the *difference* in NLL.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A complete solution will be provided.\n\n**Derivation of the Closed-Form Expression**\n\nThe optimal expected Negative Log-Likelihood (NLL) for a predictive model is the entropy of the target variable, conditioned on the information available to the model. We analyze the NLL for a fixed context $C$.\n\nLet $NLL_{generic}$ be the optimal expected NLL for the generic masking scheme. In this case, the model must predict the word $W$ given only the context $C$. Therefore, the NLL is the conditional entropy of $W$ given $C$:\n$$NLL_{generic} = H(W \\mid C)$$\n\nLet $NLL_{specialized}$ be the optimal expected NLL for the specialized masking scheme. Here, the model is given both the context $C$ and the true part-of-speech $P$ of the masked word. The task is still to predict $W$. Therefore, the NLL is the conditional entropy of $W$ given both $C$ and $P$:\n$$NLL_{specialized} = H(W \\mid C, P)$$\n\nThe problem asks for the difference in these quantities, which we denote as $\\Delta NLL$:\n$$\\Delta NLL = NLL_{generic} - NLL_{specialized} = H(W \\mid C) - H(W \\mid C, P)$$\nBy the chain rule of entropy, $H(X, Y | Z) = H(X | Y, Z) + H(Y | Z)$. Applying this to our variables, we have:\n$$H(W, P \\mid C) = H(W \\mid P, C) + H(P \\mid C)$$\nRearranging this equation gives:\n$$H(W \\mid C) - H(W \\mid C, P) = H(P \\mid C) - H(P \\mid W, C)$$\nThis expression, $I(P; W \\mid C)$, is the conditional mutual information. However, a more direct derivation based on the problem's premises is more instructive.\n\nLet us first compute $NLL_{specialized} = H(W \\mid C, P)$. By definition of conditional entropy:\n$$H(W \\mid C, P) = \\sum_{p \\in P} p(p \\mid C) H(W \\mid C, P=p)$$\nThe problem states a Markov chain $C \\rightarrow P \\rightarrow W$, which means $W$ is conditionally independent of $C$ given $P$. Thus, $p(W \\mid C, P=p) = p(W \\mid P=p)$, and consequently $H(W \\mid C, P=p) = H(W \\mid P=p)$.\nThe problem also specifies that given $P=p$, the word $W$ is drawn from a uniform distribution over the vocabulary subset $\\mathcal{V}_p$ of size $V_p$. The entropy of a uniform distribution over $V_p$ outcomes is $\\ln(V_p)$. Therefore, $H(W \\mid P=p) = \\ln(V_p)$.\nSubstituting this back, we get:\n$$NLL_{specialized} = \\sum_{p} p(p \\mid C) \\ln(V_p)$$\n\nNow, let us compute $NLL_{generic} = H(W \\mid C)$. By definition:\n$$H(W \\mid C) = - \\sum_{w \\in \\mathcal{V}} p(w \\mid C) \\ln p(w \\mid C)$$\nTo evaluate this, we first need the distribution $p(w \\mid C)$. Using the law of total probability and the Markov assumption:\n$$p(w \\mid C) = \\sum_{p} p(w, p \\mid C) = \\sum_{p} p(p \\mid C) p(w \\mid p, C) = \\sum_{p} p(p \\mid C) p(w \\mid p)$$\nThe problem states the vocabulary subsets $\\mathcal{V}_p$ are disjoint. This means for any word $w$, it can belong to at most one subset, which we may denote $\\mathcal{V}_{p_w}$. For this specific $p_w$, $p(w \\mid p_w) = 1/V_{p_w}$, and for any other $p \\neq p_w$, $p(w \\mid p) = 0$. Consequently, the sum over $p$ collapses to a single term:\n$$p(w \\mid C) = p(p_w \\mid C) \\frac{1}{V_{p_w}} \\quad \\text{for } w \\in \\mathcal{V}_{p_w}$$\nWe can now substitute this into the entropy formula for $H(W \\mid C)$. We group the sum over all words $w$ into sums over the disjoint subsets $\\mathcal{V}_p$:\n$$H(W \\mid C) = - \\sum_{p} \\sum_{w \\in \\mathcal{V}_p} p(w \\mid C) \\ln p(w \\mid C)$$\nFor any $w \\in \\mathcal{V}_p$, the term $p(w \\mid C)$ is constant and equal to $\\frac{p(p \\mid C)}{V_p}$. The inner sum consists of $V_p$ identical terms:\n$$H(W \\mid C) = - \\sum_{p} V_p \\left( \\frac{p(p \\mid C)}{V_p} \\ln\\left(\\frac{p(p \\mid C)}{V_p}\\right) \\right)$$\n$$H(W \\mid C) = - \\sum_{p} p(p \\mid C) \\left( \\ln(p(p \\mid C)) - \\ln(V_p) \\right)$$\n$$H(W \\mid C) = - \\sum_{p} p(p \\mid C) \\ln(p(p \\mid C)) + \\sum_{p} p(p \\mid C) \\ln(V_p)$$\nThe first term is the definition of the conditional entropy of $P$ given $C$, denoted $H(P \\mid C)$. The second term is exactly our previously derived expression for $NLL_{specialized}$.\n$$H(W \\mid C) = H(P \\mid C) + NLL_{specialized}$$\nTherefore, the difference in optimal NLL is:\n$$\\Delta NLL = NLL_{generic} - NLL_{specialized} = H(W \\mid C) - NLL_{specialized} = H(P \\mid C)$$\nThe closed-form expression for the reduction in NLL is simply the conditional entropy of the part-of-speech distribution given the context, $H(P \\mid C)$. This result is intuitive: the advantage of the specialized mask is that it resolves all uncertainty about the part-of-speech, and this reduction in uncertainty is quantified precisely by $H(P \\mid C)$.\n\n**Numerical Calculation**\n\nWe are asked to compute the value of the NLL reduction for a specific context $C$ where the part-of-speech distribution is given. The reduction is $\\Delta NLL = H(P \\mid C)$.\nThe formula for this entropy is:\n$$H(P \\mid C) = - \\sum_{i} p(P=p_i \\mid C) \\ln (p(P=p_i \\mid C))$$\nThe given probabilities are:\n$p(P=\\text{noun} \\mid C) = 0.6$\n$p(P=\\text{verb} \\mid C) = 0.3$\n$p(P=\\text{adjective} \\mid C) = 0.1$\n\nSubstituting these values into the entropy formula:\n$$H(P \\mid C) = - \\left( 0.6 \\ln(0.6) + 0.3 \\ln(0.3) + 0.1 \\ln(0.1) \\right)$$\nWe compute the terms individually:\n$0.6 \\ln(0.6) \\approx 0.6 \\times (-0.5108256) \\approx -0.30649536$\n$0.3 \\ln(0.3) \\approx 0.3 \\times (-1.2039728) \\approx -0.36119184$\n$0.1 \\ln(0.1) \\approx 0.1 \\times (-2.3025851) \\approx -0.23025851$\n\nSumming these terms:\n$$-0.30649536 - 0.36119184 - 0.23025851 = -0.89794571$$\nThe entropy is the negative of this sum:\n$$H(P \\mid C) = -(-0.89794571) = 0.89794571 \\text{ nats}$$\nRounding to four significant figures, we get $0.8979$.",
            "answer": "$$\\boxed{0.8979}$$"
        }
    ]
}