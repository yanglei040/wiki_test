{
    "hands_on_practices": [
        {
            "introduction": "标准的缩放点积注意力机制虽然十分强大，但其核心的点积操作对输入向量的范数（norm）非常敏感。本练习将引导你通过一个“对抗性劫持”的假想场景，亲手揭示这一内在漏洞，并实现两种有效的防御策略：范数裁剪和余弦相似度归一化。通过这个实践，你将超越公式本身，深刻理解注意力分数的本质及其稳健性问题。",
            "id": "3193536",
            "problem": "考虑 Transformer 架构中使用的缩放点积注意力 (Scaled Dot-Product Attention, SDPA) 机制。给定一组查询 (Query, Q)、键 (Key, K) 和值 (Value, V) 向量，注意力权重是通过对一组注意力 logit 应用 softmax 函数形成的。注意力 logit 源自查询与每个键之间的内积，并由键维度平方根进行缩放。softmax 函数将任意实值分数转换为键上的概率分布。假设以下经过充分检验的定义：向量之间的内积是逐元素乘积之和，欧几里得范数是元素平方和的平方根，softmax 函数通过对每个元素取幂并除以所有指数的总和进行归一化，将一个实数向量映射到一个总和为一的非负实数向量。\n\n当攻击者添加一个具有异常大范数的新键令牌 (key token) 时，可能会发生对抗性的“注意力劫持”，从而与查询产生巨大的内积并主导 softmax 分布。本问题要求您通过计算来演示这种攻击如何主导注意力，并实现两种缓解策略：键的范数裁剪 (norm clipping) 和 logit 的余弦相似度归一化 (cosine similarity normalization)。\n\n您必须从基本原理出发实现以下内容：\n- 对于单个查询 $Q$ 和一组维度为 $d_k$ 的键 $\\{K_i\\}$，计算缩放点积注意力 logit，将每个 logit 乘以 $1/\\sqrt{d_k}$ 进行缩放，并应用 softmax 函数以获得注意力权重。\n- 通过附加一个方向已指定且范数被设置为一个较大值的攻击键 $K_{\\text{attack}}$，来演示对抗性劫持。\n- 实现两种缓解措施：\n    1. 范数裁剪：对于一个裁剪上限 $c$，将每个键 $K_i$ 替换为 $\\tilde{K}_i = \\min\\left(1, \\frac{c}{\\|K_i\\|}\\right) K_i$，此操作应用于包括 $K_{\\text{attack}}$ 在内的所有键。\n    2. 余弦相似度归一化：在应用 softmax 函数之前，将每个 logit 替换为 $\\frac{Q^\\top K_i}{\\|Q\\| \\|K_i\\| + \\varepsilon}$，其中 $\\varepsilon > 0$ 是一个很小的数。\n\n对于下方的每个测试用例，计算四个量：\n- 基线条件下攻击令牌的注意力权重（攻击令牌范数设置为 $1$）。\n- 对抗性条件下攻击令牌的注意力权重（攻击令牌范数设置为一个较大值）。\n- 对抗性键应用范数裁剪后的攻击令牌的注意力权重（裁剪上限为 $c$）。\n- 对抗性键应用余弦相似度归一化后的攻击令牌的注意力权重（使用一个很小的 $\\varepsilon$ 以避免除以零）。\n\n您的程序必须使用以下测试套件。在每种情况下，攻击令牌都作为最后一个键附加。请完全按照给定的向量使用。\n\n测试用例 1（顺利路径：查询与攻击方向对齐）：\n- 维度 $d_k = 4$\n- 查询 $Q = [0.9, 0.1, 0.0, 0.0]$\n- 正常键：$K_1 = [1, 0, 0, 0]$，$K_2 = [0, 1, 0, 0]$，$K_3 = [0, 0, 1, 0]$\n- 攻击方向：$K_{\\text{dir}} = [1, 0, 0, 0]$\n- 基线攻击范数：$1$\n- 对抗性攻击范数：$100$\n- 裁剪上限：$2$\n\n测试用例 2（边界情况：零查询向量）：\n- 维度 $d_k = 4$\n- 查询 $Q = [0.0, 0.0, 0.0, 0.0]$\n- 正常键：$K_1 = [1, 0, 0, 0]$，$K_2 = [0, 1, 0, 0]$，$K_3 = [0, 0, 1, 0]$\n- 攻击方向：$K_{\\text{dir}} = [0, 0, 0, 1]$\n- 基线攻击范数：$1$\n- 对抗性攻击范数：$100$\n- 裁剪上限：$2$\n\n测试用例 3（查询与攻击方向正交）：\n- 维度 $d_k = 4$\n- 查询 $Q = [0.0, 1.0, 0.0, 0.0]$\n- 正常键：$K_1 = [0, 1, 0, 0]$，$K_2 = [0, 0, 1, 0]$，$K_3 = [0, 0, 0, 1]$\n- 攻击方向：$K_{\\text{dir}} = [1, 0, 0, 0]$\n- 基线攻击范数：$1$\n- 对抗性攻击范数：$100$\n- 裁剪上限：$2$\n\n测试用例 4（极端的对抗性范数）：\n- 维度 $d_k = 8$\n- 查询 $Q = [1.0, -0.5, 0.3, -0.2, 0.0, 0.0, 0.0, 0.0]$\n- 正常键：$K_1 = [0, 1, 0, 0, 0, 0, 0, 0]$，$K_2 = [0, 0, 1, 0, 0, 0, 0, 0]$，$K_3 = [0, 0, 0, 1, 0, 0, 0, 0]$\n- 攻击方向：$K_{\\text{dir}} = [1, 0, 0, 0, 0, 0, 0, 0]$\n- 基线攻击范数：$1$\n- 对抗性攻击范数：$10^6$\n- 裁剪上限：$10$\n\n您的程序必须为每个测试用例计算一个包含四个浮点数的列表，顺序如上所述。程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果，每个元素是对应测试用例的四元数列表。例如：$[[w_{1,\\text{base}}, w_{1,\\text{adv}}, w_{1,\\text{clip}}, w_{1,\\text{cos}}],[w_{2,\\text{base}}, w_{2,\\text{adv}}, w_{2,\\text{clip}}, w_{2,\\text{cos}}],\\ldots]$。",
            "solution": "该问题要求实现并演示对缩放点积注意力 (SDPA) 机制的对抗性攻击，以及两种缓解策略。我将首先对操作进行形式化，然后概述问题每个部分的计算步骤。\n\n### 1. 基本概念\n\n**缩放点积注意力：**\n注意力机制基于查询向量 $Q$，计算一组键值对上的权重分布 $\\alpha_i$。对于单个查询 $Q \\in \\mathbb{R}^{d_k}$ 和一组键 $\\{K_1, K_2, \\ldots, K_N\\}$，其中每个 $K_i \\in \\mathbb{R}^{d_k}$，注意力 logit $e_i$ 计算如下：\n$$\ne_i = \\frac{Q^\\top K_i}{\\sqrt{d_k}}\n$$\n此处，$d_k$ 是键向量的维度。然后通过对 logit 向量应用 softmax 函数获得注意力权重 $\\alpha_i$：\n$$\n\\alpha_i = \\text{softmax}(e)_i = \\frac{\\exp(e_i)}{\\sum_{j=1}^{N} \\exp(e_j)}\n$$\n注意力层的输出是值向量的加权和 $\\sum_i \\alpha_i V_i$，但这不属于本问题的范围。\n\n**对抗性劫持：**\n内积 $Q^\\top K_i$ 与键向量的范数 $\\|K_i\\|$ 成正比。攻击者可以利用这一点，引入一个具有异常大范数的对抗性键 $K_{\\text{attack}}$。如果 $K_{\\text{attack}}$ 与 $Q$ 不正交，得到的点积 $Q^\\top K_{\\text{attack}}$ 将会很大，从而导致一个很大的 logit $e_{\\text{attack}}$。因此，在取指数后，$\\exp(e_{\\text{attack}})$ 将在 softmax 的分母求和中占主导地位，导致注意力权重 $\\alpha_{\\text{attack}}$ 趋近于 1。这实际上迫使模型几乎只关注对抗性令牌，从而“劫持”了注意力机制。\n\n### 2. 缓解策略\n\n需要实现两种缓解策略。\n\n**缓解策略 1：范数裁剪**\n该方法对所有键向量强制施加一个最大范数 $c$。每个键 $K_i$ 都被替换为一个裁剪后的版本 $\\tilde{K}_i$：\n$$\n\\tilde{K}_i = \\min\\left(1, \\frac{c}{\\|K_i\\|}\\right) K_i\n$$\n如果一个键的范数 $\\|K_i\\|$ 已经小于或等于裁剪上限 $c$，则它保持不变。如果 $\\|K_i\\| > c$，它将被按比例缩小，使其新范数恰好为 $c$。这可以防止任何单个键具有任意大的范数，从而对 logit 产生过大的影响。对于此计算，需要注意范数为零的键可能导致除以零，但在测试数据中不存在这种情况。\n\n**缓解策略 2：余弦相似度归一化**\n该策略用查询和键向量之间的余弦相似度替换缩放点积 logit。logit $e_i$ 被重新定义为：\n$$\ne'_i = \\frac{Q^\\top K_i}{\\|Q\\| \\|K_i\\| + \\varepsilon}\n$$\n余弦相似度通过向量的模长进行内在归一化，使其对范数不敏感。得到的 logit 值被限制在 $[-1, 1]$ 区间内。添加一个小的常数 $\\varepsilon > 0$ 是为了数值稳定性，以防止当 $Q$ 或 $K_i$ 是零向量时出现除以零的情况。在此实现中，将使用标准值 $\\varepsilon = 10^{-8}$。请注意，此公式替换了整个缩放点积，包括 $1/\\sqrt{d_k}$ 缩放因子。\n\n### 3. 计算流程\n\n对于每个测试用例，我们必须在四种不同场景下计算攻击令牌的注意力权重。攻击键 $K_{\\text{attack}}$ 是通过将给定的方向向量 $K_{\\text{dir}}$ 缩放到指定的范数来构建的。如果 $\\|K_{\\text{dir}}\\| \\neq 0$，则 $K_{\\text{attack}} = (\\text{norm} / \\|K_{\\text{dir}}\\| ) \\cdot K_{\\text{dir}}$。攻击键总是作为序列中的最后一个键附加。\n\n设正常键的集合为 $\\{K_1, \\ldots, K_{N-1}\\}$，攻击键为 $K_N = K_{\\text{attack}}$。\n\n**A. 基线注意力：**\n1. 使用指定的 `baseline_attack_norm` 构建 $K_{\\text{attack}}$。\n2. 形成完整的键集合 $\\{K_1, \\ldots, K_{N-1}, K_{\\text{attack}}\\}$。\n3. 对所有 $i=1, \\ldots, N$，计算缩放点积 logit $e_i = \\frac{Q^\\top K_i}{\\sqrt{d_k}}$。\n4. 应用 softmax 函数以获得权重 $\\{\\alpha_1, \\ldots, \\alpha_N\\}$。\n5. 结果为 $\\alpha_N$。\n\n**B. 对抗性注意力：**\n1. 使用 `adversarial_attack_norm` 构建 $K_{\\text{attack}}$。\n2. 使用这个新的对抗性键形成完整的键集合。\n3. 重复基线流程中的步骤 3-5。\n4. 结果为新的 $\\alpha_N$。\n\n**C. 范数裁剪缓解：**\n1. 使用来自对抗性场景的键集合，包括高范数的 $K_{\\text{attack}}$。\n2. 使用指定的裁剪上限 $c$，将范数裁剪公式应用于每个键 $K_i$，以生成一组裁剪后的键 $\\{\\tilde{K}_1, \\ldots, \\tilde{K}_N\\}$。\n3. 使用这些裁剪后的键计算缩放点积 logit：$\\tilde{e}_i = \\frac{Q^\\top \\tilde{K}_i}{\\sqrt{d_k}}$。\n4. 应用 softmax 函数以获得权重 $\\{\\tilde{\\alpha}_1, \\ldots, \\tilde{\\alpha}_N\\}$。\n5. 结果为 $\\tilde{\\alpha}_N$。\n\n**D. 余弦相似度缓解：**\n1. 使用来自对抗性场景的键集合。\n2. 对每个键使用余弦相似度公式计算 logit：$e'_i = \\frac{Q^\\top K_i}{\\|Q\\| \\|K_i\\| + \\varepsilon}$。\n3. 对这些新的 logit $\\{e'_1, \\ldots, e'_N\\}$ 应用 softmax 函数以获得权重 $\\{\\alpha'_1, \\ldots, \\alpha'_N\\}$。\n4. 结果为 $\\alpha'_N$。\n\n按顺序计算的这四个值构成了单个测试用例的解。最终输出按规定汇总所有测试用例的结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes attentional hijacking and mitigation effects for SDPA.\n    \"\"\"\n    # A small constant for numerical stability in cosine similarity.\n    EPSILON = 1e-8\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"d_k\": 4,\n            \"Q\": [0.9, 0.1, 0.0, 0.0],\n            \"normal_keys\": [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]],\n            \"attack_direction\": [1, 0, 0, 0],\n            \"baseline_attack_norm\": 1,\n            \"adversarial_attack_norm\": 100,\n            \"clip_cap\": 2,\n        },\n        {\n            \"d_k\": 4,\n            \"Q\": [0.0, 0.0, 0.0, 0.0],\n            \"normal_keys\": [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]],\n            \"attack_direction\": [0, 0, 0, 1],\n            \"baseline_attack_norm\": 1,\n            \"adversarial_attack_norm\": 100,\n            \"clip_cap\": 2,\n        },\n        {\n            \"d_k\": 4,\n            \"Q\": [0.0, 1.0, 0.0, 0.0],\n            \"normal_keys\": [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]],\n            \"attack_direction\": [1, 0, 0, 0],\n            \"baseline_attack_norm\": 1,\n            \"adversarial_attack_norm\": 100,\n            \"clip_cap\": 2,\n        },\n        {\n            \"d_k\": 8,\n            \"Q\": [1.0, -0.5, 0.3, -0.2, 0.0, 0.0, 0.0, 0.0],\n            \"normal_keys\": [\n                [0, 1, 0, 0, 0, 0, 0, 0],\n                [0, 0, 1, 0, 0, 0, 0, 0],\n                [0, 0, 0, 1, 0, 0, 0, 0],\n            ],\n            \"attack_direction\": [1, 0, 0, 0, 0, 0, 0, 0],\n            \"baseline_attack_norm\": 1,\n            \"adversarial_attack_norm\": 1e6,\n            \"clip_cap\": 10,\n        },\n    ]\n\n    def softmax(x):\n        \"\"\"Computes softmax of vector x for numerical stability.\"\"\"\n        if x.size == 0:\n            return np.array([])\n        e_x = np.exp(x - np.max(x))\n        return e_x / np.sum(e_x)\n\n    def create_attack_key(direction, norm):\n        \"\"\"Creates a key vector from a direction and a norm.\"\"\"\n        direction_vec = np.array(direction, dtype=float)\n        dir_norm = np.linalg.norm(direction_vec)\n        if dir_norm == 0:\n            return np.zeros_like(direction_vec)\n        return norm * (direction_vec / dir_norm)\n\n    def calculate_sdpa_weights(Q, keys, d_k):\n        \"\"\"Computes weights using scaled dot-product attention.\"\"\"\n        logits = np.array([np.dot(Q, k) for k in keys]) / np.sqrt(d_k)\n        return softmax(logits)\n\n    def calculate_cosine_sim_weights(Q, keys):\n        \"\"\"Computes weights using cosine similarity logits.\"\"\"\n        Q_norm = np.linalg.norm(Q)\n        logits = []\n        for k in keys:\n            k_norm = np.linalg.norm(k)\n            dot_product = np.dot(Q, k)\n            logit = dot_product / (Q_norm * k_norm + EPSILON)\n            logits.append(logit)\n        return softmax(np.array(logits))\n\n    results = []\n    for case in test_cases:\n        Q_vec = np.array(case[\"Q\"], dtype=float)\n        normal_keys_vecs = [np.array(k, dtype=float) for k in case[\"normal_keys\"]]\n        d_k = case[\"d_k\"]\n        clip_cap = case[\"clip_cap\"]\n\n        # 1. Baseline calculation\n        K_attack_base = create_attack_key(case[\"attack_direction\"], case[\"baseline_attack_norm\"])\n        all_keys_base = normal_keys_vecs + [K_attack_base]\n        weights_base = calculate_sdpa_weights(Q_vec, all_keys_base, d_k)\n        w_base = weights_base[-1]\n\n        # 2. Adversarial calculation\n        K_attack_adv = create_attack_key(case[\"attack_direction\"], case[\"adversarial_attack_norm\"])\n        all_keys_adv = normal_keys_vecs + [K_attack_adv]\n        weights_adv = calculate_sdpa_weights(Q_vec, all_keys_adv, d_k)\n        w_adv = weights_adv[-1]\n\n        # 3. Norm clipping mitigation\n        all_keys_clipped = []\n        for k in all_keys_adv:\n            k_norm = np.linalg.norm(k)\n            # Use min(1, c/||K||) * K formulation\n            scale_factor = min(1.0, clip_cap / k_norm if k_norm > 0 else 1.0)\n            all_keys_clipped.append(k * scale_factor)\n        weights_clip = calculate_sdpa_weights(Q_vec, all_keys_clipped, d_k)\n        w_clip = weights_clip[-1]\n\n        # 4. Cosine similarity mitigation\n        weights_cos = calculate_cosine_sim_weights(Q_vec, all_keys_adv)\n        w_cos = weights_cos[-1]\n\n        results.append([w_base, w_adv, w_clip, w_cos])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在执行如文本生成这类自回归任务时，Transformer模型必须被严格限制，以防止其“偷看”未来的信息。本练习将带你从第一性原理出发，推导并实现用于此目的的关键技术——掩码自注意力（masked self-attention）。你将通过实践验证掩码操作的数学等价性，并直观地观察到不正确的掩码如何导致致命的“信息泄漏”问题。",
            "id": "3193602",
            "problem": "您需要从第一性原理出发，对 Transformer 架构中的掩码注意力（masked attention）进行推理。仅可使用以下基本依据：SoftMax 函数的定义、基本矩阵乘法、指数和对数函数的性质，以及缩放点积注意力（scaled dot-product attention）的定义。您的任务是推导并凭经验验证：在 SoftMax 之前应用二进制掩码等同于在指数域中与该掩码进行逐元素相乘。同时，您还需要演示不正确的掩码如何导致未来词元（token）的信息泄漏到过去的输出中。\n\n用作起点的定义：\n- 对于向量 $x \\in \\mathbb{R}^n$ 的 SoftMax 函数为 $$\n\\operatorname{SoftMax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}} \\quad \\text{for } i \\in \\{1,\\dots,n\\}.\n$$\n- 对于查询 $Q \\in \\mathbb{R}^{L \\times d_k}$ 和键 $K \\in \\mathbb{R}^{L \\times d_k}$ 的缩放点积注意力 (SDPA) logits 为 $$\nZ = \\frac{QK^\\top}{\\sqrt{d_k}} \\in \\mathbb{R}^{L \\times L}.\n$$\n- 一个二进制掩码 $M \\in \\{0,1\\}^{L \\times L}$ 用 $0$ 表示不允许的位置，用 $1$ 表示允许的位置。Hadamard（逐元素）积表示为 $Z \\odot M$。\n\n您的程序必须：\n- 根据上述定义推导（在您自己的推理中，而非在程序内部）：在 SoftMax 之前，将条目为 $A_{ij} = \\log M_{ij}$ 的加性掩码 $A$ 添加到 logits $Z$ 上，等同于在归一化之前，在指数域中与 $M$ 进行逐元素相乘。即，掩码后的 SoftMax 等于 $$\n\\operatorname{SoftMax}(Z + \\log M) = \\frac{e^{Z} \\odot M}{\\sum_{j} e^{Z_{:,j}} \\odot M_{:,j}} \\quad \\text{row-wise}.\n$$\n- 解释为什么对被掩码的条目使用一个有限大的负常数 $-C$ 来代替 $-\\infty$ 会导致被掩码位置上产生微小但非零的概率质量，并且如果掩码不正确，这可能导致信息泄漏。\n\n然后实现一个单一程序，使用下面给出的确切数值参数计算以下四个测试用例，并按规定格式输出其结果。\n\n给定的常数和矩阵：\n- 使用序列长度 $L = 4$，键维度 $d_k = 3$，以及值维度 $d_v = 2$。\n- 使用\n$$\nQ = \\begin{bmatrix}\n1  0  0\\\\\n0  1  0\\\\\n0  0  1\\\\\n1  1  1\n\\end{bmatrix}, \\quad\nK = \\begin{bmatrix}\n1  0  0\\\\\n0  1  0\\\\\n0  0  1\\\\\n1  1  1\n\\end{bmatrix}, \\quad\nV = \\begin{bmatrix}\n1  2\\\\\n3  4\\\\\n5  6\\\\\n7  8\n\\end{bmatrix}.\n$$\n- 定义正确的因果二进制掩码 $M^{\\mathrm{causal}} \\in \\{0,1\\}^{4 \\times 4}$，其中如果 $j \\le i$，则 $M^{\\mathrm{causal}}_{ij} = 1$，否则 $M^{\\mathrm{causal}}_{ij} = 0$。\n- 定义错误的差一二进制掩码 $M^{\\mathrm{off}} \\in \\{0,1\\}^{4 \\times 4}$，其中如果 $j \\le i+1$，则 $M^{\\mathrm{off}}_{ij} = 1$，否则 $M^{\\mathrm{off}}_{ij} = 0$。\n- 为了演示修改效果，定义 $V^{\\mathrm{mod}}$ 与 $V$ 相等，但其最后一行更改为 $[700, 800]$。\n\n此外，对于 SoftMax 掩码等效性测试，请使用以下较小的 logits 和掩码：\n$$\nZ^{(a)} = \\begin{bmatrix}\n0.2  -0.1  0.4\\\\\n1.0  -1.0  0.0\n\\end{bmatrix}, \\quad\nM^{(a)} = \\begin{bmatrix}\n1  0  1\\\\\n0  1  1\n\\end{bmatrix}.\n$$\n\n要实现和计算的测试套件：\n- 案例 1（SoftMax 前乘法掩码的等效性）：用两种方式计算带有掩码 $M^{(a)}$ 的 $Z^{(a)}$ 的逐行 SoftMax：\n  - 理想乘法形式：分子为 $e^{Z^{(a)}} \\odot M^{(a)}$，分母为该分子的逐行和。\n  - 加法近似形式：对被掩码位置的 logits 使用一个大的负常数 $-C$，其中 $C = 10^9$。\n  将两个结果概率矩阵之间的最大绝对差作为浮点数报告。\n- 案例 2（使用正确因果掩码无泄漏）：使用 $Q$、$K$、$V$ 和 $M^{\\mathrm{causal}}$ 计算注意力输出 $Y$。然后使用 $Q$、$K$、$V^{\\mathrm{mod}}$ 和 $M^{\\mathrm{causal}}$ 重新计算。检查输出的前 3 行（位置 0、1 和 2）是否在 $10^{-12}$ 的绝对容差内保持不变。如果不变则报告布尔值 true，否则报告 false。\n- 案例 3（使用不正确掩码导致泄漏）：使用带有 $M^{\\mathrm{off}}$ 的 $Q$、$K$、$V$ 计算注意力输出，然后使用带有 $M^{\\mathrm{off}}$ 的 $V^{\\mathrm{mod}}$ 重新计算。检查前 3 行是否有任何值的变化超过 $10^{-6}$ 的绝对容差。如果检测到泄漏则报告布尔值 true，否则报告 false。\n- 案例 4（有限掩码边界效应）：重复案例 1，但在加法近似中使用 $C = 50$ 代替 $C = 10^9$。将最大绝对差作为浮点数报告。\n\n不涉及角度单位。不涉及物理单位。所有报告的数值答案必须是指定的布尔值或浮点数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[\\text{case1\\_float}, \\text{case2\\_bool}, \\text{case3\\_bool}, \\text{case4\\_float}]$。例如，一个语法上有效的格式是 $[0.0,True,False,1e-12]$。",
            "solution": "问题的核心在于理解在缩放点积注意力机制中，掩码是如何在 SoftMax 函数中实现的，以及不完美实现所带来的后果。\n\n**第 1 部分：掩码等效性推导**\n\n我们的任务是推导：在应用 SoftMax 函数之前，将加性掩码 $A$（其中 $A_{ij} = \\log M_{ij}$）加到 logits $Z$ 上，等同于在指数域中与二进制掩码 $M$ 进行逐元素相乘。\n\n设 $Z \\in \\mathbb{R}^{L \\times L}$ 为注意力 logits 矩阵，设 $M \\in \\{0, 1\\}^{L \\times L}$ 为一个二进制掩码，其中 $M_{ij}=1$ 表示允许的注意力连接，$M_{ij}=0$ 表示不允许的连接。我们定义一个加性掩码矩阵 $A \\in (\\mathbb{R} \\cup \\{-\\infty\\})^{L \\times L}$，其条目为 $A_{ij} = \\log M_{ij}$。\n\nSoftMax 函数逐行应用于 logits 矩阵。对于给定行 $i$，到列 $j$ 的连接的掩码后 SoftMax 概率是在加性掩码后的 logits $Z'_{ij} = Z_{ij} + A_{ij}$ 上计算的。\n\n根据提供的 SoftMax 函数定义，对于行 $i$：\n$$\n\\operatorname{SoftMax}(Z'_i)_j = \\frac{e^{Z'_{ij}}}{\\sum_{k=1}^L e^{Z'_{ik}}}\n$$\n代入 $Z'_{ij} = Z_{ij} + A_{ij} = Z_{ij} + \\log M_{ij}$：\n$$\n\\operatorname{SoftMax}(Z_i + \\log M_i)_j = \\frac{e^{Z_{ij} + \\log M_{ij}}}{\\sum_{k=1}^L e^{Z_{ik} + \\log M_{ik}}}\n$$\n使用指数函数的性质 $e^{a+b} = e^a e^b$，我们可以将表达式重写为：\n$$\n\\frac{e^{Z_{ij}} e^{\\log M_{ij}}}{\\sum_{k=1}^L e^{Z_{ik}} e^{\\log M_{ik}}}\n$$\n现在，我们分析 $e^{\\log M_{ij}}$ 这一项。二进制掩码 $M$ 的条目为 $1$ 或 $0$。\n- 如果 $M_{ij} = 1$，那么 $\\log M_{ij} = \\log 1 = 0$。因此，$e^{\\log M_{ij}} = e^0 = 1$。所以，$e^{\\log M_{ij}} = M_{ij}$。\n- 如果 $M_{ij} = 0$，那么 $\\log M_{ij} = \\log 0$。在此上下文所需的极限意义下，$\\log 0 \\to -\\infty$。因此，$e^{\\log M_{ij}} \\to e^{-\\infty} = 0$。所以，$e^{\\log M_{ij}} = M_{ij}$。\n\n在这两种情况下，都有 $e^{\\log M_{ij}} = M_{ij}$。将此恒等式代回主表达式中：\n$$\n\\frac{e^{Z_{ij}} M_{ij}}{\\sum_{k=1}^L e^{Z_{ik}} M_{ik}}\n$$\n该表达式可以使用 Hadamard（逐元素）积表示法来书写。设 $E$ 是一个矩阵，其条目为 $E_{ij}=e^{Z_{ij}}$。那么分子是 $(E \\odot M)_{ij}$。分母是矩阵 $E \\odot M$ 的第 $i$ 行之和。这证明了等效性：\n$$\n\\operatorname{SoftMax}(Z + \\log M) = \\frac{e^{Z} \\odot M}{\\sum_{j} (e^{Z} \\odot M)_{:,j}} \\quad \\text{(row-wise)}\n$$\n该推导正式表明，将掩码的对数加到 logits 上，在数学上等同于在进行指数化之后、SoftMax 归一化步骤之前，乘以该掩码。\n\n**第 2 部分：使用不正确或有限掩码导致的信息泄漏**\n\n在实际的浮点实现中，我们无法表示 $-\\infty$。作为替代，我们用一个大数量级的负数 $-C$ 来近似 $\\log 0$，其中 $C$ 是一个大的正常数（例如，$10^9$）。\n\n加性掩码的应用如下：对于一个不允许的连接（$M_{ij}=0$），我们将 $-C$ 加到 logit $Z_{ij}$ 上。SoftMax 计算中的相应项变为：\n$$\ne^{Z_{ij} - C} = e^{Z_{ij}} e^{-C}\n$$\n对于一个大的 $C$，值 $e^{-C}$ 极小但关键在于它非零。这意味着注意力矩阵中不允许的位置将被分配一个微小但非零的概率质量，而理想的数学公式会给它们分配精确为零的概率。\n\n当这种非零概率与**不正确的掩码**结合时，就会发生信息泄漏。解码器的正确因果掩码 $M^{\\mathrm{causal}}$ 确保对于任何查询位置 $i$，所有键位置 $j > i$ 的注意力权重 $P_{ij}$ 均为零。这可以防止一个词元“看到”未来的词元。\n\n给定的不正确掩码 $M^{\\mathrm{off}}$ 定义为如果 $j \\le i+1$，则 $M^{\\mathrm{off}}_{ij} = 1$。这允许位置 $i$ 的查询关注到位置 $i+1$ 的键/值，即未来一个步长。例如，对于 $i=2$，它可以关注到 $j=0, 1, 2, 3$。因为它能关注到位置 $j=3$，如果值向量 $V_3$ 发生改变，位置 $Y_2$ 的输出也会改变。\n\n查询 $i$ 的注意力输出是一个加权和：\n$$\nY_i = \\sum_{j=1}^L P_{ij} V_j\n$$\n如果使用 $M^{\\mathrm{off}}$，注意力概率 $P_{2,3}$ 将为非零。让我们比较使用原始值 $V$ 的输出 $Y_2$ 和使用修改后值 $V^{\\mathrm{mod}}$ 的输出 $Y'_2$。位置 2 的输出变化为：\n$$\nY'_2 - Y_2 = \\sum_{j=1}^L P_{2,j} V^{\\mathrm{mod}}_j - \\sum_{j=1}^L P_{2,j} V_j = \\sum_{j=1}^L P_{2,j} (V^{\\mathrm{mod}}_j - V_j)\n$$\n由于 $V^{\\mathrm{mod}}$ 仅在最后一行（索引 3）与 $V$ 不同，这可以简化为：\n$$\nY'_2 - Y_2 = P_{2,3} (V^{\\mathrm{mod}}_3 - V_3)\n$$\n由于 $M^{\\mathrm{off}}$ 允许从 $i=2$ 到 $j=3$ 的注意力，所以 $P_{2,3}$ 是非零的。问题中定义了 $V_3$ 的一个非常大的变化，因此 $(V^{\\mathrm{mod}}_3 - V_3)$ 很大。这个乘积代表了一个“过去”词元的输出变化，它会变得很显著。这直接演示了信息从未来（步骤 3 的值）泄漏到过去（步骤 2 的输出）。\n\n使用正确的因果掩码 $M^{\\mathrm{causal}}$，权重 $P_{2,3}$ 将精确为零，因此 $Y_2$ 完全不受对 $V_3$ 的任何更改的影响。\n\n在有限近似中，常数 $C$ 的大小决定了被掩码的概率与零的接近程度。一个较小的 $C$（如 50）会导致 $e^{-50}$ 比 $e^{-10^9}$ 大，从而导致与理想的零概率情况有更大的偏差，以及更显著的“泄漏”或数值误差。这正是案例 4 所要演示的。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs all calculations for the four test cases regarding\n    masked attention in Transformers.\n    \"\"\"\n\n    # --- Given Constants and Matrices ---\n    L = 4\n    dk = 3\n    dv = 2\n\n    Q = np.array([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [1, 1, 1]\n    ], dtype=np.float64)\n\n    K = np.array([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [1, 1, 1]\n    ], dtype=np.float64)\n\n    V = np.array([\n        [1, 2],\n        [3, 4],\n        [5, 6],\n        [7, 8]\n    ], dtype=np.float64)\n\n    V_mod = np.array([\n        [1, 2],\n        [3, 4],\n        [5, 6],\n        [700, 800]\n    ], dtype=np.float64)\n\n    # M_causal: j = i\n    M_causal = np.tril(np.ones((L, L), dtype=np.float64))\n\n    # M_off: j = i + 1\n    i_indices = np.arange(L).reshape(-1, 1)\n    j_indices = np.arange(L).reshape(1, -1)\n    M_off = (j_indices = i_indices + 1).astype(np.float64)\n\n    # Matrices for smaller test cases\n    Z_a = np.array([\n        [0.2, -0.1, 0.4],\n        [1.0, -1.0, 0.0]\n    ], dtype=np.float64)\n\n    M_a = np.array([\n        [1, 0, 1],\n        [0, 1, 1]\n    ], dtype=np.float64)\n\n    # --- Helper Function for Attention Calculation ---\n    def compute_attention(q_mat, k_mat, v_mat, mask, C=1e9):\n        \"\"\"\n        Computes Scaled Dot-Product Attention with an additive mask.\n        Uses a numerically stable softmax.\n        \"\"\"\n        dk_val = q_mat.shape[1]\n        logits = (q_mat @ k_mat.T) / np.sqrt(dk_val)\n        \n        # Apply the additive mask\n        additive_mask = (mask - 1) * C\n        masked_logits = logits + additive_mask\n        \n        # Numerically stable softmax\n        shifted_logits = masked_logits - np.max(masked_logits, axis=1, keepdims=True)\n        attention_weights = np.exp(shifted_logits)\n        attention_weights /= np.sum(attention_weights, axis=1, keepdims=True)\n        \n        output = attention_weights @ v_mat\n        return output\n\n    # --- Test Cases ---\n    results = []\n\n    # Case 1: Equivalence of masking with C = 1e9\n    # Ideal multiplicative form\n    exp_Z_a = np.exp(Z_a)\n    numerator_ideal = exp_Z_a * M_a\n    denominator_ideal = np.sum(numerator_ideal, axis=1, keepdims=True)\n    # Handle cases where a whole row is masked, to avoid division by zero\n    denominator_ideal[denominator_ideal == 0] = 1.0 \n    P_ideal = numerator_ideal / denominator_ideal\n    \n    # Additive approximation with C = 1e9\n    C1 = 1e9\n    additive_mask_1 = (M_a - 1) * C1\n    Z_masked_1 = Z_a + additive_mask_1\n    # Stable softmax for additive form\n    shifted_Z_masked_1 = Z_masked_1 - np.max(Z_masked_1, axis=1, keepdims=True)\n    exp_Z_masked_1 = np.exp(shifted_Z_masked_1)\n    P_approx_1 = exp_Z_masked_1 / np.sum(exp_Z_masked_1, axis=1, keepdims=True)\n    \n    case1_diff = np.max(np.abs(P_ideal - P_approx_1))\n    results.append(case1_diff)\n\n    # Case 2: No leakage with correct causal mask\n    Y1 = compute_attention(Q, K, V, M_causal)\n    Y2 = compute_attention(Q, K, V_mod, M_causal)\n    case2_unchanged = bool(np.allclose(Y1[:3, :], Y2[:3, :], atol=1e-12, rtol=0))\n    results.append(case2_unchanged)\n\n    # Case 3: Leakage with incorrect mask\n    Y3 = compute_attention(Q, K, V, M_off)\n    Y4 = compute_attention(Q, K, V_mod, M_off)\n    diff = np.abs(Y3[:3, :] - Y4[:3, :])\n    case3_leakage_detected = bool(np.any(diff > 1e-6))\n    results.append(case3_leakage_detected)\n\n    # Case 4: Finite-mask boundary effect with C = 50\n    C4 = 50.0\n    additive_mask_4 = (M_a - 1) * C4\n    Z_masked_4 = Z_a + additive_mask_4\n    # Stable softmax\n    shifted_Z_masked_4 = Z_masked_4 - np.max(Z_masked_4, axis=1, keepdims=True)\n    exp_Z_masked_4 = np.exp(shifted_Z_masked_4)\n    P_approx_4 = exp_Z_masked_4 / np.sum(exp_Z_masked_4, axis=1, keepdims=True)\n    \n    case4_diff = np.max(np.abs(P_ideal - P_approx_4))\n    results.append(case4_diff)\n    \n    # Final print statement\n    # Explicitly format booleans to be lowercase 'true'/'false' if needed, but str() is fine.\n    print(f\"[{results[0]},{str(results[1])},{str(results[2])},{results[3]}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "注意力机制的行为并非一成不变，我们可以通过一个称为“温度”（temperature）的超参数 $\\tau$ 来精细调节其特性。本练习将指导你探索温度 $\\tau$ 如何控制注意力权重的分布，使其在“软性”的加权平均和“硬性”的 argmax 选择之间平滑过渡。理解这一机制对于控制模型生成内容的多样性与准确性至关重要。",
            "id": "3193530",
            "problem": "要求您使用第一性原理研究 Transformer 架构中注意力机制的温度缩放。考虑单个注意力步骤，其中一个查询生成一个 logits 向量 $Z \\in \\mathbb{R}^n$，注意力权重由带温度 $\\tau \\in \\mathbb{R}_{0}$ 的温度缩放 softmax 函数 $\\mathrm{softmax}(Z / \\tau)$ 计算得出。输出是注意力加权聚合 $y(\\tau) = \\sum_{i=1}^n a_i(\\tau) \\, v_i$，其中 $a_i(\\tau)$ 是 $\\mathrm{softmax}(Z / \\tau)$ 的坐标，$V \\in \\mathbb{R}^n$ 是一个值向量。\n\n任务：\n1) 理论：从 softmax 函数的定义和基础微积分出发，推导对于通用温度 $\\tau$ 的关于 $Z$ 的雅可比矩阵 $\\partial a_i / \\partial z_j$，并分析在以下情况下当 $\\tau \\to 0$ 时其行为：\n- 唯一最大化子：存在唯一的索引 $m$，使得对于所有 $k \\neq m$，都有 $z_m  z_k$。\n- 非唯一最大化子（平局）：存在一个包含 $r \\ge 2$ 个索引的集合 $\\mathcal{M}$，其中对于所有 $i \\in \\mathcal{M}$，都有 $z_i = \\max_k z_k$。\n使用极限严格地推断，当 $\\tau \\to 0$ 时，梯度是否以及如何变得局部化。\n\n2) 计算：实现一个数值稳定的计算，以研究一个能区分软聚合和硬选择需求的简单任务族。对于每种情况，定义预测的标量输出 $y(\\tau) = \\sum_{i=1}^n a_i(\\tau) v_i$ 和一个目标 $y^\\star$。然后定义平方误差损失 $L(\\tau) = \\big(y(\\tau) - y^\\star\\big)^2$。您必须在一个固定的网格上扫描温度 $\\tau$ 并报告：\n- 在扫描中实现最小 $L(\\tau)$ 的最小化温度 $\\tau_{\\min}$。如果存在平局，选择最小化子中最小的 $\\tau$。\n- 一个相变代理温度 $\\tau_{\\mathrm{pt}}$，定义为扫描中满足 $\\max_i a_i(\\tau) \\ge \\alpha$ 的最小 $\\tau$，其中 $\\alpha$ 是一个在 $(0,1)$ 内的固定阈值。如果扫描中没有温度满足该不等式，则报告 $-1.0$。\n\n使用以下固定的扫描和阈值：\n- 温度扫描：$\\{\\tau\\} = \\{\\,0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0\\,\\}$。\n- 阈值：$\\alpha = 0.9$。\n\n定义以下四个测试案例，每个案例都有一个元组 $(Z, V, \\text{task})$：\n- 案例 A（偏好软聚合）：$Z$ 的坐标为 $3.0, 2.9, 0.0, -1.0$，$V$ 的坐标为 $1.0, -1.0, 0.5, 2.0$。目标要求对给定集合 $S$ 索引的值进行平均：这里 $S = \\{0, 1\\}$，所以 $y^\\star = \\frac{1}{|S|} \\sum_{i \\in S} v_i$。这个案例应被视为软聚合任务。\n- 案例 B（偏好硬选择）：$Z$ 的坐标为 $3.0, 2.0, 1.0, 0.0$，$V$ 的坐标为 $2.0, -1.0, 0.0, 0.0$。目标要求选择最大 logit 索引处的值：$y^\\star = v_{\\arg\\max_i z_i}$。如果出现平局，定义 $\\arg\\max$ 返回最大化子中最小的索引。这个案例应被视为硬选择任务。\n- 案例 C（logits 中存在平局的软聚合）：$Z$ 的坐标为 $1.0, 1.0, 0.0, 0.0$，$V$ 的坐标为 $1.0, -1.0, 0.2, -0.2$。使用与 $S = \\{0, 1\\}$ 相同的软聚合目标，即 $y^\\star = \\frac{1}{2} (v_0 + v_1)$。\n- 案例 D（所有 logits 相等的硬选择）：$Z$ 的坐标为 $0.0, 0.0, 0.0, 0.0$，$V$ 的坐标为 $1.0, 0.0, 0.0, 0.0$。使用硬选择目标 $y^\\star = v_{\\arg\\max_i z_i}$，并采用与上述相同的平局打破规则。\n\n数值稳定性要求：使用数值稳定的方法（例如在求幂之前减去 $Z / \\tau$ 的最大值）来实现 $\\mathrm{softmax}(Z / \\tau)$ 的计算。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。对于A、B、C、D四个案例，按顺序为每个案例附加两个浮点数：首先是 $\\tau_{\\min}$，然后是 $\\tau_{\\mathrm{pt}}$。因此，总输出包含八个数字。例如，您的程序应打印一行形如 [$r_1$,$r_2$,$r_3$,$r_4$,$r_5$,$r_6$,$r_7$,$r_8$] 的内容，其中每个 $r_k$ 都是一个浮点数。",
            "solution": "该问题要求对温度缩放的 softmax 函数的雅可比矩阵进行理论分析，并对其在平衡软聚合和硬选择任务中的作用进行计算研究。\n\n### 第一部分：理论分析\n\n给定一个 logits 向量 $Z \\in \\mathbb{R}^n$，其温度缩放的注意力权重 $a_i(\\tau)$ 为：\n$$a_i(\\tau) = \\frac{\\exp(z_i / \\tau)}{\\sum_{k=1}^n \\exp(z_k / \\tau)}$$\n我们的目标是推导雅可比矩阵元素 $\\frac{\\partial a_i}{\\partial z_j}$ 并分析当温度 $\\tau \\to 0^+$ 时它们的行为。\n\n**雅可比矩阵 $\\frac{\\partial a_i}{\\partial z_j}$ 的推导**\n\n我们使用除法法则进行微分。设 $N_i = \\exp(z_i / \\tau)$ 且 $D = \\sum_{k=1}^n \\exp(z_k / \\tau)$。则 $a_i = N_i / D$。\n\n情况 1：$i = j$。\n$a_i$ 关于 $z_i$ 的导数为：\n$$\\frac{\\partial a_i}{\\partial z_i} = \\frac{\\frac{\\partial N_i}{\\partial z_i} D - N_i \\frac{\\partial D}{\\partial z_i}}{D^2}$$\n$N_i$ 和 $D$ 的偏导数为：\n$$\\frac{\\partial N_i}{\\partial z_i} = \\frac{1}{\\tau}\\exp(z_i / \\tau) = \\frac{1}{\\tau} N_i$$\n$$\\frac{\\partial D}{\\partial z_i} = \\frac{1}{\\tau}\\exp(z_i / \\tau) = \\frac{1}{\\tau} N_i$$\n将这些代入除法法则表达式中：\n$$\\frac{\\partial a_i}{\\partial z_i} = \\frac{(\\frac{1}{\\tau} N_i) D - N_i (\\frac{1}{\\tau} N_i)}{D^2} = \\frac{1}{\\tau} \\frac{N_i}{D} \\frac{D - N_i}{D} = \\frac{1}{\\tau} a_i(1 - a_i)$$\n\n情况 2：$i \\neq j$。\n$a_i$ 关于 $z_j$ 的导数为：\n$$\\frac{\\partial a_i}{\\partial z_j} = \\frac{\\frac{\\partial N_i}{\\partial z_j} D - N_i \\frac{\\partial D}{\\partial z_j}}{D^2}$$\n偏导数为：\n$$\\frac{\\partial N_i}{\\partial z_j} = 0$$\n$$\\frac{\\partial D}{\\partial z_j} = \\frac{1}{\\tau}\\exp(z_j / \\tau) = \\frac{1}{\\tau} N_j$$\n代入这些：\n$$\\frac{\\partial a_i}{\\partial z_j} = \\frac{0 \\cdot D - N_i (\\frac{1}{\\tau} N_j)}{D^2} = -\\frac{1}{\\tau} \\frac{N_i}{D} \\frac{N_j}{D} = -\\frac{1}{\\tau} a_i a_j$$\n\n使用克罗内克 δ 符号 $\\delta_{ij}$ 结合这两种情况：\n$$\\frac{\\partial a_i}{\\partial z_j} = \\frac{1}{\\tau} (a_i \\delta_{ij} - a_i a_j)$$\n\n**当 $\\tau \\to 0^+$ 时的极限分析**\n\n首先，我们分析注意力权重 $a_i(\\tau)$ 的极限。设 $z_{\\max} = \\max_k z_k$。我们可以将 $a_i(\\tau)$ 写为：\n$$a_i(\\tau) = \\frac{\\exp((z_i - z_{\\max}) / \\tau)}{\\sum_{k=1}^n \\exp((z_k - z_{\\max}) / \\tau)}$$\n\n**情况 A：唯一最大化子**\n存在一个唯一的索引 $m$，使得 $z_m = z_{\\max}$ 且对于所有 $k \\neq m$ 都有 $z_k  z_{\\max}$。\n当 $\\tau \\to 0^+$ 时，对于 $k \\neq m$，项 $(z_k - z_{\\max})/\\tau$ 趋于 $-\\infty$，而对于 $k=m$，该项为 $0$。因此，对于 $k \\neq m$，$\\exp((z_k - z_{\\max})/\\tau) \\to 0$，而对于 $k=m$，该项为 $1$。\n分母的极限为 $\\sum_{k=1}^n \\exp((z_k - z_{\\max})/\\tau) \\to 1$。\n因此，注意力权重收敛到一个独热（one-hot）向量：\n$$\\lim_{\\tau \\to 0^+} a_m(\\tau) = 1 \\quad \\text{且} \\quad \\lim_{\\tau \\to 0^+} a_k(\\tau) = 0 \\text{ 对于 } k \\neq m$$\n现在，我们分析雅可比矩阵的元素。对于任意 $i, j$：\n表达式 $\\frac{1}{\\tau}(a_i \\delta_{ij} - a_i a_j)$ 包含形如 $\\frac{1}{\\tau} \\exp(-C/\\tau)$（其中 $C0$）的项。\n设 $u = 1/\\tau$。当 $\\tau \\to 0^+$ 时，$u \\to \\infty$。极限变为 $\\lim_{u \\to \\infty} u e^{-Cu} = 0$。\n对每个子情况进行更严格的分析表明，雅可比矩阵的每个元素都趋于零：\n- 如果 $i \\neq m$ 且 $j \\neq m$：$\\frac{\\partial a_i}{\\partial z_j} = \\frac{1}{\\tau} (\\delta_{ij} a_i - a_i a_j)$。$a_i$ 和 $a_j$ 的数量级均为 $\\exp((z_{i,j}-z_m)/\\tau)$，因此该项消失的速度比 $1/\\tau$ 增长的速度快。极限为 $0$。\n- 如果一个索引是 $m$，例如，对于 $k \\neq m$ 的 $\\frac{\\partial a_m}{\\partial z_k}$：$-\\frac{1}{\\tau} a_m a_k \\approx -\\frac{1}{\\tau} (1) \\exp((z_k-z_m)/\\tau) \\to 0$。\n- 对于 $\\frac{\\partial a_m}{\\partial z_m}$：$\\frac{1}{\\tau} a_m (1-a_m) = \\frac{1}{\\tau} a_m \\sum_{k \\neq m} a_k$。这大约是 $\\frac{1}{\\tau} \\sum_{k \\neq m} \\exp((z_k-z_m)/\\tau)$，是一个各项都趋于 $0$ 的和。极限为 $0$。\n\n唯一最大化子的结论：在极限 $\\tau \\to 0^+$ 下，雅可比矩阵 $\\frac{\\partial a}{\\partial z}$ 收敛于零矩阵。梯度信号完全消失，这种现象被称为梯度饱和。因此，梯度并非“局部化”到有影响的输入的子集上，而是完全消失了。\n\n**情况 B：非唯一最大化子**\n存在一个包含 $r \\ge 2$ 个索引的集合 $\\mathcal{M}$，使得对于所有 $k \\in \\mathcal{M}$ 都有 $z_k = z_{\\max}$。\n当 $\\tau \\to 0^+$ 时，对于 $k \\in \\mathcal{M}$，$(z_k - z_{\\max})/\\tau = 0$，所以 $\\exp \\to 1$。对于 $k \\notin \\mathcal{M}$，$(z_k - z_{\\max})/\\tau \\to -\\infty$，所以 $\\exp \\to 0$。\n分母趋于 $\\sum_{k \\in \\mathcal{M}} 1 + \\sum_{k \\notin \\mathcal{M}} 0 = r$。\n因此，注意力权重收敛到最大化子上的均匀分布：\n$$\\lim_{\\tau \\to 0^+} a_k(\\tau) = \\frac{1}{r} \\text{ 对于 } k \\in \\mathcal{M} \\quad \\text{且} \\quad \\lim_{\\tau \\to 0^+} a_k(\\tau) = 0 \\text{ 对于 } k \\notin \\mathcal{M}$$\n现在，我们分析雅可比矩阵的元素。\n- 如果 $i \\notin \\mathcal{M}$ 或 $j \\notin \\mathcal{M}$：$a_i$ 或 $a_j$ 中至少有一个会以指数速度消失，导致相应的雅可比矩阵元素趋于 $0$，类似于唯一最大化子的情况。\n- 如果 $i, j \\in \\mathcal{M}$：在 $z_i=z_j=z_{\\max}$ 的点上，注意力值为 $a_i=a_j=\\frac{1}{r+\\dots}$。对于小的 $\\tau$，$a_i \\approx 1/r$。\n  - 对于 $i=j \\in \\mathcal{M}$：\n    $$\\frac{\\partial a_i}{\\partial z_i} = \\frac{1}{\\tau} a_i(1 - a_i) \\approx \\frac{1}{\\tau} \\frac{1}{r} \\left(1 - \\frac{1}{r}\\right) = \\frac{r-1}{r^2 \\tau}$$\n    当 $\\tau \\to 0^+$ 时，该项发散到 $+\\infty$。\n  - 对于 $i, j \\in \\mathcal{M}$ 且 $i \\neq j$：\n    $$\\frac{\\partial a_i}{\\partial z_j} = -\\frac{1}{\\tau} a_i a_j \\approx -\\frac{1}{\\tau} \\frac{1}{r^2}$$\n    当 $\\tau \\to 0^+$ 时，该项发散到 $-\\infty$。\n\n非唯一最大化子的结论：极限函数 $a_i(Z)$ 在存在最大值平局的 $Z$ 点上是不可微的。当 $\\tau \\to 0^+$ 时，导数发散。具体来说，雅可比矩阵中对应于集合 $\\mathcal{M}$ 中索引的子矩阵的元素发散到 $\\pm\\infty$，而雅可比矩阵的所有其他元素都趋于 $0$。从这个意义上说，梯度被强力局部化到最大化子的集合上。\n\n### 第二部分：计算研究\n\n我们实现一个数值稳定的 softmax 函数，并在温度扫描中评估四个测试案例的损失。\n\n带温度 $\\tau$ 的数值稳定 softmax 是通过首先将缩放后的 logits $Z/\\tau$ 减去它们的最大值来计算的，以防止在求幂时发生溢出。设 $X = Z/\\tau$。\n$$a_i = \\frac{\\exp(x_i - \\max_k x_k)}{\\sum_j \\exp(x_j - \\max_k x_k)}$$\n我们定义测试案例和目标：\n- 案例 A (偏好软聚合)：$Z=[3.0, 2.9, 0.0, -1.0]$，$V=[1.0, -1.0, 0.5, 2.0]$。目标 $y^\\star = \\frac{1}{2}(v_0+v_1) = 0.0$。对于能够平均 $v_0$ 和 $v_1$ 的较高 $\\tau$，预期损失较低。\n- 案例 B (偏好硬选择)：$Z=[3.0, 2.0, 1.0, 0.0]$，$V=[2.0, -1.0, 0.0, 0.0]$。目标 $y^\\star = v_{\\arg\\max Z} = v_0 = 2.0$。对于能够选择 $v_0$ 的较低 $\\tau$，预期损失较低。\n- 案例 C (软聚合，logits 中存在平局)：$Z=[1.0, 1.0, 0.0, 0.0]$，$V=[1.0, -1.0, 0.2, -0.2]$。目标 $y^\\star = \\frac{1}{2}(v_0+v_1) = 0.0$。由于 $z_0=z_1$ 且 $z_2=z_3$，根据对称性，$a_0=a_1$ 且 $a_2=a_3$。输出为 $y(\\tau) = a_0(v_0+v_1) + a_2(v_2+v_3) = a_0(0) + a_2(0) = 0$。对于所有 $\\tau$，损失均为 $0$。因此，$\\tau_{\\min}$ 将是扫描范围中的最小值 $0.02$。最大注意力值为 $a_0 = 1/(2+2e^{-1/\\tau})$，该值永远不会超过 $0.5$，因此无法达到阈值 $\\alpha=0.9$。$\\tau_{\\mathrm{pt}}$ 将为 $-1.0$。\n- 案例 D (硬选择，所有 logits 均相等)：$Z=[0.0, 0.0, 0.0, 0.0]$，$V=[1.0, 0.0, 0.0, 0.0]$。目标 $y^\\star = v_{\\arg\\max Z} = v_0 = 1.0$。当所有 logits 相等时，对于所有 $\\tau$，$a_i=1/4$。输出是常数：$y(\\tau) = \\frac{1}{4}\\sum v_i = 0.25$。损失是常数。$\\tau_{\\min}$ 将是扫描范围中的最小值 $0.02$。最大注意力值始终为 $0.25$，因此 $\\tau_{\\mathrm{pt}}$ 将为 $-1.0$。\n\n该实现将扫描给定的温度网格，计算每个 $\\tau$ 的损失 $L(\\tau) = (y(\\tau) - y^\\star)^2$ 和 $\\max_i a_i(\\tau)$，并根据指定规则确定 $\\tau_{\\min}$ 和 $\\tau_{\\mathrm{pt}}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the temperature scaling problem by analyzing four test cases.\n    For each case, it finds the temperature that minimizes a squared error loss (tau_min)\n    and a phase-transition proxy temperature (tau_pt).\n    \"\"\"\n\n    # Define the fixed sweep grid and threshold from the problem statement.\n    temp_sweep = np.array([0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0])\n    alpha_threshold = 0.9\n\n    # Define the four test cases as tuples: (Z, V, y_star).\n    # y_star is pre-calculated based on a soft aggregation or hard selection task.\n    test_cases = [\n        # Case A: Soft aggregation preferred\n        (\n            np.array([3.0, 2.9, 0.0, -1.0]),\n            np.array([1.0, -1.0, 0.5, 2.0]),\n            0.5 * (1.0 - 1.0)  # y_star = (v_0 + v_1) / 2\n        ),\n        # Case B: Hard selection preferred\n        (\n            np.array([3.0, 2.0, 1.0, 0.0]),\n            np.array([2.0, -1.0, 0.0, 0.0]),\n            2.0  # y_star = v_argmax(Z) = v_0\n        ),\n        # Case C: Soft aggregation with a tie in logits\n        (\n            np.array([1.0, 1.0, 0.0, 0.0]),\n            np.array([1.0, -1.0, 0.2, -0.2]),\n            0.5 * (1.0 - 1.0)  # y_star = (v_0 + v_1) / 2\n        ),\n        # Case D: Hard selection with all logits equal\n        (\n            np.array([0.0, 0.0, 0.0, 0.0]),\n            np.array([1.0, 0.0, 0.0, 0.0]),\n            1.0  # y_star = v_argmin_idx(argmax(Z)) = v_0\n        )\n    ]\n\n    # List to store the final results: [tau_min_A, tau_pt_A, tau_min_B, tau_pt_B, ...]\n    final_results = []\n\n    def stable_softmax(z, tau):\n        \"\"\"Computes numerically stable softmax with temperature.\"\"\"\n        if tau == 0:\n            raise ValueError(\"Temperature tau must be positive.\")\n        z_scaled = z / tau\n        # Subtract max for numerical stability (log-sum-exp trick)\n        z_stable = z_scaled - np.max(z_scaled)\n        exps = np.exp(z_stable)\n        return exps / np.sum(exps)\n\n    for Z, V, y_star in test_cases:\n        losses = []\n        max_attentions = []\n\n        # Sweep through the temperature grid\n        for tau in temp_sweep:\n            # Calculate attention weights\n            a = stable_softmax(Z, tau)\n            \n            # Calculate the predicted scalar output\n            y_tau = np.sum(a * V)\n            \n            # Calculate and store the squared error loss\n            loss = (y_tau - y_star)**2\n            losses.append(loss)\n            \n            # Store the maximum attention weight\n            max_attentions.append(np.max(a))\n\n        # Determine tau_min: the smallest temperature that results in the minimum loss.\n        # np.argmin returns the index of the first occurrence of the minimum value.\n        min_loss_idx = np.argmin(losses)\n        tau_min = temp_sweep[min_loss_idx]\n\n        # Determine tau_pt: the smallest temperature where max attention weight >= threshold.\n        tau_pt = -1.0\n        for i, max_a in enumerate(max_attentions):\n            if max_a >= alpha_threshold:\n                tau_pt = temp_sweep[i]\n                break # Found the smallest tau, so we can stop.\n\n        # Append results for the current case\n        final_results.append(tau_min)\n        final_results.append(tau_pt)\n\n    # Format the final output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        }
    ]
}