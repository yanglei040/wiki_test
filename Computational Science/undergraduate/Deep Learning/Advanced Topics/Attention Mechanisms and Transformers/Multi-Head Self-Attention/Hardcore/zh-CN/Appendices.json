{
    "hands_on_practices": [
        {
            "introduction": "在构建复杂的深度学习模型之前，理解其基本单元的成本和构成至关重要。本练习将引导你剖析一个标准的 Transformer 编码器模块（与 BERT 中使用的类似），并为其总可学习参数量推导出一个精确的公式。通过这项实践，你将巩固对多头自注意力、前馈网络和层归一化等关键组件的理解，并清晰地认识到它们各自对模型规模的贡献 。",
            "id": "3102535",
            "problem": "考虑双向编码器表示来自Transformer (BERT) 中的单个编码器块。该块包含多头自注意力（MHSA），其后是一个双层逐点前馈网络，带有残差连接和两个层归一化（LN）模块。假设采用以下与原始架构一致的规范设计选择：\n- 隐藏层大小为 $d$，注意力头的数量为 $h$，前馈网络内部维度为 $d_{ff}$。\n- 查询、键和值投影实现为从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d}$ 的学习仿射映射，每个都带有一个 $\\mathbb{R}^{d \\times d}$ 的权重矩阵和一个 $\\mathbb{R}^{d}$ 的偏置向量，而注意力输出投影同样是一个学习的仿射映射，其权重在 $\\mathbb{R}^{d \\times d}$ 中，偏置在 $\\mathbb{R}^{d}$ 中。\n- 前馈网络由两个学习的仿射映射组成：第一个从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d_{ff}}$，第二个从 $\\mathbb{R}^{d_{ff}}$ 回到 $\\mathbb{R}^{d}$，每个都带有一个权重矩阵和一个偏置向量。中间的非线性函数是高斯误差线性单元（GELU），没有可学习的参数。\n- 每个块中应用两次层归一化，每个LN都有一个学习的缩放参数 $\\gamma \\in \\mathbb{R}^{d}$ 和一个平移参数 $\\beta \\in \\mathbb{R}^{d}$。该块中没有其他可学习的参数。\n- 词元嵌入和位置编码是块外部嵌入层的一部分，此处不得计算。\n\n仅从核心定义出发，即一个从 $\\mathbb{R}^{m}$ 到 $\\mathbb{R}^{n}$ 的学习仿射映射有 $n \\times m$ 个权重参数和 $n$ 个偏置参数，并且每次应用层归一化会贡献 $2 \\times d$ 个参数（缩放和平移），推导该编码器块中可学习参数总数的封闭式表达式，用 $d$、$d_{ff}$ 和 $h$ 表示。然后，提出一个变体，将头的数量从 $h$ 增加到 $\\alpha h$，同时对于给定的隐藏大小 $d$ 和序列长度，保持 MHSA 投影和注意力分数乘法的总计算量不变（您可以假设每个头的维度是 $d/h$，并且保持 $d$ 不变可以固定主要的 MHSA 计算量）。对于 $\\alpha = 2$ 的这个变体，给出得到的每个头的维度。\n\n最后，对 BERT-Base 配置 $d = 768$，$d_{ff} = 3072$，$h = 12$ 和 $\\alpha = 2$ 对您的表达式进行数值评估。以一个包含两个条目的行矩阵形式提供您的最终答案：\n- 编码器块的总参数数量，以及\n- 将头的数量增加因子 $\\alpha$ 后的新每个头的维度。\n\n无需四舍五入；报告精确整数。以行矩阵形式 $\\begin{pmatrix} \\text{param\\_count}  \\text{new\\_head\\_dim} \\end{pmatrix}$ 表示您的最终答案。",
            "solution": "问题陈述经评估有效。它在科学上基于已建立的Transformer模型架构，问题定义良好，有足够的信息得出唯一解，并以客观、正式的语言表述。所提供的数据一致且现实，对应于著名的BERT-Base配置。该问题是分析深度学习模型参数化的一个标准的、非平凡的练习。\n\n解决方案分三部分进行：首先，推导BERT编码器块中可学习参数数量的通用符号表达式；其次，分析所提出的架构变体；第三，对给定配置的推导表达式进行数值评估。\n\n**第一部分：可学习参数总数的推导**\n\n可学习参数的总数 $P_{\\text{total}}$ 是其组成部分参数的总和：多头自注意力（MHSA）模块、逐点前馈网络（FFN）和两个层归一化（LN）模块。我们将根据提供的定义计算每个组件的参数。\n\n一个从 $\\mathbb{R}^{m}$ 到 $\\mathbb{R}^{n}$ 的学习仿射映射有一个大小为 $n \\times m$ 的权重矩阵和一个大小为 $n$ 的偏置向量，总共有 $n \\times m + n$ 个参数。\n\n1.  **多头自注意力（MHSA）参数（$P_{\\text{MHSA}}$）**\n    MHSA机制包括问题陈述中定义的四个学习仿射投影。\n    *   **查询投影（Q）：** 从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d}$ 的仿射映射。\n        - 权重矩阵 $W_Q \\in \\mathbb{R}^{d \\times d}$：$d \\times d = d^2$ 个参数。\n        - 偏置向量 $b_Q \\in \\mathbb{R}^{d}$：$d$ 个参数。\n        - Q 的总参数：$d^2 + d$。\n    *   **键投影（K）：** 从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d}$ 的仿射映射。\n        - 权重矩阵 $W_K \\in \\mathbb{R}^{d \\times d}$：$d^2$ 个参数。\n        - 偏置向量 $b_K \\in \\mathbb{R}^{d}$：$d$ 个参数。\n        - K 的总参数：$d^2 + d$。\n    *   **值投影（V）：** 从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d}$ 的仿射映射。\n        - 权重矩阵 $W_V \\in \\mathbb{R}^{d \\times d}$：$d^2$ 个参数。\n        - 偏置向量 $b_V \\in \\mathbb{R}^{d}$：$d$ 个参数。\n        - V 的总参数：$d^2 + d$。\n    *   **输出投影（O）：** 从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d}$ 的仿射映射。\n        - 权重矩阵 $W_O \\in \\mathbb{R}^{d \\times d}$：$d^2$ 个参数。\n        - 偏置向量 $b_O \\in \\mathbb{R}^{d}$：$d$ 个参数。\n        - O 的总参数：$d^2 + d$。\n    MHSA 模块中的参数总数是这四个分量的总和。请注意，头的数量 $h$ 在此公式中没有明确出现，因为投影被定义为对整个维度为 $d$ 的隐藏状态的单个变换。\n    $$P_{\\text{MHSA}} = (d^2 + d) + (d^2 + d) + (d^2 + d) + (d^2 + d) = 4(d^2 + d) = 4d^2 + 4d$$\n\n2.  **逐点前馈网络（FFN）参数（$P_{\\text{FFN}}$）**\n    FFN 由两个学习的仿射映射和一个没有参数的 GELU 非线性函数组成。\n    *   **第一层：** 从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d_{ff}}$ 的仿射映射。\n        - 权重矩阵 $W_1 \\in \\mathbb{R}^{d_{ff} \\times d}$：$d_{ff} \\times d$ 个参数。\n        - 偏置向量 $b_1 \\in \\mathbb{R}^{d_{ff}}$：$d_{ff}$ 个参数。\n        - 第一层的总参数：$d \\cdot d_{ff} + d_{ff}$。\n    *   **第二层：** 从 $\\mathbb{R}^{d_{ff}}$ 到 $\\mathbb{R}^{d}$ 的仿射映射。\n        - 权重矩阵 $W_2 \\in \\mathbb{R}^{d \\times d_{ff}}$：$d \\times d_{ff}$ 个参数。\n        - 偏置向量 $b_2 \\in \\mathbb{R}^{d}$：$d$ 个参数。\n        - 第二层的总参数：$d \\cdot d_{ff} + d$。\n    FFN 中的参数总数是这两层的总和。\n    $$P_{\\text{FFN}} = (d \\cdot d_{ff} + d_{ff}) + (d \\cdot d_{ff} + d) = 2d \\cdot d_{ff} + d_{ff} + d$$\n\n3.  **层归一化（LN）参数（$P_{\\text{LN}}$）**\n    该块包含两个层归一化模块。每个模块都有一个可学习的缩放向量 $\\gamma \\in \\mathbb{R}^{d}$ 和一个可学习的平移向量 $\\beta \\in \\mathbb{R}^{d}$。\n    - 每个 LN 模块的参数：$d + d = 2d$。\n    - 两个 LN 模块的总参数：\n    $$P_{\\text{LN}} = 2 \\times (2d) = 4d$$\n\n**总参数（$P_{\\text{total}}$）**\n编码器块中可学习参数的总数是所有组件参数的总和。\n$$P_{\\text{total}} = P_{\\text{MHSA}} + P_{\\text{FFN}} + P_{\\text{LN}}$$\n$$P_{\\text{total}} = (4d^2 + 4d) + (2d \\cdot d_{ff} + d_{ff} + d) + (4d)$$\n$$P_{\\text{total}} = 4d^2 + (4d + d + 4d) + 2d \\cdot d_{ff} + d_{ff}$$\n$$P_{\\text{total}} = 4d^2 + 9d + d_{ff}(2d + 1)$$\n这就是总参数数量的封闭式表达式。\n\n**第二部分：架构变体和新的每个头的维度**\n\n问题提出了一个变体，其中注意力头的数量从 $h$ 变为 $h' = \\alpha h$，而模型的隐藏大小 $d$ 保持不变。在标准的 Transformer 架构中，所有头的总维度等于模型的隐藏大小。也就是说，如果 $d_k$ 是每个头的查询、键和值向量的维度，那么 $h \\cdot d_k = d$。\n\n原始的每个头的维度是 $d_k = \\frac{d}{h}$。\n新的头的数量是 $h' = \\alpha h$。\n为了保持关系 $h' \\cdot d'_k = d$，其中 $d'_k$ 是新的每个头的维度，我们可以解出 $d'_k$：\n$$d'_k = \\frac{d}{h'} = \\frac{d}{\\alpha h} = \\frac{1}{\\alpha} \\left(\\frac{d}{h}\\right)$$\n新的每个头的维度是原始每个头的维度乘以 $1/\\alpha$。\n\n**第三部分：数值评估**\n\n我们给定了 BERT-Base 配置：$d = 768$，$d_{ff} = 3072$，$h = 12$，以及修改因子 $\\alpha = 2$。\n\n*   **总参数数量：**\n    我们将给定值代入推导出的 $P_{\\text{total}}$ 公式中。\n    $$P_{\\text{total}} = 4(768)^2 + 9(768) + 3072(2 \\cdot 768 + 1)$$\n    $$P_{\\text{total}} = 4(589824) + 6912 + 3072(1536 + 1)$$\n    $$P_{\\text{total}} = 2359296 + 6912 + 3072(1537)$$\n    $$P_{\\text{total}} = 2359296 + 6912 + 4721664$$\n    $$P_{\\text{total}} = 7087872$$\n\n*   **新的每个头的维度：**\n    原始头的数量是 $h=12$。新的头的数量是 $h' = \\alpha h = 2 \\times 12 = 24$。\n    隐藏大小为 $d = 768$。新的每个头的维度 $d'_k$ 是：\n    $$d'_k = \\frac{d}{h'} = \\frac{768}{24}$$\n    要计算这个值，我们可以先除以 $12$ 再除以 $2$：\n    $$d'_k = \\frac{768/12}{2} = \\frac{64}{2} = 32$$\n    新的每个头的维度是 $32$。\n\n最终答案要求一个包含总参数数量和新的每个头维度的行矩阵。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} 7087872  32 \\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "我们为什么需要多个注意力头？难道不能用一个更强大的注意力机制来代替吗？这个思想实验将直面这一核心问题，通过对比多头自注意力（MHSA）与一个更高效的变体——多查询注意力（MQA）——来揭示答案。通过分析一个精心设计的场景，你将发现共享键值投影的根本局限性，并体会到为何独立的注意力头对于捕捉数据中多样化甚至相互冲突的关系模式至关重要 。",
            "id": "3154513",
            "problem": "一个输入序列由3个词元（token）组成，它们在 $\\mathbb{R}^2$ 中表示为 $x_1 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$、$x_2 = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$ 和 $x_3 = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$。考虑一个具有 $H = 2$ 个头的自注意力层。您将比较两种架构：多头自注意力（Multi-Head Self-Attention, MHSA）和多查询注意力（Multi-Query Attention, MQA）。在 MHSA 中，每个头都有其自己独立的用于查询（query）、键（key）和值（value）的线性投影。在 MQA 中，所有头共享一个单一的键投影和一个单一的值投影，但每个头仍然有其自己的查询投影。除非另有说明，否则假设两种架构中每个头的键维度均为 $d_k = 1$。请只关注单个查询位置以及每个头内3个词元的注意力权重的相对排序；忽略输出投影以及值在注意力权重计算之外的内容。\n\n我们寻求一种配置，使得头 $1$ 对词元的注意力排序为 $x_1 \\succ x_3 \\succ x_2$（对 $x_1$ 的注意力严格大于对 $x_3$ 的，对 $x_3$ 的注意力严格大于对 $x_2$ 的），而头 $2$ 的排序为 $x_2 \\succ x_1 \\succ x_3$。\n\n下列哪个陈述是正确的？\n\nA. 在 MHSA 中，当每个头的 $d_k = 1$ 时，存在逐头的键投影和查询的选择，可以实现所需的排序 $x_1 \\succ x_3 \\succ x_2$（头 $1$）和 $x_2 \\succ x_1 \\succ x_3$（头 $2$）。\n\nB. 在 MQA 中，当共享键的维度为 $d_k = 1$ 时，存在逐头的查询，可以在不改变共享键投影的情况下，也实现这两个相同的排序。\n\nC. 如果 MQA 的共享键维度增加到 $d_k = 2$（仍然在各头之间共享），则存在一个单一的共享键投影和逐头的查询，可以同时实现这两个排序。\n\nD. 在共享键（shared-K）设置中的根本障碍完全来自于在各头之间共享值；如果值是逐头的，而键在 $d_k = 1$ 的情况下保持共享，那么这两种排序将变得可以实现。",
            "solution": "首先验证问题陈述，以确保其在科学上是合理的、适定的和完整的。\n\n### 第一步：提取已知条件\n- 输入词元：$x_1 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$，$x_2 = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$，$x_3 = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$，在 $\\mathbb{R}^2$ 中。\n- 具有 $H=2$ 个头的自注意力层。\n- 两种架构：\n    1.  **多头自注意力 (MHSA):** 每个头 $h \\in \\{1, 2\\}$ 都有独立的查询 ($W_Q^h$)、键 ($W_K^h$) 和值 ($W_V^h$) 权重矩阵。\n    2.  **多查询注意力 (MQA):** 各头之间共享键 ($W_K$) 和值 ($W_V$) 投影；查询投影 ($W_Q^h$) 是独立的。\n- 每个头的键维度为 $d_k = 1$，除非另有说明。\n- 分析关注的是注意力权重的相对排序，这由 softmax 前的注意力分数决定。\n- 头1所需的排序：对 $x_1$ 的注意力 > 对 $x_3$ 的注意力 > 对 $x_2$ 的注意力。记作 $x_1 \\succ x_3 \\succ x_2$。\n- 头2所需的排序：对 $x_2$ 的注意力 > 对 $x_1$ 的注意力 > 对 $x_3$ 的注意力。记作 $x_2 \\succ x_1 \\succ x_3$。\n\n### 第二步：使用提取的已知条件进行验证\n- **科学依据：** 该问题基于深度学习中自注意力机制的既定原则，特别是 Transformer 架构。MHSA 和 MQA 是标准变体。数学运算（线性投影、点积）是基础。\n- **适定性：** 问题要求判断是否存在满足一组严格不等式约束的投影矩阵和查询向量。输入、维度和约束都已明确定义，使得问题可解。\n- **客观性：** 问题使用精确的数学语言和深度学习领域的定义来表述。所需的排序是客观标准。\n- **完整性和一致性：** 问题是自洽的。一个关键的观察是输入词元是线性相关的：$x_3 = x_1 + x_2$。这个性质是一致的，并且是分析的核心。似乎没有信息缺失或矛盾。\n\n### 第三步：结论与行动\n问题陈述有效。我们可以继续进行解题推导。\n\n### 推导\n\n注意力权重的排序由注意力分数的排序决定。对于给定的头 $h$，词元 $i$ 的分数计算为查询向量 $q_h$ 和键向量 $k_{h,i}$ 的点积。\n$s_{h,i} = q_h^T k_{h,i}$\n输入词元 $x_i$ 的键向量 $k_{h,i}$ 是由一个线性投影生成的：$k_{h,i} = W_K^h x_i$（在 MHSA 中）或 $k_{h,i} = W_K x_i$（在 MQA 中）。\n\n一个关键属性来自于输入词元的线性相关性，$x_3 = x_1 + x_2$。由于键投影的线性性质，生成的键向量也将是线性相关的：\n$$k_{h,3} = W_K^{h/\\text{shared}} x_3 = W_K^{h/\\text{shared}} (x_1 + x_2) = W_K^{h/\\text{shared}} x_1 + W_K^{h/\\text{shared}} x_2 = k_{h,1} + k_{h,2}$$\n这个关系对于任何头 $h$ 和任何键投影矩阵 $W_K$ 都成立。\n\n因此，注意力分数也表现出这种线性关系：\n$$s_{h,3} = q_h^T k_{h,3} = q_h^T(k_{h,1} + k_{h,2}) = q_h^T k_{h,1} + q_h^T k_{h,2} = s_{h,1} + s_{h,2}$$\n这个基本约束 $s_{h,3} = s_{h,1} + s_{h,2}$ 必须对两个头都满足，无论采用哪种架构。\n\n让我们根据这个约束来分析所需的排序。\n**头1的要求：** $s_{1,1} \\succ s_{1,3} \\succ s_{1,2}$，意即 $s_{1,1} > s_{1,3}$ 且 $s_{1,3} > s_{1,2}$。\n1.  使用 $s_{1,3} = s_{1,1} + s_{1,2}$，第一个不等式 $s_{1,1} > s_{1,3}$ 变为 $s_{1,1} > s_{1,1} + s_{1,2}$，这意味着 $0 > s_{1,2}$。\n2.  第二个不等式 $s_{1,3} > s_{1,2}$ 变为 $s_{1,1} + s_{1,2} > s_{1,2}$，这意味着 $s_{1,1} > 0$。\n因此，对于头1，我们必须有 $s_{1,1} > 0$ 和 $s_{1,2}  < 0$。\n\n**头2的要求：** $s_{2,2} \\succ s_{2,1} \\succ s_{2,3}$，意即 $s_{2,2} > s_{2,1}$ 且 $s_{2,1} > s_{2,3}$。\n1.  使用 $s_{2,3} = s_{2,1} + s_{2,2}$，不等式 $s_{2,1} > s_{2,3}$ 变为 $s_{2,1} > s_{2,1} + s_{2,2}$，这意味着 $0 > s_{2,2}$。\n2.  另一个不等式是 $s_{2,2} > s_{2,1}$。\n因此，对于头2，我们必须有 $s_{2,2}  < 0$ 和 $s_{2,1}  < s_{2,2}$。\n\n### 逐项分析\n\n**A. 在 MHSA 中，当每个头的 $d_k = 1$ 时，存在逐头的键投影和查询的选择，可以实现所需的排序 $x_1 \\succ x_3 \\succ x_2$（头 $1$）和 $x_2 \\succ x_1 \\succ x_3$（头 $2$）。**\n\n在 MHSA 中，每个头 $h$ 都有自己的键投影 $W_K^h$ 和查询投影 $W_Q^h$。对于 $d_k=1$，$W_K^h$ 是一个 $1 \\times 2$ 的矩阵，因此设 $W_K^h = \\begin{bmatrix} w_{h,1}  w_{h,2} \\end{bmatrix}$。键是标量：$k_{h,1} = w_{h,1}$，$k_{h,2} = w_{h,2}$，以及 $k_{h,3} = w_{h,1} + w_{h,2}$。设查询 $q_h$ 是一个我们可以选择的标量。\n分数为 $s_{h,1} = q_h k_{h,1}$ 和 $s_{h,2} = q_h k_{h,2}$。\n\n对于头1：我们需要 $s_{1,1} > 0$ 和 $s_{1,2}  < 0$。我们可以选择 $q_1 = 1$。这要求 $k_{1,1} = w_{1,1} > 0$ 和 $k_{1,2} = w_{1,2}  < 0$。例如，设 $W_K^1 = \\begin{bmatrix} 2  -1 \\end{bmatrix}$。当 $q_1=1$ 时，分数为 $s_{1,1}=2$，$s_{1,2}=-1$，以及 $s_{1,3}=2-1=1$。排序为 $2 > 1 > -1$，即 $s_{1,1} > s_{1,3} > s_{1,2}$。这满足要求。\n\n对于头2：我们需要 $s_{2,2}  < 0$ 和 $s_{2,1}  < s_{2,2}$。我们可以选择 $q_2 = 1$。这要求 $k_{2,2} = w_{2,2}  < 0$ 和 $k_{2,1} = w_{2,1}  < w_{2,2}$。例如，设 $W_K^2 = \\begin{bmatrix} -2  -1 \\end{bmatrix}$。当 $q_2=1$ 时，分数为 $s_{2,1}=-2$，$s_{2,2}=-1$，以及 $s_{2,3}=-2-1=-3$。排序为 $-1 > -2 > -3$，即 $s_{2,2} > s_{2,1} > s_{2,3}$。这满足要求。\n\n由于在 MHSA 中我们可以独立选择 $W_K^1$ 和 $W_K^2$，我们已经证明了这样的配置存在。\n\n结论：**正确**。\n\n**B. 在 MQA 中，当共享键的维度为 $d_k = 1$ 时，存在逐头的查询，可以在不改变共享键投影的情况下，也实现这两个相同的排序。**\n\n在 MQA 中，有一个单一的共享键投影 $W_K$。在 $d_k=1$ 的情况下，两个头共用一组标量键 $\\{k_1, k_2, k_3\\}$。查询 $q_1$ 和 $q_2$ 是逐头的标量。\n头 $h$ 的分数为 $s_{h,i} = q_h k_i$。分数 $\\{s_{h,1}, s_{h,2}, s_{h,3}\\}$ 的排序仅取决于键 $\\{k_1, k_2, k_3\\}$ 的排序和 $q_h$ 的符号。如果 $q_h > 0$，分数排序与键排序相同。如果 $q_h  < 0$，分数排序是键排序的完全逆序。\n因此，一个共享的 $d_k=1$ 键投影在所有头中最多只允许两种不同的分数排序：一种特定排序及其完全逆序。\n头1所需的排序（基于词元索引）是 $(1, 3, 2)$。此排序的逆序是 $(2, 3, 1)$。\n头2所需的排序是 $(2, 1, 3)$。\n由于排序 $(2, 1, 3)$ 不是 $(1, 3, 2)$ 的逆序，因此单一共享的标量键集合不可能产生这两种排序。\n\n结论：**不正确**。\n\n**C. 如果 MQA 的共享键维度增加到 $d_k = 2$（仍然在各头之间共享），则存在一个单一的共享键投影和逐头的查询，可以同时实现这两个排序。**\n\n现在，$d_k=2$。我们有一个单一的共享键投影 $W_K$，它是一个 $2 \\times 2$ 矩阵。键 $k_1, k_2, k_3 \\in \\mathbb{R}^2$ 是共享的。查询 $q_1, q_2 \\in \\mathbb{R}^2$ 是逐头的。\n先前推导出的分数约束仍然成立：\n头1：$s_{1,1} = q_1^T k_1 > 0$ 和 $s_{1,2} = q_1^T k_2  < 0$。\n头2：$s_{2,2} = q_2^T k_2  < 0$ 和 $s_{2,1} = q_2^T k_1  < s_{2,2}$。\n\n让我们找一个配置。选择一个可逆的键投影以确保生成的键是线性无关的，例如 $W_K = I = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$。\n那么共享键为 $k_1 = W_K x_1 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$ 和 $k_2 = W_K x_2 = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$。和之前一样，$k_3 = k_1+k_2 = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$。\n\n对于头1：设 $q_1 = \\begin{bmatrix} q_{11} \\\\ q_{12} \\end{bmatrix}$。我们需要 $q_1^T k_1 = q_{11} > 0$ 和 $q_1^T k_2 = q_{12}  < 0$。我们可以选择 $q_1 = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}$。\n那么 $s_{1,1} = 2$，$s_{1,2} = -1$，以及 $s_{1,3} = 2-1 = 1$。排序 $2 > 1 > -1$ 对应于 $s_{1,1} > s_{1,3} > s_{1,2}$，这是正确的。\n\n对于头2：设 $q_2 = \\begin{bmatrix} q_{21} \\\\ q_{22} \\end{bmatrix}$。我们需要 $q_2^T k_2 = q_{22}  < 0$ 和 $q_2^T k_1 = q_{21}  < q_{22}$。我们可以选择 $q_2 = \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix}$。\n那么 $s_{2,1} = -2$，$s_{2,2} = -1$，以及 $s_{2,3} = -2-1 = -3$。排序 $-1 > -2 > -3$ 对应于 $s_{2,2} > s_{2,1} > s_{2,3}$，这是正确的。\n\n从几何上看，当 $d_k=2$ 时，查询向量 $q_1$ 和 $q_2$ 在二维键空间中定义了穿过原点的直线。分数 $q^T k$ 取决于键向量 $k$ 位于由 $q$ 定义的直线的哪一侧。通过选择不同的查询向量（直线），我们可以在这些键向量在直线上的投影上引导出不同的排序，从而实现不同的分数排序。因为我们找到了一个有效的共享键投影和逐头查询，所以该陈述成立。\n\n结论：**正确**。\n\n**D. 在共享键（shared-K）设置中的根本障碍完全来自于在各头之间共享值；如果值是逐头的，而键在 $d_k = 1$ 的情况下保持共享，那么这两种排序将变得可以实现。**\n\n决定排序的注意力权重仅根据查询和键计算：$\\alpha_{h,ij} \\propto \\exp(s_{h,j}) = \\exp(q_h^T k_{h,j})$。值向量 $v_{h,j}$ 用于在权重确定后计算注意力头的最终输出 $\\sum_j \\alpha_{h,ij} v_{h,j}$。值投影，无论是共享的还是非共享的，对注意力分数或权重本身的计算没有影响。\n对于共享键、$d_k = 1$ 的设置，其障碍（如选项 B 中所解释的）是无法从单个标量键集合中生成多于一种排序（及其逆序）。将值向量设为逐头并不会改变这个限制。因此，该陈述基于一个关于自注意力机制的错误前提。\n\n结论：**不正确**。",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "在理论上理解了多头的优势之后，让我们在实践中见证它们的力量。这项编程练习将挑战你构建一个简化的自注意力机制来识别“回文”序列 。你将设计出分别专注于内容或位置的不同注意力头，从而具体地展示多头注意力如何允许模型从多个视角同时分析序列，以解决复杂的模式识别任务。",
            "id": "3154564",
            "problem": "考虑一个在有限维实向量空间中表示的标记（token）序列。每个标记被嵌入为内容向量和位置向量的拼接。内容向量是维度为 $d_c$ 的独热向量（one-hot vector），位置向量是维度为 $d_p$ 的独热向量，其中序列长度有一个固定的最大值。令组合嵌入维度为 $d_{\\text{model}} = d_c + d_p$。您将实现一个三头多头自注意力机制，该机制通过将查询与镜像键对齐来研究回文序列中的对称性。\n\n基础和假设：\n- 两个嵌入向量之间的相似度通过欧几里得内积来衡量。对于向量 $u, v \\in \\mathbb{R}^{d}$，其相似度为 $u \\cdot v = \\sum_{k=1}^{d} u_k v_k$。\n- 为了将相似度转换为每个查询在所有键上的概率分布，使用指数加权规则（softmax）。对于位置 $i$ 处的查询，其在键位置 $j$ 上的 logits 为 $\\ell_{i,j}$，则注意力权重为 $a_{i,j} = \\exp(\\ell_{i,j}) \\big/ \\sum_{j'} \\exp(\\ell_{i,j'})$。\n- 为了在维度增长时稳定内积的尺度，需要将 logits 按比例因子 $\\sqrt{d}$ 进行缩放，其中 $d$ 是对点积有贡献的活动维度的数量。\n\n综合设置的设计：\n1. 词汇表和内容嵌入：\n   - 固定 $d_c = 6$，并将标记 $\\{a, b, c, d, x, y\\}$ 分别映射到索引 $\\{0, 1, 2, 3, 4, 5\\}$。一个标记的内容嵌入是 $\\mathbb{R}^{d_c}$ 中对应的独热向量。\n2. 位置嵌入：\n   - 固定最大位置维度 $d_p = 5$。对于长度为 $L$ 的序列，位置 $i$（从零开始的索引，$i \\in \\{0, 1, \\dots, L-1\\}$）被嵌入为 $\\mathbb{R}^{d_p}$ 中的一个独热向量，其在索引 $i$ 处为 $1$，其他位置为 $0$。\n3. 组合嵌入：\n   - 对于长度为 $L$ 的序列 $s$，位置 $i$ 处的嵌入 $x_i \\in \\mathbb{R}^{d_{\\text{model}}}$ 是内容独热向量和位置独热向量的拼接。\n4. 镜像键（Mirrored keys）：\n   - 对于键，使用反转序列的嵌入：逻辑索引 $j$ 处的键使用原始序列中位置 $L - 1 - j$ 的嵌入。\n5. 三个头：\n   - 头 1（仅内容）：应用一个对角权重掩码，保留内容维度并将位置维度置零。在每个活动内容维度上使用权重值 $2$，在位置维度上使用 $0$。\n   - 头 2（仅位置）：应用一个对角权重掩码，保留位置维度并将内容维度置零。在每个活动位置维度上使用权重值 $2$，在内容维度上使用 $0$。\n   - 头 3（组合）：应用一个对角权重掩码，保留内容和位置维度。在每个活动维度上使用权重值 $2$。\n   - 对于每个头，logits 的缩放因子必须是 $\\sqrt{d_{\\text{active}}}$，其中 $d_{\\text{active}}$ 是该头中具有非零掩码条目的维度数（仅内容：$d_{\\text{active}} = d_c$，仅位置：$d_{\\text{active}} = d_p$，组合：$d_{\\text{active}} = d_c + d_p$）。\n6. 注意力计算：\n   - 对于每个头 $h$，通过将嵌入 $x_i$ 与该头的掩码进行逐元素相乘来计算查询 $Q^{(h)}$，并通过将镜像嵌入与该头的掩码进行逐元素相乘来计算键 $K^{(h)}$。\n   - 计算 logits $\\ell^{(h)}_{i,j} = \\left(Q^{(h)}_i \\cdot K^{(h)}_j\\right) / \\sqrt{d^{(h)}_{\\text{active}}}$。\n   - 使用行级 softmax 将 logits 转换为注意力权重 $A^{(h)}$。\n7. 对称性分数：\n   - 对于序列长度 $L$，将头 $h$ 的反对角线对称性分数定义为\n     $$S^{(h)} = \\frac{1}{L} \\sum_{i=0}^{L-1} A^{(h)}_{i,\\,L-1-i}.$$\n   - 这量化了每个位置 $i$ 对其镜像位置 $L-1-i$ 的注意力权重。\n\n任务：\n- 实现上述带有镜像键的多头自注意力计算，并为每个测试用例计算 $h \\in \\{1,2,3\\}$ 的对称性分数 $S^{(h)}$。\n\n测试套件：\n- 使用以下序列（每个标记来自固定的词汇表）：\n  1. $[a,b,c,b,a]$ (回文，长度 $L=5$)。\n  2. $[a,b,c,b,x]$ (近似回文，有一个不匹配， $L=5$)。\n  3. $[a,c,b,d,y]$ (非回文， $L=5$)。\n  4. $[x,y,y,x]$ (回文， $L=4$)。\n  5. $[d]$ (平凡回文， $L=1$)。\n- 对于每个序列，计算并报告列表 $[S^{(1)}, S^{(2)}, S^{(3)}]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个元素对应一个测试用例，并且本身必须是包含浮点数的列表 $[S^{(1)}, S^{(2)}, S^{(3)}]$。例如，输出应类似于 $[[s_{11},s_{12},s_{13}],[s_{21},s_{22},s_{23}],\\dots]$，其中每个 $s_{ij}$ 是一个十进制数。不涉及物理单位，所有数值输出必须是十进制数（而不是百分比）。",
            "solution": "用户提供的问题已经过严格验证，并被确定为深度学习领域中一个有效、适定且具有科学依据的练习。所有必要的参数、定义和过程都已明确且精确地指定，足以得出一个唯一的、可验证的解决方案。该问题要求实现一个合成的多头自注意力机制，该机制具有特定的“镜像键”设置，用于为几个测试序列计算一个已定义的“对称性分数”。虽然“对称性分数”这一术语可能与直觉相悖——因为对于对称性较差的序列，定义的量值会更高——但其数学定义是明确的，必须严格遵守。\n\n解决方案首先建立问题陈述中定义的模型基础组件。\n\n**1. 基础设置**\n- **词汇表和维度**：一个包含 $6$ 个标记 $\\{a, b, c, d, x, y\\}$ 的词汇表被映射到索引 $\\{0, 1, 2, 3, 4, 5\\}$。内容嵌入维度为 $d_c = 6$，最大位置维度为 $d_p = 5$。总模型维度为 $d_{\\text{model}} = d_c + d_p = 11$。\n- **嵌入**：对于长度为 $L$ 的序列 $s = (s_0, s_1, \\dots, s_{L-1})$，位置 $i$ 处标记 $s_i$ 的嵌入是一个向量 $x_i \\in \\mathbb{R}^{d_{\\text{model}}}$。该向量是内容独热向量 $c_i \\in \\mathbb{R}^{d_c}$ 和位置独热向量 $p_i \\in \\mathbb{R}^{d_p}$ 的拼接。序列的嵌入矩阵为 $X \\in \\mathbb{R}^{L \\times d_{\\text{model}}}$，其中第 $i$ 行为 $x_i^T$。\n\n**2. 注意力头机制**\n三个注意力头由对角掩码定义，这些掩码表示为权重向量 $W^{(h)} \\in \\mathbb{R}^{d_{\\text{model}}}$。每个头 $h$ 的查询 $Q^{(h)}$ 和键 $K^{(h)}$ 计算如下：\n- **查询**：查询矩阵 $Q^{(h)} \\in \\mathbb{R}^{L \\times d_{\\text{model}}}$ 是通过将掩码应用于标准嵌入而形成的：$Q^{(h)}_i = x_i \\odot W^{(h)}$，其中 $\\odot$ 是逐元素乘积。\n- **镜像键**：键矩阵 $K^{(h)} \\in \\mathbb{R}^{L \\times d_{\\text{model}}}$ 是通过将掩码应用于*反转*序列的嵌入而形成的。令 $X_{\\text{rev}}$ 为反转序列的嵌入矩阵，其中第 $j$ 行为 $x_{L-1-j}^T$。那么，$K^{(h)}_j = x_{L-1-j} \\odot W^{(h)}$。\n\n三个头的配置如下：\n- **头 1（仅内容）**：掩码 $W^{(1)}$ 在前 $d_c = 6$ 个维度上的值为 $2$，其余为 $0$。活动维度数为 $d^{(1)}_{\\text{active}} = 6$。\n- **头 2（仅位置）**：掩码 $W^{(2)}$ 在后 $d_p = 5$ 个维度上的值为 $2$，其余为 $0$。活动维度数为 $d^{(2)}_{\\text{active}} = 5$。\n- **头 3（组合）**：掩码 $W^{(3)}$ 在所有 $d_{\\text{model}} = 11$ 个维度上的值为 $2$。活动维度数为 $d^{(3)}_{\\text{active}} = 11$。\n\n**3. Logit 和注意力权重计算**\n对于每个头 $h$，计算 logits 矩阵 $\\mathcal{L}^{(h)} \\in \\mathbb{R}^{L \\times L}$。元素 $\\ell_{i,j}^{(h)}$ 表示位置 $i$ 处的查询与逻辑索引 $j$ 处的键之间的相似度：\n$$\n\\ell_{i,j}^{(h)} = \\frac{Q^{(h)}_i \\cdot K^{(h)}_j}{\\sqrt{d^{(h)}_{\\text{active}}}} = \\frac{(x_i \\odot W^{(h)}) \\cdot (x_{L-1-j} \\odot W^{(h)})}{\\sqrt{d^{(h)}_{\\text{active}}}}\n$$\n然后，通过对 logits 矩阵应用行级 softmax 函数来计算注意力权重矩阵 $A^{(h)} \\in \\mathbb{R}^{L \\times L}$：\n$$\nA^{(h)}_{i,j} = \\frac{\\exp(\\ell_{i,j}^{(h)})}{\\sum_{k=0}^{L-1} \\exp(\\ell_{i,k}^{(h)})}\n$$\n\n**4. 对称性分数计算**\n每个头的对称性分数 $S^{(h)}$ 是矩阵 $A^{(h)}$ 反对角线上注意力权重的平均值。这衡量了每个查询 $Q_i$ 对键 $K_{L-1-i}$ 的关注程度，而该键本身是从原始嵌入 $x_i$ 导出的。\n$$\nS^{(h)} = \\frac{1}{L} \\sum_{i=0}^{L-1} A^{(h)}_{i, L-1-i}\n$$\n与此反对角线元素对应的 logit 是 $\\ell_{i, L-1-i}^{(h)}$，它简化为带掩码的嵌入向量的缩放自点积：\n$$\n\\ell_{i, L-1-i}^{(h)} = \\frac{(x_i \\odot W^{(h)}) \\cdot (x_i \\odot W^{(h)})}{\\sqrt{d^{(h)}_{\\text{active}}}}\n$$\n该 logit 将始终很高。最终的注意力权重 $A^{(h)}_{i, L-1-i}$ 取决于该 logit 与同一行中其他 logits（即 $j \\neq L-1-i$ 的 $\\ell_{i,j}^{(h)}$）的比较。这些其他 logits 衡量了位置 $i$ 处的带掩码查询嵌入与其他镜像位置 $L-1-j$ 处的带掩码键嵌入之间的相似度。对于真正的回文，其他 logits 也可能很大，这会减小 $A^{(h)}_{i, L-1-i}$ 的值，从而与直觉相反地降低了定义的“对称性分数”。\n\n实现将首先为每个序列创建嵌入，然后为三个头中的每一个计算查询、键、logits、注意力权重，最后计算定义的对称性分数。对每个测试用例重复此过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import softmax\n\ndef solve():\n    # Define constants and mappings from the problem statement.\n    D_C = 6\n    D_P = 5\n    D_MODEL = D_C + D_P\n    VOCAB_MAP = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'x': 4, 'y': 5}\n    \n    # Define the test cases.\n    test_cases = [\n        ['a', 'b', 'c', 'b', 'a'],\n        ['a', 'b', 'c', 'b', 'x'],\n        ['a', 'c', 'b', 'd', 'y'],\n        ['x', 'y', 'y', 'x'],\n        ['d']\n    ]\n    \n    # Define the configurations for the three attention heads.\n    # Each tuple: (mask_vector, active_dimensions)\n    mask_weight = 2.0\n    \n    mask1 = np.zeros(D_MODEL)\n    mask1[:D_C] = mask_weight\n    \n    mask2 = np.zeros(D_MODEL)\n    mask2[D_C:] = mask_weight\n    \n    mask3 = np.full(D_MODEL, mask_weight)\n    \n    heads = [\n        (mask1, D_C),\n        (mask2, D_P),\n        (mask3, D_MODEL)\n    ]\n\n    final_results = []\n    \n    for sequence in test_cases:\n        L = len(sequence)\n        \n        # 1. Generate combined embeddings\n        X = np.zeros((L, D_MODEL))\n        for i, token in enumerate(sequence):\n            # Content one-hot vector\n            content_idx = VOCAB_MAP[token]\n            X[i, content_idx] = 1.0\n            # Position one-hot vector\n            pos_idx = i\n            X[i, D_C + pos_idx] = 1.0\n            \n        # 2. Get embeddings for mirrored keys\n        # The key at logical index j uses the embedding from position L-1-j.\n        # This is equivalent to reversing the rows of the embedding matrix.\n        X_rev = np.flip(X, axis=0)\n\n        sequence_scores = []\n        for h, (mask, d_active) in enumerate(heads):\n            # 3. Compute Queries and Keys\n            # Q_i = x_i * mask\n            # K_j = x_{L-1-j} * mask\n            Q = X * mask\n            K = X_rev * mask\n            \n            # 4. Compute logits\n            # Logits_{i,j} = (Q_i . K_j) / sqrt(d_active)\n            # The matrix multiplication Q @ K.T computes all dot products.\n            logits = (Q @ K.T) / np.sqrt(d_active)\n            \n            # 5. Compute attention weights\n            # Row-wise softmax\n            attention_weights = softmax(logits, axis=1)\n            \n            # 6. Compute symmetry score\n            # S = (1/L) * sum(A_{i, L-1-i})\n            # This is the mean of the anti-diagonal of the attention matrix.\n            # np.diag(np.fliplr(A)) extracts the anti-diagonal.\n            anti_diagonal = np.diag(np.fliplr(attention_weights))\n            symmetry_score = np.mean(anti_diagonal)\n            \n            sequence_scores.append(symmetry_score)\n            \n        final_results.append(sequence_scores)\n    \n    # Format the final output string as specified\n    output_str = \"[\" + \",\".join([f\"[{s1:.8f},{s2:.8f},{s3:.8f}]\" for s1, s2, s3 in final_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}