{
    "hands_on_practices": [
        {
            "introduction": "多头自注意力机制的核心优势在于它能让模型同时从不同的表示子空间中学习信息。通过一个精心设计的思想实验 ，我们将对比多头自注意力（MHSA）与一个简化版本——多查询注意力（MQA），以揭示为何独立的头对于捕捉数据中多样化的关系模式至关重要。这个练习将帮助你从根本上理解“多头”设计的初衷和强大之处。",
            "id": "3154513",
            "problem": "一个输入序列由 $3$ 个词元组成，它们在 $\\mathbb{R}^2$ 中表示为 $x_1 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$，$x_2 = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$ 和 $x_3 = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$。考虑一个具有 $H = 2$ 个头的自注意力层。您将比较两种架构：多头自注意力 (MHSA) 和多查询注意力 (MQA)。在 MHSA 中，每个头都有其自己独立的用于查询、键和值的线性投影。在 MQA 中，所有头共享一个键投影和一个值投影，但每个头仍然有其自己的查询投影。除非另有说明，否则假设两种架构中每个头的键维度均为 $d_k = 1$。请只关注单个查询位置以及每个头内部3个词元之间的注意力权重的相对排序；忽略输出投影以及值在注意力加权作用之外的内容。\n\n我们寻求一种配置，其中头 $1$ 对各词元的注意力排序为 $x_1 \\succ x_3 \\succ x_2$（对 $x_1$ 的注意力严格大于对 $x_3$ 的，对 $x_3$ 的注意力严格大于对 $x_2$ 的），而头 $2$ 的排序为 $x_2 \\succ x_1 \\succ x_3$。\n\n以下哪个陈述是正确的？\n\nA. 对于每个头 $d_k = 1$ 的 MHSA，存在每个头的键投影和查询的选择，可以实现所需的排序 $x_1 \\succ x_3 \\succ x_2$（头 $1$）和 $x_2 \\succ x_1 \\succ x_3$（头 $2$）。\n\nB. 对于维度 $d_k = 1$ 的共享键的 MQA，存在特定于头的查询，可以在不改变共享键投影的情况下也实现这两个相同的排序。\n\nC. 如果 MQA 的共享键维度增加到 $d_k = 2$（仍然在各头之间共享），则存在一个单一的共享键投影和特定于头的查询，可以同时实现这两个排序。\n\nD. 在共享K的设置中的根本障碍仅仅源于在各头之间共享值；如果值是每个头独有的，而键在 $d_k = 1$ 的情况下保持共享，那么这两个排序将变得可以实现。",
            "solution": "首先验证问题陈述，以确保其在科学上是合理的、良构的和完整的。\n\n### 步骤1：提取已知条件\n- 输入词元：$\\mathbb{R}^2$ 中的 $x_1 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$，$x_2 = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$，$x_3 = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$。\n- 具有 $H=2$ 个头的自注意力层。\n- 两种架构：\n    1.  **多头自注意力 (MHSA):** 每个头 $h \\in \\{1, 2\\}$ 都有独立的查询 ($W_Q^h$)、键 ($W_K^h$) 和值 ($W_V^h$) 权重矩阵。\n    2.  **多查询注意力 (MQA):** 在各头之间共享键 ($W_K$) 和值 ($W_V$) 投影；查询投影 ($W_Q^h$) 是独立的。\n- 每个头的键维度为 $d_k = 1$，除非另有说明。\n- 分析涉及注意力权重的相对排序，这由 softmax之前的注意力分数决定。\n- 头1所需的排序：对 $x_1$ 的注意力 > 对 $x_3$ 的注意力 > 对 $x_2$ 的注意力。记为 $x_1 \\succ x_3 \\succ x_2$。\n- 头2所需的排序：对 $x_2$ 的注意力 > 对 $x_1$ 的注意力 > 对 $x_3$ 的注意力。记为 $x_2 \\succ x_1 \\succ x_3$。\n\n### 步骤2：使用提取的已知条件进行验证\n- **科学依据：** 该问题基于深度学习中自注意力机制的既定原理，特别是 Transformer 架构。MHSA 和 MQA 是标准变体。所涉及的数学运算（线性投影、点积）是基础的。\n- **良构性：** 该问题要求判断是否存在满足一组严格不等式约束的投影矩阵和查询向量。输入、维度和约束都得到了清晰的定义，使得问题是可解的。\n- **客观性：** 该问题使用深度学习领域的精确数学语言和定义进行表述。所需的排序是客观标准。\n- **完整性和一致性：** 该问题是自洽的。一个关键的观察是输入词元是线性相关的：$x_3 = x_1 + x_2$。这一性质是一致的，并且是分析的核心。似乎没有信息缺失或矛盾。\n\n### 步骤3：结论和行动\n问题陈述是有效的。我们可以继续进行解题推导。\n\n### 推导过程\n\n注意力权重的排序由注意力分数的排序决定。对于给定的头 $h$，词元 $i$ 的分数计算为查询向量 $q_h$ 和键向量 $k_{h,i}$ 的点积。\n$s_{h,i} = q_h^T k_{h,i}$\n输入词元 $x_i$ 的键向量 $k_{h,i}$ 是通过线性投影生成的：$k_{h,i} = W_K^h x_i$（在 MHSA 中）或 $k_{h,i} = W_K x_i$（在 MQA 中）。\n\n一个关键性质源于输入词元的线性相关性，$x_3 = x_1 + x_2$。由于键投影的线性性质，所得到的键向量也将是线性相关的：\n$$k_{h,3} = W_K^{h/\\text{shared}} x_3 = W_K^{h/\\text{shared}} (x_1 + x_2) = W_K^{h/\\text{shared}} x_1 + W_K^{h/\\text{shared}} x_2 = k_{h,1} + k_{h,2}$$\n这个关系对任何头 $h$ 和任何键投影矩阵 $W_K$ 都成立。\n\n因此，注意力分数也表现出这种线性关系：\n$$s_{h,3} = q_h^T k_{h,3} = q_h^T(k_{h,1} + k_{h,2}) = q_h^T k_{h,1} + q_h^T k_{h,2} = s_{h,1} + s_{h,2}$$\n这个基本约束 $s_{h,3} = s_{h,1} + s_{h,2}$ 必须对两个头都满足，无论采用哪种架构。\n\n让我们根据这个约束来分析所需的排序。\n**头1的要求：** $s_{1,1} \\succ s_{1,3} \\succ s_{1,2}$，这意味着 $s_{1,1} > s_{1,3}$ 并且 $s_{1,3} > s_{1,2}$。\n1.  使用 $s_{1,3} = s_{1,1} + s_{1,2}$，第一个不等式 $s_{1,1} > s_{1,3}$ 变为 $s_{1,1} > s_{1,1} + s_{1,2}$，这意味着 $0 > s_{1,2}$。\n2.  第二个不等式 $s_{1,3} > s_{1,2}$ 变为 $s_{1,1} + s_{1,2} > s_{1,2}$，这意味着 $s_{1,1} > 0$。\n因此，对于头1，我们必须有 $s_{1,1} > 0$ 和 $s_{1,2}  0$。\n\n**头2的要求：** $s_{2,2} \\succ s_{2,1} \\succ s_{2,3}$，这意味着 $s_{2,2} > s_{2,1}$ 并且 $s_{2,1} > s_{2,3}$。\n1.  使用 $s_{2,3} = s_{2,1} + s_{2,2}$，不等式 $s_{2,1} > s_{2,3}$ 变为 $s_{2,1} > s_{2,1} + s_{2,2}$，这意味着 $0 > s_{2,2}$。\n2.  另一个不等式是 $s_{2,2} > s_{2,1}$。\n因此，对于头2，我们必须有 $s_{2,2}  0$ 和 $s_{2,1}  s_{2,2}$。\n\n### 逐项分析\n\n**A. 对于每个头 $d_k = 1$ 的 MHSA，存在每个头的键投影和查询的选择，可以实现所需的排序 $x_1 \\succ x_3 \\succ x_2$（头 $1$）和 $x_2 \\succ x_1 \\succ x_3$（头 $2$）。**\n\n在 MHSA 中，每个头 $h$ 都有其自己的键投影 $W_K^h$ 和查询投影 $W_Q^h$。对于 $d_k=1$，$W_K^h$ 是一个 $1 \\times 2$ 的矩阵，因此设 $W_K^h = \\begin{bmatrix} w_{h,1}  w_{h,2} \\end{bmatrix}$。键是标量：$k_{h,1} = w_{h,1}$，$k_{h,2} = w_{h,2}$，以及 $k_{h,3} = w_{h,1} + w_{h,2}$。设查询 $q_h$ 为我们可以选择的标量。\n分数为 $s_{h,1} = q_h k_{h,1}$ 和 $s_{h,2} = q_h k_{h,2}$。\n\n对于头1：我们需要 $s_{1,1} > 0$ 和 $s_{1,2}  0$。我们可以选择 $q_1 = 1$。这要求 $k_{1,1} = w_{1,1} > 0$ 和 $k_{1,2} = w_{1,2}  0$。例如，设 $W_K^1 = \\begin{bmatrix} 2  -1 \\end{bmatrix}$。当 $q_1=1$ 时，分数为 $s_{1,1}=2$，$s_{1,2}=-1$，以及 $s_{1,3}=2-1=1$。排序为 $2 > 1 > -1$，即 $s_{1,1} > s_{1,3} > s_{1,2}$。这满足要求。\n\n对于头2：我们需要 $s_{2,2}  0$ 和 $s_{2,1}  s_{2,2}$。我们可以选择 $q_2 = 1$。这要求 $k_{2,2} = w_{2,2}  0$ 和 $k_{2,1} = w_{2,1}  w_{2,2}$。例如，设 $W_K^2 = \\begin{bmatrix} -2  -1 \\end{bmatrix}$。当 $q_2=1$ 时，分数为 $s_{2,1}=-2$，$s_{2,2}=-1$，以及 $s_{2,3}=-2-1=-3$。排序为 $-1 > -2 > -3$，即 $s_{2,2} > s_{2,1} > s_{2,3}$。这满足要求。\n\n由于在 MHSA 中我们可以独立地选择 $W_K^1$ 和 $W_K^2$，我们已经证明了这样的配置是存在的。\n\n结论：**正确**。\n\n**B. 对于维度 $d_k = 1$ 的共享键的 MQA，存在特定于头的查询，可以在不改变共享键投影的情况下也实现这两个相同的排序。**\n\n在 MQA 中，有一个单一的共享键投影 $W_K$。当 $d_k=1$ 时，两个头共用一组标量键 $\\{k_1, k_2, k_3\\}$。查询 $q_1$ 和 $q_2$ 是特定于头的标量。\n头 $h$ 的分数为 $s_{h,i} = q_h k_i$。分数 $\\{s_{h,1}, s_{h,2}, s_{h,3}\\}$ 的排序只取决于键 $\\{k_1, k_2, k_3\\}$ 的排序和 $q_h$ 的符号。如果 $q_h > 0$，分数排序与键排序相同。如果 $q_h  0$，分数排序与键排序完全相反。\n因此，一个共享的 $d_k=1$ 键投影在所有头中最多只允许两种不同的分数排序：一种特定的排序及其完全相反的排序。\n头1所需的排序（基于词元索引）是 $(1, 3, 2)$。这个排序的逆序是 $(2, 3, 1)$。\n头2所需的排序是 $(2, 1, 3)$。\n由于排序 $(2, 1, 3)$ 不是 $(1, 3, 2)$ 的逆序，所以单一的共享标量键集合不可能产生这两种排序。\n\n结论：**不正确**。\n\n**C. 如果 MQA 的共享键维度增加到 $d_k=2$（仍然在各头之间共享），则存在一个单一的共享键投影和特定于头的查询，可以同时实现这两个排序。**\n\n现在，$d_k=2$。我们有一个单一的共享键投影 $W_K$，它是一个 $2 \\times 2$ 的矩阵。键 $k_1, k_2, k_3 \\in \\mathbb{R}^2$ 是共享的。查询 $q_1, q_2 \\in \\mathbb{R}^2$ 是特定于头的。\n前面推导出的分数约束仍然成立：\n头1：$s_{1,1} = q_1^T k_1 > 0$ 且 $s_{1,2} = q_1^T k_2  0$。\n头2：$s_{2,2} = q_2^T k_2  0$ 且 $s_{2,1} = q_2^T k_1  s_{2,2}$。\n\n让我们找一个配置。选择一个可逆的键投影以确保生成的键是线性无关的，例如，令 $W_K = I = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$。\n共享键即为 $k_1 = W_K x_1 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$ 和 $k_2 = W_K x_2 = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$。和之前一样，$k_3 = k_1+k_2 = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$。\n\n对于头1：设 $q_1 = \\begin{bmatrix} q_{11} \\\\ q_{12} \\end{bmatrix}$。我们需要 $q_1^T k_1 = q_{11} > 0$ 和 $q_1^T k_2 = q_{12}  0$。我们可以选择 $q_1 = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}$。\n那么 $s_{1,1} = 2$，$s_{1,2} = -1$，以及 $s_{1,3} = 2-1 = 1$。排序 $2 > 1 > -1$ 对应于 $s_{1,1} > s_{1,3} > s_{1,2}$，这是正确的。\n\n对于头2：设 $q_2 = \\begin{bmatrix} q_{21} \\\\ q_{22} \\end{bmatrix}$。我们需要 $q_2^T k_2 = q_{22}  0$ 和 $q_2^T k_1 = q_{21}  q_{22}$。我们可以选择 $q_2 = \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix}$。\n那么 $s_{2,1} = -2$，$s_{2,2} = -1$，以及 $s_{2,3} = -2-1 = -3$。排序 $-1 > -2 > -3$ 对应于 $s_{2,2} > s_{2,1} > s_{2,3}$，这是正确的。\n\n从几何角度看，当 $d_k=2$ 时，查询向量 $q_1$ 和 $q_2$ 在二维键空间中定义了穿过原点的直线。分数 $q^T k$ 取决于键向量 $k$ 位于由 $q$ 定义的直线的哪一侧。通过选择不同的查询向量（直线），我们可以在键向量到这些直线上的投影上导出不同的顺序，从而实现不同的分数排序。由于我们找到了一个有效的共享键投影和特定于头的查询，该陈述成立。\n\n结论：**正确**。\n\n**D. 在共享K的设置中的根本障碍仅仅源于在各头之间共享值；如果值是每个头独有的，而键在 $d_k = 1$ 的情况下保持共享，那么这两个排序将变得可以实现。**\n\n决定排序的注意力权重仅根据查询和键来计算：$\\alpha_{h,ij} \\propto \\exp(s_{h,j}) = \\exp(q_h^T k_{h,j})$。值向量 $v_{h,j}$ 用于在权重确定之后计算注意力头的最终输出 $\\sum_j \\alpha_{h,ij} v_{h,j}$。值投影，无论是否共享，对注意力分数或权重本身的计算都没有影响。\n对于共享键、$d_k = 1$ 的设置，如选项B中所解释的，其障碍在于无法从单一的标量键集合生成超过一种排序（及其逆序）。将值向量设为每个头独有并不会改变这个限制。因此，该陈述是基于对自注意力机制的错误前提。\n\n结论：**不正确**。",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "在理解了多头注意力的概念优势之后，下一步是分析其计算成本，这对任何深度学习实践者都至关重要。本练习  将引导你推导出一个视觉 Transformer (ViT) 中多头自注意力模块的精确计算复杂度。通过这个过程，你将深入理解 Transformer 架构的伸缩特性，特别是其计算量如何随输入序列长度 $L$ 以 $\\mathcal{O}(L^2)$ 的级别增长，这是设计高效模型的关键考量。",
            "id": "3199246",
            "problem": "一个视觉 Transformer (ViT) 通过将图像分割成不重叠的图像块、嵌入每个图像块并应用多头自注意力 (MHSA) 来处理图像。考虑一个 ViT 层，它接收一个空间尺寸为 $H \\times W$ 的图像，使用大小为 $p$ 的图像块（假设 $p$ 能同时整除 $H$ 和 $W$），并为每个令牌（token）生成一个维度为 $D$ 的嵌入。令令牌数量为 $L = \\frac{H}{p} \\cdot \\frac{W}{p}$，并令 MHSA 使用 $M$ 个头，每个头的维度为 $d = \\frac{D}{M}$，其中 $M$ 能整除 $D$。该层实现了标准的缩放点积注意力：查询、键和值是通过对输入进行线性投影计算得出的，注意力分数是通过查询和键之间的点积计算的，对每个查询的分数向量应用 softmax，得到的权重用于对值进行加权，最后将拼接后的头部输出线性投影回维度 $D$。\n\n仅从以下公认的基础出发：\n- 一个大小为 $a \\times b$ 的密集矩阵与一个大小为 $b \\times c$ 的密集矩阵相乘的计算成本是 $abc$ 次融合乘加 (MAC) 运算。\n- 一个将大小为 $D \\times D$ 的权重矩阵应用于 $L$ 个令牌的线性层成本为 $LDD$ 次 MAC 运算。\n- 与矩阵乘法相比，您可以忽略偏置加法、softmax 和逐元素缩放的成本。\n\n推导出单个 MHSA 块（不包括层归一化和多层感知器）所需 MAC 运算的确切总数，并仅用 $H$、$W$、$p$ 和 $D$ 表示。您的推导必须涵盖查询、键和值的投影、注意力分数的乘法、注意力权重对值的应用以及最终的输出投影。然后，使用不等式推理，讨论在哪种情况下，与 $\\mathcal{O}(LD^{2})$ 项相比，$\\mathcal{O}(L^{2} D)$ 项成为计算瓶颈，其中 $L = \\frac{H}{p} \\cdot \\frac{W}{p}$。\n\n请提供您的最终答案，形式为一个单一的闭式解析表达式，用 $H$、$W$、$p$ 和 $D$ 表示 MHSA 块的总 MAC 计数。最终答案中不要包含大$\\mathcal{O}$符号。不需要进行数值评估。",
            "solution": "在进行解答之前，首先对问题陈述的有效性进行严格评估。\n\n### 步骤 1：提取已知条件\n- 图像空间尺寸：$H \\times W$\n- 图像块尺寸：$p$\n- 约束条件：$p$ 能同时整除 $H$ 和 $W$\n- 每个令牌的嵌入维度：$D$\n- 令牌数量：$L = \\frac{H}{p} \\cdot \\frac{W}{p}$\n- 注意力头数：$M$\n- 每个头的维度：$d = \\frac{D}{M}$\n- 约束条件：$M$ 能整除 $D$\n- 注意力机制：标准缩放点积注意力\n- 大小为 $a \\times b$ 的矩阵与大小为 $b \\times c$ 的矩阵相乘的计算成本基础：$abc$ 次融合乘加 (MAC) 运算。\n- 将大小为 $D \\times D$ 的权重矩阵应用于 $L$ 个令牌的线性层的计算成本基础：$LDD$ 次 MAC 运算。\n- 排除项：偏置加法、softmax 和逐元素缩放的成本将被忽略。\n- 推导必须涵盖：\n    1. 查询、键和值的投影。\n    2. 注意力分数的乘法。\n    3. 注意力权重对值的应用。\n    4. 最终输出投影。\n- 总 MAC 计数的最终表达式必须用 $H$、$W$、$p$ 和 $D$ 表示。\n- 需要讨论 $\\mathcal{O}(L^{2} D)$ 项与 $\\mathcal{O}(L D^{2})$ 项的计算模式。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n- **科学性：** 该问题牢固地植根于视觉 Transformer 的既定架构，并使用深度学习中计算复杂性分析的标准定义。所有概念都是该领域的标准概念。\n- **适定性：** 该问题提供了所有必要的变量（$H$、$W$、$p$、$D$）、定义（$L = \\frac{HW}{p^2}$，$d = \\frac{D}{M}$）、约束条件（$p$ 能整除 $H, W$；$M$ 能整除 $D$）和一个清晰的成本模型（矩阵乘法为 $abc$ 次 MAC 运算）。其目标是推导一个由所提供信息唯一确定的特定量。\n- **客观性：** 该问题以精确、正式的语言陈述，没有歧义或主观性陈述。\n\n该问题没有任何使其无效的缺陷。它是自洽的、科学上合理的，并且结构正式。\n\n### 步骤 3：结论与行动\n该问题被判定为 **有效**。将提供一个完整的、有理有据的解答。\n\n### 解答推导\n一个多头自注意力 (MHSA) 块的 MAC 运算总计算成本是其组成矩阵乘法运算成本的总和。设该块的输入是一个大小为 $L \\times D$ 的张量 $X$，其中 $L$ 是令牌（图像块）的数量，$D$ 是嵌入维度。\n\n1.  **查询、键和值 (QKV) 投影：**\n    输入张量 $X$ 经过线性投影，生成查询 ($Q$)、键 ($K$) 和值 ($V$) 张量。每个投影都使用一个独立的大小为 $D \\times D$ 的权重矩阵。\n    - $Q = X W_Q$\n    - $K = X W_K$\n    - $V = X W_V$\n    输入矩阵 $X$（大小为 $L \\times D$）与一个权重矩阵（大小为 $D \\times D$）相乘的成本为 $L \\times D \\times D = L D^2$ 次 MAC 运算。由于有三个这样的投影，此步骤的总成本为：\n    $$ \\text{Cost}_{\\text{QKV}} = 3 \\times (L D^2) = 3 L D^2 $$\n\n2.  **注意力分数计算 ($Q K^T$)：**\n    对于多头注意力，张量 $Q$、$K$ 和 $V$ 被分割成 $M$ 个头。每个头的维度是 $d = \\frac{D}{M}$。对于每个头 $i$，我们有 $Q_i$、$K_i$ 和 $V_i$，它们是大小为 $L \\times d$ 的矩阵。\n    单个头的注意力分数通过矩阵乘积 $A_i = Q_i K_i^T$ 计算。\n    - $Q_i$ 的维度是 $L \\times d$。\n    - $K_i^T$ 的维度是 $d \\times L$。\n    这次乘法的成本是 $L \\times d \\times L = L^2 d$ 次 MAC 运算。\n    由于有 $M$ 个头，计算所有注意力分数的总成本是：\n    $$ \\text{Cost}_{\\text{scores}} = M \\times (L^2 d) = L^2 (M d) $$\n    鉴于 $D = M d$，这可以简化为：\n    $$ \\text{Cost}_{\\text{scores}} = L^2 D $$\n\n3.  **注意力权重对值的应用：**\n    每个头的注意力分数（在被忽略的 softmax 操作之后）是一个大小为 $L \\times L$ 的矩阵，它与相应的值矩阵 $V_i$（大小为 $L \\times d$）相乘。\n    - 头 $i$ 的注意力矩阵维度为 $L \\times L$。\n    - 头 $i$ 的值矩阵 $V_i$ 维度为 $L \\times d$。\n    单个头的这次乘法成本是 $L \\times L \\times d = L^2 d$ 次 MAC 运算。\n    同样，对所有 $M$ 个头求和，总成本是：\n    $$ \\text{Cost}_{\\text{values}} = M \\times (L^2 d) = L^2 (M d) $$\n    这可以简化为：\n    $$ \\text{Cost}_{\\text{values}} = L^2 D $$\n\n4.  **最终输出投影：**\n    所有 $M$ 个头的输出（每个大小为 $L \\times d$）被拼接起来，形成一个大小为 $L \\times (M d) = L \\times D$ 的张量。然后，该张量通过一个最终的线性投影层，其权重矩阵 $W_O$ 的大小为 $D \\times D$。\n    - 拼接后的输出张量维度为 $L \\times D$。\n    - 输出权重矩阵 $W_O$ 的维度为 $D \\times D$。\n    这个最终投影的成本是：\n    $$ \\text{Cost}_{\\text{output}} = L \\times D \\times D = L D^2 $$\n\n5.  **总计算成本：**\n    MHSA 块的 MAC 运算总数是这四个步骤成本的总和：\n    $$ \\text{Total MACs} = \\text{Cost}_{\\text{QKV}} + \\text{Cost}_{\\text{scores}} + \\text{Cost}_{\\text{values}} + \\text{Cost}_{\\text{output}} $$\n    $$ \\text{Total MACs} = 3 L D^2 + L^2 D + L^2 D + L D^2 $$\n    $$ \\text{Total MACs} = 4 L D^2 + 2 L^2 D $$\n\n6.  **用 $H$、$W$、$p$ 和 $D$ 表示的表达式：**\n    问题要求最终答案用初始图像和模型参数表示。我们将 $L = \\frac{H W}{p^2}$ 代入总成本表达式中：\n    $$ \\text{Total MACs} = 4 \\left( \\frac{H W}{p^2} \\right) D^2 + 2 \\left( \\frac{H W}{p^2} \\right)^2 D $$\n    $$ \\text{Total MACs} = \\frac{4 H W D^2}{p^2} + \\frac{2 H^2 W^2 D}{p^4} $$\n    这就是总 MAC 计数的最终闭式解析表达式。\n\n### 计算模式讨论\n总计算成本由两个具有不同缩放性质的项组成：一个与 $L D^2$ 成正比的项（来自线性投影），以及一个与 $L^2 D$ 成正比的项（来自注意力机制本身）。具体来说，我们有 $4 L D^2$ 和 $2 L^2 D$。\n\n当 $\\mathcal{O}(L^2 D)$ 项（即 $2 L^2 D$）的大小超过 $\\mathcal{O}(L D^2)$ 项（即 $4 L D^2$）时，它就成为计算瓶颈。我们可以通过建立一个不等式来确定这种模式：\n$$ 2 L^2 D > 4 L D^2 $$\n由于 $L$（令牌数量）和 $D$（嵌入维度）是正整数，我们可以将不等式两边同时除以 $2 L D$ 而不改变不等号的方向：\n$$ L > 2 D $$\n这个不等式定义了这样一种模式：自注意力相对于序列长度（令牌数量）的二次复杂度在成本上超过了线性投影的成本。\n代入 $L = \\frac{H W}{p^2}$，条件变为：\n$$ \\frac{H W}{p^2} > 2 D $$\n这一分析突显了 Transformer 架构的一个基本特征。对于固定的嵌入维度 $D$ 和图像块大小 $p$，注意力成本 $2L^2D$ 随像素数量（$H \\times W$）呈二次方增长，而投影成本 $4LD^2$ 呈线性增长。因此，对于涉及高分辨率图像（大的 $H$ 和 $W$）或小图像块（小的 $p$）的应用，令牌数量 $L$ 会变得非常大，导致满足 $L > 2D$ 的条件，并使注意力计算成为主要的性能瓶颈。这种二次方扩展是标准 ViT 在处理高分辨率视觉任务时计算密集的主要原因，并推动了更高效注意力机制（例如，窗口化、稀疏或线性注意力）的发展。",
            "answer": "$$ \\boxed{\\frac{4 H W D^{2}}{p^{2}} + \\frac{2 H^{2} W^{2} D}{p^{4}}} $$"
        },
        {
            "introduction": "最后，让我们将视野从单个模块扩展到一个完整的、真实的 Transformer 编码器层。本练习  要求你精确计算一个标准 BERT 编码器块中的所有可学习参数。通过将多头自注意力（MHSA）视为其中的一个关键组件，并与其他部分（如前馈网络和层归一化）一起分析，你将对整个架构的构成和模型大小有更具体、更量化的认识。",
            "id": "3102535",
            "problem": "考虑一个来自“来自变换器的双向编码器表示”（Bidirectional Encoder Representations from Transformers, BERT）中的单个编码器块。该块包含一个多头自注意力（Multi-Head Self-Attention, MHSA）模块，其后是一个双层逐位置前馈网络，并带有残差连接和两个层归一化（Layer Normalization, LN）模块。假设采用以下与原始架构一致的规范设计选择：\n- 隐藏层大小为 $d$，注意力头的数量为 $h$，前馈网络内部维度为 $d_{ff}$。\n- 查询、键和值投影实现为从 $\\mathbb{R}^{d}$到$\\mathbb{R}^{d}$ 的可学习仿射映射，每个映射都带有一个 $\\mathbb{R}^{d \\times d}$ 的权重矩阵和一个 $\\mathbb{R}^{d}$ 的偏置，注意力输出投影同样是一个可学习的仿射映射，其权重在 $\\mathbb{R}^{d \\times d}$ 中，偏置在 $\\mathbb{R}^{d}$ 中。\n- 前馈网络由两个可学习的仿射映射组成：第一个从 $\\mathbb{R}^{d}$到$\\mathbb{R}^{d_{ff}}$，第二个从 $\\mathbb{R}^{d_{ff}}$ 回到 $\\mathbb{R}^{d}$，每个映射都带有一个权重矩阵和一个偏置向量。两者之间的非线性激活函数是高斯误差线性单元（Gaussian Error Linear Unit, GELU），它没有可学习的参数。\n- 每个块中应用两次层归一化，每个LN都有一个可学习的缩放因子 $\\gamma \\in \\mathbb{R}^{d}$ 和一个位移因子 $\\beta \\in \\mathbb{R}^{d}$。该块中没有其他可学习的参数。\n- 词元嵌入和位置编码是块外部嵌入层的一部分，此处不得计算在内。\n\n仅从核心定义出发——一个从 $\\mathbb{R}^{m}$ 到 $\\mathbb{R}^{n}$ 的可学习仿射映射有 $n \\times m$ 个权重参数和 $n$ 个偏置参数，且每次应用层归一化会贡献 $2 \\times d$ 个参数（缩放和位移）——推导这个编码器块总可学习参数数量关于 $d$、$d_{ff}$ 和 $h$ 的封闭形式表达式。然后，提出一个变体，将头的数量从 $h$ 增加到 $\\alpha h$，同时对于给定的隐藏层大小 $d$ 和序列长度，保持 MHSA 投影和注意力分数乘法的总计算量不变（你可以假设每个头的维度是 $d/h$，并且保持 $d$ 不变可以固定主要的 MHSA 计算量）。对于这个 $\\alpha = 2$ 的变体，给出最终的每个头的维度。\n\n最后，对 BERT-Base 配置 $d = 768$，$d_{ff} = 3072$，$h = 12$ 和 $\\alpha = 2$ 对你的表达式进行数值计算。将你的最终答案以一个包含两个元素的行矩阵形式给出：\n- 编码器块的总参数数量，以及\n- 将头的数量增加因子 $\\alpha$ 后的新每个头的维度。\n\n无需四舍五入；报告精确整数。将最终答案表示为 $\\begin{pmatrix} \\text{参数数量}  \\text{新头维度} \\end{pmatrix}$ 的行矩阵形式。",
            "solution": "问题陈述被评估为有效。它在科学上基于已建立的 Transformer 模型架构，问题定义明确，信息充分，可得出唯一解，并以客观、正式的语言表述。所提供的数据是一致且现实的，对应于众所周知的 BERT-Base 配置。该问题是分析深度学习模型参数化的一个标准且非平凡的练习。\n\n解决方案分三部分进行：首先，推导 BERT 编码器块中可学习参数数量的一般符号表达式；其次，分析所提出的架构变体；第三，对给定配置下的推导表达式进行数值计算。\n\n**第一部分：总可学习参数数量的推导**\n\n总可学习参数数量 $P_{\\text{total}}$ 是其组成部分参数的总和：多头自注意力（MHSA）模块、逐位置前馈网络（FFN）和两个层归一化（LN）模块。我们将根据所提供的定义计算每个组件的参数。\n\n一个从 $\\mathbb{R}^{m}$ 到 $\\mathbb{R}^{n}$ 的可学习仿射映射有一个大小为 $n \\times m$ 的权重矩阵和一个大小为 $n$ 的偏置向量，总共有 $n \\times m + n$ 个参数。\n\n1.  **多头自注意力（MHSA）参数 ($P_{\\text{MHSA}}$)**\n    MHSA 机制由问题陈述中定义的四个可学习仿射投影组成。\n    *   **查询投影（Q）：** 从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d}$ 的仿射映射。\n        - 权重矩阵 $W_Q \\in \\mathbb{R}^{d \\times d}$：$d \\times d = d^2$ 个参数。\n        - 偏置向量 $b_Q \\in \\mathbb{R}^{d}$：$d$ 个参数。\n        - Q 的总参数：$d^2 + d$。\n    *   **键投影（K）：** 从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d}$ 的仿射映射。\n        - 权重矩阵 $W_K \\in \\mathbb{R}^{d \\times d}$：$d^2$ 个参数。\n        - 偏置向量 $b_K \\in \\mathbb{R}^{d}$：$d$ 个参数。\n        - K 的总参数：$d^2 + d$。\n    *   **值投影（V）：** 从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d}$ 的仿射映射。\n        - 权重矩阵 $W_V \\in \\mathbb{R}^{d \\times d}$：$d^2$ 个参数。\n        - 偏置向量 $b_V \\in \\mathbb{R}^{d}$：$d$ 个参数。\n        - V 的总参数：$d^2 + d$。\n    *   **输出投影（O）：** 从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d}$ 的仿射映射。\n        - 权重矩阵 $W_O \\in \\mathbb{R}^{d \\times d}$：$d^2$ 个参数。\n        - 偏置向量 $b_O \\in \\mathbb{R}^{d}$：$d$ 个参数。\n        - O 的总参数：$d^2 + d$。\n    MHSA 模块中的总参数数量是这四个组件的总和。请注意，头的数量 $h$ 在此公式中没有明确出现，因为投影被定义为对维度为 $d$ 的整个隐藏状态的单一变换。\n    $$P_{\\text{MHSA}} = (d^2 + d) + (d^2 + d) + (d^2 + d) + (d^2 + d) = 4(d^2 + d) = 4d^2 + 4d$$\n\n2.  **逐位置前馈网络（FFN）参数 ($P_{\\text{FFN}}$)**\n    FFN 由两个可学习的仿射映射和一个 GELU 非线性激活函数组成，该激活函数没有参数。\n    *   **第一层：** 从 $\\mathbb{R}^{d}$ 到 $\\mathbb{R}^{d_{ff}}$ 的仿射映射。\n        - 权重矩阵 $W_1 \\in \\mathbb{R}^{d_{ff} \\times d}$：$d_{ff} \\times d$ 个参数。\n        - 偏置向量 $b_1 \\in \\mathbb{R}^{d_{ff}}$：$d_{ff}$ 个参数。\n        - 第一层的总参数：$d \\cdot d_{ff} + d_{ff}$。\n    *   **第二层：** 从 $\\mathbb{R}^{d_{ff}}$ 到 $\\mathbb{R}^{d}$ 的仿射映射。\n        - 权重矩阵 $W_2 \\in \\mathbb{R}^{d \\times d_{ff}}$：$d \\times d_{ff}$ 个参数。\n        - 偏置向量 $b_2 \\in \\mathbb{R}^{d}$：$d$ 个参数。\n        - 第二层的总参数：$d \\cdot d_{ff} + d$。\n    FFN 中的总参数数量是这两层的总和。\n    $$P_{\\text{FFN}} = (d \\cdot d_{ff} + d_{ff}) + (d \\cdot d_{ff} + d) = 2d \\cdot d_{ff} + d_{ff} + d$$\n\n3.  **层归一化（LN）参数 ($P_{\\text{LN}}$)**\n    该块包含两个层归一化模块。每个模块都有一个可学习的缩放向量 $\\gamma \\in \\mathbb{R}^{d}$ 和一个可学习的位移向量 $\\beta \\in \\mathbb{R}^{d}$。\n    - 每个 LN 模块的参数：$d + d = 2d$。\n    - 两个 LN 模块的总参数：\n    $$P_{\\text{LN}} = 2 \\times (2d) = 4d$$\n\n**总参数 ($P_{\\text{total}}$)**\n编码器块中可学习参数的总数是所有组件参数的总和。\n$$P_{\\text{total}} = P_{\\text{MHSA}} + P_{\\text{FFN}} + P_{\\text{LN}}$$\n$$P_{\\text{total}} = (4d^2 + 4d) + (2d \\cdot d_{ff} + d_{ff} + d) + (4d)$$\n$$P_{\\text{total}} = 4d^2 + (4d + d + 4d) + 2d \\cdot d_{ff} + d_{ff}$$\n$$P_{\\text{total}} = 4d^2 + 9d + d_{ff}(2d + 1)$$\n这就是总参数数量的封闭形式表达式。\n\n**第二部分：架构变体与新每个头的维度**\n\n问题提出了一个变体，其中注意力头的数量从 $h$ 变为 $h' = \\alpha h$，而模型的隐藏层大小 $d$ 保持不变。在标准的 Transformer 架构中，所有头的总维度等于模型的隐藏层大小。也就是说，如果 $d_k$ 是每个头的查询、键和值向量的维度，则 $h \\cdot d_k = d$。\n\n原始的每个头的维度是 $d_k = \\frac{d}{h}$。\n新的头的数量是 $h' = \\alpha h$。\n为了维持关系 $h' \\cdot d'_k = d$（其中 $d'_k$ 是新的每个头的维度），我们可以解出 $d'_k$：\n$$d'_k = \\frac{d}{h'} = \\frac{d}{\\alpha h} = \\frac{1}{\\alpha} \\left(\\frac{d}{h}\\right)$$\n新的每个头的维度是原始每个头的维度乘以 $1/\\alpha$。\n\n**第三部分：数值计算**\n\n我们给定的 BERT-Base 配置为：$d = 768$，$d_{ff} = 3072$，$h = 12$，以及修改因子 $\\alpha = 2$。\n\n*   **总参数数量：**\n    我们将给定值代入推导出的 $P_{\\text{total}}$ 公式中。\n    $$P_{\\text{total}} = 4(768)^2 + 9(768) + 3072(2 \\cdot 768 + 1)$$\n    $$P_{\\text{total}} = 4(589824) + 6912 + 3072(1536 + 1)$$\n    $$P_{\\text{total}} = 2359296 + 6912 + 3072(1537)$$\n    $$P_{\\text{total}} = 2359296 + 6912 + 4721664$$\n    $$P_{\\text{total}} = 7087872$$\n\n*   **新的每个头的维度：**\n    原始头的数量是 $h=12$。新的头的数量是 $h' = \\alpha h = 2 \\times 12 = 24$。\n    隐藏层大小为 $d = 768$。新的每个头的维度 $d'_k$ 是：\n    $$d'_k = \\frac{d}{h'} = \\frac{768}{24}$$\n    为了计算这个值，我们可以先除以 $12$ 再除以 $2$：\n    $$d'_k = \\frac{768/12}{2} = \\frac{64}{2} = 32$$\n    新的每个头的维度是 $32$。\n\n最终答案要求以一个行矩阵的形式，包含总参数数量和新的每个头的维度。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} 7087872  32 \\end{pmatrix}\n}\n$$"
        }
    ]
}