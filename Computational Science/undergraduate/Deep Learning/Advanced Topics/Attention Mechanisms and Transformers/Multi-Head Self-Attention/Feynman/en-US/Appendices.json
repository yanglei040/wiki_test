{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the power of multi-head self-attention, we must first answer a fundamental question: why use multiple heads at all? This exercise provides the answer by creating a scenario where a single attention \"view\" is insufficient. By comparing Multi-Head Self-Attention (MHSA) with a more constrained variant, Multi-Query Attention (MQA), you will discover how independent heads act as parallel \"lenses\", each capable of projecting the input into a different subspace to capture distinct relational patterns. This practice  illuminates the core design principle that gives Transformers their remarkable representational power.",
            "id": "3154513",
            "problem": "An input sequence consists of $3$ tokens represented in $\\mathbb{R}^2$ as $x_1 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$, $x_2 = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$, and $x_3 = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$. Consider a self-attention layer with $H = 2$ heads. You will compare two architectures: Multi-Head Self-Attention (MHSA) and Multi-Query Attention (MQA). In MHSA, each head has its own independent linear projections for queries, keys, and values. In MQA, all heads share a single key projection and a single value projection, but each head still has its own query projection. Assume the per-head key dimension is $d_k = 1$ for both architectures unless otherwise stated. Focus exclusively on a single query position and the relative ranking of attention weights across the $3$ tokens within each head; ignore output projections and the values' content beyond their role in the attention weighting.\n\nWe seek a configuration where head $1$’s attention ranking over the tokens is $x_1 \\succ x_3 \\succ x_2$ (strictly greater attention on $x_1$ than $x_3$, and on $x_3$ than $x_2$), while head $2$’s ranking is $x_2 \\succ x_1 \\succ x_3$.\n\nWhich of the following statements are correct?\n\nA. With MHSA and $d_k = 1$ per head, there exist choices of per-head key projections and queries that realize the required rankings $x_1 \\succ x_3 \\succ x_2$ (head $1$) and $x_2 \\succ x_1 \\succ x_3$ (head $2$).\n\nB. With MQA and shared keys of dimension $d_k = 1$, there exist head-specific queries that can also realize the same two rankings without changing the shared key projection.\n\nC. If MQA’s shared key dimension is increased to $d_k = 2$ (still shared across heads), there exists a single shared key projection and head-specific queries that realize the two rankings simultaneously.\n\nD. The fundamental obstacle in the shared-$K$ setting arises solely from sharing values across heads; if values were per-head while keys remained shared with $d_k = 1$, the two rankings would become realizable.",
            "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and complete.\n\n### Step 1: Extract Givens\n- Input tokens: $x_1 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$, $x_2 = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$, $x_3 = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$ in $\\mathbb{R}^2$.\n- Self-attention layer with $H=2$ heads.\n- Two architectures:\n    1.  **Multi-Head Self-Attention (MHSA):** Independent weight matrices for query ($W_Q^h$), key ($W_K^h$), and value ($W_V^h$) for each head $h \\in \\{1, 2\\}$.\n    2.  **Multi-Query Attention (MQA):** Shared key ($W_K$) and value ($W_V$) projections across heads; independent query projections ($W_Q^h$).\n- Per-head key dimension is $d_k = 1$, unless specified otherwise.\n- The analysis concerns the relative ranking of attention weights, which is determined by the pre-softmax attention scores.\n- Required ranking for head $1$: attention to $x_1 >$ attention to $x_3 >$ attention to $x_2$. Denoted as $x_1 \\succ x_3 \\succ x_2$.\n- Required ranking for head $2$: attention to $x_2 >$ attention to $x_1 >$ attention to $x_3$. Denoted as $x_2 \\succ x_1 \\succ x_3$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding:** The problem is based on the well-established principles of self-attention mechanisms in deep learning, specifically the Transformer architecture. MHSA and MQA are standard variants. The mathematical operations (linear projection, dot product) are fundamental.\n- **Well-Posedness:** The problem asks for the existence of projection matrices and query vectors that satisfy a set of strict inequality constraints. The inputs, dimensions, and constraints are clearly defined, making the problem solvable.\n- **Objectivity:** The problem is phrased using precise mathematical language and definitions from the field of deep learning. The required rankings are objective criteria.\n- **Completeness and Consistency:** The problem is self-contained. A key observation is that the input tokens are linearly dependent: $x_3 = x_1 + x_2$. This property is consistent and central to the analysis. No information appears to be missing or contradictory.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. We may proceed with the solution derivation.\n\n### Derivation\n\nThe ranking of attention weights is determined by the ranking of the attention scores. For a given head $h$, the score for token $i$ is calculated as the dot product of the query vector $q_h$ and the key vector $k_{h,i}$.\n$s_{h,i} = q_h^T k_{h,i}$\nThe key vector $k_{h,i}$ for input token $x_i$ is generated by a linear projection: $k_{h,i} = W_K^h x_i$ (in MHSA) or $k_{h,i} = W_K x_i$ (in MQA).\n\nA critical property arises from the linear dependency of the input tokens, $x_3 = x_1 + x_2$. Due to the linearity of the key projection, the resulting key vectors will also be linearly dependent:\n$$k_{h,3} = W_K^{h/\\text{shared}} x_3 = W_K^{h/\\text{shared}} (x_1 + x_2) = W_K^{h/\\text{shared}} x_1 + W_K^{h/\\text{shared}} x_2 = k_{h,1} + k_{h,2}$$\nThis relationship holds for any head $h$ and any key projection matrix $W_K$.\n\nConsequently, the attention scores also exhibit this linear relationship:\n$$s_{h,3} = q_h^T k_{h,3} = q_h^T(k_{h,1} + k_{h,2}) = q_h^T k_{h,1} + q_h^T k_{h,2} = s_{h,1} + s_{h,2}$$\nThis fundamental constraint, $s_{h,3} = s_{h,1} + s_{h,2}$, must be satisfied for both heads, regardless of the architecture.\n\nLet's analyze the required rankings in light of this constraint.\n**Head 1 requirement:** $s_{1,1} \\succ s_{1,3} \\succ s_{1,2}$, which means $s_{1,1} > s_{1,3}$ and $s_{1,3} > s_{1,2}$.\n1.  Using $s_{1,3} = s_{1,1} + s_{1,2}$, the first inequality $s_{1,1} > s_{1,3}$ becomes $s_{1,1} > s_{1,1} + s_{1,2}$, which implies $0 > s_{1,2}$.\n2.  The second inequality $s_{1,3} > s_{1,2}$ becomes $s_{1,1} + s_{1,2} > s_{1,2}$, which implies $s_{1,1} > 0$.\nThus, for head $1$, we must have $s_{1,1} > 0$ and $s_{1,2} < 0$.\n\n**Head 2 requirement:** $s_{2,2} \\succ s_{2,1} \\succ s_{2,3}$, which means $s_{2,2} > s_{2,1}$ and $s_{2,1} > s_{2,3}$.\n1.  Using $s_{2,3} = s_{2,1} + s_{2,2}$, the inequality $s_{2,1} > s_{2,3}$ becomes $s_{2,1} > s_{2,1} + s_{2,2}$, which implies $0 > s_{2,2}$.\n2.  The other inequality is $s_{2,2} > s_{2,1}$.\nThus, for head $2$, we must have $s_{2,2} < 0$ and $s_{2,1} < s_{2,2}$.\n\n### Option-by-Option Analysis\n\n**A. With MHSA and $d_k = 1$ per head, there exist choices of per-head key projections and queries that realize the required rankings $x_1 \\succ x_3 \\succ x_2$ (head $1$) and $x_2 \\succ x_1 \\succ x_3$ (head $2$).**\n\nIn MHSA, each head $h$ has its own key projection $W_K^h$ and query projection $W_Q^h$. For $d_k=1$, $W_K^h$ is a $1 \\times 2$ matrix, so let $W_K^h = \\begin{bmatrix} w_{h,1} & w_{h,2} \\end{bmatrix}$. The keys are scalars: $k_{h,1} = w_{h,1}$, $k_{h,2} = w_{h,2}$, and $k_{h,3} = w_{h,1} + w_{h,2}$. Let the query $q_h$ be a scalar which we can choose.\nThe scores are $s_{h,1} = q_h k_{h,1}$ and $s_{h,2} = q_h k_{h,2}$.\n\nFor Head $1$: We need $s_{1,1} > 0$ and $s_{1,2} < 0$. We can choose $q_1 = 1$. This requires $k_{1,1} = w_{1,1} > 0$ and $k_{1,2} = w_{1,2} < 0$. For example, let $W_K^1 = \\begin{bmatrix} 2 & -1 \\end{bmatrix}$. With $q_1=1$, the scores are $s_{1,1}=2$, $s_{1,2}=-1$, and $s_{1,3}=2-1=1$. The ranking is $2 > 1 > -1$, which is $s_{1,1} > s_{1,3} > s_{1,2}$. This satisfies the requirement.\n\nFor Head $2$: We need $s_{2,2} < 0$ and $s_{2,1} < s_{2,2}$. We can choose $q_2 = 1$. This requires $k_{2,2} = w_{2,2} < 0$ and $k_{2,1} = w_{2,1} < w_{2,2}$. For example, let $W_K^2 = \\begin{bmatrix} -2 & -1 \\end{bmatrix}$. With $q_2=1$, the scores are $s_{2,1}=-2$, $s_{2,2}=-1$, and $s_{2,3}=-2-1=-3$. The ranking is $-1 > -2 > -3$, which is $s_{2,2} > s_{2,1} > s_{2,3}$. This satisfies the requirement.\n\nSince we can choose $W_K^1$ and $W_K^2$ independently in MHSA, we have demonstrated that such a configuration exists.\n\nVerdict: **Correct**.\n\n**B. With MQA and shared keys of dimension $d_k = 1$, there exist head-specific queries that can also realize the same two rankings without changing the shared key projection.**\n\nIn MQA, there is a single shared key projection $W_K$. With $d_k=1$, there is a single set of scalar keys $\\{k_1, k_2, k_3\\}$ for both heads. The queries $q_1$ and $q_2$ are head-specific scalars.\nThe scores for head $h$ are $s_{h,i} = q_h k_i$. The ranking of the scores $\\{s_{h,1}, s_{h,2}, s_{h,3}\\}$ depends only on the ranking of the keys $\\{k_1, k_2, k_3\\}$ and the sign of $q_h$. If $q_h > 0$, the score ranking is identical to the key ranking. If $q_h < 0$, the score ranking is the exact reverse of the key ranking.\nTherefore, a shared $d_k=1$ key projection allows for at most two distinct score rankings across all heads: one specific ranking and its exact reverse.\nThe required ranking for head $1$ is $(1, 3, 2)$ based on token indices. The reverse of this ranking is $(2, 3, 1)$.\nThe required ranking for head $2$ is $(2, 1, 3)$.\nSince the ranking $(2, 1, 3)$ is not the reverse of $(1, 3, 2)$, it is impossible for a single shared set of scalar keys to produce both rankings.\n\nVerdict: **Incorrect**.\n\n**C. If MQA’s shared key dimension is increased to $d_k=2$ (still shared across heads), there exists a single shared key projection and head-specific queries that realize the two rankings simultaneously.**\n\nNow, $d_k=2$. We have a single shared key projection $W_K$, which is a $2 \\times 2$ matrix. The keys $k_1, k_2, k_3 \\in \\mathbb{R}^2$ are shared. The queries $q_1, q_2 \\in \\mathbb{R}^2$ are head-specific.\nThe score constraints derived earlier still hold:\nHead $1$: $s_{1,1} = q_1^T k_1 > 0$ and $s_{1,2} = q_1^T k_2 < 0$.\nHead $2$: $s_{2,2} = q_2^T k_2 < 0$ and $s_{2,1} = q_2^T k_1 < s_{2,2}$.\n\nLet's find a configuration. Choose an invertible key projection to ensure the resulting keys are linearly independent, for instance, $W_K = I = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$.\nThe shared keys are then $k_1 = W_K x_1 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$ and $k_2 = W_K x_2 = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$. As before, $k_3 = k_1+k_2 = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$.\n\nFor Head $1$: Let $q_1 = \\begin{bmatrix} q_{11} \\\\ q_{12} \\end{bmatrix}$. We need $q_1^T k_1 = q_{11} > 0$ and $q_1^T k_2 = q_{12} < 0$. We can choose $q_1 = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}$.\nThen $s_{1,1} = 2$, $s_{1,2} = -1$, and $s_{1,3} = 2-1 = 1$. The ranking $2 > 1 > -1$ corresponds to $s_{1,1} > s_{1,3} > s_{1,2}$, which is correct.\n\nFor Head $2$: Let $q_2 = \\begin{bmatrix} q_{21} \\\\ q_{22} \\end{bmatrix}$. We need $q_2^T k_2 = q_{22} < 0$ and $q_2^T k_1 = q_{21} < q_{22}$. We can choose $q_2 = \\begin{bmatrix} -2 \\\\ -1 \\end{bmatrix}$.\nThen $s_{2,1} = -2$, $s_{2,2} = -1$, and $s_{2,3} = -2-1 = -3$. The ranking $-1 > -2 > -3$ corresponds to $s_{2,2} > s_{2,1} > s_{2,3}$, which is correct.\n\nGeometrically, with $d_k=2$, the query vectors $q_1$ and $q_2$ define lines through the origin in the 2D key space. The score $q^T k$ depends on which side of the line defined by $q$ the key vector $k$ lies. By choosing different query vectors (lines), we can induce different orderings on the projections of the key vectors onto these lines, thus achieving different score rankings. Since we have found a valid shared key projection and head-specific queries, the statement holds.\n\nVerdict: **Correct**.\n\n**D. The fundamental obstacle in the shared-K setting arises solely from sharing values across heads; if values were per-head while keys remained shared with $d_k = 1$, the two rankings would become realizable.**\n\nThe attention weights, which determine the ranking, are computed based on queries and keys only: $\\alpha_{h,ij} \\propto \\exp(s_{h,j}) = \\exp(q_h^T k_{h,j})$. The value vectors $v_{h,j}$ are used to compute the final output of the attention head, $\\sum_j \\alpha_{h,ij} v_{h,j}$, after the weights have been determined. The value projections, whether shared or not, have no impact on the calculation of the attention scores or weights themselves.\nThe obstacle for the shared-key, $d_k = 1$ setting, as explained for option B, is the inability to generate more than one ranking (and its reverse) from a single set of scalar keys. Making the value vectors per-head would not alter this limitation. Therefore, the statement is based on a false premise about the mechanics of self-attention.\n\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "Having established the conceptual need for multiple heads, we now turn to the concrete architecture of a Transformer encoder block. This practice is a \"nuts and bolts\" analysis, requiring you to deconstruct a standard BERT-style block into its constituent parts—the attention mechanism, the feed-forward network, and layer normalization modules. By systematically deriving the total number of learnable parameters , you will build a precise mental model of the block's structure and develop an intuition for how design choices like hidden size and feed-forward dimension impact model size.",
            "id": "3102535",
            "problem": "Consider a single encoder block in Bidirectional Encoder Representations from Transformers (BERT). The block contains Multi-Head Self-Attention (MHSA) followed by a two-layer position-wise feed-forward network, with residual connections and two Layer Normalization (LN) modules. Assume the following canonical design choices, which are consistent with the original architecture: \n- The hidden size is $d$, the number of attention heads is $h$, and the feed-forward inner dimension is $d_{ff}$. \n- Query, key, and value projections are implemented as learned affine maps from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{d}$, each with a weight matrix in $\\mathbb{R}^{d \\times d}$ and a bias in $\\mathbb{R}^{d}$, and the attention output projection is likewise a learned affine map with weight in $\\mathbb{R}^{d \\times d}$ and bias in $\\mathbb{R}^{d}$. \n- The feed-forward network consists of two learned affine maps: the first from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{d_{ff}}$ and the second from $\\mathbb{R}^{d_{ff}}$ back to $\\mathbb{R}^{d}$, each with a weight matrix and a bias vector. The nonlinearity in between is Gaussian Error Linear Unit (GELU) and has no learnable parameters. \n- Layer Normalization is applied twice per block, each LN having a learned scale $\\gamma \\in \\mathbb{R}^{d}$ and shift $\\beta \\in \\mathbb{R}^{d}$. There are no other learned parameters in the block. \n- Token embeddings and positional encodings are part of the embedding layer outside the block and must not be counted here.\n\nStarting only from the core definition that a learned affine map from $\\mathbb{R}^{m}$ to $\\mathbb{R}^{n}$ has $n \\times m$ weight parameters and $n$ bias parameters, and that Layer Normalization contributes $2 \\times d$ parameters per application (scale and shift), derive a closed-form expression for the total learnable parameter count of this encoder block in terms of $d$, $d_{ff}$, and $h$. Then, propose a variant that increases the number of heads from $h$ to $\\alpha h$ while keeping the total compute of the MHSA projections and attention score multiplications fixed for a given hidden size $d$ and sequence length (you may assume that the per-head dimension is $d/h$ and that keeping $d$ constant fixes the dominant MHSA compute). For this variant with $\\alpha = 2$, give the resulting per-head dimension.\n\nFinally, evaluate your expression numerically for the BERT-Base configuration $d = 768$, $d_{ff} = 3072$, $h = 12$, and $\\alpha = 2$. Provide your final answer as a two-entry row matrix containing:\n- The total parameter count of the encoder block, and\n- The new per-head dimension after increasing the number of heads by the factor $\\alpha$.\n\nNo rounding is required; report exact integers. Express your final answer as a row matrix in the form $\\begin{pmatrix} \\text{param\\_count} & \\text{new\\_head\\_dim} \\end{pmatrix}$.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the established architecture of Transformer models, is well-posed with sufficient information for a unique solution, and is expressed in objective, formal language. The provided data are consistent and realistic, corresponding to the well-known BERT-Base configuration. The problem is a standard, non-trivial exercise in analyzing the parameterization of a deep learning model.\n\nThe solution is approached in three parts: first, deriving a general symbolic expression for the number of learnable parameters in a BERT encoder block; second, analyzing the proposed architectural variant; and third, evaluating the derived expressions numerically for the given configuration.\n\n**Part 1: Derivation of the Total Learnable Parameter Count**\n\nThe total number of learnable parameters, $P_{\\text{total}}$, is the sum of the parameters from its constituent components: the Multi-Head Self-Attention (MHSA) module, the position-wise Feed-Forward Network (FFN), and the two Layer Normalization (LN) modules. We will calculate the parameters for each component based on the provided definitions.\n\nA learned affine map from $\\mathbb{R}^{m}$ to $\\mathbb{R}^{n}$ has a weight matrix of size $n \\times m$ and a bias vector of size $n$, for a total of $n \\times m + n$ parameters.\n\n1.  **Multi-Head Self-Attention (MHSA) Parameters ($P_{\\text{MHSA}}$)**\n    The MHSA mechanism consists of four learned affine projections as defined in the problem statement.\n    *   **Query projection (Q):** An affine map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{d}$.\n        - Weight matrix $W_Q \\in \\mathbb{R}^{d \\times d}$: $d \\times d = d^2$ parameters.\n        - Bias vector $b_Q \\in \\mathbb{R}^{d}$: $d$ parameters.\n        - Total for Q: $d^2 + d$.\n    *   **Key projection (K):** An affine map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{d}$.\n        - Weight matrix $W_K \\in \\mathbb{R}^{d \\times d}$: $d^2$ parameters.\n        - Bias vector $b_K \\in \\mathbb{R}^{d}$: $d$ parameters.\n        - Total for K: $d^2 + d$.\n    *   **Value projection (V):** An affine map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{d}$.\n        - Weight matrix $W_V \\in \\mathbb{R}^{d \\times d}$: $d^2$ parameters.\n        - Bias vector $b_V \\in \\mathbb{R}^{d}$: $d$ parameters.\n        - Total for V: $d^2 + d$.\n    *   **Output projection (O):** An affine map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{d}$.\n        - Weight matrix $W_O \\in \\mathbb{R}^{d \\times d}$: $d^2$ parameters.\n        - Bias vector $b_O \\in \\mathbb{R}^{d}$: $d$ parameters.\n        - Total for O: $d^2 + d$.\n    The total number of parameters in the MHSA module is the sum of these four components. Note that the number of heads, $h$, does not explicitly appear in this formulation, as the projections are defined as single transformations on the entire hidden state of dimension $d$.\n    $$P_{\\text{MHSA}} = (d^2 + d) + (d^2 + d) + (d^2 + d) + (d^2 + d) = 4(d^2 + d) = 4d^2 + 4d$$\n\n2.  **Position-wise Feed-Forward Network (FFN) Parameters ($P_{\\text{FFN}}$)**\n    The FFN consists of two learned affine maps with a GELU non-linearity, which has no parameters.\n    *   **First layer:** An affine map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{d_{ff}}$.\n        - Weight matrix $W_1 \\in \\mathbb{R}^{d_{ff} \\times d}$: $d_{ff} \\times d$ parameters.\n        - Bias vector $b_1 \\in \\mathbb{R}^{d_{ff}}$: $d_{ff}$ parameters.\n        - Total for the first layer: $d \\cdot d_{ff} + d_{ff}$.\n    *   **Second layer:** An affine map from $\\mathbb{R}^{d_{ff}}$ to $\\mathbb{R}^{d}$.\n        - Weight matrix $W_2 \\in \\mathbb{R}^{d \\times d_{ff}}$: $d \\times d_{ff}$ parameters.\n        - Bias vector $b_2 \\in \\mathbb{R}^{d}$: $d$ parameters.\n        - Total for the second layer: $d \\cdot d_{ff} + d$.\n    The total number of parameters in the FFN is the sum of these two layers.\n    $$P_{\\text{FFN}} = (d \\cdot d_{ff} + d_{ff}) + (d \\cdot d_{ff} + d) = 2d \\cdot d_{ff} + d_{ff} + d$$\n\n3.  **Layer Normalization (LN) Parameters ($P_{\\text{LN}}$)**\n    The block contains two Layer Normalization modules. Each module has a learnable scale vector $\\gamma \\in \\mathbb{R}^{d}$ and a learnable shift vector $\\beta \\in \\mathbb{R}^{d}$.\n    - Parameters per LN module: $d + d = 2d$.\n    - Total parameters for two LN modules:\n    $$P_{\\text{LN}} = 2 \\times (2d) = 4d$$\n\n**Total Parameters ($P_{\\text{total}}$)**\nThe total number of learnable parameters in the encoder block is the sum of the parameters from all components.\n$$P_{\\text{total}} = P_{\\text{MHSA}} + P_{\\text{FFN}} + P_{\\text{LN}}$$\n$$P_{\\text{total}} = (4d^2 + 4d) + (2d \\cdot d_{ff} + d_{ff} + d) + (4d)$$\n$$P_{\\text{total}} = 4d^2 + (4d + d + 4d) + 2d \\cdot d_{ff} + d_{ff}$$\n$$P_{\\text{total}} = 4d^2 + 9d + d_{ff}(2d + 1)$$\nThis is the closed-form expression for the total parameter count.\n\n**Part 2: Architectural Variant and New Per-Head Dimension**\n\nThe problem proposes a variant where the number of attention heads is changed from $h$ to $h' = \\alpha h$, while the model's hidden size $d$ is kept constant. In the standard Transformer architecture, the total dimensionality across all heads equals the model's hidden size. That is, if $d_k$ is the dimension of each head's query, key, and value vectors, then $h \\cdot d_k = d$.\n\nThe original per-head dimension is $d_k = \\frac{d}{h}$.\nThe new number of heads is $h' = \\alpha h$.\nTo maintain the relationship $h' \\cdot d'_k = d$, where $d'_k$ is the new per-head dimension, we can solve for $d'_k$:\n$$d'_k = \\frac{d}{h'} = \\frac{d}{\\alpha h} = \\frac{1}{\\alpha} \\left(\\frac{d}{h}\\right)$$\nThe new per-head dimension is the original per-head dimension scaled by $1/\\alpha$.\n\n**Part 3: Numerical Evaluation**\n\nWe are given the BERT-Base configuration: $d = 768$, $d_{ff} = 3072$, $h = 12$, and the modification factor $\\alpha = 2$.\n\n*   **Total Parameter Count:**\n    We substitute the given values into the derived formula for $P_{\\text{total}}$.\n    $$P_{\\text{total}} = 4(768)^2 + 9(768) + 3072(2 \\cdot 768 + 1)$$\n    $$P_{\\text{total}} = 4(589824) + 6912 + 3072(1536 + 1)$$\n    $$P_{\\text{total}} = 2359296 + 6912 + 3072(1537)$$\n    $$P_{\\text{total}} = 2359296 + 6912 + 4721664$$\n    $$P_{\\text{total}} = 7087872$$\n\n*   **New Per-Head Dimension:**\n    The original number of heads is $h=12$. The new number of heads is $h' = \\alpha h = 2 \\times 12 = 24$.\n    The hidden size is $d = 768$. The new per-head dimension $d'_k$ is:\n    $$d'_k = \\frac{d}{h'} = \\frac{768}{24}$$\n    To compute this, we can divide by $12$ then by $2$:\n    $$d'_k = \\frac{768/12}{2} = \\frac{64}{2} = 32$$\n    The new per-head dimension is $32$.\n\nThe final answer requires a row matrix containing the total parameter count and the new per-head dimension.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} 7087872 & 32 \\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond model size, a critical aspect of any architecture is its computational cost. This final practice explores the performance profile of multi-head self-attention, guiding you to derive its computational complexity in terms of MAC (Multiply-Accumulate) operations. The key takeaway from this exercise  is the famous quadratic scaling of attention with respect to sequence length, a fundamental property that dictates the practical application of Transformer models and has inspired a vast field of research into more efficient attention mechanisms.",
            "id": "3199246",
            "problem": "A Vision Transformer (ViT) processes an image by dividing it into non-overlapping patches, embedding each patch, and applying Multi-Head Self-Attention (MHSA). Consider a ViT layer that receives an image of spatial size $H \\times W$, uses patch size $p$ (assume $p$ divides both $H$ and $W$), and produces an embedding of dimension $D$ per token. Let the number of tokens be $L = \\frac{H}{p} \\cdot \\frac{W}{p}$, and let MHSA use $M$ heads with per-head dimension $d = \\frac{D}{M}$, where $M$ divides $D$. The layer implements standard scaled dot-product attention: queries, keys, and values are computed by linear projections of the input, attention scores are computed by dot-products between queries and keys, softmax is applied over each query’s score vector, the resulting weights are used to weight values, and the concatenated head outputs are linearly projected back to dimension $D$.\n\nStarting only from the following generally accepted bases:\n- The computational cost of multiplying a dense matrix of size $a \\times b$ by a dense matrix of size $b \\times c$ is $a b c$ fused multiply-accumulate (MAC) operations.\n- A linear layer applying a weight matrix of size $D \\times D$ to $L$ tokens costs $L D D$ MACs.\n- You may neglect the cost of bias additions, softmax, and elementwise scaling compared to matrix multiplications.\n\nDerive the exact total number of MACs for a single MHSA block (excluding layer normalization and the multilayer perceptron), expressed solely in terms of $H$, $W$, $p$, and $D$. Your derivation must account for the query, key, and value projections, the attention score multiplication, the application of attention weights to values, and the final output projection. Then, using inequality reasoning, discuss the regimes in which the $\\mathcal{O}(L^{2} D)$ term becomes the computational bottleneck compared to the $\\mathcal{O}(L D^{2})$ term, with $L = \\frac{H}{p} \\cdot \\frac{W}{p}$.\n\nProvide your final answer as a single closed-form analytic expression in terms of $H$, $W$, $p$, and $D$ giving the total MAC count for the MHSA block. Do not include big-$\\mathcal{O}$ notation in the final answer. No numerical evaluation is required.",
            "solution": "The problem statement is critically evaluated for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- Image spatial size: $H \\times W$\n- Patch size: $p$\n- Constraint: $p$ divides both $H$ and $W$\n- Embedding dimension per token: $D$\n- Number of tokens: $L = \\frac{H}{p} \\cdot \\frac{W}{p}$\n- Number of attention heads: $M$\n- Per-head dimension: $d = \\frac{D}{M}$\n- Constraint: $M$ divides $D$\n- Attention mechanism: Standard scaled dot-product attention\n- Computational cost basis for matrix multiplication of size $a \\times b$ by size $b \\times c$: $a b c$ fused multiply-accumulate (MAC) operations.\n- Computational cost basis for a linear layer with a $D \\times D$ weight matrix applied to $L$ tokens: $L D D$ MACs.\n- Exclusions: The cost of bias additions, softmax, and elementwise scaling is to be neglected.\n- The derivation must account for:\n    1. Query, key, and value projections.\n    2. Attention score multiplication.\n    3. Application of attention weights to values.\n    4. Final output projection.\n- The final expression for the total MACs must be in terms of $H$, $W$, $p$, and $D$.\n- A discussion on the computational regimes of the $\\mathcal{O}(L^{2} D)$ vs. $\\mathcal{O}(L D^{2})$ terms is required.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded:** The problem is firmly rooted in the established architecture of Vision Transformers and uses standard definitions of computational complexity analysis in deep learning. All concepts are standard in the field.\n- **Well-Posed:** The problem provides all necessary variables ($H$, $W$, $p$, $D$), definitions ($L = \\frac{HW}{p^2}$, $d = \\frac{D}{M}$), constraints ($p$ divides $H, W$; $M$ divides $D$), and a clear cost model ($abc$ MACs for matrix multiplication). The objective is to derive a specific quantity, which is uniquely determined by the provided information.\n- **Objective:** The problem is stated in precise, formal language, free of ambiguity or subjective claims.\n\nThe problem exhibits none of the invalidating flaws. It is self-contained, scientifically sound, and formally structured.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full, reasoned solution will be provided.\n\n### Solution Derivation\nThe total computational cost in MACs for a Multi-Head Self-Attention (MHSA) block is the sum of the costs of its constituent matrix multiplication operations. Let the input to the block be a tensor $X$ of size $L \\times D$, where $L$ is the number of tokens (patches) and $D$ is the embedding dimension.\n\n1.  **Query, Key, and Value (QKV) Projections:**\n    The input tensor $X$ is linearly projected to generate the Query ($Q$), Key ($K$), and Value ($V$) tensors. Each projection uses a separate weight matrix of size $D \\times D$.\n    - $Q = X W_Q$\n    - $K = X W_K$\n    - $V = X W_V$\n    The multiplication of the input matrix $X$ (size $L \\times D$) with a weight matrix (size $D \\times D$) costs $L \\times D \\times D = L D^2$ MACs. Since there are three such projections, the total cost for this step is:\n    $$ \\text{Cost}_{\\text{QKV}} = 3 \\times (L D^2) = 3 L D^2 $$\n\n2.  **Attention Score Calculation ($Q K^T$):**\n    For multi-head attention, the $Q$, $K$, and $V$ tensors are split into $M$ heads. The dimension of each head is $d = \\frac{D}{M}$. For each head $i$, we have $Q_i$, $K_i$, and $V_i$, which are matrices of size $L \\times d$.\n    The attention scores for a single head are computed by the matrix product $A_i = Q_i K_i^T$.\n    - $Q_i$ has dimensions $L \\times d$.\n    - $K_i^T$ has dimensions $d \\times L$.\n    The cost of this multiplication is $L \\times d \\times L = L^2 d$ MACs.\n    Since there are $M$ heads, the total cost for computing all attention scores is:\n    $$ \\text{Cost}_{\\text{scores}} = M \\times (L^2 d) = L^2 (M d) $$\n    Given that $D = M d$, this simplifies to:\n    $$ \\text{Cost}_{\\text{scores}} = L^2 D $$\n\n3.  **Application of Attention Weights to Values:**\n    The attention scores (after the neglected softmax operation) for each head, a matrix of size $L \\times L$, are multiplied by the corresponding Value matrix $V_i$ (size $L \\times d$).\n    - The attention matrix for head $i$ has dimensions $L \\times L$.\n    - The value matrix $V_i$ for head $i$ has dimensions $L \\times d$.\n    The cost of this multiplication for a single head is $L \\times L \\times d = L^2 d$ MACs.\n    Again, summing over all $M$ heads, the total cost is:\n    $$ \\text{Cost}_{\\text{values}} = M \\times (L^2 d) = L^2 (M d) $$\n    This simplifies to:\n    $$ \\text{Cost}_{\\text{values}} = L^2 D $$\n\n4.  **Final Output Projection:**\n    The outputs from all $M$ heads, each of size $L \\times d$, are concatenated to form a single tensor of size $L \\times (M d) = L \\times D$. This tensor is then passed through a final linear projection layer with a weight matrix $W_O$ of size $D \\times D$.\n    - The concatenated output tensor has dimensions $L \\times D$.\n    - The output weight matrix $W_O$ has dimensions $D \\times D$.\n    The cost of this final projection is:\n    $$ \\text{Cost}_{\\text{output}} = L \\times D \\times D = L D^2 $$\n\n5.  **Total Computational Cost:**\n    The total number of MACs for the MHSA block is the sum of the costs from the four steps:\n    $$ \\text{Total MACs} = \\text{Cost}_{\\text{QKV}} + \\text{Cost}_{\\text{scores}} + \\text{Cost}_{\\text{values}} + \\text{Cost}_{\\text{output}} $$\n    $$ \\text{Total MACs} = 3 L D^2 + L^2 D + L^2 D + L D^2 $$\n    $$ \\text{Total MACs} = 4 L D^2 + 2 L^2 D $$\n\n6.  **Expression in terms of $H$, $W$, $p$, and $D$:**\n    The problem requires the final answer to be expressed in terms of the initial image and model parameters. We substitute $L = \\frac{H W}{p^2}$ into the total cost expression:\n    $$ \\text{Total MACs} = 4 \\left( \\frac{H W}{p^2} \\right) D^2 + 2 \\left( \\frac{H W}{p^2} \\right)^2 D $$\n    $$ \\text{Total MACs} = \\frac{4 H W D^2}{p^2} + \\frac{2 H^2 W^2 D}{p^4} $$\n    This is the final closed-form analytic expression for the total MAC count.\n\n### Discussion of Computational Regimes\nThe total computational cost consists of two terms with different scaling properties: a term proportional to $L D^2$ (from linear projections) and a term proportional to $L^2 D$ (from the attention mechanism itself). Specifically, we have $4 L D^2$ and $2 L^2 D$.\n\nThe $\\mathcal{O}(L^2 D)$ term, $2 L^2 D$, becomes the computational bottleneck compared to the $\\mathcal{O}(L D^2)$ term, $4 L D^2$, when its magnitude is greater. We can establish this regime by setting up an inequality:\n$$ 2 L^2 D > 4 L D^2 $$\nSince $L$ (the number of tokens) and $D$ (the embedding dimension) are positive integers, we can divide both sides by $2 L D$ without changing the inequality's direction:\n$$ L > 2 D $$\nThis inequality defines the regime where the quadratic complexity of self-attention with respect to the sequence length (number of tokens) dominates the cost of the linear projections.\nSubstituting $L = \\frac{H W}{p^2}$, the condition becomes:\n$$ \\frac{H W}{p^2} > 2 D $$\nThis analysis highlights a fundamental characteristic of Transformer architectures. For a fixed embedding dimension $D$ and patch size $p$, the cost of attention, $2L^2D$, grows quadratically with the number of pixels ($H \\times W$), while the cost of projections, $4LD^2$, grows linearly. Therefore, for applications involving high-resolution images (large $H$ and $W$) or small patch sizes (small $p$), the number of tokens $L$ can become very large, causing the $L > 2D$ condition to be met and making the attention computation the primary performance bottleneck. This quadratic scaling is the main reason why standard ViTs are computationally intensive for high-resolution vision tasks and has motivated the development of more efficient attention mechanisms (e.g., windowed, sparse, or linear attention).",
            "answer": "$$ \\boxed{\\frac{4 H W D^{2}}{p^{2}} + \\frac{2 H^{2} W^{2} D}{p^{4}}} $$"
        }
    ]
}