{
    "hands_on_practices": [
        {
            "introduction": "为了真正掌握一个复杂的概念，最好的方法往往是从最简单的特例入手。这个练习将缩放点积注意力机制简化到了极致，通过考察键维度 $d_k=1$ 的情况，带你回归其核心思想。通过处理标量（而非向量）的查询、键和值，你将直观地理解注意力权重是如何计算的，以及它们如何体现查询与各个键之间的“相似度”关系。",
            "id": "3172468",
            "problem": "一个系统使用通常称为“注意力”的机制，根据一组标量值与一个标量查询的相似度来组合这些值。在键维度等于一的边缘情况下，查询和键之间的相似度简化为它们的标量积。设查询为 $q$，键为 $k_1, k_2, k_3$，值为 $v_1, v_2, v_3$，它们都是实数。假设在通过归一化指数映射转换相似度分数之前，先用键维度的平方根对它们进行常规缩放。从标量的点积定义和归一化指数映射的定义出发，推导出分配给每个值的权重，并使用它们为以下数据计算注意力操作的单一标量输出：\n$q = 2$，$k_1 = 0.1$，$k_2 = 0.2$，$k_3 = 0.5$，$v_1 = 1$，$v_2 = -1$，$v_3 = 3$，键维度 $d_k = 1$。\n将您的最终数值结果四舍五入到四位有效数字，并将其表示为无单位的纯数。",
            "solution": "目标是在键维度等于一的情况下，从第一性原理出发构建注意力输出。我们使用的基本依据是点积的定义和归一化指数映射的定义，后者通常被称为 $\\mathrm{softmax}$ 函数。\n1. 来自标量点积和常规缩放的兼容性分数。当键维度 $d_k = 1$ 时，平方根缩放因子为 $\\sqrt{d_k} = \\sqrt{1} = 1$。对于一个标量查询 $q$ 和标量键 $k_i$，其相似度（兼容性）分数为\n$$\ns_i = \\frac{q \\cdot k_i}{\\sqrt{d_k}} = q\\,k_i.\n$$\n2. 归一化指数映射 ($\\mathrm{softmax}$)。给定一组分数 $(s_1, s_2, s_3)$，通过对分数进行指数化并归一化使其和为一，来获得注意力权重：\n$$\n\\alpha_i = \\frac{\\exp(s_i)}{\\sum_{j=1}^{3} \\exp(s_j)}.\n$$\n3. 作为值的凸组合的注意力输出。注意力输出 $y$ 是这些值的加权和：\n$$\ny = \\sum_{i=1}^{3} \\alpha_i v_i.\n$$\n我们现在将这些步骤应用于给定的数值数据：$q = 2$，$k_1 = 0.1$，$k_2 = 0.2$，$k_3 = 0.5$，$v_1 = 1$，$v_2 = -1$，$v_3 = 3$，以及 $d_k = 1$。\n- 使用 $s_i = q k_i$ 计算分数：\n$$\ns_1 = 2 \\cdot 0.1 = 0.2,\\quad s_2 = 2 \\cdot 0.2 = 0.4,\\quad s_3 = 2 \\cdot 0.5 = 1.0.\n$$\n- 对分数进行指数化：\n$$\n\\exp(s_1) = \\exp(0.2),\\quad \\exp(s_2) = \\exp(0.4),\\quad \\exp(s_3) = \\exp(1.0).\n$$\n在最后的数值计算步骤之前，保持这些表达式为符号形式。归一化常数为\n$$\nZ = \\exp(0.2) + \\exp(0.4) + \\exp(1.0).\n$$\n- 权重为\n$$\n\\alpha_1 = \\frac{\\exp(0.2)}{Z},\\quad \\alpha_2 = \\frac{\\exp(0.4)}{Z},\\quad \\alpha_3 = \\frac{\\exp(1.0)}{Z}.\n$$\n- 输出为\n$$\ny = \\alpha_1 \\cdot 1 + \\alpha_2 \\cdot (-1) + \\alpha_3 \\cdot 3 = \\frac{\\exp(0.2) - \\exp(0.4) + 3\\exp(1.0)}{Z}.\n$$\n将指数的数值计算结果代入，以得出最终的数值：\n$$\n\\exp(0.2) \\approx 1.221402758160170,\\quad \\exp(0.4) \\approx 1.491824697641270,\\quad \\exp(1.0) \\approx 2.718281828459045.\n$$\n那么\n$$\nZ \\approx 1.221402758160170 + 1.491824697641270 + 2.718281828459045 \\approx 5.431509284260485,\n$$\n并且\n$$\n\\text{分子} \\approx 1.221402758160170 - 1.491824697641270 + 3 \\cdot 2.718281828459045 \\approx 7.884423545896035.\n$$\n因此\n$$\ny \\approx \\frac{7.884423545896035}{5.431509284260485} \\approx 1.45160822\\ldots\n$$\n四舍五入到四位有效数字，得到 $1.452$。\n与标量相关模型的关联解释：当 $d_k = 1$ 时，兼容性 $s_i = q k_i$ 只是两个标量的乘积。对于一个正的查询 $q$，更大（更正）的键 $k_i$ 会产生更大的 $s_i$，因此在经过归一化指数映射后会得到更大的权重。这种行为反映了一个简单的标量相关模型，其中对齐性由两个标量信号的乘积捕获，而 $\\mathrm{softmax}$ 提供了一种归一化的、正的加权方式，它强调较高的相关性，同时保持对值的凸组合。",
            "answer": "$$\\boxed{1.452}$$"
        },
        {
            "introduction": "注意力机制是一个两阶段过程：首先计算权重，然后用权重混合值（Value）向量。这个练习聚焦于第二阶段，揭示了即使注意力权重高度集中于某个输入，如果另一个权重很小但对应的值向量范数极大，最终的输出也可能被后者“主导”。这个思想实验强调了控制值向量范数的重要性，并探讨了实现这种鲁棒性的有效方法。",
            "id": "3172404",
            "problem": "Transformer 中的单头注意力模块计算值向量的凸组合。具体来说，给定值向量 $v_1,\\dots,v_n \\in \\mathbb{R}^{d_v}$ 和非负的注意力权重 $a_1,\\dots,a_n$（其和为 1，来自于对查询和键的缩放点积进行 softmax 运算得到），该头的输出是线性混合 $o = \\sum_{j=1}^n a_j v_j$。考虑一种注意力呈尖峰分布（peaky）的情形：$a_1 = 0.99$，$a_2 = 0.01$，且对于所有其他 $j$，$a_j = 0$。进一步假设 $\\|v_1\\|_2 = 1$ 且 $\\|v_2\\|_2 = B$，其中 $B$ 是一个大的正实数。仅使用混合的线性性质和欧几里得范数的基本性质，首先论证即使当 $a_2 \\ll a_1$ 时，$v_2$ 的大小（magnitude）如何能够影响 $\\|o\\|_2$。然后，为了在不考虑尺度 $B$ 的情况下减轻这种值主导（value-dominance）现象，你需要评估几种设计干预措施。\n\n如果存在一个不依赖于任何特定 $\\|v_j\\|_2$ 的常数 $C$，使得对于所有词元（token）$j$，每个词元的贡献大小满足 $a_j \\|v_j\\|_2 \\le C$，我们就说一个干预措施在值混合（value-mixing）阶段强制执行了以下的鲁棒性属性 $\\mathcal{P}$。换句话说，没有任何单个词元可以仅仅通过拥有一个极大的值范数来主导混合结果。\n\n假设使用常规的 Transformer 预归一化（pre-normalization），其中值投影（value projection）的输入经过层归一化（Layer Normalized），因此每个词元嵌入 $x_j$ 都满足 $\\|x_j\\|_2 \\le R$，对于某个固定的 $R > 0$。这里的层归一化（LayerNorm）表示对特征进行逐词元的仿射归一化。\n\n在这些假设下，以下哪些干预措施可被证明能够强制执行 $\\mathcal{P}$？选择所有适用的选项。\n\nA. 约束值投影矩阵 $W_V \\in \\mathbb{R}^{d_v \\times d_{\\text{model}}}$ 的谱范数至多为 $\\sigma_{\\max}$（例如，通过谱范数正则化），并且在 $W_V$ 之后不应用任何逐词元的缩放。\n\nB. 将每个值 $v_j$ 替换为重新缩放后的值 $v_j' = g \\, v_j / (\\|v_j\\|_2 + \\epsilon)$，其中 $g > 0$ 是一个在所有词元间共享的学习到的全局增益，$\\epsilon > 0$ 是一个很小的数，然后使用相同的注意力权重混合 $v_j'$。\n\nC. 通过在常规缩放之外，将注意力对数（logits）除以一个额外的常数 $c > 1$ 来降低 softmax 温度，从而使注意力分布不那么尖峰。\n\nD. 裁剪每个注意力权重，使得对于一个固定的 $\\alpha \\in (0,1)$，有 $a_j \\le \\alpha$，然后在混合前重新归一化权重使其和为 1。\n\nE. 在值混合 $\\sum_{j=1}^n a_j v_j$ 之后、传递到下一层之前，对输出 $o$ 应用带有学习尺度 $\\gamma$ 的层归一化（LayerNorm）。\n\n你的推理应从 $o$ 作为凸组合的定义以及范数的基本性质开始。你应该在尖峰情况下建立一个定量的下界，以显示由大的 $\\|v_j\\|_2$ 引起主导的可能性，然后确定哪些干预措施能保证一个形式为 $a_j \\|v_j\\|_2 \\le C$ 且独立于 $\\|v_j\\|_2$ 的界。",
            "solution": "从注意力头输出作为值向量的凸组合的定义开始：$o = \\sum_{j=1}^n a_j v_j$，其中 $a_j \\ge 0$ 且 $\\sum_{j=1}^n a_j = 1$。考虑尖峰情况：$a_1 = 0.99$，$a_2 = 0.01$，对所有其他 $j$ 有 $a_j = 0$，且 $\\|v_1\\|_2 = 1$ 和 $\\|v_2\\|_2 = B$。\n\n根据反三角不等式，对于任意向量 $u$ 和 $w$，有 $\\|u + w\\|_2 \\ge \\big|\\|u\\|_2 - \\|w\\|_2\\big|$。将此应用于 $u = a_2 v_2$ 和 $w = a_1 v_1$ 可得\n$$\n\\|o\\|_2 \\;=\\; \\|a_1 v_1 + a_2 v_2\\|_2 \\;\\ge\\; \\big| \\|a_2 v_2\\|_2 - \\|a_1 v_1\\|_2 \\big| \\;=\\; \\big| 0.01\\,B - 0.99 \\big|.\n$$\n因此，只要 $B > 99$，下界就变为 $0.01\\,B - 0.99$，它随 $B$ 线性增长，这表明即使是一个很小的注意力权重 $a_2 = 0.01$，当 $B$ 足够大时，也无法阻止 $v_2$ 的大小主导混合输出。在最佳对齐情况下（当 $v_1$ 和 $v_2$ 指向相同方向时），其大小更大：\n$$\n\\|o\\|_2 \\;=\\; \\|0.99\\,v_1 + 0.01\\,v_2\\|_2 \\;=\\; 0.99\\,\\|v_1\\|_2 + 0.01\\,\\|v_2\\|_2 \\;=\\; 0.99 + 0.01\\,B,\n$$\n同样，当 $B \\to \\infty$ 时，它会无界增长。这显示了这样的现象：一个施加在极大范数值上的小权重，可以显著影响输出。\n\n我们现在分析哪些干预措施能强制执行鲁棒性属性 $\\mathcal{P}$：存在一个不依赖于任何特定 $\\|v_j\\|_2$ 的常数 $C$，使得在混合阶段对所有词元 $j$ 都有 $a_j \\|v_j\\|_2 \\le C$。\n\n选项 A：约束 $W_V$ 的谱范数并假设输入有界。\n设 $v_j = W_V x_j$，其中每个 $x_j$ 是经过层归一化的词元嵌入，满足 $\\|x_j\\|_2 \\le R$。谱范数约束 $\\|W_V\\|_2 \\le \\sigma_{\\max}$ 保证了\n$$\n\\|v_j\\|_2 \\;=\\; \\|W_V x_j\\|_2 \\;\\le\\; \\|W_V\\|_2 \\,\\|x_j\\|_2 \\;\\le\\; \\sigma_{\\max} R.\n$$\n因此，对于每个 $j$，\n$$\na_j \\|v_j\\|_2 \\;\\le\\; a_j \\, (\\sigma_{\\max} R) \\;\\le\\; \\sigma_{\\max} R,\n$$\n因为 $a_j \\le 1$。这满足了属性 $\\mathcal{P}$，其中 $C = \\sigma_{\\max} R$ 是一个不依赖于任何特定 $\\|v_j\\|_2$ 的常数。结论：正确。\n\n选项 B：使用共享增益进行逐词元的值 $\\ell_2$-归一化。\n定义 $v_j' = g \\, v_j / (\\|v_j\\|_2 + \\epsilon)$，其中 $g > 0$ 是跨词元共享的增益，$\\epsilon > 0$ 是一个很小的数。那么\n$$\n\\|v_j'\\|_2 \\;=\\; g \\, \\frac{\\|v_j\\|_2}{\\|v_j\\|_2 + \\epsilon} \\;\\le\\; g.\n$$\n混合这些重新缩放的值，得到的每个词元的贡献有界，即\n$$\na_j \\|v_j'\\|_2 \\;\\le\\; a_j \\, g \\;\\le\\; g,\n$$\n因此属性 $\\mathcal{P}$ 成立，其中 $C = g$。这个界与原始的 $\\|v_j\\|_2$ 无关。结论：正确。\n\n选项 C：通过将 logits 除以一个额外的常数 $c > 1$ 来降低 softmax 温度。\n改变温度会修改 $a_j$ 的分布，通常使其不那么尖峰。然而，属性 $\\mathcal{P}$ 要求一个独立于 $\\|v_j\\|_2$ 的界。对于任何 $c$，都可以选择一种配置，使得一个具有大 $\\|v_j\\|_2$ 的词元保留一个不可忽略的权重 $a_j$（回顾 $\\sum_j a_j = 1$），从而得到 $a_j \\|v_j\\|_2$ 随 $\\|v_j\\|_2 \\to \\infty$ 而无界增长。因此，温度调整不会产生一个独立于 $\\|v_j\\|_2$ 的常数 $C$。结论：不正确。\n\n选项 D：裁剪 $a_j \\le \\alpha$ 并重新归一化使其和为 1。\n裁剪确保了 $a_j \\le \\alpha$，但在重新归一化后，权重之和仍然为 1。对于任何固定的 $\\alpha \\in (0,1)$，每个词元的贡献可以大到 $a_j \\|v_j\\|_2 \\le \\alpha \\, \\|v_j\\|_2$，当 $\\|v_j\\|_2 \\to \\infty$ 时，这是无界的。因此，不存在一个满足 $\\mathcal{P}$ 且独立于 $\\|v_j\\|_2$ 的常数 $C$。结论：不正确。\n\n选项 E：在混合后应用带有学习尺度 $\\gamma$ 的层归一化。\n应用于 $o$ 的层归一化（LayerNorm）可以在归一化和缩放后限制输出的大小，但 $\\mathcal{P}$ 明确是关于值混合阶段贡献 $a_j \\|v_j\\|_2$ 的一个约束，这个阶段在任何混合后归一化之前。事后（Post-hoc）的层归一化本身并不限制 $a_j \\|v_j\\|_2$；归一化前的混合仍然可能在大小和方向上被一个大范数的 $v_j$ 主导。因此，$\\mathcal{P}$ 没有被强制执行。结论：不正确。\n\n总而言之，在混合前直接限制值范数的干预措施，无论是通过约束映射到值的算子（选项 A），还是通过将每个值归一化到固定的范数尺度（选项 B），都可以被证明能够强制执行鲁棒性属性 $\\mathcal{P}$。而那些只重塑注意力权重（选项 C 和 D）或在混合后进行归一化（选项 E）的策略，则不能按照定义强制执行 $\\mathcal{P}$。",
            "answer": "$$\\boxed{AB}$$"
        }
    ]
}