{
    "hands_on_practices": [
        {
            "introduction": "To truly understand why some models are designed the way they are, we must analyze their properties from first principles. This practice guides you through a theoretical analysis of additive and multiplicative attention at initialization, using basic probability theory. By deriving the expectation and variance of the attention energies, you will uncover a fundamental scaling issue in multiplicative attention that necessitates the \"scaling\" factor in scaled dot-product attention, a crucial detail for ensuring stable training in models like the Transformer .",
            "id": "3097429",
            "problem": "Consider a sequence-to-sequence model with attention, where the decoder state at time $t$ is a vector $s_t \\in \\mathbb{R}^{d_h}$ and an encoder hidden state for position $i$ is a vector $h_i \\in \\mathbb{R}^{d_h}$. The multiplicative (Luong) energy is defined as $e_i^{\\text{mult}} = s_t^{\\top} W h_i$ for a weight matrix $W \\in \\mathbb{R}^{d_h \\times d_h}$. The additive (Bahdanau) energy is defined as $e_i^{\\text{add}} = v^{\\top} \\tanh(W_s s_t + W_h h_i)$ for a weight vector $v \\in \\mathbb{R}^{d_h}$ and matrices $W_s, W_h \\in \\mathbb{R}^{d_h \\times d_h}$. Assume the following random initialization and distributional assumptions:\n- The entries of $s_t$ are independent and identically distributed (i.i.d.) with $s_{t,a} \\sim \\mathcal{N}(0, \\sigma_s^2)$ for all $a \\in \\{1, \\dots, d_h\\}$.\n- The entries of $h_i$ are i.i.d. with $h_{i,b} \\sim \\mathcal{N}(0, \\sigma_h^2)$ for all $b \\in \\{1, \\dots, d_h\\}$.\n- The entries of $W$ are i.i.d. with $W_{ab} \\sim \\mathcal{N}(0, \\sigma_W^2)$ and are independent of $s_t$ and $h_i$.\n- The entries of $v$ are i.i.d. with $v_k \\sim \\mathcal{N}(0, \\sigma_v^2)$ and are independent of all other variables.\n- The entries of $W_s$ and $W_h$ are i.i.d. with $W_{s,ka} \\sim \\mathcal{N}(0, \\sigma_{W_s}^2)$ and $W_{h,kb} \\sim \\mathcal{N}(0, \\sigma_{W_h}^2)$, independent of $s_t$, $h_i$, and $v$.\n- All random variables mentioned are mutually independent across indices and across parameter groups, and the hyperbolic tangent function $\\tanh(\\cdot)$ is applied elementwise.\nStarting from the fundamental definitions of expectation and variance, properties of independence, and symmetry of zero-mean Gaussian distributions, derive the expectation $\\mathbb{E}[e_i^{\\text{mult}}]$ and the variance $\\operatorname{Var}(e_i^{\\text{mult}})$ of the multiplicative energy. Then, specialize your variance expression to the case of Xavier-normal initialization for $W$, where $\\sigma_W^2 = 1/d_h$. Next, derive the expectation $\\mathbb{E}[e_i^{\\text{add}}]$ of the additive energy under the same independence and symmetry assumptions. Explicitly quantify how the variance of the multiplicative energy depends on the key dimensionality $d_h$ under the Xavier-normal choice. Your final answer must be a single analytic expression collecting the three quantities $\\mathbb{E}[e_i^{\\text{mult}}]$, $\\operatorname{Var}(e_i^{\\text{mult}})$ under $\\sigma_W^2 = 1/d_h$, and $\\mathbb{E}[e_i^{\\text{add}}]$, presented in that order. No numerical evaluation is required.",
            "solution": "The problem statement is first subjected to a rigorous validation procedure.\n\n### Step 1: Extract Givens\n- **Decoder state:** $s_t \\in \\mathbb{R}^{d_h}$\n- **Encoder state:** $h_i \\in \\mathbb{R}^{d_h}$\n- **Multiplicative energy:** $e_i^{\\text{mult}} = s_t^{\\top} W h_i$, where $W \\in \\mathbb{R}^{d_h \\times d_h}$\n- **Additive energy:** $e_i^{\\text{add}} = v^{\\top} \\tanh(W_s s_t + W_h h_i)$, where $v \\in \\mathbb{R}^{d_h}$ and $W_s, W_h \\in \\mathbb{R}^{d_h \\times d_h}$\n- **Distributional assumptions:**\n  - Entries of $s_t$: $s_{t,a} \\sim \\mathcal{N}(0, \\sigma_s^2)$ are independent and identically distributed (i.i.d.).\n  - Entries of $h_i$: $h_{i,b} \\sim \\mathcal{N}(0, \\sigma_h^2)$ are i.i.d.\n  - Entries of $W$: $W_{ab} \\sim \\mathcal{N}(0, \\sigma_W^2)$ are i.i.d.\n  - Entries of $v$: $v_k \\sim \\mathcal{N}(0, \\sigma_v^2)$ are i.i.d.\n  - Entries of $W_s$: $W_{s,ka} \\sim \\mathcal{N}(0, \\sigma_{W_s}^2)$ are i.i.d.\n  - Entries of $W_h$: $W_{h,kb} \\sim \\mathcal{N}(0, \\sigma_{W_h}^2)$ are i.i.d.\n- **Independence:** All specified random variables are mutually independent.\n- **Special condition:** For one part of the analysis, Xavier-normal initialization is assumed for $W$, meaning $\\sigma_W^2 = 1/d_h$.\n- **Objective:** Derive $\\mathbb{E}[e_i^{\\text{mult}}]$, $\\operatorname{Var}(e_i^{\\text{mult}})$ (and its specialized form for Xavier initialization), and $\\mathbb{E}[e_i^{\\text{add}}]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity:\n- **Scientifically Grounded:** The problem is firmly situated within the theoretical analysis of neural networks, specifically attention mechanisms. The definitions for multiplicative (Luong) and additive (Bahdanau) attention are standard. The assumptions of zero-mean Gaussian priors for weights and states are common for analyzing network behavior at initialization. The problem is a standard exercise in probability theory and mathematical statistics applied to a well-established machine learning concept.\n- **Well-Posed:** The problem provides a complete set of definitions, distributional assumptions, and independence criteria necessary to derive the requested quantities (expectation and variance). The objectives are clear and mathematically unambiguous, guaranteeing a unique and meaningful solution.\n- **Objective:** The problem is stated using formal mathematical language, free from any subjective or biased phrasing.\n\nThe problem does not exhibit any of the listed flaws (e.g., scientific unsoundness, incompleteness, ambiguity). All terms are well-defined, and the premises are mutually consistent.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete, reasoned solution will now be provided.\n\n### Solution Derivation\n\n**Part 1: Expectation of Multiplicative Energy $\\mathbb{E}[e_i^{\\text{mult}}]$**\n\nThe multiplicative energy is defined as $e_i^{\\text{mult}} = s_t^{\\top} W h_i$. This can be written in summation form:\n$$e_i^{\\text{mult}} = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} s_{t,a} W_{ab} h_{i,b}$$\nTo find its expectation, we apply the expectation operator and use its linearity property:\n$$\\mathbb{E}[e_i^{\\text{mult}}] = \\mathbb{E}\\left[\\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} s_{t,a} W_{ab} h_{i,b}\\right] = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} \\mathbb{E}[s_{t,a} W_{ab} h_{i,b}]$$\nThe problem states that all random variables $s_{t,a}$, $W_{ab}$, and $h_{i,b}$ are mutually independent. Therefore, the expectation of their product is the product of their expectations:\n$$\\mathbb{E}[s_{t,a} W_{ab} h_{i,b}] = \\mathbb{E}[s_{t,a}] \\mathbb{E}[W_{ab}] \\mathbb{E}[h_{i,b}]$$\nAccording to the given distributional assumptions, all these variables are drawn from zero-mean Gaussian distributions:\n- $\\mathbb{E}[s_{t,a}] = 0$\n- $\\mathbb{E}[W_{ab}] = 0$\n- $\\mathbb{E}[h_{i,b}] = 0$\nConsequently, for every triplet of indices $(a,b)$, we have:\n$$\\mathbb{E}[s_{t,a} W_{ab} h_{i,b}] = 0 \\cdot 0 \\cdot 0 = 0$$\nSubstituting this back into the sum, we find the total expectation:\n$$\\mathbb{E}[e_i^{\\text{mult}}] = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} 0 = 0$$\n\n**Part 2: Variance of Multiplicative Energy $\\operatorname{Var}(e_i^{\\text{mult}}]$**\n\nThe variance of a random variable $X$ is defined as $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. Since we have shown that $\\mathbb{E}[e_i^{\\text{mult}}] = 0$, the variance simplifies to the expectation of the square:\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\mathbb{E}\\left[ (e_i^{\\text{mult}})^2 \\right] = \\mathbb{E}\\left[ \\left(\\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} s_{t,a} W_{ab} h_{i,b}\\right)^2 \\right]$$\nLet us expand the square of the summation:\n$$\\left(\\sum_{a,b} s_{t,a} W_{ab} h_{i,b}\\right)^2 = \\sum_{a,b} \\sum_{c,d} (s_{t,a} W_{ab} h_{i,b}) (s_{t,c} W_{cd} h_{i,d})$$\nwhere the sums over $a, b, c, d$ all run from $1$ to $d_h$. Taking the expectation:\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\sum_{a,b,c,d} \\mathbb{E}[s_{t,a} s_{t,c} W_{ab} W_{cd} h_{i,b} h_{i,d}]$$\nDue to independence, the expectation of the product can be separated. The expectation of any term will be zero if any of its constituent random variables has an exponent of $1$, as all variables are zero-mean. For the expectation to be non-zero, every random variable in the product must be paired with itself. This occurs only when the indices match:\n- $s_{t,a}$ must be paired with $s_{t,c}$, which requires $a=c$.\n- $W_{ab}$ must be paired with $W_{cd}$, which requires $(a,b)=(c,d)$.\n- $h_{i,b}$ must be paired with $h_{i,d}$, which requires $b=d$.\nThe condition $(a,b)=(c,d)$ subsumes the other two. Thus, the cross-terms where $(a,b) \\neq (c,d)$ have an expectation of zero. We only need to consider the terms where $a=c$ and $b=d$:\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} \\mathbb{E}[(s_{t,a} W_{ab} h_{i,b})^2] = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} \\mathbb{E}[s_{t,a}^2 W_{ab}^2 h_{i,b}^2]$$\nBy independence, this becomes:\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} \\mathbb{E}[s_{t,a}^2] \\mathbb{E}[W_{ab}^2] \\mathbb{E}[h_{i,b}^2]$$\nFor any zero-mean random variable $Z$ with variance $\\sigma^2$, we have $\\mathbb{E}[Z^2] = \\operatorname{Var}(Z) + (\\mathbb{E}[Z])^2 = \\sigma^2 + 0^2 = \\sigma^2$. Applying this:\n- $\\mathbb{E}[s_{t,a}^2] = \\sigma_s^2$\n- $\\mathbb{E}[W_{ab}^2] = \\sigma_W^2$\n- $\\mathbb{E}[h_{i,b}^2] = \\sigma_h^2$\nSubstituting these into the sum:\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} (\\sigma_s^2 \\sigma_W^2 \\sigma_h^2)$$\nThe term inside the summation is constant with respect to the indices $a$ and $b$. The sum has $d_h \\times d_h = d_h^2$ identical terms.\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = d_h^2 \\sigma_s^2 \\sigma_W^2 \\sigma_h^2$$\n\n**Part 3: Specialization for Xavier Initialization and Dependence on $d_h$**\n\nThe problem requires specializing this result for the case of Xavier-normal initialization for the matrix $W$, where $\\sigma_W^2 = 1/d_h$. Substituting this into our variance expression:\n$$\\operatorname{Var}(e_i^{\\text{mult}})_{\\text{Xavier}} = d_h^2 \\sigma_s^2 \\left(\\frac{1}{d_h}\\right) \\sigma_h^2 = d_h \\sigma_s^2 \\sigma_h^2$$\nThis result explicitly quantifies the dependence of the energy's variance on the hidden dimension $d_h$. Under Xavier initialization, the variance of the multiplicative attention energy grows linearly with $d_h$. This scaling behavior is a primary motivation for the \"scaled dot-product attention\" mechanism, which divides the energy by $\\sqrt{d_k}$ (where $d_k$ is the key dimension, equivalent to $d_h$ here) to counteract this growth and stabilize gradients during training.\n\n**Part 4: Expectation of Additive Energy $\\mathbb{E}[e_i^{\\text{add}}]$**\n\nThe additive energy is defined as $e_i^{\\text{add}} = v^{\\top} \\tanh(W_s s_t + W_h h_i)$. In summation form:\n$$e_i^{\\text{add}} = \\sum_{k=1}^{d_h} v_k \\tanh((W_s s_t + W_h h_i)_k)$$\nwhere the subscript $k$ denotes the $k$-th element of the resulting vector. Let $z_k = \\tanh((W_s s_t + W_h h_i)_k)$. The expression becomes $e_i^{\\text{add}} = \\sum_{k=1}^{d_h} v_k z_k$.\nTo find the expectation, we again use linearity:\n$$\\mathbb{E}[e_i^{\\text{add}}] = \\mathbb{E}\\left[\\sum_{k=1}^{d_h} v_k z_k\\right] = \\sum_{k=1}^{d_h} \\mathbb{E}[v_k z_k]$$\nThe term $z_k$ is a function of the random variables in $s_t$, $h_i$, $W_s$, and $W_h$. The problem states that the entries of the vector $v$ are independent of all other variables. Therefore, for each $k$, $v_k$ is independent of $z_k$. This allows us to separate the expectation:\n$$\\mathbb{E}[v_k z_k] = \\mathbb{E}[v_k] \\mathbb{E}[z_k]$$\nWe are given that $v_k \\sim \\mathcal{N}(0, \\sigma_v^2)$, which means $\\mathbb{E}[v_k] = 0$.\nTherefore, for each $k$:\n$$\\mathbb{E}[v_k z_k] = 0 \\cdot \\mathbb{E}[z_k] = 0$$\nThis holds regardless of the value of $\\mathbb{E}[z_k]$. The total expectation is:\n$$\\mathbb{E}[e_i^{\\text{add}}] = \\sum_{k=1}^{d_h} 0 = 0$$\nIt is worth noting that $\\mathbb{E}[z_k]$ is also zero. The argument of the $\\tanh$ function, let's call it $u_k = (W_s s_t + W_h h_i)_k$, is a sum of products of independent, zero-mean random variables. Its distribution is therefore symmetric about zero. Since $\\tanh(x)$ is an odd function ($\\tanh(-x) = -\\tanh(x)$), the expectation $\\mathbb{E}[\\tanh(u_k)]$ over a symmetric distribution is necessarily zero. However, the independence of $v$ and its zero mean provide the most direct path to the result.\n\n### Summary of Results\n1.  **Expectation of Multiplicative Energy:** $\\mathbb{E}[e_i^{\\text{mult}}] = 0$\n2.  **Variance of Multiplicative Energy (Xavier Init):** $\\operatorname{Var}(e_i^{\\text{mult}}) = d_h \\sigma_s^2 \\sigma_h^2$\n3.  **Expectation of Additive Energy:** $\\mathbb{E}[e_i^{\\text{add}}] = 0$\nThese three quantities are collected in a row matrix for the final answer.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & d_h \\sigma_s^2 \\sigma_h^2 & 0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from pure theory to a concrete example, this hands-on coding practice explores how the two attention mechanisms respond to the magnitude of the query vector, which can be seen as a proxy for its importance. You will set up a simplified scenario to observe how the attention weights change as the query's magnitude increases. This exercise provides a clear, numerical demonstration of the saturation effect inherent in additive attention due to its bounded nonlinearity, in stark contrast to the unbounded, and potentially more expressive, scaling of multiplicative attention .",
            "id": "3097423",
            "problem": "Consider a sequence-to-sequence model with an attention mechanism over encoder states, focusing on a single scalar dimension to isolate the role of magnitude in the decoder state. Let the encoder hidden states be three scalars $h_{1}$, $h_{2}$, and $h_{3}$, each in $\\mathbb{R}$, and let the decoder state at time $t$ be a scalar $s_{t} \\in \\mathbb{R}$. The magnitude of $s_{t}$ encodes importance: larger $|s_{t}|$ should lead to more selective attention toward inputs aligned with the sign of $s_{t}$. We compare two attention score constructions: additive attention (Bahdanau) and multiplicative attention (Luong).\n\nUse the following foundational definitions and parameterizations:\n\n- The Softmax function with temperature $\\tau$ for a vector of real-valued energies $e \\in \\mathbb{R}^{n}$ is defined by\n$$\n\\alpha_{i} = \\frac{\\exp\\left(\\frac{e_{i}}{\\tau}\\right)}{\\sum_{j=1}^{n} \\exp\\left(\\frac{e_{j}}{\\tau}\\right)}.\n$$\nIn all computations, set $\\tau = 1$.\n\n- Additive attention energy uses a one-hidden-layer feed-forward construction with hyperbolic tangent nonlinearity:\n$$\ne_{i}^{\\mathrm{add}} = v \\cdot \\tanh\\left(w_{h} h_{i} + w_{s} s_{t} + b\\right),\n$$\nwith fixed scalar parameters $v = 1$, $w_{h} = 1$, $w_{s} = 10$, and $b = 0$.\n\n- Multiplicative attention energy uses a bilinear (scaled dot-product) form:\n$$\ne_{i}^{\\mathrm{mul}} = s_{t} \\cdot W \\cdot h_{i},\n$$\nwith fixed scalar parameter $W = 1$.\n\nLet the encoder states be\n$$\nh_{1} = 1, \\quad h_{2} = -1, \\quad h_{3} = 0.\n$$\nInterpret $h_{1}$ as the aligned state for positive $s_{t}$ because it shares sign alignment.\n\nFor each energy construction, convert energies $\\{e_{i}\\}$ to attention weights $\\{\\alpha_{i}\\}$ using the Softmax definition above. The attention weight assigned to $h_{1}$ under a given construction is the corresponding $\\alpha_{1}$.\n\nTest suite specification (each test case specifies a value for $s_{t}$):\n- Case $1$: $s_{t} = 0$ (boundary case with no magnitude).\n- Case $2$: $s_{t} = 0.1$ (small magnitude).\n- Case $3$: $s_{t} = 1$ (moderate magnitude).\n- Case $4$: $s_{t} = 5$ (large magnitude).\n- Case $5$: $s_{t} = 50$ (extreme magnitude).\n\nFor each case, compute:\n- The multiplicative attention weight on $h_{1}$, denoted $w^{\\mathrm{mul}}(s_{t}) = \\alpha_{1}^{\\mathrm{mul}}$.\n- The additive attention weight on $h_{1}$, denoted $w^{\\mathrm{add}}(s_{t}) = \\alpha_{1}^{\\mathrm{add}}$.\n\nThen, evaluate the following boolean properties across the entire test suite:\n- $B_{\\mathrm{mul}}$: $w^{\\mathrm{mul}}(s_{t})$ is nondecreasing as $s_{t}$ increases across the test suite. Formally, for the ordered set $s_{t} \\in \\{0, 0.1, 1, 5, 50\\}$, check $w^{\\mathrm{mul}}(0) \\le w^{\\mathrm{mul}}(0.1) \\le w^{\\mathrm{mul}}(1) \\le w^{\\mathrm{mul}}(5) \\le w^{\\mathrm{mul}}(50)$.\n- $B_{\\mathrm{add}}$: additive attention saturates and loses magnitude information at high $|s_{t}|$, quantified by near-constancy of the weight at large magnitudes. Check that the change between the large and extreme cases is negligible:\n$$\n\\left| w^{\\mathrm{add}}(50) - w^{\\mathrm{add}}(5) \\right| \\le \\epsilon,\n$$\nwith $\\epsilon = 10^{-12}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order:\n- The five values $w^{\\mathrm{mul}}(s_{t})$ for $s_{t} \\in \\{0, 0.1, 1, 5, 50\\}$.\n- The five values $w^{\\mathrm{add}}(s_{t})$ for $s_{t} \\in \\{0, 0.1, 1, 5, 50\\}$.\n- The boolean $B_{\\mathrm{mul}}$.\n- The boolean $B_{\\mathrm{add}}$.\n\nFor example, an output line has the form\n$$\n[\\underbrace{w^{\\mathrm{mul}}(0), w^{\\mathrm{mul}}(0.1), w^{\\mathrm{mul}}(1), w^{\\mathrm{mul}}(5), w^{\\mathrm{mul}}(50)}_{\\text{five multiplicative weights}}, \\underbrace{w^{\\mathrm{add}}(0), w^{\\mathrm{add}}(0.1), w^{\\mathrm{add}}(1), w^{\\mathrm{add}}(5), w^{\\mathrm{add}}(50)}_{\\text{five additive weights}}, B_{\\mathrm{mul}}, B_{\\mathrm{add}} ].\n$$\nAll values are unitless, and no physical units are involved. The answers are floats for the weights and booleans for the final two properties.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of neural attention mechanisms, well-posed with all necessary definitions and parameters provided, and objective in its formulation. The problem asks for a quantitative comparison between additive (Bahdanau-style) and multiplicative (Luong-style) attention, focusing on how each mechanism responds to changes in the magnitude of a decoder state. We will now proceed with a complete, reasoned solution.\n\nThe core of the problem lies in calculating attention weights, which are derived from a set of energy scores using the Softmax function. For a set of energies $\\{e_{1}, e_{2}, \\dots, e_{n}\\}$, the corresponding attention weights $\\{\\alpha_{1}, \\alpha_{2}, \\dots, \\alpha_{n}\\}$ are given by:\n$$\n\\alpha_{i} = \\frac{\\exp\\left(\\frac{e_{i}}{\\tau}\\right)}{\\sum_{j=1}^{n} \\exp\\left(\\frac{e_{j}}{\\tau}\\right)}\n$$\nThe problem specifies a temperature $\\tau = 1$. The encoder hidden states are the scalars $h_{1} = 1$, $h_{2} = -1$, and $h_{3} = 0$. We will compute the attention weight $\\alpha_{1}$ for each specified value of the decoder state $s_{t}$ in the test suite $\\{0, 0.1, 1, 5, 50\\}$.\n\n**1. Multiplicative Attention Analysis**\n\nThe multiplicative attention energy is defined by the bilinear form $e_{i}^{\\mathrm{mul}} = s_{t} \\cdot W \\cdot h_{i}$. With the scalar parameter $W = 1$, this simplifies to:\n$$\ne_{i}^{\\mathrm{mul}} = s_{t} h_{i}\n$$\nFor the given encoder states, the energies are:\n- $e_{1}^{\\mathrm{mul}} = s_{t} \\cdot (1) = s_{t}$\n- $e_{2}^{\\mathrm{mul}} = s_{t} \\cdot (-1) = -s_{t}$\n- $e_{3}^{\\mathrm{mul}} = s_{t} \\cdot (0) = 0$\n\nThe attention weight on the first encoder state, $w^{\\mathrm{mul}}(s_{t}) = \\alpha_{1}^{\\mathrm{mul}}$, is calculated using the Softmax function:\n$$\nw^{\\mathrm{mul}}(s_{t}) = \\alpha_{1}^{\\mathrm{mul}} = \\frac{\\exp(e_{1}^{\\mathrm{mul}})}{\\exp(e_{1}^{\\mathrm{mul}}) + \\exp(e_{2}^{\\mathrm{mul}}) + \\exp(e_{3}^{\\mathrm{mul}})} = \\frac{\\exp(s_{t})}{\\exp(s_{t}) + \\exp(-s_{t}) + \\exp(0)} = \\frac{\\exp(s_{t})}{\\exp(s_{t}) + \\exp(-s_{t}) + 1}\n$$\nWe compute this value for each $s_{t}$ in the test suite:\n- For $s_{t} = 0$: $w^{\\mathrm{mul}}(0) = \\frac{\\exp(0)}{\\exp(0) + \\exp(0) + 1} = \\frac{1}{1 + 1 + 1} = \\frac{1}{3} \\approx 0.3333333333333333$\n- For $s_{t} = 0.1$: $w^{\\mathrm{mul}}(0.1) = \\frac{\\exp(0.1)}{\\exp(0.1) + \\exp(-0.1) + 1} \\approx 0.3670997184200147$\n- For $s_{t} = 1$: $w^{\\mathrm{mul}}(1) = \\frac{\\exp(1)}{\\exp(1) + \\exp(-1) + 1} \\approx 0.6652409557224216$\n- For $s_{t} = 5$: $w^{\\mathrm{mul}}(5) = \\frac{\\exp(5)}{\\exp(5) + \\exp(-5) + 1} \\approx 0.9932620531633535$\n- For $s_{t} = 50$: $w^{\\mathrm{mul}}(50) = \\frac{\\exp(50)}{\\exp(50) + \\exp(-50) + 1} \\approx 0.9999999999999999$\n\n**2. Additive Attention Analysis**\n\nThe additive attention energy is defined by $e_{i}^{\\mathrm{add}} = v \\cdot \\tanh(w_{h} h_{i} + w_{s} s_{t} + b)$. With parameters $v = 1$, $w_{h} = 1$, $w_{s} = 10$, and $b = 0$, this becomes:\n$$\ne_{i}^{\\mathrm{add}} = \\tanh(h_{i} + 10s_{t})\n$$\nThe energies for the given encoder states are:\n- $e_{1}^{\\mathrm{add}} = \\tanh(1 + 10s_{t})$\n- $e_{2}^{\\mathrm{add}} = \\tanh(-1 + 10s_{t})$\n- $e_{3}^{\\mathrm{add}} = \\tanh(0 + 10s_{t}) = \\tanh(10s_{t})$\n\nThe attention weight $w^{\\mathrm{add}}(s_{t}) = \\alpha_{1}^{\\mathrm{add}}$ is:\n$$\nw^{\\mathrm{add}}(s_{t}) = \\alpha_{1}^{\\mathrm{add}} = \\frac{\\exp(\\tanh(1 + 10s_{t}))}{\\exp(\\tanh(1 + 10s_{t})) + \\exp(\\tanh(-1 + 10s_{t})) + \\exp(\\tanh(10s_{t}))}\n$$\nWe compute this for each $s_{t}$:\n- For $s_{t} = 0$: $e_{1} = \\tanh(1)$, $e_{2} = \\tanh(-1)$, $e_{3} = \\tanh(0) = 0$.\n  $w^{\\mathrm{add}}(0) = \\frac{\\exp(\\tanh(1))}{\\exp(\\tanh(1)) + \\exp(-\\tanh(1)) + 1} \\approx 0.5052857441183307$\n- For $s_{t} = 0.1$: $e_{1} = \\tanh(2)$, $e_{2} = \\tanh(0) = 0$, $e_{3} = \\tanh(1)$.\n  $w^{\\mathrm{add}}(0.1) = \\frac{\\exp(\\tanh(2))}{\\exp(\\tanh(2)) + \\exp(0) + \\exp(\\tanh(1))} \\approx 0.44917951253013896$\n- For $s_{t} = 1$: $e_{1} = \\tanh(11)$, $e_{2} = \\tanh(9)$, $e_{3} = \\tanh(10)$.\n  $w^{\\mathrm{add}}(1) = \\frac{\\exp(\\tanh(11))}{\\exp(\\tanh(11)) + \\exp(\\tanh(9)) + \\exp(\\tanh(10))} \\approx 0.3333333333333333$\n- For $s_{t} = 5$: $e_{1} = \\tanh(51)$, $e_{2} = \\tanh(49)$, $e_{3} = \\tanh(50)$.\n  $w^{\\mathrm{add}}(5) \\approx 0.3333333333333333$\n- For $s_{t} = 50$: $e_{1} = \\tanh(501)$, $e_{2} = \\tanh(499)$, $e_{3} = \\tanh(500)$.\n  $w^{\\mathrm{add}}(50) \\approx 0.3333333333333333$\n\nThe behavior for large $s_{t}$ is explained by the saturation of the hyperbolic tangent function. For a large positive argument $x$, $\\tanh(x) \\approx 1$. For $s_{t} = 5$ and $s_{t} = 50$, the arguments to $\\tanh$ are large positive numbers ($49, 50, 51$ and $499, 500, 501$, respectively). Consequently, all three energies $e_{1}^{\\mathrm{add}}, e_{2}^{\\mathrm{add}}, e_{3}^{\\mathrm{add}}$ are extremely close to $1$. When all energies are nearly identical, the Softmax function distributes the probability mass almost uniformly, causing each weight to approach $\\frac{1}{3}$. This demonstrates the loss of sensitivity to input magnitude in additive attention at large scales, a direct result of the saturating nonlinearity.\n\n**3. Boolean Property Evaluation**\n\n- **$B_{\\mathrm{mul}}$**: This property checks if $w^{\\mathrm{mul}}(s_{t})$ is nondecreasing for $s_{t} \\in \\{0, 0.1, 1, 5, 50\\}$. The function $f(x) = \\frac{\\exp(x)}{\\exp(x) + \\exp(-x) + 1}$ has the derivative $f'(x) = \\frac{\\exp(x) + 2}{(\\exp(x) + \\exp(-x) + 1)^2}$, which is strictly positive for all real $x$. Thus, $w^{\\mathrm{mul}}(s_{t})$ is a strictly increasing function of $s_{t}$, which satisfies the nondecreasing condition. The calculated values confirm this: $0.333... \\le 0.367... \\le 0.665... \\le 0.993... \\le 0.999...$. Therefore, $B_{\\mathrm{mul}}$ is true.\n\n- **$B_{\\mathrm{add}}$**: This property checks if $|w^{\\mathrm{add}}(50) - w^{\\mathrm{add}}(5)| \\le \\epsilon$ with $\\epsilon = 10^{-12}$. As explained, for $s_t=5$ and $s_t=50$, all arguments to the $\\tanh$ function are large, causing the energies to saturate to values extremely close to $1$. Minor differences in these arguments (e.g., between $\\tanh(49)$ and $\\tanh(499)$) result in differences in the energies that are exponentially small. The subsequent application of the $\\exp$ function and Softmax normalization results in attention weights for $s_{t}=5$ and $s_{t}=50$ that are practically identical. The numerical calculation confirms their difference is well below the threshold $\\epsilon = 10^{-12}$. Therefore, $B_{\\mathrm{add}}$ is true.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and verifies properties of additive and multiplicative attention\n    for a given set of encoder and decoder states.\n    \"\"\"\n    # Define givens from the problem statement\n    h = np.array([1, -1, 0])\n    s_t_values = [0, 0.1, 1, 5, 50]\n    tau = 1.0\n\n    # Parameters for additive attention\n    v_add = 1.0\n    w_h_add = 1.0\n    w_s_add = 10.0\n    b_add = 0.0\n\n    # Parameter for multiplicative attention\n    W_mul = 1.0\n    \n    epsilon = 1e-12\n\n    # Lists to store results\n    w_mul_results = []\n    w_add_results = []\n\n    def softmax(energies, temperature):\n        \"\"\"Computes softmax probabilities for a vector of energies.\"\"\"\n        # The temperature is given as 1, so division is nominal.\n        # Stabilize by subtracting the max energy.\n        e_stable = energies / temperature\n        e_stable = e_stable - np.max(e_stable)\n        exps = np.exp(e_stable)\n        return exps / np.sum(exps)\n\n    # Loop through each test case for s_t\n    for s_t in s_t_values:\n        # 1. Multiplicative Attention Calculation\n        energies_mul = s_t * W_mul * h\n        alphas_mul = softmax(energies_mul, tau)\n        w_mul = alphas_mul[0]\n        w_mul_results.append(w_mul)\n\n        # 2. Additive Attention Calculation\n        args_add = w_h_add * h + w_s_add * s_t + b_add\n        energies_add = v_add * np.tanh(args_add)\n        alphas_add = softmax(energies_add, tau)\n        w_add = alphas_add[0]\n        w_add_results.append(w_add)\n\n    # 3. Boolean Property Evaluation\n    # B_mul: Check if w_mul is nondecreasing\n    B_mul = all(w_mul_results[i] = w_mul_results[i+1] for i in range(len(w_mul_results) - 1))\n\n    # B_add: Check for saturation at large magnitudes\n    B_add = abs(w_add_results[4] - w_add_results[3]) = epsilon\n    \n    # Combine all results into a single list\n    final_results = w_mul_results + w_add_results + [B_mul, B_add]\n\n    # Format the output as specified\n    formatted_results = []\n    for r in final_results:\n        if isinstance(r, bool):\n            formatted_results.append(str(r))\n        else:\n            # Format floats to a high precision to show stability\n            formatted_results.append(f\"{r:.16f}\")\n            \n    print(f\"[{','.join(formatted_results)}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "While multiplicative attention is computationally efficient, its simpler bilinear structure has inherent limitations in expressive power. This final practice challenges you to implement and test both mechanisms on carefully constructed sequences containing repeated or subtly different tokens. By analyzing how each model resolves ambiguity, you will discover scenarios where the feed-forward neural network structure of additive attention provides a decisive advantage, highlighting the practical trade-offs between computational complexity and the ability to learn more intricate alignment patterns .",
            "id": "3097330",
            "problem": "You are given a sequence-to-sequence attention alignment task formulated in purely mathematical terms. Consider an encoder producing a finite sequence of hidden state vectors and a decoder producing a query vector. An attention mechanism maps a query vector and each encoder hidden state vector to a scalar score, and then normalizes these scores with the softmax function to obtain attention weights over the sequence positions. Two different attention scoring mechanisms must be implemented: a one-hidden-layer feedforward scoring function with a pointwise hyperbolic tangent and a bilinear scoring function. Your goal is to construct a dataset with frequent token repetitions, compute both attention weight distributions, and determine whether the mechanism correctly resolves which position is the intended alignment target. You must also determine whether the presence of nonlinearity in the additive scoring improves disambiguation according to a well-defined criterion.\n\nFundamental base and definitions:\n- Let the encoder produce a sequence of $n$ hidden state vectors $\\mathbf{h}_1, \\dots, \\mathbf{h}_n \\in \\mathbb{R}^d$ and the decoder produce a query (current state) vector $\\mathbf{s} \\in \\mathbb{R}^d$. An attention scoring function maps $(\\mathbf{s}, \\mathbf{h}_i)$ to a scalar score $a_i$. The normalized attention weights are computed with the softmax function: for scores $a_1, \\dots, a_n$, the attention weight at position $i$ is $\\alpha_i = \\exp(a_i) \\big/ \\sum_{j=1}^n \\exp(a_j)$.\n- The additive scoring must be implemented as a one-hidden-layer feedforward map using a linear transformation of $\\mathbf{h}_i$ and a linear transformation of $\\mathbf{s}$, summed with a bias, passed through the hyperbolic tangent nonlinearity, and finally linearly combined by a readout vector to produce a scalar. Concretely, this requires parameter matrices and vectors with compatible dimensions, and the hyperbolic tangent acts componentwise.\n- The multiplicative scoring must be implemented as a bilinear form between $\\mathbf{s}$ and $\\mathbf{h}_i$ using a matrix of compatible dimension.\n\nDataset construction and parameters:\n- Use dimension $d = 4$ for all vectors. Use a hidden layer width $p = 3$ for the additive scoring mechanism.\n- Define two prototype token vectors with frequent repetition: $\\mathbf{v}_A = \\left[1, 0, 0, 0\\right]$ and $\\mathbf{v}_B = \\left[0, 1, 0, 0\\right]$. Define a small perturbation vector $\\boldsymbol{\\delta} = \\left[0, 0.2, 0, 0\\right]$.\n- For the additive mechanism, fix the parameter matrices and vectors as follows:\n  - Let $\\mathbf{M} \\in \\mathbb{R}^{p \\times d}$ be $\\mathbf{M} = \\begin{bmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  1  0  0 \\end{bmatrix}$.\n  - Let the bias vector $\\mathbf{b} \\in \\mathbb{R}^p$ be $\\mathbf{b} = \\left[0, 0, 0\\right]$.\n  - Let the readout vector $\\mathbf{v} \\in \\mathbb{R}^p$ be $\\mathbf{v} = \\left[0.0, 3.0, 3.0\\right]$.\n- For the multiplicative mechanism, the bilinear matrix $\\mathbf{W} \\in \\mathbb{R}^{d \\times d}$ will be specified per test case.\n\nAttention weight computation:\n- For each position $i \\in \\{1, \\dots, n\\}$, compute the scalar scores for both mechanisms:\n  - The additive score is obtained by first forming the hidden layer pre-activation $\\mathbf{z}_i = \\mathbf{M}\\mathbf{h}_i + \\mathbf{M}\\mathbf{s} + \\mathbf{b} \\in \\mathbb{R}^p$, then applying the hyperbolic tangent componentwise to get $\\tanh(\\mathbf{z}_i)$, and finally taking the dot product with $\\mathbf{v}$ to get $a^{\\text{add}}_i$.\n  - The multiplicative score is the bilinear form $a^{\\text{mult}}_i = \\mathbf{s}^\\top \\mathbf{W}\\mathbf{h}_i$.\n- Normalize each set of scores $\\{a^{\\text{add}}_i\\}$ and $\\{a^{\\text{mult}}_i\\}$ with the softmax to obtain attention weights $\\{\\alpha^{\\text{add}}_i\\}$ and $\\{\\alpha^{\\text{mult}}_i\\}$, respectively.\n\nResolution and improvement criteria:\n- A case is said to be resolved for a mechanism if the attention weight at the designated correct index $i^\\star$ is strictly greater than the attention weights at all other positions. Formally, mechanism resolves if $\\alpha_{i^\\star}  \\max_{j \\neq i^\\star} \\alpha_j$.\n- Define the confidence margin for a mechanism as $\\Delta = \\alpha_{i^\\star} - \\max_{j \\neq i^\\star} \\alpha_j$.\n- The presence of nonlinearity in the additive mechanism is deemed to improve disambiguation if either:\n  1. The additive mechanism resolves the case while the multiplicative mechanism does not, or\n  2. Both mechanisms resolve the case but the additive confidence margin is strictly larger than the multiplicative confidence margin.\n\nTest suite:\nImplement the following four test cases to cover a general case, boundary conditions, and edge conditions. For each, construct $\\{\\mathbf{h}_i\\}$, $\\mathbf{s}$, specify $\\mathbf{W}$, and give $i^\\star$.\n1. Case $1$ (general, both mechanisms expected to resolve): $n = 7$. Encoder sequence: positions $i \\in \\{1, \\dots, 7\\}$ with $\\mathbf{h}_i = \\mathbf{v}_A$ except position $i^\\star = 4$ where $\\mathbf{h}_{4} = \\mathbf{v}_B$. Query vector $\\mathbf{s} = \\mathbf{v}_B$. Multiplicative matrix $\\mathbf{W} = \\mathbf{I}_4$ (the $4 \\times 4$ identity).\n2. Case $2$ (boundary, multiplicative degeneracy): $n = 5$. Encoder sequence: $\\mathbf{h}_i = \\mathbf{v}_A$ for all $i \\neq i^\\star$ and $\\mathbf{h}_{3} = \\mathbf{v}_B$ with $i^\\star = 3$. Query vector $\\mathbf{s} = \\mathbf{v}_B$. Multiplicative matrix $\\mathbf{W} = \\mathbf{0}_{4 \\times 4}$ (the $4 \\times 4$ zero matrix).\n3. Case $3$ (edge, all tokens identical): $n = 6$. Encoder sequence: $\\mathbf{h}_i = \\mathbf{v}_A$ for all $i$. Query vector $\\mathbf{s} = \\mathbf{v}_A$. Designated correct index $i^\\star = 5$. Multiplicative matrix $\\mathbf{W} = \\mathbf{I}_4$.\n4. Case $4$ (near-duplicate with a small perturbation, multiplicative insensitive to a masked dimension): $n = 8$. Encoder sequence: $\\mathbf{h}_i = \\mathbf{v}_A$ for all $i \\neq i^\\star$ and $\\mathbf{h}_{6} = \\mathbf{v}_A + \\boldsymbol{\\delta}$ with $i^\\star = 6$. Query vector $\\mathbf{s} = \\mathbf{v}_A + \\boldsymbol{\\delta}$. Multiplicative matrix $\\mathbf{W} = \\operatorname{diag}\\left(1, 0, 1, 1\\right)$.\n\nProgram requirements:\n- Implement functions to compute the additive and multiplicative scores and attention weights according to the definitions above, using the specified parameters.\n- For each test case, compute:\n  - A boolean indicating whether the additive mechanism resolves the correct index.\n  - A boolean indicating whether the multiplicative mechanism resolves the correct index.\n  - A boolean indicating whether the additive mechanism improves disambiguation per the improvement criterion.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test caseâ€™s result must itself be a list of three booleans in the order described above. For example, the output should look like $\\left[\\left[\\text{True},\\text{False},\\text{True}\\right],\\dots\\right]$ with no extra whitespace-dependent constraints beyond being valid Python boolean printing.",
            "solution": "The user-provided problem is assessed as **valid**. It is a well-posed, scientifically grounded, and objective problem in the domain of deep learning, specifically concerning attention mechanisms. All parameters, definitions, and criteria are provided, allowing for a unique and verifiable solution.\n\n### 1. Mathematical Formulation\n\nThe problem requires the implementation and comparison of two attention mechanisms: additive (Bahdanau-style) and multiplicative (Luong-style).\n\nLet the sequence of $n$ encoder hidden states be $\\{\\mathbf{h}_i\\}_{i=1}^n$, where each $\\mathbf{h}_i \\in \\mathbb{R}^d$. Let the decoder query vector be $\\mathbf{s} \\in \\mathbb{R}^d$. The attention score for the $i$-th position is a scalar $a_i$, and the corresponding attention weight $\\alpha_i$ is computed via the softmax function:\n$$ \\alpha_i = \\frac{\\exp(a_i)}{\\sum_{j=1}^n \\exp(a_j)} $$\n\n**Additive Attention Score:**\nThe additive score is defined by a single-hidden-layer feedforward network. Given parameter matrices $\\mathbf{M} \\in \\mathbb{R}^{p \\times d}$, a bias vector $\\mathbf{b} \\in \\mathbb{R}^p$, and a readout vector $\\mathbf{v} \\in \\mathbb{R}^p$, the score $a^{\\text{add}}_i$ is:\n$$ a^{\\text{add}}_i = \\mathbf{v}^\\top \\tanh(\\mathbf{M}\\mathbf{h}_i + \\mathbf{M}\\mathbf{s} + \\mathbf{b}) $$\nwhere $\\tanh$ is applied element-wise.\n\n**Multiplicative Attention Score:**\nThe multiplicative score is defined by a bilinear form. Given a parameter matrix $\\mathbf{W} \\in \\mathbb{R}^{d \\times d}$, the score $a^{\\text{mult}}_i$ is:\n$$ a^{\\text{mult}}_i = \\mathbf{s}^\\top \\mathbf{W} \\mathbf{h}_i $$\n\n### 2. Parameter Specification and Simplification\n\nThe problem specifies the following parameters:\n- Vector dimension: $d = 4$\n- Additive hidden layer dimension: $p = 3$\n- Prototype vectors: $\\mathbf{v}_A = [1, 0, 0, 0]^\\top$ and $\\mathbf{v}_B = [0, 1, 0, 0]^\\top$\n- Perturbation vector: $\\boldsymbol{\\delta} = [0, 0.2, 0, 0]^\\top$\n- Additive mechanism parameters:\n  - $\\mathbf{M} = \\begin{bmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  1  0  0 \\end{bmatrix}$\n  - $\\mathbf{b} = [0, 0, 0]^\\top$\n  - $\\mathbf{v} = [0, 3, 3]^\\top$\n\nA significant simplification arises from these specific parameters for the additive score. For any vector $\\mathbf{x} = [x_1, x_2, x_3, x_4]^\\top \\in \\mathbb{R}^4$, the product $\\mathbf{M}\\mathbf{x}$ is:\n$$ \\mathbf{M}\\mathbf{x} = \\begin{bmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  1  0  0 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_2 \\end{bmatrix} $$\nThe pre-activation for the additive score is $\\mathbf{z}_i = \\mathbf{M}(\\mathbf{h}_i + \\mathbf{s}) + \\mathbf{b}$. Let $\\mathbf{u}_i = \\mathbf{h}_i + \\mathbf{s} = [u_{i1}, u_{i2}, u_{i3}, u_{i4}]^\\top$. Since $\\mathbf{b} = \\mathbf{0}$, we have $\\mathbf{z}_i = [u_{i1}, u_{i2}, u_{i2}]^\\top$.\nThe final score is:\n$$ a^{\\text{add}}_i = \\mathbf{v}^\\top \\tanh(\\mathbf{z}_i) = \\begin{bmatrix} 0 \\\\ 3 \\\\ 3 \\end{bmatrix}^\\top \\begin{bmatrix} \\tanh(u_{i1}) \\\\ \\tanh(u_{i2}) \\\\ \\tanh(u_{i2}) \\end{bmatrix} = 6 \\tanh(u_{i2}) $$\nThus, the additive score simplifies to $a^{\\text{add}}_i = 6 \\tanh(h_{i2} + s_2)$, where $h_{i2}$ and $s_2$ are the second components of $\\mathbf{h}_i$ and $\\mathbf{s}$, respectively.\n\n### 3. Evaluation Criteria\n- **Resolution**: A mechanism resolves if the attention weight at the target index, $\\alpha_{i^\\star}$, is strictly greater than the maximum attention weight at any other index: $\\alpha_{i^\\star}  \\max_{j \\neq i^\\star} \\alpha_j$.\n- **Confidence Margin**: The margin is $\\Delta = \\alpha_{i^\\star} - \\max_{j \\neq i^\\star} \\alpha_j$.\n- **Improvement**: The additive mechanism improves disambiguation if either (1) it resolves while the multiplicative one does not, or (2) both resolve and $\\Delta^{\\text{add}}  \\Delta^{\\text{mult}}$.\n\n### 4. Case-by-Case Analysis\n\n**Case 1:**\n- Parameters: $n=7$, $i^\\star=4$, $\\mathbf{h}_{i \\ne 4} = \\mathbf{v}_A$, $\\mathbf{h}_4 = \\mathbf{v}_B$, $\\mathbf{s} = \\mathbf{v}_B$, $\\mathbf{W} = \\mathbf{I}_4$.\n- **Multiplicative:** $a^{\\text{mult}}_i = \\mathbf{s}^\\top \\mathbf{h}_i$.\n    - For $i \\ne 4$: $a^{\\text{mult}}_i = \\mathbf{v}_B^\\top \\mathbf{v}_A = 0$.\n    - For $i = 4$: $a^{\\text{mult}}_4 = \\mathbf{v}_B^\\top \\mathbf{v}_B = 1$.\n    - Scores: $\\{0,0,0,1,0,0,0\\}$. Since $1  0$, the maximum score is at $i=4$. Thus, by the monotonicity of softmax, it resolves. **Resolution: True**.\n    - Margin: $\\Delta^{\\text{mult}} = \\frac{e^1}{6e^0+e^1} - \\frac{e^0}{6e^0+e^1} = \\frac{e-1}{6+e} \\approx 0.197$.\n- **Additive:** $a^{\\text{add}}_i = 6 \\tanh(h_{i2} + s_2)$. $s_2 = (\\mathbf{v}_B)_2 = 1$.\n    - For $i \\ne 4$: $h_{i2} = (\\mathbf{v}_A)_2 = 0$. $a^{\\text{add}}_i = 6\\tanh(0+1) = 6\\tanh(1) \\approx 4.570$.\n    - For $i = 4$: $h_{4,2} = (\\mathbf{v}_B)_2 = 1$. $a^{\\text{add}}_4 = 6\\tanh(1+1) = 6\\tanh(2) \\approx 5.784$.\n    - Since $6\\tanh(2)  6\\tanh(1)$, it resolves. **Resolution: True**.\n    - Margin: $\\Delta^{\\text{add}} = \\frac{e^{6\\tanh(2)}}{6e^{6\\tanh(1)}+e^{6\\tanh(2)}} - \\frac{e^{6\\tanh(1)}}{6e^{6\\tanh(1)}+e^{6\\tanh(2)}} \\approx 0.253$.\n- **Improvement:** Both resolve, and $\\Delta^{\\text{add}} \\approx 0.253  \\Delta^{\\text{mult}} \\approx 0.197$. **Improvement: True**.\n- Result: `[True, True, True]`\n\n**Case 2:**\n- Parameters: $n=5$, $i^\\star=3$, $\\mathbf{h}_{i \\ne 3} = \\mathbf{v}_A$, $\\mathbf{h}_3 = \\mathbf{v}_B$, $\\mathbf{s} = \\mathbf{v}_B$, $\\mathbf{W} = \\mathbf{0}_{4 \\times 4}$.\n- **Multiplicative:** $a^{\\text{mult}}_i = \\mathbf{s}^\\top \\mathbf{0}_{4 \\times 4} \\mathbf{h}_i = 0$ for all $i$.\n    - All scores are equal. The attention weights will be uniform, $\\alpha_i = 1/5$. Thus, $\\alpha_3 = \\max_{j \\neq 3} \\alpha_j$, so it does not resolve. **Resolution: False**.\n- **Additive:** $a^{\\text{add}}_i = 6 \\tanh(h_{i2} + s_2)$. $s_2 = (\\mathbf{v}_B)_2 = 1$.\n    - For $i \\ne 3$: $h_{i2} = (\\mathbf{v}_A)_2 = 0$. $a^{\\text{add}}_i = 6\\tanh(1)$.\n    - For $i = 3$: $h_{3,2} = (\\mathbf{v}_B)_2 = 1$. $a^{\\text{add}}_3 = 6\\tanh(2)$.\n    - Since $6\\tanh(2)  6\\tanh(1)$, it resolves. **Resolution: True**.\n- **Improvement:** Additive resolves and multiplicative does not. **Improvement: True**.\n- Result: `[True, False, True]`\n\n**Case 3:**\n- Parameters: $n=6$, $i^\\star=5$, $\\mathbf{h}_i = \\mathbf{v}_A$ for all $i$, $\\mathbf{s} = \\mathbf{v}_A$, $\\mathbf{W} = \\mathbf{I}_4$.\n- **Multiplicative:** $a^{\\text{mult}}_i = \\mathbf{s}^\\top \\mathbf{h}_i = \\mathbf{v}_A^\\top \\mathbf{v}_A = 1$ for all $i$.\n    - All scores are equal. It does not resolve. **Resolution: False**.\n- **Additive:** $a^{\\text{add}}_i = 6 \\tanh(h_{i2} + s_2)$. $s_2 = (\\mathbf{v}_A)_2 = 0$.\n    - For all $i$: $h_{i2} = (\\mathbf{v}_A)_2 = 0$. $a^{\\text{add}}_i = 6\\tanh(0+0) = 0$.\n    - All scores are equal. It does not resolve. **Resolution: False**.\n- **Improvement:** Additive does not resolve. **Improvement: False**.\n- Result: `[False, False, False]`\n\n**Case 4:**\n- Parameters: $n=8$, $i^\\star=6$, $\\mathbf{h}_{i \\ne 6}=\\mathbf{v}_A$, $\\mathbf{h}_6=\\mathbf{v}_A+\\boldsymbol{\\delta}$, $\\mathbf{s}=\\mathbf{v}_A+\\boldsymbol{\\delta}$, $\\mathbf{W}=\\operatorname{diag}(1,0,1,1)$.\n- Note: $\\mathbf{v}_A+\\boldsymbol{\\delta} = [1, 0.2, 0, 0]^\\top$. So $\\mathbf{s}=[1, 0.2, 0, 0]^\\top$.\n- **Multiplicative:** $a^{\\text{mult}}_i = \\mathbf{s}^\\top \\mathbf{W} \\mathbf{h}_i$.\n    - For $i \\ne 6$: $\\mathbf{h}_i = \\mathbf{v}_A = [1,0,0,0]^\\top$. $\\mathbf{W}\\mathbf{h}_i = [1,0,0,0]^\\top$. $a^{\\text{mult}}_i = [1,0.2,0,0] \\cdot [1,0,0,0]^\\top = 1$.\n    - For $i = 6$: $\\mathbf{h}_6 = [1,0.2,0,0]^\\top$. $\\mathbf{W}\\mathbf{h}_6 = [1,0,0,0]^\\top$. $a^{\\text{mult}}_6 = [1,0.2,0,0] \\cdot [1,0,0,0]^\\top = 1$.\n    - The perturbation is in the second dimension, which is zeroed out by $\\mathbf{W}$. All scores are equal. It does not resolve. **Resolution: False**.\n- **Additive:** $a^{\\text{add}}_i = 6 \\tanh(h_{i2} + s_2)$. $s_2 = (\\mathbf{v}_A+\\boldsymbol{\\delta})_2 = 0.2$.\n    - For $i \\ne 6$: $h_{i2} = (\\mathbf{v}_A)_2 = 0$. $a^{\\text{add}}_i = 6\\tanh(0+0.2) = 6\\tanh(0.2)$.\n    - For $i = 6$: $h_{6,2} = (\\mathbf{v}_A+\\boldsymbol{\\delta})_2 = 0.2$. $a^{\\text{add}}_6 = 6\\tanh(0.2+0.2) = 6\\tanh(0.4)$.\n    - Since $\\tanh(x)$ is strictly increasing for $x0$, $\\tanh(0.4)  \\tanh(0.2)$, so $a^{\\text{add}}_6  a^{\\text{add}}_{i \\neq 6}$. It resolves. **Resolution: True**.\n- **Improvement:** Additive resolves and multiplicative does not. **Improvement: True**.\n- Result: `[True, False, True]`",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the attention mechanism comparison problem.\n    \"\"\"\n    \n    # Define global parameters\n    d = 4\n    p = 3\n    v_A = np.array([1.0, 0.0, 0.0, 0.0])\n    v_B = np.array([0.0, 1.0, 0.0, 0.0])\n    delta = np.array([0.0, 0.2, 0.0, 0.0])\n\n    M = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0]], dtype=float)\n    b = np.array([0, 0, 0], dtype=float)\n    v = np.array([0.0, 3.0, 3.0])\n\n    def compute_additive_scores(h_states, s):\n        scores = []\n        for h_i in h_states:\n            # As derived in the solution, a_i = 6 * tanh(h_{i2} + s_2)\n            # This is significantly more efficient than full matrix multiplication.\n            u_i2 = h_i[1] + s[1]\n            score = 6 * np.tanh(u_i2)\n            scores.append(score)\n        return np.array(scores)\n\n    def compute_multiplicative_scores(h_states, s, W):\n        scores = []\n        for h_i in h_states:\n            score = s.T @ W @ h_i\n            scores.append(score)\n        return np.array(scores)\n    \n    def softmax(scores):\n        # Numerically stable softmax\n        if scores.size == 0:\n            return np.array([])\n        scores_stable = scores - np.max(scores)\n        exps = np.exp(scores_stable)\n        return exps / np.sum(exps)\n\n    def analyze_attention(alphas, i_star_0based):\n        n = len(alphas)\n        if n == 0:\n            return False, 0.0\n\n        alpha_star = alphas[i_star_0based]\n        \n        other_indices = [j for j in range(n) if j != i_star_0based]\n        if not other_indices:\n            # Case where n=1, resolution is trivially true.\n            return True, alpha_star \n            \n        max_other_alpha = np.max(alphas[other_indices])\n        \n        resolves = alpha_star > max_other_alpha\n        margin = alpha_star - max_other_alpha\n        \n        return resolves, margin\n\n    # Define test cases\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"n\": 7,\n            \"h_seq_builder\": lambda n, i_star_0: [v_B if i == i_star_0 else v_A for i in range(n)],\n            \"s\": v_B,\n            \"i_star\": 4, # 1-based index\n            \"W\": np.identity(d)\n        },\n        {\n            \"name\": \"Case 2\",\n            \"n\": 5,\n            \"h_seq_builder\": lambda n, i_star_0: [v_B if i == i_star_0 else v_A for i in range(n)],\n            \"s\": v_B,\n            \"i_star\": 3,\n            \"W\": np.zeros((d, d))\n        },\n        {\n            \"name\": \"Case 3\",\n            \"n\": 6,\n            \"h_seq_builder\": lambda n, i_star_0: [v_A for _ in range(n)],\n            \"s\": v_A,\n            \"i_star\": 5,\n            \"W\": np.identity(d)\n        },\n        {\n            \"name\": \"Case 4\",\n            \"n\": 8,\n            \"h_seq_builder\": lambda n, i_star_0: [v_A + delta if i == i_star_0 else v_A for i in range(n)],\n            \"s\": v_A + delta,\n            \"i_star\": 6,\n            \"W\": np.diag([1.0, 0.0, 1.0, 1.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case[\"n\"]\n        i_star_0based = case[\"i_star\"] - 1\n        h_states = case[\"h_seq_builder\"](n, i_star_0based)\n        s = case[\"s\"]\n        W = case[\"W\"]\n        \n        # Additive mechanism\n        scores_add = compute_additive_scores(h_states, s)\n        alphas_add = softmax(scores_add)\n        add_resolves, add_margin = analyze_attention(alphas_add, i_star_0based)\n        \n        # Multiplicative mechanism\n        scores_mult = compute_multiplicative_scores(h_states, s, W)\n        alphas_mult = softmax(scores_mult)\n        mult_resolves, mult_margin = analyze_attention(alphas_mult, i_star_0based)\n        \n        # Improvement criterion\n        improves = False\n        if add_resolves and not mult_resolves:\n            improves = True\n        elif add_resolves and mult_resolves and add_margin > mult_margin:\n            improves = True\n            \n        results.append([add_resolves, mult_resolves, improves])\n\n    # Format the final output string precisely as requested\n    case_results_str = []\n    for res_list in results:\n        # e.g., \"[True,False,True]\"\n        case_str = f\"[{','.join(str(b) for b in res_list)}]\"\n        case_results_str.append(case_str)\n    \n    final_output = f\"[{','.join(case_results_str)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}