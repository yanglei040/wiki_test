## 引言
在[强化学习](@entry_id:141144)领域，智能体通过与环境的交互来学习如何做出最优决策以最大化累积奖励。在众多方法中，基于[价值函数](@entry_id:144750)的方法，尤其是[Q学习](@entry_id:144980)，构成了理解和构建智能系统的基石。然而，从掌握[Q学习](@entry_id:144980)的基本方程到能将其成功应用于复杂的现实世界问题之间，存在着一条充满挑战的鸿沟。许多学习者在面对巨大的[状态空间](@entry_id:177074)、训练不稳定以及奖励稀疏等实际障碍时感到困惑。

本文旨在弥合这一理论与实践的差距。我们将系统性地从[Q学习](@entry_id:144980)的核心原理出发，逐步深入其在现代[深度强化学习](@entry_id:638049)中的高级变体和实际应用。通过本文的学习，您将不仅理解[Q学习](@entry_id:144980)“是什么”和“为什么”，更将掌握“如何”应对其固有挑战并将其扩展到新的问题领域。

文章将分为三个核心部分展开。在**“原理与机制”**一章中，我们将建立坚实的理论基础，从动作[价值函数](@entry_id:144750)和贝尔曼最优性方程讲起，剖析[Q学习](@entry_id:144980)的更新规则，并系统地探讨其面临的基本挑战及前沿解决方案。接着，在**“应用与跨学科连接”**一章中，我们将视野扩展到现实世界，展示[Q学习](@entry_id:144980)如何被改造以解决金融、机器人学和自然语言处理等领域中的大规模和复杂问题。最后，在**“动手实践”**部分，您将有机会通过具体的编程练习，亲手实现和观察[Q学习](@entry_id:144980)中的关键概念，如[状态表示](@entry_id:141201)的重要性、函数近似带来的不稳定性以及优先[经验回放](@entry_id:634839)的有效性，从而将理论知识内化为实践技能。

## 原理与机制

在理解了[强化学习](@entry_id:141144)的基本框架之后，本章将深入探讨基于价值的[强化学习](@entry_id:141144)方法的核心原理与机制。我们将从动作价值函数（action-value function）的基本概念出发，介绍其所遵循的贝尔曼最优性方程，并引出一种核心的求解算法——[Q学习](@entry_id:144980)。随后，我们将系统性地剖析[Q学习](@entry_id:144980)在实践中面临的一系列根本性挑战，例如[探索与利用](@entry_id:174107)的权衡、函数近似带来的风险、以及训练过程中的不稳定性。对于每一个挑战，我们不仅将阐明其背后的理论根源，还将介绍学术界与工业界提出的前沿解决方案，如乐观初始化、双[Q学习](@entry_id:144980)、[分布](@entry_id:182848)式强化学习、[目标网络](@entry_id:635025)以及保守策略迭代等。本章旨在为您构建一个关于[Q学习](@entry_id:144980)及其现代变体的坚实理论基础。

### 动作[价值函数](@entry_id:144750)与贝尔曼最优性方程

在强化学习中，我们的目标是找到一个[最优策略](@entry_id:138495) $\pi^*$，它能够在任何状态下最大化累积[折扣](@entry_id:139170)奖励的[期望值](@entry_id:153208)。为了评估在特定状态 $s$ 下执行某个动作 $a$ 的好坏，我们定义了**动作[价值函数](@entry_id:144750) (action-value function)** $Q^\pi(s, a)$。它表示在状态 $s$ 执行动作 $a$，然后继续遵循策略 $\pi$ 所能获得的期望回报：

$Q^\pi(s, a) = \mathbb{E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t=s, A_t=a \right]$

其中 $\gamma \in [0, 1)$ 是[折扣](@entry_id:139170)因子，$R_{t+k+1}$ 是在时间步 $t+k+1$ 获得的奖励。

所有动作[价值函数](@entry_id:144750)中，存在一个最优的动作[价值函数](@entry_id:144750) $Q^*(s, a)$，它对应于在所有策略中能够获得的最大期望回报：

$Q^*(s, a) = \max_{\pi} Q^\pi(s, a)$

一旦我们知道了 $Q^*(s, a)$，[最优策略](@entry_id:138495)就可以通过在每个状态下选择使得 $Q^*$ 值最大的动作来直接获得：$\pi^*(s) = \arg\max_{a} Q^*(s, a)$。因此，[强化学习](@entry_id:141144)的核心任务之一就是求解 $Q^*$。

$Q^*$ 满足一个重要的性质，即**贝尔曼最优性方程 (Bellman optimality equation)**。对于一个确定性环境，该方程可以写作：

$Q^*(s, a) = r(s, a) + \gamma \max_{a'} Q^*(s', a')$

其中 $r(s, a)$ 是在状态 $s$ 执行动作 $a$ 得到的即时奖励，$s'$ 是确定的下一个状态。这个方程表明，一个状态-动作对的最优价值，等于执行该动作获得的即时奖励，加上经过折扣的、在下一状态所能达到的最大未来价值。这个方程构成了所有[Q学习](@entry_id:144980)算法的理论基石。对于一个有限[状态和](@entry_id:193625)动作空间的[马尔可夫决策过程](@entry_id:140981)（MDP），$Q^*$ 是这个非线性方程组的唯一解 。

### [Q学习](@entry_id:144980)：近似最优动作[价值函数](@entry_id:144750)

[Q学习](@entry_id:144980)是一种无需模型（model-free）的、基于时序差分（Temporal Difference, TD）的算法，它旨在直接学习最优动作[价值函数](@entry_id:144750) $Q^*$，而无需知道环境的动力学模型（即转移概率 $P$ 和[奖励函数](@entry_id:138436) $R$）。

[Q学习](@entry_id:144980)的核心是其更新规则。当智能体在状态 $s_t$ 执行动作 $a_t$，获得奖励 $r_{t+1}$ 并转移到新状态 $s_{t+1}$ 后，它会根据以下规则更新其对 $Q(s_t, a_t)$ 的估计：

$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ \underbrace{r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a')}_{\text{TD Target}} - Q(s_t, a_t) \right]$

这里的 $\alpha$ 是学习率，控制着更新的步长。方括号内的部分被称为**时序差分误差 (TD error)**，它代表了当前估计 $Q(s_t, a_t)$ 与一个更好的目标——**TD目标 (TD Target)**——之间的差距。TD目标 $r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a')$ 是对 $Q^*(s_t, a_t)$ 的一个自举（bootstrapped）估计，它使用了当前的[Q值](@entry_id:265045)估计来“引导”更新。正是这个 `max` 算子，使得[Q学习](@entry_id:144980)成为一种异策略（off-policy）算法，因为它在更新Q值时，总是考虑下一状态中最优的可能动作，而不管智能体实际遵循的行为策略是什么。

### [Q学习](@entry_id:144980)中的基本挑战与对策

尽管[Q学习](@entry_id:144980)的理念简洁而强大，但在实际应用中，尤其是在与[深度神经网络](@entry_id:636170)等函数近似方法结合时，会遇到一系列挑战。

#### [探索与利用](@entry_id:174107)的权衡

为了学习到[最优策略](@entry_id:138495)，智能体必须充分探索环境，以发现可能带来高回报的路径。然而，为了最大化累积奖励，它又应该利用已有的知识，选择当前看来最好的动作。这就是著名的**[探索-利用困境](@entry_id:171683) (exploration-exploitation dilemma)**。

最简单的探索策略是 $\epsilon$-贪心（epsilon-greedy），即以 $1-\epsilon$ 的概率选择当前最优动作（利用），以 $\epsilon$ 的概率随机选择一个动作（探索）。然而，这种探索方式是盲目的。更高级的探索机制旨在实现更有效、更有针对性的探索。

**乐观初始化 (Optimistic Initialization)** 是一种简单而有效的探索策略，其核心思想是“面对不确定性时的乐观主义” 。该方法将所有[Q值](@entry_id:265045)初始化为一个可能达到的上限值。例如，如果奖励被归一化到 $[0, 1]$，那么任何回报的上限是 $\frac{1}{1-\gamma}$。通过将所有初始Q值设为这个上限，智能体会被激励去尝试那些尚未被充分探索的动作。每当一个动作被尝试后，如果其回报低于这个乐观的初始值，它的[Q值](@entry_id:265045)就会下降，使得其他仍保持高Q值的未探索动作显得更具吸[引力](@entry_id:175476)。这种“失望驱动”的机制会引导智能体系统地探索所有状态-动作对，直到它们的真实价值被学到。在一个简单的链式环境中，一个采用乐观初始化的智能体会依次尝试每个状态下的“坏”动作和“好”动作，从而高效地找到通往最终奖励的路径；而一个采用中性初始化（如全零初始化）的贪心智能体则可能在初始状态陷入自循环，永远无法发现奖励 。在[深度Q网络](@entry_id:635281)（DQN）中，这种乐观主义可以通过初始化输出层的偏置项（bias）为一个较大的正数来实现，同时保持网络权重较小，以确保训练初期的稳定性。

**[参数空间](@entry_id:178581)噪声 (Parameter-Space Noise)** 提供了另一种更复杂的探索方式，代表性工作是Noisy Nets 。与在[动作选择](@entry_id:151649)阶段添加随机性的 $\epsilon$-贪心不同，Noisy Nets将噪声直接注入到Q[网络模型](@entry_id:136956)的参数中。例如，网络最后一层的权重和偏置可以被建模为带有可学习噪声参数的[随机变量](@entry_id:195330)。在每个决策时刻，智能体从这些参数的[分布](@entry_id:182848)中采样一组具体的权重和偏置，然后基于这组“受扰动”的参数计算[Q值](@entry_id:265045)并进行贪心选择。这种探索是状态依赖的：在某些状态下，参数的扰动可能对[Q值](@entry_id:265045)排序影响不大，导致策略确定；而在另一些状态下，扰动可能显著改变Q值排序，导致探索性行为。通过训练噪声参数，智能体可以学会在需要探索的地方增加Q值的不确定性，从而实现更智能的探索。与 $\epsilon$-贪心策略相比，参数噪声探索赋予了次优动作一个非零的被选择概率，该概率平滑地依赖于Q值之间的差距以及噪声的幅度，从而实现了更细致的权衡。例如，对于两个动作，其[Q值](@entry_id:265045)分别为 $Q_1 > Q_2$，在参数噪声下选择次优动作2的概率为 $1 - \Phi\left(\frac{Q_1 - Q_2}{\sigma\sqrt{2}}\right)$，其中 $\Phi$ 是标准正态分布的累积分布函数，$\sigma$ 是噪声[标准差](@entry_id:153618)。这表明，当Q值差距很大时，探索概率很小；当差距很小时，探索概率接近0.5，这是一种非常合理的行为 。

#### [函数近似](@entry_id:141329)的风险

对于状态空间巨大的问题，使用表格来存储所有[Q值](@entry_id:265045)（即表格化[Q学习](@entry_id:144980)）是不可行的。我们必须使用**函数近似 (function approximation)**，例如线性模型或深度神经网络，来从状态特征中估计[Q值](@entry_id:265045)，即 $Q(s, a; \theta)$，其中 $\theta$ 是模型参数。函数近似带来了强大的泛化能力，但也引入了新的挑战。

**泛化与[异策略学习](@entry_id:634676)的冲突**：[函数近似](@entry_id:141329)通过共享参数在不同状态-动作对之间泛化。然而，在[异策略学习](@entry_id:634676)中，行为策略可能永远不会访问某些动作，这使得对这些动作的价值估计完全依赖于模型的泛化能力。这种泛化不一定正确。在一个简单的MDP中，假设行为策略从不在状态 $s_0$ 选择动作 $a_R$。如果使用表格表示，由于 $(s_0, a_R)$ 从未被经历，其[Q值](@entry_id:265045) $Q(s_0, a_R)$ 将永远停留在其初始值。如果使用[函数近似](@entry_id:141329)器，例如 $Q_\theta(s,a) = \theta^\top \phi(s) + \beta \cdot \mathbf{1}[a=a_R]$，即使只观察到动作 $a_L$ 的数据，对共享参数 $\theta$ 的更新也会改变对 $Q(s_0, a_R)$ 的估计。然而，特定于动作 $a_R$ 的参数 $\beta$ 却无法从数据中学到，因为它的梯度总是零。因此，泛化得到的 $Q(s_0, a_R)$ 的值能否收敛到最优值 $Q^*(s_0, a_R)$ 取决于模型结构和参数初始化的偶然性，没有任何保证 。

**最大化偏差 (Maximization Bias)**：[Q学习](@entry_id:144980)更新中的 `max` 算子是偏差的来源。由于[Q值](@entry_id:265045)估计本身是带噪声的[随机变量](@entry_id:195330)，取最大值操作会系统性地偏向于那些因噪声而被高估的动作。这种偏差在动作价值差距很小而估计[方差](@entry_id:200758)很大时尤为严重 。假设有两个动作，其真实[Q值](@entry_id:265045)非常接近，但我们的估计 $\hat{Q}$ 存在[高斯噪声](@entry_id:260752)。即使两个动作的期望回报相同，$\max(\hat{Q}(a_1), \hat{Q}(a_2))$ 的[期望值](@entry_id:153208)也会大于真实的 $\max(Q^*(a_1), Q^*(a_2))$。**双[Q学习](@entry_id:144980) (Double Q-learning)** 通过[解耦](@entry_id:637294)[动作选择](@entry_id:151649)和价值评估来解决这个问题。它维护两个独立的[Q值](@entry_id:265045)估计器，$Q_A$ 和 $Q_B$。在构建TD目标时，它使用 $Q_A$ 来选择最优动作 $a^* = \arg\max_{a'} Q_A(s', a')$，但使用 $Q_B$ 来评估该动作的价值 $Q_B(s', a^*)$。TD目标变为 $r + \gamma Q_B(s', \arg\max_{a'} Q_A(s', a'))$。由于 $Q_A$ 和 $Q_B$ 的估计噪声是独立的，高估某个动作的 $Q_A$ 值并不意味着其 $Q_B$ 值也被高估，从而有效消除了最大化偏差。理论分析表明，在存在估计噪声时，双[Q学习](@entry_id:144980)的偏差（甚至是负偏差，即低估）的[绝对值](@entry_id:147688)远小于标准[Q学习](@entry_id:144980)的正向偏差 。

**价值[分布](@entry_id:182848)的复杂性 (Complexity of Value Distribution)**：标准[Q学习](@entry_id:144980)只估计回报的[期望值](@entry_id:153208) $\mathbb{E}[G]$，但这忽略了回报[分布](@entry_id:182848)的完整信息。在具有高度随机性的环境中，回报的[分布](@entry_id:182848)可能是多峰的、偏斜的，或者具有重尾。只关注均值可能会导致风险不敏感的决策。**[分布](@entry_id:182848)式强化学习 (Distributional RL)** 的核心思想是学习回报的完整[概率分布](@entry_id:146404) $\mathcal{Z}(s, a)$，而不仅仅是其期望。一种流行的方法是**分类[分布](@entry_id:182848)式RL (Categorical Distributional RL)**，它将回报的[分布](@entry_id:182848)表示为一个在固定离散支撑点（原子）上的分类[分布](@entry_id:182848)。例如，将回报范围 $[v_{\min}, v_{\max}]$ 划分成51个原子 $\{z_i\}$。网络输出的不再是一个Q值，而是一个[概率向量](@entry_id:200434)，表示回报落在每个原子上的概率。对于观察到的回报 $g$，算法会将其概率质量投影到相邻的两个原子上，然后用这个投影后的[分布](@entry_id:182848)去更新原有的[概率分布](@entry_id:146404)估计。通过学习完整的[分布](@entry_id:182848)，智能体可以获得更丰富的信息，这不仅可以提高样本效率，还能让其做出更稳健的决策。在回报[分布](@entry_id:182848)非高斯或多峰的场景下（例如，由随机延迟导致的 $\gamma^D$ 回报），[分布](@entry_id:182848)式RL估计的[分布](@entry_id:182848)与真实[分布](@entry_id:182848)之间的[Wasserstein距离](@entry_id:147338)显著小于标准[Q学习](@entry_id:144980)估计值（一个位于均值的狄拉克[分布](@entry_id:182848)）与真实[分布](@entry_id:182848)的距离 。

### 稳定化深度[Q学习](@entry_id:144980)：致命三元组

当我们将函数近似（特别是深度神经网络）、自举（bootstrapping，即用当前估计更新自身）和[异策略学习](@entry_id:634676)这三个要素结合在一起时，训练过程可能变得极不稳定，甚至发散。这个组合被称为**致命三元组 (The Deadly Triad)**。理解和缓解这种不稳定性是[深度强化学习](@entry_id:638049)的核心课题。

**[目标网络](@entry_id:635025) (Target Network)** 是DQN中引入的第一个关键稳定技术。其思想是，在计算TD目标时，使用一个独立的、周期性更新的**[目标网络](@entry_id:635025)** $Q_{\text{target}}$，而不是当前正在快速更新的**在线网络** $Q_{\text{online}}$。TD目标变为 $r + \gamma \max_{a'} Q_{\text{target}}(s', a')$。[目标网络](@entry_id:635025)的参数 $\theta_{\text{target}}$ 每隔 $K$ 步才从在线网络复制一次（$\theta_{\text{target}} \leftarrow \theta_{\text{online}}$），在此期间保持不变。这种延迟更新打破了更新目标与被更新值之间的直接耦合，使得TD目标在短期内更加稳定。我们可以将这个过程建模为一个延迟的[不动点迭代](@entry_id:749443) 。贝尔曼最优算子 $\mathcal{T}$ 是一个在[无穷范数](@entry_id:637586)下的 $\gamma$-压缩映射，保证了表格化值迭代的收敛。引入[目标网络](@entry_id:635025)相当于将更新规则 $Q_{t+1} \leftarrow (1-\alpha)Q_t + \alpha \mathcal{T}Q_t$ 修改为 $Q_{t+1} \leftarrow (1-\alpha)Q_t + \alpha \mathcal{T}Q_{t-d(t)}$，其中 $d(t)$ 是延迟。这种延迟会减慢[收敛速度](@entry_id:636873)，并可能引入[振荡](@entry_id:267781)。延迟周期 $K$ 越大、折扣因子 $\gamma$ 越接近1，收敛越慢，潜在的滞后偏差也越大。

**从谱分析看稳定性**：更深入地，我们可以通过线性化动态系统来分析不稳定性。在最优解 $Q^*$ 附近，[Q学习](@entry_id:144980)的[更新过程](@entry_id:273573)可以近似为一个线性系统 $e_{k+1} \approx A e_k$，其中 $e_k$ 是第 $k$ 步的误差向量，$A$ 是一个结合了贝尔曼算子和函数近似器[雅可比矩阵](@entry_id:264467)的有效算子。系统的稳定性由 $A$ 的[谱半径](@entry_id:138984) $\rho(A)$（最大[特征值](@entry_id:154894)的模）决定。如果 $\rho(A) \ge 1$，误差就会放大，导致发散。对于表格化[Q学习](@entry_id:144980)，在最优策略固定的局部区域，其更新算子的雅可比矩阵的谱半径不大于 $\gamma$，因此是稳定的 。然而，当与[非线性](@entry_id:637147)[函数近似](@entry_id:141329)器（如[神经网](@entry_id:276355)络）结合时，有效算子 $A$ 的[谱半径](@entry_id:138984)可能超过1。例如，如果 $A$ 有模大于1的[复共轭](@entry_id:174690)[特征值](@entry_id:154894)，[训练误差](@entry_id:635648)将呈现[指数增长](@entry_id:141869)的[振荡](@entry_id:267781)，最终导致发散 。

**通过正则化保证收缩**：致命三元组的核心问题在于，更新算子不再保证是一个[压缩映射](@entry_id:139989)。我们可以从理论上分析这个算子的[利普希茨常数](@entry_id:146583)（Lipschitz constant）。一个算子的[利普希茨常数](@entry_id:146583)是其“放大”输入变化的上限。如果该常数严格小于1，则算子是压缩映射，迭代应用保证收敛。在包含致命三元组的设置中，单步更新的有效[放大系数](@entry_id:144315)可以被上界为 $\gamma \cdot c_{\text{mis}} \cdot L(Q)$，其中 $\gamma$ 来自自举， $c_{\text{mis}} \ge 1$ 是由异策略采样引入的[分布](@entry_id:182848)不匹配系数，而 $L(Q)$ 是Q网络函数近似器的[利普希茨常数](@entry_id:146583)。如果这个乘积大于等于1，收敛性就无法保证。对于一个由多层[线性变换](@entry_id:149133) $W_\ell$ 和[ReLU激活函数](@entry_id:138370)构成的网络，$L(Q)$ 可以被其各层权重的[谱范数](@entry_id:143091)（最大[奇异值](@entry_id:152907)）之积所[上界](@entry_id:274738)。因此，一个直接的稳定化方法是通过正则化来约束网络的[利普希茨常数](@entry_id:146583)。例如，我们可以对每一层权重矩阵 $W_\ell$ 施加[谱范数](@entry_id:143091)约束，即 $\lVert W_\ell \rVert_2 \le s$。通过选择合适的 $s$，我们可以确保总的[放大系数](@entry_id:144315) $\gamma \cdot c_{\text{mis}} \cdot \prod_\ell \lVert W'_\ell \rVert_2  1$，从而在理论上恢复了更新算子的压缩性质，保证了训练的稳定性 。

### 价值函数的高级应用

除了直接用于决策，价值函数在[策略改进](@entry_id:139587)和引导学习方面还有更广泛的应用。

**保守[策略改进](@entry_id:139587) (Conservative Policy Improvement)**：标准的[策略改进](@entry_id:139587)步骤是，在评估完当前策略 $\pi_k$ 的Q函数 $Q^{\pi_k}$ 后，通过贪心化得到新策略 $\pi_{k+1}(s) = \arg\max_a Q^{\pi_k}(s, a)$。这个过程在拥有精确的 $Q^{\pi_k}$ 时保证了性能的单调提升。然而，当使用[函数近似](@entry_id:141329)时，我们得到的是一个有误差的估计 $\tilde{Q} \approx Q^{\pi_k}$。此时，对 $\tilde{Q}$ 进行完全贪心化可能是危险的，甚至会导致灾难性的性能下降。例如，在一个环境中，安全动作回报为0，而一个有风险的动作会带来巨大负回报。如果近似Q函数由于误差错误地高估了风险动作的价值，贪心策略会立刻切换到这个坏动作，导致价值崩溃 。为了避免这种情况，现代算法（如TRPO, PPO）采用了**保守[策略改进](@entry_id:139587)**的思想。它们限制了新策略与旧策略之间的变化幅度，通常通过[KL散度](@entry_id:140001)（Kullback–Leibler divergence）来度量。例如，**保守策略迭代（[CPI](@entry_id:748135)）**通过混合新旧策略 $\pi' = (1-\alpha)\pi_0 + \alpha\pi_{\text{greedy}}$，并选择最大的混合系数 $\alpha$ 使得 $D_{\text{KL}}(\pi_0 \| \pi') \le \varepsilon$，其中 $\varepsilon$ 是一个小的信任域半径。这种方法通过小步更新来防止因近似误差导致的策略剧变，从而实现更安全、更稳定的学习。

**回报塑形 (Reward Shaping)**：在许多实际问题中，环境的自然奖励非常稀疏（例如，只有在任务最终完成时才有奖励），这使得学习异常困难。**回报塑形**是一种通过设计额外的、更密集的奖励信号来引导智能体学习的技术。然而，随意的奖励设计很容易改变问题的[最优策略](@entry_id:138495)。**基于[势能](@entry_id:748988)的回报塑形 (Potential-Based Reward Shaping, PBRS)** 提供了一种保证最优策略不变的理论框架 。它通过一个定义在[状态空间](@entry_id:177074)上的[势能函数](@entry_id:200753) $\Phi(s)$ 来构造塑形奖励 $F(s, a, s') = \gamma \Phi(s') - \Phi(s)$。塑形后的总奖励为 $R_{\text{shaped}} = R_{\text{base}} + F$。可以证明，在PBRS下，新的最优Q函数 $Q^*_{\text{shaped}}$ 与原最优Q函数 $Q^*$ 的关系为 $Q^*_{\text{shaped}}(s, a) = Q^*(s, a) - \Phi(s)$。由于在任何状态 $s$ 下，$\Phi(s)$ 对于所有动作 $a$ 都是一个常数，因此它不影响 $\arg\max_a$ 的结果，即 $\arg\max_a Q^*_{\text{shaped}}(s, a) = \arg\max_a Q^*(s, a)$。这意味着最优策略保持不变。通过精心设计势能函数（例如，使其朝向目标状态递增），PBRS可以提供有用的学习梯度，显著加快[收敛速度](@entry_id:636873)，同时保证最终学到的是原问题的最优解。