{
    "hands_on_practices": [
        {
            "introduction": "此练习在一个包含固有风险的场景中，比较了两种基础的强化学习算法：Q-learning 和 SARSA。通过亲手实现这两种算法，你将获得关于离策略（off-policy）和同策略（on-policy）学习之间关键差异的第一手经验。你将理解这一选择如何导致智能体分别表现出“乐观”或“保守”的行为模式。",
            "id": "3113683",
            "problem": "考虑一个有限马尔可夫决策过程 (MDP)，它有一个非终止的起始状态和两条通往目标的不相交走廊。其中一条走廊是危险的，因为其中的一个动作可能导致坠崖并获得很大的负回报；另一条走廊是安全的，因为所有动作都能避免灾难性终止。此题的目的是在一个具有探索性行为策略的随机环境中，比较状态-动作-奖励-状态-动作 (SARSA) 和 Q-learning 的收敛动态，通过动作价值函数 $Q^{\\pi}$ 和 $Q^{*}$ 突出显示在线策略与离线策略的差异。\n\n该有限 MDP 的具体规定如下。状态集为 $\\{s_{0}, r_{1}, r_{2}, u_{1}, u_{2}, u_{3}, g, c\\}$，其中 $s_{0}$ 是起始状态，$g$ 是目标终止状态，$c$ 是悬崖终止状态。所有状态的动作集都相同，为 $\\{0,1,2\\}$；为便于理解：\n- 在 $s_{0}$，动作 $0$ 是“走危险路线”，动作 $1$ 是“走安全路线”，动作 $2$ 是“原地不动”。\n- 在走廊状态 $r_{1}, r_{2}, u_{1}, u_{2}, u_{3}$，动作 $0$ 是“前进”，动作 $1$ 是“原地不动”，动作 $2$ 是“滑倒”。\n- 在终止状态 $g$ 和 $c$，回合立即结束。\n\n确定性的转移和奖励如下：\n- 从 $s_{0}$ 出发：\n  - 动作 $0$ 转移到 $r_{1}$，奖励为 $-1$。\n  - 动作 $1$ 转移到 $u_{1}$，奖励为 $-1$。\n  - 动作 $2$ 转移到 $s_{0}$，奖励为 $-1$。\n- 危险走廊：\n  - 从 $r_{1}$ 出发：动作 $0$ 到 $r_{2}$，奖励为 $-1$；动作 $1$ 到 $r_{1}$，奖励为 $-1$；动作 $2$ 到 $c$，奖励为 $-100$。\n  - 从 $r_{2}$ 出发：动作 $0$ 到 $g$，奖励为 $0$；动作 $1$ 到 $r_{2}$，奖励为 $-1$；动作 $2$ 到 $c$，奖励为 $-100$。\n- 安全走廊：\n  - 从 $u_{1}$ 出发：动作 $0$ 到 $u_{2}$，奖励为 $-1$；动作 $1$ 到 $u_{1}$，奖励为 $-1$；动作 $2$ 到 $u_{1}$，奖励为 $-1$。\n  - 从 $u_{2}$ 出发：动作 $0$ 到 $u_{3}$，奖励为 $-1$；动作 $1$ 到 $u_{2}$，奖励为 $-1$；动作 $2$ 到 $u_{2}$，奖励为 $-1$。\n  - 从 $u_{3}$ 出发：动作 $0$ 到 $g$，奖励为 $0$；动作 $1$ 到 $u_{3}$，奖励为 $-1$；动作 $2$ 到 $u_{3}$，奖励为 $-1$。\n\n学习期间使用的行为策略是对当前动作价值估计 $Q(s,a)$ 采用 $\\epsilon$-贪心策略：以 $1-\\epsilon$ 的概率选择一个最大化 $Q(s,a)$ 的动作，以 $\\epsilon$ 的概率从动作集中均匀随机选择一个动作。如果多个动作的 $Q(s,a)$ 值相同且最大，则选择索引最小的动作。\n\n训练要求：\n- 实现表格 Q-learning 和表格状态-动作-奖励-状态-动作 (SARSA)，使用固定的学习率 $\\alpha$ 和折扣因子 $\\gamma$；将所有动作价值估计 $Q(s,a)$ 初始化为 $0$。\n- 回合从 $s_{0}$ 开始，在到达 $g$ 或 $c$ 时终止，或在达到固定的 $M$ 步上限后终止以防止无限循环。使用 $M = 100$。\n- 使用固定的伪随机数种子，以便在 $\\epsilon$-贪心探索期间的动作选择是可复现的。\n\n评估：\n- 训练后，计算差值\n  $$d_{\\mathrm{QL}} = Q_{\\mathrm{QL}}(s_{0}, 0) - Q_{\\mathrm{QL}}(s_{0}, 1), \\quad d_{\\mathrm{SARSA}} = Q_{\\mathrm{SARSA}}(s_{0}, 0) - Q_{\\mathrm{SARSA}}(s_{0}, 1),$$\n  其中 $Q_{\\mathrm{QL}}$ 和 $Q_{\\mathrm{SARSA}}$ 分别是在 Q-learning 和 SARSA 下学到的动作价值函数。正差值表示在起始时学到了对危险动作的偏好，而负差值表示学到了对安全动作的偏好。\n\n推导和实现中使用的基本原理：\n- 有限马尔可夫决策过程 (MDP) 中策略的动作价值函数 $Q^{\\pi}(s,a)$ 的定义。\n- $Q^{\\pi}(s,a)$ 的贝尔曼期望方程和 $Q^{*}(s,a)$ 的贝尔曼最优方程。\n- 用于强化学习中探索的 $\\epsilon$-贪心策略的构建。\n\n测试套件：\n- 情况 1：$\\epsilon = 0.10$，$\\alpha = 0.50$，$\\gamma = 1.00$，回合数 $= 20000$，种子 $= 7$。\n- 情况 2 (边界)：$\\epsilon = 0.00$，$\\alpha = 0.50$，$\\gamma = 1.00$，回合数 $= 10000$，种子 $= 7$。\n- 情况 3 (重度探索)：$\\epsilon = 0.40$，$\\alpha = 0.50$，$\\gamma = 1.00$，回合数 $= 30000$，种子 $= 7$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。该列表必须按顺序包含每个测试用例的差值对，平铺成一个列表：\n  $$[d_{\\mathrm{QL}}^{(1)}, d_{\\mathrm{SARSA}}^{(1)}, d_{\\mathrm{QL}}^{(2)}, d_{\\mathrm{SARSA}}^{(2)}, d_{\\mathrm{QL}}^{(3)}, d_{\\mathrm{SARSA}}^{(3)}],$$\n  其中上标 $(i)$ 表示测试用例 $1$ 至 $3$。每个条目必须是实数（浮点数）。",
            "solution": "该问题要求对强化学习中两种基本的时间差分 (TD) 控制算法进行比较分析：Q-learning 和状态-动作-奖励-状态-动作 (SARSA)。分析在一个专门构建的有限马尔可夫决策过程 (MDP) 中进行，该过程旨在突显离线策略学习和在线策略学习之间的关键区别。\n\n首先，我们形式化基本概念。一个有限 MDP 是一个元组 $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$，其中 $\\mathcal{S}$ 是有限的状态集，$\\mathcal{A}$ 是有限的动作集，$P$ 是状态转移概率函数 $P(s'|s,a) = \\Pr(S_{t+1}=s'|S_t=s, A_t=a)$，$R$ 是奖励函数 $R(s,a,s') = \\mathbb{E}[R_{t+1}|S_t=s, A_t=a, S_{t+1}=s']$，$\\gamma \\in [0, 1]$ 是折扣因子。在这个问题中，转移是确定性的，因此对于某个特定的 $s'$，$P(s'|s,a) = 1$，否则为 $0$。\n\n智能体的目标是学习一个策略 $\\pi(a|s) = \\Pr(A_t=a|S_t=s)$，以最大化期望的折扣累积奖励。在策略 $\\pi$ 下，一个状态-动作对 $(s,a)$ 的价值由动作价值函数 $Q^{\\pi}(s,a)$ 给出：\n$$Q^{\\pi}(s,a) = \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\bigg| S_t=s, A_t=a \\right]$$\n该函数满足贝尔曼期望方程：\n$$Q^{\\pi}(s,a) = \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^{\\pi}(s',a') \\right]$$\n最优动作价值函数 $Q^{*}(s,a) = \\max_{\\pi} Q^{\\pi}(s,a)$，提供了从任何状态-动作对可获得的最高期望回报。它满足贝尔曼最优方程：\n$$Q^{*}(s,a) = \\sum_{s'} P(s'|s,a) \\left[ R(s,a,s') + \\gamma \\max_{a'} Q^{*}(s',a') \\right]$$\n\nQ-learning 和 SARSA 是 TD 方法，它们无需环境模型即可迭代地学习这些动作价值函数。它们的更新规则揭示了它们之间的根本区别。\n\nQ-learning 是一种离线策略算法。在从状态 $S_t$ 转移到 $S_{t+1}$ 并获得奖励 $R_{t+1}$ 后，其对动作价值 $Q(S_t, A_t)$ 的更新规则是：\n$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t) \\right]$$\n项 $\\max_{a'} Q(S_{t+1}, a')$ 表示从下一个状态 $S_{t+1}$ 出发的最优动作的估计价值。Q-learning 使用这个贪心选择来更新其价值函数，而不管行为策略接下来实际采取哪个动作。因此，它直接学习最优动作价值函数 $Q^*$ 的估计。\n\nSARSA 是一种在线策略算法。它的更新规则使用元组 $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$：\n$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right]$$\n这里，更新目标包括 $Q(S_{t+1}, A_{t+1})$，即在状态 $S_{t+1}$ 中根据行为策略实际选择的动作 $A_{t+1}$ 的价值。因此，SARSA 学习的是它当前正在遵循的策略的动作价值函数 $Q^{\\pi}$。在这个问题中，策略 $\\pi$ 是相对于当前 $Q$ 值的 $\\epsilon$-贪心策略。\n\n现在，我们来分析这个特定的 MDP。位于起始状态 $s_0$ 的智能体可以选择“危险”走廊（动作 0）或“安全”走廊（动作 1）。折扣因子为 $\\gamma=1$，意味着我们寻求最大化未折扣的奖励总和。\n通往目标的最优路径是经过危险走廊：$s_0 \\to r_1 \\to r_2 \\to g$。这条路径需要 3 步，总奖励为 $(-1) + (-1) + 0 = -2$。\n安全路径更长：$s_0 \\to u_1 \\to u_2 \\to u_3 \\to g$。这条路径需要 4 步，总奖励为 $(-1) + (-1) + (-1) + 0 = -3$。\n因此，从 $s_0$ 开始的最优动作价值为 $Q^*(s_0, 0) = -2$ 和 $Q^*(s_0, 1) = -3$。\n\n当 $\\epsilon > 0$ 时，行为策略是探索性的。\nQ-learning 作为一种离线策略算法，旨在找到 $Q^*$。它会学到危险路径更优（$Q(s_0, 0) \\approx -2$ 和 $Q(s_0, 1) \\approx -3$），尽管它自己的探索性动作可能偶尔会使它坠崖。它的价值估计是基于未来它将采取贪心行动的假设。因此，我们预测 $Q_{\\mathrm{QL}}(s_0, 0) > Q_{\\mathrm{QL}}(s_0, 1)$，从而得到一个正差值 $d_{\\mathrm{QL}} > 0$。\n\nSARSA 作为一种在线策略算法，学习的是 $\\epsilon$-贪心策略本身的价值。该策略有非零概率（$\\epsilon/3$）在状态 $r_1$ 和 $r_2$ 选择“滑倒”动作（动作 2），这会导致坠崖并获得 -100 的巨大负奖励。这种可能性被计入 SARSA 的价值估计中。因此，采取危险路径的期望回报是最佳结果和灾难性“滑倒”结果的加权平均。对于足够大的惩罚，这个期望回报将低于安全路径的期望回报，因为在安全路径中探索（“滑倒”或“原地不动”）不是灾难性的。SARSA 将学到，在它自己的探索性策略下，安全走廊是更可取的。我们预测 $Q_{\\mathrm{SARSA}}(s_0, 0)  Q_{\\mathrm{SARSA}}(s_0, 1)$，从而得到一个负差值 $d_{\\mathrm{SARSA}}  0$。这种效应的幅度应随 $\\epsilon$ 的增大而增大。\n\n对于 $\\epsilon=0.00$ 的情况，策略是纯粹贪心的。在 SARSA 中选择的动作 $A_{t+1}$ 将是 $\\arg\\max_{a'} Q(S_{t+1}, a')$。在这种特定情况下，假设存在唯一的最大值，SARSA 的更新规则在功能上变得与 Q-learning 的更新规则相同。两种算法都将遵循相同的贪心轨迹并以相同的方式更新它们的价值。它们都将收敛到 $Q^*$，并学到危险路径是最优的。因此，我们期望 $d_{\\mathrm{QL}}^{(2)} > 0$ 和 $d_{\\mathrm{SARSA}}^{(2)} > 0$，并且它们的值会非常接近。这表明 SARSA 的“保守”性质是在线策略评估探索性策略的直接结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares Q-learning and SARSA on a specified MDP\n    to demonstrate the difference between off-policy and on-policy learning.\n    \"\"\"\n    \n    #\n    # Environment Setup\n    #\n    states = ['s0', 'r1', 'r2', 'u1', 'u2', 'u3', 'g', 'c']\n    state_map = {name: i for i, name in enumerate(states)}\n    num_states = len(states)\n    num_actions = 3\n    \n    s0, r1, r2, u1, u2, u3, g, c = (state_map[s] for s in states)\n    terminal_states = {g, c}\n    \n    # Transitions: T[state_idx][action_idx] = (next_state_idx, reward)\n    T = {\n        s0: {0: (r1, -1.0), 1: (u1, -1.0), 2: (s0, -1.0)},\n        r1: {0: (r2, -1.0), 1: (r1, -1.0), 2: (c, -100.0)},\n        r2: {0: (g, 0.0),  1: (r2, -1.0), 2: (c, -100.0)},\n        u1: {0: (u2, -1.0), 1: (u1, -1.0), 2: (u1, -1.0)},\n        u2: {0: (u3, -1.0), 1: (u2, -1.0), 2: (u2, -1.0)},\n        u3: {0: (g, 0.0),  1: (u3, -1.0), 2: (u3, -1.0)},\n    }\n    \n    #\n    # Agent and Policy Logic\n    #\n    def select_action(q_s, epsilon, rng):\n        \"\"\"Selects an action using an epsilon-greedy policy.\"\"\"\n        if rng.random()  epsilon:\n            return rng.integers(num_actions)\n        else:\n            # np.argmax handles tie-breaking by returning the first index\n            # with the maximum value, which matches the problem spec.\n            return np.argmax(q_s)\n\n    #\n    # Training Function\n    #\n    def run_experiment(algorithm, params, rng):\n        \"\"\"Runs a full training experiment for a given algorithm.\"\"\"\n        epsilon, alpha, gamma, total_episodes, max_steps = params\n        \n        Q = np.zeros((num_states, num_actions))\n\n        for _ in range(total_episodes):\n            state = s0\n            \n            # For SARSA, the first action must be selected before the loop begins.\n            if algorithm == 'sarsa':\n                action = select_action(Q[state], epsilon, rng)\n\n            for _ in range(max_steps):\n                if state in terminal_states:\n                    break\n                \n                # For Q-learning, action is selected inside the main loop.\n                if algorithm == 'q_learning':\n                    action = select_action(Q[state], epsilon, rng)\n\n                # Get next state and reward from the environment model.\n                next_state, reward = T[state][action]\n                \n                if algorithm == 'q_learning':\n                    # Off-policy update: uses max Q-value at next state\n                    if next_state in terminal_states:\n                        target = reward\n                    else:\n                        target = reward + gamma * np.max(Q[next_state])\n                    Q[state, action] += alpha * (target - Q[state, action])\n                \n                elif algorithm == 'sarsa':\n                    # On-policy update: uses Q-value of the actual next action\n                    if next_state in terminal_states:\n                        # No next action, target is just the reward\n                        target = reward\n                    else:\n                        next_action = select_action(Q[next_state], epsilon, rng)\n                        target = reward + gamma * Q[next_state, next_action]\n                    Q[state, action] += alpha * (target - Q[state, action])\n                    # Update for next iteration\n                    action = next_action if next_state not in terminal_states else 0\n\n                state = next_state\n        return Q\n\n    #\n    # Main Execution Logic\n    #\n    test_cases = [\n        # (epsilon, alpha, gamma, episodes, seed)\n        (0.10, 0.50, 1.00, 20000, 7),\n        (0.00, 0.50, 1.00, 10000, 7),\n        (0.40, 0.50, 1.00, 30000, 7),\n    ]\n    max_steps_per_episode = 100\n    all_results = []\n    \n    for epsilon, alpha, gamma, episodes, seed in test_cases:\n        params = (epsilon, alpha, gamma, episodes, max_steps_per_episode)\n        \n        # Run Q-learning for the current case\n        rng_ql = np.random.default_rng(seed)\n        Q_ql = run_experiment('q_learning', params, rng_ql)\n        d_ql = Q_ql[s0, 0] - Q_ql[s0, 1]\n        \n        # Run SARSA for the current case, resetting the seed for fair comparison\n        rng_sarsa = np.random.default_rng(seed)\n        Q_sarsa = run_experiment('sarsa', params, rng_sarsa)\n        d_sarsa = Q_sarsa[s0, 0] - Q_sarsa[s0, 1]\n        \n        all_results.extend([d_ql, d_sarsa])\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "深度Q网络（DQN）可能会遭遇学习不稳定的问题，而引入目标网络（target network）在很大程度上解决了这一难题。本练习将DQN的更新过程建模为一个线性系统，通过分析其性质来深入探究这种不稳定性的动态。你将推导出系统产生振荡的条件，并通过模拟来验证你的理论预测，从而揭示为何目标网络更新频率是一个关键的超参数。",
            "id": "3113592",
            "problem": "给定一个深度Q网络 (DQN) 的学习动态的简化线性模型，DQN是一种深度强化学习 (DRL) 算法。目标是解析地近似振荡行为何时作为目标网络更新周期的函数出现，并通过使用快速傅里叶变换 (FFT) 测量学习到的动作价值函数的振荡谱来验证此近似。该近似必须从强化学习和线性系统理论的第一性原理出发，不依赖于任何快捷公式。\n\n考虑一个确定性的双状态马尔可夫决策过程 (MDP)，其具有两个标量动作价值函数分量 $q_1$ 和 $q_2$，分别代表两个状态的在线网络估计值。令 $h_1$ 和 $h_2$ 表示相应的目标网络估计值。环境是确定性的，从状态 1 转换到状态 2，再从状态 2 转换到状态 1。奖励是恒定的，由 $r_1$ 和 $r_2$ 给出。折扣因子为 $\\gamma \\in [0,1)$，学习率为 $\\alpha \\in (0,1)$。\n\n在此确定性设置下的规范 Bellman 最优性关系为\n$$\nQ^\\star_1 = r_1 + \\gamma Q^\\star_2, \\quad Q^\\star_2 = r_2 + \\gamma Q^\\star_1,\n$$\n其中 $Q^\\star_i$ 表示状态 $i$ 的最优动作价值。深度Q网络 (DQN) 通过时序差分更新来近似这些关系。在目标保持间隔内，目标网络是固定的，每步在线更新如下：\n$$\nq_1(t+1) = q_1(t) + \\alpha \\left( r_1 + \\gamma h_2(t) - q_1(t) \\right),\n$$\n$$\nq_2(t+1) = q_2(t) + \\alpha \\left( r_2 + \\gamma h_1(t) - q_2(t) \\right),\n$$\n目标网络每 $T$ 步通过一次硬拷贝进行更新：\n$$\nh_i(t+1) = \n\\begin{cases}\nq_i(t+1),  \\text{if } (t+1) \\bmod T = 0, \\\\\nh_i(t),  \\text{otherwise},\n\\end{cases}\n\\quad i \\in \\{1,2\\}.\n$$\n\n在任何长度为 $T$ 的目标保持间隔内，将 $h_1$ 和 $h_2$ 视为等于最后复制的值 $s_1$ 和 $s_2$ 的常数。推导经过 $T$ 次在线更新和一次目标拷贝后，将对 $(s_1, s_2)$推进到下一周期的对 $(s'_1, s'_2)$ 的跨周期线性仿射映射。然后，求出所得线性部分的特征值，以确定是否存在振荡模式。利用负特征值的存在来定义预测的每步振荡频率，并通过计算差分模式 $d(t) = q_1(t) - q_2(t)$ 的时间序列的 FFT 来验证此预测。\n\n你的程序必须：\n- 从上述每步 DQN 动态出发，推导出 $(s_1, s_2)$ 的周期到周期的线性映射，用 $\\alpha$、$\\gamma$ 和 $T$ 表示其矩阵，并计算其特征值。\n- 使用特征结构来预测振荡模式的出现。具体来说，如果较小的特征值为负，则将预测的每步角频率定义为 $\\omega_{\\text{pred}} = \\pi / T$；否则，设置 $\\omega_{\\text{pred}} = 0$。\n- 模拟 $N$ 步的每步动态以生成 $d(t)$，并计算其 FFT 振幅谱（不包括零频段），以获得以弧度/步为单位的测量峰值角频率 $\\omega_{\\text{meas}}$。\n- 对于每个测试用例，报告元组 $[\\omega_{\\text{pred}}, \\omega_{\\text{meas}}, \\text{resonant}]$，其中如果较小的特征值为负，则 $\\text{resonant}$ 是布尔值 $\\text{True}$，否则为 $\\text{False}$。\n\n对所有测试用例使用以下固定参数值：\n- 折扣因子 $\\gamma = 0.9$，\n- 学习率 $\\alpha = 0.2$，\n- 奖励 $r_1 = 0$ 和 $r_2 = 0$，\n- 初始在线值 $q_1(0) = 1$ 和 $q_2(0) = -1$，\n- 初始目标值 $h_1(0) = q_1(0)$ 和 $h_2(0) = q_2(0)$，\n- 模拟长度 $N = 4096$ 步。\n\n角度单位规范：所有角频率必须以弧度/步为单位表示。\n\n测试套件：\n- 扫描目标更新周期 $T$ 的值 $T \\in \\{1, 3, 4, 20, 50\\}$，以覆盖频繁更新、近阈值行为和长目标保持间隔的情况。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。\n- 对于每个测试用例，输出一个嵌套列表 $[\\omega_{\\text{pred}}, \\omega_{\\text{meas}}, \\text{resonant}]$，其中 $\\omega_{\\text{pred}}$ 和 $\\omega_{\\text{meas}}$ 均四舍五入到六位小数，$\\text{resonant}$ 为布尔值。\n- 示例格式：$[[0.785398,0.780000,True],[0.000000,0.020000,False],\\dots]$。",
            "solution": "该问题要求分析简化深度Q网络 (DQN) 模型中的振荡动态。这将分两部分完成：首先，基于学习更新的线性系统分析，从理论上推导振荡条件；其次，通过数值模拟来测量振荡频率并验证理论预测。\n\n### 第1部分：理论分析\n\n学习动态由一组关于在线动作价值估计值 $q_1(t)$ 和 $q_2(t)$ 的耦合一阶差分方程组描述。目标网络值 $h_1(t)$ 和 $h_2(t)$ 在 $T$ 步的周期内保持恒定。让我们分析系统在这样一个周期内的演化。\n\n在一个周期的开始，目标网络用在线网络的当前值进行更新。设这些值为 $s_1$ 和 $s_2$。因此，在该周期内（从周期内的第 $t=0$ 步到第 $t=T-1$ 步），我们有 $h_1 = s_1$ 和 $h_2 = s_2$。在线网络的每步更新由以下公式给出：\n$$\nq_1(t+1) = q_1(t) + \\alpha \\left( r_1 + \\gamma h_2 - q_1(t) \\right) = (1-\\alpha) q_1(t) + \\alpha (r_1 + \\gamma s_2)\n$$\n$$\nq_2(t+1) = q_2(t) + \\alpha \\left( r_2 + \\gamma h_1 - q_2(t) \\right) = (1-\\alpha) q_2(t) + \\alpha (r_2 + \\gamma s_1)\n$$\n问题指定，在一个周期开始时，在线网络的初始值与该周期的目标值相同，即 $q_1(0) = s_1$ 和 $q_2(0) = s_2$。\n\n这些是线性一阶非齐次差分方程。对于形式为 $x(t+1) = a x(t) + b$ 的方程，其通解为 $x(t) = a^t x(0) + b \\sum_{k=0}^{t-1} a^k = a^t x(0) + b \\frac{1-a^t}{1-a}$。\n将此应用于我们的系统，其中 $a = (1-\\alpha)$，我们得到：\n$$\nq_1(t) = (1-\\alpha)^t q_1(0) + \\alpha(r_1 + \\gamma s_2) \\frac{1-(1-\\alpha)^t}{\\alpha} = (1-\\alpha)^t s_1 + (1-(1-\\alpha)^t)(r_1 + \\gamma s_2)\n$$\n$$\nq_2(t) = (1-\\alpha)^t q_2(0) + \\alpha(r_2 + \\gamma s_1) \\frac{1-(1-\\alpha)^t}{\\alpha} = (1-\\alpha)^t s_2 + (1-(1-\\alpha)^t)(r_2 + \\gamma s_1)\n$$\n经过 $T$ 步后，在线网络的值被用来更新下一个周期的目标网络。设新的目标值为 $s'_1$ 和 $s'_2$。\n$$\ns'_1 = q_1(T) = (1-\\alpha)^T s_1 + (1-(1-\\alpha)^T)(r_1 + \\gamma s_2)\n$$\n$$\ns'_2 = q_2(T) = (1-\\alpha)^T s_2 + (1-(1-\\alpha)^T)(r_2 + \\gamma s_1)\n$$\n问题指定奖励为常数 $r_1=0$ 和 $r_2=0$。该映射简化为一个线性变换。令 $\\beta = (1-\\alpha)^T$。方程变为：\n$$\ns'_1 = \\beta s_1 + (1-\\beta)\\gamma s_2\n$$\n$$\ns'_2 = \\beta s_2 + (1-\\beta)\\gamma s_1\n$$\n以矩阵形式表示，即为 $\\mathbf{s'} = A \\mathbf{s}$，其中 $\\mathbf{s} = \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix}$，周期到周期的转移矩阵 $A$ 为：\n$$\nA = \\begin{pmatrix} \\beta   (1-\\beta)\\gamma \\\\ (1-\\beta)\\gamma   \\beta \\end{pmatrix}\n$$\n\n为确定此映射的稳定性和振荡性质，我们通过求解特征方程 $\\det(A - \\lambda I) = 0$ 来计算 $A$ 的特征值：\n$$\n\\det \\begin{pmatrix} \\beta-\\lambda   (1-\\beta)\\gamma \\\\ (1-\\beta)\\gamma   \\beta-\\lambda \\end{pmatrix} = (\\beta-\\lambda)^2 - ((1-\\beta)\\gamma)^2 = 0\n$$\n这得出了特征值 $\\lambda$ 的两个解：\n$$\n\\beta - \\lambda = \\pm (1-\\beta)\\gamma \\implies \\lambda = \\beta \\mp (1-\\beta)\\gamma\n$$\n两个特征值为：\n$$\n\\lambda_1 = \\beta + (1-\\beta)\\gamma = (1-\\alpha)^T + \\gamma(1-(1-\\alpha)^T)\n$$\n$$\n\\lambda_2 = \\beta - (1-\\beta)\\gamma = (1-\\alpha)^T - \\gamma(1-(1-\\alpha)^T)\n$$\n当系统轨迹在状态空间中某个方向上从一个周期到下一个周期反转其方向时，振荡模式就会出现。在线性系统中，这对应于一个负特征值。由于 $0  \\alpha  1$ 和 $0 \\le \\gamma  1$，我们有 $0  \\beta  1$，这意味着 $(1-\\beta)\\gamma \\ge 0$。因此，$\\lambda_2$ 总是较小的特征值。振荡模式（我们称之为`共振`）出现的条件是 $\\lambda_2  0$。\n$$\n(1-\\alpha)^T - \\gamma(1-(1-\\alpha)^T)  0\n$$\n$$\n(1-\\alpha)^T  \\gamma - \\gamma(1-\\alpha)^T\n$$\n$$\n(1+\\gamma)(1-\\alpha)^T  \\gamma\n$$\n$$\n(1-\\alpha)^T  \\frac{\\gamma}{1+\\gamma}\n$$\n如果此条件成立，系统处于`共振`状态。对应于 $\\lambda_2$ 的特征向量与 $(1, -1)^T$ 成正比，这代表了差分模式 $d(t) = q_1(t) - q_2(t)$。负特征值意味着该模式每个周期（每 $T$ 步）符号反转一次。这对应于一个周期为 $2T$ 步的振荡。这种振荡的基角频率是 $\\omega = 2\\pi / (2T) = \\pi/T$ 弧度/步。因此，预测的频率为：\n$$\n\\omega_{\\text{pred}} = \\begin{cases} \\pi/T,   \\text{if } (1-\\alpha)^T  \\frac{\\gamma}{1+\\gamma} \\\\ 0,   \\text{otherwise} \\end{cases}\n$$\n\n### 第2部分：数值模拟与测量\n\n我们现在将模拟 $N=4096$ 步的每步动态，以生成差分模式 $d(t) = q_1(t) - q_2(t)$ 的时间序列。该模拟直接实现了给定的 $q_i(t)$ 更新规则，以及当 $(t+1) \\bmod T = 0$ 时 $h_i(t+1) = q_i(t+1)$，否则 $h_i(t+1) = h_i(t)$。\n\n在模拟并存储 $q_1(t)$ 和 $q_2(t)$ 的历史记录后，我们构建 $t = 0, \\dots, N-1$ 的时间序列 $d(t)$。然后，我们计算该序列的快速傅里叶变换 (FFT) 以找到其频谱。\n测量的角频率 $\\omega_{\\text{meas}}$ 是通过在频谱中找到最大幅值的频率分量来确定的，不包括零频（直流）分量。FFT 算法提供的频率 $f$ 的单位是 周/步。它们通过关系式 $\\omega = 2\\pi f$ 转换为单位为 弧度/步 的角频率。\n\n对于每个给定的目标更新周期 $T$ 值，我们将计算三元组 $[\\omega_{\\text{pred}}, \\omega_{\\text{meas}}, \\text{resonant}]$ 并报告结果。固定参数为 $\\gamma = 0.9$，$\\alpha = 0.2$，$r_1=0$，$r_2=0$，初始条件为 $q_1(0)=1, q_2(0)=-1, h_1(0)=1, h_2(0)=-1$。\n振荡的临界阈值为 $(1-0.2)^T  \\frac{0.9}{1+0.9}$，可简化为 $0.8^T  0.9/1.9 \\approx 0.47368$。\n\n对于 $T=1, 3$：$0.8^1 = 0.8 \\not 0.47368$，$0.8^3 = 0.512 \\not 0.47368$。系统不是`共振`的。\n对于 $T=4, 20, 50$：$0.8^4 = 0.4096  0.47368$，$0.8^{20} \\approx 0.0115  0.47368$，$0.8^{50} \\approx 1.4 \\times 10^{-5}  0.47368$。系统是`共振`的。\n这些理论预测将通过模拟得到验证。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes and simulates the dynamics of a simplified DQN to find\n    and verify oscillatory behavior based on the target network update period.\n    \"\"\"\n    \n    # Define the fixed parameters from the problem statement.\n    GAMMA = 0.9\n    ALPHA = 0.2\n    R1 = 0.0\n    R2 = 0.0\n    Q1_0 = 1.0\n    Q2_0 = -1.0\n    N_STEPS = 4096\n    \n    # Define the test cases (sweep over target update period T).\n    test_cases = [1, 3, 4, 20, 50]\n\n    results = []\n\n    for T in test_cases:\n        # Part 1: Theoretical Prediction\n        \n        # The condition for oscillatory resonance is (1-alpha)^T  gamma / (1+gamma)\n        resonant_threshold = GAMMA / (1 + GAMMA)\n        is_resonant = (1 - ALPHA)**T  resonant_threshold\n        \n        if is_resonant:\n            omega_pred = np.pi / T\n        else:\n            omega_pred = 0.0\n\n        # Part 2: Simulation\n        \n        # Initialize arrays to store the time series of q and h values.\n        # q_hist stores q(0), q(1), ..., q(N)\n        q_hist = np.zeros((N_STEPS + 1, 2))\n        h_hist = np.zeros((N_STEPS + 1, 2))\n        \n        # Set initial conditions at t=0\n        q_hist[0] = [Q1_0, Q2_0]\n        h_hist[0] = [Q1_0, Q2_0]\n        \n        for t in range(N_STEPS):\n            # Current online and target values\n            q1_t, q2_t = q_hist[t]\n            h1_t, h2_t = h_hist[t]\n            \n            # Calculate next online values q(t+1)\n            q1_tp1 = q1_t + ALPHA * (R1 + GAMMA * h2_t - q1_t)\n            q2_tp1 = q2_t + ALPHA * (R2 + GAMMA * h1_t - q2_t)\n            q_hist[t+1] = [q1_tp1, q2_tp1]\n            \n            # Update target network for next step h(t+1)\n            if (t + 1) % T == 0:\n                h_hist[t+1] = [q1_tp1, q2_tp1]\n            else:\n                h_hist[t+1] = h_hist[t]\n\n        # Part 3: Measurement via FFT\n        \n        # Create the difference mode time series d(t) = q1(t) - q2(t) for t=0...N-1\n        d_series = q_hist[:N_STEPS, 0] - q_hist[:N_STEPS, 1]\n        \n        # Compute the FFT magnitude spectrum\n        fft_magnitudes = np.abs(np.fft.fft(d_series))\n        \n        # Get the corresponding frequencies in cycles per step\n        fft_frequencies = np.fft.fftfreq(N_STEPS)\n        \n        # Find the peak frequency, excluding the DC component (at index 0)\n        # We only need to check the positive frequencies (first half of the array)\n        positive_freq_range = slice(1, N_STEPS // 2)\n        \n        if np.all(fft_magnitudes[positive_freq_range] == 0):\n             peak_idx = 0 # No signal\n        else:\n            # Add 1 to peak_idx because we searched in a slice starting at index 1\n            peak_idx = np.argmax(fft_magnitudes[positive_freq_range]) + 1\n        \n        # Peak frequency in cycles per step\n        peak_freq_cycles = fft_frequencies[peak_idx]\n        \n        # Convert to angular frequency in radians per step\n        omega_meas = 2 * np.pi * peak_freq_cycles\n\n        # Store the results for this test case\n        results.append([\n            round(omega_pred, 6), \n            round(omega_meas, 6), \n            is_resonant\n        ])\n\n    # Final print statement in the exact required format.\n    # The format requires a boolean literal 'True' or 'False'\n    result_str = ','.join([\n        f\"[{res[0]},{res[1]},{str(res[2])}]\" for res in results\n    ])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "强化学习智能体会不折不扣地优化你提供的奖励函数，但如果该函数指定不当，可能导致意料之外的“奖励 hacking”行为。本练习展示了一个简单的代理奖励（proxy reward）如何被智能体利用，并介绍了基于势函数（potential-based）的奖励 shaping 作为一种有原则的方法来引导学习，同时不改变最优策略。这是设计有效且安全的强化学习智能体过程中至关重要的一课。",
            "id": "3113621",
            "problem": "考虑一个具有有限状态空间和确定性动态的马尔可夫决策过程（MDP）。设状态集为 $\\mathcal{S} = \\{s_0, s_{\\mathrm{loop}}, s_{\\mathrm{goal}}\\}$，动作集为 $\\mathcal{A} = \\{a_{\\mathrm{goal}}, a_{\\mathrm{loop}}\\}$，初始状态分布为 $\\mu$，其中 $\\mu(s_0) = 1$，$\\mu(s_{\\mathrm{loop}}) = 0$ 且 $\\mu(s_{\\mathrm{goal}}) = 0$。转移动态是确定性的，定义如下：在 $s_0$ 状态下执行动作 $a_{\\mathrm{goal}}$ 会转移到 $s_{\\mathrm{goal}}$，在 $s_0$ 状态下执行动作 $a_{\\mathrm{loop}}$ 会转移到 $s_{\\mathrm{loop}}$，在 $s_{\\mathrm{loop}}$ 状态下执行动作 $a_{\\mathrm{loop}}$ 会使智能体保持在 $s_{\\mathrm{loop}}$，在 $s_{\\mathrm{goal}}$ 状态下执行动作 $a_{\\mathrm{goal}}$ 会使智能体保持在 $s_{\\mathrm{goal}}$。\n\n在状态-动作-下一状态三元组上定义两个奖励函数，即预期奖励 $r$ 和代理奖励 $r'$，两者均为确定性函数：\n- $r(s_0, a_{\\mathrm{goal}}, s_{\\mathrm{goal}}) = 10$, $r(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$, $r(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$, and $r(s_{\\mathrm{goal}}, a_{\\mathrm{goal}}, s_{\\mathrm{goal}}) = 0$.\n- $r'(s_0, a_{\\mathrm{goal}}, s_{\\mathrm{goal}}) = 0$, $r'(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$, $r'(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 1$, and $r'(s_{\\mathrm{goal}}, a_{\\mathrm{goal}}, s_{\\mathrm{goal}}) = 0$.\n\n考虑将状态映射到动作的静态确定性策略 $\\pi$。设 $\\pi_{\\mathrm{loop}}$ 为在 $s_0$ 状态下选择 $a_{\\mathrm{loop}}$、在 $s_{\\mathrm{loop}}$ 状态下选择 $a_{\\mathrm{loop}}$、在 $s_{\\mathrm{goal}}$ 状态下选择 $a_{\\mathrm{goal}}$ 的策略。设 $\\pi_{\\mathrm{goal}}$ 为在 $s_0$ 状态下选择 $a_{\\mathrm{goal}}$、在 $s_{\\mathrm{loop}}$ 状态下选择 $a_{\\mathrm{loop}}$、在 $s_{\\mathrm{goal}}$ 状态下选择 $a_{\\mathrm{goal}}$ 的策略。\n\n对于折扣因子 $\\gamma \\in [0,1)$，在奖励函数 $r$ 和策略 $\\pi$ 下的价值函数 $V^{\\pi}$ 由 Bellman 不动点方程定义 $V^{\\pi}(s) = \\mathbb{E}[r(s, \\pi(s), s')] + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, \\pi(s)) V^{\\pi}(s')$，其中 $P(s' \\mid s, a)$ 是转移概率。从 $\\mu$ 开始的期望性能为 $J(\\pi; r) = \\sum_{s \\in \\mathcal{S}} \\mu(s) V^{\\pi}(s)$，类似地，在 $r'$ 下的期望性能为 $J(\\pi; r')$。\n\n你需要演示奖励函数破解（reward hacking）以及通过基于势函数的奖励塑造（potential-based shaping）的缓解方法。通过 $r''(s,a,s') = r'(s,a,s') + \\gamma \\Phi(s') - \\Phi(s)$ 定义一个塑造后的代理奖励 $r''$，其中 $\\Phi : \\mathcal{S} \\to \\mathbb{R}$ 是一个势函数。选择 $\\Phi$ 使得塑造后的代理奖励通过强制执行 $r''(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$ 和 $r''(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$ 来消除循环转移中的利用动机。预期目标和代理目标之间的差异度量为 $\\left| J(\\pi; r) - J(\\pi; r') \\right|$；用 $r''$ 替代 $r'$ 时也类似。\n\n仅从 MDP 的基本定义、Bellman 不动点方程和性能 $J(\\pi; r)$ 出发，编写一个程序，该程序：\n- 构建由给定策略 $\\pi$ 导出的转移矩阵，在 $r$（以及 $r'$ 和 $r''$）下的每个状态的期望即时奖励向量，使用 Bellman 方程的线性系统形式求解 $V^{\\pi}$，并为指定的初始分布 $\\mu$ 计算 $J(\\pi; r)$ 和 $J(\\pi; r')$（以及 $J(\\pi; r'')$）。\n- 为给定的 $\\gamma$ 确定一个与所述塑造约束一致的势函数 $\\Phi$，并用它来构建 $r''$。\n- 为下面的每个测试用例计算差异 $\\left| J(\\pi; r) - J(\\pi; r') \\right|$（以及 $\\left| J(\\pi; r) - J(\\pi; r'') \\right|$）。\n\n测试套件：\n- 测试用例 1（正常路径利用）：$\\gamma = 0.9$，策略 $\\pi_{\\mathrm{loop}}$，差异 $\\left| J(\\pi_{\\mathrm{loop}}; r) - J(\\pi_{\\mathrm{loop}}; r') \\right|$。\n- 测试用例 2（目标导向策略差异）：$\\gamma = 0.9$，策略 $\\pi_{\\mathrm{goal}}$，差异 $\\left| J(\\pi_{\\mathrm{goal}}; r) - J(\\pi_{\\mathrm{goal}}; r') \\right|$。\n- 测试用例 3（塑造缓解）：$\\gamma = 0.9$，策略 $\\pi_{\\mathrm{loop}}$，差异 $\\left| J(\\pi_{\\mathrm{loop}}; r) - J(\\pi_{\\mathrm{loop}}; r'') \\right|$，其中 $r''$ 是通过满足所述约束的基于势函数的奖励塑造构建的。\n- 测试用例 4（边界条件）：$\\gamma = 0.0$，策略 $\\pi_{\\mathrm{loop}}$，差异 $\\left| J(\\pi_{\\mathrm{loop}}; r) - J(\\pi_{\\mathrm{loop}}; r') \\right|$。\n- 测试用例 5（高折扣因子边缘情况）：$\\gamma = 0.99$，策略 $\\pi_{\\mathrm{loop}}$，差异 $\\left| J(\\pi_{\\mathrm{loop}}; r) - J(\\pi_{\\mathrm{loop}}; r') \\right|$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$\\left[ \\text{result}_1, \\text{result}_2, \\text{result}_3, \\text{result}_4, \\text{result}_5 \\right]$），其中每个 $\\text{result}_i$ 是一个浮点数。",
            "solution": "该问题要求对一个指定的马尔可夫决策过程（MDP）进行分析，以演示奖励函数破解（reward hacking）现象及其通过基于势函数的奖励塑造（potential-based reward shaping）的缓解方法。这涉及到计算两种不同策略在三种不同奖励函数下的期望性能：预期奖励 $r$、代理奖励 $r'$ 和塑造后的代理奖励 $r''$。\n\n此分析的基础是静态策略 $\\pi$ 的 Bellman 方程。对于一个状态 $s \\in \\mathcal{S}$，价值函数 $V^{\\pi}(s)$ 是 Bellman 算子的不动点：\n$$V^{\\pi}(s) = R^{\\pi}(s) + \\gamma \\sum_{s' \\in \\mathcal{S}} P^{\\pi}(s, s') V^{\\pi}(s')$$\n其中 $R^{\\pi}(s) = \\mathbb{E}[r(s, \\pi(s), s')]$ 是在策略 $\\pi$ 下从状态 $s$ 获得的期望即时奖励，而 $P^{\\pi}(s, s')$ 是在策略 $\\pi$ 下从状态 $s$ 转移到状态 $s'$ 的概率。鉴于本问题中的确定性动态，$P^{\\pi}(s, s')$ 对于唯一的后继状态 $s'$ 为 $1$，否则为 $0$。\n\n这个线性方程组可以用矩阵-向量形式表示。设 $V^{\\pi}$ 是状态价值的列向量，$R^{\\pi}$ 是期望即时奖励的列向量，$P^{\\pi}$ 是由策略 $\\pi$ 导出的状态转移矩阵。Bellman 方程变为：\n$$V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi} V^{\\pi}$$\n对于折扣因子 $\\gamma \\in [0, 1)$，矩阵 $(I - \\gamma P^{\\pi})$ 是可逆的，这使得价值函数有一个直接解：\n$$V^{\\pi} = (I - \\gamma P^{\\pi})^{-1} R^{\\pi}$$\n在奖励函数 $r$ 下，一个策略 $\\pi$ 的总体性能是在给定的初始状态分布 $\\mu$ 下的初始状态的期望价值：\n$$J(\\pi; r) = \\mathbb{E}_{s_0 \\sim \\mu}[V^{\\pi}(s_0)] = \\mu^T V^{\\pi}$$\n在本问题中，分布 $\\mu$ 集中在 $s_0$ 上，即 $\\mu(s_0) = 1$。因此，$J(\\pi; r) = V^{\\pi}(s_0)$。\n\n我们将使用数值表示来代表状态：$s_0 \\to 0$，$s_{\\mathrm{loop}} \\to 1$，$s_{\\mathrm{goal}} \\to 2$。初始状态分布向量为 $\\mu = [1, 0, 0]^T$。\n\n首先，我们为给定的策略 $\\pi_{\\mathrm{loop}}$ 和 $\\pi_{\\mathrm{goal}}$ 构建转移矩阵和奖励向量。\n对于 $\\pi_{\\mathrm{loop}} = \\{ s_0 \\mapsto a_{\\mathrm{loop}}, s_{\\mathrm{loop}} \\mapsto a_{\\mathrm{loop}}, s_{\\mathrm{goal}} \\mapsto a_{\\mathrm{goal}} \\}$：\n转移是 $s_0 \\to s_{\\mathrm{loop}}$，$s_{\\mathrm{loop}} \\to s_{\\mathrm{loop}}$ 和 $s_{\\mathrm{goal}} \\to s_{\\mathrm{goal}}$。相应的转移矩阵和奖励向量为：\n$$P^{\\pi_{\\mathrm{loop}}} = \\begin{pmatrix} 0  1  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}, \\quad R^{\\pi_{\\mathrm{loop}}}(r) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad R^{\\pi_{\\mathrm{loop}}}(r') = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\n对于 $\\pi_{\\mathrm{goal}} = \\{ s_0 \\mapsto a_{\\mathrm{goal}}, s_{\\mathrm{loop}} \\mapsto a_{\\mathrm{loop}}, s_{\\mathrm{goal}} \\mapsto a_{\\mathrm{goal}} \\}$：\n转移是 $s_0 \\to s_{\\mathrm{goal}}$，$s_{\\mathrm{loop}} \\to s_{\\mathrm{loop}}$ 和 $s_{\\mathrm{goal}} \\to s_{\\mathrm{goal}}$。\n$$P^{\\pi_{\\mathrm{goal}}} = \\begin{pmatrix} 0  0  1 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}, \\quad R^{\\pi_{\\mathrm{goal}}}(r) = \\begin{pmatrix} 10 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad R^{\\pi_{\\mathrm{goal}}}(r') = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\n\n第三个奖励函数 $r''$ 是 $r'$ 的一个基于势函数的塑造：$r''(s,a,s') = r'(s,a,s') + \\gamma \\Phi(s') - \\Phi(s)$。势函数 $\\Phi: \\mathcal{S} \\to \\mathbb{R}$ 必须满足两个约束：$r''(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$ 和 $r''(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = 0$。\n第一个约束给出：\n$r''(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = r'(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) + \\gamma \\Phi(s_{\\mathrm{loop}}) - \\Phi(s_{\\mathrm{loop}}) = 0$\n$1 + (\\gamma - 1)\\Phi(s_{\\mathrm{loop}}) = 0 \\implies \\Phi(s_{\\mathrm{loop}}) = \\frac{1}{1-\\gamma}$\n第二个约束给出：\n$r''(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) = r'(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) + \\gamma \\Phi(s_{\\mathrm{loop}}) - \\Phi(s_0) = 0$\n$0 + \\gamma \\left(\\frac{1}{1-\\gamma}\\right) - \\Phi(s_0) = 0 \\implies \\Phi(s_0) = \\frac{\\gamma}{1-\\gamma}$\n目标状态的势函数 $\\Phi(s_{\\mathrm{goal}})$ 是无约束的。为简单起见，我们设置 $\\Phi(s_{\\mathrm{goal}}) = 0$。\n然后构建在策略 $\\pi_{\\mathrm{loop}}$ 下，塑造后的奖励 $r''$ 的奖励向量 $R^{\\pi_{\\mathrm{loop}}}(r'')$。\n- 对于 $s_0$：$R^{\\pi_{\\mathrm{loop}}}(s_0, r'') = r'(s_0, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) + \\gamma \\Phi(s_{\\mathrm{loop}}) - \\Phi(s_0) = 0 + \\gamma(\\frac{1}{1-\\gamma}) - \\frac{\\gamma}{1-\\gamma} = 0$。\n- 对于 $s_{\\mathrm{loop}}$：$R^{\\pi_{\\mathrm{loop}}}(s_{\\mathrm{loop}}, r'') = r'(s_{\\mathrm{loop}}, a_{\\mathrm{loop}}, s_{\\mathrm{loop}}) + \\gamma \\Phi(s_{\\mathrm{loop}}) - \\Phi(s_{\\mathrm{loop}}) = 1 + (\\gamma-1)(\\frac{1}{1-\\gamma}) = 0$。\n- 对于 $s_{\\mathrm{goal}}$：$R^{\\pi_{\\mathrm{loop}}}(s_{\\mathrm{goal}}, r'') = r'(s_{\\mathrm{goal}}, a_{\\mathrm{goal}}, s_{\\mathrm{goal}}) + \\gamma \\Phi(s_{\\mathrm{goal}}) - \\Phi(s_{\\mathrm{goal}}) = 0 + (\\gamma-1)(0) = 0$。\n因此，$R^{\\pi_{\\mathrm{loop}}}(r'') = [0, 0, 0]^T$。值得注意的是，这与 $R^{\\pi_{\\mathrm{loop}}}(r)$ 相同。\n\n程序将为每个测试用例实现此过程。它计算所需的转移矩阵和奖励向量，求解价值函数 $V^{\\pi}(r)$、$V^{\\pi}(r')$ 和（在适用情况下）$V^{\\pi}(r'')$，然后将初始状态 $s_0$ 的价值作为性能 $J(\\pi; \\cdot)$ 进行计算。最后一步是计算预期奖励与代理/塑造后奖励下性能的绝对差值。\n\n例如，考虑测试用例 1（$\\gamma = 0.9$，$\\pi_{\\mathrm{loop}}$）。\n$V^{\\pi_{\\mathrm{loop}}}(r) = (I - 0.9 P^{\\pi_{\\mathrm{loop}}})^{-1} R^{\\pi_{\\mathrm{loop}}}(r) = (I - 0.9 P^{\\pi_{\\mathrm{loop}}})^{-1} [0, 0, 0]^T = [0, 0, 0]^T$。\n因此，$J(\\pi_{\\mathrm{loop}}; r) = V^{\\pi_{\\mathrm{loop}}}(s_0, r) = 0$。\n对于代理奖励 $r'$：\n$V^{\\pi_{\\mathrm{loop}}}(r') = \\begin{pmatrix} 1  -0.9  0 \\\\ 0  0.1  0 \\\\ 0  0  0.1 \\end{pmatrix}^{-1} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1  9  0 \\\\ 0  10  0 \\\\ 0  0  10 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 9 \\\\ 10 \\\\ 0 \\end{pmatrix}$。\n因此，$J(\\pi_{\\mathrm{loop}}; r') = V^{\\pi_{\\mathrm{loop}}}(s_0, r') = 9$。\n差异为 $|0 - 9| = 9$。策略 $\\pi_{\\mathrm{loop}}$ 获得的真实奖励为 $0$，但被代理奖励激励进入循环，从而在 $r'$ 下获得 $9$ 的高分，这演示了奖励函数破解。\n\n其余的测试用例通过应用相同的计算过程及各自的参数来解决。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the MDP problem for all test cases as specified.\n    \"\"\"\n    # State and action mappings to indices\n    S_MAP = {'s0': 0, 's_loop': 1, 's_goal': 2}\n    A_MAP = {'a_goal': 0, 'a_loop': 1}\n    NUM_STATES = 3\n\n    # Define deterministic transition dynamics T(s, a) -> s'\n    transitions = {\n        (S_MAP['s0'], A_MAP['a_goal']): S_MAP['s_goal'],\n        (S_MAP['s0'], A_MAP['a_loop']): S_MAP['s_loop'],\n        (S_MAP['s_loop'], A_MAP['a_loop']): S_MAP['s_loop'],\n        (S_MAP['s_goal'], A_MAP['a_goal']): S_MAP['s_goal'],\n    }\n\n    # Define deterministic reward functions r(s, a, s')\n    rewards_r = {\n        (S_MAP['s0'], A_MAP['a_goal'], S_MAP['s_goal']): 10.0,\n    }\n    rewards_rp = {\n        (S_MAP['s_loop'], A_MAP['a_loop'], S_MAP['s_loop']): 1.0,\n    }\n\n    # Define policies pi(s) -> a\n    pi_loop = {\n        S_MAP['s0']: A_MAP['a_loop'],\n        S_MAP['s_loop']: A_MAP['a_loop'],\n        S_MAP['s_goal']: A_MAP['a_goal'],\n    }\n    pi_goal = {\n        S_MAP['s0']: A_MAP['a_goal'],\n        S_MAP['s_loop']: A_MAP['a_loop'],\n        S_MAP['s_goal']: A_MAP['a_goal'],\n    }\n    policies = {'loop': pi_loop, 'goal': pi_goal}\n\n    # Start state distribution mu(s)\n    mu = np.zeros(NUM_STATES)\n    mu[S_MAP['s0']] = 1.0\n\n    def compute_performance(gamma, pi, R_pi):\n        \"\"\"\n        Computes the performance J(pi) = mu^T * V^pi.\n        V^pi is solved via the matrix inversion form of the Bellman equation.\n        \"\"\"\n        P_pi = np.zeros((NUM_STATES, NUM_STATES))\n        for s in range(NUM_STATES):\n            a = pi[s]\n            s_prime = transitions[(s, a)]\n            P_pi[s, s_prime] = 1.0\n\n        I = np.identity(NUM_STATES)\n        \n        # Handle gamma=0 case separately to avoid linalg with zero matrix\n        if gamma == 0.0:\n            V_pi = R_pi\n        else:\n            # V_pi = (I - gamma * P_pi)^-1 * R_pi\n            try:\n                inv_matrix = np.linalg.inv(I - gamma * P_pi)\n                V_pi = inv_matrix @ R_pi\n            except np.linalg.LinAlgError:\n                # Should not happen for gamma in [0, 1)\n                return float('nan')\n        \n        # J(pi) = sum_s mu(s) * V^pi(s)\n        J = mu @ V_pi\n        return J\n\n    def calculate_divergence(gamma, policy_name, divergence_type):\n        \"\"\"\n        Calculates the divergence for a given test case configuration.\n        \"\"\"\n        pi = policies[policy_name]\n\n        # Construct per-state reward vectors R_pi for r and r'\n        R_pi_r = np.zeros(NUM_STATES)\n        R_pi_rp = np.zeros(NUM_STATES)\n        for s in range(NUM_STATES):\n            a = pi[s]\n            s_prime = transitions[(s, a)]\n            R_pi_r[s] = rewards_r.get((s, a, s_prime), 0.0)\n            R_pi_rp[s] = rewards_rp.get((s, a, s_prime), 0.0)\n\n        # Compute performance under the intended reward 'r'\n        J_r = compute_performance(gamma, pi, R_pi_r)\n\n        if divergence_type == 'proxy':\n            # Compute performance under the proxy reward 'r_prime'\n            J_rp = compute_performance(gamma, pi, R_pi_rp)\n            return abs(J_r - J_rp)\n        \n        elif divergence_type == 'shaped':\n            # This case is only for pi_loop as per problem statement\n            # Determine potential function Phi from constraints\n            # Phi(s_loop) = 1 / (1-gamma)\n            # Phi(s0) = gamma * Phi(s_loop)\n            # Phi(s_goal) = 0 (unconstrained, set to 0 for simplicity)\n            if gamma  1.0:\n                phi_s_loop = 1.0 / (1.0 - gamma)\n                phi_s0 = gamma * phi_s_loop\n            else: # Should not be reached with gamma  1\n                phi_s_loop = float('inf')\n                phi_s0 = float('inf')\n\n            Phi = np.array([phi_s0, phi_s_loop, 0.0])\n\n            # Construct shaped reward vector R_pi_rpp\n            R_pi_rpp = np.zeros(NUM_STATES)\n            for s in range(NUM_STATES):\n                a = pi[s]\n                s_prime = transitions[(s, a)]\n                r_prime_val = rewards_rp.get((s, a, s_prime), 0.0)\n                R_pi_rpp[s] = r_prime_val + gamma * Phi[s_prime] - Phi[s]\n            \n            # Compute performance under the shaped reward 'r_double_prime'\n            J_rpp = compute_performance(gamma, pi, R_pi_rpp)\n            return abs(J_r - J_rpp)\n            \n        return float('nan')\n\n    test_cases = [\n        # (gamma, policy_name, divergence_type)\n        (0.9, 'loop', 'proxy'),\n        (0.9, 'goal', 'proxy'),\n        (0.9, 'loop', 'shaped'),\n        (0.0, 'loop', 'proxy'),\n        (0.99, 'loop', 'proxy'),\n    ]\n\n    results = []\n    for gamma, policy_name, divergence_type in test_cases:\n        result = calculate_divergence(gamma, policy_name, divergence_type)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.1f}' for r in results)}]\")\n\nsolve()\n\n```"
        }
    ]
}