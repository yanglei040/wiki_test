{
    "hands_on_practices": [
        {
            "introduction": "The basic REINFORCE algorithm often suffers from high variance, making learning slow and unstable. This exercise explores a powerful variance reduction technique called antithetic sampling, which leverages symmetry in the action sampling process to create more reliable gradient estimates. By working through this problem , you will quantify the performance gain and build an intuition for why such methods are crucial in practice.",
            "id": "3158001",
            "problem": "Consider a one-dimensional stochastic bandit with action $a \\in \\mathbb{R}$ drawn from a symmetric Gaussian policy $\\pi_{\\theta}(a) = \\mathcal{N}(\\theta, \\sigma^{2})$, where the variance $\\sigma^{2} > 0$ is fixed and only the mean $\\theta \\in \\mathbb{R}$ is the trainable parameter. The reward is linear in the action, $r(a) = k a$, where $k \\in \\mathbb{R}$ is a nonzero constant. The objective is $J(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[r(a)]$. Use the likelihood-ratio (score-function) identity $\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta}}[r(a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a)]$ as the fundamental starting point.\n\nDefine the following unbiased gradient estimators for $\\nabla_{\\theta} J(\\theta)$:\n- A two-sample independent-and-identically-distributed averaged estimator: draw $a_{1}, a_{2} \\stackrel{\\text{i.i.d.}}{\\sim} \\pi_{\\theta}$ and set $\\hat{g}_{\\text{iid}} = \\frac{1}{2}\\big[r(a_{1}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{1}) + r(a_{2}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{2})\\big]$.\n- An antithetic-pair averaged estimator: draw $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$, set $a_{+} = \\theta + \\varepsilon$ and $a_{-} = \\theta - \\varepsilon$, and define $\\hat{g}_{\\text{anti}} = \\frac{1}{2}\\big[r(a_{+}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{+}) + r(a_{-}) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a_{-})\\big]$.\n\nStarting from the definition of $\\nabla_{\\theta} \\ln \\pi_{\\theta}(a)$ for a univariate Gaussian with fixed variance and basic moment identities for a Gaussian random variable, derive closed-form expressions for $\\operatorname{Var}[\\hat{g}_{\\text{iid}}]$ and $\\operatorname{Var}[\\hat{g}_{\\text{anti}}]$. Then, quantify the variance reduction achieved by antithetic sampling under an equal reward-evaluation budget by providing the closed-form analytic expression for the variance-reduction factor\n$$V(\\theta, \\sigma) := \\frac{\\operatorname{Var}[\\hat{g}_{\\text{iid}}]}{\\operatorname{Var}[\\hat{g}_{\\text{anti}}]}.$$\n\nExpress your final result as a single simplified expression in terms of $\\theta$ and $\\sigma$. No numerical rounding is required.",
            "solution": "First, we establish some preliminary quantities. The probability density function for the policy $\\pi_{\\theta}(a) = \\mathcal{N}(\\theta, \\sigma^2)$ is\n$$p(a; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(a-\\theta)^2}{2\\sigma^2}\\right).$$\nThe log-policy is $\\ln \\pi_{\\theta}(a) = -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(a-\\theta)^2}{2\\sigma^2}$. The score function, which is the gradient of the log-policy with respect to the parameter $\\theta$, is\n$$ \\nabla_{\\theta} \\ln \\pi_{\\theta}(a) = \\frac{\\partial}{\\partial\\theta}\\left(-\\frac{(a-\\theta)^2}{2\\sigma^2}\\right) = -\\frac{2(a-\\theta)(-1)}{2\\sigma^2} = \\frac{a-\\theta}{\\sigma^2}. $$\nLet's define a single-sample gradient estimate as $\\hat{g}(a) = r(a) \\nabla_{\\theta} \\ln \\pi_{\\theta}(a)$. Using the given reward and the derived score function, we have\n$$ \\hat{g}(a) = (ka) \\left(\\frac{a-\\theta}{\\sigma^2}\\right) = \\frac{k}{\\sigma^2} a(a-\\theta). $$\nThe true gradient $\\nabla_{\\theta} J(\\theta)$ is the expectation of this estimator. Let $a \\sim \\mathcal{N}(\\theta, \\sigma^2)$. We can write $a = \\theta + z$, where $z \\sim \\mathcal{N}(0, \\sigma^2)$.\n$$ \\nabla_{\\theta} J(\\theta) = \\mathbb{E}[\\hat{g}(a)] = \\mathbb{E}\\left[\\frac{k}{\\sigma^2} a(a-\\theta)\\right] = \\frac{k}{\\sigma^2}\\mathbb{E}[(\\theta+z)(z)] = \\frac{k}{\\sigma^2}\\mathbb{E}[\\theta z + z^2]. $$\nUsing the linearity of expectation and noting that for $z \\sim \\mathcal{N}(0, \\sigma^2)$, we have $\\mathbb{E}[z]=0$ and $\\mathbb{E}[z^2]=\\sigma^2$, we get\n$$ \\nabla_{\\theta} J(\\theta) = \\frac{k}{\\sigma^2}(\\theta \\mathbb{E}[z] + \\mathbb{E}[z^2]) = \\frac{k}{\\sigma^2}(0 + \\sigma^2) = k. $$\nThis confirms that $\\hat{g}(a)$ is an unbiased estimator of the true gradient $k$.\n\nNow, we derive the variance of the two estimators.\n\n**1. Variance of the I.I.D. Estimator, $\\operatorname{Var}[\\hat{g}_{\\text{iid}}]$**\n\nThe estimator $\\hat{g}_{\\text{iid}}$ is the average of two i.i.d. random variables, $\\hat{g}(a_1)$ and $\\hat{g}(a_2)$. Due to independence, the variance is\n$$ \\operatorname{Var}[\\hat{g}_{\\text{iid}}] = \\operatorname{Var}\\left[\\frac{1}{2}(\\hat{g}(a_1) + \\hat{g}(a_2))\\right] = \\frac{1}{4}(\\operatorname{Var}[\\hat{g}(a_1)] + \\operatorname{Var}[\\hat{g}(a_2)]) = \\frac{1}{2}\\operatorname{Var}[\\hat{g}(a)]. $$\nWe need to compute $\\operatorname{Var}[\\hat{g}(a)] = \\mathbb{E}[\\hat{g}(a)^2] - (\\mathbb{E}[\\hat{g}(a)])^2$. We already know $\\mathbb{E}[\\hat{g}(a)] = k$.\nLet's compute the second moment $\\mathbb{E}[\\hat{g}(a)^2]$:\n$$ \\mathbb{E}[\\hat{g}(a)^2] = \\mathbb{E}\\left[\\left(\\frac{k}{\\sigma^2} a(a-\\theta)\\right)^2\\right] = \\frac{k^2}{\\sigma^4} \\mathbb{E}[a^2(a-\\theta)^2]. $$\nSubstitute $a = \\theta + z$ where $z \\sim \\mathcal{N}(0, \\sigma^2)$:\n$$ \\mathbb{E}[a^2(a-\\theta)^2] = \\mathbb{E}[(\\theta+z)^2 z^2] = \\mathbb{E}[(\\theta^2+2\\theta z+z^2)z^2] = \\mathbb{E}[\\theta^2 z^2 + 2\\theta z^3 + z^4]. $$\nUsing the moments of a centered Gaussian, $\\mathbb{E}[z^2]=\\sigma^2$, $\\mathbb{E}[z^3]=0$ (by symmetry), and $\\mathbb{E}[z^4]=3\\sigma^4$:\n$$ \\mathbb{E}[a^2(a-\\theta)^2] = \\theta^2\\mathbb{E}[z^2] + 2\\theta\\mathbb{E}[z^3] + \\mathbb{E}[z^4] = \\theta^2\\sigma^2 + 0 + 3\\sigma^4 = \\sigma^2(\\theta^2 + 3\\sigma^2). $$\nSo, the second moment of $\\hat{g}(a)$ is\n$$ \\mathbb{E}[\\hat{g}(a)^2] = \\frac{k^2}{\\sigma^4}\\sigma^2(\\theta^2 + 3\\sigma^2) = k^2\\left(\\frac{\\theta^2}{\\sigma^2} + 3\\right). $$\nThen, the variance of a single-sample estimator is\n$$ \\operatorname{Var}[\\hat{g}(a)] = \\mathbb{E}[\\hat{g}(a)^2] - (\\mathbb{E}[\\hat{g}(a)])^2 = k^2\\left(\\frac{\\theta^2}{\\sigma^2} + 3\\right) - k^2 = k^2\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right). $$\nFinally, the variance of the two-sample i.i.d. estimator is\n$$ \\operatorname{Var}[\\hat{g}_{\\text{iid}}] = \\frac{1}{2}\\operatorname{Var}[\\hat{g}(a)] = \\frac{k^2}{2}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right). $$\n\n**2. Variance of the Antithetic Estimator, $\\operatorname{Var}[\\hat{g}_{\\text{anti}}]$**\n\nThe antithetic estimator uses a correlated pair of samples. Let's first express the estimator in terms of the random variable $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\nThe two components of the estimator are $\\hat{g}(a_{+})$ and $\\hat{g}(a_{-})$.\nFor $a_{+} = \\theta + \\varepsilon$:\n$$ \\hat{g}(a_{+}) = r(a_{+}) \\nabla_{\\theta}\\ln\\pi_{\\theta}(a_{+}) = k(\\theta+\\varepsilon) \\left(\\frac{(\\theta+\\varepsilon)-\\theta}{\\sigma^2}\\right) = \\frac{k}{\\sigma^2}(\\theta+\\varepsilon)\\varepsilon = \\frac{k}{\\sigma^2}(\\theta\\varepsilon + \\varepsilon^2). $$\nFor $a_{-} = \\theta - \\varepsilon$:\n$$ \\hat{g}(a_{-}) = r(a_{-}) \\nabla_{\\theta}\\ln\\pi_{\\theta}(a_{-}) = k(\\theta-\\varepsilon) \\left(\\frac{(\\theta-\\varepsilon)-\\theta}{\\sigma^2}\\right) = \\frac{k}{\\sigma^2}(\\theta-\\varepsilon)(-\\varepsilon) = \\frac{k}{\\sigma^2}(-\\theta\\varepsilon + \\varepsilon^2). $$\nThe antithetic estimator is the average of these two components:\n$$ \\hat{g}_{\\text{anti}} = \\frac{1}{2}(\\hat{g}(a_{+}) + \\hat{g}(a_{-})) = \\frac{1}{2} \\left[ \\frac{k}{\\sigma^2}(\\theta\\varepsilon + \\varepsilon^2) + \\frac{k}{\\sigma^2}(-\\theta\\varepsilon + \\varepsilon^2) \\right] = \\frac{1}{2} \\frac{k}{\\sigma^2} (2\\varepsilon^2) = \\frac{k}{\\sigma^2}\\varepsilon^2. $$\nThe term dependent on $\\theta$ has been cancelled out. Now we can compute the variance of this simplified expression.\n$$ \\operatorname{Var}[\\hat{g}_{\\text{anti}}] = \\operatorname{Var}\\left[\\frac{k}{\\sigma^2}\\varepsilon^2\\right] = \\left(\\frac{k}{\\sigma^2}\\right)^2 \\operatorname{Var}[\\varepsilon^2] = \\frac{k^2}{\\sigma^4}\\operatorname{Var}[\\varepsilon^2]. $$\nThe variance of $\\varepsilon^2$ is $\\operatorname{Var}[\\varepsilon^2] = \\mathbb{E}[(\\varepsilon^2)^2] - (\\mathbb{E}[\\varepsilon^2])^2 = \\mathbb{E}[\\varepsilon^4] - (\\mathbb{E}[\\varepsilon^2])^2$.\nUsing the moments for $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, $\\mathbb{E}[\\varepsilon^2] = \\sigma^2$ and $\\mathbb{E}[\\varepsilon^4] = 3\\sigma^4$, we have\n$$ \\operatorname{Var}[\\varepsilon^2] = 3\\sigma^4 - (\\sigma^2)^2 = 2\\sigma^4. $$\nSubstituting this back, we find the variance of the antithetic estimator:\n$$ \\operatorname{Var}[\\hat{g}_{\\text{anti}}] = \\frac{k^2}{\\sigma^4}(2\\sigma^4) = 2k^2. $$\n\n**3. Variance Reduction Factor, $V(\\theta, \\sigma)$**\n\nFinally, we compute the ratio of the two variances.\n$$ V(\\theta, \\sigma) = \\frac{\\operatorname{Var}[\\hat{g}_{\\text{iid}}]}{\\operatorname{Var}[\\hat{g}_{\\text{anti}}]} = \\frac{\\frac{k^2}{2}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right)}{2k^2}. $$\nSince $k \\neq 0$, we can cancel the $k^2$ terms:\n$$ V(\\theta, \\sigma) = \\frac{\\frac{1}{2}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right)}{2} = \\frac{1}{4}\\left(\\frac{\\theta^2}{\\sigma^2} + 2\\right) = \\frac{\\theta^2}{4\\sigma^2} + \\frac{2}{4} = \\frac{\\theta^2}{4\\sigma^2} + \\frac{1}{2}. $$\nTo write this as a single simplified expression, we can use a common denominator:\n$$ V(\\theta, \\sigma) = \\frac{\\theta^2 + 2\\sigma^2}{4\\sigma^2}. $$\nThis is the closed-form analytic expression for the variance-reduction factor.",
            "answer": "$$\\boxed{\\frac{\\theta^2 + 2\\sigma^2}{4\\sigma^2}}$$"
        },
        {
            "introduction": "In many real-world applications, not all actions are available in every state. This practice  addresses the common challenge of action masking, where the policy must adapt to changing sets of legal actions. You will derive the score function for a masked softmax policy, a fundamental skill for implementing policy gradient agents in environments with dynamic constraints, such as games or resource allocation problems.",
            "id": "3158020",
            "problem": "Consider a discrete-action policy in Reinforcement Learning (RL) that uses action masking to forbid illegal actions in a given state. Let the action set be $\\{1,2,3,4\\}$ and let the mask be $m \\in \\{0,1\\}^{4}$, where $m_{i} = 1$ indicates that action $i$ is legal and $m_{i} = 0$ indicates that action $i$ is illegal. The policy is parameterized by a vector of logits $\\theta \\in \\mathbb{R}^{4}$, and the probability assigned to action $a$ in state $s$ under mask $m$ is defined by\n$$\n\\pi(a \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} ,\n$$\nwhich enforces zero probability for illegal actions and normalizes only over legal actions. Assume the chosen action $a$ is legal, so that $\\pi(a \\mid s, m, \\theta) > 0$.\n\nStarting from the definition of the score function and basic rules of differentiation, derive the gradient $\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)$ for a legal chosen action $a$ under arbitrary mask $m$, taking care to justify why any components corresponding to illegal actions vanish. Then, evaluate this gradient at\n$$\n\\theta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad m = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad a = 3 .\n$$\nYour final answer must be the resulting gradient vector expressed exactly in analytic form as a single row matrix. Do not approximate; no rounding is required. No physical units are involved.",
            "solution": "The primary objective is to derive the gradient of the log-policy, also known as the score function. The policy is given by:\n$$\n\\pi(a \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}}\n$$\nTo find the gradient $\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)$, we first compute the logarithm of the policy probability.\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\ln \\left( \\frac{\\exp(\\theta_{a}) \\, m_{a}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} \\right)\n$$\nUsing the property of logarithms $\\ln(x/y) = \\ln(x) - \\ln(y)$, we get:\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\ln(\\exp(\\theta_{a}) \\, m_{a}) - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\nThe problem states that the chosen action $a$ is legal, which means $m_a = 1$. This simplifies the first term: $\\ln(\\exp(\\theta_{a}) \\cdot 1) = \\theta_a$.\n$$\n\\ln \\pi(a \\mid s, m, \\theta) = \\theta_a - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\nNow, we compute the gradient of this expression with respect to the parameter vector $\\theta$. The gradient is a vector whose $j$-th component is the partial derivative with respect to $\\theta_j$.\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_j = \\frac{\\partial}{\\partial \\theta_j} \\left( \\theta_a - \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) \\right)\n$$\nWe differentiate term by term:\n$$\n\\frac{\\partial}{\\partial \\theta_j} (\\theta_a) = \\delta_{aj}\n$$\nwhere $\\delta_{aj}$ is the Kronecker delta, which is $1$ if $j=a$ and $0$ otherwise.\n\nFor the second term, we use the chain rule for differentiation ($\\frac{d}{dx}\\ln(f(x)) = \\frac{f'(x)}{f(x)}$):\n$$\n\\frac{\\partial}{\\partial \\theta_j} \\ln\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) = \\frac{1}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} \\cdot \\frac{\\partial}{\\partial \\theta_j}\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right)\n$$\nThe derivative of the sum with respect to $\\theta_j$ is:\n$$\n\\frac{\\partial}{\\partial \\theta_j}\\left(\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}\\right) = \\exp(\\theta_j) \\, m_j\n$$\nCombining these, the derivative of the second term is:\n$$\n\\frac{\\exp(\\theta_j) \\, m_j}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}}\n$$\nThis expression is exactly the definition of the policy probability for action $j$, $\\pi(j \\mid s, m, \\theta)$.\n\nSo, the $j$-th component of the gradient is:\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_j = \\delta_{aj} - \\pi(j \\mid s, m, \\theta)\n$$\nThis is the general form of the score function for a masked softmax policy.\n\nThe problem asks to justify why components corresponding to illegal actions vanish. Let $k$ be an index for an illegal action, meaning $m_k=0$. The $k$-th component of the gradient is:\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_k = \\delta_{ak} - \\pi(k \\mid s, m, \\theta)\n$$\nSince $a$ is a legal action ($m_a=1$) and $k$ is an illegal action ($m_k=0$), we must have $a \\neq k$. Therefore, $\\delta_{ak} = 0$.\nThe probability of the illegal action $k$ is:\n$$\n\\pi(k \\mid s, m, \\theta) = \\frac{\\exp(\\theta_{k}) \\, m_{k}}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} = \\frac{\\exp(\\theta_{k}) \\cdot 0}{\\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}} = 0\n$$\nThus, the $k$-th component of the gradient becomes:\n$$\n\\left[\\nabla_{\\theta} \\ln \\pi(a \\mid s, m, \\theta)\\right]_k = 0 - 0 = 0\n$$\nThis confirms that the gradient components for all illegal actions are zero.\n\nNow we evaluate the gradient for the given values:\n$$\n\\theta = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad m = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad a = 3\n$$\nFirst, we compute the normalization term, $Z = \\sum_{i=1}^{4} \\exp(\\theta_{i}) \\, m_{i}$:\n$$\nZ = \\exp(\\theta_1)m_1 + \\exp(\\theta_2)m_2 + \\exp(\\theta_3)m_3 + \\exp(\\theta_4)m_4\n$$\n$$\nZ = \\exp(0) \\cdot 1 + \\exp(1) \\cdot 0 + \\exp(2) \\cdot 1 + \\exp(-1) \\cdot 1\n$$\n$$\nZ = 1 \\cdot 1 + 0 + e^2 \\cdot 1 + e^{-1} \\cdot 1 = 1 + e^2 + e^{-1}\n$$\nNext, we compute the probability vector $\\vec{\\pi}$ whose components are $\\pi(j \\mid s, m, \\theta)$:\n$$\n\\pi(1) = \\frac{\\exp(0) \\cdot 1}{Z} = \\frac{1}{Z}\n$$\n$$\n\\pi(2) = \\frac{\\exp(1) \\cdot 0}{Z} = 0\n$$\n$$\n\\pi(3) = \\frac{\\exp(2) \\cdot 1}{Z} = \\frac{e^2}{Z}\n$$\n$$\n\\pi(4) = \\frac{\\exp(-1) \\cdot 1}{Z} = \\frac{e^{-1}}{Z}\n$$\nThe gradient vector is given by $\\nabla_{\\theta} \\ln \\pi(a=3 \\mid \\dots) = \\mathbf{e}_3 - \\vec{\\pi}$, where $\\mathbf{e}_3$ is the one-hot vector $(0, 0, 1, 0)^T$.\n$$\n\\nabla_{\\theta} \\ln \\pi = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} \\pi(1) \\\\ \\pi(2) \\\\ \\pi(3) \\\\ \\pi(4) \\end{pmatrix} = \\begin{pmatrix} -\\pi(1) \\\\ -\\pi(2) \\\\ 1 - \\pi(3) \\\\ -\\pi(4) \\end{pmatrix}\n$$\nSubstituting the probabilities:\n$$\n\\nabla_{\\theta} \\ln \\pi = \\begin{pmatrix} -1/Z \\\\ 0 \\\\ 1 - e^2/Z \\\\ -e^{-1}/Z \\end{pmatrix} = \\begin{pmatrix} -1/Z \\\\ 0 \\\\ (Z - e^2)/Z \\\\ -e^{-1}/Z \\end{pmatrix}\n$$\nSubstitute $Z = 1 + e^2 + e^{-1}$:\n$$\nZ - e^2 = (1 + e^2 + e^{-1}) - e^2 = 1 + e^{-1}\n$$\nSo the gradient vector is:\n$$\n\\nabla_{\\theta} \\ln \\pi = \\frac{1}{1 + e^2 + e^{-1}} \\begin{pmatrix} -1 \\\\ 0 \\\\ 1+e^{-1} \\\\ -e^{-1} \\end{pmatrix}\n$$\nTo simplify, we multiply the numerator and denominator by $e$:\n$$\n\\nabla_{\\theta} \\ln \\pi = \\frac{e}{e(1 + e^2 + e^{-1})} \\begin{pmatrix} -1 \\\\ 0 \\\\ 1+e^{-1} \\\\ -e^{-1} \\end{pmatrix} = \\frac{e}{e + e^3 + 1} \\begin{pmatrix} -1 \\\\ 0 \\\\ 1+e^{-1} \\\\ -e^{-1} \\end{pmatrix} = \\frac{1}{1+e+e^3} \\begin{pmatrix} -e \\\\ 0 \\\\ e(1+e^{-1}) \\\\ e(-e^{-1}) \\end{pmatrix} = \\frac{1}{1+e+e^3} \\begin{pmatrix} -e \\\\ 0 \\\\ e+1 \\\\ -1 \\end{pmatrix}\n$$\nExpressed as a row matrix, this is:\n$$\n\\begin{pmatrix} -\\frac{e}{e^3+e+1} & 0 & \\frac{e+1}{e^3+e+1} & -\\frac{1}{e^3+e+1} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{e}{e^3+e+1} & 0 & \\frac{e+1}{e^3+e+1} & -\\frac{1}{e^3+e+1} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Beyond score-function methods, zeroth-order or 'black-box' optimization offers an alternative for policy gradients, especially when the policy is non-differentiable. This problem  delves into Evolution Strategies (ES), a type of zeroth-order method, and asks you to analyze the bias introduced by using correlated perturbations. This advanced exercise will sharpen your analytical skills by connecting the properties of a gradient estimator back to the underlying dynamics of the environment.",
            "id": "3157968",
            "problem": "Consider a finite-horizon linear Markov Decision Process (MDP) with horizon $T=2$ and scalar state-action dynamics. The state space and action space are both the real line. The dynamics are given by $s_{t+1} = \\alpha s_{t} + \\beta a_{t}$ for $t \\in \\{0,1\\}$, with a known initial state $s_{0} \\in \\mathbb{R}$. The step reward is $r_{t} = c_{s} s_{t} + c_{a} a_{t}$, and the total return is $R = r_{0} + r_{1}$. The policy is time-indexed and Gaussian: at time $t \\in \\{0,1\\}$, the action $a_{t}$ is drawn from a normal distribution with mean $\\theta_{t} s_{t}$ and variance $\\sigma^{2}$, that is $a_{t} \\sim \\mathcal{N}(\\theta_{t} s_{t}, \\sigma^{2})$. Let the policy parameters be $\\theta = (\\theta_{0}, \\theta_{1})$. Define the expected return under the policy and dynamics as $J(\\theta) = \\mathbb{E}[R]$, where the expectation is taken over the action distributions induced by the policy.\n\nTasks:\n- Derive $J(\\theta)$ from first principles, using only the definitions of the dynamics, reward, and linearity of expectation.\n- Compute the exact gradient $\\nabla_{\\theta} J(\\theta)$.\n- Define a central finite-difference evolution strategies estimator with antithetic pairs using correlated perturbations. Let $u = (u_{0}, u_{1})^{\\top}$ be a jointly Gaussian random vector with zero mean and covariance matrix $\\Sigma = \\begin{pmatrix}1 & \\rho \\\\ \\rho & 1\\end{pmatrix}$, where $|\\rho| < 1$. For a small step size $\\epsilon > 0$, define the estimator\n$$\\hat{g}(u) = \\frac{R(\\theta + \\epsilon u) - R(\\theta - \\epsilon u)}{2 \\epsilon}\\, u,$$\nwhere $R(\\cdot)$ denotes the realized return under the perturbed parameters and the same environment model as above. Assume $u$ is independent of the policy action noises and that expectations are taken both over the trajectory randomness and the perturbations $u$.\n- Compare $\\mathbb{E}[\\hat{g}(u)]$ to the exact gradient $\\nabla_{\\theta} J(\\theta)$, and provide the bias vector defined by $\\operatorname{Bias}(\\theta, \\rho) = \\mathbb{E}[\\hat{g}(u)] - \\nabla_{\\theta} J(\\theta)$.\n\nYour final answer must be the closed-form analytic expression for the bias vector $\\operatorname{Bias}(\\theta, \\rho)$ as a row vector in terms of $\\rho$, $\\alpha$, $\\beta$, $c_{s}$, $c_{a}$, $s_{0}$, $\\theta_{0}$, and $\\theta_{1}$. No numerical approximation is required.",
            "solution": "The derivation proceeds by completing the tasks outlined in the problem statement.\n\nFirst, we derive the expected return $J(\\theta)$. The objective is the expected total return, $J(\\theta) = \\mathbb{E}[R] = \\mathbb{E}[r_{0}] + \\mathbb{E}[r_{1}]$.\nThe expected reward at $t=0$ is:\n$$\\mathbb{E}[r_{0}] = \\mathbb{E}[c_{s} s_{0} + c_{a} a_{0}] = c_{s} s_{0} + c_{a} \\mathbb{E}[a_{0}]$$\nSince $a_{0} \\sim \\mathcal{N}(\\theta_{0} s_{0}, \\sigma^{2})$, its expectation is $\\mathbb{E}[a_{0}] = \\theta_{0} s_{0}$.\n$$\\mathbb{E}[r_{0}] = c_{s} s_{0} + c_{a} (\\theta_{0} s_{0}) = s_{0} (c_{s} + c_{a} \\theta_{0})$$\nThe expected reward at $t=1$ is $\\mathbb{E}[r_{1}] = c_{s} \\mathbb{E}[s_{1}] + c_{a} \\mathbb{E}[a_{1}]$. We find the required expectations:\n$$\\mathbb{E}[s_{1}] = \\mathbb{E}[\\alpha s_{0} + \\beta a_{0}] = \\alpha s_{0} + \\beta \\mathbb{E}[a_{0}] = \\alpha s_{0} + \\beta (\\theta_{0} s_{0}) = s_{0} (\\alpha + \\beta \\theta_{0})$$\n$$\\mathbb{E}[a_{1}] = \\mathbb{E}[\\mathbb{E}[a_{1} | s_{1}]] = \\mathbb{E}[\\theta_{1} s_{1}] = \\theta_{1} \\mathbb{E}[s_{1}] = \\theta_{1} s_{0} (\\alpha + \\beta \\theta_{0})$$\nSubstituting these back into $\\mathbb{E}[r_{1}]$:\n$$\\mathbb{E}[r_{1}] = c_{s} [s_{0} (\\alpha + \\beta \\theta_{0})] + c_{a} [\\theta_{1} s_{0} (\\alpha + \\beta \\theta_{0})] = s_{0} (c_{s} + c_{a} \\theta_{1}) (\\alpha + \\beta \\theta_{0})$$\nSumming the expected rewards gives the total expected return:\n$$J(\\theta) = s_{0} (c_{s} + c_{a} \\theta_{0}) + s_{0} (c_{s} + c_{a} \\theta_{1}) (\\alpha + \\beta \\theta_{0})$$\n\nSecond, we compute the exact gradient $\\nabla_{\\theta} J(\\theta)$. Expanding $J(\\theta)$ simplifies differentiation:\n$$J(\\theta) = s_{0} [c_{s} + c_{a} \\theta_{0} + c_{s}\\alpha + c_{s}\\beta\\theta_{0} + c_{a}\\alpha\\theta_{1} + c_{a}\\beta\\theta_{0}\\theta_{1}]$$\nThe partial derivative with respect to $\\theta_{0}$ is:\n$$\\frac{\\partial J}{\\partial \\theta_{0}} = \\frac{\\partial}{\\partial \\theta_{0}} \\left( s_{0} [c_{a} \\theta_{0} + c_{s}\\beta\\theta_{0} + c_{a}\\beta\\theta_{0}\\theta_{1}] \\right) = s_{0} (c_{a} + c_{s}\\beta + c_{a}\\beta\\theta_{1})$$\nThe partial derivative with respect to $\\theta_{1}$ is:\n$$\\frac{\\partial J}{\\partial \\theta_{1}} = \\frac{\\partial}{\\partial \\theta_{1}} \\left( s_{0} [c_{a}\\alpha\\theta_{1} + c_{a}\\beta\\theta_{0}\\theta_{1}] \\right) = s_{0} (c_{a}\\alpha + c_{a}\\beta\\theta_{0}) = s_{0} c_{a} (\\alpha + \\beta\\theta_{0})$$\nSo, the gradient vector is:\n$$\\nabla_{\\theta} J(\\theta) = \\begin{pmatrix} s_{0} (c_{a} + c_{s}\\beta + c_{a}\\beta\\theta_{1}) \\\\ s_{0} c_{a} (\\alpha + \\beta\\theta_{0}) \\end{pmatrix}$$\n\nThird, we compute the expectation of the estimator $\\mathbb{E}[\\hat{g}(u)]$. We take the expectation over both the trajectory randomness (from actions $a_t$) and the perturbation randomness (from $u$).\n$$\\mathbb{E}[\\hat{g}(u)] = \\mathbb{E}_{u, \\tau} \\left[ \\frac{R(\\theta + \\epsilon u) - R(\\theta - \\epsilon u)}{2 \\epsilon} u \\right]$$\nTaking the expectation over the trajectory first yields the objective function $J(\\cdot)$.\n$$\\mathbb{E}[\\hat{g}(u)] = \\mathbb{E}_{u} \\left[ \\frac{J(\\theta + \\epsilon u) - J(\\theta - \\epsilon u)}{2 \\epsilon} u \\right]$$\nSince $J(\\theta)$ is a quadratic polynomial in $\\theta$, the central finite difference is exact and gives the directional derivative:\n$$\\frac{J(\\theta + \\epsilon u) - J(\\theta - \\epsilon u)}{2\\epsilon} = u^{\\top} \\nabla_{\\theta} J(\\theta)$$\nSubstituting this back into the expectation:\n$$\\mathbb{E}[\\hat{g}(u)] = \\mathbb{E}_{u} [ (u^{\\top} \\nabla_{\\theta} J(\\theta)) u ]$$\nLet $g = \\nabla_{\\theta} J(\\theta)$ be the true gradient, which is constant with respect to $u$.\n$$\\mathbb{E}[\\hat{g}(u)] = \\mathbb{E}_{u} [ u (u^{\\top} g) ] = \\mathbb{E}_{u} [ u u^{\\top} ] g$$\nThe term $\\mathbb{E}_{u} [ u u^{\\top} ]$ is the covariance matrix of the random vector $u$, which is given as $\\Sigma$.\nTherefore, the expected value of the estimator is:\n$$\\mathbb{E}[\\hat{g}(u)] = \\Sigma \\nabla_{\\theta} J(\\theta)$$\n\nFourth, we compute the bias vector $\\operatorname{Bias}(\\theta, \\rho)$.\n$$\\operatorname{Bias}(\\theta, \\rho) = \\mathbb{E}[\\hat{g}(u)] - \\nabla_{\\theta} J(\\theta) = \\Sigma \\nabla_{\\theta} J(\\theta) - I \\nabla_{\\theta} J(\\theta) = (\\Sigma - I) \\nabla_{\\theta} J(\\theta)$$\nwhere $I$ is the $2 \\times 2$ identity matrix.\n$$\\Sigma - I = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & \\rho \\\\ \\rho & 0 \\end{pmatrix}$$\nLet $g = \\nabla_{\\theta} J(\\theta) = (g_{0}, g_{1})^{\\top}$. The bias is:\n$$\\operatorname{Bias}(\\theta, \\rho) = \\begin{pmatrix} 0 & \\rho \\\\ \\rho & 0 \\end{pmatrix} \\begin{pmatrix} g_{0} \\\\ g_{1} \\end{pmatrix} = \\begin{pmatrix} \\rho g_{1} \\\\ \\rho g_{0} \\end{pmatrix}$$\nSubstituting the expressions for $g_{0}$ and $g_{1}$:\n$$g_{0} = s_{0} (c_{a} + c_{s}\\beta + c_{a}\\beta\\theta_{1})$$\n$$g_{1} = s_{0} c_{a} (\\alpha + \\beta\\theta_{0})$$\nThe bias vector components are:\n-   First component: $\\rho g_{1} = \\rho s_{0} c_{a} (\\alpha + \\beta\\theta_{0})$\n-   Second component: $\\rho g_{0} = \\rho s_{0} (c_{a} + c_{s}\\beta + c_{a}\\beta\\theta_{1})$\n\nAs a row vector, the bias is:\n$$\\operatorname{Bias}(\\theta, \\rho) = \\begin{pmatrix} \\rho s_{0} c_{a} (\\alpha + \\beta\\theta_{0}) & \\rho s_{0} (c_{a} + c_{s}\\beta + c_{a}\\beta\\theta_{1}) \\end{pmatrix}$$",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\rho s_{0} c_{a} (\\alpha + \\beta\\theta_{0}) & \\rho s_{0} (c_{a} + c_{s}\\beta + c_{a}\\beta\\theta_{1}) \\end{pmatrix} } $$"
        }
    ]
}