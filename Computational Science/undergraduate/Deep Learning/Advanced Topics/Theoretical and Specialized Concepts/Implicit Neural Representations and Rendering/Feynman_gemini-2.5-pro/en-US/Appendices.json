{
    "hands_on_practices": [
        {
            "introduction": "Before we can render complex 3D scenes, we must first be able to represent their fine details within the weights of a neural network. This exercise dives into the core representational challenge of implicit models: their inherent bias towards learning low-frequency functions. You will construct a toy example to demonstrate how a simple MLP struggles to represent a high-frequency signal and how this leads to aliasing, providing a clear justification for the use of positional encoding in modern neural rendering. ",
            "id": "3136712",
            "problem": "Consider the task of modeling a one-dimensional, band-limited signal using an Implicit Neural Representation (INR) with a restricted Positional Encoding (PE). The input coordinate $x$ lies in the interval $[0,1)$ and angles must be handled in radians. The target signal is a synthetic stripe pattern defined by $f_K(x) = \\cos(2\\pi K x)$, where $K$ is an integer frequency specifying the number of cycles across the unit interval. The INR is constructed as a linear model over a truncated Fourier feature map, which serves as the PE. Specifically, for a chosen cutoff $K_{\\text{cut}} \\in \\mathbb{N}$, the PE is defined by the feature vector\n$$\n\\phi_{K_{\\text{cut}}}(x) = \\left[1, \\cos(2\\pi \\cdot 1 \\cdot x), \\sin(2\\pi \\cdot 1 \\cdot x), \\ldots, \\cos(2\\pi \\cdot k \\cdot x), \\sin(2\\pi \\cdot k \\cdot x), \\ldots, \\cos(2\\pi \\cdot K_{\\text{cut}} \\cdot x), \\sin(2\\pi \\cdot K_{\\text{cut}} \\cdot x)\\right],\n$$\nand the INR predicts\n$$\n\\hat{f}(x) = \\mathbf{w}^{\\top} \\phi_{K_{\\text{cut}}}(x),\n$$\nwhere $\\mathbf{w}$ is obtained by minimizing the squared error on a training set.\n\nThe training set consists of $N$ uniformly spaced samples $x_n = \\frac{n}{N}$ for $n = 0,1,\\ldots,N-1$, with corresponding targets $y_n = f_K(x_n)$. The model is fit by linear least squares on these $N$ samples.\n\nYour task is to build a toy example that demonstrates aliasing due to insufficient positional encoding, and to quantify the phenomenon using this synthetic stripe pattern. Use the following well-tested bases for reasoning: the properties of Fourier series on $[0,1)$, the concept of band-limited representations via truncated Fourier features, and the Nyquist sampling perspective on uniformly sampled sequences. Do not assume any special identities beyond these foundational facts; derive the observed behavior algorithmically.\n\nFor each test case below, you must:\n- Construct the training design matrix with the specified $K_{\\text{cut}}$, fit $\\mathbf{w}$ by least squares, and compute the training Mean Squared Error (MSE) defined as\n$$\n\\text{MSE}_{\\text{train}} = \\frac{1}{N} \\sum_{n=0}^{N-1} \\left(\\hat{f}(x_n) - f_K(x_n)\\right)^2.\n$$\n- Evaluate the fitted model on a dense grid of $M$ points $x_m = \\frac{m}{M}$ for $m = 0,1,\\ldots,M-1$ with $M$ chosen as a power of two, and compute the evaluation MSE\n$$\n\\text{MSE}_{\\text{eval}} = \\frac{1}{M} \\sum_{m=0}^{M-1} \\left(\\hat{f}(x_m) - f_K(x_m)\\right)^2.\n$$\n- Estimate the dominant frequency of the fitted model by applying the Fast Fourier Transform (FFT) on $\\{\\hat{f}(x_m)\\}_{m=0}^{M-1}$ and selecting the index $k^{\\star} \\in \\{1,2,\\ldots,\\lfloor M/2 \\rfloor\\}$ that maximizes the magnitude of the one-sided spectrum. Report $k^{\\star}$ as an integer.\n\nUse the following test suite of parameter values, which is designed to cover different facets of the phenomenon:\n1. Happy path case: $(N, K, K_{\\text{cut}}) = (64, 4, 10)$.\n2. Aliasing case where the target frequency exceeds the PE cutoff but can alias into the representable band on the training grid: $(N, K, K_{\\text{cut}}) = (64, 60, 10)$.\n3. Edge case where the target frequency exceeds the PE cutoff and cannot alias into the representable band, resulting in a low-pass approximation: $(N, K, K_{\\text{cut}}) = (64, 53, 10)$.\n4. Boundary condition at the Nyquist limit of the training grid: $(N, K, K_{\\text{cut}}) = (64, 32, 32)$.\n\nSet the dense evaluation grid size to $M = 2048$. For each test case, produce three quantities: $\\text{MSE}_{\\text{train}}$, $\\text{MSE}_{\\text{eval}}$, and the integer dominant frequency $k^{\\star}$ detected on the dense grid. Express the MSE values as decimal floats rounded to six decimal places. There are no physical units involved; angles are in radians.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by test case and metric as follows:\n$$\n\\left[\\text{MSE}_{\\text{train}}^{(1)}, \\text{MSE}_{\\text{eval}}^{(1)}, k^{\\star (1)}, \\text{MSE}_{\\text{train}}^{(2)}, \\text{MSE}_{\\text{eval}}^{(2)}, k^{\\star (2)}, \\text{MSE}_{\\text{train}}^{(3)}, \\text{MSE}_{\\text{eval}}^{(3)}, k^{\\star (3)}, \\text{MSE}_{\\text{train}}^{(4)}, \\text{MSE}_{\\text{eval}}^{(4)}, k^{\\star (4)}\\right].\n$$\nAll quantities must be computed by your program. No external input or files are permitted. Angles must be treated in radians throughout.",
            "solution": "The user's request has been assessed and is valid. It presents a well-posed, scientifically grounded problem in the domain of deep learning and signal processing, free of any inconsistencies or ambiguities. We shall proceed with a complete solution.\n\nThe problem requires an analysis of an Implicit Neural Representation (INR) based on a truncated Fourier feature map, also known as Positional Encoding (PE). The goal is to demonstrate aliasing and other representation artifacts by fitting this model to a simple sinusoidal signal, $f_K(x) = \\cos(2\\pi K x)$, under different conditions. The INR is a linear model $\\hat{f}(x) = \\mathbf{w}^{\\top} \\phi_{K_{\\text{cut}}}(x)$, where the weights $\\mathbf{w}$ are determined by linear least squares on a set of $N$ uniform samples.\n\nThe core of the analysis rests on three principles:\n1.  **Representation Capacity**: The PE, $\\phi_{K_{\\text{cut}}}(x)$, consists of sinusoidal basis functions up to a maximum frequency of $K_{\\text{cut}}$. The INR, being a linear combination of these basis functions, is inherently band-limited to the frequency range $[0, K_{\\text{cut}}]$. It cannot perfectly represent any signal containing frequencies greater than $K_{\\text{cut}}$.\n2.  **Least Squares Projection**: The least squares fitting process finds the optimal weights $\\mathbf{w}$ by projecting the sampled target vector $\\mathbf{y} = [y_0, \\ldots, y_{N-1}]^\\top$ onto the subspace spanned by the sampled basis vectors (the columns of the design matrix $\\Phi$). When the basis vectors are orthogonal on the sampling grid, the solution simplifies to independent projections onto each basis vector.\n3.  **Sampling and Aliasing**: According to the Nyquist-Shannon sampling theorem, sampling a continuous signal at a rate of $N$ samples per unit interval can unambiguously capture frequencies only up to the Nyquist frequency, $N/2$. A signal with frequency $K > N/2$ becomes indistinguishable from a lower-frequency alias on the sampling grid. For a pure cosine signal $\\cos(2\\pi K x)$ sampled at $x_n = n/N$, the values $\\cos(2\\pi K n/N)$ are identical to those of $\\cos(2\\pi K' n/N)$ where $K'$ is the frequency of smallest magnitude congruent to $\\pm K$ modulo $N$. That is, $K' = \\min_{j \\in \\mathbb{Z}} |K - jN|$.\n\nWe will now apply this framework to analyze each test case. For all cases, the evaluation grid has size $M=2048$.\n\n**Case 1: $(N, K, K_{\\text{cut}}) = (64, 4, 10)$**\n- **Analysis**: The target signal frequency is $K=4$, and the PE cutoff is $K_{\\text{cut}}=10$. Since $K \\le K_{\\text{cut}}$, the target signal $f_4(x) = \\cos(2\\pi \\cdot 4 \\cdot x)$ is directly representable by the PE basis. The least squares fit will identify the correct non-zero weight for the $\\cos(2\\pi \\cdot 4 \\cdot x)$ basis function and near-zero weights for all others.\n- **Prediction**: The model will perfectly reconstruct the target signal. We expect both $\\text{MSE}_{\\text{train}}$ and $\\text{MSE}_{\\text{eval}}$ to be close to $0$ (within machine precision). The dominant frequency of the fitted model $\\hat{f}(x)$ on the dense grid will be $k^{\\star}=4$.\n\n**Case 2: $(N, K, K_{\\text{cut}}) = (64, 60, 10)$**\n- **Analysis**: The target frequency $K=60$ is much larger than the PE cutoff $K_{\\text{cut}}=10$. However, the training is performed on a grid with $N=64$ samples. The Nyquist frequency is $N/2 = 32$. The frequency $K=60$ will alias. The aliased frequency is $K' = |60 - 64| = 4$. On the training grid, the sampled data from $f_{60}(x)$ is identical to that from $f_4(x)$:\n$$y_n = \\cos\\left(2\\pi \\cdot 60 \\cdot \\frac{n}{64}\\right) = \\cos\\left(2\\pi \\cdot (64-4) \\cdot \\frac{n}{64}\\right) = \\cos\\left(2\\pi n - 2\\pi \\cdot 4 \\cdot \\frac{n}{64}\\right) = \\cos\\left(2\\pi \\cdot 4 \\cdot \\frac{n}{64}\\right)$$\nThe least squares algorithm is thus presented with data that appears to come from a frequency-$4$ signal. Since $K'=4 \\le K_{\\text{cut}}=10$, the model will fit this aliased signal perfectly.\n- **Prediction**: $\\text{MSE}_{\\text{train}}$ will be near $0$. The learned model will be $\\hat{f}(x) \\approx \\cos(2\\pi \\cdot 4 \\cdot x)$. When evaluated on the dense grid, this low-frequency model is compared against the true high-frequency signal $f_{60}(x) = \\cos(2\\pi \\cdot 60 \\cdot x)$. The two are very different, so $\\text{MSE}_{\\text{eval}}$ will be large. The expected value is approximately $\\int_0^1 (\\cos(2\\pi \\cdot 4x) - \\cos(2\\pi \\cdot 60x))^2 dx = \\int_0^1 \\cos^2(2\\pi \\cdot 4x) dx + \\int_0^1 \\cos^2(2\\pi \\cdot 60x) dx = \\frac{1}{2} + \\frac{1}{2} = 1$. The dominant frequency found in $\\hat{f}(x)$ will be that of the alias, so $k^{\\star}=4$.\n\n**Case 3: $(N, K, K_{\\text{cut}}) = (64, 53, 10)$**\n- **Analysis**: The target frequency is $K=53$, with $N=64$ and $K_{\\text{cut}}=10$. The aliased frequency on the training grid is $K' = |53-64| = 11$. The training data thus appears to come from a frequency-$11$ signal. However, this aliased frequency $K'=11$ is *still greater* than the PE cutoff $K_{\\text{cut}}=10$. The model's basis functions (frequencies $0, 1, \\ldots, 10$) cannot represent the frequency-$11$ signal they are being trained on. The discrete Fourier basis functions are orthogonal on the uniform grid. The sampled target vector, corresponding to frequency $11$, is orthogonal to all the basis vectors provided by the PE (frequencies $0$ to $10$). The projection onto this basis space is thus the zero vector.\n- **Prediction**: The least squares fit will result in $\\mathbf{w} \\approx \\mathbf{0}$, meaning $\\hat{f}(x) \\approx 0$ for all $x$. The training MSE will be $\\text{MSE}_{\\text{train}} = \\frac{1}{N}\\sum_{n=0}^{N-1}(0 - y_n)^2 = \\frac{1}{N}\\sum_{n=0}^{N-1}\\cos^2(2\\pi \\cdot 11 \\cdot n/N) \\approx \\frac{1}{2}$. Similarly, $\\text{MSE}_{\\text{eval}} = \\frac{1}{M}\\sum_{m=0}^{M-1}(0-f_{53}(x_m))^2 \\approx \\frac{1}{2}\\int_0^1 \\cos^2(2\\pi \\cdot 53 x) dx = 0.5$. Since $\\hat{f}(x)$ is the zero function, its FFT will be zero everywhere. The `argmax` function will return the first index of the search range $\\{1, \\ldots, \\lfloor M/2 \\rfloor\\}$, so we expect $k^{\\star}=1$.\n\n**Case 4: $(N, K, K_{\\text{cut}}) = (64, 32, 32)$**\n- **Analysis**: The target frequency $K=32$ is exactly the Nyquist frequency for $N=64$. The PE cutoff is $K_{\\text{cut}}=32$, so the basis includes the function $\\cos(2\\pi \\cdot 32 \\cdot x)$. At the sampling points $x_n = n/64$, the target is $y_n = \\cos(2\\pi \\cdot 32 \\cdot n/64) = \\cos(\\pi n) = (-1)^n$. This is exactly the sampled Nyquist-frequency cosine basis function. It is important to note that the corresponding sine term, $\\sin(2\\pi \\cdot 32 \\cdot n/64) = \\sin(\\pi n)$, is zero for all integer $n$, resulting in a column of zeros in the design matrix. This introduces a rank deficiency. However, since the target signal lies perfectly within the span of the remaining basis vectors, a perfect fit is still achieved. `numpy.linalg.lstsq` will correctly find the solution that sets the weight for the $\\cos(2\\pi \\cdot 32 \\cdot x)$ term to $1$ and all other weights to $0$.\n- **Prediction**: The model will perfectly represent the target. Both $\\text{MSE}_{\\text{train}}$ and $\\text{MSE}_{\\text{eval}}$ will be near $0$. The dominant frequency of the fitted model $\\hat{f}(x)=\\cos(2\\pi \\cdot 32 \\cdot x)$ will be $k^{\\star}=32$.\n\nThese analyses lead to the implementation provided in the final answer.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of fitting a band-limited INR to a sinusoidal signal\n    under various conditions of sampling, signal frequency, and PE cutoff.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, K, K_cut)\n        (64, 4, 10),    # Happy path\n        (64, 60, 10),   # Aliasing into representable band\n        (64, 53, 10),   # Aliasing outside representable band\n        (64, 32, 32),   # Nyquist boundary case\n    ]\n    \n    # Dense evaluation grid size\n    M = 2048\n\n    all_results = []\n\n    for N, K, K_cut in test_cases:\n        # 1. Construct the training data\n        x_train = np.arange(N) / N\n        y_train = np.cos(2 * np.pi * K * x_train)\n        \n        # 2. Construct the Positional Encoding (PE) design matrix for training\n        num_features = 1 + 2 * K_cut\n        Phi_train = np.zeros((N, num_features))\n        \n        # DC component (frequency 0)\n        Phi_train[:, 0] = 1.0\n        \n        # Sinusoidal components (frequencies 1 to K_cut)\n        freqs = np.arange(1, K_cut + 1)\n        # Use broadcasting for efficient construction\n        cos_terms = np.cos(2 * np.pi * freqs[None, :] * x_train[:, None])\n        sin_terms = np.sin(2 * np.pi * freqs[None, :] * x_train[:, None])\n        \n        # Interleave cos and sin terms as per the problem definition\n        Phi_train[:, 1::2] = cos_terms\n        Phi_train[:, 2::2] = sin_terms\n        \n        # 3. Fit the model by solving the linear least squares problem\n        # w = (Phi^T Phi)^-1 Phi^T y\n        w = np.linalg.lstsq(Phi_train, y_train, rcond=None)[0]\n        \n        # 4. Compute the training MSE\n        y_hat_train = Phi_train @ w\n        mse_train = np.mean((y_hat_train - y_train) ** 2)\n        \n        # 5. Evaluate the fitted model on a dense grid\n        x_eval = np.arange(M) / M\n        y_eval_true = np.cos(2 * np.pi * K * x_eval)\n        \n        # Construct the PE design matrix for evaluation\n        Phi_eval = np.zeros((M, num_features))\n        Phi_eval[:, 0] = 1.0\n        cos_terms_eval = np.cos(2 * np.pi * freqs[None, :] * x_eval[:, None])\n        sin_terms_eval = np.sin(2 * np.pi * freqs[None, :] * x_eval[:, None])\n        Phi_eval[:, 1::2] = cos_terms_eval\n        Phi_eval[:, 2::2] = sin_terms_eval\n        \n        y_hat_eval = Phi_eval @ w\n        \n        # 6. Compute the evaluation MSE\n        mse_eval = np.mean((y_hat_eval - y_eval_true) ** 2)\n\n        # 7. Estimate the dominant frequency of the fitted model\n        # Apply FFT on the densely sampled predictions\n        fft_vals = np.fft.fft(y_hat_eval)\n        # Get magnitudes of the one-sided spectrum\n        fft_mags = np.abs(fft_vals)\n        \n        # Find the peak frequency index k* in {1, 2, ..., floor(M/2)}\n        # This corresponds to array indices 1 through M//2\n        search_range = fft_mags[1 : M//2 + 1]\n        # argmax returns 0-based index relative to the slice; add 1 for frequency\n        k_star = int(np.argmax(search_range) + 1)\n\n        # 8. Append results for the current test case\n        all_results.append(f\"{mse_train:.6f}\")\n        all_results.append(f\"{mse_eval:.6f}\")\n        all_results.append(str(k_star))\n\n    #\n    # Final print statement in the exact required format.\n    #\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once a scene is represented implicitly, the challenge shifts to rendering it efficiently. This practice tackles the core of volumetric rendering by implementing neural importance sampling, a technique that dramatically improves efficiency by concentrating sample points in regions of high density. You will learn to derive a target distribution from rendering weights and train a small sampling network to match it, minimizing the Kullback–Leibler ($D_{\\mathrm{KL}}$) divergence and gaining insight into NeRF's hierarchical sampling strategy. ",
            "id": "3136700",
            "problem": "You are given a one-dimensional ray parameter domain and asked to implement a neural importance sampling probability density function that adapts to a target distribution induced by a differentiable implicit neural representation along the ray. Your goal is to construct a complete program that learns a piecewise-constant sampling distribution using gradient descent to reduce the Kullback–Leibler divergence (KL divergence) against a target distribution derived from volume rendering weights, and to report the final KL divergences for a set of specified test cases.\n\nThe fundamental base for this problem is as follows. A ray is parameterized by $t \\in [t_{\\min}, t_{\\max}]$. An implicit neural representation $f_\\theta$ maps $t$ to a nonnegative volumetric density $\\sigma_\\theta(t)$. The rendering weight along the ray at $t$ is defined through transmittance and opacity. In continuous form, transmittance is $T(t) = \\exp\\left(-\\int_{t_{\\min}}^{t} \\sigma_\\theta(s) \\,\\mathrm{d}s\\right)$ and an infinitesimal opacity is $\\alpha(t) = 1 - \\exp(-\\sigma_\\theta(t)\\,\\Delta t)$ for a small step $\\Delta t$. The rendering weight at $t$ is $w(t) = T(t)\\,\\alpha(t)$, and an importance target distribution is proportional to $w(t)$. The probability density function must integrate to $1$, hence the target distribution $q(t)$ satisfies $q(t) = \\frac{w(t)}{\\int_{t_{\\min}}^{t_{\\max}} w(u)\\,\\mathrm{d}u}$.\n\nYou will implement the following steps in discrete form on a uniform grid:\n- Use $t_{\\min} = 0$ and $t_{\\max} = 1$.\n- Use $N_s$ uniform samples with $t_i = t_{\\min} + i\\,\\Delta t$ for $i = 0,1,\\dots,N_s-1$, where $\\Delta t = \\frac{t_{\\max} - t_{\\min}}{N_s}$.\n- Compute $\\sigma_\\theta(t_i)$ for each $t_i$, then compute $T_i$ with $T_0 = 1$ and $T_{i+1} = T_i \\cdot \\exp(-\\sigma_\\theta(t_i)\\,\\Delta t)$, and compute $\\alpha_i = 1 - \\exp(-\\sigma_\\theta(t_i)\\,\\Delta t)$. Set $w_i = T_i\\,\\alpha_i$. The discrete target distribution is $q_i = \\frac{w_i}{\\sum_{j=0}^{N_s-1} w_j}$.\n\nYou will parameterize a piecewise-constant importance distribution over $B$ equal-width bins via logits $\\phi \\in \\mathbb{R}^B$. Let $m_b = \\frac{\\exp(\\phi_b)}{\\sum_{k=0}^{B-1}\\exp(\\phi_k)}$ be the bin mass for bin $b$. Within bin $b$, the probability mass is uniformly spread across its grid cells, so the probability mass at a grid index $i$ belonging to bin $b$ is $p_i = \\frac{m_b}{C_b}$, where $C_b$ is the number of grid samples in bin $b$. The discrete Kullback–Leibler divergence between the target distribution and the learned importance distribution is\n$$\nD_{\\mathrm{KL}}(q \\Vert p) = \\sum_{i=0}^{N_s-1} q_i \\,\\log\\left(\\frac{q_i}{p_i}\\right).\n$$\nTo learn $\\phi$, minimize the cross-entropy between the aggregated target bin masses and the softmax bin masses. Let $Q_b = \\sum_{i \\in \\text{bin } b} q_i$ be the target mass per bin. The cross-entropy objective is\n$$\n\\mathcal{L}(\\phi) = -\\sum_{b=0}^{B-1} Q_b \\,\\log m_b,\n$$\nand its gradient with respect to the logits is\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\phi_b} = m_b - Q_b.\n$$\nYou must implement a gradient descent update on $\\phi$ using this gradient.\n\nTo satisfy the coupling to $f_\\theta$, define $f_\\theta$ as a small Sine Representation Network (commonly called a SIREN) that maps $t$ to a scalar, then passes through a Softplus to ensure nonnegativity:\n- Let $h_1(t) = \\sin(w_1\\,t + b_1)$.\n- Let $h_2(t) = \\sin(w_2\\,h_1(t) + b_2)$.\n- Let $\\sigma_\\theta(t) = \\operatorname{softplus}(a\\,h_2(t) + c + d\\,t)$, where $\\operatorname{softplus}(x) = \\log(1 + e^x)$.\n\nYou must also implement a sampler for $p_\\phi(t)$ that draws $K$ samples along the ray by inverse transform sampling at the bin level with uniform sampling inside each selected bin. Although the sampler does not affect the required final outputs, it is required to be implemented.\n\nSet the following fixed numerical hyperparameters:\n- Use $N_s = 256$.\n- Use $B = 16$.\n- Use a learning rate of $\\eta = 0.5$.\n- Use $S = 800$ gradient descent steps.\n- Use $K = 64$ samples for the sampler demonstration (not part of the final outputs).\n\nDesign the program to run the following test suite, each test case defining a distinct $\\theta = (w_1, b_1, w_2, b_2, a, c, d)$:\n- Case $1$: $(w_1, b_1, w_2, b_2, a, c, d) = (8.0, 0.3, 2.0, 0.2, 1.5, 0.1, 0.0)$.\n- Case $2$ (near-uniform density): $(w_1, b_1, w_2, b_2, a, c, d) = (4.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0)$.\n- Case $3$ (high-frequency variation): $(w_1, b_1, w_2, b_2, a, c, d) = (30.0, 0.1, 4.0, 0.7, 0.8, 0.0, 0.0)$.\n- Case $4$ (density decreasing with $t$): $(w_1, b_1, w_2, b_2, a, c, d) = (10.0, -0.5, 3.0, 0.0, 1.0, 0.2, -1.0)$.\n\nYour program must:\n- For each case, compute the discrete target distribution $q_i$ induced by $f_\\theta$ via discrete transmittance and opacity as specified.\n- Train the logits $\\phi$ by minimizing the cross-entropy with gradient descent for $S$ steps.\n- After training, compute the final discrete KL divergence $D_{\\mathrm{KL}}(q \\Vert p)$ with $p_i$ obtained from the learned bin masses uniformly distributed across grid cells in each bin.\n- Produce a single line of output containing the final KL divergences for the four cases as a comma-separated list enclosed in square brackets.\n\nAll computations are dimensionless; there are no physical units or angles involved. All outputs must be floating-point numbers. The final output format must be exactly like $[r_1,r_2,r_3,r_4]$, where each $r_k$ is the floating-point KL divergence for case $k$.\n\nScientific realism and self-consistency must be ensured by following the definitions and formulas above, and by implementing numerically stable operations, including appropriate handling to avoid division by zero in the KL divergence computation via small positive clamping when needed.",
            "solution": "The problem is valid. It presents a well-posed and scientifically grounded task in the domain of deep learning and computer graphics, specifically related to neural importance sampling for volume rendering. All necessary parameters, definitions, and mathematical formulations are provided, and the task is computationally verifiable.\n\nThe objective is to learn a piecewise-constant probability density function, $p(t)$, that approximates a complex target distribution, $q(t)$, derived from the volume rendering equation applied to a neural representation of volumetric density. The quality of the approximation is measured by the Kullback–Leibler divergence, $D_{\\mathrm{KL}}(q \\Vert p)$, which is minimized through a gradient-based optimization procedure. The process is divided into three main stages: generation of the target distribution, parameterization and optimization of the importance distribution, and final evaluation.\n\n**1. Target Distribution from Neural Volume Rendering**\n\nThe foundation of the problem is an implicit neural representation, specifically a small Sine Representation Network (SIREN) denoted by $f_\\theta$. This network maps a one-dimensional ray parameter, $t$, to a non-negative volumetric density, $\\sigma_\\theta(t)$. The network parameters are given by $\\theta = (w_1, b_1, w_2, b_2, a, c, d)$. The network is defined by the following sequence of operations:\n$$ h_1(t) = \\sin(w_1 t + b_1) $$\n$$ h_2(t) = \\sin(w_2 h_1(t) + b_2) $$\n$$ \\sigma_\\theta(t) = \\operatorname{softplus}(a h_2(t) + c + d t) $$\nThe $\\operatorname{softplus}(x) = \\log(1 + e^x)$ function ensures that the output density $\\sigma_\\theta(t)$ is always non-negative.\n\nThe continuous ray domain, $t \\in [t_{\\min}, t_{\\max}] = [0, 1]$, is discretized into $N_s = 256$ uniform segments. The sample points are $t_i = t_{\\min} + i \\cdot \\Delta t$ for $i \\in \\{0, 1, \\dots, N_s-1\\}$, where the step size is $\\Delta t = (t_{\\max} - t_{\\min}) / N_s$.\n\nThe target probability distribution $q$ is derived using the principles of discrete volume rendering. The transmittance, $T_i$, which represents the probability that light travels from the start of the ray to the beginning of the $i$-th segment at $t_i$ without being occluded, is calculated recursively.\n$$ T_i = \\exp\\left(-\\sum_{j=0}^{i-1} \\sigma_\\theta(t_j) \\Delta t\\right), \\quad \\text{with } T_0 = 1 $$\nThe opacity of the $i$-th segment, $\\alpha_i$, which is the probability of light interacting with the medium within that segment, is given by:\n$$ \\alpha_i = 1 - \\exp(-\\sigma_\\theta(t_i) \\Delta t) $$\nThe unnormalized rendering weight, $w_i$, which represents the contribution of the $i$-th segment to the final rendered value, is the product of the light reaching it and the opacity of the segment: $w_i = T_i \\alpha_i$.\nFinally, these weights are normalized to form a valid discrete probability distribution, $q$:\n$$ q_i = \\frac{w_i}{\\sum_{j=0}^{N_s-1} w_j} $$\nThis distribution $q$ serves as the target for our importance sampling model.\n\n**2. Parameterizing and Optimizing the Importance Distribution**\n\nThe learned importance distribution, $p(t)$, is modeled as a piecewise-constant function over $B=16$ contiguous, equal-width bins spanning the domain $[0, 1]$. The probability mass of each bin, $b \\in \\{0, \\dots, B-1\\}$, is determined by a corresponding logit, $\\phi_b$. The set of all logits is a vector $\\phi \\in \\mathbb{R}^B$. The probability mass, $m_b$, for bin $b$ is calculated using the softmax function, which ensures the masses are positive and sum to $1$.\n$$ m_b = \\frac{\\exp(\\phi_b)}{\\sum_{k=0}^{B-1} \\exp(\\phi_k)} $$\nWithin each bin, this mass is spread uniformly across the $C_b = N_s / B = 16$ discrete sample points that fall into it. The probability $p_i$ for a sample point $t_i$ belonging to bin $b$ is therefore:\n$$ p_i = \\frac{m_b}{C_b} $$\nThe goal is to find the logits $\\phi$ that make the resulting distribution $p$ as close as possible to the target $q$. This is achieved by minimizing the $D_{\\mathrm{KL}}(q \\Vert p)$. For a piecewise-constant model, this is equivalent to minimizing the cross-entropy between the aggregated target-bin masses, $Q_b = \\sum_{i \\in \\text{bin } b} q_i$, and the learned bin masses, $m_b$. The loss function is:\n$$ \\mathcal{L}(\\phi) = -\\sum_{b=0}^{B-1} Q_b \\log m_b $$\nA key property of this loss function is its simple gradient with respect to the logits:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\phi_b} = m_b - Q_b $$\nThis elegant result allows for efficient optimization using gradient descent. The logits are initialized (e.g., to $0$) and iteratively updated for $S=800$ steps with a learning rate of $\\eta=0.5$:\n$$ \\phi \\leftarrow \\phi - \\eta \\cdot (m - Q) $$\n\n**3. Implementation and Evaluation**\n\nThe overall algorithm for each test case proceeds as follows:\nFirst, the SIREN parameters $\\theta$ are used to compute the density values $\\sigma_\\theta(t_i)$ across the discretized ray. Second, the discrete volume rendering formulas are applied to these densities to generate the target distribution $q$. Third, the target distribution is binned to form the aggregate target masses $Q_b$. Fourth, the logits $\\phi$ are trained for $S=800$ steps using the gradient descent update rule.\nAfter training, the final optimized logits are used to construct the discrete probability distribution $p$. The final performance is evaluated by computing the Kullback–Leibler divergence $D_{\\mathrm{KL}}(q \\Vert p)$. The implementation also includes a function to draw $K=64$ samples from the learned distribution $p_\\phi(t)$ using inverse transform sampling, which involves first sampling a bin according to the masses $m_b$ and then drawing a uniform sample within that bin's spatial range. This demonstrates how the learned distribution would be used in practice for importance sampling in rendering.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import softmax\n\ndef solve():\n    \"\"\"\n    Main function to run the neural importance sampling simulation for all test cases.\n    \"\"\"\n    # Define fixed numerical hyperparameters\n    N_S = 256\n    B = 16\n    ETA = 0.5\n    S = 800\n    K = 64\n    T_MIN = 0.0\n    T_MAX = 1.0\n\n    def _softplus(x):\n        \"\"\"Numerically stable softplus function.\"\"\"\n        return np.logaddexp(0, x)\n\n    def _siren(t, params):\n        \"\"\"\n        Computes density sigma from a SIREN model.\n        params = (w1, b1, w2, b2, a, c, d)\n        \"\"\"\n        w1, b1, w2, b2, a, c, d = params\n        h1 = np.sin(w1 * t + b1)\n        h2 = np.sin(w2 * h1 + b2)\n        output = a * h2 + c + d * t\n        sigma = _softplus(output)\n        return sigma\n\n    def _get_target_distribution(params):\n        \"\"\"\n        Computes the discrete target distribution q_i from the SIREN parameters.\n        \"\"\"\n        t_points = np.linspace(T_MIN, T_MAX, N_S, endpoint=False, dtype=np.float64)\n        delta_t = (T_MAX - T_MIN) / N_S\n        \n        sigma = _siren(t_points, params)\n        \n        alpha = 1.0 - np.exp(-sigma * delta_t)\n        \n        # T_i is the transmittance to the start of interval i.\n        # This is an exclusive cumulative product.\n        transmittance_contrib = np.exp(-sigma * delta_t)\n        T = np.concatenate(([1.0], np.cumprod(transmittance_contrib[:-1])))\n        \n        w = T * alpha\n        \n        w_sum = np.sum(w)\n        if w_sum < 1e-9: # Robustness for near-zero weights\n            return np.full(N_S, 1.0 / N_S)\n        \n        q = w / w_sum\n        return q\n\n    def _train_importance_distribution(q):\n        \"\"\"\n        Trains the piecewise-constant distribution logits phi using gradient descent.\n        \"\"\"\n        if N_S % B != 0:\n            raise ValueError(\"N_s must be divisible by B.\")\n        C_b = N_S // B\n\n        q_reshaped = q.reshape((B, C_b))\n        Q = q_reshaped.sum(axis=1)\n\n        phi = np.zeros(B, dtype=np.float64)\n\n        for _ in range(S):\n            m = softmax(phi)\n            grad = m - Q\n            phi -= ETA * grad\n            \n        return phi\n\n    def _calculate_kl_divergence(q, trained_phi):\n        \"\"\"\n        Calculates the final KL divergence between q and the learned distribution p.\n        \"\"\"\n        C_b = N_S // B\n        \n        m_final = softmax(trained_phi)\n        \n        p_per_bin = m_final / C_b\n        p = np.repeat(p_per_bin, C_b)\n        \n        # Add a small epsilon for numerical stability in a practical setting,\n        # although with softplus, sigma, w, and q should be positive.\n        eps = 1e-10\n        kl_div = np.sum(q * (np.log(q + eps) - np.log(p + eps)))\n\n        return kl_div\n\n    def _sample_from_distribution(phi, k_samples):\n        \"\"\"\n        Draws samples from the learned piecewise-constant distribution p_phi(t).\n        This function is implemented as required but not used for the final output.\n        \"\"\"\n        m = softmax(phi)\n        \n        bin_indices = np.random.choice(np.arange(B), size=k_samples, p=m)\n        \n        bin_width = (T_MAX - T_MIN) / B\n        bin_starts = bin_indices * bin_width + T_MIN\n        offsets = np.random.uniform(low=0.0, high=bin_width, size=k_samples)\n        \n        samples = bin_starts + offsets\n        return samples\n\n    # Define the test cases\n    test_cases = [\n        (8.0, 0.3, 2.0, 0.2, 1.5, 0.1, 0.0),\n        (4.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0),\n        (30.0, 0.1, 4.0, 0.7, 0.8, 0.0, 0.0),\n        (10.0, -0.5, 3.0, 0.0, 1.0, 0.2, -1.0),\n    ]\n\n    results = []\n    for case_params in test_cases:\n        # Step 1: Compute target distribution q\n        q = _get_target_distribution(case_params)\n        \n        # Step 2: Train the importance distribution to get final logits phi\n        trained_phi = _train_importance_distribution(q)\n        \n        # Step 3: Compute final KL divergence\n        final_kl = _calculate_kl_divergence(q, trained_phi)\n        results.append(final_kl)\n\n        # The sampler is implemented but its output is not part of the final result.\n        _ = _sample_from_distribution(trained_phi, K)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Purely implicit models like NeRF offer incredible quality but can be slow to train and query. This exercise moves to the frontier of neural rendering by analyzing hybrid implicit-explicit architectures, which combine the strengths of fast, explicit data structures (like voxel grids) with the expressive power of compact, implicit networks. You will derive and implement formulas to perform a principled cost-benefit analysis, exploring the critical trade-offs between memory usage and computational speed that drive modern system design. ",
            "id": "3136751",
            "problem": "You are tasked with building a principled computational model for a hybrid implicit-explicit rendering pipeline that combines a coarse explicit voxel grid and an implicit residual neural network. The explicit component is a regular voxel grid storing features, while the implicit component is a residual multilayer perceptron that refines the interpolated voxel features. Your goal is to derive and implement formulas for memory usage and floating-point operation counts based on first principles and core definitions, and then compute these quantities for a provided test suite.\n\nThe pipeline is defined as follows:\n- A voxel grid stores a feature vector at each grid vertex. The grid resolution is a cube of side length $N$, so there are $N^3$ vertices. The feature dimension is $C$, and each feature element is a $32$-bit floating point number. Let the number of bytes per element be $B$, where $B = 4$.\n- At each query location with continuous coordinates in the unit cube, the feature is obtained by trilinear interpolation over the $8$ neighboring vertices. The interpolation weights depend only on the fractional parts along each axis for that query point.\n- A residual multilayer perceptron (MLP), denoted $r_{\\theta}$, refines the interpolated feature. The input to $r_{\\theta}$ is the concatenation of the $3$-dimensional query position and the interpolated feature vector, so the input dimension is $d_{\\text{in}} = C + 3$. The MLP has $L$ hidden layers, each of width $W$, and an output dimension $d_{\\text{out}} = C$. The residual output is added to the interpolated feature to produce the final feature, i.e., the final output is $f + r_{\\theta}([x, f])$, where $x \\in \\mathbb{R}^3$ is the query point and $f \\in \\mathbb{R}^C$ is the interpolated feature.\n- The rendering pass evaluates the pipeline at $R$ rays, each with $S$ samples, so the total number of query evaluations is $Q = R \\cdot S$.\n\nAdopt the following universally accepted computational cost model:\n- Each floating-point addition, subtraction, or multiplication counts as one floating-point operation (FLOP).\n- A dense layer with weight matrix of shape $a \\times b$ and a vector input of size $b$ uses $2ab$ FLOPs (one multiplication and one addition per weight) for the matrix-vector multiplication. Adding a bias vector of size $a$ costs $a$ FLOPs. Applying a Rectified Linear Unit (ReLU) activation to $a$ elements costs $a$ FLOPs. Assume ReLU is applied after each hidden layer and not applied after the final output layer.\n- Trilinear interpolation at a single query point uses the $8$ neighboring vertices. Computing the $8$ weights from the three axis-wise fractional parts requires $3$ subtractions to form complements and $16$ multiplications to form all triple products. For each channel, forming the weighted sum from the $8$ neighbor values requires $8$ multiplications and $7$ additions. The weights are shared across channels for a given query point.\n\nFrom these bases, do the following:\n1. Derive an expression for the voxel parameter memory in bytes as a function of $N$, $C$, and $B$.\n2. Derive an expression for the residual MLP parameter count (including biases) as a function of $C$, $W$, and $L$, and convert it to bytes using $B$.\n3. Derive the total memory in megabytes, where one megabyte is defined as $2^{20}$ bytes. Express memory in MB using base $2^{20}$ bytes.\n4. Derive an expression for the per-sample FLOPs for the trilinear interpolation across all $C$ channels, including the shared weight computation.\n5. Derive an expression for the per-sample FLOPs of the MLP forward pass, including bias additions and ReLU costs on the hidden layers, and include the final residual addition of size $C$.\n6. Derive the total FLOPs per rendering pass, aggregating $Q$ samples, and report totals both for interpolation and for the MLP, as well as their sum. Convert FLOPs to gigaflops (GFLOPs) using base $10^9$.\n\nYour program must implement these derivations and compute, for each test case, a list containing:\n- Total memory in MB,\n- Voxel memory in MB,\n- MLP parameter memory in MB,\n- Total GFLOPs per pass,\n- Interpolation-only GFLOPs per pass,\n- MLP-only GFLOPs per pass.\n\nNumerical and unit conventions:\n- Use $B = 4$ bytes for each floating-point parameter and voxel feature element.\n- Use megabytes (MB) as $2^{20}$ bytes.\n- Use gigaflops (GFLOPs) as $10^9$ FLOPs.\n- Round all reported floating-point results to exactly three digits after the decimal point.\n\nTest suite:\n- Case $1$: $N = 64$, $C = 8$, $W = 64$, $L = 2$, $R = 1024$, $S = 64$.\n- Case $2$: $N = 1$, $C = 1$, $W = 32$, $L = 1$, $R = 1$, $S = 1$.\n- Case $3$: $N = 16$, $C = 4$, $W = 256$, $L = 4$, $R = 512$, $S = 48$.\n- Case $4$: $N = 128$, $C = 16$, $W = 64$, $L = 2$, $R = 256$, $S = 32$.\n- Case $5$: $N = 32$, $C = 12$, $W = 1$, $L = 1$, $R = 2048$, $S = 8$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of results, one per test case, where each result is itself a list in the following order: $[$total\\_MB, voxel\\_MB, mlp\\_MB, total\\_GF, interp\\_GF, mlp\\_GF$]$.\n- The line must be exactly of the form: a single top-level list with comma-separated inner lists, all numeric values rounded to three digits after the decimal point. For example: $[[1.234,0.123,1.111,2.222,0.333,1.889],[...],...]$.",
            "solution": "The problem statement has been critically validated and is deemed to be scientifically grounded, well-posed, objective, and internally consistent. It provides a clear and formalizable description of a hybrid rendering pipeline and a set of computational cost models. All necessary parameters and definitions for deriving the required formulas for memory and floating-point operations are explicitly provided. The problem is valid and a unique solution can be determined.\n\nThe task is to derive and compute the memory footprint and computational cost for a hybrid implicit-explicit rendering pipeline. The derivation proceeds from the first principles and definitions provided in the problem statement.\n\nThe key parameters are:\n- $N$: Voxel grid side length.\n- $C$: Feature dimension.\n- $W$: MLP hidden layer width.\n- $L$: Number of MLP hidden layers.\n- $R$: Number of rays per pass.\n- $S$: Number of samples per ray.\n- $B = 4$: Number of bytes per $32$-bit float.\n\nThe total number of queries per pass is $Q = R \\cdot S$.\n\n**1. Voxel Parameter Memory Derivation**\nThe explicit component is a voxel grid of resolution $N \\times N \\times N$. Each of the $N^3$ vertices stores a feature vector of dimension $C$. Each element is a $32$-bit float, consuming $B$ bytes.\nThe total memory required for the voxel grid, denoted $M_{vox}$, is the product of the number of vertices, the feature dimension, and the bytes per element.\n$$M_{vox} = N^3 \\cdot C \\cdot B \\text{ bytes}$$\n\n**2. Residual MLP Parameter Memory Derivation**\nThe implicit component is a residual multilayer perceptron, $r_{\\theta}$.\n- The input dimension is $d_{\\text{in}} = C + 3$, concatenating a $C$-dimensional feature and a $3$-dimensional position.\n- There are $L$ hidden layers, each of width $W$.\n- The output dimension is $d_{\\text{out}} = C$.\n\nThe total number of parameters (weights and biases), $P_{mlp}$, is the sum of parameters from all layers:\n- **Input to Hidden Layer 1**: A linear transformation from $d_{\\text{in}}$ to $W$. This requires a weight matrix of size $W \\times d_{\\text{in}}$ and a bias vector of size $W$.\n  Parameters: $(d_{\\text{in}} \\cdot W) + W = ((C+3) \\cdot W) + W$.\n- **Hidden Layer $i$ to Hidden Layer $i+1$**: For $i = 1, \\dots, L-1$. This network has $L-1$ such hidden-to-hidden transformations, each from $W$ to $W$. Each requires a weight matrix of size $W \\times W$ and a bias vector of size $W$.\n  Parameters per layer: $W^2 + W$.\n  Total for $L-1$ layers: $(L-1) \\cdot (W^2 + W)$.\n- **Hidden Layer $L$ to Output**: A linear transformation from $W$ to $d_{\\text{out}}$. This requires a weight matrix of size $d_{\\text{out}} \\times W$ and a bias vector of size $d_{\\text{out}}$.\n  Parameters: $(d_{\\text{out}} \\cdot W) + d_{\\text{out}} = (C \\cdot W) + C$.\n\nSumming these components gives the total parameter count for the MLP:\n$$P_{mlp} = \\left[ (C+3)W + W \\right] + \\left[ (L-1)(W^2 + W) \\right] + \\left[ CW + C \\right]$$\nThe memory for the MLP, $M_{mlp}$, is this parameter count multiplied by the bytes per parameter, $B$:\n$$M_{mlp} = P_{mlp} \\cdot B \\text{ bytes}$$\n\n**3. Total Memory in Megabytes**\nThe total memory, $M_{total}$, is the sum of the voxel and MLP memory.\n$$M_{total} = M_{vox} + M_{mlp}$$\nTo convert from bytes to megabytes (MB), where $1 \\text{ MB} = 2^{20} \\text{ bytes}$, we divide by $2^{20}$.\n$$ M_{total, MB} = \\frac{M_{total}}{2^{20}}, \\quad M_{vox, MB} = \\frac{M_{vox}}{2^{20}}, \\quad M_{mlp, MB} = \\frac{M_{mlp}}{2^{20}} $$\n\n**4. Per-Sample Trilinear Interpolation FLOPs Derivation**\nFor each query point, the computation of the interpolated feature involves two steps:\n- **Weight Calculation**: To find the $8$ trilinear interpolation weights, we need the fractional coordinates along each of the $3$ axes. This requires $3$ subtractions to find the complements (e.g., $1-d_x$) and $16$ multiplications to form the $8$ weights (e.g., $(1-d_x)(1-d_y)(1-d_z)$ requires $2$ multiplications). This part is shared across all $C$ channels.\n  FLOPs for weights: $F_{weights} = 3 + 16 = 19$.\n- **Weighted Sum**: For each of the $C$ channels, the interpolated value is a weighted sum of the $8$ neighboring vertex features. This requires $8$ multiplications and $7$ additions.\n  FLOPs per channel: $F_{sum, chan} = 8 + 7 = 15$.\nThe total per-sample interpolation FLOPs, $F_{interp}$, is the sum of the one-time weight computation and the per-channel summations.\n$$F_{interp} = F_{weights} + C \\cdot F_{sum, chan} = 19 + 15C$$\n\n**5. Per-Sample MLP Forward Pass FLOPs Derivation**\nThe FLOPs for the MLP forward pass, $F_{mlp}$, are calculated based on the given cost model. This includes the final residual addition.\n- **Input to Hidden Layer 1 ($d_{in} \\to W$):**\n  Matrix-vector multiplication: $2 \\cdot W \\cdot d_{\\text{in}} = 2W(C+3)$ FLOPs.\n  Bias addition: $W$ FLOPs.\n  ReLU activation: $W$ FLOPs.\n  Total: $2W(C+3) + 2W$.\n- **Hidden Layers ($W \\to W$):** For $L-1$ layers:\n  Matrix-vector multiplication: $2 \\cdot W \\cdot W = 2W^2$ FLOPs.\n  Bias addition: $W$ FLOPs.\n  ReLU activation: $W$ FLOPs.\n  Total per layer: $2W^2 + 2W$.\n  Total for $L-1$ layers: $(L-1)(2W^2 + 2W)$.\n- **Hidden Layer to Output ($W \\to d_{out}$):**\n  Matrix-vector multiplication: $2 \\cdot d_{\\text{out}} \\cdot W = 2CW$ FLOPs.\n  Bias addition: $d_{\\text{out}} = C$ FLOPs.\n  (No ReLU on the output).\n  Total: $2CW + C$.\n- **Residual Addition**: The MLP output (size $C$) is added to the interpolated feature (size $C$).\n  Total: $C$ FLOPs.\n\nSumming these costs gives the total per-sample FLOPs for the MLP part of the pipeline:\n$$F_{mlp} = \\left[ 2W(C+3) + 2W \\right] + \\left[ (L-1)(2W^2 + 2W) \\right] + \\left[ 2CW + C \\right] + C$$\n\n**6. Total FLOPs per Rendering Pass**\nThe total number of samples evaluated in a pass is $Q = R \\cdot S$.\nThe total FLOPs for interpolation is $F_{total, interp} = Q \\cdot F_{interp}$.\nThe total FLOPs for the MLP is $F_{total, mlp} = Q \\cdot F_{mlp}$.\nThe total FLOPs for the pass is the sum: $F_{pass} = F_{total, interp} + F_{total, mlp}$.\nTo convert from FLOPs to gigaflops (GFLOPs), where $1 \\text{ GFLOP} = 10^9 \\text{ FLOPs}$, we divide by $10^9$.\n$$ GFLOPs = \\frac{\\text{FLOPs}}{10^9} $$\nThe final quantities to be reported are the total, voxel, and MLP memory in MB, and the total, interpolation, and MLP costs in GFLOPs.",
            "answer": "```python\nimport math\n\ndef solve():\n    \"\"\"\n    Computes memory and computational costs for a hybrid rendering pipeline.\n    The function iterates through a set of test cases, calculates memory in megabytes (MB)\n    and floating-point operations in gigaflops (GFLOPs) based on derived formulas,\n    and prints the results in the specified format.\n    \"\"\"\n    test_cases = [\n        # (N, C, W, L, R, S)\n        (64, 8, 64, 2, 1024, 64),\n        (1, 1, 32, 1, 1, 1),\n        (16, 4, 256, 4, 512, 48),\n        (128, 16, 64, 2, 256, 32),\n        (32, 12, 1, 1, 2048, 8),\n    ]\n\n    # Universal constants\n    B = 4.0  # Bytes per 32-bit float\n    MB_DIV = 2.0**20  # Bytes in one megabyte (2^20)\n    GF_DIV = 10.0**9  # FLOPs in one gigaflop (10^9)\n\n    str_results = []\n    \n    for case in test_cases:\n        N, C, W, L, R, S = case\n\n        # 1. Memory Calculations\n        \n        # Voxel grid memory: N^3 vertices * C features * B bytes/feature\n        mem_voxel_bytes = (N**3) * C * B\n        \n        # MLP parameter count\n        # An MLP with L hidden layers has 1 input layer, L-1 hidden-to-hidden layers, and 1 output layer.\n        \n        # Parameters for input layer ((C+3) -> W): (C+3)*W weights + W biases\n        params_layer1 = (C + 3) * W + W\n        \n        # Parameters for L-1 hidden layers (W -> W): (W*W weights + W biases) per layer\n        params_hidden_layers = (L - 1) * (W**2 + W)\n        \n        # Parameters for output layer (W -> C): W*C weights + C biases\n        params_output_layer = W * C + C\n        \n        params_mlp = params_layer1 + params_hidden_layers + params_output_layer\n        mem_mlp_bytes = params_mlp * B\n        \n        mem_total_bytes = mem_voxel_bytes + mem_mlp_bytes\n\n        # Convert memory to Megabytes (MB)\n        voxel_mb = mem_voxel_bytes / MB_DIV\n        mlp_mb = mem_mlp_bytes / MB_DIV\n        total_mb = mem_total_bytes / MB_DIV\n\n        # 2. FLOPs Calculations\n        \n        # Total number of queries per rendering pass\n        Q = R * S\n\n        # Trilinear interpolation FLOPs per sample\n        # 19 FLOPs for weight computation + 15 FLOPs per channel for weighted sum\n        flops_interp_per_sample = 19 + 15 * C\n        total_interp_flops = Q * flops_interp_per_sample\n\n        # MLP FLOPs per sample\n        \n        # Input layer ((C+3) -> W): mat-vec mul + bias add + relu\n        flops_layer1 = 2 * W * (C + 3) + W + W\n        \n        # Hidden layers (W -> W): (mat-vec mul + bias add + relu) per layer\n        flops_hidden_layers = (L - 1) * (2 * W**2 + W + W)\n        \n        # Output layer (W -> C): mat-vec mul + bias add\n        flops_output_layer = 2 * W * C + C\n        \n        # Final residual addition\n        flops_residual = C\n        \n        flops_mlp_per_sample = flops_layer1 + flops_hidden_layers + flops_output_layer + flops_residual\n        total_mlp_flops = Q * flops_mlp_per_sample\n\n        total_flops = total_interp_flops + total_mlp_flops\n        \n        # Convert FLOPs to Gigaflops (GFLOPs)\n        interp_gf = total_interp_flops / GF_DIV\n        mlp_gf = total_mlp_flops / GF_DIV\n        total_gf = total_flops / GF_DIV\n        \n        # Format the output string for the current test case\n        case_str = (\n            f\"[{total_mb:.3f},{voxel_mb:.3f},{mlp_mb:.3f},\"\n            f\"{total_gf:.3f},{interp_gf:.3f},{mlp_gf:.3f}]\"\n        )\n        str_results.append(case_str)\n        \n    # Join all case results into a single string in list-of-lists format\n    final_output = f\"[{','.join(str_results)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}