{
    "hands_on_practices": [
        {
            "introduction": "理论是指导，但实践出真知。本节的第一个练习将理论与经验数据联系起来。我们将探索所谓的“缩放定律”（Scaling Laws），即描述模型性能如何随其规模（深度与宽度）、数据量和计算资源变化的经验关系。通过这个练习，你将亲手处理假设的实验数据，使用统计建模技术来量化网络深度 $L$ 和宽度 $w$ 对测试损失的独特影响，从而将观察性能趋势的直觉转化为严谨的定量理解 。",
            "id": "3157535",
            "problem": "您的任务是使用一个基于统计原理的程序，来估计测试损失如何随网络深度和宽度进行缩放。考虑一个由深度 $L$（正整数）和宽度 $w$（正整数）表征的神经网络族。对于一个固定的数据集规模体系（被视为一个分类组），观测到的经验测试损失遵循一个带有特定于组的比例性的乘性幂律：\n$$\nt \\approx A_g \\, L^{\\alpha} \\, w^{\\beta} \\cdot \\varepsilon,\n$$\n其中 $t$ 是观测到的测试损失（一个正实数），$A_g$ 是特定于组 $g$（数据集规模体系）的未知正常数，$\\alpha$ 和 $\\beta$ 是跨组不变的实值指数，而 $\\varepsilon$ 是一个正噪声因子。假设噪声是乘性的且为对数正态分布，在对数域中的均值为零，这意味着观测测试损失的自然对数具有加性的、独立同分布的高斯扰动。您的任务是使用一个与这些假设一致且基于统计原理的估计器来推断指数 $\\alpha$ 和 $\\beta$，同时汇集所有组的数据，并允许每个组有其自身的截距。\n\n您必须依赖的基础原理：\n- 将乘性关系转换为加性关系的自然对数变换的定义。\n- 高斯分布的性质，以及在加性高斯噪声下的最大似然估计与最小二乘估计器的等价性。\n- 线性最小二乘法和正规方程。\n\n您的程序必须实现一个基于这些原理的估计器，以从提供的观测数据中推断 $\\alpha$ 和 $\\beta$。将每个数据集规模视为一个独立的组，其拥有自己的常数 $A_g$，并通过在对数域中使用特定于组的截距最小化残差平方和，来联合估计所有组的 $\\alpha$ 和 $\\beta$。\n\n您将获得包含三个案例的以下测试套件。在每个案例中，您会得到按数据集规模（组标签显示为 $N$）分组的观测数据。每个观测数据是一个元组 $(L,w,t)$，其中 $L$ 是深度，$w$ 是宽度，$t$ 是观测到的测试损失。所有数字都是无量纲的。\n\n测试案例 1（理想情况，三个组，变化充分）：\n- 组 $N = 10^3$：\n  - $(L,w,t) = (\\,4,\\,128,\\,0.0883883476483184\\,)$\n  - $(L,w,t) = (\\,16,\\,128,\\,0.0441941738241592\\,)$\n- 组 $N = 10^4$：\n  - $(L,w,t) = (\\,8,\\,64,\\,0.0441941738241592\\,)$\n  - $(L,w,t) = (\\,8,\\,256,\\,0.0220970869120796\\,)$\n- 组 $N = 10^5$：\n  - $(L,w,t) = (\\,16,\\,256,\\,0.0125\\,)$\n  - $(L,w,t) = (\\,4,\\,64,\\,0.05\\,)$\n\n测试案例 2（可辨识性边界：恰好确定，两个组）：\n- 组 $N = 2 \\times 10^3$：\n  - $(L,w,t) = (\\,4,\\,64,\\,0.75\\,)$\n  - $(L,w,t) = (\\,8,\\,64,\\,0.375\\,)$\n- 组 $N = 2 \\times 10^4$：\n  - $(L,w,t) = (\\,8,\\,128,\\,0.15\\,)$\n  - $(L,w,t) = (\\,8,\\,256,\\,0.15\\,)$\n\n测试案例 3（边缘情况，$(L,w)$ 模式中存在近似共线性，两个组）：\n- 组 $N = 5 \\times 10^2$：\n  - $(L,w,t) = (\\,4,\\,64,\\,0.078125\\,)$\n  - $(L,w,t) = (\\,16,\\,64,\\,0.078125\\,)$\n- 组 $N = 5 \\times 10^3$：\n  - $(L,w,t) = (\\,8,\\,128,\\,0.01953125\\,)$\n  - $(L,w,t) = (\\,8,\\,256,\\,0.009765625\\,)$\n\n要求：\n- 将观测测试损失的对数建模为一个具有特定于组的截距和 $\\ln L$ 与 $\\ln w$ 的公共斜率的加性线性模型。使用一个在对数域中与高斯噪声假设下的最大似然估计一致的最小二乘估计器来估计 $\\alpha$ 和 $\\beta$。\n- 实现一个单一程序，处理所有三个测试案例（固定在您的代码中），并按顺序输出每个案例估计的 $\\alpha$ 和 $\\beta$，四舍五入到三位小数。如果一个值的绝对值小于 $5 \\times 10^{-4}$，则在四舍五入后将其打印为 $0.0$。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试案例的结果本身必须是一个双元素列表 $[\\alpha,\\beta]$。例如，一个有效的最终输出形式为 $[[a_1,b_1],[a_2,b_2],[a_3,b_3]]$，其中 $a_i$ 和 $b_i$ 是浮点数，打印时没有额外的空格。\n\n不涉及物理单位。不涉及角度。不使用百分比。每个测试案例的答案必须是一个浮点数列表。最终打印的行必须严格符合指定的格式。",
            "solution": "用户希望为一个神经网络测试损失模型找到缩放指数 $\\alpha$ 和 $\\beta$。\n\n### 步骤 1：提取已知条件\n- **模型**：测试损失 $t$ 被建模为一个乘性幂律：$t \\approx A_g \\, L^{\\alpha} \\, w^{\\beta} \\cdot \\varepsilon$。\n- **变量**：\n    - $t$：观测到的测试损失（正实数）。\n    - $L$：网络深度（正整数）。\n    - $w$：网络宽度（正整数）。\n    - $g$：组标识符，代表一个数据集规模体系（分类）。\n- **待估计参数**：\n    - $\\alpha, \\beta$：实值指数，跨组不变。\n    - $A_g$：特定于组 $g$ 的未知正常比例常数。\n- **噪声模型**：$\\varepsilon$ 是一个正的、乘性的、对数正态的噪声因子。这意味着 $\\ln(\\varepsilon)$ 服从均值为零的高斯分布。\n- **任务**：通过对模型的线性化版本使用最小二乘估计器，联合估计所有组的 $\\alpha$ 和 $\\beta$。该方法在指定的噪声模型下与最大似然估计（MLE）一致。\n- **数据**：提供了三个测试案例，每个案例都有一组按组分类的观测数据 $(L, w, t)$。\n- **输出格式**：对于每个测试案例，生成一个列表 $[\\alpha, \\beta]$，其值四舍五入到三位小数。绝对值小于 $5 \\times 10^{-4}$ 的值应报告为 $0.0$。最终输出应为这些列表的列表。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学依据**：该问题在深度学习缩放定律的实证研究中有充分的依据。幂律模型是一个标准的拟设（ansatz），而乘性对数正态噪声（在对数域中导致加性高斯模型）的假设，是用于此类模型中参数估计的常规且统计上合理的方法。该问题依赖于对数变换、线性回归和正规方程等基本原理。\n- **良定性**：该问题描述了一个线性模型（具体来说是协方差分析或 ANCOVA 模型）中参数的估计。只要设计矩阵是满秩的，就存在唯一解。测试案例的陈述中提到了“可辨识性边界”和“近似共线性”，这些都正确地暗示了回归分析中的标准条件，这些条件会影响解的稳定性和唯一性，表明这些是数据需要由估计器处理的特征，而非问题表述中的缺陷。\n- **客观性**：问题使用精确、客观的数学语言陈述，没有主观或模糊的术语。\n- **结论**：该问题在科学上是合理的、良定的、客观的，并包含了所有必要信息。它被判定为**有效**。\n\n### 步骤 3：行动\n继续提供一个完整、合理的解决方案。\n\n### 基于原理的解决方案设计\n核心原理是将乘性的、非线性的模型转换为加性的、线性的模型，然后可以使用标准的线性最小二乘法求解。这一做法的合理性在于对数正态噪声的假设，该假设在变换后变为加性高斯噪声。\n\n**1. 模型线性化**\n给定的模型是：\n$$\nt \\approx A_g \\, L^{\\alpha} \\, w^{\\beta} \\cdot \\varepsilon\n$$\n对两边取自然对数，可以使变量和参数之间的关系线性化：\n$$\n\\ln(t) \\approx \\ln(A_g \\, L^{\\alpha} \\, w^{\\beta} \\cdot \\varepsilon)\n$$\n利用对数的性质 $\\ln(ab) = \\ln(a) + \\ln(b)$ 和 $\\ln(a^b) = b\\ln(a)$，我们得到：\n$$\n\\ln(t) \\approx \\ln(A_g) + \\alpha \\ln(L) + \\beta \\ln(w) + \\ln(\\varepsilon)\n$$\n这个方程具有线性模型的形式。让我们为我们的回归定义变量：\n- 响应变量：$y = \\ln(t)$\n- 预测变量：$x_1 = \\ln(L)$ 和 $x_2 = \\ln(w)$\n- 参数：指数 $\\alpha$ 和 $\\beta$ 是斜率。项 $\\ln(A_g)$ 是每个组 $g$ 特有的截距。让我们将这个特定于组的截距表示为 $\\gamma_g = \\ln(A_g)$。\n- 误差项：$\\epsilon' = \\ln(\\varepsilon)$。根据问题陈述，$\\epsilon'$ 是一个从均值为零的高斯分布中抽取的加性噪声项。\n\n对于属于组 $g(i)$ 的第 $i$ 个观测，模型为：\n$$\ny_i = \\gamma_{g(i)} + \\alpha x_{1,i} + \\beta x_{2,i} + \\epsilon'_i\n$$\n\n**2. 最小二乘估计**\n我们的目标是找到参数值 $(\\hat{\\gamma}_g, \\hat{\\alpha}, \\hat{\\beta})$，以最小化残差平方和（SSR），即观测值 $y_i$ 和预测值之间差异的平方和。\n$$\n\\text{SSR} = \\sum_{i=1}^{n} (y_i - (\\hat{\\gamma}_{g(i)} + \\hat{\\alpha} x_{1,i} + \\hat{\\beta} x_{2,i}))^2\n$$\n在独立同分布的加性高斯噪声假设下，最小化此量等同于对参数进行最大似然估计。\n\n**3. 矩阵形式**\n这个问题可以用矩阵形式表示为一个通用线性模型 $\\mathbf{y} \\approx \\mathbf{X}\\boldsymbol{\\theta}$。\n- 假设总共有 $n$ 个观测值和 $k$ 个不同的组。\n- 参数向量 $\\boldsymbol{\\theta}$ 包含 $k$ 个组截距和两个公共斜率：\n$$\n\\boldsymbol{\\theta} = [\\gamma_1, \\gamma_2, \\dots, \\gamma_k, \\alpha, \\beta]^T\n$$\n这是一个大小为 $(k+2) \\times 1$ 的向量。\n- 响应向量 $\\mathbf{y}$ 包含所有 $n$ 个观测的测试损失的对数：\n$$\n\\mathbf{y} = [\\ln(t_1), \\ln(t_2), \\dots, \\ln(t_n)]^T\n$$\n- 设计矩阵 $\\mathbf{X}$ 是一个 $n \\times (k+2)$ 的矩阵。对于每个对应于组 $g(i)$ 的观测 $i$，$\\mathbf{X}$ 的第 $i$ 行构造如下：\n    - 前 $k$ 列是组员资格的指示变量（独热编码）。与组 $g(i)$ 对应的列将为 $1$，而其他 $k-1$ 个组的列将为 $0$。\n    - 第 $(k+1)$ 列包含值 $\\ln(L_i)$。\n    - 第 $(k+2)$ 列包含值 $\\ln(w_i)$。\n与组 $j$ 相关联的第 $i$ 行的结构是：\n$$\n\\mathbf{x}_i^T = [0, \\dots, 1, \\dots, 0, \\ln(L_i), \\ln(w_i)]\n$$\n其中 $1$ 在第 $j$ 个位置。\n\n**4. 求解系统**\n最小化 SSR 的最小二乘估计 $\\hat{\\boldsymbol{\\theta}}$ 由正规方程的解给出：\n$$\n\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\n在实践中，直接计算矩阵的逆可能在数值上不稳定。一个更稳健的方法是使用标准的线性最小二乘求解器，例如 `numpy.linalg.lstsq`，它通常采用 QR 分解或 SVD 等技术。\n\n一旦计算出向量 $\\hat{\\boldsymbol{\\theta}}$，$\\alpha$ 和 $\\beta$ 的估计值就是该向量的最后两个元素：\n$$\n\\hat{\\alpha} = \\hat{\\boldsymbol{\\theta}}_{k} \\quad (\\text{第 } (k+1)\\text{ 个元素})\n$$\n$$\n\\hat{\\beta} = \\hat{\\boldsymbol{\\theta}}_{k+1} \\quad (\\text{第 } (k+2)\\text{ 个元素})\n$$\n此过程将应用于所提供的三个测试案例中的每一个。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Processes all test cases to estimate scaling exponents alpha and beta.\n    \"\"\"\n    \n    test_cases = [\n        # Test Case 1 (happy path, three groups, ample variation)\n        {\n            \"10^3\": [\n                (4, 128, 0.0883883476483184),\n                (16, 128, 0.0441941738241592)\n            ],\n            \"10^4\": [\n                (8, 64, 0.0441941738241592),\n                (8, 256, 0.0220970869120796)\n            ],\n            \"10^5\": [\n                (16, 256, 0.0125),\n                (4, 64, 0.05)\n            ]\n        },\n        # Test Case 2 (boundary of identifiability: exactly determined, two groups)\n        {\n            \"2x10^3\": [\n                (4, 64, 0.75),\n                (8, 64, 0.375)\n            ],\n            \"2x10^4\": [\n                (8, 128, 0.15),\n                (8, 256, 0.15)\n            ]\n        },\n        # Test Case 3 (edge case with near-collinearity in (L,w) patterns, two groups)\n        {\n            \"5x10^2\": [\n                (4, 64, 0.078125),\n                (16, 64, 0.078125)\n            ],\n            \"5x10^3\": [\n                (8, 128, 0.01953125),\n                (8, 256, 0.009765625)\n            ]\n        }\n    ]\n\n    def custom_round(value):\n        \"\"\"\n        Rounds a value to 3 decimal places. If the magnitude is less than\n        5e-4, it returns 0.0.\n        \"\"\"\n        if abs(value)  5e-4:\n            return 0.0\n        return round(value, 3)\n\n    results_str_list = []\n    \n    for case in test_cases:\n        observations = []\n        group_indices = []\n        \n        # Assign an integer index to each unique group label\n        group_labels = sorted(case.keys())\n        group_map = {label: i for i, label in enumerate(group_labels)}\n        num_groups = len(group_map)\n\n        # Collect all observations and their corresponding group indices\n        for group_label, data_points in case.items():\n            group_idx = group_map[group_label]\n            for point in data_points:\n                observations.append(point)\n                group_indices.append(group_idx)\n\n        num_obs = len(observations)\n        \n        # Extract L, w, t and transform them to the log domain\n        L_vals = np.array([obs[0] for obs in observations])\n        w_vals = np.array([obs[1] for obs in observations])\n        t_vals = np.array([obs[2] for obs in observations])\n\n        ln_L = np.log(L_vals)\n        ln_w = np.log(w_vals)\n        y = np.log(t_vals) # This is the response vector\n        \n        # Construct the design matrix X\n        # Columns: k group intercepts, 1 for alpha (ln L), 1 for beta (ln w)\n        num_params = num_groups + 2\n        X = np.zeros((num_obs, num_params))\n\n        # Populate the one-hot encoded group intercept columns\n        X[np.arange(num_obs), group_indices] = 1\n        \n        # Populate the slope columns for alpha and beta\n        X[:, num_groups] = ln_L\n        X[:, num_groups + 1] = ln_w\n        \n        # Perform linear least squares regression\n        # theta = (X^T X)^-1 X^T y\n        theta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        \n        # Extract estimated alpha and beta from the parameter vector theta\n        alpha_hat = theta[num_groups]\n        beta_hat = theta[num_groups + 1]\n        \n        # Apply custom rounding as per problem specification\n        rounded_alpha = custom_round(alpha_hat)\n        rounded_beta = custom_round(beta_hat)\n\n        # Format the result for this case\n        results_str_list.append(f\"[{rounded_alpha},{rounded_beta}]\")\n\n    # Print the final output in the required format\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在宏观上理解了深度和宽度的影响后，我们现在来深入探究宽度为何如此强大。这个练习模拟了一个宽度剪枝实验，旨在找到“表征坍缩阈值”，即网络在一个小型数据集上能够表达任意函数所需的最小宽度。这个实践将为你提供一个具体的视角，让你明白网络宽度如何直接关联到其表征能力以及特征空间的维度 。",
            "id": "3157479",
            "problem": "您需要为一个完全指定的深度网络设计并实现一个受控的宽度剪枝实验，以确定当仅有最终线性读出层可训练时，固定深度的网络所实现的功能空间在哪个最小宽度阈值上会失去对固定有限数据集上任意标签的插值能力。该实验必须遵循线性代数的第一性原理和精确的架构定义。所有数学符号、函数、运算符和数字都必须用 LaTeX 书写。\n\n考虑以下设定。假设有一个数据集，包含 $m$ 个输入样本，收集为一个矩阵 $X \\in \\mathbb{R}^{m \\times d}$。考虑一个深度为 $L$ 的前馈网络，该网络有 $L$ 个隐藏层和一个单个线性输出神经元，其中只有最终线性读出层的权重是可训练的，而所有隐藏层的参数都是固定的。经过 $L$ 层后的隐藏表示记为 $H \\in \\mathbb{R}^{m \\times w}$，其中 $w$ 是每个隐藏层共享的宽度（通道数）。当仅训练最后一层来拟合标签向量 $y \\in \\mathbb{R}^m$ 时，最终的网络输出对应于求解一个关于特征 $H$ 的线性系统，因此，要能对 $m$ 个样本上的任意标签进行插值，需要 $H$ 的列空间维度为 $m$；等价地，$\\mathrm{rank}(H) = m$。\n\n架构。隐藏层的定义如下：\n- 第一个隐藏层：$h^{(1)} = \\mathrm{ReLU}(X A + \\mathbf{1} b^{(1)\\top})$，其中 $A \\in \\mathbb{R}^{d \\times W_0}$ 是初始第一层的权重矩阵，$b^{(1)} \\in \\mathbb{R}^{W_0}$ 是偏置向量，$\\mathbf{1} \\in \\mathbb{R}^{m}$ 是全1向量。\n- 对于 $\\ell \\in \\{2,\\dots,L\\}$ 的后续隐藏层：$h^{(\\ell)} = \\mathrm{ReLU}\\!\\left(h^{(\\ell-1)} \\odot s^{(\\ell)} + \\mathbf{1} b^{(\\ell)\\top}\\right)$，其中 $s^{(\\ell)} \\in \\mathbb{R}^{W_0}$ 是一个按通道的缩放向量，$b^{(\\ell)} \\in \\mathbb{R}^{W_0}$ 是一个按通道的偏置向量，$\\odot$ 表示在 $m$ 个样本上广播的逐元素乘法。\n- 经过如下定义的结构化宽度剪枝至宽度 $w$ 后，最终的特征矩阵为 $H = h^{(L)} \\in \\mathbb{R}^{m \\times w}$。\n\n宽度剪枝协议。您必须实现一个宽度剪枝过程，该过程使用从固定隐藏参数计算出的重要性度量，从初始宽度 $W_0$ 中选择一个大小为 $w$ 的通道子集。通道 $j \\in \\{1,\\dots,W_0\\}$ 的重要性定义为\n$$\nI_j \\;=\\; \\left\\|A_{:,j}\\right\\|_2 \\,\\times\\, \\prod_{\\ell=2}^{L}\\left|s^{(\\ell)}_j\\right| ,\n$$\n按照约定，当 $L = 1$ 时，空积等于 $1$。要剪枝到宽度 $w$，选择具有最大 $I_j$ 值的 $w$ 个通道并保留这些索引，同时保持它们的原始顺序。设选定的索引集为 $S_w \\subset \\{1,\\dots,W_0\\}$，且 $|S_w| = w$。剪枝后的隐藏参数即为 $A_{:,S_w}$，$b^{(1)}_{S_w}$，以及对于 $\\ell \\in \\{2,\\dots,L\\}$ 的向量 $s^{(\\ell)}_{S_w}$ 和 $b^{(\\ell)}_{S_w}$。应用上述前向传播定义和这些剪枝后的参数来生成 $H_w \\in \\mathbb{R}^{m \\times w}$。只有最后的线性读出层（不属于此计算部分）会被训练；因此，在 $m$ 个数据点上，仅通过改变最后一层可实现的功能空间是 $H_w$ 的列空间，其维度为 $\\mathrm{rank}(H_w)$。\n\n坍塌阈值。将坍塌阈值宽度 $w^\\star$ 定义为满足 $\\mathrm{rank}(H_w) = m$ 的最小宽度 $w \\in \\{1,\\dots,W_0\\}$。如果不存在这样的 $w$，则定义 $w^\\star = -1$。直观上，对于任何 $w  w^\\star$，功能空间会发生坍塌，即当仅训练最后一层时，它无法对 $m$ 个样本上的任意标签进行插值。\n\n数据和参数生成。对于每个测试用例，使用指定的整数种子初始化的伪随机数生成器，并从均值为零、方差为一的标准正态分布中独立抽取所有条目：\n- $X \\in \\mathbb{R}^{m \\times d}$,\n- $A \\in \\mathbb{R}^{d \\times W_0}$,\n- $b^{(1)} \\in \\mathbb{R}^{W_0}$,\n- 对于每个 $\\ell \\in \\{2,\\dots,L\\}$: $s^{(\\ell)} \\in \\mathbb{R}^{W_0}$ 和 $b^{(\\ell)} \\in \\mathbb{R}^{W_0}$.\n\n秩的计算。给定 $H_w \\in \\mathbb{R}^{m \\times w}$，使用基于奇异值分解的标准定义和一个数值稳定的阈值来计算 $\\mathrm{rank}(H_w)$。您可以使用一个基于奇异值分解、经过充分测试的用于计算矩阵秩的数值线性代数例程。\n\n任务。实现一个程序，对下面的每个测试用例，构建数据和参数，对所有候选宽度 $w \\in \\{1,2,\\dots,W_0\\}$ 执行剪枝，计算每个 $w$ 对应的 $\\mathrm{rank}(H_w)$，并返回如上定义的阈值 $w^\\star$。\n\n测试套件。使用以下四个测试用例，每个用例以元组 $(L, d, m, W_0, \\text{seed})$ 的形式给出：\n- 用例 1: $(L, d, m, W_0, \\text{seed}) = (2, 8, 6, 16, 1)$。\n- 用例 2: $(L, d, m, W_0, \\text{seed}) = (4, 8, 6, 12, 2)$。\n- 用例 3 (边缘情况，其中 $W_0  m$): $(L, d, m, W_0, \\text{seed}) = (3, 8, 10, 7, 3)$。\n- 用例 4 (边界情况，其中 $L=1$): $(L, d, m, W_0, \\text{seed}) = (1, 6, 5, 5, 4)$。\n\n要求的最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的结果顺序与上述测试套件的顺序相同。例如，如果阈值为 $a_1,a_2,a_3,a_4$，则需精确打印单行\n$[a_1,a_2,a_3,a_4]$。\n\n此问题中没有物理单位、角度单位和百分比。所有输出均为整数，其中值 $-1$ 用于表示对于给定的测试用例，没有宽度能够达到 $\\mathrm{rank}(H_w)=m$。",
            "solution": "问题陈述已经过分析并被确定为有效。它在科学上基于神经网络容量理论，在数学上是适定的，并为要执行的计算实验提供了完整、客观且可形式化的描述。所有术语都得到了清晰的定义，并且为每个测试用例指定了所需的参数。\n\n解决方案通过为每个测试用例实现指定的计算实验来展开。目标是找到坍塌阈值宽度 $w^\\star$，它被定义为使得最终隐藏表示矩阵 $H_w$ 的秩等于数据样本数 $m$ 的最小宽度 $w \\in \\{1, \\dots, W_0\\}$。如果不存在这样的宽度，则 $w^\\star = -1$。\n\n对于单个测试用例 $(L, d, m, W_0, \\text{seed})$，总体流程如下：\n\n1.  **参数生成**：使用提供的 `seed` 初始化伪随机数生成器。数据矩阵 $X \\in \\mathbb{R}^{m \\times d}$ 和所有固定的网络参数——$A \\in \\mathbb{R}^{d \\times W_0}$、$b^{(1)} \\in \\mathbb{R}^{W_0}$，以及对于每一层 $\\ell \\in \\{2, \\dots, L\\}$ 的缩放向量和偏置向量 $s^{(\\ell)}, b^{(\\ell)} \\in \\mathbb{R}^{W_0}$——都从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取。\n\n2.  **重要性分数计算**：根据以下公式计算 $W_0$ 个初始通道中每个通道（由 $j \\in \\{1, \\dots, W_0\\}$ 索引）的重要性 $I_j$：\n    $$\n    I_j = \\left\\|A_{:,j}\\right\\|_2 \\times \\prod_{\\ell=2}^{L}\\left|s^{(\\ell)}_j\\right|\n    $$\n    其中，$A_{:,j}$ 是第一层权重矩阵 $A$ 的第 $j$ 列。L2范数 $\\| \\cdot \\|_2$ 衡量了连接输入特征到第 $j$ 个通道的权重的量级。连乘项汇集了所有后续层中按通道缩放因子的量级。对于 $L=1$ 的情况，连乘为空，按惯例定义为 $1$。计算并存储所有 $W_0$ 个通道的重要性分数。\n\n3.  **通道剪枝顺序**：根据其重要性分数 $I_j$ 对通道进行降序排名。这决定了在剪枝网络中考虑纳入通道的顺序。具体来说，我们获得一个从最重要到最不重要的通道索引 $\\{1, \\dots, W_0\\}$ 的有序列表。\n\n4.  **搜索坍塌阈值 $w^\\star$**：任务的核心是找到满足秩条件的最小宽度 $w$。\n    -   矩阵秩的一个基本性质是 $\\mathrm{rank}(H_w) \\le \\min(m, w)$。要满足条件 $\\mathrm{rank}(H_w) = m$，必须有 $w \\ge m$。因此，如果最大可能宽度 $W_0$ 小于样本数 $m$，则不可能存在解。在这种情况下，根据定义 $w^\\star = -1$。\n    -   如果 $W_0 \\ge m$，我们搜索满足条件的最小 $w$。搜索通过按升序测试宽度 $w$ 来进行。由于任何宽度 $w  m$ 都无法满足条件，搜索可以有效地从 $w=m$ 开始，一直进行到 $W_0$。\n    -   对于每个候选宽度 $w \\in \\{m, m+1, \\dots, W_0\\}$：\n        a.  **选择通道**：选择重要性分数最高的 $w$ 个通道。设它们的原始索引为集合 $S_w$。为了保留原始通道编号，这些索引在使用前会按升序排序，以对参数张量进行切片。\n        b.  **构建剪枝网络**：通过从原始参数张量中选择与 $S_w$ 中索引对应的列和元素，来形成宽度为 $w$ 的剪枝网络的参数：$A_{:,S_w}$、$b^{(1)}_{S_w}$、$s^{(\\ell)}_{S_w}$ 和 $b^{(\\ell)}_{S_w}$。\n        c.  **前向传播**：将输入数据 $X$ 通过剪枝网络传播，以计算最终的隐藏表示矩阵 $H_w \\in \\mathbb{R}^{m \\times w}$。\n            -   第一个隐藏层的激活为 $h^{(1)} = \\mathrm{ReLU}(X A_{:,S_w} + \\mathbf{1} (b^{(1)}_{S_w})^\\top)$。\n            -   对于层 $\\ell \\in \\{2, \\dots, L\\}$，激活被迭代更新：$h^{(\\ell)} = \\mathrm{ReLU}(h^{(\\ell-1)} \\odot s^{(\\ell)}_{S_w} + \\mathbf{1} (b^{(\\ell)}_{S_w})^\\top)$，其中 $\\odot$ 是带广播的逐元素乘法。\n            -   最终矩阵是 $H_w = h^{(L)}$。\n        d.  **秩计算**：使用基于奇异值分解 (SVD) 的数值稳定方法计算所得矩阵 $H_w$ 的秩，该方法由标准数值库提供。\n        e.  **条件检查**：如果 $\\mathrm{rank}(H_w) = m$，那么 $w$ 就是（从 $m$ 开始）满足插值条件的最小宽度。该值被记录为 $w^\\star$，当前测试用例的搜索终止。\n\n5.  **最终结果**：如果搜索循环完成而没有找到合适的 $w$，则意味着直到 $W_0$ 的所有宽度都不能满足秩条件，因此将 $w^\\star$ 设置为 $-1$。收集每个测试用例计算出的 $w^\\star$ 值。最终输出是这些值的列表。",
            "answer": "```python\nimport numpy as np\n\ndef compute_w_star(L: int, d: int, m: int, W0: int, seed: int) - int:\n    \"\"\"\n    Computes the collapse threshold width w_star for a single test case.\n\n    Args:\n        L: Number of hidden layers.\n        d: Input dimension.\n        m: Number of samples.\n        W0: Initial width of hidden layers.\n        seed: Seed for the pseudorandom number generator.\n\n    Returns:\n        The collapse threshold width w_star, or -1 if no such width exists.\n    \"\"\"\n    # According to the problem, rank(H_w) must be m. The rank of a matrix\n    # cannot exceed its number of columns, w. Thus, if W0  m, it's impossible\n    # to achieve rank m.\n    if W0  m:\n        return -1\n\n    # 1. Parameter Generation\n    rng = np.random.default_rng(seed)\n    X = rng.standard_normal(size=(m, d))\n    A = rng.standard_normal(size=(d, W0))\n    b1 = rng.standard_normal(size=W0)\n    \n    s_params = []\n    b_params = []\n    if L  1:\n        for _ in range(2, L + 1):\n            s_params.append(rng.standard_normal(size=W0))\n            b_params.append(rng.standard_normal(size=W0))\n\n    # 2. Importance Score Calculation\n    # I_j = ||A_(:,j)||_2 * product(|s^(l)_j|) for l=2..L\n    A_col_norms = np.linalg.norm(A, axis=0)\n    \n    # The product is 1 if L=1 (empty product)\n    s_prods = np.ones(W0)\n    if L  1:\n        for s_l in s_params:\n            s_prods *= np.abs(s_l)\n            \n    importances = A_col_norms * s_prods\n\n    # 3. Channel Pruning Order\n    # Get indices of channels sorted by importance in descending order\n    sorted_channel_indices = np.argsort(importances)[::-1]\n\n    # 4. Search for Collapse Threshold w_star\n    # Search starts from w=m, as rank(H_w)  m for w  m is not possible.\n    # The first w >= m that satisfies the condition is the minimal one.\n    for w in range(m, W0 + 1):\n        # a. Select Channels\n        top_w_indices = sorted_channel_indices[:w]\n        # Sort indices to preserve original order as per problem spec\n        current_indices = np.sort(top_w_indices)\n\n        # b. Construct Pruned Network\n        A_w = A[:, current_indices]\n        b1_w = b1[current_indices]\n\n        # c. Forward Pass\n        h = np.maximum(0, X @ A_w + b1_w)\n        \n        if L  1:\n            for l_idx in range(L - 1):\n                s_l_w = s_params[l_idx][current_indices]\n                b_l_w = b_params[l_idx][current_indices]\n                h = np.maximum(0, h * s_l_w + b_l_w)\n        \n        H_w = h\n        \n        # d. Rank Computation and e. Condition Check\n        if np.linalg.matrix_rank(H_w) == m:\n            return w\n\n    # If the loop finishes, no width w in [m, W0] satisfied the condition\n    return -1\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (2, 8, 6, 16, 1),  # Case 1\n        (4, 8, 6, 12, 2),  # Case 2\n        (3, 8, 10, 7, 3),  # Case 3\n        (1, 6, 5, 5, 4),   # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        L, d, m, W0, seed = case\n        w_star = compute_w_star(L, d, m, W0, seed)\n        results.append(w_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "增加网络深度并非简单地堆叠更多层，因为它可能导致梯度消失等优化难题。这个练习将研究深度网络成功的关键功臣——跳跃连接（Skip Connections）。通过计算带有不同频率跳跃连接的网络中路径的数量和长度，你将能定量地看到这些“捷径”如何缩短了梯度的有效传播路径，这是成功训练极深模型的根本原因之一 。",
            "id": "3157518",
            "problem": "考虑一个带有残差跳跃连接的前馈深度神经网络的有向无环图 (DAG) 表示。该网络具有索引为 $0,1,2,\\dots,L$ 的层，其中层 $0$ 是输入层，层 $L$ 是输出层。每个层的宽度固定为 $w$ 个神经元。对于所有 $i \\in \\{0,1,\\dots,L-1\\}$，从层 $i$ 中的每个神经元到层 $i+1$ 中的每个神经元都存在一个完全二分边集。此外，以指定的频率参数 $s \\in \\mathbb{N}$ 添加跳跃连接：如果 $i \\equiv 0 \\pmod{s}$ 且 $i+2 \\leq L$，则从层 $i$ 中的每个神经元到层 $i+2$ 中的每个神经元都存在一个完全二分边集。不存在其他跳跃连接。\n\n从输出层 $L$ 到输入层 $0$ 的有向路径通过反向遍历边来定义（等效于从 $0$ 到 $L$ 的前向遍历）。对于任何路径，将梯度路径长度 $N_{\\text{edges}}$ 定义为该路径遍历的边数。假设在一个线性化状态下，每条边的单位局部雅可比增益为1。在此假设下，每条神经元到神经元的路径对梯度幅度的贡献相等，因此梯度路径长度的分布与通过计算神经元到神经元路径所产生的分布一致。\n\n对于给定的 $L$、$w$ 和 $s$，定义以下量：\n- 有效深度利用率 $U$，定义为从层 $0$ 到层 $L$ 的所有神经元到神经元路径的期望边数 $\\mathbb{E}[N_{\\text{edges}}]$ 除以 $L$，即 $U = \\mathbb{E}[N_{\\text{edges}}] / L$。这衡量了在存在跳跃连接的情况下，梯度通常利用的层间转换的比例。\n- 最小梯度路径长度 $N_{\\min}$，定义为从层 $0$ 到层 $L$ 的所有有效路径中可能的最小边数。\n- 达到最小梯度路径长度 $N_{\\min}$ 的神经元到神经元路径的比例 $F_{\\min}$，计算方法为最小长度的神经元到神经元路径数量与神经元到神经元路径总数的比率。\n\n您的任务是编写一个完整的程序，对于固定的宽度 $w$，通过精确组合计数法计算几种 $(L,s)$ 配置下的 $U$ 和 $F_{\\min}$。您必须使用以下基本依据，从第一性原理推导出这些量：\n- 反向传播的链式法则，它意味着梯度传播遵循有向计算路径，并沿边进行乘法累积。\n- 前馈分层架构和作为 DAG 中边的残差跳跃连接的定义。\n- 通过动态规划对 DAG 中的路径进行计数，以及层间的完全二分连接性使每遍历一条边，神经元到神经元的路径数乘以 $w$ 这一事实。\n\n除了这些基本依据，您不得假设或使用任何捷径公式。特别是，您不得为这种特定的跳跃方案假设路径计数的闭式解；相反，计数必须是构造性地执行。\n\n测试套件：\n使用固定宽度 $w = 8$ 并评估以下四种情况：\n1. $(L,s) = (10,1)$\n2. $(L,s) = (10,2)$\n3. $(L,s) = (11,3)$\n4. $(L,s) = (10,64)$\n\n对于每种情况，计算如上定义的 $U$ 和 $F_{\\min}$。不涉及任何物理单位。将 $U$ 和 $F_{\\min}$ 表示为四舍五入到六位小数的十进制浮点数。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素对应一个测试用例，本身是一个双元素列表 $[U,F_{\\min}]$，两个条目都四舍五入到六位小数。例如，输出应如下所示：\n$[[U_1,F_1],[U_2,F_2],[U_3,F_3],[U_4,F_4]]$，\n不打印任何额外文本。",
            "solution": "用户的请求是解决一个与特定类型神经网络图中的路径计数相关的计算问题。我将首先根据指定的标准验证问题陈述。\n\n### 第一步：提取已知条件\n- **网络结构**：代表前馈神经网络的有向无环图 (DAG)。\n- **层**：从 $0$（输入）到 $L$（输出）进行索引。\n- **层宽度**：每个层 $i$ 都有固定的宽度 $w$ 个神经元。\n- **顺序连接**：对于所有 $i \\in \\{0, 1, \\dots, L-1\\}$，一个完全二分边集将层 $i$ 中的每个神经元连接到层 $i+1$ 中的每个神经元。\n- **跳跃连接**：当且仅当 $i \\equiv 0 \\pmod{s}$ 且 $i+2 \\leq L$ 时，从层 $i$ 到层 $i+2$ 存在一个完全二分边集形式的跳跃连接，其中 $s \\in \\mathbb{N}$ 是一个频率参数。\n- **路径定义**：从层 $0$ 到层 $L$ 的有向路径。\n- **梯度路径长度 ($N_{\\text{edges}}$)**：路径遍历的边数。\n- **线性化状态假设**：每条边的单位局部雅可比增益为1，使得梯度路径长度的分布等同于神经元到神经元路径计数的分布。\n- **待计算的量**：\n    1.  **有效深度利用率 ($U$)**：$U = \\mathbb{E}[N_{\\text{edges}}] / L$，其中 $\\mathbb{E}[N_{\\text{edges}}]$ 是从层 $0$ 到层 $L$ 的所有神经元到神经元路径的期望边数。\n    2.  **最小路径比例 ($F_{\\min}$)**：长度为最小长度 ($N_{\\min}$) 的路径数量与从层 $0$ 到层 $L$ 的总路径数量之比。\n- **推导约束**：这些量必须从第一性原理（反向传播、DAG结构、组合计数）推导得出，而不是使用针对此特定问题的现有闭式解。\n- **测试套件**：\n    - 固定宽度：$w = 8$。\n    - 情况1：$(L, s) = (10, 1)$\n    - 情况2：$(L, s) = (10, 2)$\n    - 情况3：$(L, s) = (11, 3)$\n    - 情况4：$(L, s) = (10, 64)$\n- **输出格式**：一个单行字符串 `[[U_1,F_1],[U_2,F_2],[U_3,F_3],[U_4,F_4]]`，其中的值四舍五入到六位小数。\n\n### 第二步：使用提取的已知条件进行验证\n根据验证标准对问题陈述进行评估：\n\n- **科学基础**：该问题是图论在深度神经网络理想化模型中的应用。层、跳跃连接（如ResNets中）、路径计数及其与梯度流的关系等概念在深度学习领域是标准且完善的。该问题在科学上和数学上都是合理的。\n- **定义明确**：图结构由参数 $L$、$w$ 和 $s$ 明确定义。待计算的量 $U$ 和 $F_{\\min}$ 也被精确定义。该问题是自洽的，并且对于每个测试用例都有唯一可计算的解。\n- **客观性**：问题以精确的数学语言陈述，没有任何主观性或模糊性。\n\n该问题没有表现出任何列举的无效性缺陷。它是一个可形式化、完整且可验证的数学问题，需要实质性的推理。\n\n### 第三步：结论与行动\n问题是**有效的**。将推导并提供一个解决方案。\n\n### 基于原理的设计\n\n该问题要求我们分析DAG中路径长度的分布。路径是从第0层开始到第L层结束的神经元序列。网络的结构高度规则，这允许使用一种系统性的计数方法。我们将按照要求，使用动态规划来构造性地计算路径。\n\n**对称性与简化**\n网络表现出高度的对称性。给定层内的所有神经元在拓扑上是等价的。源于第0层中任意一个 $w$ 神经元的路径具有相同的长度分布。因此，我们可以通过计算源于第0层中*单个任意源神经元*的路径来简化问题。我们感兴趣的量，$U$ 和 $F_{\\min}$，是比率，其中起始神经元的总数 ($w$) 将作为分子和分母中的公因子出现，从而相互抵消。\n\n**动态规划公式**\n让我们为我们的动态规划方法定义状态。我们需要跟踪到达每一层的不同长度的路径数量。\n\n设 $C_j$ 是一个数据结构（具体来说，是一个映射或字典），它存储了从第0层中单个源神经元开始，到第j层中*任何*神经元结束的所有路径的长度分布。映射的键将是路径长度 $k$（边数），值将是此类路径的总计数。\n\n**基本情况**\n该过程从第0层开始。我们认为从源神经元到其自身的路径长度为0。这作为我们递推的基础。因此，第0层的分布是：\n$$C_0 = \\{0: 1\\}$$\n这表示有一条长度为0的路径在第0层结束。\n\n**递推关系**\n我们可以通过考虑所有可能的前驱层来计算第 $j  0$ 层的路径分布 $C_j$。根据问题描述，第 $j$ 层的神经元可以从以下位置到达：\n1.  **第 $j-1$ 层（顺序连接）**：从第 $j-1$ 层到第 $j$ 层总是有连接。\n2.  **第 $j-2$ 层（跳跃连接）**：当且仅当 $j-2 \\geq 0$ 且 $(j-2) \\pmod{s} = 0$ 时，从第 $j-2$ 层到第 $j$ 层存在连接。\n\n考虑一条长度为 $k'$ 的路径，它结束于某个前驱层（例如，第 $j-1$ 层）的某个神经元。要将此路径扩展到第 $j$ 层，我们需遍历一条额外的边。由于从第 $j-1$ 层中任何神经元到第 $j$ 层的连接是完全二分的，这条单一路径可以扩展为 $w$ 条不同的路径，每条对应第 $j$ 层中的一个神经元。\n\n因此，对于每条结束于第 $j-1$ 层、长度为 $k'$ 的路径（总计数为 $C_{j-1}[k']$），我们生成 $w \\times C_{j-1}[k']$ 条新的长度为 $k'+1$ 并结束于第 $j$ 层的路径。\n\n应用此逻辑，我们推导出构建 $C_j$ 的递推公式：\n1.  将 $C_j$ 初始化为空映射。\n2.  **来自第 $j-1$ 层的贡献**：对于 $C_{j-1}$ 中的每个长度 $k'$ 及其计数 $N_{k'}$，将 $w \\cdot N_{k'}$ 添加到 $C_j$ 中长度为 $k'+1$ 的计数中。\n3.  **来自第 $j-2$ 层的贡献**：如果存在从第 $j-2$ 层到第 $j$ 层的跳跃连接，则对于 $C_{j-2}$ 中的每个长度 $k'$ 及其计数 $N_{k'}$，将 $w \\cdot N_{k'}$ 添加到 $C_j$ 中长度为 $k'+1$ 的计数中。\n\n对 $j=1, 2, \\dots, L$ 迭代此过程。最终的映射 $C_L$ 将包含从第0层源神经元到第L层所有神经元的完整路径长度分布。\n\n**最终量的计算**\n一旦计算出 $C_L$，我们就可以确定所需的量。设 $C_L = \\{k_1: N_1, k_2: N_2, \\dots\\}$。\n\n1.  **路径总数 ($N_{\\text{total}}$)**：这是所有路径长度的计数总和。\n    $$N_{\\text{total}} = \\sum_{i} N_i$$\n2.  **期望路径长度 ($\\mathbb{E}[N_{\\text{edges}}]$)**：这是路径长度的加权平均值，其中权重是路径计数。\n    $$\\mathbb{E}[N_{\\text{edges}}] = \\frac{\\sum_{i} k_i \\cdot N_i}{N_{\\text{total}}}$$\n3.  **有效深度利用率 ($U$)**：\n    $$U = \\frac{\\mathbb{E}[N_{\\text{edges}}]}{L}$$\n4.  **最小路径长度 ($N_{\\min}$)**：这只是映射 $C_L$ 中的最小键。\n    $$N_{\\min} = \\min(\\{k_1, k_2, \\dots\\})$$\n5.  **最小路径比例 ($F_{\\min}$)**：这是长度为 $N_{\\min}$ 的路径计数除以路径总数。\n    $$F_{\\min} = \\frac{C_L[N_{\\min}]}{N_{\\text{total}}}$$\n\n计数值可能会变得非常大，但 Python 的任意精度整数可以处理这种情况而不会溢出。计算 $U$ 和 $F_{\\min}$ 的最终除法将使用浮点运算完成。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the path counting problem for the specified test cases.\n    \"\"\"\n\n    # Fixed parameters as per the problem statement.\n    w = 8\n    \n    # Test cases: tuples of (L, s).\n    test_cases = [\n        (10, 1),\n        (10, 2),\n        (11, 3),\n        (10, 64)\n    ]\n\n    results = []\n\n    for L, s in test_cases:\n        # C is a list of dictionaries. C[j] will store the path length distribution\n        # for paths from a single source neuron in layer 0 to all neurons in layer j.\n        # C[j][k] = count of paths with length k.\n        C = [{} for _ in range(L + 1)]\n\n        # Base case: A path of length 0 from the source neuron to itself.\n        C[0] = {0: 1}\n\n        # Dynamic programming to compute path distributions for each layer.\n        for j in range(1, L + 1):\n            \n            current_dist = {}\n\n            # 1. Contribution from the sequential connection (layer j-1)\n            # Paths to layer j-1 are extended by one edge.\n            prev_dist_seq = C[j-1]\n            for k, count in prev_dist_seq.items():\n                new_length = k + 1\n                # Each of the 'count' paths can be extended to 'w' new paths,\n                # one for each neuron in layer j.\n                num_new_paths = w * count\n                current_dist[new_length] = current_dist.get(new_length, 0) + num_new_paths\n\n            # 2. Contribution from the skip connection (layer j-2), if it exists.\n            if j - 2 = 0 and (j - 2) % s == 0:\n                prev_dist_skip = C[j-2]\n                for k, count in prev_dist_skip.items():\n                    new_length = k + 1\n                    num_new_paths = w * count\n                    current_dist[new_length] = current_dist.get(new_length, 0) + num_new_paths\n            \n            C[j] = current_dist\n\n        # The final distribution of path lengths to layer L.\n        final_dist = C[L]\n        \n        # Calculate the required quantities from the final distribution.\n        # Python's integers handle arbitrarily large numbers, preventing overflow.\n        total_paths = 0\n        weighted_sum_of_lengths = 0\n        \n        for k, count in final_dist.items():\n            total_paths += count\n            weighted_sum_of_lengths += k * count\n\n        # Calculate U (Effective Depth Utilization)\n        if total_paths  0:\n            expected_n_edges = float(weighted_sum_of_lengths) / float(total_paths)\n            U = expected_n_edges / L\n        else:\n            U = 0.0\n\n        # Calculate F_min (Fraction of Minimal Paths)\n        if total_paths  0:\n            n_min = min(final_dist.keys())\n            count_min_paths = final_dist[n_min]\n            F_min = float(count_min_paths) / float(total_paths)\n        else:\n            F_min = 0.0\n\n        # Store results rounded to six decimal places.\n        results.append([round(U, 6), round(F_min, 6)])\n\n    # Format the final output string as required.\n    # e.g., [[U1,F1],[U2,F2],...]\n    output_str = \"[\" + \",\".join([f\"[{u},{f}]\" for u, f in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}