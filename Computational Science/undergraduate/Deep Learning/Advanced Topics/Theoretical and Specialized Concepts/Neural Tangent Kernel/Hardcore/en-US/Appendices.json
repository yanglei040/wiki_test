{
    "hands_on_practices": [
        {
            "introduction": "The Neural Tangent Kernel (NTK) provides a powerful description of an infinitely wide network's behavior, but it can often feel abstract. This first exercise  makes the concept concrete by examining an NTK derived from a simple model built with Fourier features. By connecting the kernel's structure to the well-understood Fourier basis, you will gain a tangible understanding of what a kernel's eigenfunctions represent and how they are determined by the underlying model's features.",
            "id": "3159099",
            "problem": "You are given a one-dimensional real input domain with points sampled on a periodic interval. Consider the Neural Tangent Kernel (NTK) associated with a linear-in-parameters model built from fixed Fourier features. Specifically, let the model be defined by\n$$\nf(x;\\theta) \\;=\\; \\sqrt{\\alpha_0}\\,a_0 \\cdot \\phi_0(x) \\;+\\; \\sum_{k=1}^{M} \\sqrt{\\alpha_k}\\,\\Big(a_k^{(c)} \\,\\phi_k^{(c)}(x) \\;+\\; a_k^{(s)} \\,\\phi_k^{(s)}(x)\\Big),\n$$\nwith parameters $\\theta = \\big(a_0, \\{a_k^{(c)}, a_k^{(s)}\\}_{k=1}^M\\big)$ and fixed features\n$$\n\\phi_0(x)=1,\\quad \\phi_k^{(c)}(x)=\\cos(2\\pi k x),\\quad \\phi_k^{(s)}(x)=\\sin(2\\pi k x),\n$$\nfor integer $k \\ge 1$. Here $\\alpha_k \\ge 0$ are nonnegative scalars that set the relative contribution of each frequency. Using the definition of the Neural Tangent Kernel (NTK),\n$$\nk(x,x') \\;=\\; \\nabla_{\\theta} f(x;\\theta_0)^{\\top} \\nabla_{\\theta} f(x';\\theta_0),\n$$\nshow that for this model the kernel is\n$$\nk(x,x') \\;=\\; \\alpha_0 \\;+\\; \\sum_{k=1}^{M} \\alpha_k \\,\\cos\\!\\big(2\\pi k (x - x')\\big),\n$$\nwhere all angles are in radians. Consider the discrete set of evenly spaced points on the unit circle,\n$$\nx_n \\;=\\; \\frac{n}{N}, \\quad n \\in \\{0,1,\\dots,N-1\\},\n$$\nand let $K \\in \\mathbb{R}^{N\\times N}$ be the kernel matrix with entries $K_{n,m} = k(x_n,x_m)$. Note that $K$ is real symmetric and depends only on the difference $n-m$ modulo $N$, hence it is circulant.\n\nTasks:\n1) Compute the eigendecomposition $K = U \\Lambda U^{\\top}$ numerically, where $U \\in \\mathbb{R}^{N\\times N}$ is orthonormal and $\\Lambda \\in \\mathbb{R}^{N\\times N}$ is diagonal with nonnegative entries. \n2) Relate the eigenfunctions to Fourier modes: For a circulant matrix, the complex Discrete Fourier Transform (DFT) matrix diagonalizes $K$, and the eigenvalues are given by the DFT of the first row of $K$. Use this fact to produce an analytical reference for the eigenvalues by computing the DFT of the first row. \n3) Validate the numerical eigendecomposition by comparing its eigenvalues to the analytical eigenvalues from the DFT of the first row and report the maximum absolute difference.\n\nYour program must implement the above for the following test suite. For each test case, you are given $N$ and a dictionary of nonzero $\\alpha_k$ values. Any $\\alpha_k$ not listed should be treated as zero. All angles are in radians.\n\nTest suite:\n- Test 1 (single nonzero frequency): $N=8$, $\\{\\alpha_0=0.0,\\;\\alpha_1=2.0\\}$.\n- Test 2 (mixture including the Nyquist term): $N=16$, $\\{\\alpha_0=1.5,\\;\\alpha_1=1.0,\\;\\alpha_2=0.5,\\;\\alpha_8=0.25\\}$.\n- Test 3 (pure constant kernel): $N=10$, $\\{\\alpha_0=3.0\\}$.\n\nYour program should:\n- Construct $K$ using the exact kernel formula above for each test case.\n- Compute the numerical eigenvalues of $K$ via a symmetric eigensolver.\n- Compute the analytical eigenvalues as the complex DFT of the first row of $K$.\n- Compare the sorted lists of numerical and analytical eigenvalues by computing the maximum absolute difference for each test case.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the maximum absolute difference (a float) for the corresponding test case, rounded to ten decimal places, in the order of the tests above. For example, an output of the form \"[0.0,0.0,0.0]\" is acceptable if the differences are numerically zero to within rounding. No extra text should be printed.",
            "solution": "We begin from the definition of the Neural Tangent Kernel (NTK). For a model $f(x;\\theta)$, the NTK at an initialization $\\theta_0$ is\n$$\nk(x,x') \\;=\\; \\nabla_{\\theta} f(x;\\theta_0)^{\\top}\\nabla_{\\theta} f(x';\\theta_0).\n$$\nWhen the model is linear in its parameters, with a fixed feature map, the gradient with respect to parameters is independent of $\\theta_0$ and equals the feature vector. In the present case, the model is\n$$\nf(x;\\theta) \\;=\\; \\sqrt{\\alpha_0}\\,a_0\\,\\phi_0(x) \\;+\\; \\sum_{k=1}^{M} \\sqrt{\\alpha_k}\\,\\Big(a_k^{(c)} \\,\\phi_k^{(c)}(x) \\;+\\; a_k^{(s)} \\,\\phi_k^{(s)}(x)\\Big).\n$$\nThe gradient with respect to parameters is\n$$\n\\nabla_{\\theta} f(x;\\theta) \\;=\\; \\big[\\sqrt{\\alpha_0}\\,\\phi_0(x),\\;\\{\\sqrt{\\alpha_k}\\,\\phi_k^{(c)}(x),\\sqrt{\\alpha_k}\\,\\phi_k^{(s)}(x)\\}_{k=1}^M\\big]^{\\top}.\n$$\nThe NTK is therefore the inner product of these gradients:\n$$\n\\begin{aligned}\nk(x,x') \n&= \\alpha_0 \\,\\phi_0(x)\\phi_0(x') \\;+\\; \\sum_{k=1}^{M} \\alpha_k \\,\\Big(\\phi_k^{(c)}(x)\\phi_k^{(c)}(x') \\;+\\; \\phi_k^{(s)}(x)\\phi_k^{(s)}(x')\\Big) \\\\\n&= \\alpha_0 \\cdot 1 \\cdot 1 \\;+\\; \\sum_{k=1}^{M} \\alpha_k \\,\\Big(\\cos(2\\pi k x)\\cos(2\\pi k x') \\;+\\; \\sin(2\\pi k x)\\sin(2\\pi k x')\\Big).\n\\end{aligned}\n$$\nUsing the trigonometric identity $\\cos(A)\\cos(B)+\\sin(A)\\sin(B)=\\cos(A-B)$, we obtain\n$$\nk(x,x') \\;=\\; \\alpha_0 \\;+\\; \\sum_{k=1}^{M} \\alpha_k \\,\\cos\\!\\big(2\\pi k (x - x')\\big).\n$$\nAll angles are in radians. This kernel is translation-invariant on the circle, depending only on the difference $x-x'$ modulo $1$.\n\nNow consider the discrete set of points $x_n = n/N$ for $n\\in\\{0,\\dots,N-1\\}$. Define the kernel matrix $K \\in \\mathbb{R}^{N\\times N}$ by $K_{n,m} = k(x_n,x_m)$. Because $k$ depends only on $x_n - x_m$ modulo $1$, and $x_n - x_m = (n-m)/N$ modulo $1$, $K$ depends only on $(n-m)\\bmod N$. Therefore, $K$ is a real symmetric circulant matrix. A core property of circulant matrices is that they are diagonalized by the complex Discrete Fourier Transform (DFT) matrix. Let $F$ be the $N\\times N$ DFT matrix with entries $F_{r,m} = \\exp\\!\\big(-2\\pi i r m / N\\big)$ for $r,m\\in\\{0,\\dots,N-1\\}$. Then,\n$$\nK \\;=\\; \\frac{1}{N} F^{\\ast} \\,\\mathrm{diag}(\\lambda_0,\\dots,\\lambda_{N-1})\\, F,\n$$\nwhere $(\\lambda_r)_{r=0}^{N-1}$ are the eigenvalues of $K$, and are given by the DFT of the first row of $K$. If the first row is $(c_0,\\dots,c_{N-1})$, then\n$$\n\\lambda_r \\;=\\; \\sum_{m=0}^{N-1} c_m \\, e^{-2\\pi i r m / N}, \\quad r=0,\\dots,N-1.\n$$\nBecause $K$ is real symmetric and $c_m$ is real with $c_m=c_{N-m}$, the eigenvalues $\\lambda_r$ are real and nonnegative. The corresponding eigenvectors are the complex Fourier modes, and in the real eigendecomposition these appear as cosine and sine pairs spanning the same eigenspaces.\n\nAlgorithmic plan:\n1) Construct the kernel matrix $K$ from the given $N$ and $\\{\\alpha_k\\}$ using\n$$\nK_{n,m} \\;=\\; \\alpha_0 \\;+\\; \\sum_{k\\ge 1} \\alpha_k \\,\\cos\\!\\Big(2\\pi k \\,\\frac{n-m}{N}\\Big).\n$$\n2) Compute the numerical eigendecomposition using a symmetric eigensolver to obtain the eigenvalues. Sort them in ascending order.\n3) Compute the analytical eigenvalues by taking the complex DFT of the first row of $K$. Extract the real part (numerical imaginary parts are due to floating-point rounding) and sort in ascending order.\n4) For each test case, report the maximum absolute difference between the sorted numerical eigenvalues and the sorted analytical eigenvalues. A small difference validates both the eigendecomposition and the relation between eigenfunctions and Fourier modes.\n\nTest suite specifics:\n- Test 1: $N=8$, $\\alpha_0=0.0$, $\\alpha_1=2.0$. The kernel is rank-$2$ with nonzero eigenvalues at frequencies $1$ and $7$; each expected to be $N\\alpha_1/2 = 8$.\n- Test 2: $N=16$, $\\alpha_0=1.5$, $\\alpha_1=1.0$, $\\alpha_2=0.5$, $\\alpha_8=0.25$. The expected eigenvalues are at $r=0$ with $N\\alpha_0=24$, at $r=1,15$ with $N\\alpha_1/2=8$, at $r=2,14$ with $N\\alpha_2/2=4$, and at the Nyquist frequency $r=8$ with $N\\alpha_8=4$. All others are zero.\n- Test 3: $N=10$, $\\alpha_0=3.0$. This is a pure constant kernel with a single nonzero eigenvalue $N\\alpha_0=30$ at $r=0$ and zeros elsewhere.\n\nThe program implements these steps and prints a single line containing a list of three floating-point numbers: the maximum absolute eigenvalue differences for Tests 1â€“3, rounded to ten decimal places. This directly connects the eigendecomposition of the NTK on evenly spaced points to Fourier modes via the circulant structure and the DFT.",
            "answer": "```python\nimport numpy as np\n\ndef build_kernel_and_first_row(N: int, alpha: dict) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Build the NTK kernel matrix K and its first row c for evenly spaced points x_n = n/N on [0,1).\n    Kernel: k(x, x') = alpha_0 + sum_{k>=1} alpha_k cos(2*pi*k*(x - x')).\n    Angles are in radians.\n\n    Parameters:\n        N: number of evenly spaced points.\n        alpha: dict mapping frequency k to alpha_k (nonnegative). Unspecified k treated as 0.\n\n    Returns:\n        K: NxN kernel matrix.\n        c: length-N first row of K.\n    \"\"\"\n    n = np.arange(N)\n    # Differences tau = n - m mod N; shape (N, N)\n    tau = (n[:, None] - n[None, :]) % N\n    tau = tau.astype(float)\n\n    # Start with alpha_0 term\n    a0 = alpha.get(0, 0.0)\n    K = np.full((N, N), fill_value=a0, dtype=float)\n\n    # Add cosine terms for k >= 1\n    for k, ak in alpha.items():\n        if k == 0 or ak == 0.0:\n            continue\n        # cos(2*pi*k*(n-m)/N)\n        K += ak * np.cos(2.0 * np.pi * k * tau / N)\n\n    # First row c_m = K_{0, m}\n    c = K[0, :].copy()\n    return K, c\n\ndef numerical_sorted_eigenvalues(K: np.ndarray) -> np.ndarray:\n    \"\"\"Compute and return sorted eigenvalues of a real symmetric matrix K.\"\"\"\n    w = np.linalg.eigh(K)[0]\n    return np.sort(w)\n\ndef analytical_sorted_eigenvalues_from_first_row(c: np.ndarray) -> np.ndarray:\n    \"\"\"\n    For a circulant matrix with first row c, eigenvalues are the DFT of c.\n    Return sorted real parts of these eigenvalues.\n    \"\"\"\n    lam = np.fft.fft(c)\n    lam_real = np.real_if_close(lam, tol=1000)  # strip negligible imaginary parts\n    lam_real = lam_real.astype(float)\n    return np.sort(lam_real)\n\ndef max_abs_diff(a: np.ndarray, b: np.ndarray) -> float:\n    \"\"\"Maximum absolute difference between two arrays of same shape.\"\"\"\n    return float(np.max(np.abs(a - b)))\n\ndef run_test_cases():\n    # Define the test cases as per the problem statement.\n    test_cases = [\n        # Test 1: N=8, alpha_0=0.0, alpha_1=2.0\n        {\"N\": 8, \"alpha\": {0: 0.0, 1: 2.0}},\n        # Test 2: N=16, alpha_0=1.5, alpha_1=1.0, alpha_2=0.5, alpha_8=0.25\n        {\"N\": 16, \"alpha\": {0: 1.5, 1: 1.0, 2: 0.5, 8: 0.25}},\n        # Test 3: N=10, alpha_0=3.0\n        {\"N\": 10, \"alpha\": {0: 3.0}},\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        alpha = case[\"alpha\"]\n        K, c = build_kernel_and_first_row(N, alpha)\n\n        # Numerical eigenvalues (sorted)\n        w_num = numerical_sorted_eigenvalues(K)\n\n        # Analytical eigenvalues via DFT of first row (sorted)\n        w_ana = analytical_sorted_eigenvalues_from_first_row(c)\n\n        # Compute maximum absolute difference\n        diff = max_abs_diff(w_num, w_ana)\n        results.append(diff)\n\n    # Print results as a single line with specified format: list of floats rounded to 10 decimal places.\n    formatted = \"[\" + \",\".join(f\"{val:.10f}\" for val in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    run_test_cases()\n```"
        },
        {
            "introduction": "Building on the concept of a kernel's eigensystem, this practice explores its functional consequences for learning. The NTK doesn't just describe the model; it dictates the dynamics of training. This exercise  uses a small, concrete kernel matrix to let you directly observe how gradient descent behaves, demonstrating that target functions aligned with large-eigenvalue eigenvectors are learned much faster. This provides a hands-on feel for the principle that networks have an \"inductive bias\" to learn certain functions more easily than others.",
            "id": "3159036",
            "problem": "You are given a small Neural Tangent Kernel (NTK) Gram matrix and asked to analyze gradient descent dynamics in the corresponding kernel regression on the training set. The Neural Tangent Kernel (NTK) Gram matrix $K \\in \\mathbb{R}^{n \\times n}$ is symmetric positive semidefinite and acts as a linear operator on the vector of predictions over the $n$ training examples. The task is to demonstrate, by explicit computation, that when the target vector is aligned with the top eigenvector of $K$, the error under kernel gradient descent decays rapidly in a few steps.\n\nFundamental base and setup:\n- The kernel regression training loss on the training set is the squared error $L(f) = \\tfrac{1}{2} \\lVert f - y \\rVert_2^2$, where $f \\in \\mathbb{R}^n$ is the vector of model predictions on the $n$ training points, and $y \\in \\mathbb{R}^n$ is the target vector.\n- Kernel gradient descent (in function space) with learning rate $\\eta$ and NTK Gram matrix $K$ performs the update\n$$\nf_{t+1} = f_t - \\eta \\, K \\,(f_t - y),\n$$\nstarting from $f_0 = 0$. This follows from applying gradient descent to $L(f)$, since $\\nabla_f L = f - y$, and the NTK preconditions updates by $K$ in function space.\n- Let the eigendecomposition be $K = U \\Lambda U^\\top$, where $U = [u_1, \\dots, u_n]$ is orthonormal and $\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$ with $\\lambda_1 \\ge \\cdots \\ge \\lambda_n \\ge 0$. Any target $y$ decomposes as $y = \\sum_{i=1}^n c_i u_i$, where $c_i = u_i^\\top y$.\n\nYour program must:\n1. Use the fixed NTK Gram matrix\n$$\nK = \\begin{bmatrix}\n2 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 2\n\\end{bmatrix}.\n$$\n2. Compute its eigendecomposition to obtain eigenvalues and corresponding unit eigenvectors $u_1, u_2, u_3$, with $u_1$ associated with the largest eigenvalue.\n3. For each test case, construct the specified target $y \\in \\mathbb{R}^3$, run kernel gradient descent for the given number of steps $T$ with learning rate $\\eta$, starting from $f_0 = 0$, and compute the training loss\n$$\nL_T = \\tfrac{1}{2} \\lVert f_T - y \\rVert_2^2.\n$$\n4. Output the results for all test cases as a single line containing a Python-style list with each float rounded to six decimal places.\n\nTest suite:\n- Case $1$: $y = u_1$ (unit vector), $\\eta = 0.25$, $T = 3$.\n- Case $2$: $y = u_3$ (unit vector), $\\eta = 0.25$, $T = 3$.\n- Case $3$: $y = \\mathrm{normalize}(0.8\\, u_1 + 0.2\\, u_3)$ to unit norm, $\\eta = 0.25$, $T = 3$. Here $0.8$ and $0.2$ are real coefficients and $\\mathrm{normalize}(\\cdot)$ scales the vector to have unit Euclidean norm.\n- Case $4$: $y = u_1$, $\\eta = 0.1$, $T = 3$.\n- Case $5$: $y = u_1$, $\\eta = 0.25$, $T = 1$.\n\nAngle units are not applicable. There are no physical units. All outputs must be floats.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, e.g., $[r_1,r_2,r_3,r_4,r_5]$, where each $r_i$ is the value of $L_T$ for the corresponding test case, rounded to six decimal places.",
            "solution": "The problem is valid as it is scientifically grounded in the theory of kernel methods, well-posed with a complete and consistent set of definitions and data, and objectively formulated. We shall proceed with a full solution.\n\nThe problem asks us to analyze the dynamics of kernel gradient descent for a specific Neural Tangent Kernel (NTK) Gram matrix $K$. We will first derive a general analytical expression for the training loss dynamics and then apply it to the given matrix and test cases. This will demonstrate how the convergence rate depends on the alignment of the target vector with the eigenvectors of the kernel matrix.\n\nLet $f_t \\in \\mathbb{R}^n$ be the vector of model predictions on the $n$ training points at step $t$, and let $y \\in \\mathbb{R}^n$ be the target vector. The training loss is the mean squared error $L(f) = \\frac{1}{2} \\lVert f - y \\rVert_2^2$.\nThe kernel gradient descent update rule, with learning rate $\\eta$ and NTK Gram matrix $K$, is given as:\n$$\nf_{t+1} = f_t - \\eta K (f_t - y)\n$$\nWe start from the initial condition $f_0 = 0$. Let the error vector at step $t$ be $e_t = f_t - y$. The initial error is $e_0 = f_0 - y = -y$.\nWe can express the update rule in terms of the error vector by subtracting $y$ from both sides:\n$$\nf_{t+1} - y = (f_t - y) - \\eta K (f_t - y)\n$$\n$$\ne_{t+1} = e_t - \\eta K e_t = (I - \\eta K) e_t\n$$\nwhere $I$ is the $n \\times n$ identity matrix. This is a linear dynamical system for the error vector. By unrolling the recursion, we find the error at step $t$:\n$$\ne_t = (I - \\eta K)^t e_0 = - (I - \\eta K)^t y\n$$\nThe training loss at step $T$ is $L_T = \\frac{1}{2} \\lVert e_T \\rVert_2^2$. Substituting the expression for $e_T$:\n$$\nL_T = \\frac{1}{2} \\left\\lVert - (I - \\eta K)^T y \\right\\rVert_2^2 = \\frac{1}{2} \\left\\lVert (I - \\eta K)^T y \\right\\rVert_2^2\n$$\nTo analyze this expression, we use the eigendecomposition of the symmetric matrix $K$, given by $K = U \\Lambda U^\\top$. Here, $U$ is an orthonormal matrix whose columns are the eigenvectors $u_1, u_2, \\dots, u_n$, and $\\Lambda$ is a diagonal matrix of the corresponding eigenvalues $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$.\nThe operator $(I - \\eta K)$ can be diagonalized in the same basis:\n$$\nI - \\eta K = U I U^\\top - \\eta U \\Lambda U^\\top = U (I - \\eta \\Lambda) U^\\top\n$$\nRaising this to the power of $T$:\n$$\n(I - \\eta K)^T = U (I - \\eta \\Lambda)^T U^\\top\n$$\nSubstituting this into the expression for $L_T$:\n$$\nL_T = \\frac{1}{2} \\left\\lVert U (I - \\eta \\Lambda)^T U^\\top y \\right\\rVert_2^2\n$$\nSince $U$ is an orthonormal matrix, it preserves the Euclidean norm, i.e., $\\lVert Uv \\rVert_2 = \\lVert v \\rVert_2$ for any vector $v$. Therefore:\n$$\nL_T = \\frac{1}{2} \\left\\lVert (I - \\eta \\Lambda)^T U^\\top y \\right\\rVert_2^2\n$$\nThe vector $U^\\top y$ represents the coordinates of $y$ in the basis of eigenvectors. Its $i$-th component is $c_i = u_i^\\top y$. The matrix $(I - \\eta \\Lambda)^T$ is diagonal with entries $(1 - \\eta \\lambda_i)^T$. Applying this diagonal matrix to the vector $U^\\top y$ scales its $i$-th component by $(1 - \\eta \\lambda_i)^T$. The squared norm of the resulting vector is the sum of the squares of its components:\n$$\nL_T = \\frac{1}{2} \\sum_{i=1}^n \\left( c_i (1 - \\eta \\lambda_i)^T \\right)^2 = \\frac{1}{2} \\sum_{i=1}^n c_i^2 (1 - \\eta \\lambda_i)^{2T}\n$$\nThis final expression reveals the core principle: the contribution of each eigen-component of the target $y$ to the loss decays exponentially. The rate of decay for the $i$-th component is determined by the factor $(1 - \\eta \\lambda_i)^2$. For a stable learning rate where $0 < \\eta \\lambda_i < 2$, this factor is less than $1$. A larger eigenvalue $\\lambda_i$ results in a smaller factor $(1 - \\eta \\lambda_i)^2$, leading to faster convergence for that component.\n\nNow, we perform the explicit computation for the given problem.\nThe NTK Gram matrix is:\n$$\nK = \\begin{bmatrix} 2 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 2 \\end{bmatrix}\n$$\nThe eigenvalues are the roots of the characteristic equation $\\det(K - \\lambda I) = 0$, which are $\\lambda_1 = 2 + \\sqrt{2}$, $\\lambda_2 = 2$, and $\\lambda_3 = 2 - \\sqrt{2}$. The corresponding orthonormal eigenvectors are:\n$$\nu_1 = \\begin{pmatrix} 1/2 \\\\ \\sqrt{2}/2 \\\\ 1/2 \\end{pmatrix}, \\quad u_2 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 0 \\\\ -1/\\sqrt{2} \\end{pmatrix}, \\quad u_3 = \\begin{pmatrix} 1/2 \\\\ -\\sqrt{2}/2 \\\\ 1/2 \\end{pmatrix}\n$$\nWe now evaluate the loss $L_T$ for each test case.\n\nCase $1$: $y = u_1$, $\\eta = 0.25$, $T = 3$.\nThe target $y$ is aligned with the first eigenvector $u_1$. The coefficients are $c_1 = u_1^\\top y = u_1^\\top u_1 = 1$, $c_2 = 0$, $c_3 = 0$.\nThe loss is determined solely by the first eigen-component:\n$L_3 = \\frac{1}{2} c_1^2 (1 - \\eta \\lambda_1)^{2T} = \\frac{1}{2} (1 - 0.25 (2+\\sqrt{2}))^6 = \\frac{1}{2} (0.5 - \\frac{\\sqrt{2}}{4})^6 \\approx 0.000004$.\n\nCase $2$: $y = u_3$, $\\eta = 0.25$, $T = 3$.\nThe target $y$ is aligned with the third eigenvector $u_3$. The coefficients are $c_1 = 0$, $c_2 = 0$, $c_3 = 1$.\nThe loss is determined solely by the third eigen-component:\n$L_3 = \\frac{1}{2} c_3^2 (1 - \\eta \\lambda_3)^{2T} = \\frac{1}{2} (1 - 0.25 (2-\\sqrt{2}))^6 = \\frac{1}{2} (0.5 + \\frac{\\sqrt{2}}{4})^6 \\approx 0.192317$.\nThe convergence is much slower compared to Case $1$ because $\\lambda_3$ is small.\n\nCase $3$: $y = \\mathrm{normalize}(0.8 u_1 + 0.2 u_3)$, $\\eta = 0.25$, $T = 3$.\nLet $v = 0.8 u_1 + 0.2 u_3$. Then $y = v / \\lVert v \\rVert_2$. Since $u_1$ and $u_3$ are orthonormal, $\\lVert v \\rVert_2 = \\sqrt{0.8^2 + 0.2^2} = \\sqrt{0.68}$.\nThe coefficients are $c_1 = u_1^\\top y = 0.8/\\sqrt{0.68}$ and $c_3 = u_3^\\top y = 0.2/\\sqrt{0.68}$, with $c_2 = 0$.\nSo, $c_1^2 = 0.64/0.68 = 16/17$ and $c_3^2 = 0.04/0.68 = 1/17$.\nThe loss is a weighted sum of the contributions from the first and third components:\n$L_3 = \\frac{1}{2} [c_1^2 (1 - \\eta \\lambda_1)^6 + c_3^2 (1 - \\eta \\lambda_3)^6] = \\frac{1}{2} [ \\frac{16}{17} (0.5 - \\frac{\\sqrt{2}}{4})^6 + \\frac{1}{17} (0.5 + \\frac{\\sqrt{2}}{4})^6 ] \\approx 0.011317$.\nThe final error is dominated by the slowly decaying component associated with $u_3$.\n\nCase $4$: $y = u_1$, $\\eta = 0.1$, $T = 3$.\nThis is similar to Case $1$ but with a smaller learning rate. $c_1 = 1$, $c_2 = 0$, $c_3 = 0$.\n$L_3 = \\frac{1}{2} (1 - 0.1 \\lambda_1)^6 = \\frac{1}{2} (1 - 0.1(2+\\sqrt{2}))^6 = \\frac{1}{2} (0.8 - \\frac{\\sqrt{2}}{10})^6 \\approx 0.040727$.\nThe smaller learning rate results in a decay factor $(1 - \\eta \\lambda_1)$ closer to $1$, hence slower convergence compared to Case $1$.\n\nCase $5$: $y = u_1$, $\\eta = 0.25$, $T = 1$.\nThis is similar to Case $1$ but with fewer steps. $c_1 = 1$, $c_2 = 0$, $c_3 = 0$.\n$L_1 = \\frac{1}{2} (1 - \\eta \\lambda_1)^{2 \\times 1} = \\frac{1}{2} (1 - 0.25(2+\\sqrt{2}))^2 = \\frac{1}{2} (0.5 - \\frac{\\sqrt{2}}{4})^2 \\approx 0.010723$.\nWith only one step, the error has less time to decay compared to Case $1$.\n\nThe results confirm the theoretical analysis: components of the target aligned with eigenvectors corresponding to larger eigenvalues of the NTK Gram matrix are learned much more rapidly under kernel gradient descent.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the training loss for kernel gradient descent under several scenarios.\n    \"\"\"\n    # 1. Define the fixed NTK Gram matrix K.\n    K = np.array([\n        [2.0, 1.0, 0.0],\n        [1.0, 2.0, 1.0],\n        [0.0, 1.0, 2.0]\n    ])\n\n    # 2. Compute the eigendecomposition of K.\n    # np.linalg.eigh returns eigenvalues in ascending order. We sort them descending.\n    eigenvalues, eigenvectors = np.linalg.eigh(K)\n    sort_indices = np.argsort(eigenvalues)[::-1]\n    # lambdas = eigenvalues[sort_indices] # Not used directly in simulation\n    U = eigenvectors[:, sort_indices]\n\n    # Extract the unit eigenvectors u1, u2, u3.\n    u1 = U[:, 0]\n    # u2 = U[:, 1] # Not used in test cases\n    u3 = U[:, 2]\n\n    # 3. Define the test suite.\n    # Each case is a tuple: (y_description, eta, T)\n    # y_description is a tuple: (type, spec)\n    test_cases = [\n        # Case 1: y = u1, eta = 0.25, T = 3\n        (('eigenvector', u1), 0.25, 3),\n        # Case 2: y = u3, eta = 0.25, T = 3\n        (('eigenvector', u3), 0.25, 3),\n        # Case 3: y = normalize(0.8*u1 + 0.2*u3), eta = 0.25, T = 3\n        (('combo', (0.8, u1, 0.2, u3)), 0.25, 3),\n        # Case 4: y = u1, eta = 0.1, T = 3\n        (('eigenvector', u1), 0.1, 3),\n        # Case 5: y = u1, eta = 0.25, T = 1\n        (('eigenvector', u1), 0.25, 1),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        y_desc, eta, T = case\n        y_type, y_spec = y_desc\n\n        # Construct the target vector y for the current case.\n        if y_type == 'eigenvector':\n            y = y_spec\n        elif y_type == 'combo':\n            c1, v1, c2, v2 = y_spec\n            v = c1 * v1 + c2 * v2\n            y = v / np.linalg.norm(v)\n        else:\n            raise ValueError(\"Unknown y_type\")\n        \n        # Initialize predictions f_0 = 0.\n        f = np.zeros_like(y)\n\n        # Run kernel gradient descent for T steps.\n        for _ in range(T):\n            gradient_term = f - y\n            update = K @ gradient_term\n            f = f - eta * update\n        \n        # This is f_T. Now compute the final loss L_T.\n        loss = 0.5 * np.linalg.norm(f - y)**2\n        results.append(loss)\n\n    # 4. Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The ultimate goal of theory is to guide practice. This final exercise bridges the gap between NTK analysis and practical application by using the kernel as a tool for model design. Here, we frame architecture selection as a search for the kernel that is best \"aligned\" with the target function you wish to learn . By calculating this alignment for different kernels on various learning tasks, you will see how the NTK framework can help you make principled architectural choices without the need for exhaustive trial-and-error training.",
            "id": "3159100",
            "problem": "You are given a selection criterion for choosing a neural network architecture based on the Neural Tangent Kernel (NTK), defined as the architecture that maximizes the expected alignment between a target function and the kernel regression predictor induced by the architecture's NTK. Specifically, for a target function $f^*$ and an architecture $a$ with an associated positive semi-definite kernel $k_a$, the kernel regression predictor $f_{K_a}$ trained on a dataset $\\{(x_i, y_i)\\}_{i=1}^n$ with ridge parameter $\\lambda > 0$ is defined by\n$$\nf_{K_a}(x) = k_a(x, X)^\\top (K_a + \\lambda I)^{-1} y,\n$$\nwhere $X = [x_1,\\dots,x_n]$, $y = [y_1,\\dots,y_n]^\\top$, $K_a$ is the Gram matrix with entries $(K_a)_{ij} = k_a(x_i, x_j)$, $k_a(x, X) = [k_a(x, x_1), \\dots, k_a(x, x_n)]^\\top$, and $I$ is the identity matrix of size $n \\times n$. The alignment criterion is\n$$\nA_a = \\mathbb{E}_{x}\\left[f^*(x)\\, f_{K_a}(x)\\right],\n$$\nwhich you will approximate by a Monte Carlo average over test points drawn from the specified input distribution:\n$$\nA_a \\approx \\frac{1}{M} \\sum_{m=1}^M f^*(x_m)\\, f_{K_a}(x_m).\n$$\nYour task is to implement a program that, for each test case below, computes $A_a$ for each architecture $a$ in the provided family and outputs the index of the architecture that maximizes $A_a$.\n\nArchitectures and kernels:\n- Architecture $a_0$ (index $0$): linear kernel $k_{a_0}(x, x') = x^\\top x' + 1$.\n- Architecture $a_1$ (index $1$): degree-$2$ polynomial kernel $k_{a_1}(x, x') = \\left(x^\\top x' + 1\\right)^2$.\n- Architecture $a_2$ (index $2$): an NTK-inspired arc-cosine-plus-linear kernel\n$$\nk_{a_2}(x, x') = \\alpha \\|x\\| \\|x'\\| \\left(\\sin \\theta + (\\pi - \\theta) \\cos \\theta\\right) + \\beta\\, x^\\top x' + \\gamma,\n$$\nwhere $\\theta$ is the angle (in radians) between $x$ and $x'$, defined by $\\cos \\theta = \\frac{x^\\top x'}{\\|x\\| \\|x'\\|}$ when $\\|x\\| \\neq 0$ and $\\|x'\\| \\neq 0$, and $\\theta = 0$ otherwise. Use constants $\\alpha = \\frac{1}{2\\pi}$, $\\beta = 1$, and $\\gamma = 10^{-3}$.\n\nFor all cases, training labels are noise-free and defined as $y_i = f^*(x_i)$.\n\nTest Suite:\nCase $1$ (happy path, linear target):\n- Dimension $d = 3$.\n- Training size $n = 20$.\n- Input distribution: multivariate normal $\\mathcal{N}(0, I_d)$.\n- Target function: $f^*(x) = a^\\top x + b$, with $a = (1, -2, 0.5)$ and $b = 0.3$.\n- Ridge parameter: $\\lambda = 10^{-6}$.\n- Monte Carlo budget: $M = 400$ test points.\n- Random seed: $42$.\n\nCase $2$ (quadratic target, uniform distribution):\n- Dimension $d = 2$.\n- Training size $n = 25$.\n- Input distribution: uniform on the hypercube $[-1, 1]^d$.\n- Target function: $f^*(x) = x^\\top B x + b$, with $B = \\mathrm{diag}(2, -1)$ and $b = 0$.\n- Ridge parameter: $\\lambda = 10^{-5}$.\n- Monte Carlo budget: $M = 500$ test points.\n- Random seed: $123$.\n\nCase $3$ (mixed nonlinearity and anisotropy, edge stability):\n- Dimension $d = 4$.\n- Training size $n = 15$.\n- Input distribution: multivariate normal with diagonal covariance $\\mathrm{diag}(1, 0.1, 0.1, 0.01)$ and mean $0$.\n- Target function: $f^*(x) = x_1^2 - 0.5 x_2 + 0.1 \\sin(3 x_3) + 0.05 x_4$, where the argument of the sine is in radians.\n- Ridge parameter: $\\lambda = 10^{-4}$.\n- Monte Carlo budget: $M = 600$ test points.\n- Random seed: $7$.\n\nCase $4$ (structured manifold, angle-based target):\n- Dimension $d = 2$.\n- Training size $n = 12$.\n- Input distribution: points on the unit circle parameterized by an angle $\\theta$ uniformly sampled from $[0, 2\\pi]$, with $x = (\\cos \\theta, \\sin \\theta)$.\n- Target function: $f^*(x) = \\cos(2\\theta)$, with $\\theta$ in radians.\n- Ridge parameter: $\\lambda = 10^{-6}$.\n- Monte Carlo budget: $M = 720$ test points.\n- Random seed: $99$.\n\nYour program must implement:\n1. Construction of the training set $\\{(x_i, y_i)\\}_{i=1}^n$ for each case according to its distribution and target function $f^*$.\n2. Computation of $f_{K_a}(x)$ for each architecture $a \\in \\{0,1,2\\}$ via the kernel regression formula with the specified $\\lambda$.\n3. Monte Carlo approximation of $A_a$ using the specified $M$ test points drawn from the same distribution as the training inputs for the corresponding case.\n4. Selection of the architecture index $a^\\star = \\arg\\max_a A_a$ for each case.\n\nFinal Output Format:\nYour program should produce a single line of output containing the selected indices for the four cases as a comma-separated list enclosed in square brackets, for example, $[0,1,2,1]$. Angles must be treated in radians wherever they appear. No physical units apply in this problem. All numeric outputs are integers.",
            "solution": "The problem requires us to select an optimal neural network architecture from a given set of three candidates for four different learning tasks. The selection criterion is to maximize the *expected alignment*, denoted $A_a$, between a known target function $f^*$ and the predictor $f_{K_a}$ produced by a given architecture $a$. This problem is firmly grounded in the principles of statistical learning theory, specifically using kernel methods as a lens to analyze the properties of neural networks via the Neural Tangent Kernel (NTK).\n\nThe problem is well-posed and all necessary components are defined. For each architecture $a$, identified by its kernel $k_a$, the kernel regression predictor $f_{K_a}$ on a training set $\\{(x_i, y_i)\\}_{i=1}^n$ is given by:\n$$\nf_{K_a}(x) = k_a(x, X)^\\top (K_a + \\lambda I)^{-1} y\n$$\nHere, $X$ is the $n \\times d$ matrix of training inputs, $y$ is the $n \\times 1$ vector of training labels, $K_a$ is the $n \\times n$ Gram matrix with entries $(K_a)_{ij} = k_a(x_i, x_j)$, $k_a(x, X)$ is a vector of kernel evaluations between $x$ and all training inputs, $\\lambda > 0$ is a regularization parameter, and $I$ is the $n \\times n$ identity matrix. The condition $\\lambda > 0$ ensures that the matrix $(K_a + \\lambda I)$ is invertible, guaranteeing a unique solution.\n\nThe alignment $A_a = \\mathbb{E}_{x}\\left[f^*(x)\\, f_{K_a}(x)\\right]$ is approximated using a Monte Carlo estimate over a set of $M$ test points $\\{x_m\\}_{m=1}^M$:\n$$\nA_a \\approx \\frac{1}{M} \\sum_{m=1}^M f^*(x_m)\\, f_{K_a}(x_m)\n$$\n\nOur procedure to solve this problem for each of the $4$ test cases will be as follows:\n1.  **Data Generation**: For each case, we first set the specified random seed to ensure reproducibility. We then generate a training set of size $n$ and a test set of size $M$ by drawing samples from the specified input distribution. The training labels $y_i$ and true test labels $f^*(x_m)$ are computed using the given target function $f^*$.\n2.  **Model Evaluation for Each Architecture**: For each of the $3$ candidate architectures ($a_0, a_1, a_2$), we calculate the alignment score $A_a$. This calculation proceeds via these steps:\n    a.  **Pre-computation of Weights**: We compute the vector of weights $\\alpha_a = (K_a + \\lambda I)^{-1} y$. This is done by first constructing the $n \\times n$ training Gram matrix $K_a$ where $(K_a)_{ij} = k_a(x_i, x_j)$, and then solving the linear system $(K_a + \\lambda I)\\alpha_a = y$ for $\\alpha_a$. This is computationally more stable and efficient than explicitly inverting the matrix.\n    b.  **Prediction on Test Data**: We compute the predictions on the $M$ test points. This is done by first evaluating the $M \\times n$ kernel matrix $K_{test,train}$ where $(K_{test,train})_{mi} = k_a(x_m, x_i)$. The vector of predictions is then given by $\\hat{y}_{test} = K_{test,train} \\alpha_a$.\n    c.  **Alignment Calculation**: The Monte Carlo estimate of the alignment is then computed as the scaled inner product of the true test values and the predicted values: $A_a = \\frac{1}{M} y_{test}^\\top \\hat{y}_{test}$, where $y_{test}$ is the vector of true function values $f^*(x_m)$.\n3.  **Architecture Selection**: After computing $A_a$ for all three architectures, we select the index of the architecture that yields the highest alignment score.\n\nThis process is repeated for each of the $4$ test cases. The implementation of the kernel functions is critical.\n\n-   **Architecture $a_0$ (Linear Kernel)**: $k_{a_0}(x, x') = x^\\top x' + 1$. This is a standard linear kernel, suitable for linear target functions.\n-   **Architecture $a_1$ (Polynomial Kernel)**: $k_{a_1}(x, x') = (x^\\top x' + 1)^2$. This is a degree-$2$ polynomial kernel, well-suited for quadratic target functions.\n-   **Architecture $a_2$ (Arc-cosine-plus-Linear Kernel)**:\n    $$\n    k_{a_2}(x, x') = \\alpha \\|x\\| \\|x'\\| \\left(\\sin \\theta + (\\pi - \\theta) \\cos \\theta\\right) + \\beta\\, x^\\top x' + \\gamma\n    $$\n    with $\\alpha = \\frac{1}{2\\pi}$, $\\beta = 1$, $\\gamma = 10^{-3}$. The angle $\\theta \\in [0, \\pi]$ is defined by $\\cos \\theta = \\frac{x^\\top x'}{\\|x\\| \\|x'\\|}$. This kernel combines a term derived from the NTK of a single-hidden-layer ReLU network (the arc-cosine part), a linear term, and a constant offset. Its rich structure makes it a flexible function approximator capable of capturing complex non-linearities. Numerically, $\\theta$ is computed via $\\arccos$ of the normalized dot product, which must be clipped to the range $[-1, 1]$ to prevent floating-point errors. If either $\\|x\\|=0$ or $\\|x'\\|=0$, the dot product and norm product are $0$, and the kernel value correctly simplifies to $\\gamma$.\n\nThe provided Python program implements this logic in a vectorized manner for efficiency. For each case, it generates the data, computes the alignment for each of the three kernels, and determines the index of the maximizing kernel. The final output is the list of these indices.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"d\": 3, \"n\": 20, \"M\": 400, \"lambda\": 1e-6, \"seed\": 42,\n            \"dist\": \"normal\", \"dist_params\": {\"mean\": np.zeros(3), \"cov\": np.eye(3)},\n            \"target_func\": lambda x: x @ np.array([1, -2, 0.5]) + 0.3\n        },\n        {\n            \"d\": 2, \"n\": 25, \"M\": 500, \"lambda\": 1e-5, \"seed\": 123,\n            \"dist\": \"uniform\", \"dist_params\": {\"low\": -1, \"high\": 1},\n            \"target_func\": lambda x: x @ np.diag([2, -1]) @ x.T\n        },\n        {\n            \"d\": 4, \"n\": 15, \"M\": 600, \"lambda\": 1e-4, \"seed\": 7,\n            \"dist\": \"normal\", \"dist_params\": {\"mean\": np.zeros(4), \"cov\": np.diag([1, 0.1, 0.1, 0.01])},\n            \"target_func\": lambda x: x[0]**2 - 0.5*x[1] + 0.1*np.sin(3*x[2]) + 0.05*x[3]\n        },\n        {\n            \"d\": 2, \"n\": 12, \"M\": 720, \"lambda\": 1e-6, \"seed\": 99,\n            \"dist\": \"circle\", \"dist_params\": {},\n            \"target_func\": lambda x, angle: np.cos(2 * angle)\n        }\n    ]\n\n    # Kernel functions (vectorized)\n    def k_linear(X1, X2):\n        return X1 @ X2.T + 1\n\n    def k_poly2(X1, X2):\n        return (X1 @ X2.T + 1)**2\n\n    def k_arccosine(X1, X2):\n        alpha = 1 / (2 * np.pi)\n        beta = 1.0\n        gamma = 1e-3\n\n        dot_prod = X1 @ X2.T\n        \n        norm1 = np.linalg.norm(X1, axis=1)\n        norm2 = np.linalg.norm(X2, axis=1)\n        norm_prod = np.outer(norm1, norm2)\n        \n        # Handle cases where norm is zero to avoid division by zero\n        # If a norm is zero, the vector is zero, dot product is zero, and norm_prod is zero.\n        # The kernel value should be gamma.\n        # My numerical approach below implicitly computes this correctly.\n        mask_nonzero = norm_prod != 0\n        cos_theta = np.divide(dot_prod, norm_prod, out=np.zeros_like(dot_prod), where=mask_nonzero)\n        \n        # Clip for numerical stability of arccos\n        cos_theta = np.clip(cos_theta, -1.0, 1.0)\n        \n        theta = np.arccos(cos_theta)\n        \n        sin_theta = np.sqrt(1 - cos_theta**2)\n\n        k_arc = alpha * norm_prod * (sin_theta + (np.pi - theta) * cos_theta)\n        \n        return k_arc + beta * dot_prod + gamma\n\n    kernel_functions = [k_linear, k_poly2, k_arccosine]\n    \n    best_indices = []\n\n    for case in test_cases:\n        rng = np.random.default_rng(case['seed'])\n        d, n, M = case['d'], case['n'], case['M']\n\n        # Generate data\n        if case['dist'] == 'normal':\n            mean = case['dist_params']['mean']\n            cov = case['dist_params']['cov']\n            X_train = rng.multivariate_normal(mean, cov, size=n)\n            X_test = rng.multivariate_normal(mean, cov, size=M)\n            y_train = np.apply_along_axis(case['target_func'], 1, X_train)\n            y_test = np.apply_along_axis(case['target_func'], 1, X_test)\n        elif case['dist'] == 'uniform':\n            low = case['dist_params']['low']\n            high = case['dist_params']['high']\n            X_train = rng.uniform(low, high, size=(n, d))\n            X_test = rng.uniform(low, high, size=(M, d))\n            # For quadratic form x.T B x, np.einsum is efficient\n            y_train = np.einsum('ij,jk,ik->i', X_train, np.diag([2, -1]), X_train)\n            y_test = np.einsum('ij,jk,ik->i', X_test, np.diag([2, -1]), X_test)\n        elif case['dist'] == 'circle':\n            angles_train = rng.uniform(0, 2 * np.pi, n)\n            X_train = np.stack([np.cos(angles_train), np.sin(angles_train)], axis=-1)\n            y_train = case['target_func'](X_train, angles_train)\n\n            angles_test = rng.uniform(0, 2 * np.pi, M)\n            X_test = np.stack([np.cos(angles_test), np.sin(angles_test)], axis=-1)\n            y_test = case['target_func'](X_test, angles_test)\n\n        alignments = []\n        for kernel_func in kernel_functions:\n            # 1. Compute training Gram matrix\n            K_train = kernel_func(X_train, X_train)\n            \n            # 2. Solve for alpha weights\n            A = K_train + case['lambda'] * np.eye(n)\n            alpha_weights = np.linalg.solve(A, y_train)\n            \n            # 3. Compute test-train kernel matrix\n            K_test_train = kernel_func(X_test, X_train)\n            \n            # 4. Make predictions\n            y_pred = K_test_train @ alpha_weights\n            \n            # 5. Compute alignment\n            alignment = np.mean(y_test * y_pred)\n            alignments.append(alignment)\n        \n        best_indices.append(np.argmax(alignments))\n\n    print(f\"[{','.join(map(str, best_indices))}]\")\n\nsolve()\n```"
        }
    ]
}