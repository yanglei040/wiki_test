## 引言
随着[深度学习](@entry_id:142022)在[自动驾驶](@entry_id:270800)、医疗诊断等关键领域的广泛应用，确保人工智能系统的安全性和可靠性已成为一个至关重要的问题。然而，研究表明，即使是性能最先进的[神经网](@entry_id:276355)络也极易受到“[对抗性攻击](@entry_id:635501)”的影响——即由人精心设计的、对输入数据的微小扰动，可能导致模型做出完全错误的预测。为了构建真正值得信赖的AI，我们需要的不仅仅是经验性的防御措施，更需要能够提供数学上严格保证的“可验证鲁棒性”（Certified Robustness）。

本文旨在系统性地介绍可验证鲁棒性的核心思想与前沿技术。它解决了从经验性防御到形式化保证的关键知识鸿沟，为读者提供了一套分析和构建鲁棒[深度学习模型](@entry_id:635298)的理论工具箱。通过本文的学习，您将能够理解和应用这些强大的技术，为您的模型构建坚不可摧的“安全护栏”。

在接下来的内容中，我们将分三个章节展开：
- **原理与机制**：我们将深入探讨可验证鲁棒性的数学基础，从基于[利普希茨连续性](@entry_id:142246)的基本思想出发，逐步过渡到如区间边界传播（IBP）和[凸松弛](@entry_id:636024)等可扩展的[认证算法](@entry_id:635947)，揭示它们的工作原理、优势与局限性。
- **应用与跨学科联系**：我们将展示这些核心原理如何超越传统的图像[分类任务](@entry_id:635433)，在[时间序列分析](@entry_id:178930)、图神经网络、[联邦学习](@entry_id:637118)等多样化的应用中发挥作用，并探索其与控制理论、发育生物学等其他科学领域的深刻联系。
- **动手实践**：通过一系列精心设计的编程练习，您将有机会亲手实现IBP、分析认证半径，并构建一个完整的分支定界验证器，将理论知识转化为实践能力。

## 原理与机制

本章在前一章介绍的基础上，深入探讨可验证鲁棒性的核心原理与机制。我们将从基于**[利普希茨连续性](@entry_id:142246) (Lipschitz continuity)** 的基本认证思想出发，逐步过渡到可扩展的[认证算法](@entry_id:635947)，如**区间边界传播 (Interval Bound Propagation, IBP)** 和**[凸松弛](@entry_id:636024) (convex relaxation)**。本章的目标是不仅阐述这些技术“如何”工作，更要通过精确的数学推理和直观的几何解释，揭示它们“为何”有效，以及各自的优势与局限性。

### 基于[利普希茨连续性](@entry_id:142246)的认证基础

理解可验证鲁棒性的最基本途径之一，是通过函数的[利普希茨常数](@entry_id:146583)。一个函数的[利普希茨常数](@entry_id:146583)限定了其输出变化相对于输入变化的最大速率，即函数的“陡峭”程度。这一性质为我们提供了一种强有力的工具，来保证在输入受到扰动时，模型的预测结果保持不变。

#### 认证条件：从边距到半径

假设我们有一个实值[评分函数](@entry_id:175243) $g(x)$，其输出值表示分类的置信度或边距。例如，在二[分类问题](@entry_id:637153)中，$g(x)$ 可以是模型的logit输出；在多[分类问题](@entry_id:637153)中，它可以是正确类别的logit与所有其他类别logit最大值之间的差值。如果在一个输入点 $x$ 处，我们有正的边距 $m(x) = g(x) > 0$，这意味着分类是正确的。

现在，假设函数 $g$ 是 $L$-[利普希茨连续的](@entry_id:267396)，这意味着对于任意输入 $x$ 和扰动 $\delta$，都满足以下不等式：
$$
|g(x+\delta) - g(x)| \le L \|\delta\|
$$
其中 $\|\cdot\|$ 是某种范数（例如 $\ell_2$ 或 $\ell_\infty$ 范数），$L$ 是对应的[利普希茨常数](@entry_id:146583)。这个不等式意味着函数输出的变化量被输入的扰动大小和[利普希茨常数](@entry_id:146583)所约束。

我们可以利用这个性质来为分类的稳定性提供一个保证。扰动后输入 $x+\delta$ 的边距 $g(x+\delta)$ 的下界为：
$$
g(x+\delta) \ge g(x) - |g(x+\delta) - g(x)| \ge m(x) - L \|\delta\|
$$
为了保证分类结果在扰动后保持不变，即 $g(x+\delta) > 0$，我们只需要确保其下界为正：
$$
m(x) - L \|\delta\| > 0 \quad \iff \quad \|\delta\|  \frac{m(x)}{L}
$$
这为我们提供了一个**可验证鲁棒性半径 (certified robustness radius)** $r = m(x)/L$。我们能够证明，对于任何范数小于该半径的扰动 $\delta$，分类结果都不会改变。

在多分类场景中，情况稍微复杂一些。假设模型输出 $C$ 个logit，$f(x) = (f_1(x), \dots, f_C(x))$，预测类别为 $y$。我们定义的分类边距为 $m(x) = f_y(x) - \max_{j \neq y} f_j(x)$。如果我们知道整个向量函数 $f$ 的[利普希茨常数](@entry_id:146583)为 $L_f$，那么对于任意分量 $k$，我们有 $|f_k(x+\delta) - f_k(x)| \le L_f \|\delta\|$。为了保证分类不变，我们需要在扰动后，正确类别的logit值仍然大于其他任何类别的logit值。我们可以通过[最坏情况分析](@entry_id:168192)来建立这个保证：
$$
f_y(x+\delta) \ge f_y(x) - L_f \|\delta\|
$$
$$
f_j(x+\delta) \le f_j(x) + L_f \|\delta\| \quad (\text{对于所有 } j \neq y)
$$
要保证 $f_y(x+\delta) > f_j(x+\delta)$ 对所有 $j \neq y$ 成立，我们必须满足：
$$
f_y(x) - L_f \|\delta\| > f_j(x) + L_f \|\delta\|
$$
这需要对所有 $j \neq y$ 都成立，因此我们必须针对最挑战的类别（即原始logit值第二大的类别）来保证它：
$$
f_y(x) - \max_{j \neq y} f_j(x) > 2 L_f \|\delta\|
$$
这正是 $m(x) > 2 L_f \|\delta\|$。这个关键不等式给出了一个可验证的 $\ell_2$ 鲁棒性半径 $r  m(x) / (2L_f)$ 。这里的因子 $2$ 非常重要，它来源于我们需要同时为正确类别的logit下降和错误类别的logit上升提供边界。

#### 范数的作用

值得注意的是，[利普希茨常数](@entry_id:146583)和可验证半径都与所选择的**范数**密切相关，这个范数定义了我们如何衡量扰动的大小（即威胁模型）。例如，一个针对 $\ell_2$ 范数计算的[利普希茨常数](@entry_id:146583)，不能直接用于 $\ell_\infty$ 范数的扰动认证。

然而，我们可以利用范数之间的标准不等式来转换不同范数下的可验证半径。在 $\mathbb{R}^d$ 空间中，我们有不等式 $\|\delta\|_2 \le \sqrt{d} \|\delta\|_\infty$ 和 $\|\delta\|_\infty \le \|\delta\|_2$。假设我们有一个基于 $\ell_2$ 范数的[利普希茨常数](@entry_id:146583) $L_2$ 和一个保证鲁棒性的条件 $\|\delta\|_2  r_2$。如果我们想在一个 $\ell_\infty$ 球内进行认证，即 $\|\delta\|_\infty \le r_\infty$，我们必须保证这个球内所有扰动的 $\ell_2$ 范数都满足条件。最坏情况是 $\|\delta\|_2$ 达到其最大可能值 $\sqrt{d} r_\infty$。因此，认证条件变为 $\sqrt{d} r_\infty  r_2$，这给出了一个 $\ell_\infty$ 认证半径 $r_\infty = r_2 / \sqrt{d}$ 。这个关系表明，在相同条件下，更高维空间中的 $\ell_\infty$ 认证半径会比 $\ell_2$ 半径小得多，这反映了 $\ell_\infty$ 球的“角落”在 $\ell_2$ 意义下距离[中心点](@entry_id:636820)更远。

### 如何为深度网络计算[利普希茨常数](@entry_id:146583)

既然[利普希茨常数](@entry_id:146583)如此关键，那么下一个自然的问题就是：我们如何为复杂的[深度神经网络](@entry_id:636170)计算它？

#### 局部与全局[利普希茨常数](@entry_id:146583)

对于一个[可微函数](@entry_id:144590) $f$，其在点 $x$ 处的**雅可比矩阵 (Jacobian matrix)** $J_x$ 描述了函数在该点的[局部线性近似](@entry_id:263289)。该点的**局部[利普希茨常数](@entry_id:146583) (local Lipschitz constant)** 由雅可比矩阵的[算子范数](@entry_id:752960)给出，例如，对于 $\ell_2$ 范数，它就是**[谱范数](@entry_id:143091) (spectral norm)** $\|J_x\|_2$。

然而，认证需要在输入的一个邻域（例如，半径为 $r$ 的球 $\mathcal{B}(x, r)$）内都有效。由于[神经网](@entry_id:276355)络是高度[非线性](@entry_id:637147)的，雅可比矩阵在不同点是不同的。因此，我们需要一个在该区域内处处有效的[利普希茨常数](@entry_id:146583)，即 $L = \sup_{\xi \in \mathcal{B}(x,r)} \|J_\xi\|_2$。在一般情况下，计算这个[上确界](@entry_id:140512)是困难的。但在某些特殊情况下，例如，如果一个[ReLU网络](@entry_id:637021)的激活模式在整个球 $\mathcal{B}(x, r)$ 内保持不变，那么网络在该区域内表现为[仿射函数](@entry_id:635019)，其雅可比矩阵是恒定的。此时，局部[利普希茨常数](@entry_id:146583) $\|J_x\|_2$ 就足以进行认证 。

#### 逐层分析与组合

计算整个网络的[利普希茨常数](@entry_id:146583)通常是不可行的。一个更实用的策略是利用函数组合的性质：如果 $f = g \circ h$，那么 $L_f \le L_g \cdot L_h$。这意味着我们可以通过计算每一层的[利普希茨常数](@entry_id:146583)然后将它们相乘，来得到整个网络的[利普希茨常数](@entry_id:146583)的一个上界。

- **线性层 (Linear Layers)**：对于一个线性层 $f(x)=Wx+b$，其[利普希茨常数](@entry_id:146583)就是权重矩阵 $W$ 的[算子范数](@entry_id:752960)，例如 $\|W\|_2$。偏置项 $b$ 不影响[利普希茨常数](@entry_id:146583)，因为它只导致平移。

- **激活函数 (Activations)**：许多激活函数的[利普希茨常数](@entry_id:146583)很容易确定。例如，**ReLU** 函数 $\phi(z) = \max(0, z)$ 的[利普希茨常数](@entry_id:146583)为 $1$。对于更复杂的激活函数，如**[高斯误差线性单元](@entry_id:638032) (GELU)** $\sigma(x) = x \Phi(x)$，我们可以通过分析其导数的最大[绝对值](@entry_id:147688)来确定其在特定输入域上的[利普希茨常数](@entry_id:146583)。例如，在区间 $[-2, 2]$ 上，GELU的导数 $\sigma'(z) = \Phi(z) + z\varphi(z)$ 的最大值出现在 $z=\sqrt{2}$ 处，这为我们提供了在该区域内的一个紧凑的利普希茨上界 。

- **批归一化 (Batch Normalization)**：在推理（评估）模式下，批[归一化层](@entry_id:636850)使用固定的均值和[方差](@entry_id:200758)，其操作变为一个简单的[仿射变换](@entry_id:144885)：$\mathrm{BN}(x)_c = \gamma_c \frac{x_c - \mu_c}{\sqrt{\sigma_c^2 + \varepsilon}} + \beta_c$。这是一个对角[仿射变换](@entry_id:144885)，其[利普希茨常数](@entry_id:146583)由缩放因子决定，即 $L_{\mathrm{BN}} = \max_c \left|\frac{\gamma_c}{\sqrt{\sigma_c^2 + \varepsilon}}\right|$。这表明BN层的[利普希茨常数](@entry_id:146583)直接依赖于训练过程中学到的统计量。如果部署环境的数据[分布](@entry_id:182848)发生变化并导致BN统计量被重新估计，那么原始的[利普希茨常数](@entry_id:146583)和基于它的鲁棒性证书将可能失效 。

#### 利用网络结构获得更紧的界

简单地将各层的[利普希茨常数](@entry_id:146583)相乘可能会导致一个非常宽松（过高）的界。通过更精细地分析[网络结构](@entry_id:265673)，我们可以获得更紧的界。一个典型的例子是**[残差网络](@entry_id:634620) (Residual Networks)**。考虑一个[残差块](@entry_id:637094) $F(x) = x + W_2 \sigma(W_1 x)$ 和一个最终的线性输出 $g(x) = t^\top F(x)$。

一种天真的方法是先计算 $F(x)$ 的[利普希茨常数](@entry_id:146583) $L_F$，然后得到 $L_g \le \|t\|_2 L_F$。$L_F$ 的界可以通过[三角不等式](@entry_id:143750)得到：$L_F \le L_{\text{identity}} + L_{\text{residual}} = 1 + \|W_2\|_2 L_\sigma \|W_1\|_2$。这导致 $L_g \le \|t\|_2 (1 + \|W_2\|_2 L_\sigma \|W_1\|_2)$。

一个更紧的方法是直接分析最终的标量输出 $g(x)$。
$$
g(x) = t^\top x + t^\top W_2 \sigma(W_1 x)
$$
我们可以分别计算两条路径——“恒等路径” $x \mapsto t^\top x$ 和“残差路径” $x \mapsto t^\top W_2 \sigma(W_1 x)$——的[利普希茨常数](@entry_id:146583)，然后将它们相加。第一条路径的[利普希茨常数](@entry_id:146583)是 $\|t\|_2$。第二条路径的[利普希茨常数](@entry_id:146583)是 $\|W_2^\top t\|_2 L_\sigma \|W_1\|_2$。因此，我们得到一个更紧的界：
$$
L_g \le \|t\|_2 + \|W_2^\top t\|_2 L_\sigma \|W_1\|_2
$$
这个界通常比前一个更紧，因为 $\|W_2^\top t\|_2 \le \|W_2\|_2 \|t\|_2$。这个改进源于我们只考虑了残差信号在最终投影方向 $t$ 上的增益，而不是在所有方向上的[最坏情况增益](@entry_id:262400) $\|W_2\|_2$ 。这说明，精细地[分析信号](@entry_id:190094)路径对于获得有意义的鲁棒性证书至关重要。

### 可扩展的认证方法：抽象解释

直接计算或紧密地界定[利普希茨常数](@entry_id:146583)对于大型网络来说非常困难。因此，研究人员开发了另一类称为**抽象解释 (Abstract Interpretation)** 的方法。其核心思想是，不追踪单个输入点的变换，而是追踪一个包含所有可能输入的“抽象域”（如区间或更复杂的几何形状）如何通过网络进行变换。

#### [区间边界传播 (IBP)](@entry_id:637641)

最简单的抽象解释方法是**区间边界传播 (Interval Bound Propagation, IBP)**。如果输入 $x$ 的每个分量 $x_i$ 都被限制在一个区间 $[\ell_i, u_i]$ 内，IBP的目标是计算网络输出的相应区间。

IBP的传播规则非常直观 ：
1.  **仿射层** $y = Wx+b$：输出区间 $[ \ell', u' ]$ 可以通过[区间算术](@entry_id:145176)精确计算。对于每个输出分量 $y_i = \sum_j W_{ij}x_j + b_i$，其下界 $\ell'_i$ 是通过将 $x_j$ 设置为其区间的下界（如果 $W_{ij} > 0$）或[上界](@entry_id:274738)（如果 $W_{ij}  0$）来获得的。上界 $u'_i$ 的计算则相反。
2.  **[ReLU激活](@entry_id:166554)** $a = \max(0, z)$：如果输入 $z$ 的区间是 $[\ell_z, u_z]$，则输出 $a$ 的区间是 $[\max(0, \ell_z), \max(0, u_z)]$。

通过逐层应用这些规则，我们可以得到网络最终输出logit的上下界。如果对于预测类别 $c$，其logit的下界仍然大于所有其他类别logit的[上界](@entry_id:274738)，那么我们就获得了一个鲁棒性证书。

#### IBP的精确性与局限性

IBP的主要优点是其计算速度快且易于实现。然而，它也可能非常不精确。理解其局限性对于正确使用它至关重要。

- **IBP何时精确？** 当网络在整个输入域上表现为[仿射函数](@entry_id:635019)时，IBP是精确的。对于[ReLU网络](@entry_id:637021)，这发生在所有神经元的预激活值都是**符号稳定 (sign-stable)** 的情况下，即它们的预激活区间完全在零的一侧（要么全为正，要么全为负）。在这种情况下，每个ReLU的行为要么是[恒等函数](@entry_id:152136)，要么是零函数，使得整个网络变为[仿射变换](@entry_id:144885) 。

- **IBP为何不精确？依赖问题 (Dependency Problem)。** IBP的主要缺点是它忽略了神经元激活值之间的相关性。在计算每一层的输出区间时，IBP假设前一层的所有激活值都可以在各自的区间内独立变化。这是一个根本性的过度近似。

考虑一个简单的例子：输入为标量 $x \in [-1, 1]$，网络有两个神经元，其预激活值分别为 $z_1 = x$ 和 $z_2 = -x$。经过ReLU后，我们得到 $a_1 = \max(0, x)$ 和 $a_2 = \max(0, -x)$。IBP会如下计算：
- $z_1 \in [-1, 1] \implies a_1 \in [\max(0, -1), \max(0, 1)] = [0, 1]$
- $z_2 \in [-1, 1] \implies a_2 \in [\max(0, -1), \max(0, 1)] = [0, 1]$

如果最终输出是 $y = a_1 + a_2$，IBP会得出 $y \in [0+0, 1+1] = [0, 2]$。然而，真实的输出是 $y(x) = \max(0, x) + \max(0, -x) = |x|$。在 $x \in [-1, 1]$ 的范围内，$|x|$ 的真实范围是 $[0, 1]$。

IBP的界（[上界](@entry_id:274738)为2）比真实值（上界为1）宽松了一倍。其根本原因在于，$a_1$ 和 $a_2$ 并非独立的。事实上，它们是完全负相关的：当 $a_1 > 0$ 时，必然有 $a_2 = 0$，反之亦然。IBP的计算隐式地考虑了不可能发生的情况，例如在 $x=1$ 时 $a_1=1$ 且在 $x=-1$ 时 $a_2=1$ 同时发生，从而推断出总和可以达到 $2$。这种对依赖关系的忽略是IBP不精确性的核心来源 。

### 超越IBP：[凸松弛](@entry_id:636024)

为了解决IBP的依赖问题，研究人员开发了更强大的方法，统称为**[凸松弛](@entry_id:636024) (Convex Relaxation)**。其核心思想是使用比独立区间更精确的几何形状来包围神经元激活值的真实可行集。

对于一个预激活区间跨越零的“不稳定”ReLU神经元（即 $p \in [L, U]$ 且 $L  0  U$），IBP用一个矩形（在 $(p, z)$ 空间中，即 $[L, U] \times [0, \max(0, U)]$）来近似ReLU的输出。而[凸松弛](@entry_id:636024)方法则使用[ReLU函数](@entry_id:273016)图在该区间上的**凸包 (convex hull)** 来进行更紧的近似。这个凸包是一个由三条线段构成的三角形区域，其约束为：
$$
z \ge 0, \quad z \ge p, \quad z \le \frac{U}{U-L}(p-L)
$$
这些[线性不等式](@entry_id:174297)比IBP的区间界限更具表达力。像**CROWN**这样的方法通过[反向传播](@entry_id:199535)这些线性约束，为网络输出相对于输入的界限构建了一个[仿射函数](@entry_id:635019)，从而在各层之间保持了神经元之间的线性依赖关系。

#### 理解松弛间隙

通过一个例子可以很好地理解[凸松弛](@entry_id:636024)为何更优，以及它与精确验证（例如使用[混合整数线性规划](@entry_id:636618) MILP）之间的关系。考虑一个网络，输入 $x=(x_1, x_2) \in [-1, 1]^2$，预激活为 $p_1 = x_1+x_2$ 和 $p_2 = x_1-x_2$，输出为 $y = \max(0, p_1) + \max(0, p_2)$ 。

- **精确计算 (MILP)**：这是一个[凸函数](@entry_id:143075)在[凸集](@entry_id:155617)上的最大化问题，最大值出现在顶点。通过测试输入域的四个顶点 $(1,1), (1,-1), (-1,1), (-1,-1)$，我们发现 $y$ 的最大值为 $2$。

- **[凸松弛](@entry_id:636024)计算**：
    1. 首先，IBP给出预激活区间 $p_1 \in [-2, 2]$ 和 $p_2 \in [-2, 2]$。
    2. 对于每个神经元，我们应用[凸松弛](@entry_id:636024)约束 $z_i \le \frac{2}{4}(p_i+2) = 0.5 p_i + 1$。
    3. 最终输出 $y$ 的松弛[上界](@entry_id:274738)为 $y_{relax} \le (0.5p_1 + 1) + (0.5p_2 + 1) = 0.5(p_1+p_2) + 2$。
    4. 代入 $p_1$ 和 $p_2$ 的定义，我们有 $p_1+p_2 = (x_1+x_2) + (x_1-x_2) = 2x_1$。
    5. 因此，$y_{relax} \le 0.5(2x_1) + 2 = x_1 + 2$。
    6. 在输入域 $x_1 \in [-1, 1]$ 上最大化这个表达式，我们得到的[上界](@entry_id:274738)是 $1+2=3$。

这里的**松弛间隙 (relaxation gap)** 为 $3-2=1$。这个间隙的产生原因与IBP的依赖问题如出一辙：松弛允许了在现实中不可能同时发生的神经元激活值组合。例如，为了达到[上界](@entry_id:274738) $3$，松弛模型允许 $z_1=1.5$ 和 $z_2=1.5$ 的情况发生，但这需要 $x_1=1.5$，超出了输入域的限制。精确的MILP方法通过引入整数变量来编码ReLU的“开/关”状态，从而能精确地捕捉这些互斥关系，因此不会产生间隙。

#### IBP 与 CROWN 的定[性比](@entry_id:172643)较

IBP 和 CROWN 代表了可扩展认证方法的两个不同精度级别。它们的相对性能取决于网络和输入扰动的具体情况 ：

- **[稳定区域](@entry_id:166035) (Stable Regime)**：如果网络中的所有神经元对于给定的输入扰动范围都是符号稳定的，那么网络在该区域内是仿射的。此时，IBP和CROWN都能精确计算输出范围，两者结果相同且无松弛间隙。

- **广泛不稳定区域 (Widely Unstable Regime)**：如果网络中存在大量不稳定神经元，且其预激活区间很宽，那么IBP的独立性假设会导致误差在层间迅速累积，得到非常宽松的界。CROWN通过保持神经元间的[线性依赖](@entry_id:185830)关系，能显著减轻这种[误差累积](@entry_id:137710)，从而得到比IBP紧得多的界。

- **狭窄不[稳定区域](@entry_id:166035) (Narrowly Unstable Regime)**：如果神经元虽然不稳定，但其预激活区间非常窄（例如，接近于零），那么ReLU[非线性](@entry_id:637147)的不确定性区域本身就很小。在这种情况下，IBP和CROWN引入的松弛误差都很小，它们之间的性能差距也会随之缩小。当区间宽度趋近于零时，两者的界将趋于一致。

### 替代[范式](@entry_id:161181)：[随机平滑](@entry_id:634498)

除了上述基于几何和优化的确定性方法外，还有一种基于概率统计的认证[范式](@entry_id:161181)，称为**[随机平滑](@entry_id:634498) (Randomized Smoothing)**。其核心思想是通过在输入上添加随机噪声（通常是[高斯噪声](@entry_id:260752)）来“平滑”分类器的决策边界。

一个标准的[随机平滑](@entry_id:634498)分类器 $H(x)$ 的定义是，在输入 $x$ 周围添加[高斯噪声](@entry_id:260752) $Z \sim \mathcal{N}(0, \sigma^2 I)$ 后，返回最有可能被基础分类器 $f$ 预测的类别：
$$
H(x) = \underset{c}{\operatorname{argmax}} \; \mathbb{P}(f(x+Z) = c)
$$
通过统计学论证可以证明，这样的平滑分类器 $H(x)$ 具有可验证的鲁棒性。

有趣的是，[随机平滑](@entry_id:634498)的思想也可以与[利普希茨连续性](@entry_id:142246)联系起来。考虑一个不同的平滑方案：我们不平滑最终的离散决策，而是平滑一个 $L$-利普希茨的实值[评分函数](@entry_id:175243) $f(x)$，得到平滑后的[评分函数](@entry_id:175243) $g(x) = \mathbb{E}[f(x+Z)]$。可以证明，平滑操作保持了[利普希茨常数](@entry_id:146583)，即 $g(x)$ 也是 $L$-利普希茨的 。
$$
|g(x) - g(x')| = |\mathbb{E}[f(x+Z) - f(x'+Z)]| \le \mathbb{E}[|f(x+Z) - f(x'+Z)|] \le \mathbb{E}[L\|(x+Z)-(x'+Z)\|_2] = L\|x-x'\|_2
$$
一旦我们有了平滑后函数的[利普希茨常数](@entry_id:146583) $L$ 和在某点 $x$ 的边距 $\gamma = g(x)$，我们就可以直接应用本章开头介绍的利普希茨认证原理，得到一个可验证半径 $r = \gamma/L$。这种方法将两种看似不同的认证思想——几何界定与统计平均——优雅地统一起来。