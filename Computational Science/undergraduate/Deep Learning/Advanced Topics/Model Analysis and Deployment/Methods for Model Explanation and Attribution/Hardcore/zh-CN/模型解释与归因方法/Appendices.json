{
    "hands_on_practices": [
        {
            "introduction": "将模型预测归因于其输入的最直接方法是使用梯度。然而，对于像带有ReLU这样的非线性激活函数的神经网络，梯度可能会“饱和”并变为零，即使输入特征对最终输出有显著的间接影响。本练习将指导您从第一性原理出发，为简单的ReLU神经元推导积分梯度（Integrated Gradients）方法，通过它来诊断梯度饱和问题，并验证其关键的完备性（Completeness）公理。",
            "id": "3150467",
            "problem": "给定一个由标量值函数 $f(\\mathbf{x})=\\max\\left(0,\\mathbf{w}^\\top \\mathbf{x}+b\\right)$ 定义的单隐单元修正线性单元 (ReLU) 模型，其中 $\\mathbf{w}\\in\\mathbb{R}^d$，$b\\in\\mathbb{R}$，且 $\\mathbf{x}\\in\\mathbb{R}^d$。考虑针对目标输入 $\\mathbf{x}$ 的两种归因方法：\n- 原始梯度：$a_i=\\frac{\\partial f}{\\partial x_i}(\\mathbf{x})$。\n- 积分梯度（从基线 $\\mathbf{x}'$ 沿直线路径）：对于每个坐标 $i$，其归因为\n$$\\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=(x_i-x'_i)\\int_{0}^{1}\\frac{\\partial f}{\\partial x_i}\\big(\\mathbf{x}'+\\alpha(\\mathbf{x}-\\mathbf{x}')\\big)\\,d\\alpha.$$\n\n您的任务是仅使用梯度、ReLU 函数和路径积分的基本定义来推导、实现和比较这两种归因方法，而不依赖任何预先推导的快捷公式。完全以纯数学术语进行。不涉及任何物理单位。\n\n要求：\n1) 从梯度、ReLU 函数以及沿直线路径的线积分的定义出发，推导一个专门针对 $f(\\mathbf{x})=\\max\\left(0,\\mathbf{w}^\\top \\mathbf{x}+b\\right)$ 的 $\\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')$ 的闭式表达式，该表达式仅用 $\\mathbf{w}$、$b$、$\\mathbf{x}$ 和 $\\mathbf{x}'$ 表示。您必须从第一性原理出发，论证 ReLU 的导数在直线路径上何时是激活的或非激活的。\n2) 定义在 $\\mathbf{x}$ 处的原始梯度归因为 $a(\\mathbf{x})=\\nabla_{\\mathbf{x}} f(\\mathbf{x})$，并约定当 $\\mathbf{w}^\\top\\mathbf{x}+b=0$ 时，为了确定性，梯度取为零向量。\n3) 定义两个基线 $\\mathbf{x}'$：\n   - 零基线 $\\mathbf{x}'=\\mathbf{0}$。\n   - 一个在 ReLU 决策边界上的原则性基线 $\\mathbf{x}'$：选择 $\\mathbf{x}$ 在超平面 $\\{\\mathbf{z}:\\mathbf{w}^\\top \\mathbf{z}+b=0\\}$ 上的正交投影，即\n   $$\\mathbf{x}'=\\mathbf{x}-\\frac{\\mathbf{w}^\\top \\mathbf{x}+b}{\\lVert \\mathbf{w}\\rVert_2^2}\\,\\mathbf{w},$$\n   当 $\\lVert \\mathbf{w}\\rVert_2>0$ 时。在退化情况 $\\lVert \\mathbf{w}\\rVert_2=0$ 下（此时 $f(\\mathbf{x})=\\max(0,b)$），定义边界基线为 $\\mathbf{x}'=\\mathbf{x}$，以使直线路径是平凡的。\n4) 定义在 $\\mathbf{x}$ 处相对于零基线的饱和度诊断为一个布尔值\n$$S(\\mathbf{x})=\\big(\\lVert \\nabla_{\\mathbf{x}} f(\\mathbf{x})\\rVert_2\\le \\varepsilon\\big)\\ \\wedge\\ \\big(\\lVert \\mathrm{IG}(\\mathbf{x},\\mathbf{0})\\rVert_2> \\varepsilon\\big),$$\n其中 $\\varepsilon=10^{-9}$ 是用于数值比较的固定容差。\n5) 对两种基线验证积分梯度的完备性关系：\n   $$C_0(\\mathbf{x})=\\left|\\sum_{i=1}^d \\mathrm{IG}_i(\\mathbf{x},\\mathbf{0})-\\big(f(\\mathbf{x})-f(\\mathbf{0})\\big)\\right|\\le \\varepsilon,$$\n   $$C_b(\\mathbf{x})=\\left|\\sum_{i=1}^d \\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')-\\big(f(\\mathbf{x})-f(\\mathbf{x}')\\big)\\right|\\le \\varepsilon,$$\n   其中 $\\mathbf{x}'$ 是第 3 项中定义的边界基线。\n6) 实现一个程序，对于下面的每个测试用例，计算：\n   - 饱和度诊断 $S(\\mathbf{x})$（布尔值）。\n   - 完备性检查 $C_0(\\mathbf{x})\\le\\varepsilon$ 和 $C_b(\\mathbf{x})\\le\\varepsilon$（布尔值）。\n   对积分梯度使用在第 1 项中推导的精确闭式推理，而非数值积分。\n\n测试套件（每个用例给出 $(\\mathbf{w},b,\\mathbf{x})$）：\n- 情况 1：$\\mathbf{w}=[1.0,-2.0]$，$b=0.5$，$\\mathbf{x}=[2.0,1.0]$。\n- 情况 2：$\\mathbf{w}=[1.0,1.0]$，$b=1.0$，$\\mathbf{x}=[-2.0,-2.0]$。\n- 情况 3：$\\mathbf{w}=[1.0,3.0]$，$b=-4.0$，$\\mathbf{x}=[1.0,1.0]$。\n- 情况 4：$\\mathbf{w}=[2.0,-2.0]$，$b=1.0$，$\\mathbf{x}=[1.0,1.0]$。\n- 情况 5：$\\mathbf{w}=[0.0,0.0]$，$b=1.5$，$\\mathbf{x}=[3.0,-7.0]$。\n\n角度单位不相关。没有物理单位。\n\n最终输出格式：\n- 您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表。\n- 对于每个情况 $k\\in\\{1,2,3,4,5\\}$，输出三元组 $\\big(S(\\mathbf{x}^{(k)}),\\ C_0(\\mathbf{x}^{(k)})\\le\\varepsilon,\\ C_b(\\mathbf{x}^{(k)})\\le\\varepsilon\\big)$ 作为三个布尔值。\n- 将所有情况的三元组连接成一个长度为 15 的扁平列表，并在一行中打印，例如 $[r_1,r_2,\\ldots,r_{15}]$，其中每个 $r_j$ 为 $\\mathrm{True}$ 或 $\\mathrm{False}$。",
            "solution": "我们从基本定义开始。模型为 $f(\\mathbf{x})=\\max(0,z(\\mathbf{x}))$，其中 $z(\\mathbf{x})=\\mathbf{w}^\\top\\mathbf{x}+b$。$f$ 相对于 $\\mathbf{x}$ 的梯度处处存在，除了在 $z(\\mathbf{x})=0$ 处；我们采用约定，在 $z(\\mathbf{x})=0$ 处梯度为零向量。因此，\n$$\n\\nabla_{\\mathbf{x}} f(\\mathbf{x})=\n\\begin{cases}\n\\mathbf{0},  \\text{如果 } z(\\mathbf{x})\\le 0,\\\\\n\\mathbf{w},  \\text{如果 } z(\\mathbf{x})>0.\n\\end{cases}\n$$\n\n积分梯度（沿直线路径）由梯度在输入差值上的投影的路径积分定义。对于基线 $\\mathbf{x}'$ 和目标 $\\mathbf{x}$，令 $\\gamma(\\alpha)=\\mathbf{x}'+\\alpha(\\mathbf{x}-\\mathbf{x}')$，对于 $\\alpha\\in[0,1]$。第 $i$ 个坐标的积分梯度是\n$$\n\\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=(x_i-x'_i)\\int_{0}^{1}\\frac{\\partial f}{\\partial x_i}\\big(\\gamma(\\alpha)\\big)\\,d\\alpha.\n$$\n根据 $f$ 的结构，沿路径的导数在 $\\mathbf{0}$ 和 $\\mathbf{w}$ 之间切换，具体取决于 $z(\\gamma(\\alpha))$ 的符号。我们计算 $z(\\gamma(\\alpha))$：\n$$\nz(\\gamma(\\alpha))=\\mathbf{w}^\\top\\big(\\mathbf{x}'+\\alpha(\\mathbf{x}-\\mathbf{x}')\\big)+b=\\underbrace{\\big(\\mathbf{w}^\\top \\mathbf{x}'+b\\big)}_{z_0}+\\alpha\\underbrace{\\mathbf{w}^\\top(\\mathbf{x}-\\mathbf{x}')}_{c}=z_0+\\alpha c.\n$$\n因此 $z(\\gamma(\\alpha))$ 是 $\\alpha$ 的一个仿射函数。定义交叉点（如果存在）为\n$$\n\\alpha^\\star=-\\frac{z_0}{c},\\quad\\text{当 }c\\ne 0.\n$$\n我们分情况分析：\n\n- 如果 $\\lVert \\mathbf{w}\\rVert_2=0$，则 $f(\\mathbf{x})=\\max(0,b)$ 对于 $\\mathbf{x}$ 是常数，且对所有 $\\mathbf{x}$ 都有 $\\nabla_{\\mathbf{x}}f(\\mathbf{x})=\\mathbf{0}$。因此对于任何 $\\mathbf{x},\\mathbf{x}'$ 都有 $\\mathrm{IG}(\\mathbf{x},\\mathbf{x}')=\\mathbf{0}$，并且完备性成立，因为当 $b>0$ 时 $f(\\mathbf{x})-f(\\mathbf{x}')=0$，当 $b\\le 0$ 时也等于 $0$。\n\n- 假设 $\\lVert \\mathbf{w}\\rVert_2>0$。那么沿路径，梯度在 $\\alpha\\in[0,1]$ 中 $z(\\gamma(\\alpha))>0$ 的子集上等于 $\\mathbf{w}$，在 $z(\\gamma(\\alpha))\\le 0$ 的地方等于 $\\mathbf{0}$。由于 $z(\\gamma(\\alpha))=z_0+\\alpha c$ 是仿射的，其为正的集合是一个区间，其长度等于：\n  - 如果 $c=0$，则 $z(\\gamma(\\alpha))\\equiv z_0$。激活区域的正长度测度 $L$ 在 $z_0>0$ 时为 $L=1$，否则为 $L=0$。\n  - 如果 $c>0$，则当 $\\alpha>\\alpha^\\star$ 时 $z(\\gamma(\\alpha))>0$；激活长度为 $L=\\begin{cases}0, & \\alpha^\\star\\ge 1,\\\\1, & \\alpha^\\star\\le 0,\\\\1-\\alpha^\\star, & \\text{其他情况。}\\end{cases}$\n  - 如果 $c<0$，则当 $\\alpha<\\alpha^\\star$ 时 $z(\\gamma(\\alpha))>0$；激活长度为 $L=\\begin{cases}0, & \\alpha^\\star\\le 0,\\\\1, & \\alpha^\\star\\ge 1,\\\\\\alpha^\\star, & \\text{其他情况。}\\end{cases}$\n\n由于在激活集上梯度等于 $\\mathbf{w}$，在非激活集上等于 $\\mathbf{0}$，我们对每个 $i$ 有：\n$$\n\\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=(x_i-x'_i)\\cdot w_i \\cdot L.\n$$\n在所有坐标上汇总，向量形式为\n$$\n\\mathrm{IG}(\\mathbf{x},\\mathbf{x}')=\\big(\\mathbf{x}-\\mathbf{x}'\\big)\\odot \\mathbf{w}\\cdot L,\n$$\n其中 $\\odot$ 表示逐元素乘积，而 $L$ 是上面确定的激活长度标量。\n\n该推导为何正确：由于导数沿直线路径是分段常数，该积分简化为被积函数非零的集合的测度。这仅利用了 ReLU 导数和路径积分的定义。\n\n完备性属性：积分梯度的总和等于梯度沿路径在 $\\mathbf{x}-\\mathbf{x}'$ 方向上的线积分：\n$$\n\\sum_{i=1}^d \\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=\\int_0^1 \\nabla_{\\mathbf{x}} f\\big(\\gamma(\\alpha)\\big)^\\top (\\mathbf{x}-\\mathbf{x}')\\,d\\alpha.\n$$\n由于 $\\nabla_{\\mathbf{x}} f(\\gamma(\\alpha))$ 在激活区域为 $\\mathbf{w}$，在其他区域为 $\\mathbf{0}$，该积分等于 $L\\cdot \\mathbf{w}^\\top(\\mathbf{x}-\\mathbf{x}')=L\\cdot c$。但是\n$$\nf(\\mathbf{x})-f(\\mathbf{x}')=\\max(0,z_0+c)-\\max(0,z_0).\n$$\n鉴于仿射形式和除可能在单个测度为零的点 $\\alpha^\\star$ 外的分段常数导数，沿路径的微积分基本定理意味着\n$$\n\\int_0^1 \\frac{d}{d\\alpha} f\\big(\\gamma(\\alpha)\\big)\\,d\\alpha=f(\\gamma(1))-f(\\gamma(0))=f(\\mathbf{x})-f(\\mathbf{x}'),\n$$\n并且由于 $\\frac{d}{d\\alpha} f(\\gamma(\\alpha))=\\nabla_{\\mathbf{x}} f(\\gamma(\\alpha))^\\top (\\mathbf{x}-\\mathbf{x}')$，我们得到\n$$\n\\sum_{i=1}^d \\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=f(\\mathbf{x})-f(\\mathbf{x}').\n$$\n我们基于 $L$ 的显式表达式逐情况满足此等式，包括退化情况 $c=0$ 和 $\\lVert\\mathbf{w}\\rVert_2=0$。\n\nReLU 决策边界上的原则性基线：对于 $\\lVert\\mathbf{w}\\rVert_2>0$，定义边界基线为 $\\mathbf{x}$ 在超平面 $\\mathbf{w}^\\top \\mathbf{z}+b=0$ 上的正交投影：\n$$\n\\mathbf{x}'=\\mathbf{x}-\\frac{\\mathbf{w}^\\top \\mathbf{x}+b}{\\lVert \\mathbf{w}\\rVert_2^2}\\,\\mathbf{w}.\n$$\n这个选择满足 $\\mathbf{w}^\\top \\mathbf{x}'+b=0$，因此 $f(\\mathbf{x}')=0$。对于此基线，$c=\\mathbf{w}^\\top(\\mathbf{x}-\\mathbf{x}')=\\mathbf{w}^\\top \\mathbf{x}+b$，且 $z_0=0$。因此，当 $c\\ne 0$ 时 $\\alpha^\\star=0$。如果 $c>0$（在 $\\mathbf{x}$ 处是激活情况），激活长度为 $L=1$ 且\n$$\n\\mathrm{IG}(\\mathbf{x},\\mathbf{x}')=(\\mathbf{x}-\\mathbf{x}')\\odot \\mathbf{w}.\n$$\n求和得到\n$$\n\\sum_i \\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=\\mathbf{w}^\\top(\\mathbf{x}-\\mathbf{x}')=\\mathbf{w}^\\top\\mathbf{x}+b=f(\\mathbf{x})-f(\\mathbf{x}')=f(\\mathbf{x}),\n$$\n这验证了完备性，并且归因完全解释了输出。如果 $c\\le 0$（在 $\\mathbf{x}$ 处非激活或在边界上），则 $L=0$ 且 $\\mathrm{IG}(\\mathbf{x},\\mathbf{x}')=\\mathbf{0}$，再次与完备性匹配，因为 $f(\\mathbf{x})=f(\\mathbf{x}')=0$。在退化情况 $\\lVert\\mathbf{w}\\rVert_2=0$ 下，除非 $b=0$，否则不存在这样的超平面。我们定义 $\\mathbf{x}'=\\mathbf{x}$，这产生平凡路径，$\\mathrm{IG}=\\mathbf{0}$，且 $f(\\mathbf{x})-f(\\mathbf{x}')=0$。\n\n饱和度诊断：当 $z(\\mathbf{x})\\le 0$ 时，原始梯度可能具有误导性地小（或为零），即使相对于像 $\\mathbf{0}$ 这样的基线，积分梯度可以为非零，并揭示了向基线移动会改变模型输出。我们通过以下方式将其形式化：\n$$\nS(\\mathbf{x})=\\big(\\lVert \\nabla_{\\mathbf{x}} f(\\mathbf{x})\\rVert_2\\le \\varepsilon\\big)\\ \\wedge\\ \\big(\\lVert \\mathrm{IG}(\\mathbf{x},\\mathbf{0})\\rVert_2> \\varepsilon\\big),\n$$\n其中 $\\varepsilon=10^{-9}$。\n\n每个测试用例的算法摘要：\n- 使用 ReLU 导数的定义计算 $f(\\mathbf{x})$、$f(\\mathbf{0})$ 以及在 $\\mathbf{x}$ 处的原始梯度。\n- 使用上面推导的闭式解计算零基线积分梯度，其中 $L$ 由 $z_0=\\mathbf{w}^\\top \\mathbf{0}+b=b$ 和 $c=\\mathbf{w}^\\top(\\mathbf{x}-\\mathbf{0})=\\mathbf{w}^\\top\\mathbf{x}$ 确定，包括 $c=0$ 和 $\\lVert\\mathbf{w}\\rVert_2=0$ 的情况。\n- 如果 $\\lVert\\mathbf{w}\\rVert_2>0$，则将边界基线 $\\mathbf{x}'$ 构建为正交投影，否则设置 $\\mathbf{x}'=\\mathbf{x}$。类似地计算相应的积分梯度。\n- 形成 $S(\\mathbf{x})$ 并检查完备性 $C_0(\\mathbf{x})\\le \\varepsilon$ 和 $C_b(\\mathbfx)\\le \\varepsilon$。\n\n将此程序应用于所提供的测试套件，可确保覆盖以下情况：一个激活情况，一个非激活情况（其中零基线路径穿过进入激活区域，从而揭示饱和现象），一个边界情况，一个路径与边界平行的情况（$c=0$ 但激活），以及一个 $\\mathbf{w}=\\mathbf{0}$ 的退化情况。最终输出是按指定顺序排列的扁平化布尔值列表。",
            "answer": "```python\nimport numpy as np\n\ndef relu(z):\n    return np.maximum(0.0, z)\n\ndef f_val(x, w, b):\n    return relu(np.dot(w, x) + b)\n\ndef grad_raw(x, w, b, eps=0.0):\n    # At z > 0: gradient is w; at z <= 0: zero vector (by convention at z=0)\n    z = np.dot(w, x) + b\n    if z > 0:\n        return w.copy()\n    else:\n        return np.zeros_like(w)\n\ndef ig_closed_form(x, x_base, w, b, tol=1e-12):\n    # Integrated gradients along straight-line path from x_base to x\n    # Handle degenerate w=0: IG is zero vector\n    if np.allclose(w, 0.0, atol=tol):\n        return np.zeros_like(w)\n    # Compute z0 and c\n    z0 = np.dot(w, x_base) + b\n    dx = x - x_base\n    c = np.dot(w, dx)\n    # Active length L determination\n    L = 0.0\n    if abs(c) = tol:\n        # z(alpha) = z0 constant\n        if z0  0:\n            L = 1.0\n        else:\n            L = 0.0\n    else:\n        a_star = -z0 / c\n        if c  0:\n            if a_star = 1.0:\n                L = 0.0\n            elif a_star = 0.0:\n                L = 1.0\n            else:\n                L = 1.0 - a_star\n        else:  # c  0\n            if a_star = 0.0:\n                L = 0.0\n            elif a_star = 1.0:\n                L = 1.0\n            else:\n                L = a_star\n    return dx * w * L\n\ndef boundary_baseline(x, w, b, tol=1e-12):\n    # If w is zero vector, return x (trivial path)\n    if np.allclose(w, 0.0, atol=tol):\n        return x.copy()\n    ww = np.dot(w, w)\n    # Projection of x onto the hyperplane w^T z + b = 0\n    factor = (np.dot(w, x) + b) / ww\n    return x - factor * w\n\ndef solve():\n    # Define epsilon for numerical checks\n    eps = 1e-9\n\n    # Test cases: (w, b, x)\n    test_cases = [\n        (np.array([1.0, -2.0]), 0.5, np.array([2.0, 1.0])),     # Case 1\n        (np.array([1.0, 1.0]), 1.0, np.array([-2.0, -2.0])),    # Case 2\n        (np.array([1.0, 3.0]), -4.0, np.array([1.0, 1.0])),     # Case 3\n        (np.array([2.0, -2.0]), 1.0, np.array([1.0, 1.0])),     # Case 4\n        (np.array([0.0, 0.0]), 1.5, np.array([3.0, -7.0])),     # Case 5 (degenerate w=0)\n    ]\n\n    results = []\n    for w, b, x in test_cases:\n        # Raw gradient and norms\n        g = grad_raw(x, w, b)\n        g_norm = np.linalg.norm(g, 2)\n\n        # Zero baseline IG\n        x0 = np.zeros_like(x)\n        ig0 = ig_closed_form(x, x0, w, b)\n        ig0_norm = np.linalg.norm(ig0, 2)\n\n        # Saturation diagnostic\n        saturation = (g_norm = eps) and (ig0_norm  eps)\n\n        # Boundary baseline and IG\n        xb = boundary_baseline(x, w, b)\n        igb = ig_closed_form(x, xb, w, b)\n\n        # Completeness checks\n        comp0_err = abs(np.sum(ig0) - (f_val(x, w, b) - f_val(x0, w, b)))\n        compb_err = abs(np.sum(igb) - (f_val(x, w, b) - f_val(xb, w, b)))\n        comp0_ok = comp0_err = eps\n        compb_ok = compb_err = eps\n\n        # Append booleans in the specified order per case\n        results.extend([saturation, comp0_ok, compb_ok])\n\n    # Final print statement in the exact required format: single-line list\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "除了基于路径积分的方法（如积分梯度），另一大类归因技术是基于传播规则的。逐层相关性传播（Layer-wise Relevance Propagation, LRP）提供了一种系统性地将模型输出的“相关性”逐层分解并分配回输入特征的方法。在本练习中，您将为一个简单的卷积神经网络（CNN）实现LRP的核心规则，并亲手验证其守恒（Conservation）特性，该特性确保了在反向传播的每一步中总相关性都得以保持。",
            "id": "3150507",
            "problem": "考虑一个单输出的卷积神经网络 (CNN)，类似于用于 MNIST (Modified National Institute of Standards and Technology) 手写数字识别任务的网络。该网络有一个带单个滤波器的二维卷积层、一个整流线性单元激活函数，以及一个最终的全连接层，该层将展平的激活图映射到单个标量输出。所有层都没有偏置。输入是一个空间维度为 $4 \\times 4$ 的单通道图像，卷积滤波器的空间维度为 $2 \\times 2$，在 valid 模式下步幅为 $1$，最终的全连接层处理展平的 $3 \\times 3$ 激活图以产生标量输出。\n\n您将实现逐层相关性传播 (LRP) 来将输出相关性归因回输入像素。必须实现两个 LRP 规则：稳定化 $\\epsilon$ 规则和 $\\alpha\\beta$ 规则。您必须验证各层之间的相关性守恒属性，并量化一个数值守恒误差。\n\n基本原理：\n- 对于一个具有输入 $a_i$、权重 $w_{ij}$ 和输出激活前值 $x_j$ 的线性映射，前向传播为 $x_j = \\sum_i a_i w_{ij}$。使用整流线性单元激活后，激活后值为 $a_j = \\max\\{0, x_j\\}$。对于最终的单输出层，标量输出为 $y = \\sum_j a_j w_j$。\n- 在 LRP 中，单元 $j$ 处的相关性 $R_j$ 使用经过充分测试的规则重新分配给其输入 $i$，这些规则源于贡献的局部比例性 $z_{ij} = a_i w_{ij}$（对于密集层）和在局部感受野内的 $z_{ij} = a_i w_{ij}$（对于卷积层）。为了保持相关性计算的守恒，传播过程中忽略偏置。\n- 稳定化 $\\epsilon$ 规则根据以下公式将相关性从单元 $j$ 重新分配给其输入 $i$：\n$$\nR_i = \\frac{z_{ij}}{\\sum_i z_{ij} + \\epsilon \\, \\mathrm{sign}\\!\\left(\\sum_i z_{ij}\\right)} R_j,\n$$\n其中稳定器 $\\epsilon  0$ 用于避免分母接近于零。当 $\\sum_i z_{ij} = 0$ 时，必须取 $\\mathrm{sign}(0) = +1$ 以确保为数值安全提供一个正的稳定项。\n- $\\alpha\\beta$ 规则使用正贡献 $z_{ij}^+ = \\max\\{z_{ij}, 0\\}$ 和负贡献 $z_{ij}^- = \\min\\{z_{ij}, 0\\}$ 通过以下方式重新分配相关性：\n$$\nR_i = \\left( \\alpha \\frac{z_{ij}^+}{\\sum_i z_{ij}^+} - \\beta \\frac{z_{ij}^-}{\\sum_i z_{ij}^-} \\right) R_j,\\quad \\text{其中 } \\alpha - \\beta = 1,\n$$\n其中分母中的求和分别被限制为严格为正 ($\\sum_i z_{ij}^+  0$) 或严格为负 ($\\sum_i z_{ij}^-  0$) 的值；如果相应的和为零，则相应的项定义为零。\n\n需要数值验证的相关性守恒属性：\n- 如果忽略偏置且局部映射是线性的，一个守恒的 LRP 规则应在各层之间保持相关性总和不变，即：\n$$\n\\sum_i R_i = \\sum_j R_j,\n$$\n对于每个传播步骤。对于 $\\epsilon$ 规则，当 $\\epsilon  0$ 时，守恒是近似的。对于 $\\alpha - \\beta = 1$ 的 $\\alpha\\beta$ 规则，在实数算术中，守恒是精确成立的，误差仅来自于浮点舍入。\n\n要实现的网络规格：\n- 输入图像 $X \\in \\mathbb{R}^{4 \\times 4}$：\n$$\nX = \\begin{bmatrix}\n1.0  -2.0  3.0  0.5 \\\\\n0.0  1.0  -1.0  2.0 \\\\\n2.0  -0.5  0.0  -1.5 \\\\\n1.5  0.0  -2.0  1.0\n\\end{bmatrix}.\n$$\n- 卷积滤波器 $K \\in \\mathbb{R}^{2 \\times 2}$：\n$$\nK = \\begin{bmatrix}\n0.5  -1.0 \\\\\n1.5  0.5\n\\end{bmatrix}.\n$$\n- 最终密集层权重 $w \\in \\mathbb{R}^{9}$，将展平的 $3 \\times 3$ 激活图映射为标量：\n$$\nw = \\begin{bmatrix}\n0.3  -0.2  0.1  0.5  -0.4  0.2  -0.1  0.3  0.6\n\\end{bmatrix}.\n$$\n\n要实现的前向计算：\n- 使用步幅为1的valid模式计算卷积激活前图 $Z \\in \\mathbb{R}^{3 \\times 3}$：\n$$\nZ_{u,v} = \\sum_{p=0}^{1}\\sum_{q=0}^{1} X_{u+p,\\,v+q} K_{p,q}.\n$$\n- 应用整流线性单元激活 $A_{u,v} = \\max\\{0, Z_{u,v}\\}$。\n- 将 $A$ 按行主序展平为 $\\tilde{A} \\in \\mathbb{R}^{9}$ 并计算标量输出\n$$\ny = \\sum_{k=1}^{9} \\tilde{A}_k w_k.\n$$\n\n每个测试用例要实现的 LRP 计算：\n- 初始化输出相关性 $R^{\\mathrm{out}} = y$。\n- 使用 $\\epsilon$ 规则或 $\\alpha\\beta$ 规则，根据贡献 $z_k = \\tilde{A}_k w_k$ 将相关性反向传播至密集层输入相关性 $\\tilde{R} \\in \\mathbb{R}^{9}$。\n- 将 $\\tilde{R}$ 重塑为 $R^{\\mathrm{conv}} \\in \\mathbb{R}^{3 \\times 3}$，并通过恒等映射穿过整流线性单元激活层，即将相同的相关性分配给相应的激活前位置。\n- 在每个输出位置 $(u,v)$ 处，使用 $\\epsilon$ 规则或 $\\alpha\\beta$ 规则，根据贡献 $z_{(u,v)\\to(i,j)} = X_{i,j} K_{p,q}$（其中 $(i,j) = (u+p, v+q)$ 且 $(p,q) \\in \\{0,1\\}^2$）将 $R^{\\mathrm{conv}}$ 通过卷积层进行局部反向传播，累积输入相关性 $R^{\\mathrm{in}} \\in \\mathbb{R}^{4 \\times 4}$。\n\n数值守恒误差：\n- 对于每个传播步骤，计算总和\n$$\nS^{\\mathrm{out}} = y,\\quad\nS^{\\mathrm{dense}} = \\sum_{k=1}^{9} \\tilde{R}_k,\\quad\nS^{\\mathrm{conv}} = \\sum_{u=1}^{3}\\sum_{v=1}^{3} R^{\\mathrm{conv}}_{u,v},\\quad\nS^{\\mathrm{in}} = \\sum_{i=1}^{4}\\sum_{j=1}^{4} R^{\\mathrm{in}}_{i,j}.\n$$\n- 使用稳定器 $\\delta = 10^{-12}$ 为相邻层定义归一化守恒误差：\n$$\nE_{\\mathrm{dense}} = \\frac{\\left| S^{\\mathrm{dense}} - S^{\\mathrm{out}} \\right|}{\\max\\{\\left|S^{\\mathrm{out}}\\right|, \\delta\\}},\\quad\nE_{\\mathrm{conv}} = \\frac{\\left| S^{\\mathrm{conv}} - S^{\\mathrm{out}} \\right|}{\\max\\{\\left|S^{\\mathrm{out}}\\right|, \\delta\\}},\\quad\nE_{\\mathrm{in}} = \\frac{\\left| S^{\\mathrm{in}} - S^{\\mathrm{out}} \\right|}{\\max\\{\\left|S^{\\mathrm{out}}\\right|, \\delta\\}}.\n$$\n- 对于每个测试用例，报告单个标量\n$$\nE_{\\max} = \\max\\{E_{\\mathrm{dense}}, E_{\\mathrm{conv}}, E_{\\mathrm{in}}\\}.\n$$\n\n测试套件：\n- 情况 1：$\\epsilon$ 规则，$\\epsilon = 10^{-9}$。\n- 情况 2：$\\epsilon$ 规则，$\\epsilon = 10^{-1}$。\n- 情况 3：$\\alpha\\beta$ 规则，$\\alpha = 1$ 且 $\\beta = 0$。\n- 情况 4：$\\alpha\\beta$ 规则，$\\alpha = 2$ 且 $\\beta = 1$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，即 $[E_{\\max}^{(1)},E_{\\max}^{(2)},E_{\\max}^{(3)},E_{\\max}^{(4)}]$，其中 $E_{\\max}^{(t)}$ 是测试用例 $t$ 的结果。",
            "solution": "该问题被评估为有效，因为它科学地基于卷积神经网络 (CNN) 和逐层相关性传播 (LRP) 的既定原则，具有完整且一致的给定条件，是良构的，并以客观、正式的语言表述。存在唯一的数值解，并且可以通过直接计算得出。\n\n任务是为一个简单的 CNN 实现 LRP，以计算输入图像的归因分数。该过程涉及两个主要阶段：计算网络输出的前向传播，以及将相关性从输出反向传播到输入的反向传播。通过为由特定 LRP 规则参数化定义的四个不同测试用例计算最大归一化守恒误差 $E_{\\max}$，来数值验证相关性守恒属性。\n\n**1. 前向传播计算**\n\n所有测试用例的前向传播都是相同的。它从输入图像 $X$ 计算网络的输出标量 $y$。\n\n给定：\n- 输入图像 $X \\in \\mathbb{R}^{4 \\times 4}$:\n$$\nX = \\begin{bmatrix}\n1.0  -2.0  3.0  0.5 \\\\\n0.0  1.0  -1.0  2.0 \\\\\n2.0  -0.5  0.0  -1.5 \\\\\n1.5  0.0  -2.0  1.0\n\\end{bmatrix}\n$$\n- 卷积滤波器 $K \\in \\mathbb{R}^{2 \\times 2}$:\n$$\nK = \\begin{bmatrix}\n0.5  -1.0 \\\\\n1.5  0.5\n\\end{bmatrix}\n$$\n- 最终密集层权重 $w \\in \\mathbb{R}^{9}$。\n\n**步骤 1.1：卷积层**\n首先，我们通过使用步幅为1将输入 $X$ 与滤波器 $K$ 进行卷积，计算激活前图 $Z \\in \\mathbb{R}^{3 \\times 3}$。该操作定义为 $Z_{u,v} = \\sum_{p=0}^{1}\\sum_{q=0}^{1} X_{u+p,\\,v+q} K_{p,q}$。\n\n得到的激活前图为：\n$$\nZ = \\begin{bmatrix}\n3.0  -3.0  0.5 \\\\\n1.75  0.75  -3.25 \\\\\n3.75  -1.25  -1.0\n\\end{bmatrix}\n$$\n\n**步骤 1.2：ReLU 激活**\n接下来，将整流线性单元 (ReLU) 激活函数 $A_{u,v} = \\max\\{0, Z_{u,v}\\}$ 逐元素应用于 $Z$。\n\n激活后图 $A \\in \\mathbb{R}^{3 \\times 3}$ 为：\n$$\nA = \\begin{bmatrix}\n3.0  0.0  0.5 \\\\\n1.75  0.75  0.0 \\\\\n3.75  0.0  0.0\n\\end{bmatrix}\n$$\n\n**步骤 1.3：全连接层**\n激活图 $A$ 按行主序展平为向量 $\\tilde{A} \\in \\mathbb{R}^{9}$：\n$$\n\\tilde{A} = \\begin{bmatrix}\n3.0  0.0  0.5  1.75  0.75  0.0  3.75  0.0  0.0\n\\end{bmatrix}\n$$\n最终标量输出 $y$ 通过 $\\tilde{A}$ 和密集权重向量 $w$ 的点积计算得出：\n$$\nw = \\begin{bmatrix}\n0.3  -0.2  0.1  0.5  -0.4  0.2  -0.1  0.3  0.6\n\\end{bmatrix}\n$$\n$$\ny = \\sum_{k=1}^{9} \\tilde{A}_k w_k = (3.0)(0.3) + (0.5)(0.1) + (1.75)(0.5) + (0.75)(-0.4) + (3.75)(-0.1) = 1.15\n$$\n反向传播的初始相关性为 $R^{\\mathrm{out}} = y = 1.15$。\n\n**2. LRP 反向传播**\n\nLRP 反向传播将输出相关性 $R^{\\mathrm{out}}$ 分配回前续层，最终生成输入空间的相关性图 $R^{\\mathrm{in}}$。对四个测试用例中的每一个都执行此操作。\n\n**步骤 2.1：通过全连接层的传播**\n相关性从输出 $y$ 传播到展平的激活图 $\\tilde{A}$。总相关性 $R^{\\mathrm{out}} = 1.15$ 在展平激活层的9个神经元之间分配，得到 $\\tilde{R} \\in \\mathbb{R}^{9}$。每个神经元 $k$ 对输出的贡献是 $z_k = \\tilde{A}_k w_k$。总贡献为 $\\sum_k z_k = y$。\n\n- **对于 $\\epsilon$ 规则**：神经元 $k$ 的相关性为 $R_k = \\frac{z_k}{\\sum_i z_i + \\epsilon \\, \\mathrm{sign}(\\sum_i z_i)} R^{\\mathrm{out}}$。\n- **对于 $\\alpha\\beta$ 规则**：神经元 $k$ 的相关性为 $R_k = \\left( \\alpha \\frac{z_{k}^+}{\\sum_i z_{i}^+} - \\beta \\frac{z_{k}^-}{\\sum_i z_{i}^-} \\right) R^{\\mathrm{out}}$，其中 $z_k^+ = \\max(z_k, 0)$ 且 $z_k^- = \\min(z_k, 0)$。\n\n**步骤 2.2：通过 ReLU 激活层的传播**\n问题指定相关性通过恒等映射穿过 ReLU 层。相关性向量 $\\tilde{R}$ 被重塑为 $3 \\times 3$ 的图 $R^{\\mathrm{conv}}$。\n$$ R^{\\mathrm{conv}} = \\mathrm{reshape}_{3 \\times 3}(\\tilde{R}) $$\n\n**步骤 2.3：通过卷积层的传播**\n这是最后也是最复杂的步骤。来自卷积后图谱中每个神经元 $(u,v)$ 的相关性 $R^{\\mathrm{conv}}_{u,v}$，被分配到其在 $X$ 中对应的 $2 \\times 2$ 输入区块。一个输入像素 $X_{i,j}$ 可能对卷积的多个输出有贡献，因此其最终相关性 $R^{\\mathrm{in}}_{i,j}$ 是从其影响的所有卷积输出反向传播回来的相关性之和。\n\n对于每个 $(u,v) \\in \\{0,1,2\\}^2$，我们考虑 $R^{\\mathrm{conv}}_{u,v}$ 的局部传播。局部输入是区块 $X[u:u+2, v:v+2]$ 和核 $K$。局部贡献是 $z_{p,q} = X_{u+p, v+q} K_{p,q}$，其中 $(p,q) \\in \\{0,1\\}^2$。LRP 规则应用于这些局部贡献，以将 $R^{\\mathrm{conv}}_{u,v}$ 分配到一个临时的 $2 \\times 2$ 相关性区块，然后将其加到最终的 $4 \\times 4$ 输入相关性图 $R^{\\mathrm{in}}$ 中的相应区域。\n\n**3. 相关性守恒与误差计算**\n\n对于每个传播步骤，我们计算相关性的总和，并将其与初始总相关性 $S^{\\mathrm{out}} = y$ 进行比较。\n- $S^{\\mathrm{dense}} = \\sum_{k=1}^{9} \\tilde{R}_k$\n- $S^{\\mathrm{conv}} = \\sum_{u=1}^{3}\\sum_{v=1}^{3} R^{\\mathrm{conv}}_{u,v}$\n- $S^{\\mathrm{in}} = \\sum_{i=1}^{4}\\sum_{j=1}^{4} R^{\\mathrm{in}}_{i,j}$\n\n使用稳定器 $\\delta = 10^{-12}$ 计算归一化守恒误差：\n$E_L = \\frac{| S^L - S^{\\mathrm{out}} |}{\\max\\{|S^{\\mathrm{out}}|, \\delta\\}}$ for $L \\in \\{\\mathrm{dense}, \\mathrm{conv}, \\mathrm{in}\\}$。\n每个测试用例的最终报告值为 $E_{\\max} = \\max\\{E_{\\mathrm{dense}}, E_{\\mathrm{conv}}, E_{\\mathrm{in}}\\}$。\n\n- 对于 $\\alpha - \\beta = 1$ 的 **$\\alpha\\beta$ 规则**，LRP 是守恒的，意味着在每一层 $\\sum_i R_i = \\sum_j R_j$ 都成立。任何观察到的误差 $E_{\\max}$ 都将归因于浮点精度限制，并且应该非常接近于0。\n- 对于 **$\\epsilon$ 规则**，守恒是近似的。相关性的总和在每个传播步骤中都会被一个与 $\\epsilon$ 相关的因子所修正。误差 $E_{\\max}$ 预计为非零，并随 $\\epsilon$ 的增大而增大。\n\n现在的实现将遵循这一原则性设计。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements LRP for a simple CNN and calculates conservation errors.\n    \"\"\"\n    # 1. Define Network Parameters and Input\n    X = np.array([\n        [1.0, -2.0, 3.0, 0.5],\n        [0.0, 1.0, -1.0, 2.0],\n        [2.0, -0.5, 0.0, -1.5],\n        [1.5, 0.0, -2.0, 1.0]\n    ])\n    K = np.array([\n        [0.5, -1.0],\n        [1.5, 0.5]\n    ])\n    w = np.array([\n        0.3, -0.2, 0.1, 0.5, -0.4, 0.2, -0.1, 0.3, 0.6\n    ])\n\n    # 2. Forward Pass\n    Z = np.zeros((3, 3))\n    in_h, in_w = X.shape\n    k_h, k_w = K.shape\n    out_h, out_w = Z.shape\n    \n    for r in range(out_h):\n        for c in range(out_w):\n            Z[r, c] = np.sum(X[r:r+k_h, c:c+k_w] * K)\n    \n    A = np.maximum(0, Z)\n    A_flat = A.flatten()\n    \n    y = np.dot(A_flat, w)\n\n    def propagate(R_out, z, rule, params):\n        if rule == 'epsilon':\n            epsilon = params['epsilon']\n            denom_sum = np.sum(z)\n            sign_denom = 1.0 if denom_sum >= 0 else -1.0\n            denominator = denom_sum + epsilon * sign_denom\n            if np.isclose(denominator, 0):\n                # This case should be rare due to the stabilizer.\n                # If sum(z) is exactly -epsilon, relevance cannot be distributed.\n                # A safe fallback is uniform distribution.\n                return np.full_like(z, 1.0 / z.size) * R_out if z.size > 0 else np.zeros_like(z)\n            return (z / denominator) * R_out\n        elif rule == 'alphabeta':\n            alpha, beta = params['alpha'], params['beta']\n            z_pos = np.maximum(0, z)\n            z_neg = np.minimum(0, z)\n            sum_pos = np.sum(z_pos)\n            sum_neg = np.sum(z_neg)\n\n            term_pos = (alpha * z_pos) / sum_pos if not np.isclose(sum_pos, 0) else np.zeros_like(z)\n            term_neg = (beta * z_neg) / sum_neg if not np.isclose(sum_neg, 0) else np.zeros_like(z)\n            \n            return (term_pos - term_neg) * R_out\n        else:\n            raise ValueError(\"Unknown LRP rule\")\n\n    def run_lrp(rule, params):\n        # LRP backward pass\n        \n        # Step 1: Fully-connected layer\n        z_dense = A_flat * w\n        R_dense = propagate(y, z_dense, rule, params).flatten()\n        \n        # Step 2: ReLU layer (identity)\n        R_conv = R_dense.reshape(out_h, out_w)\n        \n        # Step 3: Convolutional layer\n        R_in = np.zeros_like(X)\n        for r in range(out_h):\n            for c in range(out_w):\n                if np.isclose(R_conv[r, c], 0):\n                    continue\n                \n                local_activations = X[r:r+k_h, c:c+k_w]\n                local_relevance_out = R_conv[r, c]\n                \n                z_conv_local = (local_activations * K).flatten()\n                \n                local_relevance_in = propagate(\n                    local_relevance_out, \n                    z_conv_local, \n                    rule, \n                    params\n                )\n                \n                R_in[r:r+k_h, c:c+k_w] += local_relevance_in.reshape(k_h, k_w)\n        \n        # Calculate conservation errors\n        S_out = y\n        S_dense = np.sum(R_dense)\n        S_conv = np.sum(R_conv)\n        S_in = np.sum(R_in)\n        \n        delta = 1e-12\n        error_denom = max(np.abs(S_out), delta)\n\n        E_dense = np.abs(S_dense - S_out) / error_denom\n        # E_conv is same as E_dense due to identity propagation\n        E_conv = np.abs(S_conv - S_out) / error_denom\n        E_in = np.abs(S_in - S_out) / error_denom\n        \n        E_max = max(E_dense, E_conv, E_in)\n        return E_max\n\n    test_cases = [\n        {'rule': 'epsilon', 'params': {'epsilon': 1e-9}},\n        {'rule': 'epsilon', 'params': {'epsilon': 1e-1}},\n        {'rule': 'alphabeta', 'params': {'alpha': 1.0, 'beta': 0.0}},\n        {'rule': 'alphabeta', 'params': {'alpha': 2.0, 'beta': 1.0}},\n    ]\n    \n    results = []\n    for case in test_cases:\n        e_max = run_lrp(case['rule'], case['params'])\n        results.append(e_max)\n        \n    print(f\"[{','.join(f'{r:.8e}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在实际应用中，机器学习模型很少直接处理原始数据；特征标准化等预处理步骤几乎是标准流程。一个至关重要的问题是：我们的解释是否会因为特征的缩放而改变？本练习探讨了这一实际问题，通过分析特征标准化对不同归因方法的影响，您将发现像积分梯度这样的方法具有理想的不变性（Invariance）属性，而更简单的方法则不具备，这突显了选择具有坚实理论基础的解释方法的重要性。",
            "id": "3150465",
            "problem": "给定一个标量值的前馈模型，该模型以一个特征标准化层开始，随后是一个可微变换。特征标准化层通过逐元素标准化将输入向量 $x \\in \\mathbb{R}^d$ 映射到 $z \\in \\mathbb{R}^d$，即 $z = (x - \\mu) \\oslash \\sigma$，其中 $\\mu \\in \\mathbb{R}^d$ 和 $\\sigma \\in \\mathbb{R}^d$ 的分量严格为正，$\\oslash$ 表示逐元素除法。模型输出为 $f(x) = g(z)$，其中 $g$ 是 $z$ 的可微标量函数。请考虑在输入空间中的基于梯度的归因方法，并分析标准化层如何影响它们。你的目标是从第一性原理出发，推导缩放因子 $\\sigma$ 如何影响基于梯度的归因，并提出标准化的归因方法，以消除缩放的影响，使其与直接在标准化空间中计算的相应归因相一致。在你的推导中，仅使用基本定义（梯度、链式法则以及用于路径积分归因的路径积分）作为基础。\n\n所考虑的归因方法的定义：\n- 朴素梯度归因 (Vanilla gradient attribution)：向量 $A_{\\text{grad}}(x) = \\nabla_x f(x)$。\n- 输入乘以梯度归因 (Input-times-gradient attribution)：向量 $A_{\\text{IGR}}(x) = x \\odot \\nabla_x f(x)$，其中 $\\odot$ 表示逐元素乘法。\n- 积分梯度 (Integrated Gradients, IG)：对于选定的基线 $x' \\in \\mathbb{R}^d$，向量 $A_{\\text{IG}}(x;x')$ 按分量定义为 $A_{\\text{IG},k}(x;x') = (x_k - x'_k) \\int_0^1 \\frac{\\partial f(x' + \\alpha (x - x'))}{\\partial x_k} \\, d\\alpha$。\n\n任务：\n1.  应用链式法则于 $f(x) = g\\!\\left((x-\\mu) \\oslash \\sigma\\right)$，从第一性原理推导特征标准化层如何影响朴素梯度归因，并提出一个标准化的梯度归因 $A_{\\text{grad}}^{\\text{std}}(x)$，该归因能消除缩放效应，使其与直接在标准化空间中计算的梯度相匹配，即在 $z = (x - \\mu) \\oslash \\sigma$ 处评估的 $\\nabla_z g(z)$。\n2.  分析在特征标准化下的输入乘以梯度归因，并提出一个标准化版本 $A_{\\text{IGR}}^{\\text{std}}(x)$，该版本能消除缩放效应，并等于直接在标准化空间中计算的输入乘以梯度，即在 $z = (x - \\mu) \\oslash \\sigma$ 处评估的 $z \\odot \\nabla_z g(z)$。\n3.  分析在特征标准化下的积分梯度归因，并确定是否需要进行标准化变换以实现对缩放的不变性。使用积分梯度的基本定义以及由 $z = (x-\\mu) \\oslash \\sigma$ 引起的变量替换。\n\n程序规范：\n- 实现一个程序，对于下面的每个测试用例，计算三个量：\n  (i) 你提出的标准化梯度归因与直接在标准化空间中计算的梯度归因之差的 $\\ell_\\infty$ 范数（最大绝对值分量），\n  (ii) 你提出的标准化输入乘以梯度归因与直接在标准化空间中计算的输入乘以梯度归因之差的 $\\ell_\\infty$ 范数，\n  (iii) 在输入空间和标准化空间中计算的积分梯度之差的 $\\ell_\\infty$ 范数。\n- 对于积分梯度，使用中点法则和 $N$ 个等距步骤在从 $x'$ 到 $x$ 的直线上数值计算路径积分。使用 $N = 2000$。\n- 模型 $g$ 按每个测试用例指定。你必须实现两种形式：\n  (a) 线性模型：$g(z) = w^\\top z + b$，其中 $w \\in \\mathbb{R}^d$ 且 $b \\in \\mathbb{R}$。\n  (b) 带有双曲正切的单隐藏层：$g(z) = u^\\top \\tanh(W z + c)$，其中 $W \\in \\mathbb{R}^{m \\times d}$，$c \\in \\mathbb{R}^m$，$u \\in \\mathbb{R}^m$，且 $\\tanh(\\cdot)$ 逐元素应用。$\\tanh$ 的导数是 $\\frac{d}{da}\\tanh(a) = 1 - \\tanh^2(a)$。\n\n测试套件：\n- 测试用例 1（线性，理想情况）：\n  - 维度 $d = 3$。\n  - $\\mu = [0.0, 0.0, 0.0]$，$\\sigma = [2.0, 0.5, 10.0]$。\n  - $w = [1.0, -3.0, 0.5]$，$b = 0.7$。\n  - 输入 $x = [1.0, -2.0, 0.3]$，基线 $x' = [0.0, 0.0, 0.0]$。\n- 测试用例 2（非线性，覆盖可微性）：\n  - 维度 $d = 3$，隐藏层维度 $m = 2$。\n  - $\\mu = [0.5, -1.0, 2.0]$，$\\sigma = [1.5, 0.8, 3.0]$。\n  - $W = \\begin{bmatrix} 0.2  -0.1  0.4 \\\\ 1.0  0.5  -0.3 \\end{bmatrix}$，$c = [0.1, -0.2]$，$u = [1.2, -0.7]$。\n  - 输入 $x = [2.0, -3.0, 4.5]$，基线 $x' = [0.0, 0.0, 0.0]$。\n- 测试用例 3（非线性，具有极端缩放和非零均值的边缘情况）：\n  - 维度 $d = 3$，隐藏层维度 $m = 4$。\n  - $\\mu = [-2.0, 0.5, 1.0]$，$\\sigma = [10^{-3}, 10^{3}, 1.0]$。\n  - $W = \\begin{bmatrix} -0.5  2.0  0.1 \\\\ 0.3  -1.1  0.9 \\\\ 1.5  0.0  -0.6 \\\\ -0.7  0.2  0.8 \\end{bmatrix}$，$c = [0.0, 0.1, -0.1, 0.2]$，$u = [0.5, -1.2, 0.3, 0.8]$。\n  - 输入 $x = [0.1, -0.2, 3.0]$，基线 $x' = [0.2, -0.1, 1.5]$。\n\n输出规范：\n- 你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表。该列表必须包含 9 个浮点数，分别对应于测试用例 1、2 和 3 的三个量 (i)、(ii)、(iii)。例如，输出格式为 $[r_1,r_2,r_3,r_4,r_5,r_6,r_7,r_8,r_9]$，其中每个 $r_k$ 是一个浮点数。\n- 不涉及物理单位。如果出现角度，必须以弧度为单位，但 $\\tanh$ 不需要角度单位。所有数值结果均以浮点数表示。",
            "solution": "该问题要求分析特征标准化层 $z = (x - \\mu) \\oslash \\sigma$ 如何影响三种基于梯度的归因方法。我们必须从第一性原理出发推导这些关系，并提出对缩放参数 $\\sigma$ 不变的标准化归因。\n\n模型由 $f(x) = g(z(x))$ 给出，其中 $z(x) = (x - \\mu) \\oslash \\sigma$。函数 $g(z)$ 是一个可微的标量函数，$x, \\mu, \\sigma \\in \\mathbb{R}^d$。符号 $\\oslash$ 表示逐元素除法，$\\odot$ 表示逐元素乘法。\n\n### 任务 1：朴素梯度归因分析\n\n输入空间中的朴素梯度归因定义为 $A_{\\text{grad}}(x) = \\nabla_x f(x)$。我们的目标是将其与标准化空间中的梯度归因 $\\nabla_z g(z)$ 联系起来，并提出一个等于 $\\nabla_z g(z)$ 的标准化归因 $A_{\\text{grad}}^{\\text{std}}(x)$。\n\n我们首先应用链式法则来求 $f(x)$ 相对于 $x_k$ 的梯度。从 $x$ 到 $z$ 的映射是 $z_j = (x_j - \\mu_j) / \\sigma_j$。\n$$\n\\frac{\\partial f}{\\partial x_k} = \\sum_{j=1}^d \\frac{\\partial g}{\\partial z_j} \\frac{\\partial z_j}{\\partial x_k}\n$$\n从 $x$ 到 $z$ 的变换的雅可比矩阵是一个对角矩阵，因为每个 $z_j$ 只依赖于 $x_j$。具体来说，$\\frac{\\partial z_j}{\\partial x_k} = \\delta_{jk} / \\sigma_j$，其中 $\\delta_{jk}$ 是克罗内克δ。因此，求和可以简化为一项：\n$$\n\\frac{\\partial f}{\\partial x_k} = \\frac{\\partial g}{\\partial z_k} \\frac{\\partial z_k}{\\partial x_k} = \\frac{\\partial g}{\\partial z_k} \\frac{1}{\\sigma_k}\n$$\n用向量表示，该关系为：\n$$\nA_{\\text{grad}}(x) = \\nabla_x f(x) = \\nabla_z g(z) \\oslash \\sigma\n$$\n该方程表明，输入空间的梯度是标准化空间的梯度逐元素除以标准差 $\\sigma$。为了消除这种缩放效应并恢复标准化空间的梯度，我们必须乘以 $\\sigma$。\n\n我们定义标准化的梯度归因 $A_{\\text{grad}}^{\\text{std}}(x)$ 为：\n$$\nA_{\\text{grad}}^{\\text{std}}(x) = A_{\\text{grad}}(x) \\odot \\sigma\n$$\n代入 $A_{\\text{grad}}(x)$ 的表达式，我们验证该定义达到了预期的结果：\n$$\nA_{\\text{grad}}^{\\text{std}}(x) = (\\nabla_z g(z) \\oslash \\sigma) \\odot \\sigma = \\nabla_z g(z)\n$$\n因此，正确的标准化归因是输入空间梯度与标准差向量 $\\sigma$ 的逐元素乘积。\n\n### 任务 2：输入乘以梯度归因分析\n\n输入空间中的输入乘以梯度归因是 $A_{\\text{IGR}}(x) = x \\odot \\nabla_x f(x)$。标准化空间中相应的归因是 $z \\odot \\nabla_z g(z)$。我们寻求定义一个标准化的归因 $A_{\\text{IGR}}^{\\text{std}}(x)$，使其等于后者。\n\n让我们从目标表达式 $z \\odot \\nabla_z g(z)$ 开始，并用输入空间的量来表示它。我们代入 $z = (x - \\mu) \\oslash \\sigma$ 并使用任务 1 中的关系 $\\nabla_z g(z) = \\nabla_x f(x) \\odot \\sigma$：\n$$\nz \\odot \\nabla_z g(z) = \\left( \\frac{x - \\mu}{\\sigma} \\right) \\odot (\\nabla_x f(x) \\odot \\sigma)\n$$\n逐元素操作 $\\oslash \\sigma$ 和 $\\odot \\sigma$ 相互抵消，得到：\n$$\nz \\odot \\nabla_z g(z) = (x - \\mu) \\odot \\nabla_x f(x)\n$$\n该表达式为我们提供了标准化输入乘以梯度归因的自然定义：\n$$\nA_{\\text{IGR}}^{\\text{std}}(x) = (x - \\mu) \\odot \\nabla_x f(x)\n$$\n这种方法可以解释为一种“中心化输入乘以梯度”的归因。它对缩放 $\\sigma$ 是不变的，并且正确地对应于在标准化特征空间中计算的输入乘以梯度归因。\n\n### 任务 3：积分梯度分析\n\n第 $k$ 个特征的积分梯度（IG）归因由路径积分定义：\n$$\nA_{\\text{IG},k}(x;x') = (x_k - x'_k) \\int_0^1 \\frac{\\partial f(x' + \\alpha (x - x'))}{\\partial x_k} \\, d\\alpha\n$$\n让我们通过变量替换来分析标准化层的影响。在输入空间中，从基线 $x'$到输入 $x$ 的直线路径是 $\\gamma(\\alpha) = x' + \\alpha(x - x')$。在标准化空间中，相应的路径 $\\tilde{\\gamma}(\\alpha)$ 是：\n$$\n\\tilde{\\gamma}(\\alpha) = z(\\gamma(\\alpha)) = \\frac{(x' + \\alpha(x-x')) - \\mu}{\\sigma} = \\frac{x' - \\mu}{\\sigma} + \\alpha\\frac{x - x'}{\\sigma} = z' + \\alpha(z-z')\n$$\n其中 $z = (x-\\mu)\\oslash\\sigma$ 且 $z'=(x'-\\mu)\\oslash\\sigma$。这表明输入空间中的直线路径映射到标准化空间中的直线路径。\n\n接下来，我们使用任务 1 的结果来关联梯度分量：\n$$\n\\frac{\\partial f(\\gamma(\\alpha))}{\\partial x_k} = \\frac{1}{\\sigma_k} \\frac{\\partial g(z(\\gamma(\\alpha)))}{\\partial z_k} = \\frac{1}{\\sigma_k} \\frac{\\partial g(\\tilde{\\gamma}(\\alpha))}{\\partial z_k}\n$$\n将此代入 IG 定义中：\n$$\nA_{\\text{IG},k}(x;x') = (x_k - x'_k) \\int_0^1 \\frac{1}{\\sigma_k} \\frac{\\partial g(z' + \\alpha(z-z'))}{\\partial z_k} \\, d\\alpha\n$$\n我们可以将常数 $1/\\sigma_k$ 移到积分之外，并与 $(x_k - x'_k)$ 项组合：\n$$\nA_{\\text{IG},k}(x;x') = \\left(\\frac{x_k - x'_k}{\\sigma_k}\\right) \\int_0^1 \\frac{\\partial g(z' + \\alpha(z-z'))}{\\partial z_k} \\, d\\alpha\n$$\n括号中的项就是标准化特征分量之差：$\\frac{x_k - x'_k}{\\sigma_k} = \\frac{(x_k - \\mu_k) - (x'_k - \\mu_k)}{\\sigma_k} = z_k - z'_k$。\n表达式变为：\n$$\nA_{\\text{IG},k}(x;x') = (z_k - z'_k) \\int_0^1 \\frac{\\partial g(z' + \\alpha(z-z'))}{\\partial z_k} \\, d\\alpha\n$$\n这恰好是在标准化空间中计算的积分梯度第 $k$ 个分量的定义，即 $A_{\\text{IG, std. space}, k}(z;z')$。因此，我们已经证明：\n$$\nA_{\\text{IG}}(x;x') = A_{\\text{IG, std. space}}(z;z')\n$$\n只要基线被一致地转换，积分梯度对于逐特征的仿射变换（如缩放和平移）就具有内在不变性。因此，IG 归因本身不需要额外的标准化；它自动提供与标准化空间一致的归因。两种计算方法之差的 l-无穷范数应为零（在数值积分误差范围内）。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes differences between standardized and standard-space attributions\n    for three attribution methods across three test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"d\": 3,\n            \"mu\": np.array([0.0, 0.0, 0.0]),\n            \"sigma\": np.array([2.0, 0.5, 10.0]),\n            \"model_type\": \"linear\",\n            \"w\": np.array([1.0, -3.0, 0.5]),\n            \"b\": 0.7,\n            \"x\": np.array([1.0, -2.0, 0.3]),\n            \"x_prime\": np.array([0.0, 0.0, 0.0]),\n        },\n        {\n            \"d\": 3, \"m\": 2,\n            \"mu\": np.array([0.5, -1.0, 2.0]),\n            \"sigma\": np.array([1.5, 0.8, 3.0]),\n            \"model_type\": \"nonlinear\",\n            \"W\": np.array([[0.2, -0.1, 0.4], [1.0, 0.5, -0.3]]),\n            \"c\": np.array([0.1, -0.2]),\n            \"u\": np.array([1.2, -0.7]),\n            \"x\": np.array([2.0, -3.0, 4.5]),\n            \"x_prime\": np.array([0.0, 0.0, 0.0]),\n        },\n        {\n            \"d\": 3, \"m\": 4,\n            \"mu\": np.array([-2.0, 0.5, 1.0]),\n            \"sigma\": np.array([1e-3, 1e3, 1.0]),\n            \"model_type\": \"nonlinear\",\n            \"W\": np.array([[-0.5, 2.0, 0.1], [0.3, -1.1, 0.9], [1.5, 0.0, -0.6], [-0.7, 0.2, 0.8]]),\n            \"c\": np.array([0.0, 0.1, -0.1, 0.2]),\n            \"u\": np.array([0.5, -1.2, 0.3, 0.8]),\n            \"x\": np.array([0.1, -0.2, 3.0]),\n            \"x_prime\": np.array([0.2, -0.1, 1.5]),\n        }\n    ]\n\n    results = []\n    N = 2000\n\n    for case in test_cases:\n        x = case[\"x\"]\n        x_prime = case[\"x_prime\"]\n        mu = case[\"mu\"]\n        sigma = case[\"sigma\"]\n\n        if case[\"model_type\"] == \"linear\":\n            w = case[\"w\"]\n            \n            def grad_z_g(Z):\n                # Z is a matrix of inputs of shape (d, num_points)\n                num_points = Z.shape[1] if Z.ndim > 1 else 1\n                return np.tile(w[:, np.newaxis], (1, num_points))\n\n        elif case[\"model_type\"] == \"nonlinear\":\n            W = case[\"W\"]\n            c = case[\"c\"]\n            u = case[\"u\"]\n                \n            def grad_z_g(Z):\n                # Z is a matrix of inputs of shape (d, num_points)\n                A = W @ Z + c[:, np.newaxis]\n                H = np.tanh(A)\n                # V is of shape (m, num_points)\n                V = u[:, np.newaxis] * (1 - H**2)\n                # W.T is (d,m), so result is (d, num_points)\n                return W.T @ V\n        \n        # Standardized space vectors\n        z = (x - mu) / sigma\n        z_prime = (x_prime - mu) / sigma\n\n        # Reshape for batch-compatible gradient function\n        _z_reshaped = z.reshape(-1, 1)\n        \n        # (i) Vanilla Gradient Attribution\n        # grad_z is (d,), grad_x is (d,), A_grad_std is (d,)\n        grad_z = grad_z_g(_z_reshaped).flatten()\n        grad_x = grad_z / sigma\n        A_grad_std = grad_x * sigma\n        err1 = np.linalg.norm(A_grad_std - grad_z, ord=np.inf)\n        results.append(err1)\n        \n        # (ii) Input-times-Gradient Attribution\n        # All vectors are (d,)\n        A_IGR_in_z_space = z * grad_z\n        A_IGR_std = (x - mu) * grad_x\n        err2 = np.linalg.norm(A_IGR_std - A_IGR_in_z_space, ord=np.inf)\n        results.append(err2)\n\n        # (iii) Integrated Gradients Attribution\n        alphas = (np.arange(N) + 0.5) / N\n\n        # IG in input space (x)\n        x_path_points = x_prime[:, np.newaxis] + (x - x_prime)[:, np.newaxis] * alphas\n        z_path_from_x = (x_path_points - mu[:, np.newaxis]) / sigma[:, np.newaxis]\n        \n        grads_z_on_x_path = grad_z_g(z_path_from_x)\n        grads_x_on_x_path = grads_z_on_x_path / sigma[:, np.newaxis]\n        \n        avg_grad_x = np.mean(grads_x_on_x_path, axis=1)\n        A_IG_x = (x - x_prime) * avg_grad_x\n\n        # IG in standardized space (z)\n        z_path_points = z_prime[:, np.newaxis] + (z - z_prime)[:, np.newaxis] * alphas\n        \n        grads_z_on_z_path = grad_z_g(z_path_points)\n        \n        avg_grad_z = np.mean(grads_z_on_z_path, axis=1)\n        A_IG_z = (z - z_prime) * avg_grad_z\n\n        err3 = np.linalg.norm(A_IG_x - A_IG_z, ord=np.inf)\n        results.append(err3)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}