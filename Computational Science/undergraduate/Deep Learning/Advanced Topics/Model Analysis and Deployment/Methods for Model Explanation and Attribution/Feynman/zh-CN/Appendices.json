{
    "hands_on_practices": [
        {
            "introduction": "简单的使用梯度作为特征重要性的衡量标准可能具有误导性，特别是当神经网络中的神经元处于“饱和”或非激活状态时。本练习将通过一个简单的 $ReLU$ 模型，从第一性原理出发，揭示这一“梯度饱和”问题。你将亲手实现并比较原始梯度与积分梯度（Integrated Gradients），并理解为何基于路径积分的方法能够提供更完整的归因图像。",
            "id": "3150467",
            "problem": "给定一个由标量值函数 $f(\\mathbf{x})=\\max\\left(0,\\mathbf{w}^\\top \\mathbf{x}+b\\right)$ 定义的单隐藏单元整流线性单元 (ReLU) 模型，其中 $\\mathbf{w}\\in\\mathbb{R}^d$，$b\\in\\mathbb{R}$，且 $\\mathbf{x}\\in\\mathbb{R}^d$。考虑针对目标输入 $\\mathbf{x}$ 的两种归因方法：\n- 原始梯度：$a_i=\\frac{\\partial f}{\\partial x_i}(\\mathbf{x})$。\n- 积分梯度（从基线 $\\mathbf{x}'$ 沿直线路径）：对于每个坐标 $i$，其归因值为\n$$\\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=(x_i-x'_i)\\int_{0}^{1}\\frac{\\partial f}{\\partial x_i}\\big(\\mathbf{x}'+\\alpha(\\mathbf{x}-\\mathbf{x}')\\big)\\,d\\alpha.$$\n\n你的任务是仅使用梯度、ReLU 函数和路径积分的基本定义，推导、实现并比较这两种归因方法，而不依赖任何预先推导的捷径公式。整个过程完全使用纯数学术语。不涉及任何物理单位。\n\n要求：\n1) 从梯度、ReLU 函数和沿直线路径的线积分的定义出发，推导专门针对 $f(\\mathbf{x})=\\max\\left(0,\\mathbf{w}^\\top \\mathbf{x}+b\\right)$ 的 $\\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')$ 的闭式表达式，该表达式仅用 $\\mathbf{w}$、$b$、$\\mathbf{x}$ 和 $\\mathbf{x}'$ 表示。你必须从第一性原理出发，论证 ReLU 的导数在直线路径上何时是激活或非激活的。\n2) 定义在 $\\mathbf{x}$ 处的原始梯度归因为 $a(\\mathbf{x})=\\nabla_{\\mathbf{x}} f(\\mathbf{x})$，并约定当 $\\mathbf{w}^\\top\\mathbf{x}+b=0$ 时，为确定性起见，梯度取为零向量。\n3) 定义两个基线 $\\mathbf{x}'$：\n   - 零基线 $\\mathbf{x}'=\\mathbf{0}$。\n   - 一个在 ReLU 决策边界上的原则性基线 $\\mathbf{x}'$：选择 $\\mathbf{x}$ 在超平面 $\\{\\mathbf{z}:\\mathbf{w}^\\top \\mathbf{z}+b=0\\}$ 上的正交投影，即\n   $$\\mathbf{x}'=\\mathbf{x}-\\frac{\\mathbf{w}^\\top \\mathbf{x}+b}{\\lVert \\mathbf{w}\\rVert_2^2}\\,\\mathbf{w},$$\n   只要 $\\lVert \\mathbf{w}\\rVert_2>0$。在退化情况 $\\lVert \\mathbf{w}\\rVert_2=0$（此时 $f(\\mathbf{x})=\\max(0,b)$）下，将边界基线定义为 $\\mathbf{x}'=\\mathbf{x}$，这样直线路径就是平凡的。\n4) 定义在 $\\mathbf{x}$ 处相对于零基线的饱和度诊断为一个布尔值\n$$S(\\mathbf{x})=\\big(\\lVert \\nabla_{\\mathbf{x}} f(\\mathbf{x})\\rVert_2\\le \\varepsilon\\big)\\ \\wedge\\ \\big(\\lVert \\mathrm{IG}(\\mathbf{x},\\mathbf{0})\\rVert_2> \\varepsilon\\big),$$\n其中 $\\varepsilon=10^{-9}$ 是用于数值比较的固定容差。\n5) 验证两种基线下积分梯度的完备性关系：\n   $$C_0(\\mathbf{x})=\\left|\\sum_{i=1}^d \\mathrm{IG}_i(\\mathbf{x},\\mathbf{0})-\\big(f(\\mathbf{x})-f(\\mathbf{0})\\big)\\right|\\le \\varepsilon,$$\n   $$C_b(\\mathbf{x})=\\left|\\sum_{i=1}^d \\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')-\\big(f(\\mathbf{x})-f(\\mathbf{x}')\\big)\\right|\\le \\varepsilon,$$\n   其中 $\\mathbf{x}'$ 是第 3 项中定义的边界基线。\n6) 实现一个程序，对于下面的每个测试用例，计算：\n   - 饱和度诊断 $S(\\mathbf{x})$（布尔值）。\n   - 完备性检查 $C_0(\\mathbf{x})\\le\\varepsilon$ 和 $C_b(\\mathbf{x})\\le\\varepsilon$（布尔值）。\n   使用在第 1 项中推导出的积分梯度的精确闭式解，而不是数值积分。\n\n测试套件（每个用例给出 $(\\mathbf{w},b,\\mathbf{x})$）：\n- 用例 1：$\\mathbf{w}=[1.0,-2.0]$，$b=0.5$，$\\mathbf{x}=[2.0,1.0]$。\n- 用例 2：$\\mathbf{w}=[1.0,1.0]$，$b=1.0$，$\\mathbf{x}=[-2.0,-2.0]$。\n- 用例 3：$\\mathbf{w}=[1.0,3.0]$，$b=-4.0$，$\\mathbf{x}=[1.0,1.0]$。\n- 用例 4：$\\mathbf{w}=[2.0,-2.0]$，$b=1.0$，$\\mathbf{x}=[1.0,1.0]$。\n- 用例 5：$\\mathbf{w}=[0.0,0.0]$，$b=1.5$，$\\mathbf{x}=[3.0,-7.0]$。\n\n角度单位不相关。没有物理单位。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含一个逗号分隔的列表，用方括号括起来。\n- 对于每个用例 $k\\in\\{1,2,3,4,5\\}$，输出三元组 $\\big(S(\\mathbf{x}^{(k)}),\\ C_0(\\mathbf{x}^{(k)})\\le\\varepsilon,\\ C_b(\\mathbf{x}^{(k)})\\le\\varepsilon\\big)$，作为三个布尔值。\n- 将所有用例的三元组连接成一个长度为 15 的扁平化列表，并在一行中打印，例如 $[r_1,r_2,\\ldots,r_{15}]$，其中每个 $r_j$ 是 $\\mathrm{True}$ 或 $\\mathrm{False}$。",
            "solution": "我们从基本定义开始。模型是 $f(\\mathbf{x})=\\max(0,z(\\mathbf{x}))$，其中 $z(\\mathbf{x})=\\mathbf{w}^\\top\\mathbf{x}+b$。$f$ 相对于 $\\mathbf{x}$ 的梯度在除 $z(\\mathbf{x})=0$ 之外的所有地方都存在；我们约定在 $z(\\mathbf{x})=0$ 处梯度为零向量。因此，\n$$\n\\nabla_{\\mathbf{x}} f(\\mathbf{x})=\n\\begin{cases}\n\\mathbf{0},  \\text{若 } z(\\mathbf{x})\\le 0,\\\\\n\\mathbf{w},  \\text{若 } z(\\mathbf{x})>0.\n\\end{cases}\n$$\n\n积分梯度（沿直线路径）被定义为梯度在输入差值上投影的路径积分。对于一个基线 $\\mathbf{x}'$ 和目标 $\\mathbf{x}$，令 $\\gamma(\\alpha)=\\mathbf{x}'+\\alpha(\\mathbf{x}-\\mathbf{x}')$，其中 $\\alpha\\in[0,1]$。第 $i$ 个坐标的积分梯度是\n$$\n\\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=(x_i-x'_i)\\int_{0}^{1}\\frac{\\partial f}{\\partial x_i}\\big(\\gamma(\\alpha)\\big)\\,d\\alpha.\n$$\n根据 $f$ 的结构，沿路径的导数根据 $z(\\gamma(\\alpha))$ 的符号在 $\\mathbf{0}$ 和 $\\mathbf{w}$ 之间切换。我们计算 $z(\\gamma(\\alpha))$：\n$$\nz(\\gamma(\\alpha))=\\mathbf{w}^\\top\\big(\\mathbf{x}'+\\alpha(\\mathbf{x}-\\mathbf{x}')\\big)+b=\\underbrace{\\big(\\mathbf{w}^\\top \\mathbf{x}'+b\\big)}_{z_0}+\\alpha\\underbrace{\\mathbf{w}^\\top(\\mathbf{x}-\\mathbf{x}')}_{c}=z_0+\\alpha c.\n$$\n因此 $z(\\gamma(\\alpha))$ 是 $\\alpha$ 的一个仿射函数。定义交叉点（如果存在）为\n$$\n\\alpha^\\star=-\\frac{z_0}{c},\\quad\\text{当 }c\\ne 0.\n$$\n我们分析以下情况：\n\n- 如果 $\\lVert \\mathbf{w}\\rVert_2=0$，那么 $f(\\mathbf{x})=\\max(0,b)$ 相对于 $\\mathbf{x}$ 是一个常数，且对于所有 $\\mathbf{x}$，$\\nabla_{\\mathbf{x}}f(\\mathbf{x})=\\mathbf{0}$。因此对于任何 $\\mathbf{x},\\mathbf{x}'$，$\\mathrm{IG}(\\mathbf{x},\\mathbf{x}')=\\mathbf{0}$，并且完备性成立，因为当 $b>0$ 时对于任何 $\\mathbf{x},\\mathbf{x}'$ 都有 $f(\\mathbf{x})-f(\\mathbf{x}')=0$，当 $b\\le 0$ 时也等于 $0$。\n\n- 假设 $\\lVert \\mathbf{w}\\rVert_2>0$。那么沿路径，梯度在 $\\alpha\\in[0,1]$ 中 $z(\\gamma(\\alpha))>0$ 的子集上为 $\\mathbf{w}$，在 $z(\\gamma(\\alpha))\\le 0$ 的地方为 $\\mathbf{0}$。由于 $z(\\gamma(\\alpha))=z_0+\\alpha c$ 是仿射的，其为正的集合是一个区间，其长度等于：\n  - 如果 $c=0$，那么 $z(\\gamma(\\alpha))\\equiv z_0$。激活区域的正长度测度 $L$ 在 $z_0>0$ 时为 $L=1$，否则为 $L=0$。\n  - 如果 $c>0$，那么当 $\\alpha>\\alpha^\\star$ 时 $z(\\gamma(\\alpha))>0$；激活长度为 $L=\\begin{cases}0,\\alpha^\\star\\ge 1,\\\\1,\\alpha^\\star\\le 0,\\\\1-\\alpha^\\star,\\text{其他情况。}\\end{cases}$\n  - 如果 $c<0$，那么当 $\\alpha<\\alpha^\\star$ 时 $z(\\gamma(\\alpha))>0$；激活长度为 $L=\\begin{cases}0,\\alpha^\\star\\le 0,\\\\1,\\alpha^\\star\\ge 1,\\\\\\alpha^\\star,\\text{其他情况。}\\end{cases}$\n\n由于在激活集上梯度等于 $\\mathbf{w}$，在非激活集上为 $\\mathbf{0}$，我们对每个 $i$ 有：\n$$\n\\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=(x_i-x'_i)\\cdot w_i \\cdot L.\n$$\n在所有坐标上汇总，向量形式为\n$$\n\\mathrm{IG}(\\mathbf{x},\\mathbf{x}')=\\big(\\mathbf{x}-\\mathbf{x}'\\big)\\odot \\mathbf{w}\\cdot L,\n$$\n其中 $\\odot$ 表示逐元素乘积，$L$ 是上面确定的激活长度标量。\n\n为什么这是正确的：因为导数沿直线路径是分段常数，所以积分简化为被积函数非零的集合的测度。这只利用了 ReLU 导数和路径积分的定义。\n\n完备性属性：积分梯度的总和等于梯度沿路径在 $\\mathbf{x}-\\mathbf{x}'$ 方向上的线积分：\n$$\n\\sum_{i=1}^d \\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=\\int_0^1 \\nabla_{\\mathbf{x}} f\\big(\\gamma(\\alpha)\\big)^\\top (\\mathbf{x}-\\mathbf{x}')\\,d\\alpha.\n$$\n由于 $\\nabla_{\\mathbf{x}} f(\\gamma(\\alpha))$ 在激活区域为 $\\mathbf{w}$，否则为 $\\mathbf{0}$，该积分等于 $L\\cdot \\mathbf{w}^\\top(\\mathbf{x}-\\mathbf{x}')=L\\cdot c$。但是\n$$\nf(\\mathbf{x})-f(\\mathbf{x}')=\\max(0,z_0+c)-\\max(0,z_0).\n$$\n鉴于其仿射形式和除可能在测度为零的单个点 $\\alpha^\\star$ 外的分段常数导数，沿路径的微积分基本定理意味着\n$$\n\\int_0^1 \\frac{d}{d\\alpha} f\\big(\\gamma(\\alpha)\\big)\\,d\\alpha=f(\\gamma(1))-f(\\gamma(0))=f(\\mathbf{x})-f(\\mathbf{x}'),\n$$\n并且由于 $\\frac{d}{d\\alpha} f(\\gamma(\\alpha))=\\nabla_{\\mathbf{x}} f(\\gamma(\\alpha))^\\top (\\mathbf{x}-\\mathbf{x}')$，我们得到\n$$\n\\sum_{i=1}^d \\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=f(\\mathbf{x})-f(\\mathbf{x}').\n$$\n我们基于 $L$ 的显式表达式在各种情况下都满足这个等式，包括退化情况 $c=0$ 和 $\\lVert\\mathbf{w}\\rVert_2=0$。\n\n在 ReLU 边界上的原则性基线：对于 $\\lVert\\mathbf{w}\\rVert_2>0$，将边界基线定义为 $\\mathbf{x}$ 在超平面 $\\mathbf{w}^\\top \\mathbf{z}+b=0$ 上的正交投影：\n$$\n\\mathbf{x}'=\\mathbf{x}-\\frac{\\mathbf{w}^\\top \\mathbf{x}+b}{\\lVert \\mathbf{w}\\rVert_2^2}\\,\\mathbf{w}.\n$$\n这种选择满足 $\\mathbf{w}^\\top \\mathbf{x}'+b=0$，因此 $f(\\mathbf{x}')=0$。对于此基线，$c=\\mathbf{w}^\\top(\\mathbf{x}-\\mathbf{x}')=\\mathbf{w}^\\top \\mathbf{x}+b$，且 $z_0=0$。因此，当 $c\\ne 0$ 时 $\\alpha^\\star=0$。如果 $c>0$（在 $\\mathbf{x}$ 处是激活情况），激活长度为 $L=1$ 并且\n$$\n\\mathrm{IG}(\\mathbf{x},\\mathbf{x}')=(\\mathbf{x}-\\mathbf{x}')\\odot \\mathbf{w}.\n$$\n求和得出\n$$\n\\sum_i \\mathrm{IG}_i(\\mathbf{x},\\mathbf{x}')=\\mathbf{w}^\\top(\\mathbf{x}-\\mathbf{x}')=\\mathbf{w}^\\top\\mathbf{x}+b=f(\\mathbf{x})-f(\\mathbf{x}')=f(\\mathbf{x}),\n$$\n验证了完备性以及归因完全解释了输出。如果 $c\\le 0$（在 $\\mathbf{x}$ 处非激活或在边界上），则 $L=0$ 且 $\\mathrm{IG}(\\mathbf{x},\\mathbf{x}')=\\mathbf{0}$，再次与完备性匹配，因为 $f(\\mathbf{x})=f(\\mathbf{x}')=0$。在退化情况 $\\lVert\\mathbf{w}\\rVert_2=0$ 下，除非 $b=0$，否则不存在这样的超平面。我们定义 $\\mathbf{x}'=\\mathbf{x}$，这会产生平凡路径，$\\mathrm{IG}=\\mathbf{0}$，以及 $f(\\mathbf{x})-f(\\mathbf{x}')=0$。\n\n饱和度诊断：当 $z(\\mathbf{x})\\le 0$ 时，原始梯度可能会误导性地很小（或为零），尽管相对于像 $\\mathbf{0}$ 这样的基线，积分梯度可以是非零的，并揭示了向基线移动会改变模型输出。我们通过以下方式将其形式化\n$$\nS(\\mathbf{x})=\\big(\\lVert \\nabla_{\\mathbf{x}} f(\\mathbf{x})\\rVert_2\\le \\varepsilon\\big)\\ \\wedge\\ \\big(\\lVert \\mathrm{IG}(\\mathbf{x},\\mathbf{0})\\rVert_2> \\varepsilon\\big),\n$$\n其中 $\\varepsilon=10^{-9}$。\n\n每个测试用例的算法摘要：\n- 使用 ReLU 导数的定义计算 $f(\\mathbf{x})$、$f(\\mathbf{0})$ 以及在 $\\mathbf{x}$ 处的原始梯度。\n- 使用上面推导出的闭式解计算零基线的积分梯度，其中 $L$ 由 $z_0=\\mathbf{w}^\\top \\mathbf{0}+b=b$ 和 $c=\\mathbf{w}^\\top(\\mathbf{x}-\\mathbf{0})=\\mathbf{w}^\\top\\mathbf{x}$ 确定，包括 $c=0$ 和 $\\lVert\\mathbf{w}\\rVert_2=0$ 的情况。\n- 如果 $\\lVert\\mathbf{w}\\rVert_2>0$，则将边界基线 $\\mathbf{x}'$ 构建为正交投影，否则设置 $\\mathbf{x}'=\\mathbf{x}$。类似地计算相应的积分梯度。\n- 形成 $S(\\mathbf{x})$ 并检查完备性 $C_0(\\mathbf{x})\\le \\varepsilon$ 和 $C_b(\\mathbf{x})\\le \\varepsilon$。\n\n将此程序应用于所提供的测试套件，可确保覆盖以下情况：一个激活情况、一个零基线路径穿过激活区域的非激活情况（揭示饱和）、一个边界情况、一个路径与边界平行的情况（$c=0$ 但激活），以及一个 $\\mathbf{w}=\\mathbf{0}$ 的退化情况。最终输出是按指定顺序排列的布尔值扁平化列表。",
            "answer": "```python\nimport numpy as np\n\ndef relu(z):\n    return np.maximum(0.0, z)\n\ndef f_val(x, w, b):\n    return relu(np.dot(w, x) + b)\n\ndef grad_raw(x, w, b, eps=0.0):\n    # At z > 0: gradient is w; at z = 0: zero vector (by convention at z=0)\n    z = np.dot(w, x) + b\n    if z > 0:\n        return w.copy()\n    else:\n        return np.zeros_like(w)\n\ndef ig_closed_form(x, x_base, w, b, tol=1e-12):\n    # Integrated gradients along straight-line path from x_base to x\n    # Handle degenerate w=0: IG is zero vector\n    if np.allclose(w, 0.0, atol=tol):\n        return np.zeros_like(w)\n    # Compute z0 and c\n    z0 = np.dot(w, x_base) + b\n    dx = x - x_base\n    c = np.dot(w, dx)\n    # Active length L determination\n    if abs(c) = tol:\n        # z(alpha) = z0 constant\n        if z0 > 0:\n            L = 1.0\n        else:\n            L = 0.0\n    else:\n        a_star = -z0 / c\n        if c > 0:\n            if a_star >= 1.0:\n                L = 0.0\n            elif a_star = 0.0:\n                L = 1.0\n            else:\n                L = 1.0 - a_star\n        else:  # c  0\n            if a_star = 0.0:\n                L = 0.0\n            elif a_star >= 1.0:\n                L = 1.0\n            else:\n                L = a_star\n    return dx * w * L\n\ndef boundary_baseline(x, w, b, tol=1e-12):\n    # If w is zero vector, return x (trivial path)\n    if np.allclose(w, 0.0, atol=tol):\n        return x.copy()\n    ww = np.dot(w, w)\n    # Projection of x onto the hyperplane w^T z + b = 0\n    factor = (np.dot(w, x) + b) / ww\n    return x - factor * w\n\ndef solve():\n    # Define epsilon for numerical checks\n    eps = 1e-9\n\n    # Test cases: (w, b, x)\n    test_cases = [\n        (np.array([1.0, -2.0]), 0.5, np.array([2.0, 1.0])),     # Case 1\n        (np.array([1.0, 1.0]), 1.0, np.array([-2.0, -2.0])),    # Case 2\n        (np.array([1.0, 3.0]), -4.0, np.array([1.0, 1.0])),     # Case 3\n        (np.array([2.0, -2.0]), 1.0, np.array([1.0, 1.0])),     # Case 4\n        (np.array([0.0, 0.0]), 1.5, np.array([3.0, -7.0])),     # Case 5 (degenerate w=0)\n    ]\n\n    results = []\n    for w, b, x in test_cases:\n        # Raw gradient and norms\n        g = grad_raw(x, w, b)\n        g_norm = np.linalg.norm(g, 2)\n\n        # Zero baseline IG\n        x0 = np.zeros_like(x)\n        ig0 = ig_closed_form(x, x0, w, b)\n        ig0_norm = np.linalg.norm(ig0, 2)\n\n        # Saturation diagnostic\n        saturation = (g_norm = eps) and (ig0_norm > eps)\n\n        # Boundary baseline and IG\n        xb = boundary_baseline(x, w, b)\n        igb = ig_closed_form(x, xb, w, b)\n\n        # Completeness checks\n        comp0_err = abs(np.sum(ig0) - (f_val(x, w, b) - f_val(x0, w, b)))\n        compb_err = abs(np.sum(igb) - (f_val(x, w, b) - f_val(xb, w, b)))\n        comp0_ok = comp0_err = eps\n        compb_ok = compb_err = eps\n\n        # Append booleans in the specified order per case\n        results.extend([saturation, comp0_ok, compb_ok])\n\n    # Final print statement in the exact required format: single-line list\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "在现实世界的应用中，机器学习模型通常包含特征标准化等数据预处理步骤。这些变换会如何影响我们计算出的特征重要性？本练习旨在探讨这一关键的实际问题，要求你分析并推导不同归因方法（如普通梯度、梯度乘以输入和积分梯度）在面对特征缩放时的行为，并提出能够抵消这种影响的标准化归因。",
            "id": "3150465",
            "problem": "给定一个标量值前馈模型，它以一个特征标准化层开始，随后是一个可微变换。特征标准化层通过逐元素标准化 $z = (x - \\mu) \\oslash \\sigma$ 将输入向量 $x \\in \\mathbb{R}^d$ 映射到 $z \\in \\mathbb{R}^d$，其中 $\\mu \\in \\mathbb{R}^d$ 且 $\\sigma \\in \\mathbb{R}^d$ 的分量严格为正，$\\oslash$ 表示逐元素除法。模型输出为 $f(x) = g(z)$，其中 $g$ 是 $z$ 的一个可微标量函数。考虑输入空间中基于梯度的归因方法，并分析标准化层如何影响它们。你的目标是从第一性原理推导通过 $\\sigma$ 进行的缩放如何影响基于梯度的归因，并提出标准化的归因方法，以消除缩放的影响，使其与直接在标准化空间中计算的相应归因相一致。你的推导应仅使用基本定义（梯度、链式法则以及用于路径积分归因的线积分）作为基础。\n\n所考虑的归因方法的定义：\n- 原始梯度归因：向量 $A_{\\text{grad}}(x) = \\nabla_x f(x)$。\n- 输入乘以梯度归因：向量 $A_{\\text{IGR}}(x) = x \\odot \\nabla_x f(x)$，其中 $\\odot$ 表示逐元素乘法。\n- 积分梯度 (IG)：对于选定的基线 $x' \\in \\mathbb{R}^d$，向量 $A_{\\text{IG}}(x;x')$ 按分量定义为 $A_{\\text{IG},k}(x;x') = (x_k - x'_k) \\int_0^1 \\frac{\\partial f(x' + \\alpha (x - x'))}{\\partial x_k} \\, d\\alpha$。\n\n任务：\n1. 对 $f(x) = g\\!\\left((x-\\mu) \\oslash \\sigma\\right)$ 应用链式法则，从第一性原理推导特征标准化层如何影响原始梯度归因，并提出一个标准化的梯度归因 $A_{\\text{grad}}^{\\text{std}}(x)$，该归因能消除缩放效应，使其与直接在标准化空间中计算的梯度相匹配，即在 $z = (x - \\mu) \\oslash \\sigma$ 处求值的 $\\nabla_z g(z)$。\n2. 分析特征标准化下的输入乘以梯度归因，并提出一个标准化版本 $A_{\\text{IGR}}^{\\text{std}}(x)$，该版本能消除缩放效应，并等于直接在标准化空间中计算的输入乘以梯度，即在 $z = (x - \\mu) \\oslash \\sigma$ 处求值的 $z \\odot \\nabla_z g(z)$。\n3. 分析特征标准化下的积分梯度归因，并确定是否需要进行标准化变换以实现相对于缩放的不变性。使用积分梯度的基本定义以及由 $z = (x-\\mu) \\oslash \\sigma$ 引起的变量替换。\n\n程序规范：\n- 实现一个程序，对于下面的每个测试用例，计算三个量：\n  (i) 你提出的标准化梯度归因与直接在标准化空间中计算的梯度归因之差的 $\\ell_\\infty$ 范数（最大绝对值分量），\n  (ii) 你提出的标准化输入乘以梯度归因与直接在标准化空间中计算的输入乘以梯度归因之差的 $\\ell_\\infty$ 范数，\n  (iii) 在输入空间和标准化空间中计算的积分梯度之差的 $\\ell_\\infty$ 范数。\n- 对于积分梯度，使用中点法则沿从 $x'$ 到 $x$ 的直线路径上的 $N$ 个等间距步长来数值计算线积分。使用 $N = 2000$。\n- 模型 $g$ 按测试用例指定。你必须实现两种形式：\n  (a) 线性：$g(z) = w^\\top z + b$，其中 $w \\in \\mathbb{R}^d$ 且 $b \\in \\mathbb{R}$。\n  (b) 带双曲正切的单隐藏层：$g(z) = u^\\top \\tanh(W z + c)$，其中 $W \\in \\mathbb{R}^{m \\times d}$，$c \\in \\mathbb{R}^m$，$u \\in \\mathbb{R}^m$，且 $\\tanh(\\cdot)$ 逐元素应用。$\\tanh$ 的导数是 $\\frac{d}{da}\\tanh(a) = 1 - \\tanh^2(a)$。\n\n测试套件：\n- 测试用例 1（线性，正常路径）：\n  - 维度 $d = 3$。\n  - $\\mu = [0.0, 0.0, 0.0]$, $\\sigma = [2.0, 0.5, 10.0]$。\n  - $w = [1.0, -3.0, 0.5]$, $b = 0.7$。\n  - 输入 $x = [1.0, -2.0, 0.3]$，基线 $x' = [0.0, 0.0, 0.0]$。\n- 测试用例 2（非线性，覆盖可微性）：\n  - 维度 $d = 3$，隐藏层维度 $m = 2$。\n  - $\\mu = [0.5, -1.0, 2.0]$, $\\sigma = [1.5, 0.8, 3.0]$。\n  - $W = \\begin{bmatrix} 0.2  -0.1  0.4 \\\\ 1.0  0.5  -0.3 \\end{bmatrix}$, $c = [0.1, -0.2]$, $u = [1.2, -0.7]$。\n  - 输入 $x = [2.0, -3.0, 4.5]$，基线 $x' = [0.0, 0.0, 0.0]$。\n- 测试用例 3（非线性，极端缩放和非零均值的边缘情况）：\n  - 维度 $d = 3$，隐藏层维度 $m = 4$。\n  - $\\mu = [-2.0, 0.5, 1.0]$, $\\sigma = [10^{-3}, 10^{3}, 1.0]$。\n  - $W = \\begin{bmatrix} -0.5  2.0  0.1 \\\\ 0.3  -1.1  0.9 \\\\ 1.5  0.0  -0.6 \\\\ -0.7  0.2  0.8 \\end{bmatrix}$, $c = [0.0, 0.1, -0.1, 0.2]$, $u = [0.5, -1.2, 0.3, 0.8]$。\n  - 输入 $x = [0.1, -0.2, 3.0]$，基线 $x' = [0.2, -0.1, 1.5]$。\n\n输出规范：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须包含 $9$ 个浮点数，分别对应于测试用例 1、2 和 3 的三个量 (i)、(ii)、(iii)。例如，输出格式为 $[r_1,r_2,r_3,r_4,r_5,r_6,r_7,r_8,r_9]$，其中每个 $r_k$ 是一个浮点数。\n- 不涉及物理单位。如果出现任何角度，必须以弧度为单位，但 $\\tanh$ 不需要角度单位。将所有数值结果表示为浮点数。",
            "solution": "该问题要求分析特征标准化层 $z = (x - \\mu) \\oslash \\sigma$ 如何影响三种基于梯度的归因方法。我们必须从第一性原理推导这些关系，并提出对缩放参数 $\\sigma$ 不变的标准化归因。\n\n模型由 $f(x) = g(z(x))$ 给出，其中 $z(x) = (x - \\mu) \\oslash \\sigma$。函数 $g(z)$ 是一个可微标量函数，且 $x, \\mu, \\sigma \\in \\mathbb{R}^d$。符号 $\\oslash$ 表示逐元素除法，$\\odot$ 表示逐元素乘法。\n\n### 任务 1：原始梯度归因分析\n\n输入空间中的原始梯度归因定义为 $A_{\\text{grad}}(x) = \\nabla_x f(x)$。我们的目标是将其与标准化空间中的梯度归因 $\\nabla_z g(z)$ 联系起来，并提出一个等于 $\\nabla_z g(z)$ 的标准化归因 $A_{\\text{grad}}^{\\text{std}}(x)$。\n\n我们首先应用链式法则来求 $f(x)$ 相对于 $x_k$ 的梯度。从 $x$ 到 $z$ 的映射是 $z_j = (x_j - \\mu_j) / \\sigma_j$。\n$$\n\\frac{\\partial f}{\\partial x_k} = \\sum_{j=1}^d \\frac{\\partial g}{\\partial z_j} \\frac{\\partial z_j}{\\partial x_k}\n$$\n从 $x$ 到 $z$ 的变换的雅可比矩阵是一个对角矩阵，因为每个 $z_j$ 仅依赖于 $x_j$。具体来说，$\\frac{\\partial z_j}{\\partial x_k} = \\delta_{jk} / \\sigma_j$，其中 $\\delta_{jk}$ 是克罗内克 δ。求和简化为一项：\n$$\n\\frac{\\partial f}{\\partial x_k} = \\frac{\\partial g}{\\partial z_k} \\frac{\\partial z_k}{\\partial x_k} = \\frac{\\partial g}{\\partial z_k} \\frac{1}{\\sigma_k}\n$$\n用向量表示法，这个关系是：\n$$\nA_{\\text{grad}}(x) = \\nabla_x f(x) = \\nabla_z g(z) \\oslash \\sigma\n$$\n该方程表明，输入空间梯度是标准化空间梯度被标准差 $\\sigma$ 的倒数逐元素缩放的结果。为了消除这种缩放效应并恢复标准化空间梯度，我们必须乘以 $\\sigma$。\n\n我们将标准化的梯度归因 $A_{\\text{grad}}^{\\text{std}}(x)$ 定义为：\n$$\nA_{\\text{grad}}^{\\text{std}}(x) = A_{\\text{grad}}(x) \\odot \\sigma\n$$\n代入 $A_{\\text{grad}}(x)$ 的表达式，我们验证该定义达到了预期的结果：\n$$\nA_{\\text{grad}}^{\\text{std}}(x) = (\\nabla_z g(z) \\oslash \\sigma) \\odot \\sigma = \\nabla_z g(z)\n$$\n因此，正确的标准化归因是输入空间梯度与标准差向量 $\\sigma$ 进行逐元素相乘。\n\n### 任务 2：输入乘以梯度归因分析\n\n输入空间中的输入乘以梯度归因是 $A_{\\text{IGR}}(x) = x \\odot \\nabla_x f(x)$。标准化空间中对应的归因是 $z \\odot \\nabla_z g(z)$。我们旨在定义一个标准化的归因 $A_{\\text{IGR}}^{\\text{std}}(x)$，使其等于后者。\n\n让我们从目标表达式 $z \\odot \\nabla_z g(z)$ 开始，并用输入空间的量来表示它。我们代入 $z = (x - \\mu) \\oslash \\sigma$ 并使用任务 1 中的关系 $\\nabla_z g(z) = \\nabla_x f(x) \\odot \\sigma$：\n$$\nz \\odot \\nabla_z g(z) = \\left( \\frac{x - \\mu}{\\sigma} \\right) \\odot (\\nabla_x f(x) \\odot \\sigma)\n$$\n逐元素运算 $\\oslash \\sigma$ 和 $\\odot \\sigma$ 相互抵消，得到：\n$$\nz \\odot \\nabla_z g(z) = (x - \\mu) \\odot \\nabla_x f(x)\n$$\n这个表达式为我们提供了标准化的输入乘以梯度归因的自然定义：\n$$\nA_{\\text{IGR}}^{\\text{std}}(x) = (x - \\mu) \\odot \\nabla_x f(x)\n$$\n该方法可以解释为一种“中心化输入乘以梯度”的归因。它对缩放 $\\sigma$ 是不变的，并且正确地对应于在标准化特征空间中计算的输入乘以梯度归因。\n\n### 任务 3：积分梯度分析\n\n第 $k$ 个特征的积分梯度 (IG) 归因由路径积分定义：\n$$\nA_{\\text{IG},k}(x;x') = (x_k - x'_k) \\int_0^1 \\frac{\\partial f(x' + \\alpha (x - x'))}{\\partial x_k} \\, d\\alpha\n$$\n让我们通过执行变量替换来分析标准化层的影响。在输入空间中，从基线 $x'$ 到输入 $x$ 的直线路径是 $\\gamma(\\alpha) = x' + \\alpha(x - x')$。在标准化空间中，对应的路径 $\\tilde{\\gamma}(\\alpha)$ 是：\n$$\n\\tilde{\\gamma}(\\alpha) = z(\\gamma(\\alpha)) = \\frac{(x' + \\alpha(x-x')) - \\mu}{\\sigma} = \\frac{x' - \\mu}{\\sigma} + \\alpha\\frac{x - x'}{\\sigma} = z' + \\alpha(z-z')\n$$\n其中 $z = (x-\\mu)\\oslash\\sigma$ 且 $z'=(x'-\\mu)\\oslash\\sigma$。这表明输入空间中的直线路径映射到标准化空间中的直线路径。\n\n接下来，我们使用任务 1 的结果来关联梯度分量：\n$$\n\\frac{\\partial f(\\gamma(\\alpha))}{\\partial x_k} = \\frac{1}{\\sigma_k} \\frac{\\partial g(z(\\gamma(\\alpha)))}{\\partial z_k} = \\frac{1}{\\sigma_k} \\frac{\\partial g(\\tilde{\\gamma}(\\alpha))}{\\partial z_k}\n$$\n将此代入 IG 的定义中：\n$$\nA_{\\text{IG},k}(x;x') = (x_k - x'_k) \\int_0^1 \\frac{1}{\\sigma_k} \\frac{\\partial g(z' + \\alpha(z-z'))}{\\partial z_k} \\, d\\alpha\n$$\n我们可以将常数 $1/\\sigma_k$ 移到积分之外，并将其与 $(x_k - x'_k)$ 项组合：\n$$\nA_{\\text{IG},k}(x;x') = \\left(\\frac{x_k - x'_k}{\\sigma_k}\\right) \\int_0^1 \\frac{\\partial g(z' + \\alpha(z-z'))}{\\partial z_k} \\, d\\alpha\n$$\n括号中的项就是标准化特征分量之间的差：$\\frac{x_k - x'_k}{\\sigma_k} = \\frac{(x_k - \\mu_k) - (x'_k - \\mu_k)}{\\sigma_k} = z_k - z'_k$。\n表达式变为：\n$$\nA_{\\text{IG},k}(x;x') = (z_k - z'_k) \\int_0^1 \\frac{\\partial g(z' + \\alpha(z-z'))}{\\partial z_k} \\, d\\alpha\n$$\n这正是积分梯度在标准化空间中计算的第 $k$ 个分量的定义，即 $A_{\\text{IG, std. space}, k}(z;z')$。因此，我们已经证明：\n$$\nA_{\\text{IG}}(x;x') = A_{\\text{IG, std. space}}(z;z')\n$$\n只要基线被一致地变换，积分梯度对于逐特征的仿射变换（如缩放和平移）是内在不变的。因此，IG 归因本身不需要额外的标准化；它自动提供与标准化空间一致的归因。两种计算方法之差的 $\\ell_\\infty$ 范数应为零，除非存在数值积分误差。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes differences between standardized and standard-space attributions\n    for three attribution methods across three test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"d\": 3,\n            \"mu\": np.array([0.0, 0.0, 0.0]),\n            \"sigma\": np.array([2.0, 0.5, 10.0]),\n            \"model_type\": \"linear\",\n            \"w\": np.array([1.0, -3.0, 0.5]),\n            \"b\": 0.7,\n            \"x\": np.array([1.0, -2.0, 0.3]),\n            \"x_prime\": np.array([0.0, 0.0, 0.0]),\n        },\n        {\n            \"d\": 3, \"m\": 2,\n            \"mu\": np.array([0.5, -1.0, 2.0]),\n            \"sigma\": np.array([1.5, 0.8, 3.0]),\n            \"model_type\": \"nonlinear\",\n            \"W\": np.array([[0.2, -0.1, 0.4], [1.0, 0.5, -0.3]]),\n            \"c\": np.array([0.1, -0.2]),\n            \"u\": np.array([1.2, -0.7]),\n            \"x\": np.array([2.0, -3.0, 4.5]),\n            \"x_prime\": np.array([0.0, 0.0, 0.0]),\n        },\n        {\n            \"d\": 3, \"m\": 4,\n            \"mu\": np.array([-2.0, 0.5, 1.0]),\n            \"sigma\": np.array([1e-3, 1e3, 1.0]),\n            \"model_type\": \"nonlinear\",\n            \"W\": np.array([[-0.5, 2.0, 0.1], [0.3, -1.1, 0.9], [1.5, 0.0, -0.6], [-0.7, 0.2, 0.8]]),\n            \"c\": np.array([0.0, 0.1, -0.1, 0.2]),\n            \"u\": np.array([0.5, -1.2, 0.3, 0.8]),\n            \"x\": np.array([0.1, -0.2, 3.0]),\n            \"x_prime\": np.array([0.2, -0.1, 1.5]),\n        }\n    ]\n\n    results = []\n    N = 2000\n\n    for case in test_cases:\n        x = case[\"x\"]\n        x_prime = case[\"x_prime\"]\n        mu = case[\"mu\"]\n        sigma = case[\"sigma\"]\n\n        if case[\"model_type\"] == \"linear\":\n            w = case[\"w\"]\n            \n            def grad_z_g(Z):\n                # Z is a matrix of inputs of shape (d, num_points)\n                num_points = Z.shape[1] if Z.ndim > 1 else 1\n                return np.tile(w[:, np.newaxis], (1, num_points))\n\n        elif case[\"model_type\"] == \"nonlinear\":\n            W = case[\"W\"]\n            c = case[\"c\"]\n            u = case[\"u\"]\n                \n            def grad_z_g(Z):\n                # Z is a matrix of inputs of shape (d, num_points)\n                A = W @ Z + c[:, np.newaxis]\n                H = np.tanh(A)\n                # V is of shape (m, num_points)\n                V = u[:, np.newaxis] * (1 - H**2)\n                # W.T is (d,m), so result is (d, num_points)\n                return W.T @ V\n        \n        # Standardized space vectors\n        z = (x - mu) / sigma\n        z_prime = (x_prime - mu) / sigma\n\n        # Reshape for batch-compatible gradient function\n        _z_reshaped = z.reshape(-1, 1)\n        \n        # (i) Vanilla Gradient Attribution\n        # grad_z is (d,), grad_x is (d,), A_grad_std is (d,)\n        grad_z = grad_z_g(_z_reshaped).flatten()\n        grad_x = grad_z / sigma\n        A_grad_std = grad_x * sigma\n        err1 = np.linalg.norm(A_grad_std - grad_z, ord=np.inf)\n        results.append(err1)\n        \n        # (ii) Input-times-Gradient Attribution\n        # All vectors are (d,)\n        A_IGR_in_z_space = z * grad_z\n        A_IGR_std = (x - mu) * grad_x\n        err2 = np.linalg.norm(A_IGR_std - A_IGR_in_z_space, ord=np.inf)\n        results.append(err2)\n\n        # (iii) Integrated Gradients Attribution\n        alphas = (np.arange(N) + 0.5) / N\n\n        # IG in input space (x)\n        x_path_points = x_prime[:, np.newaxis] + (x - x_prime)[:, np.newaxis] * alphas\n        z_path_from_x = (x_path_points - mu[:, np.newaxis]) / sigma[:, np.newaxis]\n        \n        grads_z_on_x_path = grad_z_g(z_path_from_x)\n        grads_x_on_x_path = grads_z_on_x_path / sigma[:, np.newaxis]\n        \n        avg_grad_x = np.mean(grads_x_on_x_path, axis=1)\n        A_IG_x = (x - x_prime) * avg_grad_x\n\n        # IG in standardized space (z)\n        z_path_points = z_prime[:, np.newaxis] + (z - z_prime)[:, np.newaxis] * alphas\n        \n        grads_z_on_z_path = grad_z_g(z_path_points)\n        \n        avg_grad_z = np.mean(grads_z_on_z_path, axis=1)\n        A_IG_z = (z - z_prime) * avg_grad_z\n\n        err3 = np.linalg.norm(A_IG_x - A_IG_z, ord=np.inf)\n        results.append(err3)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "模型中特征的影响力并非总是独立的，它们常常通过“交互”来共同影响最终预测。本练习将引导你探究归因方法如何处理这些复杂的特征交互。你将从基本定义出发，为一个含有明确交互项的模型计算 SHAP (Shapley Additive Explanations) 值和积分梯度，并分析它们如何分配交互带来的贡献。",
            "id": "3150523",
            "problem": "给定一个双特征确定性模型，其输出函数为 $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$，且 $f(x_1,x_2)=\\beta_1 x_1+\\beta_2 x_2+\\beta_{12} x_1 x_2$。您的任务是，从第一性原理出发，在零基线下实现并比较来自 Shapley 加性解释 (SHAP) 和积分梯度 (IG) 的归因，并使用差分的差分计算来验证 Shapley 交互指数。仅使用 SHAP 和 IG 的数学定义作为基础起点，不依赖任何预先推导的简化公式。\n\n从以下基本依据开始：\n- 合作博弈论中的 Shapley 值：它通过对所有特征排序下的每个特征的边际贡献进行平均，将总价值 $f(x_1,x_2)-f(0,0)$ 分配给特征 $x_1$ 和 $x_2$。\n- 积分梯度 (IG) 方法：对于一个可微函数 $f$，特征 $x_i$ 相对于基线 $x_i'=0$ 的归因定义为 $f$ 的偏导数沿着从基线到 $x$ 的直线路径的线积分，并乘以 $(x_i-x_i')$。\n- Shapley 交互指数：对于两个特征，成对交互作用是无法由任一单个特征解释的价值部分，可通过在子集上评估的 $f$ 的离散二阶差分获得。\n\n实现以下内容，均以 $(0,0)$ 为基线：\n1. 计算 $x_1$ 和 $x_2$ 的 SHAP 归因，该归因从应用于函数 $f(x_1,x_2)$ 的 Shapley 值定义推导得出。\n2. 计算 $x_1$ 和 $x_2$ 的 IG 归因，该归因直接从沿着路径 $\\alpha\\mapsto(\\alpha x_1,\\alpha x_2)$（其中 $\\alpha\\in[0,1]$）的线积分定义推导得出。\n3. 使用离散二阶差分 $f(x_1,x_2)-f(x_1,0)-f(0,x_2)+f(0,0)$ 计算对 $(x_1,x_2)$ 的 Shapley 交互指数。\n\n然后，对于每个测试用例，将以下检查量化为非负实数：\n- IG 归因总和与 $f(x_1,x_2)$ 之间的绝对差。\n- $x_1$ 的 SHAP 归因与 $x_1$ 的 IG 归因之间的绝对差。\n- $x_2$ 的 SHAP 归因与 $x_2$ 的 IG 归因之间的绝对差。\n- 通过离散二阶差分计算的 Shapley 交互指数与 $\\beta_{12} x_1 x_2$ 之间的绝对差。\n- IG 分配给 $x_1$ 的交互部分（即 $IG_1-\\beta_1 x_1$）与成对交互作用的一半（即 $\\tfrac{1}{2}\\beta_{12} x_1 x_2$）之间的绝对差。\n- IG 分配给 $x_2$ 的交互部分（即 $IG_2-\\beta_2 x_2$）与成对交互作用的一半（即 $\\tfrac{1}{2}\\beta_{12} x_1 x_2$）之间的绝对差。\n\n使用以下参数值测试套件 $(\\beta_1,\\beta_2,\\beta_{12},x_1,x_2)$：\n- 测试 $1$：$(1.5,2.0,0.8,1.0,3.0)$。\n- 测试 $2$：$(1.0,1.0,0.0,2.0,-1.0)$。\n- 测试 $3$：$(-0.5,0.75,-1.2,1.2,0.5)$。\n- 测试 $4$：$(3.0,-2.0,0.5,0.0,4.0)$。\n- 测试 $5$：$(0.0,0.0,5.0,0.2,-0.4)$。\n\n您的程序必须输出一行，其中包含一个用方括号括起来的逗号分隔列表，该列表由测试 $1$ 的六个结果、然后是测试 $2$ 的六个结果，依此类推直到测试 $5$ 的结果连接而成。因此，最终输出是一个包含 $30$ 个实数的列表。不涉及物理单位。不涉及角度。所有结果均表示为十进制实数。",
            "solution": "该问题已经过验证，并被确定为是合理的。它在 Shapley 值和积分梯度的既定理论中具有科学依据，是适定的，提供了所有必要信息，并且陈述客观。任务是为一个指定的多项式模型从第一性原理推导并实现归因计算，并验证某些理论性质。\n\n模型由函数 $f:\\mathbb{R}^2 \\rightarrow \\mathbb{R}$ 定义为 $f(x_1, x_2) = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2$。所有归因的基线是原点 $(0,0)$。\n\n**1. SHAP 归因的推导**\n\n在特征集 $N$ 中，特征 $i$ 的 Shapley 值 $\\phi_i$ 定义为其对所有可能特征联盟的边际贡献的加权平均值：\n$$ \\phi_i(v) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|! (|N| - |S| - 1)!}{|N|!} [v(S \\cup \\{i\\}) - v(S)] $$\n在我们的例子中，特征集为 $N = \\{1, 2\\}$，因此 $|N|=2$。价值函数 $v(S)$ 是模型输出 $f$ 在特征 $j \\in S$ 时使用输入 $x_j$、在特征 $j \\notin S$ 时使用基线值 $0$ 的评估结果。\n相关的函数值为：\n- $v(\\emptyset) = f(0, 0) = 0$\n- $v(\\{1\\}) = f(x_1, 0) = \\beta_1 x_1$\n- $v(\\{2\\}) = f(0, x_2) = \\beta_2 x_2$\n- $v(\\{1, 2\\}) = f(x_1, x_2) = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2$\n\n对于特征 $x_1$，其 SHAP 归因 $\\text{SHAP}_1$ 的计算需要考虑 $N \\setminus \\{1\\} = \\{2\\}$ 的子集，即 $\\emptyset$ 和 $\\{2\\}$：\n$$ \\text{SHAP}_1 = \\frac{0!(2-0-1)!}{2!}[v(\\{1\\}) - v(\\emptyset)] + \\frac{1!(2-1-1)!}{2!}[v(\\{1, 2\\}) - v(\\{2\\})] $$\n$$ \\text{SHAP}_1 = \\frac{1}{2}[f(x_1, 0) - f(0, 0)] + \\frac{1}{2}[f(x_1, x_2) - f(0, x_2)] $$\n代入函数值：\n$$ \\text{SHAP}_1 = \\frac{1}{2}[\\beta_1 x_1 - 0] + \\frac{1}{2}[(\\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2) - (\\beta_2 x_2)] $$\n$$ \\text{SHAP}_1 = \\frac{1}{2}(\\beta_1 x_1) + \\frac{1}{2}(\\beta_1 x_1 + \\beta_{12} x_1 x_2) $$\n$$ \\text{SHAP}_1 = \\beta_1 x_1 + \\frac{1}{2}\\beta_{12} x_1 x_2 $$\n\n根据对称性，对于特征 $x_2$，其 SHAP 归因 $\\text{SHAP}_2$ 为：\n$$ \\text{SHAP}_2 = \\frac{1}{2}[f(0, x_2) - f(0, 0)] + \\frac{1}{2}[f(x_1, x_2) - f(x_1, 0)] $$\n$$ \\text{SHAP}_2 = \\frac{1}{2}[\\beta_2 x_2 - 0] + \\frac{1}{2}[(\\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2) - (\\beta_1 x_1)] $$\n$$ \\text{SHAP}_2 = \\frac{1}{2}(\\beta_2 x_2) + \\frac{1}{2}(\\beta_2 x_2 + \\beta_{12} x_1 x_2) $$\n$$ \\text{SHAP}_2 = \\beta_2 x_2 + \\frac{1}{2}\\beta_{12} x_1 x_2 $$\n\n**2. 积分梯度 (IG) 归因的推导**\n\n对于输入 $x=(x_1, x_2)$ 和基线 $x'=(0,0)$，特征 $x_i$ 的积分梯度归因由以下公式给出：\n$$ \\text{IG}_i(x) = (x_i - x'_i) \\int_{\\alpha=0}^{1} \\frac{\\partial f}{\\partial x_i}(x' + \\alpha(x - x')) d\\alpha $$\n积分路径为 $\\gamma(\\alpha) = (0,0) + \\alpha(x_1-0, x_2-0) = (\\alpha x_1, \\alpha x_2)$。项 $(x_i - x'_i)$ 就是 $x_i$。\n\n首先，我们计算 $f$ 的偏导数：\n$$ \\frac{\\partial f}{\\partial x_1} = \\beta_1 + \\beta_{12} x_2 $$\n$$ \\frac{\\partial f}{\\partial x_2} = \\beta_2 + \\beta_{12} x_1 $$\n接下来，我们沿着路径 $\\gamma(\\alpha)$ 计算这些导数：\n$$ \\frac{\\partial f}{\\partial x_1}(\\gamma(\\alpha)) = \\beta_1 + \\beta_{12}(\\alpha x_2) = \\beta_1 + \\alpha\\beta_{12}x_2 $$\n$$ \\frac{\\partial f}{\\partial x_2}(\\gamma(\\alpha)) = \\beta_2 + \\beta_{12}(\\alpha x_1) = \\beta_2 + \\alpha\\beta_{12}x_1 $$\n现在我们计算 $\\text{IG}_1$ 的积分：\n$$ \\text{IG}_1 = x_1 \\int_{0}^{1} (\\beta_1 + \\alpha\\beta_{12}x_2) d\\alpha = x_1 \\left[ \\beta_1\\alpha + \\frac{\\alpha^2}{2}\\beta_{12}x_2 \\right]_0^1 $$\n$$ \\text{IG}_1 = x_1 \\left( \\beta_1 + \\frac{1}{2}\\beta_{12}x_2 \\right) = \\beta_1 x_1 + \\frac{1}{2}\\beta_{12} x_1 x_2 $$\n类似地，对于 $\\text{IG}_2$：\n$$ \\text{IG}_2 = x_2 \\int_{0}^{1} (\\beta_2 + \\alpha\\beta_{12}x_1) d\\alpha = x_2 \\left[ \\beta_2\\alpha + \\frac{\\alpha^2}{2}\\beta_{12}x_1 \\right]_0^1 $$\n$$ \\text{IG}_2 = x_2 \\left( \\beta_2 + \\frac{1}{2}\\beta_{12}x_1 \\right) = \\beta_2 x_2 + \\frac{1}{2}\\beta_{12} x_1 x_2 $$\n对于这个特定的模型和基线，IG 和 SHAP 归因是相同的。\n\n**3. Shapley 交互指数的推导**\n\n对 $(x_1, x_2)$ 的 Shapley 交互指数由离散二阶差分给出：\n$$ \\text{InteractionIndex} = f(x_1, x_2) - f(x_1, 0) - f(0, x_2) + f(0, 0) $$\n代入函数定义：\n$$ \\text{InteractionIndex} = (\\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2) - (\\beta_1 x_1) - (\\beta_2 x_2) + (0) $$\n$$ \\text{InteractionIndex} = \\beta_{12} x_1 x_2 $$\n这表明交互指数精确地分离出了模型的交互项。\n\n**4. 验证检查分析**\n\n基于以上推导，我们可以分析所需的六个检查。\n- **检查 1**：$|\\sum \\text{IG}_i - f(x_1, x_2)|$。\nIG 归因的总和为 $\\text{IG}_1 + \\text{IG}_2 = (\\beta_1 x_1 + \\frac{1}{2}\\beta_{12} x_1 x_2) + (\\beta_2 x_2 + \\frac{1}{2}\\beta_{12} x_1 x_2) = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2 = f(x_1, x_2)$。因此，差值为 $|f(x_1, x_2) - f(x_1, x_2)| = 0$。\n- **检查 2**：$|\\text{SHAP}_1 - \\text{IG}_1|$。如上所示，$\\text{SHAP}_1 = \\text{IG}_1 = \\beta_1 x_1 + \\frac{1}{2}\\beta_{12} x_1 x_2$。差值为 $0$。\n- **检查 3**：$|\\text{SHAP}_2 - \\text{IG}_2|$。类似地，$\\text{SHAP}_2 = \\text{IG}_2 = \\beta_2 x_2 + \\frac{1}{2}\\beta_{12} x_1 x_2$。差值为 $0$。\n- **检查 4**：$|\\text{InteractionIndex} - \\beta_{12} x_1 x_2|$。如上所示，$\\text{InteractionIndex} = \\beta_{12} x_1 x_2$。差值为 $0$。\n- **检查 5**：$|(\\text{IG}_1 - \\beta_1 x_1) - \\frac{1}{2}\\beta_{12} x_1 x_2|$。代入 $\\text{IG}_1$，我们得到 $|(\\beta_1 x_1 + \\frac{1}{2}\\beta_{12} x_1 x_2 - \\beta_1 x_1) - \\frac{1}{2}\\beta_{12} x_1 x_2| = |\\frac{1}{2}\\beta_{12} x_1 x_2 - \\frac{1}{2}\\beta_{12} x_1 x_2| = 0$。这证实了 IG 将交互作用精确地一半分配给了特征 1。\n- **检查 6**：$|(\\text{IG}_2 - \\beta_2 x_2) - \\frac{1}{2}\\beta_{12} x_1 x_2|$。类似地，代入 $\\text{IG}_2$，我们得到 $|(\\beta_2 x_2 + \\frac{1}{2}\\beta_{12} x_1 x_2 - \\beta_2 x_2) - \\frac{1}{2}\\beta_{12} x_1 x_2| = |\\frac{1}{2}\\beta_{12} x_1 x_2 - \\frac{1}{2}\\beta_{12} x_1 x_2| = 0$。\n\n对于所有测试用例，所有六个检查在理论上预期结果均为 0，实现将证实这一点，直到浮点精度范围内。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares attributions from SHAP and Integrated Gradients (IG)\n    for a two-feature model and verifies the Shapley interaction index.\n    \"\"\"\n    \n    # Test suite of parameter values (beta1, beta2, beta12, x1, x2)\n    test_cases = [\n        (1.5, 2.0, 0.8, 1.0, 3.0),\n        (1.0, 1.0, 0.0, 2.0, -1.0),\n        (-0.5, 0.75, -1.2, 1.2, 0.5),\n        (3.0, -2.0, 0.5, 0.0, 4.0),\n        (0.0, 0.0, 5.0, 0.2, -0.4),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        b1, b2, b12, x1, x2 = case\n\n        # The model function f(x1, x2) = b1*x1 + b2*x2 + b12*x1*x2\n        # A lambda is used for clarity in the interaction index calculation.\n        f = lambda z1, z2: b1 * z1 + b2 * z2 + b12 * z1 * z2\n\n        # 1. Compute SHAP attributions based on the derived formula:\n        # SHAP_i = beta_i * x_i + 0.5 * beta_12 * x_1 * x_2\n        interaction_term_val = b12 * x1 * x2\n        shap1 = b1 * x1 + 0.5 * interaction_term_val\n        shap2 = b2 * x2 + 0.5 * interaction_term_val\n\n        # 2. Compute IG attributions based on the derived formula:\n        # IG_i = beta_i * x_i + 0.5 * beta_12 * x_1 * x_2\n        ig1 = b1 * x1 + 0.5 * interaction_term_val\n        ig2 = b2 * x2 + 0.5 * interaction_term_val\n\n        # 3. Compute Shapley interaction index using the discrete second-order difference\n        # InteractionIndex = f(x1, x2) - f(x1, 0) - f(0, x2) + f(0, 0)\n        interaction_index = f(x1, x2) - f(x1, 0) - f(0, x2) + f(0, 0)\n\n        # Compute the six specified checks\n        \n        # Check 1: Absolute difference between the sum of IG attributions and f(x1, x2)\n        f_val = f(x1, x2)\n        check1 = np.abs((ig1 + ig2) - f_val)\n\n        # Check 2: Absolute difference between SHAP1 and IG1\n        check2 = np.abs(shap1 - ig1)\n\n        # Check 3: Absolute difference between SHAP2 and IG2\n        check3 = np.abs(shap2 - ig2)\n\n        # Check 4: Absolute difference between Shapley interaction index and beta_12 * x_1 * x_2\n        check4 = np.abs(interaction_index - interaction_term_val)\n\n        # Check 5: Absolute difference between IG interaction part for x1 and half the interaction\n        ig1_interaction_part = ig1 - b1 * x1\n        check5 = np.abs(ig1_interaction_part - 0.5 * interaction_term_val)\n\n        # Check 6: Absolute difference between IG interaction part for x2 and half the interaction\n        ig2_interaction_part = ig2 - b2 * x2\n        check6 = np.abs(ig2_interaction_part - 0.5 * interaction_term_val)\n\n        results.extend([check1, check2, check3, check4, check5, check6])\n    \n    # Format the final output as a comma-separated list of real numbers in brackets.\n    # The map to str ensures correct formatting, e.g., 0.0 instead of 0.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}