{
    "hands_on_practices": [
        {
            "introduction": "Many uncertainty estimation techniques model a neural network's outputs (logits) not as single point estimates, but as entire probability distributions. This practice provides a foundational exercise in probabilistic modeling, guiding you through the derivation of a predictive probability when the logits themselves are uncertain . By working through the properties of Gaussian distributions, you will see how aleatoric uncertainty in latent variables propagates to the final classification decision.",
            "id": "3179687",
            "problem": "Consider a binary deep neural network classifier that, for a specific input $x^{\\star}$, outputs per-class logit means and heteroscedastic logit variances. Denote the latent logit for class $c \\in \\{0, 1\\}$ by $z_{c}$, with $z_{c}$ modeled as a Gaussian random variable with mean $\\mu_{c}$ and variance $\\sigma_{c}^{2}$, that is, $z_{c} \\sim \\mathcal{N}(\\mu_{c}, \\sigma_{c}^{2})$. Assume the latent logits are independent across classes. The predictive decision rule selects the class with the largest latent logit. The predictive probability for class $1$ is therefore the probability of the event $z_{1} > z_{0}$ under the joint distribution of $(z_{0}, z_{1})$.\n\nStarting from the definitions of independent Gaussian random variables and the Probability Density Function (PDF) of the Gaussian distribution, derive the distribution of the difference $d = z_1 - z_0$, express the predictive probability $p(y = 1 \\mid x^{\\star})$ as an integral over the appropriate region, and evaluate it in closed-form using the properties of Gaussian distributions.\n\nFinally, compute the numerical value of $p(y = 1 \\mid x^{\\star})$ for $x^{\\star}$ with model outputs $\\mu_1 = 1.3$, $\\mu_0 = 0.6$, $\\sigma_1 = 0.3$, and $\\sigma_0 = 0.4$. Round your numerical answer to four significant figures. Express your final answer as a pure number without any unit.",
            "solution": "The problem statement is critically validated and found to be valid. It is scientifically grounded in probability theory and statistics as applied to uncertainty estimation in deep learning, is well-posed with a unique and stable solution, and is expressed in objective, unambiguous language. All necessary data and conditions for a solution are provided.\n\nThe problem requires us to analyze the predictive probability of a binary classifier whose latent logits, $z_0$ and $z_1$, are modeled as independent Gaussian random variables. We are given:\n$z_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$\n$z_0 \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$\nThe two random variables $z_1$ and $z_0$ are independent. The predictive probability for class $1$ is given by $p(y = 1 \\mid x^{\\star}) = P(z_1 > z_0)$.\n\nFirst, we derive the distribution of the difference $d = z_1 - z_0$. A fundamental property of Gaussian distributions states that a linear combination of independent Gaussian random variables is also a Gaussian random variable. The difference $d$ is a linear combination of $z_1$ and $z_0$ with coefficients $1$ and $-1$, respectively.\n\nThe mean of $d$, denoted $\\mu_d$, is the expectation of the difference:\n$$\n\\mu_d = E[d] = E[z_1 - z_0] = E[z_1] - E[z_0] = \\mu_1 - \\mu_0\n$$\nThe variance of $d$, denoted $\\sigma_d^2$, is the variance of the difference. Due to the independence of $z_1$ and $z_0$, the variance of the sum (or difference) is the sum of the variances:\n$$\n\\sigma_d^2 = \\text{Var}(d) = \\text{Var}(z_1 - z_0) = \\text{Var}(z_1) + \\text{Var}(-1 \\cdot z_0) = \\text{Var}(z_1) + (-1)^2 \\text{Var}(z_0) = \\sigma_1^2 + \\sigma_0^2\n$$\nThus, the distribution of the difference $d$ is a Gaussian distribution with mean $\\mu_1 - \\mu_0$ and variance $\\sigma_1^2 + \\sigma_0^2$:\n$$\nd \\sim \\mathcal{N}(\\mu_1 - \\mu_0, \\sigma_1^2 + \\sigma_0^2)\n$$\nThe Probability Density Function (PDF) of $d$ is given by:\n$$\nf_d(x) = \\frac{1}{\\sqrt{2\\pi(\\sigma_1^2 + \\sigma_0^2)}} \\exp\\left(-\\frac{(x - (\\mu_1 - \\mu_0))^2}{2(\\sigma_1^2 + \\sigma_0^2)}\\right)\n$$\n\nNext, we express the predictive probability $p(y = 1 \\mid x^{\\star})$ as an integral. The event $z_1 > z_0$ is equivalent to the event $z_1 - z_0 > 0$, which is $d > 0$. The probability of this event is the integral of the PDF of $d$ over the region where $x > 0$:\n$$\np(y = 1 \\mid x^{\\star}) = P(d > 0) = \\int_{0}^{\\infty} f_d(x) \\,dx\n$$\nSubstituting the expression for $f_d(x)$:\n$$\np(y = 1 \\mid x^{\\star}) = \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi(\\sigma_1^2 + \\sigma_0^2)}} \\exp\\left(-\\frac{(x - (\\mu_1 - \\mu_0))^2}{2(\\sigma_1^2 + \\sigma_0^2)}\\right) \\,dx\n$$\n\nTo evaluate this integral in closed-form, we perform a change of variables to standardize the distribution. Let $\\mu_d = \\mu_1 - \\mu_0$ and $\\sigma_d = \\sqrt{\\sigma_1^2 + \\sigma_0^2}$. We introduce a new variable $u = \\frac{x - \\mu_d}{\\sigma_d}$. This is a standard normal variable, $u \\sim \\mathcal{N}(0, 1)$. The differential is $du = \\frac{1}{\\sigma_d}dx$, so $dx = \\sigma_d du$. The integration limits must also be transformed:\n- When $x = 0$, the lower limit becomes $u = \\frac{0 - \\mu_d}{\\sigma_d} = -\\frac{\\mu_d}{\\sigma_d}$.\n- When $x \\to \\infty$, the upper limit becomes $u \\to \\infty$.\n\nSubstituting these into the integral gives:\n$$\np(y = 1 \\mid x^{\\star}) = \\int_{-\\mu_d/\\sigma_d}^{\\infty} \\frac{1}{\\sqrt{2\\pi}\\sigma_d} \\exp\\left(-\\frac{u^2}{2}\\right) (\\sigma_d \\,du) = \\int_{-\\mu_d/\\sigma_d}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2}\\right) \\,du\n$$\nThis integral is the definition of the survival function of the standard normal distribution evaluated at $-\\mu_d/\\sigma_d$. It is equivalent to $1 - \\Phi(-\\mu_d/\\sigma_d)$, where $\\Phi(z)$ is the Cumulative Distribution Function (CDF) of the standard normal distribution, $\\Phi(z) = P(Z \\le z)$ for $Z \\sim \\mathcal{N}(0, 1)$. Using the symmetry property of the standard normal distribution, $\\Phi(-z) = 1 - \\Phi(z)$, we have:\n$$\nP(d > 0) = 1 - \\Phi(-\\mu_d/\\sigma_d) = 1 - (1 - \\Phi(\\mu_d/\\sigma_d)) = \\Phi(\\mu_d/\\sigma_d)\n$$\nSubstituting back the expressions for $\\mu_d$ and $\\sigma_d$, the closed-form expression for the predictive probability is:\n$$\np(y = 1 \\mid x^{\\star}) = \\Phi\\left(\\frac{\\mu_1 - \\mu_0}{\\sqrt{\\sigma_1^2 + \\sigma_0^2}}\\right)\n$$\n\nFinally, we compute the numerical value for the given model outputs: $\\mu_1 = 1.3$, $\\mu_0 = 0.6$, $\\sigma_1 = 0.3$, and $\\sigma_0 = 0.4$.\nFirst, we calculate the argument of the $\\Phi$ function:\nThe difference in means is $\\mu_1 - \\mu_0 = 1.3 - 0.6 = 0.7$.\nThe sum of variances is $\\sigma_1^2 + \\sigma_0^2 = (0.3)^2 + (0.4)^2 = 0.09 + 0.16 = 0.25$.\nThe standard deviation of the difference is $\\sqrt{\\sigma_1^2 + \\sigma_0^2} = \\sqrt{0.25} = 0.5$.\nThe argument is therefore:\n$$\n\\frac{\\mu_1 - \\mu_0}{\\sqrt{\\sigma_1^2 + \\sigma_0^2}} = \\frac{0.7}{0.5} = 1.4\n$$\nThe predictive probability is thus:\n$$\np(y = 1 \\mid x^{\\star}) = \\Phi(1.4)\n$$\nUsing a standard normal distribution table or a computational tool, we find the value of the CDF at $z = 1.4$:\n$$\n\\Phi(1.4) \\approx 0.91924334\n$$\nRounding this value to four significant figures gives $0.9192$.",
            "answer": "$$\n\\boxed{0.9192}\n$$"
        },
        {
            "introduction": "A model's raw probability outputs are often poorly calibrated, meaning a predicted confidence of $0.8$ does not correspond to an $80\\%$ chance of being correct. This hands-on coding exercise challenges you to implement and compare different post-hoc calibration techniques, a critical step for producing reliable uncertainty estimates . You will learn to use metrics like Expected Calibration Error (ECE) to quantify and improve the trustworthiness of a model's confidence.",
            "id": "3179715",
            "problem": "You are given a binary classification setting where model outputs are pre-sigmoid scores (logits). The goal is to study uncertainty estimation through calibration and entropy, by comparing raw predictions, linear post-hoc calibration in logit space, and linear post-hoc calibration in probability space. Build a program that implements the following computations and outputs specified quantities for a provided test suite.\n\nDefinitions and foundational base:\n- A binary classifier outputs a real-valued logit $z \\in \\mathbb{R}$ for each input. The predicted probability of the positive class is $p = \\sigma(z)$ where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the logistic sigmoid function.\n- The predictive entropy of a Bernoulli distribution with parameter $p$ is $H(p) = - ( p \\ln p + (1 - p) \\ln(1 - p) )$ using the natural logarithm.\n- The negative log-likelihood (NLL) for binary labels $y \\in \\{0,1\\}$ given probabilities $p$ is $\\mathrm{NLL}(p,y) = - ( y \\ln p + (1 - y) \\ln(1 - p) )$, and the average NLL over a dataset is the arithmetic mean of per-sample NLLs. Minimizing average NLL is a well-established principle for fitting calibration parameters as it corresponds to maximum likelihood estimation under a Bernoulli model.\n- Expected Calibration Error (ECE) assesses calibration by binning predicted confidences and comparing bin-wise average accuracy to bin-wise average confidence. For binary classification, define prediction $\\hat{y} = \\mathbb{I}[p \\geq 0.5]$ and confidence $c = \\max(p, 1 - p)$. Let the confidence interval $[0,1]$ be partitioned into $B$ equal-width bins. For bin $b$, let $n_b$ be the number of samples in that bin, $\\mathrm{acc}_b$ the average of $\\mathbb{I}[\\hat{y} = y]$ over samples in the bin, and $\\mathrm{conf}_b$ the average of $c$ over samples in the bin. The expected calibration error is\n$$\n\\mathrm{ECE} = \\sum_{b=1}^{B} \\frac{n_b}{N} \\left| \\mathrm{acc}_b - \\mathrm{conf}_b \\right|,\n$$\nwhere $N$ is the dataset size. Empty bins are omitted from the sum.\n\nCalibration mappings to compare:\n- Logit-space linear post-hoc calibration (Platt scaling): Given logits $z$, transform $z' = a z + b$ with parameters $a, b \\in \\mathbb{R}$. The calibrated probability is $p' = \\sigma(z')$. Fit $(a,b)$ by minimizing average NLL on the provided dataset.\n- Probability-space linear post-hoc calibration: Given raw probabilities $p = \\sigma(z)$, transform $p'' = c p + d$ using a monotone linear mapping that preserves the unit interval. Parameterize $d = \\sigma(u)$ and $c = (1 - d)\\sigma(v)$ with unconstrained $u,v \\in \\mathbb{R}$ to ensure $p'' \\in (0,1)$ for all $p \\in [0,1]$. Fit $(u,v)$ by minimizing average NLL on the provided dataset, and compute $p''$ via the implied $(c,d)$.\n\nTasks for each dataset:\n1. Compute raw probabilities $p = \\sigma(z)$.\n2. Fit $(a,b)$ and compute calibrated probabilities $p' = \\sigma(a z + b)$.\n3. Fit $(u,v)$, form $(c,d)$, and compute calibrated probabilities $p'' = c p + d$.\n4. Compute $\\mathrm{ECE}$ for $p$, $p'$, and $p''$ using $B = 10$ equal-width bins on $[0,1]$.\n5. Compute the mean predictive entropy $\\overline{H}$ for $p$, $p'$, and $p''$ using $H(p)$ defined above.\n\nYour program must implement these steps using the test suite below and produce, for each dataset, a list of six floats in the order\n$$\n\\left[ \\mathrm{ECE}(p), \\mathrm{ECE}(p'), \\mathrm{ECE}(p''), \\overline{H}(p), \\overline{H}(p'), \\overline{H}(p'') \\right].\n$$\nAggregate the results for all datasets into a single list of lists.\n\nTest suite (each dataset is a pair $(z,y)$ with logits $z$ and labels $y$; numbers are real-valued for $z$ and binary for $y$):\n- Dataset $1$: $z = \\{-2.0, -0.5, 0.0, 0.5, 1.5, -1.2, 2.3, -2.5, 0.8, -0.8\\}$, $y = \\{0, 0, 1, 1, 1, 0, 1, 0, 1, 0\\}$.\n- Dataset $2$: $z = \\{5.0, -5.0, 4.0, -4.0, 6.0, -6.0, 3.5, -3.5\\}$, $y = \\{1, 0, 0, 1, 1, 0, 0, 1\\}$.\n- Dataset $3$: $z = \\{-0.05, 0.07, -0.12, 0.15, -0.2, 0.22, -0.3, 0.35, -0.4, 0.45\\}$, $y = \\{0, 1, 0, 1, 0, 1, 0, 1, 0, 1\\}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results formatted as a comma-separated list of per-dataset result lists, enclosed in square brackets. For example, the output should look like $[ [r_{11}, r_{12}, \\dots, r_{16}], [r_{21}, \\dots, r_{26}], [r_{31}, \\dots, r_{36}] ]$ where each $r_{ij}$ is a float computed as above. No additional text should be printed.",
            "solution": "The user has provided a well-defined problem in the domain of machine learning, specifically focusing on the calibration of binary classifiers. The problem is scientifically grounded, internally consistent, and requires the implementation of standard, verifiable algorithms. I will proceed with a solution.\n\nThe task is to compare three types of probability estimates from a binary classifier's logits ($z$): raw probabilities, probabilities from a linearly calibrated logit-space model (Platt scaling), and probabilities from a linearly calibrated probability-space model. For each of these three probability sets, we must compute two metrics: the Expected Calibration Error (ECE) and the mean predictive entropy.\n\nThis process involves three main stages for each dataset provided:\n1.  **Prediction Generation**:\n    *   **Raw Probabilities ($p$)**: These are computed directly from the logits using the logistic sigmoid function, $p_i = \\sigma(z_i) = \\frac{1}{1 + e^{-z_i}}$.\n    *   **Logit-Space Calibrated Probabilities ($p'$)**: This method, known as Platt scaling, finds optimal parameters $(a,b)$ by transforming the logits $z'_i = a z_i + b$ and then applying the sigmoid function, $p'_i = \\sigma(z'_i)$. The parameters $(a, b)$ are optimized by minimizing the average Negative Log-Likelihood (NLL) on the dataset. The NLL for a single prediction $p$ and true label $y \\in \\{0, 1\\}$ is given by $\\mathrm{NLL}(p, y) = -[y \\ln(p) + (1-y)\\ln(1-p)]$. This objective function is convex, guaranteeing a unique global minimum for $(a,b)$.\n    *   **Probability-Space Calibrated Probabilities ($p''$)**: This method transforms the raw probabilities directly via a linear map $p''_i = c p_i + d$. To ensure that $p''_i$ remains a valid probability (i.e., in the interval $(0,1)$), the parameters $(c,d)$ are reparameterized using unconstrained variables $(u,v) \\in \\mathbb{R}^2$ as $d = \\sigma(u)$ and $c = (1-d)\\sigma(v)$. This construction guarantees $c > 0$ and $d \\in (0,1)$, ensuring that the transformation is monotone and maps the interval $[0,1]$ into $(0,1)$. The parameters $(u, v)$ are found by minimizing the average NLL, similar to Platt scaling. We use numerical optimization to find these parameters.\n\n2.  **Metric Calculation**:\n    *   **Expected Calibration Error (ECE)**: ECE measures the discrepancy between a model's confidence and its accuracy. We first calculate the confidence for each prediction, defined as $c_i = \\max(p_i, 1-p_i)$. The predictions' confidences are then binned into $B=10$ equal-width intervals over $[0,1]$. For each bin $b$, we compute the average confidence $\\mathrm{conf}_b$ and the average accuracy $\\mathrm{acc}_b$. The ECE is the weighted average of the absolute differences $|\\mathrm{conf}_b - \\mathrm{acc}_b|$ over all non-empty bins, weighted by the proportion of samples in each bin.\n    *   **Mean Predictive Entropy ($\\overline{H}$)**: This metric quantifies the average uncertainty of the model's predictions. For each probability $p_i$, the predictive entropy is calculated as $H(p_i) = -[p_i \\ln(p_i) + (1-p_i)\\ln(1-p_i)]$. The mean entropy is the arithmetic average of these values over the entire dataset. A higher mean entropy indicates greater overall uncertainty in the predictions.\n\n3.  **Implementation Strategy**:\n    *   Helper functions for the sigmoid, NLL, predictive entropy, and ECE are implemented.\n    - To find the optimal calibration parameters for both Platt scaling and the probability-space model, we define objective functions corresponding to the average NLL. We then use the `scipy.optimize.minimize` function with the `L-BFGS-B` algorithm to find the parameters that minimize these objectives.\n    - The entire process is encapsulated in a main function that iterates through the provided test datasets, performs all computations, and formats the results into the specified nested list structure for the final output. Numerical stability is ensured by clipping probability values to a small positive distance from $0$ and $1$ before applying the logarithm.\n\nThe program will execute these steps for each of the three datasets in the test suite and collate the six required floating-point values ($\\mathrm{ECE}(p)$, $\\mathrm{ECE}(p')$, $\\mathrm{ECE}(p'')$, $\\overline{H}(p)$, $\\overline{H}(p')$, $\\overline{H}(p'')$) for each dataset into a final list of lists.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Define a small epsilon for numerical stability in log calculations\nEPSILON = 1e-15\n\ndef sigmoid(z):\n    \"\"\"Computes the logistic sigmoid function.\"\"\"\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef predictive_entropy(p):\n    \"\"\"Computes the predictive entropy for a Bernoulli probability.\"\"\"\n    p = np.clip(p, EPSILON, 1.0 - EPSILON)\n    return -(p * np.log(p) + (1.0 - p) * np.log(1.0 - p))\n\ndef compute_nll(p, y):\n    \"\"\"Computes the negative log-likelihood for binary classification.\"\"\"\n    p = np.clip(p, EPSILON, 1.0 - EPSILON)\n    return -(y * np.log(p) + (1.0 - y) * np.log(1.0 - p))\n\ndef compute_ece(p, y, n_bins=10):\n    \"\"\"Computes the Expected Calibration Error (ECE).\"\"\"\n    if len(p) == 0:\n        return 0.0\n        \n    p = np.asarray(p)\n    y = np.asarray(y)\n\n    predictions = (p >= 0.5).astype(int)\n    confidences = np.maximum(p, 1.0 - p)\n    is_correct = (predictions == y).astype(int)\n\n    # Bin confidences. The last bin includes 1.0.\n    bin_indices = np.minimum(n_bins - 1, np.floor(confidences * n_bins)).astype(int)\n    \n    ece = 0.0\n    total_samples = len(p)\n    \n    for b in range(n_bins):\n        in_bin = (bin_indices == b)\n        num_in_bin = np.sum(in_bin)\n        \n        if num_in_bin > 0:\n            bin_accuracy = np.mean(is_correct[in_bin])\n            bin_confidence = np.mean(confidences[in_bin])\n            ece += (num_in_bin / total_samples) * np.abs(bin_accuracy - bin_confidence)\n            \n    return ece\n\ndef fit_platt_scaling(z, y):\n    \"\"\"Fits Platt scaling parameters (a, b) by minimizing NLL.\"\"\"\n    def objective(params):\n        a, b = params\n        p_calibrated = sigmoid(a * z + b)\n        return np.mean(compute_nll(p_calibrated, y))\n\n    initial_params = np.array([1.0, 0.0])\n    result = minimize(objective, initial_params, method='L-BFGS-B')\n    a_opt, b_opt = result.x\n    return a_opt, b_opt\n\ndef fit_prob_space_calibration(p, y):\n    \"\"\"Fits probability-space calibration parameters (u, v) by minimizing NLL.\"\"\"\n    def objective(params):\n        u, v = params\n        d = sigmoid(u)\n        c = (1.0 - d) * sigmoid(v)\n        p_calibrated = c * p + d\n        return np.mean(compute_nll(p_calibrated, y))\n\n    initial_params = np.array([0.0, 0.0])\n    result = minimize(objective, initial_params, method='L-BFGS-B')\n    u_opt, v_opt = result.x\n    \n    d_opt = sigmoid(u_opt)\n    c_opt = (1.0 - d_opt) * sigmoid(v_opt)\n    return c_opt, d_opt\n\ndef solve():\n    \"\"\"Main function to run the full analysis.\"\"\"\n    test_cases = [\n        (np.array([-2.0, -0.5, 0.0, 0.5, 1.5, -1.2, 2.3, -2.5, 0.8, -0.8]),\n         np.array([0, 0, 1, 1, 1, 0, 1, 0, 1, 0])),\n        (np.array([5.0, -5.0, 4.0, -4.0, 6.0, -6.0, 3.5, -3.5]),\n         np.array([1, 0, 0, 1, 1, 0, 0, 1])),\n        (np.array([-0.05, 0.07, -0.12, 0.15, -0.2, 0.22, -0.3, 0.35, -0.4, 0.45]),\n         np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1]))\n    ]\n\n    all_results = []\n    \n    for z, y in test_cases:\n        # 1. Raw probabilities and their metrics\n        p_raw = sigmoid(z)\n        ece_raw = compute_ece(p_raw, y)\n        h_raw = np.mean(predictive_entropy(p_raw))\n\n        # 2. Logit-space calibration (Platt)\n        a_opt, b_opt = fit_platt_scaling(z, y)\n        p_platt = sigmoid(a_opt * z + b_opt)\n        ece_platt = compute_ece(p_platt, y)\n        h_platt = np.mean(predictive_entropy(p_platt))\n        \n        # 3. Probability-space calibration\n        c_opt, d_opt = fit_prob_space_calibration(p_raw, y)\n        p_prob = c_opt * p_raw + d_opt\n        ece_prob = compute_ece(p_prob, y)\n        h_prob = np.mean(predictive_entropy(p_prob))\n        \n        # 4. Aggregate results for the dataset\n        case_results = [\n            ece_raw, ece_platt, ece_prob,\n            h_raw, h_platt, h_prob\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string exactly as specified\n    inner_strs = [f\"[{','.join(map(str, sublist))}]\" for sublist in all_results]\n    print(f\"[{','.join(inner_strs)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Mixup is a powerful data augmentation technique that trains models on convex combinations of input samples, which can improve generalization and calibration. This exercise explores how the interpolation inherent in mixup affects a model's predictive uncertainty, measured by Shannon entropy . Using a simplified but illustrative model, you will investigate the relationship between input ambiguity and the resulting predictive confidence.",
            "id": "3179689",
            "problem": "You will implement and analyze a principled uncertainty measure under a simplified model of mixup for classification in deep learning. Consider a multi-class classifier with $K$ classes. The classifier produces pre-softmax activations (logits) $\\mathbf{z} \\in \\mathbb{R}^{K}$ for an input, and predicts a probability vector $\\mathbf{p} \\in \\Delta^{K-1}$ using the softmax function. The predictive uncertainty of the classifier for a given input is quantified by the Shannon entropy. You will study how the mixup interpolation coefficient $\\lambda$ and an input hardness parameter $h$ jointly affect the entropy.\n\nFundamental base and modeling assumptions are as follows.\n\n- Softmax probabilities: for logits $\\mathbf{z} = (z_1,\\dots,z_K)$, the predicted probabilities are\n$$\np_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)}, \\quad i \\in \\{1,\\dots,K\\}.\n$$\n\n- Shannon entropy (in natural units, nats): for $\\mathbf{p}$ with components $p_i > 0$ and $\\sum_{i=1}^{K} p_i = 1$,\n$$\nH(\\mathbf{p}) = - \\sum_{i=1}^{K} p_i \\log p_i.\n$$\n\n- Input hardness to logit magnitude mapping: each input is associated with a hardness $h \\in [0,1]$ and a true class index $c \\in \\{0,1,\\dots,K-1\\}$. The input induces logits\n$$\n\\mathbf{z}(h,c) = \\alpha(h) \\mathbf{e}_c, \\quad \\text{where} \\quad \\alpha(h) = \\alpha_{\\max}(1 - h)\n$$\nand $\\mathbf{e}_c$ is the one-hot vector with a $1$ at index $c$ and $0$ elsewhere. Thus, smaller $h$ (easier input) yields larger $\\alpha(h)$ and more confident predictions; $h = 1$ yields $\\alpha(h) = 0$ and a uniform predictive distribution.\n\n- Mixup of two inputs under a linear-logit model: given two inputs with $(h_a, c_a)$ and $(h_b, c_b)$ and an interpolation coefficient $\\lambda \\in [0,1]$, the mixed logits are assumed to be the convex combination of the original logits,\n$$\n\\mathbf{z}_{\\text{mix}} = \\lambda \\mathbf{z}(h_a,c_a) + (1-\\lambda) \\mathbf{z}(h_b,c_b).\n$$\nThe predictive probabilities for the mixed input are $\\mathbf{p}_{\\text{mix}} = \\operatorname{softmax}(\\mathbf{z}_{\\text{mix}})$, and its uncertainty is $H(\\mathbf{p}_{\\text{mix}})$.\n\nYour task is to compute $H(\\mathbf{p}_{\\text{mix}})$ for a fixed number of classes $K$ and logit scale $\\alpha_{\\max}$, across a set of test cases that vary $\\lambda$, $(h_a, c_a)$, and $(h_b, c_b)$.\n\nImplementation constraints and test suite.\n\n- Use $K = 3$ classes and $\\alpha_{\\max} = 5.0$.\n- For each test case, you are given a tuple $(\\lambda, c_a, c_b, h_a, h_b)$ with $\\lambda \\in [0,1]$, $c_a, c_b \\in \\{0,1,2\\}$, and $h_a, h_b \\in [0,1]$.\n- For each test case, compute\n$$\n\\begin{aligned}\n\\alpha_a &= \\alpha_{\\max}(1 - h_a), \\\\\n\\alpha_b &= \\alpha_{\\max}(1 - h_b), \\\\\n\\mathbf{z}_a &= \\alpha_a \\mathbf{e}_{c_a}, \\\\\n\\mathbf{z}_b &= \\alpha_b \\mathbf{e}_{c_b}, \\\\\n\\mathbf{z}_{\\text{mix}} &= \\lambda \\mathbf{z}_a + (1-\\lambda) \\mathbf{z}_b, \\\\\n\\mathbf{p}_{\\text{mix}} &= \\operatorname{softmax}(\\mathbf{z}_{\\text{mix}}), \\\\\nH &= - \\sum_{i=1}^{K} p_{\\text{mix},i} \\log p_{\\text{mix},i}.\n\\end{aligned}\n$$\n\nTest suite to cover key regimes.\n\n- Case $1$: $(\\lambda, c_a, c_b, h_a, h_b) = (0.5, 0, 1, 0.0, 0.0)$; both inputs are easy but have different classes, mixed evenly.\n- Case $2$: $(\\lambda, c_a, c_b, h_a, h_b) = (1.0, 2, 1, 0.0, 0.0)$; boundary where the mixture equals input $a$.\n- Case $3$: $(\\lambda, c_a, c_b, h_a, h_b) = (0.0, 0, 1, 0.0, 1.0)$; boundary where the mixture equals input $b$, which is maximally hard.\n- Case $4$: $(\\lambda, c_a, c_b, h_a, h_b) = (0.7, 0, 1, 0.0, 1.0)$; easy input mixed with maximally hard input.\n- Case $5$: $(\\lambda, c_a, c_b, h_a, h_b) = (0.4, 2, 2, 0.3, 0.9)$; same class with different hardness levels.\n- Case $6$: $(\\lambda, c_a, c_b, h_a, h_b) = (0.25, 0, 1, 0.2, 0.4)$; moderate mix with intermediate hardnesses.\n- Case $7$: $(\\lambda, c_a, c_b, h_a, h_b) = (0.3, 1, 0, 1.0, 1.0)$; both inputs are maximally hard.\n\nComputation and output requirements.\n\n- For each case, compute the entropy $H$ in nats (natural logarithms), and round the result to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite above. For example, the printed line must have the exact form $\\left[ v_1, v_2, \\dots, v_7 \\right]$ but using plain characters, where each $v_i$ is the rounded entropy for case $i$.\n- No input should be read; all parameters are as specified above and must be embedded in the program.",
            "solution": "The user-provided problem is assessed to be **valid**. It is scientifically grounded in the fields of deep learning and information theory, mathematically well-posed, objective, and provides a complete and consistent set of definitions and constraints. The problem requires the computation of the Shannon entropy of a predictive distribution generated by a simplified model of mixup, a data augmentation technique in deep learning.\n\nThe solution proceeds by first principles, following the sequence of calculations specified in the problem statement. The core of the problem is to determine the uncertainty, quantified by Shannon entropy $H$, of a predictive probability vector $\\mathbf{p}_{\\text{mix}}$. This vector results from applying the softmax function to a set of pre-softmax activations (logits) $\\mathbf{z}_{\\text{mix}}$, which are themselves a linear interpolation of logits from two base inputs.\n\nThe problem defines the following fundamental relationships:\n1.  **Logit-to-Probability Mapping**: The softmax function transforms a logit vector $\\mathbf{z} \\in \\mathbb{R}^{K}$ into a probability vector $\\mathbf{p} \\in \\Delta^{K-1}$ (the $(K-1)$-simplex) over $K$ classes. For each class $i \\in \\{1, \\dots, K\\}$, the probability $p_i$ is given by:\n    $$\n    p_i(\\mathbf{z}) = \\frac{\\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)}\n    $$\n    This function ensures that $p_i > 0$ for all $i$ and $\\sum_{i=1}^{K} p_i = 1$.\n\n2.  **Uncertainty Quantification**: The Shannon entropy of the probability distribution $\\mathbf{p}$, measured in natural units (nats), is:\n    $$\n    H(\\mathbf{p}) = - \\sum_{i=1}^{K} p_i \\log p_i\n    $$\n    The entropy is maximized for a uniform distribution ($p_i = 1/K$ for all $i$), where $H_{\\max} = \\log K$. It is minimized ($H=0$) for a one-hot distribution (one $p_i=1$, others $0$).\n\n3.  **Input Model**: An input is characterized by a hardness parameter $h \\in [0,1]$ and a true class index $c \\in \\{0, \\dots, K-1\\}$. The corresponding logits are modeled as $\\mathbf{z}(h,c) = \\alpha(h) \\, \\mathbf{e}_c$, where $\\mathbf{e}_c$ is the one-hot vector for class $c$. The function $\\alpha(h) = \\alpha_{\\max}(1 - h)$ maps the hardness to the magnitude of the logit for the true class. An \"easy\" input ($h \\approx 0$) results in a large positive logit for the correct class, leading to a confident (low-entropy) prediction. A \"hard\" input ($h \\approx 1$) results in near-zero logits, leading to an uncertain (high-entropy) uniform prediction.\n\n4.  **Mixup Model**: The logits for a mixed input are a convex combination of the logits of two base inputs, $(h_a, c_a)$ and $(h_b, c_b)$, with an interpolation coefficient $\\lambda \\in [0,1]$:\n    $$\n    \\mathbf{z}_{\\text{mix}} = \\lambda \\, \\mathbf{z}(h_a,c_a) + (1-\\lambda)\\, \\mathbf{z}(h_b,c_b)\n    $$\n\nThe task specifies constant parameters $K = 3$ and $\\alpha_{\\max} = 5.0$. The calculation for each test case $\\left(\\lambda, c_a, c_b, h_a, h_b\\right)$ proceeds as follows:\n\nFirst, we compute the logit magnitudes for the two base inputs:\n$$\n\\alpha_a = \\alpha_{\\max}(1 - h_a)\n$$\n$$\n\\alpha_b = \\alpha_{\\max}(1 - h_b)\n$$\n\nNext, we construct the logit vectors for the base inputs. Since $K=3$, these are vectors in $\\mathbb{R}^3$:\n$$\n\\mathbf{z}_a = \\alpha_a \\, \\mathbf{e}_{c_a}\n$$\n$$\n\\mathbf{z}_b = \\alpha_b \\, \\mathbf{e}_{c_b}\n$$\n\nThe mixed logit vector $\\mathbf{z}_{\\text{mix}}$ is then computed:\n$$\n\\mathbf{z}_{\\text{mix}} = \\lambda \\mathbf{z}_a + (1 - \\lambda) \\mathbf{z}_b\n$$\n\nFrom $\\mathbf{z}_{\\text{mix}}$, we calculate the predictive probability vector $\\mathbf{p}_{\\text{mix}}$ using the softmax function:\n$$\n\\mathbf{p}_{\\text{mix}} = \\operatorname{softmax}(\\mathbf{z}_{\\text{mix}})\n$$\n\nFinally, we compute the Shannon entropy of this distribution:\n$$\nH = H(\\mathbf{p}_{\\text{mix}}) = - \\sum_{i=0}^{2} p_{\\text{mix},i} \\log p_{\\text{mix},i}\n$$\n\nLet us walk through **Case 1** as a concrete example: $(\\lambda, c_a, c_b, h_a, h_b) = (0.5, 0, 1, 0.0, 0.0)$.\n1.  Compute logit magnitudes:\n    $h_a = 0.0 \\implies \\alpha_a = 5.0(1 - 0.0) = 5.0$\n    $h_b = 0.0 \\implies \\alpha_b = 5.0(1 - 0.0) = 5.0$\n\n2.  Construct base logit vectors (with $c_a=0, c_b=1$):\n    $\\mathbf{z}_a = 5.0 \\cdot [1, 0, 0]^T = [5.0, 0, 0]^T$\n    $\\mathbf{z}_b = 5.0 \\cdot [0, 1, 0]^T = [0, 5.0, 0]^T$\n\n3.  Compute mixed logits with $\\lambda=0.5$:\n    $\\mathbf{z}_{\\text{mix}} = 0.5 \\cdot [5.0, 0, 0]^T + (1 - 0.5) \\cdot [0, 5.0, 0]^T = [2.5, 2.5, 0]^T$\n\n4.  Apply softmax to $\\mathbf{z}_{\\text{mix}} = [2.5, 2.5, 0]^T$:\n    The sum in the denominator is $S = \\exp(2.5) + \\exp(2.5) + \\exp(0) \\approx 12.1825 + 12.1825 + 1.0 = 25.3650$.\n    The probabilities are:\n    $p_{\\text{mix},0} = \\exp(2.5) / S \\approx 0.48029$\n    $p_{\\text{mix},1} = \\exp(2.5) / S \\approx 0.48029$\n    $p_{\\text{mix},2} = \\exp(0) / S \\approx 0.03942$\n\n5.  Compute entropy:\n    $H = - (p_{\\text{mix},0} \\log p_{\\text{mix},0} + p_{\\text{mix},1} \\log p_{\\text{mix},1} + p_{\\text{mix},2} \\log p_{\\text{mix},2})$\n    $H \\approx - (2 \\cdot 0.48029 \\cdot \\log(0.48029) + 0.03942 \\cdot \\log(0.03942))$\n    $H \\approx - (2 \\cdot 0.48029 \\cdot (-0.7333) + 0.03942 \\cdot (-3.2335)) \\approx 0.832030$\n\nThis procedure is repeated for all seven test cases. Notably, for cases where all logits are zero (e.g., Case 3 and Case 7, where $h_a=h_b=1.0$), the resulting probability distribution is uniform, $\\mathbf{p} = [1/3, 1/3, 1/3]^T$, yielding the maximum possible entropy for $K=3$, which is $H = \\log(3) \\approx 1.098612$. For cases where the resulting logits have one large positive value and others near zero (e.g., Cases 2, 4, 5), the distribution is highly peaked, and the entropy is close to zero, indicating high certainty.\n\nFor a robust implementation, it is advisable to use numerically stable library functions for softmax and entropy calculations. The softmax computation should involve subtracting the maximum logit value from all logits before exponentiation to prevent numerical overflow. Standard scientific libraries like `SciPy` provide these functions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import softmax\nfrom scipy.stats import entropy\n\ndef solve():\n    \"\"\"\n    Computes the Shannon entropy of mixed-up predictive distributions\n    for a series of test cases.\n    \"\"\"\n    # Define the fixed parameters from the problem statement.\n    K = 3\n    alpha_max = 5.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (lambda, c_a, c_b, h_a, h_b)\n        (0.5, 0, 1, 0.0, 0.0),   # Case 1\n        (1.0, 2, 1, 0.0, 0.0),   # Case 2\n        (0.0, 0, 1, 0.0, 1.0),   # Case 3\n        (0.7, 0, 1, 0.0, 1.0),   # Case 4\n        (0.4, 2, 2, 0.3, 0.9),   # Case 5\n        (0.25, 0, 1, 0.2, 0.4),  # Case 6\n        (0.3, 1, 0, 1.0, 1.0),   # Case 7\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack parameters for the current test case.\n        lmbda, c_a, c_b, h_a, h_b = case\n\n        # 1. Compute logit magnitudes alpha_a and alpha_b.\n        alpha_a = alpha_max * (1 - h_a)\n        alpha_b = alpha_max * (1 - h_b)\n\n        # 2. Construct the base logit vectors z_a and z_b.\n        # e_ca is the one-hot vector for class c_a.\n        z_a = np.zeros(K)\n        z_a[c_a] = alpha_a\n\n        # e_cb is the one-hot vector for class c_b.\n        z_b = np.zeros(K)\n        z_b[c_b] = alpha_b\n\n        # 3. Compute the mixed logit vector z_mix.\n        z_mix = lmbda * z_a + (1 - lmbda) * z_b\n\n        # 4. Compute the predictive probabilities p_mix using the softmax function.\n        # scipy.special.softmax is numerically stable.\n        p_mix = softmax(z_mix)\n\n        # 5. Compute the Shannon entropy H in nats.\n        # scipy.stats.entropy calculates it in nats by default (base=e).\n        # Softmax ensures probabilities are > 0, so no special handling for log(0).\n        H = entropy(p_mix)\n        \n        # 6. Round the result to 6 decimal places as required and append.\n        results.append(round(H, 6))\n\n    # Format the results into the required string format \"[v1,v2,...,v7]\".\n    # Using map(str, ...) converts each float to a string.\n    # We must ensure that floating point numbers are formatted without trailing zeros\n    # if they are not needed, which is standard string conversion behavior.\n    # The problem asks for 6 decimal places, so the `round` function handles this a priori.\n    output_str = f\"[{','.join(map(str, results))}]\"\n\n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}