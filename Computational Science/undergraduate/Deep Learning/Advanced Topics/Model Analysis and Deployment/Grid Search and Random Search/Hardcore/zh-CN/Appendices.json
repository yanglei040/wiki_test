{
    "hands_on_practices": [
        {
            "introduction": "理论上，随机搜索在高维空间中通常比网格搜索更有效，但这究竟是为什么呢？本练习将通过构建一个具有狭窄“山脊”的合成损失函数来直观地揭示答案。通过亲手实现这两种搜索方法，你将观察到轴对齐的网格搜索如何可能完全错过倾斜的最优区域，而随机搜索则能更可靠地发现它，从而为你提供关于随机搜索有效性的深刻直觉 。",
            "id": "3133087",
            "problem": "将深度学习中的超参数优化视为在超参数空间上最小化经验风险函数的任务。设超参数空间为矩形 $\\mathcal{H} = [\\eta_{\\min}, \\eta_{\\max}] \\times [\\lambda_{\\min}, \\lambda_{\\max}]$。由超参数引起的经验风险可以抽象为一个合成函数曲面 $f(\\eta, \\lambda)$，其设计旨在呈现尖锐的山脊。假设有以下一类函数曲面，其具有一个由弧度制角度 $\\theta$ 定向的狭长山谷（低损失山脊）：\n$$\nf(\\eta, \\lambda) = \\frac{\\left(\\cos\\theta \\cdot \\eta + \\sin\\theta \\cdot \\lambda - c\\right)^2}{w^2} + \\beta \\left(\\eta^2 + \\lambda^2\\right),\n$$\n其中 $c$ 是一个定位山脊的标量偏移，$w > 0$ 是控制尖锐度的山脊半宽，而 $\\beta > 0$ 是一个碗状曲率系数。目标是最小化 $f(\\eta, \\lambda)$。\n\n基本原理：\n- 超参数搜索问题是在固定的评估预算下找到 $\\arg\\min_{(\\eta,\\lambda) \\in \\mathcal{H}} f(\\eta,\\lambda)$。这基于经验风险最小化原则以及网格搜索和随机搜索的定义。\n- 轴对齐网格搜索在一组轴对齐的等间距点阵上评估 $f$。具体来说，为 $\\eta$ 选择 $n_\\eta$ 个点，为 $\\lambda$ 选择 $n_\\lambda$ 个点，并评估所有 $n_\\eta \\cdot n_\\lambda$ 种组合。\n- 随机搜索在从 $\\mathcal{H}$ 中独立同分布 (i.i.d.) 地均匀采样的 $N$ 个点上评估 $f$。使用带种子的随机数生成器 (RNG) 来确保采样的可复现性。\n\n你必须在合成函数曲面上实现轴对齐网格搜索和随机搜索，并展示当山脊与坐标轴未对齐时，轴对齐网格搜索可能失败而随机搜索成功的案例。成功的定义是找到一个等于或低于指定阈值 $\\tau$ 的函数值，该阈值代表测试用例的“近优”损失。\n\n角度单位说明：所有角度（变量 $\\theta$）必须以弧度表示。\n\n实现以下包含三个案例的测试套件。对于每个案例，程序必须：\n- 使用给定的参数构建 $f(\\eta,\\lambda)$。\n- 使用指定的预算 $n_\\eta$ 和 $n_\\lambda$ 运行轴对齐网格搜索。\n- 使用指定的预算 $N$ 和随机种子运行随机搜索。\n- 计算一个布尔值，指示在该案例中随机搜索是否成功而网格搜索失败。\n\n所有测试的超参数域：$[\\eta_{\\min}, \\eta_{\\max}] = [-1, 1]$ 和 $[\\lambda_{\\min}, \\lambda_{\\max}] = [-1, 1]$。\n\n测试案例：\n- 案例 A（轴对齐的山脊；“理想路径”）：\n  - $\\theta = 0$, $c = 0$, $w = 0.25$, $\\beta = 0.05$, $n_\\eta = 7$, $n_\\lambda = 7$, $N = 49$, seed $= 123$, threshold $\\tau = 0.5$。\n  - 解释：山脊与 $\\eta$ 轴对齐且足够宽。两种方法都应该成功。\n- 案例 B（对角线山脊；未对齐；网格搜索失败，随机搜索成功）：\n  - $\\theta = \\pi/4$, $c = 0.1$, $w = 0.03$, $\\beta = 0.05$, $n_\\eta = 7$, $n_\\lambda = 7$, $N = 200$, seed $= 456$, threshold $\\tau = 1.0$。\n  - 解释：山脊狭窄且未对齐。轴对齐的网格样本无法足够接近最优区域；具有更大预算的随机搜索应该会成功。\n- 案例 C（对角线山脊；极度尖锐；两者都失败）：\n  - $\\theta = \\pi/4$, $c = 0.1$, $w = 0.004$, $\\beta = 0.05$, $n_\\eta = 7$, $n_\\lambda = 7$, $N = 49$, seed $= 789$, threshold $\\tau = 0.4$。\n  - 解释：山脊极度尖锐，使得两种方法在给定预算下都不太可能达到阈值。\n\n某个方法在某个案例中的成功标准是：其在所有评估点上观测到的最小值 $\\min f$ 小于或等于该案例的 $\\tau$。\n\n输出规格：\n- 对于每个案例，生成一个布尔值，当且仅当随机搜索成功且网格搜索失败时，该值为真。\n- 你的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，例如：\"[True,False,True]\"。",
            "solution": "该问题被评估为有效。它在科学上基于数值优化和机器学习的原理，特别是超参数调优。该问题是良构的，提供了一个清晰定义的数学函数、对两种搜索算法（网格搜索和随机搜索）的无歧义描述，以及针对三个不同测试案例的一整套参数。目标是精确的，成功标准也得到了形式化定义。不存在矛盾、歧义或事实错误。\n\n这个问题的核心是比较轴对齐网格搜索和均匀随机搜索在有界超参数域 $\\mathcal{H} = [-1, 1] \\times [-1, 1]$ 上最小化合成损失函数 $f(\\eta, \\lambda)$ 的功效。该函数由下式给出：\n$$\nf(\\eta, \\lambda) = \\frac{\\left(\\cos\\theta \\cdot \\eta + \\sin\\theta \\cdot \\lambda - c\\right)^2}{w^2} + \\beta \\left(\\eta^2 + \\lambda^2\\right)\n$$\n该函数旨在模拟超参数优化中遇到的具有挑战性的函数曲面。它由两个主要部分组成：\n$1$. 项 $\\frac{\\left(\\cos\\theta \\cdot \\eta + \\sin\\theta \\cdot \\lambda - c\\right)^2}{w^2}$ 定义了一个抛物线形山谷。该项的最小值是 $0$，出现在由 $\\cos\\theta \\cdot \\eta + \\sin\\theta \\cdot \\lambda - c = 0$ 定义的直线上。这条线代表了一个低损失值的“山脊”。参数 $\\theta$ 控制该山脊在 $(\\eta, \\lambda)$ 平面上的方向，$c$ 决定其距原点的偏移，而 $w > 0$ 控制其宽度。较小的 $w$ 对应于更尖锐、更狭窄的山脊，使其成为任何搜索算法的更小目标。\n$2$. 项 $\\beta \\left(\\eta^2 + \\lambda^2\\right)$ 为函数曲面增加了一个凸形的碗状分量，其中 $\\beta > 0$ 是一个曲率系数。该项起到一种正则化的作用，确保存在唯一的全局最小值且位于原点 $(0,0)$ 附近。$f$ 的最终最小值是在山脊上和靠近原点之间的一种平衡。\n\n两种搜索算法的实现如下：\n\n**轴对齐网格搜索：** 此方法在形成规则、轴对齐网格的点上评估函数 $f(\\eta, \\lambda)$。对于 $\\eta$ 超参数的 $n_\\eta$ 个点和 $\\lambda$ 超参数的 $n_\\lambda$ 个点的预算，我们生成两组等间距的点：\n$$\n\\eta_i = -1 + i \\frac{2}{n_\\eta - 1}, \\quad i = 0, 1, \\dots, n_\\eta - 1\n$$\n$$\n\\lambda_j = -1 + j \\frac{2}{n_\\lambda - 1}, \\quad j = 0, 1, \\dots, n_\\lambda - 1\n$$\n然后在所有 $n_\\eta \\times n_\\lambda$ 个点对 $(\\eta_i, \\lambda_j)$ 上评估该函数。此方法的主要局限在于其采样模式。如果一个低损失的狭窄山脊未与网格轴对齐（即 $\\theta$ 不是 $\\pi/2$ 的倍数），所有网格点都有可能落在山脊的高损失斜坡上，从而有效地“错过”最优区域。\n\n**随机搜索：** 此方法在从搜索空间 $\\mathcal{H}$ 中独立均匀采样的 $N$ 个点上评估 $f(\\eta, \\lambda)$。每个点 $(\\eta_k, \\lambda_k)$（$k=1, \\dots, N$）都是从 $[-1, 1] \\times [-1, 1]$ 上的均匀分布中抽取的。正如 Bergstra 和 Bengio (2012) 所强调的，随机搜索的主要优势在于其对损失函数曲面几何形状的鲁棒性。通过随机采样，它不受固定轴对齐的约束。对于搜索空间中任何具有非零面积的子区域（例如狭窄的山脊），在其中放置一个样本的概率会随着样本数量 $N$ 的增加而增加。这使得当问题的“有效维度”较低时（即当函数值主要由一小部分或参数组合决定时），它在统计上更有可能找到一个好的解决方案，就像在由山脊主导的函数曲面情况中一样。\n\n测试案例的评估过程如下：\n\n**案例 A：** $\\theta = 0$，$w = 0.25$。山脊与 $\\lambda$ 轴对齐（因为 $\\cos(0)=1, \\sin(0)=0$，山脊位于 $\\eta-c=0$，即 $\\eta=0$）。网格非常适合沿着该轴进行采样。山脊也相对较宽 ($w=0.25$)。预计网格搜索和随机搜索都能找到具有低 $f$ 值的点，因此都会成功。条件 `random_succeeds and not grid_succeeds` 将为 `False`。\n\n**案例 B：** $\\theta = \\pi/4$，$w = 0.03$。山脊是对角且狭窄的。对于 $n_\\eta = 7, n_\\lambda = 7$，网格点在每个轴上的间距为 $2/(7-1) \\approx 0.333$。鉴于山脊的狭窄性 ($w=0.03$)，所有 $49$ 个网格点极有可能都远离最小损失线，导致函数值很高。因此，预计网格搜索将失败。随机搜索具有更大的预算 $N=200$ 个样本，在狭窄的对角山谷内放置至少一个样本的概率要高得多，从而能够获得低于阈值 $\\tau=1.0$ 的函数值。预计该条件将为 `True`。\n\n**案例 C：** $\\theta = \\pi/4$，$w = 0.004$。山脊是对角且极度尖銳的。低损失区域的有效面积非常小。在 $N=49$ 个样本的有限预算下，随机点落入这个微小区域的概率非常低。同样，网格搜索也会因与案例 B 相同的原因而失败。因此，预计两种方法都将无法找到低于阈值 $\\tau=0.4$ 的点。该条件将为 `False`。\n\n该实现将为每个案例计算每种方法找到的最小函数值，并将其与指定的阈值 $\\tau$ 进行比较以确定成功或失败，最终计算出所需的布尔输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares grid search and random search on synthetic \n    hyperparameter optimization landscapes.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"theta\": 0.0,\n            \"c\": 0.0,\n            \"w\": 0.25,\n            \"beta\": 0.05,\n            \"n_eta\": 7,\n            \"n_lambda\": 7,\n            \"N\": 49,\n            \"seed\": 123,\n            \"tau\": 0.5\n        },\n        {\n            \"name\": \"Case B\",\n            \"theta\": np.pi / 4.0,\n            \"c\": 0.1,\n            \"w\": 0.03,\n            \"beta\": 0.05,\n            \"n_eta\": 7,\n            \"n_lambda\": 7,\n            \"N\": 200,\n            \"seed\": 456,\n            \"tau\": 1.0\n        },\n        {\n            \"name\": \"Case C\",\n            \"theta\": np.pi / 4.0,\n            \"c\": 0.1,\n            \"w\": 0.004,\n            \"beta\": 0.05,\n            \"n_eta\": 7,\n            \"n_lambda\": 7,\n            \"N\": 49,\n            \"seed\": 789,\n            \"tau\": 0.4\n        }\n    ]\n\n    def landscape_function(eta, lamb, theta, c, w, beta):\n        \"\"\"\n        Calculates the value of the synthetic loss function f(eta, lambda).\n        This function is vectorized to work with numpy arrays.\n        \"\"\"\n        term1_numerator = np.cos(theta) * eta + np.sin(theta) * lamb - c\n        term1 = (term1_numerator**2) / (w**2)\n        term2 = beta * (eta**2 + lamb**2)\n        return term1 + term2\n\n    def run_grid_search(params):\n        \"\"\"\n        Performs an axis-aligned grid search and checks for success.\n        \"\"\"\n        eta_points = np.linspace(-1.0, 1.0, params['n_eta'])\n        lambda_points = np.linspace(-1.0, 1.0, params['n_lambda'])\n        eta_grid, lambda_grid = np.meshgrid(eta_points, lambda_points)\n\n        f_values = landscape_function(\n            eta_grid, lambda_grid,\n            params['theta'], params['c'], params['w'], params['beta']\n        )\n\n        min_f = np.min(f_values)\n        return min_f = params['tau']\n\n    def run_random_search(params):\n        \"\"\"\n        Performs a random search and checks for success.\n        Uses a fixed seed for reproducibility.\n        \"\"\"\n        np.random.seed(params['seed'])\n        eta_samples = np.random.uniform(-1.0, 1.0, size=params['N'])\n        lambda_samples = np.random.uniform(-1.0, 1.0, size=params['N'])\n\n        f_values = landscape_function(\n            eta_samples, lambda_samples,\n            params['theta'], params['c'], params['w'], params['beta']\n        )\n\n        min_f = np.min(f_values)\n        return min_f = params['tau']\n\n    results = []\n    for case in test_cases:\n        # Run both search methods for the current case\n        grid_succeeds = run_grid_search(case)\n        random_succeeds = run_random_search(case)\n        \n        # Determine if random search succeeds AND grid search fails\n        case_result = random_succeeds and not grid_succeeds\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The map to str is necessary as the list contains booleans.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在实践中，我们有时会凭直觉固定一个超参数，然后去细致地调整另一个。这种看似高效的策略可能隐藏着一个陷阱，即“阻塞效应”。本练习旨在模拟这一现象：一个次优的固定选择会如何限制我们找到全局最优解。通过将这种受限的网格搜索与能够联合探索整个参数空间的随机搜索进行比较，你将深刻理解为何同时对所有重要超参数进行采样是至关重要的 。",
            "id": "3133107",
            "problem": "要求您编写一个完整的确定性程序，模拟并量化当一个超参数固定在次优值时网格搜索中出现的阻塞效应，并将其与随机搜索摆脱此类阻塞的能力进行比较。此问题的背景是与深度学习超参数调优相关的人工合成验证损失表面，涉及两个标量超参数：学习率（用 $\\,\\eta\\,$ 表示）和权重衰减（用 $\\,\\lambda\\,$ 表示）。目标是最小化损失函数 $\\,f(\\eta,\\lambda)\\,$。\n\n基本原理和定义：\n- 一种搜索策略会评估一组超参数对，并返回观测到的 $\\,f\\,$ 的最佳值。\n- 带阻塞的网格搜索：将一个超参数固定在选定值 $\\,\\lambda_{\\mathrm{fix}}\\,$，并在 $\\,N\\,$ 个网格点上评估另一个超参数 $\\,\\eta\\,$。\n- 随机搜索：从一个指定分布中联合采样 $\\,N\\,$ 个独立点，对两个超参数进行取值。\n\n人工合成的目标函数：\n- 定义 $\\,x = \\log_{10}(\\eta)\\,$ 和 $\\,y = \\log_{10}(\\lambda)\\,$。\n- 设搜索域为 $\\,x \\in [-4,0]\\,$ 和 $\\,y \\in [-5,-1]\\,$，这对应于 $\\,\\eta \\in [10^{-4},10^{0}]\\,$ 和 $\\,\\lambda \\in [10^{-5},10^{-1}]\\,$.\n- 定义\n$$\nf(\\eta,\\lambda) \\;=\\; (x+2)^2 \\;+\\; 2.5\\,(y+3)^2 \\;+\\; 0.3\\,(x+2)(y+3) \\;+\\; 0.1\\,\\sin(5x)\\,\\sin(5y),\n$$\n其中 $\\,x=\\log_{10}(\\eta)\\,$ 且 $\\,y=\\log_{10}(\\lambda)\\,$。该函数在 $\\,x=-2\\,$ 和 $\\,y=-3\\,$ 附近（即 $\\,\\eta=10^{-2}\\,$ 和 $\\,\\lambda=10^{-3}\\,$ 附近）编码了一个具有轻微交互作用和波纹的盆地。\n\n需要实现的算法：\n- 带阻塞的网格搜索：\n  - 给定预算 $\\,N\\,$ 和一个固定的 $\\,\\lambda_{\\mathrm{fix}}\\,$，通过在闭区间 $\\,[-4,0]\\,$（即包括两个端点）上为 $\\,x=\\log_{10}(\\eta)\\,$ 选择 $\\,N\\,$ 个等距点，来构造 $\\,N\\,$ 个 $\\,\\eta\\,$ 的值，然后将每个 $\\,x\\,$ 映射到 $\\,\\eta=10^x\\,$。\n  - 在这 $\\,N\\,$ 个点上评估 $\\,f(\\eta,\\lambda_{\\mathrm{fix}})\\,$ 并报告最小值。当 $\\,N=1\\,$ 时，唯一的网格点是左端点 $\\,x=-4\\,$。\n- 随机搜索：\n  - 给定预算 $\\,N\\,$ 和随机种子 $\\,s\\,$，抽取 $\\,N\\,$ 个独立样本，其中 $\\,x \\sim \\mathrm{Uniform}([-4,0])\\,$ 且 $\\,y \\sim \\mathrm{Uniform}([-5,-1])\\,$。然后映射到 $\\,\\eta=10^x\\,$ 和 $\\,\\lambda=10^y\\,$。\n  - 在这 $\\,N\\,$ 个点上评估 $\\,f(\\eta,\\lambda)\\,$ 并报告最小值。\n\n每个测试用例需报告的量：\n- 定义随机搜索相对于受阻塞网格搜索的优势为\n$$\n\\Delta \\;=\\; f_{\\min}^{\\mathrm{grid}} \\;-\\; f_{\\min}^{\\mathrm{rand}},\n$$\n其中 $\\,f_{\\min}^{\\mathrm{grid}}\\,$ 是受阻塞的网格搜索找到的最佳值，$\\,f_{\\min}^{\\mathrm{rand}}\\,$ 是在相同预算 $\\,N\\,$ 下随机搜索找到的最佳值。一个正的 $\\,\\Delta\\,$ 表示随机搜索取得了更低（更好）的损失值。\n\n测试套件：\n- 使用以下测试用例，每个用例指定为一个三元组 $\\,(\\,N,\\,\\lambda_{\\mathrm{fix}},\\,s\\,)\\,$:\n  - 用例 $\\,1$：$\\,(\\,30,\\,10^{-1},\\,2027\\,)\\,$。\n  - 用例 $\\,2$：$\\,(\\,1,\\,10^{-1},\\,123\\,)\\,$。\n  - 用例 $\\,3$：$\\,(\\,30,\\,10^{-3},\\,2027\\,)\\,$。\n  - 用例 $\\,4$：$\\,(\\,10,\\,10^{-2},\\,42\\,)\\,$。\n对于每个用例，使用所述的算法和域精确计算如上定义的 $\\,\\Delta\\,$。必须通过为该用例的生成器设置种子 $\\,s\\,$ 来使随机性确定化。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按顺序排列的四个测试用例的结果 $\\,\\Delta\\,$，格式为用方括号括起来的逗号分隔列表。每个数字应格式化为定点小数，小数点后恰好有 $\\,6\\,$ 位数字，例如 $\\,\\bigl[0.123456,-0.010000,0.000001,0.250000\\bigr]\\,$。",
            "solution": "该问题是有效的，因为它基于超参数优化的科学原理，在数学上是适定的，并为所有必需的组件提供了完整而无歧义的规范。通过实现指定的算法和目标函数，可以推导出确定性的解。\n\n问题的核心是在一个人造的验证损失表面上，比较两种超参数搜索策略——受阻塞的网格搜索和随机搜索的性能。性能由优势值 $\\Delta$ 量化，该值衡量随机搜索找到的损失比受阻塞的网格搜索找到的损失好（低）多少。\n\n分析在超参数的对数空间中进行，这在机器学习中是处理学习率和权重衰减等通常对性能有乘性效应的参数时的标准做法。超参数是学习率 $\\eta$ 和权重衰减 $\\lambda$。它们的对数变换是 $x = \\log_{10}(\\eta)$ 和 $y = \\log_{10}(\\lambda)$。\n\n搜索域在此对数空间中被定义为一个矩形：$x \\in [-4, 0]$ 和 $y \\in [-5, -1]$。这对应于 $\\eta \\in [10^{-4}, 10^0]$ 和 $\\lambda \\in [10^{-5}, 10^{-1}]$。\n\n需要最小化的人工合成损失函数由下式给出：\n$$f(x, y) = (x+2)^2 + 2.5(y+3)^2 + 0.3(x+2)(y+3) + 0.1\\sin(5x)\\sin(5y)$$\n该函数是一个以 $(x,y)=(-2,-3)$ 附近为中心的二次碗型函数，代表了良好超参数的区域，并带有一个交互项和一个小的高频正弦分量，以模仿真实损失地貌的复杂、非凸性质。\n\n解决方案的步骤是首先实现这个函数，然后按规定实现两种搜索算法。\n\n**1. 受阻塞的网格搜索算法**\n\n该算法模拟了一种常见但可能存在缺陷的调优策略，即固定一个超参数，同时改变另一个。\n给定 $N$ 次评估的预算和一个固定的超参数值 $\\lambda_{\\mathrm{fix}}$，其在对数空间中的对应值为 $y_{\\mathrm{fix}} = \\log_{10}(\\lambda_{\\mathrm{fix}})$。搜索随后被限制在搜索域内的直线 $y = y_{\\mathrm{fix}}$ 上。\n\n该算法通过在另一个超参数 $x$ 的整个范围 $[-4, 0]$ 上创建一个均匀网格来生成 $N$ 个评估点。\n- 如果 $N1$，网格点通过 `linspace` 生成，在 -4 到 0 之间（含两端）创建 $N$ 个等距点。这些点是 $x_i = -4 + i \\cdot \\frac{4}{N-1}$，其中 $i \\in \\{0, 1, \\dots, N-1\\}$。\n- 如果 $N=1$，问题规定评估的单一点是区间的左端点 $x_0 = -4$。\n\n在每个网格点 $(x_i, y_{\\mathrm{fix}})$ 上评估损失函数 $f(x, y_{\\mathrm{fix}})$。在这些评估中找到的最小值即为搜索结果 $f_{\\min}^{\\mathrm{grid}}$。\n$$f_{\\min}^{\\mathrm{grid}} = \\min_{i \\in \\{0, \\dots, N-1\\}} f(x_i, y_{\\mathrm{fix}})$$\n\n**2. 随机搜索算法**\n\n该算法通过同时对两个超参数进行采样，来更广泛地探索超参数空间。\n给定 $N$ 次评估的预算和用于可复现性的随机种子 $s$，该算法按以下步骤进行：\n首先，用 $s$ 为伪随机数生成器设置种子。然后，从搜索域中采样 $N$ 个独立点 $(x_j, y_j)$，$j \\in \\{1, \\dots, N\\}$。采样分布在其各自的区间上是均匀的：\n$$x_j \\sim \\mathrm{Uniform}([-4, 0])$$\n$$y_j \\sim \\mathrm{Uniform}([-5, -1])$$\n在每个随机采样的点 $(x_j, y_j)$ 上评估损失函数 $f(x_j, y_j)$。找到的最小值即为搜索结果 $f_{\\min}^{\\mathrm{rand}}$。\n$$f_{\\min}^{\\mathrm{rand}} = \\min_{j \\in \\{1, \\dots, N\\}} f(x_j, y_j)$$\n\n**3. 优势计算与测试用例执行**\n\n对于每个以元组 $(N, \\lambda_{\\mathrm{fix}}, s)$ 形式给出的测试用例，都根据上述算法计算 $f_{\\min}^{\\mathrm{grid}}$ 和 $f_{\\min}^{\\mathrm{rand}}$。然后，随机搜索相对于受阻塞网格搜索的优势 $\\Delta$ 计算如下：\n$$\\Delta = f_{\\min}^{\\mathrm{grid}} - f_{\\min}^{\\mathrm{rand}}$$\n一个正的 $\\Delta$ 值表示随机搜索找到了比受阻塞网格搜索更好（更低）的损失值。\n\n此过程将对以下四个测试用例中的每一个执行：\n- 用例 1：$(N, \\lambda_{\\mathrm{fix}}, s) = (30, 10^{-1}, 2027)$\n- 用例 2：$(N, \\lambda_{\\mathrm{fix}}, s) = (1, 10^{-1}, 123)$\n- 用例 3：$(N, \\lambda_{\\mathrm{fix}}, s) = (30, 10^{-3}, 2027)$\n- 用例 4：$(N, \\lambda_{\\mathrm{fix}}, s) = (10, 10^{-2}, 42)$\n\n最终答案中的 Python 程序精确地实现了这些步骤，计算了四个 $\\Delta$ 值，并根据指定的输出格式对它们进行了格式化。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem by simulating and comparing\n    blocked grid search and random search for hyperparameter optimization.\n    \"\"\"\n\n    def objective_function(x, y):\n        \"\"\"\n        Computes the synthetic validation loss f(x, y).\n        x = log10(eta), y = log10(lambda).\n        \"\"\"\n        term1 = (x + 2.0)**2\n        term2 = 2.5 * (y + 3.0)**2\n        term3 = 0.3 * (x + 2.0) * (y + 3.0)\n        term4 = 0.1 * np.sin(5.0 * x) * np.sin(5.0 * y)\n        return term1 + term2 + term3 + term4\n\n    def grid_search_blocked(N, lambda_fix):\n        \"\"\"\n        Performs blocked grid search.\n        Fixes lambda (and thus y) and searches over a grid of eta (x) values.\n        \"\"\"\n        y_fix = np.log10(lambda_fix)\n\n        if N == 1:\n            x_points = np.array([-4.0])\n        else:\n            x_points = np.linspace(-4.0, 0.0, N)\n        \n        # Evaluate the objective function for all x points at the fixed y.\n        # This can be vectorized since objective_function is written with numpy ops.\n        y_points = np.full_like(x_points, y_fix)\n        losses = objective_function(x_points, y_points)\n        \n        return np.min(losses)\n\n    def random_search(N, s):\n        \"\"\"\n        Performs random search.\n        Samples N points (x, y) from the uniform distribution over the domain.\n        \"\"\"\n        # Create a random number generator with the specified seed for reproducibility.\n        rng = np.random.default_rng(s)\n        \n        # Sample N points from the search space.\n        x_samples = rng.uniform(-4.0, 0.0, N)\n        y_samples = rng.uniform(-5.0, -1.0, N)\n        \n        # Evaluate the objective function at the sampled points.\n        losses = objective_function(x_samples, y_samples)\n        \n        return np.min(losses)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, lambda_fix, s)\n        (30, 1e-1, 2027),  # Case 1\n        (1, 1e-1, 123),   # Case 2\n        (30, 1e-3, 2027),  # Case 3\n        (10, 1e-2, 42),   # Case 4\n    ]\n\n    results = []\n    for N, lambda_fix, s in test_cases:\n        # Calculate the minimum loss found by each method\n        f_min_grid = grid_search_blocked(N, lambda_fix)\n        f_min_rand = random_search(N, s)\n        \n        # Calculate the advantage of random search\n        delta = f_min_grid - f_min_rand\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    # Each result is formatted to six decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在通过模拟建立了直观理解后，我们能否从数学上精确地分析网格搜索和随机搜索的性能差异？本练习将引导你进入理论分析的层面。你将推导随机搜索的预期最佳验证精度，并为网格搜索建立一个最坏情况下的性能下界，从而定量地揭示“维度灾难”如何对网格搜索产生更严重的影响，为你的实践经验提供坚实的理论支撑 。",
            "id": "3133146",
            "problem": "您正在通过随机搜索或网格搜索，在超参数域 $\\Lambda = [0,1]^d$ 上为深度神经网络调整超参数。令 $g:\\Lambda \\to [0,1]$ 表示验证准确率作为超参数的函数，并假设 $g$ 对于欧几里得范数是 $L$-利普希茨（Lipschitz）的，即对所有 $\\lambda,\\lambda' \\in \\Lambda$ 都有 $|g(\\lambda) - g(\\lambda')| \\le L \\|\\lambda - \\lambda'\\|_{2}$。令 $g^{\\star} = \\max_{\\lambda \\in \\Lambda} g(\\lambda)$ 表示该域中的最优验证准确率。\n\n随机搜索从 $\\Lambda$ 中独立且均匀地采样超参数 $\\lambda_{1},\\dots,\\lambda_{n}$。将得到的验证准确率表示为 $X_{1} = g(\\lambda_{1}),\\dots,X_{n} = g(\\lambda_{n})$。为了进行此分析，我们将 $X_{i}$ 的边际分布建模为在 $[0,1]$ 上受支持的独立同分布（i.i.d.）随机变量，其累积分布函数为 $F$。特别地，假设在 $[0,1]$ 上 $F(x) = x$（即 $X_{i} \\sim \\mathrm{Uniform}(0,1)$），当不假设任何结构时，这是对 $[0,1]$ 上准确率常用的无信息先验。\n\n网格搜索使用一个规则网格，每个坐标轴上有 $m$ 个点（包括端点），因此网格点总数为 $n = m^{d}$，其中整数 $m \\ge 2$。每个坐标轴上的网格间距为 $h = \\frac{1}{m-1}$。\n\n仅使用累积分布函数、独立性和顺序统计量的定义，推导：\n- 在随机搜索下，经过 $n$ 次试验后，预期最佳验证准确率 $E[\\max\\{X_{1},\\dots,X_{n}\\}]$ 的解析表达式，特别地，针对 $F(x) = x$ 在 $[0,1]$ 上的情况。\n- $n$ 点网格搜索所能达到的最佳验证准确率的最坏情况下的下界，用 $g^{\\star}$、$L$、$d$ 和 $n$ 表示。\n\n将您的最终结果表示为一个双元素行矩阵，其第一个元素是在均匀模型下 $E[\\max\\{X_{1},\\dots,X_{n}\\}]$ 的精确表达式，第二个元素是仅用 $g^{\\star}$、$L$、$d$ 和 $n$ 表示（通过消去 $m$）的网格搜索下界。不要提供任何数值近似；最终答案必须是无单位的精确解析表达式。",
            "solution": "用户提供了一个问题陈述，要求推导与超参数优化策略相关的两个量：随机搜索的预期最佳准确率和网格搜索的最坏情况下的下界。\n\n### 问题验证\n\n**步骤1：提取给定条件**\n- 超参数域：$\\Lambda = [0,1]^d$。\n- 验证准确率函数：$g:\\Lambda \\to [0,1]$。\n- $g$ 的利普希茨（Lipschitz）性质：对于所有 $\\lambda,\\lambda' \\in \\Lambda$，有 $|g(\\lambda) - g(\\lambda')| \\le L \\|\\lambda - \\lambda'\\|_{2}$。\n- 最优验证准确率：$g^{\\star} = \\max_{\\lambda \\in \\Lambda} g(\\lambda)$。\n- **随机搜索**：\n    - 样本 $\\lambda_{1},\\dots,\\lambda_{n}$ 是从 $\\Lambda$ 中抽取的独立同分布的均匀样本。\n    - 验证准确率 $X_{i} = g(\\lambda_{i})$ 被建模为独立同分布的随机变量。\n    - $X_i$ 的累积分布函数（CDF）为 $F(x) = x$，$x \\in [0,1]$。\n- **网格搜索**：\n    - 每个坐标轴上有 $m$ 个点的规则网格。\n    - 总网格点数：$n = m^{d}$，其中 $m \\ge 2$ 为整数。\n    - 网格间距：$h = \\frac{1}{m-1}$。\n\n**步骤2：使用提取的给定条件进行验证**\n该问题在机器学习和优化理论领域具有科学依据。所使用的概念（随机搜索、网格搜索、利普希茨连续性、顺序统计量）都是标准且定义明确的。问题陈述清晰，为推导所求量提供了充分的信息。将验证准确率 $X_i$ 的分布假设为 $\\mathrm{Uniform}(0,1)$ 被明确说明是为了分析而做出的建模选择，这是算法理论比较中的常见做法。该问题是客观的，使用了精确的数学语言，并且不包含矛盾或歧义。\n\n**步骤3：结论与行动**\n该问题被判定为**有效**。将提供完整解答。\n\n---\n\n### 解答推导\n\n该问题包含两个部分。第一部分是计算随机搜索的预期最佳准确率，第二部分是找到网格搜索所能达到的最佳准确率的最坏情况下的下界。\n\n#### 第1部分：随机搜索的预期最佳准确率\n\n令 $X_1, X_2, \\dots, X_n$ 为验证准确率的独立同分布样本。问题陈述这些被建模为具有累积分布函数（CDF）$F(x) = x$（对于 $x \\in [0,1]$）的随机变量，这对应于标准均匀分布，$X_i \\sim \\mathrm{Uniform}(0,1)$。我们被要求找到这些样本最大值的期望值，$E[\\max\\{X_1, \\dots, X_n\\}]$。\n\n令 $Y = \\max\\{X_1, \\dots, X_n\\}$。随机变量 $Y$ 是样本的第 $n$ 个顺序统计量。为了求其期望，我们首先确定它的累积分布函数 $F_Y(y)$。\n根据定义，$F_Y(y) = P(Y \\le y)$。\n一组随机变量的最大值小于或等于 $y$ 的充要条件是该组中的所有变量都小于或等于 $y$。\n$$F_Y(y) = P(\\max\\{X_1, \\dots, X_n\\} \\le y) = P(X_1 \\le y, X_2 \\le y, \\dots, X_n \\le y)$$\n由于随机变量 $X_i$ 是独立的，其联合概率是边际概率的乘积：\n$$F_Y(y) = \\prod_{i=1}^n P(X_i \\le y)$$\n由于这些变量也是同分布的，其累积分布函数为 $F(x)$，因此上式可简化为：\n$$F_Y(y) = [F(y)]^n$$\n给定在 $[0,1]$ 上 $F(x)=x$，则 $Y$ 在 $y \\in [0,1]$ 上的累积分布函数为：\n$$F_Y(y) = y^n$$\n对于一个在 $[0,1]$ 上受支持的非负随机变量 $Y$，其期望值可以用其累积分布函数计算为 $E[Y] = \\int_0^1 (1 - F_Y(y)) dy$。\n代入 $F_Y(y)$ 的表达式：\n$$E[Y] = \\int_0^1 (1 - y^n) dy$$\n计算该积分：\n$$E[Y] = \\left[ y - \\frac{y^{n+1}}{n+1} \\right]_0^1 = \\left(1 - \\frac{1^{n+1}}{n+1}\\right) - \\left(0 - \\frac{0^{n+1}}{n+1}\\right) = 1 - \\frac{1}{n+1}$$\n$$E[Y] = \\frac{n+1-1}{n+1} = \\frac{n}{n+1}$$\n因此，在给定模型下，随机搜索的预期最佳验证准确率为 $\\frac{n}{n+1}$。\n\n#### 第2部分：网格搜索的最坏情况下的下界\n\n对于网格搜索，我们需要找到所能达到的最佳准确率 $\\max_{\\lambda \\in \\Lambda_{grid}} g(\\lambda)$ 的一个下界，其中 $\\Lambda_{grid}$ 是 $n$ 个网格点的集合。这个下界必须对任何 $L$-利普希茨函数 $g$ 都成立。最坏情况对应于一个函数 $g$ 及其最大值 $g^\\star$ 的位置，使得网格搜索的表现尽可能差。\n\n令 $\\lambda^\\star \\in \\Lambda = [0,1]^d$ 是一个达到真正最优验证准确率的超参数向量，即 $g(\\lambda^\\star) = g^\\star$。网格搜索方法并不知道 $\\lambda^\\star$；它只在 $\\Lambda_{grid}$ 中的点上评估 $g$。它找到的值是 $\\max_{\\lambda_i \\in \\Lambda_{grid}} g(\\lambda_i)$。\n\n我们必须找出 $\\lambda^\\star$ 离最近的网格点可以有多远。网格在 $d$ 个坐标轴的每一个上都有 $m$ 个等间距的点，包括端点 $0$ 和 $1$。每个坐标轴上的网格间距为 $h = \\frac{1}{m-1}$。沿任一坐标轴的网格点坐标为 $\\{0, h, 2h, \\dots, (m-1)h=1\\}$。\n\n对于任意点 $\\lambda = (c_1, \\dots, c_d) \\in \\Lambda$，以及每个坐标 $c_j$，在该轴上存在一个网格坐标 $p_j$，使得 $|c_j - p_j| \\le \\frac{h}{2}$。令 $\\lambda_p = (p_1, \\dots, p_d)$ 是由这些最近坐标构成的网格点。$\\lambda$ 和 $\\lambda_p$ 之间的欧几里得距离的平方有界：\n$$\\|\\lambda - \\lambda_p\\|_2^2 = \\sum_{j=1}^d (c_j - p_j)^2 \\le \\sum_{j=1}^d \\left(\\frac{h}{2}\\right)^2 = d \\frac{h^2}{4}$$\n取平方根，我们发现对于任意点 $\\lambda \\in \\Lambda$，存在一个网格点 $\\lambda_p \\in \\Lambda_{grid}$，使得：\n$$\\|\\lambda - \\lambda_p\\|_2 \\le \\frac{h\\sqrt{d}}{2}$$\n这个界是紧的，代表了域中任意点到最近网格点的最大距离（在网格超单元的中心达到）。\n\n现在，我们将此应用于最优点 $\\lambda^\\star$。必定存在一个网格点，我们称之为 $\\lambda_{closest} \\in \\Lambda_{grid}$，使得：\n$$\\|\\lambda^\\star - \\lambda_{closest}\\|_2 \\le \\frac{h\\sqrt{d}}{2}$$\n函数 $g$ 是 $L$-利普希茨的，这意味着 $|g(\\lambda^\\star) - g(\\lambda_{closest})| \\le L \\|\\lambda^\\star - \\lambda_{closest}\\|_2$。由于 $g(\\lambda^\\star)=g^\\star$ 是最大值，我们可以写出：\n$$g^\\star - g(\\lambda_{closest}) \\le L \\|\\lambda^\\star - \\lambda_{closest}\\|_2$$\n使用我们找到的距离界限：\n$$g^\\star - g(\\lambda_{closest}) \\le L \\frac{h\\sqrt{d}}{2}$$\n重新整理这个不等式，可以得到网格点 $\\lambda_{closest}$ 处准确率的一个下界：\n$$g(\\lambda_{closest}) \\ge g^\\star - \\frac{Lh\\sqrt{d}}{2}$$\n网格搜索找到的最佳准确率是 $\\max_{\\lambda_i \\in \\Lambda_{grid}} g(\\lambda_i)$。由于 $\\lambda_{closest}$ 是网格点之一，找到的最大准确率必须至少与 $g(\\lambda_{closest})$ 一样大：\n$$\\max_{\\lambda_i \\in \\Lambda_{grid}} g(\\lambda_i) \\ge g(\\lambda_{closest})$$\n结合这些不等式，可以得到最坏情况下的下界：\n$$\\max_{\\lambda_i \\in \\Lambda_{grid}} g(\\lambda_i) \\ge g^\\star - \\frac{Lh\\sqrt{d}}{2}$$\n问题要求这个界用 $g^\\star, L, d$ 和 $n$ 来表示。我们使用给定的关系式 $h = \\frac{1}{m-1}$ 和 $n=m^d$。从 $n=m^d$ 中，我们解出 $m$ 得到 $m=n^{1/d}$。我们将此代入 $h$ 的表达式中：\n$$h = \\frac{1}{n^{1/d} - 1}$$\n将 $h$ 的这个表达式代入下界中：\n$$\\max_{\\lambda_i \\in \\Lambda_{grid}} g(\\lambda_i) \\ge g^\\star - \\frac{L\\sqrt{d}}{2(n^{1/d} - 1)}$$\n这就是所求的网格搜索的最坏情况下的下界。\n\n### 最终表达式\n\n所求的两个表达式是：\n1.  随机搜索的预期最佳准确率：$\\frac{n}{n+1}$。\n2.  网格搜索的最坏情况下界：$g^\\star - \\frac{L\\sqrt{d}}{2(n^{1/d} - 1)}$。\n\n根据要求，这些将被格式化为一个双元素行矩阵。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{n}{n+1}  g^{\\star} - \\frac{L\\sqrt{d}}{2(n^{1/d} - 1)}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}