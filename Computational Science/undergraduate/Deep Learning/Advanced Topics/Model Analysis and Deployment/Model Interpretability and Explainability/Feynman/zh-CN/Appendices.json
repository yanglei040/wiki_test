{
    "hands_on_practices": [
        {
            "introduction": "本实践旨在通过编程实现积分梯度（Integrated Gradients）方法，从而加深对其核心原理的理解。你将为一个特定的函数推导精确的积分梯度，并实现两种数值积分方案（梯形法则与高斯-勒让德求积）来近似计算它，通过比较不同方案的精度和收敛速度，你将体会到理论与实践的结合。这项练习不仅能巩固你对积分梯度路径积分本质的认识，还能提升你的数值计算能力。",
            "id": "3153194",
            "problem": "构建一个程序，该程序比较两种用于计算沿直线路径的积分梯度（IG）的数值求积方案，并评估它们在求积步数增加时的准确性和稳定性。在纯数学背景下进行，设置如下。\n\n考虑定义在维度为 $d$ 的向量空间上的一个可微标量值函数，由下式给出\n$$\nf(\\mathbf{x}) = \\log\\left(1 + e^{\\mathbf{w}^{\\top}\\mathbf{x}}\\right),\n$$\n其中 $\\mathbf{x} \\in \\mathbb{R}^d$ 且 $\\mathbf{w} \\in \\mathbb{R}^d$。对于一个基线 $\\mathbf{x}_0$ 和输入 $\\mathbf{x}$，令 $\\Delta = \\mathbf{x} - \\mathbf{x}_0$。沿直线路径 $\\mathbf{x}(\\alpha) = \\mathbf{x}_0 + \\alpha \\Delta$（对于 $\\alpha \\in [0,1]$）的积分梯度（IG）归因向量定义为\n$$\n\\mathrm{IG}(\\mathbf{x}; \\mathbf{x}_0) = \\int_{0}^{1} \\nabla f\\big(\\mathbf{x}_0 + \\alpha \\Delta\\big) \\odot \\Delta \\ d\\alpha,\n$$\n其中 $\\odot$ 表示逐元素乘法。在整个过程中，使用微积分中的梯度定义和标准微分法则。\n\n您必须：\n- 根据定义和基本微积分，为此 $f(\\mathbf{x})$ 推导出一个精确 IG 的封闭形式表达式，用 $\\mathbf{w}$、$\\mathbf{x}_0$ 和 $\\mathbf{x}$ 表示。将此精确结果用作误差评估的基准。\n- 实现上述路径积分的两种数值求积近似方法：\n  1. 在 $[0,1]$ 上使用 $m$ 个子区间的梯形法则：\n     - 使用节点 $\\alpha_k = k/m$（对于 $k = 0, 1, \\dots, m$），步长为 $h = 1/m$，权重为 $w_0 = w_m = h/2$，$w_k = h$（对于 $k = 1, \\dots, m-1$）。\n  2. 在 $[0,1]$ 上的 $m$ 点高斯-勒让德求积：\n     - 使用在 $[-1,1]$ 上的 $m$ 次高斯-勒让德节点 $\\{\\tilde{\\alpha}_k\\}_{k=1}^m$ 和权重 $\\{\\tilde{w}_k\\}_{k=1}^m$，通过 $\\alpha_k = (\\tilde{\\alpha}_k + 1)/2$ 仿射映射到 $[0,1]$，相应的权重为 $w_k = \\tilde{w}_k/2$。\n- 对于每种求积方法，通过在相应节点上评估向量值被积函数 $\\nabla f(\\mathbf{x}_0 + \\alpha \\Delta) \\odot \\Delta$ 并使用指定的权重求和来近似向量积分。\n\n使用以下测试套件，其中所有向量都明确给出：\n- 测试用例 1（常规路径，中度非线性）：\n  - $d = 3$\n  - $\\mathbf{w} = [0.5, -1.0, 2.0]$\n  - $\\mathbf{x}_0 = [0.0, 0.0, 0.0]$\n  - $\\mathbf{x} = [1.0, -2.0, 0.5]$\n- 测试用例 2（边界情况，logit 中的方向变化为零，即正交性导致被积函数为常数）：\n  - $d = 2$\n  - $\\mathbf{w} = [1.0, 2.0]$\n  - $\\mathbf{x}_0 = [1.0, -1.0]$\n  - $\\mathbf{x} = [3.0, -2.0]$\n- 测试用例 3（强饱和区域）：\n  - $d = 3$\n  - $\\mathbf{w} = [3.0, -0.5, 1.0]$\n  - $\\mathbf{x}_0 = [2.0, -4.0, 3.0]$\n  - $\\mathbf{x} = [3.0, -4.0, 4.0]$\n\n对于每个测试用例，评估步数集合的近似值\n$$\n\\mathcal{M} = [1, 2, 4, 8, 16, 32, 64].\n$$\n设精度容差为 $\\varepsilon = 10^{-8}$。\n\n对于每个测试用例和每种求积方法，计算：\n- 在 $m \\in \\mathcal{M}$ 上，近似 IG 与精确 IG 之间的最大 $\\ell_2$ 误差。\n- 使得 $\\ell_2$ 误差小于或等于 $\\varepsilon$ 的最小 $m \\in \\mathcal{M}$；如果不存在这样的 $m$，则报告 $-1$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个扁平的、逗号分隔的列表，用方括号括起来，顺序如下：\n  - 对于测试用例 1：[最大梯形误差，最大高斯-勒让德误差，梯形法的最小 $m$，高斯-勒让德法的最小 $m$]\n  - 对于测试用例 2：附加相同的四个量\n  - 对于测试用例 3：附加相同的四个量\n- 具体来说，输出必须是以下形式的单行\n$$\n[\\mathrm{E}^{(1)}_{\\mathrm{trap}}, \\mathrm{E}^{(1)}_{\\mathrm{gl}}, m^{(1)}_{\\mathrm{trap}}, m^{(1)}_{\\mathrm{gl}}, \\mathrm{E}^{(2)}_{\\mathrm{trap}}, \\mathrm{E}^{(2)}_{\\mathrm{gl}}, m^{(2)}_{\\mathrm{trap}}, m^{(2)}_{\\mathrm{gl}}, \\mathrm{E}^{(3)}_{\\mathrm{trap}}, \\mathrm{E}^{(3)}_{\\mathrm{gl}}, m^{(3)}_{\\mathrm{trap}}, m^{(3)}_{\\mathrm{gl}}].\n$$\n\n所有量都是无量纲的，并且必须报告为基本数值类型（误差为浮点数，$m$ 为整数），不得有任何附加文本。",
            "solution": "用户希望构建一个程序，为特定函数计算积分梯度（IG），并比较两种数值求积方案的准确性。\n\n解决过程包括三个主要阶段：\n1.  为积分梯度向量推导一个封闭形式的解析表达式，该表达式将作为基准。\n2.  实现两种数值求积方法（梯形法则和高斯-勒让德求积）来近似 IG 路径积分。\n3.  针对一组给定的测试用例和参数，评估这些数值方法相对于精确解的准确性和收敛性。\n\n### 1. 积分梯度的解析推导\n\n函数由 $f(\\mathbf{x}) = \\log\\left(1 + e^{\\mathbf{w}^{\\top}\\mathbf{x}}\\right)$ 给出。这是 softplus 函数应用于 $\\mathbf{x}$ 的线性投影。令 $u(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x}$。函数为 $f(\\mathbf{x}) = \\log(1+e^{u(\\mathbf{x})})$。\n\n首先，我们使用链式法则计算 $f(\\mathbf{x})$ 的梯度。外部函数 $\\log(1+e^u)$ 对其参数 $u$ 的导数是 $\\frac{e^u}{1+e^u} = \\frac{1}{1+e^{-u}}$，即逻辑 sigmoid 函数，记为 $\\sigma(u)$。内部函数 $u(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x} = \\sum_j w_j x_j$ 相对于 $\\mathbf{x}$ 的梯度就是向量 $\\mathbf{w}$。\n\n因此，梯度 $\\nabla f(\\mathbf{x})$ 是：\n$$\n\\nabla f(\\mathbf{x}) = \\frac{d f}{d u} \\nabla u(\\mathbf{x}) = \\sigma(\\mathbf{w}^{\\top}\\mathbf{x}) \\mathbf{w}\n$$\n\n积分梯度归因由沿直线 $\\mathbf{x}(\\alpha) = \\mathbf{x}_0 + \\alpha \\Delta$ 的路径积分定义，其中 $\\Delta = \\mathbf{x} - \\mathbf{x}_0$ 且 $\\alpha \\in [0,1]$。\n$$\n\\mathrm{IG}(\\mathbf{x}; \\mathbf{x}_0) = \\int_{0}^{1} \\nabla f\\big(\\mathbf{x}_0 + \\alpha \\Delta\\big) \\odot \\Delta \\ d\\alpha\n$$\n代入梯度的表达式：\n$$\n\\mathrm{IG}(\\mathbf{x}; \\mathbf{x}_0) = \\int_{0}^{1} \\left( \\sigma\\left(\\mathbf{w}^{\\top}(\\mathbf{x}_0 + \\alpha \\Delta)\\right) \\mathbf{w} \\right) \\odot \\Delta \\ d\\alpha\n$$\n项 $\\mathbf{w} \\odot \\Delta$ 是一个相对于积分变量 $\\alpha$ 的常数向量。我们可以将其从积分中提取出来（这适用于分量级别）：\n$$\n\\mathrm{IG}(\\mathbf{x}; \\mathbf{x}_0) = (\\mathbf{w} \\odot \\Delta) \\int_{0}^{1} \\sigma\\left(\\mathbf{w}^{\\top}\\mathbf{x}_0 + \\alpha (\\mathbf{w}^{\\top}\\Delta)\\right) d\\alpha\n$$\n让我们定义标量常数 $C_0 = \\mathbf{w}^{\\top}\\mathbf{x}_0$ 和 $C_1 = \\mathbf{w}^{\\top}\\Delta$。积分变成一个标量积分 $I$：\n$$\nI = \\int_{0}^{1} \\sigma(C_0 + \\alpha C_1) \\ d\\alpha\n$$\n为了解这个积分，我们回想一下 $\\sigma(z)$ 的原函数是 $\\log(1+e^z)$，这是我们原始函数 $f$ 的一个原函数。令 $v(\\alpha) = C_0 + \\alpha C_1$，则 $dv = C_1 d\\alpha$。\n\n如果 $C_1 \\neq 0$：我们进行变量替换。\n$$\nI = \\frac{1}{C_1} \\int_{v(0)}^{v(1)} \\sigma(v) \\ dv = \\frac{1}{C_1} \\left[ \\log(1+e^v) \\right]_{C_0}^{C_0+C_1} = \\frac{1}{C_1} \\left( \\log(1+e^{C_0+C_1}) - \\log(1+e^{C_0}) \\right)\n$$\n代回 $C_0 = \\mathbf{w}^{\\top}\\mathbf{x}_0$ 和 $C_0+C_1 = \\mathbf{w}^{\\top}(\\mathbf{x}_0+\\Delta) = \\mathbf{w}^{\\top}\\mathbf{x}$，我们得到：\n$$\nI = \\frac{\\log(1+e^{\\mathbf{w}^{\\top}\\mathbf{x}}) - \\log(1+e^{\\mathbf{w}^{\\top}\\mathbf{x}_0})}{\\mathbf{w}^{\\top}(\\mathbf{x} - \\mathbf{x}_0)} = \\frac{f(\\mathbf{x}) - f(\\mathbf{x}_0)}{\\mathbf{w}^{\\top}\\Delta}\n$$\n因此，精确的 IG 向量是：\n$$\n\\mathrm{IG}_{\\text{exact}} = (\\mathbf{w} \\odot \\Delta) \\frac{f(\\mathbf{x}) - f(\\mathbf{x}_0)}{\\mathbf{w}^{\\top}\\Delta}\n$$\n\n如果 $C_1 = 0$：分母为零。在这种情况下，被积函数 $\\sigma(C_0 + \\alpha C_1)$ 简化为一个常数 $\\sigma(C_0)$。积分变为：\n$$\nI = \\int_{0}^{1} \\sigma(C_0) \\ d\\alpha = \\sigma(C_0) [\\alpha]_0^1 = \\sigma(C_0)\n$$\n对于这个特殊情况，精确的 IG 向量是：\n$$\n\\mathrm{IG}_{\\text{exact}} = (\\mathbf{w} \\odot \\Delta) \\sigma(\\mathbf{w}^{\\top}\\mathbf{x}_0)\n$$\n这种情况对应于积分路径与权重向量 $\\mathbf{w}$ 正交，导致沿路径的“预激活”值恒定。\n\n### 2. 数值求积方案\n\n需要近似的积分形式为 $\\int_0^1 \\mathbf{g}(\\alpha) d\\alpha \\approx \\sum_k w_k \\mathbf{g}(\\alpha_k)$，其中 $\\mathbf{g}(\\alpha) = (\\mathbf{w} \\odot \\Delta) \\sigma(C_0 + \\alpha C_1)$。近似值可以计算为 $(\\mathbf{w} \\odot \\Delta) \\left( \\sum_k w_k \\sigma(C_0 + \\alpha_k C_1) \\right)$。\n\n1.  **梯形法则：** 对于 $m$ 个子区间，步长为 $h=1/m$。节点为 $\\alpha_k = k/m$，其中 $k = 0, 1, \\dots, m$。内部点（$k=1, \\dots, m-1$）的权重为 $w_k = h$，端点（$k=0, m$）的权重为 $w_k = h/2$。近似值为：\n    $$\n    \\mathrm{IG}_{\\text{trap}}(m) = (\\mathbf{w} \\odot \\Delta) \\sum_{k=0}^{m} w_k \\sigma(C_0 + \\alpha_k C_1)\n    $$\n\n2.  **高斯-勒让德求积：** 对于一个 $m$ 点法则，我们使用标准区间 $[-1,1]$ 上的 $m$ 次高斯-勒让德多项式的节点 $\\tilde{\\alpha}_k$ 和权重 $\\tilde{w}_k$。这些必须通过仿射变换映射到区间 $[0,1]$。\n    - 在 $[0,1]$ 上的节点：$\\alpha_k = (\\tilde{\\alpha}_k + 1)/2$\n    - 对于 $[0,1]$ 的权重：$w_k = \\tilde{w}_k/2$\n    近似值是这 $m$ 个节点的加权和：\n    $$\n    \\mathrm{IG}_{\\text{gl}}(m) = (\\mathbf{w} \\odot \\Delta) \\sum_{k=1}^{m} w_k \\sigma(C_0 + \\alpha_k C_1)\n    $$\n\n### 3. 评估过程\n\n对于每个测试用例和 $\\mathcal{M} = [1, 2, 4, 8, 16, 32, 64]$ 中的每个 $m$ 值，我们使用两种方法计算近似的 IG 向量，即 $\\mathrm{IG}_{\\text{trap}}(m)$ 和 $\\mathrm{IG}_{\\text{gl}}(m)$。然后我们计算误差向量的 $\\ell_2$ 范数，即近似值与精确解 $\\mathrm{IG}_{\\text{exact}}$ 之间的差。\n$$\ne(m) = \\| \\mathrm{IG}_{\\text{approx}}(m) - \\mathrm{IG}_{\\text{exact}} \\|_2\n$$\n对于每个测试用例和求积方法，我们确定两个量：\n1.  在所有 $m \\in \\mathcal{M}$ 值中观察到的最大误差：$E = \\max_{m \\in \\mathcal{M}} e(m)$。\n2.  使误差 $e(m)$ 小于或等于容差 $\\varepsilon = 10^{-8}$ 的最小 $m \\in \\mathcal{M}$。如果找不到这样的 $m$，我们报告 $-1$。\n\n该过程系统地应用于所有三个测试用例，以生成最终的 12 元素输出向量。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import roots_legendre\n\ndef solve():\n    \"\"\"\n    Computes and compares Integrated Gradients using analytical, \n    Trapezoidal, and Gauss-Legendre methods.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"id\": 1,\n            \"d\": 3,\n            \"w\": np.array([0.5, -1.0, 2.0]),\n            \"x0\": np.array([0.0, 0.0, 0.0]),\n            \"x\": np.array([1.0, -2.0, 0.5]),\n        },\n        {\n            \"id\": 2,\n            \"d\": 2,\n            \"w\": np.array([1.0, 2.0]),\n            \"x0\": np.array([1.0, -1.0]),\n            \"x\": np.array([3.0, -2.0]),\n        },\n        {\n            \"id\": 3,\n            \"d\": 3,\n            \"w\": np.array([3.0, -0.5, 1.0]),\n            \"x0\": np.array([2.0, -4.0, 3.0]),\n            \"x\": np.array([3.0, -4.0, 4.0]),\n        },\n    ]\n\n    M_values = [1, 2, 4, 8, 16, 32, 64]\n    epsilon = 1e-8\n    \n    all_results = []\n    \n    def log_1_plus_exp(z):\n        \"\"\"Numerically stable implementation of log(1 + exp(z)).\"\"\"\n        return np.logaddexp(0, z)\n\n    def sigmoid(z):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-z))\n\n    for case in test_cases:\n        w, x0, x = case[\"w\"], case[\"x0\"], case[\"x\"]\n        delta = x - x0\n        \n        # --- 1. Compute Exact Integrated Gradients ---\n        c0 = np.dot(w, x0)\n        c1 = np.dot(w, delta)\n        v_const = w * delta\n        \n        # If c1 is very close to zero, the integrand is constant.\n        if np.abs(c1)  1e-12:\n            scalar_integral_exact = sigmoid(c0)\n        else:\n            fx = log_1_plus_exp(np.dot(w, x))\n            fx0 = log_1_plus_exp(c0)\n            scalar_integral_exact = (fx - fx0) / c1\n            \n        ig_exact = v_const * scalar_integral_exact\n\n        # --- 2. Compute Approximations and Errors ---\n        trap_errors = []\n        gl_errors = []\n        \n        for m in M_values:\n            # --- Trapezoidal Rule ---\n            h = 1.0 / m\n            nodes_trap = np.linspace(0.0, 1.0, m + 1)\n            weights_trap = np.full(m + 1, h)\n            weights_trap[0] = h / 2.0\n            weights_trap[-1] = h / 2.0\n            \n            integrand_vals_trap = sigmoid(c0 + nodes_trap * c1)\n            scalar_integral_trap = np.dot(weights_trap, integrand_vals_trap)\n            ig_trap = v_const * scalar_integral_trap\n            trap_errors.append(np.linalg.norm(ig_trap - ig_exact))\n\n            # --- Gauss-Legendre Quadrature ---\n            nodes_gl_std, weights_gl_std = roots_legendre(m)\n            nodes_gl = 0.5 * (nodes_gl_std + 1)\n            weights_gl = 0.5 * weights_gl_std\n            \n            integrand_vals_gl = sigmoid(c0 + nodes_gl * c1)\n            scalar_integral_gl = np.dot(weights_gl, integrand_vals_gl)\n            ig_gl = v_const * scalar_integral_gl\n            gl_errors.append(np.linalg.norm(ig_gl - ig_exact))\n            \n        # --- 3. Aggregate Results for the Current Case ---\n        max_trap_error = np.max(trap_errors)\n        max_gl_error = np.max(gl_errors)\n        \n        min_m_trap = -1\n        for i, err in enumerate(trap_errors):\n            if err = epsilon:\n                min_m_trap = M_values[i]\n                break\n        \n        min_m_gl = -1\n        for i, err in enumerate(gl_errors):\n            if err = epsilon:\n                min_m_gl = M_values[i]\n                break\n        \n        all_results.extend([max_trap_error, max_gl_error, min_m_trap, min_m_gl])\n\n    # --- 4. Final Output ---\n    # Using .15g to maintain precision for small floats in scientific notation\n    # and provide a clean representation for integers.\n    result_str = \",\".join(f'{r:.15g}' if isinstance(r, (float, np.floating)) else str(r) for r in all_results)\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "生成了解释之后，我们如何确定它是否“忠实”地反映了模型的决策过程？本练习将通过一个巧妙设计的思想实验来探讨这一问题，其中两个模型在特定数据集上表现出相同的准确率，但依赖于不同的“捷径”特征。你将通过实现删除和插入曲线（deletion and insertion curves）这一重要评估技术，来诊断哪一个模型的梯度显著性图谱更为可靠，从而学会批判性地评估解释方法的有效性。",
            "id": "3153222",
            "problem": "您必须编写一个完整的、可运行的程序，该程序构建两个作用于图像的可微二元分类器，并使用删除曲线和插入曲线评估其基于梯度的显著性的忠实度。程序必须仅依赖于明确定义的数学运算和定义。不需要用户输入，且所有计算都必须是确定性的。\n\n推导的基本依据必须使用：基于梯度的显著性定义为输入梯度 $\\nabla_{x} f(x)$，导数的链式法则，以及 logistic sigmoid 函数和双曲正切函数的标准导数。logistic sigmoid 函数为 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$，其导数为 $\\sigma'(z) = \\sigma(z)\\,(1-\\sigma(z))$。双曲正切函数为 $\\tanh(u) = \\frac{e^{u} - e^{-u}}{e^{u} + e^{-u}}$，其导数为 $\\frac{d}{du}\\tanh(u) = \\operatorname{sech}^{2}(u)$，其中 $\\operatorname{sech}(u) = \\frac{1}{\\cosh(u)}$。\n\n任务说明：\n\n- 输入表示：\n  - 考虑灰度图像 $x \\in \\mathbb{R}^{H \\times W}$，其中 $H = W = 16$。\n  - 定义一个信号区域 $S = \\{(i,j) : 0 \\le i  4,\\ 0 \\le j  4\\}$ 和一个捷径区域 $C = \\{(i,j) : 0 \\le i  4,\\ 12 \\le j  16\\}$。令 $|S| = |C| = 16$。\n  - 对于图像 $x$，定义 $\\mathrm{mean}_{S}(x)$ 为 $S$ 中像素的算术平均值，$\\mathrm{mean}_{C}(x)$ 为 $C$ 中像素的算术平均值。\n\n- 模型：\n  - 模型 $\\mathrm{A}$ (线性-logistic)：\n    - Logit：$z_{\\mathrm{A}}(x) = a_{S}\\,\\mathrm{mean}_{S}(x) + a_{C}\\,\\mathrm{mean}_{C}(x)$，其中 $a_{S} = 4.0$，$a_{C} = 0.2$。\n    - 概率：$f_{\\mathrm{A}}(x) = \\sigma(z_{\\mathrm{A}}(x))$。\n  - 模型 $\\mathrm{B}$ (饱和-捷径)：\n    - Logit：$z_{\\mathrm{B}}(x) = b_{S}\\,\\mathrm{mean}_{S}(x) + b_{C}\\,\\tanh(\\alpha\\,\\mathrm{mean}_{C}(x))$，其中 $b_{S} = 1.5$，$b_{C} = 3.0$，$α = 5.0$。\n    - 概率：$f_{\\mathrm{B}}(x) = \\sigma(z_{\\mathrm{B}}(x))$。\n\n- 显著性定义：\n  - 对于每个模型，显著性图是根据上述定义通过链式法则计算的绝对梯度大小 $s(x) = |\\nabla_{x} f(x)|$。\n\n- 删除曲线和插入曲线：\n  - 设 $N = H \\cdot W$ 为像素总数。\n  - 对于固定的归因 $s(x)$，通过按显著性分数降序对像素索引进行排序来定义一个排序 $\\pi$。\n  - 删除曲线：从原始图像 $x^{(0)} = x$ 开始，对于步长 $t \\in \\{0,1,\\dots,T\\}$（其中 $T = 20$），根据排序 $\\pi$ 将前 $k_{t} = \\left\\lfloor \\frac{t}{T} \\cdot N \\right\\rfloor$ 个像素值设为零，同时保持其他像素与 $x$ 中相同，从而形成 $x^{(t)}$。记录 $y^{(t)} = f(x^{(t)})$，其中 $f$ 是模型的概率。删除曲线下面积 (AUC) 是 $y^{(t)}$ 相对于分数 $\\frac{t}{T}$ 的梯形积分，通过水平轴上的单位区间和垂直轴上的概率范围 $[0,1]$ 进行归一化。\n  - 插入曲线：从零图像 $\\tilde{x}^{(0)} = 0$ 开始，对于步长 $t \\in \\{0,1,\\dots,T\\}$，根据排序 $\\pi$ 将前 $k_{t}$ 个像素设置为 $x$ 中的原始值，其余像素保持为零，从而形成 $\\tilde{x}^{(t)}$。记录 $\\tilde{y}^{(t)} = f(\\tilde{x}^{(t)})$。插入曲线 AUC 通过梯形法则类似地定义。\n\n- 忠实度分数和决策规则：\n  - 对于每个测试图像上的每个模型，定义一个忠实度分数 $F = \\mathrm{AUC}_{\\mathrm{ins}} - \\mathrm{AUC}_{\\mathrm{del}}$。\n  - 对于每个测试用例，通过比较分数来决定哪个模型的显著性更忠实：较大的 $F$ 值表示更高的忠实度。\n\n数据集构建：\n\n- 使用由 $\\mu_{S} \\in \\{2.0, 0.8, 0.2\\}$ 和固定相关因子 $\\rho = 0.8$ 参数化的三个确定性测试图像。\n- 对于每个测试图像：\n  - 将 $S$ 中的所有像素设置为 $\\mu_{S}$。\n  - 将 $C$ 中的所有像素设置为 $\\mu_{C} = \\rho \\cdot \\mu_{S}$。\n  - 将所有其他像素设置为 $0$。\n  - 预期的真实标签是 $y=1$，但程序必须仅评估模型的概率和派生的显著性指标。\n- 注意：使用上述参数，两个模型在这些案例上都达到相同的准确率（每个都预测类别为 $1$），但它们依赖于不同的机制：模型 $\\mathrm{A}$ 强调信号区域 $S$，而模型 $\\mathrm{B}$ 依赖于通过 $C$ 的饱和捷径。由于饱和抑制了模型 $\\mathrm{B}$ 在 $C$ 中的梯度，梯度 $\\nabla_{x} f_{\\mathrm{A}}(x)$ 和 $\\nabla_{x} f_{\\mathrm{B}}(x)$ 可能看起来相似。\n\n程序要求：\n\n- 完全按照规定实现两个模型及其梯度。\n- 对于每个测试图像，使用各自的显著性排名计算模型 $\\mathrm{A}$ 和模型 $\\mathrm{B}$ 的忠实度分数，然后输出哪个模型更忠实。\n- 最终输出格式：您的程序应生成单行输出，其中包含三个测试用例的决策，形式为方括号内由逗号分隔的整数列表，其中 $0$ 表示模型 $\\mathrm{A}$ 更忠实， $1$ 表示模型 $\\mathrm{B}$ 更忠实。例如，像 $[0,1,0]$ 这样的输出是可以接受的。\n\n测试套件：\n\n- 三个测试用例是集合 $\\{2.0, 0.8, 0.2\\}$ 中的三个 $\\mu_{S}$ 值，固定 $\\rho = 0.8$。\n- 预期输出为整数，每个测试用例一个，由上述算法确定。\n- 覆盖范围：\n  - 强信号案例 $\\mu_{S} = 2.0$。\n  - 中等信号案例 $\\mu_{S} = 0.8$。\n  - 接近边界案例 $\\mu_{S} = 0.2$。\n\n您的程序必须遵循确切的输出格式：单行输出一个包含三个整数的列表，如 $[r_{1},r_{2},r_{3}]$，不带任何额外文本。本问题不使用角度，也不适用任何物理单位。上述所有数值常量必须完全按规定使用。",
            "solution": "我们从可微模型、梯度和归因忠实度的核心定义开始。\n\n定义和导数。logistic sigmoid 函数为 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$，其导数为 $\\sigma'(z) = \\sigma(z)\\,(1-\\sigma(z))$。双曲正切函数为 $\\tanh(u) = \\frac{e^{u}-e^{-u}}{e^{u}+e^{-u}}$，其导数为 $\\frac{d}{du}\\tanh(u) = \\operatorname{sech}^{2}(u)$，其中 $\\operatorname{sech}(u) = \\frac{1}{\\cosh(u)}$。链式法则将用于组合这些导数以处理复合函数。\n\n图像几何与均值。设 $H=W=16$，并设 $S$ 和 $C$ 为不相交的 $4\\times 4$ 区域，每个区域的基数 $|S|=|C|=16$。对于图像 $x \\in \\mathbb{R}^{H \\times W}$，定义均值 $\\mathrm{mean}_{S}(x) = \\frac{1}{|S|} \\sum_{(i,j)\\in S} x_{ij}$ 和 $\\mathrm{mean}_{C}(x) = \\frac{1}{|C|} \\sum_{(i,j)\\in C} x_{ij}$。\n\n模型。我们定义两个模型，其概率分别为 $f_{\\mathrm{A}}(x) = \\sigma(z_{\\mathrm{A}}(x))$ 和 $f_{\\mathrm{B}}(x) = \\sigma(z_{\\mathrm{B}}(x))$，其中\n- $z_{\\mathrm{A}}(x) = a_{S}\\,\\mathrm{mean}_{S}(x) + a_{C}\\,\\mathrm{mean}_{C}(x)$，其中 $a_{S}=4.0$，$a_{C}=0.2$，\n- $z_{\\mathrm{B}}(x) = b_{S}\\,\\mathrm{mean}_{S}(x) + b_{C}\\,\\tanh(\\alpha\\,\\mathrm{mean}_{C}(x))$，其中 $b_{S}=1.5$，$b_{C}=3.0$，$α=5.0$。\n\n梯度作为显著性。显著性图为 $s(x) = |\\nabla_{x} f(x)|$。使用链式法则，$\\nabla_{x} f(x) = \\sigma'(z(x)) \\cdot \\nabla_{x} z(x)$。\n\n对于模型 $\\mathrm{A}$，$z_{\\mathrm{A}}(x)$ 在均值上是线性的。$\\mathrm{mean}_{S}(x)$ 相对于 $S$ 中任何像素的梯度是 $\\frac{1}{|S|}$，在 $S$ 之外为零。对 $C$ 也类似。因此，\n- 对于 $(i,j)\\in S$：$\\frac{\\partial z_{\\mathrm{A}}}{\\partial x_{ij}} = \\frac{a_{S}}{|S|}$，\n- 对于 $(i,j)\\in C$：$\\frac{\\partial z_{\\mathrm{A}}}{\\partial x_{ij}} = \\frac{a_{C}}{|C|}$，\n- 其他位置：$\\frac{\\partial z_{\\mathrm{A}}}{\\partial x_{ij}} = 0$。\n因此梯度为 $\\frac{\\partial f_{\\mathrm{A}}}{\\partial x_{ij}} = \\sigma'(z_{\\mathrm{A}}(x)) \\cdot \\frac{\\partial z_{\\mathrm{A}}}{\\partial x_{ij}}$，其中每个区域的导数是上述分段常数。\n\n对于模型 $\\mathrm{B}$，我们再次应用链式法则，但捷径路径使用了 $\\tanh(\\cdot)$。设 $m_{C}(x) = \\mathrm{mean}_{C}(x)$。那么\n$\\frac{\\partial z_{\\mathrm{B}}}{\\partial x_{ij}} =\n\\begin{cases}\n\\frac{b_{S}}{|S|},  (i,j)\\in S,\\\\\nb_{C}\\cdot \\frac{d}{du}\\tanh(\\alpha u)\\big|_{u=m_{C}(x)} \\cdot \\frac{\\partial m_{C}}{\\partial x_{ij}},  (i,j)\\in C,\\\\\n0,  \\text{otherwise}.\n\\end{cases}$\n我们有 $\\frac{d}{du}\\tanh(\\alpha u) = \\alpha\\,\\operatorname{sech}^{2}(\\alpha u)$ 以及对于 $(i,j)\\in C$，有 $\\frac{\\partial m_{C}}{\\partial x_{ij}} = \\frac{1}{|C|}$。因此，\n- 对于 $(i,j)\\in S$：$\\frac{\\partial z_{\\mathrm{B}}}{\\partial x_{ij}} = \\frac{b_{S}}{|S|}$，\n- 对于 $(i,j)\\in C$：$\\frac{\\partial z_{\\mathrm{B}}}{\\partial x_{ij}} = \\frac{b_{C}\\,\\alpha}{|C|}\\,\\operatorname{sech}^{2}(\\alpha\\,m_{C}(x))$，\n- 其他位置：$0$。\n因此 $\\frac{\\partial f_{\\mathrm{B}}}{\\partial x_{ij}} = \\sigma'(z_{\\mathrm{B}}(x)) \\cdot \\frac{\\partial z_{\\mathrm{B}}}{\\partial x_{ij}}$。\n\n解释。对于 $m_{C}(x) > 0$ 的正输入且 $\\alpha$ 较大时，$\\operatorname{sech}^{2}(\\alpha\\,m_{C}(x))$ 会变得非常小，从而使模型 $\\mathrm{B}$ 中的捷径路径饱和。尽管 $C$ 对 logit 有很大的贡献，但这导致了 $C$ 区域的梯度很小。因此，$\\nabla_{x} f_{\\mathrm{B}}(x)$ 通过强调区域 $S$ 而显得与 $\\nabla_{x} f_{\\mathrm{A}}(x)$ 相似；然而，其潜在的因果依赖性是不同的：模型 $\\mathrm{B}$ 通过饱和的非线性关系依赖于 $C$，而模型 $\\mathrm{A}$ 主要由于 $a_{S} \\gg a_{C}$ 而依赖于 $S$。\n\n删除曲线和插入曲线。为了量化忠实度，我们评估当按照显著性图 $s(x)$ 建议的顺序扰动特征时，模型输出 $f(x)$ 变化的速度。对于删除，我们逐步将最显著的像素设置为零，并通过梯形法则对 $t=0,\\dots,T$（其中 $T=20$）在 $\\frac{t}{T}$ 上对概率曲线进行积分。较小的删除曲线下面积 (AUC) 表明，移除排名靠前的特征会迅速降低模型的置信度，这与忠实的归因是一致的。对于插入，我们从零图像开始，逐步插入最显著的像素；较大的插入曲线 AUC 表明，在添加重要特征时，模型置信度的恢复速度更快。综合忠实度分数 $F = \\mathrm{AUC}_{\\mathrm{ins}} - \\mathrm{AUC}_{\\mathrm{del}}$ 总结了这两种效应；较大的 $F$ 意味着更忠实的显著性。\n\n数据集构建和相同的准确率。我们构建了三个确定性的测试图像，其中 $\\mu_{S} \\in \\{2.0, 0.8, 0.2\\}$ 且 $\\mu_{C} = \\rho\\,\\mu_{S}$，$\\rho=0.8$。$S$ 中的所有像素都设置为 $\\mu_{S}$，$C$ 中的所有像素都设置为 $\\mu_{C}$，其余像素为 $0$。对于这些图像，两个模型预测的概率都高于 $0.5$（类别 $1$），因此在这个测试套件上具有相同的准确率，但它们的依赖性不同：模型 $\\mathrm{A}$ 由 $S$ 驱动，而模型 $\\mathrm{B}$ 由 $C$ 的饱和函数驱动。\n\n预期的诊断结果。因为模型 $\\mathrm{B}$ 的梯度低估了 $C$ 中的捷径贡献（由于饱和），其基于梯度的显著性将相对于其对模型输出的真实因果影响，给予 $S$ 中的像素过高的排名。因此，沿梯度排名进行删除不会那么快地降低 $f_{\\mathrm{B}}(x)$，而插入也不会那么有效地恢复它，从而产生一个较小的 $F$ 值。模型 $\\mathrm{A}$ 的梯度与其对 $S$ 的依赖性一致，将表现出较低的删除 AUC 和较高的插入 AUC，从而得到一个较大的 $F$ 值。因此，对于每个测试用例，我们预计决策将倾向于模型 $\\mathrm{A}$ 更忠实，编码为整数 $0$。\n\n程序实现的算法步骤：\n- 根据 $\\mu_{S} \\in \\{2.0, 0.8, 0.2\\}$ 和 $\\rho=0.8$ 构建三张图像。\n- 通过链式法则实现 $f_{\\mathrm{A}}$、$f_{\\mathrm{B}}$ 及其梯度。\n- 对于每个图像和模型：\n  - 计算显著性 $s(x) = |\\nabla_{x} f(x)|$。\n  - 使用显著性排名和基线零，在 $T=20$ 个步骤上构建删除和插入曲线。\n  - 使用梯形法则在 $\\frac{t}{T}$ 上计算 $\\mathrm{AUC}_{\\mathrm{del}}$ 和 $\\mathrm{AUC}_{\\mathrm{ins}}$。\n  - 计算 $F = \\mathrm{AUC}_{\\mathrm{ins}} - \\mathrm{AUC}_{\\mathrm{del}}$。\n- 每个测试用例，如果 $F_{\\mathrm{A}} \\ge F_{\\mathrm{B}}$ 则输出 $0$，否则输出 $1$。\n- 最后一行必须是单个列表 $[r_{1},r_{2},r_{3}]$。\n\n这个过程将模型的可解释性和可说明性操作化：单纯的梯度相似性可能因饱和而产生误导，而删除/插入曲线提供了一种基于效应的、有原则的显著性忠实度诊断方法。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Sigmoid and its derivative\ndef sigmoid(z: np.ndarray) - np.ndarray:\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef dsigmoid(z: np.ndarray) - np.ndarray:\n    s = sigmoid(z)\n    return s * (1.0 - s)\n\n# Tanh and sech^2 utilities\ndef tanh(u: np.ndarray) - np.ndarray:\n    return np.tanh(u)\n\ndef sech2(u: np.ndarray) - np.ndarray:\n    # sech(u) = 1/cosh(u); sech^2(u) = 1 / cosh(u)^2\n    return 1.0 / (np.cosh(u) ** 2)\n\n# Image geometry and regions\nH, W = 16, 16\n# Define S: rows 0..3, cols 0..3\nS_rows = slice(0, 4)\nS_cols = slice(0, 4)\n# Define C: rows 0..3, cols 12..15\nC_rows = slice(0, 4)\nC_cols = slice(12, 16)\nS_size = (S_rows.stop - S_rows.start) * (S_cols.stop - S_cols.start)\nC_size = (C_rows.stop - C_rows.start) * (C_cols.stop - C_cols.start)\n\n# Model parameters\na_S, a_C = 4.0, 0.2\nb_S, b_C, alpha = 1.5, 3.0, 5.0\n\ndef mean_region(img: np.ndarray, rows: slice, cols: slice) - float:\n    region = img[rows, cols]\n    return float(np.mean(region))\n\ndef modelA_logit(img: np.ndarray) - float:\n    mS = mean_region(img, S_rows, S_cols)\n    mC = mean_region(img, C_rows, C_cols)\n    return a_S * mS + a_C * mC\n\ndef modelA_prob(img: np.ndarray) - float:\n    return float(sigmoid(modelA_logit(img)))\n\ndef modelA_grad(img: np.ndarray) - np.ndarray:\n    # Gradient of f_A wrt pixels: dsigmoid(z_A) * dz_A/dx\n    zA = modelA_logit(img)\n    g = dsigmoid(zA)\n    grad = np.zeros_like(img, dtype=float)\n    grad[S_rows, S_cols] = g * (a_S / S_size)\n    grad[C_rows, C_cols] = g * (a_C / C_size)\n    return grad\n\ndef modelB_logit(img: np.ndarray) - float:\n    mS = mean_region(img, S_rows, S_cols)\n    mC = mean_region(img, C_rows, C_cols)\n    return b_S * mS + b_C * tanh(alpha * mC)\n\ndef modelB_prob(img: np.ndarray) - float:\n    return float(sigmoid(modelB_logit(img)))\n\ndef modelB_grad(img: np.ndarray) - np.ndarray:\n    # Gradient of f_B wrt pixels: dsigmoid(z_B) * dz_B/dx\n    mC = mean_region(img, C_rows, C_cols)\n    zB = modelB_logit(img)\n    g = dsigmoid(zB)\n    grad = np.zeros_like(img, dtype=float)\n    # S contribution\n    grad[S_rows, S_cols] = g * (b_S / S_size)\n    # C contribution with tanh chain rule: b_C * alpha * sech^2(alpha * mC) / |C|\n    grad[C_rows, C_cols] = g * (b_C * alpha * float(sech2(alpha * mC)) / C_size)\n    return grad\n\ndef auc_trapezoid(y: np.ndarray) - float:\n    # y is an array of f-values at uniformly spaced x in [0,1]\n    T = len(y) - 1\n    x = np.linspace(0.0, 1.0, T + 1)\n    return float(np.trapz(y, x))\n\ndef deletion_insertion_auc(img: np.ndarray, prob_fn, grad_fn, T: int = 20) - tuple[float, float]:\n    # Compute saliency from gradient magnitude\n    grad = grad_fn(img)\n    sal = np.abs(grad).reshape(-1)  # flatten\n    order = np.argsort(-sal)  # descending saliency\n\n    N = img.size\n    # Deletion: start from original, progressively zero-out top-k pixels\n    del_probs = []\n    del_img = img.copy().reshape(-1)\n    for t in range(T + 1):\n        # Record\n        del_probs.append(prob_fn(del_img.reshape(img.shape)))\n        # Next step: zero next batch\n        if t  T:\n            k_next = int(np.floor((t + 1) * N / T))\n            k_curr = int(np.floor(t * N / T))\n            idx = order[k_curr:k_next]\n            del_img[idx] = 0.0\n    del_probs = np.array(del_probs, dtype=float)\n    del_auc = auc_trapezoid(del_probs)\n\n    # Insertion: start from zeros, progressively insert top-k original pixels\n    ins_probs = []\n    ins_img = np.zeros_like(img).reshape(-1)\n    x_flat = img.reshape(-1)\n    for t in range(T + 1):\n        ins_probs.append(prob_fn(ins_img.reshape(img.shape)))\n        if t  T:\n            k_next = int(np.floor((t + 1) * N / T))\n            k_curr = int(np.floor(t * N / T))\n            idx = order[k_curr:k_next]\n            ins_img[idx] = x_flat[idx]\n    ins_probs = np.array(ins_probs, dtype=float)\n    ins_auc = auc_trapezoid(ins_probs)\n\n    return del_auc, ins_auc\n\ndef build_image(mu_S: float, rho: float = 0.8) - np.ndarray:\n    mu_C = rho * mu_S\n    img = np.zeros((H, W), dtype=float)\n    img[S_rows, S_cols] = mu_S\n    img[C_rows, C_cols] = mu_C\n    return img\n\ndef solve():\n    # Define the test cases from the problem statement: mu_S values\n    mu_S_values = [2.0, 0.8, 0.2]\n    rho = 0.8\n\n    results = []\n    for mu_S in mu_S_values:\n        img = build_image(mu_S, rho=rho)\n\n        # Compute AUCs for model A\n        delA, insA = deletion_insertion_auc(img, modelA_prob, modelA_grad, T=20)\n        scoreA = insA - delA\n\n        # Compute AUCs for model B\n        delB, insB = deletion_insertion_auc(img, modelB_prob, modelB_grad, T=20)\n        scoreB = insB - delB\n\n        # Decide which model is more faithful: 0 for A if scoreA >= scoreB, else 1 for B\n        result = 0 if scoreA >= scoreB else 1\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "为了超越相关性归因，进入因果推断的层面，研究人员开发了“激活修补”（activation patching）等干预技术。在本练习中，你将为一个简化的双头自注意力分类器实现这一前沿方法。通过将一个样本中特定注意力头的激活替换为另一个反例样本的激活，并量化其对模型预测的因果影响，你将亲身体验如何精确测量模型内部组件（如单个注意力头）的功能作用。",
            "id": "3153142",
            "problem": "给定一个简化的、完全指定的双头自注意力分类器以及一种激活补丁（activation patching）程序。目标是计算当一个样本的某个注意力头的注意力后激活值被来自反例的激活值替换时，该注意力头对预测概率变化的因果贡献。您的程序必须实现该模型，计算原始预测和补丁后预测，并为给定的测试套件输出预测概率的有符号差值。\n\n从第一性原理定义的模型：\n- 令 $L$ 表示序列长度，$d_{\\text{model}}$ 表示模型维度，$H$ 表示注意力头的数量，$d_h$ 表示每个头的维度。对于每个输入序列，令词元嵌入矩阵为 $X \\in \\mathbb{R}^{L \\times d_{\\text{model}}}$。\n- 对于头 $h \\in \\{0, 1\\}$，定义查询、键和值权重矩阵 $W_Q^{(h)} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_h}$、$W_K^{(h)} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_h}$ 和 $W_V^{(h)} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_h}$。\n- 查询、键和值的计算方式为 $Q^{(h)} = X W_Q^{(h)}$、$K^{(h)} = X W_K^{(h)}$ 和 $V^{(h)} = X W_V^{(h)}$。\n- 头 $h$ 的缩放点积注意力计算如下：\n$$\nS^{(h)} = \\frac{Q^{(h)} (K^{(h)})^\\top}{\\sqrt{d_h}}, \\quad A^{(h)}_{i,:} = \\operatorname{softmax}\\left(S^{(h)}_{i,:}\\right),\n$$\n其中 $\\operatorname{softmax}(z)_j = \\frac{e^{z_j}}{\\sum_k e^{z_k}}$ 逐行应用，得到注意力权重 $A^{(h)} \\in \\mathbb{R}^{L \\times L}$。注意力后头的输出为\n$$\nO^{(h)} = A^{(h)} V^{(h)} \\in \\mathbb{R}^{L \\times d_h}.\n$$\n- 多头注意力（MHA）的输出是 $O^{(0)}$ 和 $O^{(1)}$ 的拼接 $O = \\operatorname{concat}(O^{(0)}, O^{(1)}) \\in \\mathbb{R}^{L \\times d_{\\text{model}}}$，然后进行输出投影 $W_O \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}}$，得到\n$$\n\\tilde{O} = O W_O^\\top \\in \\mathbb{R}^{L \\times d_{\\text{model}}}.\n$$\n- 分类器读取第一个词元（索引为 $0$，类似于分类词元）作为 $\\tilde{o}_{\\text{cls}} = \\tilde{O}_{0,:} \\in \\mathbb{R}^{d_{\\text{model}}}$，并计算一个 logit $y = w^\\top \\tilde{o}_{\\text{cls}} + b$ 用于二元预测，其概率为\n$$\np = \\sigma(y) = \\frac{1}{1 + e^{-y}}.\n$$\n\n激活补丁程序：\n- 对于选定的头索引 $h^\\star \\in \\{0, 1\\}$，将一个样本的整个注意力后激活矩阵 $O^{(h^\\star)}$ 替换为从反例计算出的相应矩阵，同时保持所有其他头不变。为补丁后的样本重新计算预测概率，并将因果贡献定义为有符号差值\n$$\n\\Delta = p_{\\text{patched}} - p_{\\text{original}}.\n$$\n\n具体的、数值指定的模型：\n- 维度：$L = 3$，$d_{\\text{model}} = 6$，$H = 2$，$d_h = 3$。\n- 头 0 的权重（选择前三个嵌入维度）：\n$$\nW_Q^{(0)} = \\begin{bmatrix}\n1  0  0\\\\\n0  1  0\\\\\n0  0  1\\\\\n0  0  0\\\\\n0  0  0\\\\\n0  0  0\n\\end{bmatrix},\\quad\nW_K^{(0)} = \\begin{bmatrix}\n1  0  0\\\\\n0  1  0\\\\\n0  0  1\\\\\n0  0  0\\\\\n0  0  0\\\\\n0  0  0\n\\end{bmatrix},\\quad\nW_V^{(0)} = \\begin{bmatrix}\n1  0  0\\\\\n0  1  0\\\\\n0  0  1\\\\\n0  0  0\\\\\n0  0  0\\\\\n0  0  0\n\\end{bmatrix}.\n$$\n- 头 1 的权重（选择后三个嵌入维度）：\n$$\nW_Q^{(1)} = \\begin{bmatrix}\n0  0  0\\\\\n0  0  0\\\\\n0  0  0\\\\\n1  0  0\\\\\n0  1  0\\\\\n0  0  1\n\\end{bmatrix},\\quad\nW_K^{(1)} = \\begin{bmatrix}\n0  0  0\\\\\n0  0  0\\\\\n0  0  0\\\\\n1  0  0\\\\\n0  1  0\\\\\n0  0  1\n\\end{bmatrix},\\quad\nW_V^{(1)} = \\begin{bmatrix}\n0  0  0\\\\\n0  0  0\\\\\n0  0  0\\\\\n1  0  0\\\\\n0  1  0\\\\\n0  0  1\n\\end{bmatrix}.\n$$\n- 输出投影：\n$$\nW_O = I_6,\n$$\n$6 \\times 6$ 的单位矩阵。\n- 分类器参数：\n$$\nw = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1 \\end{bmatrix},\\quad b = 0.\n$$\n\n测试套件输入：\n- 样本 $X^{(E1)} \\in \\mathbb{R}^{3 \\times 6}$：\n$$\nX^{(E1)} = \\begin{bmatrix}\n2  2  2  0.1  -0.2  0.0\\\\\n3  -1  0  0.2  0.1  -0.1\\\\\n-1  1  0.5  0.0  0.0  0.2\n\\end{bmatrix}.\n$$\n- 反例 $X^{(C1)} \\in \\mathbb{R}^{3 \\times 6}$：\n$$\nX^{(C1)} = \\begin{bmatrix}\n-2  -2  -2  0.0  0.0  0.1\\\\\n-3  0.5  1  0.1  -0.2  0.3\\\\\n1  1  1  0.0  -0.1  0.0\n\\end{bmatrix}.\n$$\n- 样本 $X^{(E3)} \\in \\mathbb{R}^{3 \\times 6}$：\n$$\nX^{(E3)} = \\begin{bmatrix}\n4  0  1  0.0  0.0  0.0\\\\\n0.5  3  1  0.1  0.0  -0.1\\\\\n1  -2  2  0.0  0.2  0.2\n\\end{bmatrix}.\n$$\n- 反例 $X^{(C3)} \\in \\mathbb{R}^{3 \\times 6}$：\n$$\nX^{(C3)} = \\begin{bmatrix}\n-4  0  -1  0.0  0.0  0.0\\\\\n0.5  -3  -1  0.0  0.1  0.0\\\\\n-1  2  -2  -0.1  0.0  0.1\n\\end{bmatrix}.\n$$\n\n测试套件和要求的输出：\n- 情况 1：用 $X^{(C1)}$ 为 $X^{(E1)}$ 的头 $h^\\star = 0$ 进行补丁；输出 $\\Delta_1 \\in \\mathbb{R}$。\n- 情况 2：用 $X^{(C1)}$ 为 $X^{(E1)}$ 的头 $h^\\star = 1$ 进行补丁；输出 $\\Delta_2 \\in \\mathbb{R}$。\n- 情况 3：用 $X^{(E1)}$ 自身为 $X^{(E1)}$ 的头 $h^\\star = 0$ 进行补丁；输出 $\\Delta_3 \\in \\mathbb{R}$（这是一个应为零的边界情况）。\n- 情况 4：用 $X^{(C3)}$ 为 $X^{(E3)}$ 的头 $h^\\star = 0$ 进行补丁；输出 $\\Delta_4 \\in \\mathbb{R}$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、逗号分隔的结果列表，具体格式为 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$。不涉及物理单位或角度；所有输出都是实数。",
            "solution": "该问题是有效的，因为它有科学依据、良定且完全指定。它在深度学习模型可解释性领域提供了一个清晰的、可计算验证的任务。我们将着手提供解决方案。\n\n目标是计算特定注意力头对模型预测的因果贡献 $\\Delta$。这被定义为当一个样本 $X_E$ 的该头的注意力后激活值被来自一个反例 $X_C$ 的激活值替换时，预测概率的变化。公式为 $\\Delta = p_{\\text{patched}} - p_{\\text{original}}$。\n\n解决方案需要实现指定的双头注意力模型的前向传播，然后对四个给定的测试用例应用激活补丁程序。\n\n**1. 模型前向传播**\n\n前向传播计算给定输入词元嵌入矩阵 $X \\in \\mathbb{R}^{L \\times d_{\\text{model}}}$ 的预测概率 $p$。给定的维度为 $L=3$，$d_{\\text{model}}=6$，$H=2$，每个头的维度为 $d_h = d_{\\text{model}} / H = 3$。\n\n**步骤 1.1：分头计算**\n\n对于每个头 $h \\in \\{0, 1\\}$，我们计算查询（$Q^{(h)}$）、键（$K^{(h)}$）和值（$V^{(h)}$）矩阵。问题指定的权重矩阵 $W_Q^{(h)}$、$W_K^{(h)}$、$W_V^{(h)}$ 作为选择器。\n\n对于头 $h=0$，权重选择输入嵌入 $X$ 的前 $d_h=3$ 列。因此：\n$$\nQ^{(0)} = K^{(0)} = V^{(0)} = X_{[ :, 0:3 ]} \\in \\mathbb{R}^{3 \\times 3}\n$$\n\n对于头 $h=1$，权重选择 $X$ 的后 $d_h=3$ 列：\n$$\nQ^{(1)} = K^{(1)} = V^{(1)} = X_{[ :, 3:6 ]} \\in \\mathbb{R}^{3 \\times 3}\n$$\n\n接下来，我们计算缩放点积注意力分数 $S^{(h)}$，通过逐行 softmax 计算注意力权重 $A^{(h)}$，以及注意力后头的输出 $O^{(h)}$。\n$$\nS^{(h)} = \\frac{Q^{(h)} (K^{(h)})^\\top}{\\sqrt{d_h}}\n$$\n$$\nA^{(h)}_{i,:} = \\operatorname{softmax}\\left(S^{(h)}_{i,:}\\right) \\quad \\text{其中 } \\operatorname{softmax}(z)_j = \\frac{e^{z_j}}{\\sum_k e^{z_k}}\n$$\n$$\nO^{(h)} = A^{(h)} V^{(h)} \\in \\mathbb{R}^{3 \\times 3}\n$$\n\n**步骤 1.2：MHA 输出与分类**\n\n两个头的输出 $O^{(0)}$ 和 $O^{(1)}$被拼接起来，形成多头注意力（MHA）的输入 $O$：\n$$\nO = \\operatorname{concat}(O^{(0)}, O^{(1)}) \\in \\mathbb{R}^{3 \\times 6}\n$$\n\n之后是一个输出投影 $\\tilde{O} = O W_O^\\top$。由于 $W_O = I_6$（$6 \\times 6$ 的单位矩阵），我们有 $W_O^\\top = I_6$，因此：\n$$\n\\tilde{O} = O I_6 = O\n$$\n\n分类器使用第一个词元（索引为 $0$）的表示，记为 $\\tilde{o}_{\\text{cls}}$，即 $\\tilde{O}$ 的第一行：\n$$\n\\tilde{o}_{\\text{cls}} = \\tilde{O}_{0,:} \\in \\mathbb{R}^{6}\n$$\n\n使用权重向量 $w = [1, 1, 1, 0.1, 0.1, 0.1]^\\top$ 和偏置 $b=0$ 计算 logit $y$：\n$$\ny = w^\\top \\tilde{o}_{\\text{cls}} + b = \\sum_{i=0}^{5} w_i (\\tilde{o}_{\\text{cls}})_i\n$$\n\n最后，通过应用 sigmoid 函数 $\\sigma(\\cdot)$ 获得预测概率 $p$：\n$$\np = \\sigma(y) = \\frac{1}{1 + e^{-y}}\n$$\n\n这样就完成了单个输入 $X$ 的前向传播。\n\n**2. 激活补丁程序**\n\n为了计算给定样本 $X_E$、反例 $X_C$ 和目标头 $h^\\star$ 的因果贡献 $\\Delta$，我们按以下步骤进行：\n\n**步骤 2.1：计算原始概率**\n首先，我们通过对样本输入 $X_E$ 执行完整的前向传播来计算原始预测概率 $p_{\\text{original}}$。这需要从 $X_E$ 计算两个头的输出，$O^{(0)}_E$ 和 $O^{(1)}_E$。\n$$\np_{\\text{original}} = \\text{compute_prob}(X_E)\n$$\n\n**步骤 2.2：计算补丁后概率**\n接下来，我们计算“补丁后”的概率 $p_{\\text{patched}}$。这涉及将目标头 $h^\\star$ 的激活值替换为来自反例 $X_C$ 的相应激活值，同时保留来自原始样本 $X_E$ 的另一个头的激活值。\n\n如果 $h^\\star = 0$，则补丁后的 MHA 输入使用 $O^{(0)}_C$（从 $X_C$ 计算）和 $O^{(1)}_E$（从 $X_E$ 计算）构成：\n$$\nO_{\\text{patched}} = \\operatorname{concat}(O^{(0)}_C, O^{(1)}_E)\n$$\n\n如果 $h^\\star = 1$，则补丁后的 MHA 输入使用 $O^{(0)}_E$ 和 $O^{(1)}_C$：\n$$\nO_{\\text{patched}} = \\operatorname{concat}(O^{(0)}_E, O^{(1)}_C)\n$$\n\n从 $O_{\\text{patched}}$ 开始，我们遵循与标准前向传播中相同的分类步骤，计算补丁后的 logit $y_{\\text{patched}}$ 和概率 $p_{\\text{patched}}$。\n\n**步骤 2.3：计算因果贡献**\n因果贡献 $\\Delta$ 是补丁后概率与原始概率之间的有符号差值：\n$$\n\\Delta = p_{\\text{patched}} - p_{\\text{original}}\n$$\n\n对于 $X_C = X_E$ 的测试用例（情况 3），“补丁后”的激活值 $O^{(h^\\star)}_C$ 与原始激活值 $O^{(h^\\star)}_E$ 相同。因此，$p_{\\text{patched}} = p_{\\text{original}}$，我们期望 $\\Delta_3 = 0$，这可作为对实现的合理性检查。\n\n以下 Python 程序实现了此逻辑，用于计算指定测试套件的 $\\Delta_1, \\Delta_2, \\Delta_3, \\Delta_4$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import softmax\n\ndef solve():\n    \"\"\"\n    Computes the causal contribution of attention heads via activation patching.\n    \"\"\"\n    \n    # Define model parameters\n    d_h = 3\n    w = np.array([1.0, 1.0, 1.0, 0.1, 0.1, 0.1])\n    b = 0.0\n\n    # Define test suite inputs as numpy arrays\n    X_E1 = np.array([\n        [2.0, 2.0, 2.0, 0.1, -0.2, 0.0],\n        [3.0, -1.0, 0.0, 0.2, 0.1, -0.1],\n        [-1.0, 1.0, 0.5, 0.0, 0.0, 0.2]\n    ])\n    X_C1 = np.array([\n        [-2.0, -2.0, -2.0, 0.0, 0.0, 0.1],\n        [-3.0, 0.5, 1.0, 0.1, -0.2, 0.3],\n        [1.0, 1.0, 1.0, 0.0, -0.1, 0.0]\n    ])\n    X_E3 = np.array([\n        [4.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.5, 3.0, 1.0, 0.1, 0.0, -0.1],\n        [1.0, -2.0, 2.0, 0.0, 0.2, 0.2]\n    ])\n    X_C3 = np.array([\n        [-4.0, 0.0, -1.0, 0.0, 0.0, 0.0],\n        [0.5, -3.0, -1.0, 0.0, 0.1, 0.0],\n        [-1.0, 2.0, -2.0, -0.1, 0.0, 0.1]\n    ])\n\n    # Define the test cases\n    test_cases = [\n        {'X_E': X_E1, 'X_C': X_C1, 'h_star': 0},  # Case 1\n        {'X_E': X_E1, 'X_C': X_C1, 'h_star': 1},  # Case 2\n        {'X_E': X_E1, 'X_C': X_E1, 'h_star': 0},  # Case 3\n        {'X_E': X_E3, 'X_C': X_C3, 'h_star': 0}   # Case 4\n    ]\n    \n    def sigmoid(y):\n        \"\"\"Computes the sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-y))\n\n    def compute_head_output(X, h, d_h_val):\n        \"\"\"\n        Computes the post-attention output for a single head.\n        \n        Args:\n            X (np.ndarray): Input token embeddings, shape (L, d_model).\n            h (int): Head index (0 or 1).\n            d_h_val (int): Per-head dimension.\n            \n        Returns:\n            np.ndarray: Post-attention head output, shape (L, d_h).\n        \"\"\"\n        if h == 0:\n            sub_X = X[:, :d_h_val]\n        else: # h == 1\n            sub_X = X[:, d_h_val:]\n        \n        Q, K, V = sub_X, sub_X, sub_X\n        \n        S = (Q @ K.T) / np.sqrt(d_h_val)\n        A = softmax(S, axis=1)\n        O_h = A @ V\n        \n        return O_h\n\n    def compute_prob_from_heads(O_0, O_1):\n        \"\"\"\n        Computes the final probability from the head outputs.\n        \"\"\"\n        O = np.concatenate((O_0, O_1), axis=1)\n        o_cls = O[0, :]\n        y = np.dot(w, o_cls) + b\n        p = sigmoid(y)\n        return p\n\n    results = []\n    \n    # Process each test case\n    for case in test_cases:\n        X_E = case['X_E']\n        X_C = case['X_C']\n        h_star = case['h_star']\n\n        # 1. Compute original probability\n        O0_E = compute_head_output(X_E, 0, d_h)\n        O1_E = compute_head_output(X_E, 1, d_h)\n        p_original = compute_prob_from_heads(O0_E, O1_E)\n\n        # 2. Compute patched probability\n        if h_star == 0:\n            O0_patched = compute_head_output(X_C, 0, d_h)\n            O1_patched = O1_E\n        else: # h_star == 1\n            O0_patched = O0_E\n            O1_patched = compute_head_output(X_C, 1, d_h)\n        \n        p_patched = compute_prob_from_heads(O0_patched, O1_patched)\n        \n        # 3. Compute and store the difference\n        delta = p_patched - p_original\n        results.append(delta)\n\n    # Format the output as specified\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}