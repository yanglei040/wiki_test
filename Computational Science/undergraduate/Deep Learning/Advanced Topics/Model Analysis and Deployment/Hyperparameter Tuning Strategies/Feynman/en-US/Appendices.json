{
    "hands_on_practices": [
        {
            "introduction": "When tuning a neural network, the learning rate and weight decay are two of the most critical hyperparameters. A common challenge is that the optimal value for one often depends on the value of the other. This exercise  provides a hands-on approach to understanding this interplay by deriving a simple yet powerful scaling law. By formalizing the concept of \"effective regularization,\" you will discover a principle that helps you adjust the weight decay coefficient $\\lambda$ in response to changes in the learning rate $\\alpha$ to maintain consistent model behavior across different optimizers.",
            "id": "3135392",
            "problem": "You are asked to formalize and test a hyperparameter scaling heuristic for weight decay in gradient-based optimization of deep learning models. The goal is to keep the effective step regularization constant when the learning rate changes. Proceed from core definitions of gradient-based updates and Euclidean norm regularization, and avoid any shortcut formulas that jump directly to the result.\n\nLet $w_t \\in \\mathbb{R}$ denote a single scalar parameter at step $t$, let $\\alpha \\in \\mathbb{R}_{\\ge 0}$ denote the learning rate, let $\\lambda \\in \\mathbb{R}_{\\ge 0}$ denote the weight decay or Euclidean norm (L2) regularization coefficient, and let $g_t \\in \\mathbb{R}$ denote the data-dependent gradient of the loss with respect to $w_t$ when no regularization is applied. Assume one of the following well-tested update rules:\n- Stochastic Gradient Descent (SGD) without momentum: $w_{t+1} = w_t - \\alpha \\nabla_w \\mathcal{L}(w_t)$ with Euclidean norm (L2) regularization included in the loss, so the gradient includes an additive term dependent on $\\lambda$ and $w_t$.\n- Momentum variant of Stochastic Gradient Descent (SGD): a velocity $v_t \\in \\mathbb{R}$ is used and updated by a linear recurrence with coefficient $\\beta \\in [0,1)$, together with the same treatment of Euclidean norm (L2) regularization as above.\n- Adaptive Moment Estimation with decoupled weight decay (AdamW): the first and second moments of the data gradient are maintained with coefficients $\\beta_1, \\beta_2 \\in [0,1)$ and the Euclidean norm (L2) penalty is applied as a multiplicative decay that is explicitly separated from the data gradient pathway.\n\nDefine the per-step effective regularization, for a given optimizer, learning rate, and weight decay, at a state where the data-driven gradient is absent, by the multiplicative shrinkage factor\n$$\ns(\\alpha, \\lambda; w_t) \\equiv \\frac{|w_{t+1}|}{|w_t|},\n$$\nwhen the update is executed with $g_t = 0$ and any internal state (such as velocity or moments) initialized to zero at step $t$. This definition isolates the influence of the regularization mechanism on a single step and ignores any data-driven change.\n\nTask A (derivation): Using only the described update rules and the definition of Euclidean norm (L2) regularization and decoupled weight decay, derive a condition relating $\\lambda$ and $\\alpha$ that keeps $s(\\alpha, \\lambda; w_t)$ invariant when $\\alpha$ changes. Your derivation must be based on the fundamental step update dynamics and should not rely on any undocumented identities.\n\nTask B (program): Implement a program that evaluates two candidate scaling laws for $\\lambda$ as $\\alpha$ changes, given a baseline $(\\alpha_0, \\lambda_0)$:\n1. Proportional scaling: $\\lambda_{\\mathrm{prop}}(\\alpha) = \\lambda_0 \\cdot \\frac{\\alpha}{\\alpha_0}$.\n2. Inverse-proportional scaling: $\\lambda_{\\mathrm{inv}}(\\alpha) = \\lambda_0 \\cdot \\frac{\\alpha_0}{\\alpha}$.\n\nFor each test case, do the following:\n1. Compute the baseline shrinkage $s_0 = s(\\alpha_0, \\lambda_0; w_0)$ at a given nonzero initial weight $w_0$ with $g_t=0$ and any internal optimizer state initialized to zero.\n2. For each $\\alpha$ in the case’s list of learning rates, compute $s_{\\mathrm{prop}}(\\alpha) = s(\\alpha, \\lambda_{\\mathrm{prop}}(\\alpha); w_0)$ and $s_{\\mathrm{inv}}(\\alpha) = s(\\alpha, \\lambda_{\\mathrm{inv}}(\\alpha); w_0)$ under the same initial conditions, using the given optimizer’s exact one-step update rule.\n3. Return two booleans per test case:\n   - The first boolean is true if $|s_{\\mathrm{inv}}(\\alpha) - s_0| \\le \\varepsilon$ for all $\\alpha$ in the list.\n   - The second boolean is true if $|s_{\\mathrm{prop}}(\\alpha) - s_0| \\le \\varepsilon$ for all $\\alpha$ in the list.\nHere $\\varepsilon > 0$ is a given tolerance for numerical comparison.\n\nThe test suite you must implement consists of the following five cases, each providing a baseline $(\\alpha_0, \\lambda_0)$, a nonzero initial weight $w_0$, a set of learning rates to test, and an optimizer with its coefficients. In all cases, evaluate a single update with $g_t = 0$ and zeroed internal state at step $t$.\n- Case 1 (happy path, coupled regularization in loss): Optimizer is SGD without momentum, baseline $\\alpha_0 = 0.1$, $\\lambda_0 = 0.01$, $w_0 = 1.0$, test learning rates $\\{0.05, 0.1, 0.2\\}$, tolerance $\\varepsilon = 10^{-12}$.\n- Case 2 (momentum, coupled regularization in loss): Optimizer is momentum SGD with $\\beta = 0.9$, baseline $\\alpha_0 = 0.1$, $\\lambda_0 = 0.01$, $w_0 = 1.0$, test learning rates $\\{0.05, 0.1, 0.2\\}$, tolerance $\\varepsilon = 10^{-12}$.\n- Case 3 (decoupled weight decay): Optimizer is AdamW with $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\varepsilon_{\\mathrm{adam}} = 10^{-8}$, baseline $\\alpha_0 = 0.001$, $\\lambda_0 = 0.01$, $w_0 = 1.0$, test learning rates $\\{0.0005, 0.001, 0.002\\}$, tolerance $\\varepsilon = 10^{-12}$.\n- Case 4 (edge case: zero weight decay): Optimizer is SGD without momentum, baseline $\\alpha_0 = 0.05$, $\\lambda_0 = 0.0$, $w_0 = 1.0$, test learning rates $\\{0.01, 0.05, 0.1\\}$, tolerance $\\varepsilon = 10^{-12}$.\n- Case 5 (coverage across scales): Optimizer is AdamW with $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\varepsilon_{\\mathrm{adam}} = 10^{-8}$, baseline $\\alpha_0 = 10^{-2}$, $\\lambda_0 = 5 \\cdot 10^{-3}$, $w_0 = 1.0$, test learning rates $\\{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\\}$, tolerance $\\varepsilon = 10^{-12}$.\n\nRequired final output format: Your program should produce a single line of output containing a flat Python list of booleans with exactly two booleans per test case, in the order of the cases above. For each case, append first the boolean for inverse-proportional scaling and then the boolean for proportional scaling. For example, a five-case run must print a single line in the exact format: \"[b1_inv,b1_prop,b2_inv,b2_prop,b3_inv,b3_prop,b4_inv,b4_prop,b5_inv,b5_prop]\".",
            "solution": "The task is to derive a hyperparameter scaling heuristic for weight decay and then programmatically test it. The goal is to maintain a constant effective regularization strength when the learning rate changes. The effective regularization for a single step is defined by the multiplicative shrinkage factor $s(\\alpha, \\lambda; w_t) \\equiv \\frac{|w_{t+1}|}{|w_t|}$, computed under the specific conditions of a zero data-dependent gradient ($g_t=0$) and zeroed internal optimizer states at step $t$. Let $w_t \\in \\mathbb{R}$ be a scalar weight, $\\alpha \\in \\mathbb{R}_{\\ge 0}$ the learning rate, and $\\lambda \\in \\mathbb{R}_{\\ge 0}$ the weight decay coefficient. We assume $w_t \\neq 0$.\n\n**Task A: Derivation of the Invariance Condition**\n\nWe will analyze the one-step update rule for each specified optimizer under the given conditions ($g_t=0$, zero initial state) to derive the relationship between $\\alpha$ and $\\lambda$ that keeps $s(\\alpha, \\lambda; w_t)$ constant.\n\n**1. Stochastic Gradient Descent (SGD) without Momentum**\n\nThe total loss function with Euclidean norm (L2) regularization is $\\mathcal{L}_{\\text{total}}(w_t) = \\mathcal{L}(w_t) + \\frac{\\lambda}{2} w_t^2$. The gradient with respect to $w_t$ is $\\nabla_w \\mathcal{L}_{\\text{total}}(w_t) = \\nabla_w \\mathcal{L}(w_t) + \\lambda w_t$. Let $g_t = \\nabla_w \\mathcal{L}(w_t)$ be the data-dependent part of the gradient. The full gradient is then $g_t + \\lambda w_t$.\n\nThe SGD update rule is:\n$$w_{t+1} = w_t - \\alpha \\nabla_w \\mathcal{L}_{\\text{total}}(w_t) = w_t - \\alpha (g_t + \\lambda w_t)$$\nUnder the condition $g_t=0$, the update simplifies to:\n$$w_{t+1} = w_t - \\alpha (\\lambda w_t) = (1 - \\alpha \\lambda) w_t$$\nThe shrinkage factor $s$ is therefore:\n$$s(\\alpha, \\lambda; w_t) = \\frac{|w_{t+1}|}{|w_t|} = \\frac{|(1 - \\alpha \\lambda) w_t|}{|w_t|} = |1 - \\alpha \\lambda|$$\nFor $s(\\alpha, \\lambda; w_t)$ to be invariant to changes in $\\alpha$, the value of $|1 - \\alpha \\lambda|$ must remain constant. Assuming that the update is stable (i.e., $1 - \\alpha \\lambda \\ge 0$), this implies that the product $\\alpha \\lambda$ must be constant.\n$$\\alpha \\lambda = C$$\nwhere $C$ is a constant. Given a baseline pair $(\\alpha_0, \\lambda_0)$, the constant is $C = \\alpha_0 \\lambda_0$. Thus, for any new learning rate $\\alpha$, the corresponding weight decay $\\lambda$ must satisfy $\\lambda = \\frac{\\alpha_0 \\lambda_0}{\\alpha}$. This is the inverse-proportional scaling law.\n\n**2. Momentum SGD**\n\nThe optimizer maintains a velocity vector $v_t$. The update rules are given for the total gradient $\\nabla_w \\mathcal{L}_{\\text{total}}(w_t) = g_t + \\lambda w_t$:\n$$v_{t+1} = \\beta v_t + (g_t + \\lambda w_t)$$\n$$w_{t+1} = w_t - \\alpha v_{t+1}$$\nwhere $\\beta \\in [0, 1)$ is the momentum coefficient.\nWe apply the conditions: $g_t=0$ and the internal state at step $t$ is zero, which means $v_t=0$.\nThe velocity update becomes:\n$$v_{t+1} = \\beta(0) + (0 + \\lambda w_t) = \\lambda w_t$$\nSubstituting this into the parameter update rule:\n$$w_{t+1} = w_t - \\alpha (\\lambda w_t) = (1 - \\alpha \\lambda) w_t$$\nThis is identical to the SGD case. The shrinkage factor is again $s(\\alpha, \\lambda; w_t) = |1 - \\alpha \\lambda|$, and the invariance condition is $\\alpha \\lambda = \\text{constant}$. This again leads to the inverse-proportional scaling law.\n\n**3. AdamW (Decoupled Weight Decay)**\n\nIn AdamW, the weight decay is \"decoupled\" from the gradient-based update. The update is performed in two conceptual steps: first applying the weight decay, then applying the Adam update based on the data gradient $g_t$. A common implementation form is:\n$$w_{t+1} = w_t (1 - \\alpha \\lambda) - \\alpha \\cdot \\text{AdamUpdate}(g_t, m_t, v_t)$$\nThe `AdamUpdate` term depends on the moment estimates:\n$$m_{t+1} = \\beta_1 m_t + (1-\\beta_1) g_t$$\n$$v_{t+1} = \\beta_2 v_t + (1-\\beta_2) g_t^2$$\nAnd their bias-corrected versions, $\\hat{m}_{t+1}$ and $\\hat{v}_{t+1}$. The update term is proportional to $\\frac{\\hat{m}_{t+1}}{\\sqrt{\\hat{v}_{t+1}} + \\varepsilon_{\\text{adam}}}$.\n\nUnder the given conditions, $g_t=0$, and the internal states at step $t$ are zero ($m_t=0, v_t=0$).\nThe moment updates yield:\n$$m_{t+1} = \\beta_1(0) + (1-\\beta_1)(0) = 0$$\n$$v_{t+1} = \\beta_2(0) + (1-\\beta_2)(0^2) = 0$$\nConsequently, their bias-corrected versions are also zero, $\\hat{m}_{t+1}=0$ and $\\hat{v}_{t+1}=0$. The entire `AdamUpdate` term becomes zero:\n$$\\alpha \\cdot \\text{AdamUpdate}(g_t=0, m_t=0, v_t=0) = 0$$\nThe AdamW update rule simplifies to:\n$$w_{t+1} = w_t (1 - \\alpha \\lambda) - 0 = (1 - \\alpha \\lambda) w_t$$\nOnce again, the update becomes identical to the previous cases under the specified conditions. The shrinkage factor is $s(\\alpha, \\lambda; w_t) = |1 - \\alpha \\lambda|$, and the invariance condition is $\\alpha \\lambda = \\text{constant}$.\n\n**Conclusion of Derivation**\n\nFor all three optimizers—SGD with coupled regularization, Momentum SGD with coupled regularization, and AdamW with decoupled weight decay—the effective single-step regularization shrinkage factor $s(\\alpha, \\lambda; w_t)$ is invariant to changes in $\\alpha$ if and only if the product $\\alpha \\lambda$ is held constant. This corresponds to the **inverse-proportional scaling law**: $\\lambda(\\alpha) = \\lambda_0 \\cdot \\frac{\\alpha_0}{\\alpha}$. The proportional scaling law, $\\lambda(\\alpha) = \\lambda_0 \\cdot \\frac{\\alpha}{\\alpha_0}$, would cause the product $\\alpha \\lambda$ to scale with $\\alpha^2$, thus failing to keep the shrinkage factor constant.\n\nThe following program will numerically validate this conclusion based on the test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef update_sgd(w_t, alpha, lambda_val, **kwargs):\n    \"\"\"\n    Performs a single update step for SGD with coupled L2 regularization.\n    Conditions: g_t = 0, no internal state.\n    \"\"\"\n    g_t = 0.0\n    total_gradient = g_t + lambda_val * w_t\n    w_t_plus_1 = w_t - alpha * total_gradient\n    return w_t_plus_1\n\ndef update_momentum_sgd(w_t, alpha, lambda_val, beta, **kwargs):\n    \"\"\"\n    Performs a single update step for Momentum SGD with coupled L2 regularization.\n    Conditions: g_t = 0, velocity v_t = 0.\n    \"\"\"\n    g_t = 0.0\n    v_t = 0.0  # Zero initial state\n    total_gradient = g_t + lambda_val * w_t\n    v_t_plus_1 = beta * v_t + total_gradient\n    w_t_plus_1 = w_t - alpha * v_t_plus_1\n    return w_t_plus_1\n\ndef update_adamw(w_t, alpha, lambda_val, beta1, beta2, epsilon_adam, **kwargs):\n    \"\"\"\n    Performs a single update step for AdamW with decoupled weight decay.\n    Conditions: g_t = 0, moments m_t = 0, v_t = 0.\n    \"\"\"\n    g_t = 0.0\n    m_t = 0.0  # Zero initial state\n    v_t = 0.0  # Zero initial state\n    step = 0  # Simulation starts at step 0\n    step_plus_1 = step + 1\n\n    # Gradient processing part, which becomes zero under g_t=0 and zeroed state\n    m_t_plus_1 = beta1 * m_t + (1.0 - beta1) * g_t\n    v_t_plus_1 = beta2 * v_t + (1.0 - beta2) * g_t**2\n\n    # Bias correction. 1 - beta**(step+1) is safe since step+1=1.\n    m_hat = m_t_plus_1 / (1.0 - beta1**step_plus_1)\n    v_hat = v_t_plus_1 / (1.0 - beta2**step_plus_1)\n\n    grad_update_term = m_hat / (np.sqrt(v_hat) + epsilon_adam)\n\n    # Decoupled weight decay update\n    w_t_plus_1 = w_t * (1.0 - alpha * lambda_val) - alpha * grad_update_term\n    return w_t_plus_1\n\ndef solve():\n    \"\"\"\n    Solves the problem by running all test cases and printing the results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, coupled regularization in loss)\n        {'optimizer': 'SGD', 'alpha0': 0.1, 'lambda0': 0.01, 'w0': 1.0, \n         'test_alphas': [0.05, 0.1, 0.2], 'epsilon': 1e-12, 'params': {}},\n        # Case 2 (momentum, coupled regularization in loss)\n        {'optimizer': 'MomentumSGD', 'alpha0': 0.1, 'lambda0': 0.01, 'w0': 1.0, \n         'test_alphas': [0.05, 0.1, 0.2], 'epsilon': 1e-12, 'params': {'beta': 0.9}},\n        # Case 3 (decoupled weight decay)\n        {'optimizer': 'AdamW', 'alpha0': 0.001, 'lambda0': 0.01, 'w0': 1.0, \n         'test_alphas': [0.0005, 0.001, 0.002], 'epsilon': 1e-12, \n         'params': {'beta1': 0.9, 'beta2': 0.999, 'epsilon_adam': 1e-8}},\n        # Case 4 (edge case: zero weight decay)\n        {'optimizer': 'SGD', 'alpha0': 0.05, 'lambda0': 0.0, 'w0': 1.0, \n         'test_alphas': [0.01, 0.05, 0.1], 'epsilon': 1e-12, 'params': {}},\n        # Case 5 (coverage across scales)\n        {'optimizer': 'AdamW', 'alpha0': 1e-2, 'lambda0': 5e-3, 'w0': 1.0, \n         'test_alphas': [1e-4, 1e-3, 1e-2, 1e-1], 'epsilon': 1e-12, \n         'params': {'beta1': 0.9, 'beta2': 0.999, 'epsilon_adam': 1e-8}}\n    ]\n    \n    optimizer_updates = {\n        'SGD': update_sgd,\n        'MomentumSGD': update_momentum_sgd,\n        'AdamW': update_adamw\n    }\n\n    final_results = []\n    for case in test_cases:\n        alpha0 = case['alpha0']\n        lambda0 = case['lambda0']\n        w0 = case['w0']\n        epsilon = case['epsilon']\n        update_func = optimizer_updates[case['optimizer']]\n\n        # 1. Compute baseline shrinkage s0\n        w1_base = update_func(w0, alpha0, lambda0, **case['params'])\n        if w0 == 0.0:\n            s0 = 0.0 if w1_base == 0.0 else float('inf')\n        else:\n            s0 = abs(w1_base / w0)\n\n        is_inv_stable = True\n        is_prop_stable = True\n\n        for alpha in case['test_alphas']:\n            # 2. Test inverse-proportional scaling\n            # Handle alpha=0 case, though not in test data.\n            lambda_inv = lambda0 * (alpha0 / alpha) if alpha != 0.0 else float('inf')\n            w1_inv = update_func(w0, alpha, lambda_inv, **case['params'])\n            s_inv = abs(w1_inv / w0) if w0 != 0 else (0.0 if w1_inv == 0.0 else float('inf'))\n            if abs(s_inv - s0) > epsilon:\n                is_inv_stable = False\n\n            # 3. Test proportional scaling\n            # Handle alpha0=0 case, though not in test data.\n            if alpha0 != 0.0:\n                lambda_prop = lambda0 * (alpha / alpha0)\n            else:\n                 lambda_prop = 0.0 if lambda0 == 0.0 else float('inf')\n            w1_prop = update_func(w0, alpha, lambda_prop, **case['params'])\n            s_prop = abs(w1_prop / w0) if w0 != 0.0 else (0.0 if w1_prop == 0.0 else float('inf'))\n            if abs(s_prop - s0) > epsilon:\n                is_prop_stable = False\n        \n        final_results.append(is_inv_stable)\n        final_results.append(is_prop_stable)\n    \n    # Format the final output string exactly as required.\n    # str(True) -> 'True', str(False) -> 'False'\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The Adam optimizer is a cornerstone of modern deep learning, but its combination of adaptive learning rates and momentum can sometimes feel like a black box. This complexity is compounded when we add gradient clipping, a technique used to prevent exploding gradients. This practice  allows you to peek inside this machinery by calculating the precise effect of clipping and Adam's moment estimates on the final parameter update. By formalizing an \"effective step normalization factor,\" you will gain a quantitative understanding of how these components interact to control the optimization trajectory.",
            "id": "3135417",
            "problem": "You are asked to formalize and quantify the interplay between gradient clipping by global norm and adaptive moment estimation in the Adam optimizer through the lens of an effective step normalization factor. Work from fundamental, widely tested definitions of gradient-based optimization, exponential moving averages, and global norm clipping.\n\nGiven a parameter vector and its gradient in a deep learning model, define the global norm clipping of the gradient as follows. For a gradient vector $\\mathbf{g} \\in \\mathbb{R}^{d}$ and a clip threshold $c \\in \\mathbb{R}_{>0}$, the clipped gradient $\\tilde{\\mathbf{g}}$ is\n$$\n\\tilde{\\mathbf{g}} \\;=\\; \\mathbf{g} \\cdot \\min\\!\\left(1, \\frac{c}{\\lVert \\mathbf{g} \\rVert_{2}}\\right),\n$$\nwith the convention that if $\\lVert \\mathbf{g} \\rVert_{2} = 0$ then $\\tilde{\\mathbf{g}} = \\mathbf{0}$.\n\nIn the Adam optimizer, define the exponential moving averages of the first and second moments for time step $t \\in \\mathbb{N}$ with hyperparameters $\\beta_{1} \\in [0,1)$, $\\beta_{2} \\in [0,1)$:\n- The first-moment estimate $m_{t}$ is an exponential moving average of $\\tilde{\\mathbf{g}}$ with decay $\\beta_{1}$.\n- The second-moment estimate $v_{t}$ is an exponential moving average of the elementwise square of $\\tilde{\\mathbf{g}}$ with decay $\\beta_{2}$.\nImplement bias corrections to obtain $\\hat{m}_{t}$ and $\\hat{v}_{t}$ using the standard bias-correction factors determined by $t$, $\\beta_{1}$, and $\\beta_{2}$. The elementwise parameter update is formed by normalizing $\\hat{m}_{t}$ by the square root of $\\hat{v}_{t}$ regularized by $\\varepsilon \\in \\mathbb{R}_{>0}$, and then scaled by the learning rate $\\alpha \\in \\mathbb{R}_{>0}$. The update vector is denoted $\\Delta \\theta \\in \\mathbb{R}^{d}$.\n\nDefine the effective step normalization factor relative to a plain stochastic gradient descent step on the clipped gradient as\n$$\ns_{\\text{eff}} \\;=\\; \n\\begin{cases}\n\\dfrac{\\lVert \\Delta \\theta \\rVert_{2}}{\\alpha \\,\\lVert \\tilde{\\mathbf{g}} \\rVert_{2}}, & \\text{if } \\lVert \\tilde{\\mathbf{g}} \\rVert_{2} > 0, \\\\\n0, & \\text{if } \\lVert \\tilde{\\mathbf{g}} \\rVert_{2} = 0,\n\\end{cases}\n$$\nand, analogously, define the effective factor relative to the unclipped gradient as\n$$\nr_{\\text{unclipped}} \\;=\\;\n\\begin{cases}\n\\dfrac{\\lVert \\Delta \\theta \\rVert_{2}}{\\alpha \\,\\lVert \\mathbf{g} \\rVert_{2}}, & \\text{if } \\lVert \\mathbf{g} \\rVert_{2} > 0, \\\\\n0, & \\text{if } \\lVert \\mathbf{g} \\rVert_{2} = 0.\n\\end{cases}\n$$\nAlso define the clipping indicator\n$$\n\\text{clipped} \\;=\\; \\big(\\lVert \\mathbf{g} \\rVert_{2} > c\\big).\n$$\n\nYour task is to:\n- Starting from the above base definitions, derive the one-step Adam update $\\Delta \\theta$ given inputs $\\mathbf{g}$, $m_{t-1}$, $v_{t-1}$, $t$, and hyperparameters $\\alpha$, $\\beta_{1}$, $\\beta_{2}$, $\\varepsilon$, and $c$.\n- Compute $s_{\\text{eff}}$, $r_{\\text{unclipped}}$, and $\\text{clipped}$ for each test case below.\n\nTest suite. For each case, all vectors are in $\\mathbb{R}^{3}$, and numbers are in standard decimal or scientific notation:\n1. $d = 3$, $\\mathbf{g} = [\\,0.1,\\,-0.2,\\,0.05\\,]$, $m_{t-1} = [\\,0,\\,0,\\,0\\,]$, $v_{t-1} = [\\,0,\\,0,\\,0\\,]$, $t = 1$, $\\alpha = 0.01$, $\\beta_{1} = 0.9$, $\\beta_{2} = 0.999$, $\\varepsilon = 10^{-8}$, $c = 1.0$.\n2. $d = 3$, $\\mathbf{g} = [\\,10.0,\\,0.0,\\,0.0\\,]$, $m_{t-1} = [\\,0,\\,0,\\,0\\,]$, $v_{t-1} = [\\,0,\\,0,\\,0\\,]$, $t = 1$, $\\alpha = 0.001$, $\\beta_{1} = 0.9$, $\\beta_{2} = 0.999$, $\\varepsilon = 10^{-8}$, $c = 1.0$.\n3. $d = 3$, $\\mathbf{g} = [\\,0.5,\\,0.5,\\,0.5\\,]$, $m_{t-1} = [\\,0.1,\\,-0.1,\\,0.0\\,]$, $v_{t-1} = [\\,0.04,\\,0.01,\\,0.09\\,]$, $t = 10$, $\\alpha = 0.005$, $\\beta_{1} = 0.8$, $\\beta_{2} = 0.9$, $\\varepsilon = 10^{-6}$, $c = 0.5$.\n4. $d = 3$, $\\mathbf{g} = [\\,0.0,\\,0.0,\\,0.0\\,]$, $m_{t-1} = [\\,0.01,\\,-0.02,\\,0.0\\,]$, $v_{t-1} = [\\,0.001,\\,0.004,\\,0.0009\\,]$, $t = 5$, $\\alpha = 0.01$, $\\beta_{1} = 0.9$, $\\beta_{2} = 0.99$, $\\varepsilon = 10^{-8}$, $c = 0.1$.\n5. $d = 3$, $\\mathbf{g} = [\\,10^{-4},\\,-2\\cdot 10^{-4},\\,3\\cdot 10^{-4}\\,]$, $m_{t-1} = [\\,0,\\,0,\\,0\\,]$, $v_{t-1} = [\\,0,\\,0,\\,0\\,]$, $t = 1$, $\\alpha = 10^{-2}$, $\\beta_{1} = 0.0$, $\\beta_{2} = 0.0$, $\\varepsilon = 10^{-2}$, $c = 10^{-3}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a list of the form $[\\,s_{\\text{eff}},\\,r_{\\text{unclipped}},\\,\\text{clipped}\\,]$, in that order. The final output must therefore look like\n[[case1_values],[case2_values],...,[case5_values]]\nwith no spaces anywhere in the line. All floating-point values must be printed in standard decimal notation or scientific notation as produced by typical programming-language default conversions, and the boolean must be either True or False. No user input is required, and no physical units or angular measures are involved in this problem. All computations must be performed exactly as specified above, and the ratios must use the conventions for zero denominators specified in the definitions of $s_{\\text{eff}}$ and $r_{\\text{unclipped}}$.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and provides all necessary information for a unique solution. It is based on standard, formalizable definitions from the field of deep learning optimization.\n\nThe task is to compute three quantities for several test cases: the effective step normalization factor relative to the clipped gradient ($s_{\\text{eff}}$), the effective step normalization factor relative to the unclipped gradient ($r_{\\text{unclipped}}$), and a boolean clipping indicator ($\\text{clipped}$). This requires implementing a single step of the Adam optimizer with gradient clipping. The procedure is as follows:\n\n### Step 1: Gradient Clipping\nFirst, we compute the $L_2$-norm of the raw gradient vector $\\mathbf{g} \\in \\mathbb{R}^{d}$, denoted as $\\lVert \\mathbf{g} \\rVert_{2}$.\n$$\n\\lVert \\mathbf{g} \\rVert_{2} = \\sqrt{\\sum_{i=1}^{d} g_i^2}\n$$\nThe clipping indicator, $\\text{clipped}$, is a boolean value determined by comparing this norm to the clipping threshold $c \\in \\mathbb{R}_{>0}$:\n$$\n\\text{clipped} = (\\lVert \\mathbf{g} \\rVert_{2} > c)\n$$\nThe gradient $\\mathbf{g}$ is then clipped to produce $\\tilde{\\mathbf{g}}$. If the norm of $\\mathbf{g}$ exceeds $c$, the gradient vector is scaled down to have a norm of $c$. Otherwise, it remains unchanged. This is expressed by the formula:\n$$\n\\tilde{\\mathbf{g}} = \\mathbf{g} \\cdot \\min\\left(1, \\frac{c}{\\lVert \\mathbf{g} \\rVert_{2}}\\right)\n$$\nA special convention is given for a zero gradient: if $\\lVert \\mathbf{g} \\rVert_{2} = 0$, then $\\tilde{\\mathbf{g}} = \\mathbf{0}$. This is naturally handled by the multiplicative nature of the clipping formula if we define the scaling factor to be $1$ when $\\lVert \\mathbf{g} \\rVert_{2}=0$ to avoid division by zero, as $\\mathbf{g}$ itself is the zero vector.\n\n### Step 2: Adam Moment Updates\nThe Adam optimizer maintains exponential moving averages of the gradient (first moment) and its square (second moment). Given the previous moment estimates $m_{t-1}$ and $v_{t-1}$ at timestep $t-1$, and the decay rates $\\beta_1, \\beta_2 \\in [0, 1)$, the new estimates at timestep $t$ are calculated using the clipped gradient $\\tilde{\\mathbf{g}}$.\n\nThe first-moment estimate $m_t$ is updated as:\n$$\nm_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\tilde{\\mathbf{g}}\n$$\nThe second-moment estimate $v_t$ is updated using the elementwise square of the clipped gradient, $\\tilde{\\mathbf{g}}^2$:\n$$\nv_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\tilde{\\mathbf{g}}^2\n$$\n\n### Step 3: Bias Correction\nThe initial moment estimates are typically initialized to zero vectors. This introduces a bias towards zero, especially during the initial stages of optimization. Adam corrects for this bias by dividing the moment estimates by bias-correction factors that depend on the timestep $t$ and the decay rates.\n\nThe bias-corrected first-moment estimate $\\hat{m}_t$ is:\n$$\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n$$\nThe bias-corrected second-moment estimate $\\hat{v}_t$ is:\n$$\n\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n$$\n\n### Step 4: Parameter Update Vector\nThe parameter update vector, $\\Delta\\theta$, is computed using the bias-corrected moment estimates. The first-moment estimate $\\hat{m}_t$ acts as the update direction, and it is normalized elementwise by the square root of the second-moment estimate $\\hat{v}_t$. A small constant $\\varepsilon \\in \\mathbb{R}_{>0}$ is added to the denominator for numerical stability. The result is scaled by the learning rate $\\alpha \\in \\mathbb{R}_{>0}$.\n$$\n\\Delta\\theta = \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}\n$$\nNote that typically the parameter update rule is $\\theta_t = \\theta_{t-1} - \\text{step}$. The vector $\\Delta\\theta$ asked for is just the update itself, and since we are only concerned with its norm $\\lVert \\Delta\\theta \\rVert_{2}$, the sign is irrelevant.\n\n### Step 5: Final Metric Calculation\nWith the parameter update vector $\\Delta\\theta$ computed, we can now calculate the required metrics.\n\nThe $L_2$-norm of the update vector, $\\lVert \\Delta\\theta \\rVert_2$, and the $L_2$-norm of the clipped gradient, $\\lVert \\tilde{\\mathbf{g}} \\rVert_2$, are calculated.\n\nThe effective step normalization factor relative to the clipped gradient, $s_{\\text{eff}}$, is defined as:\n$$\ns_{\\text{eff}} = \n\\begin{cases}\n\\dfrac{\\lVert \\Delta \\theta \\rVert_{2}}{\\alpha \\,\\lVert \\tilde{\\mathbf{g}} \\rVert_{2}}, & \\text{if } \\lVert \\tilde{\\mathbf{g}} \\rVert_{2} > 0 \\\\\n0, & \\text{if } \\lVert \\tilde{\\mathbf{g}} \\rVert_{2} = 0\n\\end{cases}\n$$\nThe effective factor relative to the unclipped gradient, $r_{\\text{unclipped}}$, is defined as:\n$$\nr_{\\text{unclipped}} = \n\\begin{cases}\n\\dfrac{\\lVert \\Delta \\theta \\rVert_{2}}{\\alpha \\,\\lVert \\mathbf{g} \\rVert_{2}}, & \\text{if } \\lVert \\mathbf{g} \\rVert_{2} > 0 \\\\\n0, & \\text{if } \\lVert \\mathbf{g} \\rVert_{2} = 0\n\\end{cases}\n$$\nThese steps are applied to each test case to obtain the final results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Adam update metrics for a series of test cases.\n    \"\"\"\n    test_cases = [\n        # 1. d=3, g=[0.1,-0.2,0.05], m_prev=[0,0,0], v_prev=[0,0,0], t=1, alpha=0.01, beta1=0.9, beta2=0.999, eps=1e-8, c=1.0\n        {'g': np.array([0.1, -0.2, 0.05]), 'm_prev': np.array([0.0, 0.0, 0.0]), 'v_prev': np.array([0.0, 0.0, 0.0]), 't': 1, 'alpha': 0.01, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8, 'c': 1.0},\n        # 2. d=3, g=[10.0,0.0,0.0], m_prev=[0,0,0], v_prev=[0,0,0], t=1, alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-8, c=1.0\n        {'g': np.array([10.0, 0.0, 0.0]), 'm_prev': np.array([0.0, 0.0, 0.0]), 'v_prev': np.array([0.0, 0.0, 0.0]), 't': 1, 'alpha': 0.001, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8, 'c': 1.0},\n        # 3. d=3, g=[0.5,0.5,0.5], m_prev=[0.1,-0.1,0.0], v_prev=[0.04,0.01,0.09], t=10, alpha=0.005, beta1=0.8, beta2=0.9, eps=1e-6, c=0.5\n        {'g': np.array([0.5, 0.5, 0.5]), 'm_prev': np.array([0.1, -0.1, 0.0]), 'v_prev': np.array([0.04, 0.01, 0.09]), 't': 10, 'alpha': 0.005, 'beta1': 0.8, 'beta2': 0.9, 'epsilon': 1e-6, 'c': 0.5},\n        # 4. d=3, g=[0,0,0], m_prev=[0.01,-0.02,0], v_prev=[0.001,0.004,0.0009], t=5, alpha=0.01, beta1=0.9, beta2=0.99, eps=1e-8, c=0.1\n        {'g': np.array([0.0, 0.0, 0.0]), 'm_prev': np.array([0.01, -0.02, 0.0]), 'v_prev': np.array([0.001, 0.004, 0.0009]), 't': 5, 'alpha': 0.01, 'beta1': 0.9, 'beta2': 0.99, 'epsilon': 1e-8, 'c': 0.1},\n        # 5. d=3, g=[1e-4, -2e-4, 3e-4], m_prev=[0,0,0], v_prev=[0,0,0], t=1, alpha=1e-2, beta1=0.0, beta2=0.0, eps=1e-2, c=1e-3\n        {'g': np.array([1e-4, -2e-4, 3e-4]), 'm_prev': np.array([0.0, 0.0, 0.0]), 'v_prev': np.array([0.0, 0.0, 0.0]), 't': 1, 'alpha': 1e-2, 'beta1': 0.0, 'beta2': 0.0, 'epsilon': 1e-2, 'c': 1e-3},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        g, m_prev, v_prev = case['g'], case['m_prev'], case['v_prev']\n        t, alpha, beta1, beta2, epsilon, c = case['t'], case['alpha'], case['beta1'], case['beta2'], case['epsilon'], case['c']\n\n        # Step 1: Gradient Clipping\n        g_norm = np.linalg.norm(g)\n        \n        clipped = bool(g_norm > c)\n\n        if g_norm > 0:\n            clip_factor = min(1.0, c / g_norm)\n            g_tilde = g * clip_factor\n        else:\n            g_tilde = np.zeros_like(g)\n\n        # Step 2: Adam Moment Updates\n        m_t = beta1 * m_prev + (1 - beta1) * g_tilde\n        v_t = beta2 * v_prev + (1 - beta2) * (g_tilde ** 2)\n\n        # Step 3: Bias Correction\n        m_hat = m_t / (1 - beta1**t)\n        v_hat = v_t / (1 - beta2**t)\n        \n        # Step 4: Parameter Update Vector\n        delta_theta = alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n        \n        # Step 5: Final Metric Calculation\n        delta_theta_norm = np.linalg.norm(delta_theta)\n        g_tilde_norm = np.linalg.norm(g_tilde)\n\n        if g_tilde_norm > 0:\n            s_eff = delta_theta_norm / (alpha * g_tilde_norm)\n        else:\n            s_eff = 0.0\n\n        if g_norm > 0:\n            r_unclipped = delta_theta_norm / (alpha * g_norm)\n        else:\n            r_unclipped = 0.0\n            \n        all_results.append([s_eff, r_unclipped, clipped])\n\n    # Format the final output string exactly as required\n    result_strings = []\n    for res in all_results:\n        # Convert each item in the sublist to its string representation\n        # str(True) is 'True', str(False) is 'False', which is correct\n        s = '[' + ','.join(map(str, res)) + ']'\n        result_strings.append(s)\n    \n    final_output = '[' + ','.join(result_strings) + ']'\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond setting static hyperparameters, advanced tuning strategies can involve procedures that modify the training process itself. Stochastic Weight Averaging (SWA) is one such powerful technique that averages weights along the SGD trajectory to find flatter and more generalizable solutions. However, SWA introduces its own hyperparameters, such as the checkpointing frequency and averaging coefficient. This exercise  uses a simplified mathematical model of the training dynamics to derive how to best tune these parameters, offering a principled way to maximize the performance gain from SWA.",
            "id": "3135390",
            "problem": "You will investigate how the checkpoint averaging frequency and smoothing weight should be tuned for Stochastic Weight Averaging (SWA), formalized as follows. Consider a deep learning model trained with stochastic gradient descent near a local minimum where the loss can be approximated quadratically. Let the training iterate at checkpoint times be modeled as a stationary scalar process $\\{x_t\\}_{t \\ge 0}$ with zero mean and covariance function $\\mathrm{Cov}(x_t, x_{t-k}) = \\sigma^2 \\rho^{|k|}$, where $\\sigma^2 > 0$ and $\\rho \\in [0,1)$ capture the stationary variance and geometric autocorrelation across successive checkpoints. Checkpoints are saved every $F \\in \\mathbb{N}$ steps, and the correlation between successive saved checkpoints is modeled as $\\rho(F) = \\exp(-F / \\tau)$ for a fixed correlation timescale $\\tau > 0$. You construct an exponential Stochastic Weight Averaging (SWA) estimator using an Exponential Moving Average (EMA) with update $y_t = (1 - \\alpha) y_{t-1} + \\alpha x_t$, where $\\alpha \\in (0,1]$ is the averaging coefficient, and the update is applied only at checkpoint times. The special case $\\alpha = 1$ corresponds to using the most recent checkpoint with no averaging. Under a local quadratic approximation of the validation loss $L(w)$ around the minimizer, $L(w) \\approx L(w^\\star) + \\frac{1}{2} (w - w^\\star)^\\top H (w - w^\\star)$ with $H \\succ 0$, the expected excess validation loss of an estimator is proportional to the trace of the covariance of its error. In a scalar reduction with an effective curvature constant $c > 0$ (absorbing $\\frac{1}{2}$ and an effective curvature scale), the expected validation improvement from averaging, relative to the baseline of using a single checkpoint, is\n$\\Delta(F,\\alpha) = c \\left( \\sigma^2 - \\mathrm{Var}(y_t) \\right)$,\nwhere $\\mathrm{Var}(y_t)$ is the stationary variance of the EMA output at checkpoint times.\n\nTasks:\n1) Starting from the given definitions and the base facts that (i) the expected excess quadratic loss is proportional to the variance and (ii) the EMA is a linear time-invariant filter with impulse response weights $\\alpha (1-\\alpha)^k$ for $k \\in \\mathbb{N}\\cup\\{0\\}$, derive a closed-form expression for $\\mathrm{Var}(y_t)$ in terms of $\\sigma^2$, $\\alpha$, and $\\rho$, using only the covariance structure $\\mathrm{Cov}(x_t, x_{t-k}) = \\sigma^2 \\rho^{|k|}$ and the EMA definition $y_t = \\sum_{k=0}^{\\infty} \\alpha (1-\\alpha)^k x_{t-k}$ under stationarity. Then express $\\Delta(F,\\alpha)$ by substituting $\\rho = \\exp(-F/\\tau)$.\n2) Implement a program that computes $\\Delta(F,\\alpha)$ for each case in the test suite below, using the derived closed-form and the fixed constants $c = 0.5$, $\\sigma^2 = 1.0$, and $\\tau = 20.0$. All numerical constants are dimensionless. Round each result to $6$ decimal places.\n\nTest suite (each case is a pair $(F,\\alpha)$ to be evaluated in the order listed):\n- $(F,\\alpha) = (1, 0.1)$\n- $(F,\\alpha) = (10, 0.1)$\n- $(F,\\alpha) = (50, 0.1)$\n- $(F,\\alpha) = (15, 1.0)$\n- $(F,\\alpha) = (5, 0.05)$\n- $(F,\\alpha) = (5, 0.5)$\n- $(F,\\alpha) = (1, 0.9)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each value rounded to $6$ decimal places, for example, $[v_1,v_2,\\dots,v_7]$ where each $v_i$ is a float.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, and objective. It provides a self-contained, consistent set of definitions and constraints to derive a solution and perform the required calculations.\n\nThe primary task is to derive a closed-form expression for the expected validation improvement, $\\Delta(F,\\alpha)$, and then compute it for a given set of parameters. The improvement is defined as $\\Delta(F,\\alpha) = c \\left( \\sigma^2 - \\mathrm{Var}(y_t) \\right)$, where $y_t$ is the Stochastic Weight Averaging (SWA) estimator. This requires us to first find the stationary variance of the process $y_t$.\n\nThe process $\\{x_t\\}_{t \\ge 0}$ represents the model iterates at checkpoint times. It is a stationary scalar process with zero mean, i.e., $\\mathbb{E}[x_t] = 0$, and covariance function $\\mathrm{Cov}(x_t, x_j) = \\sigma^2 \\rho^{|t-j|}$. The variance of any single checkpoint iterate is thus $\\mathrm{Var}(x_t) = \\mathrm{Cov}(x_t, x_t) = \\sigma^2 \\rho^0 = \\sigma^2$.\n\nThe SWA estimator $y_t$ is formed by an Exponential Moving Average (EMA) of the checkpoint iterates: $y_t = (1 - \\alpha) y_{t-1} + \\alpha x_t$, with $\\alpha \\in (0,1]$. Since $x_t$ is a zero-mean stationary process and the EMA is a linear time-invariant filter, the output process $y_t$ is also a zero-mean stationary process, so $\\mathbb{E}[y_t] = 0$. The variance of $y_t$ is therefore $\\mathrm{Var}(y_t) = \\mathbb{E}[y_t^2]$.\n\nTo find $\\mathrm{Var}(y_t)$, we use the recursive definition of $y_t$.\n$$\ny_t^2 = \\left((1 - \\alpha) y_{t-1} + \\alpha x_t\\right)^2 = (1-\\alpha)^2 y_{t-1}^2 + \\alpha^2 x_t^2 + 2\\alpha(1-\\alpha) y_{t-1} x_t\n$$\nTaking the expectation of both sides:\n$$\n\\mathbb{E}[y_t^2] = (1-\\alpha)^2 \\mathbb{E}[y_{t-1}^2] + \\alpha^2 \\mathbb{E}[x_t^2] + 2\\alpha(1-\\alpha) \\mathbb{E}[y_{t-1} x_t]\n$$\nDue to stationarity, $\\mathrm{Var}(y_t) = \\mathbb{E}[y_t^2] = \\mathbb{E}[y_{t-1}^2]$. Let's denote this stationary variance as $V_y$. Also, $\\mathbb{E}[x_t^2] = \\mathrm{Var}(x_t) = \\sigma^2$. The equation becomes:\n$$\nV_y = (1-\\alpha)^2 V_y + \\alpha^2 \\sigma^2 + 2\\alpha(1-\\alpha) \\mathbb{E}[y_{t-1} x_t]\n$$\nWe need to compute the cross-correlation term $\\mathbb{E}[y_{t-1} x_t]$. We use the infinite sum representation of the EMA, $y_{t-1} = \\sum_{k=0}^{\\infty} \\alpha (1-\\alpha)^k x_{t-1-k}$.\n$$\n\\mathbb{E}[y_{t-1} x_t] = \\mathbb{E}\\left[ \\left( \\sum_{k=0}^{\\infty} \\alpha (1-\\alpha)^k x_{t-1-k} \\right) x_t \\right]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[y_{t-1} x_t] = \\sum_{k=0}^{\\infty} \\alpha (1-\\alpha)^k \\mathbb{E}[x_{t-1-k} x_t]\n$$\nSince $\\mathbb{E}[x_t]=0$, $\\mathbb{E}[x_{t-1-k} x_t] = \\mathrm{Cov}(x_{t-1-k}, x_t) = \\sigma^2 \\rho^{|t - (t-1-k)|} = \\sigma^2 \\rho^{|k+1|} = \\sigma^2 \\rho^{k+1}$, as $k \\ge 0$.\n$$\n\\mathbb{E}[y_{t-1} x_t] = \\sum_{k=0}^{\\infty} \\alpha (1-\\alpha)^k \\sigma^2 \\rho^{k+1} = \\alpha \\sigma^2 \\rho \\sum_{k=0}^{\\infty} \\left((1-\\alpha)\\rho\\right)^k\n$$\nThis is a geometric series with ratio $(1-\\alpha)\\rho$. Since $\\rho \\in [0,1)$ and $\\alpha \\in (0,1]$, we have $|(1-\\alpha)\\rho| < 1$. The sum converges to $\\frac{1}{1 - (1-\\alpha)\\rho}$.\n$$\n\\mathbb{E}[y_{t-1} x_t] = \\frac{\\alpha \\sigma^2 \\rho}{1 - (1-\\alpha)\\rho}\n$$\nNow, we substitute this back into the equation for $V_y$:\n$$\nV_y(1 - (1-\\alpha)^2) = \\alpha^2 \\sigma^2 + 2\\alpha(1-\\alpha) \\frac{\\alpha \\sigma^2 \\rho}{1 - (1-\\alpha)\\rho}\n$$\nThe left side is $V_y (1 - (1-2\\alpha+\\alpha^2)) = V_y (2\\alpha-\\alpha^2) = V_y \\alpha(2-\\alpha)$.\nThe right side is $\\alpha^2 \\sigma^2 \\left( 1 + \\frac{2(1-\\alpha)\\rho}{1 - (1-\\alpha)\\rho} \\right) = \\alpha^2 \\sigma^2 \\left( \\frac{1 - (1-\\alpha)\\rho + 2(1-\\alpha)\\rho}{1 - (1-\\alpha)\\rho} \\right) = \\alpha^2 \\sigma^2 \\frac{1+(1-\\alpha)\\rho}{1 - (1-\\alpha)\\rho}$.\nEquating the two sides:\n$$\nV_y \\alpha(2-\\alpha) = \\alpha^2 \\sigma^2 \\frac{1+(1-\\alpha)\\rho}{1 - (1-\\alpha)\\rho}\n$$\nFor $\\alpha \\neq 0$, we can divide by $\\alpha$:\n$$\nV_y (2-\\alpha) = \\alpha \\sigma^2 \\frac{1+(1-\\alpha)\\rho}{1 - (1-\\alpha)\\rho}\n$$\nSolving for $V_y = \\mathrm{Var}(y_t)$:\n$$\n\\mathrm{Var}(y_t) = \\sigma^2 \\frac{\\alpha}{2-\\alpha} \\frac{1+(1-\\alpha)\\rho}{1-(1-\\alpha)\\rho}\n$$\nThis is the closed-form expression for the variance of the SWA estimator.\n\nThe validation improvement is $\\Delta(F,\\alpha) = c \\left( \\sigma^2 - \\mathrm{Var}(y_t) \\right)$. Substituting the expression for $\\mathrm{Var}(y_t)$:\n$$\n\\Delta(F,\\alpha) = c \\sigma^2 \\left( 1 - \\frac{\\alpha}{2-\\alpha} \\frac{1+(1-\\alpha)\\rho}{1-(1-\\alpha)\\rho} \\right)\n$$\nWe simplify the term in the parenthesis:\n$$\n1 - \\frac{\\alpha(1+(1-\\alpha)\\rho)}{(2-\\alpha)(1-(1-\\alpha)\\rho)} = \\frac{(2-\\alpha)(1-(1-\\alpha)\\rho) - \\alpha(1+(1-\\alpha)\\rho)}{(2-\\alpha)(1-(1-\\alpha)\\rho)}\n$$\nThe numerator expands to $2 - (2-\\alpha)(1-\\alpha)\\rho - \\alpha - \\alpha(1-\\alpha)\\rho - (2-\\alpha) = 2 - 2\\alpha - ((2-\\alpha)(1-\\alpha) + \\alpha(1-\\alpha))\\rho = 2(1-\\alpha) - (1-\\alpha)(2-\\alpha+\\alpha)\\rho = 2(1-\\alpha) - 2(1-\\alpha)\\rho = 2(1-\\alpha)(1-\\rho)$.\nSo the parenthesis simplifies to $\\frac{2(1-\\alpha)(1-\\rho)}{(2-\\alpha)(1-(1-\\alpha)\\rho)}$.\nThe full expression for $\\Delta(F,\\alpha)$ becomes:\n$$\n\\Delta(F,\\alpha) = c \\sigma^2 \\frac{2(1-\\alpha)(1-\\rho)}{(2-\\alpha)(1-(1-\\alpha)\\rho)}\n$$\nFinally, we substitute the given model for the correlation $\\rho = \\exp(-F/\\tau)$:\n$$\n\\Delta(F,\\alpha) = c \\sigma^2 \\frac{2(1-\\alpha)(1 - e^{-F/\\tau})}{(2-\\alpha)(1-(1-\\alpha)e^{-F/\\tau})}\n$$\nThis is the final closed-form expression used for computation. If $\\alpha=1$, the numerator is $0$, so $\\Delta(F,1)=0$, which is correct as no averaging is performed.\n\nThe computation will use the provided constants $c=0.5$, $\\sigma^2=1.0$, and $\\tau=20.0$.\nThus, the formula for calculation is:\n$$\n\\Delta(F,\\alpha) = 0.5 \\cdot \\frac{2(1-\\alpha)(1 - e^{-F/20.0})}{(2-\\alpha)(1-(1-\\alpha)e^{-F/20.0})} = \\frac{(1-\\alpha)(1 - e^{-F/20.0})}{(2-\\alpha)(1-(1-\\alpha)e^{-F/20.0})}\n$$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the expected validation improvement from Stochastic Weight Averaging (SWA)\n    for a given set of test cases.\n    \"\"\"\n    # Define the fixed constants from the problem statement.\n    # c: effective curvature constant\n    # sigma2: stationary variance of the base process\n    # tau: correlation timescale\n    c = 0.5\n    sigma2 = 1.0\n    tau = 20.0\n\n    # Define the test cases from the problem statement as a list of (F, alpha) tuples.\n    # F: checkpointing frequency\n    # alpha: EMA averaging coefficient\n    test_cases = [\n        (1, 0.1),\n        (10, 0.1),\n        (50, 0.1),\n        (15, 1.0),\n        (5, 0.05),\n        (5, 0.5),\n        (1, 0.9),\n    ]\n\n    results = []\n    \n    # The derived formula for the validation improvement Delta(F, alpha) is:\n    # Delta(F, alpha) = c * sigma^2 * [2 * (1-alpha) * (1 - exp(-F/tau))] / \n    #                   [(2-alpha) * (1 - (1-alpha) * exp(-F/tau))]\n    \n    for F, alpha in test_cases:\n        # The case alpha = 1 implies no averaging, so the improvement is 0.\n        # The formula correctly handles this since (1-alpha) becomes 0.\n        if alpha == 1.0:\n            delta_val = 0.0\n        else:\n            # Calculate the correlation rho based on checkpoint frequency F and timescale tau\n            rho_F = np.exp(-F / tau)\n            \n            # Calculate numerator of the derived fraction\n            numerator = 2 * (1 - alpha) * (1 - rho_F)\n            \n            # Calculate denominator of the derived fraction\n            denominator = (2 - alpha) * (1 - (1 - alpha) * rho_F)\n            \n            # Denominator is guaranteed to be non-zero for alpha in (0, 1] and F > 0.\n            \n            # Calculate the final improvement Delta\n            delta_val = c * sigma2 * numerator / denominator\n            \n        # Format the result to 6 decimal places and add to the list.\n        # Using f-string formatting ensures exactly 6 decimal places, avoiding\n        # potential floating point representation issues with round().\n        results.append(f\"{delta_val:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}