{
    "hands_on_practices": [
        {
            "introduction": "Tuning hyperparameters often feels like navigating a high-dimensional space where every choice affects the others. This exercise isolates a crucial interaction: the one between learning rate, $\\alpha$, and weight decay, $\\lambda$. By deriving a simple scaling law from the update rules of common optimizers, you will discover a principled way to co-adapt these two hyperparameters, ensuring that the effective regularization strength remains constant even as you experiment with different learning rates. ",
            "id": "3135392",
            "problem": "You are asked to formalize and test a hyperparameter scaling heuristic for weight decay in gradient-based optimization of deep learning models. The goal is to keep the effective step regularization constant when the learning rate changes. Proceed from core definitions of gradient-based updates and Euclidean norm regularization, and avoid any shortcut formulas that jump directly to the result.\n\nLet $w_t \\in \\mathbb{R}$ denote a single scalar parameter at step $t$, let $\\alpha \\in \\mathbb{R}_{\\ge 0}$ denote the learning rate, let $\\lambda \\in \\mathbb{R}_{\\ge 0}$ denote the weight decay or Euclidean norm (L2) regularization coefficient, and let $g_t \\in \\mathbb{R}$ denote the data-dependent gradient of the loss with respect to $w_t$ when no regularization is applied. Assume one of the following well-tested update rules:\n- Stochastic Gradient Descent (SGD) without momentum: $w_{t+1} = w_t - \\alpha \\nabla_w \\mathcal{L}(w_t)$ with Euclidean norm (L2) regularization included in the loss, so the gradient includes an additive term dependent on $\\lambda$ and $w_t$.\n- Momentum variant of Stochastic Gradient Descent (SGD): a velocity $v_t \\in \\mathbb{R}$ is used and updated by a linear recurrence with coefficient $\\beta \\in [0,1)$, together with the same treatment of Euclidean norm (L2) regularization as above.\n- Adaptive Moment Estimation with decoupled weight decay (AdamW): the first and second moments of the data gradient are maintained with coefficients $\\beta_1, \\beta_2 \\in [0,1)$ and the Euclidean norm (L2) penalty is applied as a multiplicative decay that is explicitly separated from the data gradient pathway.\n\nDefine the per-step effective regularization, for a given optimizer, learning rate, and weight decay, at a state where the data-driven gradient is absent, by the multiplicative shrinkage factor\n$$\ns(\\alpha, \\lambda; w_t) \\equiv \\frac{|w_{t+1}|}{|w_t|},\n$$\nwhen the update is executed with $g_t = 0$ and any internal state (such as velocity or moments) initialized to zero at step $t$. This definition isolates the influence of the regularization mechanism on a single step and ignores any data-driven change.\n\nTask A (derivation): Using only the described update rules and the definition of Euclidean norm (L2) regularization and decoupled weight decay, derive a condition relating $\\lambda$ and $\\alpha$ that keeps $s(\\alpha, \\lambda; w_t)$ invariant when $\\alpha$ changes. Your derivation must be based on the fundamental step update dynamics and should not rely on any undocumented identities.\n\nTask B (program): Implement a program that evaluates two candidate scaling laws for $\\lambda$ as $\\alpha$ changes, given a baseline $(\\alpha_0, \\lambda_0)$:\n1. Proportional scaling: $\\lambda_{\\mathrm{prop}}(\\alpha) = \\lambda_0 \\cdot \\frac{\\alpha}{\\alpha_0}$.\n2. Inverse-proportional scaling: $\\lambda_{\\mathrm{inv}}(\\alpha) = \\lambda_0 \\cdot \\frac{\\alpha_0}{\\alpha}$.\n\nFor each test case, do the following:\n1. Compute the baseline shrinkage $s_0 = s(\\alpha_0, \\lambda_0; w_0)$ at a given nonzero initial weight $w_0$ with $g_t=0$ and any internal optimizer state initialized to zero.\n2. For each $\\alpha$ in the case’s list of learning rates, compute $s_{\\mathrm{prop}}(\\alpha) = s(\\alpha, \\lambda_{\\mathrm{prop}}(\\alpha); w_0)$ and $s_{\\mathrm{inv}}(\\alpha) = s(\\alpha, \\lambda_{\\mathrm{inv}}(\\alpha); w_0)$ under the same initial conditions, using the given optimizer’s exact one-step update rule.\n3. Return two booleans per test case:\n   - The first boolean is true if $|s_{\\mathrm{inv}}(\\alpha) - s_0| \\le \\varepsilon$ for all $\\alpha$ in the list.\n   - The second boolean is true if $|s_{\\mathrm{prop}}(\\alpha) - s_0| \\le \\varepsilon$ for all $\\alpha$ in the list.\nHere $\\varepsilon  0$ is a given tolerance for numerical comparison.\n\nThe test suite you must implement consists of the following five cases, each providing a baseline $(\\alpha_0, \\lambda_0)$, a nonzero initial weight $w_0$, a set of learning rates to test, and an optimizer with its coefficients. In all cases, evaluate a single update with $g_t = 0$ and zeroed internal state at step $t$.\n- Case 1 (happy path, coupled regularization in loss): Optimizer is SGD without momentum, baseline $\\alpha_0 = 0.1$, $\\lambda_0 = 0.01$, $w_0 = 1.0$, test learning rates $\\{0.05, 0.1, 0.2\\}$, tolerance $\\varepsilon = 10^{-12}$.\n- Case 2 (momentum, coupled regularization in loss): Optimizer is momentum SGD with $\\beta = 0.9$, baseline $\\alpha_0 = 0.1$, $\\lambda_0 = 0.01$, $w_0 = 1.0$, test learning rates $\\{0.05, 0.1, 0.2\\}$, tolerance $\\varepsilon = 10^{-12}$.\n- Case 3 (decoupled weight decay): Optimizer is AdamW with $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\varepsilon_{\\mathrm{adam}} = 10^{-8}$, baseline $\\alpha_0 = 0.001$, $\\lambda_0 = 0.01$, $w_0 = 1.0$, test learning rates $\\{0.0005, 0.001, 0.002\\}$, tolerance $\\varepsilon = 10^{-12}$.\n- Case 4 (edge case: zero weight decay): Optimizer is SGD without momentum, baseline $\\alpha_0 = 0.05$, $\\lambda_0 = 0.0$, $w_0 = 1.0$, test learning rates $\\{0.01, 0.05, 0.1\\}$, tolerance $\\varepsilon = 10^{-12}$.\n- Case 5 (coverage across scales): Optimizer is AdamW with $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\varepsilon_{\\mathrm{adam}} = 10^{-8}$, baseline $\\alpha_0 = 10^{-2}$, $\\lambda_0 = 5 \\cdot 10^{-3}$, $w_0 = 1.0$, test learning rates $\\{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\\}$, tolerance $\\varepsilon = 10^{-12}$.\n\nRequired final output format: Your program should produce a single line of output containing a flat Python list of booleans with exactly two booleans per test case, in the order of the cases above. For each case, append first the boolean for inverse-proportional scaling and then the boolean for proportional scaling. For example, a five-case run must print a single line in the exact format: \"[b1_inv,b1_prop,b2_inv,b2_prop,b3_inv,b3_prop,b4_inv,b4_prop,b5_inv,b5_prop]\".",
            "solution": "The task is to derive a hyperparameter scaling heuristic for weight decay and then programmatically test it. The goal is to maintain a constant effective regularization strength when the learning rate changes. The effective regularization for a single step is defined by the multiplicative shrinkage factor $s(\\alpha, \\lambda; w_t) \\equiv \\frac{|w_{t+1}|}{|w_t|}$, computed under the specific conditions of a zero data-dependent gradient ($g_t=0$) and zeroed internal optimizer states at step $t$. Let $w_t \\in \\mathbb{R}$ be a scalar weight, $\\alpha \\in \\mathbb{R}_{\\ge 0}$ the learning rate, and $\\lambda \\in \\mathbb{R}_{\\ge 0}$ the weight decay coefficient. We assume $w_t \\neq 0$.\n\n**Task A: Derivation of the Invariance Condition**\n\nWe will analyze the one-step update rule for each specified optimizer under the given conditions ($g_t=0$, zero initial state) to derive the relationship between $\\alpha$ and $\\lambda$ that keeps $s(\\alpha, \\lambda; w_t)$ constant.\n\n**1. Stochastic Gradient Descent (SGD) without Momentum**\n\nThe total loss function with Euclidean norm (L2) regularization is $\\mathcal{L}_{\\text{total}}(w_t) = \\mathcal{L}(w_t) + \\frac{\\lambda}{2} w_t^2$. The gradient with respect to $w_t$ is $\\nabla_w \\mathcal{L}_{\\text{total}}(w_t) = \\nabla_w \\mathcal{L}(w_t) + \\lambda w_t$. Let $g_t = \\nabla_w \\mathcal{L}(w_t)$ be the data-dependent part of the gradient. The full gradient is then $g_t + \\lambda w_t$.\n\nThe SGD update rule is:\n$$w_{t+1} = w_t - \\alpha \\nabla_w \\mathcal{L}_{\\text{total}}(w_t) = w_t - \\alpha (g_t + \\lambda w_t)$$\nUnder the condition $g_t=0$, the update simplifies to:\n$$w_{t+1} = w_t - \\alpha (\\lambda w_t) = (1 - \\alpha \\lambda) w_t$$\nThe shrinkage factor $s$ is therefore:\n$$s(\\alpha, \\lambda; w_t) = \\frac{|w_{t+1}|}{|w_t|} = \\frac{|(1 - \\alpha \\lambda) w_t|}{|w_t|} = |1 - \\alpha \\lambda|$$\nFor $s(\\alpha, \\lambda; w_t)$ to be invariant to changes in $\\alpha$, the value of $|1 - \\alpha \\lambda|$ must remain constant. Assuming that the update is stable (i.e., $1 - \\alpha \\lambda \\ge 0$), this implies that the product $\\alpha \\lambda$ must be constant.\n$$\\alpha \\lambda = C$$\nwhere $C$ is a constant. Given a baseline pair $(\\alpha_0, \\lambda_0)$, the constant is $C = \\alpha_0 \\lambda_0$. Thus, for any new learning rate $\\alpha$, the corresponding weight decay $\\lambda$ must satisfy $\\lambda = \\frac{\\alpha_0 \\lambda_0}{\\alpha}$. This is the inverse-proportional scaling law.\n\n**2. Momentum SGD**\n\nThe optimizer maintains a velocity vector $v_t$. The update rules are given for the total gradient $\\nabla_w \\mathcal{L}_{\\text{total}}(w_t) = g_t + \\lambda w_t$:\n$$v_{t+1} = \\beta v_t + (g_t + \\lambda w_t)$$\n$$w_{t+1} = w_t - \\alpha v_{t+1}$$\nwhere $\\beta \\in [0, 1)$ is the momentum coefficient.\nWe apply the conditions: $g_t=0$ and the internal state at step $t$ is zero, which means $v_t=0$.\nThe velocity update becomes:\n$$v_{t+1} = \\beta(0) + (0 + \\lambda w_t) = \\lambda w_t$$\nSubstituting this into the parameter update rule:\n$$w_{t+1} = w_t - \\alpha (\\lambda w_t) = (1 - \\alpha \\lambda) w_t$$\nThis is identical to the SGD case. The shrinkage factor is again $s(\\alpha, \\lambda; w_t) = |1 - \\alpha \\lambda|$, and the invariance condition is $\\alpha \\lambda = \\text{constant}$. This again leads to the inverse-proportional scaling law.\n\n**3. AdamW (Decoupled Weight Decay)**\n\nIn AdamW, the weight decay is \"decoupled\" from the gradient-based update. The update is performed in two conceptual steps: first applying the weight decay, then applying the Adam update based on the data gradient $g_t$. A common implementation form is:\n$$w_{t+1} = w_t (1 - \\alpha \\lambda) - \\alpha \\cdot \\text{AdamUpdate}(g_t, m_t, v_t)$$\nThe `AdamUpdate` term depends on the moment estimates:\n$$m_{t+1} = \\beta_1 m_t + (1-\\beta_1) g_t$$\n$$v_{t+1} = \\beta_2 v_t + (1-\\beta_2) g_t^2$$\nAnd their bias-corrected versions, $\\hat{m}_{t+1}$ and $\\hat{v}_{t+1}$. The update term is proportional to $\\frac{\\hat{m}_{t+1}}{\\sqrt{\\hat{v}_{t+1}} + \\varepsilon_{\\text{adam}}}$.\n\nUnder the given conditions, $g_t=0$, and the internal states at step $t$ are zero ($m_t=0, v_t=0$).\nThe moment updates yield:\n$$m_{t+1} = \\beta_1(0) + (1-\\beta_1)(0) = 0$$\n$$v_{t+1} = \\beta_2(0) + (1-\\beta_2)(0^2) = 0$$\nConsequently, their bias-corrected versions are also zero, $\\hat{m}_{t+1}=0$ and $\\hat{v}_{t+1}=0$. The entire `AdamUpdate` term becomes zero:\n$$\\alpha \\cdot \\text{AdamUpdate}(g_t=0, m_t=0, v_t=0) = 0$$\nThe AdamW update rule simplifies to:\n$$w_{t+1} = w_t (1 - \\alpha \\lambda) - 0 = (1 - \\alpha \\lambda) w_t$$\nOnce again, the update becomes identical to the previous cases under the specified conditions. The shrinkage factor is $s(\\alpha, \\lambda; w_t) = |1 - \\alpha \\lambda|$, and the invariance condition is $\\alpha \\lambda = \\text{constant}$.\n\n**Conclusion of Derivation**\n\nFor all three optimizers—SGD with coupled regularization, Momentum SGD with coupled regularization, and AdamW with decoupled weight decay—the effective single-step regularization shrinkage factor $s(\\alpha, \\lambda; w_t)$ is invariant to changes in $\\alpha$ if and only if the product $\\alpha \\lambda$ is held constant. This corresponds to the **inverse-proportional scaling law**: $\\lambda(\\alpha) = \\lambda_0 \\cdot \\frac{\\alpha_0}{\\alpha}$. The proportional scaling law, $\\lambda(\\alpha) = \\lambda_0 \\cdot \\frac{\\alpha}{\\alpha_0}$, would cause the product $\\alpha \\lambda$ to scale with $\\alpha^2$, thus failing to keep the shrinkage factor constant.\n\nThe following program will numerically validate this conclusion based on the test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef update_sgd(w_t, alpha, lambda_val, **kwargs):\n    \"\"\"\n    Performs a single update step for SGD with coupled L2 regularization.\n    Conditions: g_t = 0, no internal state.\n    \"\"\"\n    g_t = 0.0\n    total_gradient = g_t + lambda_val * w_t\n    w_t_plus_1 = w_t - alpha * total_gradient\n    return w_t_plus_1\n\ndef update_momentum_sgd(w_t, alpha, lambda_val, beta, **kwargs):\n    \"\"\"\n    Performs a single update step for Momentum SGD with coupled L2 regularization.\n    Conditions: g_t = 0, velocity v_t = 0.\n    \"\"\"\n    g_t = 0.0\n    v_t = 0.0  # Zero initial state\n    total_gradient = g_t + lambda_val * w_t\n    v_t_plus_1 = beta * v_t + total_gradient\n    w_t_plus_1 = w_t - alpha * v_t_plus_1\n    return w_t_plus_1\n\ndef update_adamw(w_t, alpha, lambda_val, beta1, beta2, epsilon_adam, **kwargs):\n    \"\"\"\n    Performs a single update step for AdamW with decoupled weight decay.\n    Conditions: g_t = 0, moments m_t = 0, v_t = 0.\n    \"\"\"\n    g_t = 0.0\n    m_t = 0.0  # Zero initial state\n    v_t = 0.0  # Zero initial state\n    step = 0  # Simulation starts at step 0\n    step_plus_1 = step + 1\n\n    # Gradient processing part, which becomes zero under g_t=0 and zeroed state\n    m_t_plus_1 = beta1 * m_t + (1.0 - beta1) * g_t\n    v_t_plus_1 = beta2 * v_t + (1.0 - beta2) * g_t**2\n\n    # Bias correction. 1 - beta**(step+1) is safe since step+1=1.\n    m_hat = m_t_plus_1 / (1.0 - beta1**step_plus_1)\n    v_hat = v_t_plus_1 / (1.0 - beta2**step_plus_1)\n\n    grad_update_term = m_hat / (np.sqrt(v_hat) + epsilon_adam)\n\n    # Decoupled weight decay update\n    w_t_plus_1 = w_t * (1.0 - alpha * lambda_val) - alpha * grad_update_term\n    return w_t_plus_1\n\ndef solve():\n    \"\"\"\n    Solves the problem by running all test cases and printing the results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, coupled regularization in loss)\n        {'optimizer': 'SGD', 'alpha0': 0.1, 'lambda0': 0.01, 'w0': 1.0, \n         'test_alphas': [0.05, 0.1, 0.2], 'epsilon': 1e-12, 'params': {}},\n        # Case 2 (momentum, coupled regularization in loss)\n        {'optimizer': 'MomentumSGD', 'alpha0': 0.1, 'lambda0': 0.01, 'w0': 1.0, \n         'test_alphas': [0.05, 0.1, 0.2], 'epsilon': 1e-12, 'params': {'beta': 0.9}},\n        # Case 3 (decoupled weight decay)\n        {'optimizer': 'AdamW', 'alpha0': 0.001, 'lambda0': 0.01, 'w0': 1.0, \n         'test_alphas': [0.0005, 0.001, 0.002], 'epsilon': 1e-12, \n         'params': {'beta1': 0.9, 'beta2': 0.999, 'epsilon_adam': 1e-8}},\n        # Case 4 (edge case: zero weight decay)\n        {'optimizer': 'SGD', 'alpha0': 0.05, 'lambda0': 0.0, 'w0': 1.0, \n         'test_alphas': [0.01, 0.05, 0.1], 'epsilon': 1e-12, 'params': {}},\n        # Case 5 (coverage across scales)\n        {'optimizer': 'AdamW', 'alpha0': 1e-2, 'lambda0': 5e-3, 'w0': 1.0, \n         'test_alphas': [1e-4, 1e-3, 1e-2, 1e-1], 'epsilon': 1e-12, \n         'params': {'beta1': 0.9, 'beta2': 0.999, 'epsilon_adam': 1e-8}}\n    ]\n    \n    optimizer_updates = {\n        'SGD': update_sgd,\n        'MomentumSGD': update_momentum_sgd,\n        'AdamW': update_adamw\n    }\n\n    final_results = []\n    for case in test_cases:\n        alpha0 = case['alpha0']\n        lambda0 = case['lambda0']\n        w0 = case['w0']\n        epsilon = case['epsilon']\n        update_func = optimizer_updates[case['optimizer']]\n\n        # 1. Compute baseline shrinkage s0\n        w1_base = update_func(w0, alpha0, lambda0, **case['params'])\n        if w0 == 0.0:\n            s0 = 0.0 if w1_base == 0.0 else float('inf')\n        else:\n            s0 = abs(w1_base / w0)\n\n        is_inv_stable = True\n        is_prop_stable = True\n\n        for alpha in case['test_alphas']:\n            # 2. Test inverse-proportional scaling\n            # Handle alpha=0 case, though not in test data.\n            lambda_inv = lambda0 * (alpha0 / alpha) if alpha != 0.0 else float('inf')\n            w1_inv = update_func(w0, alpha, lambda_inv, **case['params'])\n            s_inv = abs(w1_inv / w0) if w0 != 0 else (0.0 if w1_inv == 0.0 else float('inf'))\n            if abs(s_inv - s0)  epsilon:\n                is_inv_stable = False\n\n            # 3. Test proportional scaling\n            # Handle alpha0=0 case, though not in test data.\n            if alpha0 != 0.0:\n                lambda_prop = lambda0 * (alpha / alpha0)\n            else:\n                 lambda_prop = 0.0 if lambda0 == 0.0 else float('inf')\n            w1_prop = update_func(w0, alpha, lambda_prop, **case['params'])\n            s_prop = abs(w1_prop / w0) if w0 != 0.0 else (0.0 if w1_prop == 0.0 else float('inf'))\n            if abs(s_prop - s0)  epsilon:\n                is_prop_stable = False\n        \n        final_results.append(is_inv_stable)\n        final_results.append(is_prop_stable)\n    \n    # Format the final output string exactly as required.\n    # str(True) -> 'True', str(False) -> 'False'\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Modern optimizers like Adam combine multiple mechanisms, and understanding their interplay is key to effective tuning. This practice moves beyond high-level principles to dissect the inner workings of the Adam optimizer when combined with gradient clipping. You will implement a single update step to calculate an 'effective step normalization factor,' providing a concrete, quantitative measure of how clipping and adaptive moment estimation jointly determine the final update size. ",
            "id": "3135417",
            "problem": "You are asked to formalize and quantify the interplay between gradient clipping by global norm and adaptive moment estimation in the Adam optimizer through the lens of an effective step normalization factor. Work from fundamental, widely tested definitions of gradient-based optimization, exponential moving averages, and global norm clipping.\n\nGiven a parameter vector and its gradient in a deep learning model, define the global norm clipping of the gradient as follows. For a gradient vector $\\mathbf{g} \\in \\mathbb{R}^{d}$ and a clip threshold $c \\in \\mathbb{R}_{0}$, the clipped gradient $\\tilde{\\mathbf{g}}$ is\n$$\n\\tilde{\\mathbf{g}} \\;=\\; \\mathbf{g} \\cdot \\min\\!\\left(1, \\frac{c}{\\lVert \\mathbf{g} \\rVert_{2}}\\right),\n$$\nwith the convention that if $\\lVert \\mathbf{g} \\rVert_{2} = 0$ then $\\tilde{\\mathbf{g}} = \\mathbf{0}$.\n\nIn the Adam optimizer, define the exponential moving averages of the first and second moments for time step $t \\in \\mathbb{N}$ with hyperparameters $\\beta_{1} \\in [0,1)$, $\\beta_{2} \\in [0,1)$:\n- The first-moment estimate $m_{t}$ is an exponential moving average of $\\tilde{\\mathbf{g}}$ with decay $\\beta_{1}$.\n- The second-moment estimate $v_{t}$ is an exponential moving average of the elementwise square of $\\tilde{\\mathbf{g}}$ with decay $\\beta_{2}$.\nImplement bias corrections to obtain $\\hat{m}_{t}$ and $\\hat{v}_{t}$ using the standard bias-correction factors determined by $t$, $\\beta_{1}$, and $\\beta_{2}$. The elementwise parameter update is formed by normalizing $\\hat{m}_{t}$ by the square root of $\\hat{v}_{t}$ regularized by $\\varepsilon \\in \\mathbb{R}_{0}$, and then scaled by the learning rate $\\alpha \\in \\mathbb{R}_{0}$. The update vector is denoted $\\Delta \\theta \\in \\mathbb{R}^{d}$.\n\nDefine the effective step normalization factor relative to a plain stochastic gradient descent step on the clipped gradient as\n$$\ns_{\\text{eff}} \\;=\\; \n\\begin{cases}\n\\dfrac{\\lVert \\Delta \\theta \\rVert_{2}}{\\alpha \\,\\lVert \\tilde{\\mathbf{g}} \\rVert_{2}},  \\text{if } \\lVert \\tilde{\\mathbf{g}} \\rVert_{2}  0,\\\\\n0,  \\text{if } \\lVert \\tilde{\\mathbf{g}} \\rVert_{2} = 0,\n\\end{cases}\n$$\nand, analogously, define the effective factor relative to the unclipped gradient as\n$$\nr_{\\text{unclipped}} \\;=\\;\n\\begin{cases}\n\\dfrac{\\lVert \\Delta \\theta \\rVert_{2}}{\\alpha \\,\\lVert \\mathbf{g} \\rVert_{2}},  \\text{if } \\lVert \\mathbf{g} \\rVert_{2}  0,\\\\\n0,  \\text{if } \\lVert \\mathbf{g} \\rVert_{2} = 0.\n\\end{cases}\n$$\nAlso define the clipping indicator\n$$\n\\text{clipped} \\;=\\; \\big(\\lVert \\mathbf{g} \\rVert_{2}  c\\big).\n$$\n\nYour task is to:\n- Starting from the above base definitions, derive the one-step Adam update $\\Delta \\theta$ given inputs $\\mathbf{g}$, $m_{t-1}$, $v_{t-1}$, $t$, and hyperparameters $\\alpha$, $\\beta_{1}$, $\\beta_{2}$, $\\varepsilon$, and $c$.\n- Compute $s_{\\text{eff}}$, $r_{\\text{unclipped}}$, and $\\text{clipped}$ for each test case below.\n\nTest suite. For each case, all vectors are in $\\mathbb{R}^{3}$, and numbers are in standard decimal or scientific notation:\n1. $d = 3$, $\\mathbf{g} = [\\,0.1,\\,-0.2,\\,0.05\\,]$, $m_{t-1} = [\\,0,\\,0,\\,0\\,]$, $v_{t-1} = [\\,0,\\,0,\\,0\\,]$, $t = 1$, $\\alpha = 0.01$, $\\beta_{1} = 0.9$, $\\beta_{2} = 0.999$, $\\varepsilon = 10^{-8}$, $c = 1.0$.\n2. $d = 3$, $\\mathbf{g} = [\\,10.0,\\,0.0,\\,0.0\\,]$, $m_{t-1} = [\\,0,\\,0,\\,0\\,]$, $v_{t-1} = [\\,0,\\,0,\\,0\\,]$, $t = 1$, $\\alpha = 0.001$, $\\beta_{1} = 0.9$, $\\beta_{2} = 0.999$, $\\varepsilon = 10^{-8}$, $c = 1.0$.\n3. $d = 3$, $\\mathbf{g} = [\\,0.5,\\,0.5,\\,0.5\\,]$, $m_{t-1} = [\\,0.1,\\,-0.1,\\,0.0\\,]$, $v_{t-1} = [\\,0.04,\\,0.01,\\,0.09\\,]$, $t = 10$, $\\alpha = 0.005$, $\\beta_{1} = 0.8$, $\\beta_{2} = 0.9$, $\\varepsilon = 10^{-6}$, $c = 0.5$.\n4. $d = 3$, $\\mathbf{g} = [\\,0.0,\\,0.0,\\,0.0\\,]$, $m_{t-1} = [\\,0.01,\\,-0.02,\\,0.0\\,]$, $v_{t-1} = [\\,0.001,\\,0.004,\\,0.0009\\,]$, $t = 5$, $\\alpha = 0.01$, $\\beta_{1} = 0.9$, $\\beta_{2} = 0.99$, $\\varepsilon = 10^{-8}$, $c = 0.1$.\n5. $d = 3$, $\\mathbf{g} = [\\,10^{-4},\\,-2\\cdot 10^{-4},\\,3\\cdot 10^{-4}\\,]$, $m_{t-1} = [\\,0,\\,0,\\,0\\,]$, $v_{t-1} = [\\,0,\\,0,\\,0\\,]$, $t = 1$, $\\alpha = 10^{-2}$, $\\beta_{1} = 0.0$, $\\beta_{2} = 0.0$, $\\varepsilon = 10^{-2}$, $c = 10^{-3}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a list of the form $[\\,s_{\\text{eff}},\\,r_{\\text{unclipped}},\\,\\text{clipped}\\,]$, in that order. The final output must therefore look like\n[[case1_values],[case2_values],...,[case5_values]]\nwith no spaces anywhere in the line. All floating-point values must be printed in standard decimal notation or scientific notation as produced by typical programming-language default conversions, and the boolean must be either True or False. No user input is required, and no physical units or angular measures are involved in this problem. All computations must be performed exactly as specified above, and the ratios must use the conventions for zero denominators specified in the definitions of $s_{\\text{eff}}$ and $r_{\\text{unclipped}}$.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and provides all necessary information for a unique solution. It is based on standard, formalizable definitions from the field of deep learning optimization.\n\nThe task is to compute three quantities for several test cases: the effective step normalization factor relative to the clipped gradient ($s_{\\text{eff}}$), the effective step normalization factor relative to the unclipped gradient ($r_{\\text{unclipped}}$), and a boolean clipping indicator ($\\text{clipped}$). This requires implementing a single step of the Adam optimizer with gradient clipping. The procedure is as follows:\n\n### Step 1: Gradient Clipping\nFirst, we compute the L$2$-norm of the raw gradient vector $\\mathbf{g} \\in \\mathbb{R}^{d}$, denoted as $\\lVert \\mathbf{g} \\rVert_{2}$.\n$$\n\\lVert \\mathbf{g} \\rVert_{2} = \\sqrt{\\sum_{i=1}^{d} g_i^2}\n$$\nThe clipping indicator, $\\text{clipped}$, is a boolean value determined by comparing this norm to the clipping threshold $c \\in \\mathbb{R}_{0}$:\n$$\n\\text{clipped} = (\\lVert \\mathbf{g} \\rVert_{2}  c)\n$$\nThe gradient $\\mathbf{g}$ is then clipped to produce $\\tilde{\\mathbf{g}}$. If the norm of $\\mathbf{g}$ exceeds $c$, the gradient vector is scaled down to have a norm of $c$. Otherwise, it remains unchanged. This is expressed by the formula:\n$$\n\\tilde{\\mathbf{g}} = \\mathbf{g} \\cdot \\min\\left(1, \\frac{c}{\\lVert \\mathbf{g} \\rVert_{2}}\\right)\n$$\nA special convention is given for a zero gradient: if $\\lVert \\mathbf{g} \\rVert_{2} = 0$, then $\\tilde{\\mathbf{g}} = \\mathbf{0}$. This is naturally handled by the multiplicative nature of the clipping formula if we define the scaling factor to be $1$ when $\\lVert \\mathbf{g} \\rVert_{2}=0$ to avoid division by zero, as $\\mathbf{g}$ itself is the zero vector.\n\n### Step 2: Adam Moment Updates\nThe Adam optimizer maintains exponential moving averages of the gradient (first moment) and its square (second moment). Given the previous moment estimates $m_{t-1}$ and $v_{t-1}$ at timestep $t-1$, and the decay rates $\\beta_1, \\beta_2 \\in [0, 1)$, the new estimates at timestep $t$ are calculated using the clipped gradient $\\tilde{\\mathbf{g}}$.\n\nThe first-moment estimate $m_t$ is updated as:\n$$\nm_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\tilde{\\mathbf{g}}\n$$\nThe second-moment estimate $v_t$ is updated using the elementwise square of the clipped gradient, $\\tilde{\\mathbf{g}}^2$:\n$$\nv_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\tilde{\\mathbf{g}}^2\n$$\n\n### Step 3: Bias Correction\nThe initial moment estimates are typically initialized to zero vectors. This introduces a bias towards zero, especially during the initial stages of optimization. Adam corrects for this bias by dividing the moment estimates by bias-correction factors that depend on the timestep $t$ and the decay rates.\n\nThe bias-corrected first-moment estimate $\\hat{m}_t$ is:\n$$\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n$$\nThe bias-corrected second-moment estimate $\\hat{v}_t$ is:\n$$\n\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n$$\n\n### Step 4: Parameter Update Vector\nThe parameter update vector, $\\Delta\\theta$, is computed using the bias-corrected moment estimates. The first-moment estimate $\\hat{m}_t$ acts as the update direction, and it is normalized elementwise by the square root of the second-moment estimate $\\hat{v}_t$. A small constant $\\varepsilon \\in \\mathbb{R}_{0}$ is added to the denominator for numerical stability. The result is scaled by the learning rate $\\alpha \\in \\mathbb{R}_{0}$.\n$$\n\\Delta\\theta = \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}\n$$\nNote that typically the parameter update rule is $\\theta_t = \\theta_{t-1} - \\text{step}$. The vector $\\Delta\\theta$ asked for is just the update itself, and since we are only concerned with its norm $\\lVert \\Delta\\theta \\rVert_{2}$, the sign is irrelevant.\n\n### Step 5: Final Metric Calculation\nWith the parameter update vector $\\Delta\\theta$ computed, we can now calculate the required metrics.\n\nThe L$2$-norm of the update vector, $\\lVert \\Delta\\theta \\rVert_2$, and the L$2$-norm of the clipped gradient, $\\lVert \\tilde{\\mathbf{g}} \\rVert_2$, are calculated.\n\nThe effective step normalization factor relative to the clipped gradient, $s_{\\text{eff}}$, is defined as:\n$$\ns_{\\text{eff}} = \n\\begin{cases}\n\\dfrac{\\lVert \\Delta \\theta \\rVert_{2}}{\\alpha \\,\\lVert \\tilde{\\mathbf{g}} \\rVert_{2}},  \\text{if } \\lVert \\tilde{\\mathbf{g}} \\rVert_{2}  0 \\\\\n0,  \\text{if } \\lVert \\tilde{\\mathbf{g}} \\rVert_{2} = 0\n\\end{cases}\n$$\nThe effective factor relative to the unclipped gradient, $r_{\\text{unclipped}}$, is defined as:\n$$\nr_{\\text{unclipped}} = \n\\begin{cases}\n\\dfrac{\\lVert \\Delta \\theta \\rVert_{2}}{\\alpha \\,\\lVert \\mathbf{g} \\rVert_{2}},  \\text{if } \\lVert \\mathbf{g} \\rVert_{2}  0 \\\\\n0,  \\text{if } \\lVert \\mathbf{g} \\rVert_{2} = 0\n\\end{cases}\n$$\nThese steps are applied to each test case to obtain the final results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Adam update metrics for a series of test cases.\n    \"\"\"\n    test_cases = [\n        # 1. d=3, g=[0.1,-0.2,0.05], m_prev=[0,0,0], v_prev=[0,0,0], t=1, alpha=0.01, beta1=0.9, beta2=0.999, eps=1e-8, c=1.0\n        {'g': np.array([0.1, -0.2, 0.05]), 'm_prev': np.array([0.0, 0.0, 0.0]), 'v_prev': np.array([0.0, 0.0, 0.0]), 't': 1, 'alpha': 0.01, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8, 'c': 1.0},\n        # 2. d=3, g=[10.0,0.0,0.0], m_prev=[0,0,0], v_prev=[0,0,0], t=1, alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-8, c=1.0\n        {'g': np.array([10.0, 0.0, 0.0]), 'm_prev': np.array([0.0, 0.0, 0.0]), 'v_prev': np.array([0.0, 0.0, 0.0]), 't': 1, 'alpha': 0.001, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8, 'c': 1.0},\n        # 3. d=3, g=[0.5,0.5,0.5], m_prev=[0.1,-0.1,0.0], v_prev=[0.04,0.01,0.09], t=10, alpha=0.005, beta1=0.8, beta2=0.9, eps=1e-6, c=0.5\n        {'g': np.array([0.5, 0.5, 0.5]), 'm_prev': np.array([0.1, -0.1, 0.0]), 'v_prev': np.array([0.04, 0.01, 0.09]), 't': 10, 'alpha': 0.005, 'beta1': 0.8, 'beta2': 0.9, 'epsilon': 1e-6, 'c': 0.5},\n        # 4. d=3, g=[0,0,0], m_prev=[0.01,-0.02,0], v_prev=[0.001,0.004,0.0009], t=5, alpha=0.01, beta1=0.9, beta2=0.99, eps=1e-8, c=0.1\n        {'g': np.array([0.0, 0.0, 0.0]), 'm_prev': np.array([0.01, -0.02, 0.0]), 'v_prev': np.array([0.001, 0.004, 0.0009]), 't': 5, 'alpha': 0.01, 'beta1': 0.9, 'beta2': 0.99, 'epsilon': 1e-8, 'c': 0.1},\n        # 5. d=3, g=[1e-4, -2e-4, 3e-4], m_prev=[0,0,0], v_prev=[0,0,0], t=1, alpha=1e-2, beta1=0.0, beta2=0.0, eps=1e-2, c=1e-3\n        {'g': np.array([1e-4, -2e-4, 3e-4]), 'm_prev': np.array([0.0, 0.0, 0.0]), 'v_prev': np.array([0.0, 0.0, 0.0]), 't': 1, 'alpha': 1e-2, 'beta1': 0.0, 'beta2': 0.0, 'epsilon': 1e-2, 'c': 1e-3},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        g, m_prev, v_prev = case['g'], case['m_prev'], case['v_prev']\n        t, alpha, beta1, beta2, epsilon, c = case['t'], case['alpha'], case['beta1'], case['beta2'], case['epsilon'], case['c']\n\n        # Step 1: Gradient Clipping\n        g_norm = np.linalg.norm(g)\n        \n        clipped = bool(g_norm  c)\n\n        if g_norm  0:\n            clip_factor = min(1.0, c / g_norm)\n            g_tilde = g * clip_factor\n        else:\n            g_tilde = np.zeros_like(g)\n\n        # Step 2: Adam Moment Updates\n        m_t = beta1 * m_prev + (1 - beta1) * g_tilde\n        v_t = beta2 * v_prev + (1 - beta2) * (g_tilde ** 2)\n\n        # Step 3: Bias Correction\n        m_hat = m_t / (1 - beta1**t)\n        v_hat = v_t / (1 - beta2**t)\n        \n        # Step 4: Parameter Update Vector\n        delta_theta = alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n        \n        # Step 5: Final Metric Calculation\n        delta_theta_norm = np.linalg.norm(delta_theta)\n        g_tilde_norm = np.linalg.norm(g_tilde)\n\n        if g_tilde_norm  0:\n            s_eff = delta_theta_norm / (alpha * g_tilde_norm)\n        else:\n            s_eff = 0.0\n\n        if g_norm  0:\n            r_unclipped = delta_theta_norm / (alpha * g_norm)\n        else:\n            r_unclipped = 0.0\n            \n        all_results.append([s_eff, r_unclipped, clipped])\n\n    # Format the final output string exactly as required\n    result_strings = []\n    for res in all_results:\n        # Convert each item in the sublist to its string representation\n        # str(True) is 'True', str(False) is 'False', which is correct\n        s = '[' + ','.join(map(str, res)) + ']'\n        result_strings.append(s)\n    \n    final_output = '[' + ','.join(result_strings) + ']'\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "With a limited computational budget, how can we efficiently search for the best hyperparameters? This exercise tackles this question by formally analyzing Successive Halving, the core engine of the Hyperband algorithm. Using a simplified model of validation loss improvement and the concept of 'simple regret,' you will quantitatively compare this adaptive resource allocation strategy to a baseline uniform search, revealing why multi-fidelity methods are so powerful for accelerating hyperparameter optimization. ",
            "id": "3135427",
            "problem": "A research team studies hyperparameter selection strategies under a concave improvement model of validation loss in deep learning. For any configuration index $i$, the validation loss after $e$ epochs is modeled as $V^{(i)}(e) = \\theta_i - c \\sqrt{e}$, where $\\theta_i$ is an unknown configuration-specific constant and $c  0$ is a known problem constant. The team compares two strategies that both start from the same pool of $K_0$ configurations and operate with a fixed total epoch budget $B$.\n\n- Strategy A: a single bracket of Successive Halving (SH), as used inside Hyperband. Assume a reduction factor $\\eta  1$, an integer number of rounds $S \\geq 1$, and $K_0 = \\eta^{S-1}$, with an initial per-configuration allocation $r_0$. At round $j \\in \\{0,1,\\dots,S-1\\}$, SH evaluates $K_0 / \\eta^{j}$ configurations for $r_0 \\eta^{j}$ epochs, retaining the top $1/\\eta$ fraction. This schedule implies a total budget $B = \\sum_{j=0}^{S-1} (K_0/\\eta^{j})(r_0 \\eta^{j}) = S K_0 r_0$, and the final surviving configuration receives $r_0 \\eta^{S-1} = r_0 K_0$ epochs in total.\n- Strategy B: uniform allocation. It evaluates all $K_0$ configurations for the same number of epochs, namely $B/K_0 = S r_0$, and then selects the best observed configuration.\n\nAssume there is no observation noise, so relative ordering by validation loss at any common epoch budget is fully determined by the $\\theta_i$ values. Define the simple regret of a strategy as the difference between the loss it achieves and the loss that an oracle would achieve if it knew in advance the identity of the configuration with the smallest $\\theta_i$ among the $K_0$ candidates and allocated the entire budget $B$ to that single configuration from the start.\n\nStarting only from the model $V^{(i)}(e) = \\theta_i - c \\sqrt{e}$, the budget accounting described above, and the definition of simple regret, derive a closed-form expression for the difference in expected simple regret between uniform allocation and SH for a single bracket, namely $\\Delta \\mathcal{R} \\equiv \\mathbb{E}[R_{\\text{uniform}} - R_{\\text{SH}}]$, as a function of $c$, $r_0$, $K_0$, and $S$. Express your final answer as a single analytic expression. No rounding is required.",
            "solution": "The problem requires the derivation of a closed-form expression for the difference in expected simple regret, $\\Delta \\mathcal{R} \\equiv \\mathbb{E}[R_{\\text{uniform}} - R_{\\text{SH}}]$, between two hyperparameter optimization strategies: Uniform Allocation and Successive Halving (SH).\n\nFirst, we must formalize the simple regret, $R$. The problem defines it as the difference between the loss a strategy achieves, $V_{\\text{strategy}}$, and the loss an oracle would achieve, $V_{\\text{oracle}}$. The oracle knows the best configuration from the start and allocates the entire budget $B$ to it. Let the set of $K_0$ initial configurations have associated parameters $\\{\\theta_1, \\theta_2, \\dots, \\theta_{K_0}\\}$. Let $\\theta_{(1)} = \\min_{i} \\theta_i$ be the parameter of the best configuration.\nThe validation loss for this configuration after $B$ epochs is:\n$$V_{\\text{oracle}} = \\theta_{(1)} - c \\sqrt{B}$$\nThe simple regret for any strategy is therefore:\n$$R_{\\text{strategy}} = V_{\\text{strategy}} - V_{\\text{oracle}} = V_{\\text{strategy}} - (\\theta_{(1)} - c \\sqrt{B})$$\n\nThe problem states there is no observation noise. This implies that for any given number of training epochs $e$, a configuration with a smaller $\\theta_i$ will always have a smaller validation loss $V^{(i)}(e) = \\theta_i - c \\sqrt{e}$. Consequently, any comparison-based selection process will always correctly identify the configuration with the smallest $\\theta_i$.\n\nLet us now analyze each strategy.\n\n**Strategy B: Uniform Allocation**\nThis strategy allocates the total budget $B$ uniformly across all $K_0$ configurations. Each configuration is evaluated for $e_{\\text{uniform}}$ epochs, where:\n$$e_{\\text{uniform}} = \\frac{B}{K_0}$$\nAfter this evaluation, the strategy selects the best-performing configuration. Due to the no-noise assumption, this will be the configuration with parameter $\\theta_{(1)}$. The \"loss it achieves\" is the final validation loss of this selected configuration, which has been trained for $e_{\\text{uniform}}$ epochs.\n$$V_{\\text{uniform}} = \\theta_{(1)} - c \\sqrt{e_{\\text{uniform}}} = \\theta_{(1)} - c \\sqrt{\\frac{B}{K_0}}$$\nThe simple regret for the uniform allocation strategy, $R_{\\text{uniform}}$, is:\n$$R_{\\text{uniform}} = V_{\\text{uniform}} - V_{\\text{oracle}} = \\left(\\theta_{(1)} - c \\sqrt{\\frac{B}{K_0}}\\right) - \\left(\\theta_{(1)} - c \\sqrt{B}\\right) = c \\left(\\sqrt{B} - \\sqrt{\\frac{B}{K_0}}\\right)$$\n\n**Strategy A: Successive Halving (SH)**\nThis strategy proceeds in $S$ rounds. In each round, a subset of configurations is evaluated, and only the top-performing fraction is promoted to the next round. Since there is no noise, the configuration with $\\theta_{(1)}$ will survive all elimination rounds and will be the final configuration selected by SH.\n\nThe problem specifies the total number of epochs the final surviving configuration receives: \"$e_{\\text{SH}} = r_0 \\eta^{S-1}$ epochs in total\". Using the given relation $K_0 = \\eta^{S-1}$, we can write:\n$$e_{\\text{SH}} = r_0 K_0$$\nThe \"loss achieved\" by the SH strategy is the final validation loss of its selected configuration (the one with $\\theta_{(1)}$) after its total allocated training time $e_{\\text{SH}}$.\n$$V_{\\text{SH}} = \\theta_{(1)} - c \\sqrt{e_{\\text{SH}}} = \\theta_{(1)} - c \\sqrt{r_0 K_0}$$\nThe simple regret for the SH strategy, $R_{\\text{SH}}$, is:\n$$R_{\\text{SH}} = V_{\\text{SH}} - V_{\\text{oracle}} = \\left(\\theta_{(1)} - c \\sqrt{r_0 K_0}\\right) - \\left(\\theta_{(1)} - c \\sqrt{B}\\right) = c \\left(\\sqrt{B} - \\sqrt{r_0 K_0}\\right)$$\n\n**Difference in Expected Simple Regret**\nWe are asked to find $\\Delta \\mathcal{R} = \\mathbb{E}[R_{\\text{uniform}} - R_{\\text{SH}}]$.\nThe expressions for $R_{\\text{uniform}}$ and $R_{\\text{SH}}$ are deterministic; they do not depend on the random values of the $\\theta_i$ parameters, as the $\\theta_{(1)}$ term has cancelled out. Therefore, the expectation operator is trivial, i.e., $\\mathbb{E}[X] = X$ for a deterministic quantity $X$.\n$$\\Delta \\mathcal{R} = R_{\\text{uniform}} - R_{\\text{SH}}$$\nSubstituting the derived expressions for the regrets:\n$$\\Delta \\mathcal{R} = c \\left(\\sqrt{B} - \\sqrt{\\frac{B}{K_0}}\\right) - c \\left(\\sqrt{B} - \\sqrt{r_0 K_0}\\right)$$\n$$\\Delta \\mathcal{R} = c \\sqrt{B} - c \\sqrt{\\frac{B}{K_0}} - c \\sqrt{B} + c \\sqrt{r_0 K_0}$$\n$$\\Delta \\mathcal{R} = c \\left(\\sqrt{r_0 K_0} - \\sqrt{\\frac{B}{K_0}}\\right)$$\nThe problem asks for the answer in terms of $c$, $r_0$, $K_0$, and $S$. We must substitute the given expression for the total budget, $B = S K_0 r_0$.\n$$\\Delta \\mathcal{R} = c \\left(\\sqrt{r_0 K_0} - \\sqrt{\\frac{S K_0 r_0}{K_0}}\\right)$$\n$$\\Delta \\mathcal{R} = c \\left(\\sqrt{r_0 K_0} - \\sqrt{S r_0}\\right)$$\nFactoring out the common term $\\sqrt{r_0}$:\n$$\\Delta \\mathcal{R} = c \\sqrt{r_0} \\left(\\sqrt{K_0} - \\sqrt{S}\\right)$$\nThis is the final closed-form expression for the difference in expected simple regret between the two strategies.",
            "answer": "$$\n\\boxed{c \\sqrt{r_0} \\left(\\sqrt{K_0} - \\sqrt{S}\\right)}\n$$"
        }
    ]
}