{
    "hands_on_practices": [
        {
            "introduction": "At the heart of any deep learning model is the loss function, which guides the learning process. To truly understand how contrastive learning shapes representations, we must go beyond the high-level intuition and delve into its mathematical mechanics. This exercise  challenges you to derive the gradient of the InfoNCE loss, a cornerstone of modern self-supervised learning, showing precisely how the model receives signals to attract positive pairs and repel negative ones.",
            "id": "77110",
            "problem": "In the field of autonomous materials discovery, deep learning models are increasingly used to analyze data from in-situ characterization experiments, such as time-series ellipsometry data collected during thin-film crystallization. The goal is to learn a representation that captures the underlying kinetic pathways of the material's transformation.\n\nA common approach is to use self-supervised contrastive learning. An encoder network, $f_\\theta$, maps a high-dimensional data vector from the ellipsometer at time $t$, denoted as $x_t$, to a lower-dimensional embedding vector $z_t = f_\\theta(x_t)$ in $\\mathbb{R}^D$. To ensure that the representations are well-behaved, the embeddings are L2-normalized, such that $\\|z_t\\|_2 = 1$.\n\nThe learning process is guided by the InfoNCE (Noise-Contrastive Estimation) loss function. For a given \"anchor\" embedding $z_i$, we select a \"positive\" sample $z_j$ from a nearby point in the time series, representing a similar material state. We also select a set of $N-1$ \"negative\" samples $\\{z_k\\}_{k=1}^{N-1}$ from distant time points, representing dissimilar states.\n\nThe InfoNCE loss for the anchor $z_i$ and positive sample $z_j$ is given by:\n$$\nL(z_i, z_j, \\{z_k\\}_{k=1}^{N-1}) = -\\log \\left[ \\frac{\\exp(z_i \\cdot z_j / \\tau)}{\\exp(z_i \\cdot z_j / \\tau) + \\sum_{k=1}^{N-1} \\exp(z_i \\cdot z_k / \\tau)} \\right]\n$$\nHere, $z_i \\cdot z_j$ is the dot product, which serves as a similarity score between the normalized vectors. The parameter $\\tau > 0$ is a scalar temperature that controls the sharpness of the distribution.\n\nThe network parameters $\\theta$ are updated using gradient descent, which requires computing the gradient of the loss with respect to the network's outputs (the embeddings). Your task is to derive the gradient of this loss function with respect to the **positive sample's embedding vector**, $z_j$.\n\nDerive the analytical expression for $\\nabla_{z_j} L(z_i, z_j, \\{z_k\\}_{k=1}^{N-1})$.",
            "solution": "1. Define similarity scores:  \n   $$s_{ij} = \\frac{z_i \\cdot z_j}{\\tau},\\quad s_{ik} = \\frac{z_i \\cdot z_k}{\\tau}.$$  \n2. Write the loss:  \n   $$L = -\\log\\frac{e^{s_{ij}}}{e^{s_{ij}}+\\sum_{k=1}^{N-1}e^{s_{ik}}} = -s_{ij} + \\log\\Bigl(e^{s_{ij}}+\\sum_{k=1}^{N-1}e^{s_{ik}}\\Bigr).$$  \n3. Compute gradient w.r.t. $z_j$:  \n   $$\\nabla_{z_j}(-s_{ij}) = -\\nabla_{z_j}\\frac{z_i\\cdot z_j}{\\tau} = -\\frac{1}{\\tau}z_i,$$  \n   $$\\nabla_{z_j}\\log\\Bigl(e^{s_{ij}}+\\sum_{k}e^{s_{ik}}\\Bigr)\n     = \\frac{1}{e^{s_{ij}}+\\sum_{k}e^{s_{ik}}}\\,\\nabla_{z_j}e^{s_{ij}}\n     = \\frac{e^{s_{ij}}}{e^{s_{ij}}+\\sum_{k}e^{s_{ik}}}\\,\\frac{1}{\\tau}z_i.$$  \n4. Combine terms:  \n   $$\\nabla_{z_j}L\n     = -\\frac{1}{\\tau}z_i + \\frac{e^{s_{ij}}}{\\tau\\bigl(e^{s_{ij}}+\\sum_{k}e^{s_{ik}}\\bigr)}\\,z_i\n     = \\frac{1}{\\tau}\\Bigl(\\frac{e^{s_{ij}}}{e^{s_{ij}}+\\sum_{k}e^{s_{ik}}}-1\\Bigr)z_i.$$",
            "answer": "$$\\boxed{\\nabla_{z_j}L=\\frac{1}{\\tau}\\Bigl(\\frac{\\exp\\bigl(z_i\\cdot z_j/\\tau\\bigr)}{\\exp\\bigl(z_i\\cdot z_j/\\tau\\bigr)+\\sum_{k=1}^{N-1}\\exp\\bigl(z_i\\cdot z_k/\\tau\\bigr)}-1\\Bigr)z_i}$$"
        },
        {
            "introduction": "Moving from theoretical understanding to practical, large-scale implementation often reveals subtle yet critical challenges. The success of methods like SimCLR depends on using very large batch sizes, often distributed across multiple processors. This practice  explores a famous 'gotcha' in this setting: the interaction between the contrastive loss and Batch Normalization, a standard network component, which can lead to a phenomenon known as 'information leakage' if not handled correctly.",
            "id": "3101675",
            "problem": "Consider training an A Simple Framework for Contrastive Learning of Visual Representations (SimCLR) model across $D$ graphics processing units (GPUs) with a per-device mini-batch size $m$, so the global batch size is $M = D m$. The encoder produces intermediate activations $x \\in \\mathbb{R}^p$ that are normalized by Batch Normalization (BN), and then passed through a projection head to yield embeddings $z \\in \\mathbb{R}^q$ that are $L_2$-normalized. During training, the Information Noise-Contrastive Estimation (InfoNCE) loss uses all $M$ embeddings as negatives by computing similarities $s_{ij} = z_i^\\top z_j$ and applies a softmax with temperature $\\tau > 0$. Assume that the BN transform of a scalar activation $x$ on device $d$ is \n$$\ny = \\gamma \\frac{x - \\hat{\\mu}_d}{\\sqrt{\\hat{\\sigma}_d^2 + \\epsilon}} + \\beta,\n$$\nwhere $\\hat{\\mu}_d$ and $\\hat{\\sigma}_d^2$ are computed over the local mini-batch on device $d$, and $\\gamma, \\beta \\in \\mathbb{R}$ are learnable parameters. In synchronized BN, $\\hat{\\mu}$ and $\\hat{\\sigma}^2$ are computed over the union of all devices so that a single pair $\\left(\\hat{\\mu}, \\hat{\\sigma}^2\\right)$ is applied across devices.\n\nStart from the core definitions of Batch Normalization (BN) and the InfoNCE loss. Using only those, reason about the effect of local versus synchronized BN on the distribution of $z$ and the calibration of similarities $s_{ij}$ across devices. In particular, analyze how the variance of the local BN estimators $\\hat{\\mu}_d$ and $\\hat{\\sigma}_d^2$ as functions of $m$ affects the noise injected into $y$, how that noise couples examples within the same device, and how mixing embeddings normalized under different $\\left(\\hat{\\mu}_d, \\hat{\\sigma}_d^2\\right)$ in the same softmax denominator biases gradient directions. Derive at least one failure mode that can occur when BN is not synchronized across devices at large $D$ and fixed $m$, and explain why synchronized BN mitigates it.\n\nWhich statements are most consistent with this first-principles analysis? Choose all that apply.\n\nA. Synchronized BN aligns the normalization statistics with the global batch used by the InfoNCE denominator, making similarity scores $s_{ij}$ comparably calibrated across devices. Without synchronization, per-device $\\left(\\hat{\\mu}_d, \\hat{\\sigma}_d^2\\right)$ induce scale and offset discrepancies that bias dot products and gradients in the softmax.\n\nB. BN has no material impact on SimCLR logits because $L_2$ normalization and the temperature $\\tau$ cancel any scale and shift differences, so synchronization is unnecessary even for large $D$ and small $m$.\n\nC. With unsynchronized BN and small $m$, the high variance of $\\hat{\\mu}_d$ and $\\hat{\\sigma}_d^2$ injects sample-dependent noise that couples each example to others on the same device. This can make within-device embeddings spuriously similar, creating per-device clusters and degrading global alignment, a failure mode that synchronization prevents.\n\nD. Replacing Batch Normalization with Group Normalization or Layer Normalization removes cross-sample dependence, alleviating calibration mismatch of logits across devices; however, it does not reproduce the advantage of many negatives, so training dynamics and optima differ from synchronized BN.\n\nE. Increasing the temperature $\\tau$ can exactly compensate for the per-device BN statistics mismatch, so choosing a sufficiently large $\\tau$ renders synchronization unnecessary regardless of $D$ and $m$.",
            "solution": "The user has requested a detailed analysis of the role of Batch Normalization statistics in the context of distributed training for the SimCLR model. I will first validate the problem statement and then proceed with a derivation from first principles, followed by an evaluation of each option.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Model:** Simple framework for Contrastive Learning of visual Representations (SimCLR).\n- **Training Configuration:** Distributed across $D$ graphics processing units (GPUs).\n- **Batch Sizes:** Per-device mini-batch size is $m$, leading to a global batch size of $M = Dm$.\n- **Network Architecture:** An encoder produces intermediate activations $x \\in \\mathbb{R}^p$. These are normalized by Batch Normalization (BN). A projection head then maps the normalized activations to embeddings $z \\in \\mathbb{R}^q$, which are subsequently $L_2$-normalized.\n- **Loss Function:** The Information Noise-Contrastive Estimation (InfoNCE) loss is used for training.\n- **Loss Calculation Details:** The loss uses all $M$ embeddings in the global batch as negative examples. It computes similarities $s_{ij} = z_i^\\top z_j$ and applies a softmax function with a temperature parameter $\\tau > 0$.\n- **Local (Unsynchronized) BN Definition:** For a scalar activation $x$ on device $d$, the transformation is given by $y = \\gamma \\frac{x - \\hat{\\mu}_d}{\\sqrt{\\hat{\\sigma}_d^2 + \\epsilon}} + \\beta$. The statistics $\\hat{\\mu}_d$ (mean) and $\\hat{\\sigma}_d^2$ (variance) are computed exclusively over the local mini-batch on device $d$. $\\gamma$ and $\\beta$ are learnable parameters.\n- **Synchronized BN Definition:** The statistics $\\hat{\\mu}$ and $\\hat{\\sigma}^2$ are computed over the union of all mini-batches across all devices (the entire global batch of size $M$). A single pair $(\\hat{\\mu}, \\hat{\\sigma}^2)$ is used for normalization on all devices.\n- **Task:** The core task is to reason from first principles about the effects of local vs. synchronized BN on the distribution of embeddings $z$ and the calibration of similarities $s_{ij}$. This includes analyzing the noise from local BN statistics, the coupling of examples within a device, and the bias in gradients. A specific failure mode for unsynchronized BN at large $D$ and fixed $m$ must be derived, along with an explanation of how synchronized BN mitigates it.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is firmly grounded in the established principles of deep learning, specifically self-supervised contrastive learning. The definitions of SimCLR, InfoNCE loss, Batch Normalization (local and synchronized), and distributed training are standard and factually correct. The issue described—the \"information leak\" from local BN statistics in contrastive learning—is a well-documented phenomenon in the machine learning literature. The problem does not contain any pseudoscience or controversial claims.\n- **Well-Posed:** The problem is well-posed. It asks for a conceptual derivation and analysis based on provided definitions, which is a standard form of scientific reasoning. The question is structured to lead to a specific conclusion about a known failure mode, which is solvable.\n- **Objective:** The language is technical, precise, and objective. It provides clear definitions and asks for an analysis based on them.\n\n**Step 3: Verdict and Action**\nThe problem statement is scientifically sound, well-posed, objective, and complete for the task required. It is a valid problem. I will proceed with the solution derivation.\n\n### First-Principles Derivation\n\nLet us begin by formalizing the core components.\n\n1.  **Batch Normalization (BN):** The purpose of BN is to normalize the distribution of activations within a neural network to have zero mean and unit variance, which are then rescaled and shifted by learnable parameters $\\gamma$ and $\\beta$. For an activation $x_i$ belonging to a mini-batch, the transform is:\n    $$ y_i = \\gamma \\frac{x_i - \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} + \\beta $$\n    In practice, the true mean $\\mathbb{E}[x]$ and variance $\\text{Var}[x]$ of the activation distribution are unknown and are estimated using the current mini-batch.\n\n2.  **Local vs. Synchronized BN Statistics:**\n    -   In the **unsynchronized (local) BN** setting, for an example $i$ on device $d$, the statistics are estimators computed from the local batch of size $m$:\n        $$ \\hat{\\mu}_d = \\frac{1}{m} \\sum_{j \\in \\text{batch}_d} x_j \\quad \\text{and} \\quad \\hat{\\sigma}^2_d = \\frac{1}{m} \\sum_{j \\in \\text{batch}_d} (x_j - \\hat{\\mu}_d)^2 $$\n        The key point is that $\\hat{\\mu}_d$ and $\\hat{\\sigma}^2_d$ are random variables whose values depend on the specific $m$ samples present on device $d$. For a small mini-batch size $m$, these estimators have high variance, i.e., they are noisy.\n    -   In the **synchronized BN (SyncBN)** setting, the statistics are computed over the global batch of size $M=Dm$:\n        $$ \\hat{\\mu}_M = \\frac{1}{M} \\sum_{k=1}^M x_k \\quad \\text{and} \\quad \\hat{\\sigma}^2_M = \\frac{1}{M} \\sum_{k=1}^M (x_k - \\hat{\\mu}_M)^2 $$\n        Since $M \\gg m$ (especially for large $D$), $\\hat{\\mu}_M$ and $\\hat{\\sigma}^2_M$ are much more stable and accurate estimators of the true statistics.\n\n3.  **InfoNCE Loss in SimCLR:** For a positive pair of embeddings $(z_i, z_j)$, the loss is designed to maximize their similarity while minimizing the similarity of $z_i$ to all other \"negative\" embeddings $z_k$ in the global batch. The loss for anchor $z_i$ is:\n    $$ \\mathcal{L}_i = -\\log \\frac{\\exp(z_i^\\top z_j / \\tau)}{\\sum_{k=1, k \\neq i}^{M} \\exp(z_i^\\top z_k / \\tau)} $$\n    The denominator is a sum over all $M-1$ other examples in the *global batch*, regardless of their device of origin.\n\n**Derivation of the Failure Mode:**\n\nConsider the unsynchronized case with large $D$ and small $m$.\nLet $x_i^{(d)}$ be the $i$-th activation on device $d$. Its normalized version is $y_i^{(d)} = \\gamma \\frac{x_i^{(d)} - \\hat{\\mu}_d}{\\sqrt{\\hat{\\sigma}_d^2 + \\epsilon}} + \\beta$.\nCritically, the transformation applied to $x_i^{(d)}$ depends on the statistics $(\\hat{\\mu}_d, \\hat{\\sigma}_d^2)$, which are themselves functions of *all other samples* $\\{x_j^{(d)}\\}_{j=1}^m$ on device $d$. This means that the output $y_i^{(d)}$ (and consequently the final embedding $z_i^{(d)}$) contains information not just about the input $i$, but also about the entire local batch on its device. This constitutes an \"information leak\".\n\nBecause the batch on device $d$ is different from the batch on device $d'$, the statistics are different: $(\\hat{\\mu}_d, \\hat{\\sigma}_d^2) \\neq (\\hat{\\mu}_{d'}, \\hat{\\sigma}_{d'}^2)$. This introduces a device-specific signature into the embeddings. All embeddings originating from device $d$ are processed with the same random offsets and scales determined by the specific samples on that device. This shared transformation couples the embeddings from the same device, making them spuriously correlated.\n\nNow consider the InfoNCE loss. The denominator $\\sum_{k=1}^{M-1} \\exp(z_i^\\top z_k / \\tau)$ mixes embeddings from all devices. Let the anchor $z_i$ be from device $d_1$. The negatives $\\{z_k\\}$ come from devices $d_1, d_2, ..., d_D$.\n-   For a negative $z_k$ from the same device $d_1$, both $z_i$ and $z_k$ have been tainted by the *same* normalization statistics $(\\hat{\\mu}_{d_1}, \\hat{\\sigma}_{d_1}^2)$.\n-   For a negative $z_k$ from a different device $d_2$, $z_k$ has been tainted by *different* statistics $(\\hat{\\mu}_{d_2}, \\hat{\\sigma}_{d_2}^2)$.\n\nThe dot product similarities $s_{ik} = z_i^\\top z_k$ are therefore not directly comparable. They are not \"calibrated\". The distribution of embeddings from device $d_1$ is systematically different from the distribution of embeddings from device $d_2$. This biases the similarities. The model can learn to \"cheat\" by identifying that samples from the same device are more likely to be similar, not because of their semantic content, but because of the shared artifact of the normalization process. This creates artificial clusters of embeddings based on their device of origin.\n\nThis is a **failure mode**: instead of learning instance-level discrimination based on visual semantics, the model learns device-level discrimination. The gradients computed from this biased softmax will push the model to reinforce these device-specific clusters, degrading the quality of the learned representations for the actual downstream tasks.\n\n**Mitigation with Synchronized BN:**\nSynchronized BN computes a single pair of statistics $(\\hat{\\mu}_M, \\hat{\\sigma}_M^2)$ across all $D$ devices. Every single example $x_k$ in the global batch, regardless of its device, is normalized using this same set of statistics.\n$$ y_k = \\gamma \\frac{x_k - \\hat{\\mu}_M}{\\sqrt{\\hat{\\sigma}_M^2 + \\epsilon}} + \\beta $$\nThis eliminates the device-specific statistical signature. The \"information leak\" is plugged. All embeddings $z_k$ in the global batch are now drawn from distributions that have been conditioned on the same normalization transformation. The similarity scores $s_{ij}$ become directly comparable and properly calibrated. The InfoNCE denominator sums over a homogeneous set of negatives, and the gradients correctly guide the model to learn instance-level discrimination, as intended.\n\n### Option-by-Option Analysis\n\n**A. Synchronized BN aligns the normalization statistics with the global batch used by the InfoNCE denominator, making similarity scores $s_{ij}$ comparably calibrated across devices. Without synchronization, per-device $(\\hat{\\mu}_d, \\hat{\\sigma}_d^2)$ induce scale and offset discrepancies that bias dot products and gradients in the softmax.**\nThis statement is a precise summary of the derived analysis. SyncBN ensures the normalization scope (global batch) matches the loss's scope (global batch). The lack of synchronization creates device-specific discrepancies, leading to uncalibrated similarities and biased gradients.\n**Verdict: Correct**\n\n**B. BN has no material impact on SimCLR logits because $L_2$ normalization and the temperature $\\tau$ cancel any scale and shift differences, so synchronization is unnecessary even for large $D$ and small $m$.**\nThis statement is incorrect. The projection head is typically a non-linear function (e.g., an MLP). A non-linear function does not commute with scaling and shifting, meaning the output direction of the embedding vector $z$ will change depending on the BN statistics $(\\hat{\\mu}_d, \\hat{\\sigma}_d^2)$ used on its input. The subsequent $L_2$ normalization, which only normalizes the magnitude to $1$, cannot correct this change in direction. The temperature $\\tau$ is a global scalar for all logits and cannot correct relative biases between them.\n**Verdict: Incorrect**\n\n**C. With unsynchronized BN and small $m$, the high variance of $\\hat{\\mu}_d$ and $\\hat{\\sigma}_d^2$ injects sample-dependent noise that couples each example to others on the same device. This can make within-device embeddings spuriously similar, creating per-device clusters and degrading global alignment, a failure mode that synchronization prevents.**\nThis statement accurately describes the failure mode derived above. It correctly identifies the high variance of local estimators for small $m$ as the source of noise, explains the coupling mechanism (shared noise on the same device), and correctly describes the outcome (spurious similarity, per-device clustering) and the solution (synchronization).\n**Verdict: Correct**\n\n**D. Replacing Batch Normalization with Group Normalization or Layer Normalization removes cross-sample dependence, alleviating calibration mismatch of logits across devices; however, it does not reproduce the advantage of many negatives, so training dynamics and optima differ from synchronized BN.**\nThis statement is correct and insightful. Layer Normalization (LN) and Group Normalization (GN) compute statistics per-sample, thus having no dependence on other samples in the batch. This intrinsically solves the information leak problem across devices. However, a significant part of BNs effectiveness, especially in contrastive learning, is attributed to its implicit regularization stemming from mini-batch statistics. LN and GN have different regularization properties. While using many negatives is a feature of the loss function, the overall performance of the SimCLR training recipe relies on the synergy between the large-batch InfoNCE loss and the properties of SyncBN. Replacing SyncBN with LN or GN, while solving the leak, is known to result in different, and often inferior, performance, because the beneficial training dynamics associated with large-batch BN are lost.\n**Verdict: Correct**\n\n**E. Increasing the temperature $\\tau$ can exactly compensate for the per-device BN statistics mismatch, so choosing a sufficiently large $\\tau$ renders synchronization unnecessary regardless of $D$ and $m$.**\nThis statement is incorrect. The temperature $\\tau$ is a scalar that rescales all logits $s_{ij}$ uniformly. It controls the sharpness of the softmax distribution. It cannot correct for a structural bias where certain classes of logits (e.g., within-device) are systematically inflated relative to others (e.g., across-device). A very large $\\tau$ would merely flatten the distribution, causing the model to learn very little, but it would not \"compensate\" for the bias.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "Self-supervised learning allows us to instill desired structures in our representations without explicit labels. While many contrastive methods focus on learning representations that are *invariant* to certain transformations (e.g., rotation), we sometimes want representations that transform *predictably*—a property called *equivariance*. This final hands-on coding challenge  makes these abstract geometric concepts concrete, asking you to design a toy system that simultaneously enforces both invariance and equivariance.",
            "id": "3173218",
            "problem": "You will design and analyze a toy self-supervised learning setup where multiple augmented views are generated via actions of known transformation groups. The aim is to combine an invariance-inducing contrastive objective with an equivariance-inducing penalty. All angles must be expressed in radians. There are no physical units in this problem.\n\nConstruct a dataset, define group actions, specify an encoder, and implement a mixed objective as follows.\n\n1. Dataset construction:\n   - Create a set of $n$ points on the unit circle in $\\mathbb{R}^2$ with $n = 8$. For $k \\in \\{0,1,\\dots,n-1\\}$, let $\\theta_k = \\frac{2\\pi k}{n}$ in radians and define $\\mathbf{x}_k = [\\cos(\\theta_k), \\sin(\\theta_k)]^\\top \\in \\mathbb{R}^2$.\n\n2. Group actions:\n   - Define the group $\\mathcal{G}$ of planar rotations by angle $\\phi \\in \\mathbb{R}$ acting on $\\mathbb{R}^2$ via the standard rotation. This is the group that should induce invariance in the learned representation, in the sense that the representation of $\\mathbf{x}$ and the representation of $g_\\phi \\cdot \\mathbf{x}$ should be indistinguishable for any $\\phi$.\n   - Define the group $\\mathcal{H}$ of reflections across the horizontal axis acting on $\\mathbb{R}^2$ by $(x,y) \\mapsto (x,-y)$. This is the group that should induce equivariance in the learned representation, in the sense that the representation should transform according to a known linear representation $\\rho(h) \\in \\mathbb{R}^{2 \\times 2}$ when $h \\in \\mathcal{H}$ acts on the input.\n\n3. Encoder:\n   - Use a fixed, known encoder $f_{\\mathbf{W}} : \\mathbb{R}^2 \\to \\mathbb{R}^2$ defined by $f_{\\mathbf{W}}(\\mathbf{x}) = \\tanh(\\mathbf{W}\\mathbf{x})$ where $\\tanh(\\cdot)$ is applied element-wise and\n     $$\n     \\mathbf{W} = \\begin{bmatrix}\n     1.0 & 0.5 \\\\\n     0.3 & 1.2\n     \\end{bmatrix}.\n     $$\n     No training or parameter updates are performed in this problem.\n\n4. Objectives to be implemented from first principles:\n   - Contrastive invariance objective: Form pairs of positive views by applying transformations from $\\mathcal{G}$ to the same base sample (two possibly different elements of $\\mathcal{G}$), and treat all other samples in the batch as negatives. Implement a normalized temperature-scaled cross-entropy over cosine similarities between all views in the batch. Use temperature $\\tau = 0.2$. This objective should only capture the invariance requirement for $\\mathcal{G}$ using view pairs from the same base sample.\n   - Equivariance penalty: For the reflection $h \\in \\mathcal{H}$ that maps $(x,y)$ to $(x,-y)$, use the known linear representation\n     $$\n     \\rho(h) = \\begin{bmatrix}\n     1 & 0 \\\\\n     0 & -1\n     \\end{bmatrix}.\n     $$\n     Enforce equivariance by penalizing the average squared deviation between $f_{\\mathbf{W}}(h \\cdot \\mathbf{x}_k)$ and $\\rho(h)\\, f_{\\mathbf{W}}(\\mathbf{x}_k)$ across the dataset. This penalty is independent of the invariance pairs and does not use any negatives.\n   - Mixed objective: Combine the two with a nonnegative mixture weight $\\lambda = 0.1$ to obtain a single scalar objective for a given augmentation pipeline. You must implement a precise, numerical definition of each component objective starting from the definitions above, without using any black-box or external functions beyond basic linear algebra and vector operations.\n\n5. Augmentation pipelines (test suite):\n   For each pipeline below, you must generate exactly two views per base sample deterministically, using the specified transformations in the given order. No randomness or noise is permitted.\n   - Pipeline A (intended to enforce invariance to $\\mathcal{G}$): Use two rotations from $\\mathcal{G}$, with the first view using $\\phi_1 = 0$ and the second view using $\\phi_2 = \\frac{\\pi}{2}$.\n   - Pipeline B (violates the intended invariance): Replace rotations with isotropic scalings by factors $s_1 = 0.5$ and $s_2 = 2.0$ applied to each base sample to form the two views. Scaling is not an element of $\\mathcal{G}$.\n   - Pipeline C (mixes groups in the views): Use a rotation by $\\phi = \\pi$ for the first view and a horizontal reflection for the second view. The reflection is not in $\\mathcal{G}$, so the two views are not in the same $\\mathcal{G}$-orbit.\n   - Pipeline D (boundary case): Use identity for both views, i.e., no transformation is applied to either view.\n\n6. Program requirements:\n   - Construct the dataset, define the group actions and the encoder, and implement the mixed objective as described. The cosine similarity must be computed between $\\ell_2$-normalized feature vectors in $\\mathbb{R}^2$.\n   - For each pipeline in the test suite, compute the mixed objective value as a single real number: the sum of the contrastive invariance objective and $\\lambda$ times the equivariance penalty, using the same dataset and encoder for all pipelines.\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The order must be $[\\text{Pipeline A result}, \\text{Pipeline B result}, \\text{Pipeline C result}, \\text{Pipeline D result}]$ with each entry a floating-point number.\n\nYour solution must derive the implemented objectives from the core definitions of group action, invariance, and equivariance. The final answers for the four test cases must be explicit floating-point values, and the final program must be complete and runnable as specified.",
            "solution": "### Problem Validation\n\nThe problem statement is critically examined and found to be **valid**.\n\n**Step 1: Extracted Givens**\n- **Dataset**: $n = 8$ points on the unit circle in $\\mathbb{R}^2$, $\\mathbf{x}_k = [\\cos(\\theta_k), \\sin(\\theta_k)]^\\top$ for $k \\in \\{0, 1, \\dots, n-1\\}$, with $\\theta_k = \\frac{2\\pi k}{n}$.\n- **Group $\\mathcal{G}$**: Planar rotations, $g_\\phi \\in \\mathcal{G}$, for inducing invariance.\n- **Group $\\mathcal{H}$**: Reflections across the horizontal axis, $h \\cdot (x,y) = (x,-y)$, for inducing equivariance.\n- **Encoder**: $f_{\\mathbf{W}}(\\mathbf{x}) = \\tanh(\\mathbf{W}\\mathbf{x})$ applied element-wise, with a fixed matrix $\\mathbf{W} = \\begin{bmatrix} 1.0 & 0.5 \\\\ 0.3 & 1.2 \\end{bmatrix}$.\n- **Contrastive objective**: Normalized temperature-scaled cross-entropy over cosine similarities. Temperature $\\tau = 0.2$. Positive pairs are views from the same base sample transformed by elements of $\\mathcal{G}$. Cosine similarity is computed on $\\ell_2$-normalized feature vectors.\n- **Equivariance penalty**: Average squared deviation $\\| f_{\\mathbf{W}}(h \\cdot \\mathbf{x}_k) - \\rho(h)\\, f_{\\mathbf{W}}(\\mathbf{x}_k) \\|^2$, with $\\rho(h) = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}$.\n- **Mixed objective**: $\\mathcal{L} = \\mathcal{L}_{\\text{inv}} + \\lambda \\mathcal{L}_{\\text{eqv}}$, with $\\lambda = 0.1$.\n- **Augmentation Pipelines**:\n    - **A**: Rotations $\\phi_1 = 0$, $\\phi_2 = \\frac{\\pi}{2}$.\n    - **B**: Isotropic scalings $s_1 = 0.5$, $s_2 = 2.0$.\n    - **C**: Rotation $\\phi = \\pi$ and horizontal reflection $h$.\n    - **D**: Identity transformations for both views.\n- **Output Format**: A single line `[result_A, result_B, result_C, result_D]`.\n\n**Step 2: Validation Using Extracted Givens**\n- **Scientifically Grounded**: The problem is a well-defined exercise in computational deep learning, specifically focusing on the core concepts of invariance and equivariance in self-supervised learning. The mathematical framework is sound and based on established principles (group theory, contrastive learning objectives like InfoNCE).\n- **Well-Posed**: All variables, constants, functions, and objectives are specified with mathematical precision. The task is a deterministic computation, which guarantees the existence of a unique, stable, and meaningful solution.\n- **Objective**: The language is formal and devoid of subjectivity or ambiguity.\n- **Completeness and Consistency**: The problem is self-contained. All necessary data ($\\mathbf{W}$, $n$, $\\tau$, $\\lambda$, pipeline definitions) are provided, and there are no internal contradictions.\n- **Feasibility**: The computations required are standard linear algebra and elementary functions, entirely feasible within a standard numerical environment.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\nThe problem requires the computation of a mixed objective function for four distinct data augmentation pipelines. The objective combines a contrastive loss for invariance and a penalty term for equivariance.\n\n**1. Preliminaries: Dataset and Encoder**\n\nThe dataset consists of $n=8$ vectors $\\mathbf{x}_k \\in \\mathbb{R}^2$ on the unit circle:\n$$ \\mathbf{x}_k = \\begin{bmatrix} \\cos(2\\pi k / n) \\\\ \\sin(2\\pi k / n) \\end{bmatrix}, \\quad k \\in \\{0, 1, \\dots, 7\\} $$\nThe encoder is a fixed non-linear function $f_{\\mathbf{W}}: \\mathbb{R}^2 \\to \\mathbb{R}^2$ defined as:\n$$ f_{\\mathbf{W}}(\\mathbf{x}) = \\tanh(\\mathbf{W}\\mathbf{x}) $$\nwhere $\\mathbf{W} = \\begin{bmatrix} 1.0 & 0.5 \\\\ 0.3 & 1.2 \\end{bmatrix}$ and the hyperbolic tangent function $\\tanh(\\cdot)$ is applied element-wise to the vector $\\mathbf{W}\\mathbf{x}$.\n\n**2. Equivariance Penalty ($\\mathcal{L}_{\\text{eqv}}$)**\n\nEquivariance requires that a transformation in the input space corresponds to a predictable transformation in the feature space. For the reflection group $\\mathcal{H}$ with generator $h \\cdot (x,y) = (x,-y)$, the feature representation is expected to transform via the matrix $\\rho(h) = \\text{diag}(1, -1)$. The deviation from perfect equivariance is quantified by the penalty term $\\mathcal{L}_{\\text{eqv}}$, defined as the mean squared error over the base dataset:\n$$ \\mathcal{L}_{\\text{eqv}} = \\frac{1}{n} \\sum_{k=0}^{n-1} \\| f_{\\mathbf{W}}(h \\cdot \\mathbf{x}_k) - \\rho(h) f_{\\mathbf{W}}(\\mathbf{x}_k) \\|_2^2 $$\nwhere $h \\cdot \\mathbf{x}_k$ is the action of reflection on $\\mathbf{x}_k$, and $\\rho(h) f_{\\mathbf{W}}(\\mathbf{x}_k)$ is the target transformed feature vector. The action of $h$ can be represented by the matrix $\\mathbf{H} = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}$, so $h \\cdot \\mathbf{x}_k = \\mathbf{H}\\mathbf{x}_k$.\nThis penalty is a fixed property of the encoder and the dataset, independent of the augmentation pipeline used for the contrastive task. It will be computed once and used for all four test cases.\n\n**3. Contrastive Invariance Objective ($\\mathcal{L}_{\\text{inv}}$)**\n\nInvariance requires that the representation be insensitive to transformations from a given group, here the rotation group $\\mathcal{G}$. The contrastive objective effectuates this by pulling representations of different views of the same object (positive pairs) together, while pushing them apart from representations of other objects (negative pairs).\n\nFor each of the $n$ base samples $\\mathbf{x}_k$, an augmentation pipeline generates two views, $\\tilde{\\mathbf{x}}_{k,1}$ and $\\tilde{\\mathbf{x}}_{k,2}$. These $2n$ views are processed by the encoder $f_{\\mathbf{W}}$ to produce feature vectors $\\mathbf{z}_{i} \\in \\mathbb{R}^2$. These are then $\\ell_2$-normalized to lie on the unit sphere in feature space:\n$$ \\mathbf{u}_i = \\frac{\\mathbf{z}_i}{\\|\\mathbf{z}_i\\|_2} $$\nThe similarity between two views is measured by their cosine similarity, $\\text{sim}(\\mathbf{u}_i, \\mathbf{u}_j) = \\mathbf{u}_i^\\top \\mathbf{u}_j$.\n\nThe objective is the normalized temperature-scaled cross-entropy loss (InfoNCE). For a batch of $2n$ views, where for each view $\\mathbf{u}_i$ there is one unique positive partner $\\mathbf{u}_{j}$ (the other view from the same base sample), the loss is:\n$$ \\mathcal{L}_{\\text{inv}} = - \\frac{1}{2n} \\sum_{i=1}^{2n} \\log \\frac{\\exp(\\text{sim}(\\mathbf{u}_i, \\mathbf{u}_{\\text{pos}(i)}) / \\tau)}{\\sum_{m=1, m \\ne i}^{2n} \\exp(\\text{sim}(\\mathbf{u}_i, \\mathbf{u}_m) / \\tau)} $$\nwhere $\\text{pos}(i)$ denotes the index of the positive pair for view $i$, and $\\tau=0.2$ is the temperature parameter. The term $\\mathcal{L}_{\\text{inv}}$ is computed for each specific augmentation pipeline.\n\n**4. Mixed Objective ($\\mathcal{L}$)**\n\nThe total objective function is a weighted sum of the invariance and equivariance losses:\n$$ \\mathcal{L} = \\mathcal{L}_{\\text{inv}} + \\lambda \\mathcal{L}_{\\text{eqv}} $$\nwith the mixing coefficient $\\lambda = 0.1$.\n\n**5. Analysis of Pipelines and Computational Procedure**\n\nWe will now compute the value of $\\mathcal{L}$ for each of the four specified pipelines. The procedure is as follows:\nFirst, the constant $\\mathcal{L}_{\\text{eqv}}$ is calculated. Then for each pipeline:\na. Generate the two sets of $n$ views, $\\{\\tilde{\\mathbf{x}}_{k,1}\\}_{k=0}^{n-1}$ and $\\{\\tilde{\\mathbf{x}}_{k,2}\\}_{k=0}^{n-1}$.\nb. Form a single batch of $2n$ views.\nc. Encode and normalize the batch to obtain the feature vectors $\\{\\mathbf{u}_i\\}_{i=1}^{2n}$.\nd. Compute $\\mathcal{L}_{\\text{inv}}$ using the formula above.\ne. Compute the total objective $\\mathcal{L} = \\mathcal{L}_{\\text{inv}} + \\lambda \\mathcal{L}_{\\text{eqv}}$.\n\n- **Pipeline A (Rotation Invariance):**\n    - View 1: $\\tilde{\\mathbf{x}}_{k,1} = g_{0} \\cdot \\mathbf{x}_k = \\mathbf{x}_k$.\n    - View 2: $\\tilde{\\mathbf{x}}_{k,2} = g_{\\pi/2} \\cdot \\mathbf{x}_k$. The rotation matrix is $\\mathbf{R}_{\\pi/2} = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$.\n    - These views are generated by transformations from group $\\mathcal{G}$, as intended by the invariance objective.\n\n- **Pipeline B (Scaling Violation):**\n    - View 1: $\\tilde{\\mathbf{x}}_{k,1} = 0.5 \\cdot \\mathbf{x}_k$.\n    - View 2: $\\tilde{\\mathbf{x}}_{k,2} = 2.0 \\cdot \\mathbf{x}_k$.\n    - Scaling is not in $\\mathcal{G}$, so this pipeline violates the assumption of the invariance objective. The representations are not expected to be similar.\n\n- **Pipeline C (Mixed-Group Violation):**\n    - View 1: $\\tilde{\\mathbf{x}}_{k,1} = g_{\\pi} \\cdot \\mathbf{x}_k = -\\mathbf{x}_k$. This is a rotation from $\\mathcal{G}$.\n    - View 2: $\\tilde{\\mathbf{x}}_{k,2} = h \\cdot \\mathbf{x}_k$. This is a reflection from $\\mathcal{H}$, not from $\\mathcal{G}$ (except for points on the axis of reflection). The two views are not in the same $\\mathcal{G}$-orbit.\n\n- **Pipeline D (Identity Baseline):**\n    - View 1: $\\tilde{\\mathbf{x}}_{k,1} = \\mathbf{x}_k$.\n    - View 2: $\\tilde{\\mathbf{x}}_{k,2} = \\mathbf{x}_k$.\n    - The two views are identical. The encoded and normalized representations will also be identical, resulting in a maximal cosine similarity of $1$ for all positive pairs. This provides a lower bound for the $\\mathcal{L}_{\\text{inv}}$ term.\n\nThe final numerical results are calculated by a program implementing these steps.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the mixed objective for four self-supervised learning augmentation pipelines.\n    \"\"\"\n    # 1. Define constants and problem setup\n    n = 8\n    tau = 0.2\n    lambda_ = 0.1\n    W = np.array([[1.0, 0.5], [0.3, 1.2]])\n    rho_h = np.array([[1.0, 0.0], [0.0, -1.0]])\n    H_matrix = np.array([[1.0, 0.0], [0.0, -1.0]])\n\n    # 2. Dataset construction\n    thetas = 2 * np.pi * np.arange(n) / n\n    X_base = np.stack([np.cos(thetas), np.sin(thetas)], axis=1)\n\n    # 3. Encoder function\n    def f_W(x_batch):\n        return np.tanh(x_batch @ W.T)\n\n    # 4. Compute the constant equivariance penalty (L_eqv)\n    X_reflected = X_base @ H_matrix.T\n    f_X_base = f_W(X_base)\n    f_X_reflected = f_W(X_reflected)\n    rho_f_X_base = f_X_base @ rho_h.T\n\n    squared_deviations = np.sum((f_X_reflected - rho_f_X_base)**2, axis=1)\n    L_eqv = np.mean(squared_deviations)\n\n    # 5. Define pipelines and compute mixed objective for each\n    pipelines = {\n        \"A\": {\n            \"transform1\": lambda X: X @ np.array([[np.cos(0), -np.sin(0)], [np.sin(0), np.cos(0)]]).T,\n            \"transform2\": lambda X: X @ np.array([[np.cos(np.pi/2), -np.sin(np.pi/2)], [np.sin(np.pi/2), np.cos(np.pi/2)]]).T,\n        },\n        \"B\": {\n            \"transform1\": lambda X: 0.5 * X,\n            \"transform2\": lambda X: 2.0 * X,\n        },\n        \"C\": {\n            \"transform1\": lambda X: X @ np.array([[np.cos(np.pi), -np.sin(np.pi)], [np.sin(np.pi), np.cos(np.pi)]]).T,\n            \"transform2\": lambda X: X @ H_matrix.T,\n        },\n        \"D\": {\n            \"transform1\": lambda X: X,\n            \"transform2\": lambda X: X,\n        },\n    }\n\n    results = []\n    \n    for _, config in pipelines.items():\n        # a. Generate views\n        views1 = config[\"transform1\"](X_base)\n        views2 = config[\"transform2\"](X_base)\n\n        # b. Form batch, encode, and normalize\n        batch_views = np.concatenate([views1, views2], axis=0)\n        z = f_W(batch_views)\n        \n        # Add a small epsilon to the norm to prevent division by zero for null vectors\n        norm_z = np.linalg.norm(z, axis=1, keepdims=True)\n        u = z / (norm_z + 1e-9)\n\n        # c. Compute contrastive invariance objective (L_inv)\n        # Cosine similarity matrix\n        sim_matrix = u @ u.T\n        logits = sim_matrix / tau\n\n        # Use log-sum-exp trick for numerical stability\n        logits_stable = logits - np.max(logits, axis=1, keepdims=True)\n        \n        # Mask to exclude self-similarity from denominator\n        mask = 1.0 - np.eye(2 * n)\n        \n        exp_logits = np.exp(logits_stable)\n        log_denom = np.log(np.sum(exp_logits * mask, axis=1))\n\n        # Identify positive pairs\n        # For a concatenated batch [v1_0..v1_n-1, v2_0..v2_n-1],\n        # positives for indices 0..n-1 are at n..2n-1\n        # positives for indices n..2n-1 are at 0..n-1\n        pos_indices = np.concatenate([np.arange(n, 2 * n), np.arange(0, n)])\n        \n        # Extract logits for positive pairs\n        pos_logits_stable = logits_stable[np.arange(2 * n), pos_indices]\n\n        # Log probability of positive pairs\n        log_probs = pos_logits_stable - log_denom\n        \n        L_inv = -np.mean(log_probs)\n\n        # d. Compute final mixed objective\n        L_total = L_inv + lambda_ * L_eqv\n        results.append(L_total)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}