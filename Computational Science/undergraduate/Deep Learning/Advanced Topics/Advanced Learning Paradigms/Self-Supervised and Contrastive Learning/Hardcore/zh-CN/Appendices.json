{
    "hands_on_practices": [
        {
            "introduction": "对比学习的核心在于更新模型，以拉近相似样本的表示，同时推远不同样本的表示。这通常通过最小化像InfoNCE这样的损失函数来实现。理解这一优化过程的关键，在于洞悉损失函数相对于模型输出（即嵌入向量）的梯度，因为它揭示了塑造表示空间的数学驱动力。本练习将引导你推导InfoNCE损失函数相对于正样本嵌入的梯度，从而让你从根本上掌握其学习机制 。",
            "id": "77110",
            "problem": "在自主材料发现领域，深度学习模型越来越多地被用于分析原位表征实验的数据，例如薄膜结晶过程中收集的时间序列椭偏数据。其目标是学习一种能够捕捉材料相变潜在动力学路径的表示。\n\n一种常见的方法是使用自监督对比学习。一个编码器网络 $f_\\theta$ 将来自椭偏仪在时间 $t$ 的高维数据向量 $x_t$ 映射到 $\\mathbb{R}^D$ 中的一个低维嵌入向量 $z_t = f_\\theta(x_t)$。为确保表示具有良好的性质，嵌入向量经过L2归一化，使得 $\\|z_t\\|_2 = 1$。\n\n学习过程由InfoNCE（噪声对比估计）损失函数指导。对于一个给定的“锚点”嵌入 $z_i$，我们从时间序列中的一个邻近点选择一个“正”样本 $z_j$，它代表一个相似的材料状态。我们还从较远的时间点选择一组 $N-1$ 个“负”样本 $\\{z_k\\}_{k=1}^{N-1}$，它们代表不相似的状态。\n\n锚点 $z_i$ 和正样本 $z_j$ 的InfoNCE损失由下式给出：\n$$\nL(z_i, z_j, \\{z_k\\}_{k=1}^{N-1}) = -\\log \\left[ \\frac{\\exp(z_i \\cdot z_j / \\tau)}{\\exp(z_i \\cdot z_j / \\tau) + \\sum_{k=1}^{N-1} \\exp(z_i \\cdot z_k / \\tau)} \\right]\n$$\n这里，$z_i \\cdot z_j$ 是点积，作为归一化向量之间的相似度分数。参数 $\\tau > 0$ 是一个标量温度，用于控制分布的锐度。\n\n网络参数 $\\theta$ 使用梯度下降进行更新，这需要计算损失函数相对于网络输出（即嵌入向量）的梯度。你的任务是推导该损失函数相对于**正样本的嵌入向量** $z_j$ 的梯度。\n\n推导 $\\nabla_{z_j} L(z_i, z_j, \\{z_k\\}_{k=1}^{N-1})$ 的解析表达式。",
            "solution": "1. 定义相似度分数：\n   $$s_{ij} = \\frac{z_i \\cdot z_j}{\\tau},\\quad s_{ik} = \\frac{z_i \\cdot z_k}{\\tau}.$$  \n2. 写出损失函数：\n   $$L = -\\log\\frac{e^{s_{ij}}}{e^{s_{ij}}+\\sum_{k=1}^{N-1}e^{s_{ik}}} = -s_{ij} + \\log\\Bigl(e^{s_{ij}}+\\sum_{k=1}^{N-1}e^{s_{ik}}\\Bigr).$$  \n3. 计算关于 $z_j$ 的梯度：\n   $$\\nabla_{z_j}(-s_{ij}) = -\\nabla_{z_j}\\frac{z_i\\cdot z_j}{\\tau} = -\\frac{1}{\\tau}z_i,$$  \n   $$\\nabla_{z_j}\\log\\Bigl(e^{s_{ij}}+\\sum_{k}e^{s_{ik}}\\Bigr)\n     = \\frac{1}{e^{s_{ij}}+\\sum_{k}e^{s_{ik}}}\\,\\nabla_{z_j}e^{s_{ij}}\n     = \\frac{e^{s_{ij}}}{e^{s_{ij}}+\\sum_{k}e^{s_{ik}}}\\,\\frac{1}{\\tau}z_i.$$  \n4. 合并各项：\n   $$\\nabla_{z_j}L\n     = -\\frac{1}{\\tau}z_i + \\frac{e^{s_{ij}}}{\\tau\\bigl(e^{s_{ij}}+\\sum_{k}e^{s_{ik}}\\bigr)}\\,z_i\n     = \\frac{1}{\\tau}\\Bigl(\\frac{e^{s_{ij}}}{e^{s_{ij}}+\\sum_{k}e^{s_{ik}}}-1\\Bigr)z_i.$$",
            "answer": "$$\\boxed{\\nabla_{z_j}L=\\frac{1}{\\tau}\\Bigl(\\frac{\\exp\\bigl(z_i\\cdot z_j/\\tau\\bigr)}{\\exp\\bigl(z_i\\cdot z_j/\\tau\\bigr)+\\sum_{k=1}^{N-1}\\exp\\bigl(z_i\\cdot z_k/\\tau\\bigr)}-1\\Bigr)z_i}$$"
        },
        {
            "introduction": "在理论之外，训练自监督模型的过程并非总是一帆风顺，其中一个常见的陷阱是“表示坍塌” (representation collapse)：模型学到了一个平凡解，虽然取得了极低的损失值，但其学到的表示对于下游任务却毫无价值。优秀的实践者必须能够通过解读训练过程中的信号（如学习曲线）来诊断这类问题。本练习模拟了一个真实的诊断场景，你将通过分析训练损失和验证准确率曲线，来识别表示坍塌的迹象并选择恰当的修正策略 。",
            "id": "3115515",
            "problem": "一个自监督对比学习系统在一个大型、无标签的图像数据集上进行训练，使用标准的编码器和线性探针评估协议。设 $L(t)$ 表示在第 $t$ 个周期（epoch）时的训练对比损失，作为在小批量（mini-batches）上计算的经验风险；设 $A_{\\mathrm{val}}(t)$ 表示在第 $t$ 个周期后，在冻结的表示（frozen representations）上训练的线性分类器的下游验证准确率。假设使用恒定学习率 $\\eta$ 的随机梯度下降法，批量大小和所有优化器超参数随时间保持不变，并且在预训练期间不使用任何有标签的数据。编码器参数根据更新规则 $\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L(\\theta_t)$ 进行演化。\n\n根据经验，观察到以下学习曲线：\n- 从周期 $t = 1$ 到 $t = 24$，$L(t)$ 从约 $4.2$ 逐渐下降到约 $1.9$。\n- 在周期 $t = 25$ 时，$L(t)$ 急剧下降至约 $0.06$，并在 $t \\in [26, 30]$ 期间保持在约 $0.03$ 和约 $0.08$ 之间。\n- 与此同时，$A_{\\mathrm{val}}(t)$ 从 $t = 1$ 时约 $42\\%$ 增加到 $t = 20$ 时约 $58\\%$，然后在 $t \\in [20, 30]$ 期间停滞在约 $57\\%$ 和约 $58\\%$ 之间，尽管在 $t=25$ 之后 $L(t)$ 急剧下降。\n- 线性探针的训练准确率 $A_{\\mathrm{train}}(t)$ 从 $t = 1$ 时的约 $45\\%$ 持续上升到 $t = 30$ 时的约 $85\\%$。\n\n仅使用这些学习曲线提供的信息，并从基本定义——$L(t)$ 的经验风险最小化和由 $A_{\\mathrm{val}}(t)$ 衡量的下游泛化能力——出发，选择最合理的诊断和最正当的纠正措施，以减少退化表示（degenerate representations）的风险。您的选择应基于以下推理：对比目标如何在不改善下游语义的情况下被最小化，以及为什么特定的曲线形状（损失突然下降而没有下游增益）预示着这种风险。\n\nA. 系统表现出退化的对齐-均匀性失衡（表示崩溃风险）：正样本变得微不足道地相似，而负样本不够多样化，这使得 $L(t)$ 在不增加语义内容的情况下骤降。措施：加强数据增强并增加对比温度 $\\tau$（和/或批量大小）以强调负样本间的均匀性；监控嵌入协方差以确认。\n\nB. 系统明显欠拟合：编码器容量不足，因此无法在提高 $A_{\\mathrm{val}}(t)$ 的同时平滑地降低 $L(t)$。措施：增加编码器的深度和宽度以提升容量。\n\nC. 下游线性探针在过拟合有标签数据：$A_{\\mathrm{train}}(t)$ 上升而 $A_{\\mathrm{val}}(t)$ 停滞。措施：对探针应用早停和更强的正则化，保持预训练不变。\n\nD. 优化器在周期 $t = 25$ 附近导致梯度消失，这解释了损失的急剧下降和 $A_{\\mathrm{val}}(t)$ 的停滞。措施：更换为不同的激活函数和优化器以恢复梯度流，不改变数据增强。",
            "solution": "我们从一个基本出发点开始：训练对比损失 $L(t)$ 是一种经验风险，其在梯度下降更新 $\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta L(\\theta_t)$ 下的减少反映了在自监督代理任务目标上的改进，而下游验证准确率 $A_{\\mathrm{val}}(t)$ 则衡量了所学表示对于一个独立任务的泛化效用。在自监督对比学习中，其目标是鼓励正样本对（同一实例的两个增强视图）的表示比负样本对（不同实例）的表示更近，而整体行为由对齐（将正样本拉近）和均匀性（将所有点推开以避免集中）之间的平衡所支配。\n\n对于学习曲线，一个关键的诊断原则是，在良好行为的训练下，如果表示的增益是语义性的且非退化的，那么代理任务损失 $L(t)$ 的改善应该与下游 $A_{\\mathrm{val}}(t)$ 相关。相反，$L(t)$ 的急剧下降若没有伴随着 $A_{\\mathrm{val}}(t)$ 的改善——或者伴随着停滞——则表明模型找到了一种最小化代理任务目标的方法，但这种方法并没有用与任务相关的信息来丰富表示。在对比框架中，当正样本变得过于相似（例如，由于弱的或设计不佳的增强）并且负样本不够多样化（例如，由于小批量大小或低的对比温度 $\\tau$ 降低了对小实例间距离的惩罚）时，通常会表现出这种退化，通过微不足道的对齐而没有足够的均匀性来产生低的损失值。这是一种表示崩溃风险，体现在信息内容减少和分布不佳（即使不是字面意义上崩溃到单个向量，表示也可能沿着少数几个方向集中），这与观察到的曲线是一致的：$L(t)$ 突然下降到接近零，而 $A_{\\mathrm{val}}(t)$ 在大约 $58\\%$ 附近停滞，此后没有改善。\n\n让我们从量化和概念上分析所提供的观察结果：\n- 从 $t = 1$ 到 $t = 24$，$L(t)$ 从约 $4.2$ 下降到约 $1.9$，这是代理任务目标上的逐步改善。在同一窗口期，$A_{\\mathrm{val}}(t)$ 从约 $42\\%$ 增加到约 $58\\%$，这与学习到非平凡特征是一致的。\n- 在 $t = 25$ 时，$L(t)$ 急剧下降至约 $0.06$，此后保持在约 $0.03$ 和约 $0.08$ 之间。如果这种剧烈的损失减少反映了真正改善的语义，人们会期望 $A_{\\mathrm{val}}(t)$ 会增加。然而，$A_{\\mathrm{val}}(t)$ 却在约 $57\\%$ 到 $58\\%$ 之间停滞，表明没有下游增益。\n- 与此同时，$A_{\\mathrm{train}}(t)$ 上升到约 $85\\%$，这表明探针可以在学习到的表示上拟合训练数据，但这并未转化为验证集上的增益。这种差异指向表示是瓶颈，而不仅仅是探针过拟合，因为 $A_{\\mathrm{val}}(t)$ 在损失急剧下降之前就停止了改善，并且没有从随后的近零损失中受益。\n\n从第一性原理出发，最小化 $L(t)$ 可以通过过度强调对齐——使正样本对在嵌入空间中几乎相同——而不强制足够的实例间均匀性来实现。当数据增强太弱（正样本视图太相似）、温度 $\\tau$ 太低（过度锐化相似度分数）或有效负样本数量太少（批量小）时，可能会发生这种情况，使得模型能够轻易地在代理任务目标上取得成功，同时丢弃了广泛的语义分离。在表示几何方面，这会产生集中的嵌入，其在许多方向上的协方差减小，从而降低了由 $A_{\\mathrm{val}}(t)$ 衡量的泛化能力。因此，曲线的形状——$L(t)$ 突然下降而没有伴随 $A_{\\mathrm{val}}(t)$ 的增加——是类似崩溃风险的一个标志。\n\n逐项分析：\n\nA. 该选项指出了一个退化的对齐-均匀性失衡，并将其标记为表示崩溃风险。它解释了 $L(t)$ 的急剧下降而没有下游改善的原因是正样本变得微不足道地相似且负样本多样性不足，这与对齐和均匀性的定义以及关联 $L(t)$ 和 $A_{\\mathrm{val}}(t)$ 的诊断原则一致。所提出的措施——加强数据增强和增加对比温度 $\\tau$（和/或批量大小）以增强均匀性压力——直接针对了那些能够产生低 $L(t)$ 而无语义增益的机制。监控嵌入协方差进一步测试了集中现象，这是崩溃风险的一个标志。该推理与观察到的曲线和对比学习的基本行为相符。结论：正确。\n\nB. 欠拟合会表现为高 $L(t)$ 和低 $A_{\\mathrm{val}}(t)$，在增加容量时会有所改善。这里，$L(t)$ 极低（约 $0.03$ 至 $0.08$），表明模型很容易优化代理任务目标，并且 $A_{\\mathrm{val}}(t)$ 在停滞之前曾有所改善。增加容量不会解决目标被最小化而没有语义增益的不匹配问题；它甚至可能加剧平凡解。结论：不正确。\n\nC. 当 $A_{\\mathrm{train}}(t)$ 上升而 $A_{\\mathrm{val}}(t)$ 下降时，表明探针过拟合，而表示质量在其他方面是足够的。然而，关键事件是在 $t = 25$ 时 $L(t)$ 的急剧下降，而 $A_{\\mathrm{val}}(t)$ 没有改善。平台期在损失崩溃之前就已存在并贯穿其中，指向表示问题而非纯粹的探针正则化问题。早停或更强的探针正则化可能会略微减小训练-验证差距，但它们没有解决根本原因：表示携带的语义信息不足。结论：不正确。\n\nD. 梯度消失通常会使优化停滞（损失下降更慢或停滞），而不是产生急剧的损失下降至近零。此外，在优化器和超参数固定的情况下，伴随着停滞的 $A_{\\mathrm{val}}(t)$ 的 $L(t)$ 突然下降不能用激活饱和来解释；更换激活函数或优化器并没有针对曲线所预示的对齐-均匀性失衡问题。结论：不正确。\n\n因此，基于学习曲线形状的最合理解释和纠正措施在选项A中给出。",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}