{
    "hands_on_practices": [
        {
            "introduction": "Federated Learning relies on the participation of many devices, each with its own resource limitations. This first exercise zooms in on a single client to explore a fundamental trade-off in on-device training. You will analyze how the choice of mini-batch size, $B$, impacts the statistical quality of the local gradient estimate while being constrained by practical memory, latency, and energy budgets . Mastering this balance is the first step toward designing efficient and feasible federated systems.",
            "id": "3124684",
            "problem": "A single mobile device participates in Federated Learning (FL) by computing a local gradient estimate $\\hat{g}$ for one scalar model parameter using a mini-batch of size $B$ of independent and identically distributed data samples from its local dataset. Let each per-sample gradient be a scalar random variable $g_{i}$ with mean $\\mathbb{E}[g_{i}] = g$ and variance $\\mathrm{Var}(g_{i}) = \\sigma^{2}$, and assume the $g_{i}$ are independent. The device aims to choose $B$ as large as possible to reduce the variance of $\\hat{g}$, subject to resource constraints for a single training round:\n\n- Memory budget: the device has $M = 80$ MiB of free memory available for the training round. The loaded model occupies $m_{\\mathrm{model}} = 40$ MiB, the runtime overhead (activations, framework buffers) occupies $m_{\\mathrm{ov}} = 10$ MiB, and each data sample requires $m_{s} = 1$ MiB of memory while processing. Assume memory usage scales linearly in $B$ and that all quantities must be held simultaneously in memory.\n- Latency budget: each data sample requires $t_{s} = 3$ ms of compute time, and the device must finish the local computation within $T_{\\max} = 120$ ms.\n- Energy budget: each data sample requires $e_{s} = 7$ mJ of energy, and the device has $E_{\\max} = 250$ mJ available for the round.\n\nAssume the device processes samples sequentially, $B$ must be a positive integer, and ignore any parallelism or pipeline effects. Also assume that no additional noise (such as quantization or differential privacy) is introduced, so that the only source of randomness in $\\hat{g}$ is the sampling of the mini-batch.\n\nStarting from fundamental definitions of the sample mean and the properties of variance for independent random variables, derive an expression for $\\mathrm{Var}(\\hat{g})$ as a function of $B$. Then, given $\\sigma^{2} = 0.81$, determine the largest feasible $B$ that satisfies all three device constraints and compute the numerical value of $\\mathrm{Var}(\\hat{g})$ at that $B$. Express the final variance as a pure number without units. No rounding is required; report the exact value.",
            "solution": "The problem asks for two main results: first, a derivation of the variance of a gradient estimate, $\\mathrm{Var}(\\hat{g})$, as a function of the mini-batch size, $B$; second, the numerical value of this variance given a specific set of resource constraints that limit $B$.\n\nFirst, we derive the expression for $\\mathrm{Var}(\\hat{g})$. The local gradient estimate, $\\hat{g}$, is defined as the sample mean of $B$ independent and identically distributed (i.i.d.) per-sample gradients, denoted by $g_i$ for $i \\in \\{1, 2, \\dots, B\\}$.\n$$ \\hat{g} = \\frac{1}{B} \\sum_{i=1}^{B} g_i $$\nEach $g_i$ is a random variable with mean $\\mathbb{E}[g_{i}] = g$ and variance $\\mathrm{Var}(g_{i}) = \\sigma^{2}$. We can find the variance of $\\hat{g}$ by applying the properties of the variance operator.\n\nFor a constant $c$ and a random variable $X$, the variance operator follows the rule $\\mathrm{Var}(cX) = c^2 \\mathrm{Var}(X)$. In our case, the constant is $\\frac{1}{B}$.\n$$ \\mathrm{Var}(\\hat{g}) = \\mathrm{Var}\\left(\\frac{1}{B} \\sum_{i=1}^{B} g_i\\right) = \\left(\\frac{1}{B}\\right)^2 \\mathrm{Var}\\left(\\sum_{i=1}^{B} g_i\\right) = \\frac{1}{B^2} \\mathrm{Var}\\left(\\sum_{i=1}^{B} g_i\\right) $$\nFor a sum of independent random variables, the variance of the sum is equal to the sum of their individual variances. The problem states that the $g_i$ are independent.\n$$ \\mathrm{Var}\\left(\\sum_{i=1}^{B} g_i\\right) = \\sum_{i=1}^{B} \\mathrm{Var}(g_i) $$\nSince the samples are also identically distributed, the variance of each $g_i$ is the same: $\\mathrm{Var}(g_i) = \\sigma^2$.\n$$ \\sum_{i=1}^{B} \\mathrm{Var}(g_i) = \\sum_{i=1}^{B} \\sigma^2 = B \\sigma^2 $$\nSubstituting this result back into the expression for $\\mathrm{Var}(\\hat{g})$ yields the desired relationship.\n$$ \\mathrm{Var}(\\hat{g}) = \\frac{1}{B^2} (B \\sigma^2) = \\frac{\\sigma^2}{B} $$\n\nNext, we must determine the largest feasible integer value for the mini-batch size, $B$, subject to the three given resource constraints.\n\n1.  Memory Constraint: The total memory consumed must not exceed the available memory, $M = 80$ MiB. The total memory is the sum of the fixed model and overhead memory, $m_{\\mathrm{model}} = 40$ MiB and $m_{\\mathrm{ov}} = 10$ MiB, and the variable memory per data sample, $m_{s} = 1$ MiB, scaled by the batch size $B$.\n    $$ m_{\\mathrm{model}} + m_{\\mathrm{ov}} + B \\cdot m_{s} \\le M $$\n    Substituting the numeric values (units are consistent and can be omitted for the inequality):\n    $$ 40 + 10 + B \\cdot 1 \\le 80 $$\n    $$ 50 + B \\le 80 $$\n    $$ B \\le 30 $$\n\n2.  Latency Constraint: The total computation time must be within the maximum allowed latency, $T_{\\max} = 120$ ms. The total time is the per-sample compute time, $t_{s} = 3$ ms, multiplied by the batch size $B$.\n    $$ B \\cdot t_{s} \\le T_{\\max} $$\n    Substituting the numeric values:\n    $$ B \\cdot 3 \\le 120 $$\n    $$ B \\le \\frac{120}{3} $$\n    $$ B \\le 40 $$\n\n3.  Energy Constraint: The total energy consumed must not exceed the energy budget, $E_{\\max} = 250$ mJ. The total energy is the per-sample energy consumption, $e_{s} = 7$ mJ, multiplied by the batch size $B$.\n    $$ B \\cdot e_{s} \\le E_{\\max} $$\n    Substituting the numeric values:\n    $$ B \\cdot 7 \\le 250 $$\n    $$ B \\le \\frac{250}{7} $$\n    Since $B$ must be a positive integer, we must take the floor of this value: $B \\le \\lfloor \\frac{250}{7} \\rfloor = \\lfloor 35.714\\dots \\rfloor = 35$.\n    $$ B \\le 35 $$\n\nTo satisfy all three constraints simultaneously, $B$ must be less than or equal to the minimum of the three derived upper bounds.\n$$ B \\le \\min(30, 40, 35) $$\n$$ B \\le 30 $$\nThe problem asks for the largest possible $B$, so we select the maximum integer value that satisfies this condition, which is $B = 30$.\n\nFinally, we compute the numerical value of $\\mathrm{Var}(\\hat{g})$ using the derived formula $\\mathrm{Var}(\\hat{g}) = \\frac{\\sigma^2}{B}$, the determined batch size $B = 30$, and the given per-sample variance $\\sigma^2 = 0.81$.\n$$ \\mathrm{Var}(\\hat{g}) = \\frac{0.81}{30} $$\nThis can be computed exactly as a decimal fraction:\n$$ \\mathrm{Var}(\\hat{g}) = \\frac{81}{100} \\div 30 = \\frac{81}{100} \\times \\frac{1}{30} = \\frac{81}{3000} = \\frac{27 \\times 3}{1000 \\times 3} = \\frac{27}{1000} = 0.027 $$",
            "answer": "$$\\boxed{0.027}$$"
        },
        {
            "introduction": "Moving from a single device to the entire network, this practice tackles a core design challenge in Federated Learning: the trade-off between computation and communication. You will investigate how to optimally balance the number of local training epochs, $E$, against the number of communication rounds, $R$, to minimize the total training time. This exercise uses a mathematical model that captures both the benefits of more computation and the negative effects of 'client drift' to help you find the sweet spot for a given accuracy target and resource budget .",
            "id": "3124716",
            "problem": "Consider a Federated Averaging (FedAvg) system optimizing a strongly convex empirical risk with Lipschitz-continuous gradient using Stochastic Gradient Descent (SGD) locally on clients. There are $K = 10$ clients participating in each communication round, each client processes $n = 10$ local examples per local epoch. Assume perfect parallelism across clients so that the wall-clock time per round can be modeled as the sum of a local computation term and a communication term. Specifically, the per-round time is $T_{\\text{round}}(E) = E\\, t_{\\text{comp}} + t_{\\text{comm}}$, where $t_{\\text{comp}} = 0.2$ and $t_{\\text{comm}} = 2$. The total wall-clock time over $R$ rounds with $E$ local epochs per round is $T_{\\text{total}}(R,E) = R\\left(E\\, t_{\\text{comp}} + t_{\\text{comm}}\\right)$.\n\nUnder standard smoothness and strong convexity assumptions for SGD, the optimization error of centralized SGD after $T$ stochastic updates scales as $O\\!\\left(\\frac{1}{T}\\right)$. In federated learning, client drift due to heterogeneous data introduces a bias that grows with the number of local steps per round. A simple, well-tested upper bound that captures these two effects is of the form\n$$\\mathcal{E}(R,E) \\le \\frac{A}{R E} + B E,$$\nwhere $A > 0$ captures the initial optimality gap and stochasticity for the decay term, and $B > 0$ captures the client-drift-induced bias per local epoch. In this problem, take $A = 4$ and $B = 0.01$.\n\nYou are given budgets that constrain the total number of local example updates and the total bandwidth. The total local example updates over the entire training are $K n E R$, which must satisfy $K n E R \\le \\mathcal{C}_{\\max}$ with $\\mathcal{C}_{\\max} = 50{,}000$. The total bandwidth used is proportional to the number of rounds: each round transmits models of aggregate size $S_{\\text{round}} = K S_{\\text{model}}$, with $S_{\\text{model}} = 1$ (normalized units), and the total bandwidth budget satisfies $R S_{\\text{round}} \\le \\mathcal{B}_{\\max}$ with $\\mathcal{B}_{\\max} = 800$. Thus, the budgets reduce to the constraints $R E \\le 500$ and $R \\le 80$.\n\nThe target optimization error is $\\epsilon = 0.1$. Starting from the foundational facts above, derive the choice of $E$ and $R$ that minimizes the total wall-clock time $T_{\\text{total}}(R,E)$ subject to the constraints\n$$\\frac{A}{R E} + B E \\le \\epsilon,\\quad R E \\le 500,\\quad R \\le 80,$$\nwith $A = 4$, $B = 0.01$, and $\\epsilon = 0.1$. Express your final answer as an exact analytical pair $\\left(E^{\\star}, R^{\\star}\\right)$; do not round. Your answer must be a single expression using the LaTeX row-matrix format.",
            "solution": "The problem asks us to find the number of local epochs $E$ and the number of communication rounds $R$ that minimize the total wall-clock time $T_{\\text{total}}(R,E)$ subject to a set of constraints on the optimization error and resource budgets.\n\nFirst, we formalize the optimization problem using the provided parameters.\nThe objective function to minimize is the total wall-clock time:\n$$T_{\\text{total}}(R,E) = R(E\\, t_{\\text{comp}} + t_{\\text{comm}})$$\nSubstituting the given values $t_{\\text{comp}} = 0.2$ and $t_{\\text{comm}} = 2$, we get:\n$$T(R,E) = R(0.2E + 2)$$\n\nThe minimization is subject to three constraints:\n1.  The optimization error constraint: $\\frac{A}{RE} + B E \\le \\epsilon$. With $A=4$, $B=0.01$, and $\\epsilon=0.1$, this becomes:\n    $$\\frac{4}{RE} + 0.01E \\le 0.1$$\n2.  The total local example updates constraint: $RE \\le 500$.\n3.  The total bandwidth constraint: $R \\le 80$.\n\nAdditionally, the variables $R$ and $E$ must be positive, as they represent the number of rounds and epochs, respectively. We treat them as continuous positive variables for the optimization.\n\nThe objective function $T(R,E)$ is a strictly increasing function of both $R$ and $E$ for $R > 0$ and $E > 0$. To minimize $T(R,E)$, we must find the point $(R,E)$ in the feasible region that is, in some sense, \"closest\" to the origin. This implies that the optimal solution $(R^{\\star}, E^{\\star})$ will not be in the interior of the feasible region (where all inequality constraints are strictly satisfied), because we could always reduce $R$ or $E$ slightly to obtain a smaller value of $T(R,E)$ without leaving the feasible region. Therefore, the minimum must lie on the boundary of the feasible region, meaning at least one of the constraints must be satisfied with equality.\n\nGiven that $T(R,E)$ increases with both variables, the solution is likely to lie on the boundary defined by the error constraint, $\\frac{4}{RE} + 0.01E = 0.1$. If the solution were in a region where $\\frac{4}{RE} + 0.01E < 0.1$, we could potentially decrease $R$ or $E$ to lower the time, so we assume the error constraint is active at the optimum:\n$$\\frac{4}{RE} + 0.01E = 0.1$$\n\nFrom this equation, we can express $R$ as a function of $E$. For $R$ to be a positive, finite value, we must have $0.1 - 0.01E > 0$, which implies $0.01E < 0.1$, or $E < 10$. So, the domain for $E$ on this boundary is $E \\in (0, 10)$.\nRearranging the equality for $R$:\n$$\\frac{4}{RE} = 0.1 - 0.01E$$\n$$RE = \\frac{4}{0.1 - 0.01E}$$\n$$R(E) = \\frac{4}{E(0.1 - 0.01E)} = \\frac{4}{0.1E - 0.01E^2}$$\n\nNow, we substitute this expression for $R$ into the objective function $T(R,E)$ to obtain a function of a single variable, $E$:\n$$T(E) = R(E)(0.2E + 2) = \\left(\\frac{4}{0.1E - 0.01E^2}\\right)(0.2E + 2)$$\n$$T(E) = \\frac{0.8E + 8}{0.1E - 0.01E^2}$$\n\nTo find the value of $E$ that minimizes $T(E)$ on the interval $(0, 10)$, we compute the derivative of $T(E)$ with respect to $E$ and set it to zero. Using the quotient rule, $\\frac{d}{dE}\\left(\\frac{u}{v}\\right) = \\frac{u'v - uv'}{v^2}$, with $u(E) = 0.8E + 8$ and $v(E) = 0.1E - 0.01E^2$:\n$$u'(E) = 0.8$$\n$$v'(E) = 0.1 - 0.02E$$\nThe derivative $T'(E)$ is zero when its numerator is zero:\n$$u'v - uv' = 0$$\n$$0.8(0.1E - 0.01E^2) - (0.8E + 8)(0.1 - 0.02E) = 0$$\nExpanding the terms:\n$$(0.08E - 0.008E^2) - (0.08E - 0.016E^2 + 0.8 - 0.16E) = 0$$\n$$0.08E - 0.008E^2 - 0.08E + 0.016E^2 - 0.8 + 0.16E = 0$$\nCombining like terms:\n$$0.008E^2 + 0.16E - 0.8 = 0$$\nTo simplify, multiply by $1000$:\n$$8E^2 + 160E - 800 = 0$$\nDivide by $8$:\n$$E^2 + 20E - 100 = 0$$\nWe solve this quadratic equation for $E$ using the quadratic formula, $E = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$E = \\frac{-20 \\pm \\sqrt{20^2 - 4(1)(-100)}}{2(1)} = \\frac{-20 \\pm \\sqrt{400 + 400}}{2} = \\frac{-20 \\pm \\sqrt{800}}{2}$$\nSimplifying the square root: $\\sqrt{800} = \\sqrt{400 \\times 2} = 20\\sqrt{2}$.\n$$E = \\frac{-20 \\pm 20\\sqrt{2}}{2} = -10 \\pm 10\\sqrt{2}$$\nSince $E$ must be positive, we take the positive root:\n$$E^{\\star} = -10 + 10\\sqrt{2} = 10(\\sqrt{2} - 1)$$\nThis value is in the domain $(0, 10)$ since $1 < \\sqrt{2} < 2$.\n\nNow we find the corresponding value of $R$ using the expression for $R(E)$:\n$$R^{\\star} = \\frac{4}{0.1E^{\\star} - 0.01(E^{\\star})^2}$$\nFrom the quadratic equation $E^2 + 20E - 100 = 0$, we have $(E^{\\star})^2 = 100 - 20E^{\\star}$. Substituting this:\n$$R^{\\star} = \\frac{4}{0.1E^{\\star} - 0.01(100 - 20E^{\\star})} = \\frac{4}{0.1E^{\\star} - 1 + 0.2E^{\\star}} = \\frac{4}{0.3E^{\\star} - 1}$$\nSubstitute the value of $E^{\\star} = 10(\\sqrt{2} - 1)$:\n$$R^{\\star} = \\frac{4}{0.3(10(\\sqrt{2} - 1)) - 1} = \\frac{4}{3(\\sqrt{2} - 1) - 1} = \\frac{4}{3\\sqrt{2} - 3 - 1} = \\frac{4}{3\\sqrt{2} - 4}$$\nTo rationalize the denominator, we multiply the numerator and denominator by the conjugate, $3\\sqrt{2} + 4$:\n$$R^{\\star} = \\frac{4(3\\sqrt{2} + 4)}{(3\\sqrt{2} - 4)(3\\sqrt{2} + 4)} = \\frac{4(3\\sqrt{2} + 4)}{(3\\sqrt{2})^2 - 4^2} = \\frac{4(3\\sqrt{2} + 4)}{18 - 16} = \\frac{4(3\\sqrt{2} + 4)}{2}$$\n$$R^{\\star} = 2(3\\sqrt{2} + 4) = 6\\sqrt{2} + 8$$\n\nThe candidate optimal pair is $(E^{\\star}, R^{\\star}) = (10(\\sqrt{2} - 1), 8 + 6\\sqrt{2})$.\nWe must verify that this solution satisfies the remaining two constraints:\n1.  Check $RE \\le 500$:\n    $$R^{\\star}E^{\\star} = (8 + 6\\sqrt{2}) \\times 10(\\sqrt{2} - 1) = 10(8\\sqrt{2} - 8 + 6(\\sqrt{2})^2 - 6\\sqrt{2})$$\n    $$= 10(2\\sqrt{2} + 12 - 8) = 10(4 + 2\\sqrt{2}) = 20(2 + \\sqrt{2})$$\n    Since $\\sqrt{2} \\approx 1.414$, $R^{\\star}E^{\\star} \\approx 20(3.414) = 68.28$. This is well below $500$.\n2.  Check $R \\le 80$:\n    $$R^{\\star} = 8 + 6\\sqrt{2} \\approx 8 + 6(1.414) = 8 + 8.484 = 16.484$$\n    This is well below $80$.\n\nBoth constraints are satisfied. The point $(E^{\\star}, R^{\\star})$ minimizes the time $T(R,E)$ along the boundary $\\frac{4}{RE} + 0.01E = 0.1$. Since this point lies strictly within the regions defined by the other two constraints, and moving to the boundary of these other constraints would require moving away from this minimum along the primary boundary (thus increasing time), this point represents the global minimum for the constrained problem.\n\nThe optimal choice of $E$ and $R$ is therefore $(E^{\\star}, R^{\\star}) = (10(\\sqrt{2} - 1), 8 + 6\\sqrt{2})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n10(\\sqrt{2} - 1) & 8 + 6\\sqrt{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Standard Federated Learning trains a single global model, which can be a poor compromise when client data is highly diverse (non-IID). This advanced, hands-on coding practice explores a powerful solution: personalized federated learning through client clustering. You will implement a system that groups clients based on the similarity of their gradient directions—a clever proxy for their underlying data distributions—and trains a specialized model for each cluster, measuring its improvement over the one-size-fits-all approach .",
            "id": "3124737",
            "problem": "Consider a federated learning setting with $C$ clients, each holding local data sampled independently from a linear model with additive noise. Each client $i$ has $n_i$ samples $\\{(x^{(i)}_t, y^{(i)}_t)\\}_{t=1}^{n_i}$ where $x^{(i)}_t \\in \\mathbb{R}^d$ is drawn i.i.d. from a zero-mean Gaussian distribution with identity covariance, and $y^{(i)}_t = (x^{(i)}_t)^\\top w^*_i + \\epsilon^{(i)}_t$ with $\\epsilon^{(i)}_t \\sim \\mathcal{N}(0, \\sigma^2)$. We train linear models $f_w(x) = x^\\top w$ with the empirical Mean Squared Error (MSE) loss on client $i$ given by\n$$\nL_i(w) = \\frac{1}{n_i} \\sum_{t=1}^{n_i} \\left((x^{(i)}_t)^\\top w - y^{(i)}_t\\right)^2.\n$$\nThe gradient of $L_i(w)$ with respect to $w$ is denoted $g_i(w) = \\nabla_w L_i(w)$. We define cosine similarity between two nonzero gradients $g_i$ and $g_j$ as\n$$\n\\mathrm{cos}(g_i, g_j) = \\frac{g_i^\\top g_j}{\\|g_i\\|_2 \\, \\|g_j\\|_2}.\n$$\nWhen either gradient is the zero vector, define the cosine similarity to be $0$.\n\nYou must implement the following procedure from first principles:\n- Compute each client's gradient at the shared initialization $w_0 = 0$, i.e., $g_i(w_0)$.\n- Cluster clients into connected components using a threshold $\\tau$: create an undirected graph on clients with an edge between $i$ and $j$ if $\\mathrm{cos}(g_i(w_0), g_j(w_0)) \\ge \\tau$, then take graph connected components as clusters.\n- Train two federated systems for $T$ communication rounds using Federated Averaging (FedAvg). In each round, every client performs $E$ steps of batch gradient descent with step size (learning rate) $\\eta$ on its local data starting from the current model it receives from the server. The server aggregates client model deltas by a weighted average proportional to $n_i$.\n    - Single global model: one shared model $w$ updated by aggregating all clients.\n    - Cluster-specific models: one model per cluster; each cluster's model is updated by aggregating only the clients in that cluster.\n- Evaluate the client-mean empirical MSE after training for both systems. For the single global model, every client uses the global $w$; for cluster-specific models, client $i$ uses its cluster's $w$. The evaluation metric is the arithmetic mean of per-client empirical MSE:\n$$\n\\overline{L}_\\mathrm{global} = \\frac{1}{C} \\sum_{i=1}^{C} L_i(w_\\mathrm{global}), \\quad\n\\overline{L}_\\mathrm{cluster} = \\frac{1}{C} \\sum_{i=1}^{C} L_i(w_{\\mathrm{cluster}(i)}).\n$$\nCompute the relative improvement as the decimal\n$$\n\\Delta = \\frac{\\overline{L}_\\mathrm{global} - \\overline{L}_\\mathrm{cluster}}{\\overline{L}_\\mathrm{global}}.\n$$\nReport $\\Delta$ for each test case.\n\nFundamental base to be used:\n- Empirical risk minimization definition for $L_i(w)$.\n- Gradient descent update $w \\leftarrow w - \\eta \\, g_i(w)$.\n- Weighted averaging for aggregating client updates proportional to $n_i$.\n- Cosine similarity definition for clustering by gradient directions.\n\nImplement a complete, runnable program that:\n- Simulates data as specified.\n- Computes initial gradients.\n- Forms clusters by thresholded cosine similarity and connected components.\n- Trains with Federated Averaging for both the single global model and cluster-specific models.\n- Evaluates and outputs the relative improvement $\\Delta$ for each test case.\n\nTest suite and parameters:\n- Test Case $1$ (happy path, two opposing clusters):\n    - Random seed $42$, $C = 8$, $d = 5$, client sample sizes $n_i = 200$ for all $i$, noise standard deviation $\\sigma = 0.1$.\n    - Ground-truth vectors: $w^*_1 = [1, 0, 0, 0, 0]^\\top$ for clients $1$ to $4$, and $w^*_2 = [-1, 0, 0, 0, 0]^\\top$ for clients $5$ to $8$.\n    - Clustering threshold $\\tau = 0.7$.\n    - FedAvg hyperparameters: $T = 15$, $E = 5$, $\\eta = 0.05$.\n- Test Case $2$ (no improvement expected, homogeneous clients):\n    - Random seed $123$, $C = 8$, $d = 5$, client sample sizes $n_i = 200$ for all $i$, noise standard deviation $\\sigma = 0.1$.\n    - Ground-truth vectors: $w^*_i = [1, 1, 0, 0, 0]^\\top$ for all clients $i$.\n    - Clustering threshold $\\tau = 0.7$.\n    - FedAvg hyperparameters: $T = 15$, $E = 5$, $\\eta = 0.05$.\n- Test Case $3$ (boundary clustering with $\\tau = 1.0$, singleton clusters):\n    - Random seed $7$, $C = 2$, $d = 3$, client sample sizes $n_1 = 300$, $n_2 = 300$, noise standard deviation $\\sigma = 0.1$.\n    - Ground-truth vectors: $w^*_1 = [1, 0, 0]^\\top$, $w^*_2 = [0, 1, 0]^\\top$.\n    - Clustering threshold $\\tau = 1.0$.\n    - FedAvg hyperparameters: $T = 20$, $E = 5$, $\\eta = 0.05$.\n- Test Case $4$ (noisy edge case, potential clustering ambiguity):\n    - Random seed $99$, $C = 6$, $d = 4$, client sample sizes $n_i = 50$ for all $i$, noise standard deviation $\\sigma = 0.5$.\n    - Ground-truth vectors: $w^*_1 = [1, 0, 0, 0]^\\top$ for clients $1$ to $3$, and $w^*_2 = [0, 1, 0, 0]^\\top$ for clients $4$ to $6$.\n    - Clustering threshold $\\tau = 0.5$.\n    - FedAvg hyperparameters: $T = 25$, $E = 5$, $\\eta = 0.05$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the entries ordered by the four test cases described above, for example, $\\texttt{[r_1,r_2,r_3,r_4]}$ where each $r_k$ is the decimal relative improvement for Test Case $k$.",
            "solution": "The problem is deemed valid after a thorough review. It is scientifically grounded in the principles of federated learning, linear regression, and graph theory. It is well-posed, with all necessary parameters and conditions specified, ensuring a unique and reproducible outcome for each test case via fixed random seeds. The problem statement is objective and free of ambiguity or contradiction.\n\n### Theoretical Formulation\n\nThe core of this problem lies in addressing client heterogeneity in federated learning. When clients have different underlying data distributions or objectives, a single global model can be suboptimal for everyone. The proposed solution is to cluster clients based on their objectives and train a personalized model for each cluster.\n\n**1. Client Loss and Gradient**\nEach client $i$ seeks to minimize its local empirical Mean Squared Error (MSE) loss, given by:\n$$\nL_i(w) = \\frac{1}{n_i} \\sum_{t=1}^{n_i} \\left((x^{(i)}_t)^\\top w - y^{(i)}_t\\right)^2\n$$\nwhere $w \\in \\mathbb{R}^d$ is the model parameter vector. This is a convex optimization problem. The gradient of this loss function with respect to $w$, used for gradient descent, is:\n$$\ng_i(w) = \\nabla_w L_i(w) = \\frac{2}{n_i} \\sum_{t=1}^{n_i} \\left((x^{(i)}_t)^\\top w - y^{(i)}_t\\right)x^{(i)}_t\n$$\nIn matrix notation, with $X_i$ being the $n_i \\times d$ data matrix and $Y_i$ the $n_i \\times 1$ label vector, the gradient is:\n$$\ng_i(w) = \\frac{2}{n_i} X_i^\\top (X_i w - Y_i)\n$$\n\n**2. Gradient-Based Clustering Rationale**\nThe clustering heuristic is based on the initial gradients calculated at a shared starting point, $w_0 = 0$. At this initialization, the gradient for client $i$ simplifies to:\n$$\ng_i(w_0) = g_i(0) = -\\frac{2}{n_i} X_i^\\top Y_i\n$$\nSubstituting the true data-generating process, $y^{(i)}_t = (x^{(i)}_t)^\\top w^*_i + \\epsilon^{(i)}_t$, we get:\n$$\ng_i(0) = -\\frac{2}{n_i} \\sum_{t=1}^{n_i} x^{(i)}_t \\left((x^{(i)}_t)^\\top w^*_i + \\epsilon^{(i)}_t\\right) = -\\frac{2}{n_i} \\left( \\left(\\sum_{t=1}^{n_i} x^{(i)}_t (x^{(i)}_t)^\\top \\right) w^*_i + \\sum_{t=1}^{n_i} x^{(i)}_t \\epsilon^{(i)}_t \\right)\n$$\nBy the law of large numbers, for a sufficiently large number of samples $n_i$, the sample covariance matrix $\\frac{1}{n_i}\\sum_{t=1}^{n_i} x^{(i)}_t (x^{(i)}_t)^\\top$ approaches the true covariance matrix, which is the identity matrix $I_d$ since $x^{(i)}_t \\sim \\mathcal{N}(0, I_d)$. The noise term $\\frac{1}{n_i}\\sum_{t=1}^{n_i} x^{(i)}_t \\epsilon^{(i)}_t$ approaches its expectation, which is $E[x \\epsilon] = E[x]E[\\epsilon] = 0$. Therefore, we have the approximation:\n$$\ng_i(0) \\approx -2 (I_d w^*_i) = -2w^*_i\n$$\nThis reveals a critical insight: the initial gradient vector at the origin is approximately proportional to the negative of the client's true optimal parameter vector $w^*_i$. Consequently, the cosine similarity between the initial gradients of two clients, $i$ and $j$, approximates the cosine similarity of their true parameter vectors:\n$$\n\\mathrm{cos}(g_i(0), g_j(0)) \\approx \\mathrm{cos}(-2w^*_i, -2w^*_j) = \\mathrm{cos}(w^*_i, w^*_j)\n$$\nThus, clustering clients based on the cosine similarity of their initial gradients is a sound heuristic for grouping clients with similar underlying objectives ($w^*_i$).\n\n### Algorithmic Procedure\n\nThe solution is implemented by following the specified procedure from first principles.\n\n**1. Data Simulation**\nFor each client $i \\in \\{1, \\dots, C\\}$, we generate a local dataset.\n-   The feature vectors $\\{x^{(i)}_t\\}_{t=1}^{n_i}$ are drawn from $\\mathcal{N}(0, I_d)$.\n-   The labels $\\{y^{(i)}_t\\}_{t=1}^{n_i}$ are computed as $y^{(i)}_t = (x^{(i)}_t)^\\top w^*_i + \\epsilon^{(i)}_t$, where the noise terms $\\{\\epsilon^{(i)}_t\\}_{t=1}^{n_i}$ are drawn from $\\mathcal{N}(0, \\sigma^2)$.\nThe random seed is set for each test case to ensure reproducibility.\n\n**2. Client Clustering**\n-   Initialize the model parameters to the zero vector, $w_0 = 0$.\n-   For each client $i$, compute the initial gradient $g_i(w_0)$ using the formula derived above.\n-   Construct an undirected graph with $C$ vertices, representing the clients. An edge is placed between client $i$ and client $j$ if and only if their initial gradients are non-zero and their cosine similarity meets or exceeds the threshold $\\tau$:\n    $$\n    \\frac{g_i(w_0)^\\top g_j(w_0)}{\\|g_i(w_0)\\|_2 \\, \\|g_j(w_0)\\|_2} \\ge \\tau\n    $$\n    If either gradient is the zero vector, the similarity is defined as $0$ and no edge is created (unless $\\tau \\le 0$).\n-   The clusters are determined by finding the connected components of this graph. This is achieved by building an adjacency matrix and using the `scipy.sparse.csgraph.connected_components` function, which efficiently partitions the graph's vertices.\n\n**3. Federated Averaging (FedAvg) Training**\nWe simulate two separate training systems for $T$ communication rounds.\n\n-   **Single Global Model:** A single server manages one model, $w_{\\mathrm{global}}$, for all $C$ clients.\n    -   In each round, the server distributes $w_{\\mathrm{global}}$ to all clients.\n    -   Each client $i$ performs $E$ steps of local batch gradient descent, starting from $w_{\\mathrm{global}}$, using its local data and a learning rate $\\eta$.\n    -   The server aggregates the resulting updated local models into a new global model using a weighted average, where weights are proportional to client sample sizes $n_i$:\n        $$ w_{\\mathrm{global}} \\leftarrow \\frac{\\sum_{i=1}^C n_i w_i}{\\sum_{i=1}^C n_i} $$\n\n-   **Cluster-Specific Models:** For each identified cluster, an independent instance of FedAvg is run.\n    -   Each cluster has its own server and model, $w_{\\mathrm{cluster}(k)}$.\n    -   Training proceeds as with the global model, but aggregation for cluster $k$ only involves the clients belonging to that cluster. All models are initialized to $w_0 = 0$.\n\n**4. Evaluation**\nAfter $T$ rounds of training, the performance of both systems is evaluated.\n-   The client-mean empirical MSE for the global model, $\\overline{L}_\\mathrm{global}$, is calculated by averaging the final loss $L_i(w_\\mathrm{global})$ over all $C$ clients.\n-   The client-mean empirical MSE for the clustered system, $\\overline{L}_\\mathrm{cluster}$, is similarly calculated. For each client $i$, the loss $L_i(w_{\\mathrm{cluster}(i)})$ is computed using the final model of the cluster to which client $i$ belongs.\n-   The relative improvement $\\Delta$ is then computed as:\n    $$\n    \\Delta = \\frac{\\overline{L}_\\mathrm{global} - \\overline{L}_\\mathrm{cluster}}{\\overline{L}_\\mathrm{global}}\n    $$\nThis metric quantifies the performance gain achieved by clustering. A positive value indicates that the clustered approach yielded a lower average client loss.",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.csgraph import connected_components\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"seed\": 42, \"C\": 8, \"d\": 5, \"n_i\": [200] * 8, \"sigma\": 0.1,\n            \"w_star\": [np.array([1., 0, 0, 0, 0])] * 4 + [np.array([-1., 0, 0, 0, 0])] * 4,\n            \"tau\": 0.7, \"T\": 15, \"E\": 5, \"eta\": 0.05\n        },\n        {\n            \"seed\": 123, \"C\": 8, \"d\": 5, \"n_i\": [200] * 8, \"sigma\": 0.1,\n            \"w_star\": [np.array([1., 1, 0, 0, 0])] * 8,\n            \"tau\": 0.7, \"T\": 15, \"E\": 5, \"eta\": 0.05\n        },\n        {\n            \"seed\": 7, \"C\": 2, \"d\": 3, \"n_i\": [300, 300], \"sigma\": 0.1,\n            \"w_star\": [np.array([1., 0, 0]), np.array([0., 1, 0])],\n            \"tau\": 1.0, \"T\": 20, \"E\": 5, \"eta\": 0.05\n        },\n        {\n            \"seed\": 99, \"C\": 6, \"d\": 4, \"n_i\": [50] * 6, \"sigma\": 0.5,\n            \"w_star\": [np.array([1., 0, 0, 0])] * 3 + [np.array([0., 1, 0, 0])] * 3,\n            \"tau\": 0.5, \"T\": 25, \"E\": 5, \"eta\": 0.05\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(params)\n        results.append(result)\n\n    # Format output as [r1,r2,r3,r4] with 8 decimal places.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef generate_data(C, d, n_i, w_star, sigma, rng):\n    \"\"\"Generates client data based on the problem specification.\"\"\"\n    client_data = []\n    for i in range(C):\n        n = n_i[i]\n        X = rng.normal(0, 1, size=(n, d))\n        epsilon = rng.normal(0, sigma, size=n)\n        y = X @ w_star[i] + epsilon\n        client_data.append({'X': X, 'y': y})\n    return client_data\n\ndef compute_gradient(client_data, w):\n    \"\"\"Computes the gradient of the MSE loss for a single client.\"\"\"\n    X, y = client_data['X'], client_data['y']\n    n = len(y)\n    if n == 0:\n        return np.zeros_like(w)\n    grad = 2/n * X.T @ (X @ w - y)\n    return grad\n\ndef find_clusters(initial_gradients, tau, C):\n    \"\"\"Forms clusters based on cosine similarity of initial gradients.\"\"\"\n    adj_matrix = np.zeros((C, C))\n    for i in range(C):\n        for j in range(i + 1, C):\n            g_i = initial_gradients[i]\n            g_j = initial_gradients[j]\n            norm_i = np.linalg.norm(g_i)\n            norm_j = np.linalg.norm(g_j)\n            \n            cosine_sim = 0.0\n            if norm_i > 0 and norm_j > 0:\n                cosine_sim = np.dot(g_i, g_j) / (norm_i * norm_j)\n            \n            if cosine_sim >= tau:\n                adj_matrix[i, j] = adj_matrix[j, i] = 1\n\n    graph = csr_matrix(adj_matrix)\n    n_components, labels = connected_components(\n        csgraph=graph, directed=False, return_labels=True\n    )\n    \n    clusters = [[] for _ in range(n_components)]\n    for client_idx, label in enumerate(labels):\n        clusters[label].append(client_idx)\n        \n    return clusters\n\ndef train_fedavg(client_data, client_indices, T, E, eta, d, n_i_all):\n    \"\"\"Runs the Federated Averaging algorithm for a given set of clients.\"\"\"\n    if not client_indices:\n        return np.zeros(d)\n        \n    w_server = np.zeros(d)\n    \n    sub_n_i = [n_i_all[i] for i in client_indices]\n    total_data_in_group = sum(sub_n_i)\n    \n    for _ in range(T):\n        local_models = []\n        for client_idx in client_indices:\n            w_local = w_server.copy()\n            data = client_data[client_idx]\n            for _ in range(E):\n                grad = compute_gradient(data, w_local)\n                w_local -= eta * grad\n            local_models.append(w_local)\n            \n        # Weighted aggregation\n        w_server = np.zeros(d)\n        for i, client_idx in enumerate(client_indices):\n            weight = n_i_all[client_idx] / total_data_in_group\n            w_server += weight * local_models[i]\n            \n    return w_server\n\ndef compute_loss(client_data, w):\n    \"\"\"Computes the empirical MSE loss.\"\"\"\n    X, y = client_data['X'], client_data['y']\n    n = len(y)\n    if n == 0:\n        return 0.0\n    error = X @ w - y\n    loss = np.mean(error**2)\n    return loss\n\ndef run_simulation(params):\n    \"\"\"Executes the full simulation for a single test case.\"\"\"\n    seed = params[\"seed\"]\n    C, d, n_i, sigma = params[\"C\"], params[\"d\"], params[\"n_i\"], params[\"sigma\"]\n    w_star, tau = params[\"w_star\"], params[\"tau\"]\n    T, E, eta = params[\"T\"], params[\"E\"], params[\"eta\"]\n\n    rng = np.random.default_rng(seed)\n    \n    # 1. Simulate data\n    all_client_data = generate_data(C, d, n_i, w_star, sigma, rng)\n\n    # 2. Compute initial gradients\n    w0 = np.zeros(d)\n    initial_gradients = [compute_gradient(all_client_data[i], w0) for i in range(C)]\n    \n    # 3. Form clusters\n    clusters = find_clusters(initial_gradients, tau, C)\n    client_to_cluster_map = {client: i for i, cluster in enumerate(clusters) for client in cluster}\n\n    # 4a. Train global model\n    all_clients = list(range(C))\n    w_global = train_fedavg(all_client_data, all_clients, T, E, eta, d, n_i)\n    \n    # 4b. Train cluster models\n    cluster_models = []\n    for cluster in clusters:\n        w_cluster = train_fedavg(all_client_data, cluster, T, E, eta, d, n_i)\n        cluster_models.append(w_cluster)\n\n    # 5. Evaluate final losses\n    total_loss_global = 0.0\n    for i in range(C):\n        total_loss_global += compute_loss(all_client_data[i], w_global)\n    mean_loss_global = total_loss_global / C\n\n    total_loss_cluster = 0.0\n    for i in range(C):\n        cluster_idx = client_to_cluster_map[i]\n        w_c = cluster_models[cluster_idx]\n        total_loss_cluster += compute_loss(all_client_data[i], w_c)\n    mean_loss_cluster = total_loss_cluster / C\n\n    # 6. Compute relative improvement\n    if mean_loss_global == 0:\n        # If global loss is zero, improvement is undefined unless cluster loss is also zero.\n        # This is highly unlikely with noise.\n        # A zero global loss implies a perfect model.\n        delta = 0.0 if mean_loss_cluster == 0.0 else -np.inf\n    else:\n        delta = (mean_loss_global - mean_loss_cluster) / mean_loss_global\n        \n    return delta\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}