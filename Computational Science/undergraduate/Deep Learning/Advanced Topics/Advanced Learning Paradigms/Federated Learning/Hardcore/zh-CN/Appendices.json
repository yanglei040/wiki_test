{
    "hands_on_practices": [
        {
            "introduction": "联邦学习的成功取决于个体客户设备的能力。本练习深入探讨了客户层面面临的核心挑战：在严格的资源限制下优化本地训练。通过在给定的内存、延迟和能源预算下确定最大可能的批量大小（batch size），你将具体理解硬件限制如何直接影响学习过程的统计特性，例如梯度方差 。",
            "id": "3124684",
            "problem": "单个移动设备参与联邦学习 (FL)，它使用大小为 $B$ 的小批量，从其本地数据集中抽取独立同分布的数据样本，来为一个标量模型参数计算局部梯度估计值 $\\hat{g}$。设每个样本的梯度是一个标量随机变量 $g_{i}$，其均值为 $\\mathbb{E}[g_{i}] = g$，方差为 $\\mathrm{Var}(g_{i}) = \\sigma^{2}$，并假设各个 $g_{i}$ 是相互独立的。该设备的目标是选择尽可能大的 $B$ 以减小 $\\hat{g}$ 的方差，同时需要满足单轮训练的以下资源限制：\n\n- 内存预算：该设备有 $M = 80$ MiB 的可用空闲内存用于该训练轮次。加载的模型占用 $m_{\\mathrm{model}} = 40$ MiB，运行时开销（激活值、框架缓冲区）占用 $m_{\\mathrm{ov}} = 10$ MiB，处理期间每个数据样本需要 $m_{s} = 1$ MiB 的内存。假设内存使用量随 $B$ 线性增长，并且所有数据必须同时保存在内存中。\n- 延迟预算：每个数据样本需要 $t_{s} = 3$ ms 的计算时间，并且设备必须在 $T_{\\max} = 120$ ms 内完成局部计算。\n- 能量预算：每个数据样本需要 $e_{s} = 7$ mJ 的能量，并且设备在该轮次有 $E_{\\max} = 250$ mJ 的可用能量。\n\n假设设备按顺序处理样本，$B$ 必须为正整数，并忽略任何并行或流水线效应。同时假设没有引入额外的噪声（例如量化或差分隐私），因此 $\\hat{g}$ 中唯一的随机性来源是小批量的采样。\n\n请从样本均值的基本定义以及独立随机变量的方差性质出发，推导出 $\\mathrm{Var}(\\hat{g})$ 作为 $B$ 的函数的表达式。然后，在给定 $\\sigma^{2} = 0.81$ 的情况下，确定满足所有三个设备约束的最大可行 $B$ 值，并计算在该 $B$ 值下 $\\mathrm{Var}(\\hat{g})$ 的数值。请将最终方差表示为一个无单位的纯数字。无需四舍五入，报告精确值。",
            "solution": "问题要求两个主要结果：首先，推导梯度估计值方差 $\\mathrm{Var}(\\hat{g})$ 作为小批量大小 $B$ 的函数表达式；其次，在给定的限制 $B$ 的资源约束条件下，计算该方差的数值。\n\n首先，我们推导 $\\mathrm{Var}(\\hat{g})$ 的表达式。局部梯度估计值 $\\hat{g}$ 定义为 $B$ 个独立同分布 (i.i.d.) 的单样本梯度的样本均值，这些梯度记为 $g_i$，其中 $i \\in \\{1, 2, \\dots, B\\}$。\n$$ \\hat{g} = \\frac{1}{B} \\sum_{i=1}^{B} g_i $$\n每个 $g_i$ 是一个随机变量，其均值为 $\\mathbb{E}[g_{i}] = g$，方差为 $\\mathrm{Var}(g_{i}) = \\sigma^{2}$。我们可以通过应用方差算子的性质来求出 $\\hat{g}$ 的方差。\n\n对于一个常数 $c$ 和一个随机变量 $X$，方差算子遵循规则 $\\mathrm{Var}(cX) = c^2 \\mathrm{Var}(X)$。在我们的例子中，常数是 $\\frac{1}{B}$。\n$$ \\mathrm{Var}(\\hat{g}) = \\mathrm{Var}\\left(\\frac{1}{B} \\sum_{i=1}^{B} g_i\\right) = \\left(\\frac{1}{B}\\right)^2 \\mathrm{Var}\\left(\\sum_{i=1}^{B} g_i\\right) = \\frac{1}{B^2} \\mathrm{Var}\\left(\\sum_{i=1}^{B} g_i\\right) $$\n对于独立随机变量的和，其和的方差等于它们各自方差的和。题目说明 $g_i$ 是独立的。\n$$ \\mathrm{Var}\\left(\\sum_{i=1}^{B} g_i\\right) = \\sum_{i=1}^{B} \\mathrm{Var}(g_i) $$\n由于样本也是同分布的，因此每个 $g_i$ 的方差是相同的：$\\mathrm{Var}(g_i) = \\sigma^2$。\n$$ \\sum_{i=1}^{B} \\mathrm{Var}(g_i) = \\sum_{i=1}^{B} \\sigma^2 = B \\sigma^2 $$\n将此结果代回 $\\mathrm{Var}(\\hat{g})$ 的表达式中，即可得到所需的关系式。\n$$ \\mathrm{Var}(\\hat{g}) = \\frac{1}{B^2} (B \\sigma^2) = \\frac{\\sigma^2}{B} $$\n\n接下来，我们必须在给定的三个资源约束条件下，确定小批量大小 $B$ 的最大可行整数值。\n\n1.  内存约束：消耗的总内存不能超过可用内存 $M = 80$ MiB。总内存是固定的模型内存 $m_{\\mathrm{model}} = 40$ MiB 和开销内存 $m_{\\mathrm{ov}} = 10$ MiB，以及每个数据样本的可变内存 $m_{s} = 1$ MiB 乘以批量大小 $B$ 的总和。\n    $$ m_{\\mathrm{model}} + m_{\\mathrm{ov}} + B \\cdot m_{s} \\le M $$\n    代入数值（单位一致，可在不等式中省略）：\n    $$ 40 + 10 + B \\cdot 1 \\le 80 $$\n    $$ 50 + B \\le 80 $$\n    $$ B \\le 30 $$\n\n2.  延迟约束：总计算时间必须在最大允许延迟 $T_{\\max} = 120$ ms 之内。总时间是每个样本的计算时间 $t_{s} = 3$ ms 乘以批量大小 $B$。\n    $$ B \\cdot t_{s} \\le T_{\\max} $$\n    代入数值：\n    $$ B \\cdot 3 \\le 120 $$\n    $$ B \\le \\frac{120}{3} $$\n    $$ B \\le 40 $$\n\n3.  能量约束：消耗的总能量不能超过能量预算 $E_{\\max} = 250$ mJ。总能量是每个样本的能耗 $e_{s} = 7$ mJ 乘以批量大小 $B$。\n    $$ B \\cdot e_{s} \\le E_{\\max} $$\n    代入数值：\n    $$ B \\cdot 7 \\le 250 $$\n    $$ B \\le \\frac{250}{7} $$\n    由于 $B$ 必须是正整数，我们必须对该值向下取整：$B \\le \\lfloor \\frac{250}{7} \\rfloor = \\lfloor 35.714\\dots \\rfloor = 35$。\n    $$ B \\le 35 $$\n\n为了同时满足所有三个约束，$B$ 必须小于或等于三个推导出的上界中的最小值。\n$$ B \\le \\min(30, 40, 35) $$\n$$ B \\le 30 $$\n问题要求的是尽可能大的 $B$，因此我们选择满足此条件的最大整数值，即 $B = 30$。\n\n最后，我们使用推导出的公式 $\\mathrm{Var}(\\hat{g}) = \\frac{\\sigma^2}{B}$、确定的批量大小 $B = 30$ 以及给定的单样本方差 $\\sigma^2 = 0.81$ 来计算 $\\mathrm{Var}(\\hat{g})$ 的数值。\n$$ \\mathrm{Var}(\\hat{g}) = \\frac{0.81}{30} $$\n这可以精确地计算为小数：\n$$ \\mathrm{Var}(\\hat{g}) = \\frac{81}{100} \\div 30 = \\frac{81}{100} \\times \\frac{1}{30} = \\frac{81}{3000} = \\frac{27 \\times 3}{1000 \\times 3} = \\frac{27}{1000} = 0.027 $$",
            "answer": "$$\\boxed{0.027}$$"
        },
        {
            "introduction": "在优化单个客户之后，我们需将视角提升到整个联邦系统，其效率取决于一个关键的权衡：本地计算与全局通信。本练习将指导你优化本地训练轮数（$E$）和通信回合数（$R$），以最小化总训练时间。通过这个实践 ，你将深入理解在设计高效联邦学习系统时，如何在降低通信开销和控制“客户漂移”（client drift）之间找到最佳平衡点。",
            "id": "3124716",
            "problem": "考虑一个联邦平均 (FedAvg) 系统，该系统在客户端上局部使用随机梯度下降 (SGD) 来优化一个具有 Lipschitz 连续梯度的强凸经验风险。在每一轮通信中，有 $K=10$ 个客户端参与，每个客户端在每个本地轮次 (local epoch) 中处理 $n=10$ 个本地样本。假设客户端之间完全并行，因此每轮的墙上时钟时间 (wall-clock time) 可以建模为本地计算项和通信项之和。具体来说，每轮的时间为 $T_{\\text{round}}(E) = E\\, t_{\\text{comp}} + t_{\\text{comm}}$，其中 $t_{\\text{comp}} = 0.2$ 且 $t_{\\text{comm}} = 2$。在 $R$ 轮中，每轮有 $E$ 个本地轮次的总墙上时钟时间为 $T_{\\text{total}}(R,E) = R\\left(E\\, t_{\\text{comp}} + t_{\\text{comm}}\\right)$。\n\n在 SGD 的标准平滑性和强凸性假设下，中心化 SGD 经过 $T$ 次随机更新后的优化误差尺度为 $O\\!\\left(\\frac{1}{T}\\right)$。在联邦学习中，由异构数据引起的客户端漂移 (client drift) 会引入一个偏差，该偏差随每轮本地步骤数的增加而增长。一个捕捉了这两种效应的、经过充分检验的简单上界具有以下形式\n$$\\mathcal{E}(R,E) \\le \\frac{A}{R E} + B E,$$\n其中 $A > 0$ 捕捉了衰减项的初始最优性差距和随机性，而 $B > 0$ 捕捉了每个本地轮次由客户端漂移引起的偏差。在本问题中，取 $A = 4$ 和 $B = 0.01$。\n\n给定预算约束了本地样本更新总数和总带宽。整个训练过程中的本地样本更新总数为 $K n E R$，必须满足 $K n E R \\le \\mathcal{C}_{\\max}$，其中 $\\mathcal{C}_{\\max} = 50{,}000$。所使用的总带宽与轮数成正比：每轮传输总大小为 $S_{\\text{round}} = K S_{\\text{model}}$ 的模型，其中 $S_{\\text{model}} = 1$（归一化单位），总带宽预算满足 $R S_{\\text{round}} \\le \\mathcal{B}_{\\max}$，其中 $\\mathcal{B}_{\\max} = 800$。因此，预算简化为约束条件 $R E \\le 500$ 和 $R \\le 80$。\n\n目标优化误差为 $\\epsilon = 0.1$。从上述基本事实出发，推导出在以下约束条件下最小化总墙上时钟时间 $T_{\\text{total}}(R,E)$ 的 $E$ 和 $R$ 的选择\n$$\\frac{A}{R E} + B E \\le \\epsilon,\\quad R E \\le 500,\\quad R \\le 80,$$\n其中 $A = 4$，$B = 0.01$ 且 $\\epsilon = 0.1$。将你的最终答案表示为一个精确的解析对 $\\left(E^{\\star}, R^{\\star}\\right)$；不要四舍五入。你的答案必须是使用 LaTeX 行矩阵格式的单个表达式。",
            "solution": "问题要求我们找到本地轮次数 $E$ 和通信轮数 $R$，以在一系列关于优化误差和资源预算的约束条件下，最小化总墙上时钟时间 $T_{\\text{total}}(R,E)$。\n\n首先，我们使用给定的参数来形式化这个优化问题。\n要最小化的目标函数是总墙上时钟时间：\n$$T_{\\text{total}}(R,E) = R(E\\, t_{\\text{comp}} + t_{\\text{comm}})$$\n代入给定值 $t_{\\text{comp}} = 0.2$ 和 $t_{\\text{comm}} = 2$，我们得到：\n$$T(R,E) = R(0.2E + 2)$$\n\n最小化受三个约束条件的限制：\n1.  优化误差约束：$\\frac{A}{RE} + B E \\le \\epsilon$。当 $A=4$，$B=0.01$ 和 $\\epsilon=0.1$ 时，这变为：\n    $$\\frac{4}{RE} + 0.01E \\le 0.1$$\n2.  本地样本更新总数约束：$RE \\le 500$。\n3.  总带宽约束：$R \\le 80$。\n\n此外，变量 $R$ 和 $E$ 必须为正，因为它们分别代表轮数和轮次。我们视其为用于优化的连续正变量。\n\n对于 $R > 0$ 和 $E > 0$，目标函数 $T(R,E)$ 是关于 $R$ 和 $E$ 的严格递增函数。为了最小化 $T(R,E)$，我们必须在可行域中找到在某种意义上“最接近”原点的点 $(R,E)$。这意味着最优解 $(R^{\\star}, E^{\\star})$ 不会位于可行域的内部（即所有不等式约束都严格满足的区域），因为我们总是可以稍微减小 $R$ 或 $E$ 来获得更小的 $T(R,E)$ 值，而不会离开可行域。因此，最小值必须位于可行域的边界上，这意味着至少有一个约束必须以等式形式满足。\n\n鉴于 $T(R,E)$ 随两个变量单调增加，解很可能位于由误差约束定义的边界上，即 $\\frac{4}{RE} + 0.01E = 0.1$。如果解位于 $\\frac{4}{RE} + 0.01E  0.1$ 的区域，我们可以通过减小 $R$ 或 $E$ 来减少时间，因此我们假设在最优点处误差约束是激活的（即取等号）：\n$$\\frac{4}{RE} + 0.01E = 0.1$$\n\n从这个方程中，我们可以将 $R$ 表示为 $E$ 的函数。为了使 $R$ 为正的有限值，我们必须有 $0.1 - 0.01E > 0$，这意味着 $0.01E  0.1$，或 $E  10$。因此，在这个边界上 $E$ 的定义域是 $E \\in (0, 10)$。\n重新整理关于 $R$ 的等式：\n$$\\frac{4}{RE} = 0.1 - 0.01E$$\n$$RE = \\frac{4}{0.1 - 0.01E}$$\n$$R(E) = \\frac{4}{E(0.1 - 0.01E)} = \\frac{4}{0.1E - 0.01E^2}$$\n\n现在，我们将这个关于 $R$ 的表达式代入目标函数 $T(R,E)$，得到一个单变量 $E$ 的函数：\n$$T(E) = R(E)(0.2E + 2) = \\left(\\frac{4}{0.1E - 0.01E^2}\\right)(0.2E + 2)$$\n$$T(E) = \\frac{0.8E + 8}{0.1E - 0.01E^2}$$\n\n为了找到在区间 $(0, 10)$ 上最小化 $T(E)$ 的 $E$ 值，我们计算 $T(E)$ 对 $E$ 的导数，并令其为零。使用商法则 $\\frac{d}{dE}\\left(\\frac{u}{v}\\right) = \\frac{u'v - uv'}{v^2}$，其中 $u(E) = 0.8E + 8$，$v(E) = 0.1E - 0.01E^2$：\n$$u'(E) = 0.8$$\n$$v'(E) = 0.1 - 0.02E$$\n当分子为零时，导数 $T'(E)$ 为零：\n$$u'v - uv' = 0$$\n$$0.8(0.1E - 0.01E^2) - (0.8E + 8)(0.1 - 0.02E) = 0$$\n展开各项：\n$$(0.08E - 0.008E^2) - (0.08E - 0.016E^2 + 0.8 - 0.16E) = 0$$\n$$0.08E - 0.008E^2 - 0.08E + 0.016E^2 - 0.8 + 0.16E = 0$$\n合并同类项：\n$$0.008E^2 + 0.16E - 0.8 = 0$$\n为简化，两边乘以 $1000$：\n$$8E^2 + 160E - 800 = 0$$\n两边除以 $8$：\n$$E^2 + 20E - 100 = 0$$\n我们使用二次方程求根公式 $E = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ 来解这个关于 $E$ 的二次方程：\n$$E = \\frac{-20 \\pm \\sqrt{20^2 - 4(1)(-100)}}{2(1)} = \\frac{-20 \\pm \\sqrt{400 + 400}}{2} = \\frac{-20 \\pm \\sqrt{800}}{2}$$\n简化平方根：$\\sqrt{800} = \\sqrt{400 \\times 2} = 20\\sqrt{2}$。\n$$E = \\frac{-20 \\pm 20\\sqrt{2}}{2} = -10 \\pm 10\\sqrt{2}$$\n由于 $E$ 必须为正，我们取正根：\n$$E^{\\star} = -10 + 10\\sqrt{2} = 10(\\sqrt{2} - 1)$$\n该值在定义域 $(0, 10)$ 内，因为 $1  \\sqrt{2}  2$。\n\n现在我们使用 $R(E)$ 的表达式来求出对应的 $R$ 值：\n$$R^{\\star} = \\frac{4}{0.1E^{\\star} - 0.01(E^{\\star})^2}$$\n根据二次方程 $E^2 + 20E - 100 = 0$，我们有 $(E^{\\star})^2 = 100 - 20E^{\\star}$。代入此式：\n$$R^{\\star} = \\frac{4}{0.1E^{\\star} - 0.01(100 - 20E^{\\star})} = \\frac{4}{0.1E^{\\star} - 1 + 0.2E^{\\star}} = \\frac{4}{0.3E^{\\star} - 1}$$\n代入 $E^{\\star} = 10(\\sqrt{2} - 1)$ 的值：\n$$R^{\\star} = \\frac{4}{0.3(10(\\sqrt{2} - 1)) - 1} = \\frac{4}{3(\\sqrt{2} - 1) - 1} = \\frac{4}{3\\sqrt{2} - 3 - 1} = \\frac{4}{3\\sqrt{2} - 4}$$\n为了使分母有理化，我们将分子和分母同乘以共轭式 $3\\sqrt{2} + 4$：\n$$R^{\\star} = \\frac{4(3\\sqrt{2} + 4)}{(3\\sqrt{2} - 4)(3\\sqrt{2} + 4)} = \\frac{4(3\\sqrt{2} + 4)}{(3\\sqrt{2})^2 - 4^2} = \\frac{4(3\\sqrt{2} + 4)}{18 - 16} = \\frac{4(3\\sqrt{2} + 4)}{2}$$\n$$R^{\\star} = 2(3\\sqrt{2} + 4) = 6\\sqrt{2} + 8$$\n\n候选最优对为 $(E^{\\star}, R^{\\star}) = (10(\\sqrt{2} - 1), 8 + 6\\sqrt{2})$。\n我们必须验证该解是否满足其余两个约束条件：\n1.  检查 $RE \\le 500$：\n    $$R^{\\star}E^{\\star} = (8 + 6\\sqrt{2}) \\times 10(\\sqrt{2} - 1) = 10(8\\sqrt{2} - 8 + 6(\\sqrt{2})^2 - 6\\sqrt{2})$$\n    $$= 10(2\\sqrt{2} + 12 - 8) = 10(4 + 2\\sqrt{2}) = 20(2 + \\sqrt{2})$$\n    由于 $\\sqrt{2} \\approx 1.414$，$R^{\\star}E^{\\star} \\approx 20(3.414) = 68.28$。这远小于 $500$。\n2.  检查 $R \\le 80$：\n    $$R^{\\star} = 8 + 6\\sqrt{2} \\approx 8 + 6(1.414) = 8 + 8.484 = 16.484$$\n    这远小于 $80$。\n\n两个约束都满足。点 $(E^{\\star}, R^{\\star})$ 在边界 $\\frac{4}{RE} + 0.01E = 0.1$ 上最小化了时间 $T(R,E)$。由于该点严格位于由另外两个约束定义的区域内，并且移动到这两个约束的边界上需要沿主边界离开该最小值点（从而增加时间），因此该点代表了此约束问题的全局最小值。\n\n因此，最优的 $E$ 和 $R$ 选择为 $(E^{\\star}, R^{\\star}) = (10(\\sqrt{2} - 1), 8 + 6\\sqrt{2})$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n10(\\sqrt{2} - 1)  8 + 6\\sqrt{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在客户端数据天然具有多样性（non-IID）的真实世界中，“一刀切”的全局模型往往难以取得理想效果。这个动手编程练习  介绍了一种强大的解决方案：通过客户聚类实现个性化联邦学习。你将亲手实现一个算法，它能根据客户的学习目标相似性将其分组，并为每个群组训练专门的模型，从而直接应对数据异构性的挑战，并显著提升模型的个性化性能。",
            "id": "3124737",
            "problem": "考虑一个联邦学习场景，其中有 $C$ 个客户端，每个客户端都持有从带有加性噪声的线性模型中独立采样得到的本地数据。每个客户端 $i$ 有 $n_i$ 个样本 $\\{(x^{(i)}_t, y^{(i)}_t)\\}_{t=1}^{n_i}$，其中 $x^{(i)}_t \\in \\mathbb{R}^d$ 是从零均值、单位协方差的高斯分布中独立同分布 (i.i.d.) 抽取的，且 $y^{(i)}_t = (x^{(i)}_t)^\\top w^*_i + \\epsilon^{(i)}_t$，其中 $\\epsilon^{(i)}_t \\sim \\mathcal{N}(0, \\sigma^2)$。我们训练线性模型 $f_w(x) = x^\\top w$，客户端 $i$ 上的经验均方误差 (MSE) 损失由下式给出：\n$$\nL_i(w) = \\frac{1}{n_i} \\sum_{t=1}^{n_i} \\left((x^{(i)}_t)^\\top w - y^{(i)}_t\\right)^2.\n$$\n$L_i(w)$ 相对于 $w$ 的梯度记为 $g_i(w) = \\nabla_w L_i(w)$。我们定义两个非零梯度 $g_i$ 和 $g_j$ 之间的余弦相似度为：\n$$\n\\mathrm{cos}(g_i, g_j) = \\frac{g_i^\\top g_j}{\\|g_i\\|_2 \\, \\|g_j\\|_2}.\n$$\n当任一梯度为零向量时，定义余弦相似度为 $0$。\n\n您必须从基本原理出发，实现以下步骤：\n- 在共享初始值 $w_0 = 0$ 处计算每个客户端的梯度，即 $g_i(w_0)$。\n- 使用阈值 $\\tau$ 将客户端聚类为连通分量：在客户端上创建一个无向图，如果 $\\mathrm{cos}(g_i(w_0), g_j(w_0)) \\ge \\tau$，则在客户端 $i$ 和 $j$ 之间添加一条边，然后将图的连通分量作为簇。\n- 使用联邦平均 (FedAvg) 训练两个联邦系统，共进行 $T$ 轮通信。在每一轮中，每个客户端从服务器接收当前模型开始，在其本地数据上执行 $E$ 步步长（学习率）为 $\\eta$ 的批量梯度下降。服务器按与 $n_i$ 成比例的加权平均来聚合客户端模型的更新量。\n    - 单一全局模型：一个通过聚合所有客户端来更新的共享模型 $w$。\n    - 簇特定模型：每个簇一个模型；每个簇的模型仅通过聚合该簇内的客户端来更新。\n- 训练后，为两个系统评估客户端平均经验均方误差。对于单一全局模型，每个客户端都使用全局模型 $w$；对于簇特定模型，客户端 $i$ 使用其所在簇的模型 $w$。评估指标是各客户端经验均方误差的算术平均值：\n$$\n\\overline{L}_\\mathrm{global} = \\frac{1}{C} \\sum_{i=1}^{C} L_i(w_\\mathrm{global}), \\quad\n\\overline{L}_\\mathrm{cluster} = \\frac{1}{C} \\sum_{i=1}^{C} L_i(w_{\\mathrm{cluster}(i)}).\n$$\n计算相对改进的小数值：\n$$\n\\Delta = \\frac{\\overline{L}_\\mathrm{global} - \\overline{L}_\\mathrm{cluster}}{\\overline{L}_\\mathrm{global}}.\n$$\n报告每个测试用例的 $\\Delta$。\n\n需要使用的基本原理：\n- $L_i(w)$ 的经验风险最小化定义。\n- 梯度下降更新 $w \\leftarrow w - \\eta \\, g_i(w)$。\n- 与 $n_i$ 成比例聚合客户端更新的加权平均。\n- 基于梯度方向进行聚类的余弦相似度定义。\n\n实现一个完整、可运行的程序，该程序：\n- 按规定模拟数据。\n- 计算初始梯度。\n- 通过阈值化余弦相似度和连通分量形成簇。\n- 对单一全局模型和簇特定模型都使用联邦平均进行训练。\n- 评估并输出每个测试用例的相对改进 $\\Delta$。\n\n测试套件和参数：\n- 测试用例 $1$（顺利情况，两个对立的簇）：\n    - 随机种子 $42$，$C = 8$，$d = 5$，所有客户端样本大小 $n_i = 200$，噪声标准差 $\\sigma = 0.1$。\n    - 真实参数向量：客户端 $1$ 到 $4$ 为 $w^*_1 = [1, 0, 0, 0, 0]^\\top$，客户端 $5$ 到 $8$ 为 $w^*_2 = [-1, 0, 0, 0, 0]^\\top$。\n    - 聚类阈值 $\\tau = 0.7$。\n    - FedAvg 超参数：$T = 15$，$E = 5$，$\\eta = 0.05$。\n- 测试用例 $2$（预计无改进，同构客户端）：\n    - 随机种子 $123$，$C = 8$，$d = 5$，所有客户端样本大小 $n_i = 200$，噪声标准差 $\\sigma = 0.1$。\n    - 真实参数向量：所有客户端 $i$ 均为 $w^*_i = [1, 1, 0, 0, 0]^\\top$。\n    - 聚类阈值 $\\tau = 0.7$。\n    - FedAvg 超参数：$T = 15$，$E = 5$，$\\eta = 0.05$。\n- 测试用例 $3$（$\\tau = 1.0$ 的边界聚类，单例簇）：\n    - 随机种子 $7$，$C = 2$，$d = 3$，客户端样本大小 $n_1 = 300, n_2 = 300$，噪声标准差 $\\sigma = 0.1$。\n    - 真实参数向量：$w^*_1 = [1, 0, 0]^\\top$，$w^*_2 = [0, 1, 0]^\\top$。\n    - 聚类阈值 $\\tau = 1.0$。\n    - FedAvg 超参数：$T = 20$，$E = 5$，$\\eta = 0.05$。\n- 测试用例 $4$（嘈杂的边缘情况，潜在的聚类模糊性）：\n    - 随机种子 $99$，$C = 6$，$d = 4$，所有客户端样本大小 $n_i = 50$，噪声标准差 $\\sigma = 0.5$。\n    - 真实参数向量：客户端 $1$ 到 $3$ 为 $w^*_1 = [1, 0, 0, 0]^\\top$，客户端 $4$ 到 $6$ 为 $w^*_2 = [0, 1, 0, 0]^\\top$。\n    - 聚类阈值 $\\tau = 0.5$。\n    - FedAvg 超参数：$T = 25$，$E = 5$，$\\eta = 0.05$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的条目按上述四个测试用例的顺序排列，例如 $\\texttt{[r_1,r_2,r_3,r_4]}$，其中每个 $r_k$ 是测试用例 $k$ 的十进制相对改进值。",
            "solution": "经过全面审查，该问题被认定为有效。它在科学上基于联邦学习、线性回归和图论的原理。问题陈述清晰，所有必要的参数和条件都已指定，通过固定的随机种子确保每个测试用例都有唯一且可复现的结果。问题陈述客观，没有歧义或矛盾。\n\n### 理论阐述\n\n这个问题的核心在于解决联邦学习中的客户端异构性。当客户端具有不同的底层数据分布或目标时，单一的全局模型对所有客户端而言可能并非最优。所提出的解决方案是根据客户端的目标对其进行聚类，并为每个簇训练一个个性化模型。\n\n**1. 客户端损失和梯度**\n每个客户端 $i$ 都试图最小化其本地经验均方误差 (MSE) 损失，其公式如下：\n$$\nL_i(w) = \\frac{1}{n_i} \\sum_{t=1}^{n_i} \\left((x^{(i)}_t)^\\top w - y^{(i)}_t\\right)^2\n$$\n其中 $w \\in \\mathbb{R}^d$ 是模型参数向量。这是一个凸优化问题。该损失函数相对于 $w$ 的梯度，用于梯度下降，其公式为：\n$$\ng_i(w) = \\nabla_w L_i(w) = \\frac{2}{n_i} \\sum_{t=1}^{n_i} \\left((x^{(i)}_t)^\\top w - y^{(i)}_t\\right)x^{(i)}_t\n$$\n在矩阵表示法中，$X_i$ 是 $n_i \\times d$ 的数据矩阵，$Y_i$ 是 $n_i \\times 1$ 的标签向量，梯度为：\n$$\ng_i(w) = \\frac{2}{n_i} X_i^\\top (X_i w - Y_i)\n$$\n\n**2. 基于梯度的聚类原理**\n聚类启发式方法基于在共享起始点 $w_0 = 0$ 处计算的初始梯度。在此初始值下，客户端 $i$ 的梯度简化为：\n$$\ng_i(w_0) = g_i(0) = -\\frac{2}{n_i} X_i^\\top Y_i\n$$\n代入真实的数据生成过程 $y^{(i)}_t = (x^{(i)}_t)^\\top w^*_i + \\epsilon^{(i)}_t$，我们得到：\n$$\ng_i(0) = -\\frac{2}{n_i} \\sum_{t=1}^{n_i} x^{(i)}_t \\left((x^{(i)}_t)^\\top w^*_i + \\epsilon^{(i)}_t\\right) = -\\frac{2}{n_i} \\left( \\left(\\sum_{t=1}^{n_i} x^{(i)}_t (x^{(i)}_t)^\\top \\right) w^*_i + \\sum_{t=1}^{n_i} x^{(i)}_t \\epsilon^{(i)}_t \\right)\n$$\n根据大数定律，对于足够大的样本数量 $n_i$，样本协方差矩阵 $\\frac{1}{n_i}\\sum_{t=1}^{n_i} x^{(i)}_t (x^{(i)}_t)^\\top$ 接近真实的协方差矩阵，即单位矩阵 $I_d$，因为 $x^{(i)}_t \\sim \\mathcal{N}(0, I_d)$。噪声项 $\\frac{1}{n_i}\\sum_{t=1}^{n_i} x^{(i)}_t \\epsilon^{(i)}_t$ 接近其期望值，即 $E[x \\epsilon] = E[x]E[\\epsilon] = 0$。因此，我们有以下近似：\n$$\ng_i(0) \\approx -2 (I_d w^*_i) = -2w^*_i\n$$\n这揭示了一个关键的洞察：原点处的初始梯度向量近似与客户端真实最优参数向量 $w^*_i$ 的负值成正比。因此，两个客户端 $i$ 和 $j$ 的初始梯度之间的余弦相似度近似于它们真实参数向量的余弦相似度：\n$$\n\\mathrm{cos}(g_i(0), g_j(0)) \\approx \\mathrm{cos}(-2w^*_i, -2w^*_j) = \\mathrm{cos}(w^*_i, w^*_j)\n$$\n因此，基于初始梯度的余弦相似度来聚类客户端，是用于将具有相似底层目标 ($w^*_i$) 的客户端分组的可靠启发式方法。\n\n### 算法流程\n\n该解决方案通过遵循从基本原理出发的指定流程来实现。\n\n**1. 数据模拟**\n对于每个客户端 $i \\in \\{1, \\dots, C\\}$，我们生成一个本地数据集。\n-   特征向量 $\\{x^{(i)}_t\\}_{t=1}^{n_i}$ 从 $\\mathcal{N}(0, I_d)$ 中抽取。\n-   标签 $\\{y^{(i)}_t\\}_{t=1}^{n_i}$ 计算为 $y^{(i)}_t = (x^{(i)}_t)^\\top w^*_i + \\epsilon^{(i)}_t$，其中噪声项 $\\{\\epsilon^{(i)}_t\\}_{t=1}^{n_i}$ 从 $\\mathcal{N}(0, \\sigma^2)$ 中抽取。\n为每个测试用例设置随机种子以确保可复现性。\n\n**2. 客户端聚类**\n-   将模型参数初始化为零向量 $w_0 = 0$。\n-   对于每个客户端 $i$，使用上面导出的公式计算初始梯度 $g_i(w_0)$。\n-   构建一个具有 $C$ 个顶点的无向图，代表所有客户端。当且仅当客户端 $i$ 和 $j$ 的初始梯度非零且它们的余弦相似度达到或超过阈值 $\\tau$ 时，在它们之间放置一条边：\n    $$\n    \\frac{g_i(w_0)^\\top g_j(w_0)}{\\|g_i(w_0)\\|_2 \\, \\|g_j(w_0)\\|_2} \\ge \\tau\n    $$\n    如果任一梯度为零向量，则相似度定义为 $0$，不创建边（除非 $\\tau \\le 0$）。\n-   通过寻找该图的连通分量来确定簇。这是通过构建邻接矩阵并使用 `scipy.sparse.csgraph.connected_components` 函数来实现的，该函数能有效地划分图的顶点。\n\n**3. 联邦平均 (FedAvg) 训练**\n我们模拟两个独立的训练系统，进行 $T$ 轮通信。\n\n-   **单一全局模型：** 单个服务器为所有 $C$ 个客户端管理一个模型 $w_{\\mathrm{global}}$。\n    -   在每一轮中，服务器将 $w_{\\mathrm{global}}$ 分发给所有客户端。\n    -   每个客户端 $i$ 从 $w_{\\mathrm{global}}$ 开始，使用其本地数据和学习率 $\\eta$ 执行 $E$ 步本地批量梯度下降。\n    -   服务器使用加权平均将更新后的本地模型聚合成一个新的全局模型，其中权重与客户端样本大小 $n_i$ 成正比：\n        $$ w_{\\mathrm{global}} \\leftarrow \\frac{\\sum_{i=1}^C n_i w_i}{\\sum_{i=1}^C n_i} $$\n\n-   **簇特定模型：** 对每个已识别的簇，运行一个独立的 FedAvg 实例。\n    -   每个簇都有自己的服务器和模型 $w_{\\mathrm{cluster}(k)}$。\n    -   训练过程与全局模型类似，但簇 $k$ 的聚合仅涉及属于该簇的客户端。所有模型都初始化为 $w_0 = 0$。\n\n**4. 评估**\n经过 $T$ 輪训练后，评估两个系统的性能。\n-   全局模型的客户端平均经验均方误差 $\\overline{L}_\\mathrm{global}$ 是通过对所有 $C$ 个客户端的最终损失 $L_i(w_\\mathrm{global})$ 进行平均计算得出的。\n-   聚类系统的客户端平均经验均方误差 $\\overline{L}_\\mathrm{cluster}$ 也以类似方式计算。对于每个客户端 $i$，使用其所属簇的最终模型计算损失 $L_i(w_{\\mathrm{cluster}(i)})$。\n-   然后，相对改进 $\\Delta$ 计算如下：\n    $$\n    \\Delta = \\frac{\\overline{L}_\\mathrm{global} - \\overline{L}_\\mathrm{cluster}}{\\overline{L}_\\mathrm{global}}\n    $$\n该指标量化了通过聚类实现的性能增益。正值表示聚类方法产生了较低的平均客户端损失。",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.csgraph import connected_components\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"seed\": 42, \"C\": 8, \"d\": 5, \"n_i\": [200] * 8, \"sigma\": 0.1,\n            \"w_star\": [np.array([1., 0, 0, 0, 0])] * 4 + [np.array([-1., 0, 0, 0, 0])] * 4,\n            \"tau\": 0.7, \"T\": 15, \"E\": 5, \"eta\": 0.05\n        },\n        {\n            \"seed\": 123, \"C\": 8, \"d\": 5, \"n_i\": [200] * 8, \"sigma\": 0.1,\n            \"w_star\": [np.array([1., 1, 0, 0, 0])] * 8,\n            \"tau\": 0.7, \"T\": 15, \"E\": 5, \"eta\": 0.05\n        },\n        {\n            \"seed\": 7, \"C\": 2, \"d\": 3, \"n_i\": [300, 300], \"sigma\": 0.1,\n            \"w_star\": [np.array([1., 0, 0]), np.array([0., 1, 0])],\n            \"tau\": 1.0, \"T\": 20, \"E\": 5, \"eta\": 0.05\n        },\n        {\n            \"seed\": 99, \"C\": 6, \"d\": 4, \"n_i\": [50] * 6, \"sigma\": 0.5,\n            \"w_star\": [np.array([1., 0, 0, 0])] * 3 + [np.array([0., 1, 0, 0])] * 3,\n            \"tau\": 0.5, \"T\": 25, \"E\": 5, \"eta\": 0.05\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(params)\n        results.append(result)\n\n    # Format output as [r1,r2,r3,r4] with 8 decimal places.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef generate_data(C, d, n_i, w_star, sigma, rng):\n    \"\"\"Generates client data based on the problem specification.\"\"\"\n    client_data = []\n    for i in range(C):\n        n = n_i[i]\n        X = rng.normal(0, 1, size=(n, d))\n        epsilon = rng.normal(0, sigma, size=n)\n        y = X @ w_star[i] + epsilon\n        client_data.append({'X': X, 'y': y})\n    return client_data\n\ndef compute_gradient(client_data, w):\n    \"\"\"Computes the gradient of the MSE loss for a single client.\"\"\"\n    X, y = client_data['X'], client_data['y']\n    n = len(y)\n    if n == 0:\n        return np.zeros_like(w)\n    grad = 2/n * X.T @ (X @ w - y)\n    return grad\n\ndef find_clusters(initial_gradients, tau, C):\n    \"\"\"Forms clusters based on cosine similarity of initial gradients.\"\"\"\n    adj_matrix = np.zeros((C, C))\n    for i in range(C):\n        for j in range(i + 1, C):\n            g_i = initial_gradients[i]\n            g_j = initial_gradients[j]\n            norm_i = np.linalg.norm(g_i)\n            norm_j = np.linalg.norm(g_j)\n            \n            cosine_sim = 0.0\n            if norm_i  0 and norm_j  0:\n                cosine_sim = np.dot(g_i, g_j) / (norm_i * norm_j)\n            \n            if cosine_sim = tau:\n                adj_matrix[i, j] = adj_matrix[j, i] = 1\n\n    graph = csr_matrix(adj_matrix)\n    n_components, labels = connected_components(\n        csgraph=graph, directed=False, return_labels=True\n    )\n    \n    clusters = [[] for _ in range(n_components)]\n    for client_idx, label in enumerate(labels):\n        clusters[label].append(client_idx)\n        \n    return clusters\n\ndef train_fedavg(client_data, client_indices, T, E, eta, d, n_i_all):\n    \"\"\"Runs the Federated Averaging algorithm for a given set of clients.\"\"\"\n    if not client_indices:\n        return np.zeros(d)\n        \n    w_server = np.zeros(d)\n    \n    sub_n_i = [n_i_all[i] for i in client_indices]\n    total_data_in_group = sum(sub_n_i)\n    \n    for _ in range(T):\n        local_models = []\n        for client_idx in client_indices:\n            w_local = w_server.copy()\n            data = client_data[client_idx]\n            for _ in range(E):\n                grad = compute_gradient(data, w_local)\n                w_local -= eta * grad\n            local_models.append(w_local)\n            \n        # Weighted aggregation\n        w_server = np.zeros(d)\n        for i, client_idx in enumerate(client_indices):\n            weight = n_i_all[client_idx] / total_data_in_group\n            w_server += weight * local_models[i]\n            \n    return w_server\n\ndef compute_loss(client_data, w):\n    \"\"\"Computes the empirical MSE loss.\"\"\"\n    X, y = client_data['X'], client_data['y']\n    n = len(y)\n    if n == 0:\n        return 0.0\n    error = X @ w - y\n    loss = np.mean(error**2)\n    return loss\n\ndef run_simulation(params):\n    \"\"\"Executes the full simulation for a single test case.\"\"\"\n    seed = params[\"seed\"]\n    C, d, n_i, sigma = params[\"C\"], params[\"d\"], params[\"n_i\"], params[\"sigma\"]\n    w_star, tau = params[\"w_star\"], params[\"tau\"]\n    T, E, eta = params[\"T\"], params[\"E\"], params[\"eta\"]\n\n    rng = np.random.default_rng(seed)\n    \n    # 1. Simulate data\n    all_client_data = generate_data(C, d, n_i, w_star, sigma, rng)\n\n    # 2. Compute initial gradients\n    w0 = np.zeros(d)\n    initial_gradients = [compute_gradient(all_client_data[i], w0) for i in range(C)]\n    \n    # 3. Form clusters\n    clusters = find_clusters(initial_gradients, tau, C)\n    client_to_cluster_map = {client: i for i, cluster in enumerate(clusters) for client in cluster}\n\n    # 4a. Train global model\n    all_clients = list(range(C))\n    w_global = train_fedavg(all_client_data, all_clients, T, E, eta, d, n_i)\n    \n    # 4b. Train cluster models\n    cluster_models = []\n    for cluster in clusters:\n        w_cluster = train_fedavg(all_client_data, cluster, T, E, eta, d, n_i)\n        cluster_models.append(w_cluster)\n\n    # 5. Evaluate final losses\n    total_loss_global = 0.0\n    for i in range(C):\n        total_loss_global += compute_loss(all_client_data[i], w_global)\n    mean_loss_global = total_loss_global / C\n\n    total_loss_cluster = 0.0\n    for i in range(C):\n        cluster_idx = client_to_cluster_map[i]\n        w_c = cluster_models[cluster_idx]\n        total_loss_cluster += compute_loss(all_client_data[i], w_c)\n    mean_loss_cluster = total_loss_cluster / C\n\n    # 6. Compute relative improvement\n    if mean_loss_global == 0:\n        # If global loss is zero, improvement is undefined unless cluster loss is also zero.\n        # This is highly unlikely with noise.\n        # A zero global loss implies a perfect model.\n        delta = 0.0 if mean_loss_cluster == 0.0 else -np.inf\n    else:\n        delta = (mean_loss_global - mean_loss_cluster) / mean_loss_global\n        \n    return delta\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}