{
    "hands_on_practices": [
        {
            "introduction": "标准的图卷积网络（GCN）在聚合邻居信息时，其输出可能对高自由度的“中心”节点过于敏感，并且容易受到图结构微小变化的干扰。这个练习将引导你探索一种对GCN架构的修改，以缓解这些问题。通过实现一个个性化的度上限来限制邻域信息的影响 ，你将亲身体验核心信息传递机制的细节及其对模型公平性和稳定性的影响，从而更深入地理解架构选择如何影响模型在具有不同节点度的图上的行为和鲁棒性。",
            "id": "3106143",
            "problem": "要求您在一个固定的无向图上实现一个带有个性化度上限的单层图卷积网络 (GCN) 变体，并评估在不同上限方案下的简单公平性和稳定性指标。您的实现必须遵循指定的数学定义，并生成一个单行输出，该输出汇集了多个测试用例的结果。\n\n使用的基本定义：\n- 一个图由一个无向邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 给出，该图有 $n$ 个节点且无自环。令 $I$ 表示大小为 $n \\times n$ 的单位矩阵。定义 $\\tilde{A} = A + I$ 以及 $\\tilde{D}$ 为 $\\tilde{A}$ 的对角度矩阵，其中 $\\tilde{D}_{ii} = \\sum_{j=1}^{n} \\tilde{A}_{ij}$。\n- 对称归一化的邻接矩阵为 $\\hat{A} = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$。\n- 令 $X \\in \\mathbb{R}^{n \\times d}$ 为节点特征，$W \\in \\mathbb{R}^{d \\times h}$ 为可训练权重；此处 $d$ 和 $h$ 均为小整数。使用逐元素应用的修正线性单元 (ReLU) 激活函数 $\\sigma(u) = \\max(0,u)$。\n\n个性化度上限：\n- 对于每个节点 $i \\in \\{1,\\dots,n\\}$，定义一个个性化上限 $c(i) \\in [0,\\infty]$，它在应用权重矩阵之前限制聚合邻域消息的大小。\n- 给定预激活的聚合消息 $m_i \\in \\mathbb{R}^{d}$（计算为 $\\hat{A} X$ 的第 $i$ 行），通过投影到半径为 $c(i)$ 的闭欧几里得球上来定义加帽消息：\n$$\n\\Pi_{c(i)}(m_i) \\;=\\; \n\\begin{cases}\nm_i,  \\text{if } \\lVert m_i \\rVert_2 \\le c(i), \\\\\n\\dfrac{c(i)}{\\lVert m_i \\rVert_2} \\, m_i,  \\text{if } \\lVert m_i \\rVert_2 > c(i),\n\\end{cases}\n$$\n约定 $\\Pi_{\\infty}(m_i) = m_i$ 且 $\\Pi_{0}(m_i) = 0$。\n- 那么，加帽的 GCN 层输出为\n$$\nH \\;=\\; \\sigma\\!\\left(\\Pi_{c}(\\hat{A} X) \\, W\\right),\n$$\n其中 $\\Pi_{c}(\\hat{A} X)$ 使用相应的 $c(i)$ 逐行应用投影。\n\n公平性和稳定性指标：\n- 令 $d(i)$ 表示原始图 $A$ 中节点 $i$ 的度（不包括自环）。\n- 公平性差距定义如下。令 $k = \\max(1, \\lfloor n/4 \\rfloor)$。令 $S_{\\text{low}}$ 为 $k$ 个度最小的节点的索引集合，$S_{\\text{high}}$ 为 $k$ 个度最大的节点的索引集合（平局可以通过任何一致的规则打破）。给定一个嵌入矩阵 $H \\in \\mathbb{R}^{n \\times h}$，令 $r_i = \\lVert H_{i,:} \\rVert_2$。公平性差距为\n$$\n\\Delta_{\\text{fair}}(H) \\;=\\; \\left| \\frac{1}{|S_{\\text{high}}|} \\sum_{i \\in S_{\\text{high}}} r_i \\;-\\; \\frac{1}{|S_{\\text{low}}|} \\sum_{i \\in S_{\\text{low}}} r_i \\right|.\n$$\n- 为边扰动定义一个稳定性指标。给定两个邻接矩阵 $A$ 和 $A^{\\prime}$（它们相差一条无向边的添加），以及相同的特征 $X$、权重 $W$ 和上限 $c$，计算\n$$\n\\Delta_{\\text{stab}} \\;=\\; \\lVert H^{\\prime} - H \\rVert_{\\mathrm{F}},\n$$\n其中 $H$ 是使用 $A$ 的输出，$H^{\\prime}$ 是使用 $A^{\\prime}$ 的输出，而 $\\lVert \\cdot \\rVert_{\\mathrm{F}}$ 表示弗罗贝尼乌斯范数 (Frobenius norm)。\n\n使用的图、特征和权重：\n- 节点数 $n = 8$。\n- 无向边集 $E$：\n$$\nE \\;=\\; \\{(0,1),(0,2),(0,3),(0,4),(0,5),(1,2),(2,3),(4,6),(6,7)\\}.\n$$\n- 因此，对于 $i,j \\in \\{0,\\dots,7\\}$，如果 $(i,j) \\in E$ 或 $(j,i) \\in E$，则 $A_{ij} = 1$，否则 $A_{ij} = 0$。\n- 节点特征 $X \\in \\mathbb{R}^{8 \\times 2}$：\n$$\nX \\;=\\; \\begin{bmatrix}\n2.0  -1.0 \\\\\n0.5  0.0 \\\\\n1.0  1.0 \\\\\n-1.5  0.5 \\\\\n0.0  -0.5 \\\\\n3.0  1.5 \\\\\n-0.5  2.0 \\\\\n1.0  -2.0\n\\end{bmatrix}.\n$$\n- 权重矩阵 $W \\in \\mathbb{R}^{2 \\times 2}$：\n$$\nW \\;=\\; \\begin{bmatrix}\n1.0  -0.5 \\\\\n0.3  0.8\n\\end{bmatrix}.\n$$\n\n要评估的上限方案：\n- 带有参数 $\\tau > 0$ 的基于度的上限：对每个节点 $i$，设置\n$$\nc_{\\tau}(i) \\;=\\; \\frac{\\tau}{\\sqrt{d(i) + 1}}.\n$$\n- 无上限基线：对所有 $i$，$c_{\\infty}(i) = \\infty$。\n- 零上限边缘情况：对所有 $i$，$c_{0}(i) = 0$。\n\n用于稳定性的边扰动：\n- 通过向 $A$ 中添加无向边 $(0,7)$ 来定义扰动图 $A^{\\prime}$。\n\n测试套件：\n- 测试 1 (上限下的公平性)：在原始图 $A$ 上，比较无上限基线与使用 $\\tau = 1.5$ 的度上限方案之间的公平性差距。如果加帽后的公平性差距小于或等于基线公平性差距，则输出布尔值 $true$，否则输出 $false$。\n- 测试 2 (上限下的稳定性)：在 $A$ 和 $A^{\\prime}$ 之间，为无上限基线和使用 $\\tau = 0.2$ 的度上限方案计算 $\\Delta_{\\text{stab}}$。如果加帽后的稳定性差异小于或等于基线稳定性差异，则输出布尔值 $true$，否则输出 $false$。\n- 测试 3 (退化上限)：当所有 $i$ 的 $c_{0}(i)=0$ 时，验证所有输出是否都恰好是零向量。如果 $H$ 的每一行都是零向量，则输出布尔值 $true$，否则输出 $false$。\n- 测试 4 (数值公平性报告)：在原始图 $A$ 上，使用 $\\tau = 0.8$ 的度上限方案计算公平性差距。将此公平性差距以浮点数形式输出，四舍五入到六位小数（使用标准四舍五入）。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果按此顺序排列：[$测试 1 结果, 测试 2 结果, 测试 3 结果, 测试 4 结果$]。例如，一个有效的输出行看起来像 $[true,true,false,0.123456]$，其中布尔值按照编程语言默认的布尔字符串表示法以小写形式打印，最后的浮点数四舍五入到六位小数。不应打印任何其他文本。",
            "solution": "所提出的问题是有效的，因为它在科学上基于图神经网络的原理，在数学上是适定的，并为获得唯一解提供了所有必要的数据和定义。我们将逐步进行解析。\n\n问题的核心是实现一个图卷积网络 (GCN) 层的变体并评估其属性。该层的输出 $H \\in \\mathbb{R}^{n \\times h}$ 由以下公式给出：\n$$\nH \\;=\\; \\sigma\\!\\left(\\Pi_{c}(\\hat{A} X) \\, W\\right)\n$$\n其中 $X \\in \\mathbb{R}^{n \\times d}$ 是输入特征矩阵，$W \\in \\mathbb{R}^{d \\times h}$ 是权重矩阵，$\\sigma$ 是 ReLU 激活函数，$\\Pi_c$ 是一个逐行投影算子，它对每个节点 $i$ 的聚合消息强制施加个性化上限 $c(i)$。矩阵 $\\hat{A}$ 是对称归一化的邻接矩阵，由图的原始邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 派生而来。\n\n我们将首先根据所提供的图结构和数据，建立基础矩阵和向量。\n\n**1. 图表示与归一化**\n\n该图有 $n=8$ 个节点和一组无向边 $E$。邻接矩阵 $A$ 的构造方式是，如果节点 $i$ 和 $j$ 之间存在边，则 $A_{ij} = 1$，否则 $A_{ij} = 0$。我们添加自环以构成 $\\tilde{A} = A + I$，其中 $I$ 是 $8 \\times 8$ 的单位矩阵。这确保了节点的自身特征包含在其聚合消息中。\n\n$\\tilde{A}$ 的度矩阵 $\\tilde{D}$ 是一个对角矩阵，其中 $\\tilde{D}_{ii}$ 是 $\\tilde{A}$ 第 $i$ 行的和，这等价于 $d(i) + 1$，其中 $d(i)$ 是原始图 $A$ 中节点 $i$ 的度。给定图的度为：\n$d = [d(0), \\dots, d(7)] = [5, 2, 3, 2, 2, 1, 2, 1]$。\n因此，$\\tilde{D}$ 的对角线元素为 $[\\tilde{d}(0), \\dots, \\tilde{d}(7)] = [6, 3, 4, 3, 3, 2, 3, 2]$。\n\n对称归一化的邻接矩阵 $\\hat{A} = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$ 起着关键作用。其元素为 $\\hat{A}_{ij} = \\frac{\\tilde{A}_{ij}}{\\sqrt{\\tilde{d}(i)}\\sqrt{\\tilde{d}(j)}}$。这种归一化对来自邻居的特征进行平均，防止聚合消息的规模被高度节点主导。\n\n**2. 核心计算函数：加帽 GCN 层**\n\n我们将构建一个计算过程，该过程以邻接矩阵 ($A$)、特征 ($X$)、权重 ($W$) 和一个上限值向量作为输入，并生成输出矩阵 $H$。该过程如下：\n- 从输入 $A$ 计算 $\\tilde{A}$、$\\tilde{D}$ 和 $\\hat{A}$。\n- 计算预激活的聚合消息 $M = \\hat{A} X$。$M$ 的每一行 $m_i$ 代表节点 $i$ 的聚合信息。\n- 应用个性化上限。对于每一行 $m_i$，我们计算其 L2 范数 $\\lVert m_i \\rVert_2$。如果该范数超过了节点的特定上限 $c(i)$，我们通过缩放将向量 $m_i$ 投影到半径为 $c(i)$ 的欧几里得球的边界上：$m_i \\leftarrow \\frac{c(i)}{\\lVert m_i \\rVert_2} m_i$。对于已在球内的向量，此操作是幂等的。具体的上限方案在问题中定义。“无上限”基线对应于 $c(i)=\\infty$，“零上限”情况对应于 $c(i)=0$，这将所有消息强制为零向量。\n- 对加帽后的消息应用权重矩阵 $W$ 进行线性变换，得到 $Z = \\Pi_c(M) W$。\n- 应用逐元素的 ReLU 激活函数 $H = \\sigma(Z)$，以生成最终的节点嵌入。\n\n**3. 公平性与稳定性指标**\n\n*公平性差距 ($ \\Delta_{\\text{fair}} $):*\n该指标评估了高度节点和低度节点之间输出嵌入大小的差异。我们首先确定对应于 $k = \\max(1, \\lfloor n/4 \\rfloor) = \\max(1, \\lfloor 8/4 \\rfloor) = 2$ 个最低度 ($S_{\\text{low}}$) 和 $k=2$ 个最高度 ($S_{\\text{high}}$) 的节点索引集。按度对节点排序得到 $d=(1,1,2,2,2,2,3,5)$。一个一致的平局打破规则（例如，使用节点索引）得出 $S_{\\text{low}} = \\{5, 7\\}$ 和 $S_{\\text{high}} = \\{0, 2\\}$。然后我们计算每组输出嵌入的平均 L2 范数，并取其绝对差：$\\Delta_{\\text{fair}}(H) = \\left| \\text{mean}_{i \\in S_{\\text{high}}} \\lVert H_{i,:} \\rVert_2 - \\text{mean}_{i \\in S_{\\text{low}}} \\lVert H_{i,:} \\rVert_2 \\right|$。\n\n*稳定性 ($\\Delta_{\\text{stab}}$):*\n该指标衡量 GCN 输出对图结构中微小扰动的敏感性。我们计算在原始图 $A$ 和扰动图 $A'$（添加了边 $(0,7)$）上分别计算出的输出嵌入矩阵 $H$ 和 $H'$ 之间差值的弗罗贝尼乌斯范数 (Frobenius norm)：$\\Delta_{\\text{stab}} = \\lVert H' - H \\rVert_F$。\n\n**4. 执行测试套件**\n\n现在我们将这些计算应用于每个测试用例。\n\n**测试 1：上限下的公平性**\n- 我们在图 $A$ 上使用无上限方案（$c(i) = \\infty$）计算 $H_{\\text{baseline}}$。\n- 我们在图 $A$ 上使用基于度的上限 $c_{\\tau}(i) = \\frac{\\tau}{\\sqrt{d(i)+1}}$（其中 $\\tau=1.5$）计算 $H_{\\text{capped}}$。\n- 然后我们计算 $\\Delta_{\\text{fair}}(H_{\\text{baseline}})$ 和 $\\Delta_{\\text{fair}}(H_{\\text{capped}})$。\n- 结果是表达式 $\\Delta_{\\text{fair}}(H_{\\text{capped}}) \\le \\Delta_{\\text{fair}}(H_{\\text{baseline}})$ 的布尔值。\n\n**测试 2：上限下的稳定性**\n- 我们通过向 $A$ 中添加边 $(0,7)$ 来定义扰动图 $A'$。这将度更改为 $d'(0)=6$ 和 $d'(7)=2$。\n- 对于基线情况，我们在 $A$ 上计算 $H$，在 $A'$ 上计算 $H'$，两者都无上限，然后求得 $\\Delta_{\\text{stab, baseline}} = \\lVert H'_{\\text{baseline}} - H_{\\text{baseline}} \\rVert_F$。\n- 对于加帽情况，使用 $\\tau=0.2$，我们使用 $d$ 为 $A$ 计算上限，使用 $d'$ 为 $A'$ 计算上限。然后我们在 $A$ 上计算 $H_{\\text{capped}}$，在 $A'$ 上计算 $H'_{\\text{capped}}$，并求得 $\\Delta_{\\text{stab, capped}} = \\lVert H'_{\\text{capped}} - H_{\\text{capped}} \\rVert_F$。\n- 结果是表达式 $\\Delta_{\\text{stab, capped}} \\le \\Delta_{\\text{stab, baseline}}$ 的布尔值。\n\n**测试 3：退化上限**\n- 我们使用零上限方案，即对所有 $i$ 都有 $c(i)=0$。对于任何消息 $m_i$，投影 $\\Pi_0(m_i)$ 的结果都是零向量。\n- 因此，加帽后的消息矩阵 $\\Pi_0(\\hat{A}X)$ 是一个全零矩阵。\n- 随后的线性变换得出 $Z = \\mathbf{0} \\cdot W = \\mathbf{0}$。\n- 最终输出为 $H = \\sigma(\\mathbf{0}) = \\mathbf{0}$。\n- 该测试验证计算出的 $H$ 是否为零矩阵，根据数学定义，这必须为真。\n\n**测试 4：数值公平性报告**\n- 我们在原始图 $A$ 上使用基于度的上限，其中 $\\tau=0.8$。\n- 我们计算相应的输出 $H$ 并计算其公平性差距 $\\Delta_{\\text{fair}}(H)$。\n- 结果是这个浮点值，四舍五入到 $6$ 位小数。\n\n最终输出是这四个结果按指定格式的聚合。以下代码实现了这一逻辑。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem by executing the four test cases.\n    \"\"\"\n    # Graph, features, and weights definition.\n    n = 8\n    E = [(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (1, 2), (2, 3), (4, 6), (6, 7)]\n    X = np.array([\n        [2.0, -1.0], [0.5, 0.0], [1.0, 1.0], [-1.5, 0.5],\n        [0.0, -0.5], [3.0, 1.5], [-0.5, 2.0], [1.0, -2.0]\n    ])\n    W = np.array([[1.0, -0.5], [0.3, 0.8]])\n\n    def build_adjacency(n_nodes, edges):\n        A = np.zeros((n_nodes, n_nodes))\n        for i, j in edges:\n            A[i, j] = 1\n            A[j, i] = 1\n        return A\n\n    A = build_adjacency(n, E)\n    d = A.sum(axis=1)\n\n    A_prime = build_adjacency(n, E + [(0, 7)])\n    d_prime = A_prime.sum(axis=1)\n\n    def _compute_h(A_matrix, X_matrix, W_matrix, cap_values):\n        \"\"\"\n        Computes the GCN layer output H for a given graph, features, weights, and caps.\n        \"\"\"\n        num_nodes = A_matrix.shape[0]\n        A_tilde = A_matrix + np.eye(num_nodes)\n        D_tilde_vec = A_tilde.sum(axis=1)\n        \n        # Guard against division by zero for isolated nodes, though none exist here.\n        D_tilde_inv_sqrt_vec = np.power(D_tilde_vec, -0.5)\n        D_tilde_inv_sqrt_vec[np.isinf(D_tilde_inv_sqrt_vec)] = 0.0\n        \n        D_inv_sqrt_mat = np.diag(D_tilde_inv_sqrt_vec)\n        A_hat = D_inv_sqrt_mat @ A_tilde @ D_inv_sqrt_mat\n\n        M = A_hat @ X_matrix\n        M_capped = M.copy()\n\n        for i in range(num_nodes):\n            cap = cap_values[i]\n            if np.isinf(cap):\n                continue\n            \n            norm_mi = np.linalg.norm(M_capped[i, :])\n            \n            if norm_mi > cap:\n                # The projection formula covers the c=0 case when norm > 0.\n                if norm_mi > 1e-9: # Avoid division by zero\n                    M_capped[i, :] = (cap / norm_mi) * M_capped[i, :]\n                else: # if norm is zero, it's already capped.\n                    pass\n        \n        Z = M_capped @ W_matrix\n        H = np.maximum(Z, 0)\n        return H\n\n    def _compute_fairness_gap(H_matrix, degree_vec):\n        \"\"\"\n        Computes the fairness gap for a given output embedding H.\n        \"\"\"\n        num_nodes = H_matrix.shape[0]\n        k = max(1, num_nodes // 4)\n        \n        node_indices = np.arange(num_nodes)\n        # Sort by degree, using node index as a consistent tie-breaker.\n        sorted_indices = np.lexsort((node_indices, degree_vec))\n        \n        s_low = sorted_indices[:k]\n        s_high = sorted_indices[-k:]\n        \n        r = np.linalg.norm(H_matrix, axis=1)\n        \n        avg_low_norm = np.mean(r[s_low])\n        avg_high_norm = np.mean(r[s_high])\n        \n        return np.abs(avg_high_norm - avg_low_norm)\n\n    results = []\n    \n    # Test 1: Fairness\n    caps_inf = np.full(n, np.inf)\n    H_baseline = _compute_h(A, X, W, caps_inf)\n    fairness_baseline = _compute_fairness_gap(H_baseline, d)\n    \n    tau1 = 1.5\n    caps_tau1 = tau1 / np.sqrt(d + 1)\n    H_capped_tau1 = _compute_h(A, X, W, caps_tau1)\n    fairness_capped_tau1 = _compute_fairness_gap(H_capped_tau1, d)\n    \n    test1_result = fairness_capped_tau1 = fairness_baseline\n    results.append(str(test1_result).lower())\n\n    # Test 2: Stability\n    H_prime_baseline = _compute_h(A_prime, X, W, caps_inf)\n    stab_baseline = np.linalg.norm(H_prime_baseline - H_baseline, 'fro')\n\n    tau2 = 0.2\n    caps_tau2_A = tau2 / np.sqrt(d + 1)\n    caps_tau2_A_prime = tau2 / np.sqrt(d_prime + 1)\n    \n    H_capped_A = _compute_h(A, X, W, caps_tau2_A)\n    H_capped_A_prime = _compute_h(A_prime, X, W, caps_tau2_A_prime)\n    stab_capped = np.linalg.norm(H_capped_A_prime - H_capped_A, 'fro')\n    \n    test2_result = stab_capped = stab_baseline\n    results.append(str(test2_result).lower())\n\n    # Test 3: Degenerate Cap\n    caps_zero = np.zeros(n)\n    H_zero = _compute_h(A, X, W, caps_zero)\n    test3_result = np.all(H_zero == 0)\n    results.append(str(test3_result).lower())\n\n    # Test 4: Numeric Fairness Report\n    tau4 = 0.8\n    caps_tau4 = tau4 / np.sqrt(d + 1)\n    H_capped_tau4 = _compute_h(A, X, W, caps_tau4)\n    fairness_tau4 = _compute_fairness_gap(H_capped_tau4, d)\n    \n    test4_result = f\"{fairness_tau4:.6f}\"\n    results.append(test4_result)\n\n    # Final print statement\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "将图神经网络（GNNs）应用于大规模图时，由于“邻居爆炸”问题，会带来巨大的计算挑战，即随着层数加深，所需计算的节点数量呈指数级增长。GraphSAGE架构通过邻居采样提供了一种优雅的解决方案。在这个练习中 ，你将实现一个简化的GraphSAGE模型，以量化分析邻居采样大小、计算成本和模型性能之间的权衡。通过这个实践，你将掌握扩展GNN的关键技术，并学会如何凭经验分析效率与准确性之间的平衡，这对于GNN在现实世界中的应用至关重要。",
            "id": "3106236",
            "problem": "您将实现并研究一个简化的图采样与聚合 (GraphSAGE) 模型，用于节点分类，以量化邻居采样大小如何影响计算成本、训练准确率和经验梯度噪声尺度。此设定纯属数学和算法层面，不涉及物理单位。所有角度在此均不相关。\n\n使用的基本原理和定义：\n- 图采样与聚合 (GraphSAGE) 与均值聚合：给定一个带有节点特征的图，GraphSAGE 的均值聚合器通过对节点的邻居特征进行平均来聚合它们。您将使用一个线性模型，其中节点的 logit 是两个线性形式的和，一个应用于节点自身的特征，另一个应用于采样邻居特征的均值。\n- 使用 logistic 函数的二元交叉熵 (BCE) 损失：对于二元标签 $y \\in \\{0,1\\}$ 和 logit $z \\in \\mathbb{R}$，logistic 函数为 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$。每个样本的 BCE 损失为 $-\\left(y \\log \\sigma(z) + (1-y)\\log(1-\\sigma(z))\\right)$。损失对 logit 的导数是 $\\sigma(z) - y$。\n- 期望和方差：经验梯度噪声尺度将定义为在邻居采样下，随机梯度与全批量梯度之间归一化均方偏差。\n\n任务和设置：\n- 构建一个包含 $n=80$ 个节点（索引为 $v \\in \\{0,1,\\dots,79\\}$）的无向环形图，其中每个节点与其两侧的 $2$ 个最近邻居相连。形式上，将每个节点 $v$ 连接到 $(v \\pm 1) \\bmod 80$ 和 $(v \\pm 2) \\bmod 80$。这将生成一个度为 $4$ 的正则图。\n- 创建维度为 $d=8$ 的节点特征 $x_v \\in \\mathbb{R}^d$，从标准正态分布中抽取。固定一个真实权重向量 $q \\in \\mathbb{R}^d$，该向量从标准正态分布中采样，然后归一化为单位长度。对于每个节点 $v$，定义全邻居平均特征 $\\bar{x}_{\\mathcal{N}(v)} = \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} x_u$，其中 $\\mathcal{N}(v)$ 表示 $v$ 的所有邻居集合。定义用于数据生成的 logit 为 $z_v^{\\text{true}} = q^\\top \\left( \\alpha x_v + (1-\\alpha)\\bar{x}_{\\mathcal{N}(v)} \\right)$，其中 $\\alpha = 0.5$。通过 $y_v = \\mathbf{1}[z_v^{\\text{true}} \\ge 0]$ 确定性地生成标签。\n- 模型：使用一个单层的线性 GraphSAGE 风格预测器，该预测器具有均值聚合和 logistic 输出。对于一个节点 $v$，通过从 $\\mathcal{N}(v)$ 中进行不放回均匀采样来抽取一个邻居集合 $\\mathcal{S}_s(v)$，采样的邻居数量为 $s$。如果 $s \\ge |\\mathcal{N}(v)|$，则使用所有邻居。计算采样均值 $\\bar{x}_{\\mathcal{S}_s(v)} = \\frac{1}{|\\mathcal{S}_s(v)|} \\sum_{u \\in \\mathcal{S}_s(v)} x_u$。模型的 logit 是：\n$$\nz_v(W_{\\text{self}}, W_{\\text{neigh}}; s) = W_{\\text{self}}^\\top x_v + W_{\\text{neigh}}^\\top \\bar{x}_{\\mathcal{S}_s(v)},\n$$\n其中参数 $W_{\\text{self}} \\in \\mathbb{R}^d$ 和 $W_{\\text{neigh}} \\in \\mathbb{R}^d$ 是待训练的。\n- 训练目标：使用全批量梯度下降最小化所有节点的平均二元交叉熵损失，其中每个节点的梯度使用采样均值 $\\bar{x}_{\\mathcal{S}_s(v)}$ 计算。对于节点 $v$，设 $p_v = \\sigma(z_v)$，误差项为 $\\delta_v = p_v - y_v$。每个节点对参数梯度的贡献是\n$$\n\\nabla_{W_{\\text{self}}} \\ell_v = \\delta_v \\, x_v, \\quad \\nabla_{W_{\\text{neigh}}} \\ell_v = \\delta_v \\, \\bar{x}_{\\mathcal{S}_s(v)}.\n$$\n- 训练协议：初始化 $W_{\\text{self}} = 0$ 和 $W_{\\text{neigh}} = 0$，使用固定的学习率 $\\eta = 0.1$，并执行 $E=200$ 个 epoch 的全批量梯度下降。在每个 epoch 中，为每个节点重新采样 $\\mathcal{S}_s(v)$。为确保可复现性，对每个 $s$ 使用固定的随机种子。\n- 评估：对于每个 $s$ 训练后，仅使用全邻居计算准确率（即，用 $\\bar{x}_{\\mathcal{N}(v)}$ 替换 $\\bar{x}_{\\mathcal{S}_s(v)}$），并预测 $\\hat{y}_v = \\mathbf{1}[z_v \\ge 0]$。将准确率报告为区间 $[0,1]$ 内的一个浮点数，四舍五入到四位小数。\n\n计算成本代理：\n- 将每个 epoch 的计算成本 $C(s)$ 定义为邻居特征读取的总次数，即，\n$$\nC(s) = \\sum_{v=0}^{n-1} \\min\\{s, |\\mathcal{N}(v)|\\} \\cdot d.\n$$\n对于给定的度为 $4$ 的正则图，这简化为 $C(s) = n \\cdot \\min\\{s, 4\\} \\cdot d$。将 $C(s)$ 报告为整数。\n\n经验梯度噪声尺度：\n- 将参数固定在初始值 $W_{\\text{self}}=0$ 和 $W_{\\text{neigh}}=0$。使用全邻居定义全批量梯度为\n$$\ng_{\\text{full}} = \\nabla \\left( \\frac{1}{n} \\sum_{v=0}^{n-1} \\ell_v \\right) \\Bigg|_{\\bar{x}_{\\mathcal{N}(v)}} \\in \\mathbb{R}^{2d}.\n$$\n对于给定的采样大小 $s$，使用采样邻居均值定义随机全批量梯度估计为\n$$\ng^{(r)}_s = \\nabla \\left( \\frac{1}{n} \\sum_{v=0}^{n-1} \\ell_v \\right) \\Bigg|_{\\bar{x}_{\\mathcal{S}^{(r)}_s(v)}} \\in \\mathbb{R}^{2d},\n$$\n其中 $\\mathcal{S}^{(r)}_s(v)$ 表示对每个节点邻居的第 $r$ 次随机采样。\n- 通过\n$$\n\\mathcal{G}(s) = \\frac{ \\mathbb{E}_r \\left[ \\left\\| g^{(r)}_s - g_{\\text{full}} \\right\\|_2^2 \\right] }{ \\left\\| g_{\\text{full}} \\right\\|_2^2 },\n$$\n来估计经验梯度噪声尺度，其中期望通过对 $R=256$ 次独立采样进行平均来近似。将 $\\mathcal{G}(s)$ 报告为四舍五入到六位小数的值。\n\n测试套件：\n- 使用采样大小 $s \\in \\{1, 2, 4, 100\\}$。请注意，$s=100$ 测试了对所有 $v$ 都有 $s \\ge |\\mathcal{N}(v)|$ 的边界情况。\n- 对于所有随机组件，按如下方式固定随机种子：\n    - 数据生成种子：$123$。\n    - 每个 $s$ 的训练种子：使用 $1000 + s$。\n    - 每个 $s$ 的梯度噪声估计种子：使用 $2000 + s$。\n- 超参数：$n=80$，$d=8$，$\\alpha=0.5$，$\\eta=0.1$，$E=200$，$R=256$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个以逗号分隔的列表的列表，不含空格，顺序如下：\n$$\n\\big[ [s\\_1,\\dots,s\\_k], [\\text{acc}(s\\_1),\\dots,\\text{acc}(s\\_k)], [C(s\\_1),\\dots,C(s\\_k)], [\\mathcal{G}(s\\_1),\\dots,\\mathcal{G}(s\\_k)] \\big],\n$$\n其中准确率四舍五入到四位小数，噪声尺度四舍五入到六位小数。对于本问题，$k=4$ 且 $[s_1,\\dots,s_4] = [1,2,4,100]$。一个精确格式的示例如下\n$[[1,2,4,100],[0.9000,0.9200,0.9300,0.9300],[640,1280,2560,2560],[0.123456,0.100000,0.000001,0.000000]]$\n但需使用您计算出的值。",
            "solution": "用户提供的问题是图神经网络领域内一个明确定义的计算任务，因此是有效的。以下解决方案实现了指定的简化版图采样与聚合 (GraphSAGE) 模型，以分析邻居采样的影响。\n\n### 1. 图和数据合成\n\n首先，我们根据问题的规范构建计算图并生成合成数据集。\n\n- **图结构**：创建一个包含 $n=80$ 个节点的无向环形图。每个节点 $v$ 与其四个最近的邻居相连，具体是索引为 $(v \\pm 1) \\pmod{80}$ 和 $(v \\pm 2) \\pmod{80}$ 的节点。这会产生一个 $k$-正则图，其中每个节点的度都为 $|\\mathcal{N}(v)|=4$。使用邻接表来存储图结构。\n- **节点特征**：对于每个节点 $v$，我们生成一个维度为 $d=8$ 的特征向量 $x_v \\in \\mathbb{R}^d$。每个向量的分量独立地从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取。为保证可复现性，此过程使用 $123$作为随机种子。\n- **真实标签**：基准真相标签是使用一个“教师”模型生成的。一个真实权重向量 $q \\in \\mathbb{R}^d$ 从标准正态分布中采样，然后归一化为单位 $\\ell_2$-范数，即 $\\|q\\|_2=1$。对于每个节点 $v$，其全邻居平均特征向量计算为 $\\bar{x}_{\\mathcal{N}(v)} = \\frac{1}{|\\mathcal{N}(v)|} \\sum_{u \\in \\mathcal{N}(v)} x_u$。然后将真实 logit 定义为 $z_v^{\\text{true}} = q^\\top \\left( \\alpha x_v + (1-\\alpha)\\bar{x}_{\\mathcal{N}(v)} \\right)$，其中 $\\alpha=0.5$。节点 $v$ 的二元标签被确定性地设置为 $y_v = \\mathbf{1}[z_v^{\\text{true}} \\ge 0]$。\n\n### 2. GraphSAGE 模型与训练\n\n我们为二元节点分类实现了一个单层的线性 GraphSAGE 风格模型。对每个指定的采样大小 $s \\in \\{1, 2, 4, 100\\}$，该过程都会被迭代执行。\n\n- **模型架构**：对于给定节点 $v$ 和采样大小 $s$，我们首先通过从全邻域 $\\mathcal{N}(v)$ 中不放回地均匀采样 $s' = \\min\\{s, |\\mathcal{N}(v)|\\}$ 个邻居，来形成一个邻居子集 $\\mathcal{S}_s(v)$。然后我们计算采样邻居的平均特征 $\\bar{x}_{\\mathcal{S}_s(v)} = \\frac{1}{|\\mathcal{S}_s(v)|} \\sum_{u \\in \\mathcal{S}_s(v)} x_u$。模型对节点 $v$ 的 logit 是一个线性组合：\n$$\nz_v(W_{\\text{self}}, W_{\\text{neigh}}; s) = W_{\\text{self}}^\\top x_v + W_{\\text{neigh}}^\\top \\bar{x}_{\\mathcal{S}_s(v)}\n$$\n其中 $W_{\\text{self}} \\in \\mathbb{R}^d$ 和 $W_{\\text{neigh}} \\in \\mathbb{R}^d$ 是可训练的权重向量，初始化为零向量。\n\n- **训练过程**：训练模型以最小化所有 $n$ 个节点的平均二元交叉熵 (BCE) 损失。我们使用全批量梯度下降法，学习率为 $\\eta = 0.1$，进行 $E=200$ 个 epoch 的训练。在每个 epoch 中：\n    1. 对于每个节点 $v$，采样一个新的邻居集合 $\\mathcal{S}_s(v)$。对于给定的 $s$，整个训练过程使用一个特定的随机种子 $1000+s$。\n    2. 使用 logistic 函数计算模型的预测值，$p_v = \\sigma(z_v) = (1 + e^{-z_v})^{-1}$。\n    3. BCE 损失梯度的误差项是 $\\delta_v = p_v - y_v$。\n    4. 每个节点的梯度为 $\\nabla_{W_{\\text{self}}} \\ell_v = \\delta_v x_v$ 和 $\\nabla_{W_{\\text{neigh}}} \\ell_v = \\delta_v \\bar{x}_{\\mathcal{S}_s(v)}$。\n    5. 将这些梯度在所有节点上求和，以形成全批量梯度：$\\nabla_{W} L = \\sum_{v=0}^{n-1} \\nabla_{W} \\ell_v$。\n    6. 权重更新：$W \\leftarrow W - \\eta \\cdot \\frac{1}{n} \\nabla_{W} L$。\n\n### 3. 评估指标\n\n在对每个 $s$ 值进行训练后，我们计算三个所需的指标：计算成本、准确率和梯度噪声尺度。\n\n- **计算成本 ($C(s)$)**：这是每个 epoch 计算工作量的代理指标，定义为访问的邻居特征总数。对于我们的 $4$-正则图，它简化为 $C(s) = n \\cdot \\min\\{s, 4\\} \\cdot d$。\n\n- **准确率**：经过 $E=200$ 个训练 epoch 后，评估模型的性能。关键的是，无论训练期间使用的采样大小 $s$ 是多少，评估始终使用全邻域信息进行。评估 logit 为 $z_v^{\\text{eval}} = W_{\\text{self}}^\\top x_v + W_{\\text{neigh}}^\\top \\bar{x}_{\\mathcal{N}(v)}$，预测值为 $\\hat{y}_v = \\mathbf{1}[z_v^{\\text{eval}} \\ge 0]$。准确率是正确分类节点的比例，即 $\\frac{1}{n}\\sum_{v=0}^{n-1} \\mathbf{1}[\\hat{y}_v=y_v]$，四舍五入到四位小数。\n\n- **经验梯度噪声尺度 ($\\mathcal{G}(s)$)**：该指标量化了在训练开始时（即在初始权重 $W_{\\text{self}}, W_{\\text{neigh}} = 0$ 时）由邻居采样引入的噪声。此计算使用专用种子 $2000+s$。\n    1. **全梯度 ($g_{\\text{full}}$)**：首先，使用所有节点的全邻域 $\\mathcal{N}(v)$ 计算“真实”的批量梯度。在 $W=0$ 时，logit $z_v=0$，预测值 $p_v = \\sigma(0) = 0.5$，误差 $\\delta_v = 0.5 - y_v$。梯度为 $g_{\\text{full}} = \\frac{1}{n} \\sum_{v=0}^{n-1} [\\delta_v x_v, \\delta_v \\bar{x}_{\\mathcal{N}(v)}]$。\n    2. **随机梯度 ($g_s^{(r)}$)**：接下来，我们生成 $R=256$ 个独立的随机梯度估计。每个估计值 $g_s^{(r)}$ 的计算方式与 $g_{\\text{full}}$ 类似，但对每个节点 $v$ 使用新采样的邻居均值 $\\bar{x}_{\\mathcal{S}^{(r)}_s(v)}$。\n    3. **噪声尺度计算**：噪声尺度是随机梯度的均方误差与全梯度范数平方的比值：\n    $$\n    \\mathcal{G}(s) = \\frac{ \\frac{1}{R} \\sum_{r=1}^{R} \\left\\| g^{(r)}_s - g_{\\text{full}} \\right\\|_2^2 }{ \\left\\| g_{\\text{full}} \\right\\|_2^2 }\n    $$\n    当 $s \\ge 4$ 时，采样是确定性的（所有邻居都被选中），因此对所有 $r$ 都有 $g_s^{(r)} = g_{\\text{full}}$，这使得 $\\mathcal{G}(s)=0$。最终值四舍五入到六位小数。\n\n对测试套件中的每个 $s$ 执行这整个过程，并将结果汇总到指定的列表的列表格式中。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a simplified GraphSAGE model analysis as specified in the problem.\n    This function performs data generation, model training, and evaluation for different\n    neighbor sampling sizes, and computes accuracy, computational cost, and gradient noise.\n    \"\"\"\n    # Hyperparameters and constants\n    N = 80\n    D = 8\n    ALPHA = 0.5\n    ETA = 0.1\n    EPOCHS = 200\n    R_SAMPLES = 256\n    S_VALUES = [1, 2, 4, 100]\n    NUM_TOTAL_NEIGHBORS = 4\n\n    # --- Part 1: Data and Graph Generation ---\n    rng_data = np.random.default_rng(123)\n\n    # 1.1. Graph Construction (undirected ring graph)\n    adj = {i: [] for i in range(N)}\n    for i in range(N):\n        for offset in [-2, -1, 1, 2]:\n            adj[i].append((i + offset) % N)\n    adj_np = np.array([sorted(adj[i]) for i in range(N)])\n\n    # 1.2. Feature Generation\n    X = rng_data.standard_normal(size=(N, D))\n\n    # 1.3. True Model and Label Generation\n    q_true = rng_data.standard_normal(size=D)\n    q_true /= np.linalg.norm(q_true)\n\n    # Pre-calculate full-neighbor means (vectorized)\n    X_bar_full = X[adj_np].mean(axis=1)\n\n    # Calculate true logits and labels\n    z_true = np.sum(q_true * (ALPHA * X + (1 - ALPHA) * X_bar_full), axis=1)\n    Y = (z_true = 0).astype(int)\n\n    # --- Part 2: Main Loop over Sampling Sizes ---\n    accuracies = []\n    costs = []\n    noise_scales = []\n\n    for s in S_VALUES:\n        num_neighbors_to_sample = min(s, NUM_TOTAL_NEIGHBORS)\n\n        # 2.1. Computational Cost Proxy\n        cost = N * num_neighbors_to_sample * D\n        costs.append(cost)\n\n        # 2.2. Training\n        rng_train = np.random.default_rng(1000 + s)\n        W_self = np.zeros(D)\n        W_neigh = np.zeros(D)\n\n        for _ in range(EPOCHS):\n            # Sample neighbors for all nodes for this epoch\n            sampled_indices = np.empty((N, num_neighbors_to_sample), dtype=int)\n            for i in range(N):\n                sampled_indices[i] = rng_train.choice(\n                    adj_np[i], size=num_neighbors_to_sample, replace=False\n                )\n            \n            X_bar_sampled = X[sampled_indices].mean(axis=1)\n            \n            # Vectorized logit and gradient calculation\n            logits = np.sum(W_self * X, axis=1) + np.sum(W_neigh * X_bar_sampled, axis=1)\n            p = 1 / (1 + np.exp(-logits))\n            delta = p - Y\n            \n            grad_W_self_batch = np.dot(delta, X)\n            grad_W_neigh_batch = np.dot(delta, X_bar_sampled)\n\n            # Update weights\n            W_self -= ETA * grad_W_self_batch / N\n            W_neigh -= ETA * grad_W_neigh_batch / N\n        \n        # 2.3. Evaluation (Accuracy)\n        eval_logits = np.sum(W_self * X, axis=1) + np.sum(W_neigh * X_bar_full, axis=1)\n        y_hat = (eval_logits = 0).astype(int)\n        accuracy = np.mean(y_hat == Y)\n        accuracies.append(round(accuracy, 4))\n        \n        # 2.4. Gradient Noise Scale\n        rng_noise = np.random.default_rng(2000 + s)\n        \n        # Calculate full gradient (g_full) at initial weights W=0\n        p_init = 0.5\n        delta_init = p_init - Y\n        grad_W_self_full = np.dot(delta_init, X) / N\n        grad_W_neigh_full = np.dot(delta_init, X_bar_full) / N\n        g_full = np.concatenate([grad_W_self_full, grad_W_neigh_full])\n        g_full_norm_sq = np.dot(g_full, g_full)\n\n        if g_full_norm_sq == 0 or num_neighbors_to_sample == NUM_TOTAL_NEIGHBORS:\n            noise_scale = 0.0\n        else:\n            total_squared_deviation = 0.0\n            grad_W_self_stoch = np.dot(delta_init, X) / N # Constant part\n            for _ in range(R_SAMPLES):\n                sampled_indices_r = np.empty((N, num_neighbors_to_sample), dtype=int)\n                for i in range(N):\n                    sampled_indices_r[i] = rng_noise.choice(\n                        adj_np[i], size=num_neighbors_to_sample, replace=False\n                    )\n                X_bar_sampled_noise = X[sampled_indices_r].mean(axis=1)\n\n                grad_W_neigh_stoch = np.dot(delta_init, X_bar_sampled_noise) / N\n                g_s = np.concatenate([grad_W_self_stoch, grad_W_neigh_stoch])\n\n                deviation = g_s - g_full\n                total_squared_deviation += np.dot(deviation, deviation)\n            \n            mean_squared_deviation = total_squared_deviation / R_SAMPLES\n            noise_scale = mean_squared_deviation / g_full_norm_sq\n\n        noise_scales.append(round(noise_scale, 6))\n\n    # --- Part 3: Final Output ---\n    s_str = f\"[{','.join(map(str, S_VALUES))}]\"\n    acc_str = f\"[{','.join([f'{acc:.4f}' for acc in accuracies])}]\"\n    cost_str = f\"[{','.join(map(str, costs))}]\"\n    noise_str = f\"[{','.join([f'{ns:.6f}' for ns in noise_scales])}]\"\n    \n    final_output = f\"[{s_str},{acc_str},{cost_str},{noise_str}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "在计算出节点级别的嵌入之后，许多GNN应用（如图分类）需要为整个图生成一个单一的、固定大小的表示。这个过程通过一个对节点顺序不敏感的“读出”（readout）或“池化”（pooling）函数来实现。本练习  要求你比较几种常见的读出函数，如求和、平均值和基于注意力的池化，并分析它们各自如何聚合节点信息以及对图大小的敏感性。这项实践将帮助你巩固从节点级表示过渡到图级预测的理解，这是许多GNN工作流中的关键一步。",
            "id": "3106222",
            "problem": "给定一个置换不变的图读出函数族，该函数族将节点嵌入的多重集 $\\{h_i\\}_{i=1}^{|V|}$（其中 $h_i \\in \\mathbb{R}^3$）映射到一个图级嵌入 $g \\in \\mathbb{R}^3$。一个图级分类器通过一个固定的线性层和一个偏置来计算一个 logit $z \\in \\mathbb{R}$。需要比较的四种读出函数是：按元素求和、按元素取最大值、按元素求均值，以及一个基于注意力并使用 softmax 权重的池化。目标是量化在指定的节点嵌入生成器下，分类器的 logit 如何随节点数 $|V|$ 的变化而变化。所有计算均为纯数值计算，无单位。\n\n基本依据和定义：\n- 如果一个读出函数仅依赖于多重集 $\\{h_i\\}$ 而与索引的顺序无关，则该读出函数是置换不变的。对于以下定义，设 $H = [h_1,\\dots,h_{|V|}]^\\top \\in \\mathbb{R}^{|V|\\times 3}$，其中每个 $h_i \\in \\mathbb{R}^3$。\n- 求和读出：$g_{\\mathrm{sum}} = \\sum_{i=1}^{|V|} h_i$。\n- 最大值读出：对于每个坐标 $k \\in \\{1,2,3\\}$，$g_{\\mathrm{max}}[k] = \\max_{1 \\le i \\le |V|} h_i[k]$。\n- 均值读出：$g_{\\mathrm{mean}} = \\frac{1}{|V|} \\sum_{i=1}^{|V|} h_i$。\n- 注意力读出：使用固定的注意力向量 $a \\in \\mathbb{R}^3$，定义注意力分数 $s_i = a^\\top h_i$，注意力权重 $\\alpha_i = \\frac{\\exp(s_i)}{\\sum_{j=1}^{|V|} \\exp(s_j)}$，以及 $g_{\\mathrm{attn}} = \\sum_{i=1}^{|V|} \\alpha_i h_i$。\n\n分类器：\n- 分类器参数固定为 $w = [0.5,\\,-1.0,\\,0.25]^\\top \\in \\mathbb{R}^3$ 和 $b = 0.1 \\in \\mathbb{R}$。\n- 对于任何读出 $g \\in \\mathbb{R}^3$，logit 为 $z = w^\\top g + b$。\n\n注意力参数：\n- 注意力向量固定为 $a = [0.3,\\,0.8,\\,-0.5]^\\top \\in \\mathbb{R}^3$。\n\n节点嵌入生成器：\n- 线性生成器 “lin”，参数为 $(a_0,b_0,c_0) \\in \\mathbb{R}^3$：对于 $i \\in \\{1,\\dots,n\\}$，\n  - $h_i[1] = a_0 + 0.1\\,i$，\n  - $h_i[2] = b_0 - 0.2\\,i$，\n  - $h_i[3] = (-1)^i\\,c_0 + 0.05\\,i$。\n- 中心化生成器 “centered”（无额外参数）：对于 $i \\in \\{1,\\dots,n\\}$，\n  - $h_i[1] = 0.2\\,(i - \\frac{n+1}{2})$，\n  - $h_i[2] = 0.3\\,(-1)^i$，\n  - $h_i[3] = 0.0$。\n- 常数生成器 “const”，参数为 $v \\in \\mathbb{R}^3$：对于 $i \\in \\{1,\\dots,n\\}$，$h_i = v$。\n\n对于每个测试对，您必须：\n- 使用指定的生成器和参数生成两个大小分别为 $n_1$ 和 $n_2$ 的图。\n- 对于四个读出函数 $r \\in \\{\\mathrm{sum},\\mathrm{max},\\mathrm{mean},\\mathrm{attn}\\}$ 中的每一个，计算相应的 logit $z_r(n_1)$ 和 $z_r(n_2)$。\n- 报告带符号的 logit 差值 $\\Delta_r = z_r(n_2) - z_r(n_1)$。\n\n测试套件：\n- 测试对 $\\mathbf{P1}$ (理想路径增长)：生成器 “lin”，参数 $(a_0,b_0,c_0) = (1.0,\\,0.5,\\,0.2)$，大小 $n_1 = 1, n_2 = 10$。\n- 测试对 $\\mathbf{P2}$ (中度增长)：生成器 “lin”，参数 $(a_0,b_0,c_0) = (1.0,\\,0.5,\\,0.2)$，大小 $n_1 = 3, n_2 = 6$。\n- 测试对 $\\mathbf{P3}$ (边缘情况，相同节点)：生成器 “const”，参数 $v = [0.4,\\,-0.1,\\,0.2]^\\top$，大小 $n_1 = 5, n_2 = 50$。\n- 测试对 $\\mathbf{P4}$ (因对称性而对大小近乎不敏感的均值)：生成器 “centered”，大小 $n_1 = 9, n_2 = 10$。\n\n对 $|V|$ 的敏感度反映在 $\\Delta_r$ 的量级上：\n- 大的 $|\\Delta_{\\mathrm{sum}}|$ 表示对 $|V|$ 的高敏感度。\n- 小的 $|\\Delta_{\\mathrm{mean}}|$ 表明当 $\\{h_i\\}$ 的经验分布稳定时，相对于 $|V|$ 进行了归一化。\n- 只有当新节点引入了新的坐标级极值时，$|\\Delta_{\\mathrm{max}}|$ 才会改变。\n- 根据 softmax 权重的构造，$|\\Delta_{\\mathrm{attn}}|$ 不会随 $|V|$ 缩放，但如果新节点改变了注意力分配，它可能会改变。\n\n您的任务：\n- 实现一个程序，为每个测试对计算四个读出函数的带符号差值列表，顺序严格按照 $[\\Delta_{\\mathrm{sum}},\\Delta_{\\mathrm{max}},\\Delta_{\\mathrm{mean}},\\Delta_{\\mathrm{attn}}]$。\n- 将测试对 $\\mathbf{P1}$、$\\mathbf{P2}$、$\\mathbf{P3}$、$\\mathbf{P4}$ 的结果汇总成一个长度为 16 的扁平列表，顺序为 $\\mathbf{P1}$、$\\mathbf{P2}$、$\\mathbf{P3}$、$\\mathbf{P4}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的结果严格遵循上述顺序。例如，其形式为 $[x_1,x_2,\\dots,x_{16}]$，其中每个 $x_j$ 是一个实数。",
            "solution": "该问题要求通过量化四种不同的图读出函数——求和、最大值、均值和基于注意力的池化——对图中节点数 $|V|$ 的敏感度，来对它们进行系统性评估。该敏感度通过当图的大小从 $n_1$ 变为 $n_2$ 时分类器输出 logit 的变化量 $\\Delta_z$ 来衡量。此分析在一系列测试对上进行，每个测试对都由特定的节点嵌入生成器和图的大小定义。\n\n问题的核心组成部分定义如下。所有向量均为列向量。\n\n**分类器与参数**\n分类器使用固定的线性变换和偏置，从图级嵌入 $g \\in \\mathbb{R}^3$ 计算出 logit $z \\in \\mathbb{R}$。\n- 权重向量：$w = [0.5, -1.0, 0.25]^\\top \\in \\mathbb{R}^3$\n- 偏置：$b = 0.1 \\in \\mathbb{R}$\n- Logit 公式：$z = w^\\top g + b$\n\n**注意力机制**\n基于注意力的读出使用固定的注意力向量为节点打分。\n- 注意力向量：$a = [0.3, 0.8, -0.5]^\\top \\in \\mathbb{R}^3$\n\n**节点嵌入生成器**\n对于大小为 $n$ 的图，其节点嵌入 $h_i \\in \\mathbb{R}^3$ 是根据三个指定函数之一生成的，其中 $i \\in \\{1, \\dots, n\\}$。\n1.  **线性生成器 (“lin”)**：给定参数 $(a_0, b_0, c_0)$，第 $i$ 个节点嵌入 $h_i$ 由以下公式给出：\n    $$\n    h_i = \\begin{bmatrix} a_0 + 0.1\\,i \\\\ b_0 - 0.2\\,i \\\\ (-1)^i\\,c_0 + 0.05\\,i \\end{bmatrix}\n    $$\n2.  **中心化生成器 (“centered”)**：对于大小为 $n$ 的图，第 $i$ 个节点嵌入 $h_i$ 为：\n    $$\n    h_i = \\begin{bmatrix} 0.2\\,(i - \\frac{n+1}{2}) \\\\ 0.3\\,(-1)^i \\\\ 0.0 \\end{bmatrix}\n    $$\n3.  **常数生成器 (“const”)**：给定参数 $v \\in \\mathbb{R}^3$，所有节点嵌入都相同：$h_i = v$。\n\n**读出函数**\n设 $\\{h_i\\}_{i=1}^{n}$ 为节点嵌入的多重集。图嵌入 $g$ 的计算方式如下：\n1.  **求和读出**：$g_{\\mathrm{sum}} = \\sum_{i=1}^{n} h_i$。如果嵌入的均值非零，此函数的输出会随 $n$ 线性缩放。\n2.  **最大值读出**：$g_{\\mathrm{max}}$ 是所有节点嵌入的按元素最大值。对于每个坐标 $k \\in \\{1,2,3\\}$：\n    $$\n    g_{\\mathrm{max}}[k] = \\max_{1 \\le i \\le n} h_i[k]\n    $$\n3.  **均值读出**：$g_{\\mathrm{mean}} = \\frac{1}{n} \\sum_{i=1}^{n} h_i$。这将求和读出的结果按节点数进行了归一化。\n4.  **注意力读出**：此函数计算节点嵌入的加权和，其中权重由注意力机制确定。\n    - 注意力分数：$s_i = a^\\top h_i$\n    - 注意力权重 (softmax)：$\\alpha_i = \\frac{\\exp(s_i)}{\\sum_{j=1}^{n} \\exp(s_j)}$\n    - 图嵌入：$g_{\\mathrm{attn}} = \\sum_{i=1}^{n} \\alpha_i h_i$。根据构造，$\\sum_i \\alpha_i = 1$，因此这是一个凸组合。\n\n**目标计算**\n对于每个读出函数 $r \\in \\{\\mathrm{sum}, \\mathrm{max}, \\mathrm{mean}, \\mathrm{attn}\\}$ 以及每个图大小为 $n_1$ 和 $n_2$ 的测试对，我们计算带符号的 logit 差值：\n$$\n\\Delta_r = z_r(n_2) - z_r(n_1)\n$$\n代入 logit 的定义 $z_r(n) = w^\\top g_r(n) + b$，我们得到：\n$$\n\\Delta_r = (w^\\top g_r(n_2) + b) - (w^\\top g_r(n_1) + b) = w^\\top (g_r(n_2) - g_r(n_1))\n$$\n该量度衡量了在给定读出机制下，由于图大小从 $n_1$ 变为 $n_2$ 而引起的分类器输出的变化。\n\n我们现在开始为每个测试对计算这些差值。\n\n**测试对 P1：生成器 \"lin\"，$(a_0,b_0,c_0) = (1.0, 0.5, 0.2)$，$n_1 = 1, n_2 = 10$。**\n- 为 $n_1=1$ 和 $n_2=10$ 生成节点嵌入。\n- 对每个读出函数，计算 $g(n_1)$、$g(n_2)$ 和 $\\Delta_r$。\n1.  $\\Delta_{\\mathrm{sum}}$：$g_{\\mathrm{sum}}(1) = [1.1, 0.3, -0.15]^\\top$。$g_{\\mathrm{sum}}(10) = \\sum_{i=1}^{10} h_i = [15.5, -6.0, 2.75]^\\top$。\n    $\\Delta_{\\mathrm{sum}} = w^\\top (g_{\\mathrm{sum}}(10) - g_{\\mathrm{sum}}(1)) = w^\\top [14.4, -6.3, 2.9]^\\top = 14.225$。\n2.  $\\Delta_{\\mathrm{max}}$：$g_{\\mathrm{max}}(1) = [1.1, 0.3, -0.15]^\\top$。对于 $n_2=10$，$h_i[1]$ 在 $i=10$ 时最大，$h_i[2]$ 在 $i=1$ 时最大，$h_i[3]$ 在最大的偶数 $i$（即 $i=10$）时最大。这得到 $g_{\\mathrm{max}}(10) = [2.0, 0.3, 0.7]^\\top$。\n    $\\Delta_{\\mathrm{max}} = w^\\top (g_{\\mathrm{max}}(10) - g_{\\mathrm{max}}(1)) = w^\\top [0.9, 0.0, 0.85]^\\top = 0.6625$。\n3.  $\\Delta_{\\mathrm{mean}}$：$g_{\\mathrm{mean}}(1) = g_{\\mathrm{sum}}(1)$。$g_{\\mathrm{mean}}(10) = g_{\\mathrm{sum}}(10)/10 = [1.55, -0.6, 0.275]^\\top$。\n    $\\Delta_{\\mathrm{mean}} = w^\\top (g_{\\mathrm{mean}}(10) - g_{\\mathrm{mean}}(1)) = w^\\top [0.45, -0.9, 0.425]^\\top = 1.23125$。\n4.  $\\Delta_{\\mathrm{attn}}$：对于 $n_1=1$，$g_{\\mathrm{attn}}(1)=h_1$。对于 $n_2=10$，我们计算分数 $s_i$、权重 $\\alpha_i$，得到 $g_{\\mathrm{attn}}(10) = \\sum \\alpha_i h_i \\approx [1.229, 0.114, -0.063]^\\top$。\n    $\\Delta_{\\mathrm{attn}} = w^\\top (g_{\\mathrm{attn}}(10) - g_{\\mathrm{attn}}(1)) \\approx w^\\top [0.129, -0.186, 0.087]^\\top \\approx 0.272$。\n\n**测试对 P2：生成器 \"lin\"，$(a_0,b_0,c_0) = (1.0, 0.5, 0.2)$，$n_1 = 3, n_2 = 6$。**\n- 一个增长较小的类似计算。\n1.  $\\Delta_{\\mathrm{sum}}$：$g_{\\mathrm{sum}}(3) = [3.6, -0.3, -0.25]^\\top$。$g_{\\mathrm{sum}}(6) = [7.5, -2.4, 1.2]^\\top$。\n    $\\Delta_{\\mathrm{sum}} = w^\\top (g_{\\mathrm{sum}}(6) - g_{\\mathrm{sum}}(3)) = w^\\top [3.9, -2.1, 1.45]^\\top = 4.4125$。\n2.  $\\Delta_{\\mathrm{max}}$：$g_{\\mathrm{max}}(3) = [1.3, 0.3, 0.3]^\\top$。$g_{\\mathrm{max}}(6) = [1.6, 0.3, 0.5]^\\top$。\n    $\\Delta_{\\mathrm{max}} = w^\\top (g_{\\mathrm{max}}(6) - g_{\\mathrm{max}}(3)) = w^\\top [0.3, 0.0, 0.2]^\\top = 0.2$。\n3.  $\\Delta_{\\mathrm{mean}}$：$g_{\\mathrm{mean}}(3) = [1.2, -0.1, -0.0833]^\\top$。$g_{\\mathrm{mean}}(6) = [1.25, -0.4, 0.2]^\\top$。\n    $\\Delta_{\\mathrm{mean}} = w^\\top (g_{\\mathrm{mean}}(6) - g_{\\mathrm{mean}}(3)) = w^\\top [0.05, -0.3, 0.2833]^\\top = 0.39583...$。\n4.  $\\Delta_{\\mathrm{attn}}$：$g_{\\mathrm{attn}}(3) \\approx [1.139, 0.222, -0.115]^\\top$。$g_{\\mathrm{attn}}(6) \\approx [1.162, 0.186, -0.091]^\\top$。\n    $\\Delta_{\\mathrm{attn}} = w^\\top (g_{\\mathrm{attn}}(6) - g_{\\mathrm{attn}}(3)) \\approx w^\\top [0.023, -0.036, 0.024]^\\top \\approx 0.0535$。\n\n**测试对 P3：生成器 \"const\"，$v = [0.4, -0.1, 0.2]^\\top$，$n_1 = 5, n_2 = 50$。**\n- 所有 $h_i$ 都与 $v$ 相同。\n1.  $\\Delta_{\\mathrm{sum}}$：$g_{\\mathrm{sum}}(n) = n \\cdot v$。$\\Delta_{\\mathrm{sum}} = w^\\top(n_2 v - n_1 v) = (n_2-n_1) w^\\top v$。\n    $w^\\top v = 0.5(0.4) - 1.0(-0.1) + 0.25(0.2) = 0.35$。\n    $\\Delta_{\\mathrm{sum}} = (50-5) \\times 0.35 = 45 \\times 0.35 = 15.75$。\n2.  $\\Delta_{\\mathrm{max}}$：$g_{\\mathrm{max}}(n) = v$。$\\Delta_{\\mathrm{max}} = w^\\top(v-v) = 0$。\n3.  $\\Delta_{\\mathrm{mean}}$：$g_{\\mathrm{mean}}(n) = \\frac{1}{n}\\sum v = v$。$\\Delta_{\\mathrm{mean}} = w^\\top(v-v) = 0$。\n4.  $\\Delta_{\\mathrm{attn}}$：分数 $s_i = a^\\top v$ 是常数，所以权重 $\\alpha_i = 1/n$。$g_{\\mathrm{attn}}(n) = \\sum \\frac{1}{n} v = v$。\n    $\\Delta_{\\mathrm{attn}} = w^\\top(v-v) = 0$。\n\n**测试对 P4：生成器 \"centered\"，$n_1 = 9, n_2 = 10$。**\n- 该生成器产生的嵌入总和接近于零。\n1.  $\\Delta_{\\mathrm{sum}}$：对于 $n_1=9$ (奇数)，$\\sum h_i[1]=0, \\sum h_i[2]=-0.3, \\sum h_i[3]=0$。因此 $g_{\\mathrm{sum}}(9) = [0, -0.3, 0]^\\top$。\n    对于 $n_2=10$ (偶数)，$\\sum h_i[1]=0, \\sum h_i[2]=0, \\sum h_i[3]=0$。因此 $g_{\\mathrm{sum}}(10) = [0, 0, 0]^\\top$。\n    $\\Delta_{\\mathrm{sum}} = w^\\top(g_{\\mathrm{sum}}(10) - g_{\\mathrm{sum}}(9)) = w^\\top [0, 0.3, 0]^\\top = -0.3$。\n2.  $\\Delta_{\\mathrm{max}}$：对于 $n=9$ 和 $n=10$，$h_i[1]$ 在 $i=1$ 时最大，$h_i[2]$ 在任何偶数 $i$ 时最大。因此 $g_{\\mathrm{max}}$ 不变。\n    $g_{\\mathrm{max}}(9) = [0.2(9-5), 0.3, 0]^\\top = [0.8, 0.3, 0]^\\top$。\n    $g_{\\mathrm{max}}(10) = [0.2(10-5.5), 0.3, 0]^\\top = [0.9, 0.3, 0]^\\top$。\n    $\\Delta_{\\mathrm{max}} = w^\\top(g_{\\mathrm{max}}(10) - g_{\\mathrm{max}}(9)) = w^\\top [0.1, 0, 0]^\\top = 0.05$。\n3.  $\\Delta_{\\mathrm{mean}}$：$g_{\\mathrm{mean}}(9) = g_{\\mathrm{sum}}(9)/9 = [0, -0.3/9, 0]^\\top$。$g_{\\mathrm{mean}}(10) = [0,0,0]^\\top$。\n    $\\Delta_{\\mathrm{mean}} = w^\\top(g_{\\mathrm{mean}}(10) - g_{\\mathrm{mean}}(9)) = w^\\top [0, 0.3/9, 0]^\\top = -0.0333...$。\n4.  $\\Delta_{\\mathrm{attn}}$：$g_{\\mathrm{attn}}(9) \\approx [-0.499, -0.3, 0]^\\top$。$g_{\\mathrm{attn}}(10) \\approx [-0.599, 0.3, 0]^\\top$。\n    $\\Delta_{\\mathrm{attn}} = w^\\top(g_{\\mathrm{attn}}(10) - g_{\\mathrm{attn}}(9)) \\approx w^\\top [-0.1, 0.6, 0]^\\top \\approx -0.65$。\n\n这些计算在数值上实现后，将得出最终的结果列表。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes logit differences for four graph readout functions across four test pairs.\n    \"\"\"\n\n    # --- Fixed Parameters ---\n    w = np.array([0.5, -1.0, 0.25])\n    b = 0.1\n    a = np.array([0.3, 0.8, -0.5])\n\n    # --- Node Embedding Generators ---\n    def generate_embeddings(gen_type, n, params):\n        \"\"\"Generates a matrix of node embeddings H of shape (n, 3).\"\"\"\n        if n == 0:\n            return np.empty((0, 3))\n        H = np.zeros((n, 3))\n        indices = np.arange(1, n + 1)\n        if gen_type == \"lin\":\n            a0, b0, c0 = params\n            H[:, 0] = a0 + 0.1 * indices\n            H[:, 1] = b0 - 0.2 * indices\n            H[:, 2] = ((-1)**indices) * c0 + 0.05 * indices\n        elif gen_type == \"centered\":\n            H[:, 0] = 0.2 * (indices - (n + 1) / 2)\n            H[:, 1] = 0.3 * ((-1)**indices)\n            H[:, 2] = 0.0\n        elif gen_type == \"const\":\n            v = params\n            H[:, :] = v\n        return H\n\n    # --- Readout Functions ---\n    def sum_readout(H):\n        if H.shape[0] == 0:\n            return np.zeros(3)\n        return np.sum(H, axis=0)\n\n    def max_readout(H):\n        if H.shape[0] == 0:\n            # Return a vector of -inf so that it doesn't affect the max with non-empty graphs\n            return np.full(3, -np.inf)\n        return np.max(H, axis=0)\n\n    def mean_readout(H):\n        if H.shape[0] == 0:\n            return np.zeros(3)\n        return np.mean(H, axis=0)\n\n    def attn_readout(H, attn_vec):\n        if H.shape[0] == 0:\n            return np.zeros(3)\n        scores = H @ attn_vec\n        # Numerically stable softmax\n        scores -= np.max(scores)\n        exp_scores = np.exp(scores)\n        weights = exp_scores / np.sum(exp_scores)\n        return weights @ H\n\n    # --- Main Calculation Logic ---\n    def calculate_deltas(case):\n        gen_type, params, n1, n2 = case\n        \n        H1 = generate_embeddings(gen_type, n1, params)\n        H2 = generate_embeddings(gen_type, n2, params)\n\n        readouts = {\n            \"sum\": sum_readout,\n            \"max\": max_readout,\n            \"mean\": mean_readout,\n            \"attn\": attn_readout\n        }\n\n        deltas = []\n        for r_name, r_func in readouts.items():\n            if r_name == \"attn\":\n                g1 = r_func(H1, a)\n                g2 = r_func(H2, a)\n            else:\n                g1 = r_func(H1)\n                g2 = r_func(H2)\n            \n            # Since z = w^T g + b, delta_z = (w^T g2 + b) - (w^T g1 + b) = w^T (g2 - g1)\n            delta_g = g2 - g1\n            delta_z = delta_g @ w\n            deltas.append(delta_z)\n            \n        return deltas\n\n    # --- Test Suite ---\n    test_cases = [\n        # Pair P1\n        (\"lin\", (1.0, 0.5, 0.2), 1, 10),\n        # Pair P2\n        (\"lin\", (1.0, 0.5, 0.2), 3, 6),\n        # Pair P3\n        (\"const\", np.array([0.4, -0.1, 0.2]), 5, 50),\n        # Pair P4\n        (\"centered\", None, 9, 10),\n    ]\n\n    # --- Execute and Format Output ---\n    all_results = []\n    for case in test_cases:\n        results_for_case = calculate_deltas(case)\n        all_results.extend(results_for_case)\n\n    # Format into a single comma-separated string in brackets\n    print(f\"[{','.join(f'{x:.10f}' for x in all_results)}]\")\n\nsolve()\n```"
        }
    ]
}