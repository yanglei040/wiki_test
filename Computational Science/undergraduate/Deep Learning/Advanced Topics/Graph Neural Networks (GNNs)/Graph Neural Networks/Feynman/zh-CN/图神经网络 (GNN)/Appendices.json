{
    "hands_on_practices": [
        {
            "introduction": "这项实践将指导您实现几种基础的图传播算法。通过在一个半监督节点分类任务上比较它们的性能，您将直接洞察消息传递的核心机制，以及一个被称为“过度平滑”的关键挑战。这个练习对于理解为何需要发展更复杂的图神经网络（GNN）架构至关重要。",
            "id": "3106157",
            "problem": "给定一个具有弱节点特征和稀疏监督的无向简单图。您的目标是实现三种基于传播的方法，在一组受控参数下比较它们的测试准确率，并研究当传播步数增加时的边界行为。这些方法是：使用归一化邻接矩阵幂的标签传播（label propagation）、简化图卷积（Simplified Graph Convolution, SGC）以及近似个性化神经预测传播（Approximate Personalized Propagation of Neural Predictions, APPNP）。您必须从核心定义推导出每种方法，并将其实现为一个无需用户输入即可生成所要求输出格式的完整程序。\n\n从以下基本概念开始：\n- 图由邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 表示，其中如果节点 $i$ 和节点 $j$ 之间存在边，则 $A_{ij} = 1$，否则 $A_{ij} = 0$。令 $I$ 表示单位矩阵。定义 $\\tilde{A} = A + I$ 以包含自环。通过 $\\tilde{D}_{ii} = \\sum_{j} \\tilde{A}_{ij}$ 定义度矩阵 $\\tilde{D}$。对称归一化邻接矩阵为 $\\hat{A} = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$。\n- 对于任何非负整数 $K$，通过 $\\hat{A}$ 进行的$K$步传播为 $\\hat{A}^K$。对于一个连通图，若其 $\\hat{A}$ 是对称的，并且由带有自环的 $\\tilde{A}$ 暗示了非周期性，则其谱半径满足 $\\rho(\\hat{A}) = 1$。当 $K \\to \\infty$ 时，在标准条件下（例如，由自环确保的非周期性），$\\hat{A}^K$ 收敛到一个秩为1的投影算子，该算子投影到主特征向量方向上。\n\n图、标签、特征和划分：\n- 节点数 $n = 8$，节点索引为 $\\{0,1,2,3,4,5,6,7\\}$。\n- 边：有两个大小为4的密集簇，由一条桥边连接。具体来说：\n  - 对于所有不同的 $i,j \\in \\{0,1,2,3\\}$，包含边 $\\{i,j\\}$。\n  - 对于所有不同的 $i,j \\in \\{4,5,6,7\\}$，包含边 $\\{i,j\\}$。\n  - 包含桥边 $\\{3,4\\}$。\n- 真实类别：节点 $\\{0,1,2,3\\}$ 属于类别0，节点 $\\{4,5,6,7\\}$ 属于类别1。令 $C=2$ 为类别数。令 $Y \\in \\mathbb{R}^{n \\times C}$ 为真实标签的独热编码，即如果节点 $i$ 的类别为 $c$，则 $Y_{ic} = 1$，否则 $Y_{ic} = 0$。\n- 有标签的训练集索引为 $L = \\{0,7\\}$，测试集索引为 $T = \\{1,2,3,4,5,6\\}$。定义半监督标签种子矩阵 $Y_{0} \\in \\mathbb{R}^{n \\times C}$，其中如果 $i \\in L$，则 $Y_{0i:} = Y_{i:}$；如果 $i \\notin L$，则 $Y_{0i:} = \\mathbf{0}$。\n- 弱节点特征：特征维度 $d = 3$。令 $X \\in \\mathbb{R}^{n \\times d}$，其条目为 $X_{ij} = 1 + \\epsilon_{ij} + s_{i} \\cdot \\delta \\cdot \\mathbb{1}\\{j=0\\}$，其中 $\\epsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^{2})$ 是独立噪声项，$\\sigma = 0.02$；如果节点 $i$ 属于类别0，则 $s_{i} = +1$，如果节点 $i$ 属于类别1，则 $s_{i} = -1$；$\\delta = 0.05$。这将产生一个弱信息性的第一特征以及其他近乎恒定的特征。\n\n需要实现的方法：\n- 标签传播基线：对于给定的整数 $K \\ge 0$，计算 $Y_{\\text{LP}} = \\hat{A}^{K} Y_{0}$，并通过 $\\arg\\max_{c} (Y_{\\text{LP}})_{ic}$ 对每个节点 $i$ 进行分类。仅通过与真实标签 $Y$ 比较，在测试集索引 $T$ 上评估准确率。\n- 简化图卷积（SGC）：对于给定的整数 $K \\ge 0$，计算 $\\tilde{X} = \\hat{A}^{K} X$，在有标签的节点 $L$ 上通过多输出岭回归拟合一个线性分类器，以获得权重 $W \\in \\mathbb{R}^{d \\times C}$，该权重最小化 $\\|\\tilde{X}_{L} W - Y_{L}\\|_{F}^{2} + \\lambda \\|W\\|_{F}^{2}$（其中 $\\lambda = 10^{-2}$）。然后预测 $Z = \\tilde{X} W$ 并通过 $\\arg\\max_{c} Z_{ic}$ 进行分类。在 $T$ 上评估准确率。\n- 近似个性化神经预测传播（APPNP）：初始化 $H^{(0)} = Y_{0}$，并对 $t = 0,1,\\dots,T_{\\max}-1$ 进行迭代更新 $H^{(t+1)} = (1-\\alpha) \\hat{A} H^{(t)} + \\alpha H^{(0)}$，其中瞬移参数 $\\alpha = 0.15$，$T_{\\max} = 100$。然后通过 $\\arg\\max_{c} H^{(T_{\\max})}_{ic}$ 进行分类，并在 $T$ 上评估准确率。\n\n需要研究的边界分析：\n- 对于标签传播和SGC平滑操作 $\\hat{A}^{K}$ 中的 $K \\to \\infty$ 情况，请论证类别得分会发生什么变化（根据 $\\hat{A}$ 的谱），以及在标签稀少和特征较弱的情况下，这将如何影响准确率。\n\n测试套件参数：\n- 使用上述指定的共享图、标签、特征和数据划分。\n- 对标签传播和SGC方法，在传播指数 $K \\in \\{0,1,5,200\\}$ 上评估这三种方法，而APPNP在所有情况下均使用固定的 $\\alpha = 0.15$ 和 $T_{\\max} = 100$。\n\n要求的输出：\n- 对于测试套件中的每个 $K$，计算测试准确率元组 $(a_{\\text{LP}}, a_{\\text{SGC}}, a_{\\text{APPNP}})$，其中每个 $a$ 是测试集中正确分类节点的比例，表示为四舍五入到三位小数的小数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素对应一个测试用例（按 $K \\in \\{0,1,5,200\\}$ 的顺序），并且本身是一个包含三个浮点数 $[a_{\\text{LP}},a_{\\text{SGC}},a_{\\text{APPNP}}]$（四舍五入到三位小数）的列表。例如，格式必须类似于 $[[x_{1},y_{1},z_{1}],[x_{2},y_{2},z_{2}],\\dots]$。\n\n所有随机量必须使用固定的种子生成，以确保结果的确定性。本问题不涉及物理单位，也不使用角度。最终输出必须严格匹配指定的单行格式。",
            "solution": "用户提供了一个有效的问题陈述。\n\n该问题要求在图上实现并比较三种半监督学习方法：标签传播（Label Propagation, LP）、简化图卷积（Simplified Graph Convolution, SGC）和近似个性化神经预测传播（Approximate Personalized Propagation of Neural Predictions, APPNP）。我们将首先建立通用的数学框架，然后推导每种方法，分析它们在无限传播步数极限下的行为，最后实现它们以获得指定的测试准确率。\n\n### 1. 图与数据表示\n\n该图有 $n=8$ 个节点，由两个4节点集团 $\\{0,1,2,3\\}$ 和 $\\{4,5,6,7\\}$ 组成，通过一条边 $\\{3,4\\}$ 连接。邻接矩阵 $A \\in \\{0,1\\}^{n \\times n}$ 表示此结构。为了引入节点自连接（这是图神经网络（GNN）中的标准做法，用以确保非周期性并将节点自身的特征包含在其更新中），我们定义 $\\tilde{A} = A + I$，其中 $I$ 是 $n \\times n$ 的单位矩阵。\n\n传播动力学由对称归一化的邻接矩阵决定。我们将 $\\tilde{A}$ 的度矩阵 $\\tilde{D}$ 定义为对角矩阵，其条目为 $\\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}$。归一化邻接矩阵则为：\n$$ \\hat{A} = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} $$\n该矩阵是对称的，其特征值 $\\lambda$ 有界，即 $|\\lambda| \\le 1$。操作 $H' = \\hat{A} H$ 可被解释为从邻居节点聚合信息，其中 $H$ 可以是节点特征或标签的矩阵。\n\n数据包括：\n- **节点特征** $X \\in \\mathbb{R}^{n \\times d}$：一个矩阵，其中第 $i$ 行是节点 $i$ 的特征向量。这些特征被设计为弱信息性的。\n- **真实标签** $Y \\in \\{0,1\\}^{n \\times C}$：真实类别分配的独热编码矩阵，共有 $C=2$ 个类别。\n- **种子标签** $Y_0 \\in \\mathbb{R}^{n \\times C}$：一个稀疏矩阵，包含训练集 $L=\\{0,7\\}$ 中节点的独热标签，而所有其他节点均为零。该矩阵为传播过程提供种子。\n\n### 2. 基于传播的方法\n\n**方法1：标签传播（LP）**\nLP 是一种简单的半监督算法，它将标签信息从已标记的节点传播到整个图中。其核心思想是相连的节点很可能共享相同的标签。$K$ 步传播是通过将种子标签矩阵 $Y_0$ 乘以归一化邻接矩阵 $\\hat{A}$ 的 $K$ 次幂来实现的。\n\n经过 $K$ 步后，预测的标签分数为：\n$$ Y_{\\text{LP}} = \\hat{A}^K Y_0 $$\n节点 $i$ 的最终类别预测是使其得分最大化的类别：$\\arg\\max_c (Y_{\\text{LP}})_{ic}$。当 $K=0$ 时，$\\hat{A}^0 = I$，因此只有初始标记的节点具有非零分数，所有未标记的节点都被预测为类别0（由于 `argmax` 的平局打破规则）。\n\n**方法2：简化图卷积（SGC）**\nSGC 通过将特征传播与非线性变换解耦来简化 GNN。它包括两个阶段：\n1.  **特征传播**：通过应用传播矩阵 $\\hat{A}^K$，节点特征 $X$ 在图上被平滑：\n    $$ \\tilde{X} = \\hat{A}^K X $$\n    这一步通过聚合来自其 $K$ 跳邻域的特征信息，为每个节点有效地创建了新特征。\n2.  **线性分类**：在已标记节点的传播特征上训练一个线性分类器。我们寻求一个权重矩阵 $W \\in \\mathbb{R}^{d \\times C}$，以最小化岭回归目标：\n    $$ \\underset{W}{\\text{minimize}} \\quad \\|\\tilde{X}_L W - Y_L\\|_F^2 + \\lambda \\|W\\|_F^2 $$\n    其中 $\\tilde{X}_L$ 和 $Y_L$ 分别是 $\\tilde{X}$ 和 $Y$ 中对应于标记集 $L$ 的行，$\\| \\cdot \\|_F$ 是 Frobenius 范数。正则化参数给定为 $\\lambda=10^{-2}$。这个优化问题有一个闭式解：\n    $$ W = (\\tilde{X}_L^T \\tilde{X}_L + \\lambda I_d)^{-1} \\tilde{X}_L^T Y_L $$\n    其中 $I_d$ 是 $d \\times d$ 的单位矩阵。然后计算所有节点的预测 $Z = \\tilde{X} W$，节点 $i$ 的类别为 $\\arg\\max_c Z_{ic}$。\n\n**方法3：近似个性化神经预测传播（APPNP）**\nAPPNP 的灵感来自于个性化 PageRank，旨在解决深度 GNN 中固有的过平滑问题。它在传播邻域信息和保留初始“根”信息之间保持平衡。虽然原始的 APPNP 使用神经网络的输出作为根信息，但在这里我们使用种子标签 $Y_0$。\n\n从 $H^{(0)} = Y_0$ 开始，APPNP 对 $t=0, 1, \\dots, T_{\\max}-1$ 执行以下迭代更新：\n$$ H^{(t+1)} = (1-\\alpha) \\hat{A} H^{(t)} + \\alpha H^{(0)} $$\n此处，$\\alpha \\in [0,1]$ 是瞬移概率，给定为 $\\alpha=0.15$。在每一步中，该过程都涉及一步邻域聚合（$\\hat{A}H^{(t)}$ 项）和一次“瞬移”回到初始预测（$H^{(0)}$ 项）。这确保了来自种子节点的影响永远不会完全消失。最终预测取自收敛或接近收敛的状态，这里是 $H^{(T_{\\max})}$，其中 $T_{\\max}=100$。通过 $\\arg\\max_c (H^{(T_{\\max})})_{ic}$ 进行分类。\n\n### 3. $K \\to \\infty$ 的边界分析\n\nLP 和 SGC 在大量传播步骤 $K$ 下的行为由 $\\hat{A}$ 的谱特性决定。由于图是连通且非周期的（因为有自环），根据 Perron-Frobenius 定理，$\\hat{A}$ 的最大特征值为 $\\lambda_1 = 1$，其重数为1。所有其他特征值 $|\\lambda_i|  1$。当 $K \\to \\infty$ 时，$\\hat{A}$ 的幂收敛到一个秩为1的投影算子，该算子投影到主特征向量 $u_1$ 上：\n$$ \\lim_{K \\to \\infty} \\hat{A}^K = u_1 u_1^T $$\n其中 $u_1$ 是对应于 $\\lambda_1=1$ 的归一化特征向量。对于对称归一化，该特征向量由 $(u_1)_i \\propto \\sqrt{\\tilde{D}_{ii}}$ 给出。\n\n**LP 过平滑**：\n对于大的 $K$，LP 的预测变为：\n$$ Y_{\\text{LP}} \\approx (u_1 u_1^T) Y_0 = u_1 (u_1^T Y_0) $$\n项 $v^T = u_1^T Y_0$ 是一个 $1 \\times C$ 的常数行向量。分数矩阵 $Y_{\\text{LP}}$ 变为 $u_1 v^T$，意味着节点 $i$ 的分数向量是 $(u_1)_i v^T$。由于 $(u_1)_i > 0$，分类决策 $\\arg\\max_c ((u_1)_i v_c)$ 简化为 $\\arg\\max_c v_c$，这与节点 $i$ 无关。因此，所有节点都会得到相同的预测。这种现象被称为“过平滑”。测试准确率将骤降至属于这个单一预测类别的测试节点的比例。在这个问题中，节点0和节点7的度是相等的，所以它们对最终聚合分数的贡献权重相等，导致平局，而 `argmax` 会倾向于类别0来打破平局。这产生的准确率为 $3/6 = 0.5$。\n\n**SGC 过平滑**：\nSGC 也遭遇了类似的命运。对于大的 $K$，平滑后的特征变为：\n$$ \\tilde{X} \\approx (u_1 u_1^T) X = u_1 (u_1^T X) $$\n项 $f^T = u_1^T X$ 是一个 $1 \\times d$ 的行向量。任何节点 $i$ 的特征向量变为 $(\\tilde{X})_{i:} = (u_1)_i f^T$。所有节点的特征都变得共线，指向 $f^T$ 的方向。在这种特征上训练的线性分类器将对每个节点做出相同的预测，因为分离超平面将划分特征空间，而所有特征向量都位于同一条线上。这同样导致了 $0.5$ 的准确率。\n\n**APPNP 对过平滑的抵抗**：\nAPPNP 由于其瞬移机制而避免了这个问题。APPNP 的收敛解是 $H^{(\\infty)} = \\alpha(I - (1-\\alpha)\\hat{A})^{-1} H^{(0)}$。单位矩阵 $I$ 和瞬移概率 $\\alpha$ 的存在，防止了传播过程完全冲刷掉 $H^{(0)}$ 中的初始信息。它确保最终的预测保持对初始种子节点的“个性化”，从而即使在易于过平滑的结构上也能保持较高的准确率。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates Label Propagation, SGC, and APPNP on a\n    specified graph and dataset.\n    \"\"\"\n\n    def build_graph():\n        \"\"\"Constructs the adjacency matrix for the specified graph.\"\"\"\n        n = 8\n        A = np.zeros((n, n), dtype=np.float64)\n        # Clique on {0, 1, 2, 3}\n        for i in range(4):\n            for j in range(i + 1, 4):\n                A[i, j] = A[j, i] = 1.0\n        # Clique on {4, 5, 6, 7}\n        for i in range(4, 8):\n            for j in range(i + 1, 8):\n                A[i, j] = A[j, i] = 1.0\n        # Bridge {3, 4}\n        A[3, 4] = A[4, 3] = 1.0\n        return A\n\n    def normalize_adjacency(A):\n        \"\"\"Computes the symmetrically normalized adjacency matrix.\"\"\"\n        A_tilde = A + np.eye(A.shape[0])\n        D_tilde_diag = A_tilde.sum(axis=1)\n        # Replace zeros in D_tilde_diag to avoid division by zero, though not expected here.\n        D_tilde_diag[D_tilde_diag == 0] = 1e-12\n        D_tilde_inv_sqrt = np.diag(np.power(D_tilde_diag, -0.5))\n        A_hat = D_tilde_inv_sqrt @ A_tilde @ D_tilde_inv_sqrt\n        return A_hat\n\n    def generate_data(n, seed):\n        \"\"\"Generates labels, features, and data splits.\"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # Ground truth labels\n        y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n        C = 2\n        Y = np.eye(C, dtype=np.float64)[y_true]\n        \n        # Labeled and test sets\n        L = [0, 7]\n        T = [1, 2, 3, 4, 5, 6]\n        \n        # Seed labels (for LP and APPNP)\n        Y_0 = np.zeros_like(Y)\n        Y_0[L] = Y[L]\n        \n        # Node features\n        d = 3\n        sigma = 0.02\n        delta = 0.05\n        s = np.array([1, 1, 1, 1, -1, -1, -1, -1])\n        \n        noise = rng.normal(0, sigma, size=(n, d))\n        signal = np.zeros((n, d))\n        signal[:, 0] = s * delta\n        \n        X = 1.0 + noise + signal\n        \n        return X, Y, y_true, Y_0, L, T\n\n    def solve_lp(A_hat, Y_0, K, T, y_true):\n        \"\"\"Calculates test accuracy for Label Propagation.\"\"\"\n        if K == 0:\n            A_hat_K = np.eye(A_hat.shape[0])\n        else:\n            A_hat_K = np.linalg.matrix_power(A_hat, K)\n        \n        Y_lp = A_hat_K @ Y_0\n        preds = np.argmax(Y_lp, axis=1)\n        \n        accuracy = np.mean(preds[T] == y_true[T])\n        return accuracy\n\n    def solve_sgc(A_hat, X, Y, K, T, y_true, L, reg_lambda):\n        \"\"\"Calculates test accuracy for Simplified Graph Convolution.\"\"\"\n        if K == 0:\n            A_hat_K = np.eye(A_hat.shape[0])\n        else:\n            A_hat_K = np.linalg.matrix_power(A_hat, K)\n            \n        X_tilde = A_hat_K @ X\n        \n        # Ridge regression\n        X_L = X_tilde[L]\n        Y_L = Y[L]\n        \n        d = X.shape[1]\n        term1 = X_L.T @ X_L + reg_lambda * np.eye(d)\n        term2 = X_L.T @ Y_L\n        W = np.linalg.solve(term1, term2)\n        \n        Z = X_tilde @ W\n        preds = np.argmax(Z, axis=1)\n        \n        accuracy = np.mean(preds[T] == y_true[T])\n        return accuracy\n\n    def solve_appnp(A_hat, Y_0, T, y_true, alpha, T_max):\n        \"\"\"Calculates test accuracy for APPNP.\"\"\"\n        H = Y_0.copy()\n        for _ in range(T_max):\n            H = (1 - alpha) * (A_hat @ H) + alpha * Y_0\n        \n        preds = np.argmax(H, axis=1)\n        \n        accuracy = np.mean(preds[T] == y_true[T])\n        return accuracy\n\n    # Main execution logic\n    n_nodes = 8\n    random_seed = 42 # For reproducibility\n    \n    A = build_graph()\n    A_hat = normalize_adjacency(A)\n    X, Y, y_true, Y_0, L, T = generate_data(n=n_nodes, seed=random_seed)\n    \n    test_suite_K = [0, 1, 5, 200]\n    sgc_lambda = 1e-2\n    appnp_alpha = 0.15\n    appnp_T_max = 100\n    \n    all_results = []\n    \n    # APPNP result is constant for all K\n    acc_appnp = solve_appnp(A_hat, Y_0, T, y_true, appnp_alpha, appnp_T_max)\n    \n    for K in test_suite_K:\n        acc_lp = solve_lp(A_hat, Y_0, K, T, y_true)\n        acc_sgc = solve_sgc(A_hat, X, Y, K, T, y_true, L, sgc_lambda)\n        \n        all_results.append([acc_lp, acc_sgc, acc_appnp])\n\n    # Format output string without spaces\n    result_strings = []\n    for r in all_results:\n        inner_str = '[' + ','.join([f'{val:.3f}' for val in r]) + ']'\n        result_strings.append(inner_str)\n    \n    final_output = '[' + ','.join(result_strings) + ']'\n    print(final_output)\n\nsolve()\n\n```"
        },
        {
            "introduction": "除了分类任务，GNN 也是分析图结构本身的强大工具，例如预测缺失的链接。在这项实践中，您将构建一个基于 GNN 的模型，利用局部邻域信息为潜在的边进行评分。这个练习展示了 GNN 学习到的节点表示如何能被有效应用于链接预测这一基础图任务。",
            "id": "3131905",
            "problem": "您的任务是设计一个纯算法的、基于图神经网络（GNN）的程序，该程序利用类似于三元闭包的局部结构先验来推断无向图中的缺失边。该程序必须源自图论和消息传递的基本定义，并必须使用一个有原则的聚合指标来量化精确率-召回率权衡。最终结果必须针对一个固定的测试套件进行计算，并以精确指定的格式输出。\n\n推导的基本基础如下。\n- 一个图由一个无向邻接矩阵 $A \\in \\{0,1\\}^{n \\times n}$ 表示，其中 $A_{ij} = A_{ji}$ 且 $A_{ii} = 0$。节点集由 $\\{0,1,\\dots,n-1\\}$ 索引，边集为 $E = \\{(i,j) \\mid i  j, A_{ij} = 1\\}$。\n- 图神经网络（GNN）中的消息传递聚合了邻居节点的特征。添加自环后，增强邻接矩阵为 $\\tilde{A} = A + I$，度矩阵为 $\\tilde{D} = \\operatorname{diag}(\\sum_j \\tilde{A}_{ij})$，对称归一化产生 $\\hat{A} = \\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}$。\n- 从单位矩阵作为节点特征 $X = I \\in \\mathbb{R}^{n \\times n}$ 开始，不带学习参数的两步邻域聚合产生的表示为 $H = \\hat{A}^2 X = \\hat{A}^2$。这捕捉了三元闭包所使用的二阶邻域信息，因为 $H$ 中较大的值反映了通过共享邻居实现的更强连接性。\n\n您的程序必须实现以下流程。\n1. 对于每个测试图，提供两个矩阵：观测邻接矩阵 $A^{\\text{obs}}$ 和真实邻接矩阵 $A^{\\text{true}}$。观测邻接矩阵 $A^{\\text{obs}}$ 是GNN必须从中推断缺失边的输入图。真实邻接矩阵 $A^{\\text{true}}$ 决定了哪些缺失边在底层图中是真实存在的，哪些是真实不存在的。\n2. 根据 $A^{\\text{obs}}$ 计算对称归一化邻接矩阵 $\\hat{A}^{\\text{obs}}$，使用 $\\tilde{A}^{\\text{obs}} = A^{\\text{obs}} + I$ 和 $\\hat{A}^{\\text{obs}} = (\\tilde{D}^{\\text{obs}})^{-1/2}\\tilde{A}^{\\text{obs}}(\\tilde{D}^{\\text{obs}})^{-1/2}$，其中 $\\tilde{D}^{\\text{obs}} = \\operatorname{diag}(\\sum_j \\tilde{A}^{\\text{obs}}_{ij})$。\n3. 计算两层消息传递表示 $H = (\\hat{A}^{\\text{obs}})^2$。\n4. 对于 $A^{\\text{obs}}$ 中每一个非边（即 $A^{\\text{obs}}_{ij} = 0$）的无序节点对 $(i,j)$（其中 $i  j$），使用节点表示的内积后跟一个logistic压缩来计算一个标量链接分数 $s(i,j)$：\n   $$ s(i,j) = \\sigma(H_{i,:}^\\top H_{j,:}), \\quad \\sigma(z) = \\frac{1}{1 + e^{-z}}. $$\n5. 为每个候选对定义二元标签 $y(i,j)$ 如下：\n   - 如果 $(i,j)$ 是 $A^{\\text{true}}$ 中的一条边（即 $A^{\\text{true}}_{ij} = 1$），则 $y(i,j) = 1$，该对是待恢复的真实缺失边。\n   - 如果 $(i,j)$ 不是 $A^{\\text{true}}$ 中的一条边（即 $A^{\\text{true}}_{ij} = 0$），则 $y(i,j) = 0$，该对是不应被提出的真实不存在的边。\n6. 通过计算平均精度（AP）来衡量精确率-召回率权衡，AP定义为通过按 $s(i,j)$ 降序对候选对进行排序而产生的精确率-召回率曲线下的面积。假设总共有 $m$ 个正标签。如果候选对的排序列表的标签为 $\\{y_1, y_2, \\dots, y_N\\}$，且排名为 $k$ 时的精确率为 $P(k) = \\frac{1}{k}\\sum_{i=1}^k y_i$，则平均精度为\n   $$ \\mathrm{AP} = \\begin{cases}\n   \\frac{1}{m}\\sum_{k=1}^N P(k)\\cdot \\mathbb{1}\\{y_k = 1\\},   \\text{若 } m > 0, \\\\\n   0,   \\text{若 } m = 0,\n   \\end{cases} $$\n   这是一个标准的、无阈值的精确率-召回率权衡摘要。精确率定义为 $\\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}$，召回率定义为 $\\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}$，其中 $\\mathrm{TP}$、$\\mathrm{FP}$ 和 $\\mathrm{FN}$ 是在给定阈值下的真阳性、假阳性和假阴性的计数。\n\n测试套件包含三个无向图，每个图由 $(A^{\\text{obs}}, A^{\\text{true}})$ 指定，节点索引从 $0$ 开始。在所有情况下，图都是简单的（无平行边）、无权的、对称的。\n- 测试用例 $1$（对三元闭包友好的稀疏图）：\n  - 节点数 $n = 6$。\n  - 观测边 $E^{\\text{obs}} = \\{(0,1),(1,2),(2,3),(3,4),(4,5)\\}$。\n  - 真实边 $E^{\\text{true}} = E^{\\text{obs}} \\cup \\{(0,2),(2,4)\\}$。\n  - 候选对是所有满足 $i  j$ 且 $(i,j) \\notin E^{\\text{obs}}$ 的 $(i,j)$。其中，真实缺失边是 $(0,2)$ 和 $(2,4)$，而所有其他候选对都是真实不存在的。\n- 测试用例 $2$（没有初始三角形的边界情况）：\n  - 节点数 $n = 5$。\n  - 观测边 $E^{\\text{obs}} = \\{(0,1),(1,2),(2,3),(3,4)\\}$。\n  - 真实边 $E^{\\text{true}} = E^{\\text{obs}} \\cup \\{(1,3)\\}$。\n  - 候选对是所有满足 $i  j$ 且 $(i,j) \\notin E^{\\text{obs}}$ 的 $(i,j)$。其中，真实缺失边是 $(1,3)$，而所有其他候选对都是真实不存在的。\n- 测试用例 $3$（混合了真实缺失和真实不存在边的近完全图）：\n  - 节点数 $n = 6$。\n  - 真实边 $E^{\\text{true}}$ 是除 $(0,2)$ 外所有满足 $i  j$ 的对 $(i,j)$，因此底层的真实图是近乎完全的，只有一个真实不存在的边 $(0,2)$。\n  - 观测边 $E^{\\text{obs}} = E^{\\text{true}} \\setminus \\{(0,5),(1,3)\\}$，因此观测图除了真实不存在的边 $(0,2)$ 外，还缺少两条真实存在的边 $(0,5)$ 和 $(1,3)$。\n  - 候选对是 $E^{\\text{obs}}$ 中缺失的三个对：$(0,2)$、$(0,5)$ 和 $(1,3)$。其中，$(0,5)$ 和 $(1,3)$ 是真实缺失的，而 $(0,2)$ 是真实不存在的。\n\n您的程序必须：\n- 为每个测试用例实现上述基于GNN的评分和平均精度计算。\n- 生成一行输出，包含三个测试用例的平均精度值，按顺序排列，格式为用方括号括起来的、逗号分隔的列表，例如 $[\\mathrm{AP}_1,\\mathrm{AP}_2,\\mathrm{AP}_3]$。此问题不涉及单位。数字应为标准的十进制浮点数，不带附加文本。\n\n您的解决方案必须是自包含的，不得读取任何输入或访问外部文件或网络。所有计算都必须使用上面指定的算法步骤执行。",
            "solution": "该问题提出了一个在图深度学习领域内有效且定义明确的任务。它要求实现一个特定的、用于链接预测的无参数图神经网络（GNN）算法，并在一个已定义的测试用例集上使用平均精度（AP）指标对其进行评估。所有定义、约束和数据均已提供，是科学合理的，并且没有歧义或矛盾。因此，我们可以进行形式化的求解。\n\n核心目标是利用观测图（由邻接矩阵 $A^{\\text{obs}}$ 表示）的局部结构来推断其缺失的边。此推断任务的基准真相由一个真实邻接矩阵 $A^{\\text{true}}$ 提供。该方法基于三元闭包原理，该原理假定共享一个共同邻居的两个节点更有可能相连。GNN通过消息传递为形式化这种直觉提供了一个自然的框架。\n\n指定的算法操作如下：\n\n首先，对于一个有 $n$ 个节点和观测邻接矩阵 $A^{\\text{obs}} \\in \\{0,1\\}^{n \\times n}$ 的输入图，我们计算邻接矩阵的对称归一化版本。这是图卷积网络（GCN）中的标准做法，以确保节点表示在传播过程中不会爆炸或消失，并能恰当地平均来自邻居节点的特征。该过程包括三个步骤：\n1.  向图中添加自环，得到增强邻接矩阵 $\\tilde{A}^{\\text{obs}} = A^{\\text{obs}} + I$，其中 $I$ 是 $n \\times n$ 的单位矩阵。这确保了节点在聚合过程中包含自身的特征。\n2.  计算增强度矩阵 $\\tilde{D}^{\\text{obs}} = \\operatorname{diag}(\\sum_j \\tilde{A}^{\\text{obs}}_{ij})$。每个对角线元素 $(\\tilde{D}^{\\text{obs}})_{ii}$ 对应于带自环图中节点 $i$ 的度。\n3.  进行对称归一化以获得 $\\hat{A}^{\\text{obs}} = (\\tilde{D}^{\\text{obs}})^{-1/2}\\tilde{A}^{\\text{obs}}(\\tilde{D}^{\\text{obs}})^{-1/2}$。矩阵 $(\\tilde{D}^{\\text{obs}})^{-1/2}$ 是一个对角矩阵，其每个元素是 $\\tilde{D}^{\\text{obs}}$ 中相应对角线元素的逆平方根。\n\n其次，我们执行一个不带任何可训练参数的两层消息传递操作。从单位矩阵特征 $X=I$ 开始，一层传播得到 $\\hat{A}^{\\text{obs}}X = \\hat{A}^{\\text{obs}}$。第二层得到 $H = \\hat{A}^{\\text{obs}}(\\hat{A}^{\\text{obs}}X) = (\\hat{A}^{\\text{obs}})^2$。结果矩阵 $H \\in \\mathbb{R}^{n \\times n}$ 包含节点表示，其中每一行 $H_{i,:}$ 是节点 $i$ 的特征向量。一个元素 $H_{ij}$ 聚合了节点 $i$ 和 $j$ 之间长度最多为 $2$ 的路径信息，从而有效地编码了它们的 $2$ 跳邻域结构。\n\n第三，我们识别所有用于链接预测的候选对。这些是无序节点对 $(i,j)$（其中 $i  j$），它们在观测图中未连接，即 $A^{\\text{obs}}_{ij} = 0$。\n\n第四，对于每个候选对 $(i,j)$，我们计算一个链接分数 $s(i,j)$。该分数源自节点表示的相似性。具体来说，我们计算它们特征向量的内积 $H_{i,:}^\\top H_{j,:}$，这衡量了它们 $2$ 跳邻域的相似性。然后，这个点积通过一个 logistic sigmoid 函数 $\\sigma(z) = (1 + e^{-z})^{-1}$，将分数压缩到 $(0,1)$ 范围内：\n$$ s(i,j) = \\sigma(H_{i,:}^\\top H_{j,:}) $$\n较高的分数表示更强的结构相似性，因此预测存在链接的可能性更高。\n\n第五，为了评估我们评分函数的性能，我们为每个候选对分配一个基准真相标签 $y(i,j)$。如果边 $(i,j)$ 存在于真实图中（$A^{\\text{true}}_{ij} = 1$），则标签为 $y(i,j) = 1$；否则（$A^{\\text{true}}_{ij} = 0$），标签为 $y(i,j) = 0$。\n\n最后，我们使用平均精度（AP）指标来量化模型将真实缺失的边（正标签）排在真实不存在的边（负标签）之前的能力。流程如下：\n1.  根据所有候选对的分数 $s(i,j)$ 对其进行降序排序。让对应的真实标签序列为 $\\{y_1, y_2, \\dots, y_N\\}$，其中 $N$ 是候选对的总数。\n2.  计算正实例的总数，$m = \\sum_{k=1}^N y_k$。\n3.  如果 $m > 0$，AP 计算为在每个找到真阳性的排名 $k$ 处的精确率值的平均值：\n    $$ \\mathrm{AP} = \\frac{1}{m}\\sum_{k=1}^N P(k)\\cdot \\mathbb{1}\\{y_k = 1\\} $$\n    其中 $P(k) = \\frac{1}{k}\\sum_{i=1}^k y_i$ 是排名为 $k$ 时的精确率，$\\mathbb{1}\\{\\cdot\\}$ 是指示函数。该指标奖励那些将真阳性置于排序列表顶部的模型。\n4.  如果 $m = 0$（没有需要寻找的缺失边），则 AP 定义为 $0$。\n\n这整个流程被独立地应用于所提供的三个测试用例中的每一个。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef create_adj_matrix(n, edges):\n    \"\"\"Creates an n x n symmetric adjacency matrix from a list of edges.\"\"\"\n    A = np.zeros((n, n), dtype=np.float64)\n    for i, j in edges:\n        A[i, j] = 1\n        A[j, i] = 1\n    return A\n\ndef compute_ap(A_obs, A_true):\n    \"\"\"\n    Computes the Average Precision for link prediction on a single graph.\n    \n    Args:\n        A_obs (np.ndarray): The observed adjacency matrix.\n        A_true (np.ndarray): The ground truth adjacency matrix.\n        \n    Returns:\n        float: The Average Precision score.\n    \"\"\"\n    n = A_obs.shape[0]\n\n    # Step 2: Compute the symmetric normalized adjacency matrix A_hat_obs\n    A_obs_tilde = A_obs + np.identity(n)\n    D_obs_tilde_vec = np.sum(A_obs_tilde, axis=1)\n    # Handle case of isolated nodes (degree 0) to avoid division by zero\n    with np.errstate(divide='ignore'):\n        D_inv_sqrt_vec = 1.0 / np.sqrt(D_obs_tilde_vec)\n    D_inv_sqrt_vec[np.isinf(D_inv_sqrt_vec)] = 0\n    D_inv_sqrt_mat = np.diag(D_inv_sqrt_vec)\n    \n    A_hat_obs = D_inv_sqrt_mat @ A_obs_tilde @ D_inv_sqrt_mat\n\n    # Step 3: Compute the two-layer message-passing representations H\n    H = A_hat_obs @ A_hat_obs\n\n    # Step 4, 5: Generate candidates, compute scores, and get labels\n    candidates = []\n    # Iterate over upper triangle to consider each unordered pair once\n    for i in range(n):\n        for j in range(i + 1, n):\n            if A_obs[i, j] == 0:\n                # Compute score s(i,j)\n                score_raw = H[i, :] @ H[j, :]\n                score = 1.0 / (1.0 + np.exp(-score_raw))\n                \n                # Get label y(i,j)\n                label = A_true[i, j]\n                \n                candidates.append({'score': score, 'label': label})\n\n    # Step 6: Compute Average Precision\n    # Sort candidates by score in descending order\n    candidates.sort(key=lambda x: x['score'], reverse=True)\n    \n    m = sum(c['label'] for c in candidates)\n    if m == 0:\n        return 0.0\n\n    ap_sum = 0.0\n    tp_count = 0\n    for k, candidate in enumerate(candidates, 1):\n        if candidate['label'] == 1:\n            tp_count += 1\n            precision_at_k = tp_count / k\n            ap_sum += precision_at_k\n            \n    return ap_sum / m\n\ndef solve():\n    \"\"\"\n    Defines the test suite, runs the GNN-based link prediction pipeline,\n    and prints the results in the specified format.\n    \"\"\"\n    \n    # Test Case 1\n    n1 = 6\n    E_obs1 = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)]\n    E_true1 = E_obs1 + [(0, 2), (2, 4)]\n    A_obs1 = create_adj_matrix(n1, E_obs1)\n    A_true1 = create_adj_matrix(n1, E_true1)\n\n    # Test Case 2\n    n2 = 5\n    E_obs2 = [(0, 1), (1, 2), (2, 3), (3, 4)]\n    E_true2 = E_obs2 + [(1, 3)]\n    A_obs2 = create_adj_matrix(n2, E_obs2)\n    A_true2 = create_adj_matrix(n2, E_true2)\n\n    # Test Case 3\n    n3 = 6\n    # True graph is complete except for edge (0, 2)\n    all_edges_n3 = [(i, j) for i in range(n3) for j in range(i + 1, n3)]\n    E_true3_list = [edge for edge in all_edges_n3 if edge != (0, 2)]\n    A_true3 = create_adj_matrix(n3, E_true3_list)\n    # Observed graph is true graph minus edges (0, 5) and (1, 3)\n    missing_in_obs = {(0, 5), (1, 3)}\n    E_obs3_list = [edge for edge in E_true3_list if edge not in missing_in_obs]\n    A_obs3 = create_adj_matrix(n3, E_obs3_list)\n\n    test_cases = [\n        (A_obs1, A_true1),\n        (A_obs2, A_true2),\n        (A_obs3, A_true3),\n    ]\n\n    results = []\n    for A_obs, A_true in test_cases:\n        ap_score = compute_ap(A_obs, A_true)\n        results.append(ap_score)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}