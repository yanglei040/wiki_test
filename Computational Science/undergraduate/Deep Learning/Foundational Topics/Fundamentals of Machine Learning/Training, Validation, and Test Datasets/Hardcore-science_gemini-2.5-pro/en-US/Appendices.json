{
    "hands_on_practices": [
        {
            "introduction": "The fundamental assumption behind using a test set is that it provides an unbiased measure of generalization performance on unseen data. However, this assumption is easily violated if the test set contains examples that are identical or near-identical to examples in the training set. This exercise  tackles this critical issue of \"split hygiene\" by using perceptual hashing to detect near-duplicates. You will work with a statistical model of hash collisions to understand and quantify the inherent trade-off between false positives (incorrectly flagging different items as duplicates) and false negatives (missing true duplicates) when setting a detection threshold.",
            "id": "3194863",
            "problem": "You are tasked with formalizing near-duplicate detection for split hygiene in a dataset used for deep learning. Consider that each example (for instance, an image) is summarized by a fixed-length perceptual hash of $L$ bits. For any pair of examples across different splits (training, validation, test), define the Hamming distance $D$ as the number of bit positions where the two hashes differ. A duplicate detector declares a pair as a duplicate if and only if $D \\leq T$, where $T$ is an integer threshold.\n\nAssume the following well-tested stochastic model. For two items that are truly the same underlying content (true duplicate pair) processed independently by the hashing pipeline, each bit is flipped between the two hashes independently with probability $p_d$, so that the Hamming distance $D$ for a true duplicate pair follows the binomial distribution with parameters $L$ and $p_d$. For two items that are truly different content (non-duplicate pair), each bit differs independently with probability $p_n$, so that the Hamming distance $D$ for a non-duplicate pair follows the binomial distribution with parameters $L$ and $p_n$. These independence and binomial assumptions are standard for modeling perceptual hashing noise and collision behavior.\n\nYour goals are:\n- From the fundamental definitions above, derive expressions for the false positive rate and false negative rate as functions of $L$, $T$, $p_d$, and $p_n$.\n- Use these expressions to compute the expected numbers of false positives and false negatives when the detector is evaluated on a specified number of cross-split pairs with known ground truth.\n\nDefinitions to use:\n- False positive rate is the probability that a non-duplicate pair is incorrectly declared a duplicate. Using the binomial model, this is the probability that $D \\leq T$ conditioned on the non-duplicate distribution with parameters $L$ and $p_n$.\n- False negative rate is the probability that a true duplicate pair is incorrectly declared a non-duplicate. Using the binomial model, this is the probability that $D > T$ conditioned on the duplicate distribution with parameters $L$ and $p_d$.\n- If there are $N_{\\text{non}}$ non-duplicate cross-split pairs and $N_{\\text{dup}}$ duplicate cross-split pairs, then the expected false positive count is the false positive rate multiplied by $N_{\\text{non}}$, and the expected false negative count is the false negative rate multiplied by $N_{\\text{dup}}$.\n\nYour program must implement these computations using exact binomial cumulative distribution values, not simulation, for the following test suite. Each test case specifies $(L, p_d, p_n, T, N_{\\text{dup}}, N_{\\text{non}})$:\n\n- Case A (typical setting): $L = 64$, $p_d = 0.06$, $p_n = 0.50$, $T = 10$, $N_{\\text{dup}} = 1000$, $N_{\\text{non}} = 200000$.\n- Case B (extremely strict threshold): $L = 64$, $p_d = 0.06$, $p_n = 0.50$, $T = 0$, $N_{\\text{dup}} = 1000$, $N_{\\text{non}} = 200000$.\n- Case C (extremely loose threshold): $L = 64$, $p_d = 0.06$, $p_n = 0.50$, $T = 64$, $N_{\\text{dup}} = 1000$, $N_{\\text{non}} = 200000$.\n- Case D (overlapping distributions): $L = 32$, $p_d = 0.22$, $p_n = 0.35$, $T = 12$, $N_{\\text{dup}} = 10000$, $N_{\\text{non}} = 50000$.\n- Case E (short hash): $L = 8$, $p_d = 0.10$, $p_n = 0.30$, $T = 2$, $N_{\\text{dup}} = 100$, $N_{\\text{non}} = 1000$.\n\nQuantified outputs:\n- For each test case, compute four quantities: the false positive rate as a decimal in $[0,1]$, the false negative rate as a decimal in $[0,1]$, the expected false positive count as a real number, and the expected false negative count as a real number. All four values must be rounded to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list in the form $[ \\text{FPR}, \\text{FNR}, \\text{EFP}, \\text{EFN} ]$. For example, a valid output looks like $[[0.001000,0.100000,123.456000,78.900000],[\\dots]]$ with no spaces. Do not print anything else.",
            "solution": "The problem requires the derivation and computation of performance metrics for a duplicate detector based on perceptual hash comparison. The core of the problem lies in the application of the binomial distribution to model the Hamming distance between hashes. We will proceed by first formalizing the random variables, then deriving the expressions for the false positive and false negative rates, and finally outlining the computation of the expected counts.\n\nLet the length of the perceptual hash be $L$. For any pair of hashes, the Hamming distance $D$ is the number of bit positions that differ. The detection rule is that a pair is flagged as a duplicate if and only if $D \\leq T$ for a given integer threshold $T$.\n\nThe problem defines two distinct stochastic models for the Hamming distance, contingent on whether a pair of items represents truly the same content (a true duplicate) or different content (a non-duplicate).\n\nLet $X_{nd}$ be the random variable for the Hamming distance $D$ between two hashes of non-duplicate items. The problem states that $X_{nd}$ follows a binomial distribution with parameters $L$ and $p_n$. We denote this as:\n$$\nX_{nd} \\sim B(L, p_n)\n$$\nThe probability mass function (PMF) is given by $P(X_{nd} = k) = \\binom{L}{k} p_n^k (1-p_n)^{L-k}$ for $k \\in \\{0, 1, \\dots, L\\}$.\n\nLet $X_{d}$ be the random variable for the Hamming distance $D$ between two hashes of a true duplicate pair. The problem states that $X_{d}$ follows a binomial distribution with parameters $L$ and $p_d$. We denote this as:\n$$\nX_{d} \\sim B(L, p_d)\n$$\nThe PMF is given by $P(X_{d} = k) = \\binom{L}{k} p_d^k (1-p_d)^{L-k}$ for $k \\in \\{0, 1, \\dots, L\\}$.\n\nWith these definitions, we can derive the required metrics.\n\n**False Positive Rate (FPR)**\nA false positive occurs when a non-duplicate pair is incorrectly classified as a duplicate. This happens when the Hamming distance for a non-duplicate pair, $X_{nd}$, satisfies the condition $X_{nd} \\leq T$.\nThe false positive rate is the probability of this event:\n$$\n\\text{FPR} = P(X_{nd} \\leq T)\n$$\nThis probability corresponds to the cumulative distribution function (CDF) of the binomial distribution $B(L, p_n)$ evaluated at the threshold $T$. Mathematically, it is the sum of probabilities for all outcomes from $0$ to $T$:\n$$\n\\text{FPR} = \\sum_{k=0}^{T} P(X_{nd} = k) = \\sum_{k=0}^{T} \\binom{L}{k} p_n^k (1-p_n)^{L-k}\n$$\n\n**False Negative Rate (FNR)**\nA false negative occurs when a true duplicate pair is incorrectly classified as a non-duplicate. This happens when the Hamming distance for a true duplicate pair, $X_{d}$, fails the detection condition, i.e., $X_{d} > T$.\nThe false negative rate is the probability of this event:\n$$\n\\text{FNR} = P(X_{d} > T)\n$$\nThis probability is the survival function (or complementary CDF) of the binomial distribution $B(L, p_d)$ evaluated at $T$. It can be calculated by summing the probabilities of all outcomes greater than $T$:\n$$\n\\text{FNR} = \\sum_{k=T+1}^{L} P(X_{d} = k) = \\sum_{k=T+1}^{L} \\binom{L}{k} p_d^k (1-p_d)^{L-k}\n$$\nAlternatively, it can be computed as $1$ minus the CDF:\n$$\n\\text{FNR} = 1 - P(X_{d} \\leq T) = 1 - \\sum_{k=0}^{T} \\binom{L}{k} p_d^k (1-p_d)^{L-k}\n$$\n\n**Expected Counts**\nGiven a set of $N_{\\text{non}}$ non-duplicate pairs and $N_{\\text{dup}}$ true duplicate pairs, the expected number of misclassifications can be calculated.\nThe expected number of false positives (EFP) is the product of the number of non-duplicate pairs and the probability of any single one being a false positive:\n$$\nE[\\text{FP}] = N_{\\text{non}} \\times \\text{FPR}\n$$\nSimilarly, the expected number of false negatives (EFN) is the product of the number of true duplicate pairs and the probability of any single one being a false negative:\n$$\nE[\\text{FN}] = N_{\\text{dup}} \\times \\text{FNR}\n$$\n\nThe computational procedure for each test case $(L, p_d, p_n, T, N_{\\text{dup}}, N_{\\text{non}})$ is as follows:\n1.  Compute $\\text{FPR} = \\sum_{k=0}^{T} \\binom{L}{k} p_n^k (1-p_n)^{L-k}$ using the CDF of $B(L, p_n)$.\n2.  Compute $\\text{FNR} = 1 - \\sum_{k=0}^{T} \\binom{L}{k} p_d^k (1-p_d)^{L-k}$ using the CDF of $B(L, p_d)$, or directly using the survival function.\n3.  Compute $E[\\text{FP}] = N_{\\text{non}} \\times \\text{FPR}$.\n4.  Compute $E[\\text{FN}] = N_{\\text{dup}} \\times \\text{FNR}$.\n5.  Round all four results to six decimal places.\n\nThese calculations are performed for each of the five specified test cases. The implementation will utilize numerical libraries that provide exact functions for the binomial CDF and survival function to avoid manual summation and ensure numerical stability.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import binom\n\ndef solve():\n    \"\"\"\n    Computes false positive/negative rates and expected counts for duplicate detection\n    based on a binomial model of Hamming distances.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each test case is a tuple: (L, p_d, p_n, T, N_dup, N_non)\n    test_cases = [\n        (64, 0.06, 0.50, 10, 1000, 200000),  # Case A\n        (64, 0.06, 0.50, 0, 1000, 200000),   # Case B\n        (64, 0.06, 0.50, 64, 1000, 200000),  # Case C\n        (32, 0.22, 0.35, 12, 10000, 50000),  # Case D\n        (8, 0.10, 0.30, 2, 100, 1000),       # Case E\n    ]\n\n    # results will store the string representation of each sub-list, e.g., \"[0.1,0.2,...]\"\n    # This is done to achieve the exact output format with no spaces.\n    results = []\n    for case in test_cases:\n        L, p_d, p_n, T, N_dup, N_non = case\n\n        # False Positive Rate (FPR): P(D = T) for non-duplicates (D ~ B(L, p_n))\n        fpr = binom.cdf(T, L, p_n)\n\n        # False Negative Rate (FNR): P(D > T) for true duplicates (D ~ B(L, p_d))\n        # scipy.stats.binom.sf (survival function) computes P(X > k) directly.\n        fnr = binom.sf(T, L, p_d)\n        \n        # Expected False Positives (EFP)\n        efp = fpr * N_non\n\n        # Expected False Negatives (EFN)\n        efn = fnr * N_dup\n        \n        # Create the string for the inner list with specific formatting and no spaces,\n        # ensuring each number is rounded to six decimal places.\n        result_str = (\n            f\"[{f'{fpr:.6f}'},\"\n            f\"{f'{fnr:.6f}'},\"\n            f\"{f'{efp:.6f}'},\"\n            f\"{f'{efn:.6f}'}]\"\n        )\n        results.append(result_str)\n\n    # Final print statement in the exact required format: [[...],[...],...]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond preparing clean datasets, we must use them efficiently. During training, the validation set is crucial for early stoppingâ€”identifying the point where the model begins to overfit. This practice  frames this process as a resource optimization problem, a common scenario in real-world deep learning where computational budgets are finite. By modeling the learning curve and the costs of training versus validation, you will derive the optimal frequency for running validation checks to best approximate the ideal stopping point, balancing the risk of overfitting against the cost of frequent evaluation.",
            "id": "3194810",
            "problem": "A team trains a deep neural network with Stochastic Gradient Descent (SGD) on a fixed training dataset and uses a separate validation dataset to select the final checkpoint by early stopping. Let the expected test risk at training step $s$ be modeled by a learning curve that strictly decreases before an optimal stopping point $S^{\\star}$ and strictly increases after it due to overfitting. Concretely, assume there exists a minimal expected test risk $R_{\\min}$ attained at $s=S^{\\star}$, and that for $sS^{\\star}$ the expected test risk increases approximately linearly with slope $\\lambda$ per additional training step, while for $sS^{\\star}$ it decreases monotonically sufficiently fast that the minimum is unique at $S^{\\star}$.\n\nValidation is performed every $k$ training steps. The final model is chosen as the checkpoint among the validated ones with the lowest validation loss. Because $S^{\\star}$ is generally not a multiple of $k$, the nearest validated checkpoint has, in expectation, a mismatch of $k/2$ steps from $S^{\\star}$, incurring an expected excess test risk of approximately $\\lambda \\cdot (k/2)$ relative to $R_{\\min}$.\n\nCompute is measured in training-step equivalents. Each training step costs $c_{t}$ units, and each full validation pass over the validation dataset costs $c_{v}$ units. With a fixed compute budget $B$, training for $N$ steps and validating every $k$ steps incurs total cost $c_{t}N + c_{v}\\cdot(N/k)$, which must not exceed $B$. To ensure availability of a checkpoint near the optimal stopping point, require $N \\geq S^{\\star}$.\n\nUnder this model, derive the optimal validation interval $k^{\\star}$ (in steps) that minimizes the expected test risk subject to the compute budget constraint and the requirement $N \\geq S^{\\star}$. Then, using the following parameters, compute $k^{\\star}$:\n- $S^{\\star} = 6.0 \\times 10^{5}$ steps,\n- $B = 8.0 \\times 10^{5}$ step-equivalents,\n- $c_{t} = 1$ step-equivalent per training step,\n- $c_{v} = 1.5224 \\times 10^{3}$ step-equivalents per validation,\n- $\\lambda = 1.0 \\times 10^{-6}$ risk units per step.\n\nRound your final answer for $k^{\\star}$ to four significant figures. Express your answer as a number of steps.",
            "solution": "The problem requires the derivation of the optimal validation interval, $k^{\\star}$, that minimizes the expected test risk under a fixed computational budget. The solution process begins with a formal validation of the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe givens are extracted verbatim from the problem statement:\n-   The expected test risk has a unique minimum, $R_{\\min}$, at training step $s=S^{\\star}$.\n-   For $s  S^{\\star}$, the expected test risk increases approximately linearly with slope $\\lambda$.\n-   Validation is performed every $k$ training steps.\n-   The expected mismatch between $S^{\\star}$ and the nearest validated checkpoint is $k/2$.\n-   The expected excess test risk is approximately $\\lambda \\cdot (k/2)$.\n-   The cost of one training step is $c_{t}$.\n-   The cost of one full validation pass is $c_{v}$.\n-   The total number of training steps is $N$.\n-   The total compute budget is $B$.\n-   The total cost is $c_{t}N + c_{v}\\cdot(N/k)$.\n-   Budget constraint: $c_{t}N + c_{v}\\cdot(N/k) \\leq B$.\n-   Training completion constraint: $N \\geq S^{\\star}$.\n-   Parameters:\n    -   $S^{\\star} = 6.0 \\times 10^{5}$ steps\n    -   $B = 8.0 \\times 10^{5}$ step-equivalents\n    -   $c_{t} = 1$ step-equivalent per training step\n    -   $c_{v} = 1.5224 \\times 10^{3}$ step-equivalents per validation\n    -   $\\lambda = 1.0 \\times 10^{-6}$ risk units per step\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the specified criteria:\n-   **Scientifically Grounded**: The problem is based on established concepts in machine learning, including early stopping, overfitting, and computational budget management. The model for the learning curve (a U-shape) and the linear approximation of risk increase post-optimum are standard simplifying assumptions. The cost model for training and validation is also a standard formulation. The explicit premise of an expected step mismatch of $k/2$ is a specific modeling choice provided in the problem statement; it does not violate any fundamental principles and defines the context in which the optimization must be performed.\n-   **Well-Posed**: The problem is a well-posed optimization task. It requires minimizing a clearly defined objective function (proportional to $k$) subject to a set of algebraic inequality constraints. All necessary parameters are provided to find a unique solution.\n-   **Objective**: The problem is stated in precise, quantitative, and unbiased language.\n\nThe problem is found to be free of scientific unsoundness, ambiguity, and internal contradictions. It presents a solvable, self-contained optimization problem.\n\n**Step 3: Verdict and Action**\n\nThe problem is deemed **valid**. A complete solution will be provided.\n\n### Solution Derivation\n\nThe objective is to minimize the expected excess test risk, which is given as a function of the validation interval $k$:\n$$\nR_{\\text{excess}}(k) = \\frac{\\lambda k}{2}\n$$\nSince the slope $\\lambda$ is a positive constant, minimizing $R_{\\text{excess}}(k)$ is equivalent to minimizing the validation interval $k$. Our goal is to find the smallest possible value of $k$ that satisfies all constraints.\n\nThe constraints are:\n1.  The total computational cost must not exceed the budget $B$. The cost for training for $N$ steps and validating $\\frac{N}{k}$ times is:\n    $$\n    C(N, k) = c_{t}N + c_{v}\\frac{N}{k}\n    $$\n    Thus, the budget constraint is:\n    $$\n    N \\left(c_{t} + \\frac{c_{v}}{k}\\right) \\leq B\n    $$\n2.  The total number of training steps $N$ must be at least the optimal stopping point $S^{\\star}$:\n    $$\n    N \\geq S^{\\star}\n    $$\n\nFrom the budget constraint, we can express an upper bound on $N$:\n$$\nN \\leq \\frac{B}{c_{t} + \\frac{c_{v}}{k}}\n$$\nCombining this with the training duration constraint, we have:\n$$\nS^{\\star} \\leq N \\leq \\frac{B}{c_{t} + \\frac{c_{v}}{k}}\n$$\nFor a feasible solution for $N$ to exist for a given $k$, the lower bound on $N$ must be less than or equal to the upper bound:\n$$\nS^{\\star} \\leq \\frac{B}{c_{t} + \\frac{c_{v}}{k}}\n$$\nThis inequality establishes a relationship between the validation interval $k$ and the fixed parameters of the problem. We must rearrange this inequality to find the constraint on $k$.\n$$\nS^{\\star} \\left(c_{t} + \\frac{c_{v}}{k}\\right) \\leq B\n$$\n$$\nS^{\\star}c_{t} + \\frac{S^{\\star}c_{v}}{k} \\leq B\n$$\n$$\n\\frac{S^{\\star}c_{v}}{k} \\leq B - S^{\\star}c_{t}\n$$\nBefore proceeding, we verify that the term $B - S^{\\star}c_{t}$ is positive. Using the given values:\n$$\nB - S^{\\star}c_{t} = (8.0 \\times 10^{5}) - (6.0 \\times 10^{5})(1) = 2.0 \\times 10^{5}  0\n$$\nSince the right-hand side is positive, we can safely invert the inequality (noting that $k  0$):\n$$\nk \\geq \\frac{S^{\\star}c_{v}}{B - S^{\\star}c_{t}}\n$$\nThis inequality provides a lower bound for the validation interval $k$. Since our objective is to minimize $k$, the optimal value $k^{\\star}$ will be this minimum possible value:\n$$\nk^{\\star} = \\frac{S^{\\star}c_{v}}{B - S^{\\star}c_{t}}\n$$\nThis optimal value $k^{\\star}$ corresponds to the strategy of training for the minimum required duration, $N=S^{\\star}$, and using the entire remaining budget for validation.\n\nNow we substitute the given numerical values to compute $k^{\\star}$:\n-   $S^{\\star} = 6.0 \\times 10^{5}$\n-   $B = 8.0 \\times 10^{5}$\n-   $c_{t} = 1$\n-   $c_{v} = 1.5224 \\times 10^{3}$\n\n$$\nk^{\\star} = \\frac{(6.0 \\times 10^{5}) \\cdot (1.5224 \\times 10^{3})}{8.0 \\times 10^{5} - (1) \\cdot (6.0 \\times 10^{5})}\n$$\n$$\nk^{\\star} = \\frac{9.1344 \\times 10^{8}}{2.0 \\times 10^{5}}\n$$\n$$\nk^{\\star} = 4.5672 \\times 10^{3} = 4567.2\n$$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures of $4567.2$ are $4$, $5$, $6$, and $7$. The fifth digit is $2$, which is less than $5$, so we round down.\n$$\nk^{\\star} \\approx 4567\n$$\nThe optimal validation interval is $4567$ steps.",
            "answer": "$$\n\\boxed{4567}\n$$"
        },
        {
            "introduction": "A trained model that outputs probabilities is only half the solution; to deploy it, we must convert these probabilities into concrete decisions. This final hands-on practice  explores how to use a validation set to select an optimal decision threshold that aligns with real-world objectives. You will move beyond simple accuracy and learn to minimize a cost function that assigns different penalties to false positives and false negatives, a crucial skill for applications in fields like medicine or finance. The exercise also introduces a robust approach for setting this threshold when the true costs are uncertain, providing a glimpse into risk-averse decision-making.",
            "id": "3194809",
            "problem": "You are given a binary classifier that outputs calibrated posterior probabilities for the positive class on both a validation dataset and a test dataset. The objective is to select a decision threshold on the validation dataset that minimizes empirical expected cost under given validation costs, analyze the mismatch when the test costs differ, and construct a robust threshold that hedges against uncertainty in the test cost ratio using a minimax criterion.\n\nFundamental base: Use the standard empirical risk (expected cost) framework and the Bayes decision rule. A prediction for an instance with probability $p$ is positive if $p \\ge t$ and negative otherwise. The empirical expected cost on a dataset $D$ of size $n$ under false positive cost $C_{\\mathrm{FP}}$ and false negative cost $C_{\\mathrm{FN}}$ is built from the false positive rate and false negative rate. Let $\\hat{y}_i(t)$ denote the prediction at threshold $t$ for the $i$-th instance with label $y_i \\in \\{0,1\\}$ and predicted probability $p_i$. Define the empirical false positive rate as\n$$\n\\mathrm{FP\\_rate}(t) = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{1}\\big(\\hat{y}_i(t)=1, y_i=0\\big),\n$$\nand the empirical false negative rate as\n$$\n\\mathrm{FN\\_rate}(t) = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{1}\\big(\\hat{y}_i(t)=0, y_i=1\\big).\n$$\nThe empirical expected cost is\n$$\nR_D(t; C_{\\mathrm{FP}}, C_{\\mathrm{FN}}) = C_{\\mathrm{FP}} \\cdot \\mathrm{FP\\_rate}(t) + C_{\\mathrm{FN}} \\cdot \\mathrm{FN\\_rate}(t).\n$$\nStarting from these definitions, your program must:\n- Minimize $R_{\\mathrm{val}}(t; C_{\\mathrm{FP}}^{\\mathrm{val}}, C_{\\mathrm{FN}}^{\\mathrm{val}})$ over $t$ to select a validation threshold.\n- Derive the Bayes decision threshold purely from $C_{\\mathrm{FP}}$ and $C_{\\mathrm{FN}}$ without using data-driven counts.\n- Evaluate the empirical expected cost on the test dataset when using the validation-selected threshold compared to the test-optimal threshold that minimizes $R_{\\mathrm{test}}(t; C_{\\mathrm{FP}}^{\\mathrm{test}}, C_{\\mathrm{FN}}^{\\mathrm{test}})$.\n- Construct a robust threshold under uncertainty in the test cost ratio. Assume the test costs are uncertain but normalized to satisfy $C_{\\mathrm{FP}} + C_{\\mathrm{FN}} = 1$ with $C_{\\mathrm{FP}} \\in [r_{\\min}, r_{\\max}]$ and $C_{\\mathrm{FN}} = 1 - C_{\\mathrm{FP}}$. Choose $t$ to minimize the worst-case empirical expected cost on the validation dataset:\n$$\nt_{\\mathrm{robust}} = \\arg\\min_t \\ \\sup_{r \\in [r_{\\min}, r_{\\max}]} \\left( r \\cdot \\mathrm{FP\\_rate}(t) + (1-r) \\cdot \\mathrm{FN\\_rate}(t) \\right).\n$$\n\nAlgorithmic constraints:\n- The threshold search space must be chosen so that the empirical cost is exactly minimized. Use the set of candidate thresholds formed by the sorted unique probability values observed on the dataset, together with $0$ and $1+\\varepsilon$ for a small positive $\\varepsilon$, ensuring coverage of all classification regimes. Use the decision rule $\\hat{y}=1$ if $p \\ge t$ and $\\hat{y}=0$ otherwise. In case of ties (multiple thresholds yielding identical minimal empirical expected cost), select the smallest threshold.\n- The Bayes decision threshold must be derived from first principles and expressed as a function of $C_{\\mathrm{FP}}$ and $C_{\\mathrm{FN}}$ only.\n\nTest suite:\nFor each of the following parameter sets, your program must compute:\n- The validation-optimal threshold $t_{\\mathrm{val}}$ minimizing $R_{\\mathrm{val}}(t; C_{\\mathrm{FP}}^{\\mathrm{val}}, C_{\\mathrm{FN}}^{\\mathrm{val}})$.\n- The Bayes threshold $t_{\\mathrm{Bayes}}$ derived from $C_{\\mathrm{FP}}^{\\mathrm{val}}$ and $C_{\\mathrm{FN}}^{\\mathrm{val}}$.\n- The test-optimal threshold $t_{\\mathrm{test}}$ minimizing $R_{\\mathrm{test}}(t; C_{\\mathrm{FP}}^{\\mathrm{test}}, C_{\\mathrm{FN}}^{\\mathrm{test}})$.\n- The robust threshold $t_{\\mathrm{robust}}$ minimizing the worst-case validation cost over $r \\in [r_{\\min}, r_{\\max}]$ with $C_{\\mathrm{FP}}=r$ and $C_{\\mathrm{FN}}=1-r$.\n- The difference in empirical expected test cost $\\Delta_{\\mathrm{test}} = R_{\\mathrm{test}}(t_{\\mathrm{val}}; C_{\\mathrm{FP}}^{\\mathrm{test}}, C_{\\mathrm{FN}}^{\\mathrm{test}}) - R_{\\mathrm{test}}(t_{\\mathrm{test}}; C_{\\mathrm{FP}}^{\\mathrm{test}}, C_{\\mathrm{FN}}^{\\mathrm{test}})$.\n\nParameter sets:\n- Case $1$ (balanced, moderate mismatch):\n    - Validation probabilities $[0.05, 0.10, 0.15, 0.20, 0.30, 0.40, 0.60, 0.70, 0.80, 0.90]$ and labels $[0, 0, 0, 0, 0, 0, 1, 1, 1, 1]$.\n    - Test probabilities $[0.02, 0.12, 0.18, 0.25, 0.35, 0.45, 0.55, 0.68, 0.78, 0.88]$ and labels $[0, 0, 0, 0, 0, 0, 1, 1, 1, 1]$.\n    - Validation costs $(C_{\\mathrm{FP}}^{\\mathrm{val}}, C_{\\mathrm{FN}}^{\\mathrm{val}}) = (1, 2)$.\n    - Test costs $(C_{\\mathrm{FP}}^{\\mathrm{test}}, C_{\\mathrm{FN}}^{\\mathrm{test}}) = (2, 1)$.\n    - Robust ratio interval $[r_{\\min}, r_{\\max}] = [0.3, 0.7]$.\n- Case $2$ (highly imbalanced, heavy false negative test cost):\n    - Validation probabilities $[0.05, 0.07, 0.09, 0.12, 0.18, 0.22, 0.27, 0.35, 0.42, 0.50, 0.62, 0.75]$ and labels $[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]$.\n    - Test probabilities $[0.03, 0.08, 0.10, 0.13, 0.19, 0.25, 0.28, 0.33, 0.41, 0.52, 0.60, 0.78]$ and labels $[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]$.\n    - Validation costs $(C_{\\mathrm{FP}}^{\\mathrm{val}}, C_{\\mathrm{FN}}^{\\mathrm{val}}) = (1, 5)$.\n    - Test costs $(C_{\\mathrm{FP}}^{\\mathrm{test}}, C_{\\mathrm{FN}}^{\\mathrm{test}}) = (1, 10)$.\n    - Robust ratio interval $[r_{\\min}, r_{\\max}] = [0.1, 0.4]$.\n- Case $3$ (near-perfect separation, equal costs):\n    - Validation probabilities $[0.01, 0.02, 0.05, 0.95, 0.97, 0.99]$ and labels $[0, 0, 0, 1, 1, 1]$.\n    - Test probabilities $[0.02, 0.03, 0.06, 0.94, 0.96, 0.98]$ and labels $[0, 0, 0, 1, 1, 1]$.\n    - Validation costs $(C_{\\mathrm{FP}}^{\\mathrm{val}}, C_{\\mathrm{FN}}^{\\mathrm{val}}) = (1, 1)$.\n    - Test costs $(C_{\\mathrm{FP}}^{\\mathrm{test}}, C_{\\mathrm{FN}}^{\\mathrm{test}}) = (1, 1)$.\n    - Robust ratio interval $[r_{\\min}, r_{\\max}] = [0.5, 0.5]$.\n- Case $4$ (degenerate identical probabilities, conflicting costs):\n    - Validation probabilities $[0.5, 0.5, 0.5, 0.5, 0.5, 0.5]$ and labels $[0, 0, 1, 1, 0, 1]$.\n    - Test probabilities $[0.5, 0.5, 0.5, 0.5, 0.5, 0.5]$ and labels $[0, 0, 1, 1, 0, 1]$.\n    - Validation costs $(C_{\\mathrm{FP}}^{\\mathrm{val}}, C_{\\mathrm{FN}}^{\\mathrm{val}}) = (3, 1)$.\n    - Test costs $(C_{\\mathrm{FP}}^{\\mathrm{test}}, C_{\\mathrm{FN}}^{\\mathrm{test}}) = (1, 3)$.\n    - Robust ratio interval $[r_{\\min}, r_{\\max}] = [0.2, 0.8]$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one case and must itself be a list of five decimal numbers in the order $[t_{\\mathrm{val}}, t_{\\mathrm{Bayes}}, t_{\\mathrm{test}}, t_{\\mathrm{robust}}, \\Delta_{\\mathrm{test}}]$. For example, a valid output line for two cases would look like $[[0.333333,0.333333,0.666667,0.500000,0.050000],[\\dots]]$ with no additional text. Angles and physical units are not applicable; all outputs are dimensionless real numbers.",
            "solution": "The problem requires a multi-faceted analysis of decision thresholds for a binary classifier. We must determine optimal thresholds under different criteria and evaluate performance mismatches between validation and test sets. The solution will be systematically constructed based on the principles of statistical decision theory and empirical risk minimization.\n\n### Step 1: Theoretical Foundation\n\nLet us begin by formalizing the components of the problem.\n\n**1.1. Empirical Risk (Expected Cost)**\nA binary classifier assigns a posterior probability $p_i = P(Y=1 | \\mathbf{x}_i)$ to each instance $i$ with features $\\mathbf{x}_i$. A decision threshold $t$ is used to make a final prediction $\\hat{y}_i(t) \\in \\{0, 1\\}$. The decision rule is given as:\n$$\n\\hat{y}_i(t) =\n\\begin{cases}\n1  \\text{if } p_i \\ge t \\\\\n0  \\text{if } p_i  t\n\\end{cases}\n$$\nThe problem defines the empirical false positive and false negative rates over a dataset $D$ of size $n$ as:\n$$\n\\mathrm{FP\\_rate}(t) = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{1}\\big(\\hat{y}_i(t)=1, y_i=0\\big) = \\frac{1}{n} \\left| \\{ i \\mid p_i \\ge t, y_i=0 \\} \\right|\n$$\n$$\n\\mathrm{FN\\_rate}(t) = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{1}\\big(\\hat{y}_i(t)=0, y_i=1\\big) = \\frac{1}{n} \\left| \\{ i \\mid p_i  t, y_i=1 \\} \\right|\n$$\nHere, $\\mathbb{1}(\\cdot)$ is the indicator function. Note that these rates are normalized by the total sample size $n$, not the number of actual negative or positive instances. Given costs for false positives ($C_{\\mathrm{FP}}$) and false negatives ($C_{\\mathrm{FN}}$), the empirical expected cost, or risk, is:\n$$\nR_D(t; C_{\\mathrm{FP}}, C_{\\mathrm{FN}}) = C_{\\mathrm{FP}} \\cdot \\mathrm{FP\\_rate}(t) + C_{\\mathrm{FN}} \\cdot \\mathrm{FN\\_rate}(t)\n$$\nMinimizing this risk over $t$ on the validation set gives the validation-optimal threshold, $t_{\\mathrm{val}}$. A similar minimization on the test set gives the test-optimal threshold, $t_{\\mathrm{test}}$.\n\n**1.2. Bayes Decision Threshold**\nThe Bayes decision rule minimizes the expected cost for a single instance. For an instance with predicted probability $p = P(Y=1)$, the expected cost of predicting positive ($\\hat{y}=1$) is $E[\\text{Cost}|\\hat{y}=1] = C_{\\mathrm{FP}} \\cdot (1-p)$. The expected cost of predicting negative ($\\hat{y}=0$) is $E[\\text{Cost}|\\hat{y}=0] = C_{\\mathrm{FN}} \\cdot p$. The optimal decision is to predict positive if $E[\\text{Cost}|\\hat{y}=1] \\le E[\\text{Cost}|\\hat{y}=0]$, which leads to:\n$$\nC_{\\mathrm{FP}}(1-p) \\le C_{\\mathrm{FN}} p \\implies C_{\\mathrm{FP}} \\le (C_{\\mathrm{FP}} + C_{\\mathrm{FN}})p \\implies p \\ge \\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FP}} + C_{\\mathrm{FN}}}\n$$\nThus, the Bayes-optimal threshold, under the assumption that the classifier's output $p$ is the true posterior probability, is:\n$$\nt_{\\mathrm{Bayes}} = \\frac{C_{\\mathrm{FP}}}{C_{\\mathrm{FP}} + C_{\\mathrm{FN}}}\n$$\nThis provides a theoretical benchmark derived purely from costs.\n\n**1.3. Robust Minimax Threshold**\nUncertainty in costs can be handled using a minimax approach. Here, the costs on the test set are unknown, but are assumed to be normalized such that $C_{\\mathrm{FP}} + C_{\\mathrm{FN}} = 1$. The uncertainty is captured by $C_{\\mathrm{FP}} = r$ varying within an interval $[r_{\\min}, r_{\\max}]$. The objective is to find a threshold $t$ that minimizes the worst-case risk on the validation data:\n$$\nt_{\\mathrm{robust}} = \\arg\\min_t \\left( \\sup_{r \\in [r_{\\min}, r_{\\max}]} R_{\\mathrm{val}}(t; r, 1-r) \\right)\n$$\nFor a fixed threshold $t$, the risk $R_{\\mathrm{val}}(t; r, 1-r) = r \\cdot \\mathrm{FP\\_rate}^{\\mathrm{val}}(t) + (1-r) \\cdot \\mathrm{FN\\_rate}^{\\mathrm{val}}(t)$ is a linear function of $r$. The supremum (maximum) of a linear function over a closed interval $[r_{\\min}, r_{\\max}]$ must occur at one of the endpoints. Therefore, the worst-case risk for a given $t$ is:\n$$\n\\sup_{r \\in [r_{\\min}, r_{\\max}]} R_{\\mathrm{val}}(t; r, 1-r) = \\max \\left( R_{\\mathrm{val}}(t; r_{\\min}, 1-r_{\\min}), R_{\\mathrm{val}}(t; r_{\\max}, 1-r_{\\max}) \\right)\n$$\nWe can then find $t_{\\mathrm{robust}}$ by searching for the threshold $t$ that minimizes this maximum value.\n\n### Step 2: Algorithmic Implementation\n\n**2.1. Threshold Candidacy and Search**\nThe empirical risk $R_D(t)$ is a step function that only changes value at thresholds $t$ equal to the probability scores $p_i$ in the dataset. Therefore, to find the exact minimum, it is sufficient to evaluate the cost at a discrete set of candidate thresholds. The problem specifies this set to be the unique probability scores present in the data, augmented with $t=0$ (classify all as positive) and $t > 1$ (e.g., $1+\\varepsilon$, classify all as negative). We will use $\\{0.0\\} \\cup \\{p_i\\}_{\\text{unique}} \\cup \\{1.0+\\varepsilon\\}$. By iterating through these sorted candidates, we can find the minimum cost. The specified tie-breaking rule is to select the smallest threshold that yields the minimum cost. This is achieved by iterating through the sorted candidates and only updating the best threshold upon finding a strictly lower cost.\n\n**2.2. Calculation of Quantities**\n1.  **$t_{\\mathrm{val}}$ and $t_{\\mathrm{test}}$**: We will implement a function that takes a dataset (probabilities and labels) and costs ($C_{\\mathrm{FP}}$, $C_{\\mathrm{FN}}$), and performs the search described in section 2.1 to find the optimal threshold by minimizing $R_D(t)$.\n2.  **$t_{\\mathrm{Bayes}}$**: This is calculated directly using the formula $t_{\\mathrm{Bayes}} = C_{\\mathrm{FP}}^{\\mathrm{val}} / (C_{\\mathrm{FP}}^{\\mathrm{val}} + C_{\\mathrm{FN}}^{\\mathrm{val}})$.\n3.  **$t_{\\mathrm{robust}}$**: We will implement a function that takes the validation dataset and the interval $[r_{\\min}, r_{\\max}]$. For each candidate threshold $t$, it computes the worst-case risk as the maximum of the risks at $r_{\\min}$ and $r_{\\max}$, and then finds the $t$ that minimizes this worst-case risk, adhering to the tie-breaking rule.\n4.  **$\\Delta_{\\mathrm{test}}$**: This is the performance gap on the test set. It is computed as $\\Delta_{\\mathrm{test}} = R_{\\mathrm{test}}(t_{\\mathrm{val}}; C_{\\mathrm{FP}}^{\\mathrm{test}}, C_{\\mathrm{FN}}^{\\mathrm{test}}) - R_{\\mathrm{test}}(t_{\\mathrm{test}}; C_{\\mathrm{FP}}^{\\mathrm{test}}, C_{\\mathrm{FN}}^{\\mathrm{test}})$. We first find $t_{\\mathrm{val}}$ and $t_{\\mathrm{test}}$, then evaluate the risk function $R_{\\mathrm{test}}$ at both thresholds and find the difference. The second term, $R_{\\mathrm{test}}(t_{\\mathrm{test}}, \\dots)$, is by definition the minimum possible empirical risk on the test set.\n\nThe following Python program implements this logic to solve for the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: balanced, moderate mismatch\n        {\n            \"val_probs\": [0.05, 0.10, 0.15, 0.20, 0.30, 0.40, 0.60, 0.70, 0.80, 0.90],\n            \"val_labels\": [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n            \"test_probs\": [0.02, 0.12, 0.18, 0.25, 0.35, 0.45, 0.55, 0.68, 0.78, 0.88],\n            \"test_labels\": [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n            \"val_costs\": (1, 2),\n            \"test_costs\": (2, 1),\n            \"robust_interval\": (0.3, 0.7),\n        },\n        # Case 2: highly imbalanced, heavy false negative test cost\n        {\n            \"val_probs\": [0.05, 0.07, 0.09, 0.12, 0.18, 0.22, 0.27, 0.35, 0.42, 0.50, 0.62, 0.75],\n            \"val_labels\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n            \"test_probs\": [0.03, 0.08, 0.10, 0.13, 0.19, 0.25, 0.28, 0.33, 0.41, 0.52, 0.60, 0.78],\n            \"test_labels\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n            \"val_costs\": (1, 5),\n            \"test_costs\": (1, 10),\n            \"robust_interval\": (0.1, 0.4),\n        },\n        # Case 3: near-perfect separation, equal costs\n        {\n            \"val_probs\": [0.01, 0.02, 0.05, 0.95, 0.97, 0.99],\n            \"val_labels\": [0, 0, 0, 1, 1, 1],\n            \"test_probs\": [0.02, 0.03, 0.06, 0.94, 0.96, 0.98],\n            \"test_labels\": [0, 0, 0, 1, 1, 1],\n            \"val_costs\": (1, 1),\n            \"test_costs\": (1, 1),\n            \"robust_interval\": (0.5, 0.5),\n        },\n        # Case 4: degenerate identical probabilities, conflicting costs\n        {\n            \"val_probs\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n            \"val_labels\": [0, 0, 1, 1, 0, 1],\n            \"test_probs\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n            \"test_labels\": [0, 0, 1, 1, 0, 1],\n            \"val_costs\": (3, 1),\n            \"test_costs\": (1, 3),\n            \"robust_interval\": (0.2, 0.8),\n        },\n    ]\n\n    all_results_str = []\n    \n    epsilon_threshold = np.nextafter(1.0, 2.0)\n\n    def calculate_cost(t, probs, labels, cfp, cfn):\n        n = len(labels)\n        if n == 0: return 0.0\n        predictions = (probs >= t).astype(int)\n        fp_count = np.sum((predictions == 1)  (labels == 0))\n        fn_count = np.sum((predictions == 0)  (labels == 1))\n        cost = (cfp * fp_count + cfn * fn_count) / n\n        return cost\n\n    def find_optimal_threshold(probs, labels, cfp, cfn):\n        unique_probs = np.unique(probs)\n        candidate_thresholds = np.concatenate(([0.0], unique_probs, [epsilon_threshold]))\n        min_cost = float('inf')\n        best_t = -1.0\n        for t in candidate_thresholds:\n            cost = calculate_cost(t, probs, labels, cfp, cfn)\n            if cost  min_cost:\n                min_cost = cost\n                best_t = t\n        return best_t, min_cost\n\n    def find_robust_threshold(probs, labels, r_min, r_max):\n        n = len(labels)\n        if n == 0: return -1.0\n        unique_probs = np.unique(probs)\n        candidate_thresholds = np.concatenate(([0.0], unique_probs, [epsilon_threshold]))\n        min_worst_case_cost = float('inf')\n        best_t = -1.0\n        for t in candidate_thresholds:\n            predictions = (probs >= t).astype(int)\n            fp_count = np.sum((predictions == 1)  (labels == 0))\n            fn_count = np.sum((predictions == 0)  (labels == 1))\n            fp_rate = fp_count / n\n            fn_rate = fn_count / n\n            cost_at_rmin = r_min * fp_rate + (1 - r_min) * fn_rate\n            cost_at_rmax = r_max * fp_rate + (1 - r_max) * fn_rate\n            worst_case_cost = max(cost_at_rmin, cost_at_rmax)\n            if worst_case_cost  min_worst_case_cost:\n                min_worst_case_cost = worst_case_cost\n                best_t = t\n        return best_t\n\n    for case in test_cases:\n        val_probs = np.array(case[\"val_probs\"])\n        val_labels = np.array(case[\"val_labels\"])\n        test_probs = np.array(case[\"test_probs\"])\n        test_labels = np.array(case[\"test_labels\"])\n        cfp_val, cfn_val = case[\"val_costs\"]\n        cfp_test, cfn_test = case[\"test_costs\"]\n        r_min, r_max = case[\"robust_interval\"]\n\n        t_val, _ = find_optimal_threshold(val_probs, val_labels, cfp_val, cfn_val)\n        t_bayes = cfp_val / (cfp_val + cfn_val) if (cfp_val + cfn_val) > 0 else 0\n        t_test, min_test_cost = find_optimal_threshold(test_probs, test_labels, cfp_test, cfn_test)\n        t_robust = find_robust_threshold(val_probs, val_labels, r_min, r_max)\n        cost_at_t_val_on_test = calculate_cost(t_val, test_probs, test_labels, cfp_test, cfn_test)\n        delta_test = cost_at_t_val_on_test - min_test_cost\n\n        case_results = [t_val, t_bayes, t_test, t_robust, delta_test]\n        \n        case_str = f\"[{','.join(f'{x:.6f}' for x in case_results)}]\"\n        all_results_str.append(case_str)\n\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        }
    ]
}