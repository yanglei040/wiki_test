{
    "hands_on_practices": [
        {
            "introduction": "Standard classification models often treat all misclassifications as equally severe. However, in many real-world applications like species identification or product categorization, classes are organized in a natural hierarchy. This exercise  invites you to explore the paradigm of hierarchy-aware risk by implementing and comparing loss functions that explicitly incorporate this taxonomic structure, demonstrating how tailoring the learning objective can lead to more nuanced and intelligent models. You will see firsthand how penalizing a misclassification to a sibling class differently from a misclassification to a distant cousin impacts model evaluation.",
            "id": "3160884",
            "problem": "Consider a hierarchical classification task in which classes are leaves of a taxonomy tree. Let the taxonomy be a rooted tree with a single root and internal nodes representing categories. The learning paradigm is empirical risk minimization: given an input, a model outputs a vector of real-valued scores (logits), which are transformed into a probability distribution over leaf classes via the Softmax function. The objective is to evaluate how the choice of loss shaping reflects the taxonomy and to compare it to flat Cross-Entropy, thereby analyzing a hierarchy-aware risk.\n\nFundamental base:\n- Empirical risk is defined as the expectation of a loss function over the data distribution. For a single example with true leaf class index $y$ and predicted class probabilities $p(k)$ over leaf classes $k$, the instantaneous risk is the chosen loss $\\ell(y, p)$.\n- Softmax converts logits $z(k)$ into probabilities $p(k) = \\exp(z(k)) \\big/ \\sum_{j} \\exp(z(j))$ using the natural logarithm base $e$.\n- Flat Cross-Entropy (defined at the leaf level) is $\\ell_{\\mathrm{CE}}(y, p) = -\\log p(y)$.\n- A hierarchy-aware expected cost is constructed from a taxonomy distance function $d(y, k)$ between leaf classes, yielding $\\ell_{\\mathrm{HEC}}(y, p) = \\sum_{k} p(k) \\, d(y, k)$.\n- A hierarchy-aware path Cross-Entropy shapes the loss to allocate probability mass along the path from the root to the true leaf. Let $P(n)$ be the probability mass of internal or leaf node $n$ defined as the sum of probabilities of its descendant leaves. Let $\\mathrm{path}(y)$ denote the set of nodes on the unique path from the root to $y$. Excluding the root (whose probability mass is identically $1$), define $\\ell_{\\mathrm{HPCE}}(y, p) = -\\sum_{n \\in \\mathrm{path}(y) \\setminus \\{\\mathrm{root}\\}} \\log P(n)$. This penalizes predictions that assign mass outside the correct branch more severely than those that confuse siblings within the correct branch.\n\nTaxonomy:\n- The taxonomy has a root and two internal nodes, each with two leaves:\n  - Internal node $A$ with leaves $A1$ and $A2$.\n  - Internal node $B$ with leaves $B1$ and $B2$.\n- Indexing of leaves is:\n  - $0 \\leftrightarrow A1$, $1 \\leftrightarrow A2$, $2 \\leftrightarrow B1$, $3 \\leftrightarrow B2$.\n- The taxonomy distance $d(i, j)$ between leaves $i$ and $j$ is the length of the shortest path in the tree between the two leaves (counted in edges). Hence:\n  - $d(i, i) = 0$ for all $i$.\n  - $d(0, 1) = 2$ and $d(2, 3) = 2$ for sibling leaves under the same internal node.\n  - $d(0, 2) = 4$, $d(0, 3) = 4$, $d(1, 2) = 4$, $d(1, 3) = 4$ for leaves under different internal nodes.\n\nTest suite:\nEach test case consists of a pair $(\\text{logits}, y)$ where logits is a list of $4$ real numbers and $y$ is the true leaf index. The Softmax probabilities $p(k)$ must be computed from logits before evaluating the losses. Use the natural logarithm and compute all losses as real numbers. Round each loss to $6$ decimal places.\n\n- Case $1$ (happy path, strong correct leaf): logits $[4, 1, -1, -2]$, $y = 0$.\n- Case $2$ (sibling confusion within $A$): logits $[1, 3.5, -1, -2]$, $y = 0$.\n- Case $3$ (cross-branch confusion into $B$): logits $[-1, -1.5, 3.0, 2.5]$, $y = 0$.\n- Case $4$ (uninformative uniform logits): logits $[0, 0, 0, 0]$, $y = 1$.\n- Case $5$ (extreme wrong branch concentration): logits $[10, -10, -10, -10]$, $y = 3$.\n\nFor each test case, compute:\n- Flat Cross-Entropy $\\ell_{\\mathrm{CE}}(y, p)$.\n- Hierarchy-aware expected cost $\\ell_{\\mathrm{HEC}}(y, p)$.\n- Hierarchy-aware path Cross-Entropy $\\ell_{\\mathrm{HPCE}}(y, p)$.\n\nFinal output format:\nYour program should produce a single line of output containing a comma-separated list of results for all test cases, enclosed in square brackets. Each test case result must itself be a list of three floats in the order $[\\ell_{\\mathrm{CE}}, \\ell_{\\mathrm{HEC}}, \\ell_{\\mathrm{HPCE}}]$, each rounded to $6$ decimal places. The final output must look like:\n$[[\\ell_{\\mathrm{CE}}^{(1)}, \\ell_{\\mathrm{HEC}}^{(1)}, \\ell_{\\mathrm{HPCE}}^{(1)}],[\\ell_{\\mathrm{CE}}^{(2)}, \\ell_{\\mathrm{HEC}}^{(2)}, \\ell_{\\mathrm{HPCE}}^{(2)}],\\dots]$.",
            "solution": "The problem requires the computation and comparison of three distinct loss functions for a hierarchical classification task. The task is defined on a specific tree-structured taxonomy. The three loss functions are the standard Flat Cross-Entropy ($\\ell_{\\mathrm{CE}}$), a Hierarchy-aware Expected Cost ($\\ell_{\\mathrm{HEC}}$), and a Hierarchy-aware Path Cross-Entropy ($\\ell_{\\mathrm{HPCE}}$). The objective is to evaluate these losses for five different scenarios represented by test cases, each consisting of a vector of logits and a true class label.\n\nFirst, we formalize the components of the problem. Given a vector of logits $z = [z(0), z(1), z(2), z(3)]$, the probability distribution over the $4$ leaf classes $p = [p(0), p(1), p(2), p(3)]$ is obtained using the Softmax function:\n$$ p(k) = \\frac{\\exp(z(k))}{\\sum_{j=0}^{3} \\exp(z(j))} \\quad \\text{for } k \\in \\{0, 1, 2, 3\\} $$\nThe natural logarithm, $\\log$, is used throughout the calculations.\n\nThe taxonomy is a rooted tree with two levels of branching. The root has two children, internal nodes $A$ and $B$. Node $A$ has two leaf children, $A1$ (index $0$) and $A2$ (index $1$). Node $B$ has two leaf children, $B1$ (index $2$) and $B2$ (index $3$).\n\nThe three loss functions are defined as follows for a given true leaf index $y$ and predicted probability distribution $p$:\n\n$1$. **Flat Cross-Entropy ($\\ell_{\\mathrm{CE}}$)**: This is the negative log-likelihood of the true class. It treats all classes as independent and does not consider the hierarchy.\n$$ \\ell_{\\mathrm{CE}}(y, p) = -\\log p(y) $$\n\n$2$. **Hierarchy-aware Expected Cost ($\\ell_{\\mathrm{HEC}}$)**: This loss function defines the risk as the expected distance between the true class and the predicted class, where the expectation is taken over the model's predicted probability distribution.\n$$ \\ell_{\\mathrm{HEC}}(y, p) = \\sum_{k=0}^{3} p(k) \\, d(y, k) $$\nThe distance $d(i, j)$ is the number of edges on the shortest path between leaf $i$ and leaf $j$. For our taxonomy, the distance matrix $D$ where $D_{ij}=d(i,j)$ is:\n$$\nD = \\begin{pmatrix}\n0 & 2 & 4 & 4 \\\\\n2 & 0 & 4 & 4 \\\\\n4 & 4 & 0 & 2 \\\\\n4 & 4 & 2 & 0\n\\end{pmatrix}\n$$\nThis loss penalizes misclassifications based on their distance in the hierarchy, with sibling confusions (e.g., $A1$ vs. $A2$, distance $2$) penalized less severely than cross-branch confusions (e.g., $A1$ vs. $B1$, distance $4$).\n\n$3$. **Hierarchy-aware Path Cross-Entropy ($\\ell_{\\mathrm{HPCE}}$)**: This loss encourages the model to assign probability mass along the correct path from the root to the true leaf. It is computed by summing the negative log probabilities of all nodes on this path (excluding the root).\n$$ \\ell_{\\mathrm{HPCE}}(y, p) = -\\sum_{n \\in \\mathrm{path}(y) \\setminus \\{\\mathrm{root}\\}} \\log P(n) $$\n$P(n)$ is the total probability mass of a node $n$, calculated by summing the probabilities of all leaf descendants of $n$. For our taxonomy:\n-   Probability of internal node $A$: $P(A) = p(0) + p(1)$.\n-   Probability of internal node $B$: $P(B) = p(2) + p(3)$.\n-   The probability of a leaf node, e.g., $P(A1)$, is simply its own probability, $p(0)$.\n\nThe specific formulas for $\\ell_{\\mathrm{HPCE}}$ for each true class $y$ are:\n-   If $y=0$ (leaf $A1$): $\\mathrm{path}(0) = \\{\\text{root}, A, A1\\}$.\n    $\\ell_{\\mathrm{HPCE}}(0, p) = -[\\log P(A) + \\log P(A1)] = -[\\log(p(0)+p(1)) + \\log p(0)]$.\n-   If $y=1$ (leaf $A2$): $\\mathrm{path}(1) = \\{\\text{root}, A, A2\\}$.\n    $\\ell_{\\mathrm{HPCE}}(1, p) = -[\\log P(A) + \\log P(A2)] = -[\\log(p(0)+p(1)) + \\log p(1)]$.\n-   If $y=2$ (leaf $B1$): $\\mathrm{path}(2) = \\{\\text{root}, B, B1\\}$.\n    $\\ell_{\\mathrm{HPCE}}(2, p) = -[\\log P(B) + \\log P(B1)] = -[\\log(p(2)+p(3)) + \\log p(2)]$.\n-   If $y=3$ (leaf $B2$): $\\mathrm{path}(3) = \\{\\text{root}, B, B2\\}$.\n    $\\ell_{\\mathrm{HPCE}}(3, p) = -[\\log P(B) + \\log P(B2)] = -[\\log(p(2)+p(3)) + \\log p(3)]$.\n\nLet's walk through the calculation for Case $1$: logits $[4, 1, -1, -2]$, $y = 0$.\n-   **Step 1: Compute Probabilities.**\n    The exponents of the logits are $[\\exp(4), \\exp(1), \\exp(-1), \\exp(-2)] \\approx [54.598, 2.718, 0.368, 0.135]$.\n    The sum is $\\approx 57.820$.\n    The probability vector is $p \\approx [54.598/57.820, 2.718/57.820, 0.368/57.820, 0.135/57.820] \\approx [0.944285, 0.047012, 0.006363, 0.002341]$.\n-   **Step 2: Compute Losses.**\n    -   $\\ell_{\\mathrm{CE}}(0, p) = -\\log(p(0)) = -\\log(0.944285) \\approx 0.057310$.\n    -   $\\ell_{\\mathrm{HEC}}(0, p) = \\sum_k p(k) d(0, k) = p(0) \\cdot 0 + p(1) \\cdot 2 + p(2) \\cdot 4 + p(3) \\cdot 4 \\approx (0.047012 \\cdot 2) + (0.006363 \\cdot 4) + (0.002341 \\cdot 4) \\approx 0.094024 + 0.025452 + 0.009364 \\approx 0.128840$.\n    -   $\\ell_{\\mathrm{HPCE}}(0, p) = -[\\log(p(0)+p(1)) + \\log(p(0))] = -[\\log(0.944285+0.047012) + \\log(0.944285)] = -[\\log(0.991297) - 0.057310] \\approx -[-0.008745 - 0.057310] \\approx 0.066055$.\n\nThe results demonstrate the properties of each loss. For instance, in Case $2$ (sibling confusion), $\\ell_{\\mathrm{HEC}}$ is significantly lower than in Case $3$ (cross-branch confusion), reflecting the smaller taxonomic distance of the error. $\\ell_{\\mathrm{HPCE}}$ is more forgiving than $\\ell_{\\mathrm{CE}}$ in Case $2$ because most probability is kept within the correct superclass, but it is high in Case $3$ where probability mass has leaked to a completely different branch of the taxonomy. The following Python code systematically applies this methodology to all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes three different loss functions for hierarchical classification\n    based on a defined taxonomy and a set of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ([4, 1, -1, -2], 0),   # Case 1: Happy path\n        ([1, 3.5, -1, -2], 0), # Case 2: Sibling confusion\n        ([-1, -1.5, 3.0, 2.5], 0), # Case 3: Cross-branch confusion\n        ([0, 0, 0, 0], 1),      # Case 4: Uninformative uniform\n        ([10, -10, -10, -10], 3)    # Case 5: Extreme wrong branch\n    ]\n\n    # Taxonomy distance matrix d(i, j)\n    # Leaves: 0=A1, 1=A2, 2=B1, 3=B2\n    # d(i,i)=0\n    # d(0,1)=2, d(2,3)=2\n    # d(0,2)=4, d(0,3)=4, d(1,2)=4, d(1,3)=4\n    distance_matrix = np.array([\n        [0, 2, 4, 4],\n        [2, 0, 4, 4],\n        [4, 4, 0, 2],\n        [4, 4, 2, 0]\n    ])\n\n    results = []\n    \n    for logits, y in test_cases:\n        # Convert logits to numpy array for vectorized operations\n        z = np.array(logits, dtype=np.float64)\n\n        # Compute probabilities using the Softmax function\n        # A numerically stable version of Softmax is used: exp(z - max(z))\n        exps = np.exp(z - np.max(z))\n        p = exps / np.sum(exps)\n\n        # --- Loss Calculation ---\n\n        # 1. Flat Cross-Entropy (l_ce)\n        l_ce = -np.log(p[y])\n\n        # 2. Hierarchy-aware Expected Cost (l_hec)\n        l_hec = np.sum(p * distance_matrix[y])\n\n        # 3. Hierarchy-aware Path Cross-Entropy (l_hpce)\n        # Taxonomy: root -> {A, B}; A -> {0, 1}; B -> {2, 3}\n        if y in [0, 1]:  # True class is in group A\n            # P(A) = p(0) + p(1)\n            p_group = p[0] + p[1]\n            # l_hpce = -(log(P(A)) + log(P(leaf)))\n            l_hpce = -(np.log(p_group) + np.log(p[y]))\n        else:  # True class is in group B (y in [2, 3])\n            # P(B) = p(2) + p(3)\n            p_group = p[2] + p[3]\n            # l_hpce = -(log(P(B)) + log(P(leaf)))\n            l_hpce = -(np.log(p_group) + np.log(p[y]))\n            \n        # Round results to 6 decimal places as required\n        case_result = [\n            round(l_ce, 6),\n            round(l_hec, 6),\n            round(l_hpce, 6)\n        ]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) correctly formats the inner lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The training dynamics of deep neural networks are notoriously complex and non-linear. The Neural Tangent Kernel (NTK) offers a powerful theoretical paradigm that linearizes these dynamics, providing a bridge to the well-understood world of kernel methods, especially in the \"infinite-width\" limit. In this practice , you will implement both the true gradient descent updates for a neural network and their NTK approximation, allowing you to empirically verify when and why a complex, non-linear model behaves like a simple linear model during training.",
            "id": "3160899",
            "problem": "You are asked to solidify the connection between parameter-space gradient descent on a two-layer neural network and its function-space linearization governed by the Neural Tangent Kernel (NTK). Work within supervised regression in a purely mathematical and algorithmic setting. Your program must implement both the actual training dynamics and the linearized NTK approximation, compare their training loss trajectories, and output a quantitative measure of their discrepancy across a small test suite.\n\nStart from the following fundamental base:\n- The supervised learning empirical risk with mean squared error: for predictions $\\mathbf{p} \\in \\mathbb{R}^{n}$ and targets $\\mathbf{y} \\in \\mathbb{R}^{n}$, the empirical loss is $L(\\boldsymbol{\\theta}) = \\frac{1}{2n} \\lVert \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{X}) - \\mathbf{y} \\rVert_2^2$.\n- Full-batch gradient descent in parameter space: $\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t - \\eta \\nabla_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta}_t)$, where $\\eta$ is the learning rate.\n- The Jacobian $\\mathbf{J}_0 = \\left.\\frac{\\partial \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{X})}{\\partial \\boldsymbol{\\theta}}\\right|_{\\boldsymbol{\\theta}=\\boldsymbol{\\theta}_0}$ evaluated at initialization $\\boldsymbol{\\theta}_0$, and the empirical Neural Tangent Kernel (NTK) defined as $\\mathbf{K} = \\mathbf{J}_0 \\mathbf{J}_0^\\top$.\n- The linearization (first-order Taylor expansion) of the model outputs around initialization: $\\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{X}) \\approx \\mathbf{f}_{\\boldsymbol{\\theta}_0}(\\mathbf{X}) + \\mathbf{J}_0 (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0)$.\n\nProblem specification:\n- Data generation:\n  - Fix integers $n = 20$ and $d = 5$. Generate an input matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ with entries drawn independently from a standard normal distribution $\\mathcal{N}(0,1)$. Use a deterministic random seed $s_{\\text{data}} = 123$ for reproducibility.\n  - Generate a teacher vector $\\mathbf{q} \\in \\mathbb{R}^{d}$ with entries drawn independently from $\\mathcal{N}(0,1)$ using the same seed $s_{\\text{data}}$. Define targets by $y_i = \\tanh(\\mathbf{q}^\\top \\mathbf{x}_i)$ for $i \\in \\{1,\\dots,n\\}$, where $\\mathbf{x}_i$ is the $i$-th row of $\\mathbf{X}$ and $\\tanh(\\cdot)$ is the hyperbolic tangent.\n- Model:\n  - Consider a two-layer neural network with one hidden layer of width $m$ and scalar output,\n    $$f_{\\boldsymbol{\\theta}}(\\mathbf{x}) = \\frac{1}{\\sqrt{m}} \\sum_{j=1}^m a_j \\, \\phi(\\mathbf{w}_j^\\top \\mathbf{x}),$$\n    where $\\phi(z) = \\tanh(z)$, $\\mathbf{w}_j \\in \\mathbb{R}^{d}$ and $a_j \\in \\mathbb{R}$ are trainable parameters, and the parameter vector is $\\boldsymbol{\\theta} = \\{\\mathbf{W} \\in \\mathbb{R}^{m \\times d}, \\mathbf{a} \\in \\mathbb{R}^{m}\\}$. Initialize $\\mathbf{W}$ and $\\mathbf{a}$ with independent standard normal entries using a deterministic seed $s_{\\text{param}} = 10 m$ (so that test cases with the same $m$ share the same initialization).\n- Actual training dynamics:\n  - Use full-batch gradient descent on $L(\\boldsymbol{\\theta}) = \\frac{1}{2n} \\lVert \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{X}) - \\mathbf{y} \\rVert_2^2$ with learning rate $\\eta$ and a specified number of steps $T$. At each step, compute and store the training loss.\n- NTK linearized dynamics:\n  - Starting only from the above fundamental base, derive the discrete-time function-space update that results from linearizing the model around $\\boldsymbol{\\theta}_0$ and applying the parameter-space gradient descent step to the linearized model. Implement this update to produce a predicted sequence of training losses under the linearized dynamics for the same initialization, learning rate $\\eta$, and number of steps $T$.\n  - Compute the empirical NTK $\\mathbf{K}$ at initialization from the Jacobian definition (do not assume any closed-form beyond definitions). Use it to drive the linearized function-space dynamics you derive.\n- Comparison metric:\n  - For a chosen $(m,\\eta,T)$, define the discrepancy between actual and linearized training as\n    $$\\Delta(m,\\eta,T) = \\frac{1}{T+1} \\sum_{t=0}^{T} \\left| L_{\\text{actual}}^{(t)} - L_{\\text{lin}}^{(t)} \\right|.$$\n  - Your program must compute $\\Delta(m,\\eta,T)$ for each test case below and output these values as floating-point numbers.\n\nTest suite:\n- Use the fixed dataset described above for all test cases, and the per-test initialization rule $s_{\\text{param}} = 10 m$.\n- The test cases are the following tuples $(m,\\eta,T)$:\n  1. $(m=\\;512,\\; \\eta=\\;0.1,\\; T=\\;200)$: large width, moderate learning rate, longer training.\n  2. $(m=\\;64,\\; \\eta=\\;0.1,\\; T=\\;200)$: smaller width, same learning rate, longer training.\n  3. $(m=\\;512,\\; \\eta=\\;0.5,\\; T=\\;200)$: large width, larger learning rate, longer training.\n  4. $(m=\\;128,\\; \\eta=\\;0.5,\\; T=\\;5)$: medium width, larger learning rate, very short training.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above, for example: $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$.\n- Each $\\Delta_i$ must be printed as a floating-point number.\n\nScientific realism and derivation requirements:\n- You must base your derivation on the stated fundamental definitions only (empirical risk, gradient descent, Jacobian, linearization) and standard calculus. Do not assume any unproven shortcut formulae in the problem statement.\n- Angles, physical units, or percentage units are not applicable to this problem; no such units are involved.\n- The answer must be a complete, runnable program that produces the required output exactly as specified.",
            "solution": "The user requires a numerical verification of the correspondence between the training dynamics of a two-layer neural network under gradient descent and its linearization described by the Neural Tangent Kernel (NTK). This involves implementing both dynamics and comparing their loss trajectories. The problem is scientifically valid, well-posed, and all necessary parameters are provided.\n\n### Principle-Based Design and Derivations\n\nThe solution is grounded in the following principles and derivations:\n\n1.  **Model and Loss**: The model is a two-layer neural network with a `tanh` activation function, given by:\n    $$f_{\\boldsymbol{\\theta}}(\\mathbf{x}) = \\frac{1}{\\sqrt{m}} \\sum_{j=1}^m a_j \\, \\tanh(\\mathbf{w}_j^\\top \\mathbf{x})$$\n    where the parameters are $\\boldsymbol{\\theta} = \\{\\mathbf{W} \\in \\mathbb{R}^{m \\times d}, \\mathbf{a} \\in \\mathbb{R}^{m}\\}$. For a dataset $(\\mathbf{X}, \\mathbf{y})$ with $n$ samples, the vector of model outputs is $\\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{X}) \\in \\mathbb{R}^n$. The mean squared error loss is:\n    $$L(\\boldsymbol{\\theta}) = \\frac{1}{2n} \\lVert \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{X}) - \\mathbf{y} \\rVert_2^2$$\n\n2.  **Actual Training Dynamics (Parameter-Space Gradient Descent)**: The parameters are updated via full-batch gradient descent:\n    $$\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t - \\eta \\nabla_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta}_t)$$\n    To implement this, we need the gradients of the loss with respect to the parameters $\\mathbf{W}$ and $\\mathbf{a}$. Let $\\mathbf{r}_t = \\mathbf{f}_{\\boldsymbol{\\theta}_t}(\\mathbf{X}) - \\mathbf{y}$ be the residual vector at step $t$. Let $z_{ij}^{(t)} = (\\mathbf{w}_j^{(t)})^\\top \\mathbf{x}_i$. The gradients are derived using the chain rule:\n    \n    -   **Gradient with respect to weights $\\mathbf{a}$**: The gradient vector $\\nabla_{\\mathbf{a}} L \\in \\mathbb{R}^m$ has components:\n        $$\\frac{\\partial L}{\\partial a_k} = \\frac{1}{n} \\sum_{i=1}^n r_i^{(t)} \\frac{\\partial f_i}{\\partial a_k} = \\frac{1}{n} \\sum_{i=1}^n r_i^{(t)} \\left(\\frac{1}{\\sqrt{m}} \\tanh(z_{ik}^{(t)})\\right)$$\n        In matrix form, this is $\\nabla_{\\mathbf{a}} L = \\frac{1}{n\\sqrt{m}} \\mathbf{\\Phi}_{t}^\\top \\mathbf{r}_t$, where $(\\mathbf{\\Phi}_t)_{ik} = \\tanh(z_{ik}^{(t)})$.\n\n    -   **Gradient with respect to weights $\\mathbf{W}$**: The gradient matrix $\\nabla_{\\mathbf{W}} L \\in \\mathbb{R}^{m \\times d}$ has rows $\\frac{\\partial L}{\\partial \\mathbf{w}_k}$:\n        $$\\frac{\\partial L}{\\partial \\mathbf{w}_k} = \\frac{1}{n} \\sum_{i=1}^n r_i^{(t)} \\frac{\\partial f_i}{\\partial \\mathbf{w}_k} = \\frac{1}{n} \\sum_{i=1}^n r_i^{(t)} \\left(\\frac{1}{\\sqrt{m}} a_k^{(t)} (1 - \\tanh^2(z_{ik}^{(t)})) \\mathbf{x}_i^\\top\\right)$$\n        In matrix form, this can be expressed as $\\nabla_{\\mathbf{W}} L = \\frac{1}{n\\sqrt{m}} (\\mathbf{S}_t^\\top \\mathbf{X})$, where the matrix $\\mathbf{S}_t \\in \\mathbb{R}^{n \\times m}$ has entries $(S_t)_{ik} = r_i^{(t)} a_k^{(t)} (1 - \\tanh^2(z_{ik}^{(t)}))$.\n\n3.  **Linearized Dynamics (Function-Space)**: The NTK framework linearizes the model's output around its initial parameters $\\boldsymbol{\\theta}_0$.\n    $$\\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{X}) \\approx \\mathbf{f}_{\\boldsymbol{\\theta}_0}(\\mathbf{X}) + \\mathbf{J}_0 (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0)$$\n    where $\\mathbf{J}_0 = \\left.\\frac{\\partial \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{X})}{\\partial \\boldsymbol{\\theta}}\\right|_{\\boldsymbol{\\theta}=\\boldsymbol{\\theta}_0}$ is the Jacobian at initialization.\n    \n    Applying a parameter-space gradient step to the linearized model's loss yields an update rule in function space. The evolution of the linearized model's output, $\\mathbf{f}_{\\text{lin}, t}$, is given by:\n    $$\\mathbf{f}_{\\text{lin}, t+1} = \\mathbf{f}_{\\text{lin}, t} - \\frac{\\eta}{n} \\mathbf{K} (\\mathbf{f}_{\\text{lin}, t} - \\mathbf{y})$$\n    where $\\mathbf{K} = \\mathbf{J}_0 \\mathbf{J}_0^\\top$ is the empirical Neural Tangent Kernel, an $n \\times n$ matrix. The simulation starts with $\\mathbf{f}_{\\text{lin}, 0} = \\mathbf{f}_{\\boldsymbol{\\theta}_0}(\\mathbf{X})$.\n\n4.  **NTK Computation**: Instead of explicitly forming the large Jacobian $\\mathbf{J}_0$, we can compute $\\mathbf{K}$ directly. The kernel can be decomposed as a sum of contributions from each layer's parameters: $\\mathbf{K} = \\mathbf{K}_{\\mathbf{a}} + \\mathbf{K}_{\\mathbf{W}}$.\n    \n    -   **Kernel from layer $\\mathbf{a}$**:\n        $$(\\mathbf{K}_{\\mathbf{a}})_{ij} = \\sum_{k=1}^m \\frac{\\partial f_i}{\\partial a_k} \\frac{\\partial f_j}{\\partial a_k} = \\frac{1}{m} \\sum_{k=1}^m \\tanh(\\mathbf{w}_k^\\top \\mathbf{x}_i) \\tanh(\\mathbf{w}_k^\\top \\mathbf{x}_j)$$\n        This is evaluated at initialization $\\boldsymbol{\\theta}_0$.\n\n    -   **Kernel from layer $\\mathbf{W}$**:\n        $$(\\mathbf{K}_{\\mathbf{W}})_{ij} = \\sum_{k=1}^m \\sum_{l=1}^d \\frac{\\partial f_i}{\\partial w_{kl}} \\frac{\\partial f_j}{\\partial w_{kl}} = \\left(\\frac{1}{m} \\sum_{k=1}^m a_k^2 (1 - \\tanh^2(\\mathbf{w}_k^\\top \\mathbf{x}_i))(1 - \\tanh^2(\\mathbf{w}_k^\\top \\mathbf{x}_j))\\right)(\\mathbf{x}_i^\\top \\mathbf{x}_j)$$\n        This is also evaluated at initialization $\\boldsymbol{\\theta}_0$.\n\n### Implementation\n\nThe algorithm proceeds as follows for each test case $(m, \\eta, T)$:\n\n1.  **Setup**: Generate data $(\\mathbf{X}, \\mathbf{y})$ using the fixed seed $s_{\\text{data}}$. Initialize model parameters $(\\mathbf{W}_0, \\mathbf{a}_0)$ using the seed $s_{\\text{param}} = 10m$.\n\n2.  **Actual Training**:\n    -   Initialize $(\\mathbf{W}, \\mathbf{a}) = (\\mathbf{W}_0, \\mathbf{a}_0)$.\n    -   Iterate $T$ times:\n        -   Compute model outputs $\\mathbf{f}_{\\boldsymbol{\\theta}_t}(\\mathbf{X})$ and store the loss $L^{(t)}_{\\text{actual}}$.\n        -   Compute gradients $\\nabla_{\\mathbf{a}} L$ and $\\nabla_{\\mathbf{W}} L$.\n        -   Update parameters: $\\mathbf{a}_{t+1} = \\mathbf{a}_t - \\eta \\nabla_{\\mathbf{a}} L$ and $\\mathbf{W}_{t+1} = \\mathbf{W}_t - \\eta \\nabla_{\\mathbf{W}} L$.\n    -   Store the final loss $L^{(T)}_{\\text{actual}}$. This yields a trajectory of $T+1$ loss values.\n\n3.  **Linearized Training**:\n    -   Compute the initial output $\\mathbf{f}_{\\text{lin}, 0} = \\mathbf{f}_{\\boldsymbol{\\theta}_0}(\\mathbf{X})$.\n    -   Compute the NTK matrix $\\mathbf{K} = \\mathbf{K}_{\\mathbf{a}} + \\mathbf{K}_{\\mathbf{W}}$ using the initial parameters $(\\mathbf{W}_0, \\mathbf{a}_0)$.\n    -   Iterate $T$ times:\n        -   Store the loss $L^{(t)}_{\\text{lin}}$ based on the current $\\mathbf{f}_{\\text{lin}, t}$.\n        -   Update the function outputs: $\\mathbf{f}_{\\text{lin}, t+1} = \\mathbf{f}_{\\text{lin}, t} - \\frac{\\eta}{n} \\mathbf{K} (\\mathbf{f}_{\\text{lin}, t} - \\mathbf{y})$.\n    -   Store the final loss $L^{(T)}_{\\text{lin}}$. This also yields a trajectory of $T+1$ loss values.\n\n4.  **Comparison**: Compute the discrepancy metric $\\Delta = \\frac{1}{T+1} \\sum_{t=0}^{T} |L^{(t)}_{\\text{actual}} - L^{(t)}_{\\text{lin}}|$. The final output is a list of these $\\Delta$ values for all test cases.",
            "answer": "```python\nimport numpy as np\n\ndef generate_data(n, d, seed):\n    \"\"\"\n    Generates the input data matrix X and target vector y.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    X = rng.standard_normal((n, d), dtype=np.float64)\n    q = rng.standard_normal((d,), dtype=np.float64)\n    y = np.tanh(X @ q)\n    return X, y\n\ndef initialize_params(m, d, seed):\n    \"\"\"\n    Initializes the model parameters W and a.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    W = rng.standard_normal((m, d), dtype=np.float64)\n    a = rng.standard_normal((m,), dtype=np.float64)\n    return W, a\n\ndef model_forward(X, W, a):\n    \"\"\"\n    Computes the forward pass of the two-layer neural network.\n    \"\"\"\n    m = W.shape[0]\n    sqrt_m = np.sqrt(m)\n    z = X @ W.T\n    phi_z = np.tanh(z)\n    # Broadcasting of a: (m,) becomes (1, m) and applied to each row of phi_z\n    f_X = (1.0 / sqrt_m) * np.sum(a * phi_z, axis=1)\n    return f_X\n\ndef loss_fn(f_X, y):\n    \"\"\"\n    Computes the mean squared error loss.\n    \"\"\"\n    n = len(y)\n    return (1.0 / (2.0 * n)) * np.sum((f_X - y)**2)\n\ndef actual_training(X, y, W0, a0, eta, T):\n    \"\"\"\n    Performs training using standard full-batch gradient descent.\n    \"\"\"\n    n, d = X.shape\n    m = W0.shape[0]\n    sqrt_m = np.sqrt(m)\n    \n    W, a = W0.copy(), a0.copy()\n    \n    losses = []\n    \n    for t in range(T + 1):\n        # Forward pass\n        z = X @ W.T\n        phi_z = np.tanh(z)\n        f_X = (1.0 / sqrt_m) * np.sum(a * phi_z, axis=1)\n        \n        # Compute and store loss\n        current_loss = loss_fn(f_X, y)\n        losses.append(current_loss)\n        \n        if t == T:\n            break\n            \n        # Backward pass (gradients)\n        residual = f_X - y\n        \n        # Gradient w.r.t. a\n        grad_a = (1.0 / (n * sqrt_m)) * (phi_z.T @ residual)\n        \n        # Gradient w.r.t. W\n        phi_prime_z = 1.0 - phi_z**2\n        S = residual[:, np.newaxis] * phi_prime_z * a[np.newaxis, :]\n        grad_W = (1.0 / (n * sqrt_m)) * (S.T @ X)\n        \n        # Parameter update\n        W -= eta * grad_W\n        a -= eta * grad_a\n        \n    return losses\n    \ndef ntk_linearized_training(X, y, W0, a0, eta, T):\n    \"\"\"\n    Simulates training using the linearized NTK dynamics.\n    \"\"\"\n    n, d = X.shape\n    m = W0.shape[0]\n    \n    # Compute NTK at initialization\n    z0 = X @ W0.T\n    phi_z0 = np.tanh(z0)\n    phi_prime_z0 = 1.0 - phi_z0**2\n    \n    # Kernel part from the second layer (a)\n    K_a = (1.0 / m) * (phi_z0 @ phi_z0.T)\n    \n    # Kernel part from the first layer (W)\n    dot_prods_X = X @ X.T\n    K_W_factor = (1.0 / m) * ((phi_prime_z0 * (a0**2)) @ phi_prime_z0.T)\n    K_W = K_W_factor * dot_prods_X\n    \n    K = K_a + K_W\n    \n    # Linearized dynamics simulation\n    f_lin = model_forward(X, W0, a0)\n    losses = []\n    \n    for t in range(T + 1):\n        # Compute and store loss for the current function estimate\n        current_loss = loss_fn(f_lin, y)\n        losses.append(current_loss)\n        \n        if t == T:\n            break\n            \n        # Update function output in function space\n        residual = f_lin - y\n        f_lin -= (eta / n) * (K @ residual)\n        \n    return losses\n\ndef compute_discrepancy(losses_actual, losses_lin):\n    \"\"\"\n    Computes the average absolute difference between two loss trajectories.\n    \"\"\"\n    return np.mean(np.abs(np.array(losses_actual) - np.array(losses_lin)))\n\ndef solve():\n    \"\"\"\n    Main solver function to run the test suite and print results.\n    \"\"\"\n    # Fixed parameters for data generation\n    n = 20\n    d = 5\n    s_data = 123\n    \n    # Generate data once for all test cases\n    X, y = generate_data(n, d, s_data)\n\n    # Test suite from the problem statement\n    test_cases = [\n        (512, 0.1, 200),\n        (64, 0.1, 200),\n        (512, 0.5, 200),\n        (128, 0.5, 5)\n    ]\n\n    results = []\n    for m, eta, T in test_cases:\n        # Per-test-case parameter initialization seed\n        s_param = 10 * m\n        W0, a0 = initialize_params(m, d, s_param)\n        \n        # Run both actual and linearized training\n        losses_actual = actual_training(X, y, W0, a0, eta, T)\n        losses_lin = ntk_linearized_training(X, y, W0, a0, eta, T)\n        \n        # Compute and store the discrepancy\n        discrepancy = compute_discrepancy(losses_actual, losses_lin)\n        results.append(discrepancy)\n\n    # Print the final results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Generative modeling seeks to capture the underlying distribution of data, with autoregressive and diffusion models representing two of the most successful modern paradigms. Autoregressive models build data sequentially using the chain rule of probability, yielding tractable likelihoods, while diffusion models learn to reverse a gradual noising process, often achieving superior sample quality. This exercise  places these two paradigms head-to-head on a tractable sequence modeling task, challenging you to compare their performance in terms of both statistical likelihood and distributional similarity to the ground truth.",
            "id": "3160960",
            "problem": "You are to implement and study two distinct machine learning paradigms on a controlled, analytically tractable sequence prediction task: an autoregressive model with conditional density $p_{\\theta}(x_t \\mid x_{<t})$ and a denoising-diffusion-inspired generator that iterates a linear denoiser. The goal is to compute and compare, under limited compute, their test log-likelihoods and their sample quality, using a toy data distribution where both calculations can be made precise. Your program must produce a single line of output that aggregates all test-case results into one list.\n\nThe data distribution is a stationary linear Gaussian autoregressive process of order one. A length-$T$ sequence $(x_1,\\dots,x_T)$ is generated by $x_1 \\sim \\mathcal{N}(0, \\sigma_x^2)$ with $\\sigma_x^2 = \\sigma_{\\varepsilon}^2/(1-a^2)$ and, for $t \\in \\{2,\\dots,T\\}$,\n$$\nx_t = a\\,x_{t-1} + \\varepsilon_t,\\quad \\varepsilon_t \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma_{\\varepsilon}^2),\n$$\nwith $|a|<1$. This induces a zero-mean Gaussian distribution on $\\mathbb{R}^T$ with covariance $\\Sigma_{\\text{true}} \\in \\mathbb{R}^{T \\times T}$, where $\\Sigma_{\\text{true}}[i,j] = \\sigma_x^2\\,a^{|i-j|}$.\n\nYou must implement two models and evaluate them as follows.\n\n- Autoregressive model:\n  1. Training under limited compute: Estimate a single scalar coefficient $\\hat{a}$ by ridge-regularized least squares using all available training pairs $(x_{t-1},x_t)$ pooled across sequences and time steps $t \\in \\{2,\\dots,T\\}$. The ridge regularization hyperparameter is $\\lambda \\ge 0$. Estimate the conditional noise variance $\\hat{\\sigma}_{\\varepsilon}^2$ by the mean squared residual under the fitted $\\hat{a}$. Treat the prior for $x_1$ as $\\mathcal{N}(0,\\hat{\\sigma}_x^2)$ with $\\hat{\\sigma}_x^2 = \\hat{\\sigma}_{\\varepsilon}^2/(1-\\hat{a}^2)$, and if $|\\hat{a}| \\ge 1$ clip it to $0.999$ before any use that requires stationarity.\n  2. Test log-likelihood: For a fresh evaluation set of sequences from the true process, compute the average negative log-likelihood per dimension under the fitted autoregressive model using the chain rule of probability, that is, \n     $$\n     -\\frac{1}{T}\\,\\frac{1}{N_{\\text{eval}}}\\sum_{n=1}^{N_{\\text{eval}}} \\left[\\log p_{\\hat{\\theta}}(x_1^{(n)}) + \\sum_{t=2}^T \\log p_{\\hat{\\theta}}(x_t^{(n)} \\mid x_{t-1}^{(n)})\\right],\n     $$\n     where $p_{\\hat{\\theta}}$ is Gaussian with the fitted parameters.\n  3. Sample quality: Form the model-implied stationary covariance $\\Sigma_{\\text{AR}}$ from $(\\hat{a},\\hat{\\sigma}_{\\varepsilon})$ with entries $\\Sigma_{\\text{AR}}[i,j] = \\hat{\\sigma}_x^2\\,\\hat{a}^{|i-j|}$, and compute the $2$-Wasserstein distance between zero-mean Gaussians with covariances $\\Sigma_{\\text{true}}$ and $\\Sigma_{\\text{AR}}$, normalized per dimension as \n     $$\n     W_{\\text{AR}} = \\sqrt{\\frac{1}{T}\\,\\mathrm{tr}\\!\\left(\\Sigma_{\\text{true}} + \\Sigma_{\\text{AR}} - 2\\left(\\Sigma_{\\text{true}}^{1/2}\\,\\Sigma_{\\text{AR}}\\,\\Sigma_{\\text{true}}^{1/2}\\right)^{1/2}\\right)}.\n     $$\n\n- Denoising diffusion-inspired generator:\n  1. Limited-compute denoiser: Using only the empirical per-dimension variances from the training set, construct a diagonal linear denoiser that, for a fixed scalar noise schedule parameter $\\beta \\in (0,1)$ and one-step forward noising $y = \\sqrt{1-\\beta}\\,x + \\sqrt{\\beta}\\,\\eta$ with $\\eta \\sim \\mathcal{N}(0,I)$, predicts $\\hat{x} = D y$ where $D=\\mathrm{diag}(\\alpha_1,\\dots,\\alpha_T)$ and \n     $$\n     \\alpha_i = \\frac{\\sqrt{1-\\beta}\\,v_i}{(1-\\beta)\\,v_i + \\beta},\n     $$\n     with $v_i$ the empirical variance of coordinate $i$ across the training set. This is the optimal linear least-squares denoiser under the diagonal covariance approximation.\n  2. Induced stationary model: Consider the iterative generator $x_{k+1} = D\\left(\\sqrt{1-\\beta}\\,x_k + \\sqrt{\\beta}\\,\\eta_k\\right)$ with $\\eta_k \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,I)$. This is a linear Gaussian state-space model whose unique stationary distribution is $\\mathcal{N}(0,\\Sigma_{\\text{diff}})$ with diagonal covariance $\\Sigma_{\\text{diff}}=\\mathrm{diag}(s_1,\\dots,s_T)$ satisfying, for each coordinate,\n     $$\n     s_i = \\frac{\\alpha_i^2\\,\\beta}{1 - \\left(\\sqrt{1-\\beta}\\,\\alpha_i\\right)^2}.\n     $$\n  3. Test log-likelihood surrogate: Evaluate the average negative log-likelihood per dimension on the evaluation set under the diagonal Gaussian $\\mathcal{N}(0,\\Sigma_{\\text{diff}})$, namely \n     $$\n     -\\frac{1}{T}\\,\\frac{1}{N_{\\text{eval}}}\\sum_{n=1}^{N_{\\text{eval}}}\\log \\mathcal{N}(x^{(n)};\\,0,\\Sigma_{\\text{diff}}),\n     $$\n     which here is tractable in closed form due to diagonality.\n  4. Sample quality: Compute the per-dimension normalized $2$-Wasserstein distance \n     $$\n     W_{\\text{diff}} = \\sqrt{\\frac{1}{T}\\,\\mathrm{tr}\\!\\left(\\Sigma_{\\text{true}} + \\Sigma_{\\text{diff}} - 2\\left(\\Sigma_{\\text{true}}^{1/2}\\,\\Sigma_{\\text{diff}}\\,\\Sigma_{\\text{true}}^{1/2}\\right)^{1/2}\\right)}.\n     $$\n\nFoundational base you must rely on:\n- Chain rule of probability for autoregressive factorization.\n- Properties of multivariate Gaussian distributions, including explicit forms of log-densities and linear minimum mean squared error estimation.\n- For $2$-Wasserstein distance between zero-mean Gaussians with covariances $\\Sigma_1$ and $\\Sigma_2$, the closed-form expression \n  $$\n  W_2^2(\\Sigma_1,\\Sigma_2) = \\mathrm{tr}\\!\\left(\\Sigma_1 + \\Sigma_2 - 2\\left(\\Sigma_1^{1/2}\\,\\Sigma_2\\,\\Sigma_1^{1/2}\\right)^{1/2}\\right).\n  $$\n\nTest suite:\n- Case $1$ (happy path): $a=0.7$, $\\sigma_{\\varepsilon}=0.6$, $T=6$, $N_{\\text{train}}=256$, $N_{\\text{eval}}=8192$, $\\lambda=0.0$, $\\beta=0.2$, $\\text{seed}=0$.\n- Case $2$ (low data, stronger regularization): $a=0.9$, $\\sigma_{\\varepsilon}=0.4$, $T=6$, $N_{\\text{train}}=16$, $N_{\\text{eval}}=8192$, $\\lambda=5.0$, $\\beta=0.2$, $\\text{seed}=1$.\n- Case $3$ (negative correlation): $a=-0.8$, $\\sigma_{\\varepsilon}=0.5$, $T=6$, $N_{\\text{train}}=128$, $N_{\\text{eval}}=8192$, $\\lambda=0.5$, $\\beta=0.2$, $\\text{seed}=2$.\n\nImplementation requirements:\n- For each case, generate training and evaluation data exactly from the specified true process using the given seed. Fit the autoregressive model with ridge penalty $\\lambda$ and construct the diagonal denoiser using only per-dimension training variances. Compute:\n  1. Autoregressive average negative log-likelihood per dimension on the evaluation set.\n  2. Diffusion-model average negative log-likelihood per dimension on the evaluation set under $\\mathcal{N}(0,\\Sigma_{\\text{diff}})$.\n  3. Per-dimension normalized $2$-Wasserstein distance $W_{\\text{AR}}$ between $\\Sigma_{\\text{true}}$ and $\\Sigma_{\\text{AR}}$.\n  4. Per-dimension normalized $2$-Wasserstein distance $W_{\\text{diff}}$ between $\\Sigma_{\\text{true}}$ and $\\Sigma_{\\text{diff}}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case in order $1,2,3$, append the four floats in the following order:\n  $[\\text{AR\\_NLL\\_per\\_dim}, \\text{DIFF\\_NLL\\_per\\_dim}, \\text{W2\\_AR\\_per\\_dim}, \\text{W2\\_DIFF\\_per\\_dim}]$.\n- The final output must therefore contain $12$ floats: $4$ per test case, flattened into a single list.",
            "solution": "We articulate the derivation and algorithm from first principles, using only foundational definitions from probability and multivariate Gaussian theory, and then explain the implementation choices that reflect limited compute constraints.\n\nData-generating process and its covariance. The sequence model is a stationary linear Gaussian autoregressive process of order one, defined by $x_1 \\sim \\mathcal{N}(0,\\sigma_x^2)$ with $\\sigma_x^2 = \\sigma_{\\varepsilon}^2/(1-a^2)$ and $x_t = a\\,x_{t-1} + \\varepsilon_t$ for $t \\in \\{2,\\dots,T\\}$ with $\\varepsilon_t \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma_{\\varepsilon}^2)$. This is a Gaussian Markov chain. By the properties of stationary autoregressive processes, the induced joint distribution over $(x_1,\\dots,x_T)$ is zero-mean Gaussian with Toeplitz covariance $\\Sigma_{\\text{true}}$ given by $\\Sigma_{\\text{true}}[i,j] = \\sigma_x^2\\,a^{|i-j|}$. This follows because $\\mathrm{Cov}(x_t,x_{t+k}) = \\sigma_x^2\\,a^{|k|}$ for all $k$ under stationarity.\n\nAutoregressive model: estimation and evaluation. The autoregressive paradigm uses the chain rule of probability to factor the joint density as $p_{\\theta}(x_1,\\dots,x_T) = p_{\\theta}(x_1)\\prod_{t=2}^T p_{\\theta}(x_t \\mid x_{t-1})$, where, under a linear Gaussian hypothesis class, $p_{\\theta}(x_t \\mid x_{t-1}) = \\mathcal{N}(x_t; a\\,x_{t-1},\\sigma_{\\varepsilon}^2)$ and $p_{\\theta}(x_1) = \\mathcal{N}(x_1;0,\\sigma_{\\varepsilon}^2/(1-a^2))$. Given pooled training pairs $(x_{t-1},x_t)$ across all sequences and $t \\in \\{2,\\dots,T\\}$, we estimate $a$ by ridge-regularized least squares, minimizing the empirical risk plus penalty\n$$\n\\min_{a \\in \\mathbb{R}} \\sum_{n,t} (x_t^{(n)} - a\\,x_{t-1}^{(n)})^2 + \\lambda a^2,\n$$\nwhose first-order optimality condition yields the closed-form estimator\n$$\n\\hat{a} = \\frac{\\sum_{n,t} x_{t-1}^{(n)} x_t^{(n)}}{\\sum_{n,t} (x_{t-1}^{(n)})^2 + \\lambda}.\n$$\nThe conditional noise variance is estimated by the mean squared residual\n$$\n\\hat{\\sigma}_{\\varepsilon}^2 = \\frac{1}{N_{\\text{pairs}}}\\sum_{n,t} \\left(x_t^{(n)} - \\hat{a}\\,x_{t-1}^{(n)}\\right)^2,\n$$\nwith $N_{\\text{pairs}}$ the number of pooled $(x_{t-1},x_t)$ pairs. The marginal variance for $x_1$ implied by the fitted model is $\\hat{\\sigma}_x^2 = \\hat{\\sigma}_{\\varepsilon}^2/(1-\\hat{a}^2)$ provided $|\\hat{a}|<1$; to ensure a valid stationary prior under finite data, we clip $|\\hat{a}|$ below $1$ (for example to $0.999$) where needed.\n\nThe average negative log-likelihood per dimension on the evaluation set follows from the Gaussian log-density formula. For a single sequence, the log-likelihood under the fitted autoregressive model is\n$$\n\\log p_{\\hat{\\theta}}(x_1,\\dots,x_T) = \\log \\mathcal{N}(x_1;0,\\hat{\\sigma}_x^2) + \\sum_{t=2}^T \\log \\mathcal{N}(x_t;\\hat{a}\\,x_{t-1},\\hat{\\sigma}_{\\varepsilon}^2).\n$$\nAveraging across the evaluation set and dividing by $T$ yields the per-dimension average negative log-likelihood\n$$\n\\text{AR\\_NLL\\_per\\_dim} = -\\frac{1}{T}\\,\\frac{1}{N_{\\text{eval}}} \\sum_{n=1}^{N_{\\text{eval}}} \\log p_{\\hat{\\theta}}(x^{(n)}).\n$$\n\nFor sample quality, we convert the fitted parameters into the implied stationary covariance of the autoregressive model, which is Toeplitz with entries $\\Sigma_{\\text{AR}}[i,j] = \\hat{\\sigma}_x^2 \\hat{a}^{|i-j|}$. A principled geometry-based notion of sample quality for Gaussian distributions is the $2$-Wasserstein distance between the true and model Gaussians, which has the closed-form\n$$\nW_2^2(\\Sigma_{\\text{true}},\\Sigma_{\\text{AR}}) = \\mathrm{tr}\\!\\left(\\Sigma_{\\text{true}} + \\Sigma_{\\text{AR}} - 2\\left(\\Sigma_{\\text{true}}^{1/2}\\,\\Sigma_{\\text{AR}}\\,\\Sigma_{\\text{true}}^{1/2}\\right)^{1/2}\\right).\n$$\nWe report the per-dimension normalized distance $W_{\\text{AR}} = \\sqrt{W_2^2(\\Sigma_{\\text{true}},\\Sigma_{\\text{AR}})/T}$.\n\nDenoising diffusion-inspired generator: linear denoiser and its implied model. The denoising diffusion paradigm models data via a learned denoiser for a noised version of the data. We consider a one-step forward noising $y = \\sqrt{1-\\beta}\\,x + \\sqrt{\\beta}\\,\\eta$ with $\\eta \\sim \\mathcal{N}(0,I)$ and a diagonal linear denoiser $\\hat{x} = D y$ with $D=\\mathrm{diag}(\\alpha_1,\\dots,\\alpha_T)$ estimated under a diagonal covariance approximation using only the empirical per-dimension training variances $v_i$. The linear minimum mean squared error estimator from $y$ to $x$ is $D^* = \\mathrm{Cov}(x,y)\\,\\mathrm{Cov}(y)^{-1}$. Since $y$ and $x$ are jointly Gaussian with $\\mathrm{Cov}(x,y) = \\sqrt{1-\\beta}\\,\\mathrm{Cov}(x)$ and $\\mathrm{Cov}(y) = (1-\\beta)\\,\\mathrm{Cov}(x) + \\beta I$, the diagonal approximation replaces $\\mathrm{Cov}(x)$ by $\\mathrm{diag}(v_1,\\dots,v_T)$, yielding\n$$\n\\alpha_i = \\frac{\\sqrt{1-\\beta}\\,v_i}{(1-\\beta)\\,v_i + \\beta}.\n$$\nThis denoiser defines an iterative generator $x_{k+1} = D\\left(\\sqrt{1-\\beta}\\,x_k + \\sqrt{\\beta}\\,\\eta_k\\right)$ with $\\eta_k \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,I)$. This is a linear Gaussian recursion with transition matrix $C = \\sqrt{1-\\beta}\\,D$ and additive noise covariance $Q = \\beta D^2$. The unique stationary covariance $\\Sigma_{\\text{diff}}$ solves the discrete-time Lyapunov equation $\\Sigma_{\\text{diff}} = C\\,\\Sigma_{\\text{diff}}\\,C^\\top + Q$. Because $C$ and $Q$ are diagonal, the solution is diagonal with entries\n$$\ns_i = \\frac{\\alpha_i^2\\,\\beta}{1 - \\left(\\sqrt{1-\\beta}\\,\\alpha_i\\right)^2}.\n$$\nThis constitutes the implicit likelihood model $\\mathcal{N}(0,\\Sigma_{\\text{diff}})$. The average negative log-likelihood per dimension on the evaluation set is then\n$$\n\\text{DIFF\\_NLL\\_per\\_dim} = -\\frac{1}{T}\\,\\frac{1}{N_{\\text{eval}}}\\sum_{n=1}^{N_{\\text{eval}}} \\log \\mathcal{N}(x^{(n)};\\,0,\\Sigma_{\\text{diff}}),\n$$\nwhich is tractable because $\\Sigma_{\\text{diff}}$ is diagonal. Sample quality is assessed by the same per-dimension normalized $2$-Wasserstein distance,\n$$\nW_{\\text{diff}} = \\sqrt{\\frac{1}{T}\\,\\mathrm{tr}\\!\\left(\\Sigma_{\\text{true}} + \\Sigma_{\\text{diff}} - 2\\left(\\Sigma_{\\text{true}}^{1/2}\\,\\Sigma_{\\text{diff}}\\,\\Sigma_{\\text{true}}^{1/2}\\right)^{1/2}\\right)}.\n$$\n\nAlgorithmic design and limited compute. The limited compute is reflected in two ways. For the autoregressive model, ridge regularization with hyperparameter $\\lambda$ biases parameter estimates when $N_{\\text{train}}$ is small, and a single global scalar $\\hat{a}$ is fitted via a single closed-form pass, without iterative refinement. For the denoising model, only per-dimension variances $v_i$ are used to construct a diagonal denoiser, thereby ignoring temporal correlations; the resulting stationary covariance is constrained to be diagonal and cannot match the true Toeplitz covariance when $a \\ne 0$. The generator uses a single noise level $\\beta$ and the implicit stationary law for evaluation, rather than a multi-step time-inhomogeneous diffusion, which keeps computation minimal.\n\nMetrics computation. The Gaussian log-density in dimension $T$ for zero mean and covariance $\\Sigma$ is\n$$\n\\log \\mathcal{N}(x;0,\\Sigma) = -\\frac{1}{2}\\left(x^\\top \\Sigma^{-1} x + \\log \\det (2\\pi \\Sigma)\\right).\n$$\nFor the autoregressive model, the chain rule decomposed Gaussian conditionals yield a sum of univariate Gaussian log-densities. For Wasserstein distance, we use the closed form for zero-mean Gaussians that involves matrix square roots; numerical square roots are computed via a standard principal square-root algorithm, and we take the real part to mitigate tiny numerical imaginary components.\n\nTest suite and outputs. For each of the three specified cases, we generate $N_{\\text{train}}$ training sequences and $N_{\\text{eval}}$ evaluation sequences from the true process with the given $(a,\\sigma_{\\varepsilon},T)$ and seed. We compute:\n- $\\text{AR\\_NLL\\_per\\_dim}$,\n- $\\text{DIFF\\_NLL\\_per\\_dim}$,\n- $W_{\\text{AR}}$,\n- $W_{\\text{diff}}$,\nand print them in order for case $1$, then case $2$, then case $3$, as a single flattened list enclosed in square brackets.\n\nParadigm tradeoffs, as reflected by the metrics. The autoregressive model directly models the conditional likelihood and thus tends to achieve superior test log-likelihood when its hypothesis class is well aligned with the data (here both are linear Gaussian and Markov of order one), even under modest regularization. Its implied sample quality (as a distributional match to the true covariance) depends on parameter estimation accuracy. The denoising diffusion-inspired model, restricted to a diagonal denoiser learned only from per-dimension variances, often yields poorer likelihood because it ignores correlations, yet it can produce a stationary distribution with reasonable marginal variances; its sample quality as measured by the $2$-Wasserstein distance degrades when temporal correlations are strong. Under very limited data ($N_{\\text{train}}$ small) with stronger regularization, autoregressive estimates can be biased, increasing $W_{\\text{AR}}$ and the negative log-likelihood, while the denoiserâ€™s diagonal nature may yield more stable but fundamentally limited performance that cannot capture off-diagonal structure. These tradeoffs are quantified by the requested outputs.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import sqrtm\n\ndef ar1_generate_sequences(a, sigma_eps, T, N, rng):\n    var_x = sigma_eps**2 / (1.0 - a**2)\n    x = np.zeros((N, T))\n    x[:, 0] = rng.normal(loc=0.0, scale=np.sqrt(var_x), size=N)\n    for t in range(1, T):\n        eps = rng.normal(loc=0.0, scale=sigma_eps, size=N)\n        x[:, t] = a * x[:, t - 1] + eps\n    return x\n\ndef estimate_ar_ridge(seqs, lam):\n    # Pool all pairs (x_{t-1}, x_t) across sequences\n    x_prev = seqs[:, :-1].ravel()\n    x_curr = seqs[:, 1:].ravel()\n    denom = (x_prev @ x_prev) + lam\n    num = x_prev @ x_curr\n    a_hat = num / denom if denom != 0.0 else 0.0\n    resid = x_curr - a_hat * x_prev\n    sigma_eps_hat2 = float(np.mean(resid ** 2))\n    return a_hat, sigma_eps_hat2\n\ndef stationary_cov_ar(a, sigma_eps, T):\n    # Ensure stability\n    a_eff = np.clip(a, -0.999, 0.999)\n    var_x = sigma_eps**2 / (1.0 - a_eff**2)\n    # Toeplitz covariance with entries var_x * a^{|i-j|}\n    idx = np.arange(T)\n    power = np.abs(idx[:, None] - idx[None, :])\n    Sigma = var_x * (a_eff ** power)\n    return Sigma\n\ndef wasserstein_gaussian_per_dim(Sigma1, Sigma2):\n    # Compute W2 distance between zero-mean Gaussians with covariances Sigma1 and Sigma2\n    # W2^2 = tr(S1 + S2 - 2 * sqrt( sqrt(S1) * S2 * sqrt(S1) ))\n    S1 = Sigma1\n    S2 = Sigma2\n    # Numerical square roots\n    S1_sqrt = sqrtm(S1)\n    middle = S1_sqrt @ S2 @ S1_sqrt\n    middle_sqrt = sqrtm(middle)\n    W2_sq = np.trace(S1 + S2 - 2.0 * middle_sqrt)\n    W2_sq = float(np.real_if_close(W2_sq))\n    # Normalize per dimension\n    T = S1.shape[0]\n    W2_per_dim = np.sqrt(max(W2_sq, 0.0) / T)\n    return W2_per_dim\n\ndef ar_average_nll_per_dim(eval_seqs, a_hat, sigma_eps_hat2):\n    N, T = eval_seqs.shape\n    # Ensure stationarity for prior\n    a_eff = float(np.clip(a_hat, -0.999, 0.999))\n    var_x_hat = sigma_eps_hat2 / (1.0 - a_eff**2)\n    # Log-likelihood components\n    x = eval_seqs\n    # Prior term for x1\n    ll = -0.5 * (np.log(2.0 * np.pi * var_x_hat) + (x[:, 0] ** 2) / var_x_hat)\n    # Conditional terms\n    for t in range(1, T):\n        mu = a_hat * x[:, t - 1]\n        ll += -0.5 * (np.log(2.0 * np.pi * sigma_eps_hat2) + ((x[:, t] - mu) ** 2) / sigma_eps_hat2)\n    avg_nll_per_dim = float(-np.mean(ll) / T)\n    return avg_nll_per_dim\n\ndef diag_denoiser_alpha_from_training(train_seqs, beta):\n    # Empirical per-dimension variances\n    v = np.var(train_seqs, axis=0, ddof=0)\n    # Optimal linear coefficient under diagonal approximation: alpha_i = sqrt(1-beta) * v_i / ((1-beta) v_i + beta)\n    alpha = np.sqrt(1.0 - beta) * v / ((1.0 - beta) * v + beta)\n    return alpha, v\n\ndef diffusion_stationary_cov_diag(alpha, beta):\n    # s_i = (alpha_i^2 * beta) / (1 - (sqrt(1-beta) * alpha_i)^2)\n    c = np.sqrt(1.0 - beta) * alpha\n    denom = 1.0 - c**2\n    # numerical safety\n    denom = np.maximum(denom, 1e-12)\n    s = (alpha**2) * beta / denom\n    return s\n\ndef diag_gaussian_average_nll_per_dim(eval_seqs, s_diag):\n    N, T = eval_seqs.shape\n    # Ensure positivity\n    s = np.maximum(s_diag, 1e-12)\n    # Log-density of diagonal Gaussian\n    const = T * np.log(2.0 * np.pi) + np.sum(np.log(s))\n    quad = np.sum((eval_seqs ** 2) / s, axis=1)\n    ll = -0.5 * (const + quad - 0.0)  # constant per sample\n    avg_nll_per_dim = float(-np.mean(ll) / T)\n    return avg_nll_per_dim\n\ndef true_covariance(a, sigma_eps, T):\n    return stationary_cov_ar(a, sigma_eps, T)\n\ndef run_case(a, sigma_eps, T, N_train, N_eval, lam, beta, seed):\n    rng = np.random.default_rng(seed)\n    # Generate data\n    train = ar1_generate_sequences(a, sigma_eps, T, N_train, rng)\n    eval_data = ar1_generate_sequences(a, sigma_eps, T, N_eval, rng)\n    # Fit AR model\n    a_hat, sigma_eps_hat2 = estimate_ar_ridge(train, lam)\n    # AR metrics\n    ar_nll = ar_average_nll_per_dim(eval_data, a_hat, sigma_eps_hat2)\n    Sigma_true = true_covariance(a, sigma_eps, T)\n    Sigma_ar = stationary_cov_ar(a_hat, np.sqrt(sigma_eps_hat2), T)\n    w2_ar = wasserstein_gaussian_per_dim(Sigma_true, Sigma_ar)\n    # Diffusion denoiser and metrics\n    alpha, _ = diag_denoiser_alpha_from_training(train, beta)\n    s_diag = diffusion_stationary_cov_diag(alpha, beta)\n    diff_nll = diag_gaussian_average_nll_per_dim(eval_data, s_diag)\n    Sigma_diff = np.diag(s_diag)\n    w2_diff = wasserstein_gaussian_per_dim(Sigma_true, Sigma_diff)\n    return ar_nll, diff_nll, w2_ar, w2_diff\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (a, sigma_eps, T, N_train, N_eval, lambda, beta, seed)\n    test_cases = [\n        (0.7, 0.6, 6, 256, 8192, 0.0, 0.2, 0),    # Case 1\n        (0.9, 0.4, 6, 16,  8192, 5.0, 0.2, 1),    # Case 2\n        (-0.8, 0.5, 6, 128, 8192, 0.5, 0.2, 2),   # Case 3\n    ]\n\n    results = []\n    for case in test_cases:\n        a, sigma_eps, T, N_train, N_eval, lam, beta, seed = case\n        ar_nll, diff_nll, w2_ar, w2_diff = run_case(a, sigma_eps, T, N_train, N_eval, lam, beta, seed)\n        # Format to 6 decimal places for stability\n        results.extend([f\"{ar_nll:.6f}\", f\"{diff_nll:.6f}\", f\"{w2_ar:.6f}\", f\"{w2_diff:.6f}\"])\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}