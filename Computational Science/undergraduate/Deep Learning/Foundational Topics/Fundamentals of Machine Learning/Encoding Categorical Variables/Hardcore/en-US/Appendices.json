{
    "hands_on_practices": [
        {
            "introduction": "While one-hot encoding is a standard approach, it treats categories as independent entities, failing to capture inherent relationships like order or cyclicality. This first exercise  challenges you to move beyond one-hot encoding by implementing Fourier features, a powerful technique for injecting domain knowledge about periodic variables, such as months of the year or hours of the day. You will discover how this inductive bias can lead to superior generalization when the data exhibits cyclical patterns.",
            "id": "3121725",
            "problem": "You are given a cyclic categorical variable representing seasons with period $12$, labeled by indices $c \\in \\{0,1,2,\\dots,11\\}$. Consider two encodings of a category $c$: (i) one-hot encoding $e(c) \\in \\mathbb{R}^{12}$ with a single entry equal to $1$ at index $c$ and $0$ elsewhere, and (ii) Fourier feature encoding $\\phi(c) \\in \\mathbb{R}^{1+2|M|}$ constructed from a set of harmonics $M = \\{1,2\\}$ as $\\phi(c) = [1,\\sin(2\\pi m c / 12),\\cos(2\\pi m c / 12)]_{m \\in M}$, where all angles are in radians. You will fit a linear model $f(x) = w^\\top x$ by minimizing Mean Squared Error (MSE), and you will compute the Ordinary Least Squares (OLS) solution using the Moore–Penrose pseudoinverse. The goal is to test whether periodic Fourier features benefit models in the presence of cyclical categorical variables.\n\nFundamental base definitions to use:\n- Mean Squared Error (MSE) for a model $f$ on a dataset $\\{(x_i,y_i)\\}_{i=1}^n$ is $\\frac{1}{n}\\sum_{i=1}^n (y_i - f(x_i))^2$.\n- Ordinary Least Squares (OLS) chooses $w$ to minimize the MSE, with the minimal-norm solution given by $w = X^+ y$, where $X^+$ is the Moore–Penrose pseudoinverse of the design matrix $X$ and $y$ is the vector of targets.\n\nConstruct a toy setting where the target is a deterministic function of the category index $c$ with no noise. For each test case, you will:\n1. Generate training and test sets by enumerating categories as specified.\n2. Encode inputs using both one-hot $e(c)$ and Fourier features $\\phi(c)$ with harmonics $M = \\{1,2\\}$.\n3. Fit linear models using OLS on the training set for both encodings.\n4. Evaluate test MSE for both models on the specified test categories.\n5. Output a boolean indicating whether the Fourier feature model achieves strictly lower test MSE than the one-hot model.\n\nAngle unit specification: all angles in trigonometric functions must be expressed in radians.\n\nTest suite:\n- Case $1$ (happy path periodic): target $y(c) = \\sin\\!\\left(\\frac{2\\pi c}{12}\\right)$; training categories $S_{\\text{train}} = \\{0,2,4,6,8,10\\}$; test categories $S_{\\text{test}} = \\{1,3,5,7,9,11\\}$.\n- Case $2$ (adversarial periodic not in chosen harmonics): target $y(c) = \\cos(\\pi c)$, equivalently $y(c) = (-1)^c$ corresponding to harmonic $m=6$ for period $12$; training categories $S_{\\text{train}} = \\{0,2,4,6,8,10\\}$; test categories $S_{\\text{test}} = \\{1,3,5,7,9,11\\}$.\n- Case $3$ (mixture of harmonics covered by features, with missing categories): target $y(c) = \\sin\\!\\left(\\frac{2\\pi c}{12}\\right) + \\frac{1}{2}\\cos\\!\\left(\\frac{4\\pi c}{12}\\right)$; training categories $S_{\\text{train}} = \\{0,1,2,4,5,6,8,9,10\\}$; test categories $S_{\\text{test}} = \\{3,7,11\\}$.\n- Case $4$ (boundary with full category coverage): target $y(c) = \\sin\\!\\left(\\frac{2\\pi c}{12}\\right) + \\frac{1}{2}\\cos\\!\\left(\\frac{4\\pi c}{12}\\right)$; training categories $S_{\\text{train}} = \\{0,1,2,3,4,5,6,7,8,9,10,11\\}$; test categories $S_{\\text{test}} = \\{0,1,2,3,4,5,6,7,8,9,10,11\\}$.\n\nModel and evaluation details:\n- For each case, form the training design matrices $X_{\\text{one-hot}}$ and $X_{\\text{fourier}}$ by stacking encodings of the training categories, and the target vector $y_{\\text{train}}$ by applying the specified $y(c)$ to each training category.\n- Compute $w_{\\text{one-hot}} = X_{\\text{one-hot}}^+ y_{\\text{train}}$ and $w_{\\text{fourier}} = X_{\\text{fourier}}^+ y_{\\text{train}}$.\n- Evaluate predictions on the test categories using the corresponding encodings to form $X_{\\text{test}}$ and compute the test MSE for each model.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a boolean corresponding to the cases in the order $1$ through $4$ and is $True$ if the Fourier feature model has strictly lower test MSE than the one-hot model and $False$ otherwise. For example, the output should look like $[True,False,True,False]$.",
            "solution": "The problem requires a comparative analysis of two encoding schemes for a cyclic categorical variable: one-hot encoding and Fourier feature encoding. The performance is evaluated based on the Mean Squared Error (MSE) of an Ordinary Least Squares (OLS) linear model on a test set. The problem is valid as it is scientifically grounded in linear algebra and machine learning principles, is well-posed with all necessary information provided, and is objective in its evaluation criteria.\n\nThe core of the analysis lies in understanding the implicit assumptions (inductive biases) of each encoding method. The one-hot encoding creates a feature space where each category is represented by an orthogonal basis vector. A linear model in this space, $f(e(c)) = w^\\top e(c) = w_c$, assigns a separate weight $w_c$ to each category $c$. This allows the model to represent any arbitrary function on the set of categories, but it offers no mechanism for generalization to unseen categories. In contrast, the Fourier feature encoding represents each category $c$ by a vector $\\phi(c)$ of values from a set of sinusoidal basis functions. A linear model $f(\\phi(c)) = w^\\top \\phi(c)$ is constrained to represent the target function as a linear combination of these basis functions. This provides a strong inductive bias, assuming the target function is periodic and smooth, which facilitates generalization but limits the model's expressiveness to the span of the chosen harmonics.\n\nThe problem specifies a period of $P=12$ and a set of harmonics $M = \\{1,2\\}$. The Fourier feature vector for a category $c \\in \\{0, 1, \\dots, 11\\}$ is given by $\\phi(c) \\in \\mathbb{R}^{5}$:\n$$\n\\phi(c) = \\left[1, \\sin\\left(\\frac{2\\pi \\cdot 1 \\cdot c}{12}\\right), \\cos\\left(\\frac{2\\pi \\cdot 1 \\cdot c}{12}\\right), \\sin\\left(\\frac{2\\pi \\cdot 2 \\cdot c}{12}\\right), \\cos\\left(\\frac{2\\pi \\cdot 2 \\cdot c}{12}\\right)\\right]^\\top\n$$\nThe one-hot encoding $e(c) \\in \\mathbb{R}^{12}$ is a vector with $1$ at index $c$ and $0$ elsewhere.\n\nFor each case, we construct training design matrices $X_{\\text{one-hot}}$ and $X_{\\text{fourier}}$ by stacking the row vectors $e(c)^\\top$ and $\\phi(c)^\\top$ for all $c$ in the training set $S_{\\text{train}}$. The target vector $y_{\\text{train}}$ is formed by applying the given target function to each $c \\in S_{\\text{train}}$. The OLS weight vectors are found using the Moore-Penrose pseudoinverse, $w = X^+ y_{\\text{train}}$. This provides the unique minimum-norm solution that minimizes the sum of squared errors. We then form test matrices $X_{\\text{test}}$ for the test set $S_{\\text{test}}$, compute predictions $y_{\\text{pred}} = X_{\\text{test}} w$, and calculate the test MSE: $\\text{MSE} = \\frac{1}{|S_{\\text{test}}|} \\sum_{i \\in S_{\\text{test}}} (y_i - y_{\\text{pred},i})^2$.\n\n**Case 1: Periodic Target, Incomplete Data**\n- Target: $y(c) = \\sin(\\frac{2\\pi c}{12})$. This function is one of the basis functions in the Fourier encoding (corresponding to harmonic $m=1$).\n- Training: $S_{\\text{train}} = \\{0,2,4,6,8,10\\}$. Test: $S_{\\text{test}} = \\{1,3,5,7,9,11\\}$.\n- **Fourier Model**: Since the target function lies entirely within the span of the Fourier features, the model can perfectly represent it. The training data is sufficiently diverse for the OLS to identify the correct weights, primarily a weight of $1$ for the $\\sin(\\frac{2\\pi c}{12})$ feature and near-zero for others. This learned function generalizes perfectly to the unseen test categories. Thus, the test MSE will be approximately $0$.\n- **One-Hot Model**: The model is trained on even-indexed categories. For any training category $c_{\\text{train}} \\in S_{\\text{train}}$, the weight $w_{c_{\\text{train}}}$ learns the target value $y(c_{\\text{train}})$. For unseen categories $c_{\\text{test}} \\in S_{\\text{test}}$, the corresponding columns in the training design matrix are all zeros. The minimum-norm property of the pseudoinverse solution sets the weights for these unseen categories, $w_{c_{\\text{test}}}$, to $0$. Consequently, the model predicts $f(e(c_{\\text{test}})) = 0$ for all test points. The true test targets $y(c_{\\text{test}})$ are non-zero, resulting in a significant test MSE.\n- **Verdict**: $\\text{MSE}_{\\text{fourier}} \\approx 0  \\text{MSE}_{\\text{one-hot}}$. The result is **True**.\n\n**Case 2: Mismatched Periodic Target**\n- Target: $y(c) = \\cos(\\pi c) = \\cos(\\frac{2\\pi \\cdot 6 \\cdot c}{12})$. This function corresponds to harmonic $m=6$, which is not in the model's feature set $M=\\{1,2\\}$.\n- Training/Test sets are the same as in Case 1.\n- For the training set of even categories, the target is $y(c) = \\cos(\\pi c) = (-1)^c = 1$ for all $c \\in S_{\\text{train}}$.\n- **Fourier Model**: The training target vector is a constant vector of ones. This vector is identical to the first column of the Fourier design matrix (the bias term). The OLS solution will therefore be $w = [1,0,0,0,0]^\\top$, resulting in a model that predicts $f(c)=1$ for all inputs. For the test set (odd categories), the true target is $y(c) = -1$. The model's prediction of $1$ leads to an error of $(-1 - 1)^2 = 4$ for each test sample. Thus, $\\text{MSE}_{\\text{fourier}} = 4$.\n- **One-Hot Model**: As in Case 1, the model predicts $0$ for all unseen test categories. The true test target is $-1$. The error for each test sample is $(-1 - 0)^2 = 1$. Thus, $\\text{MSE}_{\\text{one-hot}} = 1$.\n- **Verdict**: $\\text{MSE}_{\\text{fourier}} = 4$, $\\text{MSE}_{\\text{one-hot}} = 1$. The condition $4  1$ is false. The result is **False**.\n\n**Case 3: Mixed Harmonic Target, Incomplete Data**\n- Target: $y(c) = \\sin(\\frac{2\\pi c}{12}) + \\frac{1}{2}\\cos(\\frac{4\\pi c}{12})$. This function is a linear combination of basis functions for $m=1$ and $m=2$, which are both included in the Fourier feature set.\n- Training: $S_{\\text{train}} = \\{0,1,2,4,5,6,8,9,10\\}$. Test: $S_{\\text{test}} = \\{3,7,11\\}$.\n- **Fourier Model**: The reasoning is identical to Case 1. The model's inductive bias is perfectly aligned with the target function. With $9$ training samples for $5$ parameters, the OLS fit will accurately recover the underlying function, which then generalizes perfectly to the test set. The test MSE will be approximately $0$.\n- **One-Hot Model**: The model is not trained on categories $\\{3,7,11\\}$. It will predict $0$ for these test points. The true targets are non-zero, leading to a positive test MSE.\n- **Verdict**: $\\text{MSE}_{\\text{fourier}} \\approx 0  \\text{MSE}_{\\text{one-hot}}$. The result is **True**.\n\n**Case 4: Full Data Coverage**\n- Target: Same as Case 3.\n- Training and test sets are identical and complete: $S_{\\text{train}} = S_{\\text{test}} = \\{0, 1, \\dots, 11\\}$.\n- **One-Hot Model**: With all $12$ categories present in the training set, the design matrix $X_{\\text{one-hot}}$ is the $12 \\times 12$ identity matrix, $I_{12}$. Its pseudoinverse is also $I_{12}$. The weights are $w_{\\text{one-hot}} = I_{12} y_{\\text{train}} = y_{\\text{train}}$, meaning $w_c = y(c)$ for each category. The model perfectly memorizes the training data. Since the test set is identical to the training set, the predictions are perfect, and $\\text{MSE}_{\\text{one-hot}} = 0$.\n- **Fourier Model**: As in Case 3, the target function is in the span of the features. With the full dataset, the OLS fit is guaranteed to find the exact weights to reproduce the function perfectly. The model will also achieve zero error on the training/test set. Thus, $\\text{MSE}_{\\text{fourier}} = 0$.\n- **Verdict**: Since $\\text{MSE}_{\\text{fourier}} = 0$ and $\\text{MSE}_{\\text{one-hot}} = 0$, the strict inequality $\\text{MSE}_{\\text{fourier}}  \\text{MSE}_{\\text{one-hot}}$ (i.e., $0  0$) is false. The result is **False**.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def solve_case(target_func, s_train, s_test, P, M):\n        \"\"\"\n        Solves a single test case for the problem.\n\n        Args:\n            target_func (callable): The function generating the target values y(c).\n            s_train (list): The list of training categories.\n            s_test (list): The list of test categories.\n            P (int): The period of the cyclic variable.\n            M (list): The set of harmonics for Fourier features.\n        \n        Returns:\n            bool: True if the Fourier model's MSE is strictly less than the one-hot model's MSE.\n        \"\"\"\n        # Convert category lists to numpy arrays for vectorized operations\n        c_train = np.array(s_train)\n        c_test = np.array(s_test)\n        \n        # 1. Generate target values for training and test sets\n        y_train = target_func(c_train)\n        y_test = target_func(c_test)\n\n        # 2. Encode inputs for both training and test sets\n        \n        # One-hot encoding\n        num_classes = P\n        X_train_onehot = np.zeros((len(c_train), num_classes))\n        # Use advanced indexing to set the '1's in the one-hot vectors\n        X_train_onehot[np.arange(len(c_train)), c_train] = 1\n        \n        X_test_onehot = np.zeros((len(c_test), num_classes))\n        X_test_onehot[np.arange(len(c_test)), c_test] = 1\n\n        # Fourier feature encoding\n        num_features_fourier = 1 + 2 * len(M)\n        X_train_fourier = np.ones((len(c_train), num_features_fourier))\n        X_test_fourier = np.ones((len(c_test), num_features_fourier))\n        \n        feature_idx = 1\n        for m in M:\n            # Training set features\n            angle_train = 2.0 * np.pi * m * c_train / P\n            X_train_fourier[:, feature_idx] = np.sin(angle_train)\n            X_train_fourier[:, feature_idx + 1] = np.cos(angle_train)\n            \n            # Test set features\n            angle_test = 2.0 * np.pi * m * c_test / P\n            X_test_fourier[:, feature_idx] = np.sin(angle_test)\n            X_test_fourier[:, feature_idx + 1] = np.cos(angle_test)\n            \n            feature_idx += 2\n\n        # 3. Fit linear models using OLS with the Moore-Penrose pseudoinverse\n        w_onehot = np.linalg.pinv(X_train_onehot) @ y_train\n        w_fourier = np.linalg.pinv(X_train_fourier) @ y_train\n        \n        # 4. Evaluate test MSE for both models\n        \n        # Predictions\n        y_pred_onehot = X_test_onehot @ w_onehot\n        y_pred_fourier = X_test_fourier @ w_fourier\n        \n        # Mean Squared Error calculation\n        mse_onehot = np.mean((y_test - y_pred_onehot)**2)\n        mse_fourier = np.mean((y_test - y_pred_fourier)**2)\n        \n        # 5. Return boolean indicating if Fourier MSE is strictly lower\n        return mse_fourier  mse_onehot\n\n    # Define the test cases from the problem statement.\n    P = 12\n    M = [1, 2]\n    \n    test_cases = [\n        # Case 1\n        {'target_func': lambda c: np.sin(2.0 * np.pi * c / P),\n         's_train': [0, 2, 4, 6, 8, 10],\n         's_test': [1, 3, 5, 7, 9, 11]},\n        # Case 2\n        {'target_func': lambda c: np.cos(np.pi * c),\n         's_train': [0, 2, 4, 6, 8, 10],\n         's_test': [1, 3, 5, 7, 9, 11]},\n        # Case 3\n        {'target_func': lambda c: np.sin(2.0 * np.pi * c / P) + 0.5 * np.cos(4.0 * np.pi * c / P),\n         's_train': [0, 1, 2, 4, 5, 6, 8, 9, 10],\n         's_test': [3, 7, 11]},\n        # Case 4\n        {'target_func': lambda c: np.sin(2.0 * np.pi * c / P) + 0.5 * np.cos(4.0 * np.pi * c / P),\n         's_train': list(range(P)),\n         's_test': list(range(P))},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(case['target_func'], case['s_train'], case['s_test'], P, M)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Instead of manually engineering features, modern deep learning excels at learning them directly from data. This practice  introduces this concept through the lens of knowledge distillation, a technique for training a compact 'student' model to mimic a larger 'teacher' model. You will implement a system where a student learns a low-dimensional embedding that captures the rich, nuanced representations from a high-dimensional teacher, demonstrating a practical path toward creating efficient and powerful learned embeddings.",
            "id": "3121739",
            "problem": "You are given the task of implementing knowledge distillation for categorical variable encoders within the framework of deep learning, using only definitions and well-tested facts as the foundation. A categorical variable with a finite vocabulary is encoded through an embedding lookup. The teacher model uses a high-dimensional embedding of size $d_T$ and the student model uses a low-dimensional embedding of size $d_S$. Each model maps the embedding of a category to class logits through an affine map, and a probability distribution over classes is obtained via the Softmax function with temperature $T$. Knowledge transfer is quantified by the Kullback–Leibler divergence (KLD) between the teacher’s and student’s class distributions, scaled by the conventional temperature factor.\n\nStart from the following base principles and definitions:\n1. A categorical variable taking values in $\\{0, 1, \\dots, V-1\\}$ can be encoded by an embedding lookup using a matrix of shape $V \\times d$, where each row is the representation of a category.\n2. The Softmax with temperature $T$ maps logits $\\mathbf{z} \\in \\mathbb{R}^K$ to a probability vector $\\mathbf{p} \\in \\Delta^{K-1}$ by normalizing exponentials of scaled logits.\n3. The Kullback–Leibler divergence between two categorical distributions $\\mathbf{p}$ and $\\mathbf{q}$ over $K$ classes is defined by the expected logarithmic difference between $\\mathbf{p}$ and $\\mathbf{q}$ under $\\mathbf{p}$.\n4. The embedding encoder followed by a linear map is a standard and widely used approach to handle categorical variables in deep learning.\n\nImplement a program that, for each test case, constructs a teacher and a student as follows:\n1. The teacher has an embedding matrix $\\mathbf{E}_T \\in \\mathbb{R}^{V \\times d_T}$, a weight matrix $\\mathbf{W}_T \\in \\mathbb{R}^{d_T \\times K}$, and a bias vector $\\mathbf{b}_T \\in \\mathbb{R}^{K}$. The teacher logits for a category $x$ are given by $\\mathbf{z}_T(x) = \\mathbf{E}_T[x] \\mathbf{W}_T + \\mathbf{b}_T$.\n2. The student has an embedding matrix $\\mathbf{E}_S \\in \\mathbb{R}^{V \\times d_S}$, a weight matrix $\\mathbf{W}_S \\in \\mathbb{R}^{d_S \\times K}$, and a bias vector $\\mathbf{b}_S \\in \\mathbb{R}^{K}$. The student logits for a category $x$ are given by $\\mathbf{z}_S(x) = \\mathbf{E}_S[x] \\mathbf{W}_S + \\mathbf{b}_S$.\n3. The teacher and student class distributions are the Softmax with temperature $T$ applied to their respective logits.\n4. The knowledge distillation loss on a batch is the Kullback–Leibler divergence of the teacher distribution relative to the student distribution, averaged over the batch, and multiplied by $T^2$.\n\nUse the following data generation and initialization protocol for each test case:\n1. Random seeds are provided; all randomness must be reproducible and tied to the specified seeds.\n2. Initialize all parameters of the teacher and student using independent draws from a normal distribution with mean $0$ and standard deviation $0.1$.\n3. Construct a batch of size $N$ by sampling $N$ independent categorical inputs uniformly from $\\{0, 1, \\dots, V-1\\}$.\n\nTraining protocol for each test case:\n1. Hold the teacher fixed.\n2. Optimize the student parameters $(\\mathbf{E}_S, \\mathbf{W}_S, \\mathbf{b}_S)$ only, using batch gradient descent on the knowledge distillation objective described above, for exactly $S$ steps with learning rate $\\eta$.\n3. Use the chain rule to derive the exact gradients with respect to the student parameters from the definitions of Softmax with temperature and Kullback–Leibler divergence.\n\nYour program must compute, for each test case, the final knowledge distillation loss value after training, expressed as a float rounded to six decimal places.\n\nTest suite:\nUse the three test cases below. For each case, the teacher and student must be initialized from $\\mathcal{N}(0, 0.1^2)$ using the provided seeds as described, and the categorical batch must also be generated from the specified seed. The three test cases are:\n1. Case A: $V = 7$, $K = 4$, $N = 16$, $d_T = 12$, $d_S = 2$, $T = 2.5$, $S = 300$, $\\eta = 0.2$, random seed $s = 12345$.\n2. Case B: $V = 5$, $K = 2$, $N = 10$, $d_T = 6$, $d_S = 1$, $T = 1.0$, $S = 350$, $\\eta = 0.15$, random seed $s = 2021$.\n3. Case C: $V = 6$, $K = 3$, $N = 12$, $d_T = 5$, $d_S = 5$, $T = 3.0$, $S = 300$, $\\eta = 0.2$, random seed $s = 98765$.\n\nInitialization and sampling seeding rules per test case:\n1. Teacher parameters $(\\mathbf{E}_T, \\mathbf{W}_T, \\mathbf{b}_T)$ are initialized using the seed $s$.\n2. Student parameters $(\\mathbf{E}_S, \\mathbf{W}_S, \\mathbf{b}_S)$ are initialized using the seed $s + 1$.\n3. The batch of $N$ categorical inputs is sampled using the seed $s + 2$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the three test cases, with each value rounded to six decimals (e.g., $[0.123456,0.234567,0.345678]$). No other text should be printed.",
            "solution": "The user-provided problem is valid. It is scientifically grounded in the principles of deep learning and knowledge distillation, well-posed with all necessary parameters and initial conditions defined, and objective in its formulation. It requests the implementation of a standard training procedure for a student model to mimic a teacher model, a common task in machine learning. The solution requires the derivation of gradients for the specified loss function and the implementation of a batch gradient descent optimization scheme.\n\n### 1. Model and Objective Function Formulation\n\nLet a categorical variable take values from a vocabulary of size $V$, i.e., $x \\in \\{0, 1, \\dots, V-1\\}$. Both teacher and student models encode this variable via an embedding matrix and process it through an affine transformation to produce logits for $K$ classes.\n\nThe teacher model is defined by its parameters: an embedding matrix $\\mathbf{E}_T \\in \\mathbb{R}^{V \\times d_T}$, a weight matrix $\\mathbf{W}_T \\in \\mathbb{R}^{d_T \\times K}$, and a bias vector $\\mathbf{b}_T \\in \\mathbb{R}^{K}$. For a given category $x$, the teacher's embedding is $\\mathbf{e}_T(x) = \\mathbf{E}_T[x]$, and the logits are:\n$$ \\mathbf{z}_T(x) = \\mathbf{e}_T(x) \\mathbf{W}_T + \\mathbf{b}_T $$\n\nSimilarly, the student model is defined by its parameters: an embedding matrix $\\mathbf{E}_S \\in \\mathbb{R}^{V \\times d_S}$, a weight matrix $\\mathbf{W}_S \\in \\mathbb{R}^{d_S \\times K}$, and a bias vector $\\mathbf{b}_S \\in \\mathbb{R}^{K}$. For a given category $x$, the student's embedding is $\\mathbf{e}_S(x) = \\mathbf{E}_S[x]$, and the logits are:\n$$ \\mathbf{z}_S(x) = \\mathbf{e}_S(x) \\mathbf{W}_S + \\mathbf{b}_S $$\n\nThe class probabilities are obtained using the Softmax function with temperature $T$. For any logit vector $\\mathbf{z} \\in \\mathbb{R}^K$, the probability of class $k$ is:\n$$ \\text{softmax}(\\mathbf{z}, T)_k = \\frac{\\exp(z_k / T)}{\\sum_{j=1}^{K} \\exp(z_j / T)} $$\nLet $\\mathbf{p}_T(x)$ and $\\mathbf{p}_S(x)$ denote the probability distributions for the teacher and student models, respectively, for input category $x$.\n\nThe knowledge distillation loss for a single input $x$ is based on the Kullback–Leibler (KL) divergence from the student's distribution $\\mathbf{p}_S(x)$ to the teacher's distribution $\\mathbf{p}_T(x)$, defined as $D_{KL}(\\mathbf{p}_T(x) || \\mathbf{p}_S(x))$:\n$$ D_{KL}(\\mathbf{p}_T(x) || \\mathbf{p}_S(x)) = \\sum_{k=1}^{K} p_{T,k}(x) \\log\\left(\\frac{p_{T,k}(x)}{p_{S,k}(x)}\\right) = \\sum_{k=1}^{K} p_{T,k}(x) (\\log p_{T,k}(x) - \\log p_{S,k}(x)) $$\nThe total loss $L$ for a batch of $N$ inputs $\\{x_1, \\dots, x_N\\}$ is the average KL divergence scaled by $T^2$:\n$$ L = \\frac{T^2}{N} \\sum_{i=1}^{N} D_{KL}(\\mathbf{p}_T(x_i) || \\mathbf{p}_S(x_i)) $$\nThe objective is to minimize this loss $L$ by optimizing the student's parameters $(\\mathbf{E}_S, \\mathbf{W}_S, \\mathbf{b}_S)$ while keeping the teacher's parameters fixed.\n\n### 2. Gradient Derivation for Optimization\n\nWe use batch gradient descent to update the student parameters. This requires computing the partial derivatives of the loss $L$ with respect to $\\mathbf{E}_S$, $\\mathbf{W}_S$, and $\\mathbf{b}_S$. We use the chain rule.\n\nFirst, let's find the gradient of the loss for a single sample $x$ with respect to the student logits $\\mathbf{z}_S(x)$. The teacher parameters and its output distribution $\\mathbf{p}_T(x)$ are constant with respect to the student parameters. The loss component for one sample is $L(x) = T^2 D_{KL}(\\mathbf{p}_T(x) || \\mathbf{p}_S(x))$.\n$$ \\frac{\\partial L(x)}{\\partial z_{S,j}(x)} = -T^2 \\sum_{k=1}^{K} p_{T,k}(x) \\frac{\\partial}{\\partial z_{S,j}(x)} \\log p_{S,k}(x) = -T^2 \\sum_{k=1}^{K} \\frac{p_{T,k}(x)}{p_{S,k}(x)} \\frac{\\partial p_{S,k}(x)}{\\partial z_{S,j}(x)} $$\nThe derivative of the Softmax function with temperature is $\\frac{\\partial p_{S,k}(x)}{\\partial z_{S,j}(x)} = \\frac{1}{T} p_{S,k}(x) (\\delta_{kj} - p_{S,j}(x))$, where $\\delta_{kj}$ is the Kronecker delta. Substituting this gives:\n$$ \\frac{\\partial L(x)}{\\partial z_{S,j}(x)} = -T^2 \\sum_{k=1}^{K} \\frac{p_{T,k}(x)}{p_{S,k}(x)} \\left[ \\frac{1}{T} p_{S,k}(x) (\\delta_{kj} - p_{S,j}(x)) \\right] $$\n$$ \\frac{\\partial L(x)}{\\partial z_{S,j}(x)} = -T \\sum_{k=1}^{K} p_{T,k}(x) (\\delta_{kj} - p_{S,j}(x)) = -T \\left( p_{T,j}(x) - \\sum_{k=1}^{K} p_{T,k}(x) p_{S,j}(x) \\right) $$\nSince $\\sum_k p_{T,k}(x) = 1$, we get a simplified expression for the gradient with respect to the student logits:\n$$ \\frac{\\partial L(x)}{\\partial z_{S,j}(x)} = T (p_{S,j}(x) - p_{T,j}(x)) $$\nLet $\\boldsymbol{\\delta}_{\\mathbf{z}}(x) = T (\\mathbf{p}_S(x) - \\mathbf{p}_T(x))$ be the gradient vector for a single sample. The gradient of the total loss $L$ with respect to the logits of sample $x_i$ is $\\frac{1}{N}\\boldsymbol{\\delta}_{\\mathbf{z}}(x_i)$. Let $\\mathbf{\\Delta_P} = \\frac{T}{N}(\\mathbf{P}_S - \\mathbf{P}_T)$ be the $N \\times K$ matrix of these gradients for the whole batch, where $\\mathbf{P}_S$ and $\\mathbf{P}_T$ are the $N \\times K$ probability matrices.\n\nNow, we find the gradients for the student parameters using $\\mathbf{\\Delta_P}$:\n\n**Gradient with respect to bias $\\mathbf{b}_S$**:\nSince $\\mathbf{z}_S(x_i) = \\mathbf{e}_S(x_i)\\mathbf{W}_S + \\mathbf{b}_S$, we have $\\frac{\\partial z_{S,k}(x_i)}{\\partial b_{S,j}} = \\delta_{kj}$.\n$$ \\nabla_{\\mathbf{b}_S} L = \\frac{\\partial L}{\\partial \\mathbf{b}_S} = \\sum_{i=1}^N \\frac{\\partial L}{\\partial \\mathbf{z}_S(x_i)} \\frac{\\partial \\mathbf{z}_S(x_i)}{\\partial \\mathbf{b}_S} = \\sum_{i=1}^N \\mathbf{\\Delta_P}[i,:] = \\text{sum}(\\mathbf{\\Delta_P}, \\text{axis}=0) $$\n\n**Gradient with respect to weights $\\mathbf{W}_S$**:\nThe gradient with respect to $W_{S,mk}$ is:\n$$ \\nabla_{\\mathbf{W}_S} L = \\frac{\\partial L}{\\partial \\mathbf{W}_S} = \\sum_{i=1}^N \\left( \\mathbf{e}_S(x_i) \\right)^T \\mathbf{\\Delta_P}[i,:] = (\\mathbf{E}_S^{\\text{batch}})^T \\mathbf{\\Delta_P} $$\nwhere $\\mathbf{E}_S^{\\text{batch}}$ is the $N \\times d_S$ matrix of embeddings for the batch.\n\n**Gradient with respect to embeddings $\\mathbf{E}_S$**:\nThe gradient of the loss with respect to the embedding vector $\\mathbf{e}_S(x_i)$ for a single sample $x_i$ is:\n$$ \\frac{\\partial L}{\\partial \\mathbf{e}_S(x_i)} = \\mathbf{\\Delta_P}[i,:] (\\mathbf{W}_S)^T $$\nThe full gradient $\\nabla_{\\mathbf{E}_S} L$ is a $V \\times d_S$ matrix where the gradient for the $v$-th row, $\\mathbf{e}_S(v)$, is the sum of gradients for all samples in the batch where $x_i = v$:\n$$ \\nabla_{\\mathbf{E}_S} L[v,:] = \\sum_{i=1}^N \\mathbb{I}(x_i=v) \\frac{\\partial L}{\\partial \\mathbf{e}_S(x_i)} = \\sum_{i: x_i=v} \\mathbf{\\Delta_P}[i,:] (\\mathbf{W}_S)^T $$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. This can be implemented efficiently by computing the gradients for each sample in the batch, $\\mathbf{\\Delta_P} (\\mathbf{W}_S)^T$, and then accumulating them into the corresponding rows of the full gradient matrix $\\nabla_{\\mathbf{E}_S} L$.\n\n### 3. Training Algorithm: Batch Gradient Descent\n\nThe student parameters are updated iteratively for $S$ steps. For each step $s=1, \\dots, S$:\n1.  **Forward Pass**: Compute student logits $\\mathbf{Z}_S$ and probabilities $\\mathbf{P}_S$ for the current batch.\n2.  **Backward Pass**: Compute the gradients $\\nabla_{\\mathbf{b}_S} L$, $\\nabla_{\\mathbf{W}_S} L$, and $\\nabla_{\\mathbf{E}_S} L$ as derived above.\n3.  **Parameter Update**: Update the student parameters using the learning rate $\\eta$:\n    $$ \\mathbf{b}_S \\leftarrow \\mathbf{b}_S - \\eta \\nabla_{\\mathbf{b}_S} L $$\n    $$ \\mathbf{W}_S \\leftarrow \\mathbf{W}_S - \\eta \\nabla_{\\mathbf{W}_S} L $$\n    $$ \\mathbf{E}_S \\leftarrow \\mathbf{E}_S - \\eta \\nabla_{\\mathbf{E}_S} L $$\n\nThis entire procedure is implemented for each test case using the specified parameters and seeding protocol to ensure reproducibility. The final loss is computed after $S$ training steps.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for knowledge distillation.\n    \"\"\"\n\n    def run_distillation(V, K, N, d_T, d_S, T, S, eta, seed):\n        \"\"\"\n        Performs knowledge distillation for one test case.\n\n        Args:\n            V (int): Vocabulary size.\n            K (int): Number of classes.\n            N (int): Batch size.\n            d_T (int): Teacher embedding dimension.\n            d_S (int): Student embedding dimension.\n            T (float): Temperature for Softmax.\n            S (int): Number of training steps.\n            eta (float): Learning rate.\n            seed (int): Base random seed.\n\n        Returns:\n            float: The final knowledge distillation loss.\n        \"\"\"\n        # 1. Initialization\n        std_dev = 0.1\n\n        # Teacher parameters\n        rng_T = np.random.default_rng(seed)\n        E_T = rng_T.normal(0, std_dev, (V, d_T))\n        W_T = rng_T.normal(0, std_dev, (d_T, K))\n        b_T = rng_T.normal(0, std_dev, (1, K))\n\n        # Student parameters\n        rng_S = np.random.default_rng(seed + 1)\n        E_S = rng_S.normal(0, std_dev, (V, d_S))\n        W_S = rng_S.normal(0, std_dev, (d_S, K))\n        b_S = rng_S.normal(0, std_dev, (1, K))\n\n        # Batch of categorical inputs\n        rng_B = np.random.default_rng(seed + 2)\n        x_batch = rng_B.integers(0, V, size=N)\n\n        # 2. Pre-compute teacher probabilities (fixed)\n        def softmax_with_temp(logits, temp):\n            # Numerically stable softmax\n            scaled_logits = logits / temp\n            stable_logits = scaled_logits - np.max(scaled_logits, axis=1, keepdims=True)\n            exp_logits = np.exp(stable_logits)\n            return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n\n        E_T_batch = E_T[x_batch]\n        Z_T = E_T_batch @ W_T + b_T\n        P_T = softmax_with_temp(Z_T, T)\n        \n        # Use a small epsilon to avoid log(0)\n        epsilon = 1e-9\n        log_P_T = np.log(P_T + epsilon)\n\n        # 3. Training Loop (Batch Gradient Descent)\n        for _ in range(S):\n            # Forward pass for the student\n            E_S_batch = E_S[x_batch]\n            Z_S = E_S_batch @ W_S + b_S\n            P_S = softmax_with_temp(Z_S, T)\n\n            # Backward pass: compute gradients\n            # Gradient of loss w.r.t. student logits, averaged over the batch\n            grad_z = T * (P_S - P_T) / N\n\n            # Gradient for bias b_S\n            grad_b_S = np.sum(grad_z, axis=0, keepdims=True)\n            \n            # Gradient for weights W_S\n            grad_W_S = E_S_batch.T @ grad_z\n\n            # Gradient for embeddings E_S\n            grad_e_batch = grad_z @ W_S.T\n            grad_E_S = np.zeros_like(E_S)\n            np.add.at(grad_E_S, x_batch, grad_e_batch)\n            \n            # Parameter update\n            E_S -= eta * grad_E_S\n            W_S -= eta * grad_W_S\n            b_S -= eta * grad_b_S\n\n        # 4. Final loss calculation\n        E_S_batch_final = E_S[x_batch]\n        Z_S_final = E_S_batch_final @ W_S + b_S\n        P_S_final = softmax_with_temp(Z_S_final, T)\n\n        log_P_S_final = np.log(P_S_final + epsilon)\n        \n        # KL Divergence for each sample in the batch\n        kl_div_samples = np.sum(P_T * (log_P_T - log_P_S_final), axis=1)\n        \n        # Final loss: T^2 * mean(KL)\n        final_loss = (T**2) * np.mean(kl_div_samples)\n        \n        return final_loss\n\n    test_cases = [\n        # Case A: V, K, N, d_T, d_S, T, S, eta, seed\n        (7, 4, 16, 12, 2, 2.5, 300, 0.2, 12345),\n        # Case B\n        (5, 2, 10, 6, 1, 1.0, 350, 0.15, 2021),\n        # Case C\n        (6, 3, 12, 5, 5, 3.0, 300, 0.2, 98765),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_distillation(*case)\n        results.append(result)\n\n    # Format the output as a comma-separated list of floats rounded to 6 decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Learned embeddings are optimized via gradient descent, which requires the entire model to be differentiable. This final exercise  takes a deep dive into the mechanics of this process by exploring temperature-controlled soft assignments. By analyzing the gradient and Hessian of the loss function, you will gain a concrete understanding of how temperature can smoothly transition a representation from a 'soft' mixture to a 'hard' one-hot-like choice, and what this implies for the optimization landscape.",
            "id": "3121698",
            "problem": "Consider a categorical variable with $M$ base codes, where each base code is a vector $\\mathbf{b}_i \\in \\mathbb{R}^d$ for $i \\in \\{1, \\dots, M\\}$, assembled into a matrix $B \\in \\mathbb{R}^{M \\times d}$ whose $i$-th row is $\\mathbf{b}_i^\\top$. A category $c$ is parameterized by a logits vector $\\mathbf{z}_c \\in \\mathbb{R}^M$ and represented as a temperature-controlled soft mixture over the base codes with weights $\\boldsymbol{\\pi}(\\tau) \\in \\mathbb{R}^M$ given by the Softmax function\n$$\n\\pi_i(\\tau) = \\frac{\\exp\\left(z_{c,i} / \\tau\\right)}{\\sum_{j=1}^M \\exp\\left(z_{c,j} / \\tau\\right)}, \\quad i \\in \\{1,\\dots,M\\},\n$$\nwhere $\\tau \\in \\mathbb{R}_{0}$ is the temperature. The embedding of category $c$ at temperature $\\tau$ is defined as\n$$\n\\mathbf{e}_c(\\tau) = \\sum_{i=1}^M \\pi_i(\\tau)\\, \\mathbf{b}_i = B^\\top \\boldsymbol{\\pi}(\\tau).\n$$\nA scalar predictor is given by parameters $\\mathbf{w} \\in \\mathbb{R}^d$ and $b_0 \\in \\mathbb{R}$, producing\n$$\n\\hat{y}(\\tau) = \\mathbf{w}^\\top \\mathbf{e}_c(\\tau) + b_0 = \\mathbf{w}^\\top B^\\top \\boldsymbol{\\pi}(\\tau) + b_0.\n$$\nFor a scalar target $y \\in \\mathbb{R}$, consider the squared error loss\n$$\nL(\\tau) = \\frac{1}{2}\\left(\\hat{y}(\\tau) - y\\right)^2.\n$$\nStarting from fundamental calculus (chain rule) and the definition of the Softmax, encode the categorical variable via temperature-controlled soft assignments and analyze the optimization landscape with respect to the logits $\\mathbf{z}_c$ by computing two quantities for each test case:\n- The Euclidean norm of the gradient $\\left\\lVert \\nabla_{\\mathbf{z}_c} L(\\tau) \\right\\rVert_2$.\n- The spectral norm of the Hessian $\\left\\lVert \\nabla^2_{\\mathbf{z}_c} L(\\tau) \\right\\rVert_2$, defined as the largest singular value, which for a symmetric matrix equals the maximum absolute eigenvalue.\n\nYour program must implement the described model, compute these two quantities exactly using analytical derivatives of the Softmax where applicable, and aggregate the results across the following test suite. Use the fixed base code matrix $B$, predictor parameters $\\mathbf{w}$ and $b_0$, and target $y$ given below, and vary the logits $\\mathbf{z}_c$ and temperature $\\tau$ per test case. All vectors are row vectors unless transposed.\n\nUse:\n- $M = 4$, $d = 3$,\n- $B = \\begin{bmatrix}\n0.9  -0.4  0.1 \\\\\n0.3  0.8  -0.5 \\\\\n-0.6  0.2  0.7 \\\\\n0.5  -0.1  -0.3\n\\end{bmatrix}$,\n- $\\mathbf{w} = \\left[0.7, -1.1, 0.9\\right]$,\n- $b_0 = 0.2$,\n- $y = 0.5$.\n\nTest suite:\n1. Happy path: $\\mathbf{z}_c = \\left[1.2, -0.5, 0.3, 2.0\\right]$, $\\tau = 1.0$.\n2. Smooth regime: $\\mathbf{z}_c = \\left[1.2, -0.5, 0.3, 2.0\\right]$, $\\tau = 2.0$.\n3. Transition regime: $\\mathbf{z}_c = \\left[1.2, -0.5, 0.3, 2.0\\right]$, $\\tau = 0.5$.\n4. Near-hard regime: $\\mathbf{z}_c = \\left[1.2, -0.5, 0.3, 2.0\\right]$, $\\tau = 0.1$.\n5. Edge case (equal logits): $\\mathbf{z}_c = \\left[0.0, 0.0, 0.0, 0.0\\right]$, $\\tau = 1.0$.\n6. Edge case (equal logits, very low temperature): $\\mathbf{z}_c = \\left[0.0, 0.0, 0.0, 0.0\\right]$, $\\tau = 0.01$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a two-element list $\\left[\\left\\lVert \\nabla_{\\mathbf{z}_c} L(\\tau) \\right\\rVert_2, \\left\\lVert \\nabla^2_{\\mathbf{z}_c} L(\\tau) \\right\\rVert_2\\right]$. For example, an output with three test cases should look like $\\left[[g_1,h_1],[g_2,h_2],[g_3,h_3]\\right]$, with all entries expressed as real numbers without units. This task is relevant to understanding how temperature affects the smooth-to-hard transition in categorical encodings and its implications for methods such as Stochastic Gradient Descent (SGD).",
            "solution": "The problem requires the computation of the Euclidean norm of the gradient and the spectral norm of the Hessian of a squared error loss function with respect to a vector of logits, $\\mathbf{z}_c$. The model uses a temperature-controlled soft mixture of base codes to form a categorical embedding.\n\n### Step 1: Define Variables and the Loss Function\n\nLet the given parameters be:\n-   Number of base codes: $M \\in \\mathbb{N}$\n-   Dimension of base codes: $d \\in \\mathbb{N}$\n-   Base code matrix: $B \\in \\mathbb{R}^{M \\times d}$, with rows $\\mathbf{b}_i^\\top$\n-   Predictor weights: $\\mathbf{w} \\in \\mathbb{R}^d$\n-   Predictor bias: $b_0 \\in \\mathbb{R}$\n-   Target value: $y \\in \\mathbb{R}$\n-   Logits vector for category $c$: $\\mathbf{z}_c \\in \\mathbb{R}^M$\n-   Temperature: $\\tau \\in \\mathbb{R}_{0}$\n\nThe soft mixture weights $\\boldsymbol{\\pi}(\\tau) \\in \\mathbb{R}^M$ are given by the Softmax function applied to the scaled logits $\\mathbf{z}_c/\\tau$:\n$$\n\\pi_i(\\tau) = \\frac{\\exp\\left(z_{c,i} / \\tau\\right)}{\\sum_{j=1}^M \\exp\\left(z_{c,j} / \\tau\\right)}\n$$\nThe category embedding $\\mathbf{e}_c(\\tau) \\in \\mathbb{R}^d$ is a weighted sum of the base codes:\n$$\n\\mathbf{e}_c(\\tau) = \\sum_{i=1}^M \\pi_i(\\tau)\\, \\mathbf{b}_i = B^\\top \\boldsymbol{\\pi}(\\tau)\n$$\nThe scalar prediction $\\hat{y}(\\tau)$ is:\n$$\n\\hat{y}(\\tau) = \\mathbf{w}^\\top \\mathbf{e}_c(\\tau) + b_0 = \\mathbf{w}^\\top B^\\top \\boldsymbol{\\pi}(\\tau) + b_0\n$$\nLet's define a vector $\\mathbf{v} = B\\mathbf{w} \\in \\mathbb{R}^M$. The components of this vector are $v_i = \\mathbf{b}_i^\\top \\mathbf{w}$. We can rewrite the prediction as:\n$$\n\\hat{y}(\\tau) = (B\\mathbf{w})^\\top \\boldsymbol{\\pi}(\\tau) + b_0 = \\mathbf{v}^\\top \\boldsymbol{\\pi}(\\tau) + b_0\n$$\nThe squared error loss is:\n$$\nL(\\tau) = \\frac{1}{2}\\left(\\hat{y}(\\tau) - y\\right)^2\n$$\n\n### Step 2: Compute the Gradient $\\nabla_{\\mathbf{z}_c} L(\\tau)$\n\nWe use the chain rule to find the gradient of $L$ with respect to $\\mathbf{z}_c$. Let's denote $\\mathbf{z}_c$ as $\\mathbf{z}$ for brevity.\n$$\n\\nabla_{\\mathbf{z}} L = \\frac{\\partial L}{\\partial \\hat{y}} \\nabla_{\\mathbf{z}} \\hat{y}\n$$\nThe first term is the derivative of the loss with respect to the prediction:\n$$\n\\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y}(\\tau) - y\n$$\nThe second term is the gradient of the prediction with respect to the logits:\n$$\n\\nabla_{\\mathbf{z}} \\hat{y} = \\nabla_{\\mathbf{z}} (\\mathbf{v}^\\top \\boldsymbol{\\pi} + b_0) = (\\nabla_{\\mathbf{z}} \\boldsymbol{\\pi})^\\top \\mathbf{v}\n$$\nThe Jacobian of the Softmax function $\\boldsymbol{\\pi}$ with respect to its input $\\mathbf{u}=\\mathbf{z}/\\tau$ is a matrix $J_{\\boldsymbol{\\pi},\\mathbf{u}}$ with entries $(J_{\\boldsymbol{\\pi},\\mathbf{u}})_{ik} = \\frac{\\partial \\pi_i}{\\partial u_k} = \\pi_i(\\delta_{ik} - \\pi_k)$. This can be written as $J_{\\boldsymbol{\\pi},\\mathbf{u}} = \\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top$.\nBy the chain rule, the Jacobian of $\\boldsymbol{\\pi}$ with respect to $\\mathbf{z}$ is:\n$$\nJ_{\\boldsymbol{\\pi},\\mathbf{z}} = \\frac{\\partial \\boldsymbol{\\pi}}{\\partial \\mathbf{z}} = \\frac{\\partial \\boldsymbol{\\pi}}{\\partial \\mathbf{u}} \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{z}} = \\left(\\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top\\right) \\frac{1}{\\tau} I = \\frac{1}{\\tau} \\left(\\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top\\right)\n$$\nSince this Jacobian matrix is symmetric, $(\\nabla_{\\mathbf{z}} \\boldsymbol{\\pi})^\\top = J_{\\boldsymbol{\\pi},\\mathbf{z}}$. Thus,\n$$\n\\nabla_{\\mathbf{z}} \\hat{y} = J_{\\boldsymbol{\\pi},\\mathbf{z}} \\mathbf{v} = \\frac{1}{\\tau} \\left(\\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top\\right) \\mathbf{v}\n$$\nThe $k$-th component of this gradient is:\n$$\n(\\nabla_{\\mathbf{z}} \\hat{y})_k = \\frac{1}{\\tau} \\left( \\pi_k v_k - \\pi_k \\sum_j \\pi_j v_j \\right) = \\frac{1}{\\tau} \\pi_k(v_k - \\mathbf{v}^\\top \\boldsymbol{\\pi})\n$$\nCombining the terms, the gradient of the loss is:\n$$\n\\mathbf{g} = \\nabla_{\\mathbf{z}} L = (\\hat{y} - y) \\nabla_{\\mathbf{z}} \\hat{y} = \\frac{\\hat{y} - y}{\\tau} \\left(\\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top\\right) \\mathbf{v}\n$$\nThe quantity to compute is the Euclidean norm of this vector, $\\left\\lVert \\mathbf{g} \\right\\rVert_2$.\n\n### Step 3: Compute the Hessian $\\nabla^2_{\\mathbf{z}_c} L(\\tau)$\n\nThe Hessian matrix $H_L = \\nabla^2_{\\mathbf{z}} L$ is the gradient of $\\nabla_{\\mathbf{z}} L$. Using the product rule for vector calculus:\n$$\nH_L = \\nabla_{\\mathbf{z}} \\left( (\\hat{y} - y) \\nabla_{\\mathbf{z}} \\hat{y} \\right) = (\\nabla_{\\mathbf{z}} \\hat{y})(\\nabla_{\\mathbf{z}} \\hat{y})^\\top + (\\hat{y} - y) \\nabla^2_{\\mathbf{z}} \\hat{y}\n$$\nLet $\\mathbf{g}_{\\hat{y}} = \\nabla_{\\mathbf{z}} \\hat{y}$. The first term is the outer product $\\mathbf{g}_{\\hat{y}}\\mathbf{g}_{\\hat{y}}^\\top$. The second term requires the Hessian of the prediction, $H_{\\hat{y}} = \\nabla^2_{\\mathbf{z}} \\hat{y}$.\nThe $(k,l)$-th entry of $H_{\\hat{y}}$ is $\\frac{\\partial^2 \\hat{y}}{\\partial z_k \\partial z_l}$. Let's differentiate the $k$-th component of $\\mathbf{g}_{\\hat{y}}$ with respect to $z_l$:\n$$\n(H_{\\hat{y}})_{kl} = \\frac{\\partial}{\\partial z_l} \\left( \\frac{1}{\\tau} \\pi_k(v_k - \\mathbf{v}^\\top \\boldsymbol{\\pi}) \\right)\n$$\nUsing the product rule and chain rule again:\n$$\n(H_{\\hat{y}})_{kl} = \\frac{1}{\\tau} \\left[ \\frac{\\partial \\pi_k}{\\partial z_l}(v_k - \\mathbf{v}^\\top \\boldsymbol{\\pi}) - \\pi_k \\frac{\\partial (\\mathbf{v}^\\top \\boldsymbol{\\pi})}{\\partial z_l} \\right]\n$$\nWe know $\\frac{\\partial \\pi_k}{\\partial z_l} = \\frac{1}{\\tau}\\pi_k(\\delta_{kl}-\\pi_l)$. The derivative of the expected value $\\bar{v} = \\mathbf{v}^\\top\\boldsymbol{\\pi}$ is:\n$$\n\\frac{\\partial (\\mathbf{v}^\\top \\boldsymbol{\\pi})}{\\partial z_l} = \\mathbf{v}^\\top \\frac{\\partial \\boldsymbol{\\pi}}{\\partial z_l} = \\sum_j v_j \\frac{\\partial \\pi_j}{\\partial z_l} = \\sum_j v_j \\frac{1}{\\tau} \\pi_j(\\delta_{jl}-\\pi_l) = \\frac{1}{\\tau} (v_l\\pi_l - \\pi_l \\sum_j v_j\\pi_j) = \\frac{1}{\\tau}\\pi_l(v_l - \\bar{v})\n$$\nSubstituting these into the expression for $(H_{\\hat{y}})_{kl}$:\n$$\n(H_{\\hat{y}})_{kl} = \\frac{1}{\\tau} \\left[ \\frac{1}{\\tau}\\pi_k(\\delta_{kl}-\\pi_l)(v_k - \\bar{v}) - \\pi_k \\frac{1}{\\tau}\\pi_l(v_l - \\bar{v}) \\right]\n$$\nLet's define a difference vector $\\mathbf{d} = \\mathbf{v} - \\bar{v}\\mathbf{1}$, so $d_k = v_k - \\bar{v}$.\n$$\n(H_{\\hat{y}})_{kl} = \\frac{1}{\\tau^2} \\left[ \\pi_k(\\delta_{kl}-\\pi_l) d_k - \\pi_k\\pi_l d_l \\right] = \\frac{1}{\\tau^2} \\left( \\pi_k d_k \\delta_{kl} - \\pi_k\\pi_l d_k - \\pi_k\\pi_l d_l \\right)\n$$\n$$\n(H_{\\hat{y}})_{kl} = \\frac{1}{\\tau^2} \\left( \\pi_k d_k \\delta_{kl} - \\pi_k\\pi_l (d_k + d_l) \\right)\n$$\nThis matrix is symmetric, as expected. The full Hessian of the loss is:\n$$\nH_L = \\mathbf{g}_{\\hat{y}}\\mathbf{g}_{\\hat{y}}^\\top + (\\hat{y} - y) H_{\\hat{y}}\n$$\nThe spectral norm of a symmetric matrix is its largest absolute eigenvalue: $\\left\\lVert H_L \\right\\rVert_2 = \\max_i |\\lambda_i(H_L)|$.\n\n### Step 4: Algorithm for Implementation\nFor each test case $(\\mathbf{z}_c, \\tau)$:\n1.  Define constants $B, \\mathbf{w}, b_0, y$. Let $\\mathbf{z}_c$ and $\\tau$ be the inputs for the current case. Let all vectors be column vectors.\n2.  Compute $\\mathbf{v} = B \\mathbf{w}$.\n3.  Compute scaled logits $\\mathbf{p} = \\mathbf{z}_c / \\tau$.\n4.  Compute $\\boldsymbol{\\pi} = \\text{Softmax}(\\mathbf{p})$, using a numerically stable implementation (subtracting the maximum logit before exponentiation).\n5.  Compute the prediction $\\hat{y} = \\mathbf{v}^\\top \\boldsymbol{\\pi} + b_0$.\n6.  Compute the prediction error $\\delta_L = \\hat{y} - y$.\n7.  Compute the gradient of the prediction $\\mathbf{g}_{\\hat{y}} = \\frac{1}{\\tau}(\\text{diag}(\\boldsymbol{\\pi}) - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top) \\mathbf{v}$. A simpler way is to compute $\\bar{v} = \\mathbf{v}^\\top \\boldsymbol{\\pi}$ and then the components of $\\mathbf{g}_{\\hat{y}}$ as $(\\mathbf{g}_{\\hat{y}})_k = \\frac{1}{\\tau} \\pi_k (v_k - \\bar{v})$.\n8.  Compute the loss gradient $\\mathbf{g} = \\delta_L \\mathbf{g}_{\\hat{y}}$.\n9.  Calculate the gradient norm $\\left\\lVert \\mathbf{g} \\right\\rVert_2$.\n10. Compute the Hessian of the prediction $H_{\\hat{y}}$. First compute $\\bar{v}$ and $\\mathbf{d} = \\mathbf{v} - \\bar{v}\\mathbf{1}$. Then construct the $M \\times M$ matrix $H_{\\hat{y}}$ with elements $(H_{\\hat{y}})_{kl} = \\frac{1}{\\tau^2} (\\pi_k d_k \\delta_{kl} - \\pi_k \\pi_l (d_k + d_l))$.\n11. Construct the loss Hessian $H_L = \\mathbf{g}_{\\hat{y}}\\mathbf{g}_{\\hat{y}}^\\top + \\delta_L H_{\\hat{y}}$.\n12. Compute the eigenvalues of the symmetric matrix $H_L$.\n13. Calculate the spectral norm of the Hessian as the maximum absolute eigenvalue.\n14. Store the gradient norm and Hessian norm for the current test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of computing the gradient and Hessian norms for a\n    temperature-controlled categorical encoding model.\n    \"\"\"\n\n    # Define fixed parameters\n    M = 4\n    d = 3\n    B = np.array([\n        [0.9, -0.4, 0.1],\n        [0.3, 0.8, -0.5],\n        [-0.6, 0.2, 0.7],\n        [0.5, -0.1, -0.3]\n    ])\n    w = np.array([[0.7], [-1.1], [0.9]])\n    b0 = 0.2\n    y = 0.5\n\n    # Define the test suite\n    test_cases = [\n        # (z_c, tau)\n        (np.array([1.2, -0.5, 0.3, 2.0]), 1.0),    # Happy path\n        (np.array([1.2, -0.5, 0.3, 2.0]), 2.0),    # Smooth regime\n        (np.array([1.2, -0.5, 0.3, 2.0]), 0.5),    # Transition regime\n        (np.array([1.2, -0.5, 0.3, 2.0]), 0.1),    # Near-hard regime\n        (np.array([0.0, 0.0, 0.0, 0.0]), 1.0),    # Edge case (equal logits)\n        (np.array([0.0, 0.0, 0.0, 0.0]), 0.01),   # Edge case (equal logits, low temp)\n    ]\n\n    results = []\n\n    # Pre-compute v = Bw as it is constant\n    v = B @ w\n\n    for z_c_flat, tau in test_cases:\n        z_c = z_c_flat.reshape(-1, 1)\n\n        # 1. Compute softmax weights pi\n        p = z_c / tau\n        # Numerically stable softmax\n        p_stable = p - np.max(p)\n        exp_p = np.exp(p_stable)\n        pi = exp_p / np.sum(exp_p)\n\n        # 2. Compute prediction y_hat and error delta_L\n        # y_hat = w.T @ B.T @ pi + b0, which is v.T @ pi + b0\n        y_hat = v.T @ pi + b0\n        delta_L = (y_hat - y).item()\n        \n        # 3. Compute gradient of L w.r.t. z_c\n        v_bar = (v.T @ pi).item()\n        g_y_hat = (1.0 / tau) * pi * (v - v_bar)\n        g_L = delta_L * g_y_hat\n        grad_norm = np.linalg.norm(g_L)\n\n        # 4. Compute Hessian of L w.r.t. z_c\n        \n        # First term of H_L: g_y_hat @ g_y_hat.T\n        H_L_term1 = g_y_hat @ g_y_hat.T\n        \n        # Second term of H_L: delta_L * H_y_hat\n        d_vec = v - v_bar\n        \n        # Construct H_y_hat\n        # (H_y_hat)_kl = (1/tau^2) * (pi_k*d_k*delta_kl - pi_k*pi_l*(d_k + d_l))\n        # This can be vectorized as:\n        # Diagonal part: diag(pi * d / tau^2)\n        # Off-diagonal part: - (1/tau^2) * (pi @ pi.T) element-wise-mult (d_vec @ 1.T + 1 @ d_vec.T)\n        pi_d_diag = np.diag((pi * d_vec).flatten())\n        pi_outer = pi @ pi.T\n        d_sum_outer = d_vec @ np.ones((1, M)) + np.ones((M, 1)) @ d_vec.T\n        \n        H_y_hat = (1.0 / tau**2) * (pi_d_diag - pi_outer * d_sum_outer)\n\n        H_L = H_L_term1 + delta_L * H_y_hat\n        \n        # 5. Compute spectral norm of the Hessian\n        # For a symmetric matrix, this is the max absolute eigenvalue.\n        eigenvalues = np.linalg.eigvalsh(H_L)\n        hess_norm = np.max(np.abs(eigenvalues))\n\n        results.append([grad_norm, hess_norm])\n\n    # Format output as specified: [[g1,h1],[g2,h2],...]\n    formatted_results = [f\"[{g},{h}]\" for g, h in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}