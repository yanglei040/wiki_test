{
    "hands_on_practices": [
        {
            "introduction": "首先，我们来练习Kullback-Leibler散度的基本计算。本练习  提供了两个简单的离散概率分布，让你可以直接应用定义式来计算出一个具体的数值。通过解决这个问题，你将对KL散度如何量化当使用一个近似分布来建模真实分布时所损失的信息，建立起一个坚实的基础。",
            "id": "1370233",
            "problem": "在信息论和机器学习领域，一个基本任务是衡量两个概率分布之间的差异。Kullback-Leibler (KL) 散度，或称相对熵，提供了这样一种度量，它量化了当用一个分布来近似另一个分布时所损失的信息。\n\n考虑一个可以取三个不同结果之一的分类随机变量，其结果由 $i \\in \\{1, 2, 3\\}$ 索引。决定这些结果的真实概率分布由分布 $P$ 给出，其中各个结果的概率为 $P(1) = \\frac{1}{2}$，$P(2) = \\frac{1}{4}$ 和 $P(3) = \\frac{1}{4}$。\n\n一位分析师提出了一个简化模型，由概率分布 $Q$ 描述，其中相应的概率为 $Q(1) = \\frac{2}{5}$，$Q(2) = \\frac{2}{5}$ 和 $Q(3) = \\frac{1}{5}$。\n\n$Q$ 相对于 $P$ 的 KL 散度，记作 $D_{KL}(P || Q)$，它衡量了当真实分布为 $P$ 时，假设分布为 $Q$ 所带来的低效率。其定义如下：\n$$\nD_{KL}(P || Q) = \\sum_{i=1}^{3} P(i) \\log \\left( \\frac{P(i)}{Q(i)} \\right)\n$$\n使用自然对数（以 $e$ 为底），计算 $D_{KL}(P || Q)$ 的数值。将您的最终答案四舍五入到四位有效数字。",
            "solution": "我们使用定义\n$$\nD_{KL}(P\\|Q)=\\sum_{i=1}^{3}P(i)\\ln\\!\\left(\\frac{P(i)}{Q(i)}\\right).\n$$\n给定 $P(1)=\\frac{1}{2}$，$P(2)=\\frac{1}{4}$，$P(3)=\\frac{1}{4}$ 以及 $Q(1)=\\frac{2}{5}$，$Q(2)=\\frac{2}{5}$，$Q(3)=\\frac{1}{5}$，计算每个比率：\n$$\n\\frac{P(1)}{Q(1)}=\\frac{\\frac{1}{2}}{\\frac{2}{5}}=\\frac{5}{4},\\quad\n\\frac{P(2)}{Q(2)}=\\frac{\\frac{1}{4}}{\\frac{2}{5}}=\\frac{5}{8},\\quad\n\\frac{P(3)}{Q(3)}=\\frac{\\frac{1}{4}}{\\frac{1}{5}}=\\frac{5}{4}.\n$$\n因此，\n$$\nD_{KL}(P\\|Q)=\\frac{1}{2}\\ln\\!\\left(\\frac{5}{4}\\right)+\\frac{1}{4}\\ln\\!\\left(\\frac{5}{8}\\right)+\\frac{1}{4}\\ln\\!\\left(\\frac{5}{4}\\right)\n=\\frac{3}{4}\\ln\\!\\left(\\frac{5}{4}\\right)+\\frac{1}{4}\\ln\\!\\left(\\frac{5}{8}\\right).\n$$\n使用对数性质合并：\n$$\nD_{KL}(P\\|Q)=\\frac{1}{4}\\left[3\\ln\\!\\left(\\frac{5}{4}\\right)+\\ln\\!\\left(\\frac{5}{8}\\right)\\right]\n=\\frac{1}{4}\\ln\\!\\left(\\left(\\frac{5}{4}\\right)^{3}\\cdot\\frac{5}{8}\\right)\n=\\frac{1}{4}\\ln\\!\\left(\\frac{625}{512}\\right).\n$$\n使用 $\\frac{625}{512}=\\frac{5^{4}}{2^{9}}$，我们还得到\n$$\nD_{KL}(P\\|Q)=\\frac{1}{4}\\left(4\\ln 5-9\\ln 2\\right)=\\ln 5-\\frac{9}{4}\\ln 2.\n$$\n数值上，使用自然对数，\n$$\n\\ln 5\\approx 1.6094379124,\\quad \\ln 2\\approx 0.6931471806,\n$$\n所以\n$$\nD_{KL}(P\\|Q)\\approx 1.6094379124-\\frac{9}{4}\\times 0.6931471806\\approx 0.0498567562.\n$$\n四舍五入到四位有效数字得到 $0.04986$。",
            "answer": "$$\\boxed{0.04986}$$"
        },
        {
            "introduction": "除了简单的计算，KL散度的真正威力在于它在优化问题中的应用。在这个练习中 ，我们将使用KL散度作为损失函数，为一个简单的模型（几何分布）寻找最优参数，以最佳地逼近一个给定的数据分布（均匀分布）。这种最小化KL散度的原则是许多高级机器学习方法（如变分推断）的基石。",
            "id": "1370268",
            "problem": "一位工程师的任务是为一个生成整数的过程建模。经验数据表明，该过程从集合 $\\{1, 2, \\dots, N\\}$ 中均匀地生成整数，其中 $N$ 是一个已知的正整数。令这个真实分布由概率质量函数（PMF）$P(k)$ 表示。\n\n出于分析和计算简便性的考虑，该工程师希望用几何分布来近似这个均匀分布，该几何分布由 PMF $Q(k)$ 表示。几何分布的 PMF 定义在正整数集合 $\\{1, 2, 3, \\dots\\}$ 上，其形式为 $Q(k|p) = p(1-p)^{k-1}$，其中 $p$ 是表示成功概率的参数，满足 $0  p  1$。\n\n为了找到最佳近似，工程师决定选择参数 $p$ 以最小化使用 $Q$ 替代 $P$ 所造成的信息损失。这种损失由库尔贝克-莱布勒（KL）散度量化，定义如下：\n$$\nD_{\\text{KL}}(P || Q) = \\sum_{k} P(k) \\ln\\left(\\frac{P(k)}{Q(k|p)}\\right)\n$$\n其中求和遍及所有使得 $P(k)  0$ 的整数 $k$，$\\ln$ 表示自然对数。\n\n确定使此 KL 散度最小化的参数 $p$ 的值。请用一个关于 $N$ 的封闭形式解析表达式来表示你的答案。",
            "solution": "设 $P$ 是在 $\\{1,\\dots,N\\}$ 上的均匀 PMF，因此对于 $k=1,\\dots,N$，$P(k)=\\frac{1}{N}$，否则为 $0$。设对于 $k\\in\\{1,2,\\dots\\}$，$Q(k|p)=p(1-p)^{k-1}$。从 $P$ 到 $Q$ 的库尔贝克-莱布勒散度是\n$$\nD_{\\text{KL}}(P\\|Q)=\\sum_{k=1}^{N}\\frac{1}{N}\\ln\\!\\left(\\frac{\\frac{1}{N}}{p(1-p)^{\\,k-1}}\\right)\n=\\frac{1}{N}\\sum_{k=1}^{N}\\left[\\ln\\!\\left(\\frac{1}{N}\\right)-\\ln p-(k-1)\\ln(1-p)\\right].\n$$\n计算平均值，使用 $\\sum_{k=1}^{N}(k-1)=\\frac{N(N-1)}{2}$ 可得\n$$\nD_{\\text{KL}}(P\\|Q)=\\ln\\!\\left(\\frac{1}{N}\\right)-\\ln p-\\left(\\frac{N-1}{2}\\right)\\ln(1-p).\n$$\n为了对 $p\\in(0,1)$ 进行最小化，我们求导并令其为零：\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}p}D_{\\text{KL}}(P\\|Q)=-\\frac{1}{p}-\\left(\\frac{N-1}{2}\\right)\\frac{\\mathrm{d}}{\\mathrm{d}p}\\ln(1-p)\n=-\\frac{1}{p}+\\frac{N-1}{2(1-p)}=0.\n$$\n求解 $p$ 可得\n$$\n\\frac{N-1}{2(1-p)}=\\frac{1}{p}\\quad\\Longrightarrow\\quad (N-1)p=2(1-p)\\quad\\Longrightarrow\\quad p(N+1)=2\\quad\\Longrightarrow\\quad p=\\frac{2}{N+1}.\n$$\n二阶导数，\n$$\n\\frac{\\mathrm{d}^{2}}{\\mathrm{d}p^{2}}D_{\\text{KL}}(P\\|Q)=\\frac{1}{p^{2}}+\\frac{N-1}{2(1-p)^{2}},\n$$\n对于 $p\\in(0,1)$ 是正的，这确保了该驻点是唯一的最小值点。因此，使 $D_{\\text{KL}}(P\\|Q)$ 最小化的 $p$ 的值为 $p=\\frac{2}{N+1}$。",
            "answer": "$$\\boxed{\\frac{2}{N+1}}$$"
        },
        {
            "introduction": "最后，让我们将所学知识应用于深度学习中的一个实际问题。这个动手编程练习  要求你构建一个显著性图，以可视化输入图像的哪些部分对分类器的预测最为关键。通过测量在遮挡部分图像前后模型输出的KL散度，我们可以直接量化并定位输入的敏感区域，这是一种强大的模型可解释性技术。",
            "id": "3140423",
            "problem": "给定一个离散多类分类器，它由一个线性变换后接softmax函数定义，以及一个用于遮挡输入图像中矩形区域的程序。目标是通过计算每个遮挡位置的显著性值来量化模型预测对输入遮挡的敏感性。该显著性值基于原始预测分布与遮挡后预测分布之间的Kullback-Leibler散度。然后，您需要将这些显著性值聚合为每个测试用例的简单度量指标。\n\n从以下基本概念开始：\n- Softmax函数通过对指数进行归一化，将一个实值logits向量映射到类别上的概率分布，使得所有条目都为非负且总和为一。\n- Kullback-Leibler散度是衡量定义在相同离散支撑集上的两个概率分布之间差异的度量，源于信息论原理。\n\n将图像视为一个实值矩阵 $x \\in \\mathbb{R}^{H \\times W}$，分类器由一个权重矩阵 $W \\in \\mathbb{R}^{C \\times HW}$ 和一个偏置向量 $b \\in \\mathbb{R}^{C}$ 参数化。图像按行主序被扁平化为一个向量 $v \\in \\mathbb{R}^{HW}$，使用映射 $v_{j} = x_{i,k}$，其中 $j = i \\cdot W + k$，$i \\in \\{0,\\dots,H-1\\}$ 且 $k \\in \\{0,\\dots,W-1\\}$。分类器计算logits $\\ell = W v + b$，预测的类别分布 $p$ 是 $\\ell$ 的softmax。为了评估位置 $(r,c)$ 处的敏感性，您需要遮挡一个大小为 $P \\times Q$ 的矩形区域，其左上角位于 $(r,c)$，方法是将该区域内的 $x_{i,k}$ 替换为一个常数基线值 $b_{\\text{mask}} \\in \\mathbb{R}$。令 $x^{(r,c)}$ 表示被遮挡的图像，其预测分布为 $q^{(r,c)}$。将在 $(r,c)$ 处的显著性定义为从 $p$ 到 $q^{(r,c)}$ 的Kullback-Leibler散度。显著性图是在所有矩形区域能完整放入的 $(r,c)$ 位置上这些值的矩阵：$r \\in \\{0,\\dots,H-P\\}$ 且 $c \\in \\{0,\\dots,W-Q\\}$。\n\n您的任务：\n1. 根据未遮挡的图像计算原始分布 $p$。\n2. 对于每个有效的遮挡位置 $(r,c)$，计算 $q^{(r,c)}$ 和相应的显著性值。\n3. 聚合显著性图以生成度量指标：所有位置中的最大显著性值、平均显著性值，以及最大显著性首次出现（按行主序）的坐标 $(r^{\\star},c^{\\star})$。\n\n注意：\n- 使用行主序扁平化，并确保在计算softmax时数值稳定。\n- 本问题不涉及物理单位、角度或百分比。\n- 当多个位置共享相同的最大显著性值时，选择 $(r^{\\star},c^{\\star})$ 为行主序中字典序最小的坐标对，即从上到下、从左到右扫描时遇到的第一个。\n\n测试套件：\n为以下三个测试用例实现您的程序。在所有情况下，使用行主序扁平化来形成 $v$，并假设softmax应用于logits以产生严格为正的概率分布。\n\n- 测试用例1（通用“顺利”路径）：\n  - 维度：$H = 4$, $W = 4$, $C = 3$，矩形区域大小 $P = 2$, $Q = 2$，基线值 $b_{\\text{mask}} = 0.0$。\n  - 图像：\n    $$\n    x = \\begin{bmatrix}\n    0.0  0.5  1.0  -0.5 \\\\\n    1.0  0.0  -1.0  0.5 \\\\\n    0.3  -0.2  0.7  1.2 \\\\\n    0.0  0.0  0.5  -0.3\n    \\end{bmatrix}.\n    $$\n  - 分类器参数：\n    - 定义整数序列 $s = [0,1,2,\\dots,15]$。\n    - 权重：\n      $$\n      W^{(0)} = 0.1 \\cdot s,\\quad W^{(1)} = -0.05 \\cdot s,\\quad W^{(2)} = 0.02 \\cdot s,\n      $$\n      其中每个 $W^{(k)}$ 是 $W$ 的一行，且 $W \\in \\mathbb{R}^{3 \\times 16}$。\n    - 偏置：\n      $$\n      b = [0.1,\\,-0.2,\\,0.3].\n      $$\n\n- 测试用例2（遮挡下无变化的边界情况）：\n  - 维度：$H = 3$, $W = 3$, $C = 2$，矩形区域大小 $P = 1$, $Q = 1$，基线值 $b_{\\text{mask}} = 0.5$。\n  - 图像：\n    $$\n    x_{i,j} = 0.5 \\text{ 对于所有 } i \\in \\{0,1,2\\},\\; j \\in \\{0,1,2\\}.\n    $$\n  - 分类器参数（$W \\in \\mathbb{R}^{2 \\times 9}$，$b \\in \\mathbb{R}^{2}$）：\n    - 权重：\n      $$\n      W^{(0)} = [0.2,\\,-0.1,\\,0.1,\\,-0.05,\\,0.3,\\,0.0,\\,0.05,\\,-0.1,\\,0.02],\n      $$\n      $$\n      W^{(1)} = [-0.1,\\,0.15,\\,-0.2,\\,0.1,\\,-0.05,\\,0.2,\\,-0.05,\\,0.1,\\,-0.02].\n      $$\n    - 偏置：\n      $$\n      b = [0.0,\\,0.0].\n      $$\n\n- 测试用例3（强调强局部权重的边缘情况）：\n  - 维度：$H = 5$, $W = 5$, $C = 2$，矩形区域大小 $P = 3$, $Q = 3$，基线值 $b_{\\text{mask}} = 0.0$。\n  - 图像：\n    $$\n    x_{i,j} = 1.0 \\text{ 对于所有 } i \\in \\{0,1,2,3,4\\},\\; j \\in \\{0,1,2,3,4\\}.\n    $$\n  - 分类器参数（$W \\in \\mathbb{R}^{2 \\times 25}$，$b \\in \\mathbb{R}^{2}$）：\n    - 类别0的权重：\n      $$\n      W^{(0)}_{j} =\n      \\begin{cases}\n      2.0  \\text{如果对应于 } j \\text{ 的像素位于左上角 } 3\\times 3 \\text{ 的块中}, \\\\\n      0.1  \\text{其他情况},\n      \\end{cases}\n      $$\n      其中 $j$ 是 $5 \\times 5$ 图像按行主序扁平化后的索引。\n    - 类别1的权重：\n      $$\n      W^{(1)}_{j} = -0.05 \\text{ for all } j.\n      $$\n    - 偏置：\n      $$\n      b = [0.0,\\,0.0].\n      $$\n\n要求的最终输出格式：\n您的程序应生成一行输出，其中包含一个逗号分隔的列表，用方括号括起来，每个测试用例的结果本身也是一个形式为 $[ \\text{max\\_kl}, \\text{mean\\_kl}, r^{\\star}, c^{\\star} ]$ 的列表。例如，输出应如下所示：\n$$\n[ [a_1,\\,m_1,\\,r_1,\\,c_1],\\,[a_2,\\,m_2,\\,r_2,\\,c_2],\\,[a_3,\\,m_3,\\,r_3,\\,c_3] ].\n$$",
            "solution": "该问题定义明确，具有科学依据，并为获得唯一解提供了所有必要信息。它要求通过遮挡输入图像的部分区域，并使用Kullback-Leibler (KL) 散度测量输出概率分布的变化，来为一个简单的线性分类器计算显著性图。\n\n解法如下：首先，我们定义分类器的前向传播过程，该过程将输入图像转换为类别概率分布。其次，我们系统地遮挡输入图像的矩形区域，并计算由此产生的扰动概率分布。第三，对于每次遮挡，我们计算原始分布和扰动分布之间的KL散度，以此作为我们的显著性度量。最后，我们聚合这些显著性值，以找到最大值、平均值以及最大值的位置。\n\n**1. 分类器前向传播**\n\n分类器对输入图像 $x \\in \\mathbb{R}^{H \\times W}$ 进行操作，其中 $H$ 是高度，$W$ 是宽度。该过程包括三个步骤：\n\n- **扁平化**：使用行主序将二维图像矩阵 $x$ 转换为一维向量 $v \\in \\mathbb{R}^{HW}$。图像坐标 $(i,k)$ 处的元素（其中 $i \\in \\{0, \\dots, H-1\\}$ 且 $k \\in \\{0, \\dots, W-1\\}$）被映射到向量索引 $j = i \\cdot W + k$。\n\n- **Logit计算**：扁平化向量 $v$ 通过一个由权重矩阵 $W \\in \\mathbb{R}^{C \\times HW}$ 和偏置向量 $b \\in \\mathbb{R}^{C}$ 定义的线性层，其中 $C$ 是类别数。这会计算出logits向量 $\\ell \\in \\mathbb{R}^{C}$：\n$$\n\\ell = W v + b\n$$\n\n- **Softmax激活**：使用softmax函数将logits转换为概率分布 $p \\in \\mathbb{R}^{C}$。对于每个类别 $i \\in \\{1, \\dots, C\\}$，概率 $p_i$ 由以下公式给出：\n$$\np_i = \\text{softmax}(\\ell)_i = \\frac{e^{\\ell_i}}{\\sum_{k=1}^{C} e^{\\ell_k}}\n$$\n为确保对上溢和下溢的数值稳定性，一种常见的做法是在求幂之前从所有logits中减去最大logit值：\n$$\np_i = \\frac{e^{\\ell_i - \\max(\\ell)}}{\\sum_{k=1}^{C} e^{\\ell_k - \\max(\\ell)}}\n$$\n此变换产生相同的结果，但在更稳定的数值范围内操作。\n\n**2. 通过遮挡和Kullback-Leibler散度计算显著性**\n\n分析的核心是测量模型对输入部分内容的敏感性。这是通过遮挡一个区域并测量输出分布的相应变化来实现的。\n\n- **遮挡**：对于每个有效的左上角坐标 $(r, c)$，其中 $r \\in \\{0, \\dots, H-P\\}$ 且 $c \\in \\{0, \\dots, W-Q\\}$，一个大小为 $P \\times Q$ 的矩形区域被遮挡。遮挡意味着将此区域内的所有像素值 $x_{i,k}$（即，对于 $i \\in \\{r, \\dots, r+P-1\\}$ 和 $k \\in \\{c, \\dots, c+Q-1\\}$）替换为一个恒定的基线值 $b_{\\text{mask}}$。这将创建一个新的、被遮挡的图像 $x^{(r,c)}$。\n\n- **Kullback-Leibler (KL) 散度**：被遮挡的图像 $x^{(r,c)}$ 通过分类器以获得一个新的概率分布 $q^{(r,c)}$。在 $(r,c)$ 处的遮挡显著性定义为从原始分布 $p$ 到被遮挡分布 $q^{(r,c)}$ 的KL散度。在相同的支撑集 $\\{1, \\dots, C\\}$ 上，两个离散概率分布 $p$ 和 $q$ 之间的KL散度公式为：\n$$\nD_{KL}(p || q) = \\sum_{i=1}^{C} p_i \\log \\left( \\frac{p_i}{q_i} \\right) = \\sum_{i=1}^{C} p_i (\\log p_i - \\log q_i)\n$$\n问题陈述保证了softmax的输出是一个严格为正的分布，确保了对于所有 $i$，$q_i  0$，因此对数总是良定义的。\n\n**3. 算法步骤**\n\n对于每个测试用例，我们执行以下步骤：\n1.  定义测试用例的参数：图像维度 $(H, W)$、类别数 $C$、矩形区域维度 $(P, Q)$、基线值 $b_{\\text{mask}}$、图像矩阵 $x$、权重矩阵 $W$ 和偏置向量 $b$。\n2.  通过对未遮挡的图像 $x$ 进行前向传播，计算原始概率分布 $p$。\n3.  初始化一个空的显著性图 $S \\in \\mathbb{R}^{(H-P+1) \\times (W-Q+1)}$。\n4.  遍历每个有效的左上角遮挡坐标 $(r, c)$，从 $r=0$ 到 $H-P$，从 $c=0$ 到 $W-Q$。\n    a. 创建原始图像的副本，称之为 $x_{\\text{occ}}$。\n    b. 修改 $x_{\\text{occ}}$，将从行 $r$到 $r+P-1$、列 $c$到 $c+Q-1$ 的子矩阵设置为值 $b_{\\text{mask}}$。\n    c. 对被遮挡的图像 $x_{\\text{occ}}$ 进行前向传播，计算扰动后的概率分布 $q^{(r,c)}$。\n    d. 计算显著性值 $s_{r,c} = D_{KL}(p || q^{(r,c)})$。\n    e. 将此值存储在显著性图中：$S_{r,c} = s_{r,c}$。\n5.  计算完所有显著性值后，将它们聚合以产生最终的度量指标：\n    a. **最大显著性**：$\\text{max\\_kl} = \\max_{r,c} S_{r,c}$。\n    b. **平均显著性**：$\\text{mean\\_kl} = \\frac{1}{(H-P+1)(W-Q+1)} \\sum_{r=0}^{H-P} \\sum_{c=0}^{W-Q} S_{r,c}$。\n    c. **最大值位置**：$(r^{\\star}, c^{\\star}) = \\text{argmin}_{(r,c)} \\{ (r,c) \\mid S_{r,c} = \\text{max\\_kl} \\}$，其中最小值按字典序（行主序）取。这对应于逐行扫描显著性图时最大值的首次出现位置。\n6.  收集当前测试用例的四个度量指标 $[\\text{max\\_kl}, \\text{mean\\_kl}, r^{\\star}, c^{\\star}]$。\n\n对所有提供的测试用例重复此过程。实现将利用数值库来高效处理矩阵运算、扁平化以及softmax和KL散度的计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import kl_div, softmax\n\ndef solve():\n    \"\"\"\n    Solves the saliency map computation problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1 (general “happy path”)\n        {\n            \"dims\": (4, 4, 3),  # H, W, C\n            \"patch_dims\": (2, 2),  # P, Q\n            \"baseline\": 0.0,\n            \"image\": np.array([\n                [0.0, 0.5, 1.0, -0.5],\n                [1.0, 0.0, -1.0, 0.5],\n                [0.3, -0.2, 0.7, 1.2],\n                [0.0, 0.0, 0.5, -0.3]\n            ]),\n            \"weights_def\": {\n                \"type\": \"sequence\",\n                \"coeffs\": [0.1, -0.05, 0.02]\n            },\n            \"bias\": np.array([0.1, -0.2, 0.3])\n        },\n        # Test Case 2 (boundary case with no change under occlusion)\n        {\n            \"dims\": (3, 3, 2),\n            \"patch_dims\": (1, 1),\n            \"baseline\": 0.5,\n            \"image\": np.full((3, 3), 0.5),\n            \"weights_def\": {\n                \"type\": \"explicit\",\n                \"matrix\": np.array([\n                    [0.2, -0.1, 0.1, -0.05, 0.3, 0.0, 0.05, -0.1, 0.02],\n                    [-0.1, 0.15, -0.2, 0.1, -0.05, 0.2, -0.05, 0.1, -0.02]\n                ])\n            },\n            \"bias\": np.array([0.0, 0.0])\n        },\n        # Test Case 3 (edge case stressing strong localized weights)\n        {\n            \"dims\": (5, 5, 2),\n            \"patch_dims\": (3, 3),\n            \"baseline\": 0.0,\n            \"image\": np.full((5, 5), 1.0),\n            \"weights_def\": {\n                \"type\": \"piecewise\",\n                \"def\": [\n                    {\"value\": 2.0, \"region\": (0, 0, 3, 3)}, # Class 0, val, r, c, h, w\n                    {\"value\": -0.05, \"all\": True} # Class 1\n                ]\n            },\n            \"bias\": np.array([0.0, 0.0])\n        }\n    ]\n\n    def classifier_forward_pass(image, weights, bias):\n        \"\"\"Computes the probability distribution for a given image.\"\"\"\n        H, W = image.shape\n        v = image.flatten('C') # Row-major flattening\n        logits = weights @ v + bias\n        # Using scipy.special.softmax handles numerical stability\n        return softmax(logits)\n\n    def generate_weights(case):\n        \"\"\"Generates the weight matrix based on the case definition.\"\"\"\n        H, W, C = case[\"dims\"]\n        HW = H * W\n        weights_def = case[\"weights_def\"]\n        \n        if weights_def[\"type\"] == \"explicit\":\n            return weights_def[\"matrix\"]\n        \n        W_matrix = np.zeros((C, HW))\n        if weights_def[\"type\"] == \"sequence\":\n            s = np.arange(HW)\n            for i, coeff in enumerate(weights_def[\"coeffs\"]):\n                W_matrix[i, :] = coeff * s\n        elif weights_def[\"type\"] == \"piecewise\":\n            # For Case 3\n            # Class 0 weights\n            W0 = np.full(HW, 0.1)\n            r, c, h, w = weights_def[\"def\"][0][\"region\"]\n            for i in range(r, r + h):\n                for k in range(c, c + w):\n                    idx = i * W + k\n                    W0[idx] = weights_def[\"def\"][0][\"value\"]\n            W_matrix[0, :] = W0\n            # Class 1 weights\n            W1 = np.full(HW, weights_def[\"def\"][1][\"value\"])\n            W_matrix[1, :] = W1\n        return W_matrix\n\n    results = []\n    for case in test_cases:\n        H, W, C = case[\"dims\"]\n        P, Q = case[\"patch_dims\"]\n        b_mask = case[\"baseline\"]\n        x = case[\"image\"]\n        weights = generate_weights(case)\n        bias = case[\"bias\"]\n        \n        # 1. Compute original distribution p\n        p_original = classifier_forward_pass(x, weights, bias)\n        \n        saliency_map = np.zeros((H - P + 1, W - Q + 1))\n        \n        # 2. Iterate through occlusions\n        for r in range(H - P + 1):\n            for c in range(W - Q + 1):\n                x_occluded = x.copy()\n                x_occluded[r:r+P, c:c+Q] = b_mask\n                \n                # Compute occluded distribution q\n                q_occluded = classifier_forward_pass(x_occluded, weights, bias)\n                \n                # Compute KL divergence (saliency)\n                # kl_div(p, q) calculates p * log(p/q) element-wise\n                saliency = np.sum(kl_div(p_original, q_occluded))\n                saliency_map[r, c] = saliency\n\n        # 3. Aggregate metrics\n        max_kl = np.max(saliency_map)\n        mean_kl = np.mean(saliency_map)\n        \n        # Find first occurrence of max_kl in row-major order\n        max_loc_flat = np.argmax(saliency_map)\n        r_star, c_star = np.unravel_index(max_loc_flat, saliency_map.shape)\n        \n        results.append([max_kl, mean_kl, int(r_star), int(c_star)])\n\n    # Final print statement in the exact required format.\n    # The default str() for a list includes spaces, which `','.join` will preserve.\n    # `f\"[{','.join(str(res) for res in results)}]` creates eg: '[[1, 2],[3, 4]]'\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}