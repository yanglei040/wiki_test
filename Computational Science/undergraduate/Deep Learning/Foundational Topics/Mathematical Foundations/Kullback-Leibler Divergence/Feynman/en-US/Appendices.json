{
    "hands_on_practices": [
        {
            "introduction": "Before applying Kullback-Leibler ($D_{KL}$) divergence in complex models, it is essential to understand its behavior with fundamental probability distributions. This practice involves applying the definition of $D_{KL}$ to the Poisson distribution, a common model for count-based data. By deriving a closed-form expression, you will gain an intuition for how the divergence directly relates to the underlying parameters of the distributions, moving beyond simple numerical plug-in exercises. ",
            "id": "1370282",
            "problem": "A data scientist is analyzing the number of times users interact with a new feature on a mobile application within a one-hour window. The true underlying data-generating process is modeled by a random variable $K_1$ that follows a Poisson distribution with a rate parameter $\\lambda_1 > 0$. The corresponding probability mass function is $P_1(k) = \\frac{\\lambda_1^k e^{-\\lambda_1}}{k!}$ for $k = 0, 1, 2, \\dots$.\n\nHowever, the scientist is working with an older, simplified model which approximates the process with a different random variable $K_2$. This model also assumes a Poisson distribution, but with a rate parameter $\\lambda_2 > 0$. Its probability mass function is $P_2(k) = \\frac{\\lambda_2^k e^{-\\lambda_2}}{k!}$ for $k = 0, 1, 2, \\dots$.\n\nTo quantify the information lost when using the approximate model ($P_2$) to represent the true process ($P_1$), the scientist decides to compute the Kullback-Leibler (KL) divergence. For two discrete probability distributions $P$ and $Q$ defined on the same sample space $\\mathcal{X}$, the KL divergence from $P$ to $Q$ is given by:\n$$D_{KL}(P || Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\ln\\left(\\frac{P(x)}{Q(x)}\\right)$$\n\nFind a closed-form analytic expression for the KL divergence $D_{KL}(P_1 || P_2)$ in terms of the rate parameters $\\lambda_1$ and $\\lambda_2$.",
            "solution": "We are given two Poisson distributions with parameters $\\lambda_{1} > 0$ and $\\lambda_{2} > 0$:\n$$\nP_{1}(k) = \\frac{\\lambda_{1}^{k}\\exp(-\\lambda_{1})}{k!}, \\quad P_{2}(k) = \\frac{\\lambda_{2}^{k}\\exp(-\\lambda_{2})}{k!}, \\quad k = 0, 1, 2, \\dots\n$$\nThe Kullback-Leibler divergence from $P_{1}$ to $P_{2}$ is defined as\n$$\nD_{KL}(P_{1} \\,\\|\\, P_{2}) = \\sum_{k=0}^{\\infty} P_{1}(k)\\,\\ln\\!\\left(\\frac{P_{1}(k)}{P_{2}(k)}\\right).\n$$\nWe first compute the ratio inside the logarithm:\n$$\n\\frac{P_{1}(k)}{P_{2}(k)} = \\frac{\\lambda_{1}^{k}\\exp(-\\lambda_{1})/k!}{\\lambda_{2}^{k}\\exp(-\\lambda_{2})/k!} = \\left(\\frac{\\lambda_{1}}{\\lambda_{2}}\\right)^{k}\\exp(\\lambda_{2}-\\lambda_{1}).\n$$\nTaking the natural logarithm yields\n$$\n\\ln\\!\\left(\\frac{P_{1}(k)}{P_{2}(k)}\\right) = k\\,\\ln\\!\\left(\\frac{\\lambda_{1}}{\\lambda_{2}}\\right) + (\\lambda_{2}-\\lambda_{1}).\n$$\nSubstituting into the KL divergence and separating terms,\n$$\nD_{KL}(P_{1} \\,\\|\\, P_{2}) = \\sum_{k=0}^{\\infty} P_{1}(k)\\left[k\\,\\ln\\!\\left(\\frac{\\lambda_{1}}{\\lambda_{2}}\\right) + (\\lambda_{2}-\\lambda_{1})\\right]\n= \\ln\\!\\left(\\frac{\\lambda_{1}}{\\lambda_{2}}\\right)\\sum_{k=0}^{\\infty} k\\,P_{1}(k) + (\\lambda_{2}-\\lambda_{1})\\sum_{k=0}^{\\infty} P_{1}(k).\n$$\nUsing the facts that $\\sum_{k=0}^{\\infty} P_{1}(k) = 1$ and that the mean of a $\\operatorname{Poisson}(\\lambda_{1})$ random variable is $\\sum_{k=0}^{\\infty} k\\,P_{1}(k) = \\lambda_{1}$, we obtain\n$$\nD_{KL}(P_{1} \\,\\|\\, P_{2}) = \\lambda_{1}\\,\\ln\\!\\left(\\frac{\\lambda_{1}}{\\lambda_{2}}\\right) + (\\lambda_{2}-\\lambda_{1}).\n$$\nThis is the desired closed-form expression in terms of $\\lambda_{1}$ and $\\lambda_{2}$.",
            "answer": "$$\\boxed{\\lambda_{1}\\ln\\!\\left(\\frac{\\lambda_{1}}{\\lambda_{2}}\\right)+\\lambda_{2}-\\lambda_{1}}$$"
        },
        {
            "introduction": "One of the most powerful roles of $D_{KL}$ divergence in machine learning is serving as a loss function to find the best possible approximation for a target distribution. This exercise challenges you to find the optimal parameter for a simple geometric distribution that best mimics a uniform distribution by minimizing the $D_{KL}$ divergence between them. This task provides a tangible example of the core principle behind variational inference and models like Variational Autoencoders (VAEs), where we train a model by minimizing a $D_{KL}$ term. ",
            "id": "1370268",
            "problem": "An engineer is tasked with modeling a process that generates integers. Empirical data suggests that the process produces integers uniformly from the set $\\{1, 2, \\dots, N\\}$, where $N$ is a known positive integer. Let this true distribution be denoted by the probability mass function (PMF) $P(k)$.\n\nFor reasons of analytic and computational simplicity, the engineer wants to approximate this uniform distribution with a geometric distribution, denoted by the PMF $Q(k)$. The geometric distribution's PMF is defined on the set of positive integers $\\{1, 2, 3, \\dots\\}$ as $Q(k|p) = p(1-p)^{k-1}$, where $p$ is the parameter representing the probability of success, with $0 < p < 1$.\n\nTo find the best possible approximation, the engineer decides to select the parameter $p$ that minimizes the information loss from using $Q$ in place of $P$. This loss is quantified by the Kullback-Leibler (KL) divergence, defined as:\n$$\nD_{\\text{KL}}(P || Q) = \\sum_{k} P(k) \\ln\\left(\\frac{P(k)}{Q(k|p)}\\right)\n$$\nwhere the sum is taken over all integers $k$ for which $P(k) > 0$, and $\\ln$ denotes the natural logarithm.\n\nDetermine the value of the parameter $p$ that minimizes this KL divergence. Express your answer as a single closed-form analytic expression in terms of $N$.",
            "solution": "Let $P$ be the uniform PMF on $\\{1,\\dots,N\\}$, so $P(k)=\\frac{1}{N}$ for $k=1,\\dots,N$ and $0$ otherwise. Let $Q(k|p)=p(1-p)^{k-1}$ for $k\\in\\{1,2,\\dots\\}$. The Kullback-Leibler divergence from $P$ to $Q$ is\n$$\nD_{\\text{KL}}(P\\|Q)=\\sum_{k=1}^{N}\\frac{1}{N}\\ln\\!\\left(\\frac{\\frac{1}{N}}{p(1-p)^{\\,k-1}}\\right)\n=\\frac{1}{N}\\sum_{k=1}^{N}\\left[\\ln\\!\\left(\\frac{1}{N}\\right)-\\ln p-(k-1)\\ln(1-p)\\right].\n$$\nEvaluating the average, use $\\sum_{k=1}^{N}(k-1)=\\frac{N(N-1)}{2}$ to obtain\n$$\nD_{\\text{KL}}(P\\|Q)=\\ln\\!\\left(\\frac{1}{N}\\right)-\\ln p-\\left(\\frac{N-1}{2}\\right)\\ln(1-p).\n$$\nTo minimize with respect to $p\\in(0,1)$, differentiate and set to zero:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}p}D_{\\text{KL}}(P\\|Q)=-\\frac{1}{p}-\\left(\\frac{N-1}{2}\\right)\\frac{\\mathrm{d}}{\\mathrm{d}p}\\ln(1-p)\n=-\\frac{1}{p}+\\frac{N-1}{2(1-p)}=0.\n$$\nSolving for $p$ gives\n$$\n\\frac{N-1}{2(1-p)}=\\frac{1}{p}\\quad\\Longrightarrow\\quad (N-1)p=2(1-p)\\quad\\Longrightarrow\\quad p(N+1)=2\\quad\\Longrightarrow\\quad p=\\frac{2}{N+1}.\n$$\nThe second derivative,\n$$\n\\frac{\\mathrm{d}^{2}}{\\mathrm{d}p^{2}}D_{\\text{KL}}(P\\|Q)=\\frac{1}{p^{2}}+\\frac{N-1}{2(1-p)^{2}},\n$$\nis positive for $p\\in(0,1)$, ensuring this stationary point is the unique minimizer. Therefore, the value of $p$ that minimizes $D_{\\text{KL}}(P\\|Q)$ is $p=\\frac{2}{N+1}$.",
            "answer": "$$\\boxed{\\frac{2}{N+1}}$$"
        },
        {
            "introduction": "Beyond model training, $D_{KL}$ divergence is a crucial tool for interpreting and understanding the behavior of complex deep learning systems. In this advanced, hands-on coding task, you will implement a saliency mapping technique, a method used in Explainable AI (XAI). You will use $D_{KL}$ divergence to quantify how much a vision model's prediction changes when parts of an input image are occluded, thereby highlighting the regions most critical to the model's decision. ",
            "id": "3140423",
            "problem": "You are given a discrete multi-class classifier defined by a linear transformation followed by the softmax function and a procedure to occlude a rectangular region in an input image. The goal is to quantify the sensitivity of the model’s predictions to input occlusions by computing, for each occlusion location, a saliency value based on the Kullback-Leibler divergence between the original predicted distribution and the occluded predicted distribution. You will then aggregate these saliency values into simple metrics per test case.\n\nStart from the following fundamental bases:\n- The softmax function maps a real-valued vector of logits to a probability distribution over classes by normalizing exponentials so that all entries are nonnegative and sum to one.\n- The Kullback-Leibler divergence is a measure of the discrepancy between two probability distributions defined on the same discrete support, derived from principles of information theory.\n\nConsider an image as a real-valued matrix $x \\in \\mathbb{R}^{H \\times W}$ and a classifier parameterized by a weight matrix $W \\in \\mathbb{R}^{C \\times HW}$ and a bias vector $b \\in \\mathbb{R}^{C}$. The image is flattened in row-major order into a vector $v \\in \\mathbb{R}^{HW}$ using the mapping $v_{j} = x_{i,k}$ with $j = i \\cdot W + k$, where $i \\in \\{0,\\dots,H-1\\}$ and $k \\in \\{0,\\dots,W-1\\}$. The classifier computes logits $\\ell = W v + b$, and the predicted class distribution $p$ is the softmax of $\\ell$. To evaluate the sensitivity at a location $(r,c)$, you occlude a rectangular patch of size $P \\times Q$ whose top-left corner is at $(r,c)$ by replacing $x_{i,k}$ within that patch with a constant baseline $b_{\\text{mask}} \\in \\mathbb{R}$. Let $x^{(r,c)}$ denote the occluded image and $q^{(r,c)}$ its predicted distribution. Define the saliency at $(r,c)$ to be the Kullback-Leibler divergence from $p$ to $q^{(r,c)}$. The saliency map is the matrix of these values over all $(r,c)$ where the patch fits: $r \\in \\{0,\\dots,H-P\\}$ and $c \\in \\{0,\\dots,W-Q\\}$.\n\nYour tasks:\n1. Compute the original distribution $p$ from the unoccluded image.\n2. For each valid occlusion location $(r,c)$, compute $q^{(r,c)}$ and the corresponding saliency value.\n3. Aggregate the saliency map to produce the metrics: the maximum saliency value over all positions, the mean saliency value, and the coordinates $(r^{\\star},c^{\\star})$ of the first occurrence (in row-major order) of the maximum saliency.\n\nNotes:\n- Use row-major flattening and ensure numerical stability in computing the softmax.\n- No physical units, angles, or percentages are involved in this problem.\n- When multiple locations share the same maximum saliency, select $(r^{\\star},c^{\\star})$ to be the lexicographically smallest pair in row-major order, i.e., the first one encountered when scanning rows from top to bottom and columns from left to right.\n\nTest Suite:\nImplement your program for the following three test cases. In all cases, use row-major flattening for forming $v$ and assume the softmax is applied to logits to produce a strictly positive probability distribution.\n\n- Test Case 1 (general “happy path”):\n  - Dimensions: $H = 4$, $W = 4$, $C = 3$, patch sizes $P = 2$, $Q = 2$, baseline $b_{\\text{mask}} = 0.0$.\n  - Image:\n    $$\n    x = \\begin{bmatrix}\n    0.0 & 0.5 & 1.0 & -0.5 \\\\\n    1.0 & 0.0 & -1.0 & 0.5 \\\\\n    0.3 & -0.2 & 0.7 & 1.2 \\\\\n    0.0 & 0.0 & 0.5 & -0.3\n    \\end{bmatrix}.\n    $$\n  - Classifier parameters:\n    - Define the sequence of integers $s = [0,1,2,\\dots,15]$.\n    - Weights:\n      $$\n      W^{(0)} = 0.1 \\cdot s,\\quad W^{(1)} = -0.05 \\cdot s,\\quad W^{(2)} = 0.02 \\cdot s,\n      $$\n      where each $W^{(k)}$ is a row of $W$ and $W \\in \\mathbb{R}^{3 \\times 16}$.\n    - Bias:\n      $$\n      b = [0.1,\\,-0.2,\\,0.3].\n      $$\n\n- Test Case 2 (boundary case with no change under occlusion):\n  - Dimensions: $H = 3$, $W = 3$, $C = 2$, patch sizes $P = 1$, $Q = 1$, baseline $b_{\\text{mask}} = 0.5$.\n  - Image:\n    $$\n    x_{i,j} = 0.5 \\text{ for all } i \\in \\{0,1,2\\},\\; j \\in \\{0,1,2\\}.\n    $$\n  - Classifier parameters ($W \\in \\mathbb{R}^{2 \\times 9}$, $b \\in \\mathbb{R}^{2}$):\n    - Weights:\n      $$\n      W^{(0)} = [0.2,\\,-0.1,\\,0.1,\\,-0.05,\\,0.3,\\,0.0,\\,0.05,\\,-0.1,\\,0.02],\n      $$\n      $$\n      W^{(1)} = [-0.1,\\,0.15,\\,-0.2,\\,0.1,\\,-0.05,\\,0.2,\\,-0.05,\\,0.1,\\,-0.02].\n      $$\n    - Bias:\n      $$\n      b = [0.0,\\,0.0].\n      $$\n\n- Test Case 3 (edge case stressing strong localized weights):\n  - Dimensions: $H = 5$, $W = 5$, $C = 2$, patch sizes $P = 3$, $Q = 3$, baseline $b_{\\text{mask}} = 0.0$.\n  - Image:\n    $$\n    x_{i,j} = 1.0 \\text{ for all } i \\in \\{0,1,2,3,4\\},\\; j \\in \\{0,1,2,3,4\\}.\n    $$\n  - Classifier parameters ($W \\in \\mathbb{R}^{2 \\times 25}$, $b \\in \\mathbb{R}^{2}$):\n    - Weights for class $0$:\n      $$\n      W^{(0)}_{j} =\n      \\begin{cases}\n      2.0 & \\text{if the pixel corresponding to } j \\text{ lies in the top-left } 3\\times 3 \\text{ block}, \\\\\n      0.1 & \\text{otherwise},\n      \\end{cases}\n      $$\n      where $j$ indexes the flattened row-major order of the $5 \\times 5$ image.\n    - Weights for class $1$:\n      $$\n      W^{(1)}_{j} = -0.05 \\text{ for all } j.\n      $$\n    - Bias:\n      $$\n      b = [0.0,\\,0.0].\n      $$\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is itself a list of the form $[ \\text{max\\_kl}, \\text{mean\\_kl}, r^{\\star}, c^{\\star} ]$. For example, the output should look like\n$$\n[ [a_1,\\,m_1,\\,r_1,\\,c_1],\\,[a_2,\\,m_2,\\,r_2,\\,c_2],\\,[a_3,\\,m_3,\\,r_3,\\,c_3] ].\n$$",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. It requires the computation of a saliency map for a simple linear classifier by occluding portions of an input image and measuring the change in the output probability distribution using the Kullback-Leibler (KL) divergence.\n\nThe solution proceeds as follows: First, we define the forward pass of the classifier, which transforms an input image into a class probability distribution. Second, we systematically occlude rectangular patches of the input image and compute the resulting perturbed probability distributions. Third, for each occlusion, we calculate the KL divergence between the original and perturbed distributions, which serves as our saliency measure. Finally, we aggregate these saliency values to find the maximum, mean, and the location of the maximum.\n\n**1. Classifier Forward Pass**\n\nThe classifier operates on an input image $x \\in \\mathbb{R}^{H \\times W}$, where $H$ is the height and $W$ is the width. The process involves three steps:\n\n- **Flattening**: The $2$D image matrix $x$ is converted into a $1$D vector $v \\in \\mathbb{R}^{HW}$ using row-major ordering. The element at image coordinates $(i,k)$ where $i \\in \\{0, \\dots, H-1\\}$ and $k \\in \\{0, \\dots, W-1\\}$ is mapped to the vector index $j = i \\cdot W + k$.\n\n- **Logit Calculation**: The flattened vector $v$ is passed through a linear layer defined by a weight matrix $W \\in \\mathbb{R}^{C \\times HW}$ and a bias vector $b \\in \\mathbb{R}^{C}$, where $C$ is the number of classes. This computes the logits vector $\\ell \\in \\mathbb{R}^{C}$:\n$$\n\\ell = W v + b\n$$\n\n- **Softmax Activation**: The logits are transformed into a probability distribution $p \\in \\mathbb{R}^{C}$ using the softmax function. For each class $i \\in \\{1, \\dots, C\\}$, the probability $p_i$ is given by:\n$$\np_i = \\text{softmax}(\\ell)_i = \\frac{e^{\\ell_i}}{\\sum_{k=1}^{C} e^{\\ell_k}}\n$$\nTo ensure numerical stability against overflow and underflow, a common practice is to subtract the maximum logit value from all logits before exponentiation:\n$$\np_i = \\frac{e^{\\ell_i - \\max(\\ell)}}{\\sum_{k=1}^{C} e^{\\ell_k - \\max(\\ell)}}\n$$\nThis transformation yields the same result but operates on a more stable numerical range.\n\n**2. Saliency via Occlusion and Kullback-Leibler Divergence**\n\nThe core of the analysis is to measure the model's sensitivity to parts of the input. This is achieved by occluding a patch and measuring the resulting change in the output distribution.\n\n- **Occlusion**: For each valid top-left coordinate $(r, c)$, where $r \\in \\{0, \\dots, H-P\\}$ and $c \\in \\{0, \\dots, W-Q\\}$, a rectangular patch of size $P \\times Q$ is occluded. Occlusion means replacing all pixel values $x_{i,k}$ within this patch (i.e., for $i \\in \\{r, \\dots, r+P-1\\}$ and $k \\in \\{c, \\dots, c+Q-1\\}$) with a constant baseline value $b_{\\text{mask}}$. This creates a new, occluded image $x^{(r,c)}$.\n\n- **Kullback-Leibler (KL) Divergence**: The occluded image $x^{(r,c)}$ is passed through the classifier to obtain a new probability distribution, $q^{(r,c)}$. The saliency of the occlusion at $(r,c)$ is defined as the KL divergence from the original distribution $p$ to the occluded distribution $q^{(r,c)}$. The formula for KL divergence between two discrete probability distributions $p$ and $q$ over the same support $\\{1, \\dots, C\\}$ is:\n$$\nD_{KL}(p || q) = \\sum_{i=1}^{C} p_i \\log \\left( \\frac{p_i}{q_i} \\right) = \\sum_{i=1}^{C} p_i (\\log p_i - \\log q_i)\n$$\nThe problem statement guarantees that the softmax output is a strictly positive distribution, ensuring that $q_i > 0$ for all $i$ and the logarithm is always well-defined.\n\n**3. Algorithmic Procedure**\n\nFor each test case, we perform the following steps:\n1.  Define the parameters for the test case: image dimensions $(H, W)$, class count $C$, patch dimensions $(P, Q)$, baseline value $b_{\\text{mask}}$, image matrix $x$, weight matrix $W$, and bias vector $b$.\n2.  Compute the original probability distribution $p$ by performing a forward pass on the unoccluded image $x$.\n3.  Initialize an empty saliency map, $S \\in \\mathbb{R}^{(H-P+1) \\times (W-Q+1)}$.\n4.  Iterate through each valid top-left occlusion coordinate $(r, c)$, from $r=0$ to $H-P$ and $c=0$ to $W-Q$.\n    a. Create a copy of the original image, let's call it $x_{\\text{occ}}$.\n    b. Modify $x_{\\text{occ}}$ by setting the submatrix from row $r$ to $r+P-1$ and column $c$ to $c+Q-1$ to the value $b_{\\text{mask}}$.\n    c. Perform a forward pass on the occluded image $x_{\\text{occ}}$ to compute the perturbed probability distribution $q^{(r,c)}$.\n    d. Calculate the saliency value $s_{r,c} = D_{KL}(p || q^{(r,c)})$.\n    e. Store this value in the saliency map: $S_{r,c} = s_{r,c}$.\n5.  After computing all saliency values, aggregate them to produce the final metrics:\n    a. **Maximum Saliency**: $\\text{max\\_kl} = \\max_{r,c} S_{r,c}$.\n    b. **Mean Saliency**: $\\text{mean\\_kl} = \\frac{1}{(H-P+1)(W-Q+1)} \\sum_{r=0}^{H-P} \\sum_{c=0}^{W-Q} S_{r,c}$.\n    c. **Location of Maximum**: $(r^{\\star}, c^{\\star}) = \\text{argmin}_{(r,c)} \\{ (r,c) \\mid S_{r,c} = \\text{max\\_kl} \\}$, where the minimum is taken in lexicographical (row-major) order. This corresponds to the first occurrence of the maximum value when scanning the saliency map row by row.\n6.  Collect the four metrics $[\\text{max\\_kl}, \\text{mean\\_kl}, r^{\\star}, c^{\\star}]$ for the current test case.\n\nThis procedure is repeated for all provided test cases. The implementation will utilize numerical libraries to efficiently handle matrix operations, flattening, and the computation of the softmax and KL divergence.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import kl_div, softmax\n\ndef solve():\n    \"\"\"\n    Solves the saliency map computation problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1 (general “happy path”)\n        {\n            \"dims\": (4, 4, 3),  # H, W, C\n            \"patch_dims\": (2, 2),  # P, Q\n            \"baseline\": 0.0,\n            \"image\": np.array([\n                [0.0, 0.5, 1.0, -0.5],\n                [1.0, 0.0, -1.0, 0.5],\n                [0.3, -0.2, 0.7, 1.2],\n                [0.0, 0.0, 0.5, -0.3]\n            ]),\n            \"weights_def\": {\n                \"type\": \"sequence\",\n                \"coeffs\": [0.1, -0.05, 0.02]\n            },\n            \"bias\": np.array([0.1, -0.2, 0.3])\n        },\n        # Test Case 2 (boundary case with no change under occlusion)\n        {\n            \"dims\": (3, 3, 2),\n            \"patch_dims\": (1, 1),\n            \"baseline\": 0.5,\n            \"image\": np.full((3, 3), 0.5),\n            \"weights_def\": {\n                \"type\": \"explicit\",\n                \"matrix\": np.array([\n                    [0.2, -0.1, 0.1, -0.05, 0.3, 0.0, 0.05, -0.1, 0.02],\n                    [-0.1, 0.15, -0.2, 0.1, -0.05, 0.2, -0.05, 0.1, -0.02]\n                ])\n            },\n            \"bias\": np.array([0.0, 0.0])\n        },\n        # Test Case 3 (edge case stressing strong localized weights)\n        {\n            \"dims\": (5, 5, 2),\n            \"patch_dims\": (3, 3),\n            \"baseline\": 0.0,\n            \"image\": np.full((5, 5), 1.0),\n            \"weights_def\": {\n                \"type\": \"piecewise\",\n                \"def\": [\n                    {\"value\": 2.0, \"region\": (0, 0, 3, 3)}, # Class 0, val, r, c, h, w\n                    {\"value\": -0.05, \"all\": True} # Class 1\n                ]\n            },\n            \"bias\": np.array([0.0, 0.0])\n        }\n    ]\n\n    def classifier_forward_pass(image, weights, bias):\n        \"\"\"Computes the probability distribution for a given image.\"\"\"\n        H, W = image.shape\n        v = image.flatten('C') # Row-major flattening\n        logits = weights @ v + bias\n        # Using scipy.special.softmax handles numerical stability\n        return softmax(logits)\n\n    def generate_weights(case):\n        \"\"\"Generates the weight matrix based on the case definition.\"\"\"\n        H, W, C = case[\"dims\"]\n        HW = H * W\n        weights_def = case[\"weights_def\"]\n        \n        if weights_def[\"type\"] == \"explicit\":\n            return weights_def[\"matrix\"]\n        \n        W_matrix = np.zeros((C, HW))\n        if weights_def[\"type\"] == \"sequence\":\n            s = np.arange(HW)\n            for i, coeff in enumerate(weights_def[\"coeffs\"]):\n                W_matrix[i, :] = coeff * s\n        elif weights_def[\"type\"] == \"piecewise\":\n            # For Case 3\n            # Class 0 weights\n            W0 = np.full(HW, 0.1)\n            r, c, h, w = weights_def[\"def\"][0][\"region\"]\n            for i in range(r, r + h):\n                for k in range(c, c + w):\n                    idx = i * W + k\n                    W0[idx] = weights_def[\"def\"][0][\"value\"]\n            W_matrix[0, :] = W0\n            # Class 1 weights\n            W1 = np.full(HW, weights_def[\"def\"][1][\"value\"])\n            W_matrix[1, :] = W1\n        return W_matrix\n\n    results = []\n    for case in test_cases:\n        H, W, C = case[\"dims\"]\n        P, Q = case[\"patch_dims\"]\n        b_mask = case[\"baseline\"]\n        x = case[\"image\"]\n        weights = generate_weights(case)\n        bias = case[\"bias\"]\n        \n        # 1. Compute original distribution p\n        p_original = classifier_forward_pass(x, weights, bias)\n        \n        saliency_map = np.zeros((H - P + 1, W - Q + 1))\n        \n        # 2. Iterate through occlusions\n        for r in range(H - P + 1):\n            for c in range(W - Q + 1):\n                x_occluded = x.copy()\n                x_occluded[r:r+P, c:c+Q] = b_mask\n                \n                # Compute occluded distribution q\n                q_occluded = classifier_forward_pass(x_occluded, weights, bias)\n                \n                # Compute KL divergence (saliency)\n                # kl_div(p, q) calculates p * log(p/q) element-wise\n                saliency = np.sum(kl_div(p_original, q_occluded))\n                saliency_map[r, c] = saliency\n\n        # 3. Aggregate metrics\n        max_kl = np.max(saliency_map)\n        mean_kl = np.mean(saliency_map)\n        \n        # Find first occurrence of max_kl in row-major order\n        max_loc_flat = np.argmax(saliency_map)\n        r_star, c_star = np.unravel_index(max_loc_flat, saliency_map.shape)\n        \n        results.append([max_kl, mean_kl, int(r_star), int(c_star)])\n\n    # Final print statement in the exact required format.\n    # The default str() for a list includes spaces, which `','.join` will preserve.\n    # `f\"[{','.join(str(res) for res in results)}]` creates eg: '[[1, 2],[3, 4]]'\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}