{
    "hands_on_practices": [
        {
            "introduction": "Deep learning models excel at capturing complex, non-linear relationships that simpler linear methods cannot. This first practice exercise provides a crucial conceptual foundation by contrasting linear correlation with a more general measure of dependence. By working through a scenario where the Pearson correlation is near zero but the mutual information is high , you will develop the intuition to recognize the signature of a non-linear relationship and understand why information-theoretic measures are indispensable tools for analyzing data and model behavior.",
            "id": "1462533",
            "problem": "A bioinformatician is analyzing a large gene expression dataset from a population of cells to infer regulatory networks. They focus on the expression levels of two specific genes, which we will call Gene Alpha ($G_{\\alpha}$) and Gene Beta ($G_{\\beta}$). The expression level of a gene is a measure of its activity, quantified as the concentration of its corresponding messenger RNA.\n\nThe statistical analysis yields two key results:\n1. The Pearson correlation coefficient, which measures the strength of a *linear* relationship between two variables, is calculated to be approximately zero ($r \\approx 0$) for the expression levels of $G_{\\alpha}$ and $G_{\\beta}$.\n2. The Mutual Information, which measures the general statistical dependency (both linear and non-linear) between two variables, is found to be significantly high for the same pair of genes. A high mutual information value indicates that knowing the expression level of one gene provides a large amount of information about the expression level of the other.\n\nBased on these two statistical observations, which of the following biological scenarios is the most plausible explanation for the relationship between Gene Alpha and Gene Beta?\n\nA. The protein produced by $G_{\\alpha}$ is a simple transcription factor that directly and linearly activates the expression of $G_{\\beta}$.\n\nB. The expression of $G_{\\alpha}$ and $G_{\\beta}$ are entirely independent of one another.\n\nC. The protein produced by $G_{\\alpha}$ is a simple transcription factor that directly and linearly represses the expression of $G_{\\beta}$.\n\nD. The protein produced by $G_{\\alpha}$ has a complex, non-monotonic regulatory effect on $G_{\\beta}$. For example, it might moderately activate $G_{\\beta}$ at low concentrations but strongly repress it at high concentrations, creating a relationship that is highly structured but not linear.\n\nE. A technical error in the experiment caused a random shuffling of the measurement data for $G_{\\alpha}$, making any real relationship impossible to detect.",
            "solution": "Let $X$ denote the expression level of $G_{\\alpha}$ and $Y$ denote the expression level of $G_{\\beta}$. The Pearson correlation coefficient is\n$$\nr=\\frac{\\operatorname{Cov}(X,Y)}{\\sigma_{X}\\sigma_{Y}},\n$$\nso $r\\approx 0$ implies $\\operatorname{Cov}(X,Y)\\approx 0$, which rules out a strong linear association but does not imply independence.\n\nThe mutual information is\n$$\nI(X;Y)=\\iint p_{X,Y}(x,y)\\,\\ln\\!\\left(\\frac{p_{X,Y}(x,y)}{p_{X}(x)p_{Y}(y)}\\right)\\,\\mathrm{d}x\\,\\mathrm{d}y.\n$$\nA significantly high value of $I(X;Y)$ indicates substantial statistical dependence between $X$ and $Y$ (linear or non-linear); in particular, $I(X;Y)=0$ if and only if $X$ and $Y$ are independent.\n\nEvaluate the options against these two constraints:\n\n1. Options A and C posit a simple direct linear regulatory effect, which can be modeled as $Y=aX+b+\\varepsilon$ with $a>0$ (activation) or $a0$ (repression), where $\\varepsilon$ is noise independent of $X$. In such a case, provided the signal term $aX$ is not negligible, $\\operatorname{Cov}(X,Y)$ has the same sign as $a$ and $|r|$ is typically large in magnitude. This contradicts the observation $r\\approx 0$. Moreover, if noise were so large as to drive $r\\approx 0$, the mutual information would also be driven down rather than being significantly high. Therefore A and C are inconsistent with the pair $(r\\approx 0,\\ I(X;Y)\\ \\text{high})$.\n\n2. Option B asserts independence. Independence implies $I(X;Y)=0$, contradicting the observed high mutual information. Hence B is invalid.\n\n3. Option E describes random shuffling of $X$ measurements, which would break any dependence and similarly yield $I(X;Y)\\approx 0$, again contradicting the observed high mutual information. Hence E is invalid.\n\n4. Option D posits a complex, non-monotonic regulatory relationship. A canonical example is a biphasic or U-shaped dependence, such as $Y=f(X)$ with $f$ non-monotonic and symmetric. For instance, if $f(x)=x^{2}$ and $X$ has a symmetric distribution with mean zero and finite variance, then\n$$\n\\operatorname{Cov}(X,Y)=\\operatorname{Cov}(X,X^{2})=\\mathbb{E}[X^{3}]-\\mathbb{E}[X]\\mathbb{E}[X^{2}]=0,\n$$\nso $r=0$, while $X$ and $Y$ are clearly dependent. In fact, for a deterministic non-constant mapping $Y=f(X)$, one has $I(X;Y)=H(Y)>0$ because $H(Y|X)=0$, so the mutual information can be substantial despite zero linear correlation. Biologically, a regulator that mildly activates at low concentration but represses at high concentration creates such a structured, non-linear, non-monotonic relationship, matching the observations.\n\nTherefore, the only option consistent with $r\\approx 0$ and high $I(X;Y)$ is that $G_{\\alpha}$ exerts a complex, non-monotonic regulatory effect on $G_{\\beta}$.",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "While mutual information is a powerful concept, computing it for continuous or high-dimensional data is often intractable. This hands-on coding exercise  moves from theory to practice, guiding you to implement a neural network to estimate mutual information from data samples. You will learn to use a variational lower bound derived from the Kullback-Leibler ($D_{\\mathrm{KL}}$) divergence to create an objective function, providing a practical and powerful technique for quantifying statistical dependence where analytical methods fail.",
            "id": "3149052",
            "problem": "You are given a discrete–continuous synthetic data model and asked to estimate the mutual information between a discrete random variable and a continuous random variable using a neural estimator derived from first principles. The tasks are to derive a suitable lower bound objective starting from core definitions, implement a small neural network to optimize this objective with gradient ascent, compute the ground-truth mutual information via numerical integration on the known synthetic model, and validate the estimator on several test cases.\n\nThe synthetic data model is as follows. Let $Y \\in \\{0,1\\}$ be a binary random variable with prior probabilities $\\mathbb{P}(Y=1)=p$ and $\\mathbb{P}(Y=0)=1-p$, and let $X \\in \\mathbb{R}$ be a continuous random variable with a conditional distribution\n$$\nX \\mid Y=y \\sim \\mathcal{N}(\\mu_y,\\sigma^2)\n$$\nfor $y \\in \\{0,1\\}$. The marginal distribution of $X$ is the mixture\n$$\np_X(x)= (1-p)\\,\\mathcal{N}(x;\\mu_0,\\sigma^2) + p\\,\\mathcal{N}(x;\\mu_1,\\sigma^2) .\n$$\nThe mutual information $I(X;Y)$ is defined in terms of the Kullback–Leibler divergence as\n$$\nI(X;Y) = D_{\\mathrm{KL}}\\big(p_{X,Y}\\,\\|\\,p_X p_Y\\big),\n$$\nwhere $p_{X,Y}$ denotes the joint distribution and $p_X p_Y$ the product of the marginals. The ground-truth mutual information for this model can be computed by numerical integration using the identity\n$$\nI(X;Y) = \\sum_{y \\in \\{0,1\\}} \\mathbb{P}(Y=y)\\, \\int_{-\\infty}^{\\infty} p_{X\\mid Y}(x \\mid y)\\,\\log\\frac{p_{X\\mid Y}(x \\mid y)}{p_X(x)}\\, \\mathrm{d}x,\n$$\nwhere $\\log$ is the natural logarithm; the unit is nats.\n\nYour program must:\n- Derive, from the definition $I(X;Y)=D_{\\mathrm{KL}}(p_{X,Y}\\,\\|\\,p_X p_Y)$ and the variational characterization of $D_{\\mathrm{KL}}$, a tractable lower bound objective involving an auxiliary function $T_{\\theta}(x,y)$ that can be optimized to estimate $I(X;Y)$.\n- Implement a small neural network for $T_{\\theta}(x,y)$ that takes as input the pair $(x,y)$ encoded as a continuous scalar $x$ and a one-hot vector for $y$, and outputs a scalar. Use one hidden layer with a smooth nonlinearity.\n- Optimize the derived lower bound objective by stochastic gradient ascent using samples from the joint distribution $p_{X,Y}$ and from the product distribution $p_X p_Y$ created by breaking the dependence (e.g., shuffling $y$ across a batch). Use a fixed learning rate and gradient clipping to ensure numerical stability. No external machine learning libraries are allowed; implement forward and backward propagation manually in terms of matrix–vector operations and elementwise derivatives.\n- Compute the ground-truth mutual information using numerical integration (with appropriate absolute and relative error tolerances) for the given parameters. Report the numerical value in nats.\n- Validate the estimator by checking whether the absolute error between the neural estimate and the ground truth is below a specified tolerance.\n\nUnits: All mutual information values must be in nats. No angles or physical units apply.\n\nTest Suite:\nFor each parameter set $(p,\\mu_0,\\mu_1,\\sigma)$, train the estimator and validate against the ground truth. Use the following four cases designed to cover typical and edge scenarios:\n1. $(p,\\mu_0,\\mu_1,\\sigma) = (0.5, 0.0, 2.0, 1.0)$: a moderately separated mixture (happy path).\n2. $(p,\\mu_0,\\mu_1,\\sigma) = (0.5, 0.0, 0.0, 1.0)$: identical conditionals (boundary case where $I(X;Y)=0$).\n3. $(p,\\mu_0,\\mu_1,\\sigma) = (0.5, 0.0, 5.0, 1.0)$: strongly separated mixture (mutual information approaches $\\log 2$ nats).\n4. $(p,\\mu_0,\\mu_1,\\sigma) = (0.8, 0.0, 2.0, 1.0)$: imbalanced prior probabilities with moderate separation.\n\nEstimator specification:\n- Use a single-hidden-layer neural network for $T_{\\theta}(x,y)$ with a smooth activation function on the hidden units.\n- Optimize the lower bound objective derived from first principles via stochastic gradient ascent with a fixed learning rate and gradient clipping.\n- Use batches of independent and identically distributed samples from $p_{X,Y}$; construct product-distribution samples $p_X p_Y$ by shuffling the label indices within the batch.\n\nGround truth specification:\n- Compute $I(X;Y)$ via numerical integration using a high-accuracy univariate integral for each class $y$, summing the contributions weighted by $\\mathbb{P}(Y=y)$.\n\nValidation and final output:\n- For each test case, compute the absolute error between the neural estimate and the ground truth. If the error is less than or equal to $0.2$ nats, return the boolean value $\\mathrm{True}$; otherwise, return $\\mathrm{False}$.\n- Your program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,True]\") with no other text.\n\nImplementation constraints:\n- The final answer must be a complete, runnable Python program that uses only standard libraries plus Numerical Python (NumPy) and Scientific Python (SciPy), and it must not require any user input or external files.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in information theory and numerical methods, well-posed with a clear objective and constraints, and free of contradictions or ambiguities. It presents a substantial and standard task in machine learning: estimating mutual information using a neural network optimized via a variationally derived lower bound. We will proceed with a solution.\n\nThe solution involves four main steps:\n1.  Derivation of a tractable lower bound for mutual information from first principles.\n2.  Implementation of a neural network to serve as the auxiliary function in the lower bound, trained with manually implemented stochastic gradient ascent.\n3.  Computation of the ground-truth mutual information for the specified synthetic model using numerical integration.\n4.  Validation of the neural estimator against the ground truth for each test case.\n\n**1. Derivation of the Mutual Information Lower Bound**\n\nThe mutual information $I(X;Y)$ between two random variables $X$ and $Y$ is defined as the Kullback-Leibler ($D_{\\mathrm{KL}}$) divergence between their joint distribution $p_{X,Y}$ and the product of their marginal distributions $p_X p_Y$:\n$$\nI(X;Y) = D_{\\mathrm{KL}}(p_{X,Y} \\,\\|\\, p_X p_Y) = \\int_{\\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p_{X,Y}(x,y) \\log \\frac{p_{X,Y}(x,y)}{p_X(x) p_Y(y)} \\, \\mathrm{d}x\n$$\nThe Donsker-Varadhan representation provides a variational lower bound for the $D_{\\mathrm{KL}}$ divergence:\n$$\nD_{\\mathrm{KL}}(\\mathbb{P} \\,\\|\\, \\mathbb{Q}) = \\sup_{T} \\left( \\mathbb{E}_{\\mathbf{z} \\sim \\mathbb{P}}[T(\\mathbf{z})] - \\log \\mathbb{E}_{\\mathbf{z} \\sim \\mathbb{Q}}[e^{T(\\mathbf{z})}] \\right)\n$$\nwhere the supremum is taken over a suitable class of functions $T$.\n\nBy substituting $\\mathbb{P}$ with the joint distribution $p_{X,Y}$ and $\\mathbb{Q}$ with the product of marginals $p_X p_Y$, we obtain a variational representation for the mutual information:\n$$\nI(X;Y) = \\sup_{T} \\left( \\mathbb{E}_{(x,y) \\sim p_{X,Y}}[T(x,y)] - \\log \\mathbb{E}_{(x,y) \\sim p_X p_Y}[e^{T(x,y)}] \\right)\n$$\nTo create a practical estimator, we replace the function $T$ with a parametric function $T_\\theta$ realized by a neural network. This turns the supremum into a maximization problem over the parameters $\\theta$. The resulting objective function $L(\\theta)$ provides a lower bound on the mutual information:\n$$\nI(X;Y) \\ge L(\\theta) = \\mathbb{E}_{(x,y) \\sim p_{X,Y}}[T_\\theta(x,y)] - \\log \\mathbb{E}_{(x',y') \\sim p_X p_Y}[e^{T_\\theta(x',y')}]\n$$\nThis objective can be optimized using stochastic gradient ascent. The expectations are estimated using Monte Carlo sampling. Given a batch of $N$ samples $\\{(x_i, y_i)\\}_{i=1}^N$ drawn from the joint distribution $p_{X,Y}$ and a batch of $N$ samples $\\{(x'_i, y'_i)\\}_{i=1}^N$ drawn from the product distribution $p_X p_Y$, the empirical objective is:\n$$\nL_N(\\theta) = \\frac{1}{N} \\sum_{i=1}^N T_\\theta(x_i, y_i) - \\log\\left(\\frac{1}{N} \\sum_{i=1}^N e^{T_\\theta(x'_i, y'_i)}\\right)\n$$\nAs per the problem description, samples from $p_X p_Y$ are constructed by taking the joint samples $(x_i,y_i)$ and shuffling the labels, creating pairs $(x_i, y_{\\pi(i)})$ where $\\pi$ is a random permutation of the indices $\\{1, \\dots, N\\}$.\n\n**2. Neural Estimator Implementation**\n\nThe function $T_\\theta(x,y)$ is implemented as a single-hidden-layer neural network.\n- **Input**: The pair $(x,y)$ is encoded as a $3$-dimensional vector. For $y \\in \\{0, 1\\}$, we use one-hot encoding: $y=0 \\to [1, 0]$ and $y=1 \\to [0, 1]$. The input vector is $[x, y_0, y_1]$.\n- **Architecture**:\n    1.  An input layer of size $3$.\n    2.  A hidden layer with $32$ units and the hyperbolic tangent ($\\tanh$) activation function.\n    3.  A linear output layer with a single scalar output.\n- **Optimization**: We maximize $L_N(\\theta)$ via stochastic gradient ascent. The gradient of the empirical objective $L_N(\\theta)$ with respect to the parameters $\\theta$ is:\n$$\n\\nabla_\\theta L_N(\\theta) = \\mathbb{E}_{\\text{joint}}[\\nabla_\\theta T_\\theta(x,y)] - \\mathbb{E}_{\\text{marginal}}[\\omega(x,y) \\nabla_\\theta T_\\theta(x,y)]\n$$\nwhere the weights $\\omega(x,y)$ for the marginal samples are given by a softmax over the batch:\n$$\n\\omega(x'_k, y'_k) = \\frac{e^{T_\\theta(x'_k, y'_k)}}{\\sum_{j=1}^N e^{T_\\theta(x'_j, y'_j)}}\n$$\nThe gradient calculations are implemented manually using the chain rule (backpropagation). Let $\\theta = \\{W_1, b_1, W_2, b_2\\}$. For a batch, the gradients are computed using matrix-vector operations. For example, the gradient for the first layer weights $W_1$ is:\n$$\n\\nabla_{W_1} L_N = \\frac{1}{N} (X_{\\text{pos}})^T G_{\\text{pos}} - (X_{\\text{neg}})^T (G_{\\text{neg}} \\odot W_{\\text{sm}})\n$$\nwhere $X_{\\text{pos}}, X_{\\text{neg}}$ are input matrices for joint and marginal samples, $G$ represents the error signal at the hidden layer's pre-activation, and $W_{\\text{sm}}$ is the matrix of softmax weights. The parameter updates follow $\\theta \\leftarrow \\theta + \\eta \\nabla_\\theta L_N$, where $\\eta$ is the learning rate. Gradient clipping is applied to stabilize training.\n\n**3. Ground-Truth Calculation**\n\nThe true mutual information for the given model is computed by numerically integrating the defining formula:\n$$\nI(X;Y) = \\sum_{y \\in \\{0,1\\}} \\mathbb{P}(Y=y)\\, \\int_{-\\infty}^{\\infty} p(x \\mid y)\\,\\log\\frac{p(x \\mid y)}{p(x)}\\, \\mathrm{d}x,\n$$\nwhere $p(x|y) = \\mathcal{N}(x; \\mu_y, \\sigma^2)$ is a normal probability density function (PDF), and the marginal $p(x)$ is the mixture distribution $p(x) = (1-p)\\,\\mathcal{N}(x;\\mu_0,\\sigma^2) + p\\,\\mathcal{N}(x;\\mu_1,\\sigma^2)$. The integral for each class $y$ is computed using the high-precision `scipy.integrate.quad` function over the domain $(-\\infty, \\infty)$. The total mutual information is the weighted sum of these two integrals.\n\n**4. Validation**\n\nFor each of the four specified test cases, the neural estimator is trained for a fixed number of iterations. The final estimate, $I_{\\text{est}}$, is taken as the average of the lower bound values from the last few hundred training iterations to reduce noise. This estimate is then compared to the numerically computed ground truth, $I_{\\text{truth}}$. The validation passes if the absolute error $|I_{\\text{est}} - I_{\\text{truth}}|$ is no greater than the specified tolerance of $0.2$ nats.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.stats import norm\n\n#\n# ==============================================================================\n# MUTUAL INFORMATION ESTIMATOR (NEURAL NETWORK)\n# ==============================================================================\n#\n\ndef tanh(x):\n    \"\"\"Hyperbolic tangent activation function.\"\"\"\n    return np.tanh(x)\n\ndef network_forward(X, W1, b1, W2, b2):\n    \"\"\"Performs a forward pass through the single-hidden-layer network.\"\"\"\n    H_pre = X @ W1 + b1\n    H = tanh(H_pre)\n    T = H @ W2 + b2\n    return T, H\n\ndef estimate_mi(p, mu0, mu1, sigma, rng):\n    \"\"\"\n    Trains a neural estimator for mutual information and returns the estimate.\n    \n    The estimator is based on the Donsker-Varadhan lower bound of KL-divergence.\n    \"\"\"\n    # Hyperparameters\n    input_dim = 3\n    hidden_dim = 32\n    output_dim = 1\n    learning_rate = 1e-3\n    batch_size = 512\n    num_iterations = 10000\n    grad_clip_value = 1.0\n\n    # Initialize network parameters\n    W1 = rng.normal(0, 0.1, size=(input_dim, hidden_dim))\n    b1 = np.zeros(hidden_dim)\n    W2 = rng.normal(0, 0.1, size=(hidden_dim, output_dim))\n    b2 = np.zeros(output_dim)\n\n    mi_estimates = []\n\n    for i in range(num_iterations):\n        # 1. Sample data from the joint distribution p(x, y)\n        y_sample = rng.binomial(1, p, size=batch_size)\n        mu_sample = np.where(y_sample == 1, mu1, mu0)\n        x_sample = rng.normal(loc=mu_sample, scale=sigma)\n\n        # 2. Prepare inputs for the network\n        # Joint (positive) samples from p(x,y)\n        y_one_hot_pos = np.eye(2)[y_sample]\n        X_pos = np.c_[x_sample, y_one_hot_pos]\n\n        # Marginal (negative) samples from p(x)p(y) via shuffling\n        y_shuffled = rng.permutation(y_sample)\n        y_one_hot_neg = np.eye(2)[y_shuffled]\n        X_neg = np.c_[x_sample, y_one_hot_neg]\n\n        # 3. Forward passes\n        T_pos, H_pos = network_forward(X_pos, W1, b1, W2, b2)\n        T_neg, H_neg = network_forward(X_neg, W1, b1, W2, b2)\n\n        # 4. MI estimate (for monitoring)\n        # Use log-sum-exp trick for stability\n        t_neg_max = np.max(T_neg)\n        log_mean_exp_t_neg = t_neg_max + np.log(np.mean(np.exp(T_neg - t_neg_max)))\n        mi_est = np.mean(T_pos) - log_mean_exp_t_neg\n        \n        if i >= num_iterations - 1000:\n            mi_estimates.append(mi_est)\n\n        # 5. Backward pass / Gradient calculation\n        # Softmax weights for the negative term's gradient\n        T_neg_stabilized = T_neg - np.max(T_neg)\n        exp_T_neg = np.exp(T_neg_stabilized)\n        sum_exp_T_neg = np.sum(exp_T_neg)\n        weights = (exp_T_neg / sum_exp_T_neg).reshape(-1, 1)\n\n        # Gradients for W2 and b2\n        grad_W2 = (1/batch_size) * H_pos.T @ np.ones((batch_size, 1)) - H_neg.T @ weights\n        # grad_b2 is theoretically zero, as the objective is invariant to shifts in T\n        grad_b2 = np.array([np.mean(1.0) - np.sum(weights)])\n\n        # Error signal at the hidden layer pre-activation\n        # d(loss)/dH_pre = d(loss)/dT * dT/dH * dH/dH_pre\n        g_H_pos = (1 - H_pos**2) * W2.T\n        g_H_neg = (1 - H_neg**2) * W2.T\n\n        # Gradients for W1 and b1\n        grad_W1 = (1/batch_size) * X_pos.T @ g_H_pos - X_neg.T @ (g_H_neg * weights)\n        grad_b1 = np.mean(g_H_pos, axis=0) - np.sum(g_H_neg * weights, axis=0)\n\n        # 6. Gradient clipping and parameter update (ascent)\n        np.clip(grad_W1, -grad_clip_value, grad_clip_value, out=grad_W1)\n        np.clip(grad_b1, -grad_clip_value, grad_clip_value, out=grad_b1)\n        np.clip(grad_W2, -grad_clip_value, grad_clip_value, out=grad_W2)\n        np.clip(grad_b2, -grad_clip_value, grad_clip_value, out=grad_b2)\n\n        W1 += learning_rate * grad_W1\n        b1 += learning_rate * grad_b1\n        W2 += learning_rate * grad_W2\n        b2 += learning_rate * grad_b2\n        \n    return np.mean(mi_estimates)\n\n\n#\n# ==============================================================================\n# GROUND TRUTH CALCULATION\n# ==============================================================================\n#\n\ndef calculate_ground_truth(p, mu0, mu1, sigma):\n    \"\"\"Computes the exact mutual information by numerical integration.\"\"\"\n\n    # Marginal distribution of X\n    def p_x(x, p_val, mu0_val, mu1_val, sigma_val):\n        return (1 - p_val) * norm.pdf(x, mu0_val, sigma_val) + \\\n                 p_val * norm.pdf(x, mu1_val, sigma_val)\n\n    # Integrands for the two components of MI\n    def integrand(x, y, p_val, mu0_val, mu1_val, sigma_val):\n        mu_y = mu1_val if y == 1 else mu0_val\n        p_x_given_y_val = norm.pdf(x, mu_y, sigma_val)\n        p_x_val = p_x(x, p_val, mu0_val, mu1_val, sigma_val)\n        # Avoid log(0) - although unlikely with normal distributions\n        if p_x_given_y_val = 0 or p_x_val = 0:\n            return 0.0\n        return p_x_given_y_val * np.log(p_x_given_y_val / p_x_val)\n\n    # Numerically integrate for y=0 and y=1\n    integral_0, _ = quad(integrand, -np.inf, np.inf, args=(0, p, mu0, mu1, sigma))\n    integral_1, _ = quad(integrand, -np.inf, np.inf, args=(1, p, mu0, mu1, sigma))\n\n    # Total mutual information\n    mi_true = (1 - p) * integral_0 + p * integral_1\n    return mi_true\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases, validate the estimator, and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (p, mu0, mu1, sigma)\n        (0.5, 0.0, 2.0, 1.0),\n        (0.5, 0.0, 0.0, 1.0),\n        (0.5, 0.0, 5.0, 1.0),\n        (0.8, 0.0, 2.0, 1.0),\n    ]\n\n    results = []\n    # Use a seeded random number generator for reproducibility\n    rng = np.random.default_rng(seed=42)\n    \n    validation_tolerance = 0.2\n\n    for case in test_cases:\n        p, mu0, mu1, sigma = case\n\n        # Compute ground truth\n        mi_truth = calculate_ground_truth(p, mu0, mu1, sigma)\n\n        # Get neural estimate\n        mi_estimate = estimate_mi(p, mu0, mu1, sigma, rng)\n        \n        # Validate the estimate\n        abs_error = np.abs(mi_estimate - mi_truth)\n        is_valid = abs_error = validation_tolerance\n        \n        results.append(str(is_valid))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "We now extend the concept of dependence from pairs of variables to the collective structure among multiple variables, a central theme in modern representation learning. This advanced simulation  demonstrates the practical utility of these measures in the context of a $\\beta$-Variational Autoencoder ($\\beta$-VAE). You will compute the Total Correlation, a multivariate generalization of mutual information, and discover how explicitly penalizing this dependency in a model's latent space is a key principle for learning factorized, interpretable representations.",
            "id": "3149107",
            "problem": "You are given a simulation task to study a measure of statistical dependence called total correlation in the context of deep learning latent variables and its relationship to disentanglement. Consider a latent representation with $d$ variables $Z = (Z_1, Z_2, Z_3)$ produced by a hypothetical encoder of a $\\beta$-Variational Autoencoder ($\\beta$-VAE). The aggregated posterior of the encoder is modeled as a linear-Gaussian mapping of independent ground-truth factors. Let $V = (V_1, V_2, V_3)$ be a vector of independent factors where each $V_i$ is distributed as a standard normal. Define a mixing matrix $A \\in \\mathbb{R}^{3 \\times 3}$ and additive Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2, \\sigma_3^2))$. The latent is generated as $Z = A V + \\varepsilon$ from $N$ independent samples.\n\nThe measure of statistical dependence to estimate is the total correlation $\\mathrm{TC}(Z)$, defined from first principles via differential entropy by\n$$\\mathrm{TC}(Z) = \\sum_{i=1}^{3} H(Z_i) - H(Z),$$\nwhere $H(\\cdot)$ denotes the differential entropy. Under the well-tested Gaussian approximation, the differential entropy of a $d$-dimensional multivariate normal with covariance $\\Sigma$ is\n$$H(Z) = \\frac{1}{2} \\log\\!\\big( (2\\pi e)^d \\det(\\Sigma) \\big),$$\nand the marginal entropy of a scalar normal with variance $\\sigma_i^2$ is\n$$H(Z_i) = \\frac{1}{2} \\log\\!\\big( 2\\pi e \\, \\sigma_i^2 \\big).$$\nThis yields a computable expression for $\\mathrm{TC}(Z)$ in terms of the sample covariance matrix $\\Sigma_Z$ of $Z$ and its diagonal elements.\n\nTo relate $\\mathrm{TC}(Z)$ to disentanglement, you will also compute the Mutual Information Gap (MIG). For each factor $V_k$ and each latent $Z_i$, estimate the mutual information $I(Z_i; V_k)$ under a jointly Gaussian model using the correlation coefficient $\\rho_{ik}$ between $Z_i$ and $V_k$:\n$$I(Z_i; V_k) = -\\frac{1}{2} \\log(1 - \\rho_{ik}^2).$$\nThe MIG is then defined as the average over factors of the gap between the largest and second-largest mutual information values:\n$$\\mathrm{MIG} = \\frac{1}{3} \\sum_{k=1}^{3} \\Big( \\max_{i} I(Z_i; V_k) - \\max_{i \\neq i^*(k)} I(Z_i; V_k) \\Big),$$\nwhere $i^*(k)$ is the index achieving the maximum for factor $k$.\n\nTo model a total correlation penalty analogous to increasing the $\\beta$ parameter in a $\\beta$-VAE, apply a partial-whitening transform to $Z$ that progressively removes correlation:\n$$Z^{(\\alpha)} = \\Sigma_Z^{-\\alpha/2} \\, (Z - \\mathbb{E}[Z]),$$\nwhere $\\alpha \\in [0,1]$ is a penalty strength. When $\\alpha = 0$, $Z^{(0)} = Z - \\mathbb{E}[Z]$; when $\\alpha = 1$, $Z^{(1)}$ is fully whitened and has identity covariance, which ideally minimizes $\\mathrm{TC}(Z)$ under Gaussian assumptions. You will compute $\\mathrm{TC}(Z^{(\\alpha)})$ and $\\mathrm{MIG}(Z^{(\\alpha)}, V)$, and compare them to the original $\\mathrm{TC}(Z)$ and $\\mathrm{MIG}(Z, V)$.\n\nAll information quantities must be expressed in nats (natural logarithm base).\n\nImplement a complete, runnable program that:\n- Simulates $N$ samples of $V \\sim \\mathcal{N}(0, I_3)$.\n- Generates $Z = A V + \\varepsilon$ with $\\varepsilon$ as independent Gaussian noise with specified diagonal standard deviations.\n- Computes $\\mathrm{TC}(Z)$ from the sample covariance of $Z$ using the Gaussian differential entropy formulas.\n- Computes $\\mathrm{MIG}(Z, V)$ using the jointly Gaussian mutual information formula $I(Z_i; V_k) = -\\frac{1}{2}\\log(1-\\rho_{ik}^2)$, where $\\rho_{ik}$ is the empirical correlation between $Z_i$ and $V_k$.\n- Applies the partial-whitening transform $Z^{(\\alpha)} = \\Sigma_Z^{-\\alpha/2}(Z - \\mathbb{E}[Z])$ for specified $\\alpha$ values and recomputes $\\mathrm{TC}(Z^{(\\alpha)})$ and $\\mathrm{MIG}(Z^{(\\alpha)}, V)$.\n\nUse the following test suite of parameter values to cover different regimes:\n- Test Case $1$ (happy path): $A = \\begin{bmatrix} 1.0  0.6  0.0 \\\\ 0.4  1.0  0.5 \\\\ 0.0  0.3  1.0 \\end{bmatrix}$, noise standard deviations $(\\sigma_1,\\sigma_2,\\sigma_3) = (0.1, 0.1, 0.1)$, $N = 20000$, $\\alpha \\in \\{0.5, 1.0\\}$.\n- Test Case $2$ (boundary, already factorized): $A = I_3$, noise standard deviations $(\\sigma_1,\\sigma_2,\\sigma_3) = (0.05, 0.05, 0.05)$, $N = 20000$, $\\alpha \\in \\{0.5, 1.0\\}$.\n- Test Case $3$ (edge, strong mixing): $A = \\begin{bmatrix} 1.0  1.1  1.2 \\\\ 0.9  1.0  1.1 \\\\ 1.1  1.2  1.3 \\end{bmatrix}$, noise standard deviations $(\\sigma_1,\\sigma_2,\\sigma_3) = (0.2, 0.2, 0.2)$, $N = 20000$, $\\alpha \\in \\{0.5, 1.0\\}$.\n\nFor each test case, your program must output a list containing exactly six floating-point numbers in nats: \n$[\\mathrm{TC}(Z), \\mathrm{MIG}(Z,V), \\mathrm{TC}(Z^{(0.5)}), \\mathrm{MIG}(Z^{(0.5)},V), \\mathrm{TC}(Z^{(1.0)}), \\mathrm{MIG}(Z^{(1.0)},V)]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, where each element corresponds to one test case’s six-number list. For example, the structure must be $[[r_{1,1},r_{1,2},r_{1,3},r_{1,4},r_{1,5},r_{1,6}],[r_{2,1},\\dots,r_{2,6}],[r_{3,1},\\dots,r_{3,6}]]$.\n- All values must be in nats (natural logarithm base).\n- No user input is required; use a fixed random seed for reproducibility.",
            "solution": "The problem is valid as it is scientifically grounded in information theory and statistics, well-posed with all necessary parameters and definitions, and presents a computationally feasible simulation task relevant to deep learning research.\n\nThe problem requires the simulation of a linear-Gaussian latent variable model, $Z = AV + \\varepsilon$, and the computation of two key metrics: Total Correlation ($TC$) and Mutual Information Gap ($MIG$). These metrics are then re-evaluated after applying a partial-whitening transformation to the latent variables. The entire process will be implemented for three distinct test cases.\n\nFirst, let's establish the statistical properties of the model. The ground-truth factors $V$ are a $d=3$ dimensional random vector following a standard multivariate normal distribution, $V \\sim \\mathcal{N}(0, I_3)$, where $I_3$ is the $3 \\times 3$ identity matrix. The additive noise $\\varepsilon$ is also a $d=3$ dimensional Gaussian vector, $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma_\\varepsilon)$, where $\\Sigma_\\varepsilon = \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2, \\sigma_3^2)$ is a diagonal covariance matrix. Since $Z$ is a linear transformation of independent Gaussian vectors $V$ and $\\varepsilon$, $Z$ itself is a multivariate Gaussian variable.\nThe mean of $Z$ is $\\mathbb{E}[Z] = \\mathbb{E}[AV + \\varepsilon] = A\\mathbb{E}[V] + \\mathbb{E}[\\varepsilon] = A \\cdot 0 + 0 = 0$.\nThe covariance of $Z$, denoted $\\Sigma_Z$, is:\n$$ \\Sigma_Z = \\mathrm{Cov}(AV + \\varepsilon) = \\mathrm{Cov}(AV) + \\mathrm{Cov}(\\varepsilon) $$\ndue to the independence of $V$ and $\\varepsilon$. The terms are:\n$$ \\mathrm{Cov}(AV) = A \\mathrm{Cov}(V) A^T = A I_3 A^T = AA^T $$\n$$ \\mathrm{Cov}(\\varepsilon) = \\Sigma_\\varepsilon $$\nThus, the theoretical covariance of $Z$ is $\\Sigma_Z = AA^T + \\Sigma_\\varepsilon$. In the simulation, we will use the sample covariance matrix, $\\hat{\\Sigma}_Z$, estimated from $N$ generated samples.\n\nThe first metric, Total Correlation, measures the total statistical dependence among the components of $Z$. It is defined as the Kullback-Leibler divergence between the joint distribution $p(Z)$ and the product of its marginals $\\prod_i p(Z_i)$. For continuous variables, this is expressed using differential entropies:\n$$ TC(Z) = \\sum_{i=1}^{3} H(Z_i) - H(Z) $$\nUnder the Gaussian assumption, the differential entropy of a $d$-dimensional variable with covariance $\\Sigma$ is $H(Z) = \\frac{1}{2} \\log\\!\\big( (2\\pi e)^d \\det(\\Sigma) \\big)$, and the entropy of a single component $Z_i$ with variance $\\sigma_{Z_i}^2$ is $H(Z_i) = \\frac{1}{2} \\log\\!\\big( 2\\pi e \\, \\sigma_{Z_i}^2 \\big)$. The variance $\\sigma_{Z_i}^2$ corresponds to the $i$-th diagonal element of the covariance matrix $\\Sigma_Z$, i.e., $\\sigma_{Z_i}^2 = (\\Sigma_Z)_{ii}$.\nSubstituting these into the $TC$ formula and simplifying, we obtain a computable expression based on the covariance matrix:\n$$ TC(Z) = \\frac{1}{2} \\left[ \\log\\left(\\prod_{i=1}^{3} (\\Sigma_Z)_{ii}\\right) - \\log(\\det(\\Sigma_Z)) \\right] $$\nThis formula will be applied to the sample covariance matrix $\\hat{\\Sigma}_Z$.\n\nThe second metric, Mutual Information Gap, quantifies the degree of disentanglement between the learned latent representation $Z$ and the ground-truth factors $V$. It requires computing the pairwise mutual information $I(Z_i; V_k)$ for each latent variable $Z_i$ and each factor $V_k$. Since the joint distribution of $(Z, V)$ is Gaussian, the mutual information can be calculated from their correlation coefficient $\\rho_{ik} = \\mathrm{Corr}(Z_i, V_k)$:\n$$ I(Z_i; V_k) = -\\frac{1}{2} \\log(1 - \\rho_{ik}^2) $$\nThe correlation coefficients will be estimated empirically from the generated samples of $Z$ and $V$.\nThe $MIG$ is then computed by averaging, over all factors $V_k$, the difference between the highest and second-highest mutual information values with the latent variables:\n$$ \\mathrm{MIG} = \\frac{1}{3} \\sum_{k=1}^{3} \\Big( \\max_{i} I(Z_i; V_k) - \\max_{i \\neq i^*(k)} I(Z_i; V_k) \\Big) $$\nwhere $i^*(k)$ is the index of the latent variable that has the maximum mutual information with factor $V_k$. A high $MIG$ indicates that each factor is cleanly captured by a single, distinct latent variable.\n\nThe core of the investigation involves a partial-whitening transform, designed to reduce the correlations within $Z$. The transformed variable $Z^{(\\alpha)}$ is given by:\n$$ Z^{(\\alpha)} = \\Sigma_Z^{-\\alpha/2} (Z - \\mathbb{E}[Z]) $$\nwhere $\\alpha \\in [0, 1]$ is the penalty strength. The matrix power $\\Sigma_Z^{-\\alpha/2}$ is computed using the eigendecomposition of $\\Sigma_Z$. If $\\Sigma_Z = UDU^T$, where $D$ is a diagonal matrix of eigenvalues and $U$ is the matrix of eigenvectors, then $\\Sigma_Z^p = UD^p U^T$. For implementation, we will use the sample covariance $\\hat{\\Sigma}_Z$ and sample mean $\\bar{Z}$ and compute the transform matrix numerically. The `scipy.linalg.fractional_matrix_power` function provides a robust way to compute this.\nAfter applying the transform to the generated samples $Z$ to obtain samples of $Z^{(\\alpha)}$, we will re-calculate both $TC(Z^{(\\alpha)})$ and $\\mathrm{MIG}(Z^{(\\alpha)}, V)$ using the same methods described above. For $\\alpha=0.0$, $Z^{(0)}$ is just the centered version of $Z$, so $TC(Z^{(0)}) = TC(Z)$ and $\\mathrm{MIG}(Z^{(0)},V) = \\mathrm{MIG}(Z,V)$. For $\\alpha=1.0$, the variable $Z^{(1)}$ is fully whitened, and its covariance matrix is the identity matrix, which theoretically minimizes the total correlation to $0$.\n\nThe overall algorithm for each test case is as follows:\n1.  Initialize a random number generator with a fixed seed for reproducibility.\n2.  Generate $N=20000$ samples of the factor vector $V \\sim \\mathcal{N}(0, I_3)$.\n3.  Generate $N=20000$ samples of the noise vector $\\varepsilon \\sim \\mathcal{N}(0, \\mathrm{diag}(\\sigma_i^2))$.\n4.  Compute the latent samples via $Z = AV + \\varepsilon$. The data matrices will have shape $(N, 3)$.\n5.  Calculate $TC(Z)$ and $\\mathrm{MIG}(Z,V)$ from the sample data.\n6.  For each specified value of $\\alpha \\in \\{0.5, 1.0\\}$:\n    a.  Compute the sample covariance matrix $\\hat{\\Sigma}_Z$ of the original data $Z$.\n    b.  Compute the transformation matrix $M = \\hat{\\Sigma}_Z^{-\\alpha/2}$.\n    c.  Center the data $Z_{cent} = Z - \\bar{Z}$.\n    d.  Apply the transformation: $Z^{(\\alpha)} = Z_{cent} M$. Since $M$ is symmetric for a symmetric $\\hat{\\Sigma}_Z$, this is equivalent to $(M Z_{cent}^T)^T$.\n    e.  Calculate $TC(Z^{(\\alpha)})$ and $\\mathrm{MIG}(Z^{(\\alpha)}, V)$ using the transformed samples $Z^{(\\alpha)}$ and original factor samples $V$.\n7.  Collect the six resulting floating-point numbers: $[\\mathrm{TC}(Z), \\mathrm{MIG}(Z,V), \\mathrm{TC}(Z^{(0.5)}), \\mathrm{MIG}(Z^{(0.5)},V), \\mathrm{TC}(Z^{(1.0)}), \\mathrm{MIG}(Z^{(1.0)},V)]$ for final output.\nThis procedure will be repeated for all three test cases provided.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # A fixed random seed for reproducibility is required.\n    RANDOM_SEED = 42\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([[1.0, 0.6, 0.0], [0.4, 1.0, 0.5], [0.0, 0.3, 1.0]]),\n            \"noise_stds\": np.array([0.1, 0.1, 0.1]),\n            \"N\": 20000,\n            \"alphas\": [0.5, 1.0]\n        },\n        {\n            \"A\": np.identity(3),\n            \"noise_stds\": np.array([0.05, 0.05, 0.05]),\n            \"N\": 20000,\n            \"alphas\": [0.5, 1.0]\n        },\n        {\n            \"A\": np.array([[1.0, 1.1, 1.2], [0.9, 1.0, 1.1], [1.1, 1.2, 1.3]]),\n            \"noise_stds\": np.array([0.2, 0.2, 0.2]),\n            \"N\": 20000,\n            \"alphas\": [0.5, 1.0]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        noise_stds = case[\"noise_stds\"]\n        N = case[\"N\"]\n        alphas = case[\"alphas\"]\n        \n        # Step 1: Generate data\n        d = A.shape[0]\n        V = rng.standard_normal(size=(N, d))\n        epsilon = rng.normal(loc=0.0, scale=noise_stds, size=(N, d))\n        Z = V @ A.T + epsilon\n\n        # Step 2: Calculate metrics for original Z\n        case_results = []\n        tc_z, mig_z = _calculate_metrics(Z, V)\n        case_results.extend([tc_z, mig_z])\n        \n        # Step 3: Apply partial-whitening transform and recalculate metrics\n        Z_centered = Z - Z.mean(axis=0)\n        cov_Z = np.cov(Z, rowvar=False)\n\n        for alpha in alphas:\n            # Compute the transformation matrix Sigma_Z^(-alpha/2)\n            # Use real part to discard negligible imaginary parts from numerical errors\n            transform_M = np.real(linalg.fractional_matrix_power(cov_Z, -alpha / 2.0))\n            \n            # Apply the transformation\n            Z_alpha = Z_centered @ transform_M\n            \n            # Calculate metrics for the transformed data\n            tc_alpha, mig_alpha = _calculate_metrics(Z_alpha, V)\n            case_results.extend([tc_alpha, mig_alpha])\n\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    outer_parts = []\n    for res_list in all_results:\n        # Round to a reasonable number of digits for consistent output\n        inner_str = f\"[{','.join(f'{x:.6f}' for x in res_list)}]\"\n        outer_parts.append(inner_str)\n    final_str = f\"[{','.join(outer_parts)}]\"\n    print(final_str)\n\ndef _calculate_metrics(Z_data, V_data):\n    \"\"\"\n    Calculates Total Correlation (TC) and Mutual Information Gap (MIG).\n    \"\"\"\n    # Calculate Total Correlation (TC)\n    cov_Z = np.cov(Z_data, rowvar=False)\n    # Use slogdet for numerical stability\n    s, logdet_cov = np.linalg.slogdet(cov_Z)\n    if s = 0: # Handle non-positive definite cases, though unlikely with N >> d\n        tc = np.nan\n    else:\n        log_prod_diag = np.sum(np.log(np.diag(cov_Z)))\n        tc = 0.5 * (log_prod_diag - logdet_cov)\n\n    # Calculate Mutual Information Gap (MIG)\n    d = Z_data.shape[1]\n    combined_data = np.hstack((Z_data, V_data))\n    corr_matrix = np.corrcoef(combined_data, rowvar=False)\n    \n    # Extract the correlation submatrix between Z and V\n    rho_ZV = corr_matrix[0:d, d:]\n\n    # Clip to avoid log(0) for perfect correlations\n    rho_ZV_sq = np.clip(rho_ZV**2, 0.0, 1.0 - 1e-12)\n    mi_matrix = -0.5 * np.log(1.0 - rho_ZV_sq)\n    \n    gaps = []\n    for k in range(d): # Iterate over factors V_k\n        mi_for_factor = mi_matrix[:, k]\n        sorted_mi = np.sort(mi_for_factor)\n        gap = sorted_mi[-1] - sorted_mi[-2]\n        gaps.append(gap)\n        \n    mig = np.mean(gaps)\n    \n    return tc, mig\n\nsolve()\n```"
        }
    ]
}