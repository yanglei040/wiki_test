{
    "hands_on_practices": [
        {
            "introduction": "我们常常使用皮尔逊相关系数（Pearson correlation coefficient）来衡量变量之间的关系，但它的局限性在于只能捕捉线性依赖。这个练习旨在挑战一个常见的误解：零相关等于不相关。通过分析一个皮尔逊相关系数近似为零而互信息（Mutual Information）很高的思想实验，你将深入理解为何需要更普适的统计依赖度量，并为探索非线性关系建立直观认识 。",
            "id": "1462533",
            "problem": "一位生物信息学家正在分析来自一个细胞群体的大型基因表达数据集，以推断其调控网络。他们专注于两个特定基因的表达水平，我们称之为Alpha基因 ($G_{\\alpha}$) 和Beta基因 ($G_{\\beta}$)。基因的表达水平是其活性的度量，通过其对应信使RNA的浓度来量化。\n\n统计分析得出两个关键结果：\n1. 皮尔逊相关系数，用于衡量两个变量之间*线性*关系的强度，经计算得出 $G_{\\alpha}$ 和 $G_{\\beta}$ 表达水平之间的该系数值约等于零 ($r \\approx 0$)。\n2. 互信息，用于衡量两个变量之间的一般统计依赖性（包括线性和非线性），被发现对于同一对基因而言显著很高。高的互信息值表明，了解一个基因的表达水平可以提供关于另一个基因表达水平的大量信息。\n\n基于这两项统计观察，以下哪种生物学情景最能合理解释Alpha基因和Beta基因之间的关系？\n\nA. $G_{\\alpha}$ 产生的蛋白质是一种简单的转录因子，它直接且线性地激活 $G_{\\beta}$ 的表达。\n\nB. $G_{\\alpha}$ 和 $G_{\\beta}$ 的表达彼此完全独立。\n\nC. $G_{\\alpha}$ 产生的蛋白质是一种简单的转录因子，它直接且线性地抑制 $G_{\\beta}$ 的表达。\n\nD. $G_{\\alpha}$ 产生的蛋白质对 $G_{\\beta}$ 具有复杂的、非单调的调控效应。例如，它可能在低浓度时适度激活 $G_{\\beta}$，但在高浓度时强烈抑制它，从而形成一种高度结构化但非线性的关系。\n\nE. 实验中的技术错误导致 $G_{\\alpha}$ 的测量数据被随机打乱，使得任何真实的关系都无法被检测到。",
            "solution": "设 $X$ 表示 $G_{\\alpha}$ 的表达水平，$Y$ 表示 $G_{\\beta}$ 的表达水平。皮尔逊相关系数为\n$$\nr=\\frac{\\operatorname{Cov}(X,Y)}{\\sigma_{X}\\sigma_{Y}},\n$$\n因此 $r\\approx 0$ 意味着 $\\operatorname{Cov}(X,Y)\\approx 0$，这排除了强线性关联的可能性，但并不意味着独立。\n\n互信息为\n$$\nI(X;Y)=\\iint p_{X,Y}(x,y)\\,\\ln\\!\\left(\\frac{p_{X,Y}(x,y)}{p_{X}(x)p_{Y}(y)}\\right)\\,\\mathrm{d}x\\,\\mathrm{d}y.\n$$\n$I(X;Y)$ 的值显著很高表明 $X$ 和 $Y$ 之间存在显著的统计依赖性（线性或非线性）；特别地，$I(X;Y)=0$ 当且仅当 $X$ 和 $Y$ 独立。\n\n根据这两个约束条件评估各个选项：\n\n1. 选项A和C假设了一种简单的直接线性调控效应，可以建模为 $Y=aX+b+\\varepsilon$，其中 $a > 0$ (激活) 或 $a  0$ (抑制)，而 $\\varepsilon$ 是独立于 $X$ 的噪声。在这种情况下，只要信号项 $aX$ 不可忽略，$\\operatorname{Cov}(X,Y)$ 的符号就与 $a$ 相同，且 $|r|$ 的量级通常很大。这与观察到的 $r\\approx 0$ 相矛盾。此外，如果噪声大到足以使 $r\\approx 0$，那么互信息也应该会降低，而不是显著很高。因此，A和C与 $(r\\approx 0,\\ I(X;Y)\\ \\text{高})$ 这对观察结果不一致。\n\n2. 选项B断言独立性。独立性意味着 $I(X;Y)=0$，这与观察到的高互信息相矛盾。因此B是无效的。\n\n3. 选项E描述了对 $X$ 测量值的随机打乱，这会破坏任何依赖关系，同样会得出 $I(X;Y)\\approx 0$，再次与观察到的高互信息相矛盾。因此E是无效的。\n\n4. 选项D假设了一种复杂的、非单调的调控关系。一个典型的例子是双相或U形依赖关系，例如 $Y=f(X)$，其中 $f$ 是非单调且对称的函数。例如，如果 $f(x)=x^{2}$ 并且 $X$ 服从均值为零、方差有限的对称分布，则\n$$\n\\operatorname{Cov}(X,Y)=\\operatorname{Cov}(X,X^{2})=\\mathbb{E}[X^{3}]-\\mathbb{E}[X]\\mathbb{E}[X^{2}]=0,\n$$\n因此 $r=0$，而 $X$ 和 $Y$ 显然是相关的。事实上，对于一个确定性的非常数映射 $Y=f(X)$，我们有 $I(X;Y)=H(Y) > 0$ 因为 $H(Y|X)=0$，所以尽管线性相关为零，互信息仍然可以很大。在生物学上，一个在低浓度时轻微激活、在高浓度时又抑制的调控因子，会产生这样一种结构化的、非线性的、非单调的关系，与观察结果相符。\n\n因此，唯一与 $r\\approx 0$ 和高 $I(X;Y)$ 一致的选项是，$G_{\\alpha}$ 对 $G_{\\beta}$ 施加一种复杂的、非单调的调控效应。",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "为了检测任意复杂的非线性依赖关系，统计学与机器学习领域发展出了多种强大的非参数检验方法。本练习将带你亲手实现并比较两种主流的度量：距离相关性（distance correlation）与希尔伯特-施密特独立性准则（Hilbert-Schmidt Independence Criterion, HSIC）。你不仅会通过代码在不同类型的合成数据上检验它们的表现，还将从第一性原理出发，推导出一个特定条件下两者之间的精确数学关系，从而深刻揭示它们内在的联系 。",
            "id": "3149079",
            "problem": "您的任务是实现并比较两种在深度学习中常用的统计依赖性度量：距离相关性（distance correlation）和希尔伯特-施密特独立性准则（Hilbert-Schmidt Independence Criterion, HSIC）。目标是从第一性原理出发，推导出这两种度量在何种条件下一致，并使用一个完整的、可运行的程序在非线性合成数据上测试它们的行为。\n\n定义与计算任务：\n- 设 $\\{(x_i,y_i)\\}_{i=1}^n$ 为 $n$ 组成对的观测值，其中 $x_i \\in \\mathbb{R}$ 且 $y_i \\in \\mathbb{R}$。定义中心化矩阵 $H = I_n - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵，$\\mathbf{1}$ 是 $n$ 维全1向量。\n- 距离相关性：\n  - 对于指数 $\\alpha \\in (0,2]$，定义成对距离矩阵 $D^{(\\alpha)}_X$ 和 $D^{(\\alpha)}_Y$，其元素分别为 $(D^{(\\alpha)}_X)_{ij} = \\|x_i - x_j\\|^\\alpha$ 和 $(D^{(\\alpha)}_Y)_{ij} = \\|y_i - y_j\\|^\\alpha$。计算它们的双重中心化形式 $A_X = H D^{(\\alpha)}_X H$ 和 $A_Y = H D^{(\\alpha)}_Y H$。\n  - 样本距离协方差的平方为 $d\\mathrm{Cov}^2_{\\alpha}(X,Y) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n (A_X)_{ij} (A_Y)_{ij}$。样本距离协方差为 $d\\mathrm{Cov}_{\\alpha}(X,Y) = \\sqrt{d\\mathrm{Cov}^2_{\\alpha}(X,Y)}$。\n  - 样本距离方差为 $d\\mathrm{Var}_{\\alpha}(X) = d\\mathrm{Cov}_{\\alpha}(X,X)$，类似地 $d\\mathrm{Var}_{\\alpha}(Y) = d\\mathrm{Cov}_{\\alpha}(Y,Y)$。样本距离相关性为 $d\\mathrm{Cor}_{\\alpha}(X,Y) = \\frac{d\\mathrm{Cov}_{\\alpha}(X,Y)}{\\sqrt{d\\mathrm{Var}_{\\alpha}(X)\\, d\\mathrm{Var}_{\\alpha}(Y)}}$，约定如果分母为 $0$，则 $d\\mathrm{Cor}_{\\alpha}(X,Y)$ 为 $0$。\n- 希尔伯特-施密特独立性准则 (HSIC)：\n  - 考虑在 $\\mathcal{X}$ 上的再生核希尔伯特空间 (RKHS) 核函数 $k_X$ 和在 $\\mathcal{Y}$ 上的核函数 $k_Y$。通过 $K_{ij} = k_X(x_i,x_j)$ 和 $L_{ij} = k_Y(y_i,y_j)$ 构建格拉姆矩阵 (Gram matrices) $K$ 和 $L$，然后通过 $K_c = H K H$ 和 $L_c = H L H$ 对其进行中心化。\n  - 有偏样本 HSIC 为 $\\mathrm{HSIC}(X,Y;k_X,k_Y) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n (K_c)_{ij} (L_c)_{ij}$。\n  - 需要实现的核函数：\n    1. 线性核：$k_{\\mathrm{lin}}(x,x') = x x'$。\n    2. 高斯核：$k_{\\mathrm{gauss}}(x,x';\\sigma) = \\exp\\!\\left(-\\frac{\\|x-x'\\|^2}{2\\sigma^2}\\right)$，其中带宽参数 $\\sigma > 0$。\n    3. 拉普拉斯核：$k_{\\mathrm{lap}}(x,x';\\beta) = \\exp\\!\\left(-\\frac{\\|x-x'\\|}{\\beta}\\right)$，其中尺度参数 $\\beta > 0$。\n\n推导要求：\n- 从上述定义出发，代数推导出使用线性核的 $\\mathrm{HSIC}$ 与指数 $\\alpha = 2$ 的距离协方差的平方在何种条件下一致（相差一个常数因子）。具体而言，证明当 $k_X$ 和 $k_Y$ 是 $\\mathbb{R}$ 上的线性核时，有 $H D^{(2)}_X H = -2 H K H$ 和 $H D^{(2)}_Y H = -2 H L H$，这意味着 $d\\mathrm{Cov}^2_{2}(X,Y) = 4\\,\\mathrm{HSIC}(X,Y;k_{\\mathrm{lin}},k_{\\mathrm{lin}})$。您的程序应在每个测试用例中，通过一个在严格容差范围内的布尔值检查，来数值上验证此比例关系。\n\n合成数据与参数（为保证可复现性，请使用这些精确设置）：\n- 所有角度均为实数值；不涉及角度单位。\n- 不涉及物理单位。\n- 使用以下包含 $5$ 个数据集的测试套件，每个数据集都有固定的随机种子和样本大小：\n  1. 独立噪声：$n = 300$，种子 $= 42$。对于 $i=1,\\dots,n$，独立地从 $\\mathcal{N}(0,1)$ 中抽取 $x_i$ 和从 $\\mathcal{N}(0,1)$ 中抽取 $y_i$。\n  2. 带噪声的线性依赖：$n = 300$，种子 $= 123$。从 $\\mathcal{N}(0,1)$ 中抽取 $x_i$，并令 $y_i = 2 x_i + 0.1 \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0,1)$ 且与 $x_i$ 独立。\n  3. 非线性正弦依赖：$n = 300$，种子 $= 7$。从 $\\mathrm{Uniform}(-3,3)$ 中抽取 $x_i$，并令 $y_i = \\sin(x_i) + 0.2 \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0,1)$ 且与 $x_i$ 独立。\n  4. 非线性二次，小样本（边界情况）：$n = 5$，种子 $= 999$。从 $\\mathrm{Uniform}(-2,2)$ 中抽取 $x_i$，并令 $y_i = x_i^2 + 0.05 \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0,1)$。\n  5. 恒定响应（边缘情况）：$n = 100$，种子 $= 2024$。从 $\\mathcal{N}(0,1)$ 中抽取 $x_i$，并对所有 $i$ 设置 $y_i = 0$。\n\n需要比较的核参数：\n- 高斯核带宽 $\\sigma = 0.5$ 和 $\\sigma = 2.0$。\n- 拉普拉斯核尺度 $\\beta = 0.5$。\n\n程序要求：\n- 对每个数据集，计算：\n  1. $d\\mathrm{Cor}_{1}(X,Y)$。\n  2. $d\\mathrm{Cov}^2_{2}(X,Y)$。\n  3. $\\mathrm{HSIC}(X,Y;k_{\\mathrm{lin}},k_{\\mathrm{lin}})$。\n  4. $\\mathrm{HSIC}(X,Y;k_{\\mathrm{gauss}}(\\cdot;\\sigma{=}0.5),k_{\\mathrm{gauss}}(\\cdot;\\sigma{=}0.5))$。\n  5. $\\mathrm{HSIC}(X,Y;k_{\\mathrm{gauss}}(\\cdot;\\sigma{=}2.0),k_{\\mathrm{gauss}}(\\cdot;\\sigma{=}2.0))$。\n  6. $\\mathrm{HSIC}(X,Y;k_{\\mathrm{lap}}(\\cdot;\\beta{=}0.5),k_{\\mathrm{lap}}(\\cdot;\\beta{=}0.5))$。\n  7. 一个布尔值，指示 $4 \\times \\mathrm{HSIC}(X,Y;k_{\\mathrm{lin}},k_{\\mathrm{lin}})$ 是否在容差 $\\mathrm{rtol} = 10^{-10}$ 和 $\\mathrm{atol} = 10^{-10}$ 范围内与 $d\\mathrm{Cov}^2_{2}(X,Y)$ 在数值上相等。\n- 数值稳定性：如果 $d\\mathrm{Var}_{1}(Y) = 0$ 或 $d\\mathrm{Var}_{1}(X) = 0$，则对该数据集返回 $d\\mathrm{Cor}_{1}(X,Y) = 0$。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个有 5 个内部列表的列表（每个数据集对应一个）。每个内部列表必须按给定顺序包含上述 7 个值，形式为用方括号括起来的逗号分隔列表。例如，一个有效的输出形式为 $[[v_{1,1},\\dots,v_{1,7}],[v_{2,1},\\dots,v_{2,7}],\\dots,[v_{5,1},\\dots,v_{5,7}]]$，其中每个 $v_{i,j}$ 是布尔值或浮点数。",
            "solution": "用户提供了一个有科学依据、定义明确且包含完整解决方案所需全部信息的问题。距离相关性和希尔伯特-施密特独立性准则 (HSIC) 的定义在统计学和机器学习领域是标准的。关于推导线性核的平方距离协方差与HSIC之间关系的要求，是一项有效且具有指导意义的理论练习。数值部分被足够详细地说明，包括数据集、参数和随机种子，以确保可复现性。因此，该问题被认定为有效。\n\n按照要求，解决方案包括两部分：一个形式化的数学推导和一个用于数值验证的完整Python程序。\n\n### 第1部分：推导\n\n目标是建立指数 $\\alpha=2$ 的样本距离协方差的平方（表示为 $d\\mathrm{Cov}^2_{2}(X,Y)$）与使用线性核的样本希尔伯特-施密特独立性准则（$\\mathrm{HSIC}(X,Y; k_{\\mathrm{lin}}, k_{\\mathrm{lin}})$）之间的直接比例关系。我们将从为 $\\mathbb{R}$ 中数据提供的定义开始。\n\n设我们的数据为 $n$ 对 $\\{(x_i, y_i)\\}_{i=1}^n$ 的集合，其中每个 $x_i \\in \\mathbb{R}$ 且 $y_i \\in \\mathbb{R}$。设数据向量为 $\\mathbf{x} = (x_1, \\dots, x_n)^\\top$ 和 $\\mathbf{y} = (y_1, \\dots, y_n)^\\top$。\n\n**步骤1：分析距离矩阵 $D^{(2)}_X$。**\n指数 $\\alpha=2$ 的距离协方差的平方涉及到成对距离矩阵 $D^{(2)}_X$，其元素定义为 $(D^{(2)}_X)_{ij} = \\|x_i - x_j\\|^2$。由于 $x_i, x_j \\in \\mathbb{R}$，这可以简化为：\n$$ (D^{(2)}_X)_{ij} = (x_i - x_j)^2 = x_i^2 - 2x_i x_j + x_j^2 $$\n我们可以用数据向量 $\\mathbf{x}$ 来表示矩阵 $D^{(2)}_X$。设 $\\mathbf{x}^{\\circ 2}$ 是逐元素平方的向量，即 $\\mathbf{x}^{\\circ 2} = (x_1^2, x_2^2, \\dots, x_n^2)^\\top$。设 $\\mathbf{1}$ 是 $n$ 维全1列向量。\n对于所有 $j=1, \\dots, n$，$x_i^2$ 项对应于矩阵 $\\mathbf{x}^{\\circ 2} \\mathbf{1}^\\top$。\n对于所有 $i=1, \\dots, n$，$x_j^2$ 项对应于矩阵 $\\mathbf{1} (\\mathbf{x}^{\\circ 2})^\\top$。\n$-2x_i x_j$ 项对应于矩阵 $-2 \\mathbf{x} \\mathbf{x}^\\top$。\n因此，距离矩阵可以写为：\n$$ D^{(2)}_X = \\mathbf{x}^{\\circ 2} \\mathbf{1}^\\top + \\mathbf{1} (\\mathbf{x}^{\\circ 2})^\\top - 2 \\mathbf{x} \\mathbf{x}^\\top $$\n\n**步骤2：将 $D^{(2)}_X$ 与线性核的格拉姆矩阵 $K$ 联系起来。**\n对于线性核 $k_{\\mathrm{lin}}(x, x') = x x'$，其格拉姆矩阵 $K$ 由 $K_{ij} = x_i x_j$ 给出。其矩阵形式为 $K = \\mathbf{x} \\mathbf{x}^\\top$。\n将此代入 $D^{(2)}_X$ 的表达式中，我们得到：\n$$ D^{(2)}_X = \\mathbf{x}^{\\circ 2} \\mathbf{1}^\\top + \\mathbf{1} (\\mathbf{x}^{\\circ 2})^\\top - 2 K $$\n\n**步骤3：应用双重中心化操作。**\n双重中心化距离矩阵定义为 $A_X = H D^{(2)}_X H$，其中 $H = I_n - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^\\top$ 是中心化矩阵。$H$ 的一个关键性质是它能消除常数向量，即 $H \\mathbf{1} = \\mathbf{0}$ 和 $\\mathbf{1}^\\top H = \\mathbf{0}^\\top$。\n将此应用于我们对 $D^{(2)}_X$ 的表达式：\n$$ A_X = H \\left( \\mathbf{x}^{\\circ 2} \\mathbf{1}^\\top + \\mathbf{1} (\\mathbf{x}^{\\circ 2})^\\top - 2 K \\right) H $$\n使用矩阵乘法的分配律：\n$$ A_X = H(\\mathbf{x}^{\\circ 2} \\mathbf{1}^\\top)H + H(\\mathbf{1} (\\mathbf{x}^{\\circ 2})^\\top)H - 2 H K H $$\n现在我们使用性质 $H\\mathbf{1}=\\mathbf{0}$ 来计算前两项：\n$$ H(\\mathbf{x}^{\\circ 2} \\mathbf{1}^\\top)H = (H \\mathbf{x}^{\\circ 2})(\\mathbf{1}^\\top H) = (H \\mathbf{x}^{\\circ 2}) \\mathbf{0}^\\top = \\mathbf{0} $$\n$$ H(\\mathbf{1} (\\mathbf{x}^{\\circ 2})^\\top)H = (H \\mathbf{1})((\\mathbf{x}^{\\circ 2})^\\top H) = \\mathbf{0} ((\\mathbf{x}^{\\circ 2})^\\top H) = \\mathbf{0} $$\n前两项是零矩阵。这使得我们得到：\n$$ A_X = -2 H K H $$\n根据定义，$H K H$ 项是中心化的格拉姆矩阵 $K_c$。因此，我们证明了：\n$$ A_X = H D^{(2)}_X H = -2 K_c $$\n对 $Y$ 变量进行相同的推导，可得出 $A_Y = H D^{(2)}_Y H = -2 L_c$，其中 $L_c = H L H$，$L$ 是对 $Y$ 使用线性核的格拉姆矩阵。\n\n**步骤4：建立最终的比例关系。**\n样本距离协方差的平方定义为：\n$$ d\\mathrm{Cov}^2_{2}(X,Y) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n (A_X)_{ij} (A_Y)_{ij} $$\n代入我们对 $A_X$ 和 $A_Y$ 的结果：\n$$ d\\mathrm{Cov}^2_{2}(X,Y) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n (-2 (K_c)_{ij}) (-2 (L_c)_{ij}) $$\n$$ d\\mathrm{Cov}^2_{2}(X,Y) = \\frac{4}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n (K_c)_{ij} (L_c)_{ij} $$\n有偏样本 HSIC 的定义是：\n$$ \\mathrm{HSIC}(X,Y; k_X, k_Y) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n (K_c)_{ij} (L_c)_{ij} $$\n通过比较这两个表达式，我们得出所期望的结论：\n$$ d\\mathrm{Cov}^2_{2}(X,Y) = 4\\,\\mathrm{HSIC}(X,Y; k_{\\mathrm{lin}}, k_{\\mathrm{lin}}) $$\n推导至此完成。对于一维数据，指数 $\\alpha=2$ 的距离协方差的平方恰好是使用线性核的 HSIC 的四倍。这一关系将在实现中进行数值验证。",
            "answer": "```python\nimport numpy as np\n\ndef generate_data(case_id, n, seed):\n    \"\"\"Generates synthetic data for a given test case.\"\"\"\n    rng = np.random.default_rng(seed)\n    if case_id == 1:  # Independent noise\n        x = rng.normal(0, 1, size=n)\n        y = rng.normal(0, 1, size=n)\n    elif case_id == 2:  # Linear dependence\n        x = rng.normal(0, 1, size=n)\n        eps = rng.normal(0, 1, size=n)\n        y = 2 * x + 0.1 * eps\n    elif case_id == 3:  # Nonlinear sine dependence\n        x = rng.uniform(-3, 3, size=n)\n        eps = rng.normal(0, 1, size=n)\n        y = np.sin(x) + 0.2 * eps\n    elif case_id == 4:  # Nonlinear quadratic\n        x = rng.uniform(-2, 2, size=n)\n        eps = rng.normal(0, 1, size=n)\n        y = x**2 + 0.05 * eps\n    elif case_id == 5:  # Constant response\n        x = rng.normal(0, 1, size=n)\n        y = np.zeros(n)\n    else:\n        raise ValueError(\"Invalid case ID\")\n    return x.reshape(-1, 1), y.reshape(-1, 1)\n\ndef double_center(M):\n    \"\"\"Performs double centering on a matrix M efficiently.\"\"\"\n    mean_r = M.mean(axis=1, keepdims=True)\n    mean_c = M.mean(axis=0, keepdims=True)\n    mean_all = M.mean()\n    return M - mean_r - mean_c + mean_all\n\ndef d_cov_sq(X, Y, alpha):\n    \"\"\"Computes the squared sample distance covariance.\"\"\"\n    n = X.shape[0]\n    if n == 0:\n        return 0.0\n    \n    dist_X = np.abs(X - X.T)\n    dist_Y = np.abs(Y - Y.T)\n\n    D_X_alpha = dist_X ** alpha\n    D_Y_alpha = dist_Y ** alpha\n\n    A_X = double_center(D_X_alpha)\n    A_Y = double_center(D_Y_alpha)\n    \n    return np.sum(A_X * A_Y) / (n ** 2)\n\ndef d_cor(X, Y, alpha):\n    \"\"\"Computes the sample distance correlation.\"\"\"\n    d_cov_sq_xy = d_cov_sq(X, Y, alpha)\n    d_cov_sq_xx = d_cov_sq(X, X, alpha)\n    d_cov_sq_yy = d_cov_sq(Y, Y, alpha)\n\n    d_cov_xy = np.sqrt(max(0, d_cov_sq_xy))\n    d_var_x = np.sqrt(max(0, d_cov_sq_xx))\n    d_var_y = np.sqrt(max(0, d_cov_sq_yy))\n\n    denom = np.sqrt(d_var_x * d_var_y)\n    \n    if np.isclose(denom, 0.0):\n        return 0.0\n    \n    return d_cov_xy / denom\n\ndef kernel_linear(X1, X2):\n    \"\"\"Linear kernel for (n, 1) data.\"\"\"\n    return X1 @ X2.T\n\ndef kernel_gaussian(X1, X2, sigma):\n    \"\"\"Gaussian RBF kernel for (n, 1) data.\"\"\"\n    dist_sq = (X1 - X2.T)**2\n    return np.exp(-dist_sq / (2 * sigma**2))\n\ndef kernel_laplacian(X1, X2, beta):\n    \"\"\"Laplacian kernel for (n, 1) data.\"\"\"\n    dist = np.abs(X1 - X2.T)\n    return np.exp(-dist / beta)\n\ndef hsic(X, Y, kernel, kernel_params={}):\n    \"\"\"Computes the biased sample HSIC.\"\"\"\n    n = X.shape[0]\n    if n == 0:\n        return 0.0\n        \n    K = kernel(X, X, **kernel_params)\n    L = kernel(Y, Y, **kernel_params)\n    \n    K_c = double_center(K)\n    L_c = double_center(L)\n    \n    return np.sum(K_c * L_c) / (n ** 2)\n\ndef solve():\n    \"\"\"Main function to run the analysis.\"\"\"\n    test_cases = [\n        {'id': 1, 'n': 300, 'seed': 42},\n        {'id': 2, 'n': 300, 'seed': 123},\n        {'id': 3, 'n': 300, 'seed': 7},\n        {'id': 4, 'n': 5, 'seed': 999},\n        {'id': 5, 'n': 100, 'seed': 2024},\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        X, Y = generate_data(case['id'], case['n'], case['seed'])\n        \n        # 1. dCor_1(X, Y)\n        dcor1 = d_cor(X, Y, alpha=1)\n        \n        # 2. dCov^2_2(X, Y)\n        dcov_sq2 = d_cov_sq(X, Y, alpha=2)\n        \n        # 3. HSIC with linear kernel\n        hsic_lin = hsic(X, Y, kernel_linear)\n        \n        # 4. HSIC with Gaussian kernel (sigma=0.5)\n        hsic_gauss_05 = hsic(X, Y, kernel_gaussian, kernel_params={'sigma': 0.5})\n        \n        # 5. HSIC with Gaussian kernel (sigma=2.0)\n        hsic_gauss_20 = hsic(X, Y, kernel_gaussian, kernel_params={'sigma': 2.0})\n\n        # 6. HSIC with Laplacian kernel (beta=0.5)\n        hsic_lap_05 = hsic(X, Y, kernel_laplacian, kernel_params={'beta': 0.5})\n\n        # 7. Numerical check of the derived proportionality\n        is_proportional = np.allclose(dcov_sq2, 4 * hsic_lin, rtol=1e-10, atol=1e-10)\n\n        case_results = [\n            dcor1,\n            dcov_sq2,\n            hsic_lin,\n            hsic_gauss_05,\n            hsic_gauss_20,\n            hsic_lap_05,\n            is_proportional\n        ]\n        all_results.append(case_results)\n    \n    # Print the final list of lists as a string\n    print(str(all_results))\n\nsolve()\n```"
        },
        {
            "introduction": "在深度学习中，学习“解耦”的表征（disentangled representations）是一个核心目标，它意味着让模型的潜在变量各自捕捉到数据中独立的变化因素。这个实践练习将抽象的统计依赖概念与这一前沿应用直接联系起来。你将模拟一个 $\\beta$-VAE 模型的核心机制，通过计算总相关性（Total Correlation, $TC$）来量化潜在变量之间的依赖程度，并亲眼见证如何通过惩罚 $TC$ 来提升模型的解耦性能，这一性能的提升将通过互信息间隙（Mutual Information Gap, MIG）指标来衡量 。",
            "id": "3149107",
            "problem": "您将执行一项仿真任务，以研究深度学习潜变量背景下一种名为“总相关”的统计依赖性度量，及其与解耦的关系。考虑一个由 β-变分自编码器 (β-Variational Autoencoder, β-VAE) 的假设编码器生成的、包含 $d$ 个变量 $Z = (Z_1, Z_2, Z_3)$ 的潜变量表示。编码器的聚合后验被建模为独立真实因子的线性高斯映射。设 $V = (V_1, V_2, V_3)$ 是一个由独立因子组成的向量，其中每个 $V_i$ 服从标准正态分布。定义一个混合矩阵 $A \\in \\mathbb{R}^{3 \\times 3}$ 和加性高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2, \\sigma_3^2))$。潜变量由 $N$ 个独立样本通过 $Z = A V + \\varepsilon$ 生成。\n\n需要估计的统计依赖性度量是总相关 $TC(Z)$，它通过微分熵从第一性原理定义为：\n$$TC(Z) = \\sum_{i=1}^{3} H(Z_i) - H(Z),$$\n其中 $H(\\cdot)$ 表示微分熵。在经过充分检验的高斯近似下，一个协方差为 $\\Sigma$ 的 $d$ 维多维正态分布的微分熵为：\n$$H(Z) = \\frac{1}{2} \\log\\!\\big( (2\\pi e)^d \\det(\\Sigma) \\big),$$\n而一个方差为 $\\sigma_i^2$ 的标量正态分布的边际熵为：\n$$H(Z_i) = \\frac{1}{2} \\log\\!\\big( 2\\pi e \\, \\sigma_i^2 \\big).$$\n由此可得一个基于 $Z$ 的样本协方差矩阵 $\\Sigma_Z$ 及其对角元素的 $TC(Z)$ 可计算表达式。\n\n为了将 $TC(Z)$ 与解耦联系起来，您还将计算潜变量与真实因子之间的互信息间隙 (Mutual Information Gap, MIG)。对于每个因子 $V_k$ 和每个潜变量 $Z_i$，在联合高斯模型下，使用 $Z_i$ 和 $V_k$ 之间的相关系数 $\\rho_{ik}$ 估计互信息 $I(Z_i; V_k)$：\n$$I(Z_i; V_k) = -\\frac{1}{2} \\log(1 - \\rho_{ik}^2).$$\n然后，MIG 定义为在所有因子上，最大互信息值与第二大互信息值之间差距的平均值：\n$$\\mathrm{MIG} = \\frac{1}{3} \\sum_{k=1}^{3} \\Big( \\max_{i} I(Z_i; V_k) - \\max_{i \\neq i^*(k)} I(Z_i; V_k) \\Big),$$\n其中 $i^*(k)$ 是对因子 $k$ 实现最大值的索引。\n\n为了模拟类似于在 β-VAE 中增加 $\\beta$ 参数的总相关惩罚，对 $Z$ 应用一个部分白化变换，以逐步消除相关性：\n$$Z^{(\\alpha)} = \\Sigma_Z^{-\\alpha/2} \\, (Z - \\mathbb{E}[Z]),$$\n其中 $\\alpha \\in [0,1]$ 是一个惩罚强度。当 $\\alpha = 0$ 时，$Z^{(0)} = Z - \\mathbb{E}[Z]$；当 $\\alpha = 1$ 时，$Z^{(1)}$ 被完全白化且具有单位协方差，这在高斯假设下理想地最小化了 $TC(Z)$。您将计算 $TC(Z^{(\\alpha)})$ 和 $\\mathrm{MIG}(Z^{(\\alpha)}, V)$，并将其与原始的 $TC(Z)$ 和 $\\mathrm{MIG}(Z, V)$ 进行比较。\n\n所有信息量必须以奈特（自然对数底）为单位表示。\n\n实现一个完整、可运行的程序，该程序：\n- 模拟 $N$ 个 $V \\sim \\mathcal{N}(0, I_3)$ 的样本。\n- 使用指定的对角标准差生成 $Z = A V + \\varepsilon$ 作为独立高斯噪声。\n- 使用高斯微分熵公式，根据 $Z$ 的样本协方差计算 $TC(Z)$。\n- 使用联合高斯互信息公式 $I(Z_i; V_k) = -\\frac{1}{2}\\log(1-\\rho_{ik}^2)$ 计算 $\\mathrm{MIG}(Z, V)$，其中 $\\rho_{ik}$ 是 $Z_i$ 和 $V_k$ 之间的经验相关性。\n- 对指定的 $\\alpha$ 值应用部分白化变换 $Z^{(\\alpha)} = \\Sigma_Z^{-\\alpha/2}(Z - \\mathbb{E}[Z])$，并重新计算 $TC(Z^{(\\alpha)})$ 和 $\\mathrm{MIG}(Z^{(\\alpha)}, V)$。\n\n使用以下参数值测试套件以覆盖不同情况：\n- 测试用例 1 (正常路径)：$A = \\begin{bmatrix} 1.0  0.6  0.0 \\\\ 0.4  1.0  0.5 \\\\ 0.0  0.3  1.0 \\end{bmatrix}$，噪声标准差 $(\\sigma_1,\\sigma_2,\\sigma_3) = (0.1, 0.1, 0.1)$，$N = 20000$，$\\alpha \\in \\{0.5, 1.0\\}$。\n- 测试用例 2 (边界情况，已因子化)：$A = I_3$，噪声标准差 $(\\sigma_1,\\sigma_2,\\sigma_3) = (0.05, 0.05, 0.05)$，$N = 20000$，$\\alpha \\in \\{0.5, 1.0\\}$。\n- 测试用例 3 (边缘情况，强混合)：$A = \\begin{bmatrix} 1.0  1.1  1.2 \\\\ 0.9  1.0  1.1 \\\\ 1.1  1.2  1.3 \\end{bmatrix}$，噪声标准差 $(\\sigma_1,\\sigma_2,\\sigma_3) = (0.2, 0.2, 0.2)$，$N = 20000$，$\\alpha \\in \\{0.5, 1.0\\}$。\n\n对于每个测试用例，您的程序必须输出一个列表，其中包含恰好六个以奈特为单位的浮点数：\n$[TC(Z), \\mathrm{MIG}(Z,V), TC(Z^{(0.5)}), \\mathrm{MIG}(Z^{(0.5)},V), TC(Z^{(1.0)}), \\mathrm{MIG}(Z^{(1.0)},V)]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内没有空格，每个元素对应一个测试用例的六个数字列表。例如，结构必须是 $[[r_{1,1},r_{1,2},r_{1,3},r_{1,4},r_{1,5},r_{1,6}],[r_{2,1},\\dots,r_{2,6}],[r_{3,1},\\dots,r_{3,6}]]$.\n- 所有值必须以奈特（自然对数底）为单位。\n- 无需用户输入；使用固定的随机种子以确保可复现性。",
            "solution": "该问题是有效的，因为它在信息论和统计学方面有科学依据，定义明确，包含了所有必要的参数和定义，并提出了一个与深度学习研究相关的、计算上可行的仿真任务。\n\n该问题要求对一个线性高斯潜变量模型 $Z = AV + \\varepsilon$ 进行仿真，并计算两个关键指标：总相关 ($TC$) 和互信息间隙 ($MIG$)。然后，在对潜变量应用部分白化变换后，重新评估这些指标。整个过程将针对三个不同的测试用例进行实现。\n\n首先，让我们确定模型的统计特性。真实因子 $V$ 是一个 $d=3$ 维的随机向量，服从标准多维正态分布，$V \\sim \\mathcal{N}(0, I_3)$，其中 $I_3$ 是 $3 \\times 3$ 的单位矩阵。加性噪声 $\\varepsilon$ 也是一个 $d=3$ 维的高斯向量，$\\varepsilon \\sim \\mathcal{N}(0, \\Sigma_\\varepsilon)$，其中 $\\Sigma_\\varepsilon = \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2, \\sigma_3^2)$ 是一个对角协方差矩阵。由于 $Z$ 是独立高斯向量 $V$ 和 $\\varepsilon$ 的线性变换，因此 $Z$ 本身是一个多维高斯变量。\n$Z$ 的均值为 $\\mathbb{E}[Z] = \\mathbb{E}[AV + \\varepsilon] = A\\mathbb{E}[V] + \\mathbb{E}[\\varepsilon] = A \\cdot 0 + 0 = 0$。\n$Z$ 的协方差，记作 $\\Sigma_Z$，为：\n$$ \\Sigma_Z = \\mathrm{Cov}(AV + \\varepsilon) = \\mathrm{Cov}(AV) + \\mathrm{Cov}(\\varepsilon) $$\n这是由于 $V$ 和 $\\varepsilon$ 的独立性。各项分别为：\n$$ \\mathrm{Cov}(AV) = A \\mathrm{Cov}(V) A^T = A I_3 A^T = AA^T $$\n$$ \\mathrm{Cov}(\\varepsilon) = \\Sigma_\\varepsilon $$\n因此，$Z$ 的理论协方差是 $\\Sigma_Z = AA^T + \\Sigma_\\varepsilon$。在仿真中，我们将使用从 $N$ 个生成样本中估计出的样本协方差矩阵 $\\hat{\\Sigma}_Z$。\n\n第一个指标，总相关，衡量 $Z$ 各分量之间的总统计依赖性。它被定义为联合分布 $p(Z)$ 与其边际分布的乘积 $\\prod_i p(Z_i)$ 之间的 Kullback-Leibler 散度。对于连续变量，这用微分熵表示：\n$$ TC(Z) = \\sum_{i=1}^{3} H(Z_i) - H(Z) $$\n在高斯假设下，协方差为 $\\Sigma$ 的 $d$ 维变量的微分熵为 $H(Z) = \\frac{1}{2} \\log\\!\\big( (2\\pi e)^d \\det(\\Sigma) \\big)$，而方差为 $\\sigma_{Z_i}^2$ 的单个分量 $Z_i$ 的熵为 $H(Z_i) = \\frac{1}{2} \\log\\!\\big( 2\\pi e \\, \\sigma_{Z_i}^2 \\big)$。方差 $\\sigma_{Z_i}^2$ 对应于协方差矩阵 $\\Sigma_Z$ 的第 $i$ 个对角元素，即 $\\sigma_{Z_i}^2 = (\\Sigma_Z)_{ii}$。\n将这些代入 $TC$ 公式并简化，我们得到一个基于协方差矩阵的可计算表达式：\n$$ TC(Z) = \\frac{1}{2} \\left[ \\log\\left(\\prod_{i=1}^{3} (\\Sigma_Z)_{ii}\\right) - \\log(\\det(\\Sigma_Z)) \\right] $$\n此公式将应用于样本协方差矩阵 $\\hat{\\Sigma}_Z$。\n\n第二个指标，互信息间隙，量化了学习到的潜变量表示 $Z$ 与真实因子 $V$ 之间的解耦程度。它需要计算每个潜变量 $Z_i$ 和每个因子 $V_k$ 之间的成对互信息 $I(Z_i; V_k)$。由于 $(Z, V)$ 的联合分布是高斯分布，互信息可以从它们的相关系数 $\\rho_{ik} = \\mathrm{Corr}(Z_i, V_k)$ 计算得出：\n$$ I(Z_i; V_k) = -\\frac{1}{2} \\log(1 - \\rho_{ik}^2) $$\n相关系数将从生成的 $Z$ 和 $V$ 样本中根据经验估计。\n然后，$MIG$ 的计算方法是，对所有因子 $V_k$，计算其与潜变量的最高互信息值和次高互信息值之差的平均值：\n$$ \\mathrm{MIG} = \\frac{1}{3} \\sum_{k=1}^{3} \\Big( \\max_{i} I(Z_i; V_k) - \\max_{i \\neq i^*(k)} I(Z_i; V_k) \\Big) $$\n其中 $i^*(k)$ 是与因子 $V_k$ 具有最大互信息的潜变量的索引。高 $MIG$ 值表示每个因子都被一个单一、独立的潜变量清晰地捕获。\n\n研究的核心涉及一个部分白化变换，旨在减少 $Z$ 内部的相关性。变换后的变量 $Z^{(\\alpha)}$ 由下式给出：\n$$ Z^{(\\alpha)} = \\Sigma_Z^{-\\alpha/2} (Z - \\mathbb{E}[Z]) $$\n其中 $\\alpha \\in [0, 1]$ 是惩罚强度。矩阵的幂 $\\Sigma_Z^{-\\alpha/2}$ 是使用 $\\Sigma_Z$ 的特征分解来计算的。如果 $\\Sigma_Z = UDU^T$，其中 $D$ 是特征值的对角矩阵，而 $U$ 是特征向量矩阵，那么 $\\Sigma_Z^p = UD^p U^T$。在实现中，我们将使用样本协方差 $\\hat{\\Sigma}_Z$ 和样本均值 $\\bar{Z}$，并数值计算变换矩阵。`scipy.linalg.fractional_matrix_power` 函数提供了一个计算它的稳健方法。\n在对生成的样本 $Z$ 应用变换以获得 $Z^{(\\alpha)}$ 的样本后，我们将使用上述相同的方法重新计算 $TC(Z^{(\\alpha)})$ 和 $\\mathrm{MIG}(Z^{(\\alpha)}, V)$。对于 $\\alpha=0.0$，$Z^{(0)}$ 只是 $Z$ 的中心化版本，所以 $TC(Z^{(0)}) = TC(Z)$ 且 $\\mathrm{MIG}(Z^{(0)},V) = \\mathrm{MIG}(Z,V)$。对于 $\\alpha=1.0$，变量 $Z^{(1)}$ 被完全白化，其协方差矩阵是单位矩阵，理论上这将总相关最小化到 $0$。\n\n每个测试用例的总体算法如下：\n1.  使用固定的随机种子初始化一个随机数生成器，以确保可复现性。\n2.  生成 $N=20000$ 个因子向量 $V \\sim \\mathcal{N}(0, I_3)$ 的样本。\n3.  生成 $N=20000$ 个噪声向量 $\\varepsilon \\sim \\mathcal{N}(0, \\mathrm{diag}(\\sigma_i^2))$ 的样本。\n4.  通过 $Z = AV + \\varepsilon$ 计算潜变量样本。数据矩阵的形状将是 $(N, 3)$。\n5.  从样本数据中计算 $TC(Z)$ 和 $\\mathrm{MIG}(Z,V)$。\n6.  对于每个指定的 $\\alpha \\in \\{0.5, 1.0\\}$ 值：\n    a.  计算原始数据 $Z$ 的样本协方差矩阵 $\\hat{\\Sigma}_Z$。\n    b.  计算变换矩阵 $M = \\hat{\\Sigma}_Z^{-\\alpha/2}$。\n    c.  对数据进行中心化 $Z_{cent} = Z - \\bar{Z}$。\n    d.  应用变换：$Z^{(\\alpha)} = Z_{cent} M$。由于对于对称的 $\\hat{\\Sigma}_Z$，$M$ 是对称的，这等价于 $(M Z_{cent}^T)^T$。\n    e.  使用变换后的样本 $Z^{(\\alpha)}$ 和原始因子样本 $V$ 计算 $TC(Z^{(\\alpha)})$ 和 $\\mathrm{MIG}(Z^{(\\alpha)}, V)$。\n7.  收集最终输出所需的六个浮点数：$[TC(Z), \\mathrm{MIG}(Z,V), TC(Z^{(0.5)}), \\mathrm{MIG}(Z^{(0.5)},V), TC(Z^{(1.0)}), \\mathrm{MIG}(Z^{(1.0)},V)]$。\n对于所提供的所有三个测试用例，将重复此过程。",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # A fixed random seed for reproducibility is required.\n    RANDOM_SEED = 42\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([[1.0, 0.6, 0.0], [0.4, 1.0, 0.5], [0.0, 0.3, 1.0]]),\n            \"noise_stds\": np.array([0.1, 0.1, 0.1]),\n            \"N\": 20000,\n            \"alphas\": [0.5, 1.0]\n        },\n        {\n            \"A\": np.identity(3),\n            \"noise_stds\": np.array([0.05, 0.05, 0.05]),\n            \"N\": 20000,\n            \"alphas\": [0.5, 1.0]\n        },\n        {\n            \"A\": np.array([[1.0, 1.1, 1.2], [0.9, 1.0, 1.1], [1.1, 1.2, 1.3]]),\n            \"noise_stds\": np.array([0.2, 0.2, 0.2]),\n            \"N\": 20000,\n            \"alphas\": [0.5, 1.0]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        noise_stds = case[\"noise_stds\"]\n        N = case[\"N\"]\n        alphas = case[\"alphas\"]\n        \n        # Step 1: Generate data\n        d = A.shape[0]\n        V = rng.standard_normal(size=(N, d))\n        epsilon = rng.normal(loc=0.0, scale=noise_stds, size=(N, d))\n        Z = V @ A.T + epsilon\n\n        # Step 2: Calculate metrics for original Z\n        case_results = []\n        tc_z, mig_z = _calculate_metrics(Z, V)\n        case_results.extend([tc_z, mig_z])\n        \n        # Step 3: Apply partial-whitening transform and recalculate metrics\n        Z_centered = Z - Z.mean(axis=0)\n        cov_Z = np.cov(Z, rowvar=False)\n\n        for alpha in alphas:\n            # Compute the transformation matrix Sigma_Z^(-alpha/2)\n            # Use real part to discard negligible imaginary parts from numerical errors\n            transform_M = np.real(linalg.fractional_matrix_power(cov_Z, -alpha / 2.0))\n            \n            # Apply the transformation\n            Z_alpha = Z_centered @ transform_M\n            \n            # Calculate metrics for the transformed data\n            tc_alpha, mig_alpha = _calculate_metrics(Z_alpha, V)\n            case_results.extend([tc_alpha, mig_alpha])\n\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    outer_parts = []\n    for res_list in all_results:\n        # Round to a reasonable number of digits for consistent output\n        inner_str = f\"[{','.join(f'{x:.6f}' for x in res_list)}]\"\n        outer_parts.append(inner_str)\n    final_str = f\"[{','.join(outer_parts)}]\"\n    print(final_str)\n\ndef _calculate_metrics(Z_data, V_data):\n    \"\"\"\n    Calculates Total Correlation (TC) and Mutual Information Gap (MIG).\n    \"\"\"\n    # Calculate Total Correlation (TC)\n    cov_Z = np.cov(Z_data, rowvar=False)\n    # Use slogdet for numerical stability\n    s, logdet_cov = np.linalg.slogdet(cov_Z)\n    if s = 0: # Handle non-positive definite cases, though unlikely with N >> d\n        tc = np.nan\n    else:\n        log_prod_diag = np.sum(np.log(np.diag(cov_Z)))\n        tc = 0.5 * (log_prod_diag - logdet_cov)\n\n    # Calculate Mutual Information Gap (MIG)\n    d = Z_data.shape[1]\n    combined_data = np.hstack((Z_data, V_data))\n    corr_matrix = np.corrcoef(combined_data, rowvar=False)\n    \n    # Extract the correlation submatrix between Z and V\n    rho_ZV = corr_matrix[0:d, d:]\n\n    # Clip to avoid log(0) for perfect correlations\n    rho_ZV_sq = np.clip(rho_ZV**2, 0.0, 1.0 - 1e-12)\n    mi_matrix = -0.5 * np.log(1.0 - rho_ZV_sq)\n    \n    gaps = []\n    for k in range(d): # Iterate over factors V_k\n        mi_for_factor = mi_matrix[:, k]\n        sorted_mi = np.sort(mi_for_factor)\n        gap = sorted_mi[-1] - sorted_mi[-2]\n        gaps.append(gap)\n        \n    mig = np.mean(gaps)\n    \n    return tc, mig\n\nsolve()\n```"
        }
    ]
}