{
    "hands_on_practices": [
        {
            "introduction": "深度学习中的一个核心目标是学习到数据的有效表示。一个理想的表示既应当足够简单（即对输入信息进行压缩），又应当对我们关心的任务（如分类）具有强大的预测能力。信息瓶颈（Information Bottleneck, IB）原理为我们提供了量化这一根本性权衡的数学框架。这项实践将引导你亲手为一个经典的XOR分类任务设计编码器，你将通过枚举所有可能的编码方式，来寻找在压缩率 $I(T;X)$ 和预测信息 $I(T;Y)$ 之间达到最佳平衡的“网络层宽度” ()。这个练习将为你建立关于高效表示学习的直观理解，让你明白模型的每一“比特”信息都应用在刀刃上。",
            "id": "3138067",
            "problem": "考虑一个离散设置下的信息瓶颈（IB）原理和一个简单的分类任务。设 $X$ 是一个离散输入变量，取值为 $X \\in \\{(0,0),(0,1),(1,0),(1,1)\\}$ 中的 4 个值，每个值的概率为 $1/4$。设 $Y$ 是一个二元标签，由异或函数确定性地定义，$Y = \\mathrm{XOR}(X_1,X_2)$，因此 $Y \\in \\{0,1\\}$ 且 $P(Y=0)=P(Y=1)=1/2$。一个编码器将 $X$ 映射到一个潜层表示 $T$，该表示最多取 $k$ 个不同的分类状态，其中 $k$ 被称为层宽度。我们将注意力限制在确定性编码器 $f: \\mathcal{X} \\to \\{1,2,\\dots,k\\}$上，它通过 $T=f(X)$ 满足马尔可夫链 $Y \\leftrightarrow X \\leftrightarrow T$。\n\n从离散随机变量的 Shannon 熵和互信息的核心定义开始：对于任何具有分布 $p(z)$ 的离散随机变量 $Z$，其熵为 $H(Z) = -\\sum_{z} p(z)\\log_2 p(z)$；对于任何具有联合分布 $p(u,v)$ 的对 $(U,V)$，其互信息为 $I(U;V) = \\sum_{u,v} p(u,v)\\log_2\\frac{p(u,v)}{p(u)p(v)}$。对于一个确定性编码器 $T=f(X)$，该编码器满足 $H(T\\mid X)=0$，因此 $I(T;X)=H(T)$。信息瓶颈（IB）设计问题旨在选择一个编码器，以达到目标预测信息 $I(T;Y)$，同时将压缩率 $I(T;X)$ 保持在速率预算之下。形式上，您必须选择最小的宽度 $k$，使得存在一个确定性编码器 $f$，满足 $I(T;Y) \\ge \\tau$ 和 $I(T;X) \\le R$，其中 $\\tau$ 是目标预测信息，$R$ 是速率预算。\n\n您的任务是编写一个完整的程序，该程序：\n1. 对于每个宽度 $k \\in \\{1,2,3,4\\}$，枚举所有确定性编码器 $f$。\n2. 对于每个编码器和宽度，使用上述定义，根据由 $X$、$Y$ 和 $T=f(X)$ 导出的离散分布，精确计算 $I(T;X)$ 和 $I(T;Y)$。\n3. 对于给定的对 $(R,\\tau)$，找到存在至少一个编码器满足 $I(T;Y) \\ge \\tau$ 和 $I(T;X) \\le R$ 的最小宽度 $k$。在该最小宽度的编码器中，使用任何在 $I(T;X) \\le R$ 约束下最大化 $I(T;Y)$ 的编码器来定义一个分类器 $g$，该分类器通过贝叶斯法则 $g(t)=\\arg\\max_{y} p(y\\mid t)$ 从 $T$ 预测 $Y$。报告最终的贝叶斯分类准确率 $\\sum_{t} p(t)\\max_{y} p(y\\mid t)$。\n4. 如果在 $k \\in \\{1,2,3,4\\}$ 中没有编码器满足约束条件，则报告特殊宽度 $-1$ 以及在 $I(T;X) \\le R$ 约束下最大化 $I(T;Y)$ 的编码器（在所有 $k \\in \\{1,2,3,4\\}$ 中）的贝叶斯准确率；如果多个编码器达到相同的最大 $I(T;Y)$，则选择任何具有最高贝叶斯准确率的编码器。\n\n根据熵和互信息的基本定义以及马尔可夫性质 $Y \\leftrightarrow X \\leftrightarrow T$ 来设计您的解决方案。不要使用任何近似或连续松弛；为确定性编码器导出的离散分布精确计算所有概率和信息量。\n\n测试套件。您的程序必须在以下三个测试用例上运行，每个用例由 $(R,\\tau)$（以比特为单位）指定：\n- 情况 A（正常路径）：$(R,\\tau) = (1.0, 1.0)$。\n- 情况 B（紧凑预算下的不可行目标）：$(R,\\tau) = (0.75, 0.1)$。\n- 情况 C（接近 $3$-$1$ 划分的边界熵预算）：$(R,\\tau) = (0.811, 0.3)$。\n\n最终输出格式。您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。对于每个情况，按顺序输出两个值：所选的宽度 $k$（如果不满足约束则为 $-1$）和相应的贝叶斯准确率。贝叶斯准确率必须四舍五入到恰好三位小数。因此，最终输出必须是包含六个条目的单个列表，按情况 A、B、C 的顺序排列：\n$[k_A,\\mathrm{acc}_A,k_B,\\mathrm{acc}_B,k_C,\\mathrm{acc}_C]$。",
            "solution": "用户提供了一个有效的问题。\n\n该问题要求我们根据信息瓶颈（IB）的原理，为一个简单的分类任务找到最优的确定性编码器。该优化涉及在压缩输入 $X$ 和保留关于标签 $Y$ 的信息之间进行权衡。\n\n### 步骤 1：形式化问题设置\n\n设输入变量为 $X$，其取值于字母表 $\\mathcal{X} = \\{\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{x}_4\\}$，其中我们定义 $\\mathbf{x}_1=(0,0)$, $\\mathbf{x}_2=(0,1)$, $\\mathbf{x}_3=(1,0)$, 和 $\\mathbf{x}_4=(1,1)$。问题陈述输入分布是均匀的，因此对于 $i \\in \\{1,2,3,4\\}$，$P(X=\\mathbf{x}_i) = 1/4$。\n\n标签 $Y$ 是 $X$ 的一个确定性函数，由异或运算给出：$Y = \\mathrm{XOR}(X_1, X_2)$。$Y$ 的字母表为 $\\mathcal{Y} = \\{0,1\\}$。关系如下：\n- $Y(\\mathbf{x}_1=(0,0)) = 0$\n- $Y(\\mathbf{x}_2=(0,1)) = 1$\n- $Y(\\mathbf{x}_3=(1,0)) = 1$\n- $Y(\\mathbf{x}_4=(1,1)) = 0$\n\n这导出了一个联合概率分布 $P(X,Y)$。例如，$P(X=\\mathbf{x}_1, Y=0) = P(X=\\mathbf{x}_1) = 1/4$。由此，我们可以验证 $Y$ 的边际概率：\n$P(Y=0) = P(X=\\mathbf{x}_1) + P(X=\\mathbf{x}_4) = 1/4 + 1/4 = 1/2$。\n$P(Y=1) = P(X=\\mathbf{x}_2) + P(X=\\mathbf{x}_3) = 1/4 + 1/4 = 1/2$。\n标签的熵为 $H(Y) = -\\sum_{y \\in \\mathcal{Y}} P(Y=y)\\log_2 P(Y=y) = -2 \\cdot (1/2)\\log_2(1/2) = 1$ 比特。\n\n### 步骤 2：表征确定性编码器\n\n一个确定性编码器 $f: \\mathcal{X} \\to \\{1, 2, \\dots, k\\}$ 将 4 个输入点中的每一个映射到 $k$ 个潜层状态之一。这等价于将输入集 $\\mathcal{X}$ 划分为 $k$ 个非空的不相交子集 $\\mathcal{C} = \\{C_1, C_2, \\dots, C_k\\}$，其中每个子集 $C_j$ 包含所有被映射到潜层状态 $t_j$ 的输入。因此，潜变量 $T$ 遵循马尔可夫链 $Y \\leftrightarrow X \\leftrightarrow T$。\n\n将一个包含 4 个元素的集合划分为 $k$ 个非空子集的唯一划分数量由第二类斯特林数 $S(4,k)$ 给出：\n- $k=1$: $S(4,1) = 1$ 个划分。\n- $k=2$: $S(4,2) = 7$ 个划分。\n- $k=3$: $S(4,3) = 6$ 个划分。\n- $k=4$: $S(4,4) = 1$ 个划分。\n总共有 $1+7+6+1=15$ 个唯一的编码器需要评估。\n\n### 步骤 3：计算信息论和准确率指标\n\n对于每个由划分 $\\mathcal{C} = \\{C_1, \\dots, C_k\\}$ 定义的编码器，我们必须计算两个互信息量和贝叶斯准确率。\n\n1.  **复杂度/压缩率 ($I(T;X)$)**：由于编码器是确定性的（$T=f(X)$），我们有 $H(T|X)=0$。互信息 $I(T;X) = H(T) - H(T|X)$ 简化为潜层表示的熵 $H(T)$。一个潜层状态 $t_j$ 的概率是 $P(T=t_j) = P(X \\in C_j) = \\sum_{\\mathbf{x} \\in C_j} P(X=\\mathbf{x}) = |C_j|/4$。\n    $$I(T;X) = H(T) = -\\sum_{j=1}^{k} P(T=t_j)\\log_2 P(T=t_j) = -\\sum_{j=1}^{k} \\frac{|C_j|}{4}\\log_2 \\frac{|C_j|}{4}$$\n\n2.  **预测信息 ($I(T;Y)$)**：由 $I(T;Y) = H(Y) - H(Y|T)$ 给出。我们已经知道 $H(Y)=1$。条件熵为 $H(Y|T) = \\sum_{j=1}^{k} P(T=t_j) H(Y|T=t_j)$。为了计算 $H(Y|T=t_j) = -\\sum_{y \\in \\mathcal{Y}} P(Y=y|T=t_j)\\log_2 P(Y=y|T=t_j)$，我们需要条件概率 $P(Y=y|T=t_j) = P(Y=y, X \\in C_j) / P(X \\in C_j)$。\n    令 $N_{j,y} = |\\{\\mathbf{x} \\in C_j \\mid Y(\\mathbf{x})=y\\}|$ 为聚类 $C_j$ 中标签为 $y$ 的输入数量。那么 $P(Y=y, X \\in C_j) = N_{j,y}/4$ 且 $P(X \\in C_j) = |C_j|/4$。\n    $$P(Y=y|T=t_j) = \\frac{N_{j,y}}{|C_j|}$$\n\n3.  **贝叶斯准确率**：给定 $T$ 的最优分类器预测最可能的标签：$g(t_j) = \\arg\\max_{y \\in \\mathcal{Y}} P(Y=y|T=t_j)$。此分类器的准确率是正确预测的概率，在所有潜层状态上取平均：\n    $$\\text{Accuracy} = \\sum_{j=1}^{k} P(T=t_j) \\max_{y \\in \\mathcal{Y}} P(Y=y|T=t_j) = \\sum_{j=1}^{k} \\frac{|C_j|}{4} \\frac{\\max_{y} N_{j,y}}{|C_j|} = \\frac{1}{4} \\sum_{j=1}^{k} \\max(N_{j,0}, N_{j,1})$$\n\n### 步骤 4：枚举所有编码器\n\n我们枚举所有 15 个唯一的划分，并计算它们对应的指标。令 $S_0 = \\{\\mathbf{x}_1, \\mathbf{x}_4\\}$（标签为 0 的输入）和 $S_1 = \\{\\mathbf{x}_2, \\mathbf{x}_3\\}$（标签为 1 的输入）。\n\n| $k$ | 划分类型 | 划分示例 | $I(T;X)$ (比特) | $I(T;Y)$ (比特) | 准确率 |\n|:---:|:--------------|:-------------------|:----------------|:----------------|:---------|\n| 1 | `(4)` | $\\{\\mathcal{X}\\}$ | $0.0$ | $0.0$ | $0.5$ |\n| 2 | `(3,1)` | $\\{\\mathbf{x}_1,\\mathbf{x}_2,\\mathbf{x}_3\\}, \\{\\mathbf{x}_4\\}$ | $H_b(1/4) \\approx 0.811$ | $1 - \\frac{3}{4} H_b(1/3) \\approx 0.311$ | $0.75$ |\n| 2 | `(2,2)` (Y-混合) | $\\{\\mathbf{x}_1,\\mathbf{x}_2\\}, \\{\\mathbf{x}_3,\\mathbf{x}_4\\}$ | $1.0$ | $0.0$ | $0.5$ |\n| 2 | `(2,2)` (Y-纯粹) | $\\{S_0, S_1\\}$ | $1.0$ | $1.0$ | $1.0$ |\n| 3 | `(2,1,1)` (Y-混合) | $\\{\\mathbf{x}_1,\\mathbf{x}_2\\}, \\{\\mathbf{x}_3\\}, \\{\\mathbf{x}_4\\}$ | $1.5$ | $0.5$ | $0.75$ |\n| 3 | `(2,1,1)` (Y-纯粹) | $\\{S_0, \\{\\mathbf{x}_2\\}, \\{\\mathbf{x}_3\\}\\}$ | $1.5$ | $1.0$ | $1.0$ |\n| 4 | `(1,1,1,1)` | $\\{\\{\\mathbf{x}_1\\}, \\{\\mathbf{x}_2\\}, \\dots\\}$ | $2.0$ | $1.0$ | $1.0$ |\n*注意：$H_b(p) = -p\\log_2 p - (1-p)\\log_2(1-p)$ 是二元熵函数。该表列出了代表性划分类型的指标；每种类型都有多个划分，但它们会产生相同的指标。*\n\n### 步骤 5：应用决策逻辑\n\n对于每个测试用例 $(R, \\tau)$，我们遵循规定的逻辑：\n1.  找到最小的宽度 $k \\in \\{1,2,3,4\\}$，使得存在一个编码器满足 $I(T;Y) \\ge \\tau$ 和 $I(T;X) \\le R$。\n2.  如果找到这样的 $k$，则选择该宽度的编码器中最大化 $I(T;Y)$（在 $I(T;X) \\le R$ 约束下）的一个，并报告其准确率。\n3.  如果不存在这样的编码器，则报告 $k=-1$。对于准确率，考虑所有满足速率预算 $I(T;X) \\le R$ 的编码器（来自所有 $k$），选择一个最大化 $I(T;Y)$ 的（若有平局，则以最高准确率作为决胜标准），并报告其准确率。\n\n**情况 A: $(R, \\tau) = (1.0, 1.0)$**\n- 我们寻找一个满足 $I(T;X) \\le 1.0$ 和 $I(T;Y) \\ge 1.0$ 的编码器。\n- 对于 $k=1$，没有编码器满足条件。\n- 对于 $k=2$，Y-纯粹划分 $\\{S_0, S_1\\}$ 的 $I(T;X)=1.0$ 和 $I(T;Y)=1.0$。这完美地满足了约束。\n- 最小宽度是 $k=2$。相应的准确率为 $1.0$。\n- 结果: $k_A=2$, $\\mathrm{acc}_A=1.000$。\n\n**情况 B: $(R, \\tau) = (0.75, 0.1)$**\n- 我们寻找一个满足 $I(T;X) \\le 0.75$ 和 $I(T;Y) \\ge 0.1$ 的编码器。\n- 查看表格，对于 $k=2$，最小的非零 $I(T;X)$ 是 $\\approx 0.811$。所有 $k \\ge 2$ 的编码器都具有 $I(T;X) > 0.75$。\n- 唯一满足 $I(T;X) \\le 0.75$ 的是 $k=1$ 的平凡编码器，其 $I(T;X)=0$。然而，它的 $I(T;Y)=0$，不满足 $I(T;Y) \\ge 0.1$ 的条件。\n- 没有编码器满足约束条件。我们必须报告 $k_B=-1$。\n- 对于准确率，我们找到满足 $I(T;X) \\le 0.75$ 的编码器。只有 $k=1$ 的编码器符合条件。它的 $I(T;Y)=0$ 和准确率为 $0.5$。\n- 结果: $k_B=-1$, $\\mathrm{acc}_B=0.500$。\n\n**情况 C: $(R, \\tau) = (0.811, 0.3)$**\n- 我们寻找一个满足 $I(T;X) \\le 0.811$ 和 $I(T;Y) \\ge 0.3$ 的编码器。\n- $k=2$ 的 `(3,1)` 型划分具有 $I(T;X) = H_b(1/4) \\approx 0.811278$，这严格大于预算 $R=0.811$。\n- 所有其他 $k \\ge 2$ 的编码器都有更高的 $I(T;X)$。\n- 唯一满足 $I(T;X) \\le 0.811$ 的是 $k=1$ 的编码器。它的 $I(T;Y)=0$，小于 $\\tau=0.3$。\n- 没有编码器满足约束条件。我们报告 $k_C=-1$。\n- 对于准确率，我们考虑满足 $I(T;X) \\le 0.811$ 的编码器。同样，只有 $k=1$ 的编码器符合条件。其准确率为 $0.5$。\n- 结果: $k_C=-1$, $\\mathrm{acc}_C=0.500$。",
            "answer": "```python\nimport numpy as np\nfrom itertools import product\nfrom collections import defaultdict\nimport math\n\ndef get_partitions(n, k):\n    \"\"\"\n    Generates all unique partitions of a set of n items into k non-empty subsets.\n    \"\"\"\n    if k == 0 and n == 0:\n        yield []\n        return\n    if k == 0 or k > n:\n        return\n    \n    if k == 1:\n        yield [list(range(n))]\n        return\n    \n    # Bell number B_4 = 15, small enough to be hardcoded for robustness.\n    # The set of items is {0, 1, 2, 3}.\n    items = list(range(n))\n    \n    # This recursive implementation correctly generates all partitions.\n    # It partitions items[1:] into k-1 sets and adds {items[0]} as a new set,\n    # or partitions items[1:] into k sets and adds items[0] to one of them.\n    if n > 0:\n        first, rest = items[0], items[1:]\n        # Case 1: {first} is a partition element on its own.\n        for p in get_partitions(n - 1, k - 1):\n            yield [[first]] + p\n        # Case 2: first is added to an existing partition element.\n        for p in get_partitions(n - 1, k):\n            for i in range(len(p)):\n                new_p = [list(part) for part in p] # deep copy\n                new_p[i].append(first)\n                # Sort to ensure canonical order and avoid duplicates\n                yield sorted(new_p, key=lambda x: x[0])\n\n\ndef solve():\n    \"\"\"\n    Main function to solve the Information Bottleneck problem.\n    \"\"\"\n    test_cases = [\n        (1.0, 1.0),    # Case A\n        (0.75, 0.1),   # Case B\n        (0.811, 0.3),  # Case C\n    ]\n    \n    # Problem setup\n    X_values = [(0, 0), (0, 1), (1, 0), (1, 1)]\n    # Map from X index (0,1,2,3) to Y value\n    Y_values = [v[0] ^ v[1] for v in X_values]\n    num_inputs = len(X_values)\n    H_Y = 1.0  # Since P(Y=0) = P(Y=1) = 0.5\n    \n    all_encoders_metrics = []\n\n    # Step 1: Enumerate all encoders and pre-calculate metrics\n    for k in range(1, num_inputs + 1):\n        # Generate unique partitions for width k\n        partitions = set()\n        for p_list in get_partitions(num_inputs,k):\n            # Canonical representation to handle duplicates\n            canonical_p = tuple(sorted(tuple(sorted(part)) for part in p_list))\n            partitions.add(canonical_p)\n\n        for partition in partitions:\n            # Step 2: Calculate metrics for each encoder\n            cluster_probs = [len(c) / num_inputs for c in partition]\n            I_TX = -sum(p * np.log2(p) for p in cluster_probs if p > 0)\n\n            H_Y_given_T = 0.0\n            total_max_y_counts = 0\n            \n            for cluster_indices in partition:\n                cluster_size = len(cluster_indices)\n                if cluster_size == 0:\n                    continue\n\n                cluster_prob = cluster_size / num_inputs\n                \n                y_counts = defaultdict(int)\n                for item_idx in cluster_indices:\n                    y_val = Y_values[item_idx]\n                    y_counts[y_val] += 1\n\n                total_max_y_counts += max(y_counts.values()) if y_counts else 0\n\n                h_y_t = 0.0\n                for count in y_counts.values():\n                    p_y_t = count / cluster_size\n                    if p_y_t > 0:\n                        h_y_t -= p_y_t * np.log2(p_y_t)\n                \n                H_Y_given_T += cluster_prob * h_y_t\n\n            I_TY = H_Y - H_Y_given_T\n            accuracy = total_max_y_counts / num_inputs\n            \n            # Using a small epsilon for floating point comparisons around 0.\n            if abs(I_TX)  1e-9: I_TX = 0.0\n            if abs(I_TY)  1e-9: I_TY = 0.0\n\n            all_encoders_metrics.append({'k': k, 'I_TX': I_TX, 'I_TY': I_TY, 'acc': accuracy})\n\n    final_results = []\n    # Step 3: Apply decision logic for each test case\n    for R, tau in test_cases:\n        \n        # Part 1: Find smallest valid k\n        min_valid_k = -1\n        best_acc_for_k = -1.0\n        \n        valid_encoders = []\n        for enc in all_encoders_metrics:\n            if enc['I_TY'] >= tau and enc['I_TX'] = R:\n                 valid_encoders.append(enc)\n        \n        if valid_encoders:\n            min_valid_k = min(enc['k'] for enc in valid_encoders)\n            \n            # Find best encoder at that minimal width k\n            encoders_at_min_k = [enc for enc in valid_encoders if enc['k'] == min_valid_k]\n            \n            # Sort by I(T;Y) descending, then accuracy descending (if needed)\n            encoders_at_min_k.sort(key=lambda x: x['I_TY'], reverse=True)\n            best_acc_for_k = encoders_at_min_k[0]['acc']\n\n            final_results.extend([min_valid_k, f\"{best_acc_for_k:.3f}\"])\n\n        else:\n            # Part 2: No valid encoder found, apply fallback rule\n            min_valid_k = -1\n            \n            eligible_encoders = []\n            for enc in all_encoders_metrics:\n                if enc['I_TX'] = R:\n                    eligible_encoders.append(enc)\n            \n            if eligible_encoders:\n                # Sort by I(T;Y) descending, then accuracy descending\n                eligible_encoders.sort(key=lambda x: (x['I_TY'], x['acc']), reverse=True)\n                best_acc_fallback = eligible_encoders[0]['acc']\n            else:\n                # This case shouldn't be reached with the given test cases,\n                # as the k=1 encoder always has I(T;X)=0.\n                best_acc_fallback = 0.0\n                \n            final_results.extend([min_valid_k, f\"{best_acc_fallback:.3f}\"])\n\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在理解了表示学习的目标是保留“相关”信息之后，我们自然会进一步探究信息的本质，尤其是在数据增强这一普遍实践中。并非所有增加输入熵 $H(X)$ 的操作都能提升模型性能。这项实践通过一系列精心设计的合成实验，清晰地揭示了任务相关信息与纯粹随机噪声之间的区别 ()。你将通过计算和比较，亲眼见证某些数据增强方法虽然增加了数据的“复杂性”，但并未增加甚至反而损害了其与任务标签之间的互信息 $I(X;Y)$。这有助于你深刻理解有效数据增强的真正目标：增加任务相关的变化，而非盲目地注入随机性。",
            "id": "3138094",
            "problem": "您的任务是使用合成的离散实验来证明，增加输入熵 $H(X)$ 的数据增强不一定会增加输入与标签之间的互信息 $I(X;Y)$。您的任务是构建一个完整的程序，该程序从精确定义的离散分布中生成数据，应用指定的增强，并计算香农熵和互信息的经验估计值。目标是利用信息论的基本定义，将随机性与任务相关的信息分离开来。\n\n使用以下基础定义：\n- 对于具有经验概率 $p(x)$ 的离散随机变量 $X$，香农熵 $H(X)$ 定义为 $H(X) = -\\sum_{x} p(x) \\log_2 p(x)$。\n- 对于具有经验联合概率 $p(x,y)$ 和边缘概率 $p(x)$、$p(y)$ 的离散随机变量 $X$ 和 $Y$，互信息 (MI) $I(X;Y)$ 定义为 $I(X;Y) = \\sum_{x,y} p(x,y)\\log_2 \\frac{p(x,y)}{p(x)p(y)}$。\n\n本问题中的所有变量都是离散的，且取有限个值。您将使用大小为 $n$ 的蒙特卡洛样本的经验频率来近似计算 $H(\\cdot)$ 和 $I(\\cdot\\,;\\cdot)$。\n\n实验设置：\n- 令 $Y \\in \\{0,1\\}$ 为一个二元标签，其中 $Y \\sim \\text{Bernoulli}(q)$，$q = \\mathbb{P}(Y=1)$。\n- 令 $Z = Y$ 表示一个与任务相关的二元特征。\n- 数据增强引入了额外的随机性，这种随机性要么与 $Y$ 无关，要么是应用于观测特征的标签损坏噪声。\n- 对于复合输入，通过将元组 $(x_1, x_2)$ 编码为整数类别，将 $X$ 视为单个离散变量，从而通过上述定义计算离散的 $H(X)$ 和 $I(X;Y)$。\n\n您的程序必须实现从样本中估计 $H(X)$ 和 $I(X;Y)$ 的经验估计器，并评估布尔谓词\n$$b = \\left[H\\!\\left(X_{\\text{aug}}\\right)  H\\!\\left(X_{\\text{base}}\\right)\\right] \\wedge \\left[I\\!\\left(X_{\\text{aug}};Y\\right) \\le I\\!\\left(X_{\\text{base}};Y\\right)\\right],$$\n同时使用一个小的数值容差来考虑采样误差。$b$ 的真值表明，一次增强在增加 $H(X)$ 的同时，并没有增加与任务相关的信息 $I(X;Y)$。\n\n测试套件（每个案例都应使用指定的参数和蒙特卡洛样本大小 $n$ 来运行）：\n\n- 案例 $1$（理想路径，添加独立随机性）：$q = 0.5$，$n = 200000$。基础输入 $X_{\\text{base}} = Z$。增强输入 $X_{\\text{aug}} = (Z, N)$，其中 $N \\sim \\text{Bernoulli}(0.5)$ 且与 $(Z,Y)$ 无关。\n- 案例 $2$（类别不平衡，添加独立随机性）：$q = 0.1$，$n = 200000$。基础输入 $X_{\\text{base}} = Z$。增强输入 $X_{\\text{aug}} = (Z, N)$，其中 $N \\sim \\text{Bernoulli}(0.5)$ 是独立的。\n- 案例 $3$（标签无关的损坏噪声增加了 $H(X)$ 但减少了 $I(X;Y)$）：$q = 0.1$，$s = 0.3$，$n = 200000$。基础输入 $X_{\\text{base}} = Z$。增强输入 $X_{\\text{aug}} = \\tilde{Z}$，其中 $\\tilde{Z} = Z \\oplus B$ 且 $B \\sim \\text{Bernoulli}(s)$ 与 $(Z,Y)$ 无关；这里 $\\oplus$ 表示按位异或。\n- 案例 $4$（添加任务相关信息提高了 $I(X;Y)$）：$q = 0.1$，$s = 0.3$，$n = 200000$。基础输入 $X_{\\text{base}} = \\tilde{Z}$，定义同案例 3。增强输入 $X_{\\text{aug}} = (\\tilde{Z}, Y)$。\n- 案例 $5$（边界条件，无损坏）：$q = 0.1$，$s = 0.0$，$n = 200000$。基础输入 $X_{\\text{base}} = Z$。增强输入 $X_{\\text{aug}} = \\tilde{Z}$，其中 $\\tilde{Z} = Z \\oplus B$ 且 $B \\sim \\text{Bernoulli}(s)$，即当 $s = 0.0$ 时没有变化。\n\n实现要求：\n- 使用一个可复现的伪随机数生成器。\n- 对每个案例，使用上述基本定义直接从样本计算经验 $H(X)$ 和 $I(X;Y)$。\n- 对每个案例，计算由谓词给出的布尔值 $b$，在比较 $H(\\cdot)$ 和 $I(\\cdot\\,;\\cdot)$ 时使用 $10^{-3}$ 比特的容差。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $[result_1,result_2,\\dots,result_5]$），其中每个 $result_i$ 是上面所列顺序中案例 $i$ 的布尔值 $b$。\n\n所有答案都是无量纲的，并应按规定报告为布尔值。本问题中不涉及物理单位或角度。",
            "solution": "目标是通过合成实验证明，数据增强可以增加输入的总熵 $H(X)$，而不增加（有时甚至减少）其与目标标签 $Y$ 共享的互信息 $I(X;Y)$。这种区分对于理解哪些增强提供了与任务相关的信息，而哪些仅仅增加了与任务无关的随机性至关重要。\n\n该方法涉及一系列蒙特卡洛模拟。对于每个案例，我们从指定的离散概率分布中为基础输入 $X_{\\text{base}}$、增强输入 $X_{\\text{aug}}$ 以及二元标签 $Y$ 生成大量样本 $n$。我们根据这些样本计算香农熵和互信息的经验估计值，以评估谓词 $b = \\left[H\\!\\left(X_{\\text{aug}}\\right)  H\\!\\left(X_{\\text{base}}\\right)\\right] \\wedge \\left[I\\!\\left(X_{\\text{aug}};Y\\right) \\le I\\!\\left(X_{\\text{base}};Y\\right)\\right]$。使用 $10^{-3}$ 的数值容差，以使比较对采样噪声具有鲁棒性。\n\n分析的核心在于对离散随机变量的两个信息论量进行经验估计。\n\n首先，变量 $X$（其取值于集合 $\\mathcal{X}$）的香农熵是根据其经验概率质量函数 $p(x)$ 估计的。给定 $n$ 个样本，$p(x)$ 是值 $x$ 出现的频率。以比特为单位的熵为：\n$$H(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x)$$\n其中如果 $p(x)=0$，则项 $p(x) \\log_2 p(x)$ 取为 $0$。\n\n其次，两个变量 $X$ 和 $Y$ 之间的互信息是衡量它们统计依赖性的度量。为了数值稳定性和易于实现，我们使用恒等式：\n$$I(X;Y) = H(X) + H(Y) - H(X,Y)$$\n其中 $H(X)$ 和 $H(Y)$ 是边缘熵，$H(X,Y)$ 是对 $(X,Y)$ 的联合熵。联合熵由经验联合概率分布 $p(x,y)$ 计算得出：\n$$H(X,Y) = -\\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} p(x,y) \\log_2 p(x,y)$$\n当输入 $X$ 是多个特征（如 $X=(F_1, F_2)$）的复合体时，通过将其特征值的元组编码为唯一的整数标识符，可以将其视为单个离散变量。这允许直接应用上述针对离散变量的公式。\n\n分析通过评估五个不同案例的谓词 $b$ 来进行。\n\n**案例 1：独立随机性（类别均衡）**\n- 参数：$q = \\mathbb{P}(Y=1) = 0.5$，$n = 200000$。\n- 基础输入：$X_{\\text{base}} = Z$，其中 $Z=Y$。这里，输入是标签的完美副本。\n- 增强输入：$X_{\\text{aug}} = (Z, N)$，其中 $N \\sim \\text{Bernoulli}(0.5)$ 是一个与 $Y$ 无关的噪声变量。\n- 分析：$I(X_{\\text{base}};Y) = I(Y;Y) = H(Y) = 1$ 比特。添加独立的噪声变量 $N$ 会增加总熵，因为根据独立性，$H(X_{\\text{aug}}) = H(Z,N) = H(Z) + H(N) = H(Y) + H(N) = 1 + 1 = 2$ 比特。互信息保持不变，因为 $N$ 与 $Y$ 无关：$I(X_{\\text{aug}};Y) = I((Z,N);Y) = I(Z;Y) = 1$ 比特。因此，我们预期 $H(X_{\\text{aug}})  H(X_{\\text{base}})$ 且 $I(X_{\\text{aug}};Y) \\le I(X_{\\text{base}};Y)$。谓词 $b$ 的评估结果应为真。\n\n**案例 2：独立随机性（类别不平衡）**\n- 参数：$q = \\mathbb{P}(Y=1) = 0.1$，$n = 200000$。\n- 设置：与案例 1 相同，但标签分布不平衡。\n- 分析：推理过程与案例 1 相同。标签的熵为 $H(Y) = -0.1\\log_2(0.1) - 0.9\\log_2(0.9) \\approx 0.469$ 比特。我们有 $H(X_{\\text{base}}) = H(Y) \\approx 0.469$ 比特和 $I(X_{\\text{base}};Y) = H(Y) \\approx 0.469$ 比特。增强后的熵为 $H(X_{\\text{aug}}) = H(Y) + H(N) \\approx 0.469 + 1 = 1.469$ 比特。互信息保持为 $I(X_{\\text{aug}};Y) = I(Y;Y) = H(Y) \\approx 0.469$ 比特。同样，我们预期 $H(X_{\\text{aug}})  H(X_{\\text{base}})$ 且 $I(X_{\\text{aug}};Y) \\le I(X_{\\text{base}};Y)$。谓词 $b$ 应为真。\n\n**案例 3：标签损坏噪声**\n- 参数：$q = 0.1$，噪声水平 $s = 0.3$，$n = 200000$。\n- 基础输入：$X_{\\text{base}} = Z = Y$。\n- 增强输入：$X_{\\text{aug}} = \\tilde{Z} = Z \\oplus B$，其中 $B \\sim \\text{Bernoulli}(s)$ 是独立的比特翻转噪声，$\\oplus$ 是异或操作。这模拟了一个二元对称信道。\n- 分析：$H(X_{\\text{base}}) = H(Y) \\approx 0.469$ 比特。增强输入 $\\tilde{Z}$ 是 $Y$ 的一个带噪版本。概率 $\\mathbb{P}(\\tilde{Z}=1) = q(1-s) + (1-q)s = 0.1(0.7) + 0.9(0.3) = 0.34$。其熵为 $H(X_{\\text{aug}}) = H_b(0.34) \\approx 0.923$ 比特，大于 $H(X_{\\text{base}})$。然而，噪声损坏了关于 $Y$ 的信息。根据数据处理不等式，$I(Y;\\tilde{Z}) \\le I(Y;Z)$。具体而言，$I(X_{\\text{aug}};Y) = H(\\tilde{Z}) - H(\\tilde{Z}|Y) = H_b(0.34) - H_b(s) \\approx 0.923-0.881=0.042$ 比特。这远小于 $I(X_{\\text{base}};Y) = H(Y) \\approx 0.469$ 比特。谓词的两个条件都得到满足，所以 $b$ 应为真。\n\n**案例 4：添加任务相关信息**\n- 参数：$q=0.1, s=0.3, n=200000$。\n- 基础输入：$X_{\\text{base}} = \\tilde{Z}$，即案例 3 中的带噪变量。\n- 增强输入：$X_{\\text{aug}} = (\\tilde{Z}, Y)$。该增强明确地添加了真实标签。\n- 分析：基础量由案例 3 推导得出：$H(X_{\\text{base}}) \\approx 0.923$ 比特和 $I(X_{\\text{base}};Y) \\approx 0.042$ 比特。增强输入的熵为 $H(X_{\\text{aug}}) = H(\\tilde{Z}, Y) = H(Y) + H(\\tilde{Z}|Y) = H_b(0.1) + H_b(0.3) \\approx 0.469 + 0.881 = 1.35$ 比特，大于 $H(X_{\\text{base}})$。然而，现在的互信息是 $I(X_{\\text{aug}};Y) = I((\\tilde{Z},Y);Y) = H(Y) - H(Y|(\\tilde{Z},Y))$。因为在给定元组 $(\\tilde{Z},Y)$ 的情况下，$Y$ 是完全已知的，所以条件熵 $H(Y|(\\tilde{Z},Y))$ 为 $0$。因此，$I(X_{\\text{aug}};Y) = H(Y) \\approx 0.469$ 比特。这远大于 $I(X_{\\text{base}};Y) \\approx 0.042$ 比特。条件 $I(X_{\\text{aug}};Y) \\le I(X_{\\text{base}};Y)$ 为假。谓词 $b$ 应为假。\n\n**案例 5：边界条件（无损坏）**\n- 参数：$q=0.1, s=0.0, n=200000$。\n- 设置：与案例 3 相同，但噪声概率 $s=0$。\n- 分析：当 $s=0$ 时，噪声变量 $B$ 始终为 $0$。因此，$X_{\\text{aug}} = Z \\oplus 0 = Z = X_{\\text{base}}$。基础输入和增强输入是相同的。因此，$H(X_{\\text{aug}}) = H(X_{\\text{base}})$ 且 $I(X_{\\text{aug}};Y) = I(X_{\\text{base}};Y)$。谓词的第一部分 $H(X_{\\text{aug}})  H(X_{\\text{base}})$ 是一个严格不等式，（在数值容差范围内）其结果将为假。因此，整个谓词 $b$ 必定为假。\n\n实现部分将为每个案例生成样本，计算信息论量，并评估谓词 $b$ 以证实这些理论预期。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _calculate_entropy(samples):\n    \"\"\"\n    Computes the Shannon entropy of a discrete random variable from its samples.\n    \"\"\"\n    _, counts = np.unique(samples, return_counts=True)\n    probabilities = counts / len(samples)\n    # The contribution of p*log(p) is 0 for p=0.\n    # We filter to avoid log(0) errors.\n    probabilities = probabilities[probabilities > 0]\n    entropy = -np.sum(probabilities * np.log2(probabilities))\n    return entropy\n\ndef compute_info_metrics(x_samples, y_samples):\n    \"\"\"\n    Computes H(X) and I(X;Y) from samples.\n    I(X;Y) is calculated as H(X) + H(Y) - H(X,Y).\n    \"\"\"\n    # H(X)\n    h_x = _calculate_entropy(x_samples)\n    \n    # H(Y)\n    h_y = _calculate_entropy(y_samples)\n    \n    # H(X, Y) - Joint Entropy\n    # We get joint probabilities by counting unique pairs (rows).\n    joint_samples = np.c_[x_samples, y_samples]\n    _, joint_counts = np.unique(joint_samples, axis=0, return_counts=True)\n    joint_probabilities = joint_counts / len(x_samples)\n    joint_probabilities = joint_probabilities[joint_probabilities > 0]\n    h_xy = -np.sum(joint_probabilities * np.log2(joint_probabilities))\n    \n    # I(X;Y) = H(X) + H(Y) - H(X,Y)\n    i_xy = h_x + h_y - h_xy\n    \n    return h_x, i_xy\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute the boolean predicate for each case.\n    \"\"\"\n    test_cases = [\n        # (case_id, q, s, n)\n        (1, 0.5, None, 200000),\n        (2, 0.1, None, 200000),\n        (3, 0.1, 0.3, 200000),\n        (4, 0.1, 0.3, 200000),\n        (5, 0.1, 0.0, 200000),\n    ]\n\n    results = []\n    rng = np.random.default_rng(seed=123)  # Reproducible pseudo-random number generator\n    tol = 1e-3  # Numerical tolerance in bits\n\n    for case_id, q, s, n in test_cases:\n        # Generate base data\n        y = rng.binomial(1, q, size=n)\n        z = y\n\n        if case_id == 1:\n            # Case 1: happy path, independent randomness added\n            x_base = z\n            n_rv = rng.binomial(1, 0.5, size=n)\n            # Encode tuple (Z, N) into a single integer\n            x_aug = 2 * z + n_rv\n\n        elif case_id == 2:\n            # Case 2: class imbalance, independent randomness added\n            x_base = z\n            n_rv = rng.binomial(1, 0.5, size=n)\n            # Encode tuple (Z, N) into a single integer\n            x_aug = 2 * z + n_rv\n\n        elif case_id == 3:\n            # Case 3: label-independent corrupting noise\n            x_base = z\n            b_rv = rng.binomial(1, s, size=n)\n            z_tilde = z ^ b_rv  # bitwise XOR\n            x_aug = z_tilde\n        \n        elif case_id == 4:\n            # Case 4: adding task-relevant information\n            b_rv = rng.binomial(1, s, size=n)\n            z_tilde = z ^ b_rv\n            x_base = z_tilde\n            # Encode tuple (Z_tilde, Y) into a single integer\n            x_aug = 2 * z_tilde + y\n            \n        elif case_id == 5:\n            # Case 5: boundary condition, no corruption\n            x_base = z\n            b_rv = rng.binomial(1, s, size=n) # s=0.0 means B is all zeros\n            z_tilde = z ^ b_rv\n            x_aug = z_tilde\n        else:\n            continue\n\n        # Compute information-theoretic metrics for base and augmented inputs\n        h_base, i_base = compute_info_metrics(x_base, y)\n        h_aug, i_aug = compute_info_metrics(x_aug, y)\n\n        # Evaluate the boolean predicate b with tolerance\n        # b = [H(X_aug) > H(X_base)] AND [I(X_aug;Y) = I(X_base;Y)]\n        # H_aug > H_base + tol : Guards against noise making things unequal\n        # I_aug = I_base + tol : Guards against noise making things unequal\n        is_h_aug_greater = h_aug > h_base + tol\n        is_i_aug_le_i_base = i_aug = i_base + tol\n        \n        b = is_h_aug_greater and is_i_aug_le_i_base\n        results.append(b)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "信息论不仅为模型设计提供了理论指导，还为模型诊断提供了强大的分析工具。在生成模型这个深度学习的前沿领域，我们常常面临“模式崩溃”（mode collapse）或“覆盖不足”（poor coverage）等棘手的训练难题。这项实践将向你展示如何运用信息论的量化指标来“诊断”生成模型的健康状况 ()。你将通过计算潜变量与生成样本间的互信息 $I(Z;X)$、潜变量的熵 $H(Z)$ 以及克拉克-莱布勒（Kullback-Leibler, KL）散度，学会如何识别和量化这些常见的失败模式，从而更深刻地理解生成模型的内在工作机制。",
            "id": "3138062",
            "problem": "给定一个离散潜变量生成模型，其潜变量 $Z$ 在一个有限集中取值，可观测变量 $X$ 在一个有限集中取值。该模型定义了潜变量的先验 $p(Z)$ 和似然 $p(X \\mid Z)$，它们共同导出了模型联合分布 $p(Z, X)$ 和数据的模型边际分布 $p_{\\text{model}}(X)$。此外，还给定一个在 $X$ 的相同支撑集上的目标数据分布 $p_{\\text{data}}(X)$。你的任务是使用信息论中的基本离散定义来量化生成模型中常见的失效模式。具体来说，你将计算 $Z$ 和 $X$ 之间的互信息、$Z$ 的熵，以及 $p_{\\text{data}}(X)$ 和 $p_{\\text{model}}(X)$ 之间两个方向的 Kullback–Leibler 散度（双向的 Kullback–Leibler 散度 (KL)）。\n\n所有对数都必须是自然对数，因此信息量以奈特（nats）为单位表示。对于任何涉及零概率的求和，应用标准的离散约定，即任何形式为 $0 \\cdot \\log(\\cdot)$ 的项对总和的贡献为 $0$。你的程序必须使用基本定义，从给定的分布中精确计算这些量，除了最终报告值中的四舍五入外，不进行任何近似。\n\n对于下面的每个测试用例，按所列顺序计算并报告以下四个量：\n- 在模型联合分布下，$Z$ 和 $X$ 之间的互信息，记为 $I(Z; X)$。\n- 潜变量先验的熵，记为 $H(Z)$。\n- 从 $p_{\\text{data}}(X)$ 到 $p_{\\text{model}}(X)$ 的 Kullback–Leibler 散度，记为 $\\mathrm{KL}(p_{\\text{data}} \\| p_{\\text{model}})$。\n- 从 $p_{\\text{model}}(X)$ 到 $p_{\\text{data}}(X)$ 的 Kullback–Leibler 散度，记为 $\\mathrm{KL}(p_{\\text{model}} \\| p_{\\text{data}})$。\n\n重要的实现细节：\n- 在整个计算过程中，对上述各量使用标准的离散定义和自然对数。\n- 对于你输出的每个浮点数，四舍五入到 $6$ 位小数。\n- 你的程序必须生成单行输出，其中包含所有测试用例的结果，格式为逗号分隔的列表的列表，并用方括号括起来，例如 $[\\,[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4]\\,]$，其中每个符号代表一个四舍五入到 $6$ 位小数的浮点数。\n\n测试套件（所有概率都明确给出，并且在其各自的支撑集上总和为 $1$）：\n\n所有测试用例的共同支撑集：\n- 潜变量的支撑集为 $Z \\in \\{0, 1, 2\\}$。\n- 可观测变量的支撑集为 $X \\in \\{0, 1, 2\\}$。\n\n测试用例 1（平衡，接近理想的覆盖范围）：\n- 先验 $p(Z) = [\\,1/3,\\, 1/3,\\, 1/3\\,]$，即 $[\\,0.\\overline{3},\\, 0.\\overline{3},\\, 0.\\overline{3}\\,]$。\n- 似然 $p(X \\mid Z)$ 是确定性且一对一的：对于 $Z=0,1,2$，$3 \\times 3$ 矩阵的行分别为 $[\\,1,\\,0,\\,0\\,]$、$[\\,0,\\,1,\\,0\\,]$ 和 $[\\,0,\\,0,\\,1\\,]$。\n- 数据分布 $p_{\\text{data}}(X) = [\\,1/3,\\, 1/3,\\, 1/3\\,]$。\n\n测试用例 2（模式坍塌：所有潜变量的似然相同）：\n- 先验 $p(Z) = [\\,1/3,\\, 1/3,\\, 1/3\\,]$。\n- 似然的行是相同的：$p(X \\mid Z)$ 的每一行都等于 $[\\,0.7,\\, 0.2,\\, 0.1\\,]$。\n- 数据分布 $p_{\\text{data}}(X) = [\\,0.34,\\, 0.33,\\, 0.33\\,]$。\n\n测试用例 3（覆盖范围不匹配：重叠的似然和非均匀先验）：\n- 先验 $p(Z) = [\\,0.6,\\, 0.3,\\, 0.1\\,]$。\n- 似然矩阵，其行分别为：\n  - 当 $Z=0$ 时：$[\\,0.6,\\, 0.25,\\, 0.15\\,]$，\n  - 当 $Z=1$ 时：$[\\,0.3,\\, 0.4,\\, 0.3\\,]$，\n  - 当 $Z=2$ 时：$[\\,0.2,\\, 0.3,\\, 0.5\\,]$。\n- 数据分布 $p_{\\text{data}}(X) = [\\,0.5,\\, 0.3,\\, 0.2\\,]$。\n\n测试用例 4（边界条件：退化的潜变量先验）：\n- 先验 $p(Z) = [\\,1.0,\\, 0.0,\\, 0.0\\,]$。\n- 似然的行（注意由于先验，只有第一行有影响）：每一行都等于 $[\\,0.6,\\, 0.25,\\, 0.15\\,]$。\n- 数据分布 $p_{\\text{data}}(X) = [\\,0.6,\\, 0.25,\\, 0.15\\,]$。\n\n你的程序必须使用上述输入为每个测试用例计算所要求的四个量，并输出一个单行字符串，该字符串表示一个列表的列表，其中每个内部列表按顺序对应一个测试用例。将每个报告的浮点数四舍五入到 $6$ 位小数，并且除了指定的输出格式外，不要包含任何额外的文本。",
            "solution": "该问题要求为一个离散潜变量生成模型计算四个信息论量，这些量对于诊断其性能特征（如模式坍塌和覆盖范围）至关重要。该模型由潜变量的先验分布 $p(Z)$ 和给定潜变量时可观测变量的条件分布（似然）$p(X \\mid Z)$ 定义。它们结合起来形成一个联合分布 $p(Z, X) = p(X \\mid Z)p(Z)$。模型的有效性是相对于给定的目标数据分布 $p_{\\text{data}}(X)$ 来评估的。\n\n需要计算的四个量是：\n1.  模型联合分布下，潜变量 $Z$ 和可观测变量 $X$ 之间的互信息，记为 $I(Z; X)$。它衡量了在已知 $Z$ 的情况下 $X$ 不确定性的减少量，表明潜编码携带了多少关于生成输出的信息。\n2.  潜变量先验的熵，$H(Z)$。它量化了模型所使用的潜编码的多样性。\n3.  从数据分布到模型分布的 Kullback-Leibler (KL) 散度，$\\mathrm{KL}(p_{\\text{data}} \\| p_{\\text{model}})$。它衡量了模型对数据分布的近似程度，并严重惩罚模型未能在真实数据分布有支撑的区域生成样本（即，覆盖范围差）的情况。\n4.  从模型分布到数据分布的 KL 散度，$\\mathrm{KL}(p_{\\text{model}} \\| p_{\\text{data}})$。这种散度惩罚模型生成在真实数据分布下不太可能的样本（即，精度差或模式坍塌）。\n\n所有计算都必须遵循信息论的标准定义，使用自然对数，结果以奈特（nats）表示。\n\n**理论框架和计算策略**\n\n计算过程首先从给定的模型组件中推导出必要的分布，然后应用所需量的基本定义。\n\n**步骤 1：推导模型的边际分布 $p_{\\text{model}}(X)$**\n可观测变量 $X$ 上的模型边际分布是通过从联合分布 $p(Z, X)$ 中对潜变量 $Z$ 进行边际化得到的。\n令 $Z$ 取值于 $\\{z_i\\}_{i=1}^{N_Z}$，$X$ 取值于 $\\{x_j\\}_{j=1}^{N_X}$。\n在模型下观测到 $x_j$ 的概率由全概率定律给出：\n$$p_{\\text{model}}(X=x_j) = \\sum_{i=1}^{N_Z} p(Z=z_i, X=x_j) = \\sum_{i=1}^{N_Z} p(X=x_j \\mid Z=z_i) p(Z=z_i)$$\n作为计算 $I(Z; X)$ 和 KL 散度的先决条件，必须为每个测试用例执行此计算。\n\n**步骤 2：计算 $H(Z)$**\n离散潜变量先验 $p(Z)$ 的熵由标准公式给出：\n$$H(Z) = - \\sum_{i=1}^{N_Z} p(Z=z_i) \\log p(Z=z_i)$$\n应用了形式为 $0 \\cdot \\log(0)$ 的项对总和贡献为 $0$ 的约定。\n\n**步骤 3：计算 $I(Z; X)$**\n互信息 $I(Z; X)$ 可以用几个等价的公式计算。一个计算上方便的形式是 $I(Z; X) = H(X) - H(X \\mid Z)$。\n首先，我们计算模型边际分布的熵 $H(X)$：\n$$H(X) = - \\sum_{j=1}^{N_X} p_{\\text{model}}(X=x_j) \\log p_{\\text{model}}(X=x_j)$$\n接下来，我们计算条件熵 $H(X \\mid Z)$，它是似然分布 $p(X \\mid Z=z_i)$ 的熵的期望值，在先验 $p(Z)$ 上取平均：\n$$H(X \\mid Z) = \\sum_{i=1}^{N_Z} p(Z=z_i) H(X \\mid Z=z_i)$$\n其中每个条件分布的熵为：\n$$H(X \\mid Z=z_i) = - \\sum_{j=1}^{N_X} p(X=x_j \\mid Z=z_i) \\log p(X=x_j \\mid Z=z_i)$$\n那么互信息就是差值：\n$$I(Z; X) = H(X) - H(X \\mid Z)$$\n\n**步骤 4：计算 KL 散度**\n在相同支撑集上的两个离散概率分布 $P(Y)$ 和 $Q(Y)$ 之间的 KL 散度定义为：\n$$\\mathrm{KL}(P \\| Q) = \\sum_{y} P(y) \\log \\frac{P(y)}{Q(y)}$$\n所要求的量是该定义的实例：\n-   $\\mathrm{KL}(p_{\\text{data}} \\| p_{\\text{model}}) = \\sum_{j=1}^{N_X} p_{\\text{data}}(X=x_j) \\log \\frac{p_{\\text{data}}(X=x_j)}{p_{\\text{model}}(X=x_j)}$\n-   $\\mathrm{KL}(p_{\\text{model}} \\| p_{\\text{data}}) = \\sum_{j=1}^{N_X} p_{\\text{model}}(X=x_j) \\log \\frac{p_{\\text{model}}(X=x_j)}{p_{\\text{data}}(X=x_j)}$\n\n在分母中的概率为 $0$ 而对应的分子非零的情况下，KL 散度是无穷大的。但是，所提供的测试用例没有出现此问题。如果分子中的概率为 $0$，则该项为 $0$ 的约定也适用。\n\n这种对基本定义的系统应用为计算每个测试用例所需的量提供了一种完整而精确的方法。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef solve():\n    \"\"\"\n    Computes four information-theoretic quantities for a series of\n    latent-variable generative model test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"p_z\": np.array([1/3, 1/3, 1/3]),\n            \"p_x_cond_z\": np.array([[1.0, 0.0, 0.0],\n                                    [0.0, 1.0, 0.0],\n                                    [0.0, 0.0, 1.0]]),\n            \"p_data_x\": np.array([1/3, 1/3, 1/3])\n        },\n        {\n            \"p_z\": np.array([1/3, 1/3, 1/3]),\n            \"p_x_cond_z\": np.array([[0.7, 0.2, 0.1],\n                                    [0.7, 0.2, 0.1],\n                                    [0.7, 0.2, 0.1]]),\n            \"p_data_x\": np.array([0.34, 0.33, 0.33])\n        },\n        {\n            \"p_z\": np.array([0.6, 0.3, 0.1]),\n            \"p_x_cond_z\": np.array([[0.6, 0.25, 0.15],\n                                    [0.3, 0.4, 0.3],\n                                    [0.2, 0.3, 0.5]]),\n            \"p_data_x\": np.array([0.5, 0.3, 0.2])\n        },\n        {\n            \"p_z\": np.array([1.0, 0.0, 0.0]),\n            \"p_x_cond_z\": np.array([[0.6, 0.25, 0.15],\n                                    [0.6, 0.25, 0.15],\n                                    [0.6, 0.25, 0.15]]),\n            \"p_data_x\": np.array([0.6, 0.25, 0.15])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        p_z = case[\"p_z\"]\n        p_x_cond_z = case[\"p_x_cond_z\"]\n        p_data_x = case[\"p_data_x\"]\n\n        # Calculate H(Z), the entropy of the latent prior.\n        h_z = entropy(p_z, base=np.e)\n\n        # Calculate p_model(X), the model's marginal distribution.\n        # This is sum_z p(x|z)p(z), which is a matrix multiplication.\n        p_model_x = p_z @ p_x_cond_z\n\n        # Calculate I(Z; X) = H(X) - H(X|Z)\n        # H(X) is the entropy of the model's marginal.\n        h_x = entropy(p_model_x, base=np.e)\n        \n        # H(X|Z) is the expectation of the entropy of the likelihoods.\n        # E_p(z) [H(p(X|Z=z))]\n        h_x_cond_z_per_z = np.array([entropy(p, base=np.e) for p in p_x_cond_z])\n        h_x_cond_z = np.dot(p_z, h_x_cond_z_per_z)\n        \n        i_zx = h_x - h_x_cond_z\n\n        # Calculate KL(p_data || p_model)\n        # scipy.stats.entropy calculates KL divergence when qk is provided.\n        kl_data_model = entropy(pk=p_data_x, qk=p_model_x, base=np.e)\n\n        # Calculate KL(p_model || p_data)\n        kl_model_data = entropy(pk=p_model_x, qk=p_data_x, base=np.e)\n\n        # Store the four required quantities in order.\n        result_for_case = [i_zx, h_z, kl_data_model, kl_model_data]\n        all_results.append(result_for_case)\n\n    # Format the final output string as a list of lists with 6 decimal places.\n    inner_lists_str = []\n    for r in all_results:\n        # Using f-string formatting to ensure 6 decimal places are shown.\n        r_formatted_str = ','.join([f'{val:.6f}' for val in r])\n        inner_lists_str.append(f\"[{r_formatted_str}]\")\n    \n    final_output_str = f\"[{','.join(inner_lists_str)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}