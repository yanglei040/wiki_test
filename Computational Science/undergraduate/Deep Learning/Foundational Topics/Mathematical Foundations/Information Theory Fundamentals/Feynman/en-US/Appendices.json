{
    "hands_on_practices": [
        {
            "introduction": "A central goal in representation learning is to find a data transformation that is both simple and informative. This practice introduces the Information Bottleneck (IB) principle, which formalizes this trade-off using the language of information theory. You will explore how to design an optimal encoder by balancing the compression of the input, measured by the mutual information $I(T;X)$, against the preservation of task-relevant information, measured by $I(T;Y)$. By working through a classic classification task, you will gain hands-on experience in applying this powerful theoretical framework to make a concrete architectural decision: selecting the \"width\" of a representation layer .",
            "id": "3138067",
            "problem": "Consider the Information Bottleneck (IB) principle in a discrete setting with a simple classification task. Let $X$ be a discrete input variable taking $4$ values, $X \\in \\{(0,0),(0,1),(1,0),(1,1)\\}$, each with probability $1/4$. Let $Y$ be a binary label, deterministically defined by the exclusive-or function, $Y = \\mathrm{XOR}(X_1,X_2)$, so that $Y \\in \\{0,1\\}$ and $P(Y=0)=P(Y=1)=1/2$. An encoder maps $X$ to a latent representation $T$ that takes at most $k$ distinct categorical states, where $k$ is referred to as the layer width. We restrict attention to deterministic encoders $f: \\mathcal{X} \\to \\{1,2,\\dots,k\\}$ that obey the Markov chain $Y \\leftrightarrow X \\leftrightarrow T$ via $T=f(X)$.\n\nStart from the core definitions of Shannon entropy and mutual information for discrete random variables: for any discrete random variable $Z$ with distribution $p(z)$, the entropy is $H(Z) = -\\sum_{z} p(z)\\log_2 p(z)$; for any pair $(U,V)$ with joint distribution $p(u,v)$, the mutual information is $I(U;V) = \\sum_{u,v} p(u,v)\\log_2\\frac{p(u,v)}{p(u)p(v)}$. For a deterministic encoder $T=f(X)$, the encoder satisfies $H(T\\mid X)=0$, so $I(T;X)=H(T)$. The Information Bottleneck (IB) design problem seeks to select an encoder to achieve a target predictive information $I(T;Y)$ while keeping the compression $I(T;X)$ below a rate budget. Formally, you must select the smallest width $k$ for which there exists a deterministic $f$ such that $I(T;Y) \\ge \\tau$ and $I(T;X) \\le R$, where $\\tau$ is the target predictive information and $R$ is the rate budget.\n\nYour task is to write a complete program that:\n1. Enumerates all deterministic encoders $f$ for each width $k \\in \\{1,2,3,4\\}$.\n2. For each encoder and width, computes $I(T;X)$ and $I(T;Y)$ exactly from the discrete distributions induced by $X$, $Y$, and $T=f(X)$ using the definitions above.\n3. For a given pair $(R,\\tau)$, finds the smallest width $k$ for which there exists at least one encoder satisfying $I(T;Y) \\ge \\tau$ and $I(T;X) \\le R$. Among encoders of that smallest width, use any encoder that maximizes $I(T;Y)$ under the constraint $I(T;X) \\le R$ to define a classifier $g$ that predicts $Y$ from $T$ by the Bayes rule $g(t)=\\arg\\max_{y} p(y\\mid t)$. Report the resulting Bayes classification accuracy $\\sum_{t} p(t)\\max_{y} p(y\\mid t)$.\n4. If no encoder across $k \\in \\{1,2,3,4\\}$ satisfies the constraints, report the special width $-1$ and the Bayes accuracy of the encoder (over all $k \\in \\{1,2,3,4\\}$) that maximizes $I(T;Y)$ subject to $I(T;X) \\le R$; if multiple encoders achieve the same maximum $I(T;Y)$, choose any with the highest Bayes accuracy.\n\nDesign your solution from the fundamental definitions of entropy and mutual information and the Markov property $Y \\leftrightarrow X \\leftrightarrow T$. Do not invoke any approximations or continuous relaxations; compute all probabilities and information quantities exactly for the discrete distributions induced by the deterministic encoders.\n\nTest suite. Your program must run on the following three test cases, each specified by $(R,\\tau)$ in bits:\n- Case A (happy path): $(R,\\tau) = (1.0, 1.0)$.\n- Case B (infeasible target under tight budget): $(R,\\tau) = (0.75, 0.1)$.\n- Case C (boundary entropy budget near a $3$-$1$ partition): $(R,\\tau) = (0.811, 0.3)$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output two values in order: the selected width $k$ (or $-1$ if no encoder satisfies the constraints) and the corresponding Bayes accuracy. The Bayes accuracy must be rounded to exactly three decimal places. The final output must therefore be a single list with six entries in the order for Cases A, B, C:\n$[k_A,\\mathrm{acc}_A,k_B,\\mathrm{acc}_B,k_C,\\mathrm{acc}_C]$.",
            "solution": "The user provided a valid problem.\n\nThe problem asks us to find the optimal deterministic encoder for a simple classification task, according to the principles of the Information Bottleneck (IB). The optimization involves a trade-off between compressing the input $X$ and retaining information about the label $Y$.\n\n### Step 1: Formalizing the Problem Setup\n\nLet the input variable be $X$, taking values in the alphabet $\\mathcal{X} = \\{\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{x}_4\\}$, where we identify $\\mathbf{x}_1=(0,0)$, $\\mathbf{x}_2=(0,1)$, $\\mathbf{x}_3=(1,0)$, and $\\mathbf{x}_4=(1,1)$. The problem states that the input distribution is uniform, so $P(X=\\mathbf{x}_i) = 1/4$ for $i \\in \\{1,2,3,4\\}$.\n\nThe label $Y$ is a deterministic function of $X$, given by the XOR operation: $Y = \\mathrm{XOR}(X_1, X_2)$. The alphabet for $Y$ is $\\mathcal{Y} = \\{0,1\\}$. The relationship is as follows:\n- $Y(\\mathbf{x}_1=(0,0)) = 0$\n- $Y(\\mathbf{x}_2=(0,1)) = 1$\n- $Y(\\mathbf{x}_3=(1,0)) = 1$\n- $Y(\\mathbf{x}_4=(1,1)) = 0$\n\nThis induces a joint probability distribution $P(X,Y)$. For instance, $P(X=\\mathbf{x}_1, Y=0) = P(X=\\mathbf{x}_1) = 1/4$. From this, we can verify the marginal probability of $Y$:\n$P(Y=0) = P(X=\\mathbf{x}_1) + P(X=\\mathbf{x}_4) = 1/4 + 1/4 = 1/2$.\n$P(Y=1) = P(X=\\mathbf{x}_2) + P(X=\\mathbf{x}_3) = 1/4 + 1/4 = 1/2$.\nThe entropy of the label is $H(Y) = -\\sum_{y \\in \\mathcal{Y}} P(Y=y)\\log_2 P(Y=y) = -2 \\cdot (1/2)\\log_2(1/2) = 1$ bit.\n\n### Step 2: Characterizing Deterministic Encoders\n\nA deterministic encoder $f: \\mathcal{X} \\to \\{1, 2, \\dots, k\\}$ maps each of the $4$ input points to one of $k$ latent states. This is equivalent to partitioning the input set $\\mathcal{X}$ into $k$ non-empty, disjoint subsets $\\mathcal{C} = \\{C_1, C_2, \\dots, C_k\\}$, where each subset $C_j$ contains all inputs that are mapped to the latent state $t_j$. The latent variable $T$ thus follows the Markov chain $Y \\leftrightarrow X \\leftrightarrow T$.\n\nThe number of unique partitions of a set of $4$ elements into $k$ non-empty subsets is given by the Stirling numbers of the second kind, $S(4,k)$:\n- $k=1$: $S(4,1) = 1$ partition.\n- $k=2$: $S(4,2) = 7$ partitions.\n- $k=3$: $S(4,3) = 6$ partitions.\n- $k=4$: $S(4,4) = 1$ partition.\nIn total, there are $1+7+6+1=15$ unique encoders to evaluate.\n\n### Step 3: Calculating Information Theoretic and Accuracy Metrics\n\nFor each encoder, defined by a partition $\\mathcal{C} = \\{C_1, \\dots, C_k\\}$, we must compute two mutual information quantities and the Bayes accuracy.\n\n1.  **Complexity/Compression ($I(T;X)$)**: Since the encoder is deterministic ($T=f(X)$), we have $H(T|X)=0$. The mutual information $I(T;X) = H(T) - H(T|X)$ simplifies to the entropy of the latent representation, $H(T)$. The probability of a latent state $t_j$ is $P(T=t_j) = P(X \\in C_j) = \\sum_{\\mathbf{x} \\in C_j} P(X=\\mathbf{x}) = |C_j|/4$.\n    $$I(T;X) = H(T) = -\\sum_{j=1}^{k} P(T=t_j)\\log_2 P(T=t_j) = -\\sum_{j=1}^{k} \\frac{|C_j|}{4}\\log_2 \\frac{|C_j|}{4}$$\n\n2.  **Predictive Information ($I(T;Y)$)**: This is given by $I(T;Y) = H(Y) - H(Y|T)$. We already know $H(Y)=1$. The conditional entropy is $H(Y|T) = \\sum_{j=1}^{k} P(T=t_j) H(Y|T=t_j)$. To calculate $H(Y|T=t_j) = -\\sum_{y \\in \\mathcal{Y}} P(Y=y|T=t_j)\\log_2 P(Y=y|T=t_j)$, we need the conditional probability $P(Y=y|T=t_j) = P(Y=y, X \\in C_j) / P(X \\in C_j)$.\n    Let $N_{j,y} = |\\{\\mathbf{x} \\in C_j \\mid Y(\\mathbf{x})=y\\}|$ be the number of inputs in cluster $C_j$ with label $y$. Then $P(Y=y, X \\in C_j) = N_{j,y}/4$ and $P(X \\in C_j) = |C_j|/4$.\n    $$P(Y=y|T=t_j) = \\frac{N_{j,y}}{|C_j|}$$\n\n3.  **Bayes Accuracy**: The optimal classifier given $T$ predicts the most likely label: $g(t_j) = \\arg\\max_{y \\in \\mathcal{Y}} P(Y=y|T=t_j)$. The accuracy of this classifier is the probability of a correct prediction, averaged over all latent states:\n    $$\\text{Accuracy} = \\sum_{j=1}^{k} P(T=t_j) \\max_{y \\in \\mathcal{Y}} P(Y=y|T=t_j) = \\sum_{j=1}^{k} \\frac{|C_j|}{4} \\frac{\\max_{y} N_{j,y}}{|C_j|} = \\frac{1}{4} \\sum_{j=1}^{k} \\max(N_{j,0}, N_{j,1})$$\n\n### Step 4: Enumeration of All Encoders\n\nWe enumerate all $15$ unique partitions and compute their corresponding metrics. Let $S_0 = \\{\\mathbf{x}_1, \\mathbf{x}_4\\}$ (inputs with label $0$) and $S_1 = \\{\\mathbf{x}_2, \\mathbf{x}_3\\}$ (inputs with label $1$).\n\n| $k$ | Partition Type | Example Partition | $I(T;X)$ (bits) | $I(T;Y)$ (bits) | Accuracy |\n|:---:|:--------------|:-------------------|:----------------|:----------------|:---------|\n| 1 | `(4)` | $\\{\\mathcal{X}\\}$ | $0.0$ | $0.0$ | $0.5$ |\n| 2 | `(3,1)` | $\\{\\mathbf{x}_1,\\mathbf{x}_2,\\mathbf{x}_3\\}, \\{\\mathbf{x}_4\\}$ | $H_b(1/4) \\approx 0.811$ | $1 - \\frac{3}{4} H_b(1/3) \\approx 0.311$ | $0.75$ |\n| 2 | `(2,2)` (Y-mixed) | $\\{\\mathbf{x}_1,\\mathbf{x}_2\\}, \\{\\mathbf{x}_3,\\mathbf{x}_4\\}$ | $1.0$ | $0.0$ | $0.5$ |\n| 2 | `(2,2)` (Y-pure) | $\\{S_0, S_1\\}$ | $1.0$ | $1.0$ | $1.0$ |\n| 3 | `(2,1,1)` (Y-mixed) | $\\{\\mathbf{x}_1,\\mathbf{x}_2\\}, \\{\\mathbf{x}_3\\}, \\{\\mathbf{x}_4\\}$ | $1.5$ | $0.5$ | $0.75$ |\n| 3 | `(2,1,1)` (Y-pure) | $\\{S_0, \\{\\mathbf{x}_2\\}, \\{\\mathbf{x}_3\\}\\}$ | $1.5$ | $1.0$ | $1.0$ |\n| 4 | `(1,1,1,1)` | $\\{\\{\\mathbf{x}_1\\}, \\{\\mathbf{x}_2\\}, \\dots\\}$ | $2.0$ | $1.0$ | $1.0$ |\n*Note: $H_b(p) = -p\\log_2 p - (1-p)\\log_2(1-p)$ is the binary entropy function. The table lists metrics for representative partition types; there are multiple partitions of each type but they yield identical metrics.*\n\n### Step 5: Applying the Decision Logic\n\nFor each test case $(R, \\tau)$, we follow the prescribed logic:\n1.  Find the smallest width $k \\in \\{1,2,3,4\\}$ for which there exists an encoder satisfying $I(T;Y) \\ge \\tau$ and $I(T;X) \\le R$.\n2.  If such a $k$ is found, select an encoder of that width that maximizes $I(T;Y)$ (subject to $I(T;X) \\le R$) and report its accuracy.\n3.  If no such encoder exists, report $k=-1$. For the accuracy, consider all encoders (from all $k$) that satisfy the rate budget $I(T;X) \\le R$, pick one that maximizes $I(T;Y)$ (with highest accuracy as a tie-breaker), and report its accuracy.\n\n**Case A: $(R, \\tau) = (1.0, 1.0)$**\n- We seek an encoder with $I(T;X) \\le 1.0$ and $I(T;Y) \\ge 1.0$.\n- For $k=1$, no encoder works.\n- For $k=2$, the Y-pure partition $\\{S_0, S_1\\}$ has $I(T;X)=1.0$ and $I(T;Y)=1.0$. This perfectly satisfies the constraints.\n- The smallest width is $k=2$. The corresponding accuracy is $1.0$.\n- Result: $k_A=2$, $\\mathrm{acc}_A=1.000$.\n\n**Case B: $(R, \\tau) = (0.75, 0.1)$**\n- We seek an encoder with $I(T;X) \\le 0.75$ and $I(T;Y) \\ge 0.1$.\n- Reviewing the table, the lowest non-zero $I(T;X)$ is $\\approx 0.811$ for $k=2$. All encoders for $k \\ge 2$ have $I(T;X) > 0.75$.\n- The only encoder with $I(T;X) \\le 0.75$ is the trivial $k=1$ encoder, for which $I(T;X)=0$. However, its $I(T;Y)=0$, which fails the condition $I(T;Y) \\ge 0.1$.\n- No encoder satisfies the constraints. We must report $k_B=-1$.\n- For the accuracy, we find encoders with $I(T;X) \\le 0.75$. Only the $k=1$ encoder qualifies. It has $I(T;Y)=0$ and accuracy $0.5$.\n- Result: $k_B=-1$, $\\mathrm{acc}_B=0.500$.\n\n**Case C: $(R, \\tau) = (0.811, 0.3)$**\n- We seek an encoder with $I(T;X) \\le 0.811$ and $I(T;Y) \\ge 0.3$.\n- The $k=2$ partition of type `(3,1)` has $I(T;X) = H_b(1/4) \\approx 0.811278$, which is strictly greater than the budget $R=0.811$.\n- All other encoders for $k \\ge 2$ have even higher $I(T;X)$.\n- The only encoder satisfying $I(T;X) \\le 0.811$ is the $k=1$ encoder. Its $I(T;Y)=0$, which is less than $\\tau=0.3$.\n- No encoder satisfies the constraints. We report $k_C=-1$.\n- For the accuracy, we consider encoders with $I(T;X) \\le 0.811$. Again, only the $k=1$ encoder qualifies. Its accuracy is $0.5$.\n- Result: $k_C=-1$, $\\mathrm{acc}_C=0.500$.",
            "answer": "```python\nimport numpy as np\nfrom itertools import product\nfrom collections import defaultdict\nimport math\n\ndef get_partitions(n, k):\n    \"\"\"\n    Generates all unique partitions of a set of n items into k non-empty subsets.\n    \"\"\"\n    if k == 0 and n == 0:\n        yield []\n        return\n    if k == 0 or k > n:\n        return\n    \n    if k == 1:\n        yield [list(range(n))]\n        return\n    \n    # Bell number B_4 = 15, small enough to be hardcoded for robustness.\n    # The set of items is {0, 1, 2, 3}.\n    items = list(range(n))\n    \n    # This recursive implementation correctly generates all partitions.\n    # It partitions items[1:] into k-1 sets and adds {items[0]} as a new set,\n    # or partitions items[1:] into k sets and adds items[0] to one of them.\n    if n > 0:\n        first, rest = items[0], items[1:]\n        # Case 1: {first} is a partition element on its own.\n        for p in get_partitions(n - 1, k - 1):\n            yield [[first]] + p\n        # Case 2: first is added to an existing partition element.\n        for p in get_partitions(n - 1, k):\n            for i in range(len(p)):\n                new_p = [list(part) for part in p] # deep copy\n                new_p[i].append(first)\n                # Sort to ensure canonical order and avoid duplicates\n                yield sorted(new_p, key=lambda x: x[0])\n\n\ndef solve():\n    \"\"\"\n    Main function to solve the Information Bottleneck problem.\n    \"\"\"\n    test_cases = [\n        (1.0, 1.0),    # Case A\n        (0.75, 0.1),   # Case B\n        (0.811, 0.3),  # Case C\n    ]\n    \n    # Problem setup\n    X_values = [(0, 0), (0, 1), (1, 0), (1, 1)]\n    # Map from X index (0,1,2,3) to Y value\n    Y_values = [v[0] ^ v[1] for v in X_values]\n    num_inputs = len(X_values)\n    H_Y = 1.0  # Since P(Y=0) = P(Y=1) = 0.5\n    \n    all_encoders_metrics = []\n\n    # Step 1: Enumerate all encoders and pre-calculate metrics\n    for k in range(1, num_inputs + 1):\n        # Generate unique partitions for width k\n        partitions = set()\n        for p_list in get_partitions(num_inputs,k):\n            # Canonical representation to handle duplicates\n            canonical_p = tuple(sorted(tuple(sorted(part)) for part in p_list))\n            partitions.add(canonical_p)\n\n        for partition in partitions:\n            # Step 2: Calculate metrics for each encoder\n            cluster_probs = [len(c) / num_inputs for c in partition]\n            I_TX = -sum(p * np.log2(p) for p in cluster_probs if p > 0)\n\n            H_Y_given_T = 0.0\n            total_max_y_counts = 0\n            \n            for cluster_indices in partition:\n                cluster_size = len(cluster_indices)\n                if cluster_size == 0:\n                    continue\n\n                cluster_prob = cluster_size / num_inputs\n                \n                y_counts = defaultdict(int)\n                for item_idx in cluster_indices:\n                    y_val = Y_values[item_idx]\n                    y_counts[y_val] += 1\n\n                total_max_y_counts += max(y_counts.values()) if y_counts else 0\n\n                h_y_t = 0.0\n                for count in y_counts.values():\n                    p_y_t = count / cluster_size\n                    if p_y_t > 0:\n                        h_y_t -= p_y_t * np.log2(p_y_t)\n                \n                H_Y_given_T += cluster_prob * h_y_t\n\n            I_TY = H_Y - H_Y_given_T\n            accuracy = total_max_y_counts / num_inputs\n            \n            # Using a small epsilon for floating point comparisons around 0.\n            if abs(I_TX)  1e-9: I_TX = 0.0\n            if abs(I_TY)  1e-9: I_TY = 0.0\n\n            all_encoders_metrics.append({'k': k, 'I_TX': I_TX, 'I_TY': I_TY, 'acc': accuracy})\n\n    final_results = []\n    # Step 3: Apply decision logic for each test case\n    for R, tau in test_cases:\n        \n        # Part 1: Find smallest valid k\n        min_valid_k = -1\n        best_acc_for_k = -1.0\n        \n        valid_encoders = []\n        for enc in all_encoders_metrics:\n            if enc['I_TY'] >= tau and enc['I_TX'] = R:\n                 valid_encoders.append(enc)\n        \n        if valid_encoders:\n            min_valid_k = min(enc['k'] for enc in valid_encoders)\n            \n            # Find best encoder at that minimal width k\n            encoders_at_min_k = [enc for enc in valid_encoders if enc['k'] == min_valid_k]\n            \n            # Sort by I(T;Y) descending, then accuracy descending (if needed)\n            encoders_at_min_k.sort(key=lambda x: x['I_TY'], reverse=True)\n            best_acc_for_k = encoders_at_min_k[0]['acc']\n\n            final_results.extend([min_valid_k, f\"{best_acc_for_k:.3f}\"])\n\n        else:\n            # Part 2: No valid encoder found, apply fallback rule\n            min_valid_k = -1\n            \n            eligible_encoders = []\n            for enc in all_encoders_metrics:\n                if enc['I_TX'] = R:\n                    eligible_encoders.append(enc)\n            \n            if eligible_encoders:\n                # Sort by I(T;Y) descending, then accuracy descending\n                eligible_encoders.sort(key=lambda x: (x['I_TY'], x['acc']), reverse=True)\n                best_acc_fallback = eligible_encoders[0]['acc']\n            else:\n                # This case shouldn't be reached with the given test cases,\n                # as the k=1 encoder always has I(T;X)=0.\n                best_acc_fallback = 0.0\n                \n            final_results.extend([min_valid_k, f\"{best_acc_fallback:.3f}\"])\n\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Generative models are a cornerstone of modern deep learning, yet their training can be unstable, leading to common failure modes like \"mode collapse,\" where the model produces only a limited variety of outputs. This practice provides you with an information-theoretic toolkit to diagnose such issues quantitatively. You will learn to move beyond subjective evaluation by computing key metrics such as the mutual information $I(Z;X)$ between latent codes and generated data, and the Kullback-Leibler (KL) divergence between the model and true data distributions. These tools will enable you to precisely identify and understand the behavior of generative models .",
            "id": "3138062",
            "problem": "You are given a discrete latent-variable generative model with a latent variable $Z$ taking values in a finite set and an observable variable $X$ taking values in a finite set. The model defines a prior over latents $p(Z)$ and a likelihood $p(X \\mid Z)$, which together induce a model joint distribution $p(Z, X)$ and a model marginal over data $p_{\\text{model}}(X)$. You are also given a target data distribution $p_{\\text{data}}(X)$ over the same support for $X$. Your task is to use the basic discrete definitions from information theory to quantify failure modes that commonly occur in generative modeling. Specifically, you will compute the mutual information between $Z$ and $X$, the entropy of $Z$, and the Kullback–Leibler (KL) divergence in both directions between $p_{\\text{data}}(X)$ and $p_{\\text{model}}(X)$.\n\nAll logarithms must be the natural logarithm, so information quantities are expressed in nats. For any summation involving probabilities that are zero, apply the standard discrete convention that any term of the form $0 \\cdot \\log(\\cdot)$ contributes $0$ to the sum. Your program must compute these quantities exactly from the given distributions using the basic definitions, with no approximations other than rounding in the final reported values.\n\nFor each test case below, compute and report the following four quantities in the order listed:\n- The mutual information between $Z$ and $X$ under the model joint, denoted $I(Z; X)$.\n- The entropy of the latent prior, denoted $H(Z)$.\n- The Kullback–Leibler divergence from $p_{\\text{data}}(X)$ to $p_{\\text{model}}(X)$, denoted $\\mathrm{KL}(p_{\\text{data}} \\| p_{\\text{model}})$.\n- The Kullback–Leibler divergence from $p_{\\text{model}}(X)$ to $p_{\\text{data}}(X)$, denoted $\\mathrm{KL}(p_{\\text{model}} \\| p_{\\text{data}})$.\n\nImportant implementation details:\n- Use the standard discrete definitions for the above quantities and the natural logarithm throughout.\n- For each float you output, round to $6$ decimal places.\n- Your program must produce a single line of output containing the results for all test cases as a comma-separated list of lists, enclosed in square brackets, for example $[\\,[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4]\\,]$ where each symbol represents a float rounded to $6$ decimal places.\n\nTest suite (all probabilities are given explicitly and sum to $1$ across their respective supports):\n\nCommon support across all test cases:\n- The latent variable has support $Z \\in \\{0, 1, 2\\}$.\n- The observable variable has support $X \\in \\{0, 1, 2\\}$.\n\nTest case $1$ (balanced, near-ideal coverage):\n- Prior $p(Z) = [\\,1/3,\\, 1/3,\\, 1/3\\,]$, that is $[\\,0.\\overline{3},\\, 0.\\overline{3},\\, 0.\\overline{3}\\,]$.\n- Likelihood $p(X \\mid Z)$ is deterministic and one-to-one: the $3 \\times 3$ matrix has rows $[\\,1,\\,0,\\,0\\,]$, $[\\,0,\\,1,\\,0\\,]$, and $[\\,0,\\,0,\\,1\\,]$ for $Z=0,1,2$ respectively.\n- Data distribution $p_{\\text{data}}(X) = [\\,1/3,\\, 1/3,\\, 1/3\\,]$.\n\nTest case $2$ (mode collapse: identical likelihood across latents):\n- Prior $p(Z) = [\\,1/3,\\, 1/3,\\, 1/3\\,]$.\n- Likelihood rows are identical: each row of $p(X \\mid Z)$ equals $[\\,0.7,\\, 0.2,\\, 0.1\\,]$.\n- Data distribution $p_{\\text{data}}(X) = [\\,0.34,\\, 0.33,\\, 0.33\\,]$.\n\nTest case $3$ (coverage mismatch: overlapping likelihoods and non-uniform prior):\n- Prior $p(Z) = [\\,0.6,\\, 0.3,\\, 0.1\\,]$.\n- Likelihood matrix with rows:\n  - For $Z=0$: $[\\,0.6,\\, 0.25,\\, 0.15\\,]$,\n  - For $Z=1$: $[\\,0.3,\\, 0.4,\\, 0.3\\,]$,\n  - For $Z=2$: $[\\,0.2,\\, 0.3,\\, 0.5\\,]$.\n- Data distribution $p_{\\text{data}}(X) = [\\,0.5,\\, 0.3,\\, 0.2\\,]$.\n\nTest case $4$ (boundary condition: degenerate latent prior):\n- Prior $p(Z) = [\\,1.0,\\, 0.0,\\, 0.0\\,]$.\n- Likelihood rows (note only the first row matters due to the prior): each row equals $[\\,0.6,\\, 0.25,\\, 0.15\\,]$.\n- Data distribution $p_{\\text{data}}(X) = [\\,0.6,\\, 0.25,\\, 0.15\\,]$.\n\nYour program must compute the four requested quantities for each test case using the above inputs and output a single line string representing a list of lists, where each inner list corresponds to a test case in order. Round every reported float to $6$ decimal places and do not include any additional text beyond the specified output format.",
            "solution": "The problem requires the computation of four information-theoretic quantities for a discrete latent-variable generative model, which are instrumental in diagnosing its performance characteristics, such as mode collapse and coverage. The model is defined by a prior distribution over latent variables, $p(Z)$, and a conditional distribution (likelihood) of observable variables given the latent ones, $p(X \\mid Z)$. These combine to form a joint distribution $p(Z, X) = p(X \\mid Z)p(Z)$. The model's effectiveness is assessed relative to a given target data distribution, $p_{\\text{data}}(X)$.\n\nThe four quantities to be computed are:\n1.  The mutual information between the latent variable $Z$ and the observable variable $X$ under the model's joint distribution, denoted $I(Z; X)$. This measures the reduction in uncertainty about $X$ from knowing $Z$, indicating how much information the latent code carries about the generated output.\n2.  The entropy of the latent prior, $H(Z)$. This quantifies the diversity of the latent codes being used by the model.\n3.  The Kullback-Leibler (KL) divergence from the data distribution to the model distribution, $\\mathrm{KL}(p_{\\text{data}} \\| p_{\\text{model}})$. This measures how well the model approximates the data distribution, and it heavily penalizes the model for failing to generate samples where the true data distribution has support (i.e., poor coverage).\n4.  The KL divergence from the model distribution to the data distribution, $\\mathrm{KL}(p_{\\text{model}} \\| p_{\\text{data}})$. This divergence penalizes the model for generating samples that are unlikely under the true data distribution (i.e., poor precision or mode collapse).\n\nAll computations must adhere to the standard definitions from information theory using the natural logarithm, with results expressed in nats.\n\n**Theoretical Framework and Computational Strategy**\n\nThe computation proceeds by first deriving the necessary distributions from the given model components and then applying the fundamental definitions of the required quantities.\n\n**Step 1: Derive the Model's Marginal Distribution $p_{\\text{model}}(X)$**\nThe model's marginal distribution over the observable variable $X$ is obtained by marginalizing out the latent variable $Z$ from the joint distribution $p(Z, X)$.\nlet $Z$ take values $\\{z_i\\}_{i=1}^{N_Z}$ and $X$ take values $\\{x_j\\}_{j=1}^{N_X}$.\nThe probability of observing $x_j$ under the model is given by the law of total probability:\n$$p_{\\text{model}}(X=x_j) = \\sum_{i=1}^{N_Z} p(Z=z_i, X=x_j) = \\sum_{i=1}^{N_Z} p(X=x_j \\mid Z=z_i) p(Z=z_i)$$\nThis calculation must be performed for each test case as a prerequisite for computing $I(Z; X)$ and the KL divergences.\n\n**Step 2: Compute $H(Z)$**\nThe entropy of the discrete latent prior $p(Z)$ is given by the standard formula:\n$$H(Z) = - \\sum_{i=1}^{N_Z} p(Z=z_i) \\log p(Z=z_i)$$\nThe convention that terms of the form $0 \\cdot \\log(0)$ contribute $0$ to the sum is applied.\n\n**Step 3: Compute $I(Z; X)$**\nThe mutual information $I(Z; X)$ can be computed using several equivalent formulas. A computationally convenient form is $I(Z; X) = H(X) - H(X \\mid Z)$.\nFirst, we compute the entropy of the model's marginal distribution, $H(X)$:\n$$H(X) = - \\sum_{j=1}^{N_X} p_{\\text{model}}(X=x_j) \\log p_{\\text{model}}(X=x_j)$$\nNext, we compute the conditional entropy $H(X \\mid Z)$, which is the expected value of the entropy of the likelihood distribution $p(X \\mid Z=z_i)$, averaged over the prior $p(Z)$:\n$$H(X \\mid Z) = \\sum_{i=1}^{N_Z} p(Z=z_i) H(X \\mid Z=z_i)$$\nwhere the entropy of each conditional distribution is:\n$$H(X \\mid Z=z_i) = - \\sum_{j=1}^{N_X} p(X=x_j \\mid Z=z_i) \\log p(X=x_j \\mid Z=z_i)$$\nThe mutual information is then the difference:\n$$I(Z; X) = H(X) - H(X \\mid Z)$$\n\n**Step 4: Compute KL Divergences**\nThe KL divergence between two discrete probability distributions $P(Y)$ and $Q(Y)$ over the same support is defined as:\n$$\\mathrm{KL}(P \\| Q) = \\sum_{y} P(y) \\log \\frac{P(y)}{Q(y)}$$\nThe requested quantities are instances of this definition:\n-   $\\mathrm{KL}(p_{\\text{data}} \\| p_{\\text{model}}) = \\sum_{j=1}^{N_X} p_{\\text{data}}(X=x_j) \\log \\frac{p_{\\text{data}}(X=x_j)}{p_{\\text{model}}(X=x_j)}$\n-   $\\mathrm{KL}(p_{\\text{model}} \\| p_{\\text{data}}) = \\sum_{j=1}^{N_X} p_{\\text{model}}(X=x_j) \\log \\frac{p_{\\text{model}}(X=x_j)}{p_{\\text{data}}(X=x_j)}$\n\nIn cases where a probability in the denominator is $0$ while the corresponding numerator is non-zero, the KL divergence is infinite. However, the test cases provided do not exhibit this issue. The convention that a term is $0$ if the probability in the numerator is $0$ is also applied.\n\nThis systematic application of fundamental definitions provides a complete and exact method to calculate the required quantities for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef solve():\n    \"\"\"\n    Computes four information-theoretic quantities for a series of\n    latent-variable generative model test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"p_z\": np.array([1/3, 1/3, 1/3]),\n            \"p_x_cond_z\": np.array([[1.0, 0.0, 0.0],\n                                    [0.0, 1.0, 0.0],\n                                    [0.0, 0.0, 1.0]]),\n            \"p_data_x\": np.array([1/3, 1/3, 1/3])\n        },\n        {\n            \"p_z\": np.array([1/3, 1/3, 1/3]),\n            \"p_x_cond_z\": np.array([[0.7, 0.2, 0.1],\n                                    [0.7, 0.2, 0.1],\n                                    [0.7, 0.2, 0.1]]),\n            \"p_data_x\": np.array([0.34, 0.33, 0.33])\n        },\n        {\n            \"p_z\": np.array([0.6, 0.3, 0.1]),\n            \"p_x_cond_z\": np.array([[0.6, 0.25, 0.15],\n                                    [0.3, 0.4, 0.3],\n                                    [0.2, 0.3, 0.5]]),\n            \"p_data_x\": np.array([0.5, 0.3, 0.2])\n        },\n        {\n            \"p_z\": np.array([1.0, 0.0, 0.0]),\n            \"p_x_cond_z\": np.array([[0.6, 0.25, 0.15],\n                                    [0.6, 0.25, 0.15],\n                                    [0.6, 0.25, 0.15]]),\n            \"p_data_x\": np.array([0.6, 0.25, 0.15])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        p_z = case[\"p_z\"]\n        p_x_cond_z = case[\"p_x_cond_z\"]\n        p_data_x = case[\"p_data_x\"]\n\n        # Calculate H(Z), the entropy of the latent prior.\n        h_z = entropy(p_z, base=np.e)\n\n        # Calculate p_model(X), the model's marginal distribution.\n        # This is sum_z p(x|z)p(z), which is a matrix multiplication.\n        p_model_x = p_z @ p_x_cond_z\n\n        # Calculate I(Z; X) = H(X) - H(X|Z)\n        # H(X) is the entropy of the model's marginal.\n        h_x = entropy(p_model_x, base=np.e)\n        \n        # H(X|Z) is the expectation of the entropy of the likelihoods.\n        # E_p(z) [H(p(X|Z=z))]\n        h_x_cond_z_per_z = np.array([entropy(p, base=np.e) for p in p_x_cond_z])\n        h_x_cond_z = np.dot(p_z, h_x_cond_z_per_z)\n        \n        i_zx = h_x - h_x_cond_z\n\n        # Calculate KL(p_data || p_model)\n        # scipy.stats.entropy calculates KL divergence when qk is provided.\n        kl_data_model = entropy(pk=p_data_x, qk=p_model_x, base=np.e)\n\n        # Calculate KL(p_model || p_data)\n        kl_model_data = entropy(pk=p_model_x, qk=p_data_x, base=np.e)\n\n        # Store the four required quantities in order.\n        result_for_case = [i_zx, h_z, kl_data_model, kl_model_data]\n        all_results.append(result_for_case)\n\n    # Format the final output string as a list of lists with 6 decimal places.\n    inner_lists_str = []\n    for r in all_results:\n        # Using f-string formatting to ensure 6 decimal places are shown.\n        r_formatted_str = ','.join([f'{val:.6f}' for val in r])\n        inner_lists_str.append(f\"[{r_formatted_str}]\")\n    \n    final_output_str = f\"[{','.join(inner_lists_str)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Data augmentation is a critical technique for improving generalization in deep learning, but what makes an augmentation effective? This exercise challenges the simple intuition that making input data more \"complex\" is always better. You will construct experiments to demonstrate a fundamental concept: an augmentation can increase the total entropy of the input distribution, $H(X)$, without increasing the task-relevant mutual information, $I(X;Y)$. This hands-on analysis will help you distinguish between adding meaningless randomness and adding valuable, task-relevant information, building a deeper understanding of how to design effective data augmentation strategies .",
            "id": "3138094",
            "problem": "You are given a directive to demonstrate, using synthetic discrete experiments, that augmentations which increase input entropy $H(X)$ need not increase the mutual information $I(X;Y)$ between inputs and labels. Your task is to construct a complete program that generates data from precisely defined discrete distributions, applies specified augmentations, and computes empirical estimates of Shannon entropy and mutual information. The goal is to separate randomness from task-relevant information using fundamental definitions from information theory.\n\nUse the following foundational base:\n- Shannon entropy $H(X)$ for a discrete random variable $X$ with empirical probabilities $p(x)$ is defined by $H(X) = -\\sum_{x} p(x) \\log_2 p(x)$.\n- Mutual Information (MI) $I(X;Y)$ between discrete random variables $X$ and $Y$ with empirical joint probabilities $p(x,y)$ and marginals $p(x)$ and $p(y)$ is defined by $I(X;Y) = \\sum_{x,y} p(x,y)\\log_2 \\frac{p(x,y)}{p(x)p(y)}$.\n\nAll variables in this problem are discrete and take finitely many values. You will approximate $H(\\cdot)$ and $I(\\cdot\\,;\\cdot)$ using empirical frequencies from Monte Carlo samples of size $n$.\n\nExperimental setup:\n- Let $Y \\in \\{0,1\\}$ be a binary label with $Y \\sim \\text{Bernoulli}(q)$, where $q = \\mathbb{P}(Y=1)$.\n- Let $Z = Y$ represent a task-relevant binary feature.\n- Augmentations introduce additional randomness that is either independent of $Y$ or label-corrupting noise applied to the observed feature.\n- For composite inputs, treat $X$ as a single discrete variable by encoding tuples $(x_1, x_2)$ into an integer category to compute discrete $H(X)$ and $I(X;Y)$ via the definitions above.\n\nYour program must implement empirical estimators for $H(X)$ and $I(X;Y)$ from samples and evaluate the boolean predicate\n$$b = \\left[H\\!\\left(X_{\\text{aug}}\\right)  H\\!\\left(X_{\\text{base}}\\right)\\right] \\wedge \\left[I\\!\\left(X_{\\text{aug}};Y\\right) \\le I\\!\\left(X_{\\text{base}};Y\\right)\\right],$$\nwith a small numerical tolerance to account for sampling error. A true value of $b$ demonstrates that an augmentation increased $H(X)$ while not increasing task-relevant information $I(X;Y)$.\n\nTest suite (each case should be run with the stated parameters and Monte Carlo sample size $n$):\n\n- Case $1$ (happy path, independent randomness added): $q = 0.5$, $n = 200000$. Base input $X_{\\text{base}} = Z$. Augmented input $X_{\\text{aug}} = (Z, N)$ where $N \\sim \\text{Bernoulli}(0.5)$ is independent of $(Z,Y)$.\n- Case $2$ (class imbalance, independent randomness added): $q = 0.1$, $n = 200000$. Base input $X_{\\text{base}} = Z$. Augmented input $X_{\\text{aug}} = (Z, N)$ with $N \\sim \\text{Bernoulli}(0.5)$ independent.\n- Case $3$ (label-independent corrupting noise increases $H(X)$ but reduces $I(X;Y)$): $q = 0.1$, $s = 0.3$, $n = 200000$. Base input $X_{\\text{base}} = Z$. Augmented input $X_{\\text{aug}} = \\tilde{Z}$ where $\\tilde{Z} = Z \\oplus B$ and $B \\sim \\text{Bernoulli}(s)$ is independent of $(Z,Y)$; here $\\oplus$ denotes bitwise exclusive-or.\n- Case $4$ (adding task-relevant information raises $I(X;Y)$): $q = 0.1$, $s = 0.3$, $n = 200000$. Base input $X_{\\text{base}} = \\tilde{Z}$ defined as in Case $3$. Augmented input $X_{\\text{aug}} = (\\tilde{Z}, Y)$.\n- Case $5$ (boundary condition, no corruption): $q = 0.1$, $s = 0.0$, $n = 200000$. Base input $X_{\\text{base}} = Z$. Augmented input $X_{\\text{aug}} = \\tilde{Z}$ with $\\tilde{Z} = Z \\oplus B$ and $B \\sim \\text{Bernoulli}(s)$, i.e., no change when $s = 0.0$.\n\nImplementation requirements:\n- Use a reproducible pseudo-random number generator.\n- Compute empirical $H(X)$ and $I(X;Y)$ for each case directly from samples using the fundamental definitions above.\n- For each case, compute the boolean $b$ given by the predicate, using a tolerance of $10^{-3}$ bits for comparisons of $H(\\cdot)$ and $I(\\cdot\\,;\\cdot)$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\dots,result_5]$), where each $result_i$ is the boolean $b$ for Case $i$ in the order they are listed above.\n\nAll answers are dimensionless and should be reported as booleans as specified. No physical units or angles appear in this problem.",
            "solution": "The objective is to demonstrate through synthetic experiments that data augmentations can increase the total entropy of the input, $H(X)$, without increasing, and sometimes decreasing, the mutual information $I(X;Y)$ it shares with the target label $Y$. This distinction is fundamental to understanding which augmentations provide task-relevant information versus those that merely add task-irrelevant randomness.\n\nThe methodology involves a series of Monte Carlo simulations. For each case, we generate a large number of samples, $n$, from specified discrete probability distributions for a base input $X_{\\text{base}}$ and an augmented input $X_{\\text{aug}}$, along with a binary label $Y$. From these samples, we compute empirical estimates of Shannon entropy and mutual information to evaluate the predicate $b = \\left[H\\!\\left(X_{\\text{aug}}\\right)  H\\!\\left(X_{\\text{base}}\\right)\\right] \\wedge \\left[I\\!\\left(X_{\\text{aug}};Y\\right) \\le I\\!\\left(X_{\\text{base}};Y\\right)\\right]$. A numerical tolerance of $10^{-3}$ is used to make the comparisons robust to sampling noise.\n\nThe core of the analysis rests on the empirical estimation of two information-theoretic quantities for discrete random variables.\n\nFirst, the Shannon entropy of a variable $X$, which can take values in a set $\\mathcal{X}$, is estimated from its empirical probability mass function $p(x)$. Given $n$ samples, $p(x)$ is the frequency of occurrence of the value $x$. The entropy, measured in bits, is:\n$$H(X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x)$$\nwhere the term $p(x) \\log_2 p(x)$ is taken to be $0$ if $p(x)=0$.\n\nSecond, the mutual information between two variables $X$ and $Y$ is a measure of their statistical dependence. For numerical stability and ease of implementation, we use the identity:\n$$I(X;Y) = H(X) + H(Y) - H(X,Y)$$\nwhere $H(X)$ and $H(Y)$ are the marginal entropies, and $H(X,Y)$ is the joint entropy of the pair $(X,Y)$. The joint entropy is calculated from the empirical joint probability distribution $p(x,y)$:\n$$H(X,Y) = -\\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} p(x,y) \\log_2 p(x,y)$$\nWhen an input $X$ is a composite of multiple features, such as $X=(F_1, F_2)$, it is treated as a single discrete variable by encoding the tuple of feature values into a unique integer identifier. This allows the direct application of the above formulae for discrete variables.\n\nThe analysis proceeds by evaluating the predicate $b$ for five distinct cases.\n\n**Case 1: Independent Randomness (Balanced Classes)**\n- Parameters: $q = \\mathbb{P}(Y=1) = 0.5$, $n = 200000$.\n- Base Input: $X_{\\text{base}} = Z$, where $Z=Y$. Here, the input is a perfect copy of the label.\n- Augmented Input: $X_{\\text{aug}} = (Z, N)$, where $N \\sim \\text{Bernoulli}(0.5)$ is a noise variable independent of $Y$.\n- Analysis: $I(X_{\\text{base}};Y) = I(Y;Y) = H(Y) = 1$ bit. Adding the independent noise variable $N$ increases the total entropy, since by independence $H(X_{\\text{aug}}) = H(Z,N) = H(Z) + H(N) = H(Y) + H(N) = 1 + 1 = 2$ bits. The mutual information is unchanged because $N$ is independent of $Y$: $I(X_{\\text{aug}};Y) = I((Z,N);Y) = I(Z;Y) = 1$ bit. Thus, we expect $H(X_{\\text{aug}}) > H(X_{\\text{base}})$ and $I(X_{\\text{aug}};Y) \\le I(X_{\\text{base}};Y)$. The predicate $b$ should evaluate to true.\n\n**Case 2: Independent Randomness (Imbalanced Classes)**\n- Parameters: $q = \\mathbb{P}(Y=1) = 0.1$, $n = 200000$.\n- Setup: Same as Case 1, but with an imbalanced label distribution.\n- Analysis: The reasoning is identical to Case 1. The entropy of the label is $H(Y) = -0.1\\log_2(0.1) - 0.9\\log_2(0.9) \\approx 0.469$ bits. We have $H(X_{\\text{base}}) = H(Y) \\approx 0.469$ bits and $I(X_{\\text{base}};Y) = H(Y) \\approx 0.469$ bits. The augmented entropy is $H(X_{\\text{aug}}) = H(Y) + H(N) \\approx 0.469 + 1 = 1.469$ bits. The mutual information remains $I(X_{\\text{aug}};Y) = I(Y;Y) = H(Y) \\approx 0.469$ bits. Again, we expect $H(X_{\\text{aug}}) > H(X_{\\text{base}})$ and $I(X_{\\text{aug}};Y) \\le I(X_{\\text{base}};Y)$. The predicate $b$ should be true.\n\n**Case 3: Label-Corrupting Noise**\n- Parameters: $q = 0.1$, noise level $s = 0.3$, $n = 200000$.\n- Base Input: $X_{\\text{base}} = Z = Y$.\n- Augmented Input: $X_{\\text{aug}} = \\tilde{Z} = Z \\oplus B$, where $B \\sim \\text{Bernoulli}(s)$ is independent bit-flip noise and $\\oplus$ is the exclusive-or operation. This models a binary symmetric channel.\n- Analysis: $H(X_{\\text{base}}) = H(Y) \\approx 0.469$ bits. The augmented input $\\tilde{Z}$ is a noisy version of $Y$. The probability $\\mathbb{P}(\\tilde{Z}=1) = q(1-s) + (1-q)s = 0.1(0.7) + 0.9(0.3) = 0.34$. The entropy is $H(X_{\\text{aug}}) \\approx H_b(0.34) \\approx 0.923$ bits, which is greater than $H(X_{\\text{base}})$. The noise, however, corrupts the information about $Y$. By the data processing inequality, $I(Y;\\tilde{Z}) \\le I(Y;Z)$. Specifically, $I(X_{\\text{aug}};Y) = H(\\tilde{Z}) - H(\\tilde{Z}|Y) \\approx H_b(0.34) - H_b(s) \\approx 0.923-0.881 \\approx 0.042$ bits. This is significantly less than $I(X_{\\text{base}};Y) = H(Y) \\approx 0.469$ bits. Both conditions of the predicate are met, so $b$ should be true.\n\n**Case 4: Adding Task-Relevant Information**\n- Parameters: $q=0.1, s=0.3, n=200000$.\n- Base Input: $X_{\\text{base}} = \\tilde{Z}$, the noisy variable from Case 3.\n- Augmented Input: $X_{\\text{aug}} = (\\tilde{Z}, Y)$. The augmentation explicitly adds the true label.\n- Analysis: The base quantities are derived from Case 3: $H(X_{\\text{base}}) \\approx 0.923$ bits and $I(X_{\\text{base}};Y) \\approx 0.042$ bits. The augmented input's entropy is $H(X_{\\text{aug}}) = H(\\tilde{Z}, Y) = H(Y) + H(\\tilde{Z}|Y) \\approx H_b(0.1) + H_b(0.3) \\approx 0.469 + 0.881 \\approx 1.35$ bits, which is greater than $H(X_{\\text{base}})$. However, the mutual information is now $I(X_{\\text{aug}};Y) = I((\\tilde{Z},Y);Y) = H(Y) - H(Y|(\\tilde{Z},Y))$. Since $Y$ is perfectly known given the tuple $(\\tilde{Z},Y)$, the conditional entropy $H(Y|(\\tilde{Z},Y))$ is $0$. Thus, $I(X_{\\text{aug}};Y) = H(Y) \\approx 0.469$ bits. This is much larger than $I(X_{\\text{base}};Y) \\approx 0.042$ bits. The condition $I(X_{\\text{aug}};Y) \\le I(X_{\\text{base}};Y)$ is false. The predicate $b$ should be false.\n\n**Case 5: Boundary Condition (No Corruption)**\n- Parameters: $q=0.1, s=0.0, n=200000$.\n- Setup: Same as Case 3, but with the noise probability $s=0$.\n- Analysis: With $s=0$, the noise variable $B$ is always $0$. Thus, $X_{\\text{aug}} = Z \\oplus 0 = Z = X_{\\text{base}}$. The base and augmented inputs are identical. Consequently, $H(X_{\\text{aug}}) = H(X_{\\text{base}})$ and $I(X_{\\text{aug}};Y) = I(X_{\\text{base}};Y)$. The first part of the predicate, $H(X_{\\text{aug}}) > H(X_{\\text{base}})$, is a strict inequality and will be false (within the numerical tolerance). Therefore, the overall predicate $b$ must be false.\n\nThe implementation will generate samples for each case, compute the information-theoretic quantities, and evaluate the predicate $b$ to confirm these theoretical expectations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _calculate_entropy(samples):\n    \"\"\"\n    Computes the Shannon entropy of a discrete random variable from its samples.\n    \"\"\"\n    _, counts = np.unique(samples, return_counts=True)\n    probabilities = counts / len(samples)\n    # The contribution of p*log(p) is 0 for p=0.\n    # We filter to avoid log(0) errors.\n    probabilities = probabilities[probabilities > 0]\n    entropy = -np.sum(probabilities * np.log2(probabilities))\n    return entropy\n\ndef compute_info_metrics(x_samples, y_samples):\n    \"\"\"\n    Computes H(X) and I(X;Y) from samples.\n    I(X;Y) is calculated as H(X) + H(Y) - H(X,Y).\n    \"\"\"\n    # H(X)\n    h_x = _calculate_entropy(x_samples)\n    \n    # H(Y)\n    h_y = _calculate_entropy(y_samples)\n    \n    # H(X, Y) - Joint Entropy\n    # We get joint probabilities by counting unique pairs (rows).\n    joint_samples = np.c_[x_samples, y_samples]\n    _, joint_counts = np.unique(joint_samples, axis=0, return_counts=True)\n    joint_probabilities = joint_counts / len(x_samples)\n    joint_probabilities = joint_probabilities[joint_probabilities > 0]\n    h_xy = -np.sum(joint_probabilities * np.log2(joint_probabilities))\n    \n    # I(X;Y) = H(X) + H(Y) - H(X,Y)\n    i_xy = h_x + h_y - h_xy\n    \n    return h_x, i_xy\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute the boolean predicate for each case.\n    \"\"\"\n    test_cases = [\n        # (case_id, q, s, n)\n        (1, 0.5, None, 200000),\n        (2, 0.1, None, 200000),\n        (3, 0.1, 0.3, 200000),\n        (4, 0.1, 0.3, 200000),\n        (5, 0.1, 0.0, 200000),\n    ]\n\n    results = []\n    rng = np.random.default_rng(seed=123)  # Reproducible pseudo-random number generator\n    tol = 1e-3  # Numerical tolerance in bits\n\n    for case_id, q, s, n in test_cases:\n        # Generate base data\n        y = rng.binomial(1, q, size=n)\n        z = y\n\n        if case_id == 1:\n            # Case 1: happy path, independent randomness added\n            x_base = z\n            n_rv = rng.binomial(1, 0.5, size=n)\n            # Encode tuple (Z, N) into a single integer\n            x_aug = 2 * z + n_rv\n\n        elif case_id == 2:\n            # Case 2: class imbalance, independent randomness added\n            x_base = z\n            n_rv = rng.binomial(1, 0.5, size=n)\n            # Encode tuple (Z, N) into a single integer\n            x_aug = 2 * z + n_rv\n\n        elif case_id == 3:\n            # Case 3: label-independent corrupting noise\n            x_base = z\n            b_rv = rng.binomial(1, s, size=n)\n            z_tilde = z ^ b_rv  # bitwise XOR\n            x_aug = z_tilde\n        \n        elif case_id == 4:\n            # Case 4: adding task-relevant information\n            b_rv = rng.binomial(1, s, size=n)\n            z_tilde = z ^ b_rv\n            x_base = z_tilde\n            # Encode tuple (Z_tilde, Y) into a single integer\n            x_aug = 2 * z_tilde + y\n            \n        elif case_id == 5:\n            # Case 5: boundary condition, no corruption\n            x_base = z\n            b_rv = rng.binomial(1, s, size=n) # s=0.0 means B is all zeros\n            z_tilde = z ^ b_rv\n            x_aug = z_tilde\n        else:\n            continue\n\n        # Compute information-theoretic metrics for base and augmented inputs\n        h_base, i_base = compute_info_metrics(x_base, y)\n        h_aug, i_aug = compute_info_metrics(x_aug, y)\n\n        # Evaluate the boolean predicate b with tolerance\n        # b = [H(X_aug) > H(X_base)] AND [I(X_aug;Y) = I(X_base;Y)]\n        # H_aug > H_base + tol : Guards against noise making things unequal\n        # I_aug = I_base + tol : Guards against noise making things unequal\n        is_h_aug_greater = h_aug > h_base + tol\n        is_i_aug_le_i_base = i_aug = i_base + tol\n        \n        b = is_h_aug_greater and is_i_aug_le_i_base\n        results.append(b)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}