{
    "hands_on_practices": [
        {
            "introduction": "The power of linear algebra lies in its ability to describe structures far beyond the familiar vectors in $\\mathbb{R}^n$. Vector spaces can be composed of functions, such as polynomials, which are central to many areas of science and engineering. This exercise  invites you to explore this abstract concept by finding a basis for a specific subspace of polynomials. By testing sets for linear independence and the ability to span the subspace, you will gain a concrete understanding of what constitutes a basis, the fundamental framework for any vector space.",
            "id": "1868582",
            "problem": "Let $P_3$ be the vector space of all polynomials of degree at most 3 with real coefficients. Consider the subspace $W$ of $P_3$ defined by the set $W = \\{ p(x) \\in P_3 \\mid p(1) = 0 \\}$. Which of the following sets of polynomials forms a basis for the subspace $W$?\n\nA. $S_1 = \\{ x-1, x^2-1, x^3-1 \\}$\n\nB. $S_2 = \\{ x-1, x^2-1, x^2-x \\}$\n\nC. $S_3 = \\{ x-1, x^2-1 \\}$\n\nD. $S_4 = \\{ 1, x, x^2 \\}$\n\nE. $S_5 = \\{ x-1, x^2-1, x^3 \\}$",
            "solution": "Let $P_{3}$ be the $4$-dimensional real vector space of polynomials of degree at most $3$. Define the linear map $T:P_{3}\\to\\mathbb{R}$ by $T(p)=p(1)$. The subspace $W$ is $W=\\ker(T)$. Since $T$ is surjective (for example, $T(1)=1$), by the rank-nullity theorem,\n$$\n\\dim(W)=\\dim(P_{3})-\\dim(\\operatorname{im}T)=4-1=3.\n$$\nEquivalently, by the factor theorem, $p(1)=0$ if and only if $x-1$ divides $p(x)$, so every $p\\in W$ can be written as $p(x)=(x-1)q(x)$ with $q\\in P_{2}$, again giving $\\dim(W)=3$.\n\nWe test each candidate set.\n\nFor $S_{1}=\\{x-1,\\ x^{2}-1,\\ x^{3}-1\\}$:\n- Membership in $W$: $(x-1)(1)=0$, $(x^{2}-1)(1)=1-1=0$, $(x^{3}-1)(1)=1-1=0$, so all three are in $W$.\n- Linear independence: Suppose\n$$\na(x-1)+b(x^{2}-1)+c(x^{3}-1)=0.\n$$\nExpand and collect coefficients:\n$$\nc x^{3}+b x^{2}+a x+(-a-b-c)=0.\n$$\nEquating coefficients gives\n$$\nc=0,\\quad b=0,\\quad a=0,\\quad -a-b-c=0,\n$$\nso only the trivial solution exists. Thus the three vectors are linearly independent. Since $W$ has dimension $3$, $S_{1}$ is a basis for $W$.\n\nFor $S_{2}=\\{x-1,\\ x^{2}-1,\\ x^{2}-x\\}$:\n- Membership in $W$: $(x-1)(1)=0$, $(x^{2}-1)(1)=0$, $(x^{2}-x)(1)=0$.\n- Linear independence: Suppose\n$$\na(x-1)+b(x^{2}-1)+c(x^{2}-x)=0.\n$$\nThen\n$$\n(b+c)x^{2}+(a-c)x+(-a-b)=0,\n$$\nso\n$$\nb+c=0,\\quad a-c=0,\\quad -a-b=0.\n$$\nThese imply $a=c$ and $b=-c$, yielding nontrivial solutions (e.g., take $c\\neq 0$). Hence the set is linearly dependent and not a basis.\n\nFor $S_{3}=\\{x-1,\\ x^{2}-1\\}$:\n- It has only $2$ vectors, but $\\dim(W)=3$, so it cannot span $W$ and is not a basis.\n\nFor $S_{4}=\\{1,\\ x,\\ x^{2}\\}$:\n- Membership: $1\\notin W$ since $1(1)=1\\neq 0$. Thus it is not even a subset of $W$ and cannot be a basis of $W$.\n\nFor $S_{5}=\\{x-1,\\ x^{2}-1,\\ x^{3}\\}$:\n- Membership: $x^{3}\\notin W$ since $x^{3}(1)=1\\neq 0$. Hence not a basis of $W$.\n\nTherefore, the only valid basis among the options is $S_{1}$.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "While constructing a basis explicitly is a fundamental skill, it is often more efficient to determine its size—the dimension of the space—using higher-level theorems. This practice  demonstrates this by asking for the dimension of a vector space of matrices, which are the building blocks of neural networks. You will learn to reframe a constraint on the matrices as a linear map and apply the rank-nullity theorem, providing a powerful shortcut to understanding the degrees of freedom within the space without listing every basis vector.",
            "id": "1868614",
            "problem": "Consider the set of all $3 \\times 3$ matrices with real-valued entries. Within this set, we are interested in a special collection of matrices, which we'll call 'zero-sum matrices'. A matrix is defined as a zero-sum matrix if the sum of the elements on its main diagonal is equal to zero. Any zero-sum matrix can be constructed by taking a weighted sum (a linear combination) of a smaller, fundamental set of zero-sum matrices. What is the minimum number of matrices required in such a fundamental set, such that any $3 \\times 3$ zero-sum matrix can be represented as a weighted sum of the matrices in this set?",
            "solution": "The problem asks for the minimum number of matrices in a \"fundamental set\" that can be used to generate any $3 \\times 3$ \"zero-sum matrix\" through weighted sums. In the language of linear algebra, this is equivalent to finding the dimension of the subspace of traceless $3 \\times 3$ matrices.\n\nLet $V = M_{3}(\\mathbb{R})$ be the vector space of all $3 \\times 3$ matrices with real entries. The dimension of this vector space is $\\dim(V) = 3 \\times 3 = 9$. A standard basis for $V$ consists of the nine matrices $E_{ij}$ (for $i,j \\in \\{1, 2, 3\\}$), where $E_{ij}$ is the matrix with a 1 in the $i$-th row and $j$-th column and zeros elsewhere.\n\nLet $W$ be the set of \"zero-sum matrices\". A matrix $A \\in V$ is in $W$ if its trace is zero. The trace of a matrix is the sum of its diagonal elements. So, for $A = (a_{ij})$, $A \\in W$ if and only if $\\text{tr}(A) = a_{11} + a_{22} + a_{33} = 0$.\n\nFirst, we establish that $W$ is a subspace of $V$.\n1.  The zero matrix, $O$, has a trace of $0+0+0=0$, so $O \\in W$.\n2.  Let $A, B \\in W$. Then $\\text{tr}(A)=0$ and $\\text{tr}(B)=0$. The trace is a linear operation, so $\\text{tr}(A+B) = \\texttr(A) + \\text{tr}(B) = 0 + 0 = 0$. Thus, $A+B \\in W$.\n3.  Let $A \\in W$ and $c \\in \\mathbb{R}$ be a scalar. Then $\\text{tr}(cA) = c \\cdot \\text{tr}(A) = c \\cdot 0 = 0$. Thus, $cA \\in W$.\nSince $W$ is closed under addition and scalar multiplication, it is a subspace of $V$. The question asks for the dimension of this subspace, $\\dim(W)$.\n\nWe can find this dimension by using the rank-nullity theorem. Let's define a linear map $T: M_{3}(\\mathbb{R}) \\to \\mathbb{R}$ by $T(A) = \\text{tr}(A)$.\nThis map is a linear transformation (also called a linear functional) because:\n-   $T(A+B) = \\text{tr}(A+B) = \\text{tr}(A) + \\text{tr}(B) = T(A) + T(B)$\n-   $T(cA) = \\text{tr}(cA) = c \\cdot \\text{tr}(A) = c \\cdot T(A)$\n\nThe set $W$ is precisely the set of matrices $A$ for which $T(A) = 0$. This means that $W$ is the kernel (or null space) of the linear transformation $T$. So, $W = \\ker(T)$.\n\nThe rank-nullity theorem states that for a linear map $T: V \\to U$, we have:\n$$ \\dim(V) = \\dim(\\ker(T)) + \\dim(\\text{Im}(T)) $$\nHere, $V = M_{3}(\\mathbb{R})$ and the codomain is $\\mathbb{R}$.\n\nWe already know $\\dim(V) = 9$. We need to find the dimension of the image (or range) of $T$, denoted $\\text{Im}(T)$. The image of $T$ is the set of all possible values that the trace can take. For any real number $r \\in \\mathbb{R}$, we can construct a matrix $A \\in M_{3}(\\mathbb{R})$ such that $\\text{tr}(A)=r$. For example, the matrix\n$$ A = \\begin{pmatrix} r & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\nhas $\\text{tr}(A) = r$. Since the trace can be any real number, the image of $T$ is the entire set of real numbers, $\\mathbb{R}$. The dimension of $\\mathbb{R}$ as a vector space over itself is 1. So, $\\dim(\\text{Im}(T)) = 1$.\n\nNow, we can apply the rank-nullity theorem:\n$$ \\dim(M_{3}(\\mathbb{R})) = \\dim(\\ker(T)) + \\dim(\\text{Im}(T)) $$\n$$ 9 = \\dim(W) + 1 $$\nSolving for $\\dim(W)$, we get:\n$$ \\dim(W) = 9 - 1 = 8 $$\n\nTherefore, the minimum number of matrices required in the fundamental set is 8.",
            "answer": "$$\\boxed{8}$$"
        },
        {
            "introduction": "Linear independence is not just an algebraic condition; it has profound geometric meaning, especially in the study of curves, surfaces, and manifolds. At any point on a smooth surface, a set of linearly independent tangent vectors forms a basis for a tangent plane, the local linear approximation of the surface. This problem  explores what happens when this condition fails at a singularity, like the apex of a cone. By analyzing the tangent vectors at this special point, you will discover how linear dependence manifests geometrically, signaling a point where the surface is not smooth.",
            "id": "1651253",
            "problem": "Consider a surface in three-dimensional Cartesian space parametrized by the vector function $\\vec{r}(u, v) = \\langle v \\cos(u), v \\sin(u), v \\rangle$, where the parameters are in the domain $u \\in [0, 2\\pi)$ and $v \\ge 0$. This surface describes a right circular cone with its apex located at the origin of the coordinate system. The tangent vectors associated with the parameter curves are defined as $\\vec{T}_u = \\frac{\\partial \\vec{r}}{\\partial u}$ and $\\vec{T}_v = \\frac{\\partial \\vec{r}}{\\partial v}$.\n\nEvaluate the set of tangent vectors $\\{\\vec{T}_u, \\vec{T}_v\\}$ at the apex of the cone and analyze their linear dependence. Based on your analysis, which one of the following statements is correct?\n\nA. The vectors $\\vec{T}_u$ and $\\vec{T}_v$ are linearly independent at the apex, and thus form a valid basis for a tangent plane at that point.\n\nB. The vectors $\\vec{T}_u$ and $\\vec{T}_v$ are linearly dependent at the apex because $\\vec{T}_u$ evaluates to the zero vector.\n\nC. The vectors $\\vec{T}_u$ and $\\vec{T}_v$ are linearly dependent at the apex because $\\vec{T}_v$ evaluates to the zero vector.\n\nD. The vectors $\\vec{T}_u$ and $\\vec{T}_v$ are linearly dependent at the apex because they are non-zero but are parallel (scalar multiples of one another).\n\nE. The linear independence of $\\vec{T}_u$ and $\\vec{T}_v$ at the apex cannot be determined because it depends on the specific value of the parameter $u$.",
            "solution": "The surface is parametrized by $\\vec{r}(u,v)=\\langle v\\cos(u),v\\sin(u),v\\rangle$ with $u\\in[0,2\\pi)$ and $v\\ge 0$. By definition of tangent vectors to a parametrized surface, \n$$\n\\vec{T}_{u}=\\frac{\\partial\\vec{r}}{\\partial u},\\qquad \\vec{T}_{v}=\\frac{\\partial\\vec{r}}{\\partial v}.\n$$\nCompute the partial derivatives componentwise:\n$$\n\\vec{T}_{u}(u,v)=\\left\\langle -v\\sin(u),\\,v\\cos(u),\\,0\\right\\rangle,\\qquad\n\\vec{T}_{v}(u,v)=\\left\\langle \\cos(u),\\,\\sin(u),\\,1\\right\\rangle.\n$$\nThe apex of the cone is the point $\\vec{r}(u,0)=\\langle 0,0,0\\rangle$ for any $u$. Evaluating the tangent vectors at $v=0$ gives\n$$\n\\vec{T}_{u}(u,0)=\\langle 0,0,0\\rangle,\\qquad \\vec{T}_{v}(u,0)=\\langle \\cos(u),\\sin(u),1\\rangle.\n$$\nThus $\\vec{T}_{u}(u,0)$ is the zero vector, while $\\vec{T}_{v}(u,0)$ is nonzero for all $u$ because its third component equals $1$. A set containing the zero vector is linearly dependent, so $\\{\\vec{T}_{u},\\vec{T}_{v}\\}$ is linearly dependent at the apex specifically because $\\vec{T}_{u}$ vanishes there. Therefore, the correct choice is B.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}