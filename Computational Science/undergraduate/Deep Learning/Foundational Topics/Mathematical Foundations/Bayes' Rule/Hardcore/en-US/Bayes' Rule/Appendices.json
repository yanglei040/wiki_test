{
    "hands_on_practices": [
        {
            "introduction": "At its heart, Bayes' rule is a formula for updating our beliefs in light of new evidence. This first exercise provides a clean, foundational workout in applying the rule. By analyzing a hypothetical digital memory device , you will isolate the core components of a Bayesian update—the prior, the likelihood, and the evidence—to calculate a posterior probability from scratch.",
            "id": "1603705",
            "problem": "A digital memory device stores data as a sequence of binary bits. Let the source bit, denoted by the random variable $X$, be either $0$ or $1$. From extensive analysis of the data patterns, it is known that the prior probability of a bit being $0$ is $P(X=0) = \\alpha$.\n\nWhen a bit is read from the device, one of three outcomes can occur for the received symbol, denoted by $Y$: the bit is read correctly as $0$, correctly as $1$, or the read operation fails, resulting in an 'erasure' symbol, which we denote as '?'. The device is designed such that it never flips a bit; a stored $0$ is never read as a $1$, and a stored $1$ is never read as a $0$.\n\nThe reliability of the read operation, however, depends on the stored value. The probability of an erasure occurring when the stored bit is $0$ is $P(Y='?'|X=0) = p_0$. The probability of an erasure occurring when the stored bit is $1$ is $P(Y='?'|X=1) = p_1$.\n\nSuppose a single bit is read from the device and the outcome is an erasure, '?'. Determine the posterior probability that the bit originally stored in the device was a $0$. Provide your answer as a single closed-form analytic expression in terms of $\\alpha$, $p_0$, and $p_1$.",
            "solution": "We are asked to find the posterior probability that the transmitted bit was a $0$ given that the received symbol was an erasure. This can be written as $P(X=0 | Y='?')$.\n\nTo solve this, we apply Bayes' rule, which states:\n$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n\nIn the context of our problem, let event $A$ be $X=0$ (the stored bit is 0) and event $B$ be $Y='?'$ (an erasure is observed). Substituting these into Bayes' rule, we get:\n$$ P(X=0 | Y='?') = \\frac{P(Y='?' | X=0) \\cdot P(X=0)}{P(Y='?')} $$\n\nThe problem provides the following values:\n- $P(X=0) = \\alpha$\n- $P(Y='?' | X=0) = p_0$\n\nThe only term we need to calculate is the total probability of observing an erasure, $P(Y='?')$. We can find this using the law of total probability, summing over all possible inputs for $X$:\n$$ P(Y='?') = P(Y='?' | X=0) \\cdot P(X=0) + P(Y='?' | X=1) \\cdot P(X=1) $$\n\nWe are given $P(X=0) = \\alpha$, which implies that the probability of the stored bit being $1$ is $P(X=1) = 1 - P(X=0) = 1 - \\alpha$. We are also given the conditional probability $P(Y='?' | X=1) = p_1$.\n\nNow, we can substitute all the known probabilities into the expression for $P(Y='?')$:\n$$ P(Y='?') = (p_0) \\cdot (\\alpha) + (p_1) \\cdot (1 - \\alpha) $$\n$$ P(Y='?') = \\alpha p_0 + p_1(1-\\alpha) $$\n\nWith the expression for $P(Y='?')$, we can now complete the calculation for our target posterior probability using Bayes' rule:\n$$ P(X=0 | Y='?') = \\frac{P(Y='?' | X=0) \\cdot P(X=0)}{P(Y='?')} $$\n$$ P(X=0 | Y='?') = \\frac{p_0 \\cdot \\alpha}{\\alpha p_0 + p_1(1-\\alpha)} $$\n\nThis is the final expression for the posterior probability that the stored bit was a $0$ given that an erasure was observed.",
            "answer": "$$\\boxed{\\frac{\\alpha p_{0}}{\\alpha p_{0} + p_{1}(1-\\alpha)}}$$"
        },
        {
            "introduction": "With the mechanics in place, we now turn to a classic and critically important application of Bayesian reasoning: interpreting diagnostic tests. This practice problem  delves into why a test's Positive Predictive Value (PPV) is not a fixed property, but is instead highly dependent on the prevalence of a condition. By exploring a realistic medical screening scenario, you will uncover the base-rate fallacy and solidify your understanding of how prior probabilities can dramatically alter our conclusions.",
            "id": "2523977",
            "problem": "A hospital laboratory is evaluating a new polymerase chain reaction (PCR) assay to screen for carbapenem-resistant Enterobacterales (CRE). The assay has empirically validated sensitivity and specificity, which are assumed constant across settings. The infection control team plans to deploy the same assay in two distinct settings: a low-prevalence community screening program and a high-prevalence outbreak ward. Your goal is to decide, using only core definitions and Bayes’ theorem, which statements must be true and thereby explain why positive predictive value and negative predictive value are not intrinsic properties of the test.\n\nFoundational base to use:\n- Sensitivity is $P(T^{+}\\mid D)$.\n- Specificity is $P(T^{-}\\mid \\bar D)$.\n- Prevalence is $P(D)$, denote it by $\\pi$.\n- Positive predictive value (PPV) is $P(D\\mid T^{+})$.\n- Negative predictive value (NPV) is $P(\\bar D\\mid T^{-})$.\n- Bayes’ theorem relates $P(D\\mid T^{+})$ to $P(T^{+}\\mid D)$, $P(D)$, and $P(T^{+})$.\n\nAssume the following empirically established test characteristics are the same in both settings: sensitivity $Se = 0.90$ and specificity $Sp = 0.995$. The community program screens an asymptomatic population with prevalence $\\pi = 0.0005$ (that is, $0.05\\%$). The outbreak ward screens a high-risk population with prevalence $\\pi = 0.20$ (that is, $20\\%$).\n\nWhich of the following statements are correct?\n\nA. Because sensitivity and specificity are intrinsic to the assay and do not change across settings, the positive predictive value equals sensitivity whenever specificity is high, and therefore PPV is independent of prevalence.\n\nB. Using Bayes’ theorem with pretest probability equal to prevalence $\\pi$, the positive predictive value satisfies $PPV = \\dfrac{Se\\cdot \\pi}{Se\\cdot \\pi + (1-Sp)\\cdot (1-\\pi)}$ and the negative predictive value satisfies $NPV = \\dfrac{Sp\\cdot (1-\\pi)}{Sp\\cdot (1-\\pi) + (1-Se)\\cdot \\pi}$, making both PPV and NPV explicit functions of $\\pi$.\n\nC. With $Se=0.90$ and $Sp=0.995$, in the community program with $\\pi=0.0005$, $PPV \\approx 0.083$ (about $8.3\\%$), whereas in the outbreak ward with $\\pi=0.20$, $PPV \\approx 0.978$ (about $97.8\\%$); this difference arises solely from the different prevalence.\n\nD. As prevalence decreases toward $0$, the negative predictive value approaches sensitivity, because negative results are dominated by true negatives determined by $Se$.\n\nE. Holding $Se$ and $Sp$ fixed, as $\\pi \\to 0$ one has $PPV \\to 0$ and $NPV \\to 1$, and as $\\pi \\to 1$ one has $PPV \\to 1$ and $NPV \\to 0$; thus PPV and NPV are not intrinsic properties of the test.\n\nF. Because the (positive and negative) likelihood ratios (likelihood ratio, LR) are intrinsic to the test, both PPV and NPV are intrinsic and independent of prevalence.\n\nG. The base-rate fallacy in screening arises from conflating $P(T^{+}\\mid D)$ with $P(D\\mid T^{+})$; at very low $\\pi$, even high $Sp$ and $Se$ can yield a low PPV because false positives from the large pool of non-diseased individuals dominate true positives.",
            "solution": "The problem statement is scientifically sound and well-posed. It provides clear definitions and all necessary data to evaluate the claims. It describes a realistic scenario in clinical microbiology and hinges on the correct application of probability theory, specifically Bayes' theorem, to diagnostic testing. Therefore, we proceed with the derivation and analysis.\n\nLet $D$ be the event that a subject has the disease (is a CRE carrier) and $\\bar D$ be the event that the subject does not have the disease. Let $T^{+}$ be the event of a positive test result and $T^{-}$ be the event of a negative test result.\n\nThe provided definitions are:\n- Sensitivity: $Se = P(T^{+} \\mid D)$\n- Specificity: $Sp = P(T^{-} \\mid \\bar D)$\n- Prevalence: $\\pi = P(D)$\n- Positive Predictive Value (PPV): $P(D \\mid T^{+})$\n- Negative Predictive Value (NPV): $P(\\bar D \\mid T^{-})$\n\nThe problem asks to determine which statements are correct based on these definitions, Bayes' theorem, and the provided numerical values: $Se = 0.90$, $Sp = 0.995$, and two prevalence values, $\\pi_1 = 0.0005$ and $\\pi_2 = 0.20$.\n\nOur first task is to derive the explicit expressions for PPV and NPV as functions of $Se$, $Sp$, and $\\pi$.\n\n**Derivation of Positive Predictive Value (PPV)**\nBy definition, $PPV = P(D \\mid T^{+})$. Using Bayes' theorem:\n$$PPV = \\frac{P(T^{+} \\mid D) P(D)}{P(T^{+})}$$\nThe total probability of a positive test, $P(T^{+})$, is given by the law of total probability:\n$$P(T^{+}) = P(T^{+} \\mid D) P(D) + P(T^{+} \\mid \\bar D) P(\\bar D)$$\nWe substitute the given definitions and relations: $P(T^{+} \\mid D) = Se$, $P(D) = \\pi$, $P(\\bar D) = 1 - \\pi$, and $P(T^{+} \\mid \\bar D) = 1 - P(T^{-} \\mid \\bar D) = 1 - Sp$.\n$$P(T^{+}) = Se \\cdot \\pi + (1 - Sp)(1 - \\pi)$$\nSubstituting this into the Bayes' theorem expression for PPV, we obtain:\n$$PPV = \\frac{Se \\cdot \\pi}{Se \\cdot \\pi + (1 - Sp)(1 - \\pi)}$$\n\n**Derivation of Negative Predictive Value (NPV)**\nBy definition, $NPV = P(\\bar D \\mid T^{-})$. Using Bayes' theorem:\n$$NPV = \\frac{P(T^{-} \\mid \\bar D) P(\\bar D)}{P(T^{-})}$$\nThe total probability of a negative test, $P(T^{-})$, is:\n$$P(T^{-}) = P(T^{-} \\mid \\bar D) P(\\bar D) + P(T^{-} \\mid D) P(D)$$\nWe substitute the given definitions and relations: $P(T^{-} \\mid \\bar D) = Sp$, $P(\\bar D) = 1 - \\pi$, $P(D) = \\pi$, and $P(T^{-} \\mid D) = 1 - P(T^{+} \\mid D) = 1 - Se$.\n$$P(T^{-}) = Sp \\cdot (1 - \\pi) + (1 - Se) \\cdot \\pi$$\nSubstituting this into the Bayes' theorem expression for NPV, we obtain:\n$$NPV = \\frac{Sp \\cdot (1 - \\pi)}{Sp \\cdot (1 - \\pi) + (1 - Se) \\cdot \\pi}$$\n\nThese derivations confirm that both PPV and NPV are explicit functions of prevalence, $\\pi$. Now we evaluate each statement.\n\n**A. Because sensitivity and specificity are intrinsic to the assay and do not change across settings, the positive predictive value equals sensitivity whenever specificity is high, and therefore PPV is independent of prevalence.**\nThis statement is fundamentally flawed. PPV, $P(D \\mid T^{+})$, and sensitivity, $P(T^{+} \\mid D)$, are probabilities of different events. Conflating them is a logical error known as the base-rate fallacy. The derived formula for PPV clearly shows its dependence on prevalence $\\pi$. The claim that $PPV$ equals $Se$ is false. For example, if we assume a perfect test where $Se=1$ and $Sp=1$, the formula gives $PPV = \\frac{1 \\cdot \\pi}{1 \\cdot \\pi + 0 \\cdot (1-\\pi)} = 1$, which is not necessarily equal to $Se$. The entire statement is incorrect.\n**Verdict: Incorrect.**\n\n**B. Using Bayes’ theorem with pretest probability equal to prevalence $\\pi$, the positive predictive value satisfies $PPV = \\dfrac{Se\\cdot \\pi}{Se\\cdot \\pi + (1-Sp)\\cdot (1-\\pi)}$ and the negative predictive value satisfies $NPV = \\dfrac{Sp\\cdot (1-\\pi)}{Sp\\cdot (1-\\pi) + (1-Se)\\cdot \\pi}$, making both PPV and NPV explicit functions of $\\pi$.**\nThe formulas presented in this statement are identical to those we derived from first principles using Bayes' theorem. The conclusion that both PPV and NPV are explicit functions of $\\pi$ is a direct consequence of these formulas. This statement is a correct representation of the mathematical relationships.\n**Verdict: Correct.**\n\n**C. With $Se=0.90$ and $Sp=0.995$, in the community program with $\\pi=0.0005$, $PPV \\approx 0.083$ (about $8.3\\%$), whereas in the outbreak ward with $\\pi=0.20$, $PPV \\approx 0.978$ (about $97.8\\%$); this difference arises solely from the different prevalence.**\nWe must perform the calculations using the derived PPV formula and the provided data.\nFor the community program with $\\pi_1 = 0.0005$:\n$$PPV_1 = \\frac{0.90 \\cdot 0.0005}{0.90 \\cdot 0.0005 + (1 - 0.995)(1 - 0.0005)} = \\frac{0.00045}{0.00045 + (0.005)(0.9995)} = \\frac{0.00045}{0.00045 + 0.0049975} = \\frac{0.00045}{0.0054475} \\approx 0.082606...$$\nThis value is approximately $0.083$, or $8.3\\%$. The calculation is correct.\nFor the outbreak ward with $\\pi_2 = 0.20$:\n$$PPV_2 = \\frac{0.90 \\cdot 0.20}{0.90 \\cdot 0.20 + (1 - 0.995)(1 - 0.20)} = \\frac{0.18}{0.18 + (0.005)(0.80)} = \\frac{0.18}{0.18 + 0.004} = \\frac{0.18}{0.184} \\approx 0.97826...$$\nThis value is approximately $0.978$, or $97.8\\%$. The calculation is also correct.\nSince $Se$ and $Sp$ were held constant, the dramatic difference in PPV is indeed caused solely by the change in prevalence $\\pi$. The statement is correct.\n**Verdict: Correct.**\n\n**D. As prevalence decreases toward $0$, the negative predictive value approaches sensitivity, because negative results are dominated by true negatives determined by $Se$.**\nWe analyze the limit of NPV as $\\pi \\to 0$:\n$$\\lim_{\\pi \\to 0} NPV = \\lim_{\\pi \\to 0} \\frac{Sp \\cdot (1 - \\pi)}{Sp \\cdot (1 - \\pi) + (1 - Se) \\cdot \\pi} = \\frac{Sp \\cdot (1 - 0)}{Sp \\cdot (1 - 0) + (1 - Se) \\cdot 0} = \\frac{Sp}{Sp} = 1$$\nThe NPV approaches $1$ (or $100\\%$), not sensitivity ($Se$). Additionally, the reasoning is incorrect: true negatives are related to specificity ($Sp$, via $P(T^{-} \\mid \\bar D)$), not sensitivity ($Se$, via $P(T^{+} \\mid D)$). The statement is incorrect on both its claim and its justification.\n**Verdict: Incorrect.**\n\n**E. Holding $Se$ and $Sp$ fixed, as $\\pi \\to 0$ one has $PPV \\to 0$ and $NPV \\to 1$, and as $\\pi \\to 1$ one has $PPV \\to 1$ and $NPV \\to 0$; thus PPV and NPV are not intrinsic properties of the test.**\nWe evaluate the stated limits.\nAs $\\pi \\to 0$:\n$$ \\lim_{\\pi \\to 0} PPV = \\lim_{\\pi \\to 0} \\frac{Se \\cdot \\pi}{Se \\cdot \\pi + (1 - Sp)(1 - \\pi)} = \\frac{Se \\cdot 0}{Se \\cdot 0 + (1 - Sp) \\cdot 1} = 0 $$\n$$ \\lim_{\\pi \\to 0} NPV = \\lim_{\\pi \\to 0} \\frac{Sp \\cdot (1 - \\pi)}{Sp \\cdot (1 - \\pi) + (1 - Se) \\cdot \\pi} = \\frac{Sp \\cdot 1}{Sp \\cdot 1 + (1 - Se) \\cdot 0} = 1 $$\nAs $\\pi \\to 1$:\n$$ \\lim_{\\pi \\to 1} PPV = \\lim_{\\pi \\to 1} \\frac{Se \\cdot \\pi}{Se \\cdot \\pi + (1 - Sp)(1 - \\pi)} = \\frac{Se \\cdot 1}{Se \\cdot 1 + (1 - Sp) \\cdot 0} = 1 $$\n$$ \\lim_{\\pi \\to 1} NPV = \\lim_{\\pi \\to 1} \\frac{Sp \\cdot (1 - \\pi)}{Sp \\cdot (1 - \\pi) + (1 - Se) \\cdot \\pi} = \\frac{Sp \\cdot 0}{Sp \\cdot 0 + (1 - Se) \\cdot 1} = 0 $$\nAll four limits are correctly stated (assuming an imperfect test where $Se < 1$ and $Sp < 1$). Since PPV and NPV vary dramatically with prevalence $\\pi$, they are not intrinsic properties of the test itself but rather are properties of the test applied to a specific population. The conclusion follows directly from the analysis.\n**Verdict: Correct.**\n\n**F. Because the (positive and negative) likelihood ratios (likelihood ratio, LR) are intrinsic to the test, both PPV and NPV are intrinsic and independent of prevalence.**\nThe likelihood ratios are defined as $LR^{+} = \\frac{Se}{1-Sp}$ and $LR^{-} = \\frac{1-Se}{Sp}$. Since $Se$ and $Sp$ are intrinsic, so are $LR^{+}$ and $LR^{-}$. However, the relationship between predictive values and LRs involves the pre-test odds of disease, $\\frac{\\pi}{1-\\pi}$. Specifically, the post-test odds are the pre-test odds multiplied by the likelihood ratio. For PPV:\n$$\\frac{PPV}{1-PPV} = \\frac{\\pi}{1-\\pi} \\cdot LR^{+}$$\nThis equation demonstrates that PPV is a function of both the intrinsic $LR^{+}$ and the extrinsic prevalence $\\pi$. The argument that intrinsic LRs imply intrinsic predictive values is a non sequitur and is demonstrably false.\n**Verdict: Incorrect.**\n\n**G. The base-rate fallacy in screening arises from conflating $P(T^{+}\\mid D)$ with $P(D\\mid T^{+})$; at very low $\\pi$, even high $Sp$ and $Se$ can yield a low PPV because false positives from the large pool of non-diseased individuals dominate true positives.**\nThis statement provides a correct definition of the base-rate fallacy in this context: confusing sensitivity with PPV. It then provides a clear and correct mechanistic explanation for the phenomenon. The number of true positives is proportional to $\\pi \\cdot Se$, while the number of false positives is proportional to $(1-\\pi)(1-Sp)$. When $\\pi$ is very low, the population of non-diseased individuals $(1-\\pi)$ is very large. Consequently, even a small false positive rate $(1-Sp)$ can generate a number of false positives that is large relative to, or even exceeds, the number of true positives. Our calculation in C for the low-prevalence setting numerically confirms this: out of $5447.5$ expected positive tests per million people screened, $4997.5$ are false positives and only $450$ are true positives. The statement is entirely correct.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{BCEG}$$"
        },
        {
            "introduction": "Having explored the foundational mechanics and the conceptual weight of priors, we now connect Bayes' rule to the inner workings of a machine learning model. This problem  uses the framework of Linear Discriminant Analysis (LDA) to demonstrate how prior probabilities have a direct, geometric impact on classification. You will derive precisely how an error in estimating the priors shifts the decision boundary, making the abstract role of the prior tangible and quantifiable.",
            "id": "3139733",
            "problem": "Consider a binary classification problem under the Linear Discriminant Analysis (LDA) generative model, where the feature is one-dimensional. Assume the class-conditional densities are Gaussian with a common variance: for class labels $Y \\in \\{0,1\\}$ and feature $x \\in \\mathbb{R}$,\n$$\np(x \\mid Y=k) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(x - \\mu_{k})^{2}}{2\\sigma^{2}}\\right), \\quad k \\in \\{0,1\\},\n$$\nwith $\\mu_{0} \\neq \\mu_{1}$ and $\\sigma^{2} > 0$. Let the true prior probabilities be $\\pi_{0}$ and $\\pi_{1}$, and suppose that in training you estimate priors $\\hat{\\pi}_{0}$ and $\\hat{\\pi}_{1}$ that are mis-specified. Classification is performed by the Bayes decision rule: assign $x$ to class $1$ if the log posterior odds $\\ln\\!\\big(P(Y=1 \\mid x)/P(Y=0 \\mid x)\\big)$ is nonnegative, and to class $0$ otherwise. In practice, the log posterior odds are computed using the estimated priors $\\hat{\\pi}_{k}$.\n\nStarting from Bayes’ theorem and the Gaussian likelihoods above, derive a closed-form analytic expression for the shift in the location of the decision boundary on the real line caused by replacing the true priors $\\pi_{k}$ with the estimated priors $\\hat{\\pi}_{k}$. Express your final answer as a function of\n$$\n\\Delta \\equiv \\ln\\!\\left(\\frac{\\hat{\\pi}_{1}}{\\hat{\\pi}_{0}}\\right) - \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right), \\quad \\sigma^{2}, \\quad \\mu_{0}, \\quad \\mu_{1}.\n$$\nYour answer must be a single closed-form expression for the signed shift (new boundary minus true boundary). Do not provide derivations or intermediate steps in your final answer. No rounding is required.",
            "solution": "The problem is well-posed and scientifically grounded within the framework of statistical learning theory. All necessary parameters and conditions for deriving the shift in the decision boundary are provided. I will proceed with the solution.\n\nThe objective is to find the shift in the decision boundary, defined as $x_{\\text{new}} - x_{\\text{true}}$, where $x_{\\text{true}}$ is the boundary computed with the true priors $\\pi_k$ and $x_{\\text{new}}$ is the boundary computed with the estimated priors $\\hat{\\pi}_k$.\n\nThe Bayes decision rule assigns a feature $x$ to class $1$ if the log posterior odds are non-negative. The decision boundary is the point $x$ where the log posterior odds are exactly zero.\n$$\n\\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right) = 0\n$$\nUsing Bayes' theorem, $P(Y=k \\mid x) = \\frac{p(x \\mid Y=k)P(Y=k)}{p(x)}$, we can express the log posterior odds in terms of the class-conditional likelihoods $p(x \\mid Y=k)$ and the prior probabilities $\\pi_k = P(Y=k)$.\n$$\n\\ln\\left(\\frac{p(x \\mid Y=1)\\pi_1}{p(x \\mid Y=0)\\pi_0}\\right) = \\ln\\left(\\frac{p(x \\mid Y=1)}{p(x \\mid Y=0)}\\right) + \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right) = 0\n$$\nThe problem specifies that the class-conditional densities are Gaussian with a common variance $\\sigma^2$:\n$$\np(x \\mid Y=k) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x - \\mu_{k})^{2}}{2\\sigma^{2}}\\right)\n$$\nThe log-likelihood of each class is:\n$$\n\\ln(p(x \\mid Y=k)) = -\\ln(\\sqrt{2\\pi}\\sigma) - \\frac{(x - \\mu_{k})^{2}}{2\\sigma^{2}}\n$$\nThe log-likelihood ratio is therefore:\n$$\n\\ln\\left(\\frac{p(x \\mid Y=1)}{p(x \\mid Y=0)}\\right) = \\ln(p(x \\mid Y=1)) - \\ln(p(x \\mid Y=0))\n$$\n$$\n= \\left(-\\ln(\\sqrt{2\\pi}\\sigma) - \\frac{(x - \\mu_{1})^{2}}{2\\sigma^{2}}\\right) - \\left(-\\ln(\\sqrt{2\\pi}\\sigma) - \\frac{(x - \\mu_{0})^{2}}{2\\sigma^{2}}\\right)\n$$\n$$\n= \\frac{(x - \\mu_{0})^{2}}{2\\sigma^{2}} - \\frac{(x - \\mu_{1})^{2}}{2\\sigma^{2}}\n$$\nExpanding the squared terms:\n$$\n= \\frac{1}{2\\sigma^{2}} \\left[ (x^2 - 2x\\mu_0 + \\mu_0^2) - (x^2 - 2x\\mu_1 + \\mu_1^2) \\right]\n$$\n$$\n= \\frac{1}{2\\sigma^{2}} \\left[ 2x(\\mu_1 - \\mu_0) + \\mu_0^2 - \\mu_1^2 \\right]\n$$\nThis expression is the linear discriminant function characteristic of LDA. The decision boundary $x_b$ is found by setting the sum of the log-likelihood ratio and the log-prior odds to zero:\n$$\n\\frac{1}{2\\sigma^{2}} \\left[ 2x_b(\\mu_1 - \\mu_0) + \\mu_0^2 - \\mu_1^2 \\right] + \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right) = 0\n$$\nNow, we solve for the generic decision boundary $x_b$:\n$$\n2x_b(\\mu_1 - \\mu_0) + \\mu_0^2 - \\mu_1^2 = -2\\sigma^2 \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right)\n$$\n$$\n2x_b(\\mu_1 - \\mu_0) = \\mu_1^2 - \\mu_0^2 - 2\\sigma^2 \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right)\n$$\n$$\n2x_b(\\mu_1 - \\mu_0) = (\\mu_1 - \\mu_0)(\\mu_1 + \\mu_0) - 2\\sigma^2 \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right)\n$$\nSince it is given that $\\mu_0 \\neq \\mu_1$, we can divide by $2(\\mu_1 - \\mu_0)$:\n$$\nx_b = \\frac{\\mu_1 + \\mu_0}{2} - \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right)\n$$\nThis is the general form of the decision boundary.\n\nFirst, we find the true decision boundary, $x_{\\text{true}}$, by using the true priors $\\pi_0$ and $\\pi_1$:\n$$\nx_{\\text{true}} = \\frac{\\mu_1 + \\mu_0}{2} - \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right)\n$$\nNext, we find the new decision boundary, $x_{\\text{new}}$, which results from using the mis-specified priors $\\hat{\\pi}_0$ and $\\hat{\\pi}_1$:\n$$\nx_{\\text{new}} = \\frac{\\mu_1 + \\mu_0}{2} - \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\hat{\\pi}_1}{\\hat{\\pi}_0}\\right)\n$$\nThe shift in the location of the decision boundary is the difference $x_{\\text{new}} - x_{\\text{true}}$:\n$$\n\\text{Shift} = \\left(\\frac{\\mu_1 + \\mu_0}{2} - \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\hat{\\pi}_1}{\\hat{\\pi}_0}\\right)\\right) - \\left(\\frac{\\mu_1 + \\mu_0}{2} - \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right)\\right)\n$$\nThe term $\\frac{\\mu_1 + \\mu_0}{2}$ cancels out:\n$$\n\\text{Shift} = -\\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\hat{\\pi}_1}{\\hat{\\pi}_0}\\right) + \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right)\n$$\nFactoring out the common term $-\\frac{\\sigma^2}{\\mu_1 - \\mu_0}$:\n$$\n\\text{Shift} = -\\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\left[ \\ln\\left(\\frac{\\hat{\\pi}_1}{\\hat{\\pi}_0}\\right) - \\ln\\left(\\frac{\\pi_1}{\\pi_0}\\right) \\right]\n$$\nThe problem defines the quantity $\\Delta$ as:\n$$\n\\Delta \\equiv \\ln\\left(\\frac{\\hat{\\pi}_{1}}{\\hat{\\pi}_{0}}\\right) - \\ln\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right)\n$$\nSubstituting this definition into our expression for the shift yields the final answer:\n$$\n\\text{Shift} = -\\frac{\\sigma^2 \\Delta}{\\mu_1 - \\mu_0}\n$$\nThis is the closed-form analytic expression for the shift in the decision boundary as a function of the specified quantities $\\Delta$, $\\sigma^2$, $\\mu_0$, and $\\mu_1$.",
            "answer": "$$\n\\boxed{-\\frac{\\sigma^2 \\Delta}{\\mu_1 - \\mu_0}}\n$$"
        }
    ]
}