{
    "hands_on_practices": [
        {
            "introduction": "掌握任何一个数学工具的第一步都是学会如何计算它。这个练习将带你从最基础的定义出发，为一个从二维平面到另一个二维平面的映射计算雅可比矩阵。通过逐一求解偏导数并将其排列成矩阵，你将熟悉雅可比矩阵的结构，并为理解其更深层的应用打下坚实的基础。",
            "id": "2216501",
            "problem": "在计算流体动力学领域，通常使用变换将复杂的物理域映射到更简单的计算网格上。考虑一个从笛卡尔坐标系 $(x,y)$ 到由向量函数 $\\mathbf{F}: \\mathbb{R}^2 \\to \\mathbb{R}^2$ 定义的新坐标系 $(u,v)$ 的坐标变换。该映射由以下分量函数给出：\n$u(x, y) = x^3 - 3xy^2$\n$v(x, y) = 3x^2y - y^3$\n\n在任意点，此变换的局部线性近似由其雅可比矩阵描述，该矩阵是向量函数所有一阶偏导数构成的矩阵。计算此变换的雅可比矩阵 $\\mathbf{J}_{\\mathbf{F}}(x,y)$ 在点 $(x, y) = (2, -1)$ 处的值。",
            "solution": "设 $\\mathbf{F}(x,y) = \\big(u(x,y), v(x,y)\\big)$，其中 $u(x,y) = x^{3} - 3xy^{2}$ 且 $v(x,y) = 3x^{2}y - y^{3}$。雅可比矩阵是一阶偏导数矩阵：\n$$\n\\mathbf{J}_{\\mathbf{F}}(x,y) = \\begin{pmatrix}\n\\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\\n\\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y}\n\\end{pmatrix}.\n$$\n逐一计算每个偏导数：\n$$\n\\frac{\\partial u}{\\partial x} = \\frac{\\partial}{\\partial x}\\left(x^{3} - 3xy^{2}\\right) = 3x^{2} - 3y^{2},\n$$\n$$\n\\frac{\\partial u}{\\partial y} = \\frac{\\partial}{\\partial y}\\left(x^{3} - 3xy^{2}\\right) = -6xy,\n$$\n$$\n\\frac{\\partial v}{\\partial x} = \\frac{\\partial}{\\partial x}\\left(3x^{2}y - y^{3}\\right) = 6xy,\n$$\n$$\n\\frac{\\partial v}{\\partial y} = \\frac{\\partial}{\\partial y}\\left(3x^{2}y - y^{3}\\right) = 3x^{2} - 3y^{2}.\n$$\n因此，\n$$\n\\mathbf{J}_{\\mathbf{F}}(x,y) = \\begin{pmatrix}\n3x^{2} - 3y^{2} & -6xy \\\\\n6xy & 3x^{2} - 3y^{2}\n\\end{pmatrix}.\n$$\n在点 $(x,y) = (2,-1)$ 处求值得到\n$$\n\\mathbf{J}_{\\mathbf{F}}(2,-1) = \\begin{pmatrix}\n3\\cdot 2^{2} - 3\\cdot (-1)^{2} & -6\\cdot 2 \\cdot (-1) \\\\\n6\\cdot 2 \\cdot (-1) & 3\\cdot 2^{2} - 3\\cdot (-1)^{2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n9 & 12 \\\\\n-12 & 9\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}9 & 12 \\\\ -12 & 9\\end{pmatrix}}$$"
        },
        {
            "introduction": "计算出雅可比矩阵后，我们如何运用它呢？这个练习将展示雅可比矩阵的核心应用：作为多变量函数的局部线性近似。通过一个直观的机器人臂场景，你将学会如何使用雅可比矩阵来预测当输入（关节角度）发生微小变化时，输出（末端执行器位置）将如何响应，这正是微分学在工程问题中强大威力的体现。",
            "id": "2325300",
            "problem": "一个带有两个关节的机械臂在二维平面上运行。其末端执行器的笛卡尔坐标 $(x, y)$ 由其两个关节的角度 $(\\theta_1, \\theta_2)$ 决定，这可以由一个函数 $P: \\mathbb{R}^2 \\to \\mathbb{R}^2$ 描述，使得 $(x, y) = P(\\theta_1, \\theta_2)$。角度以弧度为单位，坐标以米为单位。\n\n在关节角度为 $(\\theta_1, \\theta_2) = (\\frac{\\pi}{6}, \\frac{\\pi}{3})$ 的特定位形下，末端执行器位于位置 $(x, y) = (2.50, 1.75)$ 米。在此位形下，末端执行器位置对关节角度微小变化的灵敏度由函数 $P$ 的雅可比矩阵描述：\n$$ J_P\\left(\\frac{\\pi}{6}, \\frac{\\pi}{3}\\right) = \\begin{pmatrix} -1.2 & 0.8 \\\\ 2.0 & 1.5 \\end{pmatrix} $$\n雅可比矩阵中元素的单位是米每弧度。\n\n假设控制系统对关节角度引入一个小的扰动，将其移动到一个新的位形 $(\\theta_1', \\theta_2') = (\\frac{\\pi}{6} + 0.02, \\frac{\\pi}{3} - 0.04)$。\n\n使用线性近似，估计末端执行器的新笛卡尔坐标 $(x', y')$。将您的答案表示为一对坐标。将每个坐标四舍五入到三位有效数字。最终答案应以米为单位给出。",
            "solution": "我们使用函数 $P$ 在位形 $(\\theta_{1},\\theta_{2})$ 附近的一阶（线性）近似：\n$$\n\\begin{pmatrix} x' \\\\ y' \\end{pmatrix} \\approx \\begin{pmatrix} x \\\\ y \\end{pmatrix} + J_{P}(\\theta_{1},\\theta_{2}) \\begin{pmatrix} \\Delta \\theta_{1} \\\\ \\Delta \\theta_{2} \\end{pmatrix},\n$$\n其中 $\\Delta \\theta_{1} = \\theta_{1}' - \\theta_{1}$ 且 $\\Delta \\theta_{2} = \\theta_{2}' - \\theta_{2}$。在给定位形下，$(x,y) = (2.50,1.75)$ 米，并且\n$$\nJ_{P}\\left(\\frac{\\pi}{6},\\frac{\\pi}{3}\\right) = \\begin{pmatrix} -1.2 & 0.8 \\\\ 2.0 & 1.5 \\end{pmatrix}.\n$$\n关节角度的扰动为\n$$\n\\Delta \\theta_{1} = 0.02, \\quad \\Delta \\theta_{2} = -0.04.\n$$\n计算位置变化：\n$$\n\\Delta \\mathbf{p} = J_{P} \\begin{pmatrix} 0.02 \\\\ -0.04 \\end{pmatrix} = \\begin{pmatrix} -1.2 & 0.8 \\\\ 2.0 & 1.5 \\end{pmatrix} \\begin{pmatrix} 0.02 \\\\ -0.04 \\end{pmatrix} = \\begin{pmatrix} -1.2\\cdot 0.02 + 0.8\\cdot(-0.04) \\\\ 2.0\\cdot 0.02 + 1.5\\cdot(-0.04) \\end{pmatrix} = \\begin{pmatrix} -0.056 \\\\ -0.02 \\end{pmatrix}.\n$$\n将此变化加到标称位置上：\n$$\n\\begin{pmatrix} x' \\\\ y' \\end{pmatrix} \\approx \\begin{pmatrix} 2.50 \\\\ 1.75 \\end{pmatrix} + \\begin{pmatrix} -0.056 \\\\ -0.02 \\end{pmatrix} = \\begin{pmatrix} 2.444 \\\\ 1.73 \\end{pmatrix}.\n$$\n将每个坐标四舍五入到三位有效数字，得到 $x' \\approx 2.44$ 和 $y' \\approx 1.73$（米）。",
            "answer": "$$\\boxed{(2.44, 1.73)}$$"
        },
        {
            "introduction": "在深度学习等高维问题中，显式地构建和存储整个雅可比矩阵是不可行的。幸运的是，我们通常只需要雅可比矩阵与某个向量的乘积，即雅可比向量积（JVP）或向量雅可比积（VJP）。这个高级练习将引导你为一个复杂的非线性层推导并实现这些关键操作，让你深入了解现代自动微分框架（如 PyTorch 和 TensorFlow）背后的核心计算机制。",
            "id": "3187081",
            "problem": "给定一个在深度学习架构中经常遇到的自定义非线性层，对于输入向量 $x \\in \\mathbb{R}^n$ 和固定参数，它定义为一个映射 $f : \\mathbb{R}^n \\to \\mathbb{R}^m$。该层的输出为\n$$\ny(x) = f(x) = \\operatorname{softplus}\\!\\big(Wx + b\\big) \\odot \\exp\\!\\big(-\\alpha \\lVert x \\rVert_2^2\\big) \\;+\\; M\\big(x \\odot \\tanh(x)\\big)\n$$\n其中 $W \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，$M \\in \\mathbb{R}^{m \\times n}$，$\\alpha \\in \\mathbb{R}_{>0}$，且 $\\odot$ 表示逐元素乘法。函数 $\\operatorname{softplus}$ 逐元素定义为\n$$\n\\operatorname{softplus}(z) = \\log\\big(1 + e^{z}\\big),\n$$\n且 $\\tanh$ 是逐元素应用的双曲正切函数。\n\n您的任务是：\n1. 从雅可比矩阵和多元链式法则的基本定义出发，推导 $f$ 关于 $x$ 的完整雅可比矩阵 $J(x) \\in \\mathbb{R}^{m \\times n}$。不要从任何现成的快捷公式开始或引用它们；使用第一性原理（导数的定义、链式法则、乘积法则以及基本函数的已知导数）。\n2. 实现与雅可比矩阵相关的两个线性算子，它们在自动微分中至关重要：\n   - 向量-雅可比积 (VJP)：给定一个向量 $u \\in \\mathbb{R}^m$，精确计算 $J(x)^\\top u$，即标量 $u^\\top f(x)$ 关于 $x$ 的梯度。\n   - 雅可比-向量积 (JVP)：给定一个向量 $v \\in \\mathbb{R}^n$，精确计算 $J(x) v$，即 $f$ 在 $x$ 点沿方向 $v$ 的方向导数。\n3. 通过将 VJP 和 JVP 的结果与完整雅可比矩阵的作用进行比较，来验证您实现的正确性。此外，还要通过有限差分进行方向导数检验。\n\n使用以下参数集测试套件，所有随机量均使用指定的种子从按指定比例缩放的标准正态分布中确定性地生成。对于一个种子 $s$，使用可复现的伪随机数生成器生成数组 $W$、$M$、$b$、$x$、$u$、$v$ 作为来自 $\\mathcal{N}(0, 1)$ 的独立样本，并根据下面的案例定义进行缩放。在所有案例中，$W \\in \\mathbb{R}^{m \\times n}$，$M \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，$x \\in \\mathbb{R}^n$，$u \\in \\mathbb{R}^m$，$v \\in \\mathbb{R}^n$。\n\n- 案例 1 (常规成功路径):\n  - $m = 4$，$n = 4$，种子 $s = 0$，$\\alpha = 0.3$。\n  - 缩放因子：$W$ 乘以 $0.5$，$M$ 乘以 $0.3$，$b$ 乘以 $0.1$，$x$ 乘以 $1.0$，$u$ 乘以 $1.0$，$v$ 乘以 $1.0$。\n\n- 案例 2 (零输入边界情况):\n  - $m = 5$，$n = 5$，种子 $s = 1$，$\\alpha = 0.2$。\n  - 缩放因子：$W$ 乘以 $0.4$，$M$ 乘以 $0.3$，$b$ 乘以 $0.05$，$x$ 设置为 $\\mathbb{R}^n$ 中的零向量 (忽略随机生成的 $x$)，$u$ 乘以 $1.0$，$v$ 乘以 $1.0$。\n\n- 案例 3 (维度不匹配 $m \\neq n$):\n  - $m = 3$，$n = 5$，种子 $s = 2$，$\\alpha = 0.05$。\n  - 缩放因子：$W$ 乘以 $0.7$，$M$ 乘以 $0.6$，$b$ 乘以 $0.2$，$x$ 乘以 $0.8$，$u$ 乘以 $0.9$，$v$ 乘以 $1.1$。\n\n- 案例 4 (大输入幅值，小阻尼):\n  - $m = 6$，$n = 6$，种子 $s = 3$，$\\alpha = 0.02$。\n  - 缩放因子：$W$ 乘以 $0.3$，$M$ 乘以 $0.3$，$b$ 乘以 $0.1$，$x$ 乘以 $3.0$，$u$ 乘以 $0.5$，$v$ 乘以 $0.5$。\n\n对于每个案例，计算并报告以下四个实值诊断指标：\n- $r_{\\mathrm{vjp}} = \\lVert \\mathrm{VJP}(u) - J(x)^\\top u \\rVert_2$,\n- $r_{\\mathrm{jvp}} = \\lVert \\mathrm{JVP}(v) - J(x) v \\rVert_2$,\n- $r_{\\mathrm{fd}} = \\lVert f(x + \\varepsilon v) - f(x) - \\varepsilon\\, J(x) v \\rVert_2$ 使用 $\\varepsilon = 10^{-6}$,\n- $r_{\\mathrm{scalar}} = \\left| u^\\top (J(x) v) - v^\\top (J(x)^\\top u) \\right|$.\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，结果的顺序为案例 1 到 4，每个案例贡献四个诊断指标，顺序为 $[r_{\\mathrm{vjp}}, r_{\\mathrm{jvp}}, r_{\\mathrm{fd}}, r_{\\mathrm{scalar}}]$。将每个浮点数以科学记数法表示，并保留小数点后六位数字。例如，单个案例的输出格式如下 $[1.234567\\text{e-}08,2.345678\\text{e-}09,3.456789\\text{e-}10,4.567890\\text{e-}11]$，总输出应将所有案例的结果相应地连接成一个列表。",
            "solution": "此问题是有效的。我们的任务是分析由下式定义的自定义非线性层 $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ 的微分性质\n$$\nf(x) = \\operatorname{softplus}\\!\\big(Wx + b\\big) \\odot \\exp\\!\\big(-\\alpha \\lVert x \\rVert_2^2\\big) \\;+\\; M\\big(x \\odot \\tanh(x)\\big)\n$$\n其中 $x \\in \\mathbb{R}^n$是输入，$W \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，$M \\in \\mathbb{R}^{m \\times n}$ 和 $\\alpha \\in \\mathbb{R}_{>0}$ 是固定参数。符号 $\\odot$ 表示逐元素乘法。\n\n$f(x)$ 的总雅可比矩阵可以通过利用微分算子的线性性质来求得。我们将 $f(x)$ 分解为两部分：\n$f_1(x) = \\operatorname{softplus}\\!\\big(Wx + b\\big) \\odot \\exp\\!\\big(-\\alpha \\lVert x \\rVert_2^2\\big)$\n$f_2(x) = M\\big(x \\odot \\tanh(x)\\big)$\n那么，$f(x) = f_1(x) + f_2(x)$，其雅可比矩阵 $J(x) \\in \\mathbb{R}^{m \\times n}$ 是 $f_1(x)$ 和 $f_2(x)$ 的雅可比矩阵之和：\n$$\nJ(x) = \\frac{\\partial f}{\\partial x} = \\frac{\\partial f_1}{\\partial x} + \\frac{\\partial f_2}{\\partial x} = J_1(x) + J_2(x)\n$$\n我们将分别从第一性原理推导 $J_1(x)$ 和 $J_2(x)$。\n\n**1. 雅可比矩阵 $J_1(x)$ 的推导**\n\n函数 $f_1(x)$ 是一个向量值函数 $p(x) = \\operatorname{softplus}(Wx+b)$ 和一个标量值函数 $s(x) = \\exp(-\\alpha \\lVert x \\rVert_2^2)$ 的乘积。所以，$f_1(x) = p(x) s(x)$。\n\n雅可比矩阵 $J_1(x)$ 的第 $(i, j)$ 个元素由 $f_1(x)$ 的第 $i$ 个分量对 $x$ 的第 $j$ 个分量的偏导数给出：\n$$\n[J_1(x)]_{ij} = \\frac{\\partial [f_1(x)]_i}{\\partial x_j} = \\frac{\\partial (p_i(x) s(x))}{\\partial x_j}\n$$\n使用微分的乘积法则，我们得到：\n$$\n[J_1(x)]_{ij} = \\frac{\\partial p_i(x)}{\\partial x_j} s(x) + p_i(x) \\frac{\\partial s(x)}{\\partial x_j}\n$$\n我们需要求出 $p_i(x)$ 和 $s(x)$ 的导数。\n\n首先，对于标量函数 $s(x) = \\exp(-\\alpha \\sum_{k=1}^n x_k^2)$：\n$$\n\\frac{\\partial s(x)}{\\partial x_j} = \\exp\\left(-\\alpha \\sum_{k=1}^n x_k^2\\right) \\cdot \\frac{\\partial}{\\partial x_j}\\left(-\\alpha \\sum_{k=1}^n x_k^2\\right) = s(x) \\cdot (-2\\alpha x_j)\n$$\n$s(x)$ 的梯度因此是 $\\nabla_x s(x) = -2\\alpha s(x) x$。\n\n接着，对于向量函数 $p(x) = \\operatorname{softplus}(Wx+b)$。令 $a(x) = Wx+b$。那么 $p_i(x) = \\operatorname{softplus}(a_i(x))$。使用链式法则：\n$$\n\\frac{\\partial p_i(x)}{\\partial x_j} = \\frac{d\\operatorname{softplus}}{dz}\\bigg|_{z=a_i(x)} \\cdot \\frac{\\partial a_i(x)}{\\partial x_j}\n$$\n$\\operatorname{softplus}(z) = \\log(1+e^z)$ 的导数是 $\\frac{e^z}{1+e^z}$，即逻辑S型函数 (logistic sigmoid function)，$\\sigma(z)$。\n项 $a_i(x) = \\sum_{k=1}^n W_{ik}x_k + b_i$。其偏导数是 $\\frac{\\partial a_i(x)}{\\partial x_j} = W_{ij}$。\n所以，$\\frac{\\partial p_i(x)}{\\partial x_j} = \\sigma(a_i(x)) W_{ij}$。以矩阵形式表示，$p(x)$ 的雅可比矩阵是 $J_p(x) = \\operatorname{diag}(\\sigma(Wx+b))W$。\n\n结合这些结果：\n$$\n[J_1(x)]_{ij} = (\\sigma(a_i(x))W_{ij}) s(x) + p_i(x) (-2\\alpha s(x) x_j) = s(x) \\left( \\sigma(a_i(x))W_{ij} - 2\\alpha p_i(x) x_j \\right)\n$$\n这可以表示为矩阵形式。项 $\\sigma(a_i(x))W_{ij}$ 对应于 $\\operatorname{diag}(\\sigma(Wx+b))W$ 的第 $(i, j)$ 个元素。项 $-2\\alpha p_i(x) x_j$ 对应于外积 $-2\\alpha p(x) x^\\top$ 的第 $(i, j)$ 个元素。\n因此，雅可比矩阵 $J_1(x)$ 为：\n$$\nJ_1(x) = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( \\operatorname{diag}\\big(\\sigma(Wx+b)\\big) W - 2\\alpha \\operatorname{softplus}(Wx+b) x^\\top \\right)\n$$\n\n**2. 雅可比矩阵 $J_2(x)$ 的推导**\n\n函数 $f_2(x) = M(x \\odot \\tanh(x))$。令 $g(x) = x \\odot \\tanh(x)$。那么 $f_2(x) = M g(x)$。\n使用多元链式法则，$J_2(x) = J_{h}(g(x)) \\cdot J_g(x)$，其中 $h(g) = Mg$。函数 $h$ 是一个线性变换，所以其雅可比矩阵就是矩阵 $M$。\n$$\nJ_2(x) = M J_g(x)\n$$\n函数 $g(x)$ 是逐元素运算，因此其雅可比矩阵 $J_g(x)$ 是一个对角矩阵。对角线上的元素是其分量 $g_k(x) = x_k \\tanh(x_k)$ 的导数。对于 $k=j$：\n$$\n\\frac{d g_j(x)}{d x_j} = \\frac{d}{dx_j}(x_j \\tanh(x_j)) = 1 \\cdot \\tanh(x_j) + x_j \\cdot \\frac{d}{dx_j}\\tanh(x_j)\n$$\n$\\tanh(z)$ 的导数是 $1 - \\tanh^2(z)$。所以，\n$$\n\\frac{d g_j(x)}{d x_j} = \\tanh(x_j) + x_j(1 - \\tanh^2(x_j))\n$$\n记这个导数向量为 $g'(x)$。那么 $J_g(x) = \\operatorname{diag}(g'(x))$。$f_2$ 的完整雅可比矩阵为：\n$$\nJ_2(x) = M \\operatorname{diag}\\big(\\tanh(x) + x \\odot (1-\\tanh^2(x))\\big)\n$$\n\n**3. 完整雅可比矩阵及相关线性算子**\n\n将两部分结合起来得到完整的雅可比矩阵 $J(x) = J_1(x) + J_2(x)$。\n\n**雅可比-向量积 (JVP): $J(x)v$**\nJVP 是 $f(x)$ 沿向量 $v \\in \\mathbb{R}^n$ 的方向导数。我们可以不显式地构造矩阵来计算 $J(x)v = J_1(x)v + J_2(x)v$。\n$$\nJ_1(x)v = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( \\operatorname{diag}\\big(\\sigma(Wx+b)\\big) Wv - 2\\alpha \\operatorname{softplus}(Wx+b) (x^\\top v) \\right)\n$$\n注意到 $\\operatorname{diag}(z)y = z \\odot y$ 且 $x^\\top v$ 是一个标量点积：\n$$\nJ_1(x)v = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( \\sigma(Wx+b) \\odot (Wv) - (2\\alpha (x^\\top v)) \\operatorname{softplus}(Wx+b) \\right)\n$$\n对于第二项：\n$$\nJ_2(x)v = M \\operatorname{diag}(g'(x)) v = M (g'(x) \\odot v)\n$$\n其中 $g'(x) = \\tanh(x) + x \\odot (1-\\tanh^2(x))$。\n\n**向量-雅可比积 (VJP): $J(x)^\\top u$**\nVJP 计算转置雅可比矩阵对向量 $u \\in \\mathbb{R}^m$ 的作用。它对应于标量投影 $u^\\top f(x)$ 的梯度。我们计算 $J(x)^\\top u = J_1(x)^\\top u + J_2(x)^\\top u$。\n$$\nJ_1(x)^\\top = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( (\\operatorname{diag}(\\sigma)W)^\\top - (2\\alpha p x^\\top)^\\top \\right)\n$$\n$$\nJ_1(x)^\\top = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( W^\\top \\operatorname{diag}(\\sigma) - 2\\alpha x p^\\top \\right)\n$$\n将此应用于 $u$：\n$$\nJ_1(x)^\\top u = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( W^\\top (\\sigma(Wx+b) \\odot u) - (2\\alpha (\\operatorname{softplus}(Wx+b)^\\top u)) x \\right)\n$$\n对于第二项：\n$$\nJ_2(x)^\\top u = \\left(M \\operatorname{diag}(g'(x))\\right)^\\top u = \\operatorname{diag}(g'(x)) M^\\top u = g'(x) \\odot (M^\\top u)\n$$\n这些表达式允许高效地、无矩阵地计算 VJP 和 JVP。\n\n**4. 验证诊断指标**\n数值诊断指标按如下方式计算：\n- $r_{\\mathrm{vjp}} = \\lVert \\mathrm{VJP}(u) - J(x)^\\top u \\rVert_2$: 这衡量了无矩阵的 VJP 实现与使用计算出的雅可比矩阵的显式 VJP 之间的一致性。预期结果接近机器精度。\n- $r_{\\mathrm{jvp}} = \\lVert \\mathrm{JVP}(v) - J(x) v \\rVert_2$: 类似地，这衡量了 JVP 的一致性。预期结果接近机器精度。\n- $r_{\\mathrm{fd}} = \\lVert f(x + \\varepsilon v) - f(x) - \\varepsilon\\, J(x) v \\rVert_2$: 这将 JVP（真实的方向导数）与有限差分近似进行比较。根据泰勒定理，这个误差是 $O(\\varepsilon^2)$ 阶的，因此对于 $\\varepsilon = 10^{-6}$，结果应该很小（大约 $10^{-12}$）。\n- $r_{\\mathrm{scalar}} = \\left| u^\\top (J(x) v) - v^\\top (J(x)^\\top u) \\right|$: 这检验了线性算子的基本伴随性质 $\\langle u, Jv \\rangle = \\langle J^\\top u, v \\rangle$。差值应接近机器精度。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving and implementing the Jacobian, JVP, and VJP\n    for the given nonlinear function and verifying them on a test suite.\n    \"\"\"\n    \n    # Helper functions for nonlinearities and their derivatives\n    def softplus(z):\n        \"\"\"Numerically stable softplus function.\"\"\"\n        return np.logaddexp(0, z)\n\n    def sigmoid(z):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-z))\n\n    # Implementation of the function f(x)\n    def compute_f(x, W, b, M, alpha):\n        a = W @ x + b\n        softplus_a = softplus(a)\n        \n        x_norm_sq = np.dot(x, x)\n        exp_term = np.exp(-alpha * x_norm_sq)\n        \n        term1 = softplus_a * exp_term\n        \n        tanh_x = np.tanh(x)\n        x_tanh_x = x * tanh_x\n        term2 = M @ x_tanh_x\n        \n        return term1 + term2\n\n    # Implementation of the full Jacobian matrix J(x)\n    def compute_jacobian(x, W, b, M, alpha):\n        m, n = W.shape\n        \n        a = W @ x + b\n        softplus_a = softplus(a)\n        sigma_a = sigmoid(a)\n        \n        x_norm_sq = np.dot(x, x)\n        exp_term = np.exp(-alpha * x_norm_sq)\n        \n        tanh_x = np.tanh(x)\n        \n        # J1 = exp_term * (diag(sigma(a)) @ W - 2*alpha*outer(softplus(a), x))\n        term1_part1 = np.diag(sigma_a) @ W\n        term1_part2 = -2 * alpha * np.outer(softplus_a, x)\n        J1 = exp_term * (term1_part1 + term1_part2)\n        \n        # J2 = M @ diag(tanh(x) + x * (1 - tanh(x)**2))\n        g_prime_x = tanh_x + x * (1 - tanh_x**2)\n        J2 = M * g_prime_x  # Broadcasting g_prime_x (n,) across rows of M (m, n)\n        \n        return J1 + J2\n\n    # Implementation of the Jacobian-Vector Product (JVP)\n    def compute_jvp(x, v, W, b, M, alpha):\n        a = W @ x + b\n        softplus_a = softplus(a)\n        sigma_a = sigmoid(a)\n        \n        x_norm_sq = np.dot(x, x)\n        exp_term = np.exp(-alpha * x_norm_sq)\n        \n        tanh_x = np.tanh(x)\n        \n        # JVP for term 1\n        W_v = W @ v\n        sigma_a_W_v = sigma_a * W_v\n        x_dot_v = np.dot(x, v)\n        term1_jvp = exp_term * (sigma_a_W_v - (2 * alpha * x_dot_v) * softplus_a)\n        \n        # JVP for term 2\n        g_prime_x = tanh_x + x * (1 - tanh_x**2)\n        g_prime_x_v = g_prime_x * v\n        term2_jvp = M @ g_prime_x_v\n        \n        return term1_jvp + term2_jvp\n\n    # Implementation of the Vector-Jacobian Product (VJP)\n    def compute_vjp(x, u, W, b, M, alpha):\n        a = W @ x + b\n        softplus_a = softplus(a)\n        sigma_a = sigmoid(a)\n        \n        x_norm_sq = np.dot(x, x)\n        exp_term = np.exp(-alpha * x_norm_sq)\n        \n        tanh_x = np.tanh(x)\n        \n        # VJP for term 1\n        sigma_a_u = sigma_a * u\n        WT_sigma_a_u = W.T @ sigma_a_u\n        softplus_a_dot_u = np.dot(softplus_a, u)\n        term1_vjp = exp_term * (WT_sigma_a_u - (2 * alpha * softplus_a_dot_u) * x)\n        \n        # VJP for term 2\n        g_prime_x = tanh_x + x * (1 - tanh_x**2)\n        MT_u = M.T @ u\n        term2_vjp = g_prime_x * MT_u\n        \n        return term1_vjp + term2_vjp\n\n    test_cases = [\n        {'m': 4, 'n': 4, 's': 0, 'alpha': 0.3, 'scales': {'W': 0.5, 'M': 0.3, 'b': 0.1, 'x': 1.0, 'u': 1.0, 'v': 1.0}},\n        {'m': 5, 'n': 5, 's': 1, 'alpha': 0.2, 'scales': {'W': 0.4, 'M': 0.3, 'b': 0.05, 'u': 1.0, 'v': 1.0}, 'x_zero': True},\n        {'m': 3, 'n': 5, 's': 2, 'alpha': 0.05, 'scales': {'W': 0.7, 'M': 0.6, 'b': 0.2, 'x': 0.8, 'u': 0.9, 'v': 1.1}},\n        {'m': 6, 'n': 6, 's': 3, 'alpha': 0.02, 'scales': {'W': 0.3, 'M': 0.3, 'b': 0.1, 'x': 3.0, 'u': 0.5, 'v': 0.5}},\n    ]\n\n    all_diagnostics = []\n    \n    for case in test_cases:\n        m, n, s, alpha = case['m'], case['n'], case['s'], case['alpha']\n        scales = case['scales']\n        rng = np.random.default_rng(s)\n        \n        W = rng.standard_normal((m, n)) * scales['W']\n        M = rng.standard_normal((m, n)) * scales['M']\n        b = rng.standard_normal(m) * scales['b']\n        u = rng.standard_normal(m) * scales['u']\n        v = rng.standard_normal(n) * scales['v']\n        \n        if case.get('x_zero', False):\n            x = np.zeros(n)\n        else:\n            x = rng.standard_normal(n) * scales['x']\n\n        params = (W, b, M, alpha)\n        \n        # 1. Compute full Jacobian matrix\n        j_matrix = compute_jacobian(x, *params)\n        \n        # 2. Compute VJP and JVP directly and via matrix\n        vjp_direct = compute_vjp(x, u, *params)\n        vjp_matrix = j_matrix.T @ u\n        \n        jvp_direct = compute_jvp(x, v, *params)\n        jvp_matrix = j_matrix @ v\n        \n        # 3. Calculate diagnostics\n        r_vjp = np.linalg.norm(vjp_direct - vjp_matrix)\n        r_jvp = np.linalg.norm(jvp_direct - jvp_matrix)\n        \n        eps = 1e-6\n        f_x_plus_eps_v = compute_f(x + eps * v, *params)\n        f_x = compute_f(x, *params)\n        r_fd = np.linalg.norm(f_x_plus_eps_v - f_x - eps * jvp_matrix)\n        \n        r_scalar = np.abs(np.dot(u, jvp_matrix) - np.dot(v, vjp_matrix))\n        \n        all_diagnostics.extend([r_vjp, r_jvp, r_fd, r_scalar])\n\n    # Format and print the final results\n    formatted_results = [f\"{val:.6e}\" for val in all_diagnostics]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}