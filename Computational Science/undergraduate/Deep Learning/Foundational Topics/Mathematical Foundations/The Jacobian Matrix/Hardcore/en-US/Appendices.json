{
    "hands_on_practices": [
        {
            "introduction": "Mastering the concept of the Jacobian begins with the fundamental skill of its computation. This exercise provides direct practice in constructing the Jacobian matrix for a given vector function, which involves calculating all the first-order partial derivatives and arranging them correctly. Understanding this mechanical process is the essential first step before exploring the Jacobian's powerful applications in approximation and analysis. ",
            "id": "2216501",
            "problem": "In the field of computational fluid dynamics, transformations are often used to map a complex physical domain onto a simpler computational grid. Consider a coordinate transformation from a Cartesian system $(x,y)$ to a new system $(u,v)$ defined by the vector function $\\mathbf{F}: \\mathbb{R}^2 \\to \\mathbb{R}^2$. The mapping is given by the component functions:\n$u(x, y) = x^3 - 3xy^2$\n$v(x, y) = 3x^2y - y^3$\n\nThe local linear approximation of this transformation at any point is described by its Jacobian matrix, which is the matrix of all first-order partial derivatives of the vector function. Calculate the Jacobian matrix of this transformation, $\\mathbf{J}_{\\mathbf{F}}(x,y)$, evaluated at the point $(x, y) = (2, -1)$.",
            "solution": "Let $\\mathbf{F}(x,y) = \\big(u(x,y), v(x,y)\\big)$ with $u(x,y) = x^{3} - 3xy^{2}$ and $v(x,y) = 3x^{2}y - y^{3}$. The Jacobian matrix is the matrix of first-order partial derivatives:\n$$\n\\mathbf{J}_{\\mathbf{F}}(x,y) = \\begin{pmatrix}\n\\frac{\\partial u}{\\partial x}  \\frac{\\partial u}{\\partial y} \\\\\n\\frac{\\partial v}{\\partial x}  \\frac{\\partial v}{\\partial y}\n\\end{pmatrix}.\n$$\nCompute each partial derivative explicitly:\n$$\n\\frac{\\partial u}{\\partial x} = \\frac{\\partial}{\\partial x}\\left(x^{3} - 3xy^{2}\\right) = 3x^{2} - 3y^{2},\n$$\n$$\n\\frac{\\partial u}{\\partial y} = \\frac{\\partial}{\\partial y}\\left(x^{3} - 3xy^{2}\\right) = -6xy,\n$$\n$$\n\\frac{\\partial v}{\\partial x} = \\frac{\\partial}{\\partial x}\\left(3x^{2}y - y^{3}\\right) = 6xy,\n$$\n$$\n\\frac{\\partial v}{\\partial y} = \\frac{\\partial}{\\partial y}\\left(3x^{2}y - y^{3}\\right) = 3x^{2} - 3y^{2}.\n$$\nThus,\n$$\n\\mathbf{J}_{\\mathbf{F}}(x,y) = \\begin{pmatrix}\n3x^{2} - 3y^{2}  -6xy \\\\\n6xy  3x^{2} - 3y^{2}\n\\end{pmatrix}.\n$$\nEvaluating at $(x,y) = (2,-1)$ gives\n$$\n\\mathbf{J}_{\\mathbf{F}}(2,-1) = \\begin{pmatrix}\n3\\cdot 2^{2} - 3\\cdot (-1)^{2}  -6\\cdot 2 \\cdot (-1) \\\\\n6\\cdot 2 \\cdot (-1)  3\\cdot 2^{2} - 3\\cdot (-1)^{2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n9  12 \\\\\n-12  9\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}9  12 \\\\ -12  9\\end{pmatrix}}$$"
        },
        {
            "introduction": "Beyond its definition as a matrix of partial derivatives , the Jacobian's true power lies in its role as a tool for local linear approximation. This practice demonstrates how the Jacobian can be used to predict the output of a function for a small change in its input, a cornerstone of sensitivity analysis and control systems. The intuitive context of a robotic arm makes this abstract mathematical concept tangible and highlights its practical engineering value. ",
            "id": "2325300",
            "problem": "A robotic arm with two joints operates on a 2D plane. The Cartesian coordinates $(x, y)$ of its end-effector are determined by the angles of its two joints, $(\\theta_1, \\theta_2)$, which can be described by a function $P: \\mathbb{R}^2 \\to \\mathbb{R}^2$ such that $(x, y) = P(\\theta_1, \\theta_2)$. The angles are measured in radians, and the coordinates are in meters.\n\nAt a specific configuration where the joint angles are $(\\theta_1, \\theta_2) = (\\frac{\\pi}{6}, \\frac{\\pi}{3})$, the end-effector is located at the position $(x, y) = (2.50, 1.75)$ meters. The sensitivity of the end-effector's position to small changes in joint angles at this configuration is described by the Jacobian matrix of the function $P$:\n$$ J_P\\left(\\frac{\\pi}{6}, \\frac{\\pi}{3}\\right) = \\begin{pmatrix} -1.2  0.8 \\\\ 2.0  1.5 \\end{pmatrix} $$\nThe units of the entries in the Jacobian matrix are in meters per radian.\n\nSuppose the control system introduces a small perturbation to the joint angles, moving them to a new configuration $(\\theta_1', \\theta_2') = (\\frac{\\pi}{6} + 0.02, \\frac{\\pi}{3} - 0.04)$.\n\nUsing a linear approximation, estimate the new Cartesian coordinates $(x', y')$ of the end-effector. Express your answer as a pair of coordinates. Round each coordinate to three significant figures. The final answer should be given in meters.",
            "solution": "We use the first-order (linear) approximation of $P$ about the configuration $(\\theta_{1},\\theta_{2})$:\n$$\n\\begin{pmatrix} x' \\\\ y' \\end{pmatrix} \\approx \\begin{pmatrix} x \\\\ y \\end{pmatrix} + J_{P}(\\theta_{1},\\theta_{2}) \\begin{pmatrix} \\Delta \\theta_{1} \\\\ \\Delta \\theta_{2} \\end{pmatrix},\n$$\nwhere $\\Delta \\theta_{1} = \\theta_{1}' - \\theta_{1}$ and $\\Delta \\theta_{2} = \\theta_{2}' - \\theta_{2}$. At the given configuration, $(x,y) = (2.50,1.75)$ meters, and\n$$\nJ_{P}\\left(\\frac{\\pi}{6},\\frac{\\pi}{3}\\right) = \\begin{pmatrix} -1.2  0.8 \\\\ 2.0  1.5 \\end{pmatrix}.\n$$\nThe perturbations in joint angles are\n$$\n\\Delta \\theta_{1} = 0.02, \\quad \\Delta \\theta_{2} = -0.04.\n$$\nCompute the position change:\n$$\n\\Delta \\mathbf{p} = J_{P} \\begin{pmatrix} 0.02 \\\\ -0.04 \\end{pmatrix} = \\begin{pmatrix} -1.2  0.8 \\\\ 2.0  1.5 \\end{pmatrix} \\begin{pmatrix} 0.02 \\\\ -0.04 \\end{pmatrix} = \\begin{pmatrix} -1.2\\cdot 0.02 + 0.8\\cdot(-0.04) \\\\ 2.0\\cdot 0.02 + 1.5\\cdot(-0.04) \\end{pmatrix} = \\begin{pmatrix} -0.056 \\\\ -0.02 \\end{pmatrix}.\n$$\nAdd this to the nominal position:\n$$\n\\begin{pmatrix} x' \\\\ y' \\end{pmatrix} \\approx \\begin{pmatrix} 2.50 \\\\ 1.75 \\end{pmatrix} + \\begin{pmatrix} -0.056 \\\\ -0.02 \\end{pmatrix} = \\begin{pmatrix} 2.444 \\\\ 1.73 \\end{pmatrix}.\n$$\nRounding each coordinate to three significant figures gives $x' \\approx 2.44$ and $y' \\approx 1.73$ (meters).",
            "answer": "$$\\boxed{\\begin{pmatrix} 2.44  1.73 \\end{pmatrix}}$$"
        },
        {
            "introduction": "In large-scale systems like neural networks, explicitly forming the Jacobian matrix as in  is computationally infeasible. Building on the idea of linear approximation , this advanced practice explores the computational backbone of modern deep learning: computing the *action* of the Jacobian without forming the matrix itself. You will derive and implement the Jacobian-Vector Product (JVP) and Vector-Jacobian Product (VJP), the key operations behind forward-mode and reverse-mode automatic differentiation (backpropagation), respectively, bridging the gap between theory and efficient implementation. ",
            "id": "3187081",
            "problem": "You are given a custom nonlinear layer frequently encountered in deep learning architectures, defined for an input vector $x \\in \\mathbb{R}^n$ and fixed parameters as a mapping $f : \\mathbb{R}^n \\to \\mathbb{R}^m$. The layer output is\n$$\ny(x) = f(x) = \\operatorname{softplus}\\!\\big(Wx + b\\big) \\odot \\exp\\!\\big(-\\alpha \\lVert x \\rVert_2^2\\big) \\;+\\; M\\big(x \\odot \\tanh(x)\\big),\n$$\nwhere $W \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, $M \\in \\mathbb{R}^{m \\times n}$, $\\alpha \\in \\mathbb{R}_{0}$, and $\\odot$ denotes elementwise multiplication. The function $\\operatorname{softplus}$ is defined elementwise by\n$$\n\\operatorname{softplus}(z) = \\log\\big(1 + e^{z}\\big),\n$$\nand $\\tanh$ is the hyperbolic tangent applied elementwise.\n\nYour tasks are:\n1. Starting from the fundamental definitions of the Jacobian matrix and the multivariate chain rule, derive the full Jacobian matrix $J(x) \\in \\mathbb{R}^{m \\times n}$ of $f$ with respect to $x$. Do not begin with or cite any pre-packaged shortcut formulas; use first principles (definition of derivatives, the chain rule, product rule, and known derivatives of elementary functions).\n2. Implement two linear actions associated with the Jacobian that are central in automatic differentiation:\n   - The Vector-Jacobian Product (VJP): given a vector $u \\in \\mathbb{R}^m$, compute $J(x)^\\top u$ exactly, as the gradient of the scalar $u^\\top f(x)$ with respect to $x$.\n   - The Jacobian-Vector Product (JVP): given a vector $v \\in \\mathbb{R}^n$, compute $J(x) v$ exactly, as the directional derivative of $f$ at $x$ in the direction $v$.\n3. Verify the correctness of your implementations by comparing the VJP and JVP results against the action of the full Jacobian matrix. Additionally, verify a finite-difference directional derivative check.\n\nUse the following test suite of parameter sets, with all random quantities generated deterministically from a specified seed using a standard normal distribution scaled as indicated. For a seed $s$, generate with a reproducible pseudorandom generator the arrays $W$, $M$, $b$, $x$, $u$, $v$ as independent samples from $\\mathcal{N}(0, 1)$, scaled per the case definition below. In all cases, $W \\in \\mathbb{R}^{m \\times n}$, $M \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, $x \\in \\mathbb{R}^n$, $u \\in \\mathbb{R}^m$, $v \\in \\mathbb{R}^n$.\n\n- Case $1$ (general happy path):\n  - $m = 4$, $n = 4$, seed $s = 0$, $\\alpha = 0.3$.\n  - Scales: $W$ by $0.5$, $M$ by $0.3$, $b$ by $0.1$, $x$ by $1.0$, $u$ by $1.0$, $v$ by $1.0$.\n\n- Case $2$ (boundary with zero input):\n  - $m = 5$, $n = 5$, seed $s = 1$, $\\alpha = 0.2$.\n  - Scales: $W$ by $0.4$, $M$ by $0.3$, $b$ by $0.05$, $u$ by $1.0$, $v$ by $1.0$.\n  - $x$ set to the zero vector in $\\mathbb{R}^n$ (ignore random $x$).\n\n- Case $3$ (dimension mismatch $m \\neq n$):\n  - $m = 3$, $n = 5$, seed $s = 2$, $\\alpha = 0.05$.\n  - Scales: $W$ by $0.7$, $M$ by $0.6$, $b$ by $0.2$, $x$ by $0.8$, $u$ by $0.9$, $v$ by $1.1$.\n\n- Case $4$ (large input magnitude, small damping):\n  - $m = 6$, $n = 6$, seed $s = 3$, $\\alpha = 0.02$.\n  - Scales: $W$ by $0.3$, $M$ by $0.3$, $b$ by $0.1$, $x$ by $3.0$, $u$ by $0.5$, $v$ by $0.5$.\n\nFor each case, compute and report the following four real-valued diagnostics:\n- $r_{\\mathrm{vjp}} = \\lVert \\mathrm{VJP}(u) - J(x)^\\top u \\rVert_2$,\n- $r_{\\mathrm{jvp}} = \\lVert \\mathrm{JVP}(v) - J(x) v \\rVert_2$,\n- $r_{\\mathrm{fd}} = \\lVert f(x + \\varepsilon v) - f(x) - \\varepsilon\\, J(x) v \\rVert_2$ with $\\varepsilon = 10^{-6}$,\n- $r_{\\mathrm{scalar}} = \\left| u^\\top (J(x) v) - v^\\top (J(x)^\\top u) \\right|$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of cases $1$ through $4$, with each case contributing the four diagnostics in the sequence $[r_{\\mathrm{vjp}}, r_{\\mathrm{jvp}}, r_{\\mathrm{fd}}, r_{\\mathrm{scalar}}]$. Express each floating-point number in scientific notation with six digits after the decimal point. For example, an output for a single case would look like $[1.234567e-08,2.345678e-09,3.456789e-10,4.567890e-11]$, and the overall output should concatenate the cases accordingly into one list.",
            "solution": "The problem is valid. We are tasked with analyzing the differential properties of a custom nonlinear layer $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ defined by\n$$\nf(x) = \\operatorname{softplus}\\!\\big(Wx + b\\big) \\odot \\exp\\!\\big(-\\alpha \\lVert x \\rVert_2^2\\big) \\;+\\; M\\big(x \\odot \\tanh(x)\\big)\n$$\nwhere $x \\in \\mathbb{R}^n$ is the input, and $W \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, $M \\in \\mathbb{R}^{m \\times n}$, and $\\alpha \\in \\mathbb{R}_{0}$ are fixed parameters. The symbol $\\odot$ denotes elementwise multiplication.\n\nThe overall Jacobian of $f(x)$ can be found by leveraging the linearity of the derivative operator. Let us decompose $f(x)$ into two parts:\n$f_1(x) = \\operatorname{softplus}\\!\\big(Wx + b\\big) \\odot \\exp\\!\\big(-\\alpha \\lVert x \\rVert_2^2\\big)$\n$f_2(x) = M\\big(x \\odot \\tanh(x)\\big)$\nThen, $f(x) = f_1(x) + f_2(x)$, and its Jacobian matrix $J(x) \\in \\mathbb{R}^{m \\times n}$ is the sum of the Jacobians of $f_1(x)$ and $f_2(x)$:\n$$\nJ(x) = \\frac{\\partial f}{\\partial x} = \\frac{\\partial f_1}{\\partial x} + \\frac{\\partial f_2}{\\partial x} = J_1(x) + J_2(x)\n$$\nWe will derive $J_1(x)$ and $J_2(x)$ separately from first principles.\n\n**1. Derivation of the Jacobian $J_1(x)$**\n\nThe function $f_1(x)$ is the product of a vector-valued function $p(x) = \\operatorname{softplus}(Wx+b)$ and a scalar-valued function $s(x) = \\exp(-\\alpha \\lVert x \\rVert_2^2)$. So, $f_1(x) = p(x) s(x)$.\n\nThe $(i, j)$-th entry of the Jacobian $J_1(x)$ is given by the partial derivative of the $i$-th component of $f_1(x)$ with respect to the $j$-th component of $x$:\n$$\n[J_1(x)]_{ij} = \\frac{\\partial [f_1(x)]_i}{\\partial x_j} = \\frac{\\partial (p_i(x) s(x))}{\\partial x_j}\n$$\nUsing the product rule for differentiation, we have:\n$$\n[J_1(x)]_{ij} = \\frac{\\partial p_i(x)}{\\partial x_j} s(x) + p_i(x) \\frac{\\partial s(x)}{\\partial x_j}\n$$\nWe need to find the derivatives of $p_i(x)$ and $s(x)$.\n\nFirst, for the scalar function $s(x) = \\exp(-\\alpha \\sum_{k=1}^n x_k^2)$:\n$$\n\\frac{\\partial s(x)}{\\partial x_j} = \\exp\\left(-\\alpha \\sum_{k=1}^n x_k^2\\right) \\cdot \\frac{\\partial}{\\partial x_j}\\left(-\\alpha \\sum_{k=1}^n x_k^2\\right) = s(x) \\cdot (-2\\alpha x_j)\n$$\nThe gradient of $s(x)$ is thus $\\nabla_x s(x) = -2\\alpha s(x) x$.\n\nNext, for the vector function $p(x) = \\operatorname{softplus}(Wx+b)$. Let $a(x) = Wx+b$. Then $p_i(x) = \\operatorname{softplus}(a_i(x))$. Using the chain rule:\n$$\n\\frac{\\partial p_i(x)}{\\partial x_j} = \\frac{d\\operatorname{softplus}}{dz}\\bigg|_{z=a_i(x)} \\cdot \\frac{\\partial a_i(x)}{\\partial x_j}\n$$\nThe derivative of $\\operatorname{softplus}(z) = \\log(1+e^z)$ is $\\frac{d}{dz}\\log(1+e^z) = \\frac{e^z}{1+e^z}$, which is the logistic sigmoid function, $\\sigma(z)$.\nThe term $a_i(x) = \\sum_{k=1}^n W_{ik}x_k + b_i$. Its partial derivative is $\\frac{\\partial a_i(x)}{\\partial x_j} = W_{ij}$.\nSo, $\\frac{\\partial p_i(x)}{\\partial x_j} = \\sigma(a_i(x)) W_{ij}$. In matrix form, the Jacobian of $p(x)$ is $J_p(x) = \\operatorname{diag}(\\sigma(Wx+b))W$.\n\nCombining these results:\n$$\n[J_1(x)]_{ij} = (\\sigma(a_i(x))W_{ij}) s(x) + p_i(x) (-2\\alpha s(x) x_j) = s(x) \\left( \\sigma(a_i(x))W_{ij} - 2\\alpha p_i(x) x_j \\right)\n$$\nThis can be expressed in matrix form. The term $\\sigma(a_i(x))W_{ij}$ corresponds to the $(i, j)$-th entry of $\\operatorname{diag}(\\sigma(Wx+b))W$. The term $-2\\alpha p_i(x) x_j$ corresponds to the $(i, j)$-th entry of the outer product $-2\\alpha p(x) x^\\top$.\nThus, the Jacobian $J_1(x)$ is:\n$$\nJ_1(x) = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( \\operatorname{diag}\\big(\\sigma(Wx+b)\\big) W - 2\\alpha \\operatorname{softplus}(Wx+b) x^\\top \\right)\n$$\n\n**2. Derivation of the Jacobian $J_2(x)$**\n\nThe function $f_2(x) = M(x \\odot \\tanh(x))$. Let $g(x) = x \\odot \\tanh(x)$. Then $f_2(x) = M g(x)$.\nUsing the multivariate chain rule, $J_2(x) = J_{h}(g(x)) \\cdot J_g(x)$, where $h(g) = Mg$. The function $h$ is a linear transformation, so its Jacobian is simply the matrix $M$.\n$$\nJ_2(x) = M J_g(x)\n$$\nThe function $g(x)$ is an elementwise operation, so its Jacobian $J_g(x)$ is a diagonal matrix. The diagonal entries are the derivatives of its components $g_k(x) = x_k \\tanh(x_k)$. For $k=j$:\n$$\n\\frac{d g_j(x)}{d x_j} = \\frac{d}{dx_j}(x_j \\tanh(x_j)) = 1 \\cdot \\tanh(x_j) + x_j \\cdot \\frac{d}{dx_j}\\tanh(x_j)\n$$\nThe derivative of $\\tanh(z)$ is $1 - \\tanh^2(z)$. So,\n$$\n\\frac{d g_j(x)}{d x_j} = \\tanh(x_j) + x_j(1 - \\tanh^2(x_j))\n$$\nLet this derivative vector be denoted $g'(x)$. Then $J_g(x) = \\operatorname{diag}(g'(x))$. The full Jacobian for $f_2$ is:\n$$\nJ_2(x) = M \\operatorname{diag}\\big(\\tanh(x) + x \\odot (1-\\tanh^2(x))\\big)\n$$\n\n**3. Full Jacobian and Associated Linear Actions**\n\nCombining the two parts gives the full Jacobian $J(x) = J_1(x) + J_2(x)$.\n\n**Jacobian-Vector Product (JVP): $J(x)v$**\nThe JVP is the directional derivative of $f(x)$ along a vector $v \\in \\mathbb{R}^n$. We can compute $J(x)v = J_1(x)v + J_2(x)v$ without explicitly forming the matrices.\n$$\nJ_1(x)v = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( \\operatorname{diag}\\big(\\sigma(Wx+b)\\big) Wv - 2\\alpha \\operatorname{softplus}(Wx+b) (x^\\top v) \\right)\n$$\nNoting that $\\operatorname{diag}(z)y = z \\odot y$ and $x^\\top v$ is a scalar dot product:\n$$\nJ_1(x)v = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( \\sigma(Wx+b) \\odot (Wv) - (2\\alpha (x^\\top v)) \\operatorname{softplus}(Wx+b) \\right)\n$$\nFor the second term:\n$$\nJ_2(x)v = M \\operatorname{diag}(g'(x)) v = M (g'(x) \\odot v)\n$$\nwhere $g'(x) = \\tanh(x) + x \\odot (1-\\tanh^2(x))$.\n\n**Vector-Jacobian Product (VJP): $J(x)^\\top u$**\nThe VJP computes the action of the transpose Jacobian on a vector $u \\in \\mathbb{R}^m$. It corresponds to the gradient of the scalar projection $u^\\top f(x)$. We compute $J(x)^\\top u = J_1(x)^\\top u + J_2(x)^\\top u$.\n$$\nJ_1(x)^\\top = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( (\\operatorname{diag}(\\sigma)W)^\\top - (2\\alpha p x^\\top)^\\top \\right)\n$$\n$$\nJ_1(x)^\\top = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( W^\\top \\operatorname{diag}(\\sigma) - 2\\alpha x p^\\top \\right)\n$$\nApplying this to $u$:\n$$\nJ_1(x)^\\top u = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( W^\\top (\\sigma(Wx+b) \\odot u) - (2\\alpha (\\operatorname{softplus}(Wx+b)^\\top u)) x \\right)\n$$\nFor the second term:\n$$\nJ_2(x)^\\top u = \\left(M \\operatorname{diag}(g'(x))\\right)^\\top u = \\operatorname{diag}(g'(x)) M^\\top u = g'(x) \\odot (M^\\top u)\n$$\nThese expressions allow for efficient, matrix-free computation of the VJP and JVP.\n\n**4. Verification Diagnostics**\nThe numerical diagnostics are computed as follows:\n- $r_{\\mathrm{vjp}} = \\lVert \\mathrm{VJP}(u) - J(x)^\\top u \\rVert_2$: This measures the consistency between the matrix-free VJP implementation and the explicit VJP using the computed Jacobian matrix. The expected result is near machine precision.\n- $r_{\\mathrm{jvp}} = \\lVert \\mathrm{JVP}(v) - J(x) v \\rVert_2$: Similarly, this measures the consistency for the JVP. The expected result is near machine precision.\n- $r_{\\mathrm{fd}} = \\lVert f(x + \\varepsilon v) - f(x) - \\varepsilon\\, J(x) v \\rVert_2$: This compares the JVP (the true directional derivative) to a finite-difference approximation. By Taylor's theorem, this error is of order $O(\\varepsilon^2)$, so for $\\varepsilon = 10^{-6}$, the result should be small (around $10^{-12}$).\n- $r_{\\mathrm{scalar}} = \\left| u^\\top (J(x) v) - v^\\top (J(x)^\\top u) \\right|$: This checks the fundamental adjoint property of linear operators, $\\langle u, Jv \\rangle = \\langle J^\\top u, v \\rangle$. The difference should be near machine precision.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving and implementing the Jacobian, JVP, and VJP\n    for the given nonlinear function and verifying them on a test suite.\n    \"\"\"\n    \n    # Helper functions for nonlinearities and their derivatives\n    def softplus(z):\n        \"\"\"Numerically stable softplus function.\"\"\"\n        return np.logaddexp(0, z)\n\n    def sigmoid(z):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-z))\n\n    # Implementation of the function f(x)\n    def compute_f(x, W, b, M, alpha):\n        a = W @ x + b\n        softplus_a = softplus(a)\n        \n        x_norm_sq = np.dot(x, x)\n        exp_term = np.exp(-alpha * x_norm_sq)\n        \n        term1 = softplus_a * exp_term\n        \n        tanh_x = np.tanh(x)\n        x_tanh_x = x * tanh_x\n        term2 = M @ x_tanh_x\n        \n        return term1 + term2\n\n    # Implementation of the full Jacobian matrix J(x)\n    def compute_jacobian(x, W, b, M, alpha):\n        m, n = W.shape\n        \n        a = W @ x + b\n        softplus_a = softplus(a)\n        sigma_a = sigmoid(a)\n        \n        x_norm_sq = np.dot(x, x)\n        exp_term = np.exp(-alpha * x_norm_sq)\n        \n        tanh_x = np.tanh(x)\n        \n        # J1 = exp_term * (diag(sigma(a)) @ W - 2*alpha*outer(softplus(a), x))\n        term1_part1 = np.diag(sigma_a) @ W\n        term1_part2 = -2 * alpha * np.outer(softplus_a, x)\n        J1 = exp_term * (term1_part1 + term1_part2)\n        \n        # J2 = M @ diag(tanh(x) + x * (1 - tanh(x)**2))\n        g_prime_x = tanh_x + x * (1 - tanh_x**2)\n        J2 = M * g_prime_x  # Broadcasting g_prime_x (n,) across rows of M (m, n)\n        \n        return J1 + J2\n\n    # Implementation of the Jacobian-Vector Product (JVP)\n    def compute_jvp(x, v, W, b, M, alpha):\n        a = W @ x + b\n        softplus_a = softplus(a)\n        sigma_a = sigmoid(a)\n        \n        x_norm_sq = np.dot(x, x)\n        exp_term = np.exp(-alpha * x_norm_sq)\n        \n        tanh_x = np.tanh(x)\n        \n        # JVP for term 1\n        W_v = W @ v\n        sigma_a_W_v = sigma_a * W_v\n        x_dot_v = np.dot(x, v)\n        term1_jvp = exp_term * (sigma_a_W_v - (2 * alpha * x_dot_v) * softplus_a)\n        \n        # JVP for term 2\n        g_prime_x = tanh_x + x * (1 - tanh_x**2)\n        g_prime_x_v = g_prime_x * v\n        term2_jvp = M @ g_prime_x_v\n        \n        return term1_jvp + term2_jvp\n\n    # Implementation of the Vector-Jacobian Product (VJP)\n    def compute_vjp(x, u, W, b, M, alpha):\n        a = W @ x + b\n        softplus_a = softplus(a)\n        sigma_a = sigmoid(a)\n        \n        x_norm_sq = np.dot(x, x)\n        exp_term = np.exp(-alpha * x_norm_sq)\n        \n        tanh_x = np.tanh(x)\n        \n        # VJP for term 1\n        sigma_a_u = sigma_a * u\n        WT_sigma_a_u = W.T @ sigma_a_u\n        softplus_a_dot_u = np.dot(softplus_a, u)\n        term1_vjp = exp_term * (WT_sigma_a_u - (2 * alpha * softplus_a_dot_u) * x)\n        \n        # VJP for term 2\n        g_prime_x = tanh_x + x * (1 - tanh_x**2)\n        MT_u = M.T @ u\n        term2_vjp = g_prime_x * MT_u\n        \n        return term1_vjp + term2_vjp\n\n    test_cases = [\n        {'m': 4, 'n': 4, 's': 0, 'alpha': 0.3, 'scales': {'W': 0.5, 'M': 0.3, 'b': 0.1, 'x': 1.0, 'u': 1.0, 'v': 1.0}},\n        {'m': 5, 'n': 5, 's': 1, 'alpha': 0.2, 'scales': {'W': 0.4, 'M': 0.3, 'b': 0.05, 'u': 1.0, 'v': 1.0}, 'x_zero': True},\n        {'m': 3, 'n': 5, 's': 2, 'alpha': 0.05, 'scales': {'W': 0.7, 'M': 0.6, 'b': 0.2, 'x': 0.8, 'u': 0.9, 'v': 1.1}},\n        {'m': 6, 'n': 6, 's': 3, 'alpha': 0.02, 'scales': {'W': 0.3, 'M': 0.3, 'b': 0.1, 'x': 3.0, 'u': 0.5, 'v': 0.5}},\n    ]\n\n    all_diagnostics = []\n    \n    for case in test_cases:\n        m, n, s, alpha = case['m'], case['n'], case['s'], case['alpha']\n        scales = case['scales']\n        rng = np.random.default_rng(s)\n        \n        W = rng.standard_normal((m, n)) * scales['W']\n        M = rng.standard_normal((m, n)) * scales['M']\n        b = rng.standard_normal(m) * scales['b']\n        u = rng.standard_normal(m) * scales['u']\n        v = rng.standard_normal(n) * scales['v']\n        \n        if case.get('x_zero', False):\n            x = np.zeros(n)\n        else:\n            x = rng.standard_normal(n) * scales['x']\n\n        params = (W, b, M, alpha)\n        \n        # 1. Compute full Jacobian matrix\n        j_matrix = compute_jacobian(x, *params)\n        \n        # 2. Compute VJP and JVP directly and via matrix\n        vjp_direct = compute_vjp(x, u, *params)\n        vjp_matrix = j_matrix.T @ u\n        \n        jvp_direct = compute_jvp(x, v, *params)\n        jvp_matrix = j_matrix @ v\n        \n        # 3. Calculate diagnostics\n        r_vjp = np.linalg.norm(vjp_direct - vjp_matrix)\n        r_jvp = np.linalg.norm(jvp_direct - jvp_matrix)\n        \n        eps = 1e-6\n        f_x_plus_eps_v = compute_f(x + eps * v, *params)\n        f_x = compute_f(x, *params)\n        r_fd = np.linalg.norm(f_x_plus_eps_v - f_x - eps * jvp_matrix)\n        \n        r_scalar = np.abs(np.dot(u, jvp_matrix) - np.dot(v, vjp_matrix))\n        \n        all_diagnostics.extend([r_vjp, r_jvp, r_fd, r_scalar])\n\n    # Format and print the final results\n    formatted_results = [f\"{val:.6e}\" for val in all_diagnostics]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}