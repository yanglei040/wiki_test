{
    "hands_on_practices": [
        {
            "introduction": "掌握了雅可比矩阵的理论定义后，首要的实践任务便是能够准确地计算它。这个练习将引导你为一个从笛卡尔坐标 $(x,y)$ 到新坐标系 $(u,v)$ 的变换计算雅可比矩阵，这是理解多变量函数局部行为的基础一步。通过求解这个问题 ，你将熟练掌握偏导数的计算和矩阵的构建过程。",
            "id": "2216501",
            "problem": "在计算流体动力学领域，通常使用变换将复杂的物理域映射到更简单的计算网格上。考虑一个从笛卡尔坐标系 $(x,y)$ 到新坐标系 $(u,v)$ 的坐标变换，该变换由向量函数 $\\mathbf{F}: \\mathbb{R}^2 \\to \\mathbb{R}^2$ 定义。该映射由以下分量函数给出：\n$u(x, y) = x^3 - 3xy^2$\n$v(x, y) = 3x^2y - y^3$\n\n该变换在任意点的局部线性近似由其雅可比矩阵描述，雅可比矩阵是该向量函数所有一阶偏导数构成的矩阵。计算该变换的雅可比矩阵 $\\mathbf{J}_{\\mathbf{F}}(x,y)$ 在点 $(x, y) = (2, -1)$ 处的值。",
            "solution": "令 $\\mathbf{F}(x,y) = \\big(u(x,y), v(x,y)\\big)$，其中 $u(x,y) = x^{3} - 3xy^{2}$ 且 $v(x,y) = 3x^{2}y - y^{3}$。雅可比矩阵是一阶偏导数的矩阵：\n$$\n\\mathbf{J}_{\\mathbf{F}}(x,y) = \\begin{pmatrix}\n\\frac{\\partial u}{\\partial x}  \\frac{\\partial u}{\\partial y} \\\\\n\\frac{\\partial v}{\\partial x}  \\frac{\\partial v}{\\partial y}\n\\end{pmatrix}.\n$$\n显式地计算每个偏导数：\n$$\n\\frac{\\partial u}{\\partial x} = \\frac{\\partial}{\\partial x}\\left(x^{3} - 3xy^{2}\\right) = 3x^{2} - 3y^{2},\n$$\n$$\n\\frac{\\partial u}{\\partial y} = \\frac{\\partial}{\\partial y}\\left(x^{3} - 3xy^{2}\\right) = -6xy,\n$$\n$$\n\\frac{\\partial v}{\\partial x} = \\frac{\\partial}{\\partial x}\\left(3x^{2}y - y^{3}\\right) = 6xy,\n$$\n$$\n\\frac{\\partial v}{\\partial y} = \\frac{\\partial}{\\partial y}\\left(3x^{2}y - y^{3}\\right) = 3x^{2} - 3y^{2}.\n$$\n因此，\n$$\n\\mathbf{J}_{\\mathbf{F}}(x,y) = \\begin{pmatrix}\n3x^{2} - 3y^{2}  -6xy \\\\\n6xy  3x^{2} - 3y^{2}\n\\end{pmatrix}.\n$$\n在点 $(x,y) = (2,-1)$ 处求值得到\n$$\n\\mathbf{J}_{\\mathbf{F}}(2,-1) = \\begin{pmatrix}\n3\\cdot 2^{2} - 3\\cdot (-1)^{2}  -6\\cdot 2 \\cdot (-1) \\\\\n6\\cdot 2 \\cdot (-1)  3\\cdot 2^{2} - 3\\cdot (-1)^{2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n9  12 \\\\\n-12  9\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}9  12 \\\\ -12  9\\end{pmatrix}}$$"
        },
        {
            "introduction": "计算雅可比矩阵本身只是手段，其真正的威力在于它提供了对复杂非线性函数的最佳线性近似。这个练习  将带你进入机器人学领域，利用已知的雅可比矩阵来预测机器人手臂在关节角度发生微小变化后的新位置。这直观地展示了雅可比矩阵在预测和控制系统中的核心作用，让你体会到如何用线性工具来分析和估算非线性系统的行为。",
            "id": "2325300",
            "problem": "一个具有两个关节的机械臂在二维平面上运行。其末端执行器的笛卡尔坐标 $(x, y)$ 由其两个关节的角度 $(\\theta_1, \\theta_2)$ 决定，这可以由一个函数 $P: \\mathbb{R}^2 \\to \\mathbb{R}^2$ 描述，使得 $(x, y) = P(\\theta_1, \\theta_2)$。角度以弧度为单位，坐标以米为单位。\n\n在关节角度为 $(\\theta_1, \\theta_2) = (\\frac{\\pi}{6}, \\frac{\\pi}{3})$ 的特定位形下，末端执行器位于 $(x, y) = (2.50, 1.75)$ 米的位置。在此位形下，末端执行器位置对关节角度微小变化的敏感度由函数 $P$ 的雅可比矩阵描述：\n$$ J_P\\left(\\frac{\\pi}{6}, \\frac{\\pi}{3}\\right) = \\begin{pmatrix} -1.2  0.8 \\\\ 2.0  1.5 \\end{pmatrix} $$\n雅可比矩阵中元素的单位是米/弧度。\n\n假设控制系统对关节角度引入一个微小扰动，将其移动到一个新的位形 $(\\theta_1', \\theta_2') = (\\frac{\\pi}{6} + 0.02, \\frac{\\pi}{3} - 0.04)$。\n\n使用线性近似，估计末端执行器的新笛卡尔坐标 $(x', y')$。将您的答案表示为一对坐标。将每个坐标四舍五入到三位有效数字。最终答案应以米为单位。",
            "solution": "我们使用 $P$ 在位形 $(\\theta_{1},\\theta_{2})$ 附近的一阶（线性）近似：\n$$\n\\begin{pmatrix} x' \\\\ y' \\end{pmatrix} \\approx \\begin{pmatrix} x \\\\ y \\end{pmatrix} + J_{P}(\\theta_{1},\\theta_{2}) \\begin{pmatrix} \\Delta \\theta_{1} \\\\ \\Delta \\theta_{2} \\end{pmatrix},\n$$\n其中 $\\Delta \\theta_{1} = \\theta_{1}' - \\theta_{1}$ 且 $\\Delta \\theta_{2} = \\theta_{2}' - \\theta_{2}$。在给定位形下，$(x,y) = (2.50,1.75)$ 米，并且\n$$\nJ_{P}\\left(\\frac{\\pi}{6},\\frac{\\pi}{3}\\right) = \\begin{pmatrix} -1.2  0.8 \\\\ 2.0  1.5 \\end{pmatrix}.\n$$\n关节角度的扰动为\n$$\n\\Delta \\theta_{1} = 0.02, \\quad \\Delta \\theta_{2} = -0.04.\n$$\n计算位置变化：\n$$\n\\Delta \\mathbf{p} = J_{P} \\begin{pmatrix} 0.02 \\\\ -0.04 \\end{pmatrix} = \\begin{pmatrix} -1.2  0.8 \\\\ 2.0  1.5 \\end{pmatrix} \\begin{pmatrix} 0.02 \\\\ -0.04 \\end{pmatrix} = \\begin{pmatrix} -1.2\\cdot 0.02 + 0.8\\cdot(-0.04) \\\\ 2.0\\cdot 0.02 + 1.5\\cdot(-0.04) \\end{pmatrix} = \\begin{pmatrix} -0.056 \\\\ -0.02 \\end{pmatrix}.\n$$\n将其加到标称位置上：\n$$\n\\begin{pmatrix} x' \\\\ y' \\end{pmatrix} \\approx \\begin{pmatrix} 2.50 \\\\ 1.75 \\end{pmatrix} + \\begin{pmatrix} -0.056 \\\\ -0.02 \\end{pmatrix} = \\begin{pmatrix} 2.444 \\\\ 1.73 \\end{pmatrix}.\n$$\n将每个坐标四舍五入到三位有效数字，得到 $x' \\approx 2.44$ 和 $y' \\approx 1.73$ (米)。",
            "answer": "$$\\boxed{\\begin{pmatrix} 2.44  1.73 \\end{pmatrix}}$$"
        },
        {
            "introduction": "在深度学习等大规模问题中，显式构建整个雅可比矩阵既不高效也不必要。更关键的是计算雅可比矩阵与特定向量的乘积，即雅可比-向量积 (JVP) 和向量-雅可比积 (VJP)，它们是自动微分框架的基石。这个高级练习  将让你深入现代深度学习的底层，为一个复杂的非线性层推导并实现这些关键操作，从而理解神经网络训练的计算核心。",
            "id": "3187081",
            "problem": "给定一个深度学习架构中常见的自定义非线性层，它被定义为一个映射 $f : \\mathbb{R}^n \\to \\mathbb{R}^m$，其输入向量为 $x \\in \\mathbb{R}^n$，参数固定。该层的输出为\n$$\ny(x) = f(x) = \\operatorname{softplus}\\!\\big(Wx + b\\big) \\odot \\exp\\!\\big(-\\alpha \\lVert x \\rVert_2^2\\big) \\;+\\; M\\big(x \\odot \\tanh(x)\\big),\n$$\n其中 $W \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，$M \\in \\mathbb{R}^{m \\times n}$，$\\alpha \\in \\mathbb{R}_{0}$，且 $\\odot$ 表示逐元素乘法。函数 $\\operatorname{softplus}$ 逐元素定义为\n$$\n\\operatorname{softplus}(z) = \\log\\big(1 + e^{z}\\big),\n$$\n并且 $\\tanh$ 是逐元素应用的双曲正切函数。\n\n你的任务是：\n1. 从雅可比矩阵和多元链式法则的基本定义出发，推导出 $f$ 关于 $x$ 的完整雅可比矩阵 $J(x) \\in \\mathbb{R}^{m \\times n}$。不要从任何现成的快捷公式开始或引用它们；请使用第一性原理（导数的定义、链式法则、乘法法则以及已知基本函数的导数）。\n2. 实现与雅可比矩阵相关的两个线性作用，这在自动微分中是核心：\n   - 向量-雅可比积 (VJP)：给定一个向量 $u \\in \\mathbb{R}^m$，精确计算 $J(x)^\\top u$，其结果为标量 $u^\\top f(x)$ 关于 $x$ 的梯度。\n   - 雅可比-向量积 (JVP)：给定一个向量 $v \\in \\mathbb{R}^n$，精确计算 $J(x) v$，其结果为 $f$ 在 $x$ 点沿 $v$ 方向的方向导数。\n3. 通过将 VJP 和 JVP 的结果与完整雅可比矩阵的作用进行比较，来验证你的实现的正确性。此外，还需验证一个有限差分方向导数检验。\n\n使用以下参数集测试套件，所有随机量均使用标准正态分布从指定种子确定性地生成，并按指示进行缩放。对于一个种子 $s$，使用一个可复现的伪随机数生成器生成数组 $W$、$M$、$b$、$x$、$u$、$v$ 作为来自 $\\mathcal{N}(0, 1)$ 的独立样本，并根据下面的案例定义进行缩放。在所有案例中，$W \\in \\mathbb{R}^{m \\times n}$，$M \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，$x \\in \\mathbb{R}^n$，$u \\in \\mathbb{R}^m$，$v \\in \\mathbb{R}^n$。\n\n- 案例 1 (常规顺利路径)：\n  - $m = 4$，$n = 4$，种子 $s = 0$，$\\alpha = 0.3$。\n  - 缩放比例：$W$ 乘以 $0.5$，$M$ 乘以 $0.3$，$b$ 乘以 $0.1$，$x$ 乘以 $1.0$，$u$ 乘以 $1.0$，$v$ 乘以 $1.0$。\n\n- 案例 2 (零输入边界情况)：\n  - $m = 5$，$n = 5$，种子 $s = 1$，$\\alpha = 0.2$。\n  - 缩放比例：$W$ 乘以 $0.4$，$M$ 乘以 $0.3$，$b$ 乘以 $0.05$，$x$ 设置为 $\\mathbb{R}^n$ 中的零向量 (忽略随机生成的 $x$)，$u$ 乘以 $1.0$，$v$ 乘以 $1.0$。\n\n- 案例 3 (维度不匹配 $m \\neq n$)：\n  - $m = 3$，$n = 5$，种子 $s = 2$，$\\alpha = 0.05$。\n  - 缩放比例：$W$ 乘以 $0.7$，$M$ 乘以 $0.6$，$b$ 乘以 $0.2$，$x$ 乘以 $0.8$，$u$ 乘以 $0.9$，$v$ 乘以 $1.1$。\n\n- 案例 4 (大输入幅值，小阻尼)：\n  - $m = 6$，$n = 6$，种子 $s = 3$，$\\alpha = 0.02$。\n  - 缩放比例：$W$ 乘以 $0.3$，$M$ 乘以 $0.3$，$b$ 乘以 $0.1$，$x$ 乘以 $3.0$，$u$ 乘以 $0.5$，$v$ 乘以 $0.5$。\n\n对于每个案例，计算并报告以下四个实值诊断指标：\n- $r_{\\mathrm{vjp}} = \\lVert \\mathrm{VJP}(u) - J(x)^\\top u \\rVert_2$，\n- $r_{\\mathrm{jvp}} = \\lVert \\mathrm{JVP}(v) - J(x) v \\rVert_2$，\n- $r_{\\mathrm{fd}} = \\lVert f(x + \\varepsilon v) - f(x) - \\varepsilon\\, J(x) v \\rVert_2$，其中 $\\varepsilon = 10^{-6}$，\n- $r_{\\mathrm{scalar}} = \\left| u^\\top (J(x) v) - v^\\top (J(x)^\\top u) \\right|$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，顺序为案例 1 到 4，每个案例按 $[r_{\\mathrm{vjp}}, r_{\\mathrm{jvp}}, r_{\\mathrm{fd}}, r_{\\mathrm{scalar}}]$ 的顺序提供四个诊断指标。将每个浮点数用科学记数法表示，小数点后保留六位数字。例如，单个案例的输出可能看起来像 $[1.234567e-08,2.345678e-09,3.456789e-10,4.567890e-11]$，并且总输出应相应地将各案例结果连接成一个列表。",
            "solution": "该问题有效。我们的任务是分析一个自定义非线性层 $f: \\mathbb{R}^n \\to \\mathbb{R}^m$ 的微分性质，该层定义如下：\n$$\nf(x) = \\operatorname{softplus}\\!\\big(Wx + b\\big) \\odot \\exp\\!\\big(-\\alpha \\lVert x \\rVert_2^2\\big) \\;+\\; M\\big(x \\odot \\tanh(x)\\big)\n$$\n其中 $x \\in \\mathbb{R}^n$ 是输入，$W \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，$M \\in \\mathbb{R}^{m \\times n}$ 和 $\\alpha \\in \\mathbb{R}_{0}$ 是固定参数。符号 $\\odot$ 表示逐元素乘法。\n\n我们可以利用微分算子的线性性质来求得 $f(x)$ 的总雅可比矩阵。让我们将 $f(x)$ 分解为两部分：\n$f_1(x) = \\operatorname{softplus}\\!\\big(Wx + b\\big) \\odot \\exp\\!\\big(-\\alpha \\lVert x \\rVert_2^2\\big)$\n$f_2(x) = M\\big(x \\odot \\tanh(x)\\big)$\n那么，$f(x) = f_1(x) + f_2(x)$，其雅可比矩阵 $J(x) \\in \\mathbb{R}^{m \\times n}$ 是 $f_1(x)$ 和 $f_2(x)$ 的雅可比矩阵之和：\n$$\nJ(x) = \\frac{\\partial f}{\\partial x} = \\frac{\\partial f_1}{\\partial x} + \\frac{\\partial f_2}{\\partial x} = J_1(x) + J_2(x)\n$$\n我们将从第一性原理出发，分别推导 $J_1(x)$ 和 $J_2(x)$。\n\n**1. 雅可比矩阵 $J_1(x)$ 的推导**\n\n函数 $f_1(x)$ 是一个向量值函数 $p(x) = \\operatorname{softplus}(Wx+b)$ 和一个标量值函数 $s(x) = \\exp(-\\alpha \\lVert x \\rVert_2^2)$ 的乘积。因此，$f_1(x) = p(x) s(x)$。\n\n雅可比矩阵 $J_1(x)$ 的第 $(i, j)$ 个元素由 $f_1(x)$ 的第 $i$ 个分量对 $x$ 的第 $j$ 个分量的偏导数给出：\n$$\n[J_1(x)]_{ij} = \\frac{\\partial [f_1(x)]_i}{\\partial x_j} = \\frac{\\partial (p_i(x) s(x))}{\\partial x_j}\n$$\n使用微分的乘法法则，我们得到：\n$$\n[J_1(x)]_{ij} = \\frac{\\partial p_i(x)}{\\partial x_j} s(x) + p_i(x) \\frac{\\partial s(x)}{\\partial x_j}\n$$\n我们需要求出 $p_i(x)$ 和 $s(x)$ 的导数。\n\n首先，对于标量函数 $s(x) = \\exp(-\\alpha \\sum_{k=1}^n x_k^2)$：\n$$\n\\frac{\\partial s(x)}{\\partial x_j} = \\exp\\left(-\\alpha \\sum_{k=1}^n x_k^2\\right) \\cdot \\frac{\\partial}{\\partial x_j}\\left(-\\alpha \\sum_{k=1}^n x_k^2\\right) = s(x) \\cdot (-2\\alpha x_j)\n$$\n因此 $s(x)$ 的梯度是 $\\nabla_x s(x) = -2\\alpha s(x) x$。\n\n接下来，对于向量函数 $p(x) = \\operatorname{softplus}(Wx+b)$。令 $a(x) = Wx+b$。那么 $p_i(x) = \\operatorname{softplus}(a_i(x))$。使用链式法则：\n$$\n\\frac{\\partial p_i(x)}{\\partial x_j} = \\frac{d\\operatorname{softplus}}{dz}\\bigg|_{z=a_i(x)} \\cdot \\frac{\\partial a_i(x)}{\\partial x_j}\n$$\n$\\operatorname{softplus}(z) = \\log(1+e^z)$ 的导数是 $\\frac{d}{dz}\\log(1+e^z) = \\frac{e^z}{1+e^z}$，这就是 logistic sigmoid 函数 $\\sigma(z)$。\n项 $a_i(x) = \\sum_{k=1}^n W_{ik}x_k + b_i$。其偏导数为 $\\frac{\\partial a_i(x)}{\\partial x_j} = W_{ij}$。因此，$\\frac{\\partial p_i(x)}{\\partial x_j} = \\sigma(a_i(x)) W_{ij}$。以矩阵形式表示，$p(x)$ 的雅可比矩阵是 $J_p(x) = \\operatorname{diag}(\\sigma(Wx+b))W$。\n\n结合这些结果：\n$$\n[J_1(x)]_{ij} = (\\sigma(a_i(x))W_{ij}) s(x) + p_i(x) (-2\\alpha s(x) x_j) = s(x) \\left( \\sigma(a_i(x))W_{ij} - 2\\alpha p_i(x) x_j \\right)\n$$\n这可以表示成矩阵形式。项 $\\sigma(a_i(x))W_{ij}$ 对应于 $\\operatorname{diag}(\\sigma(Wx+b))W$ 的第 $(i, j)$ 个元素。项 $-2\\alpha p_i(x) x_j$ 对应于外积 $-2\\alpha p(x) x^\\top$ 的第 $(i, j)$ 个元素。因此，雅可比矩阵 $J_1(x)$ 是：\n$$\nJ_1(x) = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( \\operatorname{diag}\\big(\\sigma(Wx+b)\\big) W - 2\\alpha \\operatorname{softplus}(Wx+b) x^\\top \\right)\n$$\n\n**2. 雅可比矩阵 $J_2(x)$ 的推导**\n\n函数 $f_2(x) = M(x \\odot \\tanh(x))$。令 $g(x) = x \\odot \\tanh(x)$。则 $f_2(x) = M g(x)$。根据多元链式法则，$J_2(x) = J_{h}(g(x)) \\cdot J_g(x)$，其中 $h(g) = Mg$。函数 $h$ 是一个线性变换，因此其雅可比矩阵就是矩阵 $M$。\n$$\nJ_2(x) = M J_g(x)\n$$\n函数 $g(x)$ 是一个逐元素运算，因此其雅可比矩阵 $J_g(x)$ 是一个对角矩阵。对角线上的元素是其分量 $g_k(x) = x_k \\tanh(x_k)$ 的导数。对于 $k=j$：\n$$\n\\frac{d g_j(x)}{d x_j} = \\frac{d}{dx_j}(x_j \\tanh(x_j)) = 1 \\cdot \\tanh(x_j) + x_j \\cdot \\frac{d}{dx_j}\\tanh(x_j)\n$$\n$\\tanh(z)$ 的导数是 $1 - \\tanh^2(z)$。所以，\n$$\n\\frac{d g_j(x)}{d x_j} = \\tanh(x_j) + x_j(1 - \\tanh^2(x_j))\n$$\n将此导数向量记为 $g'(x)$。则 $J_g(x) = \\operatorname{diag}(g'(x))$。$f_2$ 的完整雅可比矩阵是：\n$$\nJ_2(x) = M \\operatorname{diag}\\big(\\tanh(x) + x \\odot (1-\\tanh^2(x))\\big)\n$$\n\n**3. 完整雅可比矩阵及相关线性作用**\n\n结合这两个部分，得到完整的雅可比矩阵 $J(x) = J_1(x) + J_2(x)$。\n\n**雅可比-向量积 (JVP): $J(x)v$**\nJVP 是 $f(x)$ 沿向量 $v \\in \\mathbb{R}^n$ 的方向导数。我们可以计算 $J(x)v = J_1(x)v + J_2(x)v$ 而无需显式地构造矩阵。\n$$\nJ_1(x)v = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( \\operatorname{diag}\\big(\\sigma(Wx+b)\\big) Wv - 2\\alpha \\operatorname{softplus}(Wx+b) (x^\\top v) \\right)\n$$\n注意到 $\\operatorname{diag}(z)y = z \\odot y$ 且 $x^\\top v$ 是一个标量点积：\n$$\nJ_1(x)v = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( \\sigma(Wx+b) \\odot (Wv) - (2\\alpha (x^\\top v)) \\operatorname{softplus}(Wx+b) \\right)\n$$\n对于第二项：\n$$\nJ_2(x)v = M \\operatorname{diag}(g'(x)) v = M (g'(x) \\odot v)\n$$\n其中 $g'(x) = \\tanh(x) + x \\odot (1-\\tanh^2(x))$。\n\n**向量-雅可比积 (VJP): $J(x)^\\top u$**\nVJP 计算转置雅可比矩阵作用于向量 $u \\in \\mathbb{R}^m$ 的结果。它对应于标量投影 $u^\\top f(x)$ 的梯度。我们计算 $J(x)^\\top u = J_1(x)^\\top u + J_2(x)^\\top u$。\n$$\nJ_1(x)^\\top = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( (\\operatorname{diag}(\\sigma)W)^\\top - (2\\alpha p x^\\top)^\\top \\right)\n$$\n$$\nJ_1(x)^\\top = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( W^\\top \\operatorname{diag}(\\sigma) - 2\\alpha x p^\\top \\right)\n$$\n将此应用于 $u$：\n$$\nJ_1(x)^\\top u = \\exp(-\\alpha \\lVert x \\rVert_2^2) \\left( W^\\top (\\sigma(Wx+b) \\odot u) - (2\\alpha (\\operatorname{softplus}(Wx+b)^\\top u)) x \\right)\n$$\n对于第二项：\n$$\nJ_2(x)^\\top u = \\left(M \\operatorname{diag}(g'(x))\\right)^\\top u = \\operatorname{diag}(g'(x)) M^\\top u = g'(x) \\odot (M^\\top u)\n$$\n这些表达式允许对 VJP 和 JVP 进行高效的、无需矩阵的计算。\n\n**4. 验证诊断指标**\n数值诊断指标计算如下：\n- $r_{\\mathrm{vjp}} = \\lVert \\mathrm{VJP}(u) - J(x)^\\top u \\rVert_2$：此指标衡量无需矩阵的 VJP 实现与使用计算出的雅可比矩阵的显式 VJP 之间的一致性。预期结果接近机器精度。\n- $r_{\\mathrm{jvp}} = \\lVert \\mathrm{JVP}(v) - J(x) v \\rVert_2$：类似地，此指标衡量 JVP 的一致性。预期结果接近机器精度。\n- $r_{\\mathrm{fd}} = \\lVert f(x + \\varepsilon v) - f(x) - \\varepsilon\\, J(x) v \\rVert_2$：此指标将 JVP（真实的方向导数）与有限差分近似进行比较。根据泰勒定理，该误差的阶数为 $O(\\varepsilon^2)$，因此对于 $\\varepsilon = 10^{-6}$，结果应该很小（约为 $10^{-12}$）。\n- $r_{\\mathrm{scalar}} = \\left| u^\\top (J(x) v) - v^\\top (J(x)^\\top u) \\right|$：此指标检验线性算子的基本伴随性质 $\\langle u, Jv \\rangle = \\langle J^\\top u, v \\rangle$。差值应接近机器精度。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving and implementing the Jacobian, JVP, and VJP\n    for the given nonlinear function and verifying them on a test suite.\n    \"\"\"\n    \n    # Helper functions for nonlinearities and their derivatives\n    def softplus(z):\n        \"\"\"Numerically stable softplus function.\"\"\"\n        return np.logaddexp(0, z)\n\n    def sigmoid(z):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-z))\n\n    # Implementation of the function f(x)\n    def compute_f(x, W, b, M, alpha):\n        a = W @ x + b\n        softplus_a = softplus(a)\n        \n        x_norm_sq = np.dot(x, x)\n        exp_term = np.exp(-alpha * x_norm_sq)\n        \n        term1 = softplus_a * exp_term\n        \n        tanh_x = np.tanh(x)\n        x_tanh_x = x * tanh_x\n        term2 = M @ x_tanh_x\n        \n        return term1 + term2\n\n    # Implementation of the full Jacobian matrix J(x)\n    def compute_jacobian(x, W, b, M, alpha):\n        m, n = W.shape\n        \n        a = W @ x + b\n        softplus_a = softplus(a)\n        sigma_a = sigmoid(a)\n        \n        x_norm_sq = np.dot(x, x)\n        exp_term = np.exp(-alpha * x_norm_sq)\n        \n        tanh_x = np.tanh(x)\n        \n        # J1 = exp_term * (diag(sigma(a)) @ W - 2*alpha*outer(softplus(a), x))\n        term1_part1 = np.diag(sigma_a) @ W\n        term1_part2 = -2 * alpha * np.outer(softplus_a, x)\n        J1 = exp_term * (term1_part1 + term1_part2)\n        \n        # J2 = M @ diag(tanh(x) + x * (1 - tanh(x)**2))\n        g_prime_x = tanh_x + x * (1 - tanh_x**2)\n        J2 = M * g_prime_x  # Broadcasting g_prime_x (n,) across rows of M (m, n)\n        \n        return J1 + J2\n\n    # Implementation of the Jacobian-Vector Product (JVP)\n    def compute_jvp(x, v, W, b, M, alpha):\n        a = W @ x + b\n        softplus_a = softplus(a)\n        sigma_a = sigmoid(a)\n        \n        x_norm_sq = np.dot(x, x)\n        exp_term = np.exp(-alpha * x_norm_sq)\n        \n        tanh_x = np.tanh(x)\n        \n        # JVP for term 1\n        W_v = W @ v\n        sigma_a_W_v = sigma_a * W_v\n        x_dot_v = np.dot(x, v)\n        term1_jvp = exp_term * (sigma_a_W_v - (2 * alpha * x_dot_v) * softplus_a)\n        \n        # JVP for term 2\n        g_prime_x = tanh_x + x * (1 - tanh_x**2)\n        g_prime_x_v = g_prime_x * v\n        term2_jvp = M @ g_prime_x_v\n        \n        return term1_jvp + term2_jvp\n\n    # Implementation of the Vector-Jacobian Product (VJP)\n    def compute_vjp(x, u, W, b, M, alpha):\n        a = W @ x + b\n        softplus_a = softplus(a)\n        sigma_a = sigmoid(a)\n        \n        x_norm_sq = np.dot(x, x)\n        exp_term = np.exp(-alpha * x_norm_sq)\n        \n        tanh_x = np.tanh(x)\n        \n        # VJP for term 1\n        sigma_a_u = sigma_a * u\n        WT_sigma_a_u = W.T @ sigma_a_u\n        softplus_a_dot_u = np.dot(softplus_a, u)\n        term1_vjp = exp_term * (WT_sigma_a_u - (2 * alpha * softplus_a_dot_u) * x)\n        \n        # VJP for term 2\n        g_prime_x = tanh_x + x * (1 - tanh_x**2)\n        MT_u = M.T @ u\n        term2_vjp = g_prime_x * MT_u\n        \n        return term1_vjp + term2_vjp\n\n    test_cases = [\n        {'m': 4, 'n': 4, 's': 0, 'alpha': 0.3, 'scales': {'W': 0.5, 'M': 0.3, 'b': 0.1, 'x': 1.0, 'u': 1.0, 'v': 1.0}},\n        {'m': 5, 'n': 5, 's': 1, 'alpha': 0.2, 'scales': {'W': 0.4, 'M': 0.3, 'b': 0.05, 'u': 1.0, 'v': 1.0}, 'x_zero': True},\n        {'m': 3, 'n': 5, 's': 2, 'alpha': 0.05, 'scales': {'W': 0.7, 'M': 0.6, 'b': 0.2, 'x': 0.8, 'u': 0.9, 'v': 1.1}},\n        {'m': 6, 'n': 6, 's': 3, 'alpha': 0.02, 'scales': {'W': 0.3, 'M': 0.3, 'b': 0.1, 'x': 3.0, 'u': 0.5, 'v': 0.5}},\n    ]\n\n    all_diagnostics = []\n    \n    for case in test_cases:\n        m, n, s, alpha = case['m'], case['n'], case['s'], case['alpha']\n        scales = case['scales']\n        rng = np.random.default_rng(s)\n        \n        W = rng.standard_normal((m, n)) * scales['W']\n        M = rng.standard_normal((m, n)) * scales['M']\n        b = rng.standard_normal(m) * scales['b']\n        u = rng.standard_normal(m) * scales['u']\n        v = rng.standard_normal(n) * scales['v']\n        \n        if case.get('x_zero', False):\n            x = np.zeros(n)\n        else:\n            x = rng.standard_normal(n) * scales['x']\n\n        params = (W, b, M, alpha)\n        \n        # 1. Compute full Jacobian matrix\n        j_matrix = compute_jacobian(x, *params)\n        \n        # 2. Compute VJP and JVP directly and via matrix\n        vjp_direct = compute_vjp(x, u, *params)\n        vjp_matrix = j_matrix.T @ u\n        \n        jvp_direct = compute_jvp(x, v, *params)\n        jvp_matrix = j_matrix @ v\n        \n        # 3. Calculate diagnostics\n        r_vjp = np.linalg.norm(vjp_direct - vjp_matrix)\n        r_jvp = np.linalg.norm(jvp_direct - jvp_matrix)\n        \n        eps = 1e-6\n        f_x_plus_eps_v = compute_f(x + eps * v, *params)\n        f_x = compute_f(x, *params)\n        r_fd = np.linalg.norm(f_x_plus_eps_v - f_x - eps * jvp_matrix)\n        \n        r_scalar = np.abs(np.dot(u, jvp_matrix) - np.dot(v, vjp_matrix))\n        \n        all_diagnostics.extend([r_vjp, r_jvp, r_fd, r_scalar])\n\n    # Format and print the final results\n    formatted_results = [f\"{val:.6e}\" for val in all_diagnostics]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}