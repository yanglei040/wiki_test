{
    "hands_on_practices": [
        {
            "introduction": "In deep learning, the connection between a model's output and its training objective is fundamental. For classification tasks, models produce a Probability Mass Function (PMF)—such as a Bernoulli or Categorical distribution—to represent the likelihood of each class. This practice solidifies the crucial link between this PMF and the Negative Log-Likelihood (NLL) loss function, which drives learning. By deriving and implementing the loss and its derivatives from first principles, you will gain a deeper intuition for how the shape of the output PMF directly influences the gradient signals used in backpropagation .",
            "id": "3166266",
            "problem": "You will compare two output heads used in deep learning for classification tasks and quantify how the probability mass function (PMF) shape affects the negative log-likelihood and its derivatives under label imbalance. The two heads are: a Bernoulli head for binary classification and a Categorical head for multi-class classification. The comparison should be made in terms of the gradient magnitude with respect to the logits and a notion of curvature for the loss surface. The computation must start from the fundamental definitions of the Probability Mass Function (PMF), the Negative Log-Likelihood (NLL), and the mapping from logits to probabilities via standard link functions.\n\nThe base definitions to use are:\n- The Bernoulli PMF for a binary label $y \\in \\{0,1\\}$ parameterized by a probability $p \\in (0,1)$ is $P(Y=y \\mid p) = p^{y} (1-p)^{1-y}$. The logit $z \\in \\mathbb{R}$ maps to $p$ through the logistic function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$.\n- The Categorical PMF for one-hot labels $\\mathbf{y} \\in \\{0,1\\}^{K}$ with exactly one $1$ and $K-1$ zeros parameterized by a probability vector $\\mathbf{q} \\in \\Delta^{K-1}$ (the probability simplex) is $P(\\mathbf{Y}=\\mathbf{y} \\mid \\mathbf{q}) = \\prod_{i=1}^{K} q_{i}^{y_{i}}$. The logits $\\mathbf{z} \\in \\mathbb{R}^{K}$ map to $\\mathbf{q}$ through the softmax function $q_{i}(\\mathbf{z}) = \\frac{e^{z_{i}}}{\\sum_{j=1}^{K} e^{z_{j}}}$.\n\nDefine the Negative Log-Likelihood (NLL) loss functions as the negative logarithm of the PMF under these link functions. For the Bernoulli head, denote the loss by $\\mathcal{L}_{\\mathrm{B}}(z; y)$, and for the Categorical head, denote the loss by $\\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y})$.\n\nTasks:\n1. For the Bernoulli head, compute:\n   - The gradient with respect to the scalar logit $z$, and report its magnitude $\\lVert \\nabla_{z} \\mathcal{L}_{\\mathrm{B}}(z; y) \\rVert_{2}$.\n   - The curvature defined as the second derivative $\\frac{d^{2}\\mathcal{L}_{\\mathrm{B}}(z; y)}{dz^{2}}$.\n   Both quantities must be derived and implemented from the PMF, the NLL definition, and the chain rule, without using any shortcut formulas not justified by these base definitions.\n\n2. For the Categorical head, compute:\n   - The gradient vector with respect to the logits $\\mathbf{z}$, and report its Euclidean magnitude $\\lVert \\nabla_{\\mathbf{z}} \\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y}) \\rVert_{2}$.\n   - The curvature magnitude defined as the largest eigenvalue of the Hessian matrix $\\nabla^{2}_{\\mathbf{z}} \\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y})$.\n   Both quantities must be derived and implemented from the PMF, the NLL definition, and the chain rule, and the softmax Jacobian, without using any shortcut formulas not justified by these base definitions.\n\nImbalanced labels are defined here in terms of predictions strongly favoring the majority class while the observed label belongs to a minority class. You will quantify how the PMF form (Bernoulli versus Categorical) changes both the gradient magnitude and an appropriate curvature measure under such conditions.\n\nTest Suite:\nUse zero-based indexing for class labels in the Categorical head. Implement the following six test cases.\n\n- Bernoulli head (binary):\n  1. Case B1 (rare positive, moderately confident negative prediction): $y = 1$, $z = -2.197224577$ (which corresponds to $p \\approx 0.1$).\n  2. Case B2 (rare positive, extremely confident negative prediction): $y = 1$, $z = -6.907755278$ (which corresponds to $p \\approx 0.001$).\n  3. Case B3 (majority negative, confident positive prediction): $y = 0$, $z = 2.197224577$ (which corresponds to $p \\approx 0.9$).\n\n- Categorical head (multi-class):\n  4. Case C1 (rare class index $3$ against majority-biased logits): $K = 5$, $y = 3$, $\\mathbf{z} = [2.0, 1.0, 0.5, -1.0, -2.0]$.\n  5. Case C2 (rare class index $2$, extreme majority bias): $K = 3$, $y = 2$, $\\mathbf{z} = [5.0, 0.0, -2.5]$.\n  6. Case C3 (balanced prediction, uniform logits): $K = 4$, $y = 1$, $\\mathbf{z} = [0.0, 0.0, 0.0, 0.0]$.\n\nAnswer Specification:\n- For each test case, output two floats: first the gradient magnitude and second the curvature magnitude (as defined above).\n- The final program output must be a single line containing all $12$ floats in order, aggregated as a comma-separated list enclosed in square brackets, following the order of the six test cases: B1, B2, B3, C1, C2, C3. That is, the output format must be exactly `` `[g_B1,c_B1,g_B2,c_B2,g_B3,c_B3,g_C1,c_C1,g_C2,c_C2,g_C3,c_C3]` ``.\n- No physical units are involved in this problem. Angles are not applicable.\n\nYour program must be complete and runnable as specified, take no external input, and compute these quantities directly from the definitions. Ensure numerical stability where appropriate.",
            "solution": "The user has requested a detailed analysis of the negative log-likelihood (NLL) loss function and its derivatives for Bernoulli and Categorical distributions, which are commonly used as output heads in deep learning models for classification tasks. The analysis will be performed from first principles, starting with the probability mass functions (PMF) and the link functions (logistic and softmax) that map real-valued logits to probabilities.\n\n### 1. Bernoulli Head for Binary Classification\n\nFor a binary classification problem, the label is $y \\in \\{0, 1\\}$. The model predicts a probability $p \\in (0,1)$ of the label being $1$.\n\n**1.1. Loss Function Derivation**\nThe Bernoulli PMF is given by:\n$$P(Y=y \\mid p) = p^{y} (1-p)^{1-y}$$\nThe NLL loss, $\\mathcal{L}_{\\mathrm{B}}$, is the negative logarithm of the PMF:\n$$\\mathcal{L}_{\\mathrm{B}}(p; y) = -\\log(p^{y} (1-p)^{1-y}) = -[y \\log(p) + (1-y)\\log(1-p)]$$\nThe probability $p$ is obtained from a scalar logit $z \\in \\mathbb{R}$ via the logistic (sigmoid) function $\\sigma(z)$:\n$$p = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\nSubstituting this into the loss function gives $\\mathcal{L}_{\\mathrm{B}}(z; y)$.\n\n**1.2. Gradient Calculation**\nTo find the gradient of the loss with respect to the logit $z$, we use the chain rule: $\\frac{d\\mathcal{L}_{\\mathrm{B}}}{dz} = \\frac{d\\mathcal{L}_{\\mathrm{B}}}{dp} \\frac{dp}{dz}$.\n\nFirst, we find the derivative of the loss with respect to the probability $p$:\n$$\\frac{d\\mathcal{L}_{\\mathrm{B}}}{dp} = -\\left[ \\frac{y}{p} - \\frac{1-y}{1-p} \\right] = \\frac{1-y}{1-p} - \\frac{y}{p} = \\frac{p(1-y) - y(1-p)}{p(1-p)} = \\frac{p - py - y + py}{p(1-p)} = \\frac{p-y}{p(1-p)}$$\nNext, we find the derivative of the logistic function with respect to $z$:\n$$\\frac{dp}{dz} = \\frac{d}{dz} (1 + e^{-z})^{-1} = -1(1 + e^{-z})^{-2}(-e^{-z}) = \\frac{e^{-z}}{(1+e^{-z})^2} = \\frac{1}{1+e^{-z}}\\frac{e^{-z}}{1+e^{-z}} = \\sigma(z)(1-\\sigma(z)) = p(1-p)$$\nCombining these results yields the gradient:\n$$\\frac{d\\mathcal{L}_{\\mathrm{B}}}{dz} = \\left( \\frac{p-y}{p(1-p)} \\right) \\cdot (p(1-p)) = p - y = \\sigma(z) - y$$\nThe gradient is a scalar, so its Euclidean magnitude is its absolute value:\n$$\\lVert \\nabla_{z} \\mathcal{L}_{\\mathrm{B}}(z; y) \\rVert_{2} = |\\sigma(z) - y|$$\n\n**1.3. Curvature Calculation**\nThe curvature for the Bernoulli case is defined as the second derivative of the loss with respect to the logit $z$:\n$$\\frac{d^{2}\\mathcal{L}_{\\mathrm{B}}}{dz^{2}} = \\frac{d}{dz} \\left( \\frac{d\\mathcal{L}_{\\mathrm{B}}}{dz} \\right) = \\frac{d}{dz}(\\sigma(z) - y) = \\frac{d}{dz}\\sigma(z)$$\nUsing the derivative of the logistic function we found earlier:\n$$\\frac{d^{2}\\mathcal{L}_{\\mathrm{B}}}{dz^{2}} = \\sigma(z)(1-\\sigma(z)) = p(1-p)$$\n\n### 2. Categorical Head for Multi-Class Classification\n\nFor a multi-class classification problem with $K$ classes, the label is represented by a one-hot vector $\\mathbf{y} \\in \\{0, 1\\}^K$, where exactly one component is $1$. The model predicts a probability vector $\\mathbf{q} \\in \\Delta^{K-1}$ (the probability simplex).\n\n**2.1. Loss Function Derivation**\nThe Categorical PMF is given by:\n$$P(\\mathbf{Y}=\\mathbf{y} \\mid \\mathbf{q}) = \\prod_{i=1}^{K} q_{i}^{y_{i}}$$\nThe NLL loss, $\\mathcal{L}_{\\mathrm{C}}$, is the negative logarithm of the PMF:\n$$\\mathcal{L}_{\\mathrm{C}}(\\mathbf{q}; \\mathbf{y}) = -\\log\\left(\\prod_{i=1}^{K} q_{i}^{y_{i}}\\right) = -\\sum_{i=1}^{K} y_i \\log(q_i)$$\nSince $\\mathbf{y}$ is one-hot, if the true class is $c$, then $y_c = 1$ and $y_i=0$ for $i \\neq c$. The loss simplifies to the standard cross-entropy loss:\n$$\\mathcal{L}_{\\mathrm{C}} = -\\log(q_c)$$\nThe probability vector $\\mathbf{q}$ is obtained from a logit vector $\\mathbf{z} \\in \\mathbb{R}^K$ via the softmax function:\n$$q_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n\n**2.2. Gradient Calculation**\nTo find the gradient of the loss with respect to the logit vector $\\mathbf{z}$, we compute the partial derivative $\\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial z_k}$ for each component $k$.\n$$\\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial z_k} = \\sum_{i=1}^{K} \\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial q_i} \\frac{\\partial q_i}{\\partial z_k}$$\nThe derivative of the loss with respect to $q_i$ is $\\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial q_i} = -\\frac{y_i}{q_i}$.\nThe partial derivative of the softmax function, which forms the Jacobian matrix $J_{ik} = \\frac{\\partial q_i}{\\partial z_k}$, is:\n$$\\frac{\\partial q_i}{\\partial z_k} = \\begin{cases} q_i(1-q_i)  \\text{if } i=k \\\\ -q_i q_k  \\text{if } i \\neq k \\end{cases} \\quad \\text{or concisely, } \\quad \\frac{\\partial q_i}{\\partial z_k} = q_i(\\delta_{ik} - q_k)$$\nwhere $\\delta_{ik}$ is the Kronecker delta.\nCombining these, we get:\n$$\\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial z_k} = \\sum_{i=1}^{K} \\left(-\\frac{y_i}{q_i}\\right) (q_i(\\delta_{ik} - q_k)) = -\\sum_{i=1}^{K} y_i(\\delta_{ik} - q_k) = - (y_k - q_k \\sum_{i=1}^{K} y_i)$$\nSince $\\mathbf{y}$ is a one-hot vector, $\\sum_{i=1}^{K} y_i = 1$. Thus, the partial derivative simplifies to:\n$$\\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial z_k} = q_k - y_k$$\nThe gradient vector is therefore $\\nabla_{\\mathbf{z}} \\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y}) = \\mathbf{q} - \\mathbf{y}$. The Euclidean magnitude is:\n$$\\lVert \\nabla_{\\mathbf{z}} \\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y}) \\rVert_{2} = \\lVert \\mathbf{q} - \\mathbf{y} \\rVert_{2} = \\sqrt{\\sum_{i=1}^{K} (q_i - y_i)^2}$$\n\n**2.3. Curvature Calculation**\nThe curvature for the Categorical case is defined as the largest eigenvalue of the Hessian matrix $\\mathbf{H} = \\nabla^{2}_{\\mathbf{z}} \\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y})$. The elements of the Hessian, $H_{k\\ell}$, are:\n$$H_{k\\ell} = \\frac{\\partial}{\\partial z_\\ell} \\left( \\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial z_k} \\right) = \\frac{\\partial}{\\partial z_\\ell} (q_k - y_k) = \\frac{\\partial q_k}{\\partial z_\\ell}$$\nThis is exactly the Jacobian of the softmax function derived earlier. So, the Hessian matrix is:\n$$\\mathbf{H} = \\begin{pmatrix} q_1(1-q_1)  -q_1 q_2  \\dots \\\\ -q_2 q_1  q_2(1-q_2)  \\dots \\\\ \\vdots  \\vdots  \\ddots \\end{pmatrix} \\quad \\text{or concisely, } \\quad \\mathbf{H} = \\text{diag}(\\mathbf{q}) - \\mathbf{q}\\mathbf{q}^T$$\nThis matrix is the covariance matrix of a Categorical distribution. It is a real symmetric positive semi-definite matrix. The curvature is its largest eigenvalue, $\\lambda_{\\max}(\\mathbf{H})$. This will be computed numerically.\n\n### 3. Summary of Formulas for Implementation\n\n-   **Bernoulli Head:**\n    -   $p = 1 / (1 + e^{-z})$\n    -   Gradient magnitude: $|p - y|$\n    -   Curvature: $p(1-p)$\n\n-   **Categorical Head:**\n    -   Stable softmax: $z'_i = z_i - \\max(\\mathbf{z})$, then $q_i = e^{z'_i} / \\sum_j e^{z'_j}$\n    -   One-hot label: $\\mathbf{y}$ from index $y_{idx}$\n    -   Gradient magnitude: $\\sqrt{\\sum_{i=1}^{K} (q_i - y_i)^2}$\n    -   Hessian: $\\mathbf{H} = \\text{diag}(\\mathbf{q}) - \\mathbf{q}\\mathbf{q}^T$\n    -   Curvature: $\\lambda_{\\max}(\\mathbf{H})$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes gradient magnitude and curvature for Bernoulli and Categorical NLL loss\n    functions based on specified test cases.\n    \"\"\"\n\n    # --- Helper Functions ---\n\n    def bernoulli_analysis(y, z):\n        \"\"\"\n        Computes gradient magnitude and curvature for the Bernoulli NLL loss.\n        \n        Args:\n            y (int): The true label (0 or 1).\n            z (float): The logit.\n            \n        Returns:\n            A tuple (gradient_magnitude, curvature).\n        \"\"\"\n        # Calculate probability p = sigma(z)\n        p = 1.0 / (1.0 + np.exp(-z))\n        \n        # Gradient of NLL w.r.t. z is p - y\n        grad_scalar = p - y\n        \n        # Gradient magnitude is the absolute value of the scalar gradient\n        grad_magnitude = np.abs(grad_scalar)\n        \n        # Curvature (second derivative) is p * (1 - p)\n        curvature = p * (1.0 - p)\n        \n        return grad_magnitude, curvature\n\n    def categorical_analysis(y_idx, z_vec):\n        \"\"\"\n        Computes gradient magnitude and curvature for the Categorical NLL loss.\n        \n        Args:\n            y_idx (int): The index of the true class.\n            z_vec (np.ndarray): The vector of logits.\n            \n        Returns:\n            A tuple (gradient_magnitude, curvature).\n        \"\"\"\n        K = len(z_vec)\n        \n        # 1. Compute probability vector q using a stable softmax\n        z_shifted = z_vec - np.max(z_vec)\n        exps = np.exp(z_shifted)\n        q = exps / np.sum(exps)\n        \n        # 2. Create one-hot label vector y\n        y_vec = np.zeros(K)\n        y_vec[y_idx] = 1.0\n        \n        # 3. Compute gradient vector and its magnitude\n        # Gradient of NLL w.r.t. z is q - y\n        grad_vec = q - y_vec\n        grad_magnitude = np.linalg.norm(grad_vec)\n        \n        # 4. Construct the Hessian matrix H = diag(q) - q*q^T\n        # hessian = np.diag(q) - np.outer(q, q)\n        # Using a numerically stable way to compute eigenvalues of I-uu^T style matrices is not needed here\n        # standard computation is fine given the problem constraints and numpy's robustness.\n        q_diag = np.diag(q)\n        q_outer = np.outer(q, q)\n        hessian = q_diag - q_outer\n\n\n        # 5. Compute eigenvalues of the Hessian\n        # The Hessian is real and symmetric, use eigh for efficiency and stability.\n        # eigh returns eigenvalues in ascending order.\n        eigenvalues = np.linalg.eigh(hessian)[0]\n        \n        # 6. Curvature is the largest eigenvalue\n        curvature = eigenvalues[-1]\n        \n        return grad_magnitude, curvature\n\n    # --- Test Cases ---\n    \n    test_cases = [\n        # Type, (parameters...)\n        ('B', (1, -2.197224577)),     # Case B1\n        ('B', (1, -6.907755278)),     # Case B2\n        ('B', (0, 2.197224577)),      # Case B3\n        ('C', (3, np.array([2.0, 1.0, 0.5, -1.0, -2.0]))), # Case C1\n        ('C', (2, np.array([5.0, 0.0, -2.5]))),          # Case C2\n        ('C', (1, np.array([0.0, 0.0, 0.0, 0.0]))),       # Case C3\n    ]\n\n    results = []\n    for case_type, params in test_cases:\n        if case_type == 'B':\n            y, z = params\n            grad_mag, curv = bernoulli_analysis(y, z)\n        elif case_type == 'C':\n            y_idx, z_vec = params\n            grad_mag, curv = categorical_analysis(y_idx, z_vec)\n        \n        results.extend([grad_mag, curv])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.7f}' for x in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond single-point predictions, modern regression models can output an entire predictive probability distribution, typically as a Cumulative Distribution Function (CDF). But how can we determine if these predicted distributions are accurate and reliable? This exercise introduces a powerful technique based on the Probability Integral Transform (PIT), a classic result from probability theory. You will use the PIT to transform your model's predictive CDFs into values that should be uniformly distributed, and then statistically test this uniformity to assess whether the model is well-calibrated, offering a robust method to evaluate the trustworthiness of a model's uncertainty estimates .",
            "id": "3166216",
            "problem": "You are given a conditional regression setting used in deep learning where a model parameterized by $\\theta$ outputs a predictive Cumulative Distribution Function (CDF) $F_{\\theta}(y \\mid x)$ for a continuous target variable $Y$ given features $X$. The Probability Integral Transform (PIT) maps an observation to $u = F_{\\theta}(Y \\mid X)$. If the predictive distribution is correctly calibrated, then $U = F_{\\theta}(Y \\mid X)$ is conditionally uniform on $[0,1]$, and hence unconditionally uniform on $[0,1]$. Your task is to implement a calibration check by testing the uniformity of $U$ using a distribution-free hypothesis test.\n\nStart from the following fundamental base:\n- A Probability Density Function (PDF) $f(y)$ satisfies $\\int_{-\\infty}^{\\infty} f(y) \\, dy = 1$ and $f(y) \\ge 0$.\n- A Probability Mass Function (PMF) $p(y)$ satisfies $\\sum_{y} p(y) = 1$ and $p(y) \\ge 0$.\n- A Cumulative Distribution Function (CDF) $F(y)$ for a continuous random variable satisfies $F(y) = \\int_{-\\infty}^{y} f(t) \\, dt$, is non-decreasing, right-continuous, and $\\lim_{y \\to -\\infty} F(y) = 0$, $\\lim_{y \\to \\infty} F(y) = 1$.\n- For conditional distributions, the conditional CDF $F(y \\mid x)$ obeys the same properties for each fixed $x$.\n- The Probability Integral Transform (PIT) states that if $Y \\mid X$ has CDF $F(y \\mid x)$ that is continuous and strictly increasing in $y$ for each fixed $x$, then $U = F(Y \\mid X)$ is conditionally uniform on $[0,1]$ given $X$, and thus uniform on $[0,1]$ unconditionally.\n\nYou must implement the following calibration check for a Gaussian predictive model in four test cases. In each case, generate data by first sampling $n$ independent features $X_i$ and then sampling targets $Y_i$ from a ground-truth conditional Normal distribution. Evaluate the predictive CDF $F_{\\theta}(Y_i \\mid X_i)$ of the model and compute the PIT values $U_i = F_{\\theta}(Y_i \\mid X_i)$. Then perform a Kolmogorov–Smirnov (KS) test of the empirical $\\{U_i\\}$ against the standard Uniform distribution on $[0,1]$ to obtain a $p$-value. Finally, for each case, return a boolean indicating whether calibration \"passes,\" defined as the event that the KS test does not reject uniformity at significance level $\\alpha$ (i.e., $p \\ge \\alpha$).\n\nUse the following parameterized test suite. In all cases, use $n = 500$ samples and set the significance level to $\\alpha = 0.05$. Let $\\Phi$ denote the standard Normal CDF, and for each case, use the Gaussian predictive CDF $F_{\\theta}(y \\mid x) = \\Phi\\!\\big((y - \\mu_{\\theta}(x))/\\sigma_{\\theta}(x)\\big)$ computed with the specified predictive mean $\\mu_{\\theta}(x)$ and predictive standard deviation $\\sigma_{\\theta}(x)$.\n\n- Case $1$ (well-calibrated \"happy path\"):\n  - Feature generation: $X \\sim \\mathcal{N}(0,1)$.\n  - Ground truth: $Y \\mid X=x \\sim \\mathcal{N}(a_{\\text{true}} x + b_{\\text{true}}, \\sigma_{\\text{true}}^2)$ with $a_{\\text{true}} = 1.0$, $b_{\\text{true}} = 0.5$, $\\sigma_{\\text{true}} = 0.8$.\n  - Predictive model: $\\mu_{\\theta}(x) = a_{\\text{pred}} x + b_{\\text{pred}}$ with $a_{\\text{pred}} = 1.0$, $b_{\\text{pred}} = 0.5$; $\\sigma_{\\theta}(x) = \\sigma_{\\text{pred}}$ with $\\sigma_{\\text{pred}} = 0.8$.\n\n- Case $2$ (variance miscalibration: predictive variance too small):\n  - Feature generation: $X \\sim \\mathcal{N}(0,1)$.\n  - Ground truth: $Y \\mid X=x \\sim \\mathcal{N}(a_{\\text{true}} x + b_{\\text{true}}, \\sigma_{\\text{true}}^2)$ with $a_{\\text{true}} = 1.0$, $b_{\\text{true}} = 0.5$, $\\sigma_{\\text{true}} = 1.2$.\n  - Predictive model: $\\mu_{\\theta}(x) = a_{\\text{pred}} x + b_{\\text{pred}}$ with $a_{\\text{pred}} = 1.0$, $b_{\\text{pred}} = 0.5$; $\\sigma_{\\theta}(x) = \\sigma_{\\text{pred}}$ with $\\sigma_{\\text{pred}} = 0.6$.\n\n- Case $3$ (mean miscalibration: predictive bias):\n  - Feature generation: $X \\sim \\mathcal{N}(0,1)$.\n  - Ground truth: $Y \\mid X=x \\sim \\mathcal{N}(a_{\\text{true}} x + b_{\\text{true}}, \\sigma_{\\text{true}}^2)$ with $a_{\\text{true}} = 1.0$, $b_{\\text{true}} = 0.5$, $\\sigma_{\\text{true}} = 0.8$.\n  - Predictive model: $\\mu_{\\theta}(x) = a_{\\text{pred}} x + b_{\\text{pred}}$ with $a_{\\text{pred}} = 1.0$, $b_{\\text{pred}} = -0.5$; $\\sigma_{\\theta}(x) = \\sigma_{\\text{pred}}$ with $\\sigma_{\\text{pred}} = 0.8$.\n\n- Case $4$ (heteroscedastic miscalibration: true noise depends on $x$ but predictive noise is constant):\n  - Feature generation: $X \\sim \\mathcal{N}(0,1)$.\n  - Ground truth: $Y \\mid X=x \\sim \\mathcal{N}(a_{\\text{true}} x + b_{\\text{true}}, \\sigma_{\\text{true}}(x)^2)$ with $a_{\\text{true}} = 1.0$, $b_{\\text{true}} = 0.5$, and $\\sigma_{\\text{true}}(x) = 0.5 + 0.7 \\lvert x \\rvert$.\n  - Predictive model: $\\mu_{\\theta}(x) = a_{\\text{pred}} x + b_{\\text{pred}}$ with $a_{\\text{pred}} = 1.0$, $b_{\\text{pred}} = 0.5$; $\\sigma_{\\theta}(x) = \\sigma_{\\text{pred}}$ with $\\sigma_{\\text{pred}} = 0.8$.\n\nYour program must:\n- Use a fixed random seed for reproducibility.\n- For each case, generate $\\{(X_i, Y_i)\\}_{i=1}^{n}$, compute $U_i = F_{\\theta}(Y_i \\mid X_i)$, perform the Kolmogorov–Smirnov (KS) test against the Uniform distribution on $[0,1]$, and decide pass/fail at the significance level $\\alpha$.\n- Produce a single line of output containing the four boolean results in order of the cases, as a comma-separated list enclosed in square brackets (for example, `` `[True,False,True,False]` ``). No other text should be printed.\n\nAll quantities and parameters above are dimensionless; no physical units are involved. The returned booleans are the final answers for the test suite.",
            "solution": "The user has provided a valid problem statement.\n\nThe problem asks for an implementation of a calibration check for a series of probabilistic regression models. The core principle being tested is the Probability Integral Transform (PIT), a fundamental tool for evaluating the calibration of predictive distributions.\n\nAccording to the PIT, if a continuous random variable $Y$ has a cumulative distribution function (CDF) $F_Y(y)$, then the random variable $U = F_Y(Y)$ follows a standard uniform distribution on the interval $[0, 1]$. This principle extends to conditional settings. If a predictive model provides a conditional CDF $F_{\\theta}(y \\mid x)$ for a target $Y$ given features $X$, and this model is perfectly calibrated (i.e., $F_{\\theta}$ is the true conditional CDF of $Y$ given $X$), then the transformed variable $U = F_{\\theta}(Y \\mid X)$ will be uniformly distributed on $[0, 1]$.\n\nWe can test for miscalibration by generating a set of PIT values $\\{U_i\\}_{i=1}^n$ from a dataset of observations $\\{(X_i, Y_i)\\}_{i=1}^n$ and then performing a statistical test for uniformity on the sample $\\{U_i\\}$. A common choice for this is the distribution-free Kolmogorov-Smirnov (KS) test. The null hypothesis, $H_0$, of the KS test is that the PIT values are drawn from a standard uniform distribution. If we obtain a small $p$-value (typically less than a chosen significance level $\\alpha$), we reject $H_0$ and conclude that the model is miscalibrated. If the $p$-value is not small ($p \\ge \\alpha$), we do not have sufficient evidence to reject the hypothesis of good calibration.\n\nThe procedure for each of the four test cases is as follows:\n1.  **Set Global Parameters**: Fix the sample size to $n=500$ and the significance level for the hypothesis test to $\\alpha=0.05$. A fixed random seed is used to ensure reproducibility.\n2.  **Generate Data**: For each case, we simulate a dataset of size $n$.\n    a. We first draw $n$ independent samples of the feature, $X_i$, from a standard normal distribution, i.e., $X_i \\sim \\mathcal{N}(0, 1)$.\n    b. For each $X_i$, we draw a corresponding target value $Y_i$ from the specified \"ground-truth\" conditional normal distribution. The mean and variance of this distribution are functions of $X_i$ as defined in each case.\n3.  **Compute PIT Values**: For each data pair $(X_i, Y_i)$, we calculate the PIT value $U_i = F_{\\theta}(Y_i \\mid X_i)$. The predictive model's CDF, $F_{\\theta}$, is given as a Gaussian distribution, $F_{\\theta}(y \\mid x) = \\Phi\\left(\\frac{y - \\mu_{\\theta}(x)}{\\sigma_{\\theta}(x)}\\right)$, where $\\Phi$ is the standard normal CDF, and $\\mu_{\\theta}(x)$ and $\\sigma_{\\theta}(x)$ are the predictive mean and standard deviation specified for the case.\n4.  **Perform KS Test**: The resulting sample of PIT values, $\\{U_i\\}_{i=1}^n$, is tested for uniformity using the one-sample Kolmogorov-Smirnov test against the theoretical standard uniform distribution $\\mathcal{U}[0, 1]$. This test yields a $p$-value.\n5.  **Make a Decision**: We compare the $p$-value to the significance level $\\alpha$. The calibration is considered to \"pass\" (returning `True`) if the null hypothesis is not rejected, i.e., if $p \\ge \\alpha$. Otherwise, the model is deemed miscalibrated and \"fails\" (returning `False`).\n\nWe now apply this procedure to each of the four cases.\n\n**Case 1: Well-calibrated model**\n- The predictive model's parameters ($\\mu_{\\theta}(x) = 1.0x + 0.5$, $\\sigma_{\\theta}(x) = 0.8$) perfectly match the parameters of the ground-truth data generating process.\n- Therefore, $F_{\\theta}(y \\mid x)$ is the true conditional CDF.\n- By the PIT property, the resulting sample $\\{U_i\\}$ should be indistinguishable from a standard uniform sample.\n- We expect the KS test to yield a high $p$-value, leading to the conclusion that the calibration passes ($p \\ge 0.05$).\n\n**Case 2: Variance miscalibration**\n- The predictive model uses $\\sigma_{\\theta}(x) = 0.6$, while the true data has $\\sigma_{\\text{true}} = 1.2$.\n- The model's predictive variance is too small, making it overconfident. The observed values $Y_i$ will tend to be in the tails of the narrow predictive distributions.\n- The standardized residuals $Z_i = (Y_i - \\mu_{\\theta}(X_i))/\\sigma_{\\theta}(X_i)$ will have a variance greater than $1$.\n- Applying the standard normal CDF, $\\Phi(Z_i)$, will produce PIT values $U_i$ that cluster near $0$ and $1$, forming a U-shaped distribution. This is not uniform.\n- We expect the KS test to detect this deviation, yield a very small $p$-value, and thus fail the calibration check ($p  0.05$).\n\n**Case 3: Mean miscalibration**\n- The predictive model uses a biased mean, $\\mu_{\\theta}(x) = 1.0x - 0.5$, while the true mean is $\\mu_{\\text{true}}(x) = 1.0x + 0.5$. The model is systematically biased low by $1.0$.\n- The standardized residuals $Z_i = (Y_i - \\mu_{\\theta}(X_i))/\\sigma_{\\theta}(X_i)$ will have a non-zero mean. Specifically, the mean of $Z_i$ will be $(\\mu_{\\text{true}}(X_i) - \\mu_{\\theta}(X_i)) / \\sigma_{\\theta}(X_i) = (1.0x_i + 0.5 - (1.0x_i - 0.5)) / 0.8 = 1.0 / 0.8 = 1.25$.\n- The PIT values $U_i = \\Phi(Z_i)$ will be drawn from a distribution skewed towards $1$. This is not uniform.\n- We expect the KS test to reject uniformity, resulting in a failure ($p  0.05$).\n\n**Case 4: Heteroscedastic miscalibration**\n- The true data-generating process has a standard deviation that depends on the feature, $\\sigma_{\\text{true}}(x) = 0.5 + 0.7|x|$. This is a heteroscedastic model.\n- The predictive model wrongly assumes a constant standard deviation (homoscedasticity) with $\\sigma_{\\theta}(x) = 0.8$.\n- The mean is correctly specified, so the standardized residuals $Z_i = (Y_i - \\mu_{\\theta}(X_i))/\\sigma_{\\theta}(X_i)$ have a mean of $0$. However, their conditional variance is $\\text{Var}(Z_i \\mid X_i) = \\sigma_{\\text{true}}(X_i)^2 / \\sigma_{\\theta}(X_i)^2 = (0.5 + 0.7|X_i|)^2 / 0.8^2$.\n- This variance is not constant and not equal to $1$. The unconditional distribution of the collected $\\{Z_i\\}$ will be a mixture of normal distributions with different variances, which is not a standard normal distribution. Typically, it will be leptokurtic (heavier tails).\n- The PIT values $U_i = \\Phi(Z_i)$ will again form a non-uniform, U-shaped distribution.\n- We expect the KS test to detect this misspecification and fail the calibration check ($p  0.05$).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, kstest\n\ndef solve():\n    \"\"\"\n    Performs a calibration check for four probabilistic regression model scenarios.\n\n    For each case, it generates data, computes Probability Integral Transform (PIT)\n    values using a predictive model, and performs a Kolmogorov-Smirnov (KS) test\n    for uniformity. The result is a boolean indicating if the calibration check\n    \"passes\" (i.e., the KS test p-value is greater than or equal to alpha).\n    \"\"\"\n\n    # Global parameters\n    n = 500  # Sample size\n    alpha = 0.05  # Significance level\n    random_seed = 42\n\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Define the four test cases using parameter dictionaries\n    test_cases = [\n        # Case 1 (well-calibrated \"happy path\")\n        {\n            \"description\": \"Well-calibrated\",\n            \"a_true\": 1.0, \"b_true\": 0.5, \"sigma_true_func\": lambda x: np.full_like(x, 0.8),\n            \"a_pred\": 1.0, \"b_pred\": 0.5, \"sigma_pred_const\": 0.8,\n        },\n        # Case 2 (variance miscalibration: predictive variance too small)\n        {\n            \"description\": \"Variance miscalibrated (underestimated)\",\n            \"a_true\": 1.0, \"b_true\": 0.5, \"sigma_true_func\": lambda x: np.full_like(x, 1.2),\n            \"a_pred\": 1.0, \"b_pred\": 0.5, \"sigma_pred_const\": 0.6,\n        },\n        # Case 3 (mean miscalibration: predictive bias)\n        {\n            \"description\": \"Mean miscalibrated (bias)\",\n            \"a_true\": 1.0, \"b_true\": 0.5, \"sigma_true_func\": lambda x: np.full_like(x, 0.8),\n            \"a_pred\": 1.0, \"b_pred\": -0.5, \"sigma_pred_const\": 0.8,\n        },\n        # Case 4 (heteroscedastic miscalibration)\n        {\n            \"description\": \"Heteroscedastic miscalibrated\",\n            \"a_true\": 1.0, \"b_true\": 0.5, \"sigma_true_func\": lambda x: 0.5 + 0.7 * np.abs(x),\n            \"a_pred\": 1.0, \"b_pred\": 0.5, \"sigma_pred_const\": 0.8,\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Step 1: Generate data\n        # Generate features X from a standard normal distribution\n        X = np.random.normal(loc=0.0, scale=1.0, size=n)\n\n        # Generate targets Y from the ground-truth conditional distribution\n        mu_true = case[\"a_true\"] * X + case[\"b_true\"]\n        sigma_true = case[\"sigma_true_func\"](X)\n        Y = np.random.normal(loc=mu_true, scale=sigma_true)\n\n        # Step 2: Compute PIT values using the predictive model\n        # Calculate predictive mean and standard deviation\n        mu_pred = case[\"a_pred\"] * X + case[\"b_pred\"]\n        sigma_pred = case[\"sigma_pred_const\"]\n\n        # Standardize the observations according to the predictive model\n        z_scores = (Y - mu_pred) / sigma_pred\n\n        # Apply the standard normal CDF to get PIT values\n        pit_values = norm.cdf(z_scores)\n\n        # Step 3: Perform Kolmogorov-Smirnov test for uniformity\n        # H0: The pit_values are drawn from a standard uniform distribution\n        ks_statistic, p_value = kstest(pit_values, 'uniform')\n\n        # Step 4: Decide if calibration passes\n        # Calibration passes if we do not reject H0 at significance level alpha\n        passes_calibration = (p_value = alpha)\n        results.append(passes_calibration)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Generative models like large language models operate by producing a Probability Mass Function (PMF) over a vast vocabulary of possible next tokens. However, naively sampling from this distribution often leads to incoherent or uninspired results. This practice dives into Nucleus Sampling, a sophisticated method that intelligently reshapes the model's output PMF by truncating its tail and renormalizing the probabilities. By implementing this technique, you will learn how to actively manipulate a PMF to control the trade-off between creativity and factuality, a core skill in the practical application of generative AI .",
            "id": "3166223",
            "problem": "Consider a discrete vocabulary of size $n$ with tokens indexed by $i \\in \\{1,2,\\dots,n\\}$. A language model defines a Probability Mass Function (PMF) $q$ over the vocabulary, where $q_i$ is the probability assigned to token $i$ and $\\sum_{i=1}^{n} q_i = 1$. Each token $i$ is associated with a token-level factuality score $s_i \\in [0,1]$, interpreted as an intrinsic estimate of factual correctness for that token. Categorical sampling draws tokens according to the PMF $q$. Nucleus (top-$p$) sampling is defined by sorting tokens in descending order of probability and selecting the smallest prefix set of tokens whose cumulative probability mass reaches or exceeds a threshold $p \\in [0,1]$; the PMF is then renormalized to this set to define a valid truncated PMF over the selected support.\n\nStarting from the definitions of a Probability Mass Function (PMF) and a Cumulative Distribution Function (CDF), and the requirement that a PMF renormalizes to unity after support truncation, implement nucleus sampling as a truncation-and-renormalization procedure on $q$ that yields a truncated PMF $r$ on the selected support. Quantify the following two quantities for the truncated PMF $r$:\n- Diversity, measured by Shannon entropy in natural units (nats).\n- Factuality, measured by the expected factuality score under $r$.\n\nIn this problem, you must derive from first principles how these quantities arise from the PMF $q$, the threshold $p$, and the factuality scores $s$, and then implement a program that computes them exactly without randomness.\n\nYour program should process the following test suite of cases. For each case, the inputs are $(q, s, p)$:\n- Case $1$ (happy path): $n=5$, $q=[0.50, 0.20, 0.12, 0.10, 0.08]$, $s=[0.40, 0.80, 0.30, 0.60, 0.90]$, $p=0.85$.\n- Case $2$ (near-zero threshold boundary): same $q$ and $s$ as Case $1$, $p=0.01$.\n- Case $3$ (ties and uniform distribution): $n=5$, $q=[0.20, 0.20, 0.20, 0.20, 0.20]$, $s=[0.90, 0.10, 0.50, 0.50, 0.10]$, $p=0.40$. When sorting by descending probability, break ties by ascending token index to ensure determinism.\n- Case $4$ (long tail, Zipf-like head, moderate threshold): $n=10$, $q_i = \\dfrac{1/i}{Z}$ for $i \\in \\{1,\\dots,10\\}$ where $Z=\\sum_{j=1}^{10} 1/j$, $s_i = 1 - 0.05 \\cdot i$, $p=0.90$.\n- Case $5$ (full distribution boundary): same $q$ and $s$ as Case $4$, $p=1.00$.\n\nEdge-case conventions for the truncation are as follows: clamp $p$ into $[0,1]$; for $p \\le 0$, choose only the single highest-probability token; for $p \\ge 1$, choose the full vocabulary. After truncation, renormalize the PMF to the selected support so it sums to $1$.\n\nFor each case, compute:\n- The Shannon entropy of $r$ in nats, rounded to $6$ decimal places.\n- The expected factuality $\\sum_i r_i s_i$, rounded to $6$ decimal places.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case’s result is a two-element list of the form $[H,F]$ with $H$ the entropy and $F$ the expected factuality. For example, the final output format must be `` `[[H_1,F_1],[H_2,F_2],[H_3,F_3],[H_4,F_4],[H_5,F_5]]` ``, with each numeric entry rounded to $6$ decimal places and no additional text.",
            "solution": "The problem requires the implementation of nucleus (top-$p$) sampling and the subsequent calculation of two key metrics for the resulting probability distribution: Shannon entropy (a measure of diversity) and expected factuality. The entire process must be derived from fundamental principles of probability theory.\n\nLet the discrete vocabulary be of size $n$, with tokens indexed by $i \\in \\{1, 2, \\dots, n\\}$. The language model provides an initial Probability Mass Function (PMF) $q$ where $q_i$ is the probability of token $i$, such that $q_i \\ge 0$ for all $i$ and $\\sum_{i=1}^{n} q_i = 1$. Each token $i$ is also associated with a factuality score $s_i \\in [0, 1]$. For implementation purposes, we will use zero-based indexing $i \\in \\{0, 1, \\dots, n-1\\}$.\n\n### Step 1: Formalizing Nucleus Sampling\n\nNucleus sampling truncates the vocabulary to a smaller set, the *nucleus*, based on a cumulative probability threshold $p \\in [0, 1]$. This involves three main steps: sorting, truncation, and renormalization.\n\n**1. Sorting:**\nThe tokens are sorted based on their probabilities in descending order. To ensure a deterministic outcome when probabilities are equal (a tie), the tie-breaking rule is to sort by the original token index in ascending order. Let $\\pi$ be a permutation of the indices $\\{0, 1, \\dots, n-1\\}$ such that $q_{\\pi(0)} \\ge q_{\\pi(1)} \\ge \\dots \\ge q_{\\pi(n-1)}$. If $q_{\\pi(j)} = q_{\\pi(j+1)}$ for some $j$, then it must be that $\\pi(j)  \\pi(j+1)$.\n\n**2. Truncation:**\nThe nucleus is the smallest set of tokens from the top of the sorted list whose cumulative probability mass is at least $p$. Let this nucleus of indices be denoted by $V_p$. The problem specifies edge-case conventions:\n- If $p \\ge 1$, the nucleus includes the entire vocabulary, so $V_p = \\{0, 1, \\dots, n-1\\}$.\n- If $p \\le 0$, the nucleus includes only the single highest-probability token, so $V_p = \\{\\pi(0)\\}$.\nFor $p \\in (0, 1)$, we find the smallest integer $k \\ge 1$ such that:\n$$ \\sum_{j=0}^{k-1} q_{\\pi(j)} \\ge p $$\nThe nucleus is then the set of the first $k$ indices in the sorted list: $V_p = \\{\\pi(0), \\pi(1), \\dots, \\pi(k-1)\\}$. This general rule also correctly covers the specified edge cases (after clamping $p$ to $[0, 1]$), as for $p \\le 0$ the condition is met with $k=1$ (since $q_{\\pi(0)} \\ge 0$), and for $p=1$ it requires summing all probabilities, leading to $k=n$.\n\n**3. Renormalization:**\nThe probabilities of the tokens in the nucleus $V_p$ must be rescaled to form a valid PMF, which we denote as $r$. The total probability mass of the nucleus is the normalization constant, $C_p$:\n$$ C_p = \\sum_{i \\in V_p} q_i $$\nThe renormalized probability $r_i$ for each token $i$ is defined as:\n$$ r_i = \\begin{cases} \\frac{q_i}{C_p}  \\text{if } i \\in V_p \\\\ 0  \\text{if } i \\notin V_p \\end{cases} $$\nBy construction, this new distribution $r$ is a valid PMF, as $r_i \\ge 0$ and:\n$$ \\sum_{i=0}^{n-1} r_i = \\sum_{i \\in V_p} r_i = \\sum_{i \\in V_p} \\frac{q_i}{C_p} = \\frac{1}{C_p} \\sum_{i \\in V_p} q_i = \\frac{C_p}{C_p} = 1 $$\n\n### Step 2: Derivation of Diversity and Factuality Metrics\n\nWith the renormalized PMF $r$ established, we can now define the required metrics.\n\n**1. Diversity (Shannon Entropy):**\nThe diversity of the distribution is measured by its Shannon entropy. For a discrete PMF, the entropy in natural units (nats) is defined using the natural logarithm ($\\ln$). The entropy of the distribution $r$, denoted $H(r)$, is:\n$$ H(r) = - \\sum_{i=0}^{n-1} r_i \\ln(r_i) $$\nSince $r_i=0$ for tokens not in the nucleus $V_p$, and the expression $x \\ln(x)$ is taken to be $0$ for $x=0$, the sum can be restricted to the support of $r$:\n$$ H(r) = - \\sum_{i \\in V_p} r_i \\ln(r_i) $$\nThis formula is directly used for computation after calculating the renormalized probabilities $r_i$.\n\n**2. Expected Factuality:**\nThe factuality of the distribution is the expected value of the token-level factuality scores $s_i$ under the PMF $r$. The expectation of a function $g(X)$ for a discrete random variable $X$ with PMF $p(x)$ is $E[g(X)] = \\sum_x g(x) p(x)$. Here, the random variable is the token choice, its PMF is $r$, and the function is the factuality score $s$. The expected factuality, denoted $F(r)$, is:\n$$ F(r) = E_r[s] = \\sum_{i=0}^{n-1} s_i r_i $$\nAs with entropy, since $r_i=0$ for $i \\notin V_p$, the sum is effectively over the nucleus:\n$$ F(r) = \\sum_{i \\in V_p} s_i r_i $$\nThis is a weighted average of the factuality scores of the tokens in the nucleus, where the weights are their renormalized probabilities. Substituting the definition of $r_i$:\n$$ F(r) = \\frac{1}{C_p} \\sum_{i \\in V_p} s_i q_i $$\n\nThe implementation will follow these derived steps and formulas to process each test case deterministically.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_metrics(q_in, s_in, p_in):\n    \"\"\"\n    Performs nucleus sampling and computes entropy and expected factuality.\n    \n    Args:\n        q_in (list or np.ndarray): The initial Probability Mass Function (PMF).\n        s_in (list or np.ndarray): The token-level factuality scores.\n        p_in (float): The nucleus sampling threshold.\n\n    Returns:\n        tuple[float, float]: A tuple containing the Shannon entropy and the\n                             expected factuality of the truncated distribution.\n    \"\"\"\n    \n    q = np.array(q_in, dtype=np.float64)\n    s = np.array(s_in, dtype=np.float64)\n    p = np.clip(p_in, 0.0, 1.0)\n    \n    n = len(q)\n    indices = np.arange(n)\n    \n    # Sort indices: primary key is probability q (desc), secondary is index (asc)\n    # np.lexsort sorts by the last row, then the second-to-last, etc.\n    # We want to sort by -q (descending q), then by index (ascending index).\n    sorted_indices = np.lexsort((indices, -q))\n    \n    # Handle edge cases for p\n    if p = 0.0:\n        # Choose only the single highest-probability token\n        k = 1\n    elif p = 1.0:\n        # Choose the full vocabulary\n        k = n\n    else:\n        # Find smallest k such that cumulative probability = p\n        sorted_q = q[sorted_indices]\n        cumulative_q = np.cumsum(sorted_q)\n        # Find the first index where the cumulative sum exceeds p\n        # np.searchsorted finds the insertion point, which corresponds to index.\n        # k should be index + 1\n        k_idx = np.searchsorted(cumulative_q, p, side='left')\n        k = k_idx + 1\n\n    # Define the nucleus set of indices\n    nucleus_indices = sorted_indices[:k]\n    \n    # Get probabilities and scores for the nucleus\n    q_nucleus = q[nucleus_indices]\n    s_nucleus = s[nucleus_indices]\n    \n    # Renormalize the PMF\n    norm_constant_cp = np.sum(q_nucleus)\n    \n    if norm_constant_cp = 0:\n        # This case occurs if the nucleus is empty (prevented by k=1) or\n        # if all selected probabilities are zero.\n        # Entropy of a single point mass at 0 is 0. Factuality is ill-defined, 0 is a safe bet.\n        return 0.0, 0.0\n        \n    r_nucleus = q_nucleus / norm_constant_cp\n    \n    # Calculate Shannon entropy (H)\n    # We must handle r_i=0 case where r_i*ln(r_i) is 0.\n    # np.log will produce -inf for 0, then 0*(-inf) is nan.\n    # We can filter out the zero probability terms before calculation.\n    non_zero_r = r_nucleus[r_nucleus > 0]\n    entropy = -np.sum(non_zero_r * np.log(non_zero_r))\n    \n    # Calculate expected factuality (F)\n    factuality = np.sum(r_nucleus * s_nucleus)\n    \n    # Round to 6 decimal places as required\n    return round(entropy, 6), round(factuality, 6)\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the calculations, and prints the results in the specified format.\n    \"\"\"\n    \n    # Case 1 (happy path)\n    q1 = [0.50, 0.20, 0.12, 0.10, 0.08]\n    s1 = [0.40, 0.80, 0.30, 0.60, 0.90]\n    p1 = 0.85\n\n    # Case 2 (near-zero threshold boundary)\n    q2 = q1\n    s2 = s1\n    p2 = 0.01\n\n    # Case 3 (ties and uniform distribution)\n    q3 = [0.20, 0.20, 0.20, 0.20, 0.20]\n    s3 = [0.90, 0.10, 0.50, 0.50, 0.10]\n    p3 = 0.40\n\n    test_cases_static = [\n        (q1, s1, p1),\n        (q2, s2, p2),\n        (q3, s3, p3),\n    ]\n    \n    results = []\n    for q, s, p in test_cases_static:\n        h, f = calculate_metrics(q, s, p)\n        results.append([h, f])\n\n    # Case 4 (long tail, Zipf-like head, moderate threshold)\n    n4 = 10\n    p4 = 0.90\n    indices4 = np.arange(1, n4 + 1)\n    q4_unnormalized = 1.0 / indices4\n    z4 = np.sum(q4_unnormalized)\n    q4 = q4_unnormalized / z4\n    s4 = 1.0 - 0.05 * indices4\n    h4, f4 = calculate_metrics(q4, s4, p4)\n    results.append([h4, f4])\n\n    # Case 5 (full distribution boundary)\n    p5 = 1.00\n    q5 = q4\n    s5 = s4\n    h5, f5 = calculate_metrics(q5, s5, p5)\n    results.append([h5, f5])\n\n    # Format the final output string\n    # e.g., [[1.170067,0.495652],[0.0,0.4],[0.693147,0.5],[1.813253,0.852825],[2.030364,0.805396]]\n    results_str = [f\"[{h:.6f},{f:.6f}]\" for h, f in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        }
    ]
}