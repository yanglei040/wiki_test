{
    "hands_on_practices": [
        {
            "introduction": "The probability mass function ($PMF$) is the cornerstone of classification in deep learning, directly defining the loss function that guides training. This exercise unpacks the ubiquitous cross-entropy loss by deriving it from the $PMF$s of the Bernoulli and Categorical distributions. By implementing this from first principles, you will discover how the shape of the $PMF$ influences the gradients and loss landscape, a crucial insight for diagnosing optimization issues like those caused by imbalanced data. ",
            "id": "3166266",
            "problem": "You will compare two output heads used in deep learning for classification tasks and quantify how the probability mass function (PMF) shape affects the negative log-likelihood and its derivatives under label imbalance. The two heads are: a Bernoulli head for binary classification and a Categorical head for multi-class classification. The comparison should be made in terms of the gradient magnitude with respect to the logits and a notion of curvature for the loss surface. The computation must start from the fundamental definitions of the Probability Mass Function (PMF), the Negative Log-Likelihood (NLL), and the mapping from logits to probabilities via standard link functions.\n\nThe base definitions to use are:\n- The Bernoulli PMF for a binary label $y \\in \\{0,1\\}$ parameterized by a probability $p \\in (0,1)$ is $P(Y=y \\mid p) = p^{y} (1-p)^{1-y}$. The logit $z \\in \\mathbb{R}$ maps to $p$ through the logistic function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$.\n- The Categorical PMF for one-hot labels $\\mathbf{y} \\in \\{0,1\\}^{K}$ with exactly one $1$ and $K-1$ zeros parameterized by a probability vector $\\mathbf{q} \\in \\Delta^{K-1}$ (the probability simplex) is $P(\\mathbf{Y}=\\mathbf{y} \\mid \\mathbf{q}) = \\prod_{i=1}^{K} q_{i}^{y_{i}}$. The logits $\\mathbf{z} \\in \\mathbb{R}^{K}$ map to $\\mathbf{q}$ through the softmax function $q_{i}(\\mathbf{z}) = \\frac{e^{z_{i}}}{\\sum_{j=1}^{K} e^{z_{j}}}$.\n\nDefine the Negative Log-Likelihood (NLL) loss functions as the negative logarithm of the PMF under these link functions. For the Bernoulli head, denote the loss by $\\mathcal{L}_{\\mathrm{B}}(z; y)$, and for the Categorical head, denote the loss by $\\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y})$.\n\nTasks:\n1. For the Bernoulli head, compute:\n   - The gradient with respect to the scalar logit $z$, and report its magnitude $\\lVert \\nabla_{z} \\mathcal{L}_{\\mathrm{B}}(z; y) \\rVert_{2}$.\n   - The curvature defined as the second derivative $\\frac{d^{2}\\mathcal{L}_{\\mathrm{B}}(z; y)}{dz^{2}}$.\n   Both quantities must be derived and implemented from the PMF, the NLL definition, and the chain rule, without using any shortcut formulas not justified by these base definitions.\n\n2. For the Categorical head, compute:\n   - The gradient vector with respect to the logits $\\mathbf{z}$, and report its Euclidean magnitude $\\lVert \\nabla_{\\mathbf{z}} \\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y}) \\rVert_{2}$.\n   - The curvature magnitude defined as the largest eigenvalue of the Hessian matrix $\\nabla^{2}_{\\mathbf{z}} \\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y})$.\n   Both quantities must be derived and implemented from the PMF, the NLL definition, and the chain rule, and the softmax Jacobian, without using any shortcut formulas not justified by these base definitions.\n\nImbalanced labels are defined here in terms of predictions strongly favoring the majority class while the observed label belongs to a minority class. You will quantify how the PMF form (Bernoulli versus Categorical) changes both the gradient magnitude and an appropriate curvature measure under such conditions.\n\nTest Suite:\nUse zero-based indexing for class labels in the Categorical head. Implement the following six test cases.\n\n- Bernoulli head (binary):\n  1. Case B1 (rare positive, moderately confident negative prediction): $y = 1$, $z = -2.197224577$ (which corresponds to $p \\approx 0.1$).\n  2. Case B2 (rare positive, extremely confident negative prediction): $y = 1$, $z = -6.907755278$ (which corresponds to $p \\approx 0.001$).\n  3. Case B3 (majority negative, confident positive prediction): $y = 0$, $z = 2.197224577$ (which corresponds to $p \\approx 0.9$).\n\n- Categorical head (multi-class):\n  4. Case C1 (rare class index $3$ against majority-biased logits): $K = 5$, $y = 3$, $\\mathbf{z} = [2.0, 1.0, 0.5, -1.0, -2.0]$.\n  5. Case C2 (rare class index $2$, extreme majority bias): $K = 3$, $y = 2$, $\\mathbf{z} = [5.0, 0.0, -2.5]$.\n  6. Case C3 (balanced prediction, uniform logits): $K = 4$, $y = 1$, $\\mathbf{z} = [0.0, 0.0, 0.0, 0.0]$.\n\nAnswer Specification:\n- For each test case, output two floats: first the gradient magnitude and second the curvature magnitude (as defined above).\n- The final program output must be a single line containing all $12$ floats in order, aggregated as a comma-separated list enclosed in square brackets, following the order of the six test cases: B1, B2, B3, C1, C2, C3. That is, the output format must be exactly \"[g_B1,c_B1,g_B2,c_B2,g_B3,c_B3,g_C1,c_C1,g_C2,c_C2,g_C3,c_C3]\".\n- No physical units are involved in this problem. Angles are not applicable.\n\nYour program must be complete and runnable as specified, take no external input, and compute these quantities directly from the definitions. Ensure numerical stability where appropriate.",
            "solution": "The user has requested a detailed analysis of the negative log-likelihood (NLL) loss function and its derivatives for Bernoulli and Categorical distributions, which are commonly used as output heads in deep learning models for classification tasks. The analysis will be performed from first principles, starting with the probability mass functions (PMF) and the link functions (logistic and softmax) that map real-valued logits to probabilities.\n\n### 1. Bernoulli Head for Binary Classification\n\nFor a binary classification problem, the label is $y \\in \\{0, 1\\}$. The model predicts a probability $p \\in (0,1)$ of the label being $1$.\n\n**1.1. Loss Function Derivation**\nThe Bernoulli PMF is given by:\n$$P(Y=y \\mid p) = p^{y} (1-p)^{1-y}$$\nThe NLL loss, $\\mathcal{L}_{\\mathrm{B}}$, is the negative logarithm of the PMF:\n$$\\mathcal{L}_{\\mathrm{B}}(p; y) = -\\log(p^{y} (1-p)^{1-y}) = -[y \\log(p) + (1-y)\\log(1-p)]$$\nThe probability $p$ is obtained from a scalar logit $z \\in \\mathbb{R}$ via the logistic (sigmoid) function $\\sigma(z)$:\n$$p = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\nSubstituting this into the loss function gives $\\mathcal{L}_{\\mathrm{B}}(z; y)$.\n\n**1.2. Gradient Calculation**\nTo find the gradient of the loss with respect to the logit $z$, we use the chain rule: $\\frac{d\\mathcal{L}_{\\mathrm{B}}}{dz} = \\frac{d\\mathcal{L}_{\\mathrm{B}}}{dp} \\frac{dp}{dz}$.\n\nFirst, we find the derivative of the loss with respect to the probability $p$:\n$$\\frac{d\\mathcal{L}_{\\mathrm{B}}}{dp} = -\\left[ \\frac{y}{p} - \\frac{1-y}{1-p} \\right] = \\frac{1-y}{1-p} - \\frac{y}{p} = \\frac{p(1-y) - y(1-p)}{p(1-p)} = \\frac{p - py - y + py}{p(1-p)} = \\frac{p-y}{p(1-p)}$$\nNext, we find the derivative of the logistic function with respect to $z$:\n$$\\frac{dp}{dz} = \\frac{d}{dz} (1 + e^{-z})^{-1} = -1(1 + e^{-z})^{-2}(-e^{-z}) = \\frac{e^{-z}}{(1+e^{-z})^2} = \\frac{1}{1+e^{-z}}\\frac{e^{-z}}{1+e^{-z}} = \\sigma(z)(1-\\sigma(z)) = p(1-p)$$\nCombining these results yields the gradient:\n$$\\frac{d\\mathcal{L}_{\\mathrm{B}}}{dz} = \\left( \\frac{p-y}{p(1-p)} \\right) \\cdot (p(1-p)) = p - y = \\sigma(z) - y$$\nThe gradient is a scalar, so its Euclidean magnitude is its absolute value:\n$$\\lVert \\nabla_{z} \\mathcal{L}_{\\mathrm{B}}(z; y) \\rVert_{2} = |\\sigma(z) - y|$$\n\n**1.3. Curvature Calculation**\nThe curvature for the Bernoulli case is defined as the second derivative of the loss with respect to the logit $z$:\n$$\\frac{d^{2}\\mathcal{L}_{\\mathrm{B}}}{dz^{2}} = \\frac{d}{dz} \\left( \\frac{d\\mathcal{L}_{\\mathrm{B}}}{dz} \\right) = \\frac{d}{dz}(\\sigma(z) - y) = \\frac{d}{dz}\\sigma(z)$$\nUsing the derivative of the logistic function we found earlier:\n$$\\frac{d^{2}\\mathcal{L}_{\\mathrm{B}}}{dz^{2}} = \\sigma(z)(1-\\sigma(z)) = p(1-p)$$\n\n### 2. Categorical Head for Multi-Class Classification\n\nFor a multi-class classification problem with $K$ classes, the label is represented by a one-hot vector $\\mathbf{y} \\in \\{0, 1\\}^K$, where exactly one component is $1$. The model predicts a probability vector $\\mathbf{q} \\in \\Delta^{K-1}$ (the probability simplex).\n\n**2.1. Loss Function Derivation**\nThe Categorical PMF is given by:\n$$P(\\mathbf{Y}=\\mathbf{y} \\mid \\mathbf{q}) = \\prod_{i=1}^{K} q_{i}^{y_{i}}$$\nThe NLL loss, $\\mathcal{L}_{\\mathrm{C}}$, is the negative logarithm of the PMF:\n$$\\mathcal{L}_{\\mathrm{C}}(\\mathbf{q}; \\mathbf{y}) = -\\log\\left(\\prod_{i=1}^{K} q_{i}^{y_{i}}\\right) = -\\sum_{i=1}^{K} y_i \\log(q_i)$$\nSince $\\mathbf{y}$ is one-hot, if the true class is $c$, then $y_c = 1$ and $y_i=0$ for $i \\neq c$. The loss simplifies to the standard cross-entropy loss:\n$$\\mathcal{L}_{\\mathrm{C}} = -\\log(q_c)$$\nThe probability vector $\\mathbf{q}$ is obtained from a logit vector $\\mathbf{z} \\in \\mathbb{R}^K$ via the softmax function:\n$$q_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n\n**2.2. Gradient Calculation**\nTo find the gradient of the loss with respect to the logit vector $\\mathbf{z}$, we compute the partial derivative $\\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial z_k}$ for each component $k$.\n$$\\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial z_k} = \\sum_{i=1}^{K} \\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial q_i} \\frac{\\partial q_i}{\\partial z_k}$$\nThe derivative of the loss with respect to $q_i$ is $\\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial q_i} = -\\frac{y_i}{q_i}$.\nThe partial derivative of the softmax function, which forms the Jacobian matrix $J_{ik} = \\frac{\\partial q_i}{\\partial z_k}$, is:\n$$\\frac{\\partial q_i}{\\partial z_k} = \\begin{cases} q_i(1-q_i) & \\text{if } i=k \\\\ -q_i q_k & \\text{if } i \\neq k \\end{cases} \\quad \\text{or concisely, } \\quad \\frac{\\partial q_i}{\\partial z_k} = q_i(\\delta_{ik} - q_k)$$\nwhere $\\delta_{ik}$ is the Kronecker delta.\nCombining these, we get:\n$$\\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial z_k} = \\sum_{i=1}^{K} \\left(-\\frac{y_i}{q_i}\\right) (q_i(\\delta_{ik} - q_k)) = -\\sum_{i=1}^{K} y_i(\\delta_{ik} - q_k) = - (y_k - q_k \\sum_{i=1}^{K} y_i)$$\nSince $\\mathbf{y}$ is a one-hot vector, $\\sum_{i=1}^{K} y_i = 1$. Thus, the partial derivative simplifies to:\n$$\\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial z_k} = q_k - y_k$$\nThe gradient vector is therefore $\\nabla_{\\mathbf{z}} \\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y}) = \\mathbf{q} - \\mathbf{y}$. The Euclidean magnitude is:\n$$\\lVert \\nabla_{\\mathbf{z}} \\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y}) \\rVert_{2} = \\lVert \\mathbf{q} - \\mathbf{y} \\rVert_{2} = \\sqrt{\\sum_{i=1}^{K} (q_i - y_i)^2}$$\n\n**2.3. Curvature Calculation**\nThe curvature for the Categorical case is defined as the largest eigenvalue of the Hessian matrix $\\mathbf{H} = \\nabla^{2}_{\\mathbf{z}} \\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y})$. The elements of the Hessian, $H_{k\\ell}$, are:\n$$H_{k\\ell} = \\frac{\\partial}{\\partial z_\\ell} \\left( \\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial z_k} \\right) = \\frac{\\partial}{\\partial z_\\ell} (q_k - y_k) = \\frac{\\partial q_k}{\\partial z_\\ell}$$\nThis is exactly the Jacobian of the softmax function derived earlier. So, the Hessian matrix is:\n$$\\mathbf{H} = \\begin{pmatrix} q_1(1-q_1) & -q_1 q_2 & \\dots \\\\ -q_2 q_1 & q_2(1-q_2) & \\dots \\\\ \\vdots & \\vdots & \\ddots \\end{pmatrix} \\quad \\text{or concisely, } \\quad \\mathbf{H} = \\text{diag}(\\mathbf{q}) - \\mathbf{q}\\mathbf{q}^T$$\nThis matrix is the covariance matrix of a Categorical distribution. It is a real symmetric positive semi-definite matrix. The curvature is its largest eigenvalue, $\\lambda_{\\max}(\\mathbf{H})$. This will be computed numerically.\n\n### 3. Summary of Formulas for Implementation\n\n-   **Bernoulli Head:**\n    -   $p = 1 / (1 + e^{-z})$\n    -   Gradient magnitude: $|p - y|$\n    -   Curvature: $p(1-p)$\n\n-   **Categorical Head:**\n    -   Stable softmax: $z'_i = z_i - \\max(\\mathbf{z})$, then $q_i = e^{z'_i} / \\sum_j e^{z'_j}$\n    -   One-hot label: $\\mathbf{y}$ from index $y_{idx}$\n    -   Gradient magnitude: $\\sqrt{\\sum_{i=1}^{K} (q_i - y_i)^2}$\n    -   Hessian: $\\mathbf{H} = \\text{diag}(\\mathbf{q}) - \\mathbf{q}\\mathbf{q}^T$\n    -   Curvature: $\\lambda_{\\max}(\\mathbf{H})$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes gradient magnitude and curvature for Bernoulli and Categorical NLL loss\n    functions based on specified test cases.\n    \"\"\"\n\n    # --- Helper Functions ---\n\n    def bernoulli_analysis(y, z):\n        \"\"\"\n        Computes gradient magnitude and curvature for the Bernoulli NLL loss.\n        \n        Args:\n            y (int): The true label (0 or 1).\n            z (float): The logit.\n            \n        Returns:\n            A tuple (gradient_magnitude, curvature).\n        \"\"\"\n        # Calculate probability p = sigma(z)\n        p = 1.0 / (1.0 + np.exp(-z))\n        \n        # Gradient of NLL w.r.t. z is p - y\n        grad_scalar = p - y\n        \n        # Gradient magnitude is the absolute value of the scalar gradient\n        grad_magnitude = np.abs(grad_scalar)\n        \n        # Curvature (second derivative) is p * (1 - p)\n        curvature = p * (1.0 - p)\n        \n        return grad_magnitude, curvature\n\n    def categorical_analysis(y_idx, z_vec):\n        \"\"\"\n        Computes gradient magnitude and curvature for the Categorical NLL loss.\n        \n        Args:\n            y_idx (int): The index of the true class.\n            z_vec (np.ndarray): The vector of logits.\n            \n        Returns:\n            A tuple (gradient_magnitude, curvature).\n        \"\"\"\n        K = len(z_vec)\n        \n        # 1. Compute probability vector q using a stable softmax\n        z_shifted = z_vec - np.max(z_vec)\n        exps = np.exp(z_shifted)\n        q = exps / np.sum(exps)\n        \n        # 2. Create one-hot label vector y\n        y_vec = np.zeros(K)\n        y_vec[y_idx] = 1.0\n        \n        # 3. Compute gradient vector and its magnitude\n        # Gradient of NLL w.r.t. z is q - y\n        grad_vec = q - y_vec\n        grad_magnitude = np.linalg.norm(grad_vec)\n        \n        # 4. Construct the Hessian matrix H = diag(q) - q*q^T\n        # hessian = np.diag(q) - np.outer(q, q)\n        # Using a numerically stable way to compute eigenvalues of I-uu^T style matrices is not needed here\n        # standard computation is fine given the problem constraints and numpy's robustness.\n        q_diag = np.diag(q)\n        q_outer = np.outer(q, q)\n        hessian = q_diag - q_outer\n\n\n        # 5. Compute eigenvalues of the Hessian\n        # The Hessian is real and symmetric, use eigh for efficiency and stability.\n        # eigh returns eigenvalues in ascending order.\n        eigenvalues = np.linalg.eigh(hessian)[0]\n        \n        # 6. Curvature is the largest eigenvalue\n        curvature = eigenvalues[-1]\n        \n        return grad_magnitude, curvature\n\n    # --- Test Cases ---\n    \n    test_cases = [\n        # Type, (parameters...)\n        ('B', (1, -2.197224577)),     # Case B1\n        ('B', (1, -6.907755278)),     # Case B2\n        ('B', (0, 2.197224577)),      # Case B3\n        ('C', (3, np.array([2.0, 1.0, 0.5, -1.0, -2.0]))), # Case C1\n        ('C', (2, np.array([5.0, 0.0, -2.5]))),          # Case C2\n        ('C', (1, np.array([0.0, 0.0, 0.0, 0.0]))),       # Case C3\n    ]\n\n    results = []\n    for case_type, params in test_cases:\n        if case_type == 'B':\n            y, z = params\n            grad_mag, curv = bernoulli_analysis(y, z)\n        elif case_type == 'C':\n            y_idx, z_vec = params\n            grad_mag, curv = categorical_analysis(y_idx, z_vec)\n        \n        results.extend([grad_mag, curv])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.7f}' for x in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A trained model produces a probability mass function ($PMF$) for each prediction, but is the confidence expressed by this $PMF$ trustworthy? This practice introduces model calibration, the concept that a model's predictive confidence should match its empirical accuracy. You will implement the Expected Calibration Error ($ECE$) from scratch, providing a hands-on understanding of how to quantify and diagnose over- and under-confident model behavior. ",
            "id": "3166196",
            "problem": "You are given a multiclass classification scenario where a model produces, for each input, a vector of predicted probabilities over classes that sum to $1$. Such vectors constitute a probability mass function (PMF). In deep learning, a standard way to construct a PMF from unnormalized scores (logits) is through the softmax function: given logits $\\mathbf{z} \\in \\mathbb{R}^K$, the PMF is defined by $p_i = \\exp(z_i) / \\sum_{j=1}^K \\exp(z_j)$ for $i \\in \\{1,\\dots,K\\}$.\n\nThe goal is to compute the Expected Calibration Error (ECE) that measures the discrepancy between predicted confidences and observed accuracies. Work from first principles, using only these foundational definitions:\n- A PMF over $K$ classes is a function $p: \\{1,\\dots,K\\} \\to [0,1]$ such that $\\sum_{i=1}^K p(i) = 1$.\n- For each prediction $\\mathbf{p}$, define the predicted class as the index of the largest component and define the confidence as the maximum probability value. For a given true label, define correctness as $1$ if the predicted class equals the true label and $0$ otherwise.\n- An expected value is the average of a quantity under its probability distribution. A histogram-based approximation of an expected value partitions the domain into bins, computes averages within bins, and aggregates them weighted by the relative bin frequencies.\n\nDefine the following procedure to approximate ECE using $B$ equal-width bins on the interval $[0,1]$:\n- For each sample $n \\in \\{1,\\dots,N\\}$ with PMF $\\mathbf{p}^{(n)}$, let the confidence be $c^{(n)} = \\max_i p^{(n)}_i$ and let the predicted class be $\\hat{y}^{(n)} = \\arg\\max_i p^{(n)}_i$. Given the true label $y^{(n)}$, define the indicator $a^{(n)} = 1$ if $\\hat{y}^{(n)} = y^{(n)}$ and $a^{(n)} = 0$ otherwise.\n- Partition $[0,1]$ into $B$ equal-width bins. Assign each $c^{(n)}$ to a bin by the rule $b^{(n)} = \\min\\{\\lfloor B \\cdot c^{(n)} \\rfloor, B-1\\}$, which realizes bins $[0, 1/B), [1/B, 2/B), \\dots, [(B-1)/B, 1]$.\n- For each bin $b \\in \\{0,\\dots,B-1\\}$, consider only those $n$ with $b^{(n)} = b$. If a bin has no samples, it contributes nothing. For a nonempty bin, compute the empirical accuracy as the average of $a^{(n)}$ within the bin and the empirical confidence as the average of $c^{(n)}$ within the bin. The ECE is the weighted aggregation over bins of the absolute deviation between bin accuracy and bin confidence, with weights equal to the fraction of samples falling in each bin.\n\nYour task: Implement a program that computes the ECE for each of the following test cases. For each case, you are given a set of PMFs (as rows) and a vector of true labels, along with the number of bins $B$. Treat labels as zero-based class indices. Use the exact binning rule stated above and skip empty bins (i.e., bins with zero samples contribute $0$). Your program should produce a single line of output containing the ECE values, rounded to six decimal places, as a comma-separated list enclosed in square brackets.\n\nTest Suite:\n- Case A (perfect calibration across two confidence levels, $K=5$ classes):\n  - Number of samples $N = 50$, number of bins $B = 10$.\n  - Construct the PMF matrix by stacking $25$ copies of `[0.2, 0.2, 0.2, 0.2, 0.2]` followed by $25$ copies of `[0.8, 0.05, 0.05, 0.05, 0.05]`.\n  - True labels vector of length $50$:\n    - Indices $0$ through $4$: label $0$.\n    - Indices $5$ through $24$: label $1$.\n    - Indices $25$ through $44$: label $0$.\n    - Indices $45$ through $49$: label $1$.\n- Case B (overconfident predictions, $K=3$ classes):\n  - Number of samples $N = 20$, number of bins $B = 10$.\n  - PMF matrix with $20$ copies of `[0.9, 0.05, 0.05]`.\n  - True labels vector of length $20$:\n    - Indices $0$ through $9$: label $0$.\n    - Indices $10$ through $19$: label $1$.\n- Case C (underconfident predictions, $K=3$ classes):\n  - Number of samples $N = 20$, number of bins $B = 10$.\n  - PMF matrix with $20$ copies of `[0.4, 0.3, 0.3]`.\n  - True labels vector of length $20$:\n    - Indices $0$ through $15$: label $0$.\n    - Indices $16$ through $19$: label $1$.\n- Case D (sparse coverage of bins, tests empty-bin handling, $K=4$ classes):\n  - Number of samples $N = 4$, number of bins $B = 20$.\n  - PMF matrix (each row is one sample):\n    - Row $1$: `[0.51, 0.49, 0.0, 0.0]`.\n    - Row $2$: `[0.99, 0.01, 0.0, 0.0]`.\n    - Row $3$: `[0.0, 0.34, 0.33, 0.33]`.\n    - Row $4$: `[0.0, 0.26, 0.25, 0.49]`.\n  - True labels vector of length $4$: `[0, 1, 1, 2]`.\n- Case E (boundary case with deterministic PMFs at confidence $1$, $K=3$ classes):\n  - Number of samples $N = 4$, number of bins $B = 15$.\n  - PMF matrix:\n    - Row $1$: `[1.0, 0.0, 0.0]`.\n    - Row $2$: `[1.0, 0.0, 0.0]`.\n    - Row $3$: `[0.0, 1.0, 0.0]`.\n    - Row $4$: `[0.0, 1.0, 0.0]`.\n  - True labels vector of length $4$: `[0, 1, 1, 2]`.\n\nFinal Output Format:\n- Your program should produce a single line containing the ECE values for cases A through E, in order, rounded to six decimal places, formatted exactly as a Python-style list with no spaces: for example, `[x_A,x_B,x_C,x_D,x_E]` where each $x_\\cdot$ is a float rounded to six decimals.",
            "solution": "The problem requires the computation of the Expected Calibration Error (ECE), a metric used to evaluate the calibration of a probabilistic classifier. The procedure is based on a histogram binning approach. The solution is developed from the first principles and definitions provided.\n\nThe Expected Calibration Error is defined as the expected absolute difference between a model's predicted confidence and its actual accuracy. A histogram-based approximation of ECE is given by the formula:\n$$ECE = \\sum_{b=0}^{B-1} \\frac{|S_b|}{N} |\\text{Acc}(b) - \\text{Conf}(b)|$$\nHere, the confidence range $[0, 1]$ is divided into $B$ bins. For each bin $b$, $S_b$ is the set of sample indices that fall into that bin, $|S_b|$ is the number of such samples, and $N$ is the total number of samples. $\\text{Acc}(b)$ is the average accuracy of predictions within bin $b$, and $\\text{Conf}(b)$ is the average confidence of predictions within bin $b$. Bins with no samples ($|S_b|=0$) contribute $0$ to the sum.\n\nThe step-by-step procedure to compute ECE for a given set of $N$ samples is as follows:\n\n**1. Per-Sample Data Extraction**\nFor each sample $n \\in \\{1, \\dots, N\\}$, we are given a probability mass function (PMF), which is a vector of class probabilities $\\mathbf{p}^{(n)} = (p_1^{(n)}, \\dots, p_K^{(n)})$, and a true class label $y^{(n)}$. From these, we derive three essential quantities:\n- **Confidence**: The confidence of the prediction is the maximum probability value in the PMF.\n  $$c^{(n)} = \\max_{i \\in \\{1, \\dots, K\\}} p_i^{(n)}$$\n- **Predicted Class**: The predicted class is the class index corresponding to the maximum probability.\n  $$\\hat{y}^{(n)} = \\arg\\max_{i \\in \\{1, \\dots, K\\}} p_i^{(n)}$$\n- **Correctness**: The correctness is a binary indicator, $a^{(n)}$, which is $1$ if the prediction is correct and $0$ otherwise.\n  $$a^{(n)} = \\mathbb{I}(\\hat{y}^{(n)} = y^{(n)}) = \\begin{cases} 1 & \\text{if } \\hat{y}^{(n)} = y^{(n)} \\\\ 0 & \\text{if } \\hat{y}^{(n)} \\neq y^{(n)} \\end{cases}$$\n\n**2. Binning of Samples**\nThe confidence scores $c^{(n)}$, which are continuous values in the interval $[0, 1]$, are discretized by partitioning the interval into $B$ equal-width bins. The problem specifies an exact rule for assigning each sample $n$ to a bin index $b^{(n)} \\in \\{0, 1, \\dots, B-1\\}$:\n$$b^{(n)} = \\min\\{\\lfloor B \\cdot c^{(n)} \\rfloor, B-1\\}$$\nThis rule correctly maps a confidence $c^{(n)}$ to an integer bin index. For example, a confidence $c^{(n)} \\in [k/B, (k+1)/B)$ for $k < B-1$ is mapped to bin $k$. Due to the `min` function, any confidence $c^{(n)}$ in the last interval $[(B-1)/B, 1]$ (including the endpoint $c^{(n)}=1$) is mapped to the last bin, $B-1$.\n\n**3. Per-Bin Aggregation**\nAfter all $N$ samples have been assigned to their respective bins, we aggregate the data within each bin. The problem states that empty bins are skipped. We only need to consider non-empty bins $b$ for which the set of samples $S_b = \\{n \\mid b^{(n)} = b\\}$ is not empty. For each such bin, we compute two averages:\n- **Bin Accuracy**: The average accuracy for bin $b$ is the mean of the correctness indicators of all samples in that bin.\n  $$\\text{Acc}(b) = \\frac{1}{|S_b|} \\sum_{n \\in S_b} a^{(n)}$$\n- **Bin Confidence**: The average confidence for bin $b$ is the mean of the confidences of all samples in that bin.\n  $$\\text{Conf}(b) = \\frac{1}{|S_b|} \\sum_{n \\in S_b} c^{(n)}$$\n\n**4. Final ECE Calculation**\nThe final ECE value is a weighted average of the absolute difference between the accuracy and confidence for each bin. The weight for bin $b$ is the fraction of total samples that fall into it, $\\frac{|S_b|}{N}$. Summing these weighted differences over all non-empty bins gives the final ECE, as per the formula introduced at the beginning.\n\n**Implementation Design**\nThis algorithm is implemented in Python using the `numpy` library for efficient, vectorized computations.\n- A function `compute_ece` encapsulates the logic. It takes a PMF matrix (samples $\\times$ classes), a vector of true labels, and the number of bins $B$ as input.\n- Inside this function, `numpy.max(axis=1)` and `numpy.argmax(axis=1)` are used to compute the confidences and predicted classes for all samples simultaneously.\n- The correctness array is obtained via a simple element-wise comparison between the predicted classes and true labels.\n- The binning rule is implemented using `numpy.floor` and `numpy.minimum` as per the formula.\n- To process only non-empty bins efficiently, we can find the unique bin indices present in the data. For each unique bin index, a boolean mask is created to select the relevant samples. The bin accuracy, bin confidence, and bin proportion are then calculated using this mask.\n- The main `solve` function sets up the data for each of the five test cases, calls the `compute_ece` function for each, and collects the results.\n- Finally, the results are formatted to six decimal places and printed in the specified comma-separated list format.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the ECE calculation problem for all test cases specified.\n    \"\"\"\n\n    def compute_ece(pmfs, true_labels, num_bins):\n        \"\"\"\n        Computes the Expected Calibration Error (ECE).\n\n        Args:\n            pmfs (np.ndarray): A 2D array of shape (N, K) where N is the number of samples\n                               and K is the number of classes. Each row is a PMF.\n            true_labels (np.ndarray): A 1D array of shape (N,) containing the true class\n                                      indices for each sample.\n            num_bins (int): The number of bins to use for the ECE calculation.\n\n        Returns:\n            float: The computed ECE value.\n        \"\"\"\n        num_samples = len(true_labels)\n        if num_samples == 0:\n            return 0.0\n\n        # Step 1: Per-Sample Data Extraction\n        confidences = np.max(pmfs, axis=1)\n        predicted_classes = np.argmax(pmfs, axis=1)\n        correctness_arr = (predicted_classes == true_labels).astype(float)\n\n        # Step 2: Binning of Samples\n        # The problem states binning rule: b(n) = min(floor(B*c(n)), B-1)\n        # This handles the edge case c(n)=1.0, where floor(B*1.0) = B\n        bin_indices = np.minimum(np.floor(num_bins * confidences), num_bins - 1).astype(int)\n\n        ece = 0.0\n        \n        # Step 3 & 4: Per-Bin Aggregation and Final ECE Calculation\n        # Iterate through unique bin indices that are actually populated\n        for b in range(num_bins):\n            in_bin_mask = (bin_indices == b)\n            num_in_bin = np.sum(in_bin_mask)\n            \n            if num_in_bin > 0:\n                # Proportion of samples in this bin\n                prop_in_bin = num_in_bin / num_samples\n                \n                # Average accuracy in this bin\n                acc_in_bin = np.mean(correctness_arr[in_bin_mask])\n                \n                # Average confidence in this bin\n                conf_in_bin = np.mean(confidences[in_bin_mask])\n                \n                ece += prop_in_bin * np.abs(acc_in_bin - conf_in_bin)\n        \n        return ece\n\n    # --- Test Case A ---\n    B_a = 10\n    pmf_a1 = np.tile([0.2, 0.2, 0.2, 0.2, 0.2], (25, 1))\n    pmf_a2 = np.tile([0.8, 0.05, 0.05, 0.05, 0.05], (25, 1))\n    pmfs_a = np.vstack((pmf_a1, pmf_a2))\n    labels_a = np.zeros(50, dtype=int)\n    labels_a[5:25] = 1   # 20 samples\n    labels_a[45:50] = 1  # 5 samples\n\n    # --- Test Case B ---\n    B_b = 10\n    pmfs_b = np.tile([0.9, 0.05, 0.05], (20, 1))\n    labels_b = np.zeros(20, dtype=int)\n    labels_b[10:20] = 1\n\n    # --- Test Case C ---\n    B_c = 10\n    pmfs_c = np.tile([0.4, 0.3, 0.3], (20, 1))\n    labels_c = np.zeros(20, dtype=int)\n    labels_c[16:20] = 1\n\n    # --- Test Case D ---\n    B_d = 20\n    pmfs_d = np.array([\n        [0.51, 0.49, 0.0, 0.0],\n        [0.99, 0.01, 0.0, 0.0],\n        [0.0, 0.34, 0.33, 0.33],\n        [0.0, 0.26, 0.25, 0.49]\n    ])\n    labels_d = np.array([0, 1, 1, 2])\n\n    # --- Test Case E ---\n    B_e = 15\n    pmfs_e = np.array([\n        [1.0, 0.0, 0.0],\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 1.0, 0.0]\n    ])\n    labels_e = np.array([0, 1, 1, 2])\n\n    test_cases = [\n        (pmfs_a, labels_a, B_a),\n        (pmfs_b, labels_b, B_b),\n        (pmfs_c, labels_c, B_c),\n        (pmfs_d, labels_d, B_d),\n        (pmfs_e, labels_e, B_e),\n    ]\n\n    results = []\n    for pmfs, labels, b_val in test_cases:\n        ece_val = compute_ece(pmfs, labels, b_val)\n        results.append(ece_val)\n\n    # Format output as required: rounded to 6 decimal places, no spaces.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The principle of calibration extends beyond discrete classification tasks to continuous regression models that predict a full probability density function ($PDF$). This exercise demonstrates a powerful and elegant evaluation technique based on the Probability Integral Transform ($PIT$), which states that a perfect model's predictive cumulative distribution function ($CDF$) will transform true outcomes into a uniform distribution. By testing for deviations from uniformity, you will learn to identify subtle forms of miscalibration in probabilistic regression, a vital skill for building reliable predictive systems. ",
            "id": "3166216",
            "problem": "You are given a conditional regression setting used in deep learning where a model parameterized by $\\theta$ outputs a predictive Cumulative Distribution Function (CDF) $F_{\\theta}(y \\mid x)$ for a continuous target variable $Y$ given features $X$. The Probability Integral Transform (PIT) maps an observation to $u = F_{\\theta}(Y \\mid X)$. If the predictive distribution is correctly calibrated, then $U = F_{\\theta}(Y \\mid X)$ is conditionally uniform on $[0,1]$, and hence unconditionally uniform on $[0,1]$. Your task is to implement a calibration check by testing the uniformity of $U$ using a distribution-free hypothesis test.\n\nStart from the following fundamental base:\n- A Probability Density Function (PDF) $f(y)$ satisfies $\\int_{-\\infty}^{\\infty} f(y) \\, dy = 1$ and $f(y) \\ge 0$.\n- A Probability Mass Function (PMF) $p(y)$ satisfies $\\sum_{y} p(y) = 1$ and $p(y) \\ge 0$.\n- A Cumulative Distribution Function (CDF) $F(y)$ for a continuous random variable satisfies $F(y) = \\int_{-\\infty}^{y} f(t) \\, dt$, is non-decreasing, right-continuous, and $\\lim_{y \\to -\\infty} F(y) = 0$, $\\lim_{y \\to \\infty} F(y) = 1$.\n- For conditional distributions, the conditional CDF $F(y \\mid x)$ obeys the same properties for each fixed $x$.\n- The Probability Integral Transform (PIT) states that if $Y \\mid X$ has CDF $F(y \\mid x)$ that is continuous and strictly increasing in $y$ for each fixed $x$, then $U = F(Y \\mid X)$ is conditionally uniform on $[0,1]$ given $X$, and thus uniform on $[0,1]$ unconditionally.\n\nYou must implement the following calibration check for a Gaussian predictive model in four test cases. In each case, generate data by first sampling $n$ independent features $X_i$ and then sampling targets $Y_i$ from a ground-truth conditional Normal distribution. Evaluate the predictive CDF $F_{\\theta}(Y_i \\mid X_i)$ of the model and compute the PIT values $U_i = F_{\\theta}(Y_i \\mid X_i)$. Then perform a Kolmogorov–Smirnov (KS) test of the empirical $\\{U_i\\}$ against the standard Uniform distribution on $[0,1]$ to obtain a $p$-value. Finally, for each case, return a boolean indicating whether calibration \"passes,\" defined as the event that the KS test does not reject uniformity at significance level $\\alpha$ (i.e., $p \\ge \\alpha$).\n\nUse the following parameterized test suite. In all cases, use $n = 500$ samples and set the significance level to $\\alpha = 0.05$. Let $\\Phi$ denote the standard Normal CDF, and for each case, use the Gaussian predictive CDF $F_{\\theta}(y \\mid x) = \\Phi\\!\\big((y - \\mu_{\\theta}(x))/\\sigma_{\\theta}(x)\\big)$ computed with the specified predictive mean $\\mu_{\\theta}(x)$ and predictive standard deviation $\\sigma_{\\theta}(x)$.\n\n- Case $1$ (well-calibrated \"happy path\"):\n  - Feature generation: $X \\sim \\mathcal{N}(0,1)$.\n  - Ground truth: $Y \\mid X=x \\sim \\mathcal{N}(a_{\\text{true}} x + b_{\\text{true}}, \\sigma_{\\text{true}}^2)$ with $a_{\\text{true}} = 1.0$, $b_{\\text{true}} = 0.5$, $\\sigma_{\\text{true}} = 0.8$.\n  - Predictive model: $\\mu_{\\theta}(x) = a_{\\text{pred}} x + b_{\\text{pred}}$ with $a_{\\text{pred}} = 1.0$, $b_{\\text{pred}} = 0.5$; $\\sigma_{\\theta}(x) = \\sigma_{\\text{pred}}$ with $\\sigma_{\\text{pred}} = 0.8$.\n\n- Case $2$ (variance miscalibration: predictive variance too small):\n  - Feature generation: $X \\sim \\mathcal{N}(0,1)$.\n  - Ground truth: $Y \\mid X=x \\sim \\mathcal{N}(a_{\\text{true}} x + b_{\\text{true}}, \\sigma_{\\text{true}}^2)$ with $a_{\\text{true}} = 1.0$, $b_{\\text{true}} = 0.5$, $\\sigma_{\\text{true}} = 1.2$.\n  - Predictive model: $\\mu_{\\theta}(x) = a_{\\text{pred}} x + b_{\\text{pred}}$ with $a_{\\text{pred}} = 1.0$, $b_{\\text{pred}} = 0.5$; $\\sigma_{\\theta}(x) = \\sigma_{\\text{pred}}$ with $\\sigma_{\\text{pred}} = 0.6$.\n\n- Case $3$ (mean miscalibration: predictive bias):\n  - Feature generation: $X \\sim \\mathcal{N}(0,1)$.\n  - Ground truth: $Y \\mid X=x \\sim \\mathcal{N}(a_{\\text{true}} x + b_{\\text{true}}, \\sigma_{\\text{true}}^2)$ with $a_{\\text{true}} = 1.0$, $b_{\\text{true}} = 0.5$, $\\sigma_{\\text{true}} = 0.8$.\n  - Predictive model: $\\mu_{\\theta}(x) = a_{\\text{pred}} x + b_{\\text{pred}}$ with $a_{\\text{pred}} = 1.0$, $b_{\\text{pred}} = -0.5$; $\\sigma_{\\theta}(x) = \\sigma_{\\text{pred}}$ with $\\sigma_{\\text{pred}} = 0.8$.\n\n- Case $4$ (heteroscedastic miscalibration: true noise depends on $x$ but predictive noise is constant):\n  - Feature generation: $X \\sim \\mathcal{N}(0,1)$.\n  - Ground truth: $Y \\mid X=x \\sim \\mathcal{N}(a_{\\text{true}} x + b_{\\text{true}}, \\sigma_{\\text{true}}(x)^2)$ with $a_{\\text{true}} = 1.0$, $b_{\\text{true}} = 0.5$, and $\\sigma_{\\text{true}}(x) = 0.5 + 0.7 \\lvert x \\rvert$.\n  - Predictive model: $\\mu_{\\theta}(x) = a_{\\text{pred}} x + b_{\\text{pred}}$ with $a_{\\text{pred}} = 1.0$, $b_{\\text{pred}} = 0.5$; $\\sigma_{\\theta}(x) = \\sigma_{\\text{pred}}$ with $\\sigma_{\\text{pred}} = 0.8$.\n\nYour program must:\n- Use a fixed random seed for reproducibility.\n- For each case, generate $\\{(X_i, Y_i)\\}_{i=1}^{n}$, compute $U_i = F_{\\theta}(Y_i \\mid X_i)$, perform the Kolmogorov–Smirnov (KS) test against the Uniform distribution on $[0,1]$, and decide pass/fail at the significance level $\\alpha$.\n- Produce a single line of output containing the four boolean results in order of the cases, as a comma-separated list enclosed in square brackets (for example, `[True,False,True,False]`). No other text should be printed.\n\nAll quantities and parameters above are dimensionless; no physical units are involved. The returned booleans are the final answers for the test suite.",
            "solution": "The user has provided a valid problem statement.\n\nThe problem asks for an implementation of a calibration check for a series of probabilistic regression models. The core principle being tested is the Probability Integral Transform (PIT), a fundamental tool for evaluating the calibration of predictive distributions.\n\nAccording to the PIT, if a continuous random variable $Y$ has a cumulative distribution function (CDF) $F_Y(y)$, then the random variable $U = F_Y(Y)$ follows a standard uniform distribution on the interval $[0, 1]$. This principle extends to conditional settings. If a predictive model provides a conditional CDF $F_{\\theta}(y \\mid x)$ for a target $Y$ given features $X$, and this model is perfectly calibrated (i.e., $F_{\\theta}$ is the true conditional CDF of $Y$ given $X$), then the transformed variable $U = F_{\\theta}(Y \\mid X)$ will be uniformly distributed on $[0, 1]$.\n\nWe can test for miscalibration by generating a set of PIT values $\\{U_i\\}_{i=1}^n$ from a dataset of observations $\\{(X_i, Y_i)\\}_{i=1}^n$ and then performing a statistical test for uniformity on the sample $\\{U_i\\}$. A common choice for this is the distribution-free Kolmogorov-Smirnov (KS) test. The null hypothesis, $H_0$, of the KS test is that the PIT values are drawn from a standard uniform distribution. If we obtain a small $p$-value (typically less than a chosen significance level $\\alpha$), we reject $H_0$ and conclude that the model is miscalibrated. If the $p$-value is not small ($p \\ge \\alpha$), we do not have sufficient evidence to reject the hypothesis of good calibration.\n\nThe procedure for each of the four test cases is as follows:\n1.  **Set Global Parameters**: Fix the sample size to $n=500$ and the significance level for the hypothesis test to $\\alpha=0.05$. A fixed random seed is used to ensure reproducibility.\n2.  **Generate Data**: For each case, we simulate a dataset of size $n$.\n    a. We first draw $n$ independent samples of the feature, $X_i$, from a standard normal distribution, i.e., $X_i \\sim \\mathcal{N}(0, 1)$.\n    b. For each $X_i$, we draw a corresponding target value $Y_i$ from the specified \"ground-truth\" conditional normal distribution. The mean and variance of this distribution are functions of $X_i$ as defined in each case.\n3.  **Compute PIT Values**: For each data pair $(X_i, Y_i)$, we calculate the PIT value $U_i = F_{\\theta}(Y_i \\mid X_i)$. The predictive model's CDF, $F_{\\theta}$, is given as a Gaussian distribution, $F_{\\theta}(y \\mid x) = \\Phi\\left(\\frac{y - \\mu_{\\theta}(x)}{\\sigma_{\\theta}(x)}\\right)$, where $\\Phi$ is the standard normal CDF, and $\\mu_{\\theta}(x)$ and $\\sigma_{\\theta}(x)$ are the predictive mean and standard deviation specified for the case.\n4.  **Perform KS Test**: The resulting sample of PIT values, $\\{U_i\\}_{i=1}^n$, is tested for uniformity using the one-sample Kolmogorov-Smirnov test against the theoretical standard uniform distribution $\\mathcal{U}[0, 1]$. This test yields a $p$-value.\n5.  **Make a Decision**: We compare the $p$-value to the significance level $\\alpha$. The calibration is considered to \"pass\" (returning `True`) if the null hypothesis is not rejected, i.e., if $p \\ge \\alpha$. Otherwise, the model is deemed miscalibrated and \"fails\" (returning `False`).\n\nWe now apply this procedure to each of the four cases.\n\n**Case 1: Well-calibrated model**\n- The predictive model's parameters ($\\mu_{\\theta}(x) = 1.0x + 0.5$, $\\sigma_{\\theta}(x) = 0.8$) perfectly match the parameters of the ground-truth data generating process.\n- Therefore, $F_{\\theta}(y \\mid x)$ is the true conditional CDF.\n- By the PIT property, the resulting sample $\\{U_i\\}$ should be indistinguishable from a standard uniform sample.\n- We expect the KS test to yield a high $p$-value, leading to the conclusion that the calibration passes ($p \\ge 0.05$).\n\n**Case 2: Variance miscalibration**\n- The predictive model uses $\\sigma_{\\theta}(x) = 0.6$, while the true data has $\\sigma_{\\text{true}} = 1.2$.\n- The model's predictive variance is too small, making it overconfident. The observed values $Y_i$ will tend to be in the tails of the narrow predictive distributions.\n- The standardized residuals $Z_i = (Y_i - \\mu_{\\theta}(X_i))/\\sigma_{\\theta}(X_i)$ will have a variance greater than $1$.\n- Applying the standard normal CDF, $\\Phi(Z_i)$, will produce PIT values $U_i$ that cluster near $0$ and $1$, forming a U-shaped distribution. This is not uniform.\n- We expect the KS test to detect this deviation, yield a very small $p$-value, and thus fail the calibration check ($p < 0.05$).\n\n**Case 3: Mean miscalibration**\n- The predictive model uses a biased mean, $\\mu_{\\theta}(x) = 1.0x - 0.5$, while the true mean is $\\mu_{\\text{true}}(x) = 1.0x + 0.5$. The model is systematically biased low by $1.0$.\n- The standardized residuals $Z_i = (Y_i - \\mu_{\\theta}(X_i))/\\sigma_{\\theta}(X_i)$ will have a non-zero mean. Specifically, the mean of $Z_i$ will be $(\\mu_{\\text{true}}(X_i) - \\mu_{\\theta}(X_i)) / \\sigma_{\\theta}(X_i) = (1.0x_i + 0.5 - (1.0x_i - 0.5)) / 0.8 = 1.0 / 0.8 = 1.25$.\n- The PIT values $U_i = \\Phi(Z_i)$ will be drawn from a distribution skewed towards $1$. This is not uniform.\n- We expect the KS test to reject uniformity, resulting in a failure ($p < 0.05$).\n\n**Case 4: Heteroscedastic miscalibration**\n- The true data-generating process has a standard deviation that depends on the feature, $\\sigma_{\\text{true}}(x) = 0.5 + 0.7|x|$. This is a heteroscedastic model.\n- The predictive model wrongly assumes a constant standard deviation (homoscedasticity) with $\\sigma_{\\theta}(x) = 0.8$.\n- The mean is correctly specified, so the standardized residuals $Z_i = (Y_i - \\mu_{\\theta}(X_i))/\\sigma_{\\theta}(X_i)$ have a mean of $0$. However, their conditional variance is $\\text{Var}(Z_i \\mid X_i) = \\sigma_{\\text{true}}(X_i)^2 / \\sigma_{\\theta}(X_i)^2 = (0.5 + 0.7|X_i|)^2 / 0.8^2$.\n- This variance is not constant and not equal to $1$. The unconditional distribution of the collected $\\{Z_i\\}$ will be a mixture of normal distributions with different variances, which is not a standard normal distribution. Typically, it will be leptokurtic (heavier tails).\n- The PIT values $U_i = \\Phi(Z_i)$ will again form a non-uniform, U-shaped distribution.\n- We expect the KS test to detect this misspecification and fail the calibration check ($p < 0.05$).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm, kstest\n\ndef solve():\n    \"\"\"\n    Performs a calibration check for four probabilistic regression model scenarios.\n\n    For each case, it generates data, computes Probability Integral Transform (PIT)\n    values using a predictive model, and performs a Kolmogorov-Smirnov (KS) test\n    for uniformity. The result is a boolean indicating if the calibration check\n    \"passes\" (i.e., the KS test p-value is greater than or equal to alpha).\n    \"\"\"\n\n    # Global parameters\n    n = 500  # Sample size\n    alpha = 0.05  # Significance level\n    random_seed = 42\n\n    # Set the random seed for reproducibility\n    np.random.seed(random_seed)\n\n    # Define the four test cases using parameter dictionaries\n    test_cases = [\n        # Case 1 (well-calibrated \"happy path\")\n        {\n            \"description\": \"Well-calibrated\",\n            \"a_true\": 1.0, \"b_true\": 0.5, \"sigma_true_func\": lambda x: np.full_like(x, 0.8),\n            \"a_pred\": 1.0, \"b_pred\": 0.5, \"sigma_pred_const\": 0.8,\n        },\n        # Case 2 (variance miscalibration: predictive variance too small)\n        {\n            \"description\": \"Variance miscalibrated (underestimated)\",\n            \"a_true\": 1.0, \"b_true\": 0.5, \"sigma_true_func\": lambda x: np.full_like(x, 1.2),\n            \"a_pred\": 1.0, \"b_pred\": 0.5, \"sigma_pred_const\": 0.6,\n        },\n        # Case 3 (mean miscalibration: predictive bias)\n        {\n            \"description\": \"Mean miscalibrated (bias)\",\n            \"a_true\": 1.0, \"b_true\": 0.5, \"sigma_true_func\": lambda x: np.full_like(x, 0.8),\n            \"a_pred\": 1.0, \"b_pred\": -0.5, \"sigma_pred_const\": 0.8,\n        },\n        # Case 4 (heteroscedastic miscalibration)\n        {\n            \"description\": \"Heteroscedastic miscalibrated\",\n            \"a_true\": 1.0, \"b_true\": 0.5, \"sigma_true_func\": lambda x: 0.5 + 0.7 * np.abs(x),\n            \"a_pred\": 1.0, \"b_pred\": 0.5, \"sigma_pred_const\": 0.8,\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Step 1: Generate data\n        # Generate features X from a standard normal distribution\n        X = np.random.normal(loc=0.0, scale=1.0, size=n)\n\n        # Generate targets Y from the ground-truth conditional distribution\n        mu_true = case[\"a_true\"] * X + case[\"b_true\"]\n        sigma_true = case[\"sigma_true_func\"](X)\n        Y = np.random.normal(loc=mu_true, scale=sigma_true)\n\n        # Step 2: Compute PIT values using the predictive model\n        # Calculate predictive mean and standard deviation\n        mu_pred = case[\"a_pred\"] * X + case[\"b_pred\"]\n        sigma_pred = case[\"sigma_pred_const\"]\n\n        # Standardize the observations according to the predictive model\n        z_scores = (Y - mu_pred) / sigma_pred\n\n        # Apply the standard normal CDF to get PIT values\n        pit_values = norm.cdf(z_scores)\n\n        # Step 3: Perform Kolmogorov-Smirnov test for uniformity\n        # H0: The pit_values are drawn from a standard uniform distribution\n        ks_statistic, p_value = kstest(pit_values, 'uniform')\n\n        # Step 4: Decide if calibration passes\n        # Calibration passes if we do not reject H0 at significance level alpha\n        passes_calibration = (p_value >= alpha)\n        results.append(passes_calibration)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}