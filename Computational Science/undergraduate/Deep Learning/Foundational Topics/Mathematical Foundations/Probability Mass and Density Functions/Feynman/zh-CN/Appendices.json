{
    "hands_on_practices": [
        {
            "introduction": "在深度学习分类任务中，模型的输出本质上是关于各个类别的概率质量函数（PMF）。此练习将引导您从第一性原理出发，探讨两种核心的分类模型输出端——伯努利（Bernoulli）和分类（Categorical）分布——是如何通过其PMF形式定义损失函数的。通过亲手推导和计算负对数似然损失的梯度与曲率，您将深刻理解模型训练的内在机制，以及PMF的选择如何影响模型在面对数据不平衡时的学习动态 。",
            "id": "3166266",
            "problem": "你将比较深度学习中用于分类任务的两种输出头，并量化在标签不平衡的情况下，概率质量函数 (PMF) 的形状如何影响负对数似然及其导数。这两种输出头是：用于二元分类的伯努利头和用于多类分类的分类头。比较应根据相对于 logit 的梯度大小和损失曲面的曲率概念进行。计算必须从概率质量函数 (PMF)、负对数似然 (NLL) 的基本定义以及通过标准链接函数从 logit 到概率的映射开始。\n\n需要使用的基本定义如下：\n- 对于由概率 $p \\in (0,1)$ 参数化的二元标签 $y \\in \\{0,1\\}$，其伯努利 PMF 为 $P(Y=y \\mid p) = p^{y} (1-p)^{1-y}$。logit $z \\in \\mathbb{R}$ 通过逻辑斯谛函数 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ 映射到 $p$。\n- 对于由概率向量 $\\mathbf{q} \\in \\Delta^{K-1}$ (概率单纯形) 参数化的、只有一个 $1$ 和 $K-1$ 个零的独热标签 $\\mathbf{y} \\in \\{0,1\\}^{K}$，其分类 PMF 为 $P(\\mathbf{Y}=\\mathbf{y} \\mid \\mathbf{q}) = \\prod_{i=1}^{K} q_{i}^{y_{i}}$。logits $\\mathbf{z} \\in \\mathbb{R}^{K}$ 通过 softmax 函数 $q_{i}(\\mathbf{z}) = \\frac{e^{z_{i}}}{\\sum_{j=1}^{K} e^{z_{j}}}$ 映射到 $\\mathbf{q}$。\n\n将负对数似然 (NLL) 损失函数定义为在这些链接函数下 PMF 的负对数。对于伯努利头，将损失表示为 $\\mathcal{L}_{\\mathrm{B}}(z; y)$；对于分类头，将损失表示为 $\\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y})$。\n\n任务：\n1. 对于伯努利头，计算：\n   - 相对于标量 logit $z$ 的梯度，并报告其大小 $\\lVert \\nabla_{z} \\mathcal{L}_{\\mathrm{B}}(z; y) \\rVert_{2}$。\n   - 定义为二阶导数的曲率 $\\frac{d^{2}\\mathcal{L}_{\\mathrm{B}}(z; y)}{dz^{2}}$。\n   这两个量都必须从 PMF、NLL 定义和链式法则推导和实现，不得使用任何未经这些基本定义证明的快捷公式。\n\n2. 对于分类头，计算：\n   - 相对于 logits $\\mathbf{z}$ 的梯度向量，并报告其欧几里得模长 $\\lVert \\nabla_{\\mathbf{z}} \\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y}) \\rVert_{2}$。\n   - 定义为海森矩阵 $\\nabla^{2}_{\\mathbf{z}} \\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y})$ 最大特征值的曲率大小。\n   这两个量都必须从 PMF、NLL 定义、链式法则和 softmax 雅可比矩阵推导和实现，不得使用任何未经这些基本定义证明的快捷公式。\n\n此处的标签不平衡定义为预测强烈偏向多数类，而观察到的标签属于少数类。你将量化 PMF 形式（伯努利与分类）在这种情况下如何改变梯度大小和适当的曲率度量。\n\n测试套件：\n在分类头中对类别标签使用从零开始的索引。实现以下六个测试用例。\n\n- 伯努利头 (二元):\n  1. 案例 B1 (罕见正类，中等置信度的负类预测): $y = 1$, $z = -2.197224577$ (对应 $p \\approx 0.1$)。\n  2. 案例 B2 (罕见正类，极高置信度的负类预测): $y = 1$, $z = -6.907755278$ (对应 $p \\approx 0.001$)。\n  3. 案例 B3 (多数负类，高置信度的正类预测): $y = 0$, $z = 2.197224577$ (对应 $p \\approx 0.9$)。\n\n- 分类头 (多类):\n  4. 案例 C1 (罕见类别索引 $3$ 对抗多数偏向的 logits): $K = 5$, $y = 3$, $\\mathbf{z} = [2.0, 1.0, 0.5, -1.0, -2.0]$。\n  5. 案例 C2 (罕见类别索引 $2$，极端多数偏向): $K = 3$, $y = 2$, $\\mathbf{z} = [5.0, 0.0, -2.5]$。\n  6. 案例 C3 (平衡预测，均匀 logits): $K = 4$, $y = 1$, $\\mathbf{z} = [0.0, 0.0, 0.0, 0.0]$。\n\n答案规格：\n- 对于每个测试用例，输出两个浮点数：第一个是梯度大小，第二个是曲率大小（如上定义）。\n- 最终程序输出必须是单行，包含所有 $12$ 个浮点数，按顺序聚合为逗号分隔的列表，并用方括号括起来，遵循六个测试用例的顺序：B1, B2, B3, C1, C2, C3。即，输出格式必须严格为 \"[g_B1,c_B1,g_B2,c_B2,g_B3,c_B3,g_C1,c_C1,g_C2,c_C2,g_C3,c_C3]\"。\n- 此问题不涉及物理单位。角度不适用。\n\n你的程序必须完整且可按规定运行，不接受外部输入，并直接从定义计算这些量。在适当的地方确保数值稳定性。",
            "solution": "用户要求对负对数似然 (NLL) 损失函数及其导数进行详细分析，该函数用于伯努利分布和分类分布，这些分布通常在深度学习模型中用作分类任务的输出头。分析将从第一性原理出发，从概率质量函数 (PMF) 和将实值 logit 映射到概率的链接函数 (逻辑斯谛函数和 softmax 函数) 开始。\n\n### 1. 用于二元分类的伯努利头\n\n对于二元分类问题，标签为 $y \\in \\{0, 1\\}$。模型预测标签为 $1$ 的概率为 $p \\in (0,1)$。\n\n**1.1. 损失函数推导**\n伯努利 PMF 由下式给出：\n$$P(Y=y \\mid p) = p^{y} (1-p)^{1-y}$$\nNLL 损失 $\\mathcal{L}_{\\mathrm{B}}$ 是 PMF 的负对数：\n$$\\mathcal{L}_{\\mathrm{B}}(p; y) = -\\log(p^{y} (1-p)^{1-y}) = -[y \\log(p) + (1-y)\\log(1-p)]$$\n概率 $p$ 通过逻辑斯谛 (sigmoid) 函数 $\\sigma(z)$ 从标量 logit $z \\in \\mathbb{R}$ 获得：\n$$p = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n将此代入损失函数得到 $\\mathcal{L}_{\\mathrm{B}}(z; y)$。\n\n**1.2. 梯度计算**\n为了求得损失相对于 logit $z$ 的梯度，我们使用链式法则：$\\frac{d\\mathcal{L}_{\\mathrm{B}}}{dz} = \\frac{d\\mathcal{L}_{\\mathrm{B}}}{dp} \\frac{dp}{dz}$。\n\n首先，我们求损失相对于概率 $p$ 的导数：\n$$\\frac{d\\mathcal{L}_{\\mathrm{B}}}{dp} = -\\left[ \\frac{y}{p} - \\frac{1-y}{1-p} \\right] = \\frac{1-y}{1-p} - \\frac{y}{p} = \\frac{p(1-y) - y(1-p)}{p(1-p)} = \\frac{p - py - y + py}{p(1-p)} = \\frac{p-y}{p(1-p)}$$\n接下来，我们求逻辑斯谛函数相对于 $z$ 的导数：\n$$\\frac{dp}{dz} = \\frac{d}{dz} (1 + e^{-z})^{-1} = -1(1 + e^{-z})^{-2}(-e^{-z}) = \\frac{e^{-z}}{(1+e^{-z})^2} = \\frac{1}{1+e^{-z}}\\frac{e^{-z}}{1+e^{-z}} = \\sigma(z)(1-\\sigma(z)) = p(1-p)$$\n结合这些结果，得到梯度：\n$$\\frac{d\\mathcal{L}_{\\mathrm{B}}}{dz} = \\left( \\frac{p-y}{p(1-p)} \\right) \\cdot (p(1-p)) = p - y = \\sigma(z) - y$$\n梯度是一个标量，因此其欧几里得模长是其绝对值：\n$$\\lVert \\nabla_{z} \\mathcal{L}_{\\mathrm{B}}(z; y) \\rVert_{2} = |\\sigma(z) - y|$$\n\n**1.3. 曲率计算**\n伯努利情况下的曲率定义为损失相对于 logit $z$ 的二阶导数：\n$$\\frac{d^{2}\\mathcal{L}_{\\mathrm{B}}}{dz^{2}} = \\frac{d}{dz} \\left( \\frac{d\\mathcal{L}_{\\mathrm{B}}}{dz} \\right) = \\frac{d}{dz}(\\sigma(z) - y) = \\frac{d}{dz}\\sigma(z)$$\n使用我们之前求出的逻辑斯谛函数的导数：\n$$\\frac{d^{2}\\mathcal{L}_{\\mathrm{B}}}{dz^{2}} = \\sigma(z)(1-\\sigma(z)) = p(1-p)$$\n\n### 2. 用于多类分类的分类头\n\n对于一个有 $K$ 个类别的多类分类问题，标签由一个独热向量 $\\mathbf{y} \\in \\{0, 1\\}^K$ 表示，其中只有一个分量为 $1$。模型预测一个概率向量 $\\mathbf{q} \\in \\Delta^{K-1}$ (概率单纯形)。\n\n**2.1. 损失函数推导**\n分类 PMF 由下式给出：\n$$P(\\mathbf{Y}=\\mathbf{y} \\mid \\mathbf{q}) = \\prod_{i=1}^{K} q_{i}^{y_{i}}$$\nNLL 损失 $\\mathcal{L}_{\\mathrm{C}}$ 是 PMF 的负对数：\n$$\\mathcal{L}_{\\mathrm{C}}(\\mathbf{q}; \\mathbf{y}) = -\\log\\left(\\prod_{i=1}^{K} q_{i}^{y_{i}}\\right) = -\\sum_{i=1}^{K} y_i \\log(q_i)$$\n由于 $\\mathbf{y}$ 是独热向量，如果真实类别为 $c$，则 $y_c = 1$ 且对于 $i \\neq c$ 有 $y_i=0$。损失简化为标准的交叉熵损失：\n$$\\mathcal{L}_{\\mathrm{C}} = -\\log(q_c)$$\n概率向量 $\\mathbf{q}$ 通过 softmax 函数从 logit 向量 $\\mathbf{z} \\in \\mathbb{R}^K$ 获得：\n$$q_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n\n**2.2. 梯度计算**\n为了求得损失相对于 logit 向量 $\\mathbf{z}$ 的梯度，我们计算每个分量 $k$ 的偏导数 $\\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial z_k}$。\n$$\\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial z_k} = \\sum_{i=1}^{K} \\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial q_i} \\frac{\\partial q_i}{\\partial z_k}$$\n损失相对于 $q_i$ 的导数为 $\\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial q_i} = -\\frac{y_i}{q_i}$。\nsoftmax 函数的偏导数构成了雅可比矩阵 $J_{ik} = \\frac{\\partial q_i}{\\partial z_k}$，其形式为：\n$$\\frac{\\partial q_i}{\\partial z_k} = \\begin{cases} q_i(1-q_i)  \\text{if } i=k \\\\ -q_i q_k  \\text{if } i \\neq k \\end{cases} \\quad \\text{或简写为, } \\quad \\frac{\\partial q_i}{\\partial z_k} = q_i(\\delta_{ik} - q_k)$$\n其中 $\\delta_{ik}$ 是克罗内克 δ。\n结合这些，我们得到：\n$$\\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial z_k} = \\sum_{i=1}^{K} \\left(-\\frac{y_i}{q_i}\\right) (q_i(\\delta_{ik} - q_k)) = -\\sum_{i=1}^{K} y_i(\\delta_{ik} - q_k) = - (y_k - q_k \\sum_{i=1}^{K} y_i)$$\n由于 $\\mathbf{y}$ 是一个独热向量，$\\sum_{i=1}^{K} y_i = 1$。因此，偏导数简化为：\n$$\\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial z_k} = q_k - y_k$$\n因此，梯度向量为 $\\nabla_{\\mathbf{z}} \\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y}) = \\mathbf{q} - \\mathbf{y}$。其欧几里得模长为：\n$$\\lVert \\nabla_{\\mathbf{z}} \\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y}) \\rVert_{2} = \\lVert \\mathbf{q} - \\mathbf{y} \\rVert_{2} = \\sqrt{\\sum_{i=1}^{K} (q_i - y_i)^2}$$\n\n**2.3. 曲率计算**\n分类情况下的曲率定义为海森矩阵 $\\mathbf{H} = \\nabla^{2}_{\\mathbf{z}} \\mathcal{L}_{\\mathrm{C}}(\\mathbf{z}; \\mathbf{y})$ 的最大特征值。海森矩阵的元素 $H_{k\\ell}$ 是：\n$$H_{k\\ell} = \\frac{\\partial}{\\partial z_\\ell} \\left( \\frac{\\partial \\mathcal{L}_{\\mathrm{C}}}{\\partial z_k} \\right) = \\frac{\\partial}{\\partial z_\\ell} (q_k - y_k) = \\frac{\\partial q_k}{\\partial z_\\ell}$$\n这正是我们之前推导出的 softmax 函数的雅可比矩阵。所以，海森矩阵是：\n$$\\mathbf{H} = \\begin{pmatrix} q_1(1-q_1)  -q_1 q_2  \\dots \\\\ -q_2 q_1  q_2(1-q_2)  \\dots \\\\ \\vdots  \\vdots  \\ddots \\end{pmatrix} \\quad \\text{或简写为, } \\quad \\mathbf{H} = \\text{diag}(\\mathbf{q}) - \\mathbf{q}\\mathbf{q}^T$$\n该矩阵是分类分布的协方差矩阵。它是一个实对称半正定矩阵。曲率是其最大特征值 $\\lambda_{\\max}(\\mathbf{H})$。这将通过数值计算得出。\n\n### 3. 用于实现的公式摘要\n\n-   **伯努利头：**\n    -   $p = 1 / (1 + e^{-z})$\n    -   梯度模长：$|p - y|$\n    -   曲率：$p(1-p)$\n\n-   **分类头：**\n    -   稳定的 softmax：$z'_i = z_i - \\max(\\mathbf{z})$，然后 $q_i = e^{z'_i} / \\sum_j e^{z'_j}$\n    -   独热标签：从索引 $y_{idx}$ 生成 $\\mathbf{y}$\n    -   梯度模长：$\\sqrt{\\sum_{i=1}^{K} (q_i - y_i)^2}$\n    -   海森矩阵：$\\mathbf{H} = \\text{diag}(\\mathbf{q}) - \\mathbf{q}\\mathbf{q}^T$\n    -   曲率：$\\lambda_{\\max}(\\mathbf{H})$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes gradient magnitude and curvature for Bernoulli and Categorical NLL loss\n    functions based on specified test cases.\n    \"\"\"\n\n    # --- Helper Functions ---\n\n    def bernoulli_analysis(y, z):\n        \"\"\"\n        Computes gradient magnitude and curvature for the Bernoulli NLL loss.\n        \n        Args:\n            y (int): The true label (0 or 1).\n            z (float): The logit.\n            \n        Returns:\n            A tuple (gradient_magnitude, curvature).\n        \"\"\"\n        # Calculate probability p = sigma(z)\n        p = 1.0 / (1.0 + np.exp(-z))\n        \n        # Gradient of NLL w.r.t. z is p - y\n        grad_scalar = p - y\n        \n        # Gradient magnitude is the absolute value of the scalar gradient\n        grad_magnitude = np.abs(grad_scalar)\n        \n        # Curvature (second derivative) is p * (1 - p)\n        curvature = p * (1.0 - p)\n        \n        return grad_magnitude, curvature\n\n    def categorical_analysis(y_idx, z_vec):\n        \"\"\"\n        Computes gradient magnitude and curvature for the Categorical NLL loss.\n        \n        Args:\n            y_idx (int): The index of the true class.\n            z_vec (np.ndarray): The vector of logits.\n            \n        Returns:\n            A tuple (gradient_magnitude, curvature).\n        \"\"\"\n        K = len(z_vec)\n        \n        # 1. Compute probability vector q using a stable softmax\n        z_shifted = z_vec - np.max(z_vec)\n        exps = np.exp(z_shifted)\n        q = exps / np.sum(exps)\n        \n        # 2. Create one-hot label vector y\n        y_vec = np.zeros(K)\n        y_vec[y_idx] = 1.0\n        \n        # 3. Compute gradient vector and its magnitude\n        # Gradient of NLL w.r.t. z is q - y\n        grad_vec = q - y_vec\n        grad_magnitude = np.linalg.norm(grad_vec)\n        \n        # 4. Construct the Hessian matrix H = diag(q) - q*q^T\n        # hessian = np.diag(q) - np.outer(q, q)\n        # Using a numerically stable way to compute eigenvalues of I-uu^T style matrices is not needed here\n        # standard computation is fine given the problem constraints and numpy's robustness.\n        q_diag = np.diag(q)\n        q_outer = np.outer(q, q)\n        hessian = q_diag - q_outer\n\n\n        # 5. Compute eigenvalues of the Hessian\n        # The Hessian is real and symmetric, use eigh for efficiency and stability.\n        # eigh returns eigenvalues in ascending order.\n        eigenvalues = np.linalg.eigh(hessian)[0]\n        \n        # 6. Curvature is the largest eigenvalue\n        curvature = eigenvalues[-1]\n        \n        return grad_magnitude, curvature\n\n    # --- Test Cases ---\n    \n    test_cases = [\n        # Type, (parameters...)\n        ('B', (1, -2.197224577)),     # Case B1\n        ('B', (1, -6.907755278)),     # Case B2\n        ('B', (0, 2.197224577)),      # Case B3\n        ('C', (3, np.array([2.0, 1.0, 0.5, -1.0, -2.0]))), # Case C1\n        ('C', (2, np.array([5.0, 0.0, -2.5]))),          # Case C2\n        ('C', (1, np.array([0.0, 0.0, 0.0, 0.0]))),       # Case C3\n    ]\n\n    results = []\n    for case_type, params in test_cases:\n        if case_type == 'B':\n            y, z = params\n            grad_mag, curv = bernoulli_analysis(y, z)\n        elif case_type == 'C':\n            y_idx, z_vec = params\n            grad_mag, curv = categorical_analysis(y_idx, z_vec)\n        \n        results.extend([grad_mag, curv])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.7f}' for x in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个能够输出概率的模型，其预测的可信度有多高？高准确率的模型也可能存在过度自信或自信不足的问题，这即是校准（calibration）问题。本练习将带领您实现期望校准误差（Expected Calibration Error, ECE），这是一种衡量模型预测置信度与实际准确率之间差异的关键指标。通过将连续的置信度分箱并计算每个箱内的平均准确率与置信度，您将学会如何量化评估模型输出PMF的可靠性，这是超越传统准确率指标、进行更精细模型诊断的重要一步 。",
            "id": "3166196",
            "problem": "给定一个多类分类场景，其中模型为每个输入生成一个关于各个类别的预测概率向量，且向量元素之和为 $1$。这样的向量构成一个概率质量函数 (PMF)。在深度学习中，从非标准化分数 (logits) 构建 PMF 的一种标准方法是通过 softmax 函数：给定 logits $\\mathbf{z} \\in \\mathbb{R}^K$，PMF 定义为 $p_i = \\exp(z_i) / \\sum_{j=1}^K \\exp(z_j)$，其中 $i \\in \\{1,\\dots,K\\}$。\n\n目标是计算预期校准误差 (ECE)，该误差用于衡量预测置信度与观测准确率之间的差异。请仅使用以下基本定义，从第一性原理出发进行计算：\n- 一个关于 $K$ 个类别的 PMF 是一个函数 $p: \\{1,\\dots,K\\} \\to [0,1]$，满足 $\\sum_{i=1}^K p(i) = 1$。\n- 对于每个预测 $\\mathbf{p}$，将预测类别定义为最大分量的索引，并将置信度定义为最大概率值。对于给定的真实标签，如果预测类别与真实标签相等，则将正确性定义为 $1$，否则为 $0$。\n- 期望值是某个量在其概率分布下的平均值。基于直方图的期望值近似方法将域划分为多个区间（bin），计算每个区间内的平均值，并按区间的相对频率进行加权聚合。\n\n定义以下程序，使用 $B$ 个等宽区间在区间 $[0,1]$ 上近似计算 ECE：\n- 对于每个具有 PMF $\\mathbf{p}^{(n)}$ 的样本 $n \\in \\{1,\\dots,N\\}$，设置信度为 $c^{(n)} = \\max_i p^{(n)}_i$，预测类别为 $\\hat{y}^{(n)} = \\arg\\max_i p^{(n)}_i$。给定真实标签 $y^{(n)}$，如果 $\\hat{y}^{(n)} = y^{(n)}$，则定义指示符 $a^{(n)} = 1$，否则 $a^{(n)} = 0$。\n- 将 $[0,1]$ 划分为 $B$ 个等宽区间。根据规则 $b^{(n)} = \\min\\{\\lfloor B \\cdot c^{(n)} \\rfloor, B-1\\}$ 将每个 $c^{(n)}$ 分配到一个区间，这实现了区间 $[0, 1/B), [1/B, 2/B), \\dots, [(B-1)/B, 1]$。\n- 对于每个区间 $b \\in \\{0,\\dots,B-1\\}$，仅考虑那些 $b^{(n)} = b$ 的样本 $n$。如果一个区间没有样本，则其贡献为零。对于非空区间，计算其经验准确率（区间内 $a^{(n)}$ 的平均值）和经验置信度（区间内 $c^{(n)}$ 的平均值）。ECE 是各区间准确率与置信度之间绝对偏差的加权聚合，权重等于落入每个区间的样本比例。\n\n您的任务：实现一个程序，为以下每个测试用例计算 ECE。对于每个案例，您将获得一组 PMF（以行的形式）和一个真实标签向量，以及区间数量 $B$。将标签视为从零开始的类别索引。使用上述确切的分箱规则，并跳过空区间（即，样本数为零的区间贡献为 $0$）。您的程序应生成单行输出，其中包含 ECE 值，四舍五入到六位小数，格式为方括号括起来的逗号分隔列表。\n\n测试套件：\n- 案例 A（在两个置信度水平上完美校准，$K=5$ 个类别）：\n  - 样本数 $N = 50$，区间数 $B = 10$。\n  - 通过将 25 个 $[0.2, 0.2, 0.2, 0.2, 0.2]$ 的副本与 25 个 $[0.8, 0.05, 0.05, 0.05, 0.05]$ 的副本堆叠来构建 PMF 矩阵。\n  - 长度为 50 的真实标签向量：\n    - 索引 0 到 4：标签 0。\n    - 索引 5 到 24：标签 1。\n    - 索引 25 到 44：标签 0。\n    - 索引 45 到 49：标签 1。\n- 案例 B（过度自信的预测，$K=3$ 个类别）：\n  - 样本数 $N = 20$，区间数 $B = 10$。\n  - PMF 矩阵包含 20 个 $[0.9, 0.05, 0.05]$ 的副本。\n  - 长度为 20 的真实标签向量：\n    - 索引 0 到 9：标签 0。\n    - 索引 10 到 19：标签 1。\n- 案例 C（置信度不足的预测，$K=3$ 个类别）：\n  - 样本数 $N = 20$，区间数 $B = 10$。\n  - PMF 矩阵包含 20 个 $[0.4, 0.3, 0.3]$ 的副本。\n  - 长度为 20 的真实标签向量：\n    - 索引 0 到 15：标签 0。\n    - 索引 16 到 19：标签 1。\n- 案例 D（区间覆盖稀疏，测试空区间处理，$K=4$ 个类别）：\n  - 样本数 $N = 4$，区间数 $B = 20$。\n  - PMF 矩阵（每行为一个样本）：\n    - 第 1 行：$[0.51, 0.49, 0.0, 0.0]$。\n    - 第 2 行：$[0.99, 0.01, 0.0, 0.0]$。\n    - 第 3 行：$[0.0, 0.34, 0.33, 0.33]$。\n    - 第 4 行：$[0.0, 0.26, 0.25, 0.49]$。\n  - 长度为 4 的真实标签向量：$[0, 1, 1, 2]$。\n- 案例 E（边界情况，确定性 PMF 的置信度为 1，$K=3$ 个类别）：\n  - 样本数 $N = 4$，区间数 $B = 15$。\n  - PMF 矩阵：\n    - 第 1 行：$[1.0, 0.0, 0.0]$。\n    - 第 2 行：$[1.0, 0.0, 0.0]$。\n    - 第 3 行：$[0.0, 1.0, 0.0]$。\n    - 第 4 行：$[0.0, 1.0, 0.0]$。\n  - 长度为 4 的真实标签向量：$[0, 1, 1, 2]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含案例 A 到 E 的 ECE 值，按顺序排列，四舍五入到六位小数，格式与 Python 风格的列表完全一致，不含空格：例如，$ [x_A, x_B, x_C, x_D, x_E] $，其中每个 $x_\\cdot$ 是一个四舍五入到六位小数的浮点数。",
            "solution": "该问题要求计算预期校准误差 (ECE)，这是一个用于评估概率分类器校准度的指标。该过程基于直方图分箱方法。解决方案是根据所提供的第一性原理和定义推导出来的。\n\n预期校准误差定义为模型预测置信度与其际准确率之间绝对差异的期望值。ECE 的一种基于直方图的近似计算公式如下：\n$$ECE = \\sum_{b=0}^{B-1} \\frac{|S_b|}{N} |\\text{Acc}(b) - \\text{Conf}(b)|$$\n在这里，置信度范围 $[0, 1]$ 被划分为 $B$ 个区间。对于每个区间 $b$，$S_b$ 是落入该区间的样本索引集合，_b_$ 是这类样本的数量，$N$ 是总样本数。$\\text{Acc}(b)$ 是区间 $b$ 内预测的平均准确率，$\\text{Conf}(b)$ 是区间 $b$ 内预测的平均置信度。没有样本的区间（$|S_b|=0$）对总和的贡献为 $0$。\n\n为一组给定的 $N$ 个样本计算 ECE 的分步过程如下：\n\n**1. 单一样本数据提取**\n对于每个样本 $n \\in \\{1, \\dots, N\\}$，我们给定一个概率质量函数 (PMF)，即类别概率向量 $\\mathbf{p}^{(n)} = (p_1^{(n)}, \\dots, p_K^{(n)})$，以及一个真实类别标签 $y^{(n)}$。由此，我们推导出三个基本量：\n- **置信度**：预测的置信度是 PMF 中的最大概率值。\n  $$c^{(n)} = \\max_{i \\in \\{1, \\dots, K\\}} p_i^{(n)}$$\n- **预测类别**：预测的类别是与最大概率相对应的类别索引。\n  $$\\hat{y}^{(n)} = \\arg\\max_{i \\in \\{1, \\dots, K\\}} p_i^{(n)}$$\n- **正确性**：正确性是一个二元指示符 $a^{(n)}$，如果预测正确则为 $1$，否则为 $0$。\n  $$a^{(n)} = \\mathbb{I}(\\hat{y}^{(n)} = y^{(n)}) = \\begin{cases} 1  \\text{if } \\hat{y}^{(n)} = y^{(n)} \\\\ 0  \\text{if } \\hat{y}^{(n)} \\neq y^{(n)} \\end{cases}$$\n\n**2. 样本分箱**\n置信度分数 $c^{(n)}$ 是在区间 $[0, 1]$ 内的连续值，通过将该区间划分为 $B$ 个等宽区间来进行离散化。问题指定了为每个样本 $n$ 分配一个区间索引 $b^{(n)} \\in \\{0, 1, \\dots, B-1\\}$ 的确切规则：\n$$b^{(n)} = \\min\\{\\lfloor B \\cdot c^{(n)} \\rfloor, B-1\\}$$\n此规则将置信度 $c^{(n)}$ 正确映射到整数区间索引。例如，对于 $k  B-1$，置信度 $c^{(n)} \\in [k/B, (k+1)/B)$ 会被映射到区间 $k$。由于 `min` 函数的存在，任何在最后一个区间 $[(B-1)/B, 1]$ 内的置信度 $c^{(n)}$（包括端点 $c^{(n)}=1$）都会被映射到最后一个区间 $B-1$。\n\n**3. 各区间内聚合**\n在所有 $N$ 个样本都被分配到各自的区间后，我们在每个区间内聚合数据。问题指出空区间将被跳过。我们只需要考虑样本集 $S_b = \\{n \\mid b^{(n)} = b\\}$ 不为空的非空区间 $b$。对于每个这样的区间，我们计算两个平均值：\n- **区间准确率**：区间 $b$ 的平均准确率是该区间内所有样本正确性指示符的均值。\n  $$\\text{Acc}(b) = \\frac{1}{|S_b|} \\sum_{n \\in S_b} a^{(n)}$$\n- **区间置信度**：区间 $b$ 的平均置信度是该区间内所有样本置信度的均值。\n  $$\\text{Conf}(b) = \\frac{1}{|S_b|} \\sum_{n \\in S_b} c^{(n)}$$\n\n**4. 最终 ECE 计算**\n最终的 ECE 值是每个区间准确率和置信度之间绝对差的加权平均值。区间 $b$ 的权重是落入该区间的总样本比例，即 $\\frac{|S_b|}{N}$。将所有非空区间的这些加权差值相加，即可得到最终的 ECE，如开头介绍的公式所示。\n\n**实现设计**\n该算法使用 Python 的 `numpy` 库实现，以进行高效的向量化计算。\n- 一个名为 `compute_ece` 的函数封装了该逻辑。它接收一个 PMF 矩阵（样本数 $\\times$ 类别数）、一个真实标签向量和区间数 $B$ 作为输入。\n- 在该函数内部，使用 `numpy.max(axis=1)` 和 `numpy.argmax(axis=1)` 同时计算所有样本的置信度和预测类别。\n- 通过预测类别和真实标签之间的简单逐元素比较来获得正确性数组。\n- 分箱规则根据公式使用 `numpy.floor` 和 `numpy.minimum` 实现。\n- 为了高效地只处理非空区间，我们可以找到数据中存在的唯一区间索引。对于每个唯一的区间索引，创建一个布尔掩码来选择相关样本。然后使用此掩码计算区间准确率、区间置信度和区间比例。\n- 主函数 `solve` 为五个测试用例中的每一个设置数据，为每个用例调用 `compute_ece` 函数，并收集结果。\n- 最后，结果被格式化为六位小数，并以指定的逗号分隔列表格式打印。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the ECE calculation problem for all test cases specified.\n    \"\"\"\n\n    def compute_ece(pmfs, true_labels, num_bins):\n        \"\"\"\n        Computes the Expected Calibration Error (ECE).\n\n        Args:\n            pmfs (np.ndarray): A 2D array of shape (N, K) where N is the number of samples\n                               and K is the number of classes. Each row is a PMF.\n            true_labels (np.ndarray): A 1D array of shape (N,) containing the true class\n                                      indices for each sample.\n            num_bins (int): The number of bins to use for the ECE calculation.\n\n        Returns:\n            float: The computed ECE value.\n        \"\"\"\n        num_samples = len(true_labels)\n        if num_samples == 0:\n            return 0.0\n\n        # Step 1: Per-Sample Data Extraction\n        confidences = np.max(pmfs, axis=1)\n        predicted_classes = np.argmax(pmfs, axis=1)\n        correctness_arr = (predicted_classes == true_labels).astype(float)\n\n        # Step 2: Binning of Samples\n        # The problem states binning rule: b(n) = min(floor(B*c(n)), B-1)\n        # This handles the edge case c(n)=1.0, where floor(B*1.0) = B\n        bin_indices = np.minimum(np.floor(num_bins * confidences), num_bins - 1).astype(int)\n\n        ece = 0.0\n        \n        # Step 3  4: Per-Bin Aggregation and Final ECE Calculation\n        # Iterate through unique bin indices that are actually populated\n        for b in range(num_bins):\n            in_bin_mask = (bin_indices == b)\n            num_in_bin = np.sum(in_bin_mask)\n            \n            if num_in_bin > 0:\n                # Proportion of samples in this bin\n                prop_in_bin = num_in_bin / num_samples\n                \n                # Average accuracy in this bin\n                acc_in_bin = np.mean(correctness_arr[in_bin_mask])\n                \n                # Average confidence in this bin\n                conf_in_bin = np.mean(confidences[in_bin_mask])\n                \n                ece += prop_in_bin * np.abs(acc_in_bin - conf_in_bin)\n        \n        return ece\n\n    # --- Test Case A ---\n    B_a = 10\n    pmf_a1 = np.tile([0.2, 0.2, 0.2, 0.2, 0.2], (25, 1))\n    pmf_a2 = np.tile([0.8, 0.05, 0.05, 0.05, 0.05], (25, 1))\n    pmfs_a = np.vstack((pmf_a1, pmf_a2))\n    labels_a = np.zeros(50, dtype=int)\n    labels_a[5:25] = 1   # 20 samples\n    labels_a[45:50] = 1  # 5 samples\n\n    # --- Test Case B ---\n    B_b = 10\n    pmfs_b = np.tile([0.9, 0.05, 0.05], (20, 1))\n    labels_b = np.zeros(20, dtype=int)\n    labels_b[10:20] = 1\n\n    # --- Test Case C ---\n    B_c = 10\n    pmfs_c = np.tile([0.4, 0.3, 0.3], (20, 1))\n    labels_c = np.zeros(20, dtype=int)\n    labels_c[16:20] = 1\n\n    # --- Test Case D ---\n    B_d = 20\n    pmfs_d = np.array([\n        [0.51, 0.49, 0.0, 0.0],\n        [0.99, 0.01, 0.0, 0.0],\n        [0.0, 0.34, 0.33, 0.33],\n        [0.0, 0.26, 0.25, 0.49]\n    ])\n    labels_d = np.array([0, 1, 1, 2])\n\n    # --- Test Case E ---\n    B_e = 15\n    pmfs_e = np.array([\n        [1.0, 0.0, 0.0],\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 1.0, 0.0]\n    ])\n    labels_e = np.array([0, 1, 1, 2])\n\n    test_cases = [\n        (pmfs_a, labels_a, B_a),\n        (pmfs_b, labels_b, B_b),\n        (pmfs_c, labels_c, B_c),\n        (pmfs_d, labels_d, B_d),\n        (pmfs_e, labels_e, B_e),\n    ]\n\n    results = []\n    for pmfs, labels, b_val in test_cases:\n        ece_val = compute_ece(pmfs, labels, b_val)\n        results.append(ece_val)\n\n    # Format output as required: rounded to 6 decimal places, no spaces.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在大型语言模型等生成式任务中，模型输出的词汇表PMF不仅是最终答案，更是一个充满可能性的创作空间。为了控制生成内容的多样性与事实准确性之间的平衡，我们需要对这个PMF进行精巧的操控。本练习将向您介绍一种前沿技术——核采样（Nucleus Sampling），通过截断并重新归一化PMF，来塑造生成过程。您将通过实现这一算法，并计算其对熵（多样性）和期望事实性（准确性）的影响，从而掌握如何主动调控概率分布以实现特定的应用目标 。",
            "id": "3166223",
            "problem": "考虑一个大小为 $n$ 的离散词汇表，其词元由 $i \\in \\{1,2,\\dots,n\\}$ 索引。语言模型定义了一个词汇表上的概率质量函数 (PMF) $q$，其中 $q_i$ 是赋给词元 $i$ 的概率，且 $\\sum_{i=1}^{n} q_i = 1$。每个词元 $i$ 都关联一个词元级别的事实性分数 $s_i \\in [0,1]$，该分数被解释为对该词元事实正确性的内在估计。分类采样根据 PMF $q$ 来抽取词元。核采样 (top-$p$ 采样) 的定义是：按概率降序对词元进行排序，并选择一个最小的词元前缀集，使其累积概率质量达到或超过阈值 $p \\in [0,1]$；然后将 PMF 重新归一化到这个集合上，以定义所选支持集上的一个有效的截断 PMF。\n\n从概率质量函数 (PMF) 和累积分布函数 (CDF) 的定义出发，并根据 PMF 在支持集截断后需要重新归一化为 1 的要求，将核采样实现为一个对 $q$ 进行截断和重新归一化的过程，从而得到一个在所选支持集上的截断 PMF $r$。为截断后的 PMF $r$ 量化以下两个指标：\n- 多样性，以自然单位（奈特）的香农熵衡量。\n- 事实性，以 $r$ 分布下的期望事实性分数衡量。\n\n在本问题中，您必须从第一性原理出发，推导出这些量如何从 PMF $q$、阈值 $p$ 和事实性分数 $s$ 中产生，然后实现一个程序，无需随机性即可精确计算它们。\n\n您的程序应处理以下测试用例集。对于每种情况，输入为 $(q, s, p)$:\n- 情况 $1$ (正常路径)：$n=5$，$q=[0.50, 0.20, 0.12, 0.10, 0.08]$，$s=[0.40, 0.80, 0.30, 0.60, 0.90]$，$p=0.85$。\n- 情况 $2$ (近零阈值边界)：与情况 1 相同的 $q$ 和 $s$，$p=0.01$。\n- 情况 $3$ (平局和均匀分布)：$n=5$，$q=[0.20, 0.20, 0.20, 0.20, 0.20]$，$s=[0.90, 0.10, 0.50, 0.50, 0.10]$，$p=0.40$。当按概率降序排序时，通过词元索引升序来打破平局，以确保确定性。\n- 情况 $4$ (长尾，类齐夫分布的头部，中等阈值)：$n=10$，$q_i = \\dfrac{1/i}{Z}$ 其中 $i \\in \\{1,\\dots,10\\}$ 且 $Z=\\sum_{j=1}^{10} 1/j$，$s_i = 1 - 0.05 \\cdot i$，$p=0.90$。\n- 情况 $5$ (完整分布边界)：与情况 4 相同的 $q$ 和 $s$，$p=1.00$。\n\n截断的边界情况约定如下：将 $p$ 限制在 $[0,1]$ 范围内；对于 $p \\le 0$，只选择概率最高的单个词元；对于 $p \\ge 1$，选择整个词汇表。截断后，将 PMF 重新归一化到所选支持集上，使其总和为 $1$。\n\n对于每种情况，计算：\n- $r$ 的香农熵（单位：奈特），四舍五入到 $6$ 位小数。\n- 期望事实性 $\\sum_i r_i s_i$，四舍五入到 $6$ 位小数。\n\n您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表。其中每个情况的结果是一个形式为 $[H,F]$ 的双元素列表，$H$ 是熵，$F$ 是期望事实性。例如，最终输出格式必须是 $[[H_1,F_1],[H_2,F_2],[H_3,F_3],[H_4,F_4],[H_5,F_5]]$，每个数值都四舍五入到 $6$ 位小数，且不含额外文本。",
            "solution": "该问题要求实现核采样（top-$p$ 采样），并随后计算所得概率分布的两个关键指标：香农熵（一种多样性度量）和期望事实性。整个过程必须从概率论的基本原理推导得出。\n\n设离散词汇表大小为 $n$，词元通过 $i \\in \\{1, 2, \\dots, n\\}$ 进行索引。语言模型提供一个初始的概率质量函数 (PMF) $q$，其中 $q_i$ 是词元 $i$ 的概率，满足 $q_i \\ge 0$ 对所有 $i$ 成立且 $\\sum_{i=1}^{n} q_i = 1$。每个词元 $i$ 也关联一个事实性分数 $s_i \\in [0, 1]$。为了方便实现，我们将使用从零开始的索引 $i \\in \\{0, 1, \\dots, n-1\\}$。\n\n### 步骤 1：核采样的形式化\n\n核采样基于一个累积概率阈值 $p \\in [0, 1]$，将词汇表截断为一个更小的集合，即*核*（nucleus）。这包括三个主要步骤：排序、截断和重新归一化。\n\n**1. 排序：**\n词元根据其概率按降序排序。为确保在概率相等（平局）时结果具有确定性，打破平局的规则是按原始词元索引的升序排序。设 $\\pi$ 是索引 $\\{0, 1, \\dots, n-1\\}$ 的一个排列，使得 $q_{\\pi(0)} \\ge q_{\\pi(1)} \\ge \\dots \\ge q_{\\pi(n-1)}$。如果对于某个 $j$，$q_{\\pi(j)} = q_{\\pi(j+1)}$，则必须有 $\\pi(j)  \\pi(j+1)$。\n\n**2. 截断：**\n核是从排序列表顶部开始的、其累积概率质量至少为 $p$ 的最小词元集合。将这个索引核表示为 $V_p$。问题指定了边界情况约定：\n- 如果 $p \\ge 1$，核包含整个词汇表，所以 $V_p = \\{0, 1, \\dots, n-1\\}$。\n- 如果 $p \\le 0$，核只包含概率最高的单个词元，所以 $V_p = \\{\\pi(0)\\}$。\n对于 $p \\in (0, 1)$，我们找到最小的整数 $k \\ge 1$ 使得：\n$$ \\sum_{j=0}^{k-1} q_{\\pi(j)} \\ge p $$\n那么，核就是排序列表中前 $k$ 个索引的集合：$V_p = \\{\\pi(0), \\pi(1), \\dots, \\pi(k-1)\\}$。这个通用规则在将 $p$ 限制在 $[0, 1]$ 后，也正确地覆盖了指定的边界情况，因为对于 $p \\le 0$，条件在 $k=1$ 时满足（由于 $q_{\\pi(0)} \\ge 0$），而对于 $p=1$，则需要对所有概率求和，导致 $k=n$。\n\n**3. 重新归一化：**\n核 $V_p$ 中词元的概率必须被重新缩放，以形成一个有效的 PMF，我们将其表示为 $r$。核的总概率质量是归一化常数 $C_p$：\n$$ C_p = \\sum_{i \\in V_p} q_i $$\n每个词元 $i$ 的重新归一化概率 $r_i$ 定义为：\n$$ r_i = \\begin{cases} \\frac{q_i}{C_p}  \\text{if } i \\in V_p \\\\ 0  \\text{if } i \\notin V_p \\end{cases} $$\n根据构造，这个新分布 $r$ 是一个有效的 PMF，因为 $r_i \\ge 0$ 且：\n$$ \\sum_{i=0}^{n-1} r_i = \\sum_{i \\in V_p} r_i = \\sum_{i \\in V_p} \\frac{q_i}{C_p} = \\frac{1}{C_p} \\sum_{i \\in V_p} q_i = \\frac{C_p}{C_p} = 1 $$\n\n### 步骤 2：多样性和事实性指标的推导\n\n在建立了重新归一化的 PMF $r$ 之后，我们现在可以定义所需的指标。\n\n**1. 多样性（香农熵）：**\n分布的多样性由其香农熵来衡量。对于一个离散 PMF，以自然单位（奈特）计算的熵使用自然对数 ($\\ln$) 定义。分布 $r$ 的熵，记为 $H(r)$，是：\n$$ H(r) = - \\sum_{i=0}^{n-1} r_i \\ln(r_i) $$\n由于对于不在核 $V_p$ 中的词元，$r_i=0$，并且表达式 $x \\ln(x)$ 在 $x=0$ 时取值为 $0$，所以求和可以限制在 $r$ 的支持集上：\n$$ H(r) = - \\sum_{i \\in V_p} r_i \\ln(r_i) $$\n在计算出重新归一化的概率 $r_i$ 后，此公式直接用于计算。\n\n**2. 期望事实性：**\n分布的事实性是在 PMF $r$ 下，词元级别事实性分数 $s_i$ 的期望值。对于具有 PMF $p(x)$ 的离散随机变量 $X$，函数 $g(X)$ 的期望为 $E[g(X)] = \\sum_x g(x) p(x)$。在这里，随机变量是词元选择，其 PMF 是 $r$，函数是事实性分数 $s$。期望事实性，记为 $F(r)$，是：\n$$ F(r) = E_r[s] = \\sum_{i=0}^{n-1} s_i r_i $$\n与熵一样，由于对于 $i \\notin V_p$，$r_i=0$，求和实际上是在核上进行的：\n$$ F(r) = \\sum_{i \\in V_p} s_i r_i $$\n这是核中词元事实性分数的加权平均值，权重是它们重新归一化后的概率。代入 $r_i$ 的定义：\n$$ F(r) = \\sum_{i \\in V_p} s_i \\left(\\frac{q_i}{C_p}\\right) = \\frac{1}{C_p} \\sum_{i \\in V_p} s_i q_i $$\n\n实现将遵循这些推导出的步骤和公式，以确定性地处理每个测试用例。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_metrics(q_in, s_in, p_in):\n    \"\"\"\n    Performs nucleus sampling and computes entropy and expected factuality.\n    \n    Args:\n        q_in (list or np.ndarray): The initial Probability Mass Function (PMF).\n        s_in (list or np.ndarray): The token-level factuality scores.\n        p_in (float): The nucleus sampling threshold.\n\n    Returns:\n        tuple[float, float]: A tuple containing the Shannon entropy and the\n                             expected factuality of the truncated distribution.\n    \"\"\"\n    \n    q = np.array(q_in, dtype=np.float64)\n    s = np.array(s_in, dtype=np.float64)\n    p = np.clip(p_in, 0.0, 1.0)\n    \n    n = len(q)\n    indices = np.arange(n)\n    \n    # Sort indices: primary key is probability q (desc), secondary is index (asc)\n    # np.lexsort sorts by the last key first, so we provide indices then -q.\n    # We want to sort by -q (descending q), then by index (ascending index).\n    sorted_indices = np.lexsort((indices, -q))\n    \n    # Handle edge cases for p\n    if p == 0.0:\n        # Choose only the single highest-probability token\n        k = 1\n    elif p >= 1.0:\n        # Choose the full vocabulary\n        k = n\n    else:\n        # Find smallest k such that cumulative probability >= p\n        sorted_q = q[sorted_indices]\n        cumulative_q = np.cumsum(sorted_q)\n        # Find the first index where the cumulative sum exceeds p\n        # np.searchsorted finds the insertion point, which corresponds to index.\n        # k should be index + 1\n        k_idx = np.searchsorted(cumulative_q, p, side='left')\n        k = k_idx + 1\n\n    # Define the nucleus set of indices\n    nucleus_indices = sorted_indices[:k]\n    \n    # Get probabilities and scores for the nucleus\n    q_nucleus = q[nucleus_indices]\n    s_nucleus = s[nucleus_indices]\n    \n    # Renormalize the PMF\n    norm_constant_cp = np.sum(q_nucleus)\n    \n    if norm_constant_cp == 0:\n        # This case occurs if the nucleus is empty (prevented by k>=1) or\n        # if all selected probabilities are zero.\n        # Entropy of a single point mass at 0 is 0. Factuality is ill-defined, 0 is a safe bet.\n        return 0.0, 0.0\n        \n    r_nucleus = q_nucleus / norm_constant_cp\n    \n    # Calculate Shannon entropy (H)\n    # We must handle r_i=0 case where r_i*ln(r_i) is 0.\n    # np.log will produce -inf for 0, then 0*(-inf) is nan.\n    # We can filter out the zero probability terms before calculation.\n    non_zero_r = r_nucleus[r_nucleus > 0]\n    entropy = -np.sum(non_zero_r * np.log(non_zero_r))\n    \n    # Calculate expected factuality (F)\n    factuality = np.sum(r_nucleus * s_nucleus)\n    \n    # Round to 6 decimal places as required\n    return round(entropy, 6), round(factuality, 6)\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the calculations, and prints the results in the specified format.\n    \"\"\"\n    \n    # Case 1 (happy path)\n    q1 = [0.50, 0.20, 0.12, 0.10, 0.08]\n    s1 = [0.40, 0.80, 0.30, 0.60, 0.90]\n    p1 = 0.85\n\n    # Case 2 (near-zero threshold boundary)\n    q2 = q1\n    s2 = s1\n    p2 = 0.01\n\n    # Case 3 (ties and uniform distribution)\n    q3 = [0.20, 0.20, 0.20, 0.20, 0.20]\n    s3 = [0.90, 0.10, 0.50, 0.50, 0.10]\n    p3 = 0.40\n\n    test_cases_static = [\n        (q1, s1, p1),\n        (q2, s2, p2),\n        (q3, s3, p3),\n    ]\n    \n    results = []\n    for q, s, p in test_cases_static:\n        h, f = calculate_metrics(q, s, p)\n        results.append([h, f])\n\n    # Case 4 (long tail, Zipf-like head, moderate threshold)\n    n4 = 10\n    p4 = 0.90\n    indices4 = np.arange(1, n4 + 1)\n    q4_unnormalized = 1.0 / indices4\n    z4 = np.sum(q4_unnormalized)\n    q4 = q4_unnormalized / z4\n    s4 = 1.0 - 0.05 * indices4\n    h4, f4 = calculate_metrics(q4, s4, p4)\n    results.append([h4, f4])\n\n    # Case 5 (full distribution boundary)\n    p5 = 1.00\n    q5 = q4\n    s5 = s4\n    h5, f5 = calculate_metrics(q5, s5, p5)\n    results.append([h5, f5])\n\n    # Format the final output string\n    # e.g., [[1.170067,0.495652],[0.0,0.4],[0.693147,0.5],[1.813253,0.852825],[2.030364,0.805396]]\n    results_str = [f\"[{h:.6f},{f:.6f}]\" for h, f in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        }
    ]
}