{
    "hands_on_practices": [
        {
            "introduction": "Ensuring the correctness of tensor operations is the first step in building reliable models. This practice  moves beyond simple numerical checks to explore how fundamental algebraic properties, such as invariance under a change of basis, can be used to create powerful and robust unit tests. By reasoning about these properties, you can design tests that detect subtle yet critical bugs, like incorrect axis contractions, which might otherwise go unnoticed.",
            "id": "3143508",
            "problem": "A data scientist implements a linear layer in a sequence model using tensor contraction. Let $X \\in \\mathbb{R}^{B \\times T \\times D}$ denote a mini-batch ($B$) of sequences over $T$ time steps with $D$ feature channels per time step, and let $W \\in \\mathbb{R}^{D \\times K}$ be a weight matrix mapping features to $K$ outputs. The intended operation is to produce $Y \\in \\mathbb{R}^{B \\times T \\times K}$ via contraction of the last axis of $X$ with the first axis of $W$, i.e., $Y_{b,t,k}$ should equal the sum over the feature index $i$ of $X_{b,t,i} W_{i,k}$ for each $b$, $t$, and $k$. Suppose a bug contracts the wrong axis (for instance, the time axis), which can silently pass shape checks when $T=D$ but yields incorrect semantics.\n\nYou are tasked with selecting a minimal set of unit tests based on invariances and equivariances that would reliably detect axis misalignment, regardless of particular numeric values and even in cases where $T=D$ makes shapes deceptively compatible. Base your choice on fundamental properties of tensor contraction, the identity matrix, permutation matrices, and orthogonal change of basis.\n\nWhich option describes a minimal set of unit tests that would detect an axis misalignment in this scenario?\n\nA. Two tests:\n- Identity-weight test: choose $D=K$ and set $W = I_{D}$; assert $f(X,W) = X$ exactly.\n- Time-permutation equivariance: let $\\Pi \\in \\mathbb{R}^{T \\times T}$ be a permutation matrix; assert $f(X \\Pi^{\\top}, W) = (f(X,W)) \\Pi^{\\top}$.\n\nB. Single test:\n- Feature-axis change-of-basis invariance: choose a random orthogonal matrix $Q \\in \\mathbb{R}^{D \\times D}$ with $Q^{\\top} Q = I_{D}$ and set $W' = Q^{-1} W$; assert $f(X Q, W') = f(X, W)$.\n\nC. Single test:\n- Linearity in $X$: choose $X_{1}, X_{2} \\in \\mathbb{R}^{B \\times T \\times D}$; assert $f(X_{1}+X_{2}, W) = f(X_{1}, W) + f(X_{2}, W)$.\n\nD. Two tests:\n- Batch-permutation equivariance: let $P \\in \\mathbb{R}^{B \\times B}$ be a permutation matrix; assert $f(P X, W) = P f(X, W)$.\n- Positive homogeneity: for any $\\alpha \\in \\mathbb{R}$, assert $f(\\alpha X, W) = \\alpha f(X, W)$.\n\nHere $f$ denotes the implemented tensor contraction that is supposed to realize the described layer. Pick the option that yields the minimal set of tests that would detect the misalignment robustly and does not include redundant checks.",
            "solution": "Let $f_{correct}(X, W)$ be the intended operation and $f_{bug}(X, W)$ be the buggy implementation.\n\nThe correct operation is given by the formula:\n$$ (f_{correct}(X, W))_{b,t,k} = \\sum_{i=1}^{D} X_{b,t,i} W_{i,k} $$\nThis operation is a linear transformation defined by $W$ applied to each $D$-dimensional feature vector $X_{b,t,:}$ for each batch $b$ and time step $t$.\n\nThe buggy operation, as described, contracts the time axis of $X$ (axis 1, dimension $T$) with the feature axis of $W$ (axis 0, dimension $D$). This is computationally possible only if $T=D$. A plausible form for the buggy operation when $T=D$ is:\n$$ (f_{bug}(X, W))_{b,i,k} = \\sum_{t=1}^{T} X_{b,t,i} W_{t,k} $$\nNote the swapping of roles for indices $t$ and $i$. We must find a test that $f_{correct}$ passes but $f_{bug}$ fails. The test set must also be minimal.\n\n#### Evaluation of Option C\n**C. Single test: Linearity in $X$.**\nThis test asserts that $f(X_{1}+X_{2}, W) = f(X_{1}, W) + f(X_{2}, W)$. Both the correct and buggy operations are tensor contractions, which are fundamentally linear operations. This test checks for linearity, which both implementations possess. Therefore, it cannot distinguish between them.\n**Verdict: Incorrect.**\n\n#### Evaluation of Option D\n**D. Two tests: Batch-permutation equivariance and Positive homogeneity.**\nBoth positive homogeneity and batch-permutation equivariance are general properties that hold because the operation (correct or buggy) is linear and applied independently to each batch element. Both tests will pass for both implementations and are not discriminative.\n**Verdict: Incorrect.**\n\n#### Evaluation of Option A\n**A. Two tests: Identity-weight test and Time-permutation equivariance.**\n1.  **Identity-weight test**: For $D=K$, set $W=I_D$. Assert $f(X, W) = X$.\n    -   For $f_{correct}$: $(f_{correct}(X, I_D))_{b,t,k} = \\sum_{i} X_{b,t,i} \\delta_{ik} = X_{b,t,k}$. The test passes.\n    -   For $f_{bug}$ (requires $T=D=K$): $(f_{bug}(X, I_D))_{b,i,k} = \\sum_{t} X_{b,t,i} \\delta_{tk} = X_{b,k,i}$. The output has its time and feature axes swapped relative to the input. Unless $X$ is symmetric, this test fails. This test is effective.\n2.  **Time-permutation equivariance**: Let $\\Pi \\in \\mathbb{R}^{T \\times T}$ be a permutation matrix. Assert $f(X \\Pi^{\\top}, W) = (f(X,W)) \\Pi^{\\top}$.\n    -   For $f_{correct}$: The operation is independent for each time-slice. Permuting time-slices before or after the operation gives the same result. The test passes.\n    -   For $f_{bug}$: The operation involves summing over the time axis. This summation breaks the independence between time steps. Permuting the time steps changes the sum. The test fails. This test is also effective.\n\nBoth tests are capable of detecting the bug. However, the problem asks for a **minimal set**. Since either test is sufficient on its own, a set containing both is not minimal.\n**Verdict: Incorrect.**\n\n#### Evaluation of Option B\n**B. Single test: Feature-axis change-of-basis invariance.**\nThis test asserts that for any orthogonal matrix $Q \\in \\mathbb{R}^{D \\times D}$, $f(X Q, Q^{\\top} W) = f(X, W)$.\n-   For $f_{correct}$: This property describes the behavior of a linear map under a change of basis. We apply a change of basis $Q$ to the input feature vectors and apply the inverse transformation $Q^{\\top}$ to the matrix of the linear map $W$. The result of the overall transformation must be invariant.\n    $$ (f_{correct}(XQ, Q^{\\top}W))_{b,t,k} = \\sum_{i} (XQ)_{b,t,i} (Q^{\\top}W)_{i,k} $$\n    $$ = \\sum_{i} \\left(\\sum_{j} X_{b,t,j} Q_{j,i}\\right) \\left(\\sum_{l} (Q^{\\top})_{i,l} W_{l,k}\\right) = \\sum_{j,l} X_{b,t,j} W_{l,k} \\left(\\sum_{i} Q_{j,i} (Q^{\\top})_{i,l}\\right) $$\n    Since $Q Q^{\\top} = I_D$, the term in parentheses is $\\delta_{jl}$. The expression simplifies to $(f_{correct}(X,W))_{b,t,k}$. The test passes.\n-   For $f_{bug}$ (requires $T=D$): The change of basis $Q$ is applied to the feature axis of $X$. The buggy implementation, however, contracts over the time axis.\n    $$ (f_{bug}(XQ, Q^{\\top}W))_{b,i,k} = \\sum_{t} (XQ)_{b,t,i} (Q^{\\top}W)_{t,k} = \\sum_{t} \\left(\\sum_{j} X_{b,t,j} Q_{j,i}\\right) \\left(\\sum_{l} (Q^{\\top})_{t,l} W_{l,k}\\right) $$\n    This expression does not simplify because the matrices $Q$ and $Q^\\top$ do not cancel. The test will fail for a generic choice of inputs.\n\nThis single test is sufficient to detect the axis misalignment. It correctly probes the central property of the intended operation: that it is a linear map acting on the $D$-dimensional feature space. Since it is a single, sufficient test, it is minimal.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "In practice, we often need to simulate large batch sizes on memory-constrained hardware, a common scenario addressed by gradient accumulation. This exercise  delves into the mathematical underpinnings of this technique, asking you to identify when summing gradients from microbatches is truly equivalent to computing a single large-batch gradient. You will discover that the answer hinges on the linear properties of differentiation and whether operations like Batch Normalization introduce dependencies between examples in a batch.",
            "id": "3143513",
            "problem": "Consider a supervised learning setting with inputs $\\boldsymbol{x}_i \\in \\mathbb{R}^{n}$ and targets $\\boldsymbol{y}_i \\in \\mathbb{R}^{m}$ for $i \\in \\{1,\\dots,N\\}$. A feedforward model with parameters $(\\boldsymbol{W}, \\boldsymbol{b})$ maps each input to a prediction via $f(\\boldsymbol{x}_i; \\boldsymbol{W}, \\boldsymbol{b}) = \\phi(\\boldsymbol{W}\\boldsymbol{x}_i + \\boldsymbol{b})$, where $\\phi$ is an elementwise nonlinearity that may be the identity function or the Rectified Linear Unit (ReLU). The per-example loss is $\\ell_i = \\frac{1}{2}\\lVert f(\\boldsymbol{x}_i; \\boldsymbol{W}, \\boldsymbol{b}) - \\boldsymbol{y}_i \\rVert_2^2$, and the batch loss is either the sum $\\mathcal{L}_{\\mathrm{sum}}(\\boldsymbol{W}, \\boldsymbol{b}) = \\sum_{i=1}^{N} \\ell_i$ or the mean $\\mathcal{L}_{\\mathrm{mean}}(\\boldsymbol{W}, \\boldsymbol{b}) = \\frac{1}{N}\\sum_{i=1}^{N} \\ell_i$. You perform gradient accumulation over $K$ microbatches of sizes $n_1,\\dots,n_K$ that partition the set $\\{1,\\dots,N\\}$ (i.e., $\\sum_{k=1}^{K} n_k = N$). In the accumulation scheme, you compute gradients for each microbatch and combine them before a single optimizer step.\n\nNow consider two variants of the model:\n- Variant 1: The model uses only standard linear algebra operations over each example independently (matrix-vector products and elementwise nonlinearities) with no cross-example operations.\n- Variant 2: The model inserts Batch Normalization (BN) immediately after the affine preactivation, i.e., the preactivation $\\boldsymbol{z}_i = \\boldsymbol{W}\\boldsymbol{x}_i + \\boldsymbol{b}$ is normalized using batch-dependent statistics before applying $\\phi$. Batch Normalization (BN) computes its mean and variance from the current batch it sees during forward propagation.\n\nAssume the optimizer is Stochastic Gradient Descent (SGD) with momentum that performs exactly one parameter update after all $K$ microbatches have been processed and their gradients accumulated. No optimizer update is performed between microbatches.\n\nBased on first principles of linear algebra and multivariable calculus (definitions of gradient, linearity of summation, and the chain rule), decide which statements are true about when gradient accumulation over microbatches is exactly equivalent to a single large-batch computation.\n\nSelect all that apply:\n- A. In Variant 1, if gradients from each microbatch are summed and then divided by $N$ (to match the mean-loss convention), the resulting gradient tensor with respect to $(\\boldsymbol{W}, \\boldsymbol{b})$ is exactly equal to the gradient computed on the single concatenated batch, regardless of the specific nonlinearity $\\phi$.\n\n- B. If BN in Variant 2 uses statistics computed separately within each microbatch, then the accumulated gradient over microbatches is generally equal to the gradient obtained by processing the entire batch at once with BN statistics computed over all $N$ examples.\n\n- C. For losses that couple examples within a batch (for instance, a contrastive softmax that uses all examples as negatives, where denominators involve sums over all examples), naive microbatch accumulation is equivalent to a single large batch only if the coupling terms are computed over the entire set of $N$ examples across microbatches.\n\n- D. With SGD with momentum, performing one optimizer update after accumulating a single gradient that represents the whole batch yields the same parameter update as using the single large batch directly, provided the optimizer state is identical and no intermediate updates occur.\n\n- E. Nonlinear activations such as ReLU break the linear additivity of gradients over examples, so accumulation over microbatches can never match the gradient from a single large batch, even in Variant 1.\n\nAnswer as a multiple-choice selection.",
            "solution": "The fundamental principle governing the equivalence between a single large-batch gradient and an accumulated microbatch gradient lies in the linearity of the differentiation operator. For a total loss that is a sum of per-example losses, $\\mathcal{L} = \\sum_{i=1}^{N} \\ell_i$, the gradient with respect to any parameter (or vector of parameters) $\\boldsymbol{\\theta}$ is:\n$$\n\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L} = \\nabla_{\\boldsymbol{\\theta}} \\left( \\sum_{i=1}^{N} \\ell_i \\right) = \\sum_{i=1}^{N} \\nabla_{\\boldsymbol{\\theta}} \\ell_i\n$$\nThis mathematical identity holds universally. The equivalence holds if and only if the computation of the per-example loss $\\ell_i$ and its gradient $\\nabla_{\\boldsymbol{\\theta}} \\ell_i$ for a given example $i$ is independent of all other examples $j \\neq i$ in the batch.\n\nLet's analyze the variants:\n*   **Variant 1**: The computation of $f(\\boldsymbol{x}_i)$ and $\\ell_i$ only involves $\\boldsymbol{x}_i$ and $\\boldsymbol{y}_i$. There are no cross-example operations. Thus, each $\\ell_i$ is independent, and gradient accumulation is exactly equivalent to a single large-batch computation.\n*   **Variant 2**: Batch Normalization (BN) introduces a dependency of $\\ell_i$ on all other examples in the batch $B'$. The statistics $(\\boldsymbol{\\mu}_{B_k}, \\boldsymbol{\\sigma}^2_{B_k})$ for a microbatch $B_k$ will differ from the statistics $(\\boldsymbol{\\mu}_{B}, \\boldsymbol{\\sigma}^2_{B})$ of the full batch $B$. Hence, the accumulated gradient will not be equivalent to the large-batch gradient.\n\nNow, we evaluate each option.\n\n**A. In Variant 1, if gradients from each microbatch are summed and then divided by $N$ (to match the mean-loss convention), the resulting gradient tensor with respect to $(\\boldsymbol{W}, \\boldsymbol{b})$ is exactly equal to the gradient computed on the single concatenated batch, regardless of the specific nonlinearity $\\phi$.**\nThe gradient of the mean loss over the full batch is $\\nabla \\mathcal{L}_{\\mathrm{mean}} = \\frac{1}{N}\\sum_{i=1}^{N} \\nabla \\ell_i$. In Variant 1, $\\nabla \\ell_i$ is independent of other examples. The accumulation procedure computes $\\frac{1}{N} \\sum_{k=1}^K (\\sum_{i \\in B_k} \\nabla \\ell_i) = \\frac{1}{N}\\sum_{i=1}^N \\nabla \\ell_i$, which is identical to the large-batch gradient. The elementwise nature of $\\phi$ does not introduce cross-example dependencies. This statement is **Correct**.\n\n**B. If BN in Variant 2 uses statistics computed separately within each microbatch, then the accumulated gradient over microbatches is generally equal to the gradient obtained by processing the entire batch at once with BN statistics computed over all $N$ examples.**\nAs established, the BN statistics for a microbatch will differ from the statistics of the full batch. Because the normalization applied during the forward pass is different, the resulting loss function and its gradient are also different. The accumulated sum of gradients from microbatches is not equivalent to the gradient of the large batch. This statement is **Incorrect**.\n\n**C. For losses that couple examples within a batch (for instance, a contrastive softmax that uses all examples as negatives, where denominators involve sums over all examples), naive microbatch accumulation is equivalent to a single large batch only if the coupling terms are computed over the entire set of $N$ examples across microbatches.**\nLosses like contrastive softmax introduce explicit dependencies between examples. \"Naive\" accumulation would compute the normalization term only over the microbatch, thus changing the loss function itself. To achieve equivalence, the per-example loss $\\ell_i$ must be consistently defined in both scenarios. This requires that for any example $i$, the coupling terms (e.g., the set of \"negatives\") must be computed using the full set of $N$ examples. This is the necessary (\"only if\") condition for equivalence. This statement is **Correct**.\n\n**D. With SGD with momentum, performing one optimizer update after accumulating a single gradient that represents the whole batch yields the same parameter update as using the single large batch directly, provided the optimizer state is identical and no intermediate updates occur.**\nThe optimizer update rule is a deterministic function of the current parameters, optimizer state (velocity), and the computed gradient. If the accumulated gradient is exactly equal to the large-batch gradient, and the initial states are identical, then applying the deterministic update rule will produce the exact same final parameters. This statement accurately describes the correct implementation of gradient accumulation. This statement is **Correct**.\n\n**E. Nonlinear activations such as ReLU break the linear additivity of gradients over examples, so accumulation over microbatches can never match the gradient from a single large batch, even in Variant 1.**\nThis statement misunderstands the required linearity. The equivalence of accumulation depends on the linearity of the differentiation operator, $\\nabla(\\sum \\ell_i) = \\sum \\nabla \\ell_i$, which holds regardless of how nonlinear $\\ell_i$ is. An elementwise nonlinearity like ReLU is part of the computation of each independent $\\ell_i$ and does not introduce inter-example dependencies. Accumulation remains exactly equivalent in Variant 1. This statement is **Incorrect**.",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "Beyond mathematical correctness, the performance of deep learning models depends critically on how efficiently tensor operations are executed on hardware. This problem  challenges you to think like a performance engineer, optimizing a batched matrix multiplication by considering its memory access pattern. By reordering tensor axes and analyzing the resulting memory strides, you will learn how to maximize cache utilization and improve the arithmetic intensity of your computations.",
            "id": "3143504",
            "problem": "A deep learning model performs a batched linear transformation where, for each batch index $b$, the matrix $\\mathbf{A}^{(b)}$ of shape $(n \\times k)$ multiplies the matrix $\\mathbf{B}^{(b)}$ of shape $(k \\times m)$ to produce $\\mathbf{C}^{(b)}$ of shape $(n \\times m)$. In tensor form, the inputs are $\\mathbf{A} \\in \\mathbb{R}^{b \\times n \\times k}$ and $\\mathbf{B} \\in \\mathbb{R}^{b \\times k \\times m}$, and the output is $\\mathbf{C} \\in \\mathbb{R}^{b \\times n \\times m}$. The matrices are stored in row-major (C-contiguous) layout with $32$-bit floating point elements (that is, $4$ bytes per element). The hardware fetches memory in cache lines of size $64$ bytes. You must represent the batch operation as an Einstein summation contraction via the function $\\mathrm{einsum}$ and propose an optimization that reorders axes to increase arithmetic intensity (Floating Point Operations (FLOPs) per byte moved) by improving spatial locality, justified via tensor stride arithmetic.\n\nWork from first principles:\n- Use the core definition of matrix multiplication, $C_{b,i,j} = \\sum_{t=1}^{k} A_{b,i,t}\\,B_{b,t,j}$, and the definition of Einstein summation convention, where repeated indices indicate summation.\n- Use the definition of row-major strides: for a tensor with shape $(d_0, d_1, d_2)$, the stride in elements along axis $0$ is $d_1 d_2$, along axis $1$ is $d_2$, and along axis $2$ is $1$. The byte stride equals the element stride times $4$.\n- Assume the kernel computes each output element $C_{b,i,j}$ by accumulating over the reduction dimension $k$ as the innermost loop. For spatial locality, contiguous access (stride $1$ in elements) along the innermost loop uses each cache line fully, whereas a stride of at least $16$ elements (because $64$ bytes per line and $4$ bytes per element) causes each access to likely land on a distinct cache line with minimal reuse.\n\nLet the concrete sizes be $b=32$, $n=64$, $k=128$, $m=64$. Choose the option that both:\n(i) provides a correct $\\mathrm{einsum}$ contraction string that computes $\\mathbf{C}$ from $\\mathbf{A}$ and $\\mathbf{B}$, and\n(ii) proposes an axis reordering that maximizes arithmetic intensity by making the reduction dimension $k$ contiguous in memory in both inputs and making output writes contiguous along the innermost produced dimension.\n\nWhich option satisfies the above and achieves the best FLOP-to-memory ratio under the stated model?\n\nA. Use $\\mathrm{einsum}(\\texttt{'bnk,bmk->bnm'})$ after transposing $\\mathbf{B}$ to shape $(b,m,k)$, keep $\\mathbf{A}$ as $(b,n,k)$, and write $\\mathbf{C}$ as $(b,n,m)$.\n\nB. Use $\\mathrm{einsum}(\\texttt{'bnk,bkm->bnm'})$ with the original shapes $\\mathbf{A}$ as $(b,n,k)$ and $\\mathbf{B}$ as $(b,k,m)$, writing $\\mathbf{C}$ as $(b,n,m)$.\n\nC. Use $\\mathrm{einsum}(\\texttt{'bkn,bkm->bmn'})$ after transposing $\\mathbf{A}$ to $(b,k,n)$, keep $\\mathbf{B}$ as $(b,k,m)$, and write $\\mathbf{C}$ as $(b,m,n)$.\n\nD. Use $\\mathrm{einsum}(\\texttt{'bnk,bmk->bmn'})$ after transposing $\\mathbf{B}$ to $(b,m,k)$, keep $\\mathbf{A}$ as $(b,n,k)$, and write $\\mathbf{C}$ as $(b,m,n)$.",
            "solution": "#### 1. First Principles: Strides and Baseline Performance\n\nThe problem uses row-major memory layout. For a tensor with shape $(d_0, d_1, ..., d_{N-1})$, the stride in elements for axis $i$ is $S_i = \\prod_{j=i+1}^{N-1} d_j$, with $S_{N-1} = 1$. The operation is the batched matrix multiplication $C_{b,i,j} = \\sum_{t=1}^{k} A_{b,i,t}\\,B_{b,t,j}$. The index $t$ corresponds to the dimension of size $k$. The standard `einsum` representation for this is `'bnk,bkm->bnm'`. This corresponds to Option B, which we analyze as the baseline.\n\n**Baseline Analysis (Original Layout - Option B):**\n- $\\mathbf{A}$ has shape $(b, n, k) = (32, 64, 128)$. Its element strides are $(n \\times k, k, 1) = (8192, 128, 1)$.\n- $\\mathbf{B}$ has shape $(b, k, m) = (32, 128, 64)$. Its element strides are $(k \\times m, m, 1) = (8192, 64, 1)$.\n\nThe kernel's innermost loop iterates over the reduction dimension $k$.\n- Accessing $A_{b,i,t}$: As $t$ increments, we access elements along the last dimension of $\\mathbf{A}$. The stride is $1$. This is contiguous and demonstrates excellent spatial locality.\n- Accessing $B_{b,t,j}$: As $t$ increments, we access elements along the middle dimension of $\\mathbf{B}$. The stride is the stride of the $k$-dimension, which is $m=64$ elements. Since $64 > 16$ (the number of elements per cache line), each memory access is likely to be in a different cache line, leading to very poor cache utilization. This is the performance bottleneck. Option B is therefore incorrect as it is not optimized.\n\n#### 2. Optimization Strategy\n\nTo satisfy the optimization goal, the reduction dimension $k$ must be the last dimension (stride $1$) for both input tensors to ensure contiguous access.\n- $\\mathbf{A}$ already has shape $(b,n,k)$, so its 'k' dimension is already the last one. We keep it as is.\n- $\\mathbf{B}$ has shape $(b,k,m)$. To make 'k' the last dimension, we must transpose the last two axes, creating a new tensor $\\mathbf{B'}$ with shape $(b,m,k)$. This new tensor will be C-contiguous with its new shape.\n- The elements of $\\mathbf{B'}$ are related to $\\mathbf{B}$ by $B'_{b,j,t} = B_{b,t,j}$.\n- The multiplication formula becomes: $C_{b,i,j} = \\sum_{t=1}^{k} A_{b,i,t}\\,B'_{b,j,t}$.\n\nThe `einsum` string for this new operation involves inputs with indices corresponding to `bnk` for $\\mathbf{A}$ and `bmk` for $\\mathbf{B'}$. The summation is over `k`, and the output indices are `b`, `n`, `m`. This leads to the string `'bnk,bmk->bnm'`.\n\n#### 3. Option-by-Option Analysis\n\n**A. Use $\\mathrm{einsum}(\\texttt{'bnk,bmk->bnm'})$ after transposing $\\mathbf{B}$ to shape $(b,m,k)$, keep $\\mathbf{A}$ as $(b,n,k)$, and write $\\mathbf{C}$ as $(b,n,m)$.**\n- **Correctness:** As derived above, transposing $\\mathbf{B}$ to $\\mathbf{B'}$ of shape $(b,m,k)$ and using the `einsum` string `'bnk,bmk->bnm'` correctly computes $C_{b,i,j} = \\sum_{t} A_{b,i,t}B'_{b,j,t} = \\sum_{t} A_{b,i,t}B_{b,t,j}$. The output shape $(b,n,m)$ is also correct.\n- **Performance:**\n    - Input $\\mathbf{A}$ (shape $(b,n,k)$): Accessing over `k` has stride $1$. Optimal.\n    - Input $\\mathbf{B'}$ (shape $(b,m,k)$): Accessing over `k` has stride $1$. Optimal.\n    - Output $\\mathbf{C}$ (shape $(b,n,m)$): For a typical loop nest `for b, for n, for m, ...`, writing to $C_{b,n,m}$ while incrementing $m$ means accessing memory with stride $1$. Optimal.\n- **Verdict:** **Correct**. This option satisfies all stated requirements and achieves the desired optimization.\n\n**C. Use $\\mathrm{einsum}(\\texttt{'bkn,bkm->bmn'})$ after transposing $\\mathbf{A}$ to $(b,k,n)$, keep $\\mathbf{B}$ as $(b,k,m)$, and write $\\mathbf{C}$ as $(b,m,n)$.**\n- **Correctness:** This option computes $C'_{b,j,i} = \\sum_t A'_{b,t,i} B_{b,t,j}$, which calculates the transpose of the desired matrix, $\\mathbf{C}^T$.\n- **Performance:**\n    - Input $\\mathbf{A'}$ (shape $(b,k,n)$): Accessing over the reduction dimension `k` (the middle dimension) has a stride of $n=64$. Inefficient.\n    - Input $\\mathbf{B}$ (shape $(b,k,m)$): Accessing over `k` has a stride of $m=64$. Inefficient.\n- **Verdict:** **Incorrect**. The reordering leads to poor memory access for both inputs.\n\n**D. Use $\\mathrm{einsum}(\\texttt{'bnk,bmk->bmn'})$ after transposing $\\mathbf{B}$ to $(b,m,k)$, keep $\\mathbf{A}$ as $(b,n,k)$, and write $\\mathbf{C}$ as $(b,m,n)$.**\n- **Correctness:** This uses the same optimized inputs as Option A, but the `einsum` string `'bnk,bmk->bmn'` computes $C'_{b,j,i} = \\sum_t A_{b,i,t}B'_{b,j,t} = C_{b,i,j}$. The output shape $(b,m,n)$ indicates it produces the transpose of the desired matrix, $\\mathbf{C}^T$.\n- **Performance:** While the core computation is optimized, it does not produce the final output in its specified native shape $(b,n,m)$. It would require an additional transpose, making it an inferior strategy to Option A, which produces the result directly.\n- **Verdict:** **Incorrect**.\n\n#### Conclusion\n\nOption A provides the correct `einsum` string for the optimized memory layout that directly produces the output tensor $\\mathbf{C}$ in its specified shape $(b,n,m)$. It achieves optimal spatial locality for both input reads (stride $1$ on the reduction dimension) and output writes (stride $1$ on the innermost produced dimension).",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}