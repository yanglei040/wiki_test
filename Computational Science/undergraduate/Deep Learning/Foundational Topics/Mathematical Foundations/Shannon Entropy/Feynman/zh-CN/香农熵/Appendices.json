{
    "hands_on_practices": [
        {
            "introduction": "要掌握香农熵，最好的起点是将其应用于我们熟悉的数据类型。此练习将指导您计算图像像素分布的熵，通过比较灰度图像和彩色图像，您将亲手量化信息内容。此练习旨在巩固熵的基本计算，并揭示数据维度与估计其分布所需的样本数量之间的重要关系 。",
            "id": "3174049",
            "problem": "在深度学习的图像分类任务中，给定两类数据集：单通道的灰度图像和三通道（红、绿、蓝）的彩色图像。每个像素被建模为一个在有限字母表上取值的离散随机变量。在灰度图像的情况下，一个像素由单个离散随机变量 $X$ 建模，其概率质量函数分布在 $m$ 个强度区间上。在彩色图像的情况下，一个像素由一组三个离散随机变量 $(X_{R}, X_{G}, X_{B})$ 建模，每个通道对应一个变量。在整个问题中，假设在像素级别上，各通道是统计独立的。\n\n从基本定义出发，推导并实现像素在灰度图像和彩色图像中的不确定性有何不同，以及这种差异如何影响以对训练深度分类器有用的方式可靠地估计像素分布所需的样本数量。具体而言：\n\n- 使用离散随机变量的 Shannon 熵的标准定义（以比特为单位，即使用以 2 为底的对数）作为每像素不确定性的度量。对于彩色数据集，仅使用熵的独立性属性，从各通道的熵中获得每像素的联合熵。\n- 对于样本复杂度，使用一个来自经验分布估计的成熟结论：为确保一个多项式随机变量的经验分布的期望 $\\ell_{1}$ 误差至多为容差 $\\varepsilon$，独立样本的数量 $n$ 必须满足形式为 $n \\geq c \\cdot \\frac{|\\mathcal{A}| - 1}{\\varepsilon^{2}}$ 的不等式，其中 $|\\mathcal{A}|$ 是字母表大小，而 $c$ 是一个不依赖于 $|\\mathcal{A}|$ 或 $\\varepsilon$ 的常数。为了本问题的目的，取 $c = 1$ 以获得一个保守的缩放定律，并将其视为深度分类器为将像素级分布估计到所需容差而需要的标记样本数量的下界代理。你的程序应通过向上取整到最近的整数来计算 $n$。\n\n你的任务是编写一个完整的程序，该程序：\n1. 计算灰度数据集和彩色数据集的每像素熵（以比特为单位），假设通道间相互独立。\n2. 使用界 $n = \\left\\lceil \\frac{|\\mathcal{A}| - 1}{\\varepsilon^{2}} \\right\\rceil$ 计算灰度数据集和彩色数据集的代理样本复杂度 $n$。\n\n所有数学量必须精确处理，并遵循以下约定：\n- 对于熵，使用以 2 为底的对数，因此单位是比特。\n- 使用约定 $0 \\log 0 = 0$。\n- 对于灰度字母表大小，使用 $m = | \\text{bins for } X |$。\n- 对于彩色字母表大小，使用 $m_{\\text{color}} = | \\text{bins for } X_{R} | \\cdot | \\text{bins for } X_{G} | \\cdot | \\text{bins for } X_{B} |$。\n\n实现以下参数值的测试套件。每个测试用例都为灰度图像和每个颜色通道指定了概率质量函数，以及一个容差 $\\varepsilon$：\n- 测试用例 1 (正常情况，均匀分布): 灰度 $p = [0.25, 0.25, 0.25, 0.25]$，彩色通道与灰度相同，$\\varepsilon = 0.05$。\n- 测试用例 2 (边界情况，退化灰度): 灰度 $p = [1.0, 0.0, 0.0, 0.0]$，彩色 $p_{R} = p_{G} = p_{B} = [0.97, 0.01, 0.01, 0.01]$，$\\varepsilon = 0.05$。\n- 测试用例 3 (异构通道): 灰度 $p = [0.10, 0.20, 0.30, 0.40]$，彩色 $p_{R} = [0.70, 0.10, 0.10, 0.10]$, $p_{G} = [0.50, 0.20, 0.20, 0.10]$, $p_{B} = [0.25, 0.25, 0.25, 0.25]$, $\\varepsilon = 0.02$。\n- 测试用例 4 (小容差，相同分布): 灰度 $p = [0.40, 0.40, 0.10, 0.10]$，彩色通道与灰度相同，$\\varepsilon = 0.01$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果必须是一个列表 $[H_{\\text{gray}}, H_{\\text{color}}, n_{\\text{gray}}, n_{\\text{color}}]$，其中 $H_{\\text{gray}}$ 和 $H_{\\text{color}}$ 是浮点数（单位为比特），$n_{\\text{gray}}$ 和 $n_{\\text{color}}$ 是整数。例如：$[[h_{1}, h_{1}^{c}, n_{1}, n_{1}^{c}], [h_{2}, h_{2}^{c}, n_{2}, n_{2}^{c}], \\dots]$。不应打印任何单位，但熵必须以比特为单位计算，并且 $n$ 是无量纲的样本计数。",
            "solution": "该问题是有效的，因为它在科学上基于信息论和统计学，问题提出得很好，提供了所有必要的信息，并且其表述是客观的。我们将逐步进行求解。\n\n该问题要求我们从信息论和统计估计的角度比较灰度图像和彩色图像数据的复杂性。我们将像素值建模为离散随机变量，并使用 Shannon 熵作为不确定性的度量，以及一个推导出的样本复杂度公式来估计潜在的概率分布。\n\n### 1. 每像素不确定性：Shannon 熵\n\n对于一个字母表为 $\\mathcal{X}$、概率质量函数 (PMF) 为 $p(x) = P(X=x)$ 的离散随机变量 $X$，其不确定性或“信息内容”的基本度量是 Shannon 熵 $H(X)$。当使用以 2 为底的对数时，熵的单位是比特。其定义为：\n$$ H(X) = - \\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x) $$\n按照惯例，我们定义 $0 \\log_2 0 = 0$ 来处理概率为零的结果。\n\n#### 灰度图像熵 ($H_{\\text{gray}}$)\n一个灰度像素被建模为在 $m$ 个强度区间的字母表上的单个离散随机变量 $X$。给定一个 PMF $p_{\\text{gray}} = [p_1, p_2, \\ldots, p_m]$，每像素熵可以直接根据定义计算：\n$$ H_{\\text{gray}} = H(X) = - \\sum_{i=1}^{m} p_i \\log_2 p_i $$\n\n#### 彩色图像熵 ($H_{\\text{color}}$)\n一个彩色像素被建模为三个随机变量的向量 $(X_R, X_G, X_B)$，每个颜色通道对应一个。该问题陈述了一个关键假设：各通道在统计上是独立的。对于独立的随机变量，联合熵就是它们各自熵的总和：\n$$ H(X_R, X_G, X_B) = H(X_R) + H(X_G) + H(X_B) $$\n因此，彩色图像的每像素熵 $H_{\\text{color}}$ 由每个通道的 PMF ($p_R$, $p_G$, $p_B$) 的熵之和给出：\n$$ H_{\\text{color}} = \\left( - \\sum_{i} p_{R,i} \\log_2 p_{R,i} \\right) + \\left( - \\sum_{j} p_{G,j} \\log_2 p_{G,j} \\right) + \\left( - \\sum_{k} p_{B,k} \\log_2 p_{B,k} \\right) $$\n\n### 2. 用于分布估计的样本复杂度\n\n该问题提供了一个代理，用于表示可靠地估计多项式随机变量的 PMF 所需的样本数量 $n$。这个量被称为样本复杂度，由以下公式给出：\n$$ n = \\left\\lceil \\frac{|\\mathcal{A}| - 1}{\\varepsilon^{2}} \\right\\rceil $$\n其中 $|\\mathcal{A}|$ 是随机变量的字母表大小（可能结果的数量），$\\varepsilon$ 是期望 $\\ell_{1}$ 误差的目标容差，$\\lceil \\cdot \\rceil$ 是向上取整函数，即取不小于该数的最小整数。\n\n这个公式揭示了所需样本数量随着字母表大小 $|\\mathcal{A}|$ 线性增长。\n\n#### 灰度样本复杂度 ($n_{\\text{gray}}$)\n对于灰度像素，随机变量是 $X$，其字母表大小 $|\\mathcal{A}_{\\text{gray}}|$ 是强度区间的数量 $m$。\n$$ n_{\\text{gray}} = \\left\\lceil \\frac{m - 1}{\\varepsilon^{2}} \\right\\rceil $$\n\n#### 彩色样本复杂度 ($n_{\\text{color}}$)\n对于彩色像素，随机变量是向量 $(X_R, X_G, X_B)$。我们正在估计这个向量的联合分布。联合分布的字母表由所有可能的通道值三元组构成。如果各通道分别有 $m_R$、$m_G$ 和 $m_B$ 个区间，则联合字母表的大小是这些单个大小的乘积：\n$$ |\\mathcal{A}_{\\text{color}}| = m_R \\cdot m_G \\cdot m_B $$\n样本复杂度则为：\n$$ n_{\\text{color}} = \\left\\lceil \\frac{(m_R \\cdot m_G \\cdot m_B) - 1}{\\varepsilon^{2}} \\right\\rceil $$\n对于高维数据，字母表大小的这种乘法增长是“维度灾难”的一种表现。它表明，在假设相同容差 $\\varepsilon$ 的情况下，估计彩色像素的联合分布比估计单个灰度通道需要多得多的样本。对于独立变量而言，熵是可加的，其增长速度远慢于样本复杂度，后者依赖于乘法增长的字母表大小。这一区别是本分析的核心要点。\n\n### 每个测试用例的计算摘要\n1.  **计算 $H_{\\text{gray}}$**：将熵公式应用于灰度 PMF。\n2.  **计算 $H_{\\text{color}}$**：计算每个颜色通道 PMF 的熵，并将结果相加。\n3.  **计算 $n_{\\text{gray}}$**：根据灰度 PMF 的长度确定字母表大小 $m$，并应用样本复杂度公式。\n4.  **计算 $n_{\\text{color}}$**：根据通道 PMF 的长度确定字母表大小 $m_R, m_G, m_B$，计算联合字母表大小 $m_R \\cdot m_G \\cdot m_B$，并应用样本复杂度公式。\n\n将为提供的每个测试用例系统地实现这些步骤。",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes per-pixel entropy and sample complexity for grayscale and color image models\n    across a suite of test cases.\n    \"\"\"\n\n    # Test cases: (p_gray, p_red, p_green, p_blue, epsilon)\n    # The structure allows for cases where channel PMFs are distinct or identical to grayscale.\n    p1 = [0.25, 0.25, 0.25, 0.25]\n    p2_gray = [1.0, 0.0, 0.0, 0.0]\n    p2_color_channel = [0.97, 0.01, 0.01, 0.01]\n    p3_gray = [0.10, 0.20, 0.30, 0.40]\n    p3_r = [0.70, 0.10, 0.10, 0.10]\n    p3_g = [0.50, 0.20, 0.20, 0.10]\n    p3_b = [0.25, 0.25, 0.25, 0.25]\n    p4 = [0.40, 0.40, 0.10, 0.10]\n    \n    test_cases = [\n        (p1, p1, p1, p1, 0.05),\n        (p2_gray, p2_color_channel, p2_color_channel, p2_color_channel, 0.05),\n        (p3_gray, p3_r, p3_g, p3_b, 0.02),\n        (p4, p4, p4, p4, 0.01),\n    ]\n\n    results = []\n\n    def calculate_entropy(pmf: np.ndarray) -> float:\n        \"\"\"\n        Calculates the Shannon entropy for a given probability mass function.\n        Handles the 0*log(0) = 0 case.\n        \"\"\"\n        # Filter out zero probabilities to avoid log(0)\n        pmf_nz = pmf[pmf > 0]\n        if pmf_nz.size == 0:\n            return 0.0\n        return -np.sum(pmf_nz * np.log2(pmf_nz))\n\n    def calculate_sample_complexity(alphabet_size: int, epsilon: float) -> int:\n        \"\"\"\n        Calculates the sample complexity n based on the given formula.\n        \"\"\"\n        if alphabet_size == 1:\n            return 0\n        n = math.ceil((alphabet_size - 1) / (epsilon**2))\n        return int(n)\n\n    for case in test_cases:\n        p_gray_list, p_r_list, p_g_list, p_b_list, epsilon = case\n        \n        p_gray = np.array(p_gray_list, dtype=np.float64)\n        p_r = np.array(p_r_list, dtype=np.float64)\n        p_g = np.array(p_g_list, dtype=np.float64)\n        p_b = np.array(p_b_list, dtype=np.float64)\n\n        # 1. Grayscale Calculations\n        h_gray = calculate_entropy(p_gray)\n        m_gray = len(p_gray)\n        n_gray = calculate_sample_complexity(m_gray, epsilon)\n\n        # 2. Color Calculations\n        h_r = calculate_entropy(p_r)\n        h_g = calculate_entropy(p_g)\n        h_b = calculate_entropy(p_b)\n        h_color = h_r + h_g + h_b\n\n        m_r, m_g, m_b = len(p_r), len(p_g), len(p_b)\n        m_color = m_r * m_g * m_b\n        n_color = calculate_sample_complexity(m_color, epsilon)\n        \n        results.append([h_gray, h_color, n_gray, n_color])\n\n    # Format the final output string exactly as required, handling list-of-lists.\n    # The standard str() representation of a list includes spaces, which is fine here.\n    # The example f-string correctly joins string representations of the inner lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "在了解了如何计算单个数据点的熵之后，我们可以将这一概念扩展到分析整个数据集的结构。此练习探讨了如何使用熵来表征分类任务中的类别不平衡问题，并测试一个基于熵的假设来改进模型性能。通过这个实践，您将学会使用熵作为诊断工具，并将其与关键的机器学习评估指标（如少数类召回率和校准误差）联系起来 。",
            "id": "3174037",
            "problem": "给定一个受生态物种识别启发的分类场景。一个固定的物种类别集合由随机变量 $Y$ 表示，其取值范围为 $\\{0,1,\\dots,K-1\\}$。对于每个测试用例，您将获得每个类别的样本数量、每个样本的模型 logits 矩阵以及相应的真实标签。您的任务是计算类别先验的 Shannon 熵 $H(Y)$，并检验通过熵的倒数对 logits 进行重加权是否能改善少数类别的识别效果并校准模型的预测概率。\n\n从以下基本依据出发：\n- 定义了类别 $Y$ 上离散分布的概率公理，其中类别先验概率 $p_k$ 满足 $p_k \\ge 0$ 且 $\\sum_{k=0}^{K-1} p_k = 1$。\n- 结果 $y$ 的自信息的操作性定义为其先验似然的倒数的对数，而 Shannon 熵 $H(Y)$ 是在该先验分布下类别标签的期望自信息。\n- 通过归一化指数函数将 logits 映射到概率的标准方法，该方法为每个样本生成一个有效的分类分布。\n\n您必须实现以下计算，使用自然对数，以使所有信息量都以奈特（nats）为单位：\n1. 通过将非负计数归一化以使其总和为一来，从提供的计数 $\\mathbf{c} = (c_0,\\dots,c_{K-1})$ 计算类别先验概率 $\\mathbf{p} = (p_0,\\dots,p_{K-1})$。然后计算先验分布的 Shannon 熵 $H(Y)$，单位为奈特。以与熵的极限行为一致的方式，在数值上处理计数为零的类别。\n2. 使用为每个样本提供的 logits，通过归一化指数函数将其转换为预测的类别概率。将预测类别定义为每个样本最大概率的索引。计算两个指标：\n   - 少数类别召回率（Minority recall）：将少数类别集合定义为 $\\{k : p_k < 1/K\\}$。计算每个少数类别 $k$ 的召回率，即真实标签为 $k$ 的样本中被正确预测为类别 $k$ 的样本比例，并报告这些少数类别的宏平均值。如果少数类别集合为空，则将少数类别召回率定义为所有类别的宏平均召回率。\n   - 期望校准误差（Expected Calibration Error, ECE）：将区间 $[0,1]$ 划分为 $B=10$ 个等宽的区间（bin）。对于每个区间，计算其预测的 top-1 置信度落入该区间的样本比例、它们的经验准确率以及它们的平均置信度。ECE 是所有区间的样本比例乘以准确率与平均置信度之间绝对差的总和。\n3. 在将 logits 转换为概率之前，通过标量 $s = 1/H(Y)$ 对每个样本的 logits 进行缩放，执行“按熵倒数重加权”。在此缩放后，重新计算少数类别召回率和 ECE。确定相对于未缩放的基线，少数类别召回率是否有所改善，以及 ECE 是否有所改善（严格减小）。\n\n测试套件：\n对于每个测试用例，$K=3$。您将获得类别计数 $\\mathbf{c}$、形状为 $(N,K)$ 的 logits 矩阵 $\\mathbf{L}$，以及长度为 $N$ 的真实标签向量 $\\mathbf{y}$，其中 $N = \\sum_k c_k$。\n\n- 测试用例 1（均衡先验）：\n  - $\\mathbf{c} = [4,4,4]$。\n  - $\\mathbf{L}$ 行（每行为一个长度为 3 的 logits 向量）：\n    $[3.0,1.0,0.0]$, $[2.5,2.0,1.0]$, $[1.0,1.2,0.8]$, $[2.2,-0.5,0.1]$,\n    $[0.5,2.8,1.0]$, $[1.0,2.2,2.0]$, $[0.2,1.5,1.6]$, $[0.1,2.5,-0.3]$,\n    $[0.0,0.5,2.5]$, $[1.0,0.9,1.2]$, $[-0.2,1.3,1.1]$, $[0.3,-0.4,2.0]$.\n  - $\\mathbf{y}$ （$\\mathbf{L}$ 每行对应的真实标签）：$[0,0,0,0,1,1,1,1,2,2,2,2]$。\n\n- 测试用例 2（中度不均衡先验）：\n  - $\\mathbf{c} = [8,3,1]$。\n  - $\\mathbf{L}$ 行：\n    $[3.4,0.1,-0.2]$, $[2.8,0.5,0.0]$, $[3.1,0.3,0.2]$, $[2.0,1.1,0.9]$,\n    $[1.8,1.5,1.2]$, $[2.5,1.6,1.6]$, $[1.7,1.9,1.6]$, $[2.6,0.8,0.7]$,\n    $[1.5,2.7,1.0]$, $[2.2,1.8,0.9]$, $[2.0,2.1,1.9]$, $[2.4,1.3,1.6]$.\n  - $\\mathbf{y}$: $[0,0,0,0,0,0,0,0,1,1,1,2]$。\n\n- 测试用例 3（高度不均衡先验）：\n  - $\\mathbf{c} = [10,1,1]$。\n  - $\\mathbf{L}$ 行：\n    $[3.5,0.2,0.1]$, $[3.0,0.5,0.4]$, $[2.9,0.1,-0.2]$, $[2.3,0.7,0.6]$,\n    $[2.6,1.0,0.9]$, $[3.2,0.8,0.7]$, $[2.8,0.4,0.3]$, $[3.1,1.2,1.0]$,\n    $[2.7,1.4,1.3]$, $[2.4,1.6,1.5]$, $[2.5,2.2,2.1]$, $[2.5,2.1,2.2]$.\n  - $\\mathbf{y}$: $[0,0,0,0,0,0,0,0,0,0,1,2]$。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含三个测试用例的结果，形式为一个用方括号括起来的逗号分隔列表。每个测试用例的结果本身必须是一个列表，顺序如下：\n$[H(Y), \\text{minority\\_recall\\_before}, \\text{minority\\_recall\\_after}, \\text{ECE\\_before}, \\text{ECE\\_after}, \\text{improved\\_recall}, \\text{improved\\_ECE}]$，\n其中前五个条目是浮点数（$H(Y)$ 的单位是奈特；使用自然对数），最后两个条目是布尔值。例如，整体输出应如下所示：\n$[[h_1,r^{\\text{min}}_{1,\\text{before}},r^{\\text{min}}_{1,\\text{after}},e_{1,\\text{before}},e_{1,\\text{after}},b^{\\text{recall}}_1,b^{\\text{ECE}}_1],[\\dots],[\\dots]]$。",
            "solution": "该问题要求通过计算 Shannon 熵、少数类别召回率和期望校准误差（ECE）来分析分类模型的输出，然后在执行所提出的 logit 缩放程序后重新评估这些指标。该解决方案是通过系统地应用概率论、信息论和模型评估的基本原则构建的。\n\n首先，我们定义类别先验概率及相关的 Shannon 熵。给定来自总共 $N$ 个样本的 $K$ 个类别的一组类别计数 $\\mathbf{c} = (c_0, c_1, \\dots, c_{K-1})$，其中 $N = \\sum_{k=0}^{K-1} c_k$，每个类别 $k$ 的先验概率 $p_k$ 被估计为经验频率：\n$$p_k = \\frac{c_k}{N}$$\n这组概率 $\\mathbf{p} = (p_0, \\dots, p_{K-1})$ 构成一个离散概率分布。代表类别标签的随机变量 $Y$ 的 Shannon 熵 $H(Y)$ 是对此分布不确定性的度量。使用自然对数（单位为奈特），它被定义为期望自信息：\n$$H(Y) = -\\sum_{k=0}^{K-1} p_k \\ln(p_k)$$\n在此计算中，我们遵循极限 $\\lim_{x\\to 0^+} x \\ln(x) = 0$，因此计数为零的类别对熵没有贡献。\n\n接下来，我们形式化地定义从模型 logits 到预测的转换。对于给定的样本，模型会生成一个 logits 向量 $\\mathbf{l} = (l_0, l_1, \\dots, l_{K-1})$。这些实值分数通过 softmax 函数转换为关于类别的有效概率分布：\n$$P(Y=k|\\mathbf{l}) = \\frac{\\exp(l_k)}{\\sum_{j=0}^{K-1} \\exp(l_j)}$$\n预测类别 $\\hat{y}$ 是对应于最大概率的类别，即 $\\hat{y} = \\arg\\max_k P(Y=k|\\mathbf{l})$，而此预测的置信度就是这个最大概率，即 $\\text{conf} = \\max_k P(Y=k|\\mathbf{l})$。\n\n有了这些预测，我们就可以评估模型的性能和校准情况。\n1.  **少数类别召回率 (Minority Recall)**：该指标评估模型正确识别代表性不足类别实例的能力。如果一个类别 $k$ 的先验概率 $p_k$ 小于均匀先验，即 $p_k < 1/K$，则该类别被定义为少数类别。特定类别 $k$ 的召回率 $\\text{Recall}_k$ 是类别 $k$ 的实际实例中被正确预测为类别 $k$ 的比例。总的少数类别召回率是所有少数类别 $\\text{Recall}_k$ 的宏平均值。如果少数类别集合为空（即数据集是均衡的），则该指标计算为所有类别的宏平均召回率。\n\n2.  **期望校准误差 (Expected Calibration Error, ECE)**：该指标量化了模型的预测置信度在多大程度上反映了其实际准确率。置信度范围 $[0, 1]$ 被划分为 $B$ 个等宽的区间（这里 $B=10$）。对于每个区间 $m$，我们找出所有预测置信度落入该区间的样本。然后，我们计算该区间内样本的平均准确率 $\\text{acc}(b_m)$ 和平均置信度 $\\text{conf}(b_m)$。ECE 是这两个量之间绝对差的加权平均值：\n    $$ \\text{ECE} = \\sum_{m=1}^{B} \\frac{|b_m|}{N} |\\text{acc}(b_m) - \\text{conf}(b_m)| $$\n    其中 $|b_m|$ 是区间 $m$ 中的样本数，而 $N$ 是样本总数。较低的 ECE 意味着更好的校准。\n\n问题的核心是检验“按熵倒数重加权”的假设。这涉及将每个样本的 logit 向量 $\\mathbf{l}$ 乘以一个标量 $s = 1/H(Y)$。新的 logits 为 $\\mathbf{l}' = s \\cdot \\mathbf{l}$。所有指标——概率、预测、少数类别召回率和 ECE——都使用这些缩放后的 logits 重新计算。此操作类似于将 softmax 函数的温度调整为 $T=H(Y)$。高熵（不确定）的先验会导致高温，从而“软化”概率；而低熵（峰值）的先验会导致低温，从而“锐化”它们。\n\n最后一步是比较缩放前后的性能。如果 $\\text{recall}_{\\text{after}} > \\text{recall}_{\\text{before}}$，则记录为召回率有所改善；如果 $\\text{ECE}_{\\text{after}} < \\text{ECE}_{\\text{before}}$，则记录为校准情况有所改善。将此程序应用于每个测试用例以生成所需的结果。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import softmax\n\ndef calculate_metrics(logits, y_true, K, minority_classes_idx, n_bins=10):\n    \"\"\"\n    Calculates minority recall and ECE for a given set of logits and labels.\n    \"\"\"\n    N = len(y_true)\n    if N == 0:\n        return 0.0, 0.0\n\n    # 1. Get predictions and confidences\n    probs = softmax(logits, axis=1)\n    confidences = np.max(probs, axis=1)\n    y_pred = np.argmax(probs, axis=1)\n    correct = (y_pred == y_true)\n\n    # 2. Calculate Minority Recall\n    recall_set = minority_classes_idx\n    if not recall_set:  # If no minority classes, use all classes\n        recall_set = list(range(K))\n\n    recalls = []\n    for k in recall_set:\n        class_mask = (y_true == k)\n        n_class_samples = np.sum(class_mask)\n        if n_class_samples > 0:\n            tp = np.sum(correct[class_mask])\n            recall_k = tp / n_class_samples\n            recalls.append(recall_k)\n\n    minority_recall = np.mean(recalls) if recalls else 0.0\n\n    # 3. Calculate ECE\n    ece = 0.0\n    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n    \n    for i in range(n_bins):\n        lower, upper = bin_boundaries[i], bin_boundaries[i+1]\n        \n        # The first bin is inclusive of 0.\n        if i == 0:\n             in_bin = (confidences >= lower) & (confidences <= upper)\n        else:\n             in_bin = (confidences > lower) & (confidences <= upper)\n        \n        num_in_bin = np.sum(in_bin)\n\n        if num_in_bin > 0:\n            accuracy_in_bin = np.mean(correct[in_bin])\n            confidence_in_bin = np.mean(confidences[in_bin])\n            ece += (num_in_bin / N) * np.abs(accuracy_in_bin - confidence_in_bin)\n\n    return minority_recall, ece\n\ndef process_case(c, L, y, K, n_bins=10):\n    \"\"\"\n    Processes a single test case according to the problem description.\n    \"\"\"\n    c = np.array(c, dtype=float)\n    L = np.array(L, dtype=float)\n    y = np.array(y, dtype=int)\n    \n    # 1. Compute prior probabilities and Shannon entropy H(Y)\n    N = np.sum(c)\n    p = c / N\n    p_nonzero = p[p > 0]\n    H_Y = -np.sum(p_nonzero * np.log(p_nonzero))\n\n    # 2. Identify minority classes\n    uniform_prob = 1.0 / K\n    minority_classes_idx = [k for k, pk in enumerate(p) if pk < uniform_prob]\n\n    # 3. Compute baseline metrics\n    recall_before, ece_before = calculate_metrics(L, y, K, minority_classes_idx, n_bins)\n    \n    # 4. Perform reweighting and recompute metrics\n    if H_Y == 0:\n        s = 1.0  # Avoid division by zero, effectively no scaling\n    else:\n        s = 1.0 / H_Y\n    \n    L_after = L * s\n    recall_after, ece_after = calculate_metrics(L_after, y, K, minority_classes_idx, n_bins)\n\n    # 5. Determine if metrics improved\n    improved_recall = recall_after > recall_before\n    improved_ece = ece_after < ece_before\n\n    return [H_Y, recall_before, recall_after, ece_before, ece_after, improved_recall, improved_ece]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"c\": [4, 4, 4],\n            \"L\": [\n                [3.0, 1.0, 0.0], [2.5, 2.0, 1.0], [1.0, 1.2, 0.8], [2.2, -0.5, 0.1],\n                [0.5, 2.8, 1.0], [1.0, 2.2, 2.0], [0.2, 1.5, 1.6], [0.1, 2.5, -0.3],\n                [0.0, 0.5, 2.5], [1.0, 0.9, 1.2], [-0.2, 1.3, 1.1], [0.3, -0.4, 2.0]\n            ],\n            \"y\": [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2],\n            \"K\": 3\n        },\n        {\n            \"c\": [8, 3, 1],\n            \"L\": [\n                [3.4, 0.1, -0.2], [2.8, 0.5, 0.0], [3.1, 0.3, 0.2], [2.0, 1.1, 0.9],\n                [1.8, 1.5, 1.2], [2.5, 1.6, 1.6], [1.7, 1.9, 1.6], [2.6, 0.8, 0.7],\n                [1.5, 2.7, 1.0], [2.2, 1.8, 0.9], [2.0, 2.1, 1.9], [2.4, 1.3, 1.6]\n            ],\n            \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2],\n            \"K\": 3\n        },\n        {\n            \"c\": [10, 1, 1],\n            \"L\": [\n                [3.5, 0.2, 0.1], [3.0, 0.5, 0.4], [2.9, 0.1, -0.2], [2.3, 0.7, 0.6],\n                [2.6, 1.0, 0.9], [3.2, 0.8, 0.7], [2.8, 0.4, 0.3], [3.1, 1.2, 1.0],\n                [2.7, 1.4, 1.3], [2.4, 1.6, 1.5], [2.5, 2.2, 2.1], [2.5, 2.1, 2.2]\n            ],\n            \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2],\n            \"K\": 3\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case['c'], case['L'], case['y'], case['K'])\n        results.append(result)\n    \n    # Format the final output string exactly as specified\n    formatted_results = [\n        f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]},{str(res[5]).lower()},{str(res[6]).lower()}]\" \n        for res in results\n    ]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后，我们将熵的应用从分析数据转向分析模型本身。这项高级练习介绍了一种基于神经元激活熵的神经网络剪枝方法。您将实施一个标准，以识别和移除卷积神经网络中信息量较低的“冗余”通道，从而将抽象的熵概念与深度学习模型优化的具体目标联系起来 。",
            "id": "3174074",
            "problem": "考虑一个在加拿大高等研究院 (CIFAR) $10$ 类数据集上训练的卷积神经网络 (CNN)。设通道 $c$ 在一组样本上的激活值为一个实值随机变量 $Z_c$。根据信息论的第一性原理，一个具有概率 $\\{p_i\\}_{i=1}^B$ 的离散随机变量 $Z$ 的香农熵定义为 $$H(Z) = -\\sum_{i=1}^{B} p_i \\log_2 p_i,$$ 单位为比特。在实践中，激活值是连续值的；为了估计熵，可以将 $Z_c$ 离散化为 $B$ 个区间，并将经验区间频率视为概率。剪枝的核心思想是移除激活熵较低的通道，其假设是低熵通道携带的信息量少，因此是冗余的。\n\n您的任务是实现并评估一个基于熵的剪枝标准，这纯粹是一个基于这些定义的计算过程，无需访问任何外部数据。具体来说：\n\n1. 给定单层的一组通道激活值，将每个通道的激活值离散化到 $B$ 个区间中，这些区间覆盖该通道的观测范围，并使用以 2 为底的对数计算估计的香农熵 $H(Z_c)$（单位为比特）。通过在归一化前向每个区间计数添加一个小的常数 $\\varepsilon$ 来使用加性平滑，以避免零概率。\n\n2. 当且仅当 $H(Z_c) < \\tau$ 时，剪枝通道 $c$，其中 $\\tau$ 是用户指定的阈值（单位为比特）。剪枝决策是严格不等式，因此 $H(Z_c) = \\tau$ 的通道不会被剪枝。\n\n3. 将压缩量化为被移除通道的比例，即 $$\\text{compression\\_fraction} = \\frac{\\text{number of pruned channels}}{\\text{total number of channels}}.$$ 该量无单位。\n\n4. 通过从给定的基线准确率 $A_0$（剪枝前）开始，并减去与移除的归一化信息成比例的惩罚，来估计在 CIFAR-$10$ 上的剪枝后准确率（表示为 0 到 1 之间的小数）。对于 $B$ 个区间，每个通道的最大可能熵为 $$H_{\\max} = \\log_2(B).$$ 设 $\\eta$ 为一个比例常数（无单位）。定义剪枝后的预测准确率为 $$A_{\\text{pred}} = \\max\\left(0, \\min\\left(1, A_0 - \\eta \\sum_{c \\in \\mathcal{P}} \\frac{H(Z_c)}{H_{\\max}}\\right)\\right),$$ 其中 $\\mathcal{P}$ 是被剪枝通道的集合。这将预测准确率限制在区间 $[0,1]$ 内。以小数形式报告准确率，不要使用百分号。\n\n5. 为了将剪枝与冗余相关联，还需报告被剪枝通道的平均熵， $$\\overline{H}_{\\text{pruned}} = \\begin{cases}\\frac{1}{|\\mathcal{P}|} \\sum_{c \\in \\mathcal{P}} H(Z_c),  \\text{if } |\\mathcal{P}| > 0,\\\\ 0,  \\text{otherwise.}\\end{cases}$$\n\n实现细节：\n- 每个通道使用 $B$ 个区间，在该通道的激活值范围内进行等宽分箱。\n- 在归一化之前，对所有区间使用加性平滑常数 $\\varepsilon$。\n- 使用以 2 为底的对数计算熵，以比特为单位报告值。\n\n测试套件：\n实现并评估以下四个测试用例。在每个用例中，通道由确定性序列定义，因此您的程序会产生固定的输出。\n\n用例 1（正常路径，中等阈值）：\n- 每个通道的样本数 $N = 100$。\n- 通道（共 $6$ 个）：\n  - 通道 $0$：对于 $n = 1, \\dots, N$，有 $z_n = 0$。\n  - 通道 $1$：周期性小数值 $z_n \\in \\{-0.02, 0, 0.02\\}$，重复至长度为 $N$。\n  - 通道 $2$：均匀序列 $z_n = -1 + \\frac{2(n-1)}{N-1}$，对于 $n = 1, \\dots, N$。\n  - 通道 $3$：由两个簇组成的双峰序列：前 $50$ 个样本线性分布在 $[-0.6,-0.4]$，后 $50$ 个样本线性分布在 $[0.4,0.6]$。\n  - 通道 $4$：对于所有 $n$，为恒定小数值 $z_n = 0.1$。\n  - 通道 $5$：正弦序列 $z_n = 0.5 \\sin\\left(\\frac{8\\pi (n-1)}{N-1}\\right)$，对于 $n = 1, \\dots, N$。\n- 区间数 $B = 8$，阈值 $\\tau = 0.5$ 比特，平滑系数 $\\varepsilon = 10^{-12}$，基线准确率 $A_0 = 0.92$，比例常数 $\\eta = 0.03$。\n\n用例 2（边界阈值，不剪枝）：\n- 与用例 1 相同的通道和 $N$。\n- 区间数 $B = 8$，阈值 $\\tau = 0$，平滑系数 $\\varepsilon = 10^{-12}$，基线准确率 $A_0 = 0.92$，比例常数 $\\eta = 0.03$。\n\n用例 3（激进阈值，大量剪枝）：\n- 与用例 1 相同的通道和 $N$。\n- 区间数 $B = 8$，阈值 $\\tau = 2.5$ 比特，平滑系数 $\\varepsilon = 10^{-12}$，基线准确率 $A_0 = 0.92$，比例常数 $\\eta = 0.03$。\n\n用例 4（不同的分布形状和区间数）：\n- 每个通道的样本数 $N = 120$。\n- 通道（共 $4$ 个）：\n  - 通道 $0$：对于所有 $n$，有 $z_n = 0$。\n  - 通道 $1$：均匀序列 $z_n = -2 + \\frac{4(n-1)}{N-1}$，对于 $n = 1, \\dots, N$。\n  - 通道 $2$：重尾混合：前 $60$ 个样本线性分布在 $[-0.1,0.1]$，后 $30$ 个样本在 $[-3,-2]$，最后 $30$ 个样本在 $[2,3]$。\n  - 通道 $3$：近常数伴有偶尔变化：前 $100$ 个样本 $z_n = 1$，后 $20$ 个样本线性分布在 $[0.9,1.1]$。\n- 区间数 $B = 4$，阈值 $\\tau = 1.0$ 比特，平滑系数 $\\varepsilon = 10^{-12}$，基线准确率 $A_0 = 0.90$，比例常数 $\\eta = 0.025$。\n\n所需输出：\n- 对于每个用例，按顺序生成三个浮点数：compression\\_fraction、predicted\\_accuracy、average\\_entropy\\_pruned。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，对于 4 个用例，每个用例有 3 个浮点数，按用例 1、用例 2、用例 3、用例 4 的顺序，结果为 $[r_1,r_2,\\dots,r_{12}]$）。\n\n不允许使用外部文件、用户输入或网络访问。仅使用指定的运行时环境。",
            "solution": "该问题要求基于香农熵原理，实现一个用于神经网络通道剪枝的确定性计算过程。该过程涉及计算通道激活值的熵，应用剪枝规则，并报告几个派生指标：压缩率、预测的剪枝后准确率以及被移除通道的平均熵。\n\n对问题陈述的验证证实了其科学性、适定性和客观性。它基于信息论中香农熵的标准定义。用于估计连续变量熵的方法——通过分箱进行离散化——是一种常用且有效（尽管是近似的）技术。派生指标的公式在数学上是合理的且无歧义。测试用例使用确定性序列构建，确保问题有唯一、可验证的解。因此，该问题是有效的，我们可以着手解决。\n\n通过对每个提供的测试用例遵循一系列步骤来实现解决方案。\n\n首先，对于每个通道，我们必须按规定生成激活值。问题将这些激活值定义为确定性序列，而非随机样本。对于给定的通道 $c$，我们有一组 $N$ 个激活值 $\\{z_n\\}_{n=1}^N$。\n\n其次，我们估计每个通道激活变量 $Z_c$ 的香农熵 $H(Z_c)$。由于激活值是连续的，我们必须首先将其离散化。该过程指定在特定通道的观测激活值范围内使用 $B$ 个等宽区间。设通道 $c$ 的最小和最大激活值分别为 $z_{\\min, c} = \\min_n(z_n)$ 和 $z_{\\max, c} = \\max_n(z_n)$。范围 $[z_{\\min, c}, z_{\\max, c}]$ 被划分为 $B$ 个等宽区间，宽度为 $w = (z_{\\max, c} - z_{\\min, c}) / B$。然后我们统计落入每个区间 $i$ 的激活值数量 $k_i$（其中 $i = 1, \\dots, B$）。如果一个通道中的所有激活值都是恒定的，即 $z_{\\min, c} = z_{\\max, c}$，则会出现特殊情况。在这种情况下，所有 $N$ 个激活值都落入单个区间，因此其计数为 $N$，而其他所有 $B-1$ 个区间的计数为 $0$。\n\n第三，为了计算熵公式所需的概率，我们必须处理零计数的可能性，这会使对数无定义。问题要求使用加性平滑（也称为拉普拉斯平滑）。将一个小的常数 $\\varepsilon$ 加到每个区间的计数上。区间 $i$ 的平滑后计数为 $k'_i = k_i + \\varepsilon$。平滑后的总计数为 $N' = \\sum_{i=1}^{B} k'_i = (\\sum k_i) + B\\varepsilon = N + B\\varepsilon$。然后每个区间的概率估计为 $p_i = k'_i / N'$。\n\n利用这些概率，使用以 2 为底的对数计算通道 $c$ 的香农熵（单位为比特）：\n$$\nH(Z_c) = -\\sum_{i=1}^{B} p_i \\log_2 p_i\n$$\n对层中的每个通道都执行此计算。\n\n第四，我们应用剪枝标准。当且仅当一个通道 $c$ 计算出的熵 $H(Z_c)$ 严格小于给定的阈值 $\\tau$ 时，该通道被标记为待剪枝。我们确定被剪枝通道的集合，记为 $\\mathcal{P}$。\n$$\n\\mathcal{P} = \\{c \\mid H(Z_c) < \\tau\\}\n$$\n\n第五，我们基于被剪枝通道的集合 $\\mathcal{P}$ 计算三个所需的输出指标。\n\n1.  **压缩率**：这是被剪枝通道数与总通道数 $C$ 的比率。\n    $$\n    \\text{compression\\_fraction} = \\frac{|\\mathcal{P}|}{C}\n    $$\n\n2.  **被剪枝通道的平均熵**：该指标量化了被视为冗余的通道的平均信息含量。其定义为：\n    $$\n    \\overline{H}_{\\text{pruned}} = \\begin{cases}\\frac{1}{|\\mathcal{P}|} \\sum_{c \\in \\mathcal{P}} H(Z_c),  \\text{if } |\\mathcal{P}| > 0,\\\\ 0,  \\text{otherwise.}\\end{cases}\n    $$\n    当 $|\\mathcal{P}|=0$ 时的情况确保了在没有通道被剪枝时有明确定义的输出。\n\n3.  **预测的剪枝后准确率**：这是一个基于惩罚模型的估计。该模型从基线准确率 $A_0$ 开始，减去一个与剪枝移除的总归一化信息成比例的惩罚。对于分布在 $B$ 个区间的分布，最大可能熵为 $H_{\\max} = \\log_2(B)$，这在分布均匀时发生。预测准确率 $A_{\\text{pred}}$ 由以下公式给出：\n    $$\n    A_{\\text{pred}} = A_0 - \\eta \\sum_{c \\in \\mathcal{P}} \\frac{H(Z_c)}{H_{\\max}}\n    $$\n    其中 $\\eta$ 是一个无单位的比例常数。最终值被限制在区间 $[0, 1]$ 内，以确保它是一个有效的准确率值：\n    $$\n    A_{\\text{pred}} = \\max\\left(0, \\min\\left(1, A_{\\text{pred}}\\right)\\right)\n    $$\n\n通过系统地将这些步骤应用于问题陈述中定义的四个测试用例，我们可以生成所需的数值结果。每个步骤都是对所提供公式和规则的直接实现。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the channel pruning problem for all test cases.\n    \"\"\"\n\n    def compute_metrics(activations_list, B, tau, epsilon, A0, eta):\n        \"\"\"\n        Computes entropy for each channel, prunes, and calculates metrics.\n        \"\"\"\n        num_channels = len(activations_list)\n        entropies = []\n        N = len(activations_list[0]) if num_channels > 0 else 0\n\n        for act in activations_list:\n            min_val, max_val = np.min(act), np.max(act)\n            \n            if min_val == max_val:\n                # All values are constant, they fall into a single bin.\n                counts = np.zeros(B)\n                counts[0] = N\n            else:\n                # Use numpy's histogram function for equal-width binning.\n                # The range is inclusive of min_val but exclusive of max_val,\n                # except for the last bin which is inclusive of max_val.\n                counts, _ = np.histogram(act, bins=B, range=(min_val, max_val))\n            \n            # Additive smoothing\n            smoothed_counts = counts.astype(np.float64) + epsilon\n            \n            # Normalize to get probabilities\n            total_smoothed_count = np.sum(smoothed_counts)\n            probs = smoothed_counts / total_smoothed_count\n            \n            # Calculate Shannon entropy in bits (log base 2)\n            # We filter out zero probabilities, though smoothing prevents this.\n            # This is just for robustness.\n            non_zero_probs = probs[probs > 0]\n            entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs))\n            entropies.append(entropy)\n\n        # Identify pruned channels\n        pruned_indices = [i for i, h in enumerate(entropies) if h < tau]\n        pruned_entropies = [entropies[i] for i in pruned_indices]\n        \n        num_pruned = len(pruned_indices)\n        \n        # 1. Compression Fraction\n        compression_fraction = num_pruned / num_channels if num_channels > 0 else 0.0\n\n        # 2. Average Entropy of Pruned Channels\n        avg_entropy_pruned = np.mean(pruned_entropies) if num_pruned > 0 else 0.0\n\n        # 3. Predicted Post-Pruning Accuracy\n        H_max = np.log2(B)\n        # Avoid division by zero if B = 1, though problem B >= 4\n        if H_max == 0:\n            penalty = 0\n        else:\n            total_info_removed = np.sum(pruned_entropies)\n            penalty = eta * total_info_removed / H_max\n        \n        predicted_accuracy = A0 - penalty\n        predicted_accuracy = np.clip(predicted_accuracy, 0.0, 1.0)\n        \n        return compression_fraction, predicted_accuracy, avg_entropy_pruned\n\n    # --- Test Cases Definition ---\n    test_cases = [\n        {\n            \"id\": 1,\n            \"N\": 100, \"B\": 8, \"tau\": 0.5, \"epsilon\": 1e-12, \"A0\": 0.92, \"eta\": 0.03,\n            \"channels_def\": [\n                lambda n: np.zeros(n),\n                lambda n: np.tile(np.array([-0.02, 0, 0.02]), n // 3 + 1)[:n],\n                lambda n: np.linspace(-1.0, 1.0, n),\n                lambda n: np.concatenate([np.linspace(-0.6, -0.4, n // 2), np.linspace(0.4, 0.6, n // 2)]),\n                lambda n: np.full(n, 0.1),\n                lambda n: 0.5 * np.sin(8 * np.pi * np.arange(n) / (n - 1)),\n            ]\n        },\n        {\n            \"id\": 2,\n            \"N\": 100, \"B\": 8, \"tau\": 0.0, \"epsilon\": 1e-12, \"A0\": 0.92, \"eta\": 0.03,\n            \"channels_def\": [\n                lambda n: np.zeros(n),\n                lambda n: np.tile(np.array([-0.02, 0, 0.02]), n // 3 + 1)[:n],\n                lambda n: np.linspace(-1.0, 1.0, n),\n                lambda n: np.concatenate([np.linspace(-0.6, -0.4, n // 2), np.linspace(0.4, 0.6, n // 2)]),\n                lambda n: np.full(n, 0.1),\n                lambda n: 0.5 * np.sin(8 * np.pi * np.arange(n) / (n - 1)),\n            ]\n        },\n        {\n            \"id\": 3,\n            \"N\": 100, \"B\": 8, \"tau\": 2.5, \"epsilon\": 1e-12, \"A0\": 0.92, \"eta\": 0.03,\n            \"channels_def\": [\n                lambda n: np.zeros(n),\n                lambda n: np.tile(np.array([-0.02, 0, 0.02]), n // 3 + 1)[:n],\n                lambda n: np.linspace(-1.0, 1.0, n),\n                lambda n: np.concatenate([np.linspace(-0.6, -0.4, n // 2), np.linspace(0.4, 0.6, n // 2)]),\n                lambda n: np.full(n, 0.1),\n                lambda n: 0.5 * np.sin(8 * np.pi * np.arange(n) / (n - 1)),\n            ]\n        },\n        {\n            \"id\": 4,\n            \"N\": 120, \"B\": 4, \"tau\": 1.0, \"epsilon\": 1e-12, \"A0\": 0.90, \"eta\": 0.025,\n            \"channels_def\": [\n                lambda n: np.zeros(n),\n                lambda n: np.linspace(-2.0, 2.0, n),\n                lambda n: np.concatenate([\n                    np.linspace(-0.1, 0.1, n // 2), \n                    np.linspace(-3.0, -2.0, n // 4), \n                    np.linspace(2.0, 3.0, n // 4)\n                ]),\n                lambda n: np.concatenate([\n                    np.full(100, 1.0),\n                    np.linspace(0.9, 1.1, 20)\n                ]),\n            ]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        activations_list = [gen(N) for gen in case[\"channels_def\"]]\n        \n        results = compute_metrics(\n            activations_list,\n            case[\"B\"],\n            case[\"tau\"],\n            case[\"epsilon\"],\n            case[\"A0\"],\n            case[\"eta\"]\n        )\n        all_results.extend(results)\n\n    print(f\"[{','.join(f'{r:.7f}' for r in all_results)}]\")\n\nsolve()\n```"
        }
    ]
}