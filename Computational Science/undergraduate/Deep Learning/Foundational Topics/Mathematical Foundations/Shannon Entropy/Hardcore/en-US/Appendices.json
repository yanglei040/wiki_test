{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the implications of Shannon entropy, we begin by applying it to the very foundation of any machine learning model: its input data. This first exercise explores how entropy quantifies the uncertainty inherent in different types of image data—specifically, grayscale versus color pixels. By calculating the entropy and a proxy for the sample complexity required to learn the data's distribution, you will gain a concrete, quantitative understanding of how higher-dimensional data like color images increases both information content and the challenge of statistical estimation, a concept often referred to as the 'curse of dimensionality' .",
            "id": "3174049",
            "problem": "You are given two classes of datasets for image classification in deep learning: grayscale images with a single channel and color images with three channels (red, green, blue). Each pixel is modeled as a discrete random variable taking values in a finite alphabet. In the grayscale case, a pixel is modeled by a single discrete random variable $X$ with a probability mass function over $m$ intensity bins. In the color case, a pixel is modeled by a triple of discrete random variables $(X_{R}, X_{G}, X_{B})$, one for each channel. Assume throughout that the channels are statistically independent at the pixel level.\n\nStarting from fundamental definitions, reason about and implement how the uncertainty in pixels differs between grayscale and color images, and how this difference influences the number of samples needed to reliably estimate the pixel distribution in a way useful for training deep classifiers. Specifically:\n\n- Use the standard definition of the Shannon entropy of a discrete random variable (expressed in bits, i.e., using logarithm base $2$) as the measure of uncertainty per pixel. For the color dataset, use only the independence property of entropy to obtain the per-pixel joint entropy from the channel entropies.\n- For sample complexity, use a well-tested fact from empirical distribution estimation: to ensure that the expected $\\ell_{1}$ error of the empirical distribution of a multinomial random variable is at most a tolerance $\\varepsilon$, the number of independent samples $n$ must satisfy an inequality of the form $n \\geq c \\cdot \\frac{|\\mathcal{A}| - 1}{\\varepsilon^{2}}$, where $|\\mathcal{A}|$ is the alphabet size and $c$ is a constant that does not depend on $|\\mathcal{A}|$ or $\\varepsilon$. For the purpose of this problem, take $c = 1$ to obtain a conservative scaling law, and treat this as a lower bound proxy for the number of labeled examples a deep classifier would need to estimate pixel-level distributions to the desired tolerance. Your program should compute $n$ by rounding up to the nearest integer.\n\nYour task is to write a complete program that:\n1. Computes the per-pixel entropy in bits for a grayscale dataset and for a color dataset (assuming independence across channels).\n2. Computes the proxy sample complexity $n$ for the grayscale dataset and for the color dataset using the bound $n = \\left\\lceil \\frac{|\\mathcal{A}| - 1}{\\varepsilon^{2}} \\right\\rceil$.\n\nAll mathematical quantities must be handled precisely, with the following conventions:\n- For entropy, use base-$2$ logarithms so the unit is bits.\n- Use the convention $0 \\log 0 = 0$.\n- For the grayscale alphabet size, use $m = | \\text{bins for } X |$.\n- For the color alphabet size, use $m_{\\text{color}} = | \\text{bins for } X_{R} | \\cdot | \\text{bins for } X_{G} | \\cdot | \\text{bins for } X_{B} |$.\n\nImplement the following test suite of parameter values. Each test case specifies probability mass functions for grayscale and for each color channel, and a tolerance $\\varepsilon$:\n- Test case $1$ (happy path, uniform): Grayscale $p = [0.25, 0.25, 0.25, 0.25]$, Color channels identical to grayscale, $\\varepsilon = 0.05$.\n- Test case $2$ (boundary, degenerate grayscale): Grayscale $p = [1.0, 0.0, 0.0, 0.0]$, Color $p_{R} = p_{G} = p_{B} = [0.97, 0.01, 0.01, 0.01]$, $\\varepsilon = 0.05$.\n- Test case $3$ (heterogeneous channels): Grayscale $p = [0.10, 0.20, 0.30, 0.40]$, Color $p_{R} = [0.70, 0.10, 0.10, 0.10]$, $p_{G} = [0.50, 0.20, 0.20, 0.10]$, $p_{B} = [0.25, 0.25, 0.25, 0.25]$, $\\varepsilon = 0.02$.\n- Test case $4$ (small tolerance, identical distributions): Grayscale $p = [0.40, 0.40, 0.10, 0.10]$, Color channels identical to grayscale, $\\varepsilon = 0.01$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list $[H_{\\text{gray}}, H_{\\text{color}}, n_{\\text{gray}}, n_{\\text{color}}]$, where $H_{\\text{gray}}$ and $H_{\\text{color}}$ are floats (in bits), and $n_{\\text{gray}}$ and $n_{\\text{color}}$ are integers. For example: $[[h_{1}, h_{1}^{c}, n_{1}, n_{1}^{c}], [h_{2}, h_{2}^{c}, n_{2}, n_{2}^{c}], \\dots]$. No units should be printed, but entropies must be computed in bits and $n$ is a dimensionless count of samples.",
            "solution": "The problem is valid as it is scientifically grounded in information theory and statistics, well-posed with all necessary information provided, and objective in its formulation. We will proceed with a step-by-step solution.\n\nThe problem asks us to compare the complexity of grayscale and color image data from an information-theoretic and statistical estimation perspective. We model a pixel's value as a discrete random variable and use Shannon entropy as the measure of uncertainty and a derived formula for sample complexity to estimate the underlying probability distribution.\n\n### 1. Per-Pixel Uncertainty: Shannon Entropy\n\nThe fundamental measure of uncertainty or \"information content\" of a discrete random variable $X$ with alphabet $\\mathcal{X}$ and probability mass function (PMF) $p(x) = P(X=x)$ is the Shannon entropy, $H(X)$. When using the logarithm to the base $2$, the entropy is measured in units of bits. The definition is:\n$$ H(X) = - \\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x) $$\nBy convention, we define $0 \\log_2 0 = 0$ to handle outcomes with zero probability.\n\n#### Grayscale Image Entropy ($H_{\\text{gray}}$)\nA grayscale pixel is modeled by a single discrete random variable $X$ over an alphabet of $m$ intensity bins. Given a PMF $p_{\\text{gray}} = [p_1, p_2, \\ldots, p_m]$, the per-pixel entropy is calculated directly from the definition:\n$$ H_{\\text{gray}} = H(X) = - \\sum_{i=1}^{m} p_i \\log_2 p_i $$\n\n#### Color Image Entropy ($H_{\\text{color}}$)\nA color pixel is modeled as a vector of three random variables, $(X_R, X_G, X_B)$, one for each color channel. The problem states a critical assumption: the channels are statistically independent. For independent random variables, the joint entropy is simply the sum of their individual entropies:\n$$ H(X_R, X_G, X_B) = H(X_R) + H(X_G) + H(X_B) $$\nTherefore, the per-pixel entropy for a color image, $H_{\\text{color}}$, is given by the sum of the entropies of each channel's PMF ($p_R$, $p_G$, $p_B$):\n$$ H_{\\text{color}} = \\left( - \\sum_{i} p_{R,i} \\log_2 p_{R,i} \\right) + \\left( - \\sum_{j} p_{G,j} \\log_2 p_{G,j} \\right) + \\left( - \\sum_{k} p_{B,k} \\log_2 p_{B,k} \\right) $$\n\n### 2. Sample Complexity for Distribution Estimation\n\nThe problem provides a proxy for the number of samples $n$ required to reliably estimate the PMF of a multinomial random variable. This quantity, termed sample complexity, is given by the formula:\n$$ n = \\left\\lceil \\frac{|\\mathcal{A}| - 1}{\\varepsilon^{2}} \\right\\rceil $$\nwhere $|\\mathcal{A}|$ is the size of the alphabet (the number of possible outcomes) for the random variable, $\\varepsilon$ is the desired tolerance for the expected $\\ell_{1}$ error, and $\\lceil \\cdot \\rceil$ is the ceiling function, which rounds up to the nearest integer.\n\nThis formula reveals that the number of samples required grows linearly with the size of the alphabet, $|\\mathcal{A}|$.\n\n#### Grayscale Sample Complexity ($n_{\\text{gray}}$)\nFor a grayscale pixel, the random variable is $X$ and its alphabet size, $|\\mathcal{A}_{\\text{gray}}|$, is the number of intensity bins, $m$.\n$$ n_{\\text{gray}} = \\left\\lceil \\frac{m - 1}{\\varepsilon^{2}} \\right\\rceil $$\n\n#### Color Sample Complexity ($n_{\\text{color}}$)\nFor a color pixel, the random variable is the vector $(X_R, X_G, X_B)$. We are estimating the joint distribution of this vector. The alphabet of the joint distribution consists of all possible triples of channel values. If the channels have $m_R$, $m_G$, and $m_B$ bins respectively, the size of the joint alphabet is the product of these individual sizes:\n$$ |\\mathcal{A}_{\\text{color}}| = m_R \\cdot m_G \\cdot m_B $$\nThe sample complexity is then:\n$$ n_{\\text{color}} = \\left\\lceil \\frac{(m_R \\cdot m_G \\cdot m_B) - 1}{\\varepsilon^{2}} \\right\\rceil $$\nThis multiplicative growth in the alphabet size for higher-dimensional data is a manifestation of the \"curse of dimensionality.\" It demonstrates that estimating a joint distribution for color pixels requires a substantially larger number of samples than for a single grayscale channel, assuming the same tolerance $\\varepsilon$. The entropy, which is additive for independent variables, grows much more slowly than the sample complexity, which depends on the multiplicatively growing alphabet size. This distinction is a central point of the analysis.\n\n### Summary of Computations for Each Test Case\n1.  **Calculate $H_{\\text{gray}}$**: Apply the entropy formula to the grayscale PMF.\n2.  **Calculate $H_{\\text{color}}$**: Calculate the entropy for each color channel PMF and sum the results.\n3.  **Calculate $n_{\\text{gray}}$**: Determine the alphabet size $m$ from the length of the grayscale PMF and apply the sample complexity formula.\n4.  **Calculate $n_{\\text{color}}$**: Determine the alphabet sizes $m_R, m_G, m_B$ from the lengths of the channel PMFs, calculate the joint alphabet size $m_R \\cdot m_G \\cdot m_B$, and apply the sample complexity formula.\n\nThese steps will be systematically implemented for each test case provided.",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes per-pixel entropy and sample complexity for grayscale and color image models\n    across a suite of test cases.\n    \"\"\"\n\n    # Test cases: (p_gray, p_red, p_green, p_blue, epsilon)\n    # The structure allows for cases where channel PMFs are distinct or identical to grayscale.\n    p1 = [0.25, 0.25, 0.25, 0.25]\n    p2_gray = [1.0, 0.0, 0.0, 0.0]\n    p2_color_channel = [0.97, 0.01, 0.01, 0.01]\n    p3_gray = [0.10, 0.20, 0.30, 0.40]\n    p3_r = [0.70, 0.10, 0.10, 0.10]\n    p3_g = [0.50, 0.20, 0.20, 0.10]\n    p3_b = [0.25, 0.25, 0.25, 0.25]\n    p4 = [0.40, 0.40, 0.10, 0.10]\n    \n    test_cases = [\n        (p1, p1, p1, p1, 0.05),\n        (p2_gray, p2_color_channel, p2_color_channel, p2_color_channel, 0.05),\n        (p3_gray, p3_r, p3_g, p3_b, 0.02),\n        (p4, p4, p4, p4, 0.01),\n    ]\n\n    results = []\n\n    def calculate_entropy(pmf: np.ndarray) -> float:\n        \"\"\"\n        Calculates the Shannon entropy for a given probability mass function.\n        Handles the 0*log(0) = 0 case.\n        \"\"\"\n        # Filter out zero probabilities to avoid log(0)\n        pmf_nz = pmf[pmf > 0]\n        if pmf_nz.size == 0:\n            return 0.0\n        return -np.sum(pmf_nz * np.log2(pmf_nz))\n\n    def calculate_sample_complexity(alphabet_size: int, epsilon: float) -> int:\n        \"\"\"\n        Calculates the sample complexity n based on the given formula.\n        \"\"\"\n        if alphabet_size <= 1:\n            return 0\n        n = math.ceil((alphabet_size - 1) / (epsilon**2))\n        return int(n)\n\n    for case in test_cases:\n        p_gray_list, p_r_list, p_g_list, p_b_list, epsilon = case\n        \n        p_gray = np.array(p_gray_list, dtype=np.float64)\n        p_r = np.array(p_r_list, dtype=np.float64)\n        p_g = np.array(p_g_list, dtype=np.float64)\n        p_b = np.array(p_b_list, dtype=np.float64)\n\n        # 1. Grayscale Calculations\n        h_gray = calculate_entropy(p_gray)\n        m_gray = len(p_gray)\n        n_gray = calculate_sample_complexity(m_gray, epsilon)\n\n        # 2. Color Calculations\n        h_r = calculate_entropy(p_r)\n        h_g = calculate_entropy(p_g)\n        h_b = calculate_entropy(p_b)\n        h_color = h_r + h_g + h_b\n\n        m_r, m_g, m_b = len(p_r), len(p_g), len(p_b)\n        m_color = m_r * m_g * m_b\n        n_color = calculate_sample_complexity(m_color, epsilon)\n        \n        results.append([h_gray, h_color, n_gray, n_color])\n\n    # Format the final output string exactly as required, handling list-of-lists.\n    # The standard str() representation of a list includes spaces, which is fine here.\n    # The example f-string correctly joins string representations of the inner lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Having explored entropy at the input level, we now turn our attention to the model's output. A well-calibrated model should not only be accurate but also express an appropriate level of confidence in its predictions, a quality directly related to the entropy of its output distribution. This practice requires you to analytically derive the relationship between temperature scaling—a standard technique for calibrating classifiers—and predictive entropy, revealing precisely how temperature can be used to 'soften' or 'sharpen' a model's confidence without altering its final decision .",
            "id": "3174127",
            "problem": "Consider a $K$-class classifier in a deep learning model with parameters $\\theta$. For a fixed input $x$, the model outputs a logit vector $z_{\\theta}(x) \\in \\mathbb{R}^{K}$, with components $z_{\\theta,k}(x)$ for $k \\in \\{1,2,\\dots,K\\}$. Temperature scaling with temperature $T>0$ is applied to the logits to define the predictive distribution\n$$\np_{\\theta}^{T}(y=k \\mid x) \\;=\\; \\frac{\\exp\\!\\big(z_{\\theta,k}(x)/T\\big)}{\\sum_{j=1}^{K} \\exp\\!\\big(z_{\\theta,j}(x)/T\\big)} \\quad \\text{for } k=1,\\dots,K.\n$$\nLet the Shannon entropy (in natural units, i.e., nats) of this distribution be\n$$\nH\\!\\big(p_{\\theta}^{T}(\\cdot \\mid x)\\big) \\;=\\; -\\sum_{k=1}^{K} p_{\\theta}^{T}(y=k \\mid x)\\,\\ln\\!\\big(p_{\\theta}^{T}(y=k \\mid x)\\big).\n$$\n\nStarting only from the definitions above and standard properties of the exponential function and the natural logarithm, address the following:\n\n(i) Derive conditions on the temperature $T$ and the logits $z_{\\theta}(x)$ under which temperature scaling preserves the ranking of class probabilities. In particular, determine when $\\arg\\max_{k} p_{\\theta}^{T}(y=k \\mid x)$ coincides with $\\arg\\max_{k} z_{\\theta,k}(x)$, and when the full ranking of $\\{p_{\\theta}^{T}(y=k \\mid x)\\}_{k=1}^{K}$ matches that of $\\{z_{\\theta,k}(x)\\}_{k=1}^{K}$.\n\n(ii) Using the definitions above and fundamental properties of sums and derivatives, show that $H\\!\\big(p_{\\theta}^{T}(\\cdot \\mid x)\\big)$ is differentiable with respect to $T$ for $T>0$, and derive a closed-form analytic expression for $\\frac{d}{dT} H\\!\\big(p_{\\theta}^{T}(\\cdot \\mid x)\\big)$ in terms of $T$, $\\{z_{\\theta,k}(x)\\}_{k=1}^{K}$, and $\\{p_{\\theta}^{T}(y=k \\mid x)\\}_{k=1}^{K}$.\n\n(iii) Based on your expression, state the precise condition on the logits under which increasing $T$ strictly increases the entropy and thus reduces the confidence concentration (while preserving the ranking). Explain how this relates to calibration in classification without invoking any external metrics.\n\nYour final boxed answer should be the closed-form analytic expression for $\\frac{d}{dT} H\\!\\big(p_{\\theta}^{T}(\\cdot \\mid x)\\big)$ obtained in part (ii). No numerical rounding is required, and you should express the result using only the quantities defined above.",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- A $K$-class classifier with parameters $\\theta$.\n- Input: $x$.\n- Logit vector: $z_{\\theta}(x) \\in \\mathbb{R}^{K}$, with components $z_{\\theta,k}(x)$ for $k \\in \\{1, 2, \\dots, K\\}$.\n- Temperature: $T > 0$.\n- Predictive distribution (softmax with temperature):\n$$\np_{\\theta}^{T}(y=k \\mid x) \\;=\\; \\frac{\\exp(z_{\\theta,k}(x)/T)}{\\sum_{j=1}^{K} \\exp(z_{\\theta,j}(x)/T)}.\n$$\n- Shannon entropy (in nats):\n$$\nH(p_{\\theta}^{T}(\\cdot \\mid x)) \\;=\\; -\\sum_{k=1}^{K} p_{\\theta}^{T}(y=k \\mid x)\\,\\ln(p_{\\theta}^{T}(y=k \\mid x)).\n$$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the required criteria.\n1.  **Scientifically Grounded**: The problem is based on standard, correct definitions from information theory and machine learning. Temperature scaling is a widely used technique for calibrating neural network classifiers. The definitions for the temperature-scaled softmax function and Shannon entropy are accurate. The problem is firmly rooted in established mathematical and computational principles.\n2.  **Well-Posed**: The problem is clearly stated with all necessary definitions and constraints ($T>0$) provided. It is divided into three logical parts that lead to a unique, derivable result.\n3.  **Objective**: The problem is formulated using precise, formal mathematical language. There are no subjective or ambiguous terms.\nThe problem exhibits no flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity. It is a formal, verifiable mathematical problem relevant to its stated field.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution is warranted.\n\nWe will address the three parts of the problem sequentially. For notational convenience within the derivation, we will temporarily denote $p_k \\equiv p_{\\theta}^{T}(y=k \\mid x)$ and $z_k \\equiv z_{\\theta,k}(x)$.\n\n**(i) Conditions for Preserving Probability Ranking**\n\nWe want to find the conditions under which the ranking of probabilities $\\{p_k\\}_{k=1}^K$ is the same as the ranking of logits $\\{z_k\\}_{k=1}^K$.\nLet us compare the probabilities for two distinct classes, $k_1$ and $k_2$.\n$$\np_{k_1} > p_{k_2} \\iff \\frac{\\exp(z_{k_1}/T)}{\\sum_{j=1}^{K} \\exp(z_j/T)} > \\frac{\\exp(z_{k_2}/T)}{\\sum_{j=1}^{K} \\exp(z_j/T)}\n$$\nThe denominator, $\\sum_{j=1}^{K} \\exp(z_j/T)$, is a positive sum and is common to both sides of the inequality. Thus, it can be cancelled without changing the direction of the inequality.\n$$\np_{k_1} > p_{k_2} \\iff \\exp(z_{k_1}/T) > \\exp(z_{k_2}/T)\n$$\nThe exponential function, $f(u) = \\exp(u)$, is strictly monotonically increasing for all real $u$. This means that for any $u_1, u_2 \\in \\mathbb{R}$, $u_1 > u_2 \\iff \\exp(u_1) > \\exp(u_2)$. Applying this property, we get:\n$$\n\\exp(z_{k_1}/T) > \\exp(z_{k_2}/T) \\iff \\frac{z_{k_1}}{T} > \\frac{z_{k_2}}{T}\n$$\nThe problem states that the temperature $T$ is strictly positive, i.e., $T > 0$. We can therefore multiply both sides of the inequality by $T$ without altering its direction.\n$$\n\\frac{z_{k_1}}{T} > \\frac{z_{k_2}}{T} \\iff z_{k_1} > z_{k_2}\n$$\nCombining these steps, we have shown that for any two classes $k_1$ and $k_2$, $p_{k_1} > p_{k_2} \\iff z_{k_1} > z_{k_2}$, under the condition $T > 0$. Since this holds for any pair of classes, the entire ranking of the probabilities $\\{p_k\\}_{k=1}^K$ is identical to the ranking of the logits $\\{z_k\\}_{k=1}^K$. Consequently, the class with the maximum probability is the same as the class with the maximum logit: $\\arg\\max_{k} p_k = \\arg\\max_{k} z_k$.\nThe sole condition required is $T > 0$, which is already given as a premise of the problem.\n\n**(ii) Differentiability and Derivative of Entropy**\n\nFirst, we establish differentiability. The probability $p_k$ is defined as a ratio of functions of $T$. For $T > 0$, the arguments $z_k/T$ are well-defined. The exponential function $\\exp(u)$ is infinitely differentiable for all real $u$. The denominator $\\sum_{j=1}^{K} \\exp(z_j/T)$ is a sum of positive terms, so it is strictly positive and differentiable. Thus, $p_k(T)$ is a composition and ratio of differentiable functions, making it differentiable for $T>0$. For any finite logits, $p_k(T) \\in (0,1)$, so $\\ln(p_k(T))$ is also well-defined and differentiable. The entropy $H$ is a finite sum of products of these differentiable functions, $p_k(T) \\ln(p_k(T))$, and is therefore differentiable with respect to $T$ for $T>0$.\n\nNow, we derive the expression for $\\frac{d}{dT} H(p_{\\theta}^{T}(\\cdot \\mid x))$. Let $H = -\\sum_{k=1}^{K} p_k \\ln p_k$.\nUsing the product and chain rules for differentiation with respect to $T$:\n$$\n\\frac{dH}{dT} = -\\sum_{k=1}^{K} \\frac{d}{dT} (p_k \\ln p_k) = -\\sum_{k=1}^{K} \\left( \\frac{dp_k}{dT} \\ln p_k + p_k \\frac{1}{p_k} \\frac{dp_k}{dT} \\right) = -\\sum_{k=1}^{K} \\left( \\frac{dp_k}{dT} (\\ln p_k + 1) \\right)\n$$\nSince $\\sum_{k=1}^{K} p_k = 1$, its derivative with respect to any variable is zero: $\\frac{d}{dT} \\sum_{k=1}^{K} p_k = \\sum_{k=1}^{K} \\frac{dp_k}{dT} = 0$.\nThe sum can be split:\n$$\n\\frac{dH}{dT} = -\\sum_{k=1}^{K} \\frac{dp_k}{dT} \\ln p_k - \\sum_{k=1}^{K} \\frac{dp_k}{dT} = -\\sum_{k=1}^{K} \\frac{dp_k}{dT} \\ln p_k - 0 = -\\sum_{k=1}^{K} \\frac{dp_k}{dT} \\ln p_k\n$$\nNext, we find the derivative of $p_k$ with respect to $T$. Let $N_k = \\exp(z_k/T)$ and $S = \\sum_{j=1}^{K} N_j$. Then $p_k = N_k/S$.\nThe derivative of $N_k$ is $\\frac{dN_k}{dT} = \\exp(z_k/T) \\cdot \\frac{d}{dT}(z_k/T) = \\exp(z_k/T) \\cdot (-z_k/T^2) = -N_k \\frac{z_k}{T^2}$.\nThe derivative of the sum $S$ is $\\frac{dS}{dT} = \\sum_{j=1}^{K} \\frac{dN_j}{dT} = -\\frac{1}{T^2} \\sum_{j=1}^{K} z_j N_j$.\nUsing the quotient rule for $\\frac{dp_k}{dT} = \\frac{d}{dT}(\\frac{N_k}{S})$:\n$$\n\\frac{dp_k}{dT} = \\frac{S \\frac{dN_k}{dT} - N_k \\frac{dS}{dT}}{S^2} = \\frac{S(-N_k z_k/T^2) - N_k(-\\frac{1}{T^2} \\sum_{j=1}^{K} z_j N_j)}{S^2}\n$$\n$$\n= \\frac{N_k}{S \\cdot T^2} \\left( -z_k + \\frac{1}{S} \\sum_{j=1}^{K} z_j N_j \\right) = \\frac{p_k}{T^2} \\left( \\sum_{j=1}^{K} z_j \\frac{N_j}{S} - z_k \\right) = \\frac{p_k}{T^2} \\left( \\sum_{j=1}^{K} z_j p_j - z_k \\right)\n$$\nLet $\\mathbb{E}_{p}[z] = \\sum_{j=1}^{K} z_j p_j$ be the expectation of the logits under the distribution $p$. Then $\\frac{dp_k}{dT} = \\frac{p_k}{T^2} (\\mathbb{E}_{p}[z] - z_k)$.\n\nNow, substitute this back into the expression for $\\frac{dH}{dT}$:\n$$\n\\frac{dH}{dT} = -\\sum_{k=1}^{K} \\left[ \\frac{p_k}{T^2} (\\mathbb{E}_{p}[z] - z_k) \\right] \\ln p_k\n$$\nFrom the definition of $p_k$, we can write $\\ln p_k = \\ln(\\exp(z_k/T)) - \\ln(\\sum_j \\exp(z_j/T)) = z_k/T - \\ln S$.\n$$\n\\frac{dH}{dT} = -\\frac{1}{T^2} \\sum_{k=1}^{K} p_k (\\mathbb{E}_{p}[z] - z_k) \\left(\\frac{z_k}{T} - \\ln S\\right)\n$$\nDistributing the terms in the sum:\n$$\n\\frac{dH}{dT} = -\\frac{1}{T^3} \\sum_{k=1}^{K} p_k (\\mathbb{E}_{p}[z] - z_k) z_k + \\frac{\\ln S}{T^2} \\sum_{k=1}^{K} p_k (\\mathbb{E}_{p}[z] - z_k)\n$$\nThe second sum is $\\sum_{k=1}^{K} p_k \\mathbb{E}_{p}[z] - \\sum_{k=1}^{K} p_k z_k = \\mathbb{E}_{p}[z] \\sum_{k=1}^{K} p_k - \\mathbb{E}_{p}[z] = \\mathbb{E}_{p}[z] \\cdot 1 - \\mathbb{E}_{p}[z] = 0$.\nSo the second term vanishes, leaving:\n$$\n\\frac{dH}{dT} = -\\frac{1}{T^3} \\sum_{k=1}^{K} p_k (\\mathbb{E}_{p}[z] - z_k) z_k = -\\frac{1}{T^3} \\left( \\mathbb{E}_{p}[z] \\sum_{k=1}^{K} p_k z_k - \\sum_{k=1}^{K} p_k z_k^2 \\right)\n$$\n$$\n= -\\frac{1}{T^3} \\left( (\\mathbb{E}_{p}[z])^2 - \\mathbb{E}_{p}[z^2] \\right) = \\frac{1}{T^3} \\left( \\mathbb{E}_{p}[z^2] - (\\mathbb{E}_{p}[z])^2 \\right)\n$$\nThis expression is the variance of the logits under the distribution $p$, which we denote as $\\text{Var}_{p}(z)$.\n$$\n\\frac{d}{dT} H(p_{\\theta}^{T}(\\cdot \\mid x)) = \\frac{1}{T^3} \\text{Var}_{p}(z)\n$$\nWriting this out in the full notation required by the problem:\n$$\n\\frac{d}{dT} H(p_{\\theta}^{T}(\\cdot \\mid x)) = \\frac{1}{T^3} \\left[ \\sum_{k=1}^{K} p_{\\theta}^{T}(y=k \\mid x) (z_{\\theta,k}(x))^2 - \\left(\\sum_{k=1}^{K} p_{\\theta}^{T}(y=k \\mid x) z_{\\theta,k}(x)\\right)^2 \\right]\n$$\n\n**(iii) Condition for Increasing Entropy and Relation to Calibration**\n\nBased on the expression derived in part (ii), we can determine when increasing the temperature $T$ strictly increases the entropy $H$. We are given $T > 0$, which implies $T^3 > 0$. Therefore, the sign of $\\frac{dH}{dT}$ is determined by the sign of $\\text{Var}_{p}(z)$.\n$$\n\\frac{dH}{dT} > 0 \\iff \\text{Var}_{p}(z) > 0\n$$\nThe variance of a random variable (in this case, the logits $z_k$ with probabilities $p_k$) is strictly positive if and only if the variable is not constant over its support. The support of the distribution $p$ consists of all classes $k$ for which $p_k > 0$. For any finite logits, $p_k > 0$ for all $k \\in \\{1, \\dots, K\\}$. Therefore, $\\text{Var}_{p}(z) > 0$ if and only if the logits $z_{\\theta,k}(x)$ are not all equal to each other.\nIf all logits are equal, i.e., $z_{\\theta,k}(x) = c$ for all $k$, then the distribution becomes uniform, $p_k = 1/K$ for all $k$, regardless of $T$. The entropy is constant at $H=\\ln K$, and its derivative with respect to $T$ is $0$, consistent with our finding that $\\text{Var}_{p}(z)=0$.\n\nThe precise condition for entropy to strictly increase with $T$ is that the logits $\\{z_{\\theta,k}(x)\\}_{k=1}^{K}$ are not all identical.\n\nRelation to calibration: Model calibration refers to the alignment between a model's predicted probabilities and the actual frequencies of correctness. A common issue in deep learning classifiers is overconfidence, where the model outputs predictive distributions that are too \"sharp\" (low entropy), assigning probabilities close to $1$ to the predicted class, even when the model's accuracy does not warrant such certainty.\nAs established in part (i), temperature scaling with $T>0$ does not change the model's prediction (the $\\arg\\max_k p_k$). As established here in part (iii), increasing $T$ (for non-uniform logits) strictly increases the entropy of the predictive distribution. An increase in entropy corresponds to a \"softer,\" less concentrated probability distribution. Therefore, by increasing $T$, one can systematically reduce the model's confidence without altering its class predictions. This process directly counteracts overconfidence. Temperature scaling provides a mechanism to tune the \"sharpness\" of the output distribution post-hoc, finding a temperature $T^*$ that yields better-calibrated probabilities on a held-out dataset. It adjusts the confidence level (concentration) of the model's output to better reflect its empirical accuracy, which is a core goal of calibration.",
            "answer": "$$\n\\boxed{\\frac{1}{T^3} \\left[ \\sum_{k=1}^{K} p_{\\theta}^{T}(y=k \\mid x) (z_{\\theta,k}(x))^2 - \\left(\\sum_{k=1}^{K} p_{\\theta}^{T}(y=k \\mid x) z_{\\theta,k}(x)\\right)^2 \\right]}\n$$"
        },
        {
            "introduction": "Our final practice takes us deep inside the model, using entropy as a lens to examine its internal workings. The principle that entropy measures information content can be powerfully applied to identify redundancy within a neural network's learned features. In this exercise, you will implement an entropy-based pruning criterion for a Convolutional Neural Network, where channels with low activation entropy are considered less informative and thus candidates for removal . This hands-on task demonstrates how information theory can guide practical efforts in model compression and optimization.",
            "id": "3174074",
            "problem": "Consider a Convolutional Neural Network (CNN) trained on the Canadian Institute for Advanced Research (CIFAR) $10$-class dataset. Let the activation of channel $c$ over a set of samples be a real-valued random variable $Z_c$. From first principles in information theory, the Shannon entropy of a discrete random variable $Z$ with probabilities $\\{p_i\\}_{i=1}^B$ is defined as $$H(Z) = -\\sum_{i=1}^{B} p_i \\log_2 p_i,$$ measured in bits. In practice, activations are continuous-valued; to estimate entropy, one may discretize $Z_c$ into $B$ bins and treat the empirical bin frequencies as probabilities. The core idea for pruning is to remove channels with low activation entropy, under the hypothesis that low-entropy channels carry little information and are therefore redundant. \n\nYour task is to implement and evaluate an entropy-based pruning criterion, purely as a computational procedure grounded in these definitions, with no external data access. Specifically:\n\n1. Given a collection of channel activations for a single layer, discretize each channel’s activation values into $B$ bins spanning the observed range for that channel, and compute the estimated Shannon entropy $H(Z_c)$ in bits using base-$2$ logarithms. Use additive smoothing to avoid zero probabilities by adding a small constant $\\varepsilon$ to each bin count before normalizing.\n\n2. Prune channel $c$ if and only if $H(Z_c) &lt; \\tau$, where $\\tau$ is a user-specified threshold in bits. The pruning decision is strict inequality, so channels with $H(Z_c) = \\tau$ are not pruned.\n\n3. Quantify compression as the fraction of channels removed, i.e., $$\\text{compression\\_fraction} = \\frac{\\text{number of pruned channels}}{\\text{total number of channels}}.$$ This quantity is unitless.\n\n4. Estimate post-pruning accuracy on CIFAR-$10$ (expressed as a decimal between $0$ and $1$) by starting from a given baseline accuracy $A_0$ (before pruning) and subtracting a penalty proportional to the normalized information removed. For $B$ bins, the maximum possible entropy per channel is $$H_{\\max} = \\log_2(B).$$ Let $\\eta$ be a proportionality constant (unitless). Define the predicted accuracy after pruning as $$A_{\\text{pred}} = \\max\\left(0, \\min\\left(1, A_0 - \\eta \\sum_{c \\in \\mathcal{P}} \\frac{H(Z_c)}{H_{\\max}}\\right)\\right),$$ where $\\mathcal{P}$ is the set of pruned channels. This clamps the predicted accuracy to the interval $[0,1]$. Report accuracy as a decimal, not using a percentage sign.\n\n5. To relate pruning to redundancy, also report the average entropy of pruned channels, $$\\overline{H}_{\\text{pruned}} = \\begin{cases}\\frac{1}{|\\mathcal{P}|} \\sum_{c \\in \\mathcal{P}} H(Z_c), & \\text{if } |\\mathcal{P}| &gt; 0,\\\\ 0, & \\text{otherwise.}\\end{cases}$$\n\nImplementation details:\n- Use $B$ bins per channel with equal-width binning across that channel’s activation range.\n- Use additive smoothing constant $\\varepsilon$ for all bins before normalization.\n- Use base-$2$ logarithms for entropy to report values in bits.\n\nTest suite:\nImplement and evaluate the following four test cases. In each case, channels are defined by deterministic sequences so your program produces fixed outputs.\n\nCase $1$ (happy path, moderate threshold):\n- Number of samples per channel $N = 100$.\n- Channels ($6$ total):\n  - Channel $0$: $z_n = 0$ for $n = 1, \\dots, N$.\n  - Channel $1$: periodic small values $z_n \\in \\{-0.02, 0, 0.02\\}$ repeating to length $N$.\n  - Channel $2$: uniform sequence $z_n = -1 + \\frac{2(n-1)}{N-1}$ for $n = 1, \\dots, N$.\n  - Channel $3$: bimodal sequence composed of two clusters: first $50$ samples linearly spaced in $[-0.6,-0.4]$, next $50$ samples linearly spaced in $[0.4,0.6]$.\n  - Channel $4$: constant small value $z_n = 0.1$ for all $n$.\n  - Channel $5$: sinusoidal sequence $z_n = 0.5 \\sin\\left(\\frac{8\\pi (n-1)}{N-1}\\right)$ for $n = 1, \\dots, N$.\n- Bins $B = 8$, threshold $\\tau = 0.5$ bits, smoothing $\\varepsilon = 10^{-12}$, baseline accuracy $A_0 = 0.92$, proportionality constant $\\eta = 0.03$.\n\nCase $2$ (boundary threshold, no pruning):\n- Same channels and $N$ as Case $1$.\n- Bins $B = 8$, threshold $\\tau = 0$, smoothing $\\varepsilon = 10^{-12}$, baseline accuracy $A_0 = 0.92$, proportionality constant $\\eta = 0.03$.\n\nCase $3$ (aggressive threshold, substantial pruning):\n- Same channels and $N$ as Case $1$.\n- Bins $B = 8$, threshold $\\tau = 2.5$ bits, smoothing $\\varepsilon = 10^{-12}$, baseline accuracy $A_0 = 0.92$, proportionality constant $\\eta = 0.03$.\n\nCase $4$ (different distribution shapes and bin count):\n- Number of samples per channel $N = 120$.\n- Channels ($4$ total):\n  - Channel $0$: $z_n = 0$ for all $n$.\n  - Channel $1$: uniform sequence $z_n = -2 + \\frac{4(n-1)}{N-1}$ for $n = 1, \\dots, N$.\n  - Channel $2$: heavy-tailed mixture: first $60$ samples linearly spaced in $[-0.1,0.1]$, next $30$ samples in $[-3,-2]$, and last $30$ samples in $[2,3]$.\n  - Channel $3$: near-constant with occasional variation: first $100$ samples $z_n = 1$, last $20$ samples linearly spaced in $[0.9,1.1]$.\n- Bins $B = 4$, threshold $\\tau = 1.0$ bits, smoothing $\\varepsilon = 10^{-12}$, baseline accuracy $A_0 = 0.90$, proportionality constant $\\eta = 0.025$.\n\nRequired output:\n- For each case, produce three floats in order: compression\\_fraction, predicted\\_accuracy, average\\_entropy\\_pruned.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots,r_{12}]$ for $4$ cases with $3$ floats each, in the order of Case $1$, Case $2$, Case $3$, Case $4$).\n\nNo external files, user input, or network access are permitted. Use only the specified runtime environment.",
            "solution": "The problem requires the implementation of a deterministic computational procedure for channel pruning in a neural network based on the principle of Shannon entropy. The procedure involves calculating the entropy of channel activations, applying a pruning rule, and reporting several derived metrics: compression fraction, a predicted post-pruning accuracy, and the average entropy of the channels that were removed.\n\nThe validation of the problem statement confirms that it is scientifically grounded, well-posed, and objective. It is based on the standard definition of Shannon entropy from information theory. The method for estimating entropy for continuous variables—discretization via binning—is a common and valid, albeit approximate, technique. The formulas for the derived metrics are mathematically sound and unambiguous. The test cases are constructed using deterministic sequences, ensuring that the problem has a unique, verifiable solution. Therefore, the problem is valid, and we may proceed with the solution.\n\nThe solution is implemented by following a sequence of steps for each test case provided.\n\nFirst, for each channel, we must generate the activation values as specified. The problem defines these activations not as random samples, but as deterministic sequences. For a given channel $c$, we have a set of $N$ activation values $\\{z_n\\}_{n=1}^N$.\n\nSecond, we estimate the Shannon entropy $H(Z_c)$ for each channel's activation variable $Z_c$. Since the activations are continuous-valued, we must first discretize them. The procedure specifies using $B$ equal-width bins over the observed range of activations for that specific channel. Let the minimum and maximum activation values for channel $c$ be $z_{\\min, c} = \\min_n(z_n)$ and $z_{\\max, c} = \\max_n(z_n)$. The range $[z_{\\min, c}, z_{\\max, c}]$ is divided into $B$ bins of equal width $w = (z_{\\max, c} - z_{\\min, c}) / B$. We then count the number of activations, $k_i$, that fall into each bin $i$, for $i = 1, \\dots, B$. A special case arises if all activations in a channel are constant, i.e., $z_{\\min, c} = z_{\\max, c}$. In this scenario, all $N$ activations fall into a single bin, so its count is $N$, and all other $B-1$ bins have a count of $0$.\n\nThird, to compute the probabilities required for the entropy formula, we must handle the possibility of zero counts, which would make the logarithm undefined. The problem mandates additive smoothing (also known as Laplace smoothing). A small constant $\\varepsilon$ is added to every bin count. The smoothed count for bin $i$ is $k'_i = k_i + \\varepsilon$. The total smoothed count is $N' = \\sum_{i=1}^{B} k'_i = (\\sum k_i) + B\\varepsilon = N + B\\varepsilon$. The probability for each bin is then estimated as $p_i = k'_i / N'$.\n\nWith these probabilities, the Shannon entropy for channel $c$ is calculated in bits using the base-$2$ logarithm:\n$$\nH(Z_c) = -\\sum_{i=1}^{B} p_i \\log_2 p_i\n$$\nThis calculation is performed for every channel in the layer.\n\nFourth, we apply the pruning criterion. A channel $c$ is marked for pruning if and only if its calculated entropy $H(Z_c)$ is strictly less than a given threshold $\\tau$. We identify the set of pruned channels, which we denote as $\\mathcal{P}$.\n$$\n\\mathcal{P} = \\{c \\mid H(Z_c) < \\tau\\}\n$$\n\nFifth, we compute the three required output metrics based on the set of pruned channels $\\mathcal{P}$.\n\n1.  **Compression Fraction**: This is the ratio of the number of pruned channels to the total number of channels, $C$.\n    $$\n    \\text{compression\\_fraction} = \\frac{|\\mathcal{P}|}{C}\n    $$\n\n2.  **Average Entropy of Pruned Channels**: This metric quantifies the average information content of the channels that were deemed redundant. It is defined as:\n    $$\n    \\overline{H}_{\\text{pruned}} = \\begin{cases}\\frac{1}{|\\mathcal{P}|} \\sum_{c \\in \\mathcal{P}} H(Z_c), & \\text{if } |\\mathcal{P}| > 0,\\\\ 0, & \\text{otherwise.}\\end{cases}\n    $$\n    The case for $|\\mathcal{P}|=0$ ensures a well-defined output when no channels are pruned.\n\n3.  **Predicted Post-Pruning Accuracy**: This is an estimate based on a penalty model. The model starts with a baseline accuracy $A_0$ and subtracts a penalty proportional to the total normalized information removed by pruning. The maximum possible entropy for a distribution across $B$ bins is $H_{\\max} = \\log_2(B)$, which occurs when the distribution is uniform. The predicted accuracy, $A_{\\text{pred}}$, is given by:\n    $$\n    A_{\\text{pred}} = A_0 - \\eta \\sum_{c \\in \\mathcal{P}} \\frac{H(Z_c)}{H_{\\max}}\n    $$\n    where $\\eta$ is a unitless proportionality constant. The final value is clamped to the interval $[0, 1]$ to ensure it remains a valid accuracy value:\n    $$\n    A_{\\text{pred}} = \\max\\left(0, \\min\\left(1, A_{\\text{pred}}\\right)\\right)\n    $$\n\nBy systematically applying these steps to each of the four test cases defined in the problem statement, we can generate the required numerical results. Each step is a direct implementation of the provided formulas and rules.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the channel pruning problem for all test cases.\n    \"\"\"\n\n    def compute_metrics(activations_list, B, tau, epsilon, A0, eta):\n        \"\"\"\n        Computes entropy for each channel, prunes, and calculates metrics.\n        \"\"\"\n        num_channels = len(activations_list)\n        entropies = []\n        N = len(activations_list[0]) if num_channels > 0 else 0\n\n        for act in activations_list:\n            min_val, max_val = np.min(act), np.max(act)\n            \n            if min_val == max_val:\n                # All values are constant, they fall into a single bin.\n                counts = np.zeros(B)\n                counts[0] = N\n            else:\n                # Use numpy's histogram function for equal-width binning.\n                # The range is inclusive of min_val but exclusive of max_val,\n                # except for the last bin which is inclusive of max_val.\n                counts, _ = np.histogram(act, bins=B, range=(min_val, max_val))\n            \n            # Additive smoothing\n            smoothed_counts = counts.astype(np.float64) + epsilon\n            \n            # Normalize to get probabilities\n            total_smoothed_count = np.sum(smoothed_counts)\n            probs = smoothed_counts / total_smoothed_count\n            \n            # Calculate Shannon entropy in bits (log base 2)\n            # We filter out zero probabilities, though smoothing prevents this.\n            # This is just for robustness.\n            non_zero_probs = probs[probs > 0]\n            entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs))\n            entropies.append(entropy)\n\n        # Identify pruned channels\n        pruned_indices = [i for i, h in enumerate(entropies) if h < tau]\n        pruned_entropies = [entropies[i] for i in pruned_indices]\n        \n        num_pruned = len(pruned_indices)\n        \n        # 1. Compression Fraction\n        compression_fraction = num_pruned / num_channels if num_channels > 0 else 0.0\n\n        # 2. Average Entropy of Pruned Channels\n        avg_entropy_pruned = np.mean(pruned_entropies) if num_pruned > 0 else 0.0\n\n        # 3. Predicted Post-Pruning Accuracy\n        H_max = np.log2(B)\n        # Avoid division by zero if B = 1, though problem B >= 4\n        if H_max == 0:\n            penalty = 0\n        else:\n            total_info_removed = np.sum(pruned_entropies)\n            penalty = eta * total_info_removed / H_max\n        \n        predicted_accuracy = A0 - penalty\n        predicted_accuracy = np.clip(predicted_accuracy, 0.0, 1.0)\n        \n        return compression_fraction, predicted_accuracy, avg_entropy_pruned\n\n    # --- Test Cases Definition ---\n    test_cases = [\n        {\n            \"id\": 1,\n            \"N\": 100, \"B\": 8, \"tau\": 0.5, \"epsilon\": 1e-12, \"A0\": 0.92, \"eta\": 0.03,\n            \"channels_def\": [\n                lambda n: np.zeros(n),\n                lambda n: np.tile(np.array([-0.02, 0, 0.02]), n // 3 + 1)[:n],\n                lambda n: np.linspace(-1.0, 1.0, n),\n                lambda n: np.concatenate([np.linspace(-0.6, -0.4, n // 2), np.linspace(0.4, 0.6, n // 2)]),\n                lambda n: np.full(n, 0.1),\n                lambda n: 0.5 * np.sin(8 * np.pi * np.arange(n) / (n - 1)),\n            ]\n        },\n        {\n            \"id\": 2,\n            \"N\": 100, \"B\": 8, \"tau\": 0.0, \"epsilon\": 1e-12, \"A0\": 0.92, \"eta\": 0.03,\n            \"channels_def\": [\n                lambda n: np.zeros(n),\n                lambda n: np.tile(np.array([-0.02, 0, 0.02]), n // 3 + 1)[:n],\n                lambda n: np.linspace(-1.0, 1.0, n),\n                lambda n: np.concatenate([np.linspace(-0.6, -0.4, n // 2), np.linspace(0.4, 0.6, n // 2)]),\n                lambda n: np.full(n, 0.1),\n                lambda n: 0.5 * np.sin(8 * np.pi * np.arange(n) / (n - 1)),\n            ]\n        },\n        {\n            \"id\": 3,\n            \"N\": 100, \"B\": 8, \"tau\": 2.5, \"epsilon\": 1e-12, \"A0\": 0.92, \"eta\": 0.03,\n            \"channels_def\": [\n                lambda n: np.zeros(n),\n                lambda n: np.tile(np.array([-0.02, 0, 0.02]), n // 3 + 1)[:n],\n                lambda n: np.linspace(-1.0, 1.0, n),\n                lambda n: np.concatenate([np.linspace(-0.6, -0.4, n // 2), np.linspace(0.4, 0.6, n // 2)]),\n                lambda n: np.full(n, 0.1),\n                lambda n: 0.5 * np.sin(8 * np.pi * np.arange(n) / (n - 1)),\n            ]\n        },\n        {\n            \"id\": 4,\n            \"N\": 120, \"B\": 4, \"tau\": 1.0, \"epsilon\": 1e-12, \"A0\": 0.90, \"eta\": 0.025,\n            \"channels_def\": [\n                lambda n: np.zeros(n),\n                lambda n: np.linspace(-2.0, 2.0, n),\n                lambda n: np.concatenate([\n                    np.linspace(-0.1, 0.1, n // 2), \n                    np.linspace(-3.0, -2.0, n // 4), \n                    np.linspace(2.0, 3.0, n // 4)\n                ]),\n                lambda n: np.concatenate([\n                    np.full(100, 1.0),\n                    np.linspace(0.9, 1.1, 20)\n                ]),\n            ]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        activations_list = [gen(N) for gen in case[\"channels_def\"]]\n        \n        results = compute_metrics(\n            activations_list,\n            case[\"B\"],\n            case[\"tau\"],\n            case[\"epsilon\"],\n            case[\"A0\"],\n            case[\"eta\"]\n        )\n        all_results.extend(results)\n\n    print(f\"[{','.join(f'{r:.7f}' for r in all_results)}]\")\n\nsolve()\n```"
        }
    ]
}