{
    "hands_on_practices": [
        {
            "introduction": "The multivariate chain rule is the engine of backpropagation, allowing us to compute how a final loss changes with respect to any parameter. This practice goes a step further, showing how we can intentionally manipulate this process using a special-purpose module like the Gradient Reversal Layer (GRL), a key component in techniques like domain adaptation. By working through this exercise , you will gain concrete, hands-on practice tracing gradients through a computation graph with multiple paths and a non-standard information flow.",
            "id": "3190232",
            "problem": "Consider a simple deep learning architecture with a shared linear feature extractor and two linear heads: a classification head and a domain head. Let the input be a column vector $x \\in \\mathbb{R}^{2}$, the shared features be $h \\in \\mathbb{R}^{2}$, the classification logits be $u \\in \\mathbb{R}^{2}$, and the domain logits be $v \\in \\mathbb{R}^{2}$. The mappings are defined as follows:\n$$h = W x,$$\n$$u = A h,$$\n$$\\text{GRL: } y = h \\text{ in forward; in backward, } \\frac{\\partial L}{\\partial h} = -\\lambda \\frac{\\partial L}{\\partial y},$$\n$$v = B y,$$\nwith the total loss\n$$L(x) = \\frac{1}{2}\\|u - c\\|^{2} + \\frac{1}{2}\\|v - d\\|^{2}.$$\nHere, the Gradient Reversal Layer (GRL) is an operation with a scalar hyperparameter $\\lambda > 0$. All vectors are column vectors and $\\|\\cdot\\|$ denotes the Euclidean norm.\n\nUsing only the multivariate chain rule from first principles (via Jacobians) and the basic derivative of the squared norm, model the Gradient Reversal Layer as a chain-rule multiplier in the backward pass and derive the upstream gradient $\\nabla_{x} L$ at the specific point given below. Then, report the second component of $\\nabla_{x} L$.\n\nUse the following numerical values:\n$$x = \\begin{pmatrix}1 \\\\ -2\\end{pmatrix}, \\quad W = \\begin{pmatrix}2 & 1 \\\\ 0 & -1\\end{pmatrix}, \\quad A = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}, \\quad B = \\begin{pmatrix}1 & 1 \\\\ 2 & -1\\end{pmatrix},$$\n$$c = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}, \\quad d = \\begin{pmatrix}3 \\\\ -1\\end{pmatrix}, \\quad \\lambda = \\frac{3}{2}.$$\n\nYour final answer must be the exact value of the second component of $\\nabla_{x} L$ at the specified point. Do not round your answer. No units are required.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of multivariate calculus and its application to deep learning, specifically in the context of backpropagation through a domain-adversarial architecture. The problem is well-posed, with all necessary functions, variables, and constants provided to calculate a unique solution. The language is objective and mathematically precise.\n\nThe goal is to compute the gradient of the total loss $L$ with respect to the input $x$, denoted as $\\nabla_{x} L$, and then report its second component. The gradient vector $\\nabla_{x} L$ is the transpose of the derivative row vector $\\frac{\\partial L}{\\partial x}$. We will use the multivariate chain rule to derive an expression for $\\frac{\\partial L}{\\partial x}$.\n\nThe total loss $L$ is a function of $u$ and $v$, which in turn depend on $h$, and $h$ depends on $x$. The dependency graph is as follows: $x \\to h$, and from $h$ there are two branches: $h \\to u \\to L$ and $h \\to y \\to v \\to L$. The total gradient is the sum of contributions from all paths from the output $L$ to the input $x$.\n\nThe total loss is given by $L(u, v) = \\frac{1}{2}\\|u - c\\|^{2} + \\frac{1}{2}\\|v - d\\|^{2}$. Let $L_u = \\frac{1}{2}\\|u - c\\|^{2}$ and $L_v = \\frac{1}{2}\\|v - d\\|^{2}$.\n\nWe apply the chain rule starting from the loss $L$ and working backwards to the input $x$. The derivative of the loss with respect to $x$ can be expressed via the intermediate variable $h$:\n$$\n\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial h} \\frac{\\partial h}{\\partial x}\n$$\nThe derivative of the loss with respect to the shared features $h$ receives contributions from both the classification head (via $u$) and the domain head (via $v$ and $y$):\n$$\n\\frac{\\partial L}{\\partial h} = \\left(\\frac{\\partial L}{\\partial h}\\right)_{\\text{path } u} + \\left(\\frac{\\partial L}{\\partial h}\\right)_{\\text{path } v}\n$$\nLet's compute each contribution separately.\n\n1.  Contribution from the classification head ($L \\to u \\to h$):\n    The derivative of the loss with respect to $u$ is $\\frac{\\partial L}{\\partial u} = (u-c)^T$.\n    The function $u = Ah$ has a Jacobian matrix $\\frac{\\partial u}{\\partial h} = A$.\n    By the chain rule, the contribution to $\\frac{\\partial L}{\\partial h}$ from this path is:\n    $$\n    \\left(\\frac{\\partial L}{\\partial h}\\right)_{\\text{path } u} = \\frac{\\partial L}{\\partial u} \\frac{\\partial u}{\\partial h} = (u-c)^T A\n    $$\n\n2.  Contribution from the domain head ($L \\to v \\to y \\to h$):\n    This path involves the Gradient Reversal Layer (GRL). First, we find the gradient signal arriving at the GRL's output, $y$.\n    The derivative of the loss with respect to $v$ is $\\frac{\\partial L}{\\partial v} = (v-d)^T$.\n    The function $v = By$ has a Jacobian matrix $\\frac{\\partial v}{\\partial y} = B$.\n    The derivative of the loss with respect to $y$ is:\n    $$\n    \\frac{\\partial L}{\\partial y} = \\frac{\\partial L}{\\partial v} \\frac{\\partial v}{\\partial y} = (v-d)^T B\n    $$\n    The GRL is defined by the mapping $y=h$ in the forward pass. In the backward pass, it takes the upstream gradient $\\frac{\\partial L}{\\partial y}$ and outputs the gradient with respect to its input $h$. The problem statement specifies this behavior as: \"in backward, $\\frac{\\partial L}{\\partial h} = -\\lambda \\frac{\\partial L}{\\partial y}$\". This defines the contribution from the domain head to the gradient at $h$.\n    $$\n    \\left(\\frac{\\partial L}{\\partial h}\\right)_{\\text{path } v} = -\\lambda \\frac{\\partial L}{\\partial y} = -\\lambda (v-d)^T B\n    $$\n    This models the GRL as replacing the standard Jacobian for the identity function ($I$) with a scaled negative identity ($-\\lambda I$) in the chain rule computation for this branch.\n\nCombining both contributions, the total derivative of the loss with respect to $h$ is:\n$$\n\\frac{\\partial L}{\\partial h} = (u-c)^T A - \\lambda (v-d)^T B\n$$\nFinally, we propagate this gradient back to $x$. The function $h = Wx$ has a Jacobian matrix $\\frac{\\partial h}{\\partial x} = W$.\n$$\n\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial h} \\frac{\\partial h}{\\partial x} = \\left( (u-c)^T A - \\lambda (v-d)^T B \\right) W\n$$\nThe gradient vector $\\nabla_x L$ is the transpose of the row vector $\\frac{\\partial L}{\\partial x}$:\n$$\n\\nabla_x L = \\left( \\frac{\\partial L}{\\partial x} \\right)^T = W^T \\left( A^T(u-c) - \\lambda B^T(v-d) \\right)\n$$\nNow, we substitute the given numerical values. First, we perform the forward pass to compute the values of the intermediate vectors.\nGiven values:\n$$\nx = \\begin{pmatrix}1 \\\\ -2\\end{pmatrix}, \\quad W = \\begin{pmatrix}2 & 1 \\\\ 0 & -1\\end{pmatrix}, \\quad A = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}, \\quad B = \\begin{pmatrix}1 & 1 \\\\ 2 & -1\\end{pmatrix},\n$$\n$$\nc = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}, \\quad d = \\begin{pmatrix}3 \\\\ -1\\end{pmatrix}, \\quad \\lambda = \\frac{3}{2}.\n$$\n\nForward pass:\n$$\nh = Wx = \\begin{pmatrix}2 & 1 \\\\ 0 & -1\\end{pmatrix} \\begin{pmatrix}1 \\\\ -2\\end{pmatrix} = \\begin{pmatrix}2(1)+1(-2) \\\\ 0(1)+(-1)(-2)\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 2\\end{pmatrix}\n$$\n$$\nu = Ah = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix} \\begin{pmatrix}0 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 2\\end{pmatrix}\n$$\nThe GRL forward pass is $y=h$:\n$$\ny = h = \\begin{pmatrix}0 \\\\ 2\\end{pmatrix}\n$$\n$$\nv = By = \\begin{pmatrix}1 & 1 \\\\ 2 & -1\\end{pmatrix} \\begin{pmatrix}0 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}1(0)+1(2) \\\\ 2(0)+(-1)(2)\\end{pmatrix} = \\begin{pmatrix}2 \\\\ -2\\end{pmatrix}\n$$\n\nBackward pass (gradient computation):\nFirst, compute the error vectors:\n$$\nu-c = \\begin{pmatrix}0 \\\\ 2\\end{pmatrix} - \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}\n$$\n$$\nv-d = \\begin{pmatrix}2 \\\\ -2\\end{pmatrix} - \\begin{pmatrix}3 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}-1 \\\\ -1\\end{pmatrix}\n$$\nNow, substitute these into the expression for $\\nabla_x L$.\nWe need the transposes of $A$, $B$, and $W$:\n$$\nA^T = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}, \\quad B^T = \\begin{pmatrix}1 & 2 \\\\ 1 & -1\\end{pmatrix}, \\quad W^T = \\begin{pmatrix}2 & 0 \\\\ 1 & -1\\end{pmatrix}\n$$\nCompute the term inside the parentheses in the gradient expression:\n$$\nA^T(u-c) = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix} \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}\n$$\n$$\nB^T(v-d) = \\begin{pmatrix}1 & 2 \\\\ 1 & -1\\end{pmatrix} \\begin{pmatrix}-1 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}1(-1)+2(-1) \\\\ 1(-1)+(-1)(-1)\\end{pmatrix} = \\begin{pmatrix}-3 \\\\ 0\\end{pmatrix}\n$$\nThe full term in parentheses is:\n$$\nA^T(u-c) - \\lambda B^T(v-d) = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} - \\frac{3}{2}\\begin{pmatrix}-3 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} - \\begin{pmatrix}-\\frac{9}{2} \\\\ 0\\end{pmatrix} = \\begin{pmatrix}\\frac{9}{2} \\\\ 1\\end{pmatrix}\n$$\nFinally, compute $\\nabla_x L$:\n$$\n\\nabla_x L = W^T \\left( A^T(u-c) - \\lambda B^T(v-d) \\right) = \\begin{pmatrix}2 & 0 \\\\ 1 & -1\\end{pmatrix} \\begin{pmatrix}\\frac{9}{2} \\\\ 1\\end{pmatrix}\n$$\n$$\n\\nabla_x L = \\begin{pmatrix}2\\left(\\frac{9}{2}\\right) + 0(1) \\\\ 1\\left(\\frac{9}{2}\\right) + (-1)(1)\\end{pmatrix} = \\begin{pmatrix}9 \\\\ \\frac{9}{2}-1\\end{pmatrix} = \\begin{pmatrix}9 \\\\ \\frac{7}{2}\\end{pmatrix}\n$$\nThe gradient of the loss function with respect to the input $x$ is $\\nabla_x L = \\begin{pmatrix}9 \\\\ \\frac{7}{2}\\end{pmatrix}$.\nThe problem asks for the second component of this gradient vector.\nThe second component is $\\frac{7}{2}$.",
            "answer": "$$\\boxed{\\frac{7}{2}}$$"
        },
        {
            "introduction": "A common challenge in deep learning is training networks that include non-differentiable operations, such as the rounding function used in model quantization. Mathematically, the chain rule would yield a zero or undefined gradient, halting the learning process. This practice explores a popular and pragmatic workaround: the Straight-Through Estimator (STE), which substitutes a \"fake\" but useful gradient during the backward pass . By calculating both the true gradient and the STE approximation, you will develop a deeper appreciation for the distinction between the formal chain rule and its creative application in making intractable problems trainable.",
            "id": "3190214",
            "problem": "Consider a two-dimensional parameter vector $\\boldsymbol{x} \\in \\mathbb{R}^2$ that is passed through an element-wise quantization function $\\boldsymbol{q}(\\boldsymbol{x}) = \\mathrm{round}(\\boldsymbol{x})$, where $\\mathrm{round}(\\cdot)$ maps each component to the nearest integer. Let the output be $\\boldsymbol{y} = \\boldsymbol{q}(\\boldsymbol{x})$, and define the loss $$L(\\boldsymbol{y}) = \\tfrac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b}\\|_2^2,$$ where $\\boldsymbol{A} \\in \\mathbb{R}^{2 \\times 2}$ and $\\boldsymbol{b} \\in \\mathbb{R}^2$ are fixed. Suppose the training procedure uses the Straight-Through Estimator (STE), defined here as using the exact $\\boldsymbol{q}(\\boldsymbol{x})$ in the forward pass but approximating its Jacobian by the identity matrix in the backward pass, i.e., $\\partial \\boldsymbol{q} / \\partial \\boldsymbol{x} \\approx \\boldsymbol{I}$ element-wise. Your goal is to reason from first principles using the multivariate chain rule to characterize the gradient $\\nabla_{\\boldsymbol{x}} L$ and the bias introduced by the STE.\n\nWork with the concrete values $\\boldsymbol{A} = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $\\boldsymbol{b} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, and $\\boldsymbol{x} = \\begin{bmatrix} 0.3 \\\\ -1.7 \\end{bmatrix}$. Using $\\boldsymbol{y} = \\mathrm{round}(\\boldsymbol{x})$ computed element-wise, apply the multivariate chain rule to analyze $\\nabla_{\\boldsymbol{x}} L$ under the exact quantization and under the STE approximation, and assess the bias in $\\partial L/\\partial \\boldsymbol{x}$ attributable to the STE at the given point.\n\nWhich of the following statements are correct?\n\nA. Using the multivariate chain rule with the Straight-Through Estimator (STE), the gradient satisfies $\\nabla_{\\boldsymbol{x}} L \\approx \\nabla_{\\boldsymbol{y}} L$ at the given point, and the resulting estimate equals $\\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix}$.\n\nB. The exact gradient $\\nabla_{\\boldsymbol{x}} L$ at the given $\\boldsymbol{x}$ equals $\\boldsymbol{0}$, because $\\boldsymbol{q}(\\boldsymbol{x})$ is locally constant almost everywhere and the composition $L(\\boldsymbol{q}(\\boldsymbol{x}))$ is therefore locally flat in $\\boldsymbol{x}$ away from the quantization thresholds.\n\nC. The STE gradient is an unbiased estimator of the true gradient $\\nabla_{\\boldsymbol{x}} L$ at the given $\\boldsymbol{x}$.\n\nD. For the given invertible $\\boldsymbol{A}$, the bias in $\\nabla_{\\boldsymbol{x}} L$ introduced by the STE vanishes if and only if $\\boldsymbol{A}\\boldsymbol{y} = \\boldsymbol{b}$.\n\nE. Under exact quantization $\\boldsymbol{q}(\\boldsymbol{x}) = \\mathrm{round}(\\boldsymbol{x})$, the true Jacobian $\\partial \\boldsymbol{q} / \\partial \\boldsymbol{x}$ equals the identity matrix $\\boldsymbol{I}$ for almost all $\\boldsymbol{x}$.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the context of deep learning (specifically, training with non-differentiable functions), well-posed with all necessary data provided, and formulated using objective, precise mathematical language. There are no contradictions, ambiguities, or factual unsoundness.\n\nWe are given the following:\n- Parameter vector $\\boldsymbol{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\in \\mathbb{R}^2$.\n- An element-wise quantization function $\\boldsymbol{q}(\\boldsymbol{x}) = \\mathrm{round}(\\boldsymbol{x})$, where $\\mathrm{round}(\\cdot)$ maps a real number to the nearest integer.\n- The output $\\boldsymbol{y} = \\boldsymbol{q}(\\boldsymbol{x})$.\n- A loss function $L(\\boldsymbol{y}) = \\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b}\\|_2^2$.\n- The Straight-Through Estimator (STE) approximation for the backward pass: $\\frac{\\partial \\boldsymbol{q}}{\\partial \\boldsymbol{x}} \\approx \\boldsymbol{I}$, where $\\boldsymbol{I}$ is the $2 \\times 2$ identity matrix.\n- Specific values: $\\boldsymbol{A} = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $\\boldsymbol{b} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, and $\\boldsymbol{x} = \\begin{bmatrix} 0.3 \\\\ -1.7 \\end{bmatrix}$.\n\nOur goal is to analyze the gradient $\\nabla_{\\boldsymbol{x}} L$ using both the exact derivative and the STE approximation.\n\nFirst, let's establish the general form of the gradients using the multivariate chain rule. The loss $L$ is a composition of functions $L(\\boldsymbol{y}(\\boldsymbol{x}))$. The chain rule gives the gradient of $L$ with respect to $\\boldsymbol{x}$ as:\n$$ \\nabla_{\\boldsymbol{x}} L = \\left(\\frac{\\partial \\boldsymbol{y}}{\\partial \\boldsymbol{x}}\\right)^T \\nabla_{\\boldsymbol{y}} L $$\nwhere $\\frac{\\partial \\boldsymbol{y}}{\\partial \\boldsymbol{x}}$ is the Jacobian matrix of $\\boldsymbol{y} = \\boldsymbol{q}(\\boldsymbol{x})$ with respect to $\\boldsymbol{x}$.\n\nLet's compute the gradient of the loss with respect to $\\boldsymbol{y}$, $\\nabla_{\\boldsymbol{y}} L$.\nThe loss is $L(\\boldsymbol{y}) = \\frac{1}{2}(\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b})^T(\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b})$. Expanding this gives:\n$$ L(\\boldsymbol{y}) = \\frac{1}{2}(\\boldsymbol{y}^T\\boldsymbol{A}^T\\boldsymbol{A}\\boldsymbol{y} - 2\\boldsymbol{b}^T\\boldsymbol{A}\\boldsymbol{y} + \\boldsymbol{b}^T\\boldsymbol{b}) $$\nTaking the gradient with respect to $\\boldsymbol{y}$ yields:\n$$ \\nabla_{\\boldsymbol{y}} L = \\frac{1}{2}(2\\boldsymbol{A}^T\\boldsymbol{A}\\boldsymbol{y} - 2\\boldsymbol{A}^T\\boldsymbol{b}) = \\boldsymbol{A}^T(\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b}) $$\n\nNow, we perform the forward pass with the given values:\n$\\boldsymbol{x} = \\begin{bmatrix} 0.3 \\\\ -1.7 \\end{bmatrix}$.\nThe output $\\boldsymbol{y}$ is computed using the exact quantization function:\n$$ \\boldsymbol{y} = \\boldsymbol{q}(\\boldsymbol{x}) = \\mathrm{round}\\left(\\begin{bmatrix} 0.3 \\\\ -1.7 \\end{bmatrix}\\right) = \\begin{bmatrix} \\mathrm{round}(0.3) \\\\ \\mathrm{round}(-1.7) \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -2 \\end{bmatrix} $$\n\nUsing this value of $\\boldsymbol{y}$, we can compute $\\nabla_{\\boldsymbol{y}} L$:\n$$ \\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b} = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}\\begin{bmatrix} 0 \\\\ -2 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -2 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix} $$\nThe matrix $\\boldsymbol{A}$ is diagonal, so $\\boldsymbol{A}^T = \\boldsymbol{A}$.\n$$ \\nabla_{\\boldsymbol{y}} L = \\boldsymbol{A}^T(\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b}) = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}\\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix} $$\n\nNow, let's analyze the two different gradients for $\\boldsymbol{x}$.\n\n**1. The Exact Gradient $\\nabla_{\\boldsymbol{x}} L$**\nThe function $\\boldsymbol{q}(\\boldsymbol{x}) = \\mathrm{round}(\\boldsymbol{x})$ is a step function. Its derivative is defined and equal to zero almost everywhere. The points of non-differentiability occur where the argument is a half-integer (e.g., $0.5, -0.5, 1.5, \\dots$).\nThe given point is $\\boldsymbol{x} = \\begin{bmatrix} 0.3 \\\\ -1.7 \\end{bmatrix}$. Neither component is a half-integer. Therefore, in a small open neighborhood around this point, the function $\\boldsymbol{q}(\\boldsymbol{x})$ is constant: $\\boldsymbol{q}(\\boldsymbol{x}) = \\begin{bmatrix} 0 \\\\ -2 \\end{bmatrix}$.\nSince $\\boldsymbol{q}(\\boldsymbol{x})$ is constant in this neighborhood, the composite function $L(\\boldsymbol{q}(\\boldsymbol{x}))$ is also constant. The gradient of a constant function is the zero vector.\nThus, the exact gradient is $\\nabla_{\\boldsymbol{x}} L = \\boldsymbol{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\nAlternatively, the true Jacobian of $\\boldsymbol{q}(\\boldsymbol{x})$ at this point is the zero matrix:\n$$ \\frac{\\partial \\boldsymbol{q}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\frac{d(\\mathrm{round}(x_1))}{dx_1} & 0 \\\\ 0 & \\frac{d(\\mathrm{round}(x_2))}{dx_2} \\end{bmatrix} = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix} $$\nUsing the chain rule, $\\nabla_{\\boldsymbol{x}} L = \\left(\\frac{\\partial \\boldsymbol{q}}{\\partial \\boldsymbol{x}}\\right)^T \\nabla_{\\boldsymbol{y}} L = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}^T \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n\n**2. The Straight-Through Estimator (STE) Gradient $(\\nabla_{\\boldsymbol{x}} L)_{\\text{STE}}$**\nThe STE uses the exact forward pass but approximates the Jacobian in the backward pass. The approximation is given as $\\frac{\\partial \\boldsymbol{q}}{\\partial \\boldsymbol{x}} \\approx \\boldsymbol{I}$.\nApplying this to the chain rule:\n$$ (\\nabla_{\\boldsymbol{x}} L)_{\\text{STE}} = (\\boldsymbol{I})^T \\nabla_{\\boldsymbol{y}} L = \\boldsymbol{I} \\nabla_{\\boldsymbol{y}} L = \\nabla_{\\boldsymbol{y}} L $$\nWe already calculated $\\nabla_{\\boldsymbol{y}} L = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix}$.\nTherefore, $(\\nabla_{\\boldsymbol{x}} L)_{\\text{STE}} = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix}$.\n\n**3. Bias of the STE**\nThe bias of the estimator at this point is the difference between the estimated gradient and the true gradient:\n$$ \\text{Bias} = (\\nabla_{\\boldsymbol{x}} L)_{\\text{STE}} - \\nabla_{\\boldsymbol{x}} L = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix} $$\n\nNow we evaluate each option.\n\n**A. Using the multivariate chain rule with the Straight-Through Estimator (STE), the gradient satisfies $\\nabla_{\\boldsymbol{x}} L \\approx \\nabla_{\\boldsymbol{y}} L$ at the given point, and the resulting estimate equals $\\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix}$.**\nThe STE approximation is defined as using $\\frac{\\partial \\boldsymbol{q}}{\\partial \\boldsymbol{x}} \\approx \\boldsymbol{I}$. Applying this to the chain rule, $(\\nabla_{\\boldsymbol{x}} L)_{\\text{STE}} = (\\boldsymbol{I})^T \\nabla_{\\boldsymbol{y}} L = \\nabla_{\\boldsymbol{y}} L$. This establishes the relationship $\\nabla_{\\boldsymbol{x}} L \\approx \\nabla_{\\boldsymbol{y}} L$ under STE. Our calculation showed that $\\nabla_{\\boldsymbol{y}} L = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix}$. The statement is fully consistent with our derivation.\n**Verdict: Correct.**\n\n**B. The exact gradient $\\nabla_{\\boldsymbol{x}} L$ at the given $\\boldsymbol{x}$ equals $\\boldsymbol{0}$, because $\\boldsymbol{q}(\\boldsymbol{x})$ is locally constant almost everywhere and the composition $L(\\boldsymbol{q}(\\boldsymbol{x}))$ is therefore locally flat in $\\boldsymbol{x}$ away from the quantization thresholds.**\nAs analyzed above, the point $\\boldsymbol{x} = \\begin{bmatrix} 0.3 \\\\ -1.7 \\end{bmatrix}$ is not at a quantization threshold (a half-integer). Thus, $\\boldsymbol{q}(\\boldsymbol{x})$ is constant in an open neighborhood of $\\boldsymbol{x}$. Consequently, the composite function $L(\\boldsymbol{q}(\\boldsymbol{x}))$ is also locally constant, and its gradient is the zero vector $\\boldsymbol{0}$. The reasoning provided in the option is correct.\n**Verdict: Correct.**\n\n**C. The STE gradient is an unbiased estimator of the true gradient $\\nabla_{\\boldsymbol{x}} L$ at the given $\\boldsymbol{x}$.**\nAn estimator is unbiased at a point in a deterministic setting if it equals the true value.\nThe STE gradient is $(\\nabla_{\\boldsymbol{x}} L)_{\\text{STE}} = \\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix}$.\nThe true gradient is $\\nabla_{\\boldsymbol{x}} L = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\nSince $\\begin{bmatrix} -2 \\\\ -2 \\end{bmatrix} \\neq \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, the estimator is biased at this point.\n**Verdict: Incorrect.**\n\n**D. For the given invertible $\\boldsymbol{A}$, the bias in $\\nabla_{\\boldsymbol{x}} L$ introduced by the STE vanishes if and only if $\\boldsymbol{A}\\boldsymbol{y} = \\boldsymbol{b}$.**\nThe bias, for any $\\boldsymbol{x}$ not on a quantization threshold, is:\n$$ \\text{Bias} = (\\nabla_{\\boldsymbol{x}} L)_{\\text{STE}} - \\nabla_{\\boldsymbol{x}} L = \\nabla_{\\boldsymbol{y}} L - \\boldsymbol{0} = \\boldsymbol{A}^T(\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b}) $$\nThe bias vanishes when $\\boldsymbol{A}^T(\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b}) = \\boldsymbol{0}$.\nThe problem specifies that $\\boldsymbol{A}$ is invertible. The given matrix $\\boldsymbol{A} = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}$ has a determinant of $2$, so it is indeed invertible. If $\\boldsymbol{A}$ is invertible, its transpose $\\boldsymbol{A}^T$ is also invertible. For an invertible matrix $\\boldsymbol{M}$, the equation $\\boldsymbol{M}\\boldsymbol{z} = \\boldsymbol{0}$ holds if and only if $\\boldsymbol{z} = \\boldsymbol{0}$.\nApplying this here, with $\\boldsymbol{M} = \\boldsymbol{A}^T$ and $\\boldsymbol{z} = \\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b}$, we have that $\\boldsymbol{A}^T(\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b}) = \\boldsymbol{0}$ if and only if $\\boldsymbol{A}\\boldsymbol{y} - \\boldsymbol{b} = \\boldsymbol{0}$, which is equivalent to $\\boldsymbol{A}\\boldsymbol{y} = \\boldsymbol{b}$. The condition $\\boldsymbol{A}\\boldsymbol{y} = \\boldsymbol{b}$ corresponds to the loss being at its minimum possible value of $0$.\n**Verdict: Correct.**\n\n**E. Under exact quantization $\\boldsymbol{q}(\\boldsymbol{x}) = \\mathrm{round}(\\boldsymbol{x})$, the true Jacobian $\\partial \\boldsymbol{q} / \\partial \\boldsymbol{x}$ equals the identity matrix $\\boldsymbol{I}$ for almost all $\\boldsymbol{x}$.**\nThe function $\\boldsymbol{q}(\\boldsymbol{x})$ is a vector of element-wise `round` functions. The derivative of a single `round` function, $\\frac{d}{dt}\\mathrm{round}(t)$, is $0$ for all $t$ that are not half-integers, and is undefined at the half-integers. It is never equal to $1$.\nThe Jacobian matrix is:\n$$ \\frac{\\partial \\boldsymbol{q}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\frac{\\partial q_1}{\\partial x_1} & \\frac{\\partial q_1}{\\partial x_2} \\\\ \\frac{\\partial q_2}{\\partial x_1} & \\frac{\\partial q_2}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} \\frac{d}{dx_1}\\mathrm{round}(x_1) & 0 \\\\ 0 & \\frac{d}{dx_2}\\mathrm{round}(x_2) \\end{bmatrix} $$\nFor almost all $\\boldsymbol{x}$ (i.e., those with no half-integer components), this Jacobian is the zero matrix $\\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}$. It is not the identity matrix $\\boldsymbol{I}$. The statement that the true Jacobian equals $\\boldsymbol{I}$ is the core (and false) assumption made by the STE, not a property of the true Jacobian itself.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "Beyond updating model weights, the chain rule is a powerful analytical tool for optimizing hyperparameters and understanding fundamental model properties. This exercise focuses on deriving the gradient with respect to the temperature parameter in a softmax function, a common technique for controlling model confidence and performing calibration . The problem then challenges you to use the structure of the chain rule to reason about parameter identifiability, a critical concept in model design that questions whether a unique set of parameters can be learned from the data.",
            "id": "3190253",
            "problem": "Consider a classification model with $K$ classes that outputs a vector of logits $z \\in \\mathbb{R}^{K}$ and uses a temperature-scaled softmax to produce probabilities $p \\in \\mathbb{R}^{K}$, where $p_{i}=\\frac{\\exp\\left(z_{i}/T\\right)}{\\sum_{j=1}^{K}\\exp\\left(z_{j}/T\\right)}$ with $T>0$. The training objective is the Cross-Entropy (CE) loss $L(z,T;y)=-\\sum_{i=1}^{K}y_{i}\\ln\\left(p_{i}\\right)$, where $y \\in \\mathbb{R}^{K}$ is a target distribution on the probability simplex, so $\\sum_{i=1}^{K}y_{i}=1$ and $y_{i}\\geq 0$. Using only fundamental definitions and the multivariate chain rule, derive the closed-form expression for $\\frac{\\partial L}{\\partial T}$ in terms of $z$, $y$, $p$, and $T$. Then, consider an upstream scalar parameter $s>0$ that scales the logits before softmax, so the model probabilities become $p_{i}(s,T)=\\frac{\\exp\\left(s z_{i}/T\\right)}{\\sum_{j=1}^{K}\\exp\\left(s z_{j}/T\\right)}$. Based on the composition of mappings from $(s,T)$ to $p(s,T)$ to $L$, discuss whether $(s,T)$ are jointly identifiable from the loss and explain why or why not. Your final answer must be a single closed-form analytic expression for $\\frac{\\partial L}{\\partial T}$.",
            "solution": "The problem asks for two things: first, to derive the closed-form expression for the partial derivative of the Cross-Entropy loss $L$ with respect to the temperature parameter $T$, and second, to discuss the joint identifiability of a logit scaling parameter $s$ and the temperature $T$.\n\nThe problem is valid as it is scientifically grounded in established principles of machine learning and calculus, is well-posed with sufficient information for a unique solution and reasoned conclusion, and is expressed in objective, formal language. It is a direct application of the multivariate chain rule to a relevant problem in deep learning.\n\nFirst, we derive the expression for $\\frac{\\partial L}{\\partial T}$.\nThe loss function is given by $L(z,T;y)=-\\sum_{i=1}^{K}y_{i}\\ln(p_{i})$, where the probabilities $p_i$ are functions of the logits $z$ and the temperature $T$. Specifically, $p_{i}(z, T) = \\frac{\\exp(z_{i}/T)}{\\sum_{j=1}^{K}\\exp(z_{j}/T)}$.\nThe loss $L$ is a function of $T$ through the intermediate probability vector $p$. To find the derivative $\\frac{\\partial L}{\\partial T}$, we apply the multivariate chain rule. A direct application would be $\\frac{\\partial L}{\\partial T} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial T}$. However, a more structured approach is to define an intermediate variable representing the scaled logits.\n\nLet $a_i = \\frac{z_i}{T}$. The probability $p_i$ is the $i$-th component of the softmax function applied to the vector $a \\in \\mathbb{R}^K$: $p_i = \\frac{\\exp(a_i)}{\\sum_{j=1}^K \\exp(a_j)}$.\nThe loss $L$ is a function of the vector $a$, which in turn is a function of $T$. The structure of the dependencies is $T \\rightarrow a \\rightarrow p \\rightarrow L$. We can apply the chain rule as follows:\n$$ \\frac{\\partial L}{\\partial T} = \\sum_{i=1}^{K} \\frac{\\partial L}{\\partial a_i} \\frac{\\partial a_i}{\\partial T} $$\nWe will compute the two terms in the sum separately.\n\n1.  Compute $\\frac{\\partial L}{\\partial a_i}$:\n    The loss function can be written in terms of $a_i$ as:\n    $$ L = -\\sum_{j=1}^{K} y_j \\ln(p_j) = -\\sum_{j=1}^{K} y_j \\ln\\left(\\frac{\\exp(a_j)}{\\sum_{k=1}^K \\exp(a_k)}\\right) $$\n    Using the properties of the logarithm, we expand this expression:\n    $$ L = -\\sum_{j=1}^{K} y_j \\left( a_j - \\ln\\left(\\sum_{k=1}^K \\exp(a_k)\\right) \\right) $$\n    $$ L = -\\sum_{j=1}^{K} y_j a_j + \\left(\\sum_{j=1}^{K} y_j\\right) \\ln\\left(\\sum_{k=1}^K \\exp(a_k)\\right) $$\n    Since $y$ is a target distribution on the probability simplex, we have $\\sum_{j=1}^{K} y_j = 1$. The expression for the loss simplifies to:\n    $$ L = -\\sum_{j=1}^{K} y_j a_j + \\ln\\left(\\sum_{k=1}^K \\exp(a_k)\\right) $$\n    Now, we take the partial derivative with respect to $a_i$:\n    $$ \\frac{\\partial L}{\\partial a_i} = \\frac{\\partial}{\\partial a_i} \\left( -\\sum_{j=1}^{K} y_j a_j \\right) + \\frac{\\partial}{\\partial a_i} \\left( \\ln\\left(\\sum_{k=1}^K \\exp(a_k)\\right) \\right) $$\n    $$ \\frac{\\partial L}{\\partial a_i} = -y_i + \\frac{1}{\\sum_{k=1}^K \\exp(a_k)} \\cdot \\frac{\\partial}{\\partial a_i}\\left(\\sum_{k=1}^K \\exp(a_k)\\right) $$\n    $$ \\frac{\\partial L}{\\partial a_i} = -y_i + \\frac{\\exp(a_i)}{\\sum_{k=1}^K \\exp(a_k)} $$\n    Recognizing the definition of $p_i$, we get the well-known result for the derivative of the cross-entropy loss with respect to the pre-softmax activations:\n    $$ \\frac{\\partial L}{\\partial a_i} = p_i - y_i $$\n\n2.  Compute $\\frac{\\partial a_i}{\\partial T}$:\n    This derivative is straightforward from the definition $a_i = z_i/T$:\n    $$ \\frac{\\partial a_i}{\\partial T} = \\frac{\\partial}{\\partial T}\\left(\\frac{z_i}{T}\\right) = -\\frac{z_i}{T^2} $$\n\n3.  Combine the results using the chain rule:\n    Now we substitute the expressions for $\\frac{\\partial L}{\\partial a_i}$ and $\\frac{\\partial a_i}{\\partial T}$ back into the chain rule formula:\n    $$ \\frac{\\partial L}{\\partial T} = \\sum_{i=1}^{K} \\frac{\\partial L}{\\partial a_i} \\frac{\\partial a_i}{\\partial T} = \\sum_{i=1}^{K} (p_i - y_i) \\left(-\\frac{z_i}{T^2}\\right) $$\n    We can factor out the term $-\\frac{1}{T^2}$ as it does not depend on the summation index $i$:\n    $$ \\frac{\\partial L}{\\partial T} = -\\frac{1}{T^2} \\sum_{i=1}^{K} (p_i z_i - y_i z_i) $$\n    Rearranging the terms inside the summation gives the final elegant form:\n    $$ \\frac{\\partial L}{\\partial T} = \\frac{1}{T^2} \\left( \\sum_{i=1}^{K} y_i z_i - \\sum_{i=1}^{K} p_i z_i \\right) $$\n    This expression depends on $z$, $y$, $p$, and $T$, as requested. In vector notation, this can be written as $\\frac{1}{T^2}(y^Tz - p^Tz)$.\n\nNext, we address the identifiability of parameters $(s, T)$.\nThe problem introduces a scalar parameter $s>0$ that scales the logits, such that the probabilities become $p_{i}(s,T)=\\frac{\\exp(s z_{i}/T)}{\\sum_{j=1}^{K}\\exp(s z_{j}/T)}$. The loss is a function of these probabilities, $L(s,T) = -\\sum_{i=1}^K y_i \\ln(p_i(s,T))$.\nTwo parameters are jointly identifiable if a change in their values necessarily leads to a change in the model's output or the loss function. If different sets of parameter values can produce the exact same model output for all inputs, the parameters are not identifiable.\n\nLet's examine the argument of the exponential function in the definition of $p_i(s, T)$: the term is $\\frac{s z_i}{T}$. This can be rewritten as $\\frac{z_i}{T/s}$.\nLet's define a new parameter, the effective temperature $\\tau$, as the ratio of $T$ and $s$:\n$$ \\tau = \\frac{T}{s} $$\nWith this definition, the probabilities can be expressed in terms of $\\tau$:\n$$ p_i(s, T) = \\frac{\\exp(z_i / \\tau)}{\\sum_{j=1}^K \\exp(z_j / \\tau)} $$\nThis shows that the probability vector $p$ depends only on the single combined parameter $\\tau = T/s$, and not on $s$ and $T$ independently.\nSince the loss function $L$ is a function of the probabilities $p$, the loss $L$ also depends only on the ratio $\\tau = T/s$.\nThis means that any pair of parameters $(s, T)$ with $s>0, T>0$ will produce the same probability vector $p$ and thus the same loss $L$ as any other pair $(s', T')$ as long as the ratio is the same, i.e., $T/s = T'/s'$.\nFor example, let's consider a parameter pair $(s_1, T_1)$. Now consider a different pair $(s_2, T_2) = (c s_1, c T_1)$ for any constant $c > 0$ and $c \\neq 1$.\nThe new pair is distinct from the original one: $(s_2, T_2) \\neq (s_1, T_1)$.\nHowever, the ratio for the new pair is:\n$$ \\frac{T_2}{s_2} = \\frac{c T_1}{c s_1} = \\frac{T_1}{s_1} $$\nSince the ratio is unchanged, the effective temperature $\\tau$ is the same. This implies that the probability vector $p$ is identical for both parameter pairs, for any input logits $z$. Consequently, the loss $L(s_1, T_1; y) = L(s_2, T_2; y)$ for any target distribution $y$.\nBecause we can find infinitely many distinct pairs of $(s, T)$ that result in the exact same loss value for all possible inputs, the parameters $s$ and $T$ are not jointly identifiable from the loss. Only their ratio, the effective temperature $\\tau=T/s$, can be uniquely determined.",
            "answer": "$$ \\boxed{\\frac{1}{T^2} \\left( \\sum_{i=1}^{K} y_i z_i - \\sum_{i=1}^{K} p_i z_i \\right)} $$"
        }
    ]
}