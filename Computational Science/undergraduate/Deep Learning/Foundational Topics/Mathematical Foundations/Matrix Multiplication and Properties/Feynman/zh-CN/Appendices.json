{
    "hands_on_practices": [
        {
            "introduction": "在构建深度网络时，我们经常串联多个线性层。一个基本但深刻的问题是：这些层的应用顺序重要吗？本练习将通过矩阵可交换性的概念来探讨这个问题，揭示两个线性变换在何种条件下可以互换，以及这对于网络设计的函数冗余意味着什么 。",
            "id": "3148079",
            "problem": "考虑一个前馈神经网络中的两个线性层，由实 $3 \\times 3$ 矩阵 $W_1$ 和 $W_2$ 表示。您将通过展示一个共享的特征基（同时对角化）来分析这些层何时交换，并解释这对于在层间有和没有非线性函数的情况下，网络中的功能冗余意味着什么。仅使用矩阵乘法、特征值和特征向量的基本定义，以及非线性函数的逐元素作用。\n\n设 $W_1$ 和 $W_2$ 如下给出\n$$\nW_1 = \\begin{pmatrix}\na  0  0 \\\\\n0  b  0 \\\\\n0  0  c\n\\end{pmatrix},\n\\qquad\nW_2 = \\begin{pmatrix}\nd  0  0 \\\\\n0  e  0 \\\\\n0  0  f\n\\end{pmatrix},\n$$\n其中 $a,b,c,d,e,f \\in \\mathbb{R}$。它们在标准基中已经是对角矩阵，这是一个候选的共享特征基。\n\n首先，通过确定一个共享的特征基并解释同时对角化如何强制实现交换性，来构建两个实矩阵 $W_1$ 和 $W_2$ 交换的充分条件。您的构建必须基于特征值和特征向量的定义以及对角矩阵的性质，而不假设任何未由此推导出的特定快捷定理。\n\n接下来，考虑两个线性层之间没有任何非线性函数时的复合，其作用为 $x \\mapsto W_2 W_1 x$（对于 $x \\in \\mathbb{R}^3$）。解释在这种线性情况下的功能冗余概念，并对于上面给出的特定 $W_1$ 和 $W_2$，计算标量 $\\operatorname{tr}(W_2 W_1)$，作为关于 $a,b,c,d,e,f$ 的封闭形式解析表达式。\n\n最后，在层之间插入一个整流线性单元（ReLU）非线性函数，定义为逐元素映射 $\\sigma(z) = \\max\\{0,z\\}$，形成 $x \\mapsto W_2 \\,\\sigma(W_1 x)$。在共享的特征基（此处为标准基）中，给出在该输入上此复合可简化为单个线性映射的关于 $x$ 的条件，并解释当存在非线性函数时，这对两层的功能冗余意味着什么。您无需为此部分计算任何数值。\n\n您的最终答案必须是关于 $a,b,c,d,e,f$ 的 $\\operatorname{tr}(W_2 W_1)$ 的单一封闭形式表达式。",
            "solution": "我们从基本定义开始。由矩阵 $W_1$ 和 $W_2$ 表示的两个线性算子可交换，当且仅当 $W_1 W_2 = W_2 W_1$。如果存在一个可逆矩阵 $S$ 使得 $S^{-1} W S = D$（其中 $D$ 是一个对角矩阵），则矩阵 $W$ 是可对角化的。对角矩阵 $D$ 具有这样的性质：$D u$ 会独立地缩放向量 $u$ 的每个坐标。并且对角矩阵是可交换的，因为对角矩阵的乘法是逐元素的，因此在相应对角线元素的层面上是结合的和可交换的。\n\n通过同时对角化构建交换的充分条件的过程如下。假设 $W_1$ 和 $W_2$ 是可对角化的，并且共享一个完备的特征向量集合；也就是说，存在一个可逆矩阵 $S$，其列构成了 $W_1$ 和 $W_2$ 共有的特征向量基。那么我们可以写出\n$$\nS^{-1} W_1 S = D_1, \\qquad S^{-1} W_2 S = D_2,\n$$\n其中 $D_1$ 和 $D_2$ 是对角矩阵，包含了 $W_1$ 和 $W_2$ 相对于共享特征基的特征值。考虑乘积：\n$$\nW_1 W_2 = S D_1 S^{-1} S D_2 S^{-1} = S (D_1 D_2) S^{-1},\n$$\n和\n$$\nW_2 W_1 = S D_2 S^{-1} S D_1 S^{-1} = S (D_2 D_1) S^{-1}.\n$$\n因为对角矩阵是可交换的，我们有 $D_1 D_2 = D_2 D_1$，这立即意味着 $W_1 W_2 = W_2 W_1$。因此，$W_1$ 和 $W_2$ 可交换的一个充分条件是它们可以被同一个可逆矩阵 $S$ 同时对角化（等价地，它们共享一个完备的特征基）。特别地，如果 $W_1$ 和 $W_2$ 在同一基中都是对角矩阵，那么它们是可交换的。\n\n对于给定的特定矩阵，\n$$\nW_1 = \\begin{pmatrix}\na  0  0 \\\\\n0  b  0 \\\\\n0  0  c\n\\end{pmatrix},\n\\qquad\nW_2 = \\begin{pmatrix}\nd  0  0 \\\\\n0  e  0 \\\\\n0  0  f\n\\end{pmatrix},\n$$\n标准基向量是它们的共同特征向量，并且两个矩阵在该基中都是对角矩阵。因此，$W_1$ 和 $W_2$ 是可交换的。没有非线性函数时的复合是线性算子 $W_2 W_1$，它也是一个对角矩阵，其对角线元素由相应元素的乘积给出：\n$$\nW_2 W_1 = \\begin{pmatrix}\nad  0  0 \\\\\n0  be  0 \\\\\n0  0  cf\n\\end{pmatrix}.\n$$\n在这种线性情况下的功能冗余意味着，双层线性子网络 $x \\mapsto W_2 W_1 x$ 可以被单个线性层 $x \\mapsto W_{\\text{eff}} x$ (其中 $W_{\\text{eff}} = W_2 W_1$) 替代，而不改变输入-输出映射。为了计算所要求的标量，根据迹的定义即对角线元素之和，我们有：\n$$\n\\operatorname{tr}(W_2 W_1) = ad + be + cf.\n$$\n现在考虑在层之间插入一个整流线性单元（ReLU）非线性函数，形成映射 $x \\mapsto W_2 \\,\\sigma(W_1 x)$，其中 $\\sigma$ 逐元素应用：$\\sigma(z) = \\max\\{0,z\\}$。因为 $W_1$ 是对角矩阵，\n$$\nW_1 x = \\begin{pmatrix}\na x_1 \\\\ b x_2 \\\\ c x_3\n\\end{pmatrix}, \\qquad \\sigma(W_1 x) = \\begin{pmatrix}\n\\max\\{0, a x_1\\} \\\\ \\max\\{0, b x_2\\} \\\\ \\max\\{0, c x_3\\}\n\\end{pmatrix}.\n$$\n随后乘以 $W_2$ 会将 $\\sigma(W_1 x)$ 的每个坐标分别缩放 $d$、$e$ 和 $f$。当且仅当 $\\sigma$ 对 $W_1 x$ 的作用是恒等映射时，复合 $x \\mapsto W_2 \\,\\sigma(W_1 x)$ 在特定输入 $x$ 上可以简化为单个线性映射 $x \\mapsto W_2 W_1 x$。这种情况发生当且仅当 $W_1 x$ 的每个坐标都是非负的：\n$$\na x_1 \\ge 0, \\quad b x_2 \\ge 0, \\quad c x_3 \\ge 0.\n$$\n在这些条件下，$\\sigma(W_1 x) = W_1 x$，所以 $W_2 \\,\\sigma(W_1 x) = W_2 W_1 x$，非线性函数的存在不会改变该输入上的函数；对于这样的 $x$，该双层结构是功能冗余的。如果这些逐坐标的不等式中有任何一个不成立，那么至少有一个分量会被 ReLU 截断为零，并且整体映射不能被一个对所有输入都固定的单个线性算子所表示——功能冗余被非线性函数打破了。特别地，全局冗余（对所有 $x$）将要求输入域被限制在满足 $a x_1 \\ge 0$，$b x_2 \\ge 0$ 和 $c x_3 \\ge 0$ 的集合上，或者 $a, b, c$ 的符号与输入坐标对齐，使得 ReLU 在感兴趣的域中处处充当恒等函数。\n\n因此，所要求的线性情况下的标量表达式是上面计算出的迹。",
            "answer": "$$\\boxed{ad + be + cf}$$"
        },
        {
            "introduction": "除了简单地堆叠层，我们还可以通过对权重矩阵施加结构性约束来主动设计模型的行为。这个练习介绍了一种通过正交投影来限制层输出的方法，迫使其存在于一个预定义的子空间中 。通过分析这种约束对模型表达能力和梯度计算的影响，我们可以更深入地理解如何将先验知识编码到网络架构中。",
            "id": "3148026",
            "problem": "深度网络中的一个全连接线性层通过 $y = W x$（其中 $W \\in \\mathbb{R}^{m \\times n}$）将输入 $x \\in \\mathbb{R}^{n}$ 映射到输出 $y \\in \\mathbb{R}^{m}$。为了强制输出位于一个预定的子空间中，训练过程被修改，使得在每次参数更新后，前向传播中使用的有效权重被下式覆盖：\n$$\nW_{\\mathrm{eff}} \\leftarrow P W,\n$$\n其中 $P \\in \\mathbb{R}^{m \\times m}$ 是一个对称幂等矩阵，即 $P = P^{\\top}$ 且 $P^{2} = P$。令 $\\mathrm{range}(P)$ 表示 $P$ 的列空间，$\\mathrm{null}(P)$ 表示其零空间，并令 $\\mathrm{rank}(P) = r$。考虑该层在约束 $W_{\\mathrm{eff}} = P W$ 下实现的函数类别，并分析其相对于无约束情况的表达能力。\n\n以下哪个陈述是正确的？\n\nA. 对于每个满足 $P T = T$ 的目标矩阵 $T \\in \\mathbb{R}^{m \\times n}$，存在一个参数矩阵 $W \\in \\mathbb{R}^{m \\times n}$，使得有效权重等于 $T$，即 $P W = T$。\n\nB. 可实现集合 $\\{P W : W \\in \\mathbb{R}^{m \\times n}\\}$ 等于 $\\{M \\in \\mathbb{R}^{m \\times n} : \\mathrm{col}(M) \\subseteq \\mathrm{null}(P)\\}$。\n\nC. 可实现的有效权重构成的线性空间的维度是 $r n$。\n\nD. 如果这个受约束的层后面跟着一个权重为 $U \\in \\mathbb{R}^{k \\times m}$ 且具有满行秩 $k$ 的可训练线性层，那么复合 $U (P W)$ 可以表示 $\\mathbb{R}^{k \\times n}$ 中的任何矩阵，因此没有表达能力的损失。\n\nE. 对于任何输入 $x \\in \\mathbb{R}^{n}$，输出 $y = (P W) x$ 必然位于 $\\mathrm{range}(P)^{\\perp}$ 中。\n\nF. 在参数化 $W_{\\mathrm{eff}} = P W$ 的情况下，如果 $L$ 是一个仅通过 $W_{\\mathrm{eff}}$ 依赖于 $W$ 的标量损失，那么梯度满足\n$$\n\\nabla_{W} L \\;=\\; P^{\\top} \\nabla_{W_{\\mathrm{eff}}} L \\;=\\; P \\,\\nabla_{W_{\\mathrm{eff}}} L.\n$$\n\n选择所有适用的选项。",
            "solution": "经分析和验证，该问题陈述是自洽的、数学上合理的且定义良好的。线性代数中的概念，如幂等矩阵、对称矩阵、列空间、零空间和秩，都是标准概念。将其应用于神经网络中的受约束线性层是一个有效且具体的背景。因此，该问题可以按其陈述求解。\n\n问题的核心在于理解矩阵 $P \\in \\mathbb{R}^{m \\times m}$ 的性质。我们已知 $P$ 是对称的 ($P = P^{\\top}$) 和幂等的 ($P^2 = P$)。同时具有这两个性质的矩阵是正交投影矩阵。它将 $\\mathbb{R}^m$ 中的任何向量正交投影到其列空间上，记为 $\\mathrm{range}(P)$。该投影的秩为 $\\mathrm{rank}(P) = r$，即子空间 $\\mathrm{range}(P)$ 的维度。\n\n正交投影矩阵 $P$ 的关键性质：\n1.  对任意向量 $v \\in \\mathrm{range}(P)$，有 $P v = v$。\n2.  对任意向量 $v \\in \\mathrm{range}(P)^{\\perp}$，有 $P v = 0$。由于 $P$ 是对称的，其零空间是其值域（列空间）的正交补：$\\mathrm{null}(P) = \\mathrm{range}(P)^{\\perp}$。\n3.  任何向量 $u \\in \\mathbb{R}^m$ 可以唯一地分解为 $u = v + w$，其中 $v \\in \\mathrm{range}(P)$ 且 $w \\in \\mathrm{null}(P)$。应用 $P$ 得到 $Pu = P(v+w) = Pv + Pw = v + 0 = v$。\n\n可实现的有效权重集合为 $\\mathcal{W}_{\\mathrm{eff}} = \\{P W : W \\in \\mathbb{R}^{m \\times n}\\}$。\n令 $M = PW$ 为一个有效权重矩阵。$M$ 的第 $j$ 列是 $M_{:,j} = P W_{:,j}$。由于 $W_{:,j}$ 是 $\\mathbb{R}^m$ 中的一个向量，其投影 $P W_{:,j}$ 必须位于 $P$ 的列空间中，即 $M_{:,j} \\in \\mathrm{range}(P)$。这对 $M$ 的所有列都成立。因此，任何可实现的有效权重矩阵的列空间都是 $\\mathrm{range}(P)$ 的一个子空间：$\\mathrm{col}(M) \\subseteq \\mathrm{range}(P)$。这等价于条件 $PM = M$，因为将 $P$ 应用于 $M$ 的任何一列都会使该列保持不变。\n\n现在我们评估每个陈述。\n\n**A. 对于每个满足 $P T = T$ 的目标矩阵 $T \\in \\mathbb{R}^{m \\times n}$，存在一个参数矩阵 $W \\in \\mathbb{R}^{m \\times n}$，使得有效权重等于 $T$，即 $P W = T$。**\n\n该陈述询问在给定条件 $PT = T$ 的情况下，方程 $PW = T$ 是否有解 $W$。\n条件 $PT=T$ 意味着目标矩阵 $T$ 位于可行的有效权重集合中。我们需要找到在投影映射 $W \\mapsto PW$ 下是否存在一个“原像”$W$。\n让我们尝试一个简单的候选解：$W = T$。\n将其代入方程，我们得到 $P W = P T$。\n根据给定条件，$P T = T$。\n因此，对于 $W=T$ 这个选择， $P W = T$ 是满足的。\n因为我们已经为 $W$ 找到了至少一个解，所以该陈述是正确的。\n\n**A 的结论：正确。**\n\n**B. 可实现集合 $\\{P W : W \\in \\mathbb{R}^{m \\times n}\\}$ 等于 $\\{M \\in \\mathbb{R}^{m \\times n} : \\mathrm{col}(M) \\subseteq \\mathrm{null}(P)\\}$。**\n\n如前文所述，可实现集合是 $\\mathcal{W}_{\\mathrm{eff}} = \\{PW : W \\in \\mathbb{R}^{m \\times n}\\}$。对于任何矩阵 $M \\in \\mathcal{W}_{\\mathrm{eff}}$，其列必须位于 $\\mathrm{range}(P)$ 中。\n该陈述表明可实现集合是 $\\{M \\in \\mathbb{R}^{m \\times n} : \\mathrm{col}(M) \\subseteq \\mathrm{null}(P)\\}$。\n对于该集合中的矩阵 $M$，每一列都必须在 $P$ 的零空间中，这意味着对所有 $j$ 都有 $P M_{:,j} = 0$，或者说 $PM = 0$。\n所以该陈述声称 $\\mathcal{W}_{\\mathrm{eff}} = \\{M \\in \\mathbb{R}^{m \\times n} : PM = 0\\}$。\n然而，我们知道对于任何 $M \\in \\mathcal{W}_{\\mathrm{eff}}$，都有 $PM = M$。\n所以该陈述将意味着对于任何 $W$，矩阵 $M=PW$ 都必须满足 $M=0$。这仅在 $P$ 是零矩阵时成立（即 $r=0$）。\n通常情况下，对于 $r0$，我们可以构造一个非零的有效权重矩阵。例如，令 $v \\in \\mathrm{range}(P)$ 为一个非零向量，并令 $W$ 为一个其第一列为 $v$ 的矩阵。那么 $PW$ 的第一列是 $Pv = v \\neq 0$，所以 $PW \\neq 0$。\n一个可实现矩阵的列位于 $\\mathrm{range}(P)$ 中，而该陈述声称它们位于 $\\mathrm{null}(P)$ 中。由于 $P$ 是一个正交投影，$\\mathrm{range}(P) \\cap \\mathrm{null}(P) = \\{0\\}$。这两个集合是根本不同的（除非 $r=0$）。\n\n**B 的结论：不正确。**\n\n**C. 可实现的有效权重构成的线性空间的维度是 $r n$。**\n\n可实现的有效权重集合是 $\\mathcal{W}_{\\mathrm{eff}} = \\{M \\in \\mathbb{R}^{m \\times n} : PM=M\\}$。这是一个向量空间，因为它是线性变换 $M \\mapsto M - PM$ 的核。我们需要求出它的维度。\n条件 $PM=M$ 等价于要求 $M$ 的 $n$ 个列（记为 $M_{:,j}$）中的每一个都位于 $P$ 的列空间中。\n子空间 $\\mathrm{range}(P)$ 的维度是 $r$。所以，对于每一列 $M_{:,j}$，我们必须从一个 $r$ 维空间中选择一个向量。\n令 $\\{p_1, \\dots, p_r\\}$ 为 $\\mathrm{range}(P)$ 的一组基。那么任何一列 $M_{:,j}$ 都可以唯一地表示为一个线性组合 $M_{:,j} = \\sum_{k=1}^r c_{jk} p_k$。系数 $c_{j1}, \\dots, c_{jr}$ 是 $M_{:,j}$ 在这组基下的坐标。\n一个矩阵 $M \\in \\mathcal{W}_{\\mathrm{eff}}$ 完全由这 $n$ 组 $r$ 个系数 $\\{c_{jk}\\}_{k=1 \\dots r}^{j=1 \\dots n}$ 决定。\n自由参数（或自由度）的总数就是这些系数的总数，即 $r \\times n$。\n因此，线性空间 $\\mathcal{W}_{\\mathrm{eff}}$ 的维度是 $rn$。\n\n**C 的结论：正确。**\n\n**D. 如果这个受约束的层后面跟着一个权重为 $U \\in \\mathbb{R}^{k \\times m}$ 且具有满行秩 $k$ 的可训练线性层，那么复合 $U (P W)$ 可以表示 $\\mathbb{R}^{k \\times n}$ 中的任何矩阵，因此没有表达能力的损失。**\n\n代表这两层复合的矩阵是 $T_{\\mathrm{comp}} = U (PW)$。我们需要确定集合 $\\{U(PW) : W \\in \\mathbb{R}^{m \\times n}, U \\in \\mathbb{R}^{k \\times m} \\text{ with rank } k\\}$ 是否等于 $\\mathbb{R}^{k \\times n}$。\n令 $W_{\\mathrm{eff}} = PW$。有效权重矩阵的秩受 $P$ 的秩的限制：\n$\\mathrm{rank}(W_{\\mathrm{eff}}) = \\mathrm{rank}(PW) \\le \\min(\\mathrm{rank}(P), \\mathrm{rank}(W)) \\le \\mathrm{rank}(P) = r$。\n现在，考虑复合矩阵 $T_{\\mathrm{comp}} = U W_{\\mathrm{eff}}$ 的秩：\n$\\mathrm{rank}(T_{\\mathrm{comp}}) = \\mathrm{rank}(U W_{\\mathrm{eff}}) \\le \\min(\\mathrm{rank}(U), \\mathrm{rank}(W_{\\mathrm{eff}})) \\le \\mathrm{rank}(W_{\\mathrm{eff}})$。\n结合这些不等式，我们得到 $\\mathrm{rank}(T_{\\mathrm{comp}}) \\le r$。\n$\\mathbb{R}^{k \\times n}$ 中所有矩阵的集合包含秩最高可达 $\\min(k, n)$ 的矩阵。如果我们选择的 $k$ 和 $n$ 使得 $\\min(k,n)  r$，那么就不可能表示一个秩 $\\mathrm{rank}(T)  r$ 的矩阵 $T \\in \\mathbb{R}^{k \\times n}$。例如，如果 $m=10, n=5, k=5$，并且我们有一个秩 $r=2$ 的投影，那么复合映射只能产生秩最多为 $2$ 的矩阵。它不能产生一个可逆的 $5 \\times 5$ 矩阵（如果 $n=k=5$），因为其秩为 $5$。\n因此，该复合通常不能表示 $\\mathbb{R}^{k \\times n}$ 中的任何矩阵。表达能力的损失由投影 $P$ 的秩 $r$ 决定。\n\n**D 的结论：不正确。**\n\n**E. 对于任何输入 $x \\in \\mathbb{R}^{n}$，输出 $y = (P W) x$ 必然位于 $\\mathrm{range}(P)^{\\perp}$ 中。**\n\n该层的输出是 $y = (PW)x$。\n令 $W_{\\mathrm{eff}} = PW$。输出向量 $y$ 可以写成 $W_{\\mathrm{eff}}$ 各列的线性组合：\n$y = \\sum_{j=1}^n x_j (W_{\\mathrm{eff}})_{:,j}$，其中 $x_j$ 是输入向量 $x$ 的分量。\n如前所示，每一列 $(W_{\\mathrm{eff}})_{:,j} = P W_{:,j}$ 都是 $\\mathrm{range}(P)$ 中的一个向量。\n由于 $\\mathrm{range}(P)$ 是一个向量子空间，该子空间中向量的任何线性组合也必须在该子空间中。\n因此，对于任何输入 $x$，$y \\in \\mathrm{range}(P)$。\n该陈述声称 $y \\in \\mathrm{range}(P)^{\\perp}$。由于 $P=P^\\top$, $\\mathrm{range}(P)^\\perp = \\mathrm{null}(P)$。\n一个向量只有在它是零向量时才能同时位于 $\\mathrm{range}(P)$ 和 $\\mathrm{range}(P)^{\\perp}$ 中。所以该陈述将意味着对所有 $x$ 都有 $y=0$，这通常是不正确的。该陈述与事实正好相反。\n\n**E 的结论：不正确。**\n\n**F. 在参数化 $W_{\\mathrm{eff}} = P W$ 的情况下，如果 $L$ 是一个仅通过 $W_{\\mathrm{eff}}$ 依赖于 $W$ 的标量损失，那么梯度满足 $\\nabla_{W} L \\;=\\; P^{\\top} \\nabla_{W_{\\mathrm{eff}}} L \\;=\\; P \\,\\nabla_{W_{\\mathrm{eff}}} L$。**\n\n我们被要求计算损失 $L$ 相对于参数 $W$ 的梯度。损失 $L$ 是 $W_{\\mathrm{eff}}$ 的函数，而 $W_{\\mathrm{eff}}$ 又是 $W$ 的函数：$L(W) = L(W_{\\mathrm{eff}}(W))$。\n我们使用矩阵微积分的链式法则。$L$ 的微分可以写为：\n$dL = \\mathrm{Tr}((\\nabla_{W} L)^\\top dW)$。\n同样，使用链式法则，$dL = \\mathrm{Tr}((\\nabla_{W_{\\mathrm{eff}}} L)^\\top dW_{\\mathrm{eff}})$。\n微分 $dW$ 和 $dW_{\\mathrm{eff}}$ 之间的关系通过对 $W_{\\mathrm{eff}} = PW$ 求导得到：\n$dW_{\\mathrm{eff}} = d(PW) = P(dW)$，因为 $P$ 是一个常数矩阵。\n将此代入 $dL$ 的表达式中：\n$dL = \\mathrm{Tr}((\\nabla_{W_{\\mathrm{eff}}} L)^\\top P dW)$。\n使用迹的循环性质 $\\mathrm{Tr}(ABC) = \\mathrm{Tr}(CAB)$，我们可以写出：\n$dL = \\mathrm{Tr}(P dW (\\nabla_{W_{\\mathrm{eff}}} L)^\\top)$。这不是我们想要的形式。让我们使用 $\\mathrm{Tr}(AB) = \\mathrm{Tr}(BA)$ 和 $\\mathrm{Tr}(A^\\top) = \\mathrm{Tr}(A)$。\n令 $A = (\\nabla_{W_{\\mathrm{eff}}} L)^\\top P$ 且 $B = dW$。\n$dL = \\mathrm{Tr}(A B)$。我们想把它写成 $\\mathrm{Tr}(C^\\top B)$ 的形式。\n我们有 $A^\\top = ((\\nabla_{W_{\\mathrm{eff}}} L)^\\top P)^\\top = P^\\top ((\\nabla_{W_{\\mathrm{eff}}} L)^\\top)^\\top = P^\\top \\nabla_{W_{\\mathrm{eff}}} L$。\n所以我们可以写成 $A = (A^\\top)^\\top = (P^\\top \\nabla_{W_{\\mathrm{eff}}} L)^\\top$。\n那么 $dL = \\mathrm{Tr}((P^\\top \\nabla_{W_{\\mathrm{eff}}} L)^\\top dW)$。\n通过与 $dL = \\mathrm{Tr}((\\nabla_{W} L)^\\top dW)$ 比较，我们确定了梯度：\n$\\nabla_{W} L = P^{\\top} \\nabla_{W_{\\mathrm{eff}}} L$。\n我们已知 $P$ 是对称的，所以 $P^{\\top} = P$。\n因此，$\\nabla_{W} L = P \\nabla_{W_{\\mathrm{eff}}} L$。\n该陈述声称 $\\nabla_{W} L = P^{\\top} \\nabla_{W_{\\mathrm{eff}}} L = P \\nabla_{W_{\\mathrm{eff}}} L$，这与我们的推导一致。\n\n**F 的结论：正确。**\n\n最终总结：陈述 A、C 和 F 是正确的。",
            "answer": "$$\\boxed{ACF}$$"
        },
        {
            "introduction": "衡量一个线性层（即权重矩阵）的“大小”或“影响力”对于模型正则化和训练稳定性分析至关重要。谱范数是衡量这种影响力的一个关键指标，它表示矩阵对向量的最大拉伸程度。本练习将指导你通过编程实现幂迭代法，这是一种仅使用矩阵-向量乘法来有效估计矩阵谱范数的经典算法 。",
            "id": "3148029",
            "problem": "给定一个实数矩阵 $W \\in \\mathbb{R}^{m \\times n}$，它代表深度学习中一个线性层的权重矩阵。目标是仅使用矩阵乘法及其性质，并基于基本定义，实现一个算法来估计谱范数（矩阵算子 $2$-范数）$\\|W\\|_2$。谱范数从第一性原理定义为\n$$\n\\|W\\|_2 = \\sup_{\\|x\\|_2 = 1} \\|W x\\|_2,\n$$\n其中 $\\|\\cdot\\|_2$ 表示向量的欧几里得范数。估计 $\\|W\\|_2$ 的迭代过程依赖于以下步骤：初始化一个单位向量 $v \\in \\mathbb{R}^{n}$，然后对指定的迭代次数 $T$ 重复以下更新：\n$$\nu \\leftarrow \\frac{W v}{\\|W v\\|_2} \\quad \\text{if } \\|W v\\|_2 \\neq 0,\n$$\n$$\nv \\leftarrow \\frac{W^\\top u}{\\|W^\\top u\\|_2} \\quad \\text{if } \\|W^\\top u\\|_2 \\neq 0,\n$$\n最后报告估计值\n$$\n\\hat{\\sigma} = \\|W v\\|_2,\n$$\n在适当的条件下，该值近似于 $\\|W\\|_2$。如果在此过程中任一归一化分母变为零，则用一个适当维度的随机单位向量替换相应的向量并继续。\n\n仅从上述定义和迭代过程出发，编写一个完整的程序，该程序：\n- 实现所述的用于 $\\|W\\|_2$ 的幂迭代估计器，仅使用矩阵乘法和向量归一化。\n- 在需要随机单位向量时，使用以 $12345$ 为种子的可复现随机数生成器。\n- 为以下测试套件生成结果，该套件旨在覆盖一般行为和边界情况。在所有情况下，迭代次数 $T$ 都是整数，维度由正整数指定。\n\n测试套件：\n$1.$ 一般矩形情况（正常路径）：令 $W_1 \\in \\mathbb{R}^{5 \\times 3}$ 填充有独立的标准正态分布项，使用固定种子 $12345$ 生成。使用 $T = 50$。以浮点数形式输出估计的谱范数 $\\hat{\\sigma}_1$。\n\n$2.$ 单位矩阵边界情况：令 $W_2 = I_4 \\in \\mathbb{R}^{4 \\times 4}$。使用 $T = 10$。以浮点数形式输出估计的谱范数 $\\hat{\\sigma}_2$。\n\n$3.$ 零矩阵边界情况：令 $W_3 \\in \\mathbb{R}^{3 \\times 2}$ 为零矩阵。使用 $T = 10$。以浮点数形式输出估计的谱范数 $\\hat{\\sigma}_3$。\n\n$4.$ 对角已知值情况：令 $W_4 = \\mathrm{diag}(3,1,2) \\in \\mathbb{R}^{3 \\times 3}$。使用 $T = 10$。输出一个布尔值，指示估计的谱范数是否在 $3$ 的 $10^{-6}$ 绝对容差范围内，即输出\n$$\n\\left|\\hat{\\sigma}_4 - 3\\right| \\leq 10^{-6}.\n$$\n\n$5.$ 具有已知奇异值的构造矩形矩阵：将 $W_5 \\in \\mathbb{R}^{2 \\times 5}$ 构造为 $W_5 = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{2 \\times 2}$ 和 $V \\in \\mathbb{R}^{5 \\times 5}$ 是从随机高斯矩阵（种子 $12345$）的 $\\mathrm{QR}$ 分解中得到的标准正交矩阵，且\n$$\n\\Sigma = \\begin{bmatrix}\n4  0  0  0  0 \\\\\n0  1  0  0  0\n\\end{bmatrix}.\n$$\n使用 $T = 60$。以浮点数形式输出绝对差 $|\\hat{\\sigma}_5 - 4|$。\n\n$6.$ 标量乘法性质检查：令 $W_6 \\in \\mathbb{R}^{4 \\times 4}$ 为随机高斯矩阵（种子 $12345$），并令 $\\alpha = 0.5$。使用 $T = 60$。输出一个布尔值，指示是否\n$$\n\\left|\\hat{\\sigma}(\\alpha W_6) - |\\alpha| \\, \\hat{\\sigma}(W_6)\\right| \\leq 10^{-5},\n$$\n其中 $\\hat{\\sigma}(\\cdot)$ 表示算法的估计值。\n\n$7.$ 与向量的次可乘性检查：使用第一个测试中的 $W_1$，抽取一个随机向量 $x \\in \\mathbb{R}^{3}$（种子 $12345$），并仅在此性质检查中，使用奇异值分解（SVD）计算精确的 $\\|W_1\\|_2$ 以进行验证。输出一个布尔值，指示是否\n$$\n\\|W_1 x\\|_2 \\leq \\|W_1\\|_2 \\cdot \\|x\\|_2.\n$$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。该列表必须按测试 $1$ 到 $7$ 的顺序出现，并且每个元素的类型必须是指定的浮点数或布尔值。例如，您的输出应类似于\n$[\\hat{\\sigma}_1,\\hat{\\sigma}_2,\\hat{\\sigma}_3,\\text{bool}_4,\\text{float}_5,\\text{bool}_6,\\text{bool}_7]$。",
            "solution": "问题要求实现一个算法来估计实数矩阵 $W \\in \\mathbb{R}^{m \\times n}$ 的谱范数。谱范数，或矩阵 $2$-范数，定义为矩阵作用于任意单位向量上的最大拉伸因子：\n$$\n\\|W\\|_2 = \\sup_{\\|x\\|_2 = 1} \\|W x\\|_2\n$$\n该范数等价于矩阵的最大奇异值，记为 $\\sigma_{\\max}(W)$。所述算法是幂迭代法的一种变体，该方法旨在找到与最大模特征值对应的特征向量。\n\n通过考察矩阵 $A = W^\\top W \\in \\mathbb{R}^{n \\times n}$，可以理解该算法与谱范数之间的联系。该矩阵是对称半正定的。其特征值 $\\lambda_i(A)$ 是实数且非负的，它们通过 $\\lambda_i(W^\\top W) = \\sigma_i(W)^2$ 与 $W$ 的奇异值相关联。因此，$W^\\top W$ 的最大特征值是 $W$ 的最大奇异值的平方：$\\lambda_{\\max}(W^\\top W) = \\sigma_{\\max}(W)^2 = \\|W\\|_2^2$。\n\n当幂迭代法应用于 $A = W^\\top W$ 时，它会找到一个与 $\\lambda_{\\max}(A)$ 对应的特征向量 $v$。幂迭代法的迭代步骤是 $v_k \\propto A v_{k-1} = (W^\\top W) v_{k-1}$。让我们分析问题陈述中给出的过程：\n$1.$ $u_k \\leftarrow \\frac{W v_{k-1}}{\\|W v_{k-1}\\|_2}$\n$2.$ $v_k \\leftarrow \\frac{W^\\top u_k}{\\|W^\\top u_k\\|_2}$\n\n将 $u_k$ 的表达式代入 $v_k$ 的更新中：\n$$\nv_k \\propto W^\\top u_k \\propto W^\\top \\left( \\frac{W v_{k-1}}{\\|W v_{k-1}\\|_2} \\right) \\propto W^\\top W v_{k-1}\n$$\n这表明向量序列 $\\{v_k\\}$ 是通过将幂迭代法应用于矩阵 $W^\\top W$ 生成的。在适当的条件下（具体来说，初始向量 $v_0$ 在对应于 $\\lambda_{\\max}(W^\\top W)$ 的特征向量方向上具有非零分量，且 $\\lambda_{\\max}$ 严格大于其他特征值），向量 $v_k$ 收敛到与 $\\lambda_{\\max}$ 相关联的 $W^\\top W$ 的特征向量。这个特征向量是 $W$ 对应于 $\\sigma_{\\max}(W)$ 的右奇异向量。\n\n谱范数的最终估计值为 $\\hat{\\sigma} = \\|W v_T\\|_2$，其中 $v_T$ 是经过 $T$ 次迭代后的向量。当 $v_T$ 趋近于主导右奇异向量 $v_{\\max}$ 时，我们有：\n$$\n\\|W v_T\\|_2^2 = v_T^\\top W^\\top W v_T \\xrightarrow{k \\to \\infty} v_{\\max}^\\top (W^\\top W) v_{\\max} = v_{\\max}^\\top (\\lambda_{\\max} v_{\\max}) = \\lambda_{\\max} \\|v_{\\max}\\|_2^2 = \\lambda_{\\max}(W^\\top W)\n$$\n由于 $v_{\\max}$ 是一个单位向量，这得出 $\\hat{\\sigma}^2 \\to \\lambda_{\\max}(W^\\top W) = \\sigma_{\\max}(W)^2$。因此，估计值 $\\hat{\\sigma} = \\|W v_T\\|_2$ 收敛到 $\\sigma_{\\max}(W) = \\|W\\|_2$。\n\n算法必须处理归一化因子为零的情况。如果 $\\|W v\\|_2 = 0$，这意味着 $Wv=0$，所以 $v$ 在 $W$ 的零空间中。如果 $\\|W^\\top u\\|_2 = 0$，则 $u$ 在 $W^\\top$ 的零空间中。问题规定，在这种情况下，待归一化的向量（即 $u$ 或 $v$）应被替换为适当维度的随机单位向量。这可以防止迭代卡在零向量上。对于实现中的所有随机方面，必须使用一个以 $12345$ 为种子的可复现随机数生成器。\n\n现在，解决方案将通过实现此算法并将其应用于七个指定的测试用例来继续。\n\n$1.$ **一般矩形情况**：从标准正态分布生成一个随机矩阵 $W_1 \\in \\mathbb{R}^{5 \\times 3}$。这是“正常路径”情况，其中奇异值互不相同且非零，幂迭代法预计会良好收敛。我们运行迭代 $T=50$ 次并报告估计值 $\\hat{\\sigma}_1$。\n\n$2.$ **单位矩阵边界情况**：对于 $W_2 = I_4 \\in \\mathbb{R}^{4 \\times 4}$，谱范数已知为 $\\|I_4\\|_2 = 1$。对于任何单位向量 $v$，$\\|I_4 v\\|_2 = \\|v\\|_2 = 1$。算法将计算 $u = \\frac{Iv}{\\|Iv\\|_2} = \\frac{v}{1} = v$，然后 $v_{new} = \\frac{I^\\top u}{\\|I^\\top u\\|_2} = \\frac{I v}{\\|I v\\|_2} = v$。向量不会改变，最终的估计值 $\\|I_4 v\\|_2$ 将是 $1$，无论初始向量是什么。结果 $\\hat{\\sigma}_2$ 应该为 $1.0$。\n\n$3.$ **零矩阵边界情况**：对于 $W_3=0 \\in \\mathbb{R}^{3 \\times 2}$，谱范数为 $\\|0\\|_2 = 0$。对于任何向量 $v$，$W_3 v = 0$，所以其范数为 $0$。根据指定的过程，$u$ 被重置为一个随机单位向量。然后，$W_3^\\top u = 0$，所以其范数也为 $0$。向量 $v$ 接着被重置为一个随机单位向量。这个过程重复 $T=10$ 次迭代。最终的估计值是 $\\hat{\\sigma}_3 = \\|W_3 v_T\\|_2 = \\|0 \\cdot v_T\\|_2 = 0$。结果应为 $0.0$。\n\n$4.$ **对角已知值情况**：对于对角矩阵 $W_4 = \\mathrm{diag}(3,1,2)$，谱范数是其对角线元素绝对值的最大值，即 $\\max(|3|, |1|, |2|) = 3$。预计算法将收敛到这个值。测试检查估计值 $\\hat{\\sigma}_4$ 是否在真值 $3$ 的 $10^{-6}$ 容差范围内。\n\n$5.$ **构造的矩形矩阵**：一个矩阵 $W_5 \\in \\mathbb{R}^{2 \\times 5}$ 由其奇异值分解 (SVD) $W_5 = U \\Sigma V^\\top$ 构造而成。矩阵 $U$ 和 $V$ 由随机矩阵生成为标准正交基，以确保一般性。矩阵 $\\Sigma$ 明确地将奇异值设置为 $4$ 和 $1$。谱范数是最大的奇异值，所以 $\\|W_5\\|_2 = 4$。测试计算估计值 $\\hat{\\sigma}_5$ 与真值 $4$ 之间的绝对差。\n\n$6.$ **标量乘法性质检查**：该测试验证范数的绝对齐次性，即 $\\|\\alpha W\\|_2 = |\\alpha| \\|W\\|_2$。算法在随机矩阵 $W_6$ 和 $\\alpha W_6$（其中 $\\alpha=0.5$）上运行。测试验证估计值是否遵循此性质，即 $|\\hat{\\sigma}(\\alpha W_6) - |\\alpha| \\hat{\\sigma}(W_6)| \\leq 10^{-5}$。我们上面的理论分析表明，在浮点精度范围内，这应该成立，因为归一化步骤会抵消标量因子 $|\\alpha|$。\n\n$7.$ **次可乘性检查**：该测试验证诱导算子范数的基本定义：对于任何向量 $x$，$\\|W x\\|_2 \\leq \\|W\\|_2 \\|x\\|_2$。该检查使用第一个测试中的矩阵 $W_1$ 和一个新生成的随机向量 $x$ 来执行。关键是，问题要求使用通过标准 SVD 库函数计算的*精确*谱范数 $\\|W_1\\|_2$ 进行此验证，而不是算法的估计值。这个不等式是一个数学上的事实，所以测试结果应为 `True`，作为对概念及其实现的理解的合理性检查。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    # Master random number generator for reproducibility across all tests.\n    rng = np.random.default_rng(12345)\n    \n    # This is the core algorithm as described in the problem.\n    def estimate_spectral_norm(W, T, prng):\n        \"\"\"\n        Estimates the spectral norm of a matrix W using power iteration.\n\n        Args:\n            W (np.ndarray): The matrix for which to estimate the spectral norm.\n            T (int): The number of power iterations.\n            prng (np.random.Generator): A seeded random number generator.\n\n        Returns:\n            float: The estimated spectral norm of W.\n        \"\"\"\n        m, n = W.shape\n        \n        # Initialize v with a random unit vector.\n        v = prng.standard_normal(size=n)\n        v = v / np.linalg.norm(v)\n\n        for _ in range(T):\n            # Update u\n            Wv = W @ v\n            norm_Wv = np.linalg.norm(Wv)\n            if norm_Wv == 0:\n                u = prng.standard_normal(size=m)\n                u = u / np.linalg.norm(u)\n            else:\n                u = Wv / norm_Wv\n            \n            # Update v\n            Wtu = W.T @ u\n            norm_Wtu = np.linalg.norm(Wtu)\n            if norm_Wtu == 0:\n                v = prng.standard_normal(size=n)\n                v = v / np.linalg.norm(v)\n            else:\n                v = Wtu / norm_Wtu\n        \n        # Final estimate\n        sigma_hat = np.linalg.norm(W @ v)\n        return sigma_hat\n\n    results = []\n\n    # Test case 1: General rectangular case\n    W1 = rng.standard_normal(size=(5, 3))\n    sigma1_hat = estimate_spectral_norm(W1, 50, rng)\n    results.append(sigma1_hat)\n\n    # Test case 2: Identity boundary case\n    W2 = np.identity(4)\n    sigma2_hat = estimate_spectral_norm(W2, 10, rng)\n    results.append(sigma2_hat)\n\n    # Test case 3: Zero boundary case\n    W3 = np.zeros((3, 2))\n    sigma3_hat = estimate_spectral_norm(W3, 10, rng)\n    results.append(sigma3_hat)\n\n    # Test case 4: Diagonal known-values case\n    W4 = np.diag([3, 1, 2])\n    sigma4_hat = estimate_spectral_norm(W4, 10, rng)\n    bool4 = np.abs(sigma4_hat - 3) = 1e-6\n    results.append(bool4)\n\n    # Test case 5: Constructed rectangular matrix with known singular values\n    A_U = rng.standard_normal(size=(2, 2))\n    U, _ = np.linalg.qr(A_U)\n    A_V = rng.standard_normal(size=(5, 5))\n    V, _ = np.linalg.qr(A_V)\n    Sigma = np.zeros((2, 5))\n    Sigma[0, 0] = 4\n    Sigma[1, 1] = 1\n    W5 = U @ Sigma @ V.T\n    sigma5_hat = estimate_spectral_norm(W5, 60, rng)\n    float5 = np.abs(sigma5_hat - 4)\n    results.append(float5)\n\n    # Test case 6: Scalar multiplication property check\n    W6 = rng.standard_normal(size=(4, 4))\n    alpha = 0.5\n    sigma_W6_hat = estimate_spectral_norm(W6, 60, rng)\n    sigma_alphaW6_hat = estimate_spectral_norm(alpha * W6, 60, rng)\n    bool6 = np.abs(sigma_alphaW6_hat - np.abs(alpha) * sigma_W6_hat) = 1e-5\n    results.append(bool6)\n    \n    # Test case 7: Submultiplicativity check with a vector\n    # Using W1 from test case 1.\n    x = rng.standard_normal(size=3)\n    norm_W1x = np.linalg.norm(W1 @ x)\n    norm_x = np.linalg.norm(x)\n    # Compute the exact spectral norm using SVD\n    singular_values_W1 = np.linalg.svd(W1, compute_uv=False)\n    true_norm_W1 = singular_values_W1[0]\n    bool7 = norm_W1x = true_norm_W1 * norm_x\n    results.append(bool7)\n\n    # Final print statement in the exact required format.\n    # The map(str,...) converts each element (float, bool) to its string representation\n    # e.g., True becomes \"True\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}