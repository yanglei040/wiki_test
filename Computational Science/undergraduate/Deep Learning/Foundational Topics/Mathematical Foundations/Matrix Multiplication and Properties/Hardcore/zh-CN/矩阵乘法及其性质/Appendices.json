{
    "hands_on_practices": [
        {
            "introduction": "深入研究神经网络的结构时，理解线性层之间如何相互作用至关重要。本练习通过分析两个线性层何时可交换（即 $W_1 W_2 = W_2 W_1$），揭示了网络中潜在的函数冗余，并阐明了非线性激活函数在打破这种冗余、构建复杂模型中的关键作用 。这个思想实验有助于我们从根本上理解为什么深度（而非仅仅是宽度）和非线性是构建强大神经网络的基石。",
            "id": "3148079",
            "problem": "考虑一个前馈神经网络中的两个线性层，由实$3 \\times 3$矩阵$W_1$和$W_2$表示。您将通过展示一个共享特征基（同时对角化）来分析这些层何时交换，并解释这对于层间有和没有非线性函数的网络中的功能冗余意味着什么。仅使用矩阵乘法、特征值和特征向量以及非线性函数的逐元素作用的基本定义。\n\n设$W_1$和$W_2$由下式给出\n$$\nW_1 = \\begin{pmatrix}\na  0  0 \\\\\n0  b  0 \\\\\n0  0  c\n\\end{pmatrix},\n\\qquad\nW_2 = \\begin{pmatrix}\nd  0  0 \\\\\n0  e  0 \\\\\n0  0  f\n\\end{pmatrix},\n$$\n其中$a,b,c,d,e,f \\in \\mathbb{R}$。这些矩阵在标准基中已经是​​对角矩阵，该标准基是一个候选的共享特征基。\n\n首先，通过确定一个共享特征基并解释同时对角化如何强制实现交换性，来构建两个实矩阵$W_1$和$W_2$交换的充分条件。您的构建必须基于特征值和特征向量的定义，以及对角矩阵的性质，而不假设任何未从这些定义推导出的特定快捷定理。\n\n接下来，考虑两个线性层之间没有任何非线性函数的复合，其作用为$x \\mapsto W_2 W_1 x$，其中$x \\in \\mathbb{R}^3$。解释在这种线性情况下的功能冗余概念，并对于上面给出的特定$W_1$和$W_2$，计算标量$\\operatorname{tr}(W_2 W_1)$，作为$a,b,c,d,e,f$的闭式解析表达式。\n\n最后，在层之间插入一个整流线性单元（ReLU）非线性函数，定义为逐元素映射$\\sigma(z) = \\max\\{0,z\\}$，形成$x \\mapsto W_2 \\,\\sigma(W_1 x)$。在共享特征基（这里是标准基）中，给出在该输入$x$下，这种复合简化为单一线性映射的条件，并解释这对于存在非线性函数时两层的功能冗余意味着什么。您无需为此部分计算任何数值。\n\n您的最终答案必须是$\\operatorname{tr}(W_2 W_1)$关于$a,b,c,d,e,f$的单一闭式表达式。",
            "solution": "我们从基本定义开始。由矩阵$W_1$和$W_2$表示的两个线性算子交换，当且仅当$W_1 W_2 = W_2 W_1$。一个矩阵$W$是可对角化的，如果存在一个可逆矩阵$S$使得$S^{-1} W S = D$，其中$D$是一个对角矩阵。对角矩阵$D$的性质是，$D u$独立地缩放向量$u$的每个坐标，并且对角矩阵是可交换的，因为对角矩阵的乘法是逐项的，因此在相应对角线元素的层面上是结合的和可交换的。\n\n通过同时对角化构建交换的充分条件的过程如下。假设$W_1$和$W_2$是可对角化的，并共享一个完备的特征向量集；也就是说，存在一个可逆矩阵$S$，其列构成了$W_1$和$W_2$共有的特征向量基。那么我们可以写出\n$$\nS^{-1} W_1 S = D_1, \\qquad S^{-1} W_2 S = D_2,\n$$\n其中$D_1$和$D_2$是由$W_1$和$W_2$相对于共享特征基的特征值组成的对角矩阵。考虑其乘积：\n$$\nW_1 W_2 = S D_1 S^{-1} S D_2 S^{-1} = S (D_1 D_2) S^{-1},\n$$\n和\n$$\nW_2 W_1 = S D_2 S^{-1} S D_1 S^{-1} = S (D_2 D_1) S^{-1}.\n$$\n因为对角矩阵是可交换的，所以我们有$D_1 D_2 = D_2 D_1$，这直接意味着$W_1 W_2 = W_2 W_1$。因此，$W_1$和$W_2$交换的一个充分条件是它们可以被同一个可逆矩阵$S$同时对角化（等价地，它们共享一个完备的特征基）。特别是，如果$W_1$和$W_2$在同一基中都是对角矩阵，那么它们是可交换的。\n\n对于给定的特定矩阵，\n$$\nW_1 = \\begin{pmatrix}\na  0  0 \\\\\n0  b  0 \\\\\n0  0  c\n\\end{pmatrix},\n\\qquad\nW_2 = \\begin{pmatrix}\nd  0  0 \\\\\n0  e  0 \\\\\n0  0  f\n\\end{pmatrix},\n$$\n标准基向量是它们的共同特征向量，并且两个矩阵在该基中都是对角矩阵。因此，$W_1$和$W_2$是可交换的。没有非线性函数的复合是线性算子$W_2 W_1$，它也是一个对角矩阵，其对角线元素由相应元素的乘积给出：\n$$\nW_2 W_1 = \\begin{pmatrix}\nad  0  0 \\\\\n0  be  0 \\\\\n0  0  cf\n\\end{pmatrix}.\n$$\n在这种线性情况下，功能冗余意味着这个两层线性子网络$x \\mapsto W_2 W_1 x$可以被一个单一线性层$x \\mapsto W_{\\text{eff}} x$替代，其中$W_{\\text{eff}} = W_2 W_1$，而不会改变输入输出映射。为了计算所要求的标量，\n$$\n\\operatorname{tr}(W_2 W_1) = ad + be + cf,\n$$\n根据迹的定义，即对角线元素之和。\n\n现在考虑在层之间插入一个整流线性单元（ReLU）非线性函数，形成映射$x \\mapsto W_2 \\,\\sigma(W_1 x)$，其中$\\sigma$是逐元素应用的：$\\sigma(z) = \\max\\{0,z\\}$。因为$W_1$是对角矩阵，\n$$\nW_1 x = \\begin{pmatrix}\na x_1 \\\\ b x_2 \\\\ c x_3\n\\end{pmatrix}, \\qquad \\sigma(W_1 x) = \\begin{pmatrix}\n\\max\\{0, a x_1\\} \\\\ \\max\\{0, b x_2\\} \\\\ \\max\\{0, c x_3\\}\n\\end{pmatrix}.\n$$\n随后乘以$W_2$会将$\\sigma(W_1 x)$的每个坐标分别乘以$d$、$e$和$f$。复合映射$x \\mapsto W_2 \\,\\sigma(W_1 x)$在特定输入$x$上简化为单一线性映射$x \\mapsto W_2 W_1 x$的条件是，当且仅当$\\sigma$对$W_1 x$的作用是恒等的，这发生在$W_1 x$的每个坐标都为非负数时：\n$$\na x_1 \\ge 0, \\quad b x_2 \\ge 0, \\quad c x_3 \\ge 0.\n$$\n在这些条件下，$\\sigma(W_1 x) = W_1 x$，因此$W_2 \\,\\sigma(W_1 x) = W_2 W_1 x$，非线性的存在不会改变函数在该输入上的作用；对于这样的$x$，该两层结构是功能冗余的。如果这些逐坐标的不等式中有任何一个不成立，那么至少有一个分量会被ReLU截断为零，并且整体映射不能被一个对所有输入都固定的单一线性算子表示——功能冗余被非线性函数破坏了。特别是，全局冗余（对所有$x$）将要求输入域被限制在满足$a x_1 \\ge 0$、$b x_2 \\ge 0$和$c x_3 \\ge 0$的集合上，或者$a$、$b$、$c$的符号和输入坐标的符号需要对齐，以便ReLU在感兴趣的域中处处充当恒等函数。\n\n因此，所要求的线性情况下的标量表达式是上面计算的迹。",
            "answer": "$$\\boxed{ad + be + cf}$$"
        },
        {
            "introduction": "在大规模深度学习中，矩阵乘法的应用远不止于前向传播。本练习将矩阵向量乘法作为一个分析工具，用于模拟分布式数据并行训练中的梯度聚合过程 。通过将所有局部梯度之和建模为单个矩阵与全一向量的乘积，我们可以运用通信模型来精确推导在分布式系统中计算全局梯度所需的最小时间，这对于设计高效的训练算法至关重要。",
            "id": "3148022",
            "problem": "在一个有 $p$ 个工作节点的深度神经网络的同步数据并行训练中，每个工作节点 $i \\in \\{1,\\dots,p\\}$ 为当前的小批量计算一个局部梯度向量 $g_i \\in \\mathbb{R}^{d}$。将这些局部梯度按列堆叠，形成矩阵 $G \\in \\mathbb{R}^{d \\times p}$，即 $G = \\begin{bmatrix} g_1  \\cdots  g_p \\end{bmatrix}$。要应用于模型参数的全局梯度是列向和 $g \\in \\mathbb{R}^{d}$，它等于矩阵向量乘积 $g = G \\mathbf{1}_p$，其中 $\\mathbf{1}_p \\in \\mathbb{R}^{p}$ 表示全1向量。\n\n您将分析在一个指定的根工作节点上，使用一个遵循基本矩阵乘法结构和向量加法结合律的平衡归约协议来计算 $g$ 所需的最小通信时间。假设如下：\n\n- 唯一重要的成本是通信；本地算术（包括实现归约所需的向量加法）与通信相比，时间可忽略不计。\n- 通信遵循标准的 $\\alpha$–$\\beta$ 模型：发送一个包含 $k$ 个实数的消息所需时间为 $T(k) = \\alpha + k \\beta$，其中 $\\alpha  0$ 是延迟项，$\\beta  0$ 是每个数值的传输时间。\n- 归约在一个具有 $p$ 个叶节点和一个指定根节点的完美平衡二叉树上执行。假设 $p$ 是 $2$ 的幂，链路是全双工且无竞争的，并且在同一树层级上的所有消息交换都并发进行。每一层的每条消息都恰好包含要相加的整个向量，这与操作 $G \\mathbf{1}_p$ 的线性以及加法的结合律相一致。\n\n仅使用矩阵乘法的定义和上述的 $\\alpha$–$\\beta$ 模型，推导在此平衡二叉树归约下，在根节点计算 $g = G \\mathbf{1}_p$ 所需总通信时间的闭式表达式。\n\n以 $p$、$d$、$\\alpha$ 和 $\\beta$ 的单个闭式符号表达式形式提供您的最终答案。不要包含单位，也不要进行近似或四舍五入。",
            "solution": "我们从矩阵乘法的定义和 $G \\mathbf{1}_p$ 的结构开始。设 $G = \\begin{bmatrix} g_1  \\cdots  g_p \\end{bmatrix} \\in \\mathbb{R}^{d \\times p}$ 且 $\\mathbf{1}_p \\in \\mathbb{R}^{p}$ 为全1向量。根据矩阵向量乘法的定义，乘积 $g = G \\mathbf{1}_p \\in \\mathbb{R}^{d}$ 得出\n$$\ng = \\sum_{i=1}^{p} g_i,\n$$\n也就是说，$g$ 的每个分量是 $G$ 的 $p$ 个列中对应分量的总和。这依赖于矩阵乘法对每一列的线性，以及乘以 $\\mathbf{1}_p$ 会使每一列 $g_i$ 的权重为 $1$ 这一事实。\n\n为了在分布式环境中实现这一计算，我们使用平衡二叉树归约。实现树归约的核心代数性质是向量加法的结合律和交换律：对于向量 $u,v,w \\in \\mathbb{R}^{d}$，有 $(u+v)+w = u+(v+w)$ 和 $u+v = v+u$。因此，我们可以将求和 $\\sum_{i=1}^{p} g_i$ 构造为一系列按二叉树层级排列的成对加法，而不会改变结果，这与 $G \\mathbf{1}_p$ 的概念性计算相匹配。\n\n现在考虑通信模型。树的每条边上的成对组合都需要在两个工作节点之间通信一个长度为 $d$ 的完整向量，以便其中一个节点可以将接收到的向量加到自己的部分和上。在 $\\alpha$–$\\beta$ 模型下，发送 $k$ 个实数耗时 $T(k) = \\alpha + k \\beta$。在这里，对于每条消息，$k=d$，因为每条消息都携带一个完整的 $d$ 维向量。\n\n在一个有 $p$ 个叶节点（其中 $p$ 是 $2$ 的幂）的平衡二叉树中，恰好有 $\\log_{2}(p)$ 个层级。在每个层级，工作节点被配对，在每对中，一个长度为 $d$ 的向量通过链路发送，并在接收端进行相加。因为假设链路是全双工且无竞争的，并且同一层级的所有配对都并发通信，所以整个层级的时间是单条消息的时间：$\\alpha + d \\beta$。因此，所有层级的总时间是 $\\log_{2}(p)$ 个层级的每层时间之和：\n$$\nT_{\\text{total}} \\;=\\; \\underbrace{\\log_{2}(p)}_{\\text{层级数}} \\times \\underbrace{(\\alpha + d \\beta)}_{\\text{每层时间}} \\;=\\; (\\alpha + d \\beta)\\,\\log_{2}(p).\n$$\n\n因此，在所述假设和 $\\alpha$–$\\beta$ 模型下，通过平衡二叉树归约在根节点计算 $g = G \\mathbf{1}_p$ 的最小通信时间为闭式表达式\n$$\n(\\alpha + d \\beta)\\,\\log_{2}(p).\n$$",
            "answer": "$$\\boxed{(\\alpha + d \\beta)\\,\\log_{2}(p)}$$"
        },
        {
            "introduction": "矩阵的谱范数是衡量其“大小”或“影响力”的一个核心指标，对分析模型的稳定性和泛化能力具有重要意义。这个动手编码练习要求你仅使用基本的矩阵乘法操作，从头开始实现一个经典的数值算法——幂迭代法——来估算矩阵的谱范数 。通过完成这个练习，你不仅能掌握一个实用的算法，还能加深对矩阵范数、奇异值和矩阵乘法之间内在联系的理解。",
            "id": "3148029",
            "problem": "给定一个实数矩阵 $W \\in \\mathbb{R}^{m \\times n}$，它代表深度学习中一个线性层的权重矩阵。目标是仅使用矩阵乘法及其性质，并基于基本定义，实现一个估算谱范数（矩阵算子2-范数）$\\|W\\|_2$ 的算法。谱范数从第一性原理定义为\n$$\n\\|W\\|_2 = \\sup_{\\|x\\|_2 = 1} \\|W x\\|_2,\n$$\n其中 $\\|\\cdot\\|_2$ 表示向量的欧几里得范数。估算 $\\|W\\|_2$ 的迭代过程依赖以下步骤：初始化一个单位向量 $v \\in \\mathbb{R}^{n}$，然后对指定的迭代次数 $T$ 重复以下更新：\n$$\nu \\leftarrow \\frac{W v}{\\|W v\\|_2} \\quad \\text{if } \\|W v\\|_2 \\neq 0,\n$$\n$$\nv \\leftarrow \\frac{W^\\top u}{\\|W^\\top u\\|_2} \\quad \\text{if } \\|W^\\top u\\|_2 \\neq 0,\n$$\n最后报告估计值\n$$\n\\hat{\\sigma} = \\|W v\\|_2,\n$$\n该估计值在适当条件下会近似于 $\\|W\\|_2$。如果在过程中任一归一化分母变为零，则用一个适当维度的随机单位向量替换相应的向量并继续。\n\n仅从上述定义和迭代过程出发，编写一个完整的程序，该程序：\n- 仅使用矩阵乘法和向量归一化，实现所述的用于估算 $\\|W\\|_2$ 的幂迭代估计器。\n- 在需要随机单位向量时，使用以 $12345$ 为种子的可复现随机数生成器。\n- 为以下测试套件生成结果，该套件旨在覆盖一般行为和边界情况。在所有情况下，迭代次数 $T$ 均为整数，维度由正整数指定。\n\n测试套件：\n$1.$ 一般矩形情况（理想路径）：设 $W_1 \\in \\mathbb{R}^{5 \\times 3}$ 填充有独立的标准正态分布条目，使用固定种子 $12345$ 生成。使用 $T = 50$。以浮点数形式输出估计的谱范数 $\\hat{\\sigma}_1$。\n\n$2.$ 单位矩阵边界情况：设 $W_2 = I_4 \\in \\mathbb{R}^{4 \\times 4}$。使用 $T = 10$。以浮点数形式输出估计的谱范数 $\\hat{\\sigma}_2$。\n\n$3.$ 零矩阵边界情况：设 $W_3 \\in \\mathbb{R}^{3 \\times 2}$ 为零矩阵。使用 $T = 10$。以浮点数形式输出估计的谱范数 $\\hat{\\sigma}_3$。\n\n$4.$ 对角已知值情况：设 $W_4 = \\mathrm{diag}(3,1,2) \\in \\mathbb{R}^{3 \\times 3}$。使用 $T = 10$。输出一个布尔值，指示估计的谱范数与 $3$ 的绝对容差是否在 $10^{-6}$ 之内，即输出\n$$\n\\left|\\hat{\\sigma}_4 - 3\\right| \\leq 10^{-6}.\n$$\n\n$5.$ 具有已知奇异值的构造矩形矩阵：构造 $W_5 \\in \\mathbb{R}^{2 \\times 5}$ 为 $W_5 = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{2 \\times 2}$ 和 $V \\in \\mathbb{R}^{5 \\times 5}$ 是通过对随机高斯矩阵（种子 $12345$）进行 $\\mathrm{QR}$ 分解得到的正交矩阵，且\n$$\n\\Sigma = \\begin{bmatrix}\n4  0  0  0  0 \\\\\n0  1  0  0  0\n\\end{bmatrix}.\n$$\n使用 $T = 60$。以浮点数形式输出绝对差 $|\\hat{\\sigma}_5 - 4|$。\n\n$6.$ 标量乘法性质检验：设 $W_6 \\in \\mathbb{R}^{4 \\times 4}$ 为随机高斯矩阵（种子 $12345$），并设 $\\alpha = 0.5$。使用 $T = 60$。输出一个布尔值，指示是否\n$$\n\\left|\\hat{\\sigma}(\\alpha W_6) - |\\alpha| \\, \\hat{\\sigma}(W_6)\\right| \\leq 10^{-5},\n$$\n其中 $\\hat{\\sigma}(\\cdot)$ 表示算法的估计值。\n\n$7.$ 与向量的次可乘性检验：使用第一个测试中的相同矩阵 $W_1$，抽取一个随机向量 $x \\in \\mathbb{R}^{3}$（种子 $12345$），并仅在此性质检验中为了验证而使用奇异值分解（SVD, singular value decomposition）计算精确的 $\\|W_1\\|_2$。输出一个布尔值，指示是否\n$$\n\\|W_1 x\\|_2 \\leq \\|W_1\\|_2 \\cdot \\|x\\|_2.\n$$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。该列表必须按测试 1 到 7 的顺序显示，并且每个元素的类型必须是指定的浮点数或布尔值。例如，您的输出应如下所示\n$[\\hat{\\sigma}_1,\\hat{\\sigma}_2,\\hat{\\sigma}_3,\\text{bool}_4,\\text{float}_5,\\text{bool}_6,\\text{bool}_7]$。",
            "solution": "该问题要求实现一个算法来估计实数矩阵 $W \\in \\mathbb{R}^{m \\times n}$ 的谱范数。谱范数，即矩阵2-范数，定义为矩阵对任意单位向量施加的最大拉伸因子：\n$$\n\\|W\\|_2 = \\sup_{\\|x\\|_2 = 1} \\|W x\\|_2\n$$\n这个范数等价于矩阵的最大奇异值，记为 $\\sigma_{\\max}(W)$。所规定的算法是幂迭代法的一种变体，该方法旨在寻找与最大模特征值对应的特征向量。\n\n通过检验矩阵 $A = W^\\top W \\in \\mathbb{R}^{n \\times n}$，可以理解该算法与谱范数之间的联系。该矩阵是对称半正定的。其特征值 $\\lambda_i(A)$ 是实数且非负，并且它们通过 $\\lambda_i(W^\\top W) = \\sigma_i(W)^2$ 与 $W$ 的奇异值相关联。因此，$W^\\top W$ 的最大特征值是 $W$ 的最大奇异值的平方：$\\lambda_{\\max}(W^\\top W) = \\sigma_{\\max}(W)^2 = \\|W\\|_2^2$。\n\n当幂迭代法应用于 $A = W^\\top W$ 时，它会找到对应于 $\\lambda_{\\max}(A)$ 的特征向量 $v$。幂迭代法的迭代步骤是 $v_k \\propto A v_{k-1} = (W^\\top W) v_{k-1}$。让我们分析问题描述中给出的过程：\n$1.$ $u_k \\leftarrow \\frac{W v_{k-1}}{\\|W v_{k-1}\\|_2}$\n$2.$ $v_k \\leftarrow \\frac{W^\\top u_k}{\\|W^\\top u_k\\|_2}$\n\n将 $u_k$ 的表达式代入 $v_k$ 的更新公式中：\n$$\nv_k \\propto W^\\top u_k \\propto W^\\top \\left( \\frac{W v_{k-1}}{\\|W v_{k-1}\\|_2} \\right) \\propto W^\\top W v_{k-1}\n$$\n这表明向量序列 $\\{v_k\\}$ 是通过将幂迭代法应用于矩阵 $W^\\top W$ 生成的。在适当的条件下（具体来说，初始向量 $v_0$ 在对应于 $\\lambda_{\\max}(W^\\top W)$ 的特征向量方向上具有非零分量，且 $\\lambda_{\\max}$ 严格大于其他特征值），向量 $v_k$ 收敛于与 $\\lambda_{\\max}$ 相关的 $W^\\top W$ 的特征向量。这个特征向量是 $W$ 对应于 $\\sigma_{\\max}(W)$ 的右奇异向量。\n\n谱范数的最终估计值为 $\\hat{\\sigma} = \\|W v_T\\|_2$，其中 $v_T$ 是经过 $T$ 次迭代后的向量。当 $v_T$ 趋近于主导右奇异向量 $v_{\\max}$ 时，我们有：\n$$\n\\|W v_T\\|_2^2 = v_T^\\top W^\\top W v_T \\xrightarrow{k \\to \\infty} v_{\\max}^\\top (W^\\top W) v_{\\max} = v_{\\max}^\\top (\\lambda_{\\max} v_{\\max}) = \\lambda_{\\max} \\|v_{\\max}\\|_2^2 = \\lambda_{\\max}(W^\\top W)\n$$\n由于 $v_{\\max}$ 是一个单位向量，这得出 $\\hat{\\sigma}^2 \\to \\lambda_{\\max}(W^\\top W) = \\sigma_{\\max}(W)^2$。因此，估计值 $\\hat{\\sigma} = \\|W v_T\\|_2$ 收敛于 $\\sigma_{\\max}(W) = \\|W\\|_2$。\n\n该算法必须处理归一化因子为零的情况。如果 $\\|W v\\|_2 = 0$，则意味着 $Wv=0$，所以 $v$ 在 $W$ 的零空间中。如果 $\\|W^\\top u\\|_2 = 0$，则 $u$ 在 $W^\\top$ 的零空间中。问题规定，在这种情况下，待归一化的向量（即 $u$ 或 $v$）应被替换为适当维度的随机单位向量。这可以防止迭代陷入零向量。实现的所有随机方面都必须使用以 $12345$ 为种子的单一可复现随机数生成器。\n\n现在，解决方案将继续实现此算法，并将其应用于七个指定的测试用例。\n\n$1.$ **一般矩形情况**：从标准正态分布生成一个随机矩阵 $W_1 \\in \\mathbb{R}^{5 \\times 3}$。这是“理想路径”情况，其中奇异值是不同且非零的，幂迭代法有望良好收敛。我们运行迭代 $T=50$ 次并报告估计值 $\\hat{\\sigma}_1$。\n\n$2.$ **单位矩阵边界情况**：对于 $W_2 = I_4 \\in \\mathbb{R}^{4 \\times 4}$，已知其谱范数为 $\\|I_4\\|_2 = 1$。对于任何单位向量 $v$，$\\|I_4 v\\|_2 = \\|v\\|_2 = 1$。算法将计算 $u = \\frac{Iv}{\\|Iv\\|_2} = \\frac{v}{1} = v$，然后 $v_{new} = \\frac{I^\\top u}{\\|I^\\top u\\|_2} = \\frac{I v}{\\|I v\\|_2} = v$。向量不发生改变，最终的估计值 $\\|I_4 v\\|_2$ 将为 $1$，与初始向量无关。结果 $\\hat{\\sigma}_2$ 应为 $1.0$。\n\n$3.$ **零矩阵边界情况**：对于 $W_3=0 \\in \\mathbb{R}^{3 \\times 2}$，其谱范数为 $\\|0\\|_2 = 0$。对于任何向量 $v$，$W_3 v = 0$，所以其范数为 $0$。根据指定的过程，$u$ 被重置为一个随机单位向量。然后，$W_3^\\top u = 0$，所以其范数也为 $0$。向量 $v$ 接着被重置为一个随机单位向量。这个过程重复 $T=10$ 次迭代。最终的估计值为 $\\hat{\\sigma}_3 = \\|W_3 v_T\\|_2 = \\|0 \\cdot v_T\\|_2 = 0$。结果应为 $0.0$。\n\n$4.$ **对角已知值情况**：对于对角矩阵 $W_4 = \\mathrm{diag}(3,1,2)$，其谱范数是对角元素绝对值的最大值，即 $\\max(|3|, |1|, |2|) = 3$。算法预计将收敛到这个值。测试检验估计值 $\\hat{\\sigma}_4$ 是否在真实值 $3$ 的 $10^{-6}$ 容差范围内。\n\n$5.$ **构造的矩形矩阵**：矩阵 $W_5 \\in \\mathbb{R}^{2 \\times 5}$ 是通过其奇异值分解（SVD）$W_5 = U \\Sigma V^\\top$ 构造的。矩阵 $U$ 和 $V$ 是通过对随机矩阵进行标准正交化生成，以确保一般性。矩阵 $\\Sigma$ 明确地将奇异值设置为 $4$ 和 $1$。谱范数是最大的奇异值，所以 $\\|W_5\\|_2 = 4$。测试计算估计值 $\\hat{\\sigma}_5$ 与真实值 $4$ 之间的绝对差。\n\n$6.$ **标量乘法性质检验**：此测试验证范数的绝对齐次性，即 $\\|\\alpha W\\|_2 = |\\alpha| \\|W\\|_2$。算法在一个随机矩阵 $W_6$ 和 $\\alpha W_6$（其中 $\\alpha=0.5$）上运行。测试验证估计值是否遵循此性质，即 $|\\hat{\\sigma}(\\alpha W_6) - |\\alpha| \\hat{\\sigma}(W_6)| \\leq 10^{-5}$。我们上面的理论分析表明，在浮点精度范围内，这应该是成立的，因为归一化步骤会抵消标量因子 $|\\alpha|$。\n\n$7.$ **次可乘性检验**：此测试验证了诱导算子范数的基本定义：对于任何向量 $x$，$\\|W x\\|_2 \\leq \\|W\\|_2 \\|x\\|_2$。该检验使用第一个测试中的矩阵 $W_1$ 和一个新生成的随机向量 $x$ 来执行。关键的是，问题要求在此验证中使用通过标准 SVD 库函数计算的*精确*谱范数 $\\|W_1\\|_2$，而不是算法的估计值。这个不等式是一个数学真理，因此测试结果应为 `True`，作为对概念理解及其实施的合理性检查。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    # Master random number generator for reproducibility across all tests.\n    rng = np.random.default_rng(12345)\n    \n    # This is the core algorithm as described in the problem.\n    def estimate_spectral_norm(W, T, prng):\n        \"\"\"\n        Estimates the spectral norm of a matrix W using power iteration.\n\n        Args:\n            W (np.ndarray): The matrix for which to estimate the spectral norm.\n            T (int): The number of power iterations.\n            prng (np.random.Generator): A seeded random number generator.\n\n        Returns:\n            float: The estimated spectral norm of W.\n        \"\"\"\n        m, n = W.shape\n        \n        # Initialize v with a random unit vector.\n        v = prng.standard_normal(size=n)\n        v = v / np.linalg.norm(v)\n\n        for _ in range(T):\n            # Update u\n            Wv = W @ v\n            norm_Wv = np.linalg.norm(Wv)\n            if norm_Wv == 0:\n                u = prng.standard_normal(size=m)\n                u = u / np.linalg.norm(u)\n            else:\n                u = Wv / norm_Wv\n            \n            # Update v\n            Wtu = W.T @ u\n            norm_Wtu = np.linalg.norm(Wtu)\n            if norm_Wtu == 0:\n                v = prng.standard_normal(size=n)\n                v = v / np.linalg.norm(v)\n            else:\n                v = Wtu / norm_Wtu\n        \n        # Final estimate\n        sigma_hat = np.linalg.norm(W @ v)\n        return sigma_hat\n\n    results = []\n\n    # Test case 1: General rectangular case\n    W1 = rng.standard_normal(size=(5, 3))\n    sigma1_hat = estimate_spectral_norm(W1, 50, rng)\n    results.append(sigma1_hat)\n\n    # Test case 2: Identity boundary case\n    W2 = np.identity(4)\n    sigma2_hat = estimate_spectral_norm(W2, 10, rng)\n    results.append(sigma2_hat)\n\n    # Test case 3: Zero boundary case\n    W3 = np.zeros((3, 2))\n    sigma3_hat = estimate_spectral_norm(W3, 10, rng)\n    results.append(sigma3_hat)\n\n    # Test case 4: Diagonal known-values case\n    W4 = np.diag([3, 1, 2])\n    sigma4_hat = estimate_spectral_norm(W4, 10, rng)\n    bool4 = np.abs(sigma4_hat - 3) = 1e-6\n    results.append(bool4)\n\n    # Test case 5: Constructed rectangular matrix with known singular values\n    A_U = rng.standard_normal(size=(2, 2))\n    U, _ = np.linalg.qr(A_U)\n    A_V = rng.standard_normal(size=(5, 5))\n    V, _ = np.linalg.qr(A_V)\n    Sigma = np.zeros((2, 5))\n    Sigma[0, 0] = 4\n    Sigma[1, 1] = 1\n    W5 = U @ Sigma @ V.T\n    sigma5_hat = estimate_spectral_norm(W5, 60, rng)\n    float5 = np.abs(sigma5_hat - 4)\n    results.append(float5)\n\n    # Test case 6: Scalar multiplication property check\n    W6 = rng.standard_normal(size=(4, 4))\n    alpha = 0.5\n    sigma_W6_hat = estimate_spectral_norm(W6, 60, rng)\n    sigma_alphaW6_hat = estimate_spectral_norm(alpha * W6, 60, rng)\n    bool6 = np.abs(sigma_alphaW6_hat - np.abs(alpha) * sigma_W6_hat) = 1e-5\n    results.append(bool6)\n    \n    # Test case 7: Submultiplicativity check with a vector\n    # Using W1 from test case 1.\n    x = rng.standard_normal(size=3)\n    norm_W1x = np.linalg.norm(W1 @ x)\n    norm_x = np.linalg.norm(x)\n    # Compute the exact spectral norm using SVD\n    singular_values_W1 = np.linalg.svd(W1, compute_uv=False)\n    true_norm_W1 = singular_values_W1[0]\n    bool7 = norm_W1x = true_norm_W1 * norm_x\n    results.append(bool7)\n\n    # Final print statement in the exact required format.\n    # The map(str,...) converts each element (float, bool) to its string representation\n    # e.g., True -> \"True\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}