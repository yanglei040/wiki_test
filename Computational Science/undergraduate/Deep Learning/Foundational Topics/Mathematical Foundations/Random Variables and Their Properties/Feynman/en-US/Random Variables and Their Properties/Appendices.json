{
    "hands_on_practices": [
        {
            "introduction": "The use of mini-batches is central to modern deep learning, but how does batch size affect the training process? By treating the loss on each training example as a random variable, we can use fundamental statistical principles to analyze the stability of our gradient estimates. This practice  guides you through deriving the relationship between batch size and the variance of the mini-batch loss, providing a concrete tool for making principled decisions about this critical hyperparameter.",
            "id": "3166774",
            "problem": "Consider a deep learning training scenario where each per-example loss is treated as a random variable. Let the per-example loss be denoted by $\\ell$, and assume that $\\ell$ has a finite mean $\\mu = \\mathbb{E}[\\ell]$ and finite variance $\\sigma^{2} = \\mathrm{Var}(\\ell)$. Let $\\{\\ell_{i}\\}_{i=1}^{B}$ be independent and identically distributed draws of the loss from a shuffled dataset in one training step, and define the mini-batch average loss as\n$$\n\\hat{L}_{B} = \\frac{1}{B} \\sum_{i=1}^{B} \\ell_{i}\n$$.\nYou conduct a pilot run with batch size $B=1$ over a large number of examples to estimate the variability of $\\ell$, obtaining an unbiased sample variance estimate $\\hat{\\sigma}^{2} = 0.09$ for the per-example loss. You plan to choose a batch size $B$ such that the variance of the mini-batch average $\\hat{L}_{B}$ equals a target fluctuation level $v^{\\star} = 0.001$.\n\nStarting only from core definitions of expectation and variance for independent random variables, and using the Law of Large Numbers (LLN) and the Central Limit Theorem (CLT) as foundational justifications for modeling fluctuations of the sample mean, derive the variance of $\\hat{L}_{B}$ in terms of $\\sigma^{2}$ and $B$, and determine the batch size $B$ that achieves $\\mathrm{Var}(\\hat{L}_{B}) = v^{\\star}$ when $\\sigma^{2}$ is approximated by $\\hat{\\sigma}^{2}$. Provide your final answer as a single integer. No rounding is required.",
            "solution": "The problem is first validated for scientific and logical soundness.\n\n### Step 1: Extract Givens\n- The per-example loss is a random variable, $\\ell$.\n- The mean of the loss is finite: $\\mu = \\mathbb{E}[\\ell]$.\n- The variance of the loss is finite: $\\sigma^{2} = \\mathrm{Var}(\\ell)$.\n- A mini-batch consists of $\\{\\ell_{i}\\}_{i=1}^{B}$ independent and identically distributed (i.i.d.) draws of the loss.\n- The mini-batch average loss is defined as $\\hat{L}_{B} = \\frac{1}{B} \\sum_{i=1}^{B} \\ell_{i}$.\n- An unbiased sample variance estimate for $\\ell$ is given as $\\hat{\\sigma}^{2} = 0.09$.\n- A target variance for the mini-batch average loss is specified as $v^{\\star} = 0.001$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It presents a standard, fundamental problem in the statistical analysis of stochastic optimization methods used in deep learning. The concepts of sample mean, population variance, and the variance of the sample mean are cornerstones of statistics. The problem provides all necessary data ($\\hat{\\sigma}^{2}$, $v^{\\star}$) and definitions to arrive at a unique, meaningful solution for the batch size $B$. The provided values are realistic. The reference to the Law of Large Numbers (LLN) and Central Limit Theorem (CLT) is contextually appropriate, as these theorems provide the theoretical foundation for why the mini-batch average loss $\\hat{L}_{B}$ is a good estimator for the true expected loss $\\mu$, and why its variance is a critical quantity to control. The problem is free from any of the invalidating flaws listed in the instructions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Solution Derivation\nThe objective is to derive the variance of the mini-batch average loss, $\\mathrm{Var}(\\hat{L}_{B})$, and then determine the batch size $B$ that satisfies the target condition.\n\nThe mini-batch average loss is defined as:\n$$\n\\hat{L}_{B} = \\frac{1}{B} \\sum_{i=1}^{B} \\ell_{i}\n$$\nWe seek its variance, $\\mathrm{Var}(\\hat{L}_{B})$. We start from the core definition of variance and its properties for linear combinations of random variables.\n\nFirst, we use the property that for any random variable $X$ and constant $c$, $\\mathrm{Var}(cX) = c^{2}\\mathrm{Var}(X)$. Here, our random variable is the sum $\\sum_{i=1}^{B} \\ell_{i}$ and the constant is $\\frac{1}{B}$.\n$$\n\\mathrm{Var}(\\hat{L}_{B}) = \\mathrm{Var}\\left(\\frac{1}{B} \\sum_{i=1}^{B} \\ell_{i}\\right) = \\left(\\frac{1}{B}\\right)^{2} \\mathrm{Var}\\left(\\sum_{i=1}^{B} \\ell_{i}\\right) = \\frac{1}{B^{2}} \\mathrm{Var}\\left(\\sum_{i=1}^{B} \\ell_{i}\\right)\n$$\nNext, we determine the variance of the sum of the random variables. For any two *independent* random variables $X$ and $Y$, the variance of their sum is the sum of their variances: $\\mathrm{Var}(X+Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)$. The problem states that the per-example losses $\\{\\ell_{i}\\}_{i=1}^{B}$ are independent. By induction, this property extends to the sum of $B$ independent random variables:\n$$\n\\mathrm{Var}\\left(\\sum_{i=1}^{B} \\ell_{i}\\right) = \\sum_{i=1}^{B} \\mathrm{Var}(\\ell_{i})\n$$\nThe problem also states that the losses $\\{\\ell_{i}\\}_{i=1}^{B}$ are identically distributed. This means that each $\\ell_{i}$ has the same variance, $\\mathrm{Var}(\\ell_{i}) = \\sigma^{2}$ for all $i \\in \\{1, 2, \\dots, B\\}$. Therefore, the sum of the variances is:\n$$\n\\sum_{i=1}^{B} \\mathrm{Var}(\\ell_{i}) = \\sum_{i=1}^{B} \\sigma^{2} = B\\sigma^{2}\n$$\nSubstituting this result back into our expression for $\\mathrm{Var}(\\hat{L}_{B})$:\n$$\n\\mathrm{Var}(\\hat{L}_{B}) = \\frac{1}{B^{2}} (B\\sigma^{2}) = \\frac{\\sigma^{2}}{B}\n$$\nThis derivation shows that the variance of the mini-batch average loss is inversely proportional to the batch size $B$. This relationship is fundamental to understanding the trade-off between computational cost and gradient estimate accuracy in deep learning. The LLN guarantees that $\\hat{L}_{B}$ converges to $\\mu$ as $B \\to \\infty$, and the CLT characterizes the distribution of the fluctuations around $\\mu$ for finite $B$, whose scale is governed by $\\mathrm{Var}(\\hat{L}_{B}) = \\frac{\\sigma^{2}}{B}$.\n\nNow, we must find the batch size $B$ such that the variance of the mini-batch average equals the target fluctuation level $v^{\\star}$.\n$$\n\\mathrm{Var}(\\hat{L}_{B}) = v^{\\star}\n$$\nUsing our derived formula:\n$$\n\\frac{\\sigma^{2}}{B} = v^{\\star}\n$$\nThe problem provides an unbiased sample variance estimate $\\hat{\\sigma}^{2} = 0.09$ from a pilot run, which we use as an approximation for the true population variance $\\sigma^{2}$. Substituting the given values:\n$$\n\\frac{\\hat{\\sigma}^{2}}{B} = v^{\\star} \\implies \\frac{0.09}{B} = 0.001\n$$\nSolving for $B$:\n$$\nB = \\frac{0.09}{0.001} = \\frac{9 \\times 10^{-2}}{1 \\times 10^{-3}} = 9 \\times 10^{1} = 90\n$$\nThus, a batch size of $B=90$ is required to achieve the target variance of $v^{\\star} = 0.001$ for the mini-batch average loss, based on the provided estimate of the per-example loss variance.",
            "answer": "$$\n\\boxed{90}\n$$"
        },
        {
            "introduction": "A key task in deploying reliable machine learning systems is to quantify the uncertainty in their predictions. This exercise  explores this by applying the Law of Total Variance to an ensemble of models, decomposing the overall prediction variance into two distinct types: inherent data noise (aleatoric) and model disagreement (epistemic). By both deriving this fundamental identity and empirically verifying it through code, you will gain a powerful framework for analyzing and understanding model uncertainty.",
            "id": "3166709",
            "problem": "Consider a supervised deep learning setting where a collection of independently trained Neural Networks (NNs) form an ensemble. For a fixed dataset of inputs, let $N$ denote the number of distinct inputs and let $K$ denote the number of ensemble members. For each input index $i \\in \\{1,\\dots,N\\}$ and ensemble member index $m \\in \\{1,\\dots,K\\}$, denote the scalar prediction by $y_{i,m} \\in \\mathbb{R}$. Define a random input $X$ by selecting one of the $N$ inputs uniformly at random, and define a random prediction $Y$ by selecting one of the $K$ ensemble members uniformly at random and taking its prediction at the selected input. Using only the core definitions of expectation and variance and the definition of conditional expectation, derive the variance decomposition conditioned on $X$, and then apply this decomposition to quantify the two constituent terms empirically from the finite matrix of ensemble predictions $\\{y_{i,m}\\}$.\n\nYou must proceed from the following foundational base:\n- The variance of a random variable $Z$ is defined as $\\mathrm{Var}(Z) = \\mathbb{E}\\big[(Z - \\mathbb{E}[Z])^2\\big] = \\mathbb{E}[Z^2] - \\big(\\mathbb{E}[Z]\\big)^2$.\n- The conditional expectation $\\mathbb{E}[Z \\mid W]$ is defined as a random variable (a function of $W$) satisfying $\\mathbb{E}[Z \\mathbf{1}_A] = \\mathbb{E}\\big[\\mathbb{E}[Z \\mid W] \\mathbf{1}_A\\big]$ for any event $A$ measurable with respect to $W$.\n- The law of total expectation (tower property) states $\\mathbb{E}[Z] = \\mathbb{E}\\big[\\mathbb{E}[Z \\mid W]\\big]$ for any pair of random variables $(Z,W)$.\n\nYour program must:\n- Generate synthetic ensemble predictions $\\{y_{i,m}\\}$ using the following model for each test case. Let $x_i \\in \\mathbb{R}$ denote the scalar input at index $i$. Define a deterministic base function $f(x) = a x + c$, a per-model bias $b_m \\sim \\mathcal{N}(0,\\tau^2)$ independent across $m$, and per-prediction noise $\\epsilon_{i,m} \\sim \\mathcal{N}\\big(0, \\sigma(x_i)^2\\big)$ with heteroscedastic standard deviation $\\sigma(x) = \\sigma_0 + \\sigma_1 |x|$. Then set each prediction as\n$$\ny_{i,m} = f(x_i) + b_m + \\epsilon_{i,m}\n$$.\nWithin each test case, treat the empirical distribution as uniform over all observed inputs and ensemble members. Specifically, use empirical expectations computed as averages over the finite sets with denominators $N$, $K$, and $N K$ (not unbiased corrections):\n- For each input $i$, compute the conditional mean $\\mu_i = \\frac{1}{K} \\sum_{m=1}^{K} y_{i,m}$ and the conditional variance $v_i = \\frac{1}{K} \\sum_{m=1}^{K} \\big(y_{i,m} - \\mu_i\\big)^2$.\n- Compute the empirical analogs of the two decomposition terms and the total variance as\n$$\nA = \\frac{1}{N} \\sum_{i=1}^{N} v_i, \\quad\nB = \\frac{1}{N} \\sum_{i=1}^{N} \\big(\\mu_i - \\bar{\\mu}\\big)^2, \\quad\nC = \\frac{1}{N K} \\sum_{i=1}^{N} \\sum_{m=1}^{K} \\big(y_{i,m} - \\bar{y}\\big)^2,\n$$\nwhere $\\bar{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} \\mu_i$ and $\\bar{y} = \\frac{1}{N K} \\sum_{i=1}^{N} \\sum_{m=1}^{K} y_{i,m}$. Also compute the discrepancy\n$$\nD = C - (A + B),\n$$\nwhich must be numerically close to $0$ if the decomposition is correctly implemented with empirical expectations.\n\nImplement and evaluate the following test suite, where each case provides $(N,K,a,c,\\tau,\\sigma_0,\\sigma_1,\\text{seed})$ and inputs $x_i$ are independently drawn from $\\mathcal{N}(0,1)$ using the provided seed:\n- Test case $1$ (happy path, heteroscedastic, moderate ensemble diversity): $N=200$, $K=5$, $a=2.0$, $c=0.5$, $\\tau=0.3$, $\\sigma_0=0.2$, $\\sigma_1=0.5$, $\\text{seed}=1234$.\n- Test case $2$ (boundary, no within-input variability): $N=100$, $K=7$, $a=1.5$, $c=-1.0$, $\\tau=0.0$, $\\sigma_0=0.0$, $\\sigma_1=0.0$, $\\text{seed}=42$.\n- Test case $3$ (boundary, no between-input variability in ensemble mean): $N=150$, $K=4$, $a=0.0$, $c=3.0$, $\\tau=0.8$, $\\sigma_0=0.5$, $\\sigma_1=0.0$, $\\text{seed}=7$.\n- Test case $4$ (edge, single input): $N=1$, $K=10$, $a=0.7$, $c=0.0$, $\\tau=1.0$, $\\sigma_0=0.3$, $\\sigma_1=0.0$, $\\text{seed}=31415$.\n- Test case $5$ (edge, single model): $N=120$, $K=1$, $a=1.0$, $c=0.0$, $\\tau=0.0$, $\\sigma_0=0.2$, $\\sigma_1=0.0$, $\\text{seed}=98765$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, with each test case reported as a sublist $[C,A,B,D]$ of four floats rounded to six decimal places. For example, the output must look like\n$$\n[[C_1,A_1,B_1,D_1],[C_2,A_2,B_2,D_2],\\dots,[C_5,A_5,B_5,D_5]].\n$$\nNo spaces are permitted anywhere in the printed line.",
            "solution": "The problem requires a derivation of the law of total variance and its application to a finite set of ensemble predictions. The solution is presented in two parts: first, the theoretical derivation of the variance decomposition, followed by a demonstration that this identity holds algebraically for the specific empirical estimators provided.\n\n### Part 1: Derivation of the Law of Total Variance\n\nThe goal is to prove the identity $\\mathrm{Var}(Y) = \\mathbb{E}[\\mathrm{Var}(Y \\mid X)] + \\mathrm{Var}(\\mathbb{E}[Y \\mid X])$. We start from the foundational definitions provided.\n\nThe variance of a random variable $Y$ is defined as:\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2\n$$\nWe apply the law of total expectation, $\\mathbb{E}[Z] = \\mathbb{E}\\big[\\mathbb{E}[Z \\mid W]\\big]$, to each term on the right-hand side, conditioning on the random variable $X$.\n\nFor the term $\\mathbb{E}[Y]$, we have:\n$$\n\\mathbb{E}[Y] = \\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]\n$$\nwhere the subscript on the outer expectation, $\\mathbb{E}_X[\\cdot]$, emphasizes that the expectation is taken over the distribution of $X$.\n\nFor the term $\\mathbb{E}[Y^2]$, we similarly have:\n$$\n\\mathbb{E}[Y^2] = \\mathbb{E}_X[\\mathbb{E}[Y^2 \\mid X]]\n$$\nSubstituting these into the variance definition gives:\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_X[\\mathbb{E}[Y^2 \\mid X]] - \\big(\\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]\\big)^2\n$$\nNow, we use the definition of conditional variance, $\\mathrm{Var}(Y \\mid X) = \\mathbb{E}[Y^2 \\mid X] - (\\mathbb{E}[Y \\mid X])^2$. This is a random variable that is a function of $X$. We can rearrange this definition to express $\\mathbb{E}[Y^2 \\mid X]$:\n$$\n\\mathbb{E}[Y^2 \\mid X] = \\mathrm{Var}(Y \\mid X) + (\\mathbb{E}[Y \\mid X])^2\n$$\nSubstituting this expression for $\\mathbb{E}[Y^2 \\mid X]$ back into our equation for $\\mathrm{Var}(Y)$:\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_X\\big[ \\mathrm{Var}(Y \\mid X) + (\\mathbb{E}[Y \\mid X])^2 \\big] - \\big(\\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]\\big)^2\n$$\nUsing the linearity of expectation for the outer expectation $\\mathbb{E}_X[\\cdot]$, we can split the first term:\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_X[\\mathrm{Var}(Y \\mid X)] + \\mathbb{E}_X\\big[(\\mathbb{E}[Y \\mid X])^2\\big] - \\big(\\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]\\big)^2\n$$\nLet us examine the last two terms. If we define a new random variable $Z = \\mathbb{E}[Y \\mid X]$, which is a function of $X$, then these terms are $\\mathbb{E}_X[Z^2] - (\\mathbb{E}_X[Z])^2$. This is precisely the definition of the variance of $Z$, i.e., $\\mathrm{Var}(Z) = \\mathrm{Var}_X(\\mathbb{E}[Y \\mid X])$.\n\nTherefore, we arrive at the final decomposition:\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}[\\mathrm{Var}(Y \\mid X)] + \\mathrm{Var}(\\mathbb{E}[Y \\mid X])\n$$\nThe first term, $\\mathbb{E}[\\mathrm{Var}(Y \\mid X)]$, is the expected conditional variance, often called the \"aleatoric\" or \"irreducible\" uncertainty. It represents the average variance of predictions for a fixed input. The second term, $\\mathrm{Var}(\\mathbb{E}[Y \\mid X])$, is the variance of the conditional expectation, often called the \"epistemic\" uncertainty or model disagreement. It represents the variability of the ensemble's mean prediction across different inputs.\n\n### Part 2: The Empirical Decomposition as an Algebraic Identity\n\nThe problem defines the random variables $X$ and $Y$ based on a finite matrix of predictions $\\{y_{i,m}\\}$ for $i \\in \\{1, \\dots, N\\}$ and $m \\in \\{1, \\dots, K\\}$. The sampling process is uniform, meaning any pair $(i,m)$ is chosen with probability $1/(NK)$. The empirical quantities $A$, $B$, and $C$ are defined as:\n$$\nA = \\frac{1}{N} \\sum_{i=1}^{N} v_i \\quad \\text{where} \\quad v_i = \\frac{1}{K} \\sum_{m=1}^{K} \\big(y_{i,m} - \\mu_i\\big)^2 \\quad \\text{and} \\quad \\mu_i = \\frac{1}{K} \\sum_{m=1}^{K} y_{i,m}\n$$\n$$\nB = \\frac{1}{N} \\sum_{i=1}^{N} \\big(\\mu_i - \\bar{\\mu}\\big)^2 \\quad \\text{where} \\quad \\bar{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} \\mu_i\n$$\n$$\nC = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} \\big(y_{i,m} - \\bar{y}\\big)^2 \\quad \\text{where} \\quad \\bar{y} = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} y_{i,m}\n$$\nThese quantities are the empirical analogs of the terms in the theoretical decomposition. We show that for these specific definitions, the identity $C = A + B$ holds algebraically.\n\nFirst, note that the overall mean $\\bar{y}$ is identical to the mean of the conditional means $\\bar{\\mu}$:\n$$\n\\bar{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\frac{1}{K} \\sum_{m=1}^{K} y_{i,m}\\right) = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} y_{i,m} = \\bar{y}\n$$\nNow, we expand the expression for the total variance $C$. We add and subtract the conditional mean $\\mu_i$ inside the square:\n$$\nC = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} \\big( (y_{i,m} - \\mu_i) + (\\mu_i - \\bar{y}) \\big)^2\n$$\nExpanding the square $(p+q)^2 = p^2 + q^2 + 2pq$:\n$$\nC = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} \\left[ (y_{i,m} - \\mu_i)^2 + (\\mu_i - \\bar{y})^2 + 2(y_{i,m} - \\mu_i)(\\mu_i - \\bar{y}) \\right]\n$$\nWe can split this into three terms by linearity of summation:\n$$\nC = \\frac{1}{NK} \\sum_{i,m} (y_{i,m} - \\mu_i)^2 + \\frac{1}{NK} \\sum_{i,m} (\\mu_i - \\bar{y})^2 + \\frac{2}{NK} \\sum_{i,m} (y_{i,m} - \\mu_i)(\\mu_i - \\bar{y})\n$$\nLet's analyze each term:\n1.  **First term**:\n    $$\n    \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} (y_{i,m} - \\mu_i)^2 = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{1}{K} \\sum_{m=1}^{K} (y_{i,m} - \\mu_i)^2 \\right) = \\frac{1}{N} \\sum_{i=1}^{N} v_i = A\n    $$\n2.  **Second term**: The summand $(\\mu_i - \\bar{y})^2$ does not depend on the index $m$. The sum over $m$ results in a factor of $K$:\n    $$\n    \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} (\\mu_i - \\bar{y})^2 = \\frac{1}{NK} \\sum_{i=1}^{N} K (\\mu_i - \\bar{y})^2 = \\frac{1}{N} \\sum_{i=1}^{N} (\\mu_i - \\bar{y})^2 = B\n    $$\n    (since $\\bar{y}=\\bar{\\mu}$)\n3.  **Third term (cross-term)**: We can factor out the term that does not depend on $m$:\n    $$\n    \\frac{2}{NK} \\sum_{i=1}^{N} (\\mu_i - \\bar{y}) \\left( \\sum_{m=1}^{K} (y_{i,m} - \\mu_i) \\right)\n    $$\n    The inner sum over $m$ is zero by definition of $\\mu_i$:\n    $$\n    \\sum_{m=1}^{K} (y_{i,m} - \\mu_i) = \\left( \\sum_{m=1}^{K} y_{i,m} \\right) - \\left( \\sum_{m=1}^{K} \\mu_i \\right) = (K \\mu_i) - (K \\mu_i) = 0\n    $$\n    Thus, the entire cross-term is zero.\n\nCombining the results, we have proven that for the given empirical estimators:\n$$\nC = A + B\n$$\nThis means that the discrepancy $D = C - (A+B)$ must be exactly $0$. Any non-zero value observed in a numerical implementation will be solely due to floating-point representation errors.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and empirically validates the law of total variance for an ensemble\n    of neural network predictions.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, K, a, c, tau, sigma0, sigma1, seed)\n        (200, 5, 2.0, 0.5, 0.3, 0.2, 0.5, 1234),\n        (100, 7, 1.5, -1.0, 0.0, 0.0, 0.0, 42),\n        (150, 4, 0.0, 3.0, 0.8, 0.5, 0.0, 7),\n        (1, 10, 0.7, 0.0, 1.0, 0.3, 0.0, 31415),\n        (120, 1, 1.0, 0.0, 0.0, 0.2, 0.0, 98765),\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        N, K, a, c, tau, sigma0, sigma1, seed = case\n        \n        # Set up a random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n        \n        # Generate synthetic data according to the model.\n        # 1. Inputs x_i ~ N(0, 1)\n        x = rng.normal(loc=0.0, scale=1.0, size=N)\n        \n        # 2. Per-model biases b_m ~ N(0, tau^2)\n        b = rng.normal(loc=0.0, scale=tau, size=K)\n        \n        # 3. Base function f(x_i)\n        f_x = a * x + c\n        \n        # 4. Heteroscedastic noise standard deviation sigma(x_i)\n        sigma_x = sigma0 + sigma1 * np.abs(x)\n        \n        # 5. Per-prediction noise epsilon_{i,m} ~ N(0, sigma(x_i)^2)\n        # We broadcast sigma_x (N,) to (N, K) to generate the noise matrix.\n        epsilon_scale = sigma_x[:, np.newaxis]\n        epsilon = rng.normal(loc=0.0, scale=epsilon_scale, size=(N, K))\n        \n        # 6. Final predictions y_{i,m}\n        # We use broadcasting to combine f_x (N,), b (K,), and epsilon (N, K).\n        y = f_x[:, np.newaxis] + b[np.newaxis, :] + epsilon\n        \n        # Compute the empirical quantities as defined in the problem.\n        # ddof=0 ensures we divide by n (population variance) instead of n-1.\n        \n        # Conditional mean for each input i: mu_i\n        mu = np.mean(y, axis=1)  # Shape (N,)\n        \n        # Conditional variance for each input i: v_i\n        v = np.var(y, axis=1, ddof=0)  # Shape (N,)\n        \n        # Term A: Expected value of the conditional variance\n        # A = (1/N) * sum(v_i)\n        A = np.mean(v)\n        \n        # Term B: Variance of the conditional expectation\n        # B = (1/N) * sum((mu_i - bar_mu)^2)\n        # np.var calculates variance against the mean of the input array `mu`.\n        B = np.var(mu, ddof=0)\n        \n        # Term C: Total variance\n        # C = (1/NK) * sum((y_{i,m} - bar_y)^2)\n        # np.var applied to the entire matrix calculates this.\n        C = np.var(y, ddof=0)\n        \n        # Discrepancy D = C - (A + B)\n        # This should be zero up to floating-point precision, as shown in the derivation.\n        D = C - (A + B)\n        \n        case_results = [C, A, B, D]\n        # Format the results for this case to 6 decimal places.\n        case_str = f\"[{','.join([f'{val:.6f}' for val in case_results])}]\"\n        all_results_str.append(case_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "How can we confidently report a model's performance when results fluctuate with every random initialization? This practice  addresses this critical question by treating model performance as a random variable and applying the nonparametric bootstrap, a powerful resampling method. You will learn to construct confidence intervals for the expected performance and assess whether the distribution of results is normal, empowering you to report your findings with statistical rigor.",
            "id": "3166766",
            "problem": "You are given a fixed set of hyperparameters denoted by $\\theta_0$ for a deep learning model, and the performance values (for example, validation accuracy) observed across multiple random initialization seeds. Treat the performance across random seeds as a random variable $A(\\theta_0)$, abbreviated as $A$, and assume that the observed values are independent and identically distributed (i.i.d.) draws from an unknown distribution. Your tasks are to estimate the expected performance $\\mathbb{E}[A]$, construct a nonparametric bootstrap Confidence Interval (CI) for $\\mathbb{E}[A]$, and assess non-normality using the empirical distribution of $A$.\n\nStart from the following fundamental bases and widely accepted facts:\n- The definition of a random variable, its expectation $\\mathbb{E}[A]$, and the sample mean as an estimator of $\\mathbb{E}[A]$.\n- The Law of Large Numbers, which justifies the sample mean as a consistent estimator of $\\mathbb{E}[A]$.\n- The bootstrap principle: resampling from the empirical distribution approximates sampling from the true, unknown distribution and can be used to approximate the sampling distribution of an estimator.\n\nYour program must:\n1. Treat each provided set of observed performances as an i.i.d. sample $a_1, a_2, \\dots, a_n$ from the distribution of $A$.\n2. Use the nonparametric bootstrap to approximate the sampling distribution of the sample mean. Specifically, for each test case:\n   - Generate $B = 5000$ bootstrap resamples of size $n$ by sampling with replacement from the observed $a_i$.\n   - Compute the bootstrap distribution of the sample mean for these resamples.\n   - Construct a two-sided CI for the expected performance using the percentile method with confidence level $0.95$ (so $\\alpha = 0.05$, and the interval bounds are the empirical $\\alpha/2$ and $1-\\alpha/2$ quantiles of the bootstrap means).\n3. Assess non-normality using the empirical distribution by computing the sample skewness and excess kurtosis of the observed $a_i$. Let the sample mean be $\\bar{a}$, and define the centered moments\n   $$m_k = \\frac{1}{n}\\sum_{i=1}^{n}(a_i - \\bar{a})^k.$$\n   Compute skewness as\n   $$\\text{skewness} = \\frac{m_3}{m_2^{3/2}},$$\n   and excess kurtosis as\n   $$\\text{excess kurtosis} = \\frac{m_4}{m_2^2} - 3.$$\n   Flag the distribution as non-normal if either $|\\text{skewness}| > s_{\\mathrm{th}}$ or $|\\text{excess kurtosis}| > k_{\\mathrm{th}}$, where $s_{\\mathrm{th}} = 0.5$ and $k_{\\mathrm{th}} = 0.5$. This is a heuristic assessment intended to indicate departures from normality.\n4. For reproducibility of the bootstrap procedure, use a fixed random seed $r = 17$ for the resampling process.\n\nTest Suite:\nUse the following five test cases, each of which provides observed performances for the same hyperparameters $\\theta_0$ but different sampling regimes. These values are unitless proportions (for example, accuracies in $[0,1]$). The samples are:\n- Case $1$: $A^{(1)} = [0.821, 0.834, 0.827, 0.816, 0.829, 0.824, 0.833, 0.818, 0.826, 0.831, 0.820, 0.828, 0.825, 0.819, 0.832, 0.817, 0.830, 0.823, 0.822, 0.835]$.\n- Case $2$: $A^{(2)} = [0.761, 0.775, 0.790, 0.802, 0.815, 0.860, 0.905]$.\n- Case $3$: $A^{(3)} = [0.781, 0.785, 0.779, 0.790, 0.788, 0.783, 0.776, 0.782, 0.780, 0.784, 0.552, 0.901, 0.775, 0.786, 0.789, 0.777, 0.782, 0.778, 0.790, 0.791, 0.787, 0.781, 0.783, 0.774, 0.792]$.\n- Case $4$: $A^{(4)} = [0.701, 0.699, 0.702, 0.700, 0.701, 0.700, 0.699, 0.701]$.\n- Case $5$: $A^{(5)} = [0.680, 0.682, 0.678, 0.685, 0.683, 0.681, 0.679, 0.684, 0.840, 0.842, 0.838, 0.845, 0.843, 0.841, 0.839, 0.846]$.\n\nOutput specification:\n- For each test case, produce a list of the form $[\\bar{a}, \\text{CI}_{\\mathrm{low}}, \\text{CI}_{\\mathrm{high}}, \\text{flag}]$, where $\\bar{a}$ is the sample mean, $\\text{CI}_{\\mathrm{low}}$ and $\\text{CI}_{\\mathrm{high}}$ are the lower and upper CI bounds constructed via the bootstrap percentile method, and $\\text{flag}$ is a boolean indicating non-normality as defined above.\n- The floating-point values $\\bar{a}$, $\\text{CI}_{\\mathrm{low}}$, and $\\text{CI}_{\\mathrm{high}}$ must be rounded to $4$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result itself being a list. For example, the output format must be exactly like $[[m_1,l_1,u_1,b_1],[m_2,l_2,u_2,b_2],\\dots]$ with no spaces.",
            "solution": "The problem requires a statistical analysis of simulated performance values of a deep learning model. The analysis comprises three parts for each given sample of observations: estimation of the expected performance, construction of a confidence interval for this expectation using the bootstrap method, and an assessment of the sample's deviation from a normal distribution using moments.\n\nThe problem is valid. It is scientifically grounded in established statistical theory (estimation, bootstrapping, moment analysis), common in the empirical evaluation of machine learning models. It is well-posed, providing all necessary data, parameters, and explicit mathematical definitions for the required computations, ensuring a unique solution can be algorithmically determined. The language is objective and unambiguous.\n\nThe solution proceeds by implementing the specified procedures for each test case.\n\n**1. Estimation of Expected Performance $\\mathbb{E}[A]$**\n\nGiven a set of $n$ independent and identically distributed (i.i.d.) observations $a = \\{a_1, a_2, \\dots, a_n\\}$ from the distribution of a random variable $A$, the expected value $\\mathbb{E}[A]$ is the population mean. A fundamental principle in statistics, the Law of Large Numbers, states that the sample mean converges to the population mean as the sample size increases. Therefore, the sample mean $\\bar{a}$ is a consistent estimator for $\\mathbb{E}[A]$. It is calculated as:\n$$\n\\bar{a} = \\frac{1}{n} \\sum_{i=1}^{n} a_i\n$$\nFor each test case, we compute this value from the provided sample.\n\n**2. Nonparametric Bootstrap Confidence Interval**\n\nTo quantify the uncertainty in our estimate $\\bar{a}$, we construct a confidence interval (CI) for $\\mathbb{E}[A]$. Since the underlying distribution of $A$ is unknown, we employ the nonparametric bootstrap, a resampling technique that allows us to approximate the sampling distribution of an estimator.\n\nThe bootstrap principle states that resampling with replacement from the observed sample is analogous to drawing new samples from the true underlying population. The empirical distribution of the sample, which assigns a probability of $1/n$ to each observation $a_i$, serves as an approximation to the true data-generating distribution.\n\nThe procedure is as follows:\n- A large number of bootstrap resamples, $B=5000$, are generated. Each resample, denoted $a^{*(b)}$ for $b=1, \\dots, B$, is a set of $n$ data points drawn with replacement from the original sample $a$.\n- For each bootstrap resample $a^{*(b)}$, the sample mean $\\bar{a}^{*(b)}$ is computed.\n- The collection of these $B$ means, $\\{\\bar{a}^{*(1)}, \\bar{a}^{*(2)}, \\dots, \\bar{a}^{*(B)}\\}$, forms the empirical bootstrap distribution of the sample mean. This distribution approximates the true sampling distribution of $\\bar{a}$.\n- A two-sided $95\\%$ confidence interval is constructed using the percentile method. This involves finding the quantiles of the bootstrap distribution. The confidence level is $1-\\alpha = 0.95$, so $\\alpha = 0.05$. The lower bound of the CI, $\\text{CI}_{\\mathrm{low}}$, is the $\\alpha/2 = 0.025$ quantile (the $2.5$-th percentile) of the sorted bootstrap means. The upper bound, $\\text{CI}_{\\mathrm{high}}$, is the $1-\\alpha/2 = 0.975$ quantile (the $97.5$-th percentile).\n- For reproducibility, the random number generator used for resampling is seeded with the specified value $r=17$.\n\n**3. Assessment of Non-Normality**\n\nTo assess whether the distribution of the observed performance values deviates from a normal distribution, we compute an estimate of its skewness and kurtosis. These are based on the sample's central moments.\n\nThe $k$-th sample central moment, $m_k$, for a sample $a$ with mean $\\bar{a}$ is defined as:\n$$\nm_k = \\frac{1}{n}\\sum_{i=1}^{n}(a_i - \\bar{a})^k\n$$\nThe problem specifies the use of the following formulas for sample skewness and excess kurtosis:\n- **Sample Skewness ($g_1$)**: This measures the asymmetry of the distribution.\n  $$\n  g_1 = \\frac{m_3}{m_2^{3/2}}\n  $$\n- **Sample Excess Kurtosis ($g_2$)**: This measures the \"tailedness\" of the distribution compared to a normal distribution, which has a kurtosis of $3$.\n  $$\n  g_2 = \\frac{m_4}{m_2^2} - 3\n  $$\nA normal distribution has both skewness and excess kurtosis equal to $0$. The problem provides a heuristic rule to flag a distribution as non-normal: the flag is set to true if the absolute value of the sample skewness exceeds a threshold $s_{\\mathrm{th}} = 0.5$ or if the absolute value of the sample excess kurtosis exceeds a threshold $k_{\\mathrm{th}} = 0.5$.\n$$\n\\text{flag} = ( |g_1| > s_{\\mathrm{th}} ) \\lor ( |g_2| > k_{\\mathrm{th}} )\n$$\n\n**4. Final Output Construction**\n\nFor each test case, the four computed values—the sample mean $\\bar{a}$, the CI bounds $\\text{CI}_{\\mathrm{low}}$ and $\\text{CI}_{\\mathrm{high}}$, and the boolean non-normality flag—are collected. The floating-point values are rounded to $4$ decimal places as required. The results for all test cases are then formatted into a single string representing a list of lists.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs statistical analysis on deep learning model performance values.\n\n    For each test case (a sample of performance values), this function computes:\n    1. The sample mean of the performances.\n    2. A 95% bootstrap confidence interval for the true mean performance.\n    3. A boolean flag indicating potential non-normality of the performance\n       distribution, based on sample skewness and excess kurtosis.\n\n    The results are formatted into a single string as specified in the problem.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        [0.821, 0.834, 0.827, 0.816, 0.829, 0.824, 0.833, 0.818, 0.826, 0.831, 0.820, 0.828, 0.825, 0.819, 0.832, 0.817, 0.830, 0.823, 0.822, 0.835],\n        [0.761, 0.775, 0.790, 0.802, 0.815, 0.860, 0.905],\n        [0.781, 0.785, 0.779, 0.790, 0.788, 0.783, 0.776, 0.782, 0.780, 0.784, 0.552, 0.901, 0.775, 0.786, 0.789, 0.777, 0.782, 0.778, 0.790, 0.791, 0.787, 0.781, 0.783, 0.774, 0.792],\n        [0.701, 0.699, 0.702, 0.700, 0.701, 0.700, 0.699, 0.701],\n        [0.680, 0.682, 0.678, 0.685, 0.683, 0.681, 0.679, 0.684, 0.840, 0.842, 0.838, 0.845, 0.843, 0.841, 0.839, 0.846]\n    ]\n\n    # Parameters for the analysis\n    B = 5000  # Number of bootstrap resamples\n    random_seed = 17\n    confidence_level = 0.95\n    skewness_threshold = 0.5\n    kurtosis_threshold = 0.5\n    \n    # Modern numpy random number generation\n    rng = np.random.default_rng(seed=random_seed)\n\n    all_results = []\n    for sample in test_cases:\n        data = np.array(sample)\n        n = len(data)\n\n        # 1. Compute sample mean\n        sample_mean = np.mean(data)\n\n        # 2. Construct bootstrap confidence interval\n        bootstrap_means = np.zeros(B)\n        for i in range(B):\n            resample = rng.choice(data, size=n, replace=True)\n            bootstrap_means[i] = np.mean(resample)\n        \n        alpha = 1.0 - confidence_level\n        ci_low = np.percentile(bootstrap_means, 100 * (alpha / 2))\n        ci_high = np.percentile(bootstrap_means, 100 * (1 - alpha / 2))\n\n        # 3. Assess non-normality\n        centered_data = data - sample_mean\n        m2 = np.mean(centered_data**2)\n        m3 = np.mean(centered_data**3)\n        m4 = np.mean(centered_data**4)\n\n        if m2 == 0:\n            skewness = 0.0\n            excess_kurtosis = 0.0 # Or undefined, but 0 is a safe float\n        else:\n            skewness = m3 / (m2**(3/2))\n            excess_kurtosis = m4 / (m2**2) - 3\n\n        non_normal_flag = (abs(skewness) > skewness_threshold) or \\\n                          (abs(excess_kurtosis) > kurtosis_threshold)\n\n        # Append results for the current case, with rounding\n        current_result = [\n            round(sample_mean, 4),\n            round(ci_low, 4),\n            round(ci_high, 4),\n            non_normal_flag\n        ]\n        all_results.append(current_result)\n\n    # Format the final output string exactly as specified\n    output_parts = []\n    for res in all_results:\n        m, l, u, f = res\n        part = f\"[{m},{l},{u},{str(f).lower()}]\"\n        output_parts.append(part)\n    \n    final_output_str = f\"[{','.join(output_parts)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}