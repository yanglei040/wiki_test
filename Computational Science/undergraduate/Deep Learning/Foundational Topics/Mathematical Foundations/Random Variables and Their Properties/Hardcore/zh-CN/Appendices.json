{
    "hands_on_practices": [
        {
            "introduction": "在深度学习训练中，随机梯度下降（SGD）及其变体是核心算法，它们依赖于小批量（mini-batch）数据来估计损失函数的梯度。将每个样本的损失视为一个随机变量，可以为我们提供一个强大的分析工具。这个练习  将帮助你从统计学的角度理解小批量训练的本质。通过推导小批量平均损失的方差与批量大小之间的关系，你将能够量化一个关键的权衡：更大的批量如何通过降低估计噪声来稳定训练过程。",
            "id": "3166774",
            "problem": "考虑一个深度学习训练场景，其中每个单样本损失都被视为一个随机变量。设单样本损失表示为 $\\ell$，并假设 $\\ell$ 具有有限均值 $\\mu = \\mathbb{E}[\\ell]$ 和有限方差 $\\sigma^{2} = \\mathrm{Var}(\\ell)$。设 $\\{\\ell_{i}\\}_{i=1}^{B}$ 是在一个训练步骤中从一个打乱的数据集中对损失进行的独立同分布抽取，并定义小批量平均损失为\n$$\n\\hat{L}_{B} = \\frac{1}{B} \\sum_{i=1}^{B} \\ell_{i}.\n$$\n你进行了一次批量大小为 $B=1$ 的试点运行，遍历了大量样本以估计 $\\ell$ 的变异性，得到了单样本损失的无偏样本方差估计 $\\hat{\\sigma}^{2} = 0.09$。你计划选择一个批量大小 $B$，使得小批量平均损失 $\\hat{L}_{B}$ 的方差等于一个目标波动水平 $v^{\\star} = 0.001$。\n\n仅从独立随机变量的期望和方差的核心定义出发，并使用大数定律 (LLN) 和中心极限定理 (CLT) 作为样本均值波动建模的基础性论证，推导出 $\\hat{L}_{B}$ 的方差（用 $\\sigma^{2}$ 和 $B$ 表示），并确定当 $\\sigma^{2}$ 用 $\\hat{\\sigma}^{2}$ 近似时，能使 $\\mathrm{Var}(\\hat{L}_{B}) = v^{\\star}$ 成立的批量大小 $B$。请以单个整数形式提供最终答案。无需四舍五入。",
            "solution": "首先验证问题的科学性和逻辑合理性。\n\n### 第一步：提取已知条件\n- 单样本损失是一个随机变量 $\\ell$。\n- 损失的均值是有限的：$\\mu = \\mathbb{E}[\\ell]$。\n- 损失的方差是有限的：$\\sigma^{2} = \\mathrm{Var}(\\ell)$。\n- 一个小批量由损失的 $\\{\\ell_{i}\\}_{i=1}^{B}$ 次独立同分布 (i.i.d.) 抽取组成。\n- 小批量平均损失定义为 $\\hat{L}_{B} = \\frac{1}{B} \\sum_{i=1}^{B} \\ell_{i}$。\n- 给出了 $\\ell$ 的一个无偏样本方差估计 $\\hat{\\sigma}^{2} = 0.09$。\n- 指定了小批量平均损失的目标方差为 $v^{\\star} = 0.001$。\n\n### 第二步：使用已知条件进行验证\n该问题具有科学依据，提法明确且客观。它提出了一个在深度学习中使用的随机优化方法的统计分析中的标准基础问题。样本均值、总体方差和样本均值的方差等概念是统计学的基石。该问题提供了所有必要的数据（$\\hat{\\sigma}^{2}$、$v^{\\star}$）和定义，以得出批量大小 $B$ 的唯一且有意义的解。所提供的值是符合实际的。提及大数定律 (LLN) 和中心极限定理 (CLT) 在上下文中是恰当的，因为这些定理为小批量平均损失 $\\hat{L}_{B}$ 为何是真实期望损失 $\\mu$ 的良好估计量，以及为何其方差是需要控制的关键量提供了理论基础。该问题没有任何指令中列出的导致无效的缺陷。\n\n### 第三步：结论与行动\n该问题是有效的。将提供一个完整的、有理有据的解答。\n\n### 求解推导\n目标是推导小批量平均损失的方差 $\\mathrm{Var}(\\hat{L}_{B})$，然后确定满足目标条件的批量大小 $B$。\n\n小批量平均损失定义为：\n$$\n\\hat{L}_{B} = \\frac{1}{B} \\sum_{i=1}^{B} \\ell_{i}\n$$\n我们要求其方差 $\\mathrm{Var}(\\hat{L}_{B})$。我们从方差的核心定义及其对随机变量线性组合的性质开始。\n\n首先，我们使用性质：对于任意随机变量 $X$ 和常数 $c$，有 $\\mathrm{Var}(cX) = c^{2}\\mathrm{Var}(X)$。在这里，我们的随机变量是和 $\\sum_{i=1}^{B} \\ell_{i}$，常数是 $\\frac{1}{B}$。\n$$\n\\mathrm{Var}(\\hat{L}_{B}) = \\mathrm{Var}\\left(\\frac{1}{B} \\sum_{i=1}^{B} \\ell_{i}\\right) = \\left(\\frac{1}{B}\\right)^{2} \\mathrm{Var}\\left(\\sum_{i=1}^{B} \\ell_{i}\\right) = \\frac{1}{B^{2}} \\mathrm{Var}\\left(\\sum_{i=1}^{B} \\ell_{i}\\right)\n$$\n接下来，我们确定随机变量之和的方差。对于任意两个*独立*的随机变量 $X$ 和 $Y$，它们的和的方差等于它们方差的和：$\\mathrm{Var}(X+Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)$。问题说明单样本损失 $\\{\\ell_{i}\\}_{i=1}^{B}$ 是独立的。通过归纳法，此性质可推广到 $B$ 个独立随机变量的和：\n$$\n\\mathrm{Var}\\left(\\sum_{i=1}^{B} \\ell_{i}\\right) = \\sum_{i=1}^{B} \\mathrm{Var}(\\ell_{i})\n$$\n问题还说明损失 $\\{\\ell_{i}\\}_{i=1}^{B}$ 是同分布的。这意味着每个 $\\ell_{i}$ 都具有相同的方差，即对于所有 $i \\in \\{1, 2, \\dots, B\\}$，$\\mathrm{Var}(\\ell_{i}) = \\sigma^{2}$。因此，方差之和为：\n$$\n\\sum_{i=1}^{B} \\mathrm{Var}(\\ell_{i}) = \\sum_{i=1}^{B} \\sigma^{2} = B\\sigma^{2}\n$$\n将此结果代回我们关于 $\\mathrm{Var}(\\hat{L}_{B})$ 的表达式中：\n$$\n\\mathrm{Var}(\\hat{L}_{B}) = \\frac{1}{B^{2}} (B\\sigma^{2}) = \\frac{\\sigma^{2}}{B}\n$$\n此推导表明，小批量平均损失的方差与批量大小 $B$ 成反比。这种关系是理解深度学习中计算成本和梯度估计精度之间权衡的基础。大数定律保证了当 $B \\to \\infty$ 时，$\\hat{L}_{B}$ 收敛于 $\\mu$；而中心极限定理描述了在有限 $B$ 的情况下，围绕 $\\mu$ 的波动分布，其尺度由 $\\mathrm{Var}(\\hat{L}_{B}) = \\frac{\\sigma^{2}}{B}$ 控制。\n\n现在，我们必须找到批量大小 $B$，使得小批量平均损失的方差等于目标波动水平 $v^{\\star}$。\n$$\n\\mathrm{Var}(\\hat{L}_{B}) = v^{\\star}\n$$\n使用我们推导出的公式：\n$$\n\\frac{\\sigma^{2}}{B} = v^{\\star}\n$$\n问题提供了一个来自试点运行的无偏样本方差估计 $\\hat{\\sigma}^{2} = 0.09$，我们用它作为真实总体方差 $\\sigma^{2}$ 的近似值。代入给定值：\n$$\n\\frac{\\hat{\\sigma}^{2}}{B} = v^{\\star} \\implies \\frac{0.09}{B} = 0.001\n$$\n求解 $B$：\n$$\nB = \\frac{0.09}{0.001} = \\frac{9 \\times 10^{-2}}{1 \\times 10^{-3}} = 9 \\times 10^{1} = 90\n$$\n因此，基于所提供的单样本损失方差的估计值，需要批量大小为 $B=90$ 才能使小批量平均损失达到 $v^{\\star} = 0.001$ 的目标方差。",
            "answer": "$$\n\\boxed{90}\n$$"
        },
        {
            "introduction": "许多现代深度学习模型，如变分自编码器（VAEs），在其结构中包含了随机性，其中模型参数定义了一个需要从中采样的概率分布。为了训练这些模型，我们必须计算一个期望对参数的梯度，这是一个不平凡的任务。本练习  将引导你推导并比较两种解决此问题的核心技术：分数函数估计器（也称为 REINFORCE）和重参数化技巧。通过分析它们在同一个问题上的估计方差，你将深刻理解为什么重参数化技巧在实践中通常是首选方法，并掌握设计和分析随机模型梯度估计器的关键技能。",
            "id": "3166695",
            "problem": "考虑一个深度学习回归模型，在训练期间向单个神经元的预激活中注入加性高斯噪声。设带有噪声的预激活为随机变量 $Z \\sim \\mathcal{N}(\\theta, \\sigma^{2})$，其中 $\\theta \\in \\mathbb{R}$ 是一个可训练的参数，$\\sigma^{2} > 0$ 是一个已知的常数。该神经元的训练目标包括预期的平方预激活 $J(\\theta) = \\mathbb{E}_{Z \\sim p_{\\theta}}[f(Z)]$，其中 $f(z) = z^{2}$。\n\n仅使用概率论和微积分的基本定义和结果——具体来说，期望、方差、高斯分布的矩的定义、微分的链式法则、标准正则性条件下微分与期望的互换以及变量替换——通过得分函数方法和重参数化方法推导 $\\nabla_{\\theta} J(\\theta)$ 的无偏估计量。然后，计算每个估计量在抽样分布下的方差，并构建得分函数估计量的方差与重参数化估计量的方差之比。将你的最终答案表示为关于 $\\theta$ 和 $\\sigma$ 的单一闭式解析表达式，并完全化简。不需要进行数值四舍五入。",
            "solution": "用户希望我解决这个问题，并且我已经验证了它。我现在将开始进行解答。\n\n### 步骤 1：问题验证\n\n**1.1. 提取已知条件**\n- 随机变量是带噪声的预激活，$Z$。\n- $Z$ 的分布是高斯分布：$Z \\sim \\mathcal{N}(\\theta, \\sigma^{2})$。\n- $\\theta \\in \\mathbb{R}$ 是一个可训练参数。\n- $\\sigma^{2} > 0$ 是一个已知常数。\n- 训练目标是 $J(\\theta) = \\mathbb{E}_{Z \\sim p_{\\theta}}[f(Z)]$。\n- 函数是 $f(z) = z^{2}$。\n- 任务是使用得分函数和重参数化方法推导 $\\nabla_{\\theta} J(\\theta)$ 的无偏估计量。\n- 然后，计算每个估计量的方差。\n- 最后，求出得分函数估计量的方差与重参数化估计量的方差之比。\n\n**1.2. 使用提取的已知条件进行验证**\n- **科学依据：**该问题基于概率论、微积分和机器学习中的标准基本概念，即高斯分布、期望、方差、微分以及梯度估计技术（得分函数和重参数化）。这些方法是现代深度学习的基石，特别是在变分自编码器和随机优化的背景下。该设置是用于说明这些概念的典型例子。\n- **适定性：**该问题提供了明确的目标和推導估计量及其方差所需的所有信息。需要计算的量是唯一定义的。\n- **客观性：**问题以精确的数学语言陈述，没有主观性或歧义。\n\n**1.3. 结论和行动**\n问题是有效的。它在科学上是合理的、适定的、客观的，并且与指定的主题直接相关。我将进行完整的推导。\n\n### 步骤 2：解题推导\n\n需要微分的目标函数是 $J(\\theta) = \\mathbb{E}_{Z \\sim p_{\\theta}}[f(Z)]$，其中 $Z \\sim \\mathcal{N}(\\theta, \\sigma^{2})$ 且 $f(z) = z^2$。\n首先，我们计算 $J(\\theta)$ 及其梯度的精确值，这将作为参考。\n随机变量平方的期望 $\\mathbb{E}[Z^2]$ 与其方差和均值通过恒等式 $\\text{Var}(Z) = \\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2$ 相关联。\n对于 $Z \\sim \\mathcal{N}(\\theta, \\sigma^{2})$，我们有 $\\mathbb{E}[Z] = \\theta$ 和 $\\text{Var}(Z) = \\sigma^2$。\n因此，$J(\\theta) = \\mathbb{E}[Z^2] = \\text{Var}(Z) + (\\mathbb{E}[Z])^2 = \\sigma^2 + \\theta^2$。\n目标函数关于 $\\theta$ 的真实梯度是：\n$$\n\\nabla_{\\theta} J(\\theta) = \\frac{d}{d\\theta}(\\sigma^2 + \\theta^2) = 2\\theta\n$$\n\n**A. 得分函数估计量**\n\n得分函数方法，也称为 REINFORCE，依赖于恒等式 $\\nabla_{\\theta} \\mathbb{E}_{p_{\\theta}}[f(Z)] = \\mathbb{E}_{p_{\\theta}}[f(Z) \\nabla_{\\theta} \\log p_{\\theta}(Z)]$。因此，梯度的无偏单样本估计量为 $g_{SF}(z) = f(z) \\nabla_{\\theta} \\log p_{\\theta}(z)$。\n\n1.  **推导估计量：**\n    $Z$ 的概率密度函数 (PDF) 是 $p_{\\theta}(z) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z-\\theta)^2}{2\\sigma^2}\\right)$。\n    对数概率密度函数是 $\\log p_{\\theta}(z) = -\\log(\\sqrt{2\\pi\\sigma^2}) - \\frac{(z-\\theta)^2}{2\\sigma^2}$。\n    对数概率密度函数关于 $\\theta$ 的梯度是得分函数：\n    $$\n    \\nabla_{\\theta} \\log p_{\\theta}(z) = \\frac{d}{d\\theta}\\left(-\\frac{(z-\\theta)^2}{2\\sigma^2}\\right) = -\\frac{1}{2\\sigma^2} \\cdot 2(z-\\theta) \\cdot (-1) = \\frac{z-\\theta}{\\sigma^2}\n    $$\n    当 $f(z) = z^2$ 时，得分函数估计量是：\n    $$\n    g_{SF}(z) = z^2 \\left(\\frac{z-\\theta}{\\sigma^2}\\right) = \\frac{z^3 - \\theta z^2}{\\sigma^2}\n    $$\n    为了确认它是无偏的，我们计算它的期望：\n    $\\mathbb{E}[g_{SF}(Z)] = \\frac{1}{\\sigma^2} (\\mathbb{E}[Z^3] - \\theta\\mathbb{E}[Z^2])$。\n    设 $X = Z - \\theta$，则 $X \\sim \\mathcal{N}(0, \\sigma^2)$。$X$ 的矩为 $\\mathbb{E}[X]=0$, $\\mathbb{E}[X^2]=\\sigma^2$, $\\mathbb{E}[X^3]=0$。\n    $\\mathbb{E}[Z^2] = \\mathbb{E}[(X+\\theta)^2] = \\mathbb{E}[X^2+2X\\theta+\\theta^2] = \\sigma^2+\\theta^2$。\n    $\\mathbb{E}[Z^3] = \\mathbb{E}[(X+\\theta)^3] = \\mathbb{E}[X^3+3X^2\\theta+3X\\theta^2+\\theta^3] = 3\\theta\\sigma^2+\\theta^3$。\n    $$\n    \\mathbb{E}[g_{SF}(Z)] = \\frac{1}{\\sigma^2}((3\\theta\\sigma^2+\\theta^3) - \\theta(\\sigma^2+\\theta^2)) = \\frac{1}{\\sigma^2}(3\\theta\\sigma^2+\\theta^3 - \\theta\\sigma^2-\\theta^3) = \\frac{2\\theta\\sigma^2}{\\sigma^2} = 2\\theta\n    $$\n    该估计量是无偏的，因为它等于 $\\nabla_{\\theta}J(\\theta)$。\n\n2.  **计算估计量的方差：**\n    $\\text{Var}(g_{SF}(Z)) = \\mathbb{E}[g_{SF}(Z)^2] - (\\mathbb{E}[g_{SF}(Z)])^2$。\n    我们使用重参数化 $Z = \\theta + \\sigma\\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, 1)$ 是一个标准正态变量。$\\epsilon$ 的矩为 $\\mathbb{E}[\\epsilon^{2n+1}] = 0$ 和 $\\mathbb{E}[\\epsilon^{2n}] = (2n-1)!!$。具体来说，$\\mathbb{E}[\\epsilon^2]=1$, $\\mathbb{E}[\\epsilon^4]=3$, $\\mathbb{E}[\\epsilon^6]=15$。\n    用 $\\epsilon$ 表示的估计量是：\n    $$\n    g_{SF}(Z) = f(\\theta+\\sigma\\epsilon) \\frac{(\\theta+\\sigma\\epsilon)-\\theta}{\\sigma^2} = (\\theta+\\sigma\\epsilon)^2 \\frac{\\sigma\\epsilon}{\\sigma^2} = (\\theta^2 + 2\\theta\\sigma\\epsilon + \\sigma^2\\epsilon^2) \\frac{\\epsilon}{\\sigma} = \\frac{\\theta^2}{\\sigma}\\epsilon + 2\\theta\\epsilon^2 + \\sigma\\epsilon^3\n    $$\n    现在，我们计算 $\\mathbb{E}[g_{SF}(Z)^2]$：\n    $$\n    \\mathbb{E}[g_{SF}(Z)^2] = \\mathbb{E}\\left[\\left(\\frac{\\theta^2}{\\sigma}\\epsilon + 2\\theta\\epsilon^2 + \\sigma\\epsilon^3\\right)^2\\right]\n    $$\n    展开平方时，只有 $\\epsilon$ 的偶数次幂项的期望值非零。这些项是：\n    $$\n    \\mathbb{E}\\left[\\left(\\frac{\\theta^2}{\\sigma}\\epsilon\\right)^2 + (2\\theta\\epsilon^2)^2 + (\\sigma\\epsilon^3)^2 + 2\\left(\\frac{\\theta^2}{\\sigma}\\epsilon\\right)(\\sigma\\epsilon^3)\\right]\n    $$\n    $$\n    = \\mathbb{E}\\left[\\frac{\\theta^4}{\\sigma^2}\\epsilon^2 + 4\\theta^2\\epsilon^4 + \\sigma^2\\epsilon^6 + 2\\theta^2\\epsilon^4\\right]\n    $$\n    利用期望的线性和 $\\epsilon$ 的矩：\n    $$\n    = \\frac{\\theta^4}{\\sigma^2}\\mathbb{E}[\\epsilon^2] + (4\\theta^2 + 2\\theta^2)\\mathbb{E}[\\epsilon^4] + \\sigma^2\\mathbb{E}[\\epsilon^6]\n    $$\n    $$\n    = \\frac{\\theta^4}{\\sigma^2}(1) + 6\\theta^2(3) + \\sigma^2(15) = \\frac{\\theta^4}{\\sigma^2} + 18\\theta^2 + 15\\sigma^2\n    $$\n    方差是：\n    $$\n    \\text{Var}(g_{SF}(Z)) = \\mathbb{E}[g_{SF}(Z)^2] - (2\\theta)^2 = \\left(\\frac{\\theta^4}{\\sigma^2} + 18\\theta^2 + 15\\sigma^2\\right) - 4\\theta^2 = \\frac{\\theta^4}{\\sigma^2} + 14\\theta^2 + 15\\sigma^2\n    $$\n\n**B. 重参数化估计量**\n\n重参数化技巧涉及将 $Z$ 写成基础随机变量 $\\epsilon$ 和参数 $\\theta$ 的确定性变换：$Z = g(\\theta, \\epsilon)$，其中 $\\epsilon$ 的分布不依赖于 $\\theta$。然后 $\\nabla_{\\theta}\\mathbb{E}[f(Z)] = \\mathbb{E}[\\nabla_{\\theta}f(g(\\theta, \\epsilon))]$。\n\n1.  **推导估计量：**\n    我们使用重参数化 $Z = \\theta + \\sigma\\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, 1)$。\n    $J(\\theta) = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,1)}[f(\\theta+\\sigma\\epsilon)]$。\n    我们可以交换期望和微分：\n    $$\n    \\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\epsilon}[\\nabla_{\\theta} f(\\theta+\\sigma\\epsilon)]\n    $$\n    一个无偏的单样本估计量是 $g_{RP}(\\epsilon) = \\nabla_{\\theta} f(\\theta+\\sigma\\epsilon)$。\n    当 $f(z) = z^2$ 时，我们有 $f(\\theta+\\sigma\\epsilon) = (\\theta+\\sigma\\epsilon)^2$。\n    估计量是：\n    $$\n    g_{RP}(\\epsilon) = \\frac{d}{d\\theta}(\\theta+\\sigma\\epsilon)^2 = 2(\\theta+\\sigma\\epsilon)\n    $$\n    用原始变量 $z$ 表示，通过代入 $\\epsilon = (z-\\theta)/\\sigma$，我们得到 $g_{RP}(z) = 2(\\theta + \\sigma\\frac{z-\\theta}{\\sigma}) = 2z$。\n    为了确认它是无偏的：$\\mathbb{E}[g_{RP}(Z)] = \\mathbb{E}[2Z] = 2\\mathbb{E}[Z] = 2\\theta$。这与 $\\nabla_{\\theta}J(\\theta)$ 相符。\n\n2.  **计算估计量的方差：**\n    $$\n    \\text{Var}(g_{RP}(Z)) = \\text{Var}(2Z)\n    $$\n    使用性质 $\\text{Var}(aX) = a^2\\text{Var}(X)$：\n    $$\n    \\text{Var}(g_{RP}(Z)) = 2^2 \\text{Var}(Z) = 4\\sigma^2\n    $$\n\n**C. 方差之比**\n\n最后，我们计算得分函数估计量的方差与重参数化估计量的方差之比。\n$$\n\\text{Ratio} = \\frac{\\text{Var}(g_{SF}(Z))}{\\text{Var}(g_{RP}(Z))} = \\frac{\\frac{\\theta^4}{\\sigma^2} + 14\\theta^2 + 15\\sigma^2}{4\\sigma^2}\n$$\n化简表达式：\n$$\n\\text{Ratio} = \\frac{\\theta^4}{4\\sigma^4} + \\frac{14\\theta^2}{4\\sigma^2} + \\frac{15\\sigma^2}{4\\sigma^2} = \\frac{\\theta^4}{4\\sigma^4} + \\frac{7\\theta^2}{2\\sigma^2} + \\frac{15}{4}\n$$\n这是方差之比的最终闭式解析表达式。",
            "answer": "$$\n\\boxed{\\frac{\\theta^4}{4\\sigma^4} + \\frac{7\\theta^2}{2\\sigma^2} + \\frac{15}{4}}\n$$"
        },
        {
            "introduction": "在评估深度学习模型时，我们不仅关心其预测的准确性，还关心其预测的置信度。集成学习（Ensemble learning）是一种强大的技术，它通过组合多个模型的预测来提高性能并量化不确定性。本练习  将指导你运用总方差定律（Law of Total Variance）这一基本概率论工具，将集成模型预测的总不确定性分解为两个有意义的部分：认知不确定性（epistemic uncertainty，源于模型自身知识的局限）和偶然不确定性（aleatoric uncertainty，源于数据固有的随机性）。通过编写代码来实证这一分解，你将获得一个用于诊断模型、建立更可靠系统的实用分析方法。",
            "id": "3166709",
            "problem": "考虑一个监督式深度学习场景，其中一组独立训练的神经网络（NN）构成一个集成。对于一个固定的输入数据集，令 $N$ 表示不同输入的数量，令 $K$ 表示集成成员的数量。对于每个输入索引 $i \\in \\{1,\\dots,N\\}$ 和集成成员索引 $m \\in \\{1,\\dots,K\\}$，将其标量预测表示为 $y_{i,m} \\in \\mathbb{R}$。通过从 $N$ 个输入中均匀随机选择一个来定义随机输入 $X$，并通过从 $K$ 个集成成员中均匀随机选择一个并取其在所选输入上的预测来定义随机预测 $Y$。仅使用期望和方差的核心定义以及条件期望的定义，推导以 $X$ 为条件的方差分解，然后应用此分解，从集成预测的有限矩阵 $\\{y_{i,m}\\}$ 中经验性地量化两个构成项。\n\n您必须从以下基本概念出发：\n- 随机变量 $Z$ 的方差定义为 $\\mathrm{Var}(Z) = \\mathbb{E}\\big[(Z - \\mathbb{E}[Z])^2\\big] = \\mathbb{E}[Z^2] - \\big(\\mathbb{E}[Z]\\big)^2$。\n- 条件期望 $\\mathbb{E}[Z \\mid W]$ 定义为一个随机变量（$W$ 的函数），对于任何关于 $W$ 可测的事件 $A$，它都满足 $\\mathbb{E}[Z \\mathbf{1}_A] = \\mathbb{E}\\big[\\mathbb{E}[Z \\mid W] \\mathbf{1}_A\\big]$。\n- 全期望定律（塔性质）指出，对于任何一对随机变量 $(Z,W)$，有 $\\mathbb{E}[Z] = \\mathbb{E}\\big[\\mathbb{E}[Z \\mid W]\\big]$。\n\n您的程序必须：\n- 为每个测试用例使用以下模型生成合成的集成预测 $\\{y_{i,m}\\}$。令 $x_i \\in \\mathbb{R}$ 表示索引 $i$ 处的标量输入。定义一个确定性基函数 $f(x) = a x + c$，一个在 $m$ 间独立的逐模型偏差 $b_m \\sim \\mathcal{N}(0,\\tau^2)$，以及带有异方差标准差 $\\sigma(x) = \\sigma_0 + \\sigma_1 |x|$ 的逐预测噪声 $\\epsilon_{i,m} \\sim \\mathcal{N}\\big(0, \\sigma(x_i)^2\\big)$。然后将每个预测设置为\n$$\ny_{i,m} = f(x_i) + b_m + \\epsilon_{i,m}.\n$$\n在每个测试用例中，将经验分布视为在所有观测到的输入和集成成员上是均匀的。具体来说，使用经验期望，其计算方式为在有限集合上求平均值，分母分别为 $N$、$K$ 和 $N K$（不使用无偏修正）：\n- 对于每个输入 $i$，计算条件均值 $\\mu_i = \\frac{1}{K} \\sum_{m=1}^{K} y_{i,m}$ 和条件方差 $v_i = \\frac{1}{K} \\sum_{m=1}^{K} \\big(y_{i,m} - \\mu_i\\big)^2$。\n- 将两个分解项和总方差的经验模拟计算为\n$$\nA = \\frac{1}{N} \\sum_{i=1}^{N} v_i, \\quad\nB = \\frac{1}{N} \\sum_{i=1}^{N} \\big(\\mu_i - \\bar{\\mu}\\big)^2, \\quad\nC = \\frac{1}{N K} \\sum_{i=1}^{N} \\sum_{m=1}^{K} \\big(y_{i,m} - \\bar{y}\\big)^2,\n$$\n其中 $\\bar{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} \\mu_i$ 且 $\\bar{y} = \\frac{1}{N K} \\sum_{i=1}^{N} \\sum_{m=1}^{K} y_{i,m}$。同时计算差异\n$$\nD = C - (A + B),\n$$\n如果使用经验期望正确实现了分解，该值在数值上必须接近于0。\n\n实现并评估以下测试套件，其中每个用例提供 $(N,K,a,c,\\tau,\\sigma_0,\\sigma_1,\\text{seed})$，并且输入 $x_i$ 是使用提供的种子从 $\\mathcal{N}(0,1)$ 中独立抽取的：\n- 测试用例1（正常路径，异方差，中等集成多样性）：$N=200$，$K=5$，$a=2.0$，$c=0.5$，$\\tau=0.3$，$\\sigma_0=0.2$，$\\sigma_1=0.5$，$\\text{seed}=1234$。\n- 测试用例2（边界情况，无输入内变异性）：$N=100$，$K=7$，$a=1.5$，$c=-1.0$，$\\tau=0.0$，$\\sigma_0=0.0$，$\\sigma_1=0.0$，$\\text{seed}=42$。\n- 测试用例3（边界情况，集成均值无输入间变异性）：$N=150$，$K=4$，$a=0.0$，$c=3.0$，$\\tau=0.8$，$\\sigma_0=0.5$，$\\sigma_1=0.0$，$\\text{seed}=7$。\n- 测试用例4（边缘情况，单一输入）：$N=1$，$K=10$，$a=0.7$，$c=0.0$，$\\tau=1.0$，$\\sigma_0=0.3$，$\\text{seed}=31415$。\n- 测试用例5（边缘情况，单一模型）：$N=120$，$K=1$，$a=1.0$，$c=0.0$，$\\tau=0.0$，$\\sigma_0=0.2$，$\\sigma_1=0.0$，$\\text{seed}=98765$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含所有测试用例的结果，格式为方括号内以逗号分隔的列表，每个测试用例报告为一个由四个浮点数组成的子列表 $[C,A,B,D]$，保留六位小数。例如，输出必须如下所示\n$$\n[[C_1,A_1,B_1,D_1],[C_2,A_2,B_2,D_2],\\dots,[C_5,A_5,B_5,D_5]].\n$$\n打印行中的任何位置都不允许有空格。",
            "solution": "该问题要求推导全方差定律，并将其应用于一个有限的集成预测集合。解决方案分为两部分：首先，方差分解的理论推导；其次，证明该恒等式对于所提供的特定经验估计量在代数上成立。\n\n### 第1部分：全方差定律的推导\n\n目标是证明恒等式 $\\mathrm{Var}(Y) = \\mathbb{E}[\\mathrm{Var}(Y \\mid X)] + \\mathrm{Var}(\\mathbb{E}[Y \\mid X])$。我们从提供的基本定义开始。\n\n随机变量 $Y$ 的方差定义为：\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2\n$$\n我们将全期望定律 $\\mathbb{E}[Z] = \\mathbb{E}\\big[\\mathbb{E}[Z \\mid W]\\big]$ 应用于右侧的每一项，以随机变量 $X$ 为条件。\n\n对于项 $\\mathbb{E}[Y]$，我们有：\n$$\n\\mathbb{E}[Y] = \\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]\n$$\n其中外部期望的下标 $\\mathbb{E}_X[\\cdot]$ 强调期望是针对 $X$ 的分布计算的。\n\n对于项 $\\mathbb{E}[Y^2]$，我们同样有：\n$$\n\\mathbb{E}[Y^2] = \\mathbb{E}_X[\\mathbb{E}[Y^2 \\mid X]]\n$$\n将这些代入方差定义中，得到：\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_X[\\mathbb{E}[Y^2 \\mid X]] - \\big(\\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]\\big)^2\n$$\n现在，我们使用条件方差的定义，$\\mathrm{Var}(Y \\mid X) = \\mathbb{E}[Y^2 \\mid X] - (\\mathbb{E}[Y \\mid X])^2$。这是一个作为 $X$ 函数的随机变量。我们可以重新排列这个定义来表示 $\\mathbb{E}[Y^2 \\mid X]$：\n$$\n\\mathbb{E}[Y^2 \\mid X] = \\mathrm{Var}(Y \\mid X) + (\\mathbb{E}[Y \\mid X])^2\n$$\n将 $\\mathbb{E}[Y^2 \\mid X]$ 的这个表达式代回到我们的 $\\mathrm{Var}(Y)$ 方程中：\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_X\\big[ \\mathrm{Var}(Y \\mid X) + (\\mathbb{E}[Y \\mid X])^2 \\big] - \\big(\\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]\\big)^2\n$$\n利用外部期望 $\\mathbb{E}_X[\\cdot]$ 的期望线性性，我们可以拆分第一项：\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}_X[\\mathrm{Var}(Y \\mid X)] + \\mathbb{E}_X\\big[(\\mathbb{E}[Y \\mid X])^2\\big] - \\big(\\mathbb{E}_X[\\mathbb{E}[Y \\mid X]]\\big)^2\n$$\n我们来考察最后两项。如果我们定义一个新的随机变量 $Z = \\mathbb{E}[Y \\mid X]$，它是 $X$ 的函数，那么这两项就是 $\\mathbb{E}_X[Z^2] - (\\mathbb{E}_X[Z])^2$。这正是 $Z$ 的方差的定义，即 $\\mathrm{Var}(Z) = \\mathrm{Var}_X(\\mathbb{E}[Y \\mid X])$。\n\n因此，我们得到最终的分解式：\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}[\\mathrm{Var}(Y \\mid X)] + \\mathrm{Var}(\\mathbb{E}[Y \\mid X])\n$$\n第一项 $\\mathbb{E}[\\mathrm{Var}(Y \\mid X)]$ 是期望条件方差，通常称为“偶然不确定性”或“不可约减不确定性”。它代表了对于一个固定输入，预测的平均方差。第二项 $\\mathrm{Var}(\\mathbb{E}[Y \\mid X])$ 是条件期望的方差，通常称为“认知不确定性”或模型分歧。它代表了集成平均预测在不同输入间的变异性。\n\n### 第2部分：作为代数恒等式的经验分解\n\n问题基于一个有限的预测矩阵 $\\{y_{i,m}\\}$（其中 $i \\in \\{1, \\dots, N\\}$ 和 $m \\in \\{1, \\dots, K\\}$）来定义随机变量 $X$ 和 $Y$。抽样过程是均匀的，意味着任何一对 $(i,m)$ 被选中的概率为 $1/(NK)$。经验量 $A$、$B$ 和 $C$ 定义如下：\n$$\nA = \\frac{1}{N} \\sum_{i=1}^{N} v_i \\quad \\text{其中} \\quad v_i = \\frac{1}{K} \\sum_{m=1}^{K} \\big(y_{i,m} - \\mu_i\\big)^2 \\quad \\text{且} \\quad \\mu_i = \\frac{1}{K} \\sum_{m=1}^{K} y_{i,m}\n$$\n$$\nB = \\frac{1}{N} \\sum_{i=1}^{N} \\big(\\mu_i - \\bar{\\mu}\\big)^2 \\quad \\text{其中} \\quad \\bar{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} \\mu_i\n$$\n$$\nC = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} \\big(y_{i,m} - \\bar{y}\\big)^2 \\quad \\text{其中} \\quad \\bar{y} = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} y_{i,m}\n$$\n这些量是理论分解中各项的经验模拟。我们证明对于这些特定定义，恒等式 $C = A + B$ 在代数上成立。\n\n首先，请注意总均值 $\\bar{y}$ 与条件均值的均值 $\\bar{\\mu}$ 是相同的：\n$$\n\\bar{\\mu} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\frac{1}{K} \\sum_{m=1}^{K} y_{i,m}\\right) = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} y_{i,m} = \\bar{y}\n$$\n现在，我们展开总方差 $C$ 的表达式。我们在平方项内部加上并减去条件均值 $\\mu_i$：\n$$\nC = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} \\big( (y_{i,m} - \\mu_i) + (\\mu_i - \\bar{y}) \\big)^2\n$$\n展开平方项 $(p+q)^2 = p^2 + q^2 + 2pq$：\n$$\nC = \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} \\left[ (y_{i,m} - \\mu_i)^2 + (\\mu_i - \\bar{y})^2 + 2(y_{i,m} - \\mu_i)(\\mu_i - \\bar{y}) \\right]\n$$\n根据求和的线性性，我们可以将其拆分为三项：\n$$\nC = \\frac{1}{NK} \\sum_{i,m} (y_{i,m} - \\mu_i)^2 + \\frac{1}{NK} \\sum_{i,m} (\\mu_i - \\bar{y})^2 + \\frac{2}{NK} \\sum_{i,m} (y_{i,m} - \\mu_i)(\\mu_i - \\bar{y})\n$$\n我们来分析每一项：\n1.  **第一项**：\n    $$\n    \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} (y_{i,m} - \\mu_i)^2 = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{1}{K} \\sum_{m=1}^{K} (y_{i,m} - \\mu_i)^2 \\right) = \\frac{1}{N} \\sum_{i=1}^{N} v_i = A\n    $$\n2.  **第二项**：被加数 $(\\mu_i - \\bar{y})^2$ 不依赖于索引 $m$。对 $m$ 的求和会产生一个因子 $K$：\n    $$\n    \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{m=1}^{K} (\\mu_i - \\bar{y})^2 = \\frac{1}{NK} \\sum_{i=1}^{N} K (\\mu_i - \\bar{y})^2 = \\frac{1}{N} \\sum_{i=1}^{N} (\\mu_i - \\bar{y})^2 = B\n    $$\n    （因为 $\\bar{y}=\\bar{\\mu}$）\n3.  **第三项（交叉项）**：我们可以将不依赖于 $m$ 的项因子提出来：\n    $$\n    \\frac{2}{NK} \\sum_{i=1}^{N} (\\mu_i - \\bar{y}) \\left( \\sum_{m=1}^{K} (y_{i,m} - \\mu_i) \\right)\n    $$\n    根据 $\\mu_i$ 的定义，内部对 $m$ 的求和为零：\n    $$\n    \\sum_{m=1}^{K} (y_{i,m} - \\mu_i) = \\left( \\sum_{m=1}^{K} y_{i,m} \\right) - \\left( \\sum_{m=1}^{K} \\mu_i \\right) = (K \\mu_i) - (K \\mu_i) = 0\n    $$\n    因此，整个交叉项为零。\n\n综合这些结果，我们证明了对于给定的经验估计量：\n$$\nC = A + B\n$$\n这意味着差异 $D = C - (A+B)$ 必须恰好为 $0$。在数值实现中观察到的任何非零值都将完全由浮点表示误差引起。\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and empirically validates the law of total variance for an ensemble\n    of neural network predictions.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, K, a, c, tau, sigma0, sigma1, seed)\n        (200, 5, 2.0, 0.5, 0.3, 0.2, 0.5, 1234),\n        (100, 7, 1.5, -1.0, 0.0, 0.0, 0.0, 42),\n        (150, 4, 0.0, 3.0, 0.8, 0.5, 0.0, 7),\n        (1, 10, 0.7, 0.0, 1.0, 0.3, 0.0, 31415),\n        (120, 1, 1.0, 0.0, 0.0, 0.2, 0.0, 98765),\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        N, K, a, c, tau, sigma0, sigma1, seed = case\n        \n        # Set up a random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n        \n        # Generate synthetic data according to the model.\n        # 1. Inputs x_i ~ N(0, 1)\n        x = rng.normal(loc=0.0, scale=1.0, size=N)\n        \n        # 2. Per-model biases b_m ~ N(0, tau^2)\n        b = rng.normal(loc=0.0, scale=tau, size=K)\n        \n        # 3. Base function f(x_i)\n        f_x = a * x + c\n        \n        # 4. Heteroscedastic noise standard deviation sigma(x_i)\n        sigma_x = sigma0 + sigma1 * np.abs(x)\n        \n        # 5. Per-prediction noise epsilon_{i,m} ~ N(0, sigma(x_i)^2)\n        # We broadcast sigma_x (N,) to (N, K) to generate the noise matrix.\n        epsilon_scale = sigma_x[:, np.newaxis]\n        epsilon = rng.normal(loc=0.0, scale=epsilon_scale, size=(N, K))\n        \n        # 6. Final predictions y_{i,m}\n        # We use broadcasting to combine f_x (N,), b (K,), and epsilon (N, K).\n        y = f_x[:, np.newaxis] + b[np.newaxis, :] + epsilon\n        \n        # Compute the empirical quantities as defined in the problem.\n        # ddof=0 ensures we divide by n (population variance) instead of n-1.\n        \n        # Conditional mean for each input i: mu_i\n        mu = np.mean(y, axis=1)  # Shape (N,)\n        \n        # Conditional variance for each input i: v_i\n        v = np.var(y, axis=1, ddof=0)  # Shape (N,)\n        \n        # Term A: Expected value of the conditional variance\n        # A = (1/N) * sum(v_i)\n        A = np.mean(v)\n        \n        # Term B: Variance of the conditional expectation\n        # B = (1/N) * sum((mu_i - bar_mu)^2)\n        # np.var calculates variance against the mean of the input array `mu`.\n        B = np.var(mu, ddof=0)\n        \n        # Term C: Total variance\n        # C = (1/NK) * sum((y_{i,m} - bar_y)^2)\n        # np.var applied to the entire matrix calculates this.\n        C = np.var(y, ddof=0)\n        \n        # Discrepancy D = C - (A + B)\n        # This should be zero up to floating-point precision, as shown in the derivation.\n        D = C - (A + B)\n        \n        case_results = [C, A, B, D]\n        # Format the results for this case to 6 decimal places.\n        case_str = f\"[{','.join([f'{val:.6f}' for val in case_results])}]\"\n        all_results_str.append(case_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```",
            "answer": "[[4.512683,0.301548,4.211135,0.000000],[2.259974,0.000000,2.259974,0.000000],[0.866504,0.250000,0.616504,0.000000],[1.066442,0.090000,0.976442,0.000000],[1.006456,0.040000,0.966456,0.000000]]"
        }
    ]
}