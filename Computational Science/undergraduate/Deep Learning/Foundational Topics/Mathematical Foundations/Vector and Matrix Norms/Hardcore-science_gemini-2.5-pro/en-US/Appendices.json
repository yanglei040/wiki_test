{
    "hands_on_practices": [
        {
            "introduction": "A firm grasp of vector and matrix norms begins with the ability to compute them directly from their definitions. This exercise provides essential practice by asking you to calculate four of the most common matrix norms for the all-ones matrix, a simple yet illustrative case. By working through the 1-norm, 2-norm, $\\infty$-norm, and Frobenius norm, you will solidify your understanding of how each norm captures a different aspect of a matrix's magnitude .",
            "id": "2449594",
            "problem": "Let $m,n \\in \\mathbb{N}$ with $m \\geq 1$ and $n \\geq 1$. For a matrix $A \\in \\mathbb{R}^{m \\times n}$ and $p \\in \\{1,2,\\infty\\}$, the operator norm induced by the vector $p$-norm is defined by\n$$\n\\|A\\|_{p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}},\n$$\nwhere, for $x \\in \\mathbb{R}^{k}$, the vector norms are $\\|x\\|_{1} = \\sum_{i=1}^{k} |x_{i}|$, $\\|x\\|_{2} = \\sqrt{\\sum_{i=1}^{k} |x_{i}|^{2}}$, and $\\|x\\|_{\\infty} = \\max_{1 \\leq i \\leq k} |x_{i}|$. The Frobenius norm of $A$ is defined by\n$$\n\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^{2}}.\n$$\nConsider the all-ones matrix $J \\in \\mathbb{R}^{m \\times n}$ with entries $J_{ij} = 1$ for all $i,j$. Determine $\\|J\\|_{1}$, $\\|J\\|_{2}$, $\\|J\\|_{\\infty}$, and $\\|J\\|_{F}$ as explicit functions of $m$ and $n$. Provide your final answer as a single row vector in the order $\\big(\\|J\\|_{1}, \\|J\\|_{2}, \\|J\\|_{\\infty}, \\|J\\|_{F}\\big)$. The answer must be exact; do not approximate.",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- $m, n \\in \\mathbb{N}$ with $m \\geq 1$ and $n \\geq 1$.\n- Matrix $A \\in \\mathbb{R}^{m \\times n}$.\n- Operator norm: $\\|A\\|_{p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}}$ for $p \\in \\{1, 2, \\infty\\}$.\n- Vector norms for $x \\in \\mathbb{R}^{k}$:\n  - $\\|x\\|_{1} = \\sum_{i=1}^{k} |x_{i}|$\n  - $\\|x\\|_{2} = \\sqrt{\\sum_{i=1}^{k} |x_{i}|^{2}}$\n  - $\\|x\\|_{\\infty} = \\max_{1 \\leq i \\leq k} |x_{i}|$\n- Frobenius norm: $\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^{2}}$.\n- The matrix under consideration is the all-ones matrix $J \\in \\mathbb{R}^{m \\times n}$ with entries $J_{ij} = 1$ for all $i,j$.\n- The objective is to determine $\\|J\\|_{1}$, $\\|J\\|_{2}$, $\\|J\\|_{\\infty}$, and $\\|J\\|_{F}$ as explicit functions of $m$ and $n$.\n- The final answer is required in the format of a row vector: $\\big(\\|J\\|_{1}, \\|J\\|_{2}, \\|J\\|_{\\infty}, \\|J\\|_{F}\\big)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed against the required criteria.\n- **Scientifically Grounded:** The problem uses standard, universally accepted definitions for vector and matrix norms from the field of linear algebra and numerical analysis. It is scientifically and mathematically sound.\n- **Well-Posed:** The problem is clearly stated. The matrix $J$ is unambiguously defined, as are the norms to be computed. A unique, meaningful solution exists for each of the four requested quantities.\n- **Objective:** The problem is formulated with precise mathematical language, devoid of any subjectivity or ambiguity.\n\nThe problem is found to be free of any flaws such as scientific unsoundness, incompleteness, or contradiction.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A complete, reasoned solution will be provided.\n\nWe proceed to calculate the four specified norms for the all-ones matrix $J \\in \\mathbb{R}^{m \\times n}$.\n\n**1. Calculation of the $1$-norm, $\\|J\\|_{1}$**\nThe operator $1$-norm of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined as the maximum absolute column sum:\n$$\n\\|A\\|_{1} = \\max_{1 \\leq j \\leq n} \\sum_{i=1}^{m} |a_{ij}|\n$$\nFor the matrix $J$, all entries are $J_{ij} = 1$. We compute the sum of the absolute values for an arbitrary column $j$:\n$$\n\\sum_{i=1}^{m} |J_{ij}| = \\sum_{i=1}^{m} |1| = \\sum_{i=1}^{m} 1 = m\n$$\nThis sum is constant for every column $j$, where $1 \\leq j \\leq n$. Therefore, the maximum of these identical sums is $m$.\n$$\n\\|J\\|_{1} = m\n$$\n\n**2. Calculation of the $\\infty$-norm, $\\|J\\|_{\\infty}$**\nThe operator $\\infty$-norm of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined as the maximum absolute row sum:\n$$\n\\|A\\|_{\\infty} = \\max_{1 \\leq i \\leq m} \\sum_{j=1}^{n} |a_{ij}|\n$$\nFor the matrix $J$, we compute the sum of the absolute values for an arbitrary row $i$:\n$$\n\\sum_{j=1}^{n} |J_{ij}| = \\sum_{j=1}^{n} |1| = \\sum_{j=1}^{n} 1 = n\n$$\nThis sum is constant for every row $i$, where $1 \\leq i \\leq m$. The maximum of these identical sums is $n$.\n$$\n\\|J\\|_{\\infty} = n\n$$\n\n**3. Calculation of the Frobenius norm, $\\|J\\|_{F}$**\nThe Frobenius norm of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined by:\n$$\n\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^{2}}\n$$\nFor the matrix $J$, this becomes:\n$$\n\\|J\\|_{F} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |1|^{2}} = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} 1}\n$$\nThe double summation represents the total number of entries in the matrix, which is $mn$.\n$$\n\\|J\\|_{F} = \\sqrt{mn}\n$$\n\n**4. Calculation of the $2$-norm (Spectral Norm), $\\|J\\|_{2}$**\nThe operator $2$-norm of a matrix $A$ is its largest singular value, $\\sigma_{\\max}(A)$. This is equivalent to the square root of the largest eigenvalue of the matrix $A^T A$.\n$$\n\\|A\\|_{2} = \\sqrt{\\lambda_{\\max}(A^T A)}\n$$\nFor our matrix $J$, we first construct the matrix $J^T J$. The matrix $J$ is an $m \\times n$ matrix of all ones, so its transpose $J^T$ is an $n \\times m$ matrix of all ones. The product $J^T J$ is an $n \\times n$ matrix. An entry $(k,l)$ of this product is:\n$$\n(J^T J)_{kl} = \\sum_{i=1}^{m} (J^T)_{ki} J_{il} = \\sum_{i=1}^{m} J_{ik} J_{il} = \\sum_{i=1}^{m} 1 \\cdot 1 = m\n$$\nThus, $J^T J$ is an $n \\times n$ matrix where every entry is equal to $m$. We can write this as $J^T J = m U_n$, where $U_n$ is the $n \\times n$ matrix of all ones. The eigenvalues of $J^T J$ are $m$ times the eigenvalues of $U_n$.\n\nThe matrix $U_n$ has rank $1$, since all its columns are identical. A matrix of rank $k$ has at least $n-k$ zero eigenvalues. Thus, $U_n$ has an eigenvalue of $0$ with multiplicity of at least $n-1$. The sum of the eigenvalues of a matrix equals its trace. The trace of $U_n$ is $\\text{Tr}(U_n) = \\sum_{i=1}^{n} 1 = n$. Since $n-1$ eigenvalues are $0$, the remaining eigenvalue must be $n$. So, the eigenvalues of $U_n$ are $n$ (with multiplicity $1$) and $0$ (with multiplicity $n-1$).\n\nThe eigenvalues of $J^T J = m U_n$ are therefore $mn$ (multiplicity $1$) and $0$ (multiplicity $n-1$). The largest eigenvalue is $\\lambda_{\\max}(J^T J) = mn$.\nThe $2$-norm is the square root of this value:\n$$\n\\|J\\|_{2} = \\sqrt{\\lambda_{\\max}(J^T J)} = \\sqrt{mn}\n$$\n\nIn summary, the four computed norms are:\n- $\\|J\\|_{1} = m$\n- $\\|J\\|_{2} = \\sqrt{mn}$\n- $\\|J\\|_{\\infty} = n$\n- $\\|J\\|_{F} = \\sqrt{mn}$\n\nThe final answer is presented as a row vector in the specified order.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} m & \\sqrt{mn} & n & \\sqrt{mn} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Moving beyond pure calculation, it is crucial to understand the deeper geometric and algebraic implications of norm properties. The triangle inequality, $\\| \\mathbf{u} + \\mathbf{v} \\| \\le \\| \\mathbf{u} \\| + \\| \\mathbf{v} \\|$, is a cornerstone of vector spaces, but exploring the specific case of equality reveals a profound connection between a matrix transformation and its invariant directions. This problem challenges you to construct a scenario where the triangle inequality for vectors $A\\mathbf{x}$ and $\\mathbf{x}$ becomes an equality, thereby uncovering the fundamental role that eigenvectors and non-negative eigenvalues play in determining how a matrix stretches space .",
            "id": "2447194",
            "problem": "In a linear state-transition setting often used in computational economics and finance, consider a real $2 \\times 2$ matrix $A$ and the identity matrix $I$. The Euclidean vector norm $\\|\\cdot\\|_2$ is used throughout. Construct a non-diagonal real $2 \\times 2$ matrix $A$ and find a nonzero vector $x \\in \\mathbb{R}^2$ such that the triangle inequality\n$$\n\\|(A+I)x\\|_2 \\leq \\|Ax\\|_2 + \\|x\\|_2\n$$\nholds with equality. Define the ratio\n$$\nR \\equiv \\frac{\\|(A+I)x\\|_2}{\\|Ax\\|_2 + \\|x\\|_2}.\n$$\nCompute the exact value of $R$ for your constructed pair $(A,x)$, and briefly interpret the condition on $(A,x)$ under which equality in the triangle inequality is achieved. Your final reported answer must be the exact value of $R$ (no rounding required).",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- A real $2 \\times 2$ non-diagonal matrix $A$.\n- The identity matrix $I$.\n- A nonzero vector $\\mathbf{x} \\in \\mathbb{R}^2$.\n- Norm: The Euclidean vector norm $\\|\\cdot\\|_2$.\n- Inequality: $\\|(A+I)\\mathbf{x}\\|_2 \\le \\|A\\mathbf{x}\\|_2 + \\|\\mathbf{x}\\|_2$.\n- Task: Construct $A$ and $\\mathbf{x}$ such that the inequality holds as an equality.\n- Definition: Ratio $R \\equiv \\frac{\\|(A+I)\\mathbf{x}\\|_2}{\\|A\\mathbf{x}\\|_2 + \\|\\mathbf{x}\\|_2}$.\n- Task: Compute the exact value of $R$ for the constructed pair $(A,\\mathbf{x})$.\n- Task: Interpret the condition for equality.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the established principles of linear algebra and vector norm theory. It is well-posed, as a valid construction is possible and leads to a unique value for the specified ratio. The problem is stated with objective and precise mathematical language, is self-contained, and free of contradictions. The scenario is a standard exercise in the study of linear operators.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A full solution will be provided.\n\nThe problem requires constructing a matrix $A$ and a vector $\\mathbf{x}$ such that the triangle inequality for vectors becomes an equality. In general, for any two vectors $\\mathbf{u}$ and $\\mathbf{v}$ in an inner product space, the triangle inequality is $\\|\\mathbf{u}+\\mathbf{v}\\| \\le \\|\\mathbf{u}\\| + \\|\\mathbf{v}\\|$. Equality holds if and only if one vector is a non-negative scalar multiple of the other.\n\nIn this problem, the vectors are $\\mathbf{u} = A\\mathbf{x}$ and $\\mathbf{v} = I\\mathbf{x} = \\mathbf{x}$. The given expression is $\\|A\\mathbf{x}+\\mathbf{x}\\|_2 \\le \\|A\\mathbf{x}\\|_2 + \\|\\mathbf{x}\\|_2$. For this to hold with equality, we must satisfy the condition $A\\mathbf{x} = k\\mathbf{x}$ for some real scalar $k \\ge 0$. The case $\\mathbf{x} = k(A\\mathbf{x})$ is also possible, but since $\\mathbf{x}$ is non-zero, $A$ would need to be invertible and this reduces to $A^{-1}\\mathbf{x} = k\\mathbf{x}$, which is equivalent to the first condition with eigenvalue $\\frac{1}{k}$. We proceed with the simpler form.\n\nThe condition $A\\mathbf{x} = k\\mathbf{x}$ means that $\\mathbf{x}$ must be an eigenvector of the matrix $A$, and the corresponding eigenvalue $k$ must be non-negative. The problem states that $\\mathbf{x}$ must be a nonzero vector, which is consistent with the definition of an eigenvector.\n\nOur task is therefore to:\n$1$. Construct a real, non-diagonal $2 \\times 2$ matrix $A$.\n$2$. Ensure $A$ has at least one non-negative eigenvalue, $\\lambda \\ge 0$.\n$3$. Find a corresponding eigenvector $\\mathbf{x}$.\n$4$. Calculate the ratio $R$ for this pair $(A,\\mathbf{x})$.\n\nLet us construct such a matrix $A$. We can design $A$ to have simple, non-negative integer eigenvalues. Let the eigenvalues be $\\lambda_1 = 1$ and $\\lambda_2 = 4$. Both are non-negative.\nFor a $2 \\times 2$ matrix, the sum of eigenvalues is the trace, and the product is the determinant.\n$\\mathrm{Tr}(A) = \\lambda_1 + \\lambda_2 = 1+4 = 5$.\n$\\det(A) = \\lambda_1 \\lambda_2 = 1 \\times 4 = 4$.\n\nLet $A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$. We need $a+d=5$ and $ad-bc=4$. To ensure $A$ is non-diagonal, we require that $b$ and $c$ are not both zero.\nLet us choose $a=3$. Then $d=5-3=2$.\nThe determinant condition becomes $(3)(2) - bc = 4$, which simplifies to $6 - bc = 4$, or $bc=2$.\nWe can choose $b=2$ and $c=1$. This gives the matrix:\n$$\nA = \\begin{pmatrix} 3 & 2 \\\\ 1 & 2 \\end{pmatrix}\n$$\nThis matrix is real, $2 \\times 2$, and non-diagonal. Its eigenvalues are, by construction, $\\lambda_1 = 1$ and $\\lambda_2 = 4$, which are both non-negative.\n\nNow, we find an eigenvector $\\mathbf{x}$ for one of these eigenvalues. Let us choose $\\lambda = 4$. We need to solve the system $(A - \\lambda I)\\mathbf{x} = \\mathbf{0}$, which is $(A - 4I)\\mathbf{x} = \\mathbf{0}$.\n$$\n(A - 4I) = \\begin{pmatrix} 3-4 & 2 \\\\ 1 & 2-4 \\end{pmatrix} = \\begin{pmatrix} -1 & 2 \\\\ 1 & -2 \\end{pmatrix}\n$$\nThe equation to solve is:\n$$\n\\begin{pmatrix} -1 & 2 \\\\ 1 & -2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis yields the single independent equation $-x_1 + 2x_2 = 0$, or $x_1 = 2x_2$.\nWe must choose a nonzero vector $\\mathbf{x}$. A simple choice is $x_2=1$, which gives $x_1=2$.\nSo, our constructed vector is $\\mathbf{x} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$.\n\nWe have now constructed the pair $(A,\\mathbf{x})$:\n$A = \\begin{pmatrix} 3 & 2 \\\\ 1 & 2 \\end{pmatrix}$ and $\\mathbf{x} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$.\n$A$ is non-diagonal and $\\mathbf{x}$ is non-zero. The corresponding eigenvalue is $\\lambda=4 \\ge 0$, so the equality condition $A\\mathbf{x} = 4\\mathbf{x}$ is met.\n\nNow we compute the ratio $R$.\n$$\nR = \\frac{\\|(A+I)\\mathbf{x}\\|_2}{\\|A\\mathbf{x}\\|_2 + \\|\\mathbf{x}\\|_2}\n$$\nBy our theoretical deduction, since the condition for equality in the triangle inequality is satisfied, the numerator and denominator must be equal, so $R$ must be $1$. We verify this by direct calculation.\n\nFirst, calculate the terms in the denominator:\n$A\\mathbf{x} = 4\\mathbf{x} = 4 \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}$.\n$\\|A\\mathbf{x}\\|_2 = \\left\\| \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix} \\right\\|_2 = \\sqrt{8^2 + 4^2} = \\sqrt{64+16} = \\sqrt{80} = 4\\sqrt{5}$.\n$\\|\\mathbf{x}\\|_2 = \\left\\| \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} \\right\\|_2 = \\sqrt{2^2 + 1^2} = \\sqrt{4+1} = \\sqrt{5}$.\nThe denominator is $\\|A\\mathbf{x}\\|_2 + \\|\\mathbf{x}\\|_2 = 4\\sqrt{5} + \\sqrt{5} = 5\\sqrt{5}$.\n\nNext, calculate the term in the numerator:\n$(A+I)\\mathbf{x} = A\\mathbf{x} + \\mathbf{x}$.\nSince $A\\mathbf{x}=4\\mathbf{x}$, we have $A\\mathbf{x}+\\mathbf{x} = 4\\mathbf{x}+\\mathbf{x}=5\\mathbf{x}$.\n$(A+I)\\mathbf{x} = 5\\mathbf{x} = 5 \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ 5 \\end{pmatrix}$.\n$\\|(A+I)\\mathbf{x}\\|_2 = \\left\\| \\begin{pmatrix} 10 \\\\ 5 \\end{pmatrix} \\right\\|_2 = \\sqrt{10^2 + 5^2} = \\sqrt{100+25} = \\sqrt{125} = 5\\sqrt{5}$.\n\nFinally, compute the ratio $R$.\n$$\nR = \\frac{5\\sqrt{5}}{5\\sqrt{5}} = 1\n$$\nThe calculation confirms the theoretical expectation.\n\nThe interpretation of the condition for equality is as follows: The equality $\\|(A+I)\\mathbf{x}\\|_2 = \\|A\\mathbf{x}\\|_2 + \\|\\mathbf{x}\\|_2$ holds if and only if the vector $A\\mathbf{x}$ is a non-negative scalar multiple of the vector $\\mathbf{x}$. Given that $\\mathbf{x}$ must be non-zero, this is precisely the definition of $\\mathbf{x}$ being an eigenvector of the matrix $A$ corresponding to a non-negative eigenvalue.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Matrix norms are not just theoretical curiosities; they are indispensable tools for analyzing the real-world behavior of computational models. In nearly all scientific computing, from training neural networks to simulating physical systems, we must understand how sensitive our results are to small errors or perturbations in our model parameters. This hands-on coding exercise demonstrates this principle by exploring the relationship between a matrix's condition number, $\\kappa(A)$, and the stability of the linear system $A\\mathbf{x} = \\mathbf{b}$. By experimentally verifying this classic inequality, you will gain a tangible understanding of how norms are used to predict and bound the potential for error amplification in numerical computations .",
            "id": "2449152",
            "problem": "Consider a nonsingular real matrix $A \\in \\mathbb{R}^{n \\times n}$ and a right-hand side vector $\\mathbf{b} \\in \\mathbb{R}^{n}$. Let $\\mathbf{x}$ be the unique solution to $A \\mathbf{x} = \\mathbf{b}$. Introduce a perturbation $\\delta A$ to the matrix and define the perturbed solution $\\hat{\\mathbf{x}}$ as the unique solution to $(A + \\delta A)\\hat{\\mathbf{x}} = \\mathbf{b}$. For the Euclidean vector norm $\\|\\cdot\\|_2$ and the induced spectral matrix norm $\\|\\cdot\\|_2$, the condition number is $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. You will experimentally verify the inequality\n$$\n\\frac{\\|\\delta \\mathbf{x}\\|_2}{\\|\\mathbf{x}\\|_2} \\le \\kappa_2(A) \\frac{\\|\\delta A\\|_2}{\\|A\\|_2},\n$$\nwhere $\\delta \\mathbf{x} = \\hat{\\mathbf{x}} - \\mathbf{x}$.\n\nYour task is to write a complete program that, for each test case below, constructs the perturbation as $\\delta A = \\varepsilon A$ with a specified scalar $\\varepsilon$, computes $\\mathbf{x}$ and $\\hat{\\mathbf{x}}$ from the definitions above, evaluates both sides of the inequality using the Euclidean norm for vectors and the induced spectral norm for matrices, and outputs whether the inequality holds.\n\nTest suite (each case provides $(A, \\mathbf{b}, \\varepsilon)$):\n\n$1.$ $A_1 = \\begin{bmatrix} 3 & 1 \\\\ 0 & 2 \\end{bmatrix}$, $\\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$, $\\varepsilon_1 = 10^{-3}$.\n\n$2.$ $A_2 = \\begin{bmatrix} 1 & \\tfrac{1}{2} & \\tfrac{1}{3} \\\\ \\tfrac{1}{2} & \\tfrac{1}{3} & \\tfrac{1}{4} \\\\ \\tfrac{1}{3} & \\tfrac{1}{4} & \\tfrac{1}{5} \\end{bmatrix}$, $\\mathbf{b}_2 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $\\varepsilon_2 = 10^{-6}$.\n\n$3.$ $A_3 = \\begin{bmatrix} 0 & -1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & -1 & 0 \\end{bmatrix}$, $\\mathbf{b}_3 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}$, $\\varepsilon_3 = 10^{-4}$.\n\n$4.$ $A_4 = \\operatorname{diag}(1, 2, 3)$, $\\mathbf{b}_4 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}$, $\\varepsilon_4 = 0$.\n\n$5.$ $A_5 = \\operatorname{diag}\\!\\big(1, 10^{-3}, 10^{-6}\\big)$, $\\mathbf{b}_5 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $\\varepsilon_5 = 10^{-12}$.\n\nRequired computations for each test case $i \\in \\{1,2,3,4,5\\}$:\n\n$\\,\\,\\,$• Compute $\\mathbf{x}_i$ from $A_i \\mathbf{x}_i = \\mathbf{b}_i$ and $\\hat{\\mathbf{x}}_i$ from $(A_i + \\delta A_i)\\hat{\\mathbf{x}}_i = \\mathbf{b}_i$ with $\\delta A_i = \\varepsilon_i A_i$.\n\n$\\,\\,\\,$• Compute $\\|\\delta \\mathbf{x}_i\\|_2 / \\|\\mathbf{x}_i\\|_2$ where $\\delta \\mathbf{x}_i = \\hat{\\mathbf{x}}_i - \\mathbf{x}_i$.\n\n$\\,\\,\\,$• Compute $\\kappa_2(A_i) = \\|A_i\\|_2 \\|A_i^{-1}\\|_2$ and the bound $\\kappa_2(A_i) \\|\\delta A_i\\|_2 / \\|A_i\\|_2$.\n\n$\\,\\,\\,$• Determine the boolean result of the inequality comparison.\n\nFinal output format: Your program should produce a single line of output containing the $5$ boolean results, in order from case $1$ to case $5$, as a comma-separated list enclosed in square brackets, for example $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True},\\mathrm{False}]$. No additional text should be printed. Angle units do not apply. No physical units apply. The only accepted norms are the Euclidean vector norm $\\|\\cdot\\|_2$ and the induced spectral matrix norm $\\|\\cdot\\|_2$ as specified above.",
            "solution": "The problem statement is subjected to validation and is found to be valid. It is scientifically grounded in the principles of numerical linear algebra, specifically the theory of matrix condition numbers and error analysis for linear systems. The problem is well-posed, with all matrices $A_i$ being nonsingular and all perturbations $\\delta A_i = \\varepsilon_i A_i$ being small enough to ensure the perturbed matrix $A_i + \\delta A_i = (1+\\varepsilon_i)A_i$ is also nonsingular. The problem is objective, self-contained, and provides all necessary data for a unique solution.\n\nThe core of the problem is to experimentally verify a fundamental inequality in numerical analysis. Given a nonsingular linear system $A\\mathbf{x} = \\mathbf{b}$, where $A \\in \\mathbb{R}^{n \\times n}$ and $\\mathbf{b} \\in \\mathbb{R}^n$, we consider a perturbation $\\delta A$ to the matrix $A$. The solution to the perturbed system, $(A + \\delta A)\\hat{\\mathbf{x}} = \\mathbf{b}$, is denoted by $\\hat{\\mathbf{x}}$. The inequality relates the relative error in the solution, $\\frac{\\|\\delta \\mathbf{x}\\|_2}{\\|\\mathbf{x}\\|_2}$, to the relative perturbation of the matrix, $\\frac{\\|\\delta A\\|_2}{\\|A\\|_2}$. The relation is given by:\n$$\n\\frac{\\|\\delta \\mathbf{x}\\|_2}{\\|\\mathbf{x}\\|_2} \\le \\kappa_2(A) \\frac{\\|\\delta A\\|_2}{\\|A\\|_2}\n$$\nwhere $\\delta \\mathbf{x} = \\hat{\\mathbf{x}} - \\mathbf{x}$. This inequality provides a first-order bound on the sensitivity of the solution of a linear system to perturbations in its coefficient matrix.\n\nThe norms used are the Euclidean vector norm, $\\|\\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^n v_i^2}$, and the induced matrix norm, known as the spectral norm, $\\|A\\|_2 = \\max_{\\|\\mathbf{v}\\|_2=1} \\|A\\mathbf{v}\\|_2$. The spectral norm of a matrix $A$ is equal to its largest singular value, $\\sigma_{\\max}(A)$.\n\nThe condition number with respect to the spectral norm, $\\kappa_2(A)$, is defined as:\n$$\n\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2\n$$\nFor a nonsingular matrix, this is also the ratio of the largest to the smallest singular values of $A$, $\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}$. The condition number is a measure of how much the relative error in the solution can be magnified compared to the relative error in the input data. A large condition number signifies an ill-conditioned problem, where small relative perturbations in $A$ can lead to large relative changes in $\\mathbf{x}$.\n\nIn this specific problem, the perturbation is of the form $\\delta A = \\varepsilon A$ for a small scalar $\\varepsilon$. This simplifies the term representing the relative matrix perturbation:\n$$\n\\frac{\\|\\delta A\\|_2}{\\|A\\|_2} = \\frac{\\|\\varepsilon A\\|_2}{\\|A\\|_2} = \\frac{|\\varepsilon| \\|A\\|_2}{\\|A\\|_2} = |\\varepsilon|\n$$\nThus, the inequality to be validated for each test case becomes:\n$$\n\\frac{\\|\\delta \\mathbf{x}\\|_2}{\\|\\mathbf{x}\\|_2} \\le |\\varepsilon| \\kappa_2(A)\n$$\n\nThe computational procedure for each test case $i \\in \\{1, 2, 3, 4, 5\\}$ is as follows:\n$1$. Given the matrix $A_i$, the vector $\\mathbf{b}_i$, and the scalar $\\varepsilon_i$.\n$2$. Compute the exact solution $\\mathbf{x}_i$ by solving the linear system $A_i \\mathbf{x}_i = \\mathbf{b}_i$.\n$3$. Construct the perturbed matrix $A'_i = A_i + \\delta A_i = A_i + \\varepsilon_i A_i = (1+\\varepsilon_i)A_i$.\n$4$. Compute the perturbed solution $\\hat{\\mathbf{x}}_i$ by solving the system $A'_i \\hat{\\mathbf{x}}_i = \\mathbf{b}_i$.\n$5$. Calculate the perturbation in the solution, $\\delta \\mathbf{x}_i = \\hat{\\mathbf{x}}_i - \\mathbf{x}_i$.\n$6$. Evaluate the left-hand side (LHS) of the inequality: $LHS_i = \\frac{\\|\\delta \\mathbf{x}_i\\|_2}{\\|\\mathbf{x}_i\\|_2}$. This is only well-defined if $\\|\\mathbf{x}_i\\|_2 \\neq 0$, which is true for all given test cases since $\\mathbf{b}_i \\neq \\mathbf{0}$.\n$7$. Evaluate the right-hand side (RHS) of the inequality: $RHS_i = |\\varepsilon_i| \\kappa_2(A_i)$. The condition number $\\kappa_2(A_i)$ will be computed numerically.\n$8$. Determine the boolean outcome of the verification $LHS_i \\le RHS_i$.\n\nThis procedure will be implemented for each of the $5$ test cases provided. The numerical calculations, including solving linear systems, and computing vector norms, matrix norms, and condition numbers, will be performed using the `numpy` library in Python.",
            "answer": "[True,True,True,True,True]"
        }
    ]
}