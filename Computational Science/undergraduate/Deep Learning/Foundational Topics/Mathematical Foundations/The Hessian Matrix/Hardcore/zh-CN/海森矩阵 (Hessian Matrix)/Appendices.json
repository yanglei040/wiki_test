{
    "hands_on_practices": [
        {
            "introduction": "海森矩阵最直接的应用之一是判断函数在临界点的性质。通过计算海森矩阵的行列式以及二阶偏导数 $f_{xx}$ 的符号，我们可以将临界点分类为局部最小值、局部最大值或鞍点。这个练习  提供了一个源于运筹学的具体优化问题，让你能够实践应用二阶导数测试来分析和解决实际问题。",
            "id": "2215318",
            "problem": "一位运筹学分析师正在为一家制造公司建立其周利润 $P(x, y)$ 的模型。该利润是两个变量的函数：$x$ 是主生产线的运行小时数，$y$ 是次要的、更专业化的生产线的运行小时数。该分析师已确定点 $(x_0, y_0) = (40, 15)$ 是利润函数的一个临界点，这意味着利润关于 $x$ 和 $y$ 的一阶偏导数在该点均为零。\n\n为了确定这个运营计划是代表局部最大利润、局部最小利润还是鞍点，必须应用二阶导数检验。通过数据分析，利润函数 $P(x,y)$ 在临界点 $(40, 15)$ 的二阶偏导数已被求出，结果如下：\n*   $\\frac{\\partial^2 P}{\\partial x^2} \\bigg|_{(40,15)} = -8.0$\n*   $\\frac{\\partial^2 P}{\\partial y^2} \\bigg|_{(40,15)} = -12.0$\n*   $\\frac{\\partial^2 P}{\\partial x \\partial y} \\bigg|_{(40,15)} = 9.0$\n\n根据这些信息，请对临界点 $(40, 15)$ 的性质进行分类。\n\nA) 局部最大值\n\nB) 局部最小值\n\nC) 鞍点\n\nD) 无法根据所提供的信息进行分类。",
            "solution": "在临界点，双变量函数的二阶导数检验使用海森矩阵\n$$\nH(x,y)=\\begin{pmatrix}\nP_{xx}(x,y) & P_{xy}(x,y) \\\\\nP_{yx}(x,y) & P_{yy}(x,y)\n\\end{pmatrix},\n$$\n及其行列式\n$$\nD(x,y)=P_{xx}(x,y)P_{yy}(x,y)-\\left(P_{xy}(x,y)\\right)^{2}.\n$$\n在点 $(40,15)$ 处，给定的值为 $P_{xx}=-8.0$，$P_{yy}=-12.0$ 以及 $P_{xy}=9.0$（根据混合偏导数的相等性，在适用情况下，$P_{xy}=P_{yx}$）。计算行列式：\n$$\nD(40,15)=(-8.0)(-12.0)-(9.0)^{2}=96-81=15.\n$$\n因此 $D(40,15)>0$。二阶导数检验法则如下：\n- 如果 $D>0$ 且 $P_{xx}<0$，则该点为局部最大值。\n- 如果 $D>0$ 且 $P_{xx}>0$，则该点为局部最小值。\n- 如果 $D<0$，则该点为鞍点。\n- 如果 $D=0$，则检验无法得出结论。\n\n此处，$D(40,15)>0$ 且 $P_{xx}(40,15)=-8.0$，所以该临界点是局部最大值。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "然而，基于海森矩阵的二阶导数测试虽然强大，却并非万无一失。在某些情况下，特别是当函数在临界点附近表现得非常“平坦”以至于二阶偏导数为零时，该测试可能会失效。这个练习  展示了这样一个经典场景，并强调了在测试失效时，需要回归到函数本身的行为进行直接分析，这对于理解深度学习中复杂的损失地形尤为重要。",
            "id": "2215356",
            "problem": "考虑二元函数 $f(x,y) = x^4 + y^4$。可以验证该函数在原点 $(0, 0)$ 有一个唯一的临界点。你的任务是对这个临界点进行分类。\n\n下列哪个陈述准确地描述了临界点 $(0,0)$ 的性质？\n\nA. 点 $(0,0)$ 是一个局部最小值，这可以通过二阶导数检验来确认。\n\nB. 点 $(0,0)$ 是一个局部最大值，这可以通过二阶导数检验来确认。\n\nC. 点 $(0,0)$ 是一个鞍点，这可以通过二阶导数检验来确认。\n\nD. 二阶导数检验无定论，但直接分析表明 $(0,0)$ 是一个局部最小值。\n\nE. 二阶导数检验无定论，但直接分析表明 $(0,0)$ 是一个局部最大值。\n\nF. 二阶导数检验无定论，且无法从给定信息中确定该点的性质。",
            "solution": "我们考虑函数 $f(x,y) = x^{4} + y^{4}$。临界点出现在梯度为零的地方。计算一阶偏导数：\n$$\nf_{x}(x,y) = \\frac{\\partial}{\\partial x}(x^{4} + y^{4}) = 4x^{3}, \\quad\nf_{y}(x,y) = \\frac{\\partial}{\\partial y}(x^{4} + y^{4}) = 4y^{3}.\n$$\n令梯度等于零，\n$$\n4x^{3} = 0 \\quad \\text{和} \\quad 4y^{3} = 0 \\;\\Rightarrow\\; x=0,\\; y=0,\n$$\n所以 $(0,0)$ 是唯一的临界点。\n\n为了应用二阶导数检验，计算二阶偏导数：\n$$\nf_{xx}(x,y) = \\frac{\\partial}{\\partial x}(4x^{3}) = 12x^{2}, \\quad\nf_{yy}(x,y) = \\frac{\\partial}{\\partial y}(4y^{3}) = 12y^{2}, \\quad\nf_{xy}(x,y) = \\frac{\\partial}{\\partial y}(4x^{3}) = 0.\n$$\n在 $(0,0)$ 点求值：\n$$\nf_{xx}(0,0) = 0, \\quad f_{yy}(0,0) = 0, \\quad f_{xy}(0,0) = 0.\n$$\n用于二阶导数检验的海森行列式是\n$$\nD = f_{xx}(0,0)\\,f_{yy}(0,0) - \\left(f_{xy}(0,0)\\right)^{2} = 0 \\cdot 0 - 0^{2} = 0.\n$$\n根据二元函数的二阶导数检验：如果 $D>0$ 且 $f_{xx}>0$，则存在一个局部最小值；如果 $D>0$ 且 $f_{xx}<0$，则存在一个局部最大值；如果 $D<0$，则存在一个鞍点；如果 $D=0$，则检验无定论。此处 $D=0$，所以检验无定论。\n\n因此，我们使用直接分析法。因为对于所有实数 $x,y$，都有 $x^{4} \\ge 0$ 和 $y^{4} \\ge 0$，我们得到\n$$\nf(x,y) = x^{4} + y^{4} \\ge 0,\n$$\n当且仅当 $x=0$ 且 $y=0$ 时等号成立。因此，对于所有 $(x,y)$，有 $f(x,y) \\ge f(0,0)$，并且对于所有 $(x,y) \\ne (0,0)$，有 $f(x,y) > f(0,0)$。这表明 $(0,0)$ 是一个严格的局部最小值（实际上是全局最小值）。\n\n因此，正确的分类是二阶导数检验无定论，但直接分析表明 $(0,0)$ 是一个局部最小值。",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "在深度学习领域，模型参数的维度极高，直接计算和存储完整的海森矩阵是不可行的。幸运的是，我们依然可以探测损失函数的曲率特性，特别是与最大特征值相关的最大曲率方向。这个练习  将指导你使用一种称为幂迭代的有效数值方法，它仅需海森向量积（Hessian-vector product）即可估算出最大特征值，从而让你亲身体验如何在不显式构造海森矩阵的情况下分析损失曲面。",
            "id": "3186508",
            "problem": "考虑一个在深度学习中使用的可微标量损失函数 $L(\\mathbf{w})$，其中 $\\mathbf{w} \\in \\mathbb{R}^d$ 表示参数向量。在点 $\\mathbf{w}$ 处的 Hessian 矩阵 $H(\\mathbf{w})$ 定义为二阶偏导数的方阵，其元素为 $H_{ij}(\\mathbf{w}) = \\frac{\\partial^2 L(\\mathbf{w})}{\\partial w_i \\partial w_j}$。对于具有连续二阶导数的函数，$H(\\mathbf{w})$ 是对称的。函数 $L$ 在点 $\\mathbf{w}$ 处沿方向 $\\mathbf{v} \\in \\mathbb{R}^d$ 的二阶泰勒展开式由 $L(\\mathbf{w} + \\mathbf{v}) \\approx L(\\mathbf{w}) + \\nabla L(\\mathbf{w})^\\top \\mathbf{v} + \\frac{1}{2} \\mathbf{v}^\\top H(\\mathbf{w}) \\mathbf{v}$ 给出。量 $\\mathbf{v}^\\top H(\\mathbf{w}) \\mathbf{v}$ 描述了沿 $\\mathbf{v}$ 方向的局部曲率，其在约束 $\\mathbf{v}^\\top \\mathbf{v} = 1$ 下的极值在 $H(\\mathbf{w})$ 的特征向量处取得。相关的极值为特征值，而最大的代数特征值近似了在 $\\mathbf{w}$ 处的最大局部曲率。\n\n你的任务是实现一个程序，仅使用重复的 Hessian-向量乘积、归一化以及瑞利商，来近似计算一个给定的实对称矩阵 $H$（解释为 Hessian 矩阵）的最大代数特征值。不要构建或使用任何闭式特征分解。在每次迭代中，通过应用矩阵 $H$ 并将结果归一化，使用当前迭代向量来计算下一个方向，然后评估瑞利商以更新特征值的估计值。持续迭代，直到瑞利商的绝对变化量低于指定的容差或达到最大迭代次数上限。如果在任何步骤中，Hessian-向量乘积为零向量（即其欧几里得范数为零或低于机器安全阈值），则终止并返回 $0$ 作为特征值估计。\n\n对所有测试用例使用以下固定参数：\n- 初始向量 $\\mathbf{v}_0$ 等于相应维度的全一向量，并归一化为单位欧几里得范数。\n- 连续迭代之间瑞利商绝对变化量的容差为 $10^{-10}$。\n- 最大迭代次数为 $1000$。\n- 将每个估计的最大代数特征值输出为浮点数，并四舍五入到六位小数。\n\n测试套件（下面的每个 $H$ 都是对称的，并应被视为在某个参数点上的 Hessian 矩阵）：\n1. 理想情况，对称正定 $3 \\times 3$ 矩阵：\n   $$H_1 = \\begin{bmatrix}\n   4  1  0 \\\\\n   1  3  0 \\\\\n   0  0  2\n   \\end{bmatrix}.$$\n2. 不定 $3 \\times 3$ 矩阵，具有主导正曲率：\n   $$H_2 = \\begin{bmatrix}\n   2  0.5  0 \\\\\n   0.5  -0.5  0 \\\\\n   0  0  0.2\n   \\end{bmatrix}.$$\n3. 退化的顶部曲率（最大特征值为重根）$3 \\times 3$ 矩阵：\n   $$H_3 = \\begin{bmatrix}\n   3  0  0 \\\\\n   0  3  0 \\\\\n   0  0  1\n   \\end{bmatrix}.$$\n4. 边界情况（无曲率）$3 \\times 3$ 矩阵：\n   $$H_4 = \\begin{bmatrix}\n   0  0  0 \\\\\n   0  0  0 \\\\\n   0  0  0\n   \\end{bmatrix}.$$\n5. 病态对称正定 $5 \\times 5$ 矩阵：\n   $$H_5 = \\begin{bmatrix}\n   10^{-6}  10^{-5}  0  0  0 \\\\\n   10^{-5}  10^{-2}  10^{-4}  0  0 \\\\\n   0  10^{-4}  10^{-1}  10^{-3}  0 \\\\\n   0  0  10^{-3}  1  10^{-2} \\\\\n   0  0  0  10^{-2}  10\n   \\end{bmatrix}.$$\n\n程序要求：\n- 对每个测试矩阵 $H$，使用上述迭代过程近似计算其最大代数特征值。\n- 生成单行输出，其中包含按测试套件顺序 $[H_1,H_2,H_3,H_4,H_5]$ 排列的结果，结果为逗号分隔的列表并用方括号括起来。每个条目必须是四舍五入到六位小数的浮点数，不含任何额外文本。例如，输出必须类似于 $[x_1,x_2,x_3,x_4,x_5]$，其中每个 $x_i$ 是矩阵 $H_i$ 的四舍五入浮点数结果。",
            "solution": "该问题要求实现一种迭代算法，以找到给定实对称矩阵 $H$ 的最大代数特征值。所提供的物理背景是深度学习中损失函数 $L(\\mathbf{w})$ 的 Hessian 矩阵 $H(\\mathbf{w})$，其中最大特征值对应于最大局部曲率的方向。指定的算法是幂迭代法，并结合使用瑞利商来估计特征值。\n\n该方法基于以下原理：对于一个可对角化的矩阵 $H$，将它重复应用于一个任意非零向量 $\\mathbf{v}_0$，会使结果向量逐渐与对应于最大模特征值（记为 $\\lambda_{dom}$）的特征向量对齐。设 $H$ 的特征值为 $|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_d|$，对应的特征向量为 $\\mathbf{u}_1, \\mathbf{u}_2, \\dots, \\mathbf{u}_d$。一个初始向量 $\\mathbf{v}_0$ 可以表示为这些特征向量的线性组合：$\\mathbf{v}_0 = c_1\\mathbf{u}_1 + c_2\\mathbf{u}_2 + \\dots + c_d\\mathbf{u}_d$。假设 $c_1 \\neq 0$，重复应用 $H$ 可得：\n$$H^k\\mathbf{v}_0 = c_1\\lambda_1^k\\mathbf{u}_1 + c_2\\lambda_2^k\\mathbf{u}_2 + \\dots + c_d\\lambda_d^k\\mathbf{u}_d = \\lambda_1^k \\left( c_1\\mathbf{u}_1 + c_2\\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^k\\mathbf{u}_2 + \\dots + c_d\\left(\\frac{\\lambda_d}{\\lambda_1}\\right)^k\\mathbf{u}_d \\right)$$\n如果 $|\\lambda_1| > |\\lambda_2|$（即存在唯一的模最大特征值），那么当 $k \\to \\infty$ 时，对于 $i > 1$ 的项 $(\\frac{\\lambda_i}{\\lambda_1})^k$ 趋近于 $0$。因此，向量 $H^k\\mathbf{v}_0$ 变得越来越平行于主特征向量 $\\mathbf{u}_1$。为防止 $H^k\\mathbf{v}_0$ 的模发散或消失，每一步都对向量进行归一化。这就得到了向量的迭代更新规则：\n$$\\mathbf{v}_{k+1} = \\frac{H\\mathbf{v}_k}{\\|H\\mathbf{v}_k\\|_2}$$\n其中 $\\mathbf{v}_k$ 是第 $k$ 次迭代时的归一化向量估计。\n\n一旦获得了特征向量的估计值 $\\mathbf{v}_k$，就可以使用瑞利商来估计相应的特征值。对于一个非零向量 $\\mathbf{v}$，瑞利商定义为：\n$$R_H(\\mathbf{v}) = \\frac{\\mathbf{v}^\\top H \\mathbf{v}}{\\mathbf{v}^\\top \\mathbf{v}}$$\n如果 $\\mathbf{v}$ 是一个精确的特征向量，那么 $H\\mathbf{v} = \\lambda\\mathbf{v}$，瑞利商就能得出精确的特征值 $\\lambda$。由于我们的迭代向量 $\\mathbf{v}_k$ 经过归一化，使得 $\\mathbf{v}_k^\\top \\mathbf{v}_k = 1$，表达式简化为 $R_H(\\mathbf{v}_k) = \\mathbf{v}_k^\\top H \\mathbf{v}_k$。随着 $\\mathbf{v}_k$ 收敛于主特征向量 $\\mathbf{u}_1$，瑞利商 $R_H(\\mathbf{v}_k)$ 也收敛于主特征值 $\\lambda_1$。对于所有提供的测试用例，最大代数特征值均为正，并且其模也最大，因此该方法将正确收敛到所需的值。\n\n根据问题规范，要实现的算法如下：\n\n$1$. **初始化**：\n   - 给定一个矩阵 $H \\in \\mathbb{R}^{d \\times d}$。\n   - 初始向量 $\\mathbf{v}_0$ 是维度为 $d$ 的全一向量，并归一化为欧几里得范数为 $1$：$\\mathbf{v}_0 = \\frac{1}{\\sqrt{d}}[1, 1, \\dots, 1]^\\top$。\n   - 收敛容差设置为 $\\epsilon = 10^{-10}$。\n   - 最大迭代次数为 $N_{max} = 1000$。\n   - 初始化特征值估计值，例如 $\\lambda_{current} = 0$，并将 $\\lambda_{previous}$ 设为一个像 $\\infty$ 的值，以确保第一次迭代的收敛性检查不会通过。\n\n$2$. **迭代**：对于 $k = 0, 1, 2, \\dots, N_{max}-1$：\n   a. 检查收敛性：如果 $|\\lambda_{current} - \\lambda_{previous}| < \\epsilon$，则算法已收敛。终止并返回 $\\lambda_{current}$。\n   b. 更新前一个特征值估计：$\\lambda_{previous} \\leftarrow \\lambda_{current}$。\n   c. 计算下一个未归一化的向量：$\\mathbf{w} = H \\mathbf{v}_k$。\n   d. 检查零特征值情况：如果 $\\|\\mathbf{w}\\|_2$ 低于机器安全阈值，这意味着 $\\mathbf{v}_k$ 位于 $H$ 的零空间中（或者 $H$ 是零矩阵）。对应的特征值为 $0$。根据指令，终止并返回 $0$。\n   e. 将向量归一化以获得下一个迭代向量：$\\mathbf{v}_{k+1} = \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|_2}$。为下一步方便，将这个新向量简记为 $\\mathbf{v}$。\n   f. 使用新计算出的向量 $\\mathbf{v}$ 评估瑞利商，以更新当前特征值的估计值：$\\lambda_{current} = \\mathbf{v}^\\top H \\mathbf{v}$。这需要在循环内进行第二次矩阵-向量乘法，但严格遵守了问题描述中的操作顺序。\n\n$3$. **终止**：如果循环在未收敛的情况下完成（即达到 $N_{max}$ 次迭代），则返回最后计算出的 $\\lambda_{current}$ 值。每个测试用例的最终结果需四舍五入到六位小数。\n\n此过程构成了一个完整且稳健的方法，用于按要求近似计算最大代数特征值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_iteration(H: np.ndarray, tol: float = 1e-10, max_iter: int = 1000) -> float:\n    \"\"\"\n    Approximates the largest algebraic eigenvalue of a symmetric matrix H\n    using the power iteration method with the Rayleigh quotient.\n\n    Args:\n        H: The symmetric matrix (as a numpy array).\n        tol: The tolerance for convergence, based on the absolute change\n             in the Rayleigh quotient.\n        max_iter: The maximum number of iterations.\n\n    Returns:\n        The estimated largest algebraic eigenvalue.\n    \"\"\"\n    # Get the dimension of the matrix\n    d = H.shape[0]\n\n    # 1. Initialize the vector v0 to the normalized all-ones vector.\n    v = np.ones(d, dtype=H.dtype)\n    v_norm = np.linalg.norm(v)\n    if v_norm > 0:\n        v /= v_norm\n\n    # Initialize eigenvalue estimates\n    lambda_current = 0.0\n    lambda_previous = np.inf  # Set to infinity to ensure the first diff is large\n\n    # A small number to check for zero vectors\n    machine_eps = np.finfo(H.dtype).eps\n\n    # 2. Iterate until convergence or max iterations\n    for _ in range(max_iter):\n        # Check for convergence based on the change in the eigenvalue estimate\n        if np.abs(lambda_current - lambda_previous) < tol:\n            break\n\n        # Store the previous eigenvalue estimate\n        lambda_previous = lambda_current\n\n        # Compute the Hessian-vector product\n        w = H @ v\n        \n        # Calculate the norm of the resulting vector\n        w_norm = np.linalg.norm(w)\n\n        # Check for the special case where H*v is the zero vector\n        if w_norm < machine_eps:\n            # The eigenvalue is 0. Terminate and return 0.\n            lambda_current = 0.0\n            break\n\n        # Normalize the vector to get the next iterate\n        v = w / w_norm\n\n        # Update the eigenvalue estimate using the Rayleigh quotient with the new vector v.\n        # This requires a second matrix-vector product in the loop as per a\n        # literal interpretation of the problem statement \"compute next direction...,\n        # then evaluate the Rayleigh quotient\".\n        lambda_current = v.T @ (H @ v)\n\n    return lambda_current\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Test suite as specified in the problem statement\n    H1 = np.array([\n        [4.0, 1.0, 0.0],\n        [1.0, 3.0, 0.0],\n        [0.0, 0.0, 2.0]\n    ])\n\n    H2 = np.array([\n        [2.0, 0.5, 0.0],\n        [0.5, -0.5, 0.0],\n        [0.0, 0.0, 0.2]\n    ])\n\n    H3 = np.array([\n        [3.0, 0.0, 0.0],\n        [0.0, 3.0, 0.0],\n        [0.0, 0.0, 1.0]\n    ])\n\n    H4 = np.array([\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0]\n    ])\n    \n    H5 = np.array([\n        [1e-6, 1e-5, 0.0, 0.0, 0.0],\n        [1e-5, 1e-2, 1e-4, 0.0, 0.0],\n        [0.0, 1e-4, 1e-1, 1e-3, 0.0],\n        [0.0, 0.0, 1e-3, 1.0, 1e-2],\n        [0.0, 0.0, 0.0, 1e-2, 10.0]\n    ])\n\n    test_cases = [H1, H2, H3, H4, H5]\n    \n    results = []\n    for H in test_cases:\n        # Calculate the largest eigenvalue for the current matrix\n        largest_eigenvalue = power_iteration(H)\n        \n        # Format the result to six decimal places and append\n        results.append(f\"{largest_eigenvalue:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}