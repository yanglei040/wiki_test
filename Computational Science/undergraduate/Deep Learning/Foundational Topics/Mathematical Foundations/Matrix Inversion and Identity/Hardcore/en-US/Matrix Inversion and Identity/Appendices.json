{
    "hands_on_practices": [
        {
            "introduction": "In theoretical analysis, it is often useful to start with an idealized assumption to derive a clean result, and then study small deviations from it. This practice exercise explores this principle within the context of Newton's method for a regularized linear model . By first assuming the feature covariance matrix is the identity matrix and then introducing a small perturbation, you will see how the identity matrix serves as a powerful analytical benchmark for understanding model optimization.",
            "id": "3147712",
            "problem": "Consider a single-layer linear model in deep learning with parameters $w \\in \\mathbb{R}^{n}$ that maps an input $x \\in \\mathbb{R}^{n}$ to a scalar prediction $\\hat{y} = w^{\\top} x$. You are given a dataset $\\{(x_{i}, y_{i})\\}_{i=1}^{m}$ and define the Mean Squared Error (MSE) with $\\ell_{2}$ regularization as\n$$\nL(w) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2},\n$$\nwhere $\\lambda  0$ is a regularization coefficient. Let $X \\in \\mathbb{R}^{n \\times m}$ be the data matrix whose columns are $x_{i}$, and let $y \\in \\mathbb{R}^{m}$ be the vector of targets. Denote by $I \\in \\mathbb{R}^{n \\times n}$ the identity matrix.\n\nStarting from $w_{0} = 0$, perform a single step of Newton’s method using the exact Hessian of $L(w)$. First, derive the closed-form update $w_{1}$ under the simplifying assumption that the empirical feature covariance equals the identity, namely $\\frac{1}{m} X X^{\\top} = I$. Then, analyze deviations from this assumption by modeling\n$$\n\\frac{1}{m} X X^{\\top} = I + \\epsilon A,\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ is symmetric with operator norm strictly less than $1$, and $0  \\epsilon \\ll 1$. Using a first-order approximation valid to order $\\epsilon$ (neglecting $\\mathcal{O}(\\epsilon^{2})$ and higher terms), derive an analytic expression for $w_{1}$ in terms of $\\lambda$, $\\epsilon$, $A$, $X$, $y$, and $m$. Your final answer should be a single closed-form expression for this first-order $w_{1}$. No rounding is required and no units are involved.",
            "solution": "The user has provided a problem from the field of machine learning, specifically concerning the application of Newton's method to a regularized linear model. The task is to derive a first-order approximation for the parameter update under a specified perturbation.\n\n### Step 1: Problem Validation\n\nFirst, I will extract the givens and validate the problem statement.\n\n**Givens:**\n-   **Model:** A single-layer linear model with parameters $w \\in \\mathbb{R}^{n}$, input $x \\in \\mathbb{R}^{n}$, and prediction $\\hat{y} = w^{\\top} x$.\n-   **Dataset:** $\\{(x_{i}, y_{i})\\}_{i=1}^{m}$.\n-   **Loss Function:** Mean Squared Error (MSE) with $\\ell_{2}$ regularization.\n    $L(w) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2}$.\n-   **Regularization Coefficient:** $\\lambda  0$.\n-   **Matrix Definitions:** Data matrix $X \\in \\mathbb{R}^{n \\times m}$ (columns are $x_i$), target vector $y \\in \\mathbb{R}^{m}$, identity matrix $I \\in \\mathbb{R}^{n \\times n}$.\n-   **Optimization Setup:** Starting from $w_{0} = 0$, perform a single step of Newton's method.\n-   **Perturbation Model:** The empirical feature covariance is modeled as $\\frac{1}{m} X X^{\\top} = I + \\epsilon A$.\n-   **Perturbation Properties:** $A \\in \\mathbb{R}^{n \\times n}$ is symmetric, its operator norm $\\|A\\|_{op}  1$, and the perturbation strength $0  \\epsilon \\ll 1$.\n-   **Objective:** Derive an analytic expression for the updated parameters $w_1$, valid to first order in $\\epsilon$.\n\n**Validation:**\n1.  **Scientific or Factual Unsoundness**: The problem is scientifically sound. It uses standard definitions for linear models, ridge regression (MSE with $\\ell_2$ regularization), and Newton's method for optimization. These are fundamental concepts in machine learning and numerical optimization.\n2.  **Non-Formalizable or Irrelevant**: The problem is formal and directly relevant to the topic of optimization in deep learning.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained and consistent. All necessary variables, constants ($\\lambda$), and conditions (e.g., $w_0=0$, $\\lambda0$, properties of $A$) are specified. The condition $\\lambda  0$ ensures the Hessian is positive definite and thus invertible.\n4.  **Unrealistic or Infeasible**: The setup is realistic for theoretical analysis. The assumption $\\frac{1}{m}XX^\\top=I$ corresponds to whitened features, a common idealization. Analyzing small deviations from this ideal case using perturbation theory is a standard and powerful technique.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. The loss function $L(w)$ is a strictly convex quadratic form (due to $\\lambda0$), possessing a unique minimum. Newton's method is well-defined.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial. It requires a solid understanding of matrix calculus and perturbation theory (specifically, Neumann series for matrix inversion).\n\n**Verdict:** The problem is valid. I will now proceed with the solution.\n\n### Step 2: Derivation of the Newton Update\n\nThe update rule for Newton's method is given by:\n$$ w_{k+1} = w_k - [H_L(w_k)]^{-1} \\nabla_w L(w_k) $$\nwhere $\\nabla_w L(w)$ is the gradient of the loss function $L(w)$ with respect to $w$, and $H_L(w)$ is the Hessian matrix. For the first step starting from $w_0 = 0$, the update is:\n$$ w_1 = w_0 - [H_L(w_0)]^{-1} \\nabla_w L(w_0) = - [H_L(0)]^{-1} \\nabla_w L(0) $$\n\nTo proceed, we first need to find the gradient and the Hessian of $L(w)$. It is convenient to rewrite the loss function in matrix form. The sum of squares can be expressed as the squared norm of a vector:\n$$ \\sum_{i=1}^{m} (w^\\top x_i - y_i)^2 = \\| X^\\top w - y \\|_2^2 = (X^\\top w - y)^\\top (X^\\top w - y) $$\nNote that $w^\\top X = [w^\\top x_1, \\dots, w^\\top x_m]$ is a $1 \\times m$ row vector, while $X^\\top w$ is an $m \\times 1$ column vector. Writing the loss function in terms of $w$:\n$$ L(w) = \\frac{1}{2m} (X^\\top w - y)^\\top (X^\\top w - y) + \\frac{\\lambda}{2} w^\\top w $$\nExpanding the quadratic term:\n$$ L(w) = \\frac{1}{2m} (w^\\top X X^\\top w - 2y^\\top X^\\top w + y^\\top y) + \\frac{\\lambda}{2} w^\\top w $$\n\nNow, we compute the gradient with respect to $w$:\n$$ \\nabla_w L(w) = \\frac{1}{2m} (2XX^\\top w - 2Xy) + \\frac{\\lambda}{2} (2w) $$\n$$ \\nabla_w L(w) = \\frac{1}{m} (XX^\\top w - Xy) + \\lambda w = \\left(\\frac{1}{m} XX^\\top + \\lambda I\\right)w - \\frac{1}{m} Xy $$\n\nNext, we compute the Hessian matrix by differentiating the gradient with respect to $w^\\top$:\n$$ H_L(w) = \\nabla_w (\\nabla_w L(w))^\\top = \\frac{1}{m} XX^\\top + \\lambda I $$\nThe Hessian is constant and independent of $w$ because the loss function is quadratic.\n\nNow we can evaluate the gradient at $w_0 = 0$:\n$$ \\nabla_w L(0) = \\left(\\frac{1}{m} XX^\\top + \\lambda I\\right)(0) - \\frac{1}{m} Xy = -\\frac{1}{m} Xy $$\nThe Hessian at $w_0=0$ is simply $H_L(0) = \\frac{1}{m} XX^\\top + \\lambda I$.\n\nSubstituting these into the Newton step equation for $w_1$:\n$$ w_1 = - \\left(\\frac{1}{m} XX^\\top + \\lambda I\\right)^{-1} \\left(-\\frac{1}{m} Xy\\right) $$\n$$ w_1 = \\left(\\frac{1}{m} XX^\\top + \\lambda I\\right)^{-1} \\frac{1}{m} Xy $$\nThis is the general expression for the first Newton step from $w_0=0$. Since the objective is quadratic, this single step actually finds the exact minimum of $L(w)$.\n\n### Step 3: Applying the Perturbation and First-Order Approximation\n\nThe problem directs us to analyze the case where the empirical feature covariance matrix is a perturbation of the identity matrix:\n$$ \\frac{1}{m} XX^\\top = I + \\epsilon A $$\nSubstituting this into our expression for $w_1$:\n$$ w_1 = \\left( (I + \\epsilon A) + \\lambda I \\right)^{-1} \\frac{1}{m} Xy $$\n$$ w_1 = \\left( (1+\\lambda)I + \\epsilon A \\right)^{-1} \\frac{1}{m} Xy $$\n\nTo find a first-order approximation in $\\epsilon$, we use the Neumann series for the matrix inverse. For a matrix $M$ with operator norm $\\|M\\|_{op}  1$, the inverse $(I-M)^{-1}$ can be expanded as a power series: $(I-M)^{-1} = I + M + M^2 + \\dots$. A first-order approximation is $(I-M)^{-1} \\approx I+M$. Similarly, $(I+M)^{-1} \\approx I-M$.\n\nLet's rearrange the term to be inverted:\n$$ (1+\\lambda)I + \\epsilon A = (1+\\lambda) \\left( I + \\frac{\\epsilon}{1+\\lambda} A \\right) $$\nThe inverse is:\n$$ \\left( (1+\\lambda)I + \\epsilon A \\right)^{-1} = \\left( (1+\\lambda) \\left( I + \\frac{\\epsilon}{1+\\lambda} A \\right) \\right)^{-1} = \\frac{1}{1+\\lambda} \\left( I + \\frac{\\epsilon}{1+\\lambda} A \\right)^{-1} $$\nLet $M = \\frac{\\epsilon}{1+\\lambda} A$. Since we are given $\\lambda  0$, $\\|A\\|_{op}  1$, and $0  \\epsilon \\ll 1$, the operator norm of $M$ is $\\|M\\|_{op} = \\frac{\\epsilon}{1+\\lambda} \\|A\\|_{op}  \\epsilon \\ll 1$. The Neumann series approximation is therefore valid. Using the first-order approximation $(I+M)^{-1} \\approx I-M$, we get:\n$$ \\left( I + \\frac{\\epsilon}{1+\\lambda} A \\right)^{-1} \\approx I - \\frac{\\epsilon}{1+\\lambda} A $$\nThis approximation neglects terms of order $\\mathcal{O}(\\epsilon^2)$ and higher.\n\nSubstituting this back into the expression for the inverse:\n$$ \\left( (1+\\lambda)I + \\epsilon A \\right)^{-1} \\approx \\frac{1}{1+\\lambda} \\left( I - \\frac{\\epsilon}{1+\\lambda} A \\right) $$\n\nFinally, we substitute this approximated inverse into our expression for $w_1$:\n$$ w_1 \\approx \\left( \\frac{1}{1+\\lambda} \\left( I - \\frac{\\epsilon}{1+\\lambda} A \\right) \\right) \\frac{1}{m} Xy $$\n$$ w_1 \\approx \\frac{1}{m(1+\\lambda)} \\left( I - \\frac{\\epsilon}{1+\\lambda} A \\right) Xy $$\nThis is the final expression for $w_1$ to first order in $\\epsilon$.",
            "answer": "$$\\boxed{\\frac{1}{m(1+\\lambda)} \\left( I - \\frac{\\epsilon}{1+\\lambda} A \\right) Xy}$$"
        },
        {
            "introduction": "While matrix inversion is a fundamental concept, its direct application in floating-point arithmetic can be fraught with peril, especially for ill-conditioned systems common in machine learning. This hands-on coding practice  allows you to experimentally quantify the numerical instability of explicit matrix inversion compared to direct linear solvers. You will discover how adding a small multiple of the identity matrix, a technique known as Tikhonov regularization, dramatically improves the robustness of the solution to noise by improving the system's conditioning.",
            "id": "3147728",
            "problem": "You are given a sequence of linear systems that model inner iterations in deep training loops, where at each step a symmetric positive semidefinite matrix $A$ arises from local quadratic approximations and one solves $A x = b$ to compute a parameter update. Your task is to quantify, in controlled synthetic experiments, the numerical stability of three approaches and to determine how using an identity-shifted system improves robustness: explicit matrix inversion to compute $A^{-1} b$, a direct linear solve of $A x = b$, and a shifted linear solve of $(A + \\lambda I) x = b$ with $\\lambda  0$. You must use a programmatic approach, implement all computations in double precision, and aggregate all test-case results into a single, machine-readable line.\n\nFundamental base for this problem:\n- Linear systems $A x = b$ and the identity matrix $I$.\n- The $2$-norm $\\| \\cdot \\|_2$ and the $2$-norm condition number $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. For symmetric positive definite $A$, $\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}$, where $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$ are the largest and smallest eigenvalues of $A$.\n- For symmetric matrices, the eigenvalues of $A + \\lambda I$ are $\\lambda_i(A) + \\lambda$, hence $\\kappa_2(A + \\lambda I) = \\frac{\\lambda_{\\max}(A) + \\lambda}{\\lambda_{\\min}(A) + \\lambda}$, with $\\lambda  0$ reducing the condition number.\n- Backward stability of solving linear systems by Gaussian elimination or Cholesky factorization versus instability of explicitly forming $A^{-1}$ in floating point arithmetic.\n\nDefinitions of metrics to compute:\n- The amplification factor $g$ for a method is defined, given a noise vector $\\eta$ and a ground truth $x^\\star$, by $g = \\frac{\\|x - x^\\star\\|_2}{\\|\\eta\\|_2}$, where $x$ is the estimate produced by the method on the noisy right-hand side $b = A x^\\star + \\eta$.\n- The condition numbers $\\kappa_2(A)$ and $\\kappa_2(A + \\lambda I)$.\n- The algorithmic discrepancy $\\delta_{\\text{inv}}$ between explicit inversion and direct solving on the clean right-hand side $b_0 = A x^\\star$, defined by $\\delta_{\\text{inv}} = \\frac{\\|x_{\\text{inv,clean}} - x_{\\text{solve,clean}}\\|_2}{\\|x_{\\text{solve,clean}}\\|_2}$, where $x_{\\text{inv,clean}} = A^{-1} b_0$ and $x_{\\text{solve,clean}}$ solves $A x = b_0$ via a direct solver.\n\nExperimental setup:\n- All random numbers must be generated by a reproducible pseudorandom number generator initialized with the specified seeds.\n- For each test case, construct the matrix $A$, sample a ground truth vector $x^\\star$ with entries uniformly distributed in $[-1,1]$, form the clean right-hand side $b_0 = A x^\\star$, draw a Gaussian noise vector $\\eta$ with independent components from $\\mathcal{N}(0, \\sigma^2)$, and set $b = b_0 + \\eta$.\n- Compute $x_{\\text{inv}} = A^{-1} b$ by explicit inversion, $x_{\\text{solve}}$ by solving $A x = b$, and $x_{\\text{shift}}$ by solving $(A + \\lambda I) x = b$.\n- For methods that fail due to singularity or numerical breakdown, set the corresponding metrics to $+\\infty$.\n- For each test case, report the list $[g_{\\text{inv}}, g_{\\text{solve}}, g_{\\text{shift}}, \\kappa_2(A), \\kappa_2(A + \\lambda I), \\delta_{\\text{inv}}, \\text{is\\_superior}]$, where $\\text{is\\_superior}$ is the boolean value of $g_{\\text{shift}}  \\min(g_{\\text{solve}}, g_{\\text{inv}})$.\n\nTest suite with parameter values:\n- Case $1$ (random symmetric positive definite): dimension $n = 10$, construct $A = Q^\\top Q + \\alpha I$ with $Q \\in \\mathbb{R}^{n \\times n}$ having entries uniform in $[-1,1]$ using seed $7$, set $\\alpha = 10^{-2}$, ground truth seed $11$, noise seed $13$, noise standard deviation $\\sigma = 10^{-6}$, shift $\\lambda = 10^{-1}$.\n- Case $2$ (Hilbert matrix): dimension $n = 12$, construct the Hilbert matrix $A$ with entries $A_{ij} = \\frac{1}{i + j + 1}$ for indices $i, j \\in \\{0, 1, \\dots, n-1\\}$, ground truth seed $17$, noise seed $19$, noise standard deviation $\\sigma = 10^{-4}$, shift $\\lambda = 10^{-1}$.\n- Case $3$ (rank-deficient symmetric positive semidefinite): dimension $n = 10$, construct $B \\in \\mathbb{R}^{r \\times n}$ with $r = 8$ with entries uniform in $[-1,1]$ using seed $23$ and set $A = B^\\top B$, ground truth seed $29$, noise seed $31$, noise standard deviation $\\sigma = 10^{-6}$, shift $\\lambda = 1$.\n- Case $4$ (prescribed spectrum): dimension $n = 12$, construct $A = U \\operatorname{diag}(e) U^\\top$ where $U$ is obtained from the $\\operatorname{QR}$ factorization of a Gaussian matrix generated with seed $37$, and eigenvalues $e_i$ satisfy $\\log_{10}(e_i)$ linearly spaced between $-8$ and $0$, i.e., $e_i = 10^{(-8 + 8 \\frac{i}{n - 1})}$ for $i \\in \\{0, 1, \\dots, n - 1\\}$, ground truth seed $41$, noise seed $43$, noise standard deviation $\\sigma = 10^{-5}$, shift $\\lambda = 10^{-2}$.\n\nYour program must:\n- Implement the above construction and metrics for each case in the specified order.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case’s result itself being a bracketed, comma-separated list in the form $[g_{\\text{inv}}, g_{\\text{solve}}, g_{\\text{shift}}, \\kappa_2(A), \\kappa_2(A + \\lambda I), \\delta_{\\text{inv}}, \\text{is\\_superior}]$.",
            "solution": "We begin with the foundational linear algebra principles. For a linear system $A x = b$, the solution mapping is $b \\mapsto x = A^{-1} b$ in exact arithmetic when $A$ is nonsingular. The sensitivity of this mapping to perturbations is governed by the condition number $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. In floating point arithmetic, explicit inversion amplifies rounding errors because computing $A^{-1}$ is itself a numerically sensitive operation, and then multiplying by $b$ introduces further error. In contrast, direct linear solvers based on Gaussian elimination or Cholesky factorization are backward stable: they compute the exact solution to a slightly perturbed system $(A + \\Delta A) x = b$ with $\\|\\Delta A\\|$ proportional to machine precision. Consequently, forming $A^{-1}$ explicitly and using it is less stable than solving $A x = b$.\n\nFor symmetric positive definite $A$, the $2$-norm equals the largest singular value which equals the largest eigenvalue, and $\\|A^{-1}\\|_2$ equals the reciprocal of the smallest eigenvalue. Hence, $\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}$. Consider the shifted system $(A + \\lambda I) x = b$ with $\\lambda  0$. The eigenvalues of $A + \\lambda I$ are $\\lambda_i(A) + \\lambda$ for each eigenvalue $\\lambda_i(A)$ of $A$. Therefore,\n$$\n\\kappa_2(A + \\lambda I) = \\frac{\\lambda_{\\max}(A) + \\lambda}{\\lambda_{\\min}(A) + \\lambda}.\n$$\nAssume $0  \\lambda_{\\min}(A) \\le \\lambda_{\\max}(A)$. Define $f(\\lambda) = \\frac{\\lambda_{\\max}(A) + \\lambda}{\\lambda_{\\min}(A) + \\lambda}$ for $\\lambda  0$. The derivative is\n$$\nf'(\\lambda) = \\frac{\\lambda_{\\min}(A) - \\lambda_{\\max}(A)}{(\\lambda_{\\min}(A) + \\lambda)^2}  0,\n$$\nimplying $f$ is strictly decreasing. Thus, adding $\\lambda I$ reduces the condition number. Since the operator norm of $(A + \\lambda I)^{-1}$ is $\\frac{1}{\\lambda_{\\min}(A) + \\lambda}$, the shifted system reduces the potential amplification of perturbations in $b$.\n\nIn the presence of noisy right-hand sides $b = A x^\\star + \\eta$, the solution $x = A^{-1} b$ experiences error $\\|x - x^\\star\\|_2$ that, in first-order perturbation analysis, scales like $\\|A^{-1}\\|_2 \\|\\eta\\|_2$ when $A$ is well-conditioned and the algorithm is backward stable. For ill-conditioned $A$, $\\|A^{-1}\\|_2$ is large, and noise is greatly amplified. Using $(A + \\lambda I)^{-1}$ reduces $\\|A^{-1}\\|_2$ to $\\|(A + \\lambda I)^{-1}\\|_2$, dampening noise amplification, at the cost of biasing the solution away from $x^\\star$. In many deep learning contexts, this shift corresponds to Levenberg–Marquardt damping or Tikhonov-type regularization, which improves robustness by controlling step sizes. The metric we use, the amplification factor\n$$\ng = \\frac{\\|x - x^\\star\\|_2}{\\|\\eta\\|_2},\n$$\ncaptures the trade-off, with smaller $g$ indicating better robustness to noise.\n\nAlgorithm design:\n- For each test case, construct $A$ deterministically following the specified recipe:\n  - Case $1$: $A = Q^\\top Q + \\alpha I$ with $Q$ uniform in $[-1,1]$.\n  - Case $2$: Hilbert matrix $A_{ij} = \\frac{1}{i + j + 1}$.\n  - Case $3$: $A = B^\\top B$ with $B$ uniform in $[-1,1]$ and rank $r  n$.\n  - Case $4$: $A = U \\operatorname{diag}(e) U^\\top$ with orthonormal $U$ from $\\operatorname{QR}$ decomposition and eigenvalues $e_i = 10^{(-8 + 8 \\frac{i}{n - 1})}$.\n- Generate $x^\\star$ uniformly in $[-1,1]$ and noise $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I)$ with the specified seeds. Compute $b_0 = A x^\\star$ and $b = b_0 + \\eta$.\n- Compute condition numbers $\\kappa_2(A)$ and $\\kappa_2(A + \\lambda I)$ using the $2$-norm.\n- Compute solutions:\n  - Explicit inverse $x_{\\text{inv}} = A^{-1} b$ and, on the clean right-hand side, $x_{\\text{inv,clean}} = A^{-1} b_0$.\n  - Direct solve $x_{\\text{solve}}$ for $A x = b$ and, on the clean right-hand side, $x_{\\text{solve,clean}}$.\n  - Shifted solve $x_{\\text{shift}}$ for $(A + \\lambda I) x = b$.\n- Compute metrics:\n  - Amplification factors $g_{\\text{inv}} = \\frac{\\|x_{\\text{inv}} - x^\\star\\|_2}{\\|\\eta\\|_2}$, $g_{\\text{solve}} = \\frac{\\|x_{\\text{solve}} - x^\\star\\|_2}{\\|\\eta\\|_2}$, and $g_{\\text{shift}} = \\frac{\\|x_{\\text{shift}} - x^\\star\\|_2}{\\|\\eta\\|_2}$.\n  - Algorithmic discrepancy $\\delta_{\\text{inv}} = \\frac{\\|x_{\\text{inv,clean}} - x_{\\text{solve,clean}}\\|_2}{\\|x_{\\text{solve,clean}}\\|_2}$.\n- Determine superiority as the boolean value of $g_{\\text{shift}}  \\min(g_{\\text{solve}}, g_{\\text{inv}})$.\n- Handle singular or near-singular cases by catching linear algebra errors; if inversion or solve fails, set the corresponding metrics to $+\\infty$.\n- Aggregate per-case results into the specified single-line output format.\n\nWhy this demonstrates superiority:\n- For symmetric positive definite $A$, adding $\\lambda I$ increases all eigenvalues uniformly, tightening the spectrum and decreasing $\\kappa_2$. The operator norm of the inverse decreases from $\\frac{1}{\\lambda_{\\min}(A)}$ to $\\frac{1}{\\lambda_{\\min}(A) + \\lambda}$, reducing noise amplification. Empirically, $g_{\\text{shift}}$ should be smaller than $g_{\\text{inv}}$ and $g_{\\text{solve}}$, particularly in ill-conditioned or rank-deficient cases.\n- Explicit inversion tends to have larger $\\delta_{\\text{inv}}$, especially for ill-conditioned $A$, revealing its inferior numerical stability compared to direct solves.\n- The test suite covers:\n  - A moderately conditioned system where all methods perform comparably and the shift offers slight robustness.\n  - A Hilbert matrix with very large condition number where the shift significantly improves stability.\n  - A rank-deficient symmetric positive semidefinite system where the unshifted methods may fail or yield very large amplification, while the shift regularizes the problem.\n  - A system with a prescribed spectrum spanning many orders of magnitude to stress conditioning and show the benefit of shifting.\n\nThe final output format is a single line containing a bracketed, comma-separated list of the per-case lists $[g_{\\text{inv}}, g_{\\text{solve}}, g_{\\text{shift}}, \\kappa_2(A), \\kappa_2(A + \\lambda I), \\delta_{\\text{inv}}, \\text{is\\_superior}]$ in the order of the four cases.",
            "answer": "```python\n# Python 3.12, numpy 1.23.5, scipy 1.11.4 (not used), standard library only.\nimport numpy as np\n\ndef hilbert(n: int) - np.ndarray:\n    i = np.arange(n).reshape(-1, 1)\n    j = np.arange(n).reshape(1, -1)\n    return 1.0 / (i + j + 1.0)\n\ndef make_spd_via_Q(n: int, alpha: float, seed: int) - np.ndarray:\n    rng = np.random.default_rng(seed)\n    Q = rng.uniform(-1.0, 1.0, size=(n, n))\n    A = Q.T @ Q + alpha * np.eye(n)\n    return A\n\ndef make_rank_def_spd(n: int, r: int, seed: int) - np.ndarray:\n    rng = np.random.default_rng(seed)\n    B = rng.uniform(-1.0, 1.0, size=(r, n))\n    A = B.T @ B\n    return A\n\ndef make_spd_with_eigs(n: int, seed: int) - np.ndarray:\n    # Create orthonormal U via QR of a Gaussian matrix\n    rng = np.random.default_rng(seed)\n    G = rng.normal(0.0, 1.0, size=(n, n))\n    Q, _ = np.linalg.qr(G)\n    # Prescribed eigenvalues: log10 spaced from -8 to 0\n    exponents = -8.0 + 8.0 * (np.arange(n) / (n - 1))\n    eigs = 10.0 ** exponents\n    A = Q @ np.diag(eigs) @ Q.T\n    return A\n\ndef amplification_factor(x_est: np.ndarray, x_true: np.ndarray, noise: np.ndarray) - float:\n    num = np.linalg.norm(x_est - x_true, ord=2)\n    den = np.linalg.norm(noise, ord=2)\n    # Avoid division by zero; if no noise, define amplification as +inf\n    if den == 0.0:\n        return float('inf')\n    return num / den\n\ndef safe_inv_solve(A: np.ndarray, b: np.ndarray):\n    # Returns solution via explicit inverse, or raises/returns inf if fails\n    try:\n        A_inv = np.linalg.inv(A)\n        return A_inv @ b\n    except Exception:\n        return None\n\ndef safe_direct_solve(A: np.ndarray, b: np.ndarray):\n    try:\n        return np.linalg.solve(A, b)\n    except Exception:\n        return None\n\ndef compute_case(case):\n    case_type = case['type']\n    if case_type == 'spd_q':\n        A = make_spd_via_Q(case['n'], case['alpha'], case['seed_A'])\n    elif case_type == 'hilbert':\n        A = hilbert(case['n'])\n    elif case_type == 'rank_def_spd':\n        A = make_rank_def_spd(case['n'], case['rank'], case['seed_A'])\n    elif case_type == 'spd_eigs':\n        A = make_spd_with_eigs(case['n'], case['seed_A'])\n    else:\n        raise ValueError(\"Unknown case type\")\n\n    n = case['n']\n    # Ground truth and noise\n    rng_x = np.random.default_rng(case['seed_x'])\n    x_true = rng_x.uniform(-1.0, 1.0, size=n)\n    b_clean = A @ x_true\n\n    rng_noise = np.random.default_rng(case['seed_noise'])\n    noise = rng_noise.normal(0.0, case['noise_std'], size=n)\n    b_noisy = b_clean + noise\n\n    # Condition numbers\n    try:\n        cond_A = np.linalg.cond(A)  # 2-norm condition number\n    except Exception:\n        cond_A = float('inf')\n\n    A_shift = A + case['lambda'] * np.eye(n)\n    try:\n        cond_A_shift = np.linalg.cond(A_shift)\n    except Exception:\n        cond_A_shift = float('inf')\n\n    # Solutions\n    x_inv_noisy = safe_inv_solve(A, b_noisy)\n    x_solve_noisy = safe_direct_solve(A, b_noisy)\n    x_shift_noisy = safe_direct_solve(A_shift, b_noisy)\n\n    # Clean RHS for algorithmic discrepancy\n    x_inv_clean = safe_inv_solve(A, b_clean)\n    x_solve_clean = safe_direct_solve(A, b_clean)\n\n    # Amplification factors\n    if x_inv_noisy is None:\n        g_inv = float('inf')\n    else:\n        g_inv = amplification_factor(x_inv_noisy, x_true, noise)\n\n    if x_solve_noisy is None:\n        g_solve = float('inf')\n    else:\n        g_solve = amplification_factor(x_solve_noisy, x_true, noise)\n\n    if x_shift_noisy is None:\n        g_shift = float('inf')\n    else:\n        g_shift = amplification_factor(x_shift_noisy, x_true, noise)\n\n    # Algorithmic discrepancy\n    if (x_inv_clean is None) or (x_solve_clean is None):\n        delta_inv = float('inf')\n    else:\n        denom = np.linalg.norm(x_solve_clean, ord=2)\n        if denom == 0.0:\n            delta_inv = float('inf')\n        else:\n            delta_inv = np.linalg.norm(x_inv_clean - x_solve_clean, ord=2) / denom\n\n    is_superior = g_shift  min(g_solve, g_inv)\n\n    return [g_inv, g_solve, g_shift, cond_A, cond_A_shift, delta_inv, is_superior]\n\ndef solve():\n    test_cases = [\n        {\n            'type': 'spd_q',\n            'n': 10,\n            'alpha': 1e-2,\n            'seed_A': 7,\n            'seed_x': 11,\n            'seed_noise': 13,\n            'noise_std': 1e-6,\n            'lambda': 1e-1,\n        },\n        {\n            'type': 'hilbert',\n            'n': 12,\n            'seed_A': None,  # Deterministic\n            'seed_x': 17,\n            'seed_noise': 19,\n            'noise_std': 1e-4,\n            'lambda': 1e-1,\n        },\n        {\n            'type': 'rank_def_spd',\n            'n': 10,\n            'rank': 8,\n            'seed_A': 23,\n            'seed_x': 29,\n            'seed_noise': 31,\n            'noise_std': 1e-6,\n            'lambda': 1.0,\n        },\n        {\n            'type': 'spd_eigs',\n            'n': 12,\n            'seed_A': 37,\n            'seed_x': 41,\n            'seed_noise': 43,\n            'noise_std': 1e-5,\n            'lambda': 1e-2,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(compute_case(case))\n\n    # Print single-line, bracket-enclosed, comma-separated list of per-case lists\n    # Ensure booleans and floats are printed via default str\n    def format_item(item):\n        if isinstance(item, list):\n            return \"[\" + \",\".join(format_item(x) for x in item) + \"]\"\n        else:\n            return str(item)\n\n    print(\"[\" + \",\".join(format_item(r) for r in results) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Many advanced models, like those using attention mechanisms, involve matrices with a special structure: an identity matrix plus a low-rank matrix ($I + UV^{\\top}$). The Sherman-Morrison-Woodbury formula allows for the efficient inversion of such matrices by exploiting this structure, turning a potentially costly cubic-time operation into a much faster one. This exercise combines derivation with implementation to reveal a powerful computational shortcut, where by deriving and applying the update rule for rank-1 modifications , you will understand how leveraging the identity matrix's structural properties can lead to significant algorithmic speedups.",
            "id": "3147731",
            "problem": "You are given square matrices of the form $A = I + U V^\\top$, where $I$ is the identity matrix of size $n \\times n$, and $U, V \\in \\mathbb{R}^{n \\times k}$ define a low-rank update. Such low-rank plus identity structures arise in computational linear algebra for deep learning when approximating certain kernelized or preconditioned transformations, and they are especially relevant when the structure is block-local or head-local in attention mechanisms. Your task is to implement and evaluate an algorithm that inverts $A$ using repeated rank-$1$ updates starting from $A_0 = I$ and incrementally incorporating each column pair $(u_i, v_i)$, where $u_i$ and $v_i$ are the $i$-th columns of $U$ and $V$, respectively. You must not assume any formula in advance; instead, you must derive the required update from first principles, starting from the definition of the matrix inverse, and implement it.\n\nFundamental base you may use: definitions and properties of matrix inverse and identity, algebra of outer products and matrix-vector multiplication, norms, and standard block matrix manipulation rules. In particular, you may rely on the facts that if $A$ is invertible, then $A A^{-1} = I$ and $A^{-1} A = I$, and that the inverse is unique when it exists.\n\nInstructions for constructing the matrices:\n- For each test, construct $U$ by drawing entries from a standard normal distribution and then scaling by a factor $\\gamma / \\sqrt{n}$, where $\\gamma$ is provided per test case. To guarantee well-conditioned and nonsingular updates, set $V = U$, so that $A = I + U U^\\top$ is symmetric positive definite for any real $\\gamma$.\n- The matrices are purely mathematical; no physical units are involved.\n\nAlgorithmic tasks:\n1. Starting from $A_0 = I$, and its inverse $A_0^{-1} = I$, incorporate each rank-$1$ term $u_i v_i^\\top$ sequentially to obtain $A_i = A_{i-1} + u_i v_i^\\top$ and update an explicit inverse $A_i^{-1}$ using only matrix-vector products, vector outer products, scalar operations, and matrix additions. Derive the update rule you need starting from the definition of the inverse.\n2. Compute a reference inverse $A_{\\mathrm{ref}}^{-1}$ using a direct method based on a robust numerical linear algebra algorithm.\n3. For each test case, compute the relative Frobenius error between your iterative inverse and the reference inverse:\n   $$ \\varepsilon = \\frac{\\lVert A_{\\mathrm{iter}}^{-1} - A_{\\mathrm{ref}}^{-1} \\rVert_F}{\\lVert A_{\\mathrm{ref}}^{-1} \\rVert_F}. $$\n4. Analyze computational cost in terms of scalar multiply-add operations under the following standardized model:\n   - Assume the cost for directly forming $A^{-1}$ by solving $A X = I$ using triangular factorization with $n$ right-hand sides is\n     $$ C_{\\text{direct}}(n) = \\frac{5}{3} \\, n^3. $$\n   - Assume the cost per rank-$1$ update in your iterative inverse update (from $A_{i-1}^{-1}$ to $A_i^{-1}$) is\n     $$ C_{\\text{rank-1}}(n) = 4 n^2, $$\n     counting two matrix-vector products, one outer product, and one matrix update at leading order, and ignoring lower-order terms. For $k$ updates, take\n     $$ C_{\\text{iter}}(n,k) = 4 k n^2. $$\n   - Report the cost ratio\n     $$ \\rho = \\frac{C_{\\text{iter}}(n,k)}{C_{\\text{direct}}(n)}. $$\n5. Your program must implement the above, using $V = U$ precisely as stated.\n\nTest suite:\n- Test 1 (general small): $n = 8$, $k = 3$, $\\gamma = 0.2$, random seed $= 0$.\n- Test 2 (moderate size): $n = 64$, $k = 4$, $\\gamma = 0.05$, random seed $= 1$.\n- Test 3 (edge case identity): $n = 5$, $k = 0$, $\\gamma = 0.0$, random seed $= 2$.\n- Test 4 (larger and more challenging): $n = 50$, $k = 10$, $\\gamma = 0.5$, random seed $= 3$.\n\nRequired final output:\n- For each test case, in the given order, output two values: first the relative Frobenius error $\\varepsilon$ (a floating-point number), then the cost ratio $\\rho$ (a floating-point number).\n- Aggregate all results into a single line string formatted as a Python-style list with comma-separated values and no spaces, for example $[\\varepsilon_1,\\rho_1,\\varepsilon_2,\\rho_2,\\dots]$.\n\nYour program must produce exactly one line in this format as its only output. No physical units or angles are involved, so no special unit handling is required.",
            "solution": "The problem of inverting a matrix of the form $A = I + U V^\\top$ is a classical problem in numerical linear algebra. The Sherman-Morrison-Woodbury formula provides a direct expression for the inverse. However, the present task requires a first-principles derivation of the update rule for a sequence of rank-$1$ modifications, which we shall now undertake.\n\nLet $A_{i-1} \\in \\mathbb{R}^{n \\times n}$ be an invertible matrix with a known inverse $A_{i-1}^{-1}$. We wish to find the inverse of the matrix $A_i$, which is formed by a rank-$1$ update:\n$$ A_i = A_{i-1} + u_i v_i^\\top $$\nwhere $u_i, v_i \\in \\mathbb{R}^n$ are column vectors. Let us denote the inverse we seek by $A_i^{-1}$.\n\nBy the definition of a matrix inverse, we must have $A_i A_i^{-1} = I$, where $I$ is the $n \\times n$ identity matrix. Substituting the expression for $A_i$:\n$$ (A_{i-1} + u_i v_i^\\top) A_i^{-1} = I $$\nAssuming $A_{i-1}$ is invertible, we can left-multiply by $A_{i-1}^{-1}$:\n$$ A_{i-1}^{-1} (A_{i-1} + u_i v_i^\\top) A_i^{-1} = A_{i-1}^{-1} I $$\n$$ (I + A_{i-1}^{-1} u_i v_i^\\top) A_i^{-1} = A_{i-1}^{-1} $$\nNow, we can isolate $A_i^{-1}$ by inverting the term in parentheses:\n$$ A_i^{-1} = (I + A_{i-1}^{-1} u_i v_i^\\top)^{-1} A_{i-1}^{-1} $$\nThe problem is now reduced to finding the inverse of the matrix $M = I + w v_i^\\top$, where we have defined the column vector $w = A_{i-1}^{-1} u_i$.\n\nWe hypothesize that the inverse of $M$ has a similar structure, namely $M^{-1} = I + \\alpha w v_i^\\top$ for some scalar $\\alpha$ to be determined. To find $\\alpha$, we enforce the condition $M M^{-1} = I$:\n$$ (I + w v_i^\\top)(I + \\alpha w v_i^\\top) = I $$\nExpanding the left-hand side using the distributive property of matrix multiplication:\n$$ I(I + \\alpha w v_i^\\top) + w v_i^\\top(I + \\alpha w v_i^\\top) = I $$\n$$ I + \\alpha w v_i^\\top + w v_i^\\top + w v_i^\\top (\\alpha w v_i^\\top) = I $$\nThe term $v_i^\\top w$ is a scalar, which is the inner product of $v_i$ and $w$. We can regroup terms:\n$$ I + (\\alpha w v_i^\\top + w v_i^\\top) + \\alpha (w v_i^\\top w v_i^\\top) = I $$\n$$ I + (\\alpha+1) w v_i^\\top + \\alpha w (v_i^\\top w) v_i^\\top = I $$\nSince $v_i^\\top w$ is a scalar, we can commute it:\n$$ I + (\\alpha+1) w v_i^\\top + \\alpha (v_i^\\top w) w v_i^\\top = I $$\nFactoring out the outer product $w v_i^\\top$:\n$$ I + [(\\alpha+1) + \\alpha(v_i^\\top w)] w v_i^\\top = I $$\nFor this equation to hold for general non-zero vectors $w$ and $v_i$, the scalar coefficient of the $w v_i^\\top$ term must be zero:\n$$ (\\alpha+1) + \\alpha(v_i^\\top w) = 0 $$\n$$ \\alpha(1 + v_i^\\top w) = -1 $$\nSolving for $\\alpha$, we find:\n$$ \\alpha = -\\frac{1}{1 + v_i^\\top w} $$\nThis expression is valid provided that the denominator $1 + v_i^\\top w \\neq 0$. If this condition holds, the inverse is:\n$$ (I + w v_i^\\top)^{-1} = I - \\frac{w v_i^\\top}{1 + v_i^\\top w} $$\nSubstituting this result back into our expression for $A_i^{-1}$:\n$$ A_i^{-1} = \\left(I - \\frac{w v_i^\\top}{1 + v_i^\\top w}\\right) A_{i-1}^{-1} $$\nSubstituting back $w = A_{i-1}^{-1} u_i$:\n$$ A_i^{-1} = \\left(I - \\frac{(A_{i-1}^{-1} u_i) v_i^\\top}{1 + v_i^\\top (A_{i-1}^{-1} u_i)}\\right) A_{i-1}^{-1} $$\nFinally, distributing the $A_{i-1}^{-1}$ term from the right:\n$$ A_i^{-1} = A_{i-1}^{-1} - \\frac{(A_{i-1}^{-1} u_i) (v_i^\\top A_{i-1}^{-1})}{1 + v_i^\\top A_{i-1}^{-1} u_i} $$\nThis is the Sherman-Morrison formula, which provides the sequential update rule for the inverse.\n\nFor this specific problem, we are given $V = U$, which means $v_i = u_i$ for all $i = 1, \\dots, k$. The matrices being constructed are $A_0 = I$ and $A_i = A_{i-1} + u_i u_i^\\top$ for $i  0$. Since $A_0=I$ is symmetric positive definite (SPD), and each update $u_i u_i^\\top$ is symmetric positive semi-definite, all matrices $A_i$ are SPD. Consequently, their inverses $A_i^{-1}$ are also SPD. This ensures that for any non-zero vector $u_i$, the quadratic form $u_i^\\top A_{i-1}^{-1} u_i  0$. Thus, the denominator $1 + u_i^\\top A_{i-1}^{-1} u_i$ is always strictly greater than $1$, precluding any division by zero.\n\nThe algorithmic implementation will thus proceed as follows:\n1. Initialize the inverse as $A_0^{-1} = I$.\n2. For each step $i = 1, \\dots, k$, update the current inverse $A_{i-1}^{-1}$ to $A_i^{-1}$ using the derived formula with $v_i=u_i$. This constitutes the iterative inverse, $A_{\\mathrm{iter}}^{-1}$.\n3. Construct the final matrix $A = I + U U^\\top$ directly and compute its inverse $A_{\\mathrm{ref}}^{-1}$ using a standard numerical library function, which typically relies on a robust factorization method like LU decomposition.\n4. Compute the relative Frobenius error $\\varepsilon = \\frac{\\lVert A_{\\mathrm{iter}}^{-1} - A_{\\mathrm{ref}}^{-1} \\rVert_F}{\\lVert A_{\\mathrm{ref}}^{-1} \\rVert_F}$.\n5. Compute the cost ratio $\\rho = C_{\\text{iter}}(n,k) / C_{\\text{direct}}(n)$, using the provided cost models:\n   $C_{\\text{iter}}(n,k) = 4 k n^2$\n   $C_{\\text{direct}}(n) = \\frac{5}{3} n^3$\n   This ratio simplifies to $\\rho = \\frac{4 k n^2}{(5/3) n^3} = \\frac{12k}{5n}$. This ratio indicates that the iterative method is more efficient when the number of updates $k$ is small relative to the matrix dimension $n$, specifically when $k  \\frac{5}{12}n$.\n\nThe implementation will follow these steps for each test case provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of iteratively inverting a matrix of the form A = I + UU^T\n    and compares it against a direct inversion method.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, k, gamma, random_seed)\n        (8, 3, 0.2, 0),\n        (64, 4, 0.05, 1),\n        (5, 0, 0.0, 2),\n        (50, 10, 0.5, 3),\n    ]\n\n    results = []\n    for n, k, gamma, seed in test_cases:\n        # Set the random seed for reproducibility\n        np.random.seed(seed)\n\n        # Construct matrix U. Handle the k=0 edge case.\n        if k  0:\n            scale_factor = gamma / np.sqrt(n)\n            U = np.random.randn(n, k) * scale_factor\n        else:\n            # An n x 0 matrix\n            U = np.zeros((n, 0))\n        \n        # As per instructions, V = U\n        V = U\n\n        # 1. Iterative inverse calculation using the derived rank-1 update rule.\n        # Start with the inverse of A_0 = I, which is I.\n        A_inv_iter = np.eye(n)\n        \n        # Sequentially apply k rank-1 updates.\n        for i in range(k):\n            u_i = U[:, i:i+1] # Get i-th column as a (n, 1) vector\n            v_i = V[:, i:i+1] # v_i is u_i\n            \n            # Sherman-Morrison formula:\n            # A_inv_new = A_inv - (A_inv @ u @ v.T @ A_inv) / (1 + v.T @ A_inv @ u)\n            \n            # Pre-calculate terms for clarity and efficiency\n            w = A_inv_iter @ u_i # This is the vector (A_{i-1}^{-1} u_i)\n            \n            # Denominator: 1 + v_i^T * (A_{i-1}^{-1} u_i)\n            denominator = 1.0 + (v_i.T @ w).item()\n            \n            # Since A_inv_iter is symmetric and v_i=u_i, the numerator term\n            # (A_inv @ u) @ (v.T @ A_inv) simplifies to w @ w.T\n            numerator_outer_product = w @ w.T\n            \n            # Update the inverse\n            A_inv_iter -= numerator_outer_product / denominator\n\n        # 2. Compute the reference inverse using a direct method.\n        # First, construct the full matrix A = I + U U^T\n        A = np.eye(n) + U @ U.T\n        \n        # Use a robust direct solver\n        A_inv_ref = np.linalg.inv(A)\n\n        # 3. Compute the relative Frobenius error.\n        norm_ref = np.linalg.norm(A_inv_ref, 'fro')\n        \n        if norm_ref == 0:\n            # This is unlikely for an invertible matrix, but for completeness:\n            # if reference is zero matrix, error is 0 if iterative is also zero, else infinity\n            epsilon = 0.0 if np.linalg.norm(A_inv_iter, 'fro') == 0.0 else np.inf\n        else:\n            norm_diff = np.linalg.norm(A_inv_iter - A_inv_ref, 'fro')\n            epsilon = norm_diff / norm_ref\n\n        # 4. Compute the computational cost ratio.\n        c_direct = (5.0 / 3.0) * (n**3)\n        c_iter = 4.0 * k * (n**2)\n        \n        # The ratio rho simplifies to 12*k / (5*n)\n        if c_direct == 0:\n            # Handle n=0 case to avoid division by zero\n            rho = np.inf if c_iter  0 else 0.0\n        else:\n            rho = c_iter / c_direct\n            \n        results.extend([epsilon, rho])\n\n    # Final print statement in the exact required format.\n    # The format is a string representing a list, with no spaces.\n    formatted_results = [f\"{val:.12g}\" for val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}