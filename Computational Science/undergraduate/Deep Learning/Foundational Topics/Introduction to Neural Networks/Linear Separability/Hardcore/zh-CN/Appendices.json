{
    "hands_on_practices": [
        {
            "introduction": "线性模型功能强大，但其能力有其固有的局限性。经典的异或（XOR）问题是证明这一点的完美示例。这个练习将引导你首先证明XOR问题在原始输入空间中是线性不可分的，然后通过添加一个简单的乘法特征，探索如何用最小的改动使其变得可分，从而直观地揭示特征工程在突破线性模型限制方面的力量。",
            "id": "3144385",
            "problem": "考虑一个在$\\mathbb{R}^{2}$中的二元分类任务，其输入$x = (x_1, x_2)$被限制在集合$\\{(-1,-1), (-1,1), (1,-1), (1,1)\\}$中。标签由一个小的非线性交互决定：对于一个固定的$\\delta$，其中$0  \\delta  1$，定义\n$$\ny(x) = \\mathrm{sign}\\!\\big(x_1 x_2 + \\delta\\big),\n$$\n其中如果$t > 0$，则$\\mathrm{sign}(t) = 1$；如果$t  0$，则$\\mathrm{sign}(t) = -1$。换句话说，满足$x_1 x_2 = 1$的点$x$的标签为$+1$，而满足$x_1 x_2 = -1$的点的标签为$-1$。一个单层线性分类器的决策函数为$f(x) = w_1 x_1 + w_2 x_2 + b$，其参数为$(w_1, w_2, b) \\in \\mathbb{R}^{3}$。\n\n从线性可分性的定义（即存在$w \\in \\mathbb{R}^{2}$和$b \\in \\mathbb{R}$，使得对于所有带标签$y_i$的训练点$x_i$，都有$y_i\\,(w^\\top x_i + b) > 0$）出发，首先确定该数据集在原始输入空间中是否在$f(x)$下线性可分。然后，考虑用$m$个乘法交互单元来增强分类器，每个单元计算标量特征$z_k(x) = x_1 x_2$（其中$k = 1, \\dots, m$），并形成增强的线性决策函数\n$$\nf_m(x) = \\tilde{w}_1 x_1 + \\tilde{w}_2 x_2 + \\sum_{k=1}^{m} \\tilde{v}_k\\, z_k(x) + \\tilde{b},\n$$\n其参数为$(\\tilde{w}_1, \\tilde{w}_2, \\tilde{v}_1, \\dots, \\tilde{v}_m, \\tilde{b}) \\in \\mathbb{R}^{m+3}$。确定最小整数$m^{\\star}$，使得存在参数能让该数据集在增强特征空间中线性可分，即对于所有四个输入$x$，都有$y(x)\\, f_{m^{\\star}}(x) > 0$。您的答案必须是$m^{\\star}$的值，以单个整数形式给出。无需四舍五入。",
            "solution": "用户希望我按照以下步骤解决问题：\n1.  **验证问题：** 问题陈述是有效的。\n    -   **给定条件：**\n        -   输入空间：$x = (x_1, x_2) \\in \\mathbb{R}^{2}$，点来自集合$\\{(-1,-1), (-1,1), (1,-1), (1,1)\\}$。\n        -   标签函数：$y(x) = \\mathrm{sign}(x_1 x_2 + \\delta)$，对于一个固定的$\\delta$，其中$0  \\delta  1$。\n        -   线性分类器：$f(x) = w_1 x_1 + w_2 x_2 + b$。\n        -   线性可分性条件：对于所有点$(x_i, y_i)$，$y_i (w^\\top x_i + b) > 0$。\n        -   增强特征单元：$z_k(x) = x_1 x_2$，其中$k = 1, \\dots, m$。\n        -   增强分类器：$f_m(x) = \\tilde{w}_1 x_1 + \\tilde{w}_2 x_2 + \\sum_{k=1}^{m} \\tilde{v}_k z_k(x) + \\tilde{b}$。\n    -   **验证：** 该问题是机器学习中一个标准的、定义明确的练习，涉及线性模型的表示能力，特别是异或（XOR）问题。所有术语都有正式定义，设置是一致的，并且在科学上基于神经网络理论。该问题是有效的。\n\n2.  **解决问题：**\n首先，我们使用给定的标签函数$y(x) = \\mathrm{sign}(x_1 x_2 + \\delta)$（其中$0  \\delta  1$）来确定四个输入点的标签。\n\n-   对于$x^{(1)} = (1,1)$，$x_1 x_2 = 1$。$y^{(1)} = \\mathrm{sign}(1+\\delta) = +1$。\n-   对于$x^{(2)} = (-1,-1)$，$x_1 x_2 = 1$。$y^{(2)} = \\mathrm{sign}(1+\\delta) = +1$。\n-   对于$x^{(3)} = (1,-1)$，$x_1 x_2 = -1$。$y^{(3)} = \\mathrm{sign}(-1+\\delta) = -1$。\n-   对于$x^{(4)} = (-1,1)$，$x_1 x_2 = -1$。$y^{(4)} = \\mathrm{sign}(-1+\\delta) = -1$。\n\n该数据集包含两个标签为$+1$的点$\\{(1,1), (-1,-1)\\}$，和两个标签为$-1$的点$\\{(1,-1), (-1,1)\\}$。这是经典的异或（XOR）问题。\n\n**第1部分：原始输入空间中的线性可分性**\n\n如果存在权重向量$w = (w_1, w_2)$和偏置$b$，使得对于所有四个点都有$y_i (w^\\top x_i + b) > 0$，则该数据集是线性可分的。让我们为我们的数据集写下不等式：\n1.  对于$(x^{(1)}, y^{(1)}) = ((1,1), +1)$：$1 \\cdot (w_1(1) + w_2(1) + b) > 0 \\implies w_1 + w_2 + b > 0$。\n2.  对于$(x^{(2)}, y^{(2)}) = ((-1,-1), +1)$：$1 \\cdot (w_1(-1) + w_2(-1) + b) > 0 \\implies -w_1 - w_2 + b > 0$。\n3.  对于$(x^{(3)}, y^{(3)}) = ((1,-1), -1)$：$-1 \\cdot (w_1(1) + w_2(-1) + b) > 0 \\implies w_1 - w_2 + b  0$。\n4.  对于$(x^{(4)}, y^{(4)}) = ((-1,1), -1)$：$-1 \\cdot (w_1(-1) + w_2(1) + b) > 0 \\implies -w_1 + w_2 + b  0$。\n\n我们将不等式(1)和(2)相加：\n$(w_1 + w_2 + b) + (-w_1 - w_2 + b) > 0 \\implies 2b > 0 \\implies b > 0$。\n\n现在我们来考虑不等式(3)和(4)：\n我们将不等式(3)和(4)相加：\n$(w_1 - w_2 + b) + (-w_1 + w_2 + b)  0 \\implies 2b  0 \\implies b  0$。\n\n我们推导出了$b > 0$和$b  0$的条件，这是一个逻辑矛盾。因此，不存在这样的参数$(w_1, w_2, b)$。该数据集在原始输入空间$\\mathbb{R}^2$中不是线性可分的。\n\n**第2部分：增强特征空间中的线性可分性与最小$m^{\\star}$**\n\n问题引入了一个增强的线性决策函数：\n$$\nf_m(x) = \\tilde{w}_1 x_1 + \\tilde{w}_2 x_2 + \\sum_{k=1}^{m} \\tilde{v}_k\\, z_k(x) + \\tilde{b}\n$$\n其中每个交互单元计算相同的特征$z_k(x) = x_1 x_2$。该函数可以通过提取公因子$x_1 x_2$来重写：\n$$\nf_m(x) = \\tilde{w}_1 x_1 + \\tilde{w}_2 x_2 + \\left(\\sum_{k=1}^{m} \\tilde{v}_k\\right) (x_1 x_2) + \\tilde{b}\n$$\n我们定义新特征$z(x) = x_1 x_2$的有效权重为$W_v = \\sum_{k=1}^{m} \\tilde{v}_k$。决策函数现在是三维特征空间中的一个线性函数，坐标为$(x_1, x_2, x_1 x_2)$：\n$$\nf_m(x) = \\tilde{w}_1 x_1 + \\tilde{w}_2 x_2 + W_v (x_1 x_2) + \\tilde{b}\n$$\n如果我们能找到满足条件$y(x) f_m(x) > 0$（对于所有四个点）的参数$(\\tilde{w}_1, \\tilde{w}_2, W_v, \\tilde{b})$，问题就变得线性可分。\n\n问题在于找到能使之成为可能的最小整数$m$。能否实现可分性取决于有效权重$W_v$是否可以为非零值。\n-   如果$m=0$，和$\\sum_{k=1}^{0} \\tilde{v}_k$是一个空和，其值为$0$。所以，$W_v=0$。决策函数简化为$f_0(x) = \\tilde{w}_1 x_1 + \\tilde{w}_2 x_2 + \\tilde{b}$。这就是原始的线性分类器，我们已经证明它无法分离数据。因此，$m^{\\star}$必须大于$0$。\n-   如果$m \\ge 1$，我们可以选择权重$\\tilde{v}_k$使其和$W_v$为非零。例如，如果$m=1$，我们有$W_v = \\tilde{v}_1$。我们可以选择$\\tilde{v}_1 = 1$。如果$m=2$，我们有$W_v = \\tilde{v}_1 + \\tilde{v}_2$。我们可以选择$\\tilde{v}_1 = 1$和$\\tilde{v}_2 = 0$来得到$W_v=1$。通常，对于任何$m \\ge 1$，我们可以设置$\\tilde{v}_1 = 1$和$\\tilde{v}_k=0$（对于$k>1$）来得到$W_v=1$。\n\n所以，当且仅当$m \\ge 1$时，线性可分是可能的。让我们证明对于$m=1$确实是可能的。\n设$m=1$。那么$W_v = \\tilde{v}_1$。我们尝试找到一组有效的参数。考虑一个简单的选择：$\\tilde{w}_1 = 0$，$\\tilde{w}_2 = 0$，和$\\tilde{b} = 0$。\n决策函数变为$f_1(x) = \\tilde{v}_1 (x_1 x_2)$。\n线性可分性的条件是$y(x) \\cdot (\\tilde{v}_1 x_1 x_2) > 0$。\n\n我们来检查一下这两类点：\n-   $+1$类：点满足$x_1 x_2 = 1$且标签为$y=+1$。\n    条件是$(+1) \\cdot (\\tilde{v}_1 \\cdot 1) > 0 \\implies \\tilde{v}_1 > 0$。\n-   $-1$类：点满足$x_1 x_2 = -1$且标签为$y=-1$。\n    条件是$(-1) \\cdot (\\tilde{v}_1 \\cdot (-1)) > 0 \\implies \\tilde{v}_1 > 0$。\n\n两类点都要求$\\tilde{v}_1 > 0$。我们可以自由选择参数，所以我们可以设置$\\tilde{v}_1 = 1$。这满足了条件。\n当$m=1$时，我们可以选择参数$(\\tilde{w}_1, \\tilde{w}_2, \\tilde{v}_1, \\tilde{b}) = (0, 0, 1, 0)$来使数据集线性可分。\n决策函数是$f_1(x) = x_1 x_2$。\n对于$y=+1$的点，$x_1x_2=1$，所以$y \\cdot f_1(x) = 1 \\cdot 1 = 1 > 0$。\n对于$y=-1$的点，$x_1x_2=-1$，所以$y \\cdot f_1(x) = (-1) \\cdot (-1) = 1 > 0$。\n所有点都满足该条件。\n\n由于$m=0$不充分而$m=1$是充分的，因此使数据集变得线性可分的最小整数值$m^{\\star}$是$1$。",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "当我们确信数据是（或通过变换后是）线性可分时，下一个关键问题是如何找到那条“完美”的分割线。这个练习将让你从零开始实现逻辑回归，这是一种用于寻找分界超平面的基础且强大的算法。通过亲手编写代码并使用梯度下降法进行优化，你将深刻理解分类器是如何从数据中“学习”决策边界的。",
            "id": "3278883",
            "problem": "你需要从第一性原理出发，实现二元逻辑回归，使用最速下降法（也称为梯度下降法）进行训练，以获得一个用于线性可分数据集的分离超平面。从以下原理开始：独立的二元标签被建模为伯努利随机变量，其成功概率通过一个线性预测器遵循逻辑连接，并且参数是通过最小化由负对数似然构建的正则化经验风险来选择的。不要使用预构建的机器学习库；从基本定义推导出所需的梯度，并显式地实现优化过程。\n\n程序必须：\n- 在二维空间中构建两个固定的确定性数据集。\n- 使用带有固定学习率和迭代次数的最速下降法，最小化一个正则化经验风险。\n- 提供一个标准化特征的选项，以减少病态条件。\n- 对于每个提供的测试用例，将训练分类准确率报告为介于 $0$ 和 $1$ 之间的小数。\n\n你必须使用的基本原理：\n- 对二元标签 $y_i \\in \\{0,1\\}$ 进行独立的伯努利建模。\n- 逻辑连接 $p_i = \\sigma(z_i)$，其中 $\\sigma$ 是逻辑S型函数，$z_i$ 是线性预测器。\n- 使用平均负对数似然加上对权重的 $\\ell_2$ 惩罚项进行经验风险最小化。\n\n使用的定义：\n- 对于输入 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 和参数 $(\\mathbf{w}, b)$，定义线性预测器 $z_i = \\mathbf{w}^\\top \\mathbf{x}_i + b$ 和逻辑S型函数 $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$。\n- 定义正则化经验风险\n$$\n\\mathcal{L}(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^n \\Big( - y_i \\log(\\sigma(z_i)) - (1 - y_i) \\log(1 - \\sigma(z_i)) \\Big) + \\frac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert_2^2,\n$$\n其中 $n$ 是样本数量，$\\lambda \\ge 0$ 是正则化参数。\n- 最速下降法通过朝 $\\mathcal{L}$ 梯度相反的方向移动来更新参数，步长为 $\\alpha > 0$，迭代固定次数。\n\n数据集：\n- 数据集 $D_1$（线性可分，尺度适中）。正类 ($y=1$) 点：\n$$\n\\{(2.5, 2.0), (3.0, 1.0), (2.0, 2.5), (3.5, 2.2), (2.2, 3.0), (2.8, 2.7), (3.2, 1.8), (2.4, 3.1)\\}.\n$$\n负类 ($y=0$) 点：\n$$\n\\{(-2.5, -1.5), (-3.0, -2.0), (-2.0, -2.2), (-3.2, -1.8), (-1.8, -2.5), (-2.7, -2.9), (-3.1, -1.7), (-2.3, -3.2)\\}.\n$$\n设 $X^{(1)}$ 是 $D_1$ 中所有点的堆叠，$y^{(1)}$ 是对应的标签。\n- 数据集 $D_2$（几何形状相同但条件不佳）。通过将 $X^{(1)}$ 的第二个特征乘以因子 $1000$ 来构建 $X^{(2)}$，也就是说，对于 $X^{(1)}$ 中的每个点 $(x_1, x_2)$，在 $X^{(2)}$ 中包含 $(x_1, 1000 x_2)$。对 $y^{(2)}$ 使用与 $y^{(1)}$ 相同的标签。\n\n标准化选项：\n- 如果启用标准化，通过 $\\tilde{X}_{ij} = \\dfrac{X_{ij} - \\mu_j}{\\sigma_j}$ 将 $X$ 转换为 $\\tilde{X}$，其中 $\\mu_j$ 和 $\\sigma_j$ 分别是特征 $j$ 在训练样本中的均值和标准差，约定如果 $\\sigma_j = 0$ 则使用 $\\sigma_j = 1$。\n\n训练与预测：\n- 将 $\\mathbf{w}$ 初始化为 $\\mathbb{R}^2$ 中的零向量，将 $b$ 初始化为 $0$。\n- 使用学习率 $\\alpha$ 执行固定次数的最速下降迭代，以最小化 $\\mathcal{L}(\\mathbf{w}, b)$。\n- 训练后，使用规则 $\\hat{y}_i = 1$ 如果 $\\sigma(z_i) \\ge 0.5$，否则 $\\hat{y}_i = 0$ 来预测类别标签 $\\hat{y}_i$。\n- 将训练准确率计算为小数 $\\dfrac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{\\hat{y}_i = y_i\\}$，其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n\n测试套件：\n提供以下四个测试用例，每个用例由元组 $(\\text{dataset}, \\alpha, \\text{iterations}, \\lambda, \\text{standardize})$ 指定，其中 $\\text{dataset} \\in \\{D_1, D_2\\}$, $\\alpha > 0$, $\\text{iterations} \\in \\mathbb{N}$, $\\lambda \\ge 0$, 且 $\\text{standardize} \\in \\{\\text{True}, \\text{False}\\}$。\n- 用例 1 (理想情况): $(D_1, 0.1, 3000, 0.01, \\text{True})$。\n- 用例 2 (边界条件：非常小的步长): $(D_1, 0.0001, 200, 0.01, \\text{True})$。\n- 用例 3 (边缘情况：无标准化的病态条件): $(D_2, 0.0001, 3000, 0.01, \\text{False})$。\n- 用例 4 (边缘情况：可能导致欠拟合的强正则化): $(D_1, 0.1, 3000, 10.0, \\text{True})$。\n\n要求的最终输出格式：\n- 你的程序应生成单行输出，其中包含四个用例的训练分类准确率，以逗号分隔的列表形式包含在方括号中，每个准确率四舍五入到六位小数，例如 $$[a_1,a_2,a_3,a_4]$$ 其中每个 $a_k$ 都是一个十进制形式的浮点数。不应打印任何其他文本。",
            "solution": "该问题要求使用最速下降法（也称为梯度下降法）训练一个二元逻辑回归模型，并且整个过程必须从第一性原理推导。在给出计算解决方案之前，我们必须形式化底层的数学模型，并推导出优化所需的方程。\n\n### 1. 概率模型与似然\n\n逻辑回归的基础是对二元结果的概率建模。给定一个包含 $n$ 个样本的数据集 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 是一个特征向量，$y_i \\in \\{0, 1\\}$ 是对应的二元标签。\n\n我们将每个标签 $y_i$ 建模为一个独立的伯努利随机变量，$Y_i \\sim \\text{Bernoulli}(p_i)$，其中 $p_i$ 是“成功”结果的概率，即 $P(Y_i=1|\\mathbf{x}_i) = p_i$。\n\n逻辑回归的核心是特征向量 $\\mathbf{x}_i$ 和概率 $p_i$ 之间的“连接”。这是通过一个线性预测器 $z_i$ 和逻辑S型函数 $\\sigma(z)$ 实现的。\n线性预测器是输入特征的线性函数，由一个权重向量 $\\mathbf{w} \\in \\mathbb{R}^d$ 和一个偏置项 $b \\in \\mathbb{R}$ 参数化：\n$$ z_i = \\mathbf{w}^\\top \\mathbf{x}_i + b $$\n逻辑S型函数将这个无界的线性预测器 $z_i \\in \\mathbb{R}$ 映射到一个在 $(0, 1)$ 范围内的有效概率：\n$$ p_i = \\sigma(z_i) = \\frac{1}{1 + e^{-z_i}} $$\n因此，对于第 $i$ 个样本，每个结果的概率由下式给出：\n$$ P(Y_i=y_i|\\mathbf{x}_i; \\mathbf{w}, b) = p_i^{y_i} (1 - p_i)^{1 - y_i} $$\n假设样本是独立同分布的（i.i.d.），在给定特征 $X = (\\mathbf{x}_1, ..., \\mathbf{x}_n)$ 和参数 $(\\mathbf{w}, b)$ 的条件下，观测到整个标签集 $\\mathbf{y} = (y_1, ..., y_n)$ 的总似然是各个概率的乘积：\n$$ L(\\mathbf{w}, b) = \\prod_{i=1}^n P(Y_i=y_i|\\mathbf{x}_i; \\mathbf{w}, b) = \\prod_{i=1}^n p_i^{y_i} (1 - p_i)^{1 - y_i} $$\n\n### 2. 目标函数：正则化经验风险\n\n最大似然估计（MLE）原则指导我们选择能使似然 $L(\\mathbf{w}, b)$ 最大化的参数 $(\\mathbf{w}, b)$。在数学上，处理对数似然 $\\ell(\\mathbf{w}, b) = \\log L(\\mathbf{w}, b)$ 更为方便，因为它将乘积转换为和，并且不会改变最大值的位置。\n$$ \\ell(\\mathbf{w}, b) = \\sum_{i=1}^n \\log \\left( p_i^{y_i} (1 - p_i)^{1 - y_i} \\right) = \\sum_{i=1}^n \\left( y_i \\log p_i + (1 - y_i) \\log(1 - p_i) \\right) $$\n代入 $p_i = \\sigma(z_i)$：\n$$ \\ell(\\mathbf{w}, b) = \\sum_{i=1}^n \\left( y_i \\log(\\sigma(z_i)) + (1 - y_i) \\log(1 - \\sigma(z_i)) \\right) $$\n在机器学习中，我们通常将问题框架化为最小化一个损失或风险函数。最大化对数似然等价于最小化负对数似然。经验风险是数据集上的平均负对数似然：\n$$ \\mathcal{R}_{\\text{emp}}(\\mathbf{w}, b) = -\\frac{1}{n} \\ell(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^n \\left( -y_i \\log(\\sigma(z_i)) - (1 - y_i) \\log(1 - \\sigma(z_i)) \\right) $$\n这个函数也被称为二元交叉熵损失。\n\n为了防止过拟合并提高泛化能力，我们在经验风险上增加了一个正则化项。问题指定了对权重的 $\\ell_2$ 惩罚项（也称为Tikhonov正则化或权重衰减），它惩罚大的权重值。我们必须最小化的正则化经验风险是：\n$$ \\mathcal{L}(\\mathbf{w}, b) = \\mathcal{R}_{\\text{emp}}(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert_2^2 $$\n$$ \\mathcal{L}(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^n \\Big( - y_i \\log(\\sigma(z_i)) - (1 - y_i) \\log(1 - \\sigma(z_i)) \\Big) + \\frac{\\lambda}{2} \\sum_{j=1}^d w_j^2 $$\n其中 $\\lambda \\ge 0$ 是正则化参数。请注意，偏置项 $b$ 通常不进行正则化。\n\n### 3. 通过最速下降法进行优化\n\n最速下降法是一种迭代优化算法，它通过在目标函数梯度的相反方向上迈出一步来更新参数。更新规则是：\n$$ \\mathbf{w}^{(k+1)} \\leftarrow \\mathbf{w}^{(k)} - \\alpha \\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}^{(k)}, b^{(k)}) $$\n$$ b^{(k+1)} \\leftarrow b^{(k)} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}(\\mathbf{w}^{(k)}, b^{(k)}) $$\n其中 $\\alpha > 0$ 是学习率。为了实现这一点，我们必须计算 $\\mathcal{L}(\\mathbf{w}, b)$ 关于 $\\mathbf{w}$ 和 $b$ 的偏导数。\n\n首先，我们建立一个关于S型函数导数的关键恒等式：\n$$ \\frac{d\\sigma}{dz} = \\frac{d}{dz} (1 + e^{-z})^{-1} = -(1+e^{-z})^{-2}(-e^{-z}) = \\frac{e^{-z}}{(1+e^{-z})^2} = \\frac{1}{1+e^{-z}} \\cdot \\frac{e^{-z}}{1+e^{-z}} = \\sigma(z)(1 - \\sigma(z)) $$\n\n让我们先计算非正则化部分的梯度。设 $L_i = -y_i \\log(\\sigma_i) - (1-y_i)\\log(1-\\sigma_i)$，其中 $\\sigma_i = \\sigma(z_i)$。对单个权重 $w_j$ 使用链式法则：\n$$ \\frac{\\partial L_i}{\\partial w_j} = \\frac{\\partial L_i}{\\partial \\sigma_i} \\frac{\\partial \\sigma_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial w_j} $$\n各部分为：\n- $\\frac{\\partial L_i}{\\partial \\sigma_i} = -\\frac{y_i}{\\sigma_i} - \\frac{1-y_i}{-(1-\\sigma_i)} = -\\frac{y_i}{\\sigma_i} + \\frac{1-y_i}{1-\\sigma_i} = \\frac{-y_i(1-\\sigma_i) + \\sigma_i(1-y_i)}{\\sigma_i(1-\\sigma_i)} = \\frac{\\sigma_i - y_i}{\\sigma_i(1-\\sigma_i)}$\n- $\\frac{\\partial \\sigma_i}{\\partial z_i} = \\sigma_i(1-\\sigma_i)$\n- $\\frac{\\partial z_i}{\\partial w_j} = \\frac{\\partial}{\\partial w_j}(\\sum_{k=1}^d w_k x_{ik} + b) = x_{ij}$ (其中 $x_{ij}$ 是样本 $i$ 的第 $j$ 个特征)\n\n将它们组合起来得到：\n$$ \\frac{\\partial L_i}{\\partial w_j} = \\left( \\frac{\\sigma_i - y_i}{\\sigma_i(1-\\sigma_i)} \\right) (\\sigma_i(1-\\sigma_i)) (x_{ij}) = (\\sigma_i - y_i) x_{ij} $$\n完整目标函数 $\\mathcal{L}$ 关于 $w_j$ 的梯度是：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial L_i}{\\partial w_j} + \\frac{\\partial}{\\partial w_j} \\left( \\frac{\\lambda}{2} \\sum_{k=1}^d w_k^2 \\right) = \\frac{1}{n} \\sum_{i=1}^n (\\sigma_i - y_i) x_{ij} + \\lambda w_j $$\n对于偏置项 $b$，链式法则类似，但 $\\frac{\\partial z_i}{\\partial b} = 1$：\n$$ \\frac{\\partial L_i}{\\partial b} = (\\sigma_i - y_i) \\cdot 1 = \\sigma_i - y_i $$\n正则化项不依赖于 $b$，所以其导数为零。因此：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma_i - y_i) $$\n最终梯度表达式 $(\\sigma_i - y_i)$ 的简洁性是使用逻辑损失与线性模型的一个显著而优雅的特性。\n\n### 4. 向量化实现\n\n为了高效计算，我们用向量形式表示梯度。设 $X$ 是 $n \\times d$ 的特征向量矩阵（设计矩阵），$\\mathbf{y}$ 是 $n \\times 1$ 的标签向量，$\\mathbf{w}$ 是 $d \\times 1$ 的权重向量，$\\boldsymbol{\\sigma}$ 是 $n \\times 1$ 的预测概率 $\\sigma_i$ 向量。\n线性预测器向量为 $\\mathbf{z} = X\\mathbf{w} + b\\mathbf{1}$，其中 $\\mathbf{1}$ 是一个全为1的向量。\n梯度向量 $\\nabla_{\\mathbf{w}} \\mathcal{L}$ 和标量导数 $\\frac{\\partial \\mathcal{L}}{\\partial b}$ 为：\n$$ \\nabla_{\\mathbf{w}} \\mathcal{L} = \\frac{1}{n} X^\\top(\\boldsymbol{\\sigma} - \\mathbf{y}) + \\lambda \\mathbf{w} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma_i - y_i) = \\frac{1}{n} \\mathbf{1}^\\top(\\boldsymbol{\\sigma} - \\mathbf{y}) $$\n一次迭代的最速下降更新规则是：\n1.  计算线性预测器：$\\mathbf{z} = X\\mathbf{w} + b$\n2.  计算概率：$\\boldsymbol{\\sigma} = \\sigma(\\mathbf{z})$\n3.  计算梯度：\n    - $\\nabla_{\\mathbf{w}} \\mathcal{L} = \\frac{1}{n} X^\\top(\\boldsymbol{\\sigma} - \\mathbf{y}) + \\lambda \\mathbf{w}$\n    - $\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{n} \\text{mean}(\\boldsymbol{\\sigma} - \\mathbf{y})$\n4.  更新参数：\n    - $\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\nabla_{\\mathbf{w}} \\mathcal{L}$\n    - $b \\leftarrow b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$\n\n### 5. 标准化与正则化\n\n- **标准化**：此预处理步骤将特征转换为零均值和单位标准差。对于尺度差异巨大的特征（如数据集 $D_2$），梯度下降法可能表现不佳。梯度的幅度将由尺度较大的特征主导，导致在某些方向上收敛缓慢，而在其他方向上可能出现振荡。标准化将所有特征置于共同的尺度上，使损失函数的等值集更接近球形，从而减轻这种病态条件。\n\n- **正则化**：$\\ell_2$ 惩罚项 $\\frac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert_2^2$ 用于控制模型复杂度。一个大的 $\\lambda$ 值会迫使权重 $\\mathbf{w}$ 变小，从而产生一个具有较不复杂决策边界的“更简单”模型。这可以防止对训练数据的过拟合。然而，如果 $\\lambda$ 过大（如案例4中所探讨的），可能会导致欠拟合，即模型过于简单，无法捕捉数据的基本结构，即使在训练集上也会导致性能不佳。\n\n### 6. 预测与评估\n\n经过固定次数的迭代训练后，最终参数 $(\\mathbf{w}, b)$ 定义了一个分离超平面 $\\mathbf{w}^\\top \\mathbf{x} + b = 0$。新样本 $\\mathbf{x}$ 的分类取决于它落在超平面的哪一侧。预测规则是：\n$$ \\hat{y}_i = \\begin{cases} 1  \\text{if } \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 0.5 \\\\ 0  \\text{if } \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b)  0.5 \\end{cases} $$\n由于 $\\sigma(z)$ 是一个单调递增函数且 $\\sigma(0) = 0.5$，这等价于：\n$$ \\hat{y}_i = \\begin{cases} 1  \\text{if } \\mathbf{w}^\\top \\mathbf{x}_i + b \\ge 0 \\\\ 0  \\text{if } \\mathbf{w}^\\top \\mathbf{x}_i + b  0 \\end{cases} $$\n模型在训练数据上的性能使用分类准确率进行评估，定义为正确分类样本的比例：\n$$ \\text{Accuracy} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{\\hat{y}_i = y_i\\} $$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。下面的程序实现了这整个过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests binary logistic regression from first principles\n    using the steepest descent method.\n    \"\"\"\n\n    # --- Dataset Construction ---\n    # Dataset D1 (linearly separable, moderate scale)\n    X1_pos = np.array([\n        [2.5, 2.0], [3.0, 1.0], [2.0, 2.5], [3.5, 2.2], \n        [2.2, 3.0], [2.8, 2.7], [3.2, 1.8], [2.4, 3.1]\n    ])\n    X1_neg = np.array([\n        [-2.5, -1.5], [-3.0, -2.0], [-2.0, -2.2], [-3.2, -1.8], \n        [-1.8, -2.5], [-2.7, -2.9], [-3.1, -1.7], [-2.3, -3.2]\n    ])\n    X1 = np.vstack((X1_pos, X1_neg))\n    y1 = np.array([1] * 8 + [0] * 8)\n\n    # Dataset D2 (ill-conditioned)\n    X2 = np.copy(X1)\n    X2[:, 1] *= 1000.0\n    y2 = np.copy(y1)\n    \n    datasets = {\n        'D1': (X1, y1),\n        'D2': (X2, y2)\n    }\n\n    # --- Test Suite ---\n    test_cases = [\n        # (dataset_name, alpha, iterations, lambda_reg, standardize)\n        ('D1', 0.1, 3000, 0.01, True),\n        ('D1', 0.0001, 200, 0.01, True),\n        ('D2', 0.0001, 3000, 0.01, False),\n        ('D1', 0.1, 3000, 10.0, True),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        dataset_name, alpha, iterations, lambda_reg, standardize = case\n        X_orig, y = datasets[dataset_name]\n        \n        X = np.copy(X_orig)\n        \n        # --- Standardization ---\n        if standardize:\n            mu = np.mean(X, axis=0)\n            sigma = np.std(X, axis=0)\n            # Per problem spec, if stddev is 0, use 1 to avoid division by zero.\n            sigma[sigma == 0] = 1.0\n            X = (X - mu) / sigma\n\n        n_samples, n_features = X.shape\n\n        # --- Initialization ---\n        w = np.zeros(n_features)\n        b = 0.0\n\n        # --- Training via Steepest Descent ---\n        for _ in range(iterations):\n            # Linear predictor: z = Xw + b\n            z = X @ w + b\n            \n            # Sigmoid activation: sigma(z)\n            # This is p_i, the predicted probability for class 1\n            predictions_prob = 1 / (1 + np.exp(-z))\n            \n            # Error term: (sigma(z) - y)\n            error = predictions_prob - y\n            \n            # Compute gradients\n            # Gradient of loss w.r.t. w: (1/n) * X^T * (sigma(z) - y) + lambda * w\n            grad_w = (1 / n_samples) * (X.T @ error) + lambda_reg * w\n            \n            # Gradient of loss w.r.t. b: (1/n) * sum(sigma(z) - y)\n            grad_b = (1 / n_samples) * np.sum(error)\n            \n            # Update parameters\n            w -= alpha * grad_w\n            b -= alpha * grad_b\n\n        # --- Prediction and Accuracy ---\n        # Note: We use the original (unstandardized) X for final evaluation if standardization\n        # was done, as standardization parameters (mu, sigma) are part of the learned model.\n        # But here, we are asked for TRAINING accuracy, so we predict on the same data we trained on (X).\n        final_z = X @ w + b\n        \n        # Predict labels: 1 if z >= 0, else 0\n        y_pred = (final_z >= 0).astype(int)\n        \n        # Calculate accuracy\n        accuracy = np.mean(y_pred == y)\n        results.append(f\"{accuracy:.6f}\")\n\n    # --- Final Output ---\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "对于像相互缠绕的螺旋线这样复杂的非线性数据，手动设计特征变换几乎是不可能的。这正是现代深度学习大放异彩的地方。本练习将带你构建一个简单的两层神经网络，并将其应用于螺旋线数据集，你将亲眼见证网络如何自动学习一个高维表示，在这个新的“视角”下，原本纠纏在一起的数据点变得线性可分。",
            "id": "3144398",
            "problem": "您的任务是研究一个双层修正线性单元 (ReLU) 网络在由 $\\mathbb{R}^2$ 中两条交织螺旋线组成的合成数据集上所引发的线性可分性。请从基本定义开始：一个标签为 $y_i \\in \\{-1,+1\\}$ 的二元数据集 $\\{(x_i,y_i)\\}_{i=1}^n$ 是线性可分的，如果存在 $(w,b)$ 使得对于所有的 $i$ 都有 $y_i(w^\\top x_i + b) > 0$。一个双层 ReLU 网络计算隐藏表示 $z = \\sigma(W_1 x + b_1)$，其中 $\\sigma(t) = \\max\\{0,t\\}$ 是逐元素应用的，并产生一个标量输出 $o = w_2^\\top z + b_2$。目标是确定网络产生的隐藏表示是否是线性可分的，并估计需要多少个隐藏神经元才能足够好地近似螺旋线的展开以引发可分性。\n\n按如下方式构建数据集。对于类别 $+1$，在 $[0,4\\pi]$ (角度单位：弧度) 中均匀采样角度 $\\theta$，设置 $r = a\\theta$ 和 $x = (r\\cos\\theta, r\\sin\\theta)$。对于类别 $-1$，采样相同的角度但平移 $\\pi$，即 $\\theta' = \\theta + \\pi$，设置 $r' = a\\theta'$ 和 $x' = (r'\\cos\\theta', r'\\sin\\theta')$。使用 $a = 0.5$ 且每个类别样本数相等。这将产生两条相位相反的交织螺旋线。\n\n定义一个具有 $m$ 个隐藏神经元的双层 ReLU 网络：$z_i = \\sigma(W_1 x_i + b_1)$，$o_i = w_2^\\top z_i + b_2$。通过对参数进行梯度下降，最小化经验铰链损失 $L = \\frac{1}{n}\\sum_{i=1}^n \\max(0, 1 - y_i o_i)$ 来训练 $(W_1,b_1,w_2,b_2)$。训练后，通过求解一个线性规划问题来评估隐藏表示 $\\{(z_i,y_i)\\}_{i=1}^n$ 的线性可分性。该线性规划在 $(w,b)$ 的 $\\ell_1$ 范数有界（即 $\\|w\\|_1 + |b| \\le 1$）以及对所有 $i$ 均满足约束 $y_i(w^\\top z_i + b) \\ge \\gamma$ 的条件下，最大化间隔变量 $\\gamma$。如果最优的 $\\gamma$ 严格为正，则判定隐藏表示为线性可分；否则，判定其为非线性可分。通过缩放属性来证明该测试的合理性：如果某个 $(w,b)$ 能将数据分开，那么通过缩放以满足 $\\|w\\|_1 + |b| \\le 1$ 会保留一个严格为正的间隔，因此存在一个正的 $\\gamma$。\n\n测试套件。使用 $a = 0.5$ 和每个类别 $N_c = 128$ 个样本 ($n = 2N_c$)。使用以下隐藏层大小 $m$ 来评估网络：$m \\in \\{4,8,16,32\\}$，它们分别探究了边缘情况（小容量）、中等情况、典型情况和较大容量情况。对于每个 $m$，按规定训练网络，然后对隐藏表示进行线性可分性测试。每个测试用例的输出必须是一个布尔值，指示隐藏表示是否线性可分（可分为 True，不可分为 False）。此外，计算并输出测试值中能够实现可分性的最小隐藏层大小；如果没有一个值能实现可分性，则该值输出 $0$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表项按 $m = 4,8,16,32$ 的顺序排列，后跟实现可分性的最小 $m$（如果无，则为 $0$），例如 $[b_4,b_8,b_{16},b_{32},m_{\\text{min}}]$，其中每个 $b_m$ 是一个布尔值，$m_{\\text{min}}$ 是一个整数。输出不需要物理单位。角度必须以弧度为单位。所有计算必须纯粹是数值的且自包含，无需外部输入。",
            "solution": "我们从问题的核心定义和属性开始。一个数据集 $\\{(x_i,y_i)\\}_{i=1}^n$（其中 $x_i \\in \\mathbb{R}^d$ 且 $y_i \\in \\{-1,+1\\}$）是线性可分的，如果存在 $(w,b)$ 使得对于每一个 $i$ 都有 $y_i(w^\\top x_i + b) > 0$。一个双层修正线性单元 (ReLU) 网络计算 $z = \\sigma(W_1 x + b_1)$（其中 $\\sigma(t) = \\max\\{0,t\\}$ 逐元素作用）和 $o = w_2^\\top z + b_2$。函数 $\\sigma$ 是分段线性的，因此与仿射映射的复合会产生一个从 $\\mathbb{R}^2$ 到 $\\mathbb{R}^m$ 的分段线性映射。当其后跟随一个在隐藏空间中的线性分类器时，输入空间中的整体决策边界是多个多胞体 (polytope) 的并集，随着隐藏神经元数量 $m$ 的增加，它可以以越来越高的保真度近似弯曲的边界。\n\n数据集构建使用极坐标：对于类别 $+1$，定义角度 $\\theta \\in [0,4\\pi]$（以弧度为单位），半径 $r = a\\theta$，以及笛卡尔坐标 $x = (r\\cos\\theta, r\\sin\\theta)$；对于类别 $-1$，将角度平移 $\\pi$，即 $\\theta' = \\theta + \\pi$，半径 $r' = a\\theta'$，以及 $x' = (r'\\cos\\theta', r'\\sin\\theta')$。当 $a = 0.5$ 且每个类别有 $N_c = 128$ 个样本时，我们得到 $n = 256$ 个点，形成两条交织的螺旋线。由于其反复交替的缠绕结构，该数据集在 $\\mathbb{R}^2$ 中不是线性可分的。\n\n训练的目标是找到参数 $(W_1,b_1,w_2,b_2)$，使得隐藏表示 $z_i = \\sigma(W_1 x_i + b_1)$ 被排列成一个允许线性分离的配置。我们通过梯度下降最小化经验铰链损失\n$$\nL(W_1,b_1,w_2,b_2) = \\frac{1}{n}\\sum_{i=1}^n \\max\\{0, 1 - y_i (w_2^\\top z_i + b_2)\\}\n$$\n铰链损失关于输出 $o_i = w_2^\\top z_i + b_2$ 的次梯度为\n$$\n\\frac{\\partial L}{\\partial o_i} = \n\\begin{cases}\n-\\frac{y_i}{n},  \\text{if } 1 - y_i o_i > 0, \\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\n使用链式法则，梯度通过 ReLU 非线性函数传播：如果 $p_i = W_1 x_i + b_1$ 且 $z_i = \\sigma(p_i)$，那么 $\\frac{\\partial z_{i,j}}{\\partial p_{i,j}} = \\mathbb{I}\\{p_{i,j} > 0\\}$，并且\n$$\n\\frac{\\partial L}{\\partial W_1} = \\frac{1}{n}\\sum_{i=1}^n \\left( \\left( \\frac{\\partial L}{\\partial o_i} \\cdot w_2 \\right) \\odot \\mathbb{I}\\{p_i > 0\\} \\right) x_i^\\top, \\quad\n\\frac{\\partial L}{\\partial b_1} = \\frac{1}{n}\\sum_{i=1}^n \\left( \\left( \\frac{\\partial L}{\\partial o_i} \\cdot w_2 \\right) \\odot \\mathbb{I}\\{p_i > 0\\} \\right),\n$$\n其中 $\\odot$ 表示逐元素乘法。类似地，$\\frac{\\partial L}{\\partial w_2} = \\frac{1}{n}\\sum_{i=1}^n \\frac{\\partial L}{\\partial o_i} z_i$ 以及 $\\frac{\\partial L}{\\partial b_2} = \\frac{1}{n}\\sum_{i=1}^n \\frac{\\partial L}{\\partial o_i}$。应用带有衰减步长的梯度下降来更新所有参数。\n\n为了评估隐藏表示 $\\{(z_i,y_i)\\}_{i=1}^n$ 的线性可分性，我们使用一个线性规划，在 $\\ell_1$ 范数界 $\\|w\\|_1 + |b| \\le 1$ 下最大化间隔变量 $\\gamma$，并施加约束 $y_i(w^\\top z_i + b) \\ge \\gamma$（对所有 $i$）。这产生了以下优化问题：\n$$\n\\max_{w,b,\\gamma,u,v} \\ \\gamma \\quad \\text{subject to} \\quad\ny_i(w^\\top z_i + b) - \\gamma \\ge 0 \\ \\forall i, \\quad\nu_j \\ge w_j, \\ u_j \\ge -w_j \\ \\forall j, \\quad\nv \\ge b, \\ v \\ge -b, \\quad\n\\sum_j u_j + v \\le 1, \\quad\nu_j \\ge 0, \\ v \\ge 0, \\ \\gamma \\ge 0.\n$$\n在引入辅助变量 $u_j$ 和 $v$ 来处理 $\\ell_1$ 界中的绝对值后，这成为一个线性规划问题。以目标 $\\min -\\gamma$（等价于 $\\max \\gamma$）求解它，可以得到最优的 $\\gamma^\\star$。关键的论证依据是缩放属性：假设某个 $(\\tilde{w},\\tilde{b})$ 分隔了数据，意味着 $\\min_i y_i(\\tilde{w}^\\top z_i + \\tilde{b}) = \\delta > 0$。令 $s = \\|\\tilde{w}\\|_1 + |\\tilde{b}|$ 并重新缩放 $(w,b) = (\\tilde{w}/s, \\tilde{b}/s)$；那么 $\\|w\\|_1 + |b| = 1$ 且得到的间隔为 $\\min_i y_i(w^\\top z_i + b) = \\delta/s > 0$。因此，线性可分性等价于在 $\\ell_1$ 范数界下存在一个严格为正的间隔，并且当且仅当隐藏表示是线性可分时，线性规划的最优解 $\\gamma^\\star$ 严格为正。\n\n我们测试隐藏层大小 $m \\in \\{4,8,16,32\\}$ 来研究容量的影响。对于每个 $m$，我们如前所述训练网络，然后求解线性规划以判断可分性。结果以每个 $m$ 对应的布尔值形式报告，其后是测试值中实现可分性的最小 $m$（若无，则为 $0$）。单行输出格式是一个用方括号括起来的逗号分隔列表，顺序为 $[b_4,b_8,b_{16},b_{32},m_{\\text{min}}]$。\n\n程序中实现的算法步骤：\n$1.$ 使用 $a = 0.5$、$N_c = 128$（角度以弧度为单位）生成两条交织的螺旋线，并将特征缩放到可比较的范围以改善数值条件。\n$2.$ 对每个隐藏层大小 $m$，初始化参数并通过带有衰减步长的梯度下降最小化铰链损失，以获得 $(W_1,b_1,w_2,b_2)$。\n$3.$ 计算隐藏表示 $z_i$，如上所述构建线性规划问题，并求解 $\\gamma^\\star$。\n$4.$ 如果 $\\gamma^\\star > 10^{-6}$ 则返回 True，否则返回 False。\n$5.$ 计算测试值中产生 True 的最小 $m$；如果不存在，则返回 $0$。\n\n此过程整合了核心原理：线性可分性的定义、支撑 $\\ell_1$ 有界间隔测试的缩放论证，以及通过算法训练分段线性网络来近似螺旋线展开。随着 $m$ 的增加，隐藏映射可以将输入空间划分为更多的线性区域，从而有助于在隐藏空间中为交织的螺旋线找到一个线性分离器。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef generate_spirals(n_per_class=128, a=0.5, seed=42):\n    rng = np.random.default_rng(seed)\n    # Angles in radians\n    theta = np.linspace(0.0, 4.0*np.pi, n_per_class, endpoint=True)\n    r_pos = a * theta\n    x_pos = np.stack([r_pos * np.cos(theta), r_pos * np.sin(theta)], axis=1)\n\n    theta_neg = theta + np.pi\n    r_neg = a * theta_neg\n    x_neg = np.stack([r_neg * np.cos(theta_neg), r_neg * np.sin(theta_neg)], axis=1)\n\n    X = np.vstack([x_pos, x_neg])\n    y = np.concatenate([np.ones(n_per_class), -np.ones(n_per_class)])\n\n    # Scale features to improve conditioning\n    # Normalize by maximum radius encountered\n    r_max = max(r_pos.max(), r_neg.max())\n    X = X / (r_max + 1e-8)\n\n    return X, y\n\ndef train_two_layer_relu_hinge(X, y, m, iters=3000, lr0=0.05, seed=1337):\n    rng = np.random.default_rng(seed)\n    n, d = X.shape\n\n    # He-style initialization for ReLU\n    W1 = rng.normal(0.0, np.sqrt(2.0 / d), size=(m, d))\n    b1 = np.zeros(m)\n    w2 = rng.normal(0.0, np.sqrt(2.0 / m), size=(m,))\n    b2 = 0.0\n\n    for t in range(iters):\n        # Forward\n        P = X @ W1.T + b1  # pre-activation\n        Z = np.maximum(P, 0.0)  # ReLU\n        o = Z @ w2 + b2\n        margins = y * o\n\n        # Hinge loss gradient w.r.t outputs\n        mask = margins  1.0\n        # Gradient of loss wrt o: -(y)/n for those violating margin\n        grad_o = np.zeros_like(o)\n        grad_o[mask] = -y[mask] / n\n\n        # Gradients\n        grad_w2 = Z.T @ grad_o  # shape (m,)\n        grad_b2 = np.sum(grad_o)\n\n        # Backprop through ReLU\n        grad_Z = grad_o[:, None] * w2[None, :]  # (n, m)\n        relu_mask = (P > 0.0).astype(float)\n        grad_P = grad_Z * relu_mask  # (n, m)\n\n        grad_W1 = grad_P.T @ X  # (m, d)\n        grad_b1 = grad_P.sum(axis=0)  # (m,)\n\n        # Learning rate schedule\n        lr = lr0 * (0.995 ** t)\n\n        # Update\n        W1 -= lr * grad_W1\n        b1 -= lr * grad_b1\n        w2 -= lr * grad_w2\n        b2 -= lr * grad_b2\n\n    return W1, b1, w2, b2\n\ndef is_separable_lp(Z, y, tol=1e-6):\n    \"\"\"\n    Check linear separability of (Z, y) by solving:\n    maximize gamma\n    subject to y_i * (w^T z_i + b) - gamma >= 0\n              ||w||_1 + |b| = 1\n              gamma >= 0\n    Converted to a linear program with auxiliary variables for absolute values.\n    \"\"\"\n    n, m = Z.shape\n    # Variables: [w (m), b (1), u (m), v (1), gamma (1)]\n    num_vars = 2*m + 3\n\n    # Objective: minimize -gamma  => c has -1 at gamma\n    c = np.zeros(num_vars)\n    c[-1] = -1.0\n\n    # Build A_ub and b_ub\n    A_rows = []\n    b_rows = []\n\n    # Margin constraints: -y_i*(Z_i·w + b) + gamma = 0\n    for i in range(n):\n        row = np.zeros(num_vars)\n        # w coefficients\n        row[:m] = -y[i] * Z[i]\n        # b coefficient\n        row[m] = -y[i]\n        # gamma coefficient\n        row[-1] = 1.0\n        # u and v are zero in this constraint\n        A_rows.append(row)\n        b_rows.append(0.0)\n\n    # Absolute value constraints for w: u_j >= w_j and u_j >= -w_j\n    # Equivalent to w_j - u_j = 0 and -w_j - u_j = 0\n    for j in range(m):\n        row1 = np.zeros(num_vars)\n        row1[j] = 1.0        # w_j\n        row1[m + 1 + j] = -1.0  # -u_j\n        A_rows.append(row1)\n        b_rows.append(0.0)\n\n        row2 = np.zeros(num_vars)\n        row2[j] = -1.0       # -w_j\n        row2[m + 1 + j] = -1.0  # -u_j\n        A_rows.append(row2)\n        b_rows.append(0.0)\n\n    # Absolute value constraints for b: v >= b and v >= -b\n    # Equivalent to b - v = 0 and -b - v = 0\n    row_b1 = np.zeros(num_vars)\n    row_b1[m] = 1.0         # b\n    row_b1[m + 1 + m] = -1.0  # -v\n    A_rows.append(row_b1)\n    b_rows.append(0.0)\n\n    row_b2 = np.zeros(num_vars)\n    row_b2[m] = -1.0        # -b\n    row_b2[m + 1 + m] = -1.0  # -v\n    A_rows.append(row_b2)\n    b_rows.append(0.0)\n\n    # L1 bound: sum(u_j) + v = 1\n    row_l1 = np.zeros(num_vars)\n    # u coefficients\n    row_l1[m + 1 : m + 1 + m] = 1.0\n    # v coefficient\n    row_l1[m + 1 + m] = 1.0\n    A_rows.append(row_l1)\n    b_rows.append(1.0)\n\n    A_ub = np.vstack(A_rows)\n    b_ub = np.array(b_rows)\n\n    # Variable bounds\n    bounds = []\n    # w_j: free\n    for _ in range(m):\n        bounds.append((None, None))\n    # b: free\n    bounds.append((None, None))\n    # u_j: >= 0\n    for _ in range(m):\n        bounds.append((0.0, None))\n    # v: >= 0\n    bounds.append((0.0, None))\n    # gamma: >= 0\n    bounds.append((0.0, None))\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=\"highs\")\n    if not res.success:\n        return False\n    # Optimal gamma is last variable or -res.fun (should match)\n    gamma_opt = res.x[-1]\n    return gamma_opt > tol\n\ndef solve():\n    # Define the test cases from the problem statement: hidden sizes m\n    test_cases = [4, 8, 16, 32]\n\n    # Generate dataset once\n    X, y = generate_spirals(n_per_class=128, a=0.5, seed=2024)\n\n    results = []\n    min_m = 0\n    for m in test_cases:\n        # Train network for each hidden size\n        # Increase iterations slightly with m to aid convergence\n        iters = 2500 if m = 8 else 3500 if m = 16 else 4500\n        W1, b1, w2, b2 = train_two_layer_relu_hinge(X, y, m=m, iters=iters, lr0=0.05, seed=1337 + m)\n\n        # Hidden representation\n        Z = np.maximum(X @ W1.T + b1, 0.0)\n\n        # Check linear separability via LP\n        separable = is_separable_lp(Z, y, tol=1e-6)\n        results.append(separable)\n\n    # Smallest m that yields separability, or 0 if none\n    for m, sep in zip(test_cases, results):\n        if sep:\n            min_m = m\n            break\n\n    # Final print statement in the exact required format.\n    # Booleans printed as True/False, followed by integer min_m\n    all_results = results + [min_m]\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}