{
    "hands_on_practices": [
        {
            "introduction": "Understanding the capabilities of a model begins with understanding its limitations. The XOR function is a classic benchmark in machine learning, representing the simplest case of a non-linearly separable problem. This exercise  moves beyond visual intuition by asking you to formally prove this property using a tool from threshold logic, the Chow parameters, providing a rigorous analytical foundation for why more powerful models are necessary.",
            "id": "1916478",
            "problem": "In the context of pattern classification and threshold logic, a Boolean function $f(x_1, x_2, \\ldots, x_n)$ with binary inputs $x_i \\in \\{0, 1\\}$ is defined as being **linearly separable** if there exist a set of real-valued weights $(w_1, w_2, \\ldots, w_n)$ and a real-valued threshold $T$ such that the function evaluates to 1 if and only if the weighted sum of its inputs meets or exceeds the threshold. That is, $f(x_1, \\ldots, x_n) = 1 \\iff \\sum_{i=1}^{n} w_i x_i \\ge T$.\n\nA set of values known as **Chow parameters** can be computed for any Boolean function to analyze its properties. For a function $f(x_1, \\ldots, x_n)$, these parameters are defined as:\n1.  $C_0(f)$: The total number of input vectors (minterms) for which $f=1$.\n2.  $C_i(f)$, for $i \\in \\{1, \\ldots, n\\}$: The number of input vectors for which both $f=1$ and the input variable $x_i=1$.\n\nConsider the 4-variable odd parity function, defined as $f(x_1, x_2, x_3, x_4) = x_1 \\oplus x_2 \\oplus x_3 \\oplus x_4$, where $\\oplus$ denotes the exclusive-OR operation.\n\nYour task is to calculate the vector of Chow parameters for this function, in the order $(C_0(f), C_1(f), C_2(f), C_3(f), C_4(f))$, and based on a formal analysis, determine whether the function is linearly separable. Select the option that correctly provides the Chow parameter vector and the separability conclusion.\n\nA. $(C_0(f), C_1(f), C_2(f), C_3(f), C_4(f)) = (8, 4, 4, 4, 4)$; Linearly separable.\n\nB. $(C_0(f), C_1(f), C_2(f), C_3(f), C_4(f)) = (8, 4, 4, 4, 4)$; Not linearly separable.\n\nC. $(C_0(f), C_1(f), C_2(f), C_3(f), C_4(f)) = (4, 1, 1, 1, 1)$; Not linearly separable.\n\nD. $(C_0(f), C_1(f), C_2(f), C_3(f), C_4(f)) = (16, 8, 8, 8, 8)$; Linearly separable.\n\nE. $(C_0(f), C_1(f), C_2(f), C_3(f), C_4(f)) = (8, 3, 3, 3, 3)$; Not linearly separable.",
            "solution": "We are given the 4-variable odd parity function $f(x_{1},x_{2},x_{3},x_{4})=x_{1}\\oplus x_{2}\\oplus x_{3}\\oplus x_{4}$, which equals $1$ exactly when the Hamming weight (number of ones) of $(x_{1},x_{2},x_{3},x_{4})$ is odd.\n\nFirst, compute $C_{0}(f)$, the number of inputs with $f=1$. For $n=4$, the number of assignments with odd Hamming weight is\n$$\n\\sum_{k\\ \\text{odd}} \\binom{4}{k}=\\binom{4}{1}+\\binom{4}{3}=4+4=8.\n$$\nThus $C_{0}(f)=8$.\n\nNext, for each $i\\in\\{1,2,3,4\\}$, compute $C_{i}(f)$, the number of inputs with $f=1$ and $x_{i}=1$. Fix $i$ and condition on $x_{i}=1$. There are $2^{3}=8$ assignments of the remaining three variables. Writing $f=x_{i}\\oplus(x_{j}\\oplus x_{k}\\oplus x_{\\ell})$, when $x_{i}=1$ we have $f=1$ if and only if the parity of the remaining three variables is even. The number of even-parity assignments among three variables is\n$$\n\\sum_{j\\ \\text{even}} \\binom{3}{j}=\\binom{3}{0}+\\binom{3}{2}=1+3=4.\n$$\nTherefore $C_{i}(f)=4$ for each $i\\in\\{1,2,3,4\\}$. Hence the Chow parameter vector is $(C_{0},C_{1},C_{2},C_{3},C_{4})=(8,4,4,4,4)$.\n\nNow determine linear separability. Suppose, for contradiction, that there exist weights $w\\in\\mathbb{R}^{4}$ and a threshold $T\\in\\mathbb{R}$ such that $f(x)=1$ if and only if $w\\cdot x\\geq T$, and thus $f(x)=0$ if and only if $w\\cdot xT$. Let $S_{1}$ be the set of inputs with $f=1$ and $S_{0}$ the set with $f=0$. Define the class means\n$$\n\\mu_{1}=\\frac{1}{|S_{1}|}\\sum_{x\\in S_{1}} x,\\qquad \\mu_{0}=\\frac{1}{|S_{0}|}\\sum_{x\\in S_{0}} x.\n$$\nUsing Chow parameters, the $i$th coordinate of $\\mu_{1}$ is $\\frac{C_{i}}{C_{0}}=\\frac{4}{8}=\\frac{1}{2}$, so\n$$\n\\mu_{1}=\\left(\\frac{1}{2},\\frac{1}{2},\\frac{1}{2},\\frac{1}{2}\\right).\n$$\nBy symmetry (or the same computation for even parity), $\\mu_{0}$ has each coordinate equal to $\\frac{1}{2}$ as well, hence\n$$\n\\mu_{0}=\\mu_{1}.\n$$\nFrom the threshold inequalities, for all $x\\in S_{1}$, $w\\cdot x\\geq T$, so averaging gives $w\\cdot \\mu_{1}\\geq T$. For all $x\\in S_{0}$, $w\\cdot xT$, so averaging gives $w\\cdot \\mu_{0}T$. But $\\mu_{0}=\\mu_{1}$, a contradiction. Therefore, no such $w$ and $T$ exist, and the odd parity function on four variables is not linearly separable.\n\nCombining both parts, the correct choice is the option with Chow parameters $(8,4,4,4,4)$ and the conclusion “Not linearly separable.”",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Having established that simple linear models fail on tasks like XOR, we explore the fundamental solution: feature transformation. This thought experiment  demonstrates how a problem that is non-separable in its original space can become linearly separable by creating a new, engineered feature. This practice reveals the core principle behind the power of neural networks, which learn to automatically create such transformations to solve complex patterns.",
            "id": "3144385",
            "problem": "Consider a binary classification task in $\\mathbb{R}^{2}$ with inputs $x = (x_1, x_2)$ constrained to the set $\\{(-1,-1), (-1,1), (1,-1), (1,1)\\}$. Labels are determined by a small nonlinear interaction: for a fixed $\\delta$ with $0  \\delta  1$, define\n$$\ny(x) = \\mathrm{sign}\\!\\big(x_1 x_2 + \\delta\\big),\n$$\nwhere $\\mathrm{sign}(t) = 1$ if $t > 0$ and $\\mathrm{sign}(t) = -1$ if $t  0$. In other words, the points $x$ with $x_1 x_2 = 1$ have label $+1$ and the points with $x_1 x_2 = -1$ have label $-1$. A single-layer linear classifier has decision function $f(x) = w_1 x_1 + w_2 x_2 + b$ with parameters $(w_1, w_2, b) \\in \\mathbb{R}^{3}$. \n\nStarting from the definition of linear separability (that there exist $w \\in \\mathbb{R}^{2}$ and $b \\in \\mathbb{R}$ such that $y_i\\,(w^\\top x_i + b) > 0$ for all training points $x_i$ with labels $y_i$), first establish whether this dataset is linearly separable under $f(x)$ in the original input space. Then, consider augmenting the classifier with $m$ multiplicative interaction units, each computing the scalar feature $z_k(x) = x_1 x_2$ for $k = 1, \\dots, m$, and form the augmented linear decision function\n$$\nf_m(x) = \\tilde{w}_1 x_1 + \\tilde{w}_2 x_2 + \\sum_{k=1}^{m} \\tilde{v}_k\\, z_k(x) + \\tilde{b},\n$$\nwith parameters $(\\tilde{w}_1, \\tilde{w}_2, \\tilde{v}_1, \\dots, \\tilde{v}_m, \\tilde{b}) \\in \\mathbb{R}^{m+3}$. Determine the minimal integer $m^{\\star}$ such that there exist parameters making the dataset linearly separable in the augmented feature space, i.e., $y(x)\\, f_{m^{\\star}}(x) > 0$ for all four inputs $x$. Your answer must be the value of $m^{\\star}$, as a single integer. No rounding is needed.",
            "solution": "First, let's determine the labels for the four input points using the given label function $y(x) = \\mathrm{sign}(x_1 x_2 + \\delta)$, where $0  \\delta  1$.\n\n*   For $x^{(1)} = (1,1)$, $x_1 x_2 = 1$. The label is $y^{(1)} = \\mathrm{sign}(1+\\delta) = +1$.\n*   For $x^{(2)} = (-1,-1)$, $x_1 x_2 = 1$. The label is $y^{(2)} = \\mathrm{sign}(1+\\delta) = +1$.\n*   For $x^{(3)} = (1,-1)$, $x_1 x_2 = -1$. The label is $y^{(3)} = \\mathrm{sign}(-1+\\delta) = -1$.\n*   For $x^{(4)} = (-1,1)$, $x_1 x_2 = -1$. The label is $y^{(4)} = \\mathrm{sign}(-1+\\delta) = -1$.\n\nThe dataset is $\\{((1,1), +1), ((-1,-1), +1), ((1,-1), -1), ((-1,1), -1)\\}$. This is the classic XOR problem configuration.\n\n**Part 1: Linear separability in the original input space**\n\nA dataset is linearly separable if there exists a weight vector $w = (w_1, w_2)$ and a bias $b$ such that $y_i (w^\\top x_i + b) > 0$ for all four points. This gives us the following system of inequalities:\n1.  For $((1,1), +1)$: $1 \\cdot (w_1 + w_2 + b) > 0 \\implies w_1 + w_2 + b > 0$.\n2.  For $((-1,-1), +1)$: $1 \\cdot (-w_1 - w_2 + b) > 0 \\implies -w_1 - w_2 + b > 0$.\n3.  For $((1,-1), -1)$: $-1 \\cdot (w_1 - w_2 + b) > 0 \\implies w_1 - w_2 + b  0$.\n4.  For $((-1,1), -1)$: $-1 \\cdot (-w_1 + w_2 + b) > 0 \\implies -w_1 + w_2 + b  0$.\n\nAdding inequalities (1) and (2) yields:\n$(w_1 + w_2 + b) + (-w_1 - w_2 + b) > 0 \\implies 2b > 0 \\implies b > 0$.\n\nAdding inequalities (3) and (4) yields:\n$(w_1 - w_2 + b) + (-w_1 + w_2 + b)  0 \\implies 2b  0 \\implies b  0$.\n\nWe have derived the contradictory conditions $b > 0$ and $b  0$. Therefore, no such parameters $(w_1, w_2, b)$ can exist, and the dataset is not linearly separable in the original $\\mathbb{R}^2$ space.\n\n**Part 2: Linear separability in the augmented feature space**\n\nThe augmented classifier uses the decision function:\n$$\nf_m(x) = \\tilde{w}_1 x_1 + \\tilde{w}_2 x_2 + \\sum_{k=1}^{m} \\tilde{v}_k z_k(x) + \\tilde{b}\n$$\nSince each interaction unit computes the same feature $z_k(x) = x_1 x_2$, we can simplify the sum:\n$$\nf_m(x) = \\tilde{w}_1 x_1 + \\tilde{w}_2 x_2 + \\left(\\sum_{k=1}^{m} \\tilde{v}_k\\right) (x_1 x_2) + \\tilde{b}\n$$\nLet $W_v = \\sum_{k=1}^{m} \\tilde{v}_k$ be the effective weight for the interaction feature. The model's ability to separate the data depends on whether it can use the $x_1 x_2$ feature, which requires that $W_v$ can be non-zero.\n\n*   If $m=0$, the sum $\\sum_{k=1}^{0} \\tilde{v}_k$ is an empty sum, which is defined as $0$. Thus, $W_v=0$. The decision function reduces to the original linear classifier, which we've shown is insufficient. Therefore, $m=0$ is not enough.\n\n*   If $m=1$, we have one interaction unit. The effective weight is $W_v = \\tilde{v}_1$. We are free to choose the parameters, so we can set $\\tilde{v}_1$ to any non-zero value, for instance, $\\tilde{v}_1 = 1$. This allows the model to use the $x_1 x_2$ feature.\n\nLet's demonstrate that $m=1$ is sufficient. We need to find parameters $(\\tilde{w}_1, \\tilde{w}_2, \\tilde{v}_1, \\tilde{b})$ that satisfy $y(x) f_1(x) > 0$ for all points. Let's try a simple set of parameters: $\\tilde{w}_1=0, \\tilde{w}_2=0, \\tilde{b}=0$, and $\\tilde{v}_1=1$.\nThe decision function becomes $f_1(x) = x_1 x_2$.\nThe condition for separability is $y(x) \\cdot (x_1 x_2) > 0$.\n*   For the class with $y=+1$, we have $x_1 x_2 = 1$. The condition is $(+1) \\cdot (1) = 1 > 0$. This holds.\n*   For the class with $y=-1$, we have $x_1 x_2 = -1$. The condition is $(-1) \\cdot (-1) = 1 > 0$. This also holds.\n\nSince we found a set of parameters for $m=1$ that linearly separates the data, and $m=0$ is insufficient, the minimal integer value is $m^{\\star} = 1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Theory comes to life through implementation. After exploring the conceptual limits and solutions related to linear separability, this practice  challenges you to build a linear classifier from the ground up. By implementing logistic regression using gradient descent, you will directly engage with the mathematical machinery that finds an optimal separating hyperplane, translating concepts like loss functions and gradients into working code.",
            "id": "3278883",
            "problem": "You are to implement binary Logistic Regression from first principles, trained by the Steepest Descent Method (also called Gradient Descent) to obtain a separating hyperplane for a linearly separable dataset. Begin from the principle that independent binary labels are modeled as Bernoulli random variables whose success probabilities follow a logistic link through a linear predictor, and that the parameters are chosen by minimizing the regularized empirical risk constructed from the negative log-likelihood. Do not use pre-built machine learning libraries; derive the needed gradients from the foundational definitions and implement the optimization explicitly.\n\nThe program must:\n- Construct two fixed deterministic datasets in two dimensions.\n- Minimize a regularized empirical risk using steepest descent with a fixed learning rate and number of iterations.\n- Offer an option to standardize features to reduce ill-conditioning.\n- Report the training classification accuracy for each provided test case as a decimal fraction between $0$ and $1$.\n\nFoundational base you must use:\n- Independent Bernoulli modeling for binary labels $y_i \\in \\{0,1\\}$.\n- The logistic link $p_i = \\sigma(z_i)$ with $\\sigma$ the logistic sigmoid and $z_i$ the linear predictor.\n- Empirical risk minimization using the average negative log-likelihood plus an $\\ell_2$ penalty on the weights.\n\nDefinitions to employ:\n- For inputs $\\mathbf{x}_i \\in \\mathbb{R}^d$ and parameters $(\\mathbf{w}, b)$, define the linear predictor $z_i = \\mathbf{w}^\\top \\mathbf{x}_i + b$ and logistic sigmoid $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$.\n- Define the regularized empirical risk\n$$\n\\mathcal{L}(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^n \\Big( - y_i \\log(\\sigma(z_i)) - (1 - y_i) \\log(1 - \\sigma(z_i)) \\Big) + \\frac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert_2^2,\n$$\nwhere $n$ is the number of samples and $\\lambda \\ge 0$ is the regularization parameter.\n- The steepest descent method updates parameters by moving opposite to the gradient of $\\mathcal{L}$ with a step size $\\alpha > 0$ for a fixed number of iterations.\n\nDatasets:\n- Dataset $D_1$ (linearly separable, moderate scale). Positive class ($y=1$) points:\n$$\n\\{(2.5, 2.0), (3.0, 1.0), (2.0, 2.5), (3.5, 2.2), (2.2, 3.0), (2.8, 2.7), (3.2, 1.8), (2.4, 3.1)\\}.\n$$\nNegative class ($y=0$) points:\n$$\n\\{(-2.5, -1.5), (-3.0, -2.0), (-2.0, -2.2), (-3.2, -1.8), (-1.8, -2.5), (-2.7, -2.9), (-3.1, -1.7), (-2.3, -3.2)\\}.\n$$\nLet $X^{(1)}$ be the stack of all points in $D_1$ and $y^{(1)}$ the corresponding labels.\n- Dataset $D_2$ (same geometry with poor conditioning). Construct $X^{(2)}$ by scaling the second feature of $X^{(1)}$ by a factor of $1000$, that is, for each point $(x_1, x_2)$ in $X^{(1)}$, include $(x_1, 1000 x_2)$ in $X^{(2)}$. Use the same labels $y^{(1)}$ for $y^{(2)}$.\n\nStandardization option:\n- If standardization is enabled, transform $X$ to $\\tilde{X}$ via $\\tilde{X}_{ij} = \\dfrac{X_{ij} - \\mu_j}{\\sigma_j}$, where $\\mu_j$ and $\\sigma_j$ are the mean and standard deviation of feature $j$ across the training samples, with the convention that if $\\sigma_j = 0$ then use $\\sigma_j = 1$.\n\nTraining and prediction:\n- Initialize $\\mathbf{w}$ to the zero vector in $\\mathbb{R}^2$ and $b$ to $0$.\n- Perform a fixed number of steepest descent iterations with learning rate $\\alpha$ to minimize $\\mathcal{L}(\\mathbf{w}, b)$.\n- After training, predict class labels $\\hat{y}_i$ using the rule $\\hat{y}_i = 1$ if $\\sigma(z_i) \\ge 0.5$ and $\\hat{y}_i = 0$ otherwise.\n- Compute training accuracy as the decimal fraction $\\dfrac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{\\hat{y}_i = y_i\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\nTest suite:\nProvide the following four test cases, each specified by the tuple $(\\text{dataset}, \\alpha, \\text{iterations}, \\lambda, \\text{standardize})$, where $\\text{dataset} \\in \\{D_1, D_2\\}$, $\\alpha > 0$, $\\text{iterations} \\in \\mathbb{N}$, $\\lambda \\ge 0$, and $\\text{standardize} \\in \\{\\text{True}, \\text{False}\\}$.\n- Case $1$ (happy path): $(D_1, 0.1, 3000, 0.01, \\text{True})$.\n- Case $2$ (boundary condition: very small step size): $(D_1, 0.0001, 200, 0.01, \\text{True})$.\n- Case $3$ (edge case: poor conditioning without standardization): $(D_2, 0.0001, 3000, 0.01, \\text{False})$.\n- Case $4$ (edge case: strong regularization that can underfit): $(D_1, 0.1, 3000, 10.0, \\text{True})$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the training classification accuracies for the four cases as a comma-separated list enclosed in square brackets, with each accuracy rounded to six decimal places, for example, $$[a_1,a_2,a_3,a_4]$$ where each $a_k$ is a float in decimal form. No other text should be printed.",
            "solution": "The problem requires the implementation of a binary logistic regression model trained using the steepest descent method (also known as gradient descent). The entire procedure must be derived from first principles. Before presenting the computational solution, we must formalize the underlying mathematical model and derive the necessary equations for optimization.\n\n### 1. Probabilistic Model and Likelihood\n\nThe foundation of logistic regression is the probabilistic modeling of binary outcomes. We are given a dataset of $n$ samples, $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$, where $\\mathbf{x}_i \\in \\mathbb{R}^d$ is a feature vector and $y_i \\in \\{0, 1\\}$ is the corresponding binary label.\n\nWe model each label $y_i$ as an independent Bernoulli random variable, $Y_i \\sim \\text{Bernoulli}(p_i)$, where $p_i$ is the probability of the \"success\" outcome, i.e., $P(Y_i=1|\\mathbf{x}_i) = p_i$.\n\nThe core of logistic regression is the \"link\" between the feature vector $\\mathbf{x}_i$ and the probability $p_i$. This is achieved through a linear predictor, $z_i$, and the logistic sigmoid function, $\\sigma(z)$.\nThe linear predictor is a linear function of the input features, parameterized by a weight vector $\\mathbf{w} \\in \\mathbb{R}^d$ and a bias term $b \\in \\mathbb{R}$:\n$$ z_i = \\mathbf{w}^\\top \\mathbf{x}_i + b $$\nThe logistic sigmoid function maps this unbounded linear predictor, $z_i \\in \\mathbb{R}$, to a valid probability in the range $(0, 1)$:\n$$ p_i = \\sigma(z_i) = \\frac{1}{1 + e^{-z_i}} $$\nConsequently, the probability of each outcome for the $i$-th sample is given by:\n$$ P(Y_i=y_i|\\mathbf{x}_i; \\mathbf{w}, b) = p_i^{y_i} (1 - p_i)^{1 - y_i} $$\nAssuming the samples are independent and identically distributed (i.i.d.), the total likelihood of observing the entire set of labels $\\mathbf{y} = (y_1, ..., y_n)$ given the features $X = (\\mathbf{x}_1, ..., \\mathbf{x}_n)$ and parameters $(\\mathbf{w}, b)$ is the product of the individual probabilities:\n$$ L(\\mathbf{w}, b) = \\prod_{i=1}^n P(Y_i=y_i|\\mathbf{x}_i; \\mathbf{w}, b) = \\prod_{i=1}^n p_i^{y_i} (1 - p_i)^{1 - y_i} $$\n\n### 2. The Objective Function: Regularized Empirical Risk\n\nThe principle of maximum likelihood estimation (MLE) directs us to choose the parameters $(\\mathbf{w}, b)$ that maximize the likelihood $L(\\mathbf{w}, b)$. It is mathematically more convenient to work with the log-likelihood, $\\ell(\\mathbf{w}, b) = \\log L(\\mathbf{w}, b)$, as this transforms the product into a sum and does not change the location of the maximum.\n$$ \\ell(\\mathbf{w}, b) = \\sum_{i=1}^n \\log \\left( p_i^{y_i} (1 - p_i)^{1 - y_i} \\right) = \\sum_{i=1}^n \\left( y_i \\log p_i + (1 - y_i) \\log(1 - p_i) \\right) $$\nSubstituting $p_i = \\sigma(z_i)$:\n$$ \\ell(\\mathbf{w}, b) = \\sum_{i=1}^n \\left( y_i \\log(\\sigma(z_i)) + (1 - y_i) \\log(1 - \\sigma(z_i)) \\right) $$\nConventionally in machine learning, we frame the problem as minimizing a loss or risk function. Maximizing the log-likelihood is equivalent to minimizing the negative log-likelihood. The empirical risk is the average negative log-likelihood over the dataset:\n$$ \\mathcal{R}_{\\text{emp}}(\\mathbf{w}, b) = -\\frac{1}{n} \\ell(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^n \\left( -y_i \\log(\\sigma(z_i)) - (1 - y_i) \\log(1 - \\sigma(z_i)) \\right) $$\nThis function is also known as the binary cross-entropy loss.\n\nTo prevent overfitting and improve generalization, a regularization term is added to the empirical risk. The problem specifies an $\\ell_2$ penalty on the weights (also known as Tikhonov regularization or weight decay), which penalizes large weight values. The regularized empirical risk, which we must minimize, is:\n$$ \\mathcal{L}(\\mathbf{w}, b) = \\mathcal{R}_{\\text{emp}}(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert_2^2 $$\n$$ \\mathcal{L}(\\mathbf{w}, b) = \\frac{1}{n} \\sum_{i=1}^n \\Big( - y_i \\log(\\sigma(z_i)) - (1 - y_i) \\log(1 - \\sigma(z_i)) \\Big) + \\frac{\\lambda}{2} \\sum_{j=1}^d w_j^2 $$\nwhere $\\lambda \\ge 0$ is the regularization parameter. Note that the bias term $b$ is typically not regularized.\n\n### 3. Optimization by Steepest Descent\n\nThe steepest descent method is an iterative optimization algorithm that updates the parameters by taking a step in the direction opposite to the gradient of the objective function. The update rules are:\n$$ \\mathbf{w}^{(k+1)} \\leftarrow \\mathbf{w}^{(k)} - \\alpha \\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}^{(k)}, b^{(k)}) $$\n$$ b^{(k+1)} \\leftarrow b^{(k)} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}(\\mathbf{w}^{(k)}, b^{(k)}) $$\nwhere $\\alpha > 0$ is the learning rate. To implement this, we must compute the partial derivatives of $\\mathcal{L}(\\mathbf{w}, b)$ with respect to $\\mathbf{w}$ and $b$.\n\nFirst, we establish a key identity for the derivative of the sigmoid function:\n$$ \\frac{d\\sigma}{dz} = \\frac{d}{dz} (1 + e^{-z})^{-1} = -(1+e^{-z})^{-2}(-e^{-z}) = \\frac{e^{-z}}{(1+e^{-z})^2} = \\frac{1}{1+e^{-z}} \\cdot \\frac{e^{-z}}{1+e^{-z}} = \\sigma(z)(1 - \\sigma(z)) $$\n\nLet's compute the gradient of the unregularized part first. Let $L_i = -y_i \\log(\\sigma_i) - (1-y_i)\\log(1-\\sigma_i)$, where $\\sigma_i = \\sigma(z_i)$. Using the chain rule for a single weight $w_j$:\n$$ \\frac{\\partial L_i}{\\partial w_j} = \\frac{\\partial L_i}{\\partial \\sigma_i} \\frac{\\partial \\sigma_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial w_j} $$\nThe components are:\n- $\\frac{\\partial L_i}{\\partial \\sigma_i} = -\\frac{y_i}{\\sigma_i} - \\frac{1-y_i}{-(1-\\sigma_i)} = -\\frac{y_i}{\\sigma_i} + \\frac{1-y_i}{1-\\sigma_i} = \\frac{-y_i(1-\\sigma_i) + \\sigma_i(1-y_i)}{\\sigma_i(1-\\sigma_i)} = \\frac{\\sigma_i - y_i}{\\sigma_i(1-\\sigma_i)}$\n- $\\frac{\\partial \\sigma_i}{\\partial z_i} = \\sigma_i(1-\\sigma_i)$\n- $\\frac{\\partial z_i}{\\partial w_j} = \\frac{\\partial}{\\partial w_j}(\\sum_{k=1}^d w_k x_{ik} + b) = x_{ij}$ (where $x_{ij}$ is the $j$-th feature of sample $i$)\n\nCombining these gives:\n$$ \\frac{\\partial L_i}{\\partial w_j} = \\left( \\frac{\\sigma_i - y_i}{\\sigma_i(1-\\sigma_i)} \\right) (\\sigma_i(1-\\sigma_i)) (x_{ij}) = (\\sigma_i - y_i) x_{ij} $$\nThe gradient of the full objective function $\\mathcal{L}$ with respect to $w_j$ is:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial L_i}{\\partial w_j} + \\frac{\\partial}{\\partial w_j} \\left( \\frac{\\lambda}{2} \\sum_{k=1}^d w_k^2 \\right) = \\frac{1}{n} \\sum_{i=1}^n (\\sigma_i - y_i) x_{ij} + \\lambda w_j $$\nFor the bias term $b$, the chain rule is similar, but $\\frac{\\partial z_i}{\\partial b} = 1$:\n$$ \\frac{\\partial L_i}{\\partial b} = (\\sigma_i - y_i) \\cdot 1 = \\sigma_i - y_i $$\nThe regularization term does not depend on $b$, so its derivative is zero. Thus:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma_i - y_i) $$\nThe simplicity of the final gradient expressions $(\\sigma_i - y_i)$ is a notable and elegant property of using the logistic loss with a linear model.\n\n### 4. Vectorized Implementation\n\nFor efficient computation, we express the gradients in vector form. Let $X$ be the $n \\times d$ matrix of feature vectors (design matrix), $\\mathbf{y}$ be the $n \\times 1$ vector of labels, $\\mathbf{w}$ be the $d \\times 1$ vector of weights, and $\\boldsymbol{\\sigma}$ be the $n \\times 1$ vector of predicted probabilities $\\sigma_i$.\nThe vector of linear predictors is $\\mathbf{z} = X\\mathbf{w} + b\\mathbf{1}$, where $\\mathbf{1}$ is a vector of ones.\nThe gradient vector $\\nabla_{\\mathbf{w}} \\mathcal{L}$ and scalar derivative $\\frac{\\partial \\mathcal{L}}{\\partial b}$ are:\n$$ \\nabla_{\\mathbf{w}} \\mathcal{L} = \\frac{1}{n} X^\\top(\\boldsymbol{\\sigma} - \\mathbf{y}) + \\lambda \\mathbf{w} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma_i - y_i) = \\frac{1}{n} \\mathbf{1}^\\top(\\boldsymbol{\\sigma} - \\mathbf{y}) $$\nThe steepest descent update rules for one iteration are:\n1.  Compute linear predictors: $\\mathbf{z} = X\\mathbf{w} + b$\n2.  Compute probabilities: $\\boldsymbol{\\sigma} = \\sigma(\\mathbf{z})$\n3.  Compute gradients:\n    - $\\nabla_{\\mathbf{w}} \\mathcal{L} = \\frac{1}{n} X^\\top(\\boldsymbol{\\sigma} - \\mathbf{y}) + \\lambda \\mathbf{w}$\n    - $\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{n} \\text{mean}(\\boldsymbol{\\sigma} - \\mathbf{y})$\n4.  Update parameters:\n    - $\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\nabla_{\\mathbf{w}} \\mathcal{L}$\n    - $b \\leftarrow b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$\n\n### 5. Standardization and Regularization\n\n- **Standardization**: This preprocessing step transforms features to have zero mean and unit standard deviation. For features with vastly different scales (as in dataset $D_2$), gradient descent can perform poorly. The gradient's magnitude will be dominated by the features with larger scales, leading to slow convergence in some directions and potential oscillations in others. Standardization places all features on a common scale, making the level sets of the loss function more spherical and thus mitigating this ill-conditioning.\n\n- **Regularization**: The $\\ell_2$ penalty term $\\frac{\\lambda}{2} \\lVert \\mathbf{w} \\rVert_2^2$ serves to control model complexity. A large value of $\\lambda$ forces the weights $\\mathbf{w}$ to be small, resulting in a \"simpler\" model with a less complex decision boundary. This can prevent overfitting to the training data. However, if $\\lambda$ is too large (as explored in Case 4), it can lead to underfitting, where the model is too simple to capture the underlying structure of the data, resulting in poor performance even on the training set.\n\n### 6. Prediction and Evaluation\n\nAfter training for a fixed number of iterations, the final parameters $(\\mathbf{w}, b)$ define a separating hyperplane $\\mathbf{w}^\\top \\mathbf{x} + b = 0$. A new sample $\\mathbf{x}$ is classified based on which side of the hyperplane it falls. The prediction rule is:\n$$ \\hat{y}_i = \\begin{cases} 1  \\text{if } \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 0.5 \\\\ 0  \\text{if } \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b)  0.5 \\end{cases} $$\nSince $\\sigma(z)$ is a monotonically increasing function and $\\sigma(0) = 0.5$, this is equivalent to:\n$$ \\hat{y}_i = \\begin{cases} 1  \\text{if } \\mathbf{w}^\\top \\mathbf{x}_i + b \\ge 0 \\\\ 0  \\text{if } \\mathbf{w}^\\top \\mathbf{x}_i + b  0 \\end{cases} $$\nThe model's performance on the training data is evaluated using classification accuracy, defined as the fraction of correctly classified samples:\n$$ \\text{Accuracy} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{\\hat{y}_i = y_i\\} $$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The following program implements this entire procedure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests binary logistic regression from first principles\n    using the steepest descent method.\n    \"\"\"\n\n    # --- Dataset Construction ---\n    # Dataset D1 (linearly separable, moderate scale)\n    X1_pos = np.array([\n        [2.5, 2.0], [3.0, 1.0], [2.0, 2.5], [3.5, 2.2], \n        [2.2, 3.0], [2.8, 2.7], [3.2, 1.8], [2.4, 3.1]\n    ])\n    X1_neg = np.array([\n        [-2.5, -1.5], [-3.0, -2.0], [-2.0, -2.2], [-3.2, -1.8], \n        [-1.8, -2.5], [-2.7, -2.9], [-3.1, -1.7], [-2.3, -3.2]\n    ])\n    X1 = np.vstack((X1_pos, X1_neg))\n    y1 = np.array([1] * 8 + [0] * 8)\n\n    # Dataset D2 (ill-conditioned)\n    X2 = np.copy(X1)\n    X2[:, 1] *= 1000.0\n    y2 = np.copy(y1)\n    \n    datasets = {\n        'D1': (X1, y1),\n        'D2': (X2, y2)\n    }\n\n    # --- Test Suite ---\n    test_cases = [\n        # (dataset_name, alpha, iterations, lambda_reg, standardize)\n        ('D1', 0.1, 3000, 0.01, True),\n        ('D1', 0.0001, 200, 0.01, True),\n        ('D2', 0.0001, 3000, 0.01, False),\n        ('D1', 0.1, 3000, 10.0, True),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        dataset_name, alpha, iterations, lambda_reg, standardize = case\n        X_orig, y = datasets[dataset_name]\n        \n        X = np.copy(X_orig)\n        \n        # --- Standardization ---\n        if standardize:\n            mu = np.mean(X, axis=0)\n            sigma = np.std(X, axis=0)\n            # Per problem spec, if stddev is 0, use 1 to avoid division by zero.\n            sigma[sigma == 0] = 1.0\n            X = (X - mu) / sigma\n\n        n_samples, n_features = X.shape\n\n        # --- Initialization ---\n        w = np.zeros(n_features)\n        b = 0.0\n\n        # --- Training via Steepest Descent ---\n        for _ in range(iterations):\n            # Linear predictor: z = Xw + b\n            z = X @ w + b\n            \n            # Sigmoid activation: sigma(z)\n            # This is p_i, the predicted probability for class 1\n            predictions_prob = 1 / (1 + np.exp(-z))\n            \n            # Error term: (sigma(z) - y)\n            error = predictions_prob - y\n            \n            # Compute gradients\n            # Gradient of loss w.r.t. w: (1/n) * X^T * (sigma(z) - y) + lambda * w\n            grad_w = (1 / n_samples) * (X.T @ error) + lambda_reg * w\n            \n            # Gradient of loss w.r.t. b: (1/n) * sum(sigma(z) - y)\n            grad_b = (1 / n_samples) * np.sum(error)\n            \n            # Update parameters\n            w -= alpha * grad_w\n            b -= alpha * grad_b\n\n        # --- Prediction and Accuracy ---\n        # Note: We use the original (unstandardized) X for final evaluation if standardization\n        # was done, as standardization parameters (mu, sigma) are part of the learned model.\n        # But here, we are asked for TRAINING accuracy, so we predict on the same data we trained on (X).\n        final_z = X @ w + b\n        \n        # Predict labels: 1 if z >= 0, else 0\n        y_pred = (final_z >= 0).astype(int)\n        \n        # Calculate accuracy\n        accuracy = np.mean(y_pred == y)\n        results.append(f\"{accuracy:.6f}\")\n\n    # --- Final Output ---\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}