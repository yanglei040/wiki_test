{
    "hands_on_practices": [
        {
            "introduction": "To build robust neural networks, we must first understand how they react to simple transformations of their inputs and parameters. This practice problem focuses on a fundamental property: scale invariance. You will derive how scaling network inputs by a factor $\\alpha$ can be compensated by adjusting weights, first in a standard linear layer and then in a layer equipped with Batch Normalization . This exercise provides a crisp, mathematical demonstration of how Batch Normalization stabilizes the network by making hidden layer activations invariant to the scale of the previous layer's outputs.",
            "id": "3125166",
            "problem": "Consider a real-valued feedforward neural network with one hidden layer, input dimension $d$, hidden dimension $m$, and output dimension $1$. The hidden nonlinearity is the rectified linear unit (ReLU), which is defined elementwise by $\\sigma(u) = \\max\\{0,u\\}$ and satisfies the positive homogeneity property $\\sigma(cu) = c\\,\\sigma(u)$ for all $c \\geq 0$. The network without normalization computes\n$$\nf(x) = w_2^{\\top}\\,\\sigma\\!\\left(W_1 x\\right),\n$$\nwhere $x \\in \\mathbb{R}^{d}$, $W_1 \\in \\mathbb{R}^{m \\times d}$, and $w_2 \\in \\mathbb{R}^{m}$. There are no bias terms. Now suppose the inputs are rescaled by a strictly positive scalar $\\alpha > 0$, so that the new inputs are $x' = \\alpha x$. You are allowed to compensate only by rescaling the first-layer weight matrix $W_1$ by a scalar $\\beta > 0$, yielding the perturbed first-layer weights $W_1' = \\beta W_1$, while leaving $w_2$ unchanged. Determine the value of $\\beta$ that preserves the original output for every input $x$.\n\nNext, consider inserting Batch Normalization (BN) immediately after the first linear layer and before the ReLU, applied per hidden unit, with trainable scale and shift parameters $\\gamma_{\\mathrm{BN}} \\in \\mathbb{R}^{m}$ and $\\beta_{\\mathrm{BN}} \\in \\mathbb{R}^{m}$ held fixed. For a given mini-batch, BN transforms the pre-activations $a = W_1 x$ into\n$$\n\\hat{a} = \\gamma_{\\mathrm{BN}} \\odot \\frac{a - \\mu}{\\sigma} + \\beta_{\\mathrm{BN}},\n$$\nwhere $\\mu \\in \\mathbb{R}^{m}$ and $\\sigma \\in \\mathbb{R}^{m}$ are the batch mean and standard deviation computed elementwise over the mini-batch, $\\odot$ denotes elementwise multiplication, and the numerical stabilizer is set to zero for this analysis (that is, $\\epsilon = 0$). The network with BN computes\n$$\ng(x) = w_2^{\\top}\\,\\sigma\\!\\left(\\hat{a}\\right).\n$$\nAgain, suppose the inputs are rescaled by the strictly positive scalar $\\alpha > 0$, and you are allowed to compensate only by rescaling the first-layer weight matrix by the same scalar $\\beta > 0$, leaving $w_2$, $\\gamma_{\\mathrm{BN}}$, and $\\beta_{\\mathrm{BN}}$ unchanged. Determine the value of $\\beta$ that preserves the original output for every input $x$ and for the entire mini-batch.\n\nYour task is to derive, from first principles, how the parameter scaling $\\beta$ compensates for the input scaling $\\alpha$ in each case. Express your final answer as a single $1 \\times 2$ row vector containing the closed-form expressions for $\\beta$ in the cases without BN and with BN, in that order. No numerical rounding is required, and no physical units are involved.",
            "solution": "The problem will be validated against the specified criteria before a solution is attempted.\n\n### Step 1: Extract Givens\n- **Case 1: Network without Batch Normalization (BN)**\n    - Network model: $f(x) = w_2^{\\top}\\,\\sigma\\!\\left(W_1 x\\right)$.\n    - Parameters: Input $x \\in \\mathbb{R}^{d}$, first-layer weights $W_1 \\in \\mathbb{R}^{m \\times d}$, second-layer weights $w_2 \\in \\mathbb{R}^{m}$. Input dimension is $d$, hidden dimension is $m$, output dimension is $1$.\n    - Activation function: Rectified Linear Unit (ReLU), $\\sigma(u) = \\max\\{0,u\\}$, which is positive homogeneous: $\\sigma(cu) = c\\,\\sigma(u)$ for any scalar $c \\geq 0$.\n    - Perturbation: Input is rescaled to $x' = \\alpha x$ with $\\alpha > 0$. First-layer weights are rescaled to $W_1' = \\beta W_1$ with $\\beta > 0$. The weight vector $w_2$ is unchanged.\n    - Objective: Find the value of $\\beta$ that ensures $f(x)$ remains unchanged for all $x$.\n\n- **Case 2: Network with Batch Normalization (BN)**\n    - BN transformation: Inserted between the first linear layer and the ReLU activation. For a pre-activation $a$, the BN layer computes $\\hat{a} = \\gamma_{\\mathrm{BN}} \\odot \\frac{a - \\mu}{\\sigma} + \\beta_{\\mathrm{BN}}$.\n    - BN Parameters: $\\mu$ and $\\sigma$ are the elementwise mean and standard deviation of the pre-activations over a mini-batch. $\\gamma_{\\mathrm{BN}} \\in \\mathbb{R}^{m}$ and $\\beta_{\\mathrm{BN}} \\in \\mathbb{R}^{m}$ are fixed trainable parameters. The numerical stabilizer $\\epsilon$ is $0$.\n    - Network model: $g(x) = w_2^{\\top}\\,\\sigma\\!\\left(\\hat{a}\\right)$, where $\\hat{a}$ is the output of the BN layer applied to the pre-activations $W_1 x$.\n    - Perturbation: Input is rescaled to $x' = \\alpha x$ with $\\alpha > 0$. First-layer weights are rescaled to $W_1' = \\beta W_1$ with $\\beta > 0$. The parameters $w_2$, $\\gamma_{\\mathrm{BN}}$, and $\\beta_{\\mathrm{BN}}$ are unchanged.\n    - Objective: Find the value of $\\beta$ that ensures $g(x)$ remains unchanged for all $x$ in a mini-batch.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the theory of deep learning, using standard definitions for a feedforward network, ReLU activation, and Batch Normalization. The formulation is objective and mathematically precise. The problem is well-posed; although the second part exhibits a special kind of invariance, interpreting the term \"compensate\" within the context of the problem leads to a unique, meaningful solution. The problem is self-contained and free from contradictions or factual errors.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full, reasoned solution will be provided.\n\n### Derivation\n\nWe analyze the two cases separately.\n\n**Case 1: Network without Batch Normalization**\n\nThe original network computes the output $f(x)$ as:\n$$\nf(x) = w_2^{\\top}\\,\\sigma\\!\\left(W_1 x\\right)\n$$\nThe inputs are rescaled by a factor $\\alpha > 0$, so the new input is $x' = \\alpha x$. The first-layer weight matrix is scaled by a factor $\\beta > 0$, resulting in $W_1' = \\beta W_1$. The new network output, which we denote $f_{\\text{new}}(x')$, is:\n$$\nf_{\\text{new}}(x') = w_2^{\\top}\\,\\sigma\\!\\left(W_1' x'\\right)\n$$\nSubstituting the expressions for $x'$ and $W_1'$ gives:\n$$\nf_{\\text{new}}(x') = w_2^{\\top}\\,\\sigma\\!\\left((\\beta W_1) (\\alpha x)\\right)\n$$\nUsing the associativity of matrix multiplication and scalar properties:\n$$\nf_{\\text{new}}(x') = w_2^{\\top}\\,\\sigma\\!\\left((\\alpha\\beta) (W_1 x)\\right)\n$$\nThe pre-activations are scaled by the factor $\\alpha\\beta$. Since $\\alpha > 0$ and $\\beta > 0$, the product $\\alpha\\beta$ is also strictly positive. We can use the positive homogeneity property of the ReLU function, $\\sigma(cu) = c\\,\\sigma(u)$ for $c \\geq 0$:\n$$\nf_{\\text{new}}(x') = w_2^{\\top}\\,\\left( (\\alpha\\beta) \\sigma(W_1 x) \\right)\n$$\nSince $\\alpha\\beta$ is a scalar, we can factor it out of the dot product:\n$$\nf_{\\text{new}}(x') = (\\alpha\\beta) \\left( w_2^{\\top} \\sigma(W_1 x) \\right) = (\\alpha\\beta) f(x)\n$$\nTo preserve the original output for every input $x$, we must have $f_{\\text{new}}(x') = f(x)$. This implies:\n$$\n(\\alpha\\beta) f(x) = f(x)\n$$\nFor this equality to hold for any arbitrary, non-trivial network (i.e., where $f(x)$ is not identically zero), the scalar coefficient must be equal to $1$:\n$$\n\\alpha\\beta = 1\n$$\nSolving for $\\beta$, we find the required scaling factor for the first-layer weights:\n$$\n\\beta = \\frac{1}{\\alpha}\n$$\n\n**Case 2: Network with Batch Normalization**\n\nIn this case, a Batch Normalization layer is applied to the pre-activations $a = W_1 x$ before the ReLU non-linearity. For a mini-batch of inputs $\\{x_i\\}_{i=1}^N$, the pre-activations are $\\{a_i = W_1 x_i\\}_{i=1}^N$. The BN layer first computes the elementwise mean $\\mu$ and standard deviation $\\sigma$ of these pre-activations across the mini-batch:\n$$\n\\mu = \\frac{1}{N} \\sum_{i=1}^N a_i, \\quad \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (a_i - \\mu)^2}\n$$\nThen, it computes the normalized activations $\\hat{a}_i$:\n$$\n\\hat{a}_i = \\gamma_{\\mathrm{BN}} \\odot \\frac{a_i - \\mu}{\\sigma} + \\beta_{\\mathrm{BN}}\n$$\nThe final network output for an input $x_i$ is $g(x_i) = w_2^{\\top}\\,\\sigma(\\hat{a}_i)$.\n\nNow, we consider the perturbed network with rescaled inputs $x_i' = \\alpha x_i$ and rescaled weights $W_1' = \\beta W_1$. The new pre-activations, $a_i'$, are:\n$$\na_i' = W_1' x_i' = (\\beta W_1)(\\alpha x_i) = (\\alpha\\beta) (W_1 x_i) = (\\alpha\\beta) a_i\n$$\nEach pre-activation in the batch is scaled by the factor $\\alpha\\beta$. Let's compute the new batch statistics, $\\mu'$ and $\\sigma'$, for this set of new pre-activations $\\{a_i'\\}$:\nThe new mean $\\mu'$ is:\n$$\n\\mu' = \\frac{1}{N} \\sum_{i=1}^N a_i' = \\frac{1}{N} \\sum_{i=1}^N (\\alpha\\beta) a_i = (\\alpha\\beta) \\left(\\frac{1}{N} \\sum_{i=1}^N a_i\\right) = (\\alpha\\beta) \\mu\n$$\nThe new standard deviation $\\sigma'$ is:\n$$\n\\sigma' = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (a_i' - \\mu')^2} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N ((\\alpha\\beta)a_i - (\\alpha\\beta)\\mu)^2} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (\\alpha\\beta)^2(a_i - \\mu)^2}\n$$\n$$\n\\sigma' = \\sqrt{(\\alpha\\beta)^2 \\left(\\frac{1}{N} \\sum_{i=1}^N (a_i - \\mu)^2\\right)} = \\sqrt{(\\alpha\\beta)^2 \\sigma^2} = |\\alpha\\beta| \\sigma\n$$\nSince $\\alpha > 0$ and $\\beta > 0$, we have $|\\alpha\\beta| = \\alpha\\beta$. Thus, $\\sigma' = (\\alpha\\beta) \\sigma$.\n\nNow we compute the new normalized activations, $\\hat{a}_i'$, using these new statistics:\n$$\n\\hat{a}_i' = \\gamma_{\\mathrm{BN}} \\odot \\frac{a_i' - \\mu'}{\\sigma'} + \\beta_{\\mathrm{BN}} = \\gamma_{\\mathrm{BN}} \\odot \\frac{(\\alpha\\beta)a_i - (\\alpha\\beta)\\mu}{(\\alpha\\beta)\\sigma} + \\beta_{\\mathrm{BN}}\n$$\nAssuming the batch variance is non-zero (so $\\sigma_j \\neq 0$ for each component $j$), we can cancel the scalar factor $\\alpha\\beta$ from the numerator and denominator:\n$$\n\\hat{a}_i' = \\gamma_{\\mathrm{BN}} \\odot \\frac{(\\alpha\\beta)(a_i - \\mu)}{(\\alpha\\beta)\\sigma} + \\beta_{\\mathrm{BN}} = \\gamma_{\\mathrm{BN}} \\odot \\frac{a_i - \\mu}{\\sigma} + \\beta_{\\mathrm{BN}} = \\hat{a}_i\n$$\nThe output of the Batch Normalization layer is identical to the original output, $\\hat{a}_i' = \\hat{a}_i$. This holds for every sample $i$ in the mini-batch. Consequently, the input to the ReLU function is unchanged, and the final network output $g(x_i)$ is also unchanged.\n\nThis invariance holds for *any* choice of the scaling factor $\\beta > 0$. The problem asks for the value of $\\beta$ that *compensates* for the input scaling $\\alpha$. Our analysis shows that the Batch Normalization layer itself perfectly compensates for any scaling of its inputs. The scaling of the BN layer's inputs is by the factor $\\alpha\\beta$. Since the layer's output is invariant to this factor, no further compensation is needed from the parameter $\\beta$ to counteract the effect of $\\alpha$. A scaling action that provides no change is a multiplication by $1$. Therefore, while any $\\beta > 0$ would work, the specific value of $\\beta$ that represents the required (null) compensation is $1$.\n\n### Summary of Results\nThe required scaling factor $\\beta$ to preserve the network output is:\n1. Without BN: $\\beta = \\frac{1}{\\alpha}$\n2. With BN: $\\beta = 1$\n\nThe final answer is presented as a $1 \\times 2$ row vector.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{\\alpha} & 1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "One of the most surprising and important properties of neural networks is their non-identifiability: vastly different sets of parameters can compute the exact same function. This exercise guides you through deriving and verifying a key cause of this phenomenonâ€”reparameterization symmetries . By exploring how scaling transformations between layers can be perfectly balanced, you will discover why this symmetry holds for homogeneous activation functions like ReLU but fails for others like the sigmoid, giving you a deeper insight into the network's functional structure.",
            "id": "3125231",
            "problem": "Consider a standard feedforward neural network with $L$ layers. Let the input be $x \\in \\mathbb{R}^{n_0}$, and for each layer index $l \\in \\{1,\\dots,L\\}$, define the pre-activation and activation as $z^{(l)} = W^{(l)} x^{(l-1)} + b^{(l)}$ and $x^{(l)} = \\phi^{(l)}(z^{(l)})$, where $W^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l-1}}$ and $b^{(l)} \\in \\mathbb{R}^{n_l}$. The output of the network is $f(x) = x^{(L)}$. Assume a fully connected architecture and that the output activation $\\phi^{(L)}$ is the identity function.\n\nYour tasks are:\n- Derive, from first principles and definitions of feedforward computation and activation functions, conditions on the activation functions and parameter transformations under which two different parameter sets produce the same function $f$, focusing on per-neuron reparameterization symmetries in hidden layers. You must derive these conditions without relying on unproven shortcuts and must identify when such symmetries hold and when they fail for common activation functions.\n- Implement a program that tests parameter identifiability by constructing randomized networks and applying per-neuron scaling transformations to hidden layers, then checking functional equality numerically on a fixed input set.\n\nYou must write a complete, runnable program that:\n- Uses a fixed random seed $42$ for reproducibility.\n- Draws weights $W^{(l)}$ and biases $b^{(l)}$ independently and identically distributed from the uniform distribution on the interval $[-0.5, 0.5]$.\n- Evaluates functional equality by computing the maximum absolute difference between outputs $f(x)$ and $\\tilde{f}(x)$ of the original and transformed networks on a batch of $N = 200$ inputs sampled independently and identically from the uniform distribution on $[-1,1]^{n_0}$. Declare equality if the maximum absolute difference is less than a tolerance $\\varepsilon = 10^{-8}$.\n- Applies the following per-neuron scaling transformation at specified hidden layers: for a hidden layer index $l$ with neuron-wise positive scaling factors $s^{(l)}_j$ assembled into the diagonal matrix $D^{(l)} = \\mathrm{diag}(s^{(l)}_1,\\dots,s^{(l)}_{n_l})$, replace $W^{(l)}$ by $\\tilde{W}^{(l)} = D^{(l)} W^{(l)}$, replace $b^{(l)}$ by $\\tilde{b}^{(l)} = D^{(l)} b^{(l)}$, and replace $W^{(l+1)}$ by $\\tilde{W}^{(l+1)} = W^{(l+1)} (D^{(l)})^{-1}$. No transformation is applied to the output layer $L$ because there is no subsequent layer to compensate.\n- Tests six cases that cover typical and edge scenarios. In all cases, the output activation is the identity. The test suite is:\n    1. Hidden activation $\\mathrm{ReLU}$ (Rectified Linear Unit), architecture $[3,4,2]$, scaling at hidden layer $l=1$ with $s^{(1)} = [1.7, 0.5, 2.2, 0.9]$. Expect functional equality to hold.\n    2. Hidden activation $\\mathrm{ReLU}$, architecture $[3,4,2]$, scaling at hidden layer $l=1$ with $s^{(1)} = [-1.3, 1.2, 0.8, 1.1]$ (note the negative value). Expect functional equality to fail.\n    3. Hidden activation $\\sigma$ (sigmoid), architecture $[2,3,1]$, scaling at hidden layer $l=1$ with $s^{(1)} = [1.4, 0.8, 1.1]$. Expect functional equality to fail.\n    4. Hidden activation identity (linear), architecture $[3,3,2]$, scaling at hidden layer $l=1$ with $s^{(1)} = [-0.8, -1.2, 0.5]$. Expect functional equality to hold.\n    5. Hidden activation $\\mathrm{ReLU}$, architecture $[4,3,2]$, scaling at hidden layer $l=1$ with very small positive scales $s^{(1)} = [10^{-6}, 3 \\cdot 10^{-6}, 2 \\cdot 10^{-6}]$. Expect functional equality to hold.\n    6. Two hidden layers with activation $\\mathrm{ReLU}$ for both, architecture $[3,4,3,2]$, scaling at hidden layer $l=1$ with $s^{(1)} = [1.3, 0.7, 2.0, 0.9]$ and at hidden layer $l=2$ with $s^{(2)} = [1.2, 0.6, 1.5]$. Expect functional equality to hold.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each entry should be a boolean indicating whether functional equality holds for the corresponding test case, in the order listed above (for example, $[{\\tt True},{\\tt False},\\dots]$). No other text should be printed.",
            "solution": "The problem asks for a derivation of the conditions under which a feedforward neural network exhibits reparameterization symmetries, specifically with respect to per-neuron scaling in hidden layers. It also requires a numerical verification of these conditions for several test cases.\n\n### Part 1: Derivation of Reparameterization Symmetry Conditions\n\nLet us consider a standard feedforward neural network with $L$ layers. The input is $x^{(0)} = x \\in \\mathbb{R}^{n_0}$. For each layer $l \\in \\{1, \\dots, L\\}$, the computation is defined as:\n$$ z^{(l)} = W^{(l)} x^{(l-1)} + b^{(l)} $$\n$$ x^{(l)} = \\phi^{(l)}(z^{(l)}) $$\nwhere $W^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l-1}}$ is the weight matrix, $b^{(l)} \\in \\mathbb{R}^{n_l}$ is the bias vector, $z^{(l)}$ is the pre-activation vector, $x^{(l)}$ is the activation vector (output of layer $l$), and $\\phi^{(l)}$ is the element-wise activation function for layer $l$. The final output of the network is $f(x) = x^{(L)}$. We are given that the output activation $\\phi^{(L)}$ is the identity function, so $f(x) = z^{(L)}$.\n\nWe are interested in a transformation applied to the parameters of a single hidden layer $l \\in \\{1,\\dots,L-1\\}$. Let the original network parameters be denoted by $\\theta = \\{W^{(k)}, b^{(k)}\\}_{k=1}^L$. The transformation involves a set of non-zero scaling factors $s^{(l)}_j$ for each neuron $j \\in \\{1, \\dots, n_l\\}$ in layer $l$. These are assembled into a diagonal matrix $D^{(l)} = \\mathrm{diag}(s^{(l)}_1, \\dots, s^{(l)}_{n_l})$.\n\nThe transformed parameter set $\\tilde{\\theta}$ is defined by modifying the parameters associated with layer $l$ and the weights of the subsequent layer $l+1$:\n1.  $\\tilde{W}^{(l)} = D^{(l)} W^{(l)}$\n2.  $\\tilde{b}^{(l)} = D^{(l)} b^{(l)}$\n3.  $\\tilde{W}^{(l+1)} = W^{(l+1)} (D^{(l)})^{-1}$\n4.  All other parameters remain unchanged: $\\tilde{W}^{(k)} = W^{(k)}$ and $\\tilde{b}^{(k)} = b^{(k)}$ for $k \\notin \\{l, l+1\\}$ and $k\\neq l$ respectively. Note from rule 3, that $\\tilde{W}^{(l+1)}$ is modified. If we are scaling layer $l$, then effectively, $\\tilde{W}^{(k)} = W^{(k)}$ for $k < l$ and $k > l+1$, and $\\tilde{b}^{(k)} = b^{(k)}$ for $k \\neq l$.\n\nLet us denote the function computed by the transformed network as $\\tilde{f}(x)$. We want to find the conditions on the activation function $\\phi^{(l)}$ under which $\\tilde{f}(x) = f(x)$ for all inputs $x$.\n\nThe computation in the transformed network, which we denote with a tilde, proceeds identically to the original network for layers $k < l$, since the parameters are unchanged. Thus, the input to layer $l$ is the same: $\\tilde{x}^{(l-1)} = x^{(l-1)}$.\n\nAt layer $l$, the pre-activation in the transformed network is:\n$$ \\tilde{z}^{(l)} = \\tilde{W}^{(l)} x^{(l-1)} + \\tilde{b}^{(l)} = (D^{(l)} W^{(l)}) x^{(l-1)} + (D^{(l)} b^{(l)}) = D^{(l)} (W^{(l)} x^{(l-1)} + b^{(l)}) = D^{(l)} z^{(l)} $$\nThe activation at layer $l$ is:\n$$ \\tilde{x}^{(l)} = \\phi^{(l)}(\\tilde{z}^{(l)}) = \\phi^{(l)}(D^{(l)} z^{(l)}) $$\n\nNow, consider layer $l+1$. Its pre-activation is:\n$$ \\tilde{z}^{(l+1)} = \\tilde{W}^{(l+1)} \\tilde{x}^{(l)} + \\tilde{b}^{(l+1)} $$\nSince only the weights $\\tilde{W}^{(l+1)}$ are transformed and the bias $\\tilde{b}^{(l+1)} = b^{(l+1)}$ is not, we have:\n$$ \\tilde{z}^{(l+1)} = (W^{(l+1)} (D^{(l)})^{-1}) \\phi^{(l)}(D^{(l)} z^{(l)}) + b^{(l+1)} $$\nFor the original network, the pre-activation at layer $l+1$ is:\n$$ z^{(l+1)} = W^{(l+1)} x^{(l)} + b^{(l+1)} = W^{(l+1)} \\phi^{(l)}(z^{(l)}) + b^{(l+1)} $$\n\nFor the two networks to compute the same function, their outputs must be identical. Since all parameters from layer $l+2$ onwards are identical in both networks, functional equality $\\tilde{f}(x) = f(x)$ is guaranteed if the inputs to layer $l+2$ are identical. This means we require $\\tilde{x}^{(l+1)} = x^{(l+1)}$. As the activation function $\\phi^{(l+1)}$ is the same for both networks, this is equivalent to requiring their pre-activations to be identical: $\\tilde{z}^{(l+1)} = z^{(l+1)}$.\n\nEquating the expressions for $\\tilde{z}^{(l+1)}$ and $z^{(l+1)}$:\n$$ W^{(l+1)} (D^{(l)})^{-1} \\phi^{(l)}(D^{(l)} z^{(l)}) + b^{(l+1)} = W^{(l+1)} \\phi^{(l)}(z^{(l)}) + b^{(l+1)} $$\nSubtracting $b^{(l+1)}$ from both sides gives:\n$$ W^{(l+1)} \\left( (D^{(l)})^{-1} \\phi^{(l)}(D^{(l)} z^{(l)}) - \\phi^{(l)}(z^{(l)}) \\right) = 0 $$\nThis equation must hold for all possible inputs $x$, which generate all reachable pre-activations $z^{(l)}$, and for any valid choice of the weight matrix $W^{(l+1)}$. For the equality to hold for an arbitrary $W^{(l+1)}$, the term in the parenthesis must be the zero vector.\n$$ (D^{(l)})^{-1} \\phi^{(l)}(D^{(l)} z^{(l)}) - \\phi^{(l)}(z^{(l)}) = 0 $$\n$$ \\implies (D^{(l)})^{-1} \\phi^{(l)}(D^{(l)} z^{(l)}) = \\phi^{(l)}(z^{(l)}) $$\nMultiplying by $D^{(l)}$ (which is invertible since all $s^{(l)}_j \\neq 0$), we arrive at the core condition:\n$$ \\phi^{(l)}(D^{(l)} z^{(l)}) = D^{(l)} \\phi^{(l)}(z^{(l)}) $$\nSince $\\phi^{(l)}$ is an element-wise function and $D^{(l)}$ is diagonal, we can analyze this condition on a per-neuron basis. For the $j$-th neuron in layer $l$, with scaling factor $s_j$ and pre-activation $z_j$, the condition is:\n$$ \\phi_j(s_j z_j) = s_j \\phi_j(z_j) $$\nThis means the activation function must be a homogeneous function of degree $1$ with respect to the scaling factor $s_j$.\n\nLet's analyze this condition for the common activation functions mentioned in the problem:\n\n1.  **Identity (Linear) Activation:** $\\phi(z) = z$.\n    The condition is $s_j z_j = s_j z_j$. This is always true for any non-zero scalar $s_j$, positive or negative. Thus, the reparameterization symmetry holds for linear layers with any non-zero scaling.\n\n2.  **Rectified Linear Unit (ReLU):** $\\phi(z) = \\max(0, z)$.\n    The condition is $\\max(0, s_j z_j) = s_j \\max(0, z_j)$.\n    -   If $s_j > 0$:\n        -   For $z_j \\ge 0$, the equation becomes $s_j z_j = s_j z_j$, which is true.\n        -   For $z_j < 0$, the equation becomes $0 = s_j \\cdot 0$, which is true.\n        The symmetry holds for any positive scaling factor $s_j > 0$.\n    -   If $s_j < 0$:\n        -   Let's test with a counterexample. Let $z_j = 1$ and $s_j = -2$.\n        -   LHS: $\\max(0, (-2) \\cdot 1) = \\max(0, -2) = 0$.\n        -   RHS: $(-2) \\cdot \\max(0, 1) = -2 \\cdot 1 = -2$.\n        -   Since $0 \\neq -2$, the condition fails. The symmetry does not hold for negative scaling factors.\n\n3.  **Sigmoid Activation:** $\\phi(z) = \\frac{1}{1 + e^{-z}}$.\n    The condition is $\\frac{1}{1 + e^{-s_j z_j}} = s_j \\frac{1}{1 + e^{-z_j}}$.\n    This equality does not hold in general for $s_j \\neq 1$. For instance, if we take $z_j=0$, the LHS is $\\phi(0) = 0.5$, while the RHS is $s_j \\phi(0) = 0.5 s_j$. For these to be equal, $s_j$ must be $1$. If $s_j \\neq 1$, the symmetry is broken.\n\n**Multi-layer Scaling:**\nFor the case where multiple layers are scaled simultaneously (as in test case $6$, with layers $l=1$ and $l=2$), the logic extends. A scaling at layer $l$ adjusts its output parameters $W^{(l)}, b^{(l)}$ and the input weights of the next layer $W^{(l+1)}$. If we also scale layer $l+1$, it adjusts its parameters $W^{(l+1)}, b^{(l+1)}$ and the input weights of layer $l+2$, which is $W^{(l+2)}$.\nThe weight matrix $W^{(l+1)}$ is affected by both transformations: its rows are scaled by $D^{(l+1)}$ and its columns are scaled by $(D^{(l)})^{-1}$. The resulting transformed weight is $\\tilde{W}^{(l+1)} = D^{(l+1)} W^{(l+1)} (D^{(l)})^{-1}$.\nFollowing the forward pass as before, we find that to preserve the network function, the homogeneity condition $\\phi_j^{(k)}(s_j^{(k)} z_j^{(k)}) = s_j^{(k)} \\phi_j^{(k)}(z_j^{(k)})$ must hold for each scaled layer $k$ and each neuron $j$ within it. If this condition is met for all scaled layers (e.g., for ReLU with positive scales), the overall network function remains invariant.\n\n### Part 2: Numerical Implementation and Verification\n\nThe Python program below implements the specified tests. It constructs neural networks with random parameters, applies the defined scaling transformations, and numerically checks for functional equality based on the derived conditions. The results of the tests directly validate the theoretical analysis above.\n- Test $1$: ReLU with positive scales. Theory: hold. Result: `True`.\n- Test $2$: ReLU with a negative scale. Theory: fail. Result: `False`.\n- Test $3$: Sigmoid with non-unit scales. Theory: fail. Result: `False`.\n- Test $4$: Linear with negative scales. Theory: hold. Result: `True`.\n- Test $5$: ReLU with small positive scales. Theory: hold. Result: `True`.\n- Test $6$: Two adjacent ReLU layers with positive scales. Theory: hold. Result: `True`.\n\nThe numerical results are expected to match the theoretical predictions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Validates neural network reparameterization symmetries by implementing and testing\n    per-neuron scaling transformations on feedforward networks.\n    \"\"\"\n    \n    # Use a fixed random seed for reproducibility, as specified.\n    rng = np.random.default_rng(42)\n\n    # Define activation functions.\n    def relu(z):\n        return np.maximum(0, z)\n\n    def sigmoid(z):\n        # Added a clamp for numerical stability with large z\n        z = np.clip(z, -500, 500)\n        return 1 / (1 + np.exp(-z))\n\n    def identity(z):\n        return z\n\n    activations_map = {'relu': relu, 'sigmoid': sigmoid, 'identity': identity}\n\n    def create_network(architecture, rng_gen):\n        \"\"\"Initializes network parameters from U[-0.5, 0.5].\"\"\"\n        weights = []\n        biases = []\n        for i in range(len(architecture) - 1):\n            n_out, n_in = architecture[i+1], architecture[i]\n            # Weights W^(l) from U[-0.5, 0.5]\n            w = rng_gen.uniform(-0.5, 0.5, size=(n_out, n_in))\n            # Biases b^(l) from U[-0.5, 0.5]\n            b = rng_gen.uniform(-0.5, 0.5, size=(n_out, 1))\n            weights.append(w)\n            biases.append(b)\n        return weights, biases\n\n    def forward_pass(weights, biases, x_batch, activation_funcs):\n        \"\"\"Computes the network output for a batch of inputs.\"\"\"\n        x_l = x_batch\n        num_layers = len(weights)\n        for l_idx in range(num_layers):\n            w = weights[l_idx]\n            b = biases[l_idx]\n            phi = activation_funcs[l_idx]\n            \n            z_l = w @ x_l + b\n            x_l = phi(z_l)\n        return x_l\n\n    def run_test_case(case_params, rng_gen):\n        \"\"\"Executes a single test case for functional equality.\"\"\"\n        arch = case_params['arch']\n        hidden_act_name = case_params['hidden_act']\n        output_act_name = 'identity'\n        scales_to_apply = case_params['scales']\n        \n        # Create the original network\n        original_weights, original_biases = create_network(arch, rng_gen)\n        \n        # Prepare activation functions for all layers\n        num_layers = len(arch) - 1\n        activation_funcs = []\n        hidden_act_func = activations_map[hidden_act_name]\n        output_act_func = activations_map[output_act_name]\n        for l_idx in range(num_layers):\n          # All hidden layers have the same activation, output is identity\n          is_output_layer = (l_idx == num_layers - 1)\n          activation_funcs.append(output_act_func if is_output_layer else hidden_act_func)\n\n        # Create the transformed network by applying scaling\n        # Start with a copy of the original network\n        transformed_weights = [w.copy() for w in original_weights]\n        transformed_biases = [b.copy() for b in original_biases]\n\n        for l_1based, s_vec in scales_to_apply.items():\n            l_idx = l_1based - 1 # Convert to 0-based index\n            s_vec_arr = np.array(s_vec)\n            \n            # Check for non-invertible scales\n            if np.any(s_vec_arr == 0):\n                raise ValueError(f\"Scaling factors must be non-zero. Found 0 in {s_vec}.\")\n\n            D = np.diag(s_vec_arr)\n            D_inv = np.diag(1.0 / s_vec_arr)\n\n            # Apply transformation: tilde_W(l) = D(l) * W(l)\n            transformed_weights[l_idx] = D @ transformed_weights[l_idx]\n            # Apply transformation: tilde_b(l) = D(l) * b(l)\n            transformed_biases[l_idx] = D @ transformed_biases[l_idx]\n            \n            # The compensation is applied to the next layer W(l+1)\n            # tilde_W(l+1) = W(l+1) * (D(l))^-1\n            # Note: This modifies the weights that might be a target for a subsequent scaling (e.g. Case 6)\n            if l_idx + 1  len(transformed_weights):\n                transformed_weights[l_idx + 1] = transformed_weights[l_idx + 1] @ D_inv\n\n        # Generate test inputs\n        N = 200 # Number of test inputs\n        n_0 = arch[0] # Input dimension\n        x_batch = rng_gen.uniform(-1, 1, size=(n_0, N))\n\n        # Perform forward passes\n        y_original = forward_pass(original_weights, original_biases, x_batch, activation_funcs)\n        y_transformed = forward_pass(transformed_weights, transformed_biases, x_batch, activation_funcs)\n        \n        # Check for functional equality\n        max_abs_diff = np.max(np.abs(y_original - y_transformed))\n        tolerance = 1e-8\n        \n        return max_abs_diff  tolerance\n\n    # Define the 6 test cases as specified in the problem statement.\n    test_cases = [\n        # 1. ReLU, positive scales -> should hold\n        {'arch': [3, 4, 2], 'hidden_act': 'relu', 'scales': {1: [1.7, 0.5, 2.2, 0.9]}},\n        # 2. ReLU, negative scale -> should fail\n        {'arch': [3, 4, 2], 'hidden_act': 'relu', 'scales': {1: [-1.3, 1.2, 0.8, 1.1]}},\n        # 3. Sigmoid, positive scales -> should fail\n        {'arch': [2, 3, 1], 'hidden_act': 'sigmoid', 'scales': {1: [1.4, 0.8, 1.1]}},\n        # 4. Identity, negative scales -> should hold\n        {'arch': [3, 3, 2], 'hidden_act': 'identity', 'scales': {1: [-0.8, -1.2, 0.5]}},\n        # 5. ReLU, small positive scales -> should hold\n        {'arch': [4, 3, 2], 'hidden_act': 'relu', 'scales': {1: [1e-6, 3e-6, 2e-6]}},\n        # 6. Two ReLU layers, positive scales -> should hold\n        {'arch': [3, 4, 3, 2], 'hidden_act': 'relu', 'scales': {1: [1.3, 0.7, 2.0, 0.9], 2: [1.2, 0.6, 1.5]}},\n    ]\n    \n    results = []\n    for case in test_cases:\n        result = run_test_case(case, rng)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Feedforward neural networks exist within a rich ecosystem of machine learning models, sharing deep theoretical connections with them. This hands-on coding problem explores one such connection by framing a shallow neural network as a \"random features\" model, linking it directly to kernel methods . By implementing both the standard weight-space (primal) regression and the equivalent kernel-space (dual) regression, you will numerically verify the celebrated primal-dual equivalence and gain a profound appreciation for the theoretical unity between these powerful modeling paradigms.",
            "id": "3125270",
            "problem": "You are asked to implement and analyze a single hidden-layer feedforward neural network with fixed random first-layer features, and to compare its trained output layer to Kernel Ridge Regression (KRR) constructed on the same features. The comparison should be grounded in first principles of linear models, feedforward neural networks, and kernel methods.\n\nFundamental base:\n- A feedforward neural network defines a mapping from an input vector $x \\in \\mathbb{R}^d$ to a scalar output via compositions of affine transforms and nonlinearities. In a single hidden-layer network with $h$ hidden units, fixed first-layer weights and bias $(W_1, b)$, and elementwise activation $\\sigma(\\cdot)$, the network output is obtained by a linear combination of hidden activations.\n- Ridge regression (also called $L_2$-regularized least squares) penalizes the squared $L_2$ norm of the parameter vector to stabilize solutions and control overfitting.\n- Kernel methods operate by expressing predictions in terms of inner products of features, allowing solutions to be written in terms of a kernel matrix.\n\nYour program must:\n1. Generate synthetic regression data. For a given input dimension $d$, training size $n$, and test size $m$, draw inputs $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$ and $X_{\\text{test}} \\in \\mathbb{R}^{m \\times d}$ from a standard normal distribution with a fixed random seed. Define a target function for $x \\in \\mathbb{R}^d$ by\n   $$ f(x) = \\sum_{j=1}^{d} \\sin(x_j) + 0.1 \\, \\|x\\|_2^2, $$\n   and observe outputs with additive Gaussian noise of standard deviation $\\sigma_{\\text{noise}}$:\n   $$ y_{\\text{train}} = f(X_{\\text{train}}) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2 I_n). $$\n2. Construct random features using a fixed random first layer. For hidden dimension $h$, sample $W_1 \\in \\mathbb{R}^{d \\times h}$ and $b \\in \\mathbb{R}^{1 \\times h}$ from standard normal distributions (with the same seed used for data construction to ensure determinism within each test case). Define the feature map\n   $$ \\phi(x) = \\sigma(x W_1 + b), $$\n   applied elementwise, where $\\sigma$ is one of the following activations:\n   - Rectified Linear Unit (ReLU): $\\sigma(z) = \\max(0, z)$ applied elementwise.\n   - Hyperbolic tangent (tanh): $\\sigma(z) = \\tanh(z)$ applied elementwise.\n   - Identity (linear): $\\sigma(z) = z$ applied elementwise.\n   For matrices, define $Z_{\\text{train}} \\in \\mathbb{R}^{n \\times h}$ and $Z_{\\text{test}} \\in \\mathbb{R}^{m \\times h}$ by applying $\\phi(\\cdot)$ rowwise to $X_{\\text{train}}$ and $X_{\\text{test}}$ respectively.\n3. Train only the output layer weights $W_2 \\in \\mathbb{R}^{h \\times 1}$ of the network by minimizing an $L_2$-regularized least squares objective with regularization strength $\\lambda  0$:\n   $$ \\min_{W_2} \\; \\frac{1}{n} \\, \\|Z_{\\text{train}} W_2 - y_{\\text{train}}\\|_2^2 + \\lambda \\, \\|W_2\\|_2^2. $$\n   Use a numerically stable linear solve to obtain the minimizer implied by the normal equations, then compute the resulting test predictions\n   $$ \\hat{y}_{\\text{test}}^{\\text{primal}} = Z_{\\text{test}} W_2. $$\n4. Define a kernel on inputs induced by the same fixed random features:\n   $$ k(x,x') = \\langle \\phi(x), \\phi(x') \\rangle, $$\n   which yields the training Gram matrix $K \\in \\mathbb{R}^{n \\times n}$ with entries $K_{ij} = k(x_i, x_j)$, i.e., $K = Z_{\\text{train}} Z_{\\text{train}}^\\top$, and the test-train kernel block $K_{\\text{test}} = Z_{\\text{test}} Z_{\\text{train}}^\\top \\in \\mathbb{R}^{m \\times n}$. Perform Kernel Ridge Regression with the same regularization $\\lambda$, and compute test predictions\n   $$ \\hat{y}_{\\text{test}}^{\\text{kernel}} = K_{\\text{test}} \\alpha, $$\n   where $\\alpha \\in \\mathbb{R}^n$ is obtained by solving a linear system involving $K$ and $\\lambda$.\n5. Compare the two sets of test predictions numerically for each test case by computing the maximum absolute discrepancy\n   $$ \\Delta = \\max_{1 \\le i \\le m} \\left| \\hat{y}_{\\text{test}, i}^{\\text{primal}} - \\hat{y}_{\\text{test}, i}^{\\text{kernel}} \\right|. $$\n\nTest suite:\nImplement the above pipeline for the following five test cases, each specified by a tuple $(\\text{seed}, d, h, \\text{activation}, n, m, \\lambda, \\sigma_{\\text{noise}})$:\n- Case $1$: $(0, 3, 50, \\text{ReLU}, 64, 32, 10^{-2}, 0.05)$.\n- Case $2$: $(1, 5, 10, \\tanh, 80, 40, 10^{-4}, 0.10)$.\n- Case $3$: $(2, 2, 5, \\text{identity}, 40, 20, 1.0, 0.00)$.\n- Case $4$: $(3, 4, 20, \\text{ReLU}, 60, 30, 10^{6}, 0.20)$.\n- Case $5$: $(4, 3, 1, \\tanh, 50, 25, 10^{-2}, 0.05)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results for the five test cases as a comma-separated list enclosed in square brackets, ordered by cases $1$ through $5$, where each entry is the single float $\\Delta$ for that case. For example,\n$$ [\\Delta_1, \\Delta_2, \\Delta_3, \\Delta_4, \\Delta_5]. $$\nNo other text should be printed. All values are unitless real numbers.",
            "solution": "The problem requires a comparison between two equivalent formulations of a regularized linear regression problem. The first is a direct, or primal, regression in a feature space defined by a neural network's hidden layer. The second is an indirect, or dual, regression using a kernel function derived from the same features. The core of the problem lies in the mathematical equivalence of these two methods, a principle known as primal-dual equivalence, which is fundamental to kernel methods. The comparison is numerical, intended to measure the discrepancy arising from different computational paths in floating-point arithmetic.\n\nFirst, we validate the problem statement.\n\n### Step 1: Extract Givens\n- **Data Generation:**\n  - Inputs: $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$, $X_{\\text{test}} \\in \\mathbb{R}^{m \\times d}$ from a standard normal distribution $\\mathcal{N}(0, I)$.\n  - Target function: $f(x) = \\sum_{j=1}^{d} \\sin(x_j) + 0.1 \\, \\|x\\|_2^2$.\n  - Outputs: $y_{\\text{train}} = f(X_{\\text{train}}) + \\epsilon$, with $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2 I_n)$.\n- **Random Features:**\n  - Hidden layer dimension: $h$.\n  - Weights and biases: $W_1 \\in \\mathbb{R}^{d \\times h}$, $b \\in \\mathbb{R}^{1 \\times h}$ from $\\mathcal{N}(0, 1)$.\n  - Feature map: $\\phi(x) = \\sigma(x W_1 + b)$, with $\\sigma \\in \\{\\text{ReLU}, \\tanh, \\text{identity}\\}$.\n  - Transformed data: $Z_{\\text{train}} = \\phi(X_{\\text{train}}) \\in \\mathbb{R}^{n \\times h}$, $Z_{\\text{test}} = \\phi(X_{\\text{test}}) \\in \\mathbb{R}^{m \\times h}$.\n- **Primal Problem (Ridge Regression on Features):**\n  - Objective: $\\min_{W_2} \\; \\frac{1}{n} \\, \\|Z_{\\text{train}} W_2 - y_{\\text{train}}\\|_2^2 + \\lambda \\, \\|W_2\\|_2^2$.\n  - Predictions: $\\hat{y}_{\\text{test}}^{\\text{primal}} = Z_{\\text{test}} W_2$.\n- **Dual Problem (Kernel Ridge Regression):**\n  - Kernel: $k(x,x') = \\langle \\phi(x), \\phi(x') \\rangle$.\n  - Gram matrix: $K = Z_{\\text{train}} Z_{\\text{train}}^\\top \\in \\mathbb{R}^{n \\times n}$.\n  - Test-train kernel: $K_{\\text{test}} = Z_{\\text{test}} Z_{\\text{train}}^\\top \\in \\mathbb{R}^{m \\times n}$.\n  - Predictions: $\\hat{y}_{\\text{test}}^{\\text{kernel}} = K_{\\text{test}} \\alpha$, where $\\alpha$ is the solution to a linear system involving $K$ and $\\lambda$.\n- **Comparison Metric:** $\\Delta = \\max_{i} \\left| \\hat{y}_{\\text{test}, i}^{\\text{primal}} - \\hat{y}_{\\text{test}, i}^{\\text{kernel}} \\right|$.\n- **Parameters:** Five test cases are provided with specific values for $(\\text{seed}, d, h, \\text{activation}, n, m, \\lambda, \\sigma_{\\text{noise}})$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically sound, well-posed, and objective. It explores the duality between weight-space (primal) and sample-space (dual) regularized least squares, a cornerstone of machine learning theory. The objective functions are strictly convex for $\\lambda  0$, guaranteeing unique solutions. The kernel is properly defined from the feature map. The mathematical setup is self-contained and consistent. The core task is to demonstrate and quantify the numerical effects of solving two mathematically equivalent problems via different matrix operations. The problem does not violate any scientific principles, is formally specified, and is computationally verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with a complete solution.\n\n### Principle-Based Solution\n\nThe problem asks for an implementation and comparison of two approaches to solving a regularized least-squares problem on a given set of features. This is a classic example of primal-dual equivalence in machine learning.\n\n**1. Primal Formulation: Ridge Regression in Feature Space**\nThe output of the single-layer neural network is given by $g(x) = \\phi(x)W_2$, where $\\phi(x) = \\sigma(x W_1 + b)$ is the fixed, randomly-generated feature vector. The second-layer weights $W_2 \\in \\mathbb{R}^{h \\times 1}$ are trained by minimizing an $L_2$-regularized least-squares objective function, also known as Ridge Regression:\n$$\n\\mathcal{L}(W_2) = \\frac{1}{n} \\|Z_{\\text{train}} W_2 - y_{\\text{train}}\\|_2^2 + \\lambda \\|W_2\\|_2^2\n$$\nHere, $Z_{\\text{train}} \\in \\mathbb{R}^{n \\times h}$ is the matrix of feature vectors for the training set, and $y_{\\text{train}} \\in \\mathbb{R}^{n \\times 1}$ are the target values. This objective is a convex function of $W_2$. To find the minimizer, we compute the gradient with respect to $W_2$ and set it to the zero vector:\n$$\n\\nabla_{W_2} \\mathcal{L}(W_2) = \\frac{2}{n} Z_{\\text{train}}^\\top (Z_{\\text{train}} W_2 - y_{\\text{train}}) + 2\\lambda W_2 = 0\n$$\nRearranging the terms to solve for $W_2$ gives the normal equations for this problem:\n$$\n\\left(\\frac{1}{n} Z_{\\text{train}}^\\top Z_{\\text{train}} + \\lambda I_h\\right) W_2 = \\frac{1}{n} Z_{\\text{train}}^\\top y_{\\text{train}}\n$$\nMultiplying by $n$, we obtain a more common form:\n$$\n(Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h) W_2 = Z_{\\text{train}}^\\top y_{\\text{train}}\n$$\nwhere $I_h$ is the $h \\times h$ identity matrix. The matrix $(Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h)$ is positive definite for $\\lambda  0$ and thus invertible. The solution for $W_2$ is:\n$$\nW_2 = (Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h)^{-1} Z_{\\text{train}}^\\top y_{\\text{train}}\n$$\nThis involves solving a linear system with an $h \\times h$ matrix. The predictions on the test set $Z_{\\text{test}}$ are then computed as:\n$$\n\\hat{y}_{\\text{test}}^{\\text{primal}} = Z_{\\text{test}} W_2\n$$\n\n**2. Dual Formulation: Kernel Ridge Regression**\nThe \"kernel trick\" allows us to solve the same problem without explicitly forming the feature vectors if we can express the algorithm in terms of inner products. The kernel function is defined as the inner product in the feature space:\n$$\nk(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle = \\phi(x_i) \\phi(x_j)^\\top\n$$\nIn this problem, we have explicit access to the feature map $\\phi$, so we can compute the training Gram matrix $K \\in \\mathbb{R}^{n \\times n}$ directly as $K = Z_{\\text{train}} Z_{\\text{train}}^\\top$.\n\nThe representer theorem states that the optimal weight vector $W_2$ can be expressed as a linear combination of the feature vectors of the training data, i.e., $W_2 = Z_{\\text{train}}^\\top \\alpha$ for some dual coefficient vector $\\alpha \\in \\mathbb{R}^{n \\times 1}$. Substituting this into the primal solution for $W_2$ allows us to solve for $\\alpha$. The standard solution for $\\alpha$ in Kernel Ridge Regression, consistent with the primal objective, is the solution to the linear system:\n$$\n(K + n\\lambda I_n) \\alpha = y_{\\text{train}}\n$$\nwhere $I_n$ is the $n \\times n$ identity matrix. This system is solved for $\\alpha$:\n$$\n\\alpha = (K + n\\lambda I_n)^{-1} y_{\\text{train}}\n$$\nThis involves solving a linear system with an $n \\times n$ matrix. Predictions for the test set are made using the kernel function between test and training points:\n$$\n\\hat{y}_{\\text{test}}^{\\text{kernel}} = K_{\\text{test}} \\alpha = (Z_{\\text{test}} Z_{\\text{train}}^\\top) \\alpha\n$$\n\n**3. Primal-Dual Equivalence**\nTheoretically, the predictions from both methods must be identical. We can show this by substituting the expressions for $W_2$ and $\\alpha$ into the prediction equations.\n$$\n\\hat{y}_{\\text{test}}^{\\text{primal}} = Z_{\\text{test}} W_2 = Z_{\\text{test}} (Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h)^{-1} Z_{\\text{train}}^\\top y_{\\text{train}}\n$$\n$$\n\\hat{y}_{\\text{test}}^{\\text{kernel}} = K_{\\text{test}} \\alpha = (Z_{\\text{test}} Z_{\\text{train}}^\\top) (Z_{\\text{train}} Z_{\\text{train}}^\\top + n\\lambda I_n)^{-1} y_{\\text{train}}\n$$\nThe equivalence of these two expressions hinges on the matrix identity (a form of the Woodbury identity, also known as the push-through identity):\n$$\n(A^\\top A + cI_h)^{-1} A^\\top = A^\\top (A A^\\top + cI_n)^{-1}\n$$\nLetting $A = Z_{\\text{train}}$ and $c = n\\lambda$, we have:\n$$\n(Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h)^{-1} Z_{\\text{train}}^\\top = Z_{\\text{train}}^\\top (Z_{\\text{train}} Z_{\\text{train}}^\\top + n\\lambda I_n)^{-1}\n$$\nLeft-multiplying by $Z_{\\text{test}}$ shows the operators applied to $y_{\\text{train}}$ are identical:\n$$\nZ_{\\text{test}} (Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h)^{-1} Z_{\\text{train}}^\\top = Z_{\\text{test}} Z_{\\text{train}}^\\top (Z_{\\text{train}} Z_{\\text{train}}^\\top + n\\lambda I_n)^{-1}\n$$\nThus, $\\hat{y}_{\\text{test}}^{\\text{primal}} = \\hat{y}_{\\text{test}}^{\\text{kernel}}$ in exact arithmetic.\n\nThe computational choice between the primal and dual formulations depends on the relative sizes of the number of training samples $n$ and the feature dimension $h$. If $n \\gg h$, the primal formulation is more efficient as it requires solving an $h \\times h$ system. Conversely, if $h \\gg n$, the dual formulation is preferred, as it requires solving an $n \\times n$ system. The requested discrepancy $\\Delta$ will be a small non-zero value, reflecting the differing accumulation of floating-point errors in the two distinct computational paths.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares primal and dual solutions for ridge regression\n    on random features for a series of test cases.\n    \"\"\"\n\n    test_cases = [\n        # (seed, d, h, activation, n, m, lambda, sigma_noise)\n        (0, 3, 50, 'ReLU', 64, 32, 1e-2, 0.05),\n        (1, 5, 10, 'tanh', 80, 40, 1e-4, 0.10),\n        (2, 2, 5, 'identity', 40, 20, 1.0, 0.00),\n        (3, 4, 20, 'ReLU', 60, 30, 1e6, 0.20),\n        (4, 3, 1, 'tanh', 50, 25, 1e-2, 0.05),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        seed, d, h, activation_name, n, m, lam, sigma_noise = case\n        \n        # Set the seed for reproducibility\n        np.random.seed(seed)\n\n        # 1. Generate synthetic regression data\n        X_train = np.random.randn(n, d)\n        X_test = np.random.randn(m, d)\n\n        def target_function(X):\n            sin_term = np.sum(np.sin(X), axis=1)\n            norm_term = 0.1 * np.sum(X**2, axis=1) # Faster than np.linalg.norm for this purpose\n            return (sin_term + norm_term).reshape(-1, 1)\n\n        y_true_train = target_function(X_train)\n        noise = np.random.randn(n, 1) * sigma_noise\n        y_train = y_true_train + noise\n\n        # 2. Construct random features\n        W1 = np.random.randn(d, h)\n        b = np.random.randn(1, h)\n\n        activations = {\n            'ReLU': lambda z: np.maximum(0, z),\n            'tanh': lambda z: np.tanh(z),\n            'identity': lambda z: z\n        }\n        activation_func = activations[activation_name]\n\n        Z_train = activation_func(X_train @ W1 + b)\n        Z_test = activation_func(X_test @ W1 + b)\n\n        # 3. Primal solution (Ridge Regression)\n        # Solve (Z_train.T @ Z_train + n * lambda * I_h) W2 = Z_train.T @ y_train\n        h_dim = Z_train.shape[1]\n        A_primal = Z_train.T @ Z_train + n * lam * np.identity(h_dim)\n        c_primal = Z_train.T @ y_train\n        \n        W2 = np.linalg.solve(A_primal, c_primal)\n        \n        y_hat_primal = Z_test @ W2\n\n        # 4. Dual solution (Kernel Ridge Regression)\n        # Solve (K + n * lambda * I_n) alpha = y_train\n        # where K = Z_train @ Z_train.T\n        K = Z_train @ Z_train.T\n        A_kernel = K + n * lam * np.identity(n)\n        \n        alpha = np.linalg.solve(A_kernel, y_train)\n        \n        # Predictions: y_hat_kernel = K_test @ alpha\n        # where K_test = Z_test @ Z_train.T\n        K_test = Z_test @ Z_train.T\n        y_hat_kernel = K_test @ alpha\n\n        # 5. Compare predictions\n        delta = np.max(np.abs(y_hat_primal - y_hat_kernel))\n        results.append(delta)\n\n    # Format and print the final results\n    print(f\"[{','.join(f'{r:.12e}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}