{
    "hands_on_practices": [
        {
            "introduction": "最简单的人工神经元模型采用线性（或恒等）激活函数，这使其在数学上等价于经典的线性回归。通过这个练习 ，你将通过最小化平方误差，从第一性原理推导出神经元的最优权重和偏置。这个过程不仅能巩固你对神经元基本工作方式的理解，还能清晰地揭示深度学习与传统统计学之间的深刻联系。",
            "id": "3180363",
            "problem": "考虑一个具有恒等激活函数的单个人工神经元，其对于输入向量 $\\mathbf{x} \\in \\mathbb{R}^{d}$ 的输出为 $f(\\mathbf{x};\\mathbf{w},b) = \\mathbf{w}^{\\top}\\mathbf{x} + b$，其中 $\\mathbf{w} \\in \\mathbb{R}^{d}$ 是突触权重，$b \\in \\mathbb{R}$ 是偏置。对于一个带标签的数据集 $\\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$，其中 $y_{i} \\in \\{-1, +1\\}$，定义经验平方误差目标函数为 $J(\\mathbf{w}, b) = \\sum_{i=1}^{n}(\\mathbf{w}^{\\top}\\mathbf{x}_{i} + b - y_{i})^{2}$。仅从这些定义出发，从第一性原理推导最小化 $J(\\mathbf{w}, b)$ 的闭式最优参数 $(\\mathbf{w}^{\\star}, b^{\\star})$。\n\n然后，将您的推导应用于 $\\mathbb{R}^{2}$ 中的特定线性可分数据集：\n- 正类 ($y_{i} = +1$)：$\\mathbf{x}_{1} = (2, 0)$，$\\mathbf{x}_{2} = (0, 2)$。\n- 负类 ($y_{i} = -1$)：$\\mathbf{x}_{3} = (-2, 0)$，$\\mathbf{x}_{4} = (0, -2)$。\n\n计算该数据集的最优 $\\mathbf{w}^{\\star} = (w_{1}^{\\star}, w_{2}^{\\star})$ 和 $b^{\\star}$，并以行向量 $\\big[w_{1}^{\\star}\\;\\;w_{2}^{\\star}\\;\\;b^{\\star}\\big]$ 的形式提供您的最终答案。将每个数值分量四舍五入到四位有效数字。\n\n最后，定性讨论（无需计算）在何种条件下，单个恒等激活神经元的平方误差最小化器所产生的决策边界与支持向量机（SVM）的大间隔分离超平面重合，例如类别的对称性和协方差属性。您的讨论应基于以上定义和任何关于几何间隔的公认事实进行推理。",
            "solution": "用户提供了一个多部分问题。在验证问题陈述后，我将按顺序解决每个部分。\n\n### 问题验证\n\n**第1步：提取已知条件**\n- **模型：** 具有恒等激活函数的单个人工神经元。\n- **输出函数：** $f(\\mathbf{x};\\mathbf{w},b) = \\mathbf{w}^{\\top}\\mathbf{x} + b$，其中 $\\mathbf{x} \\in \\mathbb{R}^{d}$，$\\mathbf{w} \\in \\mathbb{R}^{d}$，以及 $b \\in \\mathbb{R}$。\n- **数据集：** 一个带标签的数据集 $\\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$，标签 $y_{i} \\in \\{-1, +1\\}$。\n- **目标函数：** 经验平方误差，$J(\\mathbf{w}, b) = \\sum_{i=1}^{n}(\\mathbf{w}^{\\top}\\mathbf{x}_{i} + b - y_{i})^{2}$。\n- **任务1：** 推导最小化 $J(\\mathbf{w}, b)$ 的闭式最优参数 $(\\mathbf{w}^{\\star}, b^{\\star})$。\n- **任务2：** 将推导应用于 $\\mathbb{R}^{2}$ 中的特定数据集（$d=2$，$n=4$）：\n  - 正类 ($y=+1$)：$\\mathbf{x}_{1} = (2, 0)$，$\\mathbf{x}_{2} = (0, 2)$。\n  - 负类 ($y=-1$)：$\\mathbf{x}_{3} = (-2, 0)$，$\\mathbf{x}_{4} = (0, -2)$。\n- **任务3：** 计算 $(\\mathbf{w}^{\\star}, b^{\\star})$ 的数值，并以行向量 $[w_{1}^{\\star}\\;\\;w_{2}^{\\star}\\;\\;b^{\\star}]$ 的形式提供最终答案，每个分量四舍五入到四位有效数字。\n- **任务4：** 定性讨论在何种条件下，平方误差最小化器的决策边界与大间隔支持向量机（SVM）的决策边界重合。\n\n**第2步：使用提取的已知条件进行验证**\n- **科学性：** 该问题是线性回归的标准表述，是统计学和机器学习中的一个基本课题。模型是一个简单的线性神经元，目标函数是平方误差和。这在科学上和数学上都是合理的。\n- **适定性：** 目标函数 $J(\\mathbf{w}, b)$ 是关于参数 $\\mathbf{w}$ 和 $b$ 的二次函数。它是凸函数，并且有唯一的全局最小值。因此，存在一个唯一的、稳定的、有意义的解。\n- **客观性：** 问题使用精确的数学符号和客观的语言进行陈述。\n- **其他缺陷：** 问题是自洽的，没有缺失或矛盾的信息。提供的数据是一致且简单的。问题并非微不足道，需要使用多元微积分进行标准推导。它不是比喻性的，并且是可验证的。\n\n**第3步：结论与行动**\n该问题是**有效的**。我将继续进行解答。\n\n### 第1部分：最优参数的推导\n\n问题是找到最小化目标函数的参数 $(\\mathbf{w}^{\\star}, b^{\\star})$：\n$$\nJ(\\mathbf{w}, b) = \\sum_{i=1}^{n}(\\mathbf{w}^{\\top}\\mathbf{x}_{i} + b - y_{i})^{2}\n$$\n这是一个无约束优化问题。一个可微凸函数的最小值出现在其关于参数的梯度为零向量的地方。解决这个问题的一个常用方法是使用正规方程，这可以通过增广输入向量来推导。\n\n我们定义一个增广权重向量 $\\hat{\\mathbf{w}} \\in \\mathbb{R}^{d+1}$ 和一个增广输入向量 $\\hat{\\mathbf{x}}_{i} \\in \\mathbb{R}^{d+1}$ 如下：\n$$\n\\hat{\\mathbf{w}} = \\begin{pmatrix} \\mathbf{w} \\\\ b \\end{pmatrix}, \\quad \\hat{\\mathbf{x}}_{i} = \\begin{pmatrix} \\mathbf{x}_{i} \\\\ 1 \\end{pmatrix}\n$$\n通过这些定义，神经元的输出可以写成单个内积的形式：\n$$\nf(\\mathbf{x}_{i};\\mathbf{w},b) = \\mathbf{w}^{\\top}\\mathbf{x}_{i} + b = \\hat{\\mathbf{w}}^{\\top}\\hat{\\mathbf{x}}_{i}\n$$\n设 $\\hat{X}$ 是一个 $n \\times (d+1)$ 的设计矩阵，其中第 $i$ 行是 $\\hat{\\mathbf{x}}_{i}^{\\top}$，并设 $\\mathbf{y}$ 是一个 $n \\times 1$ 的标签 $y_i$ 的列向量。目标函数可以用矩阵形式写成残差向量的欧几里得范数的平方：\n$$\nJ(\\hat{\\mathbf{w}}) = \\|\\hat{X}\\hat{\\mathbf{w}} - \\mathbf{y}\\|_{2}^{2}\n$$\n为了找到最小值，我们计算关于 $\\hat{\\mathbf{w}}$ 的梯度并将其设为零：\n$$\n\\nabla_{\\hat{\\mathbf{w}}}J(\\hat{\\mathbf{w}}) = \\nabla_{\\hat{\\mathbf{w}}} \\left( (\\hat{X}\\hat{\\mathbf{w}} - \\mathbf{y})^{\\top}(\\hat{X}\\hat{\\mathbf{w}} - \\mathbf{y}) \\right) = \\mathbf{0}\n$$\n$$\n\\nabla_{\\hat{\\mathbf{w}}} \\left( \\hat{\\mathbf{w}}^{\\top}\\hat{X}^{\\top}\\hat{X}\\hat{\\mathbf{w}} - 2\\mathbf{y}^{\\top}\\hat{X}\\hat{\\mathbf{w}} + \\mathbf{y}^{\\top}\\mathbf{y} \\right) = \\mathbf{0}\n$$\n$$\n2\\hat{X}^{\\top}\\hat{X}\\hat{\\mathbf{w}} - 2\\hat{X}^{\\top}\\mathbf{y} = \\mathbf{0}\n$$\n这就得出了正规方程：\n$$\n(\\hat{X}^{\\top}\\hat{X})\\hat{\\mathbf{w}} = \\hat{X}^{\\top}\\mathbf{y}\n$$\n假设矩阵 $\\hat{X}^{\\top}\\hat{X}$ 是可逆的，则唯一的最优参数向量 $\\hat{\\mathbf{w}}^{\\star}$ 由下式给出：\n$$\n\\hat{\\mathbf{w}}^{\\star} = (\\hat{X}^{\\top}\\hat{X})^{-1}\\hat{X}^{\\top}\\mathbf{y}\n$$\n$\\hat{X}^{\\top}\\hat{X}$ 和 $\\hat{X}^{\\top}\\mathbf{y}$ 这两项可以表示为对数据集的求和：\n$$\n\\hat{X}^{\\top}\\hat{X} = \\sum_{i=1}^{n} \\hat{\\mathbf{x}}_{i}\\hat{\\mathbf{x}}_{i}^{\\top} = \\sum_{i=1}^{n} \\begin{pmatrix} \\mathbf{x}_{i} \\\\ 1 \\end{pmatrix} \\begin{pmatrix} \\mathbf{x}_{i}^{\\top}  1 \\end{pmatrix} = \\sum_{i=1}^{n} \\begin{pmatrix} \\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}  \\mathbf{x}_{i} \\\\ \\mathbf{x}_{i}^{\\top}  1 \\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^{n} \\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}  \\sum_{i=1}^{n} \\mathbf{x}_{i} \\\\ \\left(\\sum_{i=1}^{n} \\mathbf{x}_{i}\\right)^{\\top}  n \\end{pmatrix}\n$$\n$$\n\\hat{X}^{\\top}\\mathbf{y} = \\sum_{i=1}^{n} \\hat{\\mathbf{x}}_{i}y_{i} = \\sum_{i=1}^{n} \\begin{pmatrix} \\mathbf{x}_{i}y_{i} \\\\ y_{i} \\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^{n} y_{i}\\mathbf{x}_{i} \\\\ \\sum_{i=1}^{n} y_{i} \\end{pmatrix}\n$$\n最优参数 $(\\mathbf{w}^{\\star}, b^{\\star})$ 可以通过求解由这些聚合矩阵和向量定义的线性系统来找到。\n\n### 第2部分：应用于特定数据集\n\n给定的数据集如下：\n- $\\mathbf{x}_{1} = (2, 0)^{\\top}$, $y_{1} = +1$\n- $\\mathbf{x}_{2} = (0, 2)^{\\top}$, $y_{2} = +1$\n- $\\mathbf{x}_{3} = (-2, 0)^{\\top}$, $y_{3} = -1$\n- $\\mathbf{x}_{4} = (0, -2)^{\\top}$, $y_{4} = -1$\n\n这里，$n=4$ 且 $d=2$。我们计算必要的总和：\n$$\nn = 4\n$$\n$$\n\\sum_{i=1}^{4} \\mathbf{x}_{i} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\sum_{i=1}^{4} y_{i} = 1 + 1 - 1 - 1 = 0\n$$\n$$\n\\sum_{i=1}^{4} y_{i}\\mathbf{x}_{i} = (1)\\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + (1)\\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} + (-1)\\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix} + (-1)\\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 4 \\end{pmatrix}\n$$\n$$\n\\sum_{i=1}^{4} \\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top} = \\begin{pmatrix} 4  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  4 \\end{pmatrix} + \\begin{pmatrix} 4  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  4 \\end{pmatrix} = \\begin{pmatrix} 8  0 \\\\ 0  8 \\end{pmatrix}\n$$\n现在，我们构建系统 $(\\hat{X}^{\\top}\\hat{X})\\hat{\\mathbf{w}} = \\hat{X}^{\\top}\\mathbf{y}$：\n$$\n\\begin{pmatrix} \\sum \\mathbf{x}_{i}\\mathbf{x}_{i}^{\\top}  \\sum \\mathbf{x}_{i} \\\\ (\\sum \\mathbf{x}_{i})^{\\top}  n \\end{pmatrix} \\begin{pmatrix} \\mathbf{w} \\\\ b \\end{pmatrix} = \\begin{pmatrix} \\sum y_{i}\\mathbf{x}_{i} \\\\ \\sum y_{i} \\end{pmatrix}\n$$\n代入计算出的值：\n$$\n\\begin{pmatrix} 8  0  0 \\\\ 0  8  0 \\\\ 0  0  4 \\end{pmatrix} \\begin{pmatrix} w_{1} \\\\ w_{2} \\\\ b \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 4 \\\\ 0 \\end{pmatrix}\n$$\n这对应于一个简单的三元一次方程组：\n1. $8w_{1} = 4 \\implies w_{1}^{\\star} = \\frac{4}{8} = 0.5$\n2. $8w_{2} = 4 \\implies w_{2}^{\\star} = \\frac{4}{8} = 0.5$\n3. $4b = 0 \\implies b^{\\star} = 0$\n\n最优参数为 $\\mathbf{w}^{\\star} = (0.5, 0.5)^{\\top}$ 和 $b^{\\star} = 0$。\n\n### 第3部分：数值答案和定性讨论\n\n问题要求最终答案为行向量 $[w_{1}^{\\star}\\;\\;w_{2}^{\\star}\\;\\;b^{\\star}]$，每个分量四舍五入到四位有效数字。\n- $w_{1}^{\\star} = 0.5 \\implies 0.5000$\n- $w_{2}^{\\star} = 0.5 \\implies 0.5000$\n- $b^{\\star} = 0 \\implies 0.0000$\n\n最终的数值答案向量是 $[0.5000\\;\\;0.5000\\;\\;0.0000]$。\n\n**定性讨论：**\n单个神经元的决策边界是输出为零的点集 $\\mathbf{x}$：$\\mathbf{w}^{\\star\\top}\\mathbf{x} + b^{\\star} = 0$。平方误差最小化器通过对类别标签 $y_i \\in \\{-1, +1\\}$ 进行线性回归来找到这个边界。相比之下，硬间隔支持向量机（SVM）找到的是使两个类别之间的几何间隔最大化的分离超平面。SVM的解仅依赖于最靠近边界的数据点（支持向量），而最小二乘解则受到训练集中每个数据点的影响。\n\n只有在特定的、高度对称的条件下，这两个决策边界才会重合。这些条件确保了最小二乘回归的“质心”逻辑与SVM的“最大间隔”逻辑相一致。关键条件如下：\n\n1.  **类别平衡：** 每个类别中的数据点数量必须相等 ($n_+ = n_-$)。这确保了标签的均值为零（$\\bar{y}=0$），并且整体数据质心 $\\bar{\\mathbf{x}} = (\\mu_+ + \\mu_-)/2$ 位于类别质心 $\\mu_+$ 和 $\\mu_-$ 之间的几何中点。在这种条件下，最小二乘决策边界将通过这个中点，SVM的边界也是如此。\n\n2.  **对称的类别分布：** 每个类别内数据点的分布必须是对称的，以至于不会使最小二乘解偏离最大间隔解。理想情况是当类条件分布 $p(\\mathbf{x}|y=+1)$ 和 $p(\\mathbf{x}|y=-1)$ 具有相同且球形的协方差矩阵时（例如，与单位矩阵成比例，$cI$）。在这种情况下，每个类别中的数据点都围绕其各自的均值各向同性地分布。远离边界的点的影响会完美地“平均掉”，由最小二乘法确定的权重向量 $\\mathbf{w}^\\star$ 的方向（这与Fisher线性判别分析相关）将变得与连接类别均值的向量 $\\mu_+ - \\mu_-$ 平行。在这样的对称性下，这与SVM的权重向量 $\\mathbf{w}_{\\text{SVM}}$ 的方向相同。\n\n如果这些条件不满足，解通常会发散。例如，如果一个类别有离群点或者比另一个类别更分散，这些点会“拉动”最小二乘回归线向它们靠近，使其偏离SVM会找到的最优大间隔位置。所提供的数据集是这些对称性成立的一个例子，这就是为什么最终的边界 $0.5x_1 + 0.5x_2 = 0$（或 $x_1+x_2=0$）确实是最大间隔分离超平面。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.5000  0.5000  0.0000 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "现代深度学习的基石之一是修正线性单元（ReLU）激活函数，它引入了关键的非线性。这个练习  将我们的视角从单个输入输出的计算提升到统计层面。你将需要量化当输入来自一个标准分布时，一个 ReLU 神经元被“激活”的概率。这种概率性的思维对于理解大规模网络中的信号流、初始化策略和学习动态至关重要。",
            "id": "3180428",
            "problem": "考虑一个单个人工神经元，其预激活值为 $a=w^{\\top}x+b$，激活函数为修正线性单元（ReLU）$f(a)=\\max\\{0,a\\}$。设输入 $x\\in\\mathbb{R}^{d}$ 来自零均值各向同性高斯分布，$x\\sim\\mathcal{N}(0,I_{d})$，权重向量 $w\\in\\mathbb{R}^{d}$ 为固定值，且 $\\|w\\|_{2}0$。如果 $a0$，则称该神经元被激活。从多元正态分布的核心定义以及高斯随机向量线性泛函的性质出发，推导概率 $\\mathbb{P}(w^{\\top}x+b0)$ 关于 $b$ 和 $\\|w\\|_{2}$ 的闭式表达式。然后，为分析当偏置项随权重大小缩放时的灵敏度，设 $b=\\alpha\\|w\\|_{2}$，其中 $\\alpha\\in\\mathbb{R}$ 为一个标量，将概率 $\\mathbb{P}(w^{\\top}x+b0)$ 表示为 $\\alpha$ 的函数，并计算该概率在 $\\alpha=0$ 处对 $\\alpha$ 的导数。您可以用 $\\Phi$ 表示标准正态分布的累积分布函数，用 $\\phi$ 表示其概率密度函数。请将最终答案表示为在 $\\alpha=0$ 处导数的精确值。无需四舍五入。",
            "solution": "在尝试求解之前，对问题陈述进行严格验证。\n\n### 步骤 1：提取已知条件\n- 神经元预激活值：$a = w^{\\top}x + b$\n- 激活函数：修正线性单元 (ReLU)，$f(a) = \\max\\{0, a\\}$\n- 输入向量：$x \\in \\mathbb{R}^{d}$，来自零均值各向同性高斯分布，$x \\sim \\mathcal{N}(0, I_d)$\n- 权重向量：$w \\in \\mathbb{R}^{d}$，固定，范数 $\\|w\\|_2  0$\n- 偏置项：$b \\in \\mathbb{R}$\n- 神经元激活条件：$a  0$，等价于 $w^{\\top}x + b  0$\n- 任务第 1 部分：推导概率 $\\mathbb{P}(w^{\\top}x + b  0)$ 关于 $b$ 和 $\\|w\\|_2$ 的闭式表达式。\n- 任务第 2 部分：设 $b = \\alpha \\|w\\|_2$，其中 $\\alpha \\in \\mathbb{R}$ 为一个标量，并将概率表示为 $\\alpha$ 的函数。\n- 任务第 3 部分：计算该概率在 $\\alpha = 0$ 处对 $\\alpha$ 的导数。\n- 符号说明：$\\Phi$ 表示标准正态分布的累积分布函数 (CDF)，$\\phi$ 表示其概率密度函数 (PDF)。\n\n### 步骤 2：使用提取的已知条件进行验证\n评估问题的有效性。\n- **科学依据：** 该问题牢固地植根于概率论及其在人工神经网络分析中的应用。高斯随机向量、随机变量的线性变换、ReLU 激活以及神经元激活的统计特性等概念，都是机器学习理论中的标准和基础内容。\n- **适定性：** 问题陈述清晰。它提供了所有必要的信息：输入的分布、神经元组件的定义以及明确的目标。条件 $\\|w\\|_2  0$ 至关重要且被正确地包含在内，以防止除以零，从而确保解是良定义的。可以从前提中推导出唯一、稳定且有意义的解。\n- **客观性：** 问题以精确、客观的数学语言陈述，没有歧义或主观论断。\n\n### 步骤 3：结论与行动\n该问题是 **有效的**。这是一个神经网络统计分析中的标准、适定问题。现在可以开始求解过程。\n\n### 解题推导\n目标是计算神经元被激活的概率，即 $\\mathbb{P}(a  0)$。预激活值为 $a = w^{\\top}x + b$，因此我们需要计算 $\\mathbb{P}(w^{\\top}x + b  0)$。\n\n令随机变量 $Y$ 定义为输入向量 $x$ 在权重向量 $w$ 上的线性投影：\n$$Y = w^{\\top}x = \\sum_{i=1}^{d} w_i x_i$$\n输入向量 $x$ 来自多元正态分布 $x \\sim \\mathcal{N}(0, I_d)$，其中 $0$ 是零向量，$I_d$ 是 $d \\times d$ 的单位矩阵。这意味着分量 $x_i$ 是独立同分布的标准正态随机变量，$x_i \\sim \\mathcal{N}(0, 1)$。\n\n高斯随机变量的线性组合本身也是一个高斯随机变量。为了描述 $Y$ 的分布，我们必须求出它的均值 $\\mathbb{E}[Y]$ 和方差 $\\text{Var}(Y)$。\n\n$Y$ 的均值为：\n$$\\mathbb{E}[Y] = \\mathbb{E}[w^{\\top}x] = w^{\\top}\\mathbb{E}[x]$$\n由于 $\\mathbb{E}[x] = 0$，均值为：\n$$\\mathbb{E}[Y] = w^{\\top}0 = 0$$\n\n$Y$ 的方差由通用公式 $\\text{Var}(w^{\\top}x) = w^{\\top}\\text{Cov}(x)w$ 给出。$x$ 的协方差矩阵已知为 $\\text{Cov}(x) = I_d$。因此：\n$$\\text{Var}(Y) = w^{\\top}I_d w = w^{\\top}w = \\sum_{i=1}^{d} w_i^2 = \\|w\\|_2^2$$\n因此，随机变量 $Y$ 服从均值为 $0$、方差为 $\\|w\\|_2^2$ 的正态分布。我们记为 $Y \\sim \\mathcal{N}(0, \\|w\\|_2^2)$。\n\n现在，我们可以计算激活概率：\n$$\\mathbb{P}(a  0) = \\mathbb{P}(w^{\\top}x + b  0) = \\mathbb{P}(Y  -b)$$\n为了使用标准正态分布计算此概率，我们对随机变量 $Y$ 进行标准化。令 $Z$ 为一个标准正态随机变量，$Z \\sim \\mathcal{N}(0, 1)$。$Y$ 的标准化为：\n$$Z = \\frac{Y - \\mathbb{E}[Y]}{\\sqrt{\\text{Var}(Y)}} = \\frac{w^{\\top}x - 0}{\\sqrt{\\|w\\|_2^2}} = \\frac{w^{\\top}x}{\\|w\\|_2}$$\n不等式 $Y  -b$ 可以用 $Z$ 来重写：\n$$\\frac{Y}{\\|w\\|_2}  \\frac{-b}{\\|w\\|_2} \\implies Z  -\\frac{b}{\\|w\\|_2}$$\n概率则为 $\\mathbb{P}(Z  -\\frac{b}{\\|w\\|_2})$。根据标准正态累积分布函数 $\\Phi(z) = \\mathbb{P}(Z \\le z)$ 的定义，我们有 $\\mathbb{P}(Z  z) = 1 - \\Phi(z)$。此外，由于标准正态分布的对称性，$\\Phi(-z) = 1 - \\Phi(z)$。这意味着 $\\mathbb{P}(Z  z) = \\Phi(-z)$。\n\n将此性质应用于 $z = -\\frac{b}{\\|w\\|_2}$：\n$$\\mathbb{P}(Z  -\\frac{b}{\\|w\\|_2}) = \\Phi\\left(-\\left(-\\frac{b}{\\|w\\|_2}\\right)\\right) = \\Phi\\left(\\frac{b}{\\|w\\|_2}\\right)$$\n这就得到了概率关于 $b$ 和 $\\|w\\|_2$ 的闭式表达式。\n\n接下来，我们分析该概率对偏置项的灵敏度，条件是偏置项随权重大小缩放。我们设 $b = \\alpha \\|w\\|_2$，其中 $\\alpha \\in \\mathbb{R}$ 为某个标量。\n令 $P(\\alpha)$ 为激活概率作为 $\\alpha$ 的函数：\n$$P(\\alpha) = \\mathbb{P}(w^{\\top}x + \\alpha\\|w\\|_2  0) = \\Phi\\left(\\frac{\\alpha \\|w\\|_2}{\\|w\\|_2}\\right) = \\Phi(\\alpha)$$\n问题要求计算此概率对 $\\alpha$ 的导数，并在 $\\alpha=0$ 处求值。我们首先求导数 $\\frac{d}{d\\alpha}P(\\alpha)$。\n根据微积分基本定理，一个连续随机变量的累积分布函数 (CDF) 的导数是其概率密度函数 (PDF)。标准正态分布的 PDF 用 $\\phi$ 表示。\n$$\\frac{d}{d\\alpha}P(\\alpha) = \\frac{d}{d\\alpha}\\Phi(\\alpha) = \\phi(\\alpha)$$\n标准正态分布的 PDF 由以下公式给出：\n$$\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right)$$\n我们需在 $\\alpha = 0$ 处计算这个导数：\n$$\\frac{d P}{d\\alpha}\\bigg|_{\\alpha=0} = \\phi(0)$$\n将 $\\alpha=0$ 代入 PDF 公式：\n$$\\phi(0) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{0^2}{2}\\right) = \\frac{1}{\\sqrt{2\\pi}} \\exp(0) = \\frac{1}{\\sqrt{2\\pi}} \\cdot 1 = \\frac{1}{\\sqrt{2\\pi}}$$\n这就是导数的最终精确值。",
            "answer": "$$\\boxed{\\frac{1}{\\sqrt{2\\pi}}}$$"
        },
        {
            "introduction": "在作为分类器时，神经元不仅是一个函数逼近器，更是一个决策单元。这个练习  挑战我们以决策论的视角来审视偏置项 $b$。你将推导出如何设置偏置，使其在考虑类别不平衡和非对称误分类成本等现实因素下，实现贝叶斯最优决策。这揭示了神经元的参数如何与风险最小化的基本原则联系起来，从而做出更智能的判断。",
            "id": "3180386",
            "problem": "单个神经元计算预激活值 $z = \\mathbf{w}^{\\top} \\mathbf{x} + b$，如果 $z \\geq 0$，则将输入分类为正例 ($y=1$)，否则分类为负例 ($y=0$)。考虑一个二元分类任务，其类别先验概率为 $\\pi_{1} = \\mathbb{P}(Y=1)$ 和 $\\pi_{0} = \\mathbb{P}(Y=0)$，其中 $\\pi_{1} + \\pi_{0} = 1$。假设标签不平衡，因此 $\\pi_{1} \\neq \\pi_{0}$。错分类会产生非对称成本：当 $Y=0$ 时预测为 $y=1$（假正例）的成本为 $C_{10}  0$，当 $Y=1$ 时预测为 $y=0$（假负例）的成本为 $C_{01}  0$。正确分类的成本为零。\n\n假设神经元以真实类别为条件的预激活值可以很好地用等方差高斯分布建模：$z \\mid Y=1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2})$ 和 $z \\mid Y=0 \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2})$，其中 $\\sigma^{2}  0$ 且 $\\mu_{1} \\neq \\mu_{0}$。因为 $b$ 对两个类别的 $z$ 产生相同的平移，所以记 $\\mu_{1} = m_{1} + b$ 和 $\\mu_{0} = m_{0} + b$，其中 $m_{1} = \\mathbb{E}[\\mathbf{w}^{\\top} \\mathbf{x} \\mid Y=1]$ 和 $m_{0} = \\mathbb{E}[\\mathbf{w}^{\\top} \\mathbf{x} \\mid Y=0]$ 由数据和权重向量 $\\mathbf{w}$ 固定。\n\n在给定的成本和先验概率下，从最小期望风险原则出发，推导使零阈值规则 $z \\geq 0$ 在这些假设下成为贝叶斯最优的偏置 $b^{\\star}$ 的闭式表达式。在您的推理中，请将受试者工作特征 (ROC) 曲线上的最优点与成本和先验概率联系起来，但最终答案只提供 $b^{\\star}$。无需四舍五入；请将您的最终答案表示为单个无单位的解析表达式。",
            "solution": "该问题要求解单个神经元的最优偏置项 $b^{\\star}$，使其决策规则成为贝叶斯最优。决策规则是：如果预激活值 $z = \\mathbf{w}^{\\top} \\mathbf{x} + b$ 为非负 ($z \\geq 0$)，则将输入分类为正例 ($y=1$)，否则分类为负例 ($y=0$)。最优性准则是最小化期望风险，同时考虑错分类的非对称成本和类别不平衡。\n\n期望风险 $R$ 是所有可能误差的成本之和，按其概率加权：\n$$R = \\mathbb{E}[\\text{Cost}(Y, y)] = C_{01}\\mathbb{P}(y=0, Y=1) + C_{10}\\mathbb{P}(y=1, Y=0)$$\n其中 $Y$ 是真实标签，$y$ 是预测标签。假负例 ($y=0, Y=1$) 和假正例 ($y=1, Y=0$) 的成本分别为 $C_{01}$ 和 $C_{10}$。正确分类的成本为零。\n\n使用全概率定律 $\\mathbb{P}(A, B) = \\mathbb{P}(A|B)\\mathbb{P}(B)$ 和给定的先验概率 $\\pi_{1} = \\mathbb{P}(Y=1)$ 及 $\\pi_{0} = \\mathbb{P}(Y=0)$，风险可以表示为：\n$$R = C_{01}\\pi_{1}\\mathbb{P}(y=0|Y=1) + C_{10}\\pi_{0}\\mathbb{P}(y=1|Y=0)$$\n一个通用的决策规则是在 $z \\geq T$ 时分类为正例，其中 $T$ 是某个阈值。决策的概率取决于这个阈值：\n$$R(T) = C_{01}\\pi_{1}\\mathbb{P}(z  T|Y=1) + C_{10}\\pi_{0}\\mathbb{P}(z \\geq T|Y=0)$$\n设 $p(z|Y=1)$ 和 $p(z|Y=0)$ 分别是 $z$ 的条件概率密度函数 (PDF)。我们可以将风险写成积分形式：\n$$R(T) = C_{01}\\pi_{1}\\int_{-\\infty}^{T} p(z|Y=1) dz + C_{10}\\pi_{0}\\int_{T}^{\\infty} p(z|Y=0) dz$$\n为了找到最小化 $R(T)$ 的最优阈值 $T^{\\star}$，我们将 $R(T)$ 对 $T$ 求导，并令结果为零。根据微积分基本定理：\n$$\\frac{dR(T)}{dT} = C_{01}\\pi_{1}p(T|Y=1) - C_{10}\\pi_{0}p(T|Y=0)$$\n将导数设为零，得到最优阈值 $T^{\\star}$ 的条件：\n$$C_{01}\\pi_{1}p(T^{\\star}|Y=1) = C_{10}\\pi_{0}p(T^{\\star}|Y=0)$$\n这可以重新整理，以表明最优阈值处的似然比必须等于一个由成本和先验概率决定的值：\n$$\\frac{p(T^{\\star}|Y=1)}{p(T^{\\star}|Y=0)} = \\frac{C_{10}\\pi_{0}}{C_{01}\\pi_{1}}$$\n此条件定义了最优点。问题指出 $z$ 的条件分布是等方差的高斯分布：$z \\mid Y=1 \\sim \\mathcal{N}(\\mu_{1}, \\sigma^{2})$ 和 $z \\mid Y=0 \\sim \\mathcal{N}(\\mu_{0}, \\sigma^{2})$。相应的概率密度函数为：\n$$p(z|Y=1) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z-\\mu_1)^2}{2\\sigma^2}\\right)$$\n$$p(z|Y=0) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z-\\mu_0)^2}{2\\sigma^2}\\right)$$\n似然比为：\n$$\\frac{p(z|Y=1)}{p(z|Y=0)} = \\frac{\\exp\\left(-\\frac{(z-\\mu_1)^2}{2\\sigma^2}\\right)}{\\exp\\left(-\\frac{(z-\\mu_0)^2}{2\\sigma^2}\\right)} = \\exp\\left(\\frac{(z-\\mu_0)^2 - (z-\\mu_1)^2}{2\\sigma^2}\\right)$$\n指数中的项简化为：\n$$(z-\\mu_0)^2 - (z-\\mu_1)^2 = (z^2 - 2z\\mu_0 + \\mu_0^2) - (z^2 - 2z\\mu_1 + \\mu_1^2) = 2z(\\mu_1 - \\mu_0) - (\\mu_1^2 - \\mu_0^2)$$\n因此，似然比变为：\n$$\\exp\\left(\\frac{2z(\\mu_1 - \\mu_0) - (\\mu_1^2 - \\mu_0^2)}{2\\sigma^2}\\right)$$\n在最优阈值 $z=T^{\\star}$ 处，我们有：\n$$\\exp\\left(\\frac{2T^{\\star}(\\mu_1 - \\mu_0) - (\\mu_1^2 - \\mu_0^2)}{2\\sigma^2}\\right) = \\frac{C_{10}\\pi_{0}}{C_{01}\\pi_{1}}$$\n对两边取自然对数：\n$$\\frac{2T^{\\star}(\\mu_1 - \\mu_0) - (\\mu_1^2 - \\mu_0^2)}{2\\sigma^2} = \\ln\\left(\\frac{C_{10}\\pi_{0}}{C_{01}\\pi_{1}}\\right)$$\n求解 $T^{\\star}$：\n$$2T^{\\star}(\\mu_1 - \\mu_0) = (\\mu_1^2 - \\mu_0^2) + 2\\sigma^2 \\ln\\left(\\frac{C_{10}\\pi_{0}}{C_{01}\\pi_{1}}\\right)$$\n$$T^{\\star} = \\frac{\\mu_1^2 - \\mu_0^2}{2(\\mu_1 - \\mu_0)} + \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{C_{10}\\pi_{0}}{C_{01}\\pi_{1}}\\right)$$\n$$T^{\\star} = \\frac{\\mu_1 + \\mu_0}{2} + \\frac{\\sigma^2}{\\mu_1 - \\mu_0} \\ln\\left(\\frac{C_{10}\\pi_{0}}{C_{01}\\pi_{1}}\\right)$$\n神经元的分类规则 $z \\geq 0$ 实际上将决策阈值固定在 $T=0$。为了使该规则最优，必须选择参数，使得最优阈值 $T^{\\star}$ 等于 $0$。偏置项 $b$ 允许我们移动预激活值 $z$ 来实现这一点。$z$ 的分布均值由 $\\mu_{1} = m_{1} + b$ 和 $\\mu_{0} = m_{0} + b$ 给出。将这些代入 $T^{\\star}$ 的表达式并令 $T^{\\star}=0$：\n$$\\mu_1 + \\mu_0 = (m_1 + b) + (m_0 + b) = m_1 + m_0 + 2b$$\n$$\\mu_1 - \\mu_0 = (m_1 + b) - (m_0 + b) = m_1 - m_0$$\n最优阈值的方程变为：\n$$0 = \\frac{m_1 + m_0 + 2b}{2} + \\frac{\\sigma^2}{m_1 - m_0} \\ln\\left(\\frac{C_{10}\\pi_{0}}{C_{01}\\pi_{1}}\\right)$$\n$$0 = \\frac{m_1 + m_0}{2} + b + \\frac{\\sigma^2}{m_1 - m_0} \\ln\\left(\\frac{C_{10}\\pi_{0}}{C_{01}\\pi_{1}}\\right)$$\n最后，我们求解满足此条件的最优偏置 $b^{\\star}$：\n$$b^{\\star} = - \\frac{m_1 + m_0}{2} - \\frac{\\sigma^2}{m_1 - m_0} \\ln\\left(\\frac{C_{10}\\pi_{0}}{C_{01}\\pi_{1}}\\right)$$\n这个表达式给出了将神经元的决策边界调整到贝叶斯最优位置的偏置，它同时考虑了数据分布的属性（$m_0, m_1, \\sigma^2$）和分类任务的要求（先验概率 $\\pi_0, \\pi_1$ 及成本 $C_{10}, C_{01}$）。",
            "answer": "$$\n\\boxed{- \\frac{m_{1} + m_{0}}{2} - \\frac{\\sigma^{2}}{m_{1} - m_{0}} \\ln\\left(\\frac{C_{10} \\pi_{0}}{C_{01} \\pi_{1}}\\right)}\n$$"
        }
    ]
}