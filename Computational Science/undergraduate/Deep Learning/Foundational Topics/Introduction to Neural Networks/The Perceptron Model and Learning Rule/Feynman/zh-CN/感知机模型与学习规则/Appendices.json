{
    "hands_on_practices": [
        {
            "introduction": "感知器作为最简单的线性分类器，其表达能力是有限的。本练习旨在通过几个经典的逻辑函数例子，深入剖析感知器的能力边界。你将首先为一个线性可分的问题（三输入多数表决）构建一个感知器，然后直面其无法解决的“异或”（XOR）问题，并最终探索如何通过特征变换来克服这一局限性，为后续学习更复杂的模型打下概念基础。",
            "id": "3190716",
            "problem": "考虑一个二元分类任务，其感知器模型由决策函数 $\\hat{y} = \\operatorname{sign}(w^{\\top} x + b)$ 定义，其中 $x \\in \\{0,1\\}^{d}$，$w \\in \\mathbb{R}^{d}$，$b \\in \\mathbb{R}$。在下面的所有部分中，类别标签为 $y \\in \\{-1,+1\\}$，其中正类标记为 $+1$。\n\nA部分（三输入多数决）：令 $d = 3$，并将目标定义为多数表决逻辑：如果 $x_1 + x_2 + x_3 \\geq 2$，则 $y = +1$，否则 $y = -1$。假设感知机具有相等的正权重，$w_1 = w_2 = w_3 = w$ 且 $w > 0$，以及偏置 $b \\in \\mathbb{R}$。在所有满足对所有 $x \\in \\{0,1\\}^{3}$ 进行正确分类的分离超平面中，确定比值 $b/w$ 的值，该值能够最大化在决策边界与任何训练点之间的最小有符号距离（该距离沿感知机的法线方向测量）。\n\nB部分（异或（XOR）问题的失败）：考虑 $d = 2$ 的情况，其异或（XOR）目标定义为：如果 $x_1 + x_2 = 1$，则 $y = +1$；如果 $x_1 + x_2 \\in \\{0,2\\}$，则 $y = -1$。从第一性原理出发（不借助已有的定理），解释为什么在原始输入空间中，没有任何 $(w,b)$ 的选择可以产生一个分离超平面。\n\nC部分（通过成对交互特征恢复可分性）：通过添加成对交互项来增强 $d=2$ 的XOR输入，以形成特征映射 $\\phi(x_1,x_2) = (x_1, x_2, x_1 x_2) \\in \\mathbb{R}^{3}$。解释在增强空间中，如何通过适当选择权重和偏置来正确分离XOR数据。\n\n你的最终答案应该是来自A部分的 $b/w$ 的精确值，表示为单个最简分数。不包含任何单位。无需四舍五入。",
            "solution": "该问题提出了对感知机模型的三部分分析。在验证了问题的完整性之后，我将系统地处理每个部分。\n\n问题陈述是有效的。它在科学上基于线性分类器和感知机模型的既定理论。这些任务是适定的、客观的，并使用精确的数学语言，提供了推导解决方案所需的所有必要信息。该问题是机器学习基础中的一个标准的、不平凡的练习。\n\n### A部分：三输入多数决函数\n\n任务是找到比值 $b/w$，该比值能够最大化一个用于分类三输入多数决函数的感知机的最小几何间隔。\n\n输入空间为 $x \\in \\{0,1\\}^3$。共有 $2^3=8$ 个可能的输入向量。目标函数定义为：如果输入之和 $S = x_1+x_2+x_3$ 至少为2，则 $y = +1$，否则 $y=-1$。\n数据点及其对应的标签 $y$ 和和 $S$ 如下：\n- $S=0$：$x=(0,0,0)$，$y=-1$\n- $S=1$：$x \\in \\{(1,0,0), (0,1,0), (0,0,1)\\}$，$y=-1$\n- $S=2$：$x \\in \\{(1,1,0), (1,0,1), (0,1,1)\\}$，$y=+1$\n- $S=3$：$x=(1,1,1)$，$y=+1$\n\n感知机模型的决策边界由 $w^{\\top}x+b=0$ 定义。给定权重相等且为正，即 $w_1=w_2=w_3=w > 0$。因此，激活函数为：\n$$w^{\\top}x+b = w_1x_1+w_2x_2+w_3x_3+b = w(x_1+x_2+x_3)+b = wS+b$$\n对于一个带有标签 $y$ 的点 $x$，要使其被正确分类，必须满足 $y(wS+b)>0$。让我们为所有8个点的可分性建立条件。\n- 对于 $S=0, y=-1$：$(-1)(w \\cdot 0 + b) > 0 \\implies -b > 0 \\implies b  0$。\n- 对于 $S=1, y=-1$：$(-1)(w \\cdot 1 + b)  0 \\implies -w-b  0 \\implies w+b  0$。\n- 对于 $S=2, y=+1$：$(+1)(w \\cdot 2 + b)  0 \\implies 2w+b  0$。\n- 对于 $S=3, y=+1$：$(+1)(w \\cdot 3 + b)  0 \\implies 3w+b  0$。\n\n从 $w+b  0$ 我们得到 $b  -w$。从 $2w+b0$ 我们得到 $b  -2w$。结合这些，我们发现任何这种形式的分离超平面都必须满足 $-2w  b  -w$。因为 $w0$，这意味着 $b0$，与我们的第一个不等式一致。这也意味着 $3w+b  3w-2w = w  0$，满足第四个不等式。因此，可分性的条件是 $-2  b/w  -1$。\n\n点 $x_i$ 到决策边界的有符号几何距离（或间隔）由 $\\gamma_i = \\frac{y_i(w^{\\top}x_i+b)}{\\|w\\|}$ 给出。我们的目标是最大化最小距离，即 $\\gamma_{\\min} = \\min_{i} \\gamma_i$。\n权重向量的范数为 $\\|w\\| = \\sqrt{w_1^2+w_2^2+w_3^2} = \\sqrt{w^2+w^2+w^2} = \\sqrt{3w^2} = w\\sqrt{3}$（因为 $w0$）。\n\n根据和 $S$ 区分的每组点的间隔为：\n- $S=0, y=-1$：$\\gamma_0 = \\frac{(-1)(w \\cdot 0 + b)}{w\\sqrt{3}} = \\frac{-b}{w\\sqrt{3}}$\n- $S=1, y=-1$：$\\gamma_1 = \\frac{(-1)(w \\cdot 1 + b)}{w\\sqrt{3}} = \\frac{-w-b}{w\\sqrt{3}}$\n- $S=2, y=+1$：$\\gamma_2 = \\frac{(+1)(w \\cdot 2 + b)}{w\\sqrt{3}} = \\frac{2w+b}{w\\sqrt{3}}$\n- $S=3, y=+1$：$\\gamma_3 = \\frac{(+1)(w \\cdot 3 + b)}{w\\sqrt{3}} = \\frac{3w+b}{w\\sqrt{3}}$\n\n总间隔为 $\\gamma_{\\min} = \\min(\\gamma_0, \\gamma_1, \\gamma_2, \\gamma_3)$。约束间隔的点（支持向量）是那些离决策边界最近的点。在可分性区域（$-2w  b  -w$）内，$y(wS+b)$ 的最小正值将来自 $S=1$ 和 $S=2$ 的情况。让我们来验证一下。设 $f(S) = y(wS+b)$。\n- $f(0)=-b$。$f(1)=-w-b$。因为 $w0$，所以 $-b  -w-b$。因此 $\\gamma_0  \\gamma_1$。\n- $f(2)=2w+b$。$f(3)=3w+b$。因为 $w0$，所以 $2w+b  3w+b$。因此 $\\gamma_2  \\gamma_3$。\n因此，最小间隔为 $\\gamma_{\\min} = \\min(\\gamma_1, \\gamma_2)$。为了最大化这个最小值，我们必须使这两个参数相等。这在最大间隔超平面上发生。\n$$\\gamma_1 = \\gamma_2$$\n$$\\frac{-w-b}{w\\sqrt{3}} = \\frac{2w+b}{w\\sqrt{3}}$$\n$$-w-b = 2w+b$$\n$$-3w = 2b$$\n$$\\frac{b}{w} = -\\frac{3}{2}$$\n这个值位于区间 $(-2, -1)$ 内，证实了它对应一个有效的分离超平面。最大化最小距离的比值 $b/w$ 是 $-3/2$。\n\n### B部分：XOR的不可分性\n\n对于 $d=2$ 的异或（XOR）函数由以下输入-输出对定义：\n- $x^{(1)}=(0,0)$，$y^{(1)}=-1$\n- $x^{(2)}=(0,1)$，$y^{(2)}=+1$\n- $x^{(3)}=(1,0)$，$y^{(3)}=+1$\n- $x^{(4)}=(1,1)$，$y^{(4)}=-1$\n\n一个线性分离器是由 $w_1 x_1 + w_2 x_2 + b = 0$ 定义的超平面。为了使数据线性可分，必须存在参数 $(w_1, w_2, b)$，使得对所有 $i \\in \\{1,2,3,4\\}$ 都有 $y^{(i)}(w_1 x_1^{(i)} + w_2 x_2^{(i)} + b)  0$。这产生了一个由四个线性不等式组成的系统：\n1. 对于 $(0,0), y=-1$：$(-1)(w_1 \\cdot 0 + w_2 \\cdot 0 + b)  0 \\implies -b  0 \\implies b  0$。\n2. 对于 $(0,1), y=+1$：$(+1)(w_1 \\cdot 0 + w_2 \\cdot 1 + b)  0 \\implies w_2 + b  0$。\n3. 对于 $(1,0), y=+1$：$(+1)(w_1 \\cdot 1 + w_2 \\cdot 0 + b)  0 \\implies w_1 + b  0$。\n4. 对于 $(1,1), y=-1$：$(-1)(w_1 \\cdot 1 + w_2 \\cdot 1 + b)  0 \\implies w_1 + w_2 + b  0$。\n\n我们来证明这个系统无解。\n从不等式(2)得到 $w_2  -b$。\n从不等式(3)得到 $w_1  -b$。\n将这两个表达式相加得到：\n$$w_1 + w_2  -2b$$\n从不等式(1)我们知道 $b$ 是负数。令 $c = -b$，其中 $c  0$。不等式变为：\n- $w_2 > c$\n- $w_1 > c$\n- $w_1 + w_2 > 2c$\n不等式(4)可以改写为 $w_1 + w_2  -b$，即 $w_1 + w_2  c$。\n我们推导出了两个相互矛盾的要求：$w_1 + w_2  2c$ 和 $w_1 + w_2  c$。因为 $c$ 是一个正值，一个数不可能同时大于 $2c$ 又小于 $c$。这个矛盾证明了没有单个线性超平面可以满足所有四个条件。因此，XOR函数不是线性可分的。\n\n### C部分：用特征映射恢复可分性\n\n我们通过使用 $\\phi(x_1,x_2) = (x_1, x_2, x_1 x_2)$ 将输入空间 $x=(x_1, x_2)$ 映射到一个新的特征空间来增强输入空间。我们称新的坐标为 $z=(z_1, z_2, z_3)$。数据点转换如下：\n- $x=(0,0) \\implies z^{(1)}=(0,0,0)$，$y^{(1)}=-1$\n- $x=(0,1) \\implies z^{(2)}=(0,1,0)$，$y^{(2)}=+1$\n- $x=(1,0) \\implies z^{(3)}=(1,0,0)$，$y^{(3)}=+1$\n- $x=(1,1) \\implies z^{(4)}=(1,1,1)$，$y^{(4)}=-1$\n\n我们现在在这个新的三维空间中寻找一个分离超平面，定义为 $w'^{\\top}z + b' = w'_1 z_1 + w'_2 z_2 + w'_3 z_3 + b' = 0$。可分性条件是：\n1. 对于 $z=(0,0,0), y=-1$：$-b'  0 \\implies b'  0$。\n2. 对于 $z=(0,1,0), y=+1$：$w'_2 + b'  0$。\n3. 对于 $z=(1,0,0), y=+1$：$w'_1 + b'  0$。\n4. 对于 $z=(1,1,1), y=-1$：$-(w'_1 + w'_2 + w'_3 + b')  0 \\implies w'_1 + w'_2 + w'_3 + b'  0$。\n\n与原始问题不同，这个不等式系统有解。让我们构造一个解。\n选择 $w'_1 = 1$ 和 $w'_2 = 1$。\n从条件(2)和(3)，我们需要 $1+b'  0$，所以 $b'  -1$。\n条件(1)要求 $b'  0$。因此我们必须有 $-1  b'  0$。我们选择 $b' = -1/2$。\n现在，将这些值代入条件(4)：\n$1 + 1 + w'_3 - \\frac{1}{2}  0 \\implies \\frac{3}{2} + w'_3  0 \\implies w'_3  -\\frac{3}{2}$。\n我们选择 $w'_3 = -2$。\n\n一个有效的分离器由参数 $w'=(1, 1, -2)$ 和 $b'=-1/2$ 给出。我们来验证一下：\n- $z^{(1)}=(0,0,0), y=-1$：$1(0)+1(0)-2(0) - 1/2 = -1/2$。$\\operatorname{sign}(-1/2)=-1$。正确。\n- $z^{(2)}=(0,1,0), y=+1$：$1(0)+1(1)-2(0) - 1/2 = 1/2$。$\\operatorname{sign}(1/2)=+1$。正确。\n- $z^{(3)}=(1,0,0), y=+1$：$1(1)+1(0)-2(0) - 1/2 = 1/2$。$\\operatorname{sign}(1/2)=+1$。正确。\n- $z^{(4)}=(1,1,1), y=-1$：$1(1)+1(1)-2(1) - 1/2 = -1/2$。$\\operatorname{sign}(-1/2)=-1$。正确。\n\n添加非线性特征 $x_1x_2$ 将数据映射到更高维度的空间，使其变得线性可分。从几何上看，原始 $x_1, x_2$ 平面中的四个点无法被一条直线分开。特征映射将其中一个点 $(1,1)$ 从 $z_3=0$ 平面提升到 $(1,1,1)$。$\\mathbb{R}^3$ 中的四个点是 $(0,0,0)$, $(0,1,0)$, $(1,0,0)$ 和 $(1,1,1)$。现在可以放置一个平面（例如，$z_1+z_2-2z_3 - 1/2 = 0$）来将正类点 $\\{(0,1,0), (1,0,0)\\}$ 与负类点 $\\{(0,0,0), (1,1,1)\\}$ 分开。\n\n最终要求的答案是A部分的具体结果。如上所述，这就是比值 $b/w$。",
            "answer": "$$\\boxed{-\\frac{3}{2}}$$"
        },
        {
            "introduction": "感知机学习算法的优雅之处在于，其在“犯错时学习”的简单规则背后，蕴含着严格的收敛性理论保证。对于线性可分的数据集，感知机犯错的次数存在一个上限，这个上限由数据的几何特性（如数据半径 $R$ 和间隔 $\\gamma$）决定。本练习将引导你通过编程实践，亲手计算并验证这个著名的感知机错误上界理论，从而将抽象的数学定理与具体的算法行为联系起来。",
            "id": "3190718",
            "problem": "要求您在一些小型的合成线性可分数据集上实现并进行感知机学习算法的实证研究。该计算实验必须基于感知机假设类和感知机学习规则的基本定义，不应依赖任何预先推导的性能公式。您的程序必须计算感知机在训练过程中所犯的实证错误数，然后计算一个依赖于数据集和给定参考分离向量的几何量。最后，将多个测试用例的结果汇总，并以单一、机器可检查的行输出。\n\n将要使用的基本概念和定义：\n- 一个二元分类数据集由输入 $x_i \\in \\mathbb{R}^d$ 和标签 $y_i \\in \\{-1, +1\\}$ 组成。\n- 一个线性分类器由一个权重向量 $w \\in \\mathbb{R}^d$ 指定，其预测为 $\\hat{y}_i = \\mathrm{sign}(w^\\top x_i)$，其中如果 $z  0$，$\\mathrm{sign}(z)$ 返回 $+1$；如果 $z  0$，返回 $-1$；如果 $z = 0$，返回 $0$。\n- 感知机学习规则仅在出错时更新当前权重向量 $w$：当 $y_i \\, w^\\top x_i \\le 0$ 时，设置 $w \\leftarrow w + y_i x_i$。\n- 对于一个数据集 $D = \\{(x_i, y_i)\\}_{i=1}^n$，定义数据集半径 $R$ 为 $R = \\max_i \\|x_i\\|_2$，其中 $\\|\\cdot\\|_2$ 表示欧几里得范数。\n- 给定任意一个能正确分类所有点的固定参考分离向量 $w^\\ast \\in \\mathbb{R}^d$，定义数据集关于 $w^\\ast$ 的归一化几何间隔 $\\gamma$ 为 $\\gamma = \\min_i \\dfrac{y_i \\, w^{\\ast\\top} x_i}{\\|w^\\ast\\|_2}$。\n\n您的任务：\n1. 对于每个数据集，初始化 $w_0 = 0$ 并按循环顺序对样本运行感知机学习规则，直到算法完成一次没有任何错误的全遍历。为确保在出现编程错误时能够终止，将训练上限设为 $T = 1000$ 个轮次（epoch）；对于所提供的测试用例，算法将远在此上限之前收敛。\n2. 统计训练期间所犯错误的总数 $M$（这等于所应用的更新次数）。\n3. 按上述定义计算 $R$ 和 $\\gamma$，然后计算量 $B = \\left(\\dfrac{R}{\\gamma}\\right)^2$。\n4. 对于每个数据集，生成列表 $[M, B, M \\le B]$，其中 $M$ 是一个整数，$B$ 是一个浮点数，$M \\le B$ 是一个布尔值，表示实证错误数是否不超过计算出的量。聚合所有测试用例的列表，并按下面指定的精确格式将其打印为单行。\n\n不涉及角度单位。此问题中没有物理单位。\n\n测试套件：\n为以下三个数据集提供结果，每个数据集都有其相关的标签和参考 $w^\\ast$。\n\n- 测试用例 1（二维，良好分离）：\n  - 输入 $X = [\\,(2, 2),\\; (2, 0),\\; (0, 2),\\; (-2, -1),\\; (-1, -2),\\; (-2, -2)\\,]$。\n  - 标签 $Y = [\\,+1,\\; +1,\\; +1,\\; -1,\\; -1,\\; -1\\,]$。\n  - 参考向量 $w^\\ast = (1, 1)$。\n\n- 测试用例 2（二维，小间隔）：\n  - 输入 $X = [\\,(1, 0.05),\\; (1, -0.05),\\; (1, 0.0),\\; (-1, 0.05),\\; (-1, -0.05),\\; (-1, 0.0)\\,]$。\n  - 标签 $Y = [\\,+1,\\; +1,\\; +1,\\; -1,\\; -1,\\; -1\\,]$。\n  - 参考向量 $w^\\ast = (1, 0.1)$。\n\n- 测试用例 3（二维，对称性，类边界行为）：\n  - 输入 $X = [\\,(1, 0),\\; (0, 1),\\; (-1, 0),\\; (0, -1)\\,]$。\n  - 标签 $Y = [\\,+1,\\; +1,\\; -1,\\; -1\\,]$。\n  - 参考向量 $w^\\ast = (1, 1)$。\n\n最终输出格式：\n您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表。每个测试用例必须贡献一个列表 $[M, B, M \\le B]$。例如，最终打印的行应类似于\n$[ [M_1,B_1,\\text{True}], [M_2,B_2,\\text{True}], [M_3,B_3,\\text{False}] ]$\n但使用实际计算出的值，并且输出中没有空格，从而得到一个形如\n$[[M_1,B_1,True],[M_2,B_2,True],[M_3,B_3,False]]$\n的字符串。",
            "solution": "该问题要求对感知机学习算法在三个合成的、线性可分的数据集上的性能进行实证研究。任务的核心是实现该算法，统计其在训练过程中的错误次数，并将这个实证计数与一个从数据集几何特性推导出的理论上界进行比较。此分析基于著名的感知机错误界限定理（Perceptron Mistake Bound theorem）。\n\n每个测试用例的解决过程包括以下步骤：\n1.  **模拟感知机算法**：通过运行感知机学习算法来确定错误次数 $M$。\n2.  **计算几何量**：根据提供的数据和参考分离向量 $w^\\ast$，计算数据集半径 $R$ 和归一化几何间隔 $\\gamma$。\n3.  **计算界限**：理论上界量 $B$ 按 $B = (R/\\gamma)^2$ 计算。\n4.  **验证不等式**：将实证错误计数 $M$ 与计算出的界限量 $B$ 进行比较，以验证不等式 $M \\le B$。\n\n**步骤 1：感知机算法模拟**\n该算法以权重向量 $w_0 = 0 \\in \\mathbb{R}^d$ 初始化，其中 $d$ 是输入空间的维度。然后，算法以循环方式遍历训练样本 $(x_i, y_i)$。一个轮次（epoch）是对数据集中所有样本的完整遍历。\n\n对于每个样本 $(x_i, y_i)$，都会进行一次预测。如果当前分类器 $w$ 错误地分类了该点，即满足条件 $y_i(w^\\top x_i) \\le 0$，则发生一次错误。包含等式情况 $w^\\top x_i = 0$ 是至关重要的，因为位于决策边界上的点没有被正确地分类为 $+1$ 或 $-1$ 类。\n\n在发生错误时，权重向量根据感知机学习规则进行更新：\n$$ w \\leftarrow w + y_i x_i $$\n总错误数 $M$ 是整个训练过程中这些更新的累积计数。\n\n训练逐轮进行，直到完成一个没有任何错误的完整轮次。此时，算法找到了一个能够正确分类所有训练样本的权重向量 $w$，算法终止。该问题提供了一个 $T = 1000$ 个最大轮次的安全保障，尽管对于线性可分的数据，收敛是有保证的。\n\n**步骤 2：几何量的计算**\n需要数据集的两个关键几何属性。\n\n首先，数据集半径 $R$ 定义为数据集中任何输入向量的最大欧几里得范数：\n$$ R = \\max_{i} \\|x_i\\|_2 $$\n该值表示任何数据点到原点的最大距离。\n\n其次，归一化几何间隔 $\\gamma$ 是相对于一个给定的、固定的分离向量 $w^\\ast$ 定义的，该向量能正确分类所有数据点（即对所有 $i$ 都有 $y_i (w^{\\ast\\top} x_i)  0$）。间隔 $\\gamma$ 是任何点到由 $w^\\ast$ 定义的超平面的最小归一化有符号距离：\n$$ \\gamma = \\min_{i} \\frac{y_i (w^{\\ast\\top} x_i)}{\\|w^\\ast\\|_2} $$\n项 $y_i(w^{\\ast\\top} x_i)$ 对所有点都是正的，除以 $\\|w^\\ast\\|_2$ 将“函数间隔”转换为真实的欧几里得距离，使得 $\\gamma$ 成为数据点到分离超平面的最小几何距离。\n\n**步骤 3：计算界限量**\n感知机错误界限定理指出，对于任何以间隔 $\\gamma_{opt}$（在所有分离超平面上最大化）线性可分的数据集，感知机算法（从 $w_0=0$ 开始）所犯的错误数 $M$ 受 $M \\le (R/\\gamma_{opt})^2$ 的限制。该界限对于相对于*任何*分离向量 $w^\\ast$（而不仅是最优向量）定义的间隔 $\\gamma$ 也成立。该问题要求我们为每个测试用例提供的特定 $w^\\ast$ 计算并验证此界限。因此，我们计算量 $B$：\n$$ B = \\left(\\frac{R}{\\gamma}\\right)^2 $$\n\n**步骤 4：整合最终结果**\n对于三个测试用例中的每一个，我们执行上述计算以获得整数错误计数 $M$ 和浮点数界限 $B$。然后，我们形成一个包含这两个值以及一个表示错误界限是否成立的布尔值的列表：$[M, B, M \\le B]$。最终输出是所有测试用例这些列表的聚合。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the perceptron learning algorithm and computes the mistake bound quantity for three test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            # Test case 1 (two-dimensional, well-separated)\n            \"X\": np.array([[2., 2.], [2., 0.], [0., 2.], [-2., -1.], [-1., -2.], [-2., -2.]]),\n            \"Y\": np.array([1, 1, 1, -1, -1, -1]),\n            \"w_star\": np.array([1., 1.])\n        },\n        {\n            # Test case 2 (two-dimensional, small margin)\n            \"X\": np.array([[1., 0.05], [1., -0.05], [1., 0.0], [-1., 0.05], [-1., -0.05], [-1., 0.0]]),\n            \"Y\": np.array([1, 1, 1, -1, -1, -1]),\n            \"w_star\": np.array([1., 0.1])\n        },\n        {\n            # Test case 3 (two-dimensional, symmetry, boundary-like behavior)\n            \"X\": np.array([[1., 0.], [0., 1.], [-1., 0.], [0., -1.]]),\n            \"Y\": np.array([1, 1, -1, -1]),\n            \"w_star\": np.array([1., 1.])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X = case[\"X\"]\n        Y = case[\"Y\"]\n        w_star = case[\"w_star\"]\n        n_samples, n_features = X.shape\n\n        # Task 1  2: Run perceptron and count mistakes M\n        w = np.zeros(n_features)\n        M = 0\n        max_epochs = 1000\n\n        for _ in range(max_epochs):\n            mistakes_in_epoch = 0\n            for i in range(n_samples):\n                # Check for a mistake: y_i * (w^T * x_i) = 0\n                if Y[i] * np.dot(w, X[i]) = 0:\n                    # Apply the update rule: w - w + y_i * x_i\n                    w = w + Y[i] * X[i]\n                    M += 1\n                    mistakes_in_epoch += 1\n            \n            # If a full pass is completed with no mistakes, convergence is reached\n            if mistakes_in_epoch == 0:\n                break\n        \n        # Task 3: Compute R and gamma\n        # Compute R = max_i ||x_i||_2\n        norms = np.linalg.norm(X, axis=1)\n        R = np.max(norms)\n\n        # Compute gamma = min_i (y_i * w_star^T * x_i) / ||w_star||_2\n        numerator = Y * np.dot(X, w_star)\n        denominator = np.linalg.norm(w_star)\n        gamma = np.min(numerator) / denominator\n\n        # Compute B = (R / gamma)^2\n        B = (R / gamma)**2\n        \n        # Task 4: Aggregate the results\n        result_list = [M, B, M = B]\n        results.append(result_list)\n\n    # Final print statement in the exact required format with no spaces.\n    # Manually build the string to avoid spaces introduced by str(list).\n    output_str = \"[\"\n    for i, res in enumerate(results):\n        # res[2] is a boolean; str(res[2]) produces 'True' or 'False'.\n        output_str += f\"[{res[0]},{res[1]},{str(res[2])}]\"\n        if i  len(results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```"
        }
    ]
}