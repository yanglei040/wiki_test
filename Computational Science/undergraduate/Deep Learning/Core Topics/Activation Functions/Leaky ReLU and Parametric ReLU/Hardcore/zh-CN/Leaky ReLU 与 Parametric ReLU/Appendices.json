{
    "hands_on_practices": [
        {
            "introduction": "我们首先来探究参数化修正线性单元 (PReLU) 的核心机制：它的参数 $\\alpha$ 是如何被学习的。这个练习将引导你使用链式法则推导出 $\\alpha$ 的梯度，这是理解和创建自定义神经网络组件的一项基本技能 。通过这个过程，你将看到反向传播如何扩展到权重和偏置之外的参数。",
            "id": "3101068",
            "problem": "考虑一个前馈网络中的单个神经元，其激活函数为参数化修正线性单元（PReLU），形式上，该参数化激活函数 $f_{\\alpha}(z)$ 定义为\n$$\nf_{\\alpha}(z) \\;=\\; \\max(0, z) \\;+\\; \\alpha\\,\\min(0, z).\n$$\n设该神经元的预激活为 $z = \\mathbf{w}^{\\top}\\mathbf{x} + b$，其输出为 $y = f_{\\alpha}(z)$。对于一个标量目标 $t$，将训练目标定义为一个数据拟合项与一个关于 $\\alpha$ 的 $\\ell_{2}$ 正则化项之和：\n$$\nL \\;=\\; \\frac{1}{2}\\,\\big(y - t\\big)^{2} \\;+\\; \\frac{\\lambda}{2}\\,\\alpha^{2}.\n$$\n从微积分的链式法则以及 $\\max(\\cdot,\\cdot)$ 和 $\\min(\\cdot,\\cdot)$ 的定义出发，推导 $\\frac{\\partial L}{\\partial \\alpha}$ 关于 $z$、$y$、$t$、$\\alpha$ 和 $\\lambda$ 的显式表达式。然后，使用具体值 $\\mathbf{x} = \\begin{pmatrix} 1.2 \\\\ -0.5 \\\\ 0.3 \\end{pmatrix}$，$\\mathbf{w} = \\begin{pmatrix} -0.8 \\\\ 0.4 \\\\ 0.6 \\end{pmatrix}$，$b = 0.1$，$\\alpha = 0.25$，$t = -0.3$ 和 $\\lambda = 0.1$，计算 $\\frac{\\partial L}{\\partial \\alpha}$ 的数值。\n\n最后，根据你推导的表达式，用一两句话简要解释在梯度下降过程中，对 $\\alpha$ 的正则化如何影响梯度的方向和大小。\n\n将你计算出的 $\\frac{\\partial L}{\\partial \\alpha}$ 的最终数值结果四舍五入到 3 位有效数字。无需单位。",
            "solution": "首先对问题陈述进行严格的验证过程。\n\n### 步骤 1：提取已知条件\n提供了以下数据和定义：\n-   参数化修正线性单元（PReLU）激活函数：$f_{\\alpha}(z) = \\max(0, z) + \\alpha\\,\\min(0, z)$。\n-   神经元预激活：$z = \\mathbf{w}^{\\top}\\mathbf{x} + b$。\n-   神经元输出：$y = f_{\\alpha}(z)$。\n-   损失函数（目标）：$L = \\frac{1}{2}\\,(y - t)^{2} + \\frac{\\lambda}{2}\\,\\alpha^{2}$。\n-   输入向量：$\\mathbf{x} = \\begin{pmatrix} 1.2 \\\\ -0.5 \\\\ 0.3 \\end{pmatrix}$。\n-   权重向量：$\\mathbf{w} = \\begin{pmatrix} -0.8 \\\\ 0.4 \\\\ 0.6 \\end{pmatrix}$。\n-   偏置：$b = 0.1$。\n-   PReLU 参数：$\\alpha = 0.25$。\n-   目标值：$t = -0.3$。\n-   正则化超参数：$\\lambda = 0.1$。\n-   任务：\n    1.  推导 $\\frac{\\partial L}{\\partial \\alpha}$ 关于 $z, y, t, \\alpha, \\lambda$ 的显式表达式。\n    2.  根据给定值计算 $\\frac{\\partial L}{\\partial \\alpha}$ 的数值，并四舍五入到 3 位有效数字。\n    3.  解释正则化项对梯度的影响。\n\n### 步骤 2：使用提取的已知条件进行验证\n-   **科学上成立**：该问题基于机器学习和计算神经科学中标准的、公认的概念，包括神经网络激活函数（PReLU）、带 $\\ell_2$ 正则化的损失函数以及基于梯度的优化（反向传播）。其数学框架是合理的。\n-   **适定性**：该问题是适定的（或称良构的）。所涉及的函数几乎处处连续且可微。提供的数值可以得到唯一且稳定的解。所有必要信息均已提供，且无矛盾之处。对于将要遇到的情况（$z \\neq 0$），函数 $f_{\\alpha}(z)$ 及其导数是良定义的。\n-   **客观性**：问题采用精确的数学语言陈述，不含主观或模糊的术语。\n\n### 步骤 3：结论与行动\n该问题是有效的，因为它在科学上成立、适定、客观，并且包含一个完整且一致的设定。因此，我将继续进行完整解答。\n\n### 第 1 部分：梯度表达式的推导\n\n目标是求损失函数 $L$ 关于参数 $\\alpha$ 的偏导数，记为 $\\frac{\\partial L}{\\partial \\alpha}$。损失函数由下式给出：\n$$\nL(\\alpha, y) \\;=\\; \\frac{1}{2}\\,(y - t)^{2} \\;+\\; \\frac{\\lambda}{2}\\,\\alpha^{2}\n$$\n神经元的输出 $y$ 是 $\\alpha$ 和 $z$ 的函数，其中 $y = f_{\\alpha}(z)$。预激活 $z = \\mathbf{w}^{\\top}\\mathbf{x} + b$ 不依赖于 $\\alpha$。\n我们应用偏导的链式法则。由于 $L$ 既通过正则化项直接依赖于 $\\alpha$，也通过 $y$ 间接依赖于 $\\alpha$，我们有：\n$$\n\\frac{\\partial L}{\\partial \\alpha} \\;=\\; \\frac{\\partial L}{\\partial y}\\,\\frac{\\partial y}{\\partial \\alpha} \\;+\\; \\frac{\\partial}{\\partial \\alpha}\\left( \\frac{\\lambda}{2}\\,\\alpha^{2} \\right)\n$$\n我们分别计算每一项。\n首先，损失函数关于输出 $y$ 的导数：\n$$\n\\frac{\\partial L}{\\partial y} \\;=\\; \\frac{\\partial}{\\partial y}\\left( \\frac{1}{2}\\,(y - t)^{2} \\right) \\;=\\; y - t\n$$\n其次，正则化项关于 $\\alpha$ 的导数：\n$$\n\\frac{\\partial}{\\partial \\alpha}\\left( \\frac{\\lambda}{2}\\,\\alpha^{2} \\right) \\;=\\; \\lambda\\alpha\n$$\n再次，神经元输出 $y$ 关于 $\\alpha$ 的导数。输出为 $y = f_{\\alpha}(z) = \\max(0, z) + \\alpha\\,\\min(0, z)$。由于 $z$ 不是 $\\alpha$ 的函数，在求此偏导数时我们将其视为常数：\n$$\n\\frac{\\partial y}{\\partial \\alpha} \\;=\\; \\frac{\\partial}{\\partial \\alpha}\\left( \\max(0, z) + \\alpha\\,\\min(0, z) \\right) \\;=\\; 0 + 1 \\cdot \\min(0, z) \\;=\\; \\min(0, z)\n$$\n我们可以通过考察 $f_\\alpha(z)$ 的分段定义来验证这一点。\n如果 $z > 0$，则 $y = z$，所以 $\\frac{\\partial y}{\\partial \\alpha} = 0$。在这种情况下，$\\min(0, z) = 0$。\n如果 $z \\le 0$，则 $y = \\alpha z$，所以 $\\frac{\\partial y}{\\partial \\alpha} = z$。在这种情况下，$\\min(0, z) = z$。\n因此，表达式 $\\frac{\\partial y}{\\partial \\alpha} = \\min(0, z)$ 是正确的。\n\n将这些分量代回链式法则的表达式中：\n$$\n\\frac{\\partial L}{\\partial \\alpha} \\;=\\; (y - t)\\,\\min(0, z) \\;+\\; \\lambda\\alpha\n$$\n这就是用 $z, y, t, \\alpha$ 和 $\\lambda$ 表示的梯度的显式表达式。\n\n### 第 2 部分：数值计算\n\n现在我们使用所提供的数据计算 $\\frac{\\partial L}{\\partial \\alpha}$ 的数值。\n-   $\\mathbf{x} = \\begin{pmatrix} 1.2 \\\\ -0.5 \\\\ 0.3 \\end{pmatrix}$，$\\mathbf{w} = \\begin{pmatrix} -0.8 \\\\ 0.4 \\\\ 0.6 \\end{pmatrix}$，$b = 0.1$。\n-   $\\alpha = 0.25$，$t = -0.3$，$\\lambda = 0.1$。\n\n首先，我们计算预激活 $z$：\n$$\nz \\;=\\; \\mathbf{w}^{\\top}\\mathbf{x} + b \\;=\\; (-0.8)(1.2) + (0.4)(-0.5) + (0.6)(0.3) + 0.1\n$$\n$$\nz \\;=\\; -0.96 - 0.20 + 0.18 + 0.1 \\;=\\; -1.16 + 0.28 \\;=\\; -0.88\n$$\n由于 $z = -0.88  0$，该神经元在“泄漏”区工作。现在我们计算输出 $y$：\n$$\ny \\;=\\; f_{\\alpha}(z) \\;=\\; \\max(0, -0.88) + (0.25)\\,\\min(0, -0.88)\n$$\n$$\ny \\;=\\; 0 + (0.25)(-0.88) \\;=\\; -0.22\n$$\n现在我们有了计算 $\\frac{\\partial L}{\\partial \\alpha}$ 所需的所有分量：\n-   $y - t = -0.22 - (-0.3) = 0.08$\n-   $\\min(0, z) = \\min(0, -0.88) = -0.88$\n-   $\\lambda\\alpha = (0.1)(0.25) = 0.025$\n\n将这些值代入推导出的梯度表达式中：\n$$\n\\frac{\\partial L}{\\partial \\alpha} \\;=\\; (0.08)(-0.88) + 0.025\n$$\n$$\n\\frac{\\partial L}{\\partial \\alpha} \\;=\\; -0.0704 + 0.025 \\;=\\; -0.0454\n$$\n问题要求答案四舍五入到 3 位有效数字。计算出的值 $-0.0454$ 已经恰好有三位有效数字（即数字 $4$、$5$ 和 $4$）。\n\n### 第 3 部分：正则化的影响\n\n推导出的梯度为 $\\frac{\\partial L}{\\partial \\alpha} = (y - t)\\,\\min(0, z) + \\lambda\\alpha$。其中 $\\lambda\\alpha$ 项是来自 $\\ell_2$ 正则化惩罚项的直接贡献。在梯度下降的更新步骤中，$\\alpha$ 将按 $\\alpha_{new} = \\alpha_{old} - \\eta \\frac{\\partial L}{\\partial \\alpha}$ 进行更新，这意味着正则化项 $\\lambda\\alpha$ 总是为梯度贡献一个分量，其作用是将 $\\alpha$ 的值推向零，而与数据拟合项无关。这种效应被称为权重衰减，它惩罚较大的 $\\alpha$ 值，有助于控制模型复杂度并防止过拟合。",
            "answer": "$$\n\\boxed{-0.0454}\n$$"
        },
        {
            "introduction": "PReLU 的标准实现通常将参数 $\\alpha$ 约束为非负数。这个实践问题将探究这一设计选择背后的重要原因 。通过研究一个假设 $\\alpha$ 为负值的场景，你将发现非单调性和训练不稳定性等潜在问题，从而更深刻地理解激活函数的设计原则。",
            "id": "3142552",
            "problem": "考虑一个带有单个神经元的标量回归模型。预激活值为 $z = w x + b$，其中 $x \\in \\mathbb{R}$ 是输入，$w \\in \\mathbb{R}$ 是权重，$b \\in \\mathbb{R}$ 是偏置。激活函数是带参数的修正线性单元（Parametric Rectified Linear Unit, PReLU），由分段函数 $f(z)$ 定义，其参数为 $\\alpha \\in \\mathbb{R}$，使得当 $z \\geq 0$ 时 $f(z) = z$，当 $z  0$ 时 $f(z) = \\alpha z$。神经元的输出是 $y = f(z)$。训练目标是平方误差 $L(y) = (y - t)^{2}$，目标值为 $t \\in \\mathbb{R}$。请仅使用链式法则和给定的定义作为出发点。\n\n通过考虑以下特定设置，研究负斜率参数 $\\alpha  0$ 对单调性和梯度行为的影响：\n- 选择 $b = 0$，$w = 1$，$\\alpha = -\\frac{1}{3}$。\n- 选择两个输入 $x_{1} = -2$ 和 $x_{2} = -1$ 以及一个目标值 $t = 1$。\n\n任务：\n1. 仅使用 $f(z)$ 的定义和 $\\alpha$ 的符号，构建一个具体的反例，证明当限于负输入时，$y$作为$x$的函数不满足单调不减性。明确计算并比较 $y$ 在 $x_{1}$ 和 $x_{2}$ 处的值。\n2. 对于输入 $x = x_{1} = -2$，通过链式法则和 $f(z)$ 的分段定义，计算梯度 $\\frac{\\partial L}{\\partial w}$ 和 $\\frac{\\partial L}{\\partial \\alpha}$。\n3. 解释当 $\\alpha  0$ 时，这些梯度的符号与潜在的训练不稳定性有何关系。\n\n将你的最终答案表示为包含两个梯度值 $\\left(\\frac{\\partial L}{\\partial w}, \\frac{\\partial L}{\\partial \\alpha}\\right)$ 的行矩阵。无需四舍五入。",
            "solution": "首先验证问题陈述，以确保其具有科学依据、问题明确且客观。\n\n### 步骤1：提取已知条件\n- **模型**：单神经元回归。\n- **预激活值**：$z = w x + b$，其中 $x, w, b \\in \\mathbb{R}$。\n- **激活函数 (PReLU)**：$y = f(z)$，其中当 $z \\geq 0$ 时 $f(z) = z$，当 $z  0$ 时 $f(z) = \\alpha z$。$\\alpha \\in \\mathbb{R}$。\n- **损失函数**：平方误差 $L(y) = (y - t)^{2}$，目标值为 $t \\in \\mathbb{R}$。\n- **特定设置**：\n    - 权重 $w = 1$。\n    - 偏置 $b = 0$。\n    - PReLU 参数 $\\alpha = -\\frac{1}{3}$。\n    - 输入 $x_{1} = -2$ 和 $x_{2} = -1$。\n    - 目标值 $t = 1$。\n- **任务**：\n    1.  使用 $x_1$ 和 $x_2$ 提供一个反例，证明对于负输入，$y(x)$ 不是单调不减的。\n    2.  计算在 $x = x_1 = -2$ 处的梯度 $\\frac{\\partial L}{\\partial w}$ 和 $\\frac{\\partial L}{\\partial \\alpha}$。\n    3.  解释当 $\\alpha  0$ 时，这些梯度的符号与潜在的训练不稳定性有何关系。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题在深度学习领域具有科学依据，特别涉及激活函数和反向传播。模型、PReLU激活函数和损失函数的定义都是标准的。问题设定明确，为所需的计算和分析提供了所有必要的常数和变量。语言精确客观。该问题不违反任何无效标准。\n\n### 步骤3：结论与行动\n问题有效。将提供完整解答。\n\n### 解答\n\n模型由以下方程定义：\n预激活值：$z = w x + b$\n输出：$y = f(z) = \\begin{cases} z  \\text{若 } z \\geq 0 \\\\ \\alpha z  \\text{若 } z  0 \\end{cases}$\n损失：$L = (y - t)^2$\n\n给定特定参数 $w=1$，$b=0$ 和 $\\alpha = -1/3$。模型简化为：\n$z = x$\n$y = f(x) = \\begin{cases} x  \\text{若 } x \\geq 0 \\\\ -\\frac{1}{3} x  \\text{若 } x  0 \\end{cases}$\n\n#### 1. 单调性的反例\n\n为了证明当限于负输入时，$y$ 作为 $x$ 的函数不是单调不减的，我们必须找到 $x_a$ 和 $x_b$ 使得 $x_a  x_b  0$ 但 $y(x_a) > y(x_b)$。我们使用提供的输入 $x_1 = -2$ 和 $x_2 = -1$。\n我们有 $x_1  x_2$。\n\n- 计算 $y$ 在 $x_1 = -2$ 处的值：\n由于 $x_1 = -2  0$，我们使用 $y$ 的分段函数的第二种情况。\n$y_1 = f(x_1) = \\alpha x_1 = \\left(-\\frac{1}{3}\\right)(-2) = \\frac{2}{3}$。\n\n- 计算 $y$ 在 $x_2 = -1$ 处的值：\n由于 $x_2 = -1  0$，我们再次使用第二种情况。\n$y_2 = f(x_2) = \\alpha x_2 = \\left(-\\frac{1}{3}\\right)(-1) = \\frac{1}{3}$。\n\n- 比较输出：\n我们有 $x_1 = -2  x_2 = -1$，但我们发现 $y_1 = \\frac{2}{3} > y_2 = \\frac{1}{3}$。这违反了非递减函数的条件 $y(x_1) \\leq y(x_2)$。因此，我们用一个具体的反例证明了当 $\\alpha  0$ 时，PReLU激活函数在负数域不是单调的；实际上，它是递减的。\n\n#### 2. 梯度计算\n\n我们需要为输入 $x = x_1 = -2$，$w=1$，$b=0$，$\\alpha = -1/3$ 和 $t=1$ 计算 $\\frac{\\partial L}{\\partial w}$ 和 $\\frac{\\partial L}{\\partial \\alpha}$。\n\n首先，我们计算该输入的中间值：\n- 预激活值：$z = w x + b = (1)(-2) + 0 = -2$。\n- 输出：由于 $z = -2  0$，$y = \\alpha z = \\left(-\\frac{1}{3}\\right)(-2) = \\frac{2}{3}$。\n\n接下来，我们使用链式法则求梯度。所需的偏导数是：\n- $\\frac{\\partial L}{\\partial y} = 2(y - t)$\n- $\\frac{\\partial y}{\\partial z} = f'(z) = \\begin{cases} 1  \\text{若 } z > 0 \\\\ \\alpha  \\text{若 } z  0 \\end{cases}$\n- $\\frac{\\partial z}{\\partial w} = x$\n- $\\frac{\\partial y}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha}(\\alpha z) = z$ (因为 $z  0$)\n\n现在我们在给定点计算这些偏导数的值：\n- $\\frac{\\partial L}{\\partial y} = 2\\left(\\frac{2}{3} - 1\\right) = 2\\left(-\\frac{1}{3}\\right) = -\\frac{2}{3}$。\n- 由于 $z = -2  0$，$\\frac{\\partial y}{\\partial z} = \\alpha = -\\frac{1}{3}$。\n- $\\frac{\\partial z}{\\partial w} = x = -2$。\n- 由于 $z = -2  0$，$\\frac{\\partial y}{\\partial \\alpha} = z = -2$。\n\n现在我们使用链式法则将它们组合起来，求得最终梯度。\n\n- **关于 $w$ 的梯度**：\n$$ \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial w} $$\n$$ \\frac{\\partial L}{\\partial w} = \\left(-\\frac{2}{3}\\right) \\left(-\\frac{1}{3}\\right) (-2) = \\left(\\frac{2}{9}\\right) (-2) = -\\frac{4}{9} $$\n\n- **关于 $\\alpha$ 的梯度**：\n$$ \\frac{\\partial L}{\\partial \\alpha} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial \\alpha} $$\n$$ \\frac{\\partial L}{\\partial \\alpha} = \\left(-\\frac{2}{3}\\right) (-2) = \\frac{4}{3} $$\n\n所求梯度为 $\\frac{\\partial L}{\\partial w} = -\\frac{4}{9}$ 和 $\\frac{\\partial L}{\\partial \\alpha} = \\frac{4}{3}$。\n\n#### 3. 与训练不稳定性的关系\n\n梯度 $\\frac{\\partial L}{\\partial w}  0$ 和 $\\frac{\\partial L}{\\partial \\alpha} > 0$ 的符号揭示了当 $\\alpha  0$ 时训练中可能出现的问题。\n\n1.  **梯度符号翻转**：损失关于预激活值的梯度是 $\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial z} = \\frac{\\partial L}{\\partial y} \\alpha$。在我们的例子中，输出 $y = 2/3$ 小于目标值 $t=1$，因此输出误差梯度 $\\frac{\\partial L}{\\partial y} = -2/3$ 是负数。因为 $\\alpha = -1/3$ 也是负数，所以反向传播的梯度 $\\frac{\\partial L}{\\partial z} = (-2/3)(-1/3) = 2/9$ 是正数。这意味着对于负输入，误差信号在反向通过激活函数时符号被翻转了。虽然这在数学上是正确的，但这种行为并不常规。一个标准的 Leaky ReLU（$\\alpha > 0$）会保留误差梯度的符号。预激活值的误差信号的这种符号翻转直接导致了我们权重梯度的符号：$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial z} x = (2/9)(-2) = -4/9$。一个正的 $\\alpha$ 会导致一个正的权重梯度。权重梯度的符号对 $\\alpha$ 符号的这种依赖性可能导致训练过程中复杂且可能不稳定的动态。\n\n2.  **非单调性引起的冲突更新**：$\\alpha  0$ 的核心问题是激活函数的非单调性，如任务1所示。考虑使用一批包含多个负输入的数据进行训练。由于函数在 $x  0$ 时是递减的，某些数据点可能需要增加 $\\alpha$ 来减少其损失，而其他数据点则可能需要减少 $\\alpha$。例如，如果我们有一个点 $(x_a, t_a)$，其输出 $y_a = \\alpha x_a$ 太高，而另一个点 $(x_b, t_b)$ 的输出 $y_b = \\alpha x_b$ 太低，那么梯度 $\\frac{\\partial L_a}{\\partial \\alpha}$ 和 $\\frac{\\partial L_b}{\\partial \\alpha}$ 可能会有相反的符号。这些针对共享参数 $\\alpha$ 的冲突更新信号可能导致训练发生振荡或停滞，这是一种训练不稳定的形式。对于我们单个数据点，梯度 $\\frac{\\partial L}{\\partial w}$ 和 $\\frac{\\partial L}{\\partial \\alpha}$ 的相反符号正是由非单調激活函数所产生的不寻常、耦合的优化曲面的一种症状。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{4}{9}  \\frac{4}{3}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在我们理论理解的基础上，最后一个练习将转向实际代码实现。你将编写代码，使用一种称为重参数化的常见技术来强制施加 $\\alpha \\ge 0$ 的约束 。这项动手编码挑战将通过对比无约束和正确约束的 PReLU，展示理论原则对实验结果的直接影响，从而巩固你的理解。",
            "id": "3142517",
            "problem": "考虑在深度学习中使用的分段线性激活函数族：带泄露修正线性单元 (leaky ReLU) 和参数化修正线性单元 (PReLU)。应用于标量预激活值 $z$ 的单个 PReLU 单元由函数 $f_{\\alpha}(z)$ 定义，其中 $f_{\\alpha}(z) = \\max(0,z) + \\alpha \\min(0,z)$，标量参数 $\\alpha \\in \\mathbb{R}$。当 $\\alpha$ 是一个固定常数时，该函数恢复为带泄露修正线性单元；而 PReLU 则是从数据中学习 $\\alpha$。\n\n根据分段线性函数的核心定义和导数的定义，如果一个函数 $g(z)$ 的导数 $g'(z)$ 对所有 $z$ 都大于或等于 0，则该函数关于其参数 $z$ 是单调不减的。对于 $f_{\\alpha}(z)$，每个单元（作为其输入 $z$ 的函数）的单调性要求对 $\\alpha$ 施加一个约束。\n\n您的任务是：\n- 提出并实现一种参数化方法，通过在整个训练过程中确保 $\\alpha \\ge 0$ 来强制实现每个单元的单调性（例如，通过适当的重参数化）。\n- 展示当 $\\alpha$ 不受约束并可能变为负值时的违规情况，并量化其经验影响。\n\n使用以下基本基础：\n- 均方误差 (MSE) 损失：对于预测值 $\\hat{y}_i$ 和目标值 $y_i$，平均损失为 $L = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2$。\n- 梯度下降更新：对于参数 $\\theta$，更新规则为 $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} L$，其中 $\\eta > 0$ 是学习率。\n- 求导的链式法则：如果 $\\alpha = h(\\beta)$，则 $\\nabla_{\\beta} L = \\nabla_{\\alpha} L \\cdot h'(\\beta)$。\n\n需要实现的实验设置：\n- 构建一个包含 $N = 2001$ 个在区间 $[-2,2]$ 上均匀间隔的标量 $z_i$ 的合成数据集。\n- 将目标定义为标准修正线性单元的输出 $y_i = \\max(0,z_i)$。\n- 使用一个单单元模型，其输出为 $\\hat{y}_i = f_{\\alpha}(z_i)$，其中唯一的可训练参数是 $\\alpha$，并且线性预激活就是 $z$（没有额外的权重或偏置）。\n- 使用普通梯度下降法进行指定轮数的训练，从第一性原理计算精确的解析梯度。\n\n单调性评估和违规计数：\n- 一个单元是单调不减的，当且仅当 $\\alpha \\ge 0$，因为对于 $z > 0$ 时 $f'_{\\alpha}(z) = 1$，而对于 $z  0$ 时 $f'_{\\alpha}(z) = \\alpha$。报告一个指示单调性的布尔值，以及在数据集网格上的严格违规整数计数，该计数定义为局部斜率为负的 $z_i  0$ 点的数量（当 $\\alpha \\ge 0$ 时，此值为 0；当 $\\alpha  0$ 时，此值等于负数 $z_i$ 的数量）。\n\n约束设计：\n- 通过使用非负映射（例如 softplus 函数）对 $\\alpha$ 进行重参数化，来实现一个受约束的变体，使得 $\\alpha = \\log(1 + e^{\\beta})$，其中 $\\beta \\in \\mathbb{R}$ 是一个自由参数。这确保了对于所有的 $\\beta$，都有 $\\alpha \\ge 0$。链式法则所需的导数是 $d\\alpha/d\\beta = \\frac{1}{1 + e^{-\\beta}}$。\n\n测试套件：\n运行以下四个测试用例，每个用例指定为一个元组 $(\\text{mode}, \\text{init}, \\eta, E)$，其中 $\\text{mode}$ 是 \"unconstrained\"（直接优化 $\\alpha$）或 \"softplus\"（使用 $\\alpha=\\log(1+e^{\\beta})$ 优化 $\\beta$），$\\text{init}$ 是初始值（对于 \"unconstrained\" 是 $\\alpha_0$，对于 \"softplus\" 是 $\\beta_0$），$\\eta$ 是学习率，而 $E$ 是训练的轮数：\n1. 无约束，负值初始化：$(\\text{\"unconstrained\"}, \\alpha_0=-0.5, \\eta=0.05, E=10)$。\n2. 无约束，正值初始化：$(\\text{\"unconstrained\"}, \\alpha_0=0.5, \\eta=0.05, E=10)$。\n3. 受约束的 softplus，中度负值初始化：$(\\text{\"softplus\"}, \\beta_0=-1.0, \\eta=0.10, E=10)$。\n4. 受约束的 softplus，接近边界的初始化：$(\\text{\"softplus\"}, \\beta_0=-10.0, \\eta=0.10, E=10)$。\n\n对于每个测试用例，在训练后，计算并返回：\n- $\\alpha$ 的最终值，以浮点数形式表示。\n- 单元的单调性布尔值，定义为 $(\\alpha \\ge 0)$。\n- 在整个数据集上的最终均方误差 $L$，以浮点数形式表示。\n- 在网格上的整数违规计数，如上文所定义。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素对应一个测试用例，并且本身是一个包含 $[\\alpha_{\\text{final}}, \\text{is\\_monotone}, L_{\\text{final}}, \\text{violation\\_count}]$ 的列表。例如，一个有效的输出格式是 `[[0.0,True,0.123,0],[\\dots]]`。",
            "solution": "该问题要求对 PReLU 激活函数 $f_{\\alpha}(z) = \\max(0,z) + \\alpha \\min(0,z)$ 进行分析，特别是关于如何强制实现其单调性。如果一个函数的导数处处非负，则该函数是单调不减的。$f_{\\alpha}(z)$ 关于其输入 $z$ 的导数是分段常数：\n$$\nf'_{\\alpha}(z) = \\frac{d}{dz} f_{\\alpha}(z) =\n\\begin{cases}\n1  \\text{if } z > 0 \\\\\n\\alpha  \\text{if } z  0\n\\end{cases}\n$$\n为了使函数单调不减，其导数在所有定义域内的 $z$ 值上都必须大于或等于 0。这施加了约束 $\\alpha \\ge 0$。任务是为一个旨在学习标准 ReLU 函数的单个 PReLU 单元，实现并比较两种优化策略：一种是对 $\\alpha$ 的无约束优化，另一种是保证 $\\alpha \\ge 0$ 的有约束优化。\n\n实验设置涉及一个包含 $N=2001$ 个在 $[-2, 2]$ 区间内均匀间隔的点 $z_i$ 的合成数据集。目标输出为 $y_i = \\max(0, z_i)$，模型的预测为 $\\hat{y}_i = f_{\\alpha}(z_i)$。目标是找到最小化均方误差 (MSE) 损失函数的参数 $\\alpha$：\n$$\nL(\\alpha) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n$$\n代入 $y_i$ 和 $\\hat{y}_i$ 的表达式：\n$$\ny_i - \\hat{y}_i = \\max(0, z_i) - (\\max(0, z_i) + \\alpha \\min(0, z_i)) = -\\alpha \\min(0, z_i)\n$$\n损失函数简化为：\n$$\nL(\\alpha) = \\frac{1}{N} \\sum_{i=1}^{N} (-\\alpha \\min(0, z_i))^2 = \\frac{\\alpha^2}{N} \\sum_{i=1}^{N} (\\min(0, z_i))^2\n$$\n$(\\min(0, z_i))^2$ 这一项仅当 $z_i  0$ 时非零，此时其值等于 $z_i^2$。设 $z_i  0$ 的索引集合为 $\\mathcal{I}^- = \\{i \\mid z_i  0\\}$。损失可以写成：\n$$\nL(\\alpha) = \\frac{\\alpha^2}{N} \\sum_{i \\in \\mathcal{I}^-} z_i^2\n$$\n这个简化形式表明，损失是关于 $\\alpha$ 的一个简单二次函数，其全局最小值在 $\\alpha=0$ 处。这是符合预期的，因为 $\\alpha=0$ 可以恢复目标函数 $y=\\max(0,z)$。\n\n我们现在将为两种优化模式推导梯度。\n\n**无约束优化**\n在此模式下，我们使用梯度下降直接优化 $\\alpha \\in \\mathbb{R}$。损失函数关于 $\\alpha$ 的梯度是：\n$$\n\\nabla_{\\alpha} L = \\frac{\\partial L}{\\partial \\alpha} = \\frac{d}{d\\alpha} \\left( \\frac{\\alpha^2}{N} \\sum_{i \\in \\mathcal{I}^-} z_i^2 \\right) = \\frac{2\\alpha}{N} \\sum_{i \\in \\mathcal{I}^-} z_i^2\n$$\n每个轮次对 $\\alpha$ 的梯度下降更新规则是：\n$$\n\\alpha \\leftarrow \\alpha - \\eta \\, \\nabla_{\\alpha} L = \\alpha - \\eta \\left( \\frac{2\\alpha}{N} \\sum_{i \\in \\mathcal{I}^-} z_i^2 \\right)\n$$\n其中 $\\eta$ 是学习率。如果初始值 $\\alpha_0$ 为负，$\\alpha$ 将从负侧趋近于 0，从而在训练期间违反单调性条件 $\\alpha \\ge 0$。\n\n**通过 Softplus 重参数化实现的有约束优化**\n为了强制执行约束 $\\alpha \\ge 0$，我们使用一个值域为非负的函数来对 $\\alpha$ 进行重参数化。我们选择 softplus 函数，通过一个新的无约束参数 $\\beta \\in \\mathbb{R}$ 来定义 $\\alpha$：\n$$\n\\alpha(\\beta) = \\log(1 + e^{\\beta})\n$$\n这确保了对于任何实数值 $\\beta$，都有 $\\alpha(\\beta) \\ge 0$。我们现在对 $\\beta$ 执行梯度下降。梯度 $\\nabla_{\\beta} L$ 可通过链式法则求得：\n$$\n\\nabla_{\\beta} L = \\frac{\\partial L}{\\partial \\beta} = \\frac{\\partial L}{\\partial \\alpha} \\frac{d\\alpha}{d\\beta}\n$$\n我们已经计算了 $\\frac{\\partial L}{\\partial \\alpha}$。$\\alpha$ 关于 $\\beta$ 的导数是：\n$$\n\\frac{d\\alpha}{d\\beta} = \\frac{d}{d\\beta} \\log(1 + e^{\\beta}) = \\frac{e^{\\beta}}{1 + e^{\\beta}} = \\frac{1}{1 + e^{-\\beta}}\n$$\n这就是 logistic sigmoid 函数，通常表示为 $\\sigma(\\beta)$。\n结合这些部分，关于 $\\beta$ 的梯度是：\n$$\n\\nabla_{\\beta} L = \\left( \\frac{2\\alpha(\\beta)}{N} \\sum_{i \\in \\mathcal{I}^-} z_i^2 \\right) \\cdot \\sigma(\\beta)\n$$\n$\\beta$ 的梯度下降更新规则是：\n$$\n\\beta \\leftarrow \\beta - \\eta \\, \\nabla_{\\beta} L\n$$\n在此方案中，不论 $\\beta$ 的值如何，得到的参数 $\\alpha$ 将始终为非负，从而在整个训练过程中保持激活函数的单调性。优化过程将驱动 $\\beta$ 趋向于 $-\\infty$，进而驱动 $\\alpha$ 趋向其最优值 0。\n\n**评估指标**\n对于每个测试用例，在指定的训练轮数之后，我们计算四个值：\n1.  **最终 $\\alpha$**：PReLU 参数的值。对于 \"softplus\" 模式，这是从最终的 $\\beta$ 计算得出的。\n2.  **单调性 `is_monotone`**：一个布尔值，如果 $\\alpha_{\\text{final}} \\ge 0$ 则为 `True`，否则为 `False`。\n3.  **最终 MSE 损失 $L_{\\text{final}}$**：用最终的 $\\alpha$ 计算出的损失。\n4.  **违规计数**：数据集中局部斜率 $f'_{\\alpha}(z_i)$ 为负的 $z_i  0$ 的点的数量。如果 $\\alpha  0$，此计数等于负数 $z_i$ 点的数量；否则为 0。对于给定的数据集，有 1000 个点的 $z_i  0$。\n\n通过运行指定的测试用例，我们可以经验性地观察到无约束方法带来的后果（可能出现非单调性），以及 softplus 重参数化对单调性的保证强制执行。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates unconstrained and constrained optimization\n    for a single PReLU unit learning a ReLU function.\n    \"\"\"\n    # Define dataset parameters\n    N = 2001\n    z_min, z_max = -2.0, 2.0\n    z = np.linspace(z_min, z_max, N, dtype=np.float64)\n\n    # Define target output (standard ReLU)\n    y = np.maximum(0, z)\n\n    # Pre-calculate components that are constant throughout training\n    z_neg = z[z  0]\n    sum_z_neg_sq = np.sum(z_neg**2)\n    num_z_neg = len(z_neg)\n\n    # Test suite: (mode, initial_value, learning_rate, epochs)\n    test_cases = [\n        (\"unconstrained\", -0.5, 0.05, 10),\n        (\"unconstrained\", 0.5, 0.05, 10),\n        (\"softplus\", -1.0, 0.10, 10),\n        (\"softplus\", -10.0, 0.10, 10),\n    ]\n\n    results = []\n\n    for mode, init_val, eta, epochs in test_cases:\n        if mode == \"unconstrained\":\n            # Directly optimize alpha\n            alpha = float(init_val)\n            \n            for _ in range(epochs):\n                # Gradient of Loss w.r.t. alpha\n                # L(alpha) = (alpha^2 / N) * sum(z_i^2 for z_i  0)\n                # dL/d_alpha = (2 * alpha / N) * sum(z_i^2 for z_i  0)\n                grad_alpha = (2.0 * alpha / N) * sum_z_neg_sq\n                \n                # Gradient descent update\n                alpha -= eta * grad_alpha\n                \n            final_alpha = alpha\n\n        elif mode == \"softplus\":\n            # Optimize beta, where alpha = log(1 + exp(beta))\n            beta = float(init_val)\n\n            for _ in range(epochs):\n                # Calculate alpha from beta using a numerically stable method\n                # np.logaddexp(0, x) computes log(exp(0) + exp(x)) = log(1 + exp(x))\n                alpha = np.logaddexp(0, beta)\n                \n                # Derivative of alpha w.r.t. beta (sigmoid function)\n                # d_alpha/d_beta = exp(beta)/(1+exp(beta)) = 1/(1+exp(-beta))\n                # This is numerically stable.\n                d_alpha_d_beta = 1.0 / (1.0 + np.exp(-beta))\n\n                # Gradient of Loss w.r.t. alpha\n                grad_alpha = (2.0 * alpha / N) * sum_z_neg_sq\n                \n                # Gradient of Loss w.r.t. beta (using chain rule)\n                grad_beta = grad_alpha * d_alpha_d_beta\n                \n                # Gradient descent update\n                beta -= eta * grad_beta\n            \n            final_alpha = np.logaddexp(0, beta)\n\n        # Calculate final metrics after training\n        is_monotone = (final_alpha >= 0.0)\n        \n        # Final loss: L = (alpha^2 / N) * sum_z_neg_sq\n        final_loss = (final_alpha**2 / N) * sum_z_neg_sq\n        \n        # Violation count: number of negative slopes for z  0\n        violation_count = num_z_neg if final_alpha  0.0 else 0\n        \n        results.append([\n            float(final_alpha),\n            bool(is_monotone),\n            float(final_loss),\n            int(violation_count)\n        ])\n\n    # The final print statement must follow the specified format:\n    # A string representation of a list of lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}