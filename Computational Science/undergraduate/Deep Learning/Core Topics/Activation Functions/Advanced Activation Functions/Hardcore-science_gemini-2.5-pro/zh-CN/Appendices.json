{
    "hands_on_practices": [
        {
            "introduction": "激活函数从根本上改变了流经神经网络的信号的统计特性。理解在给定输入分布下，输出分布的均值和方差对于分析信号传播、权重初始化以及批量归一化 (Batch Normalization) 等技术的行为至关重要。该练习  将引导你通过第一性原理，为两种广泛应用的激活函数——ReLU 和 GELU——推导其输出统计量，从而为理解现代架构中激活函数的选择建立坚实的数学基础。",
            "id": "3097811",
            "problem": "设 $X$ 是一个标量预激活值，独立同分布地从标准高斯分布 $X \\sim \\mathcal{N}(0,1)$ 中抽取。考虑两种激活函数：整流线性单元（ReLU），定义为 $\\phi_{\\mathrm{ReLU}}(x) = \\max(0,x)$；以及高斯误差线性单元（GELU），定义为 $\\phi_{\\mathrm{GELU}}(x) = x \\, \\Phi(x)$，其中 $\\Phi(x)$ 是标准正态分布的累积分布函数。仅使用概率论和积分的第一性原理，通过计算每种激活函数下的均值 $E[\\phi(X)]$ 和方差 $\\operatorname{Var}[\\phi(X)]$，来推导其输出分布。然后，对于批量归一化（BN），其定义为 $y = \\gamma \\frac{\\phi(X) - \\mu}{\\sigma} + \\beta$，其中总体统计量为 $\\mu = E[\\phi(X)]$ 和 $\\sigma^{2} = \\operatorname{Var}[\\phi(X)]$，确定在无限批量大小的极限情况下，能够使输出 $y$ 的均值为零、方差为一的仿射参数 $(\\gamma^{\\ast}, \\beta^{\\ast})$。你的推导必须从期望和方差的定义、标准正态概率密度函数和累积分布函数，以及联合高斯随机变量的基本性质出发。请将你的最终答案表示为封闭形式的解析表达式。无需四舍五入。请按顺序 $\\big(E[\\phi_{\\mathrm{ReLU}}(X)], \\operatorname{Var}[\\phi_{\\mathrm{ReLU}}(X)], E[\\phi_{\\mathrm{GELU}}(X)], \\operatorname{Var}[\\phi_{\\mathrm{GELU}}(X)], \\gamma^{\\ast}, \\beta^{\\ast}\\big)$ 提供最终的元组。",
            "solution": "该问题经评估有效，因为它具有科学依据、问题明确、客观，并包含得出唯一解所需的所有信息。我们开始进行推导。\n\n设 $X$ 是一个服从标准正态分布的随机变量，$X \\sim \\mathcal{N}(0,1)$。其概率密度函数（PDF）为 $p(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{x^2}{2})$，其累积分布函数（CDF）为 $\\Phi(x) = \\int_{-\\infty}^{x} p(t) dt$。对于函数 $g(X)$，期望定义为 $E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) p(x) dx$，方差定义为 $\\operatorname{Var}[g(X)] = E[g(X)^2] - (E[g(X)])^2$。\n\n**第1部分：ReLU激活函数的统计特性**\n\n整流线性单元（ReLU）定义为 $\\phi_{\\mathrm{ReLU}}(x) = \\max(0,x)$。\n\n**1.1. ReLU输出的均值：**\n期望 $E[\\phi_{\\mathrm{ReLU}}(X)]$ 计算如下：\n$$E[\\phi_{\\mathrm{ReLU}}(X)] = \\int_{-\\infty}^{\\infty} \\max(0,x) p(x) dx = \\int_{0}^{\\infty} x p(x) dx$$\n$$E[\\phi_{\\mathrm{ReLU}}(X)] = \\int_{0}^{\\infty} x \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx$$\n我们进行换元，令 $u = \\frac{x^2}{2}$，这意味着 $du = x dx$。积分上下限仍为从 $0$ 到 $\\infty$。\n$$E[\\phi_{\\mathrm{ReLU}}(X)] = \\frac{1}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} \\exp(-u) du = \\frac{1}{\\sqrt{2\\pi}} [-\\exp(-u)]_{0}^{\\infty} = \\frac{1}{\\sqrt{2\\pi}} (-0 - (-1)) = \\frac{1}{\\sqrt{2\\pi}}$$\n\n**1.2. ReLU输出的方差：**\n首先，我们计算二阶矩 $E[(\\phi_{\\mathrm{ReLU}}(X))^2]$。\n$$E[(\\phi_{\\mathrm{ReLU}}(X))^2] = \\int_{-\\infty}^{\\infty} (\\max(0,x))^2 p(x) dx = \\int_{0}^{\\infty} x^2 p(x) dx$$\n$$E[(\\phi_{\\mathrm{ReLU}}(X))^2] = \\int_{0}^{\\infty} x^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx$$\n完整积分 $\\int_{-\\infty}^{\\infty} x^2 p(x) dx$ 对应于 $X$ 的二阶矩，即 $E[X^2] = \\operatorname{Var}[X] + (E[X])^2 = 1 + 0^2 = 1$。被积函数 $x^2 p(x)$ 是一个偶函数，所以从 $0$ 到 $\\infty$ 的积分是从 $-\\infty$ 到 $\\infty$ 积分的一半。\n$$E[(\\phi_{\\mathrm{ReLU}}(X))^2] = \\frac{1}{2} \\int_{-\\infty}^{\\infty} x^2 p(x) dx = \\frac{1}{2} E[X^2] = \\frac{1}{2}$$\n那么方差为：\n$$\\operatorname{Var}[\\phi_{\\mathrm{ReLU}}(X)] = E[(\\phi_{\\mathrm{ReLU}}(X))^2] - (E[\\phi_{\\mathrm{ReLU}}(X)])^2 = \\frac{1}{2} - \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^2 = \\frac{1}{2} - \\frac{1}{2\\pi} = \\frac{\\pi - 1}{2\\pi}$$\n\n**第2部分：GELU激活函数的统计特性**\n\n高斯误差线性单元（GELU）定义为 $\\phi_{\\mathrm{GELU}}(x) = x \\Phi(x)$。我们将使用斯坦因引理（Stein's Lemma），这是高斯变量的一个基本性质，它指出对于 $X \\sim \\mathcal{N}(0,1)$ 和一个可微函数 $g$，有 $E[X g(X)] = E[g'(X)]$。\n\n**2.1. GELU输出的均值：**\n我们想计算 $E[\\phi_{\\mathrm{GELU}}(X)] = E[X \\Phi(X)]$。令 $g(x) = \\Phi(x)$。其导数为 $g'(x) = p(x)$。应用斯坦因引理：\n$$E[X \\Phi(X)] = E[p(X)] = \\int_{-\\infty}^{\\infty} p(x) p(x) dx = \\int_{-\\infty}^{\\infty} p(x)^2 dx$$\n$$E[\\phi_{\\mathrm{GELU}}(X)] = \\int_{-\\infty}^{\\infty} \\left(\\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)\\right)^2 dx = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\exp(-x^2) dx$$\n使用高斯积分结果 $\\int_{-\\infty}^{\\infty} \\exp(-ax^2) dx = \\sqrt{\\frac{\\pi}{a}}$，其中 $a=1$：\n$$E[\\phi_{\\mathrm{GELU}}(X)] = \\frac{1}{2\\pi} \\sqrt{\\pi} = \\frac{1}{2\\sqrt{\\pi}}$$\n\n**2.2. GELU输出的方差：**\n我们首先计算二阶矩 $E[(\\phi_{\\mathrm{GELU}}(X))^2] = E[X^2 \\Phi(X)^2]$。我们对 $g(x) = x \\Phi(x)^2$ 应用斯坦因引理。其导数为 $g'(x) = \\frac{d}{dx}(x \\Phi(x)^2) = \\Phi(x)^2 + x(2\\Phi(x)p(x)) = \\Phi(x)^2 + 2x\\Phi(x)p(x)$。\n$$E[X^2 \\Phi(X)^2] = E[g'(X)] = E[\\Phi(X)^2 + 2X\\Phi(X)p(X)] = E[\\Phi(X)^2] + 2E[X\\Phi(X)p(X)]$$\n我们必须计算两个期望：\n\n(a) $E[\\Phi(X)^2] = \\int_{-\\infty}^{\\infty} \\Phi(x)^2 p(x) dx$。这可以解释为 $P(\\max(Y,Z) \\le X)$，其中 $Y, Z, X$ 是独立同分布的 $\\mathcal{N}(0,1)$。我们使用分部积分法来求解。令 $u = \\Phi(x)^2$ 和 $dv = p(x) dx$。则 $du = 2\\Phi(x)p(x) dx$ 且 $v = \\Phi(x)$。\n$$J = \\int_{-\\infty}^{\\infty} \\Phi(x)^2 p(x) dx = [\\Phi(x)^3]_{-\\infty}^{\\infty} - \\int_{-\\infty}^{\\infty} \\Phi(x) (2\\Phi(x)p(x)) dx = (1^3 - 0^3) - 2J$$\n$$J = 1 - 2J \\implies 3J = 1 \\implies J = E[\\Phi(X)^2] = \\frac{1}{3}$$\n\n(b) $E[X\\Phi(X)p(X)] = \\int_{-\\infty}^{\\infty} x \\Phi(x) p(x)^2 dx$。\n$$p(x)^2 = \\frac{1}{2\\pi}\\exp(-x^2)$$\n所以，$E[X\\Phi(X)p(X)] = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} x \\Phi(x) \\exp(-x^2) dx$。\n分部积分：令 $u=\\Phi(x)$ 和 $dv = x \\exp(-x^2) dx$。则 $du=p(x)dx$ 且 $v = -\\frac{1}{2}\\exp(-x^2)$。\n$$\\int x \\Phi(x) e^{-x^2} dx = \\left[-\\frac{1}{2} e^{-x^2} \\Phi(x)\\right]_{-\\infty}^\\infty - \\int_{-\\infty}^{\\infty} \\left(-\\frac{1}{2}e^{-x^2}\\right) p(x) dx$$\n边界项为 $0$。积分为 $\\frac{1}{2} \\int_{-\\infty}^{\\infty} e^{-x^2} \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} dx = \\frac{1}{2\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-3x^2/2} dx$。\n使用 $\\int_{-\\infty}^{\\infty} e^{-ax^2} dx = \\sqrt{\\pi/a}$，其中 $a=3/2$：\n$$\\frac{1}{2\\sqrt{2\\pi}} \\sqrt{\\frac{2\\pi}{3}} = \\frac{1}{2\\sqrt{3}}$$\n因此，$E[X\\Phi(X)p(X)] = \\frac{1}{2\\pi} \\frac{1}{2\\sqrt{3}} = \\frac{1}{4\\pi\\sqrt{3}}$。\n\n结合这些结果，得到GELU的二阶矩：\n$$E[(\\phi_{\\mathrm{GELU}}(X))^2] = \\frac{1}{3} + 2 \\left( \\frac{1}{4\\pi\\sqrt{3}} \\right) = \\frac{1}{3} + \\frac{1}{2\\pi\\sqrt{3}}$$\nGELU的方差是：\n$$\\operatorname{Var}[\\phi_{\\mathrm{GELU}}(X)] = E[(\\phi_{\\mathrm{GELU}}(X))^2] - (E[\\phi_{\\mathrm{GELU}}(X)])^2 = \\left(\\frac{1}{3} + \\frac{1}{2\\pi\\sqrt{3}}\\right) - \\left(\\frac{1}{2\\sqrt{\\pi}}\\right)^2$$\n$$\\operatorname{Var}[\\phi_{\\mathrm{GELU}}(X)] = \\frac{1}{3} + \\frac{1}{2\\pi\\sqrt{3}} - \\frac{1}{4\\pi} = \\frac{1}{3} + \\frac{\\sqrt{3}}{6\\pi} - \\frac{1}{4\\pi} = \\frac{4\\pi+2\\sqrt{3}-3}{12\\pi}$$\n\n**第3部分：批量归一化参数**\n\n批量归一化变换由 $y = \\gamma \\frac{\\phi(X) - \\mu}{\\sigma} + \\beta$ 给出，其中 $\\mu = E[\\phi(X)]$ 且 $\\sigma^2 = \\operatorname{Var}[\\phi(X)]$。我们需要找到参数 $(\\gamma^{\\ast}, \\beta^{\\ast})$，使得输出 $y$ 的均值为 $E[y]=0$，方差为 $\\operatorname{Var}[y]=1$。\n\n**3.1. y的均值：**\n利用期望算子的线性性质：\n$$E[y] = E\\left[\\gamma \\frac{\\phi(X) - \\mu}{\\sigma} + \\beta\\right] = \\frac{\\gamma}{\\sigma} E[\\phi(X) - \\mu] + E[\\beta] = \\frac{\\gamma}{\\sigma} (E[\\phi(X)] - \\mu) + \\beta$$\n由于 $\\mu=E[\\phi(X)]$，项 $(E[\\phi(X)] - \\mu)$ 为 $0$。\n$$E[y] = \\beta$$\n为使 $E[y]=0$，我们必须有 $\\beta^{\\ast} = 0$。\n\n**3.2. y的方差：**\n利用方差的性质 $\\operatorname{Var}[aZ+b] = a^2 \\operatorname{Var}[Z]$：\n$$\\operatorname{Var}[y] = \\operatorname{Var}\\left[\\gamma \\frac{\\phi(X) - \\mu}{\\sigma} + \\beta\\right] = \\operatorname{Var}\\left[\\frac{\\gamma}{\\sigma}\\phi(X)\\right] = \\left(\\frac{\\gamma}{\\sigma}\\right)^2 \\operatorname{Var}[\\phi(X)]$$\n由于 $\\sigma^2 = \\operatorname{Var}[\\phi(X)]$：\n$$\\operatorname{Var}[y] = \\frac{\\gamma^2}{\\sigma^2} \\sigma^2 = \\gamma^2$$\n为使 $\\operatorname{Var}[y]=1$，我们必须有 $(\\gamma^{\\ast})^2=1$。这得出 $\\gamma^{\\ast} = 1$ 或 $\\gamma^{\\ast} = -1$。在深度学习的背景下，$\\gamma$ 是一个缩放因子，通常初始化为 $1$，并常常被约束为正值。我们采用标准选择 $\\gamma^{\\ast}=1$。\n\n仿射参数是 $(\\gamma^{\\ast}, \\beta^{\\ast}) = (1, 0)$。这些参数与具体的激活函数 $\\phi$ 无关。\n\n最终的元组是 $\\big(E[\\phi_{\\mathrm{ReLU}}(X)], \\operatorname{Var}[\\phi_{\\mathrm{ReLU}}(X)], E[\\phi_{\\mathrm{GELU}}(X)], \\operatorname{Var}[\\phi_{\\mathrm{GELU}}(X)], \\gamma^{\\ast}, \\beta^{\\ast}\\big)$。\n代入推导出的表达式：\n$\\left( \\frac{1}{\\sqrt{2\\pi}}, \\frac{\\pi-1}{2\\pi}, \\frac{1}{2\\sqrt{\\pi}}, \\frac{4\\pi+2\\sqrt{3}-3}{12\\pi}, 1, 0 \\right)$。",
            "answer": "$$\n\\boxed{\\left( \\frac{1}{\\sqrt{2\\pi}}, \\frac{\\pi-1}{2\\pi}, \\frac{1}{2\\sqrt{\\pi}}, \\frac{4\\pi+2\\sqrt{3}-3}{12\\pi}, 1, 0 \\right)}\n$$"
        },
        {
            "introduction": "激活函数的作用远不止引入非线性。某些激活函数的设计可以被理解为执行一种隐式的正则化，从而直接影响学习到的特征结构。该练习  探索了软阈值（soft-thresholding）激活函数与 $L_1$ 正则化之间的深刻联系，后者是诱导稀疏性的常用技术。通过求解一个带 $L_1$ 惩罚项的优化问题，你将发现其解正是一个特定的激活函数。完成此练习后，你会认识到激活函数不仅仅是一个固定的非线性变换，更可以是一个优化过程中的算法步骤，用以促进网络表征的稀疏性等优良特性。",
            "id": "3097828",
            "problem": "考虑一个单一线性层，它在一个非线性函数之前产生预激活值 $v \\in \\mathbb{R}^n$。假设学习到的特征 $a \\in \\mathbb{R}^n$ 是通过求解一个带有平方误差项和1-范数正则化器的局部经验风险问题得到的。对于给定的 $v$ 和正则化参数 $\\lambda \\ge 0$，其目标函数为\n$$\nJ(a; v, \\lambda) = \\frac{1}{2}\\|a - v\\|_2^2 + \\lambda \\|a\\|_1.\n$$\n由于可分性，假设 $J$ 是逐坐标最小化的。请从基本原理（凸性、可分性和次梯度最优性）出发，推导出对任意坐标产生 $J$ 的唯一最小化子的激活映射，并解释为什么该映射相对于 $v$ 能够促进 $a$ 的稀疏性。然后，使用近端梯度法（PGM），论证在前向传播中应用此激活函数如何等价于对特征执行一步针对1-范数正则化的近端步骤，从而将该激活函数与带有1-范数惩罚的学习特征联系起来。\n\n实现任务：编写一个完整的、可运行的程序，对下面指定的每个测试用例，计算将推导出的激活映射应用于 $v$ 所实现的正则化目标函数值的下降量，该下降量是与保持 $a = v$ 不变的情况相比较的。对每个测试用例，计算标量\n$$\n\\Delta(v,\\lambda) = J(v; v, \\lambda) - J(\\phi(v;\\lambda); v, \\lambda),\n$$\n其中 $\\phi(\\cdot;\\lambda)$ 表示推导出的激活映射。你的程序必须将测试套件的所有 $\\Delta$ 值作为单行输出，形式为方括号内包含的逗号分隔列表（例如，$[x_1,x_2,x_3]$）。不涉及物理单位或角度单位，所有输出均为实数。\n\n测试套件：\n- 用例1：$v = [3.0,-1.5,0.2,-0.05]$，$\\lambda = 0.5$。\n- 用例2：$v = [0.5,-0.5,0.49,-0.51]$，$\\lambda = 0.5$。\n- 用例3：$v = [0.1,-0.2,0.0,0.05]$，$\\lambda = 10.0$。\n- 用例4：$v = [-2.0,2.0]$，$\\lambda = 0.0$。\n\n你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[x_1,x_2,x_3,x_4]$），其中每个 $x_i$ 是相应测试用例计算出的 $\\Delta(v,\\lambda)$，表示为浮点数。",
            "solution": "该问题是有效的。它在科学上基于凸优化和机器学习的原理，问题提法良好且有唯一解，并且陈述客观。其前提事实正确，设置完整，任务是在正则化和近端算子研究领域一个标准的、非平凡的问题。\n\n### 第一部分：从基本原理推导激活映射\n\n需要最小化的目标函数为：\n$$\nJ(a; v, \\lambda) = \\frac{1}{2}\\|a - v\\|_2^2 + \\lambda \\|a\\|_1\n$$\n其中 $a, v \\in \\mathbb{R}^n$ 且 $\\lambda \\ge 0$。平方L2范数项为 $\\|a - v\\|_2^2 = \\sum_{i=1}^n (a_i - v_i)^2$，L1范数项为 $\\|a\\|_1 = \\sum_{i=1}^n |a_i|$。该目标函数是可分的，意味着它可以写成关于各个坐标的函数之和：\n$$\nJ(a; v, \\lambda) = \\sum_{i=1}^n \\left( \\frac{1}{2}(a_i - v_i)^2 + \\lambda |a_i| \\right) = \\sum_{i=1}^n J_i(a_i; v_i, \\lambda)\n$$\n因此，我们可以通过独立地最小化每个关于 $a_i$ 的 $J_i$ 来最小化 $J$。对于单个坐标 $a_i$ 的目标函数是：\n$$\nJ_i(a_i; v_i, \\lambda) = \\frac{1}{2}(a_i - v_i)^2 + \\lambda |a_i|\n$$\n该函数是一个严格凸的可微函数 $\\frac{1}{2}(a_i - v_i)^2$ 与一个凸的不可微函数 $\\lambda |a_i|$ 之和。它们的和是严格凸的，保证了唯一最小化子的存在。\n\n为了找到最小化子 $a_i^*$，我们使用凸分析中的一阶最优性条件，该条件表明 $0$ 必须位于 $J_i$ 在最小值点的次梯度中，即 $0 \\in \\partial J_i(a_i^*)$。\n\n$J_i(a_i)$ 的次梯度由下式给出：\n$$\n\\partial J_i(a_i) = \\frac{d}{d a_i} \\left(\\frac{1}{2}(a_i - v_i)^2\\right) + \\lambda \\partial |a_i| = (a_i - v_i) + \\lambda \\partial |a_i|\n$$\n绝对值函数 $\\partial|x|$ 的次梯度定义为：\n$$\n\\partial |x| = \\begin{cases} \\{1\\} & \\text{if } x > 0 \\\\ \\{-1\\} & \\text{if } x  0 \\\\ [-1, 1]  \\text{if } x = 0 \\end{cases}\n$$\n这可以紧凑地写作 $\\text{sgn}(x)$，只要我们定义 $\\text{sgn}(0) = [-1, 1]$。最优性条件 $0 \\in \\partial J_i(a_i^*)$ 变为：\n$$\n0 \\in (a_i^* - v_i) + \\lambda \\cdot \\partial |a_i^*| \\implies v_i - a_i^* \\in \\lambda \\cdot \\partial|a_i^*|\n$$\n我们针对 $a_i^*$ 的值分三种情况分析此条件：\n1.  **情况1：$a_i^*  0$**。次梯度 $\\partial|a_i^*|$ 为 $\\{1\\}$。条件变为 $v_i - a_i^* = \\lambda$，得到 $a_i^* = v_i - \\lambda$。为使此解与假设 $a_i^*  0$ 一致，我们必须有 $v_i - \\lambda  0$，即 $v_i  \\lambda$。\n2.  **情况2：$a_i^*  0$**。次梯度 $\\partial|a_i^*|$ 为 $\\{-1\\}$。条件变为 $v_i - a_i^* = -\\lambda$，得到 $a_i^* = v_i + \\lambda$。为使此解与假设 $a_i^*  0$ 一致，我们必须有 $v_i + \\lambda  0$，即 $v_i  -\\lambda$。\n3.  **情况3：$a_i^* = 0$**。次梯度 $\\partial|a_i^*|$ 为区间 $[-1, 1]$。条件变为 $v_i - 0 \\in \\lambda [-1, 1]$，可简化为 $v_i \\in [-\\lambda, \\lambda]$，即 $|v_i| \\le \\lambda$。\n\n结合这三个互斥的情况，我们得到最小化子 $a_i^*$ 的完整解：\n$$\na_i^* = \\phi(v_i; \\lambda) = \\begin{cases} v_i - \\lambda  \\text{if } v_i  \\lambda \\\\ 0  \\text{if } |v_i| \\le \\lambda \\\\ v_i + \\lambda  \\text{if } v_i  -\\lambda \\end{cases}\n$$\n这个激活映射 $\\phi$ 被称为**软阈值算子**。它可以更紧凑地写成：\n$$\na_i^* = \\phi(v_i; \\lambda) = \\text{sgn}(v_i) \\max(0, |v_i| - \\lambda)\n$$\n\n### 第二部分：稀疏性的促进\n\n软阈值算子 $\\phi(v_i; \\lambda)$ 相对于预激活向量 $v$，促进了学习到的特征向量 $a$ 的稀疏性。稀疏性意味着向量的许多分量都精确为零。其机制从推导出的公式中可以清楚地看出：\n-   输入向量的任何坐标 $v_i$，如果其绝对值 $|v_i|$ 不超过阈值 $\\lambda$，则被映射为 $a_i^* = 0$。这就为激活值创造了一个“死区”，有效地将小信号置零。\n-   对于任何绝对值 $|v_i|$ 大于 $\\lambda$ 的坐标 $v_i$，产生的激活值 $a_i^*$ 会向零收缩一个量 $\\lambda$。例如，如果 $v_i  \\lambda$，则 $a_i^* = v_i - \\lambda  v_i$。\n\n通过将所有初始幅值低于某个阈值 $\\lambda$ 的特征置零，该映射增加了 $a$ 中零元素的数量（与 $v$ 相比），从而促进了稀疏表示。参数 $\\lambda$ 直接控制稀疏程度：较大的 $\\lambda$ 会导致更宽的死区 $[-\\lambda, \\lambda]$ 和更强的收缩，从而产生更稀疏的输出向量 $a$。\n\n### 第三部分：与近端梯度法（PGM）的关系\n\n近端梯度法（PGM）是一种用于最小化形如 $F(x) = f(x) + g(x)$ 的复合目标函数的迭代算法，其中 $f$ 是凸且可微的（光滑的），而 $g$ 是凸但可能不可微的。PGM的更新规则是：\n$$\nx_{k+1} = \\text{prox}_{\\eta g}(x_k - \\eta \\nabla f(x_k))\n$$\n其中 $\\eta  0$ 是步长。关键部分是函数 $h$ 的近端算子，定义为：\n$$\n\\text{prox}_{h}(y) = \\arg\\min_x \\left( h(x) + \\frac{1}{2}\\|x - y\\|_2^2 \\right)\n$$\n让我们将这个定义与原始问题陈述进行比较。我们的目标是找到：\n$$\na^* = \\arg\\min_a J(a; v, \\lambda) = \\arg\\min_a \\left( \\lambda \\|a\\|_1 + \\frac{1}{2}\\|a - v\\|_2^2 \\right)\n$$\n通过与近端算子的定义直接比较，我们发现找到 $J(a; v, \\lambda)$ 的最小化子 $a^*$ 等价于计算函数 $h(a) = \\lambda \\|a\\|_1$ 在点 $y=v$ 处的近端算子。\n因此，推导出的激活函数 $\\phi(v; \\lambda)$ 正是L1范数正则化器的近端算子：\n$$\n\\phi(v; \\lambda) = \\text{prox}_{\\lambda \\|\\cdot\\|_1}(v)\n$$\n这在激活函数和PGM之间建立了一个直接的联系。在典型的机器学习情境中，我们可能需要最小化一个正则化损失，例如 $\\min_w \\mathcal{L}(w) + \\gamma \\|w\\|_1$，其中 $\\mathcal{L}$ 是一个光滑的数据拟合损失项（如均方误差），而 $\\gamma \\|w\\|_1$ 是非光滑的正则化项。更新权重 $w$ 的单步PGM操作将是：\n1.  对光滑部分执行梯度下降步：$w' = w_k - \\eta \\nabla \\mathcal{L}(w_k)$。\n2.  对结果应用近端算子：$w_{k+1} = \\text{prox}_{\\eta \\gamma \\|\\cdot\\|_1}(w')$。\n\n如果我们将预激活值 $v$ 等同于梯度步骤的结果，即 $v = w'$，并设置 $\\lambda = \\eta \\gamma$，那么应用激活函数 $\\phi(v; \\lambda)$ 在数学上就等同于执行PGM中的近端更新步骤。这表明神经网络前向传播中的一个激活函数可以被解释为在诱导稀疏性的L1惩罚下，为学习特征而执行优化算法的一次迭代。\n\n### 第四部分：目标函数下降量的计算\n\n需要计算的量是 $\\Delta(v, \\lambda) = J(v; v, \\lambda) - J(\\phi(v; \\lambda); v, \\lambda)$。设 $a^* = \\phi(v; \\lambda)$。\n第一项是：\n$$\nJ(v; v, \\lambda) = \\frac{1}{2}\\|v - v\\|_2^2 + \\lambda \\|v\\|_1 = \\lambda \\|v\\|_1\n$$\n第二项是：\n$$\nJ(a^*; v, \\lambda) = \\frac{1}{2}\\|a^* - v\\|_2^2 + \\lambda \\|a^*\\|_1\n$$\n所以，目标函数的下降量是：\n$$\n\\Delta(v, \\lambda) = \\lambda \\|v\\|_1 - \\left( \\frac{1}{2}\\|a^* - v\\|_2^2 + \\lambda \\|a^*\\|_1 \\right)\n$$\n将为每个测试用例计算此值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes the decrease in the regularized objective J(a; v, lambda)\n    by applying the soft-thresholding activation versus leaving a=v.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([3.0, -1.5, 0.2, -0.05]), 0.5),\n        (np.array([0.5, -0.5, 0.49, -0.51]), 0.5),\n        (np.array([0.1, -0.2, 0.0, 0.05]), 10.0),\n        (np.array([-2.0, 2.0]), 0.0),\n    ]\n\n    results = []\n    \n    for v, lmbda in test_cases:\n        # Define the objective function J(a; v, lambda)\n        def objective_J(a, v_in, lambda_in):\n            \"\"\"\n            Computes J(a; v, lambda) = 0.5 * ||a - v||_2^2 + lambda * ||a||_1\n            \"\"\"\n            l2_term = 0.5 * np.sum((a - v_in)**2)\n            l1_term = lambda_in * np.sum(np.abs(a))\n            return l2_term + l1_term\n\n        # Define the soft-thresholding activation function phi(v; lambda)\n        def soft_thresholding(v_in, lambda_in):\n            \"\"\"\n            Computes a* = sgn(v) * max(0, |v| - lambda) element-wise.\n            \"\"\"\n            return np.sign(v_in) * np.maximum(0, np.abs(v_in) - lambda_in)\n\n        # 1. Compute the optimal activation a_star = phi(v; lambda)\n        a_star = soft_thresholding(v, lmbda)\n\n        # 2. Compute J(v; v, lambda). The L2 term is zero.\n        # J(v; v, lambda) = 0.5 * ||v-v||_2^2 + lambda * ||v||_1 = lambda * ||v||_1\n        J_v = lmbda * np.sum(np.abs(v))\n        \n        # 3. Compute J(a_star; v, lambda)\n        J_a_star = objective_J(a_star, v, lmbda)\n\n        # 4. Compute the decrease in the objective\n        # delta = J(v; v, lambda) - J(phi(v;lambda); v, lambda)\n        delta = J_v - J_a_star\n        \n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "激活函数的选择对训练过程中的梯度流有着至关重要的影响，有时甚至会导致“梯度饥饿”（gradient starvation）问题，即部分参数停止学习。本练习  旨在剖析 `max` 算子的“赢者通吃”动态，它会将全部梯度集中在单个最大输入上，而饿死其他输入。进而，练习将引导你构建一个平滑的近似函数——LogSumExp，作为一种更公平地分配梯度的方法。通过解决这个实践问题，你将学会诊断一种常见的训练病症并实现其解决方案，这对于构建和调试复杂的深度学习模型（尤其是涉及注意力机制的模型）至关重要。",
            "id": "3097871",
            "problem": "考虑一个具有 $d$ 个标量特征的玩具单层回归模型。令输入向量为 $x \\in \\mathbb{R}^d$，参数向量为 $w \\in \\mathbb{R}^d$，并定义预激活值 $u \\in \\mathbb{R}^d$，其中对于每个索引 $i \\in \\{1,2,\\dots,d\\}$，有 $u_i = w_i x_i$。该模型通过一个激活聚合器 $\\phi$ 从 $u$ 生成一个标量输出 $s$，并使用平方损失 $L = \\tfrac{1}{2} (s - y)^2$ 针对标量目标 $y \\in \\mathbb{R}$ 进行训练。考虑两种激活聚合器：\n- 一个尖锐拐点（sharp-kink）聚合器 $\\phi_{\\text{sharp}}$，定义为 $\\phi_{\\text{sharp}}(u) = \\max_{i} u_i$，它在存在相等值时不可微。\n- 一个温度平滑聚合器 $\\phi_{\\tau}$，定义为 $\\phi_{\\tau}(u)$，它对于一个正温度参数 $\\tau \\in \\mathbb{R}_{0}$ 平滑地逼近最大值。$\\phi_{\\tau}$ 的具体函数形式需要从使用指数函数平滑逼近最大值的基本原理出发进行推导。\n\n仅从损失 $L$ 的定义和求导的链式法则出发，并且不使用任何预先给定的关于 $\\phi_{\\text{sharp}}$ 或 $\\phi_{\\tau}$ 的导数公式，你必须：\n1. 对 $\\phi_{\\text{sharp}}$，推导出以 $x$、$w$ 和 $y$ 表示的 $\\,\\dfrac{\\partial L}{\\partial w_i}\\,$，并在出现相等最大值时做出合理的次梯度选择。如果多个索引达到最大值，你必须在这些索引中选择一个均匀次梯度（即，将梯度均等地分配给所有达到最大值的索引 $i$），并确保你的选择与链式法则和 $L$ 的定义相一致。\n2. 对一个基于指数函数和正温度参数 $\\tau$ 构建为平滑最大值的温度平滑聚合器 $\\phi_{\\tau}$，推导出 $\\,\\dfrac{\\partial L}{\\partial w_i}\\,$。你的推导必须清楚地说明平滑性如何为所有坐标产生非零梯度，以及梯度总量的分布如何依赖于 $\\tau$。\n3. 对给定的 $(x,w,y,\\tau)$，定义“梯度匮乏指数”（gradient starvation index）如下。令 $A \\subseteq \\{1,\\dots,d\\}$ 为向量 $u$ 中达到最大值（包括相等情况）的索引集合，令 $A^c$ 为其补集。令 $g^{\\text{sharp}} \\in \\mathbb{R}^d$ 和 $g^{\\tau} \\in \\mathbb{R}^d$ 分别为在 $\\phi_{\\text{sharp}}$ 和 $\\phi_{\\tau}$ 下的梯度 $\\,\\dfrac{\\partial L}{\\partial w}\\,$。对于非最大值参数坐标的梯度匮乏指数为\n$$\n\\operatorname{SI}_{\\text{sharp}} = \\sum_{i \\in A^c} \\left| g^{\\text{sharp}}_i \\right|, \\quad\n\\operatorname{SI}_{\\tau} = \\sum_{i \\in A^c} \\left| g^{\\tau}_i \\right|.\n$$\n计算这两个指数和“缓解量”（alleviation amount）\n$$\n\\Delta_{\\tau} = \\operatorname{SI}_{\\tau} - \\operatorname{SI}_{\\text{sharp}}.\n$$\n\n实现一个完整的程序，对于下面指定的每个测试用例，使用推导出的公式和针对 $\\phi_{\\text{sharp}}$ 在相等最大值情况下选择的均匀次梯度，计算三个浮点数量 $\\,\\operatorname{SI}_{\\text{sharp}},\\,\\operatorname{SI}_{\\tau},\\,\\Delta_{\\tau}\\,$。为平滑聚合器使用一个数值稳定的实现，使其在 $\\tau$ 非常小的情况下仍能保持良好性能。\n\n测试套件（每个用例为 $(d,x,w,y,\\tau)$）：\n- 用例 1：$d = 4$, $x = [1.0, 0.9, 0.8, 0.1]$, $w = [1.0, 1.0, 1.0, 1.0]$, $y = 2.0$, $\\tau = 0.5$。\n- 用例 2：$d = 4$, $x = [1.0, 0.9, 0.8, 0.1]$, $w = [1.0, 1.0, 1.0, 1.0]$, $y = 2.0$, $\\tau = 0.001$。\n- 用例 3：$d = 4$, $x = [1.0, 1.0, 0.1, 0.1]$, $w = [1.0, 1.0, 1.0, 1.0]$, $y = 3.0$, $\\tau = 0.5$。\n- 用例 4：$d = 3$, $x = [-0.5, 0.4, 0.39]$, $w = [-2.0, 1.0, 1.0]$, $y = 0.0$, $\\tau = 0.2$。\n\n最终输出格式：\n你的程序应该生成一行输出，其中包含结果，格式为一个由内部列表组成的逗号分隔列表，每个内部列表为 $[\\operatorname{SI}_{\\text{sharp}}, \\operatorname{SI}_{\\tau}, \\Delta_{\\tau}]$，并用方括号括起来。例如：$[[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots]$。不应打印其他任何文本。",
            "solution": "该问题要求在两种不同的激活聚合方案下，对一个单层回归模型进行梯度的推导和计算，并随后分析一个“梯度匮乏指数”。验证过程确认了该问题是适定的、有科学依据且内部一致的。我们将进行正式的推导。\n\n该模型由输入 $x \\in \\mathbb{R}^d$、参数 $w \\in \\mathbb{R}^d$ 和预激活值 $u \\in \\mathbb{R}^d$ 定义，其中 $u_i = w_i x_i$。模型输出为 $s = \\phi(u)$，相对于目标 $y \\in \\mathbb{R}$ 的损失为 $L = \\frac{1}{2}(s - y)^2$。我们的目标是求出损失函数相对于每个权重 $w_i$ 的偏导数 $\\frac{\\partial L}{\\partial w_i}$。\n\n我们首先应用链式法则。$L$ 相对于 $w_i$ 的导数可以分解如下：\n$$\n\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L}{\\partial s} \\frac{\\partial s}{\\partial w_i}\n$$\n第一项，即损失相对于模型输出 $s$ 的导数，是直观的：\n$$\n\\frac{\\partial L}{\\partial s} = \\frac{\\partial}{\\partial s} \\left( \\frac{1}{2}(s - y)^2 \\right) = s - y\n$$\n第二项 $\\frac{\\partial s}{\\partial w_i}$ 需要进一步应用链式法则，因为 $s = \\phi(u)$ 并且 $u$ 是 $w$ 的函数。输出 $s$ 依赖于 $u$ 的所有分量，而 $u$ 的分量又依赖于 $w$。\n$$\n\\frac{\\partial s}{\\partial w_i} = \\sum_{j=1}^{d} \\frac{\\partial s}{\\partial u_j} \\frac{\\partial u_j}{\\partial w_i}\n$$\n预激活值 $u_j$ 定义为 $u_j = w_j x_j$。它相对于 $w_i$ 的偏导数仅在 $j=i$ 时非零：\n$$\n\\frac{\\partial u_j}{\\partial w_i} = \\frac{\\partial}{\\partial w_i} (w_j x_j) = \\begin{cases} x_i  \\text{if } j=i \\\\ 0  \\text{if } j \\neq i \\end{cases}\n$$\n将此代入求和式中，所有 $j \\neq i$ 的项都消失了，只剩下 $j=i$ 的项：\n$$\n\\frac{\\partial s}{\\partial w_i} = \\frac{\\partial s}{\\partial u_i} \\frac{\\partial u_i}{\\partial w_i} = \\frac{\\partial s}{\\partial u_i} x_i\n$$\n结合这些结果，我们得到梯度分量 $g_i = \\frac{\\partial L}{\\partial w_i}$ 的通用公式：\n$$\ng_i = \\frac{\\partial L}{\\partial w_i} = (s - y) \\frac{\\partial s}{\\partial u_i} x_i\n$$\n因此，梯度的具体形式取决于 $\\frac{\\partial s}{\\partial u_i} = \\frac{\\partial \\phi}{\\partial u_i}$ 这一项，我们现在将为指定的两种聚合器推导它。\n\n**1. 尖锐拐点聚合器 $\\phi_{\\text{sharp}}$ 的梯度**\n\n尖锐拐点聚合器定义为最大值函数：$s = \\phi_{\\text{sharp}}(u) = \\max_{j} u_j$。\n最大值函数在除了 $u$ 的两个或多个分量等于最大值之外的所有点上都是可微的。\n令 $A = \\{k \\in \\{1,\\dots,d\\} \\mid u_k = \\max_j u_j\\}$ 为对应于 $u$ 的最大值的索引集合。令 $|A|$ 为该集合的基数。\n\n情况1：唯一最大值 ($|A|=1$) 。令 $A = \\{k\\}$。那么对于当前值邻域内的 $u$，有 $s = u_k$。其偏导数为：\n$$\n\\frac{\\partial s}{\\partial u_i} = \\frac{\\partial u_k}{\\partial u_i} = \\delta_{ik} = \\begin{cases} 1  \\text{if } i=k \\\\ 0  \\text{if } i \\neq k \\end{cases}\n$$\n情况2：最大值存在并列 ($|A|1$)。函数在这一点上不可微。我们必须选择一个次梯度。最大值函数的次微分 $\\partial(\\max)(u)$ 是对应于 $A$ 中索引的标准基向量的凸包：$\\partial(\\max)(u) = \\text{conv}\\{e_k \\mid k \\in A\\}$。一个向量 $v \\in \\mathbb{R}^d$ 是一个次梯度，如果其分量 $v_i$ 满足 $v_i \\ge 0$，$\\sum_{i \\in A} v_i = 1$，并且对于 $i \\notin A$ 有 $v_i = 0$。\n\n问题要求采用‘均匀次梯度选择’，这对应于将梯度均等地分配给所有达到最大值的分量。这是一个具体且合理的次梯度选择，我们将系数设置为均匀的：\n$$\n\\frac{\\partial s}{\\partial u_i} = \\begin{cases} 1/|A|  \\text{if } i \\in A \\\\ 0  \\text{if } i \\notin A \\end{cases}\n$$\n这个单一公式正确地处理了唯一最大值（$|A|=1$）和并列最大值两种情况。\n\n因此，尖锐拐点聚合器的梯度分量 $g^{\\text{sharp}}_i$ 为：\n$$\ng^{\\text{sharp}}_i = (\\max_j u_j - y) \\cdot x_i \\cdot \\begin{cases} 1/|A|  \\text{if } i \\in A \\\\ 0  \\text{if } i \\notin A \\end{cases}\n$$\n\n**2. 温度平滑聚合器 $\\phi_{\\tau}$ 的梯度**\n\n我们的任务是基于指数函数构建一个最大值函数的光滑近似。一个标准的构造是 LogSumExp 函数，由一个温度参数 $\\tau  0$ 进行缩放。我们定义 $\\phi_{\\tau}(u)$ 为：\n$$\ns = \\phi_{\\tau}(u) = \\tau \\ln \\left( \\sum_{j=1}^{d} e^{u_j/\\tau} \\right)\n$$\n为了说明这确实逼近了最大值，令 $u_k = \\max_j u_j$。我们可以将表达式重写为：\n$$\n\\phi_{\\tau}(u) = \\tau \\ln \\left( e^{u_k/\\tau} \\sum_{j=1}^{d} e^{(u_j - u_k)/\\tau} \\right) = u_k + \\tau \\ln \\left( 1 + \\sum_{j \\neq k} e^{(u_j - u_k)/\\tau} \\right)\n$$\n当 $\\tau \\to 0^+$ 时，对于任何 $j \\neq k$，项 $(u_j - u_k)/\\tau \\to -\\infty$，因此 $e^{(u_j - u_k)/\\tau} \\to 0$。求和项消失，对数趋近于 $\\ln(1)=0$，因此 $\\phi_{\\tau}(u) \\to u_k = \\max_j u_j$。\n\n偏导数 $\\frac{\\partial s}{\\partial u_i}$ 通过对 $\\phi_{\\tau}(u)$ 求导得到：\n$$\n\\frac{\\partial s}{\\partial u_i} = \\frac{\\partial}{\\partial u_i} \\left[ \\tau \\ln \\left( \\sum_{j=1}^{d} e^{u_j/\\tau} \\right) \\right] = \\tau \\cdot \\frac{1}{\\sum_{j=1}^{d} e^{u_j/\\tau}} \\cdot \\frac{\\partial}{\\partial u_i}\\left( \\sum_{j=1}^{d} e^{u_j/\\tau} \\right)\n$$\n该和式相对于 $u_i$ 的导数就是第 $i$ 项的导数：\n$$\n\\frac{\\partial}{\\partial u_i}\\left( \\sum_{j=1}^{d} e^{u_j/\\tau} \\right) = e^{u_i/\\tau} \\cdot \\frac{1}{\\tau}\n$$\n代入回去，我们得到：\n$$\n\\frac{\\partial s}{\\partial u_i} = \\tau \\cdot \\frac{1}{\\sum_j e^{u_j/\\tau}} \\cdot \\frac{e^{u_i/\\tau}}{\\tau} = \\frac{e^{u_i/\\tau}}{\\sum_j e^{u_j/\\tau}}\n$$\n这是应用于向量 $u/\\tau$ 的 softmax 函数。该表达式对于所有 $i$ 都是严格为正的，这表明了平滑性如何为所有权重坐标产生非零偏导数（从而产生非零梯度），这与尖锐拐点聚合器为非最大值坐标产生零梯度不同。梯度的大小取决于 $u_i$ 的相对值和温度 $\\tau$。较小的 $\\tau$ 会导致一个更‘尖锐’的 softmax 分布，将梯度总量集中在具有最大 $u_i$ 的分量上，而较大的 $\\tau$ 则导致一个更‘平滑’、更均匀的分布。\n\n为了数值稳定性，特别是当某些 $u_j/\\tau$ 很大时，我们可以提出最大值 $u_{\\max} = \\max_j u_j$：\n$$\n\\phi_{\\tau}(u) = u_{\\max} + \\tau \\ln \\left( \\sum_j e^{(u_j-u_{\\max})/\\tau} \\right)\n$$\n$$\n\\frac{\\partial s}{\\partial u_i} = \\frac{e^{(u_i-u_{\\max})/\\tau}}{\\sum_j e^{(u_j-u_{\\max})/\\tau}}\n$$\n因此，平滑聚合器的梯度分量 $g^{\\tau}_i$ 为：\n$$\ng^{\\tau}_i = \\left(\\phi_{\\tau}(u) - y\\right) \\cdot \\frac{e^{u_i/\\tau}}{\\sum_j e^{u_j/\\tau}} \\cdot x_i\n$$\n\n**3. 梯度匮乏指数**\n\n最大化索引的集合是 $A = \\{i \\mid u_i = \\max_j u_j\\}$，其补集是 $A^c$。梯度匮乏指数定义为：\n$$\n\\operatorname{SI}_{\\text{sharp}} = \\sum_{i \\in A^c} \\left| g^{\\text{sharp}}_i \\right|, \\quad \\operatorname{SI}_{\\tau} = \\sum_{i \\in A^c} \\left| g^{\\tau}_i \\right|\n$$\n从我们对 $g^{\\text{sharp}}_i$ 的推导可知，如果 $i \\notin A$（即 $i \\in A^c$），则 $\\frac{\\partial s}{\\partial u_i}$ 项为零。因此，对于所有 $i \\in A^c$，$g^{\\text{sharp}}_i=0$。这得出的结论是，对于非最大值坐标，尖锐拐点聚合器的梯度匮乏是绝对的：\n$$\n\\operatorname{SI}_{\\text{sharp}} = \\sum_{i \\in A^c} |0| = 0\n$$\n因此，术语‘梯度匮乏’对于尖锐拐点的情况是字面意义上的。\n\n对于平滑聚合器，$g^{\\tau}_i$ 通常对所有 $i$ 都非零，所以 $\\operatorname{SI}_{\\tau}$ 将是一个正值，表示‘泄漏’到非最大值分量的梯度总幅度。\n缓解量定义为 $\\Delta_{\\tau} = \\operatorname{SI}_{\\tau} - \\operatorname{SI}_{\\text{sharp}}$。根据我们的发现，这可以简化为：\n$$\n\\Delta_{\\tau} = \\operatorname{SI}_{\\tau}\n$$\n每个测试用例的计算过程是：首先计算预激活值 $u$，确定集合 $A^c$，然后使用数值稳定的公式计算所有 $i \\in A^c$ 的 $g^{\\tau}_i$。所需的量即为 $\\operatorname{SI}_{\\text{sharp}}=0$，$\\operatorname{SI}_{\\tau} = \\sum_{i \\in A^c} |g^{\\tau}_i|$ 和 $\\Delta_{\\tau} = \\operatorname{SI}_{\\tau}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating gradient starvation indices for sharp and\n    smooth max aggregators in a simple regression model.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: d=4, x=[1.0, 0.9, 0.8, 0.1], w=[1.0, 1.0, 1.0, 1.0], y=2.0, tau=0.5\n        (4, [1.0, 0.9, 0.8, 0.1], [1.0, 1.0, 1.0, 1.0], 2.0, 0.5),\n        # Case 2: d=4, x=[1.0, 0.9, 0.8, 0.1], w=[1.0, 1.0, 1.0, 1.0], y=2.0, tau=0.001\n        (4, [1.0, 0.9, 0.8, 0.1], [1.0, 1.0, 1.0, 1.0], 2.0, 0.001),\n        # Case 3: d=4, x=[1.0, 1.0, 0.1, 0.1], w=[1.0, 1.0, 1.0, 1.0], y=3.0, tau=0.5\n        (4, [1.0, 1.0, 0.1, 0.1], [1.0, 1.0, 1.0, 1.0], 3.0, 0.5),\n        # Case 4: d=3, x=[-0.5, 0.4, 0.39], w=[-2.0, 1.0, 1.0], y=0.0, tau=0.2\n        (3, [-0.5, 0.4, 0.39], [-2.0, 1.0, 1.0], 0.0, 0.2),\n    ]\n\n    results = []\n    for d, x_list, w_list, y, tau in test_cases:\n        x = np.array(x_list, dtype=float)\n        w = np.array(w_list, dtype=float)\n        \n        # 1. Compute preactivations u\n        u = w * x\n        \n        # 2. Identify maximal and non-maximal indices\n        u_max = np.max(u)\n        # Using a small tolerance for float comparison, though not strictly necessary for given inputs\n        # but is good practice. In this case, direct comparison works.\n        is_max = (u == u_max)\n        is_not_max = ~is_max\n        \n        # 3. Calculate SI_sharp\n        # As derived, the gradient for non-maximal elements is always 0 for phi_sharp.\n        si_sharp = 0.0\n        \n        # 4. Calculate SI_tau\n        # Numerically stable calculation for s_tau and softmax probabilities\n        u_shifted = u - u_max\n        exp_terms = np.exp(u_shifted / tau)\n        sum_exp_terms = np.sum(exp_terms)\n        \n        # s_tau = u_max + tau * np.log(sum_exp_terms)\n        s_tau = u_max + tau * np.log(np.sum(np.exp((u - u_max) / tau)))\n        \n        # Common loss derivative term\n        dL_ds = s_tau - y\n        \n        # Softmax probabilities\n        p = exp_terms / sum_exp_terms\n        \n        # Gradient g_tau\n        g_tau = dL_ds * p * x\n        \n        # Sum of absolute gradients for non-maximal components\n        si_tau = np.sum(np.abs(g_tau[is_not_max]))\n        \n        # 5. Calculate Delta_tau\n        delta_tau = si_tau - si_sharp\n        \n        results.append([si_sharp, si_tau, delta_tau])\n\n    # Final print statement in the exact required format.\n    # The format [[a1,b1,c1],[a2,b2,c2]] is a string representation of a list of lists.\n    # We construct this string manually to avoid spaces and ensure exact format.\n    inner_lists_str = [','.join(map(str, r)) for r in results]\n    result_str = f\"[[{'],['.join(inner_lists_str)}]]\"\n    \n    print(result_str)\n\nsolve()\n```"
        }
    ]
}