{
    "hands_on_practices": [
        {
            "introduction": "一个激活函数的内在数学形态决定了神经网络拟合复杂函数的能力。这个练习通过一个精心设计的思想实验，直观地揭示了ReLU的分段线性特性与tanh的光滑非线性特性在函数逼近能力上的巨大差异。通过这个实践，你将理解为什么选择合适的激活函数对于模型的表达能力至关重要。",
            "id": "3171992",
            "problem": "您将研究激活函数的选择如何塑造假设类别，并因此影响对具有高曲率的目标函数的近似误差。在一维空间中进行研究。考虑修正线性单元（ReLU）激活函数 $g_{\\mathrm{ReLU}}(z)=\\max(0,z)$ 和双曲正切（tanh）激活函数 $g_{\\tanh}(z)=\\tanh(z)$。设目标函数为 $f^\\ast(x)=\\sin(\\omega x)$，其中角度以弧度为单位。您将构建一个数据集和两个宽度均为 $m$ 的单隐藏层模型族，然后比较它们通过最小二乘法拟合时的经验均方误差（MSE）。\n\n使用的基本定义：\n- 一个具有固定隐藏参数 $\\{a_i,b_i\\}_{i=1}^m$ 和激活函数 $g$ 的单隐藏层模型，其形式为 $h(x)=c+\\sum_{i=1}^m w_i\\,g(a_i x+b_i)$，其中 $c$ 和 $\\{w_i\\}$ 是可训练的输出层参数。对于一个数据集 $\\{(x_j,y_j)\\}_{j=1}^N$，经验均方误差为 $\\frac{1}{N}\\sum_{j=1}^N\\left(h(x_j)-y_j\\right)^2$。\n- 通过输出层的最小二乘法拟合，意味着选择 $(c,\\{w_i\\})$ 以最小化经验均方误差，这等同于对由 $g(a_i x_j+b_i)$ 构成的、并增广了一个偏置特征的特征矩阵求解一个线性最小二乘问题。\n\n按如下方式构建数据集和模型。\n1) 数据集：设 $N=1001$，$x_j$ 是在 $[-1,1]$ 区间内的 $N$ 个等距点。设 $y_j=f^\\ast(x_j)=\\sin(\\omega x_j)$，其中 $\\omega$ 根据每个测试用例指定，并以弧度为单位解释。\n2) ReLU 模型族 $H_{\\mathrm{ReLU}}$：固定宽度 $m$。选择斜率 $a_i$ 为 $[-\\alpha,\\alpha]$（其中 $\\alpha=0.5$）区间内的 $m$ 个等距值。为所有隐藏单元选择一个共享偏置 $b_i^{\\mathrm{ReLU}}=1.0$。在 $x\\in[-1,1]$ 上，这些选择保证对所有 $i$ 都有 $a_i x + b_i^{\\mathrm{ReLU}}\\ge 0.50$，因此 $g_{\\mathrm{ReLU}}(a_i x+b_i^{\\mathrm{ReLU}})=a_i x+b_i^{\\mathrm{ReLU}}$。因此，任何 $h\\in H_{\\mathrm{ReLU}}$ 都会简化为 $x$ 的一个仿射函数。\n3) Tanh 模型族 $H_{\\tanh}$：固定宽度 $m$。选择斜率 $a_i$ 为 $[-A,A]$（其中 $A=\\max(1.0,0.7\\,\\omega)$）区间内的 $m$ 个等距值，并为所有隐藏单元设置 $b_i^{\\tanh}=0$。这将隐藏单元的预激活值置于 $g_{\\tanh}$ 的高斜率区域附近，从而使 $H_{\\tanh}$ 能够生成一个更丰富的、光滑的、非线性的函数族。\n\n对于每个模型族，构建一个特征矩阵 $\\Phi\\in\\mathbb{R}^{N\\times(m+1)}$，其第一列是偏置特征 $\\phi_0(x)\\equiv 1$，其余 $m$ 列是在 $N$ 个输入上评估的 $\\phi_i(x)=g(a_i x+b_i)$。计算最小二乘解 $\\hat{\\theta}=\\arg\\min_{\\theta}\\|\\Phi\\theta - y\\|_2^2$，并报告经验均方误差 $E=\\frac{1}{N}\\|\\Phi\\hat{\\theta}-y\\|_2^2$。\n\n角度单位要求：所有三角函数求值均使用弧度。输出不得使用百分号；任何比率必须以小数形式表示。\n\n测试套件和要求的输出：\n- 使用以下参数集 $(\\omega,m)$：$(0.0,5)$, $(3.0,5)$, $(9.0,5)$, $(9.0,2)$, $(15.0,10)$。\n- 对每个参数集，计算三个浮点数：$E_{\\mathrm{ReLU}}$、$E_{\\tanh}$ 以及差值 $E_{\\mathrm{ReLU}}-E_{\\tanh}$。\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。列表中的每个元素是对应一个测试用例的三元组，该三元组本身也格式化为由方括号括起来的逗号分隔列表。例如：$[[e_{1,\\mathrm{ReLU}},e_{1,\\tanh},d_1],[e_{2,\\mathrm{ReLU}},e_{2,\\tanh},d_2],\\ldots]$。在最终的输出字符串中，将所有浮点数四舍五入到 6 位小数。\n\n交付物：提供一个完整的、可运行的程序，该程序能够构建数据集，为每个测试构建两个特征矩阵，解决最小二乘问题，计算所要求的误差，并以指定格式精确打印一行输出。不允许用户输入或使用外部文件。",
            "solution": "用户提供的问题是一个有效的、适定的、具有科学依据的计算机器学习练习。它要求比较两种单隐藏层神经网络模型的近似能力，其中一个使用修正线性单元（ReLU）激活函数，另一个使用双曲正切（tanh）激活函数。该分析的关键在于，固定的隐藏层参数的选择如何影响每个模型可以表示的函数类别（假设空间）的丰富程度。\n\n解决方案首先将问题形式化为一个线性最小二乘任务。随后，我们为给定的测试参数构建与每个模型族相对应的特征矩阵。然后计算最小二乘解，以找到每个模型族中的最佳拟合模型，并计算和比较它们各自的均方误差。\n\n**1. 将问题形式化为线性最小二乘**\n\n一个单隐藏层模型定义为 $h(x) = c + \\sum_{i=1}^m w_i\\,g(a_i x+b_i)$。根据每个模型族的问题规范，隐藏层参数，即斜率 $\\{a_i\\}_{i=1}^m$ 和偏置 $\\{b_i\\}_{i=1}^m$，是固定的。可训练的参数是输出层的偏置 $c$ 和权重 $\\{w_i\\}_{i=1}^m$。\n\n对于一个包含 $N$ 个点的数据集 $\\{(x_j, y_j)\\}_{j=1}^N$，我们可以将模型的预测表示为矩阵-向量乘积。设 $\\theta = [c, w_1, \\ldots, w_m]^T$ 为可训练参数的列向量。我们定义一个特征矩阵 $\\Phi \\in \\mathbb{R}^{N \\times (m+1)}$，其中每一行对应一个输入点 $x_j$，每一列对应一个基函数。第一列是常数偏置项，对所有 $j$ 都有 $\\phi_0(x_j) = 1$。随后的 $m$ 列是隐藏神经元的输出，$\\phi_i(x_j) = g(a_i x_j + b_i)$，其中 $i=1, \\ldots, m$。\n\n对所有 $x_j$ 的模型预测向量由 $\\hat{y} = \\Phi\\theta$ 给出。目标是找到参数向量 $\\theta$ 以最小化经验均方误差（MSE），该误差与残差向量 $r = y - \\hat{y}$ 的欧几里得范数的平方成正比：\n$$\nE = \\frac{1}{N} \\sum_{j=1}^N (h(x_j) - y_j)^2 = \\frac{1}{N} \\| \\Phi\\theta - y \\|_2^2\n$$\n这是一个标准的线性最小二乘问题。最小化此误差的最优参数向量 $\\hat{\\theta}$ 由正规方程组的解给出：\n$$\n(\\Phi^T \\Phi) \\hat{\\theta} = \\Phi^T y\n$$\n在数值计算上，使用 QR 分解或奇异值分解（SVD）等方法来求解 $\\hat{\\theta}$ 更为稳定，这些方法已在标准数值库中实现。一旦找到 $\\hat{\\theta}$，最小 MSE 就计算为 $E = \\frac{1}{N} \\| \\Phi\\hat{\\theta} - y \\|_2^2$。\n\n**2. ReLU 模型族（$H_{\\mathrm{ReLU}}$）分析**\n\nReLU 模型族使用激活函数 $g_{\\mathrm{ReLU}}(z) = \\max(0, z)$ 定义。隐藏参数选择如下：宽度为 $m$，斜率 $a_i$ 为 $[-\\alpha, \\alpha]$（其中 $\\alpha=0.5$）区间内的 $m$ 个等距值，以及所有单元共享的偏置 $b_i^{\\mathrm{ReLU}} = 1.0$。\n\n一个关键的洞察是预激活项 $z_i(x) = a_i x + b_i^{\\mathrm{ReLU}}$ 对于输入 $x \\in [-1, 1]$ 的行为。由于 $|a_i| \\le 0.5$ 且 $|x| \\le 1$，项 $a_i x$ 是有界的：$|a_i x| \\le 0.5$。因此，预激活项的界为：\n$$\nz_i(x) = a_i x + 1.0 \\ge -0.5 + 1.0 = 0.5\n$$\n由于在定义域内对所有 $i$ 和所有 $x$ 都有 $z_i(x) \\ge 0.5$，ReLU 激活函数始终在其线性区域内运行，即 $g_{\\mathrm{ReLU}}(z_i(x)) = z_i(x) = a_i x + 1.0$。\n\n因此，模型函数 $h(x) \\in H_{\\mathrm{ReLU}}$ 简化为：\n$$\nh(x) = c + \\sum_{i=1}^m w_i (a_i x + 1.0) = \\left(c + \\sum_{i=1}^m w_i\\right) + \\left(\\sum_{i=1}^m w_i a_i\\right) x\n$$\n这是一个形式为 $C_0 + C_1 x$ 的仿射函数。这意味着，尽管被表述为神经网络，但这种特定构造的 ReLU 模型只能表示线性函数。它逼近像 $f^\\ast(x) = \\sin(\\omega x)$ 这样的非线性目标函数的能力，从根本上被限制在最佳可能的线性近似。对于 $m \\ge 2$，假设空间不会随着宽度 $m$ 的增加而扩展，因为基函数 $\\{1, a_1 x+1, \\ldots, a_m x+1\\}$ 都位于 $\\{1, x\\}$ 的生成空间内。\n\n**3. Tanh 模型族（$H_{\\tanh}$）分析**\n\nTanh 模型族使用激活函数 $g_{\\tanh}(z) = \\tanh(z)$。隐藏参数为：宽度 $m$，斜率 $a_i$ 为 $[-A, A]$（其中 $A = \\max(1.0, 0.7\\,\\omega)$）区间内的 $m$ 个等距值，以及偏置 $b_i^{\\tanh} = 0$。\n\n模型函数为 $h(x) = c + \\sum_{i=1}^m w_i \\tanh(a_i x)$。与 ReLU 的情况不同，$\\tanh$ 函数在其整个定义域（除了 $z=0$）都是非线性的。基函数 $\\phi_i(x) = \\tanh(a_i x)$ 是光滑的 S 型函数。斜率范围 $[-A, A]$ 适应于目标函数的频率 $\\omega$，确保基函数具有不同程度的陡峭度，以适于捕捉 $\\sin(\\omega x)$ 的振荡。这些非线性基函数的线性组合可以形成一个丰富的、光滑的、非线性的近似函数类别，这比 $H_{\\mathrm{ReLU}}$ 的仿射类别要强大得多。\n\n**4. 计算算法**\n\n对于测试套件中提供的每个参数集 $(\\omega, m)$，我们执行以下步骤：\n1.  **生成数据集**：创建一个包含 $N=1001$ 个在 $[-1, 1]$ 区间内均匀分布的输入点向量 $x$，以及对应的目标向量 $y = \\sin(\\omega x)$。\n2.  **计算 $E_{\\mathrm{ReLU}}$**：\n    -   确定 ReLU 的斜率 $a_i$ 和偏置 $b_i^{\\mathrm{ReLU}}$。\n    -   构建 $N \\times (m+1)$ 的特征矩阵 $\\Phi_{\\mathrm{ReLU}}$，其列为 $[1, g_{\\mathrm{ReLU}}(a_1 x + b_1^{\\mathrm{ReLU}}), \\ldots, g_{\\mathrm{ReLU}}(a_m x + b_m^{\\mathrm{ReLU}})]$。\n    -   求解最小二乘问题 $\\hat{\\theta}_{\\mathrm{ReLU}} = \\arg\\min_{\\theta} \\|\\Phi_{\\mathrm{ReLU}}\\theta - y\\|_2^2$。\n    -   计算误差 $E_{\\mathrm{ReLU}} = \\frac{1}{N} \\| \\Phi_{\\mathrm{ReLU}}\\hat{\\theta}_{\\mathrm{ReLU}} - y \\|_2^2$。\n3.  **计算 $E_{\\tanh}$**：\n    -   确定 Tanh 的斜率 $a_i$（基于 $\\omega$）和偏置 $b_i^{\\tanh}$。\n    -   构建 $N \\times (m+1)$ 的特征矩阵 $\\Phi_{\\tanh}$，其列为 $[1, g_{\\tanh}(a_1 x + b_1^{\\tanh}), \\ldots, g_{\\tanh}(a_m x + b_m^{\\tanh})]$。\n    -   求解 $\\hat{\\theta}_{\\tanh} = \\arg\\min_{\\theta} \\|\\Phi_{\\tanh}\\theta - y\\|_2^2$。\n    -   计算误差 $E_{\\tanh} = \\frac{1}{N} \\| \\Phi_{\\tanh}\\hat{\\theta}_{\\tanh} - y \\|_2^2$。\n4.  **存储结果**：计算并存储三元组 $(E_{\\mathrm{ReLU}}, E_{\\tanh}, E_{\\mathrm{ReLU}} - E_{\\tanh})$。\n\n对所有测试用例重复此过程，并将最终结果格式化为指定的单个字符串。预期的结果是，对于任何非平凡的目标（$\\omega  0$），$E_{\\mathrm{ReLU}}$ 将显著大于 $E_{\\tanh}$，这表明激活函数中的非线性对于近似复杂函数起着关键作用。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the approximation errors of two single-hidden-layer\n    neural network models (ReLU and Tanh) on a sinusoidal target function.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 5),\n        (3.0, 5),\n        (9.0, 5),\n        (9.0, 2),\n        (15.0, 10),\n    ]\n\n    all_results = []\n    \n    # Constants\n    N = 1001\n\n    for omega, m in test_cases:\n        # 1. Construct the dataset\n        x = np.linspace(-1.0, 1.0, N)\n        y = np.sin(omega * x)\n\n        # 2. ReLU Model Family Calculation\n        alpha = 0.5\n        a_relu = np.linspace(-alpha, alpha, m)\n        b_relu = 1.0\n\n        # Construct ReLU feature matrix Phi_relu\n        Phi_relu = np.ones((N, m + 1))\n        # Vectorized computation of hidden unit outputs\n        # x is (N,), a_relu is (m,). We need an (N, m) matrix.\n        # x[:, np.newaxis] is (N, 1), a_relu[np.newaxis, :] is (1, m)\n        # Broadcasting (N, 1) * (1, m) - (N, m)\n        pre_acts_relu = x[:, np.newaxis] * a_relu[np.newaxis, :] + b_relu\n        # Per problem statement logic, pre_acts_relu is always  0,\n        # so np.maximum(0, a) is equivalent to a. We use the formal definition.\n        Phi_relu[:, 1:] = np.maximum(0, pre_acts_relu)\n\n        # Solve least squares and compute MSE\n        try:\n            theta_relu, _, _, _ = np.linalg.lstsq(Phi_relu, y, rcond=None)\n            y_pred_relu = Phi_relu @ theta_relu\n            E_relu = np.mean((y - y_pred_relu)**2)\n        except np.linalg.LinAlgError:\n            # Should not happen with np.linalg.lstsq, but as a safeguard\n            E_relu = np.nan\n\n        # 3. Tanh Model Family Calculation\n        A = max(1.0, 0.7 * omega)\n        a_tanh = np.linspace(-A, A, m)\n        b_tanh = 0.0\n\n        # Construct Tanh feature matrix Phi_tanh\n        Phi_tanh = np.ones((N, m + 1))\n        pre_acts_tanh = x[:, np.newaxis] * a_tanh[np.newaxis, :] + b_tanh\n        Phi_tanh[:, 1:] = np.tanh(pre_acts_tanh)\n\n        # Solve least squares and compute MSE\n        try:\n            theta_tanh, _, _, _ = np.linalg.lstsq(Phi_tanh, y, rcond=None)\n            y_pred_tanh = Phi_tanh @ theta_tanh\n            E_tanh = np.mean((y - y_pred_tanh)**2)\n        except np.linalg.LinAlgError:\n            E_tanh = np.nan\n            \n        # 4. Difference\n        diff = E_relu - E_tanh\n        \n        all_results.append((E_relu, E_tanh, diff))\n\n    # 5. Format the final output string\n    output_parts = []\n    for e_relu, e_tanh, d in all_results:\n        part = f\"[{e_relu:.6f},{e_tanh:.6f},{d:.6f}]\"\n        output_parts.append(part)\n    \n    final_output_string = f\"[{','.join(output_parts)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "除了表达能力，激活函数还深刻影响深度网络的训练动态。为了让网络有效学习，信号（包括激活值和梯度）必须在层间稳定传递，避免消失或爆炸。本练习将引导你推导信号均值和方差在网络中的传播规律，并揭示为何像 $f(z) = \\tanh(z)$ 和 $f(z) = \\mathrm{ReLU}(z)$ 这样的经典激活函数可能导致“方差消失”问题，从而为理解更深层次的训练难题奠定基础。",
            "id": "3171987",
            "problem": "一个全连接神经层由激活前值 $z = W x + b$ 和激活后值 $a = f(z)$ 定义，其中 $x \\in \\mathbb{R}^{n}$ 是该层的输入向量，$W \\in \\mathbb{R}^{m \\times n}$ 是权重矩阵，$b \\in \\mathbb{R}^{m}$ 是偏置向量。假设前向信号传播遵循以下统计模型：\n\n1. $W$ 的元素是独立同分布 (i.i.d.) 的高斯随机变量，满足 $W_{ji} \\sim \\mathcal{N}\\!\\left(0, \\frac{g^{2}}{n}\\right)$。\n2. 分量 $\\{x_{i}\\}_{i=1}^{n}$ 是独立同分布的，且与 $W$ 独立，每个分量的均值为 $\\mu$，方差为 $\\sigma^{2}$。\n3. 偏置设为 $b = 0$。\n\n考虑三种激活函数：双曲正切函数 $f(z) = \\tanh(z)$，整流线性单元 (ReLU) $f(z) = \\max\\{0, z\\}$，以及缩放指数线性单元 (SELU)，\n$$\nf(z) =\n\\begin{cases}\n\\lambda z,  z  0, \\\\\n\\lambda \\alpha \\left(\\exp(z) - 1\\right),  z \\leq 0,\n\\end{cases}\n$$\n其中 $\\lambda = 1.050700987355480$ 且 $\\alpha = 1.673263242354377$。\n\n从期望和方差的核心定义、独立性假设以及大 $n$ 时由中心极限定理蕴含的高斯近似出发，通过将 $\\mu' = \\mathbb{E}[f(Z)]$ 和 $\\sigma'^{2} = \\mathbb{V}\\mathrm{ar}[f(Z)]$ 用 $Z$ 的高斯定律表示，推导对于一个通用激活函数 $f$ 跨层的均值/方差传播映射 $(\\mu, \\sigma^{2}) \\mapsto (\\mu', \\sigma'^{2})$。请对每个激活函数 $f = \\tanh$、$f = \\mathrm{ReLU}$ 和 $f = \\mathrm{SELU}$ 具体化你的推导。\n\n然后，限定在零均值情况 $\\mu = 0$，增益 $g = 1$ 和 $b = 0$，此时激活前值近似为零均值且方差等于输入方差的高斯分布。在此限制下，通过求解 $\\sigma'^{2} = \\sigma^{2}$ 来确定每个激活函数的不动点方差 $\\sigma_{\\star}^{2}$，并比较这三种情况下的解。将你的最终答案以行矩阵的形式报告，其中包含三种不动点方差，顺序为 $(\\tanh, \\mathrm{ReLU}, \\mathrm{SELU})$。如果需要任何数值计算，请精确使用所提供的 $\\lambda$ 和 $\\alpha$ 值，并使用标准正态累积分布函数 $\\Phi(\\cdot)$ 和概率密度函数 $\\varphi(\\cdot)$ 来解析地表达结果，无需四舍五入。最终答案必须是一个包含三个不动点方差的单行矩阵，不带单位。",
            "solution": "该问题要求在特定的统计假设下，推导一个全连接神经层的均值/方差传播映射，并确定三种激活函数：双曲正切 ($\\tanh$)、整流线性单元 (ReLU) 和缩放指数线性单元 (SELU) 的不动点方差。\n\n首先，我们描述层中神经元 $j$ 的激活前值 $z_j$ 的分布。由于偏置 $b$ 设为 0，激活前值由 $z_j = \\sum_{i=1}^{n} W_{ji} x_i$ 给出。输入分量 $x_i$ 是独立同分布的，其均值为 $\\mathbb{E}[x_i] = \\mu$，方差为 $\\mathbb{V}\\mathrm{ar}[x_i] = \\sigma^2$。权重 $W_{ji}$ 是独立同分布的高斯随机变量，$W_{ji} \\sim \\mathcal{N}(0, g^2/n)$，且与输入 $x_i$ 独立。\n\n激活前值 $z_j$ 的均值为：\n$$\n\\mathbb{E}[z_j] = \\mathbb{E}\\left[\\sum_{i=1}^{n} W_{ji} x_i\\right] = \\sum_{i=1}^{n} \\mathbb{E}[W_{ji} x_i]\n$$\n由于 $W_{ji}$ 和 $x_i$ 的独立性，我们有 $\\mathbb{E}[W_{ji} x_i] = \\mathbb{E}[W_{ji}] \\mathbb{E}[x_i]$。\n$$\n\\mathbb{E}[z_j] = \\sum_{i=1}^{n} (0) \\cdot \\mu = 0\n$$\n激活前值的均值恒为 0。\n\n激活前值 $z_j$ 的方差为：\n$$\n\\mathbb{V}\\mathrm{ar}[z_j] = \\mathbb{V}\\mathrm{ar}\\left[\\sum_{i=1}^{n} W_{ji} x_i\\right]\n$$\n由于对于不同的索引 $i$，$W_{ji} x_i$ 是独立的，所以和的方差等于方差的和：\n$$\n\\mathbb{V}\\mathrm{ar}[z_j] = \\sum_{i=1}^{n} \\mathbb{V}\\mathrm{ar}[W_{ji} x_i]\n$$\n对于两个独立的随机变量 $A$ 和 $B$，$\\mathbb{V}\\mathrm{ar}[AB] = \\mathbb{E}[A^2]\\mathbb{E}[B^2] - (\\mathbb{E}[A]\\mathbb{E}[B])^2$。\n我们有 $\\mathbb{E}[W_{ji}] = 0$ 和 $\\mathbb{V}\\mathrm{ar}[W_{ji}] = g^2/n$，所以 $\\mathbb{E}[W_{ji}^2] = \\mathbb{V}\\mathrm{ar}[W_{ji}] + (\\mathbb{E}[W_{ji}])^2 = g^2/n$。\n对于输入，$\\mathbb{E}[x_i] = \\mu$ 和 $\\mathbb{V}\\mathrm{ar}[x_i] = \\sigma^2$，所以 $\\mathbb{E}[x_i^2] = \\mathbb{V}\\mathrm{ar}[x_i] + (\\mathbb{E}[x_i])^2 = \\sigma^2 + \\mu^2$。\n因此，单项的方差为：\n$$\n\\mathbb{V}\\mathrm{ar}[W_{ji} x_i] = \\left(\\frac{g^2}{n}\\right)(\\sigma^2 + \\mu^2) - (0 \\cdot \\mu)^2 = \\frac{g^2}{n}(\\sigma^2 + \\mu^2)\n$$\n对 $n$ 项求和，我们得到激活前值的总方差：\n$$\n\\sigma_z^2 = \\mathbb{V}\\mathrm{ar}[z_j] = \\sum_{i=1}^{n} \\frac{g^2}{n}(\\sigma^2 + \\mu^2) = n \\cdot \\frac{g^2}{n}(\\sigma^2 + \\mu^2) = g^2(\\sigma^2 + \\mu^2)\n$$\n对于大的 $n$，根据中心极限定理，激活前值 $z_j$ 近似服从高斯分布。我们用 $Z$ 表示这个随机变量：\n$$\nZ \\sim \\mathcal{N}(0, g^2(\\sigma^2 + \\mu^2))\n$$\n激活函数的输出是 $a = f(Z)$。输出激活值的均值和方差分别为 $\\mu' = \\mathbb{E}[a]$ 和 $\\sigma'^2 = \\mathbb{V}\\mathrm{ar}[a]$。我们被要求在特定条件 $\\mu=0$ 和 $g=1$ 下，通过求解 $\\sigma'^2 = \\sigma^2$ 来找到不动点方差 $\\sigma_\\star^2$。\n\n在这些限制下，激活前值的分布简化为：\n$$\nZ \\sim \\mathcal{N}(0, \\sigma^2)\n$$\n方差传播映射为 $\\sigma^2 \\mapsto \\sigma'^2 = \\mathbb{V}\\mathrm{ar}[f(Z)]$。我们现在对每个激活函数求解 $\\sigma'^2 = \\sigma^2$。\n\n**1. 双曲正切函数: $f(z) = \\tanh(z)$**\n\n由于 $f(z) = \\tanh(z)$ 是一个奇函数，并且 $Z \\sim \\mathcal{N}(0, \\sigma^2)$ 的分布关于 0 对称，所以输出的均值为 $\\mu' = \\mathbb{E}[\\tanh(Z)] = 0$。\n输出的方差为 $\\sigma'^2 = \\mathbb{E}[\\tanh^2(Z)] - (\\mu')^2 = \\mathbb{E}[\\tanh^2(Z)]$。\n不动点方程为 $\\sigma^2 = \\mathbb{E}[\\tanh^2(Z)]$。\n一个平凡解是 $\\sigma^2 = 0$。如果 $\\sigma^2 = 0$，那么 $Z \\equiv 0$，所以 $\\tanh(Z) \\equiv 0$，且 $\\sigma'^2 = 0$。\n对于任何 $\\sigma^2  0$，我们有 $|\\tanh(z)|  1$ 对于任何有限的 $z$，这意味着 $\\tanh^2(z)  1$。因此，$\\sigma'^2 = \\mathbb{E}[\\tanh^2(Z)]  \\mathbb{E}[1] = 1$。这意味着任何非零不动点必须满足 $\\sigma^2  1$。\n为了分析原点附近的行为，我们使用 $\\tanh(z)$ 在 $z=0$ 附近的泰勒展开：$\\tanh(z) = z - \\frac{z^3}{3} + O(z^5)$。\n那么 $\\tanh^2(z) = \\left(z - \\frac{z^3}{3} + \\dots\\right)^2 = z^2 - \\frac{2z^4}{3} + O(z^6)$。\n对 $Z \\sim \\mathcal{N}(0, \\sigma^2)$ 取期望：\n$$\n\\sigma'^2 = \\mathbb{E}[\\tanh^2(Z)] \\approx \\mathbb{E}[Z^2] - \\frac{2}{3}\\mathbb{E}[Z^4]\n$$\n对于一个零均值高斯分布，$\\mathbb{E}[Z^2] = \\sigma^2$ 且 $\\mathbb{E}[Z^4] = 3\\sigma^4$。\n$$\n\\sigma'^2 \\approx \\sigma^2 - \\frac{2}{3}(3\\sigma^4) = \\sigma^2 - 2\\sigma^4\n$$\n不动点方程 $\\sigma^2 = \\sigma^2 - 2\\sigma^4$ 得出 $2\\sigma^4 = 0$，其唯一的实数解是 $\\sigma=0$。对于任何小的 $\\sigma^2  0$，我们有 $\\sigma'^2  \\sigma^2$，这表明该映射朝原点是收缩的。因此，唯一的不动点是 $\\sigma_{\\star, \\tanh}^2 = 0$。\n\n**2. 整流线性单元: $f(z) = \\max\\{0, z\\}$**\n\n对于 $Z \\sim \\mathcal{N}(0, \\sigma^2)$，其概率密度函数 (PDF) 为 $p(z) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp(-\\frac{z^2}{2\\sigma^2})$，我们首先计算输出均值 $\\mu'$。\n$$\n\\mu' = \\mathbb{E}[\\max\\{0, Z\\}] = \\int_{0}^{\\infty} z \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{z^2}{2\\sigma^2}\\right) dz = \\left[-\\frac{\\sigma^2}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{z^2}{2\\sigma^2}\\right)\\right]_{0}^{\\infty} = \\frac{\\sigma}{\\sqrt{2\\pi}}\n$$\n接下来，我们计算输出的二阶矩。\n$$\n\\mathbb{E}[f(Z)^2] = \\mathbb{E}[\\max\\{0, Z\\}^2] = \\int_{0}^{\\infty} z^2 p(z) dz = \\frac{1}{2}\\int_{-\\infty}^{\\infty} z^2 p(z) dz = \\frac{1}{2}\\mathbb{E}[Z^2] = \\frac{\\sigma^2}{2}\n$$\n输出的方差是：\n$$\n\\sigma'^2 = \\mathbb{E}[f(Z)^2] - (\\mu')^2 = \\frac{\\sigma^2}{2} - \\left(\\frac{\\sigma}{\\sqrt{2\\pi}}\\right)^2 = \\frac{\\sigma^2}{2} - \\frac{\\sigma^2}{2\\pi} = \\sigma^2\\left(\\frac{1}{2} - \\frac{1}{2\\pi}\\right)\n$$\n不动点方程为 $\\sigma^2 = \\sigma'^2$：\n$$\n\\sigma^2 = \\sigma^2\\left(\\frac{1}{2} - \\frac{1}{2\\pi}\\right)\n$$\n这个方程可以改写为 $\\sigma^2\\left(1 - \\left(\\frac{1}{2} - \\frac{1}{2\\pi}\\right)\\right) = 0$，或 $\\sigma^2\\left(\\frac{1}{2} + \\frac{1}{2\\pi}\\right)=0$。由于括号中的项非零，唯一的解是 $\\sigma^2=0$。\n因此，不动点方差为 $\\sigma_{\\star, \\mathrm{ReLU}}^2 = 0$。\n\n**3. 缩放指数线性单元 (SELU)**\n\nSELU 激活函数由下式给出：\n$$\nf(z) =\n\\begin{cases}\n\\lambda z,  z  0, \\\\\n\\lambda \\alpha \\left(\\exp(z) - 1\\right),  z \\leq 0,\n\\end{cases}\n$$\n其中有特定的常数 $\\lambda = 1.050700987355480$ 和 $\\alpha = 1.673263242354377$。这些常数不是任意的；它们是经过精确选择的，以确保激活函数具有自归一化属性。具体来说，如果一个层的输入激活值均值为 0，方差为 1，那么输出激活值的均值也将近似为 0，方差近似为 1。\n在我们的框架中，这意味着如果激活函数的输入 $Z$ 是一个标准正态变量，$Z \\sim \\mathcal{N}(0, 1)$，那么输出 $a=f(Z)$ 将有 $\\mathbb{E}[a] = 0$ 和 $\\mathbb{V}\\mathrm{ar}[a]=1$。\n\n让我们在我们的不动点问题中检验这一点。我们正在寻找一个 $\\sigma^2$，使得如果 $Z \\sim \\mathcal{N}(0, \\sigma^2)$，那么 $\\sigma'^2 = \\mathbb{V}\\mathrm{ar}[f(Z)] = \\sigma^2$。\n考虑输入方差为 $\\sigma^2=1$ 的情况。那么激活前值是 $Z \\sim \\mathcal{N}(0, 1)$。\n根据 SELU 的设计属性，输出激活值 $a = f(Z)$ 将具有均值 $\\mu' = \\mathbb{E}[f(Z)] = 0$ 和方差 $\\sigma'^2 = \\mathbb{V}\\mathrm{ar}[f(Z)] = 1$。\n由于输入方差 $\\sigma^2=1$ 导致输出方差 $\\sigma'^2=1$，条件 $\\sigma'^2=\\sigma^2$ 得以满足。\n因此，$\\sigma^2=1$ 是 SELU 的方差传播映射的一个不动点。这个非平凡不动点对于在深度网络中维持稳定的信号流至关重要。\n因此，不动点方差为 $\\sigma_{\\star, \\mathrm{SELU}}^2 = 1$。\n\n总之，在给定条件下，三种激活函数的不动点方差为：\n- 对于 tanh: $\\sigma_{\\star}^2 = 0$\n- 对于 ReLU: $\\sigma_{\\star}^2 = 0$\n- 对于 SELU: $\\sigma_{\\star}^2 = 1$\n\n最终答案是按指定顺序排列的这三个值的行矩阵。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  0  1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在上一个练习识别出方差消失问题后，我们现在主动设计一个解决方案。这个练习深入探讨了缩放指数线性单元（SELU）的设计原理，它是一种旨在克服信号衰减问题的激活函数。你将亲自推导并验证那些赋予SELU“自归一化”特性的“魔法常数” $\\alpha$ 和 $\\lambda$，并通过模拟实验见证信号是如何在深度网络中稳定传播的。",
            "id": "3171997",
            "problem": "考虑一个宽度为 $n \\gg 1$ 的全连接前馈层，其权重 $w_i$ 独立同分布于 $\\mathcal{N}(0, 1/n)$，且偏置为零。设输入坐标 $x_i$ 独立同分布，均值为 $\\mu$，方差为 $\\sigma^2$。在大宽度极限下，根据中心极限定理 (CLT)，预激活值 $z = \\sum_{i=1}^{n} w_i x_i$ 近似服从高斯分布，其均值为 $0$，方差为 $\\sigma_z^2 = \\sigma^2 + \\mu^2$。激活函数是缩放指数线性单元 (Scaled Exponential Linear Unit, SELU)，定义为 $f(z) = \\lambda \\, g(z)$，其中\n$$\ng(z) =\n\\begin{cases}\nz,  \\text{if } z  0, \\\\\n\\alpha \\left(e^z - 1\\right),  \\text{if } z \\le 0,\n\\end{cases}\n$$\n且 $\\alpha  0$，$\\lambda  0$ 是待确定的常数。\n\n定义层映射 $T$，它将输入均值和方差 $(\\mu, \\sigma^2)$ 映射到输出均值和方差 $(\\mu', \\sigma'^2)$。该输出是通过将函数 $f$ 应用于 $z \\sim \\mathcal{N}(0, \\sigma_z^2)$ 得到的，其中\n$$\n\\sigma_z^2 = \\sigma^2 + \\mu^2.\n$$\n利用高斯随机变量的基本概率性质（不假设 $\\alpha$ 或 $\\lambda$ 的任何具体值），推导输出均值 $\\mu'$ 和输出方差 $\\sigma'^2$ 关于 $\\sigma_z^2$、$\\alpha$ 和 $\\lambda$ 的表达式。然后，推导 $T$ 的不动点 $(\\mu^*, \\sigma^{*2})$，并确定 $\\alpha$ 和 $\\lambda$ 的值，使得 $(\\mu^*, \\sigma^{*2}) = (0, 1)$ 是一个不动点，即当输入均值和方差为 $(0, 1)$ 时，输出均值和方差也为 $(0, 1)$。\n\n在推导出所需公式后，编写一个程序，该程序：\n1. 计算使 $(0, 1)$ 成为 $T$ 的不动点的 $\\alpha$ 和 $\\lambda$。\n2. 对于一组给定的初始 $(\\mu, \\sigma^2)$ 测试用例，迭代映射 $T$ 10层，并返回到不动点的最终欧几里得距离，定义为\n$$\nd = \\sqrt{(\\mu_{10} - 0)^2 + (\\sigma_{10}^2 - 1)^2}.\n$$\n\n你的程序必须使用以下初始条件测试套件，以 $(\\mu, \\sigma^2)$ 形式给出：\n- $(0.0, 1.0)$,\n- $(0.1, 0.9)$,\n- $(1.0, 1.0)$,\n- $(0.0, 3.0)$,\n- $(-0.5, 0.5)$,\n- $(2.0, 4.0)$.\n\n你的程序应生成单行输出，其中包含六个测试用例的距离，格式为逗号分隔的列表，并用方括号括起来（例如，$[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_6]$）。所有数值均为无量纲量。输出值必须是实数（浮点数）。",
            "solution": "该问题是有效的，因为它在科学上基于神经网络动力学理论，问题陈述清晰，信息充分，有唯一解，并且表述客观。我们将进行完整的推导和求解。\n\n目标是分析激活值的均值和方差在通过一个使用缩放指数线性单元 (SELU) 激活函数的全连接神经网络层时的传播过程。我们首先将推导映射 $T: (\\mu, \\sigma^2) \\to (\\mu', \\sigma'^2)$，该映射描述了输入均值 $\\mu$ 和方差 $\\sigma^2$ 如何变换为输出均值 $\\mu'$ 和方差 $\\sigma'^2$。然后，我们将找出 SELU 函数的特定参数 $\\alpha$ 和 $\\lambda$，以确保在 $(\\mu, \\sigma^2) = (0, 1)$ 处存在一个稳定的不动点。\n\n预激活信号 $z$ 由输入 $x_i$ 的加权和形成：$z = \\sum_{i=1}^{n} w_i x_i$。输入 $x_i$ 是独立同分布的 (i.i.d.)，其均值为 $E[x_i] = \\mu$，方差为 $Var(x_i) = \\sigma^2$。权重 $w_i$ 是从正态分布 $\\mathcal{N}(0, 1/n)$ 中抽取的独立同分布随机变量。根据中心极限定理，对于较大的层宽度 $n$，预激活值 $z$ 近似是一个高斯随机变量。其均值为 $E[z] = \\sum_{i=1}^{n} E[w_i x_i] = \\sum_{i=1}^{n} E[w_i]E[x_i] = \\sum_{i=1}^{n} 0 \\cdot \\mu = 0$。由于独立性，其方差为 $Var(z) = \\sum_{i=1}^{n} Var(w_i x_i)$。我们有 $Var(w_i x_i) = E[(w_i x_i)^2] - (E[w_i x_i])^2 = E[w_i^2]E[x_i^2] - 0 = (Var(w_i) + E[w_i]^2)(Var(x_i) + E[x_i]^2) = (1/n + 0)(\\sigma^2 + \\mu^2) = (\\sigma^2 + \\mu^2)/n$。因此，$Var(z) = \\sum_{i=1}^{n} (\\sigma^2 + \\mu^2)/n = n \\cdot (\\sigma^2 + \\mu^2)/n = \\sigma^2 + \\mu^2$。我们将此方差表示为 $\\sigma_z^2 = \\sigma^2 + \\mu^2$。因此，预激活值 $z$ 是一个随机变量 $Z \\sim \\mathcal{N}(0, \\sigma_z^2)$。\n\n激活函数为 $f(z) = \\lambda g(z)$，其中\n$$\ng(z) =\n\\begin{cases}\nz,  \\text{if } z  0, \\\\\n\\alpha \\left(e^z - 1\\right),  \\text{if } z \\le 0,\n\\end{cases}\n$$\n其中常数 $\\alpha  0$ 且 $\\lambda  0$。输出激活值是一个随机变量 $Y = f(Z)$。输出均值为 $\\mu' = E[Y]$，输出方差为 $\\sigma'^2 = Var(Y)$。\n\n$\\mu' = E[f(Z)] = \\lambda E[g(Z)]$\n$\\sigma'^2 = Var[f(Z)] = \\lambda^2 Var[g(Z)] = \\lambda^2 \\left( E[g(Z)^2] - (E[g(Z)])^2 \\right)$\n\n我们必须计算 $g(Z)$ 的前两阶矩，其中 $Z \\sim \\mathcal{N}(0, \\sigma_z^2)$，其概率密度函数 (PDF) 为 $\\phi(z; \\sigma_z^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_z^2}}e^{-z^2/(2\\sigma_z^2)}$。\n\n**1. 输出均值 $\\mu'$ 的推导**\n\n一阶矩 $E[g(Z)]$ 为：\n$$\nE[g(Z)] = \\int_{-\\infty}^{\\infty} g(z) \\phi(z; \\sigma_z^2) dz = \\int_{-\\infty}^{0} \\alpha(e^z-1)\\phi(z; \\sigma_z^2)dz + \\int_{0}^{\\infty} z \\phi(z; \\sigma_z^2)dz\n$$\n第二个积分是半正态分布的一阶矩，按 $\\sigma_z$ 缩放：\n$\\int_{0}^{\\infty} z \\frac{1}{\\sqrt{2\\pi\\sigma_z^2}}e^{-z^2/(2\\sigma_z^2)}dz = \\frac{\\sigma_z}{\\sqrt{2\\pi}}$。\n\n第一个积分为：\n$\\alpha \\left( \\int_{-\\infty}^{0} e^z \\phi(z; \\sigma_z^2)dz - \\int_{-\\infty}^{0} \\phi(z; \\sigma_z^2)dz \\right)$。\n第二项是 $\\alpha \\cdot P(Z \\le 0) = \\alpha/2$。对于第一项，我们在 $e^z \\phi(z; \\sigma_z^2)$ 的指数中配方：\n$z - \\frac{z^2}{2\\sigma_z^2} = -\\frac{1}{2\\sigma_z^2}(z^2 - 2\\sigma_z^2 z) = -\\frac{(z-\\sigma_z^2)^2}{2\\sigma_z^2} + \\frac{\\sigma_z^2}{2}$。\n积分变为：\n$\\int_{-\\infty}^{0} e^{\\sigma_z^2/2} \\frac{1}{\\sqrt{2\\pi\\sigma_z^2}}e^{-\\frac{(z-\\sigma_z^2)^2}{2\\sigma_z^2}}dz = e^{\\sigma_z^2/2} P(X \\le 0)$，其中 $X \\sim \\mathcal{N}(\\sigma_z^2, \\sigma_z^2)$。\n将 $X$ 标准化得到 $P(\\frac{X-\\sigma_z^2}{\\sigma_z} \\le \\frac{0-\\sigma_z^2}{\\sigma_z}) = P(S \\le -\\sigma_z) = \\Phi(-\\sigma_z)$，其中 $S \\sim \\mathcal{N}(0,1)$，$\\Phi$ 是其累积分布函数 (CDF)。\n因此，$E[g(Z)] = \\frac{\\sigma_z}{\\sqrt{2\\pi}} + \\alpha \\left( e^{\\sigma_z^2/2} \\Phi(-\\sigma_z) - \\frac{1}{2} \\right)$。\n输出均值为 $\\mu' = \\lambda E[g(Z)]$。\n\n**2. 输出方差 $\\sigma'^2$ 的推导**\n\n二阶矩 $E[g(Z)^2]$ 为：\n$$\nE[g(Z)^2] = \\int_{-\\infty}^{\\infty} g(z)^2 \\phi(z; \\sigma_z^2) dz = \\int_{-\\infty}^{0} \\alpha^2(e^z-1)^2\\phi(z; \\sigma_z^2)dz + \\int_{0}^{\\infty} z^2 \\phi(z; \\sigma_z^2)dz\n$$\n第二个积分是 $Z$ 方差的一半：$\\int_{0}^{\\infty} z^2 \\phi(z; \\sigma_z^2)dz = \\frac{1}{2} E[Z^2] = \\frac{\\sigma_z^2}{2}$。\n第一个积分为 $\\alpha^2 \\int_{-\\infty}^{0} (e^{2z} - 2e^z + 1)\\phi(z; \\sigma_z^2)dz$。\n使用与之前相同的技巧：\n$\\int_{-\\infty}^{0} \\phi(z; \\sigma_z^2)dz = 1/2$。\n$\\int_{-\\infty}^{0} e^z\\phi(z; \\sigma_z^2)dz = e^{\\sigma_z^2/2} \\Phi(-\\sigma_z)$。\n$\\int_{-\\infty}^{0} e^{2z}\\phi(z; \\sigma_z^2)dz = e^{2\\sigma_z^2} \\Phi(-2\\sigma_z)$。\n因此，$E[g(Z)^2] = \\frac{\\sigma_z^2}{2} + \\alpha^2 \\left( e^{2\\sigma_z^2}\\Phi(-2\\sigma_z) - 2e^{\\sigma_z^2/2}\\Phi(-\\sigma_z) + \\frac{1}{2} \\right)$。\n输出方差为 $\\sigma'^2 = \\lambda^2 \\left( E[g(Z)^2] - (E[g(Z)])^2 \\right)$。\n\n**3. 确定不动点 $(0, 1)$ 对应的 $\\alpha$ 和 $\\lambda$**\n\n我们寻求参数 $\\alpha, \\lambda$，使得当输入为 $(\\mu, \\sigma^2)=(0, 1)$ 时，输出为 $(\\mu', \\sigma'^2)=(0, 1)$。\n对于此输入，$\\sigma_z^2 = \\mu^2 + \\sigma^2 = 0^2 + 1^2 = 1$，所以 $\\sigma_z = 1$，且预激活值 $Z \\sim \\mathcal{N}(0, 1)$。\n\n条件 1：$\\mu' = 0$。因为 $\\lambda  0$，这意味着 $E[g(Z)] = 0$。\n$E[g(Z)]|_{\\sigma_z=1} = \\frac{1}{\\sqrt{2\\pi}} + \\alpha \\left( e^{1/2} \\Phi(-1) - \\frac{1}{2} \\right) = 0$。\n求解 $\\alpha$：\n$$\n\\alpha = \\frac{-1/\\sqrt{2\\pi}}{e^{1/2} \\Phi(-1) - 1/2}\n$$\n条件 2：$\\sigma'^2 = 1$。\n$\\sigma'^2 = \\lambda^2 \\left( E[g(Z)^2] - (E[g(Z)])^2 \\right)$。由于 $E[g(Z)]=0$，这简化为 $1 = \\lambda^2 E[g(Z)^2]|_{\\sigma_z=1}$。\n求解 $\\lambda$：\n$$\n\\lambda = \\frac{1}{\\sqrt{E[g(Z)^2]|_{\\sigma_z=1}}}\n$$\n其中 $E[g(Z)^2]|_{\\sigma_z=1} = \\frac{1}{2} + \\alpha^2 \\left( e^{2}\\Phi(-2) - 2e^{1/2}\\Phi(-1) + \\frac{1}{2} \\right)$，使用上面确定的 $\\alpha$ 值。\n\n**4. 迭代映射与实现**\n\n参数 $\\alpha$ 和 $\\lambda$ 是只计算一次的常数。层映射 $T$ 按如下方式将 $(\\mu_k, \\sigma_k^2)$ 更新为 $(\\mu_{k+1}, \\sigma_{k+1}^2)$：\n1.  计算预激活方差：$\\sigma_{z,k}^2 = \\mu_k^2 + \\sigma_k^2$。令 $\\sigma_{z,k} = \\sqrt{\\sigma_{z,k}^2}$。\n2.  使用推导出的公式和 $\\sigma_z = \\sigma_{z,k}$ 来计算中间矩：\n    $E_k[g(Z)] = \\frac{\\sigma_{z,k}}{\\sqrt{2\\pi}} + \\alpha \\left( e^{\\sigma_{z,k}^2/2} \\Phi(-\\sigma_{z,k}) - \\frac{1}{2} \\right)$。\n    $E_k[g(Z)^2] = \\frac{\\sigma_{z,k}^2}{2} + \\alpha^2 \\left( e^{2\\sigma_{z,k}^2}\\Phi(-2\\sigma_{z,k}) - 2e^{\\sigma_{z,k}^2/2}\\Phi(-\\sigma_{z,k}) + \\frac{1}{2} \\right)$。\n3.  计算输出均值和方差：\n    $\\mu_{k+1} = \\lambda E_k[g(Z)]$。\n    $\\sigma_{k+1}^2 = \\lambda^2 \\left( E_k[g(Z)^2] - (E_k[g(Z)])^2 \\right)$。\n\n程序将首先计算常数 $\\alpha$ 和 $\\lambda$。然后，对于每个初始条件 $(\\mu_0, \\sigma_0^2)$，它将迭代此映射 10 次以获得 $(\\mu_{10}, \\sigma_{10}^2)$。最后，它将计算到不动点 $(0, 1)$ 的欧几里得距离：$d = \\sqrt{(\\mu_{10}-0)^2+(\\sigma_{10}^2-1)^2}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes SELU parameters for a fixed point at (0, 1),\n    iterates the mean/variance map for several test cases, and\n    calculates the final distance to the fixed point.\n    \"\"\"\n\n    def calculate_selu_params():\n        \"\"\"\n        Calculates alpha and lambda for the SELU activation function\n        such that (mu=0, sigma^2=1) is a fixed point.\n        The fixed point condition implies z ~ N(0, 1), so sigma_z = 1.\n        \"\"\"\n        # For E[g(Z)] = 0, where Z ~ N(0,1):\n        # E[g(Z)] = 1/sqrt(2*pi) + alpha * [exp(1/2) * Phi(-1) - 1/2] = 0\n        phi_neg_1 = norm.cdf(-1.0)\n        numerator_alpha = -1.0 / np.sqrt(2.0 * np.pi)\n        denominator_alpha = np.exp(0.5) * phi_neg_1 - 0.5\n        alpha = numerator_alpha / denominator_alpha\n\n        # For Var(f(Z)) = 1, where Z ~ N(0,1) and E[g(Z)] = 0:\n        # Var(f(Z)) = lambda^2 * E[g(Z)^2] = 1, so lambda = 1 / sqrt(E[g(Z)^2])\n        # E[g(Z)^2] = 1/2 + alpha^2 * [exp(2)*Phi(-2) - 2*exp(1/2)*Phi(-1) + 1/2]\n        phi_neg_2 = norm.cdf(-2.0)\n        \n        term1 = np.exp(2.0) * phi_neg_2\n        term2 = -2.0 * np.exp(0.5) * phi_neg_1\n        term3 = 0.5\n        \n        e_g_z_squared = 0.5 + alpha**2 * (term1 + term2 + term3)\n        lambda_ = 1.0 / np.sqrt(e_g_z_squared)\n        \n        return alpha, lambda_\n\n    def layer_map(mu, sigma2, alpha, lambda_):\n        \"\"\"\n        Applies the layer map T to transform (mu, sigma^2) to (mu', sigma'^2).\n        \"\"\"\n        # Pre-activation variance\n        sigma_z2 = mu**2 + sigma2\n        # Handle potential numerical issues where sigma_z2 may be slightly negative\n        if sigma_z2  0:\n            sigma_z2 = 0\n        sigma_z = np.sqrt(sigma_z2)\n\n        # First moment of g(Z)\n        phi_neg_sigma_z = norm.cdf(-sigma_z)\n        e_g_z = (sigma_z / np.sqrt(2 * np.pi)) + alpha * (np.exp(sigma_z2 / 2.0) * phi_neg_sigma_z - 0.5)\n        \n        # Second moment of g(Z)\n        phi_neg_2_sigma_z = norm.cdf(-2.0 * sigma_z)\n        term1 = np.exp(2.0 * sigma_z2) * phi_neg_2_sigma_z\n        term2 = -2.0 * np.exp(sigma_z2 / 2.0) * phi_neg_sigma_z\n        term3 = 0.5\n        e_g_z_squared = (sigma_z2 / 2.0) + alpha**2 * (term1 + term2 + term3)\n        \n        # Output mean and variance\n        mu_prime = lambda_ * e_g_z\n        sigma2_prime = lambda_**2 * (e_g_z_squared - e_g_z**2)\n        \n        return mu_prime, sigma2_prime\n\n    # Compute SELU parameters alpha and lambda\n    alpha, lambda_ = calculate_selu_params()\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 1.0),\n        (0.1, 0.9),\n        (1.0, 1.0),\n        (0.0, 3.0),\n        (-0.5, 0.5),\n        (2.0, 4.0),\n    ]\n\n    results = []\n    num_layers = 10\n    \n    for case in test_cases:\n        mu, sigma2 = case\n        for _ in range(num_layers):\n            mu, sigma2 = layer_map(mu, sigma2, alpha, lambda_)\n        \n        # Final values after 10 layers\n        mu_10, sigma2_10 = mu, sigma2\n        \n        # Euclidean distance to the fixed point (0, 1)\n        distance = np.sqrt(mu_10**2 + (sigma2_10 - 1.0)**2)\n        results.append(distance)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}