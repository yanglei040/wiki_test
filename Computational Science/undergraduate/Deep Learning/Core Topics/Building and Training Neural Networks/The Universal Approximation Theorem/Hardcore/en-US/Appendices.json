{
    "hands_on_practices": [
        {
            "introduction": "This first practice is about building from the ground up. We will see how a simple, single-hidden-layer network using the sigmoid activation function can be explicitly designed to approximate any piecewise constant function. This exercise demystifies the Universal Approximation Theorem by showing constructively how smooth activation functions can be combined to create sharp, step-like transitions, which are the fundamental building blocks for a wide range of functions .",
            "id": "3194212",
            "problem": "Consider a single-hidden-layer Artificial Neural Network (ANN) with a logistic sigmoid activation $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ in the hidden layer and a linear output. The network takes a scalar input $x \\in [0,1]$ and outputs a scalar. Let $f:[0,1]\\to\\mathbb{R}$ be a piecewise constant function with $k$ plateaus, with breakpoints $0 = s_{0}  s_{1}  \\dots  s_{k-1}  s_{k} = 1$ and plateau levels $h_{1}, h_{2}, \\dots, h_{k}$, where each $h_{i} \\in [0,1]$. Assume the true transitions of $f$ occur only in the $\\delta$-neighborhoods of the internal breakpoints, i.e., $f$ is constant on each $[s_{i-1}+\\frac{\\delta}{2},\\,s_{i}-\\frac{\\delta}{2}]$, and changes only within each $(s_{i}-\\frac{\\delta}{2},\\,s_{i}+\\frac{\\delta}{2})$ for $i \\in \\{1,\\dots,k-1\\}$.\n\nYou are asked to construct an explicit approximation $\\hat{f}$ by a single-hidden-layer ANN of the form\n$$\n\\hat{f}(x) = c_{0} + \\sum_{j=1}^{N} c_{j}\\,\\sigma\\!\\big(\\alpha(x - b_{j})\\big),\n$$\nwhere $N$ is the number of hidden neurons, $c_{0}, c_{1}, \\dots, c_{N} \\in \\mathbb{R}$ are output-layer weights (with $c_{0}$ being the bias), $b_{1},\\dots,b_{N} \\in [0,1]$ are hidden-layer biases that locate transitions, and $\\alpha  0$ is a common steepness parameter controlling the width of the smooth transition.\n\nSubject to choosing $b_{j}$ at the internal breakpoints and appropriate $c_{j}$ and $c_{0}$ to match the plateau levels, enforce the uniform error constraint\n$$\n\\sup_{x \\in [0,1]\\setminus \\bigcup_{i=1}^{k-1}\\left(s_{i}-\\frac{\\delta}{2},\\,s_{i}+\\frac{\\delta}{2}\\right)}\\big|\\,\\hat{f}(x)-f(x)\\,\\big| \\le \\epsilon,\n$$\nby an appropriate choice of $\\alpha$ that yields smooth transitions of width at most $\\delta$.\n\nStarting from the definitions of the logistic sigmoid and the Heaviside step function, and using only well-tested facts about single-hidden-layer ANN function composition and linear combinations, derive an explicit construction and error bound, and determine the minimal number $N$ of hidden neurons needed to realize $k$ plateaus with the above uniform error constraint away from the transition neighborhoods. Provide $N$ as a function of $k$.\n\nYour final answer must be a single closed-form expression for $N$ in terms of $k$. No rounding is required, and no physical units are involved.",
            "solution": "We begin with the logistic sigmoid definition $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$. The Heaviside step function $H(x - s)$, which equals $0$ for $x  s$ and $1$ for $x \\ge s$, can be approximated by the smooth sigmoid $\\sigma(\\alpha(x - s))$, where $\\alpha  0$ controls the steepness and hence the transition width. As $\\alpha \\to \\infty$, $\\sigma(\\alpha(x - s))$ converges pointwise to $H(x - s)$ for all $x \\neq s$.\n\nA piecewise constant function with breakpoints $s_{1},\\dots,s_{k-1}$ and plateau heights $h_{1},\\dots,h_{k}$ can be represented exactly using the Heaviside step function as\n$$\nf(x) = h_{1} + \\sum_{i=1}^{k-1}\\big(h_{i+1}-h_{i}\\big)\\,H(x - s_{i}).\n$$\nTo verify this identity, note that for any $x \\in [s_{m-1}, s_{m})$ with $m \\in \\{1,\\dots,k\\}$, we have $H(x - s_{i}) = 0$ for all $i \\ge m$ and $H(x - s_{i}) = 1$ for all $i  m$, so\n$$\nf(x) = h_{1} + \\sum_{i=1}^{m-1}\\big(h_{i+1}-h_{i}\\big) = h_{m},\n$$\nby telescoping.\n\nTo obtain a single-hidden-layer ANN approximation, we replace $H(x - s_{i})$ by $\\sigma\\!\\big(\\alpha(x - s_{i})\\big)$ and define\n$$\n\\hat{f}(x) = c_{0} + \\sum_{i=1}^{k-1} c_{i}\\,\\sigma\\!\\big(\\alpha(x - s_{i})\\big),\n$$\nwith coefficients chosen as $c_{0} = h_{1}$ and $c_{i} = h_{i+1} - h_{i}$ for $i \\in \\{1,\\dots,k-1\\}$. This yields\n$$\n\\hat{f}(x) = h_{1} + \\sum_{i=1}^{k-1}\\big(h_{i+1}-h_{i}\\big)\\,\\sigma\\!\\big(\\alpha(x - s_{i})\\big).\n$$\nTo enforce the uniform error bound away from transitions, we must bound the deviation of each sigmoid from its ideal step value outside $(s_{i}-\\frac{\\delta}{2},\\,s_{i}+\\frac{\\delta}{2})$.\n\nFix an internal breakpoint $s$, and consider $x \\le s - \\frac{\\delta}{2}$. Then $\\alpha(x - s) \\le -\\alpha\\frac{\\delta}{2}$, so\n$$\n\\sigma\\!\\big(\\alpha(x - s)\\big) \\le \\sigma\\!\\big(-\\alpha\\frac{\\delta}{2}\\big) = \\frac{1}{1+\\exp\\!\\big(\\alpha\\frac{\\delta}{2}\\big)}.\n$$\nSimilarly, for $x \\ge s + \\frac{\\delta}{2}$, we have\n$$\n\\sigma\\!\\big(\\alpha(x - s)\\big) \\ge \\sigma\\!\\big(\\alpha\\frac{\\delta}{2}\\big) = \\frac{1}{1+\\exp\\!\\big(-\\alpha\\frac{\\delta}{2}\\big)}.\n$$\nTherefore, if we require that for each transition\n$$\n\\sigma\\!\\big(-\\alpha\\frac{\\delta}{2}\\big) \\le \\varepsilon' \\quad\\text{and}\\quad \\sigma\\!\\big(\\alpha\\frac{\\delta}{2}\\big) \\ge 1 - \\varepsilon',\n$$\nthen outside the $\\delta$-neighborhood of $s$ the sigmoid approximates the step to within $\\varepsilon'$ uniformly. Both inequalities are equivalent to the same constraint on $\\alpha$:\n\n$$\n\\frac{1}{1+\\exp\\!\\big(\\alpha\\frac{\\delta}{2}\\big)} \\le \\varepsilon' \\quad\\Longleftrightarrow\\quad \\exp\\!\\big(\\alpha\\frac{\\delta}{2}\\big) \\ge \\frac{1-\\varepsilon'}{\\varepsilon'} \\quad\\Longleftrightarrow\\quad \\alpha \\ge \\frac{2}{\\delta}\\,\\ln\\!\\Big(\\frac{1-\\varepsilon'}{\\varepsilon'}\\Big).\n$$\n\nHence choosing\n$$\n\\alpha \\ge \\frac{2}{\\delta}\\,\\ln\\!\\Big(\\frac{1-\\varepsilon'}{\\varepsilon'}\\Big)\n$$\nguarantees each sigmoid matches its ideal step to within $\\varepsilon'$ outside the corresponding $\\delta$-neighborhood.\n\nWe now bound the uniform error of $\\hat{f}$ away from the union of all transition neighborhoods. For any $x$ away from transitions, each $\\sigma\\!\\big(\\alpha(x - s_{i})\\big)$ differs from its ideal $H(x - s_{i})$ value by at most $\\varepsilon'$. Using the triangle inequality,\n$$\n\\big|\\,\\hat{f}(x) - f(x)\\,\\big| \\le \\sum_{i=1}^{k-1} |c_{i}|\\,\\big|\\,\\sigma\\!\\big(\\alpha(x - s_{i})\\big) - H(x - s_{i})\\,\\big| \\le \\varepsilon' \\sum_{i=1}^{k-1} |h_{i+1}-h_{i}|.\n$$\nLet the total variation across breakpoints be\n$$\nV \\equiv \\sum_{i=1}^{k-1} |h_{i+1}-h_{i}|.\n$$\nTo enforce the target uniform error $\\epsilon$, it suffices to set\n$$\n\\varepsilon' = \\frac{\\epsilon}{V} \\quad \\text{if } V  0,\n$$\nwith the convention that if $V = 0$ (the function is constant, i.e., $k=1$), then no hidden neurons are needed and the approximation is exact with $c_{0} = h_{1}$.\n\nBecause each $h_{i} \\in [0,1]$, a worst-case bound is $V \\le k-1$, attained when the plateaus alternate between $0$ and $1$. Thus a conservative choice is $\\varepsilon' = \\frac{\\epsilon}{k-1}$ for $k \\ge 2$, which leads to a sufficient steepness\n$$\n\\alpha \\ge \\frac{2}{\\delta}\\,\\ln\\!\\Big(\\frac{1-\\frac{\\epsilon}{k-1}}{\\frac{\\epsilon}{k-1}}\\Big) = \\frac{2}{\\delta}\\,\\ln\\!\\Big(\\frac{k-1-\\epsilon}{\\epsilon}\\Big).\n$$\nThis guarantees the uniform error constraint away from the transition neighborhoods.\n\nWe now count the minimal number of hidden neurons $N$ required. The constructive representation above uses one sigmoid per internal breakpoint $s_{i}$, i.e., $N = k-1$, and the output bias $c_{0}=h_{1}$. To argue minimality, note that with a single-hidden-layer network using a monotone activation per neuron, the derivative of $\\hat{f}$ with respect to $x$ can be written as\n$$\n\\frac{d\\hat{f}}{dx}(x) = \\alpha \\sum_{i=1}^{N} c_{i}\\,\\sigma\\!\\big(\\alpha(x - b_{i})\\big)\\,\\big(1 - \\sigma\\!\\big(\\alpha(x - b_{i})\\big)\\big),\n$$\nwhich is a sum of $N$ localized, unimodal bumps centered near the $b_{i}$. A function with $k$ plateaus that changes only within the $(k-1)$ disjoint $\\delta$-neighborhoods must exhibit $(k-1)$ localized changes in value, hence at least $(k-1)$ localized bumps in its derivative away from flat regions. Since each hidden neuron contributes at most one such localized bump, any realization must satisfy $N \\ge k-1$. Combining this lower bound with the explicit construction that achieves $N = k-1$, we conclude that the minimal number of hidden neurons is\n$$\nN(k) = k - 1.\n$$\n\nTherefore, the minimal number of hidden neurons needed to build $k$ plateaus on $[0,1]$ with smooth transitions of width $\\delta$ and uniform error at most $\\epsilon$ away from transitions, using a single-hidden-layer ANN with logistic sigmoid activation and linear output, is $k-1$.",
            "answer": "$$\\boxed{k-1}$$"
        },
        {
            "introduction": "Now that we've seen *how* a network can approximate a function, a natural next question is: how large must the network be? This practice explores the relationship between the complexity of a target function—specifically its curvature—and the number of neurons required for an accurate approximation. By analyzing a function with a region of high curvature, you will determine how the size of a Rectified Linear Unit (ReLU) based network scales to capture these features within a desired error tolerance, providing a concrete understanding of the \"cost\" of approximation .",
            "id": "3194159",
            "problem": "Consider the one-dimensional target function $f:[-1,1]\\to \\mathbb{R}$ defined by the parameters $\\alpha0$ and $\\delta\\in(0,1)$ as follows: \n$$\nf(x) \\;=\\; \n\\begin{cases}\n\\alpha x^{2},  |x|\\le \\delta, \\\\\n\\alpha \\delta^{2} \\;+\\; 2\\alpha \\delta\\,(|x|-\\delta),  \\delta|x|\\le 1.\n\\end{cases}\n$$\nThus $f$ has a localized region of high curvature of width $2\\delta$ around $x=0$, and is linear outside that region. Let $\\epsilon\\in(0,1)$ be a target uniform error. You will approximate $f$ on $[-1,1]$ by a one-hidden-layer Rectified Linear Unit (ReLU) network, where the ReLU activation is $\\sigma(t)=\\max\\{0,t\\}$, so the network realizes continuous piecewise-linear functions on $[-1,1]$ whose number of linear pieces is at most the number of hidden neurons plus $1$.\n\nUsing only fundamental facts about linear spline interpolation error for twice differentiable functions and the representational property of one-hidden-layer ReLU networks as continuous piecewise-linear functions, derive an explicit function $U(\\epsilon,\\alpha,\\delta)$ such that there exists a one-hidden-layer ReLU network of width at most $U(\\epsilon,\\alpha,\\delta)$ whose output $g$ satisfies \n$$\n\\sup_{x\\in[-1,1]} |f(x)-g(x)| \\le \\epsilon.\n$$\nYour answer must be a single closed-form expression for $U(\\epsilon,\\alpha,\\delta)$ (do not use inequalities or rounding functions in your final expression). No numerical rounding is required.",
            "solution": "The goal is to find an upper bound $U(\\epsilon, \\alpha, \\delta)$ on the width of a single-hidden-layer ReLU network sufficient to approximate the target function $f(x)$ with uniform error at most $\\epsilon$.\n\nThe target function $f(x)$ is linear on the intervals $[-1, -\\delta)$ and $(\\delta, 1]$, and quadratic on $[-\\delta, \\delta]$. A ReLU network can represent the linear parts exactly. Therefore, we only need to approximate the non-linear portion, $h(x) = \\alpha x^2$, on the interval $[-\\delta, \\delta]$. We will do this using a continuous piecewise-linear (CPWL) function, $g(x)$, constructed via linear spline interpolation.\n\nThe standard error bound for linear spline interpolation of a twice-differentiable function $h(x)$ on a single interval $[a, b]$ is:\n$$ \\sup_{x \\in [a, b]} |h(x) - L(x)| \\le \\frac{1}{8} (b-a)^{2} \\sup_{t \\in [a, b]} |h''(t)| $$\nFor our function $h(x) = \\alpha x^2$, the second derivative is constant: $h''(x) = 2\\alpha$.\n\nWe partition the interval $[-\\delta, \\delta]$ into $N$ equal subintervals. The width of each subinterval is $h_s = \\frac{\\delta - (-\\delta)}{N} = \\frac{2\\delta}{N}$. Applying the error formula to each subinterval, the maximum error on that subinterval is:\n$$ \\epsilon_{\\text{sub}} \\le \\frac{1}{8} h_s^2 \\sup_{x \\in [-\\delta, \\delta]} |h''(x)| = \\frac{1}{8} \\left(\\frac{2\\delta}{N}\\right)^2 (2\\alpha) = \\frac{1}{8} \\frac{4\\delta^2}{N^2} (2\\alpha) = \\frac{\\alpha \\delta^2}{N^2} $$\nTo ensure the uniform error over the entire approximation region is at most $\\epsilon$, we require this subinterval error to be less than or equal to $\\epsilon$:\n$$ \\frac{\\alpha \\delta^2}{N^2} \\le \\epsilon $$\nSolving for $N$, we find the required number of subintervals:\n$$ N^2 \\ge \\frac{\\alpha \\delta^2}{\\epsilon} \\implies N \\ge \\sqrt{\\frac{\\alpha \\delta^2}{\\epsilon}} = \\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} $$\n\nA single-hidden-layer ReLU network can represent any CPWL function. The number of hidden neurons required is equal to the number of \"knots\" or points where the slope of the function changes. Our CPWL approximant $g(x)$ has knots at each of the $N-1$ interior interpolation points within $(-\\delta, \\delta)$, plus two additional knots at $x=-\\delta$ and $x=\\delta$ where the spline approximation meets the exact linear parts of $f(x)$. Thus, the total number of knots is $(N-1) + 2 = N+1$.\n\nThe required network width $W$ is therefore $W = N+1$. Substituting the inequality for $N$:\n$$ W \\ge \\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 1 $$\nSince the width must be an integer, the minimal required width is $W_{\\text{min}} = \\lceil \\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 1 \\rceil$.\nThe problem asks for a closed-form upper bound $U(\\epsilon, \\alpha, \\delta)$ for the width, without any rounding functions. We can use the property that for any real number $y$, $\\lceil y \\rceil \\le y + 1$.\nApplying this to our expression for $W_{\\text{min}}$:\n$$ W_{\\text{min}} \\le \\left(\\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 1\\right) + 1 = \\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 2 $$\nThus, a network width of $\\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 2$ is sufficient. We can choose this expression as our upper bound function $U$.",
            "answer": "$$\\boxed{\\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 2}$$"
        },
        {
            "introduction": "The standard Universal Approximation Theorem guarantees approximation for *continuous* functions, but many real-world signals and systems involve sharp jumps or discontinuities. This coding exercise challenges you to move beyond uniform convergence and explore approximation in the $L^p$ sense, which measures error \"on average.\" You will implement a simple Rectified Linear Unit (ReLU) network to approximate a step function and observe how the error behaves, demonstrating that neural networks can still be powerful approximators even when perfect point-wise matching is impossible .",
            "id": "3194151",
            "problem": "You are asked to formalize and test, in code, an $L^p$-approximation experiment that illustrates the Universal Approximation Theorem (UAT) for Deep Learning in the sense of $L^p$-convergence on a compact interval. The Universal Approximation Theorem (UAT) states that feed-forward neural networks with at least one hidden layer and suitable activation functions can approximate any continuous function on compact subsets of $\\mathbb{R}^n$ to arbitrary accuracy. This experiment investigates a discontinuous target function and demonstrates $L^p$-approximation by a network that introduces a narrow transition region (a boundary layer) around the point of discontinuity. The scientific base for the task is limited to the following: the definition of the $L^p$-norm, basic properties of integrals, and the definition of a one-hidden-layer feed-forward network with Rectified Linear Unit (ReLU) activation.\n\nDefine the target function $f:[0,1]\\to\\mathbb{R}$ by $f(x)=\\mathbf{1}_{\\{x1/2\\}}$, where $\\mathbf{1}_{\\{x1/2\\}}$ denotes the indicator function of the set $\\{x1/2\\}$. Consider the family of one-dimensional, one-hidden-layer networks $g_\\delta:[0,1]\\to\\mathbb{R}$ with Rectified Linear Unit (ReLU) activation, parameterized by a positive real number $\\delta\\in(0,1/2)$, that implement a linear ramp across a boundary layer of width $2\\delta$ centered at $x=1/2$, exactly matching $f(x)$ outside this layer. Concretely, the hidden units compute $h_1(x)=\\max\\{0,x-(1/2-\\delta)\\}$ and $h_2(x)=\\max\\{0,x-(1/2+\\delta)\\}$, and the output is $g_\\delta(x)=\\frac{h_1(x)-h_2(x)}{2\\delta}$.\n\nFor a given $p\\in[1,\\infty]$, define the $L^p$-error between $f$ and $g_\\delta$ as follows. For finite $p$, the $L^p$-error is $\\|f-g_\\delta\\|_{L^p([0,1])}=\\left(\\int_0^1 |f(x)-g_\\delta(x)|^p\\,dx\\right)^{1/p}$. For $p=\\infty$, define the $L^\\infty$-error to be $\\|f-g_\\delta\\|_{L^\\infty([0,1])}=\\sup_{x\\in[0,1]}|f(x)-g_\\delta(x)|$. In this experiment, numerical integration on a uniform grid should be used to approximate the integral for finite $p$, and a uniform grid search should be used for $p=\\infty$.\n\nYour task is to:\n- Implement the network $g_\\delta(x)$ as specified.\n- Compute the numerical approximation of $\\|f-g_\\delta\\|_{L^p([0,1])}$ for each test case using a uniform grid on $[0,1]$ and the trapezoidal rule for finite $p$, and the maximal absolute difference for $p=\\infty$.\n- For each test case, output a boolean indicating whether the computed error is less than or equal to a prescribed tolerance $\\epsilon$.\n\nUse the following test suite of parameter values $(p,\\epsilon,\\delta)$:\n1. $(p,\\epsilon,\\delta)=\\left(1,\\,0.1,\\,0.1\\right)$,\n2. $(p,\\epsilon,\\delta)=\\left(1,\\,0.01,\\,0.01\\right)$,\n3. $(p,\\epsilon,\\delta)=\\left(2,\\,0.05,\\,0.0135\\right)$,\n4. $(p,\\epsilon,\\delta)=\\left(4,\\,0.05,\\,2.25\\times 10^{-4}\\right)$,\n5. $(p,\\epsilon,\\delta)=\\left(2,\\,0.05,\\,0.05\\right)$,\n6. $(p,\\epsilon,\\delta)=\\left(\\infty,\\,0.4,\\,0.1\\right)$,\n7. $(p,\\epsilon,\\delta)=\\left(\\infty,\\,0.5,\\,0.1\\right)$.\n\nIn cases $1$ through $4$, the boundary layer width $\\delta$ has been chosen to be small relative to $\\epsilon$ so that the $L^p$-error is expected to be less than or equal to $\\epsilon$. Case $5$ is a deliberately mismatched choice to test failure when $\\delta$ scales like $\\epsilon$ instead of $\\epsilon^p$ for $p1$. Cases $6$ and $7$ probe the $L^\\infty$ behavior; continuous ramps cannot reduce the $L^\\infty$ error below a certain threshold due to the jump discontinuity at $x=1/2$.\n\nYour program must:\n- Construct the uniform grid on $[0,1]$ and perform the required numerical computations.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\\left[\\text{result1},\\text{result2},\\dots\\right]$, where each entry is a boolean.\n\nNo user input is allowed. No physical units are involved. All angles, if any were to appear, would be in radians, but none are required here.",
            "solution": "The problem asks for a numerical experiment to compute the $L^p$-error between a step function and its continuous, piecewise-linear neural network approximation. The core of the solution lies in deriving the analytical form of the error to understand the behavior, and then implementing a numerical procedure to verify it.\n\nFirst, we analyze the functions. The target function is a step function:\n$$\nf(x) = \\mathbf{1}_{\\{x1/2\\}} =\n\\begin{cases}\n0  \\text{if } x \\le 1/2 \\\\\n1  \\text{if } x  1/2\n\\end{cases}\n$$\nThe network approximation $g_\\delta(x)$ uses two ReLU neurons to create a linear ramp from 0 to 1 over the interval $[1/2 - \\delta, 1/2 + \\delta]$:\n$$\ng_\\delta(x) = \\frac{\\max\\{0, x - (1/2 - \\delta)\\} - \\max\\{0, x - (1/2 + \\delta)\\}}{2\\delta} =\n\\begin{cases}\n0  \\text{if } x \\le 1/2 - \\delta \\\\\n\\frac{x - 1/2 + \\delta}{2\\delta}  \\text{if } 1/2 - \\delta  x \\le 1/2 + \\delta \\\\\n1  \\text{if } x  1/2 + \\delta\n\\end{cases}\n$$\nThe error function $f(x) - g_\\delta(x)$ is non-zero only within the transition interval $(1/2 - \\delta, 1/2 + \\delta)$. We can compute the integral for the $L^p$ norm analytically. The integral of the $p$-th power of the error is:\n$$\nE_p^p = \\int_0^1 |f(x) - g_\\delta(x)|^p \\,dx = \\int_{1/2-\\delta}^{1/2} |0 - g_\\delta(x)|^p \\,dx + \\int_{1/2}^{1/2+\\delta} |1 - g_\\delta(x)|^p \\,dx\n$$\nBy substituting the expression for $g_\\delta(x)$ and noticing the symmetry of the two integrands, we can simplify:\n$$\nE_p^p = 2 \\int_{1/2-\\delta}^{1/2} \\left(\\frac{x - 1/2 + \\delta}{2\\delta}\\right)^p \\,dx\n$$\nUsing the substitution $u = (x - 1/2 + \\delta)/(2\\delta)$, where $dx = 2\\delta \\,du$ and the limits of integration change from $[1/2-\\delta, 1/2]$ to $[0, 1/2]$, we get:\n$$\nE_p^p = 2 \\int_0^{1/2} u^p (2\\delta \\,du) = 4\\delta \\left[ \\frac{u^{p+1}}{p+1} \\right]_0^{1/2} = \\frac{4\\delta}{p+1} \\left(\\frac{1}{2}\\right)^{p+1} = \\frac{\\delta}{(p+1)2^{p-1}}\n$$\nThus, the analytical $L^p$-error is:\n$$\n\\|f - g_\\delta\\|_{L^p([0,1])} = \\left( \\frac{\\delta}{(p+1)2^{p-1}} \\right)^{1/p}\n$$\nThis shows that for any finite $p$, the error can be made arbitrarily small by decreasing $\\delta$.\n\nFor the $L^\\infty$ case, we find the maximum absolute difference. The difference function $|f(x) - g_\\delta(x)|$ is zero outside $(1/2-\\delta, 1/2+\\delta)$. Inside this interval, the maximum difference occurs at the point of discontinuity, $x=1/2$. At this point, $f(1/2)=0$ and $g_\\delta(1/2) = 1/2$, so the difference is $1/2$.\n$$\n\\|f - g_\\delta\\|_{L^\\infty([0,1])} = \\sup_{x \\in [0,1]} |f(x) - g_\\delta(x)| = \\frac{1}{2}\n$$\nThis error is constant and independent of $\\delta$, demonstrating the failure of uniform convergence.\n\nThe implementation will compute these errors numerically. For finite $p$, it will evaluate $|f(x) - g_\\delta(x)|^p$ on a fine grid and use the trapezoidal rule to approximate the integral, followed by taking the $p$-th root. For $p=\\infty$, it will find the maximum absolute difference on the grid. The result is then compared to the tolerance $\\epsilon$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the L^p-error for a set of test cases to illustrate\n    the Universal Approximation Theorem for a discontinuous function.\n    \"\"\"\n    \n    # Test cases: (p, epsilon, delta)\n    test_cases = [\n        (1, 0.1, 0.1),\n        (1, 0.01, 0.01),\n        (2, 0.05, 0.0135),\n        (4, 0.05, 2.25e-4),\n        (2, 0.05, 0.05),\n        (np.inf, 0.4, 0.1),\n        (np.inf, 0.5, 0.1),\n    ]\n\n    results = []\n    \n    # Use a fine grid for numerical accuracy. N points means N-1 intervals.\n    N = 10**6 + 1\n    x = np.linspace(0.0, 1.0, N)\n\n    def target_function(x_vals):\n        \"\"\"\n        Computes f(x) = 1_{x > 1/2}.\n        \"\"\"\n        return (x_vals > 0.5).astype(float)\n\n    def network_approximator(x_vals, delta):\n        \"\"\"\n        Computes g_delta(x) = (h1(x) - h2(x)) / (2*delta).\n        \"\"\"\n        h1 = np.maximum(0, x_vals - (0.5 - delta))\n        h2 = np.maximum(0, x_vals - (0.5 + delta))\n        if delta == 0:\n            return target_function(x_vals)\n        return (h1 - h2) / (2.0 * delta)\n\n    # Evaluate the target function on the grid once.\n    f_vals = target_function(x)\n\n    for p, epsilon, delta in test_cases:\n        # Evaluate the network approximator for the given delta.\n        g_vals = network_approximator(x, delta)\n        \n        # Calculate the absolute difference.\n        abs_diff = np.abs(f_vals - g_vals)\n        \n        error = 0.0\n        if np.isinf(p):\n            # For p = infinity, compute the L-infinity norm (supremum norm).\n            # This is approximated by the maximum absolute difference on the grid.\n            error = np.max(abs_diff)\n        else:\n            # For finite p, compute the L^p norm via numerical integration.\n            # 1. Compute the integrand |f(x) - g_delta(x)|^p.\n            integrand = abs_diff**p\n            # 2. Integrate using the trapezoidal rule.\n            # np.trapz(y, x) computes the integral of y(x) dx.\n            integral_val = np.trapz(integrand, x)\n            # 3. Take the p-th root to get the L^p norm.\n            error = integral_val**(1.0 / p)\n            \n        # Compare the computed error with the tolerance epsilon.\n        results.append(error = epsilon)\n\n    # Print results in the specified format.\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}