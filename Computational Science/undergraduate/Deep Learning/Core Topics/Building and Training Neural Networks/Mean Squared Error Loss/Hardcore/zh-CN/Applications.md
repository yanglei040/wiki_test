## 应用与跨学科连接

在前面的章节中，我们已经深入探讨了[均方误差](@entry_id:175403)（Mean Squared Error, MSE）[损失函数](@entry_id:634569)的基本原理、数学性质及其在优化过程中的动态特性。作为一个基础性的目标函数，MSE 的价值远不止于简单的[线性回归](@entry_id:142318)。它的核心思想——最小化预测与目标之间平方欧氏距离的均值——构成了众多高级模型和跨学科应用的基石。

本章旨在拓宽视野，展示 MSE 如何在更复杂、更真实、更具挑战性的场景中被应用、扩展和改造。我们将不再重复 MSE 的基本定义，而是聚焦于其在不同领域的实用性，探索当基本假设（如数据[独立同分布](@entry_id:169067)、噪声同[方差](@entry_id:200758)）不成立时，研究人员如何巧妙地调整和增强 MSE 框架以解决实际问题。通过这些应用，我们将看到 MSE 不仅是一个简单的[损失函数](@entry_id:634569)，更是一种灵活的设计原则，深刻地影响着从[无监督学习](@entry_id:160566)到科学计算，再到因果推断等多个前沿领域。

### MSE 作为[表示学习](@entry_id:634436)与生成的基础

均方误差最直观的应用之一是学习数据的有效表示。在这种[范式](@entry_id:161181)下，模型的目标是捕捉并重构输入数据本身，而 MSE 成为了衡量重构质量的天然标尺。

#### 自编码器与[特征学习](@entry_id:749268)

自编码器（Autoencoder）是利用 MSE 进行无监督[表示学习](@entry_id:634436)的经典范例。一个自编码器由编码器 $h_\theta$ 和解码器 $g_\phi$ 组成，其目标是最小化重构误差，即 $L_{\text{MSE}} = \frac{1}{n}\sum_i \|g_\phi(h_\theta(x_i)) - x_i\|^2$。这里的核心思想是，通过在编码器和解码器之间设置一个“瓶颈”（即让潜在表示的维度远小于输入维度），模型被迫学习数据中最重要的、最具代表性的特征，以便能够从压缩的表示中尽可能完美地重构原始输入。

然而，简单地最小化 MSE 可能会导致平凡解。如果潜在维度不小于输入维度，且[模型容量](@entry_id:634375)足够大，自编码器可能学会一个[恒等变换](@entry_id:264671)，使得重构误差为零，但这并未学到任何有意义的特征。更糟糕的是，对于有限的数据集，如果潜在维度足够大，模型甚至可以通过类似“查表”的方式记住每一个训练样本，从而在[训练集](@entry_id:636396)上达到零误差，却完全丧失了泛化能力。为了避免这些退化解，除了设置维度瓶颈外，研究人员还引入了多种正则化策略。例如，去噪自编码器通过对输入添加噪声并要求模型重构原始的、干净的输入来迫使模型学习更鲁棒的特征。而压缩自编码器则通过在[损失函数](@entry_id:634569)中增加一项惩罚编码器[雅可比矩阵](@entry_id:264467)范数的项，来鼓励模型学习对输入变化不敏感的平滑编码。这些技术都旨在确保通过最小化 MSE 学到的表示是简洁而有意义的。

#### 图像重构中的感知挑战

在[图像处理](@entry_id:276975)领域，例如图像超分辨率、[去噪](@entry_id:165626)或修复，MSE 同样是衡量重构图像与[原始图](@entry_id:262918)像之间保真度的常用指标。最小化像素级的 MSE 能够有效地提升峰值信噪比（PSNR），这是一个广泛使用的[图像质量](@entry_id:176544)客观评价指标，因为它与 MSE 直接相关。

然而，过度依赖像素级 MSE 往往会导致一个悖论：生成的图像在 PSNR 上得分很高，但在视觉上却显得模糊不清，缺乏真实的纹理和细节。这种现象的原因在于，当面对不确定性（例如，一个低分辨率像素块可能对应多种高频纹理）时，MSE 会驱使模型预测所有可能真实图像的“平均值”，而这种平均效应会抹平高频细节，导致图像过于平滑。相比之下，结构相似性指数（SSIM）等更符合人类视觉感知的指标，不仅关注像素差异，还关注局部亮度、对比度和结构信息，因此对模糊更为敏感。

为了解决这一问题，研究者们提出将 MSE 与[感知损失](@entry_id:635083)（Perceptual Loss）相结合。[感知损失](@entry_id:635083)不再直接比较像素值，而是在一个预训练的[深度神经网络](@entry_id:636170)（如 VGG）的特征空间中计算 MSE。其形式为 $\mathcal{L}_{\text{perc}} = \sum_k w_k \| \phi_k(\hat{y}) - \phi_k(y) \|_2^2$，其中 $\phi_k$ 是网络第 $k$ 层的[特征提取器](@entry_id:637338)。通过最小化一个联合损失 $\mathcal{L}_{\text{joint}} = \mathcal{L}_{\text{MSE}} + \lambda \mathcal{L}_{\text{perc}}$，模型在保证基本像素保真度的同时，也被激励去匹配高层次的感知特征，从而生成更清晰、更真实的图像。调节权重 $\lambda$ 可以在保真度（高 PSNR）与感知质量（高 SSIM）之间进行权衡。

#### 孪生网络中的[度量学习](@entry_id:636905)

MSE 的应用并不局限于输入重构。在[度量学习](@entry_id:636905)中，MSE 可以被创造性地用于塑造一个有意义的[嵌入空间](@entry_id:637157)。在孪生网络（Siamese Network）的一种设置中，我们的目标不是让嵌入向量本身接近某个目标，而是让嵌入向量之间的*差异*去匹配一个预设的目标差异。

具体来说，对于一对输入 $(x_i, x_j)$，模型 $f_\theta$ 产生嵌入 $y_i = f_\theta(x_i)$ 和 $y_j = f_\theta(x_j)$。如果我们有一个目标差异向量 $d_{ij}$，就可以定义一个成对的 MSE 损失：$L = \sum_{(i,j)} \| (y_i - y_j) - d_{ij} \|_2^2$。如果所有的目标差异 $d_{ij}$ 都为零，最小化这个损失会迫使所有嵌入向量坍缩到同一点，这是一种不希望出现的退化。然而，如果目标差异非零且满足一定的“循环一致性”（即对于任意三点 $i, j, k$，都有 $d_{ij} + d_{jk} + d_{ki} = \mathbf{0}$），那么最小化此 MSE 将会成功地构建一个[嵌入空间](@entry_id:637157)，其几何结构精确地反映了这些目标关系。这种方法展示了 MSE 原则的灵活性，能够从直接的数值拟合扩展到对抽象结构关系的建模。

### 扩展 MSE 以适应结构化与复杂数据

标准的 MSE 损失隐含了一个强假设：所有样本的误差是[独立同分布](@entry_id:169067)的，且每个输出维度的重要性相同。然而，在现实世界的许多应用中，这些假设并不成立。本节将探讨如何通过加权和改造 MSE 来处理具有复杂依赖结构和非均匀噪声的数据。

#### 广义 MSE：处理异[方差](@entry_id:200758)与[相关噪声](@entry_id:137358)

在许多科学测量中，数据的噪声水平并非恒定，而是随情况变化（异[方差](@entry_id:200758)），并且不同输出维度之间的噪声可能是相关的。例如，在天文学中，从不同巡天项目获得的[光度学](@entry_id:178667)数据，其[测量误差](@entry_id:270998)的[协方差矩阵](@entry_id:139155) $\boldsymbol{\Sigma}$ 会因观测条件和仪器的不同而异。同样，在地理空间测绘中，预测高程图块时，沿山脊线和垂直于山脊线的[误差方差](@entry_id:636041)可能不同，且邻近像素的误差会相互关联。 

在这种情况下，使用标准的 MSE 损失（即 $\sum \|\mathbf{y}_i - f_\theta(\mathbf{x}_i)\|_2^2$）是不恰当的，因为它等价于假设所有样本的噪声协[方差](@entry_id:200758)都是 $\sigma^2 \mathbf{I}$。从[最大似然估计](@entry_id:142509)（Maximum Likelihood Estimation, MLE）的原则出发，对于服从高斯分布的噪声 $\boldsymbol{\varepsilon}_i \sim \mathcal{N}(\mathbf{0}, \boldsymbol{\Sigma}_i)$，正确的[损失函数](@entry_id:634569)（[负对数似然](@entry_id:637801)）应该是[马氏距离](@entry_id:269828)（Mahalanobis distance）的平方和：
$$
L_{\Sigma} = \frac{1}{n} \sum_{i=1}^{n} (\mathbf{y}_i - f_\theta(\mathbf{x}_i))^{\top} \boldsymbol{\Sigma}_i^{-1} (\mathbf{y}_i - f_\theta(\mathbf{x}_i))
$$
这里的 $\boldsymbol{\Sigma}_i^{-1}$ 是[精度矩阵](@entry_id:264481)，它起到了加权的作用。直观地看，这个损失函数会降低那些具有高不确定性（即 $\boldsymbol{\Sigma}_i$ 的[特征值](@entry_id:154894)较大）的样本或维度的权重，同时放大那些我们更有信心的测量的影响。

这个加权的 MSE 也可以被理解为一个“白化”（whitening）过程。存在一个“白化”矩阵 $\mathbf{W}_i$ 满足 $\mathbf{W}_i^{\top} \mathbf{W}_i = \boldsymbol{\Sigma}_i^{-1}$（例如，$\mathbf{W}_i = \boldsymbol{\Sigma}_i^{-1/2}$），使得上述损失等价于在变换后的空间中计算标准的 MSE：$L_{\Sigma} = \frac{1}{n} \sum_i \| \mathbf{W}_i(\mathbf{y}_i - f_\theta(\mathbf{x}_i)) \|_2^2$。这个视角在实现上很有用，因为它将一个加权[最小二乘问题](@entry_id:164198)转化为了一个标准的最小二乘问题。

#### 同时学习预测与不确定性

在前面的例子中，我们假设噪声协[方差](@entry_id:200758) $\boldsymbol{\Sigma}_i$ 是已知的。然而，在许多情况下，不确定性本身也需要从数据中学习。例如，在金融市场中，预测资产回报的波动性（即不确定性）与预测回报本身同样重要。

我们可以设计一个[神经网](@entry_id:276355)络，它有两个输出头：一个预测均值 $f_\theta(x)$，另一个预测[方差](@entry_id:200758) $\sigma_\phi(x)^2$。再次从高斯 MLE 出发，对应的损失函数（[负对数似然](@entry_id:637801)）变为：
$$
L(\theta, \phi) = \frac{1}{n} \sum_{i=1}^{n} \left[ \log(\sigma_\phi(x_i)^2) + \frac{(y_i - f_\theta(x_i))^2}{\sigma_\phi(x_i)^2} \right]
$$
这个损失函数包含两项：一项是经预测[方差](@entry_id:200758)加权的 MSE，另一项是 $\log(\sigma_\phi(x_i)^2)$。第二项起到了至关重要的正则化作用：如果没有它，模型可以通过预测无限大的[方差](@entry_id:200758) $\sigma_\phi(x_i)^2 \to \infty$ 来使第一项趋于零，从而得到一个无意义的解。对数[方差](@entry_id:200758)项惩罚了这种行为，迫使模型只有在数据确实表现出高可[变性](@entry_id:165583)时才预测高[方差](@entry_id:200758)。通过最小化这个联合损失，模型不仅学会了预测目标值，还学会了对其预测的“自信程度”，这对于风险评估和决策至关重要。我们可以通过检查归一化残差平方 $\frac{(y_i - f_\theta(x_i))^2}{\sigma_\phi(x_i)^2}$ 的均值是否接近 1 来评估[模型不确定性](@entry_id:265539)预测的校准程度。

#### 图结构数据上的 MSE

当数据点之间存在显式的关系结构时，例如社交网络或[分子结构](@entry_id:140109)，[独立同分布](@entry_id:169067)的假设便不再适用。[图神经网络](@entry_id:136853)（Graph Neural Networks, GNNs）正是为处理此类数据而设计的。在节点级别的回归任务中，MSE 仍然是核心的[损失函数](@entry_id:634569)，形式为 $L = \sum_{v \in \mathcal{V}} (\hat{y}_v - y_v)^2$，其中 $\hat{y}_v$ 是模型对节点 $v$ 的预测。

然而，这里的预测 $\hat{y}_v$ 是通过聚合邻居节点信息的“消息传递”机制计算出来的。这意味着单个节点的[预测误差](@entry_id:753692)会通过图的边在[反向传播](@entry_id:199535)过程中影响其邻居节点的参数更新。具体而言，对某个节[点特征](@entry_id:155984) $x_u$ 的梯度，会包含其所有邻居节点 $v \in \mathcal{N}(u)$ 的预测误差项。图的结构直接决定了梯度流的路径和大小。例如，在简单的 GNN 中，一个高度数节点的特征会影响许多其他节点的预测，从而在梯度计算中累积来自多方的误差信号。为了防止[梯度爆炸](@entry_id:635825)和“[过度平滑](@entry_id:634349)”（即所有节点表示趋同）等问题，GNN 架构中通常会采用诸如度归一化（degree normalization）等策略，这在本质上也是一种对 MSE 框架下信息流动的隐式加权与控制。

### MSE 在动态与序贯过程中的应用

许多现实世界的问题都涉及时间或序列依赖性。在这些动态系统中，MSE 及其变体被用于建模演化、控制行为和预测未来。

#### 时间序列与自[相关误差](@entry_id:268558)

在处理时间[序列数据](@entry_id:636380)时，一个常见的陷阱是忽略了噪声的序列相关性。假设我们用一个模型 $f_\theta(x_t)$ 去拟合一个时间序列 $y_t$，并最小化 MSE $\frac{1}{T}\sum_t (y_t - f_\theta(x_t))^2$。如果残差 $\varepsilon_t = y_t - f_\theta(x_t)$ 并非独立同分布，而是存在自相关（例如，$\varepsilon_t$ 与 $\varepsilon_{t-1}$ 相关），那么标准的统计推断就会失效。

尽管在这种情况下，最小化 MSE（即[普通最小二乘法](@entry_id:137121)，OLS）得到的[参数估计](@entry_id:139349)在很多条件下仍然是无偏和一致的，但所有基于 [i.i.d. 假设](@entry_id:634392)计算出的[置信区间](@entry_id:142297)和假设检验都将是错误的。例如，当残差正相关时，OLS 会严重低估[参数估计](@entry_id:139349)的真实[方差](@entry_id:200758)，导致我们对模型的精度过于自信。解决这个问题的经典方法是使用[广义最小二乘法](@entry_id:272590)（Generalized Least Squares, GLS），它通过引入残差的协方差矩阵 $\mathbf{\Sigma}$ 来对损失函数进行修正，其形式恰好就是我们之前讨论过的[马氏距离](@entry_id:269828) $\mathbf{r}^\top \mathbf{\Sigma}^{-1} \mathbf{r}$。这再次说明，理解 MSE 的局限性并从更一般的[概率模型](@entry_id:265150)出发，是处理复杂数据依赖性的关键。

#### [强化学习](@entry_id:141144)中的自举与稳定性

在强化学习（Reinforcement Learning, RL）中，MSE 是 Q-learning 等时序差分（Temporal Difference, TD）学习算法的核心。其目标是学习一个动作价值函数 $Q(s, a)$，来预测在状态 $s$ 执行动作 $a$ 后能够获得的未来累积回报。TD 算法通过最小化所谓的 TD 误差的平方来更新 $Q$ 函数，即 $\delta_t^2 = (y_t - Q_t(s,a))^2$。

这里的关键挑战在于 TD 目标 $y_t = r_t + \gamma \max_{a'} Q_t(s', a')$ 是“自举”（bootstrapped）的——它依赖于模型自身的预测 $Q_t$。这意味着模型在追逐一个“移动的目标”。这种自引用会引发不稳定性，导致 Q 值估计产生剧烈[振荡](@entry_id:267781)甚至发散。为了缓解这个问题，一个里程碑式的技术是引入“[目标网络](@entry_id:635025)”（Target Network）。[目标网络](@entry_id:635025)是主网络的一个周期性冻结的副本 $Q^{-}$，用于计算 TD 目标，即 $y_t = r_t + \gamma \max_{a'} Q^{-}(s', a')$。通过将目标暂时固定，MSE 的优化过程变得更加稳定。分析表明，使用[目标网络](@entry_id:635025)能显著减小每一步更新中 TD 误差的收缩因子，从而有效抑制[振荡](@entry_id:267781)。此外，还可以通过在损失函数中增加一项惩罚连续两次更新之间 Q 值变化的 MSE 项 $\lambda (Q_{\theta_t} - Q_{\theta_{t-1}})^2$，来进一步平滑学习过程。

#### 科学计算中的[偏微分方程](@entry_id:141332)代理模型

近年来，利用[神经网](@entry_id:276355)络作为[求解偏微分方程](@entry_id:138485)（PDEs）的代理模型（surrogate model）已成为科学计算的一个热门方向。一种常见的方法是，首先用高精度但计算昂贵的[数值模拟](@entry_id:137087)器生成大量“数据”（即在不同参数和位置上的 PDE 解），然后训练一个[神经网](@entry_id:276355)络来最小化与这些模拟数据之间的 MSE，从而学习输入（如边界条件、几何参数）到输出（PDE 解）的映射。

这种数据驱动的方法在处理“刚性”（stiff）PDE 时会遇到挑战。例如，在[流体力学](@entry_id:136788)中，当[对流](@entry_id:141806)远大于[扩散](@entry_id:141445)时，解会在某些区域（如[边界层](@entry_id:139416)）出现极其剧烈的变化。在这种情况下，即便是模拟器边界条件上一个微小的系统性偏差，也可能导致在大部分求解域上产生一个常数的偏差。这意味着，一个在这种有偏数据上通过最小化 MSE 训练出来的“完美”代理模型，其预测会系统性地偏离真实的物理。

为了增强模型的鲁棒性和物理一致性，研究者们发展了“物理信息神经网络”（Physics-Informed Neural Networks, PINNs）。其核心思想是在标准的 MSE 数据拟合项之外，增加一个惩罚 PDE 方程本身残差的 MSE 项：$\lambda \cdot \mathbb{E}[(\epsilon \hat{u}'' - c \hat{u}')^2]$。这个“物理损失”项在求解域内的大量[配置点](@entry_id:169000)上计算，它驱使模型的解在没有数据的地方也遵守已知的物理定律，从而减少了对可能带有噪声或偏差的训练数据的过度依赖。此外，通过对问题进行恰当的无量纲化，可以改善 PDE 各项的尺度，从而使整体[损失函数](@entry_id:634569)的[优化景观](@entry_id:634681)更为良好，这对于成功训练模型至关重要。

### 高级[范式](@entry_id:161181)与跨学科前沿

MSE 的思想渗透到了更广泛的[机器学习范式](@entry_id:637731)和跨学科领域中，展现出其强大的适应性和理论深度。

#### [半监督学习](@entry_id:636420)中的一致性正则化

在许多实际应用中，获取大量未标记数据是廉价的，而获取标记数据则成本高昂。[半监督学习](@entry_id:636420)（Semi-Supervised Learning, SSL）旨在同时利用这两[类数](@entry_id:156164)据。一种强大的 SSL 思想是“一致性正则化”，它基于一个平滑性假设：如果两个输入点在输入空间中很接近，那么它们的标签也应该相似。

这个思想可以通过一个类似 MSE 的损失项来实现。对于一个未标记的样本 $x$，我们可以生成它的一个增强版本 $a \cdot x$（例如，通过旋转、裁剪或添加噪声）。一致性正则化会惩罚模型在这两个版本上预测值的差异。总的[损失函数](@entry_id:634569)就变成了在标记数据上的标准监督损失（如 MSE）与在未标记数据上的一致性损失的加权和：$L = L_{\text{sup}} + \lambda \sum_{x \in \mathcal{D}_u} \| f_\theta(x) - f_\theta(a \cdot x) \|^2$。

这个方法之所以有效，是因为它利用了未标记数据来约束模型的[函数空间](@entry_id:143478)。如果增强变换保持标签不变（一个正确的[归纳偏置](@entry_id:137419)），那么这个正则化项可以显著降低模型估计的[方差](@entry_id:200758)，尤其是在标记数据稀少时，从而提高泛化性能。然而，如果这个[不变性](@entry_id:140168)假设是错误的，一致性正则化会引入系统性偏差，可能反而会损害性能。这揭示了在应用 MSE 原则时，理解其背后假设的重要性。

#### 模型蒸馏中的知识传递

模型[蒸馏](@entry_id:140660)是一种将一个大型、复杂的“教师”模型的知识迁移到一个小型、高效的“学生”模型中的技术。除了让学生模型学习真实标签外，我们还可以让它学习教师模型的输出。一种直接的方法是使用 MSE 来匹配教师和学生的“logits”（即 softmax 层之前的原始输出向量）：$L_{\text{distill}} = \| \mathbf{z}_{\text{s}} - \mathbf{z}_{\text{t}} \|_2^2$。

教师模型的 logits 包含了比硬标签更丰富的信息，例如类别之间的相似性关系（所谓的“[暗知识](@entry_id:637253)”）。通过最小化 logits 间的 MSE，学生模型被迫去模仿教师模型的“思考过程”，而不仅仅是最终的分类结果。这种方法与另一种常见的基于 KL 散度的蒸馏方法形成了有趣的对比。基于 logits 的 MSE 损失对于教师模型输出的“温度”缩放不敏感，而 KL 散度则不然。选择哪种损失取决于我们希望传递何种类型的知识。

#### 因果推断中的倾向加权

在医学、经济学和社会科学中，从观测数据中估计干预措施的因果效应是一个核心挑战。一个关键目标是估计个体[处理效应](@entry_id:636010)（Individual Treatment Effect, ITE），即 $\tau(x) = \mathbb{E}[Y(1) - Y(0) \mid X=x]$。由于我们对每个个体只能观测到一种结果（接受处理或未接受处理），ITE 无法直接计算。

一种先进的方法是构建一个“[伪结](@entry_id:168307)果” $\tilde{\tau}$，它在给定[协变](@entry_id:634097)量 $X$ 的条件下是 ITE 的[无偏估计](@entry_id:756289)。然后，我们可以训练一个模型 $f_\theta(x)$ 来拟合这些[伪结](@entry_id:168307)果。然而，由于观测数据中存在“[混淆偏倚](@entry_id:635723)”（即接受处理的个体与未接受处理的个体在协变量上存在系统性差异），直接最小化 MSE $\| f_\theta(X) - \tilde{\tau} \|^2$ 会得到有偏的估计。

为了纠正这种偏倚，研究人员使用“[倾向得分](@entry_id:635864)”（propensity score）$e(x) = \mathbb{P}(T=1 \mid X=x)$ 来对 MSE 损失进行加权。一种常用的加权 MSE [风险函数](@entry_id:166593)是：$\mathcal{R}(f) = \mathbb{E}[w(X,T)(f(X) - \tilde{\tau})^2]$，其中权重 $w(X,T) = \frac{T}{e(X)} + \frac{1-T}{1-e(X)}$。这种[逆概率](@entry_id:196307)加权（Inverse Propensity Weighting, IPW）方法创建了一个伪总体，其中处理分配与协变量无关，从而消除了[混淆偏倚](@entry_id:635723)。在这种加权方案下，最小化 MSE [风险函数](@entry_id:166593)确实能够一致地估计出我们关心的 ITE $\tau(x)$。然而，这种方法也引入了其自身的挑战：当[倾向得分](@entry_id:635864)接近 0 或 1 时，权重会变得极大，导致估计的[方差](@entry_id:200758)急剧膨胀。这再次体现了在高级应用中，对 MSE 框架的精巧改造往往伴随着新的权衡。

#### [计算神经科学](@entry_id:274500)中的[预测编码](@entry_id:150716)

最后，MSE 的核心思想——最小化[预测误差](@entry_id:753692)——与[计算神经科学](@entry_id:274500)中一个有影响力的理论“[预测编码](@entry_id:150716)”（predictive coding）不谋而合。该理论认为，大脑是一个分层的预测机器，每一层都在不断地尝试预测来自下一层的输入。大脑不只是被动地处理感官信息，而是主动地生成关于世界原因的假设，并利用感官输入来更新这些假设。

在这个框架下，神经活动主要传递的是“预测误差”——即预测与实际输入之间的差异。当一个预测误差产生时，它会向上传播，以修正更高层次的内部模型（假设），从而在未来做出更好的预测。学习过程被认为是通过调整突触权重来最小化长期的预测误差。

我们可以将这一过程与基于 MSE 的[梯度下降](@entry_id:145942)联系起来。对于一个简单的线性神经元 $f_w(x) = w^\top x$，最小化期望平方误差 $\mathbb{E}[(y - w^\top x)^2]$ 的[随机梯度下降](@entry_id:139134)更新规则为 $w \leftarrow w + \eta \cdot (y - f_w(x)) \cdot x$。这个更新规则可以被解释为一个“三因子”Hebbian 学习：突触权重的改变取决于突触前活动（$x$）、突触后活动（与 $f_w(x)$ 相关）以及一个全局的调节信号（预测误差 $e = y - f_w(x)$）。这种局部学习规则被认为比标准[反向传播算法](@entry_id:198231)（尤其是在深度网络中存在的“权重传输问题”）在生物学上更具合理性。因此，MSE 优化不仅是机器学习的有效工具，也为理解大脑的学习机制提供了一个强大的理论隐喻。