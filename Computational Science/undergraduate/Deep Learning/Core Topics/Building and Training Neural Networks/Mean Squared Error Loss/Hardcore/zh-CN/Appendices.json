{
    "hands_on_practices": [
        {
            "introduction": "在监督学习中，损失函数用于量化模型预测值与真实值之间的差异。均方误差（MSE）是其中最常用的损失函数之一，但其“平方”特性究竟意味着什么？这个练习通过一个简单的计算，直观地比较了均方误差（$L_2$ 损失）和绝对误差（$L_1$ 损失）在面对相同大小的预测偏差时所产生的惩罚值，从而揭示 MSE 对较大误差的敏感性。",
            "id": "1931773",
            "problem": "一个气象研究所的数据科学团队正在评估一个新的天气预测模型。该模型的准确性通过损失函数进行评估，损失函数量化了对不正确预测的惩罚。设某天的真实测量温度表示为 $y$，模型的预测温度表示为 $\\hat{y}$。\n\n正在考虑两种常见的损失函数：\n1.  平方误差损失，定义为 $L_2(y, \\hat{y}) = (y - \\hat{y})^2$。\n2.  绝对误差损失，定义为 $L_1(y, \\hat{y}) = |y - \\hat{y}|$。\n\n在南极洲的一个特定测试日，模型的温度预测偏差恰好为 $3.5$ 开尔文。计算对于这个特定的预测，平方误差损失所赋予的惩罚与绝对误差损失所赋予的惩罚之比的数值。",
            "solution": "问题要求计算在给定预测误差下，平方误差损失与绝对误差损失的比率。设真实温度为 $y$，预测温度为 $\\hat{y}$。\n\n平方误差损失由以下公式给出：\n$$L_2(y, \\hat{y}) = (y - \\hat{y})^2$$\n\n绝对误差损失由以下公式给出：\n$$L_1(y, \\hat{y}) = |y - \\hat{y}|$$\n\n我们已知预测误差的大小为 $3.5$ 开尔文。这意味着真实值与预测值之间的绝对差为 $3.5$。用数学方式可以写成：\n$$|y - \\hat{y}| = 3.5$$\n\n现在，我们可以为这个特定的误差计算每个损失函数的值。\n\n对于绝对误差损失，我们可以直接代入给定的误差大小：\n$$L_1 = |y - \\hat{y}| = 3.5$$\n来自绝对误差损失的惩罚是 $3.5$。\n\n对于平方误差损失，我们使用 $(y-\\hat{y})^2 = (|y-\\hat{y}|)^2$ 这一事实。代入给定的误差大小：\n$$L_2 = (y - \\hat{y})^2 = (|y - \\hat{y}|)^2 = (3.5)^2$$\n计算其值：\n$$L_2 = 3.5 \\times 3.5 = 12.25$$\n来自平方误差损失的惩罚是 $12.25$。\n\n问题要求的是平方误差损失与绝对误差损失的比率。我们称这个比率为 $R$。\n$$R = \\frac{L_2(y, \\hat{y})}{L_1(y, \\hat{y})}$$\n\n代入计算出的损失值：\n$$R = \\frac{12.25}{3.5}$$\n\n为了简化这个分数，我们可以将 $12.25$ 表示为 $(3.5)^2$：\n$$R = \\frac{(3.5)^2}{3.5} = 3.5$$\n\n因此，对于 $3.5$ 开尔文的预测误差，平方误差损失的惩罚是绝对误差损失惩罚的 $3.5$ 倍。温度的单位（误差为开尔文，$L_2$ 为开尔文$^2$，$L_1$ 为开尔文）在比率中被抵消，从而得到一个无量纲的量。",
            "answer": "$$\\boxed{3.5}$$"
        },
        {
            "introduction": "理解了均方误差如何惩罚错误之后，下一个关键问题是：最小化均方误差究竟能让模型学到什么？本练习将引导你从第一性原理出发，推导出最小化 MSE 的最优常数预测值恰好是目标数据的均值这一核心性质。通过一个包含样本复制（即加权）的场景，你将进一步探索数据分布如何通过影响这个均值来影响模型训练的结果。",
            "id": "3148476",
            "problem": "考虑一个监督学习场景，其中模型根据一个离散输入预测一个标量目标。设数据集为 $\\{(x_i,y_i)\\}_{i=1}^n$，其中 $x_i \\in \\mathcal{X}$ 且 $y_i \\in \\mathbb{R}$。定义单样本损失为平方误差 $\\ell(\\hat{y},y) = (\\hat{y}-y)^2$，并定义在子集 $\\mathcal{D} \\subseteq \\{1,\\dots,n\\}$ 上的均方误差 (MSE) 经验风险为 $L(\\theta \\mid \\mathcal{D}) = \\sum_{i \\in \\mathcal{D}} (\\theta - y_i)^2$，其中标量预测器 $\\theta$ 在该子集内为常数。在具有离散输入的条件预测设置中，考虑一个为每个输入值预测一个单独常数的模型：对于每个 $x \\in \\mathcal{X}$，模型返回 $f(x) = \\theta_x$，经验风险按输入值分解为 $L_{\\text{emp}}(\\{\\theta_x\\}) = \\sum_{x \\in \\mathcal{X}} \\sum_{i : x_i = x} (\\theta_x - y_i)^2$。\n\n从上述定义和凸优化的第一性原理（例如，通过将导数设为零来最小化一个可微凸二次函数）出发，推导当该输入的样本可能被复制时，何种标量 $\\theta_x$ 能最小化限制在单个输入值 $x$ 上的经验风险。设每个样本索引 $i$ 带有一个非负整数复制计数 $c_i \\in \\{0,1,2,\\dots\\}$，并假设输入 $x$ 的经验风险为 $L_x(\\theta_x) = \\sum_{i : x_i = x} c_i \\, (\\theta_x - y_i)^2$。用 $\\{y_i\\}$ 和 $\\{c_i\\}$ 表示该最小化器，并解释为什么复制计数实际上充当了权重，可以将最小化器偏离未加权的条件均值。\n\n然后，实现一个程序来执行以下确定性模拟。使用一个包含两个输入值和以下标签的数据集：\n- 对于 $x=0$，设标签 $y$ 值为 $[1.0, 3.0]$。\n- 对于 $x=1$，设标签 $y$ 值为 $[0.0, 4.0]$。\n\n对于下方的每个测试用例，设每个样本的复制计数由与每个输入值的标签顺序对齐的数组指定。例如，对于 $x=0$，第一个计数应用于 $y=1.0$，第二个计数应用于 $y=3.0$；对于 $x=1$，第一个计数应用于 $y=0.0$，第二个计数应用于 $y=4.0$。对每个测试用例：\n- 计算原始的每个输入的经验最小化器（所有复制计数等于 $1$）。\n- 使用提供的计数计算复制后的每个输入的经验最小化器。\n- 对于每个输入值 $x \\in \\{0,1\\}$，输出差值 $\\Delta_x = \\theta_x^{\\text{dup}} - \\theta_x^{\\text{orig}}$。\n\n设计一个测试套件，覆盖不同的复制场景：\n- 测试用例 $1$ (正常路径)：$x=0$ 计数为 $[1,1]$，$x=1$ 计数为 $[1,3]$。\n- 测试用例 $2$ (边界情况，无复制)：$x=0$ 计数为 $[1,1]$，$x=1$ 计数为 $[1,1]$。\n- 测试用例 $3$ (边缘情况，$x=0$ 中有大量复制)：$x=0$ 计数为 $[5,1]$，$x=1$ 计数为 $[1,1]$。\n- 测试用例 $4$ (边缘情况，两个输入中都有平衡复制)：$x=0$ 计数为 $[2,2]$，$x=1$ 计数为 $[2,2]$。\n- 测试用例 $5$ (边缘情况，通过零复制移除 $x=1$ 中的一个标签)：$x=0$ 计数为 $[1,1]$，$x=1$ 计数为 $[0,2]$。\n\n要求的最终输出格式是包含结果列表的单行文本，每个测试用例一个结果，其中每个结果是该用例的列表 $[\\Delta_0,\\Delta_1]$。格式必须是方括号括起来的逗号分隔列表，例如 $[[\\Delta_0^{(1)},\\Delta_1^{(1)}],[\\Delta_0^{(2)},\\Delta_1^{(2)}],\\dots]$，数值以标准十进制浮点数表示。不适用物理单位或角度单位；所有数值输出都应为实数。你的程序应只生成包含此聚合列表的一行，不得包含任何其他文本。",
            "solution": "问题要求推导最优标量预测器 $\\theta_x$，该预测器能最小化特定输入值 $x$ 的加权经验风险。风险函数定义为加权平方误差和。\n\n设与离散输入值 $x \\in \\mathcal{X}$ 对应的样本索引集表示为 $I_x = \\{i \\mid x_i = x\\}$。包含非負整数复制计数 $c_i$ 的该输入值的经验风险由以下公式给出：\n$$\nL_x(\\theta_x) = \\sum_{i \\in I_x} c_i (\\theta_x - y_i)^2\n$$\n此处，$\\theta_x$ 是对输入 $x$ 的常数预测，$\\{y_i\\}_{i \\in I_x}$ 是对应的目标值，$\\{c_i\\}_{i \\in I_x}$ 是复制计数。\n\n函数 $L_x(\\theta_x)$ 是一个关于 $\\theta_x$ 的二次函数。它可以展开为：\n$$\nL_x(\\theta_x) = \\sum_{i \\in I_x} c_i (\\theta_x^2 - 2\\theta_x y_i + y_i^2) = \\left(\\sum_{i \\in I_x} c_i\\right)\\theta_x^2 - \\left(2\\sum_{i \\in I_x} c_i y_i\\right)\\theta_x + \\left(\\sum_{i \\in I_x} c_i y_i^2\\right)\n$$\n只要 $\\theta_x^2$ 项的系数 $\\sum_{i \\in I_x} c_i$ 为正，这就是一个关于 $\\theta_x$ 的开口向上的抛物线。如果输入 $x$ 至少有一个样本的计数 $c_i > 0$，则此条件成立。在此条件下，该函数是严格凸的，并具有唯一的全局最小值。\n\n为了找到最小化 $L_x(\\theta_x)$ 的 $\\theta_x$ 值，我们应用最优性的一阶必要条件，即求 $L_x(\\theta_x)$ 关于 $\\theta_x$ 的导数并将其设为零。\n\n$$\n\\frac{dL_x(\\theta_x)}{d\\theta_x} = \\frac{d}{d\\theta_x} \\left( \\sum_{i \\in I_x} c_i (\\theta_x - y_i)^2 \\right)\n$$\n\n根据微分的线性性质，我们可以将导数移到求和符号内部：\n$$\n\\frac{dL_x(\\theta_x)}{d\\theta_x} = \\sum_{i \\in I_x} \\frac{d}{d\\theta_x} \\left( c_i (\\theta_x - y_i)^2 \\right)\n$$\n\n应用链式法则，其中 $\\frac{d}{du}(u^2) = 2u$：\n$$\n\\frac{dL_x(\\theta_x)}{d\\theta_x} = \\sum_{i \\in I_x} c_i \\cdot 2(\\theta_x - y_i) \\cdot \\frac{d}{d\\theta_x}(\\theta_x - y_i) = \\sum_{i \\in I_x} 2 c_i (\\theta_x - y_i)\n$$\n\n将导数设为零以找到临界点：\n$$\n\\sum_{i \\in I_x} 2 c_i (\\theta_x - y_i) = 0\n$$\n\n我们可以除以常数 $2$ 并分配求和：\n$$\n\\sum_{i \\in I_x} c_i \\theta_x - \\sum_{i \\in I_x} c_i y_i = 0\n$$\n\n由于 $\\theta_x$ 相对于索引 $i$ 是一个常数，它可以从第一个和式中提出来：\n$$\n\\theta_x \\left( \\sum_{i \\in I_x} c_i \\right) = \\sum_{i \\in I_x} c_i y_i\n$$\n\n解出 $\\theta_x$，我们得到最小化器，记为 $\\theta_x^*$：\n$$\n\\theta_x^* = \\frac{\\sum_{i \\in I_x} c_i y_i}{\\sum_{i \\in I_x} c_i}\n$$\n这里假设 $\\sum_{i \\in I_x} c_i \\neq 0$。如果所有计数都为零，$L_x(\\theta_x)$ 将恒等于零，任何 $\\theta_x$ 都将是最小化器。然而，问题情境确保了在所有测试用例中分母都不为零。\n\n为了确认这个临界点是一个最小值，我们考察二阶导数：\n$$\n\\frac{d^2L_x(\\theta_x)}{d\\theta_x^2} = \\frac{d}{d\\theta_x} \\left( \\sum_{i \\in I_x} 2 c_i (\\theta_x - y_i) \\right) = \\sum_{i \\in I_x} 2 c_i\n$$\n由于计数 $c_i$ 是非负整数且至少有一个为正，二阶导数 $\\sum_{i \\in I_x} 2 c_i > 0$。正的二阶导数证实了函数 $L_x(\\theta_x)$ 是严格凸的，并且临界点 $\\theta_x^*$确实是一个唯一的全局最小值。\n\n推导出的 $\\theta_x^*$ 表达式是目标值 $\\{y_i\\}_{i \\in I_x}$ 的加权平均，其中权重是复制计数 $\\{c_i\\}_{i \\in I_x}$。在没有复制的情况下，所有计数都是 $c_i=1$。表达式便简化为：\n$$\n\\theta_x^* \\text{ (unweighted)} = \\frac{\\sum_{i \\in I_x} y_i}{\\sum_{i \\in I_x} 1} = \\frac{\\sum_{i \\in I_x} y_i}{|I_x|}\n$$\n这是给定输入 $x$ 的目标值的未加权条件均值。\n\n当计数 $c_i$ 不均匀时，它们充当权重，赋予具有更高复制计数的样本更大的重要性。一个具有大计数 $c_i$ 的样本 $(x_i, y_i)$ 会对加权平均的分子和分母都做出更显著的贡献。因此，最优预测器 $\\theta_x^*$ 会被“拉向”那些与更高计数相关联的 $y_i$ 值。这说明了复制计数，或更普遍地说是样本权重，如何能够使平方误差损失的最小化器从无权重的条件均值偏向被认为更重要的特定数据点。\n\n该模拟将应用此公式来计算不同复制场景下的最小化器，并通过差值 $\\Delta_x = \\theta_x^{\\text{dup}} - \\theta_x^{\\text{orig}}$ 来量化所产生的偏差。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and simulates the effect of sample duplication on the minimizer\n    of the Mean Squared Error loss for a conditional model.\n    \"\"\"\n    \n    # Dataset of labels for each discrete input value x.\n    y_values = {\n        0: np.array([1.0, 3.0]),\n        1: np.array([0.0, 4.0])\n    }\n\n    # Test cases defining the duplication counts for each label in y_values.\n    test_cases = [\n        # Test case 1 (normal path)\n        {'counts_x0': [1, 1], 'counts_x1': [1, 3]},\n        # Test case 2 (boundary, no duplication)\n        {'counts_x0': [1, 1], 'counts_x1': [1, 1]},\n        # Test case 3 (edge, heavy duplication in x=0)\n        {'counts_x0': [5, 1], 'counts_x1': [1, 1]},\n        # Test case 4 (edge, balanced duplication in both inputs)\n        {'counts_x0': [2, 2], 'counts_x1': [2, 2]},\n        # Test case 5 (edge, removal of one label in x=1)\n        {'counts_x0': [1, 1], 'counts_x1': [0, 2]},\n    ]\n    \n    def compute_minimizer(y, counts):\n        \"\"\"\n        Computes the weighted average that minimizes the squared error.\n        theta_x = (sum(c_i * y_i)) / (sum(c_i))\n        \"\"\"\n        c = np.array(counts, dtype=float)\n        numerator = np.sum(c * y)\n        denominator = np.sum(c)\n        if denominator == 0:\n            # This case will not be reached in this problem but is good practice.\n            return np.nan \n        return numerator / denominator\n\n    # Compute the original minimizers (all duplication counts are 1).\n    orig_counts = [1, 1]\n    theta0_orig = compute_minimizer(y_values[0], orig_counts)\n    theta1_orig = compute_minimizer(y_values[1], orig_counts)\n\n    results = []\n    \n    for case in test_cases:\n        counts0 = case['counts_x0']\n        counts1 = case['counts_x1']\n        \n        # Compute the duplicated (weighted) minimizers.\n        theta0_dup = compute_minimizer(y_values[0], counts0)\n        theta1_dup = compute_minimizer(y_values[1], counts1)\n        \n        # Calculate the differences.\n        delta0 = theta0_dup - theta0_orig\n        delta1 = theta1_dup - theta1_orig\n        \n        results.append([delta0, delta1])\n        \n    # Format the final output string as a list of lists.\n    # e.g., [[d0,d1],[d0,d1],...]\n    output_str = f\"[{','.join(f'[{d[0]},{d[1]}]' for d in results)}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在现代深度学习中，均方误差损失函数是驱动模型参数更新的核心。为了通过梯度下降法训练网络，我们必须计算损失函数关于模型中每一个可学习参数的偏导数。这个练习将带你进入实际的后台，推导在一个包含层归一化（Layer Normalization）的现代网络结构中，MSE 损失关于其可学习参数 $\\gamma$ 和 $\\beta$ 的梯度，让你亲身体验反向传播算法的计算细节。",
            "id": "3148553",
            "problem": "考虑一个深度学习中的监督学习场景，其小批量大小为 $B$，每个样本的输出维度为 $d$。对于每个样本 $b \\in \\{1,\\dots,B\\}$，网络产生一个原始输出向量 $\\hat{\\mathbf{y}}^{(b)} \\in \\mathbb{R}^{d}$，并针对一个目标向量 $\\mathbf{t}^{(b)} \\in \\mathbb{R}^{d}$ 进行训练。模型在输出端应用层归一化，使用一个在所有特征上共享的标量缩放参数 $\\gamma \\in \\mathbb{R}$ 和一个标量平移参数 $\\beta \\in \\mathbb{R}$。对于每个样本 $b$，定义样本均值\n$$\n\\mu^{(b)} \\equiv \\frac{1}{d} \\sum_{i=1}^{d} \\hat{y}^{(b)}_{i},\n$$\n和样本标准差\n$$\n\\sigma^{(b)} \\equiv \\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} \\left(\\hat{y}^{(b)}_{i} - \\mu^{(b)}\\right)^{2} + \\epsilon},\n$$\n其中 $\\epsilon > 0$ 是一个固定的常数。归一化后的输出分量为\n$$\ny^{(b)}_{\\text{norm},i} \\equiv \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta \\quad \\text{for } i \\in \\{1,\\dots,d\\}.\n$$\n训练使用在整个批次和所有特征上平均的均方误差（MSE）损失函数，\n$$\nL \\equiv \\frac{1}{2 B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right)^{2}.\n$$\n仅从 $L$ 的定义、$\\mu^{(b)}$ 和 $\\sigma^{(b)}$ 的定义以及微分学的标准法则（特别是链式法则和线性性质）出发，推导两个偏导数 $\\frac{\\partial L}{\\partial \\gamma}$ 和 $\\frac{\\partial L}{\\partial \\beta}$ 关于 $\\gamma$、$\\beta$、$\\hat{\\mathbf{y}}^{(b)}$、$\\mathbf{t}^{(b)}$、$\\mu^{(b)}$、$\\sigma^{(b)}$、$B$、$d$ 和 $\\epsilon$ 的封闭形式表达式。将最终答案表示为两个没有未定义符号的显式解析和。不需要数值近似，也不需要四舍五入。你的最终答案必须只包含这两个导数。",
            "solution": "该问题要求推导均方误差（MSE）损失函数 $L$ 关于标量层归一化参数 $\\gamma$ 和 $\\beta$ 的偏导数。推导将从给定的定义出发，使用微分学的标准法则，最主要是链式法则。\n\n损失函数 $L$ 定义为：\n$$L \\equiv \\frac{1}{2 B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right)^{2}$$\n样本 $b$ 和特征 $i$ 的归一化输出 $y^{(b)}_{\\text{norm},i}$ 由下式给出：\n$$y^{(b)}_{\\text{norm},i} \\equiv \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta$$\n样本均值 $\\mu^{(b)}$ 和样本标准差 $\\sigma^{(b)}$ 是从原始网络输出 $\\hat{\\mathbf{y}}^{(b)}$ 计算得出的。因此，它们不依赖于参数 $\\gamma$ 和 $\\beta$。这对于后续的微分计算是一个至关重要的观察。\n\n我们可以通过应用链式法则，建立一个关于泛型参数 $\\theta$ 的 $L$ 的导数的一般公式。\n$$\\frac{\\partial L}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left[ \\frac{1}{2 B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right)^{2} \\right]$$\n根据微分的线性性质，我们可以将微分算子移到求和符号内部：\n$$\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{2 B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\frac{\\partial}{\\partial \\theta} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right)^{2}$$\n对平方项应用链式法则，得到：\n$$\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{2 B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} 2 \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right) \\frac{\\partial}{\\partial \\theta}\\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right)$$\n目标值 $t^{(b)}_{i}$ 相对于模型参数是固定常数，因此它们的导数为零。表达式简化为：\n$$\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right) \\frac{\\partial y^{(b)}_{\\text{norm},i}}{\\partial \\theta}$$\n这个通用形式为求得所需的偏导数提供了一个模板。\n\n**1. 推导关于 $\\gamma$ 的偏导数（$\\frac{\\partial L}{\\partial \\gamma}$）**\n\n我们在通用公式中设 $\\theta = \\gamma$。第一步是计算 $y^{(b)}_{\\text{norm},i}$ 关于 $\\gamma$ 的偏导数：\n$$\\frac{\\partial y^{(b)}_{\\text{norm},i}}{\\partial \\gamma} = \\frac{\\partial}{\\partial \\gamma} \\left( \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta \\right)$$\n由于 $\\beta$、$\\mu^{(b)}$ 和 $\\sigma^{(b)}$ 相对于 $\\gamma$ 是常数，该导数为：\n$$\\frac{\\partial y^{(b)}_{\\text{norm},i}}{\\partial \\gamma} = \\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}}$$\n将此结果代入 $\\frac{\\partial L}{\\partial \\theta}$ 的通用公式中：\n$$\\frac{\\partial L}{\\partial \\gamma} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right) \\left(\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}}\\right)$$\n最后，为了仅用指定的输入变量来表示结果，我们代入 $y^{(b)}_{\\text{norm},i}$ 的定义：\n$$\\frac{\\partial L}{\\partial \\gamma} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left( \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta - t^{(b)}_{i} \\right) \\left( \\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} \\right)$$\n\n**2. 推导关于 $\\beta$ 的偏导数（$\\frac{\\partial L}{\\partial \\beta}$）**\n\n我们在通用公式中设 $\\theta = \\beta$。我们首先计算 $y^{(b)}_{\\text{norm},i}$ 关于 $\\beta$ 的偏导数：\n$$\\frac{\\partial y^{(b)}_{\\text{norm},i}}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} \\left( \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta \\right)$$\n第一项相对于 $\\beta$ 是常数，所以导数就是：\n$$\\frac{\\partial y^{(b)}_{\\text{norm},i}}{\\partial \\beta} = 1$$\n将此结果代入 $\\frac{\\partial L}{\\partial \\theta}$ 的通用公式中：\n$$\\frac{\\partial L}{\\partial \\beta} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left(y^{(b)}_{\\text{norm},i} - t^{(b)}_{i}\\right) (1)$$\n再次，我们代入 $y^{(b)}_{\\text{norm},i}$ 的定义以获得最终表达式：\n$$\\frac{\\partial L}{\\partial \\beta} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left( \\gamma \\,\\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta - t^{(b)}_{i} \\right)$$\n\n这两个推导出的表达式是根据问题陈述中指定的变量表示的封闭形式解析和。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\partial L}{\\partial \\gamma} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left( \\gamma \\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta - t^{(b)}_{i} \\right) \\left( \\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} \\right) \\\\ \n\\frac{\\partial L}{\\partial \\beta} = \\frac{1}{B d} \\sum_{b=1}^{B} \\sum_{i=1}^{d} \\left( \\gamma \\frac{\\hat{y}^{(b)}_{i} - \\mu^{(b)}}{\\sigma^{(b)}} + \\beta - t^{(b)}_{i} \\right)\n\\end{pmatrix}\n}\n$$"
        }
    ]
}