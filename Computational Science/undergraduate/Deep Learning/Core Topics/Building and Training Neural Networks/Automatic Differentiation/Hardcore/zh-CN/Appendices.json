{
    "hands_on_practices": [
        {
            "introduction": "理论学习之后，最佳的巩固方式莫过于亲手实践。本练习将引导你脱离复杂的深度学习框架，手动模拟一次反向模式自动微分的核心过程。通过为一个简单的函数构建计算图并逐步执行反向传播，你将能直观地理解梯度是如何通过链式法则在网络中流动和累积的，这为你深入掌握现代自动微分引擎的工作原理打下坚实基础 。",
            "id": "3100431",
            "problem": "考虑在深度学习背景下的逆向模式自动微分（AD），其中，标量损失函数关于参数的梯度是通过使用链式法则反向遍历计算图来高效计算的。我们感兴趣的函数是标量映射 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$，由 $f(x,y)=\\sin(xy)+\\frac{\\exp(x)}{y}$ 给出，其中 $y\\neq 0$。仅使用与计算图兼容的基本运算（乘法、正弦、指数和除法），构建一个能评估 $f(x,y)$ 的最小中间变量集，并构造一个记录这些运算的父子关系的“磁带”（tape）。然后，利用复合函数的链式法则原理和向量-雅可比积（VJP）的概念，手动推导获得梯度 $\\nabla f(x,y)$ 所需的反向传播（VJP拉回）的确切序列。你的推导应清楚地标明反向遍历磁带的顺序，以及在每一步中对输入伴随变量的局部贡献。以行向量的形式提供 $\\nabla f(x,y)$ 的最终解析表达式。不要四舍五入；最终答案必须是精确的符号表达式。",
            "solution": "该问题陈述是有效的。它具有科学依据，定义明确，客观，并包含足够的信息来推导出唯一且有意义的解。该任务涉及将逆向模式自动微分（AD）应用于一个可微函数，AD是计算微积分和深度学习中的一个基石算法。该过程是可形式化的，并与既定原则相符。\n\n我们的任务是使用逆向模式AD的原理，计算函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$（由 $f(x,y)=\\sin(xy)+\\frac{\\exp(x)}{y}$ 给出，其中 $y \\neq 0$）的梯度 $\\nabla f(x,y)$。这包括一个用于构建计算图和评估函数的前向传播过程，以及一个用于传播梯度的反向传播过程。\n\n首先，我们将函数分解为一系列基本运算。这个序列定义了计算图，或称为“磁带”。设输入为 $v_1 = x$ 和 $v_2 = y$。\n\n**前向传播：构建计算图**\n\n$f(x,y)$ 的求值过程可以由以下最小中间变量集表示：\n1.  $v_3 = v_1 \\cdot v_2 = x \\cdot y$\n2.  $v_4 = \\sin(v_3) = \\sin(xy)$\n3.  $v_5 = \\exp(v_1) = \\exp(x)$\n4.  $v_6 = \\frac{v_5}{v_2} = \\frac{\\exp(x)}{y}$\n5.  $v_7 = v_4 + v_6 = \\sin(xy) + \\frac{\\exp(x)}{y} = f(x,y)$\n\n这个序列构成了前向传播。“磁带”记录了这些操作及其依赖关系：$(v_3, \\text{mul}, v_1, v_2)$, $(v_4, \\sin, v_3)$, $(v_5, \\exp, v_1)$, $(v_6, \\text{div}, v_5, v_2)$, $(v_7, \\text{add}, v_4, v_6)$。\n\n**反向传播：使用链式法则计算梯度**\n\n反向传播计算最终输出 $v_7$ 相对于每个中间变量 $v_i$ 的偏导数，这些偏导数被称为伴随变量，记作 $\\bar{v}_i = \\frac{\\partial f}{\\partial v_i} = \\frac{\\partial v_7}{\\partial v_i}$。该过程从将输出节点的伴随变量初始化为1开始，即 $\\bar{v}_7 = \\frac{\\partial v_7}{\\partial v_7} = 1$。所有其他伴随变量初始化为0。然后我们以逆拓扑顺序遍历该图。\n\n核心原理是链式法则。对于一个操作 $v_k = g(v_i, v_j, \\dots)$，其父节点的伴随变量通过累加子节点的伴随变量乘以局部偏导数来更新：\n$$ \\bar{v}_i = \\bar{v}_i + \\bar{v}_k \\frac{\\partial v_k}{\\partial v_i} $$\n$$ \\bar{v}_j = \\bar{v}_j + \\bar{v}_k \\frac{\\partial v_k}{\\partial v_j} $$\n...依此类推。这个操作实际上是一个向量-雅可比积（VJP）拉回。\n\n让我们按照前向传播的逆序计算伴随变量：\n\n1.  **开始：** 初始化伴随变量：$\\bar{v}_1=0, \\bar{v}_2=0, \\bar{v}_3=0, \\bar{v}_4=0, \\bar{v}_5=0, \\bar{v}_6=0$。\n    为反向传播设置种子：$\\bar{v}_7 = 1$。\n\n2.  **节点 $v_7 = v_4 + v_6$：**\n    父节点是 $v_4$ 和 $v_6$。\n    局部偏导数：$\\frac{\\partial v_7}{\\partial v_4} = 1$, $\\frac{\\partial v_7}{\\partial v_6} = 1$。\n    更新父节点伴随变量：\n    $\\bar{v}_4 \\leftarrow \\bar{v}_4 + \\bar{v}_7 \\cdot \\frac{\\partial v_7}{\\partial v_4} = 0 + 1 \\cdot 1 = 1$。\n    $\\bar{v}_6 \\leftarrow \\bar{v}_6 + \\bar{v}_7 \\cdot \\frac{\\partial v_7}{\\partial v_6} = 0 + 1 \\cdot 1 = 1$。\n    当前状态：$\\bar{v}_4=1, \\bar{v}_6=1$。\n\n3.  **节点 $v_6 = \\frac{v_5}{v_2}$：**\n    父节点是 $v_5$ 和 $v_2$。\n    局部偏导数：$\\frac{\\partial v_6}{\\partial v_5} = \\frac{1}{v_2}$, $\\frac{\\partial v_6}{\\partial v_2} = -\\frac{v_5}{v_2^2}$。\n    更新父节点伴随变量：\n    $\\bar{v}_5 \\leftarrow \\bar{v}_5 + \\bar{v}_6 \\cdot \\frac{\\partial v_6}{\\partial v_5} = 0 + 1 \\cdot \\frac{1}{v_2} = \\frac{1}{y}$。\n    $\\bar{v}_2 \\leftarrow \\bar{v}_2 + \\bar{v}_6 \\cdot \\frac{\\partial v_6}{\\partial v_2} = 0 + 1 \\cdot \\left(-\\frac{v_5}{v_2^2}\\right) = -\\frac{\\exp(x)}{y^2}$。\n    当前状态：$\\bar{v}_5 = \\frac{1}{y}$, $\\bar{v}_2=-\\frac{\\exp(x)}{y^2}$。\n\n4.  **节点 $v_5 = \\exp(v_1)$：**\n    父节点是 $v_1$。\n    局部偏导数：$\\frac{\\partial v_5}{\\partial v_1} = \\exp(v_1)$。\n    更新父节点伴随变量：\n    $\\bar{v}_1 \\leftarrow \\bar{v}_1 + \\bar{v}_5 \\cdot \\frac{\\partial v_5}{\\partial v_1} = 0 + \\frac{1}{y} \\cdot \\exp(v_1) = \\frac{\\exp(x)}{y}$。\n    当前状态：$\\bar{v}_1 = \\frac{\\exp(x)}{y}$。\n\n5.  **节点 $v_4 = \\sin(v_3)$：**\n    父节点是 $v_3$。\n    局部偏导数：$\\frac{\\partial v_4}{\\partial v_3} = \\cos(v_3)$。\n    更新父节点伴随变量：\n    $\\bar{v}_3 \\leftarrow \\bar{v}_3 + \\bar{v}_4 \\cdot \\frac{\\partial v_4}{\\partial v_3} = 0 + 1 \\cdot \\cos(v_3) = \\cos(xy)$。\n    当前状态：$\\bar{v}_3 = \\cos(xy)$。\n\n6.  **节点 $v_3 = v_1 \\cdot v_2$：**\n    父节点是 $v_1$ 和 $v_2$。注意，$v_1$ 和 $v_2$ 已经从其他路径接收了梯度；我们累加新的贡献。\n    局部偏导数：$\\frac{\\partial v_3}{\\partial v_1} = v_2$, $\\frac{\\partial v_3}{\\partial v_2} = v_1$。\n    更新父节点伴随变量：\n    $\\bar{v}_1 \\leftarrow \\bar{v}_1 + \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_1} = \\frac{\\exp(x)}{y} + \\cos(xy) \\cdot v_2 = \\frac{\\exp(x)}{y} + y \\cos(xy)$。\n    $\\bar{v}_2 \\leftarrow \\bar{v}_2 + \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_2} = -\\frac{\\exp(x)}{y^2} + \\cos(xy) \\cdot v_1 = -\\frac{\\exp(x)}{y^2} + x \\cos(xy)$。\n\n当我们计算完所有输入节点的伴随变量后，该过程终止。\n最终的梯度是输入变量伴随变量的最终值：\n$$ \\frac{\\partial f}{\\partial x} = \\bar{v}_1 = y \\cos(xy) + \\frac{\\exp(x)}{y} $$\n$$ \\frac{\\partial f}{\\partial y} = \\bar{v}_2 = x \\cos(xy) - \\frac{\\exp(x)}{y^2} $$\n\n梯度向量 $\\nabla f(x,y)$ 是这些偏导数组成的行向量：\n$$ \\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} y \\cos(xy) + \\frac{\\exp(x)}{y} & x \\cos(xy) - \\frac{\\exp(x)}{y^2} \\end{pmatrix} $$\n这个推导严格遵循了逆向模式自动微分的机械步骤。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\ny \\cos(xy) + \\frac{\\exp(x)}{y} & x \\cos(xy) - \\frac{\\exp(x)}{y^2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在将数学公式转化为计算机代码时，一个关键的挑战是处理数值稳定性问题。直接的公式实现可能在面对极端输入值时导致上溢、下溢或精度损失。本练习以 `softplus` 函数为例，它在深度学习中时有出现，通过对比一个“朴素”实现和一个经过代数变换的稳定实现，你将学会如何预见并解决这些数值问题，这是开发稳健可靠的自定义神经网络层或损失函数的一项重要技能 。",
            "id": "3100433",
            "problem": "您的任务是为函数 $y=\\mathrm{softplus}(x)$（其中 $y=\\log(1+e^{x})$）构建一个数值稳定的反向模式自动微分 (AD) 原语，该原语提供一个自定义的向量-雅可比积 (VJP)。向量-雅可比积 (VJP) 将一个余切向量（上游敏感度）$v$ 映射为 $v$ 与 $y$ 相对于 $x$ 的梯度之积。您的实现必须在 $|x|\\gg 0$ 的情况下（特别是在 $x=100$ 和 $x=-100$ 附近）保持鲁棒，并且必须在一组更广泛的值上进行测试。\n\n从导数的数学定义和链式法则出发，实现该计算的两个版本：\n- 一个基准的“朴素 AD”版本，该版本通过直接应用规则于 $y=\\log(1+e^{x})$ 这一精确表达式来计算其值及其梯度，不进行任何为了数值稳定性而做的代数重排。\n- 一个自定义 VJP 版本，该版本返回 $y$ 和一个可调用对象。给定 $v$ 时，该可调用对象返回 VJP $v\\cdot \\frac{dy}{dx}$。其中，原始值 $y$ 和梯度 $\\frac{dy}{dx}$ 的计算对于大的正值和负值 $x$ 都是数值稳定的。\n\n设计一个测试套件，包含以下输入标量值 $x$：$100$、$-100$、$0$、$10000$、$-10000$。对于每个测试用例，计算：\n1. 朴素 AD 的原始值与自定义 VJP 的原始值是否在 $10^{-12}$ 的绝对容差内一致，并且两者都是有限值。\n2. 朴素 AD 的梯度与自定义 VJP 的梯度是否在 $10^{-12}$ 的绝对容差内一致，并且两者都是有限值。\n\n您程序的最终输出必须是单行文本，其中包含一个方括号括起来的逗号分隔列表，由布尔值组成，顺序为 $[\\text{agree\\_y}(x_1),\\text{agree\\_grad}(x_1),\\text{agree\\_y}(x_2),\\text{agree\\_grad}(x_2),\\dots]$。当且仅当两个原始值都是有限的，并且它们的绝对差小于或等于 $10^{-12}$ 时，$\\text{agree\\_y}(x)$ 为真。$\\text{agree\\_grad}(x)$ 对梯度作类似定义。不涉及物理单位，所有角度（如果有）均以弧度为单位，但本问题不使用角度。\n\n您的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表形式的结果（例如，$[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\dots]$）。",
            "solution": "该问题要求实现并比较 softplus 函数 $y(x) = \\log(1+e^x)$ 的两个反向模式自动微分 (AD) 原语。一个实现是公式的“朴素”直接转换，另一个是为数值稳定性设计的“自定义 VJP”。\n\n首先，我们定义该函数及其导数。softplus 函数由下式给出：\n$$y(x) = \\log(1+e^x)$$\n使用链式法则，其相对于 $x$ 的导数为：\n$$\\frac{dy}{dx} = \\frac{1}{1+e^x} \\cdot \\frac{d}{dx}(1+e^x) = \\frac{e^x}{1+e^x}$$\n该导数是 logistic sigmoid 函数，通常表示为 $\\sigma(x)$。\n\n一个函数 $y=f(x)$ 的反向模式 AD 原语通常包括两部分：计算原始输出值 $y$ 的前向传播，以及一个提供向量-雅可比积 (VJP) 的函数。VJP 通过关系式 $\\bar{x} = J_f^T \\bar{y}$，将输出的余切空间中的向量 $\\bar{y}$ 映射到输入的余切空间中的向量 $\\bar{x}$，其中 $J_f$ 是 $f$ 的雅可比矩阵。对于标量函数 $y=f(x)$，雅可比矩阵就是标量导数 $\\frac{dy}{dx}$。余切“向量”$\\bar{y}$ 是一个标量，在问题描述中表示为 $v$。因此，VJP 是标量积 $v \\cdot \\frac{dy}{dx}$。恢复梯度本身的标准方法是用 $v=1$ 来评估 VJP。\n\n我们现在将分析朴素实现的数值稳定性，并推导出一个稳定的替代方案。\n\n**朴素 AD 实现**\n直接实现使用如下所示的公式：\n- 原始值：$y = \\log(1+e^x)$\n- 梯度：$\\frac{dy}{dx} = \\frac{e^x}{1+e^x}$\n\n让我们分析这些表达式在浮点运算中对于极端 $x$ 值的行为。\n1.  **大的正值 $x$（例如，$x \\to \\infty$）：** $e^x$ 项呈指数增长。对于足够大的 $x$（例如，在标准 $64$ 位浮点数中 $x > 709.78$），$e^x$ 会上溢，产生一个无穷大值 (`inf`)。\n    - 朴素原始值：$y = \\log(1 + \\text{inf}) = \\log(\\text{inf}) = \\text{inf}$。这在数值上是不正确的。渐近地，对于大的 $x$，$y(x) \\approx \\log(e^x) = x$。\n    - 朴素梯度：$\\frac{dy}{dx} = \\frac{\\text{inf}}{1+\\text{inf}} = \\frac{\\text{inf}}{\\text{inf}}$，其计算结果为 `NaN` (非数值)。这也是不正确的。渐近地，梯度应趋近于 $1$。\n\n2.  **大的负值 $x$（例如，$x \\to -\\infty$）：** $e^x$ 项会下溢到 $0$。\n    - 朴素原始值：$y = \\log(1+0) = 0$。这是数值稳定且正确的。\n    - 朴素梯度：$\\frac{dy}{dx} = \\frac{0}{1+0} = 0$。这也是一个稳定且正确的计算。\n\n很明显，由于对于大的正输入会发生上溢，朴素实现是有缺陷的。\n\n**自定义 VJP：数值稳定的实现**\n为了创建一个稳定的实现，我们必须对表达式进行代数重排，以避免在 $x$ 为大的正数时计算 $e^x$。\n\n对于原始值 $y(x)$，当 $x > 0$ 时：\n$$y(x) = \\log(1+e^x) = \\log\\left(e^x \\cdot (e^{-x} + 1)\\right) = \\log(e^x) + \\log(1+e^{-x}) = x + \\log(1+e^{-x})$$\n在这种形式下，对于大的正值 $x$，我们计算 $e^{-x}$，它会安全地下溢到 $0$。表达式 $x + \\log(1+e^{-x})$ 正确地计算出约等于 $x$ 的值，避免了上溢。对于 $x \\le 0$，原始表达式 $y(x) = \\log(1+e^x)$ 是稳定的。\n\n对于梯度 $\\frac{dy}{dx}$，当 $x > 0$ 时：\n$$\\frac{dy}{dx} = \\frac{e^x}{1+e^x} = \\frac{e^x \\cdot e^{-x}}{(1+e^x) \\cdot e^{-x}} = \\frac{1}{e^{-x} + 1}$$\n这种形式通过使用 $e^{-x}$ 也避免了在 $x$ 为大的正数时发生上溢，并且正确地计算出约等于 $1$ 的值。对于 $x \\le 0$，原始表达式 $\\frac{dy}{dx} = \\frac{e^x}{1+e^x}$ 是稳定的。\n\n结合这些观察，我们可以定义一个用于稳定计算的分段函数：\n- **稳定的原始值 $y(x)$**：\n$$\ny_{stable}(x) = \n\\begin{cases} \nx + \\log(1+e^{-x})  & \\text{if } x > 0 \\\\\n\\log(1+e^x)  & \\text{if } x \\le 0 \n\\end{cases}\n$$\n\n- **稳定的梯度 $\\frac{dy}{dx}(x)$**：\n$$\ng_{stable}(x) = \n\\begin{cases} \n\\frac{1}{1+e^{-x}} & \\text{if } x \\ge 0 \\\\\n\\frac{e^x}{1+e^x} & \\text{if } x  0 \n\\end{cases}\n$$\n梯度的分割点选择为 $x=0$，在该点两个表达式的计算结果都为 $\\frac{1}{2}$，从而确保了连续性。对第一种情况使用 $x \\ge 0$ 可以确保 $x=0$ 这个点得到正确处理。\n\n自定义 VJP 原语将实现这些稳定的计算。它将计算 $y_{stable}$ 和 $g_{stable}$，然后返回 $y_{stable}$ 和一个可调用函数（一个闭包），该函数接受 $v$ 并返回乘积 $v \\cdot g_{stable}$。\n\n然后，测试套件将比较朴素原语和稳定原语在一组输入值上的输出，检查其有限性以及在指定容差内的一致性，从而证明朴素方法的失败和稳定方法的正确性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares naive vs. numerically stable reverse-mode AD\n    primitives for the softplus function.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [100.0, -100.0, 0.0, 10000.0, -10000.0]\n    tolerance = 1e-12\n\n    def naive_ad_primitive(x: float):\n        \"\"\"\n        Computes the softplus primal and gradient using a direct,\n        numerically unstable implementation.\n        \"\"\"\n        # Catch numpy warnings about overflow/invalid values during naive computation\n        with np.errstate(over='ignore', invalid='ignore'):\n            # Primal: y = log(1 + exp(x))\n            primal_y = np.log(1.0 + np.exp(x))\n            \n            # Gradient: dy/dx = exp(x) / (1 + exp(x))\n            grad = np.exp(x) / (1.0 + np.exp(x))\n        \n        return primal_y, grad\n\n    def custom_vjp_primitive(x: float):\n        \"\"\"\n        Computes the softplus primal and a VJP callable using a\n        numerically stable implementation.\n        \"\"\"\n        # Stable primal computation\n        if x > 0:\n            primal_y = x + np.log(1.0 + np.exp(-x))\n        else:\n            primal_y = np.log(1.0 + np.exp(x))\n        \n        # Stable gradient computation\n        if x >= 0:\n            grad = 1.0 / (1.0 + np.exp(-x))\n        else:\n            grad = np.exp(x) / (1.0 + np.exp(x))\n\n        # The VJP is a function that takes an upstream gradient v\n        # and multiplies it by the local gradient.\n        def vjp_callable(v: float):\n            return v * grad\n            \n        return primal_y, vjp_callable\n\n    results = []\n    for x_val in test_cases:\n        # Get results from the naive implementation\n        y_naive, grad_naive = naive_ad_primitive(x_val)\n        \n        # Get results from the custom stable implementation\n        # The gradient is recovered by calling the VJP with v=1.0\n        y_custom, vjp_fn = custom_vjp_primitive(x_val)\n        grad_custom = vjp_fn(1.0)\n        \n        # 1. Compare primal values (y)\n        y_are_finite = np.isfinite(y_naive) and np.isfinite(y_custom)\n        if y_are_finite:\n            y_agree = np.abs(y_naive - y_custom) = tolerance\n        else:\n            y_agree = False\n        results.append(y_agree)\n        \n        # 2. Compare gradient values (dy/dx)\n        grad_are_finite = np.isfinite(grad_naive) and np.isfinite(grad_custom)\n        if grad_are_finite:\n            grad_agree = np.abs(grad_naive - grad_custom) = tolerance\n        else:\n            grad_agree = False\n        results.append(grad_agree)\n\n    # Final print statement in the exact required format.\n    # str(bool) gives 'True'/'False', which is the required format.\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现代神经网络广泛使用像 ReLU 这样的分段线性激活函数，它们在某些点上是不可微的。这给基于梯度的优化带来了理论上的疑问：自动微分系统是如何处理这些“尖点”的？本练习通过一个巧妙的函数 $f(x)=\\mathrm{ReLU}(x)^3$ 进行思想实验，探讨了自动微分框架在不可微点处的行为。你将发现，即使一个中间函数（如 ReLU）在某点不可微，链式法则的机械应用有时也能产生一个明确且为零的梯度，这一洞见对于理解模型训练中可能出现的“神经元死亡”等现象至关重要 。",
            "id": "3100437",
            "problem": "考虑标量函数 $f:\\mathbb{R}\\to\\mathbb{R}$，其定义为 $f(x)=\\mathrm{ReLU}(x)^3$，其中 $\\mathrm{ReLU}(x)=\\max(0,x)$。使用导数的极限定义和链式法则作为基础工具，分析函数 $f$ 在 $x=0$ 处的行为及其对基于梯度的训练的影响。特别地，论述在不可微点处的次梯度，以及自动微分（AD）框架如何针对 $\\mathrm{ReLU}'(0)$ 实现不同的选择。假设对于标量参数 $w$ 和有限目标 $t\\in\\mathbb{R}$，平方误差损失为 $L(w)=\\frac{1}{2}(f(w)-t)^2$，并考虑初始化 $w=0$。选择所有正确的陈述。\n\nA. 函数 $f$ 在 $x=0$ 处的次微分是区间 $[0,1]$，因此不同的自动微分（AD）框架可以合法地在 $x=0$ 处为梯度返回 $[0,1]$ 中的任何值；这可能会在初始化时显著改变训练更新。\n\nB. 函数 $f$ 在 $x=0$ 处可微，且 $f'(0)=0$，因此任何使用链式法则的自动微分（AD）框架在 $x=0$ 处计算出的梯度都为 $0$，无论其对 $\\mathrm{ReLU}'(0)$ 的选择如何。\n\nC. 即使 AD 框架选择 $\\mathrm{ReLU}'(0)=1$，通过链式法则在 $x=0$ 处得到的 $f$ 的梯度仍然是 $0$。\n\nD. 对于平方损失 $L(w)=\\frac{1}{2}(f(w)-t)^2$，初始化 $w=0$ 会对任何有限目标 $t$ 在 $w=0$ 处得到 $\\frac{dL}{dw}=0$，因此基于梯度的训练在第一次更新时不会离开 $w=0$。\n\nE. 因为 $f$ 在 $x=0$ 处不可微，AD 框架必须在此处返回“非数字”（$\\text{NaN}$）；这通常会使训练不稳定。",
            "solution": "问题陈述具有科学依据，表述清晰且客观。所有给出的定义和条件在数学上都是合理的，足以进行严格的分析。该问题有效。\n\n我们将分析函数 $f(x)=\\mathrm{ReLU}(x)^3$ 及其在基于梯度的优化问题中的作用。\n\n首先，让我们使用导数的极限定义来分析 $f(x)$ 在 $x=0$ 处的可微性。函数 $f(x)$ 可以写成分段形式：\n$$\nf(x) = \\left(\\max(0,x)\\right)^3 =\n\\begin{cases}\nx^3  \\text{if } x \\ge 0 \\\\\n0  \\text{if } x  0\n\\end{cases}\n$$\n$x=0$ 处的导数（如果存在）由以下极限给出：\n$$ f'(0) = \\lim_{h \\to 0} \\frac{f(0+h) - f(0)}{h} $$\n我们有 $f(0) = (\\max(0,0))^3 = 0^3 = 0$。该极限变为：\n$$ f'(0) = \\lim_{h \\to 0} \\frac{f(h)}{h} $$\n为了计算这个极限，我们必须检查左极限和右极限。\n右极限：\n$$ \\lim_{h \\to 0^+} \\frac{f(h)}{h} = \\lim_{h \\to 0^+} \\frac{h^3}{h} = \\lim_{h \\to 0^+} h^2 = 0 $$\n左极限：\n$$ \\lim_{h \\to 0^-} \\frac{f(h)}{h} = \\lim_{h \\to 0^-} \\frac{0}{h} = \\lim_{h \\to 0^-} 0 = 0 $$\n由于左极限和右极限都存在且相等，所以导数存在且 $f'(0)=0$。因此，函数 $f(x)$ 在 $x=0$ 处是可微的。\n\n接下来，我们分析自动微分（AD）框架如何使用链式法则计算这个导数。令 $u(x) = \\mathrm{ReLU}(x)$ 和 $g(u) = u^3$。那么 $f(x) = g(u(x))$。AD 系统机械地应用链式法则：\n$$ \\frac{df}{dx} = \\frac{dg}{du} \\cdot \\frac{du}{dx} $$\n基本函数的导数是 $\\frac{dg}{du} = 3u^2$ 和 $\\frac{du}{dx} = \\mathrm{ReLU}'(x)$。将 $u(x)$ 代回：\n$$ \\frac{df}{dx} = 3(\\mathrm{ReLU}(x))^2 \\cdot \\mathrm{ReLU}'(x) $$\n在 $x=0$ 处，这个表达式变为：\n$$ \\left.\\frac{df}{dx}\\right|_{x=0} = 3(\\mathrm{ReLU}(0))^2 \\cdot \\mathrm{ReLU}'(0) = 3(0)^2 \\cdot \\mathrm{ReLU}'(0) = 0 \\cdot \\mathrm{ReLU}'(0) $$\n函数 $\\mathrm{ReLU}(x)$ 在 $x=0$ 处不可微，所以 $\\mathrm{ReLU}'(0)$ 在技术上是未定义的。AD 框架通过从 $\\mathrm{ReLU}$ 在 $0$ 处的次微分（即 $[0,1]$）中指定一个特定值来处理这种情况。$\\mathrm{ReLU}'(0)$ 的常见选择是 $0$ 或 $1$。然而，无论为 $\\mathrm{ReLU}'(0)$ 选择哪个有限值，乘积 $0 \\cdot \\mathrm{ReLU}'(0)$ 总是 $0$。因此，任何实现链式法则的 AD 框架都会计算出 $f(x)$ 在 $x=0$ 处的导数为 $0$。\n\n现在，让我们考虑损失函数 $L(w) = \\frac{1}{2}(f(w)-t)^2$ 和初始化 $w=0$。损失关于 $w$ 的梯度通过链式法则得到：\n$$ \\frac{dL}{dw} = (f(w)-t) \\cdot f'(w) $$\n在初始化 $w=0$ 处，梯度为：\n$$ \\left.\\frac{dL}{dw}\\right|_{w=0} = (f(0)-t) \\cdot f'(0) $$\n使用我们之前计算的值 $f(0)=0$ 和 $f'(0)=0$，我们得到：\n$$ \\left.\\frac{dL}{dw}\\right|_{w=0} = (0-t) \\cdot 0 = -t \\cdot 0 = 0 $$\n这个结果对任何有限的目标值 $t$ 都成立。在标准的梯度下降更新步骤 $w_{k+1} = w_k - \\eta \\frac{dL}{dw}$ 中，如果初始参数是 $w_0=0$ 并且梯度是 $0$，那么更新将是 $w_1 = 0 - \\eta \\cdot 0 = 0$。参数不会改变。\n\n基于这些分析，我们评估每个选项：\n\nA. 函数 $f$ 在 $x=0$ 处的次微分是区间 $[0,1]$，因此不同的自动微分（AD）框架可以合法地在 $x=0$ 处为梯度返回 $[0,1]$ 中的任何值；这可能会在初始化时显著改变训练更新。\n这是 **不正确** 的。一个函数在可微点的次微分是包含其导数的单点集。既然我们证明了 $f'(0)=0$，那么 $f$ 在 $x=0$ 处的次微分是 $\\partial f(0) = \\{0\\}$。区间 $[0,1]$ 是 $\\mathrm{ReLU}(x)$ 在 $x=0$ 处的次微分，而不是 $f(x) = \\mathrm{ReLU}(x)^3$ 的。因此，AD 框架计算出的 $f$ 在 $x=0$ 处的梯度明確为 $0$。\n\nB. 函数 $f$ 在 $x=0$ 处可微，且 $f'(0)=0$，因此任何使用链式法则的自动微分（AD）框架在 $x=0$ 处计算出的梯度都为 $0$，无论其对 $\\mathrm{ReLU}'(0)$ 的选择如何。\n这是 **正确** 的。我们的分析表明，这个陈述的两部分都是正确的。函数在 $x=0$ 处可微，导数为 $0$。链式法则的机械应用得到表达式 $3(\\mathrm{ReLU}(0))^2 \\cdot \\mathrm{ReLU}'(0) = 0$，无论为 $\\mathrm{ReLU}'(0)$ 选择哪个有限值。\n\nC. 即使 AD 框架选择 $\\mathrm{ReLU}'(0)=1$，通过链式法则在 $x=0$ 处得到的 $f$ 的梯度仍然是 $0$。\n这是 **正确** 的。这是 B 中建立的一般原则的一个特例。如果 $\\mathrm{ReLU}'(0)=1$，计算过程为 $\\left.\\frac{df}{dx}\\right|_{x=0} = 3(\\mathrm{ReLU}(0))^2 \\cdot 1 = 3(0)^2 \\cdot 1 = 0$。\n\nD. 对于平方损失 $L(w)=\\frac{1}{2}(f(w)-t)^2$，初始化 $w=0$ 会对任何有限目标 $t$ 在 $w=0$ 处得到 $\\frac{dL}{dw}=0$，因此基于梯度的训练在第一次更新时不会离开 $w=0$。\n这是 **正确** 的。如前所述，$\\left.\\frac{dL}{dw}\\right|_{w=0} = (f(0)-t)f'(0) = (-t)(0) = 0$。梯度为零意味着标准的梯度下降步骤不会导致参数 $w$ 发生变化。训练在初始化时就“卡住”了。\n\nE. 因为 $f$ 在 $x=0$ 处不可微，AD 框架必须在此处返回“非数字”（$\\text{NaN}$）；这通常会使训练不稳定。\n这是 **不正确** 的。其前提“$f$ 在 $x=0$ 处不可微”是错误的。我们证明了 $f$ 在 $x=0$ 处是可微的。此外，AD 框架被设计用来处理像 $\\mathrm{ReLU}$ 这样的常见函数的不可微点，方法是返回一个有效的次梯度（例如 $0$ 或 $1$），而不是 $\\text{NaN}$。",
            "answer": "$$\\boxed{BCD}$$"
        }
    ]
}