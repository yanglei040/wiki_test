{
    "hands_on_practices": [
        {
            "introduction": "修正线性单元（ReLU）是现代神经网络的基石，其定义 $\\mathrm{ReLU}(z) = \\max(0, z)$ 看似简单，但其真正的威力在于它如何作为决策边界来划分输入空间。本练习将引导你通过精心设计的权重和偏置，精确控制一个层中神经元的激活状态。这个过程将揭示，每一个ReLU神经元都像一个由超平面控制的开关，而整个神经元层则通过这些超平面的组合，将复杂的输入空间切割成不同的线性区域，这是神经网络学习和表示复杂函数的基础。",
            "id": "3185431",
            "problem": "考虑一个带有整流线性单元（ReLU）激活函数的单层前馈网络，其前向传播逐元素定义为 $a = \\mathrm{ReLU}(W x + b)$。整流线性单元（ReLU）函数定义为 $\\mathrm{ReLU}(z) = \\max(0, z)$。设输入维度为 $n = 2$，单元数为 $m = 4$。在整个问题中，将 $W$ 的行表示为 $w_1, w_2, w_3, w_4 \\in \\mathbb{R}^2$。\n\n你将分析如何通过精心选择 $x$、$W$ 和 $b$ 来强制产生恰好 $k = 2$ 个非零激活值，然后从第一性原理推断所引发的 $\\mathbb{R}^2$ 区域划分如何依赖于 $W$ 和 $b$。\n\n选择所有正确的选项。\n\nA. 设 $W$ 的行向量为 $w_1 = (1, 0)$、$w_2 = (-1, 0)$、$w_3 = (0, 1)$、$w_4 = (0, -1)$，并设 $b = \\mathbf{0} \\in \\mathbb{R}^4$。对于 $x = (1, 1)$，前向传播产生恰好 $k = 2$ 个非零激活值。在此配置下，引发的区域边界是超平面 $x_1 = 0$ 和 $x_2 = 0$。\n\nB. 使用与选项 A 中相同的 $W$ 和 $b = \\mathbf{0}$，选择 $x = (0, 0)$ 会产生恰好 $k = 2$ 个非零激活值。\n\nC. 使用与选项 A 中相同的 $W$，选择 $b = (0.5, 0.5, -0.5, -0.5)$ 和 $x = (0, 0)$。此时前向传播产生恰好 $k = 2$ 个非零激活值。\n\nD. 对于 $b = \\mathbf{0}$，用任意正标量 $\\alpha  0$ 缩放整个矩阵 $W$ 不会改变 $\\mathbb{R}^2$ 空间中具有恒定激活模式的区域划分（即，对于 $x \\in \\mathbb{R}^2$，$a$ 的支撑集保持不变）。\n\nE. 对于 $b = \\mathbf{0}$，将任意单行 $w_i$ 替换为 $-w_i$ 会保持几何超平面边界 $\\{x \\in \\mathbb{R}^2 : w_i^{\\top} x = 0\\}$ 不变，但会翻转单元 $i$ 在该边界的哪一侧是激活的，从而改变分配给每个区域的激活模式。\n\nF. 对于 $W \\in \\mathbb{R}^{m \\times n}$ 和 $b \\in \\mathbb{R}^m$ 的任意选择，当 $x$ 在 $\\mathbb{R}^n$ 上变化时，能够实现的不同激活模式的数量恰好是 $2^m$。",
            "solution": "首先验证问题陈述，以确保其科学合理、良定和客观。\n\n### 步骤 1：提取已知条件\n- 神经网络是单层前馈网络。\n- 激活函数是逐元素应用的整流线性单元（ReLU）：$\\mathrm{ReLU}(z) = \\max(0, z)$。\n- 前向传播由方程 $a = \\mathrm{ReLU}(W x + b)$ 定义。\n- 输入向量维度为 $n = 2$，因此 $x \\in \\mathbb{R}^2$。\n- 单元数（输出维度）为 $m = 4$，因此 $a, b \\in \\mathbb{R}^4$ 且 $W \\in \\mathbb{R}^{4 \\times 2}$。\n- 权重矩阵 $W$ 的行表示为 $w_1, w_2, w_3, w_4 \\in \\mathbb{R}^2$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题描述了一个标准的单神经元层网络，这是深度学习中的一个基本构建块。其组成部分——输入向量、权重矩阵、偏置向量、ReLU 激活函数和前向传播方程——在该领域都是标准且定义明确的。选项中提出的问题是关于该系统在不同参数化下行为的具体的、可检验的数学论断。\n\n- **科学依据：** 该问题基于人工神经网络已建立的数学框架。所有定义和概念都是标准的。\n- **良定性：** 该问题提供了评估选项中论断所需的所有必要信息。每个选项都提出了一个清晰、可证伪的假设。\n- **客观性：** 该问题以精确的数学语言陈述，没有歧义或主观内容。\n\n### 步骤 3：结论与行动\n问题陈述是有效的。将进行完整的解答和选项评估。\n\n分析的核心涉及预激活向量 $z = Wx + b$，其分量为 $z_i = w_i^\\top x + b_i$，其中 $i \\in \\{1, 2, 3, 4\\}$。第 $i$ 个激活值为 $a_i = \\mathrm{ReLU}(z_i)$。一个激活值 $a_i$ 为非零当且仅当其对应的预激活值 $z_i$ 为正，即 $a_i  0 \\iff w_i^\\top x + b_i  0$。第 $i$ 个单元的激活区域与非激活区域之间的边界是由方程 $w_i^\\top x + b_i = 0$ 定义的超平面（在 $\\mathbb{R}^2$ 中是一条线）。\n\n### 逐项分析\n\n**A. 设 $W$ 的行向量为 $w_1 = (1, 0)$、$w_2 = (-1, 0)$、$w_3 = (0, 1)$、$w_4 = (0, -1)$，并设 $b = \\mathbf{0} \\in \\mathbb{R}^4$。对于 $x = (1, 1)$，前向传播产生恰好 $k = 2$ 个非零激活值。在此配置下，引发的区域边界是超平面 $x_1 = 0$ 和 $x_2 = 0$。**\n\n首先，我们用给定的值计算预激活向量 $z = Wx + b$。\n$W = \\begin{pmatrix} 1  0 \\\\ -1  0 \\\\ 0  1 \\\\ 0  -1 \\end{pmatrix}$，$x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$，$b = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n\n$z = Wx = \\begin{pmatrix} 1  0 \\\\ -1  0 \\\\ 0  1 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 1 \\\\ -1 \\cdot 1 + 0 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 1 \\\\ 0 \\cdot 1 + (-1) \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{pmatrix}$。\n\n接下来，我们应用 ReLU 函数得到激活向量 $a$：\n$a = \\mathrm{ReLU}(z) = \\mathrm{ReLU}\\left(\\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{pmatrix}\\right) = \\begin{pmatrix} \\max(0, 1) \\\\ \\max(0, -1) \\\\ \\max(0, 1) \\\\ \\max(0, -1) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$。\n非零激活值的数量为 $k=2$（$a_1$ 和 $a_3$）。陈述的这一部分是正确的。\n\n现在，我们分析区域边界。边界由 $w_i^\\top x + b_i = 0$ 定义。由于 $b = \\mathbf{0}$，这可以简化为 $w_i^\\top x = 0$。\n设 $x = (x_1, x_2)^\\top$。\n- 对于 $i=1$：$w_1^\\top x = (1, 0) \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = x_1 = 0$。\n- 对于 $i=2$：$w_2^\\top x = (-1, 0) \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = -x_1 = 0 \\implies x_1 = 0$。\n- 对于 $i=3$：$w_3^\\top x = (0, 1) \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = x_2 = 0$。\n- 对于 $i=4$：$w_4^\\top x = (0, -1) \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = -x_2 = 0 \\implies x_2 = 0$。\n不同的边界超平面集合是 $\\{x_1 = 0\\}$ 和 $\\{x_2 = 0\\}$。这些是坐标轴。陈述的这一部分也是正确的。\n\n结论：**正确**。\n\n**B. 使用与选项 A 中相同的 $W$ 和 $b = \\mathbf{0}$，选择 $x = (0, 0)$ 会产生恰好 $k = 2$ 个非零激活值。**\n\n使用选项 A 中的 $W$，$b = \\mathbf{0}$，以及 $x = (0, 0)^\\top$。\n$z = Wx + b = W\\mathbf{0} + \\mathbf{0} = \\mathbf{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n$a = \\mathrm{ReLU}(z) = \\mathrm{ReLU}(\\mathbf{0}) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n非零激活值的数量为 $k=0$。该陈述声称 $k=2$。\n\n结论：**错误**。\n\n**C. 使用与选项 A 中相同的 $W$，选择 $b = (0.5, 0.5, -0.5, -0.5)$ 和 $x = (0, 0)$。此时前向传播产生恰好 $k = 2$ 个非零激活值。**\n\n使用选项 A 中的 $W$，$x = (0, 0)^\\top$，以及 $b = (0.5, 0.5, -0.5, -0.5)^\\top$。\n$z = Wx + b = W\\mathbf{0} + b = b = \\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ -0.5 \\\\ -0.5 \\end{pmatrix}$。\n$a = \\mathrm{ReLU}(z) = \\mathrm{ReLU}\\left(\\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ -0.5 \\\\ -0.5 \\end{pmatrix}\\right) = \\begin{pmatrix} \\max(0, 0.5) \\\\ \\max(0, 0.5) \\\\ \\max(0, -0.5) \\\\ \\max(0, -0.5) \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0.5 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n非零激活值的数量为 $k=2$（$a_1$ 和 $a_2$）。\n\n结论：**正确**。\n\n**D. 对于 $b = \\mathbf{0}$，用任意正标量 $\\alpha  0$ 缩放整个矩阵 $W$ 不会改变 $\\mathbb{R}^2$ 空间中具有恒定激活模式的区域划分（即，对于 $x \\in \\mathbb{R}^2$，$a$ 的支撑集保持不变）。**\n\n激活模式由预激活值 $z_i$ 的符号决定。当 $b = \\mathbf{0}$ 时，我们有 $z_i = w_i^\\top x$。第 $i$ 个单元在 $w_i^\\top x  0$ 时激活。\n设缩放后的权重矩阵为 $W' = \\alpha W$，其中 $\\alpha  0$。新的权重向量为 $w'_i = \\alpha w_i$。\n新的预激活值为 $z'_i = (w'_i)^\\top x = (\\alpha w_i)^\\top x = \\alpha(w_i^\\top x)$。\n因为 $\\alpha  0$，$z'_i$ 的符号与 $z_i = w_i^\\top x$ 的符号相同。\n- $z'_i  0 \\iff \\alpha(w_i^\\top x)  0 \\iff w_i^\\top x  0$。\n- $z'_i \\le 0 \\iff \\alpha(w_i^\\top x) \\le 0 \\iff w_i^\\top x \\le 0$。\n对于任何给定的输入 $x$，激活单元的集合（激活模式）保持不变。边界超平面由 $w_i^\\top x = 0$ 定义。对于缩放后的系统，它们由 $(\\alpha w_i)^\\top x = 0$ 定义，这等价于 $w_i^\\top x = 0$。边界的几何位置没有改变。因此，输入空间到具有恒定激活模式的区域的划分保持不变。\n\n结论：**正确**。\n\n**E. 对于 $b = \\mathbf{0}$，将任意单行 $w_i$ 替换为 $-w_i$ 会保持几何超平面边界 $\\{x \\in \\mathbb{R}^2 : w_i^{\\top} x = 0\\}$ 不变，但会翻转单元 $i$ 在该边界的哪一侧是激活的，从而改变分配给每个区域的激活模式。**\n\n设 $w'_i = -w_i$。\n单元 $i$ 的原始边界是满足 $w_i^\\top x = 0$ 的点 $x$ 的集合。\n新的边界是满足 $(w'_i)^\\top x = 0$ 的点 $x$ 的集合。这就是 $(-w_i)^\\top x = - (w_i^\\top x) = 0$，等价于 $w_i^\\top x = 0$。几何边界确实没有改变。\n\n单元 $i$ 的原始激活区域是 $w_i^\\top x  0$ 的地方。\n单元 $i$ 的新激活区域是 $(w'_i)^\\top x  0$ 的地方，这意味着 $(-w_i)^\\top x  0$，或者 $-(w_i^\\top x)  0$。这可以简化为 $w_i^\\top x  0$。\n单元 $i$ 的激活区域已经从超平面的一侧翻转到另一侧。因此，对于不在边界本身的任何点 $x$，第 $i$ 个单元的激活状态将与原始配置相比被翻转。这改变了划分中每个区域的激活模式（指示哪些单元被激活的 0 和 1 向量）。\n\n结论：**正确**。\n\n**F. 对于 $W \\in \\mathbb{R}^{m \\times n}$ 和 $b \\in \\mathbb{R}^m$ 的任意选择，当 $x$ 在 $\\mathbb{R}^n$ 上变化时，能够实现的不同激活模式的数量恰好是 $2^m$。**\n\n一个激活模式是一个长度为 $m$ 的二进制向量，因此总共有 $2^m$ 种可能的模式。问题是，对于*任何* $W$ 和 $b$ 的选择，是否所有这些模式都能实现。\n超平面 $w_i^\\top x + b_i = 0$（其中 $i=1, \\dots, m$）将输入空间 $\\mathbb{R}^n$ 划分为多个区域。在每个开放连通区域内，所有 $w_i^\\top x + b_i$ 的符号都是恒定的，因此激活模式是恒定的。因此，不同激活模式的数量受这些超平面创建的区域数量的限制。\n$m$ 个超平面可将 $\\mathbb{R}^n$ 划分出的最大区域数由 Zaslavsky 公式给出，对于处于一般位置的超平面，该公式为 $\\sum_{j=0}^{n} \\binom{m}{j}$。\n在我们的例子中，$n=2$ 且 $m=4$。最大区域数为：\n$$ \\sum_{j=0}^{2} \\binom{4}{j} = \\binom{4}{0} + \\binom{4}{1} + \\binom{4}{2} = 1 + 4 + \\frac{4 \\cdot 3}{2} = 1 + 4 + 6 = 11 $$\n可能的激活模式总数为 $2^m = 2^4 = 16$。\n由于区域数量（11）小于可能的模式数量（16），因此不可能实现所有 $2^m$ 种模式。该陈述是普遍性的（“对于任意选择...”），因此一个反例就足以推翻它。上面的几何论证表明，对于 $n=2, m=4$，没有任何 $W, b$ 的选择可以实现这一点（甚至不考虑像平行或重合超平面这样的退化情况，这些情况会进一步减少区域数量）。例如，如果我们选择所有的 $w_i$ 都相同，比如对所有 $i$ 都有 $w_i = (1,0)$，并且 $b=\\mathbf{0}$，我们只得到一个不同的超平面 $x_1=0$。这会创建两个区域（$x_10$ 和 $x_10$），只实现两种模式：所有单元开启或所有单元关闭。\n\n结论：**错误**。",
            "answer": "$$\\boxed{ACDE}$$"
        },
        {
            "introduction": "自注意力机制是驱动了人工智能领域革命的 Transformer 架构的核心。其核心公式 $\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$ 初看起来可能令人望而生畏。本练习旨在揭开其神秘面纱，通过一个精心设计的小规模数值示例，你将亲手完成从查询（Query）、键（Key）的点积计算，到缩放、softmax归一化，再到与值（Value）的加权求和的全过程。通过这个分步实践，抽象的公式将转化为具体、可循的计算流程，从而让你对注意力机制的工作原理建立起牢固的直观理解。",
            "id": "3185352",
            "problem": "考虑一个单头自注意力层的前向传播，其机制基于查询向量和键向量之间的点积，通过键维度平方根进行缩放，逐行应用softmax函数以获得注意力权重，然后与值向量进行矩阵乘法以产生输出。使用以下包含三个词符（token）且键维度为 $d_k=2$ 的小型合成示例。查询矩阵 $Q$、键矩阵 $K$ 和值矩阵 $V$ 均相等，由下式给出\n$$\nQ=K=V=\\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n1  0\n\\end{pmatrix}.\n$$\n您必须仅从基本定义出发：向量的点积、通过 $\\sqrt{d_k}$ 进行的平方根缩放、逐行应用由 $\\mathrm{softmax}(x)_i=\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$ 定义的softmax函数，以及标准矩阵乘法。在不使用任何快捷公式的情况下，计算此示例中缩放点积注意力（SDPA）的前向传播过程。通过分析 $Q$、$K$ 和 $V$ 的结构，确定输出的哪些行由于对称性而相同。然后，计算这些相同输出行的第一个分量的共同值。将您的最终数值结果四舍五入到四位有效数字，并将其报告为无量纲量。",
            "solution": "问题要求计算给定输入矩阵 $Q$、$K$ 和 $V$ 的单头缩放点积注意力（SDPA）层的前向传播。最终输出应该是从得到的注意力输出矩阵中导出的一个特定数值。该过程由以下公式控制：\n$$Z = \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n其中 $Z$ 是输出矩阵。我们被要求从基本定义出发，将计算分解为顺序步骤。\n\n给定的输入是：\n查询、键和值矩阵是相同的：\n$$Q=K=V=\\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  0 \\end{pmatrix}$$\n键的维度给定为 $d_k=2$。\n\n步骤 1：计算点积得分矩阵 $QK^T$。\n首先，我们求键矩阵 $K$ 的转置：\n$$K^T = \\begin{pmatrix} 1  0  1 \\\\ 0  1  0 \\end{pmatrix}$$\n接下来，我们执行矩阵乘法 $QK^T$：\n$$QK^T = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 1  0  1 \\\\ 0  1  0 \\end{pmatrix}$$\n该乘积产生一个 $3 \\times 3$ 的注意力得分矩阵。其元素计算如下：\n$$(QK^T)_{11} = (1)(1) + (0)(0) = 1$$\n$$(QK^T)_{12} = (1)(0) + (0)(1) = 0$$\n$$(QK^T)_{13} = (1)(1) + (0)(0) = 1$$\n$$(QK^T)_{21} = (0)(1) + (1)(0) = 0$$\n$$(QK^T)_{22} = (0)(0) + (1)(1) = 1$$\n$$(QK^T)_{23} = (0)(1) + (1)(0) = 0$$\n$$(QK^T)_{31} = (1)(1) + (0)(0) = 1$$\n$$(QK^T)_{32} = (1)(0) + (0)(1) = 0$$\n$$(QK^T)_{33} = (1)(1) + (0)(0) = 1$$\n得到的得分矩阵是：\n$$S_{raw} = QK^T = \\begin{pmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 1  0  1 \\end{pmatrix}$$\n\n步骤 2：用 $1/\\sqrt{d_k}$ 缩放得分矩阵。\n当 $d_k=2$ 时，缩放因子为 $1/\\sqrt{2}$。缩放后的得分矩阵，我们记为 $S$，是：\n$$S = \\frac{QK^T}{\\sqrt{d_k}} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 1  0  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{2}} \\\\ 0  \\frac{1}{\\sqrt{2}}  0 \\\\ \\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{2}} \\end{pmatrix}$$\n\n步骤 3：通过对 $S$ 逐行应用 softmax 函数来计算注意力权重矩阵 $A_w$。\n向量 $x$ 的 softmax 函数定义为 $\\mathrm{softmax}(x)_i=\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$。\n\n对于 $S$ 的第一行，$s_1 = (\\frac{1}{\\sqrt{2}}, 0, \\frac{1}{\\sqrt{2}})$：\nsoftmax 的分母是 $\\sum_{j=1}^3 \\exp(s_{1j}) = \\exp(\\frac{1}{\\sqrt{2}}) + \\exp(0) + \\exp(\\frac{1}{\\sqrt{2}}) = 2\\exp(\\frac{1}{\\sqrt{2}}) + 1$。\n$A_w$ 第一行的分量是：\n$$A_{w,11} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1}, \\quad A_{w,12} = \\frac{\\exp(0)}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1} = \\frac{1}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1}, \\quad A_{w,13} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1}$$\n\n对于 $S$ 的第二行，$s_2 = (0, \\frac{1}{\\sqrt{2}}, 0)$：\nsoftmax 的分母是 $\\sum_{j=1}^3 \\exp(s_{2j}) = \\exp(0) + \\exp(\\frac{1}{\\sqrt{2}}) + \\exp(0) = 2 + \\exp(\\frac{1}{\\sqrt{2}})$。\n$A_w$ 第二行的分量是：\n$$A_{w,21} = \\frac{1}{2 + \\exp(\\frac{1}{\\sqrt{2}})}, \\quad A_{w,22} = \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2 + \\exp(\\frac{1}{\\sqrt{2}})}, \\quad A_{w,23} = \\frac{1}{2 + \\exp(\\frac{1}{\\sqrt{2}})}$$\n\n$S$ 的第三行 $s_3 = (\\frac{1}{\\sqrt{2}}, 0, \\frac{1}{\\sqrt{2}})$ 与第一行 $s_1$ 相同。因此，注意力矩阵 $A_w$ 的第三行与其第一行相同：对于 $j \\in \\{1, 2, 3\\}$，有 $A_{w,3j} = A_{w,1j}$。\n\n步骤 4：计算最终输出矩阵 $Z = A_w V$。\n$$Z = A_w V = \\begin{pmatrix} A_{w,11}  A_{w,12}  A_{w,13} \\\\ A_{w,21}  A_{w,22}  A_{w,23} \\\\ A_{w,31}  A_{w,32}  A_{w,33} \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  0 \\end{pmatrix}$$\n输出矩阵 $Z$ 的行是 $V$ 的行的线性组合。设 $v_1, v_2, v_3$ 是 $V$ 的行，则 $v_1 = (1, 0)$，$v_2 = (0, 1)$，$v_3 = (1, 0)$。注意 $v_1=v_3$。\n\n问题要求确定输出的哪些行由于对称性而相同。输入查询向量为 $q_1=(1,0)$，$q_2=(0,1)$ 和 $q_3=(1,0)$。因为 $q_1=q_3$ 且键矩阵 $K$ 是共享的，所以第一个和第三个查询的注意力得分将是相同的。如步骤 2 所确立，缩放得分矩阵 $S$ 的第一行和第三行是相同的。这导致注意力权重矩阵 $A_w$ 的第一行和第三行相同，如步骤 3 所示。\n输出 $Z$ 的第一行和第三行是：\n$$Z_1 = A_{w,11}v_1 + A_{w,12}v_2 + A_{w,13}v_3$$\n$$Z_3 = A_{w,31}v_1 + A_{w,32}v_2 + A_{w,33}v_3$$\n由于对于所有 $j$ 都有 $A_{w,1j} = A_{w,3j}$，我们得到 $Z_1 = Z_3$。输出的第一行和第三行是相同的。\n\n问题接着要求计算这些相同输出行的第一个分量的共同值。我们将计算第一个输出行的第一个分量 $Z_{11}$。\n$$Z_1 = (Z_{11}, Z_{12}) = A_{w,11}(1, 0) + A_{w,12}(0, 1) + A_{w,13}(1, 0) = (A_{w,11} + A_{w,13}, A_{w,12})$$\n第一个分量是 $Z_{11} = A_{w,11} + A_{w,13}$。根据步骤 3，我们知道 $A_{w,11} = A_{w,13}$。因此：\n$$Z_{11} = 2A_{w,11} = 2 \\left( \\frac{\\exp(\\frac{1}{\\sqrt{2}})}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1} \\right) = \\frac{2\\exp(\\frac{1}{\\sqrt{2}})}{2\\exp(\\frac{1}{\\sqrt{2}}) + 1}$$\n\n最后，我们计算数值并将其四舍五入到四位有效数字。\n指数的值是 $\\frac{1}{\\sqrt{2}} \\approx 0.70710678$。\n指数项是 $\\exp(\\frac{1}{\\sqrt{2}}) \\approx 2.02813039$。\n将此代入 $Z_{11}$ 的表达式中：\n$$Z_{11} = \\frac{2 \\times 2.02813039}{2 \\times 2.02813039 + 1} = \\frac{4.05626078}{5.05626078} \\approx 0.80218349$$\n将此结果四舍五入到四位有效数字得到 $0.8022$。",
            "answer": "$$\\boxed{0.8022}$$"
        },
        {
            "introduction": "专家混合（Mixture-of-Experts, MoE）模型是近年来用于构建超大规模神经网络的关键技术，它遵循一种“分而治之”的策略。在MoE层中，前向传播不仅仅是一系列固定的矩阵运算，它还包含一个动态的“路由”决策过程，由一个“门控网络”决定将每个输入分配给哪个或哪些“专家”子网络进行处理。本练习将带你从零开始实现这一复杂的门控机制，并探索一个在实际应用中至关重要的系统属性——负载均衡。通过分析不同参数设置下专家负载的变化，你将体会到在设计高级神经网络架构时，除了保证数学正确性外，还必须关注其系统性的工程挑战。",
            "id": "3185355",
            "problem": "考虑一个专家混合（MoE）模型对一批输入的前向传播过程，其中门控（gate）通过对线性得分进行 softmax 映射来分配路由权重。设专家数量为 $m$，输入维度为 $d$，输出维度为 $p$。对于输入向量 $x \\in \\mathbb{R}^{d}$，门控定义为 $g(x) = \\mathrm{softmax}(U x)$，其中 $U \\in \\mathbb{R}^{m \\times d}$ 是一个参数矩阵。每个专家是一个仿射映射 $E_i(x) = A_i x + c_i$，其中 $A_i \\in \\mathbb{R}^{p \\times d}$ 且 $c_i \\in \\mathbb{R}^{p}$。MoE 的输出为 $y(x) = \\sum_{i=1}^{m} g_i(x) E_i(x)$，其中 $g_i(x)$ 是 $g(x)$ 的第 $i$ 个分量。任务是从第一性原理出发实现前向传播：线性得分、softmax 归一化、仿射专家输出和加权聚合。\n\n从以下基本定义开始：\n- 线性映射由矩阵-向量乘法表示，即对于 $M \\in \\mathbb{R}^{a \\times b}$ 和 $v \\in \\mathbb{R}^{b}$，输出为 $M v \\in \\mathbb{R}^{a}$。\n- softmax 函数将一个实数向量映射到一个概率单纯形：对于 $z \\in \\mathbb{R}^{m}$，$\\mathrm{softmax}(z)_i = \\exp(z_i)\\big/\\sum_{j=1}^{m}\\exp(z_j)$，生成非负且总和为 1 的分量。\n\n为了量化路由负载不平衡，对于一个批次矩阵 $X \\in \\mathbb{R}^{n \\times d}$，定义专家 $i$ 的期望负载为 $S_i = \\sum_{k=1}^{n} g_i(x_k)$，其中 $x_k$ 是 $X$ 的第 $k$ 行。定义不平衡比 $r = \\max_{1 \\leq j \\leq m} S_j \\big/ \\min_{1 \\leq j \\leq m} S_j$。高 $r$ 值表示严重的不平衡；$r = 1$ 的值表示完全平衡的路由。\n\n实现前向传播算法并为以下每个测试用例计算 $r$。所有用例均使用相同的基础矩阵和向量：\n\n- 维度：$m = 4$，$d = 3$，$p = 2$，$n = 5$。\n- 基础门控矩阵：\n$$\nB = \\begin{bmatrix}\n1.0  -2.0  0.5 \\\\\n-0.5  3.0  1.5 \\\\\n2.0  0.0  -1.0 \\\\\n-1.0  1.0  2.0\n\\end{bmatrix}.\n$$\n- 批次输入：\n$$\nX = \\begin{bmatrix}\n0.1  -0.2  0.3 \\\\\n1.0  0.0  -1.0 \\\\\n-0.5  2.0  1.0 \\\\\n0.0  0.0  0.0 \\\\\n3.0  -1.0  0.5\n\\end{bmatrix}.\n$$\n- 专家参数：\n$$\nA_1 = \\begin{bmatrix} 0.2  -0.1  0.0 \\\\ 1.0  0.5  -0.3 \\end{bmatrix}, \\quad c_1 = \\begin{bmatrix} 0.0 \\\\ 0.1 \\end{bmatrix},\n$$\n$$\nA_2 = \\begin{bmatrix} -0.3  0.7  0.2 \\\\ 0.0  -0.2  0.5 \\end{bmatrix}, \\quad c_2 = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix},\n$$\n$$\nA_3 = \\begin{bmatrix} 0.5  0.0  -0.4 \\\\ -0.6  0.9  0.0 \\end{bmatrix}, \\quad c_3 = \\begin{bmatrix} -0.2 \\\\ 0.3 \\end{bmatrix},\n$$\n$$\nA_4 = \\begin{bmatrix} 0.1  0.1  0.1 \\\\ -0.1  -0.2  -0.3 \\end{bmatrix}, \\quad c_4 = \\begin{bmatrix} 0.05 \\\\ -0.05 \\end{bmatrix}.\n$$\n\n门控矩阵 $U$ 的测试套件：\n1. 正常情况：$U = 1.0 \\cdot B$。\n2. 缩放不佳（大值）：$U = 50.0 \\cdot B$。\n3. 边界条件（均匀路由）：$U = 0.0 \\cdot B$。\n4. 行级病态缩放：$U = D B$，其中 $D = \\mathrm{diag}(10.0, 0.1, 5.0, 1.0)$。\n\n对于每种情况：\n- 对所有 $k \\in \\{1,\\dots,n\\}$ 实现前向传播 $y(x_k)$。\n- 计算门控概率 $g(x_k)$ 和期望负载 $S_i$。\n- 计算不平衡比 $r$。\n\n最终输出格式规范：\n- 你的程序应生成单行输出，其中包含四个测试用例的不平衡比率，四舍五入到 $6$ 位小数，以逗号分隔的列表形式包含在方括号中，顺序与上述用例一致（例如，$[r_1,r_2,r_3,r_4]$）。",
            "solution": "所述问题经过严格验证。\n\n### 第 1 步：提取已知条件\n- **模型定义**：一个专家混合（MoE）模型。\n  - 门控函数：$g(x) = \\mathrm{softmax}(U x)$，其中 $U \\in \\mathbb{R}^{m \\times d}$ 且 $x \\in \\mathbb{R}^{d}$。\n  - 专家函数：$E_i(x) = A_i x + c_i$，其中 $A_i \\in \\mathbb{R}^{p \\times d}$ 且 $c_i \\in \\mathbb{R}^{p}$。\n  - MoE 输出：$y(x) = \\sum_{i=1}^{m} g_i(x) E_i(x)$，其中 $g_i(x)$ 是 $g(x)$ 的第 $i$ 个分量。\n- **基本定义**：\n  - 线性映射：矩阵-向量乘法 $Mv$。\n  - Softmax：$\\mathrm{softmax}(z)_i = \\exp(z_i)\\big/\\sum_{j=1}^{m}\\exp(z_j)$。\n- **不平衡量化**：\n  - 专家 $i$ 的期望负载：$S_i = \\sum_{k=1}^{n} g_i(x_k)$，对于一个批次 $X \\in \\mathbb{R}^{n \\times d}$，其行为 $x_k$。\n  - 不平衡比：$r = \\max_{j} S_j \\big/ \\min_{j} S_j$。\n- **维度**：\n  - 专家数量，$m = 4$。\n  - 输入维度，$d = 3$。\n  - 输出维度，$p = 2$。\n  - 批次大小，$n = 5$。\n- **数据**：\n  - 基础门控矩阵，$B = \\begin{bmatrix} 1.0  -2.0  0.5 \\\\ -0.5  3.0  1.5 \\\\ 2.0  0.0  -1.0 \\\\ -1.0  1.0  2.0 \\end{bmatrix}$。\n  - 批次输入，$X = \\begin{bmatrix} 0.1  -0.2  0.3 \\\\ 1.0  0.0  -1.0 \\\\ -0.5  2.0  1.0 \\\\ 0.0  0.0  0.0 \\\\ 3.0  -1.0  0.5 \\end{bmatrix}$。\n  - 专家参数：\n    - $A_1 = \\begin{bmatrix} 0.2  -0.1  0.0 \\\\ 1.0  0.5  -0.3 \\end{bmatrix}, \\quad c_1 = \\begin{bmatrix} 0.0 \\\\ 0.1 \\end{bmatrix}$。\n    - $A_2 = \\begin{bmatrix} -0.3  0.7  0.2 \\\\ 0.0  -0.2  0.5 \\end{bmatrix}, \\quad c_2 = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix}$。\n    - $A_3 = \\begin{bmatrix} 0.5  0.0  -0.4 \\\\ -0.6  0.9  0.0 \\end{bmatrix}, \\quad c_3 = \\begin{bmatrix} -0.2 \\\\ 0.3 \\end{bmatrix}$。\n    - $A_4 = \\begin{bmatrix} 0.1  0.1  0.1 \\\\ -0.1  -0.2  -0.3 \\end{bmatrix}, \\quad c_4 = \\begin{bmatrix} 0.05 \\\\ -0.05 \\end{bmatrix}$。\n- **门控矩阵 $U$ 的测试用例**：\n  1. $U = 1.0 \\cdot B$。\n  2. $U = 50.0 \\cdot B$。\n  3. $U = 0.0 \\cdot B$。\n  4. $U = D B$，其中 $D = \\mathrm{diag}(10.0, 0.1, 5.0, 1.0)$。\n- **任务**：对于每种情况，实现前向传播，计算期望负载 $S_i$ 和不平衡比 $r$。\n\n### 第 2 步：使用提取的已知条件进行验证\n- **科学依据**：该问题描述了标准专家混合层的前向传播，这是神经网络架构中一个成熟的组件。数学运算（矩阵乘法、softmax）和定义（负载平衡）在深度学习领域是标准的。该问题具有科学合理性。\n- **适定性**：所有必需的常量（$m, d, p, n$）、矩阵（$B, X, A_i$）和向量（$c_i$）都已提供。所有指定操作的维度都是一致的。例如，对于表达式 $Ux$，$U \\in \\mathbb{R}^{4 \\times 3}$ 和 $x \\in \\mathbb{R}^{3}$，得到一个 $\\mathbb{R}^{4}$ 中的向量，这符合 $m=4$ 个专家的 softmax 函数的要求。任务定义明确，每个测试用例都有唯一的、可计算的解。\n- **客观性**：问题以精确、形式化的数学和计算术语陈述，没有主观性或歧义。\n\n### 第 3 步：结论与行动\n该问题是有效的。它自成体系、有科学依据且适定。将提供一个完整的解决方案。\n\n### 解决方案\n通过专家混合（MoE）层的前向传播涉及三个主要阶段：门控、专家评估和聚合。目标是计算路由不平衡比 $r$，该比率仅取决于门控机制的输出。\n\n设输入批次为 $X \\in \\mathbb{R}^{n \\times d}$，其中 $n$ 是批次大小，$d$ 是输入特征维度。$X$ 的每一行 $x_k$ 代表一个输入向量。门控矩阵为 $U \\in \\mathbb{R}^{m \\times d}$，其中 $m$ 是专家数量。\n\n**1. 门控网络：线性得分和 Softmax 归一化**\n\n第一步是为每个输入-专家对计算“路由 logit”或线性得分。对于整个批次，这可以高效地通过矩阵乘法计算：\n$$Z = X U^T$$\n结果矩阵 $Z \\in \\mathbb{R}^{n \\times m}$ 包含 logit，其中 $Z_{ki}$ 是将输入 $x_k$ 分配给专家 $i$ 的得分。\n\n接下来，使用 softmax 函数将这些分数为每个输入转换为专家上的概率分布。这被逐行应用于 logit 矩阵 $Z$。门控概率矩阵 $G \\in \\mathbb{R}^{n \\times m}$ 计算如下：\n$$G_{ki} = g_i(x_k) = \\frac{\\exp(Z_{ki})}{\\sum_{j=1}^{m} \\exp(Z_{kj})}$$\n$G$ 的每一行都是一个总和为 1 的概率向量，代表用于组合给定输入的专家输出的权重。为了数值稳定性，一种常用技术是在求幂之前从每行中减去最大 logit：$Z'_{ki} = Z_{ki} - \\max_j(Z_{kj})$。\n\n**2. 专家网络：仿射变换**\n\n虽然计算不平衡比 $r$ 不需要，但完整的前向传播也涉及评估每个专家的输出。每个专家 $i$ 是一个仿射函数 $E_i(x) = A_i x + c_i$。对于输入向量 $x_k$（表示为列向量），专家 $i$ 的输出是一个向量 $y_{ki} \\in \\mathbb{R}^p$。\n$$y_{ki} = A_i x_k^T + c_i$$\n这个计算需要对 $n$ 个输入和 $m$ 个专家中的每一个进行。\n\n**3. 聚合**\n\n单个输入 $x_k$ 的最终输出是各个专家输出的加权和，使用门控概率作为权重：\n$$y(x_k) = \\sum_{i=1}^{m} G_{ki} \\cdot y_{ki} = \\sum_{i=1}^{m} g_i(x_k) E_i(x_k)$$\n结果 $y(x_k)$ 是 $\\mathbb{R}^p$ 中的一个向量。\n\n**4. 不平衡比计算**\n\n核心任务是计算不平衡比 $r$。这需要每个专家的期望负载 $S_i$，即分配给该专家的门控概率在整个批次上的总和：\n$$S_i = \\sum_{k=1}^{n} G_{ki}$$\n这对应于计算门控概率矩阵 $G$ 的列和。获得负载向量 $S = [S_1, S_2, \\dots, S_m]$ 后，不平衡比 $r$ 计算为最大负载与最小负载之比：\n$$r = \\frac{\\max_{j} S_j}{\\min_{j} S_j}$$\n\n**测试用例分析**\n\n我们将此过程应用于门控矩阵 $U$ 的四个指定情况：\n\n- **情况 1: $U = 1.0 \\cdot B$**。这是基线情况。我们预计会出现中等程度的路由不平衡，具体取决于输入数据 $X$ 与 $B$ 中权重向量的对齐情况。\n\n- **情况 2: $U = 50.0 \\cdot B$**。通过一个大的正因子（$50.0$）缩放 logit 会将 softmax 函数推向“硬”最大值，其中具有最高 logit 的专家的概率接近 1，而所有其他专家的概率接近 0。这通常导致稀疏路由，即每个输入仅发送给一个专家。这种专业化常常导致显著的负载不平衡，因此我们预计 $r$ 值会很高。\n\n- **情况 3: $U = 0.0 \\cdot B$**。这里，门控矩阵 $U$ 是零矩阵。因此，所有的 logit $Z_{ki}$ 都为 0。零向量的 softmax 是一个均匀分布：$g_i(x_k) = \\exp(0) / \\sum_j \\exp(0) = 1/m$。每个输入以相等的概率（$1/4 = 0.25$）路由到每个专家。这导致完美的负载平衡。每个专家的负载将是相同的：$S_i = n \\cdot (1/m) = 5/4 = 1.25$。因此，$r = S_{\\max}/S_{\\min} = 1.0$。\n\n- **情况 4: $U = D B$ 且 $D = \\mathrm{diag}(10.0, 0.1, 5.0, 1.0)$**。这对应于缩放基础门控矩阵 $B$ 的行。专家 1 的权重被放大 10.0 倍，而专家 2 的权重被衰减 0.1 倍。这将强烈地偏置路由。输入更可能被路由到具有大缩放因子（专家 1 和 3）的专家，而不那么可能被路由到专家 2。我们预计这会造成显著的不平衡，导致较大的 $r$ 值。\n\n这四个比率的计算是通过编程方式执行的。",
            "answer": "$$\\boxed{[1.868770,2.695371,1.000000,10.603348]}$$"
        }
    ]
}