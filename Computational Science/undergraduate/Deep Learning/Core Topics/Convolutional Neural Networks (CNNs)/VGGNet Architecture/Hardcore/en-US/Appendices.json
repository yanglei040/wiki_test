{
    "hands_on_practices": [
        {
            "introduction": "Before training or deploying any deep learning model, it's crucial to understand its computational and memory footprint, especially for VGG-style networks where stacking many layers can lead to significant resource demands. This practice involves deriving the computational complexity, measured in Floating-Point Operations per second ($FLOPs$), and memory traffic for a standard convolutional layer from first principles. By applying these derivations to different VGG variants, you'll develop the essential skill of performing \"back-of-the-envelope\" calculations to select an architecture that fits within a specific hardware budget, a vital task for any machine learning engineer .",
            "id": "3198589",
            "problem": "A team is sizing simplified variants of the Visual Geometry Group (VGG) network architecture for deployment on a device with limited computational and memory bandwidth resources. Each variant consists of stacks of two-dimensional convolution layers with kernel size $K$, stride $1$, and zero-padding that preserves spatial dimensions. Ignore bias terms, batch normalization, nonlinearities, and pooling compute or traffic; average pooling only changes $(H, W)$ for subsequent layers. Every tensor element is stored in single-precision floating point, so each element occupies $4$ bytes.\n\nStarting from the definition of discrete two-dimensional convolution, derive the forward-pass floating-point operation count per convolution layer by counting multiplications and additions from first principles. Use the standard benchmarking convention that one multiply-add contributes two floating-point operations. Also derive a per-layer main-memory traffic model assuming that, for each convolution layer, the input activation tensor and the filter tensor are read once from main memory, and the output activation tensor is written once to main memory.\n\nThree candidate variants are given. For all convolution layers, the kernel size is $K=3$ and padding preserves $(H, W)$. The input image has spatial size $64 \\times 64$ with $3$ channels. The variants are:\n\n- Variant A:\n  - Layer $1$: $(H, W, C_{\\text{in}}, C_{\\text{out}}) = (64, 64, 3, 16)$.\n  - Layer $2$: $(64, 64, 16, 16)$.\n  - Average pooling halves spatial dimensions: $(H, W) \\mapsto (32, 32)$.\n  - Layer $3$: $(32, 32, 16, 32)$.\n  - Layer $4$: $(32, 32, 32, 32)$.\n  - Average pooling halves spatial dimensions: $(H, W) \\mapsto (16, 16)$.\n  - Layer $5$: $(16, 16, 32, 64)$.\n  - Layer $6$: $(16, 16, 64, 64)$.\n\n- Variant B:\n  - Layer $1$: $(64, 64, 3, 16)$.\n  - Layer $2$: $(64, 64, 16, 16)$.\n  - Average pooling: $(H, W) \\mapsto (32, 32)$.\n  - Layer $3$: $(32, 32, 16, 32)$.\n  - Layer $4$: $(32, 32, 32, 32)$.\n  - Layer $5$: $(32, 32, 32, 32)$.\n  - Average pooling: $(H, W) \\mapsto (16, 16)$.\n  - Layer $6$: $(16, 16, 32, 64)$.\n  - Layer $7$: $(16, 16, 64, 64)$.\n  - Layer $8$: $(16, 16, 64, 64)$.\n\n- Variant C:\n  - Layer $1$: $(64, 64, 3, 32)$.\n  - Layer $2$: $(64, 64, 32, 32)$.\n  - Average pooling: $(H, W) \\mapsto (32, 32)$.\n  - Layer $3$: $(32, 32, 32, 64)$.\n  - Layer $4$: $(32, 32, 64, 64)$.\n  - Average pooling: $(H, W) \\mapsto (16, 16)$.\n  - Layer $5$: $(16, 16, 64, 128)$.\n  - Layer $6$: $(16, 16, 128, 128)$.\n\nLet the compute budget be $2.0 \\times 10^{8}$ floating-point operations per forward pass, and the main-memory bandwidth budget be $3.0 \\times 10^{6}$ bytes per forward pass. Among the variants that satisfy both budgets, select the one with the largest total floating-point operation count. Report, as your final answer, the exact total floating-point operation count of the selected variant as a single integer. Do not include units in the final answer.",
            "solution": "The problem statement is critically validated and is deemed to be self-contained, consistent, and scientifically sound. It is well-posed, objective, and can be formalized into a solvable problem within the domain of deep learning performance analysis. No flaws are identified. We may proceed with the solution.\n\nFirst, we derive the general expressions for the forward-pass floating-point operation (FLOP) count and main-memory traffic for a single two-dimensional convolution layer, as specified in the problem statement.\n\nLet the input activation tensor have dimensions $(H, W, C_{\\text{in}})$, representing height, width, and number of input channels, respectively.\nLet the output activation tensor have dimensions $(H_{\\text{out}}, W_{\\text{out}}, C_{\\text{out}})$.\nLet the convolution kernel size be $K \\times K$.\nThe problem states that padding is used to preserve spatial dimensions, so $H_{\\text{out}} = H$ and $W_{\\text{out}} = W$. The stride is given as $1$.\n\nDerivation of Floating-Point Operations (FLOPs):\nTo compute a single value in one output feature map (one channel of the output tensor), a $K \\times K \\times C_{\\text{in}}$ filter is applied to a corresponding patch of the input tensor. This involves $K \\times K \\times C_{\\text{in}}$ multiplication operations and $(K \\times K \\times C_{\\text{in}} - 1)$ addition operations. Since bias is ignored, there are no further additions.\nThe total number of operations to produce one output value is $(K^2 C_{\\text{in}}) + (K^2 C_{\\text{in}} - 1)$.\nUsing the standard benchmarking convention that one multiply-add operation contributes two FLOPs, we can approximate the cost for one output value as $2 \\times K^2 \\times C_{\\text{in}}$.\nThis computation must be performed for every spatial location in the output tensor, of which there are $H \\times W$.\nFurthermore, this must be done for each of the $C_{\\text{out}}$ output channels.\nTherefore, the total FLOP count, $F$, for a single convolution layer is:\n$$F = (2 \\cdot K^2 \\cdot C_{\\text{in}}) \\cdot (H \\cdot W \\cdot C_{\\text{out}})$$\n$$F = 2 \\cdot H \\cdot W \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2$$\n\nDerivation of Main-Memory Traffic:\nThe memory traffic, $M$, is the sum of bytes read from and written to main memory. Each tensor element is a single-precision float, occupying $4$ bytes.\n$1$. Input tensor read: The entire input tensor of size $H \\times W \\times C_{\\text{in}}$ is read once. Traffic is $4 \\cdot H \\cdot W \\cdot C_{\\text{in}}$ bytes.\n$2$. Filter tensor read: The convolution filters (weights) are read once. There are $C_{\\text{out}}$ filters, each of size $K \\times K \\times C_{\\text{in}}$. Total size is $K^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}$. Traffic is $4 \\cdot K^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}$ bytes.\n$3$. Output tensor written: The resulting output tensor of size $H \\times W \\times C_{\\text{out}}$ is written once. Traffic is $4 \\cdot H \\cdot W \\cdot C_{\\text{out}}$ bytes.\nThe total memory traffic per layer is the sum of these three components:\n$$M = 4 \\cdot H \\cdot W \\cdot C_{\\text{in}} + 4 \\cdot K^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} + 4 \\cdot H \\cdot W \\cdot C_{\\text{out}}$$\n$$M = 4 \\cdot (H \\cdot W \\cdot (C_{\\text{in}} + C_{\\text{out}}) + K^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}})$$\n\nNow, we evaluate each variant using $K=3$ (so $K^2=9$).\nThe budgets are: FLOPs $\\le 2.0 \\times 10^8$ and Memory $\\le 3.0 \\times 10^6$ bytes.\n\nVariant A:\n- Layer $1$: $(H, W, C_{\\text{in}}, C_{\\text{out}}) = (64, 64, 3, 16)$\n  $F_1 = 2 \\cdot 64 \\cdot 64 \\cdot 3 \\cdot 16 \\cdot 9 = 3,538,944$\n  $M_1 = 4 \\cdot (64 \\cdot 64 \\cdot (3+16) + 9 \\cdot 3 \\cdot 16) = 4 \\cdot (4096 \\cdot 19 + 432) = 313,024$\n- Layer $2$: $(64, 64, 16, 16)$\n  $F_2 = 2 \\cdot 64 \\cdot 64 \\cdot 16 \\cdot 16 \\cdot 9 = 18,874,368$\n  $M_2 = 4 \\cdot (4096 \\cdot (16+16) + 9 \\cdot 16 \\cdot 16) = 4 \\cdot (4096 \\cdot 32 + 2304) = 533,504$\n- Layer $3$: $(32, 32, 16, 32)$\n  $F_3 = 2 \\cdot 32 \\cdot 32 \\cdot 16 \\cdot 32 \\cdot 9 = 9,437,184$\n  $M_3 = 4 \\cdot (1024 \\cdot (16+32) + 9 \\cdot 16 \\cdot 32) = 4 \\cdot (1024 \\cdot 48 + 4608) = 215,040$\n- Layer $4$: $(32, 32, 32, 32)$\n  $F_4 = 2 \\cdot 32 \\cdot 32 \\cdot 32 \\cdot 32 \\cdot 9 = 18,874,368$\n  $M_4 = 4 \\cdot (1024 \\cdot (32+32) + 9 \\cdot 32 \\cdot 32) = 4 \\cdot (1024 \\cdot 64 + 9216) = 299,008$\n- Layer $5$: $(16, 16, 32, 64)$\n  $F_5 = 2 \\cdot 16 \\cdot 16 \\cdot 32 \\cdot 64 \\cdot 9 = 9,437,184$\n  $M_5 = 4 \\cdot (256 \\cdot (32+64) + 9 \\cdot 32 \\cdot 64) = 4 \\cdot (256 \\cdot 96 + 18432) = 172,032$\n- Layer $6$: $(16, 16, 64, 64)$\n  $F_6 = 2 \\cdot 16 \\cdot 16 \\cdot 64 \\cdot 64 \\cdot 9 = 18,874,368$\n  $M_6 = 4 \\cdot (256 \\cdot (64+64) + 9 \\cdot 64 \\cdot 64) = 4 \\cdot (256 \\cdot 128 + 36864) = 278,224$\n\n- Total for Variant A:\n  $F_A = 3538944 + 18874368 + 9437184 + 18874368 + 9437184 + 18874368 = 79,036,416$\n  $M_A = 313024 + 533504 + 215040 + 299008 + 172032 + 278224 = 1,810,832$\n- Budget Check: $F_A = 7.90 \\times 10^7 \\le 2.0 \\times 10^8$ (Pass). $M_A = 1.81 \\times 10^6 \\le 3.0 \\times 10^6$ (Pass).\nVariant A is a valid candidate.\n\nVariant B:\n- Layers $1-4$ are a subset of B's layers. Layers 1-2, pooling, Layer 3-5, pooling, Layers 6-8.\n- Layers $1, 2, 3, 4, 5$: $(64,64,3,16), (64,64,16,16), (32,32,16,32), (32,32,32,32), (32,32,32,32)$\n  $F_1=3,538,944, M_1=313,024$\n  $F_2=18,874,368, M_2=533,504$\n  $F_3=9,437,184, M_3=215,040$\n  $F_4=18,874,368, M_4=299,008$\n  $F_5=18,874,368, M_5=299,008$\n- Layers $6, 7, 8$: $(16,16,32,64), (16,16,64,64), (16,16,64,64)$\n  $F_6 = 2 \\cdot 16 \\cdot 16 \\cdot 32 \\cdot 64 \\cdot 9 = 9,437,184$, $M_6=172,032$\n  $F_7 = 2 \\cdot 16 \\cdot 16 \\cdot 64 \\cdot 64 \\cdot 9 = 18,874,368$, $M_7=278,224$\n  $F_8 = 18,874,368$, $M_8=278,224$\n- Total for Variant B:\n  $F_B = 3538944 + 18874368 + 9437184 + 18874368 + 18874368 + 9437184 + 18874368 + 18874368 = 116,785,152$\n  $M_B = 313024 + 533504 + 215040 + 299008 + 299008 + 172032 + 278224 + 278224 = 2,388,064$\n- Budget Check: $F_B = 1.17 \\times 10^8 \\le 2.0 \\times 10^8$ (Pass). $M_B = 2.39 \\times 10^6 \\le 3.0 \\times 10^6$ (Pass).\nVariant B is a valid candidate.\n\nVariant C:\n- Layer $1$: $(64, 64, 3, 32)$\n  $F_1 = 2 \\cdot 64 \\cdot 64 \\cdot 3 \\cdot 32 \\cdot 9 = 7,077,888$\n  $M_1 = 4 \\cdot (4096 \\cdot (3+32) + 9 \\cdot 3 \\cdot 32) = 576,896$\n- Layer $2$: $(64, 64, 32, 32)$\n  $F_2 = 2 \\cdot 64 \\cdot 64 \\cdot 32 \\cdot 32 \\cdot 9 = 37,748,736$\n  $M_2 = 4 \\cdot (4096 \\cdot (32+32) + 9 \\cdot 32 \\cdot 32) = 1,085,440$\n- Layer $3$: $(32, 32, 32, 64)$\n  $F_3 = 2 \\cdot 32 \\cdot 32 \\cdot 32 \\cdot 64 \\cdot 9 = 37,748,736$\n  $M_3 = 4 \\cdot (1024 \\cdot (32+64) + 9 \\cdot 32 \\cdot 64) = 466,944$\n- Layer $4$: $(32, 32, 64, 64)$\n  $F_4 = 2 \\cdot 32 \\cdot 32 \\cdot 64 \\cdot 64 \\cdot 9 = 75,497,472$\n  $M_4 = 4 \\cdot (1024 \\cdot (64+64) + 9 \\cdot 64 \\cdot 64) = 671,744$\n- Layer $5$: $(16, 16, 64, 128)$\n  $F_5 = 2 \\cdot 16 \\cdot 16 \\cdot 64 \\cdot 128 \\cdot 9 = 37,748,736$\n  $M_5 = 4 \\cdot (256 \\cdot (64+128) + 9 \\cdot 64 \\cdot 128) = 491,520$\n- Layer $6$: $(16, 16, 128, 128)$\n  $F_6 = 2 \\cdot 16 \\cdot 16 \\cdot 128 \\cdot 128 \\cdot 9 = 75,497,472$\n  $M_6 = 4 \\cdot (256 \\cdot (128+128) + 9 \\cdot 128 \\cdot 128) = 851,968$\n\n- Total for Variant C:\n  $F_C = 7077888 + 37748736 + 37748736 + 75497472 + 37748736 + 75497472 = 271,319,040$\n  $M_C = 576896 + 1085440 + 466944 + 671744 + 491520 + 851968 = 4,144,512$\n- Budget Check: $F_C = 2.71 \\times 10^8 > 2.0 \\times 10^8$ (Fail). $M_C = 4.14 \\times 10^6 > 3.0 \\times 10^6$ (Fail).\nVariant C is not a valid candidate.\n\nComparison and Selection:\nThe valid candidates are Variant A and Variant B. We must select the one with the largest total FLOP count.\n$F_A = 79,036,416$\n$F_B = 116,785,152$\nSince $F_B > F_A$, Variant B is the selected model. The problem asks for the exact total floating-point operation count of this selected variant.\nThe required value is $F_B = 116,785,152$.",
            "answer": "$$\\boxed{116785152}$$"
        },
        {
            "introduction": "The performance of a neural network is highly sensitive to the distribution of its activations, and a common issue is the \"dying ReLU\" problem, where neurons become stuck outputting zero. This exercise uses the Central Limit Theorem to model the pre-activation values as a Gaussian distribution, allowing you to analytically estimate the probability of Rectified Linear Unit ($ReLU$) saturation. You will explore how different input normalization strategies directly impact this probability, providing a concrete, statistical justification for why preprocessing techniques like standardization are so critical for stable training .",
            "id": "3198668",
            "problem": "You are studying the first convolutional layers of the Visual Geometry Group (VGG) network (VGGNet), which uses small spatial filters and the Rectified Linear Unit (ReLU) nonlinearity. Consider a single pre-activation in such a layer, modeled as a linear combination of inputs followed by adding a bias. Your goal is to examine how two common input preprocessing pipelines affect the probability that the first-layer ReLU is saturated at zero. The two pipelines are: (i) full normalization $x \\mapsto (x-\\mu)/\\sigma$, and (ii) mean subtraction only $x \\mapsto x-\\mu$. Assume the following fundamental base and modeling assumptions: (a) convolution is linear, so a pre-activation is a weighted sum of receptive field inputs plus a bias; (b) inputs within a local receptive field are independent and identically distributed within each channel after preprocessing; (c) weights are independent across the receptive field, with given mean and variance; (d) a pre-activation with many small independent contributions is well-approximated as Gaussian by the Central Limit Theorem; (e) the ReLU activation saturates at zero whenever the pre-activation is non-positive. From this base, derive an analytic expression for the saturation probability using only core definitions of expectation and variance for linear combinations of independent random variables, and then implement it.\n\nDefinitions:\n- A pre-activation $z$ in a convolutional neuron is modeled as $z = \\sum_{i=1}^{n} w_i x_i + b$, where $n$ is the receptive field size $n = C \\cdot k_h \\cdot k_w$, $w_i$ are weights, $x_i$ are the preprocessed inputs, and $b$ is the bias.\n- The Rectified Linear Unit (ReLU) outputs $\\max\\{0, z\\}$ and is saturated at zero when $z \\le 0$.\n- Assume weights satisfy $\\mathbb{E}[w_i] = 0$ and $\\mathrm{Var}(w_i) = s_w^2$, and inputs after preprocessing have channel-wise mean $\\mathbb{E}[x_i]=0$ with variance $\\mathrm{Var}(x_i) = s_x^2$. Under the full normalization pipeline, $s_x^2 = 1$. Under mean subtraction only, $s_x^2 = \\sigma^2$, where $\\sigma$ is provided.\n- Use the Gaussian approximation for $z$ implied by the Central Limit Theorem and the independence assumptions to estimate the saturation probability $\\mathbb{P}[z \\le 0]$ using the standard Normal Cumulative Distribution Function (CDF).\n\nImplement a program that, for each test case, computes two quantities:\n- The saturation probability under full normalization $(x-\\mu)/\\sigma$.\n- The saturation probability under mean subtraction only $(x-\\mu)$.\n\nUse only the assumptions above and the laws of expectation and variance of sums of independent variables to derive the needed expression for each case. Do not use any external data. No user input should be read.\n\nTest suite:\nEach test case is given as a tuple $(n, s_w^2, b, \\sigma)$ with:\n- $n$: receptive field size $C \\cdot k_h \\cdot k_w$,\n- $s_w^2$: the weight variance,\n- $b$: the bias,\n- $\\sigma$: the input standard deviation after mean subtraction only (so $s_x^2 = \\sigma^2$ for mean subtraction; for full normalization, use $s_x^2 = 1$).\n\nProvide answers as decimal numbers. For numerical stability and comparability, round each probability to exactly $6$ digits after the decimal point.\n\nUse the following test cases:\n- Case $1$: $n = 27$, $s_w^2 = 2/27$, $b = 0.1$, $\\sigma = 0.5$.\n- Case $2$: $n = 576$, $s_w^2 = 2/576$, $b = 0.0$, $\\sigma = 1.8$.\n- Case $3$: $n = 27$, $s_w^2 = 1/27$, $b = -0.2$, $\\sigma = 2.0$.\n- Case $4$: $n = 27$, $s_w^2 = 2/27$, $b = 0.3$, $\\sigma = 0.1$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$[p_{\\text{norm},1}, p_{\\text{mean},1}, p_{\\text{norm},2}, p_{\\text{mean},2}, p_{\\text{norm},3}, p_{\\text{mean},3}, p_{\\text{norm},4}, p_{\\text{mean},4}]$,\nwhere $p_{\\text{norm},j}$ is the saturation probability for case $j$ under full normalization and $p_{\\text{mean},j}$ is the saturation probability for case $j$ under mean subtraction only. Each value must be rounded to exactly $6$ digits after the decimal point.",
            "solution": "The problem requires the derivation and implementation of an analytic expression for the saturation probability of a Rectified Linear Unit (ReLU) in the first layer of a VGG-like neural network. The saturation occurs when the pre-activation value is non-positive. We will analyze how this probability is affected by two different input preprocessing pipelines: full normalization and mean subtraction only.\n\nThe analysis hinges on the provided model for a pre-activation, $z$, and a set of statistical assumptions. The pre-activation is given by the linear combination:\n$$z = \\sum_{i=1}^{n} w_i x_i + b$$\nwhere $w_i$ are the weights, $x_i$ are the preprocessed inputs from the receptive field of size $n$, and $b$ is a scalar bias term. The ReLU activation function is defined as $\\max\\{0, z\\}$, which saturates at an output of $0$ if and only if its input $z$ is non-positive, i.e., $z \\le 0$. Our objective is to compute the probability of this event, $\\mathbb{P}[z \\le 0]$.\n\nThe problem states that due to the Central Limit Theorem, the pre-activation $z$, being a sum of many small independent contributions, can be well-approximated by a Gaussian (Normal) random variable. Let us denote its mean by $\\mu_z$ and its variance by $\\sigma_z^2$, such that $z \\sim \\mathcal{N}(\\mu_z, \\sigma_z^2)$.\n\nTo find the probability $\\mathbb{P}[z \\le 0]$, we can standardize the random variable $z$ and use the cumulative distribution function (CDF) of the standard normal distribution, denoted by $\\Phi(\\cdot)$.\n$$ \\mathbb{P}[z \\le 0] = \\mathbb{P}\\left(\\frac{z - \\mu_z}{\\sigma_z} \\le \\frac{0 - \\mu_z}{\\sigma_z}\\right) = \\Phi\\left(-\\frac{\\mu_z}{\\sigma_z}\\right) $$\nThe core of the problem is to determine the expressions for $\\mu_z$ and $\\sigma_z^2$ based on the statistical properties of the weights $w_i$ and inputs $x_i$.\n\n**1. Derivation of the Mean of the Pre-activation, $\\mu_z$**\n\nThe mean of $z$, $\\mu_z$, is its expectation, $\\mathbb{E}[z]$. Using the linearity of expectation:\n$$ \\mu_z = \\mathbb{E}[z] = \\mathbb{E}\\left[\\sum_{i=1}^{n} w_i x_i + b\\right] = \\sum_{i=1}^{n} \\mathbb{E}[w_i x_i] + \\mathbb{E}[b] $$\nThe bias $b$ is a constant, so $\\mathbb{E}[b] = b$. The problem assumes that the weights $w_i$ and inputs $x_i$ are independent random variables. Therefore, the expectation of their product is the product of their expectations: $\\mathbb{E}[w_i x_i] = \\mathbb{E}[w_i] \\mathbb{E}[x_i]$.\nThe problem provides the following statistical properties:\n-   $\\mathbb{E}[w_i] = 0$ for all $i \\in \\{1, \\dots, n\\}$.\n-   $\\mathbb{E}[x_i] = 0$ for all $i \\in \\{1, \\dots, n\\}$ (for both preprocessing pipelines).\nSubstituting these into the expression for $\\mathbb{E}[w_i x_i]$ gives $\\mathbb{E}[w_i x_i] = 0 \\cdot 0 = 0$.\nThus, the mean of the pre-activation is:\n$$ \\mu_z = \\sum_{i=1}^{n} 0 + b = b $$\n\n**2. Derivation of the Variance of the Pre-activation, $\\sigma_z^2$**\n\nThe variance of $z$, $\\sigma_z^2$, is $\\mathrm{Var}(z)$.\n$$ \\sigma_z^2 = \\mathrm{Var}(z) = \\mathrm{Var}\\left(\\sum_{i=1}^{n} w_i x_i + b\\right) $$\nAdding a constant bias $b$ does not affect the variance, so $\\mathrm{Var}(z) = \\mathrm{Var}\\left(\\sum_{i=1}^{n} w_i x_i\\right)$.\nThe problem states that the weights $w_i$ are independent of each other, and the inputs $x_i$ are independent of each other. A standard assumption in this context is that all $w_i$ and $x_j$ are mutually independent for all $i,j$. This implies that the product terms $w_i x_i$ are also independent. For a sum of independent random variables, the variance of the sum is the sum of the variances:\n$$ \\sigma_z^2 = \\sum_{i=1}^{n} \\mathrm{Var}(w_i x_i) $$\nTo calculate $\\mathrm{Var}(w_i x_i)$, we use the formula for the variance of a product of two independent random variables, $A$ and $B$: $\\mathrm{Var}(AB) = \\mathrm{Var}(A)\\mathrm{Var}(B) + (\\mathbb{E}[A])^2\\mathrm{Var}(B) + (\\mathbb{E}[B])^2\\mathrm{Var}(A)$. Given $\\mathbb{E}[w_i]=0$ and $\\mathbb{E}[x_i]=0$, this formula simplifies. An equivalent general formula is $\\mathrm{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$.\nFor $Y_i = w_i x_i$, we have $\\mathbb{E}[Y_i] = \\mathbb{E}[w_i]\\mathbb{E}[x_i] = 0 \\cdot 0 = 0$.\nSo, $\\mathrm{Var}(w_i x_i) = \\mathbb{E}[(w_i x_i)^2] = \\mathbb{E}[w_i^2 x_i^2]$.\nDue to independence, $\\mathbb{E}[w_i^2 x_i^2] = \\mathbb{E}[w_i^2] \\mathbb{E}[x_i^2]$.\nWe can express $\\mathbb{E}[A^2]$ in terms of variance and mean: $\\mathbb{E}[A^2] = \\mathrm{Var}(A) + (\\mathbb{E}[A])^2$.\nThe problem provides $\\mathrm{Var}(w_i) = s_w^2$ and $\\mathrm{Var}(x_i) = s_x^2$.\nSo, $\\mathbb{E}[w_i^2] = \\mathrm{Var}(w_i) + (\\mathbb{E}[w_i])^2 = s_w^2 + 0^2 = s_w^2$.\nAnd, $\\mathbb{E}[x_i^2] = \\mathrm{Var}(x_i) + (\\mathbb{E}[x_i])^2 = s_x^2 + 0^2 = s_x^2$.\nTherefore, $\\mathrm{Var}(w_i x_i) = s_w^2 s_x^2$.\nSince all weights are identically distributed, and all inputs are identically distributed, this variance is the same for all $i$. The total variance is:\n$$ \\sigma_z^2 = \\sum_{i=1}^{n} s_w^2 s_x^2 = n s_w^2 s_x^2 $$\n\n**3. Final Expressions for Saturation Probability**\n\nNow we have $\\mu_z = b$ and $\\sigma_z^2 = n s_w^2 s_x^2$. The standard deviation is $\\sigma_z = \\sqrt{n s_w^2 s_x^2}$.\nThe saturation probability is:\n$$ \\mathbb{P}[z \\le 0] = \\Phi\\left(-\\frac{\\mu_z}{\\sigma_z}\\right) = \\Phi\\left(-\\frac{b}{\\sqrt{n s_w^2 s_x^2}}\\right) $$\nWe now specialize this expression for the two preprocessing pipelines.\n\n**Case (i): Full Normalization**\nThe mapping $x \\mapsto (x - \\mu) / \\sigma$ ensures that the preprocessed inputs have a variance of $1$.\nThus, $s_x^2 = 1$.\nThe saturation probability, $p_{\\text{norm}}$, is:\n$$ p_{\\text{norm}} = \\Phi\\left(-\\frac{b}{\\sqrt{n s_w^2 \\cdot 1}}\\right) = \\Phi\\left(-\\frac{b}{\\sqrt{n s_w^2}}\\right) $$\n\n**Case (ii): Mean Subtraction Only**\nThe mapping $x \\mapsto x - \\mu$ ensures the mean is $0$, but the variance remains the variance of the original data. The problem denotes the corresponding standard deviation by $\\sigma$.\nThus, $s_x^2 = \\sigma^2$.\nThe saturation probability, $p_{\\text{mean}}$, is:\n$$ p_{\\text{mean}} = \\Phi\\left(-\\frac{b}{\\sqrt{n s_w^2 \\sigma^2}}\\right) = \\Phi\\left(-\\frac{b}{\\sigma \\sqrt{n s_w^2}}\\right) $$\n\nThese two formulas will be implemented to solve for the probabilities in the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the ReLU saturation probability for two input preprocessing pipelines\n    based on a Gaussian approximation of the pre-activation.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (n, s_w^2, b, sigma).\n    test_cases = [\n        (27, 2/27, 0.1, 0.5), # Case 1\n        (576, 2/576, 0.0, 1.8), # Case 2\n        (27, 1/27, -0.2, 2.0), # Case 3\n        (27, 2/27, 0.3, 0.1), # Case 4\n    ]\n\n    results = []\n    for n, s_w_sq, b, sigma in test_cases:\n        # The term sqrt(n * Var(w)) is common to both calculations.\n        # This term represents the standard deviation of the sum of weighted inputs\n        # when the input variance is 1.\n        std_dev_base = np.sqrt(n * s_w_sq)\n\n        # Handle the case where the base standard deviation is zero, to avoid division by zero.\n        # This occurs if n=0 or s_w_sq=0. In this case, z = b, so P(z<=0) is 1 if b<=0 and 0 if b>0.\n        if std_dev_base == 0:\n            p_norm = 1.0 if b <= 0 else 0.0\n        else:\n            # Case (i): Full Normalization (input variance s_x^2 = 1)\n            # The argument to the standard normal CDF.\n            arg_norm = -b / std_dev_base\n            p_norm = norm.cdf(arg_norm)\n        \n        results.append(p_norm)\n        \n        # Denominator for the mean subtraction case.\n        # This is sigma * std_dev_base\n        std_dev_z_mean = sigma * std_dev_base\n\n        # Handle potential division by zero if sigma=0 and/or std_dev_base=0.\n        if std_dev_z_mean == 0:\n            p_mean = 1.0 if b <= 0 else 0.0\n        else:\n            # Case (ii): Mean Subtraction Only (input variance s_x^2 = sigma^2)\n            # The argument to the standard normal CDF.\n            arg_mean = -b / std_dev_z_mean\n            p_mean = norm.cdf(arg_mean)\n        \n        results.append(p_mean)\n\n    # Format the final output as a single string, with each probability\n    # rounded and formatted to exactly 6 decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Modern deep learning relies on hardware acceleration, and techniques like mixed-precision training are key to achieving state-of-the-art speed. However, using lower-precision arithmetic, such as 16-bit floats ($FP16$), introduces numerical challenges like gradient underflow. This practice requires you to implement a training loop from scratch to simulate mixed-precision training, including the essential technique of loss scaling to preserve small gradient information. By building this simulation, you will gain a deep, first-principles understanding of the trade-offs involved and why loss scaling is a principled solution to a fundamental numerical problem .",
            "id": "3198711",
            "problem": "You are asked to implement a principled simulation of mixed-precision training for a small Visual Geometry Group Network (VGG)-like convolutional neural network, with explicit loss scaling to mitigate gradient underflow in half-precision arithmetic. Your program must be a complete, runnable implementation that trains a tiny model from first principles using only array operations, simulates mixed-precision arithmetic by casting to half-precision where appropriate, and computes a theoretical speedup and any change in final accuracy relative to a full-precision baseline. All mathematical definitions and targets are expressed below in a form suitable for algorithmic derivation.\n\nFundamental base to use:\n- Convolutional layer operation: a two-dimensional convolution with stride $1$ and zero padding that preserves spatial size, using a kernel of size $3 \\times 3$. A multiply-accumulate (MAC) operation is the repeated pattern of multiplication followed by addition. For a single convolutional layer with input shape $\\left(C_{\\text{in}}, H, W\\right)$, output channels $C_{\\text{out}}$, and kernel size $K \\times K$ with stride $1$ and padding, the total forward-pass MAC count is $C_{\\text{out}} \\cdot H \\cdot W \\cdot C_{\\text{in}} \\cdot K \\cdot K$. For training, assume the backward-pass cost for gradients with respect to inputs and weights each has a MAC count on the same order as the forward pass, so the total MACs per training step per convolution is approximately $3$ times the forward-pass MACs. For a fully connected (linear) layer with input dimension $d_{\\text{in}}$ and output dimension $d_{\\text{out}}$, the forward-pass MAC count is $d_{\\text{in}} \\cdot d_{\\text{out}}$, and similarly assume a factor of approximately $3$ for one training step including backward.\n- Mixed-precision with loss scaling: use Institute of Electrical and Electronics Engineers (IEEE) $754$ binary $16$ (half precision) for compute, and store a master copy of parameters in binary $32$ (single precision). Let $\\alpha$ be a positive scaling factor. Scaling the scalar loss by $\\alpha$ scales every gradient by $\\alpha$ during backpropagation: if $\\nabla_{\\theta} \\mathcal{L}$ denotes the gradient in full precision, then the gradient computed in half precision after scaling is approximately $\\operatorname{fp16}\\!\\left(\\alpha \\cdot \\nabla_{\\theta} \\mathcal{L}\\right)$, which is then unscaled by dividing by $\\alpha$ in full precision before applying the update. The purpose is to reduce underflow of small gradients in half precision. IEEE $754$ binary $16$ has a smallest positive normal value approximately $2^{-14} \\approx 6.1035 \\times 10^{-5}$ and a smallest positive subnormal value approximately $2^{-24} \\approx 5.9605 \\times 10^{-8}$; values below the subnormal minimum underflow to zero in half precision.\n- Throughput model: Let $T_{32}$ denote the effective throughput for binary $32$ computations and $T_{16}$ denote the effective throughput for binary $16$ computations. Assume $T_{16} = s \\cdot T_{32}$ with $s = 2.0$. For a fixed number of floating point operations (FLOPs), the idealized speedup of mixed precision over full precision is $$S = \\frac{\\text{time}_{32}}{\\text{time}_{16}} = \\frac{\\text{FLOPs} / T_{32}}{\\text{FLOPs} / T_{16}} = \\frac{T_{16}}{T_{32}} = s.$$\n\nModel and training setup to implement:\n- Architecture (a minimal VGG-like stack): two convolutional layers with kernel size $3 \\times 3$, stride $1$, and zero padding to maintain $8 \\times 8$ spatial resolution, followed by Rectified Linear Unit (ReLU) nonlinearity after each convolution, then global average pooling (channel-wise mean over spatial positions), and a final fully connected (linear) classifier. The channel configuration is $1 \\to 2 \\to 2$ for the convolutional layers, then a linear map $\\mathbb{R}^{2} \\to \\mathbb{R}^{2}$ for two classes. No bias terms are required. The input dimension is $1 \\times 8 \\times 8$.\n- Loss: softmax cross-entropy. Use a numerically stable softmax by subtracting the maximum logit per sample.\n- Optimizer: stochastic gradient descent with learning rate $\\eta$ applied to the master $32$-bit parameters after unscaling gradients (if any scaling was used). Use a constant learning rate $\\eta = 10^{8}$.\n- Data: construct a deterministic two-class dataset of $8$ samples with input scale $\\sigma = 10^{-4}$. For class $0$, the image has a centered $2 \\times 2$ block of ones scaled by $\\sigma$ and zeros elsewhere. For class $1$, the image has ones scaled by $\\sigma$ along the four borders (top row, bottom row, left column, right column) and zeros elsewhere. Create $4$ copies of each class to obtain $8$ samples. Labels are one-hot vectors in $\\mathbb{R}^{2}$. The batch size is $1$; at each step use the sample with index equal to the step modulo $8$.\n- Initialization: initialize all weights with independent Gaussian samples with zero mean and standard deviation $\\tau = 10^{-3}$, using a fixed random seed to ensure determinism.\n- Mixed-precision simulation: for full precision, perform all computations in binary $32$. For mixed precision, on each step:\n  - cast the current master weights to binary $16$ for the forward and backward computations;\n  - compute the loss and its gradient with respect to logits; multiply the loss gradient by the loss-scaling factor $\\alpha$; cast it to binary $16$ before backpropagation;\n  - compute gradients with respect to the cast weights in binary $16$;\n  - unscale the gradients by dividing by $\\alpha$ in binary $32$ and apply the stochastic gradient descent update to the master weights.\n  If any intermediate or parameter value becomes not-a-number or infinite at any point, treat the training run as failed and set the resulting accuracy to $0$ for that run.\n- Training length: run exactly $E = 20$ update steps.\n\nQuantities to compute and report:\n- Let $A_{\\text{fp32}}$ be the final top-$1$ accuracy (as a decimal fraction in $[0,1]$) of the full-precision run evaluated on the training set after $E$ steps, using forward propagation in binary $32$.\n- For each mixed-precision run with a given $\\alpha$, let $A_{\\text{mixed}}(\\alpha)$ be the final top-$1$ accuracy of the mixed-precision trained model evaluated on the training set in binary $32$ after $E$ steps.\n- For each mixed-precision run, report the theoretical speedup $S = s = 2.0$ and the accuracy change $\\Delta A(\\alpha) = A_{\\text{mixed}}(\\alpha) - A_{\\text{fp32}}$.\n\nTest suite:\n- Use the following three loss-scaling factors:\n  - $\\alpha_{1} = 1$\n  - $\\alpha_{2} = 8192$ (that is, $2^{13}$)\n  - $\\alpha_{3} = 1048576$ (that is, $2^{20}$)\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in the same order $\\left[\\alpha_{1}, \\alpha_{2}, \\alpha_{3}\\right]$. Each element must itself be a list of two floats in the order $\\left[S, \\Delta A(\\alpha)\\right]$. For example, the printed line must look like a list of lists: $\\left[\\left[S_{1}, \\Delta A\\left(\\alpha_{1}\\right)\\right], \\left[S_{2}, \\Delta A\\left(\\alpha_{2}\\right)\\right], \\left[S_{3}, \\Delta A\\left(\\alpha_{3}\\right)\\right]\\right]$. No additional text should be printed.",
            "solution": "The user demands a simulation of mixed-precision training for a minimal VGG-like convolutional neural network. The problem is a computational exercise in implementing a neural network training loop from first principles, with a specific focus on the numerical challenges of half-precision (binary16) floating-point arithmetic and the remedial technique of loss scaling.\n\n### Problem Validation\n\nFirst, I shall validate the problem statement.\n\n**Step 1: Extract Givens**\n\n- **Network Architecture**: A sequence of layers: $1 \\times 8 \\times 8$ input $\\rightarrow$ Conv1 ($1 \\rightarrow 2$ channels, $3 \\times 3$ kernel) $\\rightarrow$ ReLU $\\rightarrow$ Conv2 ($2 \\rightarrow 2$ channels, $3 \\times 3$ kernel) $\\rightarrow$ ReLU $\\rightarrow$ Global Average Pooling $\\rightarrow$ Linear ($2 \\rightarrow 2$). Convolutional layers use stride $1$ and padding $1$ to preserve spatial dimensions. No bias terms are used.\n- **Data**: $8$ samples of size $1 \\times 8 \\times 8$. Class $0$ (4 samples): a centered $2 \\times 2$ block of ones. Class $1$ (4 samples): ones on the border. All values are scaled by $\\sigma = 10^{-4}$. Labels are one-hot vectors.\n- **Initialization**: All weights are initialized from a Gaussian distribution $\\mathcal{N}(0, \\tau^2)$ with $\\tau = 10^{-3}$, using a fixed random seed.\n- **Training**: Stochastic Gradient Descent (SGD) with batch size $1$. The sample for step $i$ is at index $i \\pmod 8$. Training runs for $E = 20$ steps. The learning rate is constant at $\\eta = 10^8$.\n- **Loss Function**: Softmax cross-entropy.\n- **Precision Modes**:\n    1.  **Full Precision (FP32)**: All parameters and computations use `binary32`.\n    2.  **Mixed Precision (MP)**: A master copy of weights is stored in `binary32`. For each training step, weights are cast to `binary16` for forward and backward passes. Loss is calculated, its gradient w.r.t logits is scaled by a factor $\\alpha$ and cast to `binary16`. Gradients of weights are computed in `binary16`, then cast to `binary32`, unscaled by dividing by $\\alpha$, and used to update the master weights.\n- **Failure Condition**: If any value becomes NaN or infinity, the training run is considered failed, and its final accuracy is $0$.\n- **IEEE 754 `binary16`**: Smallest positive normal value $\\approx 6.1 \\times 10^{-5}$ ($2^{-14}$), smallest positive subnormal value $\\approx 6.0 \\times 10^{-8}$ ($2^{-24}$). Values below this underflow to zero.\n- **Test Cases**: Three loss scaling factors for MP training: $\\alpha_1 = 1$, $\\alpha_2 = 2^{13}$, and $\\alpha_3 = 2^{20}$.\n- **Metrics**: Theoretical speedup $S=2.0$ (given). Accuracy change $\\Delta A(\\alpha) = A_{\\text{mixed}}(\\alpha) - A_{\\text{fp32}}$, where accuracies are evaluated on the full training set using `binary32` arithmetic after $20$ steps.\n- **Output Format**: A list of lists: $[[S_1, \\Delta A(\\alpha_1)], [S_2, \\Delta A(\\alpha_2)], [S_3, \\Delta A(\\alpha_3)]]$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded**: The problem is well-grounded in the principles of deep learning and computer arithmetic. Mixed-precision training, loss scaling, and the behavior of IEEE 754 floating-point numbers are established concepts.\n- **Well-Posed**: The problem is deterministic. The fixed seed, data, and training procedure ensure a single, computable solution.\n- **Objective**: All instructions are quantitative and algorithmic, with no subjective elements.\n- **Completeness**: The problem is fully specified, providing all necessary parameters and logic for a complete implementation. The seemingly extreme learning rate $\\eta = 10^8$ is a deliberate choice. Combined with the small input scale ($\\sigma = 10^{-4}$) and weight initialization ($\\tau = 10^{-3}$), it forces the simulation into a regime where gradients are very small, making the effects of underflow prominent and testing the mitigation strategy. This is a valid design for a numerical experiment.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. I will proceed with the solution.\n\n### Principled Solution\n\nThe task is to simulate and contrast full-precision and mixed-precision training. The core of the problem revolves around the limited range and precision of half-precision floating-point numbers (`binary16`) and the technique of loss scaling used to counteract these limitations.\n\n**1. The Rationale for Mixed-Precision Training**\n\nTraining deep neural networks is computationally expensive. Using lower-precision arithmetic, such as `binary16` (FP16), can significantly accelerate computations on modern hardware (like GPUs with Tensor Cores) and reduce memory footprint. The problem models this with an idealized speedup factor $S = 2.0$. However, the reduced dynamic range of FP16 introduces numerical challenges.\n\n**2. The Problem of Gradient Underflow**\n\nThe dynamic range of FP16 is much smaller than that of single-precision `binary32` (FP32). Gradients, especially in deep networks or with small-magnitude inputs/weights, can become very small. The smallest positive subnormal number representable in FP16 is approximately $2^{-24} \\approx 5.96 \\times 10^{-8}$. Any computed gradient with a magnitude smaller than this will be flushed to zero (underflow). If a significant fraction of gradients become zero, the corresponding weights are not updated, and the training process stagnates or fails.\n\nIn this problem, the input scale is $\\sigma=10^{-4}$ and initial weights are of order $\\tau=10^{-3}$. Activations and subsequent gradients are therefore likely to be very small, creating a perfect scenario for underflow in an FP16-based backward pass.\n\n**3. Loss Scaling as a Mitigation Strategy**\n\nLoss scaling is a technique to prevent gradient underflow. The procedure is as follows:\n1.  After the forward pass, compute the loss $\\mathcal{L}$.\n2.  Before starting backpropagation, scale the loss by a large factor $\\alpha$: $\\mathcal{L}_{\\text{scaled}} = \\alpha \\cdot \\mathcal{L}$.\n3.  By the chain rule, this scaling propagates to all gradients: $\\nabla_{\\theta} \\mathcal{L}_{\\text{scaled}} = \\alpha \\cdot \\nabla_{\\theta} \\mathcal{L}$.\n4.  These scaled gradients, now having larger magnitudes, are less likely to underflow when computed in FP16.\n5.  After the backward pass, the computed gradients (which are approximately $\\alpha \\cdot \\nabla_{\\theta} \\mathcal{L}$) are unscaled by dividing by $\\alpha$ before the weight update is applied in FP32. The update rule becomes: $\\theta_{t+1} = \\theta_t - \\eta \\cdot (\\frac{1}{\\alpha} (\\alpha \\cdot \\nabla_{\\theta_t} \\mathcal{L})) = \\theta_t - \\eta \\cdot \\nabla_{\\theta_t} \\mathcal{L}$.\n\nThe choice of $\\alpha$ is critical:\n- If $\\alpha$ is too small, it may be insufficient to prevent underflow.\n- If $\\alpha$ is too large, the scaled gradients may overflow (exceed the maximum representable FP16 value, $65504$), resulting in `inf` values that destroy the training process.\n\n**4. Experimental Design and Predictions**\n\nThe problem sets up an experiment with three values of $\\alpha$ to demonstrate this trade-off.\n\n- **Baseline (FP32)**: We first establish a baseline accuracy $A_{\\text{fp32}}$ by training in full precision. This represents the target performance.\n- **Mixed Precision with $\\alpha_1 = 1$**: This is equivalent to no loss scaling. We predict that many gradients will underflow to zero in the FP16 backward pass. The model will learn poorly, if at all. Thus, we expect $A_{\\text{mixed}}(1) < A_{\\text{fp32}}$, leading to $\\Delta A(1) < 0$.\n- **Mixed Precision with $\\alpha_2 = 2^{13} = 8192$**: This is a moderate scaling factor. It should be large enough to shift the gradient magnitudes out of the underflow region and into the representable range of FP16, without causing overflow. We predict this will prevent underflow, allowing the model to train effectively. Thus, we expect $A_{\\text{mixed}}(8192) \\approx A_{\\text{fp32}}$, leading to $\\Delta A(8192) \\approx 0$.\n- **Mixed Precision with $\\alpha_3 = 2^{20} = 1048576$**: This is a very large scaling factor. The initial loss gradient, which is of order $O(1)$, when multiplied by $10^6$, will far exceed the FP16 maximum value. This will cause overflow to `inf` at the beginning of the backward pass. The subsequent gradient calculations and weight updates will propagate this `inf`, leading to a failed training run. The resulting accuracy will be $0$. We expect $\\Delta A(2^{20}) = 0 - A_{\\text{fp32}} = -A_{\\text{fp32}}$.\n\nThe implementation will meticulously follow the specified algorithms for each layer's forward and backward pass, data generation, and the training loop, ensuring all castings and scaling operations are performed at the correct stages to faithfully simulate the process.",
            "answer": "```python\nimport numpy as np\n\n# This program simulates mixed-precision training for a small CNN from first principles.\n# It is designed to be fully deterministic and self-contained.\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the simulation, training, and evaluation.\n    \"\"\"\n    # --- Problem Parameters ---\n    ETA = 1e8\n    SIGMA = 1e-4\n    TAU = 1e-3\n    E = 20  # Number of training steps\n    NUM_SAMPLES = 8\n    SPEEDUP_FACTOR = 2.0\n    SEED = 42\n    ALPHAS = [1.0, 8192.0, 1048576.0]\n\n    # --- Layer Implementations ---\n\n    def relu(x):\n        return np.maximum(0, x)\n\n    def relu_grad(x):\n        return (x > 0).astype(x.dtype)\n\n    def convolve_forward(x_in, W, stride=1, padding=1):\n        \"\"\"A simplified 2D convolution forward pass.\"\"\"\n        C_out, C_in, K, _ = W.shape\n        _, H_in, W_in = x_in.shape\n        \n        H_out = (H_in - K + 2 * padding) // stride + 1\n        W_out = (W_in - K + 2 * padding) // stride + 1\n\n        x_pad = np.pad(x_in, ((0, 0), (padding, padding), (padding, padding)), mode='constant')\n        out = np.zeros((C_out, H_out, W_out), dtype=x_in.dtype)\n\n        for c_o in range(C_out):\n            for h in range(H_out):\n                for w in range(W_out):\n                    h_start, w_start = h * stride, w * stride\n                    region = x_pad[:, h_start:h_start + K, w_start:w_start + K]\n                    out[c_o, h, w] = np.sum(region * W[c_o, :, :, :])\n        return out\n\n    def convolve_backward(d_out, x_in, W, stride=1, padding=1):\n        \"\"\"A simplified 2D convolution backward pass.\"\"\"\n        C_out, C_in, K, _ = W.shape\n        _, H_in, W_in = x_in.shape\n        \n        x_pad = np.pad(x_in, ((0, 0), (padding, padding), (padding, padding)), mode='constant')\n        d_x = np.zeros_like(x_pad, dtype=x_in.dtype)\n        d_W = np.zeros_like(W, dtype=x_in.dtype)\n        \n        H_out = (H_in - K + 2 * padding) // stride + 1\n        W_out = (W_in - K + 2 * padding) // stride + 1\n\n        for c_o in range(C_out):\n            for h in range(H_out):\n                for w in range(W_out):\n                    h_start, w_start = h * stride, w * stride\n                    region = x_pad[:, h_start:h_start + K, w_start:w_start + K]\n                    \n                    d_W[c_o, :, :, :] += d_out[c_o, h, w] * region\n                    d_x[:, h_start:h_start + K, w_start:w_start + K] += d_out[c_o, h, w] * W[c_o, :, :, :]\n        \n        if padding > 0:\n            d_x = d_x[:, padding:-padding, padding:-padding]\n        \n        return d_x, d_W\n\n    def global_avg_pool_forward(x):\n        return np.mean(x, axis=(1, 2))\n\n    def global_avg_pool_backward(d_out, x_shape):\n        C, H, W = x_shape\n        return d_out[:, np.newaxis, np.newaxis] / (H * W) * np.ones(x_shape, dtype=d_out.dtype)\n    \n    def stable_softmax_cross_entropy(logits, y_true):\n        # Logits and y_true are 1D arrays for a single sample.\n        stable_logits = logits - np.max(logits)\n        exp_logits = np.exp(stable_logits)\n        probs = exp_logits / np.sum(exp_logits)\n        loss = -np.sum(y_true * np.log(probs))\n        d_logits = probs - y_true\n        return loss, d_logits\n\n    # --- Data Generation ---\n    def generate_data():\n        X = np.zeros((NUM_SAMPLES, 1, 8, 8), dtype=np.float32)\n        Y = np.zeros((NUM_SAMPLES, 2), dtype=np.float32)\n        \n        # Class 0: centered 2x2 block\n        img0 = np.zeros((8, 8), dtype=np.float32)\n        img0[3:5, 3:5] = SIGMA\n        \n        # Class 1: border\n        img1 = np.zeros((8, 8), dtype=np.float32)\n        img1[[0, 7], :] = SIGMA\n        img1[:, [0, 7]] = SIGMA\n\n        for i in range(NUM_SAMPLES):\n            if i < 4:\n                X[i, 0, :, :] = img0\n                Y[i, :] = [1, 0]\n            else:\n                X[i, 0, :, :] = img1\n                Y[i, :] = [0, 1]\n        return X, Y\n\n    X_train, Y_train = generate_data()\n\n    # --- Model Initialization ---\n    def init_weights(seed):\n        rng = np.random.default_rng(seed)\n        weights = {\n            'W1': rng.normal(0, TAU, (2, 1, 3, 3)).astype(np.float32),\n            'W2': rng.normal(0, TAU, (2, 2, 3, 3)).astype(np.float32),\n            'W_lin': rng.normal(0, TAU, (2, 2)).astype(np.float32),\n        }\n        return weights\n\n    # --- Training and Evaluation ---\n    def train(X_data, Y_data, mode, alpha, seed):\n        weights = init_weights(seed)\n        fp_master_weights = {k: v.copy() for k, v in weights.items()}\n\n        for step in range(E):\n            idx = step % NUM_SAMPLES\n            x, y_true = X_data[idx], Y_data[idx]\n\n            # Set precision for this run\n            compute_dtype = np.float16 if mode == 'mixed' else np.float32\n            \n            # --- FORWARD PASS ---\n            W1 = fp_master_weights['W1'].astype(compute_dtype)\n            W2 = fp_master_weights['W2'].astype(compute_dtype)\n            W_lin = fp_master_weights['W_lin'].astype(compute_dtype)\n            x_compute = x.astype(compute_dtype)\n\n            z1 = convolve_forward(x_compute, W1)\n            a1 = relu(z1)\n            z2 = convolve_forward(a1, W2)\n            a2 = relu(z2)\n            p = global_avg_pool_forward(a2)\n            logits = p @ W_lin\n\n            # --- LOSS & BACKWARD PASS ---\n            # Loss computed in FP32 for stability\n            _, d_logits_fp32 = stable_softmax_cross_entropy(logits.astype(np.float32), y_true)\n\n            if mode == 'mixed':\n                d_logits = (d_logits_fp32 * alpha).astype(compute_dtype)\n            else:\n                d_logits = d_logits_fp32.astype(compute_dtype)\n            \n            d_p = d_logits @ W_lin.T\n            d_W_lin = np.outer(p, d_logits)\n            \n            d_a2 = global_avg_pool_backward(d_p, a2.shape)\n            d_z2 = d_a2 * relu_grad(z2)\n            d_a1, d_W2 = convolve_backward(d_z2, a1, W2)\n            \n            d_z1 = d_a1 * relu_grad(z1)\n            _, d_W1 = convolve_backward(d_z1, x_compute, W1)\n            \n            # --- WEIGHT UPDATE ---\n            grads = {\n                'W1': d_W1.astype(np.float32),\n                'W2': d_W2.astype(np.float32),\n                'W_lin': d_W_lin.astype(np.float32),\n            }\n\n            if mode == 'mixed':\n                for k in grads:\n                    grads[k] /= alpha\n\n            for k in fp_master_weights:\n                fp_master_weights[k] -= ETA * grads[k]\n            \n            # Check for failure\n            is_invalid = any(np.any(np.isinf(w)) or np.any(np.isnan(w)) for w in fp_master_weights.values())\n            if is_invalid:\n                return None  # Failed run\n\n        return fp_master_weights\n\n    def evaluate(X_data, Y_data, weights):\n        correct_predictions = 0\n        for i in range(NUM_SAMPLES):\n            x = X_data[i].astype(np.float32)\n            \n            z1 = convolve_forward(x, weights['W1'])\n            a1 = relu(z1)\n            z2 = convolve_forward(a1, weights['W2'])\n            a2 = relu(z2)\n            p = global_avg_pool_forward(a2)\n            logits = p @ weights['W_lin']\n            \n            predicted_class = np.argmax(logits)\n            true_class = np.argmax(Y_data[i])\n            \n            if predicted_class == true_class:\n                correct_predictions += 1\n        \n        return correct_predictions / NUM_SAMPLES\n\n    # --- Main Execution Logic ---\n    \n    # FP32 baseline run\n    final_weights_fp32 = train(X_train, Y_train, mode='fp32', alpha=1.0, seed=SEED)\n    acc_fp32 = evaluate(X_train, Y_train, final_weights_fp32)\n\n    results = []\n    for alpha in ALPHAS:\n        final_weights_mixed = train(X_train, Y_train, mode='mixed', alpha=alpha, seed=SEED)\n        \n        if final_weights_mixed is None:\n            acc_mixed = 0.0\n        else:\n            acc_mixed = evaluate(X_train, Y_train, final_weights_mixed)\n            \n        delta_A = acc_mixed - acc_fp32\n        results.append([SPEEDUP_FACTOR, delta_A])\n    \n    # Format and print the final output\n    formatted_results = \",\".join([f\"[{s},{da}]\" for s, da in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n\n```"
        }
    ]
}