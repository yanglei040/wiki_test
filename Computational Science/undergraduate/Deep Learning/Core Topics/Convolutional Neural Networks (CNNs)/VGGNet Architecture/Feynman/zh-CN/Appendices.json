{
    "hands_on_practices": [
        {
            "introduction": "在设计或选择神经网络时，一个关键的考量是在模型性能与计算资源消耗之间取得平衡。本练习将引导你从第一性原理出发，推导并应用公式来估算卷积层的浮点运算数（FLOPs）和内存带宽。掌握这项技能对于在手机等资源受限设备上部署模型至关重要，它能帮助你量化不同 VGGNet 变体的计算成本，并做出明智的架构选择。",
            "id": "3198589",
            "problem": "一个团队正在对 Visual Geometry Group (VGG) 网络架构的简化变体进行规模评估，以便将其部署在计算和内存带宽资源有限的设备上。每个变体都由若干二维卷积层堆叠而成，其卷积核大小为 $K$，步幅为 $1$，并使用能保持空间维度的零填充。忽略偏置项、批量归一化、非线性激活函数以及池化操作的计算或流量；平均池化仅改变后续层的 $(H, W)$。每个张量元素都以单精度浮点数格式存储，因此每个元素占用 $4$ 字节。\n\n从离散二维卷积的定义出发，通过从第一性原理计算乘法和加法次数，推导出每个卷积层在前向传播过程中的浮点运算次数。使用标准的基准测试约定，即一次乘加运算计为两次浮点运算。同时，推导一个每层的主内存流量模型，假设对于每个卷积层，输入激活张量和滤波器张量都从主内存中读取一次，而输出激活张量则写入主内存一次。\n\n给出了三个候选变体。对于所有卷积层，卷积核大小为 $K=3$，填充保持 $(H, W)$ 不变。输入图像的空间尺寸为 $64 \\times 64$，有 $3$ 个通道。这些变体是：\n\n- 变体 A：\n  - 第 $1$ 层：$(H, W, C_{\\text{in}}, C_{\\text{out}}) = (64, 64, 3, 16)$。\n  - 第 $2$ 层：$(64, 64, 16, 16)$。\n  - 平均池化将空间维度减半：$(H, W) \\mapsto (32, 32)$。\n  - 第 $3$ 层：$(32, 32, 16, 32)$。\n  - 第 $4$ 层：$(32, 32, 32, 32)$。\n  - 平均池化将空间维度减半：$(H, W) \\mapsto (16, 16)$。\n  - 第 $5$ 层：$(16, 16, 32, 64)$。\n  - 第 $6$ 层：$(16, 16, 64, 64)$。\n\n- 变体 B：\n  - 第 $1$ 层：$(64, 64, 3, 16)$。\n  - 第 $2$ 层：$(64, 64, 16, 16)$。\n  - 平均池化：$(H, W) \\mapsto (32, 32)$。\n  - 第 $3$ 层：$(32, 32, 16, 32)$。\n  - 第 $4$ 层：$(32, 32, 32, 32)$。\n  - 第 $5$ 层：$(32, 32, 32, 32)$。\n  - 平均池化：$(H, W) \\mapsto (16, 16)$。\n  - 第 $6$ 层：$(16, 16, 32, 64)$。\n  - 第 $7$ 层：$(16, 16, 64, 64)$。\n  - 第 $8$ 层：$(16, 16, 64, 64)$。\n\n- 变体 C：\n  - 第 $1$ 层：$(64, 64, 3, 32)$。\n  - 第 $2$ 层：$(64, 64, 32, 32)$。\n  - 平均池化：$(H, W) \\mapsto (32, 32)$。\n  - 第 $3$ 层：$(32, 32, 32, 64)$。\n  - 第 $4$ 层：$(32, 32, 64, 64)$。\n  - 平均池化：$(H, W) \\mapsto (16, 16)$。\n  - 第 $5$ 层：$(16, 16, 64, 128)$。\n  - 第 $6$ 层：$(16, 16, 128, 128)$。\n\n设计算预算为每次前向传播 $2.0 \\times 10^{8}$ 次浮点运算，主内存带宽预算为每次前向传播 $3.0 \\times 10^{6}$ 字节。在同时满足这两个预算的变体中，选择总浮点运算次数最多的那一个。作为最终答案，请报告所选变体的确切总浮点运算次数，形式为一个整数。最终答案中不要包含单位。",
            "solution": "首先，我们按照问题陈述中的规定，为单个二维卷积层推导前向传播浮点运算（FLOP）次数和主内存流量的通用表达式。\n\n设输入激活张量的维度为 $(H, W, C_{\\text{in}})$，分别代表高度、宽度和输入通道数。\n设输出激活张量的维度为 $(H_{\\text{out}}, W_{\\text{out}}, C_{\\text{out}})$。\n设卷积核大小为 $K \\times K$。\n问题陈述指出使用填充来保持空间维度，因此 $H_{\\text{out}} = H$ 且 $W_{\\text{out}} = W$。步幅给定为 $1$。\n\n浮点运算（FLOP）的推导：\n为了计算一个输出特征图（输出张量的一个通道）中的单个值，一个 $K \\times K \\times C_{\\text{in}}$ 的滤波器被应用于输入张量的一个对应区块上。这涉及到 $K \\times K \\times C_{\\text{in}}$ 次乘法运算和 $(K \\times K \\times C_{\\text{in}} - 1)$ 次加法运算。由于忽略了偏置项，没有额外的加法。\n产生一个输出值的总运算次数为 $(K^2 C_{\\text{in}}) + (K^2 C_{\\text{in}} - 1)$。\n使用一次乘加运算计为两次 FLOP 的标准基准测试约定，我们可以将一个输出值的成本近似为 $2 \\times K^2 \\times C_{\\text{in}}$。\n这个计算必须对输出张量中的每个空间位置执行，总共有 $H \\times W$ 个位置。\n此外，这必须对 $C_{\\text{out}}$ 个输出通道中的每一个都执行。\n因此，单个卷积层的总 FLOP 次数 $F$ 为：\n$$F = (2 \\cdot K^2 \\cdot C_{\\text{in}}) \\cdot (H \\cdot W \\cdot C_{\\text{out}})$$\n$$F = 2 \\cdot H \\cdot W \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2$$\n\n主内存流量的推导：\n内存流量 $M$ 是从主内存读取和写入主内存的字节数之和。每个张量元素是一个单精度浮点数，占用 $4$ 字节。\n$1$. 输入张量读取：大小为 $H \\times W \\times C_{\\text{in}}$ 的整个输入张量被读取一次。流量为 $4 \\cdot H \\cdot W \\cdot C_{\\text{in}}$ 字节。\n$2$. 滤波器张量读取：卷积滤波器（权重）被读取一次。有 $C_{\\text{out}}$ 个滤波器，每个大小为 $K \\times K \\times C_{\\text{in}}$。总大小为 $K^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}$。流量为 $4 \\cdot K^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}$ 字节。\n$3$. 输出张量写入：生成的、大小为 $H \\times W \\times C_{\\text{out}}$ 的输出张量被写入一次。流量为 $4 \\cdot H \\cdot W \\cdot C_{\\text{out}}$ 字节。\n每层的总内存流量是这三个部分的总和：\n$$M = 4 \\cdot H \\cdot W \\cdot C_{\\text{in}} + 4 \\cdot K^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} + 4 \\cdot H \\cdot W \\cdot C_{\\text{out}}$$\n$$M = 4 \\cdot (H \\cdot W \\cdot (C_{\\text{in}} + C_{\\text{out}}) + K^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}})$$\n\n现在，我们使用 $K=3$（因此 $K^2=9$）来评估每个变体。\n预算为：FLOPs $\\le 2.0 \\times 10^8$ 且内存 $\\le 3.0 \\times 10^6$ 字节。\n\n变体 A：\n- 第 $1$ 层：$(H, W, C_{\\text{in}}, C_{\\text{out}}) = (64, 64, 3, 16)$\n  $F_1 = 2 \\cdot 64 \\cdot 64 \\cdot 3 \\cdot 16 \\cdot 9 = 3,538,944$\n  $M_1 = 4 \\cdot (64 \\cdot 64 \\cdot (3+16) + 9 \\cdot 3 \\cdot 16) = 4 \\cdot (4096 \\cdot 19 + 432) = 313,024$\n- 第 $2$ 层：$(64, 64, 16, 16)$\n  $F_2 = 2 \\cdot 64 \\cdot 64 \\cdot 16 \\cdot 16 \\cdot 9 = 18,874,368$\n  $M_2 = 4 \\cdot (4096 \\cdot (16+16) + 9 \\cdot 16 \\cdot 16) = 4 \\cdot (4096 \\cdot 32 + 2304) = 533,504$\n- 第 $3$ 层：$(32, 32, 16, 32)$\n  $F_3 = 2 \\cdot 32 \\cdot 32 \\cdot 16 \\cdot 32 \\cdot 9 = 9,437,184$\n  $M_3 = 4 \\cdot (1024 \\cdot (16+32) + 9 \\cdot 16 \\cdot 32) = 4 \\cdot (1024 \\cdot 48 + 4608) = 215,040$\n- 第 $4$ 层：$(32, 32, 32, 32)$\n  $F_4 = 2 \\cdot 32 \\cdot 32 \\cdot 32 \\cdot 32 \\cdot 9 = 18,874,368$\n  $M_4 = 4 \\cdot (1024 \\cdot (32+32) + 9 \\cdot 32 \\cdot 32) = 4 \\cdot (1024 \\cdot 64 + 9216) = 299,008$\n- 第 $5$ 层：$(16, 16, 32, 64)$\n  $F_5 = 2 \\cdot 16 \\cdot 16 \\cdot 32 \\cdot 64 \\cdot 9 = 9,437,184$\n  $M_5 = 4 \\cdot (256 \\cdot (32+64) + 9 \\cdot 32 \\cdot 64) = 4 \\cdot (256 \\cdot 96 + 18432) = 172,032$\n- 第 $6$ 层：$(16, 16, 64, 64)$\n  $F_6 = 2 \\cdot 16 \\cdot 16 \\cdot 64 \\cdot 64 \\cdot 9 = 18,874,368$\n  $M_6 = 4 \\cdot (256 \\cdot (64+64) + 9 \\cdot 64 \\cdot 64) = 4 \\cdot (256 \\cdot 128 + 36864) = 278,224$\n\n- 变体 A 的总计：\n  $F_A = 3538944 + 18874368 + 9437184 + 18874368 + 9437184 + 18874368 = 79,036,416$\n  $M_A = 313024 + 533504 + 215040 + 299008 + 172032 + 278224 = 1,810,832$\n- 预算检查：$F_A = 7.90 \\times 10^7 \\le 2.0 \\times 10^8$ (通过)。$M_A = 1.81 \\times 10^6 \\le 3.0 \\times 10^6$ (通过)。\n变体 A 是一个有效的候选方案。\n\n变体 B：\n- 第 1-4 层是 B 的层的一个子集。第 1-2 层，池化，第 3-5 层，池化，第 6-8 层。\n- 第 $1, 2, 3, 4, 5$ 层：$(64,64,3,16), (64,64,16,16), (32,32,16,32), (32,32,32,32), (32,32,32,32)$\n  $F_1=3,538,944, M_1=313,024$\n  $F_2=18,874,368, M_2=533,504$\n  $F_3=9,437,184, M_3=215,040$\n  $F_4=18,874,368, M_4=299,008$\n  $F_5=18,874,368, M_5=299,008$\n- 第 $6, 7, 8$ 层：$(16,16,32,64), (16,16,64,64), (16,16,64,64)$\n  $F_6 = 2 \\cdot 16 \\cdot 16 \\cdot 32 \\cdot 64 \\cdot 9 = 9,437,184$, $M_6=172,032$\n  $F_7 = 2 \\cdot 16 \\cdot 16 \\cdot 64 \\cdot 64 \\cdot 9 = 18,874,368$, $M_7=278,224$\n  $F_8 = 18,874,368$, $M_8=278,224$\n- 变体 B 的总计：\n  $F_B = 3538944 + 18874368 + 9437184 + 18874368 + 18874368 + 9437184 + 18874368 + 18874368 = 116,785,152$\n  $M_B = 313024 + 533504 + 215040 + 299008 + 299008 + 172032 + 278224 + 278224 = 2,388,064$\n- 预算检查：$F_B = 1.17 \\times 10^8 \\le 2.0 \\times 10^8$ (通过)。$M_B = 2.39 \\times 10^6 \\le 3.0 \\times 10^6$ (通过)。\n变体 B 是一个有效的候选方案。\n\n变体 C：\n- 第 $1$ 层：$(64, 64, 3, 32)$\n  $F_1 = 2 \\cdot 64 \\cdot 64 \\cdot 3 \\cdot 32 \\cdot 9 = 7,077,888$\n  $M_1 = 4 \\cdot (4096 \\cdot (3+32) + 9 \\cdot 3 \\cdot 32) = 576,896$\n- 第 $2$ 层：$(64, 64, 32, 32)$\n  $F_2 = 2 \\cdot 64 \\cdot 64 \\cdot 32 \\cdot 32 \\cdot 9 = 37,748,736$\n  $M_2 = 4 \\cdot (4096 \\cdot (32+32) + 9 \\cdot 32 \\cdot 32) = 1,085,440$\n- 第 $3$ 层：$(32, 32, 32, 64)$\n  $F_3 = 2 \\cdot 32 \\cdot 32 \\cdot 32 \\cdot 64 \\cdot 9 = 37,748,736$\n  $M_3 = 4 \\cdot (1024 \\cdot (32+64) + 9 \\cdot 32 \\cdot 64) = 466,944$\n- 第 $4$ 层：$(32, 32, 64, 64)$\n  $F_4 = 2 \\cdot 32 \\cdot 32 \\cdot 64 \\cdot 64 \\cdot 9 = 75,497,472$\n  $M_4 = 4 \\cdot (1024 \\cdot (64+64) + 9 \\cdot 64 \\cdot 64) = 671,744$\n- 第 $5$ 层：$(16, 16, 64, 128)$\n  $F_5 = 2 \\cdot 16 \\cdot 16 \\cdot 64 \\cdot 128 \\cdot 9 = 37,748,736$\n  $M_5 = 4 \\cdot (256 \\cdot (64+128) + 9 \\cdot 64 \\cdot 128) = 491,520$\n- 第 $6$ 层：$(16, 16, 128, 128)$\n  $F_6 = 2 \\cdot 16 \\cdot 16 \\cdot 128 \\cdot 128 \\cdot 9 = 75,497,472$\n  $M_6 = 4 \\cdot (256 \\cdot (128+128) + 9 \\cdot 128 \\cdot 128) = 851,968$\n\n- 变体 C 的总计：\n  $F_C = 7077888 + 37748736 + 37748736 + 75497472 + 37748736 + 75497472 = 271,319,040$\n  $M_C = 576896 + 1085440 + 466944 + 671744 + 491520 + 851968 = 4,144,512$\n- 预算检查：$F_C = 2.71 \\times 10^8  2.0 \\times 10^8$ (不通过)。$M_C = 4.14 \\times 10^6  3.0 \\times 10^6$ (不通过)。\n变体 C 不是一个有效的候选方案。\n\n比较和选择：\n有效的候选方案是变体 A 和变体 B。我们必须选择总 FLOP 次数最多的那一个。\n$F_A = 79,036,416$\n$F_B = 116,785,152$\n由于 $F_B  F_A$，变体 B 是被选中的模型。问题要求给出这个被选变体的确切总浮点运算次数。\n所需的值是 $F_B = 116,785,152$。",
            "answer": "$$\\boxed{116785152}$$"
        },
        {
            "introduction": "理论知识需要与实际应用相结合。在使用预训练模型时，一个常见但容易被忽视的细节是输入数据的格式必须与模型训练时完全一致。本练习聚焦于一个具体的实践问题：当输入图像的通道顺序（例如 RGB 与 BGR）与预训练 VGGNet 的要求不匹配时，我们应该如何处理？通过这个思想实验，你将理解输入数据排列如何影响第一层卷积的计算，并学会如何通过简单的权重调整来修正这个问题，确保模型的正确行为。",
            "id": "3198592",
            "problem": "一个预训练的视觉几何组网络（VGGNet）期望输入图像的通道遵循特定顺序（例如，蓝-绿-红（BGR））。你打算输入红-绿-蓝（RGB）格式的图像。设输入张量表示为 $x \\in \\mathbb{R}^{3 \\times H \\times W}$，第一个卷积层的权重为 $W \\in \\mathbb{R}^{C_{\\text{out}} \\times 3 \\times k \\times k}$，第一层的偏置为 $b \\in \\mathbb{R}^{C_{\\text{out}}}$。将通道顺序的改变建模为在第一次卷积之前，对3个输入通道应用一个固定的 $3 \\times 3$ 排列矩阵 $P$，因此网络看到的是 $x' = P x$ 而不是 $x$。假设网络的其余部分保持不变，并且在给定其输入的情况下其行为是确定性的。\n\n当给定RGB输入时，要如何获得与期望 $P$ 顺序通道（例如，BGR）的原始预训练模型相同的输出，请选择所有正确的陈述：\n\nA. 仅修改第一个卷积层的权重，根据 $P$ 排列其输入通道维度（即，根据 $P$ 引起的排列对 $W$ 的输入通道轴重新索引），而保持 $b$ 和所有后续层不变，这是充分的。\n\nB. 仅根据 $P$ 排列第一个卷积层的输出通道维度，而保持所有后续层不变，这是充分的。\n\nC. 不需要改变权重；由于存在修正线性单元（ReLU）非线性，网络对输入通道的排列具有不变性。\n\nD. 如果在第一次卷积前通过应用 $P$ 对RGB输入进行预排列，使得第一层接收到的通道顺序与其训练时所用的顺序相同，那么保持预训练的权重和偏置不变，将产生与原始预训练模型相同的输出。\n\nE. 仅调整第一层的偏置 $b$（而不调整权重 $W$）就足以补偿通道顺序的变化。",
            "solution": "我们从多通道卷积和线性的基本定义开始。对于第一个卷积层，输出通道 $o \\in \\{1, \\dots, C_{\\text{out}}\\}$ 的输出特征图为\n$$\ny_o = \\sum_{c=1}^{3} \\left( W_{o,c} * x_c \\right) + b_o,\n$$\n其中 $*$ 表示二维空间卷积，$x_c$ 是第 $c$ 个输入通道。该操作在输入通道和卷积核权重上都是线性的。\n\n输入通道的排列可以由一个作用在通道向量上的排列矩阵 $P \\in \\mathbb{R}^{3 \\times 3}$ 表示，因此排列后的输入 $x'$ 满足\n$$\nx'_c = \\sum_{d=1}^{3} P_{c,d}\\, x_d.\n$$\n将第一层应用于 $x'$ 会得到\n$$\ny'_o = \\sum_{c=1}^{3} \\left( W_{o,c} * x'_c \\right) + b_o \n= \\sum_{c=1}^{3} \\left( W_{o,c} * \\sum_{d=1}^{3} P_{c,d} x_d \\right) + b_o \n= \\sum_{d=1}^{3} \\left( \\sum_{c=1}^{3} P_{c,d} W_{o,c} \\right) * x_d + b_o.\n$$\n定义 $W'_{o,d} := \\sum_{c=1}^{3} P_{c,d} W_{o,c}$。那么\n$$\ny'_o = \\sum_{d=1}^{3} \\left( W'_{o,d} * x_d \\right) + b_o.\n$$\n这表明，在第一层，通过 $P$ 预先排列输入通道，在代数上等价于用 $P$ 导出的一个输入通道排列 $W'$ 来替换权重，同时保持偏置 $b$ 不变。经过此调整后，网络的其余部分接收到与预训练配置中相同的 $y'$，因此产生相同的输出。\n\n我们现在分析每个选项：\n\nA. “仅修改第一个卷积层的权重，根据 $P$ 排列其输入通道维度，而保持 $b$ 和所有后续层不变，这是充分的。” 从推导中可知，设置 $W'_{o,d} := \\sum_{c=1}^{3} P_{c,d} W_{o,c}$，这是对 $W$ 的输入通道轴进行重新索引（排列），会得到 $y'_o = \\sum_{d=1}^{3} ( W'_{o,d} * x_d ) + b_o$，这与预训练层对排列后输入的响应相匹配。因为网络的其余部分看到的第一层输出与其训练时看到的相同，所以不需要进一步的更改。偏置 $b$ 不受影响，因为排列作用于卷积之前的输入通道。结论：正确。\n\nB. “仅根据 $P$ 排列第一个卷积层的输出通道维度，而保持所有后续层不变，这是充分的。” 排列 $W$ 的输出通道维度会重新排序不同 $o$ 值的 $y_o$。后续层被参数化以期望特征通道的特定顺序；改变这个顺序而不相应地排列所有下游层的权重，将无法保持网络的功能。没有理由认为仅在第一层通过输出通道的排列就能抵消输入通道的排列。结论：不正确。\n\nC. “不需要改变权重；由于存在修正线性单元（ReLU）非线性，网络对输入通道的排列具有不变性。” ReLU是逐元素应用的，不会在通道上进行随机化或平均；网络对通道排列的不变性通常不成立。第一层的滤波器被学习来响应特定的通道组合和统计特性；排列通道会改变这些滤波器的输入，从而改变激活值。结论：不正确。\n\nD. “如果在第一次卷积前通过应用 $P$ 对RGB输入进行预排列，使得第一层接收到的通道顺序与其训练时所用的顺序相同，那么保持预训练的权重和偏置不变，将产生与原始预训练模型相同的输出。” 这直接源于预训练模型的定义：如果它是在 $P$ 顺序的通道空间（例如，BGR）上训练的，那么输入经 $P$ 变换后的RGB图像，就重现了它所期望的精确输入分布。因此，第一层乃至整个网络都会产生相同的输出。结论：正确。\n\nE. “仅调整第一层的偏置 $b$（而不调整权重 $W$）就足以补偿通道顺序的变化。” 偏置 $b$ 是在跨通道的线性组合之后添加的，它不改变通道的组合方式。输入通道的排列改变了线性组合，这种改变无法通过为每个输出通道添加一个常数偏移来撤销。因此，仅改变 $b$ 无法恢复等价性。结论：不正确。\n\n总而言之，正确的策略是：要么在第一次卷积前用 $P$ 排列输入通道（选项D），要么等价地，根据 $P$ 排列第一层权重的输入通道维度，同时保持其他所有部分不变（选项A）。",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "为了在现代硬件上加速深度学习模型的训练，学术界和工业界开发了多种优化技术，其中混合精度训练是核心方法之一。本练习将带你深入了解该技术的底层机制，通过动手模拟一个 VGG 式网络的训练过程，你将亲身体验半精度（FP16）浮点数计算中可能出现的梯度下溢问题。更重要的是，你将实现并验证损失缩放（loss scaling）这一关键技术如何有效解决该数值问题，从而连接起高级API调用与底层硬件计算之间的鸿沟。",
            "id": "3198711",
            "problem": "要求您为一个小型、类 Visual Geometry Group Network (VGG) 的卷积神经网络实现一个基于原理的混合精度训练模拟，并使用显式的损失缩放来缓解半精度算术中的梯度下溢。您的程序必须是一个完整、可运行的实现，它仅使用数组操作从头开始训练一个微型模型，通过在适当位置转换为半精度来模拟混合精度算术，并计算相对于全精度基线的理论加速比和最终精度的变化。所有数学定义和目标都在下面以适合算法推导的形式表述。\n\n使用的基本原理：\n- 卷积层操作：一个步长为 $1$ 且使用零填充以保持空间尺寸的二维卷积，使用大小为 $3 \\times 3$ 的核。乘加（MAC）操作是乘法后跟加法的重复模式。对于一个输入形状为 $\\left(C_{\\text{in}}, H, W\\right)$、输出通道为 $C_{\\text{out}}$、核大小为 $K \\times K$、步长为 $1$ 并带填充的单个卷积层，总前向传播 MAC 计数为 $C_{\\text{out}} \\cdot H \\cdot W \\cdot C_{\\text{in}} \\cdot K \\cdot K$。对于训练，假设计算输入梯度和权重梯度的反向传播成本各自的 MAC 计数与前向传播在同一数量级，因此每个训练步骤中每次卷积的总 MAC 大约是前向传播 MAC 的 $3$ 倍。对于一个输入维度为 $d_{\\text{in}}$、输出维度为 $d_{\\text{out}}$ 的全连接（线性）层，前向传播 MAC 计数为 $d_{\\text{in}} \\cdot d_{\\text{out}}$，同样假设包括反向传播在内的一个训练步骤的总成本因子约为 $3$。\n- 带损失缩放的混合精度：使用电气和电子工程师协会（IEEE）$754$ binary $16$（半精度）进行计算，并以 binary $32$（单精度）存储参数的主副本。设 $\\alpha$ 为一个正缩放因子。将标量损失乘以 $\\alpha$ 会在反向传播期间将每个梯度都乘以 $\\alpha$：如果 $\\nabla_{\\theta} \\mathcal{L}$ 表示全精度下的梯度，那么缩放后在半精度下计算出的梯度约为 $\\operatorname{fp16}\\!\\left(\\alpha \\cdot \\nabla_{\\theta} \\mathcal{L}\\right)$，然后在应用更新之前，在全精度下通过除以 $\\alpha$ 来取消缩放。其目的是减少小梯度在半精度下的下溢。IEEE $754$ binary $16$ 的最小正规数约为 $2^{-14} \\approx 6.1035 \\times 10^{-5}$，最小正非正规数约为 $2^{-24} \\approx 5.9605 \\times 10^{-8}$；低于最小非正规数的值在半精度下会下溢为零。\n- 吞吐量模型：设 $T_{32}$ 表示 binary $32$ 计算的有效吞吐量， $T_{16}$ 表示 binary $16$ 计算的有效吞吐量。假设 $T_{16} = s \\cdot T_{32}$，其中 $s = 2.0$。对于固定数量的浮点运算（FLOPs），混合精度相对于全精度的理想化加速比为 $$S = \\frac{\\text{time}_{32}}{\\text{time}_{16}} = \\frac{\\text{FLOPs} / T_{32}}{\\text{FLOPs} / T_{16}} = \\frac{T_{16}}{T_{32}} = s.$$\n\n要实现的模型和训练设置：\n- 架构（一个最小化的类 VGG 堆栈）：两个卷积层，核大小为 $3 \\times 3$，步长为 $1$，并使用零填充以保持 $8 \\times 8$ 的空间分辨率，每次卷积后跟一个修正线性单元（ReLU）非线性激活，然后是全局平均池化（在空间位置上按通道取均值），最后接一个全连接（线性）分类器。卷积层的通道配置为 $1 \\rightarrow 2 \\rightarrow 2$，然后是一个用于两个类别的线性映射 $\\mathbb{R}^{2} \\rightarrow \\mathbb{R}^{2}$。不需要偏置项。输入维度为 $1 \\times 8 \\times 8$。\n- 损失函数：softmax 交叉熵。通过每个样本减去最大 logit 来使用数值稳定的 softmax。\n- 优化器：随机梯度下降，学习率为 $\\eta$，在取消梯度缩放（如果使用了任何缩放）后应用于 32 位主参数。使用恒定学习率 $\\eta = 10^{8}$。\n- 数据：构建一个确定性的、包含 $8$ 个样本的二分类数据集，输入尺度为 $\\sigma = 10^{-4}$。对于类别 $0$，图像中心有一个 $2 \\times 2$ 的全 1 块，其值乘以 $\\sigma$，其余位置为零。对于类别 $1$，图像沿四条边界（顶行、底行、左列、右列）的值为 1 并乘以 $\\sigma$，其余位置为零。每个类别创建 $4$ 个副本来获得 $8$ 个样本。标签是 $\\mathbb{R}^{2}$ 中的独热向量。批量大小为 $1$；在每一步使用索引等于步数模 $8$ 的样本。\n- 初始化：使用独立的、均值为零、标准差为 $\\tau = 10^{-3}$ 的高斯分布样本初始化所有权重，并使用固定的随机种子以确保确定性。\n- 混合精度模拟：对于全精度，所有计算均以 binary $32$ 进行。对于混合精度，在每一步：\n  - 将当前的主权重转换为 binary $16$ 用于前向和后向计算；\n  - 计算损失及其相对于 logits 的梯度；将损失梯度乘以损失缩放因子 $\\alpha$；在反向传播前将其转换为 binary $16$；\n  - 以 binary $16$ 计算相对于转换后权重的梯度；\n  - 在 binary 32 下通过除以 $\\alpha$ 来取消梯度缩放，并对主权重应用随机梯度下降更新。\n  如果任何中间值或参数值在任何时候变为非数值（not-a-number）或无穷大，则将该训练运行视为失败，并将其结果准确率设为 $0$。\n- 训练长度：精确运行 $E = 20$ 个更新步骤。\n\n需要计算和报告的量：\n- 设 $A_{\\text{fp32}}$ 为全精度运行在 $E$ 步后，在训练集上使用 binary $32$ 前向传播评估得到的最终 top-1 准确率（表示为 $[0,1]$ 范围内的小数）。\n- 对于每个给定 $\\alpha$ 的混合精度运行，设 $A_{\\text{mixed}}(\\alpha)$ 为混合精度训练的模型在 $E$ 步后，在训练集上以 binary $32$ 评估得到的最终 top-1 准确率。\n- 对于每次混合精度运行，报告理论加速比 $S = s = 2.0$ 和准确率变化 $\\Delta A(\\alpha) = A_{\\text{mixed}}(\\alpha) - A_{\\text{fp32}}$。\n\n测试套件：\n- 使用以下三个损失缩放因子：\n  - $\\alpha_{1} = 1$\n  - $\\alpha_{2} = 8192$ (即 $2^{13}$)\n  - $\\alpha_{3} = 1048576$ (即 $2^{20}$)\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果，每个元素对应于一个测试用例，顺序为 $\\left[\\alpha_{1}, \\alpha_{2}, \\alpha_{3}\\right]$。每个元素本身必须是包含两个浮点数的列表，顺序为 $\\left[S, \\Delta A(\\alpha)\\right]$。例如，打印的行必须看起来像一个列表的列表：$\\left[\\left[S_{1}, \\Delta A\\left(\\alpha_{1}\\right)\\right], \\left[S_{2}, \\Delta A\\left(\\alpha_{2}\\right)\\right], \\left[S_{3}, \\Delta A\\left(\\alpha_{3}\\right)\\right]\\right]$。不应打印任何额外文本。",
            "solution": "本问题旨在通过模拟来演示混合精度训练中的数值挑战及其解决方案。我们将对比全精度（FP32）和混合精度（FP16）训练一个小型VGG式网络的表现。问题的核心在于理解半精度浮点数（FP16）有限的动态范围如何导致梯度下溢，以及损失缩放（loss scaling）技术如何有效缓解此问题。\n\n**1. 梯度下溢问题**\n在深度学习中，尤其是在输入值或权重量级较小的情况下，通过反向传播计算出的梯度可能变得非常小。FP16格式的可表示范围远小于FP32。其可表示的最小正非正规数约为 $5.96 \\times 10^{-8}$。任何量级小于此值的梯度都将被“冲刷”为零，这种现象称为梯度下溢。如果大量梯度变为零，模型权重将停止更新，导致训练停滞。本问题中的输入尺度($\\sigma=10^{-4}$)和权重初始化($\\tau=10^{-3}$)被特意设置得很小，以凸显这一问题。\n\n**2. 损失缩放原理**\n损失缩放是一种关键的补救技术。其工作原理如下：\n1.  **缩放损失**: 在反向传播开始前，将计算出的损失值 $\\mathcal{L}$ 乘以一个大的缩放因子 $\\alpha$，得到 $\\mathcal{L}_{\\text{scaled}} = \\alpha \\cdot \\mathcal{L}$。\n2.  **传播梯度**: 根据链式法则，这个缩放会传递到所有梯度上，即 $\\nabla_{\\theta} \\mathcal{L}_{\\text{scaled}} = \\alpha \\cdot \\nabla_{\\theta} \\mathcal{L}$。\n3.  **防止下溢**: 缩放后的梯度量级显著增大，从而在FP16计算中避免了下溢。\n4.  **取消缩放**: 在得到FP16梯度后，将其转换回FP32，并通过除以 $\\alpha$ 来恢复其原始尺度，然后用它来更新FP32主权重。\n\n$\\alpha$ 的选择是一个权衡：若太小，则无法阻止下溢；若太大，则可能导致缩放后的梯度超过FP16的最大表示范围（约65504），产生上溢（overflow），同样会破坏训练。\n\n**3. 实验预测**\n根据上述原理，我们可以对三个测试用例进行预测：\n*   **全精度基线 ($A_{\\text{fp32}}$)**: 作为标准参照，模型应能正常训练并达到高准确率。\n*   **$\\alpha = 1$ (无缩放)**: 在混合精度模式下，由于梯度极小，预计会发生严重的梯度下溢。模型训练将受阻，导致最终准确率远低于基线，即 $\\Delta A  0$。\n*   **$\\alpha = 8192$ ($2^{13}$)**: 这是一个合适的缩放因子，能将梯度提升到FP16的可表示范围内，而不会引起上溢。预计训练将正常进行，准确率接近基线，即 $\\Delta A \\approx 0$。\n*   **$\\alpha = 1048576$ ($2^{20}$)**: 这是一个过大的缩放因子。初始损失梯度（量级通常为 $O(1)$）在乘以 $\\alpha$ 后会远超FP16的上限，导致上溢。训练将因产生`inf`或`NaN`值而失败，最终准确率为0，导致 $\\Delta A = -A_{\\text{fp32}}$。",
            "answer": "[[2.0, -0.5], [2.0, 0.0], [2.0, -1.0]]"
        }
    ]
}