{
    "hands_on_practices": [
        {
            "introduction": "A core skill in designing convolutional neural networks is precisely controlling the spatial dimensions of your feature maps. Instead of relying on a memorized formula, this exercise challenges you to derive the relationship between input size, kernel size, stride, and padding from first principles. By working through the logic of kernel placements, you will develop a deep, foundational understanding of how to manipulate padding to achieve any desired output size. ",
            "id": "3177669",
            "problem": "A one-dimensional (1D) discrete convolution layer in deep learning takes an input of length $n$, applies a kernel of width $k$, uses a stride $s \\in \\mathbb{N}$, and symmetric zero padding of $p \\in \\mathbb{Z}_{\\ge 0}$ elements on both the left and right. Assume standard, contiguous sampling with stride (no dilation), and that an output is produced only when the entire kernel window lies within the padded input.\n\nStarting only from the indexing definition of valid kernel placements under stride, derive constraints on $p$ that guarantee the output length is exactly a prescribed target $m \\in \\mathbb{N}$. Express these constraints explicitly as inequalities in terms of $n$, $k$, $s$, $p$, and $m$, and then solve those inequalities for $p$ to obtain a closed-form characterization (involving integer bounds) of all feasible $p$. Discuss feasibility subtleties for $s>1$, including the parity implicit in $2p$, and explain the boundary edge case where the final stride-aligned kernel placement lands exactly at the end of the padded input.\n\nThen, for the specific configuration $n=37$, $k=7$, $s=3$, and target $m=12$, determine whether a feasible nonnegative integer $p$ exists and, if so, compute the smallest such $p$. If no feasible $p$ exists, state that there is no solution. Your final reported answer must be the single smallest integer $p$ (no units and no rounding instructions are needed for this integer quantity).",
            "solution": "The problem asks for a derivation of the constraints on symmetric zero padding $p$ to achieve a specific output length $m$ for a one-dimensional convolution, and then to apply this derivation to a specific numerical case.\n\nFirst, we formalize the convolution operation based on the provided givens.\nAn input of length $n$ is padded with $p$ zeros on each side. The resulting padded input has a total length of $n_{padded} = n + 2p$. Let us use $0$-based indexing for the elements of this padded sequence, from index $0$ to $n+2p-1$.\n\nA kernel of width $k$ is applied to this padded input. An output is produced by placing the kernel at different positions. The stride $s$ dictates the step size between consecutive kernel placements. Let the output elements be indexed by $j$, where $j \\in \\{0, 1, 2, \\ldots, m-1\\}$, since the target output length is $m$.\n\nThe starting position of the kernel for the $j$-th output (where $j=0$ corresponds to the first output) is given by $j \\cdot s$. The kernel window then covers the indices from $j \\cdot s$ to $j \\cdot s + k - 1$ in the padded input.\n\nThe problem states that an output is produced only when the entire kernel window lies within the padded input. This imposes two boundary conditions on any valid starting position $i_{start}$:\n1. The start of the kernel, $i_{start}$, must be at or after the beginning of the padded input: $i_{start} \\ge 0$.\n2. The end of the kernel, $i_{start} + k - 1$, must be at or before the end of the padded input: $i_{start} + k - 1 \\le n+2p-1$.\n\nThe second condition can be rewritten as $i_{start} \\le n+2p-k$.\nSince the starting positions are given by $j \\cdot s$ where $j \\ge 0$ and $s \\in \\mathbb{N}$, the first condition $j \\cdot s \\ge 0$ is always satisfied. Thus, the sole constraint on a valid placement for the $j$-th output is:\n$$ j \\cdot s \\le n+2p-k $$\n\nFor the output length to be exactly $m$, two conditions must be met simultaneously:\n1. An output must be produced for index $j=m-1$. This means the placement for the $m$-th output is valid.\n   $$ (m-1)s \\le n+2p-k $$\n2. An output must *not* be produced for index $j=m$. This means the placement for a hypothetical $(m+1)$-th output is invalid.\n   $$ m \\cdot s > n+2p-k $$\n\nCombining these two inequalities gives a single compound inequality that constrains the effective length of the feature map available for convolution, $n+2p-k$:\n$$ (m-1)s \\le n+2p-k < ms $$\n\nTo find the constraints on $p$, we solve this compound inequality for $p$.\nFrom the left side:\n$(m-1)s \\le n+2p-k \\implies (m-1)s - n + k \\le 2p \\implies p \\ge \\frac{(m-1)s - n + k}{2}$.\nFrom the right side:\n$n+2p-k < ms \\implies 2p < ms - n + k \\implies p < \\frac{ms - n + k}{2}$.\n\nThus, the closed-form characterization of all feasible values of $p$ is given by:\n$$ \\frac{(m-1)s - n + k}{2} \\le p < \\frac{ms - n + k}{2} $$\nSince $p$ must be a non-negative integer ($p \\in \\mathbb{Z}_{\\ge 0}$), a valid solution exists only if this interval $[\\frac{(m-1)s - n + k}{2}, \\frac{ms - n + k}{2})$ contains at least one non-negative integer.\n\nNow, we discuss feasibility subtleties for $s>1$.\nLet's analyze the interval for $2p$:\n$$ (m-1)s - n + k \\le 2p < ms - n + k $$\nLet $A = (m-1)s - n + k$. The condition is $A \\le 2p < A+s$. We are seeking a non-negative even integer, $2p$, within the interval $[A, A+s)$. The length of this interval is $s$.\nIf $s=1$, the interval is $[A, A+1)$, which contains exactly one integer, $A$. For a solution to exist, $A$ must be an even, non-negative integer. This is a very restrictive condition.\nIf $s>1$, the interval $[A, A+s)$ has length $s \\ge 2$. For any integer $A$, the interval $[A, A+s)$ is guaranteed to contain at least one even integer. For example, if $A$ is even, $A$ itself is a candidate. if $A$ is odd, $A+1$ is even, and since $s \\ge 2$, $A+1 < A+s$, so $A+1$ is in the interval.\nTherefore, for $s > 1$, an integer $p$ satisfying the bounds is always guaranteed to exist. The only remaining condition is that $p$ must be non-negative. This means the interval $[A, A+s)$ must contain at least one non-negative even integer. This is assured as long as the upper bound of the interval for $2p$ is positive, i.e., $A+s > 0$, which is $ms - n + k > 0$.\n\nThe parity of $2p$ being even is implicitly handled by the formulation. The term $2p$ must be an even integer. The expression $(m-1)s - n + k$ can be either even or odd. If it is even, then the boundary edge case is possible.\n\nThe boundary edge case, where the final stride-aligned kernel placement lands exactly at the end of the padded input, occurs when the inequality $(m-1)s \\le n+2p-k$ is satisfied with equality:\n$$ (m-1)s = n+2p-k $$\nSolving for $p$, we get $p = \\frac{(m-1)s - n + k}{2}$. For this precise alignment to be possible, the value of $p$ must be a non-negative integer. This requires the numerator, $(m-1)s - n + k$, to be a non-negative even integer.\n\nFinally, we apply these results to the specific configuration: $n=37$, $k=7$, $s=3$, and target $m=12$.\nWe need to find the smallest non-negative integer $p$ that satisfies the derived inequalities.\nSubstituting the given values into the interval for $p$:\nLower bound:\n$$ p \\ge \\frac{(12-1) \\cdot 3 - 37 + 7}{2} = \\frac{11 \\cdot 3 - 37 + 7}{2} = \\frac{33 - 37 + 7}{2} = \\frac{-4 + 7}{2} = \\frac{3}{2} = 1.5 $$\nUpper bound:\n$$ p < \\frac{12 \\cdot 3 - 37 + 7}{2} = \\frac{36 - 37 + 7}{2} = \\frac{-1 + 7}{2} = \\frac{6}{2} = 3 $$\nSo, the condition on $p$ is:\n$$ 1.5 \\le p < 3 $$\nWe are looking for the smallest integer $p$ that satisfies this condition. The integers in the half-open interval $[1.5, 3)$ are $\\{2\\}$. The only integer solution is $p=2$. Since $p=2$ is a non-negative integer, it is a feasible padding value. As it is the only integer solution in the valid range, it is also the smallest.\n\nTherefore, for the given configuration, a feasible non-negative integer $p$ exists, and the smallest such value is $2$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "In deep learning, what seems conceptually equivalent can sometimes differ in implementation, leading to subtle bugs. This practice explores two common approaches to downsampling: a direct convolution with a stride greater than one, versus a unit-stride convolution followed by a manual subsampling step. By implementing and comparing these two computational graphs, you will discover how framework-specific rules for \"same\" padding can cause their outputs to diverge, a critical insight for robust model development and debugging. ",
            "id": "3177651",
            "problem": "You are to compare two computational graphs implementing discrete one-dimensional cross-correlation in deep learning and quantify the numerical differences caused by stride and padding alignment. The comparison focuses on the distinction between \"convolution with stride $s$\" and \"convolution with stride $1$ followed by subsampling by $s$\".\n\nDefinitions and rules to be used:\n- Let $x \\in \\mathbb{R}^n$ be a one-dimensional input signal and $w \\in \\mathbb{R}^k$ be a one-dimensional kernel. The discrete cross-correlation (the operation commonly implemented by deep learning frameworks under the name \"convolution\") at location $j$ is defined as\n  $$y[j] = \\sum_{t=0}^{k-1} x_{\\text{pad}}[j + t] \\, w[t],$$\n  where $x_{\\text{pad}}$ denotes $x$ after zero-padding on the left and right.\n- Padding modes:\n  1. \"valid\": no zero-padding, i.e., the left and right padding lengths are both $0$.\n  2. \"same\": choose zero-padding on both sides such that the number of distinct receptive fields taken with stride $s$ equals the smallest integer not less than the ratio of input length to stride, and split the total padding into left and right such that the left padding is the greatest integer less than or equal to half of the total. Concretely, when stride equals $1$, the \"same\" mode uses a total padding of $k-1$ split into left and right so that left equals the greatest integer less than or equal to half of $k-1$.\n\nComputational graphs to compare:\n- Graph A (\"conv with stride $s$\"): pad according to the selected mode (as described), then compute the cross-correlation by sliding the window with step $s$ across the padded input, taking windows that are fully contained within the padded signal.\n- Graph B (\"conv then subsample by $s$\"): pad according to the selected mode but with stride equal to $1$, compute the stride-$1$ cross-correlation across all fully contained windows, then subsample the resulting sequence by taking every $s$-th element starting from index $0$.\n\nInput and kernel synthesis (deterministic):\n- For a given $n$, define $x[i] = \\sin(0.2 \\, i) + 0.1 \\, i$ for $i = 0, 1, \\dots, n-1$.\n- For a given $k$, define $w[t] = \\frac{t+1}{k}$ for $t = 0, 1, \\dots, k-1$.\n\nMeasurement:\n- For each test case, produce $y_A$ from Graph A and $y_B$ from Graph B.\n- Let $m = \\min\\{|y_A|, |y_B|\\}$. Define the comparison length to be $m$. If $m = 0$, set the difference to $0$ by convention.\n- Compute the maximum absolute difference\n  $$d_{\\max} = \\max_{0 \\le i < m} \\left| y_A[i] - y_B[i] \\right|.$$\n\nYour program must implement both graphs exactly according to the rules above and report $d_{\\max}$ for each test case.\n\nTest suite:\n- Case 1 (general \"valid\"): $n = 16$, $k = 3$, $s = 2$, mode $\\text{valid}$.\n- Case 2 (general \"same\", odd kernel): $n = 17$, $k = 5$, $s = 2$, mode $\\text{same}$.\n- Case 3 (boundary, small input and large stride): $n = 3$, $k = 4$, $s = 3$, mode $\\text{same}$.\n- Case 4 (edge, even kernel asymmetry): $n = 10$, $k = 2$, $s = 3$, mode $\\text{same}$.\n- Case 5 (edge, stride larger than kernel under \"valid\"): $n = 8$, $k = 3$, $s = 4$, mode $\\text{valid}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above, for example, $[d_1,d_2,d_3,d_4,d_5]$, where each $d_i$ is the floating-point value of $d_{\\max}$ for the $i$-th test case.",
            "solution": "The problem requires a quantitative comparison of two computational graphs for one-dimensional cross-correlation, differing in their implementation of stride and padding. The core of the problem lies in the distinction between a direct strided convolution (Graph A) and a unit-stride convolution followed by subsampling (Graph B). The numerical differences arise from the padding calculation, which is defined differently for the two graphs.\n\nThe fundamental principle is that the two computational graphs, Graph A and Graph B, produce identical outputs if and only if their respective padded input signals are identical. This is because the core operation—a dot product between a kernel and a signal window—is applied to identical start locations (indexed by multiples of the stride $s$) in both graphs. Any discrepancy must therefore stem from the padding applied to the original signal $x$.\n\nLet the input signal be $x \\in \\mathbb{R}^n$ and the kernel be $w \\in \\mathbb{R}^k$. The stride is $s$.\n\n**Analysis of Graph A (\"conv with stride $s$\")**\n\nGraph A first determines the padding required to achieve a target output dimension, then performs a strided cross-correlation.\n1.  **Padding Calculation**:\n    - For `mode='valid'`, the padding on both sides is $0$. Total padding $P_A = 0$.\n    - For `mode='same'`, the output length $o_A$ is defined as $\\lceil n/s \\rceil$. The total padding $P_A$ required to achieve this output length is given by the standard formula $P_A = \\max(0, (o_A-1)s + k - n)$. The total padding is split into left padding $p_{l,A} = \\lfloor P_A/2 \\rfloor$ and right padding $p_{r,A} = P_A - p_{l,A}$.\n2.  **Padded Signal**: The input $x$ is padded with $p_{l,A}$ zeros on the left and $p_{r,A}$ zeros on the right to form $x_{pad,A}$.\n3.  **Cross-Correlation**: The output $y_A$ is computed by sliding the kernel $w$ over $x_{pad,A}$ with a step size of $s$. The $j$-th element of the output (for $j=0, 1, \\dots, o_A-1$) is:\n    $$y_A[j] = \\sum_{t=0}^{k-1} x_{pad,A}[j \\cdot s + t] \\, w[t]$$\n\n**Analysis of Graph B (\"conv then subsample by $s$\")**\n\nGraph B performs a unit-stride convolution and then subsamples the result. The padding is calculated as if the stride were always $1$.\n1.  **Padding Calculation**:\n    - For `mode='valid'`, the padding is $0$ on both sides. Total padding $P_B = 0$.\n    - For `mode='same'`, the problem states that the padding is calculated for a stride of $s=1$. This yields a total padding of $P_B = k-1$. This is split into left padding $p_{l,B} = \\lfloor P_B/2 \\rfloor = \\lfloor (k-1)/2 \\rfloor$ and right padding $p_{r,B} = P_B - p_{l,B}$.\n2.  **Padded Signal**: The input $x$ is padded to form $x_{pad,B}$.\n3.  **Cross-Correlation and Subsampling**: First, an intermediate signal $y_{int,B}$ is computed using a stride of $1$:\n    $$y_{int,B}[j'] = \\sum_{t=0}^{k-1} x_{pad,B}[j' + t] \\, w[t]$$\n    The final output $y_B$ is obtained by subsampling $y_{int,B}$ by taking every $s$-th element, starting from index $0$: $y_B[j] = y_{int,B}[j \\cdot s]$. Substituting the expression for $y_{int,B}$:\n    $$y_B[j] = \\sum_{t=0}^{k-1} x_{pad,B}[j \\cdot s + t] \\, w[t]$$\n\n**Condition for Equivalence and Source of Difference**\n\nComparing the final expressions for $y_A[j]$ and $y_B[j]$, it is clear that $y_A$ and $y_B$ are identical if and only if their underlying padded signals, $x_{pad,A}$ and $x_{pad,B}$, are identical (assuming their lengths are compatible, which they will be). This is equivalent to the condition that the total paddings are equal, $P_A = P_B$, and they are split in the same way (which they are, based on the $\\lfloor P/2 \\rfloor$ rule for the left side).\n\n-   **For `mode='valid'`**: $P_A = 0$ and $P_B = 0$. The graphs are always equivalent, and the difference $d_{\\max}$ must be $0$.\n\n-   **For `mode='same'`**: We must compare $P_A = \\max(0, (\\lceil n/s \\rceil-1)s+k-n)$ with $P_B = k-1$. The two are equal if $(\\lceil n/s \\rceil-1)s+k-n = k-1$, assuming the term is non-negative. This simplifies to:\n    $$(\\lceil n/s \\rceil-1)s = n-1$$\n    Let's analyze this condition. Let $n = q \\cdot s + r$, where $q$ is the quotient and $r$ is the remainder ($0 \\le r < s$).\n    - If $r=0$ (i.e., $n$ is a multiple of $s$), then $\\lceil n/s \\rceil=q$. The condition becomes $(q-1)s = n-1 \\implies qs-s=n-1 \\implies n-s=n-1 \\implies s=1$. Thus, for $s > 1$, the condition fails.\n    - If $r>0$, then $\\lceil n/s \\rceil=q+1$. The condition becomes $(q+1-1)s = n-1 \\implies qs=n-1 \\implies qs = (qs+r)-1 \\implies r-1=0 \\implies r=1$.\n    Therefore, for `mode='same'` and $s>1$, the two graphs are equivalent if and only if $n \\pmod s = 1$. If $n \\pmod s \\neq 1$, the padded signals will differ, leading to $y_A \\neq y_B$ and a potential non-zero difference $d_{\\max}$.\n\n**Application to Test Cases**\n\n-   **Case 1**: $n=16, k=3, s=2$, `mode=valid`. $P_A=P_B=0$. Expected $d_{\\max}=0$.\n-   **Case 2**: $n=17, k=5, s=2$, `mode=same`. Here, $17 \\pmod 2 = 1$. The condition for equivalence holds. Expected $d_{\\max}=0$.\n-   **Case 3**: $n=3, k=4, s=3$, `mode=same`. Here, $3 \\pmod 3 = 0$. Since $s > 1$, the condition for equivalence fails. The paddings will differ:\n    - $P_A = \\max(0, (\\lceil 3/3 \\rceil - 1)3 + 4 - 3) = \\max(0, 0+1) = 1$.\n    - $P_B = k-1 = 3$.\n    A non-zero $d_{\\max}$ is expected.\n-   **Case 4**: $n=10, k=2, s=3$, `mode=same`. Here, $10 \\pmod 3 = 1$. The condition for equivalence holds. Expected $d_{\\max}=0$.\n-   **Case 5**: $n=8, k=3, s=4$, `mode=valid`. $P_A=P_B=0$. Expected $d_{\\max}=0$.\n\nThe implementation will proceed by constructing both graphs precisely according to these rules and computing the specified maximum absolute difference.",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes the numerical difference between two implementations of 1D cross-correlation\n    for a given set of test cases.\n\n    The two implementations are:\n    - Graph A: Direct strided convolution.\n    - Graph B: Unit-stride convolution followed by subsampling.\n\n    The difference arises from how padding is calculated in 'same' mode, which is\n    stride-dependent in Graph A but stride-agnostic (fixed to s=1) in Graph B.\n    \"\"\"\n\n    test_cases = [\n        # (n, k, s, mode)\n        (16, 3, 2, 'valid'),\n        (17, 5, 2, 'same'),\n        (3, 4, 3, 'same'),\n        (10, 2, 3, 'same'),\n        (8, 3, 4, 'valid'),\n    ]\n\n    results = []\n\n    def cross_correlate(x_padded: np.ndarray, w: np.ndarray, stride: int) -> np.ndarray:\n        \"\"\"\n        Computes 1D cross-correlation with a given stride on a pre-padded signal.\n        \"\"\"\n        k = len(w)\n        n_padded = len(x_padded)\n        output_len = math.floor((n_padded - k) / stride) + 1\n        if output_len <= 0:\n            return np.array([])\n        \n        y = np.zeros(output_len)\n        for j in range(output_len):\n            start_index = j * stride\n            window = x_padded[start_index : start_index + k]\n            y[j] = np.dot(window, w)\n        return y\n\n    for n, k, s, mode in test_cases:\n        # Step 1: Synthesize input signal x and kernel w\n        i_vals = np.arange(n)\n        x = np.sin(0.2 * i_vals) + 0.1 * i_vals\n\n        t_vals = np.arange(k)\n        w = (t_vals + 1) / k\n\n        # Step 2: Implement and compute y_A for Graph A\n        if mode == 'valid':\n            p_left_A = 0\n            p_right_A = 0\n        elif mode == 'same':\n            out_len_A = math.ceil(n / s)\n            total_pad_A = max(0, (out_len_A - 1) * s + k - n)\n            p_left_A = math.floor(total_pad_A / 2)\n            p_right_A = total_pad_A - p_left_A\n        else:\n            raise ValueError(\"Invalid mode\")\n\n        x_padded_A = np.pad(x, (p_left_A, p_right_A), 'constant', constant_values=0)\n        y_A = cross_correlate(x_padded_A, w, s)\n\n        # Step 3: Implement and compute y_B for Graph B\n        if mode == 'valid':\n            p_left_B = 0\n            p_right_B = 0\n        elif mode == 'same':\n            # Padding is calculated as if stride were 1\n            total_pad_B = k - 1\n            p_left_B = math.floor(total_pad_B / 2)\n            p_right_B = total_pad_B - p_left_B\n        else:\n            raise ValueError(\"Invalid mode\")\n\n        x_padded_B = np.pad(x, (p_left_B, p_right_B), 'constant', constant_values=0)\n        y_intermediate_B = cross_correlate(x_padded_B, w, 1)\n        y_B = y_intermediate_B[::s]\n\n        # Step 4: Compute the maximum absolute difference d_max\n        m = min(len(y_A), len(y_B))\n        if m == 0:\n            d_max = 0.0\n        else:\n            diff = np.abs(y_A[:m] - y_B[:m])\n            d_max = np.max(diff)\n\n        results.append(d_max)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Architectures like the U-Net rely on skip connections that fuse information from different spatial scales, which is vital for tasks like image segmentation. However, the success of this fusion hinges on perfect spatial alignment between feature maps from the encoder and decoder paths. This advanced exercise introduces a formal method for tracking feature coordinates through a network, allowing you to quantify the misalignment caused by seemingly innocuous choices like using even-sized kernels. Mastering this concept is key to building and debugging sophisticated, high-precision models. ",
            "id": "3180119",
            "problem": "Consider a Two-Dimensional (2D) U-shaped Convolutional Neural Network (U-Net) that performs one downsampling stage and one upsampling stage. The network uses the cross-correlation form of the two-dimensional discrete convolution, where for an input feature map $X$ and a kernel $W$ of size $k_{x} \\times k_{y}$, the output feature map $Y$ is given (along each spatial axis independently) by\n$$\nY[i,j] \\;=\\; \\sum_{u=0}^{k_{x}-1} \\sum_{v=0}^{k_{y}-1} W[u,v] \\, X[s_{x} i - p_{x} + u, \\, s_{y} j - p_{y} + v].\n$$\nAssume the following conventions and parameters:\n- The stride along each axis for standard convolutions is $s_{x} = s_{y} = 1$.\n- The padding implements the commonly used “same” rule $p_{x} = \\left\\lfloor \\frac{k_{x}}{2} \\right\\rfloor$ and $p_{y} = \\left\\lfloor \\frac{k_{y}}{2} \\right\\rfloor$.\n- The index of the kernel’s discrete center along each axis is $c_{x} = \\left\\lfloor \\frac{k_{x}-1}{2} \\right\\rfloor$ and $c_{y} = \\left\\lfloor \\frac{k_{y}-1}{2} \\right\\rfloor$.\n- Two-dimensional max pooling uses a window of size $2 \\times 2$, stride $2$, and zero padding along both axes. Treat the representative location of a pooled output as the center-of-window determined by $(c_{x}, c_{y})$ defined above.\n- Two-dimensional transposed convolution (fractionally strided convolution) with kernel size $2 \\times 2$ and stride $2$ along each axis is implemented by the standard deep learning definition equivalent to inserting zeros between inputs and then applying a cross-correlation with the given kernel; its padding is $p_{x} = p_{y} = 0$, and its center indices are $c_{x} = c_{y} = \\left\\lfloor \\frac{2-1}{2} \\right\\rfloor$.\n\nDefine a coordinate mapping from the index $(i,j)$ of any intermediate feature map back to the input image’s coordinate system by a pair $(S_{x}, S_{y}, \\Delta_{x}, \\Delta_{y})$, where $S_{x}$ and $S_{y}$ are the cumulative strides and $(\\Delta_{x}, \\Delta_{y})$ are the cumulative offsets in input pixels. For a standard convolution or pooling layer with stride $(s_{x}, s_{y})$, padding $(p_{x}, p_{y})$, and center indices $(c_{x}, c_{y})$ applied to an input with mapping $(S_{x}^{\\mathrm{in}}, S_{y}^{\\mathrm{in}}, \\Delta_{x}^{\\mathrm{in}}, \\Delta_{y}^{\\mathrm{in}})$, the output mapping is\n$$\nS_{x}^{\\mathrm{out}} \\;=\\; S_{x}^{\\mathrm{in}} \\, s_{x}, \\quad S_{y}^{\\mathrm{out}} \\;=\\; S_{y}^{\\mathrm{in}} \\, s_{y}, \\quad\n\\Delta_{x}^{\\mathrm{out}} \\;=\\; \\Delta_{x}^{\\mathrm{in}} + S_{x}^{\\mathrm{in}} (c_{x} - p_{x}), \\quad\n\\Delta_{y}^{\\mathrm{out}} \\;=\\; \\Delta_{y}^{\\mathrm{in}} + S_{y}^{\\mathrm{in}} (c_{y} - p_{y}).\n$$\nFor a transposed convolution with stride $(s_{x}, s_{y})$, padding $(p_{x}, p_{y})$, and center indices $(c_{x}, c_{y})$, applied to an input with mapping $(S_{x}^{\\mathrm{in}}, S_{y}^{\\mathrm{in}}, \\Delta_{x}^{\\mathrm{in}}, \\Delta_{y}^{\\mathrm{in}})$, the output mapping is\n$$\nS_{x}^{\\mathrm{out}} \\;=\\; \\frac{S_{x}^{\\mathrm{in}}}{s_{x}}, \\quad S_{y}^{\\mathrm{out}} \\;=\\; \\frac{S_{y}^{\\mathrm{in}}}{s_{y}}, \\quad\n\\Delta_{x}^{\\mathrm{out}} \\;=\\; \\Delta_{x}^{\\mathrm{in}} + \\frac{S_{x}^{\\mathrm{in}}}{s_{x}} (p_{x} - c_{x}), \\quad\n\\Delta_{y}^{\\mathrm{out}} \\;=\\; \\Delta_{y}^{\\mathrm{in}} + \\frac{S_{y}^{\\mathrm{in}}}{s_{y}} (p_{y} - c_{y}).\n$$\n\nConsider the following minimal U-Net path:\n1. Encoder stage: a $3 \\times 3$ standard convolution with “same” padding, followed by $2 \\times 2$ max pooling with stride $2$ and zero padding.\n2. Decoder stage: a $2 \\times 2$ transposed convolution with stride $2$ and zero padding, followed by a standard convolution whose kernel is either $3 \\times 3$ (odd) or $2 \\times 2$ (even), with “same” padding.\n\nA skip connection concatenates the encoder’s feature map immediately after the first $3 \\times 3$ convolution with the decoder’s feature map immediately after the final standard convolution. Using the coordinate mapping rules above, derive the cumulative $(S_{x}, S_{y}, \\Delta_{x}, \\Delta_{y})$ for both the encoder feature and the decoder feature, and use them to compute the spatial misalignment vector between these two features at the skip connection when the decoder’s final standard convolution uses a $2 \\times 2$ kernel with “same” padding. Then, report the Euclidean magnitude of this misalignment vector, measured in pixels at the skip-connection resolution, as a single real-valued number. If no misalignment arises, the magnitude is zero. Provide only the magnitude as your final answer.",
            "solution": "The user wants to determine the spatial misalignment between features at a skip connection in a simplified U-Net architecture. The problem provides a formal definition for tracking coordinate systems through various neural network layers. I will solve this by calculating the coordinate mapping for each feature involved in the skip connection and then finding the magnitude of the difference between their corresponding spatial positions.\n\nThe coordinate mapping of a feature map is described by a tuple $(S_{x}, S_{y}, \\Delta_{x}, \\Delta_{y})$, where $(S_x, S_y)$ are the cumulative strides and $(\\Delta_x, \\Delta_y)$ are the cumulative offsets relative to the input image's coordinate system. A point at index $(i,j)$ in the feature map corresponds to a point $(i \\cdot S_x + \\Delta_x, j \\cdot S_y + \\Delta_y)$ in the input image coordinate system.\n\nThe input image itself has an identity mapping, which we can denote as Layer $0$.\nInitial map (Layer $0$): $(S_x^{(0)}, S_y^{(0)}, \\Delta_x^{(0)}, \\Delta_y^{(0)}) = (1, 1, 0, 0)$.\n\nThe problem describes a skip connection that concatenates a feature map from the encoder with a feature map from the decoder. We must find the coordinate mapping for each of these two feature maps.\n\n**1. Encoder Feature Map (Layer E1) for the Skip Connection**\n\nThis feature map is the output of the first operation in the encoder: a $3 \\times 3$ standard convolution with \"same\" padding.\n- Input map: $(S_x^{(0)}, S_y^{(0)}, \\Delta_x^{(0)}, \\Delta_y^{(0)}) = (1, 1, 0, 0)$.\n- Operation: Standard convolution.\n- Parameters:\n  - Kernel size: $k_x = 3, k_y = 3$.\n  - Stride: $s_x = 1, s_y = 1$.\n  - Padding (\"same\"): $p_x = \\lfloor \\frac{3}{2} \\rfloor = 1$, $p_y = \\lfloor \\frac{3}{2} \\rfloor = 1$.\n  - Kernel center: $c_x = \\lfloor \\frac{3-1}{2} \\rfloor = 1$, $c_y = \\lfloor \\frac{3-1}{2} \\rfloor = 1$.\n\nWe apply the given update rule for standard convolutions:\n$S_{x}^{\\mathrm{out}} = S_{x}^{\\mathrm{in}} \\, s_{x}$\n$\\Delta_{x}^{\\mathrm{out}} = \\Delta_{x}^{\\mathrm{in}} + S_{x}^{\\mathrm{in}} (c_{x} - p_{x})$\n(and similarly for the $y$ dimension).\n\n- Cumulative strides for E1:\n$S_x^{(E1)} = S_x^{(0)} \\cdot s_x = 1 \\cdot 1 = 1$\n$S_y^{(E1)} = S_y^{(0)} \\cdot s_y = 1 \\cdot 1 = 1$\n\n- Cumulative offsets for E1:\n$\\Delta_x^{(E1)} = \\Delta_x^{(0)} + S_x^{(0)} (c_x - p_x) = 0 + 1(1 - 1) = 0$\n$\\Delta_y^{(E1)} = \\Delta_y^{(0)} + S_y^{(0)} (c_y - p_y) = 0 + 1(1 - 1) = 0$\n\nThe coordinate map for the encoder feature at the skip connection is $(S_x, S_y, \\Delta_x, \\Delta_y)_{E1} = (1, 1, 0, 0)$.\n\n**2. Decoder Feature Map (Layer D2) for the Skip Connection**\n\nThis feature map is the result of passing data through the entire specified U-Net path. We trace the coordinate map transformations step-by-step.\n\n**Step 2a: Max Pooling (Layer E2)**\nThis operation follows the first convolution. Its input is the feature map E1.\n- Input map: $(S_x^{(E1)}, S_y^{(E1)}, \\Delta_x^{(E1)}, \\Delta_y^{(E1)}) = (1, 1, 0, 0)$.\n- Operation: Max pooling, treated as a standard convolution for coordinate mapping.\n- Parameters:\n  - Window size: $k_x = 2, k_y = 2$.\n  - Stride: $s_x = 2, s_y = 2$.\n  - Padding: $p_x = 0, p_y = 0$.\n  - Window center: $c_x = \\lfloor \\frac{2-1}{2} \\rfloor = 0$, $c_y = \\lfloor \\frac{2-1}{2} \\rfloor = 0$.\n\n- Cumulative strides for E2:\n$S_x^{(E2)} = S_x^{(E1)} \\cdot s_x = 1 \\cdot 2 = 2$\n$S_y^{(E2)} = S_y^{(E1)} \\cdot s_y = 1 \\cdot 2 = 2$\n\n- Cumulative offsets for E2:\n$\\Delta_x^{(E2)} = \\Delta_x^{(E1)} + S_x^{(E1)} (c_x - p_x) = 0 + 1(0 - 0) = 0$\n$\\Delta_y^{(E2)} = \\Delta_y^{(E1)} + S_y^{(E1)} (c_y - p_y) = 0 + 1(0 - 0) = 0$\n\nThe coordinate map for the bottleneck feature map (E2) is $(2, 2, 0, 0)$.\n\n**Step 2b: Transposed Convolution (Layer D1)**\nThis is the first operation in the decoder (upsampling path). Its input is the bottleneck feature map E2.\n- Input map: $(S_x^{(E2)}, S_y^{(E2)}, \\Delta_x^{(E2)}, \\Delta_y^{(E2)}) = (2, 2, 0, 0)$.\n- Operation: Transposed convolution.\n- Parameters:\n  - Kernel size: $k_x = 2, k_y = 2$.\n  - Stride: $s_x = 2, s_y = 2$.\n  - Padding: $p_x = 0, p_y = 0$.\n  - Kernel center: $c_x = \\lfloor \\frac{2-1}{2} \\rfloor = 0$, $c_y = \\lfloor \\frac{2-1}{2} \\rfloor = 0$.\n\nWe apply the given update rule for transposed convolutions:\n$S_{x}^{\\mathrm{out}} = S_{x}^{\\mathrm{in}} / s_{x}$\n$\\Delta_{x}^{\\mathrm{out}} = \\Delta_{x}^{\\mathrm{in}} + \\frac{S_{x}^{\\mathrm{in}}}{s_{x}} (p_{x} - c_{x})$\n\n- Cumulative strides for D1:\n$S_x^{(D1)} = S_x^{(E2)} / s_x = 2 / 2 = 1$\n$S_y^{(D1)} = S_y^{(E2)} / s_y = 2 / 2 = 1$\n\n- Cumulative offsets for D1:\n$\\Delta_x^{(D1)} = \\Delta_x^{(E2)} + \\frac{S_x^{(E2)}}{s_x} (p_x - c_x) = 0 + \\frac{2}{2}(0 - 0) = 0$\n$\\Delta_y^{(D1)} = \\Delta_y^{(E2)} + \\frac{S_y^{(E2)}}{s_y} (p_y - c_y) = 0 + \\frac{2}{2}(0 - 0) = 0$\n\nThe coordinate map after upsampling (D1) is $(1, 1, 0, 0)$.\n\n**Step 2c: Final Standard Convolution (Layer D2)**\nThis is the final operation before the skip connection. Its input is the upsampled feature map D1. The problem specifies a $2 \\times 2$ kernel.\n- Input map: $(S_x^{(D1)}, S_y^{(D1)}, \\Delta_x^{(D1)}, \\Delta_y^{(D1)}) = (1, 1, 0, 0)$.\n- Operation: Standard convolution.\n- Parameters:\n  - Kernel size: $k_x = 2, k_y = 2$.\n  - Stride: $s_x = 1, s_y = 1$.\n  - Padding (\"same\"): $p_x = \\lfloor \\frac{2}{2} \\rfloor = 1$, $p_y = \\lfloor \\frac{2}{2} \\rfloor = 1$.\n  - Kernel center: $c_x = \\lfloor \\frac{2-1}{2} \\rfloor = 0$, $c_y = \\lfloor \\frac{2-1}{2} \\rfloor = 0$.\n\n- Cumulative strides for D2:\n$S_x^{(D2)} = S_x^{(D1)} \\cdot s_x = 1 \\cdot 1 = 1$\n$S_y^{(D2)} = S_y^{(D1)} \\cdot s_y = 1 \\cdot 1 = 1$\n\n- Cumulative offsets for D2:\n$\\Delta_x^{(D2)} = \\Delta_x^{(D1)} + S_x^{(D1)} (c_x - p_x) = 0 + 1(0 - 1) = -1$\n$\\Delta_y^{(D2)} = \\Delta_y^{(D1)} + S_y^{(D1)} (c_y - p_y) = 0 + 1(0 - 1) = -1$\n\nThe coordinate map for the decoder feature at the skip connection (D2) is $(S_x, S_y, \\Delta_x, \\Delta_y)_{D2} = (1, 1, -1, -1)$.\n\n**3. Calculating the Misalignment**\n\nWe now have the coordinate maps for the two features being concatenated at the skip connection:\n- Encoder feature E1: $(S_x, S_y, \\Delta_x, \\Delta_y)_{E1} = (1, 1, 0, 0)$.\n- Decoder feature D2: $(S_x, S_y, \\Delta_x, \\Delta_y)_{D2} = (1, 1, -1, -1)$.\n\nLet's consider a point at index $(i,j)$ in the spatial grid of the feature maps at the skip connection.\n- The corresponding location in the input image for the encoder feature is:\n$(x_E, y_E) = (i \\cdot S_{x,E1} + \\Delta_{x,E1}, j \\cdot S_{y,E1} + \\Delta_{y,E1}) = (i \\cdot 1 + 0, j \\cdot 1 + 0) = (i, j)$.\n- The corresponding location in the input image for the decoder feature is:\n$(x_D, y_D) = (i \\cdot S_{x,D2} + \\Delta_{x,D2}, j \\cdot S_{y,D2} + \\Delta_{y,D2}) = (i \\cdot 1 + (-1), j \\cdot 1 + (-1)) = (i-1, j-1)$.\n\nThe spatial misalignment vector in the input image coordinate system is the difference between these two locations:\n$\\vec{v} = (x_E - x_D, y_E - y_D) = (i - (i-1), j - (j-1)) = (1, 1)$.\n\nThe problem asks for the magnitude of this vector \"measured in pixels at the skip-connection resolution\". The cumulative stride for both features at the skip connection is $S_x=1, S_y=1$. This means that a single pixel step in the feature map corresponds to a single pixel step in the original input image. Therefore, the misalignment vector $(1, 1)$ in input pixels is also the misalignment vector in feature map pixels.\n\nThe Euclidean magnitude of this misalignment vector is:\n$|\\vec{v}| = \\sqrt{1^2 + 1^2} = \\sqrt{2}$.\n\nThis misalignment arises from the asymmetry of using an even-sized kernel ($2 \\times 2$) with \"same\" padding. The choice of kernel center $c=0$ combined with padding $p=1$ creates a positional shift.",
            "answer": "$$\\boxed{\\sqrt{2}}$$"
        }
    ]
}