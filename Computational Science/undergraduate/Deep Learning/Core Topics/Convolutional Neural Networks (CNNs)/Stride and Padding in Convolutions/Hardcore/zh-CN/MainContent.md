## 引言
卷积运算是[深度学习](@entry_id:142022)，特别是[卷积神经网络](@entry_id:178973)（CNN）中[特征提取](@entry_id:164394)的基石。然而，一个成功的[CNN架构](@entry_id:635079)不仅仅依赖于学习到的[卷积核](@entry_id:635097)。两个看似简单的超参数——**步幅（stride）**和**填充（padding）**——在其中扮演着至关重要的角色，它们共同决定了网络的几何结构、[计算效率](@entry_id:270255)和最终性能。许多开发者仅将它们视为调整[特征图尺寸](@entry_id:637663)的工具，却忽略了其背后更深层次的原理和影响，这构成了从模型使用者到架构设计者之间的一道知识鸿沟。

本文旨在系统性地填补这一空白。我们将深入剖析[步幅与填充](@entry_id:635382)的核心机制及其在现代[深度学习](@entry_id:142022)实践中的多方面应用。
- 在“**原理与机制**”一章中，您将学习它们如何通过精确的数学公式定义输出几何，影响计算成本，以及不同填充策略（如反射填充）如何管理边界效应和[梯度流](@entry_id:635964)。我们还将从信号处理的视角，揭示步幅与[平移等变性](@entry_id:636340)和信号混叠之间的深刻联系。
- 随后的“**应用与跨学科联系**”一章将展示这些原理如何应用于解决实际问题，例如在[U-Net](@entry_id:635895)中实现精确的[特征对齐](@entry_id:634064)，在医学图像和[音频处理](@entry_id:273289)中保证[信号完整性](@entry_id:170139)，以及优化GPU上的计算性能。
- 最后，“**动手实践**”部分将提供一系列编程练习，让您亲手实现和验证这些概念，将理论知识转化为实践技能。

通过本文的学习，您将能够自信地运用步幅和填充来设计更高效、更稳健、更精确的[卷积神经网络](@entry_id:178973)。让我们从最基本的原理开始。

## 原理与机制

在前一章中，我们介绍了卷积运算作为[深度学习](@entry_id:142022)中[特征提取](@entry_id:164394)的核心构建模块。然而，一个卷积层的具体行为不仅由其权重（即核）决定，还由两个关键的超参数深刻地塑造：**步幅（stride）**和**填充（padding）**。这两个参数共同控制着输出特征图的几何形状、计算成本，并对网络的学习动态和性能产生深远影响。本章将深入探讨步幅和填充的原理与机制，从基本定义出发，揭示它们在现代[卷积神经网络](@entry_id:178973)中的多方面作用。

### 核心机制：定义输出几何

步幅和填充最直接的功能是控制卷积层输出的空间维度。理解它们如何做到这一点，是设计有效网络架构的第一步。

#### 输出尺寸公式

让我们从一维情况出发，通过第一性原理推导出输出尺寸的计算公式。考虑一个长度为 $H_{\text{in}}$ 的输入序列，一个大小为 $k_h$ 的[卷积核](@entry_id:635097)，步幅为 $s_h$，以及在序列两端进行非对称的零填充，前端填充 $p_{\text{top}}$ 个零，后端填充 $p_{\text{bottom}}$ 个零。

填充后的序列总长度为 $H'_{\text{in}} = H_{\text{in}} + p_{\text{top}} + p_{\text{bottom}}$。卷积核在此填充序列上滑动。对于输出序列的第 $i$ 个元素（从0开始索引），其对应的[卷积核](@entry_id:635097)窗口在填充序列上的起始位置为 $i \times s_h$。为了使整个卷积核窗口都落在填充序列的有效范围内，窗口的最后一个元素的位置必须小于填充序列的总长度。窗口覆盖的索引范围是从 $i \times s_h$ 到 $i \times s_h + k_h - 1$。因此，必须满足以下条件：

$$
i \times s_h + k_h - 1 \le H'_{\text{in}}
$$

求解 $i$，我们得到：

$$
i \le \frac{H'_{\text{in}} - k_h}{s_h} = \frac{H_{\text{in}} + p_{\text{top}} + p_{\text{bottom}} - k_h}{s_h}
$$

由于 $i$ 必须是整数，所以其最大值为 $\lfloor \frac{H_{\text{in}} + p_{\text{top}} + p_{\text{bottom}} - k_h}{s_h} \rfloor$。因为输出索引从 $0$ 开始，所以输出序列的总长度 $H_{\text{out}}$ 为最大索引值加一：

$$
H_{\text{out}} = \left\lfloor \frac{H_{\text{in}} + p_{\text{top}} + p_{\text{bottom}} - k_{h}}{s_{h}} \right\rfloor + 1
$$

这个推导过程直接源于对卷积操作的计数论证 。同样地，对于[二维卷积](@entry_id:275218)，我们可以将此公式独立地应用于宽度和高度两个维度：

$$
H_{\text{out}} = \left\lfloor \frac{H_{\text{in}} + p_{\text{top}} + p_{\text{bottom}} - k_{h}}{s_{h}} \right\rfloor + 1
$$
$$
W_{\text{out}} = \left\lfloor \frac{W_{\text{in}} + p_{\text{left}} + p_{\text{right}} - k_{w}}{s_{w}} \right\rfloor + 1
$$

在实践中，最常见的情况是**对称填充**，即 $p_{\text{top}} = p_{\text{bottom}} = p_h$ 和 $p_{\text{left}} = p_{\text{right}} = p_w$。此时，公式简化为更常见的形式：

$$
H_{\text{out}} = \left\lfloor \frac{H_{\text{in}} + 2p_{h} - k_{h}}{s_{h}} \right\rfloor + 1
$$

例如，考虑一个具体的非对称填充配置：输入尺寸 $H_{\text{in}} = 11, W_{\text{in}} = 15$；核尺寸 $k_h = 3, k_w = 5$；步幅 $s_h = 2, s_w = 4$；填充 $p_{\text{top}} = 1, p_{\text{bottom}} = 2, p_{\text{left}} = 3, p_{\text{right}} = 0$。根据我们的公式 ：

$$
H_{\text{out}} = \left\lfloor \frac{11 + 1 + 2 - 3}{2} \right\rfloor + 1 = \left\lfloor \frac{11}{2} \right\rfloor + 1 = 5 + 1 = 6
$$
$$
W_{\text{out}} = \left\lfloor \frac{15 + 3 + 0 - 5}{4} \right\rfloor + 1 = \left\lfloor \frac{13}{4} \right\rfloor + 1 = 3 + 1 = 4
$$

输出[特征图](@entry_id:637719)的尺寸将是 $6 \times 4$。

#### 填充与计算成本

一个看似违反直觉但至关重要的事实是，即使填充的是零，**填充操作通常也会增加计算成本**。原因在于，填充扩大了[卷积核](@entry_id:635097)需要滑过的有效区域，从而增加了输出位置的数量。

总的计算量（以乘加运算，即**multiply-adds**，为单位）可以通过将每个输出位置的计算量与输出位置的总数相乘来估算。对于一个大小为 $k$ 的一维[卷积核](@entry_id:635097)，计算单个输出值需要 $k$ 次乘加运算。因此，总的乘加次数 $N$ 为：

$$
N(n,k,s,p) = k \times \left( \left\lfloor \frac{n + 2p - k}{s} \right\rfloor + 1 \right)
$$

其中 $n$ 是输入长度，$k$ 是核大小，$s$ 是步幅，$p$ 是对称填充大小。从这个公式可以清楚地看到，增加填充 $p$ 会增加分子 $n + 2p - k$，这可能导致输出长度 $L_{\text{out}}$ 增加，进而导致总计算量 $N$ 的增加。

例如，考虑一个输入长度 $n = 123$，核大小 $k = 11$，步幅 $s = 5$ 的卷积。在没有填充 ($p=0$) 的情况下，输出长度为 $\lfloor (123-11)/5 \rfloor + 1 = 23$，总乘加次数为 $11 \times 23 = 253$。如果我们应用对称填充 $p=8$，输出长度增加到 $\lfloor (123 + 16 - 11)/5 \rfloor + 1 = 26$，总乘加次数为 $11 \times 26 = 286$。尽管我们只是添加了零，但计算量却净增了 $33$ 次乘加运算 。这是因为算法在新增的输出位置上仍然执行了完整的卷积计算，即使其中一些乘法是与填充的零进行的。

### 填充的角色：管理边界与对齐

填充的功能远不止于控制尺寸。它定义了我们如何处理数据的边界，这对于特征表示的连续性和学习过程本身都有着重要影响。

#### [零填充](@entry_id:637925)之外的选择：复制与反射填充

虽然**[零填充](@entry_id:637925)（zero padding）**是最常见的选择，但它引入了一个强烈的先验假设：图像外部是黑色的。这在许多情况下可能不自然，并可能在图像边界产生不必要的伪影。为了缓解这个问题，人们提出了其他填充策略。

- **复制填充（replicate padding）**：将边界像素的值复制到填充区域。这假设图像边界之外的区域是恒定的。
- **反射填充（reflect padding）**：将图像内部的像素值镜像到填充区域，就像在边界上放置了一面镜子。

这两种策略在处理靠近边界的纹理时表现出显著差异。假设图像边界附近存在一个平稳的边缘纹理，其局部统计量（如均值和[方差](@entry_id:200758)）保持近似恒定。

使用**复制填充**会在边界外创建一个恒定强度的区域，其局部[方差](@entry_id:200758)为零。这与内部的纹理（[方差](@entry_id:200758)大于零）形成了剧烈对比。当一个类似导数的核（如索贝尔算子）扫过这个边界时，它会在输出特征图中产生一个强烈的、虚假的响应，这是一种由填充引入的**边界伪影**。

相比之下，**反射填充**通过镜像内部纹理，在填充区域中创建了一个与内部统计特性非常相似的虚拟纹理。局部均值和[方差](@entry_id:200758)得以连续过渡。因此，当导数核扫过边界时，输出响应更加平滑，更好地保持了纹理的**感知连续性**。从结构相似性指数（SSIM）的角度来看，一个内部图像块和一个由反射填充生成的相邻外部块之间的SSIM值会非常接近1，而与由复制填充生成的外部块（[方差](@entry_id:200758)为零）的SSIM值则会低得多 。

#### 填充与梯度流

填充策略的选择不仅影响[前向传播](@entry_id:193086)，也深刻地影响了[反向传播](@entry_id:199535)中的梯度计算。由于不同的填充方案改变了输入像素对输出的贡献方式，它们也改变了梯度从输出流回输入的路径。

让我们以一维卷积为例，考虑在边界输入像素 $x[0]$ 上的梯度 $\frac{\partial \mathcal{L}}{\partial x[0]}$ 是如何计算的。根据[链式法则](@entry_id:190743)，这个梯度是所有通过 $x[0]$ 影响损失 $\mathcal{L}$ 的路径的贡献之和。

在**[零填充](@entry_id:637925)**下，$x[0]$ 只影响其在原始输入中的自身位置。因此，梯度只通过这个单一路径回传。

然而，在**反射填充**下（例如，$p=1$ 时，$\tilde{x}[-1] = x[0]$），输入像素 $x[0]$ 不仅决定了填充后序列中的 $\tilde{x}[0]$，还决定了填充位置 $\tilde{x}[-1]$ 的值。这意味着 $x[0]$ 通过两个位置影响了输出，因此在[反向传播](@entry_id:199535)中，梯度会从两条路径汇集到 $x[0]$ 上。

通过严谨的推导，可以量化这种差异。对于一个特定的设置 ($N=5, K=3, p=1, s=2$)，在两种填充方案下，$x[0]$ 的梯度之差为 ：
$$
\Delta = \left(\frac{\partial \mathcal{L}}{\partial x[0]}\right)_{\text{reflect}} - \left(\frac{\partial \mathcal{L}}{\partial x[0]}\right)_{\text{zero}} = g[0] w[0]
$$
其中 $g[0]$ 是来自输出 $y[0]$ 的上游梯度，$w[0]$ 是卷积核的第一个权重。这个结果明确表明，反射填充为边界像素的梯度计算引入了一个额外的项，这直接改变了网络在边界区域的学习动态。

### 步幅的角色：降采样及其后果

步幅是卷积层中实现空间[降采样](@entry_id:265757)的主要机制。虽然它能有效减小[特征图尺寸](@entry_id:637663)并扩大后续层的[感受野](@entry_id:636171)，但这种降采样并非没有代价。

#### 卷积作为矩阵运算

为了更深刻地理解步幅的作用，我们可以将一维卷积表示为[矩阵乘法](@entry_id:156035)。一个卷积操作可以被形式化为一个**托普利兹矩阵（Toeplitz matrix）**与输入向量的乘积。在这个矩阵中，每一行对应一个输出位置，包含了经过适当移位的卷积核权重。

- **填充的影响**：填充操作增加了输入向量的维度（例如，从 $N$ 增加到 $N+2P$）。相应地，它增加了卷积托普利兹矩阵的**列数** 。
- **步幅的影响**：步幅 $s > 1$ 的作用可以被看作是对步幅为1的“全”托普利兹矩阵进行**行采样**。步幅为 $s$ 的卷积所生成的矩阵，其第 $r$ 行与步幅为1的矩阵的第 $r \times s$ 行完全相同。换句话说，步幅 $s$ 从一个更密集的[卷积算子](@entry_id:747865)中跳跃式地选择了行，从而实现了输出的[降采样](@entry_id:265757)。

#### 步幅与[等变性](@entry_id:636671)

一个理想的性质是**[平移等变性](@entry_id:636340)（shift equivariance）**，即当输入发生平移时，输出也以相同方式平移。步幅为1的卷积（在忽略边界效应时）满足这个性质。然而，当步幅 $s > 1$ 时，这个性质就被破坏了。

一个微小的、一个像素的输入平移，在经过带步幅的卷积后，可能会导致输出发生剧烈变化，或者完全没有变化，而不是一个平滑的、可预测的平移。这是因为步幅引入的[下采样](@entry_id:265757)使得网络对输入信号的精确位置变得不那么敏感，但也失去了细粒度的位置信息。

我们可以通过一个具体的例子来量化这种[等变性](@entry_id:636671)的丧失 。给定一个输入 $x$，我们计算其输出 $y$。然后，我们将输入右移一个像素得到 $x_T$，并计算其输出 $y_T$。对于一个步幅 $s=2$ 的卷积，我们通常会发现 $y_T$ 并非 $y$ 的任何简单移位版本。它们之间的欧氏距离 $\|y_T - y\|_2$ 将是一个显著的非零值，这个值具体地度量了在这次单像素平移下[等变性](@entry_id:636671)的破坏程度。

#### 步幅与混叠：信号处理的视角

从[数字信号处理](@entry_id:263660)（DSP）的角度来看，步幅 $s$ 的卷积等效于先进行一次密集的（步幅为1的）卷积，然后对结果进行**下采样（downsampling）**。[下采样](@entry_id:265757)操作在[频域](@entry_id:160070)中会引发一个众所周知的问题：**混叠（aliasing）**。

下采样的过程会将原始[信号频谱](@entry_id:198418)的高频部分“折叠”回低频部分，与原始的低频内容混合在一起，造成信息失真。根据离散时间的[奈奎斯特采样定理](@entry_id:268107)，为了在[下采样](@entry_id:265757)因子为 $s$ 的情况下不产生[混叠](@entry_id:146322)，输入到下采样器的信号的带宽必须被限制在 $\frac{\pi}{s}$ [弧度](@entry_id:171693)/采样点以内。

在卷积网络中，这意味着在进行步幅为 $s$ 的卷积之前，输入信号应该被有效地低通过滤。卷积核 $h[n]$ 本身应扮演这个**[抗混叠滤波器](@entry_id:636666)（anti-aliasing filter）**的角色。为了避免[混叠](@entry_id:146322)，其频率响应 $H(\omega)$ 的带宽应该被限制。理论上，为保证无混叠，理想低通核的[截止频率](@entry_id:276383) $\omega_c$ 必须满足 ：

$$
\omega_c \le \frac{\pi}{s}
$$

标准的CNN设计通常不直接强制此约束，这意味着许多标准的带[步幅卷积](@entry_id:637216)层实际上会引入混叠，这可能导致网络对高频纹理和微小[移位](@entry_id:145848)变得不稳定。这也催生了旨在通过显式设计[抗混叠滤波器](@entry_id:636666)来改进CNN鲁棒性的研究方向。

### [步幅与填充](@entry_id:635382)的相互作用：控制感受野

步幅和填充共同决定了输出网格与输入网格之间的空间映射关系，这直接影响着每个输出神经元的**感受野中心（receptive field center）**的位置。

#### 感受野中心的对齐与错位

对于一个给定的输出位置 $(i, j)$，其感受野中心在输入[坐标系](@entry_id:156346)中的位置 $(c_x(i), c_y(j))$ 可以被精确计算。对于水平轴，该中心位置为 ：

$$
c_{x}(i) = i s_{x} - p_{x} + \frac{k_{x} - 1}{2}
$$

其中 $i$ 是输出列索引，$s_x$ 是水平步幅，$p_x$ 是左侧填充，$k_x$ 是核宽度。此公式揭示了一个关键点：对齐与否取决于核的尺寸。

- **奇数尺寸核**：如果 $k_x$ 是奇数，那么 $\frac{k_x-1}{2}$ 是一个整数。这意味着，通过恰当选择填充 $p_x$，我们可以使感受野中心 $c_x(i)$ 成为一个整数，从而完美地对齐到输入像素的中心。
- **偶数尺寸核**：如果 $k_x$ 是偶数，那么 $\frac{k_x-1}{2}$ 是一个半整数（如 $0.5, 1.5$）。这导致 $c_x(i)$ 永远是一个半整数坐标，意味着[感受野](@entry_id:636171)中心将总是落在输入像素网格的**中间**，而不是任何一个像素的中心。例如，对于 $k_x=4, s_x=2, p_x=1$ 的配置，输出 $i=3$ 的[感受野](@entry_id:636171)中心位于 $c_x(3) = 3 \times 2 - 1 + \frac{4-1}{2} = 6.5$，即输入像素6和7的正中间。这种固有的**错位**是使用偶数尺寸核的一个重要理论与实践后果 。

#### 跨层对齐的设计

在深度网络中，我们常常希望保持特征图网格的中心对齐。这可以通过精心设计填充来实现。对于奇数尺寸的核 $k_l$，如果我们选择填充 $p_l = \frac{k_l-1}{2}$，那么[感受野](@entry_id:636171)中心的计算公式会得到极大简化。这种选择被称为“SAME”填充（在某些框架中），因为它通常（但不总是，取决于步幅）能使输出尺寸与输入尺寸相同。

在这种“中心对齐”的填充策略下，第 $l$ 层输出 $m$ 的[感受野](@entry_id:636171)中心 $P_l(m)$ 与前一层输出 $m \times s_l$ 的[感受野](@entry_id:636171)中心重合。将此递归地展开，我们可以得到第 $L$ 层输出 $m$ 的感受野中心相对于原始输入坐标的位置 ：

$$
P_L(m) = m \times s_1 \times s_2 \times \dots \times s_L
$$

这揭示了一个优美的关系：通过正确的填充，多层卷积的复合效应在[感受野](@entry_id:636171)中心映射上表现为一个纯粹的尺度变换，由各层步幅的乘积决定。例如，对于一个三层网络，其步幅分别为 $s_1=2, s_2=3, s_3=2$，通过设置 $p_1=\frac{5-1}{2}=2$, $p_2=\frac{3-1}{2}=1$, $p_3=\frac{7-1}{2}=3$ 来保持中心对齐后，最终层输出索引为 $m=7$ 的神经元，其[感受野](@entry_id:636171)中心位于原始输入坐标 $7 \times 2 \times 3 \times 2 = 84$ 处。

#### 覆盖范围的不均匀性

最后，值得注意的是，由于[卷积核](@entry_id:635097)的滑动方式，输入中的每个像素对输出的贡献是不均匀的。我们可以定义一个像素的**覆盖计数（coverage count）** $C(i)$，即包含该像素的输出[感受野](@entry_id:636171)的数量。

分析表明，位于输入[特征图](@entry_id:637719)中心的像素会被更多的卷积窗口覆盖，而靠近边缘的像素则被覆盖得较少。例如，在一个 $N=10, K=5, S=2, P=1$ 的一维卷积中，中心附近的像素 $i=5$ 的覆盖计数为 $C(5)=3$，而边缘附近的像素 $i=0$ 的覆盖计数仅为 $C(0)=1$ 。

这种不均匀性具有重要的实践意义。在训练过程中，如果假设来自每个输出位置的梯度在统计上是均匀的，那么一个像素接收到的总梯度更新量将与其覆盖计数成正比。这意味着中心像素比边缘像素获得更强、更频繁的梯度信号，可能导致网络学习到对中心特征更敏感，而对边缘[特征学习](@entry_id:749268)不足。这是设计网络时需要考虑的一个微妙但重要的空间偏见。