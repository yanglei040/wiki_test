{
    "hands_on_practices": [
        {
            "introduction": "在设计卷积神经网络时，精确控制特征图的尺寸是一项核心技能。本练习将引导您从第一性原理出发，推导输入尺寸、卷积核尺寸、步幅和填充之间的数学关系，以达到预期的输出尺寸 。通过这个过程，您将不再仅仅是记忆公式，而是真正理解其背后的工作原理。",
            "id": "3177669",
            "problem": "深度学习中的一维（1D）离散卷积层接受一个长度为 $n$ 的输入，应用一个宽度为 $k$ 的卷积核，使用步长 $s \\in \\mathbb{N}$，以及在左侧和右侧各进行 $p \\in \\mathbb{Z}_{\\ge 0}$ 个元素的对称零填充。假设采用标准的、带步长的连续采样（无空洞卷积），并且只有当整个卷积核窗口位于填充后的输入内部时，才会产生一个输出。\n\n仅从步长下有效卷积核位置的索引定义出发，推导关于 $p$ 的约束条件，以保证输出长度恰好为预设的目标 $m \\in \\mathbb{N}$。将这些约束用包含 $n$、$k$、$s$、$p$ 和 $m$ 的不等式明确表示出来，然后解这些不等式以得到关于 $p$ 的一个封闭形式的描述（包含整数边界），从而表征所有可行的 $p$ 值。讨论当 $s>1$ 时的可行性细微之处，包括 $2p$ 中隐含的奇偶性，并解释最后一个与步长对齐的卷积核位置恰好落在填充后输入末端的边界情况。\n\n然后，对于具体配置 $n=37$、$k=7$、$s=3$ 和目标 $m=12$，确定是否存在一个可行的非负整数 $p$，如果存在，计算出最小的那个 $p$。如果不存在可行的 $p$，请说明无解。你最终报告的答案必须是最小的单个整数 $p$（此整数值不需要单位，也无需四舍五入）。",
            "solution": "该问题要求推导在一维卷积中为达到特定输出长度 $m$ 而对对称零填充 $p$ 施加的约束条件，然后将此推导应用于一个具体的数值案例。\n\n首先，我们根据给定的条件来形式化卷积操作。\n长度为 $n$ 的输入在两侧各填充 $p$ 个零。得到的填充后输入的总长度为 $n_{padded} = n + 2p$。我们对这个填充后序列的元素使用从0开始的索引，即从索引 $0$ 到 $n+2p-1$。\n\n一个宽度为 $k$ 的卷积核被应用于这个填充后的输入。通过将卷积核置于不同位置来产生输出。步长 $s$ 决定了连续卷积核位置之间的步进大小。设输出元素由 $j$ 索引，其中 $j \\in \\{0, 1, 2, \\ldots, m-1\\}$，因为目标输出长度为 $m$。\n\n对于第 $j$ 个输出（其中 $j=0$ 对应第一个输出），卷积核的起始位置由 $j \\cdot s$ 给出。卷积核窗口则覆盖填充后输入中从索引 $j \\cdot s$ 到 $j \\cdot s + k - 1$ 的部分。\n\n问题陈述了只有当整个卷积核窗口位于填充后输入内部时才会产生输出。这对任何有效的起始位置 $i_{start}$ 施加了两个边界条件：\n1. 卷积核的起始位置 $i_{start}$ 必须在填充后输入的起始处或之后：$i_{start} \\ge 0$。\n2. 卷积核的结束位置 $i_{start} + k - 1$ 必须在填充后输入的结束处或之前：$i_{start} + k - 1 \\le n+2p-1$。\n\n第二个条件可以重写为 $i_{start} \\le n+2p-k$。\n由于起始位置由 $j \\cdot s$ 给出，其中 $j \\ge 0$ 且 $s \\in \\mathbb{N}$，第一个条件 $j \\cdot s \\ge 0$ 总是满足的。因此，对第 $j$ 个输出的有效位置的唯一约束是：\n$$ j \\cdot s \\le n+2p-k $$\n\n为了使输出长度恰好为 $m$，必须同时满足两个条件：\n1. 必须为索引 $j=m-1$ 产生一个输出。这意味着第 $m$ 个输出的位置是有效的。\n   $$ (m-1)s \\le n+2p-k $$\n2. 必须*不*为索引 $j=m$ 产生输出。这意味着一个假设的第 $(m+1)$ 个输出的位置是无效的。\n   $$ m \\cdot s > n+2p-k $$\n\n将这两个不等式合并，得到一个复合不等式，它约束了可用于卷积的特征图的有效长度 $n+2p-k$：\n$$ (m-1)s \\le n+2p-k  ms $$\n\n为了找到对 $p$ 的约束，我们解这个关于 $p$ 的复合不等式。\n从左侧不等式：\n$(m-1)s \\le n+2p-k \\implies (m-1)s - n + k \\le 2p \\implies p \\ge \\frac{(m-1)s - n + k}{2}$。\n从右侧不等式：\n$n+2p-k  ms \\implies 2p  ms - n + k \\implies p  \\frac{ms - n + k}{2}$。\n\n因此，所有可行 $p$ 值的封闭形式描述由以下公式给出：\n$$ \\frac{(m-1)s - n + k}{2} \\le p  \\frac{ms - n + k}{2} $$\n由于 $p$ 必须是一个非负整数（$p \\in \\mathbb{Z}_{\\ge 0}$），一个有效的解仅当区间 $[\\frac{(m-1)s - n + k}{2}, \\frac{ms - n + k}{2})$ 包含至少一个非负整数时才存在。\n\n现在，我们讨论当 $s1$ 时的可行性细微之处。\n我们来分析 $2p$ 的区间：\n$$ (m-1)s - n + k \\le 2p  ms - n + k $$\n设 $A = (m-1)s - n + k$。条件是 $A \\le 2p  A+s$。我们正在寻找区间 $[A, A+s)$ 内的一个非负偶数 $2p$。这个区间的长度是 $s$。\n如果 $s=1$，区间是 $[A, A+1)$，它只包含一个整数 $A$。要存在解，$A$ 必须是一个非负偶数。这是一个非常严格的条件。\n如果 $s1$，区间 $[A, A+s)$ 的长度为 $s \\ge 2$。对于任何整数 $A$，区间 $[A, A+s)$ 保证至少包含一个偶数。例如，如果 $A$ 是偶数，那么 $A$ 本身就是一个候选值。如果 $A$ 是奇数，那么 $A+1$ 是偶数，并且由于 $s \\ge 2$，有 $A+1  A+s$，所以 $A+1$ 在区间内。\n因此，对于 $s  1$，总能保证存在一个满足边界条件的整数 $p$。唯一剩下的条件是 $p$ 必须是非负的。这意味着区间 $[A, A+s)$ 必须包含至少一个非负偶数。只要 $2p$ 的区间的上界是正的，即 $A+s > 0$，也就是 $ms - n + k > 0$，这个条件就能得到保证。\n\n$2p$ 为偶数的奇偶性已由公式隐式处理。项 $2p$ 必须是一个偶数。表达式 $(m-1)s - n + k$ 可以是偶数也可以是奇数。如果它是偶数，那么边界情况是可能发生的。\n\n最后一个与步长对齐的卷积核位置恰好落在填充后输入末端的边界情况，发生在不等式 $(m-1)s \\le n+2p-k$ 取等号时：\n$$ (m-1)s = n+2p-k $$\n解出 $p$，我们得到 $p = \\frac{(m-1)s - n + k}{2}$。为了使这种精确对齐成为可能，$p$ 的值必须是一个非负整数。这要求分子 $(m-1)s - n + k$ 是一个非负偶数。\n\n最后，我们将这些结果应用于具体配置：$n=37$、$k=7$、$s=3$ 和目标 $m=12$。\n我们需要找到满足所推导不等式的最小非负整数 $p$。\n将给定值代入 $p$ 的区间：\n下界：\n$$ p \\ge \\frac{(12-1) \\cdot 3 - 37 + 7}{2} = \\frac{11 \\cdot 3 - 37 + 7}{2} = \\frac{33 - 37 + 7}{2} = \\frac{-4 + 7}{2} = \\frac{3}{2} = 1.5 $$\n上界：\n$$ p  \\frac{12 \\cdot 3 - 37 + 7}{2} = \\frac{36 - 37 + 7}{2} = \\frac{-1 + 7}{2} = \\frac{6}{2} = 3 $$\n所以，对 $p$ 的条件是：\n$$ 1.5 \\le p  3 $$\n我们在寻找满足此条件的最小整数 $p$。半开区间 $[1.5, 3)$ 中的整数是 $\\{2\\}$。唯一的整数解是 $p=2$。由于 $p=2$ 是一个非负整数，它是一个可行的填充值。由于它是有效范围内的唯一整数解，因此它也是最小的。\n\n因此，对于给定的配置，存在一个可行的非负整数 $p$，其最小值为 $2$。",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "在实践中，我们有两种常见的方式来实现下采样：直接使用大步幅卷积，或者先进行步幅为1的卷积再进行子采样。这个动手编程练习  将揭示这两种计算图之间的细微差别，特别是当使用“same”填充时，它们可能会产生不同的结果。通过量化这种差异，您将更深刻地理解深度学习框架中填充和步幅交互的实现细节。",
            "id": "3177651",
            "problem": "你需要比较两个实现深度学习中离散一维互相关（cross-correlation）的计算图，并量化由步幅（stride）和填充（padding）对齐方式引起的数值差异。比较的重点在于“步幅为 $s$ 的卷积”与“步幅为 $1$ 的卷积后接步长为 $s$ 的下采样”之间的区别。\n\n使用的定义和规则：\n- 令 $x \\in \\mathbb{R}^n$ 为一维输入信号，$w \\in \\mathbb{R}^k$ 为一维核。在位置 $j$ 的离散互相关（深度学习框架中通常以“卷积”之名实现的操作）定义为\n  $$y[j] = \\sum_{t=0}^{k-1} x_{\\text{pad}}[j + t] \\, w[t],$$\n  其中 $x_{\\text{pad}}$ 表示在 $x$ 的左侧和右侧进行零填充（zero-padding）后的结果。\n- 填充模式：\n  1. \"valid\"：无零填充，即左右填充长度均为 $0$。\n  2. \"same\"：在两侧选择零填充，使得以步幅 $s$ 获取的不同感受野的数量等于不小于输入长度与步幅之比的最小整数。将总填充量分为左填充和右填充，其中左填充等于不大于总填充量一半的最大整数。具体来说，当步幅等于 $1$ 时，\"same\" 模式使用总计 $k-1$ 的填充量，并将其拆分为左填充和右填充，使得左填充等于不大于 $k-1$ 一半的最大整数。\n\n待比较的计算图：\n- 图 A（“步幅为 $s$ 的卷积”）：根据所选模式（如上所述）进行填充，然后通过在填充后的输入上以步长 $s$ 滑动窗口来计算互相关，只取完全包含在填充信号内的窗口。\n- 图 B（“卷积后下采样 $s$”）：根据所选模式进行填充，但步幅视为 $1$。在所有完全包含的窗口上计算步幅为 $1$ 的互相关，然后通过从索引 $0$ 开始每 $s$ 个元素取一个来对结果序列进行下采样。\n\n输入和核的合成（确定性）：\n- 对于给定的 $n$，定义 $x[i] = \\sin(0.2 \\, i) + 0.1 \\, i$，其中 $i = 0, 1, \\dots, n-1$。\n- 对于给定的 $k$，定义 $w[t] = \\frac{t+1}{k}$，其中 $t = 0, 1, \\dots, k-1$。\n\n度量：\n- 对于每个测试用例，从图 A 生成 $y_A$，从图 B 生成 $y_B$。\n- 令 $m = \\min\\{|y_A|, |y_B|\\}$。定义比较长度为 $m$。如果 $m = 0$，按惯例将差异设为 $0$。\n- 计算最大绝对差\n  $$d_{\\max} = \\max_{0 \\le i  m} \\left| y_A[i] - y_B[i] \\right|.$$\n\n你的程序必须严格按照上述规则实现两个计算图，并报告每个测试用例的 $d_{\\max}$。\n\n测试套件：\n- 用例 1 (通用 \"valid\")：$n = 16$，$k = 3$，$s = 2$，模式 $\\text{valid}$。\n- 用例 2 (通用 \"same\"，奇数核)：$n = 17$，$k = 5$，$s = 2$，模式 $\\text{same}$。\n- 用例 3 (边界情况，小输入大步幅)：$n = 3$，$k = 4$，$s = 3$，模式 $\\text{same}$。\n- 用例 4 (边缘情况，偶数核不对称)：$n = 10$，$k = 2$，$s = 3$，模式 $\\text{same}$。\n- 用例 5 (边缘情况，\"valid\" 模式下步幅大于核)：$n = 8$，$k = 3$，$s = 4$，模式 $\\text{valid}$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按上述用例的顺序排列结果，例如 $[d_1,d_2,d_3,d_4,d_5]$，其中每个 $d_i$ 是第 $i$ 个测试用例的 $d_{\\max}$ 的浮点数值。",
            "solution": "该问题要求对一维互相关的两个计算图进行定量比较，这两个计算图在步幅和填充的实现上有所不同。问题的核心在于直接带步幅的卷积（图 A）和单位步幅卷积后接下采样（图 B）之间的区别。数值差异源于填充的计算方式，两个图的定义不同。\n\n基本原理是，两个计算图（图 A 和图 B）当且仅当它们各自填充后的输入信号相同时，才会产生相同的输出。这是因为核心操作——核与信号窗口之间的点积——在两个图中都应用于相同的起始位置（由步幅 $s$ 的倍数索引）。因此，任何差异必定源于应用于原始信号 $x$ 的填充。\n\n设输入信号为 $x \\in \\mathbb{R}^n$，核为 $w \\in \\mathbb{R}^k$。步幅为 $s$。\n\n**图 A（“步幅为 $s$ 的卷积”）分析**\n\n图 A 首先确定实现目标输出维度所需的填充，然后执行带步幅的互相关。\n1.  **填充计算**：\n    - 对于 `mode='valid'`，两侧的填充均为 $0$。总填充量 $P_A = 0$。\n    - 对于 `mode='same'`，输出长度 $o_A$ 定义为 $\\lceil n/s \\rceil$。实现此输出长度所需的总填充量 $P_A$ 由标准公式给出：$P_A = \\max(0, (o_A-1)s + k - n)$。总填充量被分为左填充 $p_{l,A} = \\lfloor P_A/2 \\rfloor$ 和右填充 $p_{r,A} = P_A - p_{l,A}$。\n2.  **填充后的信号**：输入 $x$ 的左侧填充 $p_{l,A}$ 个零，右侧填充 $p_{r,A}$ 个零，形成 $x_{pad,A}$。\n3.  **互相关**：输出 $y_A$ 是通过在 $x_{pad,A}$ 上以步长 $s$ 滑动核 $w$ 来计算的。输出的第 $j$ 个元素（其中 $j=0, 1, \\dots, o_A-1$）为：\n    $$y_A[j] = \\sum_{t=0}^{k-1} x_{pad,A}[j \\cdot s + t] \\, w[t]$$\n\n**图 B（“卷积后下采样 $s$”）分析**\n\n图 B 执行单位步幅卷积，然后对结果进行下采样。填充的计算方式假定步幅始终为 $1$。\n1.  **填充计算**：\n    - 对于 `mode='valid'`，两侧填充均为 $0$。总填充量 $P_B = 0$。\n    - 对于 `mode='same'`，问题规定填充是为步幅 $s=1$ 计算的。这得出总填充量 $P_B = k-1$。它被分为左填充 $p_{l,B} = \\lfloor P_B/2 \\rfloor = \\lfloor (k-1)/2 \\rfloor$ 和右填充 $p_{r,B} = P_B - p_{l,B}$。\n2.  **填充后的信号**：输入 $x$ 经过填充形成 $x_{pad,B}$。\n3.  **互相关与下采样**：首先，使用步幅 $1$ 计算一个中间信号 $y_{int,B}$：\n    $$y_{int,B}[j'] = \\sum_{t=0}^{k-1} x_{pad,B}[j' + t] \\, w[t]$$\n    最终输出 $y_B$ 通过对 $y_{int,B}$ 进行下采样获得，即从索引 $0$ 开始，每 $s$ 个元素取一个：$y_B[j] = y_{int,B}[j \\cdot s]$。代入 $y_{int,B}$ 的表达式：\n    $$y_B[j] = \\sum_{t=0}^{k-1} x_{pad,B}[j \\cdot s + t] \\, w[t]$$\n\n**等效条件与差异来源**\n\n比较 $y_A[j]$ 和 $y_B[j]$ 的最终表达式，很明显，$y_A$ 和 $y_B$ 相等当且仅当它们底层的填充信号 $x_{pad,A}$ 和 $x_{pad,B}$ 相等（假设它们的长度兼容，事实也如此）。这等价于总填充量相等（$P_A = P_B$）且以相同方式拆分（根据左侧填充的 $\\lfloor P/2 \\rfloor$ 规则，事实确实如此）。\n\n-   对于 `mode='valid'`：$P_A = 0$ 且 $P_B = 0$。两个图总是等效的，差异 $d_{\\max}$ 必须为 $0$。\n\n-   对于 `mode='same'`：我们必须比较 $P_A = \\max(0, (\\lceil n/s \\rceil-1)s+k-n)$ 和 $P_B = k-1$。如果 $(\\lceil n/s \\rceil-1)s+k-n = k-1$（假设该项非负），则两者相等。这可以简化为：\n    $$(\\lceil n/s \\rceil-1)s = n-1$$\n    我们来分析这个条件。令 $n = q \\cdot s + r$，其中 $q$ 是商，$r$ 是余数（$0 \\le r  s$）。\n    - 如果 $r=0$（即 $n$ 是 $s$ 的倍数），则 $\\lceil n/s \\rceil=q$。条件变为 $(q-1)s = n-1 \\implies qs-s=n-1 \\implies n-s=n-1 \\implies s=1$。因此，对于 $s  1$，该条件不成立。\n    - 如果 $r0$，则 $\\lceil n/s \\rceil=q+1$。条件变为 $(q+1-1)s = n-1 \\implies qs=n-1 \\implies qs = (qs+r)-1 \\implies r-1=0 \\implies r=1$。\n    因此，对于 `mode='same'` 和 $s1$，两个图等效当且仅当 $n \\pmod s = 1$。如果 $n \\pmod s \\neq 1$，填充后的信号将不同，导致 $y_A \\neq y_B$ 和可能非零的差异 $d_{\\max}$。\n\n**应用于测试用例**\n\n-   **用例 1**：$n=16, k=3, s=2$，`mode=valid`。$P_A=P_B=0$。预期 $d_{\\max}=0$。\n-   **用例 2**：$n=17, k=5, s=2$，`mode=same`。这里，$17 \\pmod 2 = 1$。等效条件成立。预期 $d_{\\max}=0$。\n-   **用例 3**：$n=3, k=4, s=3$，`mode=same`。这里，$3 \\pmod 3 = 0$。由于 $s  1$，等效条件不成立。填充量将不同：\n    - $P_A = \\max(0, (\\lceil 3/3 \\rceil - 1)3 + 4 - 3) = \\max(0, 0+1) = 1$。\n    - $P_B = k-1 = 3$。\n    预期 $d_{\\max}$ 非零。\n-   **用例 4**：$n=10, k=2, s=3$，`mode=same`。这里，$10 \\pmod 3 = 1$。等效条件成立。预期 $d_{\\max}=0$。\n-   **用例 5**：$n=8, k=3, s=4$，`mode=valid`。$P_A=P_B=0$。预期 $d_{\\max}=0$。\n\n实现将严格按照这些规则构建两个图，并计算指定的最大绝对差。",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes the numerical difference between two implementations of 1D cross-correlation\n    for a given set of test cases.\n\n    The two implementations are:\n    - Graph A: Direct strided convolution.\n    - Graph B: Unit-stride convolution followed by subsampling.\n\n    The difference arises from how padding is calculated in 'same' mode, which is\n    stride-dependent in Graph A but stride-agnostic (fixed to s=1) in Graph B.\n    \"\"\"\n\n    test_cases = [\n        # (n, k, s, mode)\n        (16, 3, 2, 'valid'),\n        (17, 5, 2, 'same'),\n        (3, 4, 3, 'same'),\n        (10, 2, 3, 'same'),\n        (8, 3, 4, 'valid'),\n    ]\n\n    results = []\n\n    def cross_correlate(x_padded: np.ndarray, w: np.ndarray, stride: int) - np.ndarray:\n        \"\"\"\n        Computes 1D cross-correlation with a given stride on a pre-padded signal.\n        \"\"\"\n        k = len(w)\n        n_padded = len(x_padded)\n        if n_padded  k:\n            return np.array([])\n        output_len = math.floor((n_padded - k) / stride) + 1\n        if output_len = 0:\n            return np.array([])\n        \n        y = np.zeros(output_len)\n        for j in range(output_len):\n            start_index = j * stride\n            window = x_padded[start_index : start_index + k]\n            y[j] = np.dot(window, w)\n        return y\n\n    for n, k, s, mode in test_cases:\n        # Step 1: Synthesize input signal x and kernel w\n        i_vals = np.arange(n)\n        x = np.sin(0.2 * i_vals) + 0.1 * i_vals\n\n        t_vals = np.arange(k)\n        w = (t_vals + 1) / k\n\n        # Step 2: Implement and compute y_A for Graph A\n        if mode == 'valid':\n            p_left_A = 0\n            p_right_A = 0\n        elif mode == 'same':\n            out_len_A = math.ceil(n / s)\n            total_pad_A = max(0, (out_len_A - 1) * s + k - n)\n            p_left_A = math.floor(total_pad_A / 2)\n            p_right_A = total_pad_A - p_left_A\n        else:\n            raise ValueError(\"Invalid mode\")\n\n        x_padded_A = np.pad(x, (p_left_A, p_right_A), 'constant', constant_values=0)\n        y_A = cross_correlate(x_padded_A, w, s)\n\n        # Step 3: Implement and compute y_B for Graph B\n        if mode == 'valid':\n            p_left_B = 0\n            p_right_B = 0\n        elif mode == 'same':\n            # Padding is calculated as if stride were 1\n            total_pad_B = k - 1\n            p_left_B = math.floor(total_pad_B / 2)\n            p_right_B = total_pad_B - p_left_B\n        else:\n            raise ValueError(\"Invalid mode\")\n\n        x_padded_B = np.pad(x, (p_left_B, p_right_B), 'constant', constant_values=0)\n        y_intermediate_B = cross_correlate(x_padded_B, w, 1)\n        y_B = y_intermediate_B[::s]\n\n        # Step 4: Compute the maximum absolute difference d_max\n        m = min(len(y_A), len(y_B))\n        if m == 0:\n            d_max = 0.0\n        else:\n            diff = np.abs(y_A[:m] - y_B[:m])\n            d_max = np.max(diff)\n\n        results.append(d_max)\n\n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在像 U-Net 这样的高级网络架构中，确保来自不同路径的特征在跳跃连接处空间对齐至关重要。本练习  探讨了卷积核尺寸和填充的微小选择如何通过网络传播，最终影响特征的空间坐标。通过一个简化的 U-Net 模型，您将学习如何量化由偶数尺寸卷积核引起的像素级错位，从而掌握诊断和解决这类架构问题的关键技能。",
            "id": "3180119",
            "problem": "考虑一个二维（2D）U 型卷积神经网络（U-Net），该网络执行一个下采样阶段和一个上采样阶段。该网络使用二维离散卷积的互相关形式，其中对于输入特征图 $X$ 和大小为 $k_{x} \\times k_{y}$ 的卷积核 $W$，输出特征图 $Y$（沿每个空间轴独立计算）由下式给出：\n$$\nY[i,j] \\;=\\; \\sum_{u=0}^{k_{x}-1} \\sum_{v=0}^{k_{y}-1} W[u,v] \\, X[s_{x} i - p_{x} + u, \\, s_{y} j - p_{y} + v].\n$$\n假设使用以下约定和参数：\n- 对于标准卷积，沿每个轴的步幅为 $s_{x} = s_{y} = 1$。\n- 填充实现了常用的“same”规则 $p_{x} = \\left\\lfloor \\frac{k_{x}}{2} \\right\\rfloor$ 和 $p_{y} = \\left\\lfloor \\frac{k_{y}}{2} \\right\\rfloor$。\n- 沿每个轴的卷积核离散中心的索引为 $c_{x} = \\left\\lfloor \\frac{k_{x}-1}{2} \\right\\rfloor$ 和 $c_{y} = \\left\\lfloor \\frac{k_{y}-1}{2} \\right\\rfloor$。\n- 二维最大池化使用大小为 $2 \\times 2$ 的窗口、步幅为 2、以及沿两个轴的零填充。将池化输出的代表性位置视为由上面定义的 $(c_{x}, c_{y})$ 决定的窗口中心。\n- 卷积核大小为 $2 \\times 2$、沿每个轴的步幅为 2 的二维转置卷积（分数步幅卷积），通过标准的深度学习定义实现，该定义等效于在输入之间插入零，然后使用给定的卷积核应用互相关；其填充为 $p_{x} = p_{y} = 0$，其中心索引为 $c_{x} = c_{y} = \\left\\lfloor \\frac{2-1}{2} \\right\\rfloor$。\n\n通过一对参数 $(S_{x}, S_{y}, \\Delta_{x}, \\Delta_{y})$，定义一个从任何中间特征图的索引 $(i,j)$ 回到输入图像坐标系的坐标映射，其中 $S_{x}$ 和 $S_{y}$ 是累积步幅，$(\\Delta_{x}, \\Delta_{y})$ 是以输入像素为单位的累积偏移量。对于一个步幅为 $(s_{x}, s_{y})$、填充为 $(p_{x}, p_{y})$、中心索引为 $(c_{x}, c_{y})$ 的标准卷积层或池化层，若其应用于一个具有映射 $(S_{x}^{\\mathrm{in}}, S_{y}^{\\mathrm{in}}, \\Delta_{x}^{\\mathrm{in}}, \\Delta_{y}^{\\mathrm{in}})$ 的输入，则输出映射为：\n$$\nS_{x}^{\\mathrm{out}} \\;=\\; S_{x}^{\\mathrm{in}} \\, s_{x}, \\quad S_{y}^{\\mathrm{out}} \\;=\\; S_{y}^{\\mathrm{in}} \\, s_{y}, \\quad\n\\Delta_{x}^{\\mathrm{out}} \\;=\\; \\Delta_{x}^{\\mathrm{in}} + S_{x}^{\\mathrm{in}} (c_{x} - p_{x}), \\quad\n\\Delta_{y}^{\\mathrm{out}} \\;=\\; \\Delta_{y}^{\\mathrm{in}} + S_{y}^{\\mathrm{in}} (c_{y} - p_{y}).\n$$\n对于一个步幅为 $(s_{x}, s_{y})$、填充为 $(p_{x}, p_{y})$、中心索引为 $(c_{x}, c_{y})$ 的转置卷积，若其应用于一个具有映射 $(S_{x}^{\\mathrm{in}}, S_{y}^{\\mathrm{in}}, \\Delta_{x}^{\\mathrm{in}}, \\Delta_{y}^{\\mathrm{in}})$ 的输入，则输出映射为：\n$$\nS_{x}^{\\mathrm{out}} \\;=\\; \\frac{S_{x}^{\\mathrm{in}}}{s_{x}}, \\quad S_{y}^{\\mathrm{out}} \\;=\\; \\frac{S_{y}^{\\mathrm{in}}}{s_{y}}, \\quad\n\\Delta_{x}^{\\mathrm{out}} \\;=\\; \\Delta_{x}^{\\mathrm{in}} + \\frac{S_{x}^{\\mathrm{in}}}{s_{x}} (p_{x} - c_{x}), \\quad\n\\Delta_{y}^{\\mathrm{out}} \\;=\\; \\Delta_{y}^{\\mathrm{in}} + \\frac{S_{y}^{\\mathrm{in}}}{s_{y}} (p_{y} - c_{y}).\n$$\n\n考虑以下最小 U-Net 路径：\n1. 编码器阶段：一个使用“same”填充的 $3 \\times 3$ 标准卷积，随后是一个步幅为 2、零填充的 $2 \\times 2$ 最大池化。\n2. 解码器阶段：一个步幅为 2、零填充的 $2 \\times 2$ 转置卷积，随后是一个使用“same”填充的标准卷积，其卷积核大小为 $3 \\times 3$（奇数）或 $2 \\times 2$（偶数）。\n\n一个跳跃连接将编码器在第一个 $3 \\times 3$ 卷积之后的特征图与解码器在最后一个标准卷积之后的特征图进行拼接。使用上述坐标映射规则，推导编码器特征和解码器特征的累积 $(S_{x}, S_{y}, \\Delta_{x}, \\Delta_{y})$，并用它们计算当解码器的最后一个标准卷积使用一个带“same”填充的 $2 \\times 2$ 卷积核时，在跳跃连接处这两个特征之间的空间错位向量。然后，报告此错位向量的欧几里得范数，该范数以跳跃连接分辨率下的像素为单位进行测量，并以一个单一实数值的形式给出。如果没有产生错位，则范数为零。只提供该范数作为你的最终答案。",
            "solution": "用户想要确定在一个简化的 U-Net 架构中，跳跃连接处特征之间的空间错位。问题提供了一个通过不同神经网络层来追踪坐标系的形式化定义。我将通过计算跳跃连接中涉及的每个特征的坐标映射，然后找出它们对应空间位置之差的范数来解决这个问题。\n\n一个特征图的坐标映射由一个元组 $(S_{x}, S_{y}, \\Delta_{x}, \\Delta_{y})$ 描述，其中 $(S_x, S_y)$ 是累积步幅，$(\\Delta_x, \\Delta_y)$ 是相对于输入图像坐标系的累积偏移量。特征图中索引为 $(i,j)$ 的一个点对应于输入图像坐标系中的点 $(i \\cdot S_x + \\Delta_x, j \\cdot S_y + \\Delta_y)$。\n\n输入图像本身具有一个单位映射，我们可以将其表示为第 0 层。\n初始映射 (第 0 层)：$(S_x^{(0)}, S_y^{(0)}, \\Delta_x^{(0)}, \\Delta_y^{(0)}) = (1, 1, 0, 0)$。\n\n问题描述了一个跳跃连接，它将编码器的特征图与解码器的特征图进行拼接。我们必须找出这两个特征图各自的坐标映射。\n\n**1. 用于跳跃连接的编码器特征图（E1 层）**\n\n该特征图是编码器中第一个操作的输出：一个使用“same”填充的 $3 \\times 3$ 标准卷积。\n- 输入映射： $(S_x^{(0)}, S_y^{(0)}, \\Delta_x^{(0)}, \\Delta_y^{(0)}) = (1, 1, 0, 0)$。\n- 操作：标准卷积。\n- 参数：\n  - 卷积核大小： $k_x = 3, k_y = 3$。\n  - 步幅： $s_x = 1, s_y = 1$。\n  - 填充 (\"same\")： $p_x = \\lfloor \\frac{3}{2} \\rfloor = 1$, $p_y = \\lfloor \\frac{3}{2} \\rfloor = 1$。\n  - 卷积核中心： $c_x = \\lfloor \\frac{3-1}{2} \\rfloor = 1$, $c_y = \\lfloor \\frac{3-1}{2} \\rfloor = 1$。\n\n我们应用给定的标准卷积更新规则：\n$S_{x}^{\\mathrm{out}} = S_{x}^{\\mathrm{in}} \\, s_{x}$\n$\\Delta_{x}^{\\mathrm{out}} = \\Delta_{x}^{\\mathrm{in}} + S_{x}^{\\mathrm{in}} (c_{x} - p_{x})$\n（y 维度类似）。\n\n- E1 的累积步幅：\n$S_x^{(E1)} = S_x^{(0)} \\cdot s_x = 1 \\cdot 1 = 1$\n$S_y^{(E1)} = S_y^{(0)} \\cdot s_y = 1 \\cdot 1 = 1$\n\n- E1 的累积偏移量：\n$\\Delta_x^{(E1)} = \\Delta_x^{(0)} + S_x^{(0)} (c_x - p_x) = 0 + 1(1 - 1) = 0$\n$\\Delta_y^{(E1)} = \\Delta_y^{(0)} + S_y^{(0)} (c_y - p_y) = 0 + 1(1 - 1) = 0$\n\n在跳跃连接处的编码器特征的坐标映射为 $(S_x, S_y, \\Delta_x, \\Delta_y)_{E1} = (1, 1, 0, 0)$。\n\n**2. 用于跳跃连接的解码器特征图（D2 层）**\n\n该特征图是数据通过整个指定的 U-Net 路径后的结果。我们逐步追踪坐标映射的变换。\n\n**步骤 2a：最大池化（E2 层）**\n此操作在第一次卷积之后。其输入是特征图 E1。\n- 输入映射： $(S_x^{(E1)}, S_y^{(E1)}, \\Delta_x^{(E1)}, \\Delta_y^{(E1)}) = (1, 1, 0, 0)$。\n- 操作：最大池化，在坐标映射中视为标准卷积。\n- 参数：\n  - 窗口大小： $k_x = 2, k_y = 2$。\n  - 步幅： $s_x = 2, s_y = 2$。\n  - 填充： $p_x = 0, p_y = 0$。\n  - 窗口中心： $c_x = \\lfloor \\frac{2-1}{2} \\rfloor = 0$, $c_y = \\lfloor \\frac{2-1}{2} \\rfloor = 0$。\n\n- E2 的累积步幅：\n$S_x^{(E2)} = S_x^{(E1)} \\cdot s_x = 1 \\cdot 2 = 2$\n$S_y^{(E2)} = S_y^{(E1)} \\cdot s_y = 1 \\cdot 2 = 2$\n\n- E2 的累积偏移量：\n$\\Delta_x^{(E2)} = \\Delta_x^{(E1)} + S_x^{(E1)} (c_x - p_x) = 0 + 1(0 - 0) = 0$\n$\\Delta_y^{(E2)} = \\Delta_y^{(E1)} + S_y^{(E1)} (c_y - p_y) = 0 + 1(0 - 0) = 0$\n\n瓶颈特征图 (E2) 的坐标映射为 $(2, 2, 0, 0)$。\n\n**步骤 2b：转置卷积（D1 层）**\n这是解码器（上采样路径）中的第一个操作。其输入是瓶颈特征图 E2。\n- 输入映射： $(S_x^{(E2)}, S_y^{(E2)}, \\Delta_x^{(E2)}, \\Delta_y^{(E2)}) = (2, 2, 0, 0)$。\n- 操作：转置卷积。\n- 参数：\n  - 卷积核大小： $k_x = 2, k_y = 2$。\n  - 步幅： $s_x = 2, s_y = 2$。\n  - 填充： $p_x = 0, p_y = 0$。\n  - 卷积核中心： $c_x = \\lfloor \\frac{2-1}{2} \\rfloor = 0$, $c_y = \\lfloor \\frac{2-1}{2} \\rfloor = 0$。\n\n我们应用给定的转置卷积更新规则：\n$S_{x}^{\\mathrm{out}} = S_{x}^{\\mathrm{in}} / s_{x}$\n$\\Delta_{x}^{\\mathrm{out}} = \\Delta_{x}^{\\mathrm{in}} + \\frac{S_{x}^{\\mathrm{in}}}{s_{x}} (p_{x} - c_{x})$\n\n- D1 的累积步幅：\n$S_x^{(D1)} = S_x^{(E2)} / s_x = 2 / 2 = 1$\n$S_y^{(D1)} = S_y^{(E2)} / s_y = 2 / 2 = 1$\n\n- D1 的累积偏移量：\n$\\Delta_x^{(D1)} = \\Delta_x^{(E2)} + \\frac{S_x^{(E2)}}{s_x} (p_x - c_x) = 0 + \\frac{2}{2}(0 - 0) = 0$\n$\\Delta_y^{(D1)} = \\Delta_y^{(E2)} + \\frac{S_y^{(E2)}}{s_y} (p_y - c_y) = 0 + \\frac{2}{2}(0 - 0) = 0$\n\n上采样后 (D1) 的坐标映射为 $(1, 1, 0, 0)$。\n\n**步骤 2c：最终标准卷积（D2 层）**\n这是跳跃连接前的最后一个操作。其输入是上采样后的特征图 D1。问题指定使用 $2 \\times 2$ 的卷积核。\n- 输入映射： $(S_x^{(D1)}, S_y^{(D1)}, \\Delta_x^{(D1)}, \\Delta_y^{(D1)}) = (1, 1, 0, 0)$。\n- 操作：标准卷积。\n- 参数：\n  - 卷积核大小： $k_x = 2, k_y = 2$。\n  - 步幅： $s_x = 1, s_y = 1$。\n  - 填充 (\"same\")： $p_x = \\lfloor \\frac{2}{2} \\rfloor = 1$, $p_y = \\lfloor \\frac{2}{2} \\rfloor = 1$。\n  - 卷积核中心： $c_x = \\lfloor \\frac{2-1}{2} \\rfloor = 0$, $c_y = \\lfloor \\frac{2-1}{2} \\rfloor = 0$。\n\n- D2 的累积步幅：\n$S_x^{(D2)} = S_x^{(D1)} \\cdot s_x = 1 \\cdot 1 = 1$\n$S_y^{(D2)} = S_y^{(D1)} \\cdot s_y = 1 \\cdot 1 = 1$\n\n- D2 的累积偏移量：\n$\\Delta_x^{(D2)} = \\Delta_x^{(D1)} + S_x^{(D1)} (c_x - p_x) = 0 + 1(0 - 1) = -1$\n$\\Delta_y^{(D2)} = \\Delta_y^{(D1)} + S_y^{(D1)} (c_y - p_y) = 0 + 1(0 - 1) = -1$\n\n在跳跃连接处的解码器特征 (D2) 的坐标映射为 $(S_x, S_y, \\Delta_x, \\Delta_y)_{D2} = (1, 1, -1, -1)$。\n\n**3. 计算错位**\n\n我们现在拥有了在跳跃连接处被拼接的两个特征的坐标映射：\n- 编码器特征 E1： $(S_x, S_y, \\Delta_x, \\Delta_y)_{E1} = (1, 1, 0, 0)$。\n- 解码器特征 D2： $(S_x, S_y, \\Delta_x, \\Delta_y)_{D2} = (1, 1, -1, -1)$。\n\n我们考虑在跳跃连接处的特征图空间网格中索引为 $(i,j)$ 的一个点。\n- 编码器特征在输入图像中的对应位置是：\n$(x_E, y_E) = (i \\cdot S_{x,E1} + \\Delta_{x,E1}, j \\cdot S_{y,E1} + \\Delta_{y,E1}) = (i \\cdot 1 + 0, j \\cdot 1 + 0) = (i, j)$。\n- 解码器特征在输入图像中的对应位置是：\n$(x_D, y_D) = (i \\cdot S_{x,D2} + \\Delta_{x,D2}, j \\cdot S_{y,D2} + \\Delta_{y,D2}) = (i \\cdot 1 + (-1), j \\cdot 1 + (-1)) = (i-1, j-1)$。\n\n在输入图像坐标系中的空间错位向量是这两个位置之差：\n$\\vec{v} = (x_E - x_D, y_E - y_D) = (i - (i-1), j - (j-1)) = (1, 1)$。\n\n问题要求计算该向量的范数，“以跳跃连接分辨率下的像素为单位进行测量”。在跳跃连接处，两个特征的累积步幅均为 $S_x=1, S_y=1$。这意味着特征图中的一个像素步长对应于原始输入图像中的一个像素步长。因此，输入像素中的错位向量 $(1, 1)$ 也是特征图像素中的错位向量。\n\n该错位向量的欧几里得范数是：\n$|\\vec{v}| = \\sqrt{1^2 + 1^2} = \\sqrt{2}$。\n\n这种错位是由于使用偶数大小的卷积核 ($2 \\times 2$) 和“same”填充的不对称性引起的。选择卷积核中心 $c=0$ 并结合填充 $p=1$ 造成了位置偏移。",
            "answer": "$$\\boxed{\\sqrt{2}}$$"
        }
    ]
}