{
    "hands_on_practices": [
        {
            "introduction": "The remarkable efficiency of Convolutional Neural Networks stems from the core principle of weight sharing. This practice provides a concrete way to appreciate this innovation by quantifying the immense parameter reduction compared to a layer without weight sharing . Furthermore, it connects the widely used technique of $\\ell_2$ weight decay to its theoretical foundation in Bayesian inference, revealing it as a form of Maximum A Posteriori (MAP) estimation with a Gaussian prior.",
            "id": "3118617",
            "problem": "A dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$ is used to train a Convolutional Neural Network (CNN) by minimizing the empirical negative log-likelihood with $\\ell_2$ weight decay. The training objective is\n$$\n\\mathcal{J}(w) \\equiv \\sum_{i=1}^N -\\log p(y_i \\mid x_i, w) \\;+\\; \\frac{\\lambda}{2}\\,\\|w\\|_2^2,\n$$\nwhere $w$ denotes all trainable weights and $\\lambda > 0$ is the regularization coefficient. From the first principles of Bayesian inference and Maximum A Posteriori (MAP) estimation, interpret the $\\ell_2$ weight decay term as a prior over $w$, and state precisely which prior distribution yields the above objective as a MAP estimator, including the relation between $\\lambda$ and the prior covariance.\n\nThen, consider the first convolutional layer of AlexNet with input of spatial size $227 \\times 227$ and $3$ channels, using $96$ filters of spatial size $11 \\times 11$, stride $s = 4$, and no zero-padding, producing an output of spatial size $55 \\times 55$ and $96$ channels. Ignoring biases, compute:\n- the total number of weights in this convolutional layer (with weight sharing), and\n- the total number of weights in a locally connected layer (no weight sharing) that uses the same receptive field size, stride, and number of output channels, but allows distinct weights for every output spatial position and channel,\nand report the ratio of locally connected weights to convolutional weights.\n\nChoose the option that correctly states both the MAP–prior interpretation and the parameter counts and ratio.\n\nA. The prior is zero-mean isotropic Gaussian $\\mathcal{N}(0, \\sigma^2 I)$ with $\\sigma^2 = \\lambda^{-1}$. The convolutional layer has $11 \\times 11 \\times 3 \\times 96 = 34{,}848$ weights. The locally connected layer has $55 \\times 55 \\times 11 \\times 11 \\times 3 \\times 96 = 105{,}415{,}200$ weights. The ratio is $3{,}025$.\n\nB. The prior is zero-mean isotropic Gaussian $\\mathcal{N}(0, \\sigma^2 I)$ with $\\sigma^2 = \\lambda$. The convolutional layer has $34{,}848$ weights. The locally connected layer has $54 \\times 54 \\times 11 \\times 11 \\times 3 \\times 96 = 101{,}616{,}768$ weights. The ratio is $2{,}916$.\n\nC. The prior is zero-mean Laplace distribution, and the convolutional layer has $11 \\times 11 \\times 96 = 11{,}616$ weights. The locally connected layer has $55 \\times 55 \\times 11 \\times 11 \\times 96 = 35{,}138{,}400$. The ratio is $3{,}025$.\n\nD. The prior is zero-mean isotropic Gaussian $\\mathcal{N}(0, \\sigma^2 I)$ with covariance $(2\\lambda)^{-1} I$. The convolutional layer has $34{,}848$ weights. The locally connected layer has $56 \\times 56 \\times 11 \\times 11 \\times 3 \\times 96 = 111{,}974{,}400$ weights. The ratio is $3{,}136$.",
            "solution": "The user has provided a problem with two parts. The first part requires a Bayesian interpretation of $\\ell_2$ weight decay. The second part involves calculating the number of parameters in a specific convolutional layer and a corresponding locally connected layer, and then finding their ratio.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Dataset: $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$.\n- Training objective to minimize: $\\mathcal{J}(w) \\equiv \\sum_{i=1}^N -\\log p(y_i \\mid x_i, w) \\;+\\; \\frac{\\lambda}{2}\\,\\|w\\|_2^2$.\n- Regularization coefficient: $\\lambda > 0$.\n- First convolutional layer of AlexNet specifications:\n    - Input volume size: $227 \\times 227 \\times 3$.\n    - Number of filters: $96$.\n    - Filter spatial size: $11 \\times 11$.\n    - Stride: $s = 4$.\n    - Zero-padding: none.\n    - Output volume size: $55 \\times 55 \\times 96$.\n    - Biases are to be ignored.\n- Task 1: Interpret the $\\ell_2$ weight decay term as a prior on weights $w$ from Maximum A Posteriori (MAP) estimation, specifying the distribution and the relation between $\\lambda$ and the prior's parameters.\n- Task 2: Compute the number of weights for the described convolutional layer.\n- Task 3: Compute the number of weights for a locally connected layer with the same receptive field, stride, and output channels, but without weight sharing.\n- Task 4: Compute the ratio of locally connected weights to convolutional weights.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, drawing on fundamental principles of Bayesian statistics (MAP estimation) and deep learning (CNN architectures, weight sharing). The relationship between $\\ell_2$ regularization and a Gaussian prior is a canonical result in machine learning. The architectural details provided for the AlexNet layer are historically accurate and internally consistent.\n\nThe output spatial dimension $O$ of a convolutional layer is given by the formula $O = \\lfloor \\frac{W - K + 2P}{S} \\rfloor + 1$, where $W$ is the input size, $K$ is the filter size, $P$ is the padding, and $S$ is the stride. Using the given values, $W = 227$, $K = 11$, $P = 0$, and $S = 4$:\n$$O = \\left\\lfloor \\frac{227 - 11 + 2 \\cdot 0}{4} \\right\\rfloor + 1 = \\left\\lfloor \\frac{216}{4} \\right\\rfloor + 1 = 54 + 1 = 55.$$\nThe calculated output size of $55$ matches the problem statement, confirming the consistency of the provided parameters.\n\nThe problem is well-posed, objective, and contains all necessary information to arrive at a unique solution. There are no contradictions, ambiguities, or factual unsoundness.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the solution derivation.\n\n### Solution Derivation\n\n**Part 1: MAP Estimation and Prior Interpretation**\n\nMaximum A Posteriori (MAP) estimation seeks the parameters $w$ that maximize the posterior probability of the parameters given the data $\\mathcal{D}$. Using Bayes' theorem, the posterior is:\n$$p(w \\mid \\mathcal{D}) = \\frac{p(\\mathcal{D} \\mid w) p(w)}{p(\\mathcal{D})}$$\nTo find the MAP estimate, $\\hat{w}_{MAP}$, we maximize this quantity with respect to $w$:\n$$\\hat{w}_{MAP} = \\arg\\max_{w} p(w \\mid \\mathcal{D}) = \\arg\\max_{w} \\frac{p(\\mathcal{D} \\mid w) p(w)}{p(\\mathcal{D})}$$\nSince the logarithm is a monotonically increasing function and $p(\\mathcal{D})$ is independent of $w$, this is equivalent to maximizing the log-posterior (excluding the constant term $\\log p(\\mathcal{D})$):\n$$\\hat{w}_{MAP} = \\arg\\max_{w} \\left( \\log p(\\mathcal{D} \\mid w) + \\log p(w) \\right)$$\nAssuming the data points are independent and identically distributed (i.i.d.), the likelihood term $\\log p(\\mathcal{D} \\mid w)$ can be written as a sum over the training examples:\n$$\\log p(\\mathcal{D} \\mid w) = \\log \\prod_{i=1}^N p(y_i \\mid x_i, w) = \\sum_{i=1}^N \\log p(y_i \\mid x_i, w)$$\nThe MAP estimation problem is then:\n$$\\hat{w}_{MAP} = \\arg\\max_{w} \\left( \\sum_{i=1}^N \\log p(y_i \\mid x_i, w) + \\log p(w) \\right)$$\nThis is equivalent to minimizing the negative log-posterior:\n$$\\hat{w}_{MAP} = \\arg\\min_{w} \\left( -\\sum_{i=1}^N \\log p(y_i \\mid x_i, w) - \\log p(w) \\right)$$\nWe are given the training objective to minimize:\n$$\\mathcal{J}(w) = \\sum_{i=1}^N -\\log p(y_i \\mid x_i, w) + \\frac{\\lambda}{2}\\,\\|w\\|_2^2$$\nBy comparing the MAP minimization objective with the given training objective $\\mathcal{J}(w)$, we can equate the terms that depend on $w$ but not the data. This establishes the correspondence between the prior term and the regularization term.\n$$-\\log p(w) = \\frac{\\lambda}{2} \\|w\\|_2^2 + C$$\nwhere $C$ is a constant of integration (the negative log of the normalization constant of the prior). Rearranging for the prior probability $p(w)$:\n$$\\log p(w) = -\\frac{\\lambda}{2} \\|w\\|_2^2 - C$$\n$$p(w) = \\exp\\left(-\\frac{\\lambda}{2} \\|w\\|_2^2 - C\\right) \\propto \\exp\\left(-\\frac{\\lambda}{2} \\|w\\|_2^2\\right)$$\nThis functional form corresponds to a multivariate Gaussian distribution. The probability density function for a zero-mean multivariate Gaussian $w \\sim \\mathcal{N}(0, \\Sigma)$ is:\n$$p(w) \\propto \\exp\\left(-\\frac{1}{2} w^T \\Sigma^{-1} w\\right)$$\nFor a zero-mean isotropic Gaussian, the covariance matrix is $\\Sigma = \\sigma^2 I$, where $I$ is the identity matrix and $\\sigma^2$ is the variance. Its inverse is $\\Sigma^{-1} = \\frac{1}{\\sigma^2} I$. The term in the exponent becomes:\n$$-\\frac{1}{2} w^T (\\sigma^{-2}I) w = -\\frac{1}{2\\sigma^2} w^T w = -\\frac{1}{2\\sigma^2} \\|w\\|_2^2$$\nEquating the coefficients of $\\|w\\|_2^2$ from the objective function and the Gaussian prior:\n$$\\frac{\\lambda}{2} = \\frac{1}{2\\sigma^2} \\implies \\lambda = \\frac{1}{\\sigma^2} \\implies \\sigma^2 = \\lambda^{-1}$$\nTherefore, the $\\ell_2$ weight decay term corresponds to a zero-mean isotropic Gaussian prior over the weights, $w \\sim \\mathcal{N}(0, \\sigma^2 I)$, with a variance $\\sigma^2 = \\lambda^{-1}$.\n\n**Part 2: Parameter Count for Convolutional Layer**\n\nA convolutional layer with weight sharing applies the same set of filters across all spatial locations of the input. Each filter has a depth equal to the number of input channels.\n- Filter height, $K_h = 11$.\n- Filter width, $K_w = 11$.\n- Number of input channels, $C_{in} = 3$.\n- Number of filters (which equals the number of output channels), $C_{out} = 96$.\n\nThe total number of weights in the convolutional layer, denoted $W_{conv}$, is the product of these dimensions:\n$$W_{conv} = K_h \\times K_w \\times C_{in} \\times C_{out}$$\n$$W_{conv} = 11 \\times 11 \\times 3 \\times 96 = 121 \\times 3 \\times 96 = 363 \\times 96 = 34{,}848$$\n\n**Part 3: Parameter Count for Locally Connected Layer**\n\nA locally connected layer does not share weights across different spatial positions in the output map. This means for each location in the output feature map, there is a distinct set of weights.\n- Output height, $O_h = 55$.\n- Output width, $O_w = 55$.\nThe number of weights required for one spatial position in the output is the same as the total number of weights in the standard convolutional layer, as it needs a full set of $C_{out}$ filters. More precisely, for each of the $O_h \\times O_w$ output positions and for each of the $C_{out}$ output channels, there is a unique filter of size $K_h \\times K_w \\times C_{in}$.\n\nThe total number of weights in the locally connected layer, $W_{local}$, is:\n$$W_{local} = (O_h \\times O_w) \\times (K_h \\times K_w \\times C_{in} \\times C_{out})$$\n$$W_{local} = (55 \\times 55) \\times (11 \\times 11 \\times 3 \\times 96)$$\n$$W_{local} = 3{,}025 \\times 34{,}848 = 105{,}415{,}200$$\n\n**Part 4: Ratio of Weights**\n\nThe ratio of the number of weights in the locally connected layer to the convolutional layer is:\n$$\\text{Ratio} = \\frac{W_{local}}{W_{conv}} = \\frac{(O_h \\times O_w) \\times W_{conv}}{W_{conv}} = O_h \\times O_w$$\n$$\\text{Ratio} = 55 \\times 55 = 3{,}025$$\n\n### Option-by-Option Analysis\n\n**A. The prior is zero-mean isotropic Gaussian $\\mathcal{N}(0, \\sigma^2 I)$ with $\\sigma^2 = \\lambda^{-1}$. The convolutional layer has $11 \\times 11 \\times 3 \\times 96 = 34{,}848$ weights. The locally connected layer has $55 \\times 55 \\times 11 \\times 11 \\times 3 \\times 96 = 105{,}415{,}200$ weights. The ratio is $3{,}025$.**\n- Prior interpretation: Correct. Our derivation shows $\\sigma^2 = \\lambda^{-1}$.\n- Convolutional weights: Correct. Our calculation yields $34{,}848$.\n- Locally connected weights: Correct. Our calculation yields $105{,}415{,}200$.\n- Ratio: Correct. Our calculation yields $3{,}025$.\n- **Verdict: Correct.**\n\n**B. The prior is zero-mean isotropic Gaussian $\\mathcal{N}(0, \\sigma^2 I)$ with $\\sigma^2 = \\lambda$. The convolutional layer has $34{,}848$ weights. The locally connected layer has $54 \\times 54 \\times 11 \\times 11 \\times 3 \\times 96 = 101{,}616{,}768$ weights. The ratio is $2{,}916$.**\n- Prior interpretation: Incorrect. The variance is $\\sigma^2 = \\lambda^{-1}$, not $\\lambda$.\n- Convolutional weights: Correct.\n- Locally connected weights: Incorrect. This uses an incorrect output dimension of $54 \\times 54$. The correct dimension is $55 \\times 55$.\n- Ratio: Incorrect. This is $54 \\times 54 = 2{,}916$, based on the incorrect output dimension.\n- **Verdict: Incorrect.**\n\n**C. The prior is zero-mean Laplace distribution, and the convolutional layer has $11 \\times 11 \\times 96 = 11{,}616$ weights. The locally connected layer has $55 \\times 55 \\times 11 \\times 11 \\times 96 = 35{,}138{,}400$. The ratio is $3{,}025$.**\n- Prior interpretation: Incorrect. A Laplace prior corresponds to $\\ell_1$ regularization, not $\\ell_2$.\n- Convolutional weights: Incorrect. The calculation $11 \\times 11 \\times 96$ omits the input channels, $C_{in}=3$.\n- Locally connected weights: Incorrect. The expression omits the input channels.\n- Ratio: The value $3{,}025$ is numerically correct for $55 \\times 55$, but it is presented in the context of incorrect weight calculations.\n- **Verdict: Incorrect.**\n\n**D. The prior is zero-mean isotropic Gaussian $\\mathcal{N}(0, \\sigma^2 I)$ with covariance $(2\\lambda)^{-1} I$. The convolutional layer has $34{,}848$ weights. The locally connected layer has $56 \\times 56 \\times 11 \\times 11 \\times 3 \\times 96 = 111{,}974{,}400$ weights. The ratio is $3{,}136$.**\n- Prior interpretation: Incorrect. The covariance is $\\lambda^{-1}I$, not $(2\\lambda)^{-1}I$. A covariance of $(2\\lambda)^{-1}I$ corresponds to a regularization term of $\\lambda\\|w\\|_2^2$, missing the factor of $\\frac{1}{2}$.\n- Convolutional weights: Correct.\n- Locally connected weights: Incorrect. This uses an incorrect output dimension of $56 \\times 56$.\n- Ratio: Incorrect. This is $56 \\times 56 = 3{,}136$, based on the incorrect output dimension.\n- **Verdict: Incorrect.**\n\nBased on the analysis, only option A is correct in all its statements.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "One of the most significant practical challenges in training any deep neural network, including early architectures like LeNet-5, is knowing when to stop. Training for too long leads to overfitting, where the model loses its ability to generalize. This hands-on coding exercise  moves beyond simple heuristics by having you implement a principled early stopping rule based on the curvature of the validation loss, providing an algorithmic tool to detect the onset of overfitting.",
            "id": "3118548",
            "problem": "You are training a LeNet-$5$ style Convolutional Neural Network (CNN) classifier and observe the validation loss across epochs. You want to implement a principle-based early stopping rule that detects the onset of overfitting using the curvature (second derivative) of the validation loss sequence. Your task is to write a complete program that, given several validation loss sequences and rule parameters, returns the integer epoch index at which to stop training, or returns $-1$ when the rule never triggers.\n\nStart from the following fundamental bases:\n- Empirical risk minimization defines a validation loss sequence $\\{v_t\\}_{t=0}^{T-1}$ computed on held-out data, where $t$ is the epoch index.\n- Overfitting onset corresponds to a transition from decreasing validation loss to increasing validation loss; in terms of shape, this is captured by positive curvature (second derivative) together with a non-negative recent slope.\n- In discrete time, the first and second derivatives are approximated by finite differences. For any sequence $\\{v_t\\}$, define the first difference $d^{(1)}_t = v_t - v_{t-1}$ for $t \\in \\{1,\\dots,T-1\\}$ and the centered second difference $d^{(2)}_t = v_{t+1} - 2 v_t + v_{t-1}$ for $t \\in \\{1,\\dots,T-2\\}$.\n\nDefine the following second-derivative-based early stopping rule, parameterized by a curvature window length $w \\in \\mathbb{N}$ with $w \\ge 1$, a slope window length $m \\in \\mathbb{N}$ with $m \\ge 1$, a curvature threshold $\\theta \\in \\mathbb{R}$ with $\\theta > 0$, and a non-negativity slack $\\epsilon \\in \\mathbb{R}$ with $\\epsilon \\ge 0$:\n- For each epoch $t \\in \\{1,\\dots,T-2\\}$ at which both trailing windows are available (that is, $t \\ge w$ and $t \\ge m$), compute the trailing average curvature\n$$\nc_t \\;=\\; \\frac{1}{w} \\sum_{i=0}^{w-1} d^{(2)}_{t-i}\n$$\nand the trailing average slope\n$$\ng_t \\;=\\; \\frac{1}{m} \\sum_{i=0}^{m-1} d^{(1)}_{t-i}.\n$$\n- Declare the earliest detection index $t^\\star$ to be the smallest $t$ such that $c_t \\ge \\theta$ and $g_t \\ge -\\epsilon$.\n- If such a $t^\\star$ exists, output the early stopping epoch $e^\\star = \\arg\\min_{0 \\le s \\le t^\\star} v_s$. In case of multiple minimizers, choose the smallest index. If no such $t^\\star$ exists, output $-1$.\n\nImplement this rule exactly as stated. The input to your program is fixed within the program itself; you must not read any external input. Use the following test suite, where each test case is a tuple containing a validation loss sequence and the parameters $(w, m, \\theta, \\epsilon)$. For clarity, all sequences are unitless:\n\n- Test case $1$ (typical decrease then increase with smooth curvature):\n  - Losses: [$1.20$, $0.95$, $0.80$, $0.78$, $0.79$, $0.81$, $0.85$, $0.90$]\n  - Parameters: $w = 2$, $m = 2$, $\\theta = 0.01$, $\\epsilon = 0.0$\n\n- Test case $2$ (boundary-length sequence with permissive slack):\n  - Losses: [$0.50$, $0.49$, $0.495$]\n  - Parameters: $w = 1$, $m = 1$, $\\theta = 0.01$, $\\epsilon = 0.02$\n\n- Test case $3$ (monotone decrease, no overfitting):\n  - Losses: [$1.00$, $0.90$, $0.85$, $0.84$, $0.83$]\n  - Parameters: $w = 2$, $m = 2$, $\\theta = 0.02$, $\\epsilon = 0.0$\n\n- Test case $4$ (noisy trough; require smoothing and a small curvature threshold):\n  - Losses: [$0.90$, $0.82$, $0.80$, $0.805$, $0.802$, $0.808$, $0.815$, $0.814$, $0.820$]\n  - Parameters: $w = 3$, $m = 3$, $\\theta = 0.0005$, $\\epsilon = 0.002$\n\n- Test case $5$ (immediate overfitting):\n  - Losses: [$0.90$, $0.905$, $0.915$, $0.930$]\n  - Parameters: $w = 1$, $m = 2$, $\\theta = 0.001$, $\\epsilon = 0.0$\n\n- Test case $6$ (boundary window too large to be applicable):\n  - Losses: [$0.60$, $0.59$, $0.58$, $0.57$]\n  - Parameters: $w = 3$, $m = 1$, $\\theta = 0.001$, $\\epsilon = 0.0$\n\nYour program must compute the early stopping epoch $e^\\star$ for each test case using the rule above and produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, [$e_1,e_2,\\dots$]). The output type for each test case must be an integer. No angles or physical units are involved in this problem. Ensure scientific realism by implementing the discrete finite-difference definitions and the trailing-window averages exactly as defined.",
            "solution": "We begin from empirical risk minimization: training a model such as LeNet-$5$ or AlexNet minimizes the empirical loss on training data, but generalization is assessed by the validation loss sequence $\\{v_t\\}_{t=0}^{T-1}$. Overfitting is characterized by the validation loss ceasing to decrease and beginning to increase, which corresponds to a change in shape from concave-down to concave-up. In continuous time, this is where the second derivative becomes positive and the first derivative becomes non-negative. In discrete time with epochs, the appropriate approximations are finite differences.\n\nLet $v_t$ denote the validation loss at epoch $t$. The first difference $d^{(1)}_t = v_t - v_{t-1}$ estimates the slope, and the centered second difference $d^{(2)}_t = v_{t+1} - 2 v_t + v_{t-1}$ estimates curvature at epoch $t$. Positive $d^{(2)}_t$ indicates locally convex behavior. Since validation loss measurements are noisy, we do not rely on a single epoch’s differences. Instead, we average over trailing windows: the trailing average curvature\n$$\nc_t \\;=\\; \\frac{1}{w} \\sum_{i=0}^{w-1} d^{(2)}_{t-i}, \\quad \\text{for } t \\ge w,\n$$\nand the trailing average slope\n$$\ng_t \\;=\\; \\frac{1}{m} \\sum_{i=0}^{m-1} d^{(1)}_{t-i}, \\quad \\text{for } t \\ge m.\n$$\n\nThe rule detects the earliest epoch $t^\\star$ satisfying $c_{t^\\star} \\ge \\theta$ (sustained positive curvature exceeding a threshold) and $g_{t^\\star} \\ge -\\epsilon$ (slope non-negative up to a slack $\\epsilon$ to accommodate noise). This pairs geometric evidence of a “bowl bottom” with non-decreasing recent trend, tracing the onset of overfitting. Once detected, the prudent early stopping epoch $e^\\star$ is the best validation epoch up to detection,\n$$\ne^\\star \\;=\\; \\arg\\min_{0 \\le s \\le t^\\star} v_s,\n$$\nwhich is the standard practice of choosing the best-so-far model checkpoint at the detected onset of overfitting. If no $t^\\star$ exists (insufficient length or no threshold crossings), we return $-1$.\n\nAlgorithmic design from principles:\n- Compute $d^{(1)}_t$ for $t \\in \\{1,\\dots,T-1\\}$ and $d^{(2)}_t$ for $t \\in \\{1,\\dots,T-2\\}$ by direct finite differences; these arise from first principles of numerical differentiation.\n- For each epoch index $t \\in \\{1,\\dots,T-2\\}$ with $t \\ge w$ and $t \\ge m$, compute trailing averages $c_t$ and $g_t$ using simple arithmetic means, which reduce variance in noisy measurements by leveraging the law of large numbers.\n- Select the smallest $t$ meeting $c_t \\ge \\theta$ and $g_t \\ge -\\epsilon$. If found, compute $e^\\star$ as the smallest index achieving $\\min_{0 \\le s \\le t} v_s$; otherwise return $-1$.\n- Edge cases: when $T < 3$, $d^{(2)}_t$ is undefined; when $t < w$ or $t < m$, the corresponding averages are undefined. In both situations, detection cannot occur.\n\nApplying this to the provided test suite:\n- Test case $1$ has a clear trough followed by rising losses; curvature becomes positive and the recent slope becomes non-negative, yielding a detection $t^\\star$ and hence an $e^\\star$ equal to the index of the minimal validation loss before or at detection.\n- Test case $2$ demonstrates boundary-length detection with slack $\\epsilon$ allowing slightly negative recent slope; the curvature threshold is exceeded, producing an early detection and $e^\\star$ at the current best index.\n- Test case $3$ never meets the slope condition (always decreasing), so the output is $-1$.\n- Test case $4$ shows a noisy trough; averaging with $w$ and $m$ suppresses noise so that a small positive curvature threshold and non-negative average slope identify the onset near the trough; $e^\\star$ is the index of the trough.\n- Test case $5$ has immediate overfitting (consistently increasing and convex), so detection occurs early and $e^\\star$ is the earliest index giving the minimum so far.\n- Test case $6$ illustrates that with too large a curvature window relative to sequence length, the rule is inapplicable and returns $-1$.\n\nThe program implements these steps exactly and outputs a single list of integers [$e^\\star_1, e^\\star_2, \\dots$] corresponding to the six test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef early_stop_epoch_from_curvature(losses, w, m, theta, epsilon):\n    \"\"\"\n    Implements the second-derivative-based early stopping rule as specified.\n\n    Parameters:\n        losses (list or np.ndarray): Validation loss sequence v[0..T-1].\n        w (int): Trailing window length for curvature average (w >= 1).\n        m (int): Trailing window length for slope average (m >= 1).\n        theta (float): Curvature threshold (theta > 0).\n        epsilon (float): Non-negativity slack (epsilon >= 0).\n\n    Returns:\n        int: Early stopping epoch index e* if detected; otherwise -1.\n    \"\"\"\n    v = np.asarray(losses, dtype=float)\n    T = v.shape[0]\n    if T  3:\n        return -1\n\n    # First differences d1[i] = v[i+1] - v[i], corresponds to d^(1)_{i+1}\n    d1 = v[1:] - v[:-1]\n    # Second differences d2[i] = v[i+2] - 2*v[i+1] + v[i], corresponds to d^(2)_{i+1}\n    d2 = v[2:] - 2.0 * v[1:-1] + v[:-2]\n\n    # Search earliest t satisfying conditions.\n    # Epoch index t ranges from 1 .. T-2 inclusive (in original indexing).\n    detected_t = None\n    for t in range(1, T - 1):  # t is original epoch index, from 1 to T-2\n        # Trailing windows require t >= w and t >= m for curvature and slope respectively.\n        # But the sums are over d^{(2)}_{t-i} and d^{(1)}_{t-i}.\n        # d^{(2)}_{t-i} corresponds to d2[t-i-1].\n        # d^{(1)}_{t-i} corresponds to d1[t-i-1].\n        # So we need indices down to t-w-1 and t-m-1 in d2/d1.\n        # This means t-1 >= w-1 => t >= w for curvature.\n        # and t-1 >= m-1 => t >= m for slope.\n        # The loop starts at t=1. We must be able to access all needed array elements.\n        \n        # We need d2 centered at t, t-1, ... t-w+1.\n        # d2[k] is centered at k+1.\n        # So we need d2 indices from (t-w+1)-1 to t-1, i.e., t-w to t-1.\n        # This requires t-w >= 0 => t >= w.\n        if t  w:\n            continue\n\n        c_window = d2[t - w: t]\n        if len(c_window) != w:\n            continue\n        c_t = np.mean(c_window)\n        \n        # We need d1 for epochs t, t-1, ..., t-m+1\n        # d1[k] is for epoch k+1.\n        # So we need d1 indices from t-m to t-1.\n        # This requires t-m >= 0 => t >= m.\n        if t  m:\n            continue\n        \n        g_window = d1[t - m: t]\n        if len(g_window) != m:\n            continue\n        g_t = np.mean(g_window)\n\n        if c_t >= theta and g_t >= -epsilon:\n            detected_t = t\n            break\n\n    if detected_t is None:\n        return -1\n\n    # Early stop at the best-so-far validation epoch up to and including detected_t.\n    # Argmin returns first occurrence of minimum by default.\n    best_idx = int(np.argmin(v[: detected_t + 1]))\n    return best_idx\n\ndef solve():\n    # Define the test cases from the problem statement as (losses, w, m, theta, epsilon).\n    test_cases = [\n        # 1) Typical decrease then increase with smooth curvature\n        ([1.20, 0.95, 0.80, 0.78, 0.79, 0.81, 0.85, 0.90], 2, 2, 0.01, 0.0),\n        # 2) Boundary-length sequence with permissive slack\n        ([0.50, 0.49, 0.495], 1, 1, 0.01, 0.02),\n        # 3) Monotone decrease, no overfitting\n        ([1.00, 0.90, 0.85, 0.84, 0.83], 2, 2, 0.02, 0.0),\n        # 4) Noisy trough; smoothing and small curvature threshold\n        ([0.90, 0.82, 0.80, 0.805, 0.802, 0.808, 0.815, 0.814, 0.820], 3, 3, 0.0005, 0.002),\n        # 5) Immediate overfitting\n        ([0.90, 0.905, 0.915, 0.930], 1, 2, 0.001, 0.0),\n        # 6) Boundary window too large to be applicable\n        ([0.60, 0.59, 0.58, 0.57], 3, 1, 0.001, 0.0),\n    ]\n\n    results = []\n    for losses, w, m, theta, epsilon in test_cases:\n        result = early_stop_epoch_from_curvature(losses, w, m, theta, epsilon)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "The success of AlexNet in 2012 was not just due to its architecture, but also the engineering feat of training such a large model on the hardware of the time, which required using multiple GPUs. This practice problem  lets you step into the shoes of a deep learning engineer to analyze the performance of distributed training. You will model the communication costs of synchronizing gradients using a ring all-reduce algorithm and calculate the parallel efficiency to understand the practical trade-offs of scaling up.",
            "id": "3118605",
            "problem": "Consider synchronous data-parallel training of the early Convolutional Neural Network (CNN) architecture AlexNet using Stochastic Gradient Descent (SGD) on multiple Graphics Processing Units (GPUs). Assume the following scientifically realistic and self-consistent setup:\n\n- The number of trainable parameters in AlexNet is $N_{p} = 61{,}000{,}000$, stored in single-precision floating point, so each parameter occupies $4$ bytes.\n- In each training step, the gradient of all parameters is computed locally on each GPU and must be averaged across GPUs to ensure mathematical equivalence to single-GPU SGD on the combined batch.\n- Gradient aggregation is implemented via the ring all-reduce algorithm: each device partitions the gradient vector into $P$ equal contiguous chunks and performs a scatter-reduce phase of $P-1$ iterations (sending one chunk to its neighbor and accumulating received chunks) followed by an allgather phase of $P-1$ iterations (sending and receiving chunks to assemble the full averaged gradient locally). Each iteration communicates one chunk of size $\\frac{S}{P}$, where $S$ is the total gradient size in bytes.\n- The number of GPUs is $P = 8$.\n- The effective point-to-point communication bandwidth is $B = 3.125 \\times 10^{9}$ bytes per second. Ignore latency and protocol overhead; model communication time as $\\frac{\\text{bytes}}{B}$.\n- The per-GPU forward-plus-backward compute time for a fixed per-GPU mini-batch is $t_{\\mathrm{comp}} = 0.080$ seconds, and there is no overlap between communication and computation.\n\nTasks:\n1. Compute the per-GPU communication volume (in bytes) for one training step under ring all-reduce in this setup.\n2. Compute the per-GPU communication time (in seconds) for that volume, using the bandwidth model provided.\n3. Define parallel efficiency $E$ as the ratio of achieved throughput at $P$ GPUs to the ideal linear-speedup throughput at $P$ GPUs when the per-GPU mini-batch size is held constant. Using this definition, and the step time model above, compute the predicted $E$ for this setup.\n\nExpress your final answer as the value of $E$ in decimal form. Round your final answer to four significant figures.",
            "solution": "The problem statement is first validated for scientific soundness, consistency, and clarity.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n-   CNN architecture: AlexNet\n-   Number of trainable parameters: $N_{p} = 61{,}000{,}000$\n-   Data type for parameters: single-precision floating point (occupies $4$ bytes)\n-   Training paradigm: Synchronous data-parallel Stochastic Gradient Descent (SGD)\n-   Gradient aggregation algorithm: ring all-reduce\n-   Ring all-reduce model: The gradient vector of total size $S$ is split into $P$ chunks. The process involves a scatter-reduce phase of $P-1$ iterations and an all-gather phase of $P-1$ iterations. Each iteration involves communicating one chunk of size $\\frac{S}{P}$.\n-   Number of GPUs: $P = 8$\n-   Effective point-to-point communication bandwidth: $B = 3.125 \\times 10^{9}$ bytes/second\n-   Communication time model: $t_{\\text{comm}} = \\frac{\\text{bytes communicated}}{B}$, with latency and protocol overhead ignored.\n-   Per-GPU compute time (forward + backward pass): $t_{\\mathrm{comp}} = 0.080$ seconds\n-   Overlap: No overlap between communication and computation.\n-   Definition of parallel efficiency, $E$: Ratio of achieved throughput at $P$ GPUs to the ideal linear-speedup throughput at $P$ GPUs, with constant per-GPU mini-batch size.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard and realistic scenario in the field of distributed deep learning. The provided values for the number of parameters in AlexNet ($N_{p}$), the number of GPUs ($P$), the communication bandwidth ($B$), and the per-GPU computation time ($t_{\\mathrm{comp}}$) are consistent and plausible for high-performance computing systems. The model for ring all-reduce communication and the overall step-time model (computation + communication) are standard and well-defined simplifications used for performance analysis. The definition of parallel efficiency is a standard one for weak-scaling analysis. The problem is self-contained and free of internal contradictions, ambiguities, or factual unsoundness.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be developed.\n\n**Solution Derivation**\n\nThe solution requires calculating the parallel efficiency $E$. This involves determining the total time for a single training step with $P$ GPUs, $T(P)$, and with a single GPU, $T(1)$.\n\nFirst, we calculate the total size $S$ of the gradient vector that must be synchronized across all GPUs in each step. The network has $N_p = 61{,}000{,}000$ trainable parameters, and each parameter's gradient is represented by a $4$-byte single-precision floating-point number.\n$$S = N_p \\times 4 \\text{ bytes} = 61{,}000{,}000 \\times 4 = 244{,}000{,}000 \\text{ bytes}$$\nThis can be expressed in scientific notation as $S = 2.44 \\times 10^8$ bytes.\n\nSecond, we calculate the communication time, $t_{\\mathrm{comm}}$, required for the ring all-reduce operation. The algorithm proceeds in $2(P-1)$ sequential communication steps. In each step, a data chunk of size $\\frac{S}{P}$ is sent over a point-to-point link. The time for one such step is the chunk size divided by the bandwidth $B$. Since the steps are sequential and latency is ignored, the total communication time is the sum of the times for each step:\n$$t_{\\mathrm{comm}} = (\\text{Number of steps}) \\times (\\text{Time per step}) = 2(P-1) \\times \\frac{S/P}{B} = \\frac{2(P-1)S}{PB}$$\nWe substitute the given values: $P = 8$, $S = 2.44 \\times 10^8$ bytes, and $B = 3.125 \\times 10^9$ bytes/s.\n$$t_{\\mathrm{comm}} = \\frac{2(8-1)(2.44 \\times 10^8)}{8 \\times (3.125 \\times 10^9)} = \\frac{14 \\times 2.44 \\times 10^8}{25 \\times 10^9}$$\n$$t_{\\mathrm{comm}} = \\frac{34.16 \\times 10^8}{25 \\times 10^9} = \\frac{3.416 \\times 10^9}{2.5 \\times 10^{10}} = 0.13664 \\text{ s}$$\n\nThird, we compute the parallel efficiency $E$. The total time for one training step on $P$ GPUs, $T(P)$, is the sum of the computation time and communication time, as there is no overlap.\n$$T(P) = t_{\\mathrm{comp}} + t_{\\mathrm{comm}}$$\nThe time for a training step on a single GPU, $T(1)$, involves only computation, as no inter-GPU communication is needed. Thus, $t_{\\mathrm{comm}}(P=1) = 0$.\n$$T(1) = t_{\\mathrm{comp}}$$\nParallel efficiency $E$ is defined as the ratio of achieved throughput to ideal throughput. Let $B_0$ be the constant per-GPU mini-batch size.\nAchieved throughput at $P$ GPUs is $\\text{Throughput}(P) = \\frac{\\text{Total samples per step}}{\\text{Time per step}} = \\frac{P \\cdot B_0}{T(P)}$.\nThe ideal throughput at $P$ GPUs is $P$ times the throughput of a single GPU: $\\text{Ideal Throughput}(P) = P \\times \\text{Throughput}(1) = P \\times \\frac{1 \\cdot B_0}{T(1)}$.\nThe efficiency $E$ is the ratio:\n$$E = \\frac{\\text{Throughput}(P)}{\\text{Ideal Throughput}(P)} = \\frac{P \\cdot B_0 / T(P)}{P \\cdot B_0 / T(1)} = \\frac{T(1)}{T(P)}$$\nSubstituting the expressions for $T(1)$ and $T(P)$:\n$$E = \\frac{t_{\\mathrm{comp}}}{t_{\\mathrm{comp}} + t_{\\mathrm{comm}}}$$\nNow, we substitute the numerical values $t_{\\mathrm{comp}} = 0.080$ s and our calculated $t_{\\mathrm{comm}} = 0.13664$ s:\n$$E = \\frac{0.080}{0.080 + 0.13664} = \\frac{0.080}{0.21664} \\approx 0.3692762...$$\nThe problem requires the final answer to be rounded to four significant figures.\n$$E \\approx 0.3693$$",
            "answer": "$$\\boxed{0.3693}$$"
        }
    ]
}