{
    "hands_on_practices": [
        {
            "introduction": "The hallmark of the Inception architecture is its multi-branch design, processing information at different spatial scales simultaneously. This practice  explores a key motivation for this design from a signal processing viewpoint: noise reduction. You will derive how the averaging effect of larger convolutional kernels improves the Signal-to-Noise Ratio ($SNR$), providing a concrete justification for the multi-scale approach.",
            "id": "3130750",
            "problem": "You will implement and analyze a simplified Inception-like multi-branch module to study how spatial kernel size affects denoising by inherent averaging under a white noise model, using only first-principles reasoning. Consider a single-channel image and an Inception-like module with parallel branches that each apply a single convolution with a square kernel of size $k \\times k$ and stride $1$. Each branch uses a uniform averaging kernel, normalized to unit sum, which models the spatial aggregation behavior of larger receptive fields in an Inception branch. Use the following foundational base only: the linearity of convolution, the definition of convolution as a weighted sum, the definition of Signal-to-Noise Ratio (SNR) as signal power divided by noise power, and the properties of independent and identically distributed zero-mean Gaussian noise.\n\nTask. Given a clean image $S$ and additive noise $N$ where $N$ is independent and identically distributed zero-mean Gaussian with variance $\\sigma^2$, define the input SNR over a specified spatial support $\\Omega$ as\n$$\n\\mathrm{SNR_{in}} = \\frac{\\mathbb{E}_{(i,j)\\in \\Omega}\\left[S(i,j)^2\\right]}{\\mathbb{E}\\left[N(i,j)^2\\right]}.\n$$\nFor a branch with kernel $h_k$ of size $k \\times k$, define the output signal as $S_{\\mathrm{out}} = h_k * S$ and the output noise as $N_{\\mathrm{out}} = h_k * N$. The output SNR is\n$$\n\\mathrm{SNR_{out}} = \\frac{\\mathbb{E}_{(i,j)\\in \\Omega_k}\\left[S_{\\mathrm{out}}(i,j)^2\\right]}{\\mathbb{E}\\left[N_{\\mathrm{out}}(i,j)^2\\right]}.\n$$\nHere $*$ denotes convolution. To avoid edge effects and ensure the same filter is applied at each output location, you must use valid convolution (no padding) so that the output support $\\Omega_k$ is the set of all positions where the $k \\times k$ kernel is fully contained in the input. For each branch, the SNR improvement factor is defined as the ratio\n$$\nG_k = \\frac{\\mathrm{SNR_{out}}}{\\mathrm{SNR_{in}}}.\n$$\nYou must compute $G_k$ for each kernel size in each test case by deriving the noise power after filtering from first principles under the independent and identically distributed Gaussian assumption. Do not use empirical averaging over many noise realizations; instead, use the properties of linear combinations of independent variables to obtain the noise power after filtering. All convolutions must be implemented with stride $1$ and valid mode.\n\nLow-light dataset model. Construct the clean image $S$ deterministically according to the test case description. Model low light by choosing small positive amplitudes $A$ and relatively larger noise standard deviations $\\sigma$. The noise distribution is used only to determine the noise power symbolically as required by the derivation; you do not sample noise.\n\nFilters. For a given kernel size $k$, the branch filter is a uniform averaging kernel with coefficients $h(u,v)=1/k^2$ for all $u,v$ in the $k \\times k$ window.\n\nRequired outputs. For each test case, report a list of SNR improvement factors $G_k$ for the specified kernel sizes $k$, in the same order as listed in the test case. Round each reported factor to $6$ decimal places.\n\nTest suite. Implement the following test cases exactly:\n- Test case $1$: image size $H=W=32$, clean signal type: constant, amplitude $A=0.1$, noise standard deviation $\\sigma=0.2$, kernel sizes $k \\in \\{1,3,5\\}$.\n- Test case $2$: image size $H=W=32$, clean signal type: checkerboard with values in $\\{0,A\\}$ alternating per pixel, amplitude $A=0.1$, noise standard deviation $\\sigma=0.2$, kernel sizes $k \\in \\{1,3,5\\}$.\n- Test case $3$: image size $H=W=32$, clean signal type: constant, amplitude $A=0.05$, noise standard deviation $\\sigma=0.3$, kernel sizes $k \\in \\{1,5,7\\}$.\n- Test case $4$: image size $H=W=7$, clean signal type: constant, amplitude $A=0.1$, noise standard deviation $\\sigma=0.2$, kernel sizes $k \\in \\{1,7\\}$.\n\nFinal output format. Your program should produce a single line of output containing the results for all test cases as a single list of lists. Each inner list must contain the SNR improvement factors $G_k$ for that test case, in the order of kernel sizes given for that test case. All numbers must be rounded to $6$ decimal places. The line must be formatted as a JSON-like list of lists without spaces, for example using symbolic placeholders: $[[g_{1,1},g_{1,2},\\dots],[g_{2,1},\\dots],\\dots]$.",
            "solution": "The user problem is deemed valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. The task is to derive and compute the Signal-to-Noise Ratio (SNR) improvement factor $G_k$ for a simplified Inception-like module branch with a uniform averaging kernel of size $k \\times k$.\n\nThe derivation proceeds from first principles as stipulated. The SNR improvement factor $G_k$ is defined as the ratio of the output SNR to the input SNR:\n$$\nG_k = \\frac{\\mathrm{SNR_{out}}}{\\mathrm{SNR_{in}}}\n$$\nThe input and output SNRs are defined as the ratio of signal power to noise power.\n$$\n\\mathrm{SNR_{in}} = \\frac{P_{\\mathrm{S, in}}}{P_{\\mathrm{N, in}}} = \\frac{\\mathbb{E}_{(i,j)\\in \\Omega}\\left[S(i,j)^2\\right]}{\\mathbb{E}\\left[N(i,j)^2\\right]}\n$$\n$$\n\\mathrm{SNR_{out}} = \\frac{P_{\\mathrm{S, out}}}{P_{\\mathrm{N, out}}} = \\frac{\\mathbb{E}_{(i,j)\\in \\Omega_k}\\left[S_{\\mathrm{out}}(i,j)^2\\right]}{\\mathbb{E}\\left[N_{\\mathrm{out}}(i,j)^2\\right]}\n$$\nHere, $S$ and $N$ are the input signal and noise, while $S_{\\mathrm{out}}$ and $N_{\\mathrm{out}}$ are the signal and noise after convolution with the kernel $h_k$. The notation $\\mathbb{E}_{(i,j)\\in\\Omega}[\\cdot]$ denotes spatial averaging over a support $\\Omega$, while $\\mathbb{E}[\\cdot]$ denotes the statistical expectation over the noise distribution.\n\nSubstituting these definitions into the expression for $G_k$ gives:\n$$\nG_k = \\frac{P_{\\mathrm{S, out}} / P_{\\mathrm{N, out}}}{P_{\\mathrm{S, in}} / P_{\\mathrm{N, in}}} = \\left(\\frac{P_{\\mathrm{S, out}}}{P_{\\mathrm{S, in}}}\\right) \\left(\\frac{P_{\\mathrm{N, in}}}{P_{\\mathrm{N, out}}}\\right)\n$$\nWe analyze the noise power ratio and the signal power ratio separately.\n\nFirst, we analyze the noise power ratio. The input noise $N(i,j)$ is independent and identically distributed (i.i.d.) with zero mean and variance $\\sigma^2$. The input noise power at any pixel is therefore:\n$$\nP_{\\mathrm{N, in}} = \\mathbb{E}[N(i,j)^2] = \\mathrm{Var}(N(i,j)) + (\\mathbb{E}[N(i,j)])^2 = \\sigma^2 + 0^2 = \\sigma^2\n$$\nThe output noise $N_{\\mathrm{out}}$ is the result of convolving the input noise $N$ with the uniform averaging kernel $h_k$, where $h_k(u,v) = 1/k^2$. At an output location $(i,j)$, the output noise is a linear combination of input noise samples:\n$$\nN_{\\mathrm{out}}(i,j) = (h_k * N)(i,j) = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\frac{1}{k^2} N(i+u, j+v)\n$$\nThe expectation of the output noise is zero due to the linearity of expectation and $\\mathbb{E}[N]=0$:\n$$\n\\mathbb{E}[N_{\\mathrm{out}}(i,j)] = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\frac{1}{k^2} \\mathbb{E}[N(i+u, j+v)] = 0\n$$\nThe output noise power is its variance. Since the input noise samples $N(i,j)$ are independent, the variance of their weighted sum is the sum of the variances of each term:\n$$\nP_{\\mathrm{N, out}} = \\mathbb{E}[N_{\\mathrm{out}}(i,j)^2] = \\mathrm{Var}(N_{\\mathrm{out}}(i,j)) = \\mathrm{Var}\\left(\\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\frac{1}{k^2} N(i+u, j+v)\\right)\n$$\n$$\nP_{\\mathrm{N, out}} = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\mathrm{Var}\\left(\\frac{1}{k^2} N(i+u, j+v)\\right) = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\left(\\frac{1}{k^2}\\right)^2 \\mathrm{Var}(N(i+u, j+v))\n$$\nSince each of the $k^2$ noise samples has variance $\\sigma^2$:\n$$\nP_{\\mathrm{N, out}} = k^2 \\cdot \\frac{1}{k^4} \\sigma^2 = \\frac{\\sigma^2}{k^2}\n$$\nThe noise power ratio is therefore:\n$$\n\\frac{P_{\\mathrm{N, in}}}{P_{\\mathrm{N, out}}} = \\frac{\\sigma^2}{\\sigma^2 / k^2} = k^2\n$$\nThis demonstrates the fundamental principle of noise reduction through averaging: averaging $k^2$ i.i.d. noise samples reduces the noise power by a factor of $k^2$.\n\nNow, we consider the signal power ratio. This is computed through spatial averaging over the respective domains. The input signal power, over an image of size $H \\times W$, is:\n$$\nP_{\\mathrm{S, in}} = \\mathbb{E}_{(i,j)\\in \\Omega}\\left[S(i,j)^2\\right] = \\frac{1}{HW} \\sum_{i=0}^{H-1} \\sum_{j=0}^{W-1} S(i,j)^2\n$$\nThe output signal is $S_{\\mathrm{out}} = h_k * S$. Using a 'valid' convolution, the output image has dimensions $(H-k+1) \\times (W-k+1)$. The output signal power is:\n$$\nP_{\\mathrm{S, out}} = \\mathbb{E}_{(i,j)\\in \\Omega_k}\\left[S_{\\mathrm{out}}(i,j)^2\\right] = \\frac{1}{(H-k+1)(W-k+1)} \\sum_{i=0}^{H-k} \\sum_{j=0}^{W-k} S_{\\mathrm{out}}(i,j)^2\n$$\nCombining the noise and signal power ratios, the final expression for the SNR improvement factor $G_k$ is:\n$$\nG_k = k^2 \\frac{P_{\\mathrm{S, out}}}{P_{\\mathrm{S, in}}}\n$$\nThis formula is implemented to solve for $G_k$ for each test case.\nFor a constant signal $S(i,j)=A$, the averaging kernel produces a constant output $S_{\\mathrm{out}}(i,j) = A$. Thus, $P_{\\mathrm{S, out}} = A^2$ and $P_{\\mathrm{S, in}} = A^2$. The ratio of signal powers is $1$, so $G_k=k^2$.\nFor a high-frequency signal like a checkerboard, the averaging acts as a low-pass filter, attenuating the signal. This results in $P_{\\mathrm{S, out}} < P_{\\mathrm{S, in}}$, and consequently $G_k < k^2$. The implementation computes the convolution and subsequent spatial averages numerically to determine the signal power ratio for each case.",
            "answer": "```python\nimport numpy as np\n\ndef calculate_g_factors(H, W, signal_type, A, k_list):\n    \"\"\"\n    Computes the SNR improvement factor G_k for a list of kernel sizes.\n\n    Args:\n        H (int): Image height.\n        W (int): Image width.\n        signal_type (str): Type of clean signal ('constant' or 'checkerboard').\n        A (float): Amplitude of the signal.\n        k_list (list): List of kernel sizes (k).\n\n    Returns:\n        list: A list of SNR improvement factors G_k, rounded to 6 decimal places.\n    \"\"\"\n    # 1. Construct the clean signal S\n    if signal_type == 'constant':\n        S = np.full((H, W), A, dtype=np.float64)\n    elif signal_type == 'checkerboard':\n        S = np.zeros((H, W), dtype=np.float64)\n        # Use broadcasting for efficient checkerboard generation\n        i_coords = np.arange(H).reshape(-1, 1)\n        j_coords = np.arange(W).reshape(1, -1)\n        mask = (i_coords + j_coords) % 2 == 0\n        S[mask] = A\n    else:\n        raise ValueError(\"Unknown signal type\")\n\n    # 2. Calculate input signal power P_s_in\n    # This is the mean of the squared signal values\n    P_s_in = np.mean(S**2)\n\n    g_factors = []\n    \n    # Check for zero-signal case to prevent division by zero, though not in test cases\n    if P_s_in == 0:\n        # If signal is zero, SNR is zero both in and out. The ratio is ill-defined.\n        # Based on the formula, P_s_out will also be 0, leading to 0/0.\n        # We can return a placeholder or handle as an error. For this problem, A > 0.\n        # Arbitrarily returning k^2 as if signal power ratio is 1.\n        return [float(k**2) for k in k_list]\n\n    for k in k_list:\n        # 3. Calculate output signal S_out by 'valid' convolution\n        H_out = H - k + 1\n        W_out = W - k + 1\n        S_out = np.zeros((H_out, W_out), dtype=np.float64)\n        \n        # Explicitly implement convolution with a uniform averaging kernel\n        for i in range(H_out):\n            for j in range(W_out):\n                patch = S[i:i+k, j:j+k]\n                S_out[i, j] = np.mean(patch)\n\n        # 4. Calculate output signal power P_s_out\n        P_s_out = np.mean(S_out**2) if S_out.size > 0 else 0.0\n\n        # 5. Calculate G_k using the derived formula\n        g_k = (k**2) * P_s_out / P_s_in\n        g_factors.append(g_k)\n\n    return [round(g, 6) for g in g_factors]\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {'H': 32, 'W': 32, 'signal_type': 'constant', 'A': 0.1, 'k_list': [1, 3, 5]},\n        {'H': 32, 'W': 32, 'signal_type': 'checkerboard', 'A': 0.1, 'k_list': [1, 3, 5]},\n        {'H': 32, 'W': 32, 'signal_type': 'constant', 'A': 0.05, 'k_list': [1, 5, 7]},\n        {'H': 7, 'W': 7, 'signal_type': 'constant', 'A': 0.1, 'k_list': [1, 7]}\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = calculate_g_factors(case['H'], case['W'], case['signal_type'], case['A'], case['k_list'])\n        all_results.append(results)\n\n    # Format the final output as a single-line JSON-like list of lists\n    result_str = '[' + ','.join(\n        '[' + ','.join([f'{val:.6f}' for val in res]) + ']' for res in all_results\n    ) + ']'\n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "A key challenge in deep network design is balancing representational power with computational cost, and large convolutions can be particularly expensive. The Inception architecture cleverly addresses this by factorizing these convolutions into smaller, more efficient ones, such as replacing a $7 \\times 7$ filter with a $1 \\times 7$ and a $7 \\times 1$ filter in sequence. In this practice , you will quantify these computational savings by deriving the reduction in Floating Point Operations ($FLOPs$), a core optimization that made deeper Inception models practical.",
            "id": "3130734",
            "problem": "You are studying the computational efficiency techniques in the GoogLeNet (Inception) architecture, specifically the factorization of a square convolution into two separable rectangular convolutions. Using the standard definition of convolutional cost in deep learning, derive from first principles the exact Floating Point Operations (FLOPs) for a single two-dimensional convolution and for a factorized sequence of two convolutions. Then, quantify the computational savings when replacing a single $K \\times K$ convolution with a $1 \\times K$ convolution followed by a $K \\times 1$ convolution. Finally, compute top-$1$ accuracy on an abstract ImageNet-$100$ subset with provided labels and predicted classes to assess whether factorization maintains accuracy.\n\nFoundational base and definitions to be used:\n- A two-dimensional convolution with input of $C_{\\text{in}}$ channels and output of $C_{\\text{out}}$ channels, kernel size $K \\times K$, and output spatial dimensions $H_{\\text{out}} \\times W_{\\text{out}}$ performs, by convention, $2$ FLOPs per multiply-accumulate (one multiplication and one addition). Under this convention, the FLOPs of a single $K \\times K$ convolution are\n$$\nF_{\\text{orig}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2.\n$$\n- For a factorized pair of convolutions consisting of a $1 \\times K$ convolution mapping $C_{\\text{in}} \\to C_{\\text{mid}}$ followed by a $K \\times 1$ convolution mapping $C_{\\text{mid}} \\to C_{\\text{out}}$, the FLOPs are\n$$\nF_{\\text{fact}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(C_{\\text{in}} \\cdot C_{\\text{mid}} \\cdot K \\;+\\; C_{\\text{mid}} \\cdot C_{\\text{out}} \\cdot K\\right).\n$$\n- The FLOP savings fraction is defined as\n$$\nS \\;=\\; \\frac{F_{\\text{orig}} - F_{\\text{fact}}}{F_{\\text{orig}}}.\n$$\n- Top-$1$ accuracy is defined as the fraction of examples where the predicted class index equals the ground-truth label, i.e.,\n$$\n\\text{acc} \\;=\\; \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}\\{ \\hat{y}_i = y_i \\},\n$$\nwith $N$ the number of samples, $y_i \\in \\{0,\\dots,C-1\\}$ the true labels, and $\\hat{y}_i$ the predicted class indices.\n\nTasks:\n1) Prove algebraically that if $C_{\\text{mid}} = C_{\\text{out}} = C_{\\text{in}}$ and $K = 7$, then the factorized pair uses exactly a fraction $2/7$ of the FLOPs of the original $7 \\times 7$ convolution, hence the FLOPs are reduced to $2/7$ of the original. Report the exact savings fraction $S$.\n2) Compute the exact FLOP savings fraction $S$ for the following three test cases, assuming stride $1$ and output spatial dimensions as given. Count FLOPs according to the above convention and express the savings as a decimal rounded to six places.\n   - Test A (happy path): $H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 64$, $C_{\\text{mid}} = 64$, $K = 7$.\n   - Test B (boundary): $H_{\\text{out}} = 28$, $W_{\\text{out}} = 28$, $C_{\\text{in}} = 32$, $C_{\\text{out}} = 32$, $C_{\\text{mid}} = 32$, $K = 1$.\n   - Test C (bottleneck factorization): $H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 128$, $C_{\\text{mid}} = 64$, $K = 7$.\n3) Using an abstract ImageNet-$100$ subset with $N = 12$ samples and $C = 100$ classes, with labels and predictions provided below, compute the top-$1$ accuracies for a baseline model (single $K \\times K$ convolution) and a factorized model ($1 \\times K$ followed by $K \\times 1$), and report both accuracies and their difference (factorized minus baseline). The labels and predicted class indices are:\n   - Ground-truth labels $y$: $[5,12,23,23,11,65,99,0,3,42,77,13]$.\n   - Baseline predicted classes $\\hat{y}^{\\text{base}}$: $[5,11,23,3,11,65,13,0,2,42,8,13]$.\n   - Factorized predicted classes $\\hat{y}^{\\text{fact}}$: $[5,12,23,23,9,65,99,0,3,1,77,88]$.\n\nAngle or physical units do not apply. All requested numeric outputs must be decimal numbers rounded to six decimal places.\n\nProgram requirements:\n- Implement a program that computes:\n  - The FLOP savings fraction $S$ for Test A, Test B, and Test C.\n  - The top-$1$ accuracies for the baseline and factorized predictions on the provided ImageNet-$100$ subset and their difference.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order:\n  $[S_{\\text{A}}, S_{\\text{B}}, S_{\\text{C}}, \\text{acc}_{\\text{base}}, \\text{acc}_{\\text{fact}}, \\Delta \\text{acc}]$,\n  where $\\Delta \\text{acc} = \\text{acc}_{\\text{fact}} - \\text{acc}_{\\text{base}}$.\n- All six numbers must be rounded to six decimal places.",
            "solution": "The problem requires an analysis of the computational savings and performance impact of factorizing a square convolution into two separable rectangular convolutions, a technique prominently used in the GoogLeNet (Inception) architecture. I will first validate the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- FLOPs for original convolution: $F_{\\text{orig}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2$\n- FLOPs for factorized convolution: $F_{\\text{fact}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(C_{\\text{in}} \\cdot C_{\\text{mid}} \\cdot K \\;+\\; C_{\\text{mid}} \\cdot C_{\\text{out}} \\cdot K\\right)$\n- FLOP savings fraction: $S \\;=\\; \\frac{F_{\\text{orig}} - F_{\\text{fact}}}{F_{\\text{orig}}}$\n- Top-1 accuracy: $\\text{acc} \\;=\\; \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}\\{ \\hat{y}_i = y_i \\}$\n- Task 1 conditions: $C_{\\text{mid}} = C_{\\text{out}} = C_{\\text{in}}$, $K = 7$\n- Test A parameters: $H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 64$, $C_{\\text{mid}} = 64$, $K = 7$\n- Test B parameters: $H_{\\text{out}} = 28$, $W_{\\text{out}} = 28$, $C_{\\text{in}} = 32$, $C_{\\text{out}} = 32$, $C_{\\text{mid}} = 32$, $K = 1$\n- Test C parameters: $H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 128$, $C_{\\text{mid}} = 64$, $K = 7$\n- Task 3 data: $N = 12$, $C = 100$\n    - $y$: $[5,12,23,23,11,65,99,0,3,42,77,13]$\n    - $\\hat{y}^{\\text{base}}$: $[5,11,23,3,11,65,13,0,2,42,8,13]$\n    - $\\hat{y}^{\\text{fact}}$: $[5,12,23,23,9,65,99,0,3,1,77,88]$\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the principles of deep learning and computational complexity analysis of neural network operations. All definitions and formulas are standard in the field. The problem is well-posed, providing all necessary data and constraints to arrive at unique numerical solutions for each task. The language is objective and formal. The parameters given are realistic for convolutional neural network architectures. The problem does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\nThe solution proceeds by addressing each of the three tasks in order.\n\n**Task 1: Algebraic Proof and Savings Fraction**\n\nWe are asked to prove that for the specific case where $C_{\\text{in}} = C_{\\text{mid}} = C_{\\text{out}} = C$ and $K=7$, the factorized convolution uses a fraction $2/7$ of the FLOPs of the original, and to find the savings fraction $S$.\n\nLet $C_{\\text{in}} = C_{\\text{mid}} = C_{\\text{out}} = C$. The FLOPs expressions become:\n$$\nF_{\\text{orig}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C \\cdot C \\cdot K^2 \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C^2 \\cdot K^2\n$$\n$$\nF_{\\text{fact}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(C \\cdot C \\cdot K \\;+\\; C \\cdot C \\cdot K\\right) \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(2 \\cdot C^2 \\cdot K\\right)\n$$\nTo find the ratio of factorized FLOPs to original FLOPs, we compute $\\frac{F_{\\text{fact}}}{F_{\\text{orig}}}$:\n$$\n\\frac{F_{\\text{fact}}}{F_{\\text{orig}}} \\;=\\; \\frac{2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot 2 \\cdot C^2 \\cdot K}{2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C^2 \\cdot K^2} \\;=\\; \\frac{2K}{K^2} \\;=\\; \\frac{2}{K}\n$$\nThe common factor $2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C^2$ cancels out, demonstrating that this ratio is independent of the spatial dimensions and channel counts, provided they are equal.\nFor a kernel size of $K=7$, this ratio is:\n$$\n\\frac{F_{\\text{fact}}}{F_{\\text{orig}}} \\;=\\; \\frac{2}{7}\n$$\nThis proves that the factorized pair uses exactly $2/7$ of the FLOPs of the original $7 \\times 7$ convolution under these conditions.\n\nThe savings fraction $S$ is defined as $S = \\frac{F_{\\text{orig}} - F_{\\text{fact}}}{F_{\\text{orig}}} = 1 - \\frac{F_{\\text{fact}}}{F_{\\text{orig}}}$.\nSubstituting the derived ratio:\n$$\nS \\;=\\; 1 - \\frac{2}{K}\n$$\nFor $K=7$, the savings fraction is:\n$$\nS \\;=\\; 1 - \\frac{2}{7} \\;=\\; \\frac{5}{7}\n$$\nThe exact savings fraction is $5/7$.\n\n**Task 2: FLOP Savings Fraction for Test Cases**\n\nTo compute the savings fraction for the given test cases, we first derive a general simplified expression for $S$.\n$$\nS \\;=\\; 1 - \\frac{F_{\\text{fact}}}{F_{\\text{orig}}} \\;=\\; 1 - \\frac{2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(C_{\\text{in}} \\cdot C_{\\text{mid}} \\cdot K \\;+\\; C_{\\text{mid}} \\cdot C_{\\text{out}} \\cdot K\\right)}{2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2}\n$$\nThe term $2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}}$ cancels. We can also factor out $C_{\\text{mid}} \\cdot K$ from the numerator:\n$$\nS \\;=\\; 1 - \\frac{C_{\\text{mid}} \\cdot K \\cdot (C_{\\text{in}} + C_{\\text{out}})}{C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2} \\;=\\; 1 - \\frac{C_{\\text{mid}} (C_{\\text{in}} + C_{\\text{out}})}{K \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}}\n$$\nUsing this simplified formula, we compute $S$ for each test case.\n\n- **Test A:** $H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 64$, $C_{\\text{mid}} = 64$, $K = 7$.\nThis case matches the conditions of Task $1$.\n$$\nS_{\\text{A}} \\;=\\; 1 - \\frac{64 \\cdot (64 + 64)}{7 \\cdot 64 \\cdot 64} \\;=\\; 1 - \\frac{64 \\cdot 128}{7 \\cdot 4096} \\;=\\; 1 - \\frac{128}{7 \\cdot 64} \\;=\\; 1 - \\frac{2}{7} \\;=\\; \\frac{5}{7} \\approx 0.714286\n$$\n\n- **Test B:** $H_{\\text{out}} = 28$, $W_{\\text{out}} = 28$, $C_{\\text{in}} = 32$, $C_{\\text{out}} = 32$, $C_{\\text{mid}} = 32$, $K = 1$.\nThis boundary case involves a $1 \\times 1$ kernel, where factorization is not typically beneficial.\n$$\nS_{\\text{B}} \\;=\\; 1 - \\frac{32 \\cdot (32 + 32)}{1 \\cdot 32 \\cdot 32} \\;=\\; 1 - \\frac{32 \\cdot 64}{1024} \\;=\\; 1 - \\frac{2048}{1024} \\;=\\; 1 - 2 \\;=\\; -1\n$$\nA savings of $-1$ indicates that the FLOPs have doubled, which is expected when replacing one $1 \\times 1$ convolution with two sequential $1 \\times 1$ convolutions.\n\n- **Test C:** $H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 128$, $C_{\\text{mid}} = 64$, $K = 7$.\nThis represents a bottleneck design, where $C_{\\text{mid}}$ is smaller than $C_{\\text{out}}$.\n$$\nS_{\\text{C}} \\;=\\; 1 - \\frac{64 \\cdot (64 + 128)}{7 \\cdot 64 \\cdot 128} \\;=\\; 1 - \\frac{64 \\cdot 192}{7 \\cdot 8192} \\;=\\; 1 - \\frac{192}{7 \\cdot 128} \\;=\\; 1 - \\frac{192}{896}\n$$\nSimplifying the fraction $\\frac{192}{896}$ by dividing numerator and denominator by their greatest common divisor, $64$: $\\frac{192 \\div 64}{896 \\div 64} = \\frac{3}{14}$.\n$$\nS_{\\text{C}} \\;=\\; 1 - \\frac{3}{14} \\;=\\; \\frac{11}{14} \\approx 0.785714\n$$\n\n**Task 3: Top-1 Accuracy Calculation**\n\nWe compute the top-$1$ accuracy for both the baseline and factorized models on the provided abstract data set with $N=12$ samples. The accuracy is the count of correct predictions divided by the total number of samples.\n\n- **Ground-truth labels $y$**: $[5,12,23,23,11,65,99,0,3,42,77,13]$\n\n- **Baseline predicted classes $\\hat{y}^{\\text{base}}$**: $[5,11,23,3,11,65,13,0,2,42,8,13]$\nWe compare $y$ and $\\hat{y}^{\\text{base}}$ element-wise to find matches:\n- Index $0$: $5 = 5$ (Match)\n- Index $1$: $12 \\neq 11$\n- Index $2$: $23 = 23$ (Match)\n- Index $3$: $23 \\neq 3$\n- Index $4$: $11 = 11$ (Match)\n- Index $5$: $65 = 65$ (Match)\n- Index $6$: $99 \\neq 13$\n- Index $7$: $0 = 0$ (Match)\n- Index $8$: $3 \\neq 2$\n- Index $9$: $42 = 42$ (Match)\n- Index $10$: $77 \\neq 8$\n- Index $11$: $13 = 13$ (Match)\nThere are $7$ correct predictions.\n$$\n\\text{acc}_{\\text{base}} \\;=\\; \\frac{7}{12} \\approx 0.583333\n$$\n\n- **Factorized predicted classes $\\hat{y}^{\\text{fact}}$**: $[5,12,23,23,9,65,99,0,3,1,77,88]$\nWe compare $y$ and $\\hat{y}^{\\text{fact}}$ element-wise to find matches:\n- Index $0$: $5 = 5$ (Match)\n- Index $1$: $12 = 12$ (Match)\n- Index $2$: $23 = 23$ (Match)\n- Index $3$: $23 = 23$ (Match)\n- Index $4$: $11 \\neq 9$\n- Index $5$: $65 = 65$ (Match)\n- Index $6$: $99 = 99$ (Match)\n- Index $7$: $0 = 0$ (Match)\n- Index $8$: $3 = 3$ (Match)\n- Index $9$: $42 \\neq 1$\n- Index $10$: $77 = 77$ (Match)\n- Index $11$: $13 \\neq 88$\nThere are $9$ correct predictions.\n$$\n\\text{acc}_{\\text{fact}} \\;=\\; \\frac{9}{12} \\;=\\; \\frac{3}{4} \\;=\\; 0.75\n$$\n\n- **Difference in accuracy $\\Delta \\text{acc}$**:\n$$\n\\Delta \\text{acc} \\;=\\; \\text{acc}_{\\text{fact}} - \\text{acc}_{\\text{base}} \\;=\\; \\frac{9}{12} - \\frac{7}{12} \\;=\\; \\frac{2}{12} \\;=\\; \\frac{1}{6} \\approx 0.166667\n$$\nThe results will be compiled into the required output format in the program.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the three tasks defined in the problem:\n    1. Computes FLOP savings fractions S for three test cases.\n    2. Computes top-1 accuracies for baseline and factorized models.\n    3. Computes the difference in accuracies.\n    \"\"\"\n\n    # --- Task 2: Compute FLOP Savings Fraction S ---\n\n    def calculate_savings_fraction(C_in, C_out, C_mid, K):\n        \"\"\"\n        Computes the FLOP savings fraction S using the derived formula:\n        S = 1 - (C_mid * (C_in + C_out)) / (K * C_in * C_out)\n        \n        A kernel size K=0 would lead to division by zero, but is physically\n        meaningless and not present in the test cases.\n        \"\"\"\n        if K == 0 or C_in == 0 or C_out == 0:\n            # Handle edge cases to prevent division by zero, though not strictly\n            # needed for the given problem data. An invalid configuration would\n            # result in undefined savings.\n            return np.nan\n\n        # Simplified formula derived in the solution text\n        numerator = C_mid * (C_in + C_out)\n        denominator = K * C_in * C_out\n        \n        # When K=1, the original convolution cost formula for F_orig can be zero if K=0.\n        # However, the problem states KxK conv, so K>=1. For K=1, F_orig has K^2=1.\n        # F_fact has K=1. The formula is arithmetically sound.\n        \n        ratio = numerator / denominator\n        S = 1 - ratio\n        return S\n\n    # Test Case A\n    params_A = {'C_in': 64, 'C_out': 64, 'C_mid': 64, 'K': 7}\n    S_A = calculate_savings_fraction(**params_A)\n\n    # Test Case B\n    params_B = {'C_in': 32, 'C_out': 32, 'C_mid': 32, 'K': 1}\n    S_B = calculate_savings_fraction(**params_B)\n\n    # Test Case C\n    params_C = {'C_in': 64, 'C_out': 128, 'C_mid': 64, 'K': 7}\n    S_C = calculate_savings_fraction(**params_C)\n\n    # --- Task 3: Compute Top-1 Accuracies ---\n\n    # Provided data for the abstract ImageNet-100 subset\n    y_true = np.array([5, 12, 23, 23, 11, 65, 99, 0, 3, 42, 77, 13])\n    y_pred_base = np.array([5, 11, 23, 3, 11, 65, 13, 0, 2, 42, 8, 13])\n    y_pred_fact = np.array([5, 12, 23, 23, 9, 65, 99, 0, 3, 1, 77, 88])\n\n    # Top-1 accuracy is the mean of correct predictions\n    acc_base = np.mean(y_true == y_pred_base)\n    acc_fact = np.mean(y_true == y_pred_fact)\n\n    # Difference in accuracies\n    delta_acc = acc_fact - acc_base\n\n    # --- Final Output Formatting ---\n\n    results = [S_A, S_B, S_C, acc_base, acc_fact, delta_acc]\n    \n    # Format each result to six decimal places and join into the required string format\n    formatted_results = [f\"{val:.6f}\" for val in results]\n    output_string = f\"[{','.join(formatted_results)}]\"\n\n    print(output_string)\n\n# This block ensures the solve function is called when the script is executed.\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "When an Inception module concatenates outputs from its parallel branches, it risks combining feature maps with widely different statistical properties, which can hinder learning in subsequent layers. The Inception-v2 paper proposed using Batch Normalization ($BN$) to address this, but its placement is crucial. This exercise  lets you analyze this design choice by comparing the statistical mismatch that occurs when $BN$ is applied before versus after concatenation, clarifying why pre-concatenation normalization is a more stable approach.",
            "id": "3130685",
            "problem": "In a GoogLeNet (Inception) module, multiple branches produce feature maps that are concatenated along the channel dimension before feeding into the next convolution. Consider two branches, labeled $A$ and $B$, in an Inception v2-style block. Each branch outputs a set of channels whose pre-activation statistics (over the batch and spatial positions) can be modeled as independent Gaussian random variables per channel, with per-channel mean $\\mu$ and variance $\\sigma^{2}$. Concatenation forms a single tensor by stacking all channels from branch $A$ and branch $B$.\n\nBatch Normalization (BN) follows the standard definition: for an input channel with random variable $X$, the BN transform is $$Y = \\gamma \\frac{X - \\hat{\\mu}}{\\sqrt{\\hat{\\sigma}^{2} + \\varepsilon}} + \\beta$$, where $\\hat{\\mu}$ and $\\hat{\\sigma}^{2}$ are the batch mean and batch variance estimates, $\\gamma$ and $\\beta$ are learnable scaling and shifting parameters, and $\\varepsilon$ is a small constant for numerical stability. At initialization we take $\\gamma = 1$, $\\beta = 0$, and assume $\\varepsilon$ is negligible so that BN ideally normalizes each channel to zero mean and unit variance.\n\nDefine a branch-level mismatch metric at the concatenation boundary as\n$$\nD = \\left(m_{A} - m_{B}\\right)^{2} + \\left(v_{A} - v_{B}\\right)^{2},\n$$\nwhere $m_{A}$ and $m_{B}$ are the averages of the per-channel means within branches $A$ and $B$, respectively, and $v_{A}$ and $v_{B}$ are the averages of the per-channel variances within branches $A$ and $B$, respectively. This $D$ quantifies mean/variance mismatch across branches just before concatenation.\n\nYou will compare two BN placements:\n- BN-before: apply Batch Normalization independently to each channel within each branch before concatenation.\n- BN-after: omit BN in branches and apply a single Batch Normalization layer after concatenation (which normalizes each concatenated channel independently per the BN definition above).\n\nAssume branch $A$ has $3$ channels with $(\\mu,\\sigma^{2})$ values\n$$\n(\\mu_{A,1},\\sigma^{2}_{A,1}) = (0.5,\\,1.6),\\quad (\\mu_{A,2},\\sigma^{2}_{A,2}) = (-0.2,\\,0.9),\\quad (\\mu_{A,3},\\sigma^{2}_{A,3}) = (0.3,\\,1.1),\n$$\nand branch $B$ has $2$ channels with\n$$\n(\\mu_{B,1},\\sigma^{2}_{B,1}) = (-0.4,\\,0.8),\\quad (\\mu_{B,2},\\sigma^{2}_{B,2}) = (0.1,\\,1.4).\n$$\n\nStarting from the definitions of concatenation, expectation, variance, and Batch Normalization provided above (and without invoking any shortcut formulas beyond these fundamentals), derive the mismatch metric $D_{\\text{before}}$ when BN is applied before concatenation and $D_{\\text{after}}$ when BN is applied after concatenation. Then compute the difference\n$$\n\\Delta D = D_{\\text{after}} - D_{\\text{before}}.\n$$\nReport $\\Delta D$ as a single real number. No rounding is required beyond exact arithmetic, and no units are involved. Ensure your derivation explains why each BN placement affects $m_{A}$, $m_{B}$, $v_{A}$, and $v_{B}$ as used in $D$.",
            "solution": "The user-provided problem has been validated and is determined to be a valid, well-posed problem in the domain of deep learning. It is scientifically grounded, self-contained, and objective. We may now proceed with the solution.\n\nThe objective is to compute the difference in the branch-level mismatch metric, $\\Delta D = D_{\\text{after}} - D_{\\text{before}}$, between two Batch Normalization (BN) placement strategies. The mismatch metric is defined as $D = (m_{A} - m_{B})^{2} + (v_{A} - v_{B})^{2}$, where $m$ and $v$ represent the average of per-channel means and variances, respectively, for a given branch just before concatenation.\n\nFirst, we establish the initial statistics for branch $A$ and branch $B$.\nBranch $A$ has $N_A = 3$ channels with the following mean and variance pairs $(\\mu, \\sigma^2)$:\n$(\\mu_{A,1}, \\sigma^{2}_{A,1}) = (0.5, 1.6)$\n$(\\mu_{A,2}, \\sigma^{2}_{A,2}) = (-0.2, 0.9)$\n$(\\mu_{A,3}, \\sigma^{2}_{A,3}) = (0.3, 1.1)$\n\nBranch $B$ has $N_B = 2$ channels with the following statistics:\n$(\\mu_{B,1}, \\sigma^{2}_{B,1}) = (-0.4, 0.8)$\n$(\\mu_{B,2}, \\sigma^{2}_{B,2}) = (0.1, 1.4)$\n\nThe core of the problem lies in determining the values of $m_A, m_B, v_A, v_B$ under the two scenarios: `BN-before` and `BN-after`. The metric $D$ is calculated based on the statistics of the feature maps *just before* they are concatenated.\n\n**Case 1: `BN-before` Placement**\nIn this scenario, Batch Normalization is applied independently to each channel within each branch *before* the concatenation occurs. The problem states that BN, under the given ideal conditions ($\\gamma=1, \\beta=0, \\varepsilon \\approx 0$), \"ideally normalizes each channel to zero mean and unit variance.\"\n\nLet $\\mu'_{A,i}$ and $\\sigma'^2_{A,i}$ be the mean and variance of channel $i$ in branch $A$ after it has been normalized. Based on the problem's definition of ideal BN, for every channel $i$ in branch $A$:\n$$\n\\mu'_{A,i} = 0 \\\\\n\\sigma'^2_{A,i} = 1\n$$\nSimilarly, for every channel $j$ in branch $B$, the post-BN statistics are:\n$$\n\\mu'_{B,j} = 0 \\\\\n\\sigma'^2_{B,j} = 1\n$$\nThese are the statistics of the channels at the point of concatenation. We now compute the average statistics for each branch to find $D_{\\text{before}}$.\n\nThe average of per-channel means for branch $A$, denoted $m_{A, \\text{before}}$, is:\n$$\nm_{A, \\text{before}} = \\frac{1}{N_A} \\sum_{i=1}^{3} \\mu'_{A,i} = \\frac{1}{3} (0 + 0 + 0) = 0\n$$\nThe average of per-channel variances for branch $A$, denoted $v_{A, \\text{before}}$, is:\n$$\nv_{A, \\text{before}} = \\frac{1}{N_A} \\sum_{i=1}^{3} \\sigma'^2_{A,i} = \\frac{1}{3} (1 + 1 + 1) = 1\n$$\nSimilarly, for branch $B$:\nThe average of per-channel means, $m_{B, \\text{before}}$:\n$$\nm_{B, \\text{before}} = \\frac{1}{N_B} \\sum_{j=1}^{2} \\mu'_{B,j} = \\frac{1}{2} (0 + 0) = 0\n$$\nThe average of per-channel variances, $v_{B, \\text{before}}$:\n$$\nv_{B, \\text{before}} = \\frac{1}{N_B} \\sum_{j=1}^{2} \\sigma'^2_{B,j} = \\frac{1}{2} (1 + 1) = 1\n$$\nNow we can compute the mismatch metric $D_{\\text{before}}$:\n$$\nD_{\\text{before}} = (m_{A, \\text{before}} - m_{B, \\text{before}})^{2} + (v_{A, \\text{before}} - v_{B, \\text{before}})^{2} \\\\\nD_{\\text{before}} = (0 - 0)^{2} + (1 - 1)^{2} = 0^2 + 0^2 = 0\n$$\n\n**Case 2: `BN-after` Placement**\nIn this scenario, BN is omitted within the branches. The concatenation happens first, and then a single BN layer is applied to the concatenated tensor. The mismatch metric $D$ is evaluated on the statistics *before* concatenation. Therefore, we must use the original, un-normalized statistics of the channels.\n\nThe average of per-channel means for branch $A$, denoted $m_{A, \\text{after}}$, is:\n$$\nm_{A, \\text{after}} = \\frac{1}{N_A} \\sum_{i=1}^{3} \\mu_{A,i} = \\frac{1}{3} (0.5 + (-0.2) + 0.3) = \\frac{1}{3}(0.6) = 0.2\n$$\nThe average of per-channel variances for branch $A$, denoted $v_{A, \\text{after}}$, is:\n$$\nv_{A, \\text{after}} = \\frac{1}{N_A} \\sum_{i=1}^{3} \\sigma^{2}_{A,i} = \\frac{1}{3} (1.6 + 0.9 + 1.1) = \\frac{1}{3}(3.6) = 1.2\n$$\nFor branch $B$, the corresponding averages are:\nThe average of per-channel means, $m_{B, \\text{after}}$:\n$$\nm_{B, \\text{after}} = \\frac{1}{N_B} \\sum_{j=1}^{2} \\mu_{B,j} = \\frac{1}{2} (-0.4 + 0.1) = \\frac{1}{2}(-0.3) = -0.15\n$$\nThe average of per-channel variances, $v_{B, \\text{after}}$:\n$$\nv_{B, \\text{after}} = \\frac{1}{N_B} \\sum_{j=1}^{2} \\sigma^{2}_{B,j} = \\frac{1}{2} (0.8 + 1.4) = \\frac{1}{2}(2.2) = 1.1\n$$\nNow we compute the mismatch metric $D_{\\text{after}}$ using these values:\n$$\nD_{\\text{after}} = (m_{A, \\text{after}} - m_{B, \\text{after}})^{2} + (v_{A, \\text{after}} - v_{B, \\text{after}})^{2} \\\\\nD_{\\text{after}} = (0.2 - (-0.15))^{2} + (1.2 - 1.1)^{2} \\\\\nD_{\\text{after}} = (0.35)^{2} + (0.1)^{2} \\\\\nD_{\\text{after}} = 0.1225 + 0.01 = 0.1325\n$$\n\n**Final Calculation: The Difference $\\Delta D$**\nFinally, we compute the difference $\\Delta D = D_{\\text{after}} - D_{\\text{before}}$.\n$$\n\\Delta D = 0.1325 - 0 = 0.1325\n$$\nThis result quantifies the increase in statistical mismatch at the concatenation boundary when moving from a `BN-before` to a `BN-after` design. The `BN-before` strategy perfectly eliminates the mismatch by pre-normalizing each branch, while the `BN-after` strategy allows the original statistical discrepancies between branches to persist up to the concatenation point.",
            "answer": "$$\\boxed{0.1325}$$"
        }
    ]
}