{
    "hands_on_practices": [
        {
            "introduction": "为了在卷积后保持特征图的空间维度，我们通常使用“相同（same）”填充。然而，这种便捷操作也引入了一种微妙的副作用：它使得图像边界和中心区域的像素受到了不同的处理。本练习将引导你量化这种边界效应，揭示卷积操作中固有的中心偏差，从而更深刻地理解 CNN 如何在空间上不均匀地提取特征。",
            "id": "3126198",
            "problem": "考虑卷积神经网络 (CNN) 中的一个二维卷积层，其步长为 $1$，使用大小为 $k$ 的奇数尺寸卷积核，并选择零填充以使输出空间维度与输入空间维度相匹配。设输入的高度为 $H$，宽度为 $W$，其中 $H \\ge k$ 且 $W \\ge k$。定义填充量为 $p = (k - 1)/2$。使用标准定义：一个输出位置的感受野是指为产生该输出而被卷积核线性组合的输入位置（包括填充）的集合，如果一个位置的感受野至少包含一个填充值，则称该位置受填充影响。\n\n任务：\n- 令 $F_{\\text{pad}}$ 为 $H \\times W$ 输出位置中感受野至少包含一个填充元素的比例。推导 $F_{\\text{pad}}$ 作为 $H$、$W$ 和 $k$ 的函数。\n- 为了量化中心偏置，令 $n(i,j)$ 表示其感受野包含坐标为 $(i,j)$（其中 $i \\in \\{1,\\dots,H\\}$ 且 $j \\in \\{1,\\dots,W\\}$）的固定输入像素的输出位置数量。令 $R$ 为比率 $n(\\text{center})/n(\\text{corner})$，其中 $\\text{center}$ 表示满足 $i \\in \\{p+1,\\dots,H-p\\}$ 和 $j \\in \\{p+1,\\dots,W-p\\}$ 的任何像素，而 $\\text{corner}$ 表示像素 $(1,1)$。推导 $R$ 关于 $k$（或 $p$）的表达式。\n\n哪个选项给出了正确的 $(F_{\\text{pad}}, R)$ 对？\n\nA. $F_{\\text{pad}} = 1 - \\dfrac{(H - 2p)(W - 2p)}{HW}$，以及 $R = \\dfrac{k^2}{(p+1)^2}$。\n\nB. $F_{\\text{pad}} = \\dfrac{(H - 2p)(W - 2p)}{HW}$，以及 $R = \\dfrac{(p+1)^2}{k^2}$。\n\nC. $F_{\\text{pad}} = 1 - \\dfrac{(H - k + 1)(W - k + 1)}{HW}$，以及 $R = \\dfrac{k}{p+1}$。\n\nD. $F_{\\text{pad}} = 1 - \\dfrac{(H - 2p + 1)(W - 2p + 1)}{HW}$，以及 $R = \\dfrac{(k - 1)^2}{(p+1)^2}$。",
            "solution": "在尝试求解之前，将首先对问题陈述进行严格验证。\n\n### 步骤 1：提取已知条件\n- 考虑一个二维卷积层。\n- 步长：$s=1$。\n- 卷积核大小：$k$，其中 $k$ 是一个奇数。\n- 填充：使用零填充。\n- 输出维度：输出空间维度 ($H_{\\text{out}}, W_{\\text{out}}$) 与输入空间维度 ($H, W$) 相匹配。\n- 输入维度：高度 $H$ 和宽度 $W$。\n- 约束条件：$H \\ge k$ 且 $W \\ge k$。\n- 填充量：$p = (k - 1)/2$。\n- 感受野的定义：对于一个给定的输出位置，它是被卷积核线性组合以产生该输出的输入位置（包括填充）的集合。\n- 受填充影响位置的定义：其感受野至少包含一个填充值的输出位置。\n- 任务 1：推导 $F_{\\text{pad}}$，即 $H \\times W$ 输出位置中受填充影响的比例，作为 $H$、$W$ 和 $k$ 的函数。\n- 任务 2：定义 $n(i,j)$ 为其感受野包含坐标为 $(i,j)$（其中 $i \\in \\{1,\\dots,H\\}$ 且 $j \\in \\{1,\\dots,W\\}$）的固定输入像素的输出位置数量。\n- 任务 3：推导比率 $R = n(\\text{center})/n(\\text{corner})$。\n- “中心”像素的定义：任何满足 $i \\in \\{p+1,\\dots,H-p\\}$ 和 $j \\in \\{p+1,\\dots,W-p\\}$ 的像素 $(i,j)$。\n- “角落”像素的定义：坐标为 $(1,1)$ 的像素。\n\n### 步骤 2：使用提取的已知条件进行验证\n问题陈述描述了卷积网络中标准的“相同”或“半”填充场景。对于单个维度，输出大小 $O$、输入大小 $I$、卷积核大小 $k$、填充 $p$ 和步长 $s$ 之间的关系由 $O = \\lfloor \\frac{I - k + 2p}{s} \\rfloor + 1$ 给出。\n给定 $s=1$ 和 $O=I$，该公式变为 $I = I - k + 2p + 1$，可简化为 $k - 2p - 1 = 0$。\n问题将填充量定义为 $p = (k-1)/2$。将其代入一致性条件，得到 $k - 2\\left(\\frac{k-1}{2}\\right) - 1 = k - (k-1) - 1 = k - k + 1 - 1 = 0$。这些定义是内部一致的。\n该问题在科学上基于深度学习的原理，特别是卷积神经网络。术语是标准的，该任务是一个基于卷积性质进行坐标计数的适定数学练习。提供了约束条件 $H \\ge k$ 和 $W \\ge k$，确保了“中心”区域是明确定义且非空的，因为 $H \\ge 2p+1$ 且 $W \\ge 2p+1$。该问题是客观的，没有歧义。\n\n### 步骤 3：结论与行动\n问题陈述有效。我将继续推导解决方案。\n\n### $F_{\\text{pad}}$ 的推导\n输出位置的总数由输出维度的乘积给出，即 $N_{\\text{total}} = H \\times W$。\n\n如果一个输出位置的感受野从填充区域采样，则该位置受填充影响。相反，如果一个输出位置的整个感受野都位于原始的 $H \\times W$ 输入图像内，则它*不受*填充影响。让我们计算这些“安全”位置的数量，$N_{\\text{safe}}$。\n\n设输出网格坐标为 $(u,v)$，其中 $u \\in \\{1, \\dots, H\\}$ 且 $v \\in \\{1, \\dots, W\\}$。位于 $(u,v)$ 的输出的感受野是输入的一个 $k \\times k$ 的区域。在使用“相同”填充的情况下，该区域以相应的输入位置 $(u,v)$ 为中心。卷积核的半径为 $(k-1)/2 = p$。因此，输出 $(u,v)$ 的感受野覆盖的输入像素行索引范围为 $[u-p, u+p]$，列索引范围为 $[v-p, v+p]$。\n\n为了使感受野完全位于原始输入图像内（即不使用任何填充），其所有构成像素的坐标都必须有效。输入像素索引 $(i,j)$ 必须满足 $1 \\le i \\le H$ 和 $1 \\le j \\le W$。\n将此应用于输出 $(u,v)$ 的感受野边界：\n1.  $u-p \\ge 1 \\implies u \\ge p+1$\n2.  $u+p \\le H \\implies u \\le H-p$\n3.  $v-p \\ge 1 \\implies v \\ge p+1$\n4.  $v+p \\le W \\implies v \\le W-p$\n\n因此，不受填充影响的输出位置 $(u,v)$ 满足 $u \\in [p+1, H-p]$ 和 $v \\in [p+1, W-p]$。\n$u$ 的此类整数值的数量为 $(H-p) - (p+1) + 1 = H - 2p$。\n$v$ 的此类整数值的数量为 $(W-p) - (p+1) + 1 = W - 2p$。\n\n不受填充影响的（安全）输出位置总数为 $N_{\\text{safe}} = (H-2p)(W-2p)$。\n受填充影响的位置数量为 $N_{\\text{pad}} = N_{\\text{total}} - N_{\\text{safe}} = HW - (H-2p)(W-2p)$。\n受填充影响的位置所占的比例为：\n$$ F_{\\text{pad}} = \\frac{N_{\\text{pad}}}{N_{\\text{total}}} = \\frac{HW - (H-2p)(W-2p)}{HW} = 1 - \\frac{(H-2p)(W-2p)}{HW} $$\n\n### $R$ 的推导\n量 $n(i,j)$ 表示其感受野包含输入像素 $(i,j)$ 的输出位置的数量。如果输入像素 $(i,j)$ 落在以 $(u,v)$ 为中心的 $k \\times k$ 区域内，则位于 $(u,v)$ 的输出在其感受野中包含输入 $(i,j)$。这意味着：\n$u-p \\le i \\le u+p$ 且 $v-p \\le j \\le v+p$。\n\n为了找到有效输出 $(u,v)$ 的数量，我们可以为 $u$ 和 $v$ 重新排列这些不等式：\n$i-p \\le u \\le i+p$ 且 $j-p \\le v \\le j+p$。\n\n$u$ 和 $v$ 的这些范围必须限制在有效的输出坐标内，即 $u \\in [1, H]$ 和 $v \\in [1, W]$。\n所以，$u$ 的有效范围是 $[\\max(1, i-p), \\min(H, i+p)]$。\n而 $v$ 的有效范围是 $[\\max(1, j-p), \\min(W, j+p)]$。\n那么，整数位置的数量是：\n$n(i,j) = (\\min(H, i+p) - \\max(1, i-p) + 1) \\times (\\min(W, j+p) - \\max(1, j-p) + 1)$。\n\n现在我们计算 $n(\\text{center})$ 和 $n(\\text{corner})$。\n\n**$n(\\text{center})$ 的计算：**\n一个“中心”像素被定义为任何满足 $i \\in [p+1, H-p]$ 和 $j \\in [p+1, W-p]$ 的像素 $(i,j)$。\n对于这样的 $i$，我们有 $i \\ge p+1 \\implies i-p \\ge 1$，所以 $\\max(1, i-p) = i-p$。\n同样，$i \\le H-p \\implies i+p \\le H$，所以 $\\min(H, i+p) = i+p$。\n有效 $u$ 值的数量是 $(i+p) - (i-p) + 1 = 2p+1 = k$。\n类似地，对于这样的 $j$，有效 $v$ 值的数量是 $(j+p) - (j-p) + 1 = 2p+1 = k$。\n因此，$n(\\text{center}) = k \\times k = k^2$。\n\n**$n(\\text{corner})$ 的计算：**\n一个“角落”像素是 $(1,1)$。我们计算 $n(1,1)$。\n对于 $i=1$：$\\max(1, 1-p) = 1$（因为 $p \\ge 0$）。并且 $\\min(H, 1+p) = 1+p$（因为 $H \\ge k = 2p+1 > p+1$）。\n有效 $u$ 值的数量是 $(1+p) - 1 + 1 = p+1$。\n对于 $j=1$：$\\max(1, 1-p) = 1$。并且 $\\min(W, 1+p) = 1+p$（因为 $W \\ge k > p+1$）。\n有效 $v$ 值的数量是 $(1+p) - 1 + 1 = p+1$。\n因此，$n(\\text{corner}) = n(1,1) = (p+1)(p+1) = (p+1)^2$。\n\n**$R$ 的计算：**\n比率 $R$ 为：\n$$ R = \\frac{n(\\text{center})}{n(\\text{corner})} = \\frac{k^2}{(p+1)^2} $$\n\n推导出的对是 $\\left( F_{\\text{pad}}, R \\right) = \\left( 1 - \\dfrac{(H - 2p)(W - 2p)}{HW}, \\dfrac{k^2}{(p+1)^2} \\right)$。\n\n### 逐项分析\n\n**A. $F_{\\text{pad}} = 1 - \\dfrac{(H - 2p)(W - 2p)}{HW}$，以及 $R = \\dfrac{k^2}{(p+1)^2}$。**\n$F_{\\text{pad}}$ 的表达式与我们的推导相符。$R$ 的表达式也与我们的推导相符。\n结论：**正确**。\n\n**B. $F_{\\text{pad}} = \\dfrac{(H - 2p)(W - 2p)}{HW}$，以及 $R = \\dfrac{(p+1)^2}{k^2}$。**\n$F_{\\text{pad}}$ 的表达式代表的是不受填充影响的位置的比例，而不是受填充影响的。这是不正确的。$R$ 的表达式是正确值的倒数。这也是不正确的。\n结论：**不正确**。\n\n**C. $F_{\\text{pad}} = 1 - \\dfrac{(H - k + 1)(W - k + 1)}{HW}$，以及 $R = \\dfrac{k}{p+1}$。**\n$F_{\\text{pad}}$ 的表达式与推导出的表达式等价，因为 $p=(k-1)/2 \\implies 2p = k-1$，使得 $H-2p = H-(k-1) = H-k+1$。所以，这部分是正确的。然而，$R$ 的表达式是 $\\frac{k}{p+1}$，这是我们推导出的值 $\\frac{k^2}{(p+1)^2}$ 的平方根。这是不正确的。\n结论：**不正确**。\n\n**D. $F_{\\text{pad}} = 1 - \\dfrac{(H - 2p + 1)(W - 2p + 1)}{HW}$，以及 $R = \\dfrac{(k - 1)^2}{(p+1)^2}$。**\n$F_{\\text{pad}}$ 的表达式不正确。安全行的数量是 $H-2p$，而不是 $H-2p+1$。$R$ 的表达式不正确。分子应该是 $k^2$，而不是 $(k-1)^2$。\n结论：**不正确**。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "在CNN设计中，奇数尺寸的卷积核（如 $3 \\times 3$ 或 $5 \\times 5$）因其明确的中心像素而备受青睐。本练习将探讨使用偶数尺寸卷积核（如 $2 \\times 2$）所带来的非直观后果，揭示其如何在特征图中引入系统性的半像素空间偏移。通过分析和解决这个问题，你将学会如何在多层网络中精心设计填充策略，以补偿和消除这种累积的对齐误差。",
            "id": "3126199",
            "problem": "考虑卷积神经网络（CNN）中的单个二维卷积，应用于高度为 $H$、宽度为 $W$ 的输入特征图。假设离散像素中心位于整数坐标 $(i,j)$ 处，其中 $i\\in\\{0,1,\\dots,H-1\\}$ 且 $j\\in\\{0,1,\\dots,W-1\\}$。该卷积使用大小为 $2\\times 2$ 的核，步幅为 $1$，并遵循标准的离散卷积约定，即对于位于整数索引 $(u,v)$ 的输出，核窗口锚定在目标感受野的左上角。将感受野中心定义为用于计算输出的窗口内所有像素中心坐标的算术平均值。\n\n您将分析偶数尺寸的核如何影响感受野中心的对齐，并提出填充策略。请仅使用以下基本依据：网格上的离散卷积定义、标准步幅行为以及感受野中心的算术平均值定义。除了 $2\\times 2$ 窗口的左上角锚定外，不要假设任何特殊的实现怪癖。\n\n任务：\n- 从上述定义出发，确定在没有填充的情况下，输出位置 $(u,v)$ 的感受野中心。\n- 为了保持输出尺寸，对于大小为 $2\\times 2$、步幅为 $1$ 的核，一种常见方法是沿每个轴不对称地分配总共 $1$ 个像素的填充。考虑两种具体选择：$(p_{\\text{top}},p_{\\text{bottom}},p_{\\text{left}},p_{\\text{right}})=(0,1,0,1)$ 和 $(1,0,1,0)$。对于每种选择，推导输出位置 $(u,v)$ 的感受野中心，并解释其相对于整数输入网格的偏移。\n- 基于您的推导，确定一个应用于连续 $2$ 个 $2\\times 2$ 卷积层（每个层步幅为 $1$ 且使用保持输出尺寸的填充）的填充方案，使得第二层之后，感受野中心相对于原始输入网格的净空间偏移为零。\n\n哪个选项正确地陈述了对齐结果以及一个有效的、能够减轻累积偏移的填充策略？\n\nA. 在没有填充的情况下，输出 $(u,v)$ 的感受野中心是 $(u+0.5,v+0.5)$。使用保持输出尺寸的填充 $(0,1,0,1)$ 会使感受野中心保持在 $(u+0.5,v+0.5)$。如果第二层使用反向填充 $(1,0,1,0)$，则第二层的感受野中心会重新对齐到相对于原始输入的整数坐标 $(u,v)$，从而抵消净偏移。\n\nB. 使用对称填充 $(0.5,0.5,0.5,0.5)$ 可以将 $2\\times 2$ 的核精确地置于每个输出位置的中心，因此任何层都没有偏移，也无需采取进一步措施。\n\nC. 在每一层都使用保持输出尺寸的填充 $(0,1,0,1)$ 可以保持与整数网格的对齐，因此即使经过多层，也没有累积偏移。\n\nD. 对于 $2\\times 2$ 的核且没有填充，感受野中心位于 $(u,v)$（精确对齐）。只有当步幅超过 $1$ 时才会发生偏移，因此在步幅为 $1$ 的情况下，填充对于对齐是不必要的。",
            "solution": "用户提供了一个关于在卷积神经网络（CNN）中使用偶数尺寸核时感受野中心空间对齐的问题。我将首先验证问题陈述，然后进行完整的推导并评估各个选项。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n- 输入特征图维度：高度 $H$，宽度 $W$。\n- 像素中心位于整数坐标 $(i,j)$，其中 $i \\in \\{0, 1, \\dots, H-1\\}$ 且 $j \\in \\{0, 1, \\dots, W-1\\}$。\n- 卷积核大小：$2 \\times 2$。\n- 步幅：$1$。\n- 卷积约定：“左上角锚定”，意味着对于索引为 $(u,v)$ 的输出，核窗口锚定在输入中感受野的左上角。当步幅为 $1$ 时，这意味着输入块的左上角位于索引 $(u,v)$。\n- 感受野中心定义：核窗口内所有像素中心坐标的算术平均值。\n- 用于分析的填充方案：\n    1. 无填充。\n    2. 用于保持输出尺寸的非对称填充：$(p_{\\text{top}}, p_{\\text{bottom}}, p_{\\text{left}}, p_{\\text{right}}) = (0, 1, 0, 1)$。\n    3. 用于保持输出尺寸的非对称填充：$(p_{\\text{top}}, p_{\\text{bottom}}, p_{\\text{left}}, p_{\\text{right}}) = (1, 0, 1, 0)$。\n- 任务：确定感受野中心，并设计一个跨越 $2$ 层的填充策略，以实现零净空间偏移。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **科学依据**：该问题根植于离散卷积的基本原理，这是CNN的核心组成部分。使用偶数尺寸核时的感受野对齐问题是网络设计中一个已知的、非平凡的方面。所有定义都是标准的。\n- **适定性**：问题定义清晰。核大小、步幅、锚定方式以及感受野中心的定义都已明确说明，从而可以进行唯一的数学推导。\n- **客观性**：语言精确且技术性强，没有歧义或主观论断。\n\n**步骤 3：结论与行动**\n问题陈述是有效的。它在科学上是合理的、适定的和客观的。我将继续进行推导。\n\n### 推导\n\n**1. 无填充情况下的感受野中心**\n\n对于位置 $(u,v)$ 的输出，步幅为 $1$ 且左上角锚定的卷积操作使用输入的一个块。该块的左上角位于输入坐标 $(u,v)$。由于核的大小为 $2 \\times 2$，所涉及的输入像素位于以下整数坐标：\n- $(u, v)$\n- $(u, v+1)$\n- $(u+1, v)$\n- $(u+1, v+1)$\n\n感受野中心是这些坐标的算术平均值。\n中心的水平坐标是：\n$$ C_i = \\frac{u + u + (u+1) + (u+1)}{4} = \\frac{4u + 2}{4} = u + 0.5 $$\n中心的垂直坐标是：\n$$ C_j = \\frac{v + (v+1) + v + (v+1)}{4} = \\frac{4v + 2}{4} = v + 0.5 $$\n因此，在没有填充的情况下，输出 $(u,v)$ 的感受野中心位于 $(u+0.5, v+0.5)$。这表示与整数网格对齐存在半个像素的偏移。\n\n**2. 使用非对称填充时的感受野中心**\n\n为了保持输出尺寸，沿一个轴的总填充量 $P_{\\text{total}}$ 必须满足输出尺寸 $O$、输入尺寸 $I$、核尺寸 $K$ 和步幅 $S$ 之间的关系：\n$$ O = \\left\\lfloor \\frac{I - K + P_{\\text{total}}}{S} \\right\\rfloor + 1 $$\n我们需要 $O=I$。当 $K=2$ 且 $S=1$ 时，公式变为：\n$$ I = \\left\\lfloor \\frac{I - 2 + P_{\\text{total}}}{1} \\right\\rfloor + 1 \\implies I-1 = I-2+P_{\\text{total}} \\implies P_{\\text{total}} = 1 $$\n每个轴需要总共 $1$ 个像素的填充。问题提出了两种非对称的方式来分配此填充。\n\n**情况 A：填充 $(p_{\\text{top}}, p_{\\text{bottom}}, p_{\\text{left}}, p_{\\text{right}}) = (0, 1, 0, 1)$**\n在此方案中，顶部或左侧没有填充。在计算位置 $(u,v)$ 的输出时，核窗口的左上角与填充后输入的位置 $(u,v)$ 对齐。由于 $p_{\\text{top}}=0$ 且 $p_{\\text{left}}=0$，这对应于原始未填充输入网格中的位置 $(u-p_{\\text{top}}, v-p_{\\text{left}}) = (u-0, v-0) = (u,v)$。因此，感受野像素与无填充情况相同：$(u,v), (u,v+1), (u+1,v), (u+1,v+1)$。\n感受野中心保持在 $(u+0.5, v+0.5)$。\n\n**情况 B：填充 $(p_{\\text{top}}, p_{\\text{bottom}}, p_{\\text{left}}, p_{\\text{right}}) = (1, 0, 1, 0)$**\n在此方案中，在顶部和左侧添加了 $1$ 个像素的填充。用于输出 $(u,v)$ 的核窗口的左上角与填充后输入的位置 $(u,v)$ 对齐。这对应于原始输入网格中的位置 $(u-p_{\\text{top}}, v-p_{\\text{left}}) = (u-1, v-1)$。\n因此，$2 \\times 2$ 的感受野覆盖了来自原始输入的以下像素：\n- $(u-1, v-1)$\n- $(u-1, v)$\n- $(u, v-1)$\n- $(u, v)$\n感受野中心是这些坐标的算术平均值。\n中心的水平坐标是：\n$$ C_i = \\frac{(u-1) + (u-1) + u + u}{4} = \\frac{4u - 2}{4} = u - 0.5 $$\n中心的垂直坐标是：\n$$ C_j = \\frac{(v-1) + v + (v-1) + v}{4} = \\frac{4v - 2}{4} = v - 0.5 $$\n因此，使用填充 $(1,0,1,0)$，感受野中心位于 $(u-0.5, v-0.5)$。这是一个相反方向的半像素偏移。\n\n**3. 实现零净偏移的两层填充策略**\n\n让我们分析两个连续的 $2 \\times 2$ 卷积层，二者的步幅均为 $1$。我们的目标是找到一个填充方案，使得最终输出的感受野中心与原始输入网格完全对齐。让我们连续使用上面分析的两种填充方案。\n\n假设第 1 层使用填充 $(0,1,0,1)$，第 2 层使用填充 $(1,0,1,0)$。\n- **第 1 层**：输入是原始数据 $X_0$。输出是 $X_1$。对于一个输出 $X_1(u_1, v_1)$，它在 $X_0$ 中的感受野中心位于 $(u_1+0.5, v_1+0.5)$。我们将此变换表示为 $T_1((u_1,v_1)) = (u_1+0.5, v_1+0.5)$。\n- **第 2 层**：输入是 $X_1$，输出是 $X_2$。对于一个输出 $X_2(u,v)$，它在 $X_1$ 中的感受野中心位于 $(u-0.5, v-0.5)$。\n\n为了找到输出 $X_2(u,v)$ 在原始网格 $X_0$ 中的最终感受野中心位置，我们必须找到第 2 层的感受野中心在第 0 层的网格中映射回的位置。$X_2(u,v)$ 的计算中心位于 $X_1$ 网格中的坐标 $(u-0.5, v-0.5)$。我们将变换 $T_1$ 应用于此点：\n$$ C_{\\text{final}}(u,v) = T_1((u-0.5, v-0.5)) $$\n水平坐标是 $(u-0.5) + 0.5 = u$。\n垂直坐标是 $(v-0.5) + 0.5 = v$。\n在原始输入网格中的最终感受野中心是 $(u,v)$。这实现了零净偏移的完美对齐。交替使用非对称填充方案的策略是有效的。\n\n### 逐项分析\n\n**A. 在没有填充的情况下，输出 $(u,v)$ 的感受野中心是 $(u+0.5,v+0.5)$。使用保持输出尺寸的填充 $(0,1,0,1)$ 会使感受野中心保持在 $(u+0.5,v+0.5)$。如果第二层使用反向填充 $(1,0,1,0)$，则第二层的感受野中心会重新对齐到相对于原始输入的整数坐标 $(u,v)$，从而抵消净偏移。**\n- 第一个关于无填充的陈述，根据推导1是正确的。中心是 $(u+0.5, v+0.5)$。\n- 第二个关于填充 $(0,1,0,1)$ 的陈述，根据推导2情况A是正确的。中心是 $(u+0.5, v+0.5)$。\n- 第三个描述两层策略的陈述，根据推导3是正确的。交替使用填充 $(0,1,0,1)$ 和 $(1,0,1,0)$ 可以抵消半像素偏移，最终使净感受野中心位于 $(u,v)$。\n**结论：正确**\n\n**B. 使用对称填充 $(0.5,0.5,0.5,0.5)$ 可以将 $2\\times 2$ 的核精确地置于每个输出位置的中心，因此任何层都没有偏移，也无需采取进一步措施。**\n问题定义了整数坐标上的离散像素中心和标准的离散卷积。填充值表示要添加的整数行数或列数。$0.5$ 的小数填充在此框架内没有定义，并且在标准的离散卷积库中，如果不引入插值（一个未包含在问题基本依据中的概念），是无法物理实现的。此选项提出了一个超出问题既定规则的解决方案。\n**结论：错误**\n\n**C. 在每一层都使用保持输出尺寸的填充 $(0,1,0,1)$ 可以保持与整数网格的对齐，因此即使经过多层，也没有累积偏移。**\n如推导2情况A所示，填充 $(0,1,0,1)$ 导致感受野中心在 $(u+0.5, v+0.5)$。这并*没有*与整数网格对齐。如果重复应用此填充，偏移会累积。对于具有相同填充的第二层，相对于原始输入的新中心将位于 $((u+0.5)+0.5, (v+0.5)+0.5) = (u+1, v+1)$。这是一个累积偏移，与该选项的说法相矛盾。\n**结论：错误**\n\n**D. 对于 $2\\times 2$ 的核且没有填充，感受野中心位于 $(u,v)$（精确对齐）。只有当步幅超过 $1$ 时才会发生偏移，因此在步幅为 $1$ 的情况下，填充对于对齐是不必要的。**\n第一个陈述是错误的。如推导1所示，无填充时的感受野中心位于 $(u+0.5, v+0.5)$，这是未对齐的。未对齐是由偶数尺寸的核造成的，因为它缺少一个中心像素。声称只有当步幅大于 $1$ 时才会发生偏移的说法也是错误的；在这种情况下，偏移是核几何形状固有的。\n**结论：错误**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "平移等变性是卷积神经网络成功的理论基石之一，它保证了网络对目标在图像中的位置具有鲁棒性。然而，在实际应用中，诸如填充（padding）、步幅（striding）和池化（pooling）等标准操作会破坏这种理想的特性。这个编码实践提供了一个完整的实验框架，用于精确测量这种“等变性误差”，并检验旨在恢复等变性的高级技术（如抗混叠），从而将理论性质与实际工程挑战联系起来。",
            "id": "3126243",
            "problem": "要求您设计并实现一个实验，用于测量和分析二维卷积神经网络（CNNs）中的平移等变性误差。设输入为单通道图像，由矩阵 $x \\in \\mathbb{R}^{H \\times W}$ 表示。定义作用于矩阵 $z \\in \\mathbb{R}^{H \\times W}$ 上的离散循环平移算子 $T_{\\delta}$ 为 $T_{\\delta} z[i,j] = z[(i + \\delta_x) \\bmod H, (j + \\delta_y) \\bmod W]$，其中平移向量为 $\\delta = (\\delta_x,\\delta_y)$，所有索引均对 $H$ 和 $W$ 取模。对于一个网络 $f$，其对于平移 $\\delta$ 的等变性误差定义为\n$$\nE(f, x, \\delta) = \\left\\| f\\!\\left(T_{\\delta} x\\right) - T_{\\delta'} \\, f(x) \\right\\|_2 \\, ,\n$$\n其中 $\\|\\cdot\\|_2$ 是矩阵的弗罗贝尼乌斯范数（Frobenius norm），而 $\\delta' = \\left(\\left\\lfloor \\frac{\\delta_x}{s_{\\text{eff}}} \\right\\rfloor, \\left\\lfloor \\frac{\\delta_y}{s_{\\text{eff}}} \\right\\rfloor\\right)$ 是由 $f$ 的总下采样因子 $s_{\\text{eff}}$ 引起的输出空间平移。为了使结果在不同架构和平移之间具有直接可比性，使用相对误差\n$$\nE_{\\text{rel}}(f, x, \\delta) = \\frac{\\left\\| f\\!\\left(T_{\\delta} x\\right) - T_{\\delta'} \\, f(x) \\right\\|_2}{\\left\\| f(x) \\right\\|_2 + \\varepsilon} \\, ,\n$$\n其中 $\\varepsilon = 10^{-12}$ 以避免除以零。\n\n根据经过充分测试的公式和核心定义，构建以下确定性组件：\n\n1. 大小为 $H = W = 32$ 的图像 $x$，对于 $i \\in \\{0,\\dots,31\\}$ 和 $j \\in \\{0,\\dots,31\\}$ 定义为\n$$\nx[i,j] = \\sin\\!\\left(\\frac{2\\pi i}{H}\\right) + \\frac{1}{2}\\cos\\!\\left(\\frac{2\\pi j}{W}\\right) + \\frac{1}{4} \\exp\\!\\left( - \\frac{(i - c)^2 + (j - c)^2}{2 \\sigma^2} \\right),\n$$\n其中 $c = 16$ 且 $\\sigma = 3$。\n\n2. 卷积核：\n- 令 $g = \\frac{1}{16}[1, 4, 6, 4, 1]$，并定义可分离的二维高斯核 $k_1[u,v] = g[u] \\, g[v]$，其中 $u,v \\in \\{0,\\dots,4\\}$。\n- 令\n$$\nk_2 = \\begin{bmatrix}\n0  -1  0 \\\\\n-1  4  -1 \\\\\n0  -1  0\n\\end{bmatrix}.\n$$\n\n3. 非线性：使用逐点应用的修正线性单元（ReLU），$\\phi(a) = \\max(a, 0)$。\n\n定义以下网络 $f$（所有卷积均为二维，单输入通道到单输出通道）：\n\n- 基线 $f_{\\text{base}}$：使用步长为 1、循环填充（以保持空间尺寸）的卷积核 $k_1$ 进行卷积，然后应用 $\\phi$，接着使用步长为 1、循环填充的卷积核 $k_2$ 进行卷积，并再次应用 $\\phi$。有效下采样因子为 $s_{\\text{eff}} = 1$。\n\n- 填充变体 $f_{\\text{pad}}$：与 $f_{\\text{base}}$ 相同，但在两次卷积中均使用零填充代替循环填充。有效下采样因子为 $s_{\\text{eff}} = 1$。\n\n- 步长变体 $f_{\\text{stride}}$：与 $f_{\\text{base}}$ 相同，但第一次卷积使用步长 2（带循环填充）。有效下采样因子为 $s_{\\text{eff}} = 2$。\n\n- 池化变体 $f_{\\text{pool}}$：与 $f_{\\text{base}}$ 相同，但在第一个 $\\phi$ 之后插入一个步长为 2 的 $2 \\times 2$ 最大池化层。有效下采样因子为 $s_{\\text{eff}} = 2$。\n\n- 步长补救方案 $f_{\\text{stride-rem}}$：将 $f_{\\text{stride}}$ 的带步长卷积替换为抗混叠下采样：首先使用步长为 1 和循环填充的卷积核 $k_1$ 进行卷积，然后与由 $k_b[u,v] = g[u] \\, g[v]$ 定义的可分离模糊核 $k_b$ 进行卷积，最后通过在每个空间维度上每隔 2 个像素取样来进行子采样。然后继续应用 $\\phi$，接着使用步长为 1（循环填充）的卷积核 $k_2$ 进行卷积，再应用 $\\phi$。有效下采样因子为 $s_{\\text{eff}} = 2$。\n\n- 池化补救方案 $f_{\\text{pool-rem}}$：与 $f_{\\text{base}}$ 相同，但在第一个 $\\phi$ 之后，首先与 $k_b$ 进行卷积（循环填充），然后执行步长为 2 的 $2 \\times 2$ 平均池化（这等同于在不重叠的窗口上进行均匀低通滤波），然后继续使用步长为 1（循环填充）的卷积核 $k_2$ 进行卷积，再应用 $\\phi$。有效下采样因子为 $s_{\\text{eff}} = 2$。\n\n使用以下平移测试集\n$$\n\\mathcal{D} = \\left\\{ (0,0), (1,0), (2,0), (3,5), (15,16) \\right\\}.\n$$\n\n您的任务是：\n1. 仅使用指定的卷积核和逐点非线性函数来实现上述网络。\n2. 对于每个 $\\delta \\in \\mathcal{D}$，计算 $f \\in \\{ f_{\\text{base}}, f_{\\text{pad}}, f_{\\text{stride}}, f_{\\text{pool}}, f_{\\text{stride-rem}}, f_{\\text{pool-rem}} \\}$ 的 $E_{\\text{rel}}(f, x, \\delta)$。\n3. 基于这些结果，通过有限差分估计填充、步长和池化对等变性误差的加性贡献\n$$\n\\Delta_{\\text{pad}} = \\operatorname{mean}_{\\delta \\in \\mathcal{D}} \\left( E_{\\text{rel}}(f_{\\text{pad}}, x, \\delta) - E_{\\text{rel}}(f_{\\text{base}}, x, \\delta) \\right),\n$$\n$$\n\\Delta_{\\text{stride}} = \\operatorname{mean}_{\\delta \\in \\mathcal{D}} \\left( E_{\\text{rel}}(f_{\\text{stride}}, x, \\delta) - E_{\\text{rel}}(f_{\\text{base}}, x, \\delta) \\right),\n$$\n$$\n\\Delta_{\\text{pool}} = \\operatorname{mean}_{\\delta \\in \\mathcal{D}} \\left( E_{\\text{rel}}(f_{\\text{pool}}, x, \\delta) - E_{\\text{rel}}(f_{\\text{base}}, x, \\delta) \\right).\n$$\n同时报告平均基线误差\n$$\n\\overline{E}_{\\text{base}} = \\operatorname{mean}_{\\delta \\in \\mathcal{D}} E_{\\text{rel}}(f_{\\text{base}}, x, \\delta).\n$$\n\n4. 通过计算平均改进量来量化所提出的补救方案的效果\n$$\nI_{\\text{stride}} = \\operatorname{mean}_{\\delta \\in \\mathcal{D}} \\left( E_{\\text{rel}}(f_{\\text{stride}}, x, \\delta) - E_{\\text{rel}}(f_{\\text{stride-rem}}, x, \\delta) \\right),\n$$\n$$\nI_{\\text{pool}} = \\operatorname{mean}_{\\delta \\in \\mathcal{D}} \\left( E_{\\text{rel}}(f_{\\text{pool}}, x, \\delta) - E_{\\text{rel}}(f_{\\text{pool-rem}}, x, \\delta) \\right).\n$$\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果按以下顺序排列：\n$$\n\\left[ \\overline{E}_{\\text{base}}, \\Delta_{\\text{pad}}, \\Delta_{\\text{stride}}, \\Delta_{\\text{pool}}, I_{\\text{stride}}, I_{\\text{pool}} \\right].\n$$\n\n所有计算均为纯数值计算，不含物理单位。三角函数内的角度以弧度为单位。每个度量的答案必须是浮点数。通过严格遵循上述定义并使用指定的卷积核和算子来确保科学真实性。提供的测试集 $\\mathcal{D}$ 包括一个理想情况（happy-path case）$\\delta = (0,0)$、小范围平移和旨在检验边界和下采样效果的大范围平移。无需用户输入；所有参数均按规定固定。",
            "solution": "用户要求进行一项数值实验，以分析各种二维卷积神经网络（CNN）架构中的平移等变性误差。该问题定义明确、科学上合理且计算上可行。它为构建输入数据、网络组件和评估指标提供了一个精确的数学框架。以下解决方案实现了指定的实验，计算了所需的误差指标，并提供了一个完整的、可运行的程序。\n\n### 1. 理论框架和核心组件\n\n该实验以平移等变性的概念为中心。如果将函数应用于平移后的输入与平移函数的输出是相同的结果，即 $f(T_{\\delta}x) = T_{\\delta'}f(x)$，则函数 $f$ 对平移算子 $T_{\\delta}$ 是等变的。算子 $T_{\\delta'}$ 是输出空间中相应的平移，该空间可能已被下采样。问题定义了一个相对误差度量 $E_{\\text{rel}}(f, x, \\delta)$，用于量化与完美等变性的偏差。\n\n首先，我们构建实验的基本组件。\n\n**输入图像：** 输入图像 $x$ 是一个 $32 \\times 32$ 的矩阵，由以下函数定义：\n$$\nx[i,j] = \\sin\\!\\left(\\frac{2\\pi i}{H}\\right) + \\frac{1}{2}\\cos\\!\\left(\\frac{2\\pi j}{W}\\right) + \\frac{1}{4} \\exp\\!\\left( - \\frac{(i - c)^2 + (j - c)^2}{2 \\sigma^2} \\right)\n$$\n参数为 $H=W=32$，$c=16$，以及 $\\sigma=3$。这种正弦波和高斯斑点的叠加提供了一个同时包含低频和高频分量的信号，适合用于测试卷积。\n\n**平移算子：** 离散循环平移 $T_{\\delta}$ 将输入图像按向量 $\\delta = (\\delta_x, \\delta_y)$ 进行平移，并带有环绕边界。这通过使用 `numpy.roll` 来实现，该函数沿指定轴对数组元素执行循环移位。\n\n**卷积核：** 定义了两个核：\n- 一个 $5 \\times 5$ 的可分离高斯模糊核 $k_1$，源自向量 $g = \\frac{1}{16}[1, 4, 6, 4, 1]$。这是一个低通滤波器。\n- 一个 $3 \\times 3$ 的拉普拉斯核 $k_2$，它充当高通滤波器或边缘检测器。\n\n**非线性：** 修正线性单元（ReLU），$\\phi(a) = \\max(a, 0)$，在卷积和池化阶段之后逐元素应用。这引入了非线性，这是深度网络的一个关键组成部分。\n\n### 2. 网络操作\n\n指定的网络由一系列基本操作构建而成。\n\n**卷积：** 二维卷积是核心操作。问题指定了两种填充策略：\n- **循环填充：** 这确保了步长为1的卷积具有完美的等变性，因为边界条件与循环平移算子 $T_{\\delta}$ 一致。它使用 `scipy.signal.convolve2d` 并设置参数 `mode='same'` 和 `boundary='wrap'` 来实现。\n- **零填充：** 这种常见的替代方案破坏了完美的等变性，因为平移输入图像会改变信号与零填充边界交互的部分。它使用 `scipy.signal.convolve2d` 并设置 `mode='same'` 和默认的 `boundary='fill'` 来实现。\n\n**下采样操作：** 步进和池化操作会减小特征图的空间维度，这是标准CNN中等变性误差的主要来源。\n- **带步长的卷积：** 步长大于1的卷积。通过执行标准的步长为1的卷积，然后对输出网格进行子采样（例如，对于步长2，每隔一个元素取样）来实现。\n- **最大池化：** 一种下采样操作，取输入局部块中的最大值。对于 $2 \\times 2$ 的核和步长2，这通过将一个 $(H, W)$ 的特征图重塑为 $(H/2, 2, W/2, 2)$ 并对新创建的轴取最大值来实现。\n- **平均池化：** 与最大池化类似，但在局部块上取平均值。\n\n### 3. 网络架构与分析\n\n我们实现了六个网络架构（$f_{\\text{base}}, f_{\\text{pad}}, f_{\\text{stride}}, f_{\\text{pool}}, f_{\\text{stride-rem}}, f_{\\text{pool-rem}}$）作为不同的函数。每个函数按指定的顺序组合了上述操作。\n\n- $f_{\\text{base}}$ 作为理想化的、“最具等变性”的模型，使用步长为1的卷积和循环填充。其误差在理想情况下应接近于零（受浮点精度限制）。\n- $f_{\\text{pad}}$ 分离了使用零填充而非循环填充所带来的误差贡献。\n- $f_{\\text{stride}}$ 和 $f_{\\text{pool}}$ 展示了由朴素的步进和最大池化引入的显著等变性误差，这些操作以一种移位可变的方式丢弃信息。\n- $f_{\\text{stride-rem}}$ 和 $f_{\\text{pool-rem}}$ 实现了抗混叠技术。通过在下采样（子采样或平均池化）之前应用一个低通滤波器（模糊核 $k_b$），它们旨在减轻由混叠引起的误差，从而改善等变性。\n\n### 4. 计算与最终指标\n\n主计算循环遍历每个网络和每个指定的平移 $\\delta \\in \\mathcal{D}$。对于每一对 $(f, \\delta)$，它通过执行以下步骤来计算相对等变性误差 $E_{\\text{rel}}(f, x, \\delta)$：\n1. 计算原始图像的输出：$y = f(x)$。\n2. 平移输入图像：$x_{\\text{shifted}} = T_{\\delta} x$。\n3. 计算平移后图像的输出：$y_{\\text{shifted_in}} = f(x_{\\text{shifted}})$。\n4. 根据网络的有效步长平移原始输出：$y_{\\text{shifted_out}} = T_{\\delta'} y$，其中 $\\delta' = (\\lfloor \\delta_x / s_{\\text{eff}} \\rfloor, \\lfloor \\delta_y / s_{\\text{eff}} \\rfloor)$。\n5. 计算差异的弗罗贝尼乌斯范数 $\\| y_{\\text{shifted_in}} - y_{\\text{shifted_out}} \\|_2$ 和原始输出的弗罗贝尼乌斯范数 $\\|y\\|_2$。\n6. 计算最终的相对误差 $E_{\\text{rel}}$。\n\n在收集所有误差值之后，我们计算问题中定义的最终摘要指标：$\\overline{E}_{\\text{base}}$（平均基线误差）、$\\Delta_{\\text{pad}}, \\Delta_{\\text{stride}}, \\Delta_{\\text{pool}}$（由填充、步进和池化引起的误差贡献），以及 $I_{\\text{stride}}, I_{\\text{pool}}$（由抗混叠补救方案带来的改进）。这些指标是通过对相关误差值或其在平移集 $\\mathcal{D}$ 上的差值取平均来计算的。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import signal\n\ndef solve():\n    \"\"\"\n    Implements and analyzes translation equivariance error in various 2D CNNs.\n    \"\"\"\n    # Define constants and test parameters from the problem statement.\n    H, W = 32, 32\n    c, sigma = 16, 3\n    epsilon = 1e-12\n    shifts = [(0, 0), (1, 0), (2, 0), (3, 5), (15, 16)]\n\n    # 1. Construct the input image x\n    i, j = np.mgrid[0:H, 0:W]\n    x = (np.sin(2 * np.pi * i / H) +\n         0.5 * np.cos(2 * np.pi * j / W) +\n         0.25 * np.exp(-((i - c)**2 + (j - c)**2) / (2 * sigma**2)))\n\n    # 2. Define convolution kernels\n    g = np.array([1, 4, 6, 4, 1]) / 16.0\n    k1 = np.outer(g, g)\n    k2 = np.array([[0, -1, 0], [-1, 4, -1], [0, -1, 0]])\n    kb = k1  # The blur kernel is the same as k1\n\n    # Helper functions for network operations\n    def translate(mat, delta):\n        return np.roll(mat, shift=delta, axis=(0, 1))\n\n    def relu(mat):\n        return np.maximum(0, mat)\n\n    def do_conv(img, kernel, stride, padding):\n        if padding == 'circular':\n            convolved = signal.convolve2d(img, kernel, boundary='wrap', mode='same')\n        elif padding == 'zero':\n            convolved = signal.convolve2d(img, kernel, boundary='fill', fillvalue=0, mode='same')\n        else:\n            raise ValueError(f\"Unsupported padding type: {padding}\")\n        return convolved[::stride, ::stride]\n\n    def max_pool_2x2(mat):\n        h_in, w_in = mat.shape\n        h_out, w_out = h_in // 2, w_in // 2\n        return mat.reshape(h_out, 2, w_out, 2).max(axis=(1, 3))\n\n    def avg_pool_2x2(mat):\n        h_in, w_in = mat.shape\n        h_out, w_out = h_in // 2, w_in // 2\n        return mat.reshape(h_out, 2, w_out, 2).mean(axis=(1, 3))\n\n    # 3. Define the six network architectures\n    def f_base(img):\n        h = do_conv(img, k1, stride=1, padding='circular')\n        h = relu(h)\n        h = do_conv(h, k2, stride=1, padding='circular')\n        h = relu(h)\n        return h\n\n    def f_pad(img):\n        h = do_conv(img, k1, stride=1, padding='zero')\n        h = relu(h)\n        h = do_conv(h, k2, stride=1, padding='zero')\n        h = relu(h)\n        return h\n\n    def f_stride(img):\n        h = do_conv(img, k1, stride=2, padding='circular')\n        h = relu(h)\n        h = do_conv(h, k2, stride=1, padding='circular')\n        h = relu(h)\n        return h\n\n    def f_pool(img):\n        h = do_conv(img, k1, stride=1, padding='circular')\n        h = relu(h)\n        h = max_pool_2x2(h)\n        h = do_conv(h, k2, stride=1, padding='circular')\n        h = relu(h)\n        return h\n\n    def f_stride_rem(img):\n        h = do_conv(img, k1, stride=1, padding='circular')\n        h = do_conv(h, kb, stride=1, padding='circular')\n        h = h[::2, ::2]  # Subsample after blurring\n        h = relu(h)\n        h = do_conv(h, k2, stride=1, padding='circular')\n        h = relu(h)\n        return h\n\n    def f_pool_rem(img):\n        h = do_conv(img, k1, stride=1, padding='circular')\n        h = relu(h)\n        h = do_conv(h, kb, stride=1, padding='circular')\n        h = avg_pool_2x2(h)\n        h = do_conv(h, k2, stride=1, padding='circular')\n        h = relu(h)\n        return h\n\n    networks = {\n        'base':       (f_base, 1),\n        'pad':        (f_pad, 1),\n        'stride':     (f_stride, 2),\n        'pool':       (f_pool, 2),\n        'stride-rem': (f_stride_rem, 2),\n        'pool-rem':   (f_pool_rem, 2)\n    }\n\n    errors = {name: [] for name in networks}\n\n    # 4. Calculate equivariance error for each network and shift\n    for name, (f, s_eff) in networks.items():\n        y = f(x)\n        norm_y = np.linalg.norm(y)\n\n        for delta in shifts:\n            delta_x, delta_y = delta\n            x_shifted = translate(x, delta)\n            y_shifted_in = f(x_shifted)\n            \n            delta_prime = (delta_x // s_eff, delta_y // s_eff)\n            y_shifted_out = translate(y, delta_prime)\n            \n            norm_diff = np.linalg.norm(y_shifted_in - y_shifted_out)\n            e_rel = norm_diff / (norm_y + epsilon)\n            errors[name].append(e_rel)\n\n    # 5. Compute final metrics\n    mean_errors = {name: np.mean(err_list) for name, err_list in errors.items()}\n\n    E_base_mean = mean_errors['base']\n    Delta_pad = mean_errors['pad'] - mean_errors['base']\n    Delta_stride = mean_errors['stride'] - mean_errors['base']\n    Delta_pool = mean_errors['pool'] - mean_errors['base']\n    I_stride = mean_errors['stride'] - mean_errors['stride-rem']\n    I_pool = mean_errors['pool'] - mean_errors['pool-rem']\n\n    results = [E_base_mean, Delta_pad, Delta_stride, Delta_pool, I_stride, I_pool]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}