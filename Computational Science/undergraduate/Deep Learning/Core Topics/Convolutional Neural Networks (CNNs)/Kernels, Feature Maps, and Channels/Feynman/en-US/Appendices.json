{
    "hands_on_practices": [
        {
            "introduction": "A foundational principle in modern CNN design is the preference for stacking smaller kernels over using a single large one. This practice delves into why this choice is so powerful by comparing a single $5 \\times 5$ convolutional layer with a stack of two successive $3 \\times 3$ layers. By calculating the receptive field and parameter counts for both architectures , you will gain a concrete understanding of how deeper, narrower networks can be more efficient and expressive.",
            "id": "3126220",
            "problem": "Consider a Convolutional Neural Network (CNN), defined as a sequence of layers where each layer performs a discrete convolution on a multi-channel input with learnable kernels and an optional pointwise nonlinearity. Let the input feature map be $x \\in \\mathbb{R}^{H \\times W \\times C}$ with $C$ channels. All convolutions in this problem use stride $1$, zero-padding that preserves spatial dimensions, and no dilation. A single convolutional layer maps $\\mathbb{R}^{H \\times W \\times C}$ to $\\mathbb{R}^{H \\times W \\times C}$ by aggregating information at each output location from the spatial neighborhood covered by its kernel and across all input channels. The pointwise nonlinearity $\\phi$ is applied independently to each spatial position and each channel.\n\nTwo architectures are considered:\n\n- Architecture A: One convolutional layer with a $5 \\times 5$ kernel followed by one application of the nonlinearity $\\phi$.\n- Architecture B: Two successive convolutional layers, each with a $3 \\times 3$ kernel, where the nonlinearity $\\phi$ is applied after each of the two layers.\n\nAssume both architectures preserve the number of channels so that each layer maps $C$ input channels to $C$ output channels. Each output channel in a convolutional layer is produced by one learnable kernel that connects to all $C$ input channels and one scalar bias for that output channel.\n\nTasks:\n1. Using the definition of receptive field growth under composition of stride-$1$ convolutions, determine the exact side length $R$ of the receptive field, measured as the number of distinct input spatial positions influencing a single output spatial position in the second layer of Architecture B.\n2. Using the definition that each output channel in a convolutional layer has one learnable kernel connected to all $C$ input channels of spatial size $k \\times k$ and one scalar bias, derive the total number of learnable parameters in Architecture A and Architecture B as algebraic expressions in $C$.\n3. Justify why Architecture B can emulate Architecture A when the intermediate nonlinearity is set to the identity function, and state the number of pointwise nonlinearities applied per output location in Architecture A and Architecture B when $\\phi$ is nontrivial.\n\nReport your final answer as a single row vector in the order $\\left(R,\\; P_{\\text{A}},\\; P_{\\text{B}},\\; N_{\\text{A}},\\; N_{\\text{B}}\\right)$, where $P_{\\text{A}}$ and $P_{\\text{B}}$ are the total parameter counts for Architecture A and Architecture B, respectively, and $N_{\\text{A}}$ and $N_{\\text{B}}$ are the numbers of pointwise nonlinearities applied per output location. Express all quantities exactly; no rounding is required.",
            "solution": "The problem requires a comparative analysis of two Convolutional Neural Network (CNN) architectures, denoted as Architecture A and Architecture B, based on receptive field size, parameter count, and functional properties.\n\n### Step 1: Validation of the Problem Statement\nFirst, a critical validation of the problem statement is performed.\n\n**Extraction of Givens:**\n- Input feature map: $x \\in \\mathbb{R}^{H \\times W \\times C}$ with $C$ channels.\n- Convolution properties: stride is $1$, zero-padding preserves spatial dimensions, no dilation.\n- Layer mapping: $\\mathbb{R}^{H \\times W \\times C} \\to \\mathbb{R}^{H \\times W \\times C}$ (channel count $C$ is preserved).\n- Pointwise nonlinearity: $\\phi$, applied independently to each spatial position and channel.\n- Architecture A: One convolutional layer with a $5 \\times 5$ kernel, followed by one application of $\\phi$.\n- Architecture B: Two successive convolutional layers, each with a $3 \\times 3$ kernel, with $\\phi$ applied after each layer.\n- Parameter definition: Each output channel is produced by one learnable kernel of size $k \\times k$ connecting to all $C$ input channels, plus one scalar bias.\n\n**Validation:**\n1.  **Scientifically Grounded:** The problem is firmly rooted in the fundamental principles of deep learning and CNNs. The concepts of receptive fields, parameter calculation, and architectural comparison (e.g., a single large kernel vs. stacked smaller kernels) are standard and essential topics in the field.\n2.  **Well-Posed:** All necessary conditions (stride, padding, channel mapping) are explicitly defined, ensuring that the quantities to be derived ($R$, $P_{\\text{A}}$, $P_{\\text{B}}$, $N_{\\text{A}}$, $N_{\\text{B}}$) have unique and meaningful solutions.\n3.  **Objective:** The problem is stated using precise, formal, and unbiased language, free of subjective claims.\n\nThe problem statement is self-contained, scientifically sound, and well-posed. It has no discernible flaws.\n\n**Verdict:** The problem is valid.\n\n### Step 2: Solution Derivation\n\n**Task 1: Receptive Field of Architecture B**\n\nThe receptive field of a neuron defines the region of the input space that affects its activation. For a sequence of convolutional layers, the receptive field size grows with each layer. The side length of the receptive field, $R_l$, after layer $l$ can be calculated recursively. Given a receptive field $R_{l-1}$ for the previous layer, a kernel of size $k_l \\times k_l$, and a stride $S_l$, the new receptive field is:\n$$R_l = R_{l-1} + (k_l - 1) \\prod_{i=1}^{l-1} S_i$$\nLet the input (layer $0$) have a receptive field of $R_0 = 1$. The problem specifies that all strides are $S=1$. The formula simplifies to a simple sum:\n$$R_l = R_{l-1} + (k_l - 1)$$\nArchitecture B has two layers, both with a kernel size of $k_1 = k_2 = 3$.\n\nFor the first layer of Architecture B:\n$$R_1 = R_0 + (k_1 - 1) = 1 + (3 - 1) = 3$$\nThis means a neuron in the output of the first layer sees a $3 \\times 3$ patch of the input.\n\nFor the second layer of Architecture B, its input is the output of the first layer. The receptive field $R$ for a neuron in the output of the second layer is:\n$$R = R_2 = R_1 + (k_2 - 1) = 3 + (3 - 1) = 5$$\nThus, the side length of the receptive field for Architecture B is $5$.\n\n**Task 2: Number of Learnable Parameters**\n\nThe number of parameters in a convolutional layer is the sum of its weights and biases. For a layer with $C_{\\text{in}}$ input channels, $C_{\\text{out}}$ output channels, and a kernel of size $k \\times k$:\n- Number of weights = $C_{\\text{out}} \\times (k \\times k \\times C_{\\text{in}})$.\n- Number of biases = $C_{\\text{out}}$.\nTotal parameters $P = (k^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}) + C_{\\text{out}}$.\nIn this problem, all layers preserve the channel dimension, so $C_{\\text{in}} = C_{\\text{out}} = C$. The formula for a single layer's parameters becomes:\n$$P_{\\text{layer}} = k^2 C^2 + C$$\n\n- **Architecture A:** It has one layer with a $5 \\times 5$ kernel ($k=5$). The total number of parameters, $P_{\\text{A}}$, is:\n$$P_{\\text{A}} = 5^2 C^2 + C = 25C^2 + C$$\n\n- **Architecture B:** It has two successive layers, each with a $3 \\times 3$ kernel ($k=3$).\n  - Parameters for the first layer ($P_{\\text{B1}}$): $P_{\\text{B1}} = 3^2 C^2 + C = 9C^2 + C$.\n  - Parameters for the second layer ($P_{\\text{B2}}$): The input to this layer is the output of the first, which also has $C$ channels. So, $P_{\\text{B2}} = 3^2 C^2 + C = 9C^2 + C$.\nThe total number of parameters, $P_{\\text{B}}$, is the sum from both layers:\n$$P_{\\text{B}} = P_{\\text{B1}} + P_{\\text{B2}} = (9C^2 + C) + (9C^2 + C) = 18C^2 + 2C$$\n\n**Task 3: Emulation and Number of Nonlinearities**\n\n- **Emulation Justification:**\nA convolution is a linear operation (specifically, a linear map composed of cross-correlation and bias addition). Architecture B, with the intermediate nonlinearity $\\phi$ set to the identity function ($\\phi(z)=z$), becomes a composition of two linear convolutional layers. The composition of two linear transformations is itself a linear transformation. Let the two layers be $L_1(x) = W_1 * x + b_1$ and $L_2(y) = W_2 * y + b_2$. The composite function is $L_B(x) = L_2(L_1(x)) = W_2 * (W_1 * x + b_1) + b_2$. By associativity of convolution, this is equivalent to $(W_2 * W_1) * x + \\text{bias terms}$, which can be written as $W_{\\text{eff}} * x + b_{\\text{eff}}$. The convolution of a $3 \\times 3$ kernel ($W_1$) with another $3 \\times 3$ kernel ($W_2$) yields an effective kernel $W_{\\text{eff}}$ of size $5 \\times 5$. This matches the kernel size and, as shown in Task 1, the receptive field of the single layer in Architecture A. Therefore, Architecture B, without its intermediate nonlinearity, is functionally a linear convolutional layer with a $5 \\times 5$ receptive field, which is the same structure as Architecture A.\n\n- **Number of Pointwise Nonlinearities:**\nThis quantity, per output location, is interpreted as the number of non-linear transformations a signal passes through in sequence. An \"application\" of the nonlinearity refers to a stage in the network's data flow.\n  - **Architecture A ($N_{\\text{A}}$):** The architecture is $\\text{conv}_{5 \\times 5} \\rightarrow \\phi$. There is only one layer of nonlinearity applied after the single convolutional layer. Thus, $N_{\\text{A}} = 1$.\n  - **Architecture B ($N_{\\text{B}}$):** The architecture is $\\text{conv}_{3 \\times 3} \\rightarrow \\phi \\rightarrow \\text{conv}_{3 \\times 3} \\rightarrow \\phi$. The nonlinearity is applied after each of the two convolutional layers. A signal from input to output passes through two sequential nonlinear stages. Thus, $N_{\\text{B}} = 2$.\n\n**Final Computations Summary:**\n- $R = 5$\n- $P_{\\text{A}} = 25C^2 + C$\n- $P_{\\text{B}} = 18C^2 + 2C$\n- $N_{\\text{A}} = 1$\n- $N_{\\text{B}} = 2$\nThe final answer is the row vector $(R, P_{\\text{A}}, P_{\\text{B}}, N_{\\text{A}}, N_{\\text{B}})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n5 & 25C^{2} + C & 18C^{2} + 2C & 1 & 2\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While stacking small kernels improves parameter efficiency, modern architectures for mobile and embedded devices require even greater computational savings. This exercise explores depthwise separable convolutions, the core building block of networks like MobileNet, which factorize a standard convolution into two simpler steps. By calculating the exact number of Multiply-Accumulate (MAC) operations from first principles , you will quantify the dramatic speedup this technique provides and understand its impact on deploying deep learning models in resource-constrained environments.",
            "id": "3120106",
            "problem": "A MobileNet Version $1$ (MobileNetV$1$) stage at spatial resolution $H=W=112$ transforms an input tensor with $C_{in}=32$ channels into an output tensor with $C_{out}=64$ channels using a convolution with kernel size $K=3$ and stride $1$. Assume zero padding that preserves spatial dimensions. Consider two implementations of this stage: (i) a standard convolution with $K \\times K$ filters across all input channels and (ii) a depthwise separable convolution composed of a depthwise $K \\times K$ convolution followed by a pointwise $1 \\times 1$ convolution. Using first principles of discrete convolution and operation counting, compute the exact total number of Multiply-Accumulate operations (MACs), where a Multiply-Accumulate (MAC) is defined as one multiplication paired with one addition, for both implementations across the full feature maps. Then, as the comparison metric, compute the speedup factor defined as the ratio of the standard convolution MAC count to the depthwise separable convolution MAC count. Report only the speedup factor as a single simplified exact expression in your final answer. No rounding is required.",
            "solution": "The fundamental base is the definition of discrete convolution and operation counting. For each output element of a convolutional layer, the computation involves summing products between a local receptive field of the input and the corresponding filter weights. Each product and its accumulation is counted as one Multiply-Accumulate (MAC).\n\nFor a standard convolution:\n- There are $H \\times W$ spatial positions and $C_{out}$ output channels, yielding $H W C_{out}$ output elements.\n- Each output element is computed from a $K \\times K$ spatial neighborhood across all $C_{in}$ input channels, resulting in $K^{2} C_{in}$ MACs per output element.\nTherefore, the total MAC count for the standard convolution is\n$$\n\\text{MAC}_{\\text{std}} = H W C_{out} K^{2} C_{in}.\n$$\n\nFor a depthwise separable convolution, the computation is split into two parts:\n\n1. Depthwise convolution:\n- There are $H \\times W$ spatial positions and $C_{in}$ separate depthwise filters, one per input channel, yielding $H W C_{in}$ output elements.\n- Each depthwise output element uses a $K \\times K$ kernel limited to its own channel, contributing $K^{2}$ MACs per output element.\nThus, the depthwise MAC count is\n$$\n\\text{MAC}_{\\text{dw}} = H W C_{in} K^{2}.\n$$\n\n2. Pointwise $1 \\times 1$ convolution:\n- There are $H \\times W$ spatial positions and $C_{out}$ output channels, yielding $H W C_{out}$ output elements.\n- Each pointwise output element is a weighted sum over $C_{in}$ input channels with a $1 \\times 1$ kernel, contributing $C_{in}$ MACs per output element.\nThus, the pointwise MAC count is\n$$\n\\text{MAC}_{\\text{pw}} = H W C_{out} C_{in}.\n$$\n\nCombining the two, the total MAC count for the depthwise separable convolution is\n$$\n\\text{MAC}_{\\text{dws}} = \\text{MAC}_{\\text{dw}} + \\text{MAC}_{\\text{pw}} = H W C_{in} K^{2} + H W C_{out} C_{in} = H W C_{in} \\left(K^{2} + C_{out}\\right).\n$$\n\nThe speedup factor $S$ is defined as the ratio of the standard convolution MAC count to the depthwise separable convolution MAC count:\n$$\nS = \\frac{\\text{MAC}_{\\text{std}}}{\\text{MAC}_{\\text{dws}}} = \\frac{H W C_{out} K^{2} C_{in}}{H W C_{in} \\left(K^{2} + C_{out}\\right)}.\n$$\nCanceling the common factors $H$, $W$, and $C_{in}$ gives\n$$\nS = \\frac{C_{out} K^{2}}{K^{2} + C_{out}}.\n$$\n\nSubstitute the given values $C_{in}=32$, $C_{out}=64$, $K=3$, $H=W=112$ into the simplified symbolic expression for $S$:\n$$\nS = \\frac{64 \\cdot 3^{2}}{3^{2} + 64} = \\frac{64 \\cdot 9}{9 + 64} = \\frac{576}{73}.\n$$\n\nFor completeness, we can verify the exact MAC counts numerically:\n- Standard convolution:\n$$\n\\text{MAC}_{\\text{std}} = 112 \\cdot 112 \\cdot 64 \\cdot 9 \\cdot 32 = 12544 \\cdot 64 \\cdot 288 = 231{,}211{,}008.\n$$\n- Depthwise separable convolution:\n$$\n\\text{MAC}_{\\text{dws}} = 112 \\cdot 112 \\cdot 32 \\cdot \\left(9 + 64\\right) = 12544 \\cdot 32 \\cdot 73 = 29{,}302{,}784.\n$$\nTheir ratio indeed equals\n$$\n\\frac{231{,}211{,}008}{29{,}302{,}784} = \\frac{576}{73}.\n$$\n\nThe requested final output is the speedup factor as a single simplified exact expression.",
            "answer": "$$\\boxed{\\frac{576}{73}}$$"
        },
        {
            "introduction": "Effective feature maps contain channels with varying levels of importance. This practice introduces the concept of channel-wise attention through the Squeeze-and-Excitation (SE) block, a module that allows a network to learn to dynamically re-weight its own feature channels. You will implement the SE block's core mechanisms—global information squeezing and adaptive excitation—to understand how a network can intelligently recalibrate its features to improve performance .",
            "id": "3139403",
            "problem": "You are asked to implement a channel-wise Squeeze-and-Excitation (SE) block and to quantify its parameter overhead from first principles. The SE block operates on a three-dimensional input tensor with channel, height, and width axes. The operations you must implement are defined below using only core definitions of linear maps and pointwise nonlinearities.\n\nGiven an input tensor $x \\in \\mathbb{R}^{C \\times H \\times W}$, define the squeeze operation by the channel-wise global average\n$$\ns_c \\;=\\; \\frac{1}{H\\,W}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} x_{cij}, \\quad \\text{for } c \\in \\{0,\\dots,C-1\\}.\n$$\nStacking $s_c$ yields the descriptor vector $s \\in \\mathbb{R}^{C}$. Define the excitation as a two-layer Multilayer Perceptron (MLP) with reduction ratio $r$, where $r$ is a positive integer that divides $C$. Let $\\mathrm{ReLU}(u) = \\max\\{0,u\\}$ denote the Rectified Linear Unit and $\\sigma(u)=\\frac{1}{1+e^{-u}}$ denote the logistic sigmoid. With learned parameters $W_1 \\in \\mathbb{R}^{(C/r)\\times C}$, $b_1 \\in \\mathbb{R}^{C/r}$, $W_2 \\in \\mathbb{R}^{C \\times (C/r)}$, and $b_2 \\in \\mathbb{R}^{C}$, the excitation output $z \\in \\mathbb{R}^{C}$ is\n$$\nt \\;=\\; \\mathrm{ReLU}(W_1 s + b_1), \\qquad z \\;=\\; \\sigma(W_2 t + b_2).\n$$\nFinally, scale the input tensor channel-wise by $z$,\n$$\ny_{cij} \\;=\\; x_{cij}\\, z_c, \\quad \\text{for all } c,i,j,\n$$\nto obtain the SE-augmented tensor $y \\in \\mathbb{R}^{C \\times H \\times W}$.\n\nParameter overhead is defined as the number of additional parameters introduced by the excitation MLP relative to a baseline without the SE block. For two untied linear layers, the weight count is\n$$\n\\#\\text{weights}_{\\text{untied}} \\;=\\; C \\cdot \\frac{C}{r} \\;+\\; \\frac{C}{r} \\cdot C \\;=\\; \\frac{2C^2}{r},\n$$\nand the bias count is\n$$\n\\#\\text{biases} \\;=\\; \\frac{C}{r} \\;+\\; C.\n$$\nIn a tied-weights variant where $W_2 = W_1^{\\top}$, the independent weights count reduces to\n$$\n\\#\\text{weights}_{\\text{tied}} \\;=\\; C \\cdot \\frac{C}{r} \\;=\\; \\frac{C^2}{r}.\n$$\n\nImplement the SE block exactly as specified and compute the following outputs for each test case:\n- The sum of the squeezed descriptor, $\\sum_{c=0}^{C-1} s_c$.\n- The sum of all elements of the scaled output, $\\sum_{c,i,j} y_{cij}$, rounded to six decimal places.\n- The untied-weights overhead $\\#\\text{weights}_{\\text{untied}}$.\n- The tied-weights overhead $\\#\\text{weights}_{\\text{tied}}$.\n- The untied-weights-plus-biases overhead $\\#\\text{weights}_{\\text{untied}} + \\#\\text{biases}$.\n\nYour program must use the following test suite. In all cases, $C$ is divisible by $r$ and all definitions below must be implemented exactly.\n\nTest case A (happy path):\n- $C = 4$, $H = 2$, $W = 2$, $r = 2$.\n- Input defined by $x_{cij} = c - i + j$ for $c \\in \\{0,1,2,3\\}$, $i \\in \\{0,1\\}$, $j \\in \\{0,1\\}$.\n- Parameters:\n$$\nW_1 \\;=\\; \\begin{bmatrix}\n1 & -1 & 0.5 & 0 \\\\\n0.25 & 0.5 & -0.5 & 1\n\\end{bmatrix}, \\quad\nb_1 \\;=\\; \\begin{bmatrix} -0.5 \\\\ 0 \\end{bmatrix},\n$$\n$$\nW_2 \\;=\\; \\begin{bmatrix}\n1 & 0.5 \\\\\n-0.5 & 0.25 \\\\\n0 & 1 \\\\\n0.75 & -1\n\\end{bmatrix}, \\quad\nb_2 \\;=\\; \\begin{bmatrix} 0 \\\\ 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}.\n$$\n\nTest case B (boundary on spatial extent, $H W = 1$):\n- $C = 8$, $H = 1$, $W = 1$, $r = 2$.\n- Input defined by $x_{c00} = c + 1$ for $c \\in \\{0,1,\\dots,7\\}$.\n- Parameters defined elementwise by index-based formulas. Let $a$ index rows and $b$ index columns, both zero-based:\n$$\nW_1[a,b] \\;=\\; \\frac{a\\cdot 8 + b - 12}{16}, \\quad \\text{for } a \\in \\{0,1,2,3\\}, \\; b \\in \\{0,1,\\dots,7\\},\n$$\n$$\nb_1 \\;=\\; \\begin{bmatrix} -0.25 \\\\ 0 \\\\ 0.1 \\\\ -0.1 \\end{bmatrix}, \\quad\nW_2[a,b] \\;=\\; \\frac{a\\cdot 4 + b - 8}{8}, \\quad \\text{for } a \\in \\{0,1,\\dots,7\\}, \\; b \\in \\{0,1,2,3\\},\n$$\n$$\nb_2 \\;=\\; \\mathbf{0} \\in \\mathbb{R}^{8}.\n$$\n\nTest case C (edge case $r = 1$ with sign-alternating channels):\n- $C = 6$, $H = 3$, $W = 1$, $r = 1$.\n- Input defined by $x_{ci0} = (-1)^c \\cdot (i+1)$ for $c \\in \\{0,1,\\dots,5\\}$ and $i \\in \\{0,1,2\\}$.\n- Parameters:\n$$\nW_1 \\;=\\; 0.5\\, I_6 \\;-\\; 0.25\\,(\\mathbf{1}_6 \\mathbf{1}_6^{\\top} - I_6), \\quad b_1 \\;=\\; \\mathbf{0} \\in \\mathbb{R}^{6},\n$$\n$$\nW_2 \\;=\\; 0.3\\, I_6 \\;-\\; 0.05\\, \\mathbf{1}_6 \\mathbf{1}_6^{\\top}, \\quad b_2 \\;=\\; \\begin{bmatrix} -0.1 \\\\ -0.05 \\\\ 0 \\\\ 0.05 \\\\ 0.1 \\\\ 0.15 \\end{bmatrix},\n$$\nwhere $I_6$ is the $6 \\times 6$ identity matrix and $\\mathbf{1}_6$ is the $6$-vector of ones.\n\nAngle units and physical units do not apply here. All numeric outputs should be real numbers in standard decimal form.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all three test cases as a list of lists. Each inner list must be ordered as\n$$\n\\left[ \\sum_{c} s_c,\\; \\mathrm{round}\\!\\left(\\sum_{c,i,j} y_{cij},\\,6\\right),\\; \\#\\text{weights}_{\\text{untied}},\\; \\#\\text{weights}_{\\text{tied}},\\; \\#\\text{weights}_{\\text{untied}} + \\#\\text{biases} \\right].\n$$\nFor example, the printed structure must look like\n$$\n\\big[\\,[\\dots],\\,[\\dots],\\,[\\dots]\\,\\big].\n$$",
            "solution": "The problem requires the implementation and analysis of a Squeeze-and-Excitation (SE) block, a common architectural component in deep learning, based on its fundamental mathematical definition. The solution involves a step-by-step execution of the defined operations for three distinct test cases. The core principles are channel-wise feature recalibration through a data-driven mechanism.\n\nThe SE block operates on an input tensor $x \\in \\mathbb{R}^{C \\times H \\times W}$, where $C$ is the number of channels, and $H$ and $W$ are the spatial height and width, respectively. The process consists of three main stages: Squeeze, Excitation, and Scale.\n\n**1. Squeeze: Global Information Embedding**\nThe first step, \"Squeeze,\" aggregates feature maps across their spatial dimensions ($H \\times W$) to produce a channel descriptor. This is achieved using global average pooling, which calculates the mean value for each channel. The resulting vector $s \\in \\mathbb{R}^{C}$ embeds a global receptive field for each channel. The formula for the $c$-th element of $s$ is:\n$$\ns_c = \\frac{1}{H \\cdot W}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} x_{cij}\n$$\nIn implementation, this corresponds to taking the mean of the input tensor $x$ along its spatial axes (axes $1$ and $2$). The first required output for each test case is the sum of the elements of this squeezed vector, $\\sum_{c=0}^{C-1} s_c$.\n\n**2. Excitation: Adaptive Recalibration**\nThe \"Excitation\" stage generates a set of per-channel modulation weights, or \"activations,\" from the squeezed descriptor $s$. This is accomplished by a small neural network, specifically a two-layer Multilayer Perceptron (MLP). The MLP is structured to first reduce the channel dimensionality and then restore it, creating a computational bottleneck that helps capture non-linear channel interdependencies.\n\nThe MLP consists of:\n- A dimensionality-reduction linear layer with weights $W_1 \\in \\mathbb{R}^{(C/r)\\times C}$ and biases $b_1 \\in \\mathbb{R}^{C/r}$, where $r$ is the reduction ratio.\n- A Rectified Linear Unit ($\\mathrm{ReLU}$) activation function, defined as $\\mathrm{ReLU}(u) = \\max\\{0,u\\}$.\n- A dimensionality-increasing linear layer with weights $W_2 \\in \\mathbb{R}^{C \\times (C/r)}$ and biases $b_2 \\in \\mathbb{R}^{C}$.\n- A logistic sigmoid activation function, $\\sigma(u)=\\frac{1}{1+e^{-u}}$, which normalizes the output to the range $(0, 1)$.\n\nThe computation proceeds as follows:\n$$\nt = \\mathrm{ReLU}(W_1 s + b_1)\n$$\n$$\nz = \\sigma(W_2 t + b_2)\n$$\nThe resulting vector $z \\in \\mathbb{R}^{C}$ contains the channel-wise scaling factors.\n\n**3. Scale: Feature Recalibration**\nThe final stage applies the learned scaling factors from the excitation step to the original input tensor $x$. Each channel of the input tensor is multiplied by its corresponding scaling factor from the vector $z$. This recalibrates the feature maps, amplifying informative channels and suppressing less useful ones. The output tensor $y \\in \\mathbb{R}^{C \\times H \\times W}$ is given by:\n$$\ny_{cij} = x_{cij} \\cdot z_c\n$$\nIn implementation, this is achieved by broadcasting the scaling vector $z$ (reshaped to have dimensions $C \\times 1 \\times 1$) over the input tensor $x$. The second required output is the sum of all elements in the final scaled tensor, $\\sum_{c,i,j} y_{cij}$, which is calculated as $\\sum_c (z_c \\cdot \\sum_{i,j} x_{cij}) = H \\cdot W \\cdot \\sum_c (z_c \\cdot s_c)$.\n\n**4. Parameter Overhead Calculation**\nThe problem also requires quantifying the number of additional learnable parameters introduced by the SE block's MLP. The parameters consist of weights and biases from the two linear layers.\n- **Untied Weights:** The total number of weights when $W_1$ and $W_2$ are independent is the sum of the elements in both matrices: $(\\frac{C}{r} \\times C) + (C \\times \\frac{C}{r}) = \\frac{2C^2}{r}$.\n- **Tied Weights:** In a variant where $W_2 = W_1^{\\top}$, the number of independent weights reduces to the size of $W_1$, which is $\\frac{C^2}{r}$.\n- **Biases:** The number of bias parameters is the sum of the elements in $b_1$ and $b_2$, which is $\\frac{C}{r} + C$.\n- **Total Untied Overhead:** The total number of parameters for the untied-weights case is the sum of untied weights and biases: $\\frac{2C^2}{r} + \\frac{C}{r} + C$.\n\nThe implementation will compute these five quantities for each of the three test cases provided, using the specified input tensors and MLP parameters.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and format results for all test cases.\n    \"\"\"\n\n    def se_block_and_overhead(C, H, W, r, x, W1, b1, W2, b2):\n        \"\"\"\n        Implements the SE block operations and calculates parameter overhead.\n\n        Args:\n            C (int): Number of channels.\n            H (int): Height of the input tensor.\n            W (int): Width of the input tensor.\n            r (int): Reduction ratio.\n            x (np.ndarray): Input tensor of shape (C, H, W).\n            W1 (np.ndarray): Weights of the first linear layer.\n            b1 (np.ndarray): Biases of the first linear layer.\n            W2 (np.ndarray): Weights of the second linear layer.\n            b2 (np.ndarray): Biases of the second linear layer.\n\n        Returns:\n            list: A list containing the five required output values.\n        \"\"\"\n        # 1. Squeeze Operation: Global Average Pooling\n        # s_c = (1/(H*W)) * sum(x_cij for i,j)\n        s = np.mean(x, axis=(1, 2))\n        sum_s = np.sum(s)\n\n        # 2. Excitation Operation: MLP\n        # t = ReLU(W1*s + b1)\n        t = np.maximum(0, W1 @ s + b1)\n        \n        # z = sigmoid(W2*t + b2)\n        z_raw = W2 @ t + b2\n        z = 1 / (1 + np.exp(-z_raw))\n\n        # 3. Scale Operation\n        # y_cij = x_cij * z_c\n        # Reshape z to (C, 1, 1) for broadcasting\n        y = x * z.reshape((C, 1, 1))\n        sum_y = np.sum(y)\n\n        # 4. Parameter Overhead Calculation\n        weights_untied = (2 * C**2) // r\n        weights_tied = (C**2) // r\n        biases = (C // r) + C\n        total_untied_with_biases = weights_untied + biases\n        \n        return [\n            sum_s,\n            round(sum_y, 6),\n            weights_untied,\n            weights_tied,\n            total_untied_with_biases\n        ]\n\n    results = []\n\n    # --- Test Case A ---\n    C_A, H_A, W_A, r_A = 4, 2, 2, 2\n    x_A = np.fromfunction(lambda c, i, j: c - i + j, (C_A, H_A, W_A))\n    W1_A = np.array([\n        [1, -1, 0.5, 0],\n        [0.25, 0.5, -0.5, 1]\n    ])\n    b1_A = np.array([-0.5, 0])\n    W2_A = np.array([\n        [1, 0.5],\n        [-0.5, 0.25],\n        [0, 1],\n        [0.75, -1]\n    ])\n    b2_A = np.array([0, 0.1, -0.2, 0.3])\n    results.append(se_block_and_overhead(C_A, H_A, W_A, r_A, x_A, W1_A, b1_A, W2_A, b2_A))\n\n    # --- Test Case B ---\n    C_B, H_B, W_B, r_B = 8, 1, 1, 2\n    x_B = (np.arange(C_B) + 1).reshape(C_B, H_B, W_B)\n    W1_B = np.fromfunction(lambda a, b: (a * 8 + b - 12) / 16, (C_B // r_B, C_B))\n    b1_B = np.array([-0.25, 0, 0.1, -0.1])\n    W2_B = np.fromfunction(lambda a, b: (a * 4 + b - 8) / 8, (C_B, C_B // r_B))\n    b2_B = np.zeros(C_B)\n    results.append(se_block_and_overhead(C_B, H_B, W_B, r_B, x_B, W1_B, b1_B, W2_B, b2_B))\n\n    # --- Test Case C ---\n    C_C, H_C, W_C, r_C = 6, 3, 1, 1\n    c_vec_C = (-1)**np.arange(C_C)\n    i_vec_C = np.arange(1, H_C + 1)\n    x_C = (c_vec_C[:, np.newaxis, np.newaxis]\n           * i_vec_C[np.newaxis, :, np.newaxis])\n    \n    I6 = np.identity(C_C)\n    J6 = np.ones((C_C, C_C))\n    \n    W1_C = 0.5 * I6 - 0.25 * (J6 - I6)\n    b1_C = np.zeros(C_C)\n    W2_C = 0.3 * I6 - 0.05 * J6\n    b2_C = np.array([-0.1, -0.05, 0, 0.05, 0.1, 0.15])\n    results.append(se_block_and_overhead(C_C, H_C, W_C, r_C, x_C, W1_C, b1_C, W2_C, b2_C))\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to a string and join them with commas.\n    # The outer brackets are added by the f-string.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}