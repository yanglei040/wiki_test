{
    "hands_on_practices": [
        {
            "introduction": "要真正理解一个操作，最好的方法莫过于亲手实现它。本练习将通过简单的循环，将二维卷积实现为“滑动点积”的过程，从而揭开其神秘面纱。通过将你的实现结果与标准库的计算结果进行比对 ，你将能够验证自己对核心机制的理解。",
            "id": "3180075",
            "problem": "要求您从第一性原理出发，演示深度学习中使用的二维（$2\\mathrm{D}$）离散卷积如何被理解为滑动点积，并使用一个可信的数值库来验证一个朴素实现。请从有限数组上离散卷积的基本定义出发，将图像视为实值矩阵 $X \\in \\mathbb{R}^{H \\times W}$，将卷积核视为实值矩阵 $K \\in \\mathbb{R}^{R \\times S}$。使用以下约束以确保科学上的真实性和清晰性：在“有效”（valid）配置下（即无填充，步幅为 $1$，无空洞卷积）执行卷积，在滑动前沿两个轴翻转卷积核，并且不假定任何优化或专用硬件。\n\n您的程序必须将该操作实现为通过朴素循环计算的滑动点积：对于每个空间位置，取 $X$ 中大小为 $R \\times S$ 的相应子矩阵，沿两个轴翻转 $K$，并计算两者之间的标量积。独立地，使用 Scientific Python (SciPy) 计算相同的结果，以作为实现的“神谕”（oracle），并在指定的数值容差内验证相等性。然后汇总在一组测试用例上的验证结果。\n\n使用以下测试套件，它涵盖了一般情况、边界情况、矩形核、类单位元行为以及混合符号的浮点值。在以下所有情况中，数组都在 $\\mathbb{R}$ 中，并应严格按照书面形式进行解释。\n\n$X_1 = \\begin{bmatrix}\n1  2  0  -1 \\\\\n3  -1  2  0 \\\\\n0  1  -2  3\n\\end{bmatrix},\\quad\nK_1 = \\begin{bmatrix}\n1  0 \\\\\n0  -1\n\\end{bmatrix}.$\n\n$X_2 = \\begin{bmatrix}\n2  -1 \\\\\n0  4\n\\end{bmatrix},\\quad\nK_2 = \\begin{bmatrix}\n-1  2 \\\\\n3  0\n\\end{bmatrix}.$\n\n$X_3 = \\begin{bmatrix}\n0.5  -1.0  2.0  0.0  1.5 \\\\\n1.0  3.0  -2.0  4.0  -0.5 \\\\\n0.0  -1.5  2.5  -3.5  0.0 \\\\\n2.0  0.0  1.0  -1.0  2.0\n\\end{bmatrix},\\quad\nK_3 = \\begin{bmatrix}\n1.0  -1.0  0.5 \\\\\n0.0  2.0  -0.5\n\\end{bmatrix}.$\n\n$X_4 = \\begin{bmatrix}\n1.0  -2.0  3.0 \\\\\n-1.0  0.0  2.0 \\\\\n4.0  -3.0  1.0\n\\end{bmatrix},\\quad\nK_4 = \\begin{bmatrix}\n1.0\n\\end{bmatrix}.$\n\n$X_5 = \\begin{bmatrix}\n0.1  -0.2  0.3 \\\\\n1.5  0.0  -0.5 \\\\\n-0.7  2.2  1.1 \\\\\n3.0  -1.0  0.0 \\\\\n0.0  0.5  -2.0\n\\end{bmatrix},\\quad\nK_5 = \\begin{bmatrix}\n0.5  -0.5 \\\\\n1.0  0.0 \\\\\n-1.0  0.5\n\\end{bmatrix}.$\n\n验证和输出要求：\n- 通过将 $K_i$ 沿两个轴翻转并以步幅 $1$ 滑动，计算每个 $X_i$ 和 $K_i$ 之间的朴素“有效”卷积。\n- 使用 Scientific Python (SciPy) 计算相同的“有效”卷积。\n- 对于每个测试用例 $i \\in \\{1,2,3,4,5\\}$，在相对和绝对容差均为 $10^{-12}$ 的条件下验证数值相等性，并返回一个布尔值，以指示两个结果是否在容差范围内逐元素一致。\n- 您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，顺序与上面给出的测试套件一致，例如 $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$，其中每个条目都是一个布尔值。\n\n不涉及物理单位或角度单位。将数值容差表示为 $10^{-12}$，并确保所有比较都严格遵守此容差。每个测试用例的最终输出类型是一个布尔值，聚合输出是单行布尔值列表。",
            "solution": "问题要求将二维（$2\\mathrm{D}$）离散卷积演示为一系列滑动点积，从第一性原理出发进行实现，并与一个标准库函数进行验证。\n\n基本操作是实值矩阵 $X \\in \\mathbb{R}^{H \\times W}$（图像）与实值矩阵 $K \\in \\mathbb{R}^{R \\times S}$（卷积核）的二维离散卷积。设输出矩阵为 $Y$。对于“有效”（valid）卷积模式，当步幅为 $1$ 且无填充时，输出矩阵 $Y$ 的维度为 $(H - R + 1) \\times (W - S + 1)$。\n\n输出 $Y$ 在每个位置 $(m, n)$ 的值由卷积和定义：\n$$\nY(m, n) = (X * K)(m, n) = \\sum_{i=0}^{R-1} \\sum_{j=0}^{S-1} X(m+i, n+j) \\cdot K(R-1-i, S-1-j)\n$$\n此公式适用于输出索引 $m$ 和 $n$ 满足 $0 \\le m \\le H-R$ 且 $0 \\le n \\le W-S$ 的情况。表达式 $K(R-1-i, S-1-j)$ 表示将卷积核 $K$ 沿其水平和垂直轴同时翻转（或反转）。\n\n让我们将翻转后的卷积核 $K_r$ 定义为：\n$$\nK_r(i, j) = K(R-1-i, S-1-j)\n$$\n有了这个定义，卷积公式简化为：\n$$\nY(m, n) = \\sum_{i=0}^{R-1} \\sum_{j=0}^{S-1} X(m+i, n+j) \\cdot K_r(i, j)\n$$\n这种形式揭示了“滑动点积”的解释。设 $X_{m,n}$ 表示 $X$ 中大小为 $R \\times S$ 且左上角位于位置 $(m, n)$ 的子矩阵。该子矩阵的元素为 $X_{m,n}(i, j) = X(m+i, n+j)$。那么 $Y(m,n)$ 的表达式就是子矩阵 $X_{m,n}$ 与翻转后的卷积核 $K_r$ 逐元素乘积之和。这个操作等价于两个矩阵的 Frobenius 内积：\n$$\nY(m, n) = \\langle X_{m,n}, K_r \\rangle_F\n$$\n从概念上讲，翻转后的卷积核 $K_r$ 在图像 $X$ 上滑动。在每个位置 $(m, n)$，计算卷积核与下方图像块之间的点积，其结果成为输出矩阵 $Y$ 中的相应条目。\n\n对每个测试用例，实现将分三步进行：\n1.  **朴素实现**：将创建一个函数，从第一性原理出发实现滑动点积。该函数将首先显式地翻转卷积核 $K$ 以得到 $K_r$。然后，它将遍历图像 $X$ 中所有有效的左上角位置 $(m, n)$。对于每个位置，它将提取 $R \\times S$ 的子矩阵 $X_{m,n}$，计算其与 $K_r$ 的逐元素乘积，并将结果求和以产生输出值 $Y(m, n)$。\n\n2.  **库实现**：将使用带有 `mode='valid'` 参数的 `scipy.signal.convolve2d` 函数作为“神谕”。该函数是二维卷积操作的高度优化且可信的实现。根据定义，它在计算滑动乘积和之前，内部执行卷积核翻转。因此，其输出在数学上应与我们朴素实现的结果相同。\n\n3.  **验证**：将比较两种实现的数值结果。`numpy.allclose` 函数适合此任务，因为它在指定的容差内对两个数组进行逐元素比较。按照要求，相对容差（`rtol`）和绝对容差（`atol`）都将设置为 $10^{-12}$。对于每个测试用例，如果数组在此容差内相等，函数将返回 `True`，否则返回 `False`。将收集所有测试用例的布尔结果，并以指定的格式呈现。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import convolve2d\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing a naive 2D convolution,\n    validating it against SciPy's implementation, and reporting\n    the comparison results for a suite of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([[1, 2, 0, -1], [3, -1, 2, 0], [0, 1, -2, 3]], dtype=np.float64),\n         np.array([[1, 0], [0, -1]], dtype=np.float64)),\n        (np.array([[2, -1], [0, 4]], dtype=np.float64),\n         np.array([[-1, 2], [3, 0]], dtype=np.float64)),\n        (np.array([[0.5, -1.0, 2.0, 0.0, 1.5],\n                   [1.0, 3.0, -2.0, 4.0, -0.5],\n                   [0.0, -1.5, 2.5, -3.5, 0.0],\n                   [2.0, 0.0, 1.0, -1.0, 2.0]], dtype=np.float64),\n         np.array([[1.0, -1.0, 0.5], [0.0, 2.0, -0.5]], dtype=np.float64)),\n        (np.array([[1.0, -2.0, 3.0], [-1.0, 0.0, 2.0], [4.0, -3.0, 1.0]], dtype=np.float64),\n         np.array([[1.0]], dtype=np.float64)),\n        (np.array([[0.1, -0.2, 0.3],\n                   [1.5, 0.0, -0.5],\n                   [-0.7, 2.2, 1.1],\n                   [3.0, -1.0, 0.0],\n                   [0.0, 0.5, -2.0]], dtype=np.float64),\n         np.array([[0.5, -0.5], [1.0, 0.0], [-1.0, 0.5]], dtype=np.float64))\n    ]\n\n    def naive_convolution_2d(X, K):\n        \"\"\"\n        Computes the 2D \"valid\" convolution from first principles\n        as a sliding dot product.\n        \n        Args:\n            X (np.ndarray): The input matrix (image).\n            K (np.ndarray): The kernel matrix.\n            \n        Returns:\n            np.ndarray: The result of the convolution.\n        \"\"\"\n        H, W = X.shape\n        R, S = K.shape\n\n        # Dimensions of the output matrix for 'valid' convolution\n        H_out = H - R + 1\n        W_out = W - S + 1\n\n        # Handle cases where kernel is larger than image\n        if H_out = 0 or W_out = 0:\n            return np.array([[]], dtype=np.float64)\n\n        output = np.zeros((H_out, W_out))\n        \n        # Reverse the kernel across both axes\n        K_reversed = K[::-1, ::-1]\n\n        # Iterate over all possible top-left positions of the kernel on the image\n        for m in range(H_out):\n            for n in range(W_out):\n                # Extract the submatrix of X\n                sub_X = X[m : m + R, n : n + S]\n                \n                # Compute the dot product (sum of element-wise products)\n                # and assign it to the output matrix\n                output[m, n] = np.sum(sub_X * K_reversed)\n        \n        return output\n\n    results = []\n    # Set the numerical tolerance as specified in the problem\n    tolerance = 1e-12\n\n    for (X, K) in test_cases:\n        # 1. Compute the convolution using the naive implementation\n        result_naive = naive_convolution_2d(X, K)\n\n        # 2. Compute the convolution using the SciPy library function\n        result_scipy = convolve2d(X, K, mode='valid')\n        \n        # 3. Verify numerical equality within the specified tolerance\n        are_equal = np.allclose(result_naive, result_scipy, rtol=tolerance, atol=tolerance)\n        \n        # The Python bool 'True' will be capitalized, so we convert it to\n        # lowercase 'true' if needed, but the problem indicates the Python\n        # representation is fine.\n        results.append(str(are_equal))\n\n    # Final print statement in the exact required format.\n    # The map to str correctly handles Python's booleans (True, False).\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在学习了卷积的运作方式之后，我们将探索它为何在深度学习中如此高效。本练习通过一个思想实验，对比了标准卷积层与一个不具备权重共享的假想“局部连接”层。通过这个计算练习 ，你将亲身体会到权重共享带来的巨大参数节省，并理解其为何是深度卷积网络能够成功构建的关键。",
            "id": "3180099",
            "problem": "卷积神经网络（CNN）层通过在输入特征图上滑动一个空间核，并在每个空间位置重用相同的核权重来应用二维卷积。考虑一个输入张量，其空间维度为 $H \\times W = 56 \\times 56$，并有 $C_{\\text{in}} = 64$ 个通道。你应用一个卷积层，其方形核的空间大小为 $k \\times k$（其中 $k = 3$），步幅 $s = 1$，零填充 $p = 1$（因此输出空间尺寸得以保留），以及 $C_{\\text{out}} = 128$ 个输出通道。假设卷积层中为每个输出通道都包含一个加性偏置。\n\n现在定义一个假设的替代层，它通过在每个输出空间位置上，取对应的 $k \\times k \\times C_{\\text{in}}$ 输入区块，将其展平，并对 $C_{\\text{out}}$ 个输出应用全连接（仿射）变换，来产生相同的输出形状。关键在于，这个替代层在空间位置之间没有权重共享：每个空间位置都有自己独立的权重和偏置，即每个位置的每个输出通道都有一个偏置。\n\n仅从卷积在所有空间位置上重用相同核权重，而替代层在每个位置使用独立权重的定义出发，推导出每种情况下可训练参数的总数（用 $k$、$C_{\\text{in}}$、$C_{\\text{out}}$、$H$、$W$、$p$ 和 $s$ 表示），然后代入给定的数值进行计算。计算参数节省因子 $S$，其定义为无共享替代方案中的参数数量与卷积层中的参数数量之比：\n$$\nS \\equiv \\frac{\\text{无共享替代方案中的参数}}{\\text{卷积中的参数}}。\n$$\n给出所述配置下 $S$ 的最终值。最终答案必须是单个实数，无需四舍五入。",
            "solution": "用户希望我分析一个标准的二维卷积层，并将其与一个在空间位置上不共享权重的假设“局部连接”层进行比较。\n\n以下是我的计划：\n1.  **验证问题**：我将检查问题陈述的科学合理性、完整性和清晰度。\n    *   **提取已知条件**：\n        *   输入张量维度：$H \\times W = 56 \\times 56$，$C_{\\text{in}} = 64$。\n        *   卷积层参数：核大小 $k \\times k$ 且 $k=3$，步幅 $s=1$，填充 $p=1$，输出通道 $C_{\\text{out}} = 128$。\n        *   每个输出通道包含一个加性偏置。\n    *   **替代层定义**：\n        *   对大小为 $k \\times k \\times C_{\\text{in}}$ 的输入区块进行操作。\n        *   每个区块被展平并通过一个全连接（仿射）层进行变换。\n        *   在每个空间位置产生 $C_{\\text{out}}$ 个输出。\n        *   **关键假设**：无权重共享。每个输出空间位置都有自己独立的权重和偏置。\n    *   **任务**：\n        *   以符号形式推导卷积层和替代层的可训练参数总数。\n        *   计算给定参数的数值。\n        *   计算参数节省因子 $S$，即两者之比。\n    *   **验证检查**：\n        *   **科学依据**：该问题在深度学习，特别是卷积神经网络的原理中有充分的依据。卷积、权重共享、核、通道、步幅和填充等概念都是标准概念。假设的“无共享”层是一个众所周知的概念，通常称为“局部连接层”或“非共享卷积”。它是一个有效的理论结构，用于说明CNN的效率。\n        *   **适定性**：该问题是适定的。定义清晰，所需的计算能得出一个唯一且有意义的答案。\n        *   **客观性**：语言精确、客观。\n        *   **完整性**：所有必要信息都已提供（$H, W, C_{\\text{in}}, k, s, p, C_{\\text{out}}$）。问题还指明包含偏置，这对于参数计数至关重要。\n        *   **一致性**：给定的条件是一致的。对于 $56 \\times 56$ 的输入，填充 $p=1$、步幅 $s=1$ 和核大小 $k=3$ 确实会保留空间维度。我们来验证一下。输出大小 $H_{out}$ 由公式 $H_{out} = \\lfloor \\frac{H - k + 2p}{s} \\rfloor + 1$ 给出。此处，$H_{out} = \\lfloor \\frac{56 - 3 + 2(1)}{1} \\rfloor + 1 = \\lfloor 55 \\rfloor + 1 = 56$。宽度同理。因此，输出空间大小确实是 $56 \\times 56$。这证实了“输出空间尺寸得以保留”的说法。\n\n    *   **结论**：问题有效。我可以继续解答。\n\n2.  **解决问题**：\n    *   **第1部分：卷积层中的参数**\n        *   卷积的核心思想是权重共享。有一组单一的滤波器（核），应用于所有空间位置。\n        *   单个滤波器具有 $k \\times k$ 的空间大小，并且必须作用于所有 $C_{\\text{in}}$ 个输入通道。因此，一个滤波器的大小是 $k \\times k \\times C_{\\text{in}}$。\n        *   该层需要产生 $C_{\\text{out}}$ 个输出通道（或特征图）。每个输出通道由其自己的滤波器生成。\n        *   因此，所有滤波器中的总权重数为`(滤波器数量) * (单个滤波器大小)`。即 $C_{\\text{out}} \\times (k \\times k \\times C_{\\text{in}})$。\n        *   此外，每个输出通道都有一个加性偏置。由于对于给定的输出通道，在所有空间位置都使用相同的滤波器，因此整个输出通道只有一个偏置项。\n        *   对于 $C_{\\text{out}}$ 个输出通道，总共有 $C_{\\text{out}}$ 个偏置项。\n        *   所以，卷积层中的总参数数 $N_{\\text{conv}}$ 为：\n            $N_{\\text{conv}} = (\\text{权重}) + (\\text{偏置}) = (k \\times k \\times C_{\\text{in}} \\times C_{\\text{out}}) + C_{\\text{out}}$。\n            $N_{\\text{conv}} = k^2 C_{\\text{in}} C_{\\text{out}} + C_{\\text{out}}$。\n\n    *   **第2部分：无共享替代层中的参数**\n        *   该层被描述为在*每个*输出空间位置应用一个独立的全连接变换。\n        *   首先，让我们确定输出空间位置的数量。输出空间维度为 $H_{\\text{out}} \\times W_{\\text{out}}$。\n        *   输出维度的公式是 $H_{\\text{out}} = \\frac{H - k + 2p}{s} + 1$（此处为整数除法）。\n        *   给定 $H=56, W=56, k=3, p=1, s=1$:\n            $H_{\\text{out}} = \\frac{56 - 3 + 2(1)}{1} + 1 = 55 + 1 = 56$。\n            $W_{\\text{out}} = \\frac{56 - 3 + 2(1)}{1} + 1 = 55 + 1 = 56$。\n        *   输出空间位置的总数是 $H_{\\text{out}} \\times W_{\\text{out}}$。\n        *   在这些位置的*每一个*上，我们执行一个变换。该变换的输入是一个大小为 $k \\times k \\times C_{\\text{in}}$ 的区块。\n        *   问题陈述该区块被展平并应用一个全连接（仿射）变换。\n        *   展平后的输入向量大小为 $k \\times k \\times C_{\\text{in}}$。\n        *   此向量被变换为一个大小为 $C_{\\text{out}}$ 的输出向量。\n        *   一个将大小为 $D_{\\text{in}}$ 的输入向量映射到大小为 $D_{\\text{out}}$ 的输出向量的全连接层，有 $D_{\\text{in}} \\times D_{\\text{out}}$ 个权重和 $D_{\\text{out}}$ 个偏置。\n        *   在我们的例子中，对于*单个*空间位置，$D_{\\text{in}} = k \\times k \\times C_{\\text{in}}$ 且 $D_{\\text{out}} = C_{\\text{out}}$。\n        *   所以，对于一个空间位置，参数数量为 $(k \\times k \\times C_{\\text{in}} \\times C_{\\text{out}}) + C_{\\text{out}}$。\n        *   关键点在于*没有权重共享*。这意味着我们需要为 $H_{\\text{out}} \\times W_{\\text{out}}$ 个输出位置中的*每一个*都配备一套独立的参数。\n        *   因此，无共享替代方案中的总参数数 $N_{\\text{no-share}}$ 为：\n            $N_{\\text{no-share}} = (\\text{输出位置数量}) \\times (\\text{每个位置的参数})$。\n            $N_{\\text{no-share}} = (H_{\\text{out}} \\times W_{\\text{out}}) \\times ((k \\times k \\times C_{\\text{in}} \\times C_{\\text{out}}) + C_{\\text{out}})$。\n            使用 $H_{out}$ 和 $W_{out}$ 的公式：\n            $H_{out} W_{out} = (\\frac{H - k + 2p}{s} + 1) \\times (\\frac{W - k + 2p}{s} + 1)$。\n            对于这个问题，我们已知输出空间大小是保留的，所以 $H_{out} = H$ 且 $W_{out} = W$。问题陈述通过说由于 $p=1$ 输出大小得以保留，而略有简化。让我们遵循问题陈述的逻辑。输出有 $H_{\\text{out}} \\times W_{\\text{out}}$ 个空间位置。我们称之为 $H' \\times W'$。\n            $H' = (H - k + 2p)/s + 1 = (56 - 3 + 2)/1 + 1 = 56$。\n            $W' = (W - k + 2p)/s + 1 = (56 - 3 + 2)/1 + 1 = 56$。\n            所以位置数量为 $H \\times W$ (因为它们被保留了)。\n            所以，$N_{\\text{no-share}} = (H \\times W) \\times (k^2 C_{\\text{in}} C_{\\text{out}} + C_{\\text{out}})$。\n\n    *   **第3部分：计算参数节省因子 S**\n        *   定义是 $S = \\frac{N_{\\text{no-share}}}{N_{\\text{conv}}}$。\n        *   让我们代入符号表达式：\n            $S = \\frac{(H_{\\text{out}} \\times W_{\\text{out}}) \\times (k^2 C_{\\text{in}} C_{\\text{out}} + C_{\\text{out}})}{k^2 C_{\\text{in}} C_{\\text{out}} + C_{\\text{out}}}$\n        *   这看起来简单得可疑。让我们非常仔细地重读问题。\n            “在这个替代层在空间位置之间没有权重共享：每个空间位置都有自己独立的权重和偏置，即每个位置的每个输出通道都有一个偏置。”\n            这证实了我对每个位置参数的计算。\n        *   让我们重新检查卷积层的参数计数。\n            “卷积层中为每个输出通道都包含一个加性偏置”\n            这是标准术语。它意味着对于第 j 个输出特征图 $Y_j$ 的方程，我们有 $Y_j = \\sum_i X_i * K_{ij} + b_j$。偏置 $b_j$ 是一个标量，被加到结果图的所有元素上。所以总共有 $C_{\\text{out}}$ 个偏置。\n            $N_{conv} = k^2 C_{\\text{in}} C_{\\text{out}} + C_{\\text{out}}$。这似乎也是正确的。\n\n            让我们重新评估S。\n            $S = \\frac{N_{\\text{no-share}}}{N_{\\text{conv}}} = \\frac{(H_{\\text{out}} \\times W_{\\text{out}}) \\times (k^2 C_{\\text{in}} C_{\\text{out}} + C_{\\text{out}})}{k^2 C_{\\text{in}} C_{\\text{out}} + C_{\\text{out}}}$\n            $S = \\frac{(H_{\\text{out}} \\times W_{\\text{out}}) \\times C_{\\text{out}}(k^2 C_{\\text{in}} + 1)}{C_{\\text{out}}(k^2 C_{\\text{in}} + 1)}$\n            $S = H_{\\text{out}} \\times W_{\\text{out}}$\n            这是一个非常简单的结果。这意味着参数的节省直接来自于核被应用的次数。这在直觉上是说得通的。节省因子就是权重被重用的位置数量。\n\n    *   **第4部分：数值计算**\n        *   给定值：\n            $H = 56$，$W = 56$\n            $C_{\\text{in}} = 64$\n            $k = 3$\n            $s = 1$\n            $p = 1$\n            $C_{\\text{out}} = 128$\n        *   计算输出维度：\n            $H_{\\text{out}} = \\lfloor \\frac{H - k + 2p}{s} \\rfloor + 1 = \\lfloor \\frac{56 - 3 + 2(1)}{1} \\rfloor + 1 = \\lfloor 55 \\rfloor + 1 = 56$。\n            $W_{\\text{out}} = \\lfloor \\frac{W - k + 2p}{s} \\rfloor + 1 = \\lfloor \\frac{56 - 3 + 2(1)}{1} \\rfloor + 1 = \\lfloor 55 \\rfloor + 1 = 56$。\n        *   如问题所述，输出空间尺寸得以保留，$H_{\\text{out}} = H = 56$ 且 $W_{\\text{out}} = W = 56$。\n        *   参数节省因子是 $S = H_{\\text{out}} \\times W_{\\text{out}}$。\n        *   $S = 56 \\times 56$。\n        *   $56 \\times 56 = (50 + 6)^2 = 50^2 + 2 \\times 50 \\times 6 + 6^2 = 2500 + 600 + 36 = 3136$。\n        *   所以，$S = 3136$。\n\n    *   **复核逻辑和计算。**\n        *   首先，让我们写出完整的参数计数以供验证。\n        *   $N_{\\text{conv}}$:\n            权重: $k^2 \\times C_{\\text{in}} \\times C_{\\text{out}} = 3^2 \\times 64 \\times 128 = 9 \\times 64 \\times 128 = 9 \\times 8192 = 73728$。\n            偏置: $C_{\\text{out}} = 128$。\n            总计 $N_{\\text{conv}} = 73728 + 128 = 73856$。\n        *   $N_{\\text{no-share}}$:\n            位置数量: $H_{\\text{out}} \\times W_{\\text{out}} = 56 \\times 56 = 3136$。\n            每个位置的参数:\n            每个位置的权重: $k^2 \\times C_{\\text{in}} \\times C_{\\text{out}} = 73728$。\n            每个位置的偏置: $C_{\\text{out}} = 128$。\n            每个位置的总参数: $73728 + 128 = 73856$。\n            总计 $N_{\\text{no-share}} = (\\text{位置数量}) \\times (\\text{每个位置的参数}) = 3136 \\times 73856$。\n            $N_{\\text{no-share}} = 231525856$。\n        *   比率 $S$:\n            $S = \\frac{N_{\\text{no-share}}}{N_{\\text{conv}}} = \\frac{3136 \\times 73856}{73856} = 3136$。\n        *   计算成立。推理似乎很稳健。约分是正确的。\n            $N_{\\text{conv}} = C_{\\text{out}} (k^2 C_{\\text{in}} + 1)$\n            $N_{\\text{no-share}} = H_{\\text{out}} W_{\\text{out}} [C_{\\text{out}} (k^2 C_{\\text{in}} + 1)]$\n            $S = \\frac{N_{\\text{no-share}}}{N_{\\text{conv}}} = \\frac{H_{\\text{out}} W_{\\text{out}} [C_{\\text{out}} (k^2 C_{\\text{in}} + 1)]}{C_{\\text{out}} (k^2 C_{\\text{in}} + 1)} = H_{\\text{out}} W_{\\text{out}}$。\n\n    *   **解答的最终结构**：\n        1.  首先声明问题经验证有效。\n        2.  推导标准卷积层中的参数数量公式，$N_{\\text{conv}}$。\n            *   解释权重部分: $k^2 \\times C_{in} \\times C_{out}$。\n            *   解释偏置部分: $C_{out}$。\n            *   将它们结合起来: $N_{\\text{conv}} = k^2 C_{\\text{in}} C_{\\text{out}} + C_{\\text{out}}$。\n        3.  推导假设的无共享层中的参数数量公式，$N_{\\text{no-share}}$。\n            *   首先，计算输出空间维度，$H_{\\text{out}}$ 和 $W_{\\text{out}}$。\n            *   然后，确定单个输出位置的参数数量。它是一个从大小为 $k \\times k \\times C_{in}$ 的展平区块到 $C_{out}$ 个输出的全连接层。\n            *   每个位置的权重: $k^2 \\times C_{in} \\times C_{out}$。\n            *   每个位置的偏置: $C_{out}$。\n            *   每个位置的总参数: $k^2 C_{\\text{in}} C_{\\text{out}} + C_{\\text{out}}$。\n            *   由于没有权重共享，乘以位置数量 $H_{\\text{out}} W_{\\text{out}}$。\n            *   将它们结合起来: $N_{\\text{no-share}} = (H_{\\text{out}} W_{\\text{out}}) (k^2 C_{\\text{in}} C_{\\text{out}} + C_{\\text{out}})$。\n        4.  构建节省因子S的公式。\n            *   $S = N_{\\text{no-share}} / N_{\\text{conv}}$。\n            *   代入符号表达式并化简，以表明 $S = H_{\\text{out}} W_{\\text{out}}$。\n        5.  代入给定的数值。\n            *   使用提供的参数计算 $H_{\\text{out}}$ 和 $W_{\\text{out}}$。\n            *   计算 $S = H_{\\text{out}} \\times W_{\\text{out}}$。\n        6.  给出最终数值答案。\n\n    该问题直接明了，考验了CNN参数计数的基础知识。我的推导是合理的。最终答案是一个整数，按要求是实数。不需要四舍五入。整个解答需要用LaTeX格式。我将确保每一个数学符号和数字都包含在 `$...$` 或 `$$...$$` 中。",
            "answer": "$$\n\\boxed{3136}\n$$"
        },
        {
            "introduction": "最后的这个练习将聚焦于卷积的一个关键概念：感受野。通过设计一个需要邻近像素信息的任务（棋盘格奇偶校验问题），你将证明为何 $1 \\times 1$ 卷积会失败，而一个更大的卷积核则能成功解决问题。这个练习  将巩固你对卷积如何捕捉局部空间模式的理解，而这正是其在图像处理等领域取得成功的基石。",
            "id": "3180076",
            "problem": "给定二值图像，要求您研究一个取决于相邻像素间耦合关系的目标。该研究必须使用离散二维卷积的核心定义以及独立伯努利变量的基本概率论来进行。\n\n使用的基本定义：\n- 输入数组 $x$ 与有限核 $k$ 的离散二维卷积定义为\n$$\n(y \\star k)[i,j] = \\sum_{u}\\sum_{v} x[i-u, j-v]\\;k[u,v],\n$$\n并采用适当的边界约定。堆叠卷积对应于该线性算子的重复应用，而仅堆叠空间尺寸为 $1\\times 1$ 的卷积和逐点非线性操作，会在每个空间位置上产生一个逐点函数。\n\n数据集构建：\n- 对于每张图像 $x\\in\\{0,1\\}^{H\\times W}$，在以左上角为基准的有效位置上，通过每个 $2\\times 2$ 块的奇偶性来定义一个目标 $t\\in\\{0,1\\}^{(H-1)\\times (W-1)}$：\n$$\nt[i,j] = \\left( x[i,j] + x[i,j+1] + x[i+1,j] + x[i+1,j+1] \\right)\\bmod 2,\n$$\n对于所有 $i\\in\\{0,\\dots,H-2\\}$ 和 $j\\in\\{0,\\dots,W-2\\}$。每张输入图像的元素都应独立同分布于参数为 $p=\\tfrac{1}{2}$ 的伯努利分布。\n\n待比较的模型族：\n- 模型族 A（仅逐点）：任何仅由 $1\\times 1$ 卷积和逐点非线性组成的架构。这类模型在每个空间位置上实现一个形如 $f(x[i,j])$ 的函数，而不访问邻居像素。在所述的数据分布下，请确定模型族 A 在上述目标上可达准确率的一个可证明的紧上界（作为一个实数），并报告该上界。\n- 模型族 B（感知邻域）：一个使用空间支持覆盖每个 $2\\times 2$ 块的二维卷积以及逐点操作的构造。请设计一个这样的构造，使其在有效区域上能完美预测 $t$，并报告其在数据集上计算得出的经验准确率。\n\n准确率定义：\n- 对于一个给定的包含 $M$ 张形状为 $H\\times W$ 图像的数据集，汇总在所有图像和所有有效位置 $(i,j)$ 上的正确预测总数，然后除以整个数据集中的有效位置总数，即 $M\\cdot (H-1)\\cdot (W-1)$。结果是一个在 $[0,1]$ 区间内的实数。\n\n您的任务：\n- 实现一个程序，对下述每个测试用例，按描述构建数据集，计算模型族 B 在该数据集上的准确率，并输出基于此数据分布从第一性原理推导出的模型族 A 的可证明的紧上界。\n- 重要提示：要输出的模型族 A 的结果必须是可证明的上界（一个实数），而不是经验估计值。模型族 B 的结果必须是在数据集上计算得出的经验准确率。\n\n测试套件：\n- 使用以下测试用例，每个用例指定为一个元组 $(M,H,W,\\text{seed})$：\n    - 用例 1：$(M,H,W,\\text{seed})=(\\,$ $256$ $,$ $8$ $,$ $8$ $,$ $7$ $\\,)$。\n    - 用例 2：$(M,H,W,\\text{seed})=(\\,$ $64$ $,$ $2$ $,$ $2$ $,$ $13$ $\\,)$。\n    - 用例 3：$(M,H,W,\\text{seed})=(\\,$ $100$ $,$ $9$ $,$ $7$ $,$ $3$ $\\,)$。\n\n输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。对于每个测试用例，输出一个包含两个元素的列表 $[b,a]$，其中 $b$ 是模型族 A 的可证明上界，$a$ 是模型族 B 在数据集上的经验准确率。因此，最终输出必须是一个包含三个双元素列表的单一列表。将所有实数四舍五入到恰好三位小数。例如：\n\"[ [0.500,1.000],[0.500,1.000],[0.500,1.000] ]\"",
            "solution": "问题陈述经评估有效。它在离散数学和概率论方面有科学依据，与卷积网络的研究直接相关，并且定义明确、自成体系且客观。因此我们可以着手提供解决方案。\n\n该问题要求我们分析两族模型在一个二值图像任务上的性能。对于位置 $(i,j)$，目标变量 $t[i,j]$ 是从该位置开始的 $2 \\times 2$ 块中像素总和的奇偶性。输入像素 $x[i,j]$ 是参数为 $p=\\tfrac{1}{2}$ 的独立同分布的伯努利随机变量。\n\n### 模型族 A（仅逐点）的分析\n\n模型族 A 中的模型仅由 $1 \\times 1$ 卷积和逐点非线性组成。一个 $1 \\times 1$ 卷积在空间位置 $(i,j)$ 计算的输出仅是同一位置输入 $x[i,j]$ 的函数。堆叠任意数量的此类层都会保持此属性。因此，模型族 A 中的任何模型都实现了一个逐点函数，其中对目标 $t[i,j]$ 的预测（记为 $\\hat{t}[i,j]$）仅是 $x[i,j]$ 的函数。设该函数为 $f:\\{0,1\\} \\to \\{0,1\\}$，使得 $\\hat{t}[i,j] = f(x[i,j])$。\n\n目标定义为：\n$$t[i,j] = \\left( x[i,j] + x[i,j+1] + x[i+1,j] + x[i+1,j+1] \\right) \\bmod 2$$\n我们可以将对 $x[i,j]$ 的依赖与其邻居分离开来。设 $S_{i, j} = x[i,j+1] + x[i+1,j] + x[i+1,j+1]$。那么目标就是 $t[i,j] = (x[i,j] + S_{i, j}) \\bmod 2$。\n\n输入像素 $x[i',j']$ 是参数为 $p=\\tfrac{1}{2}$ 的独立伯努利随机变量。即 $P(x[i',j']=0) = P(x[i',j']=1) = \\tfrac{1}{2}$。任意数量的独立同分布伯努利($\\tfrac{1}{2}$)变量之和对 2 取模，其结果是一个新的伯努利($\\tfrac{1}{2}$)变量。由于 $S_{i,j}$ 是 3 个此类变量的和，所以 $S_{i,j} \\bmod 2$ 是一个随机变量，它以相等的概率取值 0 和 1，即 $P(S_{i,j} \\bmod 2 = 0) = P(S_{i,j} \\bmod 2 = 1) = \\tfrac{1}{2}$。\n\n关键在于，组成 $S_{i,j}$ 的像素与 $x[i,j]$ 不同，因此 $S_{i,j}$ 在统计上独立于 $x[i,j]$。模型 $f$ 仅基于 $x[i,j]$ 进行预测，而 $x[i,j]$ 不提供关于 $S_{i, j} \\bmod 2$ 值的任何信息。\n\n模型的准确率是正确预测的概率，即 $P(\\hat{t}[i,j] = t[i,j])$。我们可以通过考虑 $f(0)$ 和 $f(1)$ 的最优选择来找到最大可能准确率。\n$$ Acc = P(f(x[i,j]) = (x[i,j] + S_{i,j}) \\bmod 2) $$\n根据全概率定律，以 $x[i,j]$ 的值为条件：\n$$ Acc = P(\\hat{t}=t | x[i,j]=0)P(x[i,j]=0) + P(\\hat{t}=t | x[i,j]=1)P(x[i,j]=1) $$\n$$ Acc = \\frac{1}{2} P(f(0) = (0 + S_{i,j}) \\bmod 2) + \\frac{1}{2} P(f(1) = (1 + S_{i,j}) \\bmod 2) $$\n第一项是 $P(f(0) = S_{i,j} \\bmod 2)$。由于 $S_{i,j} \\bmod 2$ 相当于一次公平的硬币投掷，所以无论 $f(0)$ 选择为 $0$ 还是 $1$，匹配的概率都是 $\\tfrac{1}{2}$。\n第二项是 $P(f(1) = (1 + S_{i,j}) \\bmod 2)$。变量 $(1 + S_{i,j}) \\bmod 2$ 也相当于一次公平的硬币投掷，因此无论 $f(1)$ 如何选择，匹配它的概率也是 $\\tfrac{1}{2}$。\n\n因此，最大可达准确率为：\n$$ Acc_{max} = \\frac{1}{2} \\left(\\frac{1}{2}\\right) + \\frac{1}{2} \\left(\\frac{1}{2}\\right) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\n这构成了一个紧上界，因为任何常数预测器（例如，对所有 $x$ 都有 $f(x)=0$）都可以达到它，其准确率为 $P(t[i,j]=0) = \\tfrac{1}{2}$。模型族 A 的准确率的可证明的紧上界是 $0.5$。\n\n### 模型族 B（感知邻域）的分析\n\n模型族 B 中的模型被允许使用空间支持覆盖每个 $2 \\times 2$ 块的二维卷积。我们可以构建一个能完美计算目标函数的模型。\n\n目标函数是 $t[i,j] = \\left( \\sum_{u=0}^{1} \\sum_{v=0}^{1} x[i+u, j+v] \\right) \\bmod 2$。\n此计算可以分解为两个步骤，这两个步骤对于模型族 B 都是允许的：\n1.  线性空间聚合：计算总和 $y[i,j] = \\sum_{u=0}^{1} \\sum_{v=0}^{1} x[i+u,j+v]$。这是一个与全一的 $2 \\times 2$ 核 $k = \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}$ 进行的二维互相关。互相关是深度学习库中的一个标准操作，通常被称为“卷积”，并且符合问题对具有空间支持的卷积的描述。此操作使用 `'valid'` 填充，会生成一个形状为 $(H-1) \\times (W-1)$ 的输出。\n2.  逐点非线性：将函数 $g(z) = z \\bmod 2$ 应用于中间特征图 $y$ 的每个元素。\n\n最终得到的模型 $\\hat{t}[i,j] = g(y[i,j])$，在数学上与目标 $t[i,j]$ 的定义完全相同。因此，这个构造是一个完美的预测器。当在任何根据问题规范生成的数据集上进行评估时，正确预测的数量将等于有效位置的总数。其经验准确率将恰好是 $1.0$。\n\n下面的程序将为模型族 B 实现这个完美的模型并计算其经验准确率（预期为 $1.0$）。它还将报告为模型族 A 推导出的理论上界 $0.5$。",
            "answer": "```python\nimport numpy as np\nimport scipy.signal\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the provable bound for Family A and the\n    empirical accuracy for Family B for a series of test cases.\n    \"\"\"\n    test_cases = [\n        # (M, H, W, seed)\n        (256, 8, 8, 7),\n        (64, 2, 2, 13),\n        (100, 9, 7, 3),\n    ]\n\n    all_results = []\n\n    # The provable tight upper bound for Family A is derived from first principles.\n    # The input pixel x[i,j] has zero mutual information with the target t[i,j].\n    # Since the target is a balanced binary variable (P(t=0)=P(t=1)=0.5),\n    # the best achievable accuracy for a model that only sees x[i,j] is 0.5.\n    family_a_bound = 0.5\n\n    for M, H, W, seed in test_cases:\n        # Set the random seed for reproducible dataset generation.\n        np.random.seed(seed)\n\n        # --- Dataset Construction ---\n        # Generate M images of size HxW with pixels i.i.d. as Bernoulli(0.5).\n        # Use a dtype that avoids overflow for the intermediate sum.\n        images = np.random.randint(0, 2, size=(M, H, W), dtype=np.int32)\n\n        # --- Target Construction ---\n        # t[i,j] = (x[i,j] + x[i,j+1] + x[i+1,j] + x[i+1,j+1]) mod 2\n        # This is computed for all images and all valid positions.\n        if H  2 or W  2:\n            targets = np.empty((M, 0, 0))\n        else:\n            targets = (images[:, :H-1, :W-1] +\n                       images[:, :H-1, 1:W] +\n                       images[:, 1:H, :W-1] +\n                       images[:, 1:H, 1:W]) % 2\n\n        # --- Family B: Empirical Accuracy Calculation ---\n        # A Family B model is constructed to perfectly compute the target.\n        # The model consists of a 2D convolution followed by a pointwise nonlinearity.\n        kernel = np.ones((2, 2), dtype=np.int32)\n        total_correct_predictions = 0\n\n        for m in range(M):\n            image = images[m, :, :]\n            \n            if image.shape[0] >= 2 and image.shape[1] >= 2:\n                # 1. Convolutional layer: scipy.signal.convolve2d with mode='valid'\n                # computes the sum over each 2x2 block. For a symmetric kernel,\n                # convolution and correlation are equivalent.\n                convolved_output = scipy.signal.convolve2d(image, kernel, mode='valid')\n                \n                # 2. Pointwise nonlinearity (modulo 2)\n                predictions = convolved_output % 2\n                \n                # Compare predictions with the pre-computed target for this image\n                target_m = targets[m, :, :]\n                total_correct_predictions += np.sum(predictions == target_m)\n\n        # 3. Compute empirical accuracy\n        if H > 1 and W > 1:\n            total_valid_positions = M * (H - 1) * (W - 1)\n            family_b_accuracy = total_correct_predictions / total_valid_positions\n        else:\n            # No valid positions, so accuracy is vacuously 1.0 (no errors made).\n            family_b_accuracy = 1.0\n\n        all_results.append([family_a_bound, family_b_accuracy])\n\n    # --- Format the Final Output ---\n    # The format is a string representing a list of lists, with numbers\n    # rounded to three decimal places.\n    # Example: \"[[0.500,1.000],[0.500,1.000],[0.500,1.000]]\"\n    result_strings = [f\"[{b:.3f},{a:.3f}]\" for b, a in all_results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}