{
    "hands_on_practices": [
        {
            "introduction": "To truly understand a network architecture, we must analyze how information flows through it. A powerful tool for this is receptive field analysis, which tells us the size of the input region that influences a single neuron's output. In this exercise, you will derive the receptive field growth in a dense block and compare it to that of a residual block, revealing fundamental insights into how these architectures learn spatial hierarchies. ",
            "id": "3114064",
            "problem": "Consider a dense block as in Densely Connected Convolutional Networks (DenseNet) within a Convolutional Neural Network (CNN). The block consists of $L$ sequential two-dimensional convolutional layers, each with a $3 \\times 3$ kernel, stride $1$, zero-padding of $1$ on all sides (so spatial resolution is preserved), no pooling, no dilation, and pointwise nonlinearities that do not change spatial dependency. Each layer takes as input the channelwise concatenation of the original block input and all preceding layer outputs in the block. The block output is the channelwise concatenation of all layer outputs (ignore any compression or transition layers after the block). For any layer output, define its receptive field side length as the number of input pixels along a single spatial axis of the original block input that can influence a single output pixel at that layer. Define the block receptive field side length as the maximum receptive field side length among all its output channels.\n\nStarting from the definitions of discrete convolution and receptive field propagation, derive the closed-form expression for the block receptive field side length at the end of this dense block as a function of $L$. Then, consider an “equivalent-depth” residual stack composed of $L$ sequential $3 \\times 3$ stride-$1$ convolutional layers, also with zero-padding of $1$, no pooling, and identity skip connections arranged to form standard residual pathways but without altering spatial dimensions. Using the same first-principles reasoning, derive the block receptive field side length at the end of this residual stack and compare it with that of the dense block.\n\nReport only the single common closed-form expression for the receptive field side length of the block output (measured along one spatial axis) in terms of $L$. Your final answer must be a single analytic expression. No rounding is required.",
            "solution": "The problem asks for a single common closed-form expression for the receptive field side length of a dense block and a residual stack, each composed of $L$ layers. We will derive this by analyzing the receptive field propagation in each architecture based on first principles.\n\nThe fundamental recurrence relation for the side length of a receptive field, $RF$, when applying a convolutional layer to an input feature map is given by:\n$$RF_{out} = RF_{in} + (k - 1) \\times S_{in}$$\nwhere $RF_{in}$ is the receptive field side length of the input feature map with respect to the original network input, $k$ is the kernel side length of the current layer, and $S_{in}$ is the product of strides of all preceding layers from the original network input to the current layer's input. The receptive field of a single pixel in the original input is defined as $RF_{original} = 1$.\n\nIn this problem, for every convolutional layer, the kernel size is $k=3$ and the stride is $s=1$. The product of strides $S_{in}$ is always $1$ because all strides are $1$. Thus, the propagation formula for a single convolutional step simplifies to:\n$$RF_{out} = RF_{in} + (3-1) \\times 1 = RF_{in} + 2$$\nThis formula indicates that each $3 \\times 3$ convolution with stride $1$ increases the receptive field side length by $2$.\n\nLet's now analyze each architecture. We will denote the original input to the block as $x_0$.\n\n**1. Analysis of the Dense Block (DenseNet)**\n\nIn a dense block, the input to layer $l$ (where $l \\in \\{1, 2, \\dots, L\\}$) is the channel-wise concatenation of the original block input $x_0$ and the outputs of all preceding layers, $y_1, y_2, \\dots, y_{l-1}$. The output of layer $l$ is $y_l$.\nLet $RF(Z)$ denote the receptive field side length of the tensor $Z$ with respect to the original input $x_0$. We define $RF_l \\equiv RF(y_l)$. A single output pixel in $y_l$ is influenced by a $3 \\times 3$ region of its composite input, $\\text{concat}(x_0, y_1, \\dots, y_{l-1})$. The total receptive field $RF_l$ is the union of the receptive fields from all input paths. The size of this union is determined by the path that yields the largest receptive field.\n\nFor layer $l=1$:\nThe input is $x_0$. The receptive field of the input pixels is $RF(x_0) = 1$.\n$$RF_1 = RF(x_0) + (3-1) = 1 + 2 = 3$$\n\nFor layer $l=2$:\nThe input is $\\text{concat}(x_0, y_1)$. A pixel in $y_2$ is influenced by a $3 \\times 3$ region of this input.\n- The path from $x_0$ through the convolution at layer $2$ results in a receptive field of size $k=3$.\n- The path from $y_1$ through the convolution at layer $2$ results in a receptive field of size $RF_1 + (3-1) = 3 + 2 = 5$.\nThe receptive field of $y_2$ is the maximum of these:\n$$RF_2 = \\max(3, 5) = 5$$\n\nFor layer $l=3$:\nThe input is $\\text{concat}(x_0, y_1, y_2)$. The receptive field $RF_3$ is the maximum of the receptive fields from all paths:\n- Path via $x_0$: $RF=3$.\n- Path via $y_1$: $RF = RF_1 + 2 = 3 + 2 = 5$.\n- Path via $y_2$: $RF = RF_2 + 2 = 5 + 2 = 7$.\n$$RF_3 = \\max(3, 5, 7) = 7$$\n\nBy induction, for an arbitrary layer $l$, its receptive field is:\n$$RF_l = \\max(RF(\\text{path via } x_0), RF(\\text{path via } y_1), \\dots, RF(\\text{path via } y_{l-1}))$$\n$$RF_l = \\max(3, RF_1+2, RF_2+2, \\dots, RF_{l-1}+2)$$\nSince $RF_j = 2j+1$ for $j < l$, the sequence $RF_j+2 = 2j+3$ is strictly increasing with $j$. The maximum value is determined by the path through the most recent layer, $y_{l-1}$.\n$$RF_l = RF_{l-1} + 2$$\nThis is a simple arithmetic progression. With the base case $RF_1 = 3$, the closed-form solution for the receptive field of layer $l$ is:\n$$RF_l = RF_1 + (l-1) \\times 2 = 3 + 2l - 2 = 2l + 1$$\n\nThe block output is the concatenation of all layer outputs: $[y_1, y_2, \\dots, y_L]$. The \"block receptive field side length\" is defined as the maximum receptive field side length among all its output channels, which is $\\max(RF_1, RF_2, \\dots, RF_L)$. Since $RF_l = 2l+1$ is a strictly increasing function of $l$, the maximum is achieved at $l=L$.\n$$RF_{\\text{dense block}} = RF_L = 2L + 1$$\n\n**2. Analysis of the Residual Stack (ResNet)**\n\nIn a standard residual stack, the output of block $l$, which we denote as $z_l$, is the sum of the input to the block, $z_{l-1}$, and the output of the convolutional transformation of the input, $\\text{Conv}_l(z_{l-1})$. (The problem states that pointwise nonlinearities do not alter spatial dependency, so they do not affect the receptive field calculation).\n$$z_l = \\text{Conv}_l(z_{l-1}) + z_{l-1}$$\nLet $RF_l'$ be the receptive field side length of $z_l$ with respect to the initial input $z_0 = x_0$. The receptive field of the initial input is $RF_0' = 1$.\n\nFor layer $l$, an output pixel in $z_l$ is influenced by two paths originating from $z_{l-1}$:\n- The convolutional path: $\\text{Conv}_l(z_{l-1})$. The receptive field contributed by this path is $RF_{l-1}' + (3-1) = RF_{l-1}' + 2$.\n- The identity skip connection path: $z_{l-1}$. The receptive field contributed by this path is simply $RF_{l-1}'$.\n\nThe total receptive field of $z_l$ is the union of the receptive fields from these two paths. The size of the union is the maximum of the sizes of the individual receptive fields.\n$$RF_l' = \\max(RF_{l-1}' + 2, RF_{l-1}') = RF_{l-1}' + 2$$\nThis establishes the same recurrence relation as for a simple stack of convolutional layers. We have an arithmetic progression starting from $RF_0' = 1$. After $L$ layers, the receptive field is:\n$$RF_L' = RF_0' + \\sum_{i=1}^{L} 2 = 1 + 2L$$\n\nThe final output of the residual stack is $z_L$, so the block's receptive field is $RF_L'$.\n$$RF_{\\text{residual stack}} = 2L + 1$$\n\n**Conclusion**\n\nBoth the dense block and the residual stack, under the specified conditions ($3 \\times 3$ kernel, stride $1$), exhibit the same linear growth in receptive field size. The dominant path for receptive field expansion in both architectures is the sequential chain of convolutions. Shorter paths created by dense or residual connections have smaller receptive fields that are contained within the receptive field of the longest path.\n\nThe single common closed-form expression for the block receptive field side length for both architectures is $2L+1$.",
            "answer": "$$\\boxed{2L+1}$$"
        },
        {
            "introduction": "While dense connections offer powerful feature reuse, they can also lead to computationally intensive models. A key skill for a deep learning practitioner is to optimize these architectures for efficiency without sacrificing performance. This practice challenges you to analyze the computational savings gained by replacing standard convolutions with depthwise separable convolutions in a DenseNet bottleneck layer, providing a quantitative understanding of modern network design trade-offs. ",
            "id": "3113990",
            "problem": "A single layer inside a Dense Convolutional Network (DenseNet) dense block uses a bottleneck design with two stages: a $1 \\times 1$ convolution followed by a spatial convolution. The growth rate is $k$, meaning the layer produces $k$ new feature maps, and the bottleneck expansion factor is $b$, meaning the intermediate channel count after the $1 \\times 1$ convolution is $b k$. Consider two design choices for the spatial stage: (i) a standard $3 \\times 3$ convolution from $b k$ input channels to $k$ output channels, and (ii) a depthwise separable convolution (DSC), which consists of a $3 \\times 3$ depthwise stage applied channel-wise to the $b k$ channels, followed by a $1 \\times 1$ pointwise stage mapping $b k$ channels to $k$ channels. Assume stride $1$ and that the output feature map has spatial dimensions $H \\times W$.\n\nUsing the fundamental definition of discrete convolution as a sum of products and the widely accepted convention in deep learning that each multiply and each add counts as one floating-point operation (FLOPs), and ignoring bias additions, activation functions, and boundary effects, derive the exact analytical expression for the total FLOP savings $S(k,b,H,W)$ achieved when replacing the standard $3 \\times 3$ convolution with the depthwise separable convolution in this bottleneck layer. Express $S(k,b,H,W)$ only in terms of $k$, $b$, $H$, and $W$, and simplify your final expression as much as possible. The final answer must be a single closed-form analytic expression.",
            "solution": "The problem asks for the analytical expression for the total floating-point operation (FLOP) savings, denoted $S(k,b,H,W)$, when replacing a standard $3 \\times 3$ spatial convolution with a $3 \\times 3$ depthwise separable convolution (DSC) within a DenseNet bottleneck layer. The savings is the difference between the FLOPs required for the standard convolution and the FLOPs for the DSC.\n\nFirst, we establish the formula for calculating FLOPs. A convolution operation is fundamentally a series of multiply-accumulate (MAC) operations. Following the problem's stated convention, each multiplication and each addition is counted as one FLOP. A single MAC operation, which computes a sum by adding a product, therefore consists of $2$ FLOPs. For a standard convolutional layer, the total number of FLOPs is given by the product of the number of MACs and $2$.\nThe number of MACs to produce an output feature map of size $H_{\\text{out}} \\times W_{\\text{out}}$ from an input with $C_{\\text{in}}$ channels using $C_{\\text{out}}$ filters of kernel size $K_H \\times K_W$ is $H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{out}} \\times C_{\\text{in}} \\times K_H \\times K_W$.\nThus, the general formula for FLOPs, ignoring biases, is:\n$$\n\\text{FLOPs} = 2 \\times H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{out}} \\times C_{\\text{in}} \\times K_H \\times K_W\n$$\nThe problem specifies that the stride is $1$ and the output spatial dimensions are $H \\times W$, so we set $H_{\\text{out}} = H$ and $W_{\\text{out}} = W$. The input to the spatial stage of the bottleneck layer has $b k$ channels.\n\nCase (i): Standard $3 \\times 3$ Convolution\nFor this case, the parameters are:\n- Input channels, $C_{\\text{in}} = b k$\n- Output channels, $C_{\\text{out}} = k$\n- Kernel height, $K_H = 3$\n- Kernel width, $K_W = 3$\n- Output spatial dimensions: $H \\times W$\n\nUsing the general formula, the total FLOPs for the standard convolution, $\\text{FLOPs}_{\\text{standard}}$, are:\n$$\n\\text{FLOPs}_{\\text{standard}} = 2 \\times H \\times W \\times k \\times (b k) \\times 3 \\times 3\n$$\n$$\n\\text{FLOPs}_{\\text{standard}} = 18 b H W k^2\n$$\n\nCase (ii): Depthwise Separable Convolution (DSC)\nThe DSC consists of two consecutive stages: a depthwise convolution followed by a pointwise convolution. We calculate the FLOPs for each stage and sum them.\n\nStage $1$: $3 \\times 3$ Depthwise Convolution\nA depthwise convolution applies a single spatial filter to each input channel independently. The number of output channels is therefore equal to the number of input channels.\n- Input channels, $C_{\\text{in}} = b k$\n- Kernel height, $K_H = 3$\n- Kernel width, $K_W = 3$\nThe FLOPs calculation for a depthwise convolution is $\\text{FLOPs} = 2 \\times H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{in}} \\times K_H \\times K_W$.\nThe FLOPs for this stage, $\\text{FLOPs}_{\\text{DW}}$, are:\n$$\n\\text{FLOPs}_{\\text{DW}} = 2 \\times H \\times W \\times (b k) \\times 3 \\times 3\n$$\n$$\n\\text{FLOPs}_{\\text{DW}} = 18 b H W k\n$$\nThe output of this stage has $b k$ channels and spatial dimensions $H \\times W$.\n\nStage $2$: $1 \\times 1$ Pointwise Convolution\nThis stage maps the features from the depthwise stage to the final number of output channels. It is a standard convolution with a $1 \\times 1$ kernel.\n- Input channels (from Stage $1$), $C_{\\text{in}} = b k$\n- Output channels, $C_{\\text{out}} = k$\n- Kernel height, $K_H = 1$\n- Kernel width, $K_W = 1$\nThe FLOPs for this stage, $\\text{FLOPs}_{\\text{PW}}$, are:\n$$\n\\text{FLOPs}_{\\text{PW}} = 2 \\times H \\times W \\times k \\times (b k) \\times 1 \\times 1\n$$\n$$\n\\text{FLOPs}_{\\text{PW}} = 2 b H W k^2\n$$\n\nThe total FLOPs for the DSC, $\\text{FLOPs}_{\\text{DSC}}$, is the sum of the FLOPs from both stages:\n$$\n\\text{FLOPs}_{\\text{DSC}} = \\text{FLOPs}_{\\text{DW}} + \\text{FLOPs}_{\\text{PW}} = 18 b H W k + 2 b H W k^2\n$$\n\nFinally, we calculate the FLOP savings $S(k,b,H,W)$ by subtracting the FLOPs for the DSC from the FLOPs for the standard convolution:\n$$\nS(k,b,H,W) = \\text{FLOPs}_{\\text{standard}} - \\text{FLOPs}_{\\text{DSC}}\n$$\n$$\nS(k,b,H,W) = (18 b H W k^2) - (18 b H W k + 2 b H W k^2)\n$$\n$$\nS(k,b,H,W) = 18 b H W k^2 - 18 b H W k - 2 b H W k^2\n$$\nCombine the terms with $k^2$:\n$$\nS(k,b,H,W) = (18 - 2) b H W k^2 - 18 b H W k\n$$\n$$\nS(k,b,H,W) = 16 b H W k^2 - 18 b H W k\n$$\nTo simplify the expression, we factor out the common terms $2$, $b$, $H$, $W$, and $k$:\n$$\nS(k,b,H,W) = 2 b H W k (8k - 9)\n$$\nThis is the final, simplified analytical expression for the FLOP savings.",
            "answer": "$$\n\\boxed{2bHWk(8k - 9)}\n$$"
        },
        {
            "introduction": "Theoretical analysis lays the groundwork, but true mastery comes from hands-on implementation and experimentation. In this final practice, you will build a simplified dense network from scratch to investigate the empirical effects of different activation functions like ReLU, GELU, and SiLU. By training the network and observing its optimization dynamics and final performance, you will gain practical experience in how these crucial non-linearities shape a model's learning capabilities. ",
            "id": "3113972",
            "problem": "You will implement and compare dense-block neural networks that differ only in their activation functions, to study how the choice among Rectified Linear Unit (ReLU), Gaussian Error Linear Unit (GELU), and Sigmoid Linear Unit (SiLU) affects optimization dynamics and final accuracy. You must write a complete program that constructs a synthetic multi-class classification task, builds a simplified dense block with concatenated connections, trains with full-batch gradient descent, and reports quantitative metrics for a specified test suite.\n\nFundamentals:\n- A dense block is a sequence of layers where each new layer receives as input the concatenation of all previous feature maps. Concretely, for input vector $x \\in \\mathbb{R}^{d_0}$, growth rate $k \\in \\mathbb{N}$, and $L \\in \\mathbb{N}$ layers, layer $\\ell \\in \\{1,\\dots,L\\}$ computes $z_\\ell = W_\\ell \\, \\mathrm{concat}(x, h_1, \\dots, h_{\\ell-1}) + b_\\ell$, then $h_\\ell = \\phi(z_\\ell)$, where $\\phi$ is the activation function. The final representation is $H = \\mathrm{concat}(x, h_1, \\dots, h_L) \\in \\mathbb{R}^{d_0 + kL}$, which is passed to a linear classifier yielding logits $o = H W_c + b_c \\in \\mathbb{R}^{C}$ for $C$ classes.\n- Use the standard softmax with cross-entropy for multi-class classification: given logits $o_i \\in \\mathbb{R}^{C}$ for sample $i$ and true class index $y_i \\in \\{0,\\dots,C-1\\}$, define probabilities $p_{i,c} = \\exp(o_{i,c}) / \\sum_{j=1}^{C} \\exp(o_{i,j})$ and loss $\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( - \\log p_{i, y_i} \\right)$ for $N$ samples.\n- Train via full-batch gradient descent with fixed learning rate $\\eta > 0$ for a fixed number of epochs $E \\in \\mathbb{N}$. The gradient of the loss with respect to the parameters is computed by the chain rule of calculus. The classification accuracy is the fraction of correct predictions $\\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}[\\arg\\max_c p_{i,c} = y_i]$.\n\nActivation functions to compare:\n- Rectified Linear Unit (ReLU): $\\phi(x) = \\max(0, x)$.\n- Gaussian Error Linear Unit (GELU), use the widely used tanh approximation: $\\phi(x) \\approx \\frac{1}{2} x \\left( 1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} (x + 0.044715 x^3)\\right) \\right)$.\n- Sigmoid Linear Unit (SiLU), also known as Swish: $\\phi(x) = x \\cdot \\sigma(x)$ where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$.\n\nData:\n- Synthesize a $C$-class dataset in $\\mathbb{R}^{2}$ by drawing $N_c$ samples per class from isotropic Gaussians centered on a circle of radius $r$ with evenly spaced angles. For class $c \\in \\{0,\\dots,C-1\\}$, the mean is $\\mu_c = r [\\cos(2\\pi c / C), \\sin(2\\pi c / C)]$, and each sample is $\\mu_c + \\epsilon$, with $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_2)$. Use $C = 3$, $N_c = 100$, $r = 3$, and $\\sigma = 0.5$. There are $N = 300$ total samples.\n\nTraining:\n- Use He-style initialization for weights: for a layer with fan-in $f$, initialize entries as independent samples from $\\mathcal{N}(0, \\sqrt{2/f})$. Initialize biases to zero. Use the same random seed for all comparable cases to isolate the effect of activation.\n- Use full-batch gradient descent without momentum or weight decay.\n- Compute the following metrics per run:\n  1. The final average cross-entropy loss after $E$ epochs, reported as a real number rounded to four decimal places.\n  2. The final classification accuracy, reported as a real number in $[0,1]$ rounded to three decimal places.\n  3. The first epoch index (one-based) at which the average loss falls strictly below a threshold $\\tau$, or $-1$ if this never occurs within $E$ epochs. Use $\\tau = 0.6$.\n\nTest suite:\n- Fix the dataset as specified above and reuse it for all tests. For comparability, use the same random seed for weight initialization where applicable.\n- Evaluate the following five cases (each case is a tuple of activation, number of layers $L$, growth rate $k$, learning rate $\\eta$, epochs $E$, and initialization seed $s$):\n  1. Case A (happy path): activation ReLU, $L = 3$, $k = 12$, $\\eta = 0.05$, $E = 200$, $s = 123$.\n  2. Case B (happy path): activation GELU, $L = 3$, $k = 12$, $\\eta = 0.05$, $E = 200$, $s = 123$.\n  3. Case C (happy path): activation SiLU, $L = 3$, $k = 12$, $\\eta = 0.05$, $E = 200$, $s = 123$.\n  4. Case D (boundary condition on connectivity depth): activation ReLU, $L = 1$, $k = 12$, $\\eta = 0.05$, $E = 200$, $s = 123$.\n  5. Case E (edge case on optimization speed): activation GELU, $L = 3$, $k = 12$, $\\eta = 0.005$, $E = 200$, $s = 123$.\n\nAngle units:\n- All angles used in trigonometric functions are in radians.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output a list of three values in the order [final_loss, final_accuracy, first_epoch_index_or_-1]. The overall output should therefore be a list of five lists with no spaces. For example: [[0.1234,0.987,42],[...],...].",
            "solution": "### 1. Synthetic Data Generation\nA synthetic dataset is created according to the problem specification. It consists of $N=300$ samples in a $d_0=2$ dimensional space, distributed among $C=3$ classes, with $N_c=100$ samples per class. The center, or mean $\\mu_c$, for each class $c \\in \\{0, 1, 2\\}$ is positioned on a circle of radius $r=3$. The coordinates are given by the principle of polar-to-Cartesian conversion: $\\mu_c = r [\\cos(2\\pi c / C), \\sin(2\\pi c / C)]$. Each data point is generated by sampling from an isotropic Gaussian distribution $\\mathcal{N}(\\mu_c, \\sigma^2 I_2)$ with a standard deviation of $\\sigma=0.5$. A single, fixed random seed is used for data generation to ensure all models are trained and evaluated on the exact same dataset, thus isolating the effects of architectural and hyperparameter changes.\n\n### 2. Dense Block Network Architecture\nThe core of the model is a dense block, implemented as a sequence of $L$ layers. The defining principle of a dense block is its connectivity pattern: each layer receives as input the feature maps of all preceding layers, concatenated with the original input.\n\n- **Initialization**: Network parameters (weights and biases) are initialized before training. Following the He initialization principle, weights for a layer with fan-in $f$ are drawn from a normal distribution $\\mathcal{N}(0, \\sigma_W^2)$ where the standard deviation is $\\sigma_W = \\sqrt{2/f}$. This helps mitigate the vanishing/exploding gradients problem. All biases are initialized to zero. A specific random seed $s$ is used for initialization in each test case for reproducibility.\n- **Forward Pass**: The forward pass computes the network's output (logits) for a given input batch $X$. For each layer $\\ell \\in \\{1, \\dots, L\\}$:\n    1. The input to the layer, $H_{\\ell-1}$, is formed by concatenating the original input $X$ with the outputs of all previous layers: $H_{\\ell-1} = \\mathrm{concat}(X, h_1, \\dots, h_{\\ell-1})$. The dimensionality of this input is $d_0 + (\\ell-1)k$, which serves as the fan-in for weight initialization.\n    2. A linear transformation is applied: $z_\\ell = H_{\\ell-1} W_\\ell + b_\\ell$, where $W_\\ell \\in \\mathbb{R}^{(d_0+(\\ell-1)k) \\times k}$ and $b_\\ell \\in \\mathbb{R}^k$.\n    3. A non-linear activation function $\\phi$ is applied element-wise to produce the layer's output: $h_\\ell = \\phi(z_\\ell) \\in \\mathbb{R}^k$.\n- After the final block layer, all intermediate outputs are concatenated with the original input to form the final representation $H = \\mathrm{concat}(X, h_1, \\dots, h_L)$.\n- A final linear classifier layer maps this representation to logits for the $C$ classes: $o = H W_c + b_c$.\n- **Activation Functions**: Three activation functions are implemented as specified: ReLU ($\\phi(x) = \\max(0, x)$), the tanh-approximation of GELU ($\\phi(x) \\approx \\frac{1}{2} x (1 + \\tanh(\\sqrt{2/\\pi} (x + 0.044715 x^3)))$), and SiLU ($\\phi(x) = x \\cdot (1 + e^{-x})^{-1}$).\n\n### 3. Loss Calculation and Optimization\nThe model is trained to perform multi-class classification.\n- **Softmax and Cross-Entropy Loss**: The output logits $o$ are converted to probabilities $p$ using the softmax function, $p_c = \\exp(o_c) / \\sum_j \\exp(o_j)$. A log-sum-exp trick ($o_c \\leftarrow o_c - \\max(o)$) is used to ensure numerical stability. The training objective is to minimize the average cross-entropy loss over the batch, $\\mathcal{L} = -\\frac{1}{N}\\sum_i \\log p_{i, y_i}$, where $y_i$ is the true class for sample $i$.\n- **Backpropagation**: Gradients of the loss with respect to all model parameters ($W_\\ell, b_\\ell, W_c, b_c$) are computed using the chain rule, in a process known as backpropagation. The derivative of the cross-entropy loss with respect to the logits is $\\frac{\\partial \\mathcal{L}}{\\partial o_i} = (p_i - y_{i, \\text{one-hot}}) / N$. This gradient is then propagated backward through the classifier and the dense block. For the dense block, the gradient for a given hidden output $h_\\ell$ is the sum of gradients from all paths it contributes to: its direct connection to the final classifier and its use as an input to all subsequent layers $j > \\ell$. Our implementation carefully accumulates these gradients for each layer's parameters. Derivatives of each activation function are also required and are implemented accordingly.\n- **Gradient Descent**: Parameters are updated using a full-batch gradient descent rule: $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}$, where $\\theta$ represents any parameter, $\\eta$ is the learning rate, and $\\nabla_\\theta \\mathcal{L}$ is the computed gradient.\n\n### 4. Evaluation and Reporting\nThe model is trained for a fixed number of epochs $E$. For each test case, three metrics are computed:\n1.  **Final Loss**: The average cross-entropy loss on the full dataset after $E$ epochs.\n2.  **Final Accuracy**: The fraction of correctly classified samples after $E$ epochs.\n3.  **Convergence Speed**: The first one-based epoch index at which the training loss falls strictly below a threshold $\\tau = 0.6$. If the loss never drops below this threshold, $-1$ is reported.",
            "answer": "```python\nimport numpy as np\n\n# This program implements a dense-block neural network and compares the performance\n# of different activation functions (ReLU, GELU, SiLU) on a synthetic classification task.\n\ndef relu(x):\n    \"\"\"Rectified Linear Unit activation function.\"\"\"\n    return np.maximum(0, x)\n\ndef relu_grad(x):\n    \"\"\"Gradient of the ReLU function.\"\"\"\n    return (x > 0).astype(float)\n\ndef gelu_approx(x):\n    \"\"\"Gaussian Error Linear Unit (GELU) approximation.\"\"\"\n    a = np.sqrt(2 / np.pi)\n    b = 0.044715\n    return 0.5 * x * (1 + np.tanh(a * (x + b * x**3)))\n\ndef gelu_approx_grad(x):\n    \"\"\"Gradient of the GELU approximation.\"\"\"\n    a = np.sqrt(2 / np.pi)\n    b = 0.044715\n    x_cubed = x**3\n    inner_term = x + b * x_cubed\n    g_x = a * inner_term\n    tanh_g_x = np.tanh(g_x)\n    g_prime_x = a * (1 + 3 * b * x**2)\n    sech_g_x_sq = 1 - tanh_g_x**2\n    return 0.5 * (1 + tanh_g_x) + 0.5 * x * sech_g_x_sq * g_prime_x\n\ndef silu(x):\n    \"\"\"Sigmoid Linear Unit (SiLU) activation function.\"\"\"\n    # sigma(x) = 1 / (1 + exp(-x))\n    return x / (1 + np.exp(-x))\n\ndef silu_grad(x):\n    \"\"\"Gradient of the SiLU function.\"\"\"\n    sigma = 1 / (1 + np.exp(-x))\n    return sigma * (1 + x * (1 - sigma))\n\ndef generate_data(C, N_c, r, sigma, seed=42):\n    \"\"\"Generates a synthetic C-class dataset.\"\"\"\n    rng = np.random.default_rng(seed)\n    N = C * N_c\n    X = np.zeros((N, 2))\n    y = np.zeros(N, dtype=int)\n    for c in range(C):\n        angle = 2 * np.pi * c / C\n        mu = r * np.array([np.cos(angle), np.sin(angle)])\n        start_idx, end_idx = c * N_c, (c + 1) * N_c\n        X[start_idx:end_idx, :] = mu + rng.normal(0, sigma, size=(N_c, 2))\n        y[start_idx:end_idx] = c\n    return X, y\n\nclass DenseBlockNet:\n    \"\"\"A neural network with a simplified dense block architecture.\"\"\"\n    def __init__(self, input_dim, num_classes, L, k, activation, seed):\n        self.input_dim = input_dim\n        self.num_classes = num_classes\n        self.L = L\n        self.k = k\n        self.rng = np.random.default_rng(seed)\n        self.activation_fn, self.activation_grad = self._get_activation(activation)\n\n        self.weights = []\n        self.biases = []\n\n        # Initialize dense block layer parameters\n        current_dim = self.input_dim\n        for _ in range(L):\n            fan_in = current_dim\n            std_dev = np.sqrt(2.0 / fan_in)\n            self.weights.append(self.rng.normal(0, std_dev, (fan_in, k)))\n            self.biases.append(np.zeros(k))\n            current_dim += k\n        \n        # Initialize classifier layer parameters\n        fan_in = current_dim\n        std_dev = np.sqrt(2.0 / fan_in)\n        self.weights.append(self.rng.normal(0, std_dev, (fan_in, num_classes)))\n        self.biases.append(np.zeros(num_classes))\n\n    def _get_activation(self, name):\n        if name == 'ReLU': return relu, relu_grad\n        if name == 'GELU': return gelu_approx, gelu_approx_grad\n        if name == 'SiLU': return silu, silu_grad\n        raise ValueError(f\"Unknown activation function: {name}\")\n\n    def forward(self, X):\n        cache = {'h_outputs': [], 'z_outputs': [], 'layer_inputs': []}\n        layer_input = X\n        \n        for i in range(self.L):\n            cache['layer_inputs'].append(layer_input)\n            W, b = self.weights[i], self.biases[i]\n            z = layer_input @ W + b\n            h = self.activation_fn(z)\n            cache['z_outputs'].append(z)\n            cache['h_outputs'].append(h)\n            layer_input = np.concatenate([X] + cache['h_outputs'], axis=1)\n\n        final_H = layer_input\n        cache['final_H'] = final_H\n        \n        W_c, b_c = self.weights[-1], self.biases[-1]\n        logits = final_H @ W_c + b_c\n\n        stable_logits = logits - np.max(logits, axis=1, keepdims=True)\n        exp_logits = np.exp(stable_logits)\n        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n        cache['probs'] = probs\n        return probs, cache\n\n    def backward(self, X, Y_one_hot, cache):\n        N = X.shape[0]\n        grads = {'weights': [None]*(self.L + 1), 'biases': [None]*(self.L + 1)}\n\n        d_logits = (cache['probs'] - Y_one_hot) / N\n        H_final = cache['final_H']\n        W_c = self.weights[-1]\n        \n        grads['weights'][-1] = H_final.T @ d_logits\n        grads['biases'][-1] = np.sum(d_logits, axis=0)\n        d_H_final = d_logits @ W_c.T\n\n        d_h_accum = {}\n        current_dim = self.input_dim\n        for i in range(self.L):\n            d_h_accum[i] = d_H_final[:, current_dim : current_dim + self.k]\n            current_dim += self.k\n\n        for i in range(self.L - 1, -1, -1):\n            d_h = d_h_accum[i]\n            z = cache['z_outputs'][i]\n            d_z = d_h * self.activation_grad(z)\n            \n            layer_input = cache['layer_inputs'][i]\n            W = self.weights[i]\n            grads['weights'][i] = layer_input.T @ d_z\n            grads['biases'][i] = np.sum(d_z, axis=0)\n            \n            d_layer_input = d_z @ W.T\n            current_dim = self.input_dim\n            for j in range(i):\n                grad_for_hj = d_layer_input[:, current_dim : current_dim + self.k]\n                d_h_accum[j] += grad_for_hj\n                current_dim += self.k\n        return grads\n\n    def train(self, X, y, epochs, learning_rate, loss_threshold):\n        N, C = X.shape[0], self.num_classes\n        y_one_hot = np.zeros((N, C)); y_one_hot[np.arange(N), y] = 1\n        first_epoch_below_threshold = -1\n\n        for epoch in range(1, epochs + 1):\n            probs, cache = self.forward(X)\n            loss = -np.sum(np.log(probs[np.arange(N), y] + 1e-9)) / N\n\n            if loss < loss_threshold and first_epoch_below_threshold == -1:\n                first_epoch_below_threshold = epoch\n            \n            grads = self.backward(X, y_one_hot, cache)\n            for i in range(self.L + 1):\n                self.weights[i] -= learning_rate * grads['weights'][i]\n                self.biases[i] -= learning_rate * grads['biases'][i]\n        \n        final_probs, _ = self.forward(X)\n        final_loss = -np.sum(np.log(final_probs[np.arange(N), y] + 1e-9)) / N\n        predictions = np.argmax(final_probs, axis=1)\n        final_accuracy = np.mean(predictions == y)\n        \n        return final_loss, final_accuracy, first_epoch_below_threshold\n\ndef solve():\n    test_cases = [\n        ('ReLU', 3, 12, 0.05, 200, 123),\n        ('GELU', 3, 12, 0.05, 200, 123),\n        ('SiLU', 3, 12, 0.05, 200, 123),\n        ('ReLU', 1, 12, 0.05, 200, 123),\n        ('GELU', 3, 12, 0.005, 200, 123),\n    ]\n\n    X_data, y_data = generate_data(C=3, N_c=100, r=3, sigma=0.5, seed=0)\n    \n    results = []\n    for activation, L, k, eta, E, s in test_cases:\n        model = DenseBlockNet(\n            input_dim=2, num_classes=3, L=L, k=k, activation=activation, seed=s)\n        \n        loss, acc, epoch_idx = model.train(\n            X_data, y_data, epochs=E, learning_rate=eta, loss_threshold=0.6)\n        \n        results.append([round(loss, 4), round(acc, 3), epoch_idx])\n\n    outer_parts = []\n    for res in results:\n        outer_parts.append(f\"[{res[0]},{res[1]},{res[2]}]\")\n    print(f\"[[{','.join(outer_parts)}]]\")\n\nsolve()\n```"
        }
    ]
}