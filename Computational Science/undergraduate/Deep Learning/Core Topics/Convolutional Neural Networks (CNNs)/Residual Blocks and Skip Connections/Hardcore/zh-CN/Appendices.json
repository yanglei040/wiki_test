{
    "hands_on_practices": [
        {
            "introduction": "理论是指导，实践是检验。本节将通过一系列动手实践，带你从高层的架构选择深入到残差模块内部的设计细节。我们将首先通过量化分析来理解不同跳跃连接策略的根本差异，从而为你构建一个坚实的架构设计基础。这个练习将让你计算和比较不同架构下的梯度路径多样性和计算成本，揭示局部残差连接为何能创建指数级数量的有效路径，从而极大地缓解梯度消失问题，并促进了极深网络的训练 。",
            "id": "3169728",
            "problem": "您的任务是形式化并比较前馈网络中跳跃连接的两种放置方式：每个模块的局部残差跳跃和从输入到输出的单个全局跳跃。您的目标是从第一性原理出发，推导这些放置方式如何影响不同梯度路径的数量以及一个基于操作的训练时间代理指标，然后实现一个程序，为一组测试架构计算这些量。\n\n从以下基本原理开始：\n- 反向传播使用微积分的链式法则，当一个节点的输出是多个分支输出之和时，该和的雅可比矩阵等于各分支雅可比矩阵的和。设输入为 $x$，层函数为 $F_i$，输出为 $y$。\n- 对于一个将 $\\mathbb{R}^{d_i}$ 映射到 $\\mathbb{R}^{d_{i+1}}$ 的全连接线性层，其权重形状为 $d_{i+1} \\times d_i$，矩阵向量乘积的前向浮点运算次数（FLOPs）约为 $2\\,d_i\\,d_{i+1}$（输出中的每个条目需要一次乘法和一次加法）。在标准的反向模式自动微分中，对于线性层，反向传播过程包括计算关于输入和权重的梯度，这两项的成本规模都为 $2\\,d_i\\,d_{i+1}$，因此每层每次迭代（一次前向和一次反向）的总FLOPs为 $6\\,d_i\\,d_{i+1}$。此处的偏置成本可忽略不计。$\\mathbb{R}^{k}$ 中两个向量的标量加法消耗 $k$ FLOPs。\n\n需要分析的架构：\n1. 普通链式结构（无跳跃）：$L$ 个线性层，维度为 $[d_0,d_1,\\dots,d_L]$；输出为 $y = F_{L-1}\\circ \\dots \\circ F_0(x)$。\n2. 仅全局跳跃：与普通链式结构相同，但在输出端增加一个跳跃连接，$y = F_{L-1}\\circ \\dots \\circ F_0(x) + x$，这要求 $d_0 = d_L$ 以确保求和操作是良定义的。\n3. 仅局部残差跳跃：每个模块的输出为 $x_{i+1} = x_i + F_i(x_i)$，这要求 $d_i = d_{i+1}$ 以确保每次加法操作是良定义的。\n\n需要计算的定义：\n- 梯度路径多样性 $D$：梯度从 $y$ 到达 $x$ 的代数上不同的加法路径数量，这些路径因是否遍历每个残差分支而变得不同。\n- 基于操作的训练时间代理指标 $C$：一次训练迭代（一次前向加一次反向传播）的总FLOPs，通过将每层矩阵乘法的FLOPs与跳跃连接在前向传播中引入的任何加法操作的FLOPs相加来计算。在此模型中，加法节点的反向传播FLOPs被视为可忽略不计，而线性层的反向传播FLOPs如上所述被包括在内。\n\n您必须实现一个程序，对于每个测试用例，返回一个整数对 $[D,C]$。不涉及物理单位，也不使用角度。架构由一个指示跳跃类型的字符串和一个指示层宽度的维度列表指定。\n\n需要实现的测试套件（按此顺序）：\n- 案例 1：局部残差跳跃，$L = 3$，维度 $[64,64,64,64]$。\n- 案例 2：仅全局跳跃，$L = 3$，维度 $[64,64,64,64]$。\n- 案例 3：普通链式结构，$L = 3$，维度 $[64,64,64,64]$。\n- 案例 4：局部残差跳跃，$L = 1$，维度 $[32,32]$。\n- 案例 5：仅全局跳跃，$L = 10$，维度 $[16,16,16,16,16,16,16,16,16,16,16]$。\n- 案例 6：局部残差跳跃，$L = 10$，维度 $[16,16,16,16,16,16,16,16,16,16,16]$。\n\n输出规范：\n- 您的程序应生成单行输出，其中包含六个结果，形式为一个逗号分隔的列表，并用方括号括起来，其中每个结果本身是一个双元素列表 $[D,C]$。例如，格式为 $[[D_1,C_1],[D_2,C_2],\\dots,[D_6,C_6]]$，行内任何地方都没有空格。",
            "solution": "该问题要求对三种不同的前馈网络架构进行形式化和分析：普通的顺序链、带单个全局跳跃连接的链以及由局部残差块组成的网络。对于每种架构，我们必须推导两个量的表达式：梯度路径多样性（记为 $D$）和基于操作的训练时间代理指标（记为 $C$）。这些推导必须源于所提供的基本原理。\n\n网络层数记为 $L$，网络维度由列表 $[d_0, d_1, \\dots, d_L]$ 给出，其中 $d_0$ 是输入维度，$i > 0$ 时的 $d_i$ 是第 $(i-1)$ 层的输出维度。第 $i$ 层（$i$ 从 $0$ 到 $L-1$）实现的函数是 $F_i$，它从 $\\mathbb{R}^{d_i}$ 映射到 $\\mathbb{R}^{d_{i+1}}$。\n\n操作成本 $C$ 是一次完整训练迭代（一次前向传播和一次反向传播）的总浮点运算次数（FLOPs）。提供的成本模型如下：\n- 一个线性层 $F_i: \\mathbb{R}^{d_i} \\to \\mathbb{R}^{d_{i+1}}$ 的总迭代成本为 $6 d_i d_{i+1}$ FLOPs。\n- 在 $\\mathbb{R}^k$ 中两个向量相加的前向传播成本为 $k$ FLOPs，其反向传播成本可忽略不计。\n\n梯度路径多样性 $D$ 是梯度从输出 $y$ 流回输入 $x$ 的不同加法路径的数量。这是根据链式法则以及一个函数的和的雅可比矩阵等于各函数雅可比矩阵的和这一原理推导出来的。\n\n我们现在将为每种架构推导 $D$ 和 $C$ 的表达式。\n\n**1. 普通链式结构架构**\n\n在此配置中，各层按顺序组合。第 $i$ 层的输出（记为 $x_{i+1}$）是将函数 $F_i$ 应用于前一输出 $x_i$ 的结果。\n该架构定义为：\n$$ y = x_L = F_{L-1}(x_{L-1}) = F_{L-1} \\circ F_{L-2} \\circ \\dots \\circ F_0(x_0) $$\n其中 $x_0$ 是网络输入。\n\n梯度路径多样性 ($D_{plain}$):\n输出 $y$ 关于输入 $x_0$ 的雅可比矩阵通过应用链式法则得出：\n$$ \\frac{\\partial y}{\\partial x_0} = \\frac{\\partial F_{L-1}}{\\partial x_{L-1}} \\frac{\\partial F_{L-2}}{\\partial x_{L-2}} \\dots \\frac{\\partial F_0}{\\partial x_0} $$\n该表达式是雅可比矩阵的单一乘积。在 $y$ 对 $x_0$ 的函数依赖关系中没有加法项。因此，梯度只有一个传播路径。\n$$ D_{plain} = 1 $$\n\n基于操作的训练时间代理指标 ($C_{plain}$):\n总成本是 $L$ 个线性层中每一层成本的总和。层 $F_i$ 的成本为 $6 d_i d_{i+1}$。\n$$ C_{plain} = \\sum_{i=0}^{L-1} 6 d_i d_{i+1} $$\n\n**2. 仅全局跳跃架构**\n\n此架构在从输入 $x_0$到最终输出 $y$ 之间增加了一个跳跃连接。\n输出定义为：\n$$ y = (F_{L-1} \\circ F_{L-2} \\circ \\dots \\circ F_0(x_0)) + x_0 $$\n为使向量加法有明确定义，输入的维度必须与最后一层输出的维度相匹配，即 $d_0 = d_L$。\n\n梯度路径多样性 ($D_{global}$):\n输出 $y$ 是关于 $x_0$ 的两个函数之和：通过各层的深层路径（我们称之为 $H(x_0)$）和恒等路径 $I(x_0) = x_0$。根据所提供的原则，和的雅可比矩阵是雅可比矩阵的和：\n$$ \\frac{\\partial y}{\\partial x_0} = \\frac{\\partial H(x_0)}{\\partial x_0} + \\frac{\\partial I(x_0)}{\\partial x_0} = \\left( \\prod_{i=0}^{L-1} \\frac{\\partial F_{L-1-i}}{\\partial x_{L-1-i}} \\right) + I_{d_0} $$\n其中 $I_{d_0}$ 是大小为 $d_0 \\times d_0$ 的单位矩阵。这两个加法项对应于梯度的两个不同的代数路径。\n$$ D_{global} = 2 $$\n\n基于操作的训练时间代理指标 ($C_{global}$):\n成本包括 $L$ 个线性层的成本（与普通链式结构相同），外加最终向量加法的成本。该加法是在 $\\mathbb{R}^{d_L}$ 中的两个向量之间进行的。此加法的成本为 $d_L$ FLOPs。\n$$ C_{global} = \\left( \\sum_{i=0}^{L-1} 6 d_i d_{i+1} \\right) + d_L $$\n\n**3. 仅局部残差跳跃架构**\n\n此架构由一系列残差块组成。在每个块 $i$ 中，输入 $x_i$ 通过一个层 $F_i$，其结果与输入 $x_i$ 相加。\n递归定义为：\n$$ x_{i+1} = x_i + F_i(x_i) \\quad \\text{for } i = 0, \\dots, L-1 $$\n最终输出为 $y = x_L$。此结构要求每个块的输入和输出维度相同，因此对于所有 $i = 0, \\dots, L-1$，都有 $d_i = d_{i+1}$。这意味着所有维度 $d_0, \\dots, d_L$ 都必须相等。\n\n梯度路径多样性 ($D_{local}$):\n让我们将每个块的变换表示为一个算子。由于层 $F_i$ 是线性的，我们可以写成 $x_{i+1} = (I + F_i)(x_i)$，其中 $I$ 是恒等算子。从 $y=x_L$ 回溯到 $x_0$ 展开递归：\n$$ y = (I + F_{L-1}) (x_{L-1}) = (I + F_{L-1}) (I + F_{L-2}) \\dots (I + F_0) (x_0) $$\n展开这个 $L$ 个二项式的乘积，会得到一个包含 $2^L$ 个不同项的和。每个项都是一个独特的函数复合，通过从每个块 $(I+F_i)$ 中选择恒等算子 $I$ 或层函数 $F_i$ 形成。例如，当 $L=2$ 时，我们得到 $(I+F_1)(I+F_0) = F_1 \\circ F_0 + F_1 + F_0 + I$。这 $2^L$ 个项中的每一项都构成了一条从输入到输出的不同加法路径。\n$$ D_{local} = 2^L $$\n\n基于操作的训练时间代理指标 ($C_{local}$):\n总成本是 $L$ 个残差块中每个块成本的总和。对于每个块 $i$，计算涉及一个线性层 $F_i$ 和一次向量加法。该层的成本是 $6 d_i d_{i+1}$。加法 $x_i + F_i(x_i)$ 涉及 $\\mathbb{R}^{d_{i+1}}$ 中的向量，成本为 $d_{i+1}$ FLOPs。总成本是所有 $L$ 个块的这些成本之和。\n$$ C_{local} = \\sum_{i=0}^{L-1} (6 d_i d_{i+1} + d_{i+1}) $$\n\n这些推导出的公式被用于计算指定测试用例的值。",
            "answer": "```python\nimport numpy as np\n\ndef calculate_costs(skip_type: str, dims: list[int]) -> list[int]:\n    \"\"\"\n    Calculates gradient path diversity (D) and operations proxy (C) for a given architecture.\n\n    Args:\n        skip_type: A string indicating the architecture ('plain', 'global', 'local').\n        dims: A list of integers representing layer dimensions [d_0, d_1, ..., d_L].\n\n    Returns:\n        A list [D, C] containing the two computed integer values.\n    \"\"\"\n    L = len(dims) - 1\n    if L == 0: # A network with 0 layers is just an identity\n        if skip_type == 'plain':\n            return [1, 0]\n        elif skip_type == 'global': # x = x+x, not well defined in problem, assume plain\n             return [1, 0]\n        elif skip_type == 'local': # x_1 = x_0 + F_0(x_0) -> L must be at least 1\n            return [0, 0] # Or error, but problem cases have L >= 1.\n\n    # Calculate base cost from linear layers, common to all architectures\n    cost_layers = 0\n    for i in range(L):\n        d_i = dims[i]\n        d_i_plus_1 = dims[i+1]\n        cost_layers += 6 * d_i * d_i_plus_1\n\n    D = 0\n    C = 0\n\n    if skip_type == 'plain':\n        # D: Single path through all layers.\n        D = 1\n        # C: Sum of costs of linear layers.\n        C = cost_layers\n    elif skip_type == 'global':\n        # D: Two paths - one through layers, one through identity skip.\n        D = 2\n        # C: Cost of layers plus one vector addition at the output.\n        d_L = dims[L]\n        C = cost_layers + d_L\n    elif skip_type == 'local':\n        # D: 2 choices at each of L blocks (identity or layer) -> 2^L paths.\n        D = 2**L\n        # C: Cost of layers plus L vector additions, one for each block.\n        cost_additions = 0\n        for i in range(L):\n            d_i_plus_1 = dims[i+1]\n            cost_additions += d_i_plus_1\n        C = cost_layers + cost_additions\n    \n    return [int(D), int(C)]\n\ndef solve():\n    \"\"\"\n    Executes the analysis for the predefined test suite and prints the results.\n    \"\"\"\n    test_cases = [\n        ('local', [64, 64, 64, 64]),\n        ('global', [64, 64, 64, 64]),\n        ('plain', [64, 64, 64, 64]),\n        ('local', [32, 32]),\n        ('global', [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]),\n        ('local', [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]),\n    ]\n\n    results = []\n    for skip_type, dims in test_cases:\n        result = calculate_costs(skip_type, dims)\n        results.append(result)\n\n    # Format the output string to be exactly as specified: [[D1,C1],[D2,C2],...]\n    # str() on a list adds spaces, so we remove them.\n    results_str = [str(r).replace(' ', '') for r in results]\n    output_str = f\"[{','.join(results_str)}]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在确定了局部残差连接的宏观架构后，我们将注意力转向单个残差模块的内部。当来自恒等路径和残差路径的两条信息流相加时，它们的统计特性（如均值和方差）可能存在不匹配，这会影响训练的稳定性，尤其是在使用了批量归一化（Batch Normalization）等技术时。这个练习将引导你通过基本的概率论原理，设计一个仿射变换来校准跳跃连接路径，以确保模块输出的统计分布保持稳定，这是保证深度网络稳健训练的一项关键实用技巧 。",
            "id": "3169660",
            "problem": "考虑一个深度神经网络中预激活残差块的单个通道。该块将一条恒等跳跃连接通路添加到一个残差通路上。残差通路包括带有可学习仿射参数的批量归一化 (BN) 和一个逐点非线性。对于给定的一个小批量和通道，假设以下每个样本的统计数据都得到了精确估计：\n- 恒等通路上的输入是一个随机变量 $x$，其均值为 $\\mu_{x}$，方差为 $\\sigma_{x}^{2}$。\n- 残差通路的输出（经过批量归一化及其学习到的仿射变换和非线性之后），记为 $r$，其均值为 $\\mu_{r}$，方差为 $\\sigma_{r}^{2}$。\n\n为进行此分析，假设 $x$ 和 $r$ 是独立的。如果不对跳跃连接进行对齐，则块的输出为 $y = x + r$。由于 $x$ 和 $r$ 可能具有不同的均值和方差，其和 $y$ 相对于通常为保证稳定训练而偏好的归一化状态可能会经历均值和方差的漂移。\n\n为了减少这种不匹配，假设我们在恒等跳跃连接通路上插入一个仿射对齐 $s = a x + b$（其中 $a \\in \\mathbb{R}$ 且 $b \\in \\mathbb{R}$），并形成输出 $y' = r + s$。你的任务是：\n- 仅使用期望的线性性质和独立随机变量和的方差，用 $a$、$b$、$\\mu_{x}$、$\\sigma_{x}^{2}$、$\\mu_{r}$ 和 $\\sigma_{r}^{2}$ 来表示并推导 $\\mathbb{E}[y']$ 和 $\\operatorname{Var}(y')$。\n- 施加对齐目标，使求和后的输出满足 $\\mathbb{E}[y'] = 0$ 和 $\\operatorname{Var}(y') = 1$。\n- 在约束条件 $a \\ge 0$（以保持恒等映射的方向）下，针对以下统计数据，求解满足对齐目标的唯一解 $(a,b)$：\n  $\\mu_{x} = 1.5$, $\\sigma_{x}^{2} = 1.0$, $\\mu_{r} = 0.25$, $\\sigma_{r}^{2} = 0.75$。\n- 将 $a$ 和 $b$ 都四舍五入到四位有效数字，并将这对数值作为一个行向量报告。\n\n给出你的最终答案，形式为一个单行矩阵 $\\begin{pmatrix} a  b \\end{pmatrix}$，并且不要包含单位。",
            "solution": "该问题要求推导残差块中跳跃连接的仿射对齐参数 $a$ 和 $b$，使得该块的输出均值为 $0$，方差为 $1$。\n\n首先，我们验证问题陈述。\n**步骤 1：提取已知条件**\n- 块的输出为 $y' = r + s$，其中 $s = ax + b$。\n- 恒等通路上的输入是一个随机变量 $x$，其均值为 $\\mathbb{E}[x] = \\mu_{x}$，方差为 $\\operatorname{Var}(x) = \\sigma_{x}^{2}$。\n- 残差通路的输出是一个随机变量 $r$，其均值为 $\\mathbb{E}[r] = \\mu_{r}$，方差为 $\\operatorname{Var}(r) = \\sigma_{r}^{2}$。\n- 假设 $x$ 和 $r$ 是独立的。\n- 对齐目标是 $\\mathbb{E}[y'] = 0$ 和 $\\operatorname{Var}(y') = 1$。\n- 施加了一个约束条件：$a \\ge 0$。\n- 具体的统计数值为：$\\mu_{x} = 1.5$, $\\sigma_{x}^{2} = 1.0$, $\\mu_{r} = 0.25$, $\\sigma_{r}^{2} = 0.75$。\n\n**步骤 2：使用提取的已知条件进行验证**\n该问题具有科学依据，它使用概率论的标准原​​理对深度学习中的一个常见场景进行建模。这是一个适定的问题，因为它提供了一套完整且一致的条件来确定参数 $a$ 和 $b$ 的唯一解。所提供的量在量纲上是一致的并且是合理的。该问题不违反任何无效性标准。\n\n**步骤 3：结论与行动**\n问题是有效的。我们继续进行求解。\n\n块的输出由残差通路输出 $r$ 和对齐后的跳跃连接输出 $s = ax + b$ 的和给出。\n$$y' = r + s = r + (ax + b)$$\n我们必须首先推导输出的均值 $\\mathbb{E}[y']$ 和方差 $\\operatorname{Var}(y')$ 的表达式。\n\n**均值 $\\mathbb{E}[y']$ 的推导**\n根据期望算子的线性性质，我们有：\n$$\\mathbb{E}[y'] = \\mathbb{E}[r + ax + b]$$\n$$\\mathbb{E}[y'] = \\mathbb{E}[r] + \\mathbb{E}[ax] + \\mathbb{E}[b]$$\n由于 $a$ 和 $b$ 是常数，所以 $\\mathbb{E}[ax] = a\\mathbb{E}[x]$ 且 $\\mathbb{E}[b] = b$。\n$$\\mathbb{E}[y'] = \\mathbb{E}[r] + a\\mathbb{E}[x] + b$$\n代入给定的符号均值：\n$$\\mathbb{E}[y'] = \\mu_{r} + a\\mu_{x} + b$$\n\n**方差 $\\operatorname{Var}(y')$ 的推导**\n$y'$ 的方差由下式给出：\n$$\\operatorname{Var}(y') = \\operatorname{Var}(r + ax + b)$$\n加上一个常数 $b$ 不会改变方差，因此 $\\operatorname{Var}(r + ax + b) = \\operatorname{Var}(r + ax)$。\n$$\\operatorname{Var}(y') = \\operatorname{Var}(r + ax)$$\n问题陈述中指出随机变量 $x$ 和 $r$ 是独立的。因此，$ax$ 和 $r$ 也是独立的。对于独立的随机变量，其和的方差等于方差的和：\n$$\\operatorname{Var}(y') = \\operatorname{Var}(r) + \\operatorname{Var}(ax)$$\n使用性质 $\\operatorname{Var}(cX) = c^2\\operatorname{Var}(X)$（其中 $c$ 是常数），我们得到 $\\operatorname{Var}(ax) = a^2\\operatorname{Var}(x)$。\n$$\\operatorname{Var}(y') = \\operatorname{Var}(r) + a^2\\operatorname{Var}(x)$$\n代入给定的符号方差：\n$$\\operatorname{Var}(y') = \\sigma_{r}^{2} + a^2\\sigma_{x}^{2}$$\n\n**求解 $a$ 和 $b$**\n我们现在应用对齐目标：$\\mathbb{E}[y'] = 0$ 和 $\\operatorname{Var}(y') = 1$。这会得到一个由两个方程组成的方程组：\n$$(1) \\quad \\mu_{r} + a\\mu_{x} + b = 0$$\n$$(2) \\quad \\sigma_{r}^{2} + a^2\\sigma_{x}^{2} = 1$$\n\n我们从方程 $(2)$ 解出 $a$：\n$$a^2\\sigma_{x}^{2} = 1 - \\sigma_{r}^{2}$$\n$$a^2 = \\frac{1 - \\sigma_{r}^{2}}{\\sigma_{x}^{2}}$$\n$$a = \\pm \\sqrt{\\frac{1 - \\sigma_{r}^{2}}{\\sigma_{x}^{2}}}$$\n问题包含了约束条件 $a \\ge 0$，这是一个常见的选择，用以保持恒等映射的方向。因此，我们取正根：\n$$a = \\sqrt{\\frac{1 - \\sigma_{r}^{2}}{\\sigma_{x}^{2}}}$$\n\n接下来，我们从方程 $(1)$ 解出 $b$：\n$$b = -(\\mu_{r} + a\\mu_{x})$$\n\n现在我们代入给定的数值：$\\mu_{x} = 1.5$，$\\sigma_{x}^{2} = 1.0$，$\\mu_{r} = 0.25$ 和 $\\sigma_{r}^{2} = 0.75$。\n首先，我们计算 $a$：\n$$a = \\sqrt{\\frac{1 - 0.75}{1.0}} = \\sqrt{\\frac{0.25}{1.0}} = \\sqrt{0.25} = 0.5$$\n值 $a = 0.5$ 满足条件 $a \\ge 0$。\n\n接下来，我们使用已确定的 $a$ 值计算 $b$：\n$$b = -(0.25 + (0.5)(1.5))$$\n$$b = -(0.25 + 0.75)$$\n$$b = -1.0$$\n\n问题要求将 $a$ 和 $b$ 都四舍五入到四位有效数字。\n- 对于 $a = 0.5$，四位有效数字为 $0.5000$。\n- 对于 $b = -1.0$，四位有效数字为 $-1.000$。\n\n解是数对 $(a, b) = (0.5000, -1.000)$，我们将其报告为一个行向量。",
            "answer": "$$\\boxed{\\begin{pmatrix} 0.5000  -1.000 \\end{pmatrix}}$$"
        }
    ]
}