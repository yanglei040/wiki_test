{
    "hands_on_practices": [
        {
            "introduction": "在评估词嵌入时，一个基本问题是如何衡量两个词在向量空间中的“距离”或“相似度”。余弦相似度和欧几里得距离是两种最常用的度量标准。这个练习将引导你深入探究这两种度量在单位归一化向量上的内在联系，通过亲手计算来揭示它们在确定最近邻任务中是否等价。",
            "id": "3123037",
            "problem": "给定一个小的词嵌入空间和一组黄金标准的词对相似度分数。您将从第一性原理出发，形式化如何评估邻居检索以及嵌入导出的相似度与黄金标准之间的相关性。您必须实现一个完整的、可运行的程序，按照以下规定执行评估。\n\n基本原理和定义：\n- 词嵌入是词汇表到向量的映射，我们将其表示为一个嵌入矩阵 $E \\in \\mathbb{R}^{n \\times d}$，其中 $n$ 是单词数量，$d$ 是嵌入维度。对于任何单词 $w$，其嵌入为 $x_w \\in \\mathbb{R}^{d}$。\n- 向量 $x \\in \\mathbb{R}^{d}$ 的 $\\ell_2$ 范数是 $\\|x\\|_2 = \\sqrt{\\sum_{i=1}^{d} x_i^2}$。\n- 单词 $w$ 的归一化向量是 $x'_w = \\dfrac{x_w}{\\|x_w\\|_2}$。\n- 两个归一化向量 $x', y' \\in \\mathbb{R}^{d}$ 之间的余弦相似度是 $\\operatorname{cos}(x', y') = x'^\\top y'$，因为对于单位向量，夹角的余弦等于它们的点积。\n- 两个归一化向量 $x', y' \\in \\mathbb{R}^{d}$ 之间的欧几里得距离是 $d(x', y') = \\|x' - y'\\|_2$。\n\n邻居检索和重叠度：\n- 对于每个查询词 $w$，将其余弦相似度下的 top-$k$ 邻居定义为集合 $N_k^{\\mathrm{cos}}(w)$，该集合通过按 $\\operatorname{cos}(x'_w, x'_u)$ 降序对所有其他词 $u \\neq w$ 进行排序获得，为确保确定性，使用单词字符串的字典序打破平局，然后取前 $k$ 个元素。\n- 对于同一个查询词 $w$，将其欧几里得距离下的 top-$k$ 邻居定义为集合 $N_k^{\\mathrm{euc}}(w)$，该集合通过按 $d(x'_w, x'_u)$ 升序对所有其他词 $u \\neq w$ 进行排序获得，使用单词字符串的字典序打破平局，然后取前 $k$ 个元素。\n- 单词 $w$ 的 top-$k$ 邻居重叠度是 $O_k(w) = \\dfrac{|N_k^{\\mathrm{cos}}(w) \\cap N_k^{\\mathrm{euc}}(w)|}{k}$。\n- 整个词汇表的平均重叠度是 $A_k = \\dfrac{1}{n} \\sum_{w} O_k(w)$。\n\n与黄金标准的相关性：\n- 设一个黄金标准的词对集合为 $P = \\{(w_i, w_j)\\}$，其黄金分数为 $S_{\\mathrm{gold}}(w_i, w_j) \\in \\mathbb{R}$。\n- 定义模型基于余弦的词对 $(w_i, w_j)$ 相似度为 $S_{\\mathrm{cos}}(w_i, w_j) = \\operatorname{cos}(x'_{w_i}, x'_{w_j})$。\n- 定义模型基于欧几里得距离的词对 $(w_i, w_j)$ 相似度为 $S_{\\mathrm{euc}}(w_i, w_j) = - d(x'_{w_i}, x'_{w_j})$，这样更大的值表示更高的相似度，排序与接近程度保持一致。\n- 计算在所有 $P$ 中的词对上，$S_{\\mathrm{gold}}$ 与 $S_{\\mathrm{cos}}$ 之间，以及 $S_{\\mathrm{gold}}$ 与 $S_{\\mathrm{euc}}$ 之间的斯皮尔曼等级相关系数（Spearman correlation）$\\rho$。\n\n您必须按照上述规定，对单位归一化嵌入 $x'_w$ 进行所有计算。\n\n测试套件和数据：\n- 包含 $n = 6$ 个单词的词汇表：$[\\text{cat}, \\text{dog}, \\text{lion}, \\text{car}, \\text{automobile}, \\text{vehicle}]$。\n- 嵌入维度 $d = 2$，具有以下原始嵌入（归一化之前）：\n  - $\\text{cat}: [0.9, 0.1]$\n  - $\\text{dog}: [0.85, 0.15]$\n  - $\\text{lion}: [0.8, 0.2]$\n  - $\\text{car}: [-0.9, 0.05]$\n  - $\\text{automobile}: [-0.88, 0.1]$\n  - $\\text{vehicle}: [-0.82, 0.2]$\n- 黄金标准词对 $P$ 及其分数 $S_{\\mathrm{gold}}$：\n  - $(\\text{cat}, \\text{dog}): 0.90$\n  - $(\\text{cat}, \\text{lion}): 0.80$\n  - $(\\text{dog}, \\text{lion}): 0.85$\n  - $(\\text{car}, \\text{automobile}): 0.95$\n  - $(\\text{car}, \\text{vehicle}): 0.82$\n  - $(\\text{automobile}, \\text{vehicle}): 0.88$\n  - $(\\text{cat}, \\text{car}): 0.06$\n  - $(\\text{dog}, \\text{car}): 0.07$\n  - $(\\text{lion}, \\text{car}): 0.05$\n  - $(\\text{cat}, \\text{vehicle}): 0.07$\n  - $(\\text{dog}, \\text{vehicle}): 0.09$\n  - $(\\text{lion}, \\text{vehicle}): 0.10$\n- 必须为 $k \\in \\{1, 3, 5\\}$ 计算 Top-$k$ 邻居重叠度，即 $k = 1$，$k = 3$ 和 $k = 5$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[A_1, A_3, A_5, \\rho_{\\mathrm{cos}}, \\rho_{\\mathrm{euc}}]$，其中每个元素都是一个浮点数。例如，输出形式为 $[\\text{float},\\text{float},\\text{float},\\text{float},\\text{float}]$。\n- 无物理单位适用；角度通过单位向量间的点积在余弦相似度中隐式处理。将所有实数量表示为十进制数。\n\n科学真实性和推导要求：\n- 从上述定义出发，而不是任何特定于某个实现的捷径或结果。确保邻居检索和相关性计算都源自所述定义。\n- 通过指定的平局打破规则和在所有计算前严格遵守单位归一化 $x'_w = \\dfrac{x_w}{\\|x_w\\|_2}$ 来确保确定性行为。\n\n您的程序必须是完全自包含的，并且不需要任何输入。它必须使用给定的词汇表、嵌入和黄金分数，并且必须以指定的确切格式生成单行输出。",
            "solution": "该任务是根据一组黄金标准的相似度分数来评估给定的词嵌入空间。评估包括两个部分：对不同度量标准下邻居检索稳定性的分析，以及模型导出的相似度与黄金标准分数之间的相关性分析。整个过程将根据问题陈述中定义的第一性原理进行。\n\n### 步骤1：嵌入归一化\n\n每个词的基本表示是其向量嵌入。所有的相似度和距离计算都定义在归一化向量上。对于一个给定的原始嵌入向量 $x_w \\in \\mathbb{R}^d$，其归一化对应向量 $x'_w$ 通过除以其 $\\ell_2$ 范数 $\\|x_w\\|_2 = \\sqrt{\\sum_{i=1}^{d} x_{w,i}^2}$ 得到。\n\n$x'_w = \\frac{x_w}{\\|x_w\\|_2}$\n\n词汇表包含 $n=6$ 个词，其嵌入在 $\\mathbb{R}^2$ 中。原始嵌入及其对应的归一化单位向量如下：\n\n1.  $w = \\text{cat}$, $x_w = [0.9, 0.1]$。 $\\|x_w\\|_2 = \\sqrt{0.9^2 + 0.1^2} = \\sqrt{0.82} \\approx 0.9055385$。\n    $x'_w \\approx [0.9938837, 0.1104315]$\n2.  $w = \\text{dog}$, $x_w = [0.85, 0.15]$。 $\\|x_w\\|_2 = \\sqrt{0.85^2 + 0.15^2} = \\sqrt{0.745} \\approx 0.8631338$。\n    $x'_w \\approx [0.9847864, 0.1737976]$\n3.  $w = \\text{lion}$, $x_w = [0.8, 0.2]$。 $\\|x_w\\|_2 = \\sqrt{0.8^2 + 0.2^2} = \\sqrt{0.68} \\approx 0.8246211$。\n    $x'_w \\approx [0.9701425, 0.2425356]$\n4.  $w = \\text{car}$, $x_w = [-0.9, 0.05]$。 $\\|x_w\\|_2 = \\sqrt{(-0.9)^2 + 0.05^2} = \\sqrt{0.8125} \\approx 0.9013878$。\n    $x'_w \\approx [-0.9984604, 0.0554700]$\n5.  $w = \\text{automobile}$, $x_w = [-0.88, 0.1]$。 $\\|x_w\\|_2 = \\sqrt{(-0.88)^2 + 0.1^2} = \\sqrt{0.7844} \\approx 0.8856636$。\n    $x'_w \\approx [-0.9936048, 0.1129108]$\n6.  $w = \\text{vehicle}$, $x_w = [-0.82, 0.2]$。 $\\|x_w\\|_2 = \\sqrt{(-0.82)^2 + 0.2^2} = \\sqrt{0.7124} \\approx 0.8440379$。\n    $x'_w \\approx [-0.9715197, 0.2369560]$\n\n所有后续计算都将使用这些归一化向量 $x'_w$。\n\n### 步骤2：邻居检索和平均重叠度 ($A_k$)\n\n我们被要求使用两个不同的标准为每个词 $w$ 找到 top-$k$ 邻居：降序余弦相似度和升序欧几里得距离。\n-   余弦相似度: $\\operatorname{cos}(x'_w, x'_u) = x'^\\top_w x'_u$\n-   欧几里得距离: $d(x'_w, x'_u) = \\|x'_w - x'_u\\|_2$\n\n对于单位向量，这两个度量之间存在一个关键的数学关系。平方欧几里得距离可以表示为余弦相似度的函数：\n$$d(x'_w, x'_u)^2 = \\|x'_w - x'_u\\|_2^2 = (x'_w - x'_u)^\\top(x'_w - x'_u)$$\n$$= x'^\\top_w x'_w - 2x'^\\top_w x'_u + x'^\\top_u x'_u$$\n由于 $x'_w$ 和 $x'_u$ 是单位向量，$\\|x'_w\\|_2^2 = x'^\\top_w x'_w = 1$ 且 $\\|x'_u\\|_2^2 = x'^\\top_u x'_u = 1$。因此，\n$$d(x'_w, x'_u)^2 = 1 - 2\\operatorname{cos}(x'_w, x'_u) + 1 = 2(1 - \\operatorname{cos}(x'_w, x'_u))$$\n$$d(x'_w, x'_u) = \\sqrt{2(1 - \\operatorname{cos}(x'_w, x'_u))}$$\n函数 $f(c) = \\sqrt{2(1-c)}$ 对于 $c \\in [-1, 1]$ 是一个严格单调递减的函数。这意味着较高的余弦相似度分数意味着较低的欧几里得距离，反之亦然。因此，按余弦相似度降序对邻居列表进行排序在数学上等同于按欧几里得距离升序对它们进行排序。\n\n问题规定，相似度或距离值中的任何平局都必须通过词字符串的字典序来打破。由于这个平局打破规则对于两种排序方案是相同的，并且主要排序标准产生相同的排序，因此最终的邻居排序列表将是完全相同的。\n$$N_k^{\\mathrm{cos}}(w) = N_k^{\\mathrm{euc}}(w) \\quad \\forall w, k$$\n一个词 $w$ 的 top-$k$ 邻居重叠度，定义为 $O_k(w) = \\frac{|N_k^{\\mathrm{cos}}(w) \\cap N_k^{\\mathrm{euc}}(w)|}{k}$，因此大大简化。因为这两个集合是相同的，它们的交集就是集合本身：\n$$|N_k^{\\mathrm{cos}}(w) \\cap N_k^{\\mathrm{euc}}(w)| = |N_k^{\\mathrm{cos}}(w)| = k$$\n因此，任何单个词的重叠度是：\n$$O_k(w) = \\frac{k}{k} = 1$$\n在整个词汇表上的平均重叠度 $A_k = \\frac{1}{n} \\sum_{w} O_k(w)$ 变为：\n$$A_k = \\frac{1}{n} \\sum_{w} 1 = \\frac{1}{n} \\cdot n = 1$$\n这对于任何 $k$ 的选择都成立。因此，我们可以无需对邻居列表进行进一步的数值计算就得出结论：\n$$A_1 = 1.0, \\quad A_3 = 1.0, \\quad A_5 = 1.0$$\n\n### 步骤3：与黄金标准的相关性 ($\\rho$)\n\n我们需要计算黄金标准分数 $S_{\\mathrm{gold}}$ 和两个模型导出的分数 $S_{\\mathrm{cos}}$ 和 $S_{\\mathrm{euc}}$ 之间的斯皮尔曼等级相关系数 $\\rho$。\n-   $S_{\\mathrm{cos}}(w_i, w_j) = \\operatorname{cos}(x'_{w_i}, x'_{w_j})$\n-   $S_{\\mathrm{euc}}(w_i, w_j) = -d(x'_{w_i}, x'_{w_j})$\n\n使用在步骤2中导出的关系，我们可以用 $S_{\\mathrm{cos}}$ 来表示 $S_{\\mathrm{euc}}$：\n$$S_{\\mathrm{euc}}(w_i, w_j) = -\\sqrt{2(1 - S_{\\mathrm{cos}}(w_i, w_j))}$$\n设 $g(c) = -\\sqrt{2(1-c)}$。关于 $c$ 的导数是 $g'(c) = - \\frac{1}{2\\sqrt{2(1-c)}}(-1) = \\frac{1}{\\sqrt{2(1-c)}}$。对于 $c \\in [-1, 1)$，$g'(c) > 0$。这表明 $S_{\\mathrm{euc}}$ 是 $S_{\\mathrm{cos}}$ 的一个严格单调递增函数。\n\n斯皮尔曼相关性 $\\rho$ 作用于数据的等级，而不是其原始值。由于 $S_{\\mathrm{euc}}$ 是 $S_{\\mathrm{cos}}$ 的一个严格单调变换，因此由 $S_{\\mathrm{euc}}$ 评分的任何一组词对的等级顺序将与由 $S_{\\mathrm{cos}}$ 评分的同一组词对的等级顺序相同。也就是说，对于任何词对列表，$\\operatorname{rank}(S_{\\mathrm{cos}}) = \\operatorname{rank}(S_{\\mathrm{euc}})$。\n\n鉴于等级是相同的，它们与第三个变量（$S_{\\mathrm{gold}}$ 的等级）的相关性也必须相同。\n$$\\rho_{\\mathrm{cos}} = \\rho(\\operatorname{rank}(S_{\\mathrm{gold}}), \\operatorname{rank}(S_{\\mathrm{cos}}))$$\n$$\\rho_{\\mathrm{euc}} = \\rho(\\operatorname{rank}(S_{\\mathrm{gold}}), \\operatorname{rank}(S_{\\mathrm{euc}}))$$\n因为 $\\operatorname{rank}(S_{\\mathrm{cos}}) = \\operatorname{rank}(S_{\\mathrm{euc}})$，所以可以得出：\n$$\\rho_{\\mathrm{cos}} = \\rho_{\\mathrm{euc}}$$\n我们现在来计算这个值。我们为12个黄金标准词对中的每一个计算 $S_{\\mathrm{cos}}$ 分数。\n\n| 词对 $(w_i, w_j)$       | $S_{\\mathrm{gold}}$ | $S_{\\mathrm{cos}}(w_i, w_j)$ |\n| :---------------------- | :----------------- | :-------------------------- |\n| (cat, dog)              | $0.90$             | $0.99801$                   |\n| (cat, lion)             | $0.80$             | $0.99097$                   |\n| (dog, lion)             | $0.85$             | $0.99754$                   |\n| (car, automobile)       | $0.95$             | $0.99852$                   |\n| (car, vehicle)          | $0.82$             | $0.98184$                   |\n| (automobile, vehicle)   | $0.88$             | $0.99114$                   |\n| (cat, car)              | $0.06$             | $-0.98638$                  |\n| (dog, car)              | $0.07$             | $-0.97394$                  |\n| (lion, car)             | $0.05$             | $-0.95532$                  |\n| (cat, vehicle)          | $0.07$             | $-0.93666$                  |\n| (dog, vehicle)          | $0.09$             | $-0.91238$                  |\n| (lion, vehicle)         | $0.10$             | $-0.88126$                  |\n\n分数列表如下：\n$S_{\\mathrm{gold}} = [0.90, 0.80, 0.85, 0.95, 0.82, 0.88, 0.06, 0.07, 0.05, 0.07, 0.09, 0.10]$\n$S_{\\mathrm{cos}} = [0.99801, 0.99097, 0.99754, 0.99852, 0.98184, 0.99114, -0.98638, -0.97394, -0.95532, -0.93666, -0.91238, -0.88126]$\n\n计算这两个列表之间的斯皮尔曼等级相关系数，得到 $\\rho_{\\mathrm{cos}}$ 和 $\\rho_{\\mathrm{euc}}$ 的值。计算得出 $\\rho \\approx 0.93706$。\n\n### 结果总结\n\n理论推导，经直接计算确认，得出以下最终值：\n-   平均 top-$k$ 重叠度： $A_1 = 1.0$, $A_3 = 1.0$, $A_5 = 1.0$。\n-   斯皮尔曼相关性： $\\rho_{\\mathrm{cos}} = \\rho_{\\mathrm{euc}} \\approx 0.937062937062937$。\n\n最终输出是这些值的有序列表。\n$[1.0, 1.0, 1.0, 0.937062937062937, 0.937062937062937]$",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import spearmanr\n\ndef solve():\n    \"\"\"\n    Performs evaluation of word embeddings based on neighbor retrieval and correlation.\n    \"\"\"\n    # Define vocabulary and raw embeddings as per the problem statement\n    vocab = ['cat', 'dog', 'lion', 'car', 'automobile', 'vehicle']\n    raw_embeddings = {\n        'cat': np.array([0.9, 0.1]),\n        'dog': np.array([0.85, 0.15]),\n        'lion': np.array([0.8, 0.2]),\n        'car': np.array([-0.9, 0.05]),\n        'automobile': np.array([-0.88, 0.1]),\n        'vehicle': np.array([-0.82, 0.2])\n    }\n    word_to_idx = {word: i for i, word in enumerate(vocab)}\n\n    # Step 1: Normalize embeddings\n    normalized_embeddings = {}\n    for word, vec in raw_embeddings.items():\n        norm = np.linalg.norm(vec)\n        if norm > 0:\n            normalized_embeddings[word] = vec / norm\n        else:\n            # Handle zero vector case, though not present in this problem\n            normalized_embeddings[word] = vec\n\n    # Step 2: Compute average neighbor overlap A_k\n    k_values = [1, 3, 5]\n    avg_overlaps = []\n\n    for k in k_values:\n        total_overlap_ratio = 0.0\n        for query_word in vocab:\n            query_vec = normalized_embeddings[query_word]\n            \n            # Neighbors for comparison\n            other_words = [w for w in vocab if w != query_word]\n            \n            # Calculate cosine similarities and Euclidean distances\n            cos_similarities = []\n            euc_distances = []\n            for other_word in other_words:\n                other_vec = normalized_embeddings[other_word]\n                \n                # Cosine similarity\n                cos_sim = np.dot(query_vec, other_vec)\n                cos_similarities.append((other_word, cos_sim))\n                \n                # Euclidean distance\n                euc_dist = np.linalg.norm(query_vec - other_vec)\n                euc_distances.append((other_word, euc_dist))\n            \n            # Sort neighbors based on specified criteria\n            # For cosine: descending similarity, then lexicographic word order\n            # The key (-s, w) achieves this: sorts by -s (descending s) then w (ascending w)\n            cos_similarities.sort(key=lambda x: (-x[1], x[0]))\n            \n            # For Euclidean: ascending distance, then lexicographic word order\n            # The key (d, w) achieves this: sorts by d (ascending d) then w (ascending w)\n            euc_distances.sort(key=lambda x: (x[1], x[0]))\n            \n            # Get top-k neighbor sets\n            top_k_cos = set(word for word, sim in cos_similarities[:k])\n            top_k_euc = set(word for word, dist in euc_distances[:k])\n            \n            # Calculate overlap\n            intersection_size = len(top_k_cos.intersection(top_k_euc))\n            overlap_ratio = intersection_size / k\n            total_overlap_ratio += overlap_ratio\n            \n        avg_overlaps.append(total_overlap_ratio / len(vocab))\n\n    # Step 3: Compute Spearman correlation with gold standard\n    gold_standard = {\n        ('cat', 'dog'): 0.90,\n        ('cat', 'lion'): 0.80,\n        ('dog', 'lion'): 0.85,\n        ('car', 'automobile'): 0.95,\n        ('car', 'vehicle'): 0.82,\n        ('automobile', 'vehicle'): 0.88,\n        ('cat', 'car'): 0.06,\n        ('dog', 'car'): 0.07,\n        ('lion', 'car'): 0.05,\n        ('cat', 'vehicle'): 0.07,\n        ('dog', 'vehicle'): 0.09,\n        ('lion', 'vehicle'): 0.10\n    }\n\n    gold_scores = []\n    model_cos_scores = []\n    model_euc_scores = []\n\n    # Ensure a consistent order for pairs\n    pairs = sorted(list(gold_standard.keys()))\n\n    for w1, w2 in pairs:\n        gold_scores.append(gold_standard[(w1, w2)])\n        \n        vec1 = normalized_embeddings[w1]\n        vec2 = normalized_embeddings[w2]\n        \n        # Calculate model scores S_cos and S_euc\n        s_cos = np.dot(vec1, vec2)\n        s_euc = -np.linalg.norm(vec1 - vec2)\n        \n        model_cos_scores.append(s_cos)\n        model_euc_scores.append(s_euc)\n\n    # Compute Spearman correlation coefficients\n    rho_cos, _ = spearmanr(gold_scores, model_cos_scores)\n    rho_euc, _ = spearmanr(gold_scores, model_euc_scores)\n\n    # Combine all results\n    final_results = avg_overlaps + [rho_cos, rho_euc]\n\n    # Print the final output in the specified format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "语言的意义不仅取决于词汇，还取决于它们的排列顺序——“狗咬人”和“人咬狗”的含义截然不同。本练习将通过这个经典的例子，让你动手评估两种不同的短语表示模型——简单的词袋模型和考虑位置的线性模型——在捕捉句子结构方面的能力差异。通过这项实践，你将直观地理解为何简单的向量求和无法区分语序，以及如何设计对语序敏感的组合函数。",
            "id": "3123059",
            "problem": "考虑在自然语言处理（NLP）中，对由词嵌入构建的短语表示进行评估。假设每个词由一个实向量空间中的向量表示，并考虑两个主语和宾语顺序相反的短语：“dog bites man”和“man bites dog”。假设词嵌入在 $\\mathbb{R}^2$ 中给出为 $v_{\\text{dog}} = (1,0)$、$v_{\\text{bites}} = (0,1)$ 和 $v_{\\text{man}} = (1,1)$。为一个三词短语 $w_1\\,w_2\\,w_3$ 定义以下两种组合函数：\n\n1. 词袋求和（顺序不敏感）：$s = v_{w_1} + v_{w_2} + v_{w_3}$。\n\n2. 使用不同位置矩阵的位置感知线性组合（顺序敏感）：\n$$\nW_{\\text{subj}} = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix},\\quad\nW_{\\text{verb}} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix},\\quad\nW_{\\text{obj}} = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix},\n$$\n以及 $p = W_{\\text{subj}} v_{w_1} + W_{\\text{verb}} v_{w_2} + W_{\\text{obj}} v_{w_3}$。\n\n您将通过使用余弦相似度和欧几里得距离，在每种组合函数下比较这两个短语，来评估这些表示是否能检测到词序差异。选择所有正确的陈述：\n\nA. 在词袋求和 $s = v_{w_1} + v_{w_2} + v_{w_3}$ 下，“dog bites man”和“man bites dog”的组合向量是相同的，因此它们的余弦相似度等于 $1$。\n\nB. 在使用给定矩阵的位置感知组合 $p = W_{\\text{subj}} v_{w_1} + W_{\\text{verb}} v_{w_2} + W_{\\text{obj}} v_{w_3}$ 下，“dog bites man”和“man bites dog”之间的余弦相似度等于 $\\frac{12}{13}$，并且严格小于 $1$。\n\nC. 在求和前将每个词向量归一化为单位长度，即 $s = \\sum_{i=1}^{3} \\frac{v_{w_i}}{\\|v_{w_i}\\|}$，会破坏加法的交换律，从而解决词袋求和中的词序不敏感问题。\n\nD. 将求和替换为元素级平均 $s = \\frac{1}{3}\\big(v_{w_1}+v_{w_2}+v_{w_3}\\big)$ 可以防止“dog bites man”和“man bites dog”对齐，因为平均操作对不同位置的权重不同。\n\nE. 在词袋求和下，“dog bites man”和“man bites dog”这两个短语向量之间的欧几里得距离为 $0$，因此该测试正确地指出了模型未能编码词序的问题。",
            "solution": "首先验证问题陈述，以确保其科学基础扎实、定义明确且客观。\n\n### 第1步：提取已知条件\n\n-   **短语**：“dog bites man”和“man bites dog”。\n-   **词嵌入**：在 $\\mathbb{R}^2$ 中，$v_{\\text{dog}} = (1,0)$、$v_{\\text{bites}} = (0,1)$、$v_{\\text{man}} = (1,1)$。\n-   **组合函数1（词袋求和）**：对于短语 $w_1\\,w_2\\,w_3$，组合向量为 $s = v_{w_1} + v_{w_2} + v_{w_3}$。\n-   **组合函数2（位置感知线性）**：对于短语 $w_1\\,w_2\\,w_3$，组合向量为 $p = W_{\\text{subj}} v_{w_1} + W_{\\text{verb}} v_{w_2} + W_{\\text{obj}} v_{w_3}$。\n-   **位置矩阵**：\n    $$\n    W_{\\text{subj}} = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix},\\quad\n    W_{\\text{verb}} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix},\\quad\n    W_{\\text{obj}} = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}.\n    $$\n-   **任务**：使用余弦相似度和欧几里得距离，在每种组合函数下比较两个短语的表示，以评估它们是否能检测到词序差异。\n\n### 第2步：使用提取的已知条件进行验证\n\n-   **科学基础**：该问题使用了自然语言处理（NLP）和线性代数中的标准概念。词袋模型、通过线性变换的组合语义、余弦相似度和欧几里得距离都是该领域的基本工具。该场景是一个简化但概念上有效的示例，说明了如何建模和评估句法结构。\n-   **定义明确**：该问题提供了所有必要的定义、向量和矩阵。数学运算（向量加法、矩阵-向量乘法、点积、范数）都有明确定义。问题的每个部分都可以计算出唯一、无歧义的答案。\n-   **客观性**：问题使用精确的数学语言陈述，没有主观性或歧义。\n\n### 第3步：结论与行动\n\n问题陈述是有效的。这是一个清晰且自洽的练习，将线性代数应用于NLP的一个基本概念。我们可以继续进行解题推导和选项分析。\n\n### 推导与分析\n\n让我们将短语“dog bites man”表示为 $P_1$，将“man bites dog”表示为 $P_2$。词序暗示了主-谓-宾结构。\n\n对于 $P_1$（“dog bites man”）：$w_1 = \\text{dog}, w_2 = \\text{bites}, w_3 = \\text{man}$。\n相应的词向量为 $v_1 = v_{\\text{dog}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$、$v_2 = v_{\\text{bites}} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$、$v_3 = v_{\\text{man}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。\n\n对于 $P_2$（“man bites dog”）：$w'_1 = \\text{man}, w'_2 = \\text{bites}, w'_3 = \\text{dog}$。\n相应的词向量为 $v'_1 = v_{\\text{man}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$、$v'_2 = v_{\\text{bites}} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$、$v'_3 = v_{\\text{dog}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n\n**组合函数1的分析：词袋求和 ($s$)**\n\n设 $s_1$ 为 $P_1$ 的向量，$s_2$ 为 $P_2$ 的向量。\n$$\ns_1 = v_{\\text{dog}} + v_{\\text{bites}} + v_{\\text{man}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1+0+1 \\\\ 0+1+1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n$$\n因为向量加法是可交换的，所以 $P_2$ 的和将是相同的：\n$$\ns_2 = v_{\\text{man}} + v_{\\text{bites}} + v_{\\text{dog}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1+0+1 \\\\ 1+1+0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n$$\n由于 $s_1 = s_2$，该模型对词序不敏感。\n\n**组合函数2的分析：位置感知线性 ($p$)**\n\n设 $p_1$ 为 $P_1$ 的向量，$p_2$ 为 $P_2$ 的向量。\n对于 $P_1$（“dog bites man”），主语是“dog”（$v_1$），谓语是“bites”（$v_2$），宾语是“man”（$v_3$）。\n$$\np_1 = W_{\\text{subj}}v_{\\text{dog}} + W_{\\text{verb}}v_{\\text{bites}} + W_{\\text{obj}}v_{\\text{man}}\n$$\n$$\np_1 = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n$$\np_1 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2+0+1 \\\\ 0+1+1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}\n$$\n对于 $P_2$（“man bites dog”），主语是“man”（$v'_1$），谓语是“bites”（$v'_2$），宾语是“dog”（$v'_3$）。\n$$\np_2 = W_{\\text{subj}}v_{\\text{man}} + W_{\\text{verb}}v_{\\text{bites}} + W_{\\text{obj}}v_{\\text{dog}}\n$$\n$$\np_2 = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\np_2 = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2+0+0 \\\\ 1+1+1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\n由于 $p_1 \\neq p_2$，该模型对词序敏感。\n\n### 逐项分析\n\n**A. 在词袋求和 $s = v_{w_1} + v_{w_2} + v_{w_3}$ 下，“dog bites man”和“man bites dog”的组合向量是相同的，因此它们的余弦相似度等于 $1$。**\n\n如上计算，$s_1 = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$ 且 $s_2 = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$。这两个向量是相同的。对于两个相同的非零向量，其余弦相似度 $\\cos(\\theta) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|}$ 为 $\\frac{u \\cdot u}{\\|u\\| \\|u\\|} = \\frac{\\|u\\|^2}{\\|u\\|^2} = 1$。该陈述符合事实。\n**结论：正确**\n\n**B. 在使用给定矩阵的位置感知组合 $p = W_{\\text{subj}} v_{w_1} + W_{\\text{verb}} v_{w_2} + W_{\\text{obj}} v_{w_3}$ 下，“dog bites man”和“man bites dog”之间的余弦相似度等于 $\\frac{12}{13}$，并且严格小于 $1$。**\n\n我们有 $p_1 = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}$ 和 $p_2 = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}$。点积为 $p_1 \\cdot p_2 = (3)(2) + (2)(3) = 6 + 6 = 12$。范数分别为 $\\|p_1\\| = \\sqrt{3^2 + 2^2} = \\sqrt{9+4} = \\sqrt{13}$ 和 $\\|p_2\\| = \\sqrt{2^2 + 3^2} = \\sqrt{4+9} = \\sqrt{13}$。余弦相似度为 $\\cos(\\theta) = \\frac{p_1 \\cdot p_2}{\\|p_1\\| \\|p_2\\|} = \\frac{12}{\\sqrt{13} \\sqrt{13}} = \\frac{12}{13}$。由于 $12  13$，我们有 $\\frac{12}{13}  1$。该陈述符合事实。\n**结论：正确**\n\n**C. 在求和前将每个词向量归一化为单位长度，即 $s = \\sum_{i=1}^{3} \\frac{v_{w_i}}{\\|v_{w_i}\\|}$，会破坏加法的交换律，从而解决词袋求和中的词序不敏感问题。**\n\n这个陈述包含一个基本的数学错误。向量加法在任何向量空间中都是一个可交换的运算。和 $\\hat{v}_a + \\hat{v}_b + \\hat{v}_c$ 总是等于 $\\hat{v}_c + \\hat{v}_b + \\hat{v}_a$，其中 $\\hat{v}_i$ 是任意向量。在求和*之前*对向量进行归一化，并不会改变求和本身的交换律。无论加数的顺序如何，最终得到的和向量都是相同的。因此，这个过程不能解决词序不敏感问题。前提“会破坏加法的交换律”是错误的。\n**结论：不正确**\n\n**D. 将求和替换为元素级平均 $s = \\frac{1}{3}\\big(v_{w_1}+v_{w_2}+v_{w_3}\\big)$ 可以防止“dog bites man”和“man bites dog”对齐，因为平均操作对不同位置的权重不同。**\n\n设 $S = v_{w_1}+v_{w_2}+v_{w_3}$。平均值就是 $\\frac{1}{3}S$。在选项A的分析中，我们发现两个短语的和向量 $S$ 是相同的：$s_1 = s_2 = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$。因此，平均向量也将是相同的：$\\frac{1}{3}s_1 = \\frac{1}{3}s_2 = \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix}$。这两个短语向量仍然完全对齐。此外，其理由“因为平均操作对不同位置的权重不同”是错误的。如此处定义的标准平均，对每个输入向量的权重都相同（$1/3$），而与其在序列中的位置无关。\n**结论：不正确**\n\n**E. 在词袋求和下，“dog bites man”和“man bites dog”这两个短语向量之间的欧几里得距离为 $0$，因此该测试正确地指出了模型未能编码词序的问题。**\n\n我们发现，在词袋求和下，这两个短语的向量是相同的：$s_1 = s_2 = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$。它们之间的欧几里得距离是 $d(s_1, s_2) = \\|s_1 - s_2\\| = \\|\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\\| = \\|\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\| = 0$。一个能够编码词序的模型应该为具有不同词序（因此含义不同）的短语生成不同的向量。距离为 $0$（或余弦相似度为 $1$）是模型未能区分这两个输入的数学标志。因此，测试（计算距离）得到结果为 $0$ 正确地指出了这一失败。这个推理是合理的。\n**结论：正确**",
            "answer": "$$\\boxed{ABE}$$"
        },
        {
            "introduction": "词嵌入能够从海量文本中学习到丰富的语义知识，但它们同样会习得并放大社会中存在的偏见，例如性别偏见。本练习提供了一个评估和减轻这种偏见的实践框架。你将学习如何通过向量运算定义和识别“偏见方向”，并应用零空间投影技术来“去偏”，同时评估这种操作对模型原有语义能力（如类比推理）的影响。",
            "id": "3123006",
            "problem": "给定一个合成词嵌入空间，旨在测试词向量中偏见和语义结构的评估。每个词都映射到实数坐标空间中的一个向量。将每个词向量视为 $\\mathbb{R}^d$ 的一个元素，其中 $d = 4$。令 $x_w \\in \\mathbb{R}^4$ 表示与词 $w$ 相关联的向量。使用以下词汇表 $V$ 和嵌入（每个坐标均明确给出）：\n\n- $x_{\\text{man}} = [\\,0.8,\\,0.0,\\,0.1,\\,1.0\\,]$\n- $x_{\\text{woman}} = [\\,0.8,\\,0.0,\\,0.1,\\,{-1.0}\\,]$\n- $x_{\\text{he}} = [\\,0.5,\\,0.0,\\,0.0,\\,1.0\\,]$\n- $x_{\\text{she}} = [\\,0.5,\\,0.0,\\,0.0,\\,{-1.0}\\,]$\n- $x_{\\text{king}} = [\\,0.9,\\,1.0,\\,0.0,\\,0.8\\,]$\n- $x_{\\text{queen}} = [\\,0.9,\\,1.0,\\,0.0,\\,{-0.8}\\,]$\n- $x_{\\text{uncle}} = [\\,0.7,\\,0.0,\\,0.0,\\,0.9\\,]$\n- $x_{\\text{aunt}} = [\\,0.7,\\,0.0,\\,0.0,\\,{-0.9}\\,]$\n- $x_{\\text{doctor}} = [\\,0.85,\\,0.0,\\,1.0,\\,0.2\\,]$\n- $x_{\\text{nurse}} = [\\,0.85,\\,0.0,\\,1.0,\\,{-0.2}\\,]$\n- $x_{\\text{child}} = [\\,0.6,\\,0.0,\\,0.0,\\,0.0\\,]$\n\n使用的基本原理和定义：\n\n- 词嵌入是一个映射 $w \\mapsto x_w \\in \\mathbb{R}^d$，配备了欧几里得内积 $u^\\top v$ 和范数 $\\lVert u \\rVert = \\sqrt{u^\\top u}$。\n- $u$ 和 $v$ 之间的余弦相似度为 $\\cos(u,v) = \\dfrac{u^\\top v}{\\lVert u \\rVert \\lVert v \\rVert}$。\n- 给定一组被视为性别对应词的配对词 $\\{(m_i,f_i)\\}_{i=1}^n$，定义差异向量 $d_i = x_{m_i} - x_{f_i}$。我们寻求一个单位方向 $b \\in \\mathbb{R}^d$，通过在单位范数约束 $\\lVert b \\rVert = 1$ 下最大化投影 $\\{d_i^\\top b\\}$ 的经验方差，来捕捉这些差异的主要共享方向。这是中心化差异集的第一主成分方向，源于正交约束下方差最大化的一般原理。\n- 用于去偏的零空间投影：对于任何词向量 $x_w$，移除其在 $b$ 方向上的分量，以产生一个去偏向量 $x_w' = x_w - (x_w^\\top b)\\,b$。此操作是到 $b$ 的生成空间的正交补空间上的投影。\n- 一组中性词 $S$ 相对于 $b$ 的偏见分数为 $B(S,b) = \\dfrac{1}{|S|}\\sum_{w \\in S} \\big| x_w^\\top b \\big|$。\n- 类比评估使用三余弦加法（3CosAdd）规则：对于意为“$a$之于$b$犹如$c$之于$d$”的类比 $(a,b,c,d)$，预测 $\\hat{d}$ 为使 $\\cos\\!\\big(x_w,\\, x_b - x_a + x_c \\big)$ 在 $w \\in V \\setminus \\{a,b,c\\}$ 上最大化的词（从候选中排除 $a$、$b$ 和 $c$）。准确率是 $\\hat{d} = d$ 的类比所占的比例，以小数表示。\n\n您的指令：\n\n- 通过求解中心化差异向量的单位方差最大化原理，从指定的性别配对词中计算偏见方向 $b$。使用任何与此原理一致的数值稳定方法。\n- 通过零空间投影 $x_w' = x_w - (x_w^\\top b)b$ 对所有嵌入进行去偏。\n- 通过计算去偏前后的 $B(S,b)$，测量一组中性词 $S$ 上的偏见减少量。\n- 使用上述3CosAdd规则，测量一组固定类比在去偏前后的类比准确率。\n\n测试套件和参数：\n\n- 使用中性词集 $S = \\{\\text{doctor},\\text{nurse},\\text{child}\\}$。\n- 使用包含以下 $6$ 个类比的类比集 $A$：\n    $$\n    (\\text{king},\\text{queen},\\text{man},\\text{woman}),\\;\n    (\\text{uncle},\\text{aunt},\\text{man},\\text{woman}),\\;\n    (\\text{he},\\text{she},\\text{man},\\text{woman}),\\;\n    (\\text{king},\\text{man},\\text{queen},\\text{woman}),\\;\n    (\\text{doctor},\\text{nurse},\\text{man},\\text{woman}),\\;\n    (\\text{man},\\text{woman},\\text{king},\\text{queen})\n    $$\n- 定义三个测试用例，每个用例包括一组性别配对词的选择和一种偏见方向计算方法的选择：\n    $$\n    \\text{用例 }1:\\; \\text{词对 }P_1 = \\{(\\text{he},\\text{she}),(\\text{man},\\text{woman}),(\\text{uncle},\\text{aunt}),(\\text{king},\\text{queen})\\},\\; \\text{方法 }M = \\text{variance-max}\n    $$\n    $$\n    \\text{用例 }2:\\; \\text{再次使用词对 }P_1,\\; \\text{方法 }M = \\text{mean-diff}\n    $$\n    $$\n    \\text{用例 }3:\\; \\text{词对 }P_2 = \\{(\\text{he},\\text{she}),(\\text{doctor},\\text{nurse})\\},\\; \\text{方法 }M = \\text{variance-max}\n    $$\n    这里，$\\text{variance-max}$ 指的是最大化中心化 $\\{d_i\\}$ 的经验方差的单位方向 $b$，而 $\\text{mean-diff}$ 指的是平均差异向量 $\\bar{d} = \\dfrac{1}{n}\\sum_{i=1}^n d_i$ 的单位归一化。\n- 对于每个用例，计算并记录一个包含 $6$ 个浮点数的列表：\n    $$\n    \\big[\\, B_{\\text{before}},\\, B_{\\text{after}},\\, B_{\\text{reduction}},\\, \\text{Acc}_{\\text{before}},\\, \\text{Acc}_{\\text{after}},\\, \\Delta \\text{Acc}\\,\\big]\n    $$\n    其中 $B_{\\text{reduction}} = B_{\\text{before}} - B_{\\text{after}}$ 且 $\\Delta \\text{Acc} = \\text{Acc}_{\\text{after}} - \\text{Acc}_{\\text{before}}$。准确率必须表示为 $[\\,0,\\,1\\,]$ 范围内的十进制数。\n\n最终输出规范：\n\n- 您的程序应生成单行输出，其中包含三个测试用例的结果，形式为方括号括起来的逗号分隔的列表的列表，格式完全如下：\n  $[\\,\\text{case}_1,\\text{case}_2,\\text{case}_3\\,]$，\n  其中每个 $\\text{case}_i$ 是其自身的包含上述指定顺序的 $6$ 个浮点数的列表。不应打印任何额外文本。",
            "solution": "### 基本设置\n\n给定了词汇表 $V$ 及对应的词嵌入 $x_w \\in \\mathbb{R}^4$。我们将它们表示为向量：\n- $x_{\\text{man}} = [0.8, 0.0, 0.1, 1.0]^\\top$\n- $x_{\\text{woman}} = [0.8, 0.0, 0.1, -1.0]^\\top$\n- $x_{\\text{he}} = [0.5, 0.0, 0.0, 1.0]^\\top$\n- $x_{\\text{she}} = [0.5, 0.0, 0.0, -1.0]^\\top$\n- $x_{\\text{king}} = [0.9, 1.0, 0.0, 0.8]^\\top$\n- $x_{\\text{queen}} = [0.9, 1.0, 0.0, -0.8]^\\top$\n- $x_{\\text{uncle}} = [0.7, 0.0, 0.0, 0.9]^\\top$\n- $x_{\\text{aunt}} = [0.7, 0.0, 0.0, -0.9]^\\top$\n- $x_{\\text{doctor}} = [0.85, 0.0, 1.0, 0.2]^\\top$\n- $x_{\\text{nurse}} = [0.85, 0.0, 1.0, -0.2]^\\top$\n- $x_{\\text{child}} = [0.6, 0.0, 0.0, 0.0]^\\top$\n\n中性词集为 $S = \\{\\text{doctor},\\text{nurse},\\text{child}\\}$。类比集 $A$ 由 $6$ 个指定的四元组构成。每个用例所需的输出是 $[\\, B_{\\text{before}},\\, B_{\\text{after}},\\, B_{\\text{reduction}},\\, \\text{Acc}_{\\text{before}},\\, \\text{Acc}_{\\text{after}},\\, \\Delta \\text{Acc}\\,]$。\n\n### 用例 1：词对 $P_1$，方法 `variance-max`\n\n**1. 计算偏见方向 $b$**\n性别配对词集为 $P_1 = \\{(\\text{he},\\text{she}),(\\text{man},\\text{woman}),(\\text{uncle},\\text{aunt}),(\\text{king},\\text{queen})\\}$。我们首先计算差异向量 $d_i = x_{m_i} - x_{f_i}$：\n- $d_1 = x_{\\text{he}} - x_{\\text{she}} = [0.0, 0.0, 0.0, 2.0]^\\top$\n- $d_2 = x_{\\text{man}} - x_{\\text{woman}} = [0.0, 0.0, 0.0, 2.0]^\\top$\n- $d_3 = x_{\\text{uncle}} - x_{\\text{aunt}} = [0.0, 0.0, 0.0, 1.8]^\\top$\n- $d_4 = x_{\\text{king}} - x_{\\text{queen}} = [0.0, 0.0, 0.0, 1.6]^\\top$\n\n平均差异向量为 $\\bar{d} = \\frac{1}{4}\\sum_{i=1}^4 d_i = [0.0, 0.0, 0.0, 1.85]^\\top$。\n接下来，我们找到中心化的差异向量 $c_i = d_i - \\bar{d}$：\n- $c_1 = [0.0, 0.0, 0.0, 0.15]^\\top$\n- $c_2 = [0.0, 0.0, 0.0, 0.15]^\\top$\n- $c_3 = [0.0, 0.0, 0.0, -0.05]^\\top$\n- $c_4 = [0.0, 0.0, 0.0, -0.25]^\\top$\n\n`variance-max` 方法需要找到这些中心化向量的第一主成分。这是协方差矩阵 $C = \\sum_{i=1}^4 c_i c_i^\\top$ 的最大特征值对应的特征向量。\n由于所有 $c_i$ 向量仅在第四个分量上非零，矩阵 $C$ 将仅在 $C_{4,4}$ 处有一个非零项：\n$$C_{4,4} = (0.15)^2 + (0.15)^2 + (-0.05)^2 + (-0.25)^2 = 0.0225 + 0.0225 + 0.0025 + 0.0625 = 0.11$$\n协方差矩阵为 $C = \\text{diag}(0, 0, 0, 0.11)$。其特征值是其对角线元素。最大特征值为 $\\lambda_{max} = 0.11$，对应的特征向量是标准基向量 $e_4$。\n因此，偏见方向为 $b = [0.0, 0.0, 0.0, 1.0]^\\top$。\n\n**2. 计算初始指标 ($B_{\\text{before}}, \\text{Acc}_{\\text{before}}$)**\n去偏前的偏见分数为 $B_{\\text{before}} = \\frac{1}{|S|}\\sum_{w \\in S} \\big| x_w^\\top b \\big|$。当 $b = [0,0,0,1]^\\top$ 时，$x_w^\\top b$ 就是 $x_w$ 的第四个分量。\n- $|x_{\\text{doctor}}^\\top b| = |0.2| = 0.2$\n- $|x_{\\text{nurse}}^\\top b| = |-0.2| = 0.2$\n- $|x_{\\text{child}}^\\top b| = |0.0| = 0.0$\n$B_{\\text{before}} = \\frac{1}{3}(0.2 + 0.2 + 0.0) = \\frac{0.4}{3} \\approx 0.133333$。\n\n类比准确率 $\\text{Acc}_{\\text{before}}$ 是 $6$ 个类比中被正确预测的比例。数值计算表明 $6$ 个类比中有 $4$ 个被正确预测，得出 $\\text{Acc}_{\\text{before}} = 4/6 \\approx 0.666667$。\n\n**3. 去偏与最终指标 ($B_{\\text{after}}, \\text{Acc}_{\\text{after}}$)**\n我们使用零空间投影 $x'_w = x_w - (x_w^\\top b)b$ 对所有向量进行去偏。当 $b = e_4$ 时，此操作仅将每个向量的第四个分量设置为 $0$。\n去偏后的偏见是基于新向量 $x'_w$ 计算的。根据构造，对于所有 $w$，都有 $(x'_w)^\\top b = 0$。\n$B_{\\text{after}} = \\frac{1}{|S|}\\sum_{w \\in S} \\big| (x'_w)^\\top b \\big| = \\frac{1}{3}(0+0+0) = 0.0$。\n偏见减少量为 $B_{\\text{reduction}} = B_{\\text{before}} - B_{\\text{after}} = \\frac{0.4}{3}$。\n\n去偏后，性别配对词变得相同（例如，$x'_{\\text{man}} = x'_{\\text{woman}} = [0.8, 0.0, 0.1, 0.0]^\\top$）。这简化了类比任务 $x_b - x_a + x_c$。例如，在 $(\\text{king},\\text{queen},\\text{man},\\text{woman})$ 中，目标变为 $x'_{\\text{queen}} - x'_{\\text{king}} + x'_{\\text{man}} = \\vec{0} + x'_{\\text{man}} = x'_{\\text{man}}$。由于 $x'_{\\text{man}} = x'_{\\text{woman}}$，预测的词是 `woman`，这是正确的。该模式适用于所有 $6$ 个类比，因此 $\\text{Acc}_{\\text{after}} = 1.0$。\n准确率的变化为 $\\Delta \\text{Acc} = \\text{Acc}_{\\text{after}} - \\text{Acc}_{\\text{before}} = 1.0 - 4/6 = 1/3 \\approx 0.333333$。\n\n**用例 1 的结果：** $[\\,0.133333, 0.0, 0.133333, 0.666667, 1.0, 0.333333\\,]$\n\n### 用例 2：词对 $P_1$，方法 `mean-diff`\n\n**1. 计算偏见方向 $b$**\n词对 $P_1$ 与用例 1 中的相同。`mean-diff` 方法需要对平均差异向量 $\\bar{d}$ 进行归一化。从用例 1 中，我们得到 $\\bar{d} = [0.0, 0.0, 0.0, 1.85]^\\top$。\n其范数为 $\\lVert\\bar{d}\\rVert = 1.85$。\n偏见方向为 $b = \\frac{\\bar{d}}{\\lVert\\bar{d}\\rVert} = \\frac{1}{1.85}[0.0, 0.0, 0.0, 1.85]^\\top = [0.0, 0.0, 0.0, 1.0]^\\top$。\n\n**2. 比较与结果**\n计算出的偏见方向 $b$ 与用例 1 中找到的相同。这是因为 $P_1$ 中所有词对的差异向量 $d_i$ 都是共线这一特殊事实的结果。由于 $b$ 相同，所有后续计算（$B_{\\text{before}}$、$Acc_{\\text{before}}$、去偏、$B_{\\text{after}}$、$Acc_{\\text{after}}$）必然与用例 1 中的计算相同。\n\n**用例 2 的结果：** $[\\,0.133333, 0.0, 0.133333, 0.666667, 1.0, 0.333333\\,]$\n\n### 用例 3：词对 $P_2$，方法 `variance-max`\n\n**1. 计算偏见方向 $b$**\n性别配对词集为 $P_2 = \\{(\\text{he},\\text{she}),(\\text{doctor},\\text{nurse})\\}$。差异向量为：\n- $d_1 = x_{\\text{he}} - x_{\\text{she}} = [0.0, 0.0, 0.0, 2.0]^\\top$\n- $d_2 = x_{\\text{doctor}} - x_{\\text{nurse}} = [0.0, 0.0, 0.0, 0.4]^\\top$\n\n平均差异为 $\\bar{d} = \\frac{1}{2}(d_1+d_2) = [0.0, 0.0, 0.0, 1.2]^\\top$。中心化向量为：\n- $c_1 = d_1 - \\bar{d} = [0.0, 0.0, 0.0, 0.8]^\\top$\n- $c_2 = d_2 - \\bar{d} = [0.0, 0.0, 0.0, -0.8]^\\top$\n\n协方差矩阵为 $C = c_1 c_1^\\top + c_2 c_2^\\top$。同样，只有 $C_{4,4}$ 非零：\n$$C_{4,4} = (0.8)^2 + (-0.8)^2 = 0.64 + 0.64 = 1.28$$\n协方差矩阵为 $C = \\text{diag}(0, 0, 0, 1.28)$。主特征向量再次为 $e_4$。\n因此，偏见方向为 $b = [0.0, 0.0, 0.0, 1.0]^\\top$。\n\n**2. 比较与结果**\n计算出的偏见方向 $b$ 再次与前几个用例中找到的相同。这是因为 $P_2$ 中词对的差异向量也与第四个基向量共线。由于 $b$ 未改变，所有得到的指标均与用例 1 和用例 2 的指标相同。\n\n**用例 3 的结果：** $[\\,0.133333, 0.0, 0.133333, 0.666667, 1.0, 0.333333\\,]$",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the word embedding bias evaluation problem for three test cases.\n    \"\"\"\n    \n    # Vocabulary and embeddings\n    embeddings_data = {\n        \"man\":    [0.8, 0.0, 0.1, 1.0],\n        \"woman\":  [0.8, 0.0, 0.1, -1.0],\n        \"he\":     [0.5, 0.0, 0.0, 1.0],\n        \"she\":    [0.5, 0.0, 0.0, -1.0],\n        \"king\":   [0.9, 1.0, 0.0, 0.8],\n        \"queen\":  [0.9, 1.0, 0.0, -0.8],\n        \"uncle\":  [0.7, 0.0, 0.0, 0.9],\n        \"aunt\":   [0.7, 0.0, 0.0, -0.9],\n        \"doctor\": [0.85, 0.0, 1.0, 0.2],\n        \"nurse\":  [0.85, 0.0, 1.0, -0.2],\n        \"child\":  [0.6, 0.0, 0.0, 0.0],\n    }\n    \n    vocab = list(embeddings_data.keys())\n    embeddings = {k: np.array(v) for k, v in embeddings_data.items()}\n\n    # Neutral set and analogy set\n    neutral_set = [\"doctor\", \"nurse\", \"child\"]\n    analogies = [\n        (\"king\", \"queen\", \"man\", \"woman\"),\n        (\"uncle\", \"aunt\", \"man\", \"woman\"),\n        (\"he\", \"she\", \"man\", \"woman\"),\n        (\"king\", \"man\", \"queen\", \"woman\"),\n        (\"doctor\", \"nurse\", \"man\", \"woman\"),\n        (\"man\", \"woman\", \"king\", \"queen\"),\n    ]\n\n    # Test cases definition\n    test_cases = [\n        {\n            \"pairs\": [(\"he\", \"she\"), (\"man\", \"woman\"), (\"uncle\", \"aunt\"), (\"king\", \"queen\")],\n            \"method\": \"variance-max\"\n        },\n        {\n            \"pairs\": [(\"he\", \"she\"), (\"man\", \"woman\"), (\"uncle\", \"aunt\"), (\"king\", \"queen\")],\n            \"method\": \"mean-diff\"\n        },\n        {\n            \"pairs\": [(\"he\", \"she\"), (\"doctor\", \"nurse\")],\n            \"method\": \"variance-max\"\n        }\n    ]\n\n    def cos_sim(u, v):\n        norm_u = np.linalg.norm(u)\n        norm_v = np.linalg.norm(v)\n        if norm_u == 0 or norm_v == 0:\n            return 0.0\n        return np.dot(u, v) / (norm_u * norm_v)\n\n    def get_bias_direction(pairs, method, embeddings_dict):\n        diff_vectors = [embeddings_dict[m] - embeddings_dict[f] for m, f in pairs]\n        \n        if method == \"mean-diff\":\n            mean_diff = np.mean(diff_vectors, axis=0)\n            return mean_diff / np.linalg.norm(mean_diff)\n        \n        elif method == \"variance-max\":\n            mean_diff = np.mean(diff_vectors, axis=0)\n            centered_diffs = [d - mean_diff for d in diff_vectors]\n            # Covariance matrix\n            cov_matrix = np.zeros((centered_diffs[0].shape[0], centered_diffs[0].shape[0]))\n            for c in centered_diffs:\n                cov_matrix += np.outer(c, c)\n            \n            # Eigen decomposition to find the principal component\n            eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n            # The direction is the eigenvector of the largest eigenvalue\n            bias_direction = eigenvectors[:, np.argmax(eigenvalues)]\n            return bias_direction\n\n    def calculate_bias_score(neutral_words, embeddings_dict, b):\n        score = 0\n        for word in neutral_words:\n            x_w = embeddings_dict[word]\n            score += np.abs(np.dot(x_w, b))\n        return score / len(neutral_words)\n\n    def evaluate_analogies(analogy_set, embeddings_dict, vocab_list):\n        correct_predictions = 0\n        for a, b, c, d_true in analogy_set:\n            if a not in embeddings_dict or b not in embeddings_dict or c not in embeddings_dict:\n                continue\n\n            target_vec = embeddings_dict[b] - embeddings_dict[a] + embeddings_dict[c]\n            \n            best_word = None\n            max_sim = -np.inf\n            \n            candidate_words = [w for w in vocab_list if w not in [a, b, c]]\n            \n            for word_candidate in candidate_words:\n                sim = cos_sim(embeddings_dict[word_candidate], target_vec)\n                if sim  max_sim:\n                    max_sim = sim\n                    best_word = word_candidate\n            \n            if best_word == d_true:\n                correct_predictions += 1\n        \n        return correct_predictions / len(analogy_set)\n\n    def debias_embeddings(embeddings_dict, b):\n        debiased = {}\n        for word, vec in embeddings_dict.items():\n            projection = np.dot(vec, b) * b\n            debiased[word] = vec - projection\n        return debiased\n\n    all_results = []\n\n    for case in test_cases:\n        # 1. Compute bias direction\n        b = get_bias_direction(case[\"pairs\"], case[\"method\"], embeddings)\n\n        # 2. Compute metrics before debiasing\n        b_before = calculate_bias_score(neutral_set, embeddings, b)\n        acc_before = evaluate_analogies(analogies, embeddings, vocab)\n\n        # 3. Debias embeddings\n        debiased_embeddings = debias_embeddings(embeddings, b)\n\n        # 4. Compute metrics after debiasing\n        b_after = calculate_bias_score(neutral_set, debiased_embeddings, b)\n        acc_after = evaluate_analogies(analogies, debiased_embeddings, vocab)\n\n        # 5. Consolidate results\n        b_reduction = b_before - b_after\n        delta_acc = acc_after - acc_before\n        \n        case_results = [b_before, b_after, b_reduction, acc_before, acc_after, delta_acc]\n        all_results.append(case_results)\n    \n    # Using np.round for consistent float representation in the final output\n    formatted_results = [\n        \"[\" + \",\".join([f\"{val:.6f}\" for val in res]) + \"]\" \n        for res in all_results\n    ]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}