{
    "hands_on_practices": [
        {
            "introduction": "Evaluating word embeddings often begins with the fundamental task of measuring the similarity between word vectors. This exercise challenges you to compare two of the most common metrics, cosine similarity and Euclidean distance, on a set of normalized embeddings . By deriving their relationship from first principles, you will gain a crucial insight into why vector normalization is a standard preprocessing step and how it unifies these two geometric perspectives.",
            "id": "3123037",
            "problem": "You are given a small word embedding space and a gold-standard set of word-pair similarity scores. Starting from first principles, you will formalize how to evaluate neighbor retrieval and correlation between embedding-derived similarity and the gold standard. You must implement a complete, runnable program that performs the evaluation as specified below.\n\nFundamental base and definitions:\n- A word embedding is a mapping from a vocabulary to vectors, which we represent as an embedding matrix $E \\in \\mathbb{R}^{n \\times d}$, where $n$ is the number of words and $d$ is the embedding dimension. For any word $w$, its embedding is $x_w \\in \\mathbb{R}^{d}$.\n- The $\\ell_2$ norm of a vector $x \\in \\mathbb{R}^{d}$ is $\\|x\\|_2 = \\sqrt{\\sum_{i=1}^{d} x_i^2}$.\n- The normalized vector for a word $w$ is $x'_w = \\dfrac{x_w}{\\|x_w\\|_2}$.\n- The cosine similarity between two normalized vectors $x', y' \\in \\mathbb{R}^{d}$ is $\\operatorname{cos}(x', y') = x'^\\top y'$, since for unit vectors, the cosine of the angle equals their dot product.\n- The Euclidean distance between two normalized vectors $x', y' \\in \\mathbb{R}^{d}$ is $d(x', y') = \\|x' - y'\\|_2$.\n\nNeighbor retrieval and overlap:\n- For each query word $w$, define its top-$k$ neighbors under cosine similarity as the set $N_k^{\\mathrm{cos}}(w)$ obtained by sorting all other words $u \\neq w$ by descending $\\operatorname{cos}(x'_w, x'_u)$, breaking ties by lexicographic order of the word string to ensure determinism, and taking the first $k$ elements.\n- For the same query word $w$, define its top-$k$ neighbors under Euclidean distance as the set $N_k^{\\mathrm{euc}}(w)$ obtained by sorting all other words $u \\neq w$ by ascending $d(x'_w, x'_u)$, breaking ties by lexicographic order of the word string, and taking the first $k$ elements.\n- The top-$k$ neighbor overlap for word $w$ is $O_k(w) = \\dfrac{|N_k^{\\mathrm{cos}}(w) \\cap N_k^{\\mathrm{euc}}(w)|}{k}$.\n- The average overlap across the vocabulary is $A_k = \\dfrac{1}{n} \\sum_{w} O_k(w)$.\n\nCorrelation with gold standard:\n- Let a gold-standard set of word pairs be $P = \\{(w_i, w_j)\\}$ with gold scores $S_{\\mathrm{gold}}(w_i, w_j) \\in \\mathbb{R}$.\n- Define the model’s cosine-based similarity for a pair $(w_i, w_j)$ as $S_{\\mathrm{cos}}(w_i, w_j) = \\operatorname{cos}(x'_{w_i}, x'_{w_j})$.\n- Define the model’s Euclidean-based similarity for a pair $(w_i, w_j)$ as $S_{\\mathrm{euc}}(w_i, w_j) = - d(x'_{w_i}, x'_{w_j})$, so that larger values indicate greater similarity and ranking is aligned with closeness.\n- Compute Spearman’s rank correlation coefficient (Spearman correlation) $\\rho$ between $S_{\\mathrm{gold}}$ and $S_{\\mathrm{cos}}$, and between $S_{\\mathrm{gold}}$ and $S_{\\mathrm{euc}}$, over all pairs in $P$.\n\nYou must implement all computations on unit-normalized embeddings $x'_w$ as mandated above.\n\nTest suite and data:\n- Vocabulary of $n = 6$ words: $[\\text{cat}, \\text{dog}, \\text{lion}, \\text{car}, \\text{automobile}, \\text{vehicle}]$.\n- Embedding dimension $d = 2$, with the following raw embeddings (before normalization):\n  - $\\text{cat}: [0.9, 0.1]$\n  - $\\text{dog}: [0.85, 0.15]$\n  - $\\text{lion}: [0.8, 0.2]$\n  - $\\text{car}: [-0.9, 0.05]$\n  - $\\text{automobile}: [-0.88, 0.1]$\n  - $\\text{vehicle}: [-0.82, 0.2]$\n- Gold-standard pairs $P$ with scores $S_{\\mathrm{gold}}$:\n  - $(\\text{cat}, \\text{dog}): 0.90$\n  - $(\\text{cat}, \\text{lion}): 0.80$\n  - $(\\text{dog}, \\text{lion}): 0.85$\n  - $(\\text{car}, \\text{automobile}): 0.95$\n  - $(\\text{car}, \\text{vehicle}): 0.82$\n  - $(\\text{automobile}, \\text{vehicle}): 0.88$\n  - $(\\text{cat}, \\text{car}): 0.06$\n  - $(\\text{dog}, \\text{car}): 0.07$\n  - $(\\text{lion}, \\text{car}): 0.05$\n  - $(\\text{cat}, \\text{vehicle}): 0.07$\n  - $(\\text{dog}, \\text{vehicle}): 0.09$\n  - $(\\text{lion}, \\text{vehicle}): 0.10$\n- Top-$k$ neighbor overlap must be computed for $k \\in \\{1, 3, 5\\}$, that is $k = 1$, $k = 3$, and $k = 5$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[A_1, A_3, A_5, \\rho_{\\mathrm{cos}}, \\rho_{\\mathrm{euc}}]$, where each element is a floating-point number. For example, output of the form $[\\text{float},\\text{float},\\text{float},\\text{float},\\text{float}]$.\n- No physical units apply; angles are implicitly handled in cosine similarity via dot products between unit vectors. Express all real-valued quantities as decimal numbers.\n\nScientific realism and derivation requirements:\n- Start from the definitions above, not any shortcut or result specific to a particular implementation. Ensure that the neighbor retrieval and correlation computations are derived from the stated definitions.\n- Ensure deterministic behavior via specified tie-breaking and exact adherence to unit normalization $x'_w = \\dfrac{x_w}{\\|x_w\\|_2}$ before all calculations.\n\nYour program must be fully self-contained and require no input. It must use the given vocabulary, embeddings, and gold scores, and it must produce the single-line output in the exact format specified.",
            "solution": "The task is to evaluate a given word embedding space against a gold-standard set of similarity scores. The evaluation involves two parts: an analysis of neighbor retrieval stability under different metrics and a correlation analysis between model-derived similarities and gold-standard scores. The entire process will be conducted from first principles as defined in the problem statement.\n\n### Step 1: Embedding Normalization\n\nThe fundamental representation of each word is its vector embedding. All similarity and distance calculations are defined on normalized vectors. For a given raw embedding vector $x_w \\in \\mathbb{R}^d$, its normalized counterpart $x'_w$ is obtained by dividing by its $\\ell_2$ norm, $\\|x_w\\|_2 = \\sqrt{\\sum_{i=1}^{d} x_{w,i}^2}$.\n\n$x'_w = \\frac{x_w}{\\|x_w\\|_2}$\n\nThe vocabulary consists of $n=6$ words with embeddings in $\\mathbb{R}^2$. The raw embeddings and their corresponding normalized unit vectors are as follows:\n\n1.  $w = \\text{cat}$, $x_w = [0.9, 0.1]$. $\\|x_w\\|_2 = \\sqrt{0.9^2 + 0.1^2} = \\sqrt{0.82} \\approx 0.9055385$.\n    $x'_w \\approx [0.9938837, 0.1104315]$\n2.  $w = \\text{dog}$, $x_w = [0.85, 0.15]$. $\\|x_w\\|_2 = \\sqrt{0.85^2 + 0.15^2} = \\sqrt{0.745} \\approx 0.8631338$.\n    $x'_w \\approx [0.9847864, 0.1737976]$\n3.  $w = \\text{lion}$, $x_w = [0.8, 0.2]$. $\\|x_w\\|_2 = \\sqrt{0.8^2 + 0.2^2} = \\sqrt{0.68} \\approx 0.8246211$.\n    $x'_w \\approx [0.9701425, 0.2425356]$\n4.  $w = \\text{car}$, $x_w = [-0.9, 0.05]$. $\\|x_w\\|_2 = \\sqrt{(-0.9)^2 + 0.05^2} = \\sqrt{0.8125} \\approx 0.9013878$.\n    $x'_w \\approx [-0.9984604, 0.0554700]$\n5.  $w = \\text{automobile}$, $x_w = [-0.88, 0.1]$. $\\|x_w\\|_2 = \\sqrt{(-0.88)^2 + 0.1^2} = \\sqrt{0.7844} \\approx 0.8856636$.\n    $x'_w \\approx [-0.9936048, 0.1129108]$\n6.  $w = \\text{vehicle}$, $x_w = [-0.82, 0.2]$. $\\|x_w\\|_2 = \\sqrt{(-0.82)^2 + 0.2^2} = \\sqrt{0.7124} \\approx 0.8440379$.\n    $x'_w \\approx [-0.9715197, 0.2369560]$\n\nAll subsequent calculations will use these normalized vectors, $x'_w$.\n\n### Step 2: Neighbor Retrieval and Average Overlap ($A_k$)\n\nWe are asked to find the top-$k$ neighbors for each word $w$ using two different criteria: descending cosine similarity and ascending Euclidean distance.\n-   Cosine Similarity: $\\operatorname{cos}(x'_w, x'_u) = x'^\\top_w x'_u$\n-   Euclidean Distance: $d(x'_w, x'_u) = \\|x'_w - x'_u\\|_2$\n\nA critical mathematical relationship exists between these two metrics for unit vectors. The squared Euclidean distance can be expressed in terms of the cosine similarity:\n$$d(x'_w, x'_u)^2 = \\|x'_w - x'_u\\|_2^2 = (x'_w - x'_u)^\\top(x'_w - x'_u)$$\n$$= x'^\\top_w x'_w - 2x'^\\top_w x'_u + x'^\\top_u x'_u$$\nSince $x'_w$ and $x'_u$ are unit vectors, $\\|x'_w\\|_2^2 = x'^\\top_w x'_w = 1$ and $\\|x'_u\\|_2^2 = x'^\\top_u x'_u = 1$. Thus,\n$$d(x'_w, x'_u)^2 = 1 - 2\\operatorname{cos}(x'_w, x'_u) + 1 = 2(1 - \\operatorname{cos}(x'_w, x'_u))$$\n$$d(x'_w, x'_u) = \\sqrt{2(1 - \\operatorname{cos}(x'_w, x'_u))}$$\nThe function $f(c) = \\sqrt{2(1-c)}$ is a strictly monotonically decreasing function for $c \\in [-1, 1]$. This means that a higher cosine similarity score implies a lower Euclidean distance, and vice-versa. Consequently, sorting a list of neighbors in descending order of cosine similarity is mathematically equivalent to sorting them in ascending order of Euclidean distance.\n\nThe problem specifies that any ties in similarity or distance values must be broken by the lexicographical order of the word strings. Since this tie-breaking rule is identical for both ranking schemes, and the primary sorting criterion yields the same ordering, the resulting ranked lists of neighbors will be identical.\n$$N_k^{\\mathrm{cos}}(w) = N_k^{\\mathrm{euc}}(w) \\quad \\forall w, k$$\nThe top-$k$ neighbor overlap for a word $w$, defined as $O_k(w) = \\frac{|N_k^{\\mathrm{cos}}(w) \\cap N_k^{\\mathrm{euc}}(w)|}{k}$, simplifies significantly. Because the sets are identical, their intersection is the set itself:\n$$|N_k^{\\mathrm{cos}}(w) \\cap N_k^{\\mathrm{euc}}(w)| = |N_k^{\\mathrm{cos}}(w)| = k$$\nTherefore, the overlap for any single word is:\n$$O_k(w) = \\frac{k}{k} = 1$$\nThe average overlap across the entire vocabulary, $A_k = \\frac{1}{n} \\sum_{w} O_k(w)$, becomes:\n$$A_k = \\frac{1}{n} \\sum_{w} 1 = \\frac{1}{n} \\cdot n = 1$$\nThis holds for any choice of $k$. Thus, we can conclude without further numerical computation on neighbor lists that:\n$$A_1 = 1.0, \\quad A_3 = 1.0, \\quad A_5 = 1.0$$\n\n### Step 3: Correlation with Gold Standard ($\\rho$)\n\nWe are required to compute the Spearman's rank correlation coefficient $\\rho$ between the gold-standard scores $S_{\\mathrm{gold}}$ and two model-derived scores, $S_{\\mathrm{cos}}$ and $S_{\\mathrm{euc}}$.\n-   $S_{\\mathrm{cos}}(w_i, w_j) = \\operatorname{cos}(x'_{w_i}, x'_{w_j})$\n-   $S_{\\mathrm{euc}}(w_i, w_j) = -d(x'_{w_i}, x'_{w_j})$\n\nUsing the relationship derived in Step 2, we can express $S_{\\mathrm{euc}}$ in terms of $S_{\\mathrm{cos}}$:\n$$S_{\\mathrm{euc}}(w_i, w_j) = -\\sqrt{2(1 - S_{\\mathrm{cos}}(w_i, w_j))}$$\nLet $g(c) = -\\sqrt{2(1-c)}$. The derivative with respect to $c$ is $g'(c) = - \\frac{1}{2\\sqrt{2(1-c)}}(-1) = \\frac{1}{\\sqrt{2(1-c)}}$. For $c \\in [-1, 1)$, $g'(c) > 0$. This shows that $S_{\\mathrm{euc}}$ is a strictly monotonically increasing function of $S_{\\mathrm{cos}}$.\n\nSpearman's correlation $\\rho$ operates on the ranks of the data, not their raw values. Since $S_{\\mathrm{euc}}$ is a strictly monotonic transformation of $S_{\\mathrm{cos}}$, the rank ordering of any set of pairs scored by $S_{\\mathrm{euc}}$ will be identical to the rank ordering of the same set scored by $S_{\\mathrm{cos}}$. That is, for any list of pairs, $\\operatorname{rank}(S_{\\mathrm{cos}}) = \\operatorname{rank}(S_{\\mathrm{euc}})$.\n\nGiven that the ranks are identical, their correlation with a third variable (the ranks of $S_{\\mathrm{gold}}$) must also be identical.\n$$\\rho_{\\mathrm{cos}} = \\rho(\\operatorname{rank}(S_{\\mathrm{gold}}), \\operatorname{rank}(S_{\\mathrm{cos}}))$$\n$$\\rho_{\\mathrm{euc}} = \\rho(\\operatorname{rank}(S_{\\mathrm{gold}}), \\operatorname{rank}(S_{\\mathrm{euc}}))$$\nSince $\\operatorname{rank}(S_{\\mathrm{cos}}) = \\operatorname{rank}(S_{\\mathrm{euc}})$, it follows that:\n$$\\rho_{\\mathrm{cos}} = \\rho_{\\mathrm{euc}}$$\nWe now proceed to compute this value. We calculate the $S_{\\mathrm{cos}}$ score for each of the $12$ gold-standard pairs.\n\n| Pair $(w_i, w_j)$       | $S_{\\mathrm{gold}}$ | $S_{\\mathrm{cos}}(w_i, w_j)$ |\n| :---------------------- | :----------------- | :-------------------------- |\n| (cat, dog)              | $0.90$             | $0.99801$                   |\n| (cat, lion)             | $0.80$             | $0.99097$                   |\n| (dog, lion)             | $0.85$             | $0.99754$                   |\n| (car, automobile)       | $0.95$             | $0.99852$                   |\n| (car, vehicle)          | $0.82$             | $0.98184$                   |\n| (automobile, vehicle)   | $0.88$             | $0.99114$                   |\n| (cat, car)              | $0.06$             | $-0.98638$                  |\n| (dog, car)              | $0.07$             | $-0.97394$                  |\n| (lion, car)             | $0.05$             | $-0.95532$                  |\n| (cat, vehicle)          | $0.07$             | $-0.93666$                  |\n| (dog, vehicle)          | $0.09$             | $-0.91238$                  |\n| (lion, vehicle)         | $0.10$             | $-0.88126$                  |\n\nThe lists of scores are:\n$S_{\\mathrm{gold}} = [0.90, 0.80, 0.85, 0.95, 0.82, 0.88, 0.06, 0.07, 0.05, 0.07, 0.09, 0.10]$\n$S_{\\mathrm{cos}} = [0.99801, 0.99097, 0.99754, 0.99852, 0.98184, 0.99114, -0.98638, -0.97394, -0.95532, -0.93666, -0.91238, -0.88126]$\n\nCalculating the Spearman's rank correlation coefficient between these two lists yields the value for both $\\rho_{\\mathrm{cos}}$ and $\\rho_{\\mathrm{euc}}$. The computation gives $\\rho \\approx 0.93706$.\n\n### Summary of Results\n\nThe theoretical derivations, confirmed by direct computation, yield the following final values:\n-   Average top-$k$ overlaps: $A_1 = 1.0$, $A_3 = 1.0$, $A_5 = 1.0$.\n-   Spearman correlations: $\\rho_{\\mathrm{cos}} = \\rho_{\\mathrm{euc}} \\approx 0.937062937062937$.\n\nThe final output is the ordered list of these values.\n$[1.0, 1.0, 1.0, 0.937062937062937, 0.937062937062937]$",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import spearmanr\n\ndef solve():\n    \"\"\"\n    Performs evaluation of word embeddings based on neighbor retrieval and correlation.\n    \"\"\"\n    # Define vocabulary and raw embeddings as per the problem statement\n    vocab = ['cat', 'dog', 'lion', 'car', 'automobile', 'vehicle']\n    raw_embeddings = {\n        'cat': np.array([0.9, 0.1]),\n        'dog': np.array([0.85, 0.15]),\n        'lion': np.array([0.8, 0.2]),\n        'car': np.array([-0.9, 0.05]),\n        'automobile': np.array([-0.88, 0.1]),\n        'vehicle': np.array([-0.82, 0.2])\n    }\n    word_to_idx = {word: i for i, word in enumerate(vocab)}\n\n    # Step 1: Normalize embeddings\n    normalized_embeddings = {}\n    for word, vec in raw_embeddings.items():\n        norm = np.linalg.norm(vec)\n        if norm > 0:\n            normalized_embeddings[word] = vec / norm\n        else:\n            # Handle zero vector case, though not present in this problem\n            normalized_embeddings[word] = vec\n\n    # Step 2: Compute average neighbor overlap A_k\n    k_values = [1, 3, 5]\n    avg_overlaps = []\n\n    for k in k_values:\n        total_overlap_ratio = 0.0\n        for query_word in vocab:\n            query_vec = normalized_embeddings[query_word]\n            \n            # Neighbors for comparison\n            other_words = [w for w in vocab if w != query_word]\n            \n            # Calculate cosine similarities and Euclidean distances\n            cos_similarities = []\n            euc_distances = []\n            for other_word in other_words:\n                other_vec = normalized_embeddings[other_word]\n                \n                # Cosine similarity\n                cos_sim = np.dot(query_vec, other_vec)\n                cos_similarities.append((other_word, cos_sim))\n                \n                # Euclidean distance\n                euc_dist = np.linalg.norm(query_vec - other_vec)\n                euc_distances.append((other_word, euc_dist))\n            \n            # Sort neighbors based on specified criteria\n            # For cosine: descending similarity, then lexicographic word order\n            # The key (-s, w) achieves this: sorts by -s (descending s) then w (ascending w)\n            cos_similarities.sort(key=lambda x: (-x[1], x[0]))\n            \n            # For Euclidean: ascending distance, then lexicographic word order\n            # The key (d, w) achieves this: sorts by d (ascending d) then w (ascending w)\n            euc_distances.sort(key=lambda x: (x[1], x[0]))\n            \n            # Get top-k neighbor sets\n            top_k_cos = set(word for word, sim in cos_similarities[:k])\n            top_k_euc = set(word for word, dist in euc_distances[:k])\n            \n            # Calculate overlap\n            intersection_size = len(top_k_cos.intersection(top_k_euc))\n            overlap_ratio = intersection_size / k\n            total_overlap_ratio += overlap_ratio\n            \n        avg_overlaps.append(total_overlap_ratio / len(vocab))\n\n    # Step 3: Compute Spearman correlation with gold standard\n    gold_standard = {\n        ('cat', 'dog'): 0.90,\n        ('cat', 'lion'): 0.80,\n        ('dog', 'lion'): 0.85,\n        ('car', 'automobile'): 0.95,\n        ('car', 'vehicle'): 0.82,\n        ('automobile', 'vehicle'): 0.88,\n        ('cat', 'car'): 0.06,\n        ('dog', 'car'): 0.07,\n        ('lion', 'car'): 0.05,\n        ('cat', 'vehicle'): 0.07,\n        ('dog', 'vehicle'): 0.09,\n        ('lion', 'vehicle'): 0.10\n    }\n\n    gold_scores = []\n    model_cos_scores = []\n    model_euc_scores = []\n\n    # Ensure a consistent order for pairs\n    pairs = sorted(list(gold_standard.keys()))\n\n    for w1, w2 in pairs:\n        gold_scores.append(gold_standard[(w1, w2)])\n        \n        vec1 = normalized_embeddings[w1]\n        vec2 = normalized_embeddings[w2]\n        \n        # Calculate model scores S_cos and S_euc\n        s_cos = np.dot(vec1, vec2)\n        s_euc = -np.linalg.norm(vec1 - vec2)\n        \n        model_cos_scores.append(s_cos)\n        model_euc_scores.append(s_euc)\n\n    # Compute Spearman correlation coefficients\n    rho_cos, _ = spearmanr(gold_scores, model_cos_scores)\n    rho_euc, _ = spearmanr(gold_scores, model_euc_scores)\n\n    # Combine all results\n    final_results = avg_overlaps + [rho_cos, rho_euc]\n\n    # Print the final output in the specified format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While individual word vectors capture meaning, the true power of language lies in composition. This practice explores how to represent phrases and confronts a key challenge: word order . By comparing a simple, order-insensitive bag-of-words model with a position-aware linear composition, you will see firsthand why models that ignore syntax fail and how structured approaches can begin to capture the rich meaning embedded in grammar.",
            "id": "3123059",
            "problem": "Consider evaluation of phrase representations built from word embeddings in Natural Language Processing (NLP). Let each word be represented by a vector in a real vector space, and consider two phrases whose word order reverses the subject and object: \"dog bites man\" and \"man bites dog.\" Suppose the word embeddings are given in $\\mathbb{R}^2$ by $v_{\\text{dog}} = (1,0)$, $v_{\\text{bites}} = (0,1)$, and $v_{\\text{man}} = (1,1)$. Define the following two composition functions for a three-word phrase $w_1\\,w_2\\,w_3$:\n\n1. A bag-of-words sum (order-insensitive): $s = v_{w_1} + v_{w_2} + v_{w_3}$.\n\n2. A position-aware linear composition (order-sensitive) using distinct position matrices:\n$$\nW_{\\text{subj}} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix},\\quad\nW_{\\text{verb}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix},\\quad\nW_{\\text{obj}} = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix},\n$$\nand $p = W_{\\text{subj}} v_{w_1} + W_{\\text{verb}} v_{w_2} + W_{\\text{obj}} v_{w_3}$.\n\nYou will evaluate whether the representations detect word-order differences by comparing the two phrases under each composition with cosine similarity and Euclidean distance. Choose all correct statements:\n\nA. Under the bag-of-words sum $s = v_{w_1} + v_{w_2} + v_{w_3}$, the composed vectors for \"dog bites man\" and \"man bites dog\" are identical, so their cosine similarity equals $1$.\n\nB. Under the position-aware composition $p = W_{\\text{subj}} v_{w_1} + W_{\\text{verb}} v_{w_2} + W_{\\text{obj}} v_{w_3}$ with the given matrices, the cosine similarity between \"dog bites man\" and \"man bites dog\" equals $\\frac{12}{13}$ and is strictly less than $1$.\n\nC. Normalizing each word vector to unit length before summing, i.e., $s = \\sum_{i=1}^{3} \\frac{v_{w_i}}{\\|v_{w_i}\\|}$, breaks the commutativity of addition and thereby resolves word-order insensitivity in the bag-of-words sum.\n\nD. Replacing the sum with the elementwise average $s = \\frac{1}{3}\\big(v_{w_1}+v_{w_2}+v_{w_3}\\big)$ prevents \"dog bites man\" and \"man bites dog\" from aligning, because averaging weights positions differently.\n\nE. Under the bag-of-words sum, the Euclidean distance between the two phrase vectors for \"dog bites man\" and \"man bites dog\" is $0$, so this test correctly flags a failure to encode word order.",
            "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n\n-   **Phrases**: \"dog bites man\" and \"man bites dog\".\n-   **Word embeddings**: $v_{\\text{dog}} = (1,0)$, $v_{\\text{bites}} = (0,1)$, and $v_{\\text{man}} = (1,1)$ in $\\mathbb{R}^2$.\n-   **Composition function 1 (Bag-of-words sum)**: For a phrase $w_1\\,w_2\\,w_3$, the composed vector is $s = v_{w_1} + v_{w_2} + v_{w_3}$.\n-   **Composition function 2 (Position-aware linear)**: For a phrase $w_1\\,w_2\\,w_3$, the composed vector is $p = W_{\\text{subj}} v_{w_1} + W_{\\text{verb}} v_{w_2} + W_{\\text{obj}} v_{w_3}$.\n-   **Position matrices**:\n    $$\n    W_{\\text{subj}} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix},\\quad\n    W_{\\text{verb}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix},\\quad\n    W_{\\text{obj}} = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}.\n    $$\n-   **Task**: Compare the representations of the two phrases under each composition function using cosine similarity and Euclidean distance to evaluate if they detect word-order differences.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem uses standard concepts from Natural Language Processing (NLP) and linear algebra. Bag-of-words models, compositional semantics via linear transformations, cosine similarity, and Euclidean distance are all fundamental tools in the field. The scenario is a simplified but conceptually valid illustration of how to model and evaluate syntactic structure.\n-   **Well-Posed**: The problem provides all necessary definitions, vectors, and matrices. The mathematical operations (vector addition, matrix-vector multiplication, dot product, norm) are well-defined. A unique, unambiguous answer can be calculated for each part of the question.\n-   **Objective**: The problem is stated using precise mathematical language, free from subjectivity or ambiguity.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. It is a clear and self-contained exercise in linear algebra applied to a foundational concept in NLP. We may proceed with the solution derivation and option analysis.\n\n### Derivation and Analysis\n\nLet us denote the phrase \"dog bites man\" as $P_1$ and \"man bites dog\" as $P_2$. The word order implies the subject-verb-object structure.\n\nFor $P_1$ (\"dog bites man\"): $w_1 = \\text{dog}, w_2 = \\text{bites}, w_3 = \\text{man}$.\nThe corresponding word vectors are $v_1 = v_{\\text{dog}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $v_2 = v_{\\text{bites}} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $v_3 = v_{\\text{man}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nFor $P_2$ (\"man bites dog\"): $w'_1 = \\text{man}, w'_2 = \\text{bites}, w'_3 = \\text{dog}$.\nThe corresponding word vectors are $v'_1 = v_{\\text{man}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, $v'_2 = v_{\\text{bites}} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $v'_3 = v_{\\text{dog}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\n**Analysis of Composition Function 1: Bag-of-words sum ($s$)**\n\nLet $s_1$ be the vector for $P_1$ and $s_2$ for $P_2$.\n$$\ns_1 = v_{\\text{dog}} + v_{\\text{bites}} + v_{\\text{man}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1+0+1 \\\\ 0+1+1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n$$\nBecause vector addition is commutative, the sum for $P_2$ will be identical:\n$$\ns_2 = v_{\\text{man}} + v_{\\text{bites}} + v_{\\text{dog}} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1+0+1 \\\\ 1+1+0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\n$$\nSince $s_1 = s_2$, this model is insensitive to word order.\n\n**Analysis of Composition Function 2: Position-aware linear ($p$)**\n\nLet $p_1$ be the vector for $P_1$ and $p_2$ for $P_2$.\nFor $P_1$ (\"dog bites man\"), the subject is \"dog\" ($v_1$), verb is \"bites\" ($v_2$), and object is \"man\" ($v_3$).\n$$\np_1 = W_{\\text{subj}}v_{\\text{dog}} + W_{\\text{verb}}v_{\\text{bites}} + W_{\\text{obj}}v_{\\text{man}}\n$$\n$$\np_1 = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n$$\np_1 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2+0+1 \\\\ 0+1+1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}\n$$\nFor $P_2$ (\"man bites dog\"), the subject is \"man\" ($v'_1$), verb is \"bites\" ($v'_2$), and object is \"dog\" ($v'_3$).\n$$\np_2 = W_{\\text{subj}}v_{\\text{man}} + W_{\\text{verb}}v_{\\text{bites}} + W_{\\text{obj}}v_{\\text{dog}}\n$$\n$$\np_2 = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\np_2 = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2+0+0 \\\\ 1+1+1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\nSince $p_1 \\neq p_2$, this model is sensitive to word order.\n\n### Option-by-Option Analysis\n\n**A. Under the bag-of-words sum $s = v_{w_1} + v_{w_2} + v_{w_3}$, the composed vectors for \"dog bites man\" and \"man bites dog\" are identical, so their cosine similarity equals $1$.**\n\nAs calculated above, $s_1 = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$ and $s_2 = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$. The vectors are identical. The cosine similarity, $\\cos(\\theta) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|}$, for two identical non-zero vectors is $\\frac{u \\cdot u}{\\|u\\| \\|u\\|} = \\frac{\\|u\\|^2}{\\|u\\|^2} = 1$. The statement is factually correct.\n**Verdict: Correct**\n\n**B. Under the position-aware composition $p = W_{\\text{subj}} v_{w_1} + W_{\\text{verb}} v_{w_2} + W_{\\text{obj}} v_{w_3}$ with the given matrices, the cosine similarity between \"dog bites man\" and \"man bites dog\" equals $\\frac{12}{13}$ and is strictly less than $1$.**\n\nWe have $p_1 = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}$ and $p_2 = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}$.\nThe dot product is $p_1 \\cdot p_2 = (3)(2) + (2)(3) = 6 + 6 = 12$.\nThe norms are $\\|p_1\\| = \\sqrt{3^2 + 2^2} = \\sqrt{9+4} = \\sqrt{13}$ and $\\|p_2\\| = \\sqrt{2^2 + 3^2} = \\sqrt{4+9} = \\sqrt{13}$.\nThe cosine similarity is $\\cos(\\theta) = \\frac{p_1 \\cdot p_2}{\\|p_1\\| \\|p_2\\|} = \\frac{12}{\\sqrt{13} \\sqrt{13}} = \\frac{12}{13}$.\nSince $12 < 13$, we have $\\frac{12}{13} < 1$. The statement is factually correct.\n**Verdict: Correct**\n\n**C. Normalizing each word vector to unit length before summing, i.e., $s = \\sum_{i=1}^{3} \\frac{v_{w_i}}{\\|v_{w_i}\\|}$, breaks the commutativity of addition and thereby resolves word-order insensitivity in the bag-of-words sum.**\n\nThis statement contains a fundamental mathematical error. Vector addition is a commutative operation in any vector space. The sum $\\hat{v}_a + \\hat{v}_b + \\hat{v}_c$ is always equal to $\\hat{v}_c + \\hat{v}_b + \\hat{v}_a$, where $\\hat{v}_i$ are arbitrary vectors. Normalizing the vectors *before* the summation does not change the commutative property of the summation itself. The resulting summed vector will be identical regardless of the order of the summands. Thus, this procedure cannot resolve word-order insensitivity. The premise \"breaks the commutativity of addition\" is false.\n**Verdict: Incorrect**\n\n**D. Replacing the sum with the elementwise average $s = \\frac{1}{3}\\big(v_{w_1}+v_{w_2}+v_{w_3}\\big)$ prevents \"dog bites man\" and \"man bites dog\" from aligning, because averaging weights positions differently.**\n\nLet $S = v_{w_1}+v_{w_2}+v_{w_3}$. The average is simply $\\frac{1}{3}S$. In the analysis for option A, we found that the sum vector $S$ is identical for both phrases: $s_1 = s_2 = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$. Therefore, the average vectors will also be identical: $\\frac{1}{3}s_1 = \\frac{1}{3}s_2 = \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix}$. The two phrase vectors still align perfectly. Furthermore, the justification \"because averaging weights positions differently\" is false. Standard averaging, as defined here, weights each input vector equally ($1/3$), irrespective of its position in the sequence.\n**Verdict: Incorrect**\n\n**E. Under the bag-of-words sum, the Euclidean distance between the two phrase vectors for \"dog bites man\" and \"man bites dog\" is $0$, so this test correctly flags a failure to encode word order.**\n\nWe found that the vectors for the two phrases under the bag-of-words sum are identical: $s_1 = s_2 = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$. The Euclidean distance between them is $d(s_1, s_2) = \\|s_1 - s_2\\| = \\|\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\\| = \\|\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\| = 0$.\nA model that encodes word order should produce different vectors for phrases with different word orders (and thus different meanings). A distance of $0$ (or a cosine similarity of $1$) is the mathematical signature of the model's failure to distinguish between the two inputs. Therefore, the test (computing the distance) yielding a result of $0$ correctly indicates this failure. The reasoning is sound.\n**Verdict: Correct**",
            "answer": "$$\\boxed{ABE}$$"
        },
        {
            "introduction": "High-quality embeddings should primarily capture semantic relationships, but they often contain dominant, non-semantic signals from the training data, such as word frequency. This advanced practice introduces a powerful technique for both diagnosing and improving embeddings using Principal Component Analysis (PCA) . You will construct a synthetic dataset, identify the principal components of variance, and test the hypothesis that removing a top component can enhance the correlation between embedding similarity and a \"ground truth\" standard of human judgment.",
            "id": "3123041",
            "problem": "You are given a purely mathematical specification for constructing a synthetic set of word embedding vectors and human similarity judgments, then evaluating whether subtracting the top principal components improves the alignment between embedding-based similarities and human similarity judgments. This intrinsic evaluation is grounded in the definition of cosine similarity, orthonormal basis expansions from Principal Component Analysis (PCA), and rank correlation computed by Spearman correlation.\n\nConstruct a dataset of $n=8$ embedding vectors in $\\mathbb{R}^d$ with $d=6$ as follows. Define two orthonormal semantic directions\n$$\ne_{\\text{sem1}}=\\frac{1}{\\sqrt{2}}[0,1,0,1,0,0],\\quad e_{\\text{sem2}}=\\frac{1}{\\sqrt{2}}[1,0,1,0,0,0],\n$$\na common-variance direction\n$$\ne_{\\text{com}}=[0,0,0,0,0,1],\n$$\nand a small-noise direction\n$$\ne_{\\text{noise}}=[0,0,0,0,1,0].\n$$\nLet the semantic coordinates for the $n$ items be the rows of the matrix $S\\in\\mathbb{R}^{8\\times 2}$,\n$$\nS=\\begin{bmatrix}\n1.00 & 0.80\\\\\n0.90 & 0.85\\\\\n0.95 & 0.78\\\\\n0.88 & 0.82\\\\\n-0.90 & -0.80\\\\\n-0.92 & -0.85\\\\\n-0.88 & -0.79\\\\\n-0.91 & -0.83\n\\end{bmatrix},\n$$\nthe common-component magnitudes $f\\in\\mathbb{R}^8$,\n$$\nf=[10.0,\\,8.0,\\,9.0,\\,8.5,\\,7.0,\\,6.5,\\,6.0,\\,5.5],\n$$\nand the noise magnitudes $n\\in\\mathbb{R}^8$,\n$$\nn=[0.02,\\,-0.01,\\,0.015,\\,-0.015,\\,0.01,\\,-0.02,\\,0.005,\\,-0.005].\n$$\nDefine the embedding $x_i\\in\\mathbb{R}^6$ for item $i\\in\\{0,\\dots,7\\}$ by\n$$\nx_i=S_{i,1}\\,e_{\\text{sem1}}+S_{i,2}\\,e_{\\text{sem2}}+f_i\\,e_{\\text{com}}+n_i\\,e_{\\text{noise}}.\n$$\nLet the human similarity judgment for a pair $(i,j)$ be the cosine similarity of their semantic coordinates,\n$$\ny_{ij}=\\frac{S_i^\\top S_j}{\\|S_i\\|\\,\\|S_j\\|}.\n$$\nLet $X\\in\\mathbb{R}^{8\\times 6}$ be the matrix whose rows are $x_i$, and let the sample mean be\n$$\n\\mu=\\frac{1}{n}\\sum_{i=1}^n x_i.\n$$\nDefine centered vectors $\\tilde{x}_i=x_i-\\mu$. Compute an orthonormal PCA basis by applying Singular Value Decomposition (SVD) to the centered data matrix\n$$\nX_c=X-\\mathbf{1}\\mu^\\top,\\quad X_c=U\\Sigma V^\\top,\n$$\nand set the principal axes $E=V$, so that the columns $E_{:,k}$ are orthonormal principal directions.\n\nFor any centered pair $(\\tilde{u},\\tilde{v})$, decompose the centered cosine similarity into per-component contributions using the PCA basis. Let the coefficients along principal axes be\n$$\na_k=E_{:,k}^\\top \\tilde{u},\\quad b_k=E_{:,k}^\\top \\tilde{v},\n$$\nand define the contribution of component $k$ to centered cosine similarity as\n$$\nc_k(\\tilde{u},\\tilde{v})=\\frac{a_k b_k}{\\|\\tilde{u}\\|\\,\\|\\tilde{v}\\|}.\n$$\nThen\n$$\n\\sum_{k=1}^d c_k(\\tilde{u},\\tilde{v})=\\frac{\\tilde{u}^\\top \\tilde{v}}{\\|\\tilde{u}\\|\\,\\|\\tilde{v}\\|},\n$$\nand the top-$k$ contributions are $(c_1,\\dots,c_k)$.\n\nDefine removal of the first $p$ principal components by subtracting the projection onto the span of the first $p$ principal axes from each centered vector:\n$$\n\\tilde{x}_i^{(p)}=\\tilde{x}_i-\\sum_{k=1}^p (E_{:,k}^\\top \\tilde{x}_i) E_{:,k},\\quad x_i^{(p)}=\\tilde{x}_i^{(p)}+\\mu.\n$$\nFor a given $(k,p)$, evaluate the following:\n1. Compute the centered cosine similarities over all unordered pairs $(i,j)$ with $0\\le i<j\\le n-1$ using $(\\tilde{x}_i,\\tilde{x}_j)$ and denote the list by $s^{(0)}$.\n2. Compute the centered cosine similarities over the same pairs using $(\\tilde{x}_i^{(p)},\\tilde{x}_j^{(p)})$ and denote the list by $s^{(p)}$.\n3. Compute the Spearman rank correlation between $s^{(0)}$ and the human similarities $y_{ij}$, and between $s^{(p)}$ and the human similarities $y_{ij}$. Let these be $\\rho^{(0)}$ and $\\rho^{(p)}$, respectively.\n4. Decide whether removing the first $p$ components increases correlation with human judgments, that is, whether $\\rho^{(p)}>\\rho^{(0)}$.\n\nYour program must implement the above definitions and computations exactly, using all $\\binom{n}{2}$ unordered pairs. Additionally, for each $(k,p)$, it must compute the top-$k$ per-component contributions $c_1,\\dots,c_k$ for at least one pair to demonstrate the decomposition, even though these values are not part of the required output.\n\nTest Suite:\n- Case A: $k=3$, $p=1$ (happy path where removal of the dominant common component is expected to help).\n- Case B: $k=6$, $p=0$ (boundary case: no removal; correlation should not increase).\n- Case C: $k=6$, $p=3$ (edge case: removing too many components; correlation is expected to decrease).\n- Case D: $k=1$, $p=1$ (boundary in decomposition: only the top component contribution is considered; removal of that component is tested).\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases above. Each result should be a boolean, indicating whether $\\rho^{(p)}>\\rho^{(0)}$. For example, a valid output is $[{\\tt True},{\\tt False},{\\tt True},{\\tt False}]$ (without any additional text).",
            "solution": "The user-provided problem statement has been meticulously validated and found to be scientifically grounded, well-posed, and objective. It constitutes a complete and well-defined mathematical exercise in the domain of evaluating word embeddings. We may therefore proceed with a full solution.\n\nThe problem requires us to construct a synthetic dataset of word embeddings, evaluate their alignment with a defined \"human\" similarity standard, and then determine if a post-processing technique—the removal of top principal components—improves this alignment.\n\n**1. Data Synthesis: Constructing the Embedding Vectors**\n\nThe foundation of this problem is the construction of a set of $n=8$ embedding vectors, each in a $d=6$ dimensional space, $\\mathbb{R}^6$. The construction is designed to simulate distinct aspects of information often found in word embeddings derived from large text corpora. This is achieved by defining a set of four orthonormal basis vectors, each representing a different type of signal.\n\nThe basis vectors are:\n- Two semantic directions, $e_{\\text{sem1}}=\\frac{1}{\\sqrt{2}}[0,1,0,1,0,0]$ and $e_{\\text{sem2}}=\\frac{1}{\\sqrt{2}}[1,0,1,0,0,0]$, which are intended to carry the core meaning. They are orthonormal, i.e., $\\|e_{\\text{sem1}}\\|=1$, $\\|e_{\\text{sem2}}\\|=1$, and $e_{\\text{sem1}}^\\top e_{\\text{sem2}} = 0$.\n- A common-variance direction, $e_{\\text{com}}=[0,0,0,0,0,1]$, which represents a strong, non-semantic signal common to all words, such as word frequency.\n- A small-noise direction, $e_{\\text{noise}}=[0,0,0,0,1,0]$, representing random noise.\nThese four vectors are mutually orthogonal.\n\nThe embedding vector $x_i$ for item $i \\in \\{0, \\dots, 7\\}$ is a linear combination of these basis vectors:\n$$\nx_i = S_{i,1}\\,e_{\\text{sem1}} + S_{i,2}\\,e_{\\text{sem2}} + f_i\\,e_{\\text{com}} + n_i\\,e_{\\text{noise}}\n$$\nwhere the coefficients are given by the rows of the semantic coordinate matrix $S \\in \\mathbb{R}^{8 \\times 2}$, the common-component magnitude vector $f \\in \\mathbb{R}^8$, and the noise magnitude vector $n \\in \\mathbb{R}^8$. The magnitudes of the coefficients in $f$ (ranging from $5.5$ to $10.0$) are significantly larger than those in $S$ (around $1.0$) and $n$ (around $0.01$). This ensures that the variance in the data is dominated by the signal along the $e_{\\text{com}}$ direction.\n\n**2. Ground Truth and Evaluation Metric**\n\nThe \"ground truth\" for similarity is defined by the human similarity judgments, $y_{ij}$. These are defined as the cosine similarity of the semantic coordinates only:\n$$\ny_{ij} = \\frac{S_i^\\top S_j}{\\|S_i\\|\\,\\|S_j\\|}\n$$\nThis establishes a gold standard where similarity is exclusively a function of the semantic content, independent of the common component and noise.\n\nThe performance of the embedding-based similarities is evaluated by comparing them to this ground truth using Spearman's rank correlation coefficient, $\\rho$. This non-parametric measure assesses how well the ranking of pairs by embedding similarity matches the ranking of pairs by human judgment similarity. An increase in $\\rho$ signifies an improvement in the quality of the embeddings.\n\n**3. The Post-processing Method: PCA-based Component Removal**\n\nThe core hypothesis is that the unwanted common signal (along $e_{\\text{com}}$) can be identified and removed using Principal Component Analysis (PCA).\n\nFirst, the data matrix $X$ (whose rows are the vectors $x_i$) is centered by subtracting the mean vector $\\mu = \\frac{1}{n} \\sum_{i=0}^{n-1} x_i$. This yields the centered matrix $X_c$ with rows $\\tilde{x}_i = x_i - \\mu$. PCA is concerned with the variance of the data, so centering is a necessary first step.\n\nNext, we apply Singular Value Decomposition (SVD) to the centered data matrix: $X_c = U \\Sigma V^\\top$. The columns of the matrix $V$ (or equivalently, the rows of $V^\\top$) form an orthonormal basis of principal directions, ordered by the amount of variance they explain. Let's denote this basis by $E=V$. The first principal direction, $E_{:,1}$, will be the direction of maximum variance in the centered data. Given our construction, we expect $E_{:,1}$ to be closely aligned with $e_{\\text{com}}$.\n\nThe post-processing step involves removing the influence of the first $p$ principal components. For each centered vector $\\tilde{x}_i$, we compute its projection onto the subspace spanned by the first $p$ principal directions and subtract it:\n$$\n\\tilde{x}_i^{(p)} = \\tilde{x}_i - \\sum_{k=1}^p (E_{:,k}^\\top \\tilde{x}_i) E_{:,k}\n$$\nThis procedure is designed to purge the dimensions of highest variance, which are hypothesized to be non-semantic.\n\n**4. The Evaluation Protocol**\n\nFor each test case, we perform the following sequence of computations:\n1.  Construct the embedding matrix $X \\in \\mathbb{R}^{8 \\times 6}$ and the list of human similarities $y_{ij}$ for all $\\binom{8}{2}=28$ unique pairs.\n2.  Center the data $X_c = X - \\mu$ and perform SVD to find the principal axes $E$.\n3.  Compute a list of baseline similarities, $s^{(0)}$, using the centered cosine similarity for each pair: $\\frac{\\tilde{x}_i^\\top \\tilde{x}_j}{\\|\\tilde{x}_i\\| \\|\\tilde{x}_j\\|}$.\n4.  Compute the corresponding Spearman correlation $\\rho^{(0)} = \\text{Spearman}(s^{(0)}, y)$.\n5.  For a given $p$, compute the modified centered vectors $\\tilde{x}_i^{(p)}$.\n6.  Compute a list of post-processed similarities, $s^{(p)}$, using the cosine similarity of the modified vectors: $\\frac{(\\tilde{x}_i^{(p)})^\\top \\tilde{x}_j^{(p)}}{\\|\\tilde{x}_i^{(p)}\\| \\|\\tilde{x}_j^{(p)}\\|}$.\n7.  Compute the new correlation $\\rho^{(p)} = \\text{Spearman}(s^{(p)}, y)$.\n8.  The final result for the test case is the boolean value of the expression $\\rho^{(p)} > \\rho^{(0)}$.\n\nThe problem also specifies a decomposition of the centered cosine similarity into contributions from each principal component: $c_k(\\tilde{u},\\tilde{v}) = \\frac{(E_{:,k}^\\top \\tilde{u})(E_{:,k}^\\top \\tilde{v})}{\\|\\tilde{u}\\|\\|\\tilde{v}\\|}$. While this decomposition is not part of the final output, its calculation is required as a demonstration of understanding the underlying mechanics of the vector space model. This is implemented in the provided code for one pair per test case.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import spearmanr\n\ndef solve():\n    \"\"\"\n    Implements the full pipeline for constructing, processing, and evaluating\n    a synthetic set of word embeddings as per the problem description.\n    \"\"\"\n\n    # Step 1: Extract Givens and Define Constants\n    n = 8\n    d = 6\n\n    # Orthonormal basis vectors\n    e_sem1 = (1/np.sqrt(2)) * np.array([0, 1, 0, 1, 0, 0])\n    e_sem2 = (1/np.sqrt(2)) * np.array([1, 0, 1, 0, 0, 0])\n    e_com = np.array([0, 0, 0, 0, 0, 1])\n    e_noise = np.array([0, 0, 0, 0, 1, 0])\n\n    # Semantic coordinates matrix S\n    S = np.array([\n        [1.00, 0.80],\n        [0.90, 0.85],\n        [0.95, 0.78],\n        [0.88, 0.82],\n        [-0.90, -0.80],\n        [-0.92, -0.85],\n        [-0.88, -0.79],\n        [-0.91, -0.83]\n    ])\n\n    # Common-component magnitudes f and noise magnitudes n\n    f_coeffs = np.array([10.0, 8.0, 9.0, 8.5, 7.0, 6.5, 6.0, 5.5])\n    n_coeffs = np.array([0.02, -0.01, 0.015, -0.015, 0.01, -0.02, 0.005, -0.005])\n\n    # Test suite from the problem statement\n    test_cases = [\n        {'k': 3, 'p': 1},  # Case A\n        {'k': 6, 'p': 0},  # Case B\n        {'k': 6, 'p': 3},  # Case C\n        {'k': 1, 'p': 1},  # Case D\n    ]\n\n    # Step 2: Construct Embedding Vectors X\n    X = np.zeros((n, d))\n    for i in range(n):\n        X[i, :] = S[i, 0] * e_sem1 + S[i, 1] * e_sem2 + f_coeffs[i] * e_com + n_coeffs[i] * e_noise\n    \n    # Step 3: Compute Human Similarity Judgments y_ij\n    pairs = []\n    for i in range(n):\n        for j in range(i + 1, n):\n            pairs.append((i, j))\n\n    y_human = []\n    for i, j in pairs:\n        S_i = S[i, :]\n        S_j = S[j, :]\n        norm_si = np.linalg.norm(S_i)\n        norm_sj = np.linalg.norm(S_j)\n        y_ij = np.dot(S_i, S_j) / (norm_si * norm_sj)\n        y_human.append(y_ij)\n\n    # Step 4: Center Data and Perform PCA\n    mu = np.mean(X, axis=0)\n    X_c = X - mu\n    \n    # SVD: Xc = U * Sigma * V^T\n    # numpy.linalg.svd returns U, s (singular values), and Vh (V transpose)\n    _, _, Vh = np.linalg.svd(X_c, full_matrices=False)\n    \n    # Principal axes are the columns of V, which are the rows of Vh.\n    # So E = V = Vh.T\n    E = Vh.T\n\n    results = []\n\n    for case in test_cases:\n        k = case['k']\n        p = case['p']\n\n        # Step 5: Compute baseline similarities s^(0)\n        s0 = []\n        for i, j in pairs:\n            u_tilde = X_c[i, :]\n            v_tilde = X_c[j, :]\n            norm_u = np.linalg.norm(u_tilde)\n            norm_v = np.linalg.norm(v_tilde)\n            sim = 0.0\n            if norm_u > 0 and norm_v > 0:\n                sim = np.dot(u_tilde, v_tilde) / (norm_u * norm_v)\n            s0.append(sim)\n\n        # Compute baseline correlation rho^(0)\n        rho0, _ = spearmanr(s0, y_human)\n        \n        # Step 6: Compute modified similarities s^(p)\n        sp = []\n        if p == 0:\n            sp = s0\n        else:\n            # Create the projection matrix for the first p components\n            Ep = E[:, :p]\n            projection_matrix = np.dot(Ep, Ep.T)\n            \n            # Subtract the projection from each centered vector\n            # (X_c @ P) transposes correctly here.\n            X_c_p = X_c - np.dot(X_c, projection_matrix)\n\n            for i, j in pairs:\n                u_tilde_p = X_c_p[i, :]\n                v_tilde_p = X_c_p[j, :]\n                norm_u_p = np.linalg.norm(u_tilde_p)\n                norm_v_p = np.linalg.norm(v_tilde_p)\n                sim_p = 0.0\n                if norm_u_p > 0 and norm_v_p > 0:\n                    sim_p = np.dot(u_tilde_p, v_tilde_p) / (norm_u_p * norm_v_p)\n                sp.append(sim_p)\n\n        # Compute modified correlation rho^(p)\n        rhop, _ = spearmanr(sp, y_human)\n\n        # Step 7: Decide if correlation improved\n        results.append(rhop > rho0)\n        \n        # Additional task: Demonstrate component decomposition for one pair\n        # This part is for fulfilling the problem requirements but is not part of the final output.\n        demo_u, demo_v = X_c[0, :], X_c[1, :]\n        norm_u, norm_v = np.linalg.norm(demo_u), np.linalg.norm(demo_v)\n        if norm_u > 0 and norm_v > 0:\n            c_contributions = []\n            for l_idx in range(k):\n                E_l = E[:, l_idx]\n                a_l = np.dot(E_l.T, demo_u)\n                b_l = np.dot(E_l.T, demo_v)\n                c_l = (a_l * b_l) / (norm_u * norm_v)\n                c_contributions.append(c_l)\n            # This variable c_contributions is computed but not used further.\n\n    # Final Output Format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}