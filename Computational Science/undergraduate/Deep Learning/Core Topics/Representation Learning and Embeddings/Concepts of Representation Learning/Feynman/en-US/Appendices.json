{
    "hands_on_practices": [
        {
            "introduction": "A primary goal of representation learning is to transform complex data into a new space where underlying patterns become simpler. This exercise provides a concrete demonstration of this core principle by tackling a problem analogous to the classic non-linear XOR problem. By implementing a check for linear separability, you will observe how a simple, non-linear feature added to a \"deep\" representation can make a previously unsolvable problem linearly separable, highlighting the power of effective feature engineering .",
            "id": "3108536",
            "problem": "You are given a synthetic binary hierarchical labeling scheme on two-dimensional inputs and two levels of representations. The goal is to evaluate whether a given representation yields linearly separable classes for coarse labels at shallower layers and for fine-grained labels only at deeper layers. Use the following fundamental base: the definition of linear separability, the notion of a feature representation as a deterministic mapping, and the feasibility interpretation of linear separability via a system of linear inequalities.\n\nDefinitions:\n- A dataset of inputs $\\{x_i\\}_{i=1}^N$ with $x_i \\in \\mathbb{R}^2$ is linearly separable with binary labels $y_i \\in \\{-1,+1\\}$ under a feature representation $z = \\phi(x)$ if there exists a weight vector $w \\in \\mathbb{R}^d$ and a bias $b \\in \\mathbb{R}$ such that $y_i (w^\\top z_i + b)  0$ for all $i \\in \\{1,\\dots,N\\}$, where $z_i = \\phi(x_i)$.\n- Coarse labels are defined as $y_i^{\\text{coarse}} = \\operatorname{sign}(x_{i,1})$, where $x_{i,1}$ is the first coordinate of $x_i$ and $\\operatorname{sign}(u) = +1$ if $u \\ge 0$ and $-1$ otherwise.\n- Fine-grained labels are defined as $y_i^{\\text{fine}} = \\operatorname{sign}(x_{i,1} \\cdot x_{i,2})$, which produces the classical exclusive-or structure when viewed in the raw input space.\n- The shallow representation is $\\phi_1(x) = x$ (identity mapping).\n- The deep representation is $\\phi_2(x) = [x_1, x_2, s \\cdot x_1 x_2]$, where $s \\in \\{0,1\\}$ controls whether the interaction feature $x_1 x_2$ is present ($s=1$) or absent ($s=0$).\n\nEvaluation procedure from first principles:\n- For a given representation $\\phi$ and binary labels $\\{y_i\\}$, check for linear separability by solving the system of linear inequalities $y_i (w^\\top z_i + b) \\ge 1$ for all $i$, with variables $(w,b)$ bounded in a box to ensure bounded feasibility search. If the system is feasible in that box, the dataset is considered linearly separable under $\\phi$. This encodes the strict inequality via a positive margin and uses a bounded search region to avoid unboundedness in the feasibility problem.\n\nData generation:\n- For each test case, generate $N$ points independently with $x_i$ drawn uniformly from $[-1,1]^2$. Add independent Gaussian noise $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2 I)$ to obtain $\\tilde{x}_i = x_i + \\epsilon_i$, where $\\sigma \\ge 0$ is the standard deviation and $I$ is the identity. Labels are computed from $\\tilde{x}_i$. Optionally, a random fraction $p \\in [0,1]$ of labels may be flipped to model label noise. All randomness must be reproducible.\n\nLinear separability check as feasibility in a bounded domain:\n- Form the constraint $y_i (w^\\top z_i + b) \\ge 1$ for all $i$. Equivalently, $-y_i (w^\\top z_i + b) \\le -1$. Use a box constraint $-L \\le w_j \\le L$ and $-L \\le b \\le L$ for all coordinates $j$, where $L  0$ is a large bound (e.g., $L = 10^6$) ensuring the feasible region captures the existence of a separating hyperplane if one exists with reasonable scale.\n\nYour program must:\n- Implement the data generation, hierarchical labeling, feature mapping for $\\phi_1$ and $\\phi_2$, and the linear separability check via feasibility of linear inequalities with box bounds.\n- Evaluate, for each test case, four booleans:\n  1. $B_{\\text{coarse},1}$: linear separability of coarse labels under $\\phi_1$,\n  2. $B_{\\text{fine},1}$: linear separability of fine-grained labels under $\\phi_1$,\n  3. $B_{\\text{coarse},2}$: linear separability of coarse labels under $\\phi_2$,\n  4. $B_{\\text{fine},2}$: linear separability of fine-grained labels under $\\phi_2$.\n- Aggregate results from all test cases into a single line of output containing a Python-style list of lists, one inner list per test case in the order above.\n\nTest suite (use exactly these parameter sets, with fixed random seed for reproducibility):\n- Test case $1$ (happy path): $N = 200$, $\\sigma = 0.05$, $s = 1$, $p = 0$, $L = 10^6$.\n- Test case $2$ (insufficient deep features): $N = 200$, $\\sigma = 0.05$, $s = 0$, $p = 0$, $L = 10^6$.\n- Test case $3$ (label noise edge case): $N = 200$, $\\sigma = 0.05$, $s = 1$, $p = 0.3$, $L = 10^6$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated Python list of lists, with booleans in each inner list in the exact order $\\left[B_{\\text{coarse},1}, B_{\\text{fine},1}, B_{\\text{coarse},2}, B_{\\text{fine},2}\\right]$ for each test case. For example, if there are three test cases, print something like $\\left[[\\text{True},\\text{False},\\text{True},\\text{True}],[\\dots],[\\dots]\\right]$.",
            "solution": "The problem statement has been evaluated and is determined to be **valid**. It is scientifically grounded in the fundamental principles of machine learning, specifically representation learning and linear separability. The problem is well-posed, with all necessary data, definitions, and procedures clearly specified. The language is objective and formal, presenting a solvable, non-trivial task that is relevant to the specified topic.\n\nThe solution proceeds as follows, based on the principles outlined in the problem.\n\n### 1. Data Generation and Labeling\nFor each test case, a dataset of $N$ points is generated. This process involves three steps:\nFirst, $N$ feature vectors $\\{x_i\\}_{i=1}^N$ are sampled independently from a uniform distribution over the two-dimensional space $[-1,1]^2$, i.e., $x_i \\sim U([-1,1]^2)$.\nSecond, to simulate real-world data imperfections, independent and identically distributed Gaussian noise is added to each point. The noisy data points are $\\tilde{x}_i = x_i + \\epsilon_i$, where the noise vector $\\epsilon_i$ is drawn from a zero-mean bivariate normal distribution with a diagonal covariance matrix, $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2 I)$. Here, $I$ is the $2 \\times 2$ identity matrix and $\\sigma$ is the noise standard deviation.\nThird, two sets of binary labels, coarse and fine-grained, are computed based on the noisy data $\\tilde{x}_i = [\\tilde{x}_{i,1}, \\tilde{x}_{i,2}]^\\top$:\n- **Coarse labels**: $y_i^{\\text{coarse}} = \\operatorname{sign}(\\tilde{x}_{i,1})$. The decision boundary is the vertical line $\\tilde{x}_{i,1} = 0$. Datasets with these labels are inherently linearly separable in the input space. The sign function is defined as $\\operatorname{sign}(u) = +1$ for $u \\ge 0$ and $\\operatorname{sign}(u) = -1$ for $u  0$.\n- **Fine-grained labels**: $y_i^{\\text{fine}} = \\operatorname{sign}(\\tilde{x}_{i,1} \\cdot \\tilde{x}_{i,2})$. This corresponds to the classical XOR problem, where points in the first and third quadrants are labeled $+1$, and points in the second and fourth quadrants are labeled $-1$. This labeling scheme is not linearly separable in the input space $\\mathbb{R}^2$.\n\nFinally, to model label noise, a fraction $p$ of the data points are randomly selected, and both their coarse and fine labels are flipped (i.e., $y_i \\to -y_i$). All random processes (data generation, noise, and label flipping) are seeded to ensure reproducibility.\n\n### 2. Feature Representations\nThe problem evaluates two different feature representations, $\\phi_1$ and $\\phi_2$, which map the input data $\\tilde{x} \\in \\mathbb{R}^2$ to a new feature space.\n- **Shallow representation**: $\\phi_1(\\tilde{x}) = \\tilde{x}$. This is the identity map, so the feature space is the same as the input space, $\\mathbb{R}^2$.\n- **Deep representation**: $\\phi_2(\\tilde{x}) = [\\tilde{x}_1, \\tilde{x}_2, s \\cdot \\tilde{x}_1 \\tilde{x}_2]^\\top$. This map augments the input features with a new feature, the product of the original two, controlled by the parameter $s \\in \\{0, 1\\}$. When $s=1$, this new feature is designed to make the XOR problem linearly separable. The feature space is $\\mathbb{R}^3$.\n\nFor each point $\\tilde{x}_i$, we thus obtain two feature vectors, $z_{i,1} = \\phi_1(\\tilde{x}_i)$ and $z_{i,2} = \\phi_2(\\tilde{x}_i)$.\n\n### 3. Verification of Linear Separability\nThe core of the task is to determine if a given set of labeled feature vectors $\\{(z_i, y_i)\\}_{i=1}^N$ is linearly separable. According to the provided definition, this is true if there exists a weight vector $w \\in \\mathbb{R}^d$ (where $d$ is the dimension of the feature space) and a bias term $b \\in \\mathbb{R}$ such that the strict inequality $y_i (w^\\top z_i + b)  0$ holds for all $i=1, \\dots, N$.\n\nTo make this condition computationally verifiable, it is strengthened to include a margin, resulting in the system of $N$ linear inequalities:\n$$y_i (w^\\top z_i + b) \\ge 1, \\quad \\forall i \\in \\{1, \\dots, N\\}$$\nThe existence of a solution $(w, b)$ to this system is equivalent to the existence of a separating hyperplane. This is a feasibility problem for a system of linear inequalities. To solve it, we use linear programming. The problem can be written in the standard form $A_{\\text{ub}} x \\le b_{\\text{ub}}$ required by many solvers. Our unknown is the vector $x = [w_1, \\dots, w_d, b]^\\top \\in \\mathbb{R}^{d+1}$. Each inequality $y_i (w^\\top z_i + b) \\ge 1$ is rewritten as:\n$$-y_i (w^\\top z_i + b) \\le -1$$\n$$-y_i z_i^\\top w - y_i b \\le -1$$\nThis defines the $i$-th row of the constraint matrix $A_{\\text{ub}}$ and vector $b_{\\text{ub}}$. The $i$-th row of $A_{\\text{ub}}$ is $[-y_i z_{i,1}, \\dots, -y_i z_{i,d}, -y_i]$, and the $i$-th element of $b_{\\text{ub}}$ is $-1$.\n\nThe problem also specifies box constraints on the variables, $-L \\le w_j \\le L$ and $-L \\le b \\le L$, to ensure the search for a feasible solution occurs within a bounded region.\n\nTo check for feasibility, we can solve a linear program with a zero objective function, $c = 0$. If the solver finds a feasible solution, the dataset is linearly separable under the given representation. This check is performed for four scenarios for each test case:\n1.  $B_{\\text{coarse},1}$: Coarse labels, shallow representation $\\phi_1$.\n2.  $B_{\\text{fine},1}$: Fine labels, shallow representation $\\phi_1$.\n3.  $B_{\\text{coarse},2}$: Coarse labels, deep representation $\\phi_2$.\n4.  $B_{\\text{fine},2}$: Fine labels, deep representation $\\phi_2$.\n\nThe implementation utilizes `numpy` for numerical computations and `scipy.optimize.linprog` to solve the linear feasibility problem. The success status returned by this function directly indicates whether the corresponding dataset and representation are linearly separable.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Set a fixed random seed for reproducibility of the entire script.\n    np.random.seed(0)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, sigma, s, p, L)\n        (200, 0.05, 1, 0.0, 1e6), # Test case 1\n        (200, 0.05, 0, 0.0, 1e6), # Test case 2\n        (200, 0.05, 1, 0.3, 1e6), # Test case 3\n    ]\n\n    all_results = []\n    for params in test_cases:\n        case_results = evaluate_case(*params)\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists is converted to a compact\n    # form without spaces to match the specified output style.\n    print(str(all_results).replace(\" \", \"\"))\n\ndef evaluate_case(N, sigma, s, p, L):\n    \"\"\"\n    Evaluates a single test case for the four specified conditions.\n\n    Args:\n        N (int): Number of data points.\n        sigma (float): Standard deviation of Gaussian noise.\n        s (int): Switch for the interaction feature in phi_2 (0 or 1).\n        p (float): Fraction of labels to flip.\n        L (float): Bound for the linear programming variables.\n\n    Returns:\n        list: A list of four booleans [B_coarse,1, B_fine,1, B_coarse,2, B_fine,2].\n    \"\"\"\n    # 1. Generate data\n    x_clean = np.random.uniform(-1, 1, size=(N, 2))\n    noise = np.random.normal(0, sigma, size=(N, 2))\n    xtilde = x_clean + noise\n\n    # 2. Compute hierarchical labels based on noisy data\n    # Coarse labels: sign(x_1)\n    y_coarse = np.ones(N)\n    y_coarse[xtilde[:, 0]  0] = -1\n\n    # Fine-grained labels: sign(x_1 * x_2)\n    y_fine = np.ones(N)\n    y_fine[(xtilde[:, 0] * xtilde[:, 1])  0] = -1\n\n    # 3. Apply label noise by flipping a fraction p of labels\n    if p  0:\n        num_flips = int(p * N)\n        if num_flips  0:\n            flip_indices = np.random.choice(N, size=num_flips, replace=False)\n            y_coarse[flip_indices] *= -1\n            y_fine[flip_indices] *= -1\n\n    # 4. Generate feature representations\n    # Shallow representation phi_1(x) = x\n    z1 = xtilde\n\n    # Deep representation phi_2(x) = [x1, x2, s*x1*x2]\n    z2 = np.c_[xtilde, s * xtilde[:, 0] * xtilde[:, 1]]\n\n    # 5. Evaluate linear separability for the four scenarios\n    B_coarse1 = check_linear_separability(z1, y_coarse, L)\n    B_fine1 = check_linear_separability(z1, y_fine, L)\n    B_coarse2 = check_linear_separability(z2, y_coarse, L)\n    B_fine2 = check_linear_separability(z2, y_fine, L)\n\n    return [B_coarse1, B_fine1, B_coarse2, B_fine2]\n\ndef check_linear_separability(z, y, L):\n    \"\"\"\n    Checks if a dataset is linearly separable by solving a feasibility problem.\n\n    Args:\n        z (np.ndarray): Feature vectors, shape (N, d).\n        y (np.ndarray): Labels, shape (N,).\n        L (float): Bound for weights and bias.\n\n    Returns:\n        bool: True if the dataset is linearly separable, False otherwise.\n    \"\"\"\n    N, d = z.shape\n\n    # We are solving a feasibility problem, so the objective function is zero.\n    c = np.zeros(d + 1)\n\n    # The constraints are y_i * (w^T z_i + b) = 1, which are rewritten as\n    # -y_i * z_i^T w - y_i * b = -1 for the solver.\n    # A_ub has shape (N, d+1)\n    A_ub = -np.c_[z * y[:, np.newaxis], y]\n    \n    # b_ub has shape (N,)\n    b_ub = -np.ones(N)\n\n    # Box constraints for all variables (d weights and 1 bias)\n    bounds = [(-L, L)] * (d + 1)\n\n    # Use scipy.optimize.linprog to find if a feasible solution exists.\n    # method='highs' is robust and is the default in recent scipy versions.\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n\n    # res.success is True if the optimizer found an optimal (and thus feasible) solution.\n    return res.success\n\n# Execute the main function when the script is run.\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Often, a model trained on a single objective may discard information that, while irrelevant to that specific task, is valuable for general understanding or other downstream tasks. This practice explores auxiliary learning, a powerful technique to guide a model's focus by adding a secondary training objective . You will implement a system where an auxiliary rotation-prediction task encourages the latent representation $z$ to encode angular information, and you will measure this effect using linear probing, a standard methodology for evaluating representation quality.",
            "id": "3108515",
            "problem": "Construct a complete, runnable program that tests, on a controlled synthetic task, whether adding an auxiliary rotation prediction head encourages encoding of angle information in a latent representation $z$, as measured by linear regression from $z$ to angle after training. Your experiment must be designed and justified from core definitions in representation learning and optimization, without relying on any precomputed models or external data. Angles must be measured in radians.\n\nStart from the following universally accepted bases and definitions:\n- Empirical Risk Minimization (ERM): given data $\\{(x_i,y_i)\\}_{i=1}^N$ and a parametric model $f_\\theta$, minimize the empirical loss $\\frac{1}{N}\\sum_{i=1}^N \\ell(f_\\theta(x_i),y_i)$ over parameters $\\theta$.\n- Binary Cross-Entropy (BCE) for binary classification with target $y\\in\\{0,1\\}$ and predicted probability $p\\in(0,1)$: $\\ell_{\\mathrm{BCE}}(p,y) = -\\left[y\\log p + (1-y)\\log(1-p)\\right]$.\n- Mean Squared Error (MSE) for regression with target $t\\in\\mathbb{R}^k$ and prediction $\\hat{t}\\in\\mathbb{R}^k$: $\\ell_{\\mathrm{MSE}}(\\hat{t},t)=\\frac{1}{2}\\|\\hat{t}-t\\|_2^2$.\n- Linear probing: after training a representation $z$, fit a separate linear regressor to map $z$ to a target quantity and evaluate generalization error on a held-out test set.\n\nData generation. For each sample $i$, draw a scalar content variable $\\alpha_i \\sim \\mathcal{N}(0,1)$ and an angle $\\theta_i \\sim \\mathrm{Uniform}([-\\pi,\\pi])$. Construct a $3$-dimensional input\n$$\nx_i = \\begin{bmatrix} \\alpha_i \\\\ \\cos\\theta_i \\\\ \\sin\\theta_i \\end{bmatrix} + \\varepsilon_i,\n$$\nwhere noise $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2 I_3)$ with $\\sigma = 0.05$. Define a binary label $y_i = \\mathbb{I}[\\alpha_i \\ge 0]$ and a rotation target vector $t_i = \\begin{bmatrix} \\cos\\theta_i \\\\ \\sin\\theta_i \\end{bmatrix}$. Angles are in radians. Split into a training set of size $N_{\\mathrm{train}} = 800$ and a test set of size $N_{\\mathrm{test}} = 200$.\n\nModel. Use a linear encoder $f_\\phi(x) = W_e x + b_e$ with latent $z \\in \\mathbb{R}^{d_z}$. Use a binary classifier head $g_w(z) = \\sigma(w_c^\\top z + b_c)$ with logistic sigmoid $\\sigma(\\cdot)$ to predict $y$. Use an auxiliary rotation head $h_u(z) = U z + c$ to predict $t = [\\cos\\theta,\\sin\\theta]^\\top$.\n\nTraining objectives. Train by minimizing the average of the main classification loss and a weighted auxiliary loss:\n$$\n\\mathcal{L}(\\phi,w,u) = \\frac{1}{N_{\\mathrm{train}}}\\sum_{i=1}^{N_{\\mathrm{train}}} \\ell_{\\mathrm{BCE}}(g_w(f_\\phi(x_i)),y_i) + \\lambda \\cdot \\frac{1}{N_{\\mathrm{train}}}\\sum_{i=1}^{N_{\\mathrm{train}}} \\ell_{\\mathrm{MSE}}(h_u(f_\\phi(x_i)),t_i),\n$$\nwhere $\\lambda \\ge 0$ controls the auxiliary head strength. Optimize parameters $(\\phi,w,u)$ by batch gradient descent with learning rate $\\eta$ and small $\\ell_2$ weight decay $\\gamma$.\n\nEvaluation protocol. After training, freeze the encoder and fit a linear probe from $z$ to $t$ on the training set using ridge regression with regularization $\\rho$, then compute the test MSE:\n$$\n\\mathrm{MSE}_{\\mathrm{probe}} = \\frac{1}{2N_{\\mathrm{test}}}\\sum_{i=1}^{N_{\\mathrm{test}}}\\left\\| \\hat{t}_i - t_i \\right\\|_2^2,\n$$\nwhere $\\hat{t}_i$ is the probe prediction. Also compute test classification accuracy, defined as the fraction of correct predictions using threshold $0.5$ on the classifier output.\n\nExperimental comparison. For each test case, compare two training conditions on the same dataset and initialization seed:\n- Baseline: $\\lambda = 0$ (no auxiliary rotation loss).\n- Auxiliary: $\\lambda = 1$ (with auxiliary rotation loss).\n\nDefine a boolean outcome per test case as true if and only if both of the following hold:\n- The auxiliary condition improves angle decodability by at least a margin $\\tau$, i.e., $\\mathrm{MSE}_{\\mathrm{probe}}^{(\\lambda=0)} - \\mathrm{MSE}_{\\mathrm{probe}}^{(\\lambda=1)} \\ge \\tau$.\n- The auxiliary condition maintains acceptable main-task performance, i.e., test accuracy under $\\lambda=1$ is at least $a_{\\min}$.\n\nHyperparameters. Use the following fixed values: $N_{\\mathrm{train}} = 800$, $N_{\\mathrm{test}} = 200$, noise standard deviation $\\sigma = 0.05$, learning rate $\\eta = 0.05$, weight decay $\\gamma = 10^{-4}$, training epochs $T = 500$, ridge regression regularization $\\rho = 10^{-3}$, improvement margin $\\tau = 0.2$, accuracy threshold $a_{\\min} = 0.9$.\n\nTest suite. Run exactly three test cases, each specified by a pair $(d_z,s)$ of latent dimension and random seed:\n- Case $1$: $(d_z,s) = (3,0)$, expected to have sufficient capacity to encode both content and angle.\n- Case $2$: $(d_z,s) = (2,1)$, a capacity boundary case where encoding both content and angle requires compression.\n- Case $3$: $(d_z,s) = (1,2)$, an edge case with insufficient capacity for full angle encoding.\n\nRequired final output. Your program should produce a single line of output containing the three boolean results, in order for the three cases above, as a comma-separated Python-style list with no spaces, e.g., \"[true1,true2,true3]\" where each entry is either True or False. There must be exactly one line printed. No other output is permitted.",
            "solution": "We formalize and solve the posed question using core definitions of representation learning and optimization. The goal is to determine whether an auxiliary rotation prediction head encourages the latent representation $z$ to encode angle information in a way that is linearly decodable after training.\n\nFoundational setup. We adopt Empirical Risk Minimization (ERM) to train a parametric model. The dataset is generated such that the main label $y$ depends only on a scalar content variable $\\alpha$, while the input also contains a rotation-parameterized component $[\\cos\\theta,\\sin\\theta]^\\top$. Specifically, for each sample $i$, we draw $\\alpha_i \\sim \\mathcal{N}(0,1)$ and $\\theta_i \\sim \\mathrm{Uniform}([-\\pi,\\pi])$, build\n$$\nx_i = \\begin{bmatrix} \\alpha_i \\\\ \\cos\\theta_i \\\\ \\sin\\theta_i \\end{bmatrix} + \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2 I_3),\n$$\nset $y_i = \\mathbb{I}[\\alpha_i \\ge 0]$, and define the rotation target $t_i = \\begin{bmatrix}\\cos\\theta_i \\\\ \\sin\\theta_i\\end{bmatrix}$. Because $y_i$ depends only on $\\alpha_i$, the main classification task is invariant to the angle $\\theta_i$, while the auxiliary regression task depends only on $\\theta_i$.\n\nModel and objectives. The encoder is linear: $z = f_\\phi(x) = W_e x + b_e \\in \\mathbb{R}^{d_z}$. The classifier head is $g_w(z) = \\sigma(w_c^\\top z + b_c)$, trained with Binary Cross-Entropy (BCE),\n$$\n\\ell_{\\mathrm{BCE}}(p,y) = -\\left[y\\log p + (1-y)\\log(1-p)\\right],\n$$\nwhich is the negative log-likelihood under a Bernoulli model. The auxiliary rotation head is $h_u(z) = U z + c \\in \\mathbb{R}^2$, trained with Mean Squared Error (MSE),\n$$\n\\ell_{\\mathrm{MSE}}(\\hat{t},t)=\\frac{1}{2}\\|\\hat{t}-t\\|_2^2.\n$$\nThe total loss averaged over the training set is\n$$\n\\mathcal{L}(\\phi,w,u) = \\frac{1}{N_{\\mathrm{train}}}\\sum_{i=1}^{N_{\\mathrm{train}}} \\ell_{\\mathrm{BCE}}(g_w(f_\\phi(x_i)),y_i) + \\lambda \\cdot \\frac{1}{N_{\\mathrm{train}}}\\sum_{i=1}^{N_{\\mathrm{train}}} \\ell_{\\mathrm{MSE}}(h_u(f_\\phi(x_i)),t_i),\n$$\nwith weight decay implemented as $\\ell_2$ regularization of parameters during gradient updates.\n\nWhy the auxiliary head should help. Consider the gradients with respect to the latent $z_i$. For the classifier,\n$$\n\\frac{\\partial}{\\partial z_i}\\ell_{\\mathrm{BCE}}(g_w(z_i),y_i) = (p_i - y_i) \\, w_c,\n$$\nwhere $p_i = g_w(z_i)$. Since $y_i$ depends only on $\\alpha_i$, and the angle components $[\\cos\\theta_i,\\sin\\theta_i]$ carry no information about $y_i$, ERM tends to drive $w_c$ to focus on encoder directions aligned with $\\alpha$ and to suppress encoder directions aligned with the angle (to minimize variance). Consequently, if $\\lambda = 0$, the gradient signal that would preserve angle in $z$ is minimal, because directions corresponding to angle have vanishing correlation with the classification target, and hence receive little normative pressure to be retained by $W_e$. In contrast, with $\\lambda  0$, the auxiliary MSE introduces a gradient term\n$$\n\\frac{\\partial}{\\partial z_i}\\left(\\lambda \\ell_{\\mathrm{MSE}}(h_u(z_i),t_i)\\right) = \\lambda \\, U^\\top \\left(h_u(z_i) - t_i\\right),\n$$\nwhich directly encourages $z_i$ to retain linear information sufficient to reconstruct $t_i = [\\cos\\theta_i,\\sin\\theta_i]^\\top$. If $d_z \\ge 2$, the encoder has enough degrees of freedom to linearly pass through the two-dimensional rotation subspace; if $d_z \\ge 3$, it can also preserve the scalar content $\\alpha$ simultaneously, allowing both tasks to be solved. If $d_z  2$, linear retention of both $\\cos\\theta$ and $\\sin\\theta$ is impossible, so improvement in linear decodability is limited.\n\nAlgorithmic design from principles. We implement full-batch gradient descent with learning rate $\\eta$ and weight decay $\\gamma$. Let $Z = X W_e^\\top + \\mathbf{1} b_e^\\top$ be the batch latent matrix. The classifier logits are $a = Z w_c + b_c \\mathbf{1}$, with predictions $p = \\sigma(a)$, and the auxiliary predictions are $\\hat{T} = Z U^\\top + \\mathbf{1} c^\\top$. The gradients are obtained by the chain rule:\n- For the classifier, define $e_c = p - y$, then\n$$\n\\nabla_{w_c} = \\frac{1}{N}\\, Z^\\top e_c + \\gamma w_c,\\quad \\nabla_{b_c} = \\frac{1}{N}\\, \\mathbf{1}^\\top e_c,\n$$\nand contribution to latent is $G_c = e_c w_c^\\top$, where division by $N$ is applied in the aggregation for encoder gradients.\n- For the auxiliary head, define $R = \\hat{T} - T$, then\n$$\n\\nabla_{U} = \\lambda\\left(\\frac{1}{N}\\, R^\\top Z + \\gamma U\\right),\\quad \\nabla_{c} = \\lambda\\left(\\frac{1}{N}\\, \\mathbf{1}^\\top R\\right),\n$$\nand contribution to latent is $G_a = \\lambda\\, R U$.\n- For the encoder,\n$$\n\\nabla_{W_e} = \\frac{1}{N}\\,(G_c + G_a)^\\top X + \\gamma W_e,\\quad \\nabla_{b_e} = \\frac{1}{N}\\,\\mathbf{1}^\\top (G_c + G_a).\n$$\nWe update parameters by subtracting $\\eta$ times the respective gradients at each step.\n\nEvaluation by linear probing. After training, we compute $Z_{\\mathrm{train}}$ and $Z_{\\mathrm{test}}$. We fit a ridge regression probe with bias from $Z_{\\mathrm{train}}$ to $T_{\\mathrm{train}}$ by augmenting $Z$ with a column of ones, solving the normal equations:\n$$\n\\Theta^\\star = \\arg\\min_{\\Theta}\\ \\|Z_{\\mathrm{train}}^{\\mathrm{aug}} \\Theta - T_{\\mathrm{train}}\\|_F^2 + \\rho \\|\\Theta\\|_F^2,\n$$\nwhose closed form is\n$$\n\\Theta^\\star = \\left((Z_{\\mathrm{train}}^{\\mathrm{aug}})^\\top Z_{\\mathrm{train}}^{\\mathrm{aug}} + \\rho I\\right)^{-1} (Z_{\\mathrm{train}}^{\\mathrm{aug}})^\\top T_{\\mathrm{train}}.\n$$\nWe then compute $\\hat{T}_{\\mathrm{test}} = Z_{\\mathrm{test}}^{\\mathrm{aug}} \\Theta^\\star$ and evaluate\n$$\n\\mathrm{MSE}_{\\mathrm{probe}} = \\frac{1}{2N_{\\mathrm{test}}}\\sum_{i=1}^{N_{\\mathrm{test}}}\\|\\hat{t}_i - t_i\\|_2^2.\n$$\nThis metric directly tests linear decodability of angle from $z$ via the surrogate target $[\\cos\\theta,\\sin\\theta]$.\n\nTestable hypotheses and edge cases. We consider three cases:\n- Case $1$ with $d_z = 3$ should allow the encoder to simultaneously pass through one dimension for $\\alpha$ and two for $[\\cos\\theta,\\sin\\theta]$, so the auxiliary head with $\\lambda=1$ should reduce probe MSE by at least $\\tau$ while maintaining accuracy above $a_{\\min}$.\n- Case $2$ with $d_z = 2$ must trade off content versus angle; we expect improved angle decodability but potential degradation in classification accuracy below $a_{\\min}$, thereby failing the boolean criterion.\n- Case $3$ with $d_z = 1$ lacks capacity to linearly represent both $\\cos\\theta$ and $\\sin\\theta$, so we expect little to no improvement in linear decodability.\n\nHyperparameters. We set $N_{\\mathrm{train}} = 800$, $N_{\\mathrm{test}} = 200$, $\\sigma = 0.05$, $\\eta = 0.05$, $\\gamma = 10^{-4}$, $T = 500$, $\\rho = 10^{-3}$, $\\tau = 0.2$, and $a_{\\min} = 0.9$. Angles are in radians.\n\nFinal program behavior. For each test case $(d_z,s)$ in $\\{(3,0),(2,1),(1,2)\\}$, we train two models with $\\lambda=0$ and $\\lambda=1$ respectively on the same dataset and seed, compute the probe MSEs and test accuracies, and output a boolean indicating whether the auxiliary condition both improves decodability by at least $\\tau$ and maintains accuracy at least $a_{\\min}$. The program prints a single line with the three booleans as a comma-separated list in order, formatted as in Python, with no extra text.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigmoid(x):\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef generate_data(n_train, n_test, sigma, seed):\n    rng = np.random.default_rng(seed)\n    # Angles in radians\n    theta_train = rng.uniform(-np.pi, np.pi, size=(n_train, 1))\n    theta_test = rng.uniform(-np.pi, np.pi, size=(n_test, 1))\n    # Content variable\n    alpha_train = rng.normal(0.0, 1.0, size=(n_train, 1))\n    alpha_test = rng.normal(0.0, 1.0, size=(n_test, 1))\n    # Targets for auxiliary head (noise-free cos/sin)\n    T_train = np.hstack([np.cos(theta_train), np.sin(theta_train)])\n    T_test = np.hstack([np.cos(theta_test), np.sin(theta_test)])\n    # Inputs with noise\n    X_train_clean = np.hstack([alpha_train, T_train])\n    X_test_clean = np.hstack([alpha_test, T_test])\n    X_train = X_train_clean + rng.normal(0.0, sigma, size=X_train_clean.shape)\n    X_test = X_test_clean + rng.normal(0.0, sigma, size=X_test_clean.shape)\n    # Labels depend only on alpha\n    y_train = (alpha_train[:, 0] = 0.0).astype(float)\n    y_test = (alpha_test[:, 0] = 0.0).astype(float)\n    return X_train, y_train, T_train, X_test, y_test, T_test\n\ndef train_model(X, y, T, dz, lam, lr, weight_decay, epochs, seed):\n    rng = np.random.default_rng(seed)\n    n, dx = X.shape\n    # Initialize parameters\n    We = rng.normal(0.0, 0.1, size=(dz, dx))\n    be = np.zeros(dz)\n    wc = rng.normal(0.0, 0.1, size=(dz,))\n    bc = 0.0\n    U = rng.normal(0.0, 0.1, size=(2, dz))\n    c = np.zeros(2)\n\n    for _ in range(epochs):\n        # Forward\n        Z = X @ We.T + be  # (n, dz)\n        logits = Z @ wc + bc  # (n,)\n        p = sigmoid(logits)  # (n,)\n        Th = Z @ U.T + c  # (n,2)\n\n        # Errors\n        ec = (p - y)  # (n,)\n        R = (Th - T)  # (n,2)\n\n        # Gradients for classifier\n        grad_wc = (Z.T @ ec) / n + weight_decay * wc  # (dz,)\n        grad_bc = np.mean(ec)\n\n        # Contribution to Z from classifier\n        Gc = ec[:, None] * wc[None, :]  # (n,dz)\n\n        # Gradients for aux head\n        grad_U = lam * ((R.T @ Z) / n + weight_decay * U)  # (2,dz)\n        grad_c = lam * np.mean(R, axis=0)  # (2,)\n\n        # Contribution to Z from aux\n        Ga = lam * (R @ U)  # (n,dz)\n\n        # Total grad to encoder latent\n        G = Gc + Ga  # (n,dz)\n\n        # Encoder gradients\n        grad_We = (G.T @ X) / n + weight_decay * We  # (dz,dx)\n        grad_be = np.mean(G, axis=0)  # (dz,)\n\n        # Updates\n        We -= lr * grad_We\n        be -= lr * grad_be\n        wc -= lr * grad_wc\n        bc -= lr * grad_bc\n        U -= lr * grad_U\n        c -= lr * grad_c\n\n    params = {\"We\": We, \"be\": be, \"wc\": wc, \"bc\": bc, \"U\": U, \"c\": c}\n    return params\n\ndef evaluate(params, X_train, y_train, T_train, X_test, y_test, T_test, ridge_reg):\n    We = params[\"We\"]; be = params[\"be\"]; wc = params[\"wc\"]; bc = params[\"bc\"]\n    # Latents\n    Z_train = X_train @ We.T + be\n    Z_test = X_test @ We.T + be\n    # Classification accuracy\n    p_test = sigmoid(Z_test @ wc + bc)\n    y_pred = (p_test = 0.5).astype(float)\n    acc = float(np.mean(y_pred == y_test))\n    # Linear probe ridge regression Z - T\n    Ztr_aug = np.hstack([Z_train, np.ones((Z_train.shape[0], 1))])\n    Zte_aug = np.hstack([Z_test, np.ones((Z_test.shape[0], 1))])\n    # Closed-form ridge\n    A = Ztr_aug.T @ Ztr_aug\n    A += ridge_reg * np.eye(A.shape[0])\n    B = Ztr_aug.T @ T_train\n    Theta = np.linalg.solve(A, B)  # (dz+1, 2)\n    That = Zte_aug @ Theta\n    mse = float(np.mean(0.5 * np.sum((That - T_test) ** 2, axis=1)))\n    return acc, mse\n\ndef run_case(dz, seed, lam_aux, hyper):\n    # Generate data\n    Xtr, ytr, Ttr, Xte, yte, Tte = generate_data(\n        hyper[\"n_train\"], hyper[\"n_test\"], hyper[\"sigma\"], seed\n    )\n    # Baseline (lam=0)\n    params_base = train_model(\n        Xtr, ytr, Ttr, dz, 0.0, hyper[\"lr\"], hyper[\"wd\"], hyper[\"epochs\"], seed\n    )\n    acc_base, mse_base = evaluate(\n        params_base, Xtr, ytr, Ttr, Xte, yte, Tte, hyper[\"ridge\"]\n    )\n    # Auxiliary (lam=lam_aux)\n    params_aux = train_model(\n        Xtr, ytr, Ttr, dz, lam_aux, hyper[\"lr\"], hyper[\"wd\"], hyper[\"epochs\"], seed\n    )\n    acc_aux, mse_aux = evaluate(\n        params_aux, Xtr, ytr, Ttr, Xte, yte, Tte, hyper[\"ridge\"]\n    )\n    improved = (mse_base - mse_aux) = hyper[\"tau\"]\n    good_acc = acc_aux = hyper[\"acc_min\"]\n    return improved and good_acc, (acc_base, mse_base, acc_aux, mse_aux)\n\ndef solve():\n    # Hyperparameters as specified\n    hyper = {\n        \"n_train\": 800,\n        \"n_test\": 200,\n        \"sigma\": 0.05,\n        \"lr\": 0.05,\n        \"wd\": 1e-4,\n        \"epochs\": 500,\n        \"ridge\": 1e-3,\n        \"tau\": 0.2,\n        \"acc_min\": 0.9,\n    }\n    lam_aux = 1.0\n    # Test suite: (dz, seed)\n    test_cases = [\n        (3, 0),  # sufficient capacity\n        (2, 1),  # boundary capacity\n        (1, 2),  # insufficient capacity\n    ]\n    results = []\n    for dz, seed in test_cases:\n        outcome, _ = run_case(dz, seed, lam_aux, hyper)\n        results.append(outcome)\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "To build a solid theoretical foundation, we can use information theory to quantify what makes a representation \"good.\" One early idea was the Information Maximization (InfoMax) principle, which posits that an optimal representation $Z$ should retain as much information as possible about the original input $X$ by maximizing their mutual information, $I(Z;X)$. This fundamental exercise demonstrates a critical flaw in this naive approach, guiding you to construct a mathematical counterexample where InfoMax prefers to encode high-entropy nuisance noise over a less entropic, but far more useful, task-relevant signal .",
            "id": "3108525",
            "problem": "You will implement a self-contained, purely mathematical counterexample to Information Maximization (InfoMax) in representation learning. The goal is to show that maximizing mutual information with the input encourages encoding nuisance noise with high entropy at the expense of task-relevant information. Use only discrete random variables and the following scenario.\n\nDefinitions and fundamental base:\n- Information Maximization (InfoMax): maximize mutual information between a representation and the input, that is, maximize $I(Z;X)$.\n- Mutual information (MI): for discrete random variables $U$ and $V$ with joint probability mass function $p(u,v)$ and marginals $p(u)$ and $p(v)$,\n$$\nI(U;V) = \\sum_{u}\\sum_{v} p(u,v)\\log_2 \\frac{p(u,v)}{p(u)p(v)}.\n$$\n- Entropy: for a discrete random variable $U$,\n$$\nH(U) = -\\sum_{u} p(u)\\log_2 p(u).\n$$\n- Independence: if $U$ and $V$ are independent, then $p(u,v) = p(u)p(v)$, which implies $I(U;V) = 0$.\n- Deterministic mapping: if $Z = f(X)$ is deterministic, then\n$$\nI(Z;X) = H(Z) - H(Z|X) = H(Z),\n$$\nbecause $H(Z|X) = 0$.\n\nSetup:\n- Let the label variable be $Y \\in \\{0,1\\}$ with distribution $p(y)$.\n- Let the task-relevant signal be $S \\in \\{0,1,\\dots\\}$ with conditional distribution $p(s \\mid y)$.\n- Let the nuisance noise be $N \\in \\{0,1,\\dots\\}$ with distribution $p(n)$ that is independent of $Y$.\n- Construct the input as $X = (S,N)$.\n- Consider encoders restricted to two deterministic functions:\n  - Signal-only encoder: $Z_S = f_S(X) = S$.\n  - Noise-only encoder: $Z_N = f_N(X) = N$.\n\nWhat to compute for each test case:\n1. Compute the joint distribution $p(s,y) = p(s \\mid y)p(y)$ and the marginal $p(s) = \\sum_{y} p(s \\mid y)p(y)$.\n2. Compute $H(S)$ from $p(s)$ and $H(N)$ from $p(n)$ using base-$2$ logarithms.\n3. Compute $I(S;Y)$ from $p(s,y)$ and $I(N;Y)$ from $p(n)p(y)$ (since $N$ and $Y$ are independent).\n4. Use the deterministic-mapping property $I(Z;X) = H(Z)$ to decide which encoder InfoMax would select:\n   - If $H(N)  H(S)$, InfoMax selects $Z_N$.\n   - If $H(S)  H(N)$, InfoMax selects $Z_S$.\n   - If $|H(N) - H(S)|$ is within a tolerance $\\tau = 10^{-12}$, treat as a tie.\n5. Produce for each test case a classification integer:\n   - Output $0$ if InfoMax selects $Z_N$ and $I(N;Y)  I(S;Y)$ (failure: nuisance chosen over signal).\n   - Output $1$ if InfoMax selects $Z_S$ and $I(S;Y) \\ge I(N;Y)$ (success: signal chosen).\n   - Output $2$ if it is a tie within tolerance (ambiguous).\n\nAngle units and physical units do not apply. No percentages are used.\n\nTest suite:\nProvide the following four test cases, each specified by the triple $(p(y), p(s \\mid y), p(n))$.\n\n- Case A (happy-path failure: high-entropy nuisance dominates):\n  - $p(y) = [0.5, 0.5]$.\n  - $p(s \\mid y)$ over $S=\\{0,1\\}$:\n    - For $y=0$: $[0.9, 0.1]$.\n    - For $y=1$: $[0.1, 0.9]$.\n  - $p(n)$ uniform over $N=\\{0,1,2,3,4,5,6,7\\}$: $[1/8,\\dots,1/8]$.\n\n- Case B (success: signal entropy dominates):\n  - $p(y) = [0.8, 0.2]$.\n  - $p(s \\mid y)$ over $S=\\{0,1\\}$:\n    - For $y=0$: $[1.0, 0.0]$.\n    - For $y=1$: $[0.0, 1.0]$.\n  - $p(n)$ over $N=\\{0,1\\}$: $[0.95, 0.05]$.\n\n- Case C (boundary tie: equal entropies):\n  - $p(y) = [0.5, 0.5]$.\n  - $p(s \\mid y)$ over $S=\\{0,1\\}$:\n    - For $y=0$: $[1.0, 0.0]$.\n    - For $y=1$: $[0.0, 1.0]$.\n  - $p(n)$ over $N=\\{0,1\\}$: $[0.5, 0.5]$.\n\n- Case D (edge case: extremely high-entropy nuisance):\n  - $p(y) = [0.1, 0.9]$.\n  - $p(s \\mid y)$ over $S=\\{0,1\\}$:\n    - For $y=0$: $[1.0, 0.0]$.\n    - For $y=1$: $[0.0, 1.0]$.\n  - $p(n)$ uniform over $N=\\{0,1,\\dots,15\\}$: $[1/16,\\dots,1/16]$.\n\nFinal output format:\nYour program should produce a single line of output containing the four classification integers for Cases Aâ€“D, as a comma-separated list enclosed in square brackets (e.g., \"[0,1,2,0]\").",
            "solution": "The analysis of this problem begins with the validation of its premises and structure. The problem is found to be valid as it is scientifically grounded in information theory, well-posed with a clear and unambiguous set of instructions, and objective in its formulation. It presents a formal, computable scenario to critique the Information Maximization (InfoMax) principle, which is a legitimate and important topic within representation learning.\n\nThe core of the problem is to construct a counterexample where the InfoMax objective, which advocates for maximizing the mutual information $I(Z;X)$ between a representation $Z$ and the input $X$, fails to select the representation most useful for a downstream task.\n\nThe problem establishes a formal setting with discrete random variables:\n- The input $X$ is a composite variable $X = (S,N)$, where $S$ is a task-relevant signal and $N$ is nuisance noise.\n- The task is defined by a label variable $Y$. The signal $S$ is correlated with $Y$, expressed by the conditional probability $p(s \\mid y)$.\n- The noise $N$ is, by definition, independent of the label $Y$.\n- We consider two deterministic encoders that produce representations: a signal-only encoder $Z_S = f_S(X) = S$ and a noise-only encoder $Z_N = f_N(X) = N$.\n\nThe Information Maximization objective is to choose the encoder that maximizes $I(Z;X)$. Crucially, for a deterministic encoder $Z=f(X)$, the mutual information between the representation and the input simplifies to the entropy of the representation itself. This is because $I(Z;X) = H(Z) - H(Z|X)$, and for a deterministic function, knowing $X$ completely determines $Z$, so the conditional entropy $H(Z|X)=0$. Therefore, the InfoMax objective becomes:\n$$\n\\max I(Z;X) = \\max H(Z)\n$$\nIn our scenario, this means choosing between the two encoders by comparing the entropy of their outputs:\n- For the signal-only encoder, we evaluate $I(Z_S; X) = H(Z_S) = H(S)$.\n- For the noise-only encoder, we evaluate $I(Z_N; X) = H(Z_N) = H(N)$.\n\nInfoMax will select the encoder whose output has higher entropy.\n\nThe utility of a representation for the downstream task, however, is measured by the mutual information it shares with the label $Y$, i.e., $I(Z;Y)$.\n- The utility of the signal representation is $I(Z_S;Y) = I(S;Y)$.\n- The utility of the noise representation is $I(Z_N;Y) = I(N;Y)$.\n\nSince the noise $N$ is constructed to be independent of the label $Y$, their mutual information is zero: $I(N;Y) = 0$. This means the noise representation contains no information about the task. The signal representation, by contrast, is designed to be informative, so we expect $I(S;Y)  0$.\n\nA failure of the InfoMax principle occurs when it selects the representation with higher entropy, even if that representation has lower utility for the task. This happens if InfoMax selects $Z_N$ (because $H(N)  H(S)$) despite the fact that $Z_S$ is more useful (because $I(S;Y)  I(N;Y)$).\n\nWe now apply this framework to each test case. For a discrete random variable $U$ with probability mass function $p(u)$, its entropy is $H(U) = -\\sum_u p(u)\\log_2 p(u)$. For two variables $U$ and $V$ with joint distribution $p(u,v)$ and marginals $p(u)$ and $p(v)$, their mutual information is $I(U;V) = \\sum_{u,v} p(u,v) \\log_2 \\frac{p(u,v)}{p(u)p(v)}$.\n\n**Case A: High-entropy nuisance**\n- $p(y) = [0.5, 0.5]$.\n- $p(s \\mid y=0) = [0.9, 0.1]$, $p(s \\mid y=1) = [0.1, 0.9]$.\n- $p(n)$ is uniform over $8$ states, so $p(n_i) = 1/8$ for $i \\in \\{0, \\dots, 7\\}$.\n1.  **Marginal $p(s)$**: $p(s=0) = p(s=0|y=0)p(y=0) + p(s=0|y=1)p(y=1) = 0.9 \\times 0.5 + 0.1 \\times 0.5 = 0.5$. Similarly, $p(s=1) = 0.5$. So, $p(s) = [0.5, 0.5]$.\n2.  **Entropies**:\n    - $H(S) = -(0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = 1.0$ bit.\n    - $H(N) = \\log_2(8) = 3.0$ bits, since it's a uniform distribution over $8$ states.\n3.  **InfoMax Selection**: Since $H(N) = 3.0  H(S) = 1.0$, InfoMax selects the noise encoder $Z_N$.\n4.  **Task Utility**: We calculate $I(S;Y)$. Using $I(S;Y) = H(S) - H(S|Y)$, we have $H(S)=1.0$. The conditional entropy is $H(S|Y) = \\sum_y p(y) H(S|Y=y)$. Both $H(S|Y=0)$ and $H(S|Y=1)$ are the entropy of a $[0.9, 0.1]$ distribution, which is $-(0.9\\log_2 0.9 + 0.1\\log_2 0.1) \\approx 0.469$ bits. So, $H(S|Y) \\approx 0.469$ bits. This gives $I(S;Y) \\approx 1.0 - 0.469 = 0.531$ bits. $I(N;Y)=0$.\n5.  **Classification**: InfoMax selects $Z_N$, and $I(S;Y)  I(N;Y)$. This is a failure case. **Result: $0$**.\n\n**Case B: High-entropy signal**\n- $p(y) = [0.8, 0.2]$.\n- $p(s \\mid y)$ represents a deterministic mapping $S=Y$. Thus, $p(s) = p(y) = [0.8, 0.2]$.\n- $p(n) = [0.95, 0.05]$.\n1.  **Entropies**:\n    - $H(S) = -(0.8 \\log_2 0.8 + 0.2 \\log_2 0.2) \\approx 0.722$ bits.\n    - $H(N) = -(0.95 \\log_2 0.95 + 0.05 \\log_2 0.05) \\approx 0.286$ bits.\n2.  **InfoMax Selection**: Since $H(S) \\approx 0.722  H(N) \\approx 0.286$, InfoMax selects the signal encoder $Z_S$.\n3.  **Task Utility**: $I(S;Y) = H(S) \\approx 0.722$ bits (since $S=Y$), and $I(N;Y) = 0$.\n4.  **Classification**: InfoMax selects $Z_S$, and $I(S;Y) \\ge I(N;Y)$. This is a success case. **Result: $1$**.\n\n**Case C: Equal entropies**\n- $p(y) = [0.5, 0.5]$.\n- $p(s \\mid y)$ represents a deterministic mapping $S=Y$. Thus, $p(s) = p(y) = [0.5, 0.5]$.\n- $p(n) = [0.5, 0.5]$.\n1.  **Entropies**:\n    - $H(S) = -(0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = 1.0$ bit.\n    - $H(N) = -(0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = 1.0$ bit.\n2.  **InfoMax Selection**: $H(S) = H(N) = 1.0$. The difference is $0$, which is within the tolerance $\\tau=10^{-12}$. This is a tie.\n3.  **Classification**: The selection is ambiguous due to the tie. **Result: $2$**.\n\n**Case D: Extreme high-entropy nuisance**\n- $p(y) = [0.1, 0.9]$.\n- $p(s \\mid y)$ represents a deterministic mapping $S=Y$. Thus, $p(s) = p(y) = [0.1, 0.9]$.\n- $p(n)$ is uniform over $16$ states ($p(n_i)=1/16$).\n1.  **Entropies**:\n    - $H(S) = -(0.1 \\log_2 0.1 + 0.9 \\log_2 0.9) \\approx 0.469$ bits.\n    - $H(N) = \\log_2(16) = 4.0$ bits.\n2.  **InfoMax Selection**: Since $H(N) = 4.0  H(S) \\approx 0.469$, InfoMax selects the noise encoder $Z_N$.\n3.  **Task Utility**: $I(S;Y) = H(S) \\approx 0.469$ bits, and $I(N;Y) = 0$.\n4.  **Classification**: InfoMax selects $Z_N$, and $I(S;Y)  I(N;Y)$. This is another failure case. **Result: $0$**.\n\nThe sequence of results for cases A, B, C, and D is $[0, 1, 2, 0]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes a counterexample to Information Maximization (InfoMax) by\n    analyzing four test cases.\n    \"\"\"\n\n    def entropy(p):\n        \"\"\"Computes the entropy H(p) for a probability distribution p.\"\"\"\n        p = np.asarray(p)\n        p_nonzero = p[p  0]\n        if p_nonzero.size == 0:\n            return 0.0\n        return -np.sum(p_nonzero * np.log2(p_nonzero))\n\n    def mutual_information(p_xy):\n        \"\"\"Computes the mutual information I(X;Y) from a joint distribution p(x,y).\"\"\"\n        p_xy = np.asarray(p_xy)\n        if p_xy.sum() == 0:\n            return 0.0\n\n        p_x = np.sum(p_xy, axis=1)\n        p_y = np.sum(p_xy, axis=0)\n\n        h_x = entropy(p_x)\n        h_y = entropy(p_y)\n        h_xy = entropy(p_xy.flatten())\n        \n        # Clamp mi to be non-negative due to potential floating point inaccuracies\n        mi = max(0.0, h_x + h_y - h_xy)\n        return mi\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: happy-path failure\n        {\n            \"p_y\": [0.5, 0.5],\n            \"p_s_given_y\": [[0.9, 0.1], [0.1, 0.9]],\n            \"p_n\": [1/8] * 8\n        },\n        # Case B: success\n        {\n            \"p_y\": [0.8, 0.2],\n            \"p_s_given_y\": [[1.0, 0.0], [0.0, 1.0]],\n            \"p_n\": [0.95, 0.05]\n        },\n        # Case C: boundary tie\n        {\n            \"p_y\": [0.5, 0.5],\n            \"p_s_given_y\": [[1.0, 0.0], [0.0, 1.0]],\n            \"p_n\": [0.5, 0.5]\n        },\n        # Case D: edge case failure\n        {\n            \"p_y\": [0.1, 0.9],\n            \"p_s_given_y\": [[1.0, 0.0], [0.0, 1.0]],\n            \"p_n\": [1/16] * 16\n        }\n    ]\n\n    results = []\n    tau = 1e-12\n\n    for case in test_cases:\n        p_y = np.array(case[\"p_y\"])\n        p_s_given_y = np.array(case[\"p_s_given_y\"])\n        p_n = np.array(case[\"p_n\"])\n\n        # 1. Compute joint distribution p(s,y) and marginal p(s)\n        p_sy = p_s_given_y.T * p_y  # Broadcasting p_y along columns, but need to transpose p_s_given_y\n        p_s = np.sum(p_sy, axis=1)\n\n        # 2. Compute H(S) and H(N)\n        H_S = entropy(p_s)\n        H_N = entropy(p_n)\n\n        # 3. Compute I(S;Y) and I(N;Y)\n        I_SY = mutual_information(p_sy)\n        I_NY = 0.0  # N is independent of Y by definition\n\n        # 4.  5. Decide which encoder InfoMax selects and classify the outcome\n        classification = -1 # Default/error value\n\n        if abs(H_N - H_S) = tau:\n            # Tie within tolerance\n            classification = 2\n        elif H_N  H_S:\n            # InfoMax selects Z_N (noise encoder)\n            if I_NY  I_SY:\n                # Failure: nuisance chosen over signal\n                classification = 0\n            else:\n                # This case is not expected under the problem's setup,\n                # as I(S;Y) should be  0.\n                # However, if it happened, it wouldn't be a \"failure\".\n                # A success case would be triggered here if I_NY >= I_SY\n                # per the logic in the H_S > H_N branch.\n                classification = 1\n        elif H_S  H_N:\n            # InfoMax selects Z_S (signal encoder)\n            if I_SY = I_NY:\n                # Success: signal chosen and it is at least as informative\n                classification = 1\n            else:\n                # This case is impossible since I(S;Y) = 0 and I(N;Y) = 0.\n                pass\n        \n        results.append(classification)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}