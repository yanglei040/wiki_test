{
    "hands_on_practices": [
        {
            "introduction": "分布式假说可以通过两种基本方式进行操作化：从中心词预测上下文（Skip-gram），或从上下文预测中心词（遮蔽语言模型，MLM）。本练习旨在通过构建这两种模型的简化计数版本，让您亲身体验这些基础建模选择如何塑造词向量的几何特性。通过直接比较从 $p(c|w)$ 和 $p(w|c)$ 派生出的嵌入向量，您将深入理解不同目标函数对语义相似性度量的具体影响 。",
            "id": "3182958",
            "problem": "给定一个固定的、小型的语料库，要求您使用基于原则的计数估计量，在两个经典的深度学习目标——掩码语言模型（MLM）和跳字模型（SG）中操作化分布假说，并比较由此产生的词嵌入的几何结构。分布假说指出，出现在相似上下文中的词语往往具有相似的含义。您的任务是从核心的概率定义和经过充分检验的统计程序出发，推导、实现和比较条件分布以及所引出的向量空间几何。\n\n语料库（已分词，全小写，已移除标点符号；每行是一个句子）：\nthe cat sat on the mat\nthe dog sat on the rug\na cat chased a mouse\na dog chased a ball\nmusic and song fill the hall\nthe song played music softly\nquantum theory explains physics\ntheory of music and physics\nthe quantum cat thought of physics\ndogs and cats share a home\na theory about a song\n\n用作基础的基本定义：\n- 条件概率：对于事件 $A$ 和 $B$，当 $p(B) > 0$ 时，$p(A \\mid B) = \\frac{p(A, B)}{p(B)}$。\n- 多项分布的最大似然估计（MLE）：给定结果 $\\{i\\}$ 上的计数 $\\{n_i\\}$，$\\hat{p}_i = \\frac{n_i}{\\sum_j n_j}$。\n- 加性（拉普拉斯）平滑：对于 $V$ 个结果上的计数 $\\{n_i\\}$ 和平滑参数 $\\alpha > 0$，定义 $\\tilde{p}_i = \\frac{n_i + \\alpha}{\\sum_j n_j + \\alpha V}$。\n- 余弦相似度：对于 $\\mathbb{R}^d$ 中的向量 $\\mathbf{u}$ 和 $\\mathbf{v}$，$\\mathrm{cos}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}^\\top \\mathbf{v}}{\\lVert \\mathbf{u} \\rVert_2 \\lVert \\mathbf{v} \\rVert_2}$。\n\n要执行的任务：\n1. 在语料库中，围绕每个目标词 $w$ 构建一个大小为 $K$ 的对称上下文窗口。对于位置 $i$ 处的每次出现，将所有满足 $|i - j| \\le K$ 且 $j \\ne i$ 的位置 $j$ 处的词作为上下文词 $c$。在整个语料库上累积共现计数 $N(w, c)$。使用整个词元词汇表作为可能的 $w$ 和 $c$ 的集合。\n2. 定义掩码语言模型（MLM）：从 $c$ 预测 $w$。根据计数 $N(w, c)$，使用带有加性平滑参数 $\\alpha$ 的最大似然估计，推导出与上述定义一致的条件分布 $p_{\\mathrm{MLM}}(w \\mid c)$。\n3. 定义跳字模型（SG）：从 $w$ 预测 $c$。根据相同的计数 $N(w, c)$，使用带有加性平滑参数 $\\alpha$ 的最大似然估计，推导出与上述定义一致的 $p_{\\mathrm{SG}}(c \\mid w)$。\n4. 将每个词 $w$ 嵌入到两个空间中：\n   - MLM 嵌入 $\\mathbf{v}_{\\mathrm{MLM}}(w)$：将 $p_{\\mathrm{MLM}}(w \\mid c)$ 解释为 $c$ 的函数，并创建一个其分量由上下文 $c$ 索引的向量。\n   - SG 嵌入 $\\mathbf{v}_{\\mathrm{SG}}(w)$：将 $p_{\\mathrm{SG}}(c \\mid w)$ 解释为 $c$ 的函数，并创建一个其分量由上下文 $c$ 索引的向量。\n   确保两种嵌入具有相同的维度，等于词汇表大小 $V$，并按词汇表的固定、确定性顺序排列。\n5. 对于下面的每个测试用例，计算两个指定词在两种几何结构下的余弦相似度，即计算 $\\mathrm{cos}(\\mathbf{v}_{\\mathrm{MLM}}(w_1), \\mathbf{v}_{\\mathrm{MLM}}(w_2))$ 和 $\\mathrm{cos}(\\mathbf{v}_{\\mathrm{SG}}(w_1), \\mathbf{v}_{\\mathrm{SG}}(w_2))$，并报告它们的差值 $\\Delta = \\mathrm{cos}_{\\mathrm{SG}} - \\mathrm{cos}_{\\mathrm{MLM}}$。\n6. 根据测试套件中的指定，使用参数为 $\\alpha > 0$ 的加性平滑和大小为 $K \\in \\mathbb{N}$ 的对称窗口。不需要角度；仅报告作为实数的余弦相似度。\n\n测试套件（每个用例的形式为 $(K, \\alpha, w_1, w_2)$）：\n- 用例 1：$K = 2$，$\\alpha = 0.5$，$w_1 = \\text{\"cat\"}$，$w_2 = \\text{\"dog\"}$。\n- 用例 2：$K = 1$，$\\alpha = 1.0$，$w_1 = \\text{\"music\"}$，$w_2 = \\text{\"song\"}$。\n- 用例 3：$K = 3$，$\\alpha = 10^{-6}$，$w_1 = \\text{\"the\"}$，$w_2 = \\text{\"a\"}$。\n- 用例 4：$K = 2$，$\\alpha = 0.2$，$w_1 = \\text{\"quantum\"}$，$w_2 = \\text{\"theory\"}$。\n\n要求的最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例贡献一个包含三个浮点数的子列表，顺序为 $[\\mathrm{cos}_{\\mathrm{MLM}}, \\mathrm{cos}_{\\mathrm{SG}}, \\Delta]$。例如，一个包含两个假设用例的输出应如下所示：\"[[0.123456,0.234567,0.111111],[0.222222,0.333333,0.111111]]\"。不应打印任何额外的文本。\n\n科学真实性和约束条件：\n- 仅使用提供的语料库、对称窗口和加性平滑来估计条件分布。\n- 确保词汇表是语料库中所有唯一词元的集合，并且上下文是来自同一集合的词元。\n- 使用确定性的分词和词汇表排序，以便向量在不同测试用例之间具有可比性。\n- 所有报告的值都必须是实数，不带物理单位。",
            "solution": "根据指定标准对问题进行验证。\n\n### 步骤 1：提取给定信息\n- **语料库**：一个包含11个已分词句子的固定集合。\n  - `the cat sat on the mat`\n  - `the dog sat on the rug`\n  - `a cat chased a mouse`\n  - `a dog chased a ball`\n  - `music and song fill the hall`\n  - `the song played music softly`\n  - `quantum theory explains physics`\n  - `theory of music and physics`\n  - `the quantum cat thought of physics`\n  - `dogs and cats share a home`\n  - `a theory about a song`\n- **基本定义**:\n  - 条件概率：对于 $p(B) > 0$，$p(A \\mid B) = \\frac{p(A, B)}{p(B)}$。\n  - 多项分布的最大似然估计：$\\hat{p}_i = \\frac{n_i}{\\sum_j n_j}$。\n  - 加性（拉普拉斯）平滑：$\\tilde{p}_i = \\frac{n_i + \\alpha}{\\sum_j n_j + \\alpha V}$，其中平滑参数 $\\alpha > 0$，有 $V$ 个结果。\n  - 余弦相似度：$\\mathrm{cos}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}^\\top \\mathbf{v}}{\\lVert \\mathbf{u} \\rVert_2 \\lVert \\mathbf{v} \\rVert_2}$。\n- **任务**:\n  1. 使用大小为 $K$ 的对称上下文窗口构建共现计数矩阵 $N(w, c)$。\n  2. 定义并使用最大似然估计和加性平滑推导掩码语言模型（预测目标词 $w$ 来自上下文词 $c$）的条件分布 $p_{\\mathrm{MLM}}(w \\mid c)$。\n  3. 定义并使用最大似然估计和加性平滑推导跳字模型（预测上下文词 $c$ 来自目标词 $w$）的条件分布 $p_{\\mathrm{SG}}(c \\mid w)$。\n  4. 为每个词 $w$ 构建嵌入向量 $\\mathbf{v}_{\\mathrm{MLM}}(w)$ 和 $\\mathbf{v}_{\\mathrm{SG}}(w)$，其中向量分量由上下文词 $c$ 索引。\n  5. 计算 $\\mathrm{cos}_{\\mathrm{MLM}} = \\mathrm{cos}(\\mathbf{v}_{\\mathrm{MLM}}(w_1), \\mathbf{v}_{\\mathrm{MLM}}(w_2))$、$\\mathrm{cos}_{\\mathrm{SG}} = \\mathrm{cos}(\\mathbf{v}_{\\mathrm{SG}}(w_1), \\mathbf{v}_{\\mathrm{SG}}(w_2))$ 以及差值 $\\Delta = \\mathrm{cos}_{\\mathrm{SG}} - \\mathrm{cos}_{\\mathrm{MLM}}$。\n  6. 对每个测试用例使用指定的参数 $K$ 和 $\\alpha$。\n- **测试套件**:\n  - 用例 1: $(K=2, \\alpha=0.5, w_1=\\text{\"cat\"}, w_2=\\text{\"dog\"})$\n  - 用例 2: $(K=1, \\alpha=1.0, w_1=\\text{\"music\"}, w_2=\\text{\"song\"})$\n  - 用例 3: $(K=3, \\alpha=10^{-6}, w_1=\\text{\"the\"}, w_2=\\text{\"a\"})$\n  - 用例 4: $(K=2, \\alpha=0.2, w_1=\\text{\"quantum\"}, w_2=\\text{\"theory\"})$\n- **输出格式**：单行字符串 `[[cos_mlm1,cos_sg1,delta1],[cos_mlm2,cos_sg2,delta2],...]`。\n\n### 步骤 2：使用提取的给定信息进行验证\n- **科学依据**：该问题在计算语言学和机器学习的原理上有充分的依据。分布假说、掩码语言模型（MLM）、跳字模型（SG）、最大似然估计（MLE）、加性平滑和余弦相似度都是标准的、成熟的概念。该任务使用具体的、基于原则的计数方法来操作化这些抽象概念，这是一项有效且富有洞察力的练习。\n- **适定性**：该问题提供了具体的语料库、明确的定义和清晰的算法流程。每个测试用例的所有参数（$K, \\alpha$）和目标词（$w_1, w_2$）都已指定。所需的输出由输入和规定的方法唯一确定。\n- **客观性**：该问题使用精确、无歧义的数学和程序性语言进行陈述。没有主观或基于意见的元素。\n\n### 步骤 3：结论与行动\n该问题是有效的。它在科学上是合理的，问题是适定的，并且是客观的。将提供一个完整、合理的解决方案。\n\n### 解决方案推导\n\n该解决方案通过从共现统计数据构建词嵌入，并比较它们在两种不同概率模型——掩码语言模型（MLM）和跳字模型（SG）——下的几何特性，从而将分布假说付诸实践。\n\n**1. 词汇表和共现矩阵的构建**\n\n首先，我们通过收集所提供语料库中的所有唯一词元并按字母顺序排序，建立一个固定的词汇表 $\\mathcal{V}$，以确保确定性的排序。设 $V = |\\mathcal{V}|$ 为词汇表的大小。为方便计算，我们创建一个词到索引的映射。\n\n对于给定的对称窗口大小 $K$，我们填充一个 $V \\times V$ 的共现矩阵 $N$。一个条目 $N_{ij}$ 存储计数 $N(w_i, c_j)$，表示词 $c_j$ 出现在目标词 $w_i$ 的上下文中的次数。我们遍历每个句子中位于位置 $p$ 的每个词 $w_i$，并对在满足 $1 \\le |p - q| \\le K$ 的位置 $q$ 上找到的所有上下文词 $c_j$ 增加计数。\n\n**2. 跳字模型（SG）与嵌入**\n\n跳字模型旨在给定一个目标词 $w$ 来预测上下文词 $c$。这对应于估计条件概率 $p_{\\mathrm{SG}}(c \\mid w)$。\n遵循对我们的计数应用最大似然估计（MLE）的原则，给定目标词 $w_i$，观测到特定上下文词 $c_j$ 的概率由它们的共现计数与 $w_i$ 的所有上下文词的总计数之比来估计。\n为目标词 $w_i$ 观测到的上下文词总数是矩阵 $N$ 第 $i$ 行的总和：$S_i^{\\text{row}} = \\sum_{k=1}^V N_{ik}$。\n应用带有参数 $\\alpha$ 的加性平滑来处理数据稀疏性并避免零概率，条件概率为：\n$$p_{\\mathrm{SG}}(c_j \\mid w_i) = \\frac{N_{ij} + \\alpha}{S_i^{\\text{row}} + \\alpha V}$$\n一个词 $w_i$ 的跳字模型嵌入，记为 $\\mathbf{v}_{\\mathrm{SG}}(w_i)$，是一个 $V$ 维向量，其分量是这些概率，由上下文词 $c_j \\in \\mathcal{V}$ 索引：\n$$\\mathbf{v}_{\\mathrm{SG}}(w_i) = [p_{\\mathrm{SG}}(c_1 \\mid w_i), p_{\\mathrm{SG}}(c_2 \\mid w_i), \\ldots, p_{\\mathrm{SG}}(c_V \\mid w_i)]^\\top$$\n该向量对应于平滑条件概率矩阵 $P_{\\mathrm{SG}}$ 的第 $i$ 行，其中 $(P_{\\mathrm{SG}})_{ij} = p_{\\mathrm{SG}}(c_j \\mid w_i)$。\n\n**3. 掩码语言模型（MLM）与嵌入**\n\n在此背景下，掩码语言模型的目标是给定一个上下文词 $c$ 来预测目标词 $w$。这需要估计条件概率 $p_{\\mathrm{MLM}}(w \\mid c)$。\n与 SG 模型类似，我们使用共现计数。对于给定的上下文词 $c_j$，与之共现的目标词 $w_i$ 的计数是 $N_{ij}$。$c_j$ 作为任何目标词的上下文词出现的总次数是矩阵 $N$ 第 $j$ 列的总和：$S_j^{\\text{col}} = \\sum_{k=1}^V N_{kj}$。\n应用加性平滑，条件概率为：\n$$p_{\\mathrm{MLM}}(w_i \\mid c_j) = \\frac{N_{ij} + \\alpha}{S_j^{\\text{col}} + \\alpha V}$$\n一个词 $w_i$ 的 MLM 嵌入，记为 $\\mathbf{v}_{\\mathrm{MLM}}(w_i)$，是一个 $V$ 维向量。根据问题描述，其分量由上下文词 $c_j$ 索引。因此，该向量的第 $j$ 个分量是 $p_{\\mathrm{MLM}}(w_i \\mid c_j)$。\n$$\\mathbf{v}_{\\mathrm{MLM}}(w_i) = [p_{\\mathrm{MLM}}(w_i \\mid c_1), p_{\\mathrm{MLM}}(w_i \\mid c_2), \\ldots, p_{\\mathrm{MLM}}(w_i \\mid c_V)]^\\top$$\n该向量对应于矩阵 $P_{\\mathrm{MLM}}$ 的第 $i$ 行，其中 $(P_{\\mathrm{MLM}})_{ij} = p_{\\mathrm{MLM}}(w_i \\mid c_j)$。\n\n**4. 通过余弦相似度进行几何比较**\n\n为了比较两种模型引出的几何结构，我们计算两个词 $w_1$ 和 $w_2$ 的嵌入向量之间的余弦相似度。余弦相似度提供了一个衡量两个向量之间夹角的度量，从而衡量它们基于方向的相似性，这是量化向量空间中语义相似性的常用方法。\n\n对于给定的词对 $(w_1, w_2)$，我们计算：\n- MLM 相似度: $\\mathrm{cos}_{\\mathrm{MLM}} = \\mathrm{cos}(\\mathbf{v}_{\\mathrm{MLM}}(w_1), \\mathbf{v}_{\\mathrm{MLM}}(w_2)) = \\frac{\\mathbf{v}_{\\mathrm{MLM}}(w_1)^\\top \\mathbf{v}_{\\mathrm{MLM}}(w_2)}{\\lVert \\mathbf{v}_{\\mathrm{MLM}}(w_1) \\rVert_2 \\lVert \\mathbf{v}_{\\mathrm{MLM}}(w_2) \\rVert_2}$\n- SG 相似度: $\\mathrm{cos}_{\\mathrm{SG}} = \\mathrm{cos}(\\mathbf{v}_{\\mathrm{SG}}(w_1), \\mathbf{v}_{\\mathrm{SG}}(w_2)) = \\frac{\\mathbf{v}_{\\mathrm{SG}}(w_1)^\\top \\mathbf{v}_{\\mathrm{SG}}(w_2)}{\\lVert \\mathbf{v}_{\\mathrm{SG}}(w_1) \\rVert_2 \\lVert \\mathbf{v}_{\\mathrm{SG}}(w_2) \\rVert_2}$\n\n最后，我们计算差值 $\\Delta = \\mathrm{cos}_{\\mathrm{SG}} - \\mathrm{cos}_{\\mathrm{MLM}}$，以直接比较两种模型为每个测试用例生成的相似性度量。对测试套件中的每组参数 $(K, \\alpha, w_1, w_2)$ 重复此整个过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the word embedding comparison problem by following these steps:\n    1. Processes a fixed corpus to build a vocabulary.\n    2. For each test case:\n        a. Constructs a word-context co-occurrence matrix `N` for a given window size `K`.\n        b. Derives conditional probability matrices `P_MLM` and `P_SG` using additive smoothing.\n        c. Extracts word embedding vectors for the specified words from these matrices.\n        d. Computes the cosine similarity between the word vectors in both embedding spaces (MLM and SG).\n        e. Calculates the difference between the two similarities.\n    3. Formats and prints the final results as specified.\n    \"\"\"\n    corpus = [\n        \"the cat sat on the mat\",\n        \"the dog sat on the rug\",\n        \"a cat chased a mouse\",\n        \"a dog chased a ball\",\n        \"music and song fill the hall\",\n        \"the song played music softly\",\n        \"quantum theory explains physics\",\n        \"theory of music and physics\",\n        \"the quantum cat thought of physics\",\n        \"dogs and cats share a home\",\n        \"a theory about a song\"\n    ]\n\n    test_cases = [\n        (2, 0.5, \"cat\", \"dog\"),\n        (1, 1.0, \"music\", \"song\"),\n        (3, 1e-6, \"the\", \"a\"),\n        (2, 0.2, \"quantum\", \"theory\"),\n    ]\n\n    # Pre-processing: tokenize corpus and build a deterministic vocabulary\n    corpus_tokens = [line.split() for line in corpus]\n    \n    all_tokens = set()\n    for sentence in corpus_tokens:\n        all_tokens.update(sentence)\n    \n    vocabulary = sorted(list(all_tokens))\n    V = len(vocabulary)\n    word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n\n    final_results = []\n    cooccurrence_matrices_cache = {}\n\n    for K, alpha, w1_str, w2_str in test_cases:\n        \n        # Task 1: Construct co-occurrence count matrix N(w, c)\n        # We cache the matrix as it can be expensive to recompute.\n        if K not in cooccurrence_matrices_cache:\n            N = np.zeros((V, V), dtype=np.float64)\n            for sentence in corpus_tokens:\n                for i, target_word in enumerate(sentence):\n                    target_idx = word_to_idx[target_word]\n                    \n                    start = max(0, i - K)\n                    end = min(len(sentence), i + K + 1)\n                    \n                    for j in range(start, end):\n                        if i == j:\n                            continue\n                        context_word = sentence[j]\n                        context_idx = word_to_idx[context_word]\n                        N[target_idx, context_idx] += 1\n            cooccurrence_matrices_cache[K] = N\n        \n        N = cooccurrence_matrices_cache[K]\n            \n        w1_idx = word_to_idx[w1_str]\n        w2_idx = word_to_idx[w2_str]\n\n        # Task 2  4: Derive P_MLM(w|c) and MLM embeddings\n        # P_MLM(w_i | c_j) = (N[i, j] + alpha) / (sum_k N[k, j] + alpha * V)\n        col_sums = N.sum(axis=0)\n        P_mlm_denom = col_sums + alpha * V\n        P_mlm = (N + alpha) / P_mlm_denom[np.newaxis, :]\n        \n        v_mlm_w1 = P_mlm[w1_idx, :]\n        v_mlm_w2 = P_mlm[w2_idx, :]\n        \n        # Task 3  4: Derive P_SG(c|w) and SG embeddings\n        # P_SG(c_j | w_i) = (N[i, j] + alpha) / (sum_k N[i, k] + alpha * V)\n        row_sums = N.sum(axis=1)\n        # Use [:, np.newaxis] to ensure correct broadcasting for row-wise division\n        P_sg_denom = row_sums + alpha * V\n        P_sg = (N + alpha) / P_sg_denom[:, np.newaxis]\n        \n        v_sg_w1 = P_sg[w1_idx, :]\n        v_sg_w2 = P_sg[w2_idx, :]\n\n        # Task 5: Compute cosine similarities\n        def cosine_similarity(u, v):\n            dot_product = np.dot(u, v)\n            norm_u = np.linalg.norm(u)\n            norm_v = np.linalg.norm(v)\n            # Denominator is guaranteed non-zero due to alpha > 0 smoothing\n            return dot_product / (norm_u * norm_v)\n\n        cos_mlm = cosine_similarity(v_mlm_w1, v_mlm_w2)\n        cos_sg = cosine_similarity(v_sg_w1, v_sg_w2)\n        \n        # Task 5: Compute difference\n        delta = cos_sg - cos_mlm\n        \n        final_results.append([cos_mlm, cos_sg, delta])\n        \n    # Final print statement in the exact required format.\n    # e.g., \"[[0.123456,0.234567,0.111111],[0.222222,0.333333,0.111111]]\"\n    output_str = \"[\" + \",\".join([f\"[{c1:.6f},{c2:.6f},{d:.6f}]\" for c1, c2, d in final_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "虽然Skip-gram模型在概念上（即建模 $p(c|w)$）很直观，但其直接计算在计算上是昂贵的。本练习探讨了在实践中广泛使用的负采样（Negative Sampling）优化技术，并揭示了它如何隐式地分解一个经过平移的点互信息（PMI）矩阵。通过这个实践，您将研究负采样的超参数（如负样本数量 $k$ 和分布平滑度 $\\alpha$）是如何改变这个隐式矩阵的谱特性，从而更深刻地理解 word2vec 的工作原理 。",
            "id": "3182845",
            "problem": "构建一个程序，该程序通过从一个小语料库中构建一个词-上下文共现矩阵来实践分布假说 (Distributional Hypothesis)，然后分析负采样分布的选择和负采样数量 (multiplicity) 如何影响带负采样的 Skip-Gram (SGNS) 模型所隐式分解的矩阵。从以下基本基础开始：分布假说断言，出现在相似上下文中的词倾向于有相似的含义；共现频率定义了经验联合概率和边缘概率；一个词和一个上下文之间的点互信息 (Pointwise Mutual Information, PMI) 是根据这些概率定义的；SGNS 对观测到的和负采样的词对优化一个逻辑目标函数，这产生一个等于偏移 PMI 的最优内积，该偏移 PMI 取决于负采样分布和负样本的数量。除了这些基础知识，不要使用任何快捷公式。\n\n您的程序必须以纯数学和逻辑的方式实现以下步骤：\n\n1.  通过空白符分词构建一个有限语料库。使用以下四个小写句子作为整个语料库：\n   - \"the quick brown fox jumps over the lazy dog\"\n   - \"the quick blue hare jumps over the sleepy dog\"\n   - \"a fast brown fox leaps over a lazy hound\"\n   - \"the slow tortoise crawls under the lazy dog\"\n2.  通过在每个句子上滑动一个半径为 $r = 2$ 的窗口来构建一个对称的词-上下文共现矩阵。对于位置 $t$ 处的词元 $w_t$，以及所有满足 $t - r \\le j \\le t + r$ 且 $j \\ne t$ 的位置 $j$，将词对 $(w_t, w_j)$ 的计数增加 $1$。设得到的计数矩阵为 $C \\in \\mathbb{R}^{V \\times V}$，其中 $V$ 是词汇量大小，行索引词，列索引上下文。\n3.  使用加法平滑将计数转换为概率以避免未定义的对数。通过向 $C$ 的每个条目加上 $\\tau = 10^{-3}$ 来定义一个平滑矩阵 $\\tilde{C}$。设词对总数为 $N = \\sum_{i=1}^{V} \\sum_{j=1}^{V} \\tilde{C}_{ij}$。定义联合概率 $P(w_i, c_j) = \\tilde{C}_{ij} / N$，词的边缘概率 $P(w_i) = \\sum_{j=1}^{V} \\tilde{C}_{ij} / N$，以及上下文的边缘概率 $P(c_j) = \\sum_{i=1}^{V} \\tilde{C}_{ij} / N$。\n4.  计算点互信息矩阵 $M$，其条目为\n   $$M_{ij} = \\log \\frac{P(w_i, c_j)}{P(w_i) P(c_j)},$$\n   使用自然对数。\n5.  对于负采样，考虑两种上下文上的负分布族：\n   - 均匀分布 (Uniform)：$Q_{\\text{uni}}(c_j) = 1 / V$。\n   - 一元语法 (Unigram) 的 $\\alpha$ 次幂：$Q_{\\alpha}(c_j) = \\frac{P(c_j)^{\\alpha}}{\\sum_{\\ell=1}^{V} P(c_{\\ell})^{\\alpha}}$，对于给定的 $\\alpha \\in [0, 1]$。\n6.  考虑负采样数量 $k \\in \\mathbb{N}$。通过分析逻辑目标函数的最优性条件，其中正样本从 $P(w,c)$ 中抽取，负样本从 $P(w) Q(c)$ 中抽取（每个正样本对应 $k$ 个负样本），SGNS 的最优内积采用偏移 PMI 的形式：\n   $$S_{ij}(k, Q) = M_{ij} - \\log k - \\log \\frac{Q(c_j)}{P(c_j)}.$$\n   严格地将此关系作为优化条件的结果，为指定的 $(k, Q)$ 构建偏移矩阵 $S(k,Q)$。\n7.  对于每个指定的配置，通过奇异值分解计算 $S(k,Q)$ 的奇异值。设 $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_V$ 表示奇异值。提取前两个奇异值 $\\sigma_1$ 和 $\\sigma_2$。\n8.  为确保数值可复现性，请使用指定的精确语料库、窗口半径 $r = 2$、平滑参数 $\\tau = 10^{-3}$ 以及自然对数。\n\n测试套件和要求的输出：\n- 使用以下五个测试配置，每个配置由一对 $(k, \\alpha)$ 以及声明的负采样分布指定：\n  1.  均匀分布，其中 $k = 1$。\n  2.  均匀分布，其中 $k = 20$。\n  3.  一元语法的次幂分布，其中 $\\alpha = 0.75$ 且 $k = 1$。\n  4.  一元语法的次幂分布，其中 $\\alpha = 0.75$ 且 $k = 20$。\n  5.  一元语法的次幂分布，其中 $\\alpha = 1.0$ 且 $k = 5$。\n- 对于每个配置，计算并报告对 $[\\sigma_1, \\sigma_2]$，四舍五入到小数点后恰好 $6$ 位。\n\n最终输出格式：\n- 您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表，其中每个元素本身也是一个相同格式的、包含两个元素的列表，且不含空格。例如，最外层结构必须看起来像\n  $$\\big[ [a_1, a_2], [b_1, b_2], \\dots \\big],$$\n  但每个浮点数必须精确打印小数点后 $6$ 位数字，并保留所有逗号，同时删除所有空格。具体来说，打印的行必须具有以下形式\n  \"[[x11,x12],[x21,x22],[x31,x32],[x41,x42],[x51,x52]]\"。",
            "solution": "目标是构建一个程序，该程序模拟词的分布语义，并分析带负采样的 Skip-Gram (SGNS) 模型中超参数的影响。这是通过实践分布假说 (Distributional Hypothesis) 来实现的，该假说主张一个词的意义由其出现的上下文决定。该过程从语料库分析开始，最终从 SGNS 优化目标派生出的隐式矩阵中提取特征。\n\n首先，我们从给定的语料库中建立一个词汇表并量化词-上下文关系。该语料库由四个句子组成，通过空白符进行分词。词汇表是所有唯一词元的集合，其大小为 $V$。为了捕捉分布特性，我们构建一个词-上下文共现矩阵 $C \\in \\mathbb{R}^{V \\times V}$，其行和列都由词汇表索引。该矩阵通过在每个句子上滑动一个指定半径 $r=2$ 的窗口来填充。对于位置 $t$ 的每个词 $w_t$，其与附近位置 $j$（其中 $t - r \\le j \\le t + r$ 且 $j \\neq t$）的上下文词 $w_j$ 的共现计数 $C_{ij}$ 增加 $1$。这个过程自然地产生一个对称矩阵 ($C = C^T$)，因为共现关系是对称的。\n\n其次，我们从原始计数过渡到一个概率框架。为了避免零计数问题（这会导致后续的对数未定义），我们采用加法平滑。将一个小常数 $\\tau = 10^{-3}$ 添加到计数矩阵 $C$ 的每个条目中，以生成一个平滑矩阵 $\\tilde{C}$。平滑后的共现词对总数为 $N = \\sum_{i=1}^{V} \\sum_{j=1}^{V} \\tilde{C}_{ij}$。由此，我们定义经验联合概率分布 $P(w_i, c_j) = \\tilde{C}_{ij} / N$。然后，通过对联合概率矩阵的适当维度求和，可以导出词的边缘概率 $P(w_i) = \\sum_{j=1}^{V} P(w_i, c_j)$ 和上下文的边缘概率 $P(c_j) = \\sum_{i=1}^{V} P(w_i, c_j)$。\n\n第三，我们计算点互信息 (PMI) 矩阵 $M$。条目 $M_{ij}$ 量化了特定词 $w_i$ 和上下文 $c_j$ 之间的关联度。其定义如下：\n$$M_{ij} = \\log \\frac{P(w_i, c_j)}{P(w_i) P(c_j)}$$\n正的 PMI 值表示该词和上下文的共现频率高于它们独立时的预期频率，这表明存在语义相关性。我们使用自然对数进行此计算。在数值上，这被实现为 $M_{ij} = \\log P(w_i, c_j) - \\log P(w_i) - \\log P(c_j)$ 以保持稳定性。\n\n第四，我们分析 SGNS 模型。一个关键的理论结果表明，SGNS 并不显式地构建和分解 PMI 矩阵，但其优化目标隐式地使学习到的词向量和上下文向量满足一个特定关系。词 $w_i$ 的词向量和上下文 $c_j$ 的上下文向量之间的最优内积等价于它们 PMI 的一个偏移版本。这产生了 SGNS 矩阵 $S(k, Q)$，其条目为：\n$$S_{ij}(k, Q) = M_{ij} - \\log k - \\log \\frac{Q(c_j)}{P(c_j)}$$\n在这里，$k$ 是每个正样本的负样本数，$Q(c)$ 是抽取这些负上下文的概率分布。这个方程揭示了 $k$ 和 $Q$ 的选择如何系统地改变正在学习的语义空间。\n\n该程序评估两种类型的负采样分布 $Q$。第一种是均匀分布，$Q_{\\text{uni}}(c_j) = 1/V$，它将所有词视为同等可能的负样本。第二种是基于一元语法的分布，$Q_{\\alpha}(c_j) = P(c_j)^{\\alpha} / \\sum_{\\ell=1}^V P(c_{\\ell})^{\\alpha}$，它依赖于观测到的上下文频率。参数 $\\alpha$（通常取值为 $0.75$）会使分布倾斜，通常会降低非常高频词的概率。\n\n最后，对于每个由 $(k, Q)$ 定义的测试配置，我们构建相应的矩阵 $S(k, Q)$。为了分析这个隐式语义空间的结构，我们对 $S$ 进行奇异值分解 (SVD)。SVD 提供了奇异值（$\\sigma_1 \\ge \\sigma_2 \\ge \\dots$），它们代表了矩阵主成分的大小。我们提取前两个奇异值 $\\sigma_1$ 和 $\\sigma_2$，因为它们捕捉了在该配置下 SGNS 目标定义的语义空间中最重要的方差维度。该程序为五个指定的测试用例计算这些值，从而提供不同 SGNS 超参数如何塑造底层词表示的量化比较。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Operationalizes the Distributional Hypothesis to analyze the implicit matrix\n    factorized by Skip-Gram with Negative Sampling (SGNS).\n    \"\"\"\n\n    # 1. Build a finite corpus with tokenization by whitespace.\n    corpus_sentences = [\n        \"the quick brown fox jumps over the lazy dog\",\n        \"the quick blue hare jumps over the sleepy dog\",\n        \"a fast brown fox leaps over a lazy hound\",\n        \"the slow tortoise crawls under the lazy dog\"\n    ]\n    all_words = \" \".join(corpus_sentences).split()\n    vocab = sorted(list(set(all_words)))\n    V = len(vocab)\n    word_to_idx = {word: i for i, word in enumerate(vocab)}\n\n    # 2. Construct a symmetric word–context co-occurrence matrix C.\n    r = 2\n    C = np.zeros((V, V))\n    for sentence in corpus_sentences:\n        tokens = sentence.split()\n        L = len(tokens)\n        for i in range(L):\n            center_word = tokens[i]\n            center_idx = word_to_idx[center_word]\n            start_context = max(0, i - r)\n            end_context = min(L, i + r + 1)\n            for j in range(start_context, end_context):\n                if i == j:\n                    continue\n                context_word = tokens[j]\n                context_idx = word_to_idx[context_word]\n                C[center_idx, context_idx] += 1\n\n    # 3. Convert counts to probabilities with additive smoothing.\n    tau = 1e-3\n    C_tilde = C + tau\n    N = np.sum(C_tilde)\n    P_wc = C_tilde / N  # Joint probability P(w, c)\n    P_w = np.sum(P_wc, axis=1)  # Word marginal P(w)\n    P_c = np.sum(P_wc, axis=0)  # Context marginal P(c)\n\n    # 4. Compute the Pointwise Mutual Information matrix M.\n    # To avoid numerical errors with log(0), we use the smoothed probabilities.\n    # The smoothing ensures P_wc, P_w, and P_c are all positive.\n    log_P_wc = np.log(P_wc)\n    log_P_w = np.log(P_w)\n    log_P_c = np.log(P_c)\n    M = log_P_wc - log_P_w[:, np.newaxis] - log_P_c[np.newaxis, :]\n\n    # Define the test suite.\n    test_cases = [\n        # (distribution_type, k, alpha)\n        ('uniform', 1, None),\n        ('uniform', 20, None),\n        ('unigram', 1, 0.75),\n        ('unigram', 20, 0.75),\n        ('unigram', 5, 1.0),\n    ]\n\n    results = []\n    \n    for dist_type, k, alpha in test_cases:\n        # 5. Compute the negative sampling distribution Q.\n        if dist_type == 'uniform':\n            Q = np.ones(V) / V\n        elif dist_type == 'unigram':\n            Q = P_c**alpha\n            Q /= np.sum(Q)\n        else:\n            raise ValueError(\"Unknown distribution type\")\n\n        # 6. Construct the shifted PMI matrix S(k,Q).\n        log_k = np.log(k)\n        # The term log(Q(c_j) / P(c_j)) depends only on the context j.\n        log_ratio_Q_over_Pc = np.log(Q) - log_P_c\n        # Broadcasting subtracts the scalar log_k and the row vector from each row of M.\n        S = M - log_k - log_ratio_Q_over_Pc[np.newaxis, :]\n        \n        # 7. Compute the singular values of S(k,Q) and extract the top two.\n        singular_values = np.linalg.svd(S, compute_uv=False)\n        sigma_1 = singular_values[0]\n        sigma_2 = singular_values[1]\n        \n        results.append([sigma_1, sigma_2])\n\n    # Final print statement in the exact required format.\n    formatted_results = []\n    for res_pair in results:\n        formatted_pair = f\"[{res_pair[0]:.6f},{res_pair[1]:.6f}]\"\n        formatted_results.append(formatted_pair)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "有效的词表示不仅取决于模型架构，还严重依赖于数据预处理。本练习将我们的注意力从模型目标转移到数据准备上，特别是词形还原（lemmatization）这一常见步骤。您将使用信息论中的工具，如詹森-香农散度（Jensen-Shannon divergence），来量化词形还原在多大程度上合理地合并了相似的上下文用法，或是在多大程度上错误地混淆了一个词的不同词义 。",
            "id": "3182931",
            "problem": "考虑自然语言处理中的分布假说（Distributional Hypothesis），该假说指出，出现在相似上下文中的词语往往具有相似的含义。在学习词表示的深度学习模型中，诸如词形还原（将表层形式映射到共同的基本形式）之类的预处理决策会改变经验上下文分布，从而影响学习到的词嵌入。你的任务是通过比较以引理为条件的上下文分布与以每个表层形式为条件的分布，来形式化并量化这种影响。你需要实现一个程序，为每个测试用例计算两个从第一性原理推导出的信息论量。\n\n定义与设置：设离散上下文的有限集合表示为 $\\mathcal{C} = \\{c_1, c_2, \\ldots, c_K\\}$，一个引理 $\\ell$ 具有表层形式 $s_1, s_2, \\ldots, s_M$。对于每个表层形式 $s_m$，给定非负整数上下文计数 $\\{n(c_k, s_m)\\}_{k=1}^K$。你必须使用一个具有科学依据且能避免未见上下文出现零概率问题的估计器，从这些计数中估计条件上下文分布 $p(c \\mid s_m)$。然后，通过跨表层形式聚合计数并应用相同的估计原则，来估计引理级别的上下文分布 $p(c \\mid \\ell)$。使用这些估计出的分布，量化：\n- 表层形式分布 $p(c \\mid s_m)$ 之间的平均成对散度，解释为不同表层形式的上下文用法有多大区别。\n- 每个表层形式分布 $p(c \\mid s_m)$ 与引理级别分布 $p(c \\mid \\ell)$ 之间的平均散度，解释为词形还原在多大程度上混淆或分离了表层形式的独特上下文用法。\n\n估计要求：使用 add-$\\alpha$ 平滑（对称狄利克雷先验的一个特例），平滑参数 $\\alpha = 0.5$ 来从计数中估计条件分布。这意味着每个上下文概率必须通过加性平滑从计数中计算，并在 $\\mathcal{C}$ 上进行归一化，确保所有上下文的概率都严格为正。\n\n散度要求：使用信息论中的 Kullback-Leibler (KL) 散度和 Jensen-Shannon (JS) 散度来定义这些量。对于定义在相同支撑集上的任意两个离散分布 $P$ 和 $Q$，KL 散度衡量在 $P$ 分布下的期望对数似然比，而 Jensen-Shannon 散度通过在比较前混合分布来对称化和平滑化 KL 散度。所有对数都必须取自然对数。为完成此任务，表层形式间的成对散度必须使用 $p(c \\mid s_i)$ 和 $p(c \\mid s_j)$ 之间的 Jensen-Shannon 散度计算，其中 $(i,j)$ 为所有满足 $i  j$ 的无序对，并在所有对上取平均。表层形式与引理之间的散度必须使用 $p(c \\mid s_m)$ 和 $p(c \\mid \\ell)$ 之间的 Jensen-Shannon 散度计算，其中 $m=1,\\ldots,M$，并在所有 $m$ 上取平均。\n\n测试套件：实现你的程序以处理以下三个测试用例。每个用例通过名称指定 $\\mathcal{C}$（名称仅为描述性；你的计算纯粹是数值的），并提供表层形式的计数。你必须将这些名称视为索引 $1,\\ldots,K$ 的标签，并仅使用计数。\n\n- 测试用例 1（上下文集大小 $K = 4$）：引理 $\\ell = \\text{\"run\"}$，表层形式为 $s_1 = \\text{\"run\"}$、$s_2 = \\text{\"runs\"}$、$s_3 = \\text{\"running\"}$。上下文为 $\\mathcal{C} = [\\text{\"sports\"}, \\text{\"execute\"}, \\text{\"finance\"}, \\text{\"river\"}]$，计数如下：\n  - $n(c, s_1) = [40, 10, 0, 0]$\n  - $n(c, s_2) = [20, 5, 0, 0]$\n  - $n(c, s_3) = [35, 8, 0, 0]$\n\n- 测试用例 2（上下文集大小 $K = 4$）：引理 $\\ell = \\text{\"bank\"}$，表层形式为 $s_1 = \\text{\"bank\"}$、$s_2 = \\text{\"banks\"}$、$s_3 = \\text{\"banking\"}$。上下文为 $\\mathcal{C} = [\\text{\"finance\"}, \\text{\"river\"}, \\text{\"law\"}, \\text{\"sports\"}]$，计数如下：\n  - $n(c, s_1) = [45, 5, 10, 0]$\n  - $n(c, s_2) = [10, 40, 0, 0]$\n  - $n(c, s_3) = [30, 0, 5, 0]$\n\n- 测试用例 3（上下文集大小 $K = 4$）：引理 $\\ell = \\text{\"bear\"}$，表层形式为 $s_1 = \\text{\"bear\"}$、$s_2 = \\text{\"bears\"}$、$s_3 = \\text{\"bearing\"}$。上下文为 $\\mathcal{C} = [\\text{\"animal\"}, \\text{\"finance\"}, \\text{\"support\"}, \\text{\"weather\"}]$，计数如下：\n  - $n(c, s_1) = [2, 1, 0, 0]$\n  - $n(c, s_2) = [0, 0, 1, 0]$\n  - $n(c, s_3) = [0, 0, 0, 1]$\n\n计算细节：\n- 对于每个表层形式，使用 $\\alpha = 0.5$ 在 $K$ 个上下文中通过 add-$\\alpha$ 平滑估计 $p(c \\mid s_m)$。\n- 通过聚合所有表层形式的原始计数，并应用相同的 add-$\\alpha$ 平滑（$\\alpha = 0.5$）来估计 $p(c \\mid \\ell)$。\n- 计算表层形式分布之间的平均成对 Jensen-Shannon 散度，并计算每个表层形式与引理分布之间的平均 Jensen-Shannon 散度。所有散度计算均使用自然对数。\n\n最终输出格式：你的程序应生成一行输出，其中包含三个测试用例的结果，形式为一个用方括号括起来的逗号分隔列表。每个测试用例的结果本身必须是一个包含两个浮点数的列表，顺序为 $[\\text{mean\\_JS\\_among\\_forms}, \\text{mean\\_JS\\_form\\_vs\\_lemma}]$，每个数字四舍五入到 $6$ 位小数。例如，最终打印的字符串必须类似于 $[[x_1,y_1],[x_2,y_2],[x_3,y_3]]$，其中每个 $x_i$ 和 $y_i$ 都是四舍五入到 $6$ 位小数的浮点数。不应打印任何其他文本。",
            "solution": "该问题要求基于分布假说的原则，量化一个引理的各种表层形式与聚合后的引理本身之间的上下文差异。这是通过为三个给定的测试用例计算两个信息论量来完成的。整个过程建立在一个严谨的数学和统计框架之上，我们现在将详细阐述。\n\n首先，我们必须从原始计数中估计条件上下文概率分布。设 $\\mathcal{C} = \\{c_1, \\ldots, c_K\\}$ 为 $K$ 个上下文的集合，$\\{s_1, \\ldots, s_M\\}$ 为引理 $\\ell$ 的 $M$ 个表层形式的集合。我们已知每个上下文 $c_k$ 和表层形式 $s_m$ 的计数 $n(c_k, s_m)$。直接的最大似然估计 $p(c_k \\mid s_m) = n(c_k, s_m) / \\sum_j n(c_j, s_m)$ 容易出现零计数问题，这会错误地为未观察到的事件分配零概率，并在后续计算中导致数学问题（例如，对零取对数）。\n\n为了解决这个问题，题目要求使用 add-$\\alpha$ 平滑（在贝叶斯框架中也称为拉普拉斯平滑或对称狄利克雷先验）。使用平滑参数 $\\alpha = 0.5$，给定表层形式 $s_m$ 时上下文 $c_k$ 的估计概率为：\n$$\np(c_k \\mid s_m) = \\frac{n(c_k, s_m) + \\alpha}{\\sum_{j=1}^K n(c_j, s_m) + K\\alpha}\n$$\n这个估计器确保每个上下文都被分配一个非零概率。\n\n接下来，我们估计引理 $\\ell$ 的上下文分布。这首先通过聚合所有表层形式中每个上下文的计数来实现：\n$$\nn(c_k, \\ell) = \\sum_{m=1}^M n(c_k, s_m)\n$$\n然后，我们对这些聚合计数应用相同的 add-$\\alpha$ 平滑规则，以获得引理的上下文分布：\n$$\np(c_k \\mid \\ell) = \\frac{n(c_k, \\ell) + \\alpha}{\\sum_{j=1}^K n(c_j, \\ell) + K\\alpha}\n$$\n\n在建立了这些概率分布之后，我们可以使用信息论中的工具来量化它们之间的“距离”或“散度”。问题指定使用 Jensen-Shannon (JS) 散度。JS 散度是更基本的 Kullback-Leibler (KL) 散度的一个对称化和平滑化版本。\n\n对于定义在相同支撑集上的两个离散概率分布 $P = \\{p_k\\}_{k=1}^K$ 和 $Q = \\{q_k\\}_{k=1}^K$，从 $Q$ 到 $P$ 的 KL 散度由以下公式给出：\n$$\nD_{KL}(P \\| Q) = \\sum_{k=1}^K p_k \\ln\\left(\\frac{p_k}{q_k}\\right)\n$$\nKL 散度是不对称的，即通常情况下 $D_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)$。Jensen-Shannon 散度通过将两个分布都与它们的平均分布进行比较来解决这个问题。设 $M = \\frac{1}{2}(P+Q)$ 为混合分布。JS 散度定义为：\n$$\nD_{JS}(P \\| Q) = \\frac{1}{2} D_{KL}(P \\| M) + \\frac{1}{2} D_{KL}(Q \\| M)\n$$\n$D_{JS}(P \\| Q)$ 是对称、非负且有界的（具体来说，$0 \\le D_{JS}(P \\| Q) \\le \\ln 2$）。\n\n我们必须为每个测试用例计算两个特定的量：\n1.  **表层形式之间的平均成对散度**：这个度量，$\\text{mean\\_JS\\_among\\_forms}$，量化了引理不同表层形式之间上下文变化的程度。高值表明表层形式被用在不同的上下文中，将它们进行词形还原可能会混淆不同的含义。它是通过在所有唯一的表层形式对上平均 JS 散度来计算的：\n    $$\n    \\text{mean\\_JS\\_among\\_forms} = \\frac{1}{\\binom{M}{2}} \\sum_{1 \\le i  j \\le M} D_{JS}\\big(p(c \\mid s_i) \\,\\|\\, p(c \\mid s_j)\\big)\n    $$\n    对于 $M=3$，这是在 $\\binom{3}{2}=3$ 对上求平均。\n\n2.  **每个表层形式与引理之间的平均散度**：这个度量，$\\text{mean\\_JS\\_form\\_vs\\_lemma}$，衡量聚合的引理分布在多大程度上代表了每个单独表层形式的上下文。高值表示引理的“平均”上下文对于其一个或多个组成形式来说是一个糟糕的代表。它是通过平均每个表层形式的分布与引理分布之间的 JS 散度来计算的：\n    $$\n    \\text{mean\\_JS\\_form\\_vs\\_lemma} = \\frac{1}{M} \\sum_{m=1}^M D_{JS}\\big(p(c \\mid s_m) \\,\\|\\, p(c \\mid \\ell)\\big)\n    $$\n    对于 $M=3$，这是在 $3$ 对上求平均。\n\n每个测试用例的算法如下：\na. 对于 $M$ 个表层形式中的每一个，取其 $K$ 维计数向量，并使用 $\\alpha=0.5$ 计算平滑概率分布 $p(c \\mid s_m)$。\nb. 将所有 $M$ 个表层形式的计数向量相加，得到引理的聚合计数向量 $n(c, \\ell)$。\nc. 从其聚合计数计算引理的平滑概率分布 $p(c \\mid \\ell)$。\nd. 计算所有 $1 \\le i  j \\le M$ 的成对 JS 散度 $D_{JS}(p(c \\mid s_i) \\| p(c \\mid s_j))$。将这些值平均以找到第一个所需量。\ne. 计算每个表层形式与引理之间的 JS 散度 $D_{JS}(p(c \\mid s_m) \\| p(c \\mid \\ell))$，其中 $m=1, \\ldots, M$。将这些值平均以找到第二个所需量。\nf. 报告这两个平均量，四舍五入到六位小数。\n\n此过程将应用于提供的所有三个测试用例。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef solve():\n    \"\"\"\n    Computes information-theoretic quantities to measure the effect of lemmatization\n    on context distributions for a given set of test cases.\n    \"\"\"\n\n    def calculate_distribution(counts: np.ndarray, alpha: float) -> np.ndarray:\n        \"\"\"\n        Estimates a probability distribution from counts using add-alpha smoothing.\n\n        Args:\n            counts: A NumPy array of non-negative integer counts.\n            alpha: The smoothing parameter.\n\n        Returns:\n            A NumPy array representing the smoothed probability distribution.\n        \"\"\"\n        if not isinstance(counts, np.ndarray):\n            counts = np.array(counts, dtype=float)\n        \n        k = len(counts)\n        total_counts = np.sum(counts)\n        numerator = counts + alpha\n        denominator = total_counts + k * alpha\n        return numerator / denominator\n\n    def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n        \"\"\"\n        Calculates the Jensen-Shannon divergence between two probability distributions.\n        All logarithms are natural logarithms.\n\n        Args:\n            p: A NumPy array for the first probability distribution.\n            q: A NumPy array for the second probability distribution.\n\n        Returns:\n            The Jensen-Shannon divergence as a float.\n        \"\"\"\n        m = 0.5 * (p + q)\n        # scipy.stats.entropy(pk, qk) calculates KL divergence D_KL(pk || qk)\n        kl_p_m = entropy(p, m)\n        kl_q_m = entropy(q, m)\n        return 0.5 * (kl_p_m + kl_q_m)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1: lemma \"run\"\n        {\n            \"surface_form_counts\": [\n                [40, 10, 0, 0],\n                [20, 5, 0, 0],\n                [35, 8, 0, 0],\n            ]\n        },\n        # Test Case 2: lemma \"bank\"\n        {\n            \"surface_form_counts\": [\n                [45, 5, 10, 0],\n                [10, 40, 0, 0],\n                [30, 0, 5, 0],\n            ]\n        },\n        # Test Case 3: lemma \"bear\"\n        {\n            \"surface_form_counts\": [\n                [2, 1, 0, 0],\n                [0, 0, 1, 0],\n                [0, 0, 0, 1],\n            ]\n        },\n    ]\n\n    alpha = 0.5\n    all_results = []\n\n    for case in test_cases:\n        counts_per_form = np.array(case[\"surface_form_counts\"], dtype=float)\n        m, k = counts_per_form.shape\n\n        # Estimate distributions for each surface form\n        dists_s_m = [calculate_distribution(counts, alpha) for counts in counts_per_form]\n\n        # Aggregate counts and estimate distribution for the lemma\n        lemma_counts = np.sum(counts_per_form, axis=0)\n        dist_lemma = calculate_distribution(lemma_counts, alpha)\n\n        # 1. Calculate average pairwise JS divergence among surface forms\n        pairwise_js_divs = []\n        if m > 1:\n            for i in range(m):\n                for j in range(i + 1, m):\n                    div = js_divergence(dists_s_m[i], dists_s_m[j])\n                    pairwise_js_divs.append(div)\n        \n        mean_js_among_forms = np.mean(pairwise_js_divs) if pairwise_js_divs else 0.0\n\n        # 2. Calculate average JS divergence between each surface form and the lemma\n        form_lemma_js_divs = []\n        for i in range(m):\n            div = js_divergence(dists_s_m[i], dist_lemma)\n            form_lemma_js_divs.append(div)\n        \n        mean_js_form_vs_lemma = np.mean(form_lemma_js_divs) if form_lemma_js_divs else 0.0\n        \n        all_results.append([mean_js_among_forms, mean_js_form_vs_lemma])\n\n    # Format the final output string as specified: [[x1,y1],[x2,y2],[x3,y3]]\n    result_strings = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in all_results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}