{
    "hands_on_practices": [
        {
            "introduction": "The distributional hypothesis can be framed as a prediction task in two fundamental ways: predicting context from a word, the basis for Skip-gram models, or predicting a word from its context, which is the core idea behind Masked Language Modeling. This practice  provides a concrete way to see how these two perspectives, $p(c|w)$ versus $p(w|c)$, create different geometric relationships between words. By using simple, transparent probability calculations, you will demystify the foundational objectives that drive complex neural models like word2vec and BERT.",
            "id": "3182958",
            "problem": "You are given a fixed, small corpus and asked to operationalize the distributional hypothesis in two canonical deep learning objectives, Masked Language Modeling (MLM) and Skip-gram (SG), using principled count-based estimators and to compare the resulting geometries of word embeddings. The distributional hypothesis states that words occurring in similar contexts tend to have similar meanings. Your task is to start from core probabilistic definitions and well-tested statistical procedures and derive, implement, and compare conditional distributions and the induced vector-space geometry.\n\nCorpus (tokenized, all lowercase, punctuation removed; each line is a sentence): \nthe cat sat on the mat\nthe dog sat on the rug\na cat chased a mouse\na dog chased a ball\nmusic and song fill the hall\nthe song played music softly\nquantum theory explains physics\ntheory of music and physics\nthe quantum cat thought of physics\ndogs and cats share a home\na theory about a song\n\nDefinitions to use as fundamental base:\n- Conditional probability: for events $A$ and $B$, $p(A \\mid B) = \\frac{p(A, B)}{p(B)}$ whenever $p(B) > 0$.\n- Maximum Likelihood Estimation (MLE) for a multinomial distribution: given counts $\\{n_i\\}$ over outcomes $\\{i\\}$, $\\hat{p}_i = \\frac{n_i}{\\sum_j n_j}$.\n- Additive (Laplace) smoothing: for counts $\\{n_i\\}$ over $V$ outcomes and smoothing parameter $\\alpha > 0$, define $\\tilde{p}_i = \\frac{n_i + \\alpha}{\\sum_j n_j + \\alpha V}$.\n- Cosine similarity: for vectors $\\mathbf{u}$ and $\\mathbf{v}$ in $\\mathbb{R}^d$, $\\mathrm{cos}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}^\\top \\mathbf{v}}{\\lVert \\mathbf{u} \\rVert_2 \\lVert \\mathbf{v} \\rVert_2}$.\n\nTasks to perform:\n1. Construct a symmetric context window of size $K$ around each target word $w$ in the corpus. For each occurrence at position $i$, include all words at positions $j$ with $|i - j| \\le K$ and $j \\ne i$ as context words $c$. Accumulate co-occurrence counts $N(w, c)$ over the entire corpus. Use the entire token vocabulary as the set of possible $w$ and $c$.\n2. Define Masked Language Modeling (MLM): predict $w$ from $c$. From the counts $N(w, c)$, derive the conditional distribution $p_{\\mathrm{MLM}}(w \\mid c)$ using Maximum Likelihood Estimation with additive smoothing parameter $\\alpha$, consistent with the above definitions.\n3. Define Skip-gram (SG): predict $c$ from $w$. From the same counts $N(w, c)$, derive $p_{\\mathrm{SG}}(c \\mid w)$ using Maximum Likelihood Estimation with additive smoothing parameter $\\alpha$, consistent with the above definitions.\n4. Embed each word $w$ in two spaces:\n   - MLM embedding $\\mathbf{v}_{\\mathrm{MLM}}(w)$: interpret $p_{\\mathrm{MLM}}(w \\mid c)$ as a function of $c$ and create a vector whose components are indexed by contexts $c$.\n   - SG embedding $\\mathbf{v}_{\\mathrm{SG}}(w)$: interpret $p_{\\mathrm{SG}}(c \\mid w)$ as a function of $c$ and create a vector whose components are indexed by contexts $c$.\n   Ensure both embeddings have the same dimension equal to the vocabulary size $V$, ordered by a fixed, deterministic ordering of the vocabulary.\n5. For each test case below, compute the cosine similarity between two specified words under both geometries, that is, compute $\\mathrm{cos}(\\mathbf{v}_{\\mathrm{MLM}}(w_1), \\mathbf{v}_{\\mathrm{MLM}}(w_2))$ and $\\mathrm{cos}(\\mathbf{v}_{\\mathrm{SG}}(w_1), \\mathbf{v}_{\\mathrm{SG}}(w_2))$, and report their difference $\\Delta = \\mathrm{cos}_{\\mathrm{SG}} - \\mathrm{cos}_{\\mathrm{MLM}}$.\n6. Use additive smoothing with parameter $\\alpha > 0$ and symmetric window size $K \\in \\mathbb{N}$ as specified in the test suite. Angles are not required; report only cosine similarities as real numbers.\n\nTest suite (each case is of the form $(K, \\alpha, w_1, w_2)$):\n- Case $1$: $K = 2$, $\\alpha = 0.5$, $w_1 =$ \"cat\", $w_2 =$ \"dog\".\n- Case $2$: $K = 1$, $\\alpha = 1.0$, $w_1 =$ \"music\", $w_2 =$ \"song\".\n- Case $3$: $K = 3$, $\\alpha = 10^{-6}$, $w_1 =$ \"the\", $w_2 =$ \"a\".\n- Case $4$: $K = 2$, $\\alpha = 0.2$, $w_1 =$ \"quantum\", $w_2 =$ \"theory\".\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sublist of three floats in the order $[\\mathrm{cos}_{\\mathrm{MLM}}, \\mathrm{cos}_{\\mathrm{SG}}, \\Delta]$. For example, an output with two hypothetical cases should look like \"[[0.123456,0.234567,0.111111],[0.222222,0.333333,0.111111]]\". No extra text should be printed.\n\nScientific realism and constraints:\n- Use only the provided corpus, symmetric windowing, and additive smoothing to estimate the conditional distributions.\n- Ensure that the vocabulary is the set of all unique tokens from the corpus and that contexts are tokens from the same set.\n- Use deterministic tokenization and vocabulary ordering so that vectors are comparable across test cases.\n- All reported values must be real numbers with no physical units.",
            "solution": "### Solution Derivation\n\nThe solution operationalizes the distributional hypothesis by constructing word embeddings from co-occurrence statistics and comparing their geometric properties under two different probabilistic models, Masked Language Modeling (MLM) and Skip-gram (SG).\n\n**1. Vocabulary and Co-occurrence Matrix Construction**\n\nFirst, we establish a fixed vocabulary $\\mathcal{V}$ by collecting all unique tokens from the provided corpus and sorting them alphabetically to ensure a deterministic ordering. Let $V = |\\mathcal{V}|$ be the size of the vocabulary. We create a word-to-index mapping for computational convenience.\n\nFor a given symmetric window size $K$, we populate a $V \\times V$ co-occurrence matrix $N$. An entry $N_{ij}$ stores the count $N(w_i, c_j)$, representing the number of times word $c_j$ appears in the context of target word $w_i$. We iterate through each word $w_i$ at position $p$ in each sentence and increment the counts for all context words $c_j$ found at positions $q$ such that $1 \\le |p - q| \\le K$.\n\n**2. Skip-gram (SG) Model and Embeddings**\n\nThe Skip-gram model aims to predict context words $c$ given a target word $w$. This corresponds to estimating the conditional probability $p_{\\mathrm{SG}}(c \\mid w)$.\nFollowing the principle of Maximum Likelihood Estimation (MLE) on our counts, the probability of observing a specific context word $c_j$ given the target word $w_i$ is estimated by the ratio of their co-occurrence count to the total count of all context words for $w_i$.\nThe total number of context words observed for a target word $w_i$ is the sum of the $i$-th row of the matrix $N$: $S_i^{\\text{row}} = \\sum_{k=1}^V N_{ik}$.\nApplying additive smoothing with parameter $\\alpha$ to handle data sparsity and avoid zero probabilities, the conditional probability is:\n$$p_{\\mathrm{SG}}(c_j \\mid w_i) = \\frac{N_{ij} + \\alpha}{S_i^{\\text{row}} + \\alpha V}$$\nThe Skip-gram embedding for a word $w_i$, denoted $\\mathbf{v}_{\\mathrm{SG}}(w_i)$, is a $V$-dimensional vector whose components are these probabilities, indexed by the context words $c_j \\in \\mathcal{V}$:\n$$\\mathbf{v}_{\\mathrm{SG}}(w_i) = [p_{\\mathrm{SG}}(c_1 \\mid w_i), p_{\\mathrm{SG}}(c_2 \\mid w_i), \\ldots, p_{\\mathrm{SG}}(c_V \\mid w_i)]^\\top$$\nThis vector corresponds to the $i$-th row of the smoothed conditional probability matrix $P_{\\mathrm{SG}}$, where $(P_{\\mathrm{SG}})_{ij} = p_{\\mathrm{SG}}(c_j \\mid w_i)$.\n\n**3. Masked Language Modeling (MLM) Model and Embeddings**\n\nThe Masked Language Modeling objective, in this context, is to predict the target word $w$ given a context word $c$. This requires estimating the conditional probability $p_{\\mathrm{MLM}}(w \\mid c)$.\nAnalogously to the SG model, we use the co-occurrence counts. For a given context word $c_j$, the count of co-occurring target words $w_i$ is $N_{ij}$. The total number of times $c_j$ appears as a context word for any target is the sum of the $j$-th column of the matrix $N$: $S_j^{\\text{col}} = \\sum_{k=1}^V N_{kj}$.\nApplying additive smoothing, the conditional probability is:\n$$p_{\\mathrm{MLM}}(w_i \\mid c_j) = \\frac{N_{ij} + \\alpha}{S_j^{\\text{col}} + \\alpha V}$$\nThe MLM embedding for a word $w_i$, denoted $\\mathbf{v}_{\\mathrm{MLM}}(w_i)$, is a $V$-dimensional vector. As per the problem description, its components are indexed by the context word $c_j$. Therefore, the $j$-th component of this vector is $p_{\\mathrm{MLM}}(w_i \\mid c_j)$.\n$$\\mathbf{v}_{\\mathrm{MLM}}(w_i) = [p_{\\mathrm{MLM}}(w_i \\mid c_1), p_{\\mathrm{MLM}}(w_i \\mid c_2), \\ldots, p_{\\mathrm{MLM}}(w_i \\mid c_V)]^\\top$$\nThis vector corresponds to the $i$-th row of a matrix $P_{\\mathrm{MLM}}$, where $(P_{\\mathrm{MLM}})_{ij} = p_{\\mathrm{MLM}}(w_i \\mid c_j)$.\n\n**4. Geometric Comparison via Cosine Similarity**\n\nTo compare the geometries induced by the two models, we compute the cosine similarity between the embedding vectors of two words, $w_1$ and $w_2$. The cosine similarity provides a measure of the angle between two vectors, and thus their orientation-based similarity, which is a common way to quantify semantic similarity in vector spaces.\n\nFor a given pair of words $(w_1, w_2)$, we compute:\n- The MLM similarity: $\\mathrm{cos}_{\\mathrm{MLM}} = \\mathrm{cos}(\\mathbf{v}_{\\mathrm{MLM}}(w_1), \\mathbf{v}_{\\mathrm{MLM}}(w_2)) = \\frac{\\mathbf{v}_{\\mathrm{MLM}}(w_1)^\\top \\mathbf{v}_{\\mathrm{MLM}}(w_2)}{\\lVert \\mathbf{v}_{\\mathrm{MLM}}(w_1) \\rVert_2 \\lVert \\mathbf{v}_{\\mathrm{MLM}}(w_2) \\rVert_2}$\n- The SG similarity: $\\mathrm{cos}_{\\mathrm{SG}} = \\mathrm{cos}(\\mathbf{v}_{\\mathrm{SG}}(w_1), \\mathbf{v}_{\\mathrm{SG}}(w_2)) = \\frac{\\mathbf{v}_{\\mathrm{SG}}(w_1)^\\top \\mathbf{v}_{\\mathrm{SG}}(w_2)}{\\lVert \\mathbf{v}_{\\mathrm{SG}}(w_1) \\rVert_2 \\lVert \\mathbf{v}_{\\mathrm{SG}}(w_2) \\rVert_2}$\n\nFinally, we calculate the difference $\\Delta = \\mathrm{cos}_{\\mathrm{SG}} - \\mathrm{cos}_{\\mathrm{MLM}}$ to directly compare the similarity measures produced by the two models for each test case. This entire process is repeated for each set of parameters $(K, \\alpha, w_1, w_2)$ in the test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the word embedding comparison problem by following these steps:\n    1. Processes a fixed corpus to build a vocabulary.\n    2. For each test case:\n        a. Constructs a word-context co-occurrence matrix `N` for a given window size `K`.\n        b. Derives conditional probability matrices `P_MLM` and `P_SG` using additive smoothing.\n        c. Extracts word embedding vectors for the specified words from these matrices.\n        d. Computes the cosine similarity between the word vectors in both embedding spaces (MLM and SG).\n        e. Calculates the difference between the two similarities.\n    3. Formats and prints the final results as specified.\n    \"\"\"\n    corpus = [\n        \"the cat sat on the mat\",\n        \"the dog sat on the rug\",\n        \"a cat chased a mouse\",\n        \"a dog chased a ball\",\n        \"music and song fill the hall\",\n        \"the song played music softly\",\n        \"quantum theory explains physics\",\n        \"theory of music and physics\",\n        \"the quantum cat thought of physics\",\n        \"dogs and cats share a home\",\n        \"a theory about a song\"\n    ]\n\n    test_cases = [\n        (2, 0.5, \"cat\", \"dog\"),\n        (1, 1.0, \"music\", \"song\"),\n        (3, 1e-6, \"the\", \"a\"),\n        (2, 0.2, \"quantum\", \"theory\"),\n    ]\n\n    # Pre-processing: tokenize corpus and build a deterministic vocabulary\n    corpus_tokens = [line.split() for line in corpus]\n    \n    all_tokens = set()\n    for sentence in corpus_tokens:\n        all_tokens.update(sentence)\n    \n    vocabulary = sorted(list(all_tokens))\n    V = len(vocabulary)\n    word_to_idx = {word: i for i, word in enumerate(vocabulary)}\n\n    final_results = []\n    cooccurrence_matrices_cache = {}\n\n    for K, alpha, w1_str, w2_str in test_cases:\n        \n        # Task 1: Construct co-occurrence count matrix N(w, c)\n        # We cache the matrix as it can be expensive to recompute.\n        if K not in cooccurrence_matrices_cache:\n            N = np.zeros((V, V), dtype=np.float64)\n            for sentence in corpus_tokens:\n                for i, target_word in enumerate(sentence):\n                    target_idx = word_to_idx[target_word]\n                    \n                    start = max(0, i - K)\n                    end = min(len(sentence), i + K + 1)\n                    \n                    for j in range(start, end):\n                        if i == j:\n                            continue\n                        context_word = sentence[j]\n                        context_idx = word_to_idx[context_word]\n                        N[target_idx, context_idx] += 1\n            cooccurrence_matrices_cache[K] = N\n        \n        N = cooccurrence_matrices_cache[K]\n            \n        w1_idx = word_to_idx[w1_str]\n        w2_idx = word_to_idx[w2_str]\n\n        # Task 2  4: Derive P_MLM(w|c) and MLM embeddings\n        # P_MLM(w_i | c_j) = (N[i, j] + alpha) / (sum_k N[k, j] + alpha * V)\n        col_sums = N.sum(axis=0)\n        P_mlm_denom = col_sums + alpha * V\n        P_mlm = (N + alpha) / P_mlm_denom[np.newaxis, :]\n        \n        v_mlm_w1 = P_mlm[w1_idx, :]\n        v_mlm_w2 = P_mlm[w2_idx, :]\n        \n        # Task 3  4: Derive P_SG(c|w) and SG embeddings\n        # P_SG(c_j | w_i) = (N[i, j] + alpha) / (sum_k N[i, k] + alpha * V)\n        row_sums = N.sum(axis=1)\n        # Use [:, np.newaxis] to ensure correct broadcasting for row-wise division\n        P_sg_denom = row_sums + alpha * V\n        P_sg = (N + alpha) / P_sg_denom[:, np.newaxis]\n        \n        v_sg_w1 = P_sg[w1_idx, :]\n        v_sg_w2 = P_sg[w2_idx, :]\n\n        # Task 5: Compute cosine similarities\n        def cosine_similarity(u, v):\n            dot_product = np.dot(u, v)\n            norm_u = np.linalg.norm(u)\n            norm_v = np.linalg.norm(v)\n            # Denominator is guaranteed non-zero due to alpha > 0 smoothing\n            return dot_product / (norm_u * norm_v)\n\n        cos_mlm = cosine_similarity(v_mlm_w1, v_mlm_w2)\n        cos_sg = cosine_similarity(v_sg_w1, v_sg_w2)\n        \n        # Task 5: Compute difference\n        delta = cos_sg - cos_mlm\n        \n        final_results.append([cos_mlm, cos_sg, delta])\n        \n    # Final print statement in the exact required format.\n    # e.g., \"[[0.123456,0.234567,0.111111],[0.222222,0.333333,0.111111]]\"\n    output_str = \"[\" + \",\".join([f\"[{c1:.6f},{c2:.6f},{d:.6f}]\" for c1, c2, d in final_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the Skip-gram objective, this practice explores how it is made computationally feasible in modern embedding algorithms through negative sampling. You will investigate the elegant theoretical connection between this practical training strategy and the information-theoretic concept of Pointwise Mutual Information (PMI). This exercise  reveals how hyperparameters like the number of negative samples $k$ and the choice of negative sampling distribution systematically alter the implicit semantic space, providing a deeper appreciation for the design of models like word2vec.",
            "id": "3182845",
            "problem": "Construct a program that operationalizes the Distributional Hypothesis by building a word–context co-occurrence matrix from a small corpus, and then analyzes how the choice of negative sampling distribution and the negative sampling multiplicity affect the implicit matrix that Skip-Gram with Negative Sampling (SGNS) factorizes. Begin from the following fundamental base: the Distributional Hypothesis asserts that words occurring in similar contexts tend to have similar meanings; co-occurrence frequencies define empirical joint and marginal probabilities; Pointwise Mutual Information (PMI) between a word and a context is defined in terms of these probabilities; SGNS optimizes a logistic objective over observed and negatively sampled pairs, which yields an optimal inner product equal to a shifted PMI that depends on the negative sampling distribution and the number of negatives. Do not use any shortcut formulas beyond these foundations.\n\nYour program must implement the following steps in a purely mathematical and logical manner:\n\n1. Build a finite corpus with tokenization by whitespace. Use the following four sentences, lowercased, as the entire corpus:\n   - \"the quick brown fox jumps over the lazy dog\"\n   - \"the quick blue hare jumps over the sleepy dog\"\n   - \"a fast brown fox leaps over a lazy hound\"\n   - \"the slow tortoise crawls under the lazy dog\"\n2. Construct a symmetric word–context co-occurrence matrix by sliding a window of radius $r = 2$ over each sentence. For each position $t$ with token $w_t$, and for all positions $j$ with $t - r \\le j \\le t + r$ and $j \\ne t$, increment the count for the pair $(w_t, w_j)$ by $1$. Let the resulting count matrix be $C \\in \\mathbb{R}^{V \\times V}$ where $V$ is the vocabulary size, rows index words, and columns index contexts.\n3. Convert counts to probabilities with additive smoothing to avoid undefined logarithms. Define a smoothed matrix $\\tilde{C}$ by adding $\\tau = 10^{-3}$ to every entry of $C$. Let the total number of pairs be $N = \\sum_{i=1}^{V} \\sum_{j=1}^{V} \\tilde{C}_{ij}$. Define the joint probability $P(w_i, c_j) = \\tilde{C}_{ij} / N$, the word marginal $P(w_i) = \\sum_{j=1}^{V} \\tilde{C}_{ij} / N$, and the context marginal $P(c_j) = \\sum_{i=1}^{V} \\tilde{C}_{ij} / N$.\n4. Compute the Pointwise Mutual Information matrix $M$ with entries\n   $$M_{ij} = \\log \\frac{P(w_i, c_j)}{P(w_i) P(c_j)},$$\n   using the natural logarithm.\n5. For negative sampling, consider two families of negative distributions over contexts:\n   - Uniform: $Q_{\\text{uni}}(c_j) = 1 / V$.\n   - Unigram to a power $\\alpha$: $Q_{\\alpha}(c_j) = \\frac{P(c_j)^{\\alpha}}{\\sum_{\\ell=1}^{V} P(c_{\\ell})^{\\alpha}}$ for a given $\\alpha \\in [0, 1]$.\n6. Consider the negative sampling multiplicity $k \\in \\mathbb{N}$. By analyzing the optimality condition of the logistic objective where positives are drawn from $P(w,c)$ and negatives from $P(w) Q(c)$ with $k$ negatives per positive, the SGNS optimal inner product takes the form of a shifted PMI:\n   $$S_{ij}(k, Q) = M_{ij} - \\log k - \\log \\frac{Q(c_j)}{P(c_j)}.$$\n   Using this relation strictly as a consequence of the optimization condition, construct the shifted matrix $S(k,Q)$ for the specified $(k, Q)$.\n7. For each specified configuration, compute the singular values of $S(k,Q)$ via the singular value decomposition. Let $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_V$ denote the singular values. Extract the top two singular values $\\sigma_1$ and $\\sigma_2$.\n8. To ensure numerical reproducibility, use the exact corpus, window radius $r = 2$, smoothing parameter $\\tau = 10^{-3}$, and the natural logarithm as specified.\n\nTest suite and required outputs:\n- Use the following five test configurations, each specified by a pair $(k, \\alpha)$ together with a declared negative sampling distribution:\n  1. Uniform distribution with $k = 1$.\n  2. Uniform distribution with $k = 20$.\n  3. Unigram to a power with $\\alpha = 0.75$ and $k = 1$.\n  4. Unigram to a power with $\\alpha = 0.75$ and $k = 20$.\n  5. Unigram to a power with $\\alpha = 1.0$ and $k = 5$.\n- For each configuration, compute and report the pair $[\\sigma_1, \\sigma_2]$ rounded to exactly $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a two-element list in the same format, with no spaces. For example, the outermost structure must look like\n  $$\\big[ [a_1, a_2], [b_1, b_2], \\dots \\big],$$\n  but with each float printed with exactly $6$ digits after the decimal point, and all commas retained while all spaces are removed. Concretely, the printed line must have the form\n  \"[[x11,x12],[x21,x22],[x31,x32],[x41,x42],[x51,x52]]\".",
            "solution": "The objective is to construct a program that models the distributional semantics of words and analyzes the effect of hyperparameters in the Skip-Gram with Negative Sampling (SGNS) model. This is achieved by operationalizing the Distributional Hypothesis, which posits that the meaning of a word is determined by the contexts in which it appears. The process begins with corpus analysis and culminates in extracting features from the implicit matrix derived from the SGNS optimization objective.\n\nFirst, we establish a vocabulary and quantify word-context relationships from a given corpus. The corpus consists of four sentences, which are tokenized by whitespace. The vocabulary is the set of all unique tokens, with size $V$. To capture the distributional properties, we construct a word-context co-occurrence matrix, $C \\in \\mathbb{R}^{V \\times V}$, where both rows and columns are indexed by the vocabulary. The matrix is populated by sliding a window of a specified radius, $r=2$, across each sentence. For each word $w_t$ at position $t$, its co-occurrence count $C_{ij}$ with a context word $w_j$ at a nearby position $j$ (where $t - r \\le j \\le t + r$ and $j \\neq t$) is incremented by $1$. This process naturally yields a symmetric matrix ($C = C^T$), as the relationship of co-occurrence is symmetric.\n\nSecond, we transition from raw counts to a probabilistic framework. To avoid issues with zero counts, which would lead to undefined logarithms later, we employ additive smoothing. A small constant, $\\tau = 10^{-3}$, is added to every entry of the count matrix $C$ to produce a smoothed matrix $\\tilde{C}$. The total number of smoothed co-occurrence pairs is $N = \\sum_{i=1}^{V} \\sum_{j=1}^{V} \\tilde{C}_{ij}$. From this, we define the empirical joint probability distribution $P(w_i, c_j) = \\tilde{C}_{ij} / N$. The marginal probabilities for words, $P(w_i) = \\sum_{j=1}^{V} P(w_i, c_j)$, and for contexts, $P(c_j) = \\sum_{i=1}^{V} P(w_i, c_j)$, are then derived by summing over the appropriate dimensions of the joint probability matrix.\n\nThird, we compute the Pointwise Mutual Information (PMI) matrix, $M$. The entry $M_{ij}$ quantifies the association between a specific word $w_i$ and a context $c_j$. It is defined as:\n$$M_{ij} = \\log \\frac{P(w_i, c_j)}{P(w_i) P(c_j)}$$\nA positive PMI value indicates that the word and context co-occur more frequently than expected if they were independent, suggesting a semantic correlation. We use the natural logarithm for this calculation. Numerically, this is implemented as $M_{ij} = \\log P(w_i, c_j) - \\log P(w_i) - \\log P(c_j)$ to maintain stability.\n\nFourth, we analyze the SGNS model. A key theoretical result shows that SGNS does not explicitly build and factorize the PMI matrix, but its optimization objective implicitly causes the learned word and context vectors to satisfy a specific relationship. The optimal dot product between a word vector for $w_i$ and a context vector for $c_j$ is equivalent to a shifted version of their PMI. This gives rise to the SGNS matrix, $S(k, Q)$, whose entries are:\n$$S_{ij}(k, Q) = M_{ij} - \\log k - \\log \\frac{Q(c_j)}{P(c_j)}$$\nHere, $k$ is the number of negative samples per positive sample, and $Q(c)$ is the probability distribution from which these negative contexts are drawn. This equation reveals how the choice of $k$ and $Q$ systematically shifts the semantic space being learned.\n\nThe program evaluates two types of negative sampling distributions, $Q$. The first is the uniform distribution, $Q_{\\text{uni}}(c_j) = 1/V$, which treats all words as equally likely negative samples. The second is a unigram-based distribution, $Q_{\\alpha}(c_j) = P(c_j)^{\\alpha} / \\sum_{\\ell=1}^V P(c_{\\ell})^{\\alpha}$, which is dependent on the observed frequency of contexts. The parameter $\\alpha$ (with common values like $0.75$) skews the distribution, typically dampening the probabilities of very frequent words.\n\nFinally, for each test configuration defined by $(k, Q)$, we construct the corresponding matrix $S(k, Q)$. To analyze the structure of this implicit semantic space, we perform Singular Value Decomposition (SVD) on $S$. The SVD provides the singular values ($\\sigma_1 \\ge \\sigma_2 \\ge \\dots$), which represent the magnitudes of the principal components of the matrix. We extract the top two singular values, $\\sigma_1$ and $\\sigma_2$, as these capture the most significant dimensions of variance in the semantic space defined by the SGNS objective under that configuration. The program calculates these values for five specified test cases, providing a quantitative comparison of how different SGNS hyperparameters shape the underlying word representations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Operationalizes the Distributional Hypothesis to analyze the implicit matrix\n    factorized by Skip-Gram with Negative Sampling (SGNS).\n    \"\"\"\n\n    # 1. Build a finite corpus with tokenization by whitespace.\n    corpus_sentences = [\n        \"the quick brown fox jumps over the lazy dog\",\n        \"the quick blue hare jumps over the sleepy dog\",\n        \"a fast brown fox leaps over a lazy hound\",\n        \"the slow tortoise crawls under the lazy dog\"\n    ]\n    all_words = \" \".join(corpus_sentences).split()\n    vocab = sorted(list(set(all_words)))\n    V = len(vocab)\n    word_to_idx = {word: i for i, word in enumerate(vocab)}\n\n    # 2. Construct a symmetric word–context co-occurrence matrix C.\n    r = 2\n    C = np.zeros((V, V))\n    for sentence in corpus_sentences:\n        tokens = sentence.split()\n        L = len(tokens)\n        for i in range(L):\n            center_word = tokens[i]\n            center_idx = word_to_idx[center_word]\n            start_context = max(0, i - r)\n            end_context = min(L, i + r + 1)\n            for j in range(start_context, end_context):\n                if i == j:\n                    continue\n                context_word = tokens[j]\n                context_idx = word_to_idx[context_word]\n                C[center_idx, context_idx] += 1\n\n    # 3. Convert counts to probabilities with additive smoothing.\n    tau = 1e-3\n    C_tilde = C + tau\n    N = np.sum(C_tilde)\n    P_wc = C_tilde / N  # Joint probability P(w, c)\n    P_w = np.sum(P_wc, axis=1)  # Word marginal P(w)\n    P_c = np.sum(P_wc, axis=0)  # Context marginal P(c)\n\n    # 4. Compute the Pointwise Mutual Information matrix M.\n    # To avoid numerical errors with log(0), we use the smoothed probabilities.\n    # The smoothing ensures P_wc, P_w, and P_c are all positive.\n    log_P_wc = np.log(P_wc)\n    log_P_w = np.log(P_w)\n    log_P_c = np.log(P_c)\n    M = log_P_wc - log_P_w[:, np.newaxis] - log_P_c[np.newaxis, :]\n\n    # Define the test suite.\n    test_cases = [\n        # (distribution_type, k, alpha)\n        ('uniform', 1, None),\n        ('uniform', 20, None),\n        ('unigram', 1, 0.75),\n        ('unigram', 20, 0.75),\n        ('unigram', 5, 1.0),\n    ]\n\n    results = []\n    \n    for dist_type, k, alpha in test_cases:\n        # 5. Compute the negative sampling distribution Q.\n        if dist_type == 'uniform':\n            Q = np.ones(V) / V\n        elif dist_type == 'unigram':\n            Q = P_c**alpha\n            Q /= np.sum(Q)\n        else:\n            raise ValueError(\"Unknown distribution type\")\n\n        # 6. Construct the shifted PMI matrix S(k,Q).\n        log_k = np.log(k)\n        # The term log(Q(c_j) / P(c_j)) depends only on the context j.\n        log_ratio_Q_over_Pc = np.log(Q) - log_P_c\n        # Broadcasting subtracts the scalar log_k and the row vector from each row of M.\n        S = M - log_k - log_ratio_Q_over_Pc[np.newaxis, :]\n        \n        # 7. Compute the singular values of S(k,Q) and extract the top two.\n        singular_values = np.linalg.svd(S, compute_uv=False)\n        sigma_1 = singular_values[0]\n        sigma_2 = singular_values[1]\n        \n        results.append([sigma_1, sigma_2])\n\n    # Final print statement in the exact required format.\n    formatted_results = []\n    for res_pair in results:\n        formatted_pair = f\"[{res_pair[0]:.6f},{res_pair[1]:.6f}]\"\n        formatted_results.append(formatted_pair)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Word embeddings are not built in a vacuum; their structure is deeply influenced by upstream linguistic decisions. This practice  employs the precise language of information theory to quantify how a common preprocessing step—lemmatization—alters a word's contextual distribution. By calculating the Jensen-Shannon divergence between the distributions of a lemma's different surface forms (e.g., \"bank\" vs. \"banks\"), you will gain a rigorous understanding of how such choices can either merge or muddle distinct word senses in the final embedding space.",
            "id": "3182931",
            "problem": "Consider the Distributional Hypothesis in Natural Language Processing, which states that words occurring in similar contexts tend to have similar meanings. In deep learning models that learn word representations, preprocessing decisions such as lemmatization (mapping surface forms to a common base form) can alter the empirical context distributions and thus affect learned embeddings. Formalize and quantify this effect by comparing context distributions conditioned on a lemma versus distributions conditioned on each surface form. Your task is to implement a program that, for each test case, computes two information-theoretic quantities derived from first principles.\n\nDefinitions and setup: Let a finite set of discrete contexts be denoted by $\\mathcal{C} = \\{c_1, c_2, \\ldots, c_K\\}$ and let a lemma $\\ell$ have surface forms $s_1, s_2, \\ldots, s_M$. For each surface form $s_m$, you are given nonnegative integer context counts $\\{n(c_k, s_m)\\}_{k=1}^K$. You must estimate the conditional context distribution $p(c \\mid s_m)$ from these counts using a scientifically justified estimator that avoids zero-probability issues for unseen contexts. Then, estimate the lemma-level context distribution $p(c \\mid \\ell)$ by aggregating counts across surface forms and applying the same estimation principle. Using these estimated distributions, quantify:\n- The average pairwise divergence among the surface-form distributions $p(c \\mid s_m)$, interpreted as how distinct the surface forms’ contextual usage is.\n- The average divergence between each surface-form distribution $p(c \\mid s_m)$ and the lemma-level distribution $p(c \\mid \\ell)$, interpreted as how much lemmatization conflates or separates the distinct contextual usages of the surface forms.\n\nEstimation requirement: Use add-$\\alpha$ smoothing (a special case of a symmetric Dirichlet prior) with smoothing parameter $\\alpha = 0.5$ to estimate the conditional distributions from counts. This means each context probability must be computed from the counts with additive smoothing and normalized over $\\mathcal{C}$, ensuring strictly positive probabilities for all contexts.\n\nDivergence requirement: Use Kullback-Leibler (KL) divergence and Jensen-Shannon (JS) divergence from information theory to define the quantities. For any two discrete distributions $P$ and $Q$ on the same support, KL divergence measures the expected log-likelihood ratio under $P$, and Jensen-Shannon divergence symmetrizes and smooths KL divergence by mixing distributions before comparison. All logarithms must be taken as natural logarithms. For the purposes of this task, the pairwise divergence among surface forms must be computed using Jensen-Shannon divergence between $p(c \\mid s_i)$ and $p(c \\mid s_j)$ for all unordered pairs $(i,j)$ with $i  j$, averaged over pairs. The divergence between surface forms and the lemma must be computed using Jensen-Shannon divergence between $p(c \\mid s_m)$ and $p(c \\mid \\ell)$ for all $m=1,\\ldots,M$, averaged over $m$.\n\nTest suite: Implement your program to process the following three test cases. Each case specifies $\\mathcal{C}$ by name (names are descriptive only; your computation is purely numeric) and provides the surface-form counts. You must treat these names as labels for indices $1,\\ldots,K$ and only use the counts.\n\n- Test Case $1$ (context set size $K = 4$): lemma $\\ell = \\text{\"run\"}$ with surface forms $s_1 = \\text{\"run\"}$, $s_2 = \\text{\"runs\"}$, $s_3 = \\text{\"running\"}$. Contexts are $\\mathcal{C} = [\\text{\"sports\"}, \\text{\"execute\"}, \\text{\"finance\"}, \\text{\"river\"}]$ with counts:\n  - $n(c, s_1) = [40, 10, 0, 0]$\n  - $n(c, s_2) = [20, 5, 0, 0]$\n  - $n(c, s_3) = [35, 8, 0, 0]$\n\n- Test Case $2$ (context set size $K = 4$): lemma $\\ell = \\text{\"bank\"}$ with surface forms $s_1 = \\text{\"bank\"}$, $s_2 = \\text{\"banks\"}$, $s_3 = \\text{\"banking\"}$. Contexts are $\\mathcal{C} = [\\text{\"finance\"}, \\text{\"river\"}, \\text{\"law\"}, \\text{\"sports\"}]$ with counts:\n  - $n(c, s_1) = [45, 5, 10, 0]$\n  - $n(c, s_2) = [10, 40, 0, 0]$\n  - $n(c, s_3) = [30, 0, 5, 0]$\n\n- Test Case $3$ (context set size $K = 4$): lemma $\\ell = \\text{\"bear\"}$ with surface forms $s_1 = \\text{\"bear\"}$, $s_2 = \\text{\"bears\"}$, $s_3 = \\text{\"bearing\"}$. Contexts are $\\mathcal{C} = [\\text{\"animal\"}, \\text{\"finance\"}, \\text{\"support\"}, \\text{\"weather\"}]$ with counts:\n  - $n(c, s_1) = [2, 1, 0, 0]$\n  - $n(c, s_2) = [0, 0, 1, 0]$\n  - $n(c, s_3) = [0, 0, 0, 1]$\n\nComputational details:\n- Estimate $p(c \\mid s_m)$ for each surface form with add-$\\alpha$ smoothing using $\\alpha = 0.5$ across the $K$ contexts.\n- Estimate $p(c \\mid \\ell)$ by aggregating raw counts across all surface forms and applying the same add-$\\alpha$ smoothing with $\\alpha = 0.5$.\n- Compute the average pairwise Jensen-Shannon divergence among the surface-form distributions, and compute the average Jensen-Shannon divergence between each surface form and the lemma distribution. Use natural logarithms for all divergence computations.\n\nFinal output format: Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. Each test case’s result must itself be a list of two floating-point numbers in the order $[\\text{mean\\_JS\\_among\\_forms}, \\text{mean\\_JS\\_form\\_vs\\_lemma}]$, with each number rounded to $6$ decimal places. For example, the final printed string must look like $[[x_1,y_1],[x_2,y_2],[x_3,y_3]]$ where each $x_i$ and $y_i$ are floats rounded to $6$ decimal places. No additional text should be printed.",
            "solution": "The problem requires the quantification of the contextual difference between a lemma's various surface forms and the aggregated lemma itself, based on the principles of the Distributional Hypothesis. This is accomplished by computing two information-theoretic quantities for three given test cases. The entire process rests upon a rigorous mathematical and statistical framework, which we will now detail.\n\nFirst, we must estimate the conditional context probability distributions from raw counts. Let $\\mathcal{C} = \\{c_1, \\ldots, c_K\\}$ be the set of $K$ contexts and $\\{s_1, \\ldots, s_M\\}$ be the set of $M$ surface forms for a lemma $\\ell$. We are given counts $n(c_k, s_m)$ for each context $c_k$ and surface form $s_m$. A direct maximum likelihood estimate $p(c_k \\mid s_m) = n(c_k, s_m) / \\sum_j n(c_j, s_m)$ is prone to issues with zero counts, which would incorrectly assign zero probability to unobserved events and lead to mathematical problems (e.g., logarithms of zero) in subsequent calculations.\n\nTo address this, the problem mandates the use of add-$\\alpha$ smoothing (also known as Laplace smoothing or a symmetric Dirichlet prior in a Bayesian framework). With a smoothing parameter $\\alpha = 0.5$, the estimated probability of context $c_k$ given surface form $s_m$ is:\n$$\np(c_k \\mid s_m) = \\frac{n(c_k, s_m) + \\alpha}{\\sum_{j=1}^K n(c_j, s_m) + K\\alpha}\n$$\nThis estimator ensures that every context is assigned a non-zero probability.\n\nNext, we estimate the context distribution for the lemma $\\ell$. This is achieved by first aggregating the counts for each context across all surface forms:\n$$\nn(c_k, \\ell) = \\sum_{m=1}^M n(c_k, s_m)\n$$\nThen, we apply the same add-$\\alpha$ smoothing rule to these aggregated counts to obtain the lemma's context distribution:\n$$\np(c_k \\mid \\ell) = \\frac{n(c_k, \\ell) + \\alpha}{\\sum_{j=1}^K n(c_j, \\ell) + K\\alpha}\n$$\n\nWith these probability distributions established, we can quantify the \"distance\" or \"divergence\" between them using tools from information theory. The problem specifies the use of Jensen-Shannon (JS) divergence. The JS divergence is a symmetrized and smoothed version of the more fundamental Kullback-Leibler (KL) divergence.\n\nFor two discrete probability distributions $P = \\{p_k\\}_{k=1}^K$ and $Q = \\{q_k\\}_{k=1}^K$ defined on the same support, the KL divergence from $Q$ to $P$ is given by:\n$$\nD_{KL}(P \\| Q) = \\sum_{k=1}^K p_k \\ln\\left(\\frac{p_k}{q_k}\\right)\n$$\nThe KL divergence is asymmetric, i.e., $D_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)$ in general. The Jensen-Shannon divergence resolves this by comparing both distributions to their average. Let $M = \\frac{1}{2}(P+Q)$ be the mixture distribution. The JS divergence is defined as:\n$$\nD_{JS}(P \\| Q) = \\frac{1}{2} D_{KL}(P \\| M) + \\frac{1}{2} D_{KL}(Q \\| M)\n$$\n$D_{JS}(P \\| Q)$ is symmetric, non-negative, and bounded (specifically, $0 \\le D_{JS}(P \\| Q) \\le \\ln 2$).\n\nWe must compute two specific quantities for each test case:\n1.  **The average pairwise divergence among surface forms**: This metric, $\\text{mean\\_JS\\_among\\_forms}$, quantifies the degree of contextual variation among the different surface forms of the lemma. A high value suggests that the surface forms are used in distinct contexts, and lemmatizing them might conflate different meanings. It is calculated by averaging the JS divergence over all unique pairs of surface forms:\n    $$\n    \\text{mean\\_JS\\_among\\_forms} = \\frac{1}{\\binom{M}{2}} \\sum_{1 \\le i  j \\le M} D_{JS}\\big(p(c \\mid s_i) \\,\\|\\, p(c \\mid s_j)\\big)\n    $$\n    For $M=3$, this is an average over $\\binom{3}{2}=3$ pairs.\n\n2.  **The average divergence between each surface form and the lemma**: This metric, $\\text{mean\\_JS\\_form\\_vs\\_lemma}$, measures how well the aggregated lemma distribution represents the context of each individual surface form. A high value indicates that the lemma's \"average\" context is a poor representative for one or more of its constituent forms. It is calculated by averaging the JS divergence between each surface form's distribution and the lemma's distribution:\n    $$\n    \\text{mean\\_JS\\_form\\_vs\\_lemma} = \\frac{1}{M} \\sum_{m=1}^M D_{JS}\\big(p(c \\mid s_m) \\,\\|\\, p(c \\mid \\ell)\\big)\n    $$\n    For $M=3$, this is an average over $3$ pairs.\n\nThe algorithm for each test case proceeds as follows:\na. For each of the $M$ surface forms, take the $K$-dimensional vector of counts and compute the smoothed probability distribution $p(c \\mid s_m)$ using $\\alpha=0.5$.\nb. Sum the count vectors across all $M$ surface forms to get the aggregated count vector for the lemma, $n(c, \\ell)$.\nc. Compute the smoothed probability distribution for the lemma, $p(c \\mid \\ell)$, from its aggregated counts.\nd. Calculate the pairwise JS divergences, $D_{JS}(p(c \\mid s_i) \\| p(c \\mid s_j))$, for all $1 \\le i  j \\le M$. Average these values to find the first required quantity.\ne. Calculate the JS divergences between each surface form and the lemma, $D_{JS}(p(c \\mid s_m) \\| p(c \\mid \\ell))$, for all $m=1, \\ldots, M$. Average these values to find the second required quantity.\nf. Report the two averaged quantities, rounded to six decimal places.\n\nThis procedure is applied to each of the three test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef solve():\n    \"\"\"\n    Computes information-theoretic quantities to measure the effect of lemmatization\n    on context distributions for a given set of test cases.\n    \"\"\"\n\n    def calculate_distribution(counts: np.ndarray, alpha: float) - np.ndarray:\n        \"\"\"\n        Estimates a probability distribution from counts using add-alpha smoothing.\n\n        Args:\n            counts: A NumPy array of non-negative integer counts.\n            alpha: The smoothing parameter.\n\n        Returns:\n            A NumPy array representing the smoothed probability distribution.\n        \"\"\"\n        if not isinstance(counts, np.ndarray):\n            counts = np.array(counts, dtype=float)\n        \n        k = len(counts)\n        total_counts = np.sum(counts)\n        numerator = counts + alpha\n        denominator = total_counts + k * alpha\n        return numerator / denominator\n\n    def js_divergence(p: np.ndarray, q: np.ndarray) - float:\n        \"\"\"\n        Calculates the Jensen-Shannon divergence between two probability distributions.\n        All logarithms are natural logarithms.\n\n        Args:\n            p: A NumPy array for the first probability distribution.\n            q: A NumPy array for the second probability distribution.\n\n        Returns:\n            The Jensen-Shannon divergence as a float.\n        \"\"\"\n        m = 0.5 * (p + q)\n        # scipy.stats.entropy(pk, qk) calculates KL divergence D_KL(pk || qk)\n        kl_p_m = entropy(p, m)\n        kl_q_m = entropy(q, m)\n        return 0.5 * (kl_p_m + kl_q_m)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1: lemma \"run\"\n        {\n            \"surface_form_counts\": [\n                [40, 10, 0, 0],\n                [20, 5, 0, 0],\n                [35, 8, 0, 0],\n            ]\n        },\n        # Test Case 2: lemma \"bank\"\n        {\n            \"surface_form_counts\": [\n                [45, 5, 10, 0],\n                [10, 40, 0, 0],\n                [30, 0, 5, 0],\n            ]\n        },\n        # Test Case 3: lemma \"bear\"\n        {\n            \"surface_form_counts\": [\n                [2, 1, 0, 0],\n                [0, 0, 1, 0],\n                [0, 0, 0, 1],\n            ]\n        },\n    ]\n\n    alpha = 0.5\n    all_results = []\n\n    for case in test_cases:\n        counts_per_form = np.array(case[\"surface_form_counts\"], dtype=float)\n        m, k = counts_per_form.shape\n\n        # Estimate distributions for each surface form\n        dists_s_m = [calculate_distribution(counts, alpha) for counts in counts_per_form]\n\n        # Aggregate counts and estimate distribution for the lemma\n        lemma_counts = np.sum(counts_per_form, axis=0)\n        dist_lemma = calculate_distribution(lemma_counts, alpha)\n\n        # 1. Calculate average pairwise JS divergence among surface forms\n        pairwise_js_divs = []\n        if m  1:\n            for i in range(m):\n                for j in range(i + 1, m):\n                    div = js_divergence(dists_s_m[i], dists_s_m[j])\n                    pairwise_js_divs.append(div)\n        \n        mean_js_among_forms = np.mean(pairwise_js_divs) if pairwise_js_divs else 0.0\n\n        # 2. Calculate average JS divergence between each surface form and the lemma\n        form_lemma_js_divs = []\n        for i in range(m):\n            div = js_divergence(dists_s_m[i], dist_lemma)\n            form_lemma_js_divs.append(div)\n        \n        mean_js_form_vs_lemma = np.mean(form_lemma_js_divs) if form_lemma_js_divs else 0.0\n        \n        all_results.append([mean_js_among_forms, mean_js_form_vs_lemma])\n\n    # Format the final output string as specified: [[x1,y1],[x2,y2],[x3,y3]]\n    result_strings = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in all_results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}