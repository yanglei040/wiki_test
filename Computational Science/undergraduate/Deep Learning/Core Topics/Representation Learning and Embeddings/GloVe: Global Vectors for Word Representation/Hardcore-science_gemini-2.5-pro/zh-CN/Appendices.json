{
    "hands_on_practices": [
        {
            "introduction": "在训练任何机器学习模型之前，验证损失函数及其梯度的实现是否正确至关重要。本实践将指导您实现 GloVe 目标函数及其解析梯度，然后使用梯度检验的数值方法来严格验证您的代码。这是构建可靠模型的基石。",
            "id": "3130312",
            "problem": "给定一个小的共现矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{n_w \\times n_c}$，其词汇表包含 $n_w$ 个目标词和 $n_c$ 个上下文词。考虑在 Global Vectors (GloVe) 中使用的加权最小二乘目标函数，定义如下。设 $W \\in \\mathbb{R}^{n_w \\times d}$ 为目标词嵌入，$C \\in \\mathbb{R}^{n_c \\times d}$ 为上下文嵌入，$b \\in \\mathbb{R}^{n_w}$ 为目标偏置，$d \\in \\mathbb{R}^{n_c}$ 为上下文偏置。权重函数 $f(x)$ 定义为：当 $x  0$ 时，$f(x) = \\min\\{(x / x_{\\max})^\\alpha, 1\\}$；当 $x=0$ 时，$f(0) = 0$。其中 $\\alpha \\in (0,1]$ 和 $x_{\\max}  0$ 是超参数。GloVe 的目标函数是\n$$\nJ(W, C, b, d; X) = \\sum_{i=1}^{n_w} \\sum_{j=1}^{n_c} f(X_{ij}) \\left( W_i^\\top C_j + b_i + d_j - \\log X_{ij} \\right)^2,\n$$\n约定当 $X_{ij} = 0$ 时，对应的项贡献为零，因为 $f(0) = 0$。所有对数均为自然对数。\n\n您需要实现关于 $W$、$C$、$b$ 和 $d$ 的解析梯度，然后通过从第一性原理推导的中心有限差分进行梯度检验来验证其正确性。使用的基本原理包括：梯度作为偏导数向量的定义、可微函数的一阶泰勒展开，以及标量函数 $J(\\theta)$ 在参数向量 $\\theta$ 处的中心差分近似：\n$$\n\\frac{\\partial J}{\\partial \\theta_k} \\approx \\frac{J(\\theta + h e_k) - J(\\theta - h e_k)}{2h},\n$$\n其中 $e_k$ 是第 $k$ 个坐标基向量，$h  0$ 是一个小的步长。\n\n要求：\n- 实现一个计算 $J(W, C, b, d; X)$ 的函数，在 $X_{ij} = 0$ 时不产生未定义值。您必须确保在 $X_{ij} = 0$ 时不使用任何涉及 $\\log X_{ij}$ 的表达式，从而避免计算 $\\log 0$；等效地，您可以使用掩码计算，在 $X_{ij} = 0$ 的地方将 $\\log X_{ij}$ 设置为 $0$，并依赖 $f(0) = 0$ 来使这些项无效。\n- 基于目标函数 $J$ 推导并实现关于 $W$、$C$、$b$ 和 $d$ 的解析梯度。仅使用多元微积分和线性代数的标准法则。您的实现必须在索引 $i$ 和 $j$ 上完全向量化。\n- 使用中心有限差分和一个小的步长 $h$ 实现梯度检验，以近似 $J$ 相对于所有参数 $(W, C, b, d)$（打包成单个向量 $\\theta$）的梯度。对于每个测试用例，计算解析梯度和数值梯度在 $\\theta$ 所有坐标上的最大绝对差异。\n\n测试套件：\n使用以下四个测试用例。对于所有随机生成，您必须使用指定的伪随机种子以确保确定性输出。对于每个测试用例，使用从均值为 $0$、标准差为 $0.1$ 的正态分布中抽取的独立样本来初始化参数 $W$、$C$、$b$ 和 $d$。\n\n1. 包含混合计数和零值的正常路径：\n   - $n_w = 3$, $n_c = 4$, $d = 2$, 种子 $0$。\n   - 通过从 $\\{0,1,2,3,4,5,6,7\\}$ 中均匀抽取独立整数来生成 $X$。\n   - $\\alpha = 0.75$, $x_{\\max} = 10$。\n2. 全为零的边界情况：\n   - $n_w = 2$, $n_c = 2$, $d = 3$, 种子 $1$。\n   - $X$ 是全零矩阵（每个条目都为 $0$）。\n   - $\\alpha = 0.75$, $x_{\\max} = 10$。\n3. 全为一的角落情况：\n   - $n_w = 5$, $n_c = 3$, $d = 2$, 种子 $2$。\n   - $X$ 是全一矩阵（每个条目都为 $1$）。\n   - $\\alpha = 0.5$, $x_{\\max} = 1$。\n4. 混合极端值：\n   - $n_w = 4$, $n_c = 4$, $d = 1$, 种子 $42$。\n   - 通过以概率 $\\{0.4, 0.5, 0.1\\}$ 从集合 $\\{0, 1, 50\\}$ 中独立抽样每个条目来生成 $X$。\n   - $\\alpha = 0.75$, $x_{\\max} = 100$。\n\n数值微分：\n- 使用中心差分，步长 $h = 10^{-6}$。\n- 按以下顺序将所有参数 $(W, C, b, d)$ 打包成单个参数向量 $\\theta$：$W$ 的行主序扁平化，然后是 $C$ 的行主序扁平化，然后是 $b$，最后是 $d$。\n\n输出：\n- 对于每个测试用例，计算解析梯度和数值梯度在 $\\theta$ 所有坐标上的最大绝对差异。\n- 您的程序应产生单行输出，包含一个用方括号括起来的逗号分隔列表的结果（例如，$[r_1,r_2,r_3,r_4]$），其中每个 $r_k$ 是测试用例 $k$ 的差异，四舍五入到六位小数。\n- 此问题中没有物理单位或角度。所有量均为无量纲实数。\n\n您的解决方案必须是一个完整、可运行的程序，执行所有计算并以确切指定的格式打印最终结果。",
            "solution": "用户提供了一个问题，要求推导和实现 GloVe 目标函数的梯度，并使用数值梯度检验进行验证。\n\n首先，需要对问题陈述进行严格分析。问题中符号 $d$ 用于两个不同的数学对象：第一，作为表示词嵌入维度的整数，其中 $W \\in \\mathbb{R}^{n_w \\times d}$ 和 $C \\in \\mathbb{R}^{n_c \\times d}$；第二，作为上下文偏置向量 $d \\in \\mathbb{R}^{n_c}$。这种符号重载是一个产生歧义的重大缺陷。严谨的数学文本会使用不同的符号。为清晰起见，在本解释中，我们用 $d_{dim}$ 表示嵌入维度，用 $\\vec{\\beta} \\in \\mathbb{R}^{n_c}$ 表示上下文偏置向量。解决了这个歧义后，问题在其他方面是定义明确的，科学上基于机器学习和微积分的原理，并且可以求解。因此，模型的参数是目标词嵌入 $W \\in \\mathbb{R}^{n_w \\times d_{dim}}$、上下文词嵌入 $C \\in \\mathbb{R}^{n_c \\times d_{dim}}$、目标偏置 $b \\in \\mathbb{R}^{n_w}$ 和上下文偏置 $\\vec{\\beta} \\in \\mathbb{R}^{n_c}$。\n\n要最小化的目标函数是：\n$$\nJ(W, C, b, \\vec{\\beta}; X) = \\sum_{i=1}^{n_w} \\sum_{j=1}^{n_c} f(X_{ij}) \\left( W_i^\\top C_j + b_i + \\beta_j - \\log X_{ij} \\right)^2\n$$\n其中 $W_i$ 是 $W$ 的第 $i$ 行，$C_j$ 是 $C$ 的第 $j$ 行，$b_i$ 是 $b$ 的第 $i$ 个分量，$\\beta_j$ 是 $\\vec{\\beta}$ 的第 $j$ 个分量。权重函数是：当 $x  0$ 时，$f(x) = \\min\\{(x/x_{\\max})^\\alpha, 1\\}$；当 $f(0)=0$ 时。涉及 $\\log X_{ij}$ 的项仅在 $X_{ij}  0$ 时计算；否则，由于 $f(0)=0$，整个 $(i,j)$ 对的项为零。\n\n为了实现梯度检验，我们必须首先推导 $J$ 关于每个参数的解析梯度。我们应用多元微积分的链式法则。设条目 $(i,j)$ 的误差项为：\n$$\nE_{ij} = W_i^\\top C_j + b_i + \\beta_j - \\log X_{ij}\n$$\n目标函数可以写成 $J = \\sum_{i,j} f(X_{ij}) E_{ij}^2$。关于任意参数 $\\theta$ 的偏导数是：\n$$\n\\frac{\\partial J}{\\partial \\theta} = \\sum_{i,j} \\frac{\\partial}{\\partial \\theta} \\left[ f(X_{ij}) E_{ij}^2 \\right] = \\sum_{i,j} f(X_{ij}) \\cdot 2 E_{ij} \\cdot \\frac{\\partial E_{ij}}{\\partial \\theta}\n$$\n项 $f(X_{ij})$ 相对于模型参数是常数。我们定义一个中间矩阵 $S \\in \\mathbb{R}^{n_w \\times n_c}$，其条目为 $S_{ij} = 2 f(X_{ij}) E_{ij}$。梯度的表达式变为：\n$$\n\\frac{\\partial J}{\\partial \\theta} = \\sum_{i,j} S_{ij} \\frac{\\partial E_{ij}}{\\partial \\theta}\n$$\n\n1.  **关于目标嵌入 $W$ 的梯度**：\n    我们计算关于矩阵 $W$ 的单个分量 $W_{ik}$（第 $i$ 行，第 $k$ 列）的偏导数。\n    $$\n    \\frac{\\partial E_{mn}}{\\partial W_{ik}} = \\frac{\\partial}{\\partial W_{ik}} \\left( \\sum_{l=1}^{d_{dim}} W_{ml} C_{nl} + b_m + \\beta_n - \\log X_{mn} \\right) = \\delta_{mi} \\delta_{lk} C_{nl} \\implies \\frac{\\partial W_m^\\top C_n}{\\partial W_{ik}} = \\delta_{mi} C_{nk}\n    $$\n    这里，$\\delta$ 是克罗内克 $\\delta$。仅当 $m=i$ 时导数非零。\n    $$\n    \\frac{\\partial J}{\\partial W_{ik}} = \\sum_{j=1}^{n_c} S_{ij} \\frac{\\partial E_{ij}}{\\partial W_{ik}} = \\sum_{j=1}^{n_c} S_{ij} C_{jk}\n    $$\n    这个表达式代表了矩阵乘积的第 $(i,k)$ 个元素。因此，梯度矩阵 $\\nabla_W J$ 由下式给出：\n    $$\n    \\nabla_W J = [S_{ij}]_{i,j} \\cdot C\n    $$\n    其中 $[S_{ij}]_{i,j}$ 是元素为 $S_{ij}$ 的矩阵。\n\n2.  **关于上下文嵌入 $C$ 的梯度**：\n    类似地，对于矩阵 $C$ 的一个分量 $C_{jk}$：\n    $$\n    \\frac{\\partial E_{mn}}{\\partial C_{jk}} = \\frac{\\partial}{\\partial C_{jk}} \\left( \\sum_{l=1}^{d_{dim}} W_{ml} C_{nl} \\right) = \\delta_{nj} \\delta_{lk} W_{ml} \\implies \\frac{\\partial W_m^\\top C_n}{\\partial C_{jk}} = \\delta_{nj} W_{mk}\n    $$\n    仅当 $n=j$ 时导数非零。\n    $$\n    \\frac{\\partial J}{\\partial C_{jk}} = \\sum_{i=1}^{n_w} S_{ij} \\frac{\\partial E_{ij}}{\\partial C_{jk}} = \\sum_{i=1}^{n_w} S_{ij} W_{ik}\n    $$\n    这是矩阵乘积 $S^\\top W$ 的第 $(j,k)$ 个元素。梯度矩阵 $\\nabla_C J$ 是：\n    $$\n    \\nabla_C J = [S_{ij}]_{i,j}^\\top \\cdot W\n    $$\n\n3.  **关于目标偏置 $b$ 的梯度**：\n    对于向量 $b$ 的一个分量 $b_k$：\n    $$\n    \\frac{\\partial E_{ij}}{\\partial b_k} = \\delta_{ik}\n    $$\n    仅当 $i=k$ 时导数非零。\n    $$\n    \\frac{\\partial J}{\\partial b_k} = \\sum_{i,j} S_{ij} \\delta_{ik} = \\sum_{j=1}^{n_c} S_{kj}\n    $$\n    梯度向量 $\\nabla_b J$ 是通过对矩阵 $S$ 的行求和得到的。\n\n4.  **关于上下文偏置 $\\vec{\\beta}$ 的梯度**：\n    对于向量 $\\vec{\\beta}$ 的一个分量 $\\beta_k$：\n    $$\n    \\frac{\\partial E_{ij}}{\\partial \\beta_k} = \\delta_{jk}\n    $$\n    仅当 $j=k$ 时导数非零。\n    $$\n    \\frac{\\partial J}{\\partial \\beta_k} = \\sum_{i,j} S_{ij} \\delta_{jk} = \\sum_{i=1}^{n_w} S_{ik}\n    $$\n    梯度向量 $\\nabla_{\\vec{\\beta}} J$ 是通过对矩阵 $S$ 的列求和得到的。\n\n这些解析梯度必须进行数值验证。梯度检验基于函数的一阶泰勒展开。对于一个向量参数 $\\vec{\\theta}$ 的标量函数 $J(\\vec{\\theta})$，中心差分公式为其第 $k$ 个分量 $\\theta_k$ 的偏导数提供了一个二阶精确近似：\n$$\n\\frac{\\partial J}{\\partial \\theta_k} \\approx \\frac{J(\\vec{\\theta} + h \\vec{e}_k) - J(\\vec{\\theta} - h \\vec{e}_k)}{2h}\n$$\n其中 $\\vec{e}_k$ 是在第 $k$ 个位置为 $1$、其他位置为零的标准基向量，$h$ 是一个小的步长（例如，$h=10^{-6}$）。\n\n要应用此方法，我们首先将所有模型参数 $(W, C, b, \\vec{\\beta})$ 按指定顺序“展开”成一个大的单一参数向量 $\\vec{\\theta}$：$W$（行主序扁平化）、$C$（行主序扁平化）、$b$ 和 $\\vec{\\beta}$ 连接起来。然后，我们遍历 $\\vec{\\theta}$ 的每个分量 $\\theta_k$，使用上述公式计算数值导数，并将其与解析梯度向量（也按相同顺序展开）的相应分量进行比较。解析梯度和数值梯度在所有分量上的最大绝对差值将作为最终的正确性度量。\n\n实现将包括：\n1.  一个计算成本 $J$ 的函数，安全地处理 $X_{ij}=0$ 的情况。\n2.  一个使用向量化操作计算解析梯度 $\\nabla_W J, \\nabla_C J, \\nabla_b J, \\nabla_{\\vec{\\beta}} J$ 的函数。\n3.  一个主过程，对于每个测试用例：\n    a. 初始化参数。\n    b. 计算解析梯度向量 $\\vec{g}_{analytic}$。\n    c. 使用中心差分法迭代计算数值梯度向量 $\\vec{g}_{numeric}$。\n    d. 计算差异 $\\max|\\vec{g}_{analytic} - \\vec{g}_{numeric}|$。\n这个严谨的过程确保了梯度实现的正确性，这是训练机器学习模型的一个基础步骤。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the GloVe gradient checking for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"n_w\": 3, \"n_c\": 4, \"dim\": 2, \"seed\": 0, \"alpha\": 0.75, \"x_max\": 10,\n            \"X_gen\": lambda n_w, n_c: np.random.randint(0, 8, size=(n_w, n_c)).astype(float)\n        },\n        {\n            \"n_w\": 2, \"n_c\": 2, \"dim\": 3, \"seed\": 1, \"alpha\": 0.75, \"x_max\": 10,\n            \"X_gen\": lambda n_w, n_c: np.zeros((n_w, n_c), dtype=float)\n        },\n        {\n            \"n_w\": 5, \"n_c\": 3, \"dim\": 2, \"seed\": 2, \"alpha\": 0.5, \"x_max\": 1,\n            \"X_gen\": lambda n_w, n_c: np.ones((n_w, n_c), dtype=float)\n        },\n        {\n            \"n_w\": 4, \"n_c\": 4, \"dim\": 1, \"seed\": 42, \"alpha\": 0.75, \"x_max\": 100,\n            \"X_gen\": lambda n_w, n_c: np.random.choice([0, 1, 50], size=(n_w, n_c), p=[0.4, 0.5, 0.1]).astype(float)\n        },\n    ]\n\n    h = 1e-6\n    results = []\n\n    for case in test_cases:\n        n_w, n_c, dim, seed = case[\"n_w\"], case[\"n_c\"], case[\"dim\"], case[\"seed\"]\n        alpha, x_max = case[\"alpha\"], case[\"x_max\"]\n        \n        np.random.seed(seed)\n        \n        X = case[\"X_gen\"](n_w, n_c)\n        W = np.random.normal(0, 0.1, size=(n_w, dim))\n        C = np.random.normal(0, 0.1, size=(n_c, dim))\n        b = np.random.normal(0, 0.1, size=(n_w,))\n        d_biases = np.random.normal(0, 0.1, size=(n_c,))\n\n        # --- Helper functions for parameter packing/unpacking ---\n        param_sizes = {\n            'W': W.size, 'C': C.size, 'b': b.size, 'd_biases': d_biases.size\n        }\n        \n        def pack_params(W, C, b, d_biases):\n            return np.concatenate([W.flatten(), C.flatten(), b.flatten(), d_biases.flatten()])\n\n        def unpack_params(theta):\n            idx = 0\n            W_flat = theta[idx : idx + param_sizes['W']]\n            W_unpacked = W_flat.reshape(n_w, dim)\n            idx += param_sizes['W']\n            \n            C_flat = theta[idx : idx + param_sizes['C']]\n            C_unpacked = C_flat.reshape(n_c, dim)\n            idx += param_sizes['C']\n            \n            b_unpacked = theta[idx : idx + param_sizes['b']]\n            idx += param_sizes['b']\n\n            d_biases_unpacked = theta[idx:]\n            return W_unpacked, C_unpacked, b_unpacked, d_biases_unpacked\n        \n        # --- Cost and Analytic Gradient Calculation ---\n        def compute_cost_and_grads(W, C, b, d_biases, X, alpha, x_max):\n            # Mask for non-zero co-occurrences\n            non_zero_mask = X > 0\n            \n            # Weighting function f(X_ij)\n            weights = np.zeros_like(X, dtype=float)\n            if np.any(non_zero_mask):\n                x_scaled = X[non_zero_mask] / x_max\n                f_vals = np.minimum(1.0, np.power(x_scaled, alpha))\n                weights[non_zero_mask] = f_vals\n\n            # Safely compute log(X_ij)\n            log_X = np.zeros_like(X, dtype=float)\n            if np.any(non_zero_mask):\n                log_X[non_zero_mask] = np.log(X[non_zero_mask])\n\n            # Error term: W_i^T C_j + b_i + d_j - log(X_ij)\n            errors = W @ C.T + b[:, np.newaxis] + d_biases[np.newaxis, :] - log_X\n            \n            # Cost J\n            cost = np.sum(weights * (errors ** 2))\n            \n            # Common term for gradients\n            s_term = weights * errors\n            \n            # Analytic gradients\n            grad_W = 2 * s_term @ C\n            grad_C = 2 * s_term.T @ W\n            grad_b = 2 * np.sum(s_term, axis=1)\n            grad_d_biases = 2 * np.sum(s_term, axis=0)\n            \n            return cost, grad_W, grad_C, grad_b, grad_d_biases\n        \n        # --- Numerical Gradient Calculation (Gradient Checking) ---\n        theta = pack_params(W, C, b, d_biases)\n        num_grads = np.zeros_like(theta)\n        \n        for i in range(len(theta)):\n            # J(theta + h*e_i)\n            theta_plus = theta.copy()\n            theta_plus[i] += h\n            W_p, C_p, b_p, d_p = unpack_params(theta_plus)\n            cost_plus, _, _, _, _ = compute_cost_and_grads(W_p, C_p, b_p, d_p, X, alpha, x_max)\n            \n            # J(theta - h*e_i)\n            theta_minus = theta.copy()\n            theta_minus[i] -= h\n            W_m, C_m, b_m, d_m = unpack_params(theta_minus)\n            cost_minus, _, _, _, _ = compute_cost_and_grads(W_m, C_m, b_m, d_m, X, alpha, x_max)\n            \n            # Central difference\n            num_grads[i] = (cost_plus - cost_minus) / (2 * h)\n\n        # --- Comparison ---\n        _, grad_W, grad_C, grad_b, grad_d_biases = compute_cost_and_grads(W, C, b, d_biases, X, alpha, x_max)\n        analytic_grads = pack_params(grad_W, grad_C, grad_b, grad_d_biases)\n        \n        discrepancy = np.max(np.abs(analytic_grads - num_grads))\n        results.append(discrepancy)\n\n    # Final formatted output\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "GloVe 模型的成功部分归功于其特殊设计的权重函数，该函数能够处理自然语言中差异巨大的共现频率。本练习将通过推导梯度，并比较标准 GloVe 权重与简单均匀权重方案下模型的稳定性，来深入探讨这一设计背后的原因。通过这个过程，您将更深刻地理解该函数如何防止罕见或极其常见的词对破坏训练过程的稳定性。",
            "id": "3130245",
            "problem": "给定从共现矩阵中学习词向量的 Global Vectors (GloVe) 目标函数。设词汇表大小为 $V$，嵌入维度为 $d$。对于每个词索引 $i \\in \\{1,\\dots,V\\}$ 和上下文索引 $j \\in \\{1,\\dots,V\\}$，令 $w_i \\in \\mathbb{R}^d$ 表示词向量，$\\tilde{w}_j \\in \\mathbb{R}^d$ 表示上下文向量，$b_i \\in \\mathbb{R}$ 表示词偏置，$\\tilde{b}_j \\in \\mathbb{R}$ 表示上下文偏置，$X_{ij} \\in \\mathbb{R}_{\\ge 0}$ 表示共现计数。考虑以下目标函数\n$$\nJ = \\sum_{i=1}^V \\sum_{j=1}^V f(X_{ij}) \\,\\big(w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij}\\big)^2,\n$$\n其中，我们理解求和实际上仅限于 $X_{ij}  0$ 的索引，因为 $\\log X_{ij}$ 仅对严格为正的参数有定义。函数 $f: \\mathbb{R}_{\\ge 0} \\to \\mathbb{R}_{\\ge 0}$ 是一个非负权重函数。\n\n任务A（推导）。仅从多变量微积分的基本法则（求和的线性性质、链式法则以及内积的梯度）出发，推导任意 $i$ 和 $j$ 的梯度 $\\nabla_{w_i} J$ 和 $\\nabla_{\\tilde{w}_j} J$。你的推导必须明确说明为何将索引限制在 $X_{ij}  0$ 的范围内，并解释权重函数 $f(\\cdot)$ 在梯度中的作用。\n\n任务B（实现）。在一个程序中实现所推导的梯度，以便为给定的参数值精确计算它们。使用两种权重函数：\n- GloVe 权重函数，定义为\n$$\nf_{\\mathrm{std}}(x) = \\begin{cases}\n\\left(\\dfrac{x}{x_{\\max}}\\right)^\\alpha,   \\text{若 } x  x_{\\max},\\\\\n1,   \\text{其他情况},\n\\end{cases}\n$$\n其超参数为 $x_{\\max} = 100$ 和 $\\alpha = 0.75$。\n- 仅包含正共现的均匀权重函数，\n$$\nf_{\\mathrm{uni}}(x) = \\begin{cases}\n1,   \\text{若 } x  0,\\\\\n0,   \\text{若 } x = 0.\n\\end{cases}\n$$\n\n任务C（稀疏性下的稳定性分析）。使用推导出的梯度和实现，通过比较在 $f_{\\mathrm{uni}}$ 和 $f_{\\mathrm{std}}$ 下的最大绝对梯度分量（所有词梯度和上下文梯度的所有分量的 $\\ell_\\infty$-范数），定量分析在稀疏 $X_{ij}$ 条件下的稳定性。对于下面的每个测试用例，计算比率\n$$\n\\rho = \\frac{\\text{在 } f_{\\mathrm{uni}} \\text{ 下的 }\\max\\{|\\nabla_{w} J|_\\infty,\\,|\\nabla_{\\tilde{w}} J|_\\infty\\}}{\\text{在 } f_{\\mathrm{std}} \\text{ 下的 }\\max\\{|\\nabla_{w} J|_\\infty,\\,|\\nabla_{\\tilde{w}} J|_\\infty\\}},\n$$\n其中 $|\\nabla_{w} J|_\\infty$ 表示所有 $\\nabla_{w_i} J$ 的所有条目中的最大绝对值， $|\\nabla_{\\tilde{w}} J|_\\infty$ 也类似。同时报告在每种权重函数下，计算出的梯度是否为有限值（不含 NaN 或无穷大值）。\n\n测试套件。使用 $V = 4$ 和 $d = 3$。使用固定的种子 $42$ 进行确定性初始化，以确保结果可复现：\n- 使用固定的种子 $42$，从均值为零、标准差为 $0.5$ 的正态分布中独立初始化 $w_i$ 和 $\\tilde{w}_j$ 的所有条目。\n- 将所有偏置 $b_i$ 和 $\\tilde{b}_j$ 初始化为 $0$。\n\n使用以下三个大小为 $4 \\times 4$ 的共现矩阵：\n- 情况A（中等稀疏，计数值小）：\n$$\nX^{(A)} =\n\\begin{bmatrix}\n0   3   1   0\\\\\n2   0   0   5\\\\\n0   1   0   0\\\\\n4   0   0   0\n\\end{bmatrix}.\n$$\n- 情况B（极端稀疏，单一共现）：\n$$\nX^{(B)} =\n\\begin{bmatrix}\n0   1   0   0\\\\\n0   0   0   0\\\\\n0   0   0   0\\\\\n0   0   0   0\n\\end{bmatrix}.\n$$\n- 情况C（仅有大计数值，均高于 $x_{\\max}$）：\n$$\nX^{(C)} =\n\\begin{bmatrix}\n1000   0   0   0\\\\\n0   500   0   0\\\\\n0   0   200   0\\\\\n0   0   0   800\n\\end{bmatrix}.\n$$\n\n数值输出规范。对于每种情况，计算一个列表，包含：\n- 上面定义的比率 $\\rho$，四舍五入到 $6$ 位小数，\n- 一个布尔值，指示在 $f_{\\mathrm{std}}$ 下的梯度是否全部有限，\n- 一个布尔值，指示在 $f_{\\mathrm{uni}}$ 下的梯度是否全部有限。\n\n你的程序应该生成单行输出，其中包含这三种情况的结果，形式为由这三个元素列表组成的逗号分隔列表，并用方括号括起来。例如，打印的输出必须具有以下形式：\n$[\\,[\\rho_A,\\mathrm{finite}^{(A)}_{\\mathrm{std}},\\mathrm{finite}^{(A)}_{\\mathrm{uni}}],[\\rho_B,\\mathrm{finite}^{(B)}_{\\mathrm{std}},\\mathrm{finite}^{(B)}_{\\mathrm{uni}}],[\\rho_C,\\mathrm{finite}^{(C)}_{\\mathrm{std}},\\mathrm{finite}^{(C)}_{\\mathrm{uni}}]\\,]$,\n其中每个 $\\rho$ 是一个四舍五入到 $6$ 位的小数，布尔值是不带引号的布尔字面量。此问题不涉及物理单位或角度。输出中的所有实数必须是标准十进制数字。程序必须是自包含的，并且不需要任何输入。",
            "solution": "该问题要求推导 GloVe 目标函数的梯度，针对两种不同的权重方案实现这些梯度，并对它们在不同数据稀疏条件下的稳定性进行定量分析。\n\n### 任务A（推导）\n目标函数由下式给出：\n$$\nJ = \\sum_{i=1}^V \\sum_{j=1}^V f(X_{ij}) \\,\\big(w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij}\\big)^2\n$$\n其中，求和被理解为仅限于共现计数 $X_{ij}  0$ 的索引对 $(i, j)$。这个限制由权重函数 $f(x)$ 自然处理，该函数定义为 $f(0)=0$。对于任何 $X_{ij}=0$ 的索引对 $(i, j)$，求和中对应的项为 $f(0) \\times (\\dots)^2 = 0 \\times (\\dots)^2 = 0$，从而巧妙地避免了计算 $\\log(0)$ 的问题。\n\n**1. 关于词向量 $w_k$ 的梯度**\n\n我们寻求计算任意词索引 $k \\in \\{1, \\dots, V\\}$ 的梯度 $\\nabla_{w_k} J$。我们首先应用梯度算子的线性性质：\n$$\n\\nabla_{w_k} J = \\nabla_{w_k} \\left( \\sum_{i=1}^V \\sum_{j=1}^V f(X_{ij}) \\,\\big(w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij}\\big)^2 \\right) = \\sum_{i=1}^V \\sum_{j=1}^V \\nabla_{w_k} \\left( f(X_{ij}) \\,\\big(w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij}\\big)^2 \\right)\n$$\n梯度 $\\nabla_{w_k}$ 作用于模型的参数。求和内部的项仅在求和索引 $i$ 等于 $k$ 时才依赖于 $w_k$。对于 $i \\ne k$，向量 $w_i$ 与 $w_k$ 无关，因此其导数为零向量。因此，我们可以通过仅保留 $i=k$ 的项来简化表达式：\n$$\n\\nabla_{w_k} J = \\sum_{j=1}^V \\nabla_{w_k} \\left( f(X_{kj}) \\,\\big(w_k^\\top \\tilde{w}_j + b_k + \\tilde{b}_j - \\log X_{kj}\\big)^2 \\right)\n$$\n对于这个和中的每一项，权重因子 $f(X_{kj})$ 是关于 $w_k$ 的标量常数。我们可以将其移到梯度算子之外。让我们为一对 $(k, j)$ 定义标量误差项：\n$$\nS_{kj} = w_k^\\top \\tilde{w}_j + b_k + \\tilde{b}_j - \\log X_{kj}\n$$\n表达式变为：\n$$\n\\nabla_{w_k} J = \\sum_{j=1}^V f(X_{kj}) \\nabla_{w_k} (S_{kj}^2)\n$$\n使用向量导数的链式法则 $\\nabla_v g(h(v)) = g'(h(v)) \\nabla_v h(v)$，其中 $g(h) = h^2$ 且 $h(w_k) = S_{kj}$，我们得到：\n$$\n\\nabla_{w_k} (S_{kj}^2) = 2 S_{kj} \\nabla_{w_k} S_{kj}\n$$\n现在，我们必须计算 $S_{kj}$ 关于 $w_k$ 的梯度：\n$$\n\\nabla_{w_k} S_{kj} = \\nabla_{w_k} (w_k^\\top \\tilde{w}_j + b_k + \\tilde{b}_j - \\log X_{kj})\n$$\n项 $b_k$、$\\tilde{b}_j$ 和 $\\log X_{kj}$ 是不依赖于向量 $w_k$ 的标量，因此它们的梯度为零。我们剩下内积的梯度：\n$$\n\\nabla_{w_k} (w_k^\\top \\tilde{w}_j)\n$$\n使用内积梯度的基本恒等式 $\\nabla_v (v^\\top a) = a$，其中 $v = w_k$ 和 $a = \\tilde{w}_j$，我们发现：\n$$\n\\nabla_{w_k} (w_k^\\top \\tilde{w}_j) = \\tilde{w}_j\n$$\n将此代回，我们得到 $\\nabla_{w_k} S_{kj} = \\tilde{w}_j$。将所有部分组合起来，关于 $w_k$ 的梯度是：\n$$\n\\nabla_{w_k} J = \\sum_{j=1}^V f(X_{kj}) \\cdot 2 \\left( w_k^\\top \\tilde{w}_j + b_k + \\tilde{b}_j - \\log X_{kj} \\right) \\tilde{w}_j\n$$\n如前所述，由于如果 $X_{kj}=0$ 则 $f(X_{kj}) = 0$，该求和实际上是对所有 $X_{kj}  0$ 的 $j$ 进行的。\n\n**2. 关于上下文向量 $\\tilde{w}_k$ 的梯度**\n\n$\\nabla_{\\tilde{w}_k} J$ 的推导遵循对称的论证。原始双重求和中的项仅在索引 $j$ 等于 $k$ 时才依赖于 $\\tilde{w}_k$。\n$$\n\\nabla_{\\tilde{w}_k} J = \\sum_{i=1}^V \\nabla_{\\tilde{w}_k} \\left( f(X_{ik}) \\,\\big(w_i^\\top \\tilde{w}_k + b_i + \\tilde{b}_k - \\log X_{ik}\\big)^2 \\right)\n$$\n令 $S_{ik} = w_i^\\top \\tilde{w}_k + b_i + \\tilde{b}_k - \\log X_{ik}$。像之前一样应用链式法则：\n$$\n\\nabla_{\\tilde{w}_k} J = \\sum_{i=1}^V f(X_{ik}) \\cdot 2 S_{ik} \\cdot \\nabla_{\\tilde{w}_k} S_{ik}\n$$\n标量误差项 $S_{ik}$ 关于 $\\tilde{w}_k$ 的梯度是：\n$$\n\\nabla_{\\tilde{w}_k} S_{ik} = \\nabla_{\\tilde{w}_k} (w_i^\\top \\tilde{w}_k + b_i + \\tilde{b}_k - \\log X_{ik}) = \\nabla_{\\tilde{w}_k} (w_i^\\top \\tilde{w}_k)\n$$\n使用恒等式 $\\nabla_v (a^\\top v) = a$，其中 $v = \\tilde{w}_k$ 和 $a = w_i$，我们得到：\n$$\n\\nabla_{\\tilde{w}_k} (w_i^\\top \\tilde{w}_k) = w_i\n$$\n因此，关于 $\\tilde{w}_k$ 的梯度的最终表达式是：\n$$\n\\nabla_{\\tilde{w}_k} J = \\sum_{i=1}^V f(X_{ik}) \\cdot 2 \\left( w_i^\\top \\tilde{w}_k + b_i + \\tilde{b}_k - \\log X_{ik} \\right) w_i\n$$\n同样，该求和实际上是对所有 $X_{ik}  0$ 的 $i$ 进行的。\n\n### 任务 B 和 C（算法设计与稳定性分析）\n\n推导出的梯度构成了实现的基础。该算法为给定的共现矩阵 $X$、模型参数和指定的权重函数计算所有词向量和上下文向量的梯度。\n\n**算法步骤：**\n1.  根据指定的确定性程序（对向量使用种子为 $42$ 的均值为零、标准差为 $0.5$ 的正态分布，对偏置使用零）初始化参数矩阵 $W \\in \\mathbb{R}^{V \\times d}$、$\\tilde{W} \\in \\mathbb{R}^{V \\times d}$ 和偏置向量 $b \\in \\mathbb{R}^V$、$\\tilde{b} \\in \\mathbb{R}^V$。\n2.  将梯度矩阵 $\\nabla_W J$ 和 $\\nabla_{\\tilde{W}} J$ 初始化为零。\n3.  遍历共现计数 $X_{ij}$ 非零的每一对索引 $(i, j)$。\n4.  对于每一对这样的索引，计算标量误差项 $S_{ij} = w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij}$。\n5.  使用两个指定函数之一计算权重 $f(X_{ij})$：\n    - **标准 GloVe 权重 ($f_{\\mathrm{std}}$):** $f_{\\mathrm{std}}(x) = \\min(1, (x/x_{\\max})^\\alpha)$。此函数对低计数共现进行降权，并对非常频繁的共现（计数 $\\ge x_{\\max}$）将权重上限设为 $1$。\n    - **均匀权重 ($f_{\\mathrm{uni}}$):** 如果 $x  0$ 则 $f_{\\mathrm{uni}}(x) = 1$，否则为 $0$。此函数对所有观察到的共现一视同仁，无论其频率如何。\n6.  计算标量前置因子 $p_{ij} = 2 \\cdot f(X_{ij}) \\cdot S_{ij}$。\n7.  通过加上来自索引对 $(i, j)$ 的贡献来更新梯度：\n    $$\n    \\nabla_{w_i} J \\leftarrow \\nabla_{w_i} J + p_{ij} \\tilde{w}_j \\\\\n    \\nabla_{\\tilde{w}_j} J \\leftarrow \\nabla_{\\tilde{w}_j} J + p_{ij} w_i\n    $$\n\n**稳定性分析：**\n稳定性分析比较了两种权重函数产生的梯度的大小。度量标准是最大绝对梯度分量（所有梯度条目的 $\\ell_\\infty$-范数）的比率 $\\rho$：\n$$\n\\rho = \\frac{\\text{在 } f_{\\mathrm{uni}} \\text{ 下的 }\\max\\{|\\nabla_{W} J|_{\\infty}, |\\nabla_{\\tilde{W}} J|_{\\infty}\\}}{\\text{在 } f_{\\mathrm{std}} \\text{ 下的 }\\max\\{|\\nabla_{W} J|_{\\infty}, |\\nabla_{\\tilde{W}} J|_{\\infty}\\}}\n$$\n- 对于低计数（即 $X_{ij}  x_{\\max}$）的共现对 $(i, j)$，我们有 $f_{\\mathrm{std}}(X_{ij}) = (X_{ij}/x_{\\max})^\\alpha  1$，而 $f_{\\mathrm{uni}}(X_{ij}) = 1$。因此，对于这些对，$f_{\\mathrm{uni}}$ 产生的梯度将大于 $f_{\\mathrm{std}}$ 产生的梯度。这表明对于具有小计数的稀疏矩阵（情况 A、B），$\\rho > 1$。$f_{\\mathrm{std}}$ 的降权特性是模型稳定性的一个关键特征，可以防止稀有但可能带有噪声的共现产生过大的梯度。\n- 对于具有非常高计数（$X_{ij} \\ge x_{\\max}$）的共现对，$f_{\\mathrm{std}}(X_{ij}) = 1$，这与 $f_{\\mathrm{uni}}(X_{ij})$ 相同。如果矩阵中所有非零计数都高于 $x_{\\max}$（情况 C），那么两种权重方案变得等效，我们预期梯度将完全相同，从而得出 $\\rho = 1$。\n- 程序还检查所有计算出的梯度分量是否为有限数。鉴于计算明确避免了 $\\log(0)$ 并且所有输入计数都是非负的，预计不会出现非有限值（NaN/Inf）。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the GloVe gradient derivation, implementation, and analysis problem.\n    \"\"\"\n    # ------------------\n    # --- PARAMETERS ---\n    # ------------------\n    V = 4\n    d = 3\n    seed = 42\n    x_max = 100.0\n    alpha = 0.75\n\n    # ------------------\n    # --- INITIALIZATION ---\n    # ------------------\n    # Initialize parameters deterministically as specified.\n    rng = np.random.default_rng(seed)\n    W = rng.normal(loc=0, scale=0.5, size=(V, d))\n    tilde_W = rng.normal(loc=0, scale=0.5, size=(V, d))\n    b = np.zeros(V)\n    tilde_b = np.zeros(V)\n\n    # ------------------\n    # --- TEST CASES ---\n    # ------------------\n    test_cases = [\n        # Case A: Moderately sparse, small counts\n        np.array([\n            [0, 3, 1, 0],\n            [2, 0, 0, 5],\n            [0, 1, 0, 0],\n            [4, 0, 0, 0]\n        ], dtype=float),\n        # Case B: Extremely sparse, single co-occurrence\n        np.array([\n            [0, 1, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0]\n        ], dtype=float),\n        # Case C: Large counts only, above x_max\n        np.array([\n            [1000, 0, 0, 0],\n            [0, 500, 0, 0],\n            [0, 0, 200, 0],\n            [0, 0, 0, 800]\n        ], dtype=float),\n    ]\n\n    # ------------------\n    # --- WEIGHTING FUNCTIONS ---\n    # ------------------\n    def f_std(x):\n        \"\"\"Standard GloVe weighting function.\"\"\"\n        if x  x_max:\n            return (x / x_max)**alpha\n        else:\n            return 1.0\n\n    def f_uni(x):\n        \"\"\"Uniform weighting function for positive co-occurrences.\"\"\"\n        return 1.0 if x > 0 else 0.0\n\n    # ------------------\n    # --- GRADIENT COMPUTATION ---\n    # ------------------\n    def compute_gradients(X, f_func, W_in, tilde_W_in, b_in, tilde_b_in):\n        \"\"\"\n        Computes gradients for word and context vectors based on derived formula.\n        \"\"\"\n        V_dim, d_dim = W_in.shape\n        grad_W = np.zeros_like(W_in)\n        grad_tilde_W = np.zeros_like(tilde_W_in)\n        \n        # Iterate only over non-zero entries of the co-occurrence matrix X\n        rows, cols = np.nonzero(X)\n        \n        for i, j in zip(rows, cols):\n            X_ij = X[i, j]\n            \n            # The inner term of the GloVe objective\n            inner_term = (W_in[i] @ tilde_W_in[j] + b_in[i] + tilde_b_in[j] \n                          - np.log(X_ij))\n            \n            # Weight from the weighting function\n            weight = f_func(X_ij)\n            \n            # Scalar pre-factor common to both gradient updates\n            prefactor = 2 * weight * inner_term\n            \n            # Update gradients according to the derived formulas\n            grad_W[i, :] += prefactor * tilde_W_in[j, :]\n            grad_tilde_W[j, :] += prefactor * W_in[i, :]\n            \n        return grad_W, grad_tilde_W\n\n    # ------------------\n    # --- ANALYSIS  OUTPUT ---\n    # ------------------\n    results_list = []\n    \n    for X_case in test_cases:\n        # --- Compute gradients and metrics for f_std ---\n        grad_W_std, grad_tilde_W_std = compute_gradients(X_case, f_std, W, tilde_W, b, tilde_b)\n        \n        finite_std = np.all(np.isfinite(grad_W_std)) and np.all(np.isfinite(grad_tilde_W_std))\n        \n        max_abs_grad_std = 0.0\n        if grad_W_std.size > 0:\n            max_abs_grad_std = max(np.max(np.abs(grad_W_std)), np.max(np.abs(grad_tilde_W_std)))\n        \n        # --- Compute gradients and metrics for f_uni ---\n        grad_W_uni, grad_tilde_W_uni = compute_gradients(X_case, f_uni, W, tilde_W, b, tilde_b)\n        \n        finite_uni = np.all(np.isfinite(grad_W_uni)) and np.all(np.isfinite(grad_tilde_W_uni))\n        \n        max_abs_grad_uni = 0.0\n        if grad_W_uni.size > 0:\n            max_abs_grad_uni = max(np.max(np.abs(grad_W_uni)), np.max(np.abs(grad_tilde_W_uni)))\n        \n        # --- Compute the ratio rho ---\n        # If std grad is 0, uni grad must also be 0 (since f_std(x)>0 => f_uni(x)>0).\n        # In this 0/0 case, the gradients are identical, so the ratio is 1.\n        if max_abs_grad_std > 1e-9: # Use tolerance for float comparison\n            rho = max_abs_grad_uni / max_abs_grad_std\n        else:\n            rho = 1.0\n            \n        # Format the result for this case\n        case_result = f\"[{round(rho, 6)},{str(finite_std).lower()},{str(finite_uni).lower()}]\"\n        results_list.append(case_result)\n        \n    # --- Final Print ---\n    # Print the final result in the exact required single-line format.\n    print(f\"[{','.join(results_list)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个训练好的模型并非黑箱；我们应该诊断其性能以了解其优缺点。本实践引入了一种源于统计学的强大技术——分析标准化残差，用以识别 GloVe 模型未能准确表示的词对。通过找出拟合最差的“异常点”，您将学会如何解读模型的预测，并洞察其已经学会（或未能学会）的语义关系。",
            "id": "3130256",
            "problem": "给定一个大小为 $n$ 的有限词汇表和一个对称的共现矩阵 $X \\in \\mathbb{R}^{n \\times n}$，其中 $X_{ij}$ 表示在语料库中词 $i$ 出现在词 $j$ 附近的次数。假设使用词表示全局向量（GloVe）模型，其中每个词被赋予一个词向量 $w_i \\in \\mathbb{R}^d$、一个上下文向量 $\\tilde{w}_i \\in \\mathbb{R}^d$、一个词偏置 $b_i \\in \\mathbb{R}$ 和一个上下文偏置 $\\tilde{b}_i \\in \\mathbb{R}$。GloVe 的目标函数是一个对共现次数对数的加权最小二乘拟合，其权重函数 $f(x)$ 由经过充分测试的经验性设计给出：\n$$\nJ = \\sum_{i=1}^n \\sum_{j=1}^n f(X_{ij})\\left(\\, w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\,\\right)^2,\n$$\n其中\n$$\nf(x) = \\begin{cases}\n\\left(\\dfrac{x}{x_{\\max}}\\right)^{\\alpha},  x  x_{\\max},\\\\\n1,  x \\ge x_{\\max}.\n\\end{cases}\n$$\n使用这种基于原则的公式，通过从 $J$ 所隐含的模型构建一个标准化残差，推导出一个诊断方法来识别拟合不良的词对。解释残差的符号如何与语义解释相关联，并实现一个算法，该算法：\n- 对于所有 $i",
            "solution": "起点是词表示全局向量（GloVe）的加权最小二乘目标函数：\n$$\nJ = \\sum_{i=1}^n \\sum_{j=1}^n f(X_{ij})\\left(\\, w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\,\\right)^2,\n$$\n其经验性权重函数为\n$$\nf(x) = \\begin{cases}\n\\left(\\dfrac{x}{x_{\\max}}\\right)^{\\alpha},  x  x_{\\max},\\\\\n1,  x \\ge x_{\\max}.\n\\end{cases}\n$$\n这是一个标准的加权最小二乘公式，其中括号内的部分表示预测 $\\log X_{ij}$ 时的模型误差或残差。将原始残差表示为\n$$\nr_{ij} = w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij}.\n$$\n在异方差噪声下的经典加权最小二乘法中，权重与误差方差的倒数成正比。在这里，$f(X_{ij})$ 扮演了一个权重角色，它随着 $X_{ij}$ 的增加而增加，直到在 $x_{\\max}$ 处达到饱和，这捕捉了更高计数值更可靠的思想。一种有原则的诊断方法通过使用权重的平方根进行标准化，从而在共同的尺度上比较不同词对的残差，得到\n$$\ns_{ij} = \\sqrt{f(X_{ij})}\\, r_{ij}.\n$$\n在假设 $f(X_{ij})$ 近似于逆方差的情况下，这个标准化残差可以解释为一种类 z 分数。较大的 $|s_{ij}|$ 值表示相对于模型的异方差误差结构，该词对存在拟合不良。\n\n通过符号进行的语义解释直接源于 $r_{ij}$ 的定义：\n- 如果 $r_{ij}  0$，则 $w_i^\\top \\tilde{w}_j + b_i + \\tilde{b}_j  \\log X_{ij}$，意味着模型预测的对数共现次数大于观测值。这是一种过高预测，暗示在嵌入空间中可能存在虚假关联。\n- 如果 $r_{ij}  0$，则模型预测的对数共现次数小于观测值。这是一种过低预测，表明嵌入未能充分表示数据中存在的真实关联。\n\n算法设计：\n1. 对于每个测试用例，读取 $X \\in \\mathbb{R}^{n \\times n}$, $w \\in \\mathbb{R}^{n \\times d}$, $\\tilde{w} \\in \\mathbb{R}^{n \\times d}$, $b \\in \\mathbb{R}^n$, $\\tilde{b} \\in \\mathbb{R}^n$, $x_{\\max} \\in \\mathbb{R}$, $\\alpha \\in \\mathbb{R}$，一个阈值 $\\tau \\in \\mathbb{R}$ 和一个整数 $k$。\n2. 遍历 $i",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef weight_function(x, x_max, alpha):\n    # f(x) = (x/x_max)^alpha if x  x_max else 1\n    if x  x_max:\n        return (x / x_max) ** alpha\n    else:\n        return 1.0\n\ndef standardized_residuals(X, W, WT, b, bt, x_max, alpha, tau, k):\n    n = X.shape[0]\n    results = []\n    # Iterate over upper triangle i"
        }
    ]
}