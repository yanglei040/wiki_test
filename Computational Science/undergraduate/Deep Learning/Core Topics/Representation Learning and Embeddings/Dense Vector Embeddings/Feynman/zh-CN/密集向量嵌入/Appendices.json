{
    "hands_on_practices": [
        {
            "introduction": "在嵌入空间中，余弦相似度是衡量向量间关系的核心工具。然而，由它派生的“余弦相异度”（$1 - s(\\mathbf{x}, \\mathbf{y})$）是否能作为一种真正的“距离”来使用呢？这个练习将引导你深入探讨这个看似简单但至关重要的问题，通过一个具体的反例揭示为何余弦相异度不满足度量空间中的三角不等式，从而加深你对嵌入空间几何特性的理解。",
            "id": "3114488",
            "problem": "深度学习中使用的稠密向量嵌入是$\\mathbb{R}^{d}$中的实值向量，通常被归一化为单位欧几里得范数，以提高相似性搜索中的稳定性和可解释性。两个嵌入$\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^{d}$之间的余弦相似度定义为 $s(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|}$。当使用余弦相异度$d_{c}(\\mathbf{x}, \\mathbf{y}) = 1 - s(\\mathbf{x}, \\mathbf{y})$来度量嵌入之间的分离度时，请考虑在单位球面$\\mathbb{S}^{d-1} = \\{\\mathbf{u} \\in \\mathbb{R}^{d} : \\|\\mathbf{u}\\| = 1\\}$上诱导的几何。\n\n仅从欧几里得内积和范数的基本定义出发，分析$s(\\mathbf{x}, \\mathbf{y})$与单位向量之间夹角的关系，并利用此关系从概念上解释为何三角不等式对于$\\mathbb{S}^{d-1}$上的$d_{c}$可能不成立。然后，构造一个显式反例，包含三个单位归一化的嵌入$\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in \\mathbb{S}^{1} \\subset \\mathbb{S}^{d-1}$（为具体起见，取$d = 2$），如下所示：\n$$\n\\mathbf{x} = (\\cos 0^{\\circ}, \\sin 0^{\\circ}), \\quad\n\\mathbf{y} = (\\cos 60^{\\circ}, \\sin 60^{\\circ}), \\quad\n\\mathbf{z} = (\\cos 120^{\\circ}, \\sin 120^{\\circ}),\n$$\n其中角度以度为单位。计算量\n$$\nT = d_{c}(\\mathbf{x}, \\mathbf{z}) - d_{c}(\\mathbf{x}, \\mathbf{y}) - d_{c}(\\mathbf{y}, \\mathbf{z}),\n$$\n并给出其精确值。无需四舍五入。最终答案必须是单个不带单位的实数。",
            "solution": "$\\mathbb{R}^{d}$中$\\mathbf{x}$和$\\mathbf{y}$之间的欧几里得内积定义为$\\mathbf{x} \\cdot \\mathbf{y} = \\sum_{i=1}^{d} x_{i} y_{i}$，范数定义为$\\|\\mathbf{x}\\| = \\sqrt{\\mathbf{x} \\cdot \\mathbf{x}}$。一个关联内积与范数的基本几何事实是，对于任意非零$\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^{d}$，\n$$\n\\mathbf{x} \\cdot \\mathbf{y} = \\|\\mathbf{x}\\| \\|\\mathbf{y}\\| \\cos \\theta,\n$$\n其中$\\theta$是$\\mathbf{x}$和$\\mathbf{y}$之间的夹角。当$\\mathbf{x}$和$\\mathbf{y}$是单位向量时，$\\|\\mathbf{x}\\| = \\|\\mathbf{y}\\| = 1$，因此余弦相似度简化为\n$$\ns(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} = \\mathbf{x} \\cdot \\mathbf{y} = \\cos \\theta.\n$$\n因此，在单位球面$\\mathbb{S}^{d-1}$上，余弦相似度等于向量之间中心角的余弦值。余弦相异度为\n$$\nd_{c}(\\mathbf{x}, \\mathbf{y}) = 1 - s(\\mathbf{x}, \\mathbf{y}) = 1 - \\cos \\theta.\n$$\n\n为了讨论三角不等式，我们回顾度量公理：如果一个函数$d$对于所有点都满足非负性、不可辨识者同一性、对称性和三角不等式，那么它就是一个度量：\n$$\nd(\\mathbf{x}, \\mathbf{z}) \\leq d(\\mathbf{x}, \\mathbf{y}) + d(\\mathbf{y}, \\mathbf{z}).\n$$\n在$\\mathbb{S}^{d-1}$上，角距离$\\theta$本身（作为测地角测量）满足三角不等式，因为它是从球面上的测地度量导出的。然而，映射$\\theta \\mapsto 1 - \\cos \\theta$非线性地扭曲了角度：$\\cos \\theta$不是关于$\\theta$的次可加函数，并且当角度相加时（例如，当两个较小的角组合成一个较大的钝角时），$1 - \\cos \\theta$的增长速度可能快于线性增长。因此，不能保证$d_{c}$满足三角不等式。\n\n我们现在在嵌入$\\mathbb{R}^{2}$的$\\mathbb{S}^{1}$上构造一个显式反例：\n$$\n\\mathbf{x} = (\\cos 0^{\\circ}, \\sin 0^{\\circ}) = (1, 0), \\quad\n\\mathbf{y} = (\\cos 60^{\\circ}, \\sin 60^{\\circ}) = \\left(\\frac{1}{2}, \\frac{\\sqrt{3}}{2}\\right), \\quad\n\\mathbf{z} = (\\cos 120^{\\circ}, \\sin 120^{\\circ}) = \\left(-\\frac{1}{2}, \\frac{\\sqrt{3}}{2}\\right).\n$$\n各向量对之间的夹角为：\n- $\\mathbf{x}$与$\\mathbf{y}$之间：$\\theta_{xy} = 60^{\\circ}$。\n- $\\mathbf{y}$与$\\mathbf{z}$之间：$\\theta_{yz} = 60^{\\circ}$。\n- $\\mathbf{x}$与$\\mathbf{z}$之间：$\\theta_{xz} = 120^{\\circ}$。\n\n对于单位向量，使用$s(\\mathbf{u}, \\mathbf{v}) = \\cos \\theta$和$d_{c} = 1 - s$进行计算：\n$$\ns(\\mathbf{x}, \\mathbf{y}) = \\cos 60^{\\circ} = \\frac{1}{2}, \\quad d_{c}(\\mathbf{x}, \\mathbf{y}) = 1 - \\frac{1}{2} = \\frac{1}{2},\n$$\n$$\ns(\\mathbf{y}, \\mathbf{z}) = \\cos 60^{\\circ} = \\frac{1}{2}, \\quad d_{c}(\\mathbf{y}, \\mathbf{z}) = 1 - \\frac{1}{2} = \\frac{1}{2},\n$$\n$$\ns(\\mathbf{x}, \\mathbf{z}) = \\cos 120^{\\circ} = -\\frac{1}{2}, \\quad d_{c}(\\mathbf{x}, \\mathbf{z}) = 1 - \\left(-\\frac{1}{2}\\right) = \\frac{3}{2}.\n$$\n因此，\n$$\nT = d_{c}(\\mathbf{x}, \\mathbf{z}) - d_{c}(\\mathbf{x}, \\mathbf{y}) - d_{c}(\\mathbf{y}, \\mathbf{z}) = \\frac{3}{2} - \\frac{1}{2} - \\frac{1}{2} = \\frac{1}{2}.\n$$\n由于$T > 0$，三角不等式$d_{c}(\\mathbf{x}, \\mathbf{z}) \\leq d_{c}(\\mathbf{x}, \\mathbf{y}) + d_{c}(\\mathbf{y}, \\mathbf{z})$在此例中不成立，这证实了尽管底层的角距离是度量，但$d_{c}$在$\\mathbb{S}^{d-1}$上不是一个度量。",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "现代自监督学习模型（如CLIP）的成功在很大程度上归功于对比损失函数，特别是InfoNCE损失。要真正掌握这些模型的工作原理，就必须理解损失函数如何通过梯度引导嵌入向量的优化。这个练习将带你深入剖析InfoNCE损失的梯度计算过程，让你亲手推导在一个包含L2归一化的复杂场景下，梯度如何同时对嵌入向量的方向和模长施加作用力。",
            "id": "3114472",
            "problem": "考虑深度学习中的一个密集向量嵌入场景，其中一个锚点嵌入 $x \\in \\mathbb{R}^{d}$ 和一组候选嵌入 $\\{y_{j}\\}_{j=1}^{n} \\subset \\mathbb{R}^{d}$ 使用信息噪声对比估计 (InfoNCE) 损失进行比较。设温度为一个固定的标量 $\\tau > 0$。对于 $\\{y_{j}\\}$ 中的一个指定正样本 $y$，其损失为\n$$\nL(x) \\;=\\; -\\ln\\!\\left(\\frac{\\exp\\!\\big(\\hat{x}\\cdot y/\\tau\\big)}{\\sum_{j=1}^{n}\\exp\\!\\big(\\hat{x}\\cdot y_{j}/\\tau\\big)}\\right),\n$$\n其中 $\\hat{x} = x/\\|x\\|$ 表示 $x$ 的欧几里得二范数（L2）归一化，点号表示 $\\mathbb{R}^{d}$ 上的标准内积。假设所有的 $y_{j}$ 相对于 $x$ 都是固定的。\n\n仅从 softmax、log-sum-exp 和 L2 归一化映射的核心定义出发，推导关于未归一化的锚点 $x$ 的梯度 $\\nabla_{x}L(x)$ 的闭式表达式，该表达式需正确处理 L2 归一化。然后，简要地用文字说明该梯度如何编码作用于 $x$ 的范数和方向上的竞争力量。\n\n你的最终答案必须是 $\\nabla_{x}L(x)$ 的一个单一闭式解析表达式，用 $x$, $\\{y_{j}\\}$, $y$, 和 $\\tau$ 表示。不需要进行数值计算。",
            "solution": "问题要求计算信息噪声对比估计 (InfoNCE) 损失函数关于未归一化的锚点嵌入 $x$ 的梯度。\n\n损失函数由下式给出：\n$$\nL(x) = -\\ln\\left(\\frac{\\exp(\\hat{x}\\cdot y/\\tau)}{\\sum_{j=1}^{n}\\exp(\\hat{x}\\cdot y_{j}/\\tau)}\\right)\n$$\n其中 $\\hat{x} = x/\\|x\\|$ 是 L2 归一化的锚点嵌入，$y$ 是正候选嵌入，$\\{y_j\\}_{j=1}^n$ 是所有候选嵌入（包括 $y$）的集合，$\\tau > 0$ 是温度。\n\n首先，我们可以利用对数的性质来简化损失 $L(x)$ 的表达式：\n$$\nL(x) = -\\left(\\frac{\\hat{x}\\cdot y}{\\tau}\\right) + \\ln\\left(\\sum_{j=1}^{n}\\exp\\left(\\frac{\\hat{x}\\cdot y_{j}}{\\tau}\\right)\\right)\n$$\n这是一个复合函数。我们将损失定义为归一化向量 $u = \\hat{x}$ 的函数，记为 $L(u)$，并将 $u$ 定义为 $x$ 的函数，记为 $u(x) = x/\\|x\\|$。\n$$\nL(u) = -\\frac{u \\cdot y}{\\tau} + \\ln\\left(\\sum_{j=1}^{n}\\exp\\left(\\frac{u \\cdot y_{j}}{\\tau}\\right)\\right)\n$$\n为了求梯度 $\\nabla_{x}L(x)$，我们应用多元链式法则：\n$$\n\\nabla_{x}L(x) = (\\nabla_{x} u(x))^T \\nabla_{u}L(u)\n$$\n其中 $\\nabla_{x} u(x)$ 是函数 $u(x)$ 的雅可比矩阵，而 $\\nabla_{u}L(u)$ 是 $L$ 关于 $u$ 的梯度。\n\n我们分别计算这两个组成部分。\n\n1.  **关于归一化向量 $u = \\hat{x}$ 的梯度**：\n    我们计算梯度 $\\nabla_{u}L(u)$。\n    $$\n    \\nabla_{u}L(u) = \\nabla_{u}\\left(-\\frac{u \\cdot y}{\\tau}\\right) + \\nabla_{u}\\left(\\ln\\left(\\sum_{j=1}^{n}\\exp\\left(\\frac{u \\cdot y_{j}}{\\tau}\\right)\\right)\\right)\n    $$\n    第一项的梯度很简单：\n    $$\n    \\nabla_{u}\\left(-\\frac{u \\cdot y}{\\tau}\\right) = -\\frac{1}{\\tau}y\n    $$\n    对于第二项，我们再次使用链式法则。令 $Z(u) = \\sum_{j=1}^{n}\\exp((u \\cdot y_{j})/\\tau)$。\n    $$\n    \\nabla_{u}\\ln(Z(u)) = \\frac{1}{Z(u)}\\nabla_{u}Z(u) = \\frac{1}{Z(u)}\\sum_{k=1}^{n} \\nabla_{u}\\left(\\exp\\left(\\frac{u \\cdot y_{k}}{\\tau}\\right)\\right)\n    $$\n    $$\n    = \\frac{1}{Z(u)}\\sum_{k=1}^{n} \\exp\\left(\\frac{u \\cdot y_{k}}{\\tau}\\right) \\nabla_{u}\\left(\\frac{u \\cdot y_{k}}{\\tau}\\right) = \\frac{1}{Z(u)}\\sum_{k=1}^{n} \\exp\\left(\\frac{u \\cdot y_{k}}{\\tau}\\right) \\frac{y_{k}}{\\tau}\n    $$\n    我们定义 softmax 概率 $P_{j}(u) = \\frac{\\exp((u \\cdot y_{j})/\\tau)}{Z(u)}$。该表达式简化为：\n    $$\n    \\nabla_{u}\\ln(Z(u)) = \\frac{1}{\\tau}\\sum_{k=1}^{n} P_{k}(u) y_{k}\n    $$\n    合并 $\\nabla_{u}L(u)$ 的各项：\n    $$\n    \\nabla_{u}L(u) = \\frac{1}{\\tau}\\left(\\left(\\sum_{j=1}^{n} P_{j}(u)y_{j}\\right) - y\\right)\n    $$\n\n2.  **L2 归一化映射的雅可比矩阵**：\n    接下来，我们计算 $u(x) = x/\\|x\\|$ 的雅可比矩阵。该矩阵的第 $(i,k)$ 个元素是 $\\frac{\\partial u_i}{\\partial x_k}$。\n    $$\n    \\frac{\\partial u_i}{\\partial x_k} = \\frac{\\partial}{\\partial x_k}\\left(\\frac{x_i}{\\|x\\|}\\right) = \\frac{\\delta_{ik}\\|x\\| - x_i \\frac{\\partial\\|x\\|}{\\partial x_k}}{\\|x\\|^2}\n    $$\n    由于 $\\|x\\| = (x \\cdot x)^{1/2}$，其偏导数为 $\\frac{\\partial\\|x\\|}{\\partial x_k} = \\frac{x_k}{\\|x\\|}$。\n    $$\n    \\frac{\\partial u_i}{\\partial x_k} = \\frac{\\delta_{ik}\\|x\\| - x_i \\frac{x_k}{\\|x\\|}}{\\|x\\|^2} = \\frac{\\delta_{ik}}{\\|x\\|} - \\frac{x_i x_k}{\\|x\\|^3}\n    $$\n    以矩阵形式表示，雅可比矩阵为：\n    $$\n    \\nabla_{x} u(x) = \\frac{1}{\\|x\\|}I - \\frac{1}{\\|x\\|^3}xx^T = \\frac{1}{\\|x\\|}\\left(I - \\frac{x}{\\|x\\|}\\frac{x^T}{\\|x\\|}\\right) = \\frac{1}{\\|x\\|}(I - \\hat{x}\\hat{x}^T)\n    $$\n    这个矩阵是对称的，所以 $(\\nabla_{x} u(x))^T = \\nabla_{x} u(x)$。\n\n3.  **组合以求得最终梯度 $\\nabla_x L(x)$**：\n    将 $u=\\hat{x}$ 和两个分量的表达式代回链式法则公式：\n    $$\n    \\nabla_{x}L(x) = \\frac{1}{\\|x\\|}(I - \\hat{x}\\hat{x}^T) \\left[ \\frac{1}{\\tau}\\left(\\left(\\sum_{j=1}^{n} P_{j}(\\hat{x})y_{j}\\right) - y\\right) \\right]\n    $$\n    我们展开这个乘积：\n    $$\n    \\nabla_{x}L(x) = \\frac{1}{\\tau\\|x\\|} \\left[ \\left(\\left(\\sum_{j=1}^{n} P_{j}(\\hat{x})y_{j}\\right) - y\\right) - \\hat{x}\\hat{x}^T \\left(\\left(\\sum_{j=1}^{n} P_{j}(\\hat{x})y_{j}\\right) - y\\right) \\right]\n    $$\n    第二部分使用 $\\hat{x}^T v = \\hat{x} \\cdot v$ 进行简化：\n    $$\n    \\hat{x}\\hat{x}^T \\left( \\dots \\right) = \\hat{x} \\left( \\hat{x} \\cdot \\left[ \\left(\\sum_{j=1}^{n} P_{j}(\\hat{x})y_{j}\\right) - y \\right] \\right) = \\left( \\left(\\sum_{j=1}^{n} P_{j}(\\hat{x})(\\hat{x}\\cdot y_{j})\\right) - (\\hat{x}\\cdot y) \\right) \\hat{x}\n    $$\n    所以完整的梯度是：\n    $$\n    \\nabla_{x}L(x) = \\frac{1}{\\tau\\|x\\|} \\left[ \\left(\\left(\\sum_{j=1}^{n} P_{j}(\\hat{x})y_{j}\\right) - y\\right) - \\left( \\left(\\sum_{j=1}^{n} P_{j}(\\hat{x})(\\hat{x}\\cdot y_{j})\\right) - (\\hat{x}\\cdot y) \\right) \\hat{x} \\right]\n    $$\n    为了以 $x$, $\\{y_j\\}$, $y$, 和 $\\tau$ 纯粹表示最终答案，我们代入 $\\hat{x}=x/\\|x\\|$ 并展开各项：\n    $$\n    \\nabla_{x}L(x) = \\frac{1}{\\tau\\|x\\|}\\left( \\left(\\sum_{j=1}^{n} P_{j}y_{j}\\right) - y \\right) - \\frac{1}{\\tau\\|x\\|^2}\\left( \\left(\\sum_{j=1}^{n} P_{j}\\frac{x\\cdot y_j}{\\|x\\|}\\right) - \\frac{x\\cdot y}{\\|x\\|} \\right)x\n    $$\n    $$\n    \\nabla_{x}L(x) = \\frac{1}{\\tau\\|x\\|}\\left( \\left(\\sum_{j=1}^{n} P_{j}y_{j}\\right) - y \\right) - \\frac{1}{\\tau\\|x\\|^3}\\left( \\left(\\sum_{j=1}^{n} P_{j}(x\\cdot y_j)\\right) - (x\\cdot y) \\right)x\n    $$\n    其中概率 $P_j$ 本身是 $x$ 的函数：\n    $$\n    P_{j} = P_j(x) = \\frac{\\exp((x \\cdot y_{j})/(\\tau\\|x\\|))}{\\sum_{k=1}^{n}\\exp((x \\cdot y_{k})/(\\tau\\|x\\|))}\n    $$\n    这就是梯度的最终闭式表达式。\n\n**关于竞争力量的说明**\n\n深度学习中的梯度更新规则是 $x_{\\text{new}} = x - \\eta \\nabla_{x}L(x)$，其中学习率 $\\eta > 0$。更新方向 $-\\nabla_x L(x)$ 揭示了作用于 $x$ 上的各种力量。\n$$\n-\\nabla_{x}L(x) = \\frac{1}{\\tau\\|x\\|} \\left[ (y - \\bar{y}) + (\\bar{c} - c_y)\\hat{x} \\right]\n$$\n其中 $\\bar{y} = \\sum_{j}P_j y_j$ 是期望候选向量，$c_y = \\hat{x}\\cdot y$ 是与正样本的余弦相似度，$\\bar{c} = \\sum_j P_j(\\hat{x}\\cdot y_j)$ 是在所有候选样本上的期望余弦相似度。\n\n1.  **作用于 $x$ 方向上的力**：项 $(y - \\bar{y})$ 决定了方向的变化。更新试图将 $x$ 移动到一个“更像 $y$”且“更不像 $\\bar{y}$”的新方向。由于 $\\bar{y}$ 是所有候选向量的加权平均，其中对于与 $x$ 相似到容易混淆的候选向量，其权重 $P_j$ 会很高，因此该项有效地推动 $x$ 与正嵌入 $y$ 更好地对齐，同时远离负“干扰”嵌入的平均值。这股力量主要作用是旋转向量 $x$。\n\n2.  **作用于 $x$ 范数上的力**：项 $(\\bar{c} - c_y)\\hat{x}$ 是一个与 $x$ 本身平行的向量，因此其主要作用是改变范数 $\\|x\\|$。\n    - 如果锚点对齐得很好，即其与正样本的相似度 $c_y$ 大于与所有候选样本的平均相似度 $\\bar{c}$（即 $c_y > \\bar{c}$），则标量系数 $(\\bar{c} - c_y)$ 为负。因此，更新方向 $-\\nabla_x L(x)$ 有一个沿着 $\\hat{x}$ 方向的分量，这会*增加*范数 $\\|x\\|$。更大的范数会使 softmax 分布更尖锐，从而有效地使模型对其正确的对齐方式更“自信”。\n    - 相反，如果锚点对齐得不好（$c_y < \\bar{c}$），则标量系数为正。更新方向有一个与 $\\hat{x}$ 相反的分量，这会*减小*范数 $\\|x\\|$。较小的范数使 softmax 分布更“平滑”或不那么尖锐。这使得模型对其当前不正确的方向不那么“执着”，并增加了旋转梯度分量的相对大小，从而鼓励进行更大的方向校正。\n\n总而言之，梯度编码了两种相互竞争但又协同合作的力量：一种是旋转力，它将 $x$ 的方向引向正样本并远离干扰项；另一种是缩放力，它调整 $x$ 的范数，以根据模型当前的表现来调节其置信度。",
            "answer": "$$\n\\boxed{\\frac{1}{\\tau\\|x\\|}\\left( \\left(\\sum_{j=1}^{n} P_{j}y_{j}\\right) - y \\right) - \\frac{1}{\\tau\\|x\\|^3}\\left( \\left(\\sum_{j=1}^{n} P_{j}(x\\cdot y_j)\\right) - (x\\cdot y) \\right)x, \\quad \\text{其中 } P_{j} = \\frac{\\exp((x \\cdot y_{j})/(\\tau\\|x\\|))}{\\sum_{k=1}^{n}\\exp((x \\cdot y_{k})/(\\tau\\|x\\|))}}\n$$"
        },
        {
            "introduction": "理论知识的最终目的是解决实际问题，而小样本学习（few-shot learning）正是密集向量嵌入大放异彩的领域之一。本练习将指导你实现一个基于期望最大化（EM）算法的原型优化器，这是一个强大的半监督学习技术。你将从零开始，利用少量带标签的“支持集”样本初始化类原型，然后巧妙地利用无标签的“查询集”数据，通过迭代优化来提升分类器的性能，亲身体验如何将理论应用于实践，构建一个更智能的学习系统。",
            "id": "3114433",
            "problem": "您必须使用基于余弦相似度的期望最大化（EM）算法，为密集向量嵌入实现并分析一个原型精炼算法。给定几个少样本片段（few-shot episodes），每个片段都包含在同一欧几里得空间中的带标签的支持集嵌入和带标签的查询集嵌入。目标是比较一个基线分类器和一个EM精炼分类器，前者使用从支持集计算出的类原型，后者通过基于余弦相似度的软责任（soft responsibilities）引入了未标记的查询。\n\n从以下基本概念开始：\n- 对于两个非零向量 $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$，它们之间的余弦相似度定义为 $\\cos(\\theta) = \\dfrac{\\mathbf{x}^\\top \\mathbf{y}}{\\|\\mathbf{x}\\|_2 \\, \\|\\mathbf{y}\\|_2}$，其中 $\\|\\cdot\\|_2$ 表示欧几里得范数。\n- 对于一个向量 $\\mathbf{z} \\in \\mathbb{R}^K$，其softmax函数为 $\\operatorname{softmax}(\\mathbf{z})_k = \\dfrac{\\exp(z_k)}{\\sum_{j=1}^K \\exp(z_j)}$，其中 $k \\in \\{1,\\dots,K\\}$。\n- 在一个混合模型中，对于数据点 $i$ 和分量 $k$，其分量得分为 $s_{ik}$，后验责任为 $r_{ik} = \\dfrac{\\exp(s_{ik})}{\\sum_{j=1}^K \\exp(s_{ij})}$。期望最大化（EM）算法在计算责任（E步骤）和最大化关于参数的期望完全数据对数似然（M步骤）之间交替进行，在标准正则条件下，可以保证观测数据对数似然的单调改进。\n\n您的算法任务：\n1. 将所有嵌入归一化为单位欧几里得范数。对于任意向量 $\\mathbf{x} \\neq \\mathbf{0}$，定义 $\\widehat{\\mathbf{x}} = \\dfrac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}$。\n2. 基线原型：对于每个类别 $k \\in \\{1,\\dots,K\\}$，计算初始原型，即类别 $k$ 的归一化支持向量的均值，然后重新归一化为单位范数。也就是说，如果 $\\mathcal{S}_k$ 是类别 $k$ 的支持集嵌入集合，定义\n   $$\\mathbf{m}_k^{(0)} = \\operatorname{norm}\\left(\\frac{1}{|\\mathcal{S}_k|} \\sum_{\\mathbf{s}\\in \\mathcal{S}_k} \\widehat{\\mathbf{s}} \\right),$$\n   其中对于 $\\mathbf{v} \\neq \\mathbf{0}$，$\\operatorname{norm}(\\mathbf{v}) = \\dfrac{\\mathbf{v}}{\\|\\mathbf{v}\\|_2}$，而 $\\operatorname{norm}(\\mathbf{0})$ 未定义，因为它在所提供的数据中不会出现。\n3. 查询的基线分类：对于每个查询 $\\widehat{\\mathbf{q}}$，通过最大化与基线原型的余弦相似度来分配预测标签，\n   $$\\widehat{y}(\\widehat{\\mathbf{q}}) = \\arg\\max_{k \\in \\{1,\\dots,K\\}} \\widehat{\\mathbf{q}}^\\top \\mathbf{m}_k^{(0)}.$$\n   基线准确率是该片段中被正确分类的查询所占的比例，以小数形式表示。\n4. 使用余弦责任的EM精炼：固定一个温度参数 $\\tau > 0$ 和一个查询权重 $\\lambda \\ge 0$。对于 $t = 0,1,\\dots,T-1$，重复以下步骤：\n   - 对查询进行E步骤：对于每个查询 $\\widehat{\\mathbf{q}}_i$，计算其对各个类别的责任\n     $$r_{ik}^{(t)} = \\frac{\\exp\\left(\\tau \\, \\widehat{\\mathbf{q}}_i^\\top \\mathbf{m}_k^{(t)}\\right)}{\\sum_{j=1}^K \\exp\\left(\\tau \\, \\widehat{\\mathbf{q}}_i^\\top \\mathbf{m}_j^{(t)}\\right)}.$$\n   - 对原型进行M步骤：对于每个类别 $k$，通过组合带标签的支持向量（作为对其类别的硬分配）和由责任加权的未标记查询来更新原型，然后重新归一化，\n     $$\\mathbf{m}_k^{(t+1)} = \\operatorname{norm}\\left(\\sum_{\\mathbf{s}\\in \\mathcal{S}_k} \\widehat{\\mathbf{s}} \\;+\\; \\lambda \\sum_{i} r_{ik}^{(t)} \\widehat{\\mathbf{q}}_i \\right).$$\n5. 查询的精炼分类：经过 $T$ 次迭代后，计算出精炼后的原型 $\\mathbf{m}_k^{(T)}$，并如步骤3中一样通过最近余弦相似度对查询进行分类，以获得精炼后准确率。\n6. 增益：对于每个片段，计算增益，即精炼后准确率减去基线准确率。增益必须以四舍五入到三位小数的浮点数形式报告。\n\n角度单位：如果您在内部计算角度，请使用弧度。所要求的输出是无单位的小数。\n\n您必须实现此算法，并在以下测试套件的片段上对其进行评估。每个片段都由嵌入维度 $d$、类别数量 $K$、温度 $\\tau$、查询权重 $\\lambda$、EM迭代次数 $T$、每个类别的支持集嵌入以及查询集嵌入及其标签指定。所有向量都必须视为 $\\mathbb{R}^d$ 的元素并按规定进行归一化。\n\n测试套件：\n- 片段 $1$：\n  - $d = 3$, $K = 3$, $\\tau = 10$, $\\lambda = 1$, $T = 5$。\n  - 支持集：\n    - 类别 $0$： $\\{[\\,1.0,\\,0.0,\\,0.0\\,],\\;[\\,0.8,\\,0.2,\\,0.0\\,]\\}$。\n    - 类别 $1$： $\\{[\\,0.0,\\,1.0,\\,0.0\\,],\\;[\\,0.1,\\,0.9,\\,0.0\\,]\\}$。\n    - 类别 $2$： $\\{[\\,0.0,\\,0.0,\\,1.0\\,],\\;[\\,0.1,\\,0.0,\\,0.9\\,]\\}$。\n  - 查询集 (嵌入, 标签)：\n    - $[\\,0.85,\\,0.15,\\,0.0\\,]$ 标签为 $0$，\n    - $[\\,0.15,\\,0.85,\\,0.0\\,]$ 标签为 $1$，\n    - $[\\,0.10,\\,0.10,\\,0.98\\,]$ 标签为 $2$，\n    - $[\\,0.65,\\,0.35,\\,0.0\\,]$ 标签为 $0$，\n    - $[\\,0.35,\\,0.65,\\,0.0\\,]$ 标签为 $1$。\n- 片段 $2$：\n  - $d = 2$, $K = 2$, $\\tau = 5$, $\\lambda = 2$, $T = 5$。\n  - 支持集：\n    - 类别 $0$： $\\{[\\,1.0,\\,0.0\\,],\\;[\\,0.9,\\,0.2\\,]\\}$。\n    - 类别 $1$： $\\{[\\,0.2,\\,0.98\\,],\\;[\\,0.0,\\,1.0\\,]\\}$。\n  - 查询集 (嵌入, 标签)：\n    - $[\\,0.70,\\,0.30\\,]$ 标签为 $0$，\n    - $[\\,0.60,\\,0.40\\,]$ 标签为 $0$，\n    - $[\\,0.40,\\,0.60\\,]$ 标签为 $1$，\n    - $[\\,0.30,\\,0.70\\,]$ 标签为 $1$。\n- 片段 $3$ (包含重复支持样本和最小精炼的边界情况)：\n  - $d = 2$, $K = 2$, $\\tau = 1$, $\\lambda = 0.5$, $T = 1$。\n  - 支持集：\n    - 类别 $0$： $\\{[\\,1.0,\\,0.0\\,],\\;[\\,1.0,\\,0.0\\,]\\}$。\n    - 类别 $1$： $\\{[\\,0.0,\\,1.0\\,],\\;[\\,0.0,\\,1.0\\,]\\}$。\n  - 查询集 (嵌入, 标签)：\n    - $[\\,1.0,\\,0.0\\,]$ 标签为 $0$，\n    - $[\\,0.0,\\,1.0\\,]$ 标签为 $1$。\n\n实现要求：\n- 您的程序必须严格按照上述算法实现，且仅使用提供的测试套件。\n- 在进行任何余弦或平均运算之前，所有向量都必须归一化为单位范数；原型在每次更新时都必须重新归一化。\n- 基线和精炼后准确率必须计算为正确分类的查询所占的比例，表示为 $[\\,0,\\,1\\,]$ 范围内的小数。\n- 最终输出格式必须是单行，包含一个逗号分隔的列表，其中包含每个片段的增益，四舍五入到三位小数，并用方括号括起来。例如，输出行必须类似于 $[\\,g_1, g_2, g_3\\,]$，其中每个 $g_i$ 是一个四舍五入到三位小数的浮点数。\n\n不提供外部输入；所有数据都嵌入在程序中。此问题中没有物理单位。如果内部使用角度，则必须以弧度为单位。预期的输出是指定格式的无单位小数。您的程序应生成单行输出，其中包含一个逗号分隔的列表形式的结果，并用方括号括起来 (例如, \"[result1,result2,result3]\")。",
            "solution": "用户提供了一个问题，要求实现并分析一种用于密集向量嵌入的原型精炼算法。该算法是在少样本学习背景下应用的期望最大化（EM）算法的变体。任务是计算对于几个测试片段，该精炼过程相对于基线方法所获得的分类准确率增益。\n\n首先，需要对问题陈述进行验证。\n\n### 步骤1：提取已知条件\n- **向量归一化：** 所有嵌入向量 $\\mathbf{x}$ 都被归一化为单位欧几里得范数：$\\widehat{\\mathbf{x}} = \\mathbf{x} / \\|\\mathbf{x}\\|_2$。我们用 $\\operatorname{norm}(\\mathbf{x})$ 表示这个函数。\n- **基线原型 ($\\mathbf{m}_k^{(0)}$):** 对每个类别 $k$，初始原型是其归一化支持向量的归一化均值。给定类别 $k$ 的支持向量集为 $\\mathcal{S}_k$，原型为 $\\mathbf{m}_k^{(0)} = \\operatorname{norm}\\left(\\frac{1}{|\\mathcal{S}_k|} \\sum_{\\mathbf{s}\\in \\mathcal{S}_k} \\widehat{\\mathbf{s}} \\right)$。\n- **基线分类：** 通过找到具有最高余弦相似度的原型来对查询 $\\widehat{\\mathbf{q}}$ 进行分类：$\\widehat{y}(\\widehat{\\mathbf{q}}) = \\arg\\max_{k} \\widehat{\\mathbf{q}}^\\top \\mathbf{m}_k^{(0)}$。\n- **EM精炼：** 算法迭代 $T$ 步，使用温度参数 $\\tau > 0$ 和查询权重 $\\lambda \\ge 0$ 更新原型 $\\mathbf{m}_k^{(t)}$。\n- **E步骤（责任）：** 对每个查询 $\\widehat{\\mathbf{q}}_i$，类别 $k$ 对此查询的责任是使用基于余弦相似度的softmax计算的：\n  $$r_{ik}^{(t)} = \\frac{\\exp\\left(\\tau \\, \\widehat{\\mathbf{q}}_i^\\top \\mathbf{m}_k^{(t)}\\right)}{\\sum_{j=1}^K \\exp\\left(\\tau \\, \\widehat{\\mathbf{q}}_i^\\top \\mathbf{m}_j^{(t)}\\right)}$$\n- **M步骤（原型更新）：** 通过取其支持向量和所有查询向量的加权和，然后进行归一化，来更新类别 $k$ 的原型。支持向量作为硬分配（权重为1），而查询则通过责任进行软分配：\n  $$\\mathbf{m}_k^{(t+1)} = \\operatorname{norm}\\left(\\sum_{\\mathbf{s}\\in \\mathcal{S}_k} \\widehat{\\mathbf{s}} \\;+\\; \\lambda \\sum_{i} r_{ik}^{(t)} \\widehat{\\mathbf{q}}_i \\right)$$\n- **精炼后分类：** 经过 $T$ 次迭代后，使用最终原型 $\\mathbf{m}_k^{(T)}$ 对查询进行分类。\n- **性能指标（增益）：** 增益 = 精炼后准确率 - 基线准确率。增益必须四舍五入到三位小数。\n- **测试套件：** 提供了三个具体的片段，定义了所有参数（$d, K, \\tau, \\lambda, T$）和数据（支持集和查询集嵌入）。\n\n### 步骤2：使用提取的已知条件进行验证\n对问题进行严格验证。\n1.  **科学依据：** 该问题在机器学习领域，特别是在少样本学习和半监督学习与向量嵌入方面有坚实的基础。EM、余弦相似度和基于原型的分类都是标准概念。数学公式是正确的，并与该领域的文献一致。\n2.  **良置性：** 算法步骤是确定性定义的。给定每个片段的输入数据和参数，该过程会产生一个唯一的、可计算的结果。\n3.  **客观性：** 问题使用精确、客观的数学语言进行表述，没有任何主观性。\n4.  **不完整或矛盾的设置：** 为每个测试用例提供了所有必要的参数和数据。定义是自洽的。例如，在所有步骤中都一致地指定了使用归一化向量（$\\widehat{\\mathbf{s}}$、$\\widehat{\\mathbf{q}}$）。M步骤的公式虽然在其加权方案（支持向量之和与查询的加权和）上略显不寻常，但其定义明确无误，代表了一种有效的设计选择。\n5.  **不现实或不可行：** 问题使用低维向量以保证计算上的易处理性，这对于测试问题是合适的。所有提供的向量都是非零的，确保了归一化总是良定义的。\n6.  **所有其他标准：** 问题是形式化的、相关的、非病态的、非平凡的，并且是可计算验证的。\n\n### 步骤3：结论与行动\n该问题被认为是**有效的**，因为它是科学合理的、自洽的并且是良置的。我们可以着手开发一个解决方案。\n\n### 算法解决方案\n将按照每个片段的指定步骤来实现解决方案。\n\n1.  **数据准备：** 所有支持集和查询集向量首先被归一化为单位欧几里得范数。这是一个至关重要的预处理步骤，因为算法始终在单位超球面上操作。\n2.  **基线性能：**\n    - 对每个类别，通过先求该类别归一化支持向量的平均值，然后重新归一化得到的均值向量来计算基线原型。\n    - 然后，我们通过找到产生最高余弦相似度（对于单位向量，等同于最高点积）的基线原型来对每个归一化的查询向量进行分类。\n    - 基线准确率是正确分类的查询所占的比例。\n3.  **基于EM的精炼：**\n    - 原型用基线原型进行初始化。\n    - 我们通过E步骤和M步骤迭代 $T$ 次。\n    - **E步骤：** 对每个查询，我们计算它与当前原型集的相似度得分。这些得分由温度 $\\tau$ 缩放，然后使用softmax函数转换为关于类别的概率分布（责任）。使用数值稳定的softmax实现来防止由于参数过大而导致的溢出/下溢问题。\n    - **M步骤：** 计算一组新的原型。对每个类别 $k$，新原型是两个分量的和的归一化结果：(i) 其所有归一化支持向量之和，以及 (ii) 所有归一化查询向量的加权和，其中每个查询的贡献由其对类别 $k$ 的责任和全局查询权重 $\\lambda$ 进行缩放。\n4.  **精炼后性能：**\n    - 经过 $T$ 次迭代后，使用最终的精炼原型对查询向量进行分类。\n    - 精炼后准确率是使用这些最终原型正确分类的查询所占的比例。\n5.  **增益计算：**\n    - 对每个片段，增益计算为差值：`精炼后准确率 - 基线准确率`。\n    - 然后将结果按要求四舍五入到三位小数。\n\n实现将处理三个测试片段中的每一个，并计算相应的增益。然后将这些增益编译成一个列表以供输出。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the algorithm, and print the results.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"d\": 3, \"K\": 3, \"tau\": 10, \"lamb\": 1, \"T\": 5,\n            \"supports\": {\n                0: np.array([[1.0, 0.0, 0.0], [0.8, 0.2, 0.0]]),\n                1: np.array([[0.0, 1.0, 0.0], [0.1, 0.9, 0.0]]),\n                2: np.array([[0.0, 0.0, 1.0], [0.1, 0.0, 0.9]])\n            },\n            \"queries\": [\n                (np.array([0.85, 0.15, 0.0]), 0),\n                (np.array([0.15, 0.85, 0.0]), 1),\n                (np.array([0.10, 0.10, 0.98]), 2),\n                (np.array([0.65, 0.35, 0.0]), 0),\n                (np.array([0.35, 0.65, 0.0]), 1)\n            ]\n        },\n        {\n            \"d\": 2, \"K\": 2, \"tau\": 5, \"lamb\": 2, \"T\": 5,\n            \"supports\": {\n                0: np.array([[1.0, 0.0], [0.9, 0.2]]),\n                1: np.array([[0.2, 0.98], [0.0, 1.0]])\n            },\n            \"queries\": [\n                (np.array([0.70, 0.30]), 0),\n                (np.array([0.60, 0.40]), 0),\n                (np.array([0.40, 0.60]), 1),\n                (np.array([0.30, 0.70]), 1)\n            ]\n        },\n        {\n            \"d\": 2, \"K\": 2, \"tau\": 1, \"lamb\": 0.5, \"T\": 1,\n            \"supports\": {\n                0: np.array([[1.0, 0.0], [1.0, 0.0]]),\n                1: np.array([[0.0, 1.0], [0.0, 1.0]])\n            },\n            \"queries\": [\n                (np.array([1.0, 0.0]), 0),\n                (np.array([0.0, 1.0]), 1)\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        gain = run_episode(case['d'], case['K'], case['tau'], case['lamb'], case['T'],\n                           case['supports'], case['queries'])\n        results.append(gain)\n\n    # Format the final output string as [gain1,gain2,gain3] with three decimal places.\n    formatted_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef run_episode(d, K, tau, lamb, T, supports, queries_with_labels):\n    \"\"\"\n    Implements the prototype refinement algorithm for a single few-shot episode.\n    \"\"\"\n    \n    # Helper to normalize a single vector or a 2D array of vectors.\n    def normalize(v):\n        if v.ndim == 1:\n            norm = np.linalg.norm(v)\n            return v / norm if norm > 0 else v\n        # For 2D array of vectors\n        norms = np.linalg.norm(v, axis=1, keepdims=True)\n        # Handle potential division by zero for zero vectors.\n        return np.divide(v, norms, out=np.zeros_like(v, dtype=float), where=norms!=0)\n\n    # Helper for numerically stable softmax.\n    def stable_softmax(x, axis=-1):\n        x = x - np.max(x, axis=axis, keepdims=True)\n        exps = np.exp(x)\n        return exps / np.sum(exps, axis=axis, keepdims=True)\n\n    # --- Step 1: Data Preparation ---\n    queries = np.array([q[0] for q in queries_with_labels])\n    query_labels = np.array([q[1] for q in queries_with_labels])\n    \n    # Normalize all embeddings to unit norm.\n    hat_supports_per_class = {k: normalize(supports[k]) for k in range(K)}\n    hat_queries = normalize(queries)\n\n    # --- Step 2: Baseline Prototypes ---\n    baseline_prototypes = np.zeros((K, d))\n    for k in range(K):\n        mean_s = np.mean(hat_supports_per_class[k], axis=0)\n        baseline_prototypes[k] = normalize(mean_s)\n\n    # --- Step 3: Baseline Classification ---\n    # For unit vectors, cosine similarity is the dot product.\n    baseline_scores = hat_queries @ baseline_prototypes.T\n    baseline_preds = np.argmax(baseline_scores, axis=1)\n    baseline_accuracy = np.mean(baseline_preds == query_labels)\n\n    # --- Step 4: EM Refinement ---\n    refined_prototypes = np.copy(baseline_prototypes)\n    \n    # Pre-calculate sum of normalized support vectors (constant through iterations).\n    sum_hat_supports = np.array([np.sum(hat_supports_per_class[k], axis=0) for k in range(K)])\n\n    for _ in range(T):\n        # E-step: Compute responsibilities for each query.\n        em_scores = tau * (hat_queries @ refined_prototypes.T)\n        responsibilities = stable_softmax(em_scores, axis=1)\n        \n        # M-step: Update prototypes.\n        weighted_sum_queries = responsibilities.T @ hat_queries\n        new_prototypes_unnorm = sum_hat_supports + lamb * weighted_sum_queries\n        refined_prototypes = normalize(new_prototypes_unnorm)\n\n    # --- Step 5: Refined Classification ---\n    refined_scores = hat_queries @ refined_prototypes.T\n    refined_preds = np.argmax(refined_scores, axis=1)\n    refined_accuracy = np.mean(refined_preds == query_labels)\n\n    # --- Step 6: Gain Calculation ---\n    gain = refined_accuracy - baseline_accuracy\n    return gain\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}