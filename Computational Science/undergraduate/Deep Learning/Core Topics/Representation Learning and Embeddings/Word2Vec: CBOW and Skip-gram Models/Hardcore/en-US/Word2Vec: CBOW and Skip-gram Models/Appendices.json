{
    "hands_on_practices": [
        {
            "introduction": "Understanding a model like Word2Vec begins with its most fundamental operation: the learning step. This exercise demystifies the training process by focusing on a single Stochastic Gradient Descent (SGD) step for the Skip-gram model with negative sampling . By manually calculating the gradients and the resulting vector updates, you will gain a concrete intuition for the \"push-pull\" dynamic where the model encourages similarity for positive word pairs and dissimilarity for negative ones, which is the core mechanism for learning semantic relationships.",
            "id": "3200045",
            "problem": "Consider a toy corpus over the vocabulary $\\{a,b,c\\}$ consisting of two sentences: $a\\; b\\; a$ and $b\\; c\\; b$. Train a skip-gram Word2Vec model with negative sampling on this corpus. Use the following setup.\n\n- Use a symmetric context window of size $1$. Focus on a single training instance formed by taking the center word $b$ at position $2$ of the first sentence $a\\; b\\; a$ and the context word $a$ at position $1$. Thus, the positive pair is $(b \\to a)$.\n- Use negative sampling with $k=2$ negatives, drawn from the uniform distribution over the vocabulary, but for this step, treat the negatives as fixed deterministically to be $\\{b,c\\}$ in that order.\n- The embedding dimension is $d=2$. There are separate input (center) and output (context) embeddings. The initial input (center) embeddings are\n$$\nv_a=\\begin{pmatrix}0\\\\0\\end{pmatrix},\\quad\nv_b=\\begin{pmatrix}1\\\\0\\end{pmatrix},\\quad\nv_c=\\begin{pmatrix}0\\\\1\\end{pmatrix},\n$$\nand the initial output (context) embeddings are\n$$\nu_a=\\begin{pmatrix}0\\\\1\\end{pmatrix},\\quad\nu_b=\\begin{pmatrix}1\\\\0\\end{pmatrix},\\quad\nu_c=\\begin{pmatrix}0\\\\-1\\end{pmatrix}.\n$$\n- Use Stochastic Gradient Descent (SGD) with learning rate $\\alpha=0.2$.\n\nFor one training step on the instance $(b \\to a)$ with negative samples $\\{b,c\\}$, use the single-instance negative sampling loss\n$$\n\\ell\\!\\left(v_b, u_a, \\{u_b, u_c\\}\\right)\\;=\\;-\\ln \\sigma\\!\\left(u_a^{\\top} v_b\\right)\\;-\\;\\sum_{n\\in\\{b,c\\}} \\ln \\sigma\\!\\left(-\\,u_n^{\\top} v_b\\right),\n$$\nwhere $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$ is the logistic sigmoid. Starting from the above initial embeddings, perform one SGD update on $v_b$, $u_a$, $u_b$, and $u_c$ based on this single instance (all other parameters remain unchanged). Derive the necessary gradients from the stated loss and the definition of $\\sigma(x)$ using first principles.\n\nWhat is the updated value of the first component of the input vector $v_b$ after this single SGD step? Round your answer to four significant figures.\n\nAdditionally, in your working, briefly explain how the signs and magnitudes of the updates reflect the push-pull dynamics between the positive pair and the negative samples. Your final reported answer must be only the requested scalar.",
            "solution": "This problem requires us to perform a single Stochastic Gradient Descent (SGD) update for a Skip-gram model with negative sampling. We need to calculate the gradient of the loss function with respect to the input (center) word embedding $v_b$ and then apply the update rule.\n\n**1. Define the Loss and Gradients**\nThe loss function for a single positive pair (center word $w$, context word $c$) and a set of negative samples $N$ is:\n$$ \\ell = -\\ln \\sigma(u_c^{\\top} v_w) - \\sum_{n \\in N} \\ln \\sigma(-u_n^{\\top} v_w) $$\nThe gradient of this loss with respect to the center word embedding $v_w$ is given by:\n$$ \\frac{\\partial \\ell}{\\partial v_w} = (\\sigma(u_c^{\\top} v_w) - 1)u_c + \\sum_{n \\in N} \\sigma(u_n^{\\top} v_w) u_n $$\nIn our case, the center word is $b$, the positive context is $a$, and the negative samples are $\\{b, c\\}$. So, we need to compute $\\frac{\\partial \\ell}{\\partial v_b}$.\n$$ \\frac{\\partial \\ell}{\\partial v_b} = (\\sigma(u_a^{\\top} v_b) - 1)u_a + \\sigma(u_b^{\\top} v_b)u_b + \\sigma(u_c^{\\top} v_b)u_c $$\n\n**2. Calculate Dot Products**\nUsing the initial embeddings:\n$v_b = \\begin{pmatrix}1\\\\0\\end{pmatrix}$, $u_a = \\begin{pmatrix}0\\\\1\\end{pmatrix}$, $u_b = \\begin{pmatrix}1\\\\0\\end{pmatrix}$, $u_c = \\begin{pmatrix}0\\\\-1\\end{pmatrix}$\n\n-   Positive pair score: $s_{pos} = u_a^{\\top} v_b = \\begin{pmatrix}0 & 1\\end{pmatrix} \\begin{pmatrix}1\\\\0\\end{pmatrix} = 0 \\cdot 1 + 1 \\cdot 0 = 0$.\n-   Negative sample scores:\n    -   $s_{neg1} = u_b^{\\top} v_b = \\begin{pmatrix}1 & 0\\end{pmatrix} \\begin{pmatrix}1\\\\0\\end{pmatrix} = 1 \\cdot 1 + 0 \\cdot 0 = 1$.\n    -   $s_{neg2} = u_c^{\\top} v_b = \\begin{pmatrix}0 & -1\\end{pmatrix} \\begin{pmatrix}1\\\\0\\end{pmatrix} = 0 \\cdot 1 + (-1) \\cdot 0 = 0$.\n\n**3. Calculate Sigmoid Values**\n-   $\\sigma(s_{pos}) = \\sigma(0) = \\frac{1}{1+e^0} = 0.5$.\n-   $\\sigma(s_{neg1}) = \\sigma(1) = \\frac{1}{1+e^{-1}} \\approx 0.731059$.\n-   $\\sigma(s_{neg2}) = \\sigma(0) = 0.5$.\n\n**4. Calculate the Gradient Vector**\nNow substitute these values into the gradient formula for $v_b$:\n\\begin{align*}\n\\frac{\\partial \\ell}{\\partial v_b} &= (\\sigma(0) - 1)u_a + \\sigma(1)u_b + \\sigma(0)u_c \\\\\n&= (0.5 - 1)\\begin{pmatrix}0\\\\1\\end{pmatrix} + 0.731059\\begin{pmatrix}1\\\\0\\end{pmatrix} + 0.5\\begin{pmatrix}0\\\\-1\\end{pmatrix} \\\\\n&= -0.5\\begin{pmatrix}0\\\\1\\end{pmatrix} + 0.731059\\begin{pmatrix}1\\\\0\\end{pmatrix} + 0.5\\begin{pmatrix}0\\\\-1\\end{pmatrix} \\\\\n&= \\begin{pmatrix}0\\\\-0.5\\end{pmatrix} + \\begin{pmatrix}0.731059\\\\0\\end{pmatrix} + \\begin{pmatrix}0\\\\-0.5\\end{pmatrix} \\\\\n&= \\begin{pmatrix}0.731059 \\\\ -1.0\\end{pmatrix}\n\\end{align*}\n\n**5. Apply the SGD Update Rule**\nThe SGD update rule is $v_{b, \\text{new}} = v_b - \\alpha \\frac{\\partial \\ell}{\\partial v_b}$, with learning rate $\\alpha = 0.2$.\n\\begin{align*}\nv_{b, \\text{new}} &= \\begin{pmatrix}1\\\\0\\end{pmatrix} - 0.2 \\begin{pmatrix}0.731059\\\\-1.0\\end{pmatrix} \\\\\n&= \\begin{pmatrix}1\\\\0\\end{pmatrix} - \\begin{pmatrix}0.1462118\\\\-0.2\\end{pmatrix} \\\\\n&= \\begin{pmatrix}1 - 0.1462118 \\\\ 0 - (-0.2)\\end{pmatrix} \\\\\n&= \\begin{pmatrix}0.8537882 \\\\ 0.2\\end{pmatrix}\n\\end{align*}\n\nThe updated value of the first component of the input vector $v_b$ is approximately $0.8537882$. Rounding to four significant figures gives $0.8538$.\n\n**Push-Pull Dynamics Explanation**\nThe gradient $\\frac{\\partial \\ell}{\\partial v_b}$ is a sum of three vectors. The first term, $(\\sigma(u_a^\\top v_b)-1)u_a = -0.5 u_a$, is the \"pull\" from the positive sample. In an SGD step (which subtracts the gradient), this update component $-(\\sigma-1)u_a = (1-\\sigma)u_a$ moves $v_b$ *towards* the positive context vector $u_a$. The other two terms, $\\sigma(u_b^\\top v_b)u_b + \\sigma(u_c^\\top v_b)u_c$, are the \"push\" from the negative samples. The SGD update components $-\\sigma(u_n^\\top v_b)u_n$ move $v_b$ *away* from the negative sample vectors $u_b$ and $u_c$. The magnitude of each push or pull is scaled by the model's error for that pair: for the positive pair, the error is large when $\\sigma(u_a^\\top v_b)$ is small (close to 0); for negative pairs, the error is large when $\\sigma(u_n^\\top v_b)$ is large (close to 1).",
            "answer": "$$\\boxed{0.8538}$$"
        },
        {
            "introduction": "After mastering the Skip-gram update rule, it is valuable to dissect its architectural counterpart, the Continuous Bag-of-Words (CBOW) model. This practice shifts focus to the CBOW framework, where the model aggregates vectors from a context window to predict a central target word . You will derive the gradient with respect to a context word's embedding and apply this to a mini-batch setting, reinforcing your calculus skills and highlighting the differences in gradient flow compared to the Skip-gram model.",
            "id": "3200079",
            "problem": "Consider the Continuous Bag-of-Words (CBOW) model in Word2Vec. Let the context set be $C=\\{c_1,\\dots,c_{|C|}\\}$, where each context word $c_i$ has an input embedding vector $v_{c_i}\\in\\mathbb{R}^d$. The CBOW hidden representation is the average\n$$\nh=\\frac{1}{|C|}\\sum_{i=1}^{|C|}v_{c_i}.\n$$\nFor a target word $w$ with output embedding vector $u_w\\in\\mathbb{R}^d$, define the score $s=u_w^{\\top}h$ and the predicted probability $\\hat{y}=\\sigma(s)$, where $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$ is the logistic sigmoid. For a labeled pair $(C,w,y)$ with $y\\in\\{0,1\\}$, the objective to be minimized is the binary cross-entropy\n$$\nJ(C,w,y)=-\\Big(y\\,\\ln(\\sigma(s))+(1-y)\\,\\ln\\big(1-\\sigma(s)\\big)\\Big).\n$$\nFor a mini-batch that sums the losses $J$ over pairs $\\{(C^{(b)},w^{(b)},y^{(b)})\\}_{b=1}^B$, the gradient with respect to each $v_{c_i}$ is the sum of per-pair gradients.\n\nTask:\n1. Starting from the definitions above and basic calculus, derive the gradient $\\frac{\\partial J}{\\partial v_{c_i}}$ for a single pair $(C,w,y)$, expressing it in terms of $u_w$, $h$, $\\sigma(u_w^{\\top}h)$, $y$, and $|C|$.\n2. Then, consider the following specific mini-batch with shared context ($d=3$ and $|C|=2$):\n   - Context embeddings: $v_{c_1}=\\begin{pmatrix}1\\\\0\\\\2\\end{pmatrix}$ and $v_{c_2}=\\begin{pmatrix}0\\\\-1\\\\1\\end{pmatrix}$, so the same $h$ is used for all pairs in the mini-batch.\n   - One positive pair $(y=1)$ with target output embedding $u_{w^{(+)}}=\\begin{pmatrix}1\\\\2\\\\-1\\end{pmatrix}$.\n   - Two negative pairs $(y=0)$ with target output embeddings $u_{w^{(-1)}}=\\begin{pmatrix}-1\\\\0\\\\1\\end{pmatrix}$ and $u_{w^{(-2)}}=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}$.\n   The mini-batch gradient with respect to $v_{c_1}$ is the sum of the three per-pair gradients obtained in Part 1, each evaluated at the appropriate $y$ and $u_w$.\n3. Let $d=\\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix}$. Compute the scalar inner product $d^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right)$ for this mini-batch. Round your final numerical answer to four significant figures.\n\nYour final answer must be a single real number.",
            "solution": "The problem asks for a three-part calculation related to the gradients in a Continuous Bag-of-Words (CBOW) model. First, we must derive the general form of the gradient of the loss function with respect to a context word's input embedding. Second, we apply this to a specific mini-batch. Third, we compute a scalar projection of the resulting mini-batch gradient.\n\n**Part 1: Derivation of the Gradient $\\frac{\\partial J}{\\partial v_{c_i}}$**\n\nThe objective function for a single training pair $(C,w,y)$ is the binary cross-entropy loss:\n$$\nJ(C,w,y) = -\\Big(y \\ln(\\sigma(s)) + (1-y) \\ln(1-\\sigma(s))\\Big)\n$$\nwhere the score $s$ is defined as $s = u_w^{\\top} h$, and the hidden representation $h$ is the average of the context word vectors:\n$$\nh = \\frac{1}{|C|} \\sum_{j=1}^{|C|} v_{c_j}\n$$\nWe need to find the gradient of $J$ with respect to one of the context word vectors, $v_{c_i} \\in \\mathbb{R}^d$. We use the chain rule of calculus. The gradient $\\frac{\\partial J}{\\partial v_{c_i}}$ is a vector whose $k$-th component is $\\frac{\\partial J}{\\partial v_{c_{i,k}}}$, where $v_{c_{i,k}}$ is the $k$-th component of the vector $v_{c_i}$.\n\nLet's apply the chain rule:\n$$\n\\frac{\\partial J}{\\partial v_{c_{i,k}}} = \\frac{\\partial J}{\\partial s} \\frac{\\partial s}{\\partial v_{c_{i,k}}}\n$$\nFirst, we compute the derivative of the loss $J$ with respect to the score $s$. We use the fact that the derivative of the sigmoid function is $\\frac{d\\sigma(x)}{dx} = \\sigma(x)(1-\\sigma(x))$.\n\\begin{align*}\n\\frac{\\partial J}{\\partial s} &= -\\frac{\\partial}{\\partial s} \\Big(y \\ln(\\sigma(s)) + (1-y) \\ln(1-\\sigma(s))\\Big) \\\\\n&= -\\left( y \\frac{1}{\\sigma(s)} \\frac{d\\sigma(s)}{ds} + (1-y) \\frac{1}{1-\\sigma(s)} \\left(-\\frac{d\\sigma(s)}{ds}\\right) \\right) \\\\\n&= -\\left( y \\frac{1}{\\sigma(s)} \\sigma(s)(1-\\sigma(s)) - (1-y) \\frac{1}{1-\\sigma(s)} \\sigma(s)(1-\\sigma(s)) \\right) \\\\\n&= -\\big( y(1-\\sigma(s)) - (1-y)\\sigma(s) \\big) \\\\\n&= -(y - y\\sigma(s) - \\sigma(s) + y\\sigma(s)) \\\\\n&= -(y - \\sigma(s)) \\\\\n&= \\sigma(s) - y\n\\end{align*}\nThis is a standard result for binary cross-entropy loss with a final sigmoid activation.\n\nNext, we compute the derivative of the score $s$ with respect to the $k$-th component of the vector $v_{c_i}$. The score $s$ can be written in terms of components:\n$$\ns = u_w^{\\top} h = \\sum_{j=1}^d u_{w,j} h_j = \\sum_{j=1}^d u_{w,j} \\left( \\frac{1}{|C|} \\sum_{l=1}^{|C|} v_{c_{l,j}} \\right)\n$$\nNow we differentiate with respect to $v_{c_{i,k}}$:\n$$\n\\frac{\\partial s}{\\partial v_{c_{i,k}}} = \\frac{\\partial}{\\partial v_{c_{i,k}}} \\left( \\sum_{j=1}^d u_{w,j} \\frac{1}{|C|} \\sum_{l=1}^{|C|} v_{c_{l,j}} \\right)\n$$\nThe derivative $\\frac{\\partial v_{c_{l,j}}}{\\partial v_{c_{i,k}}}$ is $1$ if $l=i$ and $j=k$, and $0$ otherwise. Thus, only one term in the sums is non-zero with respect to this differentiation.\n$$\n\\frac{\\partial s}{\\partial v_{c_{i,k}}} = u_{w,k} \\frac{1}{|C|} (1) = \\frac{1}{|C|} u_{w,k}\n$$\nCombining the two parts:\n$$\n\\frac{\\partial J}{\\partial v_{c_{i,k}}} = \\frac{\\partial J}{\\partial s} \\frac{\\partial s}{\\partial v_{c_{i,k}}} = (\\sigma(s)-y) \\frac{1}{|C|} u_{w,k}\n$$\nTo form the gradient vector $\\frac{\\partial J}{\\partial v_{c_i}}$, we assemble these components for $k=1, \\dots, d$:\n$$\n\\frac{\\partial J}{\\partial v_{c_i}} = \\begin{pmatrix} \\frac{\\partial J}{\\partial v_{c_{i,1}}} \\\\ \\vdots \\\\ \\frac{\\partial J}{\\partial v_{c_{i,d}}} \\end{pmatrix} = \\frac{\\sigma(s)-y}{|C|} \\begin{pmatrix} u_{w,1} \\\\ \\vdots \\\\ u_{w,d} \\end{pmatrix} = \\frac{\\sigma(u_w^{\\top}h)-y}{|C|} u_w\n$$\nThis completes the first part of the task.\n\n**Part 2 & 3: Mini-batch Gradient Calculation and Projection**\n\nThe problem asks for the scalar value $d^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right)$. The mini-batch gradient is the sum of the gradients from each of the three pairs in the batch. Let the pairs be indexed by $b \\in \\{ (+), (-1), (-2) \\}$.\n$$\n\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}} = \\sum_{b} \\frac{\\partial J^{(b)}}{\\partial v_{c_1}}\n$$\nUsing the result from Part 1, we have:\n$$\n\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}} = \\sum_{b} \\frac{\\sigma({u_w^{(b)}}^{\\top}h)-y^{(b)}}{|C|} u_w^{(b)}\n$$\nWe want to compute the inner product of this gradient with the vector $d$:\n$$\nd^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right) = d^{\\top} \\sum_{b} \\frac{\\sigma({u_w^{(b)}}^{\\top}h)-y^{(b)}}{|C|} u_w^{(b)}\n$$\nBy linearity of the inner product, we can write:\n$$\nd^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right) = \\frac{1}{|C|} \\sum_{b} \\left(\\sigma({u_w^{(b)}}^{\\top}h)-y^{(b)}\\right) (d^{\\top}u_w^{(b)})\n$$\nFirst, we calculate the shared hidden representation $h$.\nGiven $|C|=2$, $v_{c_1}=\\begin{pmatrix}1\\\\0\\\\2\\end{pmatrix}$, and $v_{c_2}=\\begin{pmatrix}0\\\\-1\\\\1\\end{pmatrix}$:\n$$\nh = \\frac{1}{2}(v_{c_1} + v_{c_2}) = \\frac{1}{2}\\left(\\begin{pmatrix}1\\\\0\\\\2\\end{pmatrix} + \\begin{pmatrix}0\\\\-1\\\\1\\end{pmatrix}\\right) = \\frac{1}{2}\\begin{pmatrix}1\\\\-1\\\\3\\end{pmatrix} = \\begin{pmatrix}0.5\\\\-0.5\\\\1.5\\end{pmatrix}\n$$\nNext, we calculate the terms for each pair in the mini-batch.\n\n1.  **Positive pair (+):** $y^{(+)}=1$, $u_{w^{(+)}}=\\begin{pmatrix}1\\\\2\\\\-1\\end{pmatrix}$\n    - Score $s^{(+)} = {u_{w^{(+)}}}^{\\top}h = \\begin{pmatrix}1 & 2 & -1\\end{pmatrix}\\begin{pmatrix}0.5\\\\-0.5\\\\1.5\\end{pmatrix} = 0.5 - 1.0 - 1.5 = -2$.\n    - Inner product with $d$: $d^{\\top}u_{w^{(+)}} = \\begin{pmatrix}1 & 0 & -1\\end{pmatrix}\\begin{pmatrix}1\\\\2\\\\-1\\end{pmatrix} = 1+0+1 = 2$.\n    - Contribution to sum: $(\\sigma(-2)-1)(2)$.\n\n2.  **First negative pair (-1):** $y^{(-1)}=0$, $u_{w^{(-1)}}=\\begin{pmatrix}-1\\\\0\\\\1\\end{pmatrix}$\n    - Score $s^{(-1)} = {u_{w^{(-1)}}}^{\\top}h = \\begin{pmatrix}-1 & 0 & 1\\end{pmatrix}\\begin{pmatrix}0.5\\\\-0.5\\\\1.5\\end{pmatrix} = -0.5+0+1.5 = 1$.\n    - Inner product with $d$: $d^{\\top}u_{w^{(-1)}} = \\begin{pmatrix}1 & 0 & -1\\end{pmatrix}\\begin{pmatrix}-1\\\\0\\\\1\\end{pmatrix} = -1+0-1 = -2$.\n    - Contribution to sum: $(\\sigma(1)-0)(-2) = -2\\sigma(1)$.\n\n3.  **Second negative pair (-2):** $y^{(-2)}=0$, $u_{w^{(-2)}}=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}$\n    - Score $s^{(-2)} = {u_{w^{(-2)}}}^{\\top}h = \\begin{pmatrix}0 & 1 & 0\\end{pmatrix}\\begin{pmatrix}0.5\\\\-0.5\\\\1.5\\end{pmatrix} = 0-0.5+0 = -0.5$.\n    - Inner product with $d$: $d^{\\top}u_{w^{(-2)}} = \\begin{pmatrix}1 & 0 & -1\\end{pmatrix}\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix} = 0+0+0 = 0$.\n    - Contribution to sum: $(\\sigma(-0.5)-0)(0) = 0$.\n\nNow, we sum the contributions and multiply by $\\frac{1}{|C|} = \\frac{1}{2}$:\n$$\nd^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right) = \\frac{1}{2} \\Big[ (\\sigma(-2)-1)(2) - 2\\sigma(1) + 0 \\Big] = \\sigma(-2) - 1 - \\sigma(1)\n$$\nUsing the property $\\sigma(-x) = 1 - \\sigma(x)$, we have $\\sigma(-2) = 1-\\sigma(2)$. Substituting this in:\n$$\nd^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right) = (1-\\sigma(2)) - 1 - \\sigma(1) = -\\sigma(2) - \\sigma(1)\n$$\nNow we compute the numerical value:\n$$\n-\\sigma(2) - \\sigma(1) = -\\left( \\frac{1}{1+\\exp(-2)} + \\frac{1}{1+\\exp(-1)} \\right)\n$$\nWe use the values $e \\approx 2.71828$, $e^2 \\approx 7.38906$:\n- $\\exp(-2) \\approx 0.135335$\n- $\\exp(-1) \\approx 0.367879$\n$$\n\\sigma(2) = \\frac{1}{1+0.135335} \\approx \\frac{1}{1.135335} \\approx 0.880797\n$$\n$$\n\\sigma(1) = \\frac{1}{1+0.367879} \\approx \\frac{1}{1.367879} \\approx 0.731059\n$$\nThe final value is:\n$$\n-0.880797 - 0.731059 = -1.611856\n$$\nRounding to four significant figures gives $-1.612$.",
            "answer": "$$\\boxed{-1.612}$$"
        },
        {
            "introduction": "Moving beyond the mechanics of gradient updates, a deeper understanding of Word2Vec involves connecting its training objective to broader principles of distributional semantics. This advanced practice explores the profound discovery that Skip-gram with Negative Sampling (SGNS) implicitly factorizes a matrix of Pointwise Mutual Information (PMI) between words and contexts . Through a guided coding exercise, you will analyze the model's sensitivity to changes in the underlying data statistics, bridging the gap between abstract theory and the practical behavior of the algorithm.",
            "id": "3200066",
            "problem": "You are given the task of quantifying how the inner product $v_{w}^{\\top} u_{c}$ predicted by the skip-gram with negative sampling model responds to small, controlled perturbations in a co-occurrence matrix. Start from the fundamental definitions: a co-occurrence count matrix $M$ over a vocabulary of size $n$, the corresponding empirical distributions over words and contexts derived from $M$, the pointwise mutual information between a word and a context, and the skip-gram with negative sampling (SGNS) objective with $k$ negative samples per positive pair drawn from the empirical context distribution. Derive, from these bases, a relationship between the stationary point value of the inner product $x_{w,c} = v_{w}^{\\top} u_{c}$ and a statistic computed from $M$ and $k$, then use this relationship to obtain a first-order sensitivity (linear approximation) of $x_{w,c}$ with respect to perturbations $\\Delta M$ to $M$. Use additive Laplace smoothing with parameter $\\tau$ to avoid degeneracies, replacing $M$ by $\\widetilde{M} = M + \\tau$ elementwise in all count-based expressions. Use the natural logarithm. Indices are zero-based.\n\nFor each test case below, do the following:\n- Let $M \\in \\mathbb{N}_{0}^{n \\times n}$ be the baseline co-occurrence count matrix, $\\Delta M \\in \\mathbb{Z}^{n \\times n}$ be a perturbation with the property that $M + \\Delta M \\in \\mathbb{N}_{0}^{n \\times n}$ elementwise, $k \\in \\mathbb{N}$ be the number of negative samples per positive example, and $\\tau \\in \\mathbb{R}_{>0}$ be the Laplace smoothing parameter.\n- Define the smoothed counts $\\widetilde{M} = M + \\tau$ elementwise, smoothed row sums $\\widetilde{R}_{w} = \\sum_{j=0}^{n-1} \\widetilde{M}_{w,j}$, smoothed column sums $\\widetilde{C}_{c} = \\sum_{i=0}^{n-1} \\widetilde{M}_{i,c}$, and smoothed total $\\widetilde{T} = \\sum_{i=0}^{n-1} \\sum_{j=0}^{n-1} \\widetilde{M}_{i,j}$.\n- Consider a target pair $(w^{\\star}, c^{\\star})$. Compute the exact difference\n$$\n\\Delta x_{\\mathrm{exact}} = x_{w^{\\star},c^{\\star}}(M + \\Delta M, k, \\tau) - x_{w^{\\star},c^{\\star}}(M, k, \\tau),\n$$\nwhere $x_{w,c}(M,k,\\tau)$ denotes the stationary point prediction for $v_{w}^{\\top} u_{c}$ computed from $M$, $k$, and $\\tau$ obtained from the derived relationship mentioned above.\n- Compute the first-order approximation\n$$\n\\Delta x_{\\mathrm{lin}} \\approx \\sum_{i=0}^{n-1} \\sum_{j=0}^{n-1} \\frac{\\partial x_{w^{\\star},c^{\\star}}}{\\partial M_{i,j}} \\Bigg|_{M} \\cdot (\\Delta M)_{i,j},\n$$\nwhere all partial derivatives are taken at the baseline $M$ using the smoothed quantities with parameter $\\tau$.\n- Report the absolute deviation\n$$\ne = \\left| \\Delta x_{\\mathrm{exact}} - \\Delta x_{\\mathrm{lin}} \\right|.\n$$\n\nImplement a program that computes $e$ for each test case and produces a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[0.1,0.02,0.0]$).\n\nUse the following test suite. All indices are zero-based, and all integer arrays are written in row-major order.\n\n- Test case $1$:\n    - $n = 3$,\n    - $M = \\begin{bmatrix} 10 & 2 & 1 \\\\ 1 & 5 & 0 \\\\ 0 & 1 & 4 \\end{bmatrix}$,\n    - $\\Delta M$ equals $+1$ at $(0,1)$ and $0$ elsewhere,\n    - $(w^{\\star}, c^{\\star}) = (0,1)$,\n    - $k = 2$,\n    - $\\tau = 0.5$.\n\n- Test case $2$:\n    - $n = 4$,\n    - $M = \\begin{bmatrix} 20 & 1 & 0 & 0 \\\\ 0 & 15 & 2 & 1 \\\\ 1 & 0 & 10 & 1 \\\\ 0 & 2 & 1 & 8 \\end{bmatrix}$,\n    - $\\Delta M$ equals $+5$ at $(2,3)$ and $0$ elsewhere,\n    - $(w^{\\star}, c^{\\star}) = (2,3)$,\n    - $k = 5$,\n    - $\\tau = 0.1$.\n\n- Test case $3$:\n    - $n = 3$,\n    - $M = \\begin{bmatrix} 9 & 4 & 2 \\\\ 6 & 7 & 3 \\\\ 1 & 2 & 5 \\end{bmatrix}$,\n    - $\\Delta M$ has $+3$ at $(1,0)$, $-3$ at $(1,1)$, $+3$ at $(0,1)$, $-3$ at $(0,0)$, and $0$ elsewhere,\n    - $(w^{\\star}, c^{\\star}) = (1,1)$,\n    - $k = 3$,\n    - $\\tau = 0.3$.\n\n- Test case $4$:\n    - $n = 3$,\n    - $M = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}$,\n    - $\\Delta M$ equals $+1$ at $(0,2)$ and $0$ elsewhere,\n    - $(w^{\\star}, c^{\\star}) = (0,2)$,\n    - $k = 1$,\n    - $\\tau = 0.25$.\n\nYour program must:\n- Use the natural logarithm.\n- Use the additive Laplace smoothing described above.\n- Compute $e$ for each test case.\n- Produce exactly one line of output in the format $[e_{1},e_{2},e_{3},e_{4}]$, where each $e_{i}$ is a floating-point number.\n\nNote: There are no physical units, angle units, or percentages in this problem; no unit conversions are required.",
            "solution": "The user requests an analysis of the sensitivity of the skip-gram with negative sampling (SGNS) model's predictions to perturbations in the underlying co-occurrence matrix $M$. The solution requires deriving the stationary point solution for the inner product $x_{w,c} = v_w^\\top u_c$ and its first-order sensitivity to changes in $M$.\n\n### Step 1: Derivation of the Stationary Point Relationship\n\nThe objective function for SGNS, summed over all word-context pairs in a corpus represented by a count matrix $M$, is given by:\n$$\n\\mathcal{L} = \\sum_{w,c} M_{w,c} \\left( \\log \\sigma(v_w^\\top u_c) + k \\cdot \\mathbb{E}_{c_N \\sim P_C}[\\log \\sigma(-v_w^\\top u_{c_N})] \\right)\n$$\nwhere $v_w$ and $u_c$ are vector representations for word $w$ and context $c$, $\\sigma(z) = (1+e^{-z})^{-1}$ is the sigmoid function, $k$ is the number of negative samples, and $P_C$ is the noise distribution for contexts. The problem specifies using additive Laplace smoothing with parameter $\\tau$, so we replace $M$ with $\\widetilde{M} = M+\\tau$. The total count for each word $w$ is its row sum $\\widetilde{R}_w = \\sum_j \\widetilde{M}_{w,j}$, and the noise distribution $P_C$ is the empirical context distribution $P_C(c) = \\widetilde{C}_c / \\widetilde{T}$, where $\\widetilde{C}_c = \\sum_i \\widetilde{M}_{i,c}$ and $\\widetilde{T} = \\sum_{i,j} \\widetilde{M}_{i,j}$.\n\nLet $x_{w,c} = v_w^\\top u_c$. The objective function can be rewritten as:\n$$\n\\mathcal{L} = \\sum_{w,c} \\widetilde{M}_{w,c} \\log \\sigma(x_{w,c}) + \\sum_{w} \\widetilde{R}_w \\cdot k \\cdot \\sum_{c'} \\frac{\\widetilde{C}_{c'}}{\\widetilde{T}} \\log\\sigma(-x_{w,c'})\n$$\nFollowing Levy and Goldberg (2014), we find the stationary point by notionally treating each $x_{w,c}$ as an independent variable and setting the partial derivative of the objective with respect to it to zero. This is a simplification but yields the established result.\n\nThe partial derivative of $\\mathcal{L}$ with respect to a specific $x_{w,c}$ is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{w,c}} = \\widetilde{M}_{w,c} \\frac{d}{dx_{w,c}}\\log\\sigma(x_{w,c}) + \\widetilde{R}_w \\cdot k \\cdot \\frac{\\widetilde{C}_c}{\\widetilde{T}} \\frac{d}{dx_{w,c}}\\log\\sigma(-x_{w,c}) = 0\n$$\nUsing the identities $\\frac{d}{dz}\\log\\sigma(z) = 1-\\sigma(z)$ and $\\frac{d}{dz}\\log\\sigma(-z) = -\\sigma(z)$, we get:\n$$\n\\widetilde{M}_{w,c} (1-\\sigma(x_{w,c})) - \\widetilde{R}_w \\cdot k \\cdot \\frac{\\widetilde{C}_c}{\\widetilde{T}} \\sigma(x_{w,c}) = 0\n$$\nSolving for $\\sigma(x_{w,c})$:\n$$\n\\widetilde{M}_{w,c} = \\sigma(x_{w,c}) \\left(\\widetilde{M}_{w,c} + k \\frac{\\widetilde{R}_w \\widetilde{C}_c}{\\widetilde{T}}\\right)\n\\implies \\sigma(x_{w,c}) = \\frac{\\widetilde{M}_{w,c}}{\\widetilde{M}_{w,c} + k \\frac{\\widetilde{R}_w \\widetilde{C}_c}{\\widetilde{T}}}\n$$\nUsing the relation $e^{-z} = \\frac{1}{\\sigma(z)} - 1$, we solve for $x_{w,c}$:\n$$\ne^{-x_{w,c}} = \\frac{\\widetilde{M}_{w,c} + k \\frac{\\widetilde{R}_w \\widetilde{C}_c}{\\widetilde{T}}}{\\widetilde{M}_{w,c}} - 1 = \\frac{k \\widetilde{R}_w \\widetilde{C}_c}{\\widetilde{M}_{w,c} \\widetilde{T}}\n$$\nTaking the natural logarithm of both sides gives:\n$$\n-x_{w,c} = \\log\\left(\\frac{k \\widetilde{R}_w \\widetilde{C}_c}{\\widetilde{M}_{w,c} \\widetilde{T}}\\right)\n$$\n$$\nx_{w,c} = \\log\\left(\\frac{\\widetilde{M}_{w,c} \\widetilde{T}}{\\widetilde{R}_w \\widetilde{C}_c}\\right) - \\log k\n$$\nThe first term is the Pointwise Mutual Information (PMI) based on the smoothed counts from $\\widetilde{M}$. Thus, the stationary point solution is $x_{w,c} = \\mathrm{PMI}_{\\widetilde{M}}(w,c) - \\log k$.\n\n### Step 2: First-Order Sensitivity Analysis\n\nWe need to compute the first-order approximation of the change in $x_{w^{\\star},c^{\\star}}$ due to a perturbation $\\Delta M$. The approximation is given by:\n$$\n\\Delta x_{\\mathrm{lin}} = \\sum_{i=0}^{n-1} \\sum_{j=0}^{n-1} \\frac{\\partial x_{w^{\\star},c^{\\star}}}{\\partial M_{i,j}} \\Bigg|_{M} \\cdot (\\Delta M)_{i,j}\n$$\nTo compute this, we first find the general partial derivative of $x_{w^{\\star},c^{\\star}}$ with respect to an arbitrary element $M_{i,j}$ of the count matrix. Let $w=w^\\star$ and $c=c^\\star$. The expression for $x_{w,c}$ is:\n$$\nx_{w,c} = \\log(\\widetilde{M}_{w,c}) + \\log(\\widetilde{T}) - \\log(\\widetilde{R}_{w}) - \\log(\\widetilde{C}_{c}) - \\log k\n$$\nThe quantities $\\widetilde{M}, \\widetilde{T}, \\widetilde{R}, \\widetilde{C}$ are functions of $M_{i,j}$ since $\\widetilde{M}_{a,b} = M_{a,b} + \\tau$. Their partial derivatives with respect to $M_{i,j}$ are:\n- $\\frac{\\partial \\widetilde{M}_{w,c}}{\\partial M_{i,j}} = \\delta_{wi}\\delta_{cj}$ (where $\\delta$ is the Kronecker delta)\n- $\\frac{\\partial \\widetilde{T}}{\\partial M_{i,j}} = \\frac{\\partial}{\\partial M_{i,j}} \\sum_{a,b} (M_{a,b}+\\tau) = 1$\n- $\\frac{\\partial \\widetilde{R}_{w}}{\\partial M_{i,j}} = \\frac{\\partial}{\\partial M_{i,j}} \\sum_{b} (M_{w,b}+\\tau) = \\delta_{wi}$\n- $\\frac{\\partial \\widetilde{C}_{c}}{\\partial M_{i,j}} = \\frac{\\partial}{\\partial M_{i,j}} \\sum_{a} (M_{a,c}+\\tau) = \\delta_{cj}$\n\nUsing the chain rule for logarithms, $\\frac{d}{dz}\\log(f(z)) = \\frac{1}{f(z)} f'(z)$, we get:\n$$\n\\frac{\\partial x_{w,c}}{\\partial M_{i,j}} = \\frac{1}{\\widetilde{M}_{w,c}}\\frac{\\partial \\widetilde{M}_{w,c}}{\\partial M_{i,j}} + \\frac{1}{\\widetilde{T}}\\frac{\\partial \\widetilde{T}}{\\partial M_{i,j}} - \\frac{1}{\\widetilde{R}_{w}}\\frac{\\partial \\widetilde{R}_{w}}{\\partial M_{i,j}} - \\frac{1}{\\widetilde{C}_{c}}\\frac{\\partial \\widetilde{C}_{c}}{\\partial M_{i,j}}\n$$\nSubstituting the derivatives:\n$$\n\\frac{\\partial x_{w,c}}{\\partial M_{i,j}} = \\frac{\\delta_{wi}\\delta_{cj}}{\\widetilde{M}_{w,c}} + \\frac{1}{\\widetilde{T}} - \\frac{\\delta_{wi}}{\\widetilde{R}_{w}} - \\frac{\\delta_{cj}}{\\widetilde{C}_{c}}\n$$\nThis general form depends on whether the indices $(i,j)$ of the perturbation match the indices $(w,c)$ of the target pair.\n\n### Step 3: Calculation of Deviation\n\nFor each test case, we compute the absolute deviation $e = |\\Delta x_{\\mathrm{exact}} - \\Delta x_{\\mathrm{lin}}|$.\n\n1.  **Calculate Baseline and Perturbed Values**:\n    For a given matrix $A$ (which can be $M$ or $M+\\Delta M$), we first compute the smoothed quantities:\n    - $\\widetilde{A} = A + \\tau$\n    - $\\widetilde{R}_w(A) = \\sum_j \\widetilde{A}_{w,j}$\n    - $\\widetilde{C}_c(A) = \\sum_i \\widetilde{A}_{i,c}$\n    - $\\widetilde{T}(A) = \\sum_{i,j} \\widetilde{A}_{i,j}$\n    Then, for the target pair $(w^{\\star}, c^{\\star})$, we compute the stationary point prediction:\n    $x_{w^{\\star},c^{\\star}}(A, k, \\tau) = \\log\\left(\\frac{\\widetilde{A}_{w^{\\star},c^{\\star}} \\cdot \\widetilde{T}(A)}{\\widetilde{R}_{w^{\\star}}(A) \\cdot \\widetilde{C}_{c^{\\star}}(A)}\\right) - \\log k$.\n    This is calculated for $A=M$ to get $x_{\\text{base}}$ and for $A=M+\\Delta M$ to get $x_{\\text{pert}}$.\n\n2.  **Calculate Exact Difference**:\n    $\\Delta x_{\\mathrm{exact}} = x_{\\text{pert}} - x_{\\text{base}}$.\n\n3.  **Calculate Linear Approximation**:\n    $\\Delta x_{\\mathrm{lin}}$ is the sum of contributions from each non-zero element of the perturbation matrix $\\Delta M$:\n    $$\n    \\Delta x_{\\mathrm{lin}} = \\sum_{(i,j) \\text{ where } (\\Delta M)_{i,j} \\ne 0} \\left( \\frac{\\delta_{w^{\\star}i}\\delta_{c^{\\star}j}}{\\widetilde{M}_{w^{\\star},c^{\\star}}} + \\frac{1}{\\widetilde{T}} - \\frac{\\delta_{w^{\\star}i}}{\\widetilde{R}_{w^{\\star}}} - \\frac{\\delta_{c^{\\star}j}}{\\widetilde{C}_{c^{\\star}}} \\right) \\cdot (\\Delta M)_{i,j}\n    $$\n    All quantities ($\\widetilde{M}_{w^{\\star},c^{\\star}}, \\widetilde{T}, \\widetilde{R}_{w^{\\star}}, \\widetilde{C}_{c^{\\star}}$) in the derivative expression are evaluated using the baseline matrix $M$.\n\n4.  **Compute Absolute Deviation**:\n    $e = |\\Delta x_{\\mathrm{exact}} - \\Delta x_{\\mathrm{lin}}|$.\nThis procedure is then implemented for each test case provided.",
            "answer": "[0.000788647008,0.010531238914,0.006900405230,0.071853874987]"
        }
    ]
}