{
    "hands_on_practices": [
        {
            "introduction": "要真正理解Word2vec，最有效的方法之一就是亲手完成一次模型的学习步骤。这个练习将引导你为Skip-gram模型手动执行单步随机梯度下降（SGD）更新。通过处理一个具体的正样本和几个负样本，你将计算梯度并更新词向量，从而直观地体验到模型如何通过“推近”相似词（正样本）和“推远”不相关词（负样本）来学习词义 。",
            "id": "3200045",
            "problem": "考虑一个基于词汇表 $\\{a,b,c\\}$ 的玩具语料库，它由两个句子组成：$a\\; b\\; a$ 和 $b\\; c\\; b$。在此语料库上训练一个使用负采样的 skip-gram Word2Vec 模型。使用以下设置。\n\n- 使用大小为 $1$ 的对称上下文窗口。关注一个单一的训练实例，该实例由第一个句子 $a\\; b\\; a$ 中位置 $2$ 的中心词 $b$ 和位置 $1$ 的上下文词 $a$ 构成。因此，正样本对是 $(b \\to a)$。\n- 使用负采样，取 $k=2$ 个负样本。负样本从词汇表的均匀分布中抽取，但在这一步中，将负样本确定性地固定为 $\\{b,c\\}$（按此顺序）。\n- 嵌入维度为 $d=2$。存在独立的输入（中心）和输出（上下文）嵌入。初始输入嵌入为\n$$\nu_a=\\begin{pmatrix}0\\\\0\\end{pmatrix},\\quad\nu_b=\\begin{pmatrix}1\\\\0\\end{pmatrix},\\quad\nu_c=\\begin{pmatrix}0\\\\1\\end{pmatrix},\n$$\n初始输出嵌入为\n$$\nv_a=\\begin{pmatrix}0\\\\1\\end{pmatrix},\\quad\nv_b=\\begin{pmatrix}1\\\\0\\end{pmatrix},\\quad\nv_c=\\begin{pmatrix}0\\\\-1\\end{pmatrix}.\n$$\n- 使用学习率 $\\alpha=0.2$ 的随机梯度下降（SGD; Stochastic Gradient Descent）。\n\n对于带有负样本 $\\{b,c\\}$ 的实例 $(b \\to a)$ 的一个训练步骤，使用单实例负采样损失\n$$\n\\ell\\!\\left(u_b, v_a, \\{v_b, v_c\\}\\right)\\;=\\;-\\ln \\sigma\\!\\left(v_a^{\\top} u_b\\right)\\;-\\;\\sum_{n\\in\\{b,c\\}} \\ln \\sigma\\!\\left(-\\,v_n^{\\top} u_b\\right),\n$$\n其中 $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$ 是 logistic sigmoid 函数。从上述初始嵌入开始，基于这个单一实例，对 $u_b$、$v_a$、$v_b$ 和 $v_c$ 执行一次 SGD 更新（所有其他参数保持不变）。从给定的损失和 $\\sigma(x)$ 的定义出发，使用第一性原理推导必要的梯度。\n\n经过这次单一的 SGD 步骤后，输入向量 $u_b$ 的第一个分量更新后的值是多少？将您的答案四舍五入到四位有效数字。\n\n此外，在您的解题过程中，请简要解释更新的符号和大小如何反映正样本对和负样本之间的推拉动态。您最终报告的答案必须只是所要求的标量值。",
            "solution": "这个练习要求我们为 Skip-gram 模型执行一个单步 SGD 更新。损失函数为：\n$$\n\\ell = -\\ln \\sigma(v_a^{\\top} u_b) - \\sum_{n\\in\\{b,c\\}} \\ln \\sigma(-v_n^{\\top} u_b)\n$$\n我们需要最小化这个损失，所以 SGD 更新规则是 $u_b^{\\text{new}} = u_b - \\alpha \\frac{\\partial \\ell}{\\partial u_b}$。\n\n首先，我们推导损失 $\\ell$ 相对于中心词嵌入 $u_b$ 的梯度。利用 $\\frac{d}{dx}\\ln\\sigma(x) = 1-\\sigma(x)$ 和 $\\frac{d}{dx}\\ln\\sigma(-x) = -\\sigma(x)$，我们得到：\n$$\n\\frac{\\partial \\ell}{\\partial u_b} = -\\frac{\\partial}{\\partial u_b}\\left( \\ln\\sigma(v_a^\\top u_b) + \\sum_{n\\in\\{b,c\\}} \\ln\\sigma(-v_n^\\top u_b) \\right)\n$$\n$$\n\\frac{\\partial \\ell}{\\partial u_b} = -\\left( (1-\\sigma(v_a^\\top u_b))v_a + \\sum_{n\\in\\{b,c\\}} (-\\sigma(v_n^\\top u_b))v_n \\right)\n$$\n$$\n\\frac{\\partial \\ell}{\\partial u_b} = (\\sigma(v_a^\\top u_b)-1)v_a + \\sum_{n\\in\\{b,c\\}} \\sigma(v_n^\\top u_b)v_n\n$$\n这个梯度直观地反映了模型的“推拉”动态：\n- 第一项 $(\\sigma(v_a^\\top u_b)-1)v_a$ 是一个“拉力”。因为 $\\sigma(\\cdot) \\in (0,1)$，所以系数是负的。更新 $- \\alpha \\frac{\\partial \\ell}{\\partial u_b}$ 会将 $u_b$ “拉向”正样本的上下文向量 $v_a$。\n- 第二项 $\\sum \\sigma(v_n^\\top u_b)v_n$ 是一个“推力”。系数是正的，更新会使 $u_b$ “推离”负样本的上下文向量 $v_n$。\n\n接下来，我们代入初始值进行计算：\n$u_b=\\begin{pmatrix}1\\\\0\\end{pmatrix}$, $v_a=\\begin{pmatrix}0\\\\1\\end{pmatrix}$, $v_b=\\begin{pmatrix}1\\\\0\\end{pmatrix}$, $v_c=\\begin{pmatrix}0\\\\-1\\end{pmatrix}$。\n点积：\n- $v_a^\\top u_b = 0 \\cdot 1 + 1 \\cdot 0 = 0$\n- $v_b^\\top u_b = 1 \\cdot 1 + 0 \\cdot 0 = 1$\n- $v_c^\\top u_b = 0 \\cdot 1 + (-1) \\cdot 0 = 0$\nSigmoid 值：\n- $\\sigma(v_a^\\top u_b) = \\sigma(0) = 0.5$\n- $\\sigma(v_b^\\top u_b) = \\sigma(1) \\approx 0.731059$\n- $\\sigma(v_c^\\top u_b) = \\sigma(0) = 0.5$\n\n计算梯度向量：\n$$\n\\frac{\\partial \\ell}{\\partial u_b} = (\\sigma(0)-1)v_a + \\sigma(1)v_b + \\sigma(0)v_c\n$$\n$$\n\\frac{\\partial \\ell}{\\partial u_b} = (0.5-1)\\begin{pmatrix}0\\\\1\\end{pmatrix} + 0.731059\\begin{pmatrix}1\\\\0\\end{pmatrix} + 0.5\\begin{pmatrix}0\\\\-1\\end{pmatrix}\n$$\n$$\n\\frac{\\partial \\ell}{\\partial u_b} = -0.5\\begin{pmatrix}0\\\\1\\end{pmatrix} + 0.731059\\begin{pmatrix}1\\\\0\\end{pmatrix} + 0.5\\begin{pmatrix}0\\\\-1\\end{pmatrix}\n$$\n$$\n\\frac{\\partial \\ell}{\\partial u_b} = \\begin{pmatrix}0\\\\-0.5\\end{pmatrix} + \\begin{pmatrix}0.731059\\\\0\\end{pmatrix} + \\begin{pmatrix}0\\\\-0.5\\end{pmatrix} = \\begin{pmatrix}0.731059\\\\-1.0\\end{pmatrix}\n$$\n\n最后，执行 SGD 更新，学习率 $\\alpha = 0.2$：\n$$\nu_b^{\\text{new}} = u_b - \\alpha \\frac{\\partial \\ell}{\\partial u_b} = \\begin{pmatrix}1\\\\0\\end{pmatrix} - 0.2 \\begin{pmatrix}0.731059\\\\-1.0\\end{pmatrix}\n$$\n$$\nu_b^{\\text{new}} = \\begin{pmatrix}1\\\\0\\end{pmatrix} - \\begin{pmatrix}0.1462118\\\\-0.2\\end{pmatrix} = \\begin{pmatrix}1 - 0.1462118 \\\\ 0 - (-0.2)\\end{pmatrix} = \\begin{pmatrix}0.8537882 \\\\ 0.2\\end{pmatrix}\n$$\n更新后的 $u_b$ 的第一个分量是 $0.8537882$。四舍五入到四位有效数字，结果是 $0.8538$。",
            "answer": "$$\\boxed{0.8538}$$"
        },
        {
            "introduction": "在掌握了Skip-gram模型之后，我们转向Word2vec的另一个核心架构：连续词袋模型（CBOW）。与Skip-gram预测上下文不同，CBOW通过上下文来预测中心词。这个练习的核心在于推导和计算CBOW模型中上下文词向量的梯度，这会让你深刻理解梯度如何通过平均化的隐层反向传播，并均匀地影响每一个上下文词的向量表示 。",
            "id": "3200079",
            "problem": "考虑 Word2Vec 中的连续词袋模型 (CBOW)。设上下文集合为 $C=\\{c_1,\\dots,c_{|C|}\\}$，其中每个上下文词 $c_i$ 都有一个输入嵌入向量 $v_{c_i}\\in\\mathbb{R}^d$。CBOW 的隐藏表示是其平均值\n$$\nh=\\frac{1}{|C|}\\sum_{i=1}^{|C|}v_{c_i}.\n$$\n对于一个具有输出嵌入向量 $u_w\\in\\mathbb{R}^d$ 的目标词 $w$，定义分数 $s=u_w^{\\top}h$ 和预测概率 $\\hat{y}=\\sigma(s)$，其中 $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$ 是 logistic sigmoid 函数。对于一个标签对 $(C,w,y)$（其中 $y\\in\\{0,1\\}$），需要最小化的目标是二元交叉熵\n$$\nJ(C,w,y)=-\\Big(y\\,\\ln(\\sigma(s))+(1-y)\\,\\ln\\big(1-\\sigma(s)\\big)\\Big).\n$$\n对于一个对数据对 $\\{(C^{(b)},w^{(b)},y^{(b)})\\}_{b=1}^B$ 的损失 $J$ 进行求和的小批量 (mini-batch)，关于每个 $v_{c_i}$ 的梯度是每个数据对梯度的总和。\n\n任务：\n1. 从以上定义和基本微积分出发，推导单个数据对 $(C,w,y)$ 的梯度 $\\frac{\\partial J}{\\partial v_{c_i}}$，并用 $u_w$、$h$、$\\sigma(u_w^{\\top}h)$、$y$ 和 $|C|$ 表示。\n2. 然后，考虑以下具有共享上下文的特定小批量（$d=3$ 且 $|C|=2$）：\n   - 上下文嵌入：$v_{c_1}=\\begin{pmatrix}1\\\\0\\\\2\\end{pmatrix}$ 和 $v_{c_2}=\\begin{pmatrix}0\\\\-1\\\\1\\end{pmatrix}$，因此该小批量中的所有数据对都使用相同的 $h$。\n   - 一个正例对 $(y=1)$，其目标输出嵌入为 $u_{w^{(+)}}=\\begin{pmatrix}1\\\\2\\\\-1\\end{pmatrix}$。\n   - 两个负例对 $(y=0)$，其目标输出嵌入分别为 $u_{w^{(-1)}}=\\begin{pmatrix}-1\\\\0\\\\1\\end{pmatrix}$ 和 $u_{w^{(-2)}}=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}$。\n   关于 $v_{c_1}$ 的小批量梯度是在第 1 部分中得到的三个数据对梯度的总和，每个梯度都在适当的 $y$ 和 $u_w$ 下计算。\n3. 设 $d=\\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix}$。计算此小批量的标量内积 $d^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right)$。将最终数值答案四舍五入到四位有效数字。\n\n你的最终答案必须是一个实数。",
            "solution": "该问题要求进行与连续词袋模型 (CBOW) 中的梯度相关的三部分计算。首先，我们必须推导损失函数相对于上下文词输入嵌入的梯度的通用形式。其次，我们将此应用于一个特定的小批量。第三，我们计算所得小批量梯度的标量投影。\n\n**第1部分：梯度 $\\frac{\\partial J}{\\partial v_{c_i}}$ 的推导**\n\n单个训练数据对 $(C,w,y)$ 的目标函数是二元交叉熵损失：\n$$\nJ(C,w,y) = -\\Big(y \\ln(\\sigma(s)) + (1-y) \\ln(1-\\sigma(s))\\Big)\n$$\n其中分数 $s$ 定义为 $s = u_w^{\\top} h$，隐藏表示 $h$ 是上下文词向量的平均值：\n$$\nh = \\frac{1}{|C|} \\sum_{j=1}^{|C|} v_{c_j}\n$$\n我们需要求解 $J$ 相对于其中一个上下文词向量 $v_{c_i} \\in \\mathbb{R}^d$ 的梯度。我们使用微积分的链式法则。梯度 $\\frac{\\partial J}{\\partial v_{c_i}}$ 是一个向量，其第 $k$ 个分量是 $\\frac{\\partial J}{\\partial v_{c_{i,k}}}$，其中 $v_{c_{i,k}}$ 是向量 $v_{c_i}$ 的第 $k$ 个分量。\n\n让我们应用链式法则：\n$$\n\\frac{\\partial J}{\\partial v_{c_{i,k}}} = \\frac{\\partial J}{\\partial s} \\frac{\\partial s}{\\partial v_{c_{i,k}}}\n$$\n首先，我们计算损失 $J$ 相对于分数 $s$ 的导数。我们利用 sigmoid 函数的导数是 $\\frac{d\\sigma(x)}{dx} = \\sigma(x)(1-\\sigma(x))$ 这一事实。\n\\begin{align*}\n\\frac{\\partial J}{\\partial s} = -\\frac{\\partial}{\\partial s} \\Big(y \\ln(\\sigma(s)) + (1-y) \\ln(1-\\sigma(s))\\Big) \\\\\n= -\\left( y \\frac{1}{\\sigma(s)} \\frac{d\\sigma(s)}{ds} + (1-y) \\frac{1}{1-\\sigma(s)} \\left(-\\frac{d\\sigma(s)}{ds}\\right) \\right) \\\\\n= -\\left( y \\frac{1}{\\sigma(s)} \\sigma(s)(1-\\sigma(s)) - (1-y) \\frac{1}{1-\\sigma(s)} \\sigma(s)(1-\\sigma(s)) \\right) \\\\\n= -\\big( y(1-\\sigma(s)) - (1-y)\\sigma(s) \\big) \\\\\n= -(y - y\\sigma(s) - \\sigma(s) + y\\sigma(s)) \\\\\n= -(y - \\sigma(s)) \\\\\n= \\sigma(s) - y\n\\end{align*}\n对于带有最终 sigmoid 激活的二元交叉熵损失，这是一个标准结果。\n\n接下来，我们计算分数 $s$ 相对于向量 $v_{c_i}$ 的第 $k$ 个分量的导数。分数 $s$ 可以用分量表示为：\n$$\ns = u_w^{\\top} h = \\sum_{j=1}^d u_{w,j} h_j = \\sum_{j=1}^d u_{w,j} \\left( \\frac{1}{|C|} \\sum_{l=1}^{|C|} v_{c_{l,j}} \\right)\n$$\n现在我们对 $v_{c_{i,k}}$ 求导：\n$$\n\\frac{\\partial s}{\\partial v_{c_{i,k}}} = \\frac{\\partial}{\\partial v_{c_{i,k}}} \\left( \\sum_{j=1}^d u_{w,j} \\frac{1}{|C|} \\sum_{l=1}^{|C|} v_{c_{l,j}} \\right)\n$$\n导数 $\\frac{\\partial v_{c_{l,j}}}{\\partial v_{c_{i,k}}}$ 在 $l=i$ 且 $j=k$ 时为 $1$，否则为 $0$。因此，对于这个微分，求和中只有一个项不为零。\n$$\n\\frac{\\partial s}{\\partial v_{c_{i,k}}} = u_{w,k} \\frac{1}{|C|} (1) = \\frac{1}{|C|} u_{w,k}\n$$\n结合这两个部分：\n$$\n\\frac{\\partial J}{\\partial v_{c_{i,k}}} = \\frac{\\partial J}{\\partial s} \\frac{\\partial s}{\\partial v_{c_{i,k}}} = (\\sigma(s)-y) \\frac{1}{|C|} u_{w,k}\n$$\n为了构成梯度向量 $\\frac{\\partial J}{\\partial v_{c_i}}$，我们将这些分量组合起来（对于 $k=1, \\dots, d$）：\n$$\n\\frac{\\partial J}{\\partial v_{c_i}} = \\begin{pmatrix} \\frac{\\partial J}{\\partial v_{c_{i,1}}} \\\\ \\vdots \\\\ \\frac{\\partial J}{\\partial v_{c_{i,d}}} \\end{pmatrix} = \\frac{\\sigma(s)-y}{|C|} \\begin{pmatrix} u_{w,1} \\\\ \\vdots \\\\ u_{w,d} \\end{pmatrix} = \\frac{\\sigma(u_w^{\\top}h)-y}{|C|} u_w\n$$\n这就完成了任务的第一部分。\n\n**第2部分和第3部分：小批量梯度计算与投影**\n\n问题要求计算标量值 $d^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right)$。小批量梯度是批次中三个数据对的梯度之和。设数据对由 $b \\in \\{ (+), (-1), (-2) \\}$ 索引。\n$$\n\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}} = \\sum_{b} \\frac{\\partial J^{(b)}}{\\partial v_{c_1}}\n$$\n使用第1部分的结果，我们有：\n$$\n\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}} = \\sum_{b} \\frac{\\sigma({u_w^{(b)}}^{\\top}h)-y^{(b)}}{|C|} u_w^{(b)}\n$$\n我们想要计算这个梯度与向量 $d$ 的内积：\n$$\nd^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right) = d^{\\top} \\sum_{b} \\frac{\\sigma({u_w^{(b)}}^{\\top}h)-y^{(b)}}{|C|} u_w^{(b)}\n$$\n根据内积的线性性质，我们可以写成：\n$$\nd^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right) = \\frac{1}{|C|} \\sum_{b} \\left(\\sigma({u_w^{(b)}}^{\\top}h)-y^{(b)}\\right) (d^{\\top}u_w^{(b)})\n$$\n首先，我们计算共享的隐藏表示 $h$。\n给定 $|C|=2$，$v_{c_1}=\\begin{pmatrix}1\\\\0\\\\2\\end{pmatrix}$ 和 $v_{c_2}=\\begin{pmatrix}0\\\\-1\\\\1\\end{pmatrix}$：\n$$\nh = \\frac{1}{2}(v_{c_1} + v_{c_2}) = \\frac{1}{2}\\left(\\begin{pmatrix}1\\\\0\\\\2\\end{pmatrix} + \\begin{pmatrix}0\\\\-1\\\\1\\end{pmatrix}\\right) = \\frac{1}{2}\\begin{pmatrix}1\\\\-1\\\\3\\end{pmatrix} = \\begin{pmatrix}0.5\\\\-0.5\\\\1.5\\end{pmatrix}\n$$\n接下来，我们计算小批量中每个数据对的项。\n\n1.  **正例对 (+):** $y^{(+)}=1$, $u_{w^{(+)}}=\\begin{pmatrix}1\\\\2\\\\-1\\end{pmatrix}$\n    - 分数 $s^{(+)} = {u_{w^{(+)}}}^{\\top}h = \\begin{pmatrix}1  2  -1\\end{pmatrix}\\begin{pmatrix}0.5\\\\-0.5\\\\1.5\\end{pmatrix} = 0.5 - 1.0 - 1.5 = -2$。\n    - 与 $d$ 的内积：$d^{\\top}u_{w^{(+)}} = \\begin{pmatrix}1  0  -1\\end{pmatrix}\\begin{pmatrix}1\\\\2\\\\-1\\end{pmatrix} = 1+0+1 = 2$。\n    - 对总和的贡献：$(\\sigma(-2)-1)(2)$。\n\n2.  **第一个负例对 (-1):** $y^{(-1)}=0$, $u_{w^{(-1)}}=\\begin{pmatrix}-1\\\\0\\\\1\\end{pmatrix}$\n    - 分数 $s^{(-1)} = {u_{w^{(-1)}}}^{\\top}h = \\begin{pmatrix}-1  0  1\\end{pmatrix}\\begin{pmatrix}0.5\\\\-0.5\\\\1.5\\end{pmatrix} = -0.5+0+1.5 = 1$。\n    - 与 $d$ 的内积：$d^{\\top}u_{w^{(-1)}} = \\begin{pmatrix}1  0  -1\\end{pmatrix}\\begin{pmatrix}-1\\\\0\\\\1\\end{pmatrix} = -1+0-1 = -2$。\n    - 对总和的贡献：$(\\sigma(1)-0)(-2) = -2\\sigma(1)$。\n\n3.  **第二个负例对 (-2):** $y^{(-2)}=0$, $u_{w^{(-2)}}=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}$\n    - 分数 $s^{(-2)} = {u_{w^{(-2)}}}^{\\top}h = \\begin{pmatrix}0  1  0\\end{pmatrix}\\begin{pmatrix}0.5\\\\-0.5\\\\1.5\\end{pmatrix} = 0-0.5+0 = -0.5$。\n    - 与 $d$ 的内积：$d^{\\top}u_{w^{(-2)}} = \\begin{pmatrix}1  0  -1\\end{pmatrix}\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix} = 0+0+0 = 0$。\n    - 对总和的贡献：$(\\sigma(-0.5)-0)(0) = 0$。\n\n现在，我们将这些贡献相加，并乘以 $\\frac{1}{|C|} = \\frac{1}{2}$：\n$$\nd^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right) = \\frac{1}{2} \\Big[ (\\sigma(-2)-1)(2) - 2\\sigma(1) + 0 \\Big] = \\sigma(-2) - 1 - \\sigma(1)\n$$\n使用性质 $\\sigma(-x) = 1 - \\sigma(x)$，我们有 $\\sigma(-2) = 1-\\sigma(2)$。代入可得：\n$$\nd^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right) = (1-\\sigma(2)) - 1 - \\sigma(1) = -\\sigma(2) - \\sigma(1)\n$$\n现在我们计算数值：\n$$\n-\\sigma(2) - \\sigma(1) = -\\left( \\frac{1}{1+\\exp(-2)} + \\frac{1}{1+\\exp(-1)} \\right)\n$$\n我们使用数值 $e \\approx 2.71828$，$e^2 \\approx 7.38906$：\n- $\\exp(-2) \\approx 0.135335$\n- $\\exp(-1) \\approx 0.367879$\n$$\n\\sigma(2) = \\frac{1}{1+0.135335} \\approx \\frac{1}{1.135335} \\approx 0.880797\n$$\n$$\n\\sigma(1) = \\frac{1}{1+0.367879} \\approx \\frac{1}{1.367879} \\approx 0.731059\n$$\n最终值为：\n$$\n-0.880797 - 0.731059 = -1.611856\n$$\n四舍五入到四位有效数字得到 $-1.612$。",
            "answer": "$$\\boxed{-1.612}$$"
        },
        {
            "introduction": "掌握了CBOW的基本更新机制后，我们可以进一步探究其设计中一个更微妙的方面：上下文聚合方式所带来的影响。标准的CBOW模型平等地对待所有上下文词，但这可能会导致高频词在聚合表示中占据过大的权重。这个练习旨在通过分析和量化这种频率偏差，并推导一种修正方法，来培养你对模型内在偏见的批判性思维，并理解如何通过理论分析来改进模型设计 。",
            "id": "3200013",
            "problem": "考虑 Word2Vec 的连续词袋模型 (CBOW) 公式，其中目标词的得分由内积 $v_{w}^{\\top} h$ 给出，$v_{w} \\in \\mathbb{R}^{d}$ 是目标词的嵌入，而 $h \\in \\mathbb{R}^{d}$ 是聚合的上下文表示。设可能的上下文类型集合为 $C = \\{c_{1}, c_{2}, \\dots, c_{|C|}\\}$，其嵌入为 $v_{c_{i}} \\in \\mathbb{R}^{d}$，并令 $p(c_{i})$ 表示每种上下文类型的经验频率。假设从语料库中随机抽取的上下文标记的类型为 $c_{i}$ 的概率为 $p(c_{i})$，并且窗口内的上下文根据 $p$ 独立同分布。\n\n定义两种聚合方式：\n1. 按上下文类型均等平均：$h_{\\mathrm{equal}} = \\frac{1}{|C|} \\sum_{i=1}^{|C|} v_{c_{i}}$。\n2. 按语料库分布的频率加权聚合：$h_{\\mathrm{freq}} = \\sum_{i=1}^{|C|} p(c_{i}) \\, v_{c_{i}}$。\n\na) 仅使用内积的线性性和期望，推导得分对频繁上下文的偏差的通用符号表达式，\n$$\\Delta_{\\mathrm{bias}}(w) = v_{w}^{\\top} h_{\\mathrm{freq}} - v_{w}^{\\top} h_{\\mathrm{equal}}.$$\n\nb) 考虑一个具体实例，其中 $|C| = 4$，$d = 2$，目标向量 $v_{w} = \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix}$，上下文嵌入为\n$$v_{c_{1}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad v_{c_{2}} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}, \\quad v_{c_{3}} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}, \\quad v_{c_{4}} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix},$$\n经验频率为\n$$p(c_{1}) = 0.4, \\quad p(c_{2}) = 0.3, \\quad p(c_{3}) = 0.2, \\quad p(c_{4}) = 0.1.$$\n计算 $\\Delta_{\\mathrm{bias}}(w)$ 的数值。\n\nc) 你决定通过一个关于其经验频率的函数 $\\alpha(c)$ 来对上下文进行重加权，使得在语料库分布下的期望上下文表示与按类型均等平均的表示相匹配，即，\n$$\\sum_{i=1}^{|C|} p(c_{i}) \\, \\alpha(c_{i}) \\, v_{c_{i}} \\;=\\; \\frac{1}{|C|} \\sum_{i=1}^{|C|} v_{c_{i}}.$$\n求解重加权函数 $\\alpha(c)$，用 $p(c)$ 和 $|C|$ 表示，以保证该等式对任意 $\\{v_{c_{i}}\\}$ 成立。\n\nd) 使用你推导出的重加权函数 $\\alpha(c)$，定义 $h_{\\mathrm{corr}} = \\sum_{i=1}^{|C|} p(c_{i}) \\, \\alpha(c_{i}) \\, v_{c_{i}}$ 并计算校正后的得分差异\n$$\\Delta_{\\mathrm{corr}}(w) = v_{w}^{\\top} h_{\\mathrm{corr}} - v_{w}^{\\top} h_{\\mathrm{equal}}$$\n对于 b) 部分中的数值实例。将最终答案表示为一个单行矩阵，包含 b) 部分和 d) 部分的两个量，使用精确形式（不进行四舍五入）。",
            "solution": "**a) 偏差表达式的推导**\n\n得分偏差定义为使用频率加权上下文表示 $h_{\\mathrm{freq}}$ 计算的得分与使用均等平均上下文表示 $h_{\\mathrm{equal}}$ 计算的得分之间的差异。该偏差的表达式如下：\n$$\n\\Delta_{\\mathrm{bias}}(w) = v_{w}^{\\top} h_{\\mathrm{freq}} - v_{w}^{\\top} h_{\\mathrm{equal}}\n$$\n其中 $v_{w}$ 是目标词的嵌入。\n\n根据内积的线性，我们可以合并这两项：\n$$\n\\Delta_{\\mathrm{bias}}(w) = v_{w}^{\\top} (h_{\\mathrm{freq}} - h_{\\mathrm{equal}})\n$$\n现在，我们代入 $h_{\\mathrm{freq}}$ 和 $h_{\\mathrm{equal}}$ 的定义：\n$$\nh_{\\mathrm{freq}} = \\sum_{i=1}^{|C|} p(c_{i}) \\, v_{c_{i}}\n$$\n$$\nh_{\\mathrm{equal}} = \\frac{1}{|C|} \\sum_{i=1}^{|C|} v_{c_{i}} = \\sum_{i=1}^{|C|} \\frac{1}{|C|} v_{c_{i}}\n$$\n这两种上下文表示之间的差异是：\n$$\nh_{\\mathrm{freq}} - h_{\\mathrm{equal}} = \\sum_{i=1}^{|C|} p(c_{i}) \\, v_{c_{i}} - \\sum_{i=1}^{|C|} \\frac{1}{|C|} v_{c_{i}}\n$$\n合并求和项，我们得到：\n$$\nh_{\\mathrm{freq}} - h_{\\mathrm{equal}} = \\sum_{i=1}^{|C|} \\left( p(c_{i}) - \\frac{1}{|C|} \\right) v_{c_{i}}\n$$\n将此代回 $\\Delta_{\\mathrm{bias}}(w)$ 的表达式中：\n$$\n\\Delta_{\\mathrm{bias}}(w) = v_{w}^{\\top} \\left( \\sum_{i=1}^{|C|} \\left( p(c_{i}) - \\frac{1}{|C|} \\right) v_{c_{i}} \\right)\n$$\n再次利用内积的线性，我们可以将 $v_{w}^{\\top}$ 分配到求和中：\n$$\n\\Delta_{\\mathrm{bias}}(w) = \\sum_{i=1}^{|C|} \\left( p(c_{i}) - \\frac{1}{|C|} \\right) (v_{w}^{\\top} v_{c_{i}})\n$$\n这是偏差的通用符号表达式。它突显了偏差是目标词嵌入与每个上下文词嵌入之间点积（相似度）的加权和。每种上下文类型 $c_i$ 的权重是其经验频率 $p(c_i)$ 与均匀频率 $1/|C|$ 之间的偏差。\n\n**b) 偏差的数值计算**\n\n给定以下数值：\n$|C| = 4$, $d = 2$。\n目标向量：$v_{w} = \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix}$。\n上下文嵌入：\n$v_{c_{1}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$v_{c_{2}} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$，$v_{c_{3}} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$，$v_{c_{4}} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$。\n频率：\n$p(c_{1}) = 0.4$，$p(c_{2}) = 0.3$，$p(c_{3}) = 0.2$，$p(c_{4}) = 0.1$。\n\n首先，我们计算两种聚合的上下文表示 $h_{\\mathrm{freq}}$ 和 $h_{\\mathrm{equal}}$。\n$$\nh_{\\mathrm{freq}} = \\sum_{i=1}^{4} p(c_{i}) v_{c_{i}} = 0.4 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + 0.3 \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} + 0.2 \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} + 0.1 \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\n$$\n$$\nh_{\\mathrm{freq}} = \\begin{pmatrix} 0.4 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0.6 \\end{pmatrix} + \\begin{pmatrix} -0.2 \\\\ 0.2 \\end{pmatrix} + \\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix} = \\begin{pmatrix} 0.4 + 0 - 0.2 + 0.2 \\\\ 0 + 0.6 + 0.2 - 0.1 \\end{pmatrix} = \\begin{pmatrix} 0.4 \\\\ 0.7 \\end{pmatrix}\n$$\n$$\nh_{\\mathrm{equal}} = \\frac{1}{4} \\sum_{i=1}^{4} v_{c_{i}} = \\frac{1}{4} \\left( \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\right)\n$$\n$$\nh_{\\mathrm{equal}} = \\frac{1}{4} \\begin{pmatrix} 1 + 0 - 1 + 2 \\\\ 0 + 2 + 1 - 1 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix}\n$$\n现在我们计算得分及其差异：\n$$\nv_{w}^{\\top} h_{\\mathrm{freq}} = \\begin{pmatrix} 3  -2 \\end{pmatrix} \\begin{pmatrix} 0.4 \\\\ 0.7 \\end{pmatrix} = (3)(0.4) + (-2)(0.7) = 1.2 - 1.4 = -0.2\n$$\n$$\nv_{w}^{\\top} h_{\\mathrm{equal}} = \\begin{pmatrix} 3  -2 \\end{pmatrix} \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix} = (3)(0.5) + (-2)(0.5) = 1.5 - 1.0 = 0.5\n$$\n$$\n\\Delta_{\\mathrm{bias}}(w) = v_{w}^{\\top} h_{\\mathrm{freq}} - v_{w}^{\\top} h_{\\mathrm{equal}} = -0.2 - 0.5 = -0.7\n$$\n\n**c) 重加权函数 $\\alpha(c)$ 的推导**\n\n我们的任务是找到一个重加权函数 $\\alpha(c)$，使得对于任意上下文嵌入集合 $\\{v_{c_{i}}\\}$，以下等式成立：\n$$\n\\sum_{i=1}^{|C|} p(c_{i}) \\, \\alpha(c_{i}) \\, v_{c_{i}} \\;=\\; \\frac{1}{|C|} \\sum_{i=1}^{|C|} v_{c_{i}}\n$$\n我们可以重写右侧，以匹配左侧的求和结构：\n$$\n\\sum_{i=1}^{|C|} p(c_{i}) \\, \\alpha(c_{i}) \\, v_{c_{i}} \\;=\\; \\sum_{i=1}^{|C|} \\frac{1}{|C|} v_{c_{i}}\n$$\n整理各项，我们得到：\n$$\n\\sum_{i=1}^{|C|} p(c_{i}) \\, \\alpha(c_{i}) \\, v_{c_{i}} - \\sum_{i=1}^{|C|} \\frac{1}{|C|} v_{c_{i}} \\;=\\; \\vec{0}\n$$\n$$\n\\sum_{i=1}^{|C|} \\left( p(c_{i}) \\, \\alpha(c_{i}) - \\frac{1}{|C|} \\right) v_{c_{i}} \\;=\\; \\vec{0}\n$$\n关键约束是该方程必须对**任意**向量 $\\{v_{c_{i}}\\}$ 成立。如果我们考虑向量 $\\{v_{c_{i}}\\}_{i=1}^{|C|}$ 线性无关的情况（例如，如果 $|C| \\le d$ 且我们选择它们为正交向量），这些向量的线性组合只有在所有标量系数都为零时才能等于零向量。这个原理可以推广：为了使等式对任意选择的向量普遍成立，每个系数都必须为零。\n因此，对于每个 $i \\in \\{1, 2, \\dots, |C|\\}$，必须有：\n$$\np(c_{i}) \\, \\alpha(c_{i}) - \\frac{1}{|C|} = 0\n$$\n假设我们集合 $C$ 中的任何上下文类型 $c_i$ 出现的概率不为零，即 $p(c_i) > 0$，我们可以解出 $\\alpha(c_i)$：\n$$\np(c_{i}) \\, \\alpha(c_{i}) = \\frac{1}{|C|}\n$$\n$$\n\\alpha(c_{i}) = \\frac{1}{|C| \\, p(c_{i})}\n$$\n这给出了重加权函数 $\\alpha(c)$，用上下文的经验频率 $p(c)$ 和上下文类型的总数 $|C|$ 表示。\n\n**d) 校正后得分差异的计算**\n\n校正后的上下文表示定义为：\n$$\nh_{\\mathrm{corr}} = \\sum_{i=1}^{|C|} p(c_{i}) \\, \\alpha(c_{i}) \\, v_{c_{i}}\n$$\n根据 c) 部分的结果，我们知道重加权因子 $\\alpha(c_i)$ 的构造使得 $p(c_{i}) \\, \\alpha(c_{i}) = \\frac{1}{|C|}$。将此代入 $h_{\\mathrm{corr}}$ 的定义中：\n$$\nh_{\\mathrm{corr}} = \\sum_{i=1}^{|C|} \\left( \\frac{1}{|C|} \\right) v_{c_{i}}\n$$\n这个表达式正是均等平均上下文表示的定义：\n$$\nh_{\\mathrm{corr}} = \\frac{1}{|C|} \\sum_{i=1}^{|C|} v_{c_{i}} = h_{\\mathrm{equal}}\n$$\n问题要求计算校正后的得分差异 $\\Delta_{\\mathrm{corr}}(w)$：\n$$\n\\Delta_{\\mathrm{corr}}(w) = v_{w}^{\\top} h_{\\mathrm{corr}} - v_{w}^{\\top} h_{\\mathrm{equal}}\n$$\n因为我们已经证明 $h_{\\mathrm{corr}} = h_{\\mathrm{equal}}$，所以可以立即得出：\n$$\n\\Delta_{\\mathrm{corr}}(w) = v_{w}^{\\top} h_{\\mathrm{equal}} - v_{w}^{\\top} h_{\\mathrm{equal}} = 0\n$$\n这个结果是解析性的，不依赖于 b) 部分提供的具体数值。该重加权方案是专门为消除偏差而设计的，因此根据构造，校正后的差异为零。\n\n需要报告的两个量是 b) 部分的结果 $\\Delta_{\\mathrm{bias}}(w) = -0.7$ 和 d) 部分的结果 $\\Delta_{\\mathrm{corr}}(w) = 0$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-0.7 & 0\n\\end{pmatrix}\n}\n$$"
        }
    ]
}