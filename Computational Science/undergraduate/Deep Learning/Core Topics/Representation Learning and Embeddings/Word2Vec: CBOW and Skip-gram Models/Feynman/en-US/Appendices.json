{
    "hands_on_practices": [
        {
            "introduction": "This exercise drills down into the core learning mechanism of the Skip-gram model. By manually performing a single Stochastic Gradient Descent (SGD) update, you will see exactly how the model adjusts its word vectors. This practice provides a tangible understanding of the \"push-pull\" dynamic, where the embedding for a center word is moved closer to its true context word and pushed away from negative samples, which is the foundational principle behind Word2vec's ability to learn meaning from text. ",
            "id": "3200045",
            "problem": "Consider a toy corpus over the vocabulary $\\{a,b,c\\}$ consisting of two sentences: $a\\; b\\; a$ and $b\\; c\\; b$. Train a skip-gram Word2Vec model with negative sampling on this corpus. Use the following setup.\n\n- Use a symmetric context window of size $1$. Focus on a single training instance formed by taking the center word $b$ at position $2$ of the first sentence $a\\; b\\; a$ and the context word $a$ at position $1$. Thus, the positive pair is $(b \\to a)$.\n- Use negative sampling with $k=2$ negatives, drawn from the uniform distribution over the vocabulary, but for this step, treat the negatives as fixed deterministically to be $\\{b,c\\}$ in that order.\n- The embedding dimension is $d=2$. There are separate input (center) and output (context) embeddings. The initial input embeddings are\n$$\nu_a=\\begin{pmatrix}0\\\\0\\end{pmatrix},\\quad\nu_b=\\begin{pmatrix}1\\\\0\\end{pmatrix},\\quad\nu_c=\\begin{pmatrix}0\\\\1\\end{pmatrix},\n$$\nand the initial output embeddings are\n$$\nv_a=\\begin{pmatrix}0\\\\1\\end{pmatrix},\\quad\nv_b=\\begin{pmatrix}1\\\\0\\end{pmatrix},\\quad\nv_c=\\begin{pmatrix}0\\\\-1\\end{pmatrix}.\n$$\n- Use Stochastic Gradient Descent (SGD) with learning rate $\\alpha=0.2$.\n\nFor one training step on the instance $(b \\to a)$ with negative samples $\\{b,c\\}$, use the single-instance negative sampling loss\n$$\n\\ell\\!\\left(u_b, v_a, \\{v_b, v_c\\}\\right)\\;=\\;-\\ln \\sigma\\!\\left(v_a^{\\top} u_b\\right)\\;-\\;\\sum_{n\\in\\{b,c\\}} \\ln \\sigma\\!\\left(-\\,v_n^{\\top} u_b\\right),\n$$\nwhere $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$ is the logistic sigmoid. Starting from the above initial embeddings, perform one SGD update on $u_b$, $v_a$, $v_b$, and $v_c$ based on this single instance (all other parameters remain unchanged). Derive the necessary gradients from the stated loss and the definition of $\\sigma(x)$ using first principles.\n\nWhat is the updated value of the first component of the input vector $u_b$ after this single SGD step? Round your answer to four significant figures.\n\nAdditionally, in your working, briefly explain how the signs and magnitudes of the updates reflect the push-pull dynamics between the positive pair and the negative samples. Your final reported answer must be only the requested scalar.",
            "solution": "The problem asks us to perform a single Stochastic Gradient Descent (SGD) update step for a Skip-gram model with negative sampling and find the resulting value of the first component of the center word's input embedding, $u_b$.\n\nFirst, we establish the gradient of the loss function $\\ell$ with respect to the center word's input embedding $u_b$. The loss for a positive pair $(w, c)$ and a set of negative samples $N$ is:\n$$ \\ell = -\\ln \\sigma(v_c^{\\top} u_w) - \\sum_{n \\in N} \\ln \\sigma(-v_n^{\\top} u_w) $$\nUsing the property $\\frac{d}{dx} \\ln \\sigma(x) = 1-\\sigma(x)$, we find the gradient with respect to $u_w$:\n\\begin{align*} \\nabla_{u_w} \\ell &= - (1-\\sigma(v_c^{\\top} u_w))v_c - \\sum_{n \\in N} (1-\\sigma(-v_n^{\\top} u_w))(-v_n) \\\\ &= (\\sigma(v_c^{\\top} u_w)-1)v_c + \\sum_{n \\in N} \\sigma(v_n^{\\top} u_w)v_n \\end{align*}\nIn our specific case, the center word is $b$, the positive context word is $a$, and the negative samples are $\\{b, c\\}$. So we need to calculate $\\nabla_{u_b} \\ell$:\n$$ \\nabla_{u_b} \\ell = (\\sigma(v_a^{\\top} u_b)-1)v_a + \\sigma(v_b^{\\top} u_b)v_b + \\sigma(v_c^{\\top} u_b)v_c $$\nNext, we calculate the dot products using the initial embedding values:\n- Positive pair: $v_a^{\\top} u_b = \\begin{pmatrix}0 & 1\\end{pmatrix} \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} = 0$.\n- Negative sample 1: $v_b^{\\top} u_b = \\begin{pmatrix}1 & 0\\end{pmatrix} \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} = 1$.\n- Negative sample 2: $v_c^{\\top} u_b = \\begin{pmatrix}0 & -1\\end{pmatrix} \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} = 0$.\n\nNow, we evaluate the sigmoid function for these values:\n- $\\sigma(0) = \\frac{1}{1+e^0} = 0.5$.\n- $\\sigma(1) = \\frac{1}{1+e^{-1}} \\approx 0.731058$.\n\nSubstitute these into the gradient expression:\n\\begin{align*} \\nabla_{u_b} \\ell &= (\\sigma(0)-1)v_a + \\sigma(1)v_b + \\sigma(0)v_c \\\\ &= (0.5 - 1)v_a + 0.731058\\,v_b + 0.5\\,v_c \\\\ &= -0.5 \\begin{pmatrix}0\\\\1\\end{pmatrix} + 0.731058 \\begin{pmatrix}1\\\\0\\end{pmatrix} + 0.5 \\begin{pmatrix}0\\\\-1\\end{pmatrix} \\\\ &= \\begin{pmatrix}0 \\\\ -0.5\\end{pmatrix} + \\begin{pmatrix}0.731058 \\\\ 0\\end{pmatrix} + \\begin{pmatrix}0 \\\\ -0.5\\end{pmatrix} \\\\ &= \\begin{pmatrix}0.731058 \\\\ -1.0\\end{pmatrix} \\end{align*}\nThe SGD update rule is $u_{b, \\text{new}} = u_{b, \\text{old}} - \\alpha \\nabla_{u_b} \\ell$, with learning rate $\\alpha=0.2$:\n$$ u_{b, \\text{new}} = \\begin{pmatrix}1\\\\0\\end{pmatrix} - 0.2 \\begin{pmatrix}0.731058 \\\\ -1.0\\end{pmatrix} = \\begin{pmatrix}1 - 0.1462116 \\\\ 0 - (-0.2)\\end{pmatrix} = \\begin{pmatrix}0.8537884 \\\\ 0.2\\end{pmatrix} $$\nThe push-pull dynamic is visible in the SGD update term, $-\\alpha \\nabla_{u_b} \\ell$. This update vector is added to $u_b$.\n- For the positive pair $(b, a)$, the contribution to the update is $-\\alpha(\\sigma(v_a^{\\top}u_b)-1)v_a = -0.2(0.5-1)v_a = +0.1v_a$. This is a positive multiple of $v_a$, so it **pulls** $u_b$ closer to the positive context vector $v_a$.\n- For the negative samples, the contributions are $-\\alpha\\sigma(v_b^{\\top}u_b)v_b = -0.2(0.731)v_b = -0.1462v_b$ and $-\\alpha\\sigma(v_c^{\\top}u_b)v_c = -0.2(0.5)v_c = -0.1v_c$. These are negative multiples, so they **push** $u_b$ away from the negative sample vectors $v_b$ and $v_c$.\n\nThe first component of the updated vector $u_{b, \\text{new}}$ is $0.8537884$.\nRounding to four significant figures, we get $0.8538$.",
            "answer": "$$\\boxed{0.8538}$$"
        },
        {
            "introduction": "Now we shift our focus to the Continuous Bag-of-Words (CBOW) model, which predicts a target word from the average of its context word vectors. This exercise requires you to derive the gradient for a context embedding, a key step in understanding how CBOW distributes the error signal back to the multiple input words. Applying this to a mini-batch containing both positive and negative examples will solidify your understanding of how the model learns in a more realistic training scenario. ",
            "id": "3200079",
            "problem": "Consider the Continuous Bag-of-Words (CBOW) model in Word2Vec. Let the context set be $C=\\{c_1,\\dots,c_{|C|}\\}$, where each context word $c_i$ has an input embedding vector $v_{c_i}\\in\\mathbb{R}^d$. The CBOW hidden representation is the average\n$$\nh=\\frac{1}{|C|}\\sum_{i=1}^{|C|}v_{c_i}.\n$$\nFor a target word $w$ with output embedding vector $u_w\\in\\mathbb{R}^d$, define the score $s=u_w^{\\top}h$ and the predicted probability $\\hat{y}=\\sigma(s)$, where $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$ is the logistic sigmoid. For a labeled pair $(C,w,y)$ with $y\\in\\{0,1\\}$, the objective to be minimized is the binary cross-entropy\n$$\nJ(C,w,y)=-\\Big(y\\,\\ln(\\sigma(s))+(1-y)\\,\\ln\\big(1-\\sigma(s)\\big)\\Big).\n$$\nFor a mini-batch that sums the losses $J$ over pairs $\\{(C^{(b)},w^{(b)},y^{(b)})\\}_{b=1}^B$, the gradient with respect to each $v_{c_i}$ is the sum of per-pair gradients.\n\nTask:\n1. Starting from the definitions above and basic calculus, derive the gradient $\\frac{\\partial J}{\\partial v_{c_i}}$ for a single pair $(C,w,y)$, expressing it in terms of $u_w$, $h$, $\\sigma(u_w^{\\top}h)$, $y$, and $|C|$.\n2. Then, consider the following specific mini-batch with shared context ($d=3$ and $|C|=2$):\n   - Context embeddings: $v_{c_1}=\\begin{pmatrix}1\\\\0\\\\2\\end{pmatrix}$ and $v_{c_2}=\\begin{pmatrix}0\\\\-1\\\\1\\end{pmatrix}$, so the same $h$ is used for all pairs in the mini-batch.\n   - One positive pair $(y=1)$ with target output embedding $u_{w^{(+)}}=\\begin{pmatrix}1\\\\2\\\\-1\\end{pmatrix}$.\n   - Two negative pairs $(y=0)$ with target output embeddings $u_{w^{(-1)}}=\\begin{pmatrix}-1\\\\0\\\\1\\end{pmatrix}$ and $u_{w^{(-2)}}=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}$.\n   The mini-batch gradient with respect to $v_{c_1}$ is the sum of the three per-pair gradients obtained in Part 1, each evaluated at the appropriate $y$ and $u_w$.\n3. Let $d=\\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix}$. Compute the scalar inner product $d^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right)$ for this mini-batch. Round your final numerical answer to four significant figures.\n\nYour final answer must be a single real number.",
            "solution": "The problem asks for a three-part calculation related to the gradients in a Continuous Bag-of-Words (CBOW) model. First, we must derive the general form of the gradient of the loss function with respect to a context word's input embedding. Second, we apply this to a specific mini-batch. Third, we compute a scalar projection of the resulting mini-batch gradient.\n\n**Part 1: Derivation of the Gradient $\\frac{\\partial J}{\\partial v_{c_i}}$**\n\nThe objective function for a single training pair $(C,w,y)$ is the binary cross-entropy loss:\n$$\nJ(C,w,y) = -\\Big(y \\ln(\\sigma(s)) + (1-y) \\ln(1-\\sigma(s))\\Big)\n$$\nwhere the score $s$ is defined as $s = u_w^{\\top} h$, and the hidden representation $h$ is the average of the context word vectors:\n$$\nh = \\frac{1}{|C|} \\sum_{j=1}^{|C|} v_{c_j}\n$$\nWe need to find the gradient of $J$ with respect to one of the context word vectors, $v_{c_i} \\in \\mathbb{R}^d$. We use the chain rule of calculus.\n\nFirst, we compute the derivative of the loss $J$ with respect to the score $s$. This is a standard result for binary cross-entropy loss with a final sigmoid activation:\n$$\n\\frac{\\partial J}{\\partial s} = \\sigma(s) - y\n$$\nNext, we compute the derivative of the score $s$ with respect to the vector $v_{c_i}$. The score $s$ depends on $v_{c_i}$ through $h$. Using the rules of vector calculus:\n$$\n\\frac{\\partial s}{\\partial v_{c_i}} = \\frac{\\partial (u_w^{\\top} h)}{\\partial v_{c_i}} = u_w^{\\top} \\frac{\\partial h}{\\partial v_{c_i}}\n$$\nThe derivative of $h$ with respect to $v_{c_i}$ is:\n$$\n\\frac{\\partial h}{\\partial v_{c_i}} = \\frac{\\partial}{\\partial v_{c_i}} \\left( \\frac{1}{|C|} \\sum_{j=1}^{|C|} v_{c_j} \\right) = \\frac{1}{|C|} \\mathbf{I}\n$$\nwhere $\\mathbf{I}$ is the identity matrix. So, $\\frac{\\partial s}{\\partial v_{c_i}} = \\frac{1}{|C|} u_w$.\nCombining the parts using the chain rule:\n$$\n\\frac{\\partial J}{\\partial v_{c_i}} = \\frac{\\partial J}{\\partial s} \\frac{\\partial s}{\\partial v_{c_i}} = (\\sigma(s)-y) \\frac{1}{|C|} u_w\n$$\nRe-expressing in the requested terms:\n$$\n\\frac{\\partial J}{\\partial v_{c_i}} = \\frac{\\sigma(u_w^{\\top}h)-y}{|C|} u_w\n$$\nThis completes the first part of the task.\n\n**Part 2 & 3: Mini-batch Gradient Calculation and Projection**\n\nThe problem asks for the scalar value $d^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right)$. The mini-batch gradient is the sum of the gradients from each of the three pairs in the batch. Let the pairs be indexed by $b \\in \\{ (+), (-1), (-2) \\}$.\n$$\n\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}} = \\sum_{b} \\frac{\\partial J^{(b)}}{\\partial v_{c_1}} = \\sum_{b} \\frac{\\sigma({u_w^{(b)}}^{\\top}h)-y^{(b)}}{|C|} u_w^{(b)}\n$$\nWe want to compute the inner product of this gradient with the vector $d$:\n$$\nd^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right) = d^{\\top} \\sum_{b} \\frac{\\sigma({u_w^{(b)}}^{\\top}h)-y^{(b)}}{|C|} u_w^{(b)} = \\frac{1}{|C|} \\sum_{b} \\left(\\sigma({u_w^{(b)}}^{\\top}h)-y^{(b)}\\right) (d^{\\top}u_w^{(b)})\n$$\nFirst, we calculate the shared hidden representation $h$.\nGiven $|C|=2$, $v_{c_1}=\\begin{pmatrix}1\\\\0\\\\2\\end{pmatrix}$, and $v_{c_2}=\\begin{pmatrix}0\\\\-1\\\\1\\end{pmatrix}$:\n$$\nh = \\frac{1}{2}(v_{c_1} + v_{c_2}) = \\frac{1}{2}\\left(\\begin{pmatrix}1\\\\0\\\\2\\end{pmatrix} + \\begin{pmatrix}0\\\\-1\\\\1\\end{pmatrix}\\right) = \\frac{1}{2}\\begin{pmatrix}1\\\\-1\\\\3\\end{pmatrix} = \\begin{pmatrix}0.5\\\\-0.5\\\\1.5\\end{pmatrix}\n$$\nNext, we calculate the terms for each pair in the mini-batch.\n\n1.  **Positive pair (+):** $y^{(+)}=1$, $u_{w^{(+)}}=\\begin{pmatrix}1\\\\2\\\\-1\\end{pmatrix}$\n    - Score $s^{(+)} = {u_{w^{(+)}}}^{\\top}h = \\begin{pmatrix}1 & 2 & -1\\end{pmatrix}\\begin{pmatrix}0.5\\\\-0.5\\\\1.5\\end{pmatrix} = 0.5 - 1.0 - 1.5 = -2$.\n    - Inner product with $d$: $d^{\\top}u_{w^{(+)}} = \\begin{pmatrix}1 & 0 & -1\\end{pmatrix}\\begin{pmatrix}1\\\\2\\\\-1\\end{pmatrix} = 1+0+1 = 2$.\n    - Contribution to sum: $(\\sigma(-2)-1)(2)$.\n\n2.  **First negative pair (-1):** $y^{(-1)}=0$, $u_{w^{(-1)}}=\\begin{pmatrix}-1\\\\0\\\\1\\end{pmatrix}$\n    - Score $s^{(-1)} = {u_{w^{(-1)}}}^{\\top}h = \\begin{pmatrix}-1 & 0 & 1\\end{pmatrix}\\begin{pmatrix}0.5\\\\-0.5\\\\1.5\\end{pmatrix} = -0.5+0+1.5 = 1$.\n    - Inner product with $d$: $d^{\\top}u_{w^{(-1)}} = \\begin{pmatrix}1 & 0 & -1\\end{pmatrix}\\begin{pmatrix}-1\\\\0\\\\1\\end{pmatrix} = -1+0-1 = -2$.\n    - Contribution to sum: $(\\sigma(1)-0)(-2) = -2\\sigma(1)$.\n\n3.  **Second negative pair (-2):** $y^{(-2)}=0$, $u_{w^{(-2)}}=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}$\n    - Score $s^{(-2)} = {u_{w^{(-2)}}}^{\\top}h = \\begin{pmatrix}0 & 1 & 0\\end{pmatrix}\\begin{pmatrix}0.5\\\\-0.5\\\\1.5\\end{pmatrix} = 0-0.5+0 = -0.5$.\n    - Inner product with $d$: $d^{\\top}u_{w^{(-2)}} = \\begin{pmatrix}1 & 0 & -1\\end{pmatrix}\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix} = 0+0+0 = 0$.\n    - Contribution to sum: $(\\sigma(-0.5)-0)(0) = 0$.\n\nNow, we sum the contributions and multiply by $\\frac{1}{|C|} = \\frac{1}{2}$:\n$$\nd^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right) = \\frac{1}{2} \\Big[ (\\sigma(-2)-1)(2) - 2\\sigma(1) + 0 \\Big] = \\sigma(-2) - 1 - \\sigma(1)\n$$\nUsing the property $\\sigma(-x) = 1 - \\sigma(x)$, we have $\\sigma(-2) = 1-\\sigma(2)$. Substituting this in:\n$$\nd^{\\top}\\left(\\frac{\\partial J_{\\text{mini-batch}}}{\\partial v_{c_1}}\\right) = (1-\\sigma(2)) - 1 - \\sigma(1) = -\\sigma(2) - \\sigma(1)\n$$\nNow we compute the numerical value:\n$$\n-\\sigma(2) - \\sigma(1) = -\\left( \\frac{1}{1+\\exp(-2)} + \\frac{1}{1+\\exp(-1)} \\right)\n$$\n- $\\sigma(2) = \\frac{1}{1+e^{-2}} \\approx \\frac{1}{1+0.135335} \\approx 0.880797$\n- $\\sigma(1) = \\frac{1}{1+e^{-1}} \\approx \\frac{1}{1+0.367879} \\approx 0.731059$\n\nThe final value is:\n$$\n-0.880797 - 0.731059 = -1.611856\n$$\nRounding to four significant figures gives $-1.612$.",
            "answer": "$$\\boxed{-1.612}$$"
        },
        {
            "introduction": "This final practice moves from the mechanics of gradient updates to a more conceptual analysis of model behavior. You will investigate how the standard CBOW approach of equally averaging context vectors can be biased by the frequency of words in the corpus. By deriving a reweighting scheme to counteract this bias, you will gain insight into the subtle design choices that can impact an embedding model's performance and fairness, a crucial skill for any machine learning practitioner. ",
            "id": "3200013",
            "problem": "Consider the Continuous Bag-of-Words (CBOW) formulation of Word2Vec, where the score for a target word is given by the inner product $v_{w}^{\\top} h$, with $v_{w} \\in \\mathbb{R}^{d}$, the embedding of the target word and $h \\in \\mathbb{R}^{d}$ the aggregated context representation. Let the set of possible context types be $C = \\{c_{1}, c_{2}, \\dots, c_{|C|}\\}$, with embeddings $v_{c_{i}} \\in \\mathbb{R}^{d}$, and let $p(c_{i})$ denote the empirical frequency of each context type. Assume that a randomly drawn context token from the corpus has type $c_{i}$ with probability $p(c_{i})$, and that contexts within a window are independent and identically distributed according to $p$.\n\nDefine two aggregations:\n1. Equal averaging across context types: $h_{\\mathrm{equal}} = \\frac{1}{|C|} \\sum_{i=1}^{|C|} v_{c_{i}}$.\n2. Frequency-weighted aggregation by corpus distribution: $h_{\\mathrm{freq}} = \\sum_{i=1}^{|C|} p(c_{i}) \\, v_{c_{i}}$.\n\na) Using only the linearity of the inner product and expectation, derive the general symbolic expression for the bias in the score toward frequent contexts,\n$$\\Delta_{\\mathrm{bias}}(w) = v_{w}^{\\top} h_{\\mathrm{freq}} - v_{w}^{\\top} h_{\\mathrm{equal}}.$$\n\nb) Consider a concrete instance with $|C| = 4$, $d = 2$, target vector $v_{w} = \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix}$, context embeddings\n$$v_{c_{1}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad v_{c_{2}} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}, \\quad v_{c_{3}} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}, \\quad v_{c_{4}} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix},$$\nand empirical frequencies\n$$p(c_{1}) = 0.4, \\quad p(c_{2}) = 0.3, \\quad p(c_{3}) = 0.2, \\quad p(c_{4}) = 0.1.$$\nCompute the numerical value of $\\Delta_{\\mathrm{bias}}(w)$.\n\nc) You decide to reweight contexts by a function $\\alpha(c)$ of their empirical frequencies so that the expected context representation under the corpus distribution matches the equal-averaged representation across types, that is,\n$$\\sum_{i=1}^{|C|} p(c_{i}) \\, \\alpha(c_{i}) \\, v_{c_{i}} \\;=\\; \\frac{1}{|C|} \\sum_{i=1}^{|C|} v_{c_{i}}.$$\nSolve for the reweighting function $\\alpha(c)$ in terms of $p(c)$ and $|C|$ that guarantees this equality for arbitrary $\\{v_{c_{i}}\\}$.\n\nd) Using the reweighting $\\alpha(c)$ you derived, define $h_{\\mathrm{corr}} = \\sum_{i=1}^{|C|} p(c_{i}) \\, \\alpha(c_{i}) \\, v_{c_{i}}$ and compute the corrected score difference\n$$\\Delta_{\\mathrm{corr}}(w) = v_{w}^{\\top} h_{\\mathrm{corr}} - v_{w}^{\\top} h_{\\mathrm{equal}}$$\nfor the numerical instance in part b). Express the final answer as a single row matrix containing the two quantities from parts b) and d), in exact form (no rounding).",
            "solution": "**a) Derivation of the bias expression**\n\nThe bias in the score is the difference between the score from the frequency-weighted context and the score from the equal-averaged context.\n$$\n\\Delta_{\\mathrm{bias}}(w) = v_{w}^{\\top} h_{\\mathrm{freq}} - v_{w}^{\\top} h_{\\mathrm{equal}}\n$$\nUsing the linearity of the inner product:\n$$\n\\Delta_{\\mathrm{bias}}(w) = v_{w}^{\\top} (h_{\\mathrm{freq}} - h_{\\mathrm{equal}})\n$$\nSubstitute the definitions of $h_{\\mathrm{freq}}$ and $h_{\\mathrm{equal}}$:\n$$\nh_{\\mathrm{freq}} - h_{\\mathrm{equal}} = \\sum_{i=1}^{|C|} p(c_{i}) \\, v_{c_{i}} - \\frac{1}{|C|} \\sum_{i=1}^{|C|} v_{c_{i}} = \\sum_{i=1}^{|C|} \\left( p(c_{i}) - \\frac{1}{|C|} \\right) v_{c_{i}}\n$$\nSubstituting this back into the bias expression gives the general symbolic form:\n$$\n\\Delta_{\\mathrm{bias}}(w) = v_{w}^{\\top} \\left( \\sum_{i=1}^{|C|} \\left( p(c_{i}) - \\frac{1}{|C|} \\right) v_{c_{i}} \\right)\n$$\n\n**b) Numerical computation of the bias**\n\nFirst, compute the two aggregated context representations, $h_{\\mathrm{freq}}$ and $h_{\\mathrm{equal}}$.\n$$\nh_{\\mathrm{freq}} = 0.4 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + 0.3 \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} + 0.2 \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} + 0.1 \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0.4 - 0.2 + 0.2 \\\\ 0.6 + 0.2 - 0.1 \\end{pmatrix} = \\begin{pmatrix} 0.4 \\\\ 0.7 \\end{pmatrix}\n$$\n$$\nh_{\\mathrm{equal}} = \\frac{1}{4} \\left( \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} + \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\right) = \\frac{1}{4} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix}\n$$\nNow compute the scores and their difference using $v_{w} = \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix}$:\n$$\nv_{w}^{\\top} h_{\\mathrm{freq}} = \\begin{pmatrix} 3 & -2 \\end{pmatrix} \\begin{pmatrix} 0.4 \\\\ 0.7 \\end{pmatrix} = 1.2 - 1.4 = -0.2\n$$\n$$\nv_{w}^{\\top} h_{\\mathrm{equal}} = \\begin{pmatrix} 3 & -2 \\end{pmatrix} \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix} = 1.5 - 1.0 = 0.5\n$$\n$$\n\\Delta_{\\mathrm{bias}}(w) = -0.2 - 0.5 = -0.7\n$$\n\n**c) Derivation of the reweighting function $\\alpha(c)$**\n\nWe need to find $\\alpha(c_i)$ such that the following equality holds for any set of vectors $\\{v_{c_{i}}\\}$:\n$$\n\\sum_{i=1}^{|C|} p(c_{i}) \\, \\alpha(c_{i}) \\, v_{c_{i}} \\;=\\; \\frac{1}{|C|} \\sum_{i=1}^{|C|} v_{c_{i}}\n$$\nWe can rewrite this as:\n$$\n\\sum_{i=1}^{|C|} \\left( p(c_{i}) \\, \\alpha(c_{i}) - \\frac{1}{|C|} \\right) v_{c_{i}} \\;=\\; \\vec{0}\n$$\nFor this linear combination of arbitrary vectors to be the zero vector, each of the scalar coefficients must be zero. Thus, for each $i$:\n$$\np(c_{i}) \\, \\alpha(c_{i}) - \\frac{1}{|C|} = 0\n$$\nAssuming $p(c_i) > 0$, we can solve for $\\alpha(c_i)$:\n$$\n\\alpha(c_{i}) = \\frac{1}{|C| \\, p(c_{i})}\n$$\n\n**d) Computation of the corrected score difference**\n\nThe corrected context representation is $h_{\\mathrm{corr}} = \\sum_{i=1}^{|C|} p(c_{i}) \\, \\alpha(c_{i}) \\, v_{c_{i}}$.\nUsing our result from part c), the weighting factor for each vector $v_{c_i}$ becomes $p(c_{i}) \\alpha(c_{i}) = p(c_{i}) \\frac{1}{|C| p(c_i)} = \\frac{1}{|C|}$.\nTherefore, $h_{\\mathrm{corr}}$ simplifies to:\n$$\nh_{\\mathrm{corr}} = \\sum_{i=1}^{|C|} \\frac{1}{|C|} v_{c_{i}} = h_{\\mathrm{equal}}\n$$\nThe corrected score difference is:\n$$\n\\Delta_{\\mathrm{corr}}(w) = v_{w}^{\\top} h_{\\mathrm{corr}} - v_{w}^{\\top} h_{\\mathrm{equal}} = v_{w}^{\\top} h_{\\mathrm{equal}} - v_{w}^{\\top} h_{\\mathrm{equal}} = 0\n$$\nThe reweighting is designed to make the frequency-weighted average identical to the equal average, thus the difference is zero by construction.\n\nThe two quantities from parts b) and d) are $\\Delta_{\\mathrm{bias}}(w) = -0.7$ and $\\Delta_{\\mathrm{corr}}(w) = 0$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-0.7 & 0\n\\end{pmatrix}\n}\n$$"
        }
    ]
}