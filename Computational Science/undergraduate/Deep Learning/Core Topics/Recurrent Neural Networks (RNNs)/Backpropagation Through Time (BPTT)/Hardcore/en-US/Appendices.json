{
    "hands_on_practices": [
        {
            "introduction": "To truly appreciate the elegance and efficiency of Backpropagation Through Time (BPTT), it's instructive to first consider the alternative. This exercise guides you through the process of deriving the exact, closed-form gradient for a linear Recurrent Neural Network (RNN). By unrolling the recurrence relationship completely, you will see how the gradient with respect to the recurrent weights manifests as a complex sum of time-shifted correlations, revealing the deep temporal dependencies inherent in RNNs. This direct approach makes the computational burden clear and powerfully illustrates why the recursive, dynamic programming nature of BPTT is not just a convenience but a necessity for training on sequences of any significant length. ",
            "id": "3192146",
            "problem": "Consider a linear Recurrent Neural Network (RNN) with hidden state dimension $d_h$, input dimension $d_x$, and output dimension $d_y$, driven by a single sequence of length $T$. The recurrence and readout are given by the core definitions\n$h_t = W_h h_{t-1} + W_x x_t$ with $h_0 = 0$, and $y_t = W_y h_t$,\nfor $t \\in \\{1,2,\\dots,T\\}$, where $W_h \\in \\mathbb{R}^{d_h \\times d_h}$, $W_x \\in \\mathbb{R}^{d_h \\times d_x}$, and $W_y \\in \\mathbb{R}^{d_y \\times d_h}$. The input vectors are $\\{x_t \\in \\mathbb{R}^{d_x}\\}_{t=1}^T$ and the targets are $\\{s_t \\in \\mathbb{R}^{d_y}\\}_{t=1}^T$. Consider the quadratic loss\n$L = \\frac{1}{2} \\sum_{t=1}^T \\|y_t - s_t\\|_2^2$.\nLet the output residuals be $e_t = y_t - s_t$ for each $t$. Using only the recurrence definition, the readout definition, the chain rule from multivariable calculus, and standard linear algebraic facts, derive a closed-form analytic expression for the gradient $\\nabla_{W_h} L$ that makes explicit its decomposition as a sum of time-shifted correlation terms of the form $\\sum e_{\\cdot}\\, h_{\\cdot}^{\\top}$. Then, briefly discuss the computational complexity (in Bigâ€“$O$ notation) of evaluating this gradient when implemented (i) directly from your closed-form expression by summing the time-shifted terms, and (ii) via Backpropagation Through Time (BPTT) using the first-order recursion for the adjoint signals. Your final answer must be a single closed-form analytic matrix expression for $\\nabla_{W_h} L$ written in terms of $W_h$, $W_y$, $\\{e_t\\}$, and $\\{h_t\\}$ only. Do not include any numerical evaluation.",
            "solution": "The problem is assessed to be valid. It is a well-posed, scientifically grounded, and objective problem statement from the field of deep learning theory. It requires the derivation of a gradient for a standard linear Recurrent Neural Network (RNN), which is a formal and solvable task based on established principles of multivariable calculus and linear algebra. All necessary definitions and conditions are provided.\n\nWe begin by stating the core definitions provided in the problem:\nRecurrence relation: $h_t = W_h h_{t-1} + W_x x_t$ for $t \\in \\{1, 2, \\dots, T\\}$, with initial state $h_0 = 0$.\nReadout equation: $y_t = W_y h_t$.\nLoss function: $L = \\frac{1}{2} \\sum_{t=1}^T \\|y_t - s_t\\|_2^2 = \\frac{1}{2} \\sum_{t=1}^T \\|e_t\\|_2^2$, where $e_t = y_t - s_t$.\nThe parameters are matrices $W_h \\in \\mathbb{R}^{d_h \\times d_h}$, $W_x \\in \\mathbb{R}^{d_h \\times d_x}$, and $W_y \\in \\mathbb{R}^{d_y \\times d_h}$. The inputs are vectors $x_t \\in \\mathbb{R}^{d_x}$ and targets $s_t \\in \\mathbb{R}^{d_y}$.\n\nOur goal is to derive a closed-form expression for the gradient of the loss function $L$ with respect to the recurrent weight matrix $W_h$, denoted as $\\nabla_{W_h} L$. We will use the chain rule for matrix calculus.\n\nThe loss $L$ is a function of $W_h$ through its influence on the hidden states $\\{h_t\\}_{t=1}^T$. The gradient can be expressed as a sum of contributions from each timestep where $W_h$ is used in the forward pass:\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\nabla_{W_h}^{(\\text{local at } t)} L\n$$\nwhere the \"local\" gradient refers to the gradient with respect to the instance of $W_h$ used in the computation $h_t = W_h h_{t-1} + W_x x_t$. Using the chain rule, this contribution is given by the outer product of the gradient of the loss with respect to the state $h_t$, and the gradient of the state-update term with respect to $W_h$. Let $\\delta_t = \\frac{\\partial L}{\\partial h_t} \\in \\mathbb{R}^{d_h}$ be the gradient of the total loss with respect to the hidden state $h_t$. The gradient of the pre-activation $a_t = W_h h_{t-1} + W_x x_t$ with respect to $W_h$ is $h_{t-1}^T$. Thus, the contribution to the gradient from timestep $t$ is $\\delta_t h_{t-1}^T$. Summing over all timesteps gives the total gradient:\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\delta_t h_{t-1}^T\n$$\nThis is the standard formula for the recurrent weight gradient in BPTT. However, the problem requires a closed-form expression in terms of the given quantities $W_h, W_y, \\{e_t\\}, \\{h_t\\}$. To achieve this, we must derive a closed form for $\\delta_t$.\n\nThe vector $\\delta_t$ represents the total influence of $h_t$ on the loss $L$. The state $h_t$ influences the loss through the outputs $y_k$ for all subsequent timesteps $k \\ge t$.\n$$\n\\delta_t = \\frac{\\partial L}{\\partial h_t} = \\sum_{k=t}^{T} \\frac{\\partial L}{\\partial y_k} \\frac{\\partial y_k}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_t}\n$$\nHere, we apply the chain rule, summing over all paths from $h_t$ to the final loss. The partial derivatives are (using numerator-layout matrix calculus conventions where the gradient of a scalar w.r.t. a column vector is a column vector):\n$1.$ $\\frac{\\partial L}{\\partial y_k} = \\frac{\\partial}{\\partial y_k} \\left( \\frac{1}{2} \\sum_{j=1}^T \\|y_j - s_j\\|_2^2 \\right) = y_k - s_k = e_k$. This is a column vector of size $d_y$.\n$2.$ $\\frac{\\partial y_k}{\\partial h_k} = W_y$. This is the Jacobian matrix of size $d_y \\times d_h$. The term in the chain rule is its transpose, $(\\frac{\\partial y_k}{\\partial h_k})^T = W_y^T$.\n$3.$ $\\frac{\\partial h_k}{\\partial h_t}$. We find this by unrolling the recurrence relation:\n$h_k = W_h h_{k-1} + W_x x_k = W_h (W_h h_{k-2} + W_x x_{k-1}) + W_x x_k = \\dots$\nThe dependence of $h_k$ on $h_t$ for $k > t$ is given by:\n$h_k = W_h^{k-t} h_t + \\sum_{j=t+1}^{k} W_h^{k-j} W_x x_j$.\nThe Jacobian matrix is therefore $\\frac{\\partial h_k}{\\partial h_t} = W_h^{k-t}$ for $k \\ge t$. The term in the chain rule composition is its transpose, $(W_h^{k-t})^T = (W_h^T)^{k-t}$.\n\nSubstituting these derivatives back into the expression for $\\delta_t$:\n$$\n\\delta_t = \\sum_{k=t}^{T} \\left( \\frac{\\partial h_k}{\\partial h_t} \\right)^T \\left( \\frac{\\partial y_k}{\\partial h_k} \\right)^T \\frac{\\partial L}{\\partial y_k} = \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k\n$$\nThis provides the closed-form expression for the gradient signal $\\delta_t$ propagated back from all future timesteps $k \\ge t$.\n\nFinally, we substitute this expression for $\\delta_t$ into our equation for the total gradient $\\nabla_{W_h} L$:\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\left( \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k \\right) h_{t-1}^T\n$$\nThis is the required closed-form analytic expression. It decomposes the gradient into a double summation over time. Each term $(W_h^T)^{k-t} W_y^T e_k h_{t-1}^T$ can be interpreted as a \"correlation\" between the error $e_k$ at a future time $k$ and the hidden state $h_{t-1}$ at a past time $t-1$, mediated by the matrix $(W_h^T)^{k-t} W_y^T$ which accounts for the propagation of influence through the network's dynamics and readout layer.\n\nNext, we discuss the computational complexity.\n(i) Direct evaluation from the closed-form expression:\nThe formula is $\\sum_{t=1}^{T} \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k h_{t-1}^T$.\nAssume the forward pass has been run, so all $e_t \\in \\mathbb{R}^{d_y}$ and $h_t \\in \\mathbb{R}^{d_h}$ are known.\nThe double loop involves $O(T^2)$ terms. For each term $(t, k)$, the computation involves:\n- A matrix power $(W_h^T)^{k-t}$, which, if not pre-computed, takes $O((k-t) d_h^3)$. A pre-computation of all necessary powers of $W_h^T$ up to $T-1$ takes $O(T d_h^3)$.\n- A matrix-vector product $W_y^T e_k$, costing $O(d_h d_y)$.\n- A matrix-vector product $(W_h^T)^{k-t} (W_y^T e_k)$, costing $O(d_h^2)$.\n- An outer product with $h_{t-1}^T$, costing $O(d_h^2)$.\nA naive implementation would be very expensive. A more optimized direct evaluation would proceed as:\n`total_grad = 0`\n`for t = 1 to T:`\n  `inner_sum_vec = 0`\n  `for k = t to T:`\n    `vec = ... compute (W_h^T)^{k-t} W_y^T e_k ...`\n    `inner_sum_vec += vec`\n  `total_grad += outer(inner_sum_vec, h_{t-1})`\nThe most expensive part is the double loop structure. There are approximately $T^2/2$ pairs of $(t,k)$. For each pair, the dominant cost is computing the matrix-vector product with the matrix power, which is $O(d_h^2)$ assuming powers are precomputed. The precomputation costs $O(T d_h^3)$. Therefore, the total complexity is dominated by the nested loops and the precomputation, leading to a complexity of $O(T d_h^3 + T^2 d_h^2)$. The quadratic dependence on the sequence length $T$ makes this method computationally prohibitive for long sequences.\n\n(ii) Evaluation via Backpropagation Through Time (BPTT):\nBPTT avoids the explicit calculation of the double summation by using dynamic programming. It computes the gradients via a single backward pass through time. The key is the recursive relationship for $\\delta_t$:\n$\\delta_T = W_y^T e_T$\n$\\delta_t = W_y^T e_t + W_h^T \\delta_{t+1}$ for $t = T-1, \\dots, 1$.\nThe BPTT algorithm for computing $\\nabla_{W_h} L$ is:\n$1.$ Perform a forward pass from $t=1$ to $T$ to compute and store all $h_t$ and $e_t$. Cost: $O(T(d_h^2 + d_h d_x + d_y d_h))$.\n$2.$ Initialize $\\nabla_{W_h} L = 0$ and $\\delta_{T+1} = 0$.\n$3.$ Perform a backward pass from $t=T$ down to $1$:\n   a. Compute $\\delta_t = W_y^T e_t + W_h^T \\delta_{t+1}$. This involves two matrix-vector products and a vector addition, costing $O(d_h d_y + d_h^2)$.\n   b. Add the contribution to the gradient: $\\nabla_{W_h} L \\leftarrow \\nabla_{W_h} L + \\delta_t h_{t-1}^T$. This is an outer product and matrix addition, costing $O(d_h^2)$.\nThe backward pass consists of a single loop of length $T$, with each step costing $O(d_h^2 + d_h d_y)$. The total complexity of the backward pass is $O(T(d_h^2 + d_h d_y))$.\nThe total complexity of BPTT is therefore $O(T(d_h^2 + d_h d_x + d_y d_h))$, which is linear in the sequence length $T$. This is significantly more efficient than the direct evaluation of the closed-form expression.",
            "answer": "$$\n\\boxed{\\sum_{t=1}^{T} \\left( \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k \\right) h_{t-1}^T}\n$$"
        },
        {
            "introduction": "The dynamics of gradient flow are at the heart of training RNNs, with the infamous vanishing and exploding gradient problems posing significant challenges. The structure of the loss function itself can dramatically influence this flow. This practice problem uses a simplified scalar RNN to isolate and examine this effect, asking you to compare the gradients generated by a loss applied only at the final timestep versus a loss distributed across the entire sequence. By deriving a closed-form expression for the ratio of these gradients, you will gain a quantitative and intuitive understanding of how providing \"supervisory signals\" at intermediate steps can help sustain gradient flow back to the beginning of a sequence, a key motivation for various architectural and training strategies. ",
            "id": "3101225",
            "problem": "Consider a scalar linear recurrent neural network defined for discrete time indices by the recurrence\n$$h_{t+1} = w\\,h_{t}, \\quad t = 0, 1, \\dots, T-1,$$\nwith initial hidden state $$h_{0} \\neq 0,$$ scalar parameter $$w \\in \\mathbb{R}$$ satisfying $$0  |w| \\neq 1,$$ and an integer horizon $$T \\in \\mathbb{N}$$ with $$T \\geq 1.$$ The output at each time is identified with the hidden state, $$y_{t} = h_{t}.$$ The per-time-step loss is\n$$\\ell_{t} = \\frac{1}{2}\\,h_{t}^{2}.$$\nTwo overall loss placements are considered:\n- a temporally distributed loss,\n$$L_{\\mathrm{sum}} = \\sum_{t=1}^{T} \\ell_{t},$$\n- and a final-time-only loss,\n$$L_{\\mathrm{final}} = \\ell_{T}.$$\n\nUsing only the chain rule of calculus, the definition of backpropagation through time (BPTT) as repeated application of the chain rule along the unfolded computational graph, and the model equations above, derive explicit symbolic expressions for $$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{t}}$$ and $$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}}$$ for arbitrary $$t \\in \\{0,1,\\dots,T\\}.$$ Then, as a quantitative summary of how loss placement affects gradient propagation back to the start of the sequence, compute the ratio\n$$R(w,T) \\equiv \\frac{\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{0}}}{\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{0}}}$$\nas a single closed-form analytic expression in terms of $$w$$ and $$T.$$\n\nYour final answer must be the simplified closed-form expression for $$R(w,T)$$. No numerical evaluation is required and no rounding is needed. Do not include units, and express all quantities using standard mathematical notation in LaTeX.",
            "solution": "The problem is first validated to ensure it is well-posed, scientifically grounded, and objective.\n\n### Step 1: Extract Givens\n- Recurrence relation: $h_{t+1} = w h_{t}$ for $t = 0, 1, \\dots, T-1$.\n- Initial condition: $h_{0} \\neq 0$.\n- Parameter constraints: $w \\in \\mathbb{R}$, $0  |w| \\neq 1$.\n- Horizon: $T \\in \\mathbb{N}$, $T \\geq 1$.\n- Output definition: $y_{t} = h_{t}$.\n- Per-time-step loss: $\\ell_{t} = \\frac{1}{2} h_{t}^{2}$.\n- Temporally distributed loss: $L_{\\mathrm{sum}} = \\sum_{t=1}^{T} \\ell_{t}$.\n- Final-time-only loss: $L_{\\mathrm{final}} = \\ell_{T}$.\n- Quantities to derive:\n    1. Explicit symbolic expression for $\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{t}}$ for $t \\in \\{0, 1, \\dots, T\\}$.\n    2. Explicit symbolic expression for $\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}}$ for $t \\in \\{0, 1, \\dots, T\\}$.\n    3. The ratio $R(w,T) \\equiv \\frac{\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{0}}}{\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{0}}}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes a simplified linear recurrent neural network and asks for the computation of gradients using backpropagation through time (BPTT). This is a standard and fundamental topic in deep learning and is based on the chain rule of calculus. All definitions and relationships are mathematically sound.\n- **Well-Posed**: All variables, constants, and functions are clearly defined. The constraints $h_0 \\neq 0$ and $0  |w| \\neq 1$ ensure that denominators in gradient expressions and the final ratio are non-zero, leading to a unique and meaningful solution.\n- **Objective**: The problem is stated in precise mathematical language without any subjectivity or ambiguity.\n\nThe problem does not exhibit any of the flaws listed in the validation criteria.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation\n\nFirst, we establish the closed-form expression for the hidden state $h_t$. The recurrence $h_{t+1} = w h_t$ is a geometric progression. Unrolling it from the initial state $h_0$ gives:\n$$h_t = w^t h_0$$\nThis expression holds for all $t \\in \\{0, 1, \\dots, T\\}$.\n\n#### Derivation of $\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}}$\nThe final-time loss is $L_{\\mathrm{final}} = \\ell_T = \\frac{1}{2} h_T^2$.\nThe gradient of the loss with respect to an intermediate hidden state $h_t$ is found by applying the chain rule through the computational graph from $h_t$ to $h_T$. The state $h_t$ only influences $L_{\\mathrm{final}}$ through its effect on $h_T$.\n$$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}} = \\frac{\\partial L_{\\mathrm{final}}}{\\partial h_T} \\frac{\\partial h_T}{\\partial h_{t}}$$\nThe first term is the derivative of the loss function with respect to its direct input:\n$$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_T} = \\frac{\\partial}{\\partial h_T} \\left( \\frac{1}{2} h_T^2 \\right) = h_T$$\nThe second term is the derivative of a future state with respect to a past state. Using $h_T = w^{T-t} h_t$, we get:\n$$\\frac{\\partial h_T}{\\partial h_{t}} = \\frac{\\partial}{\\partial h_{t}} (w^{T-t} h_t) = w^{T-t}$$\nCombining these results, the gradient for any $t \\in \\{0, 1, \\dots, T\\}$ is:\n$$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}} = h_T w^{T-t}$$\nThis is one of the requested explicit expressions. We can also express this in terms of $h_0$ by substituting $h_T = w^T h_0$:\n$$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}} = (w^T h_0) w^{T-t} = h_0 w^{2T-t}$$\n\n#### Derivation of $\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{t}}$\nThe distributed loss is $L_{\\mathrm{sum}} = \\sum_{k=1}^{T} \\ell_k = \\sum_{k=1}^{T} \\frac{1}{2} h_k^2$.\nThe hidden state $h_t$ influences all losses $\\ell_k$ for $k \\ge t$. We must sum over all paths from $h_t$ to the total loss.\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_t} = \\frac{\\partial}{\\partial h_t} \\left( \\sum_{k=1}^{T} \\frac{1}{2} h_k^2 \\right)$$\nSince $h_k$ for $k  t$ does not depend on $h_t$, we can restrict the sum:\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_t} = \\sum_{k=t}^{T} \\frac{\\partial}{\\partial h_t} \\left( \\frac{1}{2} h_k^2 \\right) = \\sum_{k=t}^{T} \\frac{\\partial(\\frac{1}{2} h_k^2)}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_t} = \\sum_{k=t}^{T} h_k \\frac{\\partial h_k}{\\partial h_t}$$\nNote that the summation starts from $k=t$, but the loss $L_{\\mathrm{sum}}$ is defined from $t=1$. We must consider two cases for the lower bound of the sum.\n\n**Case 1: $t \\in \\{1, 2, \\dots, T\\}$**\nThe sum is valid as written. We have $\\frac{\\partial h_k}{\\partial h_t} = w^{k-t}$.\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_t} = \\sum_{k=t}^{T} h_k w^{k-t}$$\nSubstitute $h_k = w^{k-t} h_t$:\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_t} = \\sum_{k=t}^{T} (w^{k-t} h_t) w^{k-t} = h_t \\sum_{k=t}^{T} w^{2(k-t)}$$\nLet $j = k-t$. The sum becomes a geometric series:\n$$h_t \\sum_{j=0}^{T-t} (w^2)^j = h_t \\left( \\frac{(w^2)^{T-t+1} - 1}{w^2 - 1} \\right) = h_t \\frac{w^{2(T-t+1)} - 1}{w^2 - 1}$$\nSo, for $t \\in \\{1, \\dots, T\\}$, the explicit expression is:\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{t}} = h_t \\frac{w^{2(T-t+1)} - 1}{w^2 - 1}$$\n\n**Case 2: $t = 0$**\nThe loss sum starts at $k=1$, so $h_0$ has no direct contribution to any $\\ell_k$. Its influence is entirely through $h_k$ for $k \\ge 1$.\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_0} = \\sum_{k=1}^{T} \\frac{\\partial \\ell_k}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_0} = \\sum_{k=1}^{T} h_k w^k$$\nSubstitute $h_k = w^k h_0$:\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_0} = \\sum_{k=1}^{T} (w^k h_0) w^k = h_0 \\sum_{k=1}^{T} w^{2k}$$\nThis is a geometric series with first term $w^2$, ratio $w^2$, and $T$ terms.\n$$\\sum_{k=1}^{T} (w^2)^k = w^2 \\frac{(w^2)^T - 1}{w^2 - 1} = \\frac{w^2(w^{2T} - 1)}{w^2 - 1}$$\nThus, for $t=0$, the explicit expression is:\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_0} = h_0 \\frac{w^2(w^{2T} - 1)}{w^2 - 1}$$\n\n#### Calculation of the Ratio $R(w,T)$\nThe ratio is defined as $R(w,T) = \\frac{\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{0}}}{\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{0}}}$.\nFrom the derivations above, we have:\n- Numerator: $\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{0}} = h_0 \\frac{w^2(w^{2T} - 1)}{w^2 - 1}$\n- Denominator: For $t=0$, $\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_0} = h_0 w^{2T-0} = h_0 w^{2T}$.\n\nNow we compute the ratio. The condition $h_0 \\neq 0$ allows cancellation.\n$$R(w,T) = \\frac{h_0 \\frac{w^2(w^{2T} - 1)}{w^2 - 1}}{h_0 w^{2T}} = \\frac{w^2(w^{2T} - 1)}{(w^2 - 1)w^{2T}}$$\nWe can simplify this expression.\n$$R(w,T) = \\frac{w^2}{w^{2T}} \\cdot \\frac{w^{2T} - 1}{w^2 - 1} = w^{2-2T} \\frac{w^{2T} - 1}{w^2 - 1}$$\nAn alternative, more symmetric form is obtained by factoring out powers of $w$:\n$$R(w,T) = \\frac{w^{2T} - 1}{w^{2T}} \\cdot \\frac{w^2}{w^2 - 1} = \\left(1 - \\frac{1}{w^{2T}}\\right) \\cdot \\frac{1}{\\frac{w^2-1}{w^2}} = \\frac{1 - w^{-2T}}{1 - w^{-2}}$$\nThis is the final simplified closed-form expression for the ratio. The conditions $0  |w| \\neq 1$ ensure the denominator $1 - w^{-2}$ is non-zero.",
            "answer": "$$\\boxed{\\frac{1 - w^{-2T}}{1 - w^{-2}}}$$"
        },
        {
            "introduction": "Bridging the gap from theory to practice often involves handling messy, real-world data. In sequence modeling, this includes processing batches of sequences with varying lengths, which is typically managed using padding and masking. This exercise delves into the crucial role of correct masking within the BPTT framework. You will analyze a common implementation bug where a regularization term is incorrectly applied to padded timesteps, leading to \"gradient leakage.\" By calculating the resulting discrepancy in the gradient of a model parameter, you will develop a concrete understanding of why meticulous application of masks to all loss components is essential for ensuring that learning signals are derived only from the true, unpadded data. ",
            "id": "3101197",
            "problem": "Consider a single-sequence training example in a recurrent neural network (RNN), processed with Backpropagation Through Time (BPTT) and sequence masking to handle variable-length inputs. The recurrence is linear, with identity activation, and is defined by the state update and output equations\n$$h_t = a\\,h_{t-1} + b\\,x_t,\\quad y_t = c\\,h_t,$$\nwhere $h_t$ is the hidden state, $x_t$ is the input at time $t$, $y_t$ is the output, and $a$, $b$, $c$ are scalar parameters. The initial hidden state is $h_0 = 0$. The per-timestep squared-error data-fit loss and an output-regularization term are combined, and masking is applied to respect true sequence length:\n$$L_{\\text{proper}} = \\frac{1}{2}\\sum_{t=1}^{T_{\\max}} m_t\\left((y_t - r_t)^2 + \\lambda\\,y_t^2\\right),$$\nwhere $m_t \\in \\{0,1\\}$ is the mask at time $t$, $r_t$ is the target at time $t$, $\\lambda  0$ is the regularization strength, and $T_{\\max}$ is the maximum sequence length used for padding.\n\nNow suppose an implementation error where the regularization is accidentally computed across all padded timesteps without masking, yielding\n$$L_{\\text{improper}} = \\frac{1}{2}\\sum_{t=1}^{T_{\\max}} m_t\\,(y_t - r_t)^2 + \\frac{1}{2}\\lambda\\sum_{t=1}^{T_{\\max}} y_t^2.$$\n\nYou are given a single padded example with $T_{\\max} = 3$, true length $2$, and mask $(m_1, m_2, m_3) = (1, 1, 0)$, inputs $(x_1, x_2, x_3) = (2, 3, 0)$, targets $(r_1, r_2, r_3) = (1, -1, 0)$, and parameters $a = 0.5$, $b = 1$, $c = 1$, $\\lambda = 0.2$, $h_0 = 0$. Using only the chain rule of calculus as the fundamental base and the definitions above, derive gradients that respect the mask and analyze how the improper handling causes leakage of gradient across the padded timestep $t=3$.\n\nCompute the scalar discrepancy\n$$\\Delta = \\frac{\\partial L_{\\text{improper}}}{\\partial a} - \\frac{\\partial L_{\\text{proper}}}{\\partial a}$$\nfor this single example. Express your final answer as an exact real number. No rounding is required. The answer must be a single real-valued number.",
            "solution": "The problem requires the computation of the discrepancy $\\Delta = \\frac{\\partial L_{\\text{improper}}}{\\partial a} - \\frac{\\partial L_{\\text{proper}}}{\\partial a}$ for a simple recurrent neural network. This involves applying the Backpropagation Through Time (BPTT) algorithm, which is a direct application of the chain rule of calculus to the computational graph of the RNN unrolled over time.\n\nThe core of the calculation is to find the gradient of each loss function with respect to the parameter $a$. The parameter $a$ is used in the state update equation at each timestep $t$: $h_t = a\\,h_{t-1} + b\\,x_t$.\nApplying the chain rule, the total derivative of a loss function $L$ with respect to $a$ is the sum of its influence through each hidden state $h_t$:\n$$\n\\frac{\\partial L}{\\partial a} = \\sum_{t=1}^{T_{\\max}} \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial a}\n$$\nIn the context of BPTT, $\\frac{\\partial L}{\\partial h_t}$ is the total gradient of the loss with respect to the state $h_t$, and $\\frac{\\partial h_t}{\\partial a}$ is the local partial derivative from the state update equation. This local partial derivative is simply $h_{t-1}$.\nLet's define the gradient flowing back into state $h_t$ as $\\delta_t = \\frac{\\partial L}{\\partial h_t}$. The gradient for $a$ is then an accumulation over time:\n$$\n\\frac{\\partial L}{\\partial a} = \\sum_{t=1}^{T_{\\max}} \\delta_t h_{t-1}\n$$\nThe gradient $\\delta_t$ itself is computed via a recurrence relation that also follows from the chain rule. The state $h_t$ influences the loss in two ways: directly through the output $y_t$, and indirectly through the next state $h_{t+1}$.\n$$\n\\delta_t = \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L}{\\partial y_t}\\frac{\\partial y_t}{\\partial h_t} + \\frac{\\partial L}{\\partial h_{t+1}}\\frac{\\partial h_{t+1}}{\\partial h_t}\n$$\nUsing the given model equations, $y_t = c\\,h_t$ and $h_{t+1} = a\\,h_t + b\\,x_{t+1}$, we find the partial derivatives $\\frac{\\partial y_t}{\\partial h_t}=c$ and $\\frac{\\partial h_{t+1}}{\\partial h_t}=a$. This gives the BPTT recurrence for the state gradients:\n$$\n\\delta_t = \\frac{\\partial L}{\\partial y_t} c + \\delta_{t+1} a\n$$\nThis recurrence is initialized at the end of the sequence with $\\delta_{T_{\\max}+1}=0$ and computed backwards in time from $t=T_{\\max}$ down to $t=1$. The term $\\frac{\\partial L}{\\partial y_t}$ is the local gradient of the loss with respect to the output at timestep $t$.\n\nThe calculation proceeds in three main steps:\n1.  A forward pass to compute the hidden states $h_t$ and outputs $y_t$.\n2.  A backward pass for each loss function ($L_{\\text{proper}}$ and $L_{\\text{improper}}$) to compute the gradients $\\delta_t$ and accumulate the total gradient with respect to $a$.\n3.  Computation of the final discrepancy $\\Delta$.\n\n**Step 1: Forward Pass**\nGiven values: $a = 0.5$, $b = 1$, $c = 1$, and initial state $h_0 = 0$.\nThe input sequence is $(x_1, x_2, x_3) = (2, 3, 0)$.\n\nFor $t=1$:\n$h_1 = a\\,h_0 + b\\,x_1 = (0.5)(0) + (1)(2) = 2$\n$y_1 = c\\,h_1 = (1)(2) = 2$\n\nFor $t=2$:\n$h_2 = a\\,h_1 + b\\,x_2 = (0.5)(2) + (1)(3) = 1 + 3 = 4$\n$y_2 = c\\,h_2 = (1)(4) = 4$\n\nFor $t=3$:\n$h_3 = a\\,h_2 + b\\,x_3 = (0.5)(4) + (1)(0) = 2 + 0 = 2$\n$y_3 = c\\,h_3 = (1)(2) = 2$\n\nForward pass summary: $h_0=0, h_1=2, h_2=4, h_3=2$ and $y_1=2, y_2=4, y_3=2$.\n\n**Step 2a: Backward Pass for $L_{\\text{proper}}$**\nThe local output gradients are $\\frac{\\partial L_{\\text{proper}}}{\\partial y_t} = m_t((1+\\lambda)y_t - r_t)$.\nGiven: $\\lambda=0.2$, $(m_1, m_2, m_3)=(1, 1, 0)$, $(r_1, r_2, r_3) = (1, -1, 0)$.\n\nFor $t=1$: $\\frac{\\partial L_{\\text{proper}}}{\\partial y_1} = 1 \\times ((1+0.2)(2) - 1) = 1.2 \\times 2 - 1 = 2.4 - 1 = 1.4$.\nFor $t=2$: $\\frac{\\partial L_{\\text{proper}}}{\\partial y_2} = 1 \\times ((1+0.2)(4) - (-1)) = 1.2 \\times 4 + 1 = 4.8 + 1 = 5.8$.\nFor $t=3$: $\\frac{\\partial L_{\\text{proper}}}{\\partial y_3} = 0 \\times ((1+0.2)(2) - 0) = 0$.\n\nNow we compute $\\delta_t^{\\text{proper}}$ backwards, starting with $\\delta_4^{\\text{proper}} = 0$.\nFor $t=3$: $\\delta_3^{\\text{proper}} = \\frac{\\partial L_{\\text{proper}}}{\\partial y_3} c + \\delta_4^{\\text{proper}} a = (0)(1) + (0)(0.5) = 0$.\nFor $t=2$: $\\delta_2^{\\text{proper}} = \\frac{\\partial L_{\\text{proper}}}{\\partial y_2} c + \\delta_3^{\\text{proper}} a = (5.8)(1) + (0)(0.5) = 5.8$.\nFor $t=1$: $\\delta_1^{\\text{proper}} = \\frac{\\partial L_{\\text{proper}}}{\\partial y_1} c + \\delta_2^{\\text{proper}} a = (1.4)(1) + (5.8)(0.5) = 1.4 + 2.9 = 4.3$.\n\nFinally, we compute the total gradient for $a$:\n$$\n\\frac{\\partial L_{\\text{proper}}}{\\partial a} = \\sum_{t=1}^{3} \\delta_t^{\\text{proper}} h_{t-1} = \\delta_1^{\\text{proper}}h_0 + \\delta_2^{\\text{proper}}h_1 + \\delta_3^{\\text{proper}}h_2\n$$\n$$\n\\frac{\\partial L_{\\text{proper}}}{\\partial a} = (4.3)(0) + (5.8)(2) + (0)(4) = 0 + 11.6 + 0 = 11.6\n$$\n\n**Step 2b: Backward Pass for $L_{\\text{improper}}$**\nThe local output gradients are $\\frac{\\partial L_{\\text{improper}}}{\\partial y_t} = m_t(y_t - r_t) + \\lambda y_t$.\n\nFor $t=1$: $\\frac{\\partial L_{\\text{improper}}}{\\partial y_1} = 1 \\times (2 - 1) + (0.2)(2) = 1 + 0.4 = 1.4$.\nFor $t=2$: $\\frac{\\partial L_{\\text{improper}}}{\\partial y_2} = 1 \\times (4 - (-1)) + (0.2)(4) = 5 + 0.8 = 5.8$.\nFor $t=3$: $\\frac{\\partial L_{\\text{improper}}}{\\partial y_3} = 0 \\times (2 - 0) + (0.2)(2) = 0 + 0.4 = 0.4$.\n\nThe non-zero gradient at $t=3$ is the source of the \"leakage\". Although the data-fit term is masked out ($m_3=0$), the regularization term is not, creating a gradient signal at this padded timestep.\n\nNow we compute $\\delta_t^{\\text{improper}}$ backwards, starting with $\\delta_4^{\\text{improper}} = 0$.\nFor $t=3$: $\\delta_3^{\\text{improper}} = \\frac{\\partial L_{\\text{improper}}}{\\partial y_3} c + \\delta_4^{\\text{improper}} a = (0.4)(1) + (0)(0.5) = 0.4$.\nThis non-zero gradient at $h_3$ will now propagate to earlier timesteps.\nFor $t=2$: $\\delta_2^{\\text{improper}} = \\frac{\\partial L_{\\text{improper}}}{\\partial y_2} c + \\delta_3^{\\text{improper}} a = (5.8)(1) + (0.4)(0.5) = 5.8 + 0.2 = 6.0$.\nFor $t=1$: $\\delta_1^{\\text{improper}} = \\frac{\\partial L_{\\text{improper}}}{\\partial y_1} c + \\delta_2^{\\text{improper}} a = (1.4)(1) + (6.0)(0.5) = 1.4 + 3.0 = 4.4$.\n\nFinally, we compute the total gradient for $a$:\n$$\n\\frac{\\partial L_{\\text{improper}}}{\\partial a} = \\sum_{t=1}^{3} \\delta_t^{\\text{improper}} h_{t-1} = \\delta_1^{\\text{improper}}h_0 + \\delta_2^{\\text{improper}}h_1 + \\delta_3^{\\text{improper}}h_2\n$$\n$$\n\\frac{\\partial L_{\\text{improper}}}{\\partial a} = (4.4)(0) + (6.0)(2) + (0.4)(4) = 0 + 12.0 + 1.6 = 13.6\n$$\n\n**Step 3: Compute the Discrepancy**\nThe discrepancy $\\Delta$ is the difference between the two computed gradients.\n$$\n\\Delta = \\frac{\\partial L_{\\text{improper}}}{\\partial a} - \\frac{\\partial L_{\\text{proper}}}{\\partial a} = 13.6 - 11.6 = 2\n$$\nThe leakage of gradient from the unmasked regularization at $t=3$ propagates back through the network, altering the gradients at all preceding timesteps and ultimately resulting in a discrepancy of $2$ for the parameter $a$.",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}