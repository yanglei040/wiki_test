{
    "hands_on_practices": [
        {
            "introduction": "要想真正领会“通过时间反向传播”（BPTT）算法的精妙之处，首先必须理解它所要解决的问题。本练习要求你从第一性原理出发，为线性循环神经网络（RNN）推导梯度，而不依赖于 BPTT 算法本身。通过显式地将网络在时间上展开，你将揭示 RNN 梯度的基本结构，并体会其带来的计算挑战。",
            "id": "3192146",
            "problem": "考虑一个线性循环神经网络（RNN），其隐藏状态维度为 $d_h$，输入维度为 $d_x$，输出维度为 $d_y$，由一个长度为 $T$ 的单一序列驱动。其循环和读出由以下核心定义给出\n$h_t = W_h h_{t-1} + W_x x_t$，其中 $h_0 = 0$；以及 $y_t = W_y h_t$，\n对于 $t \\in \\{1,2,\\dots,T\\}$，其中 $W_h \\in \\mathbb{R}^{d_h \\times d_h}$，$W_x \\in \\mathbb{R}^{d_h \\times d_x}$，$W_y \\in \\mathbb{R}^{d_y \\times d_h}$。输入向量为 $\\{x_t \\in \\mathbb{R}^{d_x}\\}_{t=1}^T$，目标为 $\\{s_t \\in \\mathbb{R}^{d_y}\\}_{t=1}^T$。考虑二次损失\n$L = \\frac{1}{2} \\sum_{t=1}^T \\|y_t - s_t\\|_2^2$。\n令每个时刻 $t$ 的输出残差为 $e_t = y_t - s_t$。仅使用循环定义、读出定义、多元微积分中的链式法则以及标准的线性代数知识，推导梯度 $\\nabla_{W_h} L$ 的一个闭式解析表达式，该表达式需明确地将其分解为形如 $\\sum e_{\\cdot}\\, h_{\\cdot}^{\\top}$ 的时移相关项之和。然后，简要讨论在以下两种情况下评估此梯度的计算复杂性（使用大$O$表示法）：(i) 通过对时移项求和，直接根据你推导的闭式表达式实现；(ii) 使用伴随信号的一阶递归，通过时间反向传播（BPTT；Backpropagation Through Time）实现。你的最终答案必须是仅用 $W_h$、$W_y$、$\\{e_t\\}$ 和 $\\{h_t\\}$ 表示的 $\\nabla_{W_h} L$ 的单个闭式解析矩阵表达式。不应包含任何数值计算。",
            "solution": "该问题被评估为有效。这是一个来自深度学习理论领域的适定、有科学依据且客观的问题陈述。它要求推导一个标准线性循环神经网络（RNN）的梯度，这是一个基于多元微积分和线性代数的既定原则的形式化且可解的任务。所有必要的定义和条件均已提供。\n\n我们首先陈述问题中提供的核心定义：\n循环关系：$h_t = W_h h_{t-1} + W_x x_t$，对于 $t \\in \\{1, 2, \\dots, T\\}$，初始状态 $h_0 = 0$。\n读出方程：$y_t = W_y h_t$。\n损失函数：$L = \\frac{1}{2} \\sum_{t=1}^T \\|y_t - s_t\\|_2^2 = \\frac{1}{2} \\sum_{t=1}^T \\|e_t\\|_2^2$，其中 $e_t = y_t - s_t$。\n参数为矩阵 $W_h \\in \\mathbb{R}^{d_h \\times d_h}$，$W_x \\in \\mathbb{R}^{d_h \\times d_x}$ 和 $W_y \\in \\mathbb{R}^{d_y \\times d_h}$。输入为向量 $x_t \\in \\mathbb{R}^{d_x}$，目标为向量 $s_t \\in \\mathbb{R}^{d_y}$。\n\n我们的目标是推导损失函数 $L$ 相对于循环权重矩阵 $W_h$ 的梯度（记为 $\\nabla_{W_h} L$）的闭式表达式。我们将使用矩阵微积分的链式法则。\n\n损失 $L$ 是通过 $W_h$ 对隐藏状态 $\\{h_t\\}_{t=1}^T$ 的影响而成为 $W_h$ 的函数。梯度可以表示为在前向传播中每个使用 $W_h$ 的时间步的贡献之和：\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\nabla_{W_h}^{(\\text{local at } t)} L\n$$\n其中“局部”梯度指的是在计算 $h_t = W_h h_{t-1} + W_x x_t$ 中使用的 $W_h$ 实例的梯度。使用链式法则，该贡献由损失相对于状态 $h_t$ 的梯度与状态更新项相对于 $W_h$ 的梯度的外积给出。令 $\\delta_t = \\frac{\\partial L}{\\partial h_t} \\in \\mathbb{R}^{d_h}$ 为总损失相对于隐藏状态 $h_t$ 的梯度。预激活 $a_t = W_h h_{t-1} + W_x x_t$ 相对于 $W_h$ 的梯度是 $h_{t-1}^T$。因此，来自时间步 $t$ 的梯度贡献是 $\\delta_t h_{t-1}^T$。对所有时间步求和得到总梯度：\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\delta_t h_{t-1}^T\n$$\n这是 BPTT 中循环权重梯度的标准公式。然而，问题要求一个用给定数量 $W_h, W_y, \\{e_t\\}, \\{h_t\\}$ 表示的闭式表达式。为此，我们必须推导出 $\\delta_t$ 的闭式形式。\n\n向量 $\\delta_t$ 表示 $h_t$ 对损失 $L$ 的总影响。状态 $h_t$ 通过所有后续时间步 $k \\ge t$ 的输出 $y_k$ 来影响损失。\n$$\n\\delta_t = \\frac{\\partial L}{\\partial h_t} = \\sum_{k=t}^{T} \\frac{\\partial L}{\\partial y_k} \\frac{\\partial y_k}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_t}\n$$\n这里，我们应用链式法则，对从 $h_t$ 到最终损失的所有路径求和。偏导数如下（使用分子布局的矩阵微积分约定，其中标量对列向量的梯度是一个列向量）：\n1. $\\frac{\\partial L}{\\partial y_k} = \\frac{\\partial}{\\partial y_k} \\left( \\frac{1}{2} \\sum_{j=1}^T \\|y_j - s_j\\|_2^2 \\right) = y_k - s_k = e_k$。这是一个大小为 $d_y$ 的列向量。\n2. $\\frac{\\partial y_k}{\\partial h_k} = W_y$。这是一个大小为 $d_y \\times d_h$ 的雅可比矩阵。链式法则中的项是其转置，即 $(\\frac{\\partial y_k}{\\partial h_k})^T = W_y^T$。\n3. $\\frac{\\partial h_k}{\\partial h_t}$。我们通过展开循环关系来求得此项：\n$h_k = W_h h_{k-1} + W_x x_k = W_h (W_h h_{k-2} + W_x x_{k-1}) + W_x x_k = \\dots$\n对于 $k > t$，$h_k$ 对 $h_t$ 的依赖关系由下式给出：\n$h_k = W_h^{k-t} h_t + \\sum_{j=t+1}^{k} W_h^{k-j} W_x x_j$。\n因此，雅可比矩阵为 $\\frac{\\partial h_k}{\\partial h_t} = W_h^{k-t}$ (对于 $k \\ge t$)。链式法则组合中的项是其转置，即 $(W_h^{k-t})^T = (W_h^T)^{k-t}$。\n\n将这些导数代入 $\\delta_t$ 的表达式中：\n$$\n\\delta_t = \\sum_{k=t}^{T} \\left( \\frac{\\partial h_k}{\\partial h_t} \\right)^T \\left( \\frac{\\partial y_k}{\\partial h_k} \\right)^T \\frac{\\partial L}{\\partial y_k} = \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k\n$$\n这就为从所有未来时间步 $k \\ge t$ 反向传播回来的梯度信号 $\\delta_t$ 提供了闭式表达式。\n\n最后，我们将这个 $\\delta_t$ 的表达式代入总梯度 $\\nabla_{W_h} L$ 的方程中：\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\left( \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k \\right) h_{t-1}^T\n$$\n这就是所要求的闭式解析表达式。它将梯度分解为关于时间的双重求和。每一项 $(W_h^T)^{k-t} W_y^T e_k h_{t-1}^T$ 都可以被解释为未来时间 $k$ 的误差 $e_k$ 与过去时间 $t-1$ 的隐藏状态 $h_{t-1}$ 之间的“相关性”，该相关性由矩阵 $(W_h^T)^{k-t} W_y^T$ 介导，该矩阵解释了影响通过网络动态和读出层的传播。\n\n接下来，我们讨论计算复杂性。\n(i) 直接从闭式表达式求值：\n公式为 $\\sum_{t=1}^{T} \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k h_{t-1}^T$。\n假设前向传播已经运行，因此所有 $e_t \\in \\mathbb{R}^{d_y}$ 和 $h_t \\in \\mathbb{R}^{d_h}$ 都是已知的。\n双重循环涉及 $O(T^2)$ 个项。对于每一项 $(t, k)$，计算涉及：\n- 矩阵幂 $(W_h^T)^{k-t}$，如果未预计算，则需要 $O((k-t) d_h^3)$。预计算所有需要的 $W_h^T$ 的幂（最高到 $T-1$ 次）需要 $O(T d_h^3)$。\n- 矩阵-向量乘积 $W_y^T e_k$，成本为 $O(d_h d_y)$。\n- 矩阵-向量乘积 $(W_h^T)^{k-t} (W_y^T e_k)$，成本为 $O(d_h^2)$。\n- 与 $h_{t-1}^T$ 的外积，成本为 $O(d_h^2)$。\n朴素实现会非常昂贵。一个更优化的直接求值方法如下：\n`total_grad = 0`\n`for t = 1 to T:`\n  `inner_sum_vec = 0`\n  `for k = t to T:`\n    `vec = ... compute (W_h^T)^{k-t} W_y^T e_k ...`\n    `inner_sum_vec += vec`\n  `total_grad += outer(inner_sum_vec, h_{t-1})`\n最昂贵的部分是双重循环结构。大约有 $T^2/2$ 对 $(t,k)$。对于每一对，假设矩阵幂已预计算，则主要成本是与矩阵幂的矩阵-向量乘积，为 $O(d_h^2)$。预计算成本为 $O(T d_h^3)$。因此，总复杂度主要由嵌套循环和预计算决定，导致复杂度为 $O(T d_h^3 + T^2 d_h^2)$。对序列长度 $T$ 的二次依赖性使得该方法对于长序列在计算上是不可行的。\n\n(ii) 通过时间反向传播 (BPTT) 求值：\nBPTT 通过使用动态规划避免了对双重求和的显式计算。它通过一次时间上的反向传播来计算梯度。关键是 $\\delta_t$ 的递归关系：\n$\\delta_T = W_y^T e_T$\n$\\delta_t = W_y^T e_t + W_h^T \\delta_{t+1}$，对于 $t = T-1, \\dots, 1$。\n计算 $\\nabla_{W_h} L$ 的 BPTT 算法如下：\n1. 从 $t=1$到 $T$ 执行前向传播，计算并存储所有的 $h_t$ 和 $e_t$。成本：$O(T(d_h^2 + d_h d_x + d_y d_h))$。\n2. 初始化 $\\nabla_{W_h} L = 0$ 和 $\\delta_{T+1} = 0$。\n3. 从 $t=T$ 向下到 $1$ 执行反向传播：\n   a. 计算 $\\delta_t = W_y^T e_t + W_h^T \\delta_{t+1}$。这涉及两个矩阵-向量乘积和一个向量加法，成本为 $O(d_h d_y + d_h^2)$。\n   b. 将贡献加到梯度上：$\\nabla_{W_h} L \\leftarrow \\nabla_{W_h} L + \\delta_t h_{t-1}^T$。这是一个外积和矩阵加法，成本为 $O(d_h^2)$。\n反向传播包含一个长度为 $T$ 的单循环，每一步的成本为 $O(d_h^2 + d_h d_y)$。因此，反向传播的总复杂度为 $O(T(d_h^2 + d_h d_y))$。\nBPTT 的总复杂度因此为 $O(T(d_h^2 + d_h d_x + d_y d_h))$，这与序列长度 $T$ 呈线性关系。这比直接求值闭式表达式要显著更高效。",
            "answer": "$$\n\\boxed{\\sum_{t=1}^{T} \\left( \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k \\right) h_{t-1}^T}\n$$"
        },
        {
            "introduction": "在理解了 RNN 梯度的基本结构之后，我们将探讨一个关键挑战：它的稳定性。本练习通过一个简化的标量模型，让你清晰地从数学上分析损失函数的布局如何影响梯度传播。通过比较分布在所有时间步上的损失与仅在最后一个时间步上的损失，你将直接洞察梯度消失和梯度爆炸问题的内在机理。",
            "id": "3101225",
            "problem": "考虑一个标量线性循环神经网络，其在离散时间索引上由以下递归关系定义：\n$$h_{t+1} = w\\,h_{t}, \\quad t = 0, 1, \\dots, T-1,$$\n初始隐藏状态为 $$h_{0} \\neq 0,$$ 标量参数 $$w \\in \\mathbb{R}$$ 满足 $$0  |w| \\neq 1,$$ 以及一个整数时间域 $$T \\in \\mathbb{N}$$ 且 $$T \\geq 1.$$ 每个时间的输出与隐藏状态相同，即 $$y_{t} = h_{t}.$$ 每个时间步的损失为\n$$\\ell_{t} = \\frac{1}{2}\\,h_{t}^{2}.$$\n考虑两种总体损失的设置：\n- 时间上分布的损失，\n$$L_{\\mathrm{sum}} = \\sum_{t=1}^{T} \\ell_{t},$$\n- 以及仅在最终时间计算的损失，\n$$L_{\\mathrm{final}} = \\ell_{T}.$$\n\n仅使用微积分的链式法则、时间反向传播（BPTT）作为沿展开计算图重复应用链式法则的定义，以及上述模型方程，推导对于任意 $$t \\in \\{0,1,\\dots,T\\}$$ 的 $$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{t}}$$ 和 $$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}}$$ 的显式符号表达式。然后，作为损失设置如何影响梯度传播回序列起点的定量总结，计算比率\n$$R(w,T) \\equiv \\frac{\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{0}}}{\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{0}}}$$\n作为一个关于 $$w$$ 和 $$T$$ 的单一闭式解析表达式。\n\n您的最终答案必须是 $$R(w,T)$$ 的简化闭式表达式。不需要进行数值计算，也不需要四舍五入。不要包含单位，并使用标准的 LaTeX 数学符号表示所有量。",
            "solution": "首先验证问题以确保其是适定的、有科学依据的和客观的。\n\n### 第1步：提取已知条件\n- 递归关系：$h_{t+1} = w h_{t}$，其中 $t = 0, 1, \\dots, T-1$。\n- 初始条件：$h_{0} \\neq 0$。\n- 参数约束：$w \\in \\mathbb{R}$，$0  |w| \\neq 1$。\n- 时间域：$T \\in \\mathbb{N}$，$T \\geq 1$。\n- 输出定义：$y_{t} = h_{t}$。\n- 每个时间步的损失：$\\ell_{t} = \\frac{1}{2} h_{t}^{2}$。\n- 时间上分布的损失：$L_{\\mathrm{sum}} = \\sum_{t=1}^{T} \\ell_{t}$。\n- 仅在最终时间计算的损失：$L_{\\mathrm{final}} = \\ell_{T}$。\n- 需要推导的量：\n    1. 对于 $t \\in \\{0, 1, \\dots, T\\}$，$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{t}}$ 的显式符号表达式。\n    2. 对于 $t \\in \\{0, 1, \\dots, T\\}$，$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}}$ 的显式符号表达式。\n    3. 比率 $R(w,T) \\equiv \\frac{\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{0}}}{\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{0}}}$。\n\n### 第2步：使用提取的已知条件进行验证\n- **有科学依据**：该问题描述了一个简化的线性循环神经网络，并要求使用时间反向传播（BPTT）计算梯度。这是深度学习中的一个标准和基础课题，基于微积分的链式法则。所有定义和关系在数学上都是合理的。\n- **适定性**：所有变量、常数和函数都有明确定义。约束条件 $h_0 \\neq 0$ 和 $0  |w| \\neq 1$ 确保了梯度表达式和最终比率中的分母不为零，从而得到一个唯一且有意义的解。\n- **客观性**：该问题以精确的数学语言陈述，没有任何主观性或歧义。\n\n该问题没有表现出验证标准中列出的任何缺陷。\n\n### 第3步：结论与行动\n该问题是**有效**的。将提供完整解答。\n\n### 解题推导\n\n首先，我们建立隐藏状态 $h_t$ 的闭式表达式。递归关系 $h_{t+1} = w h_t$ 是一个等比数列。从初始状态 $h_0$ 展开得到：\n$$h_t = w^t h_0$$\n该表达式对所有 $t \\in \\{0, 1, \\dots, T\\}$ 均成立。\n\n#### $\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}}$ 的推导\n最终时间损失为 $L_{\\mathrm{final}} = \\ell_T = \\frac{1}{2} h_T^2$。\n损失相对于中间隐藏状态 $h_t$ 的梯度是通过从 $h_t$ 到 $h_T$ 的计算图应用链式法则来找到的。状态 $h_t$ 仅通过其对 $h_T$ 的影响来影响 $L_{\\mathrm{final}}$。\n$$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}} = \\frac{\\partial L_{\\mathrm{final}}}{\\partial h_T} \\frac{\\partial h_T}{\\partial h_{t}}$$\n第一项是损失函数对其直接输入的导数：\n$$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_T} = \\frac{\\partial}{\\partial h_T} \\left( \\frac{1}{2} h_T^2 \\right) = h_T$$\n第二项是未来状态相对于过去状态的导数。使用 $h_T = w^{T-t} h_t$，我们得到：\n$$\\frac{\\partial h_T}{\\partial h_{t}} = \\frac{\\partial}{\\partial h_{t}} (w^{T-t} h_t) = w^{T-t}$$\n结合这些结果，对于任意 $t \\in \\{0, 1, \\dots, T\\}$ 的梯度是：\n$$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}} = h_T w^{T-t}$$\n这是所要求的显式表达式之一。我们也可以通过代入 $h_T = w^T h_0$ 来用 $h_0$ 表示它：\n$$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}} = (w^T h_0) w^{T-t} = h_0 w^{2T-t}$$\n\n#### $\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{t}}$ 的推导\n分布式损失为 $L_{\\mathrm{sum}} = \\sum_{k=1}^{T} \\ell_k = \\sum_{k=1}^{T} \\frac{1}{2} h_k^2$。\n隐藏状态 $h_t$ 影响所有 $k \\ge t$ 的损失 $\\ell_k$。我们必须对从 $h_t$到总损失的所有路径求和。\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_t} = \\frac{\\partial}{\\partial h_t} \\left( \\sum_{k=1}^{T} \\frac{1}{2} h_k^2 \\right)$$\n由于当 $k  t$ 时 $h_k$ 不依赖于 $h_t$，我们可以限制求和范围：\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_t} = \\sum_{k=t}^{T} \\frac{\\partial}{\\partial h_t} \\left( \\frac{1}{2} h_k^2 \\right) = \\sum_{k=t}^{T} \\frac{\\partial(\\frac{1}{2} h_k^2)}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_t} = \\sum_{k=t}^{T} h_k \\frac{\\partial h_k}{\\partial h_t}$$\n注意，这里的求和是从 $k=t$ 开始的，但损失 $L_{\\mathrm{sum}}$ 是从 $t=1$ 定义的。我们必须考虑求和下界的两种情况。\n\n**情况1：$t \\in \\{1, 2, \\dots, T\\}$**\n如上所示的求和是有效的。我们有 $\\frac{\\partial h_k}{\\partial h_t} = w^{k-t}$。\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_t} = \\sum_{k=t}^{T} h_k w^{k-t}$$\n代入 $h_k = w^{k-t} h_t$：\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_t} = \\sum_{k=t}^{T} (w^{k-t} h_t) w^{k-t} = h_t \\sum_{k=t}^{T} w^{2(k-t)}$$\n令 $j = k-t$。该求和成为一个等比级数：\n$$h_t \\sum_{j=0}^{T-t} (w^2)^j = h_t \\left( \\frac{(w^2)^{T-t+1} - 1}{w^2 - 1} \\right) = h_t \\frac{w^{2(T-t+1)} - 1}{w^2 - 1}$$\n所以，对于 $t \\in \\{1, \\dots, T\\}$，显式表达式为：\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{t}} = h_t \\frac{w^{2(T-t+1)} - 1}{w^2 - 1}$$\n\n**情况2：$t = 0$**\n损失求和从 $k=1$ 开始，所以 $h_0$ 对任何 $\\ell_k$ 都没有直接贡献。其影响完全是通过 $k \\ge 1$ 的 $h_k$ 产生的。\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_0} = \\sum_{k=1}^{T} \\frac{\\partial \\ell_k}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_0} = \\sum_{k=1}^{T} h_k w^k$$\n代入 $h_k = w^k h_0$：\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_0} = \\sum_{k=1}^{T} (w^k h_0) w^k = h_0 \\sum_{k=1}^{T} w^{2k}$$\n这是一个首项为 $w^2$，公比为 $w^2$，共有 $T$ 项的等比级数。\n$$\\sum_{k=1}^{T} (w^2)^k = w^2 \\frac{(w^2)^T - 1}{w^2 - 1} = \\frac{w^2(w^{2T} - 1)}{w^2 - 1}$$\n因此，对于 $t=0$，显式表达式为：\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_0} = h_0 \\frac{w^2(w^{2T} - 1)}{w^2 - 1}$$\n\n#### 比率 $R(w,T)$ 的计算\n该比率定义为 $R(w,T) = \\frac{\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{0}}}{\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{0}}}$。\n根据以上推导，我们有：\n- 分子：$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{0}} = h_0 \\frac{w^2(w^{2T} - 1)}{w^2 - 1}$\n- 分母：对于 $t=0$，$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_0} = h_0 w^{2T-0} = h_0 w^{2T}$。\n\n现在我们计算该比率。条件 $h_0 \\neq 0$ 允许约去。\n$$R(w,T) = \\frac{h_0 \\frac{w^2(w^{2T} - 1)}{w^2 - 1}}{h_0 w^{2T}} = \\frac{w^2(w^{2T} - 1)}{(w^2 - 1)w^{2T}}$$\n我们可以简化此表达式。\n$$R(w,T) = \\frac{w^2}{w^{2T}} \\cdot \\frac{w^{2T} - 1}{w^2 - 1} = w^{2-2T} \\frac{w^{2T} - 1}{w^2 - 1}$$\n通过因式分解出 $w$ 的幂，可以得到一种替代的、更对称的形式：\n$$R(w,T) = \\frac{w^{2T} - 1}{w^{2T}} \\cdot \\frac{w^2}{w^2 - 1} = \\left(1 - \\frac{1}{w^{2T}}\\right) \\cdot \\frac{1}{\\frac{w^2-1}{w^2}} = \\frac{1 - w^{-2T}}{1 - w^{-2}}$$\n这是该比率的最终简化闭式表达式。条件 $0  |w| \\neq 1$ 确保了分母 $1 - w^{-2}$ 不为零。",
            "answer": "$$\\boxed{\\frac{1 - w^{-2T}}{1 - w^{-2}}}$$"
        },
        {
            "introduction": "让我们通过一个常见的现实场景——处理不同长度的序列批次——来弥合理论与实践之间的鸿沟。这个问题要求你在序列掩码（一种标准的填充技术）的背景下应用 BPTT。你将分析一个细微但至关重要的实现错误，通过具体的计算来理解不正确的掩码如何导致“梯度泄漏”，从而破坏训练过程。",
            "id": "3101197",
            "problem": "考虑一个在循环神经网络 (RNN) 中的单序列训练样本，该样本使用随时间反向传播 (BPTT) 和序列掩码进行处理，以应对可变长度的输入。其循环是线性的，使用恒等激活函数，由状态更新和输出方程定义\n$$h_t = a\\,h_{t-1} + b\\,x_t,\\quad y_t = c\\,h_t,$$\n其中 $h_t$ 是隐藏状态，$x_t$ 是时间步 $t$ 的输入，$y_t$ 是输出，$a, b, c$ 是标量参数。初始隐藏状态为 $h_0 = 0$。每个时间步的平方误差数据拟合损失与一个输出正则化项相结合，并应用掩码以遵循真实的序列长度：\n$$L_{\\text{proper}} = \\frac{1}{2}\\sum_{t=1}^{T_{\\max}} m_t\\left((y_t - r_t)^2 + \\lambda\\,y_t^2\\right),$$\n其中 $m_t \\in \\{0,1\\}$ 是时间步 $t$ 的掩码，$r_t$ 是时间步 $t$ 的目标值，$\\lambda > 0$ 是正则化强度，$T_{\\max}$ 是用于填充的最大序列长度。\n\n现在假设存在一个实现错误，导致正则化项在所有填充的时间步上被意外计算，而没有使用掩码，从而得到\n$$L_{\\text{improper}} = \\frac{1}{2}\\sum_{t=1}^{T_{\\max}} m_t\\,(y_t - r_t)^2 + \\frac{1}{2}\\lambda\\sum_{t=1}^{T_{\\max}} y_t^2.$$\n\n给定一个填充后的样本，其 $T_{\\max} = 3$，真实长度为 $2$，掩码为 $(m_1, m_2, m_3) = (1, 1, 0)$，输入为 $(x_1, x_2, x_3) = (2, 3, 0)$，目标值为 $(r_1, r_2, r_3) = (1, -1, 0)$，参数为 $a = 0.5, b = 1, c = 1, \\lambda = 0.2, h_0 = 0$。仅以微积分的链式法则为基本依据，并利用上述定义，推导尊重掩码的梯度，并分析不当处理如何导致梯度泄漏到填充时间步 $t=3$。\n\n计算标量差异\n$$\\Delta = \\frac{\\partial L_{\\text{improper}}}{\\partial a} - \\frac{\\partial L_{\\text{proper}}}{\\partial a}$$\n对于这个单一的样本。将你的最终答案表示为一个精确的实数。不需要四舍五入。答案必须是一个单一的实数值。",
            "solution": "该问题要求计算一个简单循环神经网络的差异 $\\Delta = \\frac{\\partial L_{\\text{improper}}}{\\partial a} - \\frac{\\partial L_{\\text{proper}}}{\\partial a}$。这涉及到应用随时间反向传播 (BPTT) 算法，该算法是微积分链式法则在随时间展开的 RNN 计算图上的直接应用。\n\n计算的核心是求取每个损失函数关于参数 $a$ 的梯度。参数 $a$ 用于每个时间步 $t$ 的状态更新方程中：$h_t = a\\,h_{t-1} + b\\,x_t$。\n应用链式法则，损失函数 $L$ 对 $a$ 的全导数是其通过每个隐藏状态 $h_t$ 施加影响的总和：\n$$\n\\frac{\\partial L}{\\partial a} = \\sum_{t=1}^{T_{\\max}} \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial a}\n$$\n在 BPTT 的背景下，$\\frac{\\partial L}{\\partial h_t}$ 是损失对状态 $h_t$ 的总梯度，而 $\\frac{\\partial h_t}{\\partial a}$ 是来自状态更新方程的局部偏导数。这个局部偏导数就是 $h_{t-1}$。\n我们将流回状态 $h_t$ 的梯度定义为 $\\delta_t = \\frac{\\partial L}{\\partial h_t}$。那么 $a$ 的梯度就是随时间的累积：\n$$\n\\frac{\\partial L}{\\partial a} = \\sum_{t=1}^{T_{\\max}} \\delta_t h_{t-1}\n$$\n梯度 $\\delta_t$ 本身是通过一个同样遵循链式法则的递推关系计算的。状态 $h_t$ 通过两种方式影响损失：直接通过输出 $y_t$，以及间接通过下一个状态 $h_{t+1}$。\n$$\n\\delta_t = \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L}{\\partial y_t}\\frac{\\partial y_t}{\\partial h_t} + \\frac{\\partial L}{\\partial h_{t+1}}\\frac{\\partial h_{t+1}}{\\partial h_t}\n$$\n使用给定的模型方程，$y_t = c\\,h_t$ 和 $h_{t+1} = a\\,h_t + b\\,x_{t+1}$，我们求得偏导数 $\\frac{\\partial y_t}{\\partial h_t}=c$ 和 $\\frac{\\partial h_{t+1}}{\\partial h_t}=a$。这给出了状态梯度的 BPTT 递推关系：\n$$\n\\delta_t = \\frac{\\partial L}{\\partial y_t} c + \\delta_{t+1} a\n$$\n这个递推关系在序列末尾以 $\\delta_{T_{\\max}+1}=0$ 初始化，并从 $t=T_{\\max}$ 向后计算到 $t=1$。项 $\\frac{\\partial L}{\\partial y_t}$ 是损失在时间步 $t$ 对输出的局部梯度。\n\n计算过程分为三个主要步骤：\n1.  一次前向传播，计算隐藏状态 $h_t$ 和输出 $y_t$。\n2.  对每个损失函数（$L_{\\text{proper}}$ 和 $L_{\\text{improper}}$）进行一次反向传播，以计算梯度 $\\delta_t$ 并累积关于 $a$ 的总梯度。\n3.  计算最终的差异 $\\Delta$。\n\n**步骤 1：前向传播**\n给定值：$a = 0.5$, $b = 1$, $c = 1$，以及初始状态 $h_0 = 0$。\n输入序列为 $(x_1, x_2, x_3) = (2, 3, 0)$。\n\n对于 $t=1$:\n$h_1 = a\\,h_0 + b\\,x_1 = (0.5)(0) + (1)(2) = 2$\n$y_1 = c\\,h_1 = (1)(2) = 2$\n\n对于 $t=2$:\n$h_2 = a\\,h_1 + b\\,x_2 = (0.5)(2) + (1)(3) = 1 + 3 = 4$\n$y_2 = c\\,h_2 = (1)(4) = 4$\n\n对于 $t=3$:\n$h_3 = a\\,h_2 + b\\,x_3 = (0.5)(4) + (1)(0) = 2 + 0 = 2$\n$y_3 = c\\,h_3 = (1)(2) = 2$\n\n前向传播总结：$h_0=0, h_1=2, h_2=4, h_3=2$ 和 $y_1=2, y_2=4, y_3=2$。\n\n**步骤 2a：$L_{\\text{proper}}$ 的反向传播**\n局部输出梯度为 $\\frac{\\partial L_{\\text{proper}}}{\\partial y_t} = m_t((1+\\lambda)y_t - r_t)$。\n给定：$\\lambda=0.2$，$(m_1, m_2, m_3)=(1, 1, 0)$，$(r_1, r_2, r_3) = (1, -1, 0)$。\n\n对于 $t=1$: $\\frac{\\partial L_{\\text{proper}}}{\\partial y_1} = 1 \\times ((1+0.2)(2) - 1) = 1.2 \\times 2 - 1 = 2.4 - 1 = 1.4$。\n对于 $t=2$: $\\frac{\\partial L_{\\text{proper}}}{\\partial y_2} = 1 \\times ((1+0.2)(4) - (-1)) = 1.2 \\times 4 + 1 = 4.8 + 1 = 5.8$。\n对于 $t=3$: $\\frac{\\partial L_{\\text{proper}}}{\\partial y_3} = 0 \\times ((1+0.2)(2) - 0) = 0$。\n\n现在我们从 $\\delta_4^{\\text{proper}} = 0$ 开始，向后计算 $\\delta_t^{\\text{proper}}$。\n对于 $t=3$: $\\delta_3^{\\text{proper}} = \\frac{\\partial L_{\\text{proper}}}{\\partial y_3} c + \\delta_4^{\\text{proper}} a = (0)(1) + (0)(0.5) = 0$。\n对于 $t=2$: $\\delta_2^{\\text{proper}} = \\frac{\\partial L_{\\text{proper}}}{\\partial y_2} c + \\delta_3^{\\text{proper}} a = (5.8)(1) + (0)(0.5) = 5.8$。\n对于 $t=1$: $\\delta_1^{\\text{proper}} = \\frac{\\partial L_{\\text{proper}}}{\\partial y_1} c + \\delta_2^{\\text{proper}} a = (1.4)(1) + (5.8)(0.5) = 1.4 + 2.9 = 4.3$。\n\n最后，我们计算 $a$ 的总梯度：\n$$\n\\frac{\\partial L_{\\text{proper}}}{\\partial a} = \\sum_{t=1}^{3} \\delta_t^{\\text{proper}} h_{t-1} = \\delta_1^{\\text{proper}}h_0 + \\delta_2^{\\text{proper}}h_1 + \\delta_3^{\\text{proper}}h_2\n$$\n$$\n\\frac{\\partial L_{\\text{proper}}}{\\partial a} = (4.3)(0) + (5.8)(2) + (0)(4) = 0 + 11.6 + 0 = 11.6\n$$\n\n**步骤 2b：$L_{\\text{improper}}$ 的反向传播**\n局部输出梯度为 $\\frac{\\partial L_{\\text{improper}}}{\\partial y_t} = m_t(y_t - r_t) + \\lambda y_t$。\n\n对于 $t=1$: $\\frac{\\partial L_{\\text{improper}}}{\\partial y_1} = 1 \\times (2 - 1) + (0.2)(2) = 1 + 0.4 = 1.4$。\n对于 $t=2$: $\\frac{\\partial L_{\\text{improper}}}{\\partial y_2} = 1 \\times (4 - (-1)) + (0.2)(4) = 5 + 0.8 = 5.8$。\n对于 $t=3$: $\\frac{\\partial L_{\\text{improper}}}{\\partial y_3} = 0 \\times (2 - 0) + (0.2)(2) = 0 + 0.4 = 0.4$。\n\n在 $t=3$ 处的非零梯度是“泄漏”的来源。尽管数据拟合项被掩码掉 ($m_3=0$)，但正则化项没有，这在这个填充的时间步上产生了一个梯度信号。\n\n现在我们从 $\\delta_4^{\\text{improper}} = 0$ 开始，向后计算 $\\delta_t^{\\text{improper}}$。\n对于 $t=3$: $\\delta_3^{\\text{improper}} = \\frac{\\partial L_{\\text{improper}}}{\\partial y_3} c + \\delta_4^{\\text{improper}} a = (0.4)(1) + (0)(0.5) = 0.4$。\n这个在 $h_3$ 处的非零梯度现在将传播到更早的时间步。\n对于 $t=2$: $\\delta_2^{\\text{improper}} = \\frac{\\partial L_{\\text{improper}}}{\\partial y_2} c + \\delta_3^{\\text{improper}} a = (5.8)(1) + (0.4)(0.5) = 5.8 + 0.2 = 6.0$。\n对于 $t=1$: $\\delta_1^{\\text{improper}} = \\frac{\\partial L_{\\text{improper}}}{\\partial y_1} c + \\delta_2^{\\text{improper}} a = (1.4)(1) + (6.0)(0.5) = 1.4 + 3.0 = 4.4$。\n\n最后，我们计算 $a$ 的总梯度：\n$$\n\\frac{\\partial L_{\\text{improper}}}{\\partial a} = \\sum_{t=1}^{3} \\delta_t^{\\text{improper}} h_{t-1} = \\delta_1^{\\text{improper}}h_0 + \\delta_2^{\\text{improper}}h_1 + \\delta_3^{\\text{improper}}h_2\n$$\n$$\n\\frac{\\partial L_{\\text{improper}}}{\\partial a} = (4.4)(0) + (6.0)(2) + (0.4)(4) = 0 + 12.0 + 1.6 = 13.6\n$$\n\n**步骤 3：计算差异**\n差异 $\\Delta$ 是两个计算出的梯度之差。\n$$\n\\Delta = \\frac{\\partial L_{\\text{improper}}}{\\partial a} - \\frac{\\partial L_{\\text{proper}}}{\\partial a} = 13.6 - 11.6 = 2\n$$\n在 $t=3$ 处未被掩码的正则化项所产生的梯度泄漏会反向传播整个网络，改变所有先前时间步的梯度，最终导致参数 $a$ 的梯度差异为 $2$。",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}