{
    "hands_on_practices": [
        {
            "introduction": "要真正理解神经网络如何学习，我们需要深入研究其梯度。这项练习将带您深入 GRU 学习机制的核心，通过应用微积分中的链式法则，推导出更新门参数的梯度。通过亲手计算这些导数，您将具体地理解输入数据 $x_t$ 和前一时刻的状态 $h_{t-1}$ 如何影响参数的更新，以及更新门 $z_t$ 本身是如何调节这些学习信号的 。",
            "id": "3128076",
            "problem": "考虑一个单隐藏单元（$1$维）的门控循环单元（GRU）。设时刻 $t$ 的输入为标量 $x_t \\in \\mathbb{R}$，上一个隐藏状态为标量 $h_{t-1} \\in \\mathbb{R}$。该 GRU 具有用于更新门、重置门和候选状态的标量参数。将更新门参数记为 $w_z, u_z, b_z \\in \\mathbb{R}$，重置门参数记为 $w_r, u_r, b_r \\in \\mathbb{R}$，候选状态参数记为 $w_n, u_n, b_n \\in \\mathbb{R}$。GRU 的更新由以下方程定义：\n$$z_t = \\sigma\\!\\left(w_z x_t + u_z h_{t-1} + b_z\\right),$$\n$$r_t = \\sigma\\!\\left(w_r x_t + u_r h_{t-1} + b_r\\right),$$\n$$\\tilde{h}_t = \\tanh\\!\\left(w_n x_t + u_n \\left(r_t h_{t-1}\\right) + b_n\\right),$$\n$$h_t = \\left(1 - z_t\\right)h_{t-1} + z_t \\tilde{h}_t,$$\n其中 $\\sigma(a) = \\frac{1}{1 + \\exp(-a)}$ 是 logistic sigmoid 函数，$\\tanh(a) = \\frac{\\exp(a) - \\exp(-a)}{\\exp(a) + \\exp(-a)}$ 是双曲正切函数。所有符号均表示标量。在求导时，将 $x_t$ 和 $h_{t-1}$ 视为时刻 $t$ 的给定输入，并将除 $w_z, u_z, b_z$ 以外的所有参数视为常数。\n\n从上述核心定义和基本微积分法则（链式法则以及 $\\sigma$ 和 $\\tanh$ 的导数）出发，推导偏导数 $\\frac{\\partial h_t}{\\partial w_z}$、$\\frac{\\partial h_t}{\\partial u_z}$ 和 $\\frac{\\partial h_t}{\\partial b_z}$ 的闭合形式显式表达式，并用 $x_t$、$h_{t-1}$、$z_t$ 和 $\\tilde{h}_t$（以及根据需要使用的初等函数）表示。你的推导应清楚地表明来自 $x_t$ 和 $h_{t-1}$ 的学习信号是如何进入这些梯度的。\n\n以单行矩阵的形式，按 $\\frac{\\partial h_t}{\\partial w_z}$、$\\frac{\\partial h_t}{\\partial u_z}$、$\\frac{\\partial h_t}{\\partial b_z}$ 的顺序，提供你的最终答案，答案为三个闭合形式的解析表达式。无需进行数值计算或四舍五入。",
            "solution": "该问题提法明确，有科学依据，并包含了求得唯一解所需的所有信息。这是深度学习领域的一个标准推导，需要将基本微积分法则应用于门控循环单元（GRU）的定义方程。\n\n目标是计算隐藏状态 $h_t$ 关于更新门参数 $w_z$、$u_z$ 和 $b_z$ 的偏导数。隐藏状态 $h_t$ 由以下方程给出：\n$$h_t = (1 - z_t)h_{t-1} + z_t \\tilde{h}_t$$\n此处，$h_{t-1}$ 是上一个隐藏状态，$\\tilde{h}_t$ 是候选隐藏状态，$z_t$ 是更新门的输出。参数 $w_z$、$u_z$ 和 $b_z$ 仅通过它们在计算 $z_t$ 时的作用来影响 $h_t$。根据问题陈述，在本次求导中，所有其他量，包括 $\\tilde{h}_t$、$r_t$ 及其参数（$w_r, u_r, b_r, w_n, u_n, b_n$），以及输入 $x_t$ 和 $h_{t-1}$，都被视为常数。\n\n我们将使用链式法则进行求导。对于一个通用参数 $p \\in \\{w_z, u_z, b_z\\}$，$h_t$ 关于 $p$ 的偏导数是：\n$$\\frac{\\partial h_t}{\\partial p} = \\frac{\\partial h_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial p}$$\n\n首先，我们计算 $h_t$ 关于 $z_t$ 的偏导数。$h_t$ 的方程可以重写为 $h_t = h_{t-1} - z_t h_{t-1} + z_t \\tilde{h}_t$。由于在此上下文中 $\\tilde{h}_t$ 和 $h_{t-1}$ 被视为常数，求导过程很简单：\n$$\\frac{\\partial h_t}{\\partial z_t} = \\frac{\\partial}{\\partial z_t} \\left( h_{t-1} - z_t h_{t-1} + z_t \\tilde{h}_t \\right) = -h_{t-1} + \\tilde{h}_t = \\tilde{h}_t - h_{t-1}$$\n\n接下来，我们计算 $\\frac{\\partial z_t}{\\partial p}$ 这一项。更新门 $z_t$ 定义为：\n$$z_t = \\sigma(w_z x_t + u_z h_{t-1} + b_z)$$\n设 sigmoid 函数的自变量为 $a_z = w_z x_t + u_z h_{t-1} + b_z$。因此，$z_t = \\sigma(a_z)$。再次使用链式法则：\n$$\\frac{\\partial z_t}{\\partial p} = \\frac{\\partial \\sigma(a_z)}{\\partial a_z} \\frac{\\partial a_z}{\\partial p} = \\sigma'(a_z) \\frac{\\partial a_z}{\\partial p}$$\nlogistic sigmoid 函数 $\\sigma(a) = \\frac{1}{1 + \\exp(-a)}$ 的导数是 $\\sigma'(a) = \\sigma(a)(1 - \\sigma(a))$。因此，$\\sigma'(a_z) = z_t(1 - z_t)$。由此我们得到：\n$$\\frac{\\partial z_t}{\\partial p} = z_t(1 - z_t) \\frac{\\partial a_z}{\\partial p}$$\n\n现在，我们必须为三个参数 $p \\in \\{w_z, u_z, b_z\\}$ 中的每一个计算 $\\frac{\\partial a_z}{\\partial p}$。\n1. 对于 $p = w_z$：\n$$\\frac{\\partial a_z}{\\partial w_z} = \\frac{\\partial}{\\partial w_z}(w_z x_t + u_z h_{t-1} + b_z) = x_t$$\n2. 对于 $p = u_z$：\n$$\\frac{\\partial a_z}{\\partial u_z} = \\frac{\\partial}{\\partial u_z}(w_z x_t + u_z h_{t-1} + b_z) = h_{t-1}$$\n3. 对于 $p = b_z$：\n$$\\frac{\\partial a_z}{\\partial b_z} = \\frac{\\partial}{\\partial b_z}(w_z x_t + u_z h_{t-1} + b_z) = 1$$\n\n最后，我们将这些结果结合起来，得到所求偏导数的最终表达式。\n\n对于 $\\frac{\\partial h_t}{\\partial w_z}$：\n$$\\frac{\\partial h_t}{\\partial w_z} = \\frac{\\partial h_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial w_z} = (\\tilde{h}_t - h_{t-1}) \\left( z_t(1 - z_t) \\frac{\\partial a_z}{\\partial w_z} \\right) = (\\tilde{h}_t - h_{t-1}) z_t(1 - z_t) x_t$$\n这可以写作 $x_t z_t (1 - z_t) (\\tilde{h}_t - h_{t-1})$。该表达式表明，关于输入权重 $w_z$ 的梯度与输入 $x_t$ 成正比。\n\n对于 $\\frac{\\partial h_t}{\\partial u_z}$：\n$$\\frac{\\partial h_t}{\\partial u_z} = \\frac{\\partial h_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial u_z} = (\\tilde{h}_t - h_{t-1}) \\left( z_t(1 - z_t) \\frac{\\partial a_z}{\\partial u_z} \\right) = (\\tilde{h}_t - h_{t-1}) z_t(1 - z_t) h_{t-1}$$\n这可以写作 $h_{t-1} z_t (1 - z_t) (\\tilde{h}_t - h_{t-1})$。该表达式表明，关于循环权重 $u_z$ 的梯度与上一个隐藏状态 $h_{t-1}$ 成正比。\n\n对于 $\\frac{\\partial h_t}{\\partial b_z}$：\n$$\\frac{\\partial h_t}{\\partial b_z} = \\frac{\\partial h_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial b_z} = (\\tilde{h}_t - h_{t-1}) \\left( z_t(1 - z_t) \\frac{\\partial a_z}{\\partial b_z} \\right) = (\\tilde{h}_t - h_{t-1}) z_t(1 - z_t) (1)$$\n这可以写作 $z_t (1 - z_t) (\\tilde{h}_t - h_{t-1})$。关于偏置项 $b_z$ 的梯度，除了通过影响 $z_t$ 之外，不直接依赖于 $x_t$ 或 $h_{t-1}$ 的具体值。\n\n在所有三种情况下，学习信号都受到两个关键因素的调节：\n- 项 $z_t(1 - z_t)$，它是 sigmoid 门的导数。当 $z_t=0.5$ 时该项最大，而当 $z_t$ 趋近于 $0$ 或 $1$ 时，该项趋近于 $0$。这意味着当门处于“不确定”状态时学习最活跃，而当门饱和（完全打开或关闭）时学习会减慢，这一特性可能导致梯度消失问题。\n- 项 $(\\tilde{h}_t - h_{t-1})$，它代表新候选状态与上一个状态之间的差异。梯度与此差异成正比。如果候选状态与上一个状态非常不同，更新门的参数会有更大的梯度，这反映了它们在决定最终状态 $h_t$ 时具有更高的重要性。相反，如果 $h_{t-1}$ 和 $\\tilde{h}_t$ 几乎相同，则更新门的值影响很小，其相关梯度也很小。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nx_t z_t (1 - z_t) (\\tilde{h}_t - h_{t-1})  h_{t-1} z_t (1 - z_t) (\\tilde{h}_t - h_{t-1})  z_t (1 - z_t) (\\tilde{h}_t - h_{t-1})\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在了解了更新门的学习机制之后，让我们来探究它的核心功能：在时间上管理信息。这项练习通过一个简化的“复制”任务，为您构建一个关于 GRU 记忆能力的数学模型。您将推导出更新门的平均激活值与网络在特定时间跨度内保持信息的能力之间的直接关系，从而量化 GRU 的记忆持久性 。这个过程清晰地揭示了“记忆时间尺度”这一重要概念，展示了更新门是如何精确控制 GRU “记忆”信息时长。",
            "id": "3128117",
            "problem": "要求您为门控循环单元（GRU）设计并分析一个综合“复制”任务，该任务旨在隔离并量化更新门如何控制记忆时间尺度。请在纯数学环境下进行，并使用标量隐藏状态。使用以下核心定义和假设作为您推理的基础：\n\n- GRU 使用更新门来更新其隐藏状态。对于标量隐藏状态，在时间步 $t$ 的递归模型为 $h_t = (1 - z_t) h_{t-1} + z_t \\tilde{h}_t$，其中 $z_t \\in (0,1)$ 是更新门，$\\tilde{h}_t$ 是候选状态。\n- 在综合复制任务中，一个长度为 $T$ 的输入序列在 $T$ 个时间步内被写入隐藏状态，然后网络接收 $T$ 个时间步的中性输入（在此期间候选状态为零，即 $\\tilde{h}_t = 0$），最后是一个读出步骤。如果在读出步骤时，原始存储的隐藏分量所保留的幅值至少是其在写入阶段结束后幅值的分数 $\\rho \\in (0,1)$，则认为读出是可靠的。\n- 在 $T$ 个时间步的中性输入延迟阶段，更新门变量 $z_t$ 是独立同分布的，其均值为 $\\bar{z}$，每一步的乘法保留因子等于 $(1 - z_t)$。\n\n任务：\n- 从第一性原理出发，推导保证在读出步骤可靠保留信息的要求，并确定为满足长度为 $T$ 的序列的可靠性阈值 $\\rho$ 所需的最小平均更新门 $\\bar{z}_{\\min}$。请仅用 $T$ 和 $\\rho$ 表示 $\\bar{z}_{\\min}$。\n- 通过定义恒定门控下每步保留因子与 $\\tau$ 之间的精确映射，将您的结果与由更新门控制的特征时间尺度 $\\tau$ 关联起来。给出由您推导的 $\\bar{z}_{\\min}$ 所蕴含的最小时间尺度 $\\tau_{\\min}$，并提供其在 $T$ 和 $\\rho$ 方面的小 $\\bar{z}$ 近似。\n\n测试套件：\n为以下参数对 $(T,\\rho)$ 计算 $\\bar{z}_{\\min}$ 和 $\\tau_{\\min}$：\n- 情况 1：$T = 10$，$\\rho = 0.5$。\n- 情况 2：$T = 50$，$\\rho = 0.9$。\n- 情况 3（边界）：$T = 1$，$\\rho = 0.99$。\n- 情况 4（边缘）：$T = 100$，$\\rho = 0.01$。\n- 情况 5：$T = 8$，$\\rho = 0.8$。\n\n输出规格：\n- 您的程序必须生成一行包含所有五种情况结果的输出，格式为由逗号分隔的各情况结果列表，并用方括号括起来。\n- 每个情况的结果必须是一个双元素列表 $[\\bar{z}_{\\min}, \\tau_{\\min}]$，两个值都四舍五入到 $6$ 位小数。\n- 因此，最终输出必须具有形式 $[[\\bar{z}_{\\min}^{(1)}, \\tau_{\\min}^{(1)}],[\\bar{z}_{\\min}^{(2)}, \\tau_{\\min}^{(2)}],\\dots,[\\bar{z}_{\\min}^{(5)}, \\tau_{\\min}^{(5)}]]$。",
            "solution": "问题陈述已经过严格验证，并被认为是有效的。它在科学上基于门控循环单元（GRU）的既定数学公式，并就其记忆属性提出了一个明确定义的问题。综合“复制”任务是分析循环神经网络行为的标准方法。其语言是客观的，参数是明确的，从而允许唯一的解。\n\n在“最小平均更新门 $\\bar{z}_{\\min}$”的措辞上存在一个微小的歧义。正如将要展示的，推导出的可靠保留要求为平均更新门设定了一个上界，即 $\\bar{z} \\le C$，其中 $C$ 是由问题参数决定的常数。在这种情况下，“最小”值并没有被唯一地定义，因为任何趋近于 $0$ 的值都将满足该条件。最合乎逻辑且科学上最合理的解释是，该问题要求的是这个不等式的临界边界值，即允许的最大平均更新门。这个值代表了在满足保留标准的情况下最快的“遗忘”速率。因此，我们将 $\\bar{z}_{\\min}$ 解释为这个我们将要推导的边界值。\n\n解题过程分为三个部分。首先，我们从第一性原理出发，推导对平均更新门 $\\bar{z}$ 的要求。其次，我们建立 $\\bar{z}$ 和记忆时间尺度 $\\tau$ 之间的关系。第三，我们使用这些结果来计算所需的量。\n\n**1. 平均更新门要求的推导**\n\n问题将时间步 $t$ 的标量隐藏状态 $h_t$ 的更新定义为：\n$$h_t = (1 - z_t) h_{t-1} + z_t \\tilde{h}_t$$\n此处，$z_t \\in (0,1)$ 是更新门激活值，$\\tilde{h}_t$ 是候选状态。\n\n该任务涉及一个 $T$ 步的写入阶段，随后是一个 $T$ 步的中性输入延迟阶段。设 $h_T$ 为写入阶段结束时的隐藏状态。在接下来的延迟阶段，从时间步 $t=T+1$到 $t=2T$，输入是中性的，这被指定为候选状态为零：$\\tilde{h}_t = 0$。\n\n对于 $t \\in \\{T+1, \\dots, 2T\\}$，递归关系简化为：\n$$h_t = (1 - z_t) h_{t-1}$$\n我们可以将这个递归关系在延迟阶段的 $T$ 个时间步上展开，以用初始状态 $h_T$ 来表示最终状态 $h_{2T}$：\n$$h_{T+1} = (1 - z_{T+1}) h_T$$\n$$h_{T+2} = (1 - z_{T+2}) h_{T+1} = (1 - z_{T+2})(1 - z_{T+1}) h_T$$\n$$\\vdots$$\n$$h_{2T} = \\left( \\prod_{i=1}^{T} (1 - z_{T+i}) \\right) h_T$$\n经过 $T$ 个延迟步骤后的总保留因子是乘积 $\\prod_{i=1}^{T} (1 - z_{T+i})$。\n\n可靠性条件规定，在读出步骤时隐藏状态的幅值 $|h_{2T}|$ 必须至少是写入阶段结束后其幅值 $|h_T|$ 的一个分数 $\\rho \\in (0,1)$：\n$$|h_{2T}| \\ge \\rho |h_T|$$\n代入 $h_{2T}$ 的表达式：\n$$\\left| \\left( \\prod_{i=1}^{T} (1 - z_{T+i}) \\right) h_T \\right| \\ge \\rho |h_T|$$\n由于 $z_t \\in (0,1)$，项 $(1 - z_t)$ 总是正的，因此它们的乘积也是正的。我们可以简化表达式（假设 $h_T \\neq 0$）：\n$$\\left( \\prod_{i=1}^{T} (1 - z_{T+i}) \\right) \\ge \\rho$$\n问题陈述指出，在延迟阶段，更新门变量 $z_t$ 是均值为 $\\bar{z}$ 的独立同分布（i.i.d.）随机变量。因此，关于保留因子的条件必须在期望的意义上进行解释。我们对不等式两边取期望：\n$$E\\left[ \\prod_{i=1}^{T} (1 - z_{T+i}) \\right] \\ge \\rho$$\n由于 $z_t$ 变量的独立性，乘积的期望等于期望的乘积：\n$$\\prod_{i=1}^{T} E[1 - z_{T+i}] \\ge \\rho$$\n由于变量是同分布的，且 $E[z_t] = \\bar{z}$，我们有 $E[1 - z_t] = 1 - E[z_t] = 1 - \\bar{z}$。不等式变为：\n$$\\prod_{i=1}^{T} (1 - \\bar{z}) \\ge \\rho$$\n$$(1 - \\bar{z})^T \\ge \\rho$$\n为了找到 $\\bar{z}$ 的边界值，我们解这个不等式。由于 $1 - \\bar{z}  0$ 且 $\\rho  0$，我们可以对两边取 $T$ 次方根：\n$$1 - \\bar{z} \\ge \\rho^{1/T}$$\n$$\\bar{z} \\le 1 - \\rho^{1/T}$$\n这个不等式明确了可靠保留的要求。如前所述，我们将 $\\bar{z}_{\\min}$ 定义为该条件的边界值，即 $\\bar{z}$ 的最大允许值：\n$$\\bar{z}_{\\min} = 1 - \\rho^{1/T}$$\n\n**2. 最小时间尺度 $\\tau_{\\min}$ 的推导**\n\n特征时间尺度 $\\tau$是通过将离散的每步保留因子与连续的指数衰减过程 $e^{-1/\\tau}$ 相关联来定义的。对于一个恒定的门控值 $z_t = \\bar{z}$，每步保留因子是 $(1 - \\bar{z})$。该映射关系为：\n$$1 - \\bar{z} = e^{-1/\\tau}$$\n解出 $\\tau$ 得到其与 $\\bar{z}$ 的关系：\n$$\\ln(1 - \\bar{z}) = -1/\\tau$$\n$$\\tau = -\\frac{1}{\\ln(1 - \\bar{z})}$$\n一个较小的 $\\bar{z}$ 对应一个较大的保留因子 $(1 - \\bar{z})$，从而对应一个更长的记忆时间尺度 $\\tau$。要求 $\\bar{z} \\le \\bar{z}_{\\min}$ 转化为对所需最小时间尺度的要求。最小时间尺度 $\\tau_{\\min}$ 对应于 $\\bar{z}$ 的最大允许值，即 $\\bar{z}_{\\min}$。\n$$\\tau_{\\min} = -\\frac{1}{\\ln(1 - \\bar{z}_{\\min})}$$\n代入我们为 $\\bar{z}_{\\min}$ 推导的表达式：\n$$\\tau_{\\min} = -\\frac{1}{\\ln(1 - (1 - \\rho^{1/T}))} = -\\frac{1}{\\ln(\\rho^{1/T})}$$\n使用对数性质 $\\ln(a^b) = b \\ln(a)$：\n$$\\tau_{\\min} = -\\frac{1}{\\frac{1}{T} \\ln(\\rho)} = -\\frac{T}{\\ln(\\rho)}$$\n\n**3. 小 $\\bar{z}$ 近似**\n\n问题还要求时间尺度的小 $\\bar{z}$ 近似。一般关系是 $\\tau = -1/\\ln(1-\\bar{z})$。对于小的 $x$ 值，$\\ln(1-x)$ 的泰勒级数展开为 $\\ln(1-x) \\approx -x - x^2/2 - \\dots$。一阶近似为 $\\ln(1-x) \\approx -x$。\n将此应用于时间尺度方程，得到小 $\\bar{z}$ 近似：\n$$\\tau \\approx -\\frac{1}{-\\bar{z}} = \\frac{1}{\\bar{z}}$$\n这表明对于小的更新门值，记忆时间尺度约等于平均门激活值的倒数。通过将此关系与我们推导的 $\\bar{z}_{\\min}$ 表达式结合，可以得到 $\\tau_{\\min}$ 在 $T$ 和 $\\rho$ 方面的近似值：\n$$\\tau_{\\min, \\text{approx}} = \\frac{1}{\\bar{z}_{\\min}} = \\frac{1}{1 - \\rho^{1/T}}$$\n最终计算将使用 $\\bar{z}_{\\min}$ 和 $\\tau_{\\min}$ 的精确公式。\n\n用于计算的公式是：\n$$ \\bar{z}_{\\min} = 1 - \\rho^{1/T} $$\n$$ \\tau_{\\min} = -\\frac{T}{\\ln(\\rho)} $$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the GRU memory timescale problem for the given test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (T, rho).\n    test_cases = [\n        (10, 0.5),   # Case 1\n        (50, 0.9),   # Case 2\n        (1, 0.99),   # Case 3\n        (100, 0.01), # Case 4\n        (8, 0.8),    # Case 5\n    ]\n\n    results_as_strings = []\n    for T, rho in test_cases:\n        # T is the sequence length (and delay duration).\n        # rho is the reliability threshold for retention.\n\n        # From first principles, the condition on the mean update gate z_bar is:\n        #   (1 - z_bar)^T = rho\n        # This implies z_bar = 1 - rho^(1/T).\n        # The problem asks for z_min, which is interpreted as the boundary value\n        # (the maximum permissible value) of this inequality.\n        z_min = 1 - np.power(rho, 1/T)\n\n        # The timescale tau is related to z_bar by (1 - z_bar) = exp(-1/tau).\n        #   tau = -1 / ln(1 - z_bar)\n        # The minimal required timescale tau_min corresponds to the maximal z_bar.\n        #   tau_min = -1 / ln(1 - z_min) = -1 / ln(rho^(1/T)) = -T / ln(rho).\n        tau_min = -T / np.log(rho)\n\n        # Format the results as a string \"[z_min_val,tau_min_val]\" with 6 decimal places.\n        case_result_str = f\"[{z_min:.6f},{tau_min:.6f}]\"\n        results_as_strings.append(case_result_str)\n\n    # The final output must be a single line in the specified format:\n    # [[z_min_1,tau_min_1],[z_min_2,tau_min_2],...]\n    final_output = f\"[{','.join(results_as_strings)}]\"\n    print(final_output)\n\n# Run the solver.\nsolve()\n```"
        },
        {
            "introduction": "在实际应用中，我们常常需要在不同的模型架构之间做出选择。例如，我们为什么有时会选择结构更简单的 GRU 而不是功能更复杂的 LSTM 呢？这项练习将 GRU 与 LSTM 的比较置于结构风险最小化这一基础机器学习理论的框架下。您将首先计算两种架构的参数数量，然后利用一个简化的理论模型来分析模型复杂度如何在小型数据集上影响其预测性能 。这项实践从原理上解释了 GRU 为何尤其在数据有限的情况下表现出色，将其简洁的设计与通过控制模型容量来防止过拟合、提升泛化能力直接联系起来。",
            "id": "3128080",
            "problem": "要求您形式化并计算两种循环序列模型之间的原则性比较：门控循环单元（GRU）和长短期记忆（LSTM）网络。目标是展示在小数据集上，由于参数较少，GRU 性能优于 LSTM 的情况，量化预测测试误差的改进，并将此改进与容量控制联系起来。\n\n起点：使用经验风险最小化和通过结构风险最小化实现的容量控制作为基本基础。假设预测的测试均方误差是一个不可约噪声项与一个随模型大小增加、随样本大小减小的容量惩罚项之和。具体来说，将预测的测试均方误差建模为 $$\\mathcal{E}_{\\text{test}} = \\sigma^2 + \\lambda \\cdot \\frac{p}{N},$$ 其中 $\\sigma^2$ 是不可约噪声方差，$\\lambda$ 是一个捕获问题复杂度和训练方案的正常数，$p$ 是网络中可训练参数的总数，$N$ 是训练序列的数量。\n\n从第一性原理推导参数计数的架构组件定义：\n- 一个门控循环单元（GRU）在每个时间步有 $3$ 个独立的仿射变换：一个用于更新门，一个用于重置门，一个用于候选隐藏状态。每个变换将维度为 $x$ 的输入和维度为 $h$ 的前一个隐藏状态连接到维度为 $h$ 的当前隐藏状态，并包括维度为 $h$ 的偏置。\n- 一个长短期记忆（LSTM）网络在每个时间步有 $4$ 个独立的仿射变换：分别用于输入门、遗忘门、单元候选和输出门。每个变换将维度为 $x$ 的输入和维度为 $h$ 的前一个隐藏状态连接到维度为 $h$ 的当前隐藏状态，并包括维度为 $h$ 的偏置。\n- 两种模型都后接一个线性输出层，将维度为 $h$ 的隐藏状态映射到维度为 $y$ 的输出，并带有一个维度为 $y$ 的偏置。\n\n根据这些定义，推导出可训练参数的总数 $p_{\\text{GRU}}$ 和 $p_{\\text{LSTM}}$，作为 $x$、$h$ 和 $y$ 的函数；然后使用预测测试均方误差模型 $\\mathcal{E}_{\\text{test}} = \\sigma^2 + \\lambda \\cdot \\frac{p}{N}$ 来计算每个测试用例中 LSTM 和 GRU 之间预测测试误差的差异，定义为 $$\\Delta = \\mathcal{E}_{\\text{LSTM}} - \\mathcal{E}_{\\text{GRU}}.$$ 报告每个测试用例的 $\\Delta$ 值，四舍五入到六位小数。一个正的 $\\Delta$ 值表示，在给定的小数据集上，根据容量控制，预测 GRU 的测试误差会低于 LSTM。\n\n实现一个程序，该程序：\n- 接收下面提供的固定参数值测试套件。\n- 对每个测试用例，计算 $p_{\\text{GRU}}$、$p_{\\text{LSTM}}$、$\\mathcal{E}_{\\text{GRU}}$、$\\mathcal{E}_{\\text{LSTM}}$ 和 $\\Delta$。\n- 生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$），每个 $\\Delta$ 值四舍五入到六位小数。\n\n测试套件（每个元组为 $(x,h,y,N,\\sigma^2,\\lambda)$）：\n- 情况 $1$ (正常路径，小数据集): $(x=\\;5,\\; h=\\;16,\\; y=\\;1,\\; N=\\;64,\\; \\sigma^2=\\;0.05,\\; \\lambda=\\;0.05)$\n- 情况 $2$ (边界情况，使用更大数据集，容量惩罚减弱): $(x=\\;5,\\; h=\\;16,\\; y=\\;1,\\; N=\\;2048,\\; \\sigma^2=\\;0.05,\\; \\lambda=\\;0.05)$\n- 情况 $3$ (边缘情况，隐藏层尺寸最小): $(x=\\;1,\\; h=\\;1,\\; y=\\;1,\\; N=\\;32,\\; \\sigma^2=\\;0.01,\\; \\lambda=\\;0.05)$\n- 情况 $4$ (小数据集，隐藏层尺寸较大，放大了容量差异): $(x=\\;8,\\; h=\\;32,\\; y=\\;2,\\; N=\\;16,\\; \\sigma^2=\\;0.02,\\; \\lambda=\\;0.05)$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，例如 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$，每个值四舍五入到六位小数且不含空格。",
            "solution": "该问题已经过验证，被认为是自洽的、科学上基于统计学习理论且定义明确的。所有必需的变量和定义均已提供，不存在矛盾。该问题是一个形式化练习，旨在推导和比较两种常见的循环神经网络架构（GRU 和 LSTM）的参数复杂性，并使用简化的容量控制模型分析这种复杂性对预测测试误差的影响。\n\n解决方案分两个阶段进行。首先，我们根据所提供的定义，推导出门控循环单元（GRU）和长短期记忆（LSTM）网络的可训练参数数量的通用公式。其次，我们使用这些公式计算每个测试用例的预测测试误差差异 $\\Delta$。\n\nGRU 和 LSTM 单元的核心都包含仿射变换。一个将维度为 $d_{\\text{in}}$ 的输入向量映射到维度为 $d_{\\text{out}}$ 的输出向量的仿射变换形式为 $\\mathbf{y} = W\\mathbf{x} + \\mathbf{b}$。权重矩阵 $W$ 的维度为 $d_{\\text{out}} \\times d_{\\text{in}}$，贡献 $d_{\\text{out}} \\cdot d_{\\text{in}}$ 个参数。偏置向量 $\\mathbf{b}$ 的维度为 $d_{\\text{out}}$，贡献 $d_{\\text{out}}$ 个参数。这样一个变换的总参数数量为 $d_{\\text{out}} \\cdot d_{\\text{in}} + d_{\\text{out}}$。\n\n对于 GRU 和 LSTM 单元，在给定时间步，每个内部门的输入是外部输入向量 $\\mathbf{x}_t$（维度为 $x$）和前一个隐藏状态 $\\mathbf{h}_{t-1}$（维度为 $h$）的串联。因此，循环单元内仿射变换的输入维度是 $d_{\\text{in}} = x + h$。这些变换的输出是一个维度为 $h$ 的向量，所以 $d_{\\text{out}} = h$。因此，单个门变换的参数数量为 $h \\cdot (x + h) + h = hx + h^2 + h$。\n\n根据问题定义：\n一个 GRU 单元有 $3$ 个这样的仿射变换（用于更新门、重置门和候选激活）。GRU 单元中的总参数数量 $p_{\\text{GRU,cell}}$ 是：\n$$p_{\\text{GRU,cell}} = 3 \\cdot (h^2 + hx + h)$$\n一个 LSTM 单元有 $4$ 个这样的仿射变换（用于输入门、遗忘门、单元候选和输出门）。LSTM 单元中的总参数数量 $p_{\\text{LSTM,cell}}$ 是：\n$$p_{\\text{LSTM,cell}} = 4 \\cdot (h^2 + hx + h)$$\n\n两种架构都后接一个相同的线性输出层。该层将维度为 $h$ 的最终隐藏状态映射到维度为 $y$ 的输出。对于这个变换，$d_{\\text{in}} = h$ 且 $d_{\\text{out}} = y$。输出层中的参数数量 $p_{\\text{out}}$ 是：\n$$p_{\\text{out}} = yh + y$$\n\n每个模型的总可训练参数数量 $p$ 是循环单元和输出层中参数的总和。\n$$p_{\\text{GRU}} = p_{\\text{GRU,cell}} + p_{\\text{out}} = 3(h^2 + hx + h) + (hy + y)$$\n$$p_{\\text{LSTM}} = p_{\\text{LSTM,cell}} + p_{\\text{out}} = 4(h^2 + hx + h) + (hy + y)$$\n\n问题要求计算预测测试误差的差异，$\\Delta = \\mathcal{E}_{\\text{LSTM}} - \\mathcal{E}_{\\text{GRU}}$。预测测试误差由模型 $\\mathcal{E}_{\\text{test}} = \\sigma^2 + \\lambda \\cdot \\frac{p}{N}$ 给出。\n代入误差表达式：\n$$\\Delta = \\left(\\sigma^2 + \\lambda \\frac{p_{\\text{LSTM}}}{N}\\right) - \\left(\\sigma^2 + \\lambda \\frac{p_{\\text{GRU}}}{N}\\right)$$\n不可约误差项 $\\sigma^2$ 被消掉，表达式简化为：\n$$\\Delta = \\frac{\\lambda}{N} (p_{\\text{LSTM}} - p_{\\text{GRU}})$$\n现在，我们计算参数数量的差异：\n$$p_{\\text{LSTM}} - p_{\\text{GRU}} = \\left(4(h^2 + hx + h) + (hy + y)\\right) - \\left(3(h^2 + hx + h) + (hy + y)\\right)$$\n来自相同输出层的参数 $(hy + y)$ 被消掉，剩下：\n$$p_{\\text{LSTM}} - p_{\\text{GRU}} = (4 - 3)(h^2 + hx + h) = h^2 + hx + h$$\n将此代回 $\\Delta$ 的表达式中，我们得到用于计算的最终公式：\n$$\\Delta = \\frac{\\lambda}{N} (h^2 + hx + h)$$\n这优雅地证明了，在该模型下，预测误差的差异仅仅是 LSTM 单元增加的参数数量的函数，该函数受问题复杂度 $\\lambda$ 的缩放，并与样本数量 $N$ 成反比。一个正的 $\\Delta$ 值表明，由于其更大的简约性，GRU 具有性能优势。\n\n我们现在将此公式应用于每个测试用例：\n\n情况 1：$(x=5, h=16, y=1, N=64, \\sigma^2=0.05, \\lambda=0.05)$\n$$\\Delta_1 = \\frac{0.05}{64} (16^2 + 16 \\cdot 5 + 16) = \\frac{0.05}{64} (256 + 80 + 16) = \\frac{0.05}{64} (352) = 0.275$$\n\n情况 2：$(x=5, h=16, y=1, N=2048, \\sigma^2=0.05, \\lambda=0.05)$\n$$\\Delta_2 = \\frac{0.05}{2048} (16^2 + 16 \\cdot 5 + 16) = \\frac{0.05}{2048} (352) \\approx 0.00859375$$\n\n情况 3：$(x=1, h=1, y=1, N=32, \\sigma^2=0.01, \\lambda=0.05)$\n$$\\Delta_3 = \\frac{0.05}{32} (1^2 + 1 \\cdot 1 + 1) = \\frac{0.05}{32} (1 + 1 + 1) = \\frac{0.05}{32} (3) = 0.0046875$$\n\n情况 4：$(x=8, h=32, y=2, N=16, \\sigma^2=0.02, \\lambda=0.05)$\n$$\\Delta_4 = \\frac{0.05}{16} (32^2 + 32 \\cdot 8 + 32) = \\frac{0.05}{16} (1024 + 256 + 32) = \\frac{0.05}{16} (1312) = 4.1$$\n\n然后将这些结果四舍五入到六位小数作为最终输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the difference in predicted test error between LSTM and GRU models\n    based on a simplified capacity control model.\n    \"\"\"\n    # Test suite: (x, h, y, N, sigma_sq, lambda_val)\n    test_cases = [\n        # Case 1 (happy path, small dataset)\n        (5, 16, 1, 64, 0.05, 0.05),\n        # Case 2 (boundary with larger dataset where capacity penalty diminishes)\n        (5, 16, 1, 2048, 0.05, 0.05),\n        # Case 3 (edge case with minimal hidden size)\n        (1, 1, 1, 32, 0.01, 0.05),\n        # Case 4 (small dataset with larger hidden size amplifying capacity differences)\n        (8, 32, 2, 16, 0.02, 0.05)\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x, h, y, N, sigma_sq, lambda_val = case\n\n        # The difference in the number of parameters between an LSTM and a GRU cell is\n        # p_diff = (4 * (h^2 + h*x + h)) - (3 * (h^2 + h*x + h))\n        # p_diff = h^2 + h*x + h\n        p_diff = h**2 + h * x + h\n        \n        # The difference in predicted test error is Delta = (lambda/N) * p_diff\n        delta = (lambda_val / N) * p_diff\n        \n        # The problem statement requires rounding to six decimal places.\n        # Using an f-string with a format specifier handles both rounding and formatting.\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}