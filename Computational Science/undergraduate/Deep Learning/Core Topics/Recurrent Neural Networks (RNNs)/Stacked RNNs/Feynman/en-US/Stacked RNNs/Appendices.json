{
    "hands_on_practices": [
        {
            "introduction": "Stacked Recurrent Neural Networks (RNNs) build a hierarchy of representations by processing sequences at progressively larger timescales. This exercise provides a concrete and deterministic illustration of this principle by asking you to construct a stacked RNN that emulates a binary-tree reduction algorithm . By implementing a specific \"clocking\" scheme, you will directly map the network's layers to the levels of a computational tree, gaining a clear understanding of how network depth relates to algorithmic depth. This practice sharpens your ability to reason about the computational roles of different layers and the necessity of non-linearities for solving complex tasks like parity.",
            "id": "3176027",
            "problem": "You are asked to formalize and implement a deterministic, clocked, stacked Recurrent Neural Network (RNN) that emulates a binary-tree reduction over a one-dimensional binary input sequence. The model should be used to solve two tasks on sequences whose length is a power of two: a hierarchical parity task and a hierarchical majority task. The implementation must strictly follow the definitions and constraints below, and the final program must produce results for a specified test suite in a single-line, machine-checkable format.\n\nFundamental base and definitions:\n- A Recurrent Neural Network (RNN) is defined by a hidden state update of the form $h_t = \\phi(W_h h_{t-1} + W_x x_t + b)$, where $h_t \\in \\mathbb{R}^d$ is the hidden state at time $t$, $x_t \\in \\mathbb{R}^m$ is the input at time $t$, $W_h \\in \\mathbb{R}^{d \\times d}$ and $W_x \\in \\mathbb{R}^{d \\times m}$ are weight matrices, $b \\in \\mathbb{R}^d$ is a bias, and $\\phi$ is a nonlinear activation such as the hyperbolic tangent $\\tanh$.\n- A stacked (deep) RNN is a composition of $L$ recurrent layers, where the output sequence of layer $l-1$ serves as the input sequence for layer $l$ at matching time indices.\n- In this problem, you will implement a deterministic, clocked, binary-tree reduction scheme using a stacked RNN interpretation: for a sequence length $N = 2^L$, layer $l \\in \\{1, 2, \\dots, L\\}$ emits a value only at time indices $t$ such that $t \\bmod 2^l = 2^l - 1$ (end of block), combining two values from layer $l-1$ at times $t - 2^{l-1}$ and $t$ via a fixed, known pairwise cell. For non-emission times, the layer emits a neutral placeholder $0$ that is not further used. This clocking realizes a binary-tree circuit of depth $L$, making explicit the relationship between layer depth and Boolean circuit depth.\n\nInput and coding conventions:\n- The input alphabet is binary $\\{0, 1\\}$. Each bit $b_t \\in \\{0,1\\}$ is mapped to a signed value $s_t = 2 b_t - 1 \\in \\{-1, +1\\}$ at the input to layer $0$.\n- For the hierarchical parity task, at each layer the pairwise combine function must approximate the Boolean exclusive-or (XOR) on bits by exactly computing the product $u v$ when $u, v \\in \\{-1, +1\\}$, because $u v = +1$ if and only if the two bits are equal and $u v = -1$ if and only if they are different. You must implement a fixed two-layer $\\tanh$ network that realizes this mapping on the four corner inputs $(u, v) \\in \\{-1, +1\\}^2$:\n  1. Define hidden features using affine half-spaces with an offset that separates the four corners:\n     $$h_1 = \\tanh\\big(\\alpha \\, (u + v - 0.5)\\big), \\quad h_2 = \\tanh\\big(\\alpha \\, (-u - v - 0.5)\\big),$$\n     $$h_3 = \\tanh\\big(\\alpha \\, (u - v - 0.5)\\big), \\quad h_4 = \\tanh\\big(\\alpha \\, (v - u - 0.5)\\big).$$\n  2. Linearly combine these to form a pre-output\n     $$y_{\\text{lin}} = h_1 + h_2 - h_3 - h_4.$$\n  3. Apply an output nonlinearity\n     $$y = \\tanh(\\beta \\, y_{\\text{lin}}).$$\n  For $u, v \\in \\{-1, +1\\}$ and sufficiently large positive gains $\\alpha$ and $\\beta$, this yields $y \\approx \\operatorname{sign}(u v) \\in \\{-1, +1\\}$, exactly matching the required product on the four corners.\n- For the hierarchical majority task, the correct final decision depends on the sign of the sum $S = \\sum_{t=1}^N s_t$, where $s_t \\in \\{-1, +1\\}$. You must use a linear pairwise combine function\n  $$g_{\\text{sum}}(u, v) = u + v,$$\n  so that the layer-$L$ output equals the exact total sum $S$. The final majority decision is 1 if $S > 0$, else 0 (ties map to $0$).\n- Choose and use the fixed constants $\\alpha = 6$ and $\\beta = 3$ in the parity cell.\n\nStacked, clocked reduction:\n- Let $N = 2^L$ for some integer $L \\ge 2$. Let $z^{(0)}_t = 2 b_t - 1$ for $t \\in \\{0, 1, \\dots, N-1\\}$.\n- For layer $l \\in \\{1, \\dots, L\\}$ and for each time $t \\in \\{0, 1, \\dots, N-1\\}$:\n  - If $t \\bmod 2^l = 2^l - 1$, set\n    $$z^{(l)}_t = g\\left(z^{(l-1)}_{t - 2^{l-1}}, \\; z^{(l-1)}_{t}\\right),$$\n    where $g = g_{\\text{parity}}$ for the parity task and $g = g_{\\text{sum}}$ for the majority task.\n  - Else set $z^{(l)}_t = 0$ (unused placeholder).\n- The final scalar output at depth $L$ is the emission at time $t = N - 1$, i.e., $o = z^{(L)}_{N-1}$.\n\nLabel decoding:\n- Parity label: Let $P = \\prod_{t=1}^N (2 b_t - 1) \\in \\{-1, +1\\}$ be the product of signed bits. For even $N$ the XOR parity bit is $1$ if and only if $P = -1$, and $0$ if and only if $P = +1$. Therefore, for the parity task and even $N$, decode the model output $o$ as $\\hat{y}_{\\text{parity}} = 1$ if $o  0$, else $0$.\n- Majority label: Let $S = \\sum_{t=1}^N (2 b_t - 1)$. Decode the model output $o$ as $\\hat{y}_{\\text{majority}} = 1$ if $o > 0$, else $0$.\n\nTest suite:\n- Evaluate four cases covering different depths and tasks:\n  1. Parity on all sequences of length $N = 4$ (i.e., $L = 2$), reporting the accuracy as a float.\n  2. Parity on all sequences of length $N = 8$ (i.e., $L = 3$), reporting the accuracy as a float.\n  3. Majority on all sequences of length $N = 4$, reporting the accuracy as a float.\n  4. Majority on all sequences of length $N = 8$, reporting the accuracy as a float.\n- Each accuracy must be computed over the full set of sequences $\\{0,1\\}^N$, producing a value in $[0,1]$ as a standard fraction of correct predictions (no percentage sign).\n- There are no physical units or angle units involved.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n  $$[\\text{acc\\_parity\\_4}, \\text{acc\\_parity\\_8}, \\text{acc\\_majority\\_4}, \\text{acc\\_majority\\_8}],$$\n  where each accuracy is a float rounded to three decimal places. For example, a valid output line might look like\n  $$[0.875,0.996,1.000,1.000].$$\n\nImplementation constraints:\n- Implement the pairwise parity cell exactly as specified with the fixed constants $\\alpha = 6$ and $\\beta = 3$.\n- Implement the majority cell as the exact linear sum.\n- Implement the clocked, stacked reduction as described, for lengths $N \\in \\{4, 8\\}$.\n- Do not train any parameters; all computations must be deterministic and based on the specified formulas.\n- Enumerate all binary sequences for each $N$ to compute exact accuracies.\n\nYour final submission must be a complete, runnable program that performs these computations and prints the results in the exact required single-line format.",
            "solution": "The user has provided a valid, well-posed problem statement that is grounded in the principles of deep learning and computational neuroscience. The task is to formalize and implement a specific type of stacked Recurrent Neural Network (RNN) that deterministically emulates a binary-tree reduction algorithm. This model is then applied to solve hierarchical parity and majority tasks on binary sequences of length $N=2^L$. The solution requires implementing the specified algorithms and functions, evaluating their performance on all possible input sequences for $N=4$ and $N=8$, and reporting the accuracy.\n\nThe solution is structured as follows: first, the pairwise combination functions for each task are implemented. Second, the main clocked reduction algorithm is constructed. Finally, these components are integrated into an evaluation framework to compute the required accuracies.\n\n### 1. Pairwise Combination Functions\n\nThe core of the binary-tree reduction is a pairwise combination function, $g$, that is applied recursively. The problem specifies two distinct functions, one for each task.\n\n**Majority Task**: The majority task is linearly separable. The goal is to determine if the count of $1$s exceeds the count of $0$s in the input sequence. Using the specified input encoding, where a bit $b_t \\in \\{0, 1\\}$ is mapped to a signed value $s_t=2b_t-1 \\in \\{-1, +1\\}$, this is equivalent to checking the sign of the total sum $S = \\sum_{t=0}^{N-1} s_t$. Due to the associativity of addition, this sum can be computed hierarchically. The specified pairwise function is a simple linear sum:\n$$g_{\\text{sum}}(u, v) = u + v$$\nApplying this function in a binary-tree structure will result in the final output at the root being the exact sum of all leaf (input) values.\n\n**Parity Task**: The parity task (equivalent to XOR for two bits) is a classic non-linear problem. The goal is to determine if the number of $1$s is odd. Using the signed bit encoding, this corresponds to computing the sign of the product $P = \\prod_{t=0}^{N-1} s_t$. The pairwise combination function must therefore approximate the product of its two inputs, $g(u, v) \\approx u \\cdot v$. The problem provides a specific fixed-parameter, two-layer neural network to implement this function, $g_{\\text{parity}}$:\n$$g_{\\text{parity}}(u, v) = \\tanh(\\beta \\, y_{\\text{lin}})$$\nwhere the linear pre-output $y_{\\text{lin}}$ is given by:\n$$y_{\\text{lin}} = h_1 + h_2 - h_3 - h_4$$\nand the hidden features $h_i$ are defined by affine transformations passed through a $\\tanh$ activation function:\n$$h_1 = \\tanh\\big(\\alpha \\, (u + v - 0.5)\\big)$$\n$$h_2 = \\tanh\\big(\\alpha \\, (-u - v - 0.5)\\big)$$\n$$h_3 = \\tanh\\big(\\alpha \\, (u - v - 0.5)\\big)$$\n$$h_4 = \\tanh\\big(\\alpha \\, (v - u - 0.5)\\big)$$\nThe constants are fixed at $\\alpha = 6$ and $\\beta = 3$. The high gain $\\alpha$ ensures that the hidden units operate in their saturation regime, creating sharp decision boundaries in the $(u,v)$ space that separate the four corners $(\\pm 1, \\pm 1)$. The linear combination is weighted to produce a positive $y_{\\text{lin}}$ when $u$ and $v$ have the same sign, and a negative $y_{\\text{lin}}$ when they have opposite signs. The final high gain $\\beta$ sharpens the output, ensuring that for inputs $(u,v) \\in \\{-1, +1\\}^2$, the output $y$ is extremely close to the desired product $uv$. This design is robust, meaning that even if the inputs $u$ and $v$ are not exactly $\\pm 1$ but are close (as may happen in higher layers of the network), the function still computes an output very near the correct signed value.\n\n### 2. Stacked, Clocked Reduction Algorithm\n\nThe binary-tree reduction is implemented using a stacked RNN architecture with a specific \"clocking\" mechanism. The network has $L=\\log_2 N$ layers on top of the input layer (layer $0$).\n\nLet $z^{(l)}_t$ be the activation of the neuron at layer $l$ and time step $t$.\n- **Layer 0**: The input layer simply holds the signed binary sequence: $z^{(0)}_t = 2b_t - 1$ for $t \\in \\{0, 1, \\dots, N-1\\}$.\n- **Layers $l \\in \\{1, \\dots, L\\}$**: An activation $z^{(l)}_t$ is computed only at specific time steps. The condition for computation is $t \\bmod 2^l = 2^l - 1$. This means a neuron at layer $l$ only produces an output at the end of a block of $2^l$ time steps. At these times, it combines two outputs from the preceding layer, $l-1$, which themselves mark the ends of two adjacent sub-blocks of size $2^{l-1}$. The update rule is:\n$$z^{(l)}_t = g\\left(z^{(l-1)}_{t - 2^{l-1}}, \\; z^{(l-1)}_{t}\\right)$$\nFor all other time steps where the condition is not met, the output is a placeholder value, $z^{(l)}_t = 0$. This ensures that the function $g$ is always called with valid, computed inputs from the previous layer.\nThe final output of the entire network is the single value computed at the final layer $L$ at the final time step $N-1$:\n$$o = z^{(L)}_{N-1}$$\nThis procedure exactly mirrors a binary-tree computation where the leaves are the input sequence and each internal node computes the function $g$ on its two children.\n\n### 3. Evaluation and Expected Accuracy\n\nThe performance is evaluated by computing the accuracy over the entire set of $2^N$ possible binary input sequences for $N=4$ and $N=8$.\n\n- **Majority Task**: The model output $o$ is the exact sum $S = \\sum s_t$. The decoding rule is $\\hat{y}_{\\text{majority}} = 1$ if $o > 0$, else $0$. The true label follows the same rule based on $S$. Since the model's computation is exact, the prediction will always match the true label. Therefore, the accuracy is expected to be $1.0$ for both $N=4$ and $N=8$.\n\n- **Parity Task**: The model output $o$ is a high-precision approximation of the true product $P=\\prod s_t$. The decoding rule is $\\hat{y}_{\\text{parity}} = 1$ if $o  0$, else $0$. The true label is $1$ if $P=-1$ and $0$ if $P=+1$. The robust design of the $g_{\\text{parity}}$ function, with its high-gain saturating nonlinearities, ensures that the sign of the output $o$ will correctly match the sign of the true product $P$. Therefore, the predictions are expected to be correct for all inputs, yielding an accuracy of $1.0$ for both $N=4$ and $N=8$.\n\nThe implementation will systematically generate each binary sequence, run the reduction algorithm to get the model output, decode the prediction, compute the true label, and tally the correct predictions to find the final accuracy.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Formalizes and implements a deterministic, clocked, stacked RNN \n    that emulates a binary-tree reduction for parity and majority tasks.\n    \"\"\"\n\n    ALPHA = 6.0\n    BETA = 3.0\n\n    def g_parity(u, v):\n        \"\"\"\n        Pairwise combine function for the parity task, approximating u*v.\n        \"\"\"\n        h1 = np.tanh(ALPHA * (u + v - 0.5))\n        h2 = np.tanh(ALPHA * (-u - v - 0.5))\n        h3 = np.tanh(ALPHA * (u - v - 0.5))\n        h4 = np.tanh(ALPHA * (v - u - 0.5))\n        y_lin = h1 + h2 - h3 - h4\n        return np.tanh(BETA * y_lin)\n\n    def g_sum(u, v):\n        \"\"\"\n        Pairwise combine function for the majority task, computing u+v.\n        \"\"\"\n        return u + v\n\n    def run_reduction(binary_sequence, task):\n        \"\"\"\n        Runs the stacked, clocked reduction algorithm on a binary sequence.\n        \"\"\"\n        N = len(binary_sequence)\n        # Sequence length N must be a power of two.\n        L = int(math.log2(N))\n        \n        # z_history[l] stores the activations for layer l.\n        z_history = [np.zeros(N, dtype=np.float64) for _ in range(L + 1)]\n\n        # Layer 0: Input layer with signed bits.\n        z_history[0] = np.array([2 * b - 1 for b in binary_sequence], dtype=np.float64)\n        \n        g = g_parity if task == 'parity' else g_sum\n        \n        # Layers l = 1 to L\n        for l in range(1, L + 1):\n            block_size = 2**l\n            step_size = 2**(l-1)\n            # Iterate through all time steps t = 0 to N-1.\n            for t in range(N):\n                # An output is emitted only at the end of a block.\n                if (t + 1) % block_size == 0:\n                    input1 = z_history[l-1][t - step_size]\n                    input2 = z_history[l-1][t]\n                    z_history[l][t] = g(input1, input2)\n        \n        # The final output is at layer L, time N-1.\n        return z_history[L][N-1]\n\n    def get_true_label(binary_sequence, task):\n        \"\"\"\n        Computes the ground truth label for a given sequence and task.\n        \"\"\"\n        signed_sequence = np.array([2 * b - 1 for b in binary_sequence], dtype=np.float64)\n        \n        if task == 'parity':\n            # For even N, parity is 1 iff the product of signed bits is -1.\n            product = np.prod(signed_sequence)\n            return 1 if product  0 else 0\n        elif task == 'majority':\n            # Majority is 1 iff the sum of signed bits is  0.\n            total_sum = np.sum(signed_sequence)\n            return 1 if total_sum  0 else 0\n        else:\n            raise ValueError(\"Unknown task\")\n\n    def evaluate_task(N, task):\n        \"\"\"\n        Evaluates the model's accuracy for a given N and task over all 2^N sequences.\n        \"\"\"\n        num_correct = 0\n        num_total = 2**N\n        \n        for i in range(num_total):\n            # Generate the i-th binary sequence of length N.\n            # Bits are ordered from most to least significant.\n            binary_sequence = [(i  j)  1 for j in range(N - 1, -1, -1)]\n            \n            # Get model prediction.\n            model_output = run_reduction(binary_sequence, task)\n            \n            if task == 'parity':\n                # Decode model output for parity.\n                prediction = 1 if model_output  0 else 0\n            else: # task == 'majority'\n                # Decode model output for majority.\n                prediction = 1 if model_output  0 else 0\n            \n            # Get true label.\n            true_label = get_true_label(binary_sequence, task)\n            \n            if prediction == true_label:\n                num_correct += 1\n                \n        return num_correct / num_total\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (4, 'parity'),\n        (8, 'parity'),\n        (4, 'majority'),\n        (8, 'majority')\n    ]\n\n    results = []\n    for N, task in test_cases:\n        accuracy = evaluate_task(N, task)\n        results.append(accuracy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond simple hierarchical feature extraction, stacked RNNs possess the remarkable ability to emulate formal computational machines. This practice challenges you to engineer a two-layer stacked LSTM to recognize the context-free language $L = \\{ a^n b^n \\}$, effectively behaving like a Pushdown Automaton (PDA) . You will derive and implement conditions on the LSTM gates to create a system where one layer detects the phase of the input sequence while the second layer maintains a counter in its cell state, $c_t^{(2)}$. This exercise provides a profound insight into the expressive power of LSTMs, demonstrating how their architectural components can be precisely configured to implement stateful, symbolic algorithms.",
            "id": "3175992",
            "problem": "You are given the task of determining when a Stacked Recurrent Neural Network (RNN) composed of Long Short-Term Memory (LSTM) layers can emulate a Pushdown Automaton (PDA) for recognizing nested structures. Consider the formal language $L = \\{ a^n b^n \\mid n \\in \\mathbb{N}_0 \\}$, where $\\mathbb{N}_0$ denotes the set of nonnegative integers. The alphabet is $\\{a, b\\}$ and the input is a finite sequence of tokens drawn from this alphabet. The fundamental base for this task comprises the following well-tested definitions in deep learning and automata theory.\n\nFor a single LSTM layer at time step $t$, with input vector $x_t$, previous hidden state $h_{t-1}$, and previous cell state $c_{t-1}$, the gates and states are defined by\n$$ i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i), \\quad f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f), \\quad o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o), $$\n$$ \\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c), \\quad c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t, \\quad h_t = o_t \\odot \\tanh(c_t), $$\nwhere $\\sigma$ is the logistic sigmoid function, $\\tanh$ is the hyperbolic tangent function, and $\\odot$ denotes element-wise multiplication. In a stacked architecture with $\\ell$ layers, the input to layer $\\ell$ is $x_t^{(\\ell)} = h_t^{(\\ell-1)}$, with $x_t^{(1)}$ defined by the external input encoding. A Pushdown Automaton (PDA) recognizes $L$ by incrementing a counter when reading $a$ tokens and decrementing when reading $b$ tokens, while enforcing that once any $b$ has been observed, no $a$ may follow, and the counter must never become negative. Acceptance occurs if the counter returns to $0$ at the end of the input.\n\nYour task is to:\n- Derive, from the above definitions, conditions on the gates and internal signals under which a two-layer stacked LSTM can emulate the PDA behavior for $L$. In particular, identify how the first layer can produce an internal signal indicating whether any $b$ has been seen and how the second layer can update a counter in its cell state $c_t^{(2)}$ to perform $+1$ updates for $a$ before the $b$-phase begins, and $-1$ updates for $b$ thereafter. Your derivation must start from the stated LSTM equations and the PDA acceptance semantics and argue principled requirements such as gate saturation to values near $0$ or $1$, monotone updates to the counter dimension, and the separation of roles across layers. Avoid using shortcut formulas that skip these reasoning steps.\n- Construct a self-contained program that simulates a two-layer stacked LSTM with carefully chosen, deterministic gate behaviors consistent with your derived conditions, without training. The first layer should produce outputs that include:\n  1. An indicator-like pass-through of the current token identity (whether the token is $a$ or $b$).\n  2. A monotone internal signal that increases when a $b$ is observed (so that the model can detect whether any $b$ has been seen).\n  The second layer should maintain a signed counter in its cell state $c_t^{(2)}$, incrementing by $+1$ when reading $a$ and no $b$ has yet occurred, and decrementing by $-1$ when reading $b$. Your program must enforce the PDA constraints: once any $b$ has been seen, encountering an $a$ is invalid, and the counter must never go below $0$. The program should accept an input if and only if the final counter $c_T^{(2)}$ is $0$ within a small tolerance $\\epsilon$, no invalid $a$ occurs after a $b$, and no prefix causes the counter to become negative.\n- Use the following one-hot input encoding for tokens: $a \\mapsto [1, 0]$, $b \\mapsto [0, 1]$. Track the evolving cell states $c_t^{(1)}$ for the first layer and $c_t^{(2)}$ for the second layer. Explicitly compute the counter in $c_t^{(2)}$ according to your design, using gate values that saturate toward $0$ or $1$ as needed.\n- Implement and evaluate the following test suite of sequences over $\\{a,b\\}$:\n  1. The empty string $\"\"$.\n  2. $\"ab\"$.\n  3. $\"aaabbb\"$.\n  4. $\"aabbb\"$.\n  5. $\"aaabb\"$.\n  6. $\"aba\"$.\n  7. $\"b\"$.\n  8. $\"aaaabbbb\"$.\n- Define exact acceptance for each sequence using a boolean decision. Use a tolerance $\\epsilon$ with a justified small positive value (for example, $\\epsilon = 10^{-3}$) to compare the final counter to $0$.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite (for example, $[r_1,r_2,\\dots,r_8]$), where each $r_i$ is the boolean acceptance result for test case $i$.\n\nNo physical units or angles are involved; all outputs are boolean values. The program must be completely self-contained and run without user input or external files. The logic must be universally understandable from a mathematical programming perspective and should be implementable in any modern programming language, but you must produce Python code as specified.",
            "solution": "We start from the Long Short-Term Memory (LSTM) definitions. For an LSTM, the cell state update is\n$$ c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t, $$\nwith gate activations\n$$ i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i), \\quad f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f), \\quad o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o), $$\nand candidate\n$$ \\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c). $$\nIn a stacked architecture, the input to layer $\\ell$ is the hidden output of the previous layer:\n$$ x_t^{(\\ell)} = h_t^{(\\ell-1)}, \\quad h_t^{(\\ell)} = o_t^{(\\ell)} \\odot \\tanh(c_t^{(\\ell)}). $$\n\nA Pushdown Automaton (PDA) for the language $L = \\{ a^n b^n \\mid n \\in \\mathbb{N}_0 \\}$ must obey the following semantic constraints:\n- While reading $a$ symbols, it pushes, increasing the stack height by $+1$ per $a$.\n- Upon the first $b$, it transitions to a popping phase and thereafter pops, decreasing the stack height by $-1$ per $b$.\n- No $a$ may occur after any $b$ has been seen.\n- The stack height must never be negative at any prefix.\n- Acceptance requires final stack height to be exactly $0$ at the end of input.\n\nTo emulate these behaviors with a two-layer stacked LSTM, we require the following principled conditions derived from the LSTM equations.\n\nLayer $1$ conditions (feature extraction and phase detection):\n- The layer must output signals that encode the current token identity and whether any $b$ has been encountered so far. Let the cell state $c_t^{(1)} \\in \\mathbb{R}^3$ be structured as follows:\n  - $c_{t,0}^{(1)}$ tracks a pass-through indicator for $a$ at time $t$,\n  - $c_{t,1}^{(1)}$ tracks a pass-through indicator for $b$ at time $t$,\n  - $c_{t,2}^{(1)}$ accumulates the number of $b$ tokens seen so far.\n- To accomplish this, choose gates that saturate to values near $0$ or $1$ using $\\sigma$ so that:\n  - For dimension $0$ (the $a$ indicator), set $f_{t,0}^{(1)} \\approx 0$ to reset at each step, $i_{t,0}^{(1)} \\approx \\mathbb{I}\\{x_t \\text{ is } a\\}$, and $\\tilde{c}_{t,0}^{(1)} \\approx 1$, resulting in $c_{t,0}^{(1)} \\approx \\mathbb{I}\\{x_t \\text{ is } a\\}$. Here $\\mathbb{I}\\{\\cdot\\}$ denotes an indicator function. This is consistent with the LSTM update because $c_{t,0}^{(1)} = f_{t,0}^{(1)} c_{t-1,0}^{(1)} + i_{t,0}^{(1)} \\tilde{c}_{t,0}^{(1)} \\approx 0 + 1 \\cdot 1$ when $x_t$ is $a$, and $\\approx 0$ when $x_t$ is $b$.\n  - For dimension $1$ (the $b$ indicator), set $f_{t,1}^{(1)} \\approx 0$, $i_{t,1}^{(1)} \\approx \\mathbb{I}\\{x_t \\text{ is } b\\}$, and $\\tilde{c}_{t,1}^{(1)} \\approx 1$, yielding $c_{t,1}^{(1)} \\approx \\mathbb{I}\\{x_t \\text{ is } b\\}$.\n  - For dimension $2$ (the $b$-phase accumulator), set $f_{t,2}^{(1)} \\approx 1$ to retain memory, $i_{t,2}^{(1)} \\approx \\mathbb{I}\\{x_t \\text{ is } b\\}$, and $\\tilde{c}_{t,2}^{(1)} \\approx 1$, producing $c_{t,2}^{(1)} \\approx c_{t-1,2}^{(1)} + \\mathbb{I}\\{x_t \\text{ is } b\\}$, i.e., a nondecreasing counter of $b$ observations. With $o_{t}^{(1)} \\approx 1$, we have $h_t^{(1)} \\approx \\tanh(c_t^{(1)})$, whose third component is near $0$ before any $b$ and increases toward $1$ once $b$ symbols are seen.\n- These gate behaviors are realizable via the LSTM equations by choosing $W_i$, $U_i$, and $b_i$ to yield large positive preactivations for desired cases and large negative preactivations otherwise, ensuring saturation, while $W_c$, $U_c$, and $b_c$ set $\\tilde{c}_t$ to approximately constant values for the indicator-like dimensions and to $1$ for the accumulator.\n\nLayer $2$ conditions (counter management emulating stack height):\n- The second layer maintains a scalar counter in $c_t^{(2)} \\in \\mathbb{R}$ representing stack height. It must:\n  - Increment by $+1$ for $a$ when the $b$-phase has not yet started,\n  - Decrement by $-1$ for $b$ at all times,\n  - Remain unchanged for $a$ after the $b$-phase has begun (since this is invalid), while an external error flag is raised to prevent acceptance.\n- Let $\\phi_t \\in \\{0,1\\}$ be a phase indicator derived from the first layer, where $\\phi_t \\approx 0$ if $c_{t,2}^{(1)}$ is near $0$, and $\\phi_t \\approx 1$ once any $b$ has been seen. Then choose gates so that $f_t^{(2)} \\approx 1$, $i_t^{(2)} \\approx 1$, and design $\\tilde{c}_t^{(2)}$ as\n  $$ \\tilde{c}_t^{(2)} \\approx (1 - \\phi_t) \\cdot \\mathbb{I}\\{x_t \\text{ is } a\\} - \\mathbb{I}\\{x_t \\text{ is } b\\}, $$\n  which yields\n  $$ c_t^{(2)} \\approx c_{t-1}^{(2)} + \\tilde{c}_t^{(2)}. $$\n  This is consistent with the LSTM dynamics because $f_t^{(2)} \\approx 1$ preserves the previous counter, while $i_t^{(2)} \\approx 1$ adds the signed update. The use of $\\tanh$ ensures $\\tilde{c}_t^{(2)} \\in [-1,1]$, aligning with increments and decrements by $1$.\n- Error detection conditions:\n  - If $\\phi_t \\approx 1$ and $x_t$ is $a$, mark invalid due to $a$ appearing after $b$.\n  - If $c_t^{(2)}$ ever drops below $0$, mark invalid due to underflow (the PDA would attempt to pop from an empty stack).\n- Acceptance condition:\n  $$ \\text{accept} \\iff \\left(|c_T^{(2)}| \\le \\epsilon\\right) \\land \\left(\\text{no invalid } a \\text{ after } b\\right) \\land \\left(\\text{no underflow occurred}\\right), $$\n  where $\\epsilon$ is a small positive tolerance (e.g., $\\epsilon = 10^{-3}$) that accounts for any minor numerical deviations arising from gate saturations.\n\nAlgorithmic design consistent with these principles:\n- Encode inputs with $a \\mapsto [1,0]$ and $b \\mapsto [0,1]$.\n- Implement the first layer with three cell dimensions, as described, using sigmoids with a large gain to approximate indicators. Use $f_{t,0}^{(1)} = 0$, $i_{t,0}^{(1)} \\approx \\sigma(K(x_{a,t} - 0.5))$, $\\tilde{c}_{t,0}^{(1)} = 1$, similarly for $b$ in dimension $1$, and for dimension $2$ use $f_{t,2}^{(1)} = 1$, $i_{t,2}^{(1)} \\approx \\sigma(K(x_{b,t} - 0.5))$, $\\tilde{c}_{t,2}^{(1)} = 1$, with $K$ large (e.g., $K = 20$).\n- Implement the second layer counter with $f_t^{(2)} = 1$, $i_t^{(2)} = 1$, and $\\tilde{c}_t^{(2)}$ set to $(1 - \\phi_t)\\cdot \\mathbb{I}\\{x_t \\text{ is } a\\} - \\mathbb{I}\\{x_t \\text{ is } b\\}$, where $\\phi_t$ is determined by thresholding $c_{t,2}^{(1)}$ (e.g., $\\phi_t = 1$ if $c_{t,2}^{(1)} > 0.5$, else $0$).\n- Track the counter $c_t^{(2)}$, the underflow condition $c_t^{(2)}  0$, and the invalid $a$-after-$b$ condition.\n\nCorrectness argument:\n- Before any $b$, $\\phi_t \\approx 0$, and $\\tilde{c}_t^{(2)} \\approx \\mathbb{I}\\{x_t \\text{ is } a\\} - 0$, so the counter increases by $+1$ per $a$. After observing a $b$, $\\phi_t \\approx 1$, and $\\tilde{c}_t^{(2)} \\approx 0 - \\mathbb{I}\\{x_t \\text{ is } b\\}$, so it decreases by $-1$ per $b$. This exactly emulates the PDA push/pop counts for $a^n b^n$. Invalid sequences with $a$ after $b$ do not increase the counter and are flagged invalid; sequences with too many $b$ cause the counter to go negative, triggering underflow. Acceptance requires returning the counter to $0$ with no violations, matching the PDA acceptance semantics.\n- The gates at layer $1$ separate roles into indicator pass-through and monotone phase detection, and layer $2$ focuses solely on counter updates, providing the necessary separation of concerns typical of stacked architectures. The use of saturating gates to approximate $0$ and $1$ ensures the updates are robust and consistent across time.\n\nTest suite coverage rationale:\n- The empty string $\"\"$ tests the boundary case $n = 0$.\n- $\"ab\"$ tests the minimal nonempty valid case.\n- $\"aaabbb\"$ and $\"aaaabbbb\"$ test general valid cases with multiple increments and decrements.\n- $\"aabbb\"$ and $\"aaabb\"$ test mismatched counts and underflow versus nonzero final counter.\n- $\"aba\"$ tests the invalid $a$ after $b$ condition.\n- $\"b\"$ tests starting with a pop on an empty stack, causing underflow.\n\nThe final program implements this design and outputs a single line $[r_1,r_2,\\dots,r_8]$ of booleans in the specified order.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigma(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef tanh(z):\n    return np.tanh(z)\n\ndef accept_sequence(seq, K=20.0, eps=1e-3):\n    \"\"\"\n    Simulate a two-layer stacked LSTM designed to emulate a PDA for L = {a^n b^n}.\n    Layer 1 (size 3):\n      - c1[0]: indicator-like pass-through for 'a' at time t\n      - c1[1]: indicator-like pass-through for 'b' at time t\n      - c1[2]: monotone accumulator of whether any 'b' has been seen (counts b's)\n    Layer 2 (size 1):\n      - c2: counter of stack height, +1 for 'a' before any 'b', -1 for 'b'\n    Acceptance: final c2 ~ 0, no 'a' after 'b', and no underflow (c2  0 at any prefix).\n    \"\"\"\n    # Initialize states for layer 1\n    c1 = np.zeros(3, dtype=float)\n    h1 = np.zeros(3, dtype=float)\n    # Initialize state for layer 2 counter\n    c2 = 0.0\n\n    invalid_a_after_b = False\n    underflow = False\n\n    for ch in seq:\n        # One-hot encoding for input at time t\n        if ch == 'a':\n            xa, xb = 1.0, 0.0\n        elif ch == 'b':\n            xa, xb = 0.0, 1.0\n        else:\n            # Invalid character: reject immediately\n            return False\n\n        # Layer 1 gate computations and updates\n        # Dimension 0: 'a' indicator-like pass-through\n        i0 = sigma(K * (xa - 0.5))  # ~1 if 'a', ~0 if 'b'\n        f0 = 0.0\n        g0 = 1.0\n        c1[0] = f0 * c1[0] + i0 * g0\n        h1[0] = tanh(c1[0])  # output gate ~1\n\n        # Dimension 1: 'b' indicator-like pass-through\n        i1 = sigma(K * (xb - 0.5))  # ~1 if 'b', ~0 if 'a'\n        f1 = 0.0\n        g1 = 1.0\n        c1[1] = f1 * c1[1] + i1 * g1\n        h1[1] = tanh(c1[1])  # output gate ~1\n\n        # Dimension 2: accumulator of b's (phase detector)\n        i2 = sigma(K * (xb - 0.5))  # ~1 on 'b', ~0 on 'a'\n        f2 = 1.0\n        g2 = 1.0\n        c1[2] = f2 * c1[2] + i2 * g2\n        h1[2] = tanh(c1[2])  # increases toward 1 after any 'b'\n\n        # Phase indicator: whether any 'b' has been seen (threshold on c1[2])\n        b_phase = c1[2]  0.5\n\n        # Detect invalid 'a' after 'b'\n        if xa  0.5 and b_phase:\n            invalid_a_after_b = True\n\n        # Layer 2 counter update: +1 for 'a' before any 'b', -1 for 'b'\n        inc = 1 if (xa  0.5 and not b_phase) else 0\n        dec = -1 if xb  0.5 else 0\n        # LSTM-like update with f=1, i=1, candidate = inc + dec\n        c2 = c2 + (inc + dec)\n\n        # Underflow check: counter must never go negative\n        if c2  -eps:\n            underflow = True\n\n    # Acceptance: final counter ~ 0, and no invalid events\n    accept = (abs(c2) = eps) and (not invalid_a_after_b) and (not underflow)\n    return accept\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        \"\",           # empty string\n        \"ab\",         # minimal valid\n        \"aaabbb\",     # valid, n=3\n        \"aabbb\",      # invalid: too many b\n        \"aaabb\",      # invalid: too many a\n        \"aba\",        # invalid: a after b\n        \"b\",          # invalid: starts with b, underflow\n        \"aaaabbbb\"    # valid, n=4\n    ]\n\n    results = []\n    for seq in test_cases:\n        result = accept_sequence(seq)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In modern deep learning, stacked RNNs are often combined with other powerful tools like attention to create highly effective models. This practice guides you through a numerical experiment to analyze the synergy between these two components, specifically how the model's effective receptive field is influenced by the depth of the layer feeding the attention mechanism . You will implement a system where attention queries come from the top layer, but keys are sourced from different depths, and then compute the resulting \"attention span distribution\" $p(d)$. This exercise provides crucial hands-on experience in analyzing information flow in hybrid architectures, revealing how different layers provide qualitatively distinct information that can be leveraged to control a model's temporal focus.",
            "id": "3175991",
            "problem": "You are asked to design and implement a deterministic numerical experiment that isolates how attaching a single-head, causal, scaled dot-product attention mechanism at different depths of a stacked Recurrent Neural Network (RNN) changes the effective receptive field over time lags in a sequence. Work from first principles: use only the core definitions of a stacked Recurrent Neural Network and scaled dot-product attention, and derive the attention span distribution induced by the attention mechanism when queries are taken from the top layer and keys/values are taken from a specified layer.\n\nBegin with the following well-tested definitions and facts:\n\n- A Recurrent Neural Network (RNN) is specified by a hidden-state recurrence. For layer index $l \\in \\{1,\\dots,L\\}$ at time $t \\in \\{0,\\dots,T-1\\}$, define hidden state $h_t^{(l)} \\in \\mathbb{R}^{d_h^{(l)}}$ by\n  $$h_t^{(1)} = \\tanh\\!\\left(U^{(1)} x_t + R^{(1)} h_{t-1}^{(1)} + b^{(1)}\\right), \\quad h_{-1}^{(1)} = 0,$$\n  $$h_t^{(l)} = \\tanh\\!\\left(U^{(l)} h_t^{(l-1)} + R^{(l)} h_{t-1}^{(l)} + b^{(l)}\\right), \\quad h_{-1}^{(l)} = 0 \\text{ for } l \\ge 2,$$\n  where $x_t \\in \\mathbb{R}^{d_x}$ is the input at time $t$, $U^{(l)} \\in \\mathbb{R}^{d_h^{(l)} \\times d_{in}^{(l)}}$, $R^{(l)} \\in \\mathbb{R}^{d_h^{(l)} \\times d_h^{(l)}}$, $b^{(l)} \\in \\mathbb{R}^{d_h^{(l)}}$, and $d_{in}^{(1)} = d_x$, $d_{in}^{(l)} = d_h^{(l-1)}$ for $l \\ge 2$. The nonlinearity is the hyperbolic tangent.\n\n- Scaled dot-product attention for a single head with query dimension $d_k$ is defined as follows. Given queries $q_t \\in \\mathbb{R}^{d_k}$, keys $k_s \\in \\mathbb{R}^{d_k}$, and a causal mask that allows attending only to indices $s \\le t$, the attention weights are\n  $$\\alpha_{t,s} = \\frac{\\exp\\!\\left(\\frac{q_t^\\top k_s}{\\sqrt{d_k}}\\right)}{\\sum_{u=0}^t \\exp\\!\\left(\\frac{q_t^\\top k_u}{\\sqrt{d_k}}\\right)} \\quad \\text{for } s \\in \\{0,\\dots,t\\}.$$\n  The queries and keys are formed by linear projections $q_t = W_Q h_t^{(L)}$ and $k_s = W_K h_s^{(\\ell)}$ where $W_Q \\in \\mathbb{R}^{d_k \\times d_h^{(L)}}$ and $W_K \\in \\mathbb{R}^{d_k \\times d_h^{(\\ell)}}$, with $\\ell \\in \\{1,\\dots,L\\}$ denoting the layer that provides keys (and values, though values are unused here).\n\nDefine the attention span distribution over nonnegative time lags $d \\in \\{0,1,\\dots,T-1\\}$ as the distribution that aggregates, uniformly over all time steps, the attention mass assigned to each lag:\n$$p(d) = \\frac{1}{T} \\sum_{t=0}^{T-1} \\sum_{s=0}^{t} \\alpha_{t,s} \\, \\mathbf{1}\\{t-s = d\\}.$$\nBy construction, $\\sum_{d=0}^{T-1} p(d) = 1$. This $p(d)$ quantifies the effective receptive field over relative positions induced purely by attention, conditioned on which internal layer provides the keys.\n\nYour task is to:\n\n- Implement a stacked RNN as specified above with $L$ layers.\n- Generate inputs $x_t$ as independent and identically distributed Gaussian vectors with zero mean and unit variance, i.e., each component of $x_t$ is sampled from $\\mathcal{N}(0,1)$.\n- Construct queries and keys by $q_t = W_Q h_t^{(L)}$ and $k_s = W_K h_s^{(\\ell)}$, for a specified key-providing layer index $\\ell$, then compute causal attention weights $\\alpha_{t,s}$ for all $t$ and $s \\le t$.\n- Compute the attention span distribution $p(d)$ defined above.\n- All weight matrices must be initialized from independent zero-mean Gaussian distributions with layer-appropriate scaling to avoid trivial saturation. Use $U^{(l)}_{ij} \\sim \\mathcal{N}(0, 1/d_{in}^{(l)})$, $R^{(l)}_{ij} \\sim \\mathcal{N}(0, 1/d_h^{(l)})$, $W_Q \\sim \\mathcal{N}(0, 1/d_h^{(L)})$, $W_K \\sim \\mathcal{N}(0, 1/d_h^{(\\ell)})$, and $b^{(l)} = 0$. All random draws must be deterministic via a specified pseudorandom seed.\n\nCompute $p(d)$ for the following test suite. Each test case specifies $L$, the input dimension $d_x$, the hidden dimensions $\\{d_h^{(l)}\\}_{l=1}^L$, the query/key dimension $d_k$, the sequence length $T$, the key-providing layer index $\\ell$, and two seeds: one for the model parameters and one for the input sequence. When two cases share identical seeds and dimensions but differ only in $\\ell$, the models and inputs must be identical up to the difference in the key-providing layer.\n\n- Test case $1$ (happy path, two-layer baseline):\n  - $L = 2$, $d_x = 5$, $(d_h^{(1)}, d_h^{(2)}) = (8, 8)$, $d_k = 6$, $T = 20$, $\\ell = 1$, $\\text{model\\_seed} = 2025$, $\\text{input\\_seed} = 100$.\n- Test case $2$ (same model and inputs as case $1$, but keys from the top layer):\n  - $L = 2$, $d_x = 5$, $(d_h^{(1)}, d_h^{(2)}) = (8, 8)$, $d_k = 6$, $T = 20$, $\\ell = 2$, $\\text{model\\_seed} = 2025$, $\\text{input\\_seed} = 100$.\n- Test case $3$ (deeper stack, keys from deepest layer):\n  - $L = 3$, $d_x = 5$, $(d_h^{(1)}, d_h^{(2)}, d_h^{(3)}) = (8, 6, 10)$, $d_k = 6$, $T = 30$, $\\ell = 3$, $\\text{model\\_seed} = 777$, $\\text{input\\_seed} = 555$.\n- Test case $4$ (same model and inputs as case $3$, but keys from the first layer):\n  - $L = 3$, $d_x = 5$, $(d_h^{(1)}, d_h^{(2)}, d_h^{(3)}) = (8, 6, 10)$, $d_k = 6$, $T = 30$, $\\ell = 1$, $\\text{model\\_seed} = 777$, $\\text{input\\_seed} = 555$.\n- Test case $5$ (boundary case: minimal sequence length):\n  - $L = 2$, $d_x = 4$, $(d_h^{(1)}, d_h^{(2)}) = (6, 6)$, $d_k = 4$, $T = 1$, $\\ell = 2$, $\\text{model\\_seed} = 9$, $\\text{input\\_seed} = 9$.\n\nImplementation and numerical requirements:\n\n- Use the hyperbolic tangent nonlinearity and the causal mask that restricts attention to indices $s \\le t$.\n- Use the scaled dot-product factor $1/\\sqrt{d_k}$ exactly as specified.\n- For numerical stability in the Softmax, subtract the maximum score within the allowed window before exponentiation.\n- Round each element of each output distribution $p(d)$ to $6$ decimal places.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to a test case and must itself be a list of the rounded distribution values $[p(0), p(1), \\dots, p(T-1)]$ for that case. For example, the overall structure must be of the form\n  $$\\text{[ [p\\_case1(0),...,p\\_case1(T\\_1-1)], [p\\_case2(0),...,p\\_case2(T\\_2-1)], \\dots ]}$$\nwith no additional text before or after.\n\nThere are no physical units, angles, or percentages involved in this problem. All quantities are dimensionless real numbers. Ensure scientific realism by using the precise definitions given above without introducing any auxiliary heuristics or approximations.",
            "solution": "The problem requires the design and implementation of a numerical experiment to determine the attention span distribution, $p(d)$, induced by a single-head causal attention mechanism integrated with a stacked Recurrent Neural Network (RNN). The distribution quantifies the effective receptive field over time lags $d$, and its shape is expected to change depending on which layer, $\\ell$, of the RNN provides the keys and values to the attention mechanism. The entire process must be deterministic, governed by specified pseudorandom number seeds.\n\nThe solution is implemented in four main stages, following the principles laid out in the problem statement.\n\n### 1. Model and Data Initialization\n\nThe first step is to construct the computational model and the input data. This process is governed by the `model_seed` and `input_seed` to ensure reproducibility.\n\n**Model Parameters**: The stacked RNN has $L$ layers, with parameters $\\{U^{(l)}, R^{(l)}, b^{(l)}\\}_{l=1}^L$. The attention mechanism has parameters $W_Q$ and $W_K$. All bias vectors $b^{(l)}$ are initialized to zero. The weight matrices are initialized by drawing from a zero-mean Gaussian distribution with a variance scaled by the fan-in, a standard technique to promote stable signal propagation.\n- For the RNN layers, weights are initialized as:\n  $$U^{(l)}_{ij} \\sim \\mathcal{N}(0, 1/d_{in}^{(l)}), \\quad R^{(l)}_{ij} \\sim \\mathcal{N}(0, 1/d_h^{(l)})$$\n  where $d_{in}^{(1)} = d_x$ (the input dimension) and $d_{in}^{(l)} = d_h^{(l-1)}$ for $l \\ge 2$. $d_h^{(l)}$ is the hidden state dimension of layer $l$.\n- For the attention mechanism, the query and key projection matrices are initialized as:\n  $$W_Q \\sim \\mathcal{N}(0, 1/d_h^{(L)}), \\quad W_K \\sim \\mathcal{N}(0, 1/d_h^{(\\ell)})$$\n  where $L$ is the total number of layers and $\\ell$ is the index of the key-providing layer. The initialization of $W_K$ explicitly depends on the dimension of the chosen source layer $\\ell$. For test cases sharing a `model_seed` but differing in $\\ell$, the sequence of random draws for shared parameters ($U^{(l)}, R^{(l)}, W_Q$) is identical, ensuring that the underlying models are the same up to the component ($W_K$) that is functionally dependent on $\\ell$.\n\n**Input Data**: The input sequence $\\{x_t\\}_{t=0}^{T-1}$ consists of $T$ vectors, each of dimension $d_x$. Each component of each vector $x_t$ is drawn independently from a standard normal distribution, $x_{t,i} \\sim \\mathcal{N}(0, 1)$, using a random number generator seeded with `input_seed`.\n\n### 2. Stacked RNN Forward Pass\n\nWith the model and data initialized, the next step is to compute the hidden states $h_t^{(l)}$ for all layers $l \\in \\{1, \\dots, L\\}$ and time steps $t \\in \\{0, \\dots, T-1\\}$. This is done by unrolling the network through time. The hidden states are initialized to zero, i.e., $h_{-1}^{(l)} = 0$ for all $l$.\n\nThe computation proceeds sequentially for each time step $t$:\n- For the first layer ($l=1$), the hidden state is computed based on the current input $x_t$ and the previous hidden state of the same layer, $h_{t-1}^{(1)}$:\n  $$h_t^{(1)} = \\tanh(U^{(1)} x_t + R^{(1)} h_{t-1}^{(1)})$$\n- For all subsequent layers ($l \\ge 2$), the hidden state is computed based on the hidden state of the layer below at the current time step, $h_t^{(l-1)}$, and the previous hidden state of the same layer, $h_{t-1}^{(l)}$:\n  $$h_t^{(l)} = \\tanh(U^{(l)} h_t^{(l-1)} + R^{(l)} h_{t-1}^{(l)})$$\nThis process is iterated from $t=0$ to $T-1$, and for each $t$, from $l=1$ to $L$. All computed hidden states $\\{h_t^{(l)}\\}$ are stored for use in the next stage.\n\n### 3. Causal Attention Weight Calculation\n\nThe attention mechanism computes a set of weights $\\alpha_{t,s}$ that specify how much the output at time $t$ \"attends\" to inputs from time $s$.\n\n- **Queries and Keys**: Queries are generated from the final layer of the RNN, $h_t^{(L)}$, while keys are generated from a specified layer $\\ell$, $h_s^{(\\ell)}$. This is the core of the experiment, as varying $\\ell$ changes the source of information for the attention mechanism.\n  $$q_t = W_Q h_t^{(L)}, \\quad k_s = W_K h_s^{(\\ell)}$$\n- **Attention Scores**: For each query $q_t$, scores are computed against all keys $k_s$ where $s \\le t$, respecting the causal structure of time-series data. The scores are computed using scaled dot-product:\n  $$\\text{score}(t,s) = \\frac{q_t^\\top k_s}{\\sqrt{d_k}}$$\n  where $d_k$ is the dimension of the queries and keys.\n- **Softmax Normalization**: The scores are converted into a probability distribution (the attention weights $\\alpha_{t,s}$) using the softmax function, applied over all allowed source positions $u \\in \\{0, \\dots, t\\}$:\n  $$\\alpha_{t,s} = \\frac{\\exp(\\text{score}(t,s))}{\\sum_{u=0}^t \\exp(\\text{score}(t,u))}$$\n  For numerical stability, the maximum score for a given $t$, $\\max_{u \\le t} \\{\\text{score}(t,u)\\}$, is subtracted from all scores before exponentiation. This prevents floating-point overflow without changing the final distribution. This computation is performed for each time step $t \\in \\{0, \\dots, T-1\\}$.\n\n### 4. Attention Span Distribution Computation\n\nThe final step is to aggregate the computed attention weights $\\alpha_{t,s}$ into the attention span distribution $p(d)$. This distribution measures the average attention weight assigned to a specific time lag $d = t-s$.\n\nThe distribution is defined as:\n$$p(d) = \\frac{1}{T} \\sum_{t=0}^{T-1} \\sum_{s=0}^{t} \\alpha_{t,s} \\, \\mathbf{1}\\{t-s = d\\}$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The formula can be rewritten as an average over contributions for each lag $d$:\n$$p(d) = \\frac{1}{T} \\sum_{t=d}^{T-1} \\alpha_{t, t-d}$$\nComputationally, this is equivalent to summing the elements along the $d$-th sub-diagonal of the $T \\times T$ attention matrix (where $\\alpha_{t,s}$ are the entries) and dividing by $T$. The resulting vector $p = [p(0), p(1), \\dots, p(T-1)]$ is a probability distribution. Each element of this vector is then rounded to $6$ decimal places as required. By executing this entire procedure for each test case, we obtain the desired set of distributions.",
            "answer": "```python\nimport numpy as np\n\ndef compute_dist_for_case(L, d_x, d_h, d_k, T, ell, model_seed, input_seed):\n    \"\"\"\n    Computes the attention span distribution for a single test case.\n    \"\"\"\n    # 1. Initialize RNGs and Model Parameters\n    model_rng = np.random.default_rng(model_seed)\n    \n    U_mats, R_mats = [], []\n    d_in_l = d_x\n    for l in range(L):\n        d_h_l = d_h[l]\n        \n        # U^(l) matrix\n        U_std = (1 / d_in_l)**0.5\n        U_mats.append(model_rng.normal(loc=0.0, scale=U_std, size=(d_h_l, d_in_l)))\n        \n        # R^(l) matrix\n        R_std = (1 / d_h_l)**0.5\n        R_mats.append(model_rng.normal(loc=0.0, scale=R_std, size=(d_h_l, d_h_l)))\n        \n        d_in_l = d_h_l\n    \n    # W_Q matrix for queries\n    d_h_L = d_h[L - 1]\n    W_Q_std = (1 / d_h_L)**0.5\n    W_Q = model_rng.normal(loc=0.0, scale=W_Q_std, size=(d_k, d_h_L))\n    \n    # W_K matrix for keys (ell is 1-indexed, convert to 0-indexed)\n    d_h_ell = d_h[ell - 1]\n    W_K_std = (1 / d_h_ell)**0.5\n    W_K = model_rng.normal(loc=0.0, scale=W_K_std, size=(d_k, d_h_ell))\n\n    # Initialize Input Data\n    input_rng = np.random.default_rng(input_seed)\n    X = input_rng.normal(loc=0.0, scale=1.0, size=(T, d_x))\n    \n    if T == 0:\n        return []\n\n    # 2. Stacked RNN Forward Pass\n    # Store hidden states h_t^(l) for t in -1..T-1\n    # H[l][t+1] stores h_t^(l)\n    H = [np.zeros((T + 1, d)) for d in d_h]\n    \n    for t in range(T):\n        t_idx = t + 1 # Index for current time step in H\n        \n        # Layer l=1 (0-indexed)\n        h_tm1_1 = H[0][t_idx - 1]\n        h_t_1 = np.tanh(U_mats[0] @ X[t] + R_mats[0] @ h_tm1_1)\n        H[0][t_idx] = h_t_1\n        \n        # Layers l  1\n        for l in range(1, L):\n            h_t_lm1 = H[l - 1][t_idx]\n            h_tm1_l = H[l][t_idx - 1]\n            h_t_l = np.tanh(U_mats[l] @ h_t_lm1 + R_mats[l] @ h_tm1_l)\n            H[l][t_idx] = h_t_l\n    \n    # Extract hidden states for t=0..T-1\n    H_L_states = H[L - 1][1:]\n    H_ell_states = H[ell - 1][1:]\n\n    # 3. Attention Weight Calculation\n    Q = H_L_states @ W_Q.T  # Shape (T, d_k)\n    K = H_ell_states @ W_K.T  # Shape (T, d_k)\n    \n    alpha = np.zeros((T, T))\n    sqrt_dk = np.sqrt(d_k)\n    \n    for t in range(T):\n        q_t = Q[t]\n        # Causal keys: s = t\n        K_causal = K[0:t + 1]\n        \n        scores = (q_t @ K_causal.T) / sqrt_dk\n        \n        # Numerically stable softmax\n        stable_scores = scores - np.max(scores)\n        exp_scores = np.exp(stable_scores)\n        attention_weights = exp_scores / np.sum(exp_scores)\n        \n        alpha[t, 0:t + 1] = attention_weights\n\n    # 4. Attention Span Distribution\n    p = np.zeros(T)\n    for d in range(T):\n        # Summing alpha_{t, t-d} for t from d to T-1\n        # This is the -d'th diagonal of the alpha matrix\n        p[d] = np.sum(np.diag(alpha, k=-d))\n    \n    p /= T\n    \n    return np.round(p, 6).tolist()\n\ndef solve():\n    test_cases = [\n        { \"L\": 2, \"d_x\": 5, \"d_h\": (8, 8), \"d_k\": 6, \"T\": 20, \"ell\": 1, \"model_seed\": 2025, \"input_seed\": 100 },\n        { \"L\": 2, \"d_x\": 5, \"d_h\": (8, 8), \"d_k\": 6, \"T\": 20, \"ell\": 2, \"model_seed\": 2025, \"input_seed\": 100 },\n        { \"L\": 3, \"d_x\": 5, \"d_h\": (8, 6, 10), \"d_k\": 6, \"T\": 30, \"ell\": 3, \"model_seed\": 777, \"input_seed\": 555 },\n        { \"L\": 3, \"d_x\": 5, \"d_h\": (8, 6, 10), \"d_k\": 6, \"T\": 30, \"ell\": 1, \"model_seed\": 777, \"input_seed\": 555 },\n        { \"L\": 2, \"d_x\": 4, \"d_h\": (6, 6), \"d_k\": 4, \"T\": 1, \"ell\": 2, \"model_seed\": 9, \"input_seed\": 9 },\n    ]\n\n    results = []\n    for case in test_cases:\n        p_dist = compute_dist_for_case(**case)\n        results.append(p_dist)\n\n    # Format output as a list of lists string representation\n    output_str = \"[\" + \",\".join(str(res) for res in results) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}