{
    "hands_on_practices": [
        {
            "introduction": "长短期记忆网络（LSTM）的核心在于其门控机制如何随时间调控信息流。为了真正理解其工作原理，我们首先需要揭开其递归更新规则的数学本质。这个练习将引导你推导出一个在简化（但极具启发性）假设下的闭式解，从而揭示 LSTM 的细胞状态更新与经典的指数平滑（exponential smoothing）方法之间深刻的内在联系。通过这个推导，你将不再仅仅将记忆视为一个抽象概念，而是能从数学上精确地看到信息是如何被加权、遗忘和累积的 。",
            "id": "3188449",
            "problem": "考虑一个长短期记忆（LSTM）单元的单个标量单位。根据定义，时间步 $t$ 的细胞状态更新由 $c_t = f_t c_{t-1} + i_t \\tilde{c}_t$ 给出，其中 $f_t$ 是遗忘门的值，$i_t$ 是输入门的值，$\\tilde{c}_t$ 是候选细胞状态，每个值都由相应的门控和候选机制（例如，门的逻辑S型函数和候选的双曲正切函数）产生。假设门是时不变的，即 $f_t \\equiv f$ 和 $i_t \\equiv i$，其中 $0  f  1$ 且 $0  i \\leq 1$，并且初始细胞状态 $c_0$ 是给定的。候选细胞状态序列 $\\{\\tilde{c}_k\\}_{k=1}^t$ 是任意的，但有界且已知。\n\n从上述核心更新定义出发，且不引入任何额外的快捷公式，推导出一个关于 $f$、$i$、$c_0$ 以及序列 $\\tilde{c}_1, \\tilde{c}_2, \\dots, \\tilde{c}_t$ 的 $c_t$ 的闭式解析表达式。然后，将得到的表达式解释为应用于候选序列的一种指数平滑形式，确定在时间 $t$ 赋给每个 $\\tilde{c}_k$ 的权重，并解释初始条件 $c_0$ 在此平滑视角下的作用。\n\n你的最终答案必须是你为 $c_t$ 推导出的单一闭式解析表达式。不需要数值近似。",
            "solution": "该问题是有效的，因为它科学地基于循环神经网络的原理，问题提出得很好，有足够的信息得到唯一解，并且以客观、正式的语言表述。我们可以开始推导。\n\n起点是 LSTM 细胞状态 $c_t$ 在时间步 $t$ 的核心更新定义。在给定的时不变门的假设下，递推关系为：\n$$c_t = f c_{t-1} + i \\tilde{c}_t$$\n其中 $f$ 是恒定的遗忘门值，$i$ 是恒定的输入门值，$c_{t-1}$ 是前一时间步的细胞状态，$\\tilde{c}_t$ 是当前时间步的候选细胞状态。初始细胞状态为 $c_0$。\n\n我们的目标是通过展开这个递推关系来推导 $c_t$ 的闭式解析表达式。让我们展开前几个时间步的表达式来寻找一个模式。\n\n对于 $t=1$：\n$$c_1 = f c_0 + i \\tilde{c}_1$$\n\n对于 $t=2$：\n$$c_2 = f c_1 + i \\tilde{c}_2$$\n代入 $c_1$ 的表达式：\n$$c_2 = f(f c_0 + i \\tilde{c}_1) + i \\tilde{c}_2 = f^2 c_0 + f i \\tilde{c}_1 + i \\tilde{c}_2$$\n\n对于 $t=3$：\n$$c_3 = f c_2 + i \\tilde{c}_3$$\n代入 $c_2$ 的表达式：\n$$c_3 = f(f^2 c_0 + f i \\tilde{c}_1 + i \\tilde{c}_2) + i \\tilde{c}_3 = f^3 c_0 + f^2 i \\tilde{c}_1 + f i \\tilde{c}_2 + i \\tilde{c}_3$$\n\n从这些展开式中，一个普遍的模式浮现出来。$c_t$ 的表达式似乎由两部分组成：一个涉及初始状态 $c_0$ 的项，以及一个关于候选状态序列 $\\{\\tilde{c}_k\\}$ 的求和。我们假设以下闭式表达式：\n$$c_t = f^t c_0 + i \\sum_{k=1}^{t} f^{t-k} \\tilde{c}_k$$\n\n为了严格地证明这个公式，我们对时间步 $t$ 使用数学归纳法进行证明。\n\n**基本情况：** 对于 $t=1$，假设的公式得出：\n$$c_1 = f^1 c_0 + i \\sum_{k=1}^{1} f^{1-k} \\tilde{c}_k = f c_0 + i (f^{1-1} \\tilde{c}_1) = f c_0 + i f^0 \\tilde{c}_1 = f c_0 + i \\tilde{c}_1$$\n这个结果与直接从 $t=1$ 的递推关系推导出的表达式相匹配。基本情况成立。\n\n**归纳步骤：** 假设该公式对于任意时间步 $t-1$ 成立，其中 $t > 1$。这是我们的归纳假设：\n$$c_{t-1} = f^{t-1} c_0 + i \\sum_{k=1}^{t-1} f^{(t-1)-k} \\tilde{c}_k$$\n我们必须证明该公式在时间步 $t$ 也成立。我们从 $c_t$ 的递推关系开始：\n$$c_t = f c_{t-1} + i \\tilde{c}_t$$\n代入 $c_{t-1}$ 的归纳假设：\n$$c_t = f \\left( f^{t-1} c_0 + i \\sum_{k=1}^{t-1} f^{t-1-k} \\tilde{c}_k \\right) + i \\tilde{c}_t$$\n分配因子 $f$：\n$$c_t = f \\cdot f^{t-1} c_0 + f \\cdot i \\sum_{k=1}^{t-1} f^{t-1-k} \\tilde{c}_k + i \\tilde{c}_t$$\n$$c_t = f^t c_0 + i \\sum_{k=1}^{t-1} f^{1 + (t-1-k)} \\tilde{c}_k + i \\tilde{c}_t$$\n$$c_t = f^t c_0 + i \\sum_{k=1}^{t-1} f^{t-k} \\tilde{c}_k + i \\tilde{c}_t$$\n最后一项 $i\\tilde{c}_t$ 可以写成 $i f^0 \\tilde{c}_t = i f^{t-t} \\tilde{c}_t$。这使我们能将其合并到求和中，将上限从 $t-1$ 扩展到 $t$：\n$$c_t = f^t c_0 + \\left( i \\sum_{k=1}^{t-1} f^{t-k} \\tilde{c}_k \\right) + i f^{t-t} \\tilde{c}_t$$\n$$c_t = f^t c_0 + i \\sum_{k=1}^{t} f^{t-k} \\tilde{c}_k$$\n这正是时间步 $t$ 的假设公式。因此，根据数学归纳法原理，该闭式表达式对所有 $t \\ge 1$ 均有效。\n\n**解释为指数平滑：**\n推导出的表达式 $c_t = f^t c_0 + i \\sum_{k=1}^{t} f^{t-k} \\tilde{c}_k$ 可以被解释为一种指数平滑形式。\n\n$c_t$ 的表达式是两个分量的和：\n1. 初始状态的贡献：$f^t c_0$。\n2. 候选状态的加权和：$i \\sum_{k=1}^{t} f^{t-k} \\tilde{c}_k$。\n\n第一项 $f^t c_0$ 代表初始细胞状态 $c_0$ 的持续影响。在约束条件 $0  f  1$ 下，随着 $t \\to \\infty$，因子 $f^t$ 指数衰减趋向于零。这展示了“遗忘”机制：对初始状态的记忆随着时间的推移而逐渐消失。\n\n第二项是从 $\\tilde{c}_1$到 $\\tilde{c}_t$ 的候选状态序列的加权和。分配给每个候选状态 $\\tilde{c}_k$（其中 $1 \\le k \\le t$）的权重是 $w_k = i f^{t-k}$。让我们来研究这些权重：\n- 对于最新的候选 $\\tilde{c}_t$，权重是 $w_t = i f^{t-t} = i$。\n- 对于前一个候选 $\\tilde{c}_{t-1}$，权重是 $w_{t-1} = i f^{t-(t-1)} = i f$。\n- 对于过去的候选 $\\tilde{c}_{t-j}$，权重是 $w_{t-j} = i f^{j}$。\n\n由于 $0  f  1$，当我们向更远的过去追溯时（即，随着 $k$ 减小，$t-k$ 增大，以及 $f^{t-k}$ 减小），权重 $w_k$ 会几何级数地减小。这种不相等的加权方式，即最近的输入比旧的输入获得指数级更高的权重，是指数加权移动平均（也称为指数平滑）的决定性特征。遗忘门值 $f$ 充当衰减率或平滑因子。\n\n总而言之，初始条件 $c_0$ 的作用是为这个平滑过程提供一个起始值。它的影响由 $f^t$ 加权，并呈指数衰减。因此，细胞状态 $c_t$ 是其初始状态的衰退记忆与截至时间 $t$ 所遇到的所有候选输入的指数平滑平均值的组合。在时间 $t$ 分配给特定候选 $\\tilde{c}_k$ 的权重恰好是 $i f^{t-k}$。输入门值 $i$ 充当所有新信息影响的缩放因子。",
            "answer": "$$\\boxed{c_t = f^t c_0 + i \\sum_{k=1}^{t} f^{t-k} \\tilde{c}_k}$$"
        },
        {
            "introduction": "在理解了 LSTM 记忆的理论基础之后，一个自然而然的实践问题是：我们如何量化一个 LSTM 单元的“记忆时长”？更进一步，我们如何通过调整模型参数来主动控制这种记忆能力？这个练习将理论与实践相结合，要求你定义并计算一个“有效记忆半衰期”的指标 。通过完成这个练习，你将亲手验证一个在实践中至关重要的结论：对遗忘门偏置项（forget gate bias）进行简单的初始化调整，可以极大地影响网络的长期依赖学习能力。",
            "id": "3188446",
            "problem": "您需要为长短期记忆 (LSTM) 单元的遗忘门形式化并计算一个有效的记忆度量。考虑一个长短期记忆 (LSTM) 单元，其细胞状态的更新从根本上由一个乘性保留机制定义。设在离散时间步 $t$ 的遗忘门激活值为 $f_t \\in (0,1)$，它通过 logistic sigmoid 函数 $\\sigma$ 从预激活值 $z_t$ 获得，即 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$。当没有新输入注入时，细胞状态 $c_t$ 根据一个仅保留的递归关系 $c_t = f_t \\cdot c_{t-1}$ 演化（这是 LSTM 单元中乘性保留的基本定义）。经过 $k$ 步，初始状态 $c_0$ 的保留比例是乘积 $\\prod_{t=1}^{k} f_t$。\n\n将有效记忆半衰期 $h$（以离散步数为单位）定义为唯一的正实数 $k$，在该步数下，期望保留比例等于 $0.5$。此定义基于一个假设，即 $\\{f_t\\}$ 的经验统计量具有足够的代表性和平稳性，可以用时间平均来近似期望值。请仅使用上述基本定义，推导出一个用 $\\{f_t\\}$ 的经验统计量表示的 $h$ 的可计算表达式，不要引入任何与乘性保留机制无关的额外假设。\n\n您将使用一个固定的基线预激活值时间序列 $\\{s_t\\}$，通过构建 $z_t = b_f + s_t$ 和 $f_t = \\sigma(z_t)$，来比较遗忘门偏置初始化 $b_f \\in \\{0,1,2\\}$ 对半衰期的影响。\n\n使用的基本原理：\n- LSTM 仅保留更新 $c_t = f_t \\cdot c_{t-1}$。\n- 多步保留的乘性，$\\prod_{t=1}^k f_t$。\n- logistic sigmoid 函数 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$。\n- 乘积的对数法则。\n\n您的任务：\n1. 对于每个 $b_f \\in \\{0,1,2\\}$，使用下面的测试套件计算 $t = 1, \\dots, 20$ 的 $z_t = b_f + s_t$ 和 $f_t = \\sigma(z_t)$。\n2. 仅使用上述基本原理，根据 $\\{f_t\\}_{t=1}^{20}$ 的经验统计量，推导出一个以步数为单位的有效记忆半衰期 $h$ 的可计算估计量。\n3. 实现一个程序，根据所提供的数据为每个 $b_f \\in \\{0,1,2\\}$ 计算 $h$，并按 $b_f=0, b_f=1, b_f=2$ 的顺序输出这三个值。\n4. 将每个报告的 $h$ 四舍五入到 $6$ 位小数。如果计算结果意味着期望保留不衰减或增长（即，经验平均衰减率为非负），则在该情况下输出 $+\\infty$ 作为字面量 `inf`。\n\n测试套件：\n- 使用长度为 $20$ 的固定基线预激活值序列：\n$$\n\\{s_t\\}_{t=1}^{20} = [-1.5,\\ 0.0,\\ 1.0,\\ -0.5,\\ 3.0,\\ -3.0,\\ 0.5,\\ 2.0,\\ -2.0,\\ 4.0,\\ -4.0,\\ 1.5,\\ -1.0,\\ 0.25,\\ -0.25,\\ 2.5,\\ -2.5,\\ 0.75,\\ -0.75,\\ 5.0].\n$$\n- 对于每个 $b_f \\in \\{0,1,2\\}$，构建 $z_t = b_f + s_t$ 和 $f_t = \\sigma(z_t)$，并计算相应的有效半衰期 $h$。\n\n角度单位不适用。不涉及物理单位。所有输出都是以离散步数为单位的纯数。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[h_0,h_1,h_2]$），其中 $h_0$ 对应 $b_f = 0$，$h_1$ 对应 $b_f = 1$，$h_2$ 对应 $b_f = 2$。每个 $h$ 必须按要求四舍五入到 $6$ 位小数。\n\n覆盖性设计：\n- 所提供的 $\\{s_t\\}$ 包含强负值和强正值，以测试当 $f_t$ 接近 $0$ 和 $1$ 时的边缘行为，从而确保在三个要求的 $b_f$ 值下的数值稳定性并测试边界条件。",
            "solution": "用户希望在一个简化的仅保留模型下，计算 LSTM 遗忘门的有效记忆半衰期。\n\n### 第一步：提取已知条件\n\n-   **LSTM 细胞状态更新（仅保留）：** $c_t = f_t \\cdot c_{t-1}$，其中 $c_t$ 是在时间 $t$ 的细胞状态。\n-   **遗忘门激活值：** $f_t = \\sigma(z_t)$，其中 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ 是 logistic sigmoid 函数且 $f_t \\in (0,1)$。\n-   **预激活值：** $z_t = b_f + s_t$，其中 $b_f$ 是一个偏置项。\n-   **经过 $k$ 步的保留比例：** $R_k = \\prod_{t=1}^{k} f_t$。\n-   **有效记忆半衰期 ($h$)：** 使得期望保留比例等于 $0.5$ 的唯一正实数 $k$。期望值将基于 $\\{f_t\\}$ 的经验统计量通过时间平均来近似。\n-   **基本原理：** 仅使用给定的定义和对数法则。\n-   **遗忘门偏置值：** $b_f \\in \\{0, 1, 2\\}$。\n-   **基线预激活值序列 ($N=20$)：** $\\{s_t\\}_{t=1}^{20} = [-1.5, 0.0, 1.0, -0.5, 3.0, -3.0, 0.5, 2.0, -2.0, 4.0, -4.0, 1.5, -1.0, 0.25, -0.25, 2.5, -2.5, 0.75, -0.75, 5.0]$。\n-   **特殊条件：** 如果经验平均衰减率为非正（意味着保留不衰减），则半衰期 $h$ 应报告为 $+\\infty$（字面量 `inf`）。\n-   **四舍五入：** 最终的 $h$ 值必须四舍五入到 $6$ 位小数。\n\n### 第二步：使用提取的已知条件进行验证\n\n-   **科学依据：** 该问题基于 LSTM 单元的基本数学公式，这是深度学习中的一个核心概念。该模型是一个简化模型，但在数学和概念上对于分析遗忘门的作用是合理的。它不含伪科学。（有效）\n-   **适定性：** 该问题提供了所有必要的数据（$s_t$，$b_f$）、定义（$\\sigma(z)$，$c_t$ 递归）和一个明确的目标（计算 $h$）。可以从给定的原理推导出解决方案的路径。（有效）\n-   **客观性：** 该问题使用精确的数学语言和定义进行陈述。它不含主观性或观点。（有效）\n-   **完整性：** 该问题是自包含的。所有常数、数据序列和公式都已提供。（有效）\n-   **一致性：** 所提供的定义和约束条件相互一致。（有效）\n\n### 第三步：结论和行动\n该问题有效。将推导并实现一个解决方案。\n\n### 半衰期估计量的推导\n\n目标是找到半衰期 $h$，它被定义为记忆细胞状态的期望保留比例达到 $0.5$ 时的时间步数 $k$。经过 $k$ 步后的保留比例由乘积 $R_k = \\prod_{i=1}^{k} f_i$ 给出。\n\n问题的核心在于根据使用“经验统计量”和“时间平均”的指令来解释“期望保留比例”。像这样的乘性过程在对数域中分析最为有效，这也是“乘积的对数法则”提示所指明的路径。\n\n1.  我们使用一个有效的、每步恒定的遗忘因子 $f_{\\text{eff}}$ 来建模保留过程。这个 $f_{\\text{eff}}$ 应该能代表序列 $\\{f_t\\}_{t=1}^{N}$。对于一个乘性序列，最合适的集中趋势度量是几何平均数。\n    $$\n    f_{\\text{eff}} = \\left( \\prod_{t=1}^{N} f_t \\right)^{1/N}\n    $$\n    其中 $N$ 是样本序列的长度，在本例中为 $N=20$。\n\n2.  使用这个有效因子，经过 $h$ 步后的保留比例被建模为 $(f_{\\text{eff}})^h$。半衰期 $h$ 由该比例等于 $0.5$ 的条件定义：\n    $$\n    (f_{\\text{eff}})^h = 0.5\n    $$\n\n3.  为了求解 $h$，我们对等式两边取自然对数：\n    $$\n    \\ln\\left((f_{\\text{eff}})^h\\right) = \\ln(0.5)\n    $$\n    $$\n    h \\cdot \\ln(f_{\\text{eff}}) = \\ln(0.5)\n    $$\n\n4.  求解 $h$ 得出：\n    $$\n    h = \\frac{\\ln(0.5)}{\\ln(f_{\\text{eff}})}\n    $$\n\n5.  项 $\\ln(f_{\\text{eff}})$ 可以用对数法则来表示。它是遗忘门激活值对数的经验平均值：\n    $$\n    \\ln(f_{\\text{eff}}) = \\ln\\left( \\left( \\prod_{t=1}^{N} f_t \\right)^{1/N} \\right) = \\frac{1}{N} \\ln\\left( \\prod_{t=1}^{N} f_t \\right) = \\frac{1}{N} \\sum_{t=1}^{N} \\ln(f_t)\n    $$\n    这个量，我们称之为 $\\lambda_{\\log} = \\frac{1}{N} \\sum_{t=1}^{N} \\ln(f_t)$，代表了每一步的平均对数变化。\n\n6.  将此代入 $h$ 的表达式，我们得到最终的可计算公式：\n    $$\n    h = \\frac{\\ln(0.5)}{\\frac{1}{N} \\sum_{t=1}^{N} \\ln(f_t)}\n    $$\n\n7.  问题指出，如果“经验平均衰减率”为非正，我们应输出 `inf`。在对数域中，衰减率为 $-\\lambda_{\\log} = -(\\frac{1}{N} \\sum_{t=1}^{N} \\ln(f_t))$。非正的衰减率意味着 $-\\lambda_{\\log} \\le 0$，这等价于 $\\lambda_{\\log} \\ge 0$。在这种情况下，$h$ 表达式的分母将为非负。由于 $\\ln(0.5)  0$，一个非负的分母意味着 $h$ 将是负数或无穷大。由于 $h$ 必须是一个正实数，我们得出结论：如果 $\\frac{1}{N} \\sum_{t=1}^{N} \\ln(f_t) \\ge 0$，则半衰期是无穷大，$h = +\\infty$。\n\n### 计算步骤\n\n对于每个偏置 $b_f \\in \\{0, 1, 2\\}$：\n1.  计算 $t=1, \\dots, 20$ 的预激活序列 $z_t = b_f + s_t$。\n2.  计算遗忘门激活序列 $f_t = \\sigma(z_t) = \\frac{1}{1 + e^{-z_t}}$。\n3.  计算对数激活值 $\\ln(f_t)$。\n4.  计算平均对数激活值 $\\lambda_{\\log} = \\frac{1}{20} \\sum_{t=1}^{20} \\ln(f_t)$。\n5.  如果 $\\lambda_{\\log} \\ge 0$，结果为 `inf`。\n6.  否则，计算 $h = \\frac{\\ln(0.5)}{\\lambda_{\\log}}$。\n7.  将有限结果四舍五入到 $6$ 位小数。\n8.  收集 $b_f=0, 1, 2$ 的结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the effective memory half-life of an LSTM's forget gate\n    for different bias initializations.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Baseline pre-activations sequence of length 20\n    s_t = np.array([\n        -1.5, 0.0, 1.0, -0.5, 3.0, -3.0, 0.5, 2.0, -2.0, 4.0, \n        -4.0, 1.5, -1.0, 0.25, -0.25, 2.5, -2.5, 0.75, -0.75, 5.0\n    ])\n\n    # Forget-gate bias values to test\n    biases = [0, 1, 2]\n    \n    # Sigmoid function\n    def sigmoid(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    results = []\n    \n    # Loop through each bias value\n    for b_f in biases:\n        # Step 1: Compute pre-activations z_t\n        z_t = b_f + s_t\n        \n        # Step 2: Compute forget gate activations f_t\n        f_t = sigmoid(z_t)\n        \n        # Step 3: Compute log-activations\n        log_f_t = np.log(f_t)\n        \n        # Step 4: Calculate the average log-activation\n        avg_log_f = np.mean(log_f_t)\n        \n        # Step 5: Check for non-decaying case\n        if avg_log_f >= 0:\n            h = 'inf'\n        else:\n            # Step 6: Compute the half-life h\n            h_val = np.log(0.5) / avg_log_f\n            \n            # Step 7: Round to 6 decimal places\n            h = round(h_val, 6)\n            \n        results.append(h)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "掌握了 LSTM 门控机制的分析和量化方法后，我们可以进入一个更高级的阶段：主动设计。我们能否构建一个学习任务，并设计一个特定的训练目标，从而“教导”或引导 LSTM 的门控（尤其是遗忘门）按我们预想的方式工作？这个练习就是一个关于模型行为工程的绝佳思想实验 。它要求你不仅要理解 LSTM，还要扮演架构师的角色，通过设计合成数据和可微的正则化惩罚项，来精确塑造模型的内部动态，以解决需要分段记忆和周期性重置的复杂问题。",
            "id": "3188539",
            "problem": "考虑一个单单元长短期记忆（LSTM）网络，其中长短期记忆（LSTM）表示一种带有乘法门的门控循环结构，这些门可以调节信息流。单元状态是标量 $c_t \\in \\mathbb{R}$，输入是标量 $x_t \\in \\mathbb{R}$，遗忘门、输入门和输出门是标量 $f_t, i_t, o_t \\in (0,1)$，它们由应用于 $(x_t, h_{t-1})$ 的仿射函数的logistic sigmoid非线性函数生成。单元更新遵循核心定义，即下一个状态结合了前一个状态的门控传递和门控候选更新。假设使用标准自回归训练，具有可微损失函数和随时间反向传播。\n\n你的任务是构建一个合成序列和一个训练目标，使得最优遗忘门 $f_t$ 遵循分段切换模式 $f_t \\in \\{0,1\\}$：在已知的片段边界处 $f_t \\approx 0$ 以清除先前的状态，在片段内部 $f_t \\approx 1$ 以传递记忆。假设存在一个已知的边界指示器 $b_t \\in \\{0,1\\}$，其中 $b_t = 1$ 表示在时间 $t$ 开始一个新片段，否则 $b_t = 0$。预测目标是一个可观测值 $y_t$，其设计使得要实现低预测误差，需要在片段内传递信息并在边界处重置。\n\n根据门控状态累积和基于梯度的优化的第一性原理，选择正确指定以下内容的选项：\n- 一个科学上合理的合成序列和目标 $y_t$，对于该序列和目标，最优 $f_t$ 表现出所述的切换行为，\n- 一个可微的惩罚项，该惩罚项仅使用可微项来鼓励 $f_t$ 逼近所需的二进制模式，并且不会破坏学习其他参数所需的梯度，\n- 以及对训练动态的现实讨论，包括关于遗忘门预激活的梯度形状、logistic sigmoid的饱和效应，以及如何随时间平衡门惩罚与预测损失。\n\n选项：\n\nA. 通过 $b_t$ 定义片段，并选择 $x_t$ 作为独立的、零均值的有界随机变量。令目标为当前片段内的累加和，$y_t = \\sum_{k=\\tau(t)}^{t} x_k$，其中 $\\tau(t)$ 是最近一次 $b_{\\tau(t)} = 1$ 的时间索引。设置一个门目标 $g_t = 1 - b_t$，并对遗忘门添加伯努利交叉熵正则化器，\n$$\n\\mathcal{L}_{\\text{gate}} = \\lambda \\sum_{t} \\big[ g_t \\cdot (-\\log f_t) + (1-g_t) \\cdot (-\\log(1 - f_t)) \\big],\n$$\n其中 $\\lambda  0$，该正则化器是可微的，并且在 $f_t \\in \\{0,1\\}$ 与 $g_t$ 对齐时最小化。在logistic sigmoid $f_t = \\sigma(a_t)$ 下，$\\mathcal{L}_{\\text{gate}}$ 对梯度的贡献是\n$$\n\\frac{\\partial \\mathcal{L}_{\\text{gate}}}{\\partial a_t} = \\lambda (f_t - g_t),\n$$\n这是稳定的，当 $f_t$ 错误时非零，并且仅在 $f_t$ 与 $g_t$ 匹配时才消失。讨论强调，在片段内部，当 $f_t \\approx 1$ 时，LSTM的单元状态路径支持长程梯度，而在边界处，$f_t \\approx 0$ 会有意地截断状态传递。它建议对 $\\lambda$ 进行退火，并可选择使用温度缩放的sigmoid函数 $f_t = \\sigma(a_t / \\tau)$，其中 $\\tau$ 在训练过程中减小，以避免过早的硬饱和，并允许与主预测损失进行协调。\n\nB. 令 $x_t$ 在 $x_t = 1$ 和 $x_t = -1$ 之间确定性地交替，并将 $y_t$ 定义为最近 $x_t = 1$ 值计数的奇偶性。使用一个简单的平方惩罚，\n$$\n\\mathcal{L}_{\\text{gate}} = \\lambda \\sum_t \\big( f_t - (1 - b_t) \\big)^2,\n$$\n这是可微的。声称仅此一项就可以防止梯度消失，即使 $f_t$ 在接近 0 或 1 时饱和，因为平方误差总是提供非零梯度。结论是，不需要退火或温度控制，并且在所有时间均匀地应用惩罚可确保通过门的梯度流不受阻碍。\n\nC. 在片段内选择 $x_t$ 作为随机变量，并将 $y_t$ 定义为直到时间 $t$ 的整个序列的平均值。添加一个与 $b_t$ 无关的无监督双阱正则化器，\n$$\n\\mathcal{L}_{\\text{gate}} = \\lambda \\sum_t f_t (1 - f_t),\n$$\n以在没有明确监督何时遗忘的情况下将 $f_t$ 推向 $\\{0,1\\}$。论证通过预测损失的反向传播将自动使门切换与 $b_t$ 对齐，因为最小化预测误差足以使 $f_t$ 与边界同步。\n\nD. 使用分段常数 $x_t$，在片段内具有固定值。将 $y_t$ 设置为在片段开始时观察到的最后一个 $x_{\\tau(t)}$。添加一个带边距的类似hinge的非平滑惩罚，\n$$\n\\mathcal{L}_{\\text{gate}} = \\lambda \\sum_t \\max\\big( 0, 1 - \\alpha \\lvert f_t - (1 - b_t) \\rvert \\big),\n$$\n其中 $\\alpha  0$。声称非平滑性是无害的，因为自动微分可以处理不可微点，并且边距将产生比平滑替代方案更强的梯度，从而消除sigmoid中的饱和问题并加速训练。\n\n选择最佳选项，并根据LSTM状态更新的基本定义、logistic sigmoid门的属性以及基于梯度的训练动态来证明您的选择。",
            "solution": "### 选项分析\n\n**选项A**\n- **合成序列与目标 ($y_t$):** 目标是片段内的累加和。这在科学上是合理的，因为它明确要求模型在片段内（$b_t = 0$）累积信息（需要 $f_t \\approx 1$），并在片段边界（$b_t = 1$）重置累加器（需要 $f_t \\approx 0$）。\n- **惩罚项:** 使用伯努利交叉熵损失来监督遗忘门 $f_t$ 趋向于目标 $g_t = 1 - b_t$。这是监督一个概率输出（sigmoid）趋向于二进制目标的标准且最合适的方法。\n- **梯度和训练动态:** 对预激活 $a_t$ 的梯度 $\\lambda(f_t - g_t)$ 是线性的，不会因为 sigmoid 的饱和而消失。这对于在整个训练过程中提供稳定的学习信号至关重要。对训练动态的讨论（退火、温度缩放）是现实且先进的，认识到平衡预测损失和门正则化的复杂性。这是一个非常全面和正确的解决方案。\n\n**选项B**\n- **惩罚项:** 使用平方误差（L2损失）。\n- **梯度和训练动态:** 主要缺陷在于其对训练动态的错误理解。平方误差损失对预激活的梯度包含一个 $\\sigma'(a_t) = f_t(1-f_t)$ 项。当 $f_t$ 饱和到接近0或1时，该梯度会消失，从而阻碍学习。因此，声称它“总是提供非零梯度”并且“不需要退火”是错误的。\n\n**选项C**\n- **合成序列与目标 ($y_t$):** 目标是整个序列的平均值。这个任务不需要在片段边界处重置记忆。实际上，它鼓励模型保持一个恒定的、长期的记忆来计算全局平均值。因此，任务与期望的门行为不匹配。\n- **惩罚项:** 双阱正则化器 `f_t(1-f_t)` 确实将 $f_t$ 推向0或1，但它没有提供何时应该取0或何时应该取1的监督信号。期望仅靠预测损失就能正确地引导这种切换是不现实的。\n\n**选项D**\n- **惩罚项:** Hinge-like损失（铰链损失）是非平滑的。虽然次梯度方法可以处理它，但训练可能会不稳定。声称它能“消除”饱和问题并产生“更强”的梯度是夸大其词且没有根据的。与交叉熵相比，它在理论和实践上都不是一个更优的选择。\n- **合成序列与目标 ($y_t$):** 任务是记住片段开始时的值。这个任务是合理的，但结合非平滑的惩罚项使其成为一个较差的选择。\n\n### 结论\n选项A提供了最科学、最完整、最正确的解决方案。它正确地设计了一个与期望行为相匹配的合成任务，选择了最适合监督门控行为的可微惩罚项（交叉熵），并对相关的训练动态给出了准确而深刻的分析。",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}