{
    "hands_on_practices": [
        {
            "introduction": "To understand how modern recurrent networks handle long sequences, we must first analyze the mechanisms that prevent information from decaying over time. This exercise isolates the function of the forget gate in a Long Short-Term Memory (LSTM) cell, the key innovation that mitigates the vanishing gradient problem. By manipulating the forget gate's bias, you will directly observe how it controls memory retention over long time lags, building a core intuition for gated architectures. ",
            "id": "3191179",
            "problem": "Consider a single-cell Long Short-Term Memory (LSTM) recurrence initialized to study the effect of the forget-gate bias on retaining information over long sequences. The standard cell update is defined by the following well-tested equations: the cell state $c_t$ evolves as $c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t$, where the forget gate $f_t = \\sigma(z_f(t))$, the input gate $i_t = \\sigma(z_i(t))$, and the candidate update $g_t = \\tanh(z_g(t))$. The logistic sigmoid function is $\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$, and the hyperbolic tangent is $\\tanh(x)$. At random initialization with zero-mean weights, one commonly controls the initial tendency of the forget gate to retain information by setting the forget-gate bias $b_f$.\n\nWe define a controlled, purely mathematical variant of the initialization regime to isolate retention dynamics:\n- For all time steps $t \\geq 1$, the pre-activations for the gates and candidate satisfy $z_f(t) = b_f$, $z_i(t) = 0$, and $z_g(t) = 0$, which implies $f_t = \\sigma(b_f)$, $i_t = \\sigma(0)$, and $g_t = \\tanh(0)$.\n- At time $t = 0$, a one-time write sets an initial memory $c_0$ with $c_0 = 1$ (dimensionless), and for $t \\geq 1$ there are no further writes, so $i_t \\cdot g_t = 0$ for $t \\geq 1$.\n\nStarting only from the above definitions and facts, derive the retention after $L$ steps, defined as the ratio $R(L, b_f) = \\dfrac{c_L}{c_0}$, and implement a program that computes this retention for a sweep of forget-gate bias values $b_f$ and dependency lengths $L$.\n\nTest Suite:\n- Forget-gate bias values $b_f \\in \\{-3, -1, 0, 1, 3\\}$, i.e., $b_f$ equals $-3$, $-1$, $0$, $1$, and $3$.\n- Dependency lengths $L \\in \\{0, 5, 20, 100, 200\\}$, i.e., $L$ equals $0$, $5$, $20$, $100$, and $200$.\n\nFor each ordered pair $(b_f, L)$, with $b_f$ iterated in ascending order followed by $L$ in ascending order (lexicographic order on the pair), compute the retention $R(L, b_f)$ as a real number and report it as a float rounded to $6$ decimal places. There are no physical units in this problem; all quantities are dimensionless. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact specified order of pairs: first $b_f = -3$ with all $L$, then $b_f = -1$ with all $L$, then $b_f = 0$ with all $L$, then $b_f = 1$ with all $L$, then $b_f = 3$ with all $L$. For example, the output format must be like \"[r_1,r_2,...,r_{25}]\" where each $r_k$ is the computed float for the corresponding test case. The answers for each test case are floats.",
            "solution": "The user wants me to solve a problem about LSTM cells.\n\n### Step 1: Problem Validation\n\nI will first validate the problem statement.\n\n#### Step 1.1: Extract Givens\n\nThe givens are:\n- LSTM cell state update equation: $c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t$\n- Forget gate activation: $f_t = \\sigma(z_f(t))$\n- Input gate activation: $i_t = \\sigma(z_i(t))$\n- Candidate update: $g_t = \\tanh(z_g(t))$\n- Logistic sigmoid function: $\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$\n- Hyperbolic tangent function: $\\tanh(x)$\n- Controlled model conditions for all time steps $t \\geq 1$:\n    - Pre-activation for forget gate: $z_f(t) = b_f$\n    - Pre-activation for input gate: $z_i(t) = 0$\n    - Pre-activation for candidate update: $z_g(t) = 0$\n- Condition on writes for $t \\geq 1$: $i_t \\cdot g_t = 0$\n- Initial condition at $t = 0$: $c_0 = 1$\n- Definition of retention: $R(L, b_f) = \\dfrac{c_L}{c_0}$\n- Test suite parameters:\n    - Forget-gate bias values $b_f \\in \\{-3, -1, 0, 1, 3\\}$\n    - Dependency lengths $L \\in \\{0, 5, 20, 100, 200\\}$\n- Output format: A comma-separated list of float values rounded to $6$ decimal places, for each pair $(b_f, L)$ in lexicographic order.\n\n#### Step 1.2: Validate Using Extracted Givens\n\nThe problem is subjected to validation against the established criteria.\n\n1.  **Scientific or Factual Soundness**: The problem is scientifically grounded. It uses the standard, universally accepted equations for a Long Short-Term Memory (LSTM) cell. The introduced simplifications ($z_f(t) = b_f$, $z_i(t) = 0$, $z_g(t) = 0$) are a well-defined mathematical construct designed to isolate and study the effect of the forget gate's bias on memory retention. This is a common and valid technique in scientific modeling to understand complex systems by analyzing simplified cases. The statement that for $t \\geq 1$, $i_t \\cdot g_t = 0$ is consistent with the given pre-activations, as $g_t = \\tanh(z_g(t)) = \\tanh(0) = 0$, which makes the product zero regardless of the value of $i_t$.\n\n2.  **Well-Posedness and Completeness**: The problem is well-posed and self-contained. It provides a clear initial condition ($c_0=1$), a deterministic recurrence relation for the cell state $c_t$, and a precise definition of the quantity to be calculated ($R(L, b_f)$). All necessary variables and functions are defined explicitly. The test cases are specified, and the required output format is unambiguous. This structure guarantees that a unique, stable, and meaningful solution exists and can be computed.\n\n3.  **Objectivity**: The problem is stated in precise, objective mathematical language. It is free from ambiguity, subjective claims, or opinion-based statements.\n\n#### Step 1.3: Verdict and Action\n\nThe problem statement is **valid**. It is a well-defined mathematical problem rooted in the established principles of deep learning. I will now proceed to derive the solution.\n\n### Step 2: Derivation and Solution\n\nThe objective is to derive an expression for the retention $R(L, b_f) = \\dfrac{c_L}{c_0}$ and compute its value for the given test suite.\n\nThe fundamental recurrence relation for the LSTM cell state is given as:\n$$c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t$$\n\nThe problem defines a simplified regime where for all time steps $t \\geq 1$, the input-write term is zero:\n$$i_t \\cdot g_t = 0 \\quad \\text{for } t \\geq 1$$\nThis simplification is consistent with the provided pre-activations, since $z_g(t) = 0$ implies $g_t = \\tanh(0) = 0$.\n\nSubstituting this condition into the cell state equation, the recurrence for $t \\geq 1$ simplifies to a purely multiplicative update:\n$$c_t = f_t \\cdot c_{t-1}$$\n\nThe forget gate activation, $f_t$, is determined by its pre-activation, $z_f(t)$. For $t \\geq 1$, this is held constant:\n$$z_f(t) = b_f$$\nTherefore, the forget gate's output $f_t$ is also constant for all $t \\geq 1$. Let us denote this constant value by $f$:\n$$f = f_t = \\sigma(z_f(t)) = \\sigma(b_f) = \\frac{1}{1 + e^{-b_f}}$$\n\nThe recurrence relation for $t \\geq 1$ now becomes a simple geometric progression with a constant factor $f$:\n$$c_t = f \\cdot c_{t-1}$$\n\nWe can unroll this recurrence starting from the initial state $c_0$:\n- For $t=1$: $c_1 = f \\cdot c_0$\n- For $t=2$: $c_2 = f \\cdot c_1 = f \\cdot (f \\cdot c_0) = f^2 \\cdot c_0$\n- For $t=3$: $c_3 = f \\cdot c_2 = f \\cdot (f^2 \\cdot c_0) = f^3 \\cdot c_0$\n\nBy induction, for any time step $L \\geq 1$, the cell state $c_L$ is given by:\n$$c_L = f^L \\cdot c_0$$\n\nThe retention $R(L, b_f)$ is defined as the ratio $\\dfrac{c_L}{c_0}$.\nFor $L \\geq 1$, we have:\n$$R(L, b_f) = \\frac{c_L}{c_0} = \\frac{f^L \\cdot c_0}{c_0} = f^L$$\nSubstituting the expression for $f$, we get the retention formula for $L \\geq 1$:\n$$R(L, b_f) = \\left(\\sigma(b_f)\\right)^L = \\left(\\frac{1}{1 + e^{-b_f}}\\right)^L$$\n\nA special case is at $L=0$. The retention after $0$ steps is, by definition:\n$$R(0, b_f) = \\frac{c_0}{c_0} = 1$$\nThis result is also consistent with the general formula if we adopt the convention that $x^0 = 1$.\n\nSo, the complete formula for retention is:\n$$\nR(L, b_f) =\n\\begin{cases}\n1 & \\text{if } L = 0 \\\\\n\\left(\\dfrac{1}{1 + e^{-b_f}}\\right)^L & \\text{if } L > 0\n\\end{cases}\n$$\n\nThe implementation will consist of a program that iterates through the specified pairs of $(b_f, L)$ in lexicographic order. For each pair, it will compute $R(L, b_f)$ using the derived formula, handling the $L=0$ case as a base condition. The results will be collected, formatted to $6$ decimal places, and printed as a single comma-separated string enclosed in square brackets. `numpy` is suitable for the exponential calculation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the memory retention in a simplified LSTM cell for a sweep of\n    forget-gate biases and dependency lengths.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    b_f_values = [-3, -1, 0, 1, 3]\n    L_values = [0, 5, 20, 100, 200]\n\n    # This structure ensures lexicographic ordering on the pair (b_f, L).\n    test_cases = [(b_f, L) for b_f in b_f_values for L in L_values]\n\n    results = []\n    \n    for b_f, L in test_cases:\n        # The retention R(L, b_f) is defined as c_L / c_0.\n        # The recurrence relation c_t = f * c_{t-1} for t >= 1 with c_0 given.\n        # This leads to c_L = f^L * c_0, so R(L, b_f) = f^L.\n        # The forget gate activation f is constant: f = sigma(b_f).\n\n        # Case 1: L = 0 (0 steps)\n        # By definition, R(0, b_f) = c_0 / c_0 = 1.\n        if L == 0:\n            retention = 1.0\n        # Case 2: L > 0\n        else:\n            # Calculate the sigmoid function for the given bias b_f.\n            # sigma(x) = 1 / (1 + e^(-x))\n            sigma_b_f = 1.0 / (1.0 + np.exp(-b_f))\n            \n            # Calculate retention as sigma(b_f) raised to the power of L.\n            retention = sigma_b_f ** L\n        \n        results.append(retention)\n\n    # Format the final output string as a comma-separated list of floats\n    # rounded to 6 decimal places, enclosed in square brackets.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    output_string = f\"[{','.join(formatted_results)}]\"\n    \n    # The final print statement must produce only the specified output format.\n    print(output_string)\n\nsolve()\n\n```"
        },
        {
            "introduction": "While gated RNNs improve upon their predecessors, the sequential nature of recurrence still imposes limits on capturing very long-range dependencies. This practice compares the fundamental principles of recurrent models against self-attention, the mechanism at the heart of the Transformer architecture. By analyzing their theoretical capabilities on a task requiring distant symbol matching, you will discover the structural advantages of attention's direct, parallel access to information across the entire sequence. ",
            "id": "3191175",
            "problem": "Consider the canonical nested-parentheses recognition task, which requires a stack-like memory to process hierarchical structure. Let the input be a sequence of length $T$ over the alphabet $\\{ \"(\", \")\" \\}$, encoded as $x_t \\in \\{+1, -1\\}$ for $t \\in \\{1,\\dots,T\\}$, where $+1$ denotes an opening parenthesis and $-1$ denotes a closing parenthesis. Define the cumulative sum prefix process $s_t = \\sum_{i=1}^t x_i$, with $s_0 = 0$. A string is balanced if and only if $s_T = 0$ and $s_t \\ge 0$ for all $t \\in \\{0,\\dots,T\\}$. The maximum nesting depth equals $\\max_{t} s_t$. The hardest balanced string of depth $D$ is the length-$T$ string with $T = 2D$ given by $D$ opening parentheses followed by $D$ closing parentheses, for which the first opening parenthesis must be matched to the last closing parenthesis, creating a dependency distance of $\\delta = 2D - 1$.\n\nYou will compare two abstracted model families on this task:\n1. A vanilla Recurrent Neural Network (RNN) with hidden state update $h_{t+1} = \\phi(W_h h_t + W_x x_{t+1} + b)$ and a final classifier that reads $h_T$. Assume the per-step Jacobian operator norm is bounded by a constant $\\lambda > 0$, i.e., $\\| J_t \\|_2 \\le \\lambda$ for all steps, where $J_t = \\partial h_{t+1}/\\partial h_t$. This is a standard, widely used bound with $\\lambda = \\| W_h \\|_2 L_\\phi$ for activation function Lipschitz constant $L_\\phi$.\n2. A single-layer dot-product self-attention mechanism with a local attention window of size $w \\in \\mathbb{N}$, meaning each position can directly attend only to positions within distance at most $w$. Let $p \\in \\{0, 1\\}$ indicate whether absolute positional encodings are present ($p=1$) or absent ($p=0$). Absence of positional encodings implies tokens with identical content are indistinguishable by position in a single attention layer.\n\nFrom first principles, use the following reasoning to decide whether each model can, in principle, succeed on the worst-case balanced string of depth $D$ without training, under the stated abstractions:\n- For the RNN, the chain rule with submultiplicativity of operator norms gives an upper bound on the influence of the earliest token on $h_T$ as\n$$\n\\left\\| \\frac{\\partial h_T}{\\partial h_0} \\right\\|_2 \\le \\prod_{t=0}^{T-1} \\|J_t\\|_2 \\le \\lambda^T.\n$$\nUsing a tolerance threshold $\\varepsilon = 10^{-3}$, declare the RNN as succeeding if and only if\n$$\n\\varepsilon \\le \\lambda^T \\le \\varepsilon^{-1},\n$$\nwhich rules out vanishing ($\\lambda^T < \\varepsilon$) and exploding ($\\lambda^T > \\varepsilon^{-1}$) regimes as failure modes for learning long-term dependencies.\n- For the single-layer local self-attention, the model can only directly relate tokens within distance $w$. The worst-case match distance is $\\delta = 2D - 1$. Therefore, the model succeeds only if $p = 1$ (positions are distinguishable) and $w \\ge \\delta$. Otherwise, it fails due to either locality ($w < \\delta$) or position-indistinguishability ($p=0$).\n\nYour task is to implement a program that, for each test case, applies the above criteria and returns a categorical integer code per case:\n- Return $3$ if both the RNN and attention succeed.\n- Return $2$ if only the RNN succeeds.\n- Return $1$ if only the attention model succeeds.\n- Return $0$ if both fail.\n\nUse the following test suite, which covers a general case, locality-limited attention, vanishing-gradient RNN, trade-offs at larger depth, lack of positional encoding, and a shallow edge case:\n- Case 1: $D = 5$, $\\lambda = 0.95$, $w = 10$, $p = 1$.\n- Case 2: $D = 5$, $\\lambda = 0.95$, $w = 5$, $p = 1$.\n- Case 3: $D = 50$, $\\lambda = 0.9$, $w = 200$, $p = 1$.\n- Case 4: $D = 100$, $\\lambda = 0.99$, $w = 50$, $p = 1$.\n- Case 5: $D = 100$, $\\lambda = 1.05$, $w = 1000$, $p = 0$.\n- Case 6: $D = 1$, $\\lambda = 0.5$, $w = 1$, $p = 1$.\n\nImplementation details:\n- For each case, compute $T = 2D$, $\\delta = 2D - 1$, and use $\\varepsilon = 10^{-3}$.\n- Determine the RNN success by the inequality on $\\lambda^T$ relative to $\\varepsilon$.\n- Determine attention success by checking $p = 1$ and $w \\ge \\delta$.\n- Encode the result per case into a single integer as defined above.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,\\dots,r_6]$, where each $r_i$ is the integer code for test case $i$ in the order listed above.",
            "solution": "The problem requires a comparative analysis of a vanilla Recurrent Neural Network (RNN) and a single-layer local self-attention model on the task of recognizing a specific class of balanced parenthesis strings. The analysis is based on abstract, first-principle criteria for handling long-term dependencies.\n\nFirst, we formalize the problem context. The input is a sequence $x_1, \\dots, x_T$ of length $T$ where $x_t=+1$ represents an opening parenthesis `(` and $x_t=-1$ represents a closing parenthesis `)`. The \"hardest\" balanced string of nesting depth $D$ is defined as $D$ opening parentheses followed by $D$ closing parentheses. This structure implies the sequence length is $T = 2D$. The critical dependency is between the first opening parenthesis at time $t=1$ and the last closing parenthesis at time $t=T=2D$. The distance of this dependency is $\\delta = T-1 = 2D-1$.\n\nThe success of each model is determined by a set of predefined criteria. We will analyze each model class according to these rules.\n\n**Model 1: Vanilla Recurrent Neural Network (RNN)**\n\nThe RNN processes the sequence step-by-step, updating its hidden state $h_t$ via the recurrence relation $h_{t+1} = \\phi(W_h h_t + W_x x_{t+1} + b)$. The ability of the network to learn long-range dependencies is fundamentally related to how information from early inputs propagates through this recurrence to influence the final state $h_T$. This propagation is mathematically described by the gradient of the final state with respect to an initial state, $\\frac{\\partial h_T}{\\partial h_0}$. Using the chain rule, this can be expressed as a product of Jacobians for each time step:\n$$\n\\frac{\\partial h_T}{\\partial h_0} = \\frac{\\partial h_T}{\\partial h_{T-1}} \\frac{\\partial h_{T-1}}{\\partial h_{T-2}} \\cdots \\frac{\\partial h_1}{\\partial h_0} = \\prod_{t=0}^{T-1} J_t\n$$\nwhere $J_t = \\frac{\\partial h_{t+1}}{\\partial h_t}$. Taking the operator norm and using its submultiplicative property ($\\|AB\\| \\le \\|A\\|\\|B\\|$), we get an upper bound on the norm of this gradient:\n$$\n\\left\\| \\frac{\\partial h_T}{\\partial h_0} \\right\\|_2 \\le \\prod_{t=0}^{T-1} \\|J_t\\|_2\n$$\nThe problem provides a uniform bound $\\|J_t\\|_2 \\le \\lambda$ for all $t$. This simplifies the expression to:\n$$\n\\left\\| \\frac{\\partial h_T}{\\partial h_0} \\right\\|_2 \\le \\lambda^T\n$$\nThis bound is central to the concepts of vanishing and exploding gradients. If $\\lambda < 1$, the bound $\\lambda^T$ decays exponentially to $0$ as the sequence length $T$ increases. This means the influence of early inputs can become vanishingly small, making it impossible for the model to learn long-term dependencies. This is the vanishing gradient problem. Conversely, if $\\lambda > 1$, the bound grows exponentially, which can lead to unstable training. This is the exploding gradient problem.\n\nThe success criterion for the RNN is defined to avoid these pathological regimes. Using a tolerance threshold $\\varepsilon = 10^{-3}$, the RNN is considered successful if and only if the gradient bound is within a stable range:\n$$\n\\varepsilon \\le \\lambda^T \\le \\varepsilon^{-1}\n$$\n\n**Model 2: Single-Layer Local Self-Attention**\n\nThe self-attention mechanism computes the representation of each token as a weighted average of other tokens' representations. In a local self-attention model with a window of size $w$, each token at position $i$ can only directly attend to (i.e., exchange information with) tokens in the range $[i-w, i+w]$. To solve the given task, the model must be able to associate the first opening parenthesis at $t=1$ with the last closing parenthesis at $t=T$. The distance between these tokens is $\\delta = T-1 = 2D-1$. For a single layer of local attention to create a direct path between these tokens, its window size $w$ must be large enough to span this distance, i.e., $w \\ge \\delta$.\n\nFurthermore, the problem specifies the hardest-case string consists of $D$ identical opening parentheses followed by $D$ identical closing parentheses. Without a mechanism to distinguish tokens by their position, a self-attention layer cannot differentiate between the first `(` and the second `(`, making it impossible to learn the correct nested structure. Absolute positional encodings provide this crucial information. The parameter $p=1$ indicates their presence, while $p=0$ indicates their absence.\n\nTherefore, the success of the single-layer local self-attention model is contingent on two conditions:\n$1$. Positional information must be available: $p=1$.\n$2$. The attention window must be wide enough to capture the longest dependency: $w \\ge \\delta$.\nThe model succeeds if and only if both conditions are met.\n\n**Case-by-Case Evaluation**\n\nWe now apply these criteria to each test case, using $\\varepsilon = 10^{-3}$.\n\n**Case 1:** $D=5$, $\\lambda = 0.95$, $w=10$, $p=1$.\n- Derived values: $T = 2D = 10$, $\\delta = 2D-1 = 9$.\n- RNN: We evaluate $\\lambda^T = 0.95^{10} \\approx 0.5987$. The condition is $0.001 \\le 0.5987 \\le 1000$. This is true. **RNN succeeds.**\n- Attention: We check $p=1$ (true) and $w \\ge \\delta$, which is $10 \\ge 9$ (true). **Attention succeeds.**\n- Result: Both succeed, Code = $3$.\n\n**Case 2:** $D=5$, $\\lambda = 0.95$, $w=5$, $p=1$.\n- Derived values: $T = 10$, $\\delta = 9$.\n- RNN: We evaluate $\\lambda^T = 0.95^{10} \\approx 0.5987$. The condition is $0.001 \\le 0.5987 \\le 1000$. This is true. **RNN succeeds.**\n- Attention: We check $p=1$ (true) and $w \\ge \\delta$, which is $5 \\ge 9$ (false). **Attention fails.**\n- Result: Only RNN succeeds, Code = $2$.\n\n**Case 3:** $D=50$, $\\lambda = 0.9$, $w=200$, $p=1$.\n- Derived values: $T = 2D = 100$, $\\delta = 2D-1 = 99$.\n- RNN: We evaluate $\\lambda^T = 0.9^{100} \\approx 2.656 \\times 10^{-5}$. This value is less than $\\varepsilon = 0.001$. The gradient vanishes. **RNN fails.**\n- Attention: We check $p=1$ (true) and $w \\ge \\delta$, which is $200 \\ge 99$ (true). **Attention succeeds.**\n- Result: Only Attention succeeds, Code = $1$.\n\n**Case 4:** $D=100$, $\\lambda = 0.99$, $w=50$, $p=1$.\n- Derived values: $T = 2D = 200$, $\\delta = 2D-1 = 199$.\n- RNN: We evaluate $\\lambda^T = 0.99^{200} \\approx 0.134$. The condition is $0.001 \\le 0.134 \\le 1000$. This is true. **RNN succeeds.**\n- Attention: We check $p=1$ (true) and $w \\ge \\delta$, which is $50 \\ge 199$ (false). **Attention fails.**\n- Result: Only RNN succeeds, Code = $2$.\n\n**Case 5:** $D=100$, $\\lambda = 1.05$, $w=1000$, $p=0$.\n- Derived values: $T = 200$, $\\delta = 199$.\n- RNN: We evaluate $\\lambda^T = 1.05^{200} \\approx 17293$. This value is greater than $\\varepsilon^{-1} = 1000$. The gradient explodes. **RNN fails.**\n- Attention: We check $p=1$. This is false ($p=0$). The model cannot distinguish token positions. **Attention fails.**\n- Result: Both fail, Code = $0$.\n\n**Case 6:** $D=1$, $\\lambda = 0.5$, $w=1$, $p=1$.\n- Derived values: $T = 2D = 2$, $\\delta = 2D-1 = 1$. The string is `()`.\n- RNN: We evaluate $\\lambda^T = 0.5^2 = 0.25$. The condition is $0.001 \\le 0.25 \\le 1000$. This is true. **RNN succeeds.**\n- Attention: We check $p=1$ (true) and $w \\ge \\delta$, which is $1 \\ge 1$ (true). **Attention succeeds.**\n- Result: Both succeed, Code = $3$.\n\nThese results are synthesized into a final list of integer codes.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Applies success criteria to an RNN and a local self-attention model\n    for a series of test cases on a parenthesis-matching task.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (D, lambda, w, p).\n    test_cases = [\n        (5, 0.95, 10, 1),  # Case 1\n        (5, 0.95, 5, 1),   # Case 2\n        (50, 0.9, 200, 1), # Case 3\n        (100, 0.99, 50, 1),# Case 4\n        (100, 1.05, 1000, 0),# Case 5\n        (1, 0.5, 1, 1),    # Case 6\n    ]\n\n    # The tolerance threshold epsilon.\n    epsilon = 1e-3\n\n    results = []\n    for D, lam, w, p in test_cases:\n        # Calculate derived problem parameters.\n        T = 2 * D\n        delta = 2 * D - 1\n\n        # --- RNN Success Criterion ---\n        # The RNN succeeds if its gradient norm bound lambda^T is within a stable range.\n        # This prevents vanishing (lambda^T < epsilon) and exploding (lambda^T > 1/epsilon) gradients.\n        lambda_T = np.power(lam, T)\n        rnn_succeeds = (epsilon <= lambda_T <= 1 / epsilon)\n\n        # --- Attention Success Criterion ---\n        # The single-layer local attention model succeeds if:\n        # 1. Positional encodings are present (p=1) to distinguish identical tokens.\n        # 2. The local window size (w) is large enough to span the maximum dependency distance (delta).\n        attention_succeeds = (p == 1 and w >= delta)\n\n        # --- Determine the final result code ---\n        # 3: both succeed\n        # 2: only RNN succeeds\n        # 1: only attention succeeds\n        # 0: both fail\n        if rnn_succeeds and attention_succeeds:\n            code = 3\n        elif rnn_succeeds and not attention_succeeds:\n            code = 2\n        elif not rnn_succeeds and attention_succeeds:\n            code = 1\n        else: # both fail\n            code = 0\n            \n        results.append(code)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The self-attention mechanism is powerful but inherently permutation-invariant, meaning it has no built-in sense of sequence order. This practice explores sinusoidal positional encodings, a classic method for injecting positional information into Transformer models. You will investigate how these periodic signals allow a model to learn and extrapolate sequence patterns, and critically, discover the failure modes that arise when a pattern's structure falls outside the model's representational capacity. ",
            "id": "3191166",
            "problem": "You are asked to implement and analyze a synthetic linear predictor that uses sinusoidal positional encodings to model sequences with long-term dependencies. The core objective is to probe the periodicity of positional encodings and to create tasks that require extrapolation beyond the training horizon, including cases where extrapolation is mathematically impossible due to aliasing. All angles must be expressed in radians.\n\nFundamental base definitions:\n- A discrete-time position index is denoted by $t \\in \\mathbb{Z}$.\n- For an integer period $N \\in \\mathbb{Z}_{>0}$, the standard sine function $y(t) = \\sin\\left(\\frac{2\\pi}{N} t\\right)$ is periodic with period $N$ over integer $t$.\n- A sinusoidal positional encoding using two integer periods $N_1$ and $N_2$ is defined for each position $t$ by the vector\n$$\np(t; N_1, N_2) = \\left[\n\\sin\\left(\\frac{2\\pi}{N_1} t\\right),\n\\cos\\left(\\frac{2\\pi}{N_1} t\\right),\n\\sin\\left(\\frac{2\\pi}{N_2} t\\right),\n\\cos\\left(\\frac{2\\pi}{N_2} t\\right)\n\\right].\n$$\n- The least common multiple of $N_1$ and $N_2$ is denoted by $L = \\mathrm{lcm}(N_1, N_2)$.\n\nModel and learning setup:\n- The predictor is linear in the encoding: $\\hat{y}(t) = w^\\top p(t; N_1, N_2)$ for a parameter vector $w \\in \\mathbb{R}^4$.\n- Training is performed by minimizing the sum of squared errors over a contiguous set of positions $\\{0, 1, \\dots, T_{\\mathrm{train}} - 1\\}$ where $T_{\\mathrm{train}} \\in \\mathbb{Z}_{>0}$.\n- The learned parameter $w$ is then evaluated on test positions $\\{L, L+1, \\dots, L+T_{\\mathrm{test}}-1\\}$ with $T_{\\mathrm{test}} \\in \\mathbb{Z}_{>0}$ to assess extrapolation beyond the training horizon. This choice of test positions is exactly one full encoding super-period ahead, so $p(t + L; N_1, N_2) = p(t; N_1, N_2)$ for all integer $t$.\n\nAngle unit requirement:\n- All trigonometric arguments must be expressed in radians.\n\nTargets to probe extrapolation:\n- Case A (happy path): $y_A(t) = \\sin\\left(\\frac{2\\pi}{N_1} t\\right)$. This target aligns with one encoding frequency.\n- Case B (boundary condition): $y_B(t) = \\sin\\left(\\frac{2\\pi}{L} t\\right)$. This target has the longest period shared exactly by the encoding’s super-period across integer $t$.\n- Case C (edge case requiring period extrapolation): $y_C(t) = \\sin\\left(\\frac{2\\pi}{L+1} t\\right)$. This target’s period does not divide the encoding super-period and induces aliasing when extrapolating to $t+L$.\n\nTest suite (parameters):\n- Use $N_1 = 12$, $N_2 = 25$, $L = \\mathrm{lcm}(N_1, N_2) = 300$, $T_{\\mathrm{train}} = 200$, $T_{\\mathrm{test}} = 50$.\n- Evaluate the Mean Absolute Error (MAE) on the test positions for each case:\n$$\n\\mathrm{MAE} = \\frac{1}{T_{\\mathrm{test}}} \\sum_{i=0}^{T_{\\mathrm{test}}-1} \\left| \\hat{y}(L + i) - y(L + i) \\right|.\n$$\n\nProgram requirements:\n- Construct $p(t; N_1, N_2)$ for $t \\in \\{0, 1, \\dots, T_{\\mathrm{train}} - 1\\}$ and solve for $w$ via least squares for each of the three targets $y_A$, $y_B$, $y_C$.\n- Compute the MAE on the test positions $\\{L, L+1, \\dots, L+T_{\\mathrm{test}}-1\\}$ for each target.\n- Your program should produce a single line of output containing the three MAE values corresponding to Case A, Case B, and Case C, as a comma-separated list enclosed in square brackets (for example, $[\\text{mae}_A,\\text{mae}_B,\\text{mae}_C]$).\n\nOutput format:\n- A single line with the results as a comma-separated list enclosed in square brackets, in the order $[\\mathrm{MAE}_A, \\mathrm{MAE}_B, \\mathrm{MAE}_C]$.",
            "solution": "The problem is well-defined, scientifically grounded, and provides all necessary information to proceed with a solution. The core task involves implementing a linear regression model with sinusoidal positional encodings and evaluating its extrapolation performance on three distinct target functions. The solution is derived by applying the principles of linear least squares and analyzing the periodicity of both the model and the target signals.\n\nThe methodological steps are as follows:\n\n1.  **Formulation as a Linear Least Squares Problem**: The predictor model is given by $\\hat{y}(t) = w^\\top p(t; N_1, N_2)$, which is linear in the unknown weight vector $w \\in \\mathbb{R}^4$. The goal is to find the $w$ that minimizes the sum of squared errors on a training set of time steps $\\{0, 1, \\dots, T_{\\mathrm{train}} - 1\\}$. This can be expressed in matrix form as minimizing $\\|\\mathbf{y} - Xw\\|_2^2$, where $X$ is the design matrix of size $T_{\\mathrm{train}} \\times 4$, with its $t$-th row being the encoding vector $p(t; N_1, N_2)^\\top$, and $\\mathbf{y}$ is the vector of target values over the training set.\n\n    The solution to this standard problem is $w = (X^\\top X)^{-1}X^\\top \\mathbf{y}$. This is numerically solved using standard library functions like `numpy.linalg.lstsq`, which typically employ robust methods such as Singular Value Decomposition (SVD).\n\n2.  **Data Generation**:\n    -   **Training Data**: We first construct the training time vector $t_{\\mathrm{train}} = [0, 1, \\dots, T_{\\mathrm{train}} - 1]$ where $T_{\\mathrm{train}} = 200$. The $T_{\\mathrm{train}} \\times 4$ design matrix $X$ is populated by evaluating the four components of the positional encoding $p(t; N_1, N_2)$ for each $t \\in t_{\\mathrm{train}}$ with $N_1 = 12$ and $N_2 = 25$. For each of the three cases (A, B, C), a corresponding target vector $\\mathbf{y}_{\\mathrm{train}}$ is generated by evaluating the respective target function ($y_A(t)$, $y_B(t)$, or $y_C(t)$) at each time step in $t_{\\mathrm{train}}$.\n    -   **Test Data**: The test time steps are $t_{\\mathrm{test}} = [L, L+1, \\dots, L+T_{\\mathrm{test}}-1]$, where $L = \\mathrm{lcm}(12, 25) = 300$ and $T_{\\mathrm{test}} = 50$.\n\n3.  **Model Training and Prediction**:\n    -   For each case, the optimal weight vector $w$ is computed by solving the least squares problem using the training matrix $X$ and the corresponding training target vector $\\mathbf{y}_{\\mathrm{train}}$.\n    -   Predictions on the test set are generated using the learned $w$. A key insight is the periodicity of the encoding: $p(t+L; N_1, N_2) = p(t; N_1, N_2)$. This implies that the model's prediction at time $L+i$ is identical to its prediction at time $i$: $\\hat{y}(L+i) = w^\\top p(L+i; N_1, N_2) = w^\\top p(i; N_1, N_2)$. Therefore, the vector of predictions for the test set is obtained by applying the learned model to the encodings for time steps $i \\in \\{0, 1, \\dots, T_{\\mathrm{test}}-1\\}$.\n\n4.  **Error Evaluation and Analysis**: The performance is measured by the Mean Absolute Error (MAE) between the model's predictions and the true target values on the test set.\n    -   **Case A**: The target is $y_A(t) = \\sin\\left(\\frac{2\\pi}{N_1} t\\right)$. This function is one of the basis functions in the encoding $p(t)$. Thus, the least squares fit should be nearly perfect, yielding $w \\approx [1, 0, 0, 0]^\\top$. Since $y_A(t)$ is periodic with period $N_1$, it is also periodic with period $L = 25 \\times N_1$. Consequently, $y_A(L+i) = y_A(i)$. Both the prediction and the target are periodic, leading to an expected MAE close to $0$.\n    -   **Case B**: The target is $y_B(t) = \\sin\\left(\\frac{2\\pi}{L} t\\right)$. This function has a period $L$ that matches the super-period of the encoding. The model learns the best possible approximation of $y_B(t)$ on the training interval. Because both the learned model and the target function share the same super-period $L$, the extrapolation error on the test set will be of the same magnitude as the approximation error on the first $T_{\\mathrm{test}}$ points of the training set. The MAE is expected to be small but non-zero.\n    -   **Case C**: The target is $y_C(t) = \\sin\\left(\\frac{2\\pi}{L+1} t\\right)$. Its period, $L+1$, does not match the model's inherent periodicity of $L$. The model learns an approximation of $y_C(t)$ over the training set $\\{0, \\dots, T_{\\mathrm{train}}-1\\}$ and assumes this pattern repeats every $L$ steps. However, the true signal $y_C(t)$ does not repeat. The prediction at time $L+i$ will be an approximation of $y_C(i)$, but the target is $y_C(L+i) = \\sin\\left(\\frac{2\\pi(L+i)}{L+1}\\right)$, which exhibits a significant phase shift relative to $y_C(i)$. This mismatch between the model's periodic extrapolation and the non-periodic reality of the target (relative to period $L$) will result in a large MAE, demonstrating a failure of extrapolation due to aliasing.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes a synthetic linear predictor with sinusoidal positional encodings.\n    \"\"\"\n    # Define test suite parameters from the problem statement.\n    N1 = 12\n    N2 = 25\n    L = 300  # lcm(12, 25)\n    T_train = 200\n    T_test = 50\n\n    # 1. Construct the training design matrix X_train\n    # Time steps for training: t = 0, 1, ..., T_train - 1\n    t_train = np.arange(T_train)\n    \n    # Calculate angular frequencies for training\n    omega1_train = 2 * np.pi * t_train / N1\n    omega2_train = 2 * np.pi * t_train / N2\n\n    # Assemble the design matrix, shape (T_train, 4)\n    X_train = np.column_stack([\n        np.sin(omega1_train),\n        np.cos(omega1_train),\n        np.sin(omega2_train),\n        np.cos(omega2_train)\n    ])\n\n    # 2. Define the target functions for the three cases\n    target_functions = {\n        'A': lambda t: np.sin(2 * np.pi * t / N1),\n        'B': lambda t: np.sin(2 * np.pi * t / L),\n        'C': lambda t: np.sin(2 * np.pi * t / (L + 1))\n    }\n    \n    mae_results = []\n\n    # 3. Iterate through each case, train the model, and evaluate\n    for case_id in ['A', 'B', 'C']:\n        # Get the target function for the current case\n        target_func = target_functions[case_id]\n        \n        # Generate the training target vector y_train\n        y_train = target_func(t_train)\n        \n        # Solve for the weight vector w using linear least squares\n        # w = (X^T X)^-1 X^T y\n        w, _, _, _ = np.linalg.lstsq(X_train, y_train, rcond=None)\n        \n        # 4. Evaluate the model on the test set\n        \n        # Generate model predictions for the test set.\n        # Due to periodicity p(t+L) = p(t), the model's output is also periodic.\n        # So, y_hat(L+i) = w^T p(L+i) = w^T p(i).\n        # We can reuse the first T_test rows of the training design matrix.\n        X_test = X_train[:T_test]\n        y_hat_test = X_test @ w\n        \n        # Generate the true target values for the test set.\n        # Test time steps are t = L, L+1, ..., L+T_test-1\n        t_test = np.arange(L, L + T_test)\n        y_test_true = target_func(t_test)\n        \n        # Calculate the Mean Absolute Error (MAE)\n        mae = np.mean(np.abs(y_hat_test - y_test_true))\n        mae_results.append(mae)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, mae_results))}]\")\n\nsolve()\n```"
        }
    ]
}