{
    "hands_on_practices": [
        {
            "introduction": "动量（Momentum）的概念通常通过类比来解释，但是超参数 $\\beta$ 究竟在*定量*上控制了什么？本练习将通过将动量更新视为梯度的指数移动平均（Exponential Moving Average, EMA）来揭示 $\\beta$ 的奥秘。你将从第一性原理出发，推导 $\\beta$ 如何决定优化器的“记忆”长度，并验证一个常见的启发式规则，即有效平均窗口长度约为 $\\frac{1}{1-\\beta}$。",
            "id": "3154008",
            "problem": "要求您形式化并验证在深度学习中基于动量的优化方法所使用的指数移动平均（EMA）的有效平均窗口长度这一概念。考虑在动量法和自适应矩估计等方法中用于梯度的标准EMA估计器，其递归定义如下：在离散时间步索引 $t$，估计值 $m_t$ 使用衰减参数 $\\beta \\in [0,1)$ 和标量梯度信号 $g_t$ 进行更新，该更新遵循一个线性递归关系，该关系结合了前一个估计值 $m_{t-1}$ 和当前的 $g_t$。假设 $m_0 = 0$。\n\n您的任务是：\n\n- 从给定的递归关系和线性时不变系统的叠加原理出发，定义脉冲响应权重 $w_k$，该权重用以量化 $k$ 步之前的梯度对当前EMA估计值的贡献。论证这些权重构成一个和为 $1$ 的非负序列，并将其解释为非负整数上的概率质量函数。\n\n- 仅使用关于几何级数的基本性质，定义有效平均窗口长度的两个量化概念：\n  1. 累积质量窗口长度 $L_p$，对于一个固定的阈值 $p \\in (0,1)$，定义为满足 $\\sum_{k=0}^{L-1} w_k \\ge p$ 的最小整数 $L$。对于本问题，使用 $p = 0.95$ 并将其视为一个无单位的小数十进制阈值。\n  2. 有效样本量 $N_{\\mathrm{eff}}$，定义为 $N_{\\mathrm{eff}} = 1/\\sum_{k=0}^{\\infty} w_k^2$，它对应于在独立同分布噪声下，能够产生与几何加权平均相同的方差缩减效果的等权重样本数量。\n\n- 推导一个“有效平均窗口长度”的简单闭式基准。该基准通过将归一化权重视为概率质量函数并使用一阶矩来定义特征窗口长度而获得。此基准直接源于几何级数的求和。除几何级数性质外，不要假设任何外部结论。\n\n- 设计一个计算验证任务：对于一组衰减参数 $\\beta$，计算两种有效窗口长度度量与基于几何级数的简单基准的比率。具体来说，对于每个 $\\beta$，计算\n  - $r_L = L_{0.95} \\,/\\, \\text{baseline}$，\n  - $r_N = N_{\\mathrm{eff}} \\,/\\, \\text{baseline}$。\n\n您的程序必须实现上述量，并为以下衰减参数测试集生成结果，该测试集探测了正常路径和类边界区域：\n- $\\beta \\in \\{\\, 0.0,\\, 0.5,\\, 0.9,\\, 0.99,\\, 0.999 \\,\\}$。\n\n需要考虑的边界情况：\n- 当 $\\beta = 0$ 时，EMA仅简化为当前梯度。\n- 当 $\\beta \\to 1^{-}$ 时，EMA保留非常长的记忆，有效窗口长度变得非常大。\n\n实现细节和输出格式要求：\n- 您的程序必须是自包含的，并且除了下面描述的最后一行外，不产生任何其他输出。\n- 在所有计算中，使用表示为小数的 $p = 0.95$。\n- 对于给定测试集中的每个 $\\beta$，计算如上定义的 $r_L$ 和 $r_N$。\n- 将最终结果报告为单行，其中包含一个列表的列表，每个 $\\beta$ 对应一个内部列表，顺序与上面列出的一致，每个内部列表包含两个比率 $[r_L, r_N]$，作为四舍五入到三位小数的浮点数。\n- 要求的最终输出格式为：\n  - 单独一行：一个Python列表的列表的字符串表示，例如 $[[a_{11},a_{12}],[a_{21},a_{22}],\\dots]$，其中每个 $a_{ij}$ 是一个四舍五入到三位小数的十进制数，不含任何额外文本。\n\n本问题不涉及物理单位。不使用角度。百分比必须表示为小数，而不是使用百分号。整个问题纯粹是数学和算法问题，基于EMA的定义和几何级数的性质。通过对接近 $1$ 的 $\\beta$ 值使用精确或可证明收敛的级数操作而不是简单的截断，来确保科学真实性和数值稳定性。",
            "solution": "该问题要求对深度学习中使用的指数移动平均（EMA）的有效平均窗口长度概念进行形式化和验证。我们将首先推导EMA滤波器的脉冲响应，然后基于基本原理定义三个不同的有效窗口长度度量，最后为一组给定的参数计算这些度量的比率。\n\nEMA更新规则是一个线性递归，它根据前一个估计值 $m_{t-1}$ 和当前输入信号 $g_t$（例如梯度）来计算时间步 $t$ 的新估计值 $m_t$。参数 $\\beta \\in [0,1)$ 控制衰减率。问题指出，过去梯度的权重之和必须为 $1$，这唯一地确定了递归的标准形式：\n$$m_t = \\beta m_{t-1} + (1-\\beta) g_t$$\n给定初始条件 $m_0 = 0$，我们可以展开此递归，将 $m_t$ 表示为所有过去梯度的加权和：\n$$m_t = (1-\\beta)g_t + \\beta m_{t-1} = (1-\\beta)g_t + \\beta((1-\\beta)g_{t-1} + \\beta m_{t-2})$$\n$$m_t = (1-\\beta)g_t + (1-\\beta)\\beta g_{t-1} + (1-\\beta)\\beta^2 g_{t-2} + \\dots$$\n在稳态极限下（或对于已运行许多步骤的过程，$t \\to \\infty$），当前估计值 $m_t$ 是所有过去梯度的叠加，延伸至 $k \\to \\infty$：\n$$m_t = \\sum_{k=0}^{\\infty} (1-\\beta)\\beta^k g_{t-k}$$\n因此，脉冲响应权重 $w_k$（量化了来自 $k$ 步之前的梯度 $g_{t-k}$ 的贡献）由下式给出：\n$$w_k = (1-\\beta)\\beta^k, \\quad \\text{for } k \\in \\{0, 1, 2, \\dots\\}$$\n这些权重在非负整数上构成了一个有效的概率质量函数。首先，由于 $\\beta \\in [0, 1)$，$(1-\\beta)$ 和 $\\beta^k$ 都是非负的，因此对于所有 $k$ 都有 $w_k \\ge 0$。其次，权重之和是一个几何级数：\n$$\\sum_{k=0}^{\\infty} w_k = \\sum_{k=0}^{\\infty} (1-\\beta)\\beta^k = (1-\\beta) \\sum_{k=0}^{\\infty} \\beta^k = (1-\\beta) \\left( \\frac{1}{1-\\beta} \\right) = 1$$\n这个序列 $w_k$ 表示梯度“年龄”（以时间步为单位）的几何分布。\n\n问题要求从该分布的一阶矩推导基准窗口长度。该概率分布的一阶矩（或均值）$\\mu$ 是信息的期望年龄：\n$$\\mu = \\mathbb{E}[k] = \\sum_{k=0}^{\\infty} k \\, w_k = \\sum_{k=0}^{\\infty} k (1-\\beta) \\beta^k = (1-\\beta) \\sum_{k=0}^{\\infty} k \\beta^k$$\n该和式可以使用几何级数公式的导数来求值：$\\sum_{k=0}^{\\infty} k x^k = x/(1-x)^2$。\n$$\\mu = (1-\\beta) \\frac{\\beta}{(1-\\beta)^2} = \\frac{\\beta}{1-\\beta}$$\n这个平均年龄 $\\mu$ 可以用来定义一个特征窗口长度。简单地使用 $\\mu$ 会有问题，因为当 $\\beta=0$ 时，$\\mu=0$，这不是一个有意义的长度，并且会在所需的比率计算中导致除以零。一个更稳健的定义，也与常见的启发式方法一致，是将基准长度定义为 $L_{\\text{baseline}} = \\mu + 1$。这表示从索引 $0$ 到平均索引 $\\mu$ 的样本数量。\n$$L_{\\text{baseline}} = \\mu + 1 = \\frac{\\beta}{1-\\beta} + 1 = \\frac{\\beta + 1 - \\beta}{1-\\beta} = \\frac{1}{1-\\beta}$$\n这个定义在 $\\beta=0$ 时得出长度为 $1$，并在 $\\beta \\to 1^-$ 时增长到无穷大，这对窗口长度来说是合理的行为。\n\n接下来，我们形式化两个指定的有效窗口长度度量。\n\n1.  累积质量窗口长度 $L_p$，是使得前 $L$ 个权重之和达到阈值 $p$ 的最小整数 $L$。我们给定 $p=0.95$。\n    $$\\sum_{k=0}^{L-1} w_k = \\sum_{k=0}^{L-1} (1-\\beta)\\beta^k = (1-\\beta) \\frac{1-\\beta^L}{1-\\beta} = 1-\\beta^L$$\n    我们寻求满足 $1-\\beta^L \\ge p$ 的最小整数 $L$。\n    $$1 - p \\ge \\beta^L$$\n    对于 $\\beta \\in (0,1)$，我们取对数，由于 $\\log(\\beta)$ 是负数，不等式方向反转：\n    $$\\log(1-p) \\le L \\log(\\beta) \\implies L \\ge \\frac{\\log(1-p)}{\\log(\\beta)}$$\n    由于 $L$ 必须是整数，我们对此表达式取上取整。对于特殊情况 $\\beta=0$，$w_0=1$ 且所有其他权重为 $0$，因此对于任何 $L \\ge 1$，和都为 $1$；因此，最小的 $L$ 是 $1$。\n    $$L_p = \\begin{cases} 1 & \\text{if } \\beta=0 \\\\ \\left\\lceil \\frac{\\log(1-p)}{\\log(\\beta)} \\right\\rceil & \\text{if } \\beta \\in (0,1) \\end{cases}$$\n    当 $p=0.95$ 时，对于 $\\beta > 0$，此式变为 $L_{0.95} = \\lceil \\log(0.05)/\\log(\\beta) \\rceil$。\n\n2.  有效样本量 $N_{\\text{eff}}$，定义为 $N_{\\text{eff}} = 1/\\sum_{k=0}^{\\infty} w_k^2$。此度量衡量的是能够产生相同方差缩减效果的等效独立样本数。\n    $$\\sum_{k=0}^{\\infty} w_k^2 = \\sum_{k=0}^{\\infty} ((1-\\beta)\\beta^k)^2 = (1-\\beta)^2 \\sum_{k=0}^{\\infty} (\\beta^2)^k$$\n    这是另一个公比为 $\\beta^2$ 的几何级数，其收敛于 $1/(1-\\beta^2)$。\n    $$\\sum_{k=0}^{\\infty} w_k^2 = (1-\\beta)^2 \\frac{1}{1-\\beta^2} = (1-\\beta)^2 \\frac{1}{(1-\\beta)(1+\\beta)} = \\frac{1-\\beta}{1+\\beta}$$\n    因此，有效样本量为：\n    $$N_{\\text{eff}} = \\frac{1}{(1-\\beta)/(1+\\beta)} = \\frac{1+\\beta}{1-\\beta}$$\n    此公式对所有 $\\beta \\in [0,1)$ 均有效。\n\n最后，我们为计算任务计算所需的比率。\n第一个比率是 $r_L = L_{0.95} / L_{\\text{baseline}}$：\n$$r_L = \\frac{L_{0.95}}{L_{\\text{baseline}}} = (1-\\beta) L_{0.95} = (1-\\beta) \\left\\lceil \\frac{\\log(0.05)}{\\log(\\beta)} \\right\\rceil \\quad (\\text{for } \\beta > 0)$$\n在极限 $\\beta \\to 1^-$ 下，令 $\\beta = 1-\\epsilon$ 且 $\\epsilon \\to 0^+$。则 $\\log(\\beta) \\approx -\\epsilon$。上取整内部的项变为 $\\log(0.05)/(-\\epsilon) \\approx 2.9957/\\epsilon$。该比率趋近于 $r_L \\approx \\epsilon (\\lceil 2.9957/\\epsilon \\rceil) \\to 2.9957 = \\log(20)$。\n\n第二个比率是 $r_N = N_{\\text{eff}} / L_{\\text{baseline}}$：\n$$r_N = \\frac{N_{\\text{eff}}}{L_{\\text{baseline}}} = \\frac{(1+\\beta)/(1-\\beta)}{1/(1-\\beta)} = 1+\\beta$$\n这个非常简洁的结果对所有 $\\beta \\in [0,1)$ 均有效。当 $\\beta \\to 1^-$ 时，该比率趋近于 $2$。\n\nPython代码将实现这些推导出的公式，以计算指定 $\\beta$ 值测试集的比率。对于 $\\beta=0$，使用推导出的特殊处理：$L_{0.95}(0)=1$，$N_{\\text{eff}}(0)=1$ 和 $L_{\\text{baseline}}(0)=1$，得出 $r_L=1$ 和 $r_N=1$。",
            "answer": "```python\nimport numpy as np\nimport math\n\n# The problem is valid. Proceeding with the solution.\n\ndef solve():\n    \"\"\"\n    Computes effective window length ratios for an Exponential Moving Average (EMA).\n    \n    This function calculates two ratios, r_L and r_N, for a given set of EMA\n    decay parameters (beta).\n    \n    r_L = L_0.95 / L_baseline\n    r_N = N_eff / L_baseline\n    \n    where:\n    - L_0.95 is the cumulative-mass window length for p=0.95.\n    - N_eff is the effective sample size.\n    - L_baseline is a baseline window length derived from the first moment.\n    \"\"\"\n    \n    # Test suite of decay parameters\n    betas = [0.0, 0.5, 0.9, 0.99, 0.999]\n    \n    # Threshold for cumulative-mass window length\n    p = 0.95\n    log_1_minus_p = np.log(1.0 - p)\n\n    results = []\n    for beta in betas:\n        # Handle the edge case beta = 0 separately to avoid log(0)\n        if beta == 0.0:\n            # For beta=0, EMA is just the current gradient. All window lengths are 1.\n            # L_baseline = 1/(1-0) = 1\n            # L_0.95 = 1 (since w_0 = 1 >= 0.95)\n            # N_eff = (1+0)/(1-0) = 1\n            r_L = 1.0\n            r_N = 1.0\n            results.append([round(r_L, 3), round(r_N, 3)])\n            continue\n\n        # Derived formulas from the solution section\n        \n        # 1. Baseline window length: L_baseline = 1 / (1 - beta)\n        L_baseline = 1.0 / (1.0 - beta)\n        \n        # 2. Cumulative-mass window length: L_0.95 = ceil(log(1-p) / log(beta))\n        L_095 = np.ceil(log_1_minus_p / np.log(beta))\n        \n        # 3. Effective sample size: N_eff = (1 + beta) / (1 - beta)\n        N_eff = (1.0 + beta) / (1.0 - beta)\n        \n        # Compute the ratios\n        r_L = L_095 / L_baseline\n        # r_N can be computed directly as 1 + beta, but we compute it from\n        # the definitions as a cross-check, which is more robust.\n        r_N = N_eff / L_baseline\n\n        # Round results to three decimal places before storing\n        results.append([round(r_L, 3), round(r_N, 3)])\n        \n    # Format the output into the required single-line string representation\n    # of a list of lists, with no spaces inside the inner lists.\n    # e.g., [[1.0,1.0],[2.5,1.5],...]\n    inner_strings = [str(item).replace(' ', '') for item in results]\n    final_output_string = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "优化器在损失曲面上的路径是一个动态过程。本练习将动量优化与物理世界中的阻尼振子联系起来，从而建立一种强大的直觉。通过分析一个简单的一维问题，你将看到学习率 $\\eta$ 和动量系数 $\\beta$ 如何共同决定优化器是平滑下降、过冲并振荡，还是甚至发散，为超参数调整提供坚实的基础。",
            "id": "3154030",
            "problem": "考虑一维四次目标函数 $f(\\theta)=\\frac{1}{4}\\theta^4$ 以及由学习率 $\\eta>0$ 和动量系数 $\\beta\\ge 0$ 定义的经典动量更新。经典动量迭代由两个耦合方程给出：一个速度更新和一个参数更新。用符号表示，对于 $t\\in\\{0,1,2,\\dots\\}$，\n$$\nv_{t+1}=\\beta\\,v_t+\\nabla f(\\theta_t)\\quad\\text{and}\\quad \\theta_{t+1}=\\theta_t-\\eta\\,v_{t+1}.\n$$\n从这些核心定义出发，在没有额外假设的情况下，推导出仅含 $\\theta_t$ 的等价二阶差分方程。然后，通过在参考振幅 $a>0$ 周围对非线性项进行局部线性化，将此离散时间动态系统与阻尼振子的行为联系起来。具体来说，在推导出的二阶差分方程中使用近似 $\\theta_t^3\\approx a^2\\,\\theta_t$，以获得一个线性常系数递归关系，并分析其特征方程。从第一性原理出发，描述参数对 $(\\eta,\\beta)$ 如何控制参考振幅 $a$ 附近的定性行为：无超调的单调下降、有超调的阻尼振荡、稳定边界上的持续振荡以及发散。你的描述必须用关于 $(\\eta,\\beta)$ 和参考振幅 $a$ 的不等式来表示，并且必须与特征多项式的根结构以及阻尼状态（过阻尼、临界阻尼、欠阻尼）的离散时间模拟相关联。\n\n你的程序必须实现此分类规则，以对几个测试用例进行分类。对于每个测试用例，将参考振幅 $a$ 视为线性化点，并通过分析线性化二阶递归的特征多项式来计算分类。分类整数必须遵循以下编码：\n- $0$：单调下降（无超调），对应于非振荡衰减，其根为单位圆盘内严格的实数、非负根。\n- $1$：阻尼振荡（有超调），对应于振荡衰减；当谱半径严格小于一且根结构产生符号交替或共轭复根时发生。\n- $2$：持续振荡（稳定边界），对应于单位模长的主要根，即谱半径等于一。\n- $3$：发散，对应于谱半径严格大于一。\n\n你的程序应评估以下测试套件，其中每个测试用例是一个三元组 $(\\eta,\\beta,a)$，且 $a>0$：\n- $(\\eta,\\beta,a)=\\left(0.05,\\,0.5,\\,1.0\\right)$\n- $(\\eta,\\beta,a)=\\left(0.3,\\,0.9,\\,1.0\\right)$\n- $(\\eta,\\beta,a)=\\left(3.0,\\,0.5,\\,1.0\\right)$\n- $(\\eta,\\beta,a)=\\left(4.0,\\,0.2,\\,1.0\\right)$\n- $(\\eta,\\beta,a)=\\left(0.5,\\,0.0,\\,1.0\\right)$\n- $(\\eta,\\beta,a)=\\left(1.2,\\,0.0,\\,1.0\\right)$\n\n不涉及物理单位。不使用角度。结果必须是纯数值的。你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，顺序与测试套件相同。例如，如果你的分类依次为 $r_1,\\dots,r_6$，则打印该行 $\\left[\\text{r}_1,\\text{r}_2,\\text{r}_3,\\text{r}_4,\\text{r}_5,\\text{r}_6\\right]$，不带任何附加文本。",
            "solution": "该问题要求描述经典动量优化算法在一维四次目标函数 $f(\\theta) = \\frac{1}{4}\\theta^4$ 上的动力学特性。这涉及推导一个二阶差分方程，对其进行线性化，并根据其参数分析其稳定性和行为。\n\n### 第1步：推导二阶差分方程\n\n我们从给定的经典动量耦合方程开始：\n$$v_{t+1} = \\beta v_t + \\nabla f(\\theta_t)$$\n$$\\theta_{t+1} = \\theta_t - \\eta v_{t+1}$$\n目标函数为 $f(\\theta) = \\frac{1}{4}\\theta^4$，因此其梯度为 $\\nabla f(\\theta) = \\frac{d}{d\\theta}\\left(\\frac{1}{4}\\theta^4\\right) = \\theta^3$。更新方程变为：\n$$v_{t+1} = \\beta v_t + \\theta_t^3 \\quad (1)$$\n$$\\theta_{t+1} = \\theta_t - \\eta v_{t+1} \\quad (2)$$\n\n为了获得关于 $\\theta_t$ 的单个差分方程，我们必须消除速度项 $v_t$ 和 $v_{t+1}$。从方程(2)中，我们可以用参数 $\\theta_t$ 和 $\\theta_{t+1}$ 表示 $v_{t+1}$：\n$$v_{t+1} = \\frac{1}{\\eta}(\\theta_t - \\theta_{t+1})$$\n通过将时间索引从 $t+1$ 移至 $t$，我们也可以表示 $v_t$：\n$$v_t = \\frac{1}{\\eta}(\\theta_{t-1} - \\theta_t)$$\n将 $v_t$ 和 $v_{t+1}$ 的这些表达式代入速度更新方程(1)：\n$$\\frac{1}{\\eta}(\\theta_t - \\theta_{t+1}) = \\beta \\left( \\frac{1}{\\eta}(\\theta_{t-1} - \\theta_t) \\right) + \\theta_t^3$$\n整个方程乘以 $\\eta$ 以消去分母：\n$$\\theta_t - \\theta_{t+1} = \\beta(\\theta_{t-1} - \\theta_t) + \\eta \\theta_t^3$$\n最后，我们重新整理方程以分离出 $\\theta_{t+1}$，它表示下一状态作为先前状态的函数：\n$$\\theta_{t+1} = \\theta_t - \\beta(\\theta_{t-1} - \\theta_t) - \\eta \\theta_t^3$$\n$$\\theta_{t+1} = (1+\\beta)\\theta_t - \\beta\\theta_{t-1} - \\eta \\theta_t^3$$\n这就是所求的控制序列 $\\{\\theta_t\\}$ 的二阶非线性差分方程。\n\n### 第2步：线性化与特征方程\n\n非线性系统的行为可以通过在参考点周围进行线性化来局部近似。问题指定了在参考振幅 $a > 0$ 周围对非线性项 $\\theta_t^3$ 进行局部线性化，使用近似 $\\theta_t^3 \\approx a^2 \\theta_t$。将此代入差分方程：\n$$\\theta_{t+1} \\approx (1+\\beta)\\theta_t - \\beta\\theta_{t-1} - \\eta (a^2 \\theta_t)$$\n对包含 $\\theta_t$ 的项进行分组：\n$$\\theta_{t+1} = (1+\\beta - \\eta a^2)\\theta_t - \\beta\\theta_{t-1}$$\n为分析此线性、齐次、常系数差分方程，我们寻找形式为 $\\theta_t = \\lambda^t$ 的解。代入此假设可得到特征方程：\n$$\\lambda^{t+1} = (1+\\beta - \\eta a^2)\\lambda^t - \\beta\\lambda^{t-1}$$\n除以 $\\lambda^{t-1}$ (对于 $\\lambda \\neq 0$)：\n$$\\lambda^2 - (1+\\beta - \\eta a^2)\\lambda + \\beta = 0$$\n这个二次特征多项式的根 $\\lambda_{1,2}$ 决定了线性化系统的定性行为。\n\n### 第3步：稳定性分析与分类\n\n由特征多项式 $P(\\lambda) = \\lambda^2 + A\\lambda + B = 0$ 描述的离散时间系统的稳定性由 Jury 稳定性判据确定，该判据要求所有根的模都小于1。对于二阶系统，这些判据是：\n1. $P(1) > 0$\n2. $P(-1) > 0$\n3. $|B| < 1$\n\n在我们的例子中，多项式是 $P(\\lambda) = \\lambda^2 - (1+\\beta - \\eta a^2)\\lambda + \\beta$，因此 $A = -(1+\\beta - \\eta a^2)$ 且 $B = \\beta$。\n1. $P(1) = 1 - (1+\\beta - \\eta a^2) + \\beta = \\eta a^2$。由于 $\\eta > 0$ 且 $a > 0$，$P(1) > 0$ 总是成立。这意味着根不可能在 $\\lambda=1$ 处，因此系统不会收敛到一个非零常数。\n2. $P(-1) = 1 + (1+\\beta - \\eta a^2) + \\beta = 2(1+\\beta) - \\eta a^2$。稳定性条件是 $2(1+\\beta) - \\eta a^2 > 0$，或 $\\eta a^2 < 2(1+\\beta)$。\n3. $|B| = |\\beta| < 1$。由于问题陈述 $\\beta \\ge 0$，这简化为 $0 \\le \\beta < 1$。\n\n向原点的稳定衰减（$\\rho = \\max(|\\lambda_1|, |\\lambda_2|) < 1$）要求 $0 \\le \\beta < 1$ 和 $0 < \\eta a^2 < 2(1+\\beta)$。\n\n现在我们可以定义行为的类别：\n\n- **第3类 (发散):** 谱半径 $\\rho > 1$。如果任何稳定性条件被违反，就会发生这种情况。\n    - 如果 $\\beta > 1$，根的乘积 $\\lambda_1\\lambda_2 = \\beta > 1$，因此至少有一个根必须位于单位圆外。\n    - 如果 $\\eta a^2 > 2(1+\\beta)$，则 $P(-1) < 0$。由于 $P(1) > 0$，必须存在一个实根 $\\lambda < -1$，导致发散。\n    - 一个特殊情况是 $\\beta=1$ 和 $\\eta a^2 \\ge 4$，这会导致实根，其中至少一个根满足 $|\\lambda| \\ge 1$（当 $\\eta a^2=4$ 时，在 $\\lambda=-1$ 处有一个二重根，导致振幅线性增长）。\n\n- **第2类 (持续振荡):** 谱半径 $\\rho = 1$，且单位圆外没有根。这发生在稳定边界上。\n    - 如果 $\\beta = 1$ 且 $0 < \\eta a^2 < 4$。根是模为 $\\sqrt{\\beta}=1$ 的共轭复根，位于单位圆上。\n    - 如果 $\\eta a^2 = 2(1+\\beta)$ 且 $0 \\le \\beta < 1$。这意味着 $P(-1)=0$，因此一个根是 $\\lambda_1 = -1$。另一个根是 $\\lambda_2 = \\beta/\\lambda_1 = -\\beta$。由于 $|\\beta|<1$，主导根的模为1。\n \n- **第0类和第1类 (稳定衰减):** 系统是稳定的（$0 \\le \\beta < 1$ 且 $0 < \\eta a^2 < 2(1+\\beta)$）。行为由根的性质区分，这取决于特征方程的判别式：$\\Delta = (1+\\beta - \\eta a^2)^2 - 4\\beta$。\n    - 如果 $\\Delta < 0$，根是共轭复根。解是一个衰减的正弦波，即振荡。这属于 **第1类 (阻尼振荡)**。\n    - 如果 $\\Delta \\ge 0$，根是实数。行为取决于它们的符号。根的乘积是 $\\beta \\ge 0$，所以它们同号。它们的符号由它们的和 $C_1 = 1+\\beta - \\eta a^2$ 决定。\n        - 如果 $C_1 = 1+\\beta - \\eta a^2 < 0$，两个根都是负数。解包含类似 $(-\\lambda)^t$ 的项，产生符号交替（超调）。这是 **第1类 (阻尼振荡)**。\n        - 如果 $C_1 = 1+\\beta - \\eta a^2 \\ge 0$，两个根都是非负的。解是具有非负底的衰减指数之和，导致 **第0类 (单调下降)**。\n\n### 分类逻辑总结\n\n对于给定的 $(\\eta, \\beta, a)$：\n1. 定义 $C_0 = \\eta a^2$。\n2. **第3类 (发散):**\n   如果 $\\beta > 1$ 或 $C_0 > 2(1+\\beta)$ 或 ($\\beta = 1$ 且 $C_0 \\ge 4$)。\n3. **第2类 (持续振荡):**\n   如果 ($\\beta = 1$ 且 $C_0 < 4$) 或 ($C_0 = 2(1+\\beta)$ 且 $\\beta < 1$)。\n4. **稳定情况:** 否则，系统是稳定的。\n   - 令 $C_1 = 1+\\beta-C_0$ 和 $\\Delta = C_1^2 - 4\\beta$。\n   - **第1类 (阻尼振荡):** 如果 $\\Delta < 0$ 或 $C_1 < 0$。\n   - **第0类 (单调下降):** 如果 $\\Delta \\ge 0$ 且 $C_1 \\ge 0$。\n注意：应使用一个小的容差来处理浮点数的相等比较。",
            "answer": "```python\nimport numpy as np\n\ndef classify_dynamics(eta, beta, a):\n    \"\"\"\n    Classifies the behavior of the linearized momentum dynamics.\n\n    Args:\n        eta (float): Learning rate.\n        beta (float): Momentum coefficient.\n        a (float): Reference amplitude for linearization.\n\n    Returns:\n        int: Classification code (0, 1, 2, or 3).\n    \"\"\"\n    if eta <= 0 or a <= 0 or beta < 0:\n        # Invalid parameters based on problem constraints.\n        # This case is not expected for the given test suite.\n        raise ValueError(\"Parameters eta and a must be > 0, and beta must be >= 0.\")\n    \n    c0 = eta * a**2\n    \n    # Use a tolerance for floating-point equality checks.\n    tol = 1e-9\n\n    # Category 3: Divergence (spectral radius > 1)\n    # Corresponds to violation of Jury stability conditions.\n    if beta > 1 + tol:\n        return 3\n    if c0 > 2 * (1 + beta) + tol:\n        return 3\n    if np.isclose(beta, 1.0, atol=tol) and c0 >= 4.0 - tol:\n        return 3\n\n    # Category 2: Sustained Oscillations (spectral radius = 1)\n    # Corresponds to being on the boundary of the stability region.\n    if np.isclose(beta, 1.0, atol=tol) and c0 < 4.0 - tol:\n        # Ensure c0 > 0, which is guaranteed by eta > 0, a > 0\n        return 2\n    if np.isclose(c0, 2 * (1 + beta), atol=tol) and beta < 1.0 - tol:\n        return 2\n\n    # If not divergent or sustained, the system is stable (spectral radius < 1).\n    # We now distinguish between monotone and oscillatory decay.\n    c1 = 1 + beta - c0\n    discriminant = c1**2 - 4 * beta\n\n    # Category 1: Damped Oscillations\n    # Complex roots or real negative roots (causing sign alternation).\n    if discriminant < 0 or c1 < 0:\n        return 1\n    \n    # Category 0: Monotone Descent\n    # Real, non-negative roots.\n    # This is the remaining case: discriminant >= 0 and c1 >= 0.\n    return 0\n\ndef solve():\n    \"\"\"\n    Solves the problem by classifying the dynamics for each test case.\n    \"\"\"\n    test_cases = [\n        (0.05, 0.5, 1.0),\n        (0.3, 0.9, 1.0),\n        (3.0, 0.5, 1.0),\n        (4.0, 0.2, 1.0),\n        (0.5, 0.0, 1.0),\n        (1.2, 0.0, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        eta, beta, a = case\n        classification = classify_dynamics(eta, beta, a)\n        results.append(classification)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "如果我们能让优化器学会自己调整超参数呢？这个高级练习通过学习动量系数 $\\beta$ 本身，介绍了元优化（meta-optimization）的概念。你将通过对整个优化过程进行反向传播，推导并实现“超梯度”（hypergradient）——即最终损失相对于 $\\beta$ 的梯度——从而展示优化器如何能够自动适应损失曲面的曲率变化。",
            "id": "3154053",
            "problem": "给定一个一维二次目标序列和一个基于动量的优化器。您的任务是推导用于更新动量系数的超梯度，并实现一个程序，通过优化器步骤进行反向传播来学习分段特定的动量系数。然后，您必须测试学习到的动量系数是否能跟踪不同调度下的曲率变化。\n\n考虑一个标量参数 $w \\in \\mathbb{R}$，一个目标 $a \\in \\mathbb{R}$，以及一个由分段定义的分段恒定曲率调度 $\\{c_t\\}_{t=0}^{T-1}$。对于每个时间步 $t$，将瞬时目标定义为 $L_t(w) = \\frac{1}{2} c_t (w - a)^2$。设优化器为 Stochastic Gradient Descent (SGD) 的 heavy-ball 动量变体，由以下递推公式指定\n$$\nv_{t+1} = \\beta_s v_t + g_t, \\quad g_t = \\nabla L_t(w_t) = c_t (w_t - a),\n$$\n$$\nw_{t+1} = w_t - \\alpha v_{t+1},\n$$\n其中 $v_t$ 是速度，$\\alpha$ 是学习率，$\\beta_s$ 是用于覆盖时间索引 $t$ 的分段 $s$ 的分段特定动量系数。最终标量目标是 $f = \\frac{1}{2} c_{T-1} (w_T - a)^2$，其中 $w_T$ 是经过 $T$ 次更新后的参数。\n\n从链式法则和上述定义出发，通过优化器步骤的反向模式反向传播推导出 $\\frac{\\partial f}{\\partial \\beta_s}$ 的表达式并予以实现。使用此超梯度对每个分段特定的 $\\beta_s$ 执行梯度下降，同时将每个 $\\beta_s$ 投影到区间 $[0, 0.99]$ 以确保前向动力学的稳定性。\n\n您的程序必须：\n- 实现前向动力学并计算 $f$。\n- 实现一个反向模式过程，为所有分段 $s$ 生成超梯度 $\\frac{\\partial f}{\\partial \\beta_s}$。\n- 使用超梯度，通过梯度下降更新每个 $\\beta_s$，并采用固定的超学习率。\n- 对超参数更新进行固定次数的迭代。\n- 学习后，评估学习到的动量系数是否如下文所述跟踪曲率变化。\n\n使用以下固定参数进行所有运行：\n- 初始参数 $w_0 = 0$ 和目标 $a = 3$。\n- 学习率 $\\alpha = 0.05$。\n- 初始速度 $v_0 = 0$。\n- 所有分段的初始动量系数 $\\beta_s^{(0)} = 0.9$。\n- 超学习率 $\\eta_h = 0.05$。\n- 每次超参数更新后，将 $\\beta_s$ 投影到 $[0, 0.99]$。\n- 超参数迭代次数 $H = 200$。\n\n测试套件包含四个测试用例：\n1. 正常流程情况：两个分段，曲率分别为 $[1.0, 5.0]$，每个分段长度为 $50$ 步。学习后，返回一个布尔值，指示学习到的动量系数是否满足 $\\beta_{\\text{low}} > \\beta_{\\text{high}}$，其中 \"low\" 指曲率为 $1.0$ 的分段，\"high\" 指曲率为 $5.0$ 的分段。\n2. 曲率下降情况：两个分段，曲率分别为 $[5.0, 1.0]$，每个分段长度为 $50$ 步。学习后，返回一个布尔值，指示学习到的动量系数是否满足 $\\beta_{\\text{high}} < \\beta_{\\text{low}}$，其中 \"high\" 指曲率为 $5.0$ 的分段，\"low\" 指曲率为 $1.0$ 的分段。\n3. 曲率恒定情况：两个分段，曲率分别为 $[2.0, 2.0]$，每个分段长度为 $50$ 步。学习后，返回一个布尔值，指示是否满足 $|\\beta_1 - \\beta_2| < 0.05$。\n4. 单分段边缘情况：一个分段，曲率为 $[3.0]$，长度为 $100$ 步。学习单个 $\\beta$ 后，返回一个布尔值，指示使用学习到的 $\\beta$ 得到的最终损失 $f$ 是否严格小于使用固定基线 $\\beta = 0.9$ 时的最终损失。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表的结果（例如，\"[result1,result2,result3,result4]\"），每个结果都是一个布尔值，按上述顺序列出，对应于各个测试用例。本问题不涉及物理单位或角度单位。将所有布尔值表示为原生编程语言的布尔类型，而不是字符串或百分比。",
            "solution": "该问题提出了深度学习优化领域中一个有效且良定的任务。它在科学上基于梯度优化和元优化的原理，没有矛盾，并包含推导和实现唯一解决方案所需的所有信息。该任务涉及推导最终损失函数关于动量超参数的梯度（一个称为计算超梯度的过程），并用它来调整超参数。这将通过对展开的优化器更新序列应用反向模式自动微分，特别是随时间反向传播 (BPTT) 来实现。\n\n我们的目标是找到最终损失 $f = \\frac{1}{2} c_{T-1} (w_T - a)^2$ 关于分段特定的动量系数 $\\beta_s$ 的梯度。优化器动力学定义了一个随时间变化的计算图，我们可以对其进行微分。\n\n参数 $w \\in \\mathbb{R}$ 的前向动力学由 $T$ 个时间步上的 heavy-ball 动量更新规则给出。对于从 $0$ 到 $T-1$ 的每个步骤 $t$：\n$$\ng_t = \\nabla L_t(w_t) = c_t (w_t - a)\n$$\n$$\nv_{t+1} = \\beta_{s(t)} v_t + g_t\n$$\n$$\nw_{t+1} = w_t - \\alpha v_{t+1}\n$$\n其中 $w_t$ 是参数， $v_t$ 是速度， $\\alpha$ 是学习率， $c_t$ 是曲率， $\\beta_{s(t)}$ 是对应于时间步 $t$ 的分段 $s$ 的动量系数。初始条件是 $w_0$ 和 $v_0$。为了计算超梯度 $\\frac{\\partial f}{\\partial \\beta_s}$，我们必须存储此次前向传播的状态序列 $\\{w_t\\}_{t=0}^{T}$ 和 $\\{v_t\\}_{t=0}^{T}$。\n\n为了推导超梯度 $\\frac{\\partial f}{\\partial \\beta_s}$，我们反向应用链式法则，将梯度从时间 $T$ 的最终损失一直传播回初始步骤 $t=0$。设 $\\delta w_t = \\frac{\\partial f}{\\partial w_t}$ 和 $\\delta v_t = \\frac{\\partial f}{\\partial v_t}$ 为最终损失 $f$ 关于状态 $w_t$ 和 $v_t$ 的梯度。\n\n反向传播从时间 $T$ 开始。初始梯度为：\n$$\n\\delta w_T = \\frac{\\partial f}{\\partial w_T} = \\frac{\\partial}{\\partial w_T} \\left( \\frac{1}{2} c_{T-1} (w_T - a)^2 \\right) = c_{T-1} (w_T - a)\n$$\n$$\n\\delta v_T = \\frac{\\partial f}{\\partial v_T} = 0\n$$\n因为 $v_T$ 不直接影响最终损失表达式。\n\n然后我们从 $t = T-1$ 向下迭代到 $0$。在每一步，我们使用在 $t+1$ 处的梯度（$\\delta w_{t+1}$，$\\delta v_{t+1}$）来计算在 $t$ 处的梯度（$\\delta w_t$，$\\delta v_t$）以及对超梯度 $\\frac{\\partial f}{\\partial \\beta_{s(t)}}$ 的贡献。为简化推导，我们可以将 $v_{t+1}$ 的表达式代入 $w_{t+1}$ 的更新中：\n$$\nw_{t+1} = w_t - \\alpha (\\beta_{s(t)} v_t + c_t(w_t - a))\n$$\n$$\nv_{t+1} = \\beta_{s(t)} v_t + c_t(w_t - a)\n$$\n根据链式法则，梯度 $\\frac{\\partial f}{\\partial x}$ 是所有依赖于 $x$ 的变量 $y_i$ 贡献的总和：$\\sum_i \\frac{\\partial f}{\\partial y_i} \\frac{\\partial y_i}{\\partial x}$。在我们的例子中，$w_t$ 和 $v_t$ 通过 $w_{t+1}$ 和 $v_{t+1}$ 影响 $f$。这给了我们以下关于梯度的递推关系：\n$$\n\\delta w_t = \\frac{\\partial f}{\\partial w_{t+1}}\\frac{\\partial w_{t+1}}{\\partial w_t} + \\frac{\\partial f}{\\partial v_{t+1}}\\frac{\\partial v_{t+1}}{\\partial w_t} = \\delta w_{t+1}(1 - \\alpha c_t) + \\delta v_{t+1}c_t\n$$\n$$\n\\delta v_t = \\frac{\\partial f}{\\partial w_{t+1}}\\frac{\\partial w_{t+1}}{\\partial v_t} + \\frac{\\partial f}{\\partial v_{t+1}}\\frac{\\partial v_{t+1}}{\\partial v_t} = \\delta w_{t+1}(-\\alpha \\beta_{s(t)}) + \\delta v_{t+1}\\beta_{s(t)} = \\beta_{s(t)} (\\delta v_{t+1} - \\alpha \\delta w_{t+1})\n$$\n超参数 $\\beta_{s(t)}$ 也通过 $w_{t+1}$ 和 $v_{t+1}$ 影响 $f$。此时间步的梯度贡献为：\n$$\n\\frac{\\partial f}{\\partial \\beta_{s(t)}} \\bigg|_{\\text{at } t} = \\frac{\\partial f}{\\partial w_{t+1}}\\frac{\\partial w_{t+1}}{\\partial \\beta_{s(t)}} + \\frac{\\partial f}{\\partial v_{t+1}}\\frac{\\partial v_{t+1}}{\\partial \\beta_{s(t)}} = \\delta w_{t+1}(-\\alpha v_t) + \\delta v_{t+1}(v_t) = v_t (\\delta v_{t+1} - \\alpha \\delta w_{t+1})\n$$\n一个分段 $s$ 的总超梯度 $\\frac{\\partial f}{\\partial \\beta_s}$ 是所有属于分段 $s$ 的时间步 $t$ 的这些贡献之和。\n\n学习动量系数的总体算法如下：\n对于 $H = 200$ 次超参数迭代：\n1.  **前向传播**：使用当前的动量系数集 $\\{\\beta_s\\}$，从初始条件 $w_0=0$ 和 $v_0=0$ 开始，运行优化器动力学 $T$ 步。存储完整的轨迹 $\\{w_t\\}_{t=0}^T$ 和 $\\{v_t\\}_{t=0}^T$。\n2.  **反向传播**：计算所有分段 $s$ 的超梯度 $\\frac{\\partial f}{\\partial \\beta_s}$。\n    - 初始化 $\\delta w_T = c_{T-1}(w_T - a)$ 和 $\\delta v_T = 0$。\n    - 对所有 $s$ 初始化 $\\frac{\\partial f}{\\partial \\beta_s} = 0$。\n    - 从 $T-1$ 向下迭代 $t$ 到 $0$：\n        - 识别分段索引 $s(t)$。\n        - 累积超梯度： $\\frac{\\partial f}{\\partial \\beta_{s(t)}} \\mathrel{+}= v_t (\\delta v_{t+1} - \\alpha \\delta w_{t+1})$。\n        - 反向传播状态梯度：\n            - $\\delta w_{t} = \\delta w_{t+1}(1 - \\alpha c_t) + \\delta v_{t+1}c_t$。\n            - $\\delta v_{t} = \\beta_{s(t)} (\\delta v_{t+1} - \\alpha \\delta w_{t+1})$。\n3.  **超参数更新**：使用超学习率 $\\eta_h = 0.05$ 通过梯度下降更新每个动量系数。\n    $$\n    \\beta_s \\leftarrow \\beta_s - \\eta_h \\frac{\\partial f}{\\partial \\beta_s}\n    $$\n4.  **投影**：将每个更新后的 $\\beta_s$ 投影到区间 $[0, 0.99]$ 以确保前向动力学的稳定性。\n    $$\n    \\beta_s \\leftarrow \\max(0, \\min(0.99, \\beta_s))\n    $$\n\n此过程重复 $H=200$ 次迭代。然后使用最终学习到的 $\\beta_s$ 值来评估测试用例中指定的条件。测试用例背后的基本原理是验证元优化器是否学习到一个已知的启发式规则：在低曲率区域，较高的动量有利于更快的收敛，而在高曲率区域，较低的动量有助于防止不稳定和过冲。曲率恒定的情况用作合理性检查，确保学习到的参数不会无故发散。单分段情况验证了元优化过程与固定的、未优化的超参数相比，是否成功地最小化了最终损失。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the execution of all test cases.\n    \"\"\"\n\n    def run_optimizer_forward(T, w0, v0, a, alpha, c_schedule, betas, segment_indices):\n        \"\"\"\n        Runs the forward pass of the momentum optimizer.\n        \"\"\"\n        w_history = np.zeros(T + 1)\n        v_history = np.zeros(T + 1)\n        w_history[0] = w0\n        v_history[0] = v0\n\n        w_t, v_t = w0, v0\n\n        for t in range(T):\n            c_t = c_schedule[t]\n            s_idx = segment_indices[t]\n            beta_s = betas[s_idx]\n\n            g_t = c_t * (w_t - a)\n            v_t_plus_1 = beta_s * v_t + g_t\n            w_t_plus_1 = w_t - alpha * v_t_plus_1\n            \n            w_t = w_t_plus_1\n            v_t = v_t_plus_1\n\n            w_history[t + 1] = w_t\n            v_history[t + 1] = v_t\n\n        return w_history, v_history\n\n    def calculate_hypergradients(T, a, alpha, c_schedule, betas, segment_indices, w_history, v_history, num_segments):\n        \"\"\"\n        Calculates hypergradients df/d(beta) using backpropagation through time.\n        \"\"\"\n        df_d_betas = np.zeros(num_segments)\n\n        # Initial gradients at time T\n        df_dw_t_plus_1 = c_schedule[T - 1] * (w_history[T] - a)\n        df_dv_t_plus_1 = 0.0\n\n        for t in range(T - 1, -1, -1):\n            c_t = c_schedule[t]\n            s_idx = segment_indices[t]\n            beta_s = betas[s_idx]\n            v_t = v_history[t]\n\n            # common term in gradient updates\n            common_term = df_dv_t_plus_1 - alpha * df_dw_t_plus_1\n\n            # Accumulate hypergradient for beta_s\n            df_d_betas[s_idx] += common_term * v_t\n\n            # Propagate gradients back to time t\n            df_dw_t = df_dw_t_plus_1 * (1.0 - alpha * c_t) + df_dv_t_plus_1 * c_t\n            df_dv_t = beta_s * common_term\n\n            # Update gradients for next iteration\n            df_dw_t_plus_1 = df_dw_t\n            df_dv_t_plus_1 = df_dv_t\n        \n        return df_d_betas\n    \n    def run_test_case(curvatures, segment_lengths, case_id):\n        \"\"\"\n        Runs a single test case from the problem description.\n        \"\"\"\n        # Fixed parameters\n        w0 = 0.0\n        v0 = 0.0\n        a = 3.0\n        alpha = 0.05\n        eta_h = 0.05\n        H = 200\n        \n        num_segments = len(curvatures)\n        T = sum(segment_lengths)\n\n        # Create full schedules for curvature and segment indices\n        c_schedule = np.repeat(curvatures, segment_lengths)\n        segment_indices = np.repeat(np.arange(num_segments), segment_lengths)\n\n        # Initialize momentum coefficients\n        betas = np.full(num_segments, 0.9)\n\n        # Hyper-optimization loop\n        for _ in range(H):\n            # Forward pass\n            w_history, v_history = run_optimizer_forward(T, w0, v0, a, alpha, c_schedule, betas, segment_indices)\n\n            # Backward pass (calculate hypergradients)\n            df_d_betas = calculate_hypergradients(T, a, alpha, c_schedule, betas, segment_indices, w_history, v_history, num_segments)\n            \n            # Update betas\n            betas -= eta_h * df_d_betas\n            \n            # Project betas\n            betas = np.clip(betas, 0.0, 0.99)\n        \n        # Evaluate test case condition\n        if case_id == 1:\n            # low curvature (1.0) is first, high (5.0) is second\n            return betas[0] > betas[1]\n        elif case_id == 2:\n            # high curvature (5.0) is first, low (1.0) is second\n            return betas[0] < betas[1]\n        elif case_id == 3:\n            return abs(betas[0] - betas[1]) < 0.05\n        elif case_id == 4:\n            # learned loss vs baseline loss\n            w_hist_learned, _ = run_optimizer_forward(T, w0, v0, a, alpha, c_schedule, betas, segment_indices)\n            loss_learned = 0.5 * c_schedule[T-1] * (w_hist_learned[T] - a)**2\n            \n            baseline_betas = np.full(num_segments, 0.9)\n            w_hist_baseline, _ = run_optimizer_forward(T, w0, v0, a, alpha, c_schedule, baseline_betas, segment_indices)\n            loss_baseline = 0.5 * c_schedule[T-1] * (w_hist_baseline[T] - a)**2\n            \n            return loss_learned < loss_baseline\n        else:\n            raise ValueError(\"Invalid case_id\")\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (curvatures, segment_lengths, case_id)\n        ([1.0, 5.0], [50, 50], 1),  # Happy path\n        ([5.0, 1.0], [50, 50], 2),  # Decreasing curvature\n        ([2.0, 2.0], [50, 50], 3),  # Constant curvature\n        ([3.0], [100], 4),          # Single segment\n    ]\n\n    results = []\n    for curvatures, lengths, case_id in test_cases:\n        result = run_test_case(curvatures, lengths, case_id)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Python's bool `True` and `False` will be converted to `True` and `False` strings\n    # which is standard. For json compatibility would be 'true', 'false.\n    # The requirement says \"native programming language boolean type\", outputting it as string is fine.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}