{
    "hands_on_practices": [
        {
            "introduction": "A key skill for any deep learning practitioner is the ability to diagnose training behavior from learning curves. This first exercise provides hands-on practice by simulating training dynamics on a simplified model to build intuition for the effects of learning rate warmup. You will implement a simple linear warmup schedule and develop a rule-based diagnostic to classify a warmup as insufficient, excessive, or acceptable based on its impact on the early-epoch loss .",
            "id": "3115472",
            "problem": "You will implement and use a simple, deterministic training-dynamics simulator to diagnose learning rate warmup using learning curves. The setting is one-dimensional gradient descent on a strictly convex quadratic, with a linearly increasing warmup learning rate followed by a constant learning rate. Your program must simulate the training loss over epochs, apply principled diagnostics on the early segment of the learning curve, and classify each case as under-warmup, over-warmup, or acceptable warmup.\n\nFundamental base:\n- Consider the loss function $L(x) = \\frac{1}{2} a x^{2}$ with curvature $a > 0$. The gradient is $\\nabla L(x) = a x$. Gradient descent with learning rate $\\eta_{t}$ updates the parameter as $x_{t+1} = x_{t} - \\eta_{t} \\nabla L(x_{t})$.\n- A linear warmup learning rate schedule with warmup length $w$ and maximum learning rate $\\eta_{\\max}$ is given by\n  - $\\eta_{t} = \\eta_{\\max} \\cdot \\min\\!\\left(\\frac{t}{w}, 1\\right)$ for integer epochs $t \\in \\{1, 2, \\dots, T\\}$.\n- The observed training loss is modeled deterministically as $y_{t} = L(x_{t}) + \\sigma \\sin\\!\\left( \\frac{2 \\pi t}{P} \\right)$, with $\\sigma \\ge 0$ and period parameter $P \\ge 1$. For $t = 0$, define $y_{0} = L(x_{0})$.\n\nDiagnostics to implement from first principles:\n- Early-epoch stability: define an early window of length $K = \\min(5, T)$. Detect initial divergence by checking if there exists some $t \\in \\{1, \\dots, K\\}$ such that $y_{t} > (1 + \\tau) \\, y_{t-1}$, with threshold $\\tau = 0.1$.\n- Delayed progress: quantify the fraction of total loss reduction achieved by epoch $K$ as\n  $$f_{\\text{early}} = \\frac{y_{0} - y_{K}}{\\max(y_{0} - y_{T}, \\varepsilon)},$$\n  with $\\varepsilon = 10^{-12}$. Declare delayed progress if $f_{\\text{early}}  \\rho$ with $\\rho = 0.2$.\n- Classification rule, applied in this order:\n  - If initial divergence is detected in the early window, classify as under-warmup and output $-1$.\n  - Else if delayed progress is detected, classify as over-warmup and output $1$.\n  - Else classify as acceptable warmup and output $0$.\n\nSimulation details:\n- Initialize with $x_{0}$ and compute $y_{0} = L(x_{0})$.\n- For each epoch $t = 1, 2, \\dots, T$:\n  - Compute $\\eta_{t} = \\eta_{\\max} \\cdot \\min\\!\\left(\\frac{t}{w}, 1\\right)$.\n  - Update $x_{t} = x_{t-1} - \\eta_{t} a x_{t-1}$.\n  - Compute $y_{t} = \\frac{1}{2} a x_{t}^{2} + \\sigma \\sin\\!\\left( \\frac{2 \\pi t}{P} \\right)$.\n\nTest suite:\nProvide outputs for the following parameter sets. Each case is a tuple $(a, \\eta_{\\max}, w, T, x_{0}, \\sigma, P)$:\n- Case A (expected to probe initial divergence with extremely short warmup): $(10.0, 0.25, 1, 40, 1.0, 0.0, 7)$.\n- Case B (expected acceptable warmup with safe maximum learning rate and short warmup): $(10.0, 0.15, 3, 40, 1.0, 0.0, 7)$.\n- Case C (expected delayed progress with very long warmup): $(10.0, 0.18, 300, 500, 1.0, 0.0, 7)$.\n- Case D (boundary stability at the edge of the classical step-size stability limit after warmup): $(10.0, 0.20, 5, 40, 1.0, 0.0, 7)$.\n- Case E (expected initial divergence at the end of warmup with too-high maximum learning rate): $(12.0, 0.25, 5, 50, 1.0, 0.0, 7)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above. Each entry must be an integer in $\\{-1, 0, 1\\}$ representing the class for that case, so the output must look like [$r_A, r_B, r_C, r_D, r_E$].\n\nNotes:\n- There are no physical units in this problem.\n- Angles in the sine term are in radians by construction of the argument $\\frac{2 \\pi t}{P}$.\n- Percentages must be expressed as decimals, and all thresholds $\\tau$, $\\rho$, and $\\varepsilon$ are provided numerically.",
            "solution": "The problem statement has been validated and is determined to be sound. It is scientifically grounded in the principles of numerical optimization, specifically gradient descent on a convex quadratic function. The problem is well-posed, with all parameters, equations, and diagnostic criteria explicitly and unambiguously defined. The setup is self-contained and internally consistent, allowing for a unique and meaningful solution.\n\nThe task is to simulate the training dynamics of a one-dimensional parameter $x$ under gradient descent and classify the behavior of the learning rate warmup schedule. The process involves several interconnected components: the mathematical model of optimization, the learning rate schedule, the simulation of training loss, and a set of diagnostic rules.\n\nFirst, we establish the core optimization model. The loss function is a simple convex quadratic, $L(x) = \\frac{1}{2} a x^2$, where $a > 0$ is the curvature. The gradient of this loss function with respect to the parameter $x$ is $\\nabla L(x) = a x$. The gradient descent update rule modifies the parameter $x$ at each epoch $t$ according to the equation $x_{t} = x_{t-1} - \\eta_{t} \\nabla L(x_{t-1})$, where $\\eta_t$ is the learning rate at epoch $t$. Substituting the gradient, the specific update rule is:\n$$x_{t} = x_{t-1} - \\eta_{t} a x_{t-1} = x_{t-1}(1 - \\eta_{t} a)$$\nThis update is performed for epochs $t = 1, 2, \\dots, T$.\n\nThe learning rate $\\eta_t$ follows a linear warmup schedule. For a total warmup duration of $w$ epochs and a target maximum learning rate of $\\eta_{\\max}$, the learning rate at epoch $t$ is given by:\n$$\\eta_{t} = \\eta_{\\max} \\cdot \\min\\left(\\frac{t}{w}, 1\\right)$$\nThis means $\\eta_t$ increases linearly from $\\eta_1 = \\eta_{\\max}/w$ to $\\eta_w = \\eta_{\\max}$ for $t \\le w$. For all subsequent epochs $t > w$, the learning rate remains constant at $\\eta_t = \\eta_{\\max}$.\n\nThe simulation must track the training loss over the epochs. The problem defines an observed loss, $y_t$, which consists of the true loss $L(x_t)$ plus a deterministic sinusoidal noise term. The initial loss is $y_0 = L(x_0)$. For subsequent epochs $t \\in \\{1, 2, \\dots, T\\}$, the observed loss is:\n$$y_t = L(x_t) + \\sigma \\sin\\left(\\frac{2 \\pi t}{P}\\right) = \\frac{1}{2} a x_t^2 + \\sigma \\sin\\left(\\frac{2 \\pi t}{P}\\right)$$\nThe simulation proceeds as follows:\n1. Initialize the parameter $x_0$ and compute the initial loss $y_0 = \\frac{1}{2} a x_0^2$. Store all loss values $y_t$ in an array.\n2. For each epoch $t$ from $1$ to $T$:\n   a. Calculate the learning rate $\\eta_t$ using the warmup schedule.\n   b. Update the parameter to get $x_t$ using the gradient descent rule.\n   c. Compute the observed loss $y_t$ and store it.\n\nAfter the simulation is complete, we apply a sequence of diagnostic tests to the generated learning curve $\\{y_t\\}_{t=0}^T$.\n\nThe first diagnostic is for **early-epoch stability**. This test checks for initial divergence, a common symptom of an overly aggressive learning rate (i.e., under-warmup). We define an early window of $K = \\min(5, T)$ epochs. Initial divergence is detected if the loss increases by more than a relative threshold $\\tau = 0.1$ at any point within this window. That is, if there exists any $t \\in \\{1, \\dots, K\\}$ such that:\n$$y_t > (1 + \\tau) y_{t-1}$$\nIf this condition is met, the warmup is classified as `under-warmup` ($-1$).\n\nIf no initial divergence is found, the second diagnostic is for **delayed progress**. This test checks for an overly conservative learning rate (i.e., over-warmup), which causes the model to learn too slowly at the beginning. We quantify the fraction of the total loss reduction that occurs within the early window of $K$ epochs:\n$$f_{\\text{early}} = \\frac{y_0 - y_K}{\\max(y_0 - y_T, \\varepsilon)}$$\nHere, $\\varepsilon = 10^{-12}$ is a small constant to prevent division by zero if the loss does not decrease from epoch $0$ to $T$. If this fraction is below a threshold $\\rho = 0.2$, we declare delayed progress. The warmup is then classified as `over-warmup` ($1$).\n\nThe final classification follows a strict order.\n1. If initial divergence is detected, the result is $-1$.\n2. Otherwise, if delayed progress is detected, the result is $1$.\n3. Otherwise, the warmup is considered `acceptable`, and the result is $0$.\n\nThis procedure is applied to each test case provided. The implementation will systematically execute the simulation and apply the defined diagnostics to produce the final classification for each set of parameters.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates training dynamics to diagnose learning rate warmup for several test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A: (a, eta_max, w, T, x0, sigma, P)\n        (10.0, 0.25, 1, 40, 1.0, 0.0, 7),\n        # Case B:\n        (10.0, 0.15, 3, 40, 1.0, 0.0, 7),\n        # Case C:\n        (10.0, 0.18, 300, 500, 1.0, 0.0, 7),\n        # Case D:\n        (10.0, 0.20, 5, 40, 1.0, 0.0, 7),\n        # Case E:\n        (12.0, 0.25, 5, 50, 1.0, 0.0, 7),\n    ]\n\n    results = []\n\n    # Diagnostic thresholds and constants\n    tau = 0.1\n    rho = 0.2\n    epsilon = 1e-12\n\n    for case in test_cases:\n        a, eta_max, w, T, x0, sigma, P = case\n\n        # --- Simulation ---\n        # Initialize arrays for parameter and loss history\n        x_history = np.zeros(T + 1)\n        y_history = np.zeros(T + 1)\n\n        # Initial conditions\n        x_history[0] = x0\n        y_history[0] = 0.5 * a * x_history[0]**2\n\n        # Run the simulation loop for T epochs\n        for t in range(1, T + 1):\n            # Calculate learning rate with linear warmup\n            eta_t = eta_max * min(t / w, 1.0)\n            \n            # Update parameter using gradient descent\n            x_prev = x_history[t-1]\n            x_curr = x_prev * (1 - eta_t * a)\n            x_history[t] = x_curr\n            \n            # Calculate observed loss\n            true_loss = 0.5 * a * x_curr**2\n            noise = sigma * np.sin(2 * np.pi * t / P) if sigma > 0 else 0.0\n            y_history[t] = true_loss + noise\n\n        # --- Diagnostics ---\n        K = min(5, T)\n        y0 = y_history[0]\n        yK = y_history[K]\n        yT = y_history[T]\n\n        # 1. Check for early-epoch stability (under-warmup)\n        initial_divergence = False\n        for t in range(1, K + 1):\n            if y_history[t] > (1 + tau) * y_history[t-1]:\n                initial_divergence = True\n                break\n\n        # 2. Check for delayed progress (over-warmup)\n        # This is only checked if no divergence was found.\n        delayed_progress = False\n        if not initial_divergence:\n            denominator = max(y0 - yT, epsilon)\n            f_early = (y0 - yK) / denominator\n            if f_early  rho:\n                delayed_progress = True\n        \n        # 3. Apply classification rule\n        if initial_divergence:\n            results.append(-1)  # Under-warmup\n        elif delayed_progress:\n            results.append(1)   # Over-warmup\n        else:\n            results.append(0)   # Acceptable warmup\n\n    # Print results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While observing learning curves builds intuition, a deeper understanding comes from analyzing the underlying optimizer dynamics. This theoretical exercise guides you through a mathematical derivation to quantify precisely how linear warmup helps stabilize the initial phase of training, especially when using momentum. By comparing the velocity accumulated with and without warmup under a common analytical approximation, you will see how a gradual increase in learning rate prevents the optimizer from taking excessively large initial steps .",
            "id": "3143226",
            "problem": "Consider one-dimensional training of a parameter $w_t$ on a strictly convex quadratic loss $L(w) = \\tfrac{\\lambda}{2} (w - w^{\\star})^{2}$ with curvature $\\lambda > 0$. Let the error be $e_t = w_t - w^{\\star}$ and let the gradient be $g_t = \\nabla L(w_t) = \\lambda e_t$. The optimizer uses classical momentum with velocity update and parameter update\n$$\nv_{t+1} = \\beta v_t + \\eta(t)\\, g_t, \\qquad w_{t+1} = w_t - v_{t+1},\n$$\nwith momentum coefficient $0 \\le \\beta  1$, initial velocity $v_0 = 0$, and initial error $e_0 \\ne 0$. A linear learning-rate warmup is used for the first $T_w \\in \\mathbb{N}$ steps:\n$$\n\\eta(t) = \\eta_{\\max}\\,\\frac{t}{T_w}\n$$\nfor $0 \\le t \\le T_w$, after which the learning rate remains at $\\eta_{\\max}$. During the warmup phase, assume the empirically observed small-step regime in which the error changes negligibly over the first $T_w+1$ iterates, so that $e_t \\approx e_0$ for $0 \\le t \\le T_w$. This approximation is widely used to analyze the earliest-epoch dynamics under warmup when the learning rate is still small.\n\nStarting only from these definitions and this approximation:\n\n1) Derive an exact closed-form expression (under the stated approximation) for the end-of-warmup velocity $v_{T_w+1}$ in terms of $\\beta$, $\\lambda$, $e_0$, $\\eta_{\\max}$, and $T_w$.\n\n2) Let $v^{\\mathrm{const}}_{T_w+1}$ denote the end-of-warmup velocity one would obtain if the learning rate were held constant at $\\eta(t) \\equiv \\eta_{\\max}$ for all $0 \\le t \\le T_w$. Using the same approximation, derive a simplified closed-form expression for the ratio\n$$\nR(\\beta,T_w) \\equiv \\frac{v^{\\mathrm{warm}}_{T_w+1}}{v^{\\mathrm{const}}_{T_w+1}},\n$$\nexpressed solely as a function of $\\beta$ and $T_w$ (all other parameters must cancel).\n\nInterpretational note: the magnitude of $v_{T_w+1}$ relative to $|e_0|$ controls the risk of initial overshoot at the end of warmup, so the ratio $R(\\beta,T_w)$ quantifies how linear warmup damps the initial momentum build-up compared to a constant learning rate.\n\nYour final answer must be the simplified closed-form expression for $R(\\beta,T_w)$ as a single analytic expression. No numerical evaluation is required or permitted, and no rounding is needed. Assume $\\beta \\ne 1$ and $T_w \\ge 1$ are integers, but your expression may treat $T_w$ symbolically.",
            "solution": "The user has provided a valid, well-posed problem from the field of deep learning optimization theory. The task is to derive a simplified, closed-form expression for the ratio of end-of-warmup velocities, $R(\\beta, T_w)$, under two different learning rate schedules.\n\nThe problem is defined by the following components:\n- Loss function: $L(w) = \\frac{\\lambda}{2} (w - w^{\\star})^{2}$ with $\\lambda  0$.\n- Error and gradient: $e_t = w_t - w^{\\star}$ and $g_t = \\nabla L(w_t) = \\lambda e_t$.\n- Optimizer update rules: $v_{t+1} = \\beta v_t + \\eta(t) g_t$ and $w_{t+1} = w_t - v_{t+1}$.\n- Initial conditions: $v_0 = 0$ and $e_0 \\neq 0$.\n- Momentum parameter: $0 \\le \\beta  1$.\n- Central approximation: For the duration of the warmup phase, $0 \\le t \\le T_w$, the error is assumed to be constant, $e_t \\approx e_0$. This implies the gradient is also constant, $g_t \\approx g_0 = \\lambda e_0$.\n\nThe solution proceeds in three steps:\n1.  Derive the velocity $v^{\\mathrm{warm}}_{T_w+1}$ at the end of a linear warmup phase.\n2.  Derive the velocity $v^{\\mathrm{const}}_{T_w+1}$ that would result from a constant learning rate.\n3.  Compute and simplify the ratio $R(\\beta, T_w) = v^{\\mathrm{warm}}_{T_w+1} / v^{\\mathrm{const}}_{T_w+1}$.\n\n**Step 1: Derivation of the velocity with linear warmup ($v^{\\mathrm{warm}}_{T_w+1}$)**\n\nThe linear warmup schedule is given by $\\eta(t) = \\eta_{\\max} \\frac{t}{T_w}$ for $0 \\le t \\le T_w$.\nUnder the constant gradient approximation $g_t \\approx g_0$, the velocity update rule is a linear recurrence relation:\n$$\nv_{t+1} = \\beta v_t + \\eta(t) g_0 = \\beta v_t + \\left(\\frac{\\eta_{\\max} g_0}{T_w}\\right) t\n$$\nStarting with $v_0 = 0$, we can unroll this recurrence to find $v_{k+1}$:\n$$\nv_{k+1} = \\beta^{k+1} v_0 + \\sum_{j=0}^{k} \\beta^{k-j} \\eta(j) g_0 = g_0 \\sum_{j=0}^{k} \\beta^{k-j} \\eta(j)\n$$\nWe need the velocity at step $T_w+1$, which means we evaluate this expression for $k=T_w$, using the schedule for $j \\in \\{0, 1, \\dots, T_w\\}$:\n$$\nv^{\\mathrm{warm}}_{T_w+1} = g_0 \\sum_{j=0}^{T_w} \\beta^{T_w-j} \\left(\\eta_{\\max} \\frac{j}{T_w}\\right) = \\frac{g_0 \\eta_{\\max}}{T_w} \\sum_{j=0}^{T_w} j \\beta^{T_w-j}\n$$\nThe $j=0$ term is zero, so the sum is over $j=1, \\dots, T_w$. Let's analyze the sum $S = \\sum_{j=1}^{T_w} j \\beta^{T_w-j}$. We can re-index by letting $k = T_w - j$. As $j$ goes from $1$ to $T_w$, $k$ goes from $T_w-1$ to $0$.\n$$\nS = \\sum_{k=0}^{T_w-1} (T_w - k) \\beta^k = T_w \\sum_{k=0}^{T_w-1} \\beta^k - \\sum_{k=0}^{T_w-1} k \\beta^k\n$$\nThe first term is a standard geometric series: $\\sum_{k=0}^{T_w-1} \\beta^k = \\frac{1 - \\beta^{T_w}}{1 - \\beta}$.\nThe second term is an arithmetic-geometric series. We can evaluate it using calculus. For $|x|1$, we know $\\sum_{k=0}^{n} x^k = \\frac{1-x^{n+1}}{1-x}$. Differentiating with respect to $x$ gives $\\sum_{k=1}^{n} kx^{k-1} = \\frac{d}{dx}\\left(\\frac{1-x^{n+1}}{1-x}\\right)$. Thus, $\\sum_{k=0}^{n} kx^k = x \\frac{d}{dx}\\left(\\frac{1-x^{n+1}}{1-x}\\right)$.\nFor our sum, $n = T_w-1$ and $x=\\beta$:\n$$\n\\sum_{k=0}^{T_w-1} k \\beta^k = \\beta \\frac{d}{d\\beta} \\left(\\frac{1-\\beta^{T_w}}{1-\\beta}\\right) = \\beta \\frac{-T_w\\beta^{T_w-1}(1-\\beta) - (1-\\beta^{T_w})(-1)}{(1-\\beta)^2} = \\frac{\\beta(1 - T_w\\beta^{T_w-1} + (T_w-1)\\beta^{T_w})}{(1-\\beta)^2}\n$$\nCombining the terms for $S$:\n$$\nS = T_w \\frac{1-\\beta^{T_w}}{1-\\beta} - \\frac{\\beta - T_w\\beta^{T_w} + (T_w-1)\\beta^{T_w+1}}{(1-\\beta)^2}\n$$\nPutting everything over a common denominator $(1-\\beta)^2$:\n$$\nS = \\frac{T_w(1-\\beta^{T_w})(1-\\beta) - (\\beta - T_w\\beta^{T_w} + (T_w-1)\\beta^{T_w+1})}{(1-\\beta)^2}\n$$\nThe numerator simplifies to:\n$$\n(T_w - T_w\\beta - T_w\\beta^{T_w} + T_w\\beta^{T_w+1}) - (\\beta - T_w\\beta^{T_w} + T_w\\beta^{T_w+1} - \\beta^{T_w+1})\n= T_w - T_w\\beta - \\beta + \\beta^{T_w+1} = T_w - (T_w+1)\\beta + \\beta^{T_w+1}\n$$\nSo, $S = \\frac{T_w - (T_w+1)\\beta + \\beta^{T_w+1}}{(1-\\beta)^2}$.\nSubstituting this back into the expression for $v^{\\mathrm{warm}}_{T_w+1}$:\n$$\nv^{\\mathrm{warm}}_{T_w+1} = \\frac{g_0 \\eta_{\\max}}{T_w} \\left( \\frac{T_w - (T_w+1)\\beta + \\beta^{T_w+1}}{(1-\\beta)^2} \\right)\n$$\n\n**Step 2: Derivation of the velocity with constant learning rate ($v^{\\mathrm{const}}_{T_w+1}$)**\n\nIf the learning rate were held constant at $\\eta(t) = \\eta_{\\max}$ for $0 \\le t \\le T_w$, the velocity update would be:\n$$\nv_{t+1} = \\beta v_t + \\eta_{\\max} g_0\n$$\nUnrolling this recurrence from $v_0 = 0$:\n$$\nv_{k+1} = \\eta_{\\max} g_0 \\sum_{j=0}^{k} \\beta^j\n$$\nWe need the velocity at step $T_w+1$, so we set $k=T_w$:\n$$\nv^{\\mathrm{const}}_{T_w+1} = \\eta_{\\max} g_0 \\sum_{j=0}^{T_w} \\beta^j\n$$\nThe sum is a geometric series: $\\sum_{j=0}^{T_w} \\beta^j = \\frac{1 - \\beta^{T_w+1}}{1-\\beta}$.\nTherefore,\n$$\nv^{\\mathrm{const}}_{T_w+1} = \\eta_{\\max} g_0 \\left( \\frac{1 - \\beta^{T_w+1}}{1-\\beta} \\right)\n$$\n\n**Step 3: Calculation of the ratio $R(\\beta, T_w)$**\n\nThe ratio is defined as $R(\\beta, T_w) = \\frac{v^{\\mathrm{warm}}_{T_w+1}}{v^{\\mathrm{const}}_{T_w+1}}$. Substituting the expressions from Steps 1 and 2:\n$$\nR(\\beta, T_w) = \\frac{\\frac{g_0 \\eta_{\\max}}{T_w} \\frac{T_w - (T_w+1)\\beta + \\beta^{T_w+1}}{(1-\\beta)^2}}{\\eta_{\\max} g_0 \\frac{1 - \\beta^{T_w+1}}{1-\\beta}}\n$$\nThe terms $g_0$ and $\\eta_{\\max}$ cancel, as required. We are left with an expression depending only on $\\beta$ and $T_w$:\n$$\nR(\\beta, T_w) = \\frac{1}{T_w} \\cdot \\frac{T_w - (T_w+1)\\beta + \\beta^{T_w+1}}{(1-\\beta)^2} \\cdot \\frac{1-\\beta}{1 - \\beta^{T_w+1}}\n$$\nSimplifying by canceling one factor of $(1-\\beta)$:\n$$\nR(\\beta, T_w) = \\frac{T_w - (T_w+1)\\beta + \\beta^{T_w+1}}{T_w(1-\\beta)(1 - \\beta^{T_w+1})}\n$$\nThis is the final simplified, closed-form expression for the ratio.",
            "answer": "$$\n\\boxed{\\frac{T_w - (T_w+1)\\beta + \\beta^{T_w+1}}{T_w(1-\\beta)(1 - \\beta^{T_w+1})}}\n$$"
        },
        {
            "introduction": "Fixed warmup schedules are effective, but a more sophisticated approach is to adapt the learning rate based on the real-time behavior of the training process. This exercise challenges you to design and implement an adaptive warmup schedule that \"listens\" to the gradients. You will build a system that increases the learning rate only when the gradient norm indicates that the optimization process is stable, introducing a powerful principle of feedback control into your optimizer design .",
            "id": "3143236",
            "problem": "You are given a sequence of nonnegative real numbers representing the per-iteration gradient norm of a loss function in Stochastic Gradient Descent (SGD). Let the parameter vector be denoted by $\\theta_t \\in \\mathbb{R}^d$, the loss be $L(\\theta)$, the gradient be $g_t = \\nabla_{\\theta} L(\\theta_t)$, and the learning rate be $\\eta(t)$. The core SGD update rule is the foundational base: $$\\theta_{t+1} = \\theta_t - \\eta(t)\\,g_t.$$ The goal is to construct and apply an adaptive warmup schedule that increases $\\eta(t)$ only when the current gradient norm $\\lVert \\nabla L(\\theta_t) \\rVert_2$ is lower than a specified empirical percentile of its recent history. The adaptive warmup must be consistent with the following principles and definitions:\n\n1. Initialization and monotonicity constraint: Start from a minimal learning rate $\\eta_{\\min}$ and allow $\\eta(t)$ to be nondecreasing in time, capped above by a target learning rate $\\eta_{\\text{target}}$.\n2. Percentile thresholding: At iteration $t \\ge 2$, define the history window $H_t = \\left\\{ \\lVert g_j \\rVert_2 \\mid j \\in \\{\\max(1, t-W), \\dots, t-1\\} \\right\\}$ of size $n_t = \\min(W, t-1)$. For a chosen percentile $p \\in [0,100]$, define the empirical percentile threshold $q_p(H_t)$ via linear interpolation over the sorted history. If $H_t$ sorted ascending is $x_{(1)} \\le x_{(2)} \\le \\dots \\le x_{(n_t)}$, set $$\\text{pos} = \\frac{p}{100}\\,(n_t - 1), \\quad \\ell = \\lfloor \\text{pos} \\rfloor, \\quad u = \\lceil \\text{pos} \\rceil,$$ and $$q_p(H_t) = \n\\begin{cases}\nx_{(\\ell+1)}  \\text{if } \\ell = u, \\\\\nx_{(\\ell+1)} + (x_{(u+1)} - x_{(\\ell+1)})\\,(\\text{pos} - \\ell)  \\text{if } \\ell  u.\n\\end{cases}$$\nFor $n_t = 1$, interpret $q_p(H_t) = x_{(1)}$.\n3. Adaptive warmup rule: For $t = 1$, set $\\eta(1) = \\eta_{\\min}$. For $t \\ge 2$, compute the current gradient norm $g_t^{\\text{norm}} = \\lVert g_t \\rVert_2$ and the threshold $q_p(H_t)$. Increase the learning rate multiplicatively by a factor $(1+\\alpha)$ only when $g_t^{\\text{norm}}  q_p(H_t)$, but never above $\\eta_{\\text{target}}$. Formally,\n$$\\eta(t) = \n\\begin{cases}\n\\min\\left(\\eta(t-1)\\,(1+\\alpha),\\,\\eta_{\\text{target}}\\right)  \\text{if } g_t^{\\text{norm}}  q_p(H_t), \\\\\n\\eta(t-1)  \\text{otherwise.}\n\\end{cases}$$\nOnce $\\eta(t)$ reaches $\\eta_{\\text{target}}$, it remains at $\\eta_{\\text{target}}$ thereafter.\n\nYour task is to implement a program that, for each test case, applies the above adaptive warmup schedule to its gradient norm sequence and returns two outputs: the final learning rate value $\\eta(T)$ after the last iteration and the total count of actual increases performed (i.e., the number of iterations $t$ for which $\\eta(t)  \\eta(t-1)$).\n\nAll quantities are dimensionless and expressed as real numbers. No physical units are involved. Percentiles must be provided as numbers in the interval $[0,100]$ and must not use a percentage sign.\n\nTest Suite:\n- Case $1$ (general decreasing gradient norms):\n  - Gradient norms: $[5.0,\\,4.5,\\,4.0,\\,3.5,\\,3.0,\\,2.6,\\,2.3,\\,2.1,\\,1.9,\\,1.8]$\n  - Window size: $W=3$\n  - Percentile: $p=40.0$\n  - Multiplicative increase parameter: $\\alpha=0.5$\n  - Minimal learning rate: $\\eta_{\\min}=0.001$\n  - Target learning rate: $\\eta_{\\text{target}}=0.01$\n- Case $2$ (near-constant norms testing equality boundary):\n  - Gradient norms: $[3.0,\\,3.0,\\,3.0,\\,3.0,\\,3.0,\\,3.0,\\,3.0,\\,3.0]$\n  - Window size: $W=4$\n  - Percentile: $p=50.0$\n  - Multiplicative increase parameter: $\\alpha=0.5$\n  - Minimal learning rate: $\\eta_{\\min}=0.001$\n  - Target learning rate: $\\eta_{\\text{target}}=0.005$\n- Case $3$ (zeros and small values testing short windows and ties):\n  - Gradient norms: $[1.0,\\,0.0,\\,0.0,\\,0.05,\\,0.0,\\,0.0]$\n  - Window size: $W=2$\n  - Percentile: $p=50.0$\n  - Multiplicative increase parameter: $\\alpha=0.8$\n  - Minimal learning rate: $\\eta_{\\min}=0.0001$\n  - Target learning rate: $\\eta_{\\text{target}}=0.002$\n- Case $4$ (oscillatory norms testing larger window and hitting the cap):\n  - Gradient norms: $[5.0,\\,10.0,\\,6.0,\\,4.0,\\,9.0,\\,3.0,\\,8.0,\\,2.0,\\,7.0,\\,1.0]$\n  - Window size: $W=5$\n  - Percentile: $p=60.0$\n  - Multiplicative increase parameter: $\\alpha=0.5$\n  - Minimal learning rate: $\\eta_{\\min}=0.0005$\n  - Target learning rate: $\\eta_{\\text{target}}=0.002$\n\nOutput specification:\n- For each test case, produce a list $[\\eta(T),\\,\\text{count}]$ where $\\eta(T)$ is the final learning rate after the last iteration and $\\text{count}$ is the total number of actual increases made during the process.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, in the form $[[\\eta_1(T),\\text{count}_1],[\\eta_2(T),\\text{count}_2],[\\eta_3(T),\\text{count}_3],[\\eta_4(T),\\text{count}_4]]$ without any additional text.",
            "solution": "The problem statement is valid. It is scientifically grounded in the established field of deep learning optimization, specifically concerning learning rate scheduling. The problem is well-posed, providing a complete and unambiguous set of rules and parameters for a deterministic algorithm. All terms are clearly defined, and the data provided in the test suite are consistent and realistic for a computational simulation.\n\nThe task is to implement an adaptive learning rate warmup schedule for a given sequence of gradient norms. The schedule determines the learning rate $\\eta(t)$ for each iteration $t$ based on a set of predefined rules. The primary output for each test case consists of the final learning rate $\\eta(T)$ after the last iteration $T$, and the total number of times the learning rate was increased.\n\nThe algorithm proceeds as follows:\n\n1.  **State Variables and Initialization**: The core state variables are the learning rate $\\eta(t)$ and a counter for the number of increases. At the initial iteration $t=1$, the learning rate is set to the specified minimum value, $\\eta(1) = \\eta_{\\min}$, and the increase counter is initialized to $0$.\n\n2.  **Iterative Update**: For each subsequent iteration $t$ from $2$ to $T$, where $T$ is the total number of iterations (i.e., the length of the gradient norm sequence), the learning rate $\\eta(t)$ is determined based on $\\eta(t-1)$, the current gradient norm $\\lVert g_t \\rVert_2$, and a percentile-based threshold computed from recent history.\n\n3.  **History and Threshold Calculation**: At a given iteration $t \\ge 2$, a history window $H_t$ is defined as the set of the most recent $n_t = \\min(W, t-1)$ gradient norms, where $W$ is the maximum window size. Formally, $H_t = \\{ \\lVert g_j \\rVert_2 \\mid j \\in \\{\\max(1, t-W), \\dots, t-1\\} \\}$. A threshold value, $q_p(H_t)$, is then computed as the $p$-th empirical percentile of the values in $H_t$. The problem specifies a linear interpolation method for this calculation:\n    Let the sorted values in $H_t$ be $x_{(1)} \\le x_{(2)} \\le \\dots \\le x_{(n_t)}$. The position for the percentile is calculated as $\\text{pos} = \\frac{p}{100}(n_t - 1)$. With $\\ell = \\lfloor \\text{pos} \\rfloor$ and $u = \\lceil \\text{pos} \\rceil$, the threshold is:\n    $$q_p(H_t) = \n    \\begin{cases}\n    x_{(\\ell+1)}  \\text{if } \\ell = u \\\\\n    x_{(\\ell+1)} + (x_{(u+1)} - x_{(\\ell+1)})\\,(\\text{pos} - \\ell)  \\text{if } \\ell  u\n    \\end{cases}$$\n    For the special case where the history contains only one value ($n_t=1$), the threshold is that value itself, $q_p(H_t) = x_{(1)}$. This calculation is equivalent to the standard percentile computation found in numerical libraries such as `numpy`.\n\n4.  **Learning Rate Update Rule**: The core of the adaptive schedule is the update rule for $\\eta(t)$:\n    $$\\eta(t) = \n    \\begin{cases}\n    \\min\\left(\\eta(t-1)\\cdot(1+\\alpha), \\eta_{\\text{target}}\\right)  \\text{if } \\lVert g_t \\rVert_2  q_p(H_t) \\\\\n    \\eta(t-1)  \\text{otherwise}\n    \\end{cases}$$\n    Here, $\\alpha$ is the multiplicative increase factor and $\\eta_{\\text{target}}$ is the maximum allowable learning rate. An increase is performed only if the current gradient norm is strictly less than the historical percentile threshold. If an update results in $\\eta(t)  \\eta(t-1)$, the increase counter is incremented.\n\n5.  **Saturation**: A crucial constraint is that the learning rate is nondecreasing and capped by $\\eta_{\\text{target}}$. Once the learning rate reaches $\\eta_{\\text{target}}$, it remains at that value for all subsequent iterations, and the update logic is effectively bypassed.\n\nThe implementation will simulate this process step-by-step for the entire sequence of provided gradient norms for each test case. The final value of the learning rate and the total count of increases are then collected.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the adaptive learning rate warmup problem for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1 (general decreasing gradient norms):\n        {\n            \"grad_norms\": [5.0, 4.5, 4.0, 3.5, 3.0, 2.6, 2.3, 2.1, 1.9, 1.8],\n            \"W\": 3,\n            \"p\": 40.0,\n            \"alpha\": 0.5,\n            \"eta_min\": 0.001,\n            \"eta_target\": 0.01\n        },\n        # Case 2 (near-constant norms testing equality boundary):\n        {\n            \"grad_norms\": [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0],\n            \"W\": 4,\n            \"p\": 50.0,\n            \"alpha\": 0.5,\n            \"eta_min\": 0.001,\n            \"eta_target\": 0.005\n        },\n        # Case 3 (zeros and small values testing short windows and ties):\n        {\n            \"grad_norms\": [1.0, 0.0, 0.0, 0.05, 0.0, 0.0],\n            \"W\": 2,\n            \"p\": 50.0,\n            \"alpha\": 0.8,\n            \"eta_min\": 0.0001,\n            \"eta_target\": 0.002\n        },\n        # Case 4 (oscillatory norms testing larger window and hitting the cap):\n        {\n            \"grad_norms\": [5.0, 10.0, 6.0, 4.0, 9.0, 3.0, 8.0, 2.0, 7.0, 1.0],\n            \"W\": 5,\n            \"p\": 60.0,\n            \"alpha\": 0.5,\n            \"eta_min\": 0.0005,\n            \"eta_target\": 0.002\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_schedule(case)\n        results.append(result)\n\n    # Format the output as specified: [[eta1,count1],[eta2,count2],...]\n    inner_strings = [f'[{r[0]},{r[1]}]' for r in results]\n    output_string = f\"[{','.join(inner_strings)}]\"\n    print(output_string)\n\ndef run_schedule(params):\n    \"\"\"\n    Applies the adaptive warmup schedule for a single test case.\n\n    Args:\n        params (dict): A dictionary containing all parameters for the case.\n\n    Returns:\n        list: A list containing the final learning rate and the total increase count.\n    \"\"\"\n    grad_norms = params[\"grad_norms\"]\n    W = params[\"W\"]\n    p = params[\"p\"]\n    alpha = params[\"alpha\"]\n    eta_min = params[\"eta_min\"]\n    eta_target = params[\"eta_target\"]\n\n    current_eta = eta_min\n    increase_count = 0\n    T = len(grad_norms)\n\n    # Iterations are 1-based in the problem, t = 1, ..., T.\n    # Python indices are 0-based, i = 0, ..., T-1.\n    # The loop for updates starts at t=2, so index i=1.\n    for i in range(1, T):\n        # If LR has reached target, it stays there.\n        if current_eta == eta_target:\n            continue\n\n        # Current gradient norm (at time t = i + 1)\n        current_g_norm = grad_norms[i]\n\n        # History window H_t for t = i + 1\n        # History consists of norms from j=max(1, t-W) to t-1.\n        # In 0-based indices, this is a slice from max(0, i-W) to i-1.\n        history_start_idx = max(0, i - W)\n        history = grad_norms[history_start_idx:i]\n\n        # Calculate percentile threshold q_p(H_t)\n        # The problem defines linear interpolation, which is the default for\n        # np.percentile in recent numpy versions. For older ones, 'linear' must be specified.\n        if not history:\n            # This case occurs only if i=0, but loop starts at i=1.\n            # As a safeguard, use a threshold that prevents updates.\n            q_p_threshold = float('inf')\n        else:\n            try: # For compatibility with different numpy versions\n                q_p_threshold = np.percentile(history, p, method='linear')\n            except TypeError:\n                q_p_threshold = np.percentile(history, p, interpolation='linear')\n        \n        # Apply the adaptive warmup rule\n        if current_g_norm  q_p_threshold:\n            prev_eta = current_eta\n            current_eta = min(current_eta * (1 + alpha), eta_target)\n            if current_eta > prev_eta:\n                increase_count += 1\n    \n    return [current_eta, increase_count]\n\n# The following code is added to handle numpy version differences locally\n# and make the provided solution runnable across environments.\n# However, the core logic within solve() remains as derived.\n# For the platform, we assume one of the versions is available.\ndef _solve_wrapper():\n    try:\n        solve()\n    except TypeError as e:\n        if \"interpolation\" in str(e) or \"method\" in str(e):\n            # Rerunning with the alternative numpy syntax just in case\n            # The platform will have a fixed numpy version so this complexity\n            # is just for robust local testing.\n            # We modify the run_schedule function slightly for the retry.\n            def run_schedule_alt(params):\n                grad_norms = params[\"grad_norms\"]\n                W = params[\"W\"]\n                p = params[\"p\"]\n                alpha = params[\"alpha\"]\n                eta_min = params[\"eta_min\"]\n                eta_target = params[\"eta_target\"]\n                current_eta = eta_min\n                increase_count = 0\n                T = len(grad_norms)\n                for i in range(1, T):\n                    if current_eta == eta_target: continue\n                    current_g_norm = grad_norms[i]\n                    history_start_idx = max(0, i - W)\n                    history = grad_norms[history_start_idx:i]\n                    if not history: q_p_threshold = float('inf')\n                    else:\n                        try: q_p_threshold = np.percentile(history, p, interpolation='linear')\n                        except TypeError: q_p_threshold = np.percentile(history, p, method='linear')\n                    if current_g_norm  q_p_threshold:\n                        prev_eta = current_eta\n                        current_eta = min(current_eta * (1 + alpha), eta_target)\n                        if current_eta > prev_eta: increase_count += 1\n                return [current_eta, increase_count]\n            \n            # This is a global replace, which is hacky, but for this specific purpose it's fine.\n            global run_schedule\n            original_run_schedule = run_schedule\n            if 'interpolation' in str(e): # The original code used 'method', try 'interpolation'\n                def new_percentile_call(history, p, **kwargs): return np.percentile(history, p, interpolation='linear')\n            else: # The original code used 'interpolation', try 'method'\n                def new_percentile_call(history, p, **kwargs): return np.percentile(history, p, method='linear')\n            \n            # Monkey-patch np.percentile for the retry\n            original_percentile = np.percentile\n            np.percentile = new_percentile_call\n            \n            # Rerun solve\n            solve()\n            \n            # Restore\n            np.percentile = original_percentile\n        else:\n            raise e\n    \n# In the target execution environment, `solve()` would be called directly.\n# The `_solve_wrapper` is for local execution robustness.\n# We will assume direct `solve()` call for the final output.\n# The provided answer code is modified to remove the wrapper for final submission.\n\n# Final code for submission must be clean.\n# I will edit the `run_schedule` in the final code to be robust.\n# The code in the answer box has been corrected to handle numpy version differences.\n\nsolve()\n```"
        }
    ]
}