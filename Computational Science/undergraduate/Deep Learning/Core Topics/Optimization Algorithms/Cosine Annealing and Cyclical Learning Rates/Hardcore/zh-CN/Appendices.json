{
    "hands_on_practices": [
        {
            "introduction": "理论是重要的，但通过实践来巩固理解更为关键。这项练习将带你直观地感受循环学习率的核心优势。我们将通过在一个精心设计的非凸损失函数上进行梯度下降，亲眼见证带有“热重启”的余弦退火学习率策略如何帮助优化器跳出浅层局部最小值，而传统的单调递减学习率则会受困其中。",
            "id": "3110220",
            "problem": "给定一个由四次多项式 $f(\\theta)=\\theta^4-2a\\theta^2+b\\theta$ 定义的合成一维非凸损失，其中 $a$ 和 $b$ 为实数参数。考虑确定性梯度下降，其参数更新遵循规则 $\\theta_{t+1}=\\theta_t-\\eta_t\\nabla f(\\theta_t)$，其中步骤 $t$ 处的标量学习率为 $\\eta_t>0$，梯度为 $\\nabla f(\\theta)=\\frac{d}{d\\theta}f(\\theta)$。目标是从第一性原理出发，研究学习率调度的选择如何影响梯度下降在可能同时存在浅层和深层局部最小值的非凸景观上取得进展的能力。\n\n您的任务：\n\n1. 从损失的定义和基本微积分出发，推导确定性梯度下降所需的解析梯度 $\\nabla f(\\theta)$。\n\n2. 从长度为 $T$ 的周期上的学习率序列 $\\{\\eta_t\\}_{t=0}^{T-1}$ 的定义出发，并结合其需满足平滑、在边界处斜率为零（以避免突变）、在每个周期开始时达到最大值 $\\eta_{\\max}$、在每个周期结束时达到最小值 $\\eta_{\\min}$ 的约束条件，推导一个满足这些约束的单周期学习率调度。然后定义一个每 $T$ 步重复此调度的热重启机制。任何三角函数的角度都必须以弧度表示。\n\n3. 定义一个与之对比的单调学习率调度 $\\eta_t$，该调度根据一个经过充分检验的规则随时间严格递减。使用该调度执行无重启的确定性梯度下降。\n\n4. 实现这两种调度，并对固定的梯度下降步数 (epochs) 运行确定性梯度下降。对于下面套件中的每个测试用例，计算在热重启调度下和单调调度下获得的最终损失。对于每个用例，返回一个布尔值：当且仅当热重启下的最终损失比单调衰减下的最终损失严格小至少一个裕量 $\\varepsilon$（使用 $\\varepsilon=10^{-2}$）时，该值为真，否则为假。此布尔值旨在操作化一个陈述，即与单调衰减相比，热重启能够逃离浅层局部最小值，其含义是在相同步数后达到一个严格更低的最终目标值。\n\n使用以下测试套件。每个测试用例是一个元组，指定了 $(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)$，其中 $a$ 和 $b$ 是损失函数 $f(\\theta)$ 的参数，$\\theta_0$ 是初始参数值，$E$ 是梯度下降步数 (epochs)，$(\\eta_{\\max},\\eta_{\\min},T)$ 指定了热重启调度的最大值、最小值和周期，$(\\eta_0,\\gamma)$ 指定了单调调度的初始学习率和乘法衰减因子：\n\n- 用例 1（一般非凸情况，热重启有发挥作用的空间）：$(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)=(1.0,0.01,1.30,120,0.20,0.0005,30,0.005,0.90)$。\n- 用例 2（类边界情况：一个长周期近似于单调衰减）：$(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)=(1.0,0.01,1.30,120,0.050,0.049,120,0.050,0.99)$。\n- 用例 3（具有强线性项的近似凸区域）：$(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)=(0.25,1.20,0.00,100,0.10,0.001,25,0.10,0.97)$。\n- 用例 4（频繁重启）：$(a,b,\\theta_0,E,\\eta_{\\max},\\eta_{\\min},T,\\eta_0,\\gamma)=(1.0,0.05,1.30,100,0.20,0.0005,10,0.010,0.95)$。\n\n角度单位要求：任何对三角函数的使用都必须采用弧度。\n\n输出规格：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[result1,result2,result3,result4]”），其中每个结果是对应于上述顺序的测试用例的布尔值。",
            "solution": "我们从合成非凸损失 $f(\\theta)=\\theta^4-2a\\theta^2+b\\theta$ 开始。根据导数的定义和基本微积分，通过逐项求导可得其解析梯度：\n$$\n\\nabla f(\\theta)=\\frac{d}{d\\theta}\\left(\\theta^4-2a\\theta^2+b\\theta\\right)=4\\theta^3-4a\\theta+b.\n$$\n确定性梯度下降使用以下规则更新参数：\n$$\n\\theta_{t+1}=\\theta_t-\\eta_t\\nabla f(\\theta_t)=\\theta_t-\\eta_t\\left(4\\theta_t^3-4a\\theta_t+b\\right),\n$$\n其中 $\\eta_t$ 是第 $t$ 次迭代的学习率。该规则直接源于最速下降原理：沿负梯度方向移动，并由一个正步长进行缩放。\n\n接下来，我们推导一个在长度为 $T$ 的周期上满足平滑性和边界约束的热重启学习率调度。我们需要一个函数 $g:[0,T]\\to\\mathbb{R}$，使得：\n1. $g(0)=\\eta_{\\max}$，\n2. $g(T)=\\eta_{\\min}$，\n3. $g'(0)=0$ 且 $g'(T)=0$ 以确保边界平滑，\n4. $g$ 在 $(0,T)$ 内足够平滑，\n5. 该调度每 $T$ 步重启一次，因此有效的 $\\eta_t=g(t\\bmod T)$。\n\n一个满足这些约束的最小平滑函数可以由一个在半周期内经过平移和缩放的余弦函数构造。考虑余弦函数，它平滑、是偶函数，且在其极值点处斜率为零。在区间 $[0,T]$ 上，函数 $\\cos\\left(\\pi t/T\\right)$ 满足 $\\cos(0)=1$、$\\cos(\\pi)=-1$，并且在两端点处的斜率均为零。通过缩放和平移以匹配边界值，可得：\n$$\ng(t)=\\eta_{\\min}+\\frac{1}{2}(\\eta_{\\max}-\\eta_{\\min})\\left(1+\\cos\\left(\\pi\\frac{t}{T}\\right)\\right).\n$$\n这满足所有陈述的条件，且按要求角度以弧度为单位。为了引入热重启（在此确定性设置中，即带重启的随机梯度下降 (Stochastic Gradient Descent with Restarts, SGDR)），我们通过设置以下公式每 $T$ 步重复一次 $g$：\n$$\n\\eta_t=\\eta_{\\min}+\\frac{1}{2}(\\eta_{\\max}-\\eta_{\\min})\\left(1+\\cos\\left(\\pi\\frac{t\\bmod T}{T}\\right)\\right).\n$$\n每次重启都将学习率重置为 $\\eta_{\\max}$，然后在一个周期内平滑地退火至 $\\eta_{\\min}$，通过间歇性地增大学习步长来鼓励周期性探索并逃离浅层区域。\n\n作为对比，我们定义一个根据经过充分检验的指数规则随时间递减的单调学习率调度：\n$$\n\\eta_t=\\eta_0\\gamma^t,\n$$\n其中 $\\eta_0>0$ 且 $0<\\gamma<1$，从而提供一个严格递减的序列。这种单调衰减在实践中被广泛使用，它会随时间减小步长，这有助于收敛，但可能因步长减小而阻碍逃离浅层局部最小值。\n\n算法设计：\n- 对于每个测试用例，计算两种机制下的最终损失：带热重启的余弦退火和单调指数衰减。\n- 初始化 $\\theta_0$，并使用各自的调度进行 $E$ 步确定性梯度下降迭代。在每一步 $t$，计算梯度 $\\nabla f(\\theta_t)=4\\theta_t^3-4a\\theta_t+b$ 并更新 $\\theta_{t+1}=\\theta_t-\\eta_t\\nabla f(\\theta_t)$。\n- 在 $E$ 步之后，计算两种调度下的 $f(\\theta_E)$。返回一个布尔值，表示热重启下的最终损失是否比单调衰减下的最终损失严格小至少 $\\varepsilon=10^{-2}$：\n$$\n\\text{result}=\\left(f_{\\text{warm}}+ \\varepsilon < f_{\\text{mono}}\\right).\n$$\n\n基于原理的解释：\n- 在像四次函数 $f$ 这样的非凸景观中，浅层局部最小值和平台会导致梯度变小。在单调衰减下，步长持续缩小，这可能因为乘积 $\\eta_t\\lVert\\nabla f(\\theta_t)\\rVert$ 变得过小而无法取得有意义的进展，从而将迭代点困在次优的浅层区域附近。\n- 带热重启的余弦退火会周期性地将学习率提高到 $\\eta_{\\max}$，产生更大的步长，从而能将迭代点移出浅谷或平台。由于该调度是平滑的，并且在周期边界处的斜率为零，因此它避免了可能破坏训练稳定性的突然跳跃，同时仍在重启时注入了足够的探索能力。\n\n测试套件理据：\n- 用例 1 提供了一个非凸设置，其参数使得浅层区域易于区分；预计热重启将优于单调衰减，从而得到 true。\n- 用例 2 使用一个单一的长周期和接近恒定的学习率，以近似单调行为；重启的优势减弱，通常得到 false。\n- 用例 3 有一个强线性项，使损失更接近于类凸行为（实际上只有一个主导的盆地区域），在这种情况下，两种调度往往表现相似，得到 false。\n- 用例 4 使用频繁重启来引发大量的周期性探索，这可以相对于单调衰减改善最终损失，可能得到 true。\n\n最终输出是单行文本，其中包含一个按测试用例顺序排列的四个布尔值的列表，格式如上所述。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f(theta, a, b):\n    # Quartic nonconvex loss f(theta) = theta^4 - 2 a theta^2 + b theta\n    return theta**4 - 2.0*a*theta**2 + b*theta\n\ndef grad_f(theta, a, b):\n    # Analytical gradient: df/dtheta = 4 theta^3 - 4 a theta + b\n    return 4.0*theta**3 - 4.0*a*theta + b\n\ndef cosine_annealing_lr(t, eta_max, eta_min, T):\n    # Cosine schedule over one period T with warm restarts.\n    # Angle in radians; uses a half-cosine anneal from eta_max to eta_min.\n    # t_mod cycles every T steps.\n    t_mod = t % T\n    return eta_min + 0.5*(eta_max - eta_min)*(1.0 + np.cos(np.pi * (t_mod / T)))\n\ndef monotone_exp_lr(t, eta0, gamma):\n    # Monotone exponential decay: eta_t = eta0 * gamma^t\n    return eta0 * (gamma ** t)\n\ndef run_descent(a, b, theta0, E, lr_schedule_fn, lr_params):\n    theta = theta0\n    for t in range(E):\n        eta_t = lr_schedule_fn(t, *lr_params)\n        g = grad_f(theta, a, b)\n        theta = theta - eta_t * g\n    return f(theta, a, b), theta\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (a, b, theta0, E, eta_max, eta_min, T, eta0, gamma)\n    test_cases = [\n        (1.0, 0.01, 1.30, 120, 0.20, 0.0005, 30, 0.005, 0.90),   # Case 1\n        (1.0, 0.01, 1.30, 120, 0.050, 0.049, 120, 0.050, 0.99),  # Case 2\n        (0.25, 1.20, 0.00, 100, 0.10, 0.001, 25, 0.10, 0.97),    # Case 3\n        (1.0, 0.05, 1.30, 100, 0.20, 0.0005, 10, 0.010, 0.95),   # Case 4\n    ]\n\n    epsilon = 1e-2  # Margin for declaring warm restarts strictly better\n\n    results = []\n    for case in test_cases:\n        a, b, theta0, E, eta_max, eta_min, T, eta0, gamma = case\n\n        # Warm restarts via cosine annealing with restarts\n        warm_loss, warm_theta = run_descent(\n            a, b, theta0, E,\n            lr_schedule_fn=cosine_annealing_lr,\n            lr_params=(eta_max, eta_min, T)\n        )\n\n        # Monotone exponential decay\n        mono_loss, mono_theta = run_descent(\n            a, b, theta0, E,\n            lr_schedule_fn=monotone_exp_lr,\n            lr_params=(eta0, gamma)\n        )\n\n        # Boolean result: True if warm_loss + epsilon  mono_loss\n        result = (warm_loss + epsilon)  mono_loss\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在真实的深度学习训练中，梯度是带有噪声的，这为优化过程增加了新的挑战。这项练习将我们从确定性环境带入随机梯度下降 (SGD) 的世界，探索循环学习率与梯度噪声的相互作用。你将研究梯度裁剪这一常用技术如何影响不同形状的循环策略（如三角和余弦）在高学习率阶段的收敛速度。",
            "id": "3110142",
            "problem": "您需要编写一个完整、可运行的程序，模拟在一维凸二次损失上使用周期性学习率和梯度噪声裁剪的随机梯度下降。您的目标是量化梯度噪声的裁剪如何与高学习率阶段相互作用，以及在不同周期性学习率策略下对收敛速度的影响。所有角度都必须以弧度处理。\n\n基本原理和设置：\n- 考虑凸二次目标函数 $f(x) = \\tfrac{1}{2} x^2$。其梯度为 $\\nabla f(x) = x$。\n- 在离散迭代 $t \\in \\{1,2,\\dots\\}$ 时，定义随机梯度形式为 $g_t = \\nabla f(x_{t-1}) + \\xi_t$，其中 $\\xi_t$ 是附加的零均值高斯噪声，其方差为 $\\sigma^2$，即 $\\xi_t \\sim \\mathcal{N}(0,\\sigma^2)$。\n- 引入梯度噪声裁剪如下：噪声项在一维上按分量被 $c \\ge 0$ 裁剪，因此有效噪声为 $\\tilde{\\xi}_t = \\mathrm{clip}(\\xi_t, -c, c)$。等价地，$\\tilde{\\xi}_t = \\max(-c, \\min(c, \\xi_t))$。随机梯度变为 $g_t = x_{t-1} + \\tilde{\\xi}_t$。不对确定性梯度进行裁剪，只裁剪噪声项。不进行裁剪的约定是 $c = +\\infty$。\n- 更新规则为 $x_t = x_{t-1} - \\eta_t g_t$，其中 $\\eta_t$ 是一个依赖于 $t$ 的周期性学习率。\n\n周期性学习率策略：\n- 您必须实现两种周期为 $P \\in \\mathbb{N}$ 的带重启的周期性策略：\n  1) 三角形重启策略，在每个包含 $P$ 次迭代的周期内，学习率从周期开始时的最大学习率 $\\eta_{\\max}$ 线性下降到周期结束时的最小学习率 $\\eta_{\\min}$，然后在下一个周期开始时瞬间重启至 $\\eta_{\\max}$。这是一个分段线性策略；不要为其使用任何三角函数。\n  2) 平滑重启策略，在每个包含 $P$ 次迭代的周期内，学习率从周期开始时的 $\\eta_{\\max}$ 平滑下降到周期结束时的 $\\eta_{\\min}$，且在两个端点处的斜率均为零。推导一个满足这些端点和对称属性的单位区间上的合适平滑周期映射，并使用余弦函数（角度以弧度为单位）表示它。不要使用任何角度制；所有三角函数参数必须是弧度。\n- 两种策略都是带重启的周期性策略，在每个长度为 $P$ 的周期内，它们在重启前都恰好从 $\\eta_{\\max}$ 遍历到 $\\eta_{\\min}$ 一次。\n\n收敛速度的测量：\n- 对于给定的参数集，定义一个损失函数的收敛阈值 $\\tau  0$。令首次到达时间为最小的迭代索引 $t \\in \\{1,2,\\dots\\}$，使得 $f(x_t) \\le \\tau$。如果在指定的迭代预算 $T_{\\max}$ 内从未发生这种情况，则将首次到达时间定义为 $T_{\\max}$。\n- 因为 $g_t$ 是随机的，所以通过运行 $R$ 次独立重复实验并取其首次到达时间的中位数来估计一个稳健的代表性收敛速度。每次重复实验都必须由一个独立的随机种子驱动，该种子从一个基础种子和重复实验索引确定性地派生而来，以确保结果是可复现的。将中位数四舍五入到最近的整数，以报告整数次迭代。\n\n模拟协议：\n- 确定性地初始化 $x_0 = x_{\\text{init}}$。\n- 对于每次重复实验 $r \\in \\{0,1,\\dots,R-1\\}$ 和每次迭代 $t \\in \\{1,2,\\dots,T_{\\max}\\}$：\n  1) 使用周期 $P$、$\\eta_{\\min}$ 和 $\\eta_{\\max}$ 从所选的周期性策略计算学习率 $\\eta_t$。\n  2) 使用一个伪随机数生成器抽取噪声 $\\xi_t \\sim \\mathcal{N}(0,\\sigma^2)$，该生成器的种子由指定的基础种子、案例索引和 $r$ 确定性地设置。将其裁剪为 $\\tilde{\\xi}_t = \\mathrm{clip}(\\xi_t, -c, c)$，约定 $c = +\\infty$ 表示禁用裁剪。\n  3) 形成随机梯度 $g_t = x_{t-1} + \\tilde{\\xi}_t$ 并更新 $x_t = x_{t-1} - \\eta_t g_t$。\n  4) 更新后，检查是否 $f(x_t) \\le \\tau$；如果是，则记录当前 $t$ 作为该次重复实验的结果并停止该次实验。如果在 $T_{\\max}$ 之前没有出现这样的 $t$，则记录 $T_{\\max}$。\n\n角度单位要求：\n- 所有余弦函数的使用必须将其参数解释为弧度。\n\n测试套件：\n使用以下四个参数案例，每个案例需要两次测量：一次使用平滑的基于余弦的重启策略，另一次使用三角形重启策略。在所有案例中，使用 $R = 200$ 次重复实验，基础种子 $s_{\\text{base}} = 12345$，以及 $x_{\\text{init}} = 5.0$。\n- 案例 A（带有适度裁剪的基准线）：$\\eta_{\\min} = 0.05$, $\\eta_{\\max} = 0.8$, $P = 40$, $\\sigma = 0.5$, $c = 0.2$, $\\tau = 0.01$, $T_{\\max} = 1000$。\n- 案例 B（裁剪完全消除噪声）：$\\eta_{\\min} = 0.05$, $\\eta_{\\max} = 0.8$, $P = 40$, $\\sigma = 0.5$, $c = 0$, $\\tau = 0.01$, $T_{\\max} = 1000$。\n- 案例 C（无裁剪）：$\\eta_{\\min} = 0.05$, $\\eta_{\\max} = 0.8$, $P = 40$, $\\sigma = 0.5$, $c = +\\infty$, $\\tau = 0.01$, $T_{\\max} = 1000$。\n- 案例 D（高最大学习率）：$\\eta_{\\min} = 0.1$, $\\eta_{\\max} = 1.8$, $P = 40$, $\\sigma = 0.5$, $c = 0.2$, $\\tau = 0.01$, $T_{\\max} = 1000$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个扁平列表中的八个整数结果，顺序为\n$[$A-余弦, A-三角形, B-余弦, B-三角形, C-余弦, C-三角形, D-余弦, D-三角形$]$，\n各项之间用逗号分隔，无多余空格，并用方括号括起。例如，一个语法正确的输出应类似于 $[12,10,8,8,14,13,20,18]$（这些数字仅为示例）。",
            "solution": "该问题是有效的，因为它具有科学依据、设定良好且客观。它描述了机器学习优化领域中的一个标准模拟任务。我们着手提供解决方案。\n\n目标是分析随机梯度下降（SGD）在一个简单的一维凸二次函数 $f(x) = \\tfrac{1}{2} x^2$ 上的收敛行为。该函数的梯度为 $\\nabla f(x) = x$。SGD 算法从一个初始点 $x_0$ 开始，迭代地更新位置估计 $x_t$。在迭代 $t \\in \\{1, 2, \\dots\\}$ 时的更新规则由下式给出：\n$$ x_t = x_{t-1} - \\eta_t g_t $$\n其中 $\\eta_t$ 是在迭代 $t$ 时的学习率，$g_t$ 是随机梯度。随机梯度被建模为前一位置 $x_{t-1}$ 处的真实梯度加上一个附加噪声项：\n$$ g_t = \\nabla f(x_{t-1}) + \\tilde{\\xi}_t = x_{t-1} + \\tilde{\\xi}_t $$\n噪声项 $\\tilde{\\xi}_t$ 源自一个零均值高斯随机变量 $\\xi_t \\sim \\mathcal{N}(0, \\sigma^2)$，然后对其进行裁剪。裁剪操作由一个非负阈值 $c \\ge 0$ 定义：\n$$ \\tilde{\\xi}_t = \\mathrm{clip}(\\xi_t, -c, c) = \\max(-c, \\min(c, \\xi_t)) $$\n$c = +\\infty$ 的值对应于无裁剪，而 $c = 0$ 则完全去除了噪声。\n\n学习率 $\\eta_t$ 遵循一个带重启的周期性策略，周期为 $P$ 次迭代。我们需要实现两种这样的策略，两者都在每个周期内从最大值 $\\eta_{\\max}$ 过渡到最小值 $\\eta_{\\min}$。令 $i_t = (t-1) \\pmod P$ 为全局迭代 $t \\ge 1$ 在周期内的零索引步。周期的完成比例为 $\\alpha_t = \\frac{i_t}{P-1}$ (对于 $P  1$)。\n\n1.  **三角形重启策略**：该策略在单个周期内实现学习率从 $\\eta_{\\max}$ 到 $\\eta_{\\min}$ 的线性下降。此线性插值的公式为：\n    $$ \\eta_t = (1 - \\alpha_t)\\eta_{\\max} + \\alpha_t\\eta_{\\min} = \\eta_{\\max} - \\left( \\frac{(t-1) \\pmod P}{P-1} \\right) (\\eta_{\\max} - \\eta_{\\min}) $$\n    在周期开始时（$t$ 使得 $(t-1) \\pmod P = 0$），$\\alpha_t=0$ 且 $\\eta_t = \\eta_{\\max}$。在周期结束时（$t$ 使得 $(t-1) \\pmod P = P-1$），$\\alpha_t=1$ 且 $\\eta_t = \\eta_{\\min}$。\n\n2.  **平滑重启（余弦退火）策略**：此策略确保在周期的起点和终点处具有零斜率的平滑过渡。一个从分数进度 $\\alpha_t \\in [0, 1]$ 到归一化范围 $[1, 0]$ 并满足这些性质的合适映射是半周期余弦函数 $g(\\alpha) = \\frac{1}{2}(1 + \\cos(\\pi \\alpha))$。最终的学习率通过对此函数进行缩放和平移以适应 $[\\eta_{\\min}, \\eta_{\\max}]$ 范围来获得：\n    $$ \\eta_t = \\eta_{\\min} + (\\eta_{\\max} - \\eta_{\\min}) g(\\alpha_t) = \\eta_{\\min} + \\frac{1}{2}(\\eta_{\\max} - \\eta_{\\min}) \\left(1 + \\cos\\left(\\pi \\frac{(t-1) \\pmod P}{P-1}\\right)\\right) $$\n    根据要求，余弦函数的参数以弧度为单位。\n\n收敛速度由首次到达时间来衡量，定义为损失 $f(x_t) = \\frac{1}{2} x_t^2$ 低于给定阈值 $\\tau  0$ 的最小迭代次数 $t$。如果在 $T_{\\max}$ 次迭代的预算内未能收敛，则到达时间记录为 $T_{\\max}$。为了获得稳健的估计，此过程对 $R$ 个独立的重复实验进行重复，并将所得首次到达时间的中位数作为最终度量。然后将该中位数四舍五入到最近的整数。\n\n为了保证可复现性以及两种策略之间的公平比较，模拟协议采用确定性种子策略。对于四个测试案例（A、B、C、D）中的每一个以及每一次重复实验 $r \\in \\{0, 1, \\dots, R-1\\}$，都会根据基础种子 $s_{\\text{base}}$、案例索引和重复实验索引 $r$ 生成一个唯一的种子。在同一次重复实验中，余弦和三角形策略的模拟都使用相同的种子来初始化随机数生成器，确保它们面对相同的随机噪声抽取序列 $\\xi_t$。这种配对实验设计允许在相同的随机条件下直接比较策略的性能。\n\n程序将对四个指定的参数案例实施此模拟，计算每种情况下两种学习率策略的中位首次到达时间，并按要求将八个得到的整数值格式化为单个列表。",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Simulates SGD with cyclical learning rates to find median convergence time.\n    \"\"\"\n    # Define test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'eta_min': 0.05, 'eta_max': 0.8, 'P': 40, 'sigma': 0.5, 'c': 0.2, 'tau': 0.01, 'T_max': 1000},\n        # Case B\n        {'eta_min': 0.05, 'eta_max': 0.8, 'P': 40, 'sigma': 0.5, 'c': 0.0, 'tau': 0.01, 'T_max': 1000},\n        # Case C\n        {'eta_min': 0.05, 'eta_max': 0.8, 'P': 40, 'sigma': 0.5, 'c': np.inf, 'tau': 0.01, 'T_max': 1000},\n        # Case D\n        {'eta_min': 0.1, 'eta_max': 1.8, 'P': 40, 'sigma': 0.5, 'c': 0.2, 'tau': 0.01, 'T_max': 1000},\n    ]\n\n    # Global simulation parameters\n    R = 200\n    base_seed = 12345\n    x_init = 5.0\n    \n    final_results = []\n\n    for case_idx, params in enumerate(test_cases):\n        eta_min = params['eta_min']\n        eta_max = params['eta_max']\n        P = params['P']\n        sigma = params['sigma']\n        c = params['c']\n        tau = params['tau']\n        T_max = params['T_max']\n\n        hitting_times_cosine = []\n        hitting_times_triangular = []\n        \n        for r in range(R):\n            # Generate a unique, deterministic seed for each replicate of each case\n            seed = base_seed + case_idx * R + r\n            \n            # --- Paired Simulation for Cosine Schedule ---\n            rng = np.random.default_rng(seed)\n            x = x_init\n            h_time_cosine = T_max\n            for t in range(1, T_max + 1):\n                # Calculate learning rate using cosine annealing schedule\n                i_cycle = (t - 1) % P\n                alpha_t = i_cycle / (P - 1) if P > 1 else 1.0\n                eta = eta_min + 0.5 * (eta_max - eta_min) * (1 + math.cos(math.pi * alpha_t))\n                \n                # Generate and clip noise\n                noise = rng.normal(0, sigma)\n                clipped_noise = np.clip(noise, -c, c)\n                \n                # Perform SGD update\n                stochastic_grad = x + clipped_noise\n                x = x - eta * stochastic_grad\n                \n                # Check for convergence\n                loss = 0.5 * x**2\n                if loss = tau:\n                    h_time_cosine = t\n                    break\n            hitting_times_cosine.append(h_time_cosine)\n\n            # --- Paired Simulation for Triangular Schedule ---\n            # Re-seed the generator to get the same noise sequence for this replicate\n            rng = np.random.default_rng(seed)\n            x = x_init\n            h_time_triangular = T_max\n            for t in range(1, T_max + 1):\n                # Calculate learning rate using triangular schedule\n                i_cycle = (t - 1) % P\n                alpha_t = i_cycle / (P - 1) if P > 1 else 1.0\n                eta = eta_max - alpha_t * (eta_max - eta_min)\n                \n                # Generate and clip noise\n                noise = rng.normal(0, sigma)\n                clipped_noise = np.clip(noise, -c, c)\n                \n                # Perform SGD update\n                stochastic_grad = x + clipped_noise\n                x = x - eta * stochastic_grad\n                \n                # Check for convergence\n                loss = 0.5 * x**2\n                if loss = tau:\n                    h_time_triangular = t\n                    break\n            hitting_times_triangular.append(h_time_triangular)\n            \n        # Calculate and round median hitting times\n        median_cosine = np.median(hitting_times_cosine)\n        median_triangular = np.median(hitting_times_triangular)\n\n        final_results.append(int(np.round(median_cosine)))\n        final_results.append(int(np.round(median_triangular)))\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "设置循环学习率的超参数（尤其是最大学习率 $\\eta_{\\max}$）本身就是一个挑战。这项高级练习将向你展示一种基于优化理论的自适应方法来解决此问题。你将实现一个控制器，它通过幂迭代法动态估计损失函数的局部曲率，并以此为依据来设定 $\\eta_{\\max}$ 的上限，从而在保证训练稳定性的前提下实现高效探索。",
            "id": "3110195",
            "problem": "要求您实现一个完整的、可运行的程序，该程序在深度学习的梯度下降 (Gradient Descent, GD) 背景下，使用余弦退火 (Cosine Annealing, CA) 构建一个周期性学习率调度的控制器。该控制器必须根据通过对对称半正定矩阵进行幂迭代获得的曲率估计来调整每个周期的最大学习率，并且必须强制执行最大学习率不大于从最大曲率估计推导出的倒数稳定性阈值的约束。实现必须遵循标准的数学和算法原理，而不依赖于问题陈述中提供的快捷公式。\n\n您必须使用的基本依据是以下广为接受的事实：\n- 对于一个光滑损失函数在某点附近的局部二次近似，损失可以写为 $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} \\mathbf{H} \\mathbf{x}$，其中 $\\mathbf{H}$ 是一个表示曲率的对称半正定矩阵。\n- 梯度下降 (GD) 的迭代过程是 $\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\eta \\nabla f(\\mathbf{x}_{k})$，在二次近似情况下，它简化为一个由矩阵 $\\mathbf{I} - \\eta \\mathbf{H}$ 驱动的线性系统。\n- GD 在二次目标函数上的稳定性由更新矩阵的谱半径控制，其上界为 $\\mathbf{H}$ 的最大特征值。\n- 对称矩阵的最大特征值可以通过幂迭代从第一性原理近似得到，该方法利用瑞利商，无需显式进行特征分解。\n\n您必须在程序中实现以下组件：\n- 一个幂迭代例程，给定一个对称半正定矩阵 $\\mathbf{H}$ 和一个迭代预算 $K$，返回 $\\mathbf{H}$ 的最大特征值的估计值 $\\hat{\\lambda}_{\\max}$。\n- 一个控制器，在每个周期开始时，将最大学习率 $\\eta_{\\max}$ 设置为 $\\min\\left(\\eta_{\\text{target}}, \\tfrac{2}{\\hat{\\lambda}_{\\max}}\\right)$，其中 $\\eta_{\\text{target}}$ 是用户指定的目标振幅，$\\tfrac{2}{\\hat{\\lambda}_{\\max}}$ 是从二次 GD 动态推导出的稳定性阈值。当 $\\hat{\\lambda}_{\\max} \\le 0$ 时，将 $\\tfrac{2}{\\hat{\\lambda}_{\\max}}$ 解释为 $+\\infty$。\n- 基于余弦退火 (CA) 的每个周期的周期性调度，角度以弧度为单位。在每个长度为 $T$ 步的周期中，学习率必须从 $\\eta_{\\max}$ 开始，到 $\\eta_{\\min}$ 结束，其中 $\\eta_{\\min}$ 是用户指定的下界。对于边界情况 $T = 1$，调度必须包含一个等于 $\\eta_{\\max}$ 的单一值。\n- 一个验证例程，用于在每个周期检查该周期的所有学习率值 $\\eta_t$ 是否满足 $\\eta_{\\min} \\le \\eta_t \\le \\eta_{\\max}$ 和 $\\eta_t \\le \\tfrac{2}{\\hat{\\lambda}_{\\max}}$，并且当 $T  1$ 时，调度从 $\\eta_{\\max}$ 开始，到 $\\eta_{\\min}$ 结束；当 $T = 1$ 时，调度为恒定的 $\\eta_{\\max}$。\n\n角度单位必须是弧度。不涉及带单位的物理量。\n\n测试套件：\n在以下测试用例上实现并评估您的控制器。每个测试用例指定了曲率矩阵 $\\mathbf{H}$、周期配置和控制器参数。为保证可复现性，每个周期使用给定的独立随机种子进行幂迭代。所有矩阵都是对称的，所有长度和计数都是整数。\n\n- 测试用例 1（正常路径，中等曲率）：\n  - $\\mathbf{H} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  0.5 \\end{bmatrix}$，\n  - $\\eta_{\\min} = 0.0$，\n  - $\\eta_{\\text{target}} = 0.9$，\n  - 周期数 $C = 2$，\n  - 周期长度 $T = 10$，\n  - 幂迭代步数 $K = 50$，\n  - 基础种子 $s = 101$。\n\n- 测试用例 2（高曲率，需要上限约束）：\n  - $\\mathbf{H} = \\begin{bmatrix} 5.0  0.0 \\\\ 0.0  3.0 \\end{bmatrix}$，\n  - $\\eta_{\\min} = 0.05$，\n  - $\\eta_{\\text{target}} = 0.9$，\n  - 周期数 $C = 3$，\n  - 周期长度 $T = 8$，\n  - 幂迭代步数 $K = 30$，\n  - 基础种子 $s = 202$。\n\n- 测试用例 3（接近零的曲率，大的稳定性阈值）：\n  - $\\mathbf{H} = 0.001 \\cdot \\mathbf{I}_{3}$，\n  - $\\eta_{\\min} = 0.0$，\n  - $\\eta_{\\text{target}} = 1.5$，\n  - 周期数 $C = 2$，\n  - 周期长度 $T = 4$，\n  - 幂迭代步数 $K = 20$，\n  - 基础种子 $s = 303$。\n\n- 测试用例 4（病态条件曲率）：\n  - $\\mathbf{H} = \\begin{bmatrix} 10.0  0.0 \\\\ 0.0  0.01 \\end{bmatrix}$，\n  - $\\eta_{\\min} = 0.0$，\n  - $\\eta_{\\text{target}} = 0.25$，\n  - 周期数 $C = 3$，\n  - 周期长度 $T = 7$，\n  - 幂迭代步数 $K = 25$，\n  - 基础种子 $s = 404$。\n\n- 测试用例 5（边界情况 $T = 1$）：\n  - $\\mathbf{H} = \\begin{bmatrix} 2.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$，\n  - $\\eta_{\\min} = 0.1$，\n  - $\\eta_{\\text{target}} = 1.0$，\n  - 周期数 $C = 1$，\n  - 周期长度 $T = 1$，\n  - 幂迭代步数 $K = 40$，\n  - 基础种子 $s = 505$。\n\n您的程序必须生成一行输出，其中包含五个测试用例的验证结果，格式为用方括号括起来的逗号分隔列表，每个项目是一个布尔值，指示该测试用例的所有周期是否都满足所有约束，例如 $\\left[\\text{True},\\text{False},\\dots\\right]$。所有角度单位都必须是弧度。不允许用户输入；程序必须是完全自包含的，并且可以按原样运行。",
            "solution": "我们从光滑深度学习损失函数在某点附近的局部二次近似开始，这是一个经过充分检验的事实。设损失为 $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} \\mathbf{H} \\mathbf{x}$，其中 $\\mathbf{H}$ 是一个捕捉曲率的对称半正定矩阵。学习率为 $\\eta$ 的梯度下降 (GD) 迭代过程为\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\eta \\nabla f(\\mathbf{x}_{k}) = \\mathbf{x}_{k} - \\eta \\mathbf{H} \\mathbf{x}_{k} = \\left(\\mathbf{I} - \\eta \\mathbf{H}\\right)\\mathbf{x}_{k}.\n$$\n该线性系统的收敛性和稳定性取决于谱半径 $\\rho\\left(\\mathbf{I} - \\eta \\mathbf{H}\\right)$。因为 $\\mathbf{H}$ 是对称的，所以它是可对角化的，具有非负实数特征值 $\\lambda_{1}, \\dots, \\lambda_{n}$。矩阵 $\\mathbf{I} - \\eta \\mathbf{H}$ 的特征值是 $1 - \\eta \\lambda_{i}$。稳定性要求\n$$\n\\max_{i} \\left|1 - \\eta \\lambda_{i}\\right|  1,\n$$\n这意味着对于每一个 $i$，\n$$\n-1  1 - \\eta \\lambda_{i}  1 \\quad \\Rightarrow \\quad 0  \\eta \\lambda_{i}  2.\n$$\n为了对所有特征值同时满足这个条件，只需强制执行\n$$\n0  \\eta  \\frac{2}{\\lambda_{\\max}},\n$$\n其中 $\\lambda_{\\max} = \\max_{i} \\lambda_{i}$。这个不等式在二次近似机制下为稳定学习率提供了一个有原则的上界。\n\n我们不通过显式特征分解直接计算 $\\lambda_{\\max}$；而是使用幂迭代法（一种第一性原理算法）来估计它。设 $\\mathbf{v}_{0}$ 是一个非零初始向量。幂迭代的更新过程为\n$$\n\\mathbf{w}_{k} = \\mathbf{H} \\mathbf{v}_{k}, \\quad \\mathbf{v}_{k+1} = \\frac{\\mathbf{w}_{k}}{\\lVert \\mathbf{w}_{k} \\rVert}.\n$$\n在对称矩阵具有唯一最大（按绝对值）特征值的标准条件下，$\\mathbf{v}_{k}$ 会收敛到与 $\\lambda_{\\max}$ 相关的特征向量，并且瑞利商\n$$\n\\hat{\\lambda}_{\\max}^{(k)} = \\mathbf{v}_{k}^{\\top} \\mathbf{H} \\mathbf{v}_{k}\n$$\n会收敛到 $\\lambda_{\\max}$。如果 $\\mathbf{H}$ 是零矩阵，则 $\\mathbf{H}\\mathbf{v}_{k} = \\mathbf{0}$ 且瑞利商为 $0$，我们将其解释为产生一个无限的稳定性阈值 $\\frac{2}{\\hat{\\lambda}_{\\max}} = +\\infty$。\n\n现在我们设计周期性学习率控制器。在每个周期中，在构建学习率调度之前，我们：\n- 对 $\\mathbf{H}$ 运行 $K$ 步幂迭代以获得 $\\hat{\\lambda}_{\\max}$。\n- 如果 $\\hat{\\lambda}_{\\max}  0$，计算稳定性上限 $c = \\frac{2}{\\hat{\\lambda}_{\\max}}$，否则 $c = +\\infty$。\n- 将周期内的最大学习率设置为\n$$\n\\eta_{\\max} = \\min\\left(\\eta_{\\text{target}}, c\\right).\n$$\n- 使用余弦退火 (CA) 并以弧度为单位的角度来构建一个长度为 $T$ 的调度，该调度从 $\\eta_{\\max}$ 开始，到 $\\eta_{\\min}$ 结束。CA 调度是周期性学习率 (Cyclical Learning Rate, CLR) 设计中的标准实践，它是通过将离散步长索引 $t \\in \\{0, 1, \\dots, T-1\\}$ 映射到 $[0, \\pi]$ 上的余弦曲线上的点来导出的。这种映射确保了从 $\\eta_{\\max}$ 到 $\\eta_{\\min}$ 的平滑过渡。对于边界情况 $T = 1$，调度必须是单个值 $\\eta_{\\max}$，这是余弦路径退化为一个点时的极限行为。\n\n我们对每个周期验证以下属性：\n- 范围：对于所有步长 $t$，$\\eta_{\\min} \\le \\eta_{t} \\le \\eta_{\\max}$。\n- 稳定性：对于该周期中的所有步长 $t$，$\\eta_{t} \\le \\frac{2}{\\hat{\\lambda}_{\\max}}$。\n- 端点：如果 $T  1$，第一个值等于 $\\eta_{\\max}$，最后一个值等于 $\\eta_{\\min}$。如果 $T = 1$，单个值等于 $\\eta_{\\max}$。\n\n最后，我们将整个测试套件的验证结果汇总为一个布尔值列表。每个布尔值表示相应测试用例的所有周期是否都满足约束。角度始终以弧度为单位。\n\n代码中实现的算法步骤：\n- 为每个测试用例构建给定的测试矩阵 $\\mathbf{H}$ 和参数 $(\\eta_{\\min}, \\eta_{\\text{target}}, C, T, K, s)$。\n- 对于每个周期 $c \\in \\{0, \\dots, C-1\\}$，使用从 $s + c$ 派生的可复现随机种子运行幂迭代，通过控制器计算 $\\eta_{\\max}$，并构建 CA 调度。\n- 检查约束条件，并为每个测试用例累积一个通过/失败的布尔值。\n- 以指定格式将五个布尔值的列表打印为单行。\n\n此设计符合 GD 在二次机制下的基本稳定性条件，利用了有原则的特征值估计，并使用以弧度为单位的 CA 实现了公认的 CLR 机制，同时正确处理了如 $T = 1$ 和近零曲率等边缘情况。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_iteration(H: np.ndarray, num_iters: int, seed: int) -> float:\n    \"\"\"\n    Estimate the largest eigenvalue of a symmetric matrix H using power iteration.\n    Uses the Rayleigh quotient on the normalized iterate.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    n = H.shape[0]\n    v = rng.normal(size=(n,))\n    # Handle potential zero vector initialization, enforce nonzero\n    if np.linalg.norm(v) == 0.0:\n        v = np.ones(n)\n    v = v / np.linalg.norm(v)\n\n    lambda_hat = 0.0\n    for _ in range(num_iters):\n        w = H @ v\n        norm_w = np.linalg.norm(w)\n        if norm_w == 0.0:\n            # H might be (near) zero; lambda_hat remains 0\n            lambda_hat = 0.0\n            break\n        v = w / norm_w\n        # Rayleigh quotient on the new direction\n        lambda_hat = float(v.T @ (H @ v))\n    return lambda_hat\n\ndef cosine_cycle_schedule(eta_min: float, eta_max: float, T: int) -> np.ndarray:\n    \"\"\"\n    Construct a cosine annealing schedule over T steps in radians,\n    starting at eta_max and ending at eta_min. For T == 1, return [eta_max].\n    \"\"\"\n    if T = 0:\n        raise ValueError(\"Cycle length T must be a positive integer.\")\n    if T == 1:\n        return np.array([eta_max], dtype=float)\n    t = np.arange(T, dtype=float)\n    # Cosine in radians: angle = pi * t / (T - 1)\n    lrs = eta_min + 0.5 * (eta_max - eta_min) * (1.0 + np.cos(np.pi * t / (T - 1)))\n    return lrs\n\ndef verify_cycle(lrs: np.ndarray, eta_min: float, eta_max: float, lambda_hat: float, T: int) -> bool:\n    \"\"\"\n    Verify that the schedule respects [eta_min, eta_max], is capped by 2/lambda_hat,\n    and has correct endpoints (start at eta_max, end at eta_min for T>1; constant for T==1).\n    \"\"\"\n    tol = 1e-12\n    # Range checks\n    cond_range = (lrs.min() >= eta_min - tol) and (lrs.max() = eta_max + tol)\n    # Stability cap\n    cap = np.inf if lambda_hat = 0.0 else (2.0 / lambda_hat) + tol\n    cond_cap = np.all(lrs = cap)\n    # Endpoint checks\n    if T > 1:\n        cond_start = abs(lrs[0] - eta_max) = tol\n        cond_end = abs(lrs[-1] - eta_min) = tol\n        cond_endpoints = cond_start and cond_end\n    else: # T == 1\n        cond_endpoints = abs(lrs[0] - eta_max) = tol\n    return cond_range and cond_cap and cond_endpoints\n\ndef controller_and_verify(H: np.ndarray, eta_min: float, eta_target: float,\n                          cycles: int, T: int, K: int, base_seed: int) -> bool:\n    \"\"\"\n    For each cycle, estimate curvature via power iteration, set eta_max = min(eta_target, 2/lambda_hat),\n    build the cosine annealing schedule, and verify constraints. Return True iff all cycles pass.\n    \"\"\"\n    all_ok = True\n    for c in range(cycles):\n        # Per-cycle seed to vary the initial direction deterministically\n        seed = base_seed + c\n        lambda_hat = power_iteration(H, K, seed)\n        eta_cap = np.inf if lambda_hat = 0.0 else 2.0 / lambda_hat\n        eta_max = min(eta_target, eta_cap)\n        # It's possible eta_max  eta_min if target is too low or cap is very restrictive.\n        # Problem implies eta_min = eta_max, so we enforce it.\n        # This case is not hit by test suite, but is good practice.\n        eta_max_actual = max(eta_min, eta_max)\n        \n        lrs = cosine_cycle_schedule(eta_min, eta_max_actual, T)\n        ok = verify_cycle(lrs, eta_min, eta_max_actual, lambda_hat, T)\n        all_ok = all_ok and ok\n    return all_ok\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"H\": np.array([[1.0, 0.0],\n                           [0.0, 0.5]], dtype=float),\n            \"eta_min\": 0.0,\n            \"eta_target\": 0.9,\n            \"cycles\": 2,\n            \"T\": 10,\n            \"K\": 50,\n            \"seed\": 101,\n        },\n        # Test Case 2\n        {\n            \"H\": np.array([[5.0, 0.0],\n                           [0.0, 3.0]], dtype=float),\n            \"eta_min\": 0.05,\n            \"eta_target\": 0.9,\n            \"cycles\": 3,\n            \"T\": 8,\n            \"K\": 30,\n            \"seed\": 202,\n        },\n        # Test Case 3\n        {\n            \"H\": 0.001 * np.eye(3, dtype=float),\n            \"eta_min\": 0.0,\n            \"eta_target\": 1.5,\n            \"cycles\": 2,\n            \"T\": 4,\n            \"K\": 20,\n            \"seed\": 303,\n        },\n        # Test Case 4\n        {\n            \"H\": np.array([[10.0, 0.0],\n                           [0.0, 0.01]], dtype=float),\n            \"eta_min\": 0.0,\n            \"eta_target\": 0.25,\n            \"cycles\": 3,\n            \"T\": 7,\n            \"K\": 25,\n            \"seed\": 404,\n        },\n        # Test Case 5\n        {\n            \"H\": np.array([[2.0, 0.0],\n                           [0.0, 1.0]], dtype=float),\n            \"eta_min\": 0.1,\n            \"eta_target\": 1.0,\n            \"cycles\": 1,\n            \"T\": 1,\n            \"K\": 40,\n            \"seed\": 505,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        ok = controller_and_verify(\n            H=case[\"H\"],\n            eta_min=case[\"eta_min\"],\n            eta_target=case[\"eta_target\"],\n            cycles=case[\"cycles\"],\n            T=case[\"T\"],\n            K=case[\"K\"],\n            base_seed=case[\"seed\"]\n        )\n        results.append(ok)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}