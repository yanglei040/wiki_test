{
    "hands_on_practices": [
        {
            "introduction": "The most direct way to appreciate Adagrad's power is to see it in action on a problem that is notoriously difficult for standard gradient descent. This hands-on coding exercise  challenges you to compare Adagrad with SGD on an ill-conditioned convex problem, allowing you to empirically verify how its per-parameter learning rates lead to more uniform progress across dimensions with vastly different curvatures.",
            "id": "3095498",
            "problem": "You are asked to implement and compare Stochastic Gradient Descent (SGD) and the Adaptive Gradient Method (Adagrad) on a strictly convex, separable quadratic objective and to quantify their behavior along different coordinate axes when the curvature differs by several orders of magnitude. Your implementation must be a complete, runnable program that uses only the standard library and the specified scientific libraries and prints the required outputs in the exact format.\n\nConsider the function defined by the separable convex quadratic\n$$\nf(\\boldsymbol{\\theta}) \\;=\\; \\tfrac{1}{2}\\sum_{i=1}^{d} a_i\\, \\theta_i^2,\n$$\nwhere $d$ is the dimension, $\\boldsymbol{\\theta}\\in\\mathbb{R}^d$, and $a_i \\gt 0$ are curvature coefficients. This function is differentiable with gradient given by the vector of partial derivatives and its Hessian is diagonal with diagonal entries $a_i$, ensuring strict convexity.\n\nFundamental base facts you must use:\n- For any differentiable function $f$, the gradient is the vector of partial derivatives, and moving opposite to the gradient reduces the function locally when the step size is sufficiently small (first-order optimality principle).\n- For the separable quadratic above, the partial derivative with respect to $\\theta_i$ equals the product of the curvature coefficient and the coordinate, which is a consequence of the power rule and linearity of differentiation.\n- Stochastic Gradient Descent (SGD) is the method that iteratively updates parameters by subtracting a learning-rate-scaled gradient.\n- Adagrad (Adaptive Gradient Method) is a per-coordinate adaptive step-size method that scales each coordinate’s step by the inverse of the square root of the cumulative sum of past squared gradients for that coordinate plus a small positive constant to ensure numerical stability.\n\nYour tasks:\n1) Implement two optimization procedures starting from the same initial point $\\boldsymbol{\\theta}_0$:\n   - Stochastic Gradient Descent (SGD): apply the canonical iterative update that subtracts a constant-step-size multiple of the gradient at each step.\n   - Adaptive Gradient Method (Adagrad): apply the canonical per-coordinate adaptive step-size scheme that accumulates squared gradients and scales the step size coordinate-wise by the inverse square root of that accumulation plus a fixed positive constant.\n\n2) For each optimizer and each test case, simulate exactly $T$ steps and compute:\n   - The final parameter vector $\\boldsymbol{\\theta}_T$ for each optimizer separately.\n   - The objective values $f(\\boldsymbol{\\theta}_T)$ for each optimizer.\n   - The per-axis progress fractions\n     $$\n     \\rho_i \\;=\\; \\frac{\\lvert \\theta_{0,i}\\rvert - \\lvert \\theta_{T,i}\\rvert}{\\lvert \\theta_{0,i}\\rvert},\n     $$\n     clipped into the interval $\\left[0,1\\right]$ to handle potential numerical anomalies, for each optimizer.\n   - The axis-wise progress anisotropy index for each optimizer, defined as\n     $$\n     \\mathrm{anisotropy} \\;=\\; \\frac{\\max_i \\rho_i}{\\min_i \\rho_i},\n     $$\n     with the convention that if $\\min_i \\rho_i = 0$, the anisotropy is treated as $+\\infty$.\n   - The empirical condition-number sensitivity for each optimizer, defined as the slope of the least-squares linear fit of the pairs $\\left(\\log a_i,\\, \\log \\rho_i\\right)$ over $i\\in\\{1,\\dots,d\\}$, where the logarithm is the natural logarithm and $\\rho_i$ are strictly positive by clipping to a small positive lower bound if needed. Concretely, with $x_i = \\log a_i$ and $y_i = \\log\\left(\\max(\\rho_i, \\delta)\\right)$ for a tiny $\\delta \\gt 0$, compute\n     $$\n     s \\;=\\; \\frac{\\sum_{i=1}^{d} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{d} (x_i - \\bar{x})^2},\n     $$\n     where $\\bar{x}$ and $\\bar{y}$ are the sample means of $\\{x_i\\}$ and $\\{y_i\\}$, respectively. Larger $\\lvert s\\rvert$ indicates stronger dependence of progress on curvature; values near $0$ indicate reduced sensitivity to the condition number.\n\n3) For each test case, also compute the condition number\n   $$\n   \\kappa \\;=\\; \\frac{\\max_i a_i}{\\min_i a_i}.\n   $$\n\nTest suite:\nImplement exactly the following three test cases, each with dimension $d$, curvature vector $\\boldsymbol{a}$, initial point $\\boldsymbol{\\theta}_0$, number of steps $T$, and learning-rate hyperparameters. All numerical values below must be used exactly as provided.\n\n- Test case $1$ (happy path, multi-order curvature, moderate steps):\n  - $d = 3$\n  - $\\boldsymbol{a} = \\left[10^{-2},\\, 1,\\, 10^{2}\\right]$\n  - $\\boldsymbol{\\theta}_0 = \\left[10,\\, -10,\\, 10\\right]$\n  - $T = 200$\n  - SGD learning rate $\\eta_{\\mathrm{sgd}} = 0.015$\n  - Adagrad base learning rate $\\eta_{\\mathrm{ada}} = 1.0$\n  - Adagrad stability constant $\\epsilon = 10^{-8}$\n\n- Test case $2$ (very ill-conditioned, mixed small and large curvature):\n  - $d = 4$\n  - $\\boldsymbol{a} = \\left[10^{-4},\\, 10^{-2},\\, 1,\\, 10^{3}\\right]$\n  - $\\boldsymbol{\\theta}_0 = \\left[1,\\, 1,\\, 1,\\, 1\\right]$\n  - $T = 400$\n  - SGD learning rate $\\eta_{\\mathrm{sgd}} = 0.001$\n  - Adagrad base learning rate $\\eta_{\\mathrm{ada}} = 1.0$\n  - Adagrad stability constant $\\epsilon = 10^{-8}$\n\n- Test case $3$ (edge condition with extreme curvature and large steps):\n  - $d = 2$\n  - $\\boldsymbol{a} = \\left[1,\\, 10^{6}\\right]$\n  - $\\boldsymbol{\\theta}_0 = \\left[10^{3},\\, 10^{3}\\right]$\n  - $T = 5000$\n  - SGD learning rate $\\eta_{\\mathrm{sgd}} = 10^{-6}$\n  - Adagrad base learning rate $\\eta_{\\mathrm{ada}} = 1.0$\n  - Adagrad stability constant $\\epsilon = 10^{-8}$\n\nOutput specification:\n- For each test case, produce a list of $7$ real numbers in the following order:\n  $$\n  \\left[\\kappa,\\ \\mathrm{anisotropy}_{\\mathrm{sgd}},\\ \\mathrm{anisotropy}_{\\mathrm{ada}},\\ f\\!\\left(\\boldsymbol{\\theta}^{\\mathrm{sgd}}_{T}\\right),\\ f\\!\\left(\\boldsymbol{\\theta}^{\\mathrm{ada}}_{T}\\right),\\ s_{\\mathrm{sgd}},\\ s_{\\mathrm{ada}}\\right].\n  $$\n- Aggregate the three per-test-case result lists into a single outer list and print that outer list on a single line with no spaces, using square brackets and comma separators, for example:\n  $$\n  \\left[[x_{1,1},x_{1,2},\\dots,x_{1,7}],[x_{2,1},\\dots,x_{2,7}],[x_{3,1},\\dots,x_{3,7}]\\right].\n  $$\n- All outputs are pure numbers without units. Angles are not involved. If any intermediate value would make $\\log 0$ appear, your implementation must clip the argument to a tiny positive number, for example $\\delta = 10^{-300}$, prior to taking the logarithm to ensure a finite result.\n\nYour program must not read any input and must exactly implement the values and ordering above. The final print must contain only the single-line output in the specified format.",
            "solution": "The problem has been validated and is deemed sound, well-posed, and formalizable. The provided parameters and definitions are sufficient and consistent for a unique solution.\n\nThe core of this problem is to compare the performance of two fundamental gradient-based optimization algorithms, Stochastic Gradient Descent (SGD) and the Adaptive Gradient Method (Adagrad), on a specific type of objective function. The function is a separable convex quadratic, which serves as an ideal testbed because its coordinate-wise curvature can be precisely controlled, allowing for a clear analysis of how each algorithm handles ill-conditioning.\n\nThe objective function is defined as:\n$$\nf(\\boldsymbol{\\theta}) \\;=\\; \\tfrac{1}{2}\\sum_{i=1}^{d} a_i\\, \\theta_i^2\n$$\nwhere $\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_d) \\in\\mathbb{R}^d$ is the parameter vector, and $\\boldsymbol{a} = (a_1, \\dots, a_d)$ with $a_i > 0$ is the vector of curvature coefficients. The global minimum of this function is uniquely at $\\boldsymbol{\\theta} = \\mathbf{0}$.\n\nThe gradient vector $\\nabla f(\\boldsymbol{\\theta})$ has components given by the partial derivatives:\n$$\n\\frac{\\partial f}{\\partial \\theta_i} \\;=\\; a_i \\theta_i\n$$\nThe Hessian matrix is a diagonal matrix with entries $a_i$, confirming that the $a_i$ values are indeed the curvatures along each principal axis of the function's level sets, which are hyperellipsoids.\n\n**Stochastic Gradient Descent (SGD)**\n\nSGD is an iterative optimization algorithm that updates parameters by moving them in the opposite direction of the gradient. The update rule for the entire parameter vector $\\boldsymbol{\\theta}$ at step $t$ is:\n$$\n\\boldsymbol{\\theta}_{t+1} \\;=\\; \\boldsymbol{\\theta}_t - \\eta_{\\mathrm{sgd}} \\nabla f(\\boldsymbol{\\theta}_t)\n$$\nwhere $\\eta_{\\mathrm{sgd}}$ is the learning rate, a positive scalar constant. Because the function is separable, we can analyze the update for each component $\\theta_i$ independently:\n$$\n\\theta_{t+1, i} \\;=\\; \\theta_{t, i} - \\eta_{\\mathrm{sgd}} (a_i \\theta_{t, i}) \\;=\\; (1 - \\eta_{\\mathrm{sgd}} a_i) \\theta_{t, i}\n$$\nThis reveals that each coordinate's value decays geometrically towards $0$. The rate of decay is determined by the factor $(1 - \\eta_{\\mathrm{sgd}} a_i)$. For the algorithm to converge, we must have $|1 - \\eta_{\\mathrm{sgd}} a_i| < 1$, which implies $0 < \\eta_{\\mathrm{sgd}} a_i < 2$. To ensure convergence for all coordinates simultaneously, the learning rate must be chosen to satisfy this for the largest curvature, i.e., $\\eta_{\\mathrm{sgd}} < 2 / \\max_i a_i$. This choice, however, imposes a very slow convergence rate on coordinates with small curvature $a_i$, as their decay factor $(1 - \\eta_{\\mathrm{sgd}} a_i)$ will be very close to $1$. This is the fundamental weakness of SGD in ill-conditioned problems: a single learning rate cannot be optimal for all coordinates.\n\n**Adaptive Gradient Method (Adagrad)**\n\nAdagrad addresses the shortcoming of SGD by using a per-parameter adaptive learning rate. It scales the learning rate for each parameter inversely to the square root of the sum of the squares of all its past gradients. The update rule for component $\\theta_i$ at step $t$ is:\n$$\n\\theta_{t+1, i} \\;=\\; \\theta_{t, i} - \\frac{\\eta_{\\mathrm{ada}}}{\\sqrt{S_{t,i} + \\epsilon}} g_{t,i}\n$$\nwhere $\\eta_{\\mathrm{ada}}$ is a base learning rate, $g_{t,i} = a_i \\theta_{t,i}$ is the gradient component, and $\\epsilon$ is a small-valued stability constant to prevent division by zero. The term $S_{t,i}$ is the accumulator for squared gradients:\n$$\nS_{t,i} \\;=\\; \\sum_{k=1}^{t} g_{k,i}^2\n$$\nThe effective learning rate for parameter $\\theta_i$ at step $t$ is $\\frac{\\eta_{\\mathrm{ada}}}{\\sqrt{S_{t,i} + \\epsilon}}$. For coordinates with high curvature (large $a_i$), the gradients $g_{t,i}$ will be large in magnitude, causing $S_{t,i}$ to grow quickly. This rapidly decreases the effective learning rate, preventing divergence and overshooting. Conversely, for coordinates with low curvature (small $a_i$), the gradients are small, $S_{t,i}$ grows slowly, and the effective learning rate remains high, allowing for substantial progress. This mechanism automatically balances the learning rates, leading to more uniform progress across all dimensions, even in poorly conditioned problems.\n\n**Analysis Metrics**\n\nThe problem requires the calculation of several metrics to quantify the performance and behavior of each optimizer:\n1.  **Condition Number ($\\kappa$)**: $\\kappa = \\frac{\\max_i a_i}{\\min_i a_i}$. This measures the ill-conditioning of the problem. A large $\\kappa$ indicates a wide disparity in curvatures.\n2.  **Per-axis Progress Fraction ($\\rho_i$)**: $\\rho_i = \\frac{\\lvert \\theta_{0,i}\\rvert - \\lvert \\theta_{T,i}\\rvert}{\\lvert \\theta_{0,i}\\rvert}$, clipped to $[0,1]$. This measures the fractional progress made towards the minimum along each axis, with $1$ being full progress and $0$ being no progress.\n3.  **Progress Anisotropy**: $\\mathrm{anisotropy} = \\frac{\\max_i \\rho_i}{\\min_i \\rho_i}$. This ratio quantifies the non-uniformity of progress across all axes. A value of $1$ indicates perfectly isotropic (uniform) progress. We expect Adagrad to yield a lower anisotropy than SGD.\n4.  **Condition-Number Sensitivity ($s$)**: This is the slope of the linear regression of $\\log \\rho_i$ against $\\log a_i$. It measures how strongly the progress on an axis depends on its curvature. For SGD, we expect a strong negative correlation, meaning higher curvature leads to less progress (a negative slope $s$). For Adagrad, which is designed to counteract this effect, we expect the progress $\\rho_i$ to be largely independent of $a_i$, resulting in a sensitivity $s$ close to $0$.\n\nThe following implementation will simulate SGD and Adagrad for the specified test cases and calculate these metrics to demonstrate their contrasting behaviors.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares SGD and Adagrad optimizers on a separable quadratic\n    objective function for a suite of test cases, and prints the results in the\n    specified format.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"d\": 3,\n            \"a\": np.array([1e-2, 1.0, 1e2]),\n            \"theta_0\": np.array([10.0, -10.0, 10.0]),\n            \"T\": 200,\n            \"eta_sgd\": 0.015,\n            \"eta_ada\": 1.0,\n            \"epsilon\": 1e-8,\n        },\n        {\n            \"d\": 4,\n            \"a\": np.array([1e-4, 1e-2, 1.0, 1e3]),\n            \"theta_0\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"T\": 400,\n            \"eta_sgd\": 0.001,\n            \"eta_ada\": 1.0,\n            \"epsilon\": 1e-8,\n        },\n        {\n            \"d\": 2,\n            \"a\": np.array([1.0, 1e6]),\n            \"theta_0\": np.array([1e3, 1e3]),\n            \"T\": 5000,\n            \"eta_sgd\": 1e-6,\n            \"eta_ada\": 1.0,\n            \"epsilon\": 1e-8,\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        a = case[\"a\"]\n        theta_0 = case[\"theta_0\"]\n        T = case[\"T\"]\n        eta_sgd = case[\"eta_sgd\"]\n        eta_ada = case[\"eta_ada\"]\n        epsilon = case[\"epsilon\"]\n        \n        # --- SGD Simulation ---\n        theta_sgd = theta_0.copy()\n        for _ in range(T):\n            grad = a * theta_sgd\n            theta_sgd -= eta_sgd * grad\n        \n        # --- Adagrad Simulation ---\n        theta_ada = theta_0.copy()\n        sum_sq_grad = np.zeros_like(theta_ada)\n        for _ in range(T):\n            grad = a * theta_ada\n            sum_sq_grad += grad**2\n            theta_ada -= eta_ada * grad / (np.sqrt(sum_sq_grad) + epsilon)\n\n        # --- Metrics Calculation Function ---\n        def calculate_optimizer_metrics(theta_T, theta_0, a):\n            # Final objective value\n            f_val = 0.5 * np.sum(a * theta_T**2)\n            \n            # Per-axis progress fraction (rho)\n            abs_theta_0 = np.abs(theta_0)\n            # Avoid division by zero if an initial coordinate is 0\n            safe_abs_theta_0 = np.where(abs_theta_0 == 0, 1.0, abs_theta_0)\n            rho = (abs_theta_0 - np.abs(theta_T)) / safe_abs_theta_0\n            rho = np.clip(rho, 0, 1)\n\n            # Anisotropy\n            min_rho = np.min(rho)\n            if min_rho == 0.0:\n                anisotropy = np.inf\n            else:\n                anisotropy = np.max(rho) / min_rho\n            \n            # Empirical condition-number sensitivity (s)\n            # Use a tiny delta to avoid log(0) as per problem statement\n            delta = 1e-300\n            log_a = np.log(a)\n            log_rho = np.log(np.maximum(rho, delta))\n            \n            x = log_a\n            y = log_rho\n            x_mean = np.mean(x)\n            y_mean = np.mean(y)\n            \n            # The denominator is zero only if all a_i are identical.\n            numerator = np.sum((x - x_mean) * (y - y_mean))\n            denominator = np.sum((x - x_mean)**2)\n            s = numerator / denominator if denominator != 0 else 0.0\n            \n            return anisotropy, f_val, s\n\n        # --- Aggregate Results for the Case ---\n        kappa = np.max(a) / np.min(a)\n        \n        anisotropy_sgd, f_sgd, s_sgd = calculate_optimizer_metrics(theta_sgd, theta_0, a)\n        anisotropy_ada, f_ada, s_ada = calculate_optimizer_metrics(theta_ada, theta_0, a)\n\n        case_results = [\n            kappa,\n            anisotropy_sgd,\n            anisotropy_ada,\n            f_sgd,\n            f_ada,\n            s_sgd,\n            s_ada\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string exactly as specified\n    sublist_strings = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output = f\"[{','.join(sublist_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Having seen Adagrad's empirical success, we now turn to its theoretical justification. This practice  explores the deep connection between Adagrad's adaptive scaling and the concept of preconditioning, an ideal but often computationally expensive optimization strategy. By analytically comparing the first update step of Adagrad to that of gradient descent on perfectly \"whitened\" features, you will gain insight into why Adagrad's diagonal matrix approximation is so effective.",
            "id": "3095404",
            "problem": "Consider linear regression with ridge regularization defined by the empirical risk $$L(\\mathbf{w}) = \\frac{1}{2n}\\|X\\mathbf{w} - \\mathbf{y}\\|^{2} + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^{2},$$ where $X \\in \\mathbb{R}^{n \\times d}$ is the feature matrix, $\\mathbf{y} \\in \\mathbb{R}^{n}$ is the target vector, $\\mathbf{w} \\in \\mathbb{R}^{d}$ is the parameter vector, and $\\lambda \\geq 0$ is the ridge coefficient. The gradient $\\nabla L(\\mathbf{w})$ is well-defined by basic multivariable calculus. Let the initial parameter be $\\mathbf{w}_{0} = \\mathbf{0} \\in \\mathbb{R}^{2}$ and suppose the following empirical quantities are known at $\\mathbf{w}_{0}$:\n- The feature covariance matrix is $$S = \\frac{1}{n}X^{\\top}X = \\begin{pmatrix} 3 & 2 \\\\ 2 & 3 \\end{pmatrix},$$ which is positive definite.\n- The data–gradient term is $$\\frac{1}{n}X^{\\top}\\mathbf{y} = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix}.$$\n\nDefine the Adaptive Gradient (Adagrad) method with per-coordinate accumulators initialized at zero and no numerical stabilizer, that is, $s_{0} = \\mathbf{0}$ and $\\epsilon = 0$. A fixed learning rate $\\eta > 0$ is used throughout. Also consider performing standard gradient descent on perfectly whitened features, meaning a linear transformation of the features yields identity covariance; carry out updates in the whitened coordinates and then map them back to the original parameter space.\n\nUsing only fundamental definitions and first principles:\n1. Derive the first-step Adagrad update direction in the original parameter space starting from $\\mathbf{w}_{0}$.\n2. Derive the first-step update direction in the original parameter space that results from one step of gradient descent executed in the whitened feature space starting from the corresponding origin.\n3. Compute the cosine of the angle between these two update directions.\n\nProvide the final answer as the exact value of the cosine. Do not round. No units are required. Your work must begin from the given base definitions (loss, gradient, covariance, whitening) and proceed by explicit derivation; do not invoke any shortcut formulas.",
            "solution": "The problem asks for the cosine of the angle between the first-step update directions of two different optimization algorithms, Adagrad and gradient descent in a whitened feature space, for a ridge regression problem. The process begins with validating the problem statement.\n\n### Problem Validation\n**Step 1: Extract Givens**\n- Loss Function: $L(\\mathbf{w}) = \\frac{1}{2n}\\|X\\mathbf{w} - \\mathbf{y}\\|^{2} + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^{2}$\n- Initial Parameter: $\\mathbf{w}_{0} = \\mathbf{0} \\in \\mathbb{R}^{2}$\n- Feature Covariance Matrix: $S = \\frac{1}{n}X^{\\top}X = \\begin{pmatrix} 3 & 2 \\\\ 2 & 3 \\end{pmatrix}$\n- Data–Gradient Term: $\\frac{1}{n}X^{\\top}\\mathbf{y} = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix}$\n- Adagrad Parameters: $s_{0} = \\mathbf{0}$, $\\epsilon = 0$, learning rate $\\eta > 0$.\n- Whitened Space: Gradient descent is performed in a space where features are transformed to have an identity covariance matrix.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, using standard definitions from optimization and machine learning. It is well-posed, providing all necessary quantities for a unique solution. The language is objective and precise. The data is consistent. Therefore, the problem is deemed valid.\n\n### Derivation of the Solution\nFirst, we find the gradient of the loss function $L(\\mathbf{w})$.\n$L(\\mathbf{w}) = \\frac{1}{2n}(X\\mathbf{w} - \\mathbf{y})^{\\top}(X\\mathbf{w} - \\mathbf{y}) + \\frac{\\lambda}{2}\\mathbf{w}^{\\top}\\mathbf{w}$\n$L(\\mathbf{w}) = \\frac{1}{2n}(\\mathbf{w}^{\\top}X^{\\top}X\\mathbf{w} - 2\\mathbf{y}^{\\top}X\\mathbf{w} + \\mathbf{y}^{\\top}\\mathbf{y}) + \\frac{\\lambda}{2}\\mathbf{w}^{\\top}\\mathbf{w}$\nThe gradient with respect to $\\mathbf{w}$ is:\n$\\nabla L(\\mathbf{w}) = \\frac{1}{n}X^{\\top}X\\mathbf{w} - \\frac{1}{n}X^{\\top}\\mathbf{y} + \\lambda\\mathbf{w}$\nUsing the provided definitions, we can write this as:\n$\\nabla L(\\mathbf{w}) = S\\mathbf{w} - \\frac{1}{n}X^{\\top}\\mathbf{y} + \\lambda\\mathbf{w}$\n\nWe need to evaluate the gradient at the initial parameter $\\mathbf{w}_{0} = \\mathbf{0}$:\n$\\mathbf{g}_{0} = \\nabla L(\\mathbf{w}_{0}) = S(\\mathbf{0}) - \\frac{1}{n}X^{\\top}\\mathbf{y} + \\lambda(\\mathbf{0}) = - \\frac{1}{n}X^{\\top}\\mathbf{y}$\nSubstituting the given value:\n$\\mathbf{g}_{0} = - \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$\nNotice that the gradient at the origin, $\\mathbf{g}_{0}$, is independent of the regularization parameter $\\lambda$.\n\n**1. First-step Adagrad update direction**\nThe Adagrad update rule is given by:\n$\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_{t+1}} + \\epsilon} \\odot \\mathbf{g}_{t}$\nwhere $\\mathbf{s}_{t+1} = \\mathbf{s}_{t} + \\mathbf{g}_{t} \\odot \\mathbf{g}_{t}$ (element-wise product).\nFor the first step ($t=0$ to $t=1$), we have $\\mathbf{w}_{0} = \\mathbf{0}$, $\\mathbf{s}_{0} = \\mathbf{0}$, and $\\epsilon = 0$. The gradient is $\\mathbf{g}_{0}$.\n\nFirst, we update the accumulator $\\mathbf{s}$:\n$\\mathbf{s}_{1} = \\mathbf{s}_{0} + \\mathbf{g}_{0} \\odot \\mathbf{g}_{0} = \\mathbf{0} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} \\odot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2^2 \\\\ 1^2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}$\nNext, we compute the update to the parameters. The update vector is $\\Delta\\mathbf{w}_{\\text{Adagrad}} = \\mathbf{w}_{1} - \\mathbf{w}_{0}$.\n$\\mathbf{w}_{1} = \\mathbf{w}_{0} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_{1}} + \\epsilon} \\odot \\mathbf{g}_{0} = \\mathbf{0} - \\frac{\\eta}{\\sqrt{\\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}} + 0} \\odot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$\n$\\Delta\\mathbf{w}_{\\text{Adagrad}} = -\\eta \\begin{pmatrix} 1/\\sqrt{4} \\\\ 1/\\sqrt{1} \\end{pmatrix} \\odot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -\\eta \\begin{pmatrix} 1/2 \\\\ 1 \\end{pmatrix} \\odot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -\\eta \\begin{pmatrix} (1/2) \\cdot 2 \\\\ 1 \\cdot 1 \\end{pmatrix} = -\\eta \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\nThe update direction is any vector proportional to $\\Delta\\mathbf{w}_{\\text{Adagrad}}$. We can choose the direction vector $\\mathbf{d}_{\\text{Adagrad}} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$.\n\n**2. First-step whitened gradient descent update direction**\nWhitening the features means finding a transformation matrix $W$ such that the new features $\\tilde{X} = XW$ have an identity covariance matrix. The new covariance is $\\tilde{S} = \\frac{1}{n}\\tilde{X}^{\\top}\\tilde{X} = W^{\\top}(\\frac{1}{n}X^{\\top}X)W = W^{\\top}SW$. We require $\\tilde{S} = I$. A standard choice for the whitening matrix is $W = S^{-1/2}$, where $S^{-1/2}$ is the principal square root of the inverse of $S$. Since $S$ is symmetric and positive definite, $S^{-1/2}$ is also symmetric and positive definite. With this choice, $W^{\\top}SW = (S^{-1/2})^{\\top} S S^{-1/2} = S^{-1/2} S S^{-1/2} = I$.\n\nThe parameters must be transformed accordingly to preserve predictions: $X\\mathbf{w} = (XW)\\tilde{\\mathbf{w}} = \\tilde{X}\\tilde{\\mathbf{w}}$, which implies $\\mathbf{w} = W\\tilde{\\mathbf{w}}$. The initial parameter $\\mathbf{w}_0 = \\mathbf{0}$ maps to $\\tilde{\\mathbf{w}}_0 = W^{-1}\\mathbf{0} = \\mathbf{0}$.\n\nWe perform gradient descent on the loss function expressed in terms of $\\tilde{\\mathbf{w}}$:\n$\\tilde{L}(\\tilde{\\mathbf{w}}) = L(W\\tilde{\\mathbf{w}}) = \\frac{1}{2n}\\|\\tilde{X}\\tilde{\\mathbf{w}} - \\mathbf{y}\\|^{2} + \\frac{\\lambda}{2}\\|W\\tilde{\\mathbf{w}}\\|^{2}$\nThe gradient is $\\nabla_{\\tilde{\\mathbf{w}}}\\tilde{L}(\\tilde{\\mathbf{w}}) = \\frac{1}{n}\\tilde{X}^{\\top}(\\tilde{X}\\tilde{\\mathbf{w}}-\\mathbf{y}) + \\lambda W^{\\top}W\\tilde{\\mathbf{w}}$.\nSubstituting $\\frac{1}{n}\\tilde{X}^{\\top}\\tilde{X} = I$ and $W^{\\top}W = (S^{-1/2})^{\\top}S^{-1/2} = S^{-1}$:\n$\\nabla_{\\tilde{\\mathbf{w}}}\\tilde{L}(\\tilde{\\mathbf{w}}) = \\tilde{\\mathbf{w}} - \\frac{1}{n}\\tilde{X}^{\\top}\\mathbf{y} + \\lambda S^{-1}\\tilde{\\mathbf{w}} = (I + \\lambda S^{-1})\\tilde{\\mathbf{w}} - W^{\\top}(\\frac{1}{n}X^{\\top}\\mathbf{y})$.\nAt $\\tilde{\\mathbf{w}}_0 = \\mathbf{0}$, the gradient is:\n$\\tilde{\\mathbf{g}}_0 = -\\frac{1}{n}\\tilde{X}^{\\top}\\mathbf{y} = -W^{\\top}(\\frac{1}{n}X^{\\top}\\mathbf{y}) = -S^{-1/2}(-\\mathbf{g}_0) = S^{-1/2}\\mathbf{g}_0$.\nThe gradient descent update in the whitened space is $\\tilde{\\mathbf{w}}_1 = \\tilde{\\mathbf{w}}_0 - \\eta' \\tilde{\\mathbf{g}}_0 = -\\eta' \\tilde{\\mathbf{g}}_0$.\n\nWe map this update back to the original parameter space: $\\mathbf{w}_1 = W\\tilde{\\mathbf{w}}_1$.\nThe update vector is $\\Delta\\mathbf{w}_{\\text{white}} = \\mathbf{w}_1 - \\mathbf{w}_0 = W\\tilde{\\mathbf{w}}_1 - \\mathbf{0} = S^{-1/2}(-\\eta' \\tilde{\\mathbf{g}}_0) = -\\eta' S^{-1/2} (S^{-1/2}\\mathbf{g}_0) = -\\eta' S^{-1}\\mathbf{g}_0$.\nThe update direction is proportional to $-S^{-1}\\mathbf{g}_0$. Let's compute this vector.\nWe have $S = \\begin{pmatrix} 3 & 2 \\\\ 2 & 3 \\end{pmatrix}$ and $\\mathbf{g}_{0} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$.\nFirst, we find the inverse of $S$:\n$\\det(S) = 3 \\times 3 - 2 \\times 2 = 9 - 4 = 5$.\n$S^{-1} = \\frac{1}{\\det(S)}\\begin{pmatrix} 3 & -2 \\\\ -2 & 3 \\end{pmatrix} = \\frac{1}{5}\\begin{pmatrix} 3 & -2 \\\\ -2 & 3 \\end{pmatrix}$.\nNow we compute the direction vector:\n$-S^{-1}\\mathbf{g}_0 = -\\frac{1}{5}\\begin{pmatrix} 3 & -2 \\\\ -2 & 3 \\end{pmatrix}\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -\\frac{1}{5}\\begin{pmatrix} 3(2) - 2(1) \\\\ -2(2) + 3(1) \\end{pmatrix} = -\\frac{1}{5}\\begin{pmatrix} 6-2 \\\\ -4+3 \\end{pmatrix} = -\\frac{1}{5}\\begin{pmatrix} 4 \\\\ -1 \\end{pmatrix} = \\frac{1}{5}\\begin{pmatrix} -4 \\\\ 1 \\end{pmatrix}$.\nThe update direction is specified by any vector proportional to this. We can choose $\\mathbf{d}_{\\text{white}} = \\begin{pmatrix} -4 \\\\ 1 \\end{pmatrix}$.\n\n**3. Cosine of the angle between the two update directions**\nWe need to find the cosine of the angle $\\theta$ between $\\mathbf{d}_{\\text{Adagrad}} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$ and $\\mathbf{d}_{\\text{white}} = \\begin{pmatrix} -4 \\\\ 1 \\end{pmatrix}$.\nThe cosine is given by the formula $\\cos(\\theta) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}$.\nThe dot product is:\n$\\mathbf{d}_{\\text{Adagrad}} \\cdot \\mathbf{d}_{\\text{white}} = (-1)(-4) + (-1)(1) = 4 - 1 = 3$.\nThe magnitudes (norms) of the vectors are:\n$\\|\\mathbf{d}_{\\text{Adagrad}}\\| = \\sqrt{(-1)^2 + (-1)^2} = \\sqrt{1+1} = \\sqrt{2}$.\n$\\|\\mathbf{d}_{\\text{white}}\\| = \\sqrt{(-4)^2 + 1^2} = \\sqrt{16+1} = \\sqrt{17}$.\nThe cosine of the angle is:\n$\\cos(\\theta) = \\frac{3}{\\sqrt{2} \\sqrt{17}} = \\frac{3}{\\sqrt{34}}$.\nTo rationalize the denominator, this can be written as $\\frac{3\\sqrt{34}}{34}$, but the problem does not require this. We leave it in the simpler exact form.",
            "answer": "$$\\boxed{\\frac{3}{\\sqrt{34}}}$$"
        },
        {
            "introduction": "No optimization algorithm is a silver bullet, and understanding an algorithm's limitations is as important as knowing its strengths. This exercise  investigates a key weakness of Adagrad: its monotonically decreasing learning rate can cause it to stall prematurely on certain non-convex landscapes. By observing Adagrad's behavior on a function with steep cliffs and flat valleys, you will understand the motivation behind next-generation adaptive optimizers that address this issue.",
            "id": "3095461",
            "problem": "Construct a fully specified experiment to study the Adaptive Gradient algorithm (Adagrad) in a deterministic, two-dimensional minimization problem that exhibits both flat valleys and sharp cliffs. Work with the following smooth, nonconvex objective:\n$$\nf(x,y) \\;=\\; \\exp\\!\\big(\\alpha\\,(c - x)\\big) \\;+\\; \\frac{x^2}{1 + x^2} \\;+\\; \\frac{y^2}{1 + y^2},\n$$\nwhere $\\alpha \\in \\mathbb{R}_{>0}$ and $c \\in \\mathbb{R}$ are fixed constants. The term $\\exp\\!\\big(\\alpha\\,(c - x)\\big)$ creates a steep descent direction when $x \\ll c$ (a “cliff”), while the rational terms $\\frac{x^2}{1 + x^2}$ and $\\frac{y^2}{1 + y^2}$ saturate to $1$ and have very small gradients for large $|x|$ and $|y|$ (flat valleys). Use $\\alpha = 1.2$ and $c = 0$.\n\nYour task is to implement a minimization method based on gradients that, starting from Stochastic Gradient Descent (SGD), adaptively rescales per-coordinate step sizes using the cumulative history of squared gradients for each coordinate, regularized by a small positive constant to avoid division by zero. Begin from the principle that standard SGD updates a parameter vector by subtracting a learning-rate-scaled gradient, and derive the adaptive per-coordinate rescaling rule by incorporating the cumulative squared gradients. Do not assume or quote any specific ready-made formula; instead, justify and derive the rule you implement from these base principles.\n\nThen, for the function $f(x,y)$ above:\n- Derive the exact analytic gradients with respect to $x$ and $y$.\n- Implement the adaptive method you derived.\n- Run a fixed number of iterations $T$ for each test case, with a constant base learning rate $\\eta$ and a stabilizer $\\varepsilon$.\n- For each test case, record two quantities:\n  1. The final objective value $f(x_T, y_T)$, rounded to $6$ decimal places.\n  2. A boolean that answers whether the effective learning rate along the $x$-coordinate on the last update has decreased by at least a factor of $10$ compared to its value on the first update. Here, the “effective learning rate” for a coordinate is defined as the scalar that multiplies that coordinate’s gradient component in your update rule.\n\nUse the following test suite, with $T = 300$ iterations for every case:\n- Case A (happy path, sharp cliff then flat valley): $\\eta = 0.1$, $\\varepsilon = 10^{-8}$, initial point $(x_0,y_0) = (-4.0, 6.0)$.\n- Case B (mitigated shrinkage via larger stabilizer): $\\eta = 0.1$, $\\varepsilon = 10^{-1}$, initial point $(x_0,y_0) = (-4.0, 6.0)$.\n- Case C (start in a flat region): $\\eta = 0.1$, $\\varepsilon = 10^{-8}$, initial point $(x_0,y_0) = (5.0, 5.0)$.\n- Case D (boundary with smaller base learning rate): $\\eta = 0.01$, $\\varepsilon = 10^{-8}$, initial point $(x_0,y_0) = (-4.0, 6.0)$.\n- Case E (edge: mild cliff): $\\eta = 0.1$, $\\varepsilon = 10^{-8}$, initial point $(x_0,y_0) = (-0.5, 6.0)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets.\n- For each case, output two items in order: the final objective value (a float rounded to $6$ decimal places) and the boolean described above (either True or False).\n- The list should therefore contain $2 \\times 5 = 10$ items, in the order A, B, C, D, E. For example, a syntactic template is\n\"[fA,boolA,fB,boolB,fC,boolC,fD,boolD,fE,boolE]\".",
            "solution": "The problem is valid. It presents a clear, self-contained, and scientifically grounded task: to derive, implement, and test the Adaptive Gradient (Adagrad) algorithm on a specified nonconvex function. All necessary parameters and conditions are provided, and the task is well-posed.\n\n### Derivation of the Adaptive Update Rule (Adagrad)\n\nThe objective is to develop a gradient-based optimization method that adaptively adjusts the learning rate for each parameter individually. We begin from the foundational principle of Stochastic Gradient Descent (SGD) and progressively build the adaptive rule.\n\nLet the vector of parameters to be optimized be $\\theta \\in \\mathbb{R}^d$. At each iteration $t$, the standard SGD update rule is given by:\n$$\n\\theta_{t+1} = \\theta_t - \\eta \\, g_t\n$$\nwhere $\\eta \\in \\mathbb{R}_{>0}$ is a fixed learning rate and $g_t = \\nabla f(\\theta_t)$ is the gradient of the objective function $f$ with respect to the parameters at $\\theta_t$. This update rule applies the same learning rate $\\eta$ to all parameter components.\n\nThe problem requires adapting the learning rate on a per-coordinate basis. We can express the component-wise update for the $i$-th parameter, $\\theta_{i}$, as:\n$$\n\\theta_{i, t+1} = \\theta_{i, t} - \\eta_{t,i} \\, g_{t,i}\n$$\nwhere $g_{t,i}$ is the $i$-th component of the gradient $g_t$, and $\\eta_{t,i}$ is the effective learning rate for parameter $i$ at iteration $t$.\n\nThe core principle of Adagrad is to scale the learning rate inversely to the accumulated magnitude of past gradients for that parameter. If a parameter has consistently experienced large gradients, its learning rate should decrease, as it is likely in a steep region of the loss surface. Conversely, a parameter with small historical gradients is likely in a flat region and could benefit from a larger learning rate to accelerate progress.\n\nThe problem specifies using the \"cumulative history of squared gradients\" as the measure of historical gradient magnitude. Let us define a state variable $s_{t,i}$ that accumulates the sum of squared gradients for parameter $i$ from the beginning of optimization up to iteration $t$:\n$$\ns_{t,i} = \\sum_{\\tau=0}^{t} g_{\\tau,i}^2\n$$\nwhere $g_{\\tau,i}$ is the partial derivative of the objective function with respect to $\\theta_i$ evaluated at $\\theta_\\tau$. This sum is updated iteratively: $s_{t,i} = s_{t-1,i} + g_{t,i}^2$, with $s_{-1,i} = 0$.\n\nTo achieve the inverse relationship between the effective learning rate and the accumulated squared gradients, we can define $\\eta_{t,i}$ as:\n$$\n\\eta_{t,i} = \\frac{\\eta}{\\sqrt{s_{t,i}}}\n$$\nwhere $\\eta$ is a global, base learning rate. The square root is used to moderate the scaling effect.\n\nFinally, the problem requires regularization by a small positive constant $\\varepsilon$ to prevent division by zero, which would occur if $s_{t,i}$ were zero (e.g., at the first step if the initial gradient is zero). This stabilizer is added inside the square root for numerical stability.\n$$\n\\eta_{t,i} = \\frac{\\eta}{\\sqrt{s_{t,i} + \\varepsilon}}\n$$\nThis is the \"effective learning rate\" for parameter $i$ at step $t$ as defined in the problem.\n\nCombining these elements, the full Adagrad update rule for the $i$-th parameter at iteration $t$ is:\n1.  Compute the gradient: $g_t = \\nabla f(\\theta_t)$.\n2.  Update the sum of squared gradients: $s_{t,i} = s_{t-1,i} + g_{t,i}^2$.\n3.  Update the parameter: $\\theta_{i, t+1} = \\theta_{i, t} - \\frac{\\eta}{\\sqrt{s_{t,i} + \\varepsilon}} g_{t,i}$.\n\nThis derivation constructs the Adagrad algorithm from the first principles outlined in the problem statement.\n\n### Analytic Gradients of the Objective Function\n\nThe objective function is given as $f(x,y) \\;=\\; \\exp\\!\\big(\\alpha\\,(c - x)\\big) \\;+\\; \\frac{x^2}{1 + x^2} \\;+\\; \\frac{y^2}{1 + y^2}$, with constants $\\alpha = 1.2$ and $c = 0$.\n$$\nf(x,y) = \\exp(-1.2x) + \\frac{x^2}{1 + x^2} + \\frac{y^2}{1 + y^2}\n$$\n\nWe compute the partial derivatives with respect to $x$ and $y$.\n\nFor the partial derivative with respect to $x$, a parameter in our case denoted by $\\theta_1$:\n$$\n\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x} \\left( \\exp(-1.2x) \\right) + \\frac{\\partial}{\\partial x} \\left( \\frac{x^2}{1 + x^2} \\right) + \\frac{\\partial}{\\partial x} \\left( \\frac{y^2}{1 + y^2} \\right)\n$$\nThe derivative of the exponential term is:\n$$\n\\frac{\\partial}{\\partial x} \\left( \\exp(-1.2x) \\right) = -1.2 \\exp(-1.2x)\n$$\nUsing the quotient rule for the rational term in $x$, $(\\frac{u}{v})' = \\frac{u'v - uv'}{v^2}$, with $u = x^2$ and $v = 1 + x^2$:\n$$\n\\frac{\\partial}{\\partial x} \\left( \\frac{x^2}{1 + x^2} \\right) = \\frac{(2x)(1 + x^2) - (x^2)(2x)}{(1 + x^2)^2} = \\frac{2x + 2x^3 - 2x^3}{(1 + x^2)^2} = \\frac{2x}{(1 + x^2)^2}\n$$\nThe term involving $y$ is constant with respect to $x$. Thus, the full partial derivative is:\n$$\n\\frac{\\partial f}{\\partial x} = -1.2 \\exp(-1.2x) + \\frac{2x}{(1 + x^2)^2}\n$$\n\nFor the partial derivative with respect to $y$, a parameter denoted by $\\theta_2$:\n$$\n\\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y} \\left( \\exp(-1.2x) \\right) + \\frac{\\partial}{\\partial y} \\left( \\frac{x^2}{1 + x^2} \\right) + \\frac{\\partial}{\\partial y} \\left( \\frac{y^2}{1 + y^2} \\right)\n$$\nThe first two terms are constant with respect to $y$. The derivative of the third term is symmetric to the one for $x$:\n$$\n\\frac{\\partial f}{\\partial y} = \\frac{2y}{(1 + y^2)^2}\n$$\n\nThese analytic gradients will be used in the implementation of the algorithm.\n\n### Experimental Procedure\n\nThe derived Adagrad algorithm is implemented and executed for $T=300$ iterations starting from an initial point $(x_0, y_0)$. For each of the five test cases, we initialize the parameters $(x, y)$ and the sums of squared gradients ($s_x, s_y$) to $0$. In each iteration $t=0, \\dots, T-1$, we compute the gradients $g_x$ and $g_y$, update $s_x$ and $s_y$, calculate the effective learning rates, and update $x$ and $y$. We record the effective learning rate for $x$ on the first update ($t=0$) and the last update ($t=T-1$). After $T$ iterations, we compute the final objective value $f(x_T, y_T)$ and determine if the effective learning rate for $x$ on the last update has decreased by a factor of at least $10$ compared to its value on the first update.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Adagrad optimization problem specified.\n    \"\"\"\n\n    # --- Define constants and functions ---\n    ALPHA = 1.2\n    C = 0.0\n    T = 300\n\n    def f_objective(x, y):\n        \"\"\" The objective function f(x,y). \"\"\"\n        term1 = np.exp(ALPHA * (C - x))\n        term2 = x**2 / (1.0 + x**2)\n        term3 = y**2 / (1.0 + y**2)\n        return term1 + term2 + term3\n\n    def grad_f(x, y):\n        \"\"\" The analytic gradient of the objective function. \"\"\"\n        grad_x = -ALPHA * np.exp(ALPHA * (C - x)) + 2.0 * x / (1.0 + x**2)**2\n        grad_y = 2.0 * y / (1.0 + y**2)**2\n        return grad_x, grad_y\n\n    def run_adagrad_experiment(x0, y0, eta, epsilon, iterations):\n        \"\"\"\n        Runs the Adagrad optimization for a given test case.\n        \"\"\"\n        x, y = float(x0), float(y0)\n        s_x, s_y = 0.0, 0.0\n        \n        eta_x_eff_first = 0.0\n        eta_x_eff_last = 0.0\n        \n        for t in range(iterations):\n            # 1. Compute gradient at the current position\n            g_x, g_y = grad_f(x, y)\n            \n            # 2. Accumulate squared gradients\n            s_x += g_x**2\n            s_y += g_y**2\n            \n            # 3. Calculate current effective learning rates\n            current_eta_x_eff = eta / np.sqrt(s_x + epsilon)\n            current_eta_y_eff = eta / np.sqrt(s_y + epsilon)\n            \n            # 4. Record effective learning rate for x at first and last steps\n            if t == 0:\n                eta_x_eff_first = current_eta_x_eff\n            if t == iterations - 1:\n                eta_x_eff_last = current_eta_x_eff\n            \n            # 5. Update parameters\n            x -= current_eta_x_eff * g_x\n            y -= current_eta_y_eff * g_y\n            \n        # Calculate final objective value\n        final_f_val = f_objective(x, y)\n        \n        # Check the condition on the effective learning rate for x\n        # Defensive check for division by zero, though unlikely here\n        if eta_x_eff_first == 0:\n            bool_check = False\n        else:\n            bool_check = (eta_x_eff_last <= eta_x_eff_first / 10.0)\n\n        return np.round(final_f_val, 6), bool_check\n\n    # --- Define the test cases from the problem statement. ---\n    test_cases = [\n        # Case A (happy path)\n        {'eta': 0.1, 'epsilon': 1e-8, 'initial_point': (-4.0, 6.0)},\n        # Case B (mitigated shrinkage via larger stabilizer)\n        {'eta': 0.1, 'epsilon': 1e-1, 'initial_point': (-4.0, 6.0)},\n        # Case C (start in a flat region)\n        {'eta': 0.1, 'epsilon': 1e-8, 'initial_point': (5.0, 5.0)},\n        # Case D (boundary with smaller base learning rate)\n        {'eta': 0.01, 'epsilon': 1e-8, 'initial_point': (-4.0, 6.0)},\n        # Case E (edge: mild cliff)\n        {'eta': 0.1, 'epsilon': 1e-8, 'initial_point': (-0.5, 6.0)},\n    ]\n\n    results = []\n    for case in test_cases:\n        x0, y0 = case['initial_point']\n        eta = case['eta']\n        epsilon = case['epsilon']\n        \n        f_val, bool_val = run_adagrad_experiment(x0, y0, eta, epsilon, T)\n        \n        results.append(f_val)\n        results.append(bool_val)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}