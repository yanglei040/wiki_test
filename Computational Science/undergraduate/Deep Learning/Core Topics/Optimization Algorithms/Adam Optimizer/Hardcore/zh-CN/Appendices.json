{
    "hands_on_practices": [
        {
            "introduction": "要真正理解Adam优化器，我们必须从其基本机制入手。第一个练习将引导您完成一个简单一维函数的单步更新计算 。通过手动计算动量、梯度平方的移动平均值，并应用偏差校正，您将对Adam如何计算其自适应步长有一个具体的认识。",
            "id": "2152250",
            "problem": "在数值优化领域，Adam算法是一种广泛用于寻找函数最小值的算法。考虑一维成本函数 $f(x) = 5x^2$。我们希望从初始猜测值 $x_0 = 2$ 开始，找到使该函数最小化的 $x$ 值。\n\n使用Adam算法计算一次更新步骤后参数的值，记为 $x_1$。该算法配置了以下超参数：\n- 学习率, $\\alpha = 0.1$\n- 一阶矩估计的指数衰减率, $\\beta_1 = 0.9$\n- 二阶矩估计的指数衰减率, $\\beta_2 = 0.999$\n- 用于数值稳定性的小常数, $\\epsilon = 10^{-8}$\n\n初始的一阶矩和二阶矩估计值 $m_0$ 和 $v_0$ 均初始化为零。\n\n将您的最终答案四舍五入到五位有效数字。",
            "solution": "我们从 $x_{0}=2$ 开始，使用Adam算法最小化 $f(x)=5x^{2}$。梯度为 $\\nabla f(x)=10x$。在第一步（$t=1$）时，在 $x_{0}$ 处计算的梯度为\n$$\ng_{1}=\\nabla f(x_{0})=10x_{0}=10\\cdot 2=20.\n$$\nAdam的矩更新公式为\n$$\nm_{1}=\\beta_{1}m_{0}+(1-\\beta_{1})g_{1}, \\quad v_{1}=\\beta_{2}v_{0}+(1-\\beta_{2})g_{1}^{2}.\n$$\n给定 $m_{0}=0$ 和 $v_{0}=0$，$\\beta_{1}=0.9$ 和 $\\beta_{2}=0.999$，\n$$\nm_{1}=0.9\\cdot 0+(1-0.9)\\cdot 20=2, \\quad v_{1}=0.999\\cdot 0+(1-0.999)\\cdot 20^{2}=0.001\\cdot 400=0.4.\n$$\n偏差校正后的估计值为\n$$\n\\hat{m}_{1}=\\frac{m_{1}}{1-\\beta_{1}^{1}}=\\frac{2}{1-0.9}=\\frac{2}{0.1}=20, \\quad \\hat{v}_{1}=\\frac{v_{1}}{1-\\beta_{2}^{1}}=\\frac{0.4}{1-0.999}=\\frac{0.4}{0.001}=400.\n$$\nAdam的更新步骤为\n$$\nx_{1}=x_{0}-\\alpha\\frac{\\hat{m}_{1}}{\\sqrt{\\hat{v}_{1}}+\\epsilon}=2-0.1\\cdot \\frac{20}{\\sqrt{400}+10^{-8}}=2-0.1\\cdot \\frac{20}{20+10^{-8}}=2-\\frac{2}{20+10^{-8}}.\n$$\n计算该分数，\n$$\n\\frac{2}{20+10^{-8}} \\approx 0.1-5\\times 10^{-11},\n$$\n所以\n$$\nx_{1}\\approx 2-\\left(0.1-5\\times 10^{-11}\\right)=1.90000000005.\n$$\n四舍五入到五位有效数字，结果为 $1.9000$。",
            "answer": "$$\\boxed{1.9000}$$"
        },
        {
            "introduction": "在单步更新的基础上，下一个练习将探讨Adam算法在两个时间步长内的迭代性质 。通过一个假設的振荡梯度序列，您将亲眼看到动量项如何累积过去的梯度信息以平滑优化路径。这个练习强化了递归更新和不断变化的偏差校正项的作用。",
            "id": "2152273",
            "problem": "考虑一个机器学习模型使用 Adam 优化算法对单个标量参数 $\\theta$ 进行优化。Adam 算法在时间步 $t$ 的更新规则如下：\n\n1.  更新有偏一阶矩估计：$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$\n2.  更新有偏二阶原始矩估计：$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$\n3.  计算偏差校正后的一阶矩估计：$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$\n4.  计算偏差校正后的二阶原始矩估计：$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$\n5.  更新参数：$\\theta_t = \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$\n\n这里，$g_t$ 是在时间步 $t$ 损失函数关于 $\\theta$ 的梯度。一阶矩和二阶矩估计被初始化为零，即 $m_0 = 0$ 和 $v_0 = 0$。\n\n假设初始参数值为 $\\theta_0 = 0.5$。优化过程从 $t=1$ 开始。优化器的超参数设置如下：\n- 学习率：$\\alpha = 0.1$\n- 一阶矩衰减率：$\\beta_1 = 0.9$\n- 二阶矩衰减率：$\\beta_2 = 0.999$\n- 用于数值稳定性的 Epsilon：$\\epsilon = 10^{-8}$\n\n在优化的前两个步骤中计算出的梯度为 $g_1 = 10$ 和 $g_2 = -10$。\n\n计算第二次更新步骤后参数 $\\theta$ 的值，即 $\\theta_2$。将您的最终答案四舍五入到四位有效数字。",
            "solution": "目标是计算两次更新步骤后参数 $\\theta$ 的值，即 $\\theta_2$。我们给定了初始条件和超参数：\n- 初始参数：$\\theta_0 = 0.5$\n- 初始一阶矩：$m_0 = 0$\n- 初始二阶矩：$v_0 = 0$\n- 学习率：$\\alpha = 0.1$\n- 衰减率：$\\beta_1 = 0.9$, $\\beta_2 = 0.999$\n- Epsilon：$\\epsilon = 10^{-8}$\n- 梯度：$g_1 = 10$, $g_2 = -10$\n\n我们将对每个时间步 $t=1$ 和 $t=2$ 进行计算。\n\n**第 1 步：时间步 $t=1$**\n\n首先，我们计算有偏的一阶和二阶矩估计，$m_1$ 和 $v_1$。\n$m_1 = \\beta_1 m_0 + (1-\\beta_1) g_1 = (0.9)(0) + (1-0.9)(10) = (0.1)(10) = 1$\n$v_1 = \\beta_2 v_0 + (1-\\beta_2) g_1^2 = (0.999)(0) + (1-0.999)(10^2) = (0.001)(100) = 0.1$\n\n接下来，我们计算偏差校正后的估计值，$\\hat{m}_1$ 和 $\\hat{v}_1$。\n$\\hat{m}_1 = \\frac{m_1}{1 - \\beta_1^1} = \\frac{1}{1 - 0.9} = \\frac{1}{0.1} = 10$\n$\\hat{v}_1 = \\frac{v_1}{1 - \\beta_2^1} = \\frac{0.1}{1 - 0.999} = \\frac{0.1}{0.001} = 100$\n\n最后，我们更新参数 $\\theta$ 得到 $\\theta_1$。\n$\\theta_1 = \\theta_0 - \\alpha \\frac{\\hat{m}_1}{\\sqrt{\\hat{v}_1} + \\epsilon} = 0.5 - (0.1) \\frac{10}{\\sqrt{100} + 10^{-8}} = 0.5 - \\frac{1}{10 + 10^{-8}}$\n$\\theta_1 \\approx 0.5 - 0.099999999 = 0.400000001$\n\n我们为下一步保留这个高精度的值。\n\n**第 2 步：时间步 $t=2$**\n\n现在我们使用 $t=1$ 的结果（$m_1=1, v_1=0.1, \\theta_1 \\approx 0.400000001$）来执行第二次更新。\n首先，计算有偏估计 $m_2$ 和 $v_2$。\n$m_2 = \\beta_1 m_1 + (1-\\beta_1) g_2 = (0.9)(1) + (1-0.9)(-10) = 0.9 - 1 = -0.1$\n$v_2 = \\beta_2 v_1 + (1-\\beta_2) g_2^2 = (0.999)(0.1) + (1-0.999)((-10)^2) = 0.0999 + (0.001)(100) = 0.0999 + 0.1 = 0.1999$\n\n接下来，计算偏差校正后的估计值，$\\hat{m}_2$ 和 $\\hat{v}_2$。偏差校正的指数是 $t=2$。\n$\\hat{m}_2 = \\frac{m_2}{1 - \\beta_1^2} = \\frac{-0.1}{1 - 0.9^2} = \\frac{-0.1}{1 - 0.81} = \\frac{-0.1}{0.19}$\n$\\hat{v}_2 = \\frac{v_2}{1 - \\beta_2^2} = \\frac{0.1999}{1 - 0.999^2} = \\frac{0.1999}{1 - 0.998001} = \\frac{0.1999}{0.001999}$\n\n最后，我们从 $\\theta_1$ 更新参数得到 $\\theta_2$。\n$\\theta_2 = \\theta_1 - \\alpha \\frac{\\hat{m}_2}{\\sqrt{\\hat{v}_2} + \\epsilon}$\n$\\theta_2 = 0.400000001 - (0.1) \\frac{-0.1 / 0.19}{\\sqrt{0.1999 / 0.001999} + 10^{-8}}$\n$\\theta_2 = 0.400000001 + \\frac{0.01 / 0.19}{\\sqrt{100.00000...} + 10^{-8}}$\n平方根下的项是 $\\frac{0.1999}{0.001999} \\approx 100.0$。\n$\\theta_2 \\approx 0.400000001 + \\frac{0.01 / 0.19}{10 + 10^{-8}}$\n$\\theta_2 \\approx 0.400000001 + \\frac{0.0526315789...}{10.00000001}$\n$\\theta_2 \\approx 0.400000001 + 0.0052631578...$\n$\\theta_2 \\approx 0.405263158$\n\n题目要求答案四舍五入到四位有效数字。\n$\\theta_2 \\approx 0.4053$",
            "answer": "$$\\boxed{0.4053}$$"
        },
        {
            "introduction": "既然我们已经练习了Adam的计算机制，现在让我们在一个具体而富有启发性的场景中探究其*自适应*的本质 。这个练习研究了当优化器遇到一个梯度持续很小的平坦区域时会发生什么。通过分析这种情况下的有效学习率，您将揭示Adam在复杂损失景观中表现出色的关键行为之一。",
            "id": "2152254",
            "problem": "一位工程师正在使用 Adam (自适应矩估计) 优化器来训练一个神经网络。训练过程进入了一个阶段，优化器正在损失景观中穿越一个长而平坦的高原。在这个高原上，观测到损失函数相对于特定标量参数 $\\theta$ 的梯度在连续几个时间步内是一个小的恒定值。\n\n假设优化器在时间步 $t=0$ 开始，其矩估计被初始化为零：一阶矩估计 $m_0 = 0$ 和二阶矩估计 $v_0 = 0$。对于所有后续时间步 $t \\geq 1$，梯度是恒定的，$g_t = g = 1.0 \\times 10^{-5}$。\n\nAdam 的更新规则如下：\n1.  更新有偏一阶矩估计：$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$\n2.  更新有偏二阶矩估计：$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$\n3.  计算偏差校正后的一阶矩估计：$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$\n4.  计算偏差校正后的二阶矩估计：$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$\n\n参数更新本身使用一个有效的学习率，该学习率根据梯度的历史进行自适应调整。这个有效的学习率调节基础学习率 $\\alpha$，可以定义为 $\\alpha_{\\text{eff}, t} = \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon}$。\n\n给定以下超参数：\n- 基础学习率：$\\alpha = 0.001$\n- 一阶矩的指数衰减率：$\\beta_1 = 0.9$\n- 二阶矩的指数衰减率：$\\beta_2 = 0.999$\n- 用于数值稳定性的 Epsilon：$\\epsilon = 1.0 \\times 10^{-8}$\n\n计算在时间步 $t=3$ 时的有效学习率 $\\alpha_{\\text{eff}, t}$。将你的最终答案四舍五入到四位有效数字。",
            "solution": "我们已知 Adam 的递归公式，其中 $m_{0}=0$，$v_{0}=0$，并且对于所有 $t \\geq 1$，梯度是常数 $g_{t}=g$。更新公式为\n$$\nm_{t}=\\beta_{1} m_{t-1}+(1-\\beta_{1}) g,\\qquad\nv_{t}=\\beta_{2} v_{t-1}+(1-\\beta_{2}) g^{2}.\n$$\n求解 $m_{0}=0$ 和 $v_{0}=0$ 的线性递归关系，得到几何级数求和：\n$$\nm_{t}=\\beta_{1}^{t} m_{0}+(1-\\beta_{1}) g \\sum_{k=0}^{t-1} \\beta_{1}^{k}=(1-\\beta_{1}^{t}) g,\n$$\n$$\nv_{t}=\\beta_{2}^{t} v_{0}+(1-\\beta_{2}) g^{2} \\sum_{k=0}^{t-1} \\beta_{2}^{k}=(1-\\beta_{2}^{t}) g^{2}.\n$$\n那么，偏差校正后的估计值为\n$$\n\\hat{m}_{t}=\\frac{m_{t}}{1-\\beta_{1}^{t}}=g,\\qquad\n\\hat{v}_{t}=\\frac{v_{t}}{1-\\beta_{2}^{t}}=g^{2}.\n$$\n因此，对于任何 $t \\geq 1$，特别是在 $t=3$ 时，有效学习率为\n$$\n\\alpha_{\\text{eff},t}=\\frac{\\alpha}{\\sqrt{\\hat{v}_{t}}+\\epsilon}=\\frac{\\alpha}{\\sqrt{g^{2}}+\\epsilon}=\\frac{\\alpha}{|g|+\\epsilon}.\n$$\n代入给定值 $g=1.0 \\times 10^{-5}$，$\\alpha=0.001$ 和 $\\epsilon=1.0 \\times 10^{-8}$，我们有 $|g|+\\epsilon=1.0 \\times 10^{-5}+1.0 \\times 10^{-8}=1.001 \\times 10^{-5}$。因此\n$$\n\\alpha_{\\text{eff},3}=\\frac{0.001}{1.001 \\times 10^{-5}}=\\frac{1.0 \\times 10^{-3}}{1.001 \\times 10^{-5}}=\\frac{1.0}{1.001} \\times 10^{2}\\approx 99.9000999\\ldots\n$$\n四舍五入到四位有效数字，得到 $99.90$。",
            "answer": "$$\\boxed{99.90}$$"
        }
    ]
}