{
    "hands_on_practices": [
        {
            "introduction": "The best way to truly understand an algorithm is to walk through its calculations by hand. This first exercise provides a foundational opportunity to do just that by computing a single update step of the Adam optimizer. You will apply the core mechanics, from calculating the gradient and updating the moment estimates to performing the crucial bias correction, giving you a concrete grasp of the entire process from start to finish. ",
            "id": "2152250",
            "problem": "In the field of numerical optimization, the Adam algorithm is a widely used method for finding the minimum of a function. Consider the one-dimensional cost function $f(x) = 5x^2$. We wish to find the value of $x$ that minimizes this function, starting from an initial guess of $x_0 = 2$.\n\nCalculate the value of the parameter after one update step, denoted as $x_1$, using the Adam algorithm. The algorithm is configured with the following hyperparameters:\n- Learning rate, $\\alpha = 0.1$\n- Exponential decay rate for the first moment estimate, $\\beta_1 = 0.9$\n- Exponential decay rate for the second moment estimate, $\\beta_2 = 0.999$\n- A small constant for numerical stability, $\\epsilon = 10^{-8}$\n\nThe initial first and second moment estimates, $m_0$ and $v_0$, are both initialized to zero.\n\nRound your final answer to five significant figures.",
            "solution": "We minimize $f(x)=5x^{2}$ using Adam starting from $x_{0}=2$. The gradient is $\\nabla f(x)=10x$. At the first step ($t=1$), the gradient evaluated at $x_{0}$ is\n$$\ng_{1}=\\nabla f(x_{0})=10x_{0}=10\\cdot 2=20.\n$$\nAdam moment updates are\n$$\nm_{1}=\\beta_{1}m_{0}+(1-\\beta_{1})g_{1}, \\quad v_{1}=\\beta_{2}v_{0}+(1-\\beta_{2})g_{1}^{2}.\n$$\nWith $m_{0}=0$ and $v_{0}=0$, $\\beta_{1}=0.9$, and $\\beta_{2}=0.999$,\n$$\nm_{1}=0.9\\cdot 0+(1-0.9)\\cdot 20=2, \\quad v_{1}=0.999\\cdot 0+(1-0.999)\\cdot 20^{2}=0.001\\cdot 400=0.4.\n$$\nBias-corrected estimates are\n$$\n\\hat{m}_{1}=\\frac{m_{1}}{1-\\beta_{1}^{1}}=\\frac{2}{1-0.9}=\\frac{2}{0.1}=20, \\quad \\hat{v}_{1}=\\frac{v_{1}}{1-\\beta_{2}^{1}}=\\frac{0.4}{1-0.999}=\\frac{0.4}{0.001}=400.\n$$\nThe Adam update is\n$$\nx_{1}=x_{0}-\\alpha\\frac{\\hat{m}_{1}}{\\sqrt{\\hat{v}_{1}}+\\epsilon}=2-0.1\\cdot \\frac{20}{\\sqrt{400}+10^{-8}}=2-0.1\\cdot \\frac{20}{20+10^{-8}}=2-\\frac{2}{20+10^{-8}}.\n$$\nEvaluating the fraction,\n$$\n\\frac{2}{20+10^{-8}} \\approx 0.1-5\\times 10^{-11},\n$$\nso\n$$\nx_{1}\\approx 2-\\left(0.1-5\\times 10^{-11}\\right)=1.90000000005.\n$$\nRounded to five significant figures, this is $1.9000$.",
            "answer": "$$\\boxed{1.9000}$$"
        },
        {
            "introduction": "Building upon the single-step calculation, this practice extends the process to two full timesteps, allowing you to observe the dynamics of Adam's internal state. By using a hypothetical sequence of oscillating gradients, you can directly see how the first moment estimate, or momentum, accumulates and smooths the parameter updates. This exercise is key to developing an intuition for how Adam navigates challenging optimization landscapes where gradients may change direction abruptly. ",
            "id": "2152273",
            "problem": "Consider the optimization of a single scalar parameter, $\\theta$, for a machine learning model using the Adam optimization algorithm. The update rules for Adam at timestep $t$ are given by:\n\n1.  Update biased first moment estimate: $m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$\n2.  Update biased second raw moment estimate: $v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$\n3.  Compute bias-corrected first moment estimate: $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$\n4.  Compute bias-corrected second raw moment estimate: $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$\n5.  Update parameter: $\\theta_t = \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$\n\nHere, $g_t$ is the gradient of the loss function with respect to $\\theta$ at timestep $t$. The first and second moment estimates are initialized to zero, i.e., $m_0 = 0$ and $v_0 = 0$.\n\nSuppose the initial parameter value is $\\theta_0 = 0.5$. The optimization process begins at $t=1$. The optimizer hyperparameters are set as follows:\n- Learning rate: $\\alpha = 0.1$\n- First moment decay rate: $\\beta_1 = 0.9$\n- Second moment decay rate: $\\beta_2 = 0.999$\n- Epsilon for numerical stability: $\\epsilon = 10^{-8}$\n\nThe gradients computed during the first two steps of the optimization are $g_1 = 10$ and $g_2 = -10$.\n\nCalculate the value of the parameter $\\theta$ after the second update step, i.e., $\\theta_2$. Round your final answer to four significant figures.",
            "solution": "The goal is to compute the value of the parameter $\\theta$ after two update steps, which is $\\theta_2$. We are given the initial conditions and hyperparameters:\n- Initial parameter: $\\theta_0 = 0.5$\n- Initial first moment: $m_0 = 0$\n- Initial second moment: $v_0 = 0$\n- Learning rate: $\\alpha = 0.1$\n- Decay rates: $\\beta_1 = 0.9$, $\\beta_2 = 0.999$\n- Epsilon: $\\epsilon = 10^{-8}$\n- Gradients: $g_1 = 10$, $g_2 = -10$\n\nWe will perform the calculations for each timestep, $t=1$ and $t=2$.\n\n**Step 1: Timestep $t=1$**\n\nFirst, we calculate the biased first and second moment estimates, $m_1$ and $v_1$.\n$m_1 = \\beta_1 m_0 + (1-\\beta_1) g_1 = (0.9)(0) + (1-0.9)(10) = (0.1)(10) = 1$\n$v_1 = \\beta_2 v_0 + (1-\\beta_2) g_1^2 = (0.999)(0) + (1-0.999)(10^2) = (0.001)(100) = 0.1$\n\nNext, we compute the bias-corrected estimates, $\\hat{m}_1$ and $\\hat{v}_1$.\n$\\hat{m}_1 = \\frac{m_1}{1 - \\beta_1^1} = \\frac{1}{1 - 0.9} = \\frac{1}{0.1} = 10$\n$\\hat{v}_1 = \\frac{v_1}{1 - \\beta_2^1} = \\frac{0.1}{1 - 0.999} = \\frac{0.1}{0.001} = 100$\n\nFinally, we update the parameter $\\theta$ to get $\\theta_1$.\n$\\theta_1 = \\theta_0 - \\alpha \\frac{\\hat{m}_1}{\\sqrt{\\hat{v}_1} + \\epsilon} = 0.5 - (0.1) \\frac{10}{\\sqrt{100} + 10^{-8}} = 0.5 - \\frac{1}{10 + 10^{-8}}$\n$\\theta_1 \\approx 0.5 - 0.099999999 = 0.400000001$\n\nWe keep this value with high precision for the next step.\n\n**Step 2: Timestep $t=2$**\n\nNow we use the results from $t=1$ ($m_1=1, v_1=0.1, \\theta_1 \\approx 0.400000001$) to perform the second update.\nFirst, calculate the biased estimates $m_2$ and $v_2$.\n$m_2 = \\beta_1 m_1 + (1-\\beta_1) g_2 = (0.9)(1) + (1-0.9)(-10) = 0.9 - 1 = -0.1$\n$v_2 = \\beta_2 v_1 + (1-\\beta_2) g_2^2 = (0.999)(0.1) + (1-0.999)((-10)^2) = 0.0999 + (0.001)(100) = 0.0999 + 0.1 = 0.1999$\n\nNext, compute the bias-corrected estimates, $\\hat{m}_2$ and $\\hat{v}_2$. The exponent for the bias correction is $t=2$.\n$\\hat{m}_2 = \\frac{m_2}{1 - \\beta_1^2} = \\frac{-0.1}{1 - 0.9^2} = \\frac{-0.1}{1 - 0.81} = \\frac{-0.1}{0.19}$\n$\\hat{v}_2 = \\frac{v_2}{1 - \\beta_2^2} = \\frac{0.1999}{1 - 0.999^2} = \\frac{0.1999}{1 - 0.998001} = \\frac{0.1999}{0.001999}$\n\nFinally, we update the parameter from $\\theta_1$ to get $\\theta_2$.\n$\\theta_2 = \\theta_1 - \\alpha \\frac{\\hat{m}_2}{\\sqrt{\\hat{v}_2} + \\epsilon}$\n$\\theta_2 = 0.400000001 - (0.1) \\frac{-0.1 / 0.19}{\\sqrt{0.1999 / 0.001999} + 10^{-8}}$\n$\\theta_2 = 0.400000001 + \\frac{0.01 / 0.19}{\\sqrt{100.00000...} + 10^{-8}}$\nThe term under the square root is $\\frac{0.1999}{0.001999} \\approx 100.0$.\n$\\theta_2 \\approx 0.400000001 + \\frac{0.01 / 0.19}{10 + 10^{-8}}$\n$\\theta_2 \\approx 0.400000001 + \\frac{0.0526315789...}{10.00000001}$\n$\\theta_2 \\approx 0.400000001 + 0.0052631578...$\n$\\theta_2 \\approx 0.405263158$\n\nThe problem asks for the answer rounded to four significant figures.\n$\\theta_2 \\approx 0.4053$",
            "answer": "$$\\boxed{0.4053}$$"
        },
        {
            "introduction": "One of Adam's most celebrated features is its per-parameter adaptive learning rate, which is controlled by the second moment estimate. This problem explores this mechanism in the important, hypothetical scenario of traversing a long, flat plateau in the loss landscape where gradients are consistently small. By calculating the effective learning rate, you will uncover how Adam intelligently increases its step size in such regions, a behavior that allows it to escape areas where standard gradient descent might stall. ",
            "id": "2152254",
            "problem": "An engineer is using the Adam (Adaptive Moment Estimation) optimizer to train a neural network. The training process has entered a phase where the optimizer is traversing a long, flat plateau in the loss landscape. On this plateau, the gradient of the loss function with respect to a particular scalar parameter, $\\theta$, is observed to be a small, constant value for several consecutive timesteps.\n\nAssume the optimizer starts at timestep $t=0$ with its moment estimates initialized to zero: the first moment estimate $m_0 = 0$ and the second moment estimate $v_0 = 0$. For all subsequent timesteps $t \\geq 1$, the gradient is constant, $g_t = g = 1.0 \\times 10^{-5}$.\n\nThe Adam update rules are given by:\n1.  Update the biased first moment estimate: $m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$\n2.  Update the biased second moment estimate: $v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$\n3.  Compute the bias-corrected first moment estimate: $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$\n4.  Compute the bias-corrected second moment estimate: $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$\n\nThe parameter update itself uses an effective learning rate that adapts based on the history of gradients. This effective learning rate, which modulates the base learning rate $\\alpha$, can be defined as $\\alpha_{\\text{eff}, t} = \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon}$.\n\nGiven the following hyperparameters:\n- Base learning rate: $\\alpha = 0.001$\n- Exponential decay rate for the first moment: $\\beta_1 = 0.9$\n- Exponential decay rate for the second moment: $\\beta_2 = 0.999$\n- Epsilon for numerical stability: $\\epsilon = 1.0 \\times 10^{-8}$\n\nCalculate the effective learning rate, $\\alpha_{\\text{eff}, t}$, at timestep $t=3$. Round your final answer to four significant figures.",
            "solution": "We are given the Adam recursions with $m_{0}=0$, $v_{0}=0$ and a constant gradient $g_{t}=g$ for all $t \\geq 1$. The updates are\n$$\nm_{t}=\\beta_{1} m_{t-1}+(1-\\beta_{1}) g,\\qquad\nv_{t}=\\beta_{2} v_{t-1}+(1-\\beta_{2}) g^{2}.\n$$\nSolving the linear recursions with $m_{0}=0$ and $v_{0}=0$ yields geometric sums:\n$$\nm_{t}=\\beta_{1}^{t} m_{0}+(1-\\beta_{1}) g \\sum_{k=0}^{t-1} \\beta_{1}^{k}=(1-\\beta_{1}^{t}) g,\n$$\n$$\nv_{t}=\\beta_{2}^{t} v_{0}+(1-\\beta_{2}) g^{2} \\sum_{k=0}^{t-1} \\beta_{2}^{k}=(1-\\beta_{2}^{t}) g^{2}.\n$$\nThe bias-corrected estimates are then\n$$\n\\hat{m}_{t}=\\frac{m_{t}}{1-\\beta_{1}^{t}}=g,\\qquad\n\\hat{v}_{t}=\\frac{v_{t}}{1-\\beta_{2}^{t}}=g^{2}.\n$$\nTherefore, for any $t \\geq 1$, and in particular at $t=3$, the effective learning rate is\n$$\n\\alpha_{\\text{eff},t}=\\frac{\\alpha}{\\sqrt{\\hat{v}_{t}}+\\epsilon}=\\frac{\\alpha}{\\sqrt{g^{2}}+\\epsilon}=\\frac{\\alpha}{|g|+\\epsilon}.\n$$\nWith the given values $g=1.0 \\times 10^{-5}$, $\\alpha=0.001$, and $\\epsilon=1.0 \\times 10^{-8}$, we have $|g|+\\epsilon=1.0 \\times 10^{-5}+1.0 \\times 10^{-8}=1.001 \\times 10^{-5}$. Hence\n$$\n\\alpha_{\\text{eff},3}=\\frac{0.001}{1.001 \\times 10^{-5}}=\\frac{1.0 \\times 10^{-3}}{1.001 \\times 10^{-5}}=\\frac{1.0}{1.001} \\times 10^{2}\\approx 99.9000999\\ldots\n$$\nRounding to four significant figures gives $99.90$.",
            "answer": "$$\\boxed{99.90}$$"
        }
    ]
}