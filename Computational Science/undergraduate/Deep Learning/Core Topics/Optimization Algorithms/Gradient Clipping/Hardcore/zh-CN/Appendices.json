{
    "hands_on_practices": [
        {
            "introduction": "梯度裁剪的核心思想是确保优化过程的稳定性，防止因梯度过大而导致训练发散。为了理解这一基本原则，我们可以借鉴数值优化领域中一个强大的思想——信赖域方法。这个练习  将引导你推导一个基于模型预测与实际改善之比的步长接受规则，并揭示一个看似合理的度量标准在何种情况下可能会产生误导，从而强调了建立更鲁棒机制的必要性。",
            "id": "3153333",
            "problem": "在基于模型的无导数优化 (DFO) 中，一种常见的方法是信赖域 (TR) 框架，该框架构建一个局部代理模型来近似一个未知的目标函数。设 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ 是一个光滑但没有封闭形式表达式的函数，并设 $m_{k}:\\mathbb{R}^{n}\\to\\mathbb{R}$ 是在迭代点 $x_{k}\\in\\mathbb{R}^{n}$ 处的一个局部代理模型。试探步 $s_{k}$ 是通过在信赖域 $\\{s:\\|s\\|\\leq \\Delta_{k}\\}$ 上（近似）最小化 $m_{k}$ 计算得到的，其中 $\\Delta_{k}0$ 是信赖域半径。将实际下降量和预测下降量分别定义为差值 $f(x_{k})-f(x_{k}+s_{k})$ 和 $m_{k}(0)-m_{k}(s_{k})$。仅从这些定义以及指导原则出发——即当实际改进与预测改进有意义地一致时，信赖域机制应接受该步，并应调整 $\\Delta_{k}$ 以维持模型的可靠性——完成以下任务：\n\n1) 基于比率\n$$\n\\rho_{k}\\;=\\;\\frac{f(x_{k})-f(x_{k}+s_{k})}{m_{k}(0)-m_{k}(s_{k})},\n$$\n推导一个步长接受准则，包括一对有原则的阈值 $0\\eta_{1}\\eta_{2}1$ 和用于收缩和扩大信赖域半径的乘法更新因子 $\\gamma_{\\mathrm{dec}}\\in(0,1)$ 和 $\\gamma_{\\mathrm{inc}}1$。你的推导必须从实际下降量和预测下降量的核心定义出发，并基于这样的意图：即模型的可信度仅限于其预测与观测到的下降量相符的程度。\n\n2) 构建一个具体的一维病态例子，其中比率 $\\rho_{k}$ 会因为模型偏差而产生误导。使用以下数据来实例化该例子并计算一个数值：\n- 目标函数 $f(x)=(x-1)^{2}$。\n- 当前迭代点 $x_{k}=0$ 和信赖域半径 $\\Delta_{k}=0.5$。\n- 二次模型 $m_{k}(s)=a+bs+\\tfrac{1}{2}cs^{2}$，其系数为 $b=-0.2$ 和 $c=0.1$（$a$ 可取任意值，因为它在预测下降量中会被消掉）。\n- 试探步 $s_{k}$ 是 $m_{k}(s)$ 在区间 $\\{-\\Delta_{k}\\le s\\le \\Delta_{k}\\}$ 上的精确最小化子。\n\n计算此例中 $\\rho_{k}$ 的值，并用文字解释为什么它在模型质量方面具有误导性。将你的最终答案表示为 $\\rho_{k}$ 的值，四舍五入到四位有效数字。不需要单位。",
            "solution": "该问题是有效的。它在科学上基于数值优化理论，特别是基于模型的无导数信赖域方法。该问题是适定的、客观的，并包含了推导概念框架和计算所需数值的所有必要信息。\n\n该问题包含两部分。第一部分要求从第一性原理推导标准的信赖域步长接受和半径更新机制。第二部分要求计算一个病态情况下的特定性能比率 $\\rho_k$，并解释为何该比率具有误导性。\n\n**第1部分：信赖域更新规则的推导**\n\n信赖域方法的核心在于管理局部代理模型 $m_k(s)$ 与真实目标函数 $f(x)$ 之间的一致性。衡量试探步 $s_k$ 这种一致性的量是比率 $\\rho_k$：\n$$\n\\rho_{k} = \\frac{\\text{ared}}{\\text{pred}} = \\frac{f(x_{k})-f(x_{k}+s_{k})}{m_{k}(0)-m_{k}(s_{k})}\n$$\n此处，`ared` 是目标函数的“实际下降量”，`pred` 是根据模型计算的“预测下降量”。我们正在最小化 $f$，因此假定 `pred` 为正，因为 $s_k$ 的选择是为了减小模型值 $m_k$。\n\n指导原则是，如果实际下降量是预测下降量的一个有意义的部分，则接受步长 $s_k$，并根据模型预测结果的准确性来调整信赖域半径 $\\Delta_k$。\n\n我们可以分析 $\\rho_k$ 的值来创建一套规则：\n\n情况1：模型与函数的一致性极好。\n如果 $\\rho_k$ 接近于 $1$，则 `ared` $\\approx$ `pred`，表明模型在信赖域内是函数行为的一个高度准确的预测器。如果 $\\rho_k > 1$，实际下降量甚至比预测的要好，这也是一个成功步长的标志。我们可以将这些“非常好”的结果归为一类。为了将其形式化，我们使用一个阈值 $\\eta_2 \\in (0, 1)$。如果 $\\rho_k > \\eta_2$，则认为模型的性能极好。因此，应该接受步长 $s_k$（$x_{k+1} = x_k + s_k$），并且我们对模型的信任度应该增加。这证明了为下一次迭代扩大信赖域是合理的，以便采取更具挑战性的步长：$\\Delta_{k+1} = \\gamma_{\\text{inc}} \\Delta_k$，其中 $\\gamma_{\\text{inc}} > 1$。\n\n情况2：模型与函数的一致性差。\n如果 $\\rho_k$ 是一个小的正数或负数，则模型是一个差的预测器。如果 $\\rho_k \\le 0$，则该步长完全未能减小目标函数（`ared` $\\le 0$），尽管模型预测了下降。为了处理这种情况，我们引入一个阈值 $\\eta_1$，其中 $0  \\eta_1  \\eta_2$。如果 $\\rho_k \\le \\eta_1$，则一致性很差。实际下降量要么是负的，要么是所承诺下降量的一个微不足道的部分。在这种情况下，必须拒绝步长 $s_k$（$x_{k+1} = x_k$），因为它没有成效。差的预测表明信赖域太大，模型不可靠。因此，必须收缩半径：$\\Delta_{k+1} = \\gamma_{\\text{dec}} \\Delta_k$，其中 $\\gamma_{\\text{dec}} \\in (0, 1)$。\n\n情况3：模型与函数的一致性可接受。\n当 $\\eta_1  \\rho_k \\le \\eta_2$ 时，出现这种中间情况。此时，实际下降量是预测下降量的足够大的部分，足以认为该步是成功的。因此，接受该步：$x_{k+1} = x_k + s_k$。然而，模型不够准确，不足以通过扩大信赖域来增加我们的信任度。反之，它也没有不准确到必须缩小信赖域的程度。最合乎逻辑的行动是通过保持半径不变来维持当前的信任水平：$\\Delta_{k+1} = \\Delta_k$。\n\n总而言之，推导出的规则是：\n1.  如果 $\\rho_k \\le \\eta_1$：拒绝该步。设置 $x_{k+1} = x_k$ 并收缩半径：$\\Delta_{k+1} = \\gamma_{\\text{dec}} \\Delta_k$。\n2.  如果 $\\eta_1  \\rho_k \\le \\eta_2$：接受该步。设置 $x_{k+1} = x_k + s_k$ 并保持半径不变：$\\Delta_{k+1} = \\Delta_k$。\n3.  如果 $\\rho_k > \\eta_2$：接受该步。设置 $x_{k+1} = x_k + s_k$ 并扩大半径：$\\Delta_{k+1} = \\gamma_{\\text{inc}} \\Delta_k$。\n\n这些规则从调整信赖域大小以反映所观察到的代理模型可靠性的原则中逻辑地推导出来。\n\n**第2部分：病态例子的计算与分析**\n\n我们有以下给定的数据：\n- 目标函数: $f(x) = (x-1)^2$\n- 当前迭代点: $x_k = 0$\n- 信赖域半径: $\\Delta_k = 0.5$\n- 二次模型: $m_k(s) = a + bs + \\frac{1}{2}cs^2$ 其中 $b = -0.2$ 且 $c = 0.1$。 $m_k(s) = a - 0.2s + 0.05s^2$。\n\n首先，我们通过在信赖域，即区间 $[-\\Delta_k, \\Delta_k] = [-0.5, 0.5]$ 上最小化 $m_k(s)$ 来找到试探步 $s_k$。\n二次函数 $m_k(s)$ 的无约束最小化子可通过将其导数设为零来找到：\n$$\nm_k'(s) = -0.2 + 0.1s = 0 \\implies s = \\frac{0.2}{0.1} = 2\n$$\n二阶导数是 $m_k''(s) = 0.1 > 0$，这证实了它是一个最小值点。\n无约束最小化子 $s=2$ 位于信赖域 $[-0.5, 0.5]$ 之外。由于 $m_k(s)$ 是一个凸抛物线，其在区间上的最小值必然出现在离无约束最小化子最近的边界点上。在 $[-0.5, 0.5]$ 中离 $2$ 最近的点是 $0.5$。\n因此，试探步是 $s_k = 0.5$。\n\n接下来，我们计算预测下降量 `pred`：\n$$\n\\text{pred} = m_k(0) - m_k(s_k) = m_k(0) - m_k(0.5)\n$$\n$m_k(0) = a - 0.2(0) + 0.05(0)^2 = a$。\n$m_k(0.5) = a - 0.2(0.5) + 0.05(0.5)^2 = a - 0.1 + 0.05(0.25) = a - 0.1 + 0.0125 = a - 0.0875$。\n$$\n\\text{pred} = a - (a - 0.0875) = 0.0875\n$$\n\n现在，我们计算实际下降量 `ared`：\n$$\n\\text{ared} = f(x_k) - f(x_k + s_k) = f(0) - f(0 + 0.5) = f(0) - f(0.5)\n$$\n$f(0) = (0-1)^2 = 1$。\n$f(0.5) = (0.5-1)^2 = (-0.5)^2 = 0.25$。\n$$\n\\text{ared} = 1 - 0.25 = 0.75\n$$\n\n最后，我们计算比率 $\\rho_k$：\n$$\n\\rho_k = \\frac{\\text{ared}}{\\text{pred}} = \\frac{0.75}{0.0875} = \\frac{7500}{875} = \\frac{60}{7} \\approx 8.571428...\n$$\n四舍五入到四位有效数字，我们得到 $\\rho_k \\approx 8.571$。\n\n**关于 $\\rho_k$ 为何具有误导性的解释：**\n计算出的值 $\\rho_k \\approx 8.571$ 是一个非常大的正数。根据第1部分推导的规则，这将被归类为一个“非常好”的步长（因为对于任何合理的 $\\eta_2  1$，都有 $\\rho_k \\gg \\eta_2$），这表明模型 $m_k$ 是真实函数 $f$ 的一个极好近似。这将导致算法扩大信赖域 $\\Delta_k$。\n\n然而，这个结论存在严重缺陷。$\\rho_k$ 的高值并不表示模型好，而是一个坏模型的产物。让我们比较模型与真实函数在当前点 $x_k=0$ 的性质。$f$ 在 $x_k=0$ 附近的行为由 $g(s) = f(x_k+s) = f(s) = (s-1)^2 = s^2 - 2s + 1$ 描述。\n- 真实函数在 $s=0$ 处的梯度是 $g'(0) = -2$。\n- 真实函数在 $s=0$ 处的曲率（二阶导数）是 $g''(0) = 2$。\n\n模型的性质由其系数给出：\n- 模型在 $s=0$ 处的梯度是 $m_k'(0) = b = -0.2$。\n- 模型的曲率是 $m_k''(s) = c = 0.1$。\n\n该模型表现出严重的偏差：它将真实梯度的量级低估了10倍（$-0.2$ 对比 $-2$），并将真实曲率低估了20倍（$0.1$ 对比 $2$）。这是一个非常差的局部模型。\n\n$\\rho_k$ 的值之所以大，是因为分母 `pred` $= 0.0875$ 极小。这个小的预测下降量是模型梯度和曲率错误地过小的直接后果。该模型“平坦”且“保守”，仅预测了微小的改进。分子 `ared` $= 0.75$ 很大，因为真实函数要陡峭得多。一个大数与一个非常小的数之比是一个大数。\n\n因此，比率 $\\rho_k \\approx 8.571$ 具有误导性。它发出了模型质量极佳的信号，而实际上，它是一个严重低估函数局部几何性质的极差模型的症状。一个盲目相信这个高 $\\rho_k$ 值的算法会扩大信赖域，将其对一个明显很差的模型的依赖扩展到更广的区域，这可能会降低优化过程的性能或使其停滞。这个例子凸显了在存在显著模型偏差的情况下，$\\rho_k$ 比率的一个关键弱点。",
            "answer": "$$\\boxed{8.571}$$"
        },
        {
            "introduction": "在我们将统计工具应用于复杂的梯度动态之前，通过一个更熟悉的场景——线性回归中的模型选择——来热身是很有帮助的。这个练习  旨在通过连接马洛斯 $C_p$ 准则和广义信息准则 (GIC) 来展示如何使用统计量来正则化模型。你将通过编程实验，理解基于统计估计（如噪声方差 $\\hat{\\sigma}^2$）的惩罚项如何影响模型的选择，为你掌握更高级的自适应算法奠定基础。",
            "id": "3143737",
            "problem": "考虑一个标准的线性回归设置，其中响应变量为 $y \\in \\mathbb{R}^n$，设计矩阵为 $X \\in \\mathbb{R}^{n \\times d}$，数据根据 $y = X \\beta + \\varepsilon$, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ 生成。普通最小二乘法 (OLS) 为任何包含截距项和前 $p-1$ 个特征的嵌套模型提供系数估计（因此每个候选模型计算截距项在内共有 $p$ 个参数）。残差平方和 (RSS) 用于量化样本内误差。广义信息准则 (GIC) 是一种信息准则，其定义带有一个与参数数量成正比的惩罚项。Mallows' $C_p$ 是一个经典准则，旨在产生对样本外预测误差的无偏估计。\n\n你的任务是通过受控实验，将 Mallows' $C_p$ 与惩罚项为 $c\\,p$ 的广义信息准则 (GIC) 联系起来。具体来说，你将通过改变 GIC 中的惩罚乘数 $c$ 来模拟惩罚不足和惩罚过度，并通过改变 Mallows' $C_p$ 中使用的外部噪声方差估计量的尺度来观察类似的行为。你必须展示 GIC 中改变 $c$ 与 Mallows' $C_p$ 中改变 $\\hat{\\sigma}^2$ 的尺度因子之间的映射关系。\n\n你必须遵循以下基本依据：\n- 假设线性模型 $y = X \\beta + \\varepsilon$ 以及普通最小二乘法 (OLS) 的性质，包括在正确设定的模型中的无偏性，以及模型复杂度与自由度之间的关系。\n- 使用残差平方和 (RSS) 来量化拟合质量，并使用来自全模型的噪声方差的无偏估计量。\n\n程序规范：\n1. 按如下方式确定性地合成数据：\n   - 使用固定的随机种子 $42$。\n   - 设 $n = 150$ 个观测值和 $d = 10$ 个特征。\n   - 从 $\\mathcal{N}(0,1)$ 分布中独立抽取元素，构成 $X_{\\text{feat}} \\in \\mathbb{R}^{n \\times d}$。\n   - 构建增广设计矩阵 $X = [\\mathbf{1}_n, X_{\\text{feat}}]$，其中 $\\mathbf{1}_n$ 是元素全为1的 $n$ 维向量（截距列）。因此 $X \\in \\mathbb{R}^{n \\times (d+1)}$。\n   - 设置真实系数向量 $\\beta \\in \\mathbb{R}^{d+1}$，其中 $\\beta_0 = 0$（截距项），$\\beta_1 = 3.0$, $\\beta_2 = -2.0$, $\\beta_3 = 1.0$, $\\beta_4 = 0.5$，所有其余元素均为 $0$。\n   - 设真实噪声标准差为 $\\sigma_{\\text{true}} = 1.0$，即 $\\varepsilon \\sim \\mathcal{N}(0, 1.0^2 I_n)$，并生成 $y = X \\beta + \\varepsilon$。\n2. 对于每个嵌套模型的大小 $p \\in \\{1, 2, \\dots, d+1\\}$（其中 $p$ 计算了截距项和前 $p-1$ 个特征），计算 OLS 拟合及其对应的残差平方和 $\\text{RSS}(p)$。\n3. 使用无偏估计量 $\\hat{\\sigma}^2 = \\text{RSS}(d+1) / (n - (d+1))$，从全模型（$p = d+1$）中估计噪声方差。\n4. 将带有惩罚参数 $c$ 的广义信息准则 (GIC) 定义为一个准则，对于模型大小 $p$，该准则在样本内拟合项上增加一个与 $c\\,p$ 成正比的项。你必须通过最小化一个使用 $\\text{RSS}(p)$ 和 $\\hat{\\sigma}^2$ 并包含惩罚项 $c\\,p$ 的形式的准则，来实现 GIC 下的模型选择。\n5. 使用从第一性原理推导出的规范定义（在解答中提供推导过程），实现 Mallows' $C_p$ 下的模型选择。使用一个缩放后的外部噪声方差估计 $\\hat{\\sigma}^2_k = k \\cdot \\hat{\\sigma}^2$，其中 $k$ 是一个正尺度因子。\n6. 对于下方的每个测试用例，计算：\n   - 在 GIC 下选定的模型大小，记为 $p_{\\text{GIC}}(c)$。\n   - 在使用缩放后方差 $\\hat{\\sigma}^2_k$ 的 Mallows' $C_p$ 下选定的模型大小，记为 $p_{C_p}(k)$。\n   - 报告整数差 $p_{\\text{GIC}}(c) - p_{C_p}(k)$。\n\n测试套件：\n使用以下这组 $(c, k)$ 对，它们被设计用来测试匹配和不匹配的映射、理想路径以及边界条件：\n- 用例 1：$(c, k) = (2.0, 1.0)$。\n- 用例 2：$(c, k) = (3.0, 1.5)$。\n- 用例 3：$(c, k) = (1.0, 1.0)$。\n- 用例 4：$(c, k) = (4.0, 1.0)$。\n- 用例 5：$(c, k) = (2.0, 0.5)$。\n- 用例 6：$(c, k) = (0.0, 1.0)$。\n- 用例 7：$(c, k) = (8.0, 1.0)$。\n- 用例 8：$(c, k) = (2.0, 1.25)$。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[r_1,r_2,\\dots,r_8]”），其中 $r_i$ 是按上述顺序排列的第 $i$ 个测试用例的整数 $p_{\\text{GIC}}(c_i) - p_{C_p}(k_i)$。不应打印任何其他文本。",
            "solution": "该问题要求在线性回归的背景下，研究两种模型选择准则——广义信息准则 (GIC) 和 Mallows' $C_p$——之间的关系。我们将建立它们的理论联系，然后使用一个确定性生成的数据集通过计算来验证它。\n\n这是一个标准的线性模型设置 $y = X \\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times (d+1)}$ 是设计矩阵（包含一个截距项），$\\beta \\in \\mathbb{R}^{d+1}$ 是真实系数向量，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ 是一个独立同分布的高斯噪声向量。\n\n我们考虑一组嵌套的候选模型，其中第 $p$ 个模型使用设计矩阵 $X$ 的前 $p$ 列。对于每个候选模型，我们可以计算其普通最小二乘法 (OLS) 拟合以及相应的残差平方和，记为 $\\text{RSS}(p)$。一个好的模型选择准则能够在样本内拟合（低 $\\text{RSS}(p)$）与模型复杂度（低 $p$）之间取得平衡。\n\n首先，我们来形式化 Mallows' $C_p$ 准则。Mallows' $C_p$ 旨在成为真实均方预测误差的一个无偏估计，并由噪声方差 $\\sigma^2$ 进行缩放。对于一个有 $p$ 个参数的模型，该统计量定义为：\n$$ C_p = \\frac{\\text{RSS}(p)}{\\hat{\\sigma}^2} + 2p - n $$\n此处，$\\hat{\\sigma}^2$ 是真实误差方差 $\\sigma^2$ 的一个无偏估计。一个常见的选择，也是本问题中指定的选择，是来自全模型的估计量（假设该估计量是无偏的）：\n$$ \\hat{\\sigma}^2 = \\frac{\\text{RSS}(d+1)}{n - (d+1)} $$\n由于 $n$ 对所有模型来说是一个常数，通过最小化 $C_p$ 来选择模型，等价于最小化 $\\frac{\\text{RSS}(p)}{\\hat{\\sigma}^2} + 2p$ 这个量。\n\n问题引入了 Mallows' $C_p$ 的一个变体，它使用一个缩放后的方差估计 $\\hat{\\sigma}^2_k = k \\cdot \\hat{\\sigma}^2$，其中 $k$ 为某个正尺度因子。需要最小化的准则变为：\n$$ \\min_{p} \\left( \\frac{\\text{RSS}(p)}{k \\hat{\\sigma}^2} + 2p \\right) $$\n由于 $k$ 和 $\\hat{\\sigma}^2$ 相对于对 $p$ 的最小化而言是正常数，我们可以将目标函数乘以 $k \\hat{\\sigma}^2$ 而不改变最小值的位置。这产生了一个等价的最小化问题：\n$$ \\min_{p} \\left( \\text{RSS}(p) + (2k) \\cdot p \\cdot \\hat{\\sigma}^2 \\right) $$\n这种形式将准则表示为样本内误差 ($\\text{RSS}(p)$) 与一个对参数数量 $p$ 呈线性的惩罚项之和。\n\n接下来，我们形式化广义信息准则 (GIC)。GIC 代表了一族形式为“拟合优度项 + 模型复杂度惩罚项”的准则。问题指定了一个与 $c \\cdot p$ 成正比的惩罚项。为了与上面推导出的 Mallows' $C_p$ 公式进行直接比较，我们将要最小化的 GIC 分数定义为：\n$$ \\text{GIC\\_Score}(p) = \\text{RSS}(p) + c \\cdot p \\cdot \\hat{\\sigma}^2 $$\n这是 GIC 的一个有效实例，其中每个额外参数的惩罚是 $c \\cdot \\hat{\\sigma}^2$。由该准则选择的模型 $p_{\\text{GIC}}(c)$ 是使该分数最小化的模型。\n\n通过比较缩放后 Mallows' $C_p$ 的目标函数和我们的 GIC 公式，我们可以建立一个直接的映射关系。\n$$ \\text{Scaled } C_p \\text{ objective: } \\quad \\text{RSS}(p) + (2k) \\cdot p \\cdot \\hat{\\sigma}^2 $$\n$$ \\text{GIC objective: } \\quad \\text{RSS}(p) + c \\cdot p \\cdot \\hat{\\sigma}^2 $$\n这两个准则在数学上是等价的，当且仅当它们的惩罚乘数相等，即：\n$$ c = 2k $$\n因此，我们预测，在 $c = 2k$ 的条件下，由惩罚乘数为 $c$ 的 GIC 选择的模型将与由方差缩放因子为 $k$ 的 Mallows' $C_p$ 选择的模型相同。在这种情况下，$p_{\\text{GIC}}(c) - p_{C_p}(k) = 0$。\n- 如果 $c > 2k$，GIC 对复杂度的惩罚比相应的缩放后 $C_p$ 更强，这将偏爱参数更少的模型。我们预期 $p_{\\text{GIC}}(c) \\le p_{C_p}(k)$。\n- 如果 $c  2k$，GIC 施加的惩罚较弱，会偏爱更复杂的模型。我们预期 $p_{\\text{GIC}}(c) \\ge p_{C_p}(k)$。\n\n计算流程如下：\n1.  为保证可复现性，将随机种子设为 $42$。根据问题规范生成数据：$n=150$ 个观测值，$d=10$ 个特征，一个指定的真实系数向量 $\\beta$，以及 $\\sigma_{\\text{true}}=1.0$ 的高斯噪声。\n2.  对于从 $1$ 到 $d+1=11$ 的每个嵌套模型大小 $p$，使用设计矩阵 $X$ 的前 $p$ 列拟合相应的 OLS 模型。\n3.  存储每个 $p$ 得到的 $\\text{RSS}(p)$。\n4.  使用来自全模型（$p=11$）的 $\\text{RSS}$ 计算无偏噪声方差估计 $\\hat{\\sigma}^2$。\n5.  对于测试套件中的每个 $(c, k)$ 对：\n    a. 计算所有 $p \\in \\{1, \\dots, 11\\}$ 的 GIC 分数：$\\text{GIC\\_Score}(p) = \\text{RSS}(p) + c \\cdot p \\cdot \\hat{\\sigma}^2$。\n    b. 找到最优模型大小 $p_{\\text{GIC}}(c) = \\arg\\min_{p} \\text{GIC\\_Score}(p)$。\n    c. 计算所有 $p \\in \\{1, \\dots, 11\\}$ 的等价 Mallows' $C_p$ 分数：$\\text{C}_p\\text{\\_Score}(p) = \\text{RSS}(p) + 2k \\cdot p \\cdot \\hat{\\sigma}^2$。\n    d. 找到最优模型大小 $p_{C_p}(k) = \\arg\\min_{p} \\text{C}_p\\text{\\_Score}(p)$。\n    e. 计算并存储整数差 $p_{\\text{GIC}}(c) - p_{C_p}(k)$。\n6.  最终输出是这些差值的列表。\n\n这个流程将数值上证明当 $c=2k$ 时的等价性，以及当该条件不成立时的惩罚不足/过度效应。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem by comparing GIC and Mallows' Cp.\n    \"\"\"\n    # 1. Synthesize data deterministically as specified.\n    seed = 42\n    n = 150\n    d = 10\n    \n    rng = np.random.default_rng(seed)\n\n    # Draw features from N(0,1)\n    X_feat = rng.normal(loc=0.0, scale=1.0, size=(n, d))\n    \n    # Form the augmented design matrix X with an intercept column.\n    X = np.hstack((np.ones((n, 1)), X_feat))\n\n    # Set the true coefficient vector beta.\n    beta_true = np.zeros(d + 1)\n    beta_true[0] = 0.0  # Intercept\n    beta_true[1] = 3.0\n    beta_true[2] = -2.0\n    beta_true[3] = 1.0\n    beta_true[4] = 0.5\n    # beta_true[5:] are implicitly 0.0\n\n    # Generate the response variable y.\n    sigma_true = 1.0\n    epsilon = rng.normal(loc=0.0, scale=sigma_true, size=n)\n    y = X @ beta_true + epsilon\n\n    # 2. For each nested model size p, compute RSS(p).\n    p_values = np.arange(1, d + 2)  # Model sizes from 1 to d+1 (11)\n    rss_values = []\n    \n    for p in p_values:\n        X_p = X[:, :p]\n        # Use np.linalg.lstsq to perform OLS.\n        # It returns a tuple, where the second element is the sum of squared residuals (RSS).\n        # The returned residuals is a 1-element array, so we extract the value with [0].\n        # If the model fits perfectly, residuals is an empty array.\n        res = np.linalg.lstsq(X_p, y, rcond=None)[1]\n        \n        if res.size > 0:\n            rss_values.append(res[0])\n        else:\n            # This case occurs for a perfect fit, RSS = 0.\n            rss_values.append(0.0)\n            \n    rss_values = np.array(rss_values)\n\n    # 3. Estimate the noise variance from the full model.\n    rss_full = rss_values[-1]\n    degrees_of_freedom_full = n - (d + 1)\n    sigma_hat_sq = rss_full / degrees_of_freedom_full\n\n    # Test suite of (c, k) pairs.\n    test_cases = [\n        (2.0, 1.0),\n        (3.0, 1.5),\n        (1.0, 1.0),\n        (4.0, 1.0),\n        (2.0, 0.5),\n        (0.0, 1.0),\n        (8.0, 1.0),\n        (2.0, 1.25),\n    ]\n\n    results = []\n    for c, k in test_cases:\n        # 4. Compute selected model size under GIC.\n        # The criterion to minimize is RSS(p) + c * p * sigma_hat^2.\n        gic_scores = rss_values + c * p_values * sigma_hat_sq\n        # Find the model size p that minimizes the score.\n        # np.argmin returns the 0-based index of the minimum.\n        p_gic = p_values[np.argmin(gic_scores)]\n\n        # 5. Compute selected model size under scaled Mallows' Cp.\n        # The criterion is RSS(p)/(k*sigma_hat^2) + 2p, which is equivalent to\n        # minimizing RSS(p) + 2k * p * sigma_hat^2.\n        cp_scores = rss_values + 2 * k * p_values * sigma_hat_sq\n        p_cp = p_values[np.argmin(cp_scores)]\n\n        # 6. Compute and store the difference.\n        difference = p_gic - p_cp\n        results.append(int(difference))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现在，我们将综合前面学到的知识，解决自适应梯度裁剪这一核心问题。正如我们在第一个练习中看到的，控制更新步长以保证稳定性至关重要，而在第二个练习中我们实践了如何运用统计惩罚项。这个最终练习  将指导你使用统计工具来调节梯度本身的动态，你将通过实现和比较不同的自适应阈值策略，亲身验证为何鲁棒统计量（如中位数和中位数绝对偏差）在处理梯度爆炸等异常值时，比简单统计量（如均值）能带来更稳定的训练过程。",
            "id": "3131435",
            "problem": "考虑一个离散时间的、基于梯度的优化过程，其在步骤 $t$ 的参数更新由 $w_{t+1} = w_t - \\eta \\cdot \\hat{g}_t$ 给出，其中 $\\eta  0$ 是学习率，$\\hat{g}_t$ 是原始梯度 $g_t$ 的裁剪版本。梯度裁剪在其欧几里得范数超过阈值时对 $g_t$ 进行缩放，这可以防止因步长过大而破坏训练的稳定性。形式上，令 $r_t = \\lVert g_t \\rVert_2$ 表示 $g_t$ 的欧几里得范数。通过将 $g_t$ 投影到以原点为中心、半径为 $\\tau_t$ 的闭合欧几里得球上，来定义裁剪后的梯度 $\\hat{g}_t$，这等价于将 $g_t$ 乘以因子 $\\min\\{1, \\tau_t / r_t\\}$ 进行缩放。那么，裁剪后的步长范数为 $u_t = \\eta \\cdot \\min\\{r_t, \\tau_t\\}$。\n\n您将实现自适应裁剪阈值 $\\tau_t$，该阈值根据过去梯度范数的时间窗口计算得出，以捕捉近期的动态。为了以科学严谨的方式设计和比较阈值，请从以下应用于过去梯度范数的有限窗口 $W_t$ 的描述性统计核心定义开始。对于窗口 $W_t = \\{r_{t-k} \\mid 1 \\le k \\le \\min\\{W, t-1\\}\\}$，定义：\n- 样本均值 $\\mu_t$ 为 $\\mu_t = \\frac{1}{|W_t|} \\sum_{x \\in W_t} x$。\n- 样本中位数 $m_t$ 为 $W_t$ 的中间顺序统计量（当 $|W_t|$ 为偶数时，则为中间两个值的平均值）。\n- 中位数绝对偏差 (MAD) 为 $\\mathrm{MAD}_t = \\operatorname{median}\\big(\\{|x - m_t| : x \\in W_t\\}\\big)$。\n\n使用这些统计量来指定 $\\tau_t$ 的三个自适应裁剪规则：\n1. 一个基于均值的阈值，随样本均值 $\\mu_t$ 缩放。\n2. 一个基于中位数的阈值，随样本中位数 $m_t$ 缩放。\n3. 一个稳健的阈值，它结合了样本中位数 $m_t$ 和中位数绝对偏差 $\\mathrm{MAD}_t$，并使用一个固定的比例因子 $\\kappa = 1.4826$，该因子的使用是受正态分布波动的启发。\n\n在每种情况下，缩放常数 $c  0$ 都与相关统计量相乘。当窗口 $W_t$ 为空时（即在 $t = 1$ 时），对所有方案设置 $\\tau_1 = c \\cdot r_1$。对于 $t  1$，从之前的 $\\min\\{W, t-1\\}$ 个梯度范数构建 $W_t$。\n\n您的程序必须为每种裁剪规则计算在 $T$ 个步骤的固定时间范围内，裁剪后的步长范数 $u_t$ 的经验方差，定义为\n$$\n\\operatorname{Var}(u) = \\frac{1}{T} \\sum_{t=1}^{T} \\left(u_t - \\bar{u}\\right)^2, \\quad \\text{其中} \\quad \\bar{u} = \\frac{1}{T} \\sum_{t=1}^{T} u_t.\n$$\n使用此方差来量化稳定性：方差越小，表示更新越稳定。\n\n然后，为每个测试用例报告两个浮点数，用于比较稳健统计量相对于基于均值的阈值的稳定性：\n- 比率 $R_{\\text{median}} = \\frac{\\operatorname{Var}(u)_{\\text{median}}}{\\operatorname{Var}(u)_{\\text{mean}}}$。\n- 比率 $R_{\\text{mad}} = \\frac{\\operatorname{Var}(u)_{\\text{mad}}}{\\operatorname{Var}(u)_{\\text{mean}}}$。\n\n小于 $1$ 的值表示相对于基于均值的阈值，稳定性有所提高。使用以下由平滑基线和注入的离群值构成的确定性梯度范数序列测试套件来计算这些比率。对于所有情况，通过\n$$\nr_t^{\\text{base}} = b + a \\cdot \\sin\\left(\\frac{2\\pi t}{P}\\right),\n$$\n定义基线，然后用指定的离群值在列出的索引处覆盖 $r_t$。所有量都是无量纲的，不适用任何物理单位。\n\n对四个测试用例使用以下参数：\n\n- 案例 1（理想情况，无离群值）：$T = 60$, $W = 10$, $c = 1.0$, $\\eta = 0.05$, $b = 1.0$, $a = 0.2$, $P = 12$。无离群值：对于所有 $t$，$r_t = r_t^{\\text{base}}$。\n- 案例 2（单个大离群值）：与案例 1 具有相同的基线和超参数；设置 $r_{30} = 15.0$。\n- 案例 3（多个大离群值）：与案例 1 具有相同的基线和超参数；设置 $r_{15} = 25.0$, $r_{35} = 25.0$, $r_{50} = 25.0$。\n- 案例 4（边界窗口大小）：$T = 60$, $W = 1$, $c = 1.0$, $\\eta = 0.05$, $b = 1.0$, $a = 0.2$, $P = 12$；设置 $r_{30} = 50.0$。\n\n实现细节：\n- 对于每个案例，根据基线和离群值定义，确定性地构建完整的序列 $\\{r_t\\}_{t=1}^{T}$。\n- 对于每个 $t$，用 $1 \\le k \\le \\min\\{W, t-1\\}$ 构建 $W_t = \\{r_{t-k}\\}$。\n- 对于三个裁剪规则，使用从 $W_t$ 得到的相应统计量和上述缩放常数计算阈值。\n- 计算裁剪后的步长范数 $u_t = \\eta \\cdot \\min\\{r_t, \\tau_t\\}$。\n- 对每个裁剪规则，使用上面给出的总体方差公式计算 $\\operatorname{Var}(u)$。\n- 为每个案例输出一对浮点数 $\\left(R_{\\text{median}}, R_{\\text{mad}}\\right)$，按四个案例的顺序，平铺成一个列表。\n\n最终输出格式：\n您的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表的结果（例如，$[r_1,r_2,r_3,r_4,r_5,r_6,r_7,r_8]$），其中 $r_{2k-1}$ 和 $r_{2k}$ 分别是案例 $k$ 的 $R_{\\text{median}}$ 和 $R_{\\text{mad}}$。",
            "solution": "问题陈述经评估有效。它在科学上基于数值优化和稳健统计学的原理，定义良好，具有清晰完整的定义和约束，并且表述客观。通过直接应用所提供的公式可以推导出唯一解。\n\n核心任务是比较三种自适应梯度裁剪策略的稳定性。稳定性通过裁剪后的步长范数 $u_t$ 在固定时间范围 $T$ 内的经验方差来量化。较低的方差意味着在优化过程中参数更新更稳定、更少出现不规则波动。这三种策略基于过去梯度范数的时间窗口 $W_t$ 上计算的统计数据来定义裁剪阈值 $\\tau_t$。\n\n首先，我们建立原始梯度范数序列 $\\{r_t\\}_{t=1}^T$，它作为我们分析的输入信号。对于每个测试用例，该序列是使用一个带有注入离群值的基线正弦函数确定性地生成的。基线由以下公式给出：\n$$\nr_t^{\\text{base}} = b + a \\cdot \\sin\\left(\\frac{2\\pi t}{P}\\right)\n$$\n其中 $a$ 是振幅，$b$ 是垂直位移，$P$ 是周期。然后在指定的时间步长 $t$ 处覆盖特定的 $r_t$ 值，以模拟离群梯度，这在训练大型神经网络中很常见，并可能破坏学习过程的稳定性。\n\n问题的核心在于定义自适应裁剪阈值 $\\tau_t$。裁剪后的梯度 $\\hat{g}_t$ 是通过将原始梯度 $g_t$ 乘以一个因子 $\\min\\{1, \\tau_t / r_t\\}$ 得到的，其中 $r_t = \\lVert g_t \\rVert_2$。这确保了裁剪后梯度的范数不会超过 $\\tau_t$。因此，更新步骤的范数，我们记为裁剪后的步长范数 $u_t$，是：\n$$\nu_t = \\eta \\cdot \\lVert \\hat{g}_t \\rVert_2 = \\eta \\cdot \\lVert g_t \\cdot \\min\\{1, \\tau_t / r_t\\} \\rVert_2 = \\eta \\cdot r_t \\cdot \\min\\{1, \\tau_t / r_t\\} = \\eta \\cdot \\min\\{r_t, \\tau_t\\}\n$$\n其中 $\\eta$ 是学习率。\n\n设置 $\\tau_t$ 的三个规则是基于从过去梯度范数的窗口 $W_t = \\{r_{t-k} \\mid 1 \\le k \\le \\min\\{W, t-1\\}\\}$ 计算出的统计数据。对于第一步 $t=1$，$W_1$ 窗口为空。按照规定，所有三个规则的阈值都初始化为 $\\tau_1 = c \\cdot r_1$。对于后续步骤 ($t>1$)，我们定义：\n\n1.  **基于均值的阈值**：该规则使用窗口中范数的样本均值 $\\mu_t = \\frac{1}{|W_t|} \\sum_{x \\in W_t} x$。阈值与该均值成正比：\n    $$\n    \\tau_{t, \\text{mean}} = c \\cdot \\mu_t\n    $$\n    这种方法简单但对窗口中的离群值敏感，因为单个大值可以显著抬高均值。\n\n2.  **基于中位数的阈值**：该规则使用样本中位数 $m_t = \\operatorname{median}(W_t)$，这是一种稳健的集中趋势度量。阈值为：\n    $$\n    \\tau_{t, \\text{median}} = c \\cdot m_t\n    $$\n    由于中位数对离群值具有抵抗力，当梯度范数出现突然的峰值时，预计该阈值会比基于均值的阈值更稳定。\n\n3.  **基于 MAD 的稳健阈值**：该规则通过引入统计离散度的度量——中位数绝对偏差 (MAD)，定义为 $\\mathrm{MAD}_t = \\operatorname{median}(\\{|x - m_t| : x \\in W_t\\})$ 来增强稳健性。常数 $\\kappa = 1.4826$ 用于缩放 MAD，使得 $\\kappa \\cdot \\mathrm{MAD}_t$ 成为正态分布标准差的一致估计量。问题陈述指出，阈值“结合了样本中位数 $m_t$ 和中位数绝对偏差 $\\mathrm{MAD}_t$”，并且在所有情况下，缩放常数 $c$ 都“乘以相关统计量”。对这些陈述最一致的解释是，将该统计量定义为“均值加标准差”的稳健模拟，从而得出公式：\n    $$\n    \\tau_{t, \\text{mad}} = c \\cdot (m_t + \\kappa \\cdot \\mathrm{MAD}_t)\n    $$\n    这种方法同时基于位置的稳健度量 ($m_t$) 和离散程度的稳健度量 ($\\mathrm{MAD}_t$) 来设置阈值，旨在即使存在离群值也能获得高度稳定的行为。\n\n对于四个测试用例中的每一个，我们执行以下过程。首先，我们生成整个序列 $\\{r_t\\}_{t=1}^T$。然后，对于每个时间步 $t=1, \\dots, T$，我们根据上述规则计算三个阈值 $\\tau_{t, \\text{mean}}$、$\\tau_{t, \\text{median}}$ 和 $\\tau_{t, \\text{mad}}$。随后，我们计算相应的裁剪后步长范数 $\\{u_{t, \\text{mean}}\\}_{t=1}^T$、$\\{u_{t, \\text{median}}\\}_{t=1}^T$ 和 $\\{u_{t, \\text{mad}}\\}_{t=1}^T$。\n\n最后，我们通过计算其经验总体方差来量化每个步长范数序列的稳定性：\n$$\n\\operatorname{Var}(u) = \\frac{1}{T} \\sum_{t=1}^{T} \\left(u_t - \\bar{u}\\right)^2, \\quad \\text{其中} \\quad \\bar{u} = \\frac{1}{T} \\sum_{t=1}^{T} u_t\n$$\n为了比较性能，我们计算稳健方法的方差与基准的基于均值的方法的方差的比率：\n-   $R_{\\text{median}} = \\frac{\\operatorname{Var}(u)_{\\text{median}}}{\\operatorname{Var}(u)_{\\text{mean}}}$\n-   $R_{\\text{mad}} = \\frac{\\operatorname{Var}(u)_{\\text{mad}}}{\\operatorname{Var}(u)_{\\text{mean}}}$\n\n小于 1 的比率表示对于给定的梯度范数序列，相应的稳健方法比基于均值的方法提供了更稳定的步长更新。对于案例 4，其中窗口大小 $W=1$，对于 $t>1$ 的统计数据基于单个值 $r_{t-1}$。在这种情况下，$\\mu_t = m_t = r_{t-1}$ 且 $\\mathrm{MAD}_t = 0$。因此，所有三个阈值规则都简化为 $\\tau_t = c \\cdot r_{t-1}$，使得这三种方法完全相同。因此，得到的方差将相等，两个比率都将恰好为 $1.0$。这可作为实现的一个关键内部一致性检查。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1: No outliers\n        {'T': 60, 'W': 10, 'c': 1.0, 'eta': 0.05, 'b': 1.0, 'a': 0.2, 'P': 12, 'outliers': {}},\n        # Case 2: Single large outlier\n        {'T': 60, 'W': 10, 'c': 1.0, 'eta': 0.05, 'b': 1.0, 'a': 0.2, 'P': 12, 'outliers': {30: 15.0}},\n        # Case 3: Multiple large outliers\n        {'T': 60, 'W': 10, 'c': 1.0, 'eta': 0.05, 'b': 1.0, 'a': 0.2, 'P': 12, 'outliers': {15: 25.0, 35: 25.0, 50: 25.0}},\n        # Case 4: Boundary window size W=1\n        {'T': 60, 'W': 1, 'c': 1.0, 'eta': 0.05, 'b': 1.0, 'a': 0.2, 'P': 12, 'outliers': {30: 50.0}},\n    ]\n\n    KAPPA = 1.4826\n    \n    results = []\n    for case_params in test_cases:\n        ratios = compute_variance_ratios(case_params, KAPPA)\n        results.extend(ratios)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef compute_variance_ratios(params, kappa):\n    \"\"\"\n    Computes the variance ratios for a single test case.\n    \n    Args:\n        params (dict): A dictionary of parameters for the test case.\n        kappa (float): The scaling constant for MAD.\n        \n    Returns:\n        tuple: A tuple containing (R_median, R_mad).\n    \"\"\"\n    T = params['T']\n    W = params['W']\n    c = params['c']\n    eta = params['eta']\n    b = params['b']\n    a = params['a']\n    P = params['P']\n    outliers = params['outliers']\n\n    # 1. Generate the sequence of raw gradient norms {r_t}\n    t_steps = np.arange(1, T + 1)\n    r_sequence = b + a * np.sin(2 * np.pi * t_steps / P)\n    for t, val in outliers.items():\n        # Problem uses 1-based indexing for time steps\n        if 1 = t = T:\n            r_sequence[t - 1] = val\n\n    u_mean = np.zeros(T)\n    u_median = np.zeros(T)\n    u_mad = np.zeros(T)\n\n    # 2. Iterate through time steps to compute thresholds and clipped norms\n    for t_idx in range(T):\n        t = t_idx + 1\n        r_t = r_sequence[t_idx]\n\n        if t == 1:\n            # Special case for t=1\n            tau_t_mean = c * r_t\n            tau_t_median = c * r_t\n            tau_t_mad = c * r_t\n        else:\n            # For t > 1, use trailing window\n            win_size = min(W, t - 1)\n            window = r_sequence[t_idx - win_size : t_idx]\n            \n            # Compute statistics\n            mu_t = np.mean(window)\n            m_t = np.median(window)\n            mad_t = np.median(np.abs(window - m_t))\n\n            # Compute thresholds for the three rules\n            tau_t_mean = c * mu_t\n            tau_t_median = c * m_t\n            tau_t_mad = c * (m_t + kappa * mad_t)\n        \n        # 3. Compute clipped step norms\n        u_mean[t_idx] = eta * min(r_t, tau_t_mean)\n        u_median[t_idx] = eta * min(r_t, tau_t_median)\n        u_mad[t_idx] = eta * min(r_t, tau_t_mad)\n\n    # 4. Compute population variances\n    # np.var computes population variance by default (ddof=0)\n    var_mean = np.var(u_mean)\n    var_median = np.var(u_median)\n    var_mad = np.var(u_mad)\n    \n    # Handle case where denominator is zero to avoid NaN\n    if var_mean == 0:\n        # This implies all step norms were identical, so others should be too.\n        # Ratios are 1 if variances are equal, or NaN if they differ.\n        # Safe to assume 1.0 in this context.\n        r_median = 1.0 if var_median == 0 else np.nan\n        r_mad = 1.0 if var_mad == 0 else np.nan\n    else:\n        r_median = var_median / var_mean\n        r_mad = var_mad / var_mean\n\n    return r_median, r_mad\n\nsolve()\n```"
        }
    ]
}