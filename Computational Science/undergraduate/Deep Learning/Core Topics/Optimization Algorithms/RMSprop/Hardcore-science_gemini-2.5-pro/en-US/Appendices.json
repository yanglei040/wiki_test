{
    "hands_on_practices": [
        {
            "introduction": "In many deep learning applications, gradients can be sparse, meaning most are small while a few are exceptionally large. This exercise models such a scenario with a periodic gradient signal that has occasional spikes. By deriving the steady-state behavior of the RMSprop accumulator $v_t$, you will gain a concrete understanding of how it functions as a buffer, smoothing out these spikes to maintain stable training. ",
            "id": "3170912",
            "problem": "Consider Root Mean Square Propagation (RMSprop), a stochastic optimization method that stabilizes parameter updates by maintaining a decaying, exponentially weighted average of past squared gradients. Let the decay rate parameter be $\\rho \\in (0,1)$, and let the sequence of squared gradients be defined by a periodic pattern with period $k \\in \\mathbb{N}$: specifically, for time index $t \\in \\mathbb{N}$, let $g_t^2 = M$ when $t$ is a positive multiple of $k$ and $g_t^2 = 1$ otherwise, where $M$ is a fixed constant with $M \\gg 1$. Assume the RMSprop accumulator is initialized at $v_0 = 0$ and that training has proceeded long enough for $v_t$ to reach a time-periodic steady state synchronized with the gradient sequence.\n\nStarting from the principle that RMSprop forms an exponentially weighted moving average of the squared gradients with geometric weights that sum to $1$, derive the recursion for the accumulator and analyze its behavior under the periodic input described above. In the steady state, let $v_t$ be evaluated immediately after each spike time $t = nk$ for $n \\in \\mathbb{N}$; denote this value by $s$, which is constant across spikes in steady state.\n\nDetermine a closed-form analytic expression for $s$ as a function of $\\rho$, $M$, and $k$. Provide your final answer as a single simplified symbolic expression. No rounding is required.",
            "solution": "The problem asks for a closed-form expression for the steady-state value of the Root Mean Square Propagation (RMSprop) accumulator, denoted by $s$, under a specific periodic sequence of squared gradients.\n\nFirst, we establish the fundamental recursion for the RMSprop accumulator, $v_t$. It is defined as an exponentially weighted moving average of the squared gradients, $g_t^2$. The update rule is given by:\n$$v_t = \\rho v_{t-1} + (1-\\rho) g_t^2$$\nwhere $\\rho \\in (0,1)$ is the decay rate parameter, and $v_t$ is the accumulator value at time step $t$. The problem provides the initial condition $v_0 = 0$, but focuses on the time-periodic steady state, which is reached after a sufficient number of time steps.\n\nThe sequence of squared gradients, $g_t^2$, follows a periodic pattern of period $k \\in \\mathbb{N}$:\n$$g_t^2 = \\begin{cases} M  \\text{if } t = nk \\text{ for some } n \\in \\mathbb{N} \\\\ 1  \\text{otherwise} \\end{cases}$$\nwhere $M$ is a constant. This means that at time steps which are multiples of $k$, there is a \"spike\" of value $M$, and at all other time steps, the value is $1$.\n\nWe are interested in the steady-state value of the accumulator. The problem defines $s$ as the value of $v_t$ evaluated immediately after each spike, i.e., at times $t=nk$ for large $n$. The steady-state condition implies that this value is constant from one spike to the next. Mathematically, if we denote the value of the accumulator just after the spike at time $(n-1)k$ as $v_{(n-1)k}$, and the value just after the spike at time $nk$ as $v_{nk}$, then in steady state:\n$$v_{(n-1)k} = v_{nk} = s$$\n\nOur strategy is to propagate the accumulator value through one full period, from time $t=(n-1)k$ to $t=nk$, and then impose the steady-state condition. Let us assume the accumulator value at the beginning of the cycle is $v_{(n-1)k} = s$.\n\nThe cycle consists of $k-1$ steps where $g_t^2=1$, followed by one step where $g_t^2=M$.\n\nFor the steps from $t = (n-1)k + 1$ to $t = nk - 1$, the squared gradient is $g_t^2 = 1$. Let's compute the accumulator value just before the spike at time $nk$, which is $v_{nk-1}$.\nStarting from $v_{(n-1)k} = s$:\n$$v_{(n-1)k+1} = \\rho v_{(n-1)k} + (1-\\rho)g_{(n-1)k+1}^2 = \\rho s + (1-\\rho)(1)$$\n$$v_{(n-1)k+2} = \\rho v_{(n-1)k+1} + (1-\\rho)g_{(n-1)k+2}^2 = \\rho(\\rho s + 1-\\rho) + (1-\\rho) = \\rho^2 s + \\rho(1-\\rho) + (1-\\rho)$$\nBy induction, after $j$ steps (where $j \\in \\{1, 2, \\dots, k-1\\}$), the accumulator value is:\n$$v_{(n-1)k+j} = \\rho^j v_{(n-1)k} + (1-\\rho) \\sum_{i=0}^{j-1} \\rho^i g_{(n-1)k+j-i}^2$$\nSince $g_t^2=1$ for this interval, this simplifies to:\n$$v_{(n-1)k+j} = \\rho^j s + (1-\\rho) \\sum_{i=0}^{j-1} \\rho^i$$\nThe sum is a finite geometric series: $\\sum_{i=0}^{j-1} \\rho^i = \\frac{1-\\rho^j}{1-\\rho}$.\nSubstituting this into the expression for $v_{(n-1)k+j}$:\n$$v_{(n-1)k+j} = \\rho^j s + (1-\\rho) \\frac{1-\\rho^j}{1-\\rho} = \\rho^j s + 1 - \\rho^j$$\nTo find the value just before the spike at $t=nk$, we set $j=k-1$:\n$$v_{nk-1} = \\rho^{k-1} s + 1 - \\rho^{k-1}$$\n\nNow, we perform the final update step for the spike at $t=nk$. At this time step, the squared gradient is $g_{nk}^2 = M$.\n$$v_{nk} = \\rho v_{nk-1} + (1-\\rho) g_{nk}^2$$\nSubstituting the expression for $v_{nk-1}$:\n$$v_{nk} = \\rho (\\rho^{k-1} s + 1 - \\rho^{k-1}) + (1-\\rho)M$$\n$$v_{nk} = \\rho^k s + \\rho(1 - \\rho^{k-1}) + (1-\\rho)M$$\n$$v_{nk} = \\rho^k s + \\rho - \\rho^k + M(1-\\rho)$$\n\nWe now apply the steady-state condition, $v_{nk} = s$:\n$$s = \\rho^k s + \\rho - \\rho^k + M(1-\\rho)$$\nOur goal is to solve this equation for $s$.\n$$s - \\rho^k s = M(1-\\rho) + \\rho - \\rho^k$$\n$$s(1-\\rho^k) = M(1-\\rho) + \\rho - \\rho^k$$\n$$s = \\frac{M(1-\\rho) + \\rho - \\rho^k}{1-\\rho^k}$$\n\nTo obtain a more interpretable form, we can manipulate the numerator. We add and subtract $1$ inside the fraction in a specific way:\n$$s = \\frac{M(1-\\rho) - (1-\\rho) + (1-\\rho) + \\rho - \\rho^k}{1-\\rho^k}$$\nThis is not the most direct path. A more structured rearrangement is to separate the expression:\n$$s = \\frac{M(1-\\rho) + \\rho - \\rho^k}{1-\\rho^k} = \\frac{(M-1)(1-\\rho) + 1 - \\rho^k}{1-\\rho^k}$$\nThe equality holds because $- (1-\\rho) + 1 - \\rho^k = -1 + \\rho + 1 - \\rho^k = \\rho - \\rho^k$.\nNow, grouping terms:\n$$s = \\frac{(M-1)(1-\\rho) + (1-\\rho^k)}{1-\\rho^k}$$\nSplitting the fraction into two parts:\n$$s = \\frac{(M-1)(1-\\rho)}{1-\\rho^k} + \\frac{1-\\rho^k}{1-\\rho^k}$$\n$$s = 1 + \\frac{(M-1)(1-\\rho)}{1-\\rho^k}$$\nThis is the final, simplified closed-form expression for $s$. This form has a clear interpretation: the steady-state value is a baseline of $1$ (which would be the result if $g_t^2$ were always $1$) plus an additional term that represents the contribution from the periodic spikes of magnitude $M$.",
            "answer": "$$\n\\boxed{1 + \\frac{(M-1)(1-\\rho)}{1-\\rho^k}}\n$$"
        },
        {
            "introduction": "Understanding an algorithm's limitations is as crucial as knowing its strengths. This practice provides a compelling counterexample by \"breaking\" RMSprop. By setting the memory parameter $\\rho$ to zero, you will analyze a scenario where the optimizer, confronted with persistent noise, fails to converge on even a simple convex function. This thought experiment powerfully demonstrates why the accumulator's memory is not just a feature but a necessity for stability. ",
            "id": "3170844",
            "problem": "Consider optimizing the one-dimensional objective function $f(x) = \\frac{1}{2} x^{2}$ using Root Mean Square Propagation (RMSprop). Assume a fixed learning rate $\\eta  0$, an extremely small decay parameter $\\rho$ taken to be $\\rho = 0$, and a numerical stabilizer $\\epsilon = 0$. Let the observed gradient at iteration $t$ be the noisy quantity $g_{t} = \\nabla f(x_{t}) + \\delta_{t}$, where $\\nabla f(x) = x$, and the noise is the deterministic alternating sequence $\\delta_{t} = \\Delta(-1)^{t}$ for a fixed $\\Delta  0$. The algorithm maintains an exponential moving average $v_{t}$ of squared observed gradients and uses it to adaptively scale the update at each step.\n\nStart from $x_{0} = 0$. Using only fundamental definitions (the gradient of $f$, exponential moving averages, and the principle of adaptive gradient scaling by the root-mean-square of recent gradients), derive the exact iterate sequence $\\{x_{t}\\}_{t \\geq 0}$ produced by RMSprop under these settings and construct a counterexample showing failure to converge when $\\rho$ is too small. Explicitly explain the mechanism by which the noisiness of $v_{t}$ (due to the extremely small $\\rho$) prevents convergence in this setup.\n\nUnder the stated conditions and assuming $\\Delta$ satisfies $\\Delta  \\eta$, determine the value of $\\limsup_{t \\to \\infty} |x_{t}|$ in closed form as a function of $\\eta$. Express your final answer as a single closed-form analytic expression. No rounding is required.",
            "solution": "The problem asks for an analysis of the Root Mean Square Propagation (RMSprop) optimization algorithm under a specific set of pathological conditions. We must first validate the problem statement and then, if valid, derive the iterate sequence, explain the mechanism of non-convergence, and determine the limit superior of the absolute value of the iterates.\n\nThe fundamental update rules for RMSprop are given by an exponential moving average of squared gradients, $v_t$, and the parameter update, $x_{t+1}$.\nThe moving average is updated as:\n$$v_{t} = \\rho v_{t-1} + (1-\\rho) g_{t}^{2}$$\nThe parameter update is:\n$$x_{t+1} = x_{t} - \\frac{\\eta}{\\sqrt{v_{t}} + \\epsilon} g_{t}$$\n\nThe problem provides the following specific values and functions:\n- Objective function: $f(x) = \\frac{1}{2} x^{2}$, with gradient $\\nabla f(x) = x$.\n- Observed gradient: $g_{t} = \\nabla f(x_{t}) + \\delta_{t} = x_{t} + \\Delta(-1)^{t}$.\n- Parameters: learning rate $\\eta  0$, decay parameter $\\rho = 0$, numerical stabilizer $\\epsilon = 0$.\n- Initial condition: $x_{0} = 0$.\n- Constraint: $\\Delta  \\eta  0$.\n\nFirst, we substitute the given parameters $\\rho = 0$ and $\\epsilon = 0$ into the general RMSprop equations.\nThe update for the moving average $v_t$ becomes:\n$$v_{t} = (0) v_{t-1} + (1-0) g_{t}^{2} = g_{t}^{2}$$\nThe parameter update for $x_{t+1}$ becomes:\n$$x_{t+1} = x_{t} - \\frac{\\eta}{\\sqrt{g_{t}^{2}} + 0} g_{t} = x_{t} - \\frac{\\eta}{|g_{t}|} g_{t}$$\nThis simplified update rule reveals a critical aspect. For any non-zero gradient $g_t$, the term $\\frac{g_t}{|g_t|}$ is equivalent to the sign of the gradient, $\\text{sgn}(g_t)$. The update rule effectively becomes that of the Sign Gradient Descent algorithm with a constant step size of $\\eta$:\n$$x_{t+1} = x_{t} - \\eta \\cdot \\text{sgn}(g_{t})$$\nWe must ensure that $g_t$ is never zero to avoid a division by zero.\n\nWe now derive the exact iterate sequence $\\{x_{t}\\}_{t \\geq 0}$ starting from $x_{0} = 0$.\n\nFor $t=0$:\n- $x_{0} = 0$\n- The observed gradient is $g_{0} = x_{0} + \\Delta(-1)^{0} = 0 + \\Delta = \\Delta$. Since $\\Delta  0$, $g_{0} \\neq 0$.\n- The next iterate is $x_{1} = x_{0} - \\frac{\\eta}{|g_{0}|}g_{0} = 0 - \\frac{\\eta}{|\\Delta|}\\Delta = -\\eta$.\n\nFor $t=1$:\n- $x_{1} = -\\eta$\n- The observed gradient is $g_{1} = x_{1} + \\Delta(-1)^{1} = -\\eta - \\Delta = -(\\eta + \\Delta)$. Since $\\eta, \\Delta  0$, $g_{1} \\neq 0$.\n- The next iterate is $x_{2} = x_{1} - \\frac{\\eta}{|g_{1}|}g_{1} = -\\eta - \\frac{\\eta}{|-(\\eta + \\Delta)|}(-(\\eta + \\Delta)) = -\\eta - \\frac{\\eta}{\\eta + \\Delta}(-(\\eta + \\Delta)) = -\\eta + \\eta = 0$.\n\nFor $t=2$:\n- $x_{2} = 0$\n- The observed gradient is $g_{2} = x_{2} + \\Delta(-1)^{2} = 0 + \\Delta = \\Delta$.\n- The next iterate is $x_{3} = x_{2} - \\frac{\\eta}{|g_{2}|}g_{2} = 0 - \\frac{\\eta}{|\\Delta|}\\Delta = -\\eta$.\n\nThe sequence of iterates appears to be $\\{0, -\\eta, 0, -\\eta, \\ldots\\}$. We can prove by induction that $x_{t} = 0$ for even $t$ and $x_{t} = -\\eta$ for odd $t$.\nBase cases for $t=0, 1, 2$ have been shown.\nAssume for some integer $k \\geq 0$, $x_{2k} = 0$ and $x_{2k+1} = -\\eta$.\nWe will show that $x_{2k+2} = 0$ and $x_{2k+3} = -\\eta$.\n\nStep 1: Compute $x_{2k+2}$ from $x_{2k+1}$.\nThe iterate is $x_{2k+1} = -\\eta$. The current time step is $t=2k+1$.\n$g_{2k+1} = x_{2k+1} + \\Delta(-1)^{2k+1} = -\\eta - \\Delta = -(\\eta+\\Delta)$.\n$x_{2k+2} = x_{2k+1} - \\eta \\cdot \\text{sgn}(g_{2k+1}) = -\\eta - \\eta \\cdot \\text{sgn}(-(\\eta+\\Delta)) = -\\eta - \\eta(-1) = 0$.\nThus, $x_{t}$ is $0$ for all even $t0$.\n\nStep 2: Compute $x_{2k+3}$ from $x_{2k+2}$.\nThe iterate is $x_{2k+2} = 0$. The current time step is $t=2k+2$.\n$g_{2k+2} = x_{2k+2} + \\Delta(-1)^{2k+2} = 0 + \\Delta = \\Delta$.\n$x_{2k+3} = x_{2k+2} - \\eta \\cdot \\text{sgn}(g_{2k+2}) = 0 - \\eta \\cdot \\text{sgn}(\\Delta) = 0 - \\eta(1) = -\\eta$.\nThus, $x_{t}$ is $-\\eta$ for all odd $t$.\n\nThe induction holds. The exact iterate sequence is given by:\n$$x_{t} = \\begin{cases} 0  \\text{if } t \\text{ is even} \\\\ -\\eta  \\text{if } t \\text{ is odd} \\end{cases}$$\nThe sequence $\\{x_t\\}$ oscillates between $0$ and $-\\eta$, and therefore fails to converge to the true minimum at $x=0$. This oscillating sequence serves as the requested counterexample demonstrating lack of convergence.\n\nThe mechanism for this failure is rooted in the choice of $\\rho=0$. This parameter setting completely removes the \"memory\" of the RMSprop algorithm. The scaling factor for the learning rate, $1/\\sqrt{v_t}$, is supposed to be based on a moving average of recent squared gradients. A proper average ($\\rho \\approx 1$) would smooth out gradient variations. With $\\rho=0$, $v_t$ is simply $g_t^2$, so the adaptive scaling becomes $1/|g_t|$. This normalization converts the update into a step of constant magnitude $\\eta$, with the direction determined purely by the sign of the current noisy gradient $g_t$.\n\nNear the optimum $x=0$, the true gradient $\\nabla f(x_t) = x_t$ becomes very small. Consequently, the noisy gradient $g_t = x_t + \\Delta(-1)^t$ is dominated by the noise term $\\delta_t = \\Delta(-1)^t$. The update direction, $-\\text{sgn}(g_t)$, is therefore dictated by the alternating sign of the noise, not the direction to the minimum.\nAt even steps, $x_t=0$, the true gradient is $0$, but the noise $\\delta_t = \\Delta$ pushes the iterate to $x_{t+1}=-\\eta$.\nAt odd steps, $x_t=-\\eta$, the true gradient is $-\\eta$, but the noise $\\delta_t = -\\Delta$ combines with it to give $g_t=-(\\eta+\\Delta)$, which pushes the iterate back to $x_{t+1}=0$. The algorithm perpetually takes constant-sized steps, oscillating around the minimum without ever settling.\n\nFinally, we determine the value of $\\limsup_{t \\to \\infty} |x_{t}|$.\nThe sequence of iterates is $x_0=0, x_1=-\\eta, x_2=0, x_3=-\\eta, \\ldots$.\nThe corresponding sequence of absolute values is $|x_0|=0, |x_1|=\\eta, |x_2|=0, |x_3|=\\eta, \\ldots$.\nThis sequence, $\\{|x_t|\\}_{t \\geq 0}$, is $\\{0, \\eta, 0, \\eta, \\ldots\\}$.\nThe set of subsequential limits (or limit points) of this sequence is the set of values that appear infinitely often, which is $\\{0, \\eta\\}$.\nThe limit superior of a sequence is the supremum of its set of subsequential limits.\n$$\\limsup_{t \\to \\infty} |x_{t}| = \\sup\\{0, \\eta\\}$$\nSince $\\eta  0$, the supremum is $\\eta$.\nThe condition $\\Delta  \\eta$ ensures that the sign of the gradient term $g_t$ is unambiguous at each step and prevents other potential limit cycles from emerging, thus making this oscillatory behavior the unique outcome for any starting condition within a certain range, including $x_0=0$.",
            "answer": "$$\\boxed{\\eta}$$"
        },
        {
            "introduction": "This final practice moves from theoretical analysis to hands-on implementation, tackling a subtle but critical challenge in optimization: gradient bias. You will implement and compare the standard RMSprop algorithm against two \"bias-aware\" variants that more accurately estimate the gradient's true variance. This coding exercise not only reveals the shortcomings of the basic algorithm but also illuminates the principled design choices that lead to more advanced optimizers like Adam. ",
            "id": "3170932",
            "problem": "Consider a one-dimensional stochastic optimization problem in which a learner seeks to minimize a convex quadratic objective with an imperfect gradient oracle. Let the underlying objective be $f(x)$ that is strongly convex and differentiable, and suppose the learner uses Stochastic Gradient Descent (SGD) with the canonical update $x_{t+1} = x_t - \\alpha \\, g_t$, where $g_t$ is a stochastic gradient, $\\alpha$ is a positive learning rate, and $t$ indexes discrete time. The stochastic gradient is modeled as $g_t = h(x_t) + b + \\xi_t$, where $h(x_t)$ is the true gradient of the objective, $b$ is a constant bias term reflecting a non-zero expectation from the gradient oracle (for example due to measurement drift), and $\\xi_t$ is zero-mean noise with finite variance. The expectation operator is denoted by $\\mathbb{E}[\\cdot]$.\n\nRoot Mean Square Propagation (RMSprop) is an adaptive method that scales the SGD update by a running estimate of recent gradient magnitude, constructed via an Exponentially Weighted Moving Average (EWMA). An EWMA of a sequence $\\{s_t\\}$ with decay parameter $\\beta \\in (0,1)$ is a recursive estimator based on the foundational definition of exponential weighting that places geometrically decaying weights on past values. The uncentered EWMA of the squared gradient tracks the second moment $\\mathbb{E}[g_t^2]$, not the variance. When $\\mathbb{E}[g_t] \\neq 0$, the second moment equals the variance plus the squared mean, which can cause the running estimate to be inflated by bias. A principled alternative is to use a centered EWMA (i.e., subtract a running estimate of the mean), thereby approximating the variance $\\operatorname{Var}(g_t) = \\mathbb{E}[g_t^2] - (\\mathbb{E}[g_t])^2$. Another intervention is a detrended update that subtracts the running mean directly from the gradient before updating parameters.\n\nYour task is to, starting strictly from the above definitions of SGD and EWMA, derive and implement three algorithms:\n- Uncentered RMSprop: scale the SGD step using an EWMA of $g_t^2$.\n- Centered RMSprop: estimate $\\operatorname{Var}(g_t)$ by combining EWMAs of $g_t$ and $g_t^2$, and use this estimate to scale the step.\n- Detrended RMSprop: subtract a running estimate of $\\mathbb{E}[g_t]$ from $g_t$ before both squaring and updating.\n\nThen, simulate their behavior on a controlled class of one-dimensional stochastic gradients corresponding to a quadratic objective. Specifically, let the true objective be $f(x) = \\tfrac{1}{2} a (x - x^\\star)^2$ with gradient $h(x) = a (x - x^\\star)$, where $a  0$ and $x^\\star$ is the unique minimizer. The gradient oracle provides $g_t = a (x_t - x^\\star) + b + \\xi_t$, where $b$ is a fixed bias and $\\xi_t \\sim \\mathcal{N}(0, \\sigma^2)$ is independent noise. For scientific realism, ensure the same noise realization $\\{\\xi_t\\}_{t=1}^T$ is used across all three algorithms within each test case so that differences arise solely from the update rules.\n\nImplement all three algorithms with the same hyperparameters per test case: learning rate $\\alpha$, EWMA decay $\\beta$, additive stabilizer $\\varepsilon$ in the denominator, and number of steps $T$. Initialize the parameter at $x_0$ and all EWMAs at zero. After $T$ steps, report the absolute error $|x_T - x^\\star|$.\n\nUse the following test suite of parameter values, which is chosen to probe distinct facets:\n- Test case $1$ (happy path with moderate bias and noise): $a = 1.5$, $x^\\star = 2.0$, $x_0 = -5.0$, $b = 0.6$, $\\sigma = 0.2$, $\\alpha = 0.01$, $\\beta = 0.99$, $\\varepsilon = 10^{-8}$, $T = 1000$, and a fixed random seed of $42$ for the noise sequence.\n- Test case $2$ (boundary, no noise, no bias): $a = 1.0$, $x^\\star = -3.0$, $x_0 = 10.0$, $b = 0.0$, $\\sigma = 0.0$, $\\alpha = 0.01$, $\\beta = 0.99$, $\\varepsilon = 10^{-8}$, $T = 500$, and a fixed random seed of $123$.\n- Test case $3$ (significant edge case with high noise and non-zero bias): $a = 0.8$, $x^\\star = 1.0$, $x_0 = -1.0$, $b = 0.5$, $\\sigma = 2.0$, $\\alpha = 0.01$, $\\beta = 0.99$, $\\varepsilon = 10^{-8}$, $T = 1500$, and a fixed random seed of $999$.\n- Test case $4$ (near-optimum start with small noise and non-zero bias): $a = 2.5$, $x^\\star = 0.0$, $x_0 = 0.1$, $b = 0.2$, $\\sigma = 0.05$, $\\alpha = 0.01$, $\\beta = 0.99$, $\\varepsilon = 10^{-8}$, $T = 800$, and a fixed random seed of $777$.\n\nFor each test case, compute three floats: the final absolute error $|x_T - x^\\star|$ for Uncentered RMSprop, Centered RMSprop, and Detrended RMSprop, in that order.\n\nYour program should produce a single line of output containing the $12$ results as a comma-separated list enclosed in square brackets, ordered by test case, each contributing three numbers in the method order given above. For example, the output format must be exactly like $[r_{1, \\text{unc}}, r_{1, \\text{cent}}, r_{1, \\text{det}}, r_{2, \\text{unc}}, r_{2, \\text{cent}}, r_{2, \\text{det}}, r_{3, \\text{unc}}, r_{3, \\text{cent}}, r_{3, \\text{det}}, r_{4, \\text{unc}}, r_{4, \\text{cent}}, r_{4, \\text{det}}]$, where each $r_{\\cdot}$ is a float.",
            "solution": "The task is to derive and implement three variants of the Root Mean Square Propagation (RMSprop) algorithm based on a provided formal description, and then to simulate their performance on a specific one-dimensional optimization problem. The three variants are Uncentered RMSprop, Centered RMSprop, and Detrended RMSprop. The simulation will be performed on a quadratic objective function $f(x) = \\frac{1}{2} a (x - x^\\star)^2$ with a biased, noisy gradient oracle.\n\nFirst, we establish the common mathematical framework. The stochastic gradient at time step $t$ is given by\n$$g_t = a(x_t - x^\\star) + b + \\xi_t$$\nwhere $x_t$ is the parameter value, $a$ is the curvature, $x^\\star$ is the minimizer, $b$ is a constant bias, and $\\xi_t$ is a zero-mean noise term with variance $\\sigma^2$.\n\nThe core of RMSprop and its variants is the use of an Exponentially Weighted Moving Average (EWMA) to estimate statistical properties of the gradients. The recursive formula for an EWMA of a sequence $\\{s_t\\}$ with a decay parameter $\\beta \\in (0,1)$ is:\n$$E_t[s] = \\beta E_{t-1}[s] + (1-\\beta)s_t$$\nwhere $E_t[s]$ is the moving average at time $t$. All EWMA accumulators are initialized to $0$.\n\nThe general structure of the parameter update for these adaptive methods is:\n$$x_{t+1} = x_t - \\alpha \\cdot \\text{scaled\\_gradient}_t$$\nwhere $\\alpha$ is the learning rate. We will now derive the specific update rules for each of the three algorithms. The simulations run for $t = 0, 1, \\dots, T-1$, starting from a given $x_0$.\n\n**1. Uncentered RMSprop Derivation**\n\nThis is the standard RMSprop algorithm. It scales the learning rate by the root mean square of the gradients. The \"uncentered\" aspect refers to using the raw second moment of the gradient, $\\mathbb{E}[g_t^2]$, as the basis for the scaling factor, which is estimated via an EWMA of $g_t^2$.\n\nLet $v_t$ be the EWMA of the squared gradients, $g_t^2$.\n$$v_t = \\beta v_{t-1} + (1-\\beta) g_t^2$$\nThe update rule for the parameter $x$ uses the stochastic gradient $g_t$ scaled by the inverse of the square root of this moving average. An additive stabilizer $\\varepsilon  0$ is included in the denominator to prevent division by zero.\n$$x_{t+1} = x_t - \\frac{\\alpha}{\\sqrt{v_t} + \\varepsilon} g_t$$\nThis algorithm is simple but can be suboptimal when the mean of the gradient, $\\mathbb{E}[g_t]$, is non-zero (i.e., when bias $b$ is present or the gradient is consistently non-zero away from the optimum). In such cases, $v_t$ estimates $\\mathbb{E}[g_t^2] = \\operatorname{Var}(g_t) + (\\mathbb{E}[g_t])^2$, causing the running estimate to be inflated by the squared mean, potentially leading to an overly conservative step size.\n\n**2. Centered RMSprop Derivation**\n\nThis variant aims to correct the issue of the uncentered version by using an estimate of the variance of the gradient, $\\operatorname{Var}(g_t)$, for scaling, rather than the second moment. The variance is given by $\\operatorname{Var}(g_t) = \\mathbb{E}[g_t^2] - (\\mathbb{E}[g_t])^2$. This requires estimating both the first moment (mean) and the second moment of the gradient.\n\nLet $m_t$ be the EWMA of the gradients, $g_t$, estimating $\\mathbb{E}[g_t]$.\n$$m_t = \\beta m_{t-1} + (1-\\beta) g_t$$\nLet $v_t$ be the EWMA of the squared gradients, $g_t^2$, estimating $\\mathbb{E}[g_t^2]$ (same as in Uncentered RMSprop).\n$$v_t = \\beta v_{t-1} + (1-\\beta) g_t^2$$\nAn estimate for the variance at step $t$ is then constructed from the current moving averages:\n$$\\hat{\\sigma}_t^2 = v_t - m_t^2$$\nThe parameter update rule uses this variance estimate for scaling. The update still uses the original stochastic gradient $g_t$ for the direction.\n$$x_{t+1} = x_t - \\frac{\\alpha}{\\sqrt{\\hat{\\sigma}_t^2} + \\varepsilon} g_t$$\nThis approach normalizes the step by a more accurate measure of the gradient's noisiness, potentially improving convergence when a significant bias exists.\n\n**3. Detrended RMSprop Derivation**\n\nThis algorithm takes a more direct approach to handling a non-zero mean gradient. It first estimates the mean of the gradient and then subtracts this \"trend\" from the raw stochastic gradient. This \"detrended\" gradient is then used for both the parameter update and the calculation of the adaptive scaling factor.\n\nFirst, we compute the EWMA of the gradient, $m_t$, just as in Centered RMSprop.\n$$m_t = \\beta m_{t-1} + (1-\\beta) g_t$$\nNext, we form the detrended gradient, $\\tilde{g}_t$, by subtracting the current running mean estimate from the raw gradient.\n$$\\tilde{g}_t = g_t - m_t$$\nThe scaling factor is based on the EWMA of the squared *detrended* gradient. Let this EWMA be $v_t$.\n$$v_t = \\beta v_{t-1} + (1-\\beta) \\tilde{g}_t^2$$\nThe parameter update uses the detrended gradient $\\tilde{g}_t$ for the step direction, scaled by the adaptive rate derived from $v_t$.\n$$x_{t+1} = x_t - \\frac{\\alpha}{\\sqrt{v_t} + \\varepsilon} \\tilde{g}_t$$\nThis method attempts to correct both the magnitude and the direction of the update step by actively removing the estimated bias. This is conceptually similar to the Adam optimizer, though it omits the formal bias-correction steps for the initial iterations.\n\nThe implementation will simulate these three algorithms for each test case, using the same sequence of random noise $\\xi_t$ to ensure a fair comparison. The final absolute error $|x_T - x^\\star|$ is reported for each algorithm and test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_uncentered_rmsprop(params, noise):\n    \"\"\"\n    Simulates Uncentered RMSprop.\n    \"\"\"\n    a, x_star, x0, b, _, alpha, beta, epsilon, T = (\n        params['a'], params['x_star'], params['x0'], params['b'],\n        params['sigma'], params['alpha'], params['beta'], params['epsilon'], params['T']\n    )\n    x = float(x0)\n    v = 0.0\n\n    for t in range(T):\n        g = a * (x - x_star) + b + noise[t]\n        v = beta * v + (1 - beta) * g**2\n        x = x - (alpha / (np.sqrt(v) + epsilon)) * g\n    \n    return np.abs(x - x_star)\n\ndef run_centered_rmsprop(params, noise):\n    \"\"\"\n    Simulates Centered RMSprop.\n    \"\"\"\n    a, x_star, x0, b, _, alpha, beta, epsilon, T = (\n        params['a'], params['x_star'], params['x0'], params['b'],\n        params['sigma'], params['alpha'], params['beta'], params['epsilon'], params['T']\n    )\n    x = float(x0)\n    m = 0.0\n    v = 0.0\n\n    for t in range(T):\n        g = a * (x - x_star) + b + noise[t]\n        m = beta * m + (1 - beta) * g\n        v = beta * v + (1 - beta) * g**2\n        var_est = v - m**2\n        \n        # Use np.sqrt on var_est directly as per derivation.\n        # This may result in NaN if var_est is negative, highlighting a potential instability.\n        denominator = np.sqrt(max(0, var_est)) + epsilon\n        x = x - (alpha / denominator) * g\n\n    return np.abs(x - x_star)\n\ndef run_detrended_rmsprop(params, noise):\n    \"\"\"\n    Simulates Detrended RMSprop.\n    \"\"\"\n    a, x_star, x0, b, _, alpha, beta, epsilon, T = (\n        params['a'], params['x_star'], params['x0'], params['b'],\n        params['sigma'], params['alpha'], params['beta'], params['epsilon'], params['T']\n    )\n    x = float(x0)\n    m = 0.0\n    v = 0.0\n\n    for t in range(T):\n        g = a * (x - x_star) + b + noise[t]\n        m = beta * m + (1 - beta) * g\n        g_tilde = g - m\n        v = beta * v + (1 - beta) * g_tilde**2\n\n        denominator = np.sqrt(v) + epsilon\n        x = x - (alpha / denominator) * g_tilde\n\n    return np.abs(x - x_star)\n\ndef solve():\n    \"\"\"\n    Main function to run the simulations for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"a\": 1.5, \"x_star\": 2.0, \"x0\": -5.0, \"b\": 0.6, \"sigma\": 0.2, \n            \"alpha\": 0.01, \"beta\": 0.99, \"epsilon\": 1e-8, \"T\": 1000, \"seed\": 42\n        },\n        {\n            \"a\": 1.0, \"x_star\": -3.0, \"x0\": 10.0, \"b\": 0.0, \"sigma\": 0.0, \n            \"alpha\": 0.01, \"beta\": 0.99, \"epsilon\": 1e-8, \"T\": 500, \"seed\": 123\n        },\n        {\n            \"a\": 0.8, \"x_star\": 1.0, \"x0\": -1.0, \"b\": 0.5, \"sigma\": 2.0, \n            \"alpha\": 0.01, \"beta\": 0.99, \"epsilon\": 1e-8, \"T\": 1500, \"seed\": 999\n        },\n        {\n            \"a\": 2.5, \"x_star\": 0.0, \"x0\": 0.1, \"b\": 0.2, \"sigma\": 0.05, \n            \"alpha\": 0.01, \"beta\": 0.99, \"epsilon\": 1e-8, \"T\": 800, \"seed\": 777\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        rng = np.random.default_rng(case['seed'])\n        noise = rng.normal(0, case['sigma'], case['T'])\n        \n        # Uncentered RMSprop\n        result_unc = run_uncentered_rmsprop(case, noise)\n        all_results.append(result_unc)\n        \n        # Centered RMSprop\n        # Added a max(0, var_est) to prevent NaN from small numerical errors.\n        # This is a minor, robust change that doesn't alter the core logic.\n        # The original code's comment about potential instability is addressed.\n        result_cen = run_centered_rmsprop(case, noise)\n        all_results.append(result_cen)\n\n        # Detrended RMSprop\n        result_det = run_detrended_rmsprop(case, noise)\n        all_results.append(result_det)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}