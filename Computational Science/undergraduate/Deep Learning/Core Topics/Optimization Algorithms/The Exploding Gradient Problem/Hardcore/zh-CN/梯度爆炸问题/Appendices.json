{
    "hands_on_practices": [
        {
            "introduction": "我们将从一个高度简化的多层感知机 (MLP) 开始。通过将权重矩阵设置为缩放的单位矩阵，这个理想化的设置使我们能够清晰地推导出并观察到梯度随网络深度呈指数增长的现象。这个练习将让你亲手实现并测试梯度裁剪技术，直观地理解其如何有效地缓解梯度爆炸问题，从而为你打下坚实的基础。",
            "id": "3184988",
            "problem": "考虑一个全连接的多层感知机 (MLP)，它有 $L$ 层，每层宽度为 $d$，且不含偏置项。假设激活函数为恒等函数，并且每层的权重矩阵为 $W_\\ell = \\alpha I_d$，其中 $I_d$ 是 $d \\times d$ 的单位矩阵，$\\alpha \\in \\mathbb{R}$。设网络输入为 $x_0 \\in \\mathbb{R}^d$，网络输出为 $y \\in \\mathbb{R}^d$，目标为 $t \\in \\mathbb{R}^d$。标量损失由标准均方误差 $\\ell(y,t) = \\frac{1}{2}\\|y - t\\|_2^2$ 定义。仅使用微积分的链式法则、矩阵乘法性质和欧几里得范数定义，推导梯度 $\\nabla_{x_0} \\ell$ 的显式表达式，及其幅度相对于参数 $\\alpha$ 和 $L$ 的缩放关系。然后，实现两种梯度裁剪策略：范数裁剪和值裁剪。范数裁剪对梯度 $g \\in \\mathbb{R}^d$ 乘以因子 $\\min\\left(1, \\frac{c}{\\|g\\|_2}\\right)$，其中阈值 $c > 0$。值裁剪将每个分量 $g_i$ 限制在区间 $[-v, v]$ 内，其中阈值 $v > 0$。\n\n你的程序必须为每个提供的测试用例计算以下量：\n- 比率 $R = \\frac{\\|\\nabla_{x_0} \\ell\\|_2}{\\|\\nabla_{y} \\ell\\|_2}$。\n- 裁剪前的欧几里得范数 $\\|\\nabla_{x_0} \\ell\\|_2$。\n- 范数裁剪后梯度的欧几里得范数。\n- 值裁剪后梯度的欧几里得范数。\n\n使用双精度浮点数运算。不涉及物理单位。不涉及角度。所有返回的数值必须是浮点数。\n\n测试套件（每个元组为 $(\\alpha, L, d, x_0, t, c, v)$）：\n- 案例 1 (一般情况)：$(\\alpha, L, d) = (1.2, 8, 5)$，$x_0 = [1.0, -0.5, 0.25, 2.0, -1.5]$，$t = [0.0, 0.0, 0.0, 0.0, 0.0]$，$c = 10.0$，$v = 3.0$。\n- 案例 2 (边界情况 $\\alpha = 1$)：$(\\alpha, L, d) = (1.0, 7, 4)$，$x_0 = [0.1, -0.2, 0.3, -0.4]$，$t = [0.5, -0.5, 0.5, -0.5]$，$c = 5.0$，$v = 2.5$。\n- 案例 3 (爆炸情况)：$(\\alpha, L, d) = (2.0, 12, 4)$，$x_0 = [0.01, 0.02, -0.03, 0.04]$，$t = [0.0, 0.0, 0.0, 0.0]$，$c = 1.0$，$v = 0.5$。\n- 案例 4 (消失情况)：$(\\alpha, L, d) = (0.8, 20, 6)$，$x_0 = [-0.2, 0.1, -0.1, 0.05, -0.05, 0.025]$，$t = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]$，$c = 0.1$，$v = 0.05$。\n- 案例 5 (边界情况 $L = 0$)：$(\\alpha, L, d) = (1.7, 0, 3)$，$x_0 = [2.0, -1.0, 0.5]$，$t = [-1.0, 0.0, 1.0]$，$c = 0.5$，$v = 0.3$。\n- 案例 6 (标量边缘情况)：$(\\alpha, L, d) = (3.0, 15, 1)$，$x_0 = [0.1]$，$t = [-0.2]$，$c = 2.0$，$v = 1.0$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个元素对应一个测试用例，并且本身是一个包含四个浮点数的列表，顺序完全如下：$[R, \\|\\nabla_{x_0} \\ell\\|_2, \\|\\text{norm\\_clipped}(\\nabla_{x_0} \\ell)\\|_2, \\|\\text{value\\_clipped}(\\nabla_{x_0} \\ell)\\|_2]$。例如，最终输出应类似于 $[[r_1,n_1,cn_1,vn_1],[r_2,n_2,cn_2,vn_2],\\dots]$。",
            "solution": "我们从第一性原理出发，并将自己限制在微积分的链式法则、矩阵乘法的基本性质和欧几里得范数的范围内。该多层感知机 (MLP) 有 $L$ 层，每层宽度为 $d$，每个权重矩阵为 $W_\\ell = \\alpha I_d$，并采用恒等激活函数。前向映射是线性映射的复合。用 $x_0 \\in \\mathbb{R}^d$ 表示输入，用 $y \\in \\mathbb{R}^d$ 表示输出。因为激活函数是恒等函数，所以第 $\\ell$ 层的逐层变换是 $x_\\ell = W_\\ell x_{\\ell-1}$。由于对所有 $\\ell$ 都有 $W_\\ell = \\alpha I_d$，我们得到\n$$\nx_\\ell = \\alpha I_d x_{\\ell-1} = \\alpha x_{\\ell-1}.\n$$\n通过对 $L$ 层进行归纳，\n$$\ny = x_L = \\alpha x_{L-1} = \\alpha^2 x_{L-2} = \\cdots = \\alpha^L x_0.\n$$\n损失是均方误差\n$$\n\\ell(y,t) = \\frac{1}{2} \\|y - t\\|_2^2,\n$$\n其关于 $y$ 的梯度是\n$$\n\\nabla_y \\ell = y - t.\n$$\n使用链式法则，关于输入 $x_0$ 的梯度由下式给出\n$$\n\\nabla_{x_0} \\ell = J^\\top \\nabla_y \\ell,\n$$\n其中 $J = \\frac{\\partial y}{\\partial x_0}$ 是前向映射的雅可比矩阵。从 $y = \\alpha^L x_0$ 可知，雅可比矩阵为\n$$\nJ = \\alpha^L I_d.\n$$\n因此，\n$$\n\\nabla_{x_0} \\ell = (\\alpha^L I_d)^\\top (y - t) = \\alpha^L (y - t).\n$$\n因此，欧几里得范数的缩放关系为\n$$\n\\|\\nabla_{x_0} \\ell\\|_2 = |\\alpha|^L \\|y - t\\|_2.\n$$\n由此可得，比率\n$$\nR = \\frac{\\|\\nabla_{x_0} \\ell\\|_2}{\\|\\nabla_y \\ell\\|_2} = \\frac{|\\alpha|^L \\|y - t\\|_2}{\\|y - t\\|_2} = |\\alpha|^L.\n$$\n此推导表明，对于 $|\\alpha| > 1$ 和足够大的 $L$，梯度幅度随 $L$ 呈指数增长，这就是梯度爆炸现象。相反，对于 $|\\alpha| < 1$，梯度幅度会衰减，表现为梯度消失。对于边界情况 $|\\alpha| = 1$ 或 $L = 0$，比率为 $R = 1$。\n\n现在我们定义两种裁剪策略来缓解梯度爆炸：\n- 范数裁剪：对于梯度向量 $g \\in \\mathbb{R}^d$ 和阈值 $c > 0$，定义\n$$\ng_{\\text{norm-clip}} = g \\cdot \\min\\left(1, \\frac{c}{\\|g\\|_2}\\right).\n$$\n当 $\\|g\\|_2 \\le c$ 时，它保持 $g$ 不变；当 $\\|g\\|_2 > c$ 时，它将 $g$ 按比例缩小，使得 $\\|g_{\\text{norm-clip}}\\|_2 = c$。\n- 值裁剪：对于阈值 $v > 0$，定义\n$$\n(g_{\\text{value-clip}})_i = \\max(-v, \\min(g_i, v)) \\quad \\text{对于每个分量 } i.\n$$\n这会将每个条目限制在区间 $[-v, v]$ 内，可能会减小范数并限制每个分量的幅度。\n\n每个测试用例的算法计划：\n1. 解析 $(\\alpha, L, d, x_0, t, c, v)$。\n2. 计算前向输出 $y = \\alpha^L x_0$。\n3. 计算 $\\nabla_y \\ell = y - t$。\n4. 计算 $\\nabla_{x_0} \\ell = \\alpha^L (y - t)$。\n5. 计算 $R = \\frac{\\|\\nabla_{x_0} \\ell\\|_2}{\\|\\nabla_y \\ell\\|_2}$。\n6. 计算未裁剪的范数 $\\|\\nabla_{x_0} \\ell\\|_2$。\n7. 应用范数裁剪得到 $g_{\\text{norm-clip}}$ 并计算其范数。\n8. 应用值裁剪得到 $g_{\\text{value-clip}}$ 并计算其范数。\n9. 返回列表 $[R, \\|\\nabla_{x_0} \\ell\\|_2, \\|g_{\\text{norm-clip}}\\|_2, \\|g_{\\text{value-clip}}\\|_2]$。\n\n边缘情况考虑：\n- 对于 $L = 0$，$y = x_0$，因此 $J = I_d$，且 $R = 1$；梯度未被放大。\n- 对于 $|\\alpha| = 1$，$R = 1$ 而与 $L$ 无关；梯度在幅度上得以保持。\n- 对于 $d = 1$，同样的推导适用，范数与绝对值一致；值裁剪可能与范数裁剪有很大不同，因为它直接限制唯一的那个分量。\n- 数值稳定性：双精度可以处理所选的 $\\alpha$ 和 $L$ 值，这些值产生的范数高达约 $10^{14}$，在合理的浮点动态范围内。\n\n程序会汇总所有案例的结果，并以指定的单行格式打印它们。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef forward_output(alpha: float, L: int, x0: np.ndarray) -> np.ndarray:\n    \"\"\"Compute y = alpha^L * x0 for the identity-activation MLP with W_l = alpha I.\"\"\"\n    return (alpha ** L) * x0\n\ndef loss_grad_wrt_y(y: np.ndarray, t: np.ndarray) -> np.ndarray:\n    \"\"\"Gradient of 0.5 * ||y - t||^2 with respect to y is (y - t).\"\"\"\n    return y - t\n\ndef input_grad(alpha: float, L: int, grad_y: np.ndarray) -> np.ndarray:\n    \"\"\"Gradient wrt input x0 given grad wrt y and Jacobian J = alpha^L * I.\"\"\"\n    return (alpha ** L) * grad_y\n\ndef norm_clip(g: np.ndarray, c: float) -> np.ndarray:\n    \"\"\"Clip gradient by global norm threshold c.\"\"\"\n    norm = np.linalg.norm(g)\n    if norm == 0.0:\n        return g.copy()\n    scale = min(1.0, c / norm)\n    return g * scale\n\ndef value_clip(g: np.ndarray, v: float) -> np.ndarray:\n    \"\"\"Clip gradient by value threshold v (component-wise clamp).\"\"\"\n    return np.clip(g, -v, v)\n\ndef compute_case(alpha, L, d, x0_list, t_list, c, v):\n    x0 = np.array(x0_list, dtype=float)\n    t = np.array(t_list, dtype=float)\n    assert x0.shape == (d,), \"x0 must have shape (d,)\"\n    assert t.shape == (d,), \"t must have shape (d,)\"\n\n    y = forward_output(alpha, L, x0)\n    gy = loss_grad_wrt_y(y, t)\n    gx = input_grad(alpha, L, gy)\n\n    norm_gy = np.linalg.norm(gy)\n    norm_gx = np.linalg.norm(gx)\n\n    # Ratio R = ||grad_x0|| / ||grad_y||\n    R = float(norm_gx / norm_gy) if norm_gy != 0.0 else float('inf')\n\n    gx_norm_clipped = norm_clip(gx, c)\n    gx_value_clipped = value_clip(gx, v)\n\n    return [R, norm_gx, float(np.linalg.norm(gx_norm_clipped)), float(np.linalg.norm(gx_value_clipped))]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (alpha, L, d, x0, t, c, v)\n    test_cases = [\n        (1.2, 8, 5,  [1.0, -0.5, 0.25, 2.0, -1.5], [0.0, 0.0, 0.0, 0.0, 0.0], 10.0, 3.0),\n        (1.0, 7, 4,  [0.1, -0.2, 0.3, -0.4], [0.5, -0.5, 0.5, -0.5], 5.0, 2.5),\n        (2.0, 12, 4, [0.01, 0.02, -0.03, 0.04], [0.0, 0.0, 0.0, 0.0], 1.0, 0.5),\n        (0.8, 20, 6, [-0.2, 0.1, -0.1, 0.05, -0.05, 0.025], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.1, 0.05),\n        (1.7, 0, 3,  [2.0, -1.0, 0.5], [-1.0, 0.0, 1.0], 0.5, 0.3),\n        (3.0, 15, 1, [0.1], [-0.2], 2.0, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, L, d, x0, t, c, v = case\n        result = compute_case(alpha, L, d, x0, t, c, v)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Single line, comma-separated list enclosed in square brackets, each element a list of four floats.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在理解了基本原理后，我们将从简单的单位矩阵推广到更一般的情况。通过使用谱归一化技术，我们可以精确地控制权重矩阵的最大奇异值（即谱范数），这是控制梯度增长的关键。这项计算实验将让你通过经验验证谱半径与梯度增长之间的理论关系，加深对梯度动力学的理解。",
            "id": "3185019",
            "problem": "您需要实现一个受控计算实验，使用带有谱归一化的深度线性模型来研究深度学习中的梯度爆炸现象。该实验必须基于第一性原理，特别是复合函数的微分链式法则、算子范数的定义以及范数的次可乘性。请从以下基础出发：向量微积分中的复合函数链式法则、矩阵算子 2-范数的定义 $\\|W\\|_{2} = \\max_{\\|x\\|_{2}=1} \\|W x\\|_{2}$，以及次可乘性 $\\|A B\\|_{2} \\le \\|A\\|_{2}\\|B\\|_{2}$。除这些基础知识外，不要假设任何更高级的公式。\n\n您将构建一个深度为 $L$、使用权重共享和线性损失的深度线性网络。具体来说，考虑一个输入向量 $x \\in \\mathbb{R}^{d}$，一个在所有层之间共享的方形权重矩阵 $W \\in \\mathbb{R}^{d \\times d}$，以及一个输出 $y = W^{L} x$。使用线性标量损失 $L_{\\text{loss}} = u^{\\top} y$，其中 $u \\in \\mathbb{R}^{d}$ 是一个固定的读出向量。我们感兴趣的梯度是 $L_{\\text{loss}}$ 关于输入 $x$ 的梯度。您的任务是，当权重矩阵经过谱归一化使其算子范数 $\\|W\\|_{2} = \\alpha$ 时，经验性地估计范数 $\\|\\nabla_{x} L_{\\text{loss}}\\|_{2}$ 如何作为目标谱半径 $\\alpha$ 的函数进行缩放。\n\n实现以下受控设置：\n- 使用由 $W = \\alpha \\,\\bar{W}/\\|\\bar{W}\\|_{2}$ 定义的谱归一化，其中 $\\bar{W} \\in \\mathbb{R}^{d \\times d}$ 是一个半正定矩阵，由 $\\bar{W} = A A^{\\top}$ 构建，而 $A \\in \\mathbb{R}^{d \\times d}$ 是一个具有独立标准正态分布元素的随机矩阵。这保证了 $\\bar{W}$ 是对称半正定的，并且 $\\|W\\|_{2} = \\alpha$。\n- 选择读出向量 $u$ 为 $\\bar{W}$（等价于 $W$）的单位范数主特征向量，以确保在重复应用下增长的最坏情况对齐。\n- 使用固定的输入维度 $d = 8$ 和固定的随机种子 $s = 12345$ 来构造 $A$，以使结果是确定性的。\n- 对于每个深度 $L$，在一组固定的正值上改变 $\\alpha$，并为每个 $\\alpha$ 测量 $\\|\\nabla_{x} L_{\\text{loss}}\\|_{2}$。然后，通过在对数空间中进行线性回归，拟合一个形式为 $\\|\\nabla_{x} L_{\\text{loss}}\\|_{2} \\approx C \\,\\alpha^{p}$ 的模型，以估计该深度的指数 $p$。\n\n您的程序必须实现以下测试套件和估计协议：\n- 维度和种子：$d = 8$, $s = 12345$。\n- 深度：$L \\in \\{1, 3, 5, 7\\}$。\n- 目标谱半径：$\\alpha \\in \\{0.25, 0.5, 1.0, 1.2, 1.5\\}$。\n- 对于每个固定的 $L$，使用上述带有权重共享和线性损失的构造，在每个 $\\alpha$ 处计算 $\\|\\nabla_{x} L_{\\text{loss}}\\|_{2}$，然后通过使用普通最小二乘法拟合 $\\log \\|\\nabla_{x} L_{\\text{loss}}\\|_{2}$ 与 $\\log \\alpha$ 的关系来估计指数 $p$ 并获得斜率。请使用自然对数。\n- 因为所有的 $\\alpha$ 值都是无量纲的，且没有物理单位，所以不需要进行单位转换。不涉及角度。\n\n您的程序应该生成单行输出，其中包含按上述深度顺序列出的每个深度的拟合指数，形式为方括号内以逗号分隔的列表（例如，`[result1,result2,result3,result4]`）。结果必须是浮点数。\n\n约束和说明：\n- 您必须通过显式构建梯度 $\\nabla_{x} L_{\\text{loss}} = (W^{\\top})^{L} u$ 并计算其欧几里得范数，来为所构建的线性网络精确计算 $\\|\\nabla_{x} L_{\\text{loss}}\\|_{2}$。不要使用自动微分库。\n- 您必须使用指定的谱归一化过程 $W = \\alpha \\,\\bar{W}/\\|\\bar{W}\\|_{2}$，并且必须使用归一化为单位长度的 $\\bar{W}$ 的主特征向量作为 $u$。\n- 最终程序不得要求任何输入，并且必须使用固定的种子 $s$ 以保证可复现性。",
            "solution": "问题陈述已经过验证，并被确定是合理的。其科学基础在于应用于深度学习的线性代数和向量微积分的原理，问题设定良好，具有一套完整且一致的参数，并且其表述是客观的。该任务是一个精心设计的计算实验，用于在受控环境中经验性地研究梯度爆炸问题。\n\n该实验旨在揭示权重矩阵的谱半径与关于输入的梯度大小之间的关系。我们将首先从第一性原理推导理论期望，然后描述计算验证的实现方法。\n\n### 理论基础\n\n1.  **模型与梯度计算**：\n    深度线性网络由函数 $f(x) = y = W^L x$ 定义，其中 $W \\in \\mathbb{R}^{d \\times d}$ 是在 $L$ 层之间共享的权重矩阵，$x \\in \\mathbb{R}^d$ 是输入。损失由输出的线性函数给出：$L_{\\text{loss}} = u^{\\top} y = u^{\\top} W^L x$。该损失是向量 $x$ 的标量函数。损失关于输入 $\\nabla_x L_{\\text{loss}}$ 的梯度可以使用链式法则求得。认识到 $L_{\\text{loss}}$ 的形式为 $c^{\\top}x$，其中向量 $c = (W^L)^{\\top} u$，则梯度就是 $\\nabla_x L_{\\text{loss}} = c$。\n    因此，梯度由下式给出：\n    $$\n    \\nabla_x L_{\\text{loss}} = (W^L)^{\\top} u = (W^{\\top})^L u\n    $$\n\n2.  **权重矩阵 $W$ 的性质**：\n    权重矩阵 $W$ 通过谱归一化构建。首先，从一个元素服从标准正态分布的随机矩阵 $A$ 创建一个矩阵 $\\bar{W} = A A^{\\top}$。\n    *   $\\bar{W}$ 是对称的，因为 $(\\bar{W})^{\\top} = (A A^{\\top})^{\\top} = (A^{\\top})^{\\top} A^{\\top} = A A^{\\top} = \\bar{W}$。\n    *   $\\bar{W}$ 是半正定的，因为对于任何向量 $v \\in \\mathbb{R}^d$，都有 $v^{\\top}\\bar{W}v = v^{\\top}A A^{\\top}v = (A^{\\top}v)^{\\top}(A^{\\top}v) = \\|A^{\\top}v\\|_2^2 \\ge 0$。\n    最终的权重矩阵是 $W = \\alpha \\frac{\\bar{W}}{\\|\\bar{W}\\|_2}$，其中 $\\alpha > 0$ 是目标谱半径（算子 2-范数）。由于 $\\bar{W}$ 是对称的，因此 $W$ 也是对称的，这意味着 $W^{\\top} = W$。这将梯度表达式简化为：\n    $$\n    \\nabla_x L_{\\text{loss}} = W^L u\n    $$\n    $W$ 的算子 2-范数是：\n    $$\n    \\|W\\|_2 = \\left\\| \\alpha \\frac{\\bar{W}}{\\|\\bar{W}\\|_2} \\right\\|_2 = \\alpha \\frac{\\|\\bar{W}\\|_2}{\\|\\bar{W}\\|_2} = \\alpha\n    $$\n    这证实了该构造正确地将 $W$ 的谱半径设置为 $\\alpha$。\n\n3.  **通过特征向量选择实现最坏情况对齐**：\n    问题指定读出向量 $u$ 必须是 $\\bar{W}$ 的单位范数主特征向量。由于 $\\bar{W}$ 是对称半正定矩阵，其算子 2-范数 $\\|\\bar{W}\\|_2$ 等于其最大特征值 $\\lambda_{\\text{max}}(\\bar{W})$。主特征向量 $u$ 是与该特征值对应的特征向量，因此 $\\bar{W}u = \\lambda_{\\text{max}}(\\bar{W})u = \\|\\bar{W}\\|_2 u$。\n\n    $u$ 的这种选择也使其成为 $W$ 的主特征向量：\n    $$\n    Wu = \\left( \\alpha \\frac{\\bar{W}}{\\|\\bar{W}\\|_2} \\right) u = \\frac{\\alpha}{\\|\\bar{W}\\|_2} (\\bar{W}u) = \\frac{\\alpha}{\\|\\bar{W}\\|_2} (\\|\\bar{W}\\|_2 u) = \\alpha u\n    $$\n    因此，$u$ 是 $W$ 的一个特征值为 $\\alpha$ 的特征向量。\n\n4.  **梯度范数缩放**：\n    我们现在可以计算将 $W$ 重复应用于 $u$ 的结果：\n    $$\n    W^L u = W^{L-1}(Wu) = W^{L-1}(\\alpha u) = \\alpha W^{L-1}u = \\dots = \\alpha^L u\n    $$\n    那么梯度的范数是：\n    $$\n    \\|\\nabla_x L_{\\text{loss}}\\|_2 = \\|W^L u\\|_2 = \\|\\alpha^L u\\|_2 = |\\alpha|^L \\|u\\|_2\n    $$\n    由于 $\\alpha > 0$ 且 $u$ 是一个单位范数向量（$\\|u\\|_2 = 1$），该表达式简化为一个精确的等式：\n    $$\n    \\|\\nabla_x L_{\\text{loss}}\\|_2 = \\alpha^L\n    $$\n    这个理论结果预测，梯度的范数随谱半径 $\\alpha$ 的 $L$ 次方进行缩放。\n\n5.  **指数的经验性估计**：\n    该实验旨在将测量数据拟合到模型 $\\|\\nabla_x L_{\\text{loss}}\\|_2 \\approx C \\alpha^p$。对两边取自然对数，我们得到一个线性关系：\n    $$\n    \\ln(\\|\\nabla_x L_{\\text{loss}}\\|_2) \\approx \\ln(C) + p \\ln(\\alpha)\n    $$\n    这对应于一条斜率为 $p$、截距为 $\\ln(C)$ 的直线。根据我们的理论推导，我们期望数据能完美拟合该模型，其中 $p = L$ 且 $C=1$（即 $\\ln(C)=0$）。程序将对每个深度 $L$ 的数据对 $(\\ln(\\alpha), \\ln(\\|\\nabla_x L_{\\text{loss}}\\|_2))$ 进行线性回归，以估计斜率 $p$。\n\n### 算法设计\n\n该 Python 脚本按如下方式实现所述实验：\n1.  **初始化**：设置固定参数 $d=8$、随机种子 $s=12345$，以及待测试的深度 $L$ 和谱半径 $\\alpha$ 列表。\n2.  **矩阵生成**：使用 NumPy 库和指定的种子生成一个随机矩阵 $A$，并构造对称半正定矩阵 $\\bar{W} = AA^{\\top}$。\n3.  **特征分解**：使用 `numpy.linalg.eigh` 计算 $\\bar{W}$ 的特征值和特征向量，该函数针对对称矩阵进行了优化。最大特征值即为 $\\|\\bar{W}\\|_2$，其对应的特征向量即为所需的读出向量 $u$。\n4.  **迭代实验**：\n    - 遍历每个深度 $L \\in \\{1, 3, 5, 7\\}$。\n    - 在此循环内，遍历每个谱半径 $\\alpha \\in \\{0.25, 0.5, 1.0, 1.2, 1.5\\}$。\n    - 对于每一对 $(L, \\alpha)$：\n        a. 构造权重矩阵 $W = \\alpha \\frac{\\bar{W}}{\\|\\bar{W}\\|_2}$。\n        b. 通过将 $W$ 迭代应用于 $u$ 共 $L$ 次，计算梯度向量 $g = W^L u$。\n        c. 计算欧几里得范数 $\\|g\\|_2$。\n        d. 存储对数转换后的值 $\\ln(\\alpha)$ 和 $\\ln(\\|g\\|_2)$。\n5.  **指数估计**：在固定深度 $L$ 下收集完所有 $\\alpha$ 值的数据后，使用 `scipy.stats.linregress` 对收集到的对数转换数据执行普通最小二乘法。所得回归线的斜率即为估计的指数 $p$。\n6.  **输出**：收集每个深度 $L$ 的估计指数，并以指定格式 `[p1,p3,p5,p7]` 打印。预期结果将非常接近理论值 $[1.0, 3.0, 5.0, 7.0]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef solve():\n    \"\"\"\n    Implements a controlled experiment to investigate the exploding gradient\n    phenomenon in a deep linear network, grounded in first principles.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    d = 8\n    s = 12345\n    depths = [1, 3, 5, 7]\n    alphas = [0.25, 0.5, 1.0, 1.2, 1.5]\n\n    # Set the random seed for deterministic and reproducible results.\n    np.random.seed(s)\n\n    # Construct the positive semidefinite matrix W_bar = A * A^T.\n    # This matrix is guaranteed to be symmetric positive semidefinite.\n    A = np.random.randn(d, d)\n    W_bar = A @ A.T\n\n    # Compute eigenvalues and eigenvectors of the symmetric matrix W_bar.\n    # np.linalg.eigh is used for Hermitian/symmetric matrices. It returns\n    # eigenvalues in ascending order and corresponding orthonormal eigenvectors.\n    eigenvalues, eigenvectors = np.linalg.eigh(W_bar)\n\n    # The operator 2-norm of a symmetric PSD matrix is its largest eigenvalue.\n    norm_W_bar = eigenvalues[-1]\n\n    # The readout vector u is the unit-norm dominant eigenvector of W_bar.\n    # This corresponds to the eigenvector for the largest eigenvalue.\n    u = eigenvectors[:, -1]\n\n    results = []\n    # Iterate over each specified depth L.\n    for L in depths:\n        log_alphas = []\n        log_grad_norms = []\n\n        # For each depth, iterate over the range of target spectral radii alpha.\n        for alpha in alphas:\n            # Construct the weight matrix W with spectral radius alpha.\n            # W = alpha * W_bar / ||W_bar||_2\n            W = alpha * W_bar / norm_W_bar\n\n            # Since W is symmetric, the gradient grad(L_loss) w.r.t. x is W^L * u.\n            # We compute this iteratively for numerical stability and efficiency.\n            grad = u\n            for _ in range(L):\n                grad = W @ grad\n\n            # Compute the L2 norm of the resulting gradient vector.\n            grad_norm = np.linalg.norm(grad)\n\n            # Store the natural logarithms for regression analysis.\n            # We fit log(grad_norm) = p * log(alpha) + log(C).\n            log_alphas.append(np.log(alpha))\n            log_grad_norms.append(np.log(grad_norm))\n\n        # Perform ordinary least squares regression in log-space.\n        # The slope of the regression is the desired exponent p.\n        regression_result = linregress(x=log_alphas, y=log_grad_norms)\n        exponent_p = regression_result.slope\n        results.append(exponent_p)\n    \n    # As per the problem, the theoretical expectation is that p = L.\n    # The printed results should be very close to [1.0, 3.0, 5.0, 7.0].\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "最后，我们将把所学知识应用于一个现代的复杂架构——Transformer模型中。你将发现，一个看似微小的细节——对注意力得分（logits）进行缩放——实际上是防止自注意力层梯度爆炸的关键机制。这个练习展示了梯度稳定性原理在设计前沿模型时的直接现实意义。",
            "id": "3185054",
            "problem": "要求您编写一个完整的、可运行的程序，通过一个因子 $c$ 缩放注意力 logits，来演示一个玩具多层自注意力网络中的梯度爆炸现象。目标是针对几种测试配置，计算出最小的缩放因子 $c^{\\ast}$，当超过该值时，相对于输入的梯度欧几里得范数会根据预定义的相对于基线的乘法阈值急剧上升。\n\n基本基础和前向定义。仅使用微积分的链式法则和 softmax 函数的标准定义。考虑一个单头自注意力块，以共享权重的方式顺序应用 $L$ 次，以隔离缩放因子 $c$ 的作用。设序列长度为 $T=4$，模型维度为 $d=6$（因此键的维度为 $d_k=d$）。对于一个输入矩阵 $X \\in \\mathbb{R}^{T \\times d}$，单个自注意力块计算如下：\n- $Q = X W_Q$, $K = X W_K$, $V = X W_V$，其中 $W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}$，\n- logits $M = \\frac{Q K^{\\top}}{\\sqrt{d_k}}$，缩放后的 logits $L = c \\, M$，\n- 逐行 softmax $A = \\mathrm{softmax}(L)$，\n- 预输出 $P = A V$，以及输出 $Y = P W_O$，其中 $W_O \\in \\mathbb{R}^{d \\times d}$。\n\n将 $L$ 个这样的块以共享权重 $W_Q, W_K, W_V, W_O$ 串联堆叠；也就是说，$X_{0}$ 是输入，对于 $\\ell \\in \\{0,\\dots,L-1\\}$，$X_{\\ell+1} = \\mathrm{Attn}(X_{\\ell}; c)$，其中 $\\mathrm{Attn}(\\cdot;c)$ 表示上面定义的映射。定义标量损失\n$$\n\\mathcal{L} = \\tfrac{1}{2} \\cdot \\mathrm{mean}\\left( X_{L} \\odot X_{L} \\right),\n$$\n即最终层输出 $X_{L}$ 的元素平方的均值的一半。您的程序必须通过反向传播，使用链式法则和 softmax 的雅可比矩阵，对不同的 $c$ 值精确计算欧几里得范数 $\\lVert \\nabla_{X_0} \\mathcal{L} \\rVert_2$。\n\n权重、输入和可复现性。为确保确定性，通过采样独立的标准正态分布条目并乘以一个缩放因子 $\\sigma_w$（权重缩放）来初始化 $W_Q, W_K, W_V, W_O$，并以类似方式，用独立的标准正态分布条目乘以一个固定的输入缩放因子 $\\sigma_x = 0.1$ 来初始化输入 $X_0$。使用指定的整数随机种子来初始化权重和输入。所有层共享相同的权重。\n\n临界缩放检测。对于给定的配置，定义一个基线缩放 $c_0 = 1.0$。令 $g(c) = \\lVert \\nabla_{X_0} \\mathcal{L} \\rVert_2$ 为在缩放 $c$ 下计算的值。给定一个阈值乘数 $m > 1$，将临界缩放 $c^{\\ast}$ 定义为在 $[c_{\\min}, c_{\\max}]$ 均匀网格上以步长 $c_{\\mathrm{step}}$ 的最小 $c$ 值，使得\n$$\ng(c) \\ge m \\cdot g(c_0).\n$$\n如果网格上不存在这样的 $c$，则该配置返回 $-1.0$。\n\n角度单位不适用。没有物理单位。所有数值输出必须是实数。\n\n要实现的测试套件。您的程序必须运行以下测试用例，每个用例都指定为一个元组 $(\\text{seed}, L, \\sigma_w, m, c_{\\min}, c_{\\max}, c_{\\mathrm{step}})$：\n- 测试 $1$ (正常情况): $(0, 3, 0.02, 10.0, 0.5, 10.0, 0.01)$。\n- 测试 $2$ (边界情况，可能找不到): $(1, 3, 0.02, 10.0, 0.5, 2.0, 0.01)$。\n- 测试 $3$ (更深的网络): $(2, 4, 0.02, 50.0, 0.5, 10.0, 0.01)$。\n- 测试 $4$ (特别深，权重更小): $(3, 5, 0.005, 100.0, 0.5, 10.0, 0.01)$。\n\n程序要求和输出格式。您的程序必须：\n- 使用链式法则和 softmax 雅可比矩阵实现精确的反向传播，\n- 对于每个测试用例，在 $c_0 = 1.0$ 处计算 $g(c_0)$，在指定的网格上扫描 $c$，并使用上述规则检测 $c^{\\ast}$，\n- 将所有测试用例的 $c^{\\ast}$ 值聚合到单行输出中，形式为 Python 风格的十进制数列表，四舍五入到小数点后三位，例如 $[2.154,-1.0, \\dots]$。不得有其他输出。\n\n科学真实性和指导。这种现象应通过各层敏感性因子的累积来解释。在实际的 Transformer 中，注意力 logits 通常会按 $1/\\sqrt{d_k}$ 进行缩放，以稳定激活值和梯度；在这里，$c$ 调节了该有效缩放，当 $c$ 相对于其他因素变得足够大时，可能导致梯度范数急剧增加。除了标准的浮点运算和精确的 softmax 雅可比矩阵外，您的实现不应进行任何额外的近似。不允许用户输入或外部文件。最终结果必须按上述规定打印为单行。",
            "solution": "用户提供了一个有效的问题陈述。任务是编写一个程序，系统地研究多层自注意力网络中的梯度爆炸现象。这将通过实现精确的反向传播来计算标量损失相对于输入嵌入的梯度 $\\nabla_{X_0} \\mathcal{L}$ 来实现。当应用于注意力 logits 的缩放因子 $c$ 变化时，将监测该梯度的量级，由其欧几里得范数量化。\n\n解决方案分四个主要阶段进行：(1) 定义激活值在网络中的前向传播，(2) 使用链式法则推导梯度的后向传播，(3) 构建数值实验以找到临界缩放因子 $c^*$，以及 (4) 用 Python 实现完整的算法。\n\n**1. 前向传播**\n\n该网络由 $L$ 个具有共享权重的相同自注意力块组成。第一个块的输入是 $X_0 \\in \\mathbb{R}^{T \\times d}$，其中 $T=4$ 是序列长度，$d=6$ 是模型维度。对于任何层 $\\ell \\in \\{0, \\dots, L-1\\}$，其输出 $X_{\\ell+1}$ 由输入 $X_{\\ell}$ 计算如下：\n\n-   **查询、键、值投影**：输入 $X_\\ell$ 经过线性投影，形成查询（Query, $Q$）、键（Key, $K$）和值（Value, $V$）矩阵。\n    $$Q = X_\\ell W_Q, \\quad K = X_\\ell W_K, \\quad V = X_\\ell W_V$$\n    这里，$W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d}$ 是共享的权重矩阵。\n\n-   **注意力分数和缩放**：计算点积注意力分数，按 $1/\\sqrt{d_k}$（其中 $d_k=d=6$）缩放，然后再由实验因子 $c$ 进一步缩放。\n    $$M = \\frac{Q K^\\top}{\\sqrt{d_k}}$$\n    $$L_{\\text{scaled}} = c \\cdot M$$\n\n-   **Softmax 和输出**：对缩放后的 logits 应用逐行 softmax 函数以获得注意力权重 $A$。然后使用这些权重形成值向量的加权平均。此结果由一个最终的输出权重矩阵 $W_O \\in \\mathbb{R}^{d \\times d}$ 进行投影。\n    $$A = \\mathrm{softmax}(L_{\\text{scaled}})$$\n    $$P = A V$$\n    $$X_{\\ell+1} = P W_O$$\n    这个过程重复 $L$ 次，得到最终输出 $X_L$。在整个前向传播过程中，每一层的中间值（$X_\\ell, Q, K, V, A$）都会被缓存以供反向传播使用。\n\n**2. 后向传播 (Backpropagation)**\n\n目标是计算 $\\nabla_{X_0} \\mathcal{L}$，即损失 $\\mathcal{L}$ 相对于初始输入 $X_0$ 的梯度。损失定义为：\n$$\\mathcal{L} = \\frac{1}{2} \\mathrm{mean}(X_L \\odot X_L) = \\frac{1}{2Td} \\sum_{i,j} (X_L)_{ij}^2$$\n梯度计算从损失开始，使用链式法则向后通过网络传播。\n\n-   **初始梯度**：损失相对于最终输出 $X_L$ 的梯度是：\n    $$\\nabla_{X_L} \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial X_L} = \\frac{1}{Td} X_L$$\n\n-   **单层反向传播**：我们需要在给定 $\\nabla_{X_{\\ell}} \\mathcal{L}$ 的情况下计算 $\\nabla_{X_{\\ell-1}} \\mathcal{L}$。令 $\\nabla_Z$ 表示 $\\nabla_Z \\mathcal{L}$。通过单个注意力块的操作（以相反顺序）进行反向传播的步骤如下：\n\n    1.  给定 $\\nabla_{X_\\ell}$：从 $X_\\ell = P W_O$，我们求得相对于 $P$ 的梯度：\n        $$\\nabla_P = \\nabla_{X_\\ell} W_O^\\top$$\n    2.  从 $P = A V$，我们求得相对于 $A$ 和 $V$ 的梯度：\n        $$\\nabla_A = \\nabla_P V^\\top$$\n        $$\\nabla_V = A^\\top \\nabla_P$$\n    3.  从 $A = \\mathrm{softmax}(L_{\\text{scaled}})$，我们求得 $\\nabla_{L_{\\text{scaled}}}$。逐行 softmax $S_i = \\mathrm{softmax}(\\mathbf{z}_i)$ 的雅可比矩阵的元素为 $\\frac{\\partial S_{ik}}{\\partial z_{ij}} = S_{ik}(\\delta_{kj} - S_{ij})$。对每一行 $i$ 应用链式法则得到 $(\\nabla_{\\mathbf{z}})_i = S_i \\odot (\\nabla_S)_i - S_i ((\\nabla_S)_i \\cdot S_i)$。用矩阵形式表示为：\n        $$\\nabla_{L_{\\text{scaled}}} = A \\odot (\\nabla_A - \\mathrm{sum}(\\nabla_A \\odot A, \\text{axis}=1))$$\n        其中 $\\odot$ 是逐元素乘法，求和结果跨行广播。\n    4.  从 $L_{\\text{scaled}} = c \\cdot M$：\n        $$\\nabla_M = c \\cdot \\nabla_{L_{\\text{scaled}}}$$\n    5.  从 $M = \\frac{Q K^\\top}{\\sqrt{d_k}}$：\n        $$\\nabla_Q = \\frac{1}{\\sqrt{d_k}} \\nabla_M K$$\n        $$\\nabla_K = \\frac{1}{\\sqrt{d_k}} \\nabla_M^\\top Q$$\n    6.  从 $Q=X_{\\ell-1}W_Q$, $K=X_{\\ell-1}W_K$, $V=X_{\\ell-1}W_V$，对 $X_{\\ell-1}$ 的梯度贡献求和：\n        $$\\nabla_{X_{\\ell-1}} = (\\nabla_Q W_Q^\\top) + (\\nabla_K W_K^\\top) + (\\nabla_V W_V^\\top)$$\n\n这个过程从 $\\ell=L$ 向后迭代到 $\\ell=1$，最终得到 $\\nabla_{X_0} \\mathcal{L}$。\n\n**3. 算法流程**\n\n对于每个测试用例 $(\\text{seed}, L, \\sigma_w, m, c_{\\min}, c_{\\max}, c_{\\mathrm{step}})$：\n\n1.  **初始化**：设置随机数生成器种子。根据指定的标准差缩放（$\\sigma_w$ 和 $\\sigma_x=0.1$）初始化权重矩阵 $W_Q, W_K, W_V, W_O$ 和初始输入 $X_0$。\n2.  **基线计算**：为基线缩放因子 $c_0=1.0$ 计算函数 $g(c) = \\lVert \\nabla_{X_0} \\mathcal{L} \\rVert_2$。这得到基线梯度范数 $g(c_0)$。\n3.  **阈值搜索**：建立一个阈值 $g_{\\text{thresh}} = m \\cdot g(c_0)$。然后程序遍历一个从 $c_{\\min}$ 到 $c_{\\max}$、增量为 $c_{\\mathrm{step}}$ 的均匀网格上的 $c$ 值。\n4.  **临界缩放检测**：对于网格中的每个 $c$，计算 $g(c)$。第一个满足 $g(c) \\ge g_{\\text{thresh}}$ 的 $c$ 值被确定为临界缩放 $c^*$。\n5.  **结果处理**：如果找到了这样的 $c$，则存储 $c^*$。如果循环完成而条件未满足，则将 $c^*$ 设置为 $-1.0$。\n\n对所有测试用例重复此过程，并收集得到的 $c^*$ 值。\n\n**4. 实现**\n\n该算法使用 Python 的 `numpy` 库实现所有数值和矩阵运算。单个函数 `compute_and_backprop` 封装了对于给定缩放因子 $c$ 的前向和后向传播的逻辑。主函数 `solve` 遍历测试配置，根据需要调用 `compute_and_backprop` 来为每个配置找到 $c^*$，将结果四舍五入到三位小数，并将其格式化为指定的输出字符串。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for exploding gradient detection.\n    \"\"\"\n    test_cases = [\n        # (seed, L, sigma_w, m, c_min, c_max, c_step)\n        (0, 3, 0.02, 10.0, 0.5, 10.0, 0.01),\n        (1, 3, 0.02, 10.0, 0.5, 2.0, 0.01),\n        (2, 4, 0.02, 50.0, 0.5, 10.0, 0.01),\n        (3, 5, 0.005, 100.0, 0.5, 10.0, 0.01),\n    ]\n\n    results = []\n    \n    T = 4  # Sequence length\n    d = 6  # Model dimension\n    d_k = d # Key dimension\n    sigma_x = 0.1 # Input scale\n\n    for case in test_cases:\n        seed, L, sigma_w, m, c_min, c_max, c_step = case\n\n        # Set seed for reproducibility\n        np.random.seed(seed)\n\n        # Initialize weights and input data\n        Wq = np.random.randn(d, d) * sigma_w\n        Wk = np.random.randn(d, d) * sigma_w\n        Wv = np.random.randn(d, d) * sigma_w\n        Wo = np.random.randn(d, d) * sigma_w\n        X0 = np.random.randn(T, d) * sigma_x\n\n        def get_grad_norm(c_val):\n            \"\"\"\n            Computes the norm of the gradient of the loss with respect to the input X0\n            for a given scaling factor c_val.\n            \"\"\"\n            # --- Forward Pass ---\n            cache = []\n            X_current = X0\n            sqrt_dk = np.sqrt(d_k)\n            \n            for _ in range(L):\n                Q = X_current @ Wq\n                K = X_current @ Wk\n                V = X_current @ Wv\n                \n                M = (Q @ K.T) / sqrt_dk\n                L_scaled = c_val * M\n                \n                # Numerically stable softmax\n                L_stable = L_scaled - np.max(L_scaled, axis=1, keepdims=True)\n                exp_L = np.exp(L_stable)\n                A = exp_L / np.sum(exp_L, axis=1, keepdims=True)\n                \n                P = A @ V\n                X_next = P @ Wo\n                \n                # Store intermediate values needed for backpropagation\n                cache.append((X_current, Q, K, V, A))\n                X_current = X_next\n                \n            X_L = X_current\n\n            # --- Backward Pass ---\n            # Initial gradient from the loss function\n            grad_X = (1.0 / (T * d)) * X_L\n\n            for i in range(L - 1, -1, -1):\n                X_prev, Q_i, K_i, V_i, A_i = cache[i]\n                \n                # Backprop through Wo\n                grad_P = grad_X @ Wo.T\n                \n                # Backprop through P = A @ V\n                grad_A = grad_P @ V_i.T\n                grad_V = A_i.T @ grad_P\n                \n                # Backprop through softmax(L_scaled)\n                # For each row, grad_z = s * (grad_s - sum(grad_s * s))\n                row_sum = np.sum(grad_A * A_i, axis=1, keepdims=True)\n                grad_L_scaled = A_i * (grad_A - row_sum)\n                \n                # Backprop through L_scaled = c * M\n                grad_M = c_val * grad_L_scaled\n\n                # Backprop through M = (Q @ K.T) / sqrt_dk\n                grad_Q = (grad_M @ K_i) / sqrt_dk\n                grad_K = (grad_M.T @ Q_i) / sqrt_dk\n                \n                # Backprop through Q, K, V to X_prev\n                # Sum gradients from the three paths\n                grad_X_prev = grad_Q @ Wq.T\n                grad_X_prev += grad_K @ Wk.T\n                grad_X_prev += grad_V @ Wv.T\n                \n                grad_X = grad_X_prev\n\n            grad_X0 = grad_X\n            return np.linalg.norm(grad_X0)\n\n        # Calculate baseline gradient norm\n        g_base = get_grad_norm(c_val=1.0)\n        threshold = m * g_base\n        \n        c_star = -1.0\n\n        # Create the grid for c values\n        num_steps = int(round((c_max - c_min) / c_step)) + 1\n        c_grid = np.linspace(c_min, c_max, num_steps)\n        \n        for c in c_grid:\n            g_c = get_grad_norm(c_val=c)\n            if g_c >= threshold:\n                c_star = c\n                break\n        \n        results.append(round(c_star, 3))\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}