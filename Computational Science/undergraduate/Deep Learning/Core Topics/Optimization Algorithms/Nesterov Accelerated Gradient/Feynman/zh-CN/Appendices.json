{
    "hands_on_practices": [
        {
            "introduction": "在深入研究复杂的实现之前，通过手动计算来掌握 Nesterov 加速梯度 (NAG) 的核心机制至关重要。这个练习将引导你逐步计算“前瞻”点，这是 NAG 预测能力的核心。通过这个简单的二次函数示例 ，你将清楚地理解算法如何利用当前动量来“窥探”未来，从而做出更智能的更新。",
            "id": "2187811",
            "problem": "一位工程师正在应用涅斯捷罗夫加速梯度（NAG）算法来最小化一个一维目标函数。该函数为 $f(x) = 2x^2$。该算法根据以下规则集，从初始位置 $x_0$ 和初始速度 $v_0=0$ 开始，迭代更新位置参数 $x$ 和速度项 $v$：\n$$ v_{t} = \\gamma v_{t-1} + \\eta \\nabla f(x_{t-1} - \\gamma v_{t-1}) $$\n$$ x_{t} = x_{t-1} - v_{t} $$\n在这些方程中，下标表示迭代次数，因此 $t=1, 2, 3, \\ldots$。常数 $\\gamma$ 是动量参数，$\\eta$ 是学习率，$\\nabla f$ 是函数 $f(x)$ 的梯度。梯度的计算发生在“前瞻”点，由项 $x_{t-1} - \\gamma v_{t-1}$ 给出。\n\n对于初始位置 $x_0 = 10$，动量参数 $\\gamma = 0.9$ 和学习率 $\\eta = 0.1$，计算在算法的第二次迭代（即 $t=2$ 时）使用的前瞻点的数值。",
            "solution": "我们已知涅斯捷罗夫加速梯度的更新规则：\n$$v_{t}=\\gamma v_{t-1}+\\eta \\nabla f\\!\\left(x_{t-1}-\\gamma v_{t-1}\\right), \\quad x_{t}=x_{t-1}-v_{t}.$$\n目标函数为 $f(x)=2x^{2}$，因此其梯度为\n$$\\nabla f(x)=\\frac{d}{dx}(2x^{2})=4x.$$\n\n给定 $x_{0}=10$，$v_{0}=0$，$\\gamma=0.9$ 和 $\\eta=0.1$，首先计算 $t=1$ 时的前瞻点：\n$$x_{0}-\\gamma v_{0}=10-0.9\\cdot 0=10.$$\n然后更新 $t=1$ 时的速度：\n$$v_{1}=\\gamma v_{0}+\\eta \\nabla f(10)=0.9\\cdot 0+0.1\\cdot 4\\cdot 10=4.$$\n更新位置：\n$$x_{1}=x_{0}-v_{1}=10-4=6.$$\n\n对于第二次迭代（$t=2$），前瞻点是\n$$x_{1}-\\gamma v_{1}=6-0.9\\cdot 4=6-3.6=2.4.$$\n因此，在第二次迭代中使用的前瞻点的数值是 $2.4$。",
            "answer": "$$\\boxed{2.4}$$"
        },
        {
            "introduction": "理解了 NAG 的基本计算步骤后，让我们通过编程来直观地感受它的实际优势。这个练习  要求你构建一个充满振荡的合成损失函数，这种函数在实践中很常见，对优化算法构成了挑战。通过分别实现标准梯度下降和 NAG，你将能够从量化指标上清晰地看到 NAG 如何利用其动量校正机制，更平滑地穿越这些棘手的“地形”，有效抑制困扰简单方法的剧烈振荡。",
            "id": "3157033",
            "problem": "您的任务是设计并分析一个具有振荡曲率的一维合成损失函数，然后实现两种优化方法来研究它们的行为。该合成损失函数为一个标量变量 $x \\in \\mathbb{R}$ 的函数，定义如下\n$$\nf(x) = \\sum_{i=1}^{n} a_i \\sin(b_i x),\n$$\n其中 $a_i \\in \\mathbb{R}$ 和 $b_i \\in \\mathbb{R}$ 是给定的系数，且正弦函数使用弧度制角度进行计算。目标是评估 Nesterov 加速梯度 (NAG) 相较于随机梯度下降 (SGD) 在稳定振荡行为方面的能力。\n\n您的程序必须仅使用基于数学的定义，从第一性原理出发实现以下组件：\n- 一个可微目标函数 $f(x)$ 及其梯度 $\\nabla f(x)$，该梯度使用正弦函数对 $x$ 的求导法则推导得出。\n- 一个迭代优化过程，该过程从初始条件 $x_0$ 开始，在固定的迭代次数 $T$ 内更新变量 $x$。\n- 两种用于比较的优化器：\n  1. 一种基准梯度下降法，该方法使用在 $x_t$ 处的当前梯度和正学习率 $\\eta$ 来生成下一个 $x$。\n  2. 一种基于动量的方法，该方法使用速度向量并在一个前瞻位置评估梯度以生成下一个 $x$。该方法必须与 Nesterov 加速梯度的定义一致，并且当动量参数 $\\mu$ 设置为 $0$ 时，必须退化为基准方法。\n\n将优化轨迹的振荡分数定义为连续步长增量中的符号变化次数，其中每个增量为 $s_t = x_{t+1} - x_t$，迭代索引为 $t$。当两个连续的非零增量符号相反时，即发生一次符号变化。在判断符号变化时必须忽略零值。形式上，令 $\\operatorname{sign}(z)$ 为符号函数，对于实数输入 $z$，返回 $-1$、$0$ 或 $+1$。对于序列 $\\{s_t\\}_{t=0}^{T-1}$，振荡计数 $C$ 定义为\n$$\nC = \\sum_{t=1}^{T-1} \\mathbf{1}\\Big(\\operatorname{sign}(s_{t-1}) \\cdot \\operatorname{sign}(s_t) = -1 \\ \\wedge \\ \\operatorname{sign}(s_{t-1}) \\neq 0 \\ \\wedge \\ \\operatorname{sign}(s_t) \\neq 0\\Big),\n$$\n其中 $\\mathbf{1}(\\cdot)$ 表示指示函数，当其参数为真时返回 $1$，否则返回 $0$。使用此度量来量化振荡。为比较方法，计算整数差 $\\Delta = C_{\\mathrm{SGD}} - C_{\\mathrm{NAG}}$，其中 $C_{\\mathrm{SGD}}$ 是基准方法的振荡计数，而 $C_{\\mathrm{NAG}}$ 是基于动量的前瞻方法的振荡计数。正的 $\\Delta$ 表示在给定设置下，Nesterov 加速梯度相对于随机梯度下降能稳定振荡，零 $\\Delta$ 表示振荡行为相同，负的 $\\Delta$ 表示稳定效果更差。\n\n角度必须以弧度为单位。除此角度规定外，不涉及任何物理单位。所有数值输出必须为整数。\n\n实现一个单一程序，对于下述每个测试用例，计算整数 $\\Delta$，并生成单行输出，其中包含用方括号括起来的、以逗号分隔的差值列表（例如，$[1,0,2]$）。使用以下测试套件，每个用例由 $(a, b, \\eta, \\mu, T, x_0)$ 指定：\n- 用例 1（一般振荡曲率，中等频率，预期稳定）：$a = [\\,1.0, \\ 0.5\\,]$，$b = [\\,3.0, \\ 7.0\\,]$，$\\eta = 0.05$，$\\mu = 0.9$，$T = 200$，$x_0 = 1.0$。\n- 用例 2（高频曲率，较大动量，更长时域）：$a = [\\,1.0\\,]$，$b = [\\,40.0\\,]$，$\\eta = 0.02$，$\\mu = 0.95$，$T = 400$，$x_0 = 0.1$。\n- 用例 3（关闭动量，等效性检查）：$a = [\\,1.0, \\ 0.5\\,]$，$b = [\\,3.0, \\ 7.0\\,]$，$\\eta = 0.05$，$\\mu = 0.0$，$T = 200$，$x_0 = 1.0$。\n- 用例 4（非常小的步长，近似单调运动）：$a = [\\,1.0, \\ 0.3\\,]$，$b = [\\,5.0, \\ 6.0\\,]$，$\\eta = 0.001$，$\\mu = 0.9$，$T = 200$，$x_0 = 2.0$。\n- 用例 5（单频项的零初始梯度）：$a = [\\,1.0\\,]$，$b = [\\,10.0\\,]$，$\\eta = 0.05$，$\\mu = 0.9$，$T = 200$，$x_0 = \\pi/20$。\n\n您的程序应生成单行输出，其中包含按所提供用例顺序排列的结果，格式为逗号分隔的列表并用方括号括起，即 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5]$。",
            "solution": "问题陈述已经过严格验证，被认为是科学严谨、适定、客观且内部一致的。它为合成损失函数、两种不同的优化算法以及量化振荡行为的精确度量提供了形式化规范。术语“随机梯度下降 (SGD)”在此用于指代技术上标准的梯度下降 (GD)，因为损失函数是确定性的。这是深度学习文献中的一种常见约定，在问题对“基准方法”的明确定义下，其含义是明确的。所有参数和条件都已充分指定，从而能够得出唯一且可验证的解。\n\n### 基于原理的解决方案\n\n任务是比较两种优化算法在单个一维非凸损失函数上的振荡行为。解决方案包括三个阶段：定义数学模型、指定优化算法以及实现比较度量。\n\n#### 1. 目标函数和梯度\n\n合成损失函数由以下公式给出：\n$$\nf(x) = \\sum_{i=1}^{n} a_i \\sin(b_i x)\n$$\n其中 $x \\in \\mathbb{R}$，系数 $a_i, b_i$ 以实数列表形式提供。正弦函数以弧度计算。\n\n为了执行基于梯度的优化，我们必须计算 $f(x)$ 对 $x$ 的一阶导数。利用导数的线性和链式法则 $\\frac{d}{dx}\\sin(u) = \\cos(u) \\frac{du}{dx}$，我们求得梯度 $\\nabla f(x) = \\frac{df}{dx}$：\n$$\n\\nabla f(x) = \\frac{d}{dx} \\left( \\sum_{i=1}^{n} a_i \\sin(b_i x) \\right) = \\sum_{i=1}^{n} a_i \\frac{d}{dx} (\\sin(b_i x))\n$$\n$$\n\\nabla f(x) = \\sum_{i=1}^{n} a_i b_i \\cos(b_i x)\n$$\n这个梯度表达式将用于两种优化器的更新规则。\n\n#### 2. 优化算法\n\n我们将实现两种迭代优化算法，它们都从初始位置 $x_0$ 开始生成一个位置序列 $\\{x_t\\}_{t=0}^T$。\n\n**a) 基准方法（梯度下降）**\n\n问题描述了一种基准方法，称为“SGD”，对于确定性函数而言，它就是标准的梯度下降 (GD)。在每次迭代 $t$ 中，通过沿梯度 $\\nabla f(x_t)$ 的反方向移动来更新位置 $x_t$。更新规则为：\n$$\nx_{t+1} = x_t - \\eta \\nabla f(x_t)\n$$\n其中 $\\eta > 0$ 是学习率。\n\n**b) Nesterov 加速梯度 (NAG)**\n\nNAG 是一种基于动量的方法，它通过在“前瞻”位置计算梯度来改进标准动量法。这使得优化器能够预见梯度的变化并修正其路径，这在减少振荡方面尤其有效。NAG 的一种标准实现涉及一个速度向量 $v_t$，并按以下步骤进行，初始速度 $v_0 = 0$：\n\n1.  **计算前瞻位置：** 首先，通过应用当前动量来估计一个中间的“前瞻”位置 $\\tilde{x}_t$：\n    $$\n    \\tilde{x}_t = x_t + \\mu v_t\n    $$\n    其中 $\\mu \\in [0, 1)$ 是动量参数。\n\n2.  **更新速度：** 使用在前瞻位置 $\\tilde{x}_t$ 处评估的梯度来更新速度：\n    $$\n    v_{t+1} = \\mu v_t - \\eta \\nabla f(\\tilde{x}_t)\n    $$\n\n3.  **更新位置：** 使用新的速度更新最终位置：\n    $$\n    x_{t+1} = x_t + v_{t+1}\n    $$\n\n要求是当 $\\mu=0$ 时，该方法必须退化为基准方法。我们来验证一下：如果 $\\mu = 0$，则前瞻位置为 $\\tilde{x}_t = x_t$。速度更新变为 $v_{t+1} = -\\eta \\nabla f(x_t)$。位置更新变为 $x_{t+1} = x_t - \\eta \\nabla f(x_t)$，这与梯度下降的更新规则完全相同。这证实了所选公式的一致性。\n\n#### 3. 振荡分数和比较\n\n优化轨迹 $\\{x_t\\}_{t=0}^{T}$ 中的振荡程度通过振荡计数 $C$ 来量化。此度量基于步长增量序列 $s_t = x_{t+1} - x_t$，其中 $t \\in \\{0, 1, \\dots, T-1\\}$。\n\n当两个连续的非零步长符号相反时，记录一次符号变化。总振荡计数 $C$ 定义为：\n$$\nC = \\sum_{t=1}^{T-1} \\mathbf{1}\\Big(\\operatorname{sign}(s_{t-1}) \\cdot \\operatorname{sign}(s_t) = -1 \\Big)\n$$\n问题陈述中包含了 $s_{t-1}$ 和 $s_t$ 都必须为非零的明确条件。数学表达式 $\\operatorname{sign}(s_{t-1}) \\cdot \\operatorname{sign}(s_t) = -1$ 内在地强制了这一点，因为符号函数对零输入的返回值为 0，其乘积将是 0 而不是 -1。\n\n对于每个测试用例，我们将生成两条轨迹，一条用于基准方法 (GD)，一条用于 NAG。然后我们计算它们各自的振荡计数 $C_{\\mathrm{GD}}$ 和 $C_{\\mathrm{NAG}}$。最终的比较度量是整数差：\n$$\n\\Delta = C_{\\mathrm{GD}} - C_{\\mathrm{NAG}}\n$$\n正的 $\\Delta$ 表示在给定参数下，NAG 的振荡次数少于 GD。\n\n#### 4. 计算过程\n对于由参数 $(a, b, \\eta, \\mu, T, x_0)$ 指定的每个测试用例：\n1.  使用给定的系数 $a$ 和 $b$ 定义梯度函数 $\\nabla f(x)$。\n2.  **运行基准方法 (GD)：**\n    - 使用 $x^{\\text{GD}}_0 = x_0$ 初始化轨迹。\n    - 迭代 $t = 0, \\dots, T-1$，计算 $x^{\\text{GD}}_{t+1} = x^{\\text{GD}}_{t} - \\eta \\nabla f(x^{\\text{GD}}_{t})$。\n    - 存储完整的轨迹 $\\{x^{\\text{GD}}_t\\}_{t=0}^T$。\n3.  **运行 NAG：**\n    - 使用 $x^{\\text{NAG}}_0 = x_0$ 和速度 $v_0 = 0$ 初始化轨迹。\n    - 迭代 $t = 0, \\dots, T-1$，使用 NAG 更新规则计算 $x^{\\text{NAG}}_{t+1}$。\n    - 存储完整的轨迹 $\\{x^{\\text{NAG}}_t\\}_{t=0}^T$。\n4.  **计算振荡计数：**\n    - 对于 GD 轨迹，计算步长增量 $s^{\\text{GD}}_t$ 和振荡计数 $C_{\\mathrm{GD}}$。\n    - 对于 NAG 轨迹，计算步长增量 $s^{\\text{NAG}}_t$ 和振荡计数 $C_{\\mathrm{NAG}}$。\n5.  **计算差值：** 计算 $\\Delta = C_{\\mathrm{GD}} - C_{\\mathrm{NAG}}$ 并记录整数结果。\n对所有五个测试用例重复此过程以生成最终输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization comparison problem for the given test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (a, b, eta, mu, T, x0)\n    test_cases = [\n        # Case 1\n        (np.array([1.0, 0.5]), np.array([3.0, 7.0]), 0.05, 0.9, 200, 1.0),\n        # Case 2\n        (np.array([1.0]), np.array([40.0]), 0.02, 0.95, 400, 0.1),\n        # Case 3\n        (np.array([1.0, 0.5]), np.array([3.0, 7.0]), 0.05, 0.0, 200, 1.0),\n        # Case 4\n        (np.array([1.0, 0.3]), np.array([5.0, 6.0]), 0.001, 0.9, 200, 2.0),\n        # Case 5\n        (np.array([1.0]), np.array([10.0]), 0.05, 0.9, 200, np.pi / 20.0),\n    ]\n\n    results = []\n\n    def get_gradient_func(a, b):\n        \"\"\"Returns the gradient function for a given set of coefficients.\"\"\"\n        def grad_f(x):\n            return np.sum(a * b * np.cos(b * x))\n        return grad_f\n\n    def run_sgd(grad_f, eta, T, x0):\n        \"\"\"Runs the baseline Gradient Descent optimizer.\"\"\"\n        x_traj = [x0]\n        x_current = x0\n        for _ in range(T):\n            gradient = grad_f(x_current)\n            x_next = x_current - eta * gradient\n            x_traj.append(x_next)\n            x_current = x_next\n        return np.array(x_traj)\n\n    def run_nag(grad_f, eta, mu, T, x0):\n        \"\"\"Runs the Nesterov Accelerated Gradient optimizer.\"\"\"\n        x_traj = [x0]\n        x_current = x0\n        v = 0.0\n        for _ in range(T):\n            x_lookahead = x_current + mu * v\n            gradient = grad_f(x_lookahead)\n            v_next = mu * v - eta * gradient\n            x_next = x_current + v_next\n            \n            x_traj.append(x_next)\n            x_current = x_next\n            v = v_next\n        return np.array(x_traj)\n\n    def calculate_oscillation_count(trajectory):\n        \"\"\"Calculates the oscillation score for a given trajectory.\"\"\"\n        if len(trajectory)  3:\n            return 0\n        \n        # Calculate step increments s_t = x_{t+1} - x_t\n        steps = np.diff(trajectory)\n        \n        # The sum is from t=1 to T-1, involving pairs (s_{t-1}, s_t)\n        # There are T steps (s_0 to s_{T-1}) for T iterations.\n        # The loop iterates T-1 times.\n        count = 0\n        for t in range(1, len(steps)):\n            # The condition sign(s_1)*sign(s_2) == -1 inherently handles\n            # cases where one of them is zero, as the product would be 0.\n            if np.sign(steps[t-1]) * np.sign(steps[t]) == -1:\n                count += 1\n        return count\n\n    for case in test_cases:\n        a, b, eta, mu, T, x0 = case\n        \n        grad_f = get_gradient_func(a, b)\n        \n        # Run baseline method (SGD/GD)\n        traj_sgd = run_sgd(grad_f, eta, T, x0)\n        c_sgd = calculate_oscillation_count(traj_sgd)\n        \n        # Run momentum-based method (NAG)\n        traj_nag = run_nag(grad_f, eta, mu, T, x0)\n        c_nag = calculate_oscillation_count(traj_nag)\n        \n        # Calculate the difference and append to results\n        delta = c_sgd - c_nag\n        results.append(int(delta))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "为了真正精通 NAG，关键在于将其与其他动量方法（如经典动量法）区分开来。本练习  将挑战你并排实现这两种算法，并在一个二维二次目标函数上进行对比。通过比较它们的优化轨迹并量化 NAG 特有的“前瞻”位移，你将更深刻地体会到 Nesterov 引入的这一微小但强大的改变如何带来更快、更稳定的收敛。",
            "id": "3279039",
            "problem": "要求您在最速下降法的框架内，为凸二次函数实现并比较两种一阶优化算法：经典动量法（也称为重球法）和 Nesterov 加速梯度法 (NAG)。需要探究的概念是 Nesterov 加速梯度法的“前瞻”行为。您的目标是根据光滑凸优化中最速下降和梯度评估的基本定义来设计和实现这两种算法，用数值量化“前瞻”效应，并在一个小型测试集上汇总结果。\n\n从以下基本概念开始：\n- 设 $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ 是可微的，其梯度为 $\\nabla f$。在点 $x$ 处的最速下降方向是 $- \\nabla f(x)$。\n- 固定步长 $ \\alpha  0 $ 是一个正标量，用于缩放沿选定下降方向所走的步长。\n- 经典动量法通过一个辅助“速度”变量来增强最速下降法，该变量累积过去的更新以加速沿持续方向的运动；而 Nesterov 加速梯度法在形成更新之前，会先沿当前动量方向位移到一个点，并在此点评估梯度。在 Nesterov 加速梯度法中，梯度是在一个“前瞻”点进行评估的，该点取决于当前位置和动量；这是需要量化的关键行为差异。\n\n使用纯二次目标函数\n$$\nf(x) = \\tfrac{1}{2} x^{\\top} A x,\n$$\n其中 $A \\in \\mathbb{R}^{2\\times 2}$ 是对称正定矩阵，因此\n$$\n\\nabla f(x) = A x,\n$$\n且唯一极小值点为 $x^{\\star} = 0$。\n\n为该目标函数实现这两种方法。使用固定步长 $\\alpha$ 和固定动量系数 $\\beta \\in [0,1)$，将速度初始化为零，并迭代预设的步数 $T$。对于经典动量法，必须在当前迭代点评估梯度。对于 Nesterov 加速梯度法，必须在一个“前瞻”点评估梯度，该点是当前迭代点沿当前速度的显式位移。您还必须计算一个数值代理来“可视化”前瞻行为而无需绘图：在每次迭代 $k$ 中，计算前瞻点与当前迭代点之间位移的范数，并报告这些范数在整个迭代过程中的平均值。该代理度量了 Nesterov 加速梯度法平均“前瞻”的距离。\n\n您的程序必须实现这两种算法，并对下面的每个测试用例计算并返回指定的量。对所有向量范数均使用欧几里得范数。\n\n测试集：\n- 测试用例 1（病态，理想路径）：设\n  $$\n  A = \\begin{bmatrix} 10  0 \\\\ 0  1 \\end{bmatrix}, \\quad x_0 = \\begin{bmatrix} 3 \\\\ -1 \\end{bmatrix}, \\quad \\alpha = 0.1, \\quad \\beta = 0.9, \\quad T = 50.\n  $$\n  从 $x_0$ 开始，以零初始速度运行两种方法 $T$ 步。输出一个布尔值，指示 Nesterov 加速梯度法的最终目标函数值是否严格小于经典动量法的目标函数值，即 $f(x_T^{\\mathrm{NAG}})  f(x_T^{\\mathrm{HB}})$ 是否成立。\n\n- 测试用例 2（边界条件 $\\beta = 0$）：设\n  $$\n  A = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}, \\quad x_0 = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix}, \\quad \\alpha = 0.2, \\quad \\beta = 0, \\quad T = 30.\n  $$\n  运行两种方法。输出一个布尔值，指示最终迭代点是否在数值公差范围内重合，即 $\\lVert x_T^{\\mathrm{NAG}} - x_T^{\\mathrm{HB}} \\rVert_2  10^{-12}$ 是否成立。\n\n- 测试用例 3（静止起始点）：设\n  $$\n  A = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}, \\quad x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\quad \\alpha = 0.2, \\quad \\beta = 0.9, \\quad T = 10.\n  $$\n  运行两种方法。输出一个布尔值，指示 Nesterov 加速梯度法的最终迭代点是否在数值公差范围内保持在极小值点，即 $\\lVert x_T^{\\mathrm{NAG}} \\rVert_2  10^{-12}$ 是否成立。\n\n- 测试用例 4（量化的前瞻幅度）：设\n  $$\n  A = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}, \\quad x_0 = \\begin{bmatrix} 1 \\\\ -3 \\end{bmatrix}, \\quad \\alpha = 0.2, \\quad \\beta = 0.9, \\quad T = 20.\n  $$\n  仅对 Nesterov 加速梯度法，计算 $T$ 次迭代中前瞻位移的平均范数：\n  $$\n  \\frac{1}{T} \\sum_{k=0}^{T-1} \\left\\lVert y_k - x_k \\right\\rVert_2,\n  $$\n  其中 $y_k$ 是第 $k$ 次迭代时的前瞻点，$x_k$ 是第 $k$ 次迭代时的当前迭代点。将此平均值作为浮点数输出。\n\n最终输出规范：\n- 您的程序必须生成一行包含所有测试用例结果的输出，结果为方括号内以逗号分隔的列表。各项按顺序必须是：\n  1) 测试用例 1 的布尔值，2) 测试用例 2 的布尔值，3) 测试用例 3 的布尔值，4) 测试用例 4 的浮点数值。\n- 例如，所需格式为\n  $$\n  [\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4].\n  $$\n不允许也不需要任何绘图或文件输入/输出；所有计算都必须是数值的且自包含的。不涉及角度。不涉及物理单位。所有结果必须使用双精度浮点运算计算。",
            "solution": "该问题要求实现并比较两种用于最小化凸二次函数的一阶优化算法：经典动量法（重球法）和 Nesterov 加速梯度法 (NAG)。目标是量化 NAG 的“前瞻”特性。\n\n待最小化的目标函数是一个凸二次型 $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$，由下式给出：\n$$\nf(x) = \\frac{1}{2} x^{\\top} A x\n$$\n其中 $x \\in \\mathbb{R}^2$ 是变量向量，$A \\in \\mathbb{R}^{2\\times 2}$ 是一个对称正定矩阵。该函数的梯度是线性的：\n$$\n\\nabla f(x) = A x\n$$\n鉴于 $A$ 是正定的，函数 $f(x)$ 是严格凸的，并在 $x^{\\star} = 0$ 处有唯一的极小值点。\n\n两种算法都是从初始点 $x_0$ 开始的迭代方法，生成一个收敛到极小值点 $x^{\\star}$ 的迭代序列 $\\{x_k\\}$。它们使用固定的步长 $\\alpha  0$ 和动量系数 $\\beta \\in [0,1)$。初始速度（一个累积过去更新的变量）被设为零，即 $v_0 = 0$。\n\n两种算法的核心逻辑都围绕着迭代 $k=0, 1, 2, \\dots, T-1$ 的以下更新结构：\n1. 计算梯度方向。\n2. 速度变量 $v_k$ 更新为 $v_{k+1}$。\n3. 位置变量 $x_k$ 更新为 $x_{k+1}$。\n\n两种算法的区别在于步骤 1，具体来说是在哪个点评估梯度。\n\n**经典动量法（重球法）**\n\n在重球法中，梯度是在当前位置 $x_k$ 处评估的。第 $k$ 次迭代的更新规则是：\n1. 计算梯度：$g_k = \\nabla f(x_k) = A x_k$。\n2. 更新速度：$v_{k+1} = \\beta v_k - \\alpha g_k$。\n3. 更新位置：$x_{k+1} = x_k + v_{k+1}$。\n该过程从 $x_0$ 和 $v_0 = 0$ 开始。\n\n**Nesterov 加速梯度法 (NAG)**\n\nNAG 引入了一个“前瞻”步骤。在计算梯度之前，它会沿当前速度方向迈出初步的一步。然后在这个投影点上评估梯度，从而为动量更新提供修正。\n第 $k$ 次迭代的更新规则是：\n1. 定义前瞻点：$y_k = x_k + \\beta v_k$。\n2. 在前瞻点计算梯度：$g_k = \\nabla f(y_k) = A y_k$。\n3. 更新速度：$v_{k+1} = \\beta v_k - \\alpha g_k$。\n4. 更新位置：$x_{k+1} = x_k + v_{k+1}$。\n该过程同样从 $x_0$ 和 $v_0 = 0$ 开始。\n\n**特殊情况分析**\n\n- **情况 $\\beta=0$**：如果动量系数 $\\beta$ 为零，两种算法中的速度更新将失去对前一速度 $v_k$ 的依赖。\n对于 HB：$v_{k+1} = -\\alpha \\nabla f(x_k)$，所以 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$。\n对于 NAG：前瞻点变为 $y_k = x_k + 0 \\cdot v_k = x_k$。速度更新为 $v_{k+1} = -\\alpha \\nabla f(x_k)$，因此 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$。\n两种算法都退化为标准梯度下降法。它们的轨迹 $\\{x_k\\}$ 将完全相同。测试用例 2 验证了此属性。\n\n- **情况 $x_0=x^{\\star}=0$**：如果起始点是极小值点，则梯度为零：$\\nabla f(0) = A \\cdot 0 = 0$。\n对于 HB 和 NAG，当 $x_0=0$ 和 $v_0=0$ 时，第一次迭代会得到：$g_0 = 0$，这导致 $v_1 = 0$ 并随后得到 $x_1 = 0$。通过归纳法，对于所有 $k  0$，迭代点将一直保持在极小值点，即 $x_k=0$ 和 $v_k=0$。测试用例 3 验证了这种静止行为。\n\n**量化前瞻效应**\n\nNAG 的“前瞻”行为由位移向量 $y_k - x_k = \\beta v_k$ 来表征。为了用数值量化这种效应，我们计算该位移的欧几里得范数在 $T$ 次迭代中的平均值。这个代理度量定义为：\n$$\n\\text{平均前瞻距离} = \\frac{1}{T} \\sum_{k=0}^{T-1} \\left\\lVert y_k - x_k \\right\\rVert_2 = \\frac{1}{T} \\sum_{k=0}^{T-1} \\left\\lVert \\beta v_k \\right\\rVert_2\n$$\n测试用例 4 要求计算该值。\n\n解决方案的步骤是实现这两种算法，并在 4 个指定的测试用例上运行它们，为每个用例计算所需的结果。所有向量范数均为欧几里得范数（$\\ell_2$-范数），浮点运算以双精度执行。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares Classical Momentum (Heavy-Ball) and Nesterov\n    Accelerated Gradient (NAG) on a set of test cases.\n    \"\"\"\n\n    def heavy_ball(A, x0, alpha, beta, T):\n        \"\"\"\n        Implements the Heavy-Ball (Classical Momentum) algorithm.\n\n        Args:\n            A (np.ndarray): The matrix of the quadratic objective.\n            x0 (np.ndarray): The starting point.\n            alpha (float): The step size.\n            beta (float): The momentum coefficient.\n            T (int): The number of iterations.\n\n        Returns:\n            np.ndarray: The final iterate x_T.\n        \"\"\"\n        x = np.copy(x0).astype(np.float64)\n        v = np.zeros_like(x0, dtype=np.float64)\n\n        for _ in range(T):\n            g = A @ x\n            v = beta * v - alpha * g\n            x = x + v\n        return x\n\n    def nesterov_ag(A, x0, alpha, beta, T, compute_lookahead=False):\n        \"\"\"\n        Implements the Nesterov Accelerated Gradient (NAG) algorithm.\n\n        Args:\n            A (np.ndarray): The matrix of the quadratic objective.\n            x0 (np.ndarray): The starting point.\n            alpha (float): The step size.\n            beta (float): The momentum coefficient.\n            T (int): The number of iterations.\n            compute_lookahead (bool): If True, also computes and returns the\n                                      average look-ahead displacement norm.\n        Returns:\n            np.ndarray or tuple: The final iterate x_T. If compute_lookahead is\n                                 True, returns (x_T, avg_lookahead_norm).\n        \"\"\"\n        x = np.copy(x0).astype(np.float64)\n        v = np.zeros_like(x0, dtype=np.float64)\n        \n        if compute_lookahead:\n            lookahead_norms = []\n\n        for _ in range(T):\n            if compute_lookahead:\n                # Displacement at step k is beta * v_k\n                lookahead_norms.append(np.linalg.norm(beta * v))\n            \n            # Look-ahead point and gradient calculation\n            y = x + beta * v\n            g = A @ y\n\n            # Update velocity and position\n            v = beta * v - alpha * g\n            x = x + v\n\n        if compute_lookahead:\n            avg_lookahead_norm = np.mean(lookahead_norms) if lookahead_norms else 0.0\n            return x, avg_lookahead_norm\n        else:\n            return x\n\n    def objective_function(x, A):\n        \"\"\"Computes f(x) = 0.5 * x.T @ A @ x.\"\"\"\n        return 0.5 * x.T @ A @ x\n\n    results = []\n\n    # Test Case 1: Ill-conditioned, comparison of objective values\n    A1 = np.array([[10, 0], [0, 1]], dtype=np.float64)\n    x0_1 = np.array([3, -1], dtype=np.float64)\n    alpha1, beta1, T1 = 0.1, 0.9, 50\n    \n    xT_hb_1 = heavy_ball(A1, x0_1, alpha1, beta1, T1)\n    xT_nag_1 = nesterov_ag(A1, x0_1, alpha1, beta1, T1)\n    \n    f_hb_1 = objective_function(xT_hb_1, A1)\n    f_nag_1 = objective_function(xT_nag_1, A1)\n    \n    results.append(f_nag_1  f_hb_1)\n\n    # Test Case 2: Boundary condition beta = 0\n    A2 = np.array([[4, 1], [1, 3]], dtype=np.float64)\n    x0_2 = np.array([2, 2], dtype=np.float64)\n    alpha2, beta2, T2 = 0.2, 0.0, 30\n\n    xT_hb_2 = heavy_ball(A2, x0_2, alpha2, beta2, T2)\n    xT_nag_2 = nesterov_ag(A2, x0_2, alpha2, beta2, T2)\n\n    diff_norm_2 = np.linalg.norm(xT_nag_2 - xT_hb_2)\n    results.append(diff_norm_2  1e-12)\n\n    # Test Case 3: Stationary start\n    A3 = np.array([[4, 1], [1, 3]], dtype=np.float64)\n    x0_3 = np.array([0, 0], dtype=np.float64)\n    alpha3, beta3, T3 = 0.2, 0.9, 10\n\n    xT_nag_3 = nesterov_ag(A3, x0_3, alpha3, beta3, T3)\n    norm_nag_3 = np.linalg.norm(xT_nag_3)\n    results.append(norm_nag_3  1e-12)\n\n    # Test Case 4: Quantified look-ahead magnitude\n    A4 = np.array([[4, 1], [1, 3]], dtype=np.float64)\n    x0_4 = np.array([1, -3], dtype=np.float64)\n    alpha4, beta4, T4 = 0.2, 0.9, 20\n\n    _, avg_lookahead_4 = nesterov_ag(A4, x0_4, alpha4, beta4, T4, compute_lookahead=True)\n    results.append(avg_lookahead_4)\n\n    # Format and print the final output\n    # The map(str, ...) part handles converting booleans to 'True'/'False'\n    # and floats to their string representation.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}