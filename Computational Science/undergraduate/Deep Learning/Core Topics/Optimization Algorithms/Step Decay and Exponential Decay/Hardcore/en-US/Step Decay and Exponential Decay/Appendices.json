{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of learning rate schedules, we begin with a direct comparison in a controlled setting. This practice challenges you to implement both exponential and step decay from first principles and evaluate their performance on a simple convex optimization problem. By analyzing how different parameterizations, like half-life for exponential decay versus drop points for step decay, affect convergence, you will develop a foundational intuition for their core behaviors .",
            "id": "3176459",
            "problem": "You are asked to implement and analyze learning rate schedules for discrete-time gradient descent on one-dimensional strictly convex quadratic losses. The focus is on contrasting exponential decay (parameterized by a half-life) against step decay (parameterized by a set of drop times), and evaluating the consistency of their performance across multiple datasets. Your program must be a single, complete, runnable program that produces the specified output without requiring any user input.\n\nStart from the following fundamental base:\n\n- Discrete-time gradient descent update for a differentiable function with scalar parameter: for a quadratic loss of the form $f(w) = \\tfrac{1}{2} a (w - b)^{2}$ with $a \\gt 0$, the gradient is $\\nabla f(w) = a (w - b)$. Under a time-varying learning rate $\\eta_{t}$ at step $t$, the update is\n$$\nw_{t+1} = w_{t} - \\eta_{t} \\, a \\, (w_{t} - b).\n$$\n- Define the error $e_{t} = w_{t} - b$. Then the dynamics satisfy\n$$\ne_{t+1} = \\left(1 - a \\, \\eta_{t}\\right) e_{t}.\n$$\nThese relations are the only starting point; you must derive any other relations you use from them.\n\nSchedules to implement:\n\n- Exponential decay schedule parameterized by a half-life. A half-life of $h$ steps means that the learning rate at step $h$ is exactly half of its initial value. Let $\\eta_{0}$ denote the initial learning rate and $T$ denote the total number of update steps. You must determine the decay rate so that the half-life condition holds for a given fraction $f$ of the training length, where $h = f \\, T$.\n\n- Step decay schedule with multiplicative drops by a factor of $1/2$ at prescribed milestone steps. For a set of milestone fractions $\\{f_{1}, f_{2}, \\ldots, f_{k}\\}$, the drop steps are the integer milestones $\\{\\lfloor f_{1} T \\rfloor, \\lfloor f_{2} T \\rfloor, \\ldots, \\lfloor f_{k} T \\rfloor\\}$. At each such step $t$, the learning rate is multiplied by $1/2$ before applying that step’s update. If multiple fractions map to the same integer step, merge duplicates by using each milestone at most once.\n\nExactly six schedules must be compared, indexed as follows:\n\n- Index $0$: Exponential decay with half-life fraction $0.05$.\n- Index $1$: Exponential decay with half-life fraction $0.10$.\n- Index $2$: Exponential decay with half-life fraction $0.20$.\n- Index $3$: Step decay with drops at fractions $\\{0.05\\}$.\n- Index $4$: Step decay with drops at fractions $\\{0.05, 0.10\\}$.\n- Index $5$: Step decay with drops at fractions $\\{0.05, 0.10, 0.20\\}$.\n\nFor each schedule and each dataset in a test case, compute the exact final loss after $T$ steps,\n$$\nL = \\tfrac{1}{2} a \\, e_{T}^{2},\n$$\nwhere $e_{T}$ is determined by the update dynamics and the chosen schedule. Do not simulate noisy gradients; use the exact quadratic dynamics implied by the update.\n\nEvaluation across datasets for a test case:\n\n- For each schedule index $s \\in \\{0,1,2,3,4,5\\}$, compute the mean of the final losses across the datasets in the test case; denote this mean by $M_{s}$.\n- Let $s^{\\star}$ be the smallest index achieving $\\min_{s} M_{s}$.\n- For each dataset individually, find the smallest schedule index that yields the minimal final loss on that dataset; count how many datasets select $s^{\\star}$. Denote this count by $C$.\n- Report for the test case the triple $[s^{\\star}, C, M_{s^{\\star}}]$.\n\nConventions and precise definitions:\n\n- The total number of steps $T$ is a positive integer, and steps are indexed by $t \\in \\{0,1,\\ldots,T-1\\}$.\n- For exponential decay, the learning rate at step $t$ must satisfy $\\eta_{t} = \\eta_{0}$ when $t=0$, and must satisfy the half-life condition at $t = h$ for the specified fraction $f$. You must derive the corresponding decay rate from first principles.\n- For step decay at a milestone step $t_{\\mathrm{milestone}}$, the learning rate used for that step is updated by the multiplicative drop before applying that step’s update.\n- For each dataset, use the same initial parameter $w_{0}$ across schedules within a test case.\n- Use the natural logarithm for any logarithmic derivations.\n- There are no physical units involved in this task.\n\nTest suite:\n\nImplement your program to run the following three test cases. Each test case consists of an initial learning rate $\\eta_{0}$, a total step count $T$, a shared initial parameter $w_{0}$, and a list of datasets, where each dataset is a pair $(a,b)$ defining $f(w) = \\tfrac{1}{2} a (w-b)^{2}$.\n\n- Test case $1$:\n  - $\\eta_{0} = 0.15$, $T = 100$, $w_{0} = 5.0$.\n  - Datasets: $(a,b) \\in \\{(1.0, 2.0), (3.0, -1.0), (10.0, 0.5)\\}$.\n\n- Test case $2$:\n  - $\\eta_{0} = 0.12$, $T = 120$, $w_{0} = -3.0$.\n  - Datasets: $(a,b) \\in \\{(0.8, 0.0), (2.5, 1.0), (6.0, -2.0)\\}$.\n\n- Test case $3$:\n  - $\\eta_{0} = 0.18$, $T = 60$, $w_{0} = 2.0$.\n  - Datasets: $(a,b) \\in \\{(1.5, 1.0), (4.0, -2.0), (9.0, 3.0)\\}$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain one element per test case, in order, where each element is the list $[s^{\\star}, C, M_{s^{\\star}}]$ defined above. For example, a syntactically valid output line would look like\n$[ [0,3,0.00123], [2,2,0.00456], [5,1,0.00789] ]$\nbut with the actual numeric values determined by your implementation.",
            "solution": "The problem requires an analysis of two learning rate decay schedules, exponential decay and step decay, for a one-dimensional quadratic loss function. The core of the analysis rests on the discrete-time dynamics of the error term, $e_t = w_t - b$, which represents the deviation of the parameter $w_t$ from the optimal value $b$.\n\nThe governing equation for the error at step $t+1$ is derived from the gradient descent update rule:\n$$\nw_{t+1} = w_t - \\eta_t \\nabla f(w_t)\n$$\nFor the specified quadratic loss $f(w) = \\frac{1}{2} a (w - b)^2$, the gradient is $\\nabla f(w) = a(w-b)$. Substituting this and the definition of the error $e_t$ gives:\n$$\nw_{t+1} - b = (w_t - b) - \\eta_t a (w_t - b)\n$$\n$$\ne_{t+1} = e_t - \\eta_t a e_t = (1 - a \\eta_t) e_t\n$$\nThis recurrence relation is the foundation for simulating the optimization process. The final error after $T-1$ steps, $e_T$, can be expressed as a product of the initial error $e_0 = w_0 - b$ and the reduction factors at each step:\n$$\ne_T = e_0 \\prod_{t=0}^{T-1} (1 - a \\eta_t)\n$$\nThe final loss is then computed as $L = \\frac{1}{2} a e_T^2$. Our analysis involves calculating this final loss for different learning rate schedules $\\eta_t$.\n\nThe two types of learning rate schedules are defined as follows:\n\n1.  **Exponential Decay Schedule**: The learning rate at step $t$ is given by a geometric progression $\\eta_t = \\eta_0 \\gamma^t$, where $\\eta_0$ is the initial learning rate and $\\gamma \\in (0, 1)$ is the decay rate. The problem specifies that the decay rate $\\gamma$ must be determined by a half-life condition: the learning rate at step $h = fT$ is half of its initial value, where $f$ is a given fraction and $T$ is the total number of steps. We derive $\\gamma$ from this condition:\n    $$\n    \\eta_h = \\eta_0 \\gamma^h = \\frac{1}{2} \\eta_0\n    $$\n    $$\n    \\gamma^h = \\frac{1}{2}\n    $$\n    Taking the natural logarithm of both sides:\n    $$\n    h \\ln(\\gamma) = \\ln\\left(\\frac{1}{2}\\right) = -\\ln(2)\n    $$\n    Solving for $\\gamma$:\n    $$\n    \\ln(\\gamma) = -\\frac{\\ln(2)}{h} \\implies \\gamma = \\exp\\left(-\\frac{\\ln(2)}{h}\\right)\n    $$\n    Substituting $h=fT$, the decay rate is $\\gamma = \\exp\\left(-\\frac{\\ln(2)}{fT}\\right)$, and the learning rate at step $t$ is:\n    $$\n    \\eta_t = \\eta_0 \\exp\\left(-\\frac{t \\ln(2)}{fT}\\right)\n    $$\n\n2.  **Step Decay Schedule**: The learning rate is held constant and then multiplicatively reduced at predefined milestone steps. Given a set of milestone fractions $\\{f_1, f_2, \\ldots, f_k\\}$, the corresponding integer milestone steps are $T_{\\text{miles}} = \\{\\lfloor f_1 T \\rfloor, \\lfloor f_2 T \\rfloor, \\ldots, \\lfloor f_k T \\rfloor\\}$, with duplicates removed. The learning rate starts at $\\eta_0$. At each time step $t \\in T_{\\text{miles}}$, the learning rate is updated by multiplying it by $1/2$ *before* it is used in the gradient descent update for that step. Let $\\eta'_t$ be the learning rate before the potential drop at step $t$. Then $\\eta'_0 = \\eta_0$ and for $t0$, $\\eta'_{t} = \\eta_{t-1}$. The learning rate $\\eta_t$ used for the update at step $t$ is:\n    $$\n    \\eta_t = \\begin{cases} \\frac{1}{2} \\eta'_t  \\text{if } t \\in T_{\\text{miles}} \\\\ \\eta'_t  \\text{otherwise} \\end{cases}\n    $$\n\nThe simulation proceeds by initializing the error $e_0 = w_0 - b$ and iteratively applying the error update $e_{t+1} = (1 - a \\eta_t) e_t$ for $t \\in \\{0, 1, \\ldots, T-1\\}$, using the appropriate $\\eta_t$ for each of the six specified schedules.\n\nAfter calculating the final loss $L$ for each schedule on each dataset, an evaluation is performed for each test case.\n-   For each schedule index $s \\in \\{0, \\dots, 5\\}$, the mean final loss $M_s$ is computed across all datasets in the test case.\n-   The best-performing schedule on average, $s^\\star$, is determined as the smallest index that achieves the minimum mean loss: $s^\\star = \\arg\\min_s M_s$.\n-   To measure the consistency of this best schedule, we count how many individual datasets also find $s^\\star$ to be their optimal schedule. This count is denoted by $C$.\n-   The final result for the test case is reported as the triple $[s^\\star, C, M_{s^\\star}]$.\n\nThis structured procedure allows for a rigorous and reproducible comparison of the learning rate schedules based on the provided dynamics and parameters.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        {\n            \"eta0\": 0.15, \"T\": 100, \"w0\": 5.0,\n            \"datasets\": [(1.0, 2.0), (3.0, -1.0), (10.0, 0.5)]\n        },\n        {\n            \"eta0\": 0.12, \"T\": 120, \"w0\": -3.0,\n            \"datasets\": [(0.8, 0.0), (2.5, 1.0), (6.0, -2.0)]\n        },\n        {\n            \"eta0\": 0.18, \"T\": 60, \"w0\": 2.0,\n            \"datasets\": [(1.5, 1.0), (4.0, -2.0), (9.0, 3.0)]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_simulation(case[\"eta0\"], case[\"T\"], case[\"w0\"], case[\"datasets\"])\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format [ [r1], [r2], ... ] from the example is illustrative of structure.\n    # The code template provides the authoritative format generation rule.\n    # str([1, 2, 3]) - '[1, 2, 3]' which includes spaces.\n    # ','.join(['[1,2]', '[3,4]']) - '[1,2],[3,4]' which has no space between elements.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef run_simulation(eta0, T, w0, datasets):\n    \"\"\"\n    Runs the simulation and evaluation for a single test case.\n    \"\"\"\n    # Schedule definitions\n    exp_decay_fractions = [0.05, 0.10, 0.20]\n    step_decay_fractions_sets = [[0.05], [0.05, 0.10], [0.05, 0.10, 0.20]]\n    num_schedules = len(exp_decay_fractions) + len(step_decay_fractions_sets)\n    \n    all_schedules_losses = [[] for _ in range(num_schedules)]\n\n    # Loop through each of the 6 schedules\n    for s_idx in range(num_schedules):\n        # Loop through each dataset for the current schedule\n        for a, b in datasets:\n            e = w0 - b\n            \n            # Run gradient descent for T steps\n            if s_idx  3: # Exponential decay schedules\n                f = exp_decay_fractions[s_idx]\n                h = f * T\n                # Handle potential h=0, though not possible with given data\n                gamma = np.exp(-np.log(2.0) / h) if h  0 else 1.0\n                \n                for t in range(T):\n                    eta_t = eta0 * (gamma ** t)\n                    e = e * (1.0 - a * eta_t)\n            \n            else: # Step decay schedules\n                fracs = step_decay_fractions_sets[s_idx - 3]\n                milestones = {int(f * T) for f in fracs}\n                current_eta = eta0\n                \n                for t in range(T):\n                    if t in milestones:\n                        current_eta /= 2.0\n                    eta_t = current_eta\n                    e = e * (1.0 - a * eta_t)\n            \n            # Calculate final loss\n            final_loss = 0.5 * a * e**2\n            all_schedules_losses[s_idx].append(final_loss)\n            \n    # Evaluation\n    mean_losses = [np.mean(losses) for losses in all_schedules_losses]\n    \n    s_star = int(np.argmin(mean_losses))\n    m_s_star = mean_losses[s_star]\n    \n    # Count how many datasets selected s_star as their best schedule\n    c = 0\n    losses_by_dataset = np.array(all_schedules_losses).T\n    for dataset_losses in losses_by_dataset:\n        best_s_for_dataset = int(np.argmin(dataset_losses))\n        if best_s_for_dataset == s_star:\n            c += 1\n            \n    return [s_star, c, m_s_star]\n\n# Execute the main function\nsolve()\n```"
        },
        {
            "introduction": "Real-world optimization involves more than just simple gradient descent; modern optimizers like Adam maintain internal states that create a \"memory\" of the training history. This exercise explores the critical interaction between this memory, represented by the moment estimates $m_t$ and $v_t$, and abrupt learning rate changes from a step decay schedule. You will diagnose and quantify the resulting \"update shocks\" and investigate a reset mechanism, revealing how schedule choice can profoundly impact the stability of adaptive optimizers .",
            "id": "3176478",
            "problem": "You are asked to investigate, in a controlled and purely algorithmic setting, the interaction between learning rate schedules and the internal state of Adaptive Moment Estimation (Adam) during abrupt changes. Your task is to write a complete and runnable program that simulates one-dimensional optimization using Adam under different learning rate schedules and gradient regimes, quantifies the presence of state mismatch immediately after a schedule change, and compares abrupt step decay to smooth exponential decay.\n\nBase the investigation on the following fundamental, widely accepted definitions. For integer time $t \\in \\{1,2,\\ldots,T\\}$, with scalar gradient $g_t \\in \\mathbb{R}$, Adam maintains moving averages $m_t$ and $v_t$, with hyperparameters $\\beta_1 \\in (0,1)$, $\\beta_2 \\in (0,1)$, and $\\epsilon  0$:\n$$\nm_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t,\\quad\nv_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2,\n$$\nwith bias-corrected estimates\n$$\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^{p_t}},\\quad\n\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^{q_t}},\n$$\nwhere $p_t$ and $q_t$ count the number of updates since initialization or since a reset of the bias-correction counters. The Adam parameter update at time $t$ is\n$$\n\\Delta \\theta_t = - \\eta_t \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon},\n$$\nwhere $\\eta_t  0$ is the learning rate. Consider two learning rate schedules:\n1. Step decay with step time $t_s$: \n$$\n\\eta_t = \\begin{cases}\n\\eta_0,  t  t_s\\\\\n\\gamma \\eta_0,  t \\ge t_s\n\\end{cases}\n$$\nwith $\\eta_0  0$ and $\\gamma \\in (0,1)$.\n2. Exponential decay chosen to match the value of the step decay at $t=t_s$:\n$$\n\\eta_t = \\eta_0 \\exp(-k (t-1)),\\quad \\text{where } k = \\frac{\\ln(1/\\gamma)}{t_s-1}.\n$$\nA piecewise-constant gradient regime is imposed to emulate a distribution shift:\n$$\ng_t = \\begin{cases}\ng_\\text{pre},  t  t_s\\\\\ng_\\text{post},  t \\ge t_s\n\\end{cases}\n$$\n\nDefine two quantitative diagnostics at the schedule change time $t=t_s$:\n- The update shock amplitude\n$$\nJ = \\left| \\Delta \\theta_{t_s} - \\Delta \\theta_{t_s-1} \\right|,\n$$\nwhich measures the abruptness of the change in the update; larger $J$ indicates a stronger shock.\n- The directional misalignment indicator\n$$\nB = \\begin{cases}\n\\text{True},  \\Delta \\theta_{t_s} \\cdot g_{t_s}  0\\\\\n\\text{False},  \\Delta \\theta_{t_s} \\cdot g_{t_s} \\le 0\n\\end{cases}\n$$\nwhich is `True` if the update at $t_s$ points in a direction of ascent ($\\Delta \\theta_{t_s} \\cdot g_{t_s} > 0$), and `False` otherwise. In one dimension, a descent-aligned update should satisfy $\\Delta \\theta_{t_s} \\cdot g_{t_s}  0$.\n\nTo mitigate mismatch immediately after a step change of the learning rate, implement a bias-correction tuning that resets the internal state at $t=t_s$:\n- Moment reset: set $m_{t_s-1} \\leftarrow 0$, $v_{t_s-1} \\leftarrow 0$ just before computing $m_{t_s}$ and $v_{t_s}$.\n- Bias-correction counter reset: reset the powers used in bias correction so that $p_{t_s}=1$ and $q_{t_s}=1$.\n\nThis reset forces the bias-corrected moments at $t_s$ to reflect the current gradient without lag.\n\nImplement a program that, for each of the following test cases, simulates $T$ steps, computes the pair $(J,B)$ as defined above, and prints all results in a single line as a flat list. Use the hyperparameters and constants\n- $T = 60$,\n- $t_s = 30$,\n- $\\eta_0 = 10^{-3}$,\n- $\\gamma = 10^{-1}$,\n- $\\beta_1 = 9 \\times 10^{-1}$,\n- $\\beta_2 = 9.99 \\times 10^{-1}$,\n- $\\epsilon = 10^{-8}$.\n\nTest suite (each case returns $(J,B)$ at $t=t_s$):\n1. Step decay, no reset, gradient sign flip at $t_s$: $g_\\text{pre} = 10^{-1}$, $g_\\text{post} = -1$.\n2. Step decay, with reset at $t_s$, gradient sign flip at $t_s$: $g_\\text{pre} = 10^{-1}$, $g_\\text{post} = -1$.\n3. Step decay, no reset, no gradient sign flip: $g_\\text{pre} = 10^{-1}$, $g_\\text{post} = 10^{-1}$.\n4. Exponential decay, no reset, gradient sign flip at $t_s$: $g_\\text{pre} = 10^{-1}$, $g_\\text{post} = -1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as\n$$\n[J_1, B_1, J_2, B_2, J_3, B_3, J_4, B_4].\n$$\nNo physical units are involved. All angles, if any, are to be considered dimensionless real numbers. The final outputs must be the exact computed values with default string formatting of the language, where booleans are allowed and appear as their language-native literals.",
            "solution": "The problem statement has been meticulously validated and is determined to be valid. It is scientifically grounded in the established principles of numerical optimization, specifically the Adam algorithm, and is well-posed with all necessary constants, initial conditions, and functional forms specified. The definitions are mathematically precise and objective, allowing for a unique and verifiable solution.\n\nThe task is to simulate the one-dimensional Adam optimizer over a time horizon $T$ under different configurations of learning rate schedules and gradient profiles. The core of the analysis focuses on the optimizer's behavior at a specific time point $t_s$, where both the learning rate and the gradient may change abruptly. We will quantify the resulting shock to the optimization process using two defined metrics, $J$ and $B$.\n\nThe Adam algorithm maintains an exponentially decaying moving average of past gradients (first moment, $m_t$) and past squared gradients (second moment, $v_t$). At each time step $t$, for a given gradient $g_t$, these moments are updated as follows, starting with $m_0 = 0$ and $v_0 = 0$:\n$$\nm_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\n$$\n$$\nv_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\n$$\nHere, $\\beta_1$ and $\\beta_2$ are hyperparameters in the interval $(0, 1)$ that control the decay rates of the moving averages. Because the moments are initialized to zero, they are biased towards zero, especially during the initial steps. This bias is corrected by computing:\n$$\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^{p_t}}\n$$\n$$\n\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^{q_t}}\n$$\nThe exponents $p_t$ and $q_t$ represent the number of updates that have contributed to the moments, which typically corresponds to the current time step $t$. The final parameter update $\\Delta \\theta_t$ is then calculated using the current learning rate $\\eta_t$, the bias-corrected moments, and a small stabilization constant $\\epsilon  0$:\n$$\n\\Delta \\theta_t = - \\eta_t \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n$$\nThe simulation will be performed for $T=60$ steps, with a critical event occurring at step $t_s=30$. The gradients follow a piecewise-constant regime:\n$$\ng_t = \\begin{cases}\ng_\\text{pre},  t  t_s \\\\\ng_\\text{post},  t \\ge t_s\n\\end{cases}\n$$\nWe investigate two learning rate schedules, $\\eta_t$:\n1.  **Step Decay**: The learning rate is constant at $\\eta_0$ and drops to a fraction $\\gamma$ of its initial value at time $t_s$.\n    $$\n    \\eta_t = \\begin{cases}\n    \\eta_0,  t  t_s \\\\\n    \\gamma \\eta_0,  t \\ge t_s\n    \\end{cases}\n    $$\n2.  **Exponential Decay**: The learning rate decays smoothly over time. The decay rate $k$ is chosen such that the learning rate at step $t_s$ matches that of the step decay schedule.\n    $$\n    \\eta_t = \\eta_0 \\exp(-k (t-1)), \\quad \\text{with } k = \\frac{\\ln(1/\\gamma)}{t_s-1}\n    $$\nAt the critical step $t_s$, we measure two diagnostics:\n-   **Update Shock Amplitude ($J$)**: This measures the magnitude of the change in the update step from $t_s-1$ to $t_s$. A large $J$ signifies a significant jolt to the optimization trajectory.\n    $$\n    J = \\left| \\Delta \\theta_{t_s} - \\Delta \\theta_{t_s-1} \\right|\n    $$\n-   **Directional Misalignment Indicator ($B$)**: This boolean indicator becomes $\\text{True}$ if the update at $t_s$ points in a direction of loss *ascent* rather than descent. A standard gradient descent step moves in the direction $-\\alpha g_t$, so the product of the update and the gradient should be negative. $B$ flags the case where this condition is violated.\n    $$\n    B = (\\Delta \\theta_{t_s} \\cdot g_{t_s}  0)\n    $$\nOne test case explores a mitigation strategy for the state mismatch. At $t=t_s$, the optimizer's state ($m_{t_s-1}$, $v_{t_s-1}$) reflects a history of gradients $g_\\text{pre}$, which may be misaligned with the new gradient $g_\\text{post}$. The **reset mechanism** addresses this by setting $m_{t_s-1}$ and $v_{t_s-1}$ to $0$ just before computing the new moments $m_{t_s}$ and $v_{t_s}$. Simultaneously, the bias-correction counters are reset, so $p_{t_s}=1$ and $q_{t_s}=1$. This effectively reinitializes the optimizer to adapt quickly to the new gradient regime.\n\nThe simulation will proceed for each of the four specified test cases, using the provided hyperparameters: $\\eta_0 = 10^{-3}$, $\\gamma = 10^{-1}$, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, and $\\epsilon = 10^{-8}$. For each case, the pair $(J, B)$ will be computed at $t = t_s = 30$ and reported.\n\nThe implementation will consist of a main loop iterating from $t=1$ to $T=60$. Within the loop, the appropriate gradient $g_t$ and learning rate $\\eta_t$ are determined based on the current step $t$ and the case being simulated. The logic for the reset mechanism is applied at $t=t_s$ if specified, altering the moment inputs and bias-correction powers for that step. The Adam update equations are then applied sequentially to compute $\\Delta\\theta_t$, which is stored. Finally, after the simulation, the diagnostics $J$ and $B$ are calculated using the stored updates at $t=29$ and $t=30$.",
            "answer": "```python\nimport numpy as np\n\ndef simulate_adam(\n    lr_schedule_type,\n    reset,\n    g_pre,\n    g_post,\n    T,\n    t_s,\n    eta_0,\n    gamma,\n    beta1,\n    beta2,\n    epsilon,\n):\n    \"\"\"\n    Simulates one-dimensional Adam optimization for a given scenario.\n\n    Args:\n        lr_schedule_type (str): 'step' or 'exp'.\n        reset (bool): Whether to apply the reset mechanism at t_s.\n        g_pre (float): Gradient value for t  t_s.\n        g_post (float): Gradient value for t = t_s.\n        T (int): Total number of simulation steps.\n        t_s (int): The step at which changes occur.\n        eta_0 (float): Initial learning rate.\n        gamma (float): Decay factor for step decay.\n        beta1 (float): Exponential decay rate for the 1st moment estimates.\n        beta2 (float): Exponential decay rate for the 2nd moment estimates.\n        epsilon (float): Term added to the denominator for numerical stability.\n\n    Returns:\n        tuple: A pair (J, B) containing the update shock amplitude and the\n               directional misalignment indicator.\n    \"\"\"\n    m = 0.0\n    v = 0.0\n    delta_thetas = []\n\n    # Calculate k for exponential decay if needed\n    k = 0.0\n    if lr_schedule_type == \"exp\":\n        k = np.log(1 / gamma) / (t_s - 1)\n\n    for t in range(1, T + 1):\n        # 1. Determine current gradient\n        g_t = g_pre if t  t_s else g_post\n\n        # 2. Determine current learning rate\n        if lr_schedule_type == \"step\":\n            eta_t = eta_0 if t  t_s else gamma * eta_0\n        else:  # 'exp'\n            eta_t = eta_0 * np.exp(-k * (t - 1))\n\n        # 3. Handle reset mechanism and bias-correction powers\n        m_prev_eff = m\n        v_prev_eff = v\n\n        if reset and t = t_s:\n            p_t = t - t_s + 1\n            q_t = t - t_s + 1\n            if t == t_s:\n                m_prev_eff = 0.0\n                v_prev_eff = 0.0\n        else:\n            p_t = t\n            q_t = t\n\n        # 4. Adam update equations\n        # Update biased first moment estimate\n        m = beta1 * m_prev_eff + (1 - beta1) * g_t\n        # Update biased second raw moment estimate\n        v = beta2 * v_prev_eff + (1 - beta2) * (g_t**2)\n\n        # Compute bias-corrected first moment estimate\n        m_hat = m / (1 - beta1**p_t)\n        # Compute bias-corrected second raw moment estimate\n        v_hat = v / (1 - beta2**q_t)\n\n        # Compute parameter update\n        delta_theta_t = -eta_t * m_hat / (np.sqrt(v_hat) + epsilon)\n        delta_thetas.append(delta_theta_t)\n\n    # 5. Compute diagnostics at t=t_s\n    delta_theta_ts_minus_1 = delta_thetas[t_s - 2]  # t_s-1 corresponds to index t_s-2\n    delta_theta_ts = delta_thetas[t_s - 1]          # t_s corresponds to index t_s-1\n    g_ts = g_post\n\n    # Update shock amplitude\n    J = np.abs(delta_theta_ts - delta_theta_ts_minus_1)\n\n    # Directional misalignment indicator\n    B = (delta_theta_ts * g_ts)  0\n\n    return J, B\n\n\ndef solve():\n    \"\"\"\n    Runs the simulation for all test cases and prints the results.\n    \"\"\"\n    # Hyperparameters and constants\n    T = 60\n    t_s = 30\n    eta_0 = 1e-3\n    gamma = 1e-1\n    beta1 = 0.9\n    beta2 = 0.999\n    epsilon = 1e-8\n\n    # Test suite\n    test_cases = [\n        # 1. Step decay, no reset, gradient sign flip\n        {\"lr_schedule\": \"step\", \"reset\": False, \"g_pre\": 1e-1, \"g_post\": -1.0},\n        # 2. Step decay, with reset, gradient sign flip\n        {\"lr_schedule\": \"step\", \"reset\": True, \"g_pre\": 1e-1, \"g_post\": -1.0},\n        # 3. Step decay, no reset, no gradient sign flip\n        {\"lr_schedule\": \"step\", \"reset\": False, \"g_pre\": 1e-1, \"g_post\": 1e-1},\n        # 4. Exponential decay, no reset, gradient sign flip\n        {\"lr_schedule\": \"exp\", \"reset\": False, \"g_pre\": 1e-1, \"g_post\": -1.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        J, B = simulate_adam(\n            lr_schedule_type=case[\"lr_schedule\"],\n            reset=case[\"reset\"],\n            g_pre=case[\"g_pre\"],\n            g_post=case[\"g_post\"],\n            T=T,\n            t_s=t_s,\n            eta_0=eta_0,\n            gamma=gamma,\n            beta1=beta1,\n            beta2=beta2,\n            epsilon=epsilon,\n        )\n        results.extend([J, B])\n\n    # Format the final output string\n    # Booleans are correctly converted to 'True'/'False' by str()\n    # Floats use default Python formatting\n    formatted_results = [str(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The stochastic nature of training deep learning models means that results can vary from one run to another, simply due to different random initializations or data processing orders. This practice investigates how the choice of learning rate schedule influences this variability and, therefore, the reproducibility of your experiments. By comparing the variance in final performance across multiple random seeds, you will empirically test the hypothesis that the abrupt drops of step decay can amplify the effects of noise, leading to less stable outcomes than a smooth exponential decay .",
            "id": "3176492",
            "problem": "Consider the training dynamics of an iterative parameter update in deep learning, where a single scalar parameter $w$ is optimized to minimize a convex objective centered at a target $w^\\star$. As a fundamental base, assume the standard gradient descent update with a time-varying learning rate, and additive, zero-mean, independent gradient noise at each iteration. Concretely, the iterates follow the rule that for each discrete time step $t \\in \\{0,1,\\dots,T-1\\}$, an update of the form $w_{t+1} = w_t - \\eta_t \\cdot g_t$ is applied, where $g_t$ is the gradient plus a noise perturbation, and $\\eta_t$ is the learning rate at time $t$. The objective is the differentiable quadratic function centered at $w^\\star$, and the noise is independent across steps and seeds.\n\nYour task is to implement two learning rate schedules that have been widely studied in deep learning:\n- A schedule that reduces the learning rate in discrete drops at fixed iteration thresholds.\n- A schedule that reduces the learning rate smoothly at each iteration.\n\nYou will compare the reproducibility across random seeds between these two schedules. Reproducibility is defined as the extent to which the final training outcome is sensitive to the choice of random seed. For this problem, sensitivity will be quantified as the empirical variance across random seeds of the final objective value. The hypothesis to be tested is that the schedule with discrete threshold reductions induces more sensitivity across seeds due to threshold effects when noise is present.\n\nUse the following modeling assumptions:\n- The parameter is scalar, with initialization $w_0$ drawn from a seed-dependent distribution.\n- The target value is fixed at $w^\\star = 1$.\n- The objective is the quadratic function $f(w) = \\tfrac{1}{2} (w - w^\\star)^2$, so the noiseless gradient is $w_t - w^\\star$ at iteration $t$.\n- The stochastic gradient at iteration $t$ is $g_t = (w_t - w^\\star) + \\epsilon_t$, where $\\epsilon_t$ is independent across $t$ and seeds, with $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n- The update is $w_{t+1} = w_t - \\eta_t \\cdot g_t$ for $t \\in \\{0,1,\\dots,T-1\\}$.\n\nFor each test case in the test suite below, you must:\n1. Simulate the training process under the discrete-threshold schedule and the smooth schedule using the same random seeds and noise parameters.\n2. For each schedule, compute the empirical variance across seeds of the final objective value $f(w_T)$.\n3. Produce a boolean result indicating whether the discrete-threshold schedule exhibits strictly greater variance than the smooth schedule for that test case.\n\nTest suite and parameter definitions:\n- Let the set of random seeds be $\\{0,1,\\dots,S-1\\}$ for a given $S$.\n- For each test case, you are given $(T,S,\\eta_0,\\gamma,s,\\sigma)$, where:\n  - $T$ is the total number of iterations.\n  - $S$ is the number of seeds.\n  - $\\eta_0$ is the initial learning rate.\n  - $\\gamma$ is the decay factor strictly between $0$ and $1$ that controls how rapidly the learning rate shrinks.\n  - $s$ is the step interval for threshold reductions.\n  - $\\sigma$ is the standard deviation of the gradient noise.\n- You must construct the discrete-threshold schedule to apply its reduction at iteration thresholds determined by $s$ and $\\gamma$, and the smooth schedule to reduce the learning rate continuously at each iteration using $s$ and $\\gamma$ so that the overall decay per $s$ iterations is comparable.\n\nUse the following test cases:\n- Case $1$: $(T,S,\\eta_0,\\gamma,s,\\sigma) = (240,200,0.4,0.5,40,0.1)$.\n- Case $2$: $(T,S,\\eta_0,\\gamma,s,\\sigma) = (240,200,0.4,0.5,40,0)$.\n- Case $3$: $(T,S,\\eta_0,\\gamma,s,\\sigma) = (240,200,0.05,0.5,40,0.1)$.\n- Case $4$: $(T,S,\\eta_0,\\gamma,s,\\sigma) = (240,200,0.4,0.2,40,0.2)$.\n- Case $5$: $(T,S,\\eta_0,\\gamma,s,\\sigma) = (240,200,0.4,0.5,10,0.2)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\dots]$), where each $result_i$ is a boolean corresponding to Case $i$, with value $True$ if the discrete-threshold schedule yields strictly greater empirical variance of $f(w_T)$ than the smooth schedule, and $False$ otherwise. No physical units are involved, and there are no angles or percentages to report. The output must be deterministic for the given test suite.",
            "solution": "The problem requires a comparative analysis of the stability of two learning rate schedules—discrete step decay and smooth exponential decay—within the context of stochastic gradient descent. The metric for comparison is the empirical variance of the final objective function value across multiple simulations, each initiated with a different random seed. A higher variance indicates greater sensitivity to the specific realization of stochasticity (initial parameter and gradient noise), implying lower reproducibility.\n\nFirst, we formalize the dynamical system. The parameter to be optimized is a scalar $w$, with the update rule for each time step $t \\in \\{0, 1, \\dots, T-1\\}$ given by:\n$$\nw_{t+1} = w_t - \\eta_t \\cdot g_t\n$$\nwhere $\\eta_t$ is the learning rate at step $t$ and $g_t$ is the stochastic gradient. The objective function is a simple quadratic bowl, $f(w) = \\frac{1}{2}(w - w^\\star)^2$, with the target parameter fixed at $w^\\star = 1$. The true gradient is $\\nabla f(w_t) = w_t - w^\\star$. The stochastic gradient $g_t$ includes an additive noise term $\\epsilon_t$:\n$$\ng_t = (w_t - w^\\star) + \\epsilon_t\n$$\nThe noise term $\\epsilon_t$ is modeled as an independent and identically distributed (i.i.d.) random variable drawn from a zero-mean normal distribution, $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nTo analyze the system's dynamics, we consider the error term $\\delta_t = w_t - w^\\star$. Substituting this into the update rule yields a linear recurrence relation for the error:\n$$\n\\delta_{t+1} = \\delta_t - \\eta_t (\\delta_t + \\epsilon_t) = (1 - \\eta_t)\\delta_t - \\eta_t \\epsilon_t\n$$\nThis equation governs the evolution of the parameter's deviation from the optimum, driven by the learning rate schedule $\\eta_t$ and the noise sequence $\\{\\epsilon_t\\}_{t=0}^{T-1}$.\n\nThe simulation's stochasticity originates from two sources, both dependent on a chosen seed $k \\in \\{0, 1, \\dots, S-1\\}$:\n1.  The initial parameter $w_0^{(k)}$, from which the initial error $\\delta_0^{(k)} = w_0^{(k)} - w^\\star$ is determined. We will draw $w_0^{(k)}$ from a standard normal distribution, $w_0 \\sim \\mathcal{N}(0, 1)$.\n2.  The sequence of gradient noise terms $\\{\\epsilon_t^{(k)}\\}_{t=0}^{T-1}$.\n\nFor each seed, the same initial condition $w_0^{(k)}$ and noise sequence $\\{\\epsilon_t^{(k)}\\}$ must be used to simulate the trajectories for both learning rate schedules. This ensures that any observed difference in outcome is attributable solely to the schedule itself.\n\nNext, we define the two learning rate schedules based on the initial rate $\\eta_0$, a decay factor $\\gamma \\in (0,1)$, and a step interval $s$.\n\n1.  **Discrete-Threshold Schedule ($\\eta_t^{\\text{disc}}$):** The learning rate is held constant for $s$ steps and then multiplicatively reduced by $\\gamma$. This is expressed as:\n    $$\n    \\eta_t^{\\text{disc}} = \\eta_0 \\cdot \\gamma^{\\lfloor t/s \\rfloor}\n    $$\n    The exponent $\\lfloor t/s \\rfloor$ is a step function that increments every $s$ iterations, causing abrupt drops in the learning rate at multiples of $s$.\n\n2.  **Smooth Schedule ($\\eta_t^{\\text{smth}}$):** To provide a comparable decay, this schedule is formulated as a continuous exponential decay. The decay rate per step, $\\gamma_{\\text{smth}}$, is chosen such that over $s$ steps, the total decay equals $\\gamma$. This implies $(\\gamma_{\\text{smth}})^s = \\gamma$, so $\\gamma_{\\text{smth}} = \\gamma^{1/s}$. The learning rate at step $t$ is:\n    $$\n    \\eta_t^{\\text{smth}} = \\eta_0 \\cdot (\\gamma^{1/s})^t = \\eta_0 \\cdot \\gamma^{t/s}\n    $$\n    This schedule decreases the learning rate by a small factor at every single iteration.\n\nThe core of the problem is to test the hypothesis that $\\text{Var}_{\\text{seeds}}(f(w_T^{\\text{disc}}))  \\text{Var}_{\\text{seeds}}(f(w_T^{\\text{smth}}))$, where $f(w_T)=\\frac{1}{2}\\delta_T^2$ is the final objective value.\n\nThe simulation algorithm for each test case $(T, S, \\eta_0, \\gamma, s, \\sigma)$ is as follows:\n1.  Initialize two arrays, $V_{\\text{disc}}$ and $V_{\\text{smth}}$, of size $S$ to store the final objective values.\n2.  For each seed $k$ from $0$ to $S-1$:\n    a.  Instantiate a random number generator seeded with $k$.\n    b.  Generate a common initial parameter $w_0$ and a common noise vector $(\\epsilon_0, \\dots, \\epsilon_{T-1})$ of length $T$.\n    c.  Simulate the discrete-schedule trajectory: starting with $w = w_0$, iterate the update rule $w_{t+1} = w_t - \\eta_t^{\\text{disc}} g_t$ for $t=0, \\dots, T-1$. Calculate the final objective $f(w_T)$ and store it in $V_{\\text{disc}}[k]$.\n    d.  Simulate the smooth-schedule trajectory: starting with the same $w_0$, iterate the update rule $w_{t+1} = w_t - \\eta_t^{\\text{smth}} g_t$ for $t=0, \\dots, T-1$. Calculate the final objective $f(w_T)$ and store it in $V_{\\text{smth}}[k]$.\n3.  Compute the empirical variance of the elements in $V_{\\text{disc}}$ and $V_{\\text{smth}}$.\n4.  The result is `True` if the variance of $V_{\\text{disc}}$ is strictly greater than that of $V_{\\text{smth}}$, and `False` otherwise. This procedure is repeated for all test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(T, S, eta0, gamma, s, sigma):\n    \"\"\"\n    Simulates training for discrete and smooth LR schedules and compares their final variance.\n\n    Args:\n        T (int): Total number of iterations.\n        S (int): Number of random seeds to simulate.\n        eta0 (float): Initial learning rate.\n        gamma (float): Decay factor.\n        s (int): Step interval for discrete decay.\n        sigma (float): Standard deviation of gradient noise.\n\n    Returns:\n        bool: True if the variance of the discrete schedule's final objective\n              is strictly greater than the smooth schedule's, False otherwise.\n    \"\"\"\n    w_star = 1.0\n\n    final_objs_disc = np.zeros(S)\n    final_objs_smth = np.zeros(S)\n\n    for seed in range(S):\n        # Use a seed-specific random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Generate the same initial weight and noise sequence for both schedules.\n        w0 = rng.normal(loc=0.0, scale=1.0)\n        noise = rng.normal(loc=0.0, scale=sigma, size=T)\n\n        # --- Discrete-Threshold Schedule Simulation ---\n        w = w0\n        for t in range(T):\n            lr_disc = eta0 * (gamma ** (t // s))\n            gradient = (w - w_star) + noise[t]\n            w = w - lr_disc * gradient\n        final_objs_disc[seed] = 0.5 * (w - w_star)**2\n\n        # --- Smooth Schedule Simulation ---\n        w = w0  # Reset to the same initial weight\n        for t in range(T):\n            lr_smth = eta0 * (gamma ** (t / s))\n            gradient = (w - w_star) + noise[t]\n            w = w - lr_smth * gradient\n        final_objs_smth[seed] = 0.5 * (w - w_star)**2\n\n    # Compute the empirical variance across all seeds for each schedule.\n    # np.var computes the population variance by default (ddof=0).\n    var_disc = np.var(final_objs_disc)\n    var_smth = np.var(final_objs_smth)\n\n    return var_disc  var_smth\n\ndef solve():\n    \"\"\"\n    Solves the problem by running simulations for all specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (T, S, eta0, gamma, s, sigma)\n    test_cases = [\n        (240, 200, 0.4, 0.5, 40, 0.1),\n        (240, 200, 0.4, 0.5, 40, 0.0),\n        (240, 200, 0.05, 0.5, 40, 0.1),\n        (240, 200, 0.4, 0.2, 40, 0.2),\n        (240, 200, 0.4, 0.5, 10, 0.2),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation(*case)\n        results.append(result)\n\n    # Format the final output as a comma-separated list of booleans in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}