{
    "hands_on_practices": [
        {
            "introduction": "理解学习率的第一步是回答一个基本问题：为什么我们不能使用任意大的学习率来加速收敛？本练习将通过一个简化但富有洞察力的二次损失函数，来探索梯度下降算法的稳定性边界。通过推导并数值验证最大学习率与损失函数曲率之间的关系，您将对优化器为何会发散，以及问题的几何特性如何决定这一关键超参数的选择，建立起一个坚实的基础 。",
            "id": "3187300",
            "problem": "考虑在深度学习中最小化一个二次可微的二次目标函数，其中参数向量表示为 $\\theta \\in \\mathbb{R}^d$，损失函数为 $f(\\theta)=\\tfrac{1}{2}\\theta^\\top A\\theta$，其中 $A \\in \\mathbb{R}^{d \\times d}$ 是一个对称正定矩阵。梯度下降 (GD) 的更新由迭代映射 $\\theta_{t+1}=\\theta_t-\\eta\\nabla f(\\theta_t)$ 定义，学习率为 $\\eta0$。矩阵的谱半径 (SR) 定义为 $\\rho(M)=\\max_i |\\lambda_i(M)|$，其中 $\\lambda_i(M)$ 是 $M$ 的特征值。从这些基本定义出发，推导学习率 $\\eta$ 使 GD 迭代渐近稳定（即，对于任意初始 $\\theta_0$，$\\lim_{t\\to\\infty}\\theta_t=\\mathbf{0}$）的充要条件，并将最大的渐近稳定学习率 $\\eta$ 用 $A$ 的谱半径表示。然后，通过模拟 GD 沿着 $A$ 的特征方向的迭代过程，来数值验证此稳定性条件。\n\n你的程序必须：\n- 对于每个测试用例矩阵 $A$，计算其谱半径 $\\rho(A)$ 和预测的最大渐近稳定学习率，记为 $\\eta_{\\text{max}}$。\n- 使用 $A$ 的特征向量，模拟 GD 更新 $\\theta_{t+1}=\\theta_t-\\eta A\\theta_t$ 进行 $T$ 步，初始条件选择为单个特征方向，具体为对 $A$ 的每个特征向量 $v_i$，设置 $\\theta_0=v_i$。\n- 对于每个测试用例的三个候选学习率：一个严格低于预测阈值的值 ($\\eta_{\\text{below}}=0.9\\,\\eta_{\\text{max}}$)，一个位于阈值处的值 ($\\eta_{\\text{at}}=\\eta_{\\text{max}}$)，以及一个严格高于阈值的值 ($\\eta_{\\text{above}}=1.1\\,\\eta_{\\text{max}}$)，报告 GD 迭代是否在所有特征方向上都收敛到原点，即当模拟 $T$ 步时，对于每个特征方向，$\\|\\theta_T\\|\\le \\varepsilon$ 是否成立。\n- 角度单位说明：测试定义中出现的所有角度均以弧度为单位。\n\n使用以下测试套件：\n1. 矩阵 $A_1=\\operatorname{diag}(1.0,4.0)$，因此 $d=2$。\n2. 矩阵 $A_2=Q_2 \\operatorname{diag}(0.5,1.5,3.0) Q_2^\\top$，旋转矩阵为 $Q_2=\\begin{bmatrix}\\cos(\\pi/4)-\\sin(\\pi/4)0\\\\ \\sin(\\pi/4)\\cos(\\pi/4)0\\\\ 001\\end{bmatrix}$，因此 $d=3$。\n3. 矩阵 $A_3=Q_3 \\operatorname{diag}(0.1,2.0,10.0,20.0) Q_3^\\top$，其中 $Q_3$ 是 $\\mathbb{R}^4$ 中两个块旋转的乘积：一个是在 $(0,1)$-平面内旋转 $\\phi=\\pi/6$，另一个是在 $(2,3)$-平面内旋转 $\\psi=\\pi/3$（两个角度都以弧度为单位），因此 $d=4$。\n\n模拟参数：\n- GD 步数 $T=200$。\n- 收敛容差 $\\varepsilon=10^{-6}$。\n\n你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表。对于每个测试用例 $i\\in\\{1,2,3\\}$，按顺序附加以下四个值：\n- 预测的 $\\eta_{\\text{max}}^{(i)}$，以浮点数表示，\n- 在 $\\eta_{\\text{below}}^{(i)}$ 处的稳定性的布尔值，\n- 在 $\\eta_{\\text{at}}^{(i)}$ 处的稳定性的布尔值，\n- 在 $\\eta_{\\text{above}}^{(i)}$ 处的稳定性的布尔值。\n\n因此，最终输出是一个包含 $12$ 个条目的扁平列表，形式为 $[\\eta_{\\text{max}}^{(1)}, s_{\\text{below}}^{(1)}, s_{\\text{at}}^{(1)}, s_{\\text{above}}^{(1)}, \\eta_{\\text{max}}^{(2)}, s_{\\text{below}}^{(2)}, s_{\\text{at}}^{(2)}, s_{\\text{above}}^{(2)}, \\eta_{\\text{max}}^{(3)}, s_{\\text{below}}^{(3)}, s_{\\text{at}}^{(3)}, s_{\\text{above}}^{(3)}]$，其中每个 $s$ 是一个布尔值，表示对于该 $\\eta$ 的所有特征方向模拟是否都满足 $\\|\\theta_T\\|\\le \\varepsilon$。",
            "solution": "用户希望找出在二次目标函数的梯度下降 (GD) 优化中，学习率 $\\eta$ 的稳定性条件，然后数值验证该条件。\n\n### 问题验证\n根据指定标准对问题陈述进行分析。\n\n**第一步：提取已知条件**\n- 目标函数：$f(\\theta)=\\tfrac{1}{2}\\theta^\\top A\\theta$，其中 $\\theta \\in \\mathbb{R}^d$。\n- 矩阵 $A$：一个对称正定矩阵，$A \\in \\mathbb{R}^{d \\times d}$。\n- 梯度下降 (GD) 更新规则：$\\theta_{t+1}=\\theta_t-\\eta\\nabla f(\\theta_t)$，学习率为 $\\eta0$。\n- 谱半径：$\\rho(M)=\\max_i |\\lambda_i(M)|$，其中 $\\lambda_i(M)$ 是矩阵 $M$ 的特征值。\n- 渐近稳定条件：对于任意初始向量 $\\theta_0$，$\\lim_{t\\to\\infty}\\theta_t=\\mathbf{0}$。\n- 测试矩阵：\n  1. $A_1=\\operatorname{diag}(1.0,4.0)$，$d=2$。\n  2. $A_2=Q_2 \\operatorname{diag}(0.5,1.5,3.0) Q_2^\\top$，其中 $Q_2$ 是 $\\mathbb{R}^3$ 中 $(0,1)$-平面内旋转 $\\pi/4$ 的矩阵。\n  3. $A_3=Q_3 \\operatorname{diag}(0.1,2.0,10.0,20.0) Q_3^\\top$，其中 $Q_3$ 是 $\\mathbb{R}^4$ 中 $(0,1)$-平面内旋转 $\\phi=\\pi/6$ 和 $(2,3)$-平面内旋转 $\\psi=\\pi/3$ 的乘积。\n- 模拟参数：\n  - 步数：$T=200$。\n  - 收敛容差：$\\varepsilon=10^{-6}$。\n- 数值验证：\n  - 初始条件：对于 $A$ 的每个特征向量 $v_i$，$\\theta_0=v_i$。\n  - 待测试学习率：$\\eta_{\\text{below}}=0.9\\,\\eta_{\\text{max}}$，$\\eta_{\\text{at}}=\\eta_{\\text{max}}$，$\\eta_{\\text{above}}=1.1\\,\\eta_{\\text{max}}$。\n  - 稳定性检查：对于从每个特征向量开始的模拟，必须满足 $\\|\\theta_T\\|\\le \\varepsilon$。\n\n**第二步：使用提取的已知条件进行验证**\n该问题是优化理论和数值线性代数中的一个标准练习。\n- **科学基础扎实**：该问题基于微积分（二次型的梯度）和线性代数（线性动力系统的特征值分析）的基本原理。它是对深度学习中优化器在局部最小值附近行为的简化，这是一个核心概念。\n- **问题定义明确**：问题定义清晰。它要求推导一个特定量（$\\eta_{\\text{max}}$）并进行数值验证，所有参数和矩阵都已指定。这种结构导向一个唯一且有意义的解。\n- **客观性**：问题以精确的数学术语陈述，没有任何主观性或歧义。\n\n该问题没有任何无效性缺陷。它是完整的、一致的、科学上合理的，并且定义明确。\n\n**第三步：结论与行动**\n问题有效。将提供完整的解决方案。\n\n### 稳定性条件的推导\n\n目标函数是一个二次型 $f(\\theta)=\\tfrac{1}{2}\\theta^\\top A\\theta$。由于 $A$ 是对称的，该函数关于 $\\theta$ 的梯度是 $\\nabla f(\\theta) = A\\theta$。\n\n梯度下降 (GD) 的更新规则为：\n$$\n\\theta_{t+1}=\\theta_t-\\eta\\nabla f(\\theta_t)\n$$\n代入梯度，我们得到一个线性迭代映射：\n$$\n\\theta_{t+1} = \\theta_t - \\eta A\\theta_t = (I - \\eta A)\\theta_t\n$$\n其中 $I$ 是单位矩阵。设迭代矩阵为 $M = I - \\eta A$。则更新为 $\\theta_{t+1} = M\\theta_t$。经过 $t$ 步后，参数向量由 $\\theta_t = M^t \\theta_0$ 给出。\n\n为了使迭代序列 $\\theta_t$ 对于任意初始向量 $\\theta_0$ 都收敛到零向量，即 $\\lim_{t\\to\\infty}\\theta_t=\\mathbf{0}$，迭代矩阵 $M$ 的谱半径必须严格小于 1，这是一个充要条件。谱半径定义为其特征值的最大绝对值。\n$$\n\\rho(M)  1\n$$\n设 $\\lambda_i(A)$ 是矩阵 $A$ 的特征值，对应的特征向量为 $v_i$。由于 $A$ 是实对称矩阵，其特征值是实数。迭代矩阵 $M = I - \\eta A$ 的特征值可以通过将 $M$ 应用于 $A$ 的一个特征向量 $v_i$ 来找到：\n$$\nMv_i = (I - \\eta A)v_i = Iv_i - \\eta Av_i = v_i - \\eta\\lambda_i(A)v_i = (1 - \\eta\\lambda_i(A))v_i\n$$\n因此，$M$ 的特征值（记为 $\\lambda_i(M)$）是 $\\lambda_i(M) = 1 - \\eta\\lambda_i(A)$。\n\n稳定性条件 $\\rho(M)  1$ 转化为：\n$$\n\\max_i |\\lambda_i(M)| = \\max_i |1 - \\eta\\lambda_i(A)|  1\n$$\n这必须对 $A$ 的所有特征值 $\\lambda_i(A)$ 成立。这个单一的不等式等价于以下一对不等式：\n$$\n-1  1 - \\eta\\lambda_i(A)  1 \\quad \\text{对所有 } i\n$$\n我们分析不等式的每一部分：\n1.  $1 - \\eta\\lambda_i(A)  1 \\implies -\\eta\\lambda_i(A)  0$。根据定义，学习率 $\\eta  0$，所以这简化为 $\\lambda_i(A)  0$。由于问题陈述矩阵 $A$ 是正定的，这个条件对所有 $i$ 都保证成立。\n\n2.  $-1  1 - \\eta\\lambda_i(A) \\implies \\eta\\lambda_i(A)  2 \\implies \\eta  \\frac{2}{\\lambda_i(A)}$。这个不等式必须对所有特征值 $\\lambda_i(A)$ 都成立。为了确保这一点，$\\eta$ 必须小于所有 $\\frac{2}{\\lambda_i(A)}$ 值中的最小值。这等价于选择最大的特征值 $\\lambda_{\\max}(A)$，因为它对 $\\eta$ 提供了最严格的约束：\n    $$\n    \\eta  \\frac{2}{\\lambda_{\\max}(A)}\n    $$\n\n问题将矩阵 $M$ 的谱半径定义为 $\\rho(M)=\\max_i |\\lambda_i(M)|$。对于对称正定矩阵 $A$，所有特征值 $\\lambda_i(A)$ 都是正的，所以其谱半径就是其最大特征值：$\\rho(A) = \\lambda_{\\max}(A)$。\n\n综合这些发现，GD 迭代渐近稳定的充要条件是：\n$$\n0  \\eta  \\frac{2}{\\rho(A)}\n$$\n最大的渐近稳定学习率 $\\eta_{\\text{max}}$ 是这个区间的上确界：\n$$\n\\eta_{\\text{max}} = \\frac{2}{\\rho(A)}\n$$\n\n### 数值验证\n程序将实现此推导过程。对于每个给定的矩阵 $A$，它将首先计算其特征值以找到 $\\rho(A) = \\lambda_{\\max}(A)$ 和相应的 $\\eta_{\\text{max}}$。然后，它将从 $A$ 的每个特征向量开始，模拟 GD 过程 $T=200$ 步。这个过程会对三个学习率进行：$\\eta_{\\text{below}}=0.9\\,\\eta_{\\text{max}}$（预期会收敛），$\\eta_{\\text{at}}=\\eta_{\\text{max}}$（预期对应于 $\\lambda_{\\max}(A)$ 的模式会不稳定），以及 $\\eta_{\\text{above}}=1.1\\,\\eta_{\\text{max}}$（预期会发散）。通过验证最终迭代的范数 $\\|\\theta_T\\|$ 是否小于或等于一个小的容差 $\\varepsilon=10^{-6}$ 来数值检查稳定性。\n- 对于 $\\eta_{\\text{below}}$，我们预期对所有 $i$ 都有 $|1 - \\eta\\lambda_i(A)|  1$，导致收敛。\n- 对于 $\\eta_{\\text{at}}$，对应于 $\\lambda_{\\max}(A)$ 的模式的迭代矩阵特征值为 $1 - \\frac{2}{\\lambda_{\\max}(A)}\\lambda_{\\max}(A) = -1$。该模式的迭代幅值不会衰减，导致收敛测试失败。\n- 对于 $\\eta_{\\text{above}}$，对应于 $\\lambda_{\\max}(A)$ 的模式的迭代矩阵特征值幅值将大于 1，导致发散。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and numerically verifies the stability condition for Gradient Descent\n    on a quadratic objective.\n    \"\"\"\n    # Simulation parameters from the problem statement\n    T = 200\n    eps = 1e-6\n    \n    # === Define Test Cases ===\n\n    # Test Case 1: A1 = diag(1.0, 4.0)\n    A1 = np.diag([1.0, 4.0])\n\n    # Test Case 2: A2 = Q2 * diag(0.5, 1.5, 3.0) * Q2^T\n    angle2 = np.pi / 4\n    c2, s2 = np.cos(angle2), np.sin(angle2)\n    Q2 = np.array([\n        [c2, -s2, 0],\n        [s2, c2, 0],\n        [0, 0, 1]\n    ])\n    D2 = np.diag([0.5, 1.5, 3.0])\n    A2 = Q2 @ D2 @ Q2.T\n\n    # Test Case 3: A3 = Q3 * diag(0.1, 2.0, 10.0, 20.0) * Q3^T\n    phi = np.pi / 6\n    psi = np.pi / 3\n    c_phi, s_phi = np.cos(phi), np.sin(phi)\n    c_psi, s_psi = np.cos(psi), np.sin(psi)\n    \n    # Rotation in (0,1)-plane\n    R01 = np.array([\n        [c_phi, -s_phi, 0, 0],\n        [s_phi, c_phi, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1]\n    ])\n    \n    # Rotation in (2,3)-plane\n    R23 = np.array([\n        [1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, c_psi, -s_psi],\n        [0, 0, s_psi, c_psi]\n    ])\n    \n    Q3 = R01 @ R23\n    D3 = np.diag([0.1, 2.0, 10.0, 20.0])\n    A3 = Q3 @ D3 @ Q3.T\n    \n    test_cases = [A1, A2, A3]\n    all_results = []\n\n    for A in test_cases:\n        # Since A is symmetric, eigh is preferred for numerical stability\n        # and guarantees orthogonal eigenvectors and real eigenvalues.\n        eigenvalues, eigenvectors = np.linalg.eigh(A)\n        \n        # The spectral radius of a symmetric positive definite matrix is its largest eigenvalue.\n        rho_A = np.max(eigenvalues)\n        \n        # The predicted maximum stable learning rate from the derivation.\n        eta_max = 2.0 / rho_A\n        all_results.append(eta_max)\n        \n        etas_to_test = {\n            'below': 0.9 * eta_max,\n            'at': eta_max,\n            'above': 1.1 * eta_max\n        }\n        \n        # Test stability for each candidate learning rate\n        for key in ['below', 'at', 'above']:\n            eta = etas_to_test[key]\n            is_stable_for_this_eta = True\n            \n            # Dimension of the parameter space\n            d = A.shape[0]\n            \n            # Iterate through each eigen-direction as an initial condition\n            for i in range(d):\n                theta_0 = eigenvectors[:, i]\n                theta = theta_0.copy()\n                \n                # Perform T steps of Gradient Descent\n                for _ in range(T):\n                    theta = theta - eta * (A @ theta)\n                \n                # Check if the final state has converged to the origin\n                final_norm = np.linalg.norm(theta)\n                if final_norm  eps:\n                    is_stable_for_this_eta = False\n                    # If one direction is unstable, no need to check others for this eta\n                    break\n            \n            all_results.append(is_stable_for_this_eta)\n            \n    # Final print statement in the exact required format.\n    # The default str() for booleans ('True'/'False') is used.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "学习率并非独立存在，它与优化过程中的其他部分（如正则化）相互作用。本练习将探究解耦权重衰减（decoupled weight decay）这一现代优化技术，并揭示其等效的 $L_2$ 惩罚强度并非一个常数，而是与权重衰减系数 $\\lambda$ 和学习率 $\\eta$ 的比值成正比。通过分析该更新规则的固定点，您将理解为何调整学习率会无意中改变正则化的效果，以及如何正确地分离这些影响以实现更可预测的模型训练 。",
            "id": "3187375",
            "problem": "要求您在使用基于梯度的优化中的解耦权重衰减时，对有效二次参数惩罚的缩放进行形式化、推导和验证。考虑一个一维二次损失和一个解耦权重衰减更新，该更新对参数应用一个乘性收缩，然后进行一个梯度步。从经验风险最小化和梯度下降动力学的第一性原理的基础出发，然后推导出迭代更新的不动点。使用推导出的表达式来确定有效二次惩罚如何依赖于学习率和解耦衰减参数，并设计一个数值程序来在一系列测试套件上验证该预测。\n\n所有数学实体必须用 LaTeX 书写。设损失为一个严格凸二次函数，由下式给出\n$$\n\\mathcal{L}(w) = \\frac{1}{2} a \\left(w - w^\\star\\right)^2,\n$$\n其中曲率为 $a  0$，最小化点为 $w^\\star$。将解耦权重衰减更新定义为：先对参数 $w$ 应用一个因子为 $(1 - \\lambda)$ 的乘性收缩，然后对原始损失 $\\mathcal{L}(w)$（不向 $\\mathcal{L}$ 添加惩罚项）进行一个学习率为 $\\eta  0$ 的梯度下降步。得到的离散时间更新为\n$$\nw_{t+1} = (1 - \\lambda) w_t - \\eta \\, \\nabla \\mathcal{L}(w_t).\n$$\n\n任务：\n1) 从可微函数上的梯度下降定义和所述的解耦权重衰减方案出发，当迭代收敛时，推导出不动点 $w_\\infty$ 的一个闭式表达式，用 $a$、$w^\\star$、$\\eta$ 和 $\\lambda$ 表示。明确写出您的答案，并根据线性系数的模来证明收敛条件。\n\n2) 使用您的不动点表达式，证明解耦收缩加梯度步在其不动点处等价于最小化一个形如下式的二次目标函数\n$$\n\\mathcal{L}(w) + \\frac{\\alpha}{2}\\,w^2,\n$$\n其中有效惩罚强度 $\\alpha$ 依赖于 $\\lambda$ 和 $\\eta$。求解 $\\alpha$ 并尽可能简化您的表达式。解释为什么在给定 $a$ 和 $w^\\star$ 的情况下，当改变 $\\eta$ 但保持比率 $\\lambda / \\eta$ 不变时，不动点保持不变，从而将学习率对瞬态动力学的纯粹影响与对正则化偏差的影响分离开来。\n\n3) 实现一个程序，对所述更新进行有限步数的数值模拟，并在以下测试套件上验证推导出的预测。在所有情况下，使用 $a = 3$、$w^\\star = 2$、$w_0 = 10$ 并运行 $T = 500$ 次迭代：\n- 情况 A（理想路径，适中设置）：$\\eta = 0.1$，$\\lambda = 0.02$。\n- 情况 B（$\\lambda / \\eta$ 与情况 A 相同但 $\\eta$ 不同）：$\\eta = 0.05$，$\\lambda = 0.01$。\n- 情况 C（$\\lambda / \\eta$ 与情况 A 不同但 $\\eta$ 与情况 A 相同）：$\\eta = 0.1$，$\\lambda = 0.01$。\n- 情况 D（无衰减的边界情况）：$\\eta = 0.1$，$\\lambda = 0$。\n\n对于每种情况，定义迭代\n$$\nw_{t+1} = (1 - \\lambda)\\,w_t - \\eta\\,a\\,(w_t - w^\\star),\n$$\n并计算：\n- $r_1$：在 $T$ 步之后，情况 A 和情况 B 的最终参数之间的绝对差，$|w_T^{\\mathrm{A}} - w_T^{\\mathrm{B}}|$。这测试了当 $\\lambda / \\eta$ 固定但 $\\eta$ 改变时的不变性。\n- $r_2$：在情况 A、B 和 C 中，模拟的最终参数与预测的不动点值 $w_\\infty = \\dfrac{a}{a + \\lambda / \\eta} w^\\star$ 之间的最大绝对差异。形式上，\n$$\nr_2 = \\max\\Big(\\big|w_T^{\\mathrm{A}} - \\tfrac{a}{a + \\lambda_{\\mathrm{A}} / \\eta_{\\mathrm{A}}} w^\\star\\big|,\\ \\big|w_T^{\\mathrm{B}} - \\tfrac{a}{a + \\lambda_{\\mathrm{B}} / \\eta_{\\mathrm{B}}} w^\\star\\big|,\\ \\big|w_T^{\\mathrm{C}} - \\tfrac{a}{a + \\lambda_{\\mathrm{C}} / \\eta_{\\mathrm{C}}} w^\\star\\big|\\Big).\n$$\n- $r_3$：在 $T$ 步之后，情况 A 和情况 C 的最终参数之间的绝对差，$|w_T^{\\mathrm{A}} - w_T^{\\mathrm{C}}|$。这测试了改变 $\\lambda / \\eta$ 会改变不动点。\n- $r_4$：情况 D 相对于未正则化最小化点 $w^\\star$ 的绝对误差，$|w_T^{\\mathrm{D}} - w^\\star|$。\n\n所有数值答案必须是无单位的实数。最终输出格式必须是单行，包含用方括号括起来的逗号分隔列表形式的结果，顺序为 $[r_1, r_2, r_3, r_4]$。\n\n您的程序必须是一个完整、可运行的程序，它能准确地产生这个单行输出，不带任何额外文本。",
            "solution": "该问题是有效的，因为它在科学上基于梯度优化原理，问题设定良好（well-posed），有明确的目标和充足的数据，并以精确、客观的语言表述。我们将继续进行推导和实现。\n\n该问题要求对一个简单的一维二次损失函数的解耦权重衰减进行分析。根据要求，解答分为三个部分：不动点和收敛条件的推导，有效二次惩罚的推导，以及数值验证。\n\n### 任务 1：不动点推导和收敛性\n\n给定损失函数 $\\mathcal{L}(w) = \\frac{1}{2} a (w - w^\\star)^2$，其中 $a  0$。该损失关于参数 $w$ 的梯度是：\n$$\n\\nabla \\mathcal{L}(w) = \\frac{d\\mathcal{L}}{dw} = a (w - w^\\star)\n$$\n在每个离散时间步 $t$，参数 $w$ 的更新规则定义为乘性收缩和梯度下降步的复合：\n$$\nw_{t+1} = (1 - \\lambda) w_t - \\eta \\, \\nabla \\mathcal{L}(w_t)\n$$\n其中 $\\lambda$ 是解耦权重衰减参数，$\\eta  0$ 是学习率。\n\n将梯度的表达式代入更新规则中，得到：\n$$\nw_{t+1} = (1 - \\lambda) w_t - \\eta a (w_t - w^\\star)\n$$\n为了找到此迭代的不动点，记为 $w_\\infty$，我们寻找一个 $w$ 的值，使得 $w_{t+1} = w_t = w_\\infty$。在不动点处，更新规则变成一个代数方程：\n$$\nw_\\infty = (1 - \\lambda) w_\\infty - \\eta a (w_\\infty - w^\\star)\n$$\n我们现在可以求解 $w_\\infty$：\n$$\nw_\\infty = w_\\infty - \\lambda w_\\infty - \\eta a w_\\infty + \\eta a w^\\star\n$$\n从两边减去 $w_\\infty$ 得到：\n$$\n0 = - \\lambda w_\\infty - \\eta a w_\\infty + \\eta a w^\\star\n$$\n对包含 $w_\\infty$ 的项进行分组：\n$$\n(\\lambda + \\eta a) w_\\infty = \\eta a w^\\star\n$$\n假设 $\\lambda + \\eta a \\neq 0$，这是成立的，因为 $a  0$，$\\eta  0$，并且我们考虑 $\\lambda \\ge 0$，我们可以分离出 $w_\\infty$：\n$$\nw_\\infty = \\frac{\\eta a}{\\lambda + \\eta a} w^\\star\n$$\n为了便于下一部分的分析，我们可以将分子和分母同时除以 $\\eta$：\n$$\nw_\\infty = \\frac{a}{(\\lambda / \\eta) + a} w^\\star\n$$\n这就是迭代不动点的闭式表达式。\n\n为了使迭代收敛到这个不动点，我们将其更新规则分析为一个线性递推关系。让我们重新排列更新规则中的项以分离出 $w_t$：\n$$\nw_{t+1} = (1 - \\lambda - \\eta a) w_t + \\eta a w^\\star\n$$\n这是一个形如 $w_{t+1} = C w_t + D$ 的一阶线性递推关系，其中系数 $C = 1 - \\lambda - \\eta a$，常数项 $D = \\eta a w^\\star$。这样的迭代收敛到一个唯一不动点的充要条件是线性系数 $C$ 的模严格小于 1，即 $|C|  1$。\n因此，收敛条件是：\n$$\n|1 - \\lambda - \\eta a|  1\n$$\n这个不等式可以展开为两个独立的不等式：\n1) $1 - \\lambda - \\eta a  1 \\implies -\\lambda - \\eta a  0 \\implies \\lambda + \\eta a  0$。由于 $a  0$，$\\eta  0$ 且 $\\lambda \\ge 0$，这个条件总是满足的。\n2) $1 - \\lambda - \\eta a  -1 \\implies 2  \\lambda + \\eta a$。\n\n综合这些，收敛的充要条件是 $0  \\lambda + \\eta a  2$。问题陈述中提供的所有数值案例都满足此条件。\n\n### 任务 2：有效二次惩罚\n\n我们现在证明不动点 $w_\\infty$ 等价于一个带有附加二次惩罚项的目标函数的最小化点。考虑正则化目标：\n$$\n\\mathcal{L}_{eff}(w) = \\mathcal{L}(w) + \\frac{\\alpha}{2} w^2 = \\frac{1}{2} a (w - w^\\star)^2 + \\frac{\\alpha}{2} w^2\n$$\n为了找到 $\\mathcal{L}_{eff}(w)$ 的最小化点，我们将其关于 $w$ 的一阶导数设为零：\n$$\n\\frac{d\\mathcal{L}_{eff}}{dw} = \\frac{d}{dw} \\left( \\frac{1}{2} a (w - w^\\star)^2 + \\frac{\\alpha}{2} w^2 \\right) = 0\n$$\n$$\na (w - w^\\star) + \\alpha w = 0\n$$\n求解 $w$：\n$$\na w - a w^\\star + \\alpha w = 0\n$$\n$$\n(a + \\alpha) w = a w^\\star\n$$\n正则化目标的最小化点是：\n$$\nw = \\frac{a}{a + \\alpha} w^\\star\n$$\n为使该最小化点与解耦权重衰减更新的不动点 $w_\\infty$ 相同，它们的表达式必须相等：\n$$\n\\frac{a}{a + \\alpha} w^\\star = \\frac{a}{(\\lambda / \\eta) + a} w^\\star\n$$\n假设 $w^\\star \\neq 0$ 且 $a \\neq 0$，我们可以令系数的分母相等：\n$$\na + \\alpha = a + \\frac{\\lambda}{\\eta}\n$$\n这给出了有效二次惩罚强度 $\\alpha$ 为：\n$$\n\\alpha = \\frac{\\lambda}{\\eta}\n$$\n这个结果表明，在不动点处，参数为 $\\lambda$、学习率为 $\\eta$ 的解耦权重衰减等价于惩罚强度为 $\\lambda / \\eta$ 的标准 L2 正则化（或权重衰减）。\n\n不动点的表达式 $w_\\infty = \\frac{a}{a + \\lambda / \\eta} w^\\star$ 仅通过比率 $\\lambda / \\eta$ 依赖于 $\\lambda$ 和 $\\eta$。因此，如果我们改变 $\\eta$ 和 $\\lambda$ 但保持它们的比率 $\\lambda / \\eta$ 不变，那么对于给定的曲率 $a$ 和未正则化的最小化点 $w^\\star$，不动点 $w_\\infty$ 保持不变。学习率 $\\eta$ 仍然影响优化过程的瞬态动力学——特别是收敛速度，它由 $|1 - \\lambda - \\eta a|$ 控制。通过保持 $\\lambda / \\eta$ 不变，可以调整学习率 $\\eta$ 来调节收敛速度，而不改变最终的正则化解（正则化偏差）。这种解耦是该优化方案的一个关键优势。\n\n### 任务 3：数值验证\n\n第三个任务要求编写一个数值程序，以模拟多个测试用例的更新规则，并验证上面推导出的理论预测。该程序将为指定的参数实现迭代 $w_{t+1} = (1 - \\lambda)\\,w_t - \\eta\\,a\\,(w_t - w^\\star)$，并计算所需的度量 $r_1$、$r_2$、$r_3$ 和 $r_4$。实现将在最终答案块中提供。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs numerical simulation of decoupled weight decay to verify theoretical predictions.\n    \"\"\"\n    # Define common parameters and test cases from the problem statement.\n    a = 3.0\n    w_star = 2.0\n    w0 = 10.0\n    T = 500\n\n    test_cases_params = {\n        'A': {'eta': 0.1, 'lambda': 0.02},\n        'B': {'eta': 0.05, 'lambda': 0.01},\n        'C': {'eta': 0.1, 'lambda': 0.01},\n        'D': {'eta': 0.1, 'lambda': 0.0}\n    }\n\n    # --- Simulation Phase ---\n    # Store the final parameter value for each case after T iterations.\n    final_ws = {}\n    for case_id, params in test_cases_params.items():\n        eta = params['eta']\n        lam = params['lambda']\n        \n        w = w0\n        for _ in range(T):\n            # The decoupled weight decay update rule for a quadratic loss.\n            w = (1.0 - lam) * w - eta * a * (w - w_star)\n        \n        final_ws[case_id] = w\n\n    # --- Verification Phase ---\n    # Calculate the required metrics r1, r2, r3, r4.\n\n    # r1: Absolute difference between Case A and Case B final parameters.\n    # Theoretical expectation: r1 should be very small as they share the same fixed point.\n    r1 = np.abs(final_ws['A'] - final_ws['B'])\n\n    # r2: Maximum absolute discrepancy between simulated and theoretical fixed points for A, B, C.\n    # The theoretical fixed point is w_inf = (a / (a + lambda/eta)) * w_star\n    w_inf_A = a / (a + test_cases_params['A']['lambda'] / test_cases_params['A']['eta']) * w_star\n    w_inf_B = a / (a + test_cases_params['B']['lambda'] / test_cases_params['B']['eta']) * w_star\n    w_inf_C = a / (a + test_cases_params['C']['lambda'] / test_cases_params['C']['eta']) * w_star\n    \n    err_A = np.abs(final_ws['A'] - w_inf_A)\n    err_B = np.abs(final_ws['B'] - w_inf_B)\n    err_C = np.abs(final_ws['C'] - w_inf_C)\n    \n    r2 = max(err_A, err_B, err_C)\n\n    # r3: Absolute difference between Case A and Case C final parameters.\n    # Theoretical expectation: r3 should be non-trivial as their fixed points differ.\n    r3 = np.abs(final_ws['A'] - final_ws['C'])\n    \n    # r4: Absolute error in Case D relative to the unregularized minimizer w_star.\n    # Case D is standard gradient descent, which should converge to w_star.\n    r4 = np.abs(final_ws['D'] - w_star)\n    \n    results = [r1, r2, r3, r4]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "与其依赖一个全局固定的学习率或预定义的时间表，我们能否在每次迭代时根据损失地貌的局部地形来调整步长呢？本练习将指导您设计这样一个自适应控制器。您将使用二阶泰勒展开作为损失函数的局部代理，来推导出一个能够防止损失过度增加的学习率，从而有效地使优化过程对局部曲率更加敏感，也更加稳健 。",
            "id": "3187311",
            "problem": "给定一个二次可微的损失函数 $\\,\\mathcal{L}:\\mathbb{R}^d\\to\\mathbb{R}\\,$, 当前的参数向量 $\\,\\theta_t\\in\\mathbb{R}^d\\,$, 其梯度 $\\,\\nabla\\mathcal{L}(\\theta_t)\\,$ (记作 $\\,\\mathbf{g}_t\\,$), 以及其Hessian矩阵 $\\,H_t=\\nabla^2\\mathcal{L}(\\theta_t)\\,$。考虑沿方向 $\\,\\mathbf{p}_t=-\\mathbf{g}_t\\,$ 进行一步学习率为 $\\,\\eta\\ge 0\\,$ 的梯度下降，使得 $\\,\\theta_{t+1}=\\theta_t+\\eta\\,\\mathbf{p}_t\\,$。请仅使用平滑函数的基本原理，即 $\\,\\mathcal{L}\\,$ 沿穿过 $\\,\\theta_t\\,$ 且方向为 $\\,\\mathbf{p}_t\\,$ 的直线的泰勒展开的一阶和二阶项，设计一个控制器来选择学习率 $\\,\\eta_{\\text{ctrl}}\\,$，以保证预测的单步损失变化不超过一个指定的非负上限 $\\,\\delta\\,$。该控制器必须遵守一个给定的基础学习率 $\\,\\eta_0\\,$，其返回值应为 $\\,\\eta_0\\,$ 和基于泰勒模型的约束所蕴含的最紧非负上界之间的最小值。你的推导必须从方向参数化的标量函数 $\\,\\varphi(s)=\\mathcal{L}(\\theta_t+s\\,\\mathbf{p}_t)\\,$ 及其在 $\\,s=0\\,$ 处的一阶和二阶方向导数的基本定义开始。\n\n具体来说，对每个测试用例，执行以下操作：\n- 计算 $\\,a=\\|\\mathbf{g}_t\\|_2^2\\,$ 和 $\\,c=\\mathbf{g}_t^\\top H_t \\mathbf{g}_t\\,$。\n- 使用 $\\,\\varphi\\,$ 在 $\\,s=0\\,$ 处的二阶泰勒展开，得到损失变化关于 $\\,\\eta\\,$ 的二次代理函数。\n- 施加约束，使得在步长为 $\\,\\eta\\,$ 时代理函数预测的损失增量小于或等于上限 $\\,\\delta\\ge 0\\,$，并由此约束推导出 $\\,\\eta\\,$ 的最紧非负上界。\n- 当该约束产生一个有限的正上界时，定义 $\\,\\eta_{\\text{ctrl}}=\\min\\{\\eta_0,\\text{推导出的上界}\\}\\,$。如果对于所有 $\\,\\eta\\ge 0\\,$ 该约束都是无约束力的，则返回 $\\,\\eta_{\\text{ctrl}}=\\eta_0\\,$。\n\n你的程序必须实现这个控制器，并将其应用于以下测试套件。每个测试用例由 $\\,(\\mathbf{g}_t,H_t,\\delta,\\eta_0)\\,$ 指定：\n\n- 情况 A: $\\,\\mathbf{g}_t=(3,\\,4)\\,$, $\\,H_t=\\mathrm{diag}(10,\\,15)\\,$, $\\,\\delta=0.05\\,$, $\\,\\eta_0=0.2\\,$.\n- 情况 B: $\\,\\mathbf{g}_t=(1,\\,-1,\\,2)\\,$, $\\,H_t=\\mathrm{diag}(-1,\\,-2,\\,0)\\,$, $\\,\\delta=0.1\\,$, $\\,\\eta_0=1.5\\,$.\n- 情况 C: $\\,\\mathbf{g}_t=(1,\\,0)\\,$, $\\,H_t=\\begin{bmatrix}2  1\\\\ 1  2\\end{bmatrix}\\,$, $\\,\\delta=0\\,$, $\\,\\eta_0=1.5\\,$.\n- 情况 D: $\\,\\mathbf{g}_t=(0,\\,0,\\,0)\\,$, $\\,H_t=\\mathrm{diag}(5,\\,7,\\,1)\\,$, $\\,\\delta=1.0\\,$, $\\,\\eta_0=10.0\\,$.\n- 情况 E: $\\,\\mathbf{g}_t=(2,\\,1,\\,2)\\,$, $\\,H_t=I_3\\,$, $\\,\\delta=100.0\\,$, $\\,\\eta_0=0.1\\,$.\n- 情况 F: $\\,\\mathbf{g}_t=(1,\\,1,\\,1,\\,1)\\,$, $\\,H_t=1000\\,I_4\\,$, $\\,\\delta=10^{-4}\\,$, $\\,\\eta_0=1.0\\,$.\n- 情况 G: $\\,\\mathbf{g}_t=(2,\\,-3)\\,$, $\\,H_t=0_{2\\times 2}\\,$, $\\,\\delta=0.2\\,$, $\\,\\eta_0=0.5\\,$.\n\n你的程序应输出与以上情况按所列顺序对应的 $\\,\\eta_{\\text{ctrl}}\\,$ 值列表，每个值四舍五入到恰好 $\\,10\\,$ 位小数。最终输出必须是单行，包含一个由方括号括起来的逗号分隔列表，例如 $\\,\\big[\\eta_{\\text{A}},\\eta_{\\text{B}},\\dots\\big]\\,$。不应打印任何其他文本。",
            "solution": "该问题要求为单步梯度下降设计一个学习率控制器。该控制器必须选择一个学习率 $\\eta_{\\text{ctrl}}$，以确保损失函数 $\\mathcal{L}$ 的预测增量不超过指定的上限 $\\delta$。该预测将基于损失函数的二阶泰勒展开。最终的学习率还必须遵守一个基础速率 $\\eta_0$。\n\n按照规定，推导从方向参数化的标量函数 $\\varphi(s) = \\mathcal{L}(\\theta_t + s\\,\\mathbf{p}_t)$ 开始，其中 $\\theta_t$ 是当前参数向量，$\\mathbf{p}_t = -\\mathbf{g}_t$ 是下降方向，且 $\\mathbf{g}_t = \\nabla\\mathcal{L}(\\theta_t)$。学习率 $\\eta$ 对应于步长 $s$。\n\n在步长为 $\\eta$ 的一步之后，损失函数的变化为 $\\Delta\\mathcal{L} = \\mathcal{L}(\\theta_t + \\eta\\,\\mathbf{p}_t) - \\mathcal{L}(\\theta_t) = \\varphi(\\eta) - \\varphi(0)$。我们使用 $\\varphi(s)$ 在 $s=0$ 附近的二阶泰勒展开来近似这个变化：\n$$\n\\varphi(s) \\approx \\varphi(0) + s\\,\\varphi'(0) + \\frac{1}{2}s^2\\,\\varphi''(0)\n$$\n因此，预测的损失变化，记作 $\\hat{\\Delta\\mathcal{L}}(\\eta)$，为：\n$$\n\\hat{\\Delta\\mathcal{L}}(\\eta) = \\varphi(\\eta) - \\varphi(0) \\approx \\eta\\,\\varphi'(0) + \\frac{1}{2}\\eta^2\\,\\varphi''(0)\n$$\n我们必须计算 $\\varphi(s)$ 在 $s=0$ 处的一阶和二阶导数。\n\n根据多变量函数的链式法则，$\\varphi(s)$ 关于 $s$ 的一阶导数是：\n$$\n\\varphi'(s) = \\frac{d}{ds}\\mathcal{L}(\\theta_t + s\\,\\mathbf{p}_t) = \\nabla\\mathcal{L}(\\theta_t + s\\,\\mathbf{p}_t)^\\top \\mathbf{p}_t\n$$\n在 $s=0$ 处求值：\n$$\n\\varphi'(0) = \\nabla\\mathcal{L}(\\theta_t)^\\top \\mathbf{p}_t = \\mathbf{g}_t^\\top \\mathbf{p}_t\n$$\n代入搜索方向的定义 $\\mathbf{p}_t = -\\mathbf{g}_t$：\n$$\n\\varphi'(0) = \\mathbf{g}_t^\\top (-\\mathbf{g}_t) = -\\|\\mathbf{g}_t\\|_2^2\n$$\n$\\varphi(s)$ 的二阶导数是：\n$$\n\\varphi''(s) = \\frac{d}{ds}\\left(\\nabla\\mathcal{L}(\\theta_t + s\\,\\mathbf{p}_t)^\\top \\mathbf{p}_t\\right) = \\mathbf{p}_t^\\top \\nabla^2\\mathcal{L}(\\theta_t + s\\,\\mathbf{p}_t) \\mathbf{p}_t = \\mathbf{p}_t^\\top H(\\theta_t + s\\,\\mathbf{p}_t) \\mathbf{p}_t\n$$\n其中 $H$ 是 $\\mathcal{L}$ 的Hessian矩阵。在 $s=0$ 处求值：\n$$\n\\varphi''(0) = \\mathbf{p}_t^\\top H(\\theta_t) \\mathbf{p}_t = \\mathbf{p}_t^\\top H_t \\mathbf{p}_t\n$$\n代入 $\\mathbf{p}_t = -\\mathbf{g}_t$：\n$$\n\\varphi''(0) = (-\\mathbf{g}_t)^\\top H_t (-\\mathbf{g}_t) = \\mathbf{g}_t^\\top H_t \\mathbf{g}_t\n$$\n使用给定的定义 $a = \\|\\mathbf{g}_t\\|_2^2$ 和 $c = \\mathbf{g}_t^\\top H_t \\mathbf{g}_t$，我们得到 $\\varphi'(0) = -a$ 和 $\\varphi''(0) = c$。\n\n损失变化关于学习率 $\\eta \\ge 0$ 的二次代理函数是：\n$$\n\\hat{\\Delta\\mathcal{L}}(\\eta) = \\eta(-a) + \\frac{1}{2}\\eta^2(c) = \\frac{1}{2}c\\eta^2 - a\\eta\n$$\n问题约束代理函数预测的损失 *增量* 不超过 $\\delta$。这可以形式化地表述为 $\\max(0, \\hat{\\Delta\\mathcal{L}}(\\eta)) \\le \\delta$。由于 $\\delta \\ge 0$，这等价于以下单个不等式：\n$$\n\\hat{\\Delta\\mathcal{L}}(\\eta) \\le \\delta\n$$\n代入二次代理函数，我们必须对 $\\eta \\ge 0$ 求解以下不等式：\n$$\n\\frac{1}{2}c\\eta^2 - a\\eta \\le \\delta \\implies \\frac{1}{2}c\\eta^2 - a\\eta - \\delta \\le 0\n$$\n我们通过考虑系数 $c$ 的符号来分析这个二次不等式。\n\n情况1：梯度为零，$\\mathbf{g}_t = \\mathbf{0}$。\n在这种情况下，$a = \\|\\mathbf{g}_t\\|_2^2 = 0$ 且 $c = \\mathbf{g}_t^\\top H_t \\mathbf{g}_t = 0$。不等式变为 $-\\delta \\le 0$。由于 $\\delta$ 给定为非负值，此不等式对任何 $\\eta \\ge 0$ 恒成立。该约束是无约束力的，没有对 $\\eta$ 施加任何上界。\n\n情况2：梯度非零，$\\mathbf{g}_t \\neq \\mathbf{0}$ (因此 $a  0$)。\n令 $f(\\eta) = \\frac{1}{2}c\\eta^2 - a\\eta - \\delta$。我们寻找当 $\\eta \\ge 0$ 时 $f(\\eta) \\le 0$ 的区域。\n\n子情况 2a: $c  0$。\n函数 $f(\\eta)$ 是一个开口向上的抛物线。它在其两根之间为非正值。$f(\\eta) = 0$ 的根由二次求根公式给出：\n$$\n\\eta = \\frac{a \\pm \\sqrt{a^2 - 4(\\frac{1}{2}c)(-\\delta)}}{2(\\frac{1}{2}c)} = \\frac{a \\pm \\sqrt{a^2 + 2c\\delta}}{c}\n$$\n判别式 $a^2 + 2c\\delta$ 是非负的，因为 $a^2 \\ge 0$, $c  0$, 且 $\\delta \\ge 0$。设两个实数根为 $\\eta_1$ 和 $\\eta_2$。由于 $\\sqrt{a^2 + 2c\\delta} \\ge \\sqrt{a^2} = a$，较小的根 $\\frac{a - \\sqrt{a^2+2c\\delta}}{c}$ 是非正的，而较大的根 $\\eta_{\\text{bound}} = \\frac{a + \\sqrt{a^2+2c\\delta}}{c}$ 是正的。不等式在两根之间的 $\\eta$ 值成立。由于我们被约束为 $\\eta \\ge 0$，有效范围是 $[0, \\eta_{\\text{bound}}]$。最紧的非负上界是 $\\eta_{\\text{bound}}$。\n\n子情况 2b: $c = 0$。\n不等式变为线性的：$-a\\eta - \\delta \\le 0$，即 $a\\eta \\ge -\\delta$。由于 $a  0$ 且 $\\delta \\ge 0$，此不等式对任何 $\\eta \\ge 0$ 恒成立。该约束是无约束力的。\n\n子情况 2c: $c  0$。\n函数 $f(\\eta)$ 是一个开口向下的抛物线。它在其两根之外为非正值。\n如果判别式 $a^2 + 2c\\delta  0$，则没有实数根，并且 $f(\\eta)$ 总是负的。因此，不等式对所有 $\\eta \\ge 0$ 都成立。该约束是无约束力的。\n如果 $a^2 + 2c\\delta \\ge 0$，则有两个实数根。由于 $c  0$ 且 $\\sqrt{a^2+2c\\delta} \\le a$，两个根 $\\frac{a \\pm \\sqrt{a^2+2c\\delta}}{c}$ 都是非正的。不等式在两根之外的 $\\eta$ 值成立。由于我们要求 $\\eta \\ge 0$，解是整个区间 $[0, \\infty)$。该约束再次是无约束力的。\n\n总而言之，仅当 $c  0$ 时，$\\eta$ 才存在一个有限的正上界。在所有其他情况（$c \\le 0$ 或 $\\mathbf{g}_t = \\mathbf{0}$）下，该约束对所有 $\\eta \\ge 0$ 都成立。\n\n因此，控制器逻辑如下：\n1.  计算 $a = \\|\\mathbf{g}_t\\|_2^2$ 和 $c = \\mathbf{g}_t^\\top H_t \\mathbf{g}_t$。\n2.  如果 $c  0$，计算上界 $\\eta_{\\text{bound}} = \\frac{a + \\sqrt{a^2 + 2c\\delta}}{c}$。得到的学习率是 $\\eta_{\\text{ctrl}} = \\min(\\eta_0, \\eta_{\\text{bound}})$。\n3.  如果 $c \\le 0$，来自约束的上界是无穷大。问题陈述中说明，在这种“无约束力”的情况下，我们应该返回 $\\eta_0$。因此，$\\eta_{\\text{ctrl}} = \\eta_0$。这个规则也正确地覆盖了 $\\mathbf{g}_t=\\mathbf{0}$ 的情况，因为此时 $c$ 将为 $0$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the controlled learning rate for several test cases based on a \n    second-order Taylor approximation of the loss function.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (g_t, H_t, delta, eta_0)\n    test_cases = [\n        # Case A\n        (np.array([3.0, 4.0]), np.diag([10.0, 15.0]), 0.05, 0.2),\n        # Case B\n        (np.array([1.0, -1.0, 2.0]), np.diag([-1.0, -2.0, 0.0]), 0.1, 1.5),\n        # Case C\n        (np.array([1.0, 0.0]), np.array([[2.0, 1.0], [1.0, 2.0]]), 0.0, 1.5),\n        # Case D\n        (np.array([0.0, 0.0, 0.0]), np.diag([5.0, 7.0, 1.0]), 1.0, 10.0),\n        # Case E\n        (np.array([2.0, 1.0, 2.0]), np.eye(3), 100.0, 0.1),\n        # Case F\n        (np.array([1.0, 1.0, 1.0, 1.0]), 1000.0 * np.eye(4), 1e-4, 1.0),\n        # Case G\n        (np.array([2.0, -3.0]), np.zeros((2, 2)), 0.2, 0.5),\n    ]\n\n    results = []\n    \n    for g, H, delta, eta0 in test_cases:\n        # a = ||g_t||_2^2\n        a = np.dot(g, g)\n        \n        # c = g_t^T * H_t * g_t\n        c = g.T @ H @ g\n\n        eta_ctrl = 0.0\n        \n        # The derivation shows a finite bound only exists if c > 0.\n        # This also handles the case g=0, where a=0 and c=0.\n        if c > 0:\n            # The quadratic inequality is (c/2)*eta^2 - a*eta - delta = 0.\n            # For c > 0, this holds for eta between the roots. We need the\n            # positive root as the upper bound.\n            discriminant = a**2 + 2 * c * delta\n            # Since a>=0, c>0, delta>=0, discriminant is non-negative.\n            eta_bound = (a + np.sqrt(discriminant)) / c\n            eta_ctrl = min(eta0, eta_bound)\n        else: # c = 0\n            # The constraint is vacuous (satisfied for all eta >= 0).\n            # The effective upper bound is infinite.\n            eta_ctrl = eta0\n            \n        results.append(f\"{eta_ctrl:.10f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}