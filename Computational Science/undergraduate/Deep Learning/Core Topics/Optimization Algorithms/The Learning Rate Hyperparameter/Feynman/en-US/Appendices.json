{
    "hands_on_practices": [
        {
            "introduction": "The learning rate is arguably the most critical hyperparameter in training deep neural networks. Set it too low, and training will be tediously slow; set it too high, and the training process can become unstable and diverge. This fundamental practice explores the theoretical basis for this instability by analyzing gradient descent on a simplified quadratic loss function. By deriving and verifying the maximum stable learning rate, you will connect the abstract concept of stability to the concrete geometric property of the loss landscape's maximum curvature, providing a solid foundation for why learning rate tuning is so essential .",
            "id": "3187300",
            "problem": "Consider minimizing a twice-differentiable quadratic objective in deep learning, where the parameter vector is denoted by $\\theta \\in \\mathbb{R}^d$, and the loss is $f(\\theta)=\\tfrac{1}{2}\\theta^\\top A\\theta$ with a symmetric positive definite matrix $A \\in \\mathbb{R}^{d \\times d}$. Gradient Descent (GD) updates are defined by the iterative map $\\theta_{t+1}=\\theta_t-\\eta\\nabla f(\\theta_t)$ for learning rate $\\eta0$. The Spectral Radius (SR) of a matrix is defined as $\\rho(M)=\\max_i |\\lambda_i(M)|$, where $\\lambda_i(M)$ are the eigenvalues of $M$. Starting from these foundational definitions, derive the necessary and sufficient condition on the learning rate $\\eta$ for the GD iteration to be asymptotically stable (i.e., $\\lim_{t\\to\\infty}\\theta_t=\\mathbf{0}$ for any initial $\\theta_0$) and express the maximum asymptotically stable $\\eta$ in terms of the Spectral Radius of $A$. Then, verify this stability condition numerically by simulating GD along eigen-directions of $A$.\n\nYour program must:\n- For each test case matrix $A$, compute the Spectral Radius $\\rho(A)$ and the predicted maximum asymptotically stable learning rate, denoted $\\eta_{\\text{max}}$.\n- Using eigenvectors of $A$, simulate the GD update $\\theta_{t+1}=\\theta_t-\\eta A\\theta_t$ for $T$ steps with initial conditions chosen as individual eigen-directions, specifically $\\theta_0=v_i$ for each eigenvector $v_i$ of $A$.\n- For each of the three candidate learning rates per test case: a value strictly below the predicted threshold ($\\eta_{\\text{below}}=0.9\\,\\eta_{\\text{max}}$), the value at the threshold ($\\eta_{\\text{at}}=\\eta_{\\text{max}}$), and a value strictly above the threshold ($\\eta_{\\text{above}}=1.1\\,\\eta_{\\text{max}}$), report whether the GD iterates converge to the origin along all eigen-directions, that is, whether $\\|\\theta_T\\|\\le \\varepsilon$ holds for every eigen-direction when simulated for $T$ steps.\n- Angle unit specification: all angles that appear in the test definitions are in radians.\n\nUse the following test suite:\n1. Matrix $A_1=\\operatorname{diag}(1.0,4.0)$, so $d=2$.\n2. Matrix $A_2=Q_2 \\operatorname{diag}(0.5,1.5,3.0) Q_2^\\top$ with rotation $Q_2=\\begin{bmatrix}\\cos(\\pi/4)-\\sin(\\pi/4)0\\\\ \\sin(\\pi/4)\\cos(\\pi/4)0\\\\ 001\\end{bmatrix}$, so $d=3$.\n3. Matrix $A_3=Q_3 \\operatorname{diag}(0.1,2.0,10.0,20.0) Q_3^\\top$ where $Q_3$ is the product of two block rotations in $\\mathbb{R}^4$: a rotation by $\\phi=\\pi/6$ in the $(0,1)$-plane and a rotation by $\\psi=\\pi/3$ in the $(2,3)$-plane (both angles in radians), so $d=4$.\n\nSimulation parameters:\n- Number of GD steps $T=200$.\n- Convergence tolerance $\\varepsilon=10^{-6}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case $i\\in\\{1,2,3\\}$, append the following four values in order:\n- The predicted $\\eta_{\\text{max}}^{(i)}$ as a floating point number,\n- A boolean for stability at $\\eta_{\\text{below}}^{(i)}$,\n- A boolean for stability at $\\eta_{\\text{at}}^{(i)}$,\n- A boolean for stability at $\\eta_{\\text{above}}^{(i)}$.\n\nThus, the final output is a flat list of $12$ entries of the form $[\\eta_{\\text{max}}^{(1)}, s_{\\text{below}}^{(1)}, s_{\\text{at}}^{(1)}, s_{\\text{above}}^{(1)}, \\eta_{\\text{max}}^{(2)}, s_{\\text{below}}^{(2)}, s_{\\text{at}}^{(2)}, s_{\\text{above}}^{(2)}, \\eta_{\\text{max}}^{(3)}, s_{\\text{below}}^{(3)}, s_{\\text{at}}^{(3)}, s_{\\text{above}}^{(3)}]$, where each $s$ is a boolean indicating whether all eigen-direction simulations for that $\\eta$ satisfy $\\|\\theta_T\\|\\le \\varepsilon$.",
            "solution": "The user wants to find the stability condition for the learning rate $\\eta$ in a Gradient Descent (GD) optimization of a quadratic objective function and then numerically verify this condition.\n\n### Problem Validation\nThe problem statement is analyzed according to the specified criteria.\n\n**Step 1: Extract Givens**\n- Objective function: $f(\\theta)=\\tfrac{1}{2}\\theta^\\top A\\theta$, where $\\theta \\in \\mathbb{R}^d$.\n- Matrix $A$: a symmetric positive definite matrix, $A \\in \\mathbb{R}^{d \\times d}$.\n- Gradient Descent (GD) update rule: $\\theta_{t+1}=\\theta_t-\\eta\\nabla f(\\theta_t)$ for a learning rate $\\eta0$.\n- Spectral Radius: $\\rho(M)=\\max_i |\\lambda_i(M)|$, where $\\lambda_i(M)$ are the eigenvalues of matrix $M$.\n- Asymptotic stability condition: $\\lim_{t\\to\\infty}\\theta_t=\\mathbf{0}$ for any initial vector $\\theta_0$.\n- Test Matrices:\n  1. $A_1=\\operatorname{diag}(1.0,4.0)$, $d=2$.\n  2. $A_2=Q_2 \\operatorname{diag}(0.5,1.5,3.0) Q_2^\\top$ with $Q_2$ being a rotation by $\\pi/4$ in the $(0,1)$-plane of $\\mathbb{R}^3$.\n  3. $A_3=Q_3 \\operatorname{diag}(0.1,2.0,10.0,20.0) Q_3^\\top$ with $Q_3$ being a product of rotations by $\\phi=\\pi/6$ in the $(0,1)$-plane and $\\psi=\\pi/3$ in the $(2,3)$-plane of $\\mathbb{R}^4$.\n- Simulation Parameters:\n  - Number of steps: $T=200$.\n  - Convergence tolerance: $\\varepsilon=10^{-6}$.\n- Numerical Verification:\n  - Initial conditions: $\\theta_0=v_i$ for each eigenvector $v_i$ of $A$.\n  - Learning rates to test: $\\eta_{\\text{below}}=0.9\\,\\eta_{\\text{max}}$, $\\eta_{\\text{at}}=\\eta_{\\text{max}}$, $\\eta_{\\text{above}}=1.1\\,\\eta_{\\text{max}}$.\n  - Stability check: $\\|\\theta_T\\|\\le \\varepsilon$ must hold for simulations starting from every eigenvector.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a standard exercise in optimization theory and numerical linear algebra.\n- **Scientifically Grounded**: The problem is based on fundamental principles of calculus (gradients of quadratic forms) and linear algebra (eigenvalue analysis of linear dynamical systems). It is a simplification of the behavior of optimizers near a local minimum, which is a core concept in deep learning.\n- **Well-Posed**: The problem is clearly defined. It requests a derivation for a specific quantity ($\\eta_{\\text{max}}$) and a numerical verification with all parameters and matrices specified. This structure leads to a unique and meaningful solution.\n- **Objective**: The problem is stated in precise, mathematical terms, free from any subjectivity or ambiguity.\n\nThe problem does not exhibit any of the invalidity flaws. It is complete, consistent, scientifically sound, and well-posed.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of the Stability Condition\n\nThe objective function is a quadratic form $f(\\theta)=\\tfrac{1}{2}\\theta^\\top A\\theta$. The gradient of this function with respect to $\\theta$ is $\\nabla f(\\theta) = A\\theta$, since $A$ is symmetric.\n\nThe Gradient Descent (GD) update rule is given as:\n$$\n\\theta_{t+1}=\\theta_t-\\eta\\nabla f(\\theta_t)\n$$\nSubstituting the gradient, we obtain a linear iterative map:\n$$\n\\theta_{t+1} = \\theta_t - \\eta A\\theta_t = (I - \\eta A)\\theta_t\n$$\nwhere $I$ is the identity matrix. Let the matrix for the iteration be $M = I - \\eta A$. The update is then $\\theta_{t+1} = M\\theta_t$. After $t$ steps, the parameter vector is given by $\\theta_t = M^t \\theta_0$.\n\nFor the iterates to converge to the zero vector, $\\lim_{t\\to\\infty}\\theta_t=\\mathbf{0}$, for any arbitrary initial vector $\\theta_0$, it is necessary and sufficient that the spectral radius of the iteration matrix $M$ be strictly less than $1$. The spectral radius is defined as the maximum absolute value of its eigenvalues.\n$$\n\\rho(M)  1\n$$\nLet $\\lambda_i(A)$ be the eigenvalues of matrix $A$ with corresponding eigenvectors $v_i$. Since $A$ is a real symmetric matrix, its eigenvalues are real. The eigenvalues of the iteration matrix $M = I - \\eta A$ can be found by applying $M$ to an eigenvector $v_i$ of $A$:\n$$\nMv_i = (I - \\eta A)v_i = Iv_i - \\eta Av_i = v_i - \\eta\\lambda_i(A)v_i = (1 - \\eta\\lambda_i(A))v_i\n$$\nThus, the eigenvalues of $M$, denoted $\\lambda_i(M)$, are $\\lambda_i(M) = 1 - \\eta\\lambda_i(A)$.\n\nThe stability condition $\\rho(M)  1$ translates to:\n$$\n\\max_i |\\lambda_i(M)| = \\max_i |1 - \\eta\\lambda_i(A)|  1\n$$\nThis must hold for all eigenvalues $\\lambda_i(A)$ of $A$. This single inequality is equivalent to the pair of inequalities:\n$$\n-1  1 - \\eta\\lambda_i(A)  1 \\quad \\text{for all } i\n$$\nWe analyze each part of the inequality:\n1.  $1 - \\eta\\lambda_i(A)  1 \\implies -\\eta\\lambda_i(A)  0$. Since the learning rate $\\eta  0$ by definition, this simplifies to $\\lambda_i(A)  0$. This condition is guaranteed to be true for all $i$ because the problem states that matrix $A$ is positive definite.\n\n2.  $-1  1 - \\eta\\lambda_i(A) \\implies \\eta\\lambda_i(A)  2 \\implies \\eta  \\frac{2}{\\lambda_i(A)}$. This inequality must hold for all eigenvalues $\\lambda_i(A)$. To ensure this, $\\eta$ must be smaller than the minimum of all values of $\\frac{2}{\\lambda_i(A)}$. This is equivalent to choosing the largest eigenvalue, $\\lambda_{\\max}(A)$, as it provides the tightest constraint on $\\eta$:\n    $$\n    \\eta  \\frac{2}{\\lambda_{\\max}(A)}\n    $$\n\nThe problem defines the spectral radius of a matrix $M$ as $\\rho(M)=\\max_i |\\lambda_i(M)|$. For the symmetric positive definite matrix $A$, all eigenvalues $\\lambda_i(A)$ are positive, so its spectral radius is simply its largest eigenvalue: $\\rho(A) = \\lambda_{\\max}(A)$.\n\nCombining these findings, the necessary and sufficient condition for the GD iteration to be asymptotically stable is:\n$$\n0  \\eta  \\frac{2}{\\rho(A)}\n$$\nThe maximum asymptotically stable learning rate, $\\eta_{\\text{max}}$, is the supremum of this interval:\n$$\n\\eta_{\\text{max}} = \\frac{2}{\\rho(A)}\n$$\n\n### Numerical Verification\nThe program will implement this derivation. For each given matrix $A$, it will first compute its eigenvalues to find $\\rho(A) = \\lambda_{\\max}(A)$ and the corresponding $\\eta_{\\text{max}}$. Then, it will simulate the GD process for $T=200$ steps starting from each of the eigenvectors of $A$. This is done for three learning rates: $\\eta_{\\text{below}}=0.9\\,\\eta_{\\text{max}}$ (expected to converge), $\\eta_{\\text{at}}=\\eta_{\\text{max}}$ (expected to be unstable for the mode corresponding to $\\lambda_{\\max}(A)$), and $\\eta_{\\text{above}}=1.1\\,\\eta_{\\text{max}}$ (expected to diverge). The stability is numerically checked by verifying if the norm of the final iterate, $\\|\\theta_T\\|$, is less than or equal to a small tolerance $\\varepsilon=10^{-6}$.\n- For $\\eta_{\\text{below}}$, we expect $|1 - \\eta\\lambda_i(A)|  1$ for all $i$, leading to convergence.\n- For $\\eta_{\\text{at}}$, the mode corresponding to $\\lambda_{\\max}(A)$ will have an eigenvalue of $1 - \\frac{2}{\\lambda_{\\max}(A)}\\lambda_{\\max}(A) = -1$. The iterate magnitude for this mode will not decay, failing the convergence test.\n- For $\\eta_{\\text{above}}$, the mode corresponding to $\\lambda_{\\max}(A)$ will have an eigenvalue with magnitude greater than $1$, causing divergence.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and numerically verifies the stability condition for Gradient Descent\n    on a quadratic objective.\n    \"\"\"\n    # Simulation parameters from the problem statement\n    T = 200\n    eps = 1e-6\n    \n    # === Define Test Cases ===\n\n    # Test Case 1: A1 = diag(1.0, 4.0)\n    A1 = np.diag([1.0, 4.0])\n\n    # Test Case 2: A2 = Q2 * diag(0.5, 1.5, 3.0) * Q2^T\n    angle2 = np.pi / 4\n    c2, s2 = np.cos(angle2), np.sin(angle2)\n    Q2 = np.array([\n        [c2, -s2, 0],\n        [s2, c2, 0],\n        [0, 0, 1]\n    ])\n    D2 = np.diag([0.5, 1.5, 3.0])\n    A2 = Q2 @ D2 @ Q2.T\n\n    # Test Case 3: A3 = Q3 * diag(0.1, 2.0, 10.0, 20.0) * Q3^T\n    phi = np.pi / 6\n    psi = np.pi / 3\n    c_phi, s_phi = np.cos(phi), np.sin(phi)\n    c_psi, s_psi = np.cos(psi), np.sin(psi)\n    \n    # Rotation in (0,1)-plane\n    R01 = np.array([\n        [c_phi, -s_phi, 0, 0],\n        [s_phi, c_phi, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1]\n    ])\n    \n    # Rotation in (2,3)-plane\n    R23 = np.array([\n        [1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, c_psi, -s_psi],\n        [0, 0, s_psi, c_psi]\n    ])\n    \n    Q3 = R01 @ R23\n    D3 = np.diag([0.1, 2.0, 10.0, 20.0])\n    A3 = Q3 @ D3 @ Q3.T\n    \n    test_cases = [A1, A2, A3]\n    all_results = []\n\n    for A in test_cases:\n        # Since A is symmetric, eigh is preferred for numerical stability\n        # and guarantees orthogonal eigenvectors and real eigenvalues.\n        eigenvalues, eigenvectors = np.linalg.eigh(A)\n        \n        # The spectral radius of a symmetric positive definite matrix is its largest eigenvalue.\n        rho_A = np.max(eigenvalues)\n        \n        # The predicted maximum stable learning rate from the derivation.\n        eta_max = 2.0 / rho_A\n        all_results.append(eta_max)\n        \n        etas_to_test = {\n            'below': 0.9 * eta_max,\n            'at': eta_max,\n            'above': 1.1 * eta_max\n        }\n        \n        # Test stability for each candidate learning rate\n        for key in ['below', 'at', 'above']:\n            eta = etas_to_test[key]\n            is_stable_for_this_eta = True\n            \n            # Dimension of the parameter space\n            d = A.shape[0]\n            \n            # Iterate through each eigen-direction as an initial condition\n            for i in range(d):\n                theta_0 = eigenvectors[:, i]\n                theta = theta_0.copy()\n                \n                # Perform T steps of Gradient Descent\n                for _ in range(T):\n                    theta = theta - eta * (A @ theta)\n                \n                # Check if the final state has converged to the origin\n                final_norm = np.linalg.norm(theta)\n                if final_norm  eps:\n                    is_stable_for_this_eta = False\n                    # If one direction is unstable, no need to check others for this eta\n                    break\n            \n            all_results.append(is_stable_for_this_eta)\n            \n    # Final print statement in the exact required format.\n    # The default str() for booleans ('True'/'False') is used.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While standard gradient descent moves in the direction of the steepest descent, momentum methods accelerate training by accumulating a velocity in promising directions. However, adding momentum is not a free lunch; it interacts directly with the learning rate, changing the dynamics of the parameter updates. This practice reveals the crucial interplay between the learning rate $\\eta$ and the momentum coefficient $\\beta$ by formalizing the concept of an \"effective learning rate.\" By understanding and verifying that the steady-state step size scales with $\\eta / (1 - \\beta)$, you will learn how to co-adapt these two hyperparameters to achieve consistent training behavior, a vital skill for practical deep learning .",
            "id": "3187263",
            "problem": "Consider Gradient Descent with Momentum (GDM), also known as the heavy-ball method. Let a parameter vector be denoted by $w_t \\in \\mathbb{R}^d$ at iteration $t$, the gradient by $g_t \\in \\mathbb{R}^d$, the momentum coefficient by $\\beta \\in [0,1)$, and the learning rate by $\\eta  0$. The update equations are the following fundamental definitions: velocity $v_t \\in \\mathbb{R}^d$ obeys $v_{t+1} = \\beta v_t + g_t$, and parameters update as $w_{t+1} = w_t - \\eta v_{t+1}$. Assume $v_0 = 0$ and $w_0$ is given.\n\nTask 1 (derivation): Starting solely from these update equations, and without assuming any shortcut formulas, derive the steady-state per-iteration parameter update under a constant gradient regime where $g_t = g$ is a fixed nonzero vector, and $\\beta \\in [0,1)$. Express your result in terms of $g$, $\\eta$, and $\\beta$, and provide the mathematical conditions under which the derivation is valid.\n\nTask 2 (computational test): In one dimension ($d = 1$), implement a program that simulates GDM under a constant scalar gradient $g \\in \\mathbb{R}$. For a given $(\\beta,\\eta)$ pair, run $T$ iterations starting from $v_0 = 0$ and $w_0 = 0$, record the per-iteration update magnitudes $\\Delta_t = |w_{t+1} - w_t|$, and compute the average over the last $K$ iterations. Define the notion of equalized late-stage training dynamics across different $\\beta$ values as follows: for a set of $\\beta$ values, if the maximum minus minimum of the computed averages (over the last $K$ iterations) is less than or equal to a tolerance $\\varepsilon$, then the dynamics are considered equalized.\n\nTest Suite: Your program must evaluate the following four test cases and output a boolean for each case indicating whether late-stage training dynamics are equalized according to the above criterion. All scalar numbers in the test suite below are dimensionless.\n\n- Test 1 (general case): Constant gradient $g = 1.2$, momentum coefficients $\\beta \\in \\{0.0, 0.5, 0.9\\}$, target steady-state scaling parameter $c = 0.05$, choose learning rates to satisfy $\\eta = c \\cdot (1 - \\beta)$ for each $\\beta$, number of iterations $T = 200$, last-window length $K = 50$, tolerance $\\varepsilon = 10^{-6}$.\n\n- Test 2 (boundary near $\\beta \\to 1$): Constant gradient $g = 0.7$, momentum coefficients $\\beta \\in \\{0.0, 0.99\\}$, choose $\\eta = c \\cdot (1 - \\beta)$ with $c = 0.1$, number of iterations $T = 3000$, last-window length $K = 500$, tolerance $\\varepsilon = 10^{-6}$.\n\n- Test 3 (mismatched learning rates): Constant gradient $g = 1.0$, momentum coefficients $\\beta \\in \\{0.8, 0.9\\}$, use the same learning rate $\\eta = 0.01$ for both $\\beta$ values (do not adjust by $(1 - \\beta)$), number of iterations $T = 200$, last-window length $K = 50$, tolerance $\\varepsilon = 10^{-6}$.\n\n- Test 4 (zero gradient edge case): Constant gradient $g = 0.0$, momentum coefficients $\\beta \\in \\{0.0, 0.95, 0.99\\}$, choose $\\eta = c \\cdot (1 - \\beta)$ with $c = 0.2$, number of iterations $T = 100$, last-window length $K = 50$, tolerance $\\varepsilon = 10^{-9}$.\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4]$, where each $r_i$ is a boolean corresponding to Test $i$ in the order listed above.",
            "solution": "The problem statement has been validated and found to be scientifically sound, well-posed, and complete. It describes a standard optimization algorithm, Gradient Descent with Momentum (GDM), and poses two tasks: a mathematical derivation of its steady-state behavior under a constant gradient, and a computational test to verify this behavior under specific hyperparameter settings. All parameters and conditions are clearly defined.\n\n### Task 1: Derivation of Steady-State Update\n\nThe GDM update equations are given as:\n$$v_{t+1} = \\beta v_t + g_t$$\n$$w_{t+1} = w_t - \\eta v_{t+1}$$\nwith initial conditions $v_0 = 0$ and a given $w_0$. The parameters are the momentum coefficient $\\beta \\in [0,1)$ and the learning rate $\\eta  0$. We are asked to analyze the system under a constant gradient regime, where $g_t = g$ for all iterations $t$, with $g$ being a fixed non-zero vector.\n\nThe goal is to find the steady-state per-iteration parameter update, which we denote as $\\Delta w_{ss} = \\lim_{t \\to \\infty} (w_{t+1} - w_t)$. The parameter update at any iteration $t$ is $\\Delta w_t = w_{t+1} - w_t = -\\eta v_{t+1}$. Therefore, deriving the steady-state update requires finding the steady-state velocity, $v_{ss} = \\lim_{t \\to \\infty} v_t$.\n\nLet's analyze the recurrence relation for the velocity vector $v_t$:\n$$v_{t+1} = \\beta v_t + g$$\nWe can unroll this recurrence starting from $v_0 = 0$:\nAt $t=0$: $v_1 = \\beta v_0 + g = \\beta(0) + g = g$\nAt $t=1$: $v_2 = \\beta v_1 + g = \\beta g + g = (1 + \\beta)g$\nAt $t=2$: $v_3 = \\beta v_2 + g = \\beta(1+\\beta)g + g = (1 + \\beta + \\beta^2)g$\n\nBy induction, we can see the general form for $v_t$ at any iteration $t  0$:\n$$v_t = \\left(\\sum_{i=0}^{t-1} \\beta^i\\right) g$$\nThe summation is a finite geometric series. For $\\beta \\neq 1$, the sum is given by:\n$$\\sum_{i=0}^{t-1} \\beta^i = \\frac{1 - \\beta^t}{1 - \\beta}$$\nThus, the velocity at iteration $t$ is:\n$$v_t = \\frac{1 - \\beta^t}{1 - \\beta} g$$\nTo find the steady-state velocity $v_{ss}$, we take the limit as $t \\to \\infty$:\n$$v_{ss} = \\lim_{t \\to \\infty} v_t = \\lim_{t \\to \\infty} \\left(\\frac{1 - \\beta^t}{1 - \\beta}\\right) g$$\nThis limit converges if and only if the term $\\beta^t$ converges. The problem states that $\\beta \\in [0, 1)$, which is a stricter condition than $|\\beta|  1$ but sufficient. Under this condition, $\\lim_{t \\to \\infty} \\beta^t = 0$.\n\nTherefore, the steady-state velocity is:\n$$v_{ss} = \\frac{1 - 0}{1 - \\beta} g = \\frac{g}{1 - \\beta}$$\nThe velocity exponentially approaches this terminal value. The term $1/(1-\\beta)$ acts as a scalar that amplifies the gradient $g$.\n\nNow, we can find the steady-state per-iteration parameter update, $\\Delta w_{ss}$. In the steady state, $v_{t+1} \\approx v_{ss}$, so:\n$$\\Delta w_{ss} = w_{t+1} - w_t = -\\eta v_{ss}$$\nSubstituting the expression for $v_{ss}$, we get:\n$$\\Delta w_{ss} = -\\eta \\left(\\frac{g}{1 - \\beta}\\right)$$\nThis is the derived steady-state per-iteration parameter update. The derivation is valid under the condition that the geometric series for the velocity converges, which is guaranteed by the given constraint $\\beta \\in [0, 1)$.\n\n### Task 2: Computational Test and Analysis\n\nThe computational task requires simulating GDM in one dimension ($d=1$) and evaluating a criterion for \"equalized late-stage training dynamics.\" This criterion checks if the average update magnitudes, $\\bar{\\Delta} = \\text{avg}_{t \\in \\{T-K, \\dots, T-1\\}} |w_{t+1} - w_t|$, are similar across different values of $\\beta$.\n\nFrom our derivation, the magnitude of the steady-state update is:\n$$|\\Delta w_{ss}| = \\left|-\\eta \\frac{g}{1 - \\beta}\\right| = \\frac{\\eta |g|}{1 - \\beta}$$\nThis formula is key to understanding the test cases.\n\n**Analysis of Test Cases 1, 2, and 4:**\nIn these tests, the learning rate $\\eta$ is coupled with the momentum coefficient $\\beta$ via the rule $\\eta = c \\cdot (1 - \\beta)$ for some constant $c$. Let's substitute this into our steady-state update magnitude formula:\n$$|\\Delta w_{ss}| = \\frac{(c \\cdot (1 - \\beta)) |g|}{1 - \\beta} = c|g|$$\nThis result is profound: when $\\eta$ is scaled by $(1 - \\beta)$, the steady-state update magnitude becomes independent of $\\beta$. It depends only on the constant scalar $c$ and the magnitude of the gradient $g$.\n\n- **Test 1 ($g=1.2, c=0.05$)  Test 2 ($g=0.7, c=0.1$):** For these tests, since $g \\neq 0$, we expect the system for each $\\beta$ to converge to a non-zero steady-state update magnitude of $c|g|$. For Test 1, $|\\Delta w_{ss}| = 0.05 \\times 1.2 = 0.06$. For Test 2, $|\\Delta w_{ss}| = 0.1 \\times 0.7 = 0.07$. Since this theoretical value is the same for all $\\beta$ values within a given test, we expect the difference between the maximum and minimum of the simulated average update magnitudes to be very small (within the tolerance $\\varepsilon$). The number of iterations $T$ is chosen to be much larger than the characteristic convergence time scale (which is on the order of $1/(1-\\beta)$), ensuring the system is in steady state during the final $K$ iterations. Therefore, we predict the dynamics will be equalized, and the result should be `True` for both tests.\n\n- **Test 4 ($g=0.0, c=0.2$):** Here, the gradient is zero. The velocity update is $v_{t+1} = \\beta v_t$. With $v_0 = 0$, it follows that $v_t = 0$ for all $t \\geq 1$. Consequently, the parameter update $w_{t+1} - w_t = -\\eta v_{t+1}$ is always zero. The average update magnitude over the last $K$ iterations will be $0$ for all values of $\\beta$. The difference between the maximum ($0$) and minimum ($0$) average is $0$, which is less than or equal to the tolerance $\\varepsilon = 10^{-9}$. We predict the result to be `True`.\n\n**Analysis of Test Case 3:**\nIn this test, the learning rate $\\eta = 0.01$ is held constant for two different $\\beta$ values, $\\beta_1 = 0.8$ and $\\beta_2 = 0.9$. The rule $\\eta = c(1-\\beta)$ is not used. We must use the general formula for the steady-state update magnitude: $|\\Delta w_{ss}| = \\frac{\\eta |g|}{1 - \\beta}$.\n\n- For $\\beta_1 = 0.8$: $|\\Delta w_{ss}| = \\frac{0.01 \\times |1.0|}{1 - 0.8} = \\frac{0.01}{0.2} = 0.05$.\n- For $\\beta_2 = 0.9$: $|\\Delta w_{ss}| = \\frac{0.01 \\times |1.0|}{1 - 0.9} = \\frac{0.01}{0.1} = 0.10$.\n\nThe theoretical steady-state update magnitudes are significantly different ($0.05$ vs $0.10$). The difference is $0.05$, which is much larger than the tolerance $\\varepsilon = 10^{-6}$. Therefore, we predict the dynamics will not be equalized, and the result for this test should be `False`.\n\n**Summary of Predictions:**\n- Test 1: `True`\n- Test 2: `True`\n- Test 3: `False`\n- Test 4: `True`\n\nThe following program implements the simulation to verify these predictions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_gdm_simulation(g: float, beta: float, eta: float, T: int, K: int) - float:\n    \"\"\"\n    Simulates Gradient Descent with Momentum for a constant scalar gradient.\n\n    Args:\n        g: The constant scalar gradient.\n        beta: The momentum coefficient.\n        eta: The learning rate.\n        T: The total number of iterations.\n        K: The number of last iterations to average over.\n\n    Returns:\n        The average update magnitude over the last K iterations.\n    \"\"\"\n    v = 0.0\n    update_magnitudes = []\n\n    for _ in range(T):\n        # Velocity update: v_{t+1} = beta * v_t + g_t\n        # In our case, v_t is the current v, and g_t is the constant g.\n        v = beta * v + g\n\n        # Parameter update magnitude: |w_{t+1} - w_t| = |-eta * v_{t+1}|\n        # The newly computed v is effectively v_{t+1}.\n        update_mag = eta * abs(v)\n        update_magnitudes.append(update_mag)\n        # Note: We do not need to track the parameter w itself, as only the\n        # magnitude of its change is required for the analysis.\n\n    # Calculate the average over the last K iterations.\n    # The slice update_magnitudes[T-K:] extracts the last K elements.\n    avg_update = np.mean(update_magnitudes[T - K:])\n    return avg_update\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for GDM dynamics equalization.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test 1 (general case)\n        {\n            'g': 1.2, \n            'betas': [0.0, 0.5, 0.9], \n            'eta_rule': 'c(1-beta)', \n            'c': 0.05,\n            'T': 200, \n            'K': 50, \n            'eps': 1e-6\n        },\n        # Test 2 (boundary near beta - 1)\n        {\n            'g': 0.7, \n            'betas': [0.0, 0.99], \n            'eta_rule': 'c(1-beta)',\n            'c': 0.1,\n            'T': 3000, \n            'K': 500, \n            'eps': 1e-6\n        },\n        # Test 3 (mismatched learning rates)\n        {\n            'g': 1.0, \n            'betas': [0.8, 0.9], \n            'eta_rule': 'fixed',\n            'eta': 0.01,\n            'T': 200, \n            'K': 50, \n            'eps': 1e-6\n        },\n        # Test 4 (zero gradient edge case)\n        {\n            'g': 0.0, \n            'betas': [0.0, 0.95, 0.99], \n            'eta_rule': 'c(1-beta)',\n            'c': 0.2,\n            'T': 100, \n            'K': 50, \n            'eps': 1e-9\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        averages = []\n        for beta in case['betas']:\n            # Determine the learning rate based on the rule for the test case\n            if case['eta_rule'] == 'c(1-beta)':\n                eta = case['c'] * (1.0 - beta)\n            else:  # 'fixed' rule\n                eta = case['eta']\n            \n            # Run the simulation and get the average update magnitude\n            avg_mag = run_gdm_simulation(\n                g=case['g'], \n                beta=beta, \n                eta=eta, \n                T=case['T'], \n                K=case['K']\n            )\n            averages.append(avg_mag)\n        \n        # Check if the dynamics are equalized\n        if len(averages)  0:\n            diff = max(averages) - min(averages)\n            is_equalized = diff = case['eps']\n        else:\n            is_equalized = True # Vacuously true for empty set of betas\n            \n        results.append(is_equalized)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Choosing a single fixed learning rate for an entire training run is a simplifying assumption. In reality, the ideal step size changes as we traverse the complex, non-uniform landscape of the loss function. This advanced exercise guides you to build a simple adaptive learning rate controller, moving beyond fixed hyperparameters. Using a second-order Taylor approximation to create a local model of the loss function, you will design a mechanism that caps the potential increase in loss at each step. This practice introduces the core principle behind sophisticated adaptive optimizers, demonstrating how local gradient and curvature information can be used to make optimization steps both faster and safer .",
            "id": "3187311",
            "problem": "You are given a twice-differentiable loss function $\\mathcal{L}: \\mathbb{R}^d \\to \\mathbb{R}$, a current parameter vector $\\theta_t \\in \\mathbb{R}^d$, its gradient $\\nabla\\mathcal{L}(\\theta_t)$ denoted $\\mathbf{g}_t$, and its Hessian $H_t = \\nabla^2\\mathcal{L}(\\theta_t)$. Consider one step of gradient descent with learning rate $\\eta \\ge 0$ along the direction $\\mathbf{p}_t = -\\mathbf{g}_t$ so that $\\theta_{t+1} = \\theta_t + \\eta\\,\\mathbf{p}_t$. Using only first principles for smooth functions, namely the first- and second-order terms of the Taylor expansion of $\\mathcal{L}$ along a line through $\\theta_t$ in direction $\\mathbf{p}_t$, design a controller that chooses a learning rate $\\eta_{\\text{ctrl}}$ which guarantees that the predicted one-step change in loss does not exceed a specified nonnegative cap $\\delta$. The controller must respect a provided base learning rate $\\eta_0$ by returning the minimum between $\\eta_0$ and the tightest nonnegative bound implied by the Taylor-model-based constraint. Your derivation must start from the directionally parameterized scalar function $\\varphi(s) = \\mathcal{L}(\\theta_t + s\\,\\mathbf{p}_t)$ and fundamental definitions of its first and second directional derivatives at $s=0$.\n\nPrecisely, for each test case, do the following:\n- Compute $a=\\|\\mathbf{g}_t\\|_2^2$ and $c=\\mathbf{g}_t^\\top H_t \\mathbf{g}_t$.\n- Use the second-order Taylor expansion of $\\varphi$ at $s=0$ to obtain a quadratic proxy for the change in loss as a function of $\\eta$.\n- Impose that the proxy-predicted increase in loss at step size $\\eta$ is less than or equal to the cap $\\delta\\ge 0$, and derive the tightest nonnegative upper bound on $\\eta$ implied by this constraint.\n- Define $\\eta_{\\text{ctrl}}=\\min\\{\\eta_0,\\text{the derived upper bound}\\}$ when the constraint yields a finite positive upper bound. If the constraint is vacuous for all $\\eta\\ge 0$, return $\\eta_{\\text{ctrl}}=\\eta_0$.\n\nYour program must implement this controller and apply it to the following test suite. Each test case is specified by $(\\mathbf{g}_t, H_t, \\delta, \\eta_0)$:\n\n- Case A: $\\mathbf{g}_t=(3, 4)$, $H_t=\\mathrm{diag}(10, 15)$, $\\delta=0.05$, $\\eta_0=0.2$.\n- Case B: $\\mathbf{g}_t=(1, -1, 2)$, $H_t=\\mathrm{diag}(-1, -2, 0)$, $\\delta=0.1$, $\\eta_0=1.5$.\n- Case C: $\\mathbf{g}_t=(1, 0)$, $H_t=\\begin{bmatrix}2  1 \\\\ 1  2\\end{bmatrix}$, $\\delta=0$, $\\eta_0=1.5$.\n- Case D: $\\mathbf{g}_t=(0, 0, 0)$, $H_t=\\mathrm{diag}(5, 7, 1)$, $\\delta=1.0$, $\\eta_0=10.0$.\n- Case E: $\\mathbf{g}_t=(2, 1, 2)$, $H_t=I_3$, $\\delta=100.0$, $\\eta_0=0.1$.\n- Case F: $\\mathbf{g}_t=(1, 1, 1, 1)$, $H_t=1000\\,I_4$, $\\delta=10^{-4}$, $\\eta_0=1.0$.\n- Case G: $\\mathbf{g}_t=(2, -3)$, $H_t=0_{2\\times 2}$, $\\delta=0.2$, $\\eta_0=0.5$.\n\nYour program should output the list of $\\eta_{\\text{ctrl}}$ values corresponding to the cases in the order listed, each rounded to exactly $10$ decimal places. The final output must be a single line containing a comma-separated list enclosed in square brackets, for example $[\\eta_{\\text{A}}, \\eta_{\\text{B}}, \\dots]$. No other text should be printed.",
            "solution": "The problem requires the design of a learning rate controller for a single step of gradient descent. The controller must select a learning rate $\\eta_{\\text{ctrl}}$ that ensures the predicted increase in the loss function $\\mathcal{L}$ does not exceed a specified cap $\\delta$. The prediction is to be based on a second-order Taylor expansion of the loss function. The final learning rate must also respect a base rate $\\eta_0$.\n\nThe derivation begins, as stipulated, with the directionally parameterized scalar function $\\varphi(s) = \\mathcal{L}(\\theta_t + s\\,\\mathbf{p}_t)$, where $\\theta_t$ is the current parameter vector and $\\mathbf{p}_t = -\\mathbf{g}_t$ is the descent direction, with $\\mathbf{g}_t = \\nabla\\mathcal{L}(\\theta_t)$. The learning rate $\\eta$ corresponds to the step size $s$.\n\nThe change in the loss function after a step of size $\\eta$ is $\\Delta\\mathcal{L} = \\mathcal{L}(\\theta_t + \\eta\\,\\mathbf{p}_t) - \\mathcal{L}(\\theta_t) = \\varphi(\\eta) - \\varphi(0)$. We approximate this change using the second-order Taylor expansion of $\\varphi(s)$ around $s=0$:\n$$\n\\varphi(s) \\approx \\varphi(0) + s\\,\\varphi'(0) + \\frac{1}{2}s^2\\,\\varphi''(0)\n$$\nThe predicted change in loss, denoted $\\hat{\\Delta\\mathcal{L}}(\\eta)$, is therefore:\n$$\n\\hat{\\Delta\\mathcal{L}}(\\eta) = \\varphi(\\eta) - \\varphi(0) \\approx \\eta\\,\\varphi'(0) + \\frac{1}{2}\\eta^2\\,\\varphi''(0)\n$$\nWe must compute the first and second derivatives of $\\varphi(s)$ at $s=0$.\n\nBy the chain rule for multivariable functions, the first derivative of $\\varphi(s)$ with respect to $s$ is:\n$$\n\\varphi'(s) = \\frac{d}{ds}\\mathcal{L}(\\theta_t + s\\,\\mathbf{p}_t) = \\nabla\\mathcal{L}(\\theta_t + s\\,\\mathbf{p}_t)^\\top \\mathbf{p}_t\n$$\nEvaluating at $s=0$:\n$$\n\\varphi'(0) = \\nabla\\mathcal{L}(\\theta_t)^\\top \\mathbf{p}_t = \\mathbf{g}_t^\\top \\mathbf{p}_t\n$$\nSubstituting the definition of the search direction, $\\mathbf{p}_t = -\\mathbf{g}_t$:\n$$\n\\varphi'(0) = \\mathbf{g}_t^\\top (-\\mathbf{g}_t) = -\\|\\mathbf{g}_t\\|_2^2\n$$\nThe second derivative of $\\varphi(s)$ is:\n$$\n\\varphi''(s) = \\frac{d}{ds}\\left(\\nabla\\mathcal{L}(\\theta_t + s\\,\\mathbf{p}_t)^\\top \\mathbf{p}_t\\right) = \\mathbf{p}_t^\\top \\nabla^2\\mathcal{L}(\\theta_t + s\\,\\mathbf{p}_t) \\mathbf{p}_t = \\mathbf{p}_t^\\top H(\\theta_t + s\\,\\mathbf{p}_t) \\mathbf{p}_t\n$$\nwhere $H$ is the Hessian of $\\mathcal{L}$. Evaluating at $s=0$:\n$$\n\\varphi''(0) = \\mathbf{p}_t^\\top H(\\theta_t) \\mathbf{p}_t = \\mathbf{p}_t^\\top H_t \\mathbf{p}_t\n$$\nSubstituting $\\mathbf{p}_t = -\\mathbf{g}_t$:\n$$\n\\varphi''(0) = (-\\mathbf{g}_t)^\\top H_t (-\\mathbf{g}_t) = \\mathbf{g}_t^\\top H_t \\mathbf{g}_t\n$$\nUsing the provided definitions $a = \\|\\mathbf{g}_t\\|_2^2$ and $c = \\mathbf{g}_t^\\top H_t \\mathbf{g}_t$, we have $\\varphi'(0) = -a$ and $\\varphi''(0) = c$.\n\nThe quadratic proxy for the loss change as a function of the learning rate $\\eta \\ge 0$ is:\n$$\n\\hat{\\Delta\\mathcal{L}}(\\eta) = \\eta(-a) + \\frac{1}{2}\\eta^2(c) = \\frac{1}{2}c\\eta^2 - a\\eta\n$$\nThe problem constrains the proxy-predicted *increase* in loss to be no more than $\\delta$. This is formally stated as $\\max(0, \\hat{\\Delta\\mathcal{L}}(\\eta)) \\le \\delta$. As $\\delta \\ge 0$, this is equivalent to the single inequality:\n$$\n\\hat{\\Delta\\mathcal{L}}(\\eta) \\le \\delta\n$$\nSubstituting the quadratic proxy, we must solve the following inequality for $\\eta \\ge 0$:\n$$\n\\frac{1}{2}c\\eta^2 - a\\eta \\le \\delta \\implies \\frac{1}{2}c\\eta^2 - a\\eta - \\delta \\le 0\n$$\nWe analyze this quadratic inequality by considering the sign of the coefficient $c$.\n\nCase 1: The gradient is zero, $\\mathbf{g}_t = \\mathbf{0}$.\nIn this case, $a = \\|\\mathbf{g}_t\\|_2^2 = 0$ and $c = \\mathbf{g}_t^\\top H_t \\mathbf{g}_t = 0$. The inequality becomes $-\\delta \\le 0$. Since $\\delta$ is given as nonnegative, this is always true for any $\\eta \\ge 0$. The constraint is vacuous, imposing no upper bound on $\\eta$.\n\nCase 2: The gradient is non-zero, $\\mathbf{g}_t \\neq \\mathbf{0}$ (so $a  0$).\nLet $f(\\eta) = \\frac{1}{2}c\\eta^2 - a\\eta - \\delta$. We seek the region where $f(\\eta) \\le 0$ for $\\eta \\ge 0$.\n\nSubcase 2a: $c  0$.\nThe function $f(\\eta)$ is an upward-opening parabola. It is non-positive between its roots. The roots of $f(\\eta) = 0$ are given by the quadratic formula:\n$$\n\\eta = \\frac{a \\pm \\sqrt{a^2 - 4(\\frac{1}{2}c)(-\\delta)}}{2(\\frac{1}{2}c)} = \\frac{a \\pm \\sqrt{a^2 + 2c\\delta}}{c}\n$$\nThe discriminant $a^2 + 2c\\delta$ is non-negative since $a^2 \\ge 0$, $c  0$, and $\\delta \\ge 0$. Let the two real roots be $\\eta_1$ and $\\eta_2$. Since $\\sqrt{a^2 + 2c\\delta} \\ge \\sqrt{a^2} = a$, the smaller root $\\frac{a - \\sqrt{a^2+2c\\delta}}{c}$ is non-positive, and the larger root $\\eta_{\\text{bound}} = \\frac{a + \\sqrt{a^2+2c\\delta}}{c}$ is positive. The inequality holds for $\\eta$ between the roots. As we are constrained to $\\eta \\ge 0$, the valid range is $[0, \\eta_{\\text{bound}}]$. The tightest nonnegative upper bound is $\\eta_{\\text{bound}}$.\n\nSubcase 2b: $c = 0$.\nThe inequality becomes linear: $-a\\eta - \\delta \\le 0$, or $a\\eta \\ge -\\delta$. Since $a  0$ and $\\delta \\ge 0$, this is always true for any $\\eta \\ge 0$. The constraint is vacuous.\n\nSubcase 2c: $c  0$.\nThe function $f(\\eta)$ is a downward-opening parabola. It is non-positive outside its roots.\nIf the discriminant $a^2 + 2c\\delta  0$, there are no real roots, and $f(\\eta)$ is always negative. Thus, the inequality is satisfied for all $\\eta \\ge 0$. The constraint is vacuous.\nIf $a^2 + 2c\\delta \\ge 0$, there are two real roots. Since $c0$ and $\\sqrt{a^2+2c\\delta} \\le a$, both roots $\\frac{a \\pm \\sqrt{a^2+2c\\delta}}{c}$ are non-positive. The inequality holds for $\\eta$ outside the roots. Since we require $\\eta \\ge 0$, the solution is the entire interval $[0, \\infty)$. The constraint is again vacuous.\n\nIn summary, a finite positive upper bound on $\\eta$ exists only when $c  0$. In all other scenarios ($c \\le 0$ or $\\mathbf{g}_t = \\mathbf{0}$), the constraint is satisfied for all $\\eta \\ge 0$.\n\nThe controller logic is therefore as follows:\n1.  Compute $a = \\|\\mathbf{g}_t\\|_2^2$ and $c = \\mathbf{g}_t^\\top H_t \\mathbf{g}_t$.\n2.  If $c  0$, calculate the upper bound $\\eta_{\\text{bound}} = \\frac{a + \\sqrt{a^2 + 2c\\delta}}{c}$. The resulting learning rate is $\\eta_{\\text{ctrl}} = \\min(\\eta_0, \\eta_{\\text{bound}})$.\n3.  If $c \\le 0$, the upper bound from the constraint is infinite. The problem states that in this \"vacuous\" case, we should return $\\eta_0$. Thus, $\\eta_{\\text{ctrl}} = \\eta_0$. This rule also correctly covers the $\\mathbf{g}_t=\\mathbf{0}$ case, as $c$ will be $0$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the controlled learning rate for several test cases based on a \n    second-order Taylor approximation of the loss function.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (g_t, H_t, delta, eta_0)\n    test_cases = [\n        # Case A\n        (np.array([3.0, 4.0]), np.diag([10.0, 15.0]), 0.05, 0.2),\n        # Case B\n        (np.array([1.0, -1.0, 2.0]), np.diag([-1.0, -2.0, 0.0]), 0.1, 1.5),\n        # Case C\n        (np.array([1.0, 0.0]), np.array([[2.0, 1.0], [1.0, 2.0]]), 0.0, 1.5),\n        # Case D\n        (np.array([0.0, 0.0, 0.0]), np.diag([5.0, 7.0, 1.0]), 1.0, 10.0),\n        # Case E\n        (np.array([2.0, 1.0, 2.0]), np.eye(3), 100.0, 0.1),\n        # Case F\n        (np.array([1.0, 1.0, 1.0, 1.0]), 1000.0 * np.eye(4), 1e-4, 1.0),\n        # Case G\n        (np.array([2.0, -3.0]), np.zeros((2, 2)), 0.2, 0.5),\n    ]\n\n    results = []\n    \n    for g, H, delta, eta0 in test_cases:\n        # a = ||g_t||_2^2\n        a = np.dot(g, g)\n        \n        # c = g_t^T * H_t * g_t\n        c = g.T @ H @ g\n\n        eta_ctrl = 0.0\n        \n        # The derivation shows a finite bound only exists if c  0.\n        # This also handles the case g=0, where a=0 and c=0.\n        if c  0:\n            # The quadratic inequality is (c/2)*eta^2 - a*eta - delta = 0.\n            # For c  0, this holds for eta between the roots. We need the\n            # positive root as the upper bound.\n            discriminant = a**2 + 2 * c * delta\n            # Since a=0, c0, delta=0, discriminant is non-negative.\n            eta_bound = (a + np.sqrt(discriminant)) / c\n            eta_ctrl = min(eta0, eta_bound)\n        else: # c = 0\n            # The constraint is vacuous (satisfied for all eta = 0).\n            # The effective upper bound is infinite.\n            eta_ctrl = eta0\n            \n        results.append(f\"{eta_ctrl:.10f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}