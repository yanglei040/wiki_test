{
    "hands_on_practices": [
        {
            "introduction": "小批量梯度下降的核心在于其迭代更新规则。本练习  聚焦于单次更新步骤，旨在阐明这一核心机制。通过计算新的参数值，你将亲身体验学习率和梯度如何协同作用，引导模型走向成本函数的最小值。",
            "id": "2187026",
            "problem": "一名计算科学专业的学生正在训练一个简单的预测模型。该模型的行为由一个无量纲参数 $\\theta$ 控制。目标是找到使模型预测误差最小化的 $\\theta$ 值，该误差由成本函数 $J(\\theta)$ 衡量。\n\n该学生采用一种称为梯度下降的数值优化技术。该过程从参数的初始猜测开始，并迭代地对其进行优化。每个更新步骤旨在将参数值向成本函数梯度的相反方向移动。每一步的大小由一个称为学习率的参数决定。\n\n该学生将参数初始化为 $\\theta_0 = 2$。在第一次迭代中，他们使用部分数据来计算成本函数在该初始点上关于参数的梯度。计算得到的梯度值为 $\\nabla J(\\theta_0) = 4$。优化过程的学习率设置为 $\\eta = 0.01$。\n\n计算第一次迭代后参数的更新值 $\\theta_1$。将您的答案保留三位有效数字。",
            "solution": "我们使用单个参数的标准梯度下降更新规则：\n$$\n\\theta_{k+1} = \\theta_{k} - \\eta \\frac{dJ}{d\\theta}\\bigg|_{\\theta=\\theta_{k}}.\n$$\n对于 $k=0$ 的第一次迭代，给定值为 $\\theta_{0} = 2$，学习率 $\\eta = 0.01$，梯度 $\\nabla J(\\theta_{0}) = 4$。将这些值代入更新规则可得：\n$$\n\\theta_{1} = \\theta_{0} - \\eta \\nabla J(\\theta_{0}) = 2 - 0.01 \\times 4.\n$$\n计算乘积：\n$$\n0.01 \\times 4 = 0.04,\n$$\n所以\n$$\n\\theta_{1} = 2 - 0.04 = 1.96.\n$$\n保留三位有效数字，该值为 $1.96$。",
            "answer": "$$\\boxed{1.96}$$"
        },
        {
            "introduction": "虽然基本的梯度下降更新功能强大，但在某些损失地貌中可能会遇到困难。此练习  引入了动量 (momentum) 的概念，这是一种有助于加速学习并克服平坦区域的技术。你将探索一个有趣的情景：当前小批量的梯度为零，但模型参数仍然会更新，这揭示了动量项如何让过去的梯度产生持续影响。",
            "id": "2187015",
            "problem": "一位工程师正在使用带动量项的小批量梯度下降法来训练一个形式为 $h_{\\theta}(x) = \\theta_0 + \\theta_1 x$ 的简单线性回归模型。该优化算法在每一步 $t$ 根据以下规则更新参数 $\\theta = (\\theta_0, \\theta_1)$：\n\n1.  $v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta} J_B(\\theta_{t-1})$\n2.  $\\theta_t = \\theta_{t-1} - v_t$\n\n其中 $\\theta_{t-1}$ 是上一步的参数值，$v_t$ 是当前的速度向量，$v_{t-1}$ 是之前的速度向量，$\\gamma$ 是动量系数，$\\eta$ 是学习率，$\\nabla_{\\theta} J_B(\\theta_{t-1})$ 是使用参数 $\\theta_{t-1}$ 在当前小批量 $B$ 上计算出的损失函数的梯度。\n\n损失函数是均方误差 (MSE)，对于一个包含 $m_B$ 个样本的小批量 $B$，其定义如下：\n$J_B(\\theta) = \\frac{1}{2m_B} \\sum_{(x^{(i)}, y^{(i)}) \\in B} (h_{\\theta}(x^{(i)}) - y^{(i)})^2$\n\n在当前迭代（步骤 $t$）开始时，模型的状态如下：\n- 参数：$\\theta_{t-1} = (\\theta_0, \\theta_1) = (1.0, 2.0)$\n- 先前的速度向量：$v_{t-1} = (v_{0, t-1}, v_{1, t-1}) = (0.1, -0.2)$\n\n超参数设置如下：\n- 学习率：$\\eta = 0.01$\n- 动量系数：$\\gamma = 0.9$\n\n对于当前的更新步骤，算法使用以下包含两个数据点的小批量 $B$：\n$B = \\{(1, 3), (3, 7)\\}$\n\n计算在这次迭代结束时参数 $\\theta_1$ 的更新值，记为 $\\theta_{1,t}$。",
            "solution": "目标是找到经过一步带带动量的小批量梯度下降后更新的参数 $\\theta_{1,t}$。\n\n参数 $\\theta$ 的更新规则由一个两步过程给出：\n1. 更新速度向量：$v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta} J_B(\\theta_{t-1})$\n2. 更新参数：$\\theta_t = \\theta_{t-1} - v_t$\n\n我们关心的是参数 $\\theta_1$。让我们专门为这个参数写出更新规则：\n$v_{1,t} = \\gamma v_{1,t-1} + \\eta \\frac{\\partial J_B(\\theta_{t-1})}{\\partial \\theta_1}$\n$\\theta_{1,t} = \\theta_{1,t-1} - v_{1,t}$\n\n第一步是计算损失函数 $J_B$ 关于 $\\theta_1$ 的偏导数，并在 $\\theta_{t-1} = (1.0, 2.0)$ 处进行求值。\n\n损失函数为 $J_B(\\theta) = \\frac{1}{2m_B} \\sum_{(x^{(i)}, y^{(i)}) \\in B} ((\\theta_0 + \\theta_1 x^{(i)}) - y^{(i)})^2$。\n关于 $\\theta_1$ 的偏导数是：\n$\\frac{\\partial J_B}{\\partial \\theta_1} = \\frac{1}{m_B} \\sum_{(x^{(i)}, y^{(i)}) \\in B} ((\\theta_0 + \\theta_1 x^{(i)}) - y^{(i)}) x^{(i)}$\n\n小批量是 $B = \\{(1, 3), (3, 7)\\}$，所以小批量大小 $m_B=2$。当前参数是 $\\theta_0 = 1.0$ 和 $\\theta_1 = 2.0$。\n\n让我们为小批量中的每个点计算误差项 $(\\theta_0 + \\theta_1 x^{(i)}) - y^{(i)}$。\n对于第一个点 $(x^{(1)}, y^{(1)}) = (1, 3)$：\n预测值：$h_{\\theta}(x^{(1)}) = \\theta_0 + \\theta_1 x^{(1)} = 1.0 + 2.0 \\cdot 1 = 3.0$\n误差：$h_{\\theta}(x^{(1)}) - y^{(1)} = 3.0 - 3 = 0.0$\n\n对于第二个点 $(x^{(2)}, y^{(2)}) = (3, 7)$：\n预测值：$h_{\\theta}(x^{(2)}) = \\theta_0 + \\theta_1 x^{(2)} = 1.0 + 2.0 \\cdot 3 = 1.0 + 6.0 = 7.0$\n误差：$h_{\\theta}(x^{(2)}) - y^{(2)} = 7.0 - 7 = 0.0$\n\n现在，将这些误差代入偏导数的表达式中：\n$\\frac{\\partial J_B}{\\partial \\theta_1} = \\frac{1}{2} \\left[ (0.0) \\cdot x^{(1)} + (0.0) \\cdot x^{(2)} \\right]$\n$\\frac{\\partial J_B}{\\partial \\theta_1} = \\frac{1}{2} \\left[ (0.0) \\cdot 1 + (0.0) \\cdot 3 \\right] = 0.0$\n$\\theta_1$ 的梯度分量恰好为零。\n\n接下来，我们使用给定的超参数和先前的速度来计算 $\\theta_1$ 的新速度分量 $v_{1,t}$。\n给定的值为 $\\gamma = 0.9$，$\\eta = 0.01$，并且 $\\theta_1$ 的先前速度分量为 $v_{1,t-1} = -0.2$。\n$v_{1,t} = \\gamma v_{1,t-1} + \\eta \\frac{\\partial J_B}{\\partial \\theta_1}$\n$v_{1,t} = (0.9)(-0.2) + (0.01)(0.0)$\n$v_{1,t} = -0.18 + 0 = -0.18$\n\n最后，我们更新参数 $\\theta_1$。当前值为 $\\theta_{1,t-1} = 2.0$。\n$\\theta_{1,t} = \\theta_{1,t-1} - v_{1,t}$\n$\\theta_{1,t} = 2.0 - (-0.18)$\n$\\theta_{1,t} = 2.0 + 0.18 = 2.18$\n\n所以，尽管当前小批量的梯度为零，但由于动量项从上一个更新步骤中继承了速度，参数 $\\theta_1$ 仍然发生了变化。",
            "answer": "$$\\boxed{2.18}$$"
        },
        {
            "introduction": "现代深度学习训练涉及各种技术的复杂相互作用。此高级练习  探究了梯度累积（一种模拟大批量大小的方法）与批量归一化 (Batch Normalization) 之间微妙而关键的相互作用。你会发现，由于批量归一化统计量的批次依赖性，使用梯度累积时会引入偏差，你将通过数值计算来量化这种效应。本练习能让你深入理解实现和优化大规模神经网络训练时的实际问题。",
            "id": "3150999",
            "problem": "考虑在训练模式下，对带有批量归一化 (BN) 和 Dropout 的单层线性预测器进行经验风险最小化。设有效批量大小为 $B_{\\text{eff}}$，特征维度为 $d$，Dropout 的保留概率为 $p$。定义数据集为 $X \\in \\mathbb{R}^{B_{\\text{eff}} \\times d}$，目标为 $t \\in \\mathbb{R}^{B_{\\text{eff}}}$，模型参数为 $w \\in \\mathbb{R}^{d}$。对于给定的批量，前向映射由以下操作按顺序定义：\n\n- 批量归一化 (BN)：计算每个特征的批量统计量 $\\mu \\in \\mathbb{R}^{d}$ 和 $\\sigma^2 \\in \\mathbb{R}^{d}$，其中 $\\mu_j = \\frac{1}{B}\\sum_{i=1}^{B} X_{ij}$ 且 $\\sigma_j^2 = \\frac{1}{B}\\sum_{i=1}^{B} (X_{ij} - \\mu_j)^2$，对于 $j \\in \\{1,\\dots,d\\}$。归一化后的特征为 $\\hat{X}_{ij} = \\frac{X_{ij} - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}$，其中 $\\varepsilon  0$ 是一个很小的常数，BN 的仿射参数固定为单位变换（缩放 $\\gamma = 1$ 和平移 $\\beta = 0$）。\n- Dropout：对于每个样本 $i$ 和特征 $j$，抽取一个独立的伯努利保留变量 $K_{ij} \\in \\{0,1\\}$，其概率为 $\\Pr[K_{ij} = 1] = p$，并定义反向 Dropout 掩码 $M_{ij} = \\frac{K_{ij}}{p}$。将该掩码与归一化特征进行逐元素相乘：$Z_{ij} = M_{ij} \\cdot \\hat{X}_{ij}$。\n- 线性预测：$y_i = w^\\top Z_i$，其中 $Z_i \\in \\mathbb{R}^{d}$ 是 $Z$ 的第 $i$ 行。\n\n使用均方误差 (MSE) 损失 $L = \\frac{1}{2B}\\sum_{i=1}^{B} (y_i - t_i)^2$，其中 $B$ 表示前向传播中使用的批量大小。\n\n定义关于 $w$ 的两个梯度估计器：\n\n- 有效批量大小 $B_{\\text{eff}}$ 下的真实大批量梯度：在大小为 $B = B_{\\text{eff}}$ 的整个批量上计算 BN，对该批量使用一个固定的 Dropout 掩码 $M \\in \\mathbb{R}^{B_{\\text{eff}} \\times d}$，并计算当 $B = B_{\\text{eff}}$ 时的梯度 $\\nabla_w L$。\n- 使用梯度累积的微批处理来模拟批量大小 $B_{\\text{eff}}$：将包含 $B_{\\text{eff}}$ 个样本的同一数据集分割成大小为 $m$ 的连续微批量。对于每个大小为 $B_k \\le m$ 的微批量（如果 $B_{\\text{eff}}$ 不能被 $m$ 整除，最后一个微批量的大小可能小于 $m$），仅在该微批量上重新计算 BN 统计量，应用相应的归一化特征以及限制在该微批量的同一 Dropout 掩码 $M$ 的子集，计算微批量梯度，并通过分数 $B_k / B_{\\text{eff}}$ 进行加权平均累积，以获得总体梯度估计。\n\n将偏差定义为累积微批量梯度与真实大批量梯度之间的差值。通过其欧几里得范数来量化偏差，即计算 $\\|\\Delta\\|_2$，其中 $\\Delta = \\nabla_w^{\\text{acc}} - \\nabla_w^{\\text{true}}$。\n\n仅从上述定义出发（使用 MSE 的经验风险最小化、批量归一化、反向 Dropout、线性预测以及作为样本加权平均的梯度累积），推导出在两种情况下计算关于 $w$ 的梯度所必需的表达式，解释微批量梯度与真实大批量梯度在何种条件下相等、在何处不同，并实现一个程序来数值评估和报告以下参数集测试套件的偏差范数：\n\n- 测试用例 1：$B_{\\text{eff}} = 32$, $m = 8$, $d = 5$, $p = 0.8$, 启用 BN, 随机种子 $s = 1$。\n- 测试用例 2：$B_{\\text{eff}} = 32$, $m = 1$, $d = 5$, $p = 0.8$, 启用 BN, 随机种子 $s = 2$。\n- 测试用例 3：$B_{\\text{eff}} = 32$, $m = 8$, $d = 5$, $p = 1.0$, 启用 BN, 随机种子 $s = 3$。\n- 测试用例 4：$B_{\\text{eff}} = 32$, $m = 8$, $d = 5$, $p = 0.8$, 禁用 BN (即 $\\hat{X} = X$), 随机种子 $s = 4$。\n- 测试用例 5：$B_{\\text{eff}} = 8$, $m = 8$, $d = 5$, $p = 0.8$, 启用 BN, 随机种子 $s = 5$。\n\n在所有情况下，BN 中使用 $\\varepsilon = 10^{-5}$，并根据给定的种子 $s$ 确定性地生成合成数据如下：抽取 $X$，其条目为独立的标准正态分布；抽取 $w$，其条目为独立的标准正态分布；抽取一个单独的 $w_{\\text{true}}$，其条目也为独立的标准正态分布；并设置 $t = X w_{\\text{true}} + \\eta$，其中 $\\eta$ 的条目为均值为 $0$、标准差为 $0.1$ 的独立正态分布。对于真实大批量和微批量的计算，使用相同的 Dropout 掩码 $M$，并为每个微批量进行适当的限制。\n\n您的程序应产生单行输出，其中包含五个测试用例的偏差范数，形式为一个用方括号括起来的逗号分隔的十进制浮点数列表（例如 $[x_1,x_2,x_3,x_4,x_5]$）。不应打印任何额外文本。",
            "solution": "用户提供的问题是有效的，因为它以深度学习的原理为科学基础，在数学上是适定的、客观的，并包含推导唯一可验证解所需的所有必要信息。我们将进行推导和实现。\n\n目标是量化一个带有批量归一化 (BN) 和 Dropout 的线性模型中，真实大批量梯度与累积微批量梯度之间的偏差。偏差定义为 $\\Delta = \\nabla_w^{\\text{acc}} - \\nabla_w^{\\text{true}}$，我们计算其欧几里得范数 $\\|\\Delta\\|_2$。\n\n首先，我们推导均方误差 (MSE) 损失相对于模型参数 $w$ 的梯度的通用表达式。对于一个大小为 $B$ 的批量，损失由以下公式给出：\n$$\nL = \\frac{1}{2B} \\sum_{i=1}^{B} (y_i - t_i)^2\n$$\n其中 $y_i = w^\\top Z_i$ 是模型对第 $i$ 个样本的预测。矩阵 $Z \\in \\mathbb{R}^{B \\times d}$ 包含经过 BN 和 Dropout 处理后的特征。\n\n梯度 $\\nabla_w L$ 可以使用链式法则计算。损失相对于预测值 $y_i$ 的偏导数为：\n$$\n\\frac{\\partial L}{\\partial y_i} = \\frac{1}{B} (y_i - t_i)\n$$\n预测值 $y_i$ 相对于权重参数 $w_j$ 的偏导数为：\n$$\n\\frac{\\partial y_i}{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\left( \\sum_{k=1}^{d} w_k Z_{ik} \\right) = Z_{ij}\n$$\n以向量形式表示，$\\nabla_w y_i = Z_i$，其中 $Z_i \\in \\mathbb{R}^{d}$ 是 $Z$ 的第 $i$ 行。\n\n结合以上各式，损失相对于整个权重向量 $w$ 的梯度为：\n$$\n\\nabla_w L = \\sum_{i=1}^{B} \\frac{\\partial L}{\\partial y_i} \\nabla_w y_i = \\sum_{i=1}^{B} \\frac{1}{B} (y_i - t_i) Z_i = \\frac{1}{B} \\sum_{i=1}^{B} (y_i - t_i) Z_i\n$$\n这可以用矩阵表示法表示为：\n$$\n\\nabla_w L = \\frac{1}{B} Z^\\top (y - t)\n$$\n其中 $y = Zw$ 是该批量的预测向量。这个表达式是两种梯度估计器的基础。它们之间的差异完全在于矩阵 $Z$ 是如何构建的。\n\n**1. 真实大批量梯度 ($\\nabla_w^{\\text{true}}$)**\n\n对于真实梯度，所有计算都在整个有效批量 $X \\in \\mathbb{R}^{B_{\\text{eff}} \\times d}$ 上执行。令 $B = B_{\\text{eff}}$。\n1.  批量统计量是全局计算的：\n    $$\n    \\mu_{\\text{true}} = \\frac{1}{B_{\\text{eff}}} \\sum_{i=1}^{B_{\\text{eff}}} X_i \\quad \\text{and} \\quad \\sigma^2_{\\text{true}} = \\frac{1}{B_{\\text{eff}}} \\sum_{i=1}^{B_{\\text{eff}}} (X_i - \\mu_{\\text{true}}) \\odot (X_i - \\mu_{\\text{true}})\n    $$\n2.  特征被归一化：\n    $$\n    \\hat{X}_{\\text{true}} = \\frac{X - \\mu_{\\text{true}}}{\\sqrt{\\sigma^2_{\\text{true}} + \\varepsilon}}\n    $$\n    其中减法和除法是逐元素的。\n3.  使用完整掩码 $M \\in \\mathbb{R}^{B_{\\text{eff}} \\times d}$ 应用 Dropout：\n    $$\n    Z_{\\text{true}} = M \\odot \\hat{X}_{\\text{true}}\n    $$\n4.  梯度则为：\n    $$\n    \\nabla_w^{\\text{true}} = \\frac{1}{B_{\\text{eff}}} Z_{\\text{true}}^\\top (Z_{\\text{true}}w - t)\n    $$\n\n**2. 累积微批量梯度 ($\\nabla_w^{\\text{acc}}$)**\n\n数据集 $X$ 被分割成 $K$ 个连续的微批量 $X_1, \\dots, X_K$，其中微批量 $X_k$ 的大小为 $B_k$。对于每个微批量 $k$：\n1.  批量统计量仅在该微批量上局部计算：\n    $$\n    \\mu_{k} = \\frac{1}{B_k} \\sum_{i \\in \\text{批次 } k} X_i \\quad \\text{and} \\quad \\sigma^2_{k} = \\frac{1}{B_k} \\sum_{i \\in \\text{批次 } k} (X_i - \\mu_{k}) \\odot (X_i - \\mu_{k})\n    $$\n2.  使用这些局部统计量对微批量的特征进行归一化：\n    $$\n    \\hat{X}_{k} = \\frac{X_k - \\mu_{k}}{\\sqrt{\\sigma^2_{k} + \\varepsilon}}\n    $$\n3.  使用相应的子掩码 $M_k$ 应用 Dropout：\n    $$\n    Z_{k} = M_k \\odot \\hat{X}_{k}\n    $$\n4.  这个微批量的梯度为：\n    $$\n    \\nabla_w L_k = \\frac{1}{B_k} Z_k^\\top (Z_k w - t_k)\n    $$\n5.  这些梯度根据与其批量大小成比例的权重进行累积：\n    $$\n    \\nabla_w^{\\text{acc}} = \\sum_{k=1}^{K} \\frac{B_k}{B_{\\text{eff}}} \\nabla_w L_k = \\sum_{k=1}^{K} \\frac{B_k}{B_{\\text{eff}}} \\left( \\frac{1}{B_k} Z_k^\\top (Z_k w - t_k) \\right) = \\frac{1}{B_{\\text{eff}}} \\sum_{k=1}^{K} Z_k^\\top (Z_k w - t_k)\n    $$\n\n**偏差分析**\n\n偏差 $\\Delta = \\nabla_w^{\\text{acc}} - \\nabla_w^{\\text{true}}$ 通常为非零。根本原因在于批量归一化中的归一化操作是非线性的且依赖于上下文。具体来说，对于一个微批量 $X_k$，其归一化版本 $\\hat{X}_k$ 与全局归一化批量 $\\hat{X}_{\\text{true}}$ 的对应行不同，因为用于归一化的统计量是不同的 $(\\mu_k, \\sigma^2_k) \\neq (\\mu_{\\text{true}}, \\sigma^2_{\\text{true}})$。这导致 $Z_k$ 不是 $Z_{\\text{true}}$ 的一个精确子矩阵，进而导致梯度表达式产生分歧。\n\n偏差在以下两种特定条件下会消失：\n\n1.  **禁用批量归一化**：如果不使用 BN，则 $\\hat{X} = X$。在这种情况下，$\\hat{X}_k = X_k$ 且 $\\hat{X}_{\\text{true}} = X$。因此，$Z_k = M_k \\odot X_k$ 是 $Z_{\\text{true}} = M \\odot X$ 的一个精确子矩阵。用于计算 $\\nabla_w^{\\text{acc}}$ 的表达式中的求和项可以精确地重构出 $\\nabla_w^{\\text{true}}$ 的表达式：\n    $$\n    \\sum_{k=1}^{K} Z_k^\\top (Z_k w - t_k) = Z_{\\text{true}}^\\top (Z_{\\text{true}} w - t)\n    $$\n    因此，$\\nabla_w^{\\text{acc}} = \\nabla_w^{\\text{true}}$ 且偏差为零。这种情况对应于测试用例 4。\n\n2.  **微批量大小等于有效批量大小 ($m = B_{\\text{eff}}$)**：在这种情况下，只有一个微批量，即整个有效批量。“局部”统计量 $(\\mu_1, \\sigma^2_1)$ 是在完整批量上计算的，使其与“全局”统计量 $(\\mu_{\\text{true}}, \\sigma^2_{\\text{true}})$ 完全相同。这导致 $Z_1 = Z_{\\text{true}}$，累积梯度计算可平凡地简化为真实梯度计算，从而产生零偏差。这种情况对应于测试用例 5。\n\n在所有其他启用 BN 且 $m  B_{\\text{eff}}$ 的情况下（测试用例 1、2 和 3），预计会出现非零偏差。禁用 Dropout ($p=1.0$) 并不能消除偏差，因为这种差异是由 BN 引起的，而不是 Dropout。Dropout 掩码 $M$ 对两种计算是固定的，它作为一个共享的随机源来调制特征，但不会导致两种梯度估计器之间的结构性差异。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_bias_norm(B_eff, m, d, p, bn_enabled, seed, epsilon):\n    \"\"\"\n    Computes the bias norm between true and accumulated gradients.\n\n    Args:\n        B_eff (int): Effective batch size.\n        m (int): Micro-batch size.\n        d (int): Feature dimension.\n        p (float): Dropout keep probability.\n        bn_enabled (bool): Flag to enable/disable Batch Normalization.\n        seed (int): Random seed for reproducibility.\n        epsilon (float): Small constant for BN stability.\n\n    Returns:\n        float: The Euclidean norm of the bias vector.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate synthetic data and model parameters\n    X = rng.standard_normal(size=(B_eff, d), dtype=np.float64)\n    w = rng.standard_normal(size=d, dtype=np.float64)\n    w_true = rng.standard_normal(size=d, dtype=np.float64)\n    eta = rng.normal(loc=0.0, scale=0.1, size=B_eff).astype(np.float64)\n    t = X @ w_true + eta\n\n    # 2. Generate a fixed inverted dropout mask M\n    # Handle p=0 case to avoid division by zero, though not in test cases.\n    if p > 0:\n        K = rng.binomial(1, p, size=(B_eff, d))\n        M = K / p\n    else:\n        M = np.zeros((B_eff, d), dtype=np.float64)\n\n    # 3. Compute the true large-batch gradient\n    if bn_enabled:\n        mu_true = np.mean(X, axis=0)\n        var_true = np.var(X, axis=0)\n        X_hat_true = (X - mu_true) / np.sqrt(var_true + epsilon)\n    else:\n        X_hat_true = X\n\n    Z_true = M * X_hat_true\n    y_true = Z_true @ w\n    grad_true = (1.0 / B_eff) * Z_true.T @ (y_true - t)\n\n    # 4. Compute the accumulated micro-batch gradient\n    grad_acc = np.zeros(d, dtype=np.float64)\n    for i in range(0, B_eff, m):\n        start = i\n        end = min(start + m, B_eff)\n        B_k = end - start\n\n        if B_k == 0:\n            continue\n\n        # Get slices for the current micro-batch\n        X_k = X[start:end, :]\n        t_k = t[start:end]\n        M_k = M[start:end, :]\n\n        if bn_enabled:\n            # Recompute BN statistics on the micro-batch\n            mu_k = np.mean(X_k, axis=0)\n            var_k = np.var(X_k, axis=0)\n            X_hat_k = (X_k - mu_k) / np.sqrt(var_k + epsilon)\n        else:\n            X_hat_k = X_k\n        \n        Z_k = M_k * X_hat_k\n        y_k = Z_k @ w\n        grad_k = (1.0 / B_k) * Z_k.T @ (y_k - t_k)\n\n        # Accumulate the weighted gradient\n        grad_acc += (B_k / B_eff) * grad_k\n    \n    # 5. Compute the Euclidean norm of the bias\n    bias = grad_acc - grad_true\n    bias_norm = np.linalg.norm(bias)\n    \n    return bias_norm\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (B_eff, m, d, p, bn_enabled, seed)\n        (32, 8, 5, 0.8, True, 1),\n        (32, 1, 5, 0.8, True, 2),\n        (32, 8, 5, 1.0, True, 3),\n        (32, 8, 5, 0.8, False, 4),\n        (8, 8, 5, 0.8, True, 5),\n    ]\n\n    epsilon = 1e-5\n    results = []\n    for B_eff, m, d, p, bn_enabled, seed in test_cases:\n        bias_norm = compute_bias_norm(B_eff, m, d, p, bn_enabled, seed, epsilon)\n        results.append(bias_norm)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}