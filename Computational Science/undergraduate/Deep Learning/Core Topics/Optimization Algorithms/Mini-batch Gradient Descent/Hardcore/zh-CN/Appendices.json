{
    "hands_on_practices": [
        {
            "introduction": "在我们深入研究梯度下降的更新机制之前，首先必须掌握如何将大型数据集划分为更小、更易于管理的小批量（mini-batches）。这个练习将带你完成一个基本但至关重要的步骤：计算在一个训练周期（epoch）中需要处理多少个小批量，并确定最后一个小批量的大小。理解这个过程是构建任何深度学习训练流程的起点。",
            "id": "2186998",
            "problem": "在机器学习领域，训练神经网络通常涉及一个称为小批量梯度下降的迭代优化过程。在此过程中，整个训练数据集被划分为多个较小的、大小相等的子集，称为小批量（mini-batch）。每处理完一个小批量，模型的参数就会更新一次。遍历所有小批量、覆盖整个训练数据集的一次完整过程称为一个周期（epoch）。\n\n一个数据科学团队正在训练一个卷积神经网络来对天文图像进行分类。他们的训练数据集总共包含 $N = 50,000$ 张不同的图像。他们选择使用的小批量大小为 $b = 128$ 张图像。当训练样本总数 $N$ 不能被批量大小 $b$ 整除时，标准的做法是让该周期的最后一个小批量包含所有剩余的样本。\n\n计算在一个周期内将处理的小批量的总数，并确定该周期最后一个小批量的大小。请按顺序以两个整数的形式给出你的答案：小批量的总数和最后一个小批量的大小。",
            "solution": "设 $N$ 表示训练样本的总数，$b$ 表示小批量的大小。将 $N$ 写成欧几里得除法形式：\n$$\nN = qb + r,\\quad 0 \\leq r < b,\n$$\n其中 $q = \\left\\lfloor \\frac{N}{b} \\right\\rfloor$ 且 $r = N - qb$。\n一个周期内小批量的总数为\n$$\nm = \\begin{cases}\nq, & r=0,\\\\\nq+1, & r>0,\n\\end{cases}\n$$\n因为非零的余数需要一个额外（较小）的小批量。最后一个小批量的大小为\n$$\nL = \\begin{cases}\nb, & r=0,\\\\\nr, & r>0.\n\\end{cases}\n$$\n当 $N=50000$ 和 $b=128$ 时，通过界定法计算 $q$：\n$$\n128 \\times 390 = 49920,\\quad 128 \\times 391 = 50048 > 50000 \\;\\Rightarrow\\; q = 390.\n$$\n则\n$$\nr = N - qb = 50000 - 128 \\times 390 = 50000 - 49920 = 80.\n$$\n由于 $r>0$，小批量的总数为\n$$\nm = q + 1 = 391,\n$$\n最后一个小批量的大小为\n$$\nL = r = 80.\n$$\n因此，小批量的总数为 $391$，最后一个小批量的大小为 $80$。",
            "answer": "$$\\boxed{\\begin{pmatrix}391  80\\end{pmatrix}}$$"
        },
        {
            "introduction": "将数据分批后，下一步就是执行核心的参数更新。这个练习让你直接应用小批量梯度下降的基本更新规则，即利用计算出的梯度和设定的学习率来调整模型参数。通过这个简单的计算，你可以直观地理解模型是如何朝着减小误差的方向“学习”的。",
            "id": "2187026",
            "problem": "一名计算科学专业的学生正在训练一个简单的预测模型。该模型的行为由一个无量纲参数 $\\theta$ 控制。其目标是找到使模型的预测误差最小化的 $\\theta$ 值，该误差由成本函数 $J(\\theta)$ 度量。\n\n该学生采用一种称为梯度下降的数值优化技术。该过程从参数的初始猜测值开始，并对其进行迭代优化。每个更新步骤旨在将参数值沿成本函数梯度的反方向移动。每一步的大小由一个称为学习率的参数决定。\n\n该学生将参数初始化为 $\\theta_0 = 2$。在第一次迭代中，他们使用其数据的一个子集来计算成本函数在此初始点上关于参数的梯度。计算得出的梯度值为 $\\nabla J(\\theta_0) = 4$。优化过程的学习率设置为 $\\eta = 0.01$。\n\n计算经过这第一次迭代后参数的更新值 $\\theta_1$。你的答案需保留三位有效数字。",
            "solution": "我们使用单个参数的标准梯度下降更新规则：\n$$\n\\theta_{k+1} = \\theta_{k} - \\eta \\frac{dJ}{d\\theta}\\bigg|_{\\theta=\\theta_{k}}.\n$$\n对于 $k=0$ 的第一次迭代，给定值为 $\\theta_{0} = 2$，学习率 $\\eta = 0.01$，以及梯度 $\\nabla J(\\theta_{0}) = 4$。将这些值代入更新规则可得：\n$$\n\\theta_{1} = \\theta_{0} - \\eta \\nabla J(\\theta_{0}) = 2 - 0.01 \\times 4.\n$$\n计算乘积：\n$$\n0.01 \\times 4 = 0.04,\n$$\n因此\n$$\n\\theta_{1} = 2 - 0.04 = 1.96.\n$$\n保留三位有效数字，该值为 $1.96$。",
            "answer": "$$\\boxed{1.96}$$"
        },
        {
            "introduction": "基本的梯度下降算法有时会面临收敛缓慢或陷入局部最优的问题，而动量（momentum）是解决这些问题的常用技巧。这个练习引入了动量项，并通过一个当前梯度恰好为零的特殊情景，让你亲身体会动量是如何帮助优化过程保持先前迭代的“速度”的。这种机制使得训练过程更加稳健和高效，是现代优化算法的关键组成部分。",
            "id": "2187015",
            "problem": "一位工程师正在使用带有动量项的小批量梯度下降法来训练一个形式为 $h_{\\theta}(x) = \\theta_0 + \\theta_1 x$ 的简单线性回归模型。该优化算法在每一步 $t$ 根据以下规则更新参数 $\\theta = (\\theta_0, \\theta_1)$：\n\n1.  $v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta} J_B(\\theta_{t-1})$\n2.  $\\theta_t = \\theta_{t-1} - v_t$\n\n其中 $\\theta_{t-1}$ 是上一步的参数值，$v_t$ 是当前的速度向量，$v_{t-1}$ 是之前的速度向量，$\\gamma$ 是动量系数，$\\eta$ 是学习率，而 $\\nabla_{\\theta} J_B(\\theta_{t-1})$ 是使用参数 $\\theta_{t-1}$ 在当前小批量 $B$ 上计算的损失函数的梯度。\n\n损失函数为均方误差 (MSE)，对于一个包含 $m_B$ 个样本的小批量 $B$ 定义如下：\n$J_B(\\theta) = \\frac{1}{2m_B} \\sum_{(x^{(i)}, y^{(i)}) \\in B} (h_{\\theta}(x^{(i)}) - y^{(i)})^2$\n\n在当前迭代（步骤 $t$）开始时，模型的状态如下：\n- 参数：$\\theta_{t-1} = (\\theta_0, \\theta_1) = (1.0, 2.0)$\n- 上一步的速度向量：$v_{t-1} = (v_{0, t-1}, v_{1, t-1}) = (0.1, -0.2)$\n\n超参数设置为：\n- 学习率：$\\eta = 0.01$\n- 动量系数：$\\gamma = 0.9$\n\n对于当前的更新步骤，算法使用以下包含两个数据点的小批量 $B$：\n$B = \\{(1, 3), (3, 7)\\}$\n\n计算在本次迭代结束时参数 $\\theta_1$ 的更新值，记为 $\\theta_{1,t}$。",
            "solution": "目标是找到在执行一步带有动量的小批量梯度下降后更新的参数 $\\theta_{1,t}$。\n\n参数 $\\theta$ 的更新规则由一个两步过程给出：\n1. 更新速度向量：$v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta} J_B(\\theta_{t-1})$\n2. 更新参数：$\\theta_t = \\theta_{t-1} - v_t$\n\n我们关心的是参数 $\\theta_1$。让我们专门为这个参数写出更新规则：\n$v_{1,t} = \\gamma v_{1,t-1} + \\eta \\frac{\\partial J_B(\\theta_{t-1})}{\\partial \\theta_1}$\n$\\theta_{1,t} = \\theta_{1,t-1} - v_{1,t}$\n\n第一步是计算损失函数 $J_B$ 关于 $\\theta_1$ 的偏导数，并在 $\\theta_{t-1} = (1.0, 2.0)$ 处求值。\n\n损失函数为 $J_B(\\theta) = \\frac{1}{2m_B} \\sum_{(x^{(i)}, y^{(i)}) \\in B} ((\\theta_0 + \\theta_1 x^{(i)}) - y^{(i)})^2$。\n关于 $\\theta_1$ 的偏导数是：\n$\\frac{\\partial J_B}{\\partial \\theta_1} = \\frac{1}{m_B} \\sum_{(x^{(i)}, y^{(i)}) \\in B} ((\\theta_0 + \\theta_1 x^{(i)}) - y^{(i)}) x^{(i)}$\n\n小批量是 $B = \\{(1, 3), (3, 7)\\}$，所以小批量的大小是 $m_B=2$。当前参数是 $\\theta_0 = 1.0$ 和 $\\theta_1 = 2.0$。\n\n让我们计算小批量中每个点的误差项 $(\\theta_0 + \\theta_1 x^{(i)}) - y^{(i)}$。\n对于第一个点 $(x^{(1)}, y^{(1)}) = (1, 3)$：\n预测值：$h_{\\theta}(x^{(1)}) = \\theta_0 + \\theta_1 x^{(1)} = 1.0 + 2.0 \\cdot 1 = 3.0$\n误差：$h_{\\theta}(x^{(1)}) - y^{(1)} = 3.0 - 3 = 0.0$\n\n对于第二个点 $(x^{(2)}, y^{(2)}) = (3, 7)$：\n预测值：$h_{\\theta}(x^{(2)}) = \\theta_0 + \\theta_1 x^{(2)} = 1.0 + 2.0 \\cdot 3 = 1.0 + 6.0 = 7.0$\n误差：$h_{\\theta}(x^{(2)}) - y^{(2)} = 7.0 - 7 = 0.0$\n\n现在，将这些误差代入偏导数的表达式中：\n$\\frac{\\partial J_B}{\\partial \\theta_1} = \\frac{1}{2} \\left[ (0.0) \\cdot x^{(1)} + (0.0) \\cdot x^{(2)} \\right]$\n$\\frac{\\partial J_B}{\\partial \\theta_1} = \\frac{1}{2} \\left[ (0.0) \\cdot 1 + (0.0) \\cdot 3 \\right] = 0.0$\n$\\theta_1$ 的梯度分量恰好为零。\n\n接下来，我们使用给定的超参数和上一步的速度来计算 $\\theta_1$ 的新速度分量 $v_{1,t}$。\n给定的值为 $\\gamma = 0.9$，$\\eta = 0.01$，并且 $\\theta_1$ 的上一步速度分量为 $v_{1,t-1} = -0.2$。\n$v_{1,t} = \\gamma v_{1,t-1} + \\eta \\frac{\\partial J_B}{\\partial \\theta_1}$\n$v_{1,t} = (0.9)(-0.2) + (0.01)(0.0)$\n$v_{1,t} = -0.18 + 0 = -0.18$\n\n最后，我们更新参数 $\\theta_1$。当前值为 $\\theta_{1,t-1} = 2.0$。\n$\\theta_{1,t} = \\theta_{1,t-1} - v_{1,t}$\n$\\theta_{1,t} = 2.0 - (-0.18)$\n$\\theta_{1,t} = 2.0 + 0.18 = 2.18$\n\n所以，即使当前小批量的梯度为零，参数 $\\theta_1$ 仍然发生了变化，这是因为动量项从上一个更新步骤中继承了速度。",
            "answer": "$$\\boxed{2.18}$$"
        }
    ]
}