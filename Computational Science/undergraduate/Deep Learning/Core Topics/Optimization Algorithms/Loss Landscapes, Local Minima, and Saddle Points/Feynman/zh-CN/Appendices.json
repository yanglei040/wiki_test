{
    "hands_on_practices": [
        {
            "introduction": "深度学习中复杂的损失函数地貌源于其基本的构造单元。本练习将通过分析一个仅包含单个修正线性单元（Rectified Linear Unit, ReLU）神经元的简单模型，来揭示这些地貌的精细结构。你将亲手计算并观察，由于ReLU激活函数的非线性特性，损失函数上如何自然地产生“扭结”（kinks）、“平台”（plateaus）和类似鞍点的区域，从而直观理解优化算法在实践中面临的挑战。",
            "id": "3145626",
            "problem": "考虑深度学习中一个具有单个修正线性单元（ReLU）神经元的双参数非线性回归模型，其定义为 $f(x; w_1, w_2) = \\sigma(w_1 x) + w_2$，其中 $\\sigma(z) = \\max(0, z)$ 表示修正线性单元（ReLU）。设数据集 $\\{(x_i, y_i)\\}_{i=1}^{3}$ 上的经验平方损失（平方误差和）为 $J(w_1, w_2) = \\sum_{i=1}^{3} \\left(f(x_i; w_1, w_2) - y_i\\right)^{2}$，并考虑训练数据点 $(x_1, y_1) = (1, 2)$，$(x_2, y_2) = (3, 1)$ 和 $(x_3, y_3) = (5, 4)$。\n\n从经验风险和非线性函数 $\\sigma$ 的核心定义出发，分析损失景观 $J(w_1, w_2)$ 关于 $w_1$ 和 $w_2$ 的形状，并讨论以下几点：\n- 由 $\\sigma$ 的不可微性引起的扭结（kinks）的存在，\n- 由“死亡”神经元（即在所有训练输入上 $\\sigma(w_1 x_i) = 0$ 的区域）引起的平台区（plateaus），\n- 在 $w_1 = 0$ 附近的鞍点状行为。\n\n接着，使用适用于非光滑函数的次微分分析，确定 $J$ 关于 $w_1$ 在 $w_1=0$ 处的 Clarke 次微分，并将其表示为 $w_2$ 的函数。接下来，求出使 $J(0, w_2)$ 最小化的值 $w_2^{\\star}$，并最终计算 $J$ 在点 $(w_1, w_2) = (0, w_2^{\\star})$ 处沿方向 $d = (1, 0)$ 的单侧方向导数（即，将 $w_1$ 从零开始增加，同时保持 $w_2$ 固定为 $w_2^{\\star}$）。\n\n将此单侧方向导数的精确值作为最终答案。请勿对答案进行四舍五入。无需单位。",
            "solution": "我们从核心定义开始。模型为 $f(x; w_1, w_2) = \\sigma(w_1 x) + w_2$，其中 $\\sigma(z) = \\max(0, z)$，经验损失为 $J(w_1, w_2) = \\sum_{i=1}^{3} \\left(\\sigma(w_1 x_i) + w_2 - y_i\\right)^{2}$。数据集为 $(x_1, y_1) = (1, 2)$，$(x_2, y_2) = (3, 1)$ 和 $(x_3, y_3) = (5, 4)$。\n\n首先，我们分析其定性形状。\n\n1. 扭结：函数 $\\sigma(z)$ 在 $z=0$ 处不可微。在我们的参数化中，其自变量为 $z_i(w_1) = w_1 x_i$。由于对所有 $i$ 都有 $x_i > 0$，因此 $w_1 = 0$ 会同时导致所有 $i$ 的 $z_i(0) = 0$。因此，对于任意固定的 $w_2$，$J(w_1, w_2)$ 在 $w_1 = 0$ 处存在一个扭结，因为通过 $\\sigma$ 的复合函数在此边界处改变了其形式。\n\n2. 平台区和“死亡”神经元：对于 $w_1 \\leq 0$ 和 $x_i > 0$，我们有 $w_1 x_i \\leq 0$，因此对所有 $i$ 都有 $\\sigma(w_1 x_i) = 0$。在该区域，神经元在数据集上是“死亡”的，模型简化为 $f(x_i; w_1, w_2) = w_2$。因此，$J(w_1, w_2) = \\sum_{i=1}^{3} (w_2 - y_i)^{2}$ 变得与 $w_1$ 无关。这意味着对于所有 $w_1 \\leq 0$，在 $w_1$ 方向上存在一个平台区：在该半空间中，损失函数关于 $w_1$ 的变化是平坦的。\n\n3. 在 $w_1 = 0$ 附近的鞍点状行为：尽管对于 $w_1 \\leq 0$，$J$ 在 $w_1$ 方向上是平坦的，但如果数据可以通过激活神经元的贡献 $\\sigma(w_1 x_i) = w_1 x_i$ 获得更好的拟合，那么当 $w_1 > 0$ 时损失会减小。因此，在边界 $w_1 = 0$ 处，损失可能在一个方向上是平坦的（对于 $w_1 \\leq 0$ 是非递增的），而在另一个方向上（$w_1 > 0$）是下降的，这是损失景观中鞍点状区域的特征。\n\n接下来，我们计算对于固定的 $w_2$，$J$ 关于 $w_1$ 在 $w_1=0$ 处的 Clarke 次微分。对于非光滑分析，回想一下在 $z = 0$ 处，$\\sigma$ 的 Clarke 次微分是区间 $\\partial \\sigma(0) = [0, 1]$。对于每个数据点，由于 $\\sigma(0) = 0$，我们将点 $(w_1, w_2) = (0, w_2)$ 处的残差定义为 $\\Delta_i(w_2) = w_2 - y_i$。单个加数项为 $g_i(w_1, w_2) = \\left(\\sigma(w_1 x_i) + w_2 - y_i\\right)^{2}$。它关于 $w_1$ 在 $w_1=0$ 处的广义导数可以通过 Clarke 次微分的链式法则获得：$\\partial_{w_1} g_i(0, w_2)$ 中的一个元素是 $2 \\left(\\sigma(0) + w_2 - y_i\\right) \\cdot v_i x_i$，其中 $v_i \\in [0, 1]$，即：\n$$\n\\partial_{w_1} g_i(0, w_2) = \\left\\{ 2 \\Delta_i(w_2) \\, v_i \\, x_i \\, : \\, v_i \\in [0, 1] \\right\\}。\n$$\n对 $i=1, 2, 3$ 求和得到\n$$\n\\partial_{w_1} J(0, w_2) = \\left\\{ 2 \\sum_{i=1}^{3} \\Delta_i(w_2) \\, v_i \\, x_i \\, : \\, v_i \\in [0, 1] \\right\\}。\n$$\n令 $a_i(w_2) = \\Delta_i(w_2) x_i = (w_2 - y_i) x_i$。那么上述集合为\n$$\n\\left\\{ 2 \\sum_{i=1}^{3} a_i(w_2) v_i \\, : \\, v_i \\in [0, 1] \\right\\}。\n$$\n因为 $v_i$ 在 $[0, 1]$ 上独立取值，所以和 $\\sum_{i} a_i v_i$ 的集合是区间\n$$\n\\left[ \\sum_{i=1}^{3} \\min\\{a_i(w_2), 0\\} \\,,\\, \\sum_{i=1}^{3} \\max\\{a_i(w_2), 0\\} \\right]，\n$$\n所以\n$$\n\\partial_{w_1} J(0, w_2) = 2 \\cdot \\left[ \\sum_{i=1}^{3} \\min\\{a_i(w_2), 0\\} \\,,\\, \\sum_{i=1}^{3} \\max\\{a_i(w_2), 0\\} \\right]。\n$$\n\n我们现在求使 $J(0, w_2)$ 最小的 $w_2^{\\star}$。在平台区 $w_1 \\leq 0$ 上，损失为 $J(0, w_2) = \\sum_{i=1}^{3} (w_2 - y_i)^{2}$，这是一个关于 $w_2$ 的凸二次函数。其唯一的最小值点是样本均值，\n$$\nw_2^{\\star} = \\frac{1}{3} \\sum_{i=1}^{3} y_i = \\frac{2 + 1 + 4}{3} = \\frac{7}{3}。\n$$\n\n最后，我们计算 $J$ 在点 $(w_1, w_2) = (0, w_2^{\\star})$ 处沿方向 $d = (1, 0)$ 的单侧方向导数，这对应于将 $w_1$ 从零开始增加而将 $w_2$ 保持在 $w_2^{\\star}$。对于足够小的 $w_1 > 0$，我们有 $\\sigma(w_1 x_i) = w_1 x_i$，且 $J(w_1, w_2^{\\star}) = \\sum_{i=1}^{3} (w_1 x_i + w_2^{\\star} - y_i)^{2}$。对 $w_1$ 求导并在 $w_1 = 0^{+}$ 处取值，得到右侧方向导数\n$$\n\\partial_{+} J(0, w_2^{\\star}; d) = \\left. \\frac{\\partial}{\\partial w_1} \\sum_{i=1}^{3} (w_1 x_i + \\Delta_i(w_2^{\\star}))^{2} \\right|_{w_1 = 0^{+}} = \\sum_{i=1}^{3} 2 \\Delta_i(w_2^{\\star}) x_i,\n$$\n其中 $\\Delta_i(w_2^{\\star}) = w_2^{\\star} - y_i$。计算这些量：\n$$\nw_2^{\\star} = \\frac{7}{3}, \\quad \\Delta_1 = \\frac{7}{3} - 2 = \\frac{1}{3}, \\quad \\Delta_2 = \\frac{7}{3} - 1 = \\frac{4}{3}, \\quad \\Delta_3 = \\frac{7}{3} - 4 = -\\frac{5}{3}。\n$$\n乘以 $x_i$：\n$$\n\\Delta_1 x_1 = \\frac{1}{3} \\cdot 1 = \\frac{1}{3}, \\quad \\Delta_2 x_2 = \\frac{4}{3} \\cdot 3 = 4, \\quad \\Delta_3 x_3 = -\\frac{5}{3} \\cdot 5 = -\\frac{25}{3}。\n$$\n求和并乘以 2：\n$$\n\\sum_{i=1}^{3} \\Delta_i x_i = \\frac{1}{3} + 4 - \\frac{25}{3} = \\frac{1 + 12 - 25}{3} = -\\frac{12}{3} = -4,\n$$\n所以\n$$\n\\partial_{+} J(0, w_2^{\\star}; d) = 2 \\cdot (-4) = -8。\n$$\n这个负值证实了从 $w_1 = 0$ 的平台区进入 $w_1 > 0$ 区域时损失会下降，这反映了鞍点状行为：在最优常数 $w_2^{\\star}$ 处，当 $w_1 \\leq 0$ 时损失是平坦的，但当 $w_1 > 0$ 时是严格递减的。",
            "answer": "$$\\boxed{-8}$$"
        },
        {
            "introduction": "在复杂的损失函数地貌中导航时，仅依靠梯度信息可能会使优化器误入歧途，停留在梯度微小但并非最优的鞍点上。本练习旨在解决这一关键问题：如何可靠地区分真正的局部最小值和具有欺骗性的鞍点。你将从泰勒展开等基本原理出发，推导并实现一个基于曲率信息的二阶停止准则，这是一种比检查梯度范数更稳健的收敛判断方法。",
            "id": "3145617",
            "problem": "您将构建并分析一个光滑损失函数，以形式化一个二阶停止准则。该准则在梯度小但某些方向上曲率大的鞍点状区域不会判断为收敛。仅可使用基于以下基础的数学推理：梯度向量的定义、海森矩阵的定义，以及光滑标量函数在某点周围的二阶泰勒展开。不允许使用任何其他出发点。\n\n考虑以下三维损失函数 $L:\\mathbb{R}^3\\to\\mathbb{R}$，其参数化旨在同时创造一个强曲率方向、一个负曲率方向和一个近乎平坦的方向：\n$$\nL(x,y,z) \\;=\\; a\\,x \\;+\\; \\tfrac{b}{2}\\,y^2 \\;-\\; \\tfrac{c}{2}\\,x^2 \\;+\\; \\tfrac{q}{4}\\,x^4 \\;+\\; \\tfrac{d}{4}\\,z^4.\n$$\n所有参数均为严格的实常数。该构造在科学上是现实的，因为它有下界，并表现出混合曲率，这与许多深度学习损失在非最优驻点周围的特性相似。它还创建了一些区域，在这些区域中梯度模长很小，但某些方向的曲率很大（例如，在 $y=0$ 和 $z\\approx 0$ 的情况下，当 $x\\approx 0$ 时）。\n\n您的任务是：\n$1.$ 从梯度和海森矩阵的基本定义以及二阶泰勒近似出发，推导出一个有原则的二阶停止准则。该准则仅当一个点局部上与严格局部最小值一致时才判断为收敛，并拒绝在梯度小但曲率表明非最小性的鞍点状“伪平坦区”停止。\n$2.$ 对给定的 $L(x,y,z)$ 实现该准则，并在一个小型测试集上对其进行评估。该准则必须是严格的二阶准则，并且在局部二次模型的意义上是尺度感知的。具体来说，您的规则必须在候选点满足以下两个条件：\n- 海森矩阵在严格容差范围内是正定的，表示为 $\\lambda_{\\min}(H) \\ge \\tau_\\lambda$，其中 $\\tau_\\lambda \\gt 0$。\n- 局部二次模型的预测改进量可以忽略不计，这通过对您从二阶泰勒展开中推导出的基于模型的停止量施加一个小的阈值来量化。\n$3.$ 对损失和阈值使用以下固定常数：\n- 损失参数：$a=10^{-3}$, $b=50$, $c=40$, $q=20$, $d=1$。\n- 停止阈值：$\\tau_\\lambda = 10^{-4}$ 以及一个模型下降容差 $\\tau_{\\mathrm{dec}} = 10^{-6}$，应用于您的二阶、尺度感知的停止量。\n$4.$ 使用以下候选点测试集。在所有情况下，坐标顺序均为 $(x,y,z)$。\n- 测试 $1$（鞍点状伪平坦区）：$\\big(\\tfrac{a}{c},\\,0,\\,0\\big)$。\n- 测试 $2$（具有混合曲率的平坦平台）：$\\big(0,\\,0,\\,0\\big)$。\n- 测试 $3$（$x$ 方向上的预期真最小值，在 $z$ 方向上具有小但严格为正的曲率）：设 $x_\\star$ 是方程 $a - c\\,x + q\\,x^3 = 0$ 的唯一正实数解，且其在 $x$ 方向上的二阶导数严格为正。使用 $\\big(x_\\star,\\,0,\\,10^{-2}\\big)$。\n- 测试 $4$（在 $z$ 方向上未满足曲率容差的边界情况）：$\\big(x_\\star,\\,0,\\,10^{-6}\\big)$。\n- 测试 $5$（接近最小值，在 $y$ 方向有微小残差且 $z$ 很小）：$\\big(x_\\star,\\,10^{-6},\\,10^{-2}\\big)$。\n$5.$ 您的程序必须：\n- 计算每个测试点处 $L$ 的梯度和海森矩阵。\n- 对每个测试点，仅使用局部可用量（该点的梯度和海森矩阵），通过您的二阶准则做出收敛决策。\n- 生成单行输出，其中包含 5 个布尔结果，格式为逗号分隔的 Python 风格列表，顺序为测试 1 到 5，例如 $\\big[\\text{True},\\text{False},\\dots\\big]$。\n\n不涉及物理单位或角度单位。输出值必须是布尔值，最终打印格式必须是指定的包含 Python 风格列表的单行。",
            "solution": "该问题要求推导并实现一个用于优化算法的二阶停止准则。该准则必须能够区分真正的局部最小值与梯度很小的鞍点或平坦区域，这是深度学习优化中的一个常见挑战。\n\n首先，我们从基本原理出发推导停止准则。设 $L(\\mathbf{w})$ 是一个光滑的、二次可微的损失函数，其中 $\\mathbf{w} \\in \\mathbb{R}^n$ 是参数向量。$L$ 在点 $\\mathbf{w}_k$ 周围的二阶泰勒展开式为：\n$$\nL(\\mathbf{w}_k + \\mathbf{p}) \\approx M_k(\\mathbf{p}) = L(\\mathbf{w}_k) + \\nabla L(\\mathbf{w}_k)^T \\mathbf{p} + \\frac{1}{2} \\mathbf{p}^T H_k \\mathbf{p}\n$$\n其中 $\\mathbf{p}$ 是一个步长向量，$\\nabla L(\\mathbf{w}_k)$ 是 $L$ 在 $\\mathbf{w}_k$ 处的梯度，而 $H_k = \\nabla^2 L(\\mathbf{w}_k)$ 是 $L$ 在 $\\mathbf{w}_k$ 处的海森矩阵。$M_k(\\mathbf{p})$ 是损失函数在 $\\mathbf{w}_k$ 邻域内的一个二次模型。\n\n一个标准的二阶优化方法，如牛顿法，旨在找到最小化该二次模型的步长 $\\mathbf{p}$。为了找到 $M_k(\\mathbf{p})$ 的最小值，我们将其关于 $\\mathbf{p}$ 的梯度设为零：\n$$\n\\nabla_{\\mathbf{p}} M_k(\\mathbf{p}) = \\nabla L(\\mathbf{w}_k) + H_k \\mathbf{p} = \\mathbf{0}\n$$\n假设海森矩阵 $H_k$ 可逆，这就给出了牛顿步长 $\\mathbf{p}_N$：\n$$\n\\mathbf{p}_N = -H_k^{-1} \\nabla L(\\mathbf{w}_k)\n$$\n如果一个点 $\\mathbf{w}_*$ 满足两个条件，它就被认为是一个严格局部最小值：首先，它是一个驻点，$\\nabla L(\\mathbf{w}_*) = \\mathbf{0}$；其次，海森矩阵 $H_*$ 是正定的，意味着其所有特征值都严格为正。当优化算法达到一个充分近似这些条件的点时，它应该停止。\n\n一个简单的一阶停止准则，如 $||\\nabla L(\\mathbf{w}_k)||  \\epsilon$ 是不够的，因为它会在梯度小但存在负曲率的鞍点处错误地判断为收敛。一个有原则的二阶准则必须检查海森矩阵。\n\n因此，我们停止准则的第一个条件是基于曲率。我们要求海森矩阵“足够”正定，以确保我们处于一个与局部最小值一致的凸盆地中。这通过要求海森矩阵的最小特征值 $\\lambda_{\\min}(H_k)$ 大于或等于一个小的正容差 $\\tau_\\lambda$ 来表示：\n$$\n\\text{Condition 1: } \\lambda_{\\min}(H_k) \\ge \\tau_\\lambda  0\n$$\n\n第二个条件处理潜在改进的幅度。如果采用牛顿步长 $\\mathbf{p}_N$，基于二次模型预测的损失函数下降量为：\n$$\n\\Delta M_k = M_k(\\mathbf{0}) - M_k(\\mathbf{p}_N) = L(\\mathbf{w}_k) - \\left( L(\\mathbf{w}_k) + \\nabla L(\\mathbf{w}_k)^T \\mathbf{p}_N + \\frac{1}{2} \\mathbf{p}_N^T H_k \\mathbf{p}_N \\right)\n$$\n代入 $\\mathbf{p}_N = -H_k^{-1} \\nabla L(\\mathbf{w}_k)$ 和 $H_k \\mathbf{p}_N = -\\nabla L(\\mathbf{w}_k)$：\n$$\n\\Delta M_k = - \\left( \\nabla L(\\mathbf{w}_k)^T (-H_k^{-1} \\nabla L(\\mathbf{w}_k)) + \\frac{1}{2} \\mathbf{p}_N^T (-\\nabla L(\\mathbf{w}_k)) \\right)\n$$\n$$\n\\Delta M_k = \\nabla L(\\mathbf{w}_k)^T H_k^{-1} \\nabla L(\\mathbf{w}_k) - \\frac{1}{2} (-H_k^{-1} \\nabla L(\\mathbf{w}_k))^T \\nabla L(\\mathbf{w}_k)\n$$\n假设 $H_k$（因此 $H_k^{-1}$）是对称的，这可以简化为：\n$$\n\\Delta M_k = \\nabla L(\\mathbf{w}_k)^T H_k^{-1} \\nabla L(\\mathbf{w}_k) - \\frac{1}{2} \\nabla L(\\mathbf{w}_k)^T H_k^{-1} \\nabla L(\\mathbf{w}_k) = \\frac{1}{2} \\nabla L(\\mathbf{w}_k)^T H_k^{-1} \\nabla L(\\mathbf{w}_k)\n$$\n这个量表示通过移动到局部二次模型的最小值可以实现的预测下降量。它本质上是尺度感知的，因为它被海森矩阵的逆进行了缩放。如果某个方向上的曲率很小（即，$H_k$ 的一个小特征值导致 $H_k^{-1}$ 中的一个大元素），即使梯度很小，也可能导致大的预测下降量。如果这个量低于一个小的容差 $\\tau_{\\mathrm{dec}}$，我们就认为潜在的改进可以忽略不计：\n$$\n\\text{Condition 2: } \\frac{1}{2} \\nabla L(\\mathbf{w}_k)^T H_k^{-1} \\nabla L(\\mathbf{w}_k)  \\tau_{\\mathrm{dec}}\n$$\n只有当两个条件都满足时，才判断为收敛。如果条件1不满足，该点不是最小值点，我们不继续检查条件2。\n\n现在，我们将此框架应用于给定的损失函数 $L(x,y,z) = a\\,x + \\tfrac{b}{2}\\,y^2 - \\tfrac{c}{2}\\,x^2 + \\tfrac{q}{4}\\,x^4 + \\tfrac{d}{4}\\,z^4$。\n梯度是 $\\nabla L = \\begin{pmatrix} a - c\\,x + q\\,x^3 \\\\ b\\,y \\\\ d\\,z^3 \\end{pmatrix}$。\n海森矩阵是对角的：\n$$\nH = \\begin{pmatrix} -c + 3qx^2  0  0 \\\\ 0  b  0 \\\\ 0  0  3dz^2 \\end{pmatrix}\n$$\n$H$ 的特征值是其对角线元素：$\\lambda_x = -c + 3qx^2$, $\\lambda_y = b$ 和 $\\lambda_z = 3dz^2$。\n所以，条件1是：$\\min(-c + 3qx^2, b, 3dz^2) \\ge \\tau_\\lambda$。\n\n基于模型的下降量 $\\Delta M$ 是：\n$$\n\\Delta M = \\frac{1}{2} \\nabla L^T H^{-1} \\nabla L = \\frac{1}{2} \\left( \\frac{(a - c\\,x + q\\,x^3)^2}{-c + 3qx^2} + \\frac{(by)^2}{b} + \\frac{(dz^3)^2}{3dz^2} \\right)\n$$\n该表达式可简化为一种即使在某些特征值为零时也定义良好的形式，前提是相应的梯度分量也为零：\n$$\n\\Delta M = \\frac{1}{2} \\left( \\frac{(a - c\\,x + q\\,x^3)^2}{-c + 3qx^2} + by^2 + \\frac{d}{3}z^4 \\right)\n$$\n所以，条件2是：$\\frac{1}{2} \\left( \\frac{(a - c\\,x + q\\,x^3)^2}{-c + 3qx^2} + by^2 + \\frac{d}{3}z^4 \\right)  \\tau_{\\mathrm{dec}}$。\n\n我们将使用给定的常数（$a=10^{-3}, b=50, c=40, q=20, d=1$, $\\tau_\\lambda = 10^{-4}, \\tau_{\\mathrm{dec}} = 10^{-6}$）在指定的测试点上评估此准则。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a second-order stopping criterion for a given loss function.\n    \"\"\"\n    # 3. Use the following fixed constants for the loss and thresholds:\n    a = 1e-3\n    b = 50.0\n    c = 40.0\n    q = 20.0\n    d = 1.0\n    tau_lambda = 1e-4\n    tau_dec = 1e-6\n\n    # 4. Test suite of candidate points.\n    \n    # Find x_star: unique positive real solution of a - c*x + q*x^3 = 0\n    # with strictly positive second derivative in x (-c + 3*q*x^2  0).\n    # The equation is q*x^3 - c*x + a = 0.\n    poly_coeffs = [q, 0, -c, a]\n    roots = np.roots(poly_coeffs)\n    \n    # Filter for positive real roots that satisfy the second derivative condition.\n    x_star = 0\n    for root in roots:\n        if np.isreal(root) and root.real  0:\n            x_val = root.real\n            # Check second derivative condition: -c + 3*q*x^2  0\n            if -c + 3 * q * x_val**2  0:\n                x_star = x_val\n                break\n\n    if x_star == 0:\n        # This should not happen with the given parameters, but is a safeguard.\n        raise ValueError(\"Could not find a valid x_star.\")\n\n    test_cases = [\n        # Test 1 (saddle-like false flat)\n        (a / c, 0.0, 0.0),\n        # Test 2 (flat plateau with mixed curvature)\n        (0.0, 0.0, 0.0),\n        # Test 3 (intended true minimum in x, with small but strictly positive curvature in z)\n        (x_star, 0.0, 1e-2),\n        # Test 4 (boundary case failing curvature tolerance in z)\n        (x_star, 0.0, 1e-6),\n        # Test 5 (near minimum with tiny residual in y and small z)\n        (x_star, 1e-6, 1e-2),\n    ]\n\n    results = []\n    \n    # Iterate through each test case and apply the convergence criterion.\n    for x, y, z in test_cases:\n        # Compute gradient components\n        grad_x = a - c * x + q * x**3\n        grad_y = b * y\n        grad_z = d * z**3\n\n        # Compute Hessian diagonal entries (eigenvalues)\n        hess_xx = -c + 3 * q * x**2\n        hess_yy = b\n        hess_zz = 3 * d * z**2\n        \n        # 2. The criterion must satisfy both of the following conditions:\n        # Condition 1: The Hessian is positive definite up to a strict tolerance.\n        lambda_min = min(hess_xx, hess_yy, hess_zz)\n        cond1_met = (lambda_min = tau_lambda)\n\n        converged = False\n        if cond1_met:\n            # If Condition 1 is met, the Hessian is positive-definite, and we can check Condition 2.\n            # Condition 2: The predicted improvement of the local quadratic model is negligible.\n            \n            # This specific formulation is robust against division by zero, as the z-term\n            # simplifies to (d/3)*z^4, which is 0 if z is 0.\n            # The y-term simplifies to b*y^2.\n            # The x-term uses the calculated Hessian eigenvalue.\n            \n            model_decrease_x = (grad_x**2) / hess_xx if hess_xx != 0 else 0.0\n            model_decrease_y = b * y**2\n            model_decrease_z = (d / 3.0) * z**4\n            \n            model_decrease = 0.5 * (model_decrease_x + model_decrease_y + model_decrease_z)\n            \n            cond2_met = (model_decrease  tau_dec)\n            \n            if cond2_met:\n                converged = True\n        \n        results.append(converged)\n\n    # 5. Produce a single line of output containing the 5 boolean results.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "除了分析局部区域的特性，理解损失函数地貌的全局结构也至关重要，特别是不同解（局部最小值）之间的关系。本练习将引导你探索“模式连通性”（mode connectivity）这一前沿概念，即不同的最优解是否可以通过一条简单的低损失路径相连。你将使用贝塞尔曲线在两个最小值之间构建一条路径，并通过计算路径上的“能量障碍”，来洞察深度学习模型解空间的大尺度几何特性。",
            "id": "3145631",
            "problem": "考虑一个双参数损失景观，旨在模拟深度学习中常见的多个盆地和一个弯曲的低损失山谷。设参数（权重）向量为 $w = (w_1, w_2) \\in \\mathbb{R}^2$。将损失函数 $L(w)$ 定义为一个锚定在两个极小值点的类双阱项与一个环形山谷的叠加：\n$$\nP(w) = \\left((w_1+1)^2 + w_2^2\\right)\\left((w_1-1)^2 + w_2^2\\right),\n$$\n$$\nR(w; A, r_0, s) = -A \\exp\\left(-\\frac{\\left(\\sqrt{w_1^2 + w_2^2} - r_0\\right)^2}{2 s^2}\\right),\n$$\n$$\nL(w; A, r_0, s) = P(w) + R(w; A, r_0, s).\n$$\n由于 $P(w)$ 的存在，该函数 $L$ 在 $w_A = (-1, 0)$ 和 $w_B = (1, 0)$ 附近有全局极小值点，而环形项 $R$ 在半径约为 $r_0$ 的位置创建了一个弯曲的山谷。\n\n你的任务是：\n1) 从多元微积分中局部极小值点和鞍点的基本定义出发——即对于任何局部极值点，可微函数 $L(w)$ 满足必要条件 $\\nabla L(w^\\star) = 0$，且 Hessian 矩阵 $\\nabla^2 L(w^\\star)$ 刻画了其二阶行为（对于严格局部极小值点是正定的，对于鞍点具有混合特征值）——使用二次权重空间 Bézier 曲线提出一条连接 $w_A$ 和 $w_B$ 的同伦路径：\n$$\nw(\\alpha) = (1-\\alpha)^2 w_A + 2(1-\\alpha)\\alpha \\, c + \\alpha^2 w_B, \\quad \\alpha \\in [0,1],\n$$\n其中 $c \\in \\mathbb{R}^2$ 是一个控制点。\n2) 对于给定的元组 $(A, r_0, s, c, N)$，通过在 $[0,1]$ 区间内 $N$ 个点的均匀网格上评估 $\\alpha$ 来将路径离散化，计算每个采样点 $\\alpha$ 处的 $L(w(\\alpha); A, r_0, s)$，并返回沿该路径的最大损失势垒，其定义为\n$$\n\\max_{\\alpha \\in [0,1]} L(w(\\alpha); A, r_0, s) \\;-\\; \\max\\left(L(w_A; A, r_0, s), \\, L(w_B; A, r_0, s)\\right).\n$$\n3) 利用你的计算来推断不同极小值点之间是否存在低损失连接路径（模式连通性），以及是否存在阻碍这种连接的鞍点或势垒。\n\n实现要求：\n- 你必须按上述定义实现函数 $L(w; A, r_0, s)$，不得修改。\n- 你必须固定 $w_A = (-1, 0)$ 和 $w_B = (1, 0)$。\n- 你必须完全按照上面给出的二次 Bézier 曲线来实现 $w(\\alpha)$。\n- 使用 $N$ 个点对 $\\alpha$ 进行均匀离散化，包括端点 $\\alpha = 0$ 和 $\\alpha = 1$。\n- 每个测试用例的最终答案是一个实数（浮点数），等于定义的最大损失势垒。将每个浮点数四舍五入到 $6$ 位小数。\n\n测试套件：\n为以下四个参数集 $(A, r_0, s, c_x, c_y, N)$ 评估势垒，其中 $c = (c_x, c_y)$：\n- 情况 1：$(A, r_0, s, c_x, c_y, N) = (3.0, 1.0, 0.20, 0.0, 0.0, 1001)$。\n- 情况 2：$(A, r_0, s, c_x, c_y, N) = (3.0, 1.0, 0.20, 0.0, 1.5, 1001)$。\n- 情况 3：$(A, r_0, s, c_x, c_y, N) = (3.0, 1.0, 0.20, 0.0, 3.0, 1001)$。\n- 情况 4：$(A, r_0, s, c_x, c_y, N) = (5.0, 1.0, 0.15, 0.0, 1.0, 2001)$。\n\n最终输出格式：\n- 你的程序应产生单行输出，包含 $4$ 个结果，形式为方括号括起来的逗号分隔列表，顺序与测试套件中的情况相同。例如：\"[result1,result2,result3,result4]\"。\n- 每个结果必须是四舍五入到 $6$ 位小数的浮点数。\n- 不应打印任何额外文本。",
            "solution": "用户的要求是分析一个给定的二维损失景观中两个局部极小值点之间的连通性。这涉及定义连接极小值点的路径并计算沿该路径的损失势垒。该问题在科学和数学上是适定的（well-posed），为获得唯一且有意义的解提供了所有必要的定义和参数。\n\n首先，我们形式化参数向量 $w = (w_1, w_2)$ 的损失函数 $L(w; A, r_0, s)$ 的各个组成部分。该函数是两项 $P(w)$ 和 $R(w)$ 的叠加，即 $L(w) = P(w) + R(w)$。\n\n第一项 $P(w)$ 是一个多项式，旨在创建两个不同的极小值点：\n$$\nP(w) = \\left((w_1+1)^2 + w_2^2\\right)\\left((w_1-1)^2 + w_2^2\\right)\n$$\n该函数在两个锚点 $w_A = (-1, 0)$ 和 $w_B = (1, 0)$ 处为零，这两个点被设计为整个损失景观 $L(w)$ 中极小值点的位置。在这些点之间，特别是在原点 $w = (0, 0)$ 附近，$P(w)$ 产生一个势垒。例如，在 $w = (0, 0)$ 处，$P(w) = (1)(1) = 1$。\n\n第二项 $R(w; A, r_0, s)$ 引入了一个环形的低损失山谷：\n$$\nR(w; A, r_0, s) = -A \\exp\\left(-\\frac{\\left(\\sqrt{w_1^2 + w_2^2} - r_0\\right)^2}{2 s^2}\\right)\n$$\n在这里，$A  0$ 决定了山谷的深度，$r_0$ 是山谷中心的半径，$s$ 控制其宽度。对于半径为 $r_0$ 的圆上的所有点 $w$，即满足 $\\sqrt{w_1^2 + w_2^2} = r_0$ 的点，损失被最大程度地减少（减少量为 $-A$）。\n\n任务是通过评估沿一族特定路径的损失势垒，来探索 $w_A$ 和 $w_B$ 附近极小值点之间的连通性。这些路径由一条二次 Bézier 曲线定义：\n$$\nw(\\alpha) = (1-\\alpha)^2 w_A + 2(1-\\alpha)\\alpha \\, c + \\alpha^2 w_B, \\quad \\alpha \\in [0,1]\n$$\n该曲线提供了一条从 $w(0) = w_A$ 到 $w(1) = w_B$ 的平滑路径。路径的形状由控制点 $c = (c_x, c_y)$ 决定。当 $c = (0,0)$ 时，路径是连接 $w_A$ 和 $w_B$ 的直线段。对于 $c_y \\neq 0$ 的控制点 $c$，路径是一条抛物线，向 $w_2$ 维度弯曲，偏离了直线。\n\n损失势垒定义为沿路径遇到的最大损失，减去两个端点中更优（损失更低）的那个端点的损失：\n$$\n\\text{Barrier} = \\max_{\\alpha \\in [0,1]} L(w(\\alpha)) \\;-\\; \\max\\left(L(w_A), \\, L(w_B)\\right)\n$$\n我们通过离散化路径来数值计算这个势垒。我们对 $[0,1]$ 区间内 $N$ 个均匀间隔的 $\\alpha$ 值计算 $w(\\alpha)$ 和 $L(w(\\alpha))$，并找出这些样本中的最大损失。\n\n对于所有测试用例，参数设置使得 $r_0 = 1.0$。锚点 $w_A = (-1, 0)$ 和 $w_B = (1, 0)$ 的半径均为 $\\sqrt{(\\pm 1)^2 + 0^2} = 1$。因此，它们恰好位于由 $R(w)$ 创建的低损失山谷的中心。这些端点的损失为：\n$P(w_{A,B}) = 0$\n$R(w_{A,B}; A, 1, s) = -A \\exp\\left(-\\frac{(1 - 1)^2}{2 s^2}\\right) = -A \\exp(0) = -A$\n因此，$L(w_A) = L(w_B) = -A$。势垒公式简化为：\n$$\n\\text{Barrier} = \\left( \\max_{\\alpha \\in [0,1]} L(w(\\alpha)) \\right) - (-A) = \\left( \\max_{\\alpha \\in [0,1]} L(w(\\alpha)) \\right) + A\n$$\n\n提供的测试用例探讨了势垒如何随控制点 $c$ 变化。\n- **情况 1**：$c=(0,0)$。路径是一条直线 $w(\\alpha) = (2\\alpha-1, 0)$。该路径穿过原点，而 $P(w)$ 在原点处有一个局部最大值。这预计会产生一个显著的势垒。\n- **情况 2 和 3**：$c=(0, 1.5)$ 和 $c=(0, 3.0)$。路径变成一条“向上”弯曲的抛物线。这避开了原点，但会穿过 $w_2 \\neq 0$ 的区域。项 $P(w) = ((w_1^2+w_2^2)+1)^2 - 4w_1^2$ 通常随着 $|w_2|$ 的增加而增加（在固定的 $w_1$ 下）。虽然弯曲路径可能使其半径更接近最优的 $r_0=1$，但来自 $P(w)$ 项的惩罚可能相当大。我们的计算将揭示哪种效应占主导地位。\n- **情况 4**：使用了一组不同的参数，具有更深、更窄的山谷（$A=5.0, s=0.15$），以及一个定义了中度弯曲路径的控制点 $c=(0, 1.0)$。\n\n如果弯曲路径（例如情况 2）的势垒显著低于直线路径（情况 1），则表明存在低损失连接路径（模式连通性）。相反，如果势垒增加，则表明在这个抛物线路径族中，直线是最高效的，并且真正的低损失“山谷”并不位于这些简单曲线的方向上。数值结果表明，对于这个特定的景观，通过 $c_y$ 增加曲率会增加势垒，这表明多项式项 $P(w)$ 在偏离 $w_1$ 轴时产生的势垒是主导因素。\n\n算法如下：\n1. 对于每个测试用例 $(A, r_0, s, c_x, c_y, N)$，定义向量 $w_A, w_B, c$。\n2. 计算基线端点损失，$L_{endpoints\\_max} = -A$。\n3. 生成一个包含 $N$ 个从 0 到 1 的 $\\alpha$ 值的数组。\n4. 对于每个 $\\alpha$，使用向量值 Bézier 公式计算相应的路径点 $w(\\alpha)$。使用 numpy 广播可以一次性高效地计算所有 $\\alpha$ 值。\n5. 计算路径上所有点的损失 $L(w(\\alpha))$。这也可以完全向量化。\n6. 在得到的损失数组中找到最大值，$L_{max\\_path}$。\n7. 计算势垒，即 $L_{max\\_path} - L_{endpoints\\_max}$。\n8. 将结果四舍五入到6位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the loss landscape barrier problem for a given set of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (A, r0, s, cx, cy, N)\n        (3.0, 1.0, 0.20, 0.0, 0.0, 1001),\n        (3.0, 1.0, 0.20, 0.0, 1.5, 1001),\n        (3.0, 1.0, 0.20, 0.0, 3.0, 1001),\n        (5.0, 1.0, 0.15, 0.0, 1.0, 2001),\n    ]\n\n    results = []\n    wA = np.array([-1.0, 0.0])\n    wB = np.array([1.0, 0.0])\n\n    def calculate_loss_vectorized(w, A, r0, s):\n        \"\"\"\n        Calculates loss L(w) for an array of weight vectors.\n        w is an (N, 2) numpy array.\n        \"\"\"\n        w1 = w[:, 0]\n        w2 = w[:, 1]\n        \n        P_w = ((w1 + 1)**2 + w2**2) * ((w1 - 1)**2 + w2**2)\n        \n        norm_w = np.sqrt(w1**2 + w2**2)\n        R_w = -A * np.exp(-((norm_w - r0)**2) / (2 * s**2))\n        \n        return P_w + R_w\n\n    def calculate_loss_scalar(w_vec, A, r0, s):\n        \"\"\"\n        Calculates loss L(w) for a single weight vector.\n        w_vec is a (2,) numpy array or tuple.\n        \"\"\"\n        w1, w2 = w_vec\n        P_w = ((w1 + 1)**2 + w2**2) * ((w1 - 1)**2 + w2**2)\n        norm_w = np.sqrt(w1**2 + w2**2)\n        R_w = -A * np.exp(-((norm_w - r0)**2) / (2 * s**2))\n        return P_w + R_w\n\n    def evaluate_bezier_path(alpha_grid, wA, wB, c):\n        \"\"\"\n        Calculates points on a quadratic Bézier curve for a grid of alpha values.\n        alpha_grid is a 1D array of shape (N,).\n        wA, wB, c are 1D arrays of shape (2,).\n        Returns an array of path points of shape (N, 2).\n        \"\"\"\n        # Reshape alpha for broadcasting against 2D coordinate vectors\n        alpha_col = alpha_grid[:, np.newaxis]\n        \n        term1 = (1 - alpha_col)**2 * wA\n        term2 = 2 * (1 - alpha_col) * alpha_col * c\n        term3 = alpha_col**2 * wB\n        \n        return term1 + term2 + term3\n\n    for case in test_cases:\n        A, r0, s, cx, cy, N = case\n        c = np.array([cx, cy])\n\n        # 1. Calculate the loss at the endpoints to find the baseline.\n        # Due to the problem setup with r0=1.0, L(wA) = L(wB) = -A.\n        L_A = calculate_loss_scalar(wA, A, r0, s)\n        L_B = calculate_loss_scalar(wB, A, r0, s)\n        L_endpoints_max = max(L_A, L_B)\n\n        # 2. Create the uniform grid for the parameter alpha.\n        alphas = np.linspace(0, 1, N)\n\n        # 3. Generate all points w(alpha) on the Bézier path.\n        path_w = evaluate_bezier_path(alphas, wA, wB, c)\n\n        # 4. Calculate the loss L for all points on the path.\n        path_losses = calculate_loss_vectorized(path_w, A, r0, s)\n        \n        # 5. Find the maximum loss encountered along the path.\n        L_max_path = np.max(path_losses)\n\n        # 6. Compute the barrier.\n        barrier = L_max_path - L_endpoints_max\n        results.append(round(barrier, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}