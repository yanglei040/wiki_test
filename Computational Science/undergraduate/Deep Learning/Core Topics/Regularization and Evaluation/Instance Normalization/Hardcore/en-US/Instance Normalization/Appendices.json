{
    "hands_on_practices": [
        {
            "introduction": "Instance Normalization's power in applications like style transfer comes from a two-step process: first removing an image's original style by standardizing its feature statistics, and then imposing a new style. This second step is achieved through a simple affine transformation with parameters $\\gamma$ and $\\beta$. This exercise challenges you to derive the exact values of these parameters needed to transform a feature map to a new target mean and standard deviation, revealing the mathematical core of stylistic control .",
            "id": "3138582",
            "problem": "Consider a single-channel feature map in a Convolutional Neural Network (CNN) for an image of spatial dimensions $H \\times W$. Instance Normalization (IN) computes per-instance, per-channel statistics across the spatial grid. Let the source feature map be denoted by $\\{x_{ij}\\}_{i=1,\\dots,H;\\, j=1,\\dots,W}$ with source mean $\\mu_{s}$ and source standard deviation $\\sigma_{s}$ across the spatial dimensions, where the mean and standard deviation are defined by\n$$\n\\mu_{s} \\triangleq \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_{ij}, \\quad \\sigma_{s} \\triangleq \\sqrt{\\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\left(x_{ij} - \\mu_{s}\\right)^{2}},\n$$\nand assume $\\sigma_{s} > 0$. Suppose we wish to produce a transformed feature map\n$$\ny_{ij} \\triangleq \\gamma \\cdot \\frac{x_{ij} - \\mu_{s}}{\\sigma_{s}} + \\beta\n$$\nwhose mean and standard deviation across $(H,W)$ equal specified target style statistics $\\mu_{t}$ and $\\sigma_{t}$, respectively, with $\\sigma_{t} > 0$.\n\nUsing only the core definitions of mean, variance, and standard deviation, determine the values of the affine parameters $(\\gamma, \\beta)$ that achieve target mean $\\mu_{t}$ and target standard deviation $\\sigma_{t}$ across the $(H,W)$ spatial dimensions. For uniqueness, select the positive scaling solution for $\\gamma$. Express your final answer as a closed-form analytic expression for $(\\gamma, \\beta)$.",
            "solution": "The user wants to determine the affine parameters $(\\gamma, \\beta)$ for an Instance Normalization (IN) transformation. The transformation is defined as:\n$$\ny_{ij} \\triangleq \\gamma \\cdot \\frac{x_{ij} - \\mu_{s}}{\\sigma_{s}} + \\beta\n$$\nwhere $\\{x_{ij}\\}$ is the source feature map with mean $\\mu_s$ and standard deviation $\\sigma_s$. The goal is for the transformed feature map $\\{y_{ij}\\}$ to have a target mean $\\mu_t$ and a target standard deviation $\\sigma_t$. We are given that $\\sigma_s > 0$ and $\\sigma_t > 0$.\n\nFirst, let us define a standardized variable $\\hat{x}_{ij}$ as the normalized source feature:\n$$\n\\hat{x}_{ij} \\triangleq \\frac{x_{ij} - \\mu_{s}}{\\sigma_{s}}\n$$\nThe transformation can now be written more compactly as:\n$$\ny_{ij} = \\gamma \\hat{x}_{ij} + \\beta\n$$\nOur strategy is to first compute the mean and standard deviation of the standardized feature map $\\{\\hat{x}_{ij}\\}$ and then use these properties to find the values of $\\gamma$ and $\\beta$ that satisfy the target conditions for $\\{y_{ij}\\}$.\n\nLet us calculate the mean of $\\{\\hat{x}_{ij}\\}$, which we denote as $\\mu_{\\hat{x}}$.\n$$\n\\mu_{\\hat{x}} = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\hat{x}_{ij}\n$$\nSubstituting the definition of $\\hat{x}_{ij}$:\n$$\n\\mu_{\\hat{x}} = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\left( \\frac{x_{ij} - \\mu_{s}}{\\sigma_{s}} \\right)\n$$\nUsing the linearity of summation, we can separate the terms:\n$$\n\\mu_{\\hat{x}} = \\frac{1}{\\sigma_{s}} \\left( \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_{ij} - \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\mu_{s} \\right)\n$$\nBy definition, $\\mu_s = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} x_{ij}$. The second term simplifies to $\\frac{1}{HW} (HW \\cdot \\mu_s) = \\mu_s$.\n$$\n\\mu_{\\hat{x}} = \\frac{1}{\\sigma_{s}} (\\mu_{s} - \\mu_{s}) = 0\n$$\nThus, the standardized feature map has a mean of $0$.\n\nNext, let us calculate the variance of $\\{\\hat{x}_{ij}\\}$, denoted as $\\sigma_{\\hat{x}}^2$.\n$$\n\\sigma_{\\hat{x}}^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (\\hat{x}_{ij} - \\mu_{\\hat{x}})^2\n$$\nSince $\\mu_{\\hat{x}} = 0$, this simplifies to:\n$$\n\\sigma_{\\hat{x}}^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\hat{x}_{ij}^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\left( \\frac{x_{ij} - \\mu_{s}}{\\sigma_{s}} \\right)^2\n$$\n$$\n\\sigma_{\\hat{x}}^2 = \\frac{1}{\\sigma_{s}^2} \\left( \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (x_{ij} - \\mu_{s})^2 \\right)\n$$\nThe term in the parenthesis is the definition of the source variance, $\\sigma_{s}^2$.\n$$\n\\sigma_{\\hat{x}}^2 = \\frac{1}{\\sigma_{s}^2} (\\sigma_{s}^2) = 1\n$$\nThe standard deviation of $\\{\\hat{x}_{ij}\\}$ is $\\sigma_{\\hat{x}} = \\sqrt{1} = 1$. The standardized feature map has a mean of $0$ and a standard deviation of $1$.\n\nNow, we can determine the parameters $\\gamma$ and $\\beta$ by imposing the target statistics on $y_{ij} = \\gamma \\hat{x}_{ij} + \\beta$.\n\nThe mean of the target feature map, $\\mu_y$, must equal $\\mu_t$.\n$$\n\\mu_y = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} y_{ij} = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (\\gamma \\hat{x}_{ij} + \\beta)\n$$\nUsing the linearity of summation again:\n$$\n\\mu_y = \\gamma \\left( \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\hat{x}_{ij} \\right) + \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\beta\n$$\n$$\n\\mu_y = \\gamma \\mu_{\\hat{x}} + \\beta\n$$\nSubstituting $\\mu_{\\hat{x}} = 0$:\n$$\n\\mu_y = \\gamma (0) + \\beta = \\beta\n$$\nWe require $\\mu_y = \\mu_t$, so we find the value for $\\beta$:\n$$\n\\beta = \\mu_t\n$$\n\nThe standard deviation of the target feature map, $\\sigma_y$, must equal $\\sigma_t$. We first compute the variance, $\\sigma_y^2$.\n$$\n\\sigma_y^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (y_{ij} - \\mu_y)^2\n$$\nSubstituting $y_{ij} = \\gamma \\hat{x}_{ij} + \\beta$ and $\\mu_y = \\beta$:\n$$\n\\sigma_y^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} ((\\gamma \\hat{x}_{ij} + \\beta) - \\beta)^2 = \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (\\gamma \\hat{x}_{ij})^2\n$$\n$$\n\\sigma_y^2 = \\gamma^2 \\left( \\frac{1}{HW} \\sum_{i=1}^{H} \\sum_{j=1}^{W} \\hat{x}_{ij}^2 \\right)\n$$\nThe term in the parenthesis is the variance of $\\{\\hat{x}_{ij}\\}$, which is $\\sigma_{\\hat{x}}^2 = 1$.\n$$\n\\sigma_y^2 = \\gamma^2 \\sigma_{\\hat{x}}^2 = \\gamma^2 (1) = \\gamma^2\n$$\nThe standard deviation $\\sigma_y$ is the square root of the variance:\n$$\n\\sigma_y = \\sqrt{\\gamma^2} = |\\gamma|\n$$\nWe require $\\sigma_y = \\sigma_t$. This gives the condition:\n$$\n|\\gamma| = \\sigma_t\n$$\nSince $\\sigma_t > 0$, this yields two possible solutions: $\\gamma = \\sigma_t$ or $\\gamma = -\\sigma_t$. The problem statement specifies selecting the positive scaling solution for $\\gamma$. Therefore, we must choose:\n$$\n\\gamma = \\sigma_t\n$$\n\nThe affine parameters that achieve the target statistics are $(\\gamma, \\beta) = (\\sigma_t, \\mu_t)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sigma_{t} & \\mu_{t}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "One of the most significant practical advantages of Instance Normalization (IN) over Batch Normalization (BN) is its insensitivity to batch size. This property makes IN particularly suitable for tasks where large batches are infeasible, such as in high-resolution image processing or certain federated learning scenarios. In this exercise, you will engage in a thought experiment to reason about how and why the performance of IN and BN diverges as batch size shrinks, solidifying your understanding of their fundamental operational differences .",
            "id": "3138579",
            "problem": "Consider a convolutional neural network trained for image classification using either Batch Normalization (BN) or Instance Normalization (IN) layers. Let the batch size be denoted by $B$. Suppose you conduct a controlled experiment in which all hyperparameters other than $B$ are held fixed, and $B$ is reduced from $64$ to $1$. Validation accuracy is measured on a held-out set drawn from the same distribution as the training set. Starting from the core definitions that BN normalizes using per-channel batch-level statistics and that IN normalizes using per-instance, per-channel statistics, and from well-tested facts in statistical estimation (for example, that the variance of a sample mean decreases as the number of samples increases), hypothesize how validation accuracy will vary for BN versus IN as $B$ shrinks. Your reasoning should rely on the independence of IN’s per-instance statistics $(\\mu_{nc},\\sigma_{nc})$ from $B$, and on how the reliability of batch-level estimators depends on $B$.\n\nWhich option best describes the expected trend?\n\nA. As $B$ decreases from $64$ to $1$, BN’s validation accuracy degrades noticeably due to noisier batch statistics and train–test normalization mismatch, while IN’s validation accuracy remains approximately stable because its per-instance statistics $(\\mu_{nc},\\sigma_{nc})$ do not depend on $B$.\n\nB. As $B$ decreases from $64$ to $1$, BN’s validation accuracy remains approximately constant, while IN’s validation accuracy degrades sharply because reduced batch diversity harms IN’s per-instance statistics $(\\mu_{nc},\\sigma_{nc})$.\n\nC. As $B$ decreases from $64$ to $1$, both BN and IN show improved validation accuracy because smaller batches act as a regularizer that stabilizes the learned normalization.\n\nD. As $B$ decreases from $64$ to $1$, BN’s validation accuracy improves because batch statistics become more representative of each individual sample, while IN’s validation accuracy worsens due to over-normalization of per-instance features.",
            "solution": "Let's analyze the behavior of Batch Normalization (BN) and Instance Normalization (IN) as the batch size $B$ decreases, based on their definitions.\n\n**Batch Normalization (BN):**\nBy definition, BN computes normalization statistics (mean and variance) for each feature channel across all instances in a mini-batch. The number of samples used for this estimation is proportional to the batch size $B$.\n- When $B$ is large (e.g., 64), the batch statistics are reliable estimates of the true statistics of the data distribution.\n- As $B$ shrinks, the number of samples used for estimation decreases. The calculated batch statistics become \"noisier\" and more dependent on the specific samples in that small batch. They are less representative of the overall data distribution.\n- During inference (validation), BN uses population statistics that are estimated during training, typically via a running average. If training was performed with small, noisy batches, the training-time normalization statistics will differ significantly from the stable running average statistics used at inference time. This \"train-test mismatch\" can significantly degrade model performance.\n- Therefore, as $B$ decreases from 64 to 1, the validation accuracy of a BN-based model is expected to degrade noticeably.\n\n**Instance Normalization (IN):**\nBy definition, IN computes normalization statistics for each feature channel independently for each instance. The statistics are calculated over the spatial dimensions ($H \\times W$) of a single feature map.\n- The calculation for any given instance is completely independent of the batch size $B$ and any other instances in the batch.\n- The number of samples used for the statistical estimation ($H \\times W$) is constant regardless of the batch size.\n- Because the normalization procedure is identical during training and inference (it is always performed per-instance), there is no train-test mismatch related to the normalization scheme.\n- Therefore, as $B$ decreases, the validation accuracy of an IN-based model is expected to remain approximately stable.\n\n**Evaluating the Options:**\n- **A:** This option correctly states that BN's accuracy will degrade due to noisy statistics and train-test mismatch, while IN's accuracy remains stable because its calculations are independent of batch size. This aligns with our analysis.\n- **B:** This is incorrect. BN's performance is highly dependent on batch size. IN's statistics are not harmed by reduced batch diversity as they are computed per-instance.\n- **C:** This is incorrect. While small batches can have a regularizing effect, for BN the destabilization of normalization statistics is the dominant and detrimental effect. IN's stability is not affected by batch size.\n- **D:** This is incorrect. The goal of BN is to approximate global statistics, not per-sample statistics. Making the statistics per-sample (as in $B=1$) is what causes the performance drop. The claim about IN worsening is unsubstantiated.\n\nThe most accurate hypothesis is A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Transitioning from theoretical understanding to practical implementation often reveals subtle but critical challenges, with tensor broadcasting being a frequent source of bugs in deep learning code. This coding exercise tasks you with implementing Instance Normalization correctly, focusing on ensuring that statistics are computed strictly on a per-channel basis. By also creating a deliberately \"buggy\" version that incorrectly mixes channel information, you will learn to diagnose and prevent this common pitfall .",
            "id": "3138633",
            "problem": "You are asked to implement and test the broadcasting semantics of Instance Normalization (IN) in a four-dimensional tensor setting that is standard in deep learning. Work with a feature tensor $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$, where $B$ is the batch size, $C$ is the number of channels, and $H$, $W$ are spatial dimensions. The goal is to implement a correct per-instance, per-channel normalization and to expose a common implementation pitfall where broadcasting mixes channels by using a per-instance mean across all channels.\n\nStart from the following fundamental base:\n- For any finite set of real numbers, the sample mean is the average of the values, and the sample variance is the average of squared deviations from the mean.\n- For $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$, it is meaningful to compute statistics over the spatial axes for each fixed $(b,c)$ pair. Use a small positive constant $\\epsilon$ added inside the square root to ensure numerical stability.\n\nTasks:\n1. Implement a function that, for each batch index $b \\in \\{0,\\dots,B-1\\}$ and channel index $c \\in \\{0,\\dots,C-1\\}$, computes a mean $\\mu_{b,c} \\in \\mathbb{R}$ and variance $\\sigma^2_{b,c} \\in \\mathbb{R}$ over the spatial dimensions $\\{0,\\dots,H-1\\} \\times \\{0,\\dots,W-1\\}$ only, broadcasts them to shape $\\mathbb{R}^{B \\times C \\times 1 \\times 1}$, and uses them to normalize $X$ and then apply a per-channel affine transform specified by $\\gamma \\in \\mathbb{R}^{C}$ and $\\beta \\in \\mathbb{R}^{C}$. The normalization must strictly be per-instance, per-channel over spatial dimensions, and the affine parameters must be broadcast over the correct axes, never mixing channels.\n2. Implement a deliberately buggy variant that computes a per-instance mean and variance across all channels and spatial dimensions for each batch index $b$, namely $\\tilde{\\mu}_{b} \\in \\mathbb{R}$ and $\\tilde{\\sigma}^2_{b} \\in \\mathbb{R}$ with shapes $\\mathbb{R}^{B \\times 1 \\times 1 \\times 1}$, broadcasts these across channels, and then applies the same per-channel affine transform. This simulates the broadcasting pitfall that mixes channels by using $\\tilde{\\mu}_{b}$ and $\\tilde{\\sigma}^2_{b}$ in place of $\\mu_{b,c}$ and $\\sigma^2_{b,c}$.\n3. For each test case below, compute the maximum absolute difference between the correctly normalized output and the buggy normalized output. Use $\\epsilon = 10^{-5}$ and ensure all computations are performed in real-valued arithmetic. The affine parameters must broadcast per channel, not per batch nor per spatial location.\n\nDefinitions (do not implement formulas directly here; derive them from the definitions in your solution):\n- For fixed $(b,c)$, the mean over spatial axes is the average of the $H \\times W$ entries $X_{b,c,h,w}$ as $(h,w)$ ranges over all spatial indices.\n- For fixed $(b,c)$, the variance over spatial axes is the average of the squared deviations of $X_{b,c,h,w}$ from the mean for $(h,w)$ over all spatial indices.\n- For the buggy variant, the mean for batch index $b$ is the average of all $C \\times H \\times W$ entries in $X_{b,:,:,:}$, and the variance for batch index $b$ is the average of their squared deviations.\n\nTest Suite:\n- Case A (happy path, different channel statistics):\n  - Dimensions: $B=2$, $C=2$, $H=2$, $W=2$.\n  - Tensor $X$ is specified by:\n    $$X_{0,0,:,:} = \\begin{bmatrix} 0 & 2 \\\\ 4 & 6 \\end{bmatrix}, \\quad X_{0,1,:,:} = \\begin{bmatrix} -6 & -4 \\\\ -2 & 0 \\end{bmatrix},$$\n    $$X_{1,0,:,:} = \\begin{bmatrix} 10 & 12 \\\\ 14 & 16 \\end{bmatrix}, \\quad X_{1,1,:,:} = \\begin{bmatrix} -10 & -12 \\\\ -14 & -16 \\end{bmatrix}.$$\n  - Affine parameters: $\\gamma = [1, 1]$, $\\beta = [0, 0]$.\n- Case B (edge case, one batch, three channels, one spatial dimension degenerate):\n  - Dimensions: $B=1$, $C=3$, $H=1$, $W=4$.\n  - Tensor $X$ is specified by:\n    $$X_{0,0,:,:} = \\begin{bmatrix} 1 & 2 & 3 & 4 \\end{bmatrix}, \\quad X_{0,1,:,:} = \\begin{bmatrix} 10 & 10 & 10 & 10 \\end{bmatrix},$$\n    $$X_{0,2,:,:} = \\begin{bmatrix} -1 & -2 & -3 & -4 \\end{bmatrix}.$$\n  - Affine parameters: $\\gamma = [1, 0.5, 2]$, $\\beta = [0, 1, -1]$.\n- Case C (boundary where buggy broadcasting is harmless because there is only one channel):\n  - Dimensions: $B=1$, $C=1$, $H=2$, $W=2$.\n  - Tensor $X$ is specified by:\n    $$X_{0,0,:,:} = \\begin{bmatrix} 5 & 7 \\\\ 9 & 11 \\end{bmatrix}.$$\n  - Affine parameters: $\\gamma = [1]$, $\\beta = [0]$.\n- Case D (extreme case, single spatial element per channel):\n  - Dimensions: $B=2$, $C=3$, $H=1$, $W=1$.\n  - Tensor $X$ is specified by:\n    $$X_{0,0,0,0} = 0, \\quad X_{0,1,0,0} = 10, \\quad X_{0,2,0,0} = -10,$$\n    $$X_{1,0,0,0} = 5, \\quad X_{1,1,0,0} = 15, \\quad X_{1,2,0,0} = -5.$$\n  - Affine parameters: $\\gamma = [1, 1, 1]$, $\\beta = [0, 0, 0]$.\n\nOutput Specification:\n- Your program should produce a single line of output containing the maximum absolute differences for Cases A, B, C, and D, in this order, as a comma-separated list enclosed in square brackets (e.g., `[resultA,resultB,resultC,resultD]`).\n- Each result must be a floating-point number.\n\nNo physical units or angle units are involved in this problem. All numbers are pure real values. Ensure scientific realism and correct broadcasting semantics per the definitions given above; do not mix channels when computing per-channel statistics. Use $\\epsilon = 10^{-5}$ for all computations.",
            "solution": "The problem requires the implementation and comparison of two variants of normalization for a four-dimensional tensor $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$, where $B$ is the batch size, $C$ is the number of channels, and $H, W$ are spatial dimensions. One variant is the correct Instance Normalization (IN), and the other is a deliberately buggy version that incorrectly computes statistics across channels.\n\nLet the input tensor be $X$ with dimensions $(B, C, H, W)$. Let $\\gamma \\in \\mathbb{R}^{C}$ and $\\beta \\in \\mathbb{R}^{C}$ be the per-channel learnable affine parameters (scale and shift). A small constant $\\epsilon = 10^{-5}$ is used for numerical stability.\n\nFirst, we define the correct Instance Normalization procedure. For each instance $b \\in \\{0, \\dots, B-1\\}$ and each channel $c \\in \\{0, \\dots, C-1\\}$, the mean $\\mu_{b,c}$ and variance $\\sigma^2_{b,c}$ are computed independently over the spatial dimensions $(H, W)$. The number of spatial elements is $N_{sp} = H \\times W$.\n\nThe mean for instance $b$ and channel $c$ is:\n$$ \\mu_{b,c} = \\frac{1}{N_{sp}} \\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} X_{b,c,h,w} $$\nThe variance for instance $b$ and channel $c$ is:\n$$ \\sigma^2_{b,c} = \\frac{1}{N_{sp}} \\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} (X_{b,c,h,w} - \\mu_{b,c})^2 $$\nUsing these statistics, the tensor $X$ is normalized. Note that for each element $X_{b,c,h,w}$, the corresponding $\\mu_{b,c}$ and $\\sigma^2_{b,c}$ are used. Computationally, this is achieved by broadcasting the $\\mu$ and $\\sigma^2$ tensors, which have an effective shape of $\\mathbb{R}^{B \\times C \\times 1 \\times 1}$, over the full $\\mathbb{R}^{B \\times C \\times H \\times W}$ tensor. The normalized tensor $\\hat{X}$ is given by:\n$$ \\hat{X}_{b,c,h,w} = \\frac{X_{b,c,h,w} - \\mu_{b,c}}{\\sqrt{\\sigma^2_{b,c} + \\epsilon}} $$\nFinally, the per-channel affine transformation is applied. The scale $\\gamma_c$ and shift $\\beta_c$ are broadcast across the batch and spatial dimensions. The correct final output $Y$ is:\n$$ Y_{b,c,h,w} = \\gamma_c \\hat{X}_{b,c,h,w} + \\beta_c $$\n\nSecond, we define the buggy normalization procedure. This variant simulates a common implementation error where statistics are computed across channels instead of per-channel. For each instance $b \\in \\{0, \\dots, B-1\\}$, a single mean $\\tilde{\\mu}_{b}$ and variance $\\tilde{\\sigma}^2_{b}$ are computed over all channels and spatial dimensions $(C, H, W)$. The number of elements per instance is $N_{all} = C \\times H \\times W$.\n\nThe per-instance mean is:\n$$ \\tilde{\\mu}_{b} = \\frac{1}{N_{all}} \\sum_{c=0}^{C-1} \\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} X_{b,c,h,w} $$\nThe per-instance variance is:\n$$ \\tilde{\\sigma}^2_{b} = \\frac{1}{N_{all}} \\sum_{c=0}^{C-1} \\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} (X_{b,c,h,w} - \\tilde{\\mu}_{b})^2 $$\nThese statistics, with effective shape $\\mathbb{R}^{B \\times 1 \\times 1 \\times 1}$, are then incorrectly broadcast across all channels for normalization. The incorrectly normalized tensor $\\tilde{X}$ is given by:\n$$ \\tilde{X}_{b,c,h,w} = \\frac{X_{b,c,h,w} - \\tilde{\\mu}_{b}}{\\sqrt{\\tilde{\\sigma}^2_{b} + \\epsilon}} $$\nThe affine transformation is applied identically to the correct version, maintaining its per-channel nature. The buggy final output $\\tilde{Y}$ is:\n$$ \\tilde{Y}_{b,c,h,w} = \\gamma_c \\tilde{X}_{b,c,h,w} + \\beta_c $$\n\nThe final task is to compute the maximum absolute difference between the two outputs for each test case:\n$$ \\text{MaxAbsDiff} = \\max_{b,c,h,w} | Y_{b,c,h,w} - \\tilde{Y}_{b,c,h,w} | $$\nIn our implementation, we will use `numpy`'s `mean` and `var` functions with the appropriate `axis` and `keepdims=True` arguments to efficiently compute these statistics and leverage broadcasting. For the affine parameters $\\gamma$ and $\\beta$, given as $1$-D arrays of size $C$, we will reshape them to $(1, C, 1, 1)$ to ensure correct per-channel broadcasting. The difference calculation will then be a straightforward element-wise operation followed by finding the maximum value.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares correct and buggy Instance Normalization\n    for a 4D tensor, and calculates the maximum absolute difference\n    between their outputs for a suite of test cases.\n    \"\"\"\n    epsilon = 1e-5\n\n    test_cases = [\n        # Case A: Happy path, B=2, C=2, H=2, W=2\n        {\n            \"X\": np.array([\n                [[[0, 2], [4, 6]], [[-6, -4], [-2, 0]]],\n                [[[10, 12], [14, 16]], [[-10, -12], [-14, -16]]]\n            ], dtype=float),\n            \"gamma\": np.array([1, 1], dtype=float),\n            \"beta\": np.array([0, 0], dtype=float)\n        },\n        # Case B: Edge case, B=1, C=3, H=1, W=4\n        {\n            \"X\": np.array([\n                [[[1, 2, 3, 4]]],\n                [[[10, 10, 10, 10]]],\n                [[[-1, -2, -3, -4]]]\n            ], dtype=float).reshape(1, 3, 1, 4),\n            \"gamma\": np.array([1, 0.5, 2], dtype=float),\n            \"beta\": np.array([0, 1, -1], dtype=float)\n        },\n        # Case C: Boundary case, B=1, C=1, H=2, W=2\n        {\n            \"X\": np.array([[[[5, 7], [9, 11]]]], dtype=float),\n            \"gamma\": np.array([1], dtype=float),\n            \"beta\": np.array([0], dtype=float)\n        },\n        # Case D: Extreme case, B=2, C=3, H=1, W=1\n        {\n            \"X\": np.array([\n                [[[0]], [[10]], [[-10]]],\n                [[[5]], [[15]], [[-5]]]\n            ], dtype=float),\n            \"gamma\": np.array([1, 1, 1], dtype=float),\n            \"beta\": np.array([0, 0, 0], dtype=float)\n        }\n    ]\n\n    results = []\n\n    def correct_instance_norm(X, gamma, beta, eps):\n        C = X.shape[1]\n        \n        # Per-instance, per-channel statistics over spatial dimensions (axes 2, 3)\n        mu = np.mean(X, axis=(2, 3), keepdims=True)\n        var = np.var(X, axis=(2, 3), keepdims=True)\n\n        X_normalized = (X - mu) / np.sqrt(var + eps)\n\n        # Reshape affine parameters for broadcasting\n        gamma_reshaped = gamma.reshape(1, C, 1, 1)\n        beta_reshaped = beta.reshape(1, C, 1, 1)\n        \n        return gamma_reshaped * X_normalized + beta_reshaped\n\n    def buggy_instance_norm(X, gamma, beta, eps):\n        C = X.shape[1]\n        \n        # Per-instance statistics over channel and spatial dimensions (axes 1, 2, 3)\n        mu = np.mean(X, axis=(1, 2, 3), keepdims=True)\n        var = np.var(X, axis=(1, 2, 3), keepdims=True)\n\n        X_normalized = (X - mu) / np.sqrt(var + eps)\n        \n        # Reshape affine parameters for broadcasting\n        gamma_reshaped = gamma.reshape(1, C, 1, 1)\n        beta_reshaped = beta.reshape(1, C, 1, 1)\n        \n        return gamma_reshaped * X_normalized + beta_reshaped\n\n    for case in test_cases:\n        X = case[\"X\"]\n        gamma = case[\"gamma\"]\n        beta = case[\"beta\"]\n\n        Y_correct = correct_instance_norm(X, gamma, beta, epsilon)\n        Y_buggy = buggy_instance_norm(X, gamma, beta, epsilon)\n\n        max_abs_diff = np.max(np.abs(Y_correct - Y_buggy))\n        results.append(max_abs_diff)\n    \n    # Print the results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}