{
    "hands_on_practices": [
        {
            "introduction": "批归一化在训练和推理（或评估）阶段的行为有本质区别，正确区分这两种模式至关重要。此练习将通过一个引人注目的反例，揭示在推理时错误地使用批次统计数据的危险。我们将看到，一个固定输入的预测结果会因为批次中包含的其他样本而发生改变，这凸显了使用全局运行统计数据的必要性 。",
            "id": "3101625",
            "problem": "构建一个程序，通过产生批次依赖的预测，来证明在推理时使用批归一化（BN）的批次统计数据是危险的。在一维空间中进行操作，因此每个输入都是一个标量。使用以下基本设定。\n\n对于一个批次 $\\mathcal{B} = \\{x_i\\}_{i=1}^m$，其批次均值为 $\\mu_{\\mathcal{B}} = \\frac{1}{m}\\sum_{i=1}^m x_i$，批次方差为 $\\sigma^2_{\\mathcal{B}} = \\frac{1}{m}\\sum_{i=1}^m (x_i - \\mu_{\\mathcal{B}})^2$，批归一化（BN）将样本 $x$ 变换为：\n$$\n\\operatorname{BN}_{\\mathcal{B}}(x) \\;=\\; \\gamma \\cdot \\frac{x - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma^2_{\\mathcal{B}} + \\epsilon}} \\;+\\; \\beta,\n$$\n其中 $\\gamma$ 和 $\\beta$ 是学习到的仿射参数，$\\epsilon$ 是一个小的正常数以确保数值稳定性。正确的推理方法是用训练期间累积的移动估计值 $\\mu_r$ 和 $\\sigma_r^2$ 来替代 $\\mu_{\\mathcal{B}}$ 和 $\\sigma^2_{\\mathcal{B}}$：\n$$\n\\operatorname{BN}_{\\mathrm{run}}(x) \\;=\\; \\gamma \\cdot \\frac{x - \\mu_r}{\\sqrt{\\sigma_r^2 + \\epsilon}} \\;+\\; \\beta.\n$$\n一个简单的分类器使用 BN 的输出 $z$，并通过以下阈值规则预测二元标签 $\\hat{y}$：\n$$\n\\hat{y} \\;=\\; \n\\begin{cases}\n1, & z \\ge 0,\\\\\n0, & z  0.\n\\end{cases}\n$$\n你必须为固定的目标输入 $x^\\star$ 实现这两种推理模式，并证明如果错误地在推理时使用每个批次的统计数据，那么 $x^\\star$ 的预测标签会根据批次中的其他样本而翻转。\n\n在所有计算中使用以下固定参数和定义：\n- 目标输入：$x^\\star = 0.2$。\n- BN 参数：$\\gamma = 1.0$，$\\beta = -0.1$，$\\epsilon = 10^{-5}$。\n- 移动统计数据（正确的推理统计数据）：$\\mu_r = 0.0$，$\\sigma_r^2 = 1.0$。\n- 分类规则如上所述。\n\n对于下面的每个批次 $\\mathcal{B}$，计算：\n1. 使用 $z_{\\mathrm{run}} = \\operatorname{BN}_{\\mathrm{run}}(x^\\star)$ 计算 $x^\\star$ 的正确推理预测。\n2. 使用 $z_{\\mathcal{B}} = \\operatorname{BN}_{\\mathcal{B}}(x^\\star)$ 计算 $x^\\star$ 的错误批次依赖预测，其中 $\\mu_{\\mathcal{B}}$ 和 $\\sigma^2_{\\mathcal{B}}$ 是根据批次中包括 $x^\\star$ 在内的所有元素计算的。\n3. 一个布尔值，指示错误的批次依赖预测是否与正确的推理预测不同。\n\n测试集（每个批次都是一个多重集，且始终包含 $x^\\star$）：\n- 理想情况，包含大致标准化的同批数据：$\\mathcal{B}_1 = [\\,x^\\star,\\,-1.0,\\,0.0,\\,1.0\\,]$。\n- 病态情况，均值因较大的正值同批数据而偏移：$\\mathcal{B}_2 = [\\,x^\\star,\\,10.0,\\,10.0,\\,10.0\\,]$。\n- 在低于 $x^\\star$ 的均值附近具有小方差：$\\mathcal{B}_3 = [\\,x^\\star,\\,0.0,\\,0.3,\\,-0.3\\,]$。\n- 在相同均值附近具有大方差：$\\mathcal{B}_4 = [\\,x^\\star,\\,0.0,\\,3.0,\\,-3.0\\,]$。\n- 批次大小为一的边界情况：$\\mathcal{B}_5 = [\\,x^\\star\\,]$。\n\n你的程序必须按 $\\mathcal{B}_1$ 到 $\\mathcal{B}_5$ 的顺序计算这五个布尔值，其中每个布尔值为 `true` 当且仅当对 $x^\\star$ 的错误批次依赖预测与使用移动统计数据的正确推理预测不同。\n\n最终输出格式：你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $[b_1,b_2,b_3,b_4,b_5]$，其中每个 $b_i$ 是 True 或 False。",
            "solution": "所述问题是有效的。它在科学上基于深度学习中批归一化的原理，问题定义明确，提供了所有必要的参数和定义，并且其表述是客观的。它要求对一个众所周知的概念进行计算演示，这是一个合适且可验证的任务。\n\n需要证明的核心原理是，批归一化（BN）在模型训练和推理期间必须被区别对待。在训练期间，BN 使用当前小批量数据的均值和方差对层输入进行归一化。这使得训练过程更加稳定。然而，在推理时，处理的是单个输入，而不是一个批次。在推理时，如果使用单个样本组成的“批次”或任何临时可用批次的统计数据，将导致模型对给定输入的预测依赖于同时处理的其他不相关输入。这违反了“训练好的模型应该是一个确定性函数”这一期望。正确的推理程序是使用固定的、总体级别的统计数据——这些数据是通过聚合训练期间所有小批量的统计数据（例如，通过指数移动平均）来近似得出的。\n\n我们将首先使用提供的移动统计数据，计算目标输入 $x^\\star$ 的正确的、确定性的预测。然后，对于每个测试批次，我们将计算错误的、依赖于批次的预测，并对两者进行比较。\n\n固定参数如下：\n- 目标输入：$x^\\star = 0.2$\n- BN 仿射参数：$\\gamma = 1.0$, $\\beta = -0.1$\n- 数值稳定性常数：$\\epsilon = 10^{-5}$\n- 用于正确推理的移动统计数据：$\\mu_r = 0.0$, $\\sigma_r^2 = 1.0$\n\n分类规则是：\n$$\n\\hat{y} = \\begin{cases} 1,  z \\ge 0 \\\\ 0,  z  0 \\end{cases}\n$$\n其中 $z$ 是 BN 层的输出。\n\n### 步骤 1：正确的推理预测\n\n正确的推理使用移动统计数据 $\\mu_r$ 和 $\\sigma_r^2$。对于 $x^\\star$ 的 BN 输出由下式给出：\n$$\nz_{\\mathrm{run}} = \\operatorname{BN}_{\\mathrm{run}}(x^\\star) = \\gamma \\cdot \\frac{x^\\star - \\mu_r}{\\sqrt{\\sigma_r^2 + \\epsilon}} + \\beta\n$$\n代入给定值：\n$$\nz_{\\mathrm{run}} = 1.0 \\cdot \\frac{0.2 - 0.0}{\\sqrt{1.0 + 10^{-5}}} - 0.1 \\approx \\frac{0.2}{1.000005} - 0.1 \\approx 0.199999 - 0.1 = 0.099999\n$$\n由于 $z_{\\mathrm{run}} = 0.099999 \\ge 0$，正确的预测是 $\\hat{y}_{\\mathrm{run}} = 1$。这是唯一的、恒定的基准，所有其他对 $x^\\star$ 的预测都将与之进行比较。\n\n### 步骤 2：错误的批次依赖推理\n\n现在我们为每个测试批次 $\\mathcal{B}_i$ 计算 $x^\\star$ 的预测，并错误地使用该批次的统计数据。BN 输出为：\n$$\nz_{\\mathcal{B}_i} = \\operatorname{BN}_{\\mathcal{B}_i}(x^\\star) = \\gamma \\cdot \\frac{x^\\star - \\mu_{\\mathcal{B}_i}}{\\sqrt{\\sigma^2_{\\mathcal{B}_i} + \\epsilon}} + \\beta\n$$\n其中 $\\mu_{\\mathcal{B}_i}$ 和 $\\sigma^2_{\\mathcal{B}_i}$ 是从每个批次 $\\mathcal{B}_i$ 的元素计算得出的。\n\n**情况 1：批次 $\\mathcal{B}_1 = [0.2, -1.0, 0.0, 1.0]$**\n- 批次均值：$\\mu_{\\mathcal{B}_1} = \\frac{1}{4}(0.2 - 1.0 + 0.0 + 1.0) = \\frac{0.2}{4} = 0.05$\n- 批次方差：$\\sigma^2_{\\mathcal{B}_1} = \\frac{1}{4}((0.2-0.05)^2 + (-1.0-0.05)^2 + (0.0-0.05)^2 + (1.0-0.05)^2) = \\frac{1}{4}(0.0225+1.1025+0.0025+0.9025) = \\frac{2.03}{4} = 0.5075$\n- BN 输出：$z_{\\mathcal{B}_1} = 1.0 \\cdot \\frac{0.2 - 0.05}{\\sqrt{0.5075 + 10^{-5}}} - 0.1 \\approx \\frac{0.15}{0.7124} - 0.1 \\approx 0.2105 - 0.1 = 0.1105$\n- 预测：$\\hat{y}_{\\mathcal{B}_1} = 1$ 因为 $z_{\\mathcal{B}_1} \\ge 0$。\n- 比较：$\\hat{y}_{\\mathcal{B}_1} = \\hat{y}_{\\mathrm{run}}$。布尔值为 **False**。\n\n**情况 2：批次 $\\mathcal{B}_2 = [0.2, 10.0, 10.0, 10.0]$**\n- 批次均值：$\\mu_{\\mathcal{B}_2} = \\frac{1}{4}(0.2 + 10.0 + 10.0 + 10.0) = \\frac{30.2}{4} = 7.55$\n- 批次方差：$\\sigma^2_{\\mathcal{B}_2} = \\frac{1}{4}((0.2-7.55)^2 + 3 \\cdot (10.0-7.55)^2) = \\frac{1}{4}(54.0225 + 3 \\cdot 6.0025) = \\frac{72.03}{4} = 18.0075$\n- BN 输出：$z_{\\mathcal{B}_2} = 1.0 \\cdot \\frac{0.2 - 7.55}{\\sqrt{18.0075 + 10^{-5}}} - 0.1 \\approx \\frac{-7.35}{4.2435} - 0.1 \\approx -1.732 - 0.1 = -1.832$\n- 预测：$\\hat{y}_{\\mathcal{B}_2} = 0$ 因为 $z_{\\mathcal{B}_2}  0$。\n- 比较：$\\hat{y}_{\\mathcal{B}_2} \\neq \\hat{y}_{\\mathrm{run}}$。布尔值为 **True**。这证明了批次中均值的大幅偏移如何极大地改变了 $x^\\star$ 的归一化值。\n\n**情况 3：批次 $\\mathcal{B}_3 = [0.2, 0.0, 0.3, -0.3]$**\n- 批次均值：$\\mu_{\\mathcal{B}_3} = \\frac{1}{4}(0.2 + 0.0 + 0.3 - 0.3) = \\frac{0.2}{4} = 0.05$\n- 批次方差：$\\sigma^2_{\\mathcal{B}_3} = \\frac{1}{4}((0.2-0.05)^2 + (0.0-0.05)^2 + (0.3-0.05)^2 + (-0.3-0.05)^2) = \\frac{1}{4}(0.0225+0.0025+0.0625+0.1225) = \\frac{0.21}{4} = 0.0525$\n- BN 输出：$z_{\\mathcal{B}_3} = 1.0 \\cdot \\frac{0.2 - 0.05}{\\sqrt{0.0525 + 10^{-5}}} - 0.1 \\approx \\frac{0.15}{0.22915} - 0.1 \\approx 0.6546 - 0.1 = 0.5546$\n- 预测：$\\hat{y}_{\\mathcal{B}_3} = 1$ 因为 $z_{\\mathcal{B}_3} \\ge 0$。\n- 比较：$\\hat{y}_{\\mathcal{B}_3} = \\hat{y}_{\\mathrm{run}}$。布尔值为 **False**。\n\n**情况 4：批次 $\\mathcal{B}_4 = [0.2, 0.0, 3.0, -3.0]$**\n- 批次均值：$\\mu_{\\mathcal{B}_4} = \\frac{1}{4}(0.2 + 0.0 + 3.0 - 3.0) = \\frac{0.2}{4} = 0.05$\n- 批次方差：$\\sigma^2_{\\mathcal{B}_4} = \\frac{1}{4}((0.2-0.05)^2 + (0.0-0.05)^2 + (3.0-0.05)^2 + (-3.0-0.05)^2) = \\frac{1}{4}(0.0225+0.0025+8.7025+9.3025) = \\frac{18.03}{4} = 4.5075$\n- BN 输出：$z_{\\mathcal{B}_4} = 1.0 \\cdot \\frac{0.2 - 0.05}{\\sqrt{4.5075 + 10^{-5}}} - 0.1 \\approx \\frac{0.15}{2.1231} - 0.1 \\approx 0.0706 - 0.1 = -0.0294$\n- 预测：$\\hat{y}_{\\mathcal{B}_4} = 0$ 因为 $z_{\\mathcal{B}_4}  0$。\n- 比较：$\\hat{y}_{\\mathcal{B}_4} \\neq \\hat{y}_{\\mathrm{run}}$。布尔值为 **True**。在这里，批次均值与情况 1 和 3 相同，但是大的方差减小了归一化值的幅度，在经过 $\\beta$ 偏移后将其推至分类阈值以下。\n\n**情况 5：批次 $\\mathcal{B}_5 = [0.2]$**\n- 批次均值：$\\mu_{\\mathcal{B}_5} = \\frac{1}{1}(0.2) = 0.2$\n- 批次方差：$\\sigma^2_{\\mathcal{B}_5} = \\frac{1}{1}((0.2 - 0.2)^2) = 0.0$\n- BN 输出：$z_{\\mathcal{B}_5} = 1.0 \\cdot \\frac{0.2 - 0.2}{\\sqrt{0.0 + 10^{-5}}} - 0.1 = \\frac{0.0}{\\sqrt{10^{-5}}} - 0.1 = 0.0 - 0.1 = -0.1$\n- 预测：$\\hat{y}_{\\mathcal{B}_5} = 0$ 因为 $z_{\\mathcal{B}_5}  0$。\n- 比较：$\\hat{y}_{\\mathcal{B}_5} \\neq \\hat{y}_{\\mathrm{run}}$。布尔值为 **True**。这是一个极端情况，分子 $x^\\star - \\mu_{\\mathcal{B}}$ 变为零。输出就是 $\\beta$，这突显了 $\\epsilon$ 在防止除以零方面的关键作用，但仍然导致了预测的翻转。\n\n这些计算清楚地表明，在推理时使用批次统计数据会使对固定输入 $x^\\star$ 的预测变得不稳定，并依赖于批次中其他输入的上下文，从而导致错误的结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Demonstrates the danger of using batch statistics for Batch Normalization\n    at inference time by showing that predictions can become batch-dependent.\n    \"\"\"\n    # Define fixed parameters and the target input.\n    x_star = 0.2\n    gamma = 1.0\n    beta = -0.1\n    epsilon = 1e-5\n\n    # Define running statistics for correct inference.\n    mu_r = 0.0\n    sigma_r_sq = 1.0\n\n    # Define the test cases (batches).\n    # Each batch is a list of floats and always contains x_star.\n    test_cases = [\n        # Happy path with roughly standardized peers\n        [x_star, -1.0, 0.0, 1.0],\n        # Pathological mean shift with large positive peers\n        [x_star, 10.0, 10.0, 10.0],\n        # Small variance around a mean below x_star\n        [x_star, 0.0, 0.3, -0.3],\n        # Large variance around the same mean\n        [x_star, 0.0, 3.0, -3.0],\n        # Boundary case of batch size one\n        [x_star],\n    ]\n\n    # --- Step 1: Compute the correct-inference prediction ---\n    # This prediction is deterministic and serves as the ground truth.\n    z_run = gamma * (x_star - mu_r) / np.sqrt(sigma_r_sq + epsilon) + beta\n    y_hat_run = 1 if z_run = 0 else 0\n\n    results = []\n    # --- Step 2: Compute incorrect predictions for each batch ---\n    for batch_data in test_cases:\n        batch = np.array(batch_data)\n        \n        # Calculate batch-specific statistics.\n        # The variance is calculated using the population formula (1/m),\n        # as is standard in BN implementations.\n        mu_b = np.mean(batch)\n        sigma_b_sq = np.var(batch)\n\n        # Calculate the batch-dependent BN output for x_star.\n        z_b = gamma * (x_star - mu_b) / np.sqrt(sigma_b_sq + epsilon) + beta\n\n        # Determine the batch-dependent prediction.\n        y_hat_b = 1 if z_b = 0 else 0\n\n        # Compare the incorrect prediction to the correct one.\n        # The result is True if the predictions differ.\n        prediction_differs = (y_hat_b != y_hat_run)\n        results.append(prediction_differs)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在理解了推理时不应使用批次统计数据的危险之后 ()，我们自然会问：如何正确地实现训练和评估两种模式？本练习将指导你从零开始构建一个包含批归一化层的简单模型。你将亲手实现统计数据的计算、运行统计数据的更新，并验证在“冻结”统计数据后，模型的评估模式与训练模式的输出在数值上是等价的 。",
            "id": "3101661",
            "problem": "您的任务是实现一个最小化的前馈模型，该模型包含一个仿射层，后接一个批量归一化 (BN) 层，并演示从训练模式到评估模式的转换。该模型必须纯粹使用数值编程环境中的数组和操作来实现，并且不得依赖外部的深度学习框架。该演示必须在指定容差界限内，验证冻结统计数据下的数值等价性。\n\n从以下基本定义开始，仅使用广泛接受的定义：\n\n- 给定一个小批量 (mini-batch) 的特征向量 $x \\in \\mathbb{R}^{B \\times d}$，沿批次维度定义经验性的小批量均值和方差为\n$$\n\\mu_{\\text{batch}} = \\frac{1}{B} \\sum_{i=1}^{B} x_i, \\quad\n\\sigma_{\\text{batch}}^2 = \\frac{1}{B} \\sum_{i=1}^{B} (x_i - \\mu_{\\text{batch}})^2,\n$$\n两者均为每个特征维度逐元素计算。添加一个小的正常数 $\\epsilon  0$ 以稳定方差的倒数计算。\n- 使用动量 $\\rho \\in [0,1]$ 通过指数移动平均来维护运行统计量：\n$$\n\\mu_{\\text{run}} \\leftarrow (1 - \\rho)\\, \\mu_{\\text{run}} + \\rho\\, \\mu_{\\text{batch}}, \\quad\n\\sigma_{\\text{run}}^2 \\leftarrow (1 - \\rho)\\, \\sigma_{\\text{run}}^2 + \\rho\\, \\sigma_{\\text{batch}}^2.\n$$\n将 $\\mu_{\\text{run}}$ 初始化为零向量，$\\sigma_{\\text{run}}^2$ 初始化为适当维度的一向量。\n- 设仿射变换由权重 $W \\in \\mathbb{R}^{d_{\\text{in}} \\times d_{\\text{out}}}$ 和偏置 $b \\in \\mathbb{R}^{d_{\\text{out}}}$ 定义为\n$$\ny = x W + b,\n$$\n批量归一化 (Batch Normalization) 逐特征地应用于 $y$，带有可学习的缩放和平移参数 $\\gamma, \\beta \\in \\mathbb{R}^{d_{\\text{out}}}$：\n$$\n\\hat{y} = \\frac{y - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}, \\quad\n\\text{BN}(y; \\gamma, \\beta, \\mu, \\sigma^2, \\epsilon) = \\gamma \\odot \\hat{y} + \\beta,\n$$\n其中 $\\odot$ 表示逐元素乘法。\n\n目标如下：\n\n1. 实现训练模式的 BN，它计算当前小批量的 $\\mu_{\\text{batch}}$ 和 $\\sigma_{\\text{batch}}^2$，使用动量 $\\rho$ 更新运行统计量 $(\\mu_{\\text{run}}, \\sigma_{\\text{run}}^2)$，并使用当前小批量的统计数据输出归一化后的激活值。\n2. 实现评估模式的 BN，它使用冻结的运行统计量 $(\\mu_{\\text{run}}, \\sigma_{\\text{run}}^2)$ 而不更新它们，并使用这些冻结的统计数据输出归一化后的激活值。\n3. 实现一个“冻结统计量的训练前向传播”，它不更新 $(\\mu_{\\text{run}}, \\sigma_{\\text{run}}^2)$，并使用 $(\\mu_{\\text{run}}, \\sigma_{\\text{run}}^2)$ 进行归一化，即使全局模型模式为训练模式。这是“冻结统计量”的操作性定义。\n4. 在通过处理一系列小批量数据以更新 $(\\mu_{\\text{run}}, \\sigma_{\\text{run}}^2)$ 来模拟训练后，将模型转换为评估模式，并通过检查评估模式 BN 和冻结统计量的训练前向传播在相同输入下的输出之间的最大绝对差值小于或等于给定的容差界限 $\\tau$，来验证它们的数值等价性。\n\n您的程序必须实现上述功能，并评估以下测试套件。对于每种情况，请完全按照规定构建模型参数和数据。所有随机数必须使用指定的种子确定性地生成。方差计算使用无偏自由度 $0$（即除数为 $B$）。本说明中的每个数值都是无量纲的。\n\n对于每个测试用例，定义：\n- $d_{\\text{in}}$：输入特征维度。\n- $d_{\\text{out}}$：输出特征维度。\n- $B$：小批量大小。\n- $T$：要处理的训练小批量数量。\n- $\\rho$：运行统计量的动量。\n- $\\epsilon$：添加到方差的稳定器。\n- 种子：$s_{\\text{model}}$ 用于初始化 $W$、$b$、$\\gamma$、$\\beta$；$s_{\\text{train}}$ 用于生成训练小批量；$s_{\\text{test}}$ 用于生成测试小批量。\n- 训练数据生成规则和测试数据生成规则：\n  - 如果指定了一个常数值 $c$，则训练小批量中的所有条目均等于 $c$。\n  - 否则，对于正态分布，从具有指定均值 $\\mu$ 和标准差 $\\sigma$ 的 $\\mathcal{N}(\\mu, \\sigma^2)$ 中独立抽取条目。\n\n每种情况的验证标准：\n- 在模拟训练 $T$ 个小批量后，在同一个测试小批量上计算两次模型输出：一次在评估模式下，另一次在统计量冻结的训练模式下。设 $\\Delta$ 为这两个输出之间的最大绝对差值。如果 $\\Delta \\le \\tau$，则结果为 $ \\text{True} $，否则为 $ \\text{False} $。\n\n使用以下测试套件，每个套件的容差界限 $\\tau = 10^{-12}$：\n\n- 情况 $1$ (通用“顺利”路径):\n  - $d_{\\text{in}} = 8$, $d_{\\text{out}} = 8$, $B = 32$, $T = 50$, $\\rho = 0.1$, $\\epsilon = 10^{-5}$,\n  - 种子: $s_{\\text{model}} = 0$, $s_{\\text{train}} = 1$, $s_{\\text{test}} = 2$,\n  - 训练数据: $\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$,\n  - 测试数据: $\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$。\n\n- 情况 $2$ (批量大小为 $1$ 且训练批次方差为零的边界情况):\n  - $d_{\\text{in}} = 4$, $d_{\\text{out}} = 4$, $B = 1$, $T = 3$, $\\rho = 0.5$, $\\epsilon = 10^{-3}$,\n  - 种子: $s_{\\text{model}} = 10$, $s_{\\text{train}} = 11$, $s_{\\text{test}} = 12$,\n  - 训练数据: 常数 $c = 3.3$,\n  - 测试数据: $\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$。\n\n- 情况 $3$ (大 $\\epsilon$ 和可能为负的缩放参数):\n  - $d_{\\text{in}} = 16$, $d_{\\text{out}} = 16$, $B = 64$, $T = 10$, $\\rho = 0.9$, $\\epsilon = 10^{-1}$,\n  - 种子: $s_{\\text{model}} = 21$, $s_{\\text{train}} = 22$, $s_{\\text{test}} = 23$,\n  - 训练数据: $\\mathcal{N}(\\mu = 1.5, \\sigma = 2.0)$,\n  - 测试数据: $\\mathcal{N}(\\mu = -0.5, \\sigma = 1.0)$。\n\n- 情况 $4$ (动量 $\\rho = 0$，无运行统计量更新):\n  - $d_{\\text{in}} = 5$, $d_{\\text{out}} = 5$, $B = 20$, $T = 25$, $\\rho = 0.0$, $\\epsilon = 10^{-6}$,\n  - 种子: $s_{\\text{model}} = 31$, $s_{\\text{train}} = 32$, $s_{\\text{test}} = 33$,\n  - 训练数据: $\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$,\n  - 测试数据: $\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$。\n\n- 情况 $5$ (动量 $\\rho = 1$，运行统计量等于最后一个批次的统计量):\n  - $d_{\\text{in}} = 7$, $d_{\\text{out}} = 7$, $B = 10$, $T = 7$, $\\rho = 1.0$, $\\epsilon = 10^{-6}$,\n  - 种子: $s_{\\text{model}} = 41$, $s_{\\text{train}} = 42$, $s_{\\text{test}} = 43$,\n  - 训练数据: $\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$,\n  - 测试数据: $\\mathcal{N}(\\mu = 0.0, \\sigma = 1.0)$。\n\n模型参数的构建：\n- 对于每种情况，使用种子 $s_{\\text{model}}$ 通过抽取独立的标准正态分布条目来初始化仿射参数 $W$ 和 $b$，以及 BN 参数 $\\gamma$ 和 $\\beta$。\n\n您的程序应生成单行输出，其中包含五个测试用例的结果，格式为方括号括起来的逗号分隔列表（例如，“[$\\text{True}$,$\\text{False}$,$\\text{True}$,$\\text{True}$,$\\text{True}$]”）。条目必须是按照上述情况的确切顺序，由验证标准计算出的布尔值。本问题不涉及物理单位或角度单位；仅报告布尔值。",
            "solution": "问题陈述已经过分析，被认为是有效的。它构成了一个定义明确、自成体系且可通过计算验证的任务，该任务在科学上植根于深度学习的原理。仿射层、批量归一化 (BN) 的定义以及运行统计量的更新规则都是标准的，并且在数学上是合理的。目标是实现这些组件，并验证 BN 层两种操作模式之间的数值等价性，这是对实现正确性的测试。\n\n解决方案是根据问题中阐述的架构和程序规范构建的。\n\n**模型架构**\n\n模型是一个由两个层组成的前馈序列：一个仿射层和一个批量归一化层。\n\n$1$。**仿射层**：该层执行线性变换。对于输入的小批量 $x \\in \\mathbb{R}^{B \\times d_{\\text{in}}}$，其中 $B$ 是批量大小，$d_{\\text{in}}$ 是输入特征维度，该层计算输出 $y \\in \\mathbb{R}^{B \\times d_{\\text{out}}}$。这由一个权重矩阵 $W \\in \\mathbb{R}^{d_{\\text{in}} \\times d_{\\text{out}}}$ 和一个偏置向量 $b \\in \\mathbb{R}^{d_{\\text{out}}}$ 控制。变换如下：\n$$y = x W + b$$\n偏置 $b$ 会被广播，以便加到矩阵乘积 $xW$ 的每一行。\n\n$2$。**批量归一化 (BN) 层**：该层逐个特征地对其输入 $y$ 进行归一化。它在两种不同的模式下运行：训练和评估。\n\n**训练模式下的 BN**\n\n在训练阶段，BN 层使用从当前小批量计算出的统计量对其输入进行归一化。它同时更新一组近似全局数据集统计量的运行统计量。\n\n- **步骤 $1$：小批量统计量计算**。对于输入 $y \\in \\mathbb{R}^{B \\times d_{\\text{out}}}$，跨批次维度（大小为 $B$）逐元素计算均值 $\\mu_{\\text{batch}} \\in \\mathbb{R}^{d_{\\text{out}}}$ 和方差 $\\sigma_{\\text{batch}}^2 \\in \\mathbb{R}^{d_{\\text{out}}}$：\n$$\n\\mu_{\\text{batch}} = \\frac{1}{B} \\sum_{i=1}^{B} y_i\n$$\n$$\n\\sigma_{\\text{batch}}^2 = \\frac{1}{B} \\sum_{i=1}^{B} (y_i - \\mu_{\\text{batch}})^2\n$$\n问题明确指出，方差计算使用无偏自由度 $0$，这对应于除以 $B$。\n\n- **步骤 $2$：归一化**。输入 $y$ 使用这些批次特定的统计量进行归一化。一个小的正常数 $\\epsilon  0$ 被合并到分母中以确保数值稳定性：\n$$\n\\hat{y} = \\frac{y - \\mu_{\\text{batch}}}{\\sqrt{\\sigma_{\\text{batch}}^2 + \\epsilon}}\n$$\n在此逐元素操作中，$\\mu_{\\text{batch}}$ 和分母项都会跨批次维度进行广播。\n\n- **步骤 $3$：缩放和平移变换**。归一化后的激活值 $\\hat{y}$ 随后会经过一个由缩放参数 $\\gamma \\in \\mathbb{R}^{d_{\\text{out}}}$ 和平移参数 $\\beta \\in \\mathbb{R}^{d_{\\text{out}}}$ 定义的可学习仿射变换：\n$$\n\\text{output} = \\gamma \\odot \\hat{y} + \\beta\n$$\n其中 $\\odot$ 表示逐元素乘法。\n\n- **步骤 $4$：运行统计量更新**。该层维护均值 ($\\mu_{\\text{run}}$) 和方差 ($\\sigma_{\\text{run}}^2$) 的长期运行估计。这些估计通过由动量参数 $\\rho \\in [0, 1]$ 控制的指数移动平均进行更新：\n$$\n\\mu_{\\text{run}} \\leftarrow (1 - \\rho)\\mu_{\\text{run}} + \\rho\\mu_{\\text{batch}}\n$$\n$$\n\\sigma_{\\text{run}}^2 \\leftarrow (1 - \\rho)\\sigma_{\\text{run}}^2 + \\rho\\sigma_{\\text{batch}}^2\n$$\n按照规定，$\\mu_{\\text{run}}$ 初始化为零向量，$\\sigma_{\\text{run}}^2$ 初始化为一向量。\n\n**评估模式下的 BN**\n\n在评估或推理期间，使用单个小批量的统计数据可能不理想或不可能（例如，如果 $B=1$）。因此，转而使用在训练过程中累积的“冻结”运行统计量 $\\mu_{\\text{run}}$ 和 $\\sigma_{\\text{run}}^2$。\n\n- **步骤 $1$：使用运行统计量进行归一化**。输入 $y$ 使用固定的 $\\mu_{\\text{run}}$ 和 $\\sigma_{\\text{run}}^2$ 进行归一化：\n$$\n\\hat{y} = \\frac{y - \\mu_{\\text{run}}}{\\sqrt{\\sigma_{\\text{run}}^2 + \\epsilon}}\n$$\n\n- **步骤 $2$：缩放和平移变换**。此步骤与训练模式中的相同：\n$$\n\\text{output} = \\gamma \\odot \\hat{y} + \\beta\n$$\n\n关键的是，在评估模式期间，运行统计量 $\\mu_{\\text{run}}$ 和 $\\sigma_{\\text{run}}^2$ 不会被修改。\n\n**验证程序**\n\n核心任务是确认 BN 层的正确实现，在模拟训练阶段后，对于两个概念上等效的场景能产生数值上相同的输出。\n\n$1$。**训练模拟**。对于五个测试用例中的每一个，过程始于通过从标准正态分布中抽样来初始化模型参数 $W, b, \\gamma, \\beta$，使用指定的种子 $s_{\\text{model}}$。运行统计量 $\\mu_{\\text{run}}$ 和 $\\sigma_{\\text{run}}^2$ 分别初始化为 $\\vec{0}$ 和 $\\vec{1}$。然后将模型设置为其训练配置。一个模拟循环执行 $T$ 步。在每一步中，使用以 $s_{\\text{train}}$ 为种子的随机数生成器生成一个训练小批量。执行一次前向传播，该传播使用小批量统计量进行归一化并更新运行统计量。\n\n$2$。**等价性测试**。训练模拟完成后，累积的运行统计量被视为最终或“冻結”状态。使用种子 $s_{\\text{test}}$ 生成一个新的测试小批量。然后按两种规定的方式计算模型对此测试批次的输出：\n    - **场景 A (评估模式)**：模型被切换到其评估配置（例如，通过标志 `training=False`）。对测试数据执行一次前向传播。在此模式下，该层必须使用冻结的 $\\mu_{\\text{run}}$ 和 $\\sigma_{\\text{run}}^2$ 进行归一化。\n    - **场景 B (冻结统计量的训练前向传播)**：模型保持在其名义上的训练配置（例如，`training=True`），但通过一个特殊标志（例如，`freeze_stats=True`）被指示执行一次前向传播，该传播使用冻结的运行统计量进行归一化，从而绕过批次统计量的计算和运行统计量的更新。\n\n根据它们的定义，这两种场景描述的是相同的计算过程。该测试旨在验证实现是否正确反映了这种同一性。\n\n$3$。**数值验证**。计算场景 A 和场景 B 输出矩阵之间的最大逐元素绝对差值 $\\Delta$。如果此差值小于或等于指定的容差 $\\Delta \\le \\tau$（其中 $\\tau = 10^{-12}$），则该测试用例的结果为 `True`。此布尔值结果被记录下来，用于五个测试用例中的每一个，从而得出最终的输出列表。",
            "answer": "```python\nimport numpy as np\n\nclass Affine:\n    \"\"\"Implements a single affine (fully connected) layer.\"\"\"\n    def __init__(self, W, b):\n        self.W = W\n        self.b = b\n\n    def forward(self, x):\n        \"\"\"Performs the forward pass: y = xW + b.\"\"\"\n        return x @ self.W + self.b\n\nclass BatchNorm1d:\n    \"\"\"Implements a Batch Normalization layer for 1D features.\"\"\"\n    def __init__(self, d_out, gamma, beta, eps, momentum):\n        self.gamma = gamma  # scale\n        self.beta = beta    # shift\n        self.eps = eps\n        self.momentum = momentum\n        \n        self.running_mean = np.zeros(d_out)\n        self.running_var = np.ones(d_out)\n        \n        self.training = True\n\n    def forward(self, y, freeze_stats=False):\n        \"\"\"Performs the forward pass for Batch Normalization.\"\"\"\n        if not self.training or freeze_stats:\n            # Evaluation mode or frozen-stats training mode: use running stats\n            mean_to_use = self.running_mean\n            var_to_use = self.running_var\n        else:\n            # Standard training mode: use batch stats and update running stats\n            if y.shape[0]  1:\n                batch_mean = np.mean(y, axis=0)\n                batch_var = np.var(y, axis=0, ddof=0)\n            else: # B=1 case\n                batch_mean = y[0]\n                batch_var = np.zeros_like(y[0])\n\n            mean_to_use = batch_mean\n            var_to_use = batch_var\n\n            # Update running statistics\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n            \n        # Normalize\n        y_hat = (y - mean_to_use) / np.sqrt(var_to_use + self.eps)\n        \n        # Scale and shift\n        out = self.gamma * y_hat + self.beta\n        return out\n\n    def train(self):\n        self.training = True\n\n    def eval(self):\n        self.training = False\n\nclass Model:\n    \"\"\"Combines an Affine layer and a BatchNorm1d layer.\"\"\"\n    def __init__(self, affine_layer, bn_layer):\n        self.affine = affine_layer\n        self.bn = bn_layer\n\n    def forward(self, x, freeze_stats=False):\n        y = self.affine.forward(x)\n        return self.bn.forward(y, freeze_stats=freeze_stats)\n\n    def train(self):\n        self.bn.train()\n\n    def eval(self):\n        self.bn.eval()\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        # Case 1 (general \"happy path\")\n        {\n            \"d_in\": 8, \"d_out\": 8, \"B\": 32, \"T\": 50, \"rho\": 0.1, \"epsilon\": 1e-5,\n            \"s_model\": 0, \"s_train\": 1, \"s_test\": 2,\n            \"train_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n            \"test_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n        },\n        # Case 2 (boundary with batch size 1 and zero-variance training batches)\n        {\n            \"d_in\": 4, \"d_out\": 4, \"B\": 1, \"T\": 3, \"rho\": 0.5, \"epsilon\": 1e-3,\n            \"s_model\": 10, \"s_train\": 11, \"s_test\": 12,\n            \"train_gen\": lambda rng, B, d: np.full((B, d), 3.3),\n            \"test_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n        },\n        # Case 3 (large epsilon and potentially negative scale parameters)\n        {\n            \"d_in\": 16, \"d_out\": 16, \"B\": 64, \"T\": 10, \"rho\": 0.9, \"epsilon\": 1e-1,\n            \"s_model\": 21, \"s_train\": 22, \"s_test\": 23,\n            \"train_gen\": lambda rng, B, d: rng.normal(loc=1.5, scale=2.0, size=(B, d)),\n            \"test_gen\": lambda rng, B, d: rng.normal(loc=-0.5, scale=1.0, size=(B, d)),\n        },\n        # Case 4 (momentum rho = 0, no running-statistics update)\n        {\n            \"d_in\": 5, \"d_out\": 5, \"B\": 20, \"T\": 25, \"rho\": 0.0, \"epsilon\": 1e-6,\n            \"s_model\": 31, \"s_train\": 32, \"s_test\": 33,\n            \"train_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n            \"test_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n        },\n        # Case 5 (momentum rho = 1, running statistics equal last batch statistics)\n        {\n            \"d_in\": 7, \"d_out\": 7, \"B\": 10, \"T\": 7, \"rho\": 1.0, \"epsilon\": 1e-6,\n            \"s_model\": 41, \"s_train\": 42, \"s_test\": 43,\n            \"train_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n            \"test_gen\": lambda rng, B, d: rng.normal(loc=0.0, scale=1.0, size=(B, d)),\n        },\n    ]\n\n    tau = 1e-12\n    results = []\n\n    for case in test_cases:\n        d_in, d_out, B, T = case[\"d_in\"], case[\"d_out\"], case[\"B\"], case[\"T\"]\n        \n        # 1. Initialize Model\n        rng_model = np.random.default_rng(case[\"s_model\"])\n        W = rng_model.standard_normal(size=(d_in, d_out))\n        b = rng_model.standard_normal(size=(d_out,))\n        gamma = rng_model.standard_normal(size=(d_out,))\n        beta = rng_model.standard_normal(size=(d_out,))\n        \n        affine_layer = Affine(W, b)\n        bn_layer = BatchNorm1d(d_out, gamma, beta, case[\"epsilon\"], case[\"rho\"])\n        model = Model(affine_layer, bn_layer)\n        \n        # 2. Simulate Training\n        model.train()\n        rng_train = np.random.default_rng(case[\"s_train\"])\n        for _ in range(T):\n            train_batch = case[\"train_gen\"](rng_train, B, d_in)\n            model.forward(train_batch) # Forward pass updates running stats\n\n        # 3. Verification Step\n        rng_test = np.random.default_rng(case[\"s_test\"])\n        test_batch = case[\"test_gen\"](rng_test, B, d_in)\n        \n        # Scenario A: Evaluation mode\n        model.eval()\n        output_eval = model.forward(test_batch)\n\n        # Scenario B: Frozen-stats training forward\n        model.train() # Set global mode to training\n        output_frozen = model.forward(test_batch, freeze_stats=True)\n\n        # 4. Compare and record result\n        max_abs_diff = np.max(np.abs(output_eval - output_frozen))\n        results.append(max_abs_diff = tau)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "掌握了批归一化的正确实现方法 () 后，我们可以深入探索其在特定条件下的行为。本练习将聚焦于一个关键的边缘情况：当批次大小为1时。通过直接的数学推导，你将发现为何在这种情况下，批归一化会失效，导致梯度无法反向传播，从而揭示该算法的一个重要局限性 。",
            "id": "3101723",
            "problem": "考虑一个处于训练模式的单层标量网络，其包含批量归一化 (BN)，其中小批量大小为 $m=1$。设归一化前的标量激活值为 $x \\in \\mathbb{R}$。批量归一化 (BN) 的过程是：首先使用经验批量均值和方差对激活值进行标准化，然后应用一个仿射变换。对于一个大小为 $m$ 的小批量 $\\mathcal{B}$，其经验均值定义为\n$$\n\\mu_{\\mathcal{B}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i,\n$$\n其经验方差定义为\n$$\n\\sigma_{\\mathcal{B}}^{2} = \\frac{1}{m} \\sum_{i=1}^{m} \\left(x_i - \\mu_{\\mathcal{B}}\\right)^{2}.\n$$\n标准化过程是用激活值除以经验方差加上一个正的稳定常数 $\\epsilon  0$ 的平方根。标准化之后，BN 应用一个带有可学习参数 $\\gamma \\in \\mathbb{R}$ 和 $\\beta \\in \\mathbb{R}$ 的仿射映射。设标量目标为 $t \\in \\mathbb{R}$，并考虑平方误差损失\n$$\nL = \\left(y - t\\right)^{2},\n$$\n其中 $y$ 是经过 BN 后的标量输出。\n\n从经验均值和方差的定义出发，仅使用上述的标准化和仿射变换步骤，推导在训练期间当 $m=1$ 时梯度 $\\frac{dL}{dx}$ 的显式封闭形式表达式。你的最终答案必须是一个实数或一个解析表达式。不需要进行四舍五入。",
            "solution": "用户想要找到对于小批量大小为 $m=1$ 的批量归一化 (BN) 层，损失函数 $L$ 关于输入 $x$ 的梯度 $\\frac{dL}{dx}$。损失函数由 $L = (y - t)^2$ 给出，其中 $y$ 是 BN 层的输出。\n\n首先，我们将根据问题陈述中定义的批量归一化步骤，针对小批量大小为 $m=1$ 的特定情况，将输出 $y$ 表示为输入 $x$ 的函数。\n\n该小批量由单个标量激活值组成，我们将其表示为 $\\mathcal{B} = \\{x\\}$。\n\n1.  **计算经验批量均值 $\\mu_{\\mathcal{B}}$：**\n    均值的公式为 $\\mu_{\\mathcal{B}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i$。\n    对于 $m=1$ 和 $\\mathcal{B} = \\{x\\}$，该公式变为：\n    $$\n    \\mu_{\\mathcal{B}} = \\frac{1}{1} \\sum_{i=1}^{1} x_i = x\n    $$\n\n2.  **计算经验批量方差 $\\sigma_{\\mathcal{B}}^2$：**\n    方差的公式为 $\\sigma_{\\mathcal{B}}^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_{\\mathcal{B}})^2$。\n    代入 $m=1$，$\\mathcal{B} = \\{x\\}$ 和 $\\mu_{\\mathcal{B}} = x$，我们得到：\n    $$\n    \\sigma_{\\mathcal{B}}^2 = \\frac{1}{1} (x - \\mu_{\\mathcal{B}})^2 = (x - x)^2 = 0\n    $$\n\n3.  **执行标准化：**\n    标准化后的激活值，我们称之为 $\\hat{x}$，计算如下：\n    $$\n    \\hat{x} = \\frac{x - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^2 + \\epsilon}}\n    $$\n    其中 $\\epsilon  0$ 是一个小的正常数，以确保数值稳定性。\n    代入我们刚刚找到的 $\\mu_{\\mathcal{B}}$ 和 $\\sigma_{\\mathcal{B}}^2$ 的值：\n    $$\n    \\hat{x} = \\frac{x - x}{\\sqrt{0 + \\epsilon}} = \\frac{0}{\\sqrt{\\epsilon}} = 0\n    $$\n    对于批量大小为 $m=1$ 的情况，标准化步骤的结果恰好为 $0$，与输入值 $x$ 无关。条件 $\\epsilon  0$ 在这里至关重要，因为它防止了除以零。\n\n4.  **应用仿射变换：**\n    BN 层的最终输出 $y$ 是通过使用可学习参数 $\\gamma$ 和 $\\beta$ 对标准化后的激活值 $\\hat{x}$ 进行缩放和移位得到的：\n    $$\n    y = \\gamma \\hat{x} + \\beta\n    $$\n    代入 $\\hat{x}=0$ 的值：\n    $$\n    y = \\gamma(0) + \\beta = \\beta\n    $$\n    这个结果表明，对于小批量大小为 $m=1$ 的情况，批量归一化层的输出 $y$ 就等于可学习的偏置参数 $\\beta$。输出 $y$ 对输入激活值 $x$ 没有依赖关系。\n\n现在，我们可以计算梯度 $\\frac{dL}{dx}$。损失函数为 $L = (y-t)^2$。我们可以使用链式法则：\n$$\n\\frac{dL}{dx} = \\frac{dL}{dy} \\frac{dy}{dx}\n$$\n第一项是损失函数关于 BN 层输出的导数：\n$$\n\\frac{dL}{dy} = \\frac{d}{dy}(y-t)^2 = 2(y-t)\n$$\n代入 $y=\\beta$，得到 $\\frac{dL}{dy} = 2(\\beta-t)$。\n\n第二项是 BN 层输出关于其输入的导数：\n$$\n\\frac{dy}{dx}\n$$\n正如我们所推导的，对于 $m=1$，$y$ 与输入 $x$ 的函数关系为 $y(x) = \\beta$。参数 $\\beta$ 是模型的一个可学习参数，但相对于该层的输入 $x$ 而言，它被视为一个常数。因此，它关于 $x$ 的导数为零：\n$$\n\\frac{dy}{dx} = \\frac{d}{dx}(\\beta) = 0\n$$\n最后，我们可以计算完整梯度：\n$$\n\\frac{dL}{dx} = \\frac{dL}{dy} \\frac{dy}{dx} = 2(\\beta-t) \\cdot 0 = 0\n$$\n损失函数关于输入激活值 $x$ 的梯度为零。这意味着对于大小为 1 的小批量，没有梯度信号会通过 BN 层反向传播到其前面的层。对于关于该输入的梯度计算而言，归一化过程实际上使输入信息无效了。",
            "answer": "$$\n\\boxed{0}\n$$"
        }
    ]
}