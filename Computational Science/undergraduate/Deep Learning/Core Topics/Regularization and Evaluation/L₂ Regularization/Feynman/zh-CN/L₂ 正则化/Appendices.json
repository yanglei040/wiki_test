{
    "hands_on_practices": [
        {
            "introduction": "我们首先深入 $L_2$ 正则化的数学核心。通过为一个带有 $L_2$ 惩罚项的线性模型推导其闭式解，你将清晰地看到它与统计学中的经典方法——岭回归（Ridge Regression）——在数学上是完全等价的。这项练习将神经网络中的权重衰减概念与一个成熟的统计模型联系起来，为你对正则化的理解构建坚实的理论基石。",
            "id": "3169526",
            "problem": "考虑一个单层线性神经网络，其参数向量为 $w \\in \\mathbb{R}^{d}$，它通过 $f_{w}(x) = w^{\\top} x$ 将输入 $x \\in \\mathbb{R}^{d}$ 映射到输出。给定训练数据 $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$，它们被堆叠成一个设计矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 和一个响应向量 $y \\in \\mathbb{R}^{n}$。训练目标是带有权重衰减惩罚（欧几里得范数 (L2) 正则化）的平方误差和，定义为\n$$\nJ(w) = \\|y - X w\\|_{2}^{2} + \\lambda \\|w\\|_{2}^{2},\n$$\n其中 $\\lambda > 0$ 是正则化强度。\n\n仅从 $J(w)$ 的定义以及微分和矩阵代数的基本规则出发，推导最小化 $J(w)$ 的闭式解优化器 $w^{\\star}$，并解释为什么该优化器建立了岭回归与训练带权重衰减的单层线性神经网络之间的等价性。\n\n然后，对于给定的具体数据集\n$$\nX = \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n1  1\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{pmatrix},\n$$\n以及正则化参数 $\\lambda = 1$，计算优化后的权重向量 $w^{\\star}$。请将您的最终答案表示为一个具有精确有理数分量（无四舍五入）的行矩阵。",
            "solution": "该问题要求完成两个主要任务：首先，推导使用 L2 正则化（权重衰减）训练的单层线性神经网络权重的闭式解，并解释其与岭回归的等价性；其次，为给定数据集计算具体的权重向量。\n\n该问题是有效的，因为它在科学上基于标准的机器学习理论，问题表述清晰，信息充分且一致，并且措辞客观。\n\n首先，我们推导最优权重向量 $w^{\\star}$。需要最小化的目标函数由下式给出：\n$$\nJ(w) = \\|y - X w\\|_{2}^{2} + \\lambda \\|w\\|_{2}^{2}\n$$\n其中 $w \\in \\mathbb{R}^{d}$，$X \\in \\mathbb{R}^{n \\times d}$，$y \\in \\mathbb{R}^{n}$，且 $\\lambda > 0$。\n\n为了找到 $J(w)$ 的最小值，我们必须首先计算它关于 $w$ 的梯度，记为 $\\nabla_{w} J(w)$，并将其设为零。让我们展开 $J(w)$ 中的各项。平方欧几里得范数 $\\|v\\|_{2}^{2}$ 等价于点积 $v^{\\top}v$。\n\n第一项是平方误差和：\n$$\n\\|y - X w\\|_{2}^{2} = (y - X w)^{\\top} (y - X w) = (y^{\\top} - w^{\\top} X^{\\top})(y - X w) = y^{\\top}y - y^{\\top}Xw - w^{\\top}X^{\\top}y + w^{\\top}X^{\\top}Xw\n$$\n由于 $y^{\\top}Xw$ 是一个标量，它等于其转置 $(y^{\\top}Xw)^{\\top} = w^{\\top}X^{\\top}y$。因此，我们可以合并交叉项：\n$$\n\\|y - X w\\|_{2}^{2} = y^{\\top}y - 2w^{\\top}X^{\\top}y + w^{\\top}X^{\\top}Xw\n$$\n\n第二项是权重衰减惩罚：\n$$\n\\lambda \\|w\\|_{2}^{2} = \\lambda w^{\\top}w\n$$\n\n将这些结合起来，完整的目标函数是：\n$$\nJ(w) = y^{\\top}y - 2w^{\\top}X^{\\top}y + w^{\\top}X^{\\top}Xw + \\lambda w^{\\top}w\n$$\n\n现在我们对 $J(w)$ 关于向量 $w$ 求导。我们使用以下标准的矩阵微积分法则：\n1.  对于常数 $c$，$\\nabla_{w} (c) = 0$。\n2.  对于向量 $a$，$\\nabla_{w} (w^{\\top}a) = \\nabla_{w} (a^{\\top}w) = a$。\n3.  $\\nabla_{w} (w^{\\top}Mw) = (M + M^{\\top})w$。如果 $M$ 是对称的，这可以简化为 $2Mw$。\n\n将这些法则应用于 $J(w)$：\n- 项 $y^{\\top}y$ 相对于 $w$ 是常数，因此其梯度为 $0$。\n- 对于项 $-2w^{\\top}X^{\\top}y$，向量 $X^{\\top}y$ 相对于 $w$ 是常数。所以，$\\nabla_{w}(-2w^{\\top}X^{\\top}y) = -2X^{\\top}y$。\n- 对于项 $w^{\\top}X^{\\top}Xw$，矩阵 $M = X^{\\top}X$ 是对称的。因此，$\\nabla_{w}(w^{\\top}X^{\\top}Xw) = 2(X^{\\top}X)w$。\n- 对于项 $\\lambda w^{\\top}w$，我们可以将其写为 $\\lambda w^{\\top}Iw$，其中 $I$ 是单位矩阵。由于 $I$ 是对称的，$\\nabla_{w}(\\lambda w^{\\top}Iw) = \\lambda(2Iw) = 2\\lambda w$。\n\n将这些梯度相加，我们得到：\n$$\n\\nabla_{w} J(w) = -2X^{\\top}y + 2(X^{\\top}X)w + 2\\lambda w\n$$\n\n为了找到最小化 $J(w)$ 的最优权重 $w^{\\star}$，我们将梯度设为零向量：\n$$\n\\nabla_{w} J(w^{\\star}) = -2X^{\\top}y + 2(X^{\\top}X)w^{\\star} + 2\\lambda w^{\\star} = 0\n$$\n两边除以 $2$ 并重新整理各项：\n$$\n(X^{\\top}X)w^{\\star} + \\lambda w^{\\star} = X^{\\top}y\n$$\n在左侧提出 $w^{\\star}$（注意 $\\lambda w^{\\star} = \\lambda I w^{\\star}$，其中 $I$ 是 $d \\times d$ 的单位矩阵）：\n$$\n(X^{\\top}X + \\lambda I) w^{\\star} = X^{\\top}y\n$$\n为了分离出 $w^{\\star}$，我们乘以矩阵 $(X^{\\top}X + \\lambda I)$ 的逆。这个逆矩阵存在，因为 $X^{\\top}X$ 是一个半正定矩阵，而对于 $\\lambda > 0$，$\\lambda I$ 是一个正定矩阵。一个半正定矩阵和一个正定矩阵之和总是正定的，因此是可逆的。\n$$\nw^{\\star} = (X^{\\top}X + \\lambda I)^{-1} X^{\\top}y\n$$\n这就是优化器 $w^{\\star}$ 的闭式解。\n\n这个结果建立了与岭回归的等价性。岭回归的标准公式是找到系数向量 $w$ 来最小化带惩罚的平方误差和，这正是目标函数 $J(w)$。推导出的解 $w^{\\star} = (X^{\\top}X + \\lambda I)^{-1} X^{\\top}y$ 是岭回归著名的闭式解。因此，用平方误差和损失及 L2 权重衰减来训练一个单层线性神经网络，在数学上等同于执行岭回归。\n\n接下来，我们为给定的数据集计算 $w^{\\star}$：\n$$\nX = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}, \\quad \\lambda = 1\n$$\n权重向量 $w$ 将在 $\\mathbb{R}^{2}$ 中。首先，我们计算所需的矩阵。\n$X$ 的转置是：\n$$\nX^{\\top} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix}\n$$\n接下来，我们计算 $X^{\\top}X$：\n$$\nX^{\\top}X = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} = \\begin{pmatrix} 1(1)+0(0)+1(1)  1(0)+0(1)+1(1) \\\\ 0(1)+1(0)+1(1)  0(0)+1(1)+1(1) \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}\n$$\n现在我们计算 $(X^{\\top}X + \\lambda I)$ 项。由于 $X^{\\top}X$ 是一个 $2 \\times 2$ 矩阵，$I$ 是 $2 \\times 2$ 的单位矩阵。\n$$\nX^{\\top}X + \\lambda I = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} + 1 \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 3  1 \\\\ 1  3 \\end{pmatrix}\n$$\n接下来，我们求这个矩阵的逆。对于一个 $2 \\times 2$ 矩阵 $A = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$，其逆为 $A^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$。\n行列式为 $\\det(X^{\\top}X + \\lambda I) = 3(3) - 1(1) = 9 - 1 = 8$。\n逆矩阵为：\n$$\n(X^{\\top}X + \\lambda I)^{-1} = \\frac{1}{8} \\begin{pmatrix} 3  -1 \\\\ -1  3 \\end{pmatrix}\n$$\n现在，我们计算 $X^{\\top}y$ 项：\n$$\nX^{\\top}y = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1(1)+0(2)+1(3) \\\\ 0(1)+1(2)+1(3) \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 5 \\end{pmatrix}\n$$\n最后，我们将两个结果相乘来计算 $w^{\\star}$：\n$$\nw^{\\star} = (X^{\\top}X + \\lambda I)^{-1} (X^{\\top}y) = \\frac{1}{8} \\begin{pmatrix} 3  -1 \\\\ -1  3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 5 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 3(4) - 1(5) \\\\ -1(4) + 3(5) \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 12 - 5 \\\\ -4 + 15 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 7 \\\\ 11 \\end{pmatrix}\n$$\n所以，优化后的权重向量是：\n$$\nw^{\\star} = \\begin{pmatrix} \\frac{7}{8} \\\\ \\frac{11}{8} \\end{pmatrix}\n$$\n问题要求最终答案以行矩阵的形式给出。\n$$\n(w^{\\star})^{\\top} = \\begin{pmatrix} \\frac{7}{8}  \\frac{11}{8} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7}{8}  \\frac{11}{8}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "理论知识需要通过实践来巩固。本练习是一个编码实验，旨在直观地展示 $L_2$ 正则化如何有效地抑制模型过拟合。通过在一个含有随机噪声标签的数据集上训练模型，你将亲手探索并找出一个“临界”正则化强度 $\\lambda$，该值能引导模型忽略训练数据中的噪声并提升其泛化能力，而不是简单地“记忆”训练样本。",
            "id": "3141360",
            "problem": "你需要实现一个完整、可运行的程序，构建一个受控的合成二元分类实验，以研究平方欧几里得范数正则化（也称为 $L_{2}$ 正则化或权重衰减）如何防止对随机标签的记忆，从而提高泛化能力。该程序必须以精确指定的格式产生单行输出。\n\n该程序必须遵循以下形式化设计。\n\n1. 数据生成。对于每个测试用例，生成一个训练设计矩阵 $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$ 和一个测试设计矩阵 $X_{\\text{test}} \\in \\mathbb{R}^{m \\times d}$，其中 $d = d_{\\text{sig}} + d_{\\text{noi}}$。$X_{\\text{train}}$ 和 $X_{\\text{test}}$ 的每一行都必须独立地从一个零均值、单位协方差的多元正态分布中采样。构建一个真实权重向量 $w^{\\star} \\in \\mathbb{R}^{d}$，其前 $d_{\\text{sig}}$ 个分量为非零值，后 $d_{\\text{noi}}$ 个分量恰好为零；确保 $w^{\\star}$ 具有单位欧几里得范数。通过 $y^{\\text{clean}} = \\operatorname{sign}(X w^{\\star})$ 按元素定义干净标签，取值于 $\\{-1, +1\\}$。对于训练集，通过以概率 $p$ 将每个 $y^{\\text{clean}}_{i}$ 独立地替换为从 $\\{-1, +1\\}$ 中随机抽取的一个值（并以 $1 - p$ 的概率保持不变），来构建观测到的（可能被破坏的）标签 $y_{\\text{train}}$。对于测试集，使用 $y_{\\text{test}} = \\operatorname{sign}(X_{\\text{test}} w^{\\star})$，不进行任何破坏。因此，标签在期望上是平衡的，并且由于对称性，测试集上的随机水平准确率为 $0.5$。\n\n2. 模型类别与学习目标。考虑线性预测器 $f_{w}(x) = x^{\\top} w$，其中 $w \\in \\mathbb{R}^{d}$ 且无偏置项。通过最小化带 $L_{2}$ 正则化的经验风险来训练 $w$：对 $w \\in \\mathbb{R}^{d}$ 进行最小化，\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\left(x_{i}^{\\top} w - y_{\\text{train},i}\\right)^{2} \\;+\\; \\lambda \\,\\lVert w \\rVert_{2}^{2},\n$$\n对于网格中提供的多个正则化强度 $\\lambda \\ge 0$ 值。训练后，通过 $\\hat{y} = \\operatorname{sign}(X_{\\text{test}} w)$ 进行分类，并计算相对于 $y_{\\text{test}}$ 的正确预测比例作为测试准确率。\n\n3. 转变准则与临界 $\\lambda$。定义提升阈值 $\\tau$（严格大于随机水平 $0.5$）。对于给定的测试用例和给定的 $\\lambda$ 值升序网格，将临界 $\\lambda$ 定义为网格中使得相应测试准确率至少为 $\\tau$ 的最小 $\\lambda$。如果所提供网格中没有 $\\lambda$ 能达到至少 $\\tau$ 的测试准确率，则将临界 $\\lambda$ 定义为 $-1.0$。\n\n4. 确定性。每个测试用例使用固定的伪随机数生成器种子。在计算 $\\lambda = 0$ 的解时，以确定性的方式处理正规方程中的潜在奇异性，以使训练规则是明确定义且可复现的。\n\n5. 测试套件。您的程序必须执行以下三个测试用例，并独立处理每个用例：\n   - 用例 A（$L_{2}$ 抑制随机标签记忆）：$n = 120$，$m = 4000$，$d_{\\text{sig}} = 5$，$d_{\\text{noi}} = 300$，$p = 0.4$，提升阈值 $\\tau = 0.6$，正则化网格\n     $$\n     \\Lambda_{A} = [\\,0.0,\\; 10^{-6},\\; 10^{-5},\\; 3\\cdot 10^{-5},\\; 10^{-4},\\; 3\\cdot 10^{-4},\\; 10^{-3},\\; 3\\cdot 10^{-3},\\; 10^{-2},\\; 3\\cdot 10^{-2},\\; 10^{-1},\\; 3\\cdot 10^{-1},\\; 1.0,\\; 3.0,\\; 10.0\\,].\n     $$\n   - 用例 B（完全随机标签；不可能有提升）：$n = 120$，$m = 4000$，$d_{\\text{sig}} = 5$，$d_{\\text{noi}} = 300$，$p = 1.0$，提升阈值 $\\tau = 0.6$，正则化网格\n     $$\n     \\Lambda_{B} = \\Lambda_{A}.\n     $$\n   - 用例 C（干净、低维的真实信号；立即有提升）：$n = 200$，$m = 2000$，$d_{\\text{sig}} = 1$，$d_{\\text{noi}} = 0$，$p = 0.0$，提升阈值 $\\tau = 0.6$，正则化网格\n     $$\n     \\Lambda_{C} = \\Lambda_{A}.\n     $$\n\n   为保证可复现性，请使用各用例的随机种子 $s_{A} = 12345$、$s_{B} = 13345$ 和 $s_{C} = 14345$。\n\n6. 输出规范。您的程序必须生成单行输出，其中包含一个列表，按 A、B、C 的顺序恰好有三个浮点数值，每个值是该用例的临界正则化强度（如上定义），当未观察到向改进泛化的转变时，使用值 $-1.0$。该行必须采用确切的格式\n$$\n[\\ell_{A},\\ell_{B},\\ell_{C}]\n$$\n值之间用逗号分隔，无附加文本。不涉及物理单位。\n\n您的目标是实现这个实验，以直接测试 $L_{2}$ 正则化是否以及何时通过收缩权重向量来防止对随机标签的记忆，并测量在三种情况下，测试准确率从随机水平转变为改进泛化时的最小正则化强度。",
            "solution": "该问题要求实现一个数值实验，以研究 $L_2$ 正则化在二元分类背景下的作用。该实验旨在展示正则化如何防止模型记忆训练标签中的噪声，从而提高其泛化到未见过的数据的能力。任务的核心是找到“临界”正则化强度 $\\lambda$，在该强度下，模型在测试集上的性能从随机水平转变到指定的提升阈值。这将针对三个旨在突显正则化不同方面的不同用例进行。\n\n模型是一个线性预测器 $f_w(x) = x^{\\top} w$，其中 $x \\in \\mathbb{R}^{d}$ 是特征向量，$w \\in \\mathbb{R}^{d}$ 是权重向量。权重是通过最小化基于平方误差损失的正则化经验风险目标函数来学习的。对于一个包含 $n$ 个样本 $\\{(x_i, y_i)\\}_{i=1}^{n}$ 的训练集，目标函数 $L(w)$ 由下式给出：\n$$\nL(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(x_{i}^{\\top} w - y_i\\right)^{2} + \\lambda \\lVert w \\rVert_{2}^{2}\n$$\n此处，$y_i \\in \\{-1, +1\\}$ 是训练标签，$\\lambda \\ge 0$ 是正则化参数，用于控制对权重向量的平方欧几里得范数 $\\lVert w \\rVert_{2}^{2}$ 的惩罚。\n\n为了找到最小化 $L(w)$ 的最优权重向量 $\\hat{w}$，我们计算 $L(w)$ 关于 $w$ 的梯度，并将其设为零向量。首先，我们用矩阵表示法来表达目标函数。设 $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$ 为设计矩阵，其第 $i$ 行为 $x_i^{\\top}$，设 $y_{\\text{train}} \\in \\mathbb{R}^{n}$ 为训练标签向量。目标函数变为：\n$$\nL(w) = \\frac{1}{n} \\lVert X_{\\text{train}}w - y_{\\text{train}} \\rVert_{2}^{2} + \\lambda \\lVert w \\rVert_{2}^{2} = \\frac{1}{n} (X_{\\text{train}}w - y_{\\text{train}})^{\\top}(X_{\\text{train}}w - y_{\\text{train}}) + \\lambda w^{\\top}w\n$$\n$L(w)$ 关于 $w$ 的梯度为：\n$$\n\\nabla_w L(w) = \\frac{1}{n} \\nabla_w (w^{\\top}X_{\\text{train}}^{\\top}X_{\\text{train}}w - 2y_{\\text{train}}^{\\top}X_{\\text{train}}w + y_{\\text{train}}^{\\top}y_{\\text{train}}) + \\lambda \\nabla_w (w^{\\top}w)\n$$\n使用标准的矩阵微积分恒等式，并注意到 $X_{\\text{train}}^{\\top}X_{\\text{train}}$ 是一个对称矩阵，我们得到：\n$$\n\\nabla_w L(w) = \\frac{1}{n} (2X_{\\text{train}}^{\\top}X_{\\text{train}}w - 2X_{\\text{train}}^{\\top}y_{\\text{train}}) + 2\\lambda w\n$$\n将梯度设为零以求最小值：\n$$\n\\frac{2}{n} X_{\\text{train}}^{\\top}X_{\\text{train}}w - \\frac{2}{n} X_{\\text{train}}^{\\top}y_{\\text{train}} + 2\\lambda w = 0\n$$\n$$\n(X_{\\text{train}}^{\\top}X_{\\text{train}} + n\\lambda I) w = X_{\\text{train}}^{\\top}y_{\\text{train}}\n$$\n其中 $I$ 是 $d \\times d$ 单位矩阵。\n\n这个线性方程组为最优权重向量 $\\hat{w}$ 提供了解决方案。\n\n对于 $\\lambda > 0$，矩阵 $(X_{\\text{train}}^{\\top}X_{\\text{train}} + n\\lambda I)$ 保证是可逆的。这是因为 $X_{\\text{train}}^{\\top}X_{\\text{train}}$ 是半正定的，$n\\lambda I$ 是正定的，使其和为正定矩阵，因此可逆。唯一解是：\n$$\n\\hat{w} = (X_{\\text{train}}^{\\top}X_{\\text{train}} + n\\lambda I)^{-1} X_{\\text{train}}^{\\top} y_{\\text{train}}\n$$\n在数值上，直接求解线性系统比计算矩阵的逆更稳定、更高效。\n\n对于 $\\lambda = 0$ 的特殊情况，目标函数变为标准的非正则化最小二乘问题，正规方程为 $X_{\\text{train}}^{\\top}X_{\\text{train}}w = X_{\\text{train}}^{\\top}y_{\\text{train}}$。在高维设置中（如测试用例 A 和 B），当特征数量 $d$ 大于样本数量 $n$ 时，矩阵 $X_{\\text{train}}^{\\top}X_{\\text{train}}$ 是奇异的，系统有无穷多解。问题要求对此情况进行确定性处理。标准方法是使用 Moore-Penrose 伪逆，这会产生具有最小欧几里得范数的解。这等同于求解最小二乘问题 $\\min_{w} \\lVert X_{\\text{train}}w - y_{\\text{train}} \\rVert_2^2$，这可以很容易地使用数值线性代数库来完成。\n\n每个测试用例的实验流程如下：\n1.  使用指定的种子初始化伪随机数生成器以保证可复现性。\n2.  通过从标准正态分布中生成其前 $d_{\\text{sig}}$ 个分量，将此子向量归一化为单位范数，并将其余 $d_{\\text{noi}}$ 个分量设置为零，来构建真实权重向量 $w^{\\star} \\in \\mathbb{R}^{d}$。这确保了 $\\lVert w^{\\star} \\rVert_{2} = 1$。\n3.  通过从标准正态分布 $\\mathcal{N}(0, 1)$ 中独立抽取每个条目，生成训练数据 $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$ 和测试数据 $X_{\\text{test}} \\in \\mathbb{R}^{m \\times d}$。\n4.  生成标签。真实的测试标签是 $y_{\\text{test}} = \\operatorname{sign}(X_{\\text{test}} w^{\\star})$。训练标签 $y_{\\text{train}}$ 的生成方式是：首先计算干净标签 $y^{\\text{clean}} = \\operatorname{sign}(X_{\\text{train}} w^{\\star})$，然后以概率 $p$ 将每个标签替换为从 $\\{-1, +1\\}$ 中随机抽取的值。`sign` 函数对于输入 $0$ 的输出将被确定性地映射到 $+1$。\n5.  遍历所提供的 $\\lambda$ 值升序网格。对于每个 $\\lambda$，计算相应的最优权重向量 $\\hat{w}$。\n6.  使用学习到的 $\\hat{w}$ 在测试集上进行预测：$\\hat{y}_{\\text{test}} = \\operatorname{sign}(X_{\\text{test}}\\hat{w})$。\n7.  计算测试准确率，即匹配预测的比例：$\\frac{1}{m} \\sum_{j=1}^{m} \\mathbb{I}(\\hat{y}_{\\text{test},j} = y_{\\text{test},j})$。\n8.  临界 $\\lambda$ 是网格中第一个使测试准确率大于或等于提升阈值 $\\tau$ 的值。如果找不到这样的 $\\lambda$，则临界值定义为 $-1.0$。\n\n此过程独立应用于三个测试用例中的每一个，并报告所得的临界 $\\lambda$ 值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Executes a synthetic experiment to study L2 regularization.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    lambda_grid = [0.0, 1e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 0.1, 0.3, 1.0, 3.0, 10.0]\n    \n    test_cases = [\n        # Case A: High-dimensional, noisy labels\n        {'n': 120, 'm': 4000, 'd_sig': 5, 'd_noi': 300, 'p': 0.4, 'tau': 0.6, 'seed': 12345, 'lambda_grid': lambda_grid},\n        # Case B: Fully random labels\n        {'n': 120, 'm': 4000, 'd_sig': 5, 'd_noi': 300, 'p': 1.0, 'tau': 0.6, 'seed': 13345, 'lambda_grid': lambda_grid},\n        # Case C: Clean, low-dimensional signal\n        {'n': 200, 'm': 2000, 'd_sig': 1, 'd_noi': 0, 'p': 0.0, 'tau': 0.6, 'seed': 14345, 'lambda_grid': lambda_grid},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        n, m, d_sig, d_noi, p, tau, seed, current_lambda_grid = \\\n            case['n'], case['m'], case['d_sig'], case['d_noi'], case['p'], case['tau'], case['seed'], case['lambda_grid']\n        \n        d = d_sig + d_noi\n        \n        # 1. Deterministic Setup\n        rng = np.random.default_rng(seed)\n\n        # 2. Data Generation\n        # Construct ground-truth weight vector w_star with unit norm\n        w_star_sig = rng.standard_normal(size=d_sig)\n        w_star_sig /= np.linalg.norm(w_star_sig)\n        w_star = np.concatenate((w_star_sig, np.zeros(d_noi)))\n        \n        # Generate design matrices\n        X_train = rng.standard_normal(size=(n, d))\n        X_test = rng.standard_normal(size=(m, d))\n        \n        # Generate labels\n        # Test labels (clean)\n        y_test = np.sign(X_test @ w_star)\n        y_test[y_test == 0] = 1 # Deterministic handling of sign(0)\n\n        # Training labels (potentially corrupted)\n        y_clean_train = np.sign(X_train @ w_star)\n        y_clean_train[y_clean_train == 0] = 1\n        \n        y_train = y_clean_train.copy()\n        corruption_mask = rng.random(size=n)  p\n        num_corrupt = np.sum(corruption_mask)\n        if num_corrupt > 0:\n            random_labels = rng.choice([-1, 1], size=num_corrupt)\n            y_train[corruption_mask] = random_labels\n\n        # 3. Model Training and Evaluation\n        critical_lambda = -1.0\n        \n        # Pre-compute parts of the normal equations\n        XtX = X_train.T @ X_train\n        Xty = X_train.T @ y_train\n        identity_d = np.identity(d)\n\n        for lam in current_lambda_grid:\n            # Solve for the weight vector w_hat\n            if lam == 0.0:\n                # For lambda=0, use lstsq for the min-norm solution, robust to singularity\n                w_hat = np.linalg.lstsq(X_train, y_train, rcond=None)[0]\n            else:\n                # For lambda > 0, solve the regularized normal equations\n                A = XtX + n * lam * identity_d\n                w_hat = np.linalg.solve(A, Xty)\n            \n            # Predict on test set\n            y_pred = np.sign(X_test @ w_hat)\n            y_pred[y_pred == 0] = 1 # Deterministic handling of sign(0)\n            \n            # Compute test accuracy\n            accuracy = np.mean(y_pred == y_test)\n            \n            # Check for transition\n            if accuracy >= tau:\n                critical_lambda = lam\n                break\n        \n        results.append(critical_lambda)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在现代深度学习实践中，正则化与优化算法的交互可能非常微妙。此练习旨在剖析传统 $L_2$ 正则化（将惩罚项加入损失函数）与“解耦权重衰减”（decoupled weight decay）之间的关键差异，后者是 AdamW 等现代优化器的核心特性。透彻理解这一区别，对于有效训练当前的大型神经网络模型至关重要。",
            "id": "3141373",
            "problem": "考虑参数 $\\mathbf{w} \\in \\mathbb{R}^d$ 和一个可微的数据拟合损失 $L(\\mathbf{w})$。定义带惩罚的目标函数 $J(\\mathbf{w}) = L(\\mathbf{w}) + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|_2^2$，其中正则化强度 $\\lambda  0$。学习率为 $\\eta  0$ 的随机梯度下降 (Stochastic Gradient Descent, SGD) 采用由所选目标函数的梯度驱动的一阶更新形式。相比之下，自适应矩估计 (Adaptive Moment Estimation, Adam) 使用一个逐元素的正定对角预处理器来重新缩放所选目标函数的梯度，这可以抽象地写成形式为 $\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\mathbf{D}_t \\nabla J(\\mathbf{w}_t)$ 的更新，其中 $\\mathbf{D}_t \\in \\mathbb{R}^{d \\times d}$ 是一个对角矩阵，其严格为正的对角线元素由过去的随机梯度确定。带解耦权重衰减的自适应矩估计 (Adaptive Moment Estimation with Decoupled Weight Decay, AdamW) 的定义是将权重收缩与基于梯度的步骤解耦，通过对权重直接应用 $\\eta \\lambda$ 的收缩，同时保留 Adam 对数据拟合梯度的自适应重新缩放。\n\n仅使用上述定义以及梯度和线性算子的基本性质，分析在自适应优化器下，向损失函数添加 $L_2$ 惩罚项在何种情况下等同于在更新中应用权重衰减，以及在何种情况下它们有所不同。选择所有正确的陈述。\n\nA. 在没有动量且学习率 $\\eta$ 固定的随机梯度下降 (SGD) 中，优化 $J(\\mathbf{w})$ 与对 $\\mathbf{w}_t$ 应用因子为 $(1 - \\eta \\lambda)$ 的乘性权重衰减，然后以学习率 $\\eta$ 对 $L(\\mathbf{w})$ 进行梯度步骤，在每一步上都是完全等价的。\n\nB. 在自适应矩估计 (Adam) 中，对于任何对角预处理器 $\\mathbf{D}_t$，在 $J(\\mathbf{w})$ 内部添加 $L_2$ 惩罚项等同于解耦权重衰减，因为惩罚项的梯度与 $\\mathbf{w}_t$ 共线。\n\nC. 解耦权重衰减 (AdamW) 消除了收缩对自适应重新缩放的依赖，实现了一种形式为“按 $\\eta \\lambda$ 收缩，再加上一个经过自适应预处理的数据梯度步骤”的直接更新，因此通常不等同于在 Adam 内部添加 $L_2$ 惩罚项。\n\nD. 如果自适应预处理器在每一步都是单位矩阵的标量倍数，即 $\\mathbf{D}_t = c_t \\mathbf{I}$，其中 $c_t  0$ 对所有坐标都相同，那么在梯度内部添加 $L_2$ 惩罚项就与解耦权重衰减等效，只要将 $\\lambda$ 按 $c_t$ 进行重新缩放即可。如果不调整 $\\lambda$，这两个过程会产生不同大小的收缩。\n\nE. 在原生 SGD 中，添加 $L_2$ 惩罚项与应用权重衰减之间的等价性仅在小批量数据的期望意义上成立，而在单步更新层面上则不成立，因为随机梯度噪声破坏了这种恒等关系。",
            "solution": "该问题陈述经核实具有科学依据、问题明确且客观。它为分析基于梯度的优化算法中 $L_2$ 正则化与解耦权重衰减之间的关系提供了一套正确且标准的定义。未发现任何缺陷。\n\n分析的核心在于带惩罚的目标函数 $J(\\mathbf{w})$ 的梯度。给定 $J(\\mathbf{w}) = L(\\mathbf{w}) + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|_2^2$，其关于参数 $\\mathbf{w}$ 的梯度是：\n$$ \\nabla J(\\mathbf{w}) = \\nabla L(\\mathbf{w}) + \\nabla \\left( \\frac{\\lambda}{2}\\|\\mathbf{w}\\|_2^2 \\right) $$\n由于 $\\|\\mathbf{w}\\|_2^2 = \\mathbf{w}^T\\mathbf{w}$，其梯度为 $2\\mathbf{w}$。因此，\n$$ \\nabla J(\\mathbf{w}) = \\nabla L(\\mathbf{w}) + \\lambda \\mathbf{w} $$\n在随机优化的背景下，数据拟合损失的真实梯度 $\\nabla L(\\mathbf{w})$ 在每一步 $t$ 被一个在小批量数据上计算的随机梯度 $\\mathbf{g}_t(\\mathbf{w}_t)$ 近似。更新规则基于此随机梯度进行分析。\n\n**A. 在没有动量且学习率 $\\eta$ 固定的随机梯度下降 (SGD) 中，优化 $J(\\mathbf{w})$ 与对 $\\mathbf{w}_t$ 应用因子为 $(1 - \\eta \\lambda)$ 的乘性权重衰减，然后以学习率 $\\eta$ 对 $L(\\mathbf{w})$ 进行梯度步骤，在每一步上都是完全等价的。**\n\n优化带惩罚目标函数 $J(\\mathbf{w})$ 的 SGD 更新规则是：\n$$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\nabla J(\\mathbf{w}_t) $$\n代入 $J(\\mathbf{w})$ 的随机梯度表达式：\n$$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta (\\mathbf{g}_t(\\mathbf{w}_t) + \\lambda \\mathbf{w}_t) $$\n通过分配项 $-\\eta$，我们可以重新整理方程：\n$$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\lambda \\mathbf{w}_t - \\eta \\mathbf{g}_t(\\mathbf{w}_t) $$\n$$ \\mathbf{w}_{t+1} = (1 - \\eta \\lambda) \\mathbf{w}_t - \\eta \\mathbf{g}_t(\\mathbf{w}_t) $$\n这个最终形式在代数上等同于一个两步过程：\n1.  应用乘性权重衰减：$\\mathbf{w}'_t = (1 - \\eta \\lambda) \\mathbf{w}_t$。\n2.  对数据拟合损失进行梯度步骤：$\\mathbf{w}_{t+1} = \\mathbf{w}'_t - \\eta \\mathbf{g}_t(\\mathbf{w}_t)$。\n这种等价性对每个单独的更新步骤都精确成立。\n\n结论：**正确**。\n\n**B. 在自适应矩估计 (Adam) 中，对于任何对角预处理器 $\\mathbf{D}_t$，在 $J(\\mathbf{w})$ 内部添加 $L_2$ 惩罚项等同于解耦权重衰减，因为惩罚项的梯度与 $\\mathbf{w}_t$ 共线。**\n\n像 Adam 这样的自适应优化器应用于 $J(\\mathbf{w})$ 的抽象更新规则是：\n$$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\mathbf{D}_t \\nabla J(\\mathbf{w}_t) $$\n$$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\mathbf{D}_t (\\mathbf{g}_t(\\mathbf{w}_t) + \\lambda \\mathbf{w}_t) $$\n$$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\mathbf{D}_t \\mathbf{g}_t(\\mathbf{w}_t) - \\eta \\lambda \\mathbf{D}_t \\mathbf{w}_t $$\n这是 L2 正则化方法。权重收缩由项 $-\\eta \\lambda \\mathbf{D}_t \\mathbf{w}_t$ 给出。\n\n解耦权重衰减方法（如 AdamW 中）将衰减与预处理过的梯度步骤分开：\n$$ \\mathbf{w}_{t+1} = (1 - \\eta \\lambda) \\mathbf{w}_t - \\eta \\mathbf{D}_t \\mathbf{g}_t(\\mathbf{w}_t) $$\n$$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\lambda \\mathbf{w}_t - \\eta \\mathbf{D}_t \\mathbf{g}_t(\\mathbf{w}_t) $$\n在这里，权重收缩由项 $-\\eta \\lambda \\mathbf{w}_t$ 给出。\n\n要使这两种方法等效，它们各自的收缩项必须相等：\n$$ -\\eta \\lambda \\mathbf{D}_t \\mathbf{w}_t = -\\eta \\lambda \\mathbf{w}_t \\implies \\mathbf{D}_t \\mathbf{w}_t = \\mathbf{w}_t $$\n这要求 $\\mathbf{D}_t$ 是单位矩阵 $\\mathbf{I}$，或者 $\\mathbf{w}_t$ 是 $\\mathbf{D}_t$ 的一个特征值为 1 的特征向量。在 Adam 中，$\\mathbf{D}_t$ 是一个对角矩阵，其元素基于过去梯度平方的移动平均，它几乎永远不会是单位矩阵。因此，这两种方法不等效。所给出的理由，即惩罚梯度 $\\lambda\\mathbf{w}_t$ 与 $\\mathbf{w}_t$ 共线，是一个正确但无关的事实。不等效性源于非单位算子 $\\mathbf{D}_t$ 对惩罚梯度的作用。\n\n结论：**不正确**。\n\n**C. 解耦权重衰减 (AdamW) 消除了收缩对自适应重新缩放的依赖，实现了一种形式为“按 $\\eta \\lambda$ 收缩，再加上一个经过自适应预处理的数据梯度步骤”的直接更新，因此通常不等同于在 Adam 内部添加 $L_2$ 惩罚项。**\n\n如选项 B 的推导所示，AdamW 的更新是 $\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\lambda \\mathbf{w}_t - \\eta \\mathbf{D}_t \\mathbf{g}_t(\\mathbf{w}_t)$。收缩项是 $-\\eta \\lambda \\mathbf{w}_t$。对于每个权重分量，该项的大小与权重的值成正比，并且不受自适应预处理器 $\\mathbf{D}_t$ 的影响。这证实了收缩与自适应重新缩放是解耦的。\n相比之下，带 $L_2$ 惩罚的 Adam 更新是 $\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\mathbf{D}_t \\mathbf{g}_t(\\mathbf{w}_t) - \\eta \\lambda \\mathbf{D}_t \\mathbf{w}_t$。这里的收缩项 $-\\eta \\lambda \\mathbf{D}_t \\mathbf{w}_t$ 直接受到自适应重新缩放矩阵 $\\mathbf{D}_t$ 的影响。由于通常情况下 $\\mathbf{D}_t \\neq \\mathbf{I}$，这两种方法是不等效的。该陈述准确地描述了这种差异及其后果。\n\n结论：**正确**。\n\n**D. 如果自适应预处理器在每一步都是单位矩阵的标量倍数，即 $\\mathbf{D}_t = c_t \\mathbf{I}$，其中 $c_t  0$ 对所有坐标都相同，那么在梯度内部添加 $L_2$ 惩罚项就与解耦权重衰减等效，只要将 $\\lambda$ 按 $c_t$ 进行重新缩放即可。如果不调整 $\\lambda$，这两个过程会产生不同大小的收缩。**\n\n我们假设 $\\mathbf{D}_t = c_t \\mathbf{I}$。\nL2 惩罚方法的更新变为：\n$$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta (c_t \\mathbf{I}) (\\mathbf{g}_t(\\mathbf{w}_t) + \\lambda \\mathbf{w}_t) = \\mathbf{w}_t - \\eta c_t \\mathbf{g}_t(\\mathbf{w}_t) - \\eta (\\lambda c_t) \\mathbf{w}_t $$\n相应的解耦权重衰减更新，设衰减参数为 $\\lambda'$，则为：\n$$ \\mathbf{w}_{t+1} = (1 - \\eta \\lambda') \\mathbf{w}_t - \\eta (c_t \\mathbf{I}) \\mathbf{g}_t(\\mathbf{w}_t) = \\mathbf{w}_t - \\eta \\lambda' \\mathbf{w}_t - \\eta c_t \\mathbf{g}_t(\\mathbf{w}_t) $$\n比较这两个表达式，如果我们设置 $\\lambda' = \\lambda c_t$，它们就变得完全相同。这意味着如果将解耦权重衰减的有效衰减强度按 $c_t$ 重新缩放，则 L2 正则化等同于解耦权重衰减。陈述的第一部分是正确的。\n如果不调整 $\\lambda$（即，两种方法使用相同的 $\\lambda$，因此 $\\lambda' = \\lambda$），则 L2 惩罚的收缩项为 $-\\eta (\\lambda c_t) \\mathbf{w}_t$，而解耦衰减的收缩项为 $-\\eta \\lambda \\mathbf{w}_t$。由于 $c_t$ 不一定等于 $1$，这些收缩的大小是不同的。陈述的第二部分也是正确的。\n\n结论：**正确**。\n\n**E. 在原生 SGD 中，添加 $L_2$ 惩罚项与应用权重衰减之间的等价性仅在小批量数据的期望意义上成立，而在单步更新层面上则不成立，因为随机梯度噪声破坏了这种恒等关系。**\n\n如选项 A 的分析所示，对 L2 惩罚目标进行 SGD 更新的公式是 $\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta (\\mathbf{g}_t(\\mathbf{w}_t) + \\lambda \\mathbf{w}_t)$。通过代数重排，得到 $\\mathbf{w}_{t+1} = (1 - \\eta \\lambda) \\mathbf{w}_t - \\eta \\mathbf{g}_t(\\mathbf{w}_t)$。这是一个精确的代数恒等式。其有效性不依赖于随机梯度 $\\mathbf{g}_t(\\mathbf{w}_t)$ 的具体值或其统计特性（如其期望）。“随机梯度噪声”被封装在项 $\\mathbf{g}_t(\\mathbf{w}_t)$ 中，但代数操作本身是精确的，并且对任何 $\\mathbf{g}_t(\\mathbf{w}_t)$ 都成立。因此，这种等价性在每个单独的步骤中都精确成立，而不仅仅是在期望意义上。所给出的推理是错误的。\n\n结论：**不正确**。",
            "answer": "$$\\boxed{ACD}$$"
        }
    ]
}