## 引言
在构建强大的机器学习模型时，一个永恒的挑战是如何避免**过拟合**——即模型对训练数据学习得“太好”，以至于失去了对新数据的泛化能力。在众多应对策略中，$L_2$ 正则化（又称[权重衰减](@entry_id:635934)）无疑是最基础、最普适且最有效的技术之一。它通过一个简单的数学形式，深刻地影响着模型的学习过程。然而，仅仅知道它的存在和作用是远远不够的。为了真正驾驭这一工具，我们需要从更深层次理解其背后的多重机制与广泛联系。本文旨在填补从“知其然”到“知其所以然”的鸿沟。

本文将分三个章节系统地引导你全面掌握 $L_2$ 正则化。在 **“原理与机制”** 一章中，我们将从统计学、最优化和贝叶斯推断等多个互补的视角，深入剖析其核心工作原理。接着，在 **“应用与跨学科联系”** 一章中，我们将探索 $L_2$ 正则化如何化身为岭回归、[吉洪诺夫正则化](@entry_id:140094)等工具，在从统计学到金融工程的广阔领域中解决实际问题。最后，在 **“动手实践”** 部分，你将通过一系列精心设计的编程练习，将理论知识转化为解决真实问题的实践能力。

## 原理与机制

在机器学习模型的构建中，我们的目标是学习一个能够很好地泛化到未见数据的函数。然而，仅最小化训练数据上的误差（即[经验风险](@entry_id:633993)）往往会导致模型变得过于复杂，以至于它开始学习训练数据中的噪声和特质，而非潜在的真实规律。这种现象被称为**[过拟合](@entry_id:139093)**（overfitting）。$L_2$ 正则化，也称为[权重衰减](@entry_id:635934)（weight decay）或[吉洪诺夫正则化](@entry_id:140094)（Tikhonov regularization），是应对这一挑战的最基本、最有效的方法之一。本章将深入探讨 $L_2$ 正则化的核心原理与作用机制，从多个互补的视角揭示其深刻内涵。

### L₂ 正则化的定义与形式

$L_2$ 正则化的核心思想是在模型的[损失函数](@entry_id:634569)中加入一个惩罚项，该惩罚项与模型参数的 $L_2$ 范数的平方成正比。对于一个参数为 $\mathbf{w}$ 的模型，其在包含 $n$ 个样本的数据集上的[经验风险](@entry_id:633993)（data-fit loss）为 $\mathcal{L}_{\text{data}}(\mathbf{w})$。带有 $L_2$ 正则化的总[目标函数](@entry_id:267263) $\mathcal{L}(\mathbf{w})$ 则定义为：

$$
\mathcal{L}(\mathbf{w}) = \mathcal{L}_{\text{data}}(\mathbf{w}) + \frac{\lambda}{2} \|\mathbf{w}\|_2^2
$$

其中，$\|\mathbf{w}\|_2^2 = \sum_j w_j^2$ 是参数向量 $\mathbf{w}$ 的[欧几里得范数](@entry_id:172687)的平方。超参数 $\lambda \ge 0$ 是**正则化强度**（regularization strength），它控制着[数据拟合](@entry_id:149007)项和惩罚项之间的平衡。当 $\lambda = 0$ 时，我们回到标准的[经验风险最小化](@entry_id:633880)。当 $\lambda \to \infty$ 时，为了最小化[目标函数](@entry_id:267263)，模型参数 $\mathbf{w}$ 将被迫趋近于零，而忽略数据拟合。因此，选择一个合适的 $\lambda$ 值至关重要。

在经典的[线性回归](@entry_id:142318) setting 中，这个概念被称为**[岭回归](@entry_id:140984)**（Ridge Regression）。给定[设计矩阵](@entry_id:165826) $X \in \mathbb{R}^{n \times p}$ 和响应向量 $y \in \mathbb{R}^n$，[岭回归](@entry_id:140984)的目标是最小化以下代价函数：

$$
\mathcal{L}(\boldsymbol{\beta}) = \|X\boldsymbol{\beta} - y\|_2^2 + \lambda \|\boldsymbol{\beta}\|_2^2
$$

这里，$\|X\boldsymbol{\beta} - y\|_2^2$ 是[残差平方和](@entry_id:174395) (RSS)，$\|\boldsymbol{\beta}\|_2^2$ 是参数向量的 $L_2$ 惩罚。这个形式在数值线性代数领域被称为标准的**[吉洪诺夫正则化](@entry_id:140094)**问题。一个普遍的[吉洪诺夫正则化](@entry_id:140094)问题旨在最小化 $\|A\mathbf{x} - \mathbf{b}\|_2^2 + \alpha \|\mathbf{x}\|_2^2$。通过直接比较，我们可以清楚地看到，[岭回归](@entry_id:140984)是[吉洪诺夫正则化](@entry_id:140094)在 $A=X$, $\mathbf{x}=\boldsymbol{\beta}$, $\mathbf{b}=y$ 且正则化参数 $\alpha = \lambda$ 时的直接应用 。这一联系揭示了 $L_2$ 正则化不仅是机器学习中的一种启发式技巧，更是一种用于解决**[不适定问题](@entry_id:182873)**（ill-posed problems）的成熟数学方法。[不适定问题](@entry_id:182873)，根据 Jacques Hadamard 的定义，是指那些解不存在、不唯一或解对数据不连续依赖（不稳定）的问题。当 $X^\top X$ 是奇[异或](@entry_id:172120)病态的（ill-conditioned），标准的[最小二乘法](@entry_id:137100)就属于[不适定问题](@entry_id:182873)。$L_2$ 正则化通过向 $X^\top X$ 添加一个[正定矩阵](@entry_id:155546) $\lambda I$ 来修正这个问题，确保了正则化后的问题总是有一个唯一的、稳定的解 。

在实际应用中，有一个重要的细节：**截距项（intercept term）通常不被正则化**。在一个包含截距项 $\beta_0$ 的模型 $y = \beta_0 + \sum_{j=1}^p \beta_j x_j + \epsilon$ 中，正则化项通常只应用于斜率系数 $\beta_1, \dots, \beta_p$。这是因为截距项 $\beta_0$ 的作用是确定模型的基线水平，它锚定了模型在特征均为零时的预测值。对它施加惩罚会迫使模型的平均预测值向零偏移，这在响应变量 $y$ 的均值不为零时是不合理的。不惩罚截距项可以保持模型对于响应变量 $y$ 的[平移等变性](@entry_id:636340)（translation equivariance），即如果将所有 $y_i$ 增加一个常数 $c$，新的最优截距项会是 $\hat{\beta}_0 + c$，而其他系数保持不变。惩罚截距项则会破坏这一理想属性 。

### 统计学视角：偏置-[方差](@entry_id:200758)权衡

$L_2$ 正则化为何能有效[防止过拟合](@entry_id:635166)？从统计学角度看，其核心机制在于对**偏置-[方差](@entry_id:200758)权衡**（bias-variance trade-off）的调整。一个模型的[泛化误差](@entry_id:637724)可以分解为偏置（bias）、[方差](@entry_id:200758)（variance）和不可约误差（irreducible error）三部分。

- **偏置**衡量了[模型平均](@entry_id:635177)预测值与真实值之间的差距，代表了模型的[表达能力](@entry_id:149863)。高偏置意味着模型过于简单，无法捕捉数据的复杂规律（[欠拟合](@entry_id:634904)）。
- **[方差](@entry_id:200758)**衡量了模型在不同训练数据集上预测结果的变动性。高[方差](@entry_id:200758)意味着模型对训练数据的微小扰动非常敏感，容易学习到噪声（过拟合）。

标准[最小二乘法](@entry_id:137100)是无偏的（在某些假设下），但当特征之间存在[共线性](@entry_id:270224)或特征数量接近样本数量时，其解的[方差](@entry_id:200758)会非常大。$L_2$ 正则化通过引入少量偏置来显著降低[估计量的方差](@entry_id:167223)，从而达到降低整体预测误差的目的。

对于岭回归，其[闭式](@entry_id:271343)解为：

$$
\hat{\boldsymbol{\beta}}_{\lambda} = (X^{\top}X + \lambda I)^{-1} X^{\top}y
$$

与标准最小二乘法解 $\hat{\boldsymbol{\beta}}_{\text{LS}} = (X^{\top}X)^{-1} X^{\top}y$ 相比，岭回归在[矩阵求逆](@entry_id:636005)之前向 $X^{\top}X$ 添加了 $\lambda I$。这个看似微小的改动，在 $X^{\top}X$ 病态或奇异时起到了稳定数值的作用。

我们可以精确地量化这一效应。假设真实模型为 $y = X \boldsymbol{\beta}^{\star} + \boldsymbol{\varepsilon}$，其中噪声 $\boldsymbol{\varepsilon}$ 满足 $\mathbb{E}[\boldsymbol{\varepsilon}] = 0$ 且 $\mathbb{E}[\boldsymbol{\varepsilon}\boldsymbol{\varepsilon}^\top] = \sigma^2 I$。[岭回归](@entry_id:140984)估计量 $\hat{\boldsymbol{\beta}}_{\lambda}$ 的期望（偏置）和协[方差](@entry_id:200758)（[方差](@entry_id:200758)）可以被推导出来。估计量的[均方误差 (MSE)](@entry_id:165831) 可以分解为平方偏置和[方差](@entry_id:200758)之和：

$$
\mathbb{E}\big[\|\hat{\boldsymbol{\beta}}_{\lambda} - \boldsymbol{\beta}^{\star}\|_2^2\big] = \|\text{Bias}(\hat{\boldsymbol{\beta}}_{\lambda})\|_2^2 + \text{Tr}(\text{Cov}(\hat{\boldsymbol{\beta}}_{\lambda}))
$$

通过对 $X^\top X$ 进行[特征分解](@entry_id:181333) $X^\top X = V S V^\top$，其中 $S = \text{diag}(s_1, \dots, s_d)$ 是[特征值](@entry_id:154894)对角阵，我们可以将 MSE 分解到数据的主成分方向上 。最终的 MSE 表达式为：

$$
\mathbb{E}\big[\|\hat{\boldsymbol{\beta}}_{\lambda} - \boldsymbol{\beta}^{\star}\|_2^2\big] = \sum_{i=1}^{d} \underbrace{\frac{\lambda^2 \alpha_i^2}{(s_i + \lambda)^2}}_{\text{Squared Bias}} + \underbrace{\frac{\sigma^2 s_i}{(s_i + \lambda)^2}}_{\text{Variance}}
$$

其中 $\alpha_i = v_i^\top \boldsymbol{\beta}^\star$ 是真实系数在第 $i$ 个特征方向上的投影。

从这个表达式中我们可以观察到：
- **偏置**：对于每个方向 $i$，平方偏置项 $\left(\frac{\lambda \alpha_i}{s_i + \lambda}\right)^2$ 随着 $\lambda$ 的增大而增大。这是因为正则化将系数向零“拉”，引入了系统性的偏差。
- **[方差](@entry_id:200758)**：[方差](@entry_id:200758)项 $\frac{\sigma^2 s_i}{(s_i + \lambda)^2}$ 随着 $\lambda$ 的增大而减小。这个效应在 $s_i$ 很小（即数据在该方向上[方差](@entry_id:200758)很小）时尤为显著。没有正则化时（$\lambda=0$），[方差](@entry_id:200758)为 $\sigma^2/s_i$，如果 $s_i \to 0$，[方差](@entry_id:200758)会爆炸。$L_2$ 正则化通过确保分母不为零来抑制这种[方差](@entry_id:200758)爆炸。

因此，$L_2$ 正则化在那些数据本身变化不大的方向上（$s_i$小）提供了最强的[方差缩减](@entry_id:145496)，而这些方向正是最容易受到噪声影响而导致[过拟合](@entry_id:139093)的地方。通过选择合适的 $\lambda$，我们可以在偏置的少量增加和[方差](@entry_id:200758)的大幅减少之间找到一个最佳[平衡点](@entry_id:272705)，从而最小化总体的[均方误差](@entry_id:175403)。

### 最优化视角：改变[损失函数](@entry_id:634569)[曲面](@entry_id:267450)

从最优化的角度看，$L_2$ 正则化通过改变损失函数的几何形状来影响学习过程。

首先，它确保了**唯一解的存在**。标准的[经验风险](@entry_id:633993)函数（如[线性回归](@entry_id:142318)的 RSS）可能是凸的但非严格凸，例如当特征数量大于样本数量或存在完美[共线性](@entry_id:270224)时，损失函数会有一个平坦的“谷底”，导致有无穷多个最优解。$L_2$ 惩罚项 $\frac{\lambda}{2}\|\mathbf{w}\|_2^2$ 是一个以原点为中心的严格凸的“碗”状函数。当它与一个凸的数据[损失函数](@entry_id:634569)相加时，得到的总[目标函数](@entry_id:267263)必定是**严格凸**的（只要 $\lambda > 0$）。严格[凸函数](@entry_id:143075)只有一个[全局最小值](@entry_id:165977)，因此优化算法可以收敛到一个唯一的解 。

其次，它改善了优化过程的**动态特性**。考虑使用梯度下降法来最小化正则化后的[目标函数](@entry_id:267263)。更新规则为：

$$
\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \eta \nabla_{\mathbf{w}} \mathcal{L}(\mathbf{w}^{(t)}) = \mathbf{w}^{(t)} - \eta (\nabla_{\mathbf{w}} \mathcal{L}_{\text{data}}(\mathbf{w}^{(t)}) + \lambda \mathbf{w}^{(t)})
$$

重新整理上式，我们得到：

$$
\mathbf{w}^{(t+1)} = (1 - \eta\lambda)\mathbf{w}^{(t)} - \eta \nabla_{\mathbf{w}} \mathcal{L}_{\text{data}}(\mathbf{w}^{(t)})
$$

这个形式直观地展示了为什么 $L_2$ 正则化被称为“[权重衰减](@entry_id:635934)”。在每一步更新中，除了沿着数据损失的负梯度方向移动外，权重向量 $\mathbf{w}^{(t)}$ 自身还会被一个小于 1 的因子 $(1-\eta\lambda)$ 所收缩。

为了更深入地理解其动态影响，我们可以考察连续时间下的[梯度流](@entry_id:635964)（gradient flow）模型，即 $\frac{d\mathbf{w}(t)}{dt} = -\nabla_{\mathbf{w}}\mathcal{L}(\mathbf{w}(t))$。对于一个二次损失函数，加入 $L_2$ 正则化后，系统演化的[微分方程](@entry_id:264184)变为一个[线性常微分方程](@entry_id:276013) 。通过对损失函数的 Hessian 矩阵进行[特征分解](@entry_id:181333)，我们可以发现，正则化项的作用是在每个特征方向上都增加了一个大小为 $\lambda$ 的衰减率。原本的收敛速率由 Hessian 矩阵的[特征值](@entry_id:154894) $\mu_i$ 决定，而正则化后的收敛速率则变为 $\mu_i + \lambda$。这意味着 $L_2$ 正则化像一个**线性阻尼器**（linear damper），它统一地加快了所有方向上的[收敛速度](@entry_id:636873)，特别是对于那些收敛缓慢的“平坦”方向（对应小的 $\mu_i$），这种加速效应尤为明显。

### 多重视角下的等价性与联系

$L_2$ 正则化不仅在统计和优化中有清晰的解释，还可以从[贝叶斯推断](@entry_id:146958)和约束优化的角度来理解，这些视角揭示了其深刻的内在联系。

#### [贝叶斯推断](@entry_id:146958)视角

在贝叶斯框架下，我们可以为模型参数 $\mathbf{w}$ 设定一个**先验分布**（prior distribution）$p(\mathbf{w})$，它反映了我们关于参数在看到数据之前的信念。$L_2$ 正则化与假设参数 $\mathbf{w}$ 服从一个均值为零、协[方差](@entry_id:200758)为球面（spherical）的[高斯先验](@entry_id:749752)是等价的：

$$
\mathbf{w} \sim \mathcal{N}(0, \tau^2 I)
$$

其中 $\tau^2$是先验[方差](@entry_id:200758)。这个先验的概率密度为 $p(\mathbf{w}) \propto \exp\left(-\frac{\|\mathbf{w}\|_2^2}{2\tau^2}\right)$。

根据贝叶斯定理，参数的[后验分布](@entry_id:145605) $p(\mathbf{w} | \mathcal{D})$ 正比于似然 $p(\mathcal{D} | \mathbf{w})$ 和先验 $p(\mathbf{w})$ 的乘积。在对数空间中，最大化[后验概率](@entry_id:153467)（MAP）等价于最小化负对数后验：

$$
\mathbf{w}_{\text{MAP}} = \arg\max_{\mathbf{w}} p(\mathbf{w} | \mathcal{D}) = \arg\min_{\mathbf{w}} [-\ln p(\mathcal{D} | \mathbf{w}) - \ln p(\mathbf{w})]
$$

如果假设数据噪声是高斯的，那么[负对数似然](@entry_id:637801) $-\ln p(\mathcal{D} | \mathbf{w})$ 就对应于数据损失项 $\mathcal{L}_{\text{data}}$（例如，[残差平方和](@entry_id:174395)）。负对数先验则是 $-\ln p(\mathbf{w}) = \text{const} + \frac{\|\mathbf{w}\|_2^2}{2\tau^2}$。因此，MAP 估计的目标函数变为：

$$
\mathcal{L}_{\text{MAP}}(\mathbf{w}) = \mathcal{L}_{\text{data}}(\mathbf{w}) + \frac{1}{2\tau^2} \|\mathbf{w}\|_2^2
$$

这与 $L_2$ 正则化的目标函数形式完全一致，只需令正则化强度 $\lambda = \frac{1}{\tau^2}$。这意味着，$L_2$ 正则化可以被看作是对参数施加了一个“偏爱小范数值”的[先验信念](@entry_id:264565)。$\lambda$ 越大，意味着我们相信参数的先验[方差](@entry_id:200758) $\tau^2$ 越小，从而后验估计被更强地拉向原点。这种联系也为理解正则化参数提供了新的视角：$\lambda$ 控制了[后验分布](@entry_id:145605)的不确定性。更大的 $\lambda$（即更强的先验）会导致更窄的[后验分布](@entry_id:145605)和更窄的可信区间（credible intervals），表示模型对参数的估计更加确定 。

#### [约束优化](@entry_id:635027)视角

$L_2$ 正则化问题在形式上是一个无约束的[优化问题](@entry_id:266749)。然而，通过**[拉格朗日对偶](@entry_id:638042)**（Lagrangian duality），可以证明它等价于一个有约束的[优化问题](@entry_id:266749) ：

$$
\min_{\mathbf{w}} \mathcal{L}_{\text{data}}(\mathbf{w}) \quad \text{subject to} \quad \|\mathbf{w}\|_2^2 \le C
$$

这里，$C$ 是一个正常数，定义了参数允许存在的半径为 $\sqrt{C}$ 的欧几里得球空间。这两个问题是等价的，意味着对于任意一个 $\lambda > 0$，都存在一个对应的 $C \ge 0$，使得两个问题的解相同；反之亦然。具体来说，$\lambda$ 和 $C$ 之间存在一种单调递减的反比关系：正则化强度 $\lambda$ 越大，等价的约束半径 $C$ 就越小，对参数的限制就越强。

这种等价性可以通过 [Karush-Kuhn-Tucker (KKT) 条件](@entry_id:176491)来建立。约束问题的[拉格朗日函数](@entry_id:174593)为 $\mathcal{L}(\mathbf{w}, \lambda) = \mathcal{L}_{\text{data}}(\mathbf{w}) + \frac{\lambda}{2} (\|\mathbf{w}\|_2^2 - C)$。其关于 $\mathbf{w}$ 的梯度为零的条件 $\nabla \mathcal{L}_{\text{data}}(\mathbf{w}) + \lambda \mathbf{w} = 0$，这与 $L_2$ 正则化问题的[最优性条件](@entry_id:634091)完全相同。

这个视角提供了一个直观的几何解释：$L_2$ 正则化是在数据损失函数的[等高线图](@entry_id:178003)上，寻找与一个以原点为中心的球（$\|\mathbf{w}\|_2^2 = C$）相切的那个点。如果无约束的最优解本身就在球内，那么正则化不起作用（对应 $\lambda=0$）。如果最优解在球外，那么正则化问题的解就会落在球的边界上。

### 实际应用中的考量与进阶主题

#### 对噪声的鲁棒性

$L_2$ 正则化不仅能通过控制[模型复杂度](@entry_id:145563)来提升泛化能力，还能直接增强模型对数据噪声（尤其是[标签噪声](@entry_id:636605)）的**鲁棒性**（robustness）。当训练集中存在错误标签时，一个没有正则化的模型可能会竭力调整其参数以拟合这些错误点，导致决策边界出现不自然的扭曲。

$L_2$ 正则化通过“拉平”损失函数的[曲面](@entry_id:267450)来对抗这种影响。我们可以通过分析模型预测对单个标签扰动的敏感度来量化这一点。对于一个逻辑回归模型，可以推导出，在给定测试点上的预测概率对某个训练点标签扰动的敏感度，与正则化后的 Hessian [矩阵的逆](@entry_id:140380) $H^{-1} = (\sum_i \nabla^2\ell_i + \lambda I)^{-1}$ 成正比 。增大 $\lambda$ 会使得 $H$ 的所有[特征值](@entry_id:154894)增大，从而使其[逆矩阵](@entry_id:140380) $H^{-1}$ 的范数减小。这意味着，当 $\lambda$ 更大时，单个训练点（包括可能是噪声点的）对模型参数和最终预测的影响力被削弱了。模型变得更加“迟钝”，不会轻易地被个别数据点“带偏”，从而学习到更平滑、更鲁棒的决策边界。

#### 对数据尺度的敏感性

一个重要的实践要点是，$L_2$ 正则化的效果对输入特征的尺度非常敏感。考虑一个简单的线性模型，如果我们把所有输入特征 $X$ 缩放一个因子 $\alpha$，即 $X' = \alpha X$。为了保持模型的预测函数不变，权重需要进行反向缩放 $w' = w/\alpha$。在这种情况下，新的正则化项变为 $\frac{\lambda}{2}\|w'\|_2^2 = \frac{\lambda}{2\alpha^2}\|w\|_2^2$。为了使正则化的“有效强度”保持不变，原先的正则化参数 $\lambda$ 需要被调整为 $\lambda' = \alpha^2 \lambda$ 。

这个依赖关系意味着，如果不对特征进行标准化，那些[数值范围](@entry_id:752817)较大的特征将在正则化项中占据主导地位，其对应的权重会受到更强的惩罚，而这并非我们所期望的。因此，在应用 $L_2$ 正则化之前，**[标准化](@entry_id:637219)输入特征**（例如，使其均值为0，[方差](@entry_id:200758)为1）是一个标准的、强烈推荐的[预处理](@entry_id:141204)步骤。这确保了正则化惩罚能公平地施加于所有特征，使得 $\lambda$ 的选择更加稳健，并且其值可以在不同问题之间更具可比性。

#### 在现代深度网络中的应用

在深度神经网络中，$L_2$ 正则化（即[权重衰减](@entry_id:635934)）的应用更加微妙，特别是当网络中包含**[批量归一化](@entry_id:634986)**（Batch Normalization, BN）层时。BN 层通过对每一批数据的激活值进行归一化（减去均值，除以标准差），然后进行仿射变换（乘以可学习的 $\gamma$ 并加上 $\beta$），从而稳定了训练过程。

然而，BN 引入了新的**尺度不变性**（scale invariance）。考虑一个线性层 $z = Wx$ 紧跟着一个 BN 层。如果我们把权重矩阵 $W$ 缩放一个常数 $c>0$，变为 $W' = cW$，那么其输出 $z' = cWx$ 的均值和[标准差](@entry_id:153618)也会相应地缩放 $c$ 倍。BN 层在归一化时会完全抵消掉这个缩放因子，使得归一化后的激活 $\hat{z}$ 保持不变。因此，网络的最终输出函数对于 $W$ 的尺度是不变的。

在这种情况下，对权重 $W$ 施加 $L_2$ 正则化（$\frac{\lambda}{2}\|W\|_F^2$）会产生意想不到的后果。由于[损失函数](@entry_id:634569)的数据项对 $W$ 的尺度不敏感，而正则化项却随 $\|W\|_F^2$ 增大而增大，优化器会倾向于将 $W$ 的范数尽可能地缩小以减小总损失，这并不能有效正则化模型函数本身。

一个更具原则性的方法是，只对那些其范数与模型函数复杂度直接相关的参数进行[权重衰减](@entry_id:635934) 。基于这一原则，一个普遍的现代实践是：
- **不衰减**任何偏置项（biases），包括 BN 层的 $\beta$ 参数。
- **不衰减**任何与[尺度不变性](@entry_id:180291)相关的参数，例如 BN 层的 $\gamma$ 参数，以及紧邻 BN 层之前的权重矩阵 $W$。
- **衰减**那些不具备尺度不变性的权重参数，例如网络最后一层的权重。

这种选择性地应用[权重衰减](@entry_id:635934)的策略，确保了正则化真正作用于约束模型的函数复杂度，而不是在等价的[参数表示](@entry_id:173803)中惩罚任意选择的范数。