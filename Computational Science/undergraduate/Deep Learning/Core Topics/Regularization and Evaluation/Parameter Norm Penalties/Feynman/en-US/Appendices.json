{
    "hands_on_practices": [
        {
            "introduction": "To build a strong foundation, we begin by exploring the distinct behaviors of the most common parameter norm penalties. This practice provides a direct, hands-on comparison of the sparsity-inducing $L_1$ norm (LASSO), the general shrinkage effect of the $L_2$ norm (Ridge), and an approximation of the ideal but computationally difficult $L_0$ \"norm\". By implementing and evaluating these methods on a controlled synthetic dataset, you will gain concrete intuition about their respective strengths in feature selection and model complexity control, observing firsthand how $L_2$ penalties can sometimes underfit when true signals are sparse .",
            "id": "3161377",
            "problem": "You are asked to implement, analyze, and compare parameter norm penalties in a sparse linear regression setting using a fully reproducible computational experiment. The focus is on demonstrating when an $L_2$ (ridge) penalty can cause underfitting in the presence of sparsity, contrasting it with $L_0$ approximations via hard thresholding and with the tractability of the $L_1$ (Least Absolute Shrinkage and Selection Operator (LASSO)) penalty.\n\nGiven a dataset generated from a linear model with a sparse ground-truth parameter vector, you must construct estimators based on three different parameter norm penalties and quantitatively compare their empirical performance across several test cases.\n\nBase definitions and facts you must use:\n- The empirical risk minimization objective for linear regression with squared loss is to minimize $f(\\mathbf{w}) = \\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\rVert_2^2$ over $\\mathbf{w} \\in \\mathbb{R}^d$, where $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$, $\\mathbf{y} \\in \\mathbb{R}^n$, and $n, d \\in \\mathbb{N}$.\n- The gradient of the squared loss is $\\nabla f(\\mathbf{w}) = -\\frac{1}{n}\\mathbf{X}^\\top(\\mathbf{y} - \\mathbf{X}\\mathbf{w})$.\n- The Lipschitz constant of the gradient is the largest eigenvalue of $\\frac{1}{n}\\mathbf{X}^\\top\\mathbf{X}$.\n- The $L_2$-penalized problem (ridge) adds $\\frac{\\lambda_2}{2}\\lVert \\mathbf{w}\\rVert_2^2$ to the objective, $\\lambda_2 \\in \\mathbb{R}_{\\ge 0}$.\n- The $L_1$-penalized problem (LASSO) adds $\\lambda_1\\lVert \\mathbf{w}\\rVert_1$, $\\lambda_1 \\in \\mathbb{R}_{\\ge 0}$, and can be efficiently optimized with coordinate descent using soft-thresholding derived from subgradient optimality.\n- The $L_0$-constrained problem $\\min f(\\mathbf{w})$ subject to $\\lVert \\mathbf{w}\\rVert_0 \\le s$ is non-convex and combinatorial; an approximation strategy is Iterative Hard Thresholding (IHT), which uses gradient descent steps followed by a hard-thresholding operator that keeps the $s$ largest-magnitude entries.\n\nData generation protocol (must be used verbatim):\n- For each test case, generate $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ with independent standard normal entries. Normalize each column $\\mathbf{x}_j$ to satisfy $\\frac{1}{n}\\lVert \\mathbf{x}_j\\rVert_2^2 = 1$. Generate a $k$-sparse ground-truth vector $\\mathbf{w}^\\star \\in \\mathbb{R}^d$ by selecting $k$ distinct indices uniformly at random and assigning nonzero values drawn from a zero-mean normal distribution, then enforcing a minimum absolute magnitude of $1.5$ by increasing any smaller magnitude to exactly $1.5$ with the same sign. Generate the response $\\mathbf{y} = \\mathbf{X}\\mathbf{w}^\\star + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)$.\n\nEstimators to implement:\n- Ridge ($L_2$) estimator by minimizing the penalized empirical risk with penalty weight $\\lambda_2$.\n- $L_0$ approximation via Iterative Hard Thresholding (IHT): use the exact gradient above, a constant step size strictly less than the reciprocal Lipschitz constant, and hard-thresholding to keep exactly $s$ entries at each iteration. Use a fixed number of iterations.\n- LASSO ($L_1$) estimator via cyclic coordinate descent with soft-thresholding updates. Use a fixed iteration budget and terminate early on convergence based on a small change tolerance.\n\nPerformance metrics to compute for each method in each test case:\n- Training mean squared error, defined as $\\mathrm{MSE} = \\frac{1}{n}\\lVert \\mathbf{y} - \\mathbf{X}\\hat{\\mathbf{w}}\\rVert_2^2$.\n- Support recall, defined as $r = \\frac{\\lvert \\widehat{S} \\cap S^\\star \\rvert}{k}$, where $S^\\star = \\{ j : w^\\star_j \\ne 0 \\}$ and $\\widehat{S}$ is the estimated support. Let $\\widehat{S}$ be defined as follows: for ridge, the indices of the $k$ largest $| \\hat{w}_j |$; for IHT, the nonzero indices after hard thresholding; for LASSO, the indices with $|\\hat{w}_j| > \\tau$ where $\\tau = 10^{-8}$.\n\nTest suite:\nFor each tuple $(n, d, k, \\sigma, \\lambda_2, \\lambda_1, s, \\text{seed})$, construct a dataset as specified and then compute the metrics for the three methods. Use the following four test cases that explore a typical setting, strong $L_2$ underfitting, a zero-noise boundary, and a sparsity mis-specification:\n- Case A: $(n, d, k, \\sigma, \\lambda_2, \\lambda_1, s, \\text{seed}) = (\\,80,\\, 100,\\, 5,\\, 0.05,\\, 10.0,\\, 0.05,\\, 5,\\, 1\\,)$.\n- Case B: $(\\,80,\\, 100,\\, 5,\\, 0.10,\\, 100.0,\\, 0.10,\\, 5,\\, 2\\,)$.\n- Case C: $(\\,80,\\, 100,\\, 5,\\, 0.00,\\, 10.0,\\, 0.01,\\, 5,\\, 3\\,)$.\n- Case D: $(\\,80,\\, 100,\\, 5,\\, 0.05,\\, 10.0,\\, 0.05,\\, 3,\\, 4\\,)$.\n\nOutput requirements:\n- For each case, output a list of $6$ floats in the order $[ \\mathrm{MSE}_{\\mathrm{ridge}}, \\mathrm{MSE}_{\\mathrm{IHT}}, \\mathrm{MSE}_{\\mathrm{LASSO}}, r_{\\mathrm{ridge}}, r_{\\mathrm{IHT}}, r_{\\mathrm{LASSO}} ]$, each rounded to exactly $4$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of these per-case lists, with no spaces anywhere in the line. For example: \"[[a1,a2,a3,a4,a5,a6],[b1,b2,b3,b4,b5,b6],[c1,c2,c3,c4,c5,c6],[d1,d2,d3,d4,d5,d6]]\".\n- No physical units are involved. All angles, if any appear, should be in radians. All values must be numeric.\n\nYour implementation must be deterministic given the provided seeds, use only the specified libraries, and follow the exact dataset construction and metric definitions above.",
            "solution": "The present task is to conduct a computational experiment comparing three distinct parameter norm penalties for sparse linear regression: $L_2$ (Ridge), $L_1$ (LASSO), and an approximation to $L_0$ (Iterative Hard Thresholding). The analysis will be performed on a synthetic dataset generated according to a specified protocol, with the goal of evaluating each method's ability to recover a sparse ground-truth parameter vector $\\mathbf{w}^\\star$.\n\nThe underlying model is a linear relationship $\\mathbf{y} = \\mathbf{X}\\mathbf{w} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ is the feature matrix, $\\mathbf{w} \\in \\mathbb{R}^d$ is the parameter vector, $\\mathbf{y} \\in \\mathbb{R}^n$ is the response vector, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n$ is a noise term. The empirical risk to be minimized is the mean squared error:\n$$f(\\mathbf{w}) = \\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\rVert_2^2$$\nRegularization is introduced by adding a penalty term to this objective function. We will implement and analyze three such regularizers.\n\n### Data Generation and Evaluation Metrics\n\nFor each test case, a dataset is synthesized using a fixed procedure to ensure reproducibility.\n1.  A design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ is generated with entries drawn independently from a standard normal distribution, $\\mathcal{N}(0, 1)$. Each column $\\mathbf{x}_j$ is then normalized such that its squared $L_2$ norm, scaled by $1/n$, is unity: $\\frac{1}{n}\\lVert \\mathbf{x}_j\\rVert_2^2 = 1$.\n2.  A $k$-sparse ground-truth vector $\\mathbf{w}^\\star \\in \\mathbb{R}^d$ is constructed. $k$ support indices are chosen uniformly at random. The values at these indices are drawn from $\\mathcal{N}(0, 1)$, and any value with an absolute magnitude less than $1.5$ is set to have a magnitude of exactly $1.5$ while preserving its sign. All other entries of $\\mathbf{w}^\\star$ are zero.\n3.  The response vector $\\mathbf{y}$ is generated as $\\mathbf{y} = \\mathbf{X}\\mathbf{w}^\\star + \\boldsymbol{\\varepsilon}$, where the noise $\\boldsymbol{\\varepsilon}$ is drawn from a multivariate normal distribution $\\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_n)$.\n\nThe performance of each estimator $\\hat{\\mathbf{w}}$ is quantified by two metrics:\n1.  **Training Mean Squared Error (MSE):** $\\mathrm{MSE} = \\frac{1}{n}\\lVert \\mathbf{y} - \\mathbf{X}\\hat{\\mathbf{w}}\\rVert_2^2$.\n2.  **Support Recall (r):** $r = \\frac{\\lvert \\widehat{S} \\cap S^\\star \\rvert}{k}$, where $S^\\star = \\{j \\mid w^\\star_j \\neq 0\\}$ is the true support and $\\widehat{S}$ is the estimated support.\n\n### Estimator Implementation\n\n**1. Ridge Regression ($L_2$ Penalty)**\n\nRidge regression adds a squared $L_2$ norm penalty to the objective function, which helps to prevent overfitting by shrinking the parameter estimates towards zero. The optimization problem is:\n$$\\min_{\\mathbf{w}} \\left\\{ \\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\rVert_2^2 + \\frac{\\lambda_2}{2}\\lVert \\mathbf{w}\\rVert_2^2 \\right\\}$$\nThis objective function is convex and continuously differentiable. Setting its gradient with respect to $\\mathbf{w}$ to zero yields a closed-form solution:\n$$\\nabla_{\\mathbf{w}} \\left( \\frac{1}{2n}(\\mathbf{y} - \\mathbf{X}\\mathbf{w})^\\top(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + \\frac{\\lambda_2}{2}\\mathbf{w}^\\top\\mathbf{w} \\right) = -\\frac{1}{n}\\mathbf{X}^\\top(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + \\lambda_2\\mathbf{w} = \\mathbf{0}$$\n$$(\\mathbf{X}^\\top\\mathbf{X} + n\\lambda_2\\mathbf{I})\\mathbf{w} = \\mathbf{X}^\\top\\mathbf{y}$$\n$$\\hat{\\mathbf{w}}_{\\text{ridge}} = (\\mathbf{X}^\\top\\mathbf{X} + n\\lambda_2\\mathbf{I})^{-1}\\mathbf{X}^\\top\\mathbf{y}$$\nThis linear system is solved numerically. The $L_2$ penalty produces dense solutions (i.e., most entries of $\\hat{\\mathbf{w}}_{\\text{ridge}}$ are non-zero), so the support $\\widehat{S}$ is estimated by selecting the indices of the $k$ coefficients with the largest absolute values.\n\n**2. Iterative Hard Thresholding (IHT, $L_0$ Approximation)**\n\nThe $L_0$ \"norm\", $\\lVert\\mathbf{w}\\rVert_0$, counts the number of non-zero elements in $\\mathbf{w}$. The $L_0$-constrained problem aims to find the best-fitting model with at most $s$ non-zero parameters:\n$$\\min_{\\mathbf{w}} \\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\rVert_2^2 \\quad \\text{subject to} \\quad \\lVert \\mathbf{w}\\rVert_0 \\le s$$\nThis problem is NP-hard. IHT is an iterative algorithm that provides an approximate solution. It alternates between a standard gradient descent step and a hard thresholding step. The update rule is:\n$$\\mathbf{w}_{t+1} = H_s(\\mathbf{w}_t - \\alpha \\nabla f(\\mathbf{w}_t))$$\nwhere $\\nabla f(\\mathbf{w}_t) = -\\frac{1}{n}\\mathbf{X}^\\top(\\mathbf{y} - \\mathbf{X}\\mathbf{w}_t)$ is the gradient of the loss. The operator $H_s(\\cdot)$ is the hard-thresholding projection, which keeps the $s$ elements of its vector argument with the largest magnitudes and sets all others to zero. The step size $\\alpha$ must be chosen to ensure convergence. A valid choice is $\\alpha < 1/L$, where $L$ is the Lipschitz constant of the gradient, given by the largest eigenvalue of $\\frac{1}{n}\\mathbf{X}^\\top\\mathbf{X}$. We use a step size $\\alpha = 0.99/L$ and a fixed number of $100$ iterations. The support $\\widehat{S}$ is naturally defined as the set of non-zero indices in the final estimate $\\hat{\\mathbf{w}}_{\\text{IHT}}$.\n\n**3. LASSO (Least Absolute Shrinkage and Selection Operator, $L_1$ Penalty)**\n\nThe LASSO adds an $L_1$ norm penalty, which is known for inducing sparsity in the solution. The optimization problem is:\n$$\\min_{\\mathbf{w}} \\left\\{ \\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\mathbf{w} \\rVert_2^2 + \\lambda_1\\lVert \\mathbf{w}\\rVert_1 \\right\\}$$\nwhere $\\lVert \\mathbf{w}\\rVert_1 = \\sum_j |w_j|$. The objective is convex but non-differentiable at points where any $w_j=0$. We use cyclic coordinate descent to solve this problem. For each coordinate $j$, we fix all other coordinates and solve the one-dimensional problem for $w_j$. This yields the soft-thresholding update rule. Given the column normalization $\\frac{1}{n}\\lVert \\mathbf{x}_j \\rVert_2^2 = 1$, the update for coordinate $j$ is:\n$$w_j \\leftarrow S_{\\lambda_1}(\\rho_j)$$\nwhere $S_a(z) = \\text{sign}(z)\\max(|z|-a, 0)$ is the soft-thresholding operator, and $\\rho_j = \\frac{1}{n}\\mathbf{x}_j^\\top(\\mathbf{y} - \\sum_{i \\neq j}\\mathbf{x}_i w_i)$. To implement this efficiently, we pre-calculate $\\mathbf{X}^\\top\\mathbf{X}$ and $\\mathbf{X}^\\top\\mathbf{y}$. The algorithm iterates through all coordinates cyclically until the change in $\\mathbf{w}$ between iterations is below a tolerance ($10^{-6}$) or a maximum number of iterations ($1000$) is reached. The support $\\widehat{S}$ is taken to be the set of indices where $|\\hat{w}_j| > 10^{-8}$.\n\nThe selected test cases will illustrate specific behaviors: Case A is a standard setting; Case B uses a very high $\\lambda_2$ to demonstrate how Ridge can oversmooth and \"underfit\"; Case C has zero noise, where sparse methods should excel; Case D shows the effect of misspecifying the sparsity level $s$ for IHT.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares Ridge, IHT, and LASSO for sparse linear regression.\n    \"\"\"\n\n    test_cases = [\n        # (n, d, k, sigma, lambda2, lambda1, s, seed)\n        (80, 100, 5, 0.05, 10.0, 0.05, 5, 1), # Case A\n        (80, 100, 5, 0.10, 100.0, 0.10, 5, 2), # Case B\n        (80, 100, 5, 0.00, 10.0, 0.01, 5, 3), # Case C\n        (80, 100, 5, 0.05, 10.0, 0.05, 3, 4), # Case D\n    ]\n    \n    all_results = []\n\n    def generate_data(n, d, k, sigma, seed):\n        rng = np.random.default_rng(seed)\n        X = rng.standard_normal(size=(n, d))\n        \n        # Normalize columns\n        X_col_norms = np.linalg.norm(X, axis=0)\n        scaling_factors = np.sqrt(n) / X_col_norms\n        X = X * scaling_factors\n        \n        w_star = np.zeros(d)\n        support_indices = rng.choice(d, k, replace=False)\n        w_star[support_indices] = rng.standard_normal(k)\n        \n        # Enforce minimum magnitude\n        low_mag_mask = (w_star != 0) & (np.abs(w_star) < 1.5)\n        w_star[low_mag_mask] = np.sign(w_star[low_mag_mask]) * 1.5\n        \n        epsilon = rng.normal(loc=0, scale=sigma, size=n)\n        y = X @ w_star + epsilon\n        \n        return X, y, w_star, support_indices\n\n    def calculate_mse(X, y, w):\n        n = X.shape[0]\n        return np.sum((y - X @ w)**2) / n\n\n    def train_ridge(X, y, lambda2, k, true_support):\n        n, d = X.shape\n        XtX = X.T @ X\n        Xty = X.T @ y\n        identity = np.eye(d)\n        \n        w_ridge = np.linalg.solve(XtX + n * lambda2 * identity, Xty)\n        \n        estimated_support = np.argsort(np.abs(w_ridge))[-k:]\n        true_positives = len(np.intersect1d(estimated_support, true_support))\n        recall = true_positives / k\n        \n        mse = calculate_mse(X, y, w_ridge)\n        return mse, recall\n\n    def train_iht(X, y, s, k, true_support, n_iter=100):\n        n, d = X.shape\n        L = np.max(np.linalg.eigvalsh((X.T @ X) / n))\n        alpha = 0.99 / L\n        \n        w = np.zeros(d)\n\n        for _ in range(n_iter):\n            grad = - (X.T @ (y - X @ w)) / n\n            w_temp = w - alpha * grad\n            \n            # Hard thresholding\n            indices_to_keep = np.argsort(np.abs(w_temp))[-s:]\n            w = np.zeros(d)\n            w[indices_to_keep] = w_temp[indices_to_keep]\n\n        estimated_support = np.where(w != 0)[0]\n        true_positives = len(np.intersect1d(estimated_support, true_support))\n        recall = true_positives / k\n        \n        mse = calculate_mse(X, y, w)\n        return mse, recall\n\n    def train_lasso(X, y, lambda1, k, true_support, max_iter=1000, tol=1e-6):\n        n, d = X.shape\n        w = np.zeros(d)\n        \n        XtX = X.T @ X\n        Xty = X.T @ y\n        \n        for i in range(max_iter):\n            w_old = w.copy()\n            for j in range(d):\n                # Using pre-computed matrices for efficiency\n                rho_j_num = Xty[j] - (np.dot(XtX[j, :], w) - XtX[j, j] * w[j])\n                rho_j = rho_j_num / n\n                \n                # Soft-thresholding\n                w[j] = np.sign(rho_j) * np.maximum(np.abs(rho_j) - lambda1, 0)\n            \n            if np.max(np.abs(w - w_old)) < tol:\n                break\n        \n        tau = 1e-8\n        estimated_support = np.where(np.abs(w) > tau)[0]\n        true_positives = len(np.intersect1d(estimated_support, true_support))\n        recall = true_positives / k\n        \n        mse = calculate_mse(X, y, w)\n        return mse, recall\n\n\n    for case in test_cases:\n        n, d, k, sigma, lambda2, lambda1, s, seed = case\n        \n        X, y, w_star, true_support = generate_data(n, d, k, sigma, seed)\n        \n        mse_ridge, recall_ridge = train_ridge(X, y, lambda2, k, true_support)\n        mse_iht, recall_iht = train_iht(X, y, s, k, true_support)\n        mse_lasso, recall_lasso = train_lasso(X, y, lambda1, k, true_support)\n        \n        case_results = [\n            round(mse_ridge, 4),\n            round(mse_iht, 4),\n            round(mse_lasso, 4),\n            round(recall_ridge, 4),\n            round(recall_iht, 4),\n            round(recall_lasso, 4)\n        ]\n        all_results.append(case_results)\n\n    case_strings = []\n    for case_res in all_results:\n        case_strings.append(f\"[{','.join(map(str, case_res))}]\")\n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from shallow to deeper models, we investigate how simple penalties can induce complex structural properties in the learned function. This exercise demonstrates a profound concept from deep learning theory: applying a standard $L_2$ penalty to the weights of a two-layer linear network is equivalent to penalizing the nuclear norm of the overall linear mapping, which encourages a low-rank solution . By deriving this relationship and implementing an algorithm to compute the resulting rank, you will see how explicit regularization on parameters can lead to powerful implicit regularization on the function the network computes.",
            "id": "3161416",
            "problem": "Consider a two-layer linear network with input dimension $d$, output dimension $p$, and hidden width $h$. The parameters are a pair of matrices $(W_2, W_1)$ with $W_1 \\in \\mathbb{R}^{h \\times d}$ and $W_2 \\in \\mathbb{R}^{p \\times h}$. The network computes a linear mapping $M = W_2 W_1 \\in \\mathbb{R}^{p \\times d}$. Suppose we aim to approximate a target linear mapping $T \\in \\mathbb{R}^{p \\times d}$ by minimizing a regularized empirical risk built from fundamental definitions: the squared loss and Euclidean norm. Specifically, for a fixed regularization coefficient $\\alpha \\ge 0$, consider the objective\n$$\n\\mathcal{L}(W_2, W_1) = \\frac{1}{2} \\lVert W_2 W_1 - T \\rVert_F^2 + \\frac{\\alpha}{2}\\left(\\lVert W_1 \\rVert_F^2 + \\lVert W_2 \\rVert_F^2\\right),\n$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm. The network may be overparameterized in the sense $h \\ge \\min\\{p,d\\}$, yet the choice of regularization might induce an effectively under-parameterized mapping $M$ with reduced rank.\n\nStarting only from core definitions and well-tested facts, derive a complete characterization of the optimal effective mapping $M^\\star$ that minimizes the above objective. Your derivation should begin with the definitions of the Frobenius norm, the nuclear norm, and the Singular Value Decomposition (SVD) and proceed to show how the Euclidean norm penalties on $(W_2, W_1)$ induce an implicit rank reduction in the optimal $M^\\star$ even when $h$ makes the parameterization overcomplete. Then, design an algorithm that computes the singular values of $M^\\star$ directly from those of $T$ and returns the rank of $M^\\star$. Also, adapt your characterization and algorithm to the case where $h$ is finite and acts as a hard cap on the rank of $M^\\star$, and explain how this cap interacts with regularization to determine the rank.\n\nYour program must implement this algorithm and apply it to the following test suite. In all cases, take $p=d=5$ and let $T$ be diagonal with the specified singular values $\\sigma_1, \\dots, \\sigma_5$ on the diagonal (so that its Singular Value Decomposition uses identity singular vectors). For each case, compute the integer rank of the optimal $M^\\star$ that minimizes $\\mathcal{L}(W_2, W_1)$ under the hidden width $h$ and regularization strength $\\alpha$.\n\nTest cases:\n- Case 1 (happy path, no regularization): $h=10$, $\\alpha=0.0$, $\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$.\n- Case 2 (moderate regularization leading to rank drop): $h=10$, $\\alpha=1.0$, $\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$.\n- Case 3 (lighter regularization): $h=10$, $\\alpha=0.7$, $\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$.\n- Case 4 (strong regularization eliminating all singular values): $h=10$, $\\alpha=3.5$, $\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$.\n- Case 5 (width-limited edge case where $h$ binds): $h=2$, $\\alpha=0.7$, $\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$.\n- Case 6 (zero target mapping): $h=10$, $\\alpha=0.5$, $\\sigma = [0.0, 0.0, 0.0, 0.0, 0.0]$.\n\nYour program should produce a single line of output containing the ranks for the cases in order as a comma-separated list enclosed in square brackets, for example, \"[r1,r2,r3,r4,r5,r6]\". No physical units or angle units are involved, and all outputs are integers. The solution must be fully self-contained and runnable in any modern programming language; however, the final answer must be provided as a Python program as specified below.",
            "solution": "The user wants a complete characterization and an algorithmic solution for finding the rank of an optimal linear mapping produced by a two-layer network with parameter-norm penalties.\n\n### Problem Validation\n\nFirst, I will validate the problem statement.\n\n#### Step 1: Extract Givens\n-   **Network Structure**: A two-layer linear network defined by the mapping $M = W_2 W_1$, where $W_1 \\in \\mathbb{R}^{h \\times d}$ and $W_2 \\in \\mathbb{R}^{p \\times h}$. The dimensions are input $d$, output $p$, and hidden width $h$.\n-   **Target**: A target linear mapping $T \\in \\mathbb{R}^{p \\times d}$.\n-   **Objective Function**: The regularized empirical risk to be minimized is\n    $$\n    \\mathcal{L}(W_2, W_1) = \\frac{1}{2} \\lVert W_2 W_1 - T \\rVert_F^2 + \\frac{\\alpha}{2}\\left(\\lVert W_1 \\rVert_F^2 + \\lVert W_2 \\rVert_F^2\\right)\n    $$\n    for a regularization coefficient $\\alpha \\ge 0$.\n-   **Norms**: $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm.\n-   **Constraints**: The width $h$ imposes a hard constraint on the rank of the mapping $M$, as $\\text{rank}(M) = \\text{rank}(W_2 W_1) \\le \\min(\\text{rank}(W_2), \\text{rank}(W_1)) \\le h$.\n-   **Task**: Characterize the optimal mapping $M^\\star$ that arises from minimizing $\\mathcal{L}(W_2, W_1)$, design an algorithm to compute its rank, and apply it to several test cases.\n-   **Test Data**: For all cases, $p=d=5$ and $T$ is a diagonal matrix with specified singular values $\\sigma$.\n    -   Case 1: $h=10$, $\\alpha=0.0$, $\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$.\n    -   Case 2: $h=10$, $\\alpha=1.0$, $\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$.\n    -   Case 3: $h=10$, $\\alpha=0.7$, $\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$.\n    -   Case 4: $h=10$, $\\alpha=3.5$, $\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$.\n    -   Case 5: $h=2$, $\\alpha=0.7$, $\\sigma = [3.0, 2.5, 1.0, 0.8, 0.2]$.\n    -   Case 6: $h=10$, $\\alpha=0.5$, $\\sigma = [0.0, 0.0, 0.0, 0.0, 0.0]$.\n\n#### Step 2: Validate Using Extracted Givens\nThe problem is well-defined and scientifically grounded. It addresses a fundamental question in the theory of deep learning: the nature of implicit regularization induced by explicit parameter norm penalties.\n-   **Scientific Soundness**: The problem is formulated using standard, well-established concepts from linear algebra and optimization, including the Frobenius norm, nuclear norm, and Singular Value Decomposition (SVD). The analysis of such loss functions is a common topic in machine learning research.\n-   **Well-Posedness**: The objective function, once re-parameterized in terms of the effective mapping $M$, can be shown to be a convex optimization problem, for which a unique minimizer $M^\\star$ exists. The problem is therefore well-posed.\n-   **Objectivity**: The language is precise and mathematical, free of ambiguity or subjective claims.\n\nThe problem passes all validation criteria. It is a valid, substantive problem in theoretical machine learning.\n\n#### Step 3: Verdict and Action\nThe problem is valid. I will now proceed to the solution.\n\n### Derivation of the Optimal Mapping $M^\\star$\n\nThe core of the problem is to understand how the regularization term $\\frac{\\alpha}{2}(\\lVert W_1 \\rVert_F^2 + \\lVert W_2 \\rVert_F^2)$ influences the properties of the product matrix $M = W_2 W_1$. We will show that this form of regularization is equivalent to penalizing the nuclear norm of $M$.\n\n#### Fundamental Definitions\n1.  **Frobenius Norm**: For a matrix $A \\in \\mathbb{R}^{m \\times n}$ with entries $a_{ij}$, the Frobenius norm is $\\lVert A \\rVert_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n a_{ij}^2} = \\sqrt{\\text{Tr}(A^T A)}$.\n2.  **Singular Value Decomposition (SVD)**: Any matrix $A \\in \\mathbb{R}^{m \\times n}$ can be decomposed as $A = U \\Sigma V^T$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix with non-negative entries $\\sigma_i(A)$ on its diagonal, called the singular values.\n3.  **Nuclear Norm**: The nuclear norm (or trace norm) of a matrix $A$ is the sum of its singular values: $\\lVert A \\rVert_* = \\sum_i \\sigma_i(A) = \\text{Tr}(\\sqrt{A^T A})$.\n\n#### Reformulating the Objective Function\nThe objective function is\n$$\n\\mathcal{L}(W_2, W_1) = \\frac{1}{2} \\lVert M - T \\rVert_F^2 + \\frac{\\alpha}{2}\\left(\\lVert W_1 \\rVert_F^2 + \\lVert W_2 \\rVert_F^2\\right),\n$$\nwhere $M = W_2 W_1$. The optimization is over the parameters $(W_2, W_1)$, which is a non-convex problem. However, we can rephrase this as an optimization problem over the matrix $M$. For any fixed matrix $M$, the minimization of $\\mathcal{L}$ involves finding the factors $W_1, W_2$ that produce $M$ while minimizing the penalty $\\lVert W_1 \\rVert_F^2 + \\lVert W_2 \\rVert_F^2$.\n\nA key result in matrix analysis states that for any matrix $M \\in \\mathbb{R}^{p \\times d}$ and any factorization $M = W_2 W_1$ where $W_1 \\in \\mathbb{R}^{h \\times d}$ and $W_2 \\in \\mathbb{R}^{p \\times h}$, the following inequality holds:\n$$\n\\lVert W_1 \\rVert_F^2 + \\lVert W_2 \\rVert_F^2 \\ge 2 \\lVert M \\rVert_*\n$$\nThis lower bound is achievable if the hidden dimension $h$ is sufficiently large, specifically $h \\ge \\text{rank}(M)$. The minimum is achieved by a \"balanced\" factorization derived from the SVD of $M$. Let $M=U \\Sigma V^T$ have rank $k$. A minimal norm factorization is $W_2 = U_k (\\Sigma_k)^{1/2}$ and $W_1 = (\\Sigma_k)^{1/2} V_k^T$, where the subscript $k$ denotes the parts of the SVD corresponding to the $k$ non-zero singular values. For this choice, $\\lVert W_1 \\rVert_F^2 = \\lVert W_2 \\rVert_F^2 = \\text{Tr}(\\Sigma_k) = \\lVert M \\rVert_*$.\n\nTherefore, minimizing $\\mathcal{L}(W_2, W_1)$ over all possible factors $(W_1, W_2)$ with hidden dimension $h$ is equivalent to solving the following problem for $M$:\n$$\n\\min_{M \\in \\mathbb{R}^{p \\times d}} \\left( \\frac{1}{2} \\lVert M - T \\rVert_F^2 + \\alpha \\lVert M \\rVert_* \\right) \\quad \\text{subject to} \\quad \\text{rank}(M) \\le h.\n$$\nThis is a convex optimization problem (nuclear norm regularization) with an additional rank constraint.\n\n#### Solving for the Optimal Singular Values\nLet the SVD of the target matrix be $T = U_T \\Sigma_T V_T^T$, where $\\Sigma_T = \\text{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_r)$ with $r = \\min(p, d)$ and $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$. Due to the unitary invariance of the Frobenius and nuclear norms, the optimal solution $M^\\star$ must share the same singular vectors as $T$. So, $M^\\star = U_T \\Sigma_M V_T^T$ for some diagonal matrix $\\Sigma_M = \\text{diag}(s_1, s_2, \\dots, s_r)$ with $s_1 \\ge s_2 \\ge \\dots \\ge 0$.\n\nSubstituting these forms into the objective function, it decouples into a sum of scalar problems, one for each singular value:\n$$\n\\min_{s_1, \\dots, s_r} \\sum_{i=1}^r \\left( \\frac{1}{2} (s_i - \\sigma_i)^2 + \\alpha s_i \\right) \\quad \\text{subject to} \\quad s_i \\ge 0 \\quad \\text{and} \\quad |\\{i : s_i > 0\\}| \\le h.\n$$\nLet's first find the optimal $s_i$ for each component $i$ without the rank constraint. We need to solve $\\min_{s_i \\ge 0} f(s_i) = \\frac{1}{2}(s_i - \\sigma_i)^2 + \\alpha s_i$. The derivative is $f'(s_i) = s_i - \\sigma_i + \\alpha$. Setting to zero gives $s_i = \\sigma_i - \\alpha$. Since $s_i$ must be non-negative, the solution is $s_i = \\max(0, \\sigma_i - \\alpha)$. This operation is known as the soft-thresholding operator.\n\n#### Incorporating the Rank Constraint from Finite $h$\nThe hidden layer width $h$ imposes a hard constraint $\\text{rank}(M) \\le h$, which means at most $h$ of the singular values $s_i$ can be non-zero. To minimize the total objective, we must decide which singular values to \"keep\" (allow to be non-zero) and which to \"zero out\".\n\nFor each index $i$, we can either set $s_i=0$ or $s_i = \\max(0, \\sigma_i-\\alpha)$.\n-   If we set $s_i=0$, the contribution to the objective is $\\frac{1}{2}\\sigma_i^2$.\n-   If we set $s_i = \\max(0, \\sigma_i-\\alpha)$, the contribution is:\n    -   If $\\sigma_i \\le \\alpha$, then $s_i=0$, and the contribution is $\\frac{1}{2}\\sigma_i^2$. There is no benefit to \"keeping\" this singular value.\n    -   If $\\sigma_i > \\alpha$, then $s_i=\\sigma_i-\\alpha > 0$. The contribution is $\\frac{1}{2}((\\sigma_i-\\alpha)-\\sigma_i)^2 + \\alpha(\\sigma_i-\\alpha) = \\frac{1}{2}\\alpha^2 + \\alpha\\sigma_i - \\alpha^2 = \\alpha\\sigma_i - \\frac{1}{2}\\alpha^2$.\n\nThe reduction in loss from keeping the $i$-th singular value (if $\\sigma_i > \\alpha$) versus forcing it to zero is $\\text{Gain}_i = (\\frac{1}{2}\\sigma_i^2) - (\\alpha\\sigma_i - \\frac{1}{2}\\alpha^2) = \\frac{1}{2}(\\sigma_i - \\alpha)^2$.\n\nTo maximize the total gain under the budget of keeping at most $h$ singular values, we should greedily choose to keep the singular values that provide the largest gains. Since the gain is monotonically increasing with $\\sigma_i$ (for $\\sigma_i > \\alpha$), we should keep the singular values corresponding to the largest $\\sigma_i$ values, provided they are greater than $\\alpha$.\n\n#### The Final Algorithm\nThis leads to a simple algorithm to determine the rank of the optimal mapping $M^\\star$:\n1.  Let the singular values of the target matrix $T$ be $\\{\\sigma_i\\}$.\n2.  Count the number of singular values, $k$, such that $\\sigma_i > \\alpha$. These are the singular values that would survive the soft-thresholding in an unconstrained scenario.\n3.  The rank of the optimal solution $M^\\star$ is limited by both the regularization (which sets a threshold for singular values to be non-zero) and the network architecture (which sets a hard limit on the rank).\n4.  Therefore, the final rank is $\\text{rank}(M^\\star) = \\min(h, k)$.\n\nThis provides a complete characterization. The singular values of $M^\\star$ are given by $s_i^\\star = \\max(0, \\sigma_i - \\alpha)$ for the top $\\min(h, k)$ indices (assuming $\\sigma_i$ are sorted descending), and $s_i^\\star = 0$ for all other indices. The algorithm directly computes the number of these non-zero values.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of finding the rank of an optimal linear mapping\n    in a regularized two-layer network.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (h, alpha, sigma_values)\n    test_cases = [\n        (10, 0.0, [3.0, 2.5, 1.0, 0.8, 0.2]),  # Case 1\n        (10, 1.0, [3.0, 2.5, 1.0, 0.8, 0.2]),  # Case 2\n        (10, 0.7, [3.0, 2.5, 1.0, 0.8, 0.2]),  # Case 3\n        (10, 3.5, [3.0, 2.5, 1.0, 0.8, 0.2]),  # Case 4\n        (2, 0.7, [3.0, 2.5, 1.0, 0.8, 0.2]),   # Case 5\n        (10, 0.5, [0.0, 0.0, 0.0, 0.0, 0.0])   # Case 6\n    ]\n\n    results = []\n    \n    for h, alpha, sigma in test_cases:\n        # The derivation shows that minimizing the objective for (W2, W1)\n        # is equivalent to solving a nuclear norm regularized problem for M = W2*W1:\n        # min_M (1/2 * ||M - T||_F^2 + alpha * ||M||_*)\n        # subject to rank(M) <= h.\n        #\n        # The solution to this problem is given by singular value thresholding.\n        # The optimal singular values of M, s_i*, are related to the singular\n        # values of T, sigma_i, by the soft-thresholding operator:\n        # s_i* = max(0, sigma_i - alpha).\n        #\n        # The rank of the unconstrained solution is the number of singular\n        # values of T that are greater than the regularization parameter alpha.\n        \n        # Count the number of singular values of T greater than alpha.\n        # This is the rank of the optimal solution if there were no 'h' constraint.\n        k = 0\n        for s_val in sigma:\n            if s_val > alpha:\n                k += 1\n        \n        # The hidden layer width 'h' imposes a hard constraint on the rank of M.\n        # rank(M) = rank(W2 * W1) <= min(rank(W1), rank(W2)) <= h.\n        # Thus, the final rank is the minimum of the unconstrained rank 'k'\n        # and the architectural rank limit 'h'.\n        rank = min(h, k)\n        results.append(rank)\n\n    # Format the results into the required string format: \"[r1,r2,r3,r4,r5,r6]\"\n    # np.array2string can be used for robust formatting.\n    # The output format requires no spaces and integer representation.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "A crucial practical question is how to select the optimal strength of our regularization, represented by the hyperparameter $\\lambda$. Rather than relying on manual tuning or expensive grid search, this advanced exercise introduces a principled, gradient-based method for automating this choice . You will derive and compute the \"hypergradient\"—the gradient of the validation loss with respect to $\\lambda$—using the principles of bilevel optimization. This practice elevates your understanding from merely applying regularization to actively optimizing its application in a data-driven manner.",
            "id": "3161386",
            "problem": "A model with parameter vector $\\mathbf{w} \\in \\mathbb{R}^{d}$ is trained by Empirical Risk Minimization (ERM) on a training set $(\\mathbf{X}_{\\mathrm{tr}}, \\mathbf{y}_{\\mathrm{tr}})$ using a squared error objective with a parameter norm penalty (squared Euclidean norm). The hyperparameter $\\lambda \\ge 0$ weighting the penalty is treated as a learnable parameter in a bilevel optimization, where the inner problem minimizes the training objective to produce $\\mathbf{w}^{\\star}(\\lambda)$ and the outer problem minimizes the validation loss $\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda))$ on $(\\mathbf{X}_{\\mathrm{val}}, \\mathbf{y}_{\\mathrm{val}})$. Starting only from the definitions below and first principles, derive how the validation loss changes with respect to $\\lambda$ via implicit differentiation, and assess inner-loop stability.\n\nLet the training objective be\n$$\n\\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda) \\;=\\; \\frac{1}{2n}\\|\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w} - \\mathbf{y}_{\\mathrm{tr}}\\|_{2}^{2} \\;+\\; \\frac{\\lambda}{2}\\|\\mathbf{w}\\|_{2}^{2},\n$$\nand the validation loss be\n$$\n\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}) \\;=\\; \\frac{1}{2m}\\|\\mathbf{X}_{\\mathrm{val}}\\mathbf{w} - \\mathbf{y}_{\\mathrm{val}}\\|_{2}^{2}.\n$$\nThe minimizer of the inner problem is defined by\n$$\n\\mathbf{w}^{\\star}(\\lambda) \\;=\\; \\arg\\min_{\\mathbf{w}\\in\\mathbb{R}^{d}} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda).\n$$\n\nTasks:\n- Using only the first-order optimality condition for the inner problem and standard linear algebra, derive an explicit formula for $\\mathbf{w}^{\\star}(\\lambda)$ in terms of $\\mathbf{X}_{\\mathrm{tr}}$, $\\mathbf{y}_{\\mathrm{tr}}$, and $\\lambda$.\n- Using only definitions, the chain rule, and basic matrix calculus (including the derivative of an inverse matrix), derive a closed-form expression for the hypergradient $\\frac{d}{d\\lambda}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda))$.\n- Now take a specific instance with $n=3$, $d=2$, $m=2$,\n$$\n\\mathbf{X}_{\\mathrm{tr}} \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}, \n\\quad\n\\mathbf{y}_{\\mathrm{tr}} \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix},\n\\quad\n\\mathbf{X}_{\\mathrm{val}} \\;=\\; \\begin{pmatrix} 1 & 1 \\\\ 2 & 0 \\end{pmatrix},\n\\quad\n\\mathbf{y}_{\\mathrm{val}} \\;=\\; \\begin{pmatrix} 2 \\\\ 0.8 \\end{pmatrix},\n$$\nand evaluate $\\left.\\frac{d}{d\\lambda}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda))\\right|_{\\lambda=\\frac{1}{3}}$. Round your final numerical answer to four significant figures.\n- For the inner-loop stability, consider gradient descent on $\\mathcal{L}_{\\mathrm{tr}}$ with a fixed step size $\\alpha > 0$. Using the largest eigenvalue of the Hessian of $\\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda)$, state the condition on $\\alpha$ that ensures local stability (non-divergent iterates) and evaluate the permissible range of $\\alpha$ numerically at $\\lambda=\\frac{1}{3}$. Your final reported answer must be only the value of $\\left.\\frac{d}{d\\lambda}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda))\\right|_{\\lambda=\\frac{1}{3}}$.",
            "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, and free of contradictions or ambiguities. It represents a standard bilevel optimization problem for hyperparameter tuning in the context of regularized linear regression. We may therefore proceed with a full solution.\n\nHere are the givens for this problem:\nThe training objective is $\\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda) = \\frac{1}{2n}\\|\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w} - \\mathbf{y}_{\\mathrm{tr}}\\|_{2}^{2} + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|_{2}^{2}$.\nThe validation loss is $\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}) = \\frac{1}{2m}\\|\\mathbf{X}_{\\mathrm{val}}\\mathbf{w} - \\mathbf{y}_{\\mathrm{val}}\\|_{2}^{2}$.\nThe optimal parameters for the inner loop are $\\mathbf{w}^{\\star}(\\lambda) = \\arg\\min_{\\mathbf{w}\\in\\mathbb{R}^{d}} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda)$.\nThe numerical values for the specific instance are $n=3$, $d=2$, $m=2$,\n$\\mathbf{X}_{\\mathrm{tr}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}$, \n$\\mathbf{y}_{\\mathrm{tr}} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix}$,\n$\\mathbf{X}_{\\mathrm{val}} = \\begin{pmatrix} 1 & 1 \\\\ 2 & 0 \\end{pmatrix}$,\n$\\mathbf{y}_{\\mathrm{val}} = \\begin{pmatrix} 2 \\\\ 0.8 \\end{pmatrix}$,\nand the evaluation point is $\\lambda=\\frac{1}{3}$.\n\nThe problem is addressed in four parts as requested.\n\n**Part 1: Derivation of an explicit formula for $\\mathbf{w}^{\\star}(\\lambda)$**\n\nThe training objective $\\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda)$ is a convex function in $\\mathbf{w}$. The unique minimizer $\\mathbf{w}^{\\star}(\\lambda)$ is found by setting the gradient with respect to $\\mathbf{w}$ to the zero vector. The objective can be written in matrix form as:\n$$\n\\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda) = \\frac{1}{2n}(\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w} - \\mathbf{y}_{\\mathrm{tr}})^{T}(\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w} - \\mathbf{y}_{\\mathrm{tr}}) + \\frac{\\lambda}{2}\\mathbf{w}^{T}\\mathbf{w}\n$$\nThe gradient with respect to $\\mathbf{w}$ is:\n$$\n\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda) = \\frac{1}{2n} \\cdot 2\\mathbf{X}_{\\mathrm{tr}}^{T}(\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w} - \\mathbf{y}_{\\mathrm{tr}}) + \\frac{\\lambda}{2} \\cdot 2\\mathbf{w} = \\frac{1}{n}(\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w} - \\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{y}_{\\mathrm{tr}}) + \\lambda\\mathbf{w}\n$$\nSetting the gradient to zero at $\\mathbf{w} = \\mathbf{w}^{\\star}(\\lambda)$:\n$$\n\\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w}^{\\star}(\\lambda) - \\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{y}_{\\mathrm{tr}} + \\lambda\\mathbf{w}^{\\star}(\\lambda) = \\mathbf{0}\n$$\nRearranging the terms to solve for $\\mathbf{w}^{\\star}(\\lambda)$:\n$$\n\\left(\\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}} + \\lambda\\mathbf{I}\\right)\\mathbf{w}^{\\star}(\\lambda) = \\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{y}_{\\mathrm{tr}}\n$$\nFor $\\lambda > 0$, the matrix $(\\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}} + \\lambda\\mathbf{I})$ is guaranteed to be invertible since $\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}}$ is positive semi-definite. Thus, we can write the explicit formula for $\\mathbf{w}^{\\star}(\\lambda)$:\n$$\n\\mathbf{w}^{\\star}(\\lambda) = \\left(\\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}} + \\lambda\\mathbf{I}\\right)^{-1} \\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{y}_{\\mathrm{tr}}\n$$\n\n**Part 2: Derivation of the hypergradient $\\frac{d}{d\\lambda}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda))$**\n\nWe seek to compute the derivative of the validation loss with respect to the hyperparameter $\\lambda$. The validation loss is a function of $\\mathbf{w}$, which in turn is a function of $\\lambda$. Using the chain rule:\n$$\n\\frac{d}{d\\lambda}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda)) = \\left. \\frac{\\partial\\mathcal{L}_{\\mathrm{val}}}{\\partial\\mathbf{w}^{T}} \\right|_{\\mathbf{w}=\\mathbf{w}^{\\star}(\\lambda)} \\frac{d\\mathbf{w}^{\\star}(\\lambda)}{d\\lambda} = \\left(\\nabla_{\\mathbf{w}}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda))\\right)^{T} \\frac{d\\mathbf{w}^{\\star}(\\lambda)}{d\\lambda}\n$$\nThe gradient of the validation loss is:\n$$\n\\nabla_{\\mathbf{w}}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}) = \\frac{1}{m}\\mathbf{X}_{\\mathrm{val}}^{T}(\\mathbf{X}_{\\mathrm{val}}\\mathbf{w} - \\mathbf{y}_{\\mathrm{val}})\n$$\nTo find the term $\\frac{d\\mathbf{w}^{\\star}(\\lambda)}{d\\lambda}$, we use implicit differentiation on the first-order optimality condition of the inner problem, which must hold for all relevant $\\lambda$:\n$$\n\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}^{\\star}(\\lambda), \\lambda) = \\mathbf{0}\n$$\nDifferentiating this identity with respect to $\\lambda$:\n$$\n\\frac{d}{d\\lambda} \\left[ \\nabla_{\\mathbf{w}} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}^{\\star}(\\lambda), \\lambda) \\right] = \\mathbf{0}\n$$\nApplying the multivariate chain rule:\n$$\n\\left(\\nabla_{\\mathbf{w}}^{2} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}^{\\star}(\\lambda), \\lambda)\\right) \\frac{d\\mathbf{w}^{\\star}(\\lambda)}{d\\lambda} + \\frac{\\partial}{\\partial\\lambda}\\left(\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}^{\\star}(\\lambda), \\lambda)\\right) = \\mathbf{0}\n$$\nThe first term contains the Hessian of the training loss, $\\mathbf{H}_{\\mathrm{tr}} = \\nabla_{\\mathbf{w}}^{2} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda)$:\n$$\n\\mathbf{H}_{\\mathrm{tr}} = \\nabla_{\\mathbf{w}}\\left(\\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w} - \\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{y}_{\\mathrm{tr}} + \\lambda\\mathbf{w}\\right) = \\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}} + \\lambda\\mathbf{I}\n$$\nThe second term is the partial derivative of the gradient with respect to $\\lambda$:\n$$\n\\frac{\\partial}{\\partial\\lambda}\\left(\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\mathrm{tr}}\\right) = \\frac{\\partial}{\\partial\\lambda}\\left(\\frac{1}{n}(\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}}\\mathbf{w} - \\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{y}_{\\mathrm{tr}}) + \\lambda\\mathbf{w}\\right) = \\mathbf{w}\n$$\nSubstituting these into the implicit differentiation equation and evaluating at $\\mathbf{w} = \\mathbf{w}^{\\star}(\\lambda)$:\n$$\n\\mathbf{H}_{\\mathrm{tr}} \\frac{d\\mathbf{w}^{\\star}}{d\\lambda} + \\mathbf{w}^{\\star}(\\lambda) = \\mathbf{0}\n$$\nSolving for $\\frac{d\\mathbf{w}^{\\star}}{d\\lambda}$:\n$$\n\\frac{d\\mathbf{w}^{\\star}(\\lambda)}{d\\lambda} = - \\mathbf{H}_{\\mathrm{tr}}^{-1} \\mathbf{w}^{\\star}(\\lambda) = - \\left(\\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}} + \\lambda\\mathbf{I}\\right)^{-1} \\mathbf{w}^{\\star}(\\lambda)\n$$\nSubstituting this back into the expression for the hypergradient:\n$$\n\\frac{d}{d\\lambda}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda)) = - \\left(\\nabla_{\\mathbf{w}}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\lambda))\\right)^{T} \\left(\\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}} + \\lambda\\mathbf{I}\\right)^{-1} \\mathbf{w}^{\\star}(\\lambda)\n$$\n\n**Part 3: Numerical evaluation at $\\lambda = 1/3$**\n\nFirst, we compute the necessary matrices using the provided data:\n$$\n\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{X}_{\\mathrm{tr}} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n$$\n$$\n\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{y}_{\\mathrm{tr}} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}\n$$\nWith $n=3$ and $\\lambda=1/3$, the Hessian of the training loss is:\n$$\n\\mathbf{H}_{\\mathrm{tr}} = \\frac{1}{3}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} + \\frac{1}{3}\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix} = \\begin{pmatrix} 1 & 1/3 \\\\ 1/3 & 1 \\end{pmatrix}\n$$\nThe inverse of the Hessian is:\n$$\n\\mathbf{H}_{\\mathrm{tr}}^{-1} = \\frac{1}{1 \\cdot 1 - (\\frac{1}{3} \\cdot \\frac{1}{3})} \\begin{pmatrix} 1 & -1/3 \\\\ -1/3 & 1 \\end{pmatrix} = \\frac{1}{8/9} \\begin{pmatrix} 1 & -1/3 \\\\ -1/3 & 1 \\end{pmatrix} = \\frac{9}{8} \\begin{pmatrix} 1 & -1/3 \\\\ -1/3 & 1 \\end{pmatrix}\n$$\nNext, we compute $\\mathbf{w}^{\\star}(\\lambda)$ at $\\lambda=1/3$:\n$$\n\\mathbf{w}^{\\star}(\\frac{1}{3}) = \\mathbf{H}_{\\mathrm{tr}}^{-1} \\left(\\frac{1}{n}\\mathbf{X}_{\\mathrm{tr}}^{T}\\mathbf{y}_{\\mathrm{tr}}\\right) = \\frac{9}{8}\\begin{pmatrix} 1 & -1/3 \\\\ -1/3 & 1 \\end{pmatrix} \\left(\\frac{1}{3}\\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}\\right) = \\frac{9}{8}\\begin{pmatrix} 1 & -1/3 \\\\ -1/3 & 1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{9}{8}\\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix} = \\begin{pmatrix} 3/4 \\\\ 3/4 \\end{pmatrix}\n$$\nNow, we compute the gradient of the validation loss at $\\mathbf{w}^{\\star}(1/3)$. Let's denote this $\\mathbf{g}_{\\mathrm{val}}$:\n$$\n\\mathbf{g}_{\\mathrm{val}} = \\nabla_{\\mathbf{w}}\\mathcal{L}_{\\mathrm{val}}(\\mathbf{w}^{\\star}(\\frac{1}{3})) = \\frac{1}{m}\\mathbf{X}_{\\mathrm{val}}^{T}\\left(\\mathbf{X}_{\\mathrm{val}}\\mathbf{w}^{\\star}(\\frac{1}{3}) - \\mathbf{y}_{\\mathrm{val}}\\right)\n$$\nWith $m=2$:\n$$\n\\mathbf{X}_{\\mathrm{val}}\\mathbf{w}^{\\star}(\\frac{1}{3}) - \\mathbf{y}_{\\mathrm{val}} = \\begin{pmatrix} 1 & 1 \\\\ 2 & 0 \\end{pmatrix}\\begin{pmatrix} 3/4 \\\\ 3/4 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 0.8 \\end{pmatrix} = \\begin{pmatrix} 3/4 + 3/4 \\\\ 2(3/4) \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 0.8 \\end{pmatrix} = \\begin{pmatrix} 1.5 \\\\ 1.5 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 0.8 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ 0.7 \\end{pmatrix}\n$$\n$$\n\\mathbf{g}_{\\mathrm{val}} = \\frac{1}{2}\\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\end{pmatrix}\\begin{pmatrix} -0.5 \\\\ 0.7 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} -0.5+1.4 \\\\ -0.5 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} 0.9 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} 0.45 \\\\ -0.25 \\end{pmatrix}\n$$\nFinally, we compute the hypergradient:\n$$\n\\frac{d}{d\\lambda}\\mathcal{L}_{\\mathrm{val}} = - \\mathbf{g}_{\\mathrm{val}}^{T} \\mathbf{H}_{\\mathrm{tr}}^{-1} \\mathbf{w}^{\\star}(\\frac{1}{3}) = - \\begin{pmatrix} 0.45 & -0.25 \\end{pmatrix} \\left( \\frac{9}{8} \\begin{pmatrix} 1 & -1/3 \\\\ -1/3 & 1 \\end{pmatrix} \\begin{pmatrix} 3/4 \\\\ 3/4 \\end{pmatrix} \\right)\n$$\nThe inner product is:\n$$\n\\mathbf{H}_{\\mathrm{tr}}^{-1} \\mathbf{w}^{\\star}(\\frac{1}{3}) = \\begin{pmatrix} 9/8 \\\\ -3/8 \\end{pmatrix} (3/4) + \\begin{pmatrix} -3/8 \\\\ 9/8 \\end{pmatrix} (3/4) = \\begin{pmatrix} 27/32 - 9/32 \\\\ -9/32 + 27/32 \\end{pmatrix} = \\begin{pmatrix} 18/32 \\\\ 18/32 \\end{pmatrix} = \\begin{pmatrix} 9/16 \\\\ 9/16 \\end{pmatrix}\n$$\nThen, the hypergradient is:\n$$\n\\frac{d}{d\\lambda}\\mathcal{L}_{\\mathrm{val}} = - \\begin{pmatrix} 0.45 & -0.25 \\end{pmatrix} \\begin{pmatrix} 9/16 \\\\ 9/16 \\end{pmatrix} = - \\left( 0.45 \\cdot \\frac{9}{16} - 0.25 \\cdot \\frac{9}{16} \\right) = - \\frac{9}{16} (0.45 - 0.25) = - \\frac{9}{16} (0.2) = - \\frac{9}{16} \\cdot \\frac{1}{5} = -\\frac{9}{80}\n$$\nIn decimal form, this is $-0.1125$.\n\n**Part 4: Inner-loop stability analysis**\n\nGradient descent updates for the inner loop are given by $\\mathbf{w}_{k+1} = \\mathbf{w}_{k} - \\alpha \\nabla_{\\mathbf{w}} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}_k, \\lambda)$. The process is locally stable near the minimizer $\\mathbf{w}^\\star$ if the spectral radius of the Jacobian of the update map is less than $1$. The update map is $\\mathbf{F}(\\mathbf{w}) = \\mathbf{w} - \\alpha \\nabla_{\\mathbf{w}} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda)$. Its Jacobian is:\n$$\n\\mathbf{J}_{\\mathbf{F}}(\\mathbf{w}) = \\nabla_{\\mathbf{w}}\\mathbf{F}(\\mathbf{w}) = \\mathbf{I} - \\alpha \\nabla_{\\mathbf{w}}^{2} \\mathcal{L}_{\\mathrm{tr}}(\\mathbf{w}, \\lambda) = \\mathbf{I} - \\alpha \\mathbf{H}_{\\mathrm{tr}}\n$$\nStability requires $\\rho(\\mathbf{I} - \\alpha \\mathbf{H}_{\\mathrm{tr}}) < 1$, where $\\rho(\\cdot)$ is the spectral radius. The eigenvalues of $\\mathbf{I} - \\alpha \\mathbf{H}_{\\mathrm{tr}}$ are $1 - \\alpha\\mu_i$, where $\\mu_i$ are the eigenvalues of $\\mathbf{H}_{\\mathrm{tr}}$. The condition becomes $|1 - \\alpha\\mu_i|<1$ for all $i$. Since $\\mathbf{H}_{\\mathrm{tr}}$ is positive definite for $\\lambda > 0$, all $\\mu_i > 0$. The condition simplifies to $0 < \\alpha < 2/\\mu_{\\max}(\\mathbf{H}_{\\mathrm{tr}})$. We must find the largest eigenvalue of $\\mathbf{H}_{\\mathrm{tr}}$ at $\\lambda=1/3$.\n$$\n\\mathbf{H}_{\\mathrm{tr}} = \\begin{pmatrix} 1 & 1/3 \\\\ 1/3 & 1 \\end{pmatrix}\n$$\nThe characteristic equation is $\\det(\\mathbf{H}_{\\mathrm{tr}} - \\mu\\mathbf{I}) = (1-\\mu)^{2} - (1/3)^{2} = 0$.\nThis gives $1-\\mu = \\pm 1/3$, so the eigenvalues are $\\mu_1 = 1 - 1/3 = 2/3$ and $\\mu_2 = 1 + 1/3 = 4/3$.\nThe largest eigenvalue is $\\mu_{\\max} = 4/3$. The stability condition for the step size $\\alpha$ is:\n$$\n0 < \\alpha < \\frac{2}{4/3} = \\frac{3}{2}\n$$\nThe permissible range for $\\alpha$ to ensure local stability is $(0, 1.5)$.\n\nThe final reported answer is the numerical value from Part 3.",
            "answer": "$$\n\\boxed{-0.1125}\n$$"
        }
    ]
}