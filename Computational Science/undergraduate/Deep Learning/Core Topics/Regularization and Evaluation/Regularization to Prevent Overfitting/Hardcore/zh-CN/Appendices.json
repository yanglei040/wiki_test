{
    "hands_on_practices": [
        {
            "introduction": "L2正则化，或称权重衰减，是防止过拟合最经典的技术之一。本练习  提供了一个虽简化但功能强大的理论模型，旨在揭示权重衰减在优化动力学层面的工作原理。通过在一个简单的可分数据集上分析梯度流，您将清晰地看到无正则化优化趋向于无限大权重的“隐式偏见”，并理解权重衰减如何抑制这种行为，从而收敛到一个有限且受控的解。",
            "id": "3169285",
            "problem": "考虑一个二元线性分类器，其参数向量为 $w \\in \\mathbb{R}^d$，在一个由恰好两个样本 $(x_1,y_1)$ 和 $(x_2,y_2)$ 组成的线性可分数据集上进行训练。这两个样本由 $(x_1,y_1)=(x,+1)$ 和 $(x_2,y_2)=(-x,-1)$ 给出，其中 $x \\in \\mathbb{R}^d$ 且 $x \\neq 0$。令 $r=\\|x\\|$。预测函数为 $f_w(x)=w^\\top x$，单个样本的训练损失是指数损失 $\\ell(z)=\\exp(-z)$。无正则化的经验风险为 $L(w)=\\sum_{i=1}^{2}\\ell\\!\\left(y_i w^\\top x_i\\right)$。为研究权重衰减的影响，我们同时考虑正则化目标 $F_\\lambda(w)=L(w)+\\frac{\\lambda}{2}\\|w\\|^2$，其中 $\\lambda0$。将参数 $w$ 的有符号间隔定义为 $m(w)=\\min_{i \\in \\{1,2\\}} y_i w^\\top x_i$。\n\n假设使用随机梯度下降（SGD）算法，其步长为足够小的常数，并采用标准的无偏梯度估计，分别在 $L(w)$（无权重衰减）和 $F_\\lambda(w)$（有权重衰减）上运行。并假设当 $t \\to \\infty$ 时，迭代值会追踪连续时间梯度流，并在适用情况下在方向上收敛到驻点解。请仅使用经验风险最小化、光滑函数的梯度以及凸对称问题的性质，通过研究范数 $\\|w(t)\\|$ 和有符号间隔 $m\\!\\left(w(t)\\right)$ 在 $t \\to \\infty$ 时的行为，分析并比较 SGD 在这两种情况下的隐式偏置。\n\n然后，对于 $\\lambda0$ 的正则化情况，利用数据集的对称性将优化问题简化为单个标量，推导 $F_\\lambda(w)$ 的最小化子 $w_\\lambda^\\star$ 的一阶最优性条件，并精确求解，以获得极限有符号间隔 $m_\\lambda^\\star = m\\!\\left(w_\\lambda^\\star\\right)$ 作为 $r$ 和 $\\lambda$ 的函数的闭式解。\n\n请用一个关于 $r$ 和 $\\lambda$ 的单一闭式解析表达式给出 $m_\\lambda^\\star$ 的最终答案。不需要也不允许进行数值近似。",
            "solution": "问题陈述已经过验证，并被确定为机器学习理论领域中一个有效、适定的问题。它具有科学依据、内容自洽，并且没有歧义或矛盾。因此，我们可以着手提供完整解答。\n\n该问题要求分析随机梯度下降（SGD）在一个二元线性分类任务中，分别在有和没有L2正则化（权重衰减）情况下的行为，并要求在正则化情况下推导最优间隔。\n\n我们首先根据所提供的信息来定义关键量。数据集包含两个点，$(x_1, y_1) = (x, +1)$ 和 $(x_2, y_2) = (-x, -1)$，其中 $x \\in \\mathbb{R}^d$ 是一个非零向量。令 $r = \\|x\\|$。损失函数为指数损失，$\\ell(z) = \\exp(-z)$。\n\n未正则化的经验风险由 $L(w) = \\sum_{i=1}^{2} \\ell(y_i w^\\top x_i)$ 给出。代入数据点：\n$$L(w) = \\ell( (+1) w^\\top x ) + \\ell( (-1) w^\\top (-x) ) = \\exp(-w^\\top x) + \\exp(w^\\top (-x))$$\n$$L(w) = \\exp(-w^\\top x) + \\exp(-w^\\top x) = 2 \\exp(-w^\\top x)$$\n该数据集的有符号间隔为 $m(w) = \\min_{i \\in \\{1,2\\}} y_i w^\\top x_i = \\min(w^\\top x, (-1)w^\\top(-x)) = \\min(w^\\top x, w^\\top x) = w^\\top x$。\n因此，未正则化的风险可以纯粹用间隔来表示：\n$$L(w) = 2 \\exp(-m(w))$$\n正则化目标函数为 $F_\\lambda(w) = L(w) + \\frac{\\lambda}{2} \\|w\\|^2$，其中正则化参数 $\\lambda  0$。\n\n**SGD 动态分析**\n\n我们分析在梯度流下参数向量 $w(t)$ 的行为，假设 SGD 在小步长下会追踪该梯度流。\n\n**情况 1：未正则化目标 ($L(w)$, $\\lambda=0$)**\n对于未正则化目标 $L(w)$，其梯度流由微分方程 $\\frac{dw}{dt} = -\\nabla L(w)$ 描述。\n$L(w)$ 的梯度为：\n$$\\nabla L(w) = \\nabla_w \\left( 2 \\exp(-w^\\top x) \\right) = 2 \\exp(-w^\\top x) \\cdot \\nabla_w(-w^\\top x) = -2x \\exp(-w^\\top x)$$\n因此，梯度流为：\n$$\\frac{dw}{dt} = -(-2x \\exp(-w^\\top x)) = 2x \\exp(-w^\\top x)$$\n这个方程揭示了在任何时刻 $t$，$w$ 的变化始终沿着向量 $x$ 的方向。如果我们初始化 $w(0)=0$，那么 $w(t)$ 将始终与 $x$ 平行。更一般地， $w(0)$ 中与 $x$ 正交的任何分量将保持不变，而与 $x$ 平行的分量则会演化。我们感兴趣的动力学可以通过假设 $w(t)$ 与 $x$ 共线来捕捉，因此我们可以写出 $w(t) = c(t)x$，其中 $c(t)$ 是某个标量函数。\n\n最小化 $L(w) = 2\\exp(-m(w))$ 等价于最大化间隔 $m(w) = w^\\top x$。对于一个使用指数损失的线性可分数据集，通过使间隔任意大，可以将损失驱动到任意接近于 $0$。这意味着权重向量的范数 $\\|w\\|$ 必须趋于无穷大。让我们用流方程来验证这一点。代入 $w(t) = c(t)x$：\n$$\\frac{d}{dt}(c(t)x) = 2x \\exp(-(c(t)x)^\\top x)$$\n$$x \\frac{dc}{dt} = 2x \\exp(-c(t) \\|x\\|^2) = 2x \\exp(-c(t) r^2)$$\n$$\\frac{dc}{dt} = 2 \\exp(-c(t) r^2)$$\n由于右侧恒为正，所以 $c(t)$ 是一个关于 $t$ 的严格递增函数。当 $t \\to \\infty$ 时，$c(t)$ 无界增长，因此 $c(t) \\to \\infty$。\n因此，权重的范数发散：$\\|w(t)\\| = \\|c(t)x\\| = |c(t)| \\|x\\| = c(t)r \\to \\infty$。\n间隔也发散：$m(w(t)) = w(t)^\\top x = c(t)r^2 \\to \\infty$。\n\n这种行为被称为梯度下降的**隐式偏置**。对于使用指数损失的可分数据，SGD 不会收敛到一个有限的权重向量。相反，它会找到一个范数不断增大的解，从而持续改善间隔。$w(t)$ 的方向收敛到 $x$ 的方向，对于这个简单问题而言，这就是最大间隔方向。\n\n**情况 2：正则化目标 ($F_\\lambda(w)$, $\\lambda0$)**\n对于正则化目标 $F_\\lambda(w) = L(w) + \\frac{\\lambda}{2} \\|w\\|^2$，其梯度流由 $\\frac{dw}{dt} = -\\nabla F_\\lambda(w)$ 给出。\n梯度为：\n$$\\nabla F_\\lambda(w) = \\nabla L(w) + \\nabla \\left( \\frac{\\lambda}{2} \\|w\\|^2 \\right) = -2x \\exp(-w^\\top x) + \\lambda w$$\n梯度流为：\n$$\\frac{dw}{dt} = 2x \\exp(-w^\\top x) - \\lambda w$$\n对于 $\\lambda  0$，目标函数 $F_\\lambda(w)$ 是严格凸的，因为他是一个凸函数 $L(w)$ 和一个严格凸函数 $\\frac{\\lambda}{2}\\|w\\|^2$ 的和。因此，存在一个唯一的有限最小化子 $w_\\lambda^\\star$，它是梯度流的驻点，满足 $\\frac{dw}{dt}=0$。\n在这个最小化子处，梯度为零：\n$$\\nabla F_\\lambda(w_\\lambda^\\star) = -2x \\exp(-(w_\\lambda^\\star)^\\top x) + \\lambda w_\\lambda^\\star = 0$$\n这意味着 SGD 收敛到一个有限解：当 $t \\to \\infty$ 时，$w(t) \\to w_\\lambda^\\star$。因此，范数 $\\|w(t)\\|$ 和间隔 $m(w(t))$ 分别收敛到有限值 $\\|w_\\lambda^\\star\\|$ 和 $m(w_\\lambda^\\star)$。\n\nL2 正则化项，或称权重衰减，对大的权重引入了惩罚，防止范数发散到无穷大。它抵消了来自指数损失的隐式间隔最大化压力，从而得到一个**有限的**最优权重向量 $w_\\lambda^\\star$，该向量在最小化分类损失和最小化权重范数之间取得了平衡。\n\n**最优间隔 $m_\\lambda^\\star$ 的推导**\n\n我们现在必须求解在正则化情况下极限有符号间隔 $m_\\lambda^\\star = m(w_\\lambda^\\star)$。一阶最优性条件是：\n$$\\lambda w_\\lambda^\\star = 2x \\exp(-(w_\\lambda^\\star)^\\top x)$$\n从这个方程可以明显看出，最优向量 $w_\\lambda^\\star$ 必须与向量 $x$ 平行。因此，我们可以将 $w_\\lambda^\\star$ 表示为 $w_\\lambda^\\star = c^\\star x$，其中 $c^\\star$ 是某个标量常数。为确保间隔 $w^\\top x$ 为正（以获得低损失），我们必须有 $c^\\star  0$。\n\n让我们将 $w_\\lambda^\\star = c^\\star x$ 代回最优性条件：\n$$\\lambda (c^\\star x) = 2x \\exp(-(c^\\star x)^\\top x)$$\n由于 $x \\neq 0$，我们可以从两边消去它：\n$$\\lambda c^\\star = 2 \\exp(-c^\\star \\|x\\|^2) = 2 \\exp(-c^\\star r^2)$$\n这是标量系数 $c^\\star$ 的最优性条件。\n\n问题要求的是最优间隔 $m_\\lambda^\\star$，而不是 $c^\\star$。间隔与 $c^\\star$ 的关系如下：\n$$m_\\lambda^\\star = m(w_\\lambda^\\star) = (w_\\lambda^\\star)^\\top x = (c^\\star x)^\\top x = c^\\star \\|x\\|^2 = c^\\star r^2$$\n根据这个关系，我们可以用我们所求的间隔来表示 $c^\\star$：\n$$c^\\star = \\frac{m_\\lambda^\\star}{r^2}$$\n现在，我们将这个关于 $c^\\star$ 的表达式代回其标量最优性条件：\n$$\\lambda \\left(\\frac{m_\\lambda^\\star}{r^2}\\right) = 2 \\exp\\left(-\\left(\\frac{m_\\lambda^\\star}{r^2}\\right) r^2\\right)$$\n$$\\frac{\\lambda m_\\lambda^\\star}{r^2} = 2 \\exp(-m_\\lambda^\\star)$$\n为了求解 $m_\\lambda^\\star$，我们重新整理方程以分离出它。我们将所有包含 $m_\\lambda^\\star$ 的项移到一边：\n$$m_\\lambda^\\star \\exp(m_\\lambda^\\star) = \\frac{2 r^2}{\\lambda}$$\n这个方程的形式为 $u \\exp(u) = v$，其中 $u = m_\\lambda^\\star$ 且 $v = \\frac{2 r^2}{\\lambda}$。这类方程的解由朗伯 W 函数 $W(v)$ 给出。朗伯 W 函数被定义为函数 $f(u) = u \\exp(u)$ 的反函数。\n由于 $r^2  0$ 且 $\\lambda  0$，自变量 $\\frac{2 r^2}{\\lambda}$ 是正的。对于正自变量，朗伯 W 函数的主分支（记为 $W_0$ 或简称 $W$）提供唯一的正实数解。\n因此，最优间隔为：\n$$m_\\lambda^\\star = W\\left(\\frac{2 r^2}{\\lambda}\\right)$$\n这就是极限有符号间隔作为 $r$ 和 $\\lambda$ 的函数的精确、闭式解析表达式。",
            "answer": "$$\\boxed{W\\left(\\frac{2r^2}{\\lambda}\\right)}$$"
        },
        {
            "introduction": "除了向损失函数添加惩罚项，数据增强是另一种强大的正则化形式。本练习  将挑战您量化数据增强的效果，通过推导“有效样本量”这一概念来揭示其背后的数学原理。这个推导将帮助您理解为什么增加高度相关的增强样本会带来递减的回报，并深化您对数据增强有效性的认识。",
            "id": "3169320",
            "problem": "您正在基于经验风险最小化 (ERM) 原则训练一个深度神经网络分类器，其目标总体风险为期望损失 $L = \\mathbb{E}[\\ell(f_{\\theta}(x), y)]$，而经验风险估计量则由训练数据构建。为了减少过拟合，您对每个原始样本应用了标签保持的数据增强。\n\n假设有 $n$ 个原始样本 $(x_{i}, y_{i})$，它们从一个未知分布中独立同分布 (i.i.d.) 地抽取。对于每个原始样本 $i \\in \\{1, \\ldots, n\\}$，您通过从一个固定的增强池中选择变换 $T_{i,1}, \\ldots, T_{i,K}$ 来生成 $K$ 个增强变体，并评估损失随机变量\n$$\nZ_{i,k} = \\ell\\big(f_{\\theta}(T_{i,k}(x_{i})), y_{i}\\big)\n$$\n对于 $k \\in \\{1, \\ldots, K\\}$。用以下属性对这些损失进行建模：\n- 对于所有的 $i$ 和 $k$，$\\mathbb{E}[Z_{i,k}] = \\mu$ 且 $\\operatorname{Var}(Z_{i,k}) = \\sigma^{2}$。\n- 对于固定的 $i$，当 $k \\neq k'$ 时，$Z_{i,k}$ 和 $Z_{i,k'}$ 之间的相关系数是一个常数 $\\rho$，其中 $0 \\leq \\rho  1$。当 $i \\neq j$ 时，随机变量 $Z_{i,k}$ 和 $Z_{j,k'}$ 是独立的。\n\n用于训练的经验风险是所有增强损失的平均值：\n$$\n\\hat{L} = \\frac{1}{n K} \\sum_{i=1}^{n} \\sum_{k=1}^{K} Z_{i,k}.\n$$\n\n将有效样本量 $N_{\\mathrm{eff}}(K)$ 定义为独立损失样本的数量，其中每个样本的方差为 $\\sigma^{2}$，且它们的经验均值的方差与 $\\hat{L}$ 的方差相同。仅使用所提供的假设以及方差和协方差的标准性质，推导出一个关于 $n$、$K$ 和 $\\rho$ 的 $N_{\\mathrm{eff}}(K)$ 的闭式解析表达式。请以单个解析表达式的形式给出您的最终答案。无需四舍五入。",
            "solution": "用户希望找到一个经验风险估计量 $\\hat{L}$ 的有效样本量 $N_{\\mathrm{eff}}(K)$，该估计量是在每个原始样本增强 $K$ 次的数据集上计算得出的。\n\n### 问题验证\n**步骤1：提取已知条件**\n- 原始独立同分布样本数量：$n$。\n- 每个原始样本的增强次数：$K$。\n- 损失随机变量：$Z_{i,k} = \\ell\\big(f_{\\theta}(T_{i,k}(x_{i})), y_{i}\\big)$，其中 $i \\in \\{1, \\ldots, n\\}$, $k \\in \\{1, \\ldots, K\\}$。\n- 期望损失：$\\mathbb{E}[Z_{i,k}] = \\mu$。\n- 损失的方差：$\\operatorname{Var}(Z_{i,k}) = \\sigma^{2}$。\n- 增强组内的相关性（固定的 $i$，$k \\neq k'$）：$\\operatorname{Corr}(Z_{i,k}, Z_{i,k'}) = \\rho$，其中 $0 \\leq \\rho  1$。\n- 增强组之间的独立性（$i \\neq j$）：$Z_{i,k}$ 和 $Z_{j,k'}$ 是独立的。\n- 经验风险估计量：$\\hat{L} = \\frac{1}{n K} \\sum_{i=1}^{n} \\sum_{k=1}^{K} Z_{i,k}$。\n- 有效样本量 $N_{\\mathrm{eff}}(K)$ 的定义：它是独立损失样本的数量，每个样本的方差为 $\\sigma^{2}$，使得它们的经验均值的方差等于 $\\operatorname{Var}(\\hat{L})$。\n\n**步骤2：使用提取的已知条件进行验证**\n该问题在科学上基于统计学习理论，特别是在数据增强的分析方面。所有术语都是标准的且定义明确。关于损失变量统计特性（恒定的均值、方差和相关性结构）的假设，构成了理论分析中一个常见且有效的简化。该问题是自洽的，提供了所有必要的信息，并且没有内部矛盾。这是一个适定问题，要求一个唯一的、可推导的量。语言客观而精确。因此，该问题被认为是有效的。\n\n### 解题推导\n核心任务是推导 $N_{\\mathrm{eff}}(K)$ 的表达式。根据其定义，如果我们考虑一个估计量 $\\hat{M}$ 作为 $N_{\\mathrm{eff}}(K)$ 个独立同分布随机变量 $W_j$ 的均值，每个变量的方差为 $\\sigma^2$，则其方差为：\n$$\n\\operatorname{Var}(\\hat{M}) = \\operatorname{Var}\\left(\\frac{1}{N_{\\mathrm{eff}}(K)} \\sum_{j=1}^{N_{\\mathrm{eff}}(K)} W_j\\right) = \\frac{1}{N_{\\mathrm{eff}}(K)^2} \\sum_{j=1}^{N_{\\mathrm{eff}}(K)} \\operatorname{Var}(W_j) = \\frac{N_{\\mathrm{eff}}(K) \\sigma^2}{N_{\\mathrm{eff}}(K)^2} = \\frac{\\sigma^2}{N_{\\mathrm{eff}}(K)}\n$$\n问题定义 $N_{\\mathrm{eff}}(K)$ 使得 $\\operatorname{Var}(\\hat{L}) = \\operatorname{Var}(\\hat{M})$。因此，我们有以下关系：\n$$\n\\operatorname{Var}(\\hat{L}) = \\frac{\\sigma^2}{N_{\\mathrm{eff}}(K)} \\implies N_{\\mathrm{eff}}(K) = \\frac{\\sigma^2}{\\operatorname{Var}(\\hat{L})}\n$$\n为了求得 $N_{\\mathrm{eff}}(K)$，我们必须首先计算经验风险估计量的方差 $\\operatorname{Var}(\\hat{L})$。\n\n估计量 $\\hat{L}$ 由下式给出：\n$$\n\\hat{L} = \\frac{1}{n K} \\sum_{i=1}^{n} \\sum_{k=1}^{K} Z_{i,k}\n$$\n使用方差的性质 $\\operatorname{Var}(c X) = c^2 \\operatorname{Var}(X)$，我们得到：\n$$\n\\operatorname{Var}(\\hat{L}) = \\operatorname{Var}\\left(\\frac{1}{nK} \\sum_{i=1}^{n} \\sum_{k=1}^{K} Z_{i,k}\\right) = \\frac{1}{(nK)^2} \\operatorname{Var}\\left(\\sum_{i=1}^{n} \\sum_{k=1}^{K} Z_{i,k}\\right)\n$$\n我们定义一组新的随机变量 $S_i$，表示每个原始样本 $i$ 的损失之和：\n$$\nS_i = \\sum_{k=1}^{K} Z_{i,k}\n$$\n总和可以写成 $\\sum_{i=1}^{n} S_i$。问题陈述，当 $i \\neq j$ 时，变量 $Z_{i,k}$ 和 $Z_{j,k'}$ 是独立的。这意味着当 $i \\neq j$ 时，和 $S_i$ 与 $S_j$ 也是独立的。对于独立随机变量，其和的方差等于其方差之和：\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{n} S_i\\right) = \\sum_{i=1}^{n} \\operatorname{Var}(S_i)\n$$\n这简化了 $\\operatorname{Var}(\\hat{L})$ 的表达式：\n$$\n\\operatorname{Var}(\\hat{L}) = \\frac{1}{(nK)^2} \\sum_{i=1}^{n} \\operatorname{Var}(S_i) = \\frac{1}{(nK)^2} \\sum_{i=1}^{n} \\operatorname{Var}\\left(\\sum_{k=1}^{K} Z_{i,k}\\right)\n$$\n现在我们计算固定 $i$ 时 $S_i$ 的方差。变量 $Z_{i,1}, \\ldots, Z_{i,K}$ 是相关的。随机变量之和的方差是其协方差矩阵中所有元素的总和：\n$$\n\\operatorname{Var}(S_i) = \\operatorname{Var}\\left(\\sum_{k=1}^{K} Z_{i,k}\\right) = \\sum_{k=1}^{K} \\sum_{k'=1}^{K} \\operatorname{Cov}(Z_{i,k}, Z_{i,k'})\n$$\n我们可以将这个双重求和分成两部分：$k=k'$ 的项和 $k \\neq k'$ 的项。\n1.  当 $k=k'$ 时，协方差即方差：$\\operatorname{Cov}(Z_{i,k}, Z_{i,k}) = \\operatorname{Var}(Z_{i,k}) = \\sigma^2$。共有 $K$ 个这样的项（对于每个 $k \\in \\{1, \\ldots, K\\}$）。\n2.  当 $k \\neq k'$ 时，协方差与相关系数 $\\rho$ 相关。根据定义，$\\operatorname{Corr}(Z_{i,k}, Z_{i,k'}) = \\frac{\\operatorname{Cov}(Z_{i,k}, Z_{i,k'})}{\\sqrt{\\operatorname{Var}(Z_{i,k})\\operatorname{Var}(Z_{i,k'})}}$。在方差恒为 $\\sigma^2$ 的情况下，这得到 $\\operatorname{Cov}(Z_{i,k}, Z_{i,k'}) = \\rho \\sigma^2$。其中 $k \\neq k'$ 的数对 $(k, k')$ 的数量为 $K(K-1)$。\n\n将这些部分结合起来，我们得到：\n$$\n\\operatorname{Var}(S_i) = \\sum_{_k=1}^K \\operatorname{Var}(Z_{i,k}) + \\sum_{k \\neq k'} \\operatorname{Cov}(Z_{i,k}, Z_{i,k'}) = K \\sigma^2 + K(K-1) \\rho \\sigma^2\n$$\n提取公因式：\n$$\n\\operatorname{Var}(S_i) = K\\sigma^2 \\big(1 + (K-1)\\rho\\big)\n$$\n由于对于所有 $i \\in \\{1, \\ldots, n\\}$，这个方差都是相同的，所以对 $i$ 的求和为：\n$$\n\\sum_{i=1}^{n} \\operatorname{Var}(S_i) = n \\cdot \\operatorname{Var}(S_i) = n K\\sigma^2 \\big(1 + (K-1)\\rho\\big)\n$$\n现在，将此结果代回到 $\\operatorname{Var}(\\hat{L})$ 的表达式中：\n$$\n\\operatorname{Var}(\\hat{L}) = \\frac{1}{(nK)^2} \\left[ n K\\sigma^2 \\big(1 + (K-1)\\rho\\big) \\right]\n$$\n简化表达式：\n$$\n\\operatorname{Var}(\\hat{L}) = \\frac{n K \\sigma^2 (1 + (K-1)\\rho)}{n^2 K^2} = \\frac{\\sigma^2 \\big(1 + (K-1)\\rho\\big)}{nK}\n$$\n最后，我们使用此结果来求得有效样本量 $N_{\\mathrm{eff}}(K)$：\n$$\nN_{\\mathrm{eff}}(K) = \\frac{\\sigma^2}{\\operatorname{Var}(\\hat{L})} = \\frac{\\sigma^2}{\\frac{\\sigma^2 \\big(1 + (K-1)\\rho\\big)}{nK}}\n$$\n$$\nN_{\\mathrm{eff}}(K) = \\frac{nK}{1 + (K-1)\\rho}\n$$\n这个表达式表明，有效样本量小于增强样本的总数 $nK$（除非 $\\rho=0$），且大于原始样本量 $n$（除非 $\\rho$ 趋近于 $1$）。这量化了添加高度相关的增强数据所带来的收益递减效应。",
            "answer": "$$ \\boxed{\\frac{nK}{1 + (K-1)\\rho}} $$"
        },
        {
            "introduction": "现在，让我们通过一个编码实践将理论付诸行动。数据增强技术种类繁多，例如Cutout和Mixup，它们各自引入了不同的归纳偏见。本练习  要求您构建一个完整的实验流程，以研究结合这两种策略是会产生互补增益还是冲突效应。通过亲手实现数据生成、模型训练和评估的全过程，您将在实证机器学习研究方面获得宝贵经验，并对不同正则化器如何相互作用形成更细致的看法。",
            "id": "3169254",
            "problem": "给定一个合成的二元分类任务，要求您从基本原理出发，分析组合两种数据增强策略——cutout 和 Mixup——是会引入竞争性偏差，还是对决策边界产生互补平滑。您必须实现一个完整的程序，该程序能够生成数据集、应用增强、在经验风险最小化下训练一个线性模型，并根据精确定义的标准为每个测试用例返回一个布尔决策。\n\n基本定义和设置：\n- 考虑经验风险最小化 (ERM)，它选择参数以最小化训练样本上的经验损失。对线性预测器使用带有岭正则化的平方损失。\n- 设输入维度为 $d = 16$。设训练样本数量为 $n_{\\text{train}} = 240$，测试样本数量为 $n_{\\text{test}} = 240$。\n- 设类别标签为 $y \\in \\{-1, +1\\}$。\n- 设这两个类别由均值为 $\\mu_{+}$ 和 $\\mu_{-}$、协方差为各向同性的多元正态分布生成。具体来说，设置 $\\sigma = 0.8$，并定义 $\\mu_{+}$ 的前 $k = 6$ 个分量等于 $\\delta = 1.2$，其余分量等于 $0$，即 $\\mu_{+} = (\\underbrace{\\delta, \\dots, \\delta}_{k}, \\underbrace{0, \\dots, 0}_{d-k})$。设置 $\\mu_{-} = -\\mu_{+}$。对于每个类别，从 $\\mathcal{N}(\\mu_{\\pm}, \\sigma^{2} I_{d})$ 中抽取 $n_{\\text{train}}/2$ 个训练样本和 $n_{\\text{test}}/2$ 个测试样本，其中 $I_{d}$ 是 $d \\times d$ 的单位矩阵。\n\n模型与训练：\n- 使用通过岭回归训练的线性预测器 $f(x) = w^{\\top} x$。给定设计矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 和目标 $Y \\in \\mathbb{R}^{n}$，选择 $w \\in \\mathbb{R}^{d}$ 以最小化正则化的平方经验损失\n$$\n\\mathcal{L}(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( w^{\\top} x_{i} - y_{i}\\right)^{2} + \\lambda \\|w\\|_{2}^{2},\n$$\n其中 $\\lambda = 0.1$。使用岭回归的正规方程的闭式解：\n$$\nw = \\left(X^{\\top} X + \\lambda I_{d}\\right)^{-1} X^{\\top} Y.\n$$\n\n数据增强：\n- 长度为 $c$ 的 Cutout：对于每个训练样本 $x \\in \\mathbb{R}^{d}$，从 $\\{0, 1, \\dots, d-c\\}$ 中均匀选择一个随机起始索引 $s$，并将分量 $x_{s}, x_{s+1}, \\dots, x_{s+c-1}$ 设置为 $0$。如果 $c=0$，则禁用 cutout。\n- 参数为 $\\alpha$ 的 Mixup：对于每个训练样本，抽取 $\\lambda \\sim \\text{Beta}(\\alpha, \\alpha)$（仅当 $\\alpha  0$ 时），选择一个随机的配对样本 $(x_{j}, y_{j})$，并形成一个混合样本\n$$\nx' = \\lambda x_{i} + (1 - \\lambda) x_{j}, \\quad y' = \\lambda y_{i} + (1 - \\lambda) y_{j}.\n$$\n构建一个与原始训练集大小相同（$n_{\\text{train}}$）的混合数据集。如果 $\\alpha \\leq 0$，则禁用 Mixup。当两种增强都激活时，首先对原始特征应用 cutout，然后在经过 cutout 变换的特征上执行 Mixup。\n\n评估指标：\n- 决策性能：计算测试集上的错分率，结果为 $[0,1]$ 内的小数。使用分类器 $\\hat{y} = \\text{sign}(w^{\\top} x)$ 并与真实标签 $y \\in \\{-1, +1\\}$ 进行比较。\n- 平滑度代理：使用欧几里得范数 $\\|w\\|_{2}$ 作为线性预测器利普希茨常数的代理；较小的 $\\|w\\|_{2}$ 表示决策函数更平滑。\n\n互补性决策规则：\n- 对于给定的配对 $(c, \\alpha)$，计算三种训练配置的结果：\n    1. 仅 Cutout：cutout 长度为 $c$，Mixup 禁用。\n    2. 仅 Mixup：使用参数 $\\alpha$ 的 Mixup，cutout 禁用。\n    3. 两者皆用：按顺序应用长度为 $c$ 的 cutout 和参数为 $\\alpha$ 的 Mixup（先 cutout 后 Mixup）。\n- 设 $e_{\\text{cut}}$、$e_{\\text{mix}}$、$e_{\\text{both}}$ 表示测试错分率，而 $n_{\\text{cut}}$、$n_{\\text{mix}}$、$n_{\\text{both}}$ 表示相应的 $\\|w\\|_{2}$ 范数。设 $e_{\\text{single}}^{\\min} = \\min(e_{\\text{cut}}, e_{\\text{mix}})$ 和 $n_{\\text{single}}^{\\min} = \\min(n_{\\text{cut}}, n_{\\text{mix}})$。在容差 $\\tau_{e} = 10^{-3}$ 和 $\\tau_{n} = 10^{-6}$ 的情况下，声明布尔值\n$$\n\\mathcal{C} = \\left( e_{\\text{both}} \\leq e_{\\text{single}}^{\\min} + \\tau_{e} \\right) \\wedge \\left( n_{\\text{both}} \\leq n_{\\text{single}}^{\\min} + \\tau_{n} \\right).\n$$\n如果 $\\mathcal{C}$ 为真，则解释为互补平滑；否则解释为竞争性偏差。\n\n测试套件：\n- 使用以下 $(c, \\alpha)$ 配对：\n    1. $(0, 0.0)$,\n    2. $(4, 0.0)$,\n    3. $(0, 0.4)$,\n    4. $(4, 0.4)$,\n    5. $(10, 0.4)$,\n    6. $(4, 4.0)$.\n\n最终输出规范：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，`\"[result1,result2,result3]\"`），其中每个元素是对应于一个测试用例的布尔值 $\\mathcal{C}$，顺序与上面列出的顺序相同。不应打印任何额外文本。",
            "solution": "在进行求解之前，对问题陈述的有效性进行了严格评估。\n\n### 步骤 1：提取已知条件\n- **任务**：分析组合 cutout 和 Mixup 数据增强是引入竞争性偏差还是互补平滑。\n- **模型**：使用岭回归训练的线性预测器 $f(x) = w^{\\top} x$。\n- **损失函数与解**：\n  - 定义的损失：$\\mathcal{L}(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( w^{\\top} x_{i} - y_{i}\\right)^{2} + \\lambda \\|w\\|_{2}^{2}$。\n  - 提供的 $w$ 的解：$w = \\left(X^{\\top} X + \\lambda I_{d}\\right)^{-1} X^{\\top} Y$。\n- **问题参数**：\n  - 输入维度：$d = 16$。\n  - 训练样本数：$n_{\\text{train}} = 240$。\n  - 测试样本数：$n_{\\text{test}} = 240$。\n  - 类别标签：$y \\in \\{-1, +1\\}$。\n  - 正则化参数：$\\lambda = 0.1$。\n- **数据生成**：\n  - 分布：多元正态分布 $\\mathcal{N}(\\mu, \\Sigma)$。\n  - 协方差：各向同性，$\\Sigma = \\sigma^{2} I_{d}$，其中 $\\sigma = 0.8$。\n  - 类别均值：$\\mu_{+} = (\\underbrace{\\delta, \\dots, \\delta}_{k}, \\underbrace{0, \\dots, 0}_{d-k})$ 和 $\\mu_{-} = -\\mu_{+}$，其中 $k = 6$ 且 $\\delta = 1.2$。\n  - 数据划分：训练集每类 $n_{\\text{train}}/2$ 个样本，测试集每类 $n_{\\text{test}}/2$ 个样本。\n- **数据增强**：\n  - **Cutout**：对每个样本，将一个长度为 $c$ 的连续特征块设置为 0。起始索引从 $\\{0, 1, \\dots, d-c\\}$ 中均匀选择。如果 $c = 0$ 则禁用。\n  - **Mixup**：对每个训练样本 $(x_i, y_i)$，通过 $x' = \\lambda_{\\text{mix}} x_{i} + (1 - \\lambda_{\\text{mix}}) x_{j}$ 和 $y' = \\lambda_{\\text{mix}} y_{i} + (1 - \\lambda_{\\text{mix}}) y_{j}$ 形成一个新样本，其中 $(x_j, y_j)$ 是一个随机配对样本，且 $\\lambda_{\\text{mix}} \\sim \\text{Beta}(\\alpha, \\alpha)$。如果 $\\alpha \\leq 0$ 则禁用。构建一个大小为 $n_{\\text{train}}$ 的新数据集。\n  - **组合顺序**：当两者都激活时，先应用 cutout，然后应用 Mixup。\n- **评估指标**：\n  - **决策性能**：测试集上的错分率，分类器为 $\\hat{y} = \\text{sign}(w^{\\top} x)$。\n  - **平滑度代理**：权重向量的欧几里得范数 $\\|w\\|_{2}$。\n- **互补性决策规则**：\n  - 设 $e$ 为测试误差，$n$ 为 $\\|w\\|_2$。下标 'cut'、'mix'、'both' 分别表示仅用 Cutout、仅用 Mixup 和组合配置。\n  - 设 $e_{\\text{single}}^{\\min} = \\min(e_{\\text{cut}}, e_{\\text{mix}})$ 且 $n_{\\text{single}}^{\\min} = \\min(n_{\\text{cut}}, n_{\\text{mix}})$。\n  - 容差：$\\tau_{e} = 10^{-3}$，$\\tau_{n} = 10^{-6}$。\n  - 决策布尔值：$\\mathcal{C} = \\left( e_{\\text{both}} \\leq e_{\\text{single}}^{\\min} + \\tau_{e} \\right) \\wedge \\left( n_{\\text{both}} \\leq n_{\\text{single}}^{\\min} + \\tau_{n} \\right)$。\n- **测试套件**：$(c, \\alpha)$ 配对：$(0, 0.0)$, $(4, 0.0)$, $(0, 0.4)$, $(4, 0.4)$, $(10, 0.4)$, $(4, 4.0)$。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据既定标准对问题进行评估：\n- **科学依据**：问题在统计机器学习原理方面有充分的依据。它涉及标准技术（岭回归、cutout、Mixup）和概念（正则化、泛化、决策边界）。使用来自高斯分布的合成数据是研究算法行为的经典方法。\n- **良构性**：问题是良构的。所有必需的参数都已指定，数据生成、增强、训练和评估的程序也已明确定义。$\\lambda  0$ 的岭回归问题保证了权重向量 $w$ 有一个唯一、稳定的解。最终的决策规则 $\\mathcal{C}$ 是明确的。\n- **客观性**：问题以精确、客观、形式化的数学语言陈述。评估标准是定量的，没有主观解释的余地。\n- **不完整或矛盾**：存在一个微小的不一致之处。损失函数给定为 $\\mathcal{L}(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( w^{\\top} x_{i} - y_{i}\\right)^{2} + \\lambda \\|w\\|_{2}^{2}$，但解 $w = \\left(X^{\\top} X + \\lambda I_{d}\\right)^{-1} X^{\\top} Y$ 对应的目标函数中，平方和项没有被 $\\frac{1}{n}$ 缩放，即 $\\mathcal{L'}(w) = \\sum_{i=1}^{n} \\left( w^{\\top} x_{i} - y_{i}\\right)^{2} + \\lambda \\|w\\|_{2}^{2}$。然而，由于 $w$ 的公式是明确给出的，它构成了计算的最终指令，从而解决了这个歧义。问题没有指定随机种子，这对于精确复现是必要的。对于一个计算问题来说，这是一个微小的疏忽，而不是根本性缺陷，将在实现中通过设置一个固定的种子来解决。\n- **其他缺陷**：该问题没有表现出清单中的任何其他缺陷，例如过于简单、伪深刻或无法测试。测试用例，包括基线 $(0, 0.0)$，旨在系统地探究增强方法的效果。\n\n### 步骤 3：结论与行动\n问题是**有效的**。微小的不一致通过遵循明确提供的求解公式得以解决，而缺少指定的随机种子是一个标准的细节，将在实现过程中通过设置固定种子来解决。该分析具有科学意义且计算上是可行的。我们可以继续进行求解。\n\n### 基于原理的设计与求解\n解决方案涉及一个系统的计算实验，以评估 cutout 和 Mixup 增强对线性分类器的组合效应。实现将忠实地遵循问题陈述中定义的程序。\n\n**1. 数据生成**\n首先，我们合成了训练和测试数据集。两个类别（标记为 $y=+1$ 和 $y=-1$）的数据分别从 $d$ 维多元正态分布 $\\mathcal{N}(\\mu_{+}, \\sigma^2 I_d)$ 和 $\\mathcal{N}(\\mu_{-}, \\sigma^2 I_d)$ 中抽取。参数固定为 $d=16$ 和 $\\sigma=0.8$。均值被设置为对跖的，即 $\\mu_{-} = -\\mu_{+}$，其中 $\\mu_{+}$ 是一个稀疏向量，其前 $k=6$ 个元素等于 $\\delta=1.2$，其余为零。这创建了一个线性可分但并非微不足道的分类任务。我们为训练集从每个类别生成 $n_{\\text{train}}/2=120$ 个样本，为测试集从每个类别生成 $n_{\\text{test}}/2=120$ 个样本。采用固定的随机种子以确保整个实验是可复现的。\n\n**2. 数据增强实现**\n我们实现了两个数据增强函数，`apply_cutout` 和 `apply_mixup`。\n- `apply_cutout(X, c, rng)`：给定一个数据矩阵 $X$，此函数遍历每个样本（行）。对于每个样本，如果 cutout 长度 $c  0$，它会从 $\\{0, \\dots, d-c\\}$ 中随机选择一个起始特征索引 $s$，并将从 $s$ 到 $s+c-1$ 的连续特征块设置为零。这模拟了信息丢失，并鼓励模型不要依赖于任何单一的特征组。\n- `apply_mixup(X, Y, alpha, rng)`：如果 Mixup 参数 $\\alpha  0$，此函数会生成一个大小相同的新训练集。对于每个原始样本 $(x_i, y_i)$，它会从数据集中选择一个随机的配对样本 $(x_j, y_j)$。混合系数 $\\lambda_{\\text{mix}}$ 从 Beta 分布中抽取，即 $\\lambda_{\\text{mix}} \\sim \\text{Beta}(\\alpha, \\alpha)$。新的虚拟样本是原始样本和配对样本的凸组合：$(x', y') = (\\lambda_{\\text{mix}} x_i + (1-\\lambda_{\\text{mix}})x_j, \\lambda_{\\text{mix}} y_i + (1-\\lambda_{\\text{mix}})y_j)$。这将创建线性插值的样本和标签，通过平滑决策边界并鼓励模型在数据点之间表现出线性行为，从而起到一种正则化的作用。\n\n**3. 模型训练**\n该模型是一个线性分类器，其权重 $w \\in \\mathbb{R}^d$ 通过岭回归确定。正如在验证阶段所确定的，我们使用明确提供的闭式解：\n$$w = (X^{\\top}X + \\lambda I_d)^{-1} X^{\\top}Y$$\n这里，$X$ 是（可能经过增强的）训练数据的 $n_{\\text{train}} \\times d$ 设计矩阵，$Y$ 是相应标签的向量，$I_d$ 是 $d \\times d$ 的单位矩阵，$\\lambda = 0.1$ 是正则化参数。项 $\\lambda I_d$ 确保矩阵 $X^{\\top}X + \\lambda I_d$ 是可逆且良态的，通过惩罚大的权重来正则化解，这对应于最小化 L2 范数 $\\|w\\|_2^2$。\n\n**4. 实验协议与评估**\n对于测试套件中的每个 $(c, \\alpha)$ 对，我们执行三个不同的实验来分离和比较增强的效果：\n1.  **仅 Cutout**：我们在使用 cutout ($c$) 但未使用 Mixup ($\\alpha \\leq 0$) 增强的数据上训练一个模型。我们计算测试误差 $e_{\\text{cut}}$ 和权重范数 $n_{\\text{cut}}$。\n2.  **仅 Mixup**：我们在使用 Mixup ($\\alpha$) 但未使用 cutout ($c=0$) 增强的数据上训练一个模型。我们计算测试误差 $e_{\\text{mix}}$ 和权重范数 $n_{\\text{mix}}$。\n3.  **两者皆用**：我们先通过 cutout ($c$) 然后通过 Mixup ($\\alpha$) 对数据进行增强，并在此数据上训练一个模型。我们计算测试误差 $e_{\\text{both}}$ 和权重范数 $n_{\\text{both}}$。\n\n错分率是在静态的、未增强的测试集上计算的。分类器对测试样本 $x$ 的预测是 $\\hat{y} = \\text{sign}(w^{\\top}x)$。误差是 $\\hat{y} \\neq y$ 的测试样本所占的比例。权重范数 $\\|w\\|_2$ 作为所学函数平滑度的代理。\n\n**5. 互补性决策**\n最后，使用指定的决策规则对结果进行综合。如果使用两种增强训练的模型在两种指标上都至少与两种单一增强模型中较好的那个表现一样好（在给定容差范围内），那么这种增强组合就被认为是“互补的”。形式上，计算布尔值 $\\mathcal{C}$：\n$$\\mathcal{C} = (e_{\\text{both}} \\leq \\min(e_{\\text{cut}}, e_{\\text{mix}}) + \\tau_{e}) \\wedge (n_{\\text{both}} \\leq \\min(n_{\\text{cut}}, n_{\\text{mix}}) + \\tau_{n})$$\n其中 $\\tau_{e} = 10^{-3}$ 和 $\\tau_{n} = 10^{-6}$。程序将对每个测试用例执行这整个过程，并输出最终的布尔值列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import special\n\ndef solve():\n    # Define problem parameters\n    D = 16\n    N_TRAIN = 240\n    N_TEST = 240\n    K_FEATURES = 6\n    DELTA = 1.2\n    SIGMA = 0.8\n    LAMBDA_REG = 0.1\n    TAU_E = 1e-3\n    TAU_N = 1e-6\n    SEED = 42\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0, 0.0),\n        (4, 0.0),\n        (0, 0.4),\n        (4, 0.4),\n        (10, 0.4),\n        (4, 4.0),\n    ]\n\n    rng = np.random.default_rng(seed=SEED)\n\n    def generate_data():\n        \"\"\"Generates the base synthetic dataset.\"\"\"\n        mu_plus = np.zeros(D)\n        mu_plus[:K_FEATURES] = DELTA\n        mu_minus = -mu_plus\n        cov = (SIGMA**2) * np.identity(D)\n\n        n_per_class_train = N_TRAIN // 2\n        n_per_class_test = N_TEST // 2\n\n        X_train_plus = rng.multivariate_normal(mu_plus, cov, size=n_per_class_train)\n        X_train_minus = rng.multivariate_normal(mu_minus, cov, size=n_per_class_train)\n        X_train = np.vstack((X_train_plus, X_train_minus))\n        Y_train = np.array([1] * n_per_class_train + [-1] * n_per_class_train)\n\n        X_test_plus = rng.multivariate_normal(mu_plus, cov, size=n_per_class_test)\n        X_test_minus = rng.multivariate_normal(mu_minus, cov, size=n_per_class_test)\n        X_test = np.vstack((X_test_plus, X_test_minus))\n        Y_test = np.array([1] * n_per_class_test + [-1] * n_per_class_test)\n\n        return X_train, Y_train, X_test, Y_test\n\n    def apply_cutout(X, c, local_rng):\n        \"\"\"Applies cutout augmentation.\"\"\"\n        if c == 0:\n            return X\n        X_aug = X.copy()\n        for i in range(X_aug.shape[0]):\n            s = local_rng.integers(0, D - c + 1)\n            X_aug[i, s:s+c] = 0\n        return X_aug\n\n    def apply_mixup(X, Y, alpha, local_rng):\n        \"\"\"Applies Mixup augmentation.\"\"\"\n        if alpha = 0:\n            return X, Y\n        n_samples = X.shape[0]\n        X_aug = np.zeros_like(X)\n        Y_aug = np.zeros_like(Y, dtype=float)\n        \n        partner_indices = local_rng.permutation(n_samples)\n        \n        for i in range(n_samples):\n            lambda_mix = local_rng.beta(alpha, alpha)\n            j = partner_indices[i]\n            \n            X_aug[i] = lambda_mix * X[i] + (1 - lambda_mix) * X[j]\n            Y_aug[i] = lambda_mix * Y[i] + (1 - lambda_mix) * Y[j]\n            \n        return X_aug, Y_aug\n\n    def train_ridge(X, Y, lambda_reg):\n        \"\"\"Trains a ridge regression model using the closed-form solution.\"\"\"\n        d = X.shape[1]\n        I = np.identity(d)\n        # Using the formula w = (X^T X + lambda * I_d)^-1 X^T Y\n        term1 = np.linalg.inv(X.T @ X + lambda_reg * I)\n        term2 = X.T @ Y\n        w = term1 @ term2\n        return w\n\n    def evaluate_model(w, X_test, Y_test):\n        \"\"\"Evaluates the model's performance and weight norm.\"\"\"\n        # Misclassification rate\n        Y_pred_scores = X_test @ w\n        Y_pred_labels = np.sign(Y_pred_scores)\n        Y_pred_labels[Y_pred_labels == 0] = 1 # Handle zeros\n        misclassification_rate = np.mean(Y_pred_labels != Y_test)\n        \n        # L2 norm of weights\n        weight_norm = np.linalg.norm(w)\n        \n        return misclassification_rate, weight_norm\n\n    def run_experiment(c, alpha, base_data, local_rng):\n        \"\"\"Runs a single experiment for a given (c, alpha) configuration.\"\"\"\n        X_train_base, Y_train_base, X_test, Y_test = base_data\n        \n        # Apply cutout first\n        X_cut = apply_cutout(X_train_base, c, local_rng)\n        \n        # Then apply mixup\n        X_aug, Y_aug = apply_mixup(X_cut, Y_train_base, alpha, local_rng)\n        \n        # Train model\n        w = train_ridge(X_aug, Y_aug, LAMBDA_REG)\n        \n        # Evaluate\n        error, norm = evaluate_model(w, X_test, Y_test)\n        \n        return error, norm\n\n    # Generate the base dataset once for all experiments\n    base_dataset = generate_data()\n    \n    results = []\n    for c_val, alpha_val in test_cases:\n        # 1. Cutout only\n        e_cut, n_cut = run_experiment(c=c_val, alpha=0.0, base_data=base_dataset, local_rng=rng)\n        \n        # 2. Mixup only\n        e_mix, n_mix = run_experiment(c=0, alpha=alpha_val, base_data=base_dataset, local_rng=rng)\n        \n        # 3. Both augmentations\n        e_both, n_both = run_experiment(c=c_val, alpha=alpha_val, base_data=base_dataset, local_rng=rng)\n        \n        # Complementarity decision rule\n        e_single_min = min(e_cut, e_mix)\n        n_single_min = min(n_cut, n_mix)\n        \n        is_complementary = (e_both = e_single_min + TAU_E) and (n_both = n_single_min + TAU_N)\n        results.append(is_complementary)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}