{
    "hands_on_practices": [
        {
            "introduction": "To understand the impact of label smoothing, we must first see how it directly alters the learning signal. This practice focuses on deriving the gradient of the cross-entropy loss with respect to the model's logits, $\\mathbf{z}$, when using a smoothed target vector $\\tilde{\\mathbf{t}}$. Mastering this fundamental calculation is the first step toward analyzing how this regularization technique guides the optimization process.",
            "id": "3177306",
            "problem": "Consider a $K$-class probabilistic classifier with logits $\\mathbf{z} \\in \\mathbb{R}^{K}$ and softmax output $\\mathbf{q}(\\mathbf{z}) \\in (0,1)^{K}$ defined by $q_{k}(\\mathbf{z}) = \\exp(z_{k}) \\big/ \\sum_{j=1}^{K} \\exp(z_{j})$. The training loss on a single example with target vector $\\mathbf{t} \\in \\mathbb{R}^{K}$ is the cross-entropy $L(\\mathbf{z};\\mathbf{t}) = -\\sum_{k=1}^{K} t_{k} \\ln q_{k}(\\mathbf{z})$. Assume label smoothing with parameter $\\alpha \\in [0,1)$ is used to construct the target vector $\\tilde{\\mathbf{t}}(C)$ for a sampled class $C$:\n$$\n\\tilde{t}_{k}(C)=\n\\begin{cases}\n1-\\alpha, & \\text{if } k=C, \\\\\n\\alpha/(K-1), & \\text{if } k \\neq C.\n\\end{cases}\n$$\nUsing the definitions provided, derive the vector expression for the per-sample gradient of the loss with respect to the logits, $\\nabla_{\\mathbf{z}} L(\\mathbf{z};\\tilde{\\mathbf{t}}(C))$.",
            "solution": "**Step 1: Derive the Per-Sample Gradient**\n\nThe loss function for a single sample is the cross-entropy $L(\\mathbf{z};\\tilde{\\mathbf{t}}) = -\\sum_{k=1}^{K} \\tilde{t}_{k} \\ln q_{k}(\\mathbf{z})$, where $\\tilde{\\mathbf{t}}$ is the smoothed target vector $\\tilde{\\mathbf{t}}(C)$. The softmax function is $q_{k}(\\mathbf{z}) = \\exp(z_{k}) \\big/ \\sum_{j=1}^{K} \\exp(z_{j})$.\nLet's compute the partial derivative of $L$ with respect to a logit $z_i$:\n$$ \\frac{\\partial L}{\\partial z_i} = -\\sum_{k=1}^{K} \\tilde{t}_k \\frac{\\partial}{\\partial z_i} \\ln(q_k(\\mathbf{z})) $$\nThe derivative of the log-softmax function is known to be:\n$$ \\frac{\\partial}{\\partial z_i} \\ln(q_k(\\mathbf{z})) = \\delta_{ik} - q_i(\\mathbf{z}) $$\nwhere $\\delta_{ik}$ is the Kronecker delta.\nSubstituting this into the expression for the gradient:\n$$ \\frac{\\partial L}{\\partial z_i} = -\\sum_{k=1}^{K} \\tilde{t}_k (\\delta_{ik} - q_i(\\mathbf{z})) = -\\tilde{t}_i + q_i(\\mathbf{z}) \\sum_{k=1}^{K} \\tilde{t}_k $$\nThe components of the smoothed target vector $\\tilde{\\mathbf{t}}(C)$ sum to $1$:\n$$ \\sum_{k=1}^{K} \\tilde{t}_k(C) = (1-\\alpha) + \\sum_{k \\neq C} \\frac{\\alpha}{K-1} = 1-\\alpha + (K-1)\\frac{\\alpha}{K-1} = 1 $$\nThus, the partial derivative simplifies to:\n$$ \\frac{\\partial L}{\\partial z_i} = q_i(\\mathbf{z}) - \\tilde{t}_i $$\nIn vector form, the per-sample gradient with respect to the logits $\\mathbf{z}$ is:\n$$ \\nabla_{\\mathbf{z}} L(\\mathbf{z};\\tilde{\\mathbf{t}}(C)) = \\mathbf{q}(\\mathbf{z}) - \\tilde{\\mathbf{t}}(C) $$",
            "answer": "$$\n\\boxed{\\nabla_{\\mathbf{z}} L(\\mathbf{z};\\tilde{\\mathbf{t}}(C)) = \\mathbf{q}(\\mathbf{z}) - \\tilde{\\mathbf{t}}(C)}\n$$"
        },
        {
            "introduction": "Label smoothing replaces a single, confident \"true\" class with a softer distribution of probabilities. This exercise isolates this core mechanism by asking you to explore the statistical properties of the smoothed target vector, $\\tilde{\\mathbf{t}}(C)$, where the true class $C$ is a random variable. By calculating the mean and variance of the components of this vector, you will develop a precise intuition for how smoothing tempers the training targets.",
            "id": "3177306",
            "problem": "Consider the same setup as the previous problem, with the true class $C$ drawn uniformly from $\\{1,\\dots,K\\}$. Label smoothing with parameter $\\alpha \\in [0,1)$ is used to construct the target vector $\\tilde{\\mathbf{t}}(C)$:\n$$\n\\tilde{t}_{k}(C)=\n\\begin{cases}\n1-\\alpha, & \\text{if } k=C, \\\\\n\\alpha/(K-1), & \\text{if } k \\neq C.\n\\end{cases}\n$$\nFor an arbitrary component $k$, calculate the expectation $\\mathbb{E}[\\tilde{t}_k(C)]$ and the variance $\\operatorname{Var}(\\tilde{t}_k(C))$, where the expectation is taken over the uniform randomness of the true class $C$.",
            "solution": "The variance of a random variable $X$ is $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. We need to compute the first two moments of $\\tilde{t}_k(C)$ for an arbitrary component $k \\in \\{1, \\dots, K\\}$. The expectation is over $C \\sim \\text{Uniform}(\\{1, \\dots, K\\})$.\n\n**First moment (Expectation):**\nThe value of $\\tilde{t}_k(C)$ depends on whether the randomly chosen true class $C$ is equal to $k$.\n$$ \\mathbb{E}[\\tilde{t}_k(C)] = \\sum_{c=1}^{K} \\mathbb{P}(C=c) \\tilde{t}_k(c) = \\frac{1}{K} \\sum_{c=1}^{K} \\tilde{t}_k(c) $$\nThe sum contains one term where $c=k$, for which $\\tilde{t}_k(k) = 1-\\alpha$, and $K-1$ terms where $c \\neq k$, for which $\\tilde{t}_k(c) = \\alpha/(K-1)$.\n$$ \\mathbb{E}[\\tilde{t}_k(C)] = \\frac{1}{K} \\left( (1-\\alpha) + \\sum_{c\\neq k} \\frac{\\alpha}{K-1} \\right) = \\frac{1}{K} \\left( 1-\\alpha + (K-1)\\frac{\\alpha}{K-1} \\right) = \\frac{1}{K} $$\nThe expectation of any component of the target vector is $1/K$.\n\n**Second moment:**\nWe apply the same logic for the second moment.\n$$ \\mathbb{E}[(\\tilde{t}_k(C))^2] = \\sum_{c=1}^{K} \\mathbb{P}(C=c) (\\tilde{t}_k(c))^2 = \\frac{1}{K} \\sum_{c=1}^{K} (\\tilde{t}_k(c))^2 $$\nThe sum consists of one term where $c=k$ and $K-1$ terms where $c \\neq k$:\n$$ \\mathbb{E}[(\\tilde{t}_k(C))^2] = \\frac{1}{K} \\left( (1-\\alpha)^2 + \\sum_{c\\neq k} \\left(\\frac{\\alpha}{K-1}\\right)^2 \\right) = \\frac{1}{K} \\left( (1-\\alpha)^2 + (K-1)\\frac{\\alpha^2}{(K-1)^2} \\right) $$\n$$ \\mathbb{E}[(\\tilde{t}_k(C))^2] = \\frac{1}{K} \\left( (1-\\alpha)^2 + \\frac{\\alpha^2}{K-1} \\right) $$\n\n**Variance:**\nNow, we can compute the variance. Note that this expression is the same for any $k$.\n$$ \\operatorname{Var}(\\tilde{t}_k(C)) = \\mathbb{E}[(\\tilde{t}_k(C))^2] - (\\mathbb{E}[\\tilde{t}_k(C)])^2 = \\frac{1}{K} \\left( (1-\\alpha)^2 + \\frac{\\alpha^2}{K-1} \\right) - \\left(\\frac{1}{K}\\right)^2 $$",
            "answer": "$$\n\\boxed{\\operatorname{Var}(\\tilde{t}_k(C)) = \\frac{1}{K} \\left( (1-\\alpha)^2 + \\frac{\\alpha^2}{K-1} \\right) - \\frac{1}{K^2}}\n$$"
        },
        {
            "introduction": "A key benefit of label smoothing is its role in stabilizing the training process, which can be theoretically linked to a reduction in gradient variance. In this final practice, you will synthesize the results from the previous steps to formally prove and quantify this variance reduction. By deriving the multiplicative factor that describes how variance changes with the smoothing parameter $\\alpha$, you will gain a deep, quantitative understanding of one of label smoothing's most important effects.",
            "id": "3177306",
            "problem": "Consider the same setup as the previous problems. The per-sample gradient is $\\nabla_{\\mathbf{z}} L = \\mathbf{q}(\\mathbf{z}) - \\tilde{\\mathbf{t}}(C)$, and the logits $\\mathbf{z}$ are held fixed. The only source of randomness is the true class $C$, drawn uniformly from $\\{1, \\dots, K\\}$.\nDefine the scalar \"gradient variance\" measure as the trace of the covariance matrix of the per-sample gradient:\n$$\nS(\\alpha) \\equiv \\operatorname{tr}\\!\\big(\\operatorname{Cov}\\big(\\nabla_{\\mathbf{z}} L(\\mathbf{z};\\tilde{\\mathbf{t}}(C))\\big)\\big)\n$$\nwhere the covariance is taken over the randomness in $C$. Derive a closed-form expression for the multiplicative variance factor\n$$\nR(K,\\alpha) \\equiv \\frac{S(\\alpha)}{S(0)}\n$$\nas a function of $K \\ge 2$ and $\\alpha \\in [0,1)$. Your final answer must be a single simplified analytic expression in terms of $K$ and $\\alpha$ only.",
            "solution": "The problem asks for the derivation of the multiplicative variance factor $R(K,\\alpha) \\equiv S(\\alpha)/S(0)$.\n\n**Step 1: Relate Gradient Variance to Target Vector Variance**\n\nThe gradient variance measure is $S(\\alpha) = \\operatorname{tr}(\\operatorname{Cov}(\\nabla_{\\mathbf{z}} L))$. The gradient is $\\mathbf{g}(C) = \\mathbf{q} - \\tilde{\\mathbf{t}}(C)$. Since $\\mathbf{q}$ is fixed, the covariance is:\n$$ \\operatorname{Cov}(\\mathbf{g}(C)) = \\operatorname{Cov}(\\mathbf{q} - \\tilde{\\mathbf{t}}(C)) = \\operatorname{Cov}(-\\tilde{\\mathbf{t}}(C)) = \\operatorname{Cov}(\\tilde{\\mathbf{t}}(C)) $$\nThe trace of a covariance matrix is the sum of the variances of its components. Therefore,\n$$ S(\\alpha) = \\operatorname{tr}(\\operatorname{Cov}(\\tilde{\\mathbf{t}}(C))) = \\sum_{k=1}^{K} \\operatorname{Var}(\\tilde{t}_k(C)) $$\n\n**Step 2: Calculate $S(\\alpha)$**\n\nFrom the previous exercise, we know the variance of any component $\\tilde{t}_k(C)$ is:\n$$ \\operatorname{Var}(\\tilde{t}_k(C)) = \\frac{1}{K} \\left( (1-\\alpha)^2 + \\frac{\\alpha^2}{K-1} \\right) - \\frac{1}{K^2} $$\nSince this expression is identical for all $k=1, \\dots, K$, we have:\n$$ S(\\alpha) = \\sum_{k=1}^{K} \\operatorname{Var}(\\tilde{t}_k(C)) = K \\cdot \\operatorname{Var}(\\tilde{t}_1(C)) $$\n$$ S(\\alpha) = K \\left[ \\frac{1}{K} \\left( (1-\\alpha)^2 + \\frac{\\alpha^2}{K-1} \\right) - \\frac{1}{K^2} \\right] $$\n$$ S(\\alpha) = (1-\\alpha)^2 + \\frac{\\alpha^2}{K-1} - \\frac{1}{K} $$\nThis is the closed-form expression for the gradient variance measure.\n\n**Step 3: Calculate $S(0)$**\n\nTo find the variance without label smoothing (one-hot targets), we set $\\alpha=0$ in the expression for $S(\\alpha)$:\n$$ S(0) = (1-0)^2 + \\frac{0^2}{K-1} - \\frac{1}{K} = 1 - \\frac{1}{K} = \\frac{K-1}{K} $$\n\n**Step 4: Compute the Ratio $R(K,\\alpha)$**\n\nThe final step is to compute the ratio $R(K,\\alpha) = S(\\alpha) / S(0)$. First, let's expand and rearrange the numerator, $S(\\alpha)$:\n$$ S(\\alpha) = 1 - 2\\alpha + \\alpha^2 + \\frac{\\alpha^2}{K-1} - \\frac{1}{K} $$\n$$ S(\\alpha) = \\left(1 - \\frac{1}{K}\\right) - 2\\alpha + \\alpha^2 \\left(1 + \\frac{1}{K-1}\\right) $$\n$$ S(\\alpha) = \\frac{K-1}{K} - 2\\alpha + \\alpha^2 \\left(\\frac{K-1+1}{K-1}\\right) $$\n$$ S(\\alpha) = \\frac{K-1}{K} - 2\\alpha + \\alpha^2 \\frac{K}{K-1} $$\nNow we form the ratio:\n$$ R(K,\\alpha) = \\frac{S(\\alpha)}{S(0)} = \\frac{\\frac{K-1}{K} - 2\\alpha + \\alpha^2 \\frac{K}{K-1}}{\\frac{K-1}{K}} $$\nDividing each term in the numerator by the denominator:\n$$ R(K,\\alpha) = 1 - \\frac{2\\alpha}{\\frac{K-1}{K}} + \\frac{\\alpha^2 \\frac{K}{K-1}}{\\frac{K-1}{K}} $$\n$$ R(K,\\alpha) = 1 - \\frac{2K\\alpha}{K-1} + \\alpha^2 \\frac{K^2}{(K-1)^2} $$\nThis resulting expression is a perfect square of the form $(a-b)^2 = a^2 - 2ab + b^2$, with $a=1$ and $b=\\frac{K\\alpha}{K-1}$:\n$$ R(K,\\alpha) = \\left(1 - \\frac{K\\alpha}{K-1}\\right)^2 $$\nThis is the final simplified analytic expression for the multiplicative variance factor.",
            "answer": "$$\n\\boxed{\\left(1 - \\frac{K\\alpha}{K-1}\\right)^2}\n$$"
        }
    ]
}