{
    "hands_on_practices": [
        {
            "introduction": "在处理现实世界的数据时，我们经常会遇到缺失值或可变长度的序列（例如在自然语言处理中对句子进行填充）。本练习将探讨如何调整层归一化以通过掩码（mask）忽略这些无效部分，这是一种常见的技术。通过这个实践 ，你将回归均值和方差的基本定义，推导它们的掩码版本，并批判性地思考当归一化仅基于少量数据点时可能出现的数值稳定性问题。",
            "id": "3142018",
            "problem": "深度神经网络中的单个输入样本包含一个特征向量 $x \\in \\mathbb{R}^{d}$ 和一个二元有效性掩码 $m \\in \\{0,1\\}^{d}$，其中 $m_{i} = 1$ 表示特征 $x_{i}$ 是有效的，$m_{i} = 0$ 表示其无效或缺失。层归一化 (Layer Normalization, LN) 针对每个样本在特征维度上进行应用。请从有限实数集上的算术平均值和方差的基本定义出发，并采用除以被平均元素数量的总体方差约定。\n\n任务 1：推导带掩码的均值 $\\mu_{m}$ 和带掩码的方差 $\\sigma_{m}^{2}$ 的表达式，这些表达式仅使用由 $m$ 指示的有效特征。你的推导必须从算术平均值和方差的定义开始，并且必须与对子集 $\\{i : m_{i} = 1\\}$ 求平均的操作一致。\n\n任务 2：考虑一种带掩码的层归一化，它使用任务 1 中带掩码的统计量，并仅对有效特征应用归一化。具体操作为对每个有效索引 $i$ 执行以下序列：减去带掩码的均值，除以带掩码的方差与一个严格为正的平滑参数 $\\epsilon$ 之和的平方根，然后应用一个参数为 $\\gamma$ 和 $\\beta$ 的标量仿射变换。无效特征保持不变。对于具体实例 $d = 4$，$x = (2, -1, 3, 0)$，$m = (1, 0, 1, 0)$，$\\gamma = 1$，$\\beta = 0$ 和 $\\epsilon = 1 \\times 10^{-5}$，计算索引 $i = 1$ 处的归一化输出值。报告你的数值答案，并四舍五入到四位有效数字。\n\n任务 3：使用你在任务 1 中得到的表达式，分析当 $\\sum_{i=1}^{d} m_{i}$ 很小时，带掩码的层归一化的数值稳定性。你的分析应讨论方差估计量的行为、$\\epsilon$ 的作用以及归一化因子的潜在量级。任务 3 不需要数值答案。",
            "solution": "我们从基本定义开始。对于一个有限的实数多重集 $\\{z_{1}, z_{2}, \\dots, z_{k}\\}$，其算术平均值由下式给出：\n$$\n\\bar{z} \\;=\\; \\frac{1}{k} \\sum_{j=1}^{k} z_{j},\n$$\n并且总体方差（即除以 $k$ 的二阶中心矩）为：\n$$\n\\operatorname{Var}(z) \\;=\\; \\frac{1}{k} \\sum_{j=1}^{k} (z_{j} - \\bar{z})^{2}.\n$$\n在我们带掩码的设定中，令 $K = \\sum_{i=1}^{d} m_{i}$ 表示有效特征的数量，并令有效索引集为 $S = \\{i \\in \\{1,\\dots,d\\} : m_{i} = 1\\}$。带掩码的均值仅对有效条目进行聚合。对集合 $\\{x_{i} : i \\in S\\}$ 使用均值定义，我们得到：\n$$\n\\mu_{m} \\;=\\; \\frac{1}{K} \\sum_{i \\in S} x_{i}.\n$$\n等价地，由于 $m_{i} \\in \\{0,1\\}$ 用于选择有效特征，\n$$\n\\mu_{m} \\;=\\; \\frac{\\sum_{i=1}^{d} m_{i} x_{i}}{\\sum_{i=1}^{d} m_{i}}.\n$$\n接下来，带掩码的方差可将总体方差定义直接应用于相同的有效集得到：\n$$\n\\sigma_{m}^{2} \\;=\\; \\frac{1}{K} \\sum_{i \\in S} (x_{i} - \\mu_{m})^{2} \\;=\\; \\frac{\\sum_{i=1}^{d} m_{i} (x_{i} - \\mu_{m})^{2}}{\\sum_{i=1}^{d} m_{i}}.\n$$\n这就完成了任务 1。\n\n对于任务 2，带掩码的层归一化仅在 $m_{i} = 1$ 的索引 $i$ 处应用归一化，通过执行带有平滑参数和仿射参数的标准化变换。具体来说，对于有效的 $i$，\n$$\ny_{i} \\;=\\; \\gamma \\,\\frac{x_{i} - \\mu_{m}}{\\sqrt{\\sigma_{m}^{2} + \\epsilon}} \\;+\\; \\beta,\n$$\n而对于无效的 $i$（其中 $m_{i} = 0$），我们设置 $y_{i} = x_{i}$。当 $x = (2, -1, 3, 0)$，$m = (1, 0, 1, 0)$ 时，我们有 $K = \\sum_{i=1}^{4} m_{i} = 2$。带掩码的均值为：\n$$\n\\mu_{m} \\;=\\; \\frac{m_{1} x_{1} + m_{2} x_{2} + m_{3} x_{3} + m_{4} x_{4}}{m_{1} + m_{2} + m_{3} + m_{4}} \\;=\\; \\frac{1 \\cdot 2 + 0 \\cdot (-1) + 1 \\cdot 3 + 0 \\cdot 0}{2} \\;=\\; \\frac{5}{2} \\;=\\; 2.5.\n$$\n带掩码的方差为：\n$$\n\\sigma_{m}^{2} \\;=\\; \\frac{m_{1} (x_{1} - \\mu_{m})^{2} + m_{2} (x_{2} - \\mu_{m})^{2} + m_{3} (x_{3} - \\mu_{m})^{2} + m_{4} (x_{4} - \\mu_{m})^{2}}{K}\n\\;=\\; \\frac{(2 - 2.5)^{2} + (3 - 2.5)^{2}}{2}\n\\;=\\; \\frac{0.25 + 0.25}{2}\n\\;=\\; 0.25.\n$$\n当 $\\gamma = 1$，$\\beta = 0$ 且 $\\epsilon = 1 \\times 10^{-5}$ 时，分母为：\n$$\n\\sqrt{\\sigma_{m}^{2} + \\epsilon} \\;=\\; \\sqrt{0.25 + 1 \\times 10^{-5}} \\;=\\; \\sqrt{0.25001}.\n$$\n因此，\n$$\ny_{1} \\;=\\; 1 \\cdot \\frac{2 - 2.5}{\\sqrt{0.25001}} + 0 \\;=\\; \\frac{-0.5}{\\sqrt{0.25001}}.\n$$\n为了获得数值，注意到 $\\sqrt{0.25001}$ 略大于 $0.5$。使用恒等式 $\\sqrt{0.25 + a} = 0.5 \\sqrt{1 + 4a}$，当 $a = 10^{-5}$ 时，我们有 $\\sqrt{0.25001} = 0.5 \\sqrt{1 + 4 \\times 10^{-5}}$。对于小的 $\\delta$，使用一阶展开式 $\\sqrt{1 + \\delta} \\approx 1 + \\frac{\\delta}{2}$，并取 $\\delta = 4 \\times 10^{-5}$，得到：\n$$\n\\sqrt{0.25001} \\;\\approx\\; 0.5 \\left(1 + 2 \\times 10^{-5}\\right) \\;=\\; 0.50001.\n$$\n因此，\n$$\ny_{1} \\;\\approx\\; \\frac{-0.5}{0.50001} \\;=\\; -\\frac{0.5}{0.50001} \\;=\\; -\\frac{1}{1 + 2 \\times 10^{-5}} \\;\\approx\\; -(1 - 2 \\times 10^{-5}) \\;=\\; -0.99998.\n$$\n四舍五入到四位有效数字，结果是 $-1.000$。\n\n对于任务 3，我们分析当 $K = \\sum_{i=1}^{d} m_{i}$ 很小时的稳定性。带掩码的方差\n$$\n\\sigma_{m}^{2} \\;=\\; \\frac{1}{K} \\sum_{i \\in S} (x_{i} - \\mu_{m})^{2}\n$$\n是一个在 $K$ 个样本上计算的估计量。当 $K$ 很小时，这个估计量会表现出高方差（在统计意义上），如果有效值几乎相等，这会使分母 $\\sqrt{\\sigma_{m}^{2} + \\epsilon}$ 可能变得非常小，或者在不同样本间变化很大。如果没有平滑项 $\\epsilon  0$，分母可能会趋近于零，从而产生极大的归一化值和不稳定的梯度。平滑项 $\\epsilon$ 为分母提供了一个下界 $\\sqrt{\\epsilon}$，从而得到量级界限：\n$$\n\\left|\\frac{x_{i} - \\mu_{m}}{\\sqrt{\\sigma_{m}^{2} + \\epsilon}}\\right| \\;\\leq\\; \\frac{|x_{i} - \\mu_{m}|}{\\sqrt{\\epsilon}},\n$$\n因此，适当地选择 $\\epsilon$ 可以缓解数值爆炸。然而，小的 $K$ 值也使得 $\\mu_{m}$ 本身对单个特征敏感，这会放大 $x_{i} - \\mu_{m}$ 的波动。在 $K = 1$ 的极端情况下，对于唯一的有效索引 $i^{\\star}$，我们有 $\\mu_{m} = x_{i^{\\star}}$，导致 $\\sigma_{m}^{2} = 0$ 且归一化偏差 $x_{i^{\\star}} - \\mu_{m} = 0$，因此 $y_{i^{\\star}} = \\beta$ 是良性的；当 $K$ 很小但至少为 2，且有效值几乎相同或方差估计有噪声时，更容易出现不稳定性。因此，实际的稳定性取决于 $K$、集合 $\\{x_{i} : i \\in S\\}$ 的离散程度以及 $\\epsilon$ 和 $\\gamma$ 的选择之间的相互作用；当 $K$ 很小时，较大的 $\\epsilon$ 和适中的 $\\gamma$ 可以降低敏感性。",
            "answer": "$$\\boxed{-1.000}$$"
        },
        {
            "introduction": "在理解了数值稳定性的重要性之后，让我们通过一个极限情况来加深理解。本练习  旨在引导你分析当输入特征的方差趋近于零时的“病态”场景。通过计算梯度并观察其在退化方向上的行为，你将从数学上深刻领会，为何层归一化分母中的微小常数 $\\epsilon$ 不只是一个可有可无的细节，而是保障模型训练稳定性的关键所在。",
            "id": "3142048",
            "problem": "考虑对一个特征维度 $d=3$ 的层单次应用层归一化（Layer Normalization, LN）。对于一个输入向量 $x \\in \\mathbb{R}^{3}$，LN 计算特征的均值 $\\mu(x)$ 和方差 $\\sigma^{2}(x)$，并返回归一化输出 $y(x)$，其共享的缩放参数 $\\gamma$ 和共享的平移参数 $\\beta$ 定义如下：\n$$\n\\mu(x) = \\frac{1}{3}\\sum_{i=1}^{3} x_{i},\\qquad\n\\sigma^{2}(x) = \\frac{1}{3}\\sum_{i=1}^{3} \\big(x_{i}-\\mu(x)\\big)^{2},\\qquad\ny_{i}(x) = \\gamma \\,\\frac{x_{i}-\\mu(x)}{\\sqrt{\\sigma^{2}(x)+\\epsilon}} + \\beta,\n$$\n其中 $\\epsilon0$ 是一个很小的稳定常数。\n\n通过令以下表达式，构造一个接近退化方向（接近零方差）的病态输入序列：\n$$\nx(\\alpha) = c\\,\\mathbf{1} + \\alpha\\,v,\\quad \\text{with}\\quad \\mathbf{1}=\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix},\\quad v=\\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix},\\quad c\\in\\mathbb{R},\\quad \\alpha0,\n$$\n并选择 $\\beta=0$ 和一个共享的正缩放参数 $\\gamma=g0$。将标量损失函数定义为第一个 LN 输出坐标：\n$$\nL(x) = y_{1}(x).\n$$\n\n任务：\n1. 仅从 $\\mu(x)$ 和 $\\sigma^{2}(x)$ 的定义出发，证明沿着 $x(\\alpha)$，有 $\\mu\\big(x(\\alpha)\\big)=c$ 和 $\\sigma^{2}\\big(x(\\alpha)\\big)=\\alpha^{2}\\,s_{v}^{2}$，其中 $s_{v}^{2}=\\frac{1}{3}\\sum_{i=1}^{3} v_{i}^{2}$。\n2. 使用链式法则和上述核心定义，推导对于通用 $\\alpha0$ 和 $\\epsilon0$ 的梯度 $\\nabla_{x}L\\big(x(\\alpha)\\big)$。\n3. 当 $\\alpha \\to 0^{+}$（等价于 $\\sigma^{2}\\to 0$）时，计算该梯度的极限，并以行矩阵的形式报告关于 $x$ 的极限梯度向量。此行矩阵将是您最终报告的答案。\n4. 在您的推导中，通过比较在假设 $\\epsilon=0$ 时您获得的缩放情况，解释 $\\epsilon0$ 的存在如何防止梯度在退化方向附近发生“爆炸”。\n\n不需要进行数值四舍五入。您的最终答案必须是单一的极限梯度向量，以封闭形式表示，用 $g$ 和 $\\epsilon$ 进行符号表达，并以行矩阵的形式报告。",
            "solution": "问题要求推导和分析层归一化（LN）输出相对于其输入的梯度，特别是在接近零方差的极限情况下。\n\n首先，我将验证问题陈述。\n\n### 步骤 1：提取已知条件\n-   **定义域**：输入向量 $x \\in \\mathbb{R}^{3}$（特征维度 $d=3$）。\n-   **层归一化定义**：\n    -   均值：$\\mu(x) = \\frac{1}{3}\\sum_{i=1}^{3} x_{i}$\n    -   方差：$\\sigma^{2}(x) = \\frac{1}{3}\\sum_{i=1}^{3} \\big(x_{i}-\\mu(x)\\big)^{2}$\n    -   输出：$y_{i}(x) = \\gamma \\,\\frac{x_{i}-\\mu(x)}{\\sqrt{\\sigma^{2}(x)+\\epsilon}} + \\beta$\n-   **稳定常数**：$\\epsilon0$。\n-   **病态输入序列**：$x(\\alpha) = c\\,\\mathbf{1} + \\alpha\\,v$，其中 $\\mathbf{1}=\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}$，$v=\\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix}$，$c\\in\\mathbb{R}$，且 $\\alpha0$。\n-   **参数**：共享平移 $\\beta=0$，共享正缩放 $\\gamma=g0$。\n-   **损失函数**：$L(x) = y_{1}(x)$。\n-   **辅助定义**：$s_{v}^{2}=\\frac{1}{3}\\sum_{i=1}^{3} v_{i}^{2}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题具有科学依据，定义明确且客观。\n-   **科学依据**：该问题植根于深度学习领域，这是计算机科学和应用数学的一个子领域。层归一化的定义是标准的。在退化情况下对梯度进行分析是理解神经网络优化算法数值稳定性的一个关键方面。\n-   **定义明确**：该问题提供了执行所要求的推导和分析所需的所有必要定义、约束和变量。它要求一个特定的极限梯度，如下文所示，在给定条件下该梯度存在且唯一。\n-   **客观性**：该问题以精确的数学语言陈述，没有主观或含糊的术语。\n\n### 步骤 3：结论与行动\n该问题被视为**有效**。我将继续进行解答。\n\n解决方案根据问题中列出的四个任务进行结构化。维度固定为 $d=3$。\n\n### 任务 1：$x(\\alpha)$ 的均值和方差\n输入序列定义为 $x(\\alpha) = c\\,\\mathbf{1} + \\alpha\\,v$。用分量形式表示为 $x_i(\\alpha) = c + \\alpha v_i$，其中 $i \\in \\{1, 2, 3\\}$。\n\n首先，我们计算均值 $\\mu\\big(x(\\alpha)\\big)$：\n$$\n\\mu\\big(x(\\alpha)\\big) = \\frac{1}{3}\\sum_{i=1}^{3} x_i(\\alpha) = \\frac{1}{3}\\sum_{i=1}^{3} (c + \\alpha v_i) = \\frac{1}{3} \\left( \\sum_{i=1}^{3} c + \\alpha \\sum_{i=1}^{3} v_i \\right)\n$$\n向量 $v = \\begin{pmatrix}1\\\\0\\\\-1\\end{pmatrix}$ 的分量之和为 $\\sum_{i=1}^{3} v_i = 1+0+(-1) = 0$。和 $\\sum_{i=1}^{3} c = 3c$。\n因此，均值为：\n$$\n\\mu\\big(x(\\alpha)\\big) = \\frac{1}{3} (3c + \\alpha \\cdot 0) = c\n$$\n这表明均值在定义的路径上是常数，等于 $c$。\n\n接下来，我们计算方差 $\\sigma^2\\big(x(\\alpha)\\big)$：\n$$\n\\sigma^2\\big(x(\\alpha)\\big) = \\frac{1}{3}\\sum_{i=1}^{3} \\big(x_i(\\alpha) - \\mu(x(\\alpha))\\big)^2\n$$\n使用上述结果，平方内的项为 $x_i(\\alpha) - \\mu(x(\\alpha)) = (c + \\alpha v_i) - c = \\alpha v_i$。\n将此代入方差定义：\n$$\n\\sigma^2\\big(x(\\alpha)\\big) = \\frac{1}{3}\\sum_{i=1}^{3} (\\alpha v_i)^2 = \\frac{1}{3}\\sum_{i=1}^{3} \\alpha^2 v_i^2 = \\alpha^2 \\left( \\frac{1}{3}\\sum_{i=1}^{3} v_i^2 \\right)\n$$\n根据问题的定义，$s_v^2 = \\frac{1}{3}\\sum_{i=1}^{3} v_i^2$。这给出了最终结果：\n$$\n\\sigma^2\\big(x(\\alpha)\\big) = \\alpha^2 s_v^2\n$$\n\n### 任务 2：梯度推导\n损失函数为 $L(x) = y_1(x)$。参数为 $\\gamma=g$ 和 $\\beta=0$。我们有：\n$$\nL(x) = y_1(x) = g \\frac{x_1 - \\mu(x)}{\\sqrt{\\sigma^2(x) + \\epsilon}}\n$$\n我们需要求梯度 $\\nabla_x L(x) = \\begin{pmatrix} \\frac{\\partial L}{\\partial x_1}   \\frac{\\partial L}{\\partial x_2}   \\frac{\\partial L}{\\partial x_3} \\end{pmatrix}$。我们来求对于任意分量 $k \\in \\{1, 2, 3\\}$ 的偏导数 $\\frac{\\partial L}{\\partial x_k}$。\n\n为了清晰起见，我们引入中间变量：$n_i(x) = x_i - \\mu(x)$ 和 $s(x) = \\sqrt{\\sigma^2(x) + \\epsilon}$。损失函数为 $L(x) = g \\, \\frac{n_1(x)}{s(x)}$。\n使用商法则，梯度为：\n$$\n\\nabla_x L(x) = g \\left( \\frac{\\nabla_x n_1(x)}{s(x)} - \\frac{n_1(x)}{s(x)^2} \\nabla_x s(x) \\right)\n$$\n我们需要 $n_1(x)$ 和 $s(x)$ 的梯度。维度为 $d=3$。\n均值 $\\mu(x) = \\frac{1}{3}\\sum_j x_j$ 的梯度是 $\\nabla_x \\mu(x) = \\frac{1}{3}\\mathbf{1}^T = \\begin{pmatrix} \\frac{1}{3}   \\frac{1}{3}   \\frac{1}{3} \\end{pmatrix}$。\n$n_1(x) = x_1 - \\mu(x)$ 的梯度是 $\\nabla_x n_1(x) = \\nabla_x x_1 - \\nabla_x \\mu(x) = e_1^T - \\frac{1}{3}\\mathbf{1}^T$，其中 $e_1^T = \\begin{pmatrix} 1   0   0 \\end{pmatrix}$。所以，$\\nabla_x n_1(x) = \\begin{pmatrix} \\frac{2}{3}   -\\frac{1}{3}   -\\frac{1}{3} \\end{pmatrix}$。\n\n接下来，求 $\\nabla_x s(x)$：\n$$\n\\nabla_x s(x) = \\nabla_x \\left( \\sigma^2(x) + \\epsilon \\right)^{1/2} = \\frac{1}{2s(x)} \\nabla_x \\sigma^2(x)\n$$\n方差 $\\sigma^2(x) = \\frac{1}{3}\\sum_j (x_j-\\mu)^2$ 的梯度为：\n$$\n\\frac{\\partial \\sigma^2}{\\partial x_k} = \\frac{1}{3}\\sum_j 2(x_j-\\mu)\\left(\\frac{\\partial x_j}{\\partial x_k} - \\frac{\\partial \\mu}{\\partial x_k}\\right) = \\frac{2}{3}\\sum_j(x_j-\\mu)(\\delta_{jk} - \\frac{1}{3}) = \\frac{2}{3}\\left((x_k-\\mu) - \\frac{1}{3}\\sum_j(x_j-\\mu)\\right)\n$$\n由于 $\\sum_j(x_j-\\mu) = 0$，我们有 $\\frac{\\partial \\sigma^2}{\\partial x_k} = \\frac{2}{3}(x_k-\\mu) = \\frac{2}{3}n_k(x)$。\n以向量形式表示，$\\nabla_x \\sigma^2(x) = \\frac{2}{3}n(x)$，其中 $n(x)$ 是分量为 $n_k(x)$ 的向量。\n所以，$\\nabla_x s(x) = \\frac{1}{2s(x)} \\frac{2}{3}n(x) = \\frac{n(x)}{3s(x)}$。\n\n将这些梯度代回 $\\nabla_x L(x)$ 的表达式中：\n$$\n\\nabla_x L(x) = g \\left( \\frac{e_1^T - \\frac{1}{3}\\mathbf{1}^T}{s(x)} - \\frac{n_1(x)}{s(x)^2} \\frac{n(x)}{3s(x)} \\right) = \\frac{g}{s(x)}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g n_1(x)}{3s(x)^3}n(x)\n$$\n这是一般的梯度表达式。现在我们在 $x=x(\\alpha)$ 处计算它：\n-   $n(x(\\alpha)) = x(\\alpha) - \\mu(x(\\alpha))\\mathbf{1} = (c\\mathbf{1} + \\alpha v) - c\\mathbf{1} = \\alpha v$。\n-   $n_1(x(\\alpha)) = \\alpha v_1$。\n-   $s(x(\\alpha)) = \\sqrt{\\sigma^2(x(\\alpha)) + \\epsilon} = \\sqrt{\\alpha^2 s_v^2 + \\epsilon}$。\n\n将这些代入梯度公式：\n$$\n\\nabla_x L\\big(x(\\alpha)\\big) = \\frac{g}{\\sqrt{\\alpha^2 s_v^2 + \\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g (\\alpha v_1)}{3(\\alpha^2 s_v^2 + \\epsilon)^{3/2}}(\\alpha v)\n$$\n$$\n\\nabla_x L\\big(x(\\alpha)\\big) = \\frac{g}{\\sqrt{\\alpha^2 s_v^2 + \\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g \\alpha^2 v_1}{3(\\alpha^2 s_v^2 + \\epsilon)^{3/2}}v\n$$\n\n### 任务 3：当 $\\alpha \\to 0^+$ 时的极限梯度\n我们现在取所推导梯度在 $\\alpha \\to 0^+$ 时的极限：\n$$\n\\lim_{\\alpha \\to 0^+} \\nabla_x L\\big(x(\\alpha)\\big) = \\lim_{\\alpha \\to 0^+} \\left( \\frac{g}{\\sqrt{\\alpha^2 s_v^2 + \\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g \\alpha^2 v_1}{3(\\alpha^2 s_v^2 + \\epsilon)^{3/2}}v \\right)\n$$\n我们分别计算每一项的极限。\n对于第一项，当 $\\alpha \\to 0^+$ 时，分母变为 $\\sqrt{0 + \\epsilon} = \\sqrt{\\epsilon}$。\n$$\n\\lim_{\\alpha \\to 0^+} \\frac{g}{\\sqrt{\\alpha^2 s_v^2 + \\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) = \\frac{g}{\\sqrt{\\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right)\n$$\n对于第二项，分子包含一个因子 $\\alpha^2$，它趋向于 $0$。分母趋向于 $3\\epsilon^{3/2}$，这是一个非零常数，因为 $\\epsilon  0$。\n$$\n\\lim_{\\alpha \\to 0^+} \\frac{g \\alpha^2 v_1}{3(\\alpha^2 s_v^2 + \\epsilon)^{3/2}}v = \\frac{g \\cdot 0 \\cdot v_1}{3\\epsilon^{3/2}}v = \\mathbf{0}\n$$\n结合这些极限，我们得到：\n$$\n\\lim_{\\alpha \\to 0^+} \\nabla_x L\\big(x(\\alpha)\\big) = \\frac{g}{\\sqrt{\\epsilon}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right)\n$$\n代入向量 $e_1^T = \\begin{pmatrix} 1   0   0 \\end{pmatrix}$ 和 $\\mathbf{1}^T = \\begin{pmatrix} 1   1   1 \\end{pmatrix}$：\n$$\n\\lim_{\\alpha \\to 0^+} \\nabla_x L\\big(x(\\alpha)\\big) = \\frac{g}{\\sqrt{\\epsilon}}\\left(\\begin{pmatrix} 1   0   0 \\end{pmatrix} - \\frac{1}{3}\\begin{pmatrix} 1   1   1 \\end{pmatrix}\\right) = \\frac{g}{\\sqrt{\\epsilon}}\\begin{pmatrix} 1-\\frac{1}{3}   0-\\frac{1}{3}   0-\\frac{1}{3} \\end{pmatrix}\n$$\n$$\n\\lim_{\\alpha \\to 0^+} \\nabla_x L\\big(x(\\alpha)\\big) = \\frac{g}{\\sqrt{\\epsilon}}\\begin{pmatrix} \\frac{2}{3}   -\\frac{1}{3}   -\\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{2g}{3\\sqrt{\\epsilon}}   -\\frac{g}{3\\sqrt{\\epsilon}}   -\\frac{g}{3\\sqrt{\\epsilon}} \\end{pmatrix}\n$$\n这就是极限梯度向量。\n\n### 任务 4：$\\epsilon  0$ 的作用\n为了理解 $\\epsilon$ 的作用，我们比较在有和没有 $\\epsilon$ 的情况下，梯度大小在 $\\alpha \\to 0^+$ 时的行为。\n\n**情况 1：$\\epsilon  0$（如上所述）**\n当 $\\alpha \\to 0^+$ 时，梯度 $\\nabla_x L\\big(x(\\alpha)\\big)$ 趋近于一个有限的常数向量。极限梯度的模为：\n$$\n\\left\\| \\lim_{\\alpha \\to 0^+} \\nabla_x L\\big(x(\\alpha)\\big) \\right\\| = \\left\\| \\frac{g}{\\sqrt{\\epsilon}}\\begin{pmatrix} \\frac{2}{3}   -\\frac{1}{3}   -\\frac{1}{3} \\end{pmatrix} \\right\\| = \\frac{g}{\\sqrt{\\epsilon}} \\sqrt{(\\frac{2}{3})^2 + (-\\frac{1}{3})^2 + (-\\frac{1}{3})^2} = \\frac{g}{\\sqrt{\\epsilon}}\\sqrt{\\frac{4+1+1}{9}} = \\frac{g\\sqrt{6}}{3\\sqrt{\\epsilon}}\n$$\n这是一个有限的非零值。$\\epsilon0$ 的存在确保了 LN 公式中的分母 $\\sqrt{\\sigma^2(x)+\\epsilon}$ 的下界为 $\\sqrt{\\epsilon}$，从而防止了梯度的“爆炸”。\n\n**情况 2：假设 $\\epsilon = 0$**\n如果我们设置 $\\epsilon=0$，梯度表达式变为：\n$$\n\\nabla_x L\\big(x(\\alpha)\\big) \\Big|_{\\epsilon=0} = \\frac{g}{\\sqrt{\\alpha^2 s_v^2}}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g \\alpha^2 v_1}{3(\\alpha^2 s_v^2)^{3/2}}v\n$$\n由于 $\\alpha  0$，$\\sqrt{\\alpha^2 s_v^2} = \\alpha s_v$。\n$$\n\\nabla_x L\\big(x(\\alpha)\\big) \\Big|_{\\epsilon=0} = \\frac{g}{\\alpha s_v}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g \\alpha^2 v_1}{3\\alpha^3 s_v^3}v = \\frac{1}{\\alpha}\\left[ \\frac{g}{s_v}\\left(e_1^T - \\frac{1}{3}\\mathbf{1}^T\\right) - \\frac{g v_1}{3 s_v^3}v \\right]\n$$\n方括号中的项是一个通常非零的常数向量。因此，梯度向量的模与 $1/\\alpha$ 成比例。当 $\\alpha \\to 0^+$ 时，方差 $\\sigma^2 = \\alpha^2 s_v^2 \\to 0$，梯度的模发散：\n$$\n\\left\\| \\nabla_x L\\big(x(\\alpha)\\big) \\Big|_{\\epsilon=0} \\right\\| \\to \\infty\n$$\n梯度的这种“爆炸”对应于优化算法中的数值不稳定性。稳定常数 $\\epsilon  0$至关重要，因为它确保了即使对于方差接近于零的输入，归一化分母仍然保持大于零，从而产生行为良好、有限的梯度。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2g}{3\\sqrt{\\epsilon}}   -\\frac{g}{3\\sqrt{\\epsilon}}   -\\frac{g}{3\\sqrt{\\epsilon}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在掌握了层归一化的内部工作原理后，是时候将视野拉高，审视它对整个训练过程的宏观影响。本练习  将通过分析一个简化的双层网络，让你从理论上探究层归一化如何重塑特征空间，进而影响优化过程。通过推导其对训练动态和最大稳定学习率的影响，你将为“层归一化为何能加速并稳定训练”这一问题找到一个清晰的数学解释。",
            "id": "3142044",
            "problem": "考虑一个标量输出的双层线性网络，层间使用层归一化（Layer Normalization, LN）。设隐藏层宽度为 $H \\geq 2$。设输入 $x \\in \\mathbb{R}^{H}$ 来自零均值、单位协方差的高斯分布，即 $x \\sim \\mathcal{N}(0, I_{H})$。目标由一个线性教师模型生成，即 $y = t^{\\top} x$，其中 $t \\in \\mathbb{R}^{H}$ 是一个固定但任意的向量。我们比较两个模型，它们唯一的区别在于两个线性层之间是否存在 LN。我们仅通过在总体最小二乘风险上使用全批量梯度下降（Gradient Descent, GD）来训练第二层的权重。\n\n不含 LN 的模型：网络输出为 $f_{\\mathrm{noLN}}(x; u) = u^{\\top} x$，其中 $u \\in \\mathbb{R}^{H}$ 是可训练的顶层权重。\n\n包含 LN 的模型：网络输出为 $f_{\\mathrm{LN}}(x; u) = u^{\\top} h$，其中 $h = \\mathrm{LN}(x)$ 是对每个样本跨特征计算的层归一化输入向量，不含仿射参数：\n$$\nm(x) = \\frac{1}{H} \\mathbf{1}^{\\top} x, \n\\quad \ns(x) = \\sqrt{\\frac{1}{H} \\sum_{i=1}^{H} \\bigl(x_{i} - m(x)\\bigr)^{2}},\n\\quad \nh = \\frac{x - m(x)\\mathbf{1}}{s(x)} \\in \\mathbb{R}^{H}.\n$$\n此处 $\\mathbf{1} \\in \\mathbb{R}^{H}$ 表示全1向量。\n\n在这两种情况下，我们都最小化总体风险\n$$\n\\mathcal{L}(u) = \\frac{1}{2}\\,\\mathbb{E}\\bigl[(f(x; u) - y)^{2}\\bigr]\n$$\n使用固定步长 $\\eta  0$ 的全批量梯度下降法：\n$$\nu_{k+1} = u_{k} - \\eta \\nabla \\mathcal{L}(u_{k}).\n$$\n\n任务：\n1) 从最小二乘总体风险的定义和梯度下降更新规则出发，推导两种模型的闭式训练动态 $u_{k}$。你的推导应将 $u_{k}$ 表示为 $u_{0}$、$\\eta$ 和相关特征二阶统计量的函数，并应明确特征协方差在动态过程中的作用。\n\n2) 对于 LN 模型，根据上述高斯输入和逐样本 LN 的定义，确定特征协方差矩阵 $\\Sigma_{\\mathrm{LN}} = \\mathbb{E}[h h^{\\top}]$ 的谱。使用对称性和 LN 施加的约束来证明你的结果。\n\n3) 使用你的结果，确定在每个模型中保证损失单调收敛（等价于线性 GD 动态的稳定性）的最大步长 $\\eta_{\\max}$。\n\n4) 最后，计算这些最大稳定学习率的比率 $r(H)$，\n$$\nr(H) = \\frac{\\eta_{\\max}^{\\mathrm{LN}}}{\\eta_{\\max}^{\\mathrm{noLN}}},\n$$\n以 $H$ 的闭式表达式表示。以单一解析表达式的形式提供最终答案。无需四舍五入。",
            "solution": "用户希望分析两个线性模型（一个带层归一化LN，一个不带）的训练动态，以确定每种模型的最大稳定学习率及其比率。\n\n### 步骤 1: 提取已知条件\n- 输入：$x \\in \\mathbb{R}^{H}$，$x \\sim \\mathcal{N}(0, I_{H})$，其中 $H \\geq 2$ 是隐藏层宽度。\n- 目标：$y = t^{\\top} x$，对于一个固定向量 $t \\in \\mathbb{R}^{H}$。\n- 不带 LN 的模型：$f_{\\mathrm{noLN}}(x; u) = u^{\\top} x$，其中 $u \\in \\mathbb{R}^{H}$ 是可训练的权重。\n- 带 LN 的模型：$f_{\\mathrm{LN}}(x; u) = u^{\\top} h$，其中 $h = \\mathrm{LN}(x)$ 是归一化输入。\n- LN 定义（无仿射参数）：\n  - 均值：$m(x) = \\frac{1}{H} \\mathbf{1}^{\\top} x$\n  - 标准差：$s(x) = \\sqrt{\\frac{1}{H} \\sum_{i=1}^{H} \\bigl(x_{i} - m(x)\\bigr)^{2}}$\n  - 归一化向量：$h = \\frac{x - m(x)\\mathbf{1}}{s(x)}$\n  - $\\mathbf{1} \\in \\mathbb{R}^{H}$ 是全1向量。\n- 风险函数：$\\mathcal{L}(u) = \\frac{1}{2}\\,\\mathbb{E}\\bigl[(f(x; u) - y)^{2}\\bigr]$。\n- 训练算法：全批量梯度下降（GD），固定步长 $\\eta  0$。\n- 更新规则：$u_{k+1} = u_{k} - \\eta \\nabla \\mathcal{L}(u_{k})$。\n\n### 步骤 2: 使用提取的已知条件进行验证\n- **科学依据：**该问题是深度学习领域一个定义明确的理论练习，基于优化和统计学的标准数学原理。它在科学上是合理的。\n- **适定性：**问题提供了所有必要的定义，并要求求解具体的、可推导的量。这些任务导向一个唯一且有意义的解。\n- **客观性：**问题使用精确的数学语言陈述，没有歧义或主观声明。\n\n问题陈述是完整的、一致的并且在科学上是有效的。\n\n### 步骤 3: 结论与行动\n问题是有效的。我将继续进行求解。\n\n### 任务 1: 推导训练动态\n\n风险是权重 $u$ 的二次函数。梯度的一般形式为：\n$$\n\\nabla_u \\mathcal{L}(u) = \\nabla_u \\frac{1}{2}\\,\\mathbb{E}\\bigl[(u^{\\top} z - y)^{2}\\bigr] = \\mathbb{E}\\bigl[(u^{\\top} z - y)z\\bigr] = \\mathbb{E}[zz^{\\top}]u - \\mathbb{E}[zy^{\\top}]\n$$\n其中 $z$ 表示输入到最后一层的特征。GD 更新为 $u_{k+1} = u_k - \\eta(\\mathbb{E}[zz^{\\top}]u_k - \\mathbb{E}[zy^{\\top}])$。这是一个线性动力系统。设 $u^*$ 为不动点（最优权重），满足 $\\nabla_u \\mathcal{L}(u^*) = 0$，即 $\\mathbb{E}[zz^{\\top}]u^* = \\mathbb{E}[zy^{\\top}]$。误差向量 $e_k = u_k - u^*$ 的演化过程如下：\n$$\ne_{k+1} = u_{k+1} - u^* = (I - \\eta\\mathbb{E}[zz^{\\top}])u_k + \\eta\\mathbb{E}[zy^{\\top}] - u^* = (I - \\eta\\mathbb{E}[zz^{\\top}])(e_k+u^*) + \\eta\\mathbb{E}[zy^{\\top}] - u^*\n$$\n$$\ne_{k+1} = (I - \\eta\\mathbb{E}[zz^{\\top}])e_k + u^* - \\eta\\mathbb{E}[zz^{\\top}]u^* + \\eta\\mathbb{E}[zy^{\\top}] - u^* = (I - \\eta\\mathbb{E}[zz^{\\top}])e_k\n$$\n解为 $e_k = (I - \\eta\\mathbb{E}[zz^{\\top}])^k e_0$，从而得到 $u_k$ 的闭式动态：\n$$\nu_k = u^* + (I - \\eta\\mathbb{E}[zz^{\\top}])^k (u_0 - u^*)\n$$\n\n**不带 LN 的模型：**\n对于这个模型，特征向量是 $z=x$。目标是 $y=t^\\top x$。\n所需的二阶统计量是：\n- 特征协方差：$\\Sigma_{\\mathrm{noLN}} = \\mathbb{E}[xx^{\\top}]$。由于 $x \\sim \\mathcal{N}(0, I_H)$，$\\mathbb{E}[x]=0$ 且 $\\mathrm{Cov}(x) = I_H$。因此，$\\Sigma_{\\mathrm{noLN}} = \\mathrm{Cov}(x) + \\mathbb{E}[x]\\mathbb{E}[x]^\\top = I_H$。\n- 互协方差：$\\mathbb{E}[xy^{\\top}] = \\mathbb{E}[x(t^\\top x)^\\top] = \\mathbb{E}[xx^{\\top}]t = \\Sigma_{\\mathrm{noLN}} t = I_H t = t$。\n最优权重 $u^*$ 满足 $\\Sigma_{\\mathrm{noLN}} u^* = \\mathbb{E}[xy^{\\top}]$，即 $I_H u^* = t$，所以 $u^*=t$。\n\n无 LN 模型的训练动态为：\n$$\nu_k = t + (I_H - \\eta I_H)^k (u_0 - t) = t + (1-\\eta)^k(u_0 - t)\n$$\n动态由特征协方差矩阵 $\\Sigma_{\\mathrm{noLN}} = I_H$ 控制。由于它是单位矩阵，误差向量 $u_k - t$ 的所有分量都以相同的速率 $(1-\\eta)$ 衰减。\n\n**带 LN 的模型：**\n对于这个模型，特征向量是 $z=h=\\mathrm{LN}(x)$。目标仍然是 $y=t^\\top x$。\n所需的二阶统计量是：\n- 特征协方差：$\\Sigma_{\\mathrm{LN}} = \\mathbb{E}[hh^{\\top}]$。\n- 交叉项：$C = \\mathbb{E}[hy^{\\top}] = \\mathbb{E}[h(t^\\top x)^\\top] = \\mathbb{E}[hx^{\\top}]t$。\n梯度是 $\\nabla \\mathcal{L}(u) = \\Sigma_{\\mathrm{LN}} u - C$。GD 更新为 $u_{k+1} = u_k - \\eta(\\Sigma_{\\mathrm{LN}} u_k - C)$。\n\n最优权重 $u^*$ 是 $\\Sigma_{\\mathrm{LN}} u^* = C$ 的解。我们将在任务2中证明，$\\Sigma_{\\mathrm{LN}}$ 是奇异矩阵，因此不动点不是唯一的。然而，误差 $u_k - u^*$ 的动态（对于任何有效的不动点 $u^*$）仍然由相同的齐次方程控制。闭式动态为：\n$$\nu_k = u^* + (I_H - \\eta \\Sigma_{\\mathrm{LN}})^k (u_0 - u^*)\n$$\n其中 $u^*$ 是 $\\Sigma_{\\mathrm{LN}} u^* = \\mathbb{E}[hx^{\\top}]t$ 的任意解。特征协方差 $\\Sigma_{\\mathrm{LN}}$ 决定了权重的可学习部分的演化，而交叉项 $\\mathbb{E}[hx^{\\top}]t$ 决定了目标解流形。\n\n### 任务 2: 特征协方差矩阵 $\\Sigma_{\\mathrm{LN}}$ 的谱\n\n根据构造，对于任何 $x$，归一化向量 $h$ 具有两个关键属性：\n1.  其分量之和为零：$\\mathbf{1}^{\\top}h = \\mathbf{1}^{\\top} \\frac{x - m(x)\\mathbf{1}}{s(x)} = \\frac{\\mathbf{1}^{\\top}x - m(x)\\mathbf{1}^{\\top}\\mathbf{1}}{s(x)} = \\frac{H m(x) - m(x)H}{s(x)} = 0$。\n2.  其欧几里得范数的平方为 $H$：$h^{\\top}h = \\frac{(x-m(x)\\mathbf{1})^{\\top}(x-m(x)\\mathbf{1})}{s(x)^2} = \\frac{\\sum_{i=1}^H (x_i-m(x))^2}{\\frac{1}{H}\\sum_{i=1}^H (x_i-m(x))^2} = H$。\n\n现在我们计算 $\\Sigma_{\\mathrm{LN}} = \\mathbb{E}[hh^{\\top}]$。\n根据属性1，取期望：$\\mathbb{E}[\\mathbf{1}^{\\top}h] = \\mathbf{1}^{\\top}\\mathbb{E}[h]=0$。此外，$\\mathbf{1}^{\\top}\\Sigma_{\\mathrm{LN}} = \\mathbf{1}^{\\top}\\mathbb{E}[hh^{\\top}] = \\mathbb{E}[(\\mathbf{1}^{\\top}h)h^{\\top}] = \\mathbb{E}[0 \\cdot h^{\\top}] = \\mathbf{0}^{\\top}$。这意味着 $\\Sigma_{\\mathrm{LN}}\\mathbf{1} = (\\mathbf{1}^{\\top}\\Sigma_{\\mathrm{LN}})^{\\top} = \\mathbf{0}$。因此，$\\mathbf{1}$ 是 $\\Sigma_{\\mathrm{LN}}$ 的一个特征向量，其特征值为 $0$。\n\n根据属性2，取迹的期望：$\\mathrm{Tr}(\\Sigma_{\\mathrm{LN}}) = \\mathrm{Tr}(\\mathbb{E}[hh^{\\top}]) = \\mathbb{E}[\\mathrm{Tr}(hh^{\\top})] = \\mathbb{E}[h^{\\top}h] = \\mathbb{E}[H] = H$。\n\n输入分布 $\\mathcal{N}(0,I_H)$ 在坐标置换下是不变的。LN 是一种置换等变操作。因此，矩阵 $\\Sigma_{\\mathrm{LN}}$ 必须在其索引的置换下保持不变。这意味着它具有以下结构：\n$$\n(\\Sigma_{\\mathrm{LN}})_{ij} = \\begin{cases} c_1   i=j \\\\ c_2  i \\neq j \\end{cases}\n$$\n对于某些常数 $c_1, c_2$。我们可以写作 $\\Sigma_{\\mathrm{LN}} = (c_1-c_2)I_H + c_2\\mathbf{1}\\mathbf{1}^{\\top}$。\n从 $\\mathrm{Tr}(\\Sigma_{\\mathrm{LN}}) = H$，我们得到 $H c_1 = H$，这意味着 $c_1=1$。\n从 $\\Sigma_{\\mathrm{LN}}\\mathbf{1} = \\mathbf{0}$，任何一列的和必须为零：$c_1 + (H-1)c_2 = 0$。\n代入 $c_1=1$，我们得到 $1 + (H-1)c_2 = 0$，所以 $c_2 = -\\frac{1}{H-1}$。\n\n该矩阵为 $\\Sigma_{\\mathrm{LN}} = (1 - (-\\frac{1}{H-1}))I_H + (-\\frac{1}{H-1})\\mathbf{1}\\mathbf{1}^{\\top} = \\frac{H}{H-1}I_H - \\frac{1}{H-1}\\mathbf{1}\\mathbf{1}^{\\top}$。\n我们来求它的谱。我们已经知道一个特征值是 $0$（特征向量为 $\\mathbf{1}$）。\n设 $v$ 是与 $\\mathbf{1}$ 正交的任意向量（即 $\\mathbf{1}^{\\top}v=0$）。存在一个由这类向量构成的 $(H-1)$ 维子空间。\n$$\n\\Sigma_{\\mathrm{LN}} v = \\left(\\frac{H}{H-1}I_H - \\frac{1}{H-1}\\mathbf{1}\\mathbf{1}^{\\top}\\right)v = \\frac{H}{H-1}v - \\frac{1}{H-1}\\mathbf{1}(\\mathbf{1}^{\\top}v) = \\frac{H}{H-1}v - 0 = \\frac{H}{H-1}v\n$$\n因此，在与 $\\mathbf{1}$ 正交的子空间中的任何向量都是特征值为 $\\frac{H}{H-1}$ 的特征向量。\n$\\Sigma_{\\mathrm{LN}}$ 的谱由以下部分组成：\n- 一个等于 $0$ 的特征值（重数为 $1$）。\n- 一个等于 $\\frac{H}{H-1}$ 的特征值（重数为 $H-1$）。\n\n### 任务 3: 确定最大稳定步长 $\\eta_{\\max}$\n\n对于线性 GD 更新 $e_{k+1} = (I - \\eta \\Sigma) e_k$，如果更新矩阵 $(I - \\eta \\Sigma)$ 的谱半径不大于 $1$，则可以保证损失的单调收敛（稳定性）。$(I - \\eta \\Sigma)$ 的特征值为 $1-\\eta\\lambda_i$，其中 $\\lambda_i$ 是 $\\Sigma$ 的特征值。条件是对于所有的 $i$，都有 $|1 - \\eta\\lambda_i| \\le 1$。\n由于 $\\Sigma$ 是一个协方差矩阵，其特征值是非负的（$\\lambda_i \\ge 0$）。条件变为：\n$$\n-1 \\le 1 - \\eta\\lambda_i \\le 1 \\implies 0 \\le \\eta\\lambda_i \\le 2\n$$\n对于 $\\lambda_i  0$，这要求 $\\eta \\le \\frac{2}{\\lambda_i}$。为了对所有特征值都满足这个条件，我们必须有 $\\eta \\le \\frac{2}{\\lambda_{\\max}(\\Sigma)}$，其中 $\\lambda_{\\max}(\\Sigma)$ 是 $\\Sigma$ 的最大特征值。\n因此，最大稳定步长是 $\\eta_{\\max} = \\frac{2}{\\lambda_{\\max}(\\Sigma)}$。\n\n**不带 LN 的模型：**\n特征协方差是 $\\Sigma_{\\mathrm{noLN}}=I_H$。特征值全部为 $1$。\n$\\lambda_{\\max}(\\Sigma_{\\mathrm{noLN}}) = 1$。\n最大稳定步长是 $\\eta_{\\max}^{\\mathrm{noLN}} = \\frac{2}{1} = 2$。\n\n**带 LN 的模型：**\n特征协方差是 $\\Sigma_{\\mathrm{LN}}$。其非零特征值均为 $\\frac{H}{H-1}$。\n由于 $H \\geq 2$，我们有 $\\frac{H}{H-1} = 1 + \\frac{1}{H-1}  1$。最大特征值为 $\\lambda_{\\max}(\\Sigma_{\\mathrm{LN}}) = \\frac{H}{H-1}$。\n最大稳定步长是 $\\eta_{\\max}^{\\mathrm{LN}} = \\frac{2}{\\lambda_{\\max}(\\Sigma_{\\mathrm{LN}})} = \\frac{2}{H/(H-1)} = \\frac{2(H-1)}{H}$。\n\n### 任务 4: 计算比率 $r(H)$\n\n最大稳定学习率的比率是：\n$$\nr(H) = \\frac{\\eta_{\\max}^{\\mathrm{LN}}}{\\eta_{\\max}^{\\mathrm{noLN}}} = \\frac{2(H-1)/H}{2} = \\frac{H-1}{H}\n$$\n这也可以写作 $1 - \\frac{1}{H}$。",
            "answer": "$$\n\\boxed{\\frac{H-1}{H}}\n$$"
        }
    ]
}