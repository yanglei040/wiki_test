## 引言
在深度学习领域，成功训练深度神经网络的一个先决条件是恰当的[权重初始化](@entry_id:636952)。不当的初始化会引发梯度消失或[梯度爆炸](@entry_id:635825)等问题，严重阻碍网络的学习过程，尤其是在网络层数加[深时](@entry_id:175139)。为了解决这一核心挑战，He初始化应运而生，它是一种专门为广泛使用的[ReLU激活函数](@entry_id:138370)及其变体设计的强大策略。本文旨在全面解析He初始化，从其根本原理到实际应用，为读者构建一个坚实的知识框架。在接下来的内容中，我们将在“原理与机制”一章中深入其数学推导和信号传播动力学；在“应用与跨学科连接”一章中，我们将探索其在卷积网络、Transformer等现代架构中的应用，并揭示其与其他技术的协同作用；最后，通过“动手实践”部分，读者将有机会通过编码将理论知识转化为实践技能。

## 原理与机制

在深入探讨深度神经网络的训练动力学时，一个核心挑战是如何设置初始权重，以确保信息能够在网络中有效流动。不当的初始化会导致信号在[前向传播](@entry_id:193086)中指数级放大或衰减，或者在反向传播中导致[梯度爆炸](@entry_id:635825)或消失。本章将系统地阐述一种专门为[修正线性单元](@entry_id:636721)（Rectified Linear Unit, ReLU）[激活函数](@entry_id:141784)设计的强大初始化策略——He初始化。我们将从其基本原理出发，推导其数学形式，分析其在深层网络中的优越性，并探讨其在不同架构和[激活函数](@entry_id:141784)下的变体与局限性。

### 信号[方差保持](@entry_id:634352)：He初始化的核心思想

对于使用ReLU及其变体的网络，早期的初始化方法（如Xavier/[Glorot初始化](@entry_id:637027)）被发现存在系统性偏差，会导致信号[方差](@entry_id:200758)随着层数的增加而减小。He初始化的提出正是为了解决这一问题。其核心思想在于：**在随机初始化阶段，应使得每一层的输出激活值（信号）的[方差](@entry_id:200758)与输入激活值的[方差保持](@entry_id:634352)一致。** 这种[方差](@entry_id:200758)的保持有助于维持信号的强度，从而使深度网络可以进行有效的训练。

让我们通过一个简单的单层全连接神经元来精确推导这一原理。考虑一个神经元，其接收来自前一层$n$个神经元的输入。设输入向量为$\boldsymbol{x} \in \mathbb{R}^{n}$，其分量$x_j$是[独立同分布](@entry_id:169067)（i.i.d.）的[随机变量](@entry_id:195330)，满足均值为零（$\mathbb{E}[x_j] = 0$）和[方差](@entry_id:200758)为$\sigma_x^2$（$\mathrm{Var}(x_j) = \sigma_x^2$）。连接权重$w_{ij}$同样是i.i.d.的，且独立于输入，其均值为零（$\mathbb{E}[w_{ij}] = 0$），[方差](@entry_id:200758)为$\sigma_w^2$。为简化分析，我们假设偏置为零。

神经元的预激活值（pre-activation）$z_i$是输入的加权和：
$$
z_i = \sum_{j=1}^{n} w_{ij} x_j
$$

首先，我们计算$z_i$的[方差](@entry_id:200758)。由于$w_{ij}$和$x_j$均值为零且[相互独立](@entry_id:273670)，因此$z_i$的均值也为零。$z_i$的[方差](@entry_id:200758)等于其二阶矩：
$$
\mathrm{Var}(z_i) = \mathbb{E}[z_i^2] = \mathbb{E}\left[\left(\sum_{j=1}^{n} w_{ij} x_j\right)^2\right]
$$
由于所有项$w_{ij}x_j$是[相互独立](@entry_id:273670)的，和的[方差](@entry_id:200758)等于[方差](@entry_id:200758)的和：
$$
\mathrm{Var}(z_i) = \sum_{j=1}^{n} \mathrm{Var}(w_{ij} x_j)
$$
对于两个独立的零均值[随机变量](@entry_id:195330)$W$和$X$，其乘积的[方差](@entry_id:200758)为$\mathrm{Var}(WX) = \mathbb{E}[W^2]\mathbb{E}[X^2] - (\mathbb{E}[W]\mathbb{E}[X])^2 = \mathrm{Var}(W)\mathrm{Var}(X)$。因此：
$$
\mathrm{Var}(w_{ij} x_j) = \mathrm{Var}(w_{ij}) \mathrm{Var}(x_j) = \sigma_w^2 \sigma_x^2
$$
代入上式，我们得到预激活值$z_i$的[方差](@entry_id:200758)：
$$
\mathrm{Var}(z_i) = \sum_{j=1}^{n} \sigma_w^2 \sigma_x^2 = n \sigma_w^2 \sigma_x^2
$$
这里的$n$是该层神经元的输入连接数，通常被称为**[扇入](@entry_id:165329)（fan-in）**。

接下来，我们[分析信号](@entry_id:190094)如何通过[ReLU激活函数](@entry_id:138370) $\phi(z) = \max(0, z)$。神经元的输出激活值是$y_i = \phi(z_i)$。我们的目标是使输出[方差](@entry_id:200758)$\mathrm{Var}(y_i)$约等于输入[方差](@entry_id:200758)$\sigma_x^2$。在实践中，我们通常通过保持二阶矩来近似实现这一目标，即$\mathbb{E}[y_i^2] \approx \mathbb{E}[x_j^2] = \sigma_x^2$。

当$n$足够大时，根据[中心极限定理](@entry_id:143108)，$z_i$的[分布](@entry_id:182848)近似于一个零均值的高斯分布。对于一个零均值且[分布](@entry_id:182848)对称的[随机变量](@entry_id:195330)$z$，其经过[ReLU函数](@entry_id:273016)后的二阶矩$\mathbb{E}[\max(0, z)^2]$恰好是其原始二阶矩$\mathbb{E}[z^2]$的一半。这是因为：
$$
\mathbb{E}[\max(0, z)^2] = \int_{-\infty}^{\infty} \max(0, u)^2 p(u) du = \int_{0}^{\infty} u^2 p(u) du = \frac{1}{2} \int_{-\infty}^{\infty} u^2 p(u) du = \frac{1}{2}\mathbb{E}[z^2]
$$
由于$z_i$的均值为零，$\mathbb{E}[z_i^2] = \mathrm{Var}(z_i)$。于是，我们得到输出激活值的二阶矩：
$$
\mathbb{E}[y_i^2] = \frac{1}{2} \mathrm{Var}(z_i) = \frac{1}{2} n \sigma_w^2 \sigma_x^2
$$
为了实现[方差保持](@entry_id:634352)，我们令$\mathbb{E}[y_i^2] = \sigma_x^2$：
$$
\sigma_x^2 = \frac{1}{2} n \sigma_w^2 \sigma_x^2
$$
假设输入信号非简并（$\sigma_x^2 \neq 0$），我们可以解出权重的理想[方差](@entry_id:200758)$\sigma_w^2$ ：
$$
\sigma_w^2 = \frac{2}{n} = \frac{2}{\text{fan\_in}}
$$
这就是He初始化的核心公式。它规定，对于使用[ReLU激活函数](@entry_id:138370)的层，其权重的[方差](@entry_id:200758)应设为$2$除以其[扇入](@entry_id:165329)。通常，权重从均值为$0$，[方差](@entry_id:200758)为$\frac{2}{\text{fan\_in}}$的[高斯分布](@entry_id:154414) $\mathcal{N}(0, \frac{2}{\text{fan\_in}})$ 或在一个对称区间内[均匀分布](@entry_id:194597)中抽取。

### 深层网络中的[信号传播](@entry_id:165148)动力学

He初始化的真正威力体现在深度网络中。让我们通过对比He初始化和[Xavier初始化](@entry_id:637027)（$\sigma_w^2 = \frac{1}{\text{fan\_in}}$）来理解这一点。考虑一个由$L$个宽度为$d$的相同隐藏层组成的深度网络，[激活函数](@entry_id:141784)为ReLU。

我们可以建立一个关于各层激活值[方差](@entry_id:200758)$q_\ell = \mathrm{Var}(y_j^{(\ell)})$的[递推关系](@entry_id:189264)。经过更精确的推导，可以证明[方差](@entry_id:200758)的传播遵循 ：
$$
q_L = q_0 \left(1 - \frac{1}{\pi}\right) \left(\frac{d \sigma_w^2}{2}\right)^L
$$
其中$q_0$是输入[方差](@entry_id:200758)。

*   **对于He初始化**，我们设置$\sigma_w^2 = \frac{2}{d}$。代入上式，[乘性](@entry_id:187940)因子变为：
    $$
    \frac{d \sigma_w^2}{2} = \frac{d}{2} \left(\frac{2}{d}\right) = 1
    $$
    因此，激活值的[方差](@entry_id:200758)在各层之间保持稳定：$q_L = q_0 \left(1 - \frac{1}{\pi}\right)$。这表明信号的“能量”在网络中稳定传播，不会随深度增加而消失或爆炸。

*   **对于[Xavier初始化](@entry_id:637027)**，我们设置$\sigma_w^2 = \frac{1}{d}$。[乘性](@entry_id:187940)因子变为：
    $$
    \frac{d \sigma_w^2}{2} = \frac{d}{2} \left(\frac{1}{d}\right) = \frac{1}{2}
    $$
    此时，[方差](@entry_id:200758)随层数指数衰减：$q_L = q_0 \left(1 - \frac{1}{\pi}\right) \left(\frac{1}{2}\right)^L$。在深度网络中，这意味着信号在[前向传播](@entry_id:193086)过程中会迅速消失，使得深层神经元几乎接收不到有效信息，从而无法有效学习。

这个对比鲜明地展示了为何He初始化对于训练深度[ReLU网络](@entry_id:637021)至关重要。

### 梯度的[反向传播](@entry_id:199535)与稳定性

保持[前向传播](@entry_id:193086)的信号强度固然重要，但学习的发生依赖于梯度的反向传播。一个优秀的初始化策略必须同时保证反向传播的梯度信号既不消失也不爆炸。He初始化同样满足这一要求。

在[反向传播](@entry_id:199535)中，[误差信号](@entry_id:271594)$\delta^{(l)} = \frac{\partial L}{\partial z^{(l)}}$的[方差](@entry_id:200758)遵循如下[递推关系](@entry_id:189264) ：
$$
\mathrm{Var}(\delta^{(l)}) \approx n_{l+1} \sigma_w^2 \mathbb{E}[(\phi'(z^{(l)}))^2] \mathrm{Var}(\delta^{(l+1)})
$$
其中，$n_{l+1}$是第$l+1$层的宽度（即第$l$层的[扇出](@entry_id:173211), fan-out），$\phi'(z)$是[激活函数](@entry_id:141784)的导数。对于ReLU，$\phi'(z)$在$z>0$时为$1$，在$z0$时为$0$。假设预激活值$z^{(l)}$对称[分布](@entry_id:182848)，则其大于零的概率约为$\frac{1}{2}$，因此：
$$
\mathbb{E}[(\phi'(z^{(l)}))^2] \approx 1^2 \cdot \frac{1}{2} + 0^2 \cdot \frac{1}{2} = \frac{1}{2}
$$
我们将He初始化的权重[方差](@entry_id:200758)$\sigma_w^2 = \frac{2}{n_l}$（$n_l$是第$l$层的[扇入](@entry_id:165329)）代入[递推公式](@entry_id:149465)：
$$
\mathrm{Var}(\delta^{(l)}) \approx n_{l+1} \left(\frac{2}{n_l}\right) \left(\frac{1}{2}\right) \mathrm{Var}(\delta^{(l+1)}) = \frac{n_{l+1}}{n_l} \mathrm{Var}(\delta^{(l+1)})
$$
这个结果揭示了一个深刻的性质：如果网络各层的宽度大致相等（$n_{l+1} \approx n_l$），那么梯度的[方差](@entry_id:200758)在反向传播过程中也将保持稳定。这有效地缓解了[梯度消失和梯度爆炸](@entry_id:634312)问题，确保了网络中所有层都能接收到有效的学习信号。

从一个更几何的视角来看，梯度的稳定性与层映射的[雅可比矩阵](@entry_id:264467)（Jacobian）的性质密切相关。理想情况下，我们希望网络层在初始化时近似于一个**[等距映射](@entry_id:150881)（isometry）**，即它不会显著拉伸或压缩输入空间中的向量。对于He初始化，可以证明，对于任意固定的输入向量$\mathbf{x}$，层雅可比矩阵$\mathbf{J}(\mathbf{x})$作用于该向量后，其范数的[期望值](@entry_id:153208)被保留 ：
$$
\mathbb{E}\left[\frac{\|\mathbf{J}(\mathbf{x})\,\mathbf{x}\|_{2}^{2}}{\|\mathbf{x}\|_{2}^{2}}\right] = 1
$$
这一“期望中的等距性”为He初始化能够稳定梯度提供了更深层次的理论支撑。

### 实际应用与扩展

He初始化的基本原理可以灵活地应用于不同的网络架构和激活函数。

#### 卷积层中的应用

对于[二维卷积](@entry_id:275218)层，其“[扇入](@entry_id:165329)”的概念需要重新定义。一个输出[特征图](@entry_id:637719)上的神经元，其感受野内的每个位置都与输入的所有通道相连。因此，其输入连接数等于卷积核的面积乘以输入通道数。对于一个$k \times k$的卷积核和$C_{in}$个输入通道，其[扇入](@entry_id:165329)为：
$$
\text{fan\_in} = k^2 C_{in}
$$
因此，卷积层的He初始化规则为 ：
$$
\sigma_w^2 = \frac{2}{k^2 C_{in}}
$$

#### [激活函数](@entry_id:141784)感知的初始化

He初始化的推导过程严重依赖于ReLU的数学性质，特别是$\mathbb{E}[\phi(z)^2] = \frac{1}{2}\mathbb{E}[z^2]$这一关系。当激活函数改变时，初始化策略也必须随之调整。

*   **对于饱和激活函数（如 `[tanh](@entry_id:636446)`）**：如果错误地将He初始化用于`[tanh](@entry_id:636446)`网络，权重[方差](@entry_id:200758)$\sigma_w^2 = 2/n$会过大，导致大部分预激活值$z$落入`[tanh](@entry_id:636446)`函数的[饱和区](@entry_id:262273)（即$|z|$很大）。在这些区域，`[tanh](@entry_id:636446)`的梯度接近于零，从而导致梯度消失。对于`[tanh](@entry_id:636446)`，更合适的选择是[Xavier初始化](@entry_id:637027)（$\sigma_w^2=1/n$），它能将预激活值保持在`[tanh](@entry_id:636446)`的[线性区](@entry_id:276444)域附近（$z \approx 0$），在该区域$\tanh(z) \approx z$，梯度的传播得以维持 。

*   **对于[参数化ReLU](@entry_id:635727)（[PReLU](@entry_id:634418)）**：[PReLU](@entry_id:634418)的定义为$\phi(z) = \max(z, az)$，其中$a$是一个可学习的参数。我们可以将[方差保持](@entry_id:634352)原理推广到[PReLU](@entry_id:634418)。通过重新计算激活后的二阶矩和梯度的二阶矩，可以推导出与初始斜率$a_0$相关的初始化规则 ：
    $$
    \sigma_w^2 = \frac{2}{(1+a_0^2)n}
    $$
    当$a_0=0$时，[PReLU](@entry_id:634418)退化为ReLU，此公式也退化为标准的He初始化$\sigma_w^2=2/n$。这展示了He初始化背后的原理比其具体公式更为根本和普适。

### 微妙之处与二阶效应

虽然He初始化在理论和实践中都极为成功，但理解其背后的一些微妙之处和潜在的失效模式同样重要。

#### 激活值的均值偏移

一个值得注意的现象是，即使输入预激活值$z$是零均值的，经过[ReLU函数](@entry_id:273016)后，输出激活值$a = \max(0, z)$的均值将变为正数。对于[高斯分布](@entry_id:154414)的$z \sim \mathcal{N}(0, \sigma^2)$，可以精确计算出这个均值偏移 ：
$$
\mathbb{E}[\text{ReLU}(z)] = \frac{\sigma}{\sqrt{2\pi}}
$$
在深度网络中，这种正均值可能会逐层累积，导致深层网络的激活值[分布](@entry_id:182848)发生偏移，从而影响训练的稳定性。一个简单的修正方法是在激活后减去一个固定的偏置$b = -\frac{\sigma}{\sqrt{2\pi}}$来重新中心化特征。这一思想也为后续更强大的归一化技术（如[批量归一化](@entry_id:634986), Batch Normalization）提供了动机。

#### 对损失曲率的影响

[权重初始化](@entry_id:636952)不仅影响信号传播，还塑造了初始阶段的损失函数[曲面](@entry_id:267450)。分析表明，与[Xavier初始化](@entry_id:637027)相比，He初始化会产生一个具有更大初始曲率（由损失函数关于权重的Hessian矩阵的最大[特征值](@entry_id:154894)衡量）的损失[曲面](@entry_id:267450) 。具体来说，对于一个简单的两层[ReLU网络](@entry_id:637021)，在He初始化下，Hessian矩阵的最大[特征值](@entry_id:154894)的[期望值](@entry_id:153208)是[Xavier初始化](@entry_id:637027)下的两倍。这可能意味着在训练初期，损失函数的“峡谷”更陡峭，这反过来又会影响[学习率](@entry_id:140210)的选择和[优化算法](@entry_id:147840)的性能。

#### 对输入数据尺度的敏感性

所有初始化理论都建立在输入数据被良好[预处理](@entry_id:141204)（例如，标准化为零均值和单位[方差](@entry_id:200758)）的假设之上。如果这个假设被违反，即使是He初始化也可能失效。例如，如果输入数据被错误地乘以一个很大的常数$c \gg 1$，会导致第一层的激活值[方差](@entry_id:200758)被放大$c^2$倍。这种巨大的[信号能量](@entry_id:264743)会沿着[网络传播](@entry_id:752437)，并最终在[反向传播](@entry_id:199535)时导致梯度更新的量级急剧增大，甚至超过权重本身的量级，从而使训练过程变得不稳定。可以推导出，对于给定的网络宽度$n$和[学习率](@entry_id:140210)$\eta$，存在一个临界的缩放因子$c_{\text{crit}}$，超过它就会导致训练不稳定。对于一个典型的[多层网络](@entry_id:270365)，最先失稳的往往是输出层，其临界缩放因子为 ：
$$
c_{\text{crit}} = \frac{1}{n^{1/4}\sqrt{\eta}}
$$
这警示我们，成功的深度学习实践不仅依赖于精巧的初始化策略，同样也离不开严格的[数据预处理](@entry_id:197920)。

总之，He初始化并非一个孤立的“魔法数字”，而是源于对深度[ReLU网络](@entry_id:637021)中[信号传播](@entry_id:165148)动力学的深刻理解。它体现了[方差保持](@entry_id:634352)这一基本原则，并且该原则可以被灵活地应用和推广到不同的架构和[激活函数](@entry_id:141784)中。理解其原理、应用和局限性，是每一位深度学习研究者和工程师构建和训练强大[神经网](@entry_id:276355)络的关键一步。