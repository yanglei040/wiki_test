{
    "hands_on_practices": [
        {
            "introduction": "理论是强大的，但通过具体计算来巩固理论是掌握概念的关键。这项练习将理论付诸实践，要求您将Xavier初始化的核心公式应用于一个典型的全连接层。您将计算权重均匀分布的具体边界，并利用该结果预测通过该层的信号（即“预激活”）的尺度，从而验证该初始化方案确实达到了其维持信号方差的目标。这项实践将帮助您深入理解Xavier初始化如何在数学层面实现信号尺度的平衡。",
            "id": "3200147",
            "problem": "考虑一个全连接前馈神经网络，其输入层大小为 $\\mathbb{R}^{784}$，一个隐藏层大小为 $\\mathbb{R}^{256}$，输出层大小为 $\\mathbb{R}^{10}$。设隐藏层的预激活为 $z^{(1)} = W^{(1)} x$，输出层的预激活为 $z^{(2)} = W^{(2)} h$，其中 $h$ 是对 $z^{(1)}$ 应用逐点非线性函数得到的隐藏层激活。假设在初始化时满足以下条件：\n- 输入分量是独立同分布的（i.i.d.），均值为零，方差为 $\\operatorname{Var}(x_{i}) = 1$。\n- 所有偏置均为零。\n- 非线性函数是平滑的，在初始化时在零点附近被线性化，因此其局部增益近似为 $1$。\n- 每个权重矩阵 $W^{(\\ell)}$ 的条目都是从对称区间 $\\left[-a_{\\ell}, a_{\\ell}\\right]$ 上的零均值均匀分布中独立抽取的。\n- 应用独立性近似，因此在前向和后向传播方差时，不同项之间的协方差为零。\n\n设计目标是选择权重方差，以便在线性化和独立性假设下，信号的尺度在前向和后向传播中保持平衡。具体来说，如果一个具有 $f_{\\text{in}}$ 个输入和 $f_{\\text{out}}$ 个输出的层的前向方差保持因子为 $f_{\\text{in}} \\operatorname{Var}(W)$，后向方差保持因子为 $f_{\\text{out}} \\operatorname{Var}(W)$，则通过使这两个因子的算术平均值等于 $1$ 来实现对称折衷。仅使用此原则、在 $\\left[-a, a\\right]$ 上的均匀分布的方差公式以及独立随机变量之和的方差传播法则，完成以下任务：\n- 确定层 $\\mathbb{R}^{784} \\to \\mathbb{R}^{256}$ 中 $W^{(1)}$ 的边界 $a_{1}$。\n- 确定层 $\\mathbb{R}^{256} \\to \\mathbb{R}^{10}$ 中 $W^{(2)}$ 的边界 $a_{2}$。\n- 使用你为 $W^{(1)}$ 推导出的边界，预测初始化时隐藏层预激活 $z^{(1)}$ 的均方根（RMS；root mean square），其中 RMS 定义为 $\\sqrt{\\mathbb{E}\\!\\left[(z^{(1)}_{j})^{2}\\right]}$。\n\n最终答案只报告 $z^{(1)}$ 的隐藏层预激活 RMS 的数值。将最终答案四舍五入到 $4$ 位有效数字。",
            "solution": "我得出解决方案的思考过程如下：\n\n1.  **解构问题**\n    - 目标：求隐藏层预激活 $z^{(1)}$ 的 RMS\n    - RMS 是 $\\sqrt{\\mathbb{E}[(z^{(1)}_{j})^{2}]}$。由于输入和权重的均值为零，预激活 $z^{(1)}$ 的均值也将为零。因此，$\\mathbb{E}[(z^{(1)}_{j})^{2}] = \\operatorname{Var}(z^{(1)}_{j}) + (\\mathbb{E}[z^{(1)}_{j}])^2 = \\operatorname{Var}(z^{(1)}_{j})$。所以目标是求 $\\sqrt{\\operatorname{Var}(z^{(1)}_{j})}$。\n    - 预激活的方差为 $\\operatorname{Var}(z^{(1)}_{j}) = \\operatorname{Var}\\left(\\sum_{i=1}^{f_{\\text{in},1}} W^{(1)}_{ji} x_i\\right)$。\n    - 利用 $W$ 和 $x$ 的独立性，以及两者均为独立同分布的假设，这变为 $f_{\\text{in},1} \\operatorname{Var}(W^{(1)}) \\operatorname{Var}(x)$。\n    - 已知 $\\operatorname{Var}(x) = 1$，我们有 $\\operatorname{Var}(z^{(1)}_{j}) = f_{\\text{in},1} \\operatorname{Var}(W^{(1)})$。\n    - 因此，核心任务是求出 $\\operatorname{Var}(W^{(1)})$。\n\n2.  **使用 Glorot/Xavier 初始化原则**\n    - 问题提供了一个源自 Glorot 和 Bengio 工作的特定原则，通常称为 Xavier 初始化。\n    - 该原则是在前向和后向方差保持之间进行折衷。\n    - 前向方差保持因子：$F_f = f_{\\text{in}} \\operatorname{Var}(W)$\n    - 后向方差保持因子：$F_b = f_{\\text{out}} \\operatorname{Var}(W)$\n    - 折衷规则是：$\\frac{F_f + F_b}{2} = 1$。\n    - 代入因子：$\\frac{f_{\\text{in}} \\operatorname{Var}(W) + f_{\\text{out}} \\operatorname{Var}(W)}{2} = 1$。\n    - 求解 $\\operatorname{Var}(W)$: $\\operatorname{Var}(W) (f_{\\text{in}} + f_{\\text{out}}) = 2 \\implies \\operatorname{Var}(W) = \\frac{2}{f_{\\text{in}} + f_{\\text{out}}}$。\n    - 这就是此初始化方案下任何层权重的方差公式。\n\n3.  **应用于第一层并计算 RMS**\n    - 对于第一层 ($\\mathbb{R}^{784} \\to \\mathbb{R}^{256}$):\n    - $f_{\\text{in},1} = 784$\n    - $f_{\\text{out},1} = 256$\n    - 使用推导出的权重方差公式：$\\operatorname{Var}(W^{(1)}) = \\frac{2}{f_{\\text{in},1} + f_{\\text{out},1}} = \\frac{2}{784 + 256} = \\frac{2}{1040}$。\n    - 现在，求预激活 $z^{(1)}$ 的方差：\n    - $\\operatorname{Var}(z^{(1)}_{j}) = f_{\\text{in},1} \\operatorname{Var}(W^{(1)}) = 784 \\times \\frac{2}{1040} = \\frac{1568}{1040} = \\frac{98}{65}$。\n    - RMS 是此方差的平方根：\n    - RMS($z^{(1)}_{j}$) = $\\sqrt{\\frac{98}{65}}$。\n\n4.  **数值计算与最终答案的形成**\n    - 计算数值：$\\sqrt{98/65} \\approx \\sqrt{1.5076923} \\approx 1.227881226$。\n    - 四舍五入到 4 位有效数字：$1.228$。\n    - 问题要求将边界 $a_1$ 和 $a_2$ 作为中间步骤，但不是最终答案。我们快速验证一下以确保思路正确。\n    - 对于 $[-a, a]$ 上的均匀分布，其方差为 $\\operatorname{Var}(W) = \\frac{(a - (-a))^2}{12} = \\frac{(2a)^2}{12} = \\frac{4a^2}{12} = \\frac{a^2}{3}$。\n    - 所以 $a = \\sqrt{3 \\operatorname{Var}(W)}$。\n    - $a_1 = \\sqrt{3 \\operatorname{Var}(W^{(1)})} = \\sqrt{3 \\times \\frac{2}{1040}} = \\sqrt{\\frac{6}{1040}}$。\n    - 对于第二层 ($\\mathbb{R}^{256} \\to \\mathbb{R}^{10}$)，$f_{\\text{in},2}=256, f_{\\text{out},2}=10$。\n    - $\\operatorname{Var}(W^{(2)}) = \\frac{2}{256+10} = \\frac{2}{266}$。\n    - $a_2 = \\sqrt{3 \\operatorname{Var}(W^{(2)})} = \\sqrt{3 \\times \\frac{2}{266}} = \\sqrt{\\frac{6}{266}}$。\n    - 这些步骤是正确的，但最终答案并不需要它们，最终答案只是 $z^{(1)}$ 的 RMS。RMS 的计算与分布类型（均匀分布）无关，因为它只依赖于方差 $\\operatorname{Var}(W^{(1)})$。分布类型仅用于求 'a'。\n    - 问题要求的是 $z^{(1)}$ 的 RMS 的数值。我的推导直接得出了这个结果。\n\n5.  **逻辑的最终审查**\n    - 逻辑直接源于问题陈述。\n    - 定义折衷规则：$\\frac{f_{\\text{in}}\\text{Var}(W) + f_{\\text{out}}\\text{Var}(W)}{2} = 1$。\n    - 求解 $\\text{Var}(W)$: $\\text{Var}(W) = \\frac{2}{f_{\\text{in}} + f_{\\text{out}}}$。\n    - 定义预激活的方差：$\\text{Var}(z_j) = f_{\\text{in}} \\text{Var}(W) \\text{Var}(x)$。\n    - 当 $\\text{Var}(x)=1$ 时，$\\text{Var}(z_j) = f_{\\text{in}} \\text{Var}(W)$。\n    - 代入 $\\text{Var}(W)$: $\\text{Var}(z_j) = f_{\\text{in}} \\frac{2}{f_{\\text{in}} + f_{\\text{out}}}$。\n    - 计算第一层的 RMS: $\\text{RMS} = \\sqrt{\\text{Var}(z^{(1)}_j)} = \\sqrt{784 \\frac{2}{784+256}} = \\sqrt{\\frac{1568}{1040}} = \\sqrt{\\frac{98}{65}}$。\n    - 计算数值并四舍五入：$\\sqrt{98/65} \\approx 1.228$。\n    - 逻辑是健全的，问题中的所有约束条件都得到了正确使用。",
            "answer": "$$\n\\boxed{1.228}\n$$"
        },
        {
            "introduction": "在理论和实践之间架起一座桥梁是科学探究的重要一环。在这个练习中，您将首先从理论上推导方差在一个使用Xavier初始化的深度线性网络中是如何逐层传播的。接着，您将编写代码来模拟这一过程，通过经验性地测量每一层的方差，并将其与您的理论预测进行比较，从而直观地验证Xavier初始化在维持信号稳定性方面的有效性。",
            "id": "3200162",
            "problem": "考虑一个具有 $L$ 层的全连接前馈网络。设输入维度为 $n_{0}$，第 $l$ 层有 $n_{l}$ 个单元，其中 $l \\in \\{1,\\dots,L\\}$。对于每一层 $l$，预激活向量 $z^{(l)} \\in \\mathbb{R}^{n_{l}}$ 由 $z^{(l)} = W^{(l)} a^{(l-1)}$ 给出，其中 $W^{(l)} \\in \\mathbb{R}^{n_{l} \\times n_{l-1}}$ 是权重矩阵，$a^{(l-1)} \\in \\mathbb{R}^{n_{l-1}}$ 是前一层的激活。假设使用恒等激活函数，因此对所有 $l$ 都有 $a^{(l)} = z^{(l)}$。输入 $a^{(0)}$ 是从均值为 $0$、方差为 $1$ 的高斯分布中独立同分布 (i.i.d.) 采样的，即元素级别上 $a^{(0)} \\sim \\mathcal{N}(0,1)$。\n\n权重使用 Xavier (Glorot) 正态初始化进行初始化：$W^{(l)}$ 的每个条目都是独立同分布的，均值为 $0$，方差为 $2/(n_{l-1} + n_{l})$。你可以假设在初始化时权重和激活是相互独立的。\n\n从方差在独立性和线性条件下的基本性质出发（具体而言，对于独立的零均值随机变量 $x_{i}$ 和常数 $c_{i}$，有 $\\operatorname{Var}\\left(\\sum_{i} c_{i} x_{i}\\right) = \\sum_{i} c_{i}^{2} \\operatorname{Var}(x_{i})$；以及对于独立的随机变量 $x$ 和 $y$，当两者均值为 $0$ 时，有 $\\operatorname{Var}(xy) = \\operatorname{Var}(x)\\operatorname{Var}(y)$），推导出逐层预激活方差 $v^{(l)} = \\operatorname{Var}(z^{(l)}_{j})$（根据对称性，该方差对于层内所有单元 $j$ 都是相同的）关于 $n_{l-1}$、$n_{l}$ 和前一层激活方差 $v^{(l-1)}$ 的理论演进关系。使用 $v^{(0)} = 1$。\n\n你的程序必须：\n- 针对每个测试用例，从 $\\mathcal{N}(0,1)$ 生成 $N$ 个独立的输入样本 $a^{(0)}$。\n- 对于每一层 $l$，根据上述 Xavier 正态初始化方法对 $W^{(l)}$ 进行采样。\n- 使用恒等激活函数向前传播输入，并计算每一层 $l$ 的经验预激活方差，即所有样本和单元的总体方差。\n- 使用你推导出的表达式计算每一层的理论预激活方差。\n- 对于每个测试用例，计算跨层的最大绝对相对误差，定义为 $\\max_{l \\in \\{1,\\dots,L\\}} \\left| v_{\\text{emp}}^{(l)} - v_{\\text{theory}}^{(l)} \\right| / \\max\\{v_{\\text{theory}}^{(l)}, \\epsilon\\}$，其中 $\\epsilon$ 是一个小的正常数，以避免除以零。\n\n使用以下参数值的测试套件，它们共同探测了典型行为、对称性和边缘情况：\n- 用例 1：宽度 $[n_{0}, n_{1}, n_{2}] = [100, 100, 100]$，样本数 $N = 10000$。\n- 用例 2：宽度 $[n_{0}, n_{1}, n_{2}] = [50, 10, 200]$，样本数 $N = 10000$。\n- 用例 3：宽度 $[n_{0}, n_{1}, n_{2}] = [1, 1, 1]$，样本数 $N = 20000$。\n- 用例 4：宽度 $[n_{0}, n_{1}, n_{2}] = [1000, 100, 50]$，样本数 $N = 500$。\n- 用例 5：宽度 $[n_{0}, n_{1}, n_{2}, n_{3}, n_{4}] = [64, 64, 64, 64, 64]$，样本数 $N = 2000$。\n\n你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$ [r_{1},r_{2},r_{3},r_{4},r_{5}]$），其中每个 $r_{i}$ 是第 $i$ 个测试用例的最大绝对相对误差，四舍五入到 $6$ 位小数。不涉及物理单位，角度也不适用。输出必须是指定格式的单行。",
            "solution": "该问题是有效的，可以通过首先推导方差的理论演进，然后实现一个数值模拟来验证它来解决。\n\n**1. 方差演进的理论推导**\n\n我们寻求预激活方差 $v^{(l)} = \\operatorname{Var}(z_j^{(l)})$ 的递推关系，其中 $z_j^{(l)}$ 是第 $l$ 层第 $j$ 个神经元的预激活。层的编号为 $l \\in \\{1, \\dots, L\\}$。权重和激活初始化的对称性确保了对于给定层 $l$ 内的所有神经元 $j$，该方差是相同的。\n\n预激活被定义为前一层激活 $a^{(l-1)}$ 的线性变换：\n$$z_j^{(l)} = \\sum_{k=1}^{n_{l-1}} W_{jk}^{(l)} a_k^{(l-1)}$$\n其中 $W^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l-1}}$ 是权重矩阵，$a^{(l-1)} \\in \\mathbb{R}^{n_{l-1}}$ 是来自前一层的激活向量。\n\n首先，我们确定在整个网络中，所有激活和预激活的均值保持为 $0$。输入激活 $a_k^{(0)}$ 从 $\\mathcal{N}(0, 1)$ 采样，因此 $\\mathbb{E}[a_k^{(0)}] = 0$。权重 $W_{jk}^{(l)}$ 从均值为 $0$ 的分布中初始化，因此 $\\mathbb{E}[W_{jk}^{(l)}] = 0$。\n现在，我们通过归纳法进行证明。假设对于所有 $k$，$ \\mathbb{E}[a_k^{(l-1)}] = 0$。预激活 $z_j^{(l)}$ 的均值为：\n$$\\mathbb{E}[z_j^{(l)}] = \\mathbb{E}\\left[\\sum_{k=1}^{n_{l-1}} W_{jk}^{(l)} a_k^{(l-1)}\\right]$$\n根据期望的线性性质：\n$$\\mathbb{E}[z_j^{(l)}] = \\sum_{k=1}^{n_{l-1}} \\mathbb{E}[W_{jk}^{(l)} a_k^{(l-1)}]$$\n由于权重 $W^{(l)}$ 与激活 $a^{(l-1)}$ 相互独立，我们可以分离期望：\n$$\\mathbb{E}[z_j^{(l)}] = \\sum_{k=1}^{n_{l-1}} \\mathbb{E}[W_{jk}^{(l)}] \\mathbb{E}[a_k^{(l-1)}] = \\sum_{k=1}^{n_{l-1}} 0 \\cdot 0 = 0$$\n鉴于恒等激活函数 $a^{(l)} = z^{(l)}$，可得 $\\mathbb{E}[a_j^{(l)}] = 0$。基于基础情况 $\\mathbb{E}[a_k^{(0)}] = 0$，网络中所有后续的预激活和激活的均值都为 $0$。\n\n接下来，我们计算方差 $v^{(l)} = \\operatorname{Var}(z_j^{(l)})$：\n$$v^{(l)} = \\operatorname{Var}\\left(\\sum_{k=1}^{n_{l-1}} W_{jk}^{(l)} a_k^{(l-1)}\\right)$$\n随机变量之和的方差是它们协方差的总和。和中的项为 $Y_k = W_{jk}^{(l)} a_k^{(l-1)}$。对于 $k \\neq k'$，协方差为 $\\operatorname{Cov}(Y_k, Y_{k'}) = \\mathbb{E}[Y_k Y_{k'}] - \\mathbb{E}[Y_k]\\mathbb{E}[Y_{k'}]$。由于 $\\mathbb{E}[Y_k] = 0$，这简化为 $\\mathbb{E}[Y_k Y_{k'}] = \\mathbb{E}[W_{jk}^{(l)} a_k^{(l-1)} W_{jk'}^{(l)} a_{k'}^{(l-1)}]$。所有权重彼此独立，且与激活独立。因此，对于 $k \\neq k'$，$W_{jk}^{(l)}$ 和 $W_{jk'}^{(l)}$ 是独立的。\n$$\\operatorname{Cov}(Y_k, Y_{k'}) = \\mathbb{E}[W_{jk}^{(l)} W_{jk'}^{(l)}] \\mathbb{E}[a_k^{(l-1)} a_{k'}^{(l-1)}] = \\mathbb{E}[W_{jk}^{(l)}] \\mathbb{E}[W_{jk'}^{(l)}] \\mathbb{E}[a_k^{(l-1)} a_{k'}^{(l-1)}] = 0$$\n因此，项 $Y_k$ 是不相关的，和的方差是方差的和：\n$$v^{(l)} = \\sum_{k=1}^{n_{l-1}} \\operatorname{Var}\\left(W_{jk}^{(l)} a_k^{(l-1)}\\right)$$\n我们使用提供的关于两个独立的零均值随机变量 $X$ 和 $Y$ 乘积的方差性质：$\\operatorname{Var}(XY) = \\operatorname{Var}(X)\\operatorname{Var}(Y)$。在这里，$X = W_{jk}^{(l)}$ 且 $Y = a_k^{(l-1)}$。根据定义，两者都是零均值且独立的。因此：\n$$\\operatorname{Var}\\left(W_{jk}^{(l)} a_k^{(l-1)}\\right) = \\operatorname{Var}(W_{jk}^{(l)}) \\operatorname{Var}(a_k^{(l-1)})$$\n我们已知 Xavier 正态初始化的权重方差：\n$$\\operatorname{Var}(W_{jk}^{(l)}) = \\frac{2}{n_{l-1} + n_{l}}$$\n根据对称性和恒等激活函数，前一层激活的方差在其所有单元上是统一的：$\\operatorname{Var}(a_k^{(l-1)}) = v^{(l-1)}$。\n将这些代入求和式：\n$$v^{(l)} = \\sum_{k=1}^{n_{l-1}} \\left( \\frac{2}{n_{l-1} + n_{l}} \\cdot v^{(l-1)} \\right)$$\n求和式中的项相对于索引 $k$ 是常数。因此，求和可以简化为乘以 $n_{l-1}$：\n$$v^{(l)} = n_{l-1} \\left( \\frac{2}{n_{l-1} + n_{l}} \\right) v^{(l-1)}$$\n这就是最终的递推关系。给定输入方差 $v^{(0)} = \\operatorname{Var}(a_k^{(0)}) = 1$，我们可以计算任何层 $l$ 的理论方差。\n\n**2. 模拟与验证方法**\n\n数值模拟旨在验证推导出的理论方差。该过程对每个测试用例执行，测试用例指定了网络层的宽度（$n_0, n_1, \\dots, n_L$）和独立样本的数量 $N$。\n\n1.  **理论方差计算**：对于从 $1$ 到 $L$ 的每一层 $l$，使用推导出的递推关系 $v^{(l)} = v^{(l-1)} \\cdot \\frac{2 n_{l-1}}{n_{l-1} + n_{l}}$ 迭代计算理论方差 $v_{\\text{theory}}^{(l)}$，从基础情况 $v^{(0)} = 1$ 开始。\n\n2.  **经验方差模拟**：\n    - 通过从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取 $N \\times n_0$ 个 i.i.d. 样本，生成一个大小为 $N \\times n_0$ 的输入数据矩阵。这代表 $N$ 个输入向量 $a^{(0)}$。\n    - 模擬从 $l=1$到 $L$逐層进行。对于每一层 $l$：\n      - 创建一个大小为 $n_l \\times n_{l-1}$ 的权重矩阵 $W^{(l)}$。每个元素都从方差为 $\\sigma_W^2 = 2 / (n_{l-1} + n_l)$ 的 Xavier 正态分布 $\\mathcal{N}(0, \\sigma_W^2)$ 中独立采样。\n      - 通过矩阵乘法 $a^{(l-1)} (W^{(l)})^T$ 高效地计算所有 $N$ 个样本的预激活 $z^{(l)}$，其中 $a^{(l-1)}$ 是来自前一层的 $N \\times n_{l-1}$ 激活矩阵。\n      - 经验方差 $v_{\\text{emp}}^{(l)}$ 是对整个 $N \\times n_l$ 个预激活值集合计算的总体方差。\n      - 由于使用恒等激活函数，下一层的激活被设置为 $a^{(l)} = z^{(l)}$。\n\n3.  **误差计算**：计算所有层的最大绝对相对误差，以量化理论与模拟之间的一致性：\n    $$ \\max_{l \\in \\{1,\\dots,L\\}} \\frac{\\left| v_{\\text{emp}}^{(l)} - v_{\\text{theory}}^{(l)} \\right|}{\\max\\{v_{\\text{theory}}^{(l)}, \\epsilon \\}} $$\n    其中 $\\epsilon$ 是一个小的正常数（例如，$10^{-9}$），用于防止除以零。为随机数生成器设置一个固定的种子可确保整个模拟的可复现性。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and simulates the propagation of variance in a deep linear network\n    with Xavier (Glorot) normal initialization.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        ([100, 100, 100], 10000),         # Case 1: Symmetric, constant width\n        ([50, 10, 200], 10000),           # Case 2: Bottleneck then expansion\n        ([1, 1, 1], 20000),               # Case 3: Scalar \"network\"\n        ([1000, 100, 50], 500),           # Case 4: Deep and narrowing, fewer samples\n        ([64, 64, 64, 64, 64], 2000),     # Case 5: Deeper constant width network\n    ]\n\n    results = []\n    # Use a fixed seed for the random number generator for reproducibility.\n    seed = 42\n    rng = np.random.default_rng(seed)\n    epsilon = 1e-9\n\n    for widths, N in test_cases:\n        # 1. Theoretical Calculation\n        v_theory_list = []\n        v_prev_theory = 1.0  # v^(0) = 1\n        num_layers = len(widths) - 1\n        \n        for i in range(num_layers):\n            n_in = widths[i]\n            n_out = widths[i+1]\n            v_curr_theory = v_prev_theory * (2.0 * n_in) / (n_in + n_out)\n            v_theory_list.append(v_curr_theory)\n            v_prev_theory = v_curr_theory\n\n        # 2. Empirical Simulation\n        v_emp_list = []\n        # Initial activations a^(0)\n        # Shape: (N, n_0)\n        a = rng.normal(loc=0.0, scale=1.0, size=(N, widths[0]))\n        \n        for i in range(num_layers):\n            n_in = widths[i]\n            n_out = widths[i+1]\n            \n            # Sample weights W^(l) for the current layer\n            # Shape: (n_out, n_in)\n            std_dev_w = np.sqrt(2.0 / (n_in + n_out))\n            W = rng.normal(loc=0.0, scale=std_dev_w, size=(n_out, n_in))\n            \n            # Propagate activations forward: z^(l) = a^(l-1) @ (W^(l))^T\n            # a has shape (N, n_in), W.T has shape (n_in, n_out)\n            # z will have shape (N, n_out)\n            z = a @ W.T\n            \n            # Calculate empirical population variance over all samples and units\n            emp_var = np.var(z)\n            v_emp_list.append(emp_var)\n            \n            # Update activations for the next layer (identity activation)\n            a = z\n            \n        # 3. Error Calculation\n        layer_errors = []\n        for emp, theo in zip(v_emp_list, v_theory_list):\n            error = np.abs(emp - theo) / np.maximum(theo, epsilon)\n            layer_errors.append(error)\n\n        if not layer_errors:\n            max_rel_error = 0.0\n        else:\n            max_rel_error = np.max(layer_errors)\n        \n        results.append(f\"{max_rel_error:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Xavier初始化并非万能钥匙，其有效性与激活函数的选择密切相关。本练习旨在通过实验对比，揭示这种关键的相互作用。您将通过模拟，将Xavier初始化与其著名的对应方法——He初始化进行比较，探究在对称激活函数（如$ \\tanh $）和非对称激活函数（如$ \\mathrm{ReLU} $）下，哪种初始化策略能更好地保持信号方差。这项实践将为您提供清晰的经验证据，解释为何在实践中推荐特定的初始化与激活函数配对。",
            "id": "3199598",
            "problem": "给定一个包含 $L$ 层的全连接前馈网络，定义第 $l$ 层的预激活为 $z^{(l)} = W^{(l)} a^{(l-1)}$，后激活为 $a^{(l)} = \\phi\\!\\left(z^{(l)}\\right)$，其中 $a^{(0)} = x$。假设偏置为零，权重独立同分布，输入 $x \\in \\mathbb{R}^{n}$ 的分量是独立的，均值为零，方差有限。考虑两种广泛使用的随机权重初始化策略：Xavier (Glorot) 正态初始化和 He (Kaiming) 正态初始化，以及两种激活函数：$\\tanh$ 和修正线性单元（$\\mathrm{ReLU}$）。目标是通过蒙特卡洛模拟，凭经验验证在哪种情况下，初始化策略能近似保持各层预激活的方差不变，即在给定激活函数下，对于所有层 $l \\in \\{1,\\dots,L\\}$，$\\operatorname{Var}\\!\\left(z^{(l)}\\right) \\approx \\operatorname{Var}(x)$。\n\n使用的基本原理：\n- 初始化时，权重和激活在各个坐标上的独立性，以及独立变量和的方差线性性质。\n- $\\tanh$ 和 $\\mathrm{ReLU}$ 作为逐点非线性函数的定义。\n- 对于数组 $Y \\in \\mathbb{R}^{s \\times d}$，沿 $s$ 个样本的样本方差估计量定义为 $\\widehat{\\operatorname{Var}}(Y) = \\frac{1}{d}\\sum_{j=1}^{d} \\left(\\frac{1}{s}\\sum_{i=1}^{s} Y_{ij}^{2} - \\left(\\frac{1}{s}\\sum_{i=1}^{s} Y_{ij}\\right)^{2} \\right)$。\n\n你的程序必须：\n1. 构建具有指定层数 $L$ 和层宽度的网络，其中为简化起见，每层的形状为 $(n_{\\text{in}}, n_{\\text{out}})$ 且 $n_{\\text{in}} = n_{\\text{out}}$。对每层的权重 $W^{(l)}$ 使用 Xavier 正态初始化或 He 正态初始化。使用每种策略指定的方差进行零均值正态初始化；不添加任何偏置。\n2. 将输入 $x$ 抽取为 $s$ 个维度为 $n$ 的独立样本，每个分量服从 $\\mathcal{N}(0,1)$ 分布，即均值为零，方差为 1。\n3. 对于每一层 $l$，通过对每个坐标的样本方差求平均，计算在 $s$ 个样本上的经验预激活方差 $\\widehat{\\operatorname{Var}}\\!\\left(z^{(l)}\\right)$，并类似地计算经验输入方差 $\\widehat{\\operatorname{Var}}(x)$。将第 $l$ 层的相对偏差定义为 $$\\delta^{(l)} = \\frac{\\left|\\widehat{\\operatorname{Var}}\\!\\left(z^{(l)}\\right) - \\widehat{\\operatorname{Var}}(x)\\right|}{\\widehat{\\operatorname{Var}}(x)}。$$ 如果 $\\max_{1 \\le l \\le L} \\delta^{(l)} \\le \\varepsilon$，则认为该测试用例保持了方差，容差为 $\\varepsilon = 0.25$。\n4. 使用一个固定种子为 $12345$ 的伪随机数生成器，以确保可复现性。\n\n测试套件：\n- 用例 1：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\tanh$，初始化 Xavier 正态。\n- 用例 2：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\tanh$，初始化 He 正态。\n- 用例 3：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化 He 正态。\n- 用例 4：$L=5$，宽度 $n=32$, 样本数 $s=8000$, 激活函数 $\\mathrm{ReLU}$, 初始化 Xavier 正态。\n- 用例 5：$L=1$，宽度 $n=32$，样本数 $s=20000$，激活函数 $\\tanh$，初始化 Xavier 正态。\n- 用例 6：$L=1$，宽度 $n=32$，样本数 $s=20000$，激活函数 $\\mathrm{ReLU}$，初始化 He 正态。\n- 用例 7：$L=15$，宽度 $n=16$，样本数 $s=8000$，激活函数 $\\tanh$，初始化 Xavier 正态。\n- 用例 8：$L=15$，宽度 $n=16$，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化 He 正态。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如 $\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$），其中每个 $\\text{result}_i$ 是一个布尔值，指示第 $i$ 个测试用例的方差是否被保持，顺序与上述测试套件完全一致。",
            "solution": "经评估，该问题是有效的。它在科学上基于深度学习的既定原则，特别是关于权重初始化及其对信号传播的影响。该问题提法明确，提供了所有必要的参数、定义以及清晰、客观的成功标准。它没有矛盾、歧义和事实错误。因此，我们可以着手解决。\n\n目标是通过实验验证在何种条件下，深度神经网络各层的预激活 $z^{(l)}$ 的方差得以保持。此分析的核心在于连续层之间预激活方差的递归关系。\n\n我们考虑第 $l$ 层单个神经元的预激活：\n$$ z_i^{(l)} = \\sum_{j=1}^{n_{l-1}} W_{ij}^{(l)} a_j^{(l-1)} $$\n这里，$W_{ij}^{(l)}$ 是连接第 $l-1$ 层神经元 $j$ 与第 $l$ 层神经元 $i$ 的权重，$a_j^{(l-1)}$ 是前一层神经元 $j$ 的激活值。问题指定偏置为零，权重 $W_{ij}^{(l)}$ 从零均值分布中抽取，输入分量 $x_j$（构成 $a_j^{(0)}$）也具有零均值。我们假设在初始化时，对于所有 $j$，激活值 $a_j^{(l-1)}$ 与权重 $W_{ij}^{(l)}$ 相互独立且同分布。此外，如果激活值 $a^{(l-1)}$ 是一个对称激活函数应用于零均值输入 $z^{(l-1)}$ 的输出，那么它们也将具有零均值。对于 $\\mathrm{ReLU}$ 激活函数，情况并非如此，但得到的预激活 $z^{(l)}$ 仍将具有零均值，因为权重本身是零均值的：$\\mathbb{E}[z_i^{(l)}] = \\sum_j \\mathbb{E}[W_{ij}^{(l)}] \\mathbb{E}[a_j^{(l-1)}] = \\sum_j 0 \\cdot \\mathbb{E}[a_j^{(l-1)}] = 0$。\n\n在这些条件下，$z_i^{(l)}$ 的方差由下式给出：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\operatorname{Var}\\left(\\sum_{j=1}^{n_{l-1}} W_{ij}^{(l)} a_j^{(l-1)}\\right) $$\n由于和中各项的独立性（因为权重与前一层的激活值是独立的），和的方差等于各项方差之和：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\sum_{j=1}^{n_{l-1}} \\operatorname{Var}(W_{ij}^{(l)} a_j^{(l-1)}) $$\n对于两个独立的随机变量 $U$ 和 $V$，其中至少一个均值为零（例如 $\\mathbb{E}[U]=0$），有 $\\operatorname{Var}(UV) = \\mathbb{E}[U^2V^2] - (\\mathbb{E}[UV])^2 = \\mathbb{E}[U^2]\\mathbb{E}[V^2] - (\\mathbb{E}[U]\\mathbb{E}[V])^2 = \\operatorname{Var}(U)\\operatorname{Var}(V)$。因为 $\\mathbb{E}[W_{ij}^{(l)}] = 0$，我们有：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\sum_{j=1}^{n_{l-1}} \\operatorname{Var}(W_{ij}^{(l)}) \\operatorname{Var}(a_j^{(l-1)}) $$\n假设第 $l$ 层的所有权重都是从方差为 $\\operatorname{Var}(W^{(l)})$ 的分布中独立同分布地抽取的，并且第 $l-1$ 层的所有激活值都是从方差为 $\\operatorname{Var}(a^{(l-1)})$ 的分布中独立同分布地抽取的，则上式可简化为：\n$$ \\operatorname{Var}(z^{(l)}) = n_{l-1} \\operatorname{Var}(W^{(l)}) \\operatorname{Var}(a^{(l-1)}) $$\n其中 $n_{l-1}$ 是第 $l-1$ 层的神经元数量。为了保持稳定的信号传播，我们要求方差保持不变，即 $\\operatorname{Var}(z^{(l)}) \\approx \\operatorname{Var}(z^{(l-1)})$。这需要仔细选择权重初始化方差 $\\operatorname{Var}(W^{(l)})$，以抵消激活函数 $\\phi$ 对方差的影响，该影响由 $\\operatorname{Var}(a^{(l-1)}) = \\operatorname{Var}(\\phi(z^{(l-1)}))$ 项所体现。\n\n**激活函数分析**\n\n1.  **$\\tanh$ 激活函数：** 双曲正切函数 $\\tanh(z)$ 关于原点对称（$\\tanh(0)=0$），并且对于小输入（$z \\approx 0$）其行为类似于恒等函数（$\\tanh(z) \\approx z$）。如果我们假设预激活 $z^{(l-1)}$ 集中在零附近（这是初始训练阶段的一个理想状态），那么 $\\operatorname{Var}(a^{(l-1)}) = \\operatorname{Var}(\\tanh(z^{(l-1)})) \\approx \\operatorname{Var}(z^{(l-1)})$。将此代入我们的传播方程得到：\n    $$ \\operatorname{Var}(z^{(l)}) \\approx n_{l-1} \\operatorname{Var}(W^{(l)}) \\operatorname{Var}(z^{(l-1)}) $$\n    为实现 $\\operatorname{Var}(z^{(l)}) \\approx \\operatorname{Var}(z^{(l-1)})$，我们必须设置 $n_{l-1} \\operatorname{Var}(W^{(l)}) = 1$。\n    **Xavier (Glorot) 正态初始化**正是为此情况设计的。它将从 $\\mathcal{N}(0, \\sigma^2)$ 抽取的权重 $W^{(l)}$ 的方差设置为：\n    $$ \\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1} + n_l} $$\n    在我们的具体问题中，$n_{l-1} = n_l = n$，所以这变为 $\\operatorname{Var}(W^{(l)}) = \\frac{2}{2n} = \\frac{1}{n} = \\frac{1}{n_{l-1}}$。这个选择完美地满足了条件 $n_{l-1} \\operatorname{Var}(W^{(l)}) = 1$。因此，Xavier 初始化预计能为 $\\tanh$ 激活函数保持方差。\n\n2.  **$\\mathrm{ReLU}$ 激活函数：** 修正线性单元 $\\mathrm{ReLU}(z) = \\max(0, z)$ 不是对称的。对于 $z^{(l-1)}$ 的一个零均值、对称输入分布（如高斯分布），恰好一半的输入将被设置为零。这会影响方差。设 $z \\sim \\mathcal{N}(0, \\sigma_z^2)$。输出 $a = \\mathrm{ReLU}(z)$ 的方差为 $\\operatorname{Var}(a) = \\mathbb{E}[a^2] - (\\mathbb{E}[a])^2$。\n    激活值平方的期望是 $\\mathbb{E}[a^2] = \\mathbb{E}[\\max(0, z)^2] = \\int_0^\\infty z^2 p(z) dz$。由于正态分布 $p(z)$ 的对称性，这个积分是 $z^2$ 总积分的一半：$\\mathbb{E}[a^2] = \\frac{1}{2} \\int_{-\\infty}^\\infty z^2 p(z) dz = \\frac{1}{2}\\mathbb{E}[z^2] = \\frac{1}{2}\\operatorname{Var}(z)$。\n    因此，对于 $\\mathrm{ReLU}$，我们有 $\\operatorname{Var}(a^{(l-1)}) \\approx \\frac{1}{2}\\operatorname{Var}(z^{(l-1)})$。方差传播方程变为：\n    $$ \\operatorname{Var}(z^{(l)}) \\approx n_{l-1} \\operatorname{Var}(W^{(l)}) \\left(\\frac{1}{2}\\operatorname{Var}(z^{(l-1)})\\right) $$\n    为了保持方差，我们必须有 $n_{l-1} \\operatorname{Var}(W^{(l)}) \\frac{1}{2} = 1$，这意味着 $\\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1}}$。\n    **He (Kaiming) 正态初始化**是为此情况设计的。它将方差设置为：\n    $$ \\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1}} $$\n    这个选择精确地满足了该条件。因此，He 初始化预计能为 $\\mathrm{ReLU}$ 激活函数保持方差。不匹配的组合（例如，$\\tanh$ 与 He，$\\mathrm{ReLU}$ 与 Xavier）预计将分别导致方差爆炸或消失。\n\n**模拟过程**\n\n程序将为 8 个测试用例中的每一个实现蒙特卡洛模拟。对于每个用例：\n1.  使用值 $12345$ 作为种子初始化伪随机数生成器，以确保可复现性。\n2.  生成一个输入数据矩阵 $x \\in \\mathbb{R}^{s \\times n}$，其中每个元素从 $\\mathcal{N}(0, 1)$ 中抽取。使用提供的公式计算经验输入方差 $\\widehat{\\operatorname{Var}}(x)$。\n3.  网络从 $l=1$ 到 $L$ 逐层处理。在每一层，权重矩阵 $W^{(l)}$ 从一个零均值正态分布中初始化，其方差由指定的策略（Xavier 或 He）决定。\n4.  计算预激活 $z^{(l)} = a^{(l-1)} W^{(l)}$。\n5.  计算经验方差 $\\widehat{\\operatorname{Var}}(z^{(l)})$。计算相对偏差 $\\delta^{(l)} = |\\widehat{\\operatorname{Var}}(z^{(l)}) - \\widehat{\\operatorname{Var}}(x)| / \\widehat{\\operatorname{Var}}(x)$。\n6.  追踪所有层中的最大相对偏差 $\\max_{1 \\le l \\le L} \\delta^{(l)}$。\n7.  计算后激活 $a^{(l)} = \\phi(z^{(l)})$，作为下一层的输入。\n8.  在遍历所有层之后，如果 $\\max_{l} \\delta^{(l)} \\le \\varepsilon$（其中 $\\varepsilon = 0.25$），则认为该测试用例保持了方差。此检查将为每个用例产生一个布尔结果，然后报告该结果。",
            "answer": "```python\nimport numpy as np\n\ndef tanh(x):\n    \"\"\"Hyperbolic tangent activation function.\"\"\"\n    return np.tanh(x)\n\ndef relu(x):\n    \"\"\"Rectified Linear Unit (ReLU) activation function.\"\"\"\n    return np.maximum(0, x)\n\ndef empirical_variance(Y):\n    \"\"\"\n    Computes the empirical variance as defined in the problem statement.\n    For an array Y of shape (s, d), this is the average of the biased sample \n    variances of each of the d columns.\n    \n    Formula: Var(Y) = (1/d) * sum_j [ (1/s) * sum_i(Y_ij^2) - ((1/s) * sum_i(Y_ij))^2 ]\n    This is equivalent to the mean of np.var(Y, axis=0).\n    \"\"\"\n    if Y.ndim == 1:\n        # If input is a 1D array, treat it as (s, 1)\n        return np.var(Y)\n    return np.mean(np.var(Y, axis=0))\n\ndef run_simulation(L, n, s, activation_func, init_strategy, seed, epsilon):\n    \"\"\"\n    Runs a single simulation for a given network configuration.\n\n    Args:\n        L (int): Number of layers.\n        n (int): Width of each layer.\n        s (int): Number of samples.\n        activation_func (callable): The activation function to use.\n        init_strategy (str): The weight initialization strategy ('xavier' or 'he').\n        seed (int): The seed for the random number generator.\n        epsilon (float): The tolerance for variance preservation.\n\n    Returns:\n        bool: True if variance is preserved, False otherwise.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate input data x of shape (s, n) from N(0, 1)\n    x = rng.normal(loc=0.0, scale=1.0, size=(s, n))\n    var_x = empirical_variance(x)\n    \n    a_prev = x\n    max_deviation = 0.0\n    \n    # Propagate through the network for L layers\n    for l in range(1, L + 1):\n        # Layer dimensions are (n_in, n_out)\n        n_in = n\n        n_out = n\n        \n        # Determine standard deviation for weight initialization\n        if init_strategy == 'xavier':\n            # Variance = 2 / (n_in + n_out)\n            std_dev = np.sqrt(2.0 / (n_in + n_out))\n        elif init_strategy == 'he':\n            # Variance = 2 / n_in\n            std_dev = np.sqrt(2.0 / n_in)\n        else:\n            raise ValueError(f\"Unknown initialization strategy: {init_strategy}\")\n            \n        # Initialize weights W of shape (n_in, n_out)\n        W = rng.normal(loc=0.0, scale=std_dev, size=(n_in, n_out))\n        \n        # Compute pre-activations z = a_prev @ W of shape (s, n_out)\n        z = a_prev @ W\n        \n        # Compute empirical variance and relative deviation\n        var_z = empirical_variance(z)\n        if var_x > 1e-9: # Avoid division by zero\n            deviation = np.abs(var_z - var_x) / var_x\n        else:\n            deviation = np.abs(var_z)\n\n        if deviation > max_deviation:\n            max_deviation = deviation\n            \n        # Compute post-activations for the next layer\n        a_prev = activation_func(z)\n        \n    # Check if the maximum deviation is within the tolerance\n    return max_deviation = epsilon\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define test cases as (L, n, s, activation_name, init_strategy_name)\n    test_cases = [\n        (5, 32, 8000, 'tanh', 'xavier'),\n        (5, 32, 8000, 'tanh', 'he'),\n        (5, 32, 8000, 'relu', 'he'),\n        (5, 32, 8000, 'relu', 'xavier'),\n        (1, 32, 20000, 'tanh', 'xavier'),\n        (1, 32, 20000, 'relu', 'he'),\n        (15, 16, 8000, 'tanh', 'xavier'),\n        (15, 16, 8000, 'relu', 'he'),\n    ]\n    \n    # Map string names to functions and parameters\n    activation_map = {'tanh': tanh, 'relu': relu}\n    epsilon = 0.25\n    seed = 12345\n    \n    results = []\n    \n    # Run simulation for each test case\n    for case in test_cases:\n        L, n, s, act_name, init_name = case\n        activation_func = activation_map[act_name]\n        \n        # Each test case is an independent experiment, so we use the same seed,\n        # creating a new RNG instance for each to ensure reproducibility.\n        result = run_simulation(L, n, s, activation_func, init_name, seed, epsilon)\n        results.append(result)\n        \n    # Format and print the final output as a single-line list of booleans\n    # e.g., [True,False,True,False,True,True,True,True]\n    # np.bool_ maps to Python's True/False, and str() correctly converts them.\n    # The problem asks for booleans, and Python's `str(True)` is 'True'.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solution\nsolve()\n```"
        }
    ]
}