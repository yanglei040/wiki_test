{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp Xavier initialization, it's essential to work through its mathematical foundations. This first exercise challenges you to derive the initialization bounds from the core principle of balancing forward and backward signal variance. By calculating the famous $\\sqrt{\\frac{6}{fan_{in} + fan_{out}}}$ formula and using it to predict the root mean square (RMS) of pre-activations, you will solidify your understanding of how this strategy aims to keep signals at a consistent scale at the very start of training .",
            "id": "3200147",
            "problem": "Consider a fully connected feedforward neural network with an input layer of size $\\mathbb{R}^{784}$, one hidden layer of size $\\mathbb{R}^{256}$, and an output layer of size $\\mathbb{R}^{10}$. Let the hidden pre-activation be $z^{(1)} = W^{(1)} x$ and the output pre-activation be $z^{(2)} = W^{(2)} h$, where $h$ is the hidden activation obtained by applying a pointwise nonlinearity to $z^{(1)}$. Assume the following conditions hold at initialization:\n- The input components are independent and identically distributed (i.i.d.) with zero mean and variance $\\operatorname{Var}(x_{i}) = 1$.\n- All biases are zero.\n- The nonlinearity is smooth and is linearized around zero at initialization so that its local gain is approximately $1$.\n- Each weight matrix $W^{(\\ell)}$ is initialized with independent entries drawn from a zero-mean uniform distribution supported on the symmetric interval $\\left[-a_{\\ell}, a_{\\ell}\\right]$.\n- Independence approximations apply so that covariances between distinct terms vanish when propagating variances forward and backward.\n\nThe design goal is to choose the weight variance so that, under the linearized and independence assumptions, the scale of signals is balanced in both the forward and backward directions. Concretely, if the forward variance-preservation factor for a layer with $f_{\\text{in}}$ inputs and $f_{\\text{out}}$ outputs is $f_{\\text{in}} \\operatorname{Var}(W)$ and the backward variance-preservation factor is $f_{\\text{out}} \\operatorname{Var}(W)$, enforce a symmetric compromise by matching the arithmetic mean of these two factors to $1$. Using only this principle, the variance formula for a uniform distribution on $\\left[-a, a\\right]$, and the variance propagation rule for sums of independent random variables, do the following:\n- Determine the bound $a_{1}$ for $W^{(1)}$ in the layer $\\mathbb{R}^{784} \\to \\mathbb{R}^{256}$.\n- Determine the bound $a_{2}$ for $W^{(2)}$ in the layer $\\mathbb{R}^{256} \\to \\mathbb{R}^{10}$.\n- Using your derived bound for $W^{(1)}$, predict the root mean square (RMS; root mean square) of the hidden pre-activations $z^{(1)}$ at initialization, where RMS is defined as $\\sqrt{\\mathbb{E}\\!\\left[(z^{(1)}_{j})^{2}\\right]}$.\n\nReport as your final answer only the numerical value of the hidden pre-activation RMS for $z^{(1)}$. Round your final answer to $4$ significant figures.",
            "solution": "This problem requires us to derive the Root Mean Square (RMS) of the hidden pre-activations from first principles. We will follow a step-by-step derivation.\n\n**1. Derive the Weight Variance from the Problem's Principle**\n\nThe problem states that we must choose the weight variance $\\operatorname{Var}(W)$ by matching the arithmetic mean of the forward and backward variance-preservation factors to 1. The factors for a layer with $f_{\\text{in}}$ inputs and $f_{\\text{out}}$ outputs are given as $F_{\\text{fwd}} = f_{\\text{in}} \\operatorname{Var}(W)$ and $F_{\\text{bwd}} = f_{\\text{out}} \\operatorname{Var}(W)$.\n\nThe specified rule is:\n$$ \\frac{F_{\\text{fwd}} + F_{\\text{bwd}}}{2} = 1 $$\nSubstituting the definitions of the factors:\n$$ \\frac{f_{\\text{in}} \\operatorname{Var}(W) + f_{\\text{out}} \\operatorname{Var}(W)}{2} = 1 $$\nWe can solve for $\\operatorname{Var}(W)$:\n$$ \\operatorname{Var}(W) (f_{\\text{in}} + f_{\\text{out}}) = 2 $$\n$$ \\operatorname{Var}(W) = \\frac{2}{f_{\\text{in}} + f_{\\text{out}}} $$\nThis is the general formula for the weight variance under this design principle.\n\n**2. Derive the Pre-activation Variance**\n\nNext, we derive the variance of a pre-activation unit $z^{(1)}_j$ in the first hidden layer. The pre-activation is a weighted sum of the inputs:\n$$ z^{(1)}_j = \\sum_{i=1}^{f_{\\text{in},1}} W_{ji}^{(1)} x_i $$\nGiven that the inputs $x_i$ and weights $W_{ji}^{(1)}$ are all independent random variables with zero mean, the expected value of the pre-activation is also zero: $\\mathbb{E}[z^{(1)}_j] = 0$.\nThe variance of the sum of independent random variables is the sum of their variances:\n$$ \\operatorname{Var}(z^{(1)}_j) = \\operatorname{Var}\\left(\\sum_{i=1}^{f_{\\text{in},1}} W_{ji}^{(1)} x_i\\right) = \\sum_{i=1}^{f_{\\text{in},1}} \\operatorname{Var}(W_{ji}^{(1)} x_i) $$\nFor two independent, zero-mean random variables $U$ and $V$, we know that $\\operatorname{Var}(UV) = \\operatorname{Var}(U)\\operatorname{Var}(V)$. Thus:\n$$ \\operatorname{Var}(z^{(1)}_j) = \\sum_{i=1}^{f_{\\text{in},1}} \\operatorname{Var}(W_{ji}^{(1)}) \\operatorname{Var}(x_i) $$\nSince all weights in the layer are i.i.d. with variance $\\operatorname{Var}(W^{(1)})$ and all inputs are i.i.d. with variance $\\operatorname{Var}(x) = 1$:\n$$ \\operatorname{Var}(z^{(1)}_j) = \\sum_{i=1}^{f_{\\text{in},1}} \\operatorname{Var}(W^{(1)}) \\cdot 1 = f_{\\text{in},1} \\cdot \\operatorname{Var}(W^{(1)}) $$\n\n**3. Combine and Calculate the Numerical Value**\n\nNow, we combine the results from the first two steps for the specific case of the first hidden layer. The fan-in and fan-out of the first layer are $f_{\\text{in},1} = 784$ and $f_{\\text{out},1} = 256$.\nFirst, calculate the variance of the weights in the first layer:\n$$ \\operatorname{Var}(W^{(1)}) = \\frac{2}{f_{\\text{in},1} + f_{\\text{out},1}} = \\frac{2}{784 + 256} = \\frac{2}{1040} $$\nNow, use this to calculate the variance of the pre-activations:\n$$ \\operatorname{Var}(z^{(1)}_j) = f_{\\text{in},1} \\cdot \\operatorname{Var}(W^{(1)}) = 784 \\cdot \\frac{2}{1040} = \\frac{1568}{1040} = \\frac{98}{65} $$\n\n**4. Calculate the Root Mean Square (RMS)**\n\nThe RMS is defined as $\\sqrt{\\mathbb{E}[Y^2]}$. For a random variable $Y$ with zero mean, its variance is $\\operatorname{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = \\mathbb{E}[Y^2]$. Therefore, the RMS is simply the square root of the variance.\n$$ \\text{RMS}(z^{(1)}_j) = \\sqrt{\\operatorname{Var}(z^{(1)}_j)} = \\sqrt{\\frac{98}{65}} $$\nCalculating the numerical value:\n$$ \\text{RMS}(z^{(1)}_j) \\approx 1.227881226... $$\n\nRounding to 4 significant figures, the final answer is $1.228$.",
            "answer": "$$\n\\boxed{1.228}\n$$"
        },
        {
            "introduction": "Theory provides the \"why,\" but simulation provides the \"aha!\" moment. In this practice, you will move from a single-layer calculation to observing variance propagation across a deep network . You will write code to empirically verify the theoretical predictions of Xavier initialization in a controlled linear network, building concrete intuition for how it prevents the signal from either vanishing or exploding as it travels through multiple layers.",
            "id": "3200162",
            "problem": "Consider a fully connected feedforward network with $L$ layers. Let the input dimension be $n_{0}$, and layer $l$ have $n_{l}$ units for $l \\in \\{1,\\dots,L\\}$. For each layer $l$, the pre-activation vector $z^{(l)} \\in \\mathbb{R}^{n_{l}}$ is given by $z^{(l)} = W^{(l)} a^{(l-1)}$, where $W^{(l)} \\in \\mathbb{R}^{n_{l} \\times n_{l-1}}$ is the weight matrix and $a^{(l-1)} \\in \\mathbb{R}^{n_{l-1}}$ is the previous layer’s activation. Assume the identity activation, so $a^{(l)} = z^{(l)}$ for all $l$. The inputs $a^{(0)}$ are sampled independently and identically distributed (i.i.d.) from a Gaussian distribution with mean $0$ and variance $1$, that is $a^{(0)} \\sim \\mathcal{N}(0,1)$ elementwise.\n\nThe weights are initialized using the Xavier (Glorot) normal initialization: each entry of $W^{(l)}$ is i.i.d. with mean $0$ and variance $2/(n_{l-1} + n_{l})$. You may assume independence between weights and activations at initialization.\n\nStarting from the fundamental properties of variance under independence and linearity (specifically, that for independent zero-mean random variables $x_{i}$ and constants $c_{i}$, $\\operatorname{Var}\\left(\\sum_{i} c_{i} x_{i}\\right) = \\sum_{i} c_{i}^{2} \\operatorname{Var}(x_{i})$; and that for independent random variables $x$ and $y$, $\\operatorname{Var}(xy) = \\operatorname{Var}(x)\\operatorname{Var}(y)$ when both have mean $0$), derive the theoretical progression of the layerwise pre-activation variance $v^{(l)} = \\operatorname{Var}(z^{(l)}_{j})$ (which is the same for all units $j$ within a layer by symmetry) in terms of $n_{l-1}$, $n_{l}$, and the previous layer’s activation variance $v^{(l-1)}$. Use that $v^{(0)} = 1$.\n\nYour program must:\n- Generate $N$ independent input samples of $a^{(0)}$ from $\\mathcal{N}(0,1)$ for each test case.\n- For each layer $l$, sample $W^{(l)}$ according to the Xavier normal initialization described above.\n- Propagate the inputs forward using identity activations and compute the empirical pre-activation variance at each layer $l$ as the population variance across all samples and units.\n- Compute the theoretical pre-activation variance at each layer using your derived expression.\n- For each test case, compute the maximum absolute relative error across layers, defined as $\\max_{l \\in \\{1,\\dots,L\\}} \\left| v_{\\text{emp}}^{(l)} - v_{\\text{theory}}^{(l)} \\right| / \\max\\{v_{\\text{theory}}^{(l)}, \\epsilon\\}$, where $\\epsilon$ is a small positive constant to avoid division by zero.\n\nUse the following test suite of parameter values, which together probe typical behavior, symmetry, and edge cases:\n- Case $1$: widths $[n_{0}, n_{1}, n_{2}] = [100, 100, 100]$, samples $N = 10000$.\n- Case $2$: widths $[n_{0}, n_{1}, n_{2}] = [50, 10, 200]$, samples $N = 10000$.\n- Case $3$: widths $[n_{0}, n_{1}, n_{2}] = [1, 1, 1]$, samples $N = 20000$.\n- Case $4$: widths $[n_{0}, n_{1}, n_{2}] = [1000, 100, 50]$, samples $N = 500$.\n- Case $5$: widths $[n_{0}, n_{1}, n_{2}, n_{3}, n_{4}] = [64, 64, 64, 64, 64]$, samples $N = 2000$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_{1},r_{2},r_{3},r_{4},r_{5}]$), where each $r_{i}$ is the maximum absolute relative error for the $i$-th test case rounded to $6$ decimal places. No physical units are involved, and angles are not applicable. The output must be exactly one line in the specified format.",
            "solution": "The problem is valid and can be solved by first deriving the theoretical progression of variance and then implementing a numerical simulation to verify it.\n\n**1. Theoretical Derivation of Variance Progression**\n\nWe seek a recurrence relation for the variance of the pre-activations, $v^{(l)} = \\operatorname{Var}(z_j^{(l)})$, where $z_j^{(l)}$ is the pre-activation of the $j$-th neuron in layer $l$. The layers are numbered $l \\in \\{1, \\dots, L\\}$. Symmetries in the initialization of weights and activations ensure that this variance is the same for all neurons $j$ within a given layer $l$.\n\nThe pre-activation is defined as a linear transformation of the previous layer's activations $a^{(l-1)}$:\n$$z_j^{(l)} = \\sum_{k=1}^{n_{l-1}} W_{jk}^{(l)} a_k^{(l-1)}$$\nwhere $W^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l-1}}$ is the weight matrix and $a^{(l-1)} \\in \\mathbb{R}^{n_{l-1}}$ is the activation vector from the previous layer.\n\nFirst, we establish that the mean of all activations and pre-activations remains $0$ throughout the network. The input activations $a_k^{(0)}$ are sampled from $\\mathcal{N}(0, 1)$, so $\\mathbb{E}[a_k^{(0)}] = 0$. The weights $W_{jk}^{(l)}$ are initialized from a distribution with mean $0$, so $\\mathbb{E}[W_{jk}^{(l)}] = 0$.\nNow, we proceed by induction. Assume $\\mathbb{E}[a_k^{(l-1)}] = 0$ for all $k$. The mean of the pre-activation $z_j^{(l)}$ is:\n$$\\mathbb{E}[z_j^{(l)}] = \\mathbb{E}\\left[\\sum_{k=1}^{n_{l-1}} W_{jk}^{(l)} a_k^{(l-1)}\\right]$$\nBy linearity of expectation:\n$$\\mathbb{E}[z_j^{(l)}] = \\sum_{k=1}^{n_{l-1}} \\mathbb{E}[W_{jk}^{(l)} a_k^{(l-1)}]$$\nSince the weights $W^{(l)}$ are independent of the activations $a^{(l-1)}$, we can separate the expectation:\n$$\\mathbb{E}[z_j^{(l)}] = \\sum_{k=1}^{n_{l-1}} \\mathbb{E}[W_{jk}^{(l)}] \\mathbb{E}[a_k^{(l-1)}] = \\sum_{k=1}^{n_{l-1}} 0 \\cdot 0 = 0$$\nGiven the identity activation function, $a^{(l)} = z^{(l)}$, it follows that $\\mathbb{E}[a_j^{(l)}] = 0$. With the base case $\\mathbb{E}[a_k^{(0)}] = 0$, all subsequent pre-activations and activations in the network have a mean of $0$.\n\nNext, we compute the variance $v^{(l)} = \\operatorname{Var}(z_j^{(l)})$:\n$$v^{(l)} = \\operatorname{Var}\\left(\\sum_{k=1}^{n_{l-1}} W_{jk}^{(l)} a_k^{(l-1)}\\right)$$\nThe variance of a sum of random variables is the sum of their covariances. The terms in the sum are $Y_k = W_{jk}^{(l)} a_k^{(l-1)}$. For $k \\neq k'$, the covariance is $\\operatorname{Cov}(Y_k, Y_{k'}) = \\mathbb{E}[Y_k Y_{k'}] - \\mathbb{E}[Y_k]\\mathbb{E}[Y_{k'}]$. Since $\\mathbb{E}[Y_k] = 0$, this simplifies to $\\mathbb{E}[Y_k Y_{k'}] = \\mathbb{E}[W_{jk}^{(l)} a_k^{(l-1)} W_{jk'}^{(l)} a_{k'}^{(l-1)}]$. The weights are all independent of each other and of the activations. Therefore, for $k \\neq k'$, $W_{jk}^{(l)}$ and $W_{jk'}^{(l)}$ are independent.\n$$\\operatorname{Cov}(Y_k, Y_{k'}) = \\mathbb{E}[W_{jk}^{(l)} W_{jk'}^{(l)}] \\mathbb{E}[a_k^{(l-1)} a_{k'}^{(l-1)}] = \\mathbb{E}[W_{jk}^{(l)}] \\mathbb{E}[W_{jk'}^{(l)}] \\mathbb{E}[a_k^{(l-1)} a_{k'}^{(l-1)}] = 0$$\nThus, the terms $Y_k$ are uncorrelated, and the variance of the sum is the sum of the variances:\n$$v^{(l)} = \\sum_{k=1}^{n_{l-1}} \\operatorname{Var}\\left(W_{jk}^{(l)} a_k^{(l-1)}\\right)$$\nWe use the provided property for the variance of a product of two independent, zero-mean random variables $X$ and $Y$: $\\operatorname{Var}(XY) = \\operatorname{Var}(X)\\operatorname{Var}(Y)$. Here, $X = W_{jk}^{(l)}$ and $Y = a_k^{(l-1)}$. Both are zero-mean and are independent by definition. Therefore:\n$$\\operatorname{Var}\\left(W_{jk}^{(l)} a_k^{(l-1)}\\right) = \\operatorname{Var}(W_{jk}^{(l)}) \\operatorname{Var}(a_k^{(l-1)})$$\nWe are given the variance of the weights from the Xavier normal initialization:\n$$\\operatorname{Var}(W_{jk}^{(l)}) = \\frac{2}{n_{l-1} + n_{l}}$$\nBy symmetry and the identity activation function, the variance of the previous layer's activations is uniform across its units: $\\operatorname{Var}(a_k^{(l-1)}) = v^{(l-1)}$.\nSubstituting these into the sum:\n$$v^{(l)} = \\sum_{k=1}^{n_{l-1}} \\left( \\frac{2}{n_{l-1} + n_{l}} \\cdot v^{(l-1)} \\right)$$\nThe term inside the summation is constant with respect to the index $k$. The sum therefore simplifies to a multiplication by $n_{l-1}$:\n$$v^{(l)} = n_{l-1} \\left( \\frac{2}{n_{l-1} + n_{l}} \\right) v^{(l-1)}$$\nThis is the final recurrence relation. Given the input variance $v^{(0)} = \\operatorname{Var}(a_k^{(0)}) = 1$, we can compute the theoretical variance for any layer $l$.\n\n**2. Simulation and Verification Methodology**\n\nThe numerical simulation is designed to verify the derived theoretical variance. The procedure is executed for each test case, which specifies the network layer widths ($n_0, n_1, \\dots, n_L$) and the number of independent samples $N$.\n\n1.  **Theoretical Variance Calculation**: For each layer $l$ from $1$ to $L$, the theoretical variance $v_{\\text{theory}}^{(l)}$ is computed iteratively using the derived recurrence relation $v^{(l)} = v^{(l-1)} \\cdot \\frac{2 n_{l-1}}{n_{l-1} + n_{l}}$, starting with the base case $v^{(0)} = 1$.\n\n2.  **Empirical Variance Simulation**:\n    - An input data matrix of size $N \\times n_0$ is generated by drawing $N \\times n_0$ i.i.d. samples from the standard normal distribution $\\mathcal{N}(0, 1)$. This represents $N$ input vectors $a^{(0)}$.\n    - The simulation proceeds layer by layer from $l=1$ to $L$. For each layer $l$:\n      - A weight matrix $W^{(l)}$ of size $n_l \\times n_{l-1}$ is created. Each element is sampled independently from the Xavier normal distribution $\\mathcal{N}(0, \\sigma_W^2)$ with variance $\\sigma_W^2 = 2 / (n_{l-1} + n_l)$.\n      - The pre-activations $z^{(l)}$ for all $N$ samples are computed efficiently via the matrix multiplication $a^{(l-1)} (W^{(l)})^T$, where $a^{(l-1)}$ is the $N \\times n_{l-1}$ matrix of activations from the previous layer.\n      - The empirical variance $v_{\\text{emp}}^{(l)}$ is computed as the population variance over the entire set of $N \\times n_l$ resulting pre-activation values.\n      - Due to the identity activation function, the activations for the next layer are set as $a^{(l)} = z^{(l)}$.\n\n3.  **Error Computation**: The maximum absolute relative error across all layers is calculated to quantify the agreement between theory and simulation:\n    $$ \\max_{l \\in \\{1,\\dots,L\\}} \\frac{\\left| v_{\\text{emp}}^{(l)} - v_{\\text{theory}}^{(l)} \\right|}{\\max\\{v_{\\text{theory}}^{(l)}, \\epsilon \\}} $$\n    where $\\epsilon$ is a small positive constant (e.g., $10^{-9}$) to prevent division by zero. A fixed seed for the random number generator ensures the reproducibility of the entire simulation.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and simulates the propagation of variance in a deep linear network\n    with Xavier (Glorot) normal initialization.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        ([100, 100, 100], 10000),         # Case 1: Symmetric, constant width\n        ([50, 10, 200], 10000),           # Case 2: Bottleneck then expansion\n        ([1, 1, 1], 20000),               # Case 3: Scalar \"network\"\n        ([1000, 100, 50], 500),           # Case 4: Deep and narrowing, fewer samples\n        ([64, 64, 64, 64, 64], 2000),     # Case 5: Deeper constant width network\n    ]\n\n    results = []\n    # Use a fixed seed for the random number generator for reproducibility.\n    seed = 42\n    rng = np.random.default_rng(seed)\n    epsilon = 1e-9\n\n    for widths, N in test_cases:\n        # 1. Theoretical Calculation\n        v_theory_list = []\n        v_prev_theory = 1.0  # v^(0) = 1\n        num_layers = len(widths) - 1\n        \n        for i in range(num_layers):\n            n_in = widths[i]\n            n_out = widths[i+1]\n            v_curr_theory = v_prev_theory * (2.0 * n_in) / (n_in + n_out)\n            v_theory_list.append(v_curr_theory)\n            v_prev_theory = v_curr_theory\n\n        # 2. Empirical Simulation\n        v_emp_list = []\n        # Initial activations a^(0)\n        # Shape: (N, n_0)\n        a = rng.normal(loc=0.0, scale=1.0, size=(N, widths[0]))\n        \n        for i in range(num_layers):\n            n_in = widths[i]\n            n_out = widths[i+1]\n            \n            # Sample weights W^(l) for the current layer\n            # Shape: (n_out, n_in)\n            std_dev_w = np.sqrt(2.0 / (n_in + n_out))\n            W = rng.normal(loc=0.0, scale=std_dev_w, size=(n_out, n_in))\n            \n            # Propagate activations forward: z^(l) = a^(l-1) @ (W^(l))^T\n            # a has shape (N, n_in), W.T has shape (n_in, n_out)\n            # z will have shape (N, n_out)\n            z = a @ W.T\n            \n            # Calculate empirical population variance over all samples and units\n            emp_var = np.var(z)\n            v_emp_list.append(emp_var)\n            \n            # Update activations for the next layer (identity activation)\n            a = z\n            \n        # 3. Error Calculation\n        layer_errors = []\n        for emp, theo in zip(v_emp_list, v_theory_list):\n            error = np.abs(emp - theo) / np.maximum(theo, epsilon)\n            layer_errors.append(error)\n\n        if not layer_errors:\n            max_rel_error = 0.0\n        else:\n            max_rel_error = np.max(layer_errors)\n        \n        results.append(f\"{max_rel_error:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The effectiveness of an initialization strategy is not absolute; it is deeply intertwined with the choice of activation function. This hands-on experiment places Xavier initialization in a broader practical context by comparing it with He initialization across networks using either $\\tanh$ or $\\mathrm{ReLU}$ activations. Through simulation, you will discover for yourself which pairings maintain stable signal variance, revealing the crucial design principle that links symmetric, saturating activations with Xavier initialization and non-saturating activations with He initialization .",
            "id": "3199598",
            "problem": "Given a fully connected feedforward network with $L$ layers, define the pre-activation at layer $l$ as $z^{(l)} = W^{(l)} a^{(l-1)}$ and the post-activation as $a^{(l)} = \\phi\\!\\left(z^{(l)}\\right)$, with $a^{(0)} = x$. Assume zero biases, independently and identically distributed weights, and an input $x \\in \\mathbb{R}^{n}$ whose components are independent, have zero mean, and finite variance. Consider two widely used random weight initialization strategies: Xavier (Glorot) normal initialization and He (Kaiming) normal initialization, and two activation functions: $\\tanh$ and Rectified Linear Unit ($\\mathrm{ReLU}$). The objective is to empirically verify, by Monte Carlo simulation, when an initialization strategy approximately preserves the variance of pre-activations across layers, that is, when $\\operatorname{Var}\\!\\left(z^{(l)}\\right) \\approx \\operatorname{Var}(x)$ for all layers $l \\in \\{1,\\dots,L\\}$ under a given activation function.\n\nFundamental base to use:\n- Independence of weights and activations across coordinates at initialization, and linearity of variance for independent sums.\n- The definitions of $\\tanh$ and $\\mathrm{ReLU}$ as pointwise nonlinearities.\n- The sample variance estimator defined for an array $Y \\in \\mathbb{R}^{s \\times d}$ along the $s$ samples as $\\widehat{\\operatorname{Var}}(Y) = \\frac{1}{d}\\sum_{j=1}^{d} \\left(\\frac{1}{s}\\sum_{i=1}^{s} Y_{ij}^{2} - \\left(\\frac{1}{s}\\sum_{i=1}^{s} Y_{ij}\\right)^{2} \\right)$.\n\nYour program must:\n1. Construct networks with specified $L$ and layer widths, where each layer has shape $(n_{\\text{in}}, n_{\\text{out}})$ and here $n_{\\text{in}} = n_{\\text{out}}$ for simplicity, using either Xavier normal or He normal initialization for each layer’s weights $W^{(l)}$. Use zero-mean normal initializations with variances prescribed by each strategy; do not add any biases.\n2. Draw the input $x$ as $s$ independent samples of dimension $n$, each component distributed as $\\mathcal{N}(0,1)$, i.e., zero mean and variance $1$.\n3. For each layer $l$, compute the empirical pre-activation variance $\\widehat{\\operatorname{Var}}\\!\\left(z^{(l)}\\right)$ over the $s$ samples by averaging per-coordinate sample variances, and compute the empirical input variance $\\widehat{\\operatorname{Var}}(x)$ similarly. Define the relative deviation at layer $l$ as $$\\delta^{(l)} = \\frac{\\left|\\widehat{\\operatorname{Var}}\\!\\left(z^{(l)}\\right) - \\widehat{\\operatorname{Var}}(x)\\right|}{\\widehat{\\operatorname{Var}}(x)}.$$ A test case is deemed to preserve variance if $\\max_{1 \\le l \\le L} \\delta^{(l)} \\le \\varepsilon$, with tolerance $\\varepsilon = 0.25$.\n4. Use a pseudo-random generator with a fixed seed $12345$ to ensure reproducibility.\n\nTest suite:\n- Case $1$: $L=5$, width $n=32$, samples $s=8000$, activation $\\tanh$, initialization Xavier normal.\n- Case $2$: $L=5$, width $n=32$, samples $s=8000$, activation $\\tanh$, initialization He normal.\n- Case $3$: $L=5$, width $n=32$, samples $s=8000$, activation $\\mathrm{ReLU}$, initialization He normal.\n- Case $4$: $L=5$, width $n=32$, samples $s=8000$, activation $\\mathrm{ReLU}$, initialization Xavier normal.\n- Case $5$: $L=1$, width $n=32$, samples $s=20000$, activation $\\tanh$, initialization Xavier normal.\n- Case $6$: $L=1$, width $n=32$, samples $s=20000$, activation $\\mathrm{ReLU}$, initialization He normal.\n- Case $7$: $L=15$, width $n=16$, samples $s=8000$, activation $\\tanh$, initialization Xavier normal.\n- Case $8$: $L=15$, width $n=16$, samples $s=8000$, activation $\\mathrm{ReLU}$, initialization He normal.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$), where each $\\text{result}_i$ is a boolean indicating whether variance was preserved for the $i$-th test case, in the exact order of the test suite above.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the established principles of deep learning, specifically concerning weight initialization and its impact on signal propagation. The problem is well-posed, providing all necessary parameters, definitions, and a clear, objective criterion for success. It is free of contradictions, ambiguities, and factual errors. We may therefore proceed with a solution.\n\nThe objective is to empirically verify the conditions under which the variance of pre-activations, $z^{(l)}$, is preserved across the layers of a deep neural network. The core of this analysis lies in the recursive relationship between the variance of pre-activations in successive layers.\n\nLet us consider a single neuron's pre-activation at layer $l$:\n$$ z_i^{(l)} = \\sum_{j=1}^{n_{l-1}} W_{ij}^{(l)} a_j^{(l-1)} $$\nHere, $W_{ij}^{(l)}$ is the weight connecting neuron $j$ of layer $l-1$ to neuron $i$ of layer $l$, and $a_j^{(l-1)}$ is the activation of neuron $j$ from the previous layer. The problem specifies that biases are zero, weights $W_{ij}^{(l)}$ are drawn from a distribution with zero mean, and the input components $x_j$ (which constitute $a_j^{(0)}$) also have zero mean. We assume that at initialization, the activations $a_j^{(l-1)}$ for all $j$ are independent of the weights $W_{ij}^{(l)}$ and are identically distributed. Furthermore, if the activations $a^{(l-1)}$ are the output of a symmetric activation function applied to zero-mean inputs $z^{(l-1)}$, they will also have zero mean. For the $\\mathrm{ReLU}$ activation, this is not the case, but the resulting pre-activations $z^{(l)}$ will still have zero mean because the weights themselves are zero-mean: $\\mathbb{E}[z_i^{(l)}] = \\sum_j \\mathbb{E}[W_{ij}^{(l)}] \\mathbb{E}[a_j^{(l-1)}] = \\sum_j 0 \\cdot \\mathbb{E}[a_j^{(l-1)}] = 0$.\n\nUnder these conditions, the variance of $z_i^{(l)}$ is given by:\n$$ \\operatorname{Var}(z_i^{(l)}) = \\operatorname{Var}\\left(\\sum_{j=1}^{n_{l-1}} W_{ij}^{(l)} a_j^{(l-1)}\\right) $$\nDue to the independence of the terms in the sum (as weights and previous layer activations are independent), the variance of the sum is the sum of the variances:\n$$ \\operatorname{Var}(z_i^{(l)}) = \\sum_{j=1}^{n_{l-1}} \\operatorname{Var}(W_{ij}^{(l)} a_j^{(l-1)}) $$\nFor two independent random variables $U$ and $V$ with at least one having zero mean (e.g., $\\mathbb{E}[U]=0$), $\\operatorname{Var}(UV) = \\mathbb{E}[U^2V^2] - (\\mathbb{E}[UV])^2 = \\mathbb{E}[U^2]\\mathbb{E}[V^2] - (\\mathbb{E}[U]\\mathbb{E}[V])^2 = \\operatorname{Var}(U)\\operatorname{Var}(V)$. Since $\\mathbb{E}[W_{ij}^{(l)}] = 0$, we have:\n$$ \\operatorname{Var}(z_i^{(l)}) = \\sum_{j=1}^{n_{l-1}} \\operatorname{Var}(W_{ij}^{(l)}) \\operatorname{Var}(a_j^{(l-1)}) $$\nAssuming all weights in layer $l$ are drawn i.i.d. with variance $\\operatorname{Var}(W^{(l)})$ and all activations from layer $l-1$ are i.i.d. with variance $\\operatorname{Var}(a^{(l-1)})$, this simplifies to:\n$$ \\operatorname{Var}(z^{(l)}) = n_{l-1} \\operatorname{Var}(W^{(l)}) \\operatorname{Var}(a^{(l-1)}) $$\nwhere $n_{l-1}$ is the number of neurons in layer $l-1$. To maintain stable signal propagation, we require the variance to be preserved, i.e., $\\operatorname{Var}(z^{(l)}) \\approx \\operatorname{Var}(z^{(l-1)})$. This requires a careful choice of the weight initialization variance, $\\operatorname{Var}(W^{(l)})$, to counteract the effect of the activation function $\\phi$ on the variance, which is captured by the term $\\operatorname{Var}(a^{(l-1)}) = \\operatorname{Var}(\\phi(z^{(l-1)}))$.\n\n**Activation Function Analysis**\n\n1.  **$\\tanh$ Activation:** The hyperbolic tangent function, $\\tanh(z)$, is symmetric around the origin ($\\tanh(0)=0$) and behaves like the identity function for small inputs ($\\tanh(z) \\approx z$ for $z \\approx 0$). If we assume the pre-activations $z^{(l-1)}$ are concentrated around zero, which is a desirable state during initial training phases, then $\\operatorname{Var}(a^{(l-1)}) = \\operatorname{Var}(\\tanh(z^{(l-1)})) \\approx \\operatorname{Var}(z^{(l-1)})$. Substituting this into our propagation equation yields:\n    $$ \\operatorname{Var}(z^{(l)}) \\approx n_{l-1} \\operatorname{Var}(W^{(l)}) \\operatorname{Var}(z^{(l-1)}) $$\n    To achieve $\\operatorname{Var}(z^{(l)}) \\approx \\operatorname{Var}(z^{(l-1)})$, we must set $n_{l-1} \\operatorname{Var}(W^{(l)}) = 1$.\n    **Xavier (Glorot) normal initialization** is designed precisely for this situation. It sets the variance of the weights $W^{(l)}$ drawn from $\\mathcal{N}(0, \\sigma^2)$ as:\n    $$ \\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1} + n_l} $$\n    In our specific problem, $n_{l-1} = n_l = n$, so this becomes $\\operatorname{Var}(W^{(l)}) = \\frac{2}{2n} = \\frac{1}{n} = \\frac{1}{n_{l-1}}$. This choice perfectly satisfies the condition $n_{l-1} \\operatorname{Var}(W^{(l)}) = 1$. Thus, Xavier initialization is expected to preserve variance for the $\\tanh$ activation function.\n\n2.  **$\\mathrm{ReLU}$ Activation:** The Rectified Linear Unit, $\\mathrm{ReLU}(z) = \\max(0, z)$, is not symmetric. For a zero-mean, symmetric input distribution for $z^{(l-1)}$ (like a Gaussian), exactly half of the inputs will be set to zero. This affects the variance. Let $z \\sim \\mathcal{N}(0, \\sigma_z^2)$. The variance of the output $a = \\mathrm{ReLU}(z)$ is $\\operatorname{Var}(a) = \\mathbb{E}[a^2] - (\\mathbb{E}[a])^2$.\n    The expectation of the squared activation is $\\mathbb{E}[a^2] = \\mathbb{E}[\\max(0, z)^2] = \\int_0^\\infty z^2 p(z) dz$. Due to the symmetry of the normal distribution $p(z)$, this integral is half of the total integral for $z^2$: $\\mathbb{E}[a^2] = \\frac{1}{2} \\int_{-\\infty}^\\infty z^2 p(z) dz = \\frac{1}{2}\\mathbb{E}[z^2] = \\frac{1}{2}\\operatorname{Var}(z)$.\n    So, for $\\mathrm{ReLU}$, we have $\\operatorname{Var}(a^{(l-1)}) \\approx \\frac{1}{2}\\operatorname{Var}(z^{(l-1)})$. The variance propagation equation becomes:\n    $$ \\operatorname{Var}(z^{(l)}) \\approx n_{l-1} \\operatorname{Var}(W^{(l)}) \\left(\\frac{1}{2}\\operatorname{Var}(z^{(l-1)})\\right) $$\n    To preserve variance, we must have $n_{l-1} \\operatorname{Var}(W^{(l)}) \\frac{1}{2} = 1$, which implies $\\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1}}$.\n    **He (Kaiming) normal initialization** is designed for this case. It sets the variance as:\n    $$ \\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1}} $$\n    This choice satisfies the condition exactly. Therefore, He initialization is expected to preserve variance for the $\\mathrm{ReLU}$ activation function. Mismatched pairs (e.g., $\\tanh$ with He, $\\mathrm{ReLU}$ with Xavier) are predicted to lead to exploding or vanishing variances, respectively.\n\n**Simulation Procedure**\n\nThe program will implement a Monte Carlo simulation for each of the $8$ test cases. For each case:\n1.  A pseudo-random number generator is seeded with the value $12345$ to ensure reproducibility.\n2.  An input data matrix $x \\in \\mathbb{R}^{s \\times n}$ is generated, where each element is drawn from $\\mathcal{N}(0, 1)$. The empirical input variance, $\\widehat{\\operatorname{Var}}(x)$, is calculated using the provided formula.\n3.  The network is processed layer by layer, from $l=1$ to $L$. In each layer, the weight matrix $W^{(l)}$ is initialized from a zero-mean normal distribution with the variance dictated by the specified strategy (Xavier or He).\n4.  The pre-activations $z^{(l)} = a^{(l-1)} W^{(l)}$ are computed.\n5.  The empirical variance $\\widehat{\\operatorname{Var}}(z^{(l)})$ is computed. The relative deviation $\\delta^{(l)} = |\\widehat{\\operatorname{Var}}(z^{(l)}) - \\widehat{\\operatorname{Var}}(x)| / \\widehat{\\operatorname{Var}}(x)$ is calculated.\n6.  The maximum relative deviation across all layers, $\\max_{1 \\le l \\le L} \\delta^{(l)}$, is tracked.\n7.  The post-activations $a^{(l)} = \\phi(z^{(l)})$ are computed to serve as input for the next layer.\n8.  After iterating through all layers, the test case is deemed to preserve variance if $\\max_{l} \\delta^{(l)} \\le \\varepsilon$, where $\\varepsilon = 0.25$. This check will yield a boolean result for each case, which is then reported.",
            "answer": "```python\nimport numpy as np\n\ndef tanh(x):\n    \"\"\"Hyperbolic tangent activation function.\"\"\"\n    return np.tanh(x)\n\ndef relu(x):\n    \"\"\"Rectified Linear Unit (ReLU) activation function.\"\"\"\n    return np.maximum(0, x)\n\ndef empirical_variance(Y):\n    \"\"\"\n    Computes the empirical variance as defined in the problem statement.\n    For an array Y of shape (s, d), this is the average of the biased sample \n    variances of each of the d columns.\n    \n    Formula: Var(Y) = (1/d) * sum_j [ (1/s) * sum_i(Y_ij^2) - ((1/s) * sum_i(Y_ij))^2 ]\n    This is equivalent to the mean of np.var(Y, axis=0).\n    \"\"\"\n    if Y.ndim == 1:\n        # If input is a 1D array, treat it as (s, 1)\n        return np.var(Y)\n    return np.mean(np.var(Y, axis=0))\n\ndef run_simulation(L, n, s, activation_func, init_strategy, seed, epsilon):\n    \"\"\"\n    Runs a single simulation for a given network configuration.\n\n    Args:\n        L (int): Number of layers.\n        n (int): Width of each layer.\n        s (int): Number of samples.\n        activation_func (callable): The activation function to use.\n        init_strategy (str): The weight initialization strategy ('xavier' or 'he').\n        seed (int): The seed for the random number generator.\n        epsilon (float): The tolerance for variance preservation.\n\n    Returns:\n        bool: True if variance is preserved, False otherwise.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate input data x of shape (s, n) from N(0, 1)\n    x = rng.normal(loc=0.0, scale=1.0, size=(s, n))\n    var_x = empirical_variance(x)\n    \n    a_prev = x\n    max_deviation = 0.0\n    \n    # Propagate through the network for L layers\n    for l in range(1, L + 1):\n        # Layer dimensions are (n_in, n_out)\n        n_in = n\n        n_out = n\n        \n        # Determine standard deviation for weight initialization\n        if init_strategy == 'xavier':\n            # Variance = 2 / (n_in + n_out)\n            std_dev = np.sqrt(2.0 / (n_in + n_out))\n        elif init_strategy == 'he':\n            # Variance = 2 / n_in\n            std_dev = np.sqrt(2.0 / n_in)\n        else:\n            raise ValueError(f\"Unknown initialization strategy: {init_strategy}\")\n            \n        # Initialize weights W of shape (n_in, n_out)\n        W = rng.normal(loc=0.0, scale=std_dev, size=(n_in, n_out))\n        \n        # Compute pre-activations z = a_prev @ W of shape (s, n_out)\n        z = a_prev @ W\n        \n        # Compute empirical variance and relative deviation\n        var_z = empirical_variance(z)\n        if var_x > 1e-9: # Avoid division by zero\n            deviation = np.abs(var_z - var_x) / var_x\n        else:\n            deviation = np.abs(var_z)\n\n        if deviation > max_deviation:\n            max_deviation = deviation\n            \n        # Compute post-activations for the next layer\n        a_prev = activation_func(z)\n        \n    # Check if the maximum deviation is within the tolerance\n    return max_deviation = epsilon\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define test cases as (L, n, s, activation_name, init_strategy_name)\n    test_cases = [\n        (5, 32, 8000, 'tanh', 'xavier'),\n        (5, 32, 8000, 'tanh', 'he'),\n        (5, 32, 8000, 'relu', 'he'),\n        (5, 32, 8000, 'relu', 'xavier'),\n        (1, 32, 20000, 'tanh', 'xavier'),\n        (1, 32, 20000, 'relu', 'he'),\n        (15, 16, 8000, 'tanh', 'xavier'),\n        (15, 16, 8000, 'relu', 'he'),\n    ]\n    \n    # Map string names to functions and parameters\n    activation_map = {'tanh': tanh, 'relu': relu}\n    epsilon = 0.25\n    seed = 12345\n    \n    results = []\n    \n    # Run simulation for each test case\n    for case in test_cases:\n        L, n, s, act_name, init_name = case\n        activation_func = activation_map[act_name]\n        \n        # Each test case is an independent experiment, so we use the same seed,\n        # creating a new RNG instance for each to ensure reproducibility.\n        result = run_simulation(L, n, s, activation_func, init_name, seed, epsilon)\n        results.append(result)\n        \n    # Format and print the final output as a single-line list of booleans\n    # e.g., [True,False,True,False,True,True,True,True]\n    # np.bool_ maps to Python's True/False, and str() correctly converts them.\n    # The problem asks for booleans, and Python's `str(True)` is 'True'.\n    print(f\"[{','.join(str(r).capitalize() for r in results)}]\")\n\n# Execute the solution\nsolve()\n```"
        }
    ]
}