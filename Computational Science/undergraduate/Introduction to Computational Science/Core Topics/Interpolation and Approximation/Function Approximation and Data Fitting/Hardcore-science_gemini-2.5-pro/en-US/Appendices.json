{
    "hands_on_practices": [
        {
            "introduction": "Real-world data and functions often contain sharp features or discontinuities that pose a challenge for smooth approximants. This exercise  provides hands-on practice in approximating the Rectified Linear Unit (ReLU) function, which is fundamental in machine learning and features a distinct \"kink\" at the origin. By comparing a global polynomial fit to a local smoothing spline, you will gain direct insight into their differing abilities to handle non-smooth behavior and the trade-offs involved in function and derivative accuracy.",
            "id": "3133577",
            "problem": "You are given the task of approximating the Rectified Linear Unit (ReLU) function, defined as $f(x) = \\max(0, x)$, on finite intervals using smooth approximations. The goal is to construct two families of approximants and to quantify their accuracy in terms of maximum function error and maximum derivative mismatch. The work must be grounded in fundamental principles of least-squares approximation and smoothing variational formulations.\n\nConstruct and evaluate the following approximants:\n- A polynomial least-squares approximant: For a chosen interval $[a,b]$ and polynomial degree $n$, select $N_{\\text{fit}}$ uniformly spaced sample points $\\{x_i\\}_{i=1}^{N_{\\text{fit}}}$ on $[a,b]$ and form an approximant $p_n(x)$ that minimizes the discrete least-squares objective $\\sum_{i=1}^{N_{\\text{fit}}} \\left(p_n(x_i) - f(x_i)\\right)^2$. Evaluate the resulting approximation and its derivative on a fine evaluation grid.\n- A cubic smoothing spline approximant: For the same interval $[a,b]$, sample the function at $N_{\\text{fit}}$ uniformly spaced points as above and construct a cubic smoothing spline $s(x)$ that balances fidelity to the data and smoothness. The spline is defined as the minimizer of a functional of the form $\\sum_{i=1}^{N_{\\text{fit}}} \\left(s(x_i) - f(x_i)\\right)^2 + \\lambda \\int_{a}^{b} \\left(s''(x)\\right)^2 \\, dx$, where $\\lambda \\ge 0$ denotes a smoothness penalty. Use a standard cubic spline ($k=3$) and a specified smoothing parameter. Evaluate the resulting approximation and its derivative on a fine evaluation grid.\n\nDefine the accuracy metrics to be reported:\n- The maximum function error over the interval $[a,b]$ computed on a fine grid of $N_{\\text{eval}}$ points, given by\n$$E_{\\max} = \\max_{x \\in [a,b]} \\left|g(x) - f(x)\\right|,$$\nwhere $g(x)$ denotes the approximant (either $p_n(x)$ or $s(x)$).\n- The maximum derivative mismatch over $[a,b] \\setminus \\{0\\}$ computed on a fine grid and excluding $x=0$ where $f'(x)$ is undefined, given by\n$$D_{\\max} = \\max_{x \\in [a,b] \\setminus \\{0\\}} \\left|g'(x) - f'(x)\\right|,$$\nwith $f'(x)=0$ for $x<0$ and $f'(x)=1$ for $x>0$.\n\nImplementation requirements:\n- Use uniform grids for both fitting and evaluation.\n- For polynomial least-squares approximation, derive $p_n(x)$ by solving the linear least-squares problem for the coefficients against the monomial basis $\\{1, x, x^2, \\dots, x^n\\}$ formed on the fitting grid.\n- For cubic smoothing splines, use a cubic spline ($k=3$) with the specified smoothing parameter and uniform fit grid; the spline must be differentiable and its derivative $s'(x)$ must be evaluated on the fine grid.\n- The derivative of the Rectified Linear Unit (ReLU) function $f'(x)$ is piecewise defined as $0$ for $x<0$ and $1$ for $x>0$, and is excluded at $x=0$.\n\nEvaluation grid and numerical details:\n- Use $N_{\\text{eval}} = 10001$ uniformly spaced points on each interval $[a,b]$ for evaluation.\n- When computing $D_{\\max}$, exclude the point $x=0$ from the evaluation grid by omitting points with $|x| \\le \\varepsilon$, where $\\varepsilon = 10^{-12}$.\n\nTest suite:\n- Polynomial case $1$: $[a,b]=[-1,1]$, degree $n=3$, $N_{\\text{fit}}=501$.\n- Polynomial case $2$: $[a,b]=[-3,3]$, degree $n=7$, $N_{\\text{fit}}=2001$.\n- Spline case $1$: $[a,b]=[-1,1]$, smoothing parameter $s=0.0$, $N_{\\text{fit}}=201$.\n- Spline case $2$: $[a,b]=[-1,1]$, smoothing parameter $s=0.01$, $N_{\\text{fit}}=201$.\n- Spline case $3$: $[a,b]=[-1,1]$, smoothing parameter $s=10.0$, $N_{\\text{fit}}=201$.\n\nAnswer specification:\n- For each test case, compute and return a list containing two floats $[E_{\\max}, D_{\\max}]$, each rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, specifically a list of these per-case lists. For example, an output with three cases would look like $[[e_1,d_1],[e_2,d_2],[e_3,d_3]]$, where each $e_i$ and $d_i$ are decimal floats rounded to six places.\n\nAll quantities are dimensionless. Angles are not involved in this problem. No external input or files should be used; the program must be self-contained.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the established principles of numerical analysis and function approximation, is well-posed with all necessary information provided, and is expressed in objective, formal language. The task is to approximate the Rectified Linear Unit (ReLU) function using polynomial least-squares and cubic smoothing splines and to quantify the approximation quality. The problem is a standard exercise in computational science.\n\nThe function to be approximated is the Rectified Linear Unit (ReLU), defined as:\n$$f(x) = \\max(0, x)$$\nThis function is continuous, but its first derivative is discontinuous at $x=0$. The derivative, where it exists, is the Heaviside step function:\n$$\nf'(x) = \\begin{cases}\n    0 & \\text{if } x < 0 \\\\\n    1 & \\text{if } x > 0\n\\end{cases}\n$$\nThe non-differentiability at $x=0$ poses a challenge for smooth approximants.\n\n**Method 1: Polynomial Least-Squares Approximation**\n\nThe first method seeks an approximation in the form of a polynomial of degree $n$:\n$$p_n(x) = \\sum_{j=0}^{n} c_j x^j$$\nThe coefficients $\\mathbf{c} = [c_0, c_1, \\dots, c_n]^T$ are determined by minimizing the sum of squared errors over a set of $N_{\\text{fit}}$ discrete sample points $\\{x_i\\}_{i=1}^{N_{\\text{fit}}}$ on the interval $[a,b]$. The objective function to minimize is:\n$$S(\\mathbf{c}) = \\sum_{i=1}^{N_{\\text{fit}}} \\left( p_n(x_i) - f(x_i) \\right)^2$$\nSubstituting the expression for $p_n(x)$, we have a linear least-squares problem. This can be expressed in matrix form as minimizing $\\|\\mathbf{A}\\mathbf{c} - \\mathbf{y}\\|_2^2$, where $\\mathbf{y}$ is the vector of function values $f(x_i)$ and $\\mathbf{A}$ is the Vandermonde matrix with entries $A_{ij} = x_i^j$ for $i \\in \\{1, \\dots, N_{\\text{fit}}\\}$ and $j \\in \\{0, \\dots, n\\}$. The solution is found by solving the normal equations, $\\mathbf{A}^T\\mathbf{A}\\mathbf{c} = \\mathbf{A}^T\\mathbf{y}$. Numerically stable algorithms, such as those based on QR decomposition, are employed to solve for $\\mathbf{c}$. Once the coefficients are known, the polynomial $p_n(x)$ and its derivative, $p'_n(x) = \\sum_{j=1}^{n} j c_j x^{j-1}$, can be evaluated at any point.\n\n**Method 2: Cubic Smoothing Spline Approximation**\n\nThe second method employs a cubic smoothing spline, $s(x)$. Unlike a pure interpolation, a smoothing spline is the function that minimizes a penalized sum of squares. The objective functional being minimized is:\n$$ \\sum_{i=1}^{N_{\\text{fit}}} \\left(s(x_i) - f(x_i)\\right)^2 + \\lambda \\int_{a}^{b} \\left(s''(x)\\right)^2 \\, dx $$\nThe first term enforces fidelity to the data points, while the second term, weighted by a smoothing parameter $\\lambda \\ge 0$, penalizes roughness, measured by the integrated squared second derivative. This represents a classic trade-off between bias (lack of fit) and variance (excessive oscillation). Standard numerical libraries often use a related smoothing parameter $s$, which sets an upper bound on the sum of squared residuals, $\\sum_{i=1}^{N_{\\text{fit}}} (s(x_i) - f(x_i))^2 \\le s$. When $s=0$, the spline is constrained to pass through all data points, resulting in an interpolating spline. As $s$ increases, the spline becomes smoother, deviating more from the data points. A cubic spline ($k=3$) is a piecewise polynomial of degree $3$ that is continuous and has continuous first and second derivatives ($C^2$ continuity) across the entire interval. The resulting spline object can be evaluated for both its value $s(x)$ and its derivative $s'(x)$.\n\n**Error Metrics Calculation**\n\nThe quality of each approximation $g(x)$ (where $g$ is either $p_n$ or $s$) is assessed using two metrics on a fine evaluation grid of $N_{\\text{eval}}$ points on $[a,b]$.\n\n1.  **Maximum Function Error ($E_{\\max}$)**: This metric quantifies the largest deviation of the approximant from the true function value.\n    $$E_{\\max} = \\max_{x \\in [a,b]} \\left|g(x) - f(x)\\right|$$\n\n2.  **Maximum Derivative Mismatch ($D_{\\max}$)**: This metric measures the largest deviation of the approximant's derivative from the true function's derivative.\n    $$D_{\\max} = \\max_{x \\in [a,b] \\setminus \\{0\\}} \\left|g'(x) - f'(x)\\right|$$\n    The point $x=0$ is explicitly excluded from this calculation because $f'(x)$ is undefined there. Computationally, this is handled by ignoring points within a small tolerance $\\varepsilon$ of zero, i.e., where $|x| \\le \\varepsilon = 10^{-12}$.\n\nThe following algorithm will be implemented to compute the results for each test case as specified in the problem statement.",
            "answer": "```python\nimport numpy as np\nfrom scipy.interpolate import UnivariateSpline\nfrom numpy.polynomial import polynomial as P\n\ndef solve():\n    \"\"\"\n    Constructs and evaluates polynomial and spline approximations of the ReLU function.\n    \"\"\"\n    \n    # Define the target function and its derivative\n    f_relu = lambda x: np.maximum(0, x)\n    f_relu_prime = lambda x: (x > 0).astype(float)\n    \n    # Define evaluation grid parameters\n    N_eval = 10001\n    epsilon = 1e-12\n    \n    # Test suite provided in the problem statement\n    test_cases = [\n        {'type': 'polynomial', 'interval': [-1, 1], 'degree': 3, 'N_fit': 501},\n        {'type': 'polynomial', 'interval': [-3, 3], 'degree': 7, 'N_fit': 2001},\n        {'type': 'spline', 'interval': [-1, 1], 's': 0.0, 'N_fit': 201},\n        {'type': 'spline', 'interval': [-1, 1], 's': 0.01, 'N_fit': 201},\n        {'type': 'spline', 'interval': [-1, 1], 's': 10.0, 'N_fit': 201},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        a, b = case['interval']\n        N_fit = case['N_fit']\n        \n        # Create grids\n        x_fit = np.linspace(a, b, N_fit)\n        x_eval = np.linspace(a, b, N_eval)\n        \n        # Get function values on the fitting grid\n        y_fit = f_relu(x_fit)\n        \n        g_eval = None\n        g_prime_eval = None\n        \n        if case['type'] == 'polynomial':\n            n = case['degree']\n            \n            # Perform polynomial least-squares fit\n            poly_fit = P.Polynomial.fit(x_fit, y_fit, n)\n            \n            # Evaluate the polynomial and its derivative on the fine grid\n            g_eval = poly_fit(x_eval)\n            g_prime_eval = poly_fit.deriv()(x_eval)\n\n        elif case['type'] == 'spline':\n            s_param = case['s']\n            \n            # Construct the cubic smoothing spline\n            spline = UnivariateSpline(x_fit, y_fit, k=3, s=s_param)\n            \n            # Evaluate the spline and its derivative on the fine grid\n            g_eval = spline(x_eval)\n            g_prime_eval = spline.derivative(n=1)(x_eval)\n            \n        # Evaluate the true function and its derivative on the fine grid\n        f_eval = f_relu(x_eval)\n        f_prime_eval = f_relu_prime(x_eval)\n        \n        # Calculate maximum function error\n        E_max = np.max(np.abs(g_eval - f_eval))\n        \n        # Calculate maximum derivative mismatch, excluding the point x=0\n        mask = np.abs(x_eval) > epsilon\n        D_max = np.max(np.abs(g_prime_eval[mask] - f_prime_eval[mask]))\n        \n        all_results.append([round(E_max, 6), round(D_max, 6)])\n\n    # Format the final output string as a list of lists without spaces\n    outer_list_str = []\n    for res in all_results:\n        inner_list_str = f\"[{res[0]:.6f},{res[1]:.6f}]\"\n        outer_list_str.append(inner_list_str)\n    final_output_str = f\"[{','.join(outer_list_str)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "While polynomials are a default choice for approximation, they are often poor at modeling behavior far outside the sampled domain, a task known as extrapolation. This practice  introduces rational functions—the ratio of two polynomials—which are exceptionally good at capturing phenomena like asymptotic decay. You will learn how to set up the fitting procedure as a linear problem and discover how encoding prior knowledge about a function's long-range trend into your model can dramatically improve its extrapolative power.",
            "id": "3133601",
            "problem": "You are to design and implement a program that fits rational functions to sampled data from known functions, where the rational function’s denominator encodes known asymptotic behavior. From first principles, a rational approximant is a function of the form $R(x) = \\frac{P_m(x)}{Q_n(x)},$ where $P_m(x) = \\sum_{k=0}^{m} a_k x^k$ and $Q_n(x) = \\sum_{j=0}^{n} b_j x^j.$ To remove the scaling ambiguity, fix $b_0 = 1$ so that $Q_n(x) = 1 + \\sum_{j=1}^{n} b_j x^j.$ If the target function behaves like $C x^{-k}$ as $x \\to \\infty,$ then encoding the decay rate is achieved by choosing degrees such that $n - m = k,$ so that, generically, $R(x) \\sim \\left(\\frac{a_m}{b_n}\\right) x^{m-n} = \\left(\\frac{a_m}{b_n}\\right) x^{-k}.$\n\nYou must fit the coefficients $\\{a_k\\}_{k=0}^{m}$ and $\\{b_j\\}_{j=1}^{n}$ by solving the linear least squares problem that arises from the relation $y_i Q_n(x_i) \\approx P_m(x_i)$ for sampled data points $(x_i, y_i)$. Specifically, for each sample $i$, enforce $y_i \\left(1 + \\sum_{j=1}^{n} b_j x_i^j\\right) - \\sum_{k=0}^{m} a_k x_i^k \\approx 0,$ and solve for the unknown coefficients in the least squares sense. Angles in trigonometric functions must be in radians.\n\nSampling and evaluation protocol:\n1. For each test case, sample $N$ equispaced points $x_i$ in the closed interval $[x_{\\min}, x_{\\max}]$ and compute $y_i = f(x_i)$ using the specified function $f(x)$.\n2. Fit the rational approximant $R(x) = P_m(x)/Q_n(x)$ with $b_0 = 1$ and degrees $m, n$ chosen according to the specified asymptotic behavior.\n3. Extrapolate beyond the sampled domain by evaluating $R(T)$ at the specified target $T > x_{\\max}$ and compute the absolute error $E = |R(T) - f(T)|.$\n4. Your program must produce a single line of output containing the four absolute errors for the four test cases as a comma-separated list enclosed in square brackets, with each error rounded to exactly six digits after the decimal point, for example, `[0.123456,0.000012,0.923875,0.010000]`.\n\nUse the natural logarithm for any logarithmic expressions. All angles are in radians. No physical units are involved in this problem.\n\nTest suite:\n- Test case 1 (decay like $1/x$): $f(x) = \\frac{\\sin(x)}{x},$ sampled on $[0.2, 6]$ with $N = 60.$\n  - Choose $m = 3, n = 4$ so that $n - m = 1$ encodes the $1/x$ decay.\n  - Extrapolation point: $T = 10.$\n- Test case 2 (decay like $1/x^2$): $f(x) = \\frac{1}{(1 + x)^2},$ sampled on $[0.5, 4]$ with $N = 40.$\n  - Choose $m = 2, n = 4$ so that $n - m = 2$ encodes the $1/x^2$ decay.\n  - Extrapolation point: $T = 8.$\n- Test case 3 (slowly varying factor with $1/x$-like decay): $f(x) = \\frac{\\ln(1 + x)}{x},$ sampled on $[0.5, 5]$ with $N = 50.$\n  - Choose $m = 3, n = 4$ so that $n - m = 1$ encodes the dominant $1/x$ decay.\n  - Extrapolation point: $T = 12.$\n- Test case 4 (constant asymptote): $f(x) = 2 + \\frac{1}{1 + x},$ sampled on $[0, 5]$ with $N = 30.$\n  - Choose $m = 2, n = 2$ so that $n - m = 0$ encodes a constant asymptote.\n  - Extrapolation point: $T = 20.$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases above, with each number rounded to exactly six digits after the decimal point, for example, `[e_1,e_2,e_3,e_4]`.",
            "solution": "The problem is valid. It presents a well-defined task in numerical analysis, specifically function approximation using rational functions, which is a standard topic in computational science. All parameters and functions are clearly specified, and the methodology, based on linearizing the rational function relationship to form a linear least squares problem, is mathematically sound.\n\nThe objective is to find a rational function $R(x) = \\frac{P_m(x)}{Q_n(x)}$ that approximates a given function $f(x)$. The polynomials are defined as $P_m(x) = \\sum_{k=0}^{m} a_k x^k$ and $Q_n(x) = \\sum_{j=0}^{n} b_j x^j$. To ensure uniqueness by removing scaling ambiguity, the coefficient $b_0$ is fixed to $1$, so the denominator becomes $Q_n(x) = 1 + \\sum_{j=1}^{n} b_j x^j$. The coefficients to be determined are $\\{a_k\\}_{k=0}^{m}$ and $\\{b_j\\}_{j=1}^{n}$, for a total of $(m+1) + n$ unknown parameters.\n\nThe core of the method lies in transforming the nonlinear approximation problem $R(x_i) \\approx y_i$ into a linear one. Given a set of $N$ data points $(x_i, y_i)$ sampled from the function $f(x)$, the approximation can be written as $\\frac{P_m(x_i)}{Q_n(x_i)} \\approx y_i$. Multiplying by the denominator $Q_n(x_i)$ linearizes the problem with respect to the unknown coefficients:\n$$P_m(x_i) \\approx y_i Q_n(x_i)$$\nSubstituting the polynomial forms, we obtain an approximate linear equation for each sample point $i$:\n$$\\sum_{k=0}^{m} a_k x_i^k \\approx y_i \\left(1 + \\sum_{j=1}^{n} b_j x_i^j\\right)$$\nTo set this up for a least squares solution, we rearrange the equation to group knowns and unknowns. Let's move all terms involving the unknown coefficients $\\{a_k\\}$ and $\\{b_j\\}$ to one side and the known term $y_i$ to the other:\n$$y_i \\approx \\sum_{k=0}^{m} a_k x_i^k - y_i \\sum_{j=1}^{n} b_j x_i^j$$\nThis can be expanded as:\n$$y_i \\approx (a_0 \\cdot x_i^0 + a_1 \\cdot x_i^1 + \\dots + a_m \\cdot x_i^m) - (b_1 \\cdot y_i x_i^1 + b_2 \\cdot y_i x_i^2 + \\dots + b_n \\cdot y_i x_i^n)$$\nThis structure defines a linear system for all $N$ sample points. We can express this in the matrix form $A\\mathbf{c} \\approx \\mathbf{d}$, where $\\mathbf{c}$ is the vector of unknown coefficients, $A$ is the design matrix, and $\\mathbf{d}$ is the vector of observed values.\n\nThe vector of coefficients $\\mathbf{c}$ is constructed by concatenating the $a$ and $b$ coefficients:\n$$\\mathbf{c} = [a_0, a_1, \\dots, a_m, b_1, b_2, \\dots, b_n]^T$$\nThis vector has a length of $m+1+n$.\n\nThe vector $\\mathbf{d}$ consists of the sampled function values:\n$$\\mathbf{d} = [y_1, y_2, \\dots, y_N]^T$$\n\nThe design matrix $A$ will have $N$ rows and $m+1+n$ columns. Each row $i$ corresponds to the sample point $(x_i, y_i)$. The columns of $A$ are the basis functions evaluated at $x_i$. For the $a_k$ coefficients, the basis functions are $x_i^k$. For the $b_j$ coefficients, the basis functions are $-y_i x_i^j$. The $i^{th}$ row of $A$ is therefore:\n$$A_{i, \\cdot} = [1, x_i, x_i^2, \\dots, x_i^m, -y_i x_i, -y_i x_i^2, \\dots, -y_i x_i^n]$$\n\nSince the number of samples $N$ is greater than the number of coefficients $m+1+n$, the system is overdetermined. We seek the coefficient vector $\\mathbf{c}$ that minimizes the sum of the squares of the residuals, i.e., minimizes the Euclidean norm $\\|A\\mathbf{c} - \\mathbf{d}\\|_2$. This is a standard linear least squares problem, which is reliably solved using numerical methods such as QR decomposition.\n\nOnce the optimal coefficient vector $\\mathbf{c}$ is found, the coefficients $\\{a_k\\}$ and $\\{b_j\\}$ are extracted. The rational approximant $R(x)$ is then fully specified. We can evaluate it at the target extrapolation point $T > x_{\\max}$:\n$$R(T) = \\frac{P_m(T)}{Q_n(T)} = \\frac{\\sum_{k=0}^{m} a_k T^k}{1 + \\sum_{j=1}^{n} b_j T^j}$$\nThe final step is to calculate the absolute error $E = |R(T) - f(T)|$, where $f(T)$ is the exact value of the original function at the extrapolation point. This entire procedure is applied to each of the four test cases specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_rational_fit(f, x_min, x_max, N, m, n, T):\n    \"\"\"\n    Fits a rational function R(x) = P_m(x) / Q_n(x) to a function f(x) and\n    evaluates the extrapolation error at a point T.\n\n    The fit is performed by solving a linear least squares problem derived from\n    y_i * Q_n(x_i) approx P_m(x_i).\n    \"\"\"\n\n    # 1. Generate N equispaced sample points in [x_min, x_max]\n    x_i = np.linspace(x_min, x_max, N)\n    \n    # Using np.vectorize to handle potential non-vectorized lambda functions,\n    # though the ones in this problem are fine.\n    f_vec = np.vectorize(f)\n    y_i = f_vec(x_i)\n\n    # 2. Construct the design matrix A and vector d for the linear least squares problem Ac ~ d\n    num_coeffs = m + 1 + n\n    A = np.zeros((N, num_coeffs))\n    d = y_i\n\n    # Populate columns for a_k coefficients (terms are x_i^k)\n    # Using broadcasting for an efficient construction of the Vandermonde-like matrix part\n    powers_a = np.arange(m + 1)\n    A[:, :m + 1] = x_i[:, np.newaxis] ** powers_a\n\n    # Populate columns for b_j coefficients (terms are -y_i * x_i^j)\n    if n > 0:\n        powers_b = np.arange(1, n + 1)\n        A[:, m + 1:] = -y_i[:, np.newaxis] * (x_i[:, np.newaxis] ** powers_b)\n\n    # 3. Solve the linear least squares problem for the coefficient vector c\n    # c = [a_0, ..., a_m, b_1, ..., b_n]\n    coeffs, _, _, _ = np.linalg.lstsq(A, d, rcond=None)\n\n    # 4. Extract the polynomial coefficients a_k and b_j from the solution vector\n    a_coeffs = coeffs[:m + 1]\n    b_coeffs = coeffs[m + 1:]\n\n    # 5. Evaluate the numerator polynomial P_m(T) at the extrapolation point T\n    p_T = np.sum(a_coeffs * (T ** np.arange(m + 1)))\n\n    # 6. Evaluate the denominator polynomial Q_n(T) = 1 + sum(b_j * T^j) at T\n    q_T = 1.0\n    if n > 0:\n        q_T += np.sum(b_coeffs * (T ** np.arange(1, n + 1)))\n    \n    # The rational approximant's value at T\n    R_T = p_T / q_T\n\n    # 7. Evaluate the true function f(T)\n    f_T = f(T)\n\n    # 8. Compute the absolute extrapolation error\n    error = np.abs(R_T - f_T)\n    \n    return error\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            'f': lambda x: np.sin(x) / x,\n            'x_min': 0.2, 'x_max': 6.0, 'N': 60,\n            'm': 3, 'n': 4, 'T': 10.0\n        },\n        # Test Case 2\n        {\n            'f': lambda x: 1.0 / (1.0 + x)**2,\n            'x_min': 0.5, 'x_max': 4.0, 'N': 40,\n            'm': 2, 'n': 4, 'T': 8.0\n        },\n        # Test Case 3\n        {\n            'f': lambda x: np.log(1.0 + x) / x,\n            'x_min': 0.5, 'x_max': 5.0, 'N': 50,\n            'm': 3, 'n': 4, 'T': 12.0\n        },\n        # Test Case 4\n        {\n            'f': lambda x: 2.0 + 1.0 / (1.0 + x),\n            'x_min': 0.0, 'x_max': 5.0, 'N': 30,\n            'm': 2, 'n': 2, 'T': 20.0\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # Calculate error for each case\n        error = solve_rational_fit(\n            f=case['f'],\n            x_min=case['x_min'],\n            x_max=case['x_max'],\n            N=case['N'],\n            m=case['m'],\n            n=case['n'],\n            T=case['T']\n        )\n        # Format the result to 6 decimal places\n        results.append(f\"{error:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Modern data fitting often involves flexible, non-parametric models that can capture complex patterns without assuming a simple underlying form. This exercise  delves into the powerful framework of kernel methods by implementing Kernel Ridge Regression, and then explores its scalable approximation, the Random Fourier Features method. By completing this practice, you will understand how a complex kernel-based model can be approximated by a simple linear regression in a randomized feature space, revealing the crucial trade-offs between computational efficiency and approximation accuracy.",
            "id": "3133557",
            "problem": "You are asked to implement and compare two approaches to approximate a univariate target function from noisy samples: a linear model with Random Fourier Features (Rahimi–Recht random features) and Kernel Ridge Regression with a Gaussian kernel. Your task is to construct both models from first principles, relying only on foundational facts about positive definite kernels and linear regression. You must design a program that trains on a fixed dataset, evaluates on a fixed test grid, and reports scalar errors for specific choices of the number of random features. Angles must be in radians.\n\nBackground: Any continuous, shift-invariant, positive definite kernel on $\\mathbb{R}^{d}$ can be expressed as the Fourier transform of a nonnegative finite measure. This implies that such a kernel can be approximated by a Monte Carlo average of cosine features sampled from a corresponding spectral distribution. In particular, for the Gaussian radial basis function kernel, this spectral distribution is Gaussian. You must utilize this principle to construct a random feature map and perform ridge regression in the feature space. For the kernel method, use the standard kernel ridge regression formulation.\n\nDataset specification and test suite:\n1. Target function: For $x \\in [0,1]$, define $f(x) = \\sin(6\\pi x) + 0.5 \\cos(12\\pi x)$, where the trigonometric arguments are in radians.\n2. Training inputs: $n = 80$ points, equally spaced on $[0,1)$.\n3. Training outputs: $y_{i} = f(x_{i}) + \\varepsilon_{i}$, where $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma_{\\text{noise}}^{2})$ with $\\sigma_{\\text{noise}} = 0.05$. Use a fixed random number generator seed $s_{\\text{data}} = 12345$ to generate the noise.\n4. Test inputs: $N_{\\text{test}} = 1000$ equally spaced points on $[0,1)$, with noiseless targets $f(x)$.\n5. Kernel: Gaussian radial basis function kernel $k(x,x') = \\exp\\!\\left(-\\frac{(x-x')^{2}}{2\\sigma^{2}}\\right)$ with bandwidth $\\sigma = 0.15$.\n6. Regularization: Ridge parameter $\\lambda = 10^{-4}$ for both models.\n7. Random Fourier Features:\n   - Use a single maximum feature budget $M_{\\max} = 500$ and a fixed random number generator seed $s_{\\text{feat}} = 54321$ to generate all random frequencies and phases once. Then obtain smaller feature sets by truncation to ensure a deterministic and nested sequence of features.\n   - Evaluate the model for the feature counts $M \\in \\{5, 50, 500\\}$.\n8. Angle unit: All trigonometric functions and phases must be in radians.\n\nModels to implement:\n1. Random Fourier Feature ridge regression: Construct a linear model in a random feature space corresponding to the Gaussian radial basis function kernel. Use the Monte Carlo approximation derived from the Fourier representation of the kernel to build the feature map. Fit the model by minimizing a ridge-regularized squared error on the training data.\n2. Kernel ridge regression: Use the Gaussian radial basis function kernel with the specified bandwidth. Fit the coefficients in the kernel-induced space using ridge regularization.\n\nEvaluation:\n- Use the mean squared error on the test set as the performance metric: $\\mathrm{MSE} = \\frac{1}{N_{\\text{test}}}\\sum_{j=1}^{N_{\\text{test}}}(\\hat{f}(x_{j}) - f(x_{j}))^{2}$.\n- Compute one $\\mathrm{MSE}$ for each $M \\in \\{5, 50, 500\\}$ using the random feature model, and one $\\mathrm{MSE}$ using kernel ridge regression.\n- Additionally, compute a boolean flag indicating whether the sequence of random feature errors is nonincreasing as $M$ grows, that is, whether $\\mathrm{MSE}_{M=5} \\ge \\mathrm{MSE}_{M=50} \\ge \\mathrm{MSE}_{M=500}$ holds.\n\nFinal output format:\n- Your program must produce a single line of output containing a comma-separated list enclosed in square brackets in the following order:\n  1. $\\mathrm{MSE}$ for $M=5$,\n  2. $\\mathrm{MSE}$ for $M=50$,\n  3. $\\mathrm{MSE}$ for $M=500$,\n  4. $\\mathrm{MSE}$ for kernel ridge regression,\n  5. the boolean flag for nonincreasing error with respect to $M$.\n- Each floating-point number must be rounded to $6$ decimal places. For example: $[\\text{err5},\\text{err50},\\text{err500},\\text{errKRR},\\text{flag}]$.\n\nYour program must be fully self-contained, require no user input, and strictly adhere to the above dataset and parameter specifications.",
            "solution": "We begin from well-tested facts about positive definite kernels and linear models. For a continuous, shift-invariant, positive definite kernel $k(x-x')$ on $\\mathbb{R}^{d}$, Bochner’s theorem states that $k(\\delta)$ is the Fourier transform of a nonnegative finite measure. When the measure has a density $p(\\omega)$ with respect to Lebesgue measure, we can write\n$$\nk(x-x') = \\int_{\\mathbb{R}^{d}} e^{i \\omega^{\\top}(x-x')} p(\\omega)\\, d\\omega = \\mathbb{E}_{\\omega \\sim p}\\big[\\cos(\\omega^{\\top}x - \\omega^{\\top}x')\\big],\n$$\nwhere we used that the kernel is real and even, so the imaginary part vanishes and the real part is a cosine. Introducing a random phase $b \\sim \\mathrm{Uniform}([0,2\\pi])$ independent of $\\omega$, we use the trigonometric identity $\\cos(a-b) = \\cos(a)\\cos(b) + \\sin(a)\\sin(b)$ together with the expectation over $b$ to obtain a representation in terms of an inner product of random features. A Monte Carlo approximation with $M$ samples $\\{\\omega_{j}, b_{j}\\}_{j=1}^{M}$ yields the random feature map\n$z(x) = \\sqrt{\\frac{2}{M}}\\left[\\cos(\\omega_{1}^{\\top}x + b_{1}),\\ldots,\\cos(\\omega_{M}^{\\top}x + b_{M})\\right]^{\\top}$,\nfor which $\\mathbb{E}\\big[z(x)^{\\top}z(x')\\big] = k(x,x')$. For the Gaussian radial basis function kernel\n$$\nk(x,x') = \\exp\\!\\left(-\\frac{\\lVert x-x'\\rVert^{2}}{2\\sigma^{2}}\\right),\n$$\nthe corresponding spectral density is Gaussian $p(\\omega) = \\mathcal{N}(0,\\sigma^{-2} I_{d})$. In our univariate case $d=1$, so $\\omega \\sim \\mathcal{N}(0,\\sigma^{-2})$ and $b \\sim \\mathrm{Uniform}([0,2\\pi])$. All trigonometric arguments are in radians, as required.\n\nRandom Fourier Feature ridge regression: Given training data $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$ and feature map $z(\\cdot)$, define the design matrix $Z \\in \\mathbb{R}^{n \\times M}$ with rows $z(x_{i})^{\\top}$. The ridge-regularized least squares problem minimizes\n$$\n\\min_{\\theta \\in \\mathbb{R}^{M}} \\sum_{i=1}^{n} \\big(z(x_{i})^{\\top}\\theta - y_{i}\\big)^{2} + \\lambda \\lVert \\theta \\rVert^{2}.\n$$\nThe first-order optimality condition leads to the normal equations\n$$\n\\big(Z^{\\top}Z + \\lambda I_{M}\\big)\\theta = Z^{\\top} y,\n$$\nwith solution $\\theta = \\big(Z^{\\top}Z + \\lambda I_{M}\\big)^{-1} Z^{\\top} y$. Predictions at any $x$ are $\\hat{f}(x) = z(x)^{\\top}\\theta$.\n\nKernel ridge regression: By the representer theorem, the minimizer in the reproducing kernel Hilbert space has the form $\\hat{f}(x) = \\sum_{i=1}^{n} \\alpha_{i} k(x, x_{i})$. The coefficients satisfy the linear system\n$$\n\\big(K + \\lambda I_{n}\\big)\\alpha = y,\n$$\nwhere $K \\in \\mathbb{R}^{n \\times n}$ with $K_{ij} = k(x_{i}, x_{j})$. Predictions on a test set $\\{x_{j}^{\\star}\\}_{j=1}^{N_{\\text{test}}}$ are collected as $\\hat{f}_{\\star} = K_{\\star} \\alpha$, where $K_{\\star} \\in \\mathbb{R}^{N_{\\text{test}} \\times n}$ has entries $k(x_{j}^{\\star}, x_{i})$.\n\nEvaluation: For a test grid $\\{x_{j}^{\\star}\\}_{j=1}^{N_{\\text{test}}}$ and true targets $f(x_{j}^{\\star})$, the mean squared error is\n$$\n\\mathrm{MSE} = \\frac{1}{N_{\\text{test}}}\\sum_{j=1}^{N_{\\text{test}}} \\big(\\hat{f}(x_{j}^{\\star}) - f(x_{j}^{\\star})\\big)^{2}.\n$$\n\nAlgorithmic design under the specified test suite:\n1. Construct training inputs: $n = 80$ equally spaced points $x_{i} \\in [0,1)$, and test inputs: $N_{\\text{test}} = 1000$ equally spaced points in $[0,1)$.\n2. Define $f(x) = \\sin(6\\pi x) + 0.5 \\cos(12\\pi x)$, and generate noisy training outputs with Gaussian noise of standard deviation $\\sigma_{\\text{noise}} = 0.05$ using seed $s_{\\text{data}} = 12345$. The test targets use noiseless $f(x)$.\n3. Fix Gaussian kernel bandwidth $\\sigma = 0.15$ and ridge $\\lambda = 10^{-4}$.\n4. Random Fourier Features: With seed $s_{\\text{feat}} = 54321$, sample a pool of $M_{\\max} = 500$ frequencies $\\omega_{j} \\sim \\mathcal{N}(0,\\sigma^{-2})$ and phases $b_{j} \\sim \\mathrm{Uniform}([0,2\\pi])$, form features $z(x)$, and train ridge regression via the normal equations. For $M \\in \\{5, 50, 500\\}$, use the first $M$ features to form nested models and compute their test $\\mathrm{MSE}$ values.\n5. Kernel ridge regression: Build the kernel matrix $K$ with entries $K_{ij} = \\exp\\!\\left(-\\frac{(x_{i}-x_{j})^{2}}{2\\sigma^{2}}\\right)$, solve for $\\alpha$ from $\\big(K + \\lambda I_{n}\\big)\\alpha = y$, and compute test predictions using $K_{\\star}$ with the same kernel.\n6. Report the four $\\mathrm{MSE}$ values and a boolean indicating whether the random feature errors are nonincreasing as $M$ increases.\n\nWhy this works and expected scaling: The random feature model approximates the kernel inner product by a Monte Carlo average. The deviation of $z(x)^{\\top}z(x')$ from $k(x,x')$ has variance that scales as $O(1/M)$, implying an $O(M^{-1/2})$ effect on the kernel approximation. When used in ridge regression, the predictor’s excess error due to feature randomness tends to decrease as $M$ grows, approaching the kernel ridge regression solution for large $M$. Hence, with $M \\in \\{5, 50, 500\\}$, one expects the $\\mathrm{MSE}$ to generally decrease with $M$, though finite-sample and noise effects can cause small deviations. The boolean flag explicitly checks for the nonincreasing trend in this specific deterministic setup. All angles and phases are handled in radians, per the problem requirement.\n\nThe program implements the above steps exactly and prints a single line containing the list: $[\\mathrm{MSE}_{5}, \\mathrm{MSE}_{50}, \\mathrm{MSE}_{500}, \\mathrm{MSE}_{\\text{KRR}}, \\text{is\\_nonincreasing}]$, where each $\\mathrm{MSE}$ is rounded to $6$ decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef f_target(x):\n    # f(x) = sin(6*pi*x) + 0.5*cos(12*pi*x), angles in radians\n    return np.sin(6.0 * np.pi * x) + 0.5 * np.cos(12.0 * np.pi * x)\n\ndef rbf_kernel(X1, X2, sigma):\n    # Gaussian RBF kernel: exp(-||x-y||^2 / (2*sigma^2))\n    X1 = X1.reshape(-1, 1)\n    X2 = X2.reshape( -1, 1)\n    d2 = (X1 - X2.T) ** 2\n    return np.exp(-0.5 * d2 / (sigma ** 2))\n\ndef rff_features_1d(x, w, b):\n    # x: shape (n,), w: shape (M,), b: shape (M,), radians\n    # z(x) = sqrt(2/M) * cos(x*w + b)\n    M = w.shape[0]\n    return np.sqrt(2.0 / M) * np.cos(np.outer(x, w) + b)\n\ndef ridge_fit_predict(Z_train, y_train, Z_test, lam):\n    # Solve (Z^T Z + lam I) theta = Z^T y, then predict on Z_test\n    M = Z_train.shape[1]\n    A = Z_train.T @ Z_train + lam * np.eye(M)\n    b = Z_train.T @ y_train\n    theta = np.linalg.solve(A, b)\n    return Z_test @ theta\n\ndef kernel_ridge_predict(X_train, y_train, X_test, sigma, lam):\n    K = rbf_kernel(X_train, X_train, sigma)\n    n = K.shape[0]\n    alpha = np.linalg.solve(K + lam * np.eye(n), y_train)\n    K_star = rbf_kernel(X_test, X_train, sigma)\n    return K_star @ alpha\n\ndef mean_squared_error(y_pred, y_true):\n    diff = y_pred - y_true\n    return float(np.mean(diff * diff))\n\ndef solve():\n    # Parameters as per problem statement\n    n = 80\n    N_test = 1000\n    sigma = 0.15\n    lam = 1e-4\n    sigma_noise = 0.05\n    Ms = [5, 50, 500]\n    M_max = 500\n    seed_data = 12345\n    seed_feat = 54321\n\n    # Data generation\n    X_train = np.linspace(0.0, 1.0, n, endpoint=False)\n    X_test = np.linspace(0.0, 1.0, N_test, endpoint=False)\n    y_true_test = f_target(X_test)\n\n    rng_data = np.random.default_rng(seed_data)\n    noise = rng_data.normal(loc=0.0, scale=sigma_noise, size=n)\n    y_train = f_target(X_train) + noise\n\n    # Random Fourier Features setup (nested features)\n    rng_feat = np.random.default_rng(seed_feat)\n    # For Gaussian kernel with bandwidth sigma, w ~ N(0, 1/sigma^2)\n    w_pool = rng_feat.normal(loc=0.0, scale=1.0 / sigma, size=M_max)\n    b_pool = rng_feat.uniform(low=0.0, high=2.0 * np.pi, size=M_max)\n\n    # Precompute kernel ridge baseline\n    y_pred_krr = kernel_ridge_predict(X_train, y_train, X_test, sigma, lam)\n    mse_krr = mean_squared_error(y_pred_krr, y_true_test)\n\n    # Compute RFF models for each M using truncation of the common pool\n    rff_mses = []\n    for M in Ms:\n        w = w_pool[:M].copy()\n        b = b_pool[:M].copy()\n        Z_train = rff_features_1d(X_train, w, b)\n        Z_test = rff_features_1d(X_test, w, b)\n        y_pred_rff = ridge_fit_predict(Z_train, y_train, Z_test, lam)\n        mse_rff = mean_squared_error(y_pred_rff, y_true_test)\n        rff_mses.append(mse_rff)\n\n    # Check nonincreasing trend\n    is_nonincreasing = (rff_mses[0] >= rff_mses[1]) and (rff_mses[1] >= rff_mses[2])\n\n    # Round results to 6 decimals as specified\n    results = [round(rff_mses[0], 6), round(rff_mses[1], 6), round(rff_mses[2], 6), round(mse_krr, 6), is_nonincreasing]\n\n    # Final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}