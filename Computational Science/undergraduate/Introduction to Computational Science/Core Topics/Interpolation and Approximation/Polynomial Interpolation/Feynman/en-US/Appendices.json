{
    "hands_on_practices": [
        {
            "introduction": "The journey into polynomial interpolation begins with its simplest form: linear interpolation. This fundamental technique allows us to estimate unknown values that lie between two known data points by assuming a straight-line relationship. This practice exercise  provides a tangible scenario of estimating missing environmental data, allowing you to apply the core formula and build an intuitive understanding of how interpolation works.",
            "id": "2218418",
            "problem": "An environmental monitoring station is tracking the concentration of a specific atmospheric pollutant over an afternoon. The station records data at regular intervals, but a sensor malfunction caused a data loss for a brief period. The last reliable measurement before the malfunction, taken at $t_0 = 2.00$ hours past noon, showed a pollutant concentration of $C_0 = 87.2$ parts per billion (ppb). The first reliable measurement after the sensor was restored, taken at $t_1 = 5.00$ hours past noon, showed a concentration of $C_1 = 61.4$ ppb.\n\nTo approximate the missing data, an analyst decides to model the concentration using linear interpolation between the two known data points. Using this method, estimate the pollutant concentration at $t = 3.50$ hours past noon.\n\nProvide your answer in parts per billion (ppb), rounded to three significant figures.",
            "solution": "We model the concentration between the two known data points with linear interpolation. For a quantity $C$ measured at times $t$, the linear interpolation formula between $(t_{0}, C_{0})$ and $(t_{1}, C_{1})$ is\n$$\nC(t)=C_{0}+\\frac{C_{1}-C_{0}}{t_{1}-t_{0}}\\left(t-t_{0}\\right).\n$$\nSubstituting the given values $t_{0}=2.00$, $C_{0}=87.2$, $t_{1}=5.00$, $C_{1}=61.4$, and $t=3.50$, we compute the slope\n$$\n\\frac{C_{1}-C_{0}}{t_{1}-t_{0}}=\\frac{61.4-87.2}{5.00-2.00}=\\frac{-25.8}{3.00}=-8.6,\n$$\nand the time offset\n$$\nt-t_{0}=3.50-2.00=1.50.\n$$\nTherefore,\n$$\nC(3.50)=87.2+(-8.6)\\times 1.50=87.2-12.9=74.3.\n$$\nRounded to three significant figures, the estimate is $74.3$.",
            "answer": "$$\\boxed{74.3}$$"
        },
        {
            "introduction": "While linear interpolation is useful, many real-world phenomena follow curved paths that are better described by higher-degree polynomials. This exercise  challenges you to construct a quadratic polynomial to model the deflection of a beam. It demonstrates not only how to fit a parabola through three points but also how leveraging physical insights, such as the location of a maximum or minimum, can lead to a more elegant and efficient solution.",
            "id": "2181788",
            "problem": "A materials scientist is studying the behavior of a newly developed alloy under thermal stress. The deflection, $y(x)$, of a thin beam of this alloy at a position $x$ along its length is measured. Experimental observations over a specific range suggest that the deflection profile can be accurately modeled by a quadratic polynomial of the form $y(x) = ax^2 + bx + c$. Three precise measurements are recorded:\n\n- At position $x_1 = -1$, the deflection is $y_1 = 8$.\n- At position $x_2 = 2$, the deflection is $y_2 = -1$.\n- At position $x_3 = 5$, the deflection is $y_3 = 8$.\n\nIt is also known from the principles of beam mechanics for this particular setup that one of these three measurement points corresponds to the point of maximum deflection (or minimum, depending on the sign of $a$). This point is the vertex of the parabola.\n\nDetermine the quadratic polynomial $y(x)$ that describes the deflection profile. Express your answer in the standard form $y(x) = ax^2 + bx + c$.",
            "solution": "We model the deflection by a quadratic $y(x)=ax^{2}+bx+c$. A parabola can be written in vertex form as $y(x)=a(x-h)^{2}+k$, where $(h,k)$ is the vertex, and the axis of symmetry is $x=h$. Points equidistant from $h$ have equal $y$-values.\n\nFrom the data, $y(-1)=8$ and $y(5)=8$. The midpoint of $-1$ and $5$ is $h=\\frac{-1+5}{2}=2$, so the axis of symmetry is $x=2$. It is given that one of the measured points is the vertex; since $y(2)=-1$ and this is extremal relative to the other two values, the vertex is $(h,k)=(2,-1)$.\n\nThus the quadratic has the form\n$$\ny(x)=a(x-2)^{2}-1.\n$$\nUse the point $(-1,8)$ to find $a$:\n$$\n8=a(-1-2)^{2}-1=a\\cdot 9-1 \\implies 9a=9 \\implies a=1.\n$$\nTherefore,\n$$\ny(x)=(x-2)^{2}-1=x^{2}-4x+4-1=x^{2}-4x+3.\n$$\nA check with $x=5$ gives $25-20+3=8$, consistent with the data. Hence the quadratic is $y(x)=x^{2}-4x+3$.",
            "answer": "$$\\boxed{x^{2}-4x+3}$$"
        },
        {
            "introduction": "A crucial skill in computational science is knowing not just how to use a tool, but when. Exact polynomial interpolation is powerful for smooth, precise data, but it can be misleading when applied to noisy measurements. This problem  explores the critical trade-off between fitting data perfectly and creating a model that generalizes well, introducing the concept of overfitting and showing why regression is often preferred in the presence of noise.",
            "id": "3174879",
            "problem": "An engineering team is building a surrogate model for a smooth but unknown inputâ€“output relationship measured by a sensor. They collect $n = 21$ samples at equally spaced inputs $x_i \\in [-1,1]$ with outputs $y_i$. The sensor is known to have additive zero-mean noise with standard deviation approximately $0.05$. Consider two families of models built on these data:\n- Polynomial interpolation: a single polynomial that passes through all observed pairs $(x_i,y_i)$ exactly.\n- Polynomial regression: a polynomial whose coefficients are chosen to minimize the sum of squared residuals over the data.\n\nUsing the same dataset, the team computes the Root Mean Square Error (RMSE) on the training data and a $10$-fold Cross-Validation (CV) RMSE for three candidates:\n- Exact interpolating polynomial of degree $20$: training RMSE $= 0.00$, CV RMSE $= 0.31$.\n- Least-squares polynomial regression of degree $8$: training RMSE $= 0.02$, CV RMSE $= 0.18$.\n- Least-squares polynomial regression of degree $3$: training RMSE $= 0.07$, CV RMSE $= 0.09$.\n\nThe goal is to predict the output at $x = 0.7$ and to construct a model that generalizes well to unseen inputs from the same process.\n\nWhich of the following is the most defensible conclusion about when to avoid exact interpolation and prefer regression for this dataset?\n\nA. Choose the degree $20$ interpolating polynomial because it achieves zero training error, which guarantees the smallest possible prediction error at $x = 0.7$.\n\nB. Prefer the degree $3$ least-squares regression polynomial because the data are noisy and the lowest CV RMSE indicates better expected predictive performance; exact interpolation is likely to overfit the noise in the $y_i$ values.\n\nC. Prefer the degree $8$ least-squares regression polynomial because its training RMSE is smaller than that of degree $3$, so it must generalize better even if its CV RMSE is larger.\n\nD. Choose the degree $20$ interpolating polynomial because any smooth function on $[-1,1]$ is exactly a polynomial of degree at most $20$, so interpolation cannot overfit.\n\nE. Use piecewise linear interpolation through all points to avoid overfitting; because each segment is linear, exact fitting to all $y_i$ cannot amplify noise.",
            "solution": "The problem statement is internally consistent, scientifically grounded, and provides sufficient information for a rigorous conclusion. The scenario describes a classic model selection problem involving the bias-variance trade-off in the presence of noisy data.\n\nThe goal is to select a model that generalizes well to unseen inputs. The primary metric for evaluating generalization performance from the given data is the Cross-Validation (CV) Root Mean Square Error (RMSE). A lower CV RMSE indicates a better expected performance on new data. The training RMSE, on the other hand, measures how well a model fits the data it was trained on and is not a reliable indicator of generalization, especially when comparing models of different complexity.\n\nLet's analyze the provided data for each candidate model:\nThe dataset consists of $n=21$ samples, and the sensor noise has a standard deviation of approximately $\\sigma \\approx 0.05$. This noise level represents the inherent, irreducible error. A good model should have a prediction error close to this value.\n\n1.  **Degree $20$ Interpolating Polynomial:**\n    -   Training RMSE = $0.00$: This is expected. A polynomial of degree $n-1 = 20$ can pass through $n=21$ points exactly, meaning the error on the training data is zero by definition.\n    -   CV RMSE = $0.31$: This value is extremely high, much larger than the noise level of $\\sigma \\approx 0.05$. This is a definitive sign of severe overfitting. By forcing the polynomial through every noisy data point ($x_i, y_i$), the model is fitting the random fluctuations (noise) in the $y_i$ values rather than the underlying smooth function. This leads to a model that oscillates wildly between data points and, consequently, has very poor predictive power for unseen data.\n\n2.  **Degree $8$ Least-Squares Regression Polynomial:**\n    -   Training RMSE = $0.02$: This error is very low, even lower than the known noise standard deviation ($\\sigma \\approx 0.05$). This suggests that the model is flexible enough to fit some of the noise in the training data, which is an indication of potential overfitting.\n    -   CV RMSE = $0.18$: While better than the interpolating polynomial, this error is still substantially higher than the noise level and the CV RMSE of the degree $3$ polynomial. This confirms that a degree $8$ polynomial is likely too complex for this dataset, leading to poor generalization.\n\n3.  **Degree $3$ Least-Squares Regression Polynomial:**\n    -   Training RMSE = $0.07$: This error is reasonably close to the noise standard deviation ($\\sigma \\approx 0.05$). This is a healthy sign, suggesting the model is not complex enough to perfectly fit the random noise, and its residual error is consistent with the inherent uncertainty in the data.\n    -   CV RMSE = $0.09$: This is the lowest CV RMSE among all three candidates. A model with the minimum cross-validation error is expected to have the best generalization performance. This value is also the closest to the noise standard deviation, reinforcing the conclusion that this model strikes the best balance between capturing the underlying trend (bias) and not fitting the noise (variance).\n\nBased on this analysis, the degree $3$ regression polynomial is the most suitable model because it has the lowest estimated generalization error (CV RMSE). The primary reason to avoid exact interpolation is its tendency to overfit noisy data, a conclusion strongly supported by its high CV RMSE.\n\nNow, we evaluate each option:\n\n**A. Choose the degree $20$ interpolating polynomial because it achieves zero training error, which guarantees the smallest possible prediction error at $x = 0.7$.**\nThis statement is incorrect. A training error of zero does not guarantee low prediction error on new data points. For noisy data, forcing zero training error, as interpolation does, is a hallmark of overfitting. The high CV RMSE ($0.31$) empirically demonstrates that this model has poor predictive performance.\n\n**B. Prefer the degree $3$ least-squares regression polynomial because the data are noisy and the lowest CV RMSE indicates better expected predictive performance; exact interpolation is likely to overfit the noise in the $y_i$ values.**\nThis statement is correct. It properly identifies that the presence of noise makes exact interpolation problematic due to overfitting. It correctly uses the lowest CV RMSE as the criterion for selecting the model with the best expected predictive (generalization) performance. This aligns perfectly with the principles of model selection.\n\n**C. Prefer the degree $8$ least-squares regression polynomial because its training RMSE is smaller than that of degree $3$, so it must generalize better even if its CV RMSE is larger.**\nThis statement is incorrect. It mistakenly uses lower training error as the criterion for better generalization. The CV RMSE, not the training RMSE, is the appropriate estimator for generalization error. Since the CV RMSE for the degree $8$ polynomial ($0.18$) is larger than that for the degree $3$ polynomial ($0.09$), the degree $3$ model is expected to generalize better.\n\n**D. Choose the degree $20$ interpolating polynomial because any smooth function on $[-1,1]$ is exactly a polynomial of degree at most $20$, so interpolation cannot overfit.**\nThis statement is incorrect on two fundamental points. First, the premise \"any smooth function on $[-1,1]$ is exactly a polynomial of degree at most $20$\" is false. Most smooth functions (e.g., trigonometric, exponential) are not polynomials. The Weierstrass Approximation Theorem only states they can be approximated arbitrarily well, not that they are identical. Second, interpolation most certainly can and does overfit, particularly in the presence of noise, as evidenced by the high CV RMSE.\n\n**E. Use piecewise linear interpolation through all points to avoid overfitting; because each segment is linear, exact fitting to all $y_i$ cannot amplify noise.**\nThis statement is incorrect. Piecewise linear interpolation is still a form of exact interpolation; it fits every data point $(x_i, y_i)$ perfectly and therefore fits all the noise. The claim that it \"cannot amplify noise\" is false. While it may not suffer from the same type of global oscillations as a high-degree polynomial (Runge's phenomenon), it creates a non-smooth model whose derivative is discontinuous, which is another manifestation of fitting noise. Regression is the correct approach to handle noise, not a different type of exact interpolation.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}