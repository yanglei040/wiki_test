{
    "hands_on_practices": [
        {
            "introduction": "In numerical differentiation, the choice of the step size $h$ presents a fundamental dilemma. A large $h$ leads to significant truncation error, as our approximation deviates from the true derivative. Conversely, an infinitesimally small $h$ can lead to catastrophic cancellation and round-off error due to the finite precision of computer arithmetic. This exercise guides you through finding the theoretical \"sweet spot,\" the optimal step size $h_{\\mathrm{opt}}$ that balances these two competing error sources, a crucial skill for obtaining reliable results in scientific computing. ",
            "id": "2389525",
            "problem": "Consider the approximation of the first derivative of a sufficiently smooth scalar function $f(x)$ at a point $x_0$ using the central difference formula\n$$\nD_h f(x_0) \\equiv \\frac{f(x_0+h)-f(x_0-h)}{2h}.\n$$\nAssume the following leading-order error model for the absolute error of $D_h f(x_0)$:\n$$\nE(h) = K_t h^2 + \\frac{K_r u}{h},\n$$\nwhere $K_t = \\frac{\\lvert f^{(3)}(x_0)\\rvert}{6}$, $K_r = \\lvert f(x_0)\\rvert$, and $u$ is the unit roundoff of the floating-point arithmetic used for evaluating $f$. Use the following unit roundoff values: for double precision (IEEE $754$ binary$64$), $u_{\\mathrm{double}} = 2^{-53}$; for quadruple precision (IEEE $754$ binary$128$), $u_{\\mathrm{quad}} = 2^{-113}$. Angles must be interpreted in radians.\n\nFor each test case listed below, compute the step size $h_{\\mathrm{opt}}$ that minimizes $E(h)$ under the stated model, separately for double precision and quadruple precision. Your program must output these values in the specified format.\n\nTest suite (each case specifies $f$, $x_0$, and the angle unit where applicable):\n- Case $1$: $f(x) = \\sin(x)$, $x_0 = 1$ (radians).\n- Case $2$: $f(x) = \\sin(x)$, $x_0 = 10^{-12}$ (radians).\n- Case $3$: $f(x) = \\cosh(x)$, $x_0 = 3$.\n\nRequired final output format:\n- Produce a single line containing a comma-separated list enclosed in square brackets, in the order\n$$\n[h_{\\mathrm{double},1},h_{\\mathrm{quad},1},h_{\\mathrm{double},2},h_{\\mathrm{quad},2},h_{\\mathrm{double},3},h_{\\mathrm{quad},3}],\n$$\nwhere $h_{\\mathrm{double},k}$ and $h_{\\mathrm{quad},k}$ denote, respectively, the minimizing step sizes for case $k$ in double and quadruple precision.\n- Express each number in scientific notation with exactly $10$ significant digits (for example, $1.234567890\\times 10^{-5}$ must be printed as something like $1.2345678900e-05$).",
            "solution": "The problem statement submitted for analysis is subjected to rigorous validation.\n\n**Step 1: Extract Givens**\n- **Approximation Formula**: Central difference for the first derivative, $D_h f(x_0) \\equiv \\frac{f(x_0+h)-f(x_0-h)}{2h}$.\n- **Error Model**: Absolute error $E(h) = K_t h^2 + \\frac{K_r u}{h}$.\n- **Error Coefficients**: $K_t = \\frac{\\lvert f^{(3)}(x_0)\\rvert}{6}$ and $K_r = \\lvert f(x_0)\\rvert$.\n- **Unit Roundoff Values**:\n    - Double precision: $u_{\\mathrm{double}} = 2^{-53}$.\n    - Quadruple precision: $u_{\\mathrm{quad}} = 2^{-113}$.\n- **Angular Unit**: Angles are specified in radians.\n- **Task**: For each test case, compute the step size $h_{\\mathrm{opt}}$ that minimizes $E(h)$ for both double and quadruple precision.\n- **Test Cases**:\n    1. $f(x) = \\sin(x)$ at $x_0 = 1$.\n    2. $f(x) = \\sin(x)$ at $x_0 = 10^{-12}$.\n    3. $f(x) = \\cosh(x)$ at $x_0 = 3$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is fundamentally sound. It addresses a classic topic in numerical analysis: the trade-off between truncation error and round-off error in numerical differentiation. The provided error model, $E(h)$, is a standard first-order approximation where the $h^2$ term represents the truncation error of the central difference scheme and the $u/h$ term represents the round-off error. The Taylor series expansion of the central difference formula confirms the truncation error is of order $O(h^2)$, and the round-off error model is a widely accepted simplification.\n- **Well-Posedness**: A unique, meaningful solution exists. The function $E(h)$ for $h>0$ is a sum of two positive terms, one increasing with $h$ and one decreasing. This structure guarantees a unique minimum for $h \\in (0, \\infty)$, which can be found using calculus.\n- **Objectivity**: The problem is stated using precise mathematical language, free from any subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem is scientifically grounded, well-posed, objective, and self-contained. It is a valid problem of computational physics and numerical analysis. I will thus proceed with its solution.\n\nThe objective is to find the step size, which we shall denote $h_{\\mathrm{opt}}$, that minimizes the total absolute error function $E(h)$. The error function is given as:\n$$\nE(h) = K_t h^2 + \\frac{K_r u}{h}\n$$\nTo find the minimum, we must compute the derivative of $E(h)$ with respect to $h$ and set it to zero. This gives the critical points of the function.\n$$\n\\frac{dE}{dh} = \\frac{d}{dh} \\left( K_t h^2 + K_r u h^{-1} \\right) = 2 K_t h - K_r u h^{-2}\n$$\nSetting the derivative to zero yields the optimal step size $h_{\\mathrm{opt}}$:\n$$\n2 K_t h_{\\mathrm{opt}} - \\frac{K_r u}{h_{\\mathrm{opt}}^2} = 0\n$$\nProvided $h_{\\mathrm{opt}} \\neq 0$, we can rearrange the equation:\n$$\n2 K_t h_{\\mathrm{opt}}^3 = K_r u\n$$\n$$\nh_{\\mathrm{opt}}^3 = \\frac{K_r u}{2 K_t}\n$$\nSolving for $h_{\\mathrm{opt}}$ gives:\n$$\nh_{\\mathrm{opt}} = \\left( \\frac{K_r u}{2 K_t} \\right)^{1/3}\n$$\nTo confirm this is a minimum, we examine the second derivative:\n$$\n\\frac{d^2E}{dh^2} = 2 K_t + 2 K_r u h^{-3}\n$$\nSince $K_t = \\frac{\\lvert f^{(3)}(x_0)\\rvert}{6} \\ge 0$, $K_r = \\lvert f(x_0)\\rvert \\ge 0$, $u > 0$, and $h > 0$, it follows that $\\frac{d^2E}{dh^2} > 0$ for all valid test cases (where $K_t$ and $K_r$ are not simultaneously zero). Thus, the critical point corresponds to a local minimum.\n\nNow, we substitute the expressions for $K_t$ and $K_r$:\n$$\nh_{\\mathrm{opt}} = \\left( \\frac{\\lvert f(x_0)\\rvert u}{2 \\left( \\frac{\\lvert f^{(3)}(x_0)\\rvert}{6} \\right)} \\right)^{1/3} = \\left( \\frac{3 \\lvert f(x_0)\\rvert u}{\\lvert f^{(3)}(x_0)\\rvert} \\right)^{1/3}\n$$\nThis formula is valid as long as $f^{(3)}(x_0) \\neq 0$. We will apply this formula to each test case.\n\n**Case 1: $f(x) = \\sin(x)$ at $x_0 = 1$**\nThe function and its third derivative are $f(x) = \\sin(x)$ and $f^{(3)}(x) = -\\cos(x)$. At $x_0=1$:\n- $\\lvert f(x_0) \\rvert = \\lvert\\sin(1)\\rvert$\n- $\\lvert f^{(3)}(x_0) \\rvert = \\lvert-\\cos(1)\\rvert = \\lvert\\cos(1)\\rvert$\nSince $1$ radian is in the first quadrant, both $\\sin(1)$ and $\\cos(1)$ are positive.\n$$\nh_{\\mathrm{opt}} = \\left( \\frac{3 \\sin(1) u}{\\cos(1)} \\right)^{1/3} = \\left( 3 u \\tan(1) \\right)^{1/3}\n$$\nThe values are computed for $u = u_{\\mathrm{double}} = 2^{-53}$ and $u = u_{\\mathrm{quad}} = 2^{-113}$.\n\n**Case 2: $f(x) = \\sin(x)$ at $x_0 = 10^{-12}$**\nThe derivatives are the same as in Case $1$. At $x_0=10^{-12}$:\n- $\\lvert f(x_0) \\rvert = \\lvert\\sin(10^{-12})\\rvert \\approx 10^{-12} > 0$\n- $\\lvert f^{(3)}(x_0) \\rvert = \\lvert-\\cos(10^{-12})\\rvert = \\cos(10^{-12}) \\approx 1$\nThe condition $f^{(3)}(x_0) \\neq 0$ is met.\n$$\nh_{\\mathrm{opt}} = \\left( \\frac{3 \\sin(10^{-12}) u}{\\cos(10^{-12})} \\right)^{1/3} = \\left( 3 u \\tan(10^{-12}) \\right)^{1/3}\n$$\nThe values are computed for $u = u_{\\mathrm{double}}$ and $u = u_{\\mathrm{quad}}$.\n\n**Case 3: $f(x) = \\cosh(x)$ at $x_0 = 3$**\nThe function and its third derivative are $f(x) = \\cosh(x)$ and $f^{(3)}(x) = \\sinh(x)$. At $x_0=3$:\n- $\\lvert f(x_0) \\rvert = \\lvert\\cosh(3)\\rvert = \\cosh(3)$\n- $\\lvert f^{(3)}(x_0) \\rvert = \\lvert\\sinh(3)\\rvert = \\sinh(3)$\nBoth $\\cosh(3)$ and $\\sinh(3)$ are positive, and $\\sinh(3) \\neq 0$.\n$$\nh_{\\mathrm{opt}} = \\left( \\frac{3 \\cosh(3) u}{\\sinh(3)} \\right)^{1/3} = \\left( 3 u \\coth(3) \\right)^{1/3}\n$$\nThe values are computed for $u = u_{\\mathrm{double}}$ and $u = u_{\\mathrm{quad}}$.\n\nThe final step is the programmatic computation of these six values.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal step size h_opt that minimizes the error in the\n    central difference approximation of the first derivative, based on a\n    leading-order error model.\n    \"\"\"\n\n    # Define unit roundoff values for double and quadruple precision\n    u_double = 2**-53\n    u_quad = 2**-113\n\n    # The general formula for the optimal step size is derived as:\n    # h_opt = (3 * |f(x0)| * u / |f'''(x0)|)^(1/3)\n\n    # Test suite: each tuple contains (function_name, function, third_derivative, x0)\n    test_cases = [\n        (\"sin(x) at x=1\", np.sin, lambda x: -np.cos(x), 1.0),\n        (\"sin(x) at x=1e-12\", np.sin, lambda x: -np.cos(x), 1e-12),\n        (\"cosh(x) at x=3\", np.cosh, np.sinh, 3.0),\n    ]\n\n    results = []\n    precisions = [u_double, u_quad]\n\n    for name, f, f3, x0 in test_cases:\n        # Evaluate the absolute values of the function and its third derivative at x0\n        # The problem statement guarantees f'''(x0) is not zero for the given cases.\n        abs_f_x0 = np.abs(f(x0))\n        abs_f3_x0 = np.abs(f3(x0))\n        \n        # Check for division by zero, although not expected for these cases.\n        if abs_f3_x0 == 0:\n            # If f'''(x0) is zero, the error model is inappropriate.\n            # Handle this as an invalid case within the computation.\n            # For this problem, we rely on the problem setter's guarantee.\n            # For a more robust solver, this would raise an error.\n            h_opt_double = np.nan\n            h_opt_quad = np.nan\n        else:\n            # Ratio of function value to third derivative value\n            ratio = abs_f_x0 / abs_f3_x0\n\n            # Calculate h_opt for double precision\n            arg_double = 3 * ratio * u_double\n            h_opt_double = np.cbrt(arg_double)\n\n            # Calculate h_opt for quadruple precision\n            arg_quad = 3 * ratio * u_quad\n            h_opt_quad = np.cbrt(arg_quad)\n        \n        results.append(h_opt_double)\n        results.append(h_opt_quad)\n\n    # Format the output as a comma-separated list in brackets,\n    # with each number in scientific notation with 10 significant digits.\n    # The format specifier \"{:.9e}\" provides 1 digit before the decimal\n    # and 9 digits after, for a total of 10 significant digits.\n    formatted_results = [f\"{val:.9e}\" for val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The magnitude of the truncation error depends not only on the step size $h$ but also profoundly on the behavior of the function being differentiated. Functions that oscillate rapidly or have large higher-order derivatives can produce much larger errors than smoother, more gently varying functions, even with the same $h$. This practice provides a clear, practical demonstration of this principle by comparing the numerical derivative errors for two functions: a low-frequency sinusoid, $\\sin(x)$, and a high-frequency one, $\\sin(100x)$. ",
            "id": "2389561",
            "problem": "You are to analyze and compare the numerical error of a finite difference derivative approximation applied to two sinusoidal functions with different frequencies. Consider the functions $f_1(x) = \\sin(x)$ and $f_{100}(x) = \\sin(100x)$. Angles must be interpreted in radians. For a point $x$ and a step size $h$, approximate the derivative $f'(x)$ using the centered finite difference formula:\n$$\nD_h[f](x) = \\frac{f(x+h) - f(x-h)}{2h}.\n$$\nFor each test case listed below, compute the absolute error for each function by comparing the numerical approximation to the exact derivative. The exact derivative is $f_k'(x) = k \\cos(kx)$ for $f_k(x) = \\sin(kx)$ with $k \\in \\{1, 100\\}$. Define the absolute error for function $f_k$ at $(x,h)$ as\n$$\nE_k(x,h) = \\left| D_h[f_k](x) - f_k'(x) \\right|.\n$$\nFor each test case, compute and report the ratio\n$$\nR(x,h) = \\frac{E_{100}(x,h)}{E_{1}(x,h)}.\n$$\nUse the following test suite of $(x,h)$ pairs, where all $x$ are in radians and all $h$ are dimensionless:\n- Test $1$: $x = 0$, $h = 10^{-1}$.\n- Test $2$: $x = 0$, $h = 10^{-6}$.\n- Test $3$: $x = 1.0$, $h = 10^{-3}$.\n- Test $4$: $x = \\frac{\\pi}{3}$, $h = 10^{-5}$.\n- Test $5$: $x = 2.0$, $h = 10^{-4}$.\n- Test $6$: $x = \\frac{\\pi}{8}$, $h = 10^{-8}$.\n\nYour program must compute $R(x,h)$ for each test case in the given order and produce a single line of output containing the results as a comma-separated list of decimal numbers, rounded to eight significant digits, enclosed in square brackets. For example, the output format must be\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5,\\text{result}_6].\n$$\nEach $\\text{result}_i$ must be a real number (a float). No other text should be printed.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It is a standard problem in computational physics concerning the error analysis of finite difference approximations. Therefore, the problem is valid, and we proceed with its solution.\n\nThe problem requires a comparison of the numerical error when approximating the derivative of two functions, $f_1(x) = \\sin(x)$ and $f_{100}(x) = \\sin(100x)$, using the centered finite difference formula:\n$$\nD_h[f](x) = \\frac{f(x+h) - f(x-h)}{2h}\n$$\nThe core of the analysis rests upon understanding the truncation error of this approximation. We derive this error by considering the Taylor series expansions of $f(x+h)$ and $f(x-h)$ around the point $x$:\n$$\nf(x+h) = f(x) + hf'(x) + \\frac{h^2}{2!}f''(x) + \\frac{h^3}{3!}f'''(x) + \\frac{h^4}{4!}f^{(4)}(x) + \\frac{h^5}{5!}f^{(5)}(x) + O(h^6)\n$$\n$$\nf(x-h) = f(x) - hf'(x) + \\frac{h^2}{2!}f''(x) - \\frac{h^3}{3!}f'''(x) + \\frac{h^4}{4!}f^{(4)}(x) - \\frac{h^5}{5!}f^{(5)}(x) + O(h^6)\n$$\nSubtracting the second expansion from the first yields:\n$$\nf(x+h) - f(x-h) = 2hf'(x) + \\frac{2h^3}{3!}f'''(x) + \\frac{2h^5}{5!}f^{(5)}(x) + O(h^7)\n$$\nDividing by $2h$ gives the expression for the numerical derivative:\n$$\nD_h[f](x) = f'(x) + \\frac{h^2}{6}f'''(x) + \\frac{h^4}{120}f^{(5)}(x) + O(h^6)\n$$\nThe truncation error, $E(x,h) = |D_h[f](x) - f'(x)|$, is therefore:\n$$\nE(x,h) = \\left| \\frac{h^2}{6}f'''(x) + \\frac{h^4}{120}f^{(5)}(x) + \\dots \\right|\n$$\nFor a sufficiently small step size $h$, the error is dominated by the leading term, and we can approximate it as:\n$$\nE(x,h) \\approx \\left| \\frac{h^2}{6}f'''(x) \\right|\n$$\nThis shows that the centered difference scheme has a truncation error of order $O(h^2)$.\n\nLet us apply this general result to the functions $f_k(x) = \\sin(kx)$ for $k \\in \\{1, 100\\}$.\nThe derivatives are:\n- $f_k'(x) = k \\cos(kx)$\n- $f_k''(x) = -k^2 \\sin(kx)$\n- $f_k'''(x) = -k^3 \\cos(kx)$\nSubstituting the third derivative into the error approximation gives the error for $f_k$:\n$$\nE_k(x,h) \\approx \\left| \\frac{h^2}{6}(-k^3 \\cos(kx)) \\right| = \\frac{h^2 k^3}{6}|\\cos(kx)|\n$$\nWe are asked to compute the ratio $R(x,h) = E_{100}(x,h) / E_1(x,h)$. Using the approximation above:\n$$\nR(x,h) \\approx \\frac{\\frac{h^2 (100^3)}{6}|\\cos(100x)|}{\\frac{h^2 (1^3)}{6}|\\cos(x)|} = 100^3 \\frac{|\\cos(100x)|}{|\\cos(x)|}\n$$\nThis approximation predicts that the ratio $R(x,h)$ should be on the order of $100^3 = 1,000,000$, modulated by the trigonometric term, provided that $\\cos(x)$ is not close to zero and that the $O(h^2)$ error term is indeed dominant. The latter condition holds true as long as the total error is not dominated by machine precision (round-off) effects and the argument $kh$ is small enough for the Taylor series to converge rapidly.\n\nAn exact expression for the error can be found without Taylor expansion. For $f_k(x) = \\sin(kx)$:\n$$\nD_h[f_k](x) = \\frac{\\sin(k(x+h)) - \\sin(k(x-h))}{2h} = \\frac{\\sin(kx+kh) - \\sin(kx-kh)}{2h}\n$$\nUsing the identity $\\sin(A) - \\sin(B) = 2\\cos\\left(\\frac{A+B}{2}\\right)\\sin\\left(\\frac{A-B}{2}\\right)$, we find:\n$$\nD_h[f_k](x) = \\frac{2\\cos(kx)\\sin(kh)}{2h} = \\cos(kx) \\frac{\\sin(kh)}{h}\n$$\nThe exact error is then:\n$E_k(x,h) = |D_h[f_k](x) - f_k'(x)| = \\left| \\cos(kx) \\frac{\\sin(kh)}{h} - k \\cos(kx) \\right| = |\\cos(kx)| \\left| \\frac{\\sin(kh)}{h} - k \\right|$.\nThis formula is exact for the truncation error. However, numerical computation on a machine is subject to finite precision.\n\nThis leads to two important considerations for the test suite:\n1.  **Validity of the $O(h^2)$ approximation:** The Taylor expansion of $\\frac{\\sin(y)}{y}$ is $1 - \\frac{y^2}{6} + \\dots$. The approximation $E \\propto h^2$ is valid only when $y=kh$ is small ($kh \\ll 1$). For Test $1$, where $k=100$ and $h=10^{-1}$, we have $kh=10$, which is not small. Therefore, the simple ratio prediction of $100^3$ will not hold. For all other tests, $kh \\le 0.1$, and the approximation is expected to be more accurate.\n2.  **Round-off Error:** For very small $h$, such as $h=10^{-8}$ in Test $6$, the computation of $f(x+h)-f(x-h)$ suffers from catastrophic cancellation, as it is the difference of two nearly equal numbers. The round-off error, which scales as $\\epsilon_m/h$ where $\\epsilon_m$ is the machine epsilon, can become larger than the truncation error, which scales as $h^2$.\n\nLet us analyze Test $6$ ($x = \\pi/8$, $h=10^{-8}$) specifically.\nFor $f_{100}(x)$, the truncation error is proportional to $f_{100}'''(x) = -100^3 \\cos(100x)$. At $x=\\pi/8$, $100x = 100\\pi/8 = 25\\pi/2$. Since $\\cos(25\\pi/2) = \\cos(12\\pi + \\pi/2) = 0$, the leading $O(h^2)$ term of the truncation error vanishes. In fact, all odd derivatives of $f_{100}(x)$ at $x=\\pi/8$ are zero. This means the analytical truncation error is exactly zero.\nTherefore, the entire computed error $E_{100}(\\pi/8, h)$ consists of round-off error. This error is approximately $E_{100, RO} \\approx \\frac{\\epsilon_m |\\sin(100\\pi/8)|}{h} = \\frac{\\epsilon_m}{h}$.\nFor $f_1(x)$, the truncation error is $E_{1, trunc} \\approx \\frac{h^2}{6}|\\cos(\\pi/8)| = \\frac{(10^{-8})^2}{6}|\\cos(\\pi/8)| \\approx O(10^{-17})$. The round-off error is $E_{1, RO} \\approx \\frac{\\epsilon_m |\\sin(\\pi/8)|}{h} \\approx \\frac{10^{-16} \\times 0.38}{10^{-8}} \\approx O(10^{-9})$.\nFor both functions, round-off error clearly dominates at $h=10^{-8}$. The ratio is thus approximated by the ratio of round-off errors:\n$$\nR(\\pi/8, 10^{-8}) \\approx \\frac{E_{100, RO}}{E_{1, RO}} \\approx \\frac{\\epsilon_m |\\sin(100\\pi/8)|/h}{\\epsilon_m |\\sin(\\pi/8)|/h} = \\frac{|\\sin(25\\pi/2)|}{|\\sin(\\pi/8)|} = \\frac{1}{\\sin(\\pi/8)} \\approx 2.613\n$$\nThe code below implements the direct computation of the errors $E_1(x,h)$ and $E_{100}(x,h)$ and their ratio $R(x,h)$ for each test case, using standard double-precision floating-point arithmetic. The results will reflect the theoretical considerations outlined above.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes and compares the numerical error of a finite difference\n    derivative approximation for two sinusoidal functions.\n    \"\"\"\n\n    # Define the two functions as per the problem statement.\n    # f_k(x) = sin(k*x)\n    def f1(x):\n        return np.sin(x)\n\n    def f100(x):\n        return np.sin(100 * x)\n\n    # Define the exact derivatives.\n    # f_k'(x) = k*cos(k*x)\n    def df1_exact(x):\n        return np.cos(x)\n\n    def df100_exact(x):\n        return 100 * np.cos(100 * x)\n\n    # Define the centered finite difference formula.\n    def centered_difference(f, x, h):\n        \"\"\"\n        Computes the numerical derivative of function f at point x with step size h.\n        \"\"\"\n        return (f(x + h) - f(x - h)) / (2 * h)\n\n    # List of test cases (x, h)\n    test_cases = [\n        (0.0, 1e-1),\n        (0.0, 1e-6),\n        (1.0, 1e-3),\n        (np.pi / 3, 1e-5),\n        (2.0, 1e-4),\n        (np.pi / 8, 1e-8),\n    ]\n\n    results = []\n    for x, h in test_cases:\n        # Compute numerical derivatives\n        d1_num = centered_difference(f1, x, h)\n        d100_num = centered_difference(f100, x, h)\n\n        # Compute exact derivatives\n        d1_true = df1_exact(x)\n        d100_true = df100_exact(x)\n        \n        # Compute absolute errors for each function\n        E1 = np.abs(d1_num - d1_true)\n        E100 = np.abs(d100_num - d100_true)\n\n        # Check for division by zero, although analysis suggests it won't occur.\n        if E1 == 0.0:\n            # If E1 is zero, the ratio is ill-defined.\n            # This could happen if the numerical derivative is perfect.\n            # However, with finite h and non-special x, this is unlikely.\n            # If E100 is also zero, ratio is 1, otherwise it is infinity.\n            # We handle this case by appending a placeholder if it ever occurs.\n            if E100 == 0.0:\n                R = 1.0 # Or another sensible value like 0/0 -> NaN\n            else:\n                R = np.inf\n        else:\n            # Compute the ratio of the errors\n            R = E100 / E1\n        \n        results.append(R)\n\n    # Format the final output string as a comma-separated list of numbers\n    # rounded to 8 significant digits, enclosed in brackets.\n    output_str = \"[\" + \",\".join(f\"{res:.8g}\" for res in results) + \"]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond analyzing existing formulas, a powerful skill is the ability to design new numerical methods tailored to specific needs. This advanced exercise challenges you to construct a custom, higher-order finite difference stencil for the first derivative using a wider set of points. By enforcing exactness for polynomials up to degree four, you will derive a fourth-order accurate formula, revealing the deep connection between polynomial interpolation and the order of a method's truncation error. ",
            "id": "3125013",
            "problem": "Consider a real analytic function $u$ on an open interval containing a point $x_0 \\in \\mathbb{R}$, and a uniform grid with spacing $h > 0$. You are asked to construct a linear finite difference stencil to approximate the first derivative $u'(x_0)$ using only the function values at the four neighboring points $x_0 \\pm h$ and $x_0 \\pm 2h$. The stencil must be exact for every polynomial of degree up to $m=4$. Use only symmetry and the requirement of polynomial exactness to determine the coefficients of your stencil, starting from the definition of the derivative and the existence of a convergent Taylor series for analytic functions.\n\nThen, derive the truncation error of your finite difference approximation for analytic $u$ by expanding $u$ in a Taylor series around $x_0$ and identifying the first nonvanishing term in the residual. Present the leading-order truncation error term as a single closed-form analytic expression in terms of $h$ and derivatives of $u$ evaluated at $x_0$.\n\nFinally, explain under what conditions increasing the degree of polynomial exactness $m$ improves accuracy for analytic $u$, and when such increases may not yield practical accuracy improvements. Your explanation must rely on the order of the truncation error with respect to $h$ and the smoothness of $u$, without invoking any shortcut formulas.\n\nProvide, as your final answer, the leading-order truncation error term of your stencil, expressed symbolically in terms of $h$ and derivatives of $u$ at $x_0$.",
            "solution": "The problem requires the construction and analysis of a finite difference stencil. Before proceeding, we must validate the problem statement.\n\nFirst, we extract the givens verbatim from the problem statement:\n- Function: a real analytic function $u$ on an open interval containing a point $x_0 \\in \\mathbb{R}$.\n- Grid: a uniform grid with spacing $h > 0$.\n- Stencil Points: $x_0 \\pm h$ and $x_0 \\pm 2h$.\n- Target: approximation of the first derivative $u'(x_0)$.\n- Constraint: the stencil must be linear.\n- Constraint: the stencil must be exact for every polynomial of degree up to $m=4$.\n- Methodology: use symmetry and polynomial exactness.\n- Task 1: Determine the coefficients of the stencil.\n- Task 2: Derive the leading-order truncation error.\n- Task 3: Explain conditions for accuracy improvement when increasing the degree of polynomial exactness $m$.\n\nNext, we validate these givens and the problem structure.\n- **Scientific Grounding**: The problem is firmly rooted in the principles of numerical analysis and calculus, specifically Taylor's theorem and the method of undetermined coefficients for finite difference schemes. These are standard, well-established concepts.\n- **Well-Posedness**: The problem is well-posed. We seek four unknown coefficients for the four specified points. The problem specifies using symmetry. For a central stencil approximating an odd derivative, the coefficients must be anti-symmetric, which reduces the number of independent unknowns to two. The requirement of exactness for polynomials of degrees up to $m=4$ provides sufficient constraints to uniquely determine these coefficients. The subsequent derivation of the truncation error is a direct analytical procedure.\n- **Objectivity**: The problem is stated using precise, unambiguous mathematical language and is free from subjective or opinion-based content.\n- **Completeness and Consistency**: The problem provides all necessary information and constraints to derive a unique solution without any internal contradictions.\n\nThe problem statement is scientifically sound, well-posed, objective, and complete. It does not violate any of the criteria for invalidity. Therefore, the problem is deemed valid.\n\nWe now proceed with the solution. A linear finite difference stencil for the first derivative $u'(x_0)$ using the points $x_0 \\pm h$ and $x_0 \\pm 2h$ can be written as:\n$$ D_h u(x_0) = c_{-2} u(x_0 - 2h) + c_{-1} u(x_0 - h) + c_1 u(x_0 + h) + c_2 u(x_0 + 2h) $$\nThe problem specifies a symmetric arrangement of points around $x_0$. To approximate the first derivative, which is an odd function under reflection about the point of differentiation (if the function itself is even or odd), we impose anti-symmetry on the stencil coefficients: $c_{-k} = -c_k$. This gives $c_{-1} = -c_1$ and $c_{-2} = -c_2$. The stencil simplifies to:\n$$ D_h u(x_0) = c_1 (u(x_0+h) - u(x_0-h)) + c_2 (u(x_0+2h) - u(x_0-2h)) $$\nWe have two unknown coefficients, $c_1$ and $c_2$. To determine them, we enforce the condition that the stencil is exact for polynomials $p(x)$ of degree up to $m=4$. By linearity, it suffices to test this for a basis of polynomials, such as $\\{x^k\\}_{k=0}^4$. Due to the translational invariance of the derivative operator, we can set $x_0=0$ without loss of generality. The exact derivative is $p'(0)$.\n\nFor $p(x)=x^k$, $p'(0) = 1$ if $k=1$ and $p'(0)=0$ if $k \\neq 1$.\nFor $k=0$ ($p(x)=1$): $D_h p(0) = c_1(1-1) + c_2(1-1) = 0$. This matches $p'(0)=0$.\nFor $k=2$ ($p(x)=x^2$): $D_h p(0) = c_1(h^2 - (-h)^2) + c_2((2h)^2 - (-2h)^2) = 0$. This matches $p'(0)=0$.\nThe same result holds for any even power $k=2n$ due to the anti-symmetric form of the stencil. Thus, the stencil is automatically exact for $p(x)=x^0$, $p(x)=x^2$, and $p(x)=x^4$.\n\nWe need only consider the odd powers.\nFor $k=1$ ($p(x)=x$): $p'(0)=1$. We require $D_h p(0) = 1$.\n$$ c_1(h - (-h)) + c_2(2h - (-2h)) = 1 \\implies 2h c_1 + 4h c_2 = 1 $$\n$$ 2 c_1 + 4 c_2 = \\frac{1}{h} \\quad (1) $$\nFor $k=3$ ($p(x)=x^3$): $p'(0)=0$. We require $D_h p(0) = 0$.\n$$ c_1(h^3 - (-h)^3) + c_2((2h)^3 - (-2h)^3) = 0 \\implies c_1(2h^3) + c_2(16h^3) = 0 $$\n$$ c_1 + 8 c_2 = 0 \\implies c_1 = -8 c_2 \\quad (2) $$\nSubstituting $(2)$ into $(1)$:\n$$ 2(-8 c_2) + 4 c_2 = \\frac{1}{h} \\implies -16 c_2 + 4 c_2 = \\frac{1}{h} \\implies -12 c_2 = \\frac{1}{h} $$\nThis gives $c_2 = -\\frac{1}{12h}$.\nFrom $(2)$, we find $c_1 = -8(-\\frac{1}{12h}) = \\frac{8}{12h} = \\frac{2}{3h}$.\nThe coefficients are $c_1 = \\frac{2}{3h}$, $c_{-1} = -\\frac{2}{3h}$, $c_2 = -\\frac{1}{12h}$, and $c_{-2} = \\frac{1}{12h}$.\nThe finite difference stencil is:\n$$ D_h u(x_0) = \\frac{1}{12h} [ u(x_0-2h) - 8u(x_0-h) + 8u(x_0+h) - u(x_0+2h) ] $$\n\nNext, we derive the truncation error, $\\tau = D_h u(x_0) - u'(x_0)$. Since $u$ is analytic, we can use its Taylor series expansion around $x_0$. Let $u^{(k)}_0 = u^{(k)}(x_0)$.\n$$ u(x_0 \\pm \\delta) = u(x_0) \\pm \\delta u^{(1)}_0 + \\frac{\\delta^2}{2!} u^{(2)}_0 \\pm \\frac{\\delta^3}{3!} u^{(3)}_0 + \\frac{\\delta^4}{4!} u^{(4)}_0 \\pm \\frac{\\delta^5}{5!} u^{(5)}_0 + O(\\delta^6) $$\nWe compute the differences required by the stencil:\n$$ u(x_0+\\delta) - u(x_0-\\delta) = 2\\delta u^{(1)}_0 + \\frac{2\\delta^3}{3!} u^{(3)}_0 + \\frac{2\\delta^5}{5!} u^{(5)}_0 + O(\\delta^7) $$\nSubstitute this into the form $D_h u(x_0) = c_1 [u(x_0+h) - u(x_0-h)] + c_2 [u(x_0+2h) - u(x_0-2h)]$:\n$$ D_h u(x_0) = \\frac{2}{3h} \\left( 2h u^{(1)}_0 + \\frac{h^3}{3} u^{(3)}_0 + \\frac{h^5}{60} u^{(5)}_0 \\right) - \\frac{1}{12h} \\left( 4h u^{(1)}_0 + \\frac{8h^3}{3} u^{(3)}_0 + \\frac{32h^5}{60} u^{(5)}_0 \\right) + O(h^6) $$\nWe collect terms by the order of the derivative of $u$:\nCoefficient of $u^{(1)}_0$: $\\frac{2}{3h}(2h) - \\frac{1}{12h}(4h) = \\frac{4}{3} - \\frac{1}{3} = 1$. The stencil is correctly normalized.\nCoefficient of $u^{(3)}_0$: $\\frac{2}{3h}\\left(\\frac{h^3}{3}\\right) - \\frac{1}{12h}\\left(\\frac{8h^3}{3}\\right) = \\frac{2h^2}{9} - \\frac{8h^2}{36} = \\frac{2h^2}{9} - \\frac{2h^2}{9} = 0$. This confirms exactness for cubic polynomials.\nCoefficient of $u^{(5)}_0$: This will be the leading error term.\n$$ \\frac{2}{3h}\\left(\\frac{h^5}{60}\\right) - \\frac{1}{12h}\\left(\\frac{32h^5}{60}\\right) = \\frac{2h^4}{180} - \\frac{32h^4}{720} = \\frac{h^4}{90} - \\frac{4h^4}{90} = -\\frac{3h^4}{90} = -\\frac{h^4}{30} $$\nSo, the expansion of the stencil is:\n$$ D_h u(x_0) = u^{(1)}_0 - \\frac{h^4}{30} u^{(5)}_0 + O(h^6) $$\nThe truncation error is $\\tau = D_h u(x_0) - u^{(1)}_0$. The leading-order term is therefore $-\\frac{1}{30}h^4 u^{(5)}(x_0)$. This shows the method is fourth-order accurate, as expected for a symmetric stencil exact for polynomials up to degree $m=4$.\n\nFinally, we explain the conditions under which increasing the degree of polynomial exactness $m$ improves accuracy.\nIncreasing $m$ generally increases the order of accuracy $p$ of the finite difference scheme. For a central stencil for the first derivative, the order $p$ is even, and the truncation error takes the form $\\tau \\approx C_p h^p u^{(p+1)}(x_0) + O(h^{p+2})$, where $C_p$ is a constant. For our stencil, $m=4$ yields $p=4$. A lower-order a stencil, e.g., with $m=2$, would have $p=2$ and an error $\\tau \\approx C_2 h^2 u^{(3)}(x_0)$.\nThe main advantage of increasing $m$ (and thus $p$) is that as the grid spacing $h$ approaches zero, the error term decreases much more rapidly due to the higher power of $h$. For a sufficiently small $h$, a higher-order method is always more accurate.\n\nHowever, a practical accuracy improvement is not guaranteed for any arbitrary $h$. The actual error magnitude depends on the entire term $C_p h^p u^{(p+1)}(x_0)$.\n- **Smoothness of $u$**: The term \"smoothness\" in this context refers to the behavior of the derivatives of $u$. If the higher-order derivatives of the analytic function $u$ grow very rapidly in magnitude, the advantage of the $h^p$ factor can be offset. For instance, comparing a second-order method (error $\\propto h^2 u^{(3)}(x_0)$) to our fourth-order method (error $\\propto h^4 u^{(5)}(x_0)$), if $|u^{(5)}(x_0)| \\gg |u^{(3)}(x_0)|$, the fourth-order method may be less accurate for a moderately small value of $h$. The higher-order method only becomes superior when $h$ is small enough for the $h^4$ factor to overcome the large magnitude of $u^{(5)}(x_0)$.\n- **Asymptotic Regime**: For any analytic function on a compact interval, its derivatives are bounded. Therefore, there always exists a value of $h$ small enough such that the higher-order method is more accurate. However, this \"asymptotic regime\" may require an impractically small $h$ if the function's derivatives grow quickly. For highly oscillatory functions, for example, the derivatives can be very large, making lower-order methods preferable unless $h$ is made extremely small to resolve the oscillations.\n\nIn summary, increasing the degree of polynomial exactness $m$ improves accuracy when $h$ is small enough for the algebraic convergence rate $h^p$ to dominate the analytic factor related to the magnitude of the higher-order derivative $u^{(p+1)}(x_0)$. This condition is met for smoother functions (those with well-behaved, non-exploding derivatives) over a wider range of practical $h$ values.",
            "answer": "$$\\boxed{-\\frac{1}{30}h^4 u^{(5)}(x_0)}$$"
        }
    ]
}