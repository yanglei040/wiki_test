{
    "hands_on_practices": [
        {
            "introduction": "This first practice lays the groundwork by comparing the three most common finite difference formulas: forward, backward, and central difference. By applying these methods to a known function, you can directly calculate and compare their approximation errors. This exercise  highlights the superior accuracy of the central difference formula, providing a concrete example of how the order of the truncation error ($O(h)$ vs. $O(h^2)$) translates to practical performance.",
            "id": "2191753",
            "problem": "Consider the function $f(x) = x \\exp(-x)$. We are interested in approximating its derivative, $f'(x)$, at the point $x_0 = 1$. The quality of an approximation is measured by its absolute error, defined as the absolute difference between the approximated value and the true value of the derivative.\n\nYou are to compare three common finite difference formulas for approximating the derivative:\n1.  **Forward Difference Approximation**: $D_f(x_0, h) = \\frac{f(x_0+h) - f(x_0)}{h}$\n2.  **Backward Difference Approximation**: $D_b(x_0, h) = \\frac{f(x_0) - f(x_0-h)}{h}$\n3.  **Central Difference Approximation**: $D_c(x_0, h) = \\frac{f(x_0+h) - f(x_0-h)}{2h}$\n\nLet $E_f$, $E_b$, and $E_c$ be the absolute errors corresponding to the forward, backward, and central difference approximations, respectively, when using a step size of $h = 0.1$.\n\nCalculate the value of the ratio $R = \\frac{E_f + E_b}{E_c}$. Round your final answer to three significant figures.",
            "solution": "We begin with $f(x) = x \\exp(-x)$. Its derivative is obtained by the product rule:\n$$\nf'(x) = \\exp(-x) + x \\frac{d}{dx}\\big(\\exp(-x)\\big) = \\exp(-x) - x \\exp(-x) = \\exp(-x)\\,(1 - x).\n$$\nAt $x_{0} = 1$, this gives\n$$\nf'(1) = \\exp(-1)\\,(1 - 1) = 0.\n$$\nHence, for any finite difference approximation $D$, the absolute error at $x_{0}=1$ is simply the absolute value of the approximation:\n$$\nE_{f} = |D_{f}(1,h)|,\\quad E_{b} = |D_{b}(1,h)|,\\quad E_{c} = |D_{c}(1,h)|.\n$$\n\nWith $h = 0.1$, compute the three finite differences using $f(x) = x \\exp(-x)$:\n- Forward difference:\n$$\nD_{f}(1,h) = \\frac{f(1+h) - f(1)}{h} = \\frac{(1.1)\\exp(-1.1) - \\exp(-1)}{0.1}.\n$$\n- Backward difference:\n$$\nD_{b}(1,h) = \\frac{f(1) - f(1-h)}{h} = \\frac{\\exp(-1) - (0.9)\\exp(-0.9)}{0.1}.\n$$\n- Central difference:\n$$\nD_{c}(1,h) = \\frac{f(1+h) - f(1-h)}{2h} = \\frac{(1.1)\\exp(-1.1) - (0.9)\\exp(-0.9)}{0.2}.\n$$\n\nFactor $\\exp(-1)$ by writing $\\exp(-1.1) = \\exp(-1)\\exp(-0.1)$ and $\\exp(-0.9) = \\exp(-1)\\exp(0.1)$. Define $t = \\exp(0.1)$ so that $\\exp(-0.1) = 1/t$. Then\n$$\nD_{f}(1,h) = \\frac{\\exp(-1)}{0.1}\\left(1.1\\,\\frac{1}{t} - 1 \\right),\\quad\nD_{b}(1,h) = \\frac{\\exp(-1)}{0.1}\\left(1 - 0.9\\,t \\right),\n$$\n$$\nD_{c}(1,h) = \\frac{\\exp(-1)}{0.2}\\left(1.1\\,\\frac{1}{t} - 0.9\\,t \\right).\n$$\nLet\n$$\nA = 1.1\\,\\frac{1}{t} - 1,\\quad B = 1 - 0.9\\,t,\\quad C = 1.1\\,\\frac{1}{t} - 0.9\\,t.\n$$\nThen\n$$\nE_{f} = \\frac{\\exp(-1)}{0.1}\\,|A|,\\quad E_{b} = \\frac{\\exp(-1)}{0.1}\\,|B|,\\quad E_{c} = \\frac{\\exp(-1)}{0.2}\\,|C|.\n$$\nTherefore, the ratio simplifies to\n$$\nR = \\frac{E_{f} + E_{b}}{E_{c}} = \\frac{\\frac{\\exp(-1)}{0.1}(|A|+|B|)}{\\frac{\\exp(-1)}{0.2}|C|} = 2\\,\\frac{|A|+|B|}{|C|}.\n$$\n\nNumerically, with $t = \\exp(0.1) \\approx 1.1051701859880927$ and $1/t = \\exp(-0.1) \\approx 0.9048374180359596$,\n$$\nA = 1.1\\cdot 0.9048374180359596 - 1 \\approx -0.00467884016044447,\\quad |A| \\approx 0.00467884016044447,\n$$\n$$\nB = 1 - 0.9\\cdot 1.1051701859880927 \\approx 0.00534683261071661,\\quad |B| \\approx 0.00534683261071661,\n$$\n$$\nC = 1.1\\cdot 0.9048374180359596 - 0.9\\cdot 1.1051701859880927 \\approx 0.000667992450272143,\\quad |C| \\approx 0.000667992450272143.\n$$\nHence,\n$$\nR = 2\\,\\frac{0.00467884016044447 + 0.00534683261071661}{0.000667992450272143} \\approx \\frac{0.02005134554232216}{0.000667992450272143} \\approx 30.0173.\n$$\nRounded to three significant figures,\n$$\nR \\approx 30.0\n$$",
            "answer": "$$\\boxed{30.0}$$"
        },
        {
            "introduction": "While smaller step sizes $h$ reduce truncation error, they are not always better in practice due to the limitations of computer arithmetic. This exercise  delves into the critical trade-off between truncation error and round-off error, which arises from \"catastrophic cancellation\" when subtracting nearly equal numbers. You will analyze how these two error sources behave as a function of $h$ and derive the theoretical optimal step size that minimizes the total error.",
            "id": "2415137",
            "problem": "Consider the continuously compounded price of a fixed-coupon bond as a function of the continuously compounded yield $y$:\n$$\nP(y) \\;=\\; \\sum_{k=1}^{N} c_k \\, e^{-y t_k},\n$$\nwhere $t_k$ are payment times (in years) and $c_k$ are the corresponding cash flows. Let the bond have face value $F=100$, annual coupon rate $c=0.05$, maturity $T=10$ years, and semiannual payments ($m=2$), so that $N=mT=20$, $t_k = k/m$ for $k=1,\\ldots,20$, $c_k = 2.5$ for $k=1,\\ldots,19$, and $c_{20} = 102.5$. Let $y_0 = 0.03$.\n\nSuppose you estimate the derivative $P'(y_0)$ by the forward-difference formula\n$$\nD_h \\;=\\; \\frac{P(y_0 + h) - P(y_0)}{h},\n$$\ncomputed in standard double-precision floating point arithmetic, modeled by the relative error bound $|\\delta|\\le \\epsilon_{mach}$ for each arithmetic operation, with $\\epsilon_{mach} \\approx 1.11 \\times 10^{-16}$.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. For sufficiently small $h$, the dominant floating-point rounding error in $D_h$ behaves like $O\\!\\left(\\dfrac{\\epsilon_{mach}\\,|P(y_0)|}{h}\\right)$ due to subtracting nearly equal numbers, a phenomenon known as catastrophic cancellation.\n\nB. The truncation error of the forward-difference approximation $D_h$ is $O(h^2)$ with a leading constant depending on $P^{(3)}(y_0)$.\n\nC. Balancing leading truncation and rounding errors gives an asymptotic optimal step size of the form $h^* \\asymp \\sqrt{\\dfrac{\\epsilon_{mach}\\,|P(y_0)|}{|P''(y_0)|}}$.\n\nD. For the specified bond at $y_0=0.03$ in double precision with $\\epsilon_{mach} \\approx 1.11 \\times 10^{-16}$, the order of magnitude of $h^*$ that minimizes the leading-order total error is closest to $10^{-9}$.\n\nE. Switching to a central-difference derivative would eliminate catastrophic cancellation entirely for all $h$.",
            "solution": "We begin from first principles: the forward-difference formula is defined by\n$$\nD_h \\;=\\; \\frac{P(y_0+h)-P(y_0)}{h}.\n$$\nBy Taylor’s theorem about $y_0$, for some $\\xi$ between $y_0$ and $y_0+h$,\n$$\nP(y_0+h) \\;=\\; P(y_0) \\;+\\; h\\,P'(y_0) \\;+\\; \\frac{h^2}{2}\\,P''(\\xi).\n$$\nSubstituting into $D_h$ and subtracting $P'(y_0)$ yields the truncation (discretization) error\n$$\nD_h - P'(y_0) \\;=\\; \\frac{h}{2}\\,P''(\\xi),\n$$\nso the truncation error is $O(h)$ with leading behavior $\\frac{h}{2} P''(y_0)$ as $h \\to 0$.\n\nNext, we model floating-point rounding. Under the standard relative error model, each evaluated quantity is perturbed by a multiplicative factor $(1+\\delta)$ with $|\\delta|\\le \\epsilon_{mach}$. Let the computed values be\n$$\n\\widehat{P(y_0+h)} = P(y_0+h)(1+\\delta_1), \\quad \\widehat{P(y_0)} = P(y_0)(1+\\delta_2),\n$$\nwith $|\\delta_1|,|\\delta_2| \\le \\epsilon_{mach}$. The computed numerator of the forward difference is\n$$\n\\widehat{P(y_0+h)} - \\widehat{P(y_0)} \\;=\\; \\big(P(y_0+h) - P(y_0)\\big) \\;+\\; \\delta_1 P(y_0+h) \\;-\\; \\delta_2 P(y_0).\n$$\nDividing by $h$ gives an additive rounding error term\n$$\n\\frac{\\delta_1 P(y_0+h) - \\delta_2 P(y_0)}{h}.\n$$\nFor small $h$, $P(y_0+h) \\approx P(y_0)$, so the difference $P(y_0+h)-P(y_0)$ in the numerator is small, and the term $\\frac{\\delta_1 P(y_0+h) - \\delta_2 P(y_0)}{h}$ is on the order of\n$$\nO\\!\\left(\\frac{\\epsilon_{mach}\\,|P(y_0)|}{h}\\right).\n$$\nThis is the hallmark of catastrophic cancellation: subtraction of nearly equal large quantities reduces significant digits, amplifying relative rounding error by roughly $1/h$ in the derivative quotient.\n\nCombining truncation and rounding, the leading-order total error can be expressed as\n$$\nE(h) \\;\\approx\\; \\frac{h}{2}\\,|P''(y_0)| \\;+\\; C\\,\\frac{\\epsilon_{mach}\\,|P(y_0)|}{h},\n$$\nwhere $C$ is a modest constant reflecting details of the floating-point evaluation and algebraic rearrangements. Minimizing this leading-order model with respect to $h$ gives\n$$\n\\frac{d}{dh}\\left(\\frac{h}{2}|P''(y_0)| + C\\,\\frac{\\epsilon_{mach}\\,|P(y_0)|}{h}\\right)=0\n\\;\\Rightarrow\\;\n\\frac{1}{2}|P''(y_0)| - C\\,\\frac{\\epsilon_{mach}\\,|P(y_0)|}{h^2}=0,\n$$\nso\n$$\nh^* \\;=\\; \\sqrt{\\frac{2C\\,\\epsilon_{mach}\\,|P(y_0)|}{|P''(y_0)|}}\n\\;\\asymp\\; \\sqrt{\\frac{\\epsilon_{mach}\\,|P(y_0)|}{|P''(y_0)|}},\n$$\nwhere $\\asymp$ indicates equality up to a constant factor.\n\nWe now evaluate $|P(y_0)|$ and $|P''(y_0)|$ for the specified bond. With continuous compounding and semiannual payment times $t_k = k/2$, $k=1,\\ldots,20$, we have\n$$\nP(y_0) \\;=\\; \\sum_{k=1}^{19} 2.5\\,e^{-0.03\\, (k/2)} \\;+\\; 102.5\\,e^{-0.03 \\cdot 10}.\n$$\nLet $r = e^{-0.03/2} = e^{-0.015} \\approx 0.98511194$. The first sum is a geometric series:\n$$\n\\sum_{k=1}^{19} 2.5\\,r^k \\;=\\; 2.5\\,r\\,\\frac{1-r^{19}}{1-r}.\n$$\nUsing $1-r \\approx 0.01488806$, $r^{19} = e^{-0.015\\cdot 19} = e^{-0.285} \\approx 0.7520$, we obtain\n$$\n\\sum_{k=1}^{19} 2.5\\,r^k \\;\\approx\\; 2.5 \\times 0.98511194 \\times \\frac{1-0.7520}{0.01488806} \\;\\approx\\; 41.02.\n$$\nThe final cash flow term is\n$$\n102.5\\,e^{-0.3} \\;\\approx\\; 102.5 \\times 0.740818 \\;\\approx\\; 75.93.\n$$\nHence\n$$\nP(y_0) \\;\\approx\\; 41.02 + 75.93 \\;\\approx\\; 116.95.\n$$\n\nNext, compute\n$$\nP''(y) \\;=\\; \\sum_{k=1}^{N} c_k\\, t_k^2\\, e^{-y t_k}.\n$$\nThus, at $y_0=0.03$,\n$$\nP''(y_0) \\;=\\; \\sum_{k=1}^{19} 2.5 \\left(\\frac{k}{2}\\right)^2 e^{-0.03\\,(k/2)} \\;+\\; 102.5 \\cdot 10^2 \\cdot e^{-0.3}.\n$$\nThe final term evaluates to\n$$\n102.5 \\cdot 100 \\cdot e^{-0.3} \\;\\approx\\; 102.5 \\times 100 \\times 0.740818 \\;\\approx\\; 7593.38.\n$$\nA direct summation of the coupon terms gives approximately\n$$\n\\sum_{k=1}^{19} 2.5 \\left(\\frac{k}{2}\\right)^2 e^{-0.03\\,(k/2)} \\;\\approx\\; 1242.46,\n$$\nso\n$$\nP''(y_0) \\;\\approx\\; 7593.38 + 1242.46 \\;\\approx\\; 8835.84.\n$$\nTherefore,\n$$\n\\frac{|P(y_0)|}{|P''(y_0)|} \\;\\approx\\; \\frac{116.95}{8835.84} \\;\\approx\\; 1.32 \\times 10^{-2}.\n$$\nWith $\\epsilon_{mach} \\approx 1.11 \\times 10^{-16}$, we have\n$$\n\\epsilon_{mach}\\,\\frac{|P(y_0)|}{|P''(y_0)|} \\;\\approx\\; 1.47 \\times 10^{-18},\n$$\nso\n$$\nh^* \\;\\asymp\\; \\sqrt{1.47 \\times 10^{-18}} \\;\\approx\\; 1.21 \\times 10^{-9},\n$$\nand including the constant $\\sqrt{2C}$ (with $C$ of order $1$) would keep $h^*$ on the order of $10^{-9}$. Thus the order of magnitude closest to $10^{-9}$ is correct.\n\nWe now assess each option:\n\nA. The rounding error analysis above shows the dominant term behaves like $O\\!\\left(\\dfrac{\\epsilon_{mach}\\,|P(y_0)|}{h}\\right)$ for small $h$, arising from subtracting nearly equal large quantities. Verdict: Correct.\n\nB. The forward-difference truncation error from Taylor’s theorem is $O(h)$ with leading term $\\frac{h}{2}P''(y_0)$, not $O(h^2)$, and it depends on $P''(y_0)$, not $P^{(3)}(y_0)$. Verdict: Incorrect.\n\nC. Balancing $O(h)$ truncation and $O(\\epsilon_{mach}/h)$ rounding errors yields $h^* \\asymp \\sqrt{\\dfrac{\\epsilon_{mach}\\,|P(y_0)|}{|P''(y_0)|}}$ up to a constant factor, as derived. Verdict: Correct.\n\nD. Using the computed $P(y_0)$ and $P''(y_0)$ with $\\epsilon_{mach} \\approx 1.11 \\times 10^{-16}$ gives $h^*$ on the order of $10^{-9}$. Verdict: Correct.\n\nE. Central differences reduce truncation error to $O(h^2)$ but still involve subtracting nearly equal quantities $P(y_0+h)$ and $P(y_0-h)$, so rounding error remains of order $O\\!\\left(\\dfrac{\\epsilon_{mach}\\,|P(y_0)|}{h}\\right)$. Catastrophic cancellation is not eliminated for all $h$. Verdict: Incorrect.",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "Building on the theoretical understanding of error balancing, this final practice provides a hands-on coding challenge from computational finance. You will empirically determine the optimal step size for calculating an option's \"Vega\" by minimizing the total error across a range of possible $h$ values. This exercise  demonstrates how the U-shaped error curve appears in a real-world application and solidifies the concept of finding the \"sweet spot\" for $h$ that balances truncation and round-off errors.",
            "id": "2415200",
            "problem": "Consider a European call option on a non-dividend-paying asset priced under the Black–Scholes framework. Let the spot price be $S_0$, the strike be $K$, the continuously compounded risk-free rate be $r$, the time to maturity be $T$, and the volatility be $\\sigma$. The Black–Scholes call price is\n$$\nC(S_0,K,r,T,\\sigma)=S_0\\,\\Phi(d_1)-K e^{-rT}\\,\\Phi(d_2),\n$$\nwhere\n$$\nd_1=\\frac{\\ln\\!\\left(\\frac{S_0}{K}\\right)+\\left(r+\\frac{1}{2}\\sigma^2\\right)T}{\\sigma\\sqrt{T}},\\quad d_2=d_1-\\sigma\\sqrt{T},\n$$\nand $\\Phi(\\cdot)$ is the cumulative distribution function of the standard normal distribution. The analytic Vega of the call is the partial derivative of the price with respect to $\\sigma$ given by\n$$\n\\text{Vega}(S_0,K,r,T,\\sigma)=\\frac{\\partial C}{\\partial \\sigma}=S_0 \\sqrt{T}\\,\\varphi(d_1),\n$$\nwhere $\\varphi(\\cdot)$ is the standard normal probability density function.\n\nDefine a numerical estimator of Vega at a given parameter vector $(S_0,K,r,T,\\sigma)$ by the symmetric difference quotient applied to the volatility,\n$$\n\\widehat{\\text{Vega}}(h)=\\frac{C(S_0,K,r,T,\\sigma+h)-C(S_0,K,r,T,\\sigma-h)}{2h},\n$$\nfor a step size $h>0$ such that $\\sigma-h>0$. For a given $(S_0,K,r,T,\\sigma)$ and a candidate set of step sizes\n$$\n\\mathcal{H}=\\{10^{-12},10^{-11},10^{-10},10^{-9},10^{-8},10^{-7},10^{-6},10^{-5},10^{-4},10^{-3},10^{-2}\\},\n$$\nconsider the admissible subset\n$$\n\\mathcal{H}_{\\text{adm}}=\\{h\\in\\mathcal{H}\\,:\\,0<h<\\sigma/2\\}.\n$$\nDefine the total error at step $h\\in\\mathcal{H}_{\\text{adm}}$ as the absolute error\n$$\nE(h)=\\left|\\widehat{\\text{Vega}}(h)-\\text{Vega}(S_0,K,r,T,\\sigma)\\right|.\n$$\nYour task is to determine, for each specified test case, the step size $h^\\star\\in\\mathcal{H}_{\\text{adm}}$ that empirically minimizes $E(h)$. If there are multiple minimizers, choose the largest $h$ among those that achieve the minimum error.\n\nUse the following test suite of near-the-money cases:\n1. $(S_0,K,r,T,\\sigma)=(100,100,0.01,1.0,0.2)$.\n2. $(S_0,K,r,T,\\sigma)=(100,100,0.01,0.01,0.2)$.\n3. $(S_0,K,r,T,\\sigma)=(100,102,0.02,0.5,0.05)$.\n4. $(S_0,K,r,T,\\sigma)=(100,100,0.03,2.0,0.6)$.\n\nAll quantities are non-dimensional financial parameters, so no physical units are required. Angles are not involved. The final answers for each test case must be returned as floating-point numbers.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[h_1,h_2,h_3,h_4]$), where each $h_i$ is the selected step size $h^\\star$ for test case $i$.",
            "solution": "The task is to find the optimal step size $h^\\star$ for the numerical estimation of an option's Vega. Vega is the first partial derivative of the option price with respect to the volatility, $\\sigma$. The numerical estimation is performed using the symmetric difference quotient. The optimal step size $h^\\star$ is defined as the value from a given discrete set $\\mathcal{H}_{\\text{adm}}$ that minimizes the absolute error between the numerical estimate and the analytical value of Vega.\n\nThe methodology proceeds as follows for each test case defined by the parameter set $(S_0, K, r, T, \\sigma)$:\n\n1.  **Analytic Vega Calculation**: First, the exact value of Vega is computed using the provided analytical formula:\n    $$ \\text{Vega}(S_0,K,r,T,\\sigma) = S_0 \\sqrt{T}\\,\\varphi(d_1) $$\n    where $\\varphi(\\cdot)$ is the probability density function (PDF) of the standard normal distribution, and $d_1$ is given by:\n    $$ d_1=\\frac{\\ln(S_0/K)+\\left(r+\\frac{1}{2}\\sigma^2\\right)T}{\\sigma\\sqrt{T}} $$\n    This analytical value serves as the benchmark against which the numerical estimates are compared.\n\n2.  **Numerical Estimation and Error Calculation**: For each step size $h$ in the admissible set $\\mathcal{H}_{\\text{adm}}$, we compute the numerical approximation of Vega, denoted $\\widehat{\\text{Vega}}(h)$, using the symmetric difference quotient:\n    $$ \\widehat{\\text{Vega}}(h)=\\frac{C(S_0,K,r,T,\\sigma+h)-C(S_0,K,r,T,\\sigma-h)}{2h} $$\n    Here, $C(\\cdot)$ is the Black-Scholes call price function:\n    $$ C(S_0,K,r,T,\\sigma)=S_0\\,\\Phi(d_1)-K e^{-rT}\\,\\Phi(d_2) $$\n    with $\\Phi(\\cdot)$ being the cumulative distribution function (CDF) of the standard normal distribution, and $d_2 = d_1 - \\sigma\\sqrt{T}$. The absolute error for each $h$ is then calculated as:\n    $$ E(h)=\\left|\\widehat{\\text{Vega}}(h)-\\text{Vega}(S_0,K,r,T,\\sigma)\\right| $$\n\n3.  **Optimal Step Size Selection**: The set of candidate step sizes is given by $\\mathcal{H} = \\{10^{-12}, 10^{-11}, \\dots, 10^{-2}\\}$. The admissible set $\\mathcal{H}_{\\text{adm}}$ is defined as $\\{h\\in\\mathcal{H}\\,:\\,0<h<\\sigma/2\\}$. For all provided test cases, the condition $h < \\sigma/2$ is satisfied for all $h \\in \\mathcal{H}$, thus $\\mathcal{H}_{\\text{adm}} = \\mathcal{H}$. We compute the error $E(h)$ for every $h \\in \\mathcal{H}$. We then identify the minimum error, $E_{\\min} = \\min_{h \\in \\mathcal{H}} E(h)$. The optimal step size, $h^\\star$, is the largest value of $h$ from the set of all step sizes that produce this minimum error.\n\nThis process reflects a fundamental trade-off in numerical differentiation. The total error $E(h)$ is a composite of two opposing sources:\n- **Truncation Error**: This error is inherent to the finite difference approximation. For a symmetric difference quotient, it is of order $O(h^2)$. It decreases as $h$ becomes smaller.\n- **Round-off Error**: This error arises from the finite precision of floating-point arithmetic, particularly the catastrophic cancellation when subtracting nearly identical values ($C(\\sigma+h)$ and $C(\\sigma-h)$) and subsequent division by a small $h$. This component is roughly proportional to $\\epsilon_{mach}/h$, where $\\epsilon_{mach}$ is machine epsilon. It increases as $h$ becomes smaller.\n\nThe sum of these two errors results in a total error curve that typically has a U-shape when plotted against $h$ on a logarithmic scale. The empirical search across the discrete set $\\mathcal{H}$ allows us to find the step size that is closest to the bottom of this curve, representing the optimal balance between truncation and round-off errors.\n\nThe implementation will consist of a main function that iterates through each test case. For each case, it will calculate the errors for all candidate step sizes, identify the minimum error, and select the corresponding optimal step size according to the specified tie-breaking rule. The required functions from `scipy.stats.norm` will be used to evaluate the standard normal CDF ($\\Phi$) and PDF ($\\varphi$).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (100.0, 100.0, 0.01, 1.0, 0.2),\n        (100.0, 100.0, 0.01, 0.01, 0.2),\n        (100.0, 102.0, 0.02, 0.5, 0.05),\n        (100.0, 100.0, 0.03, 2.0, 0.6),\n    ]\n\n    # Candidate set of step sizes H\n    H = [10**-12, 10**-11, 10**-10, 10**-9, 10**-8, 10**-7,\n         10**-6, 10**-5, 10**-4, 10**-3, 10**-2]\n\n    # Function to calculate Black-Scholes call price\n    def black_scholes_call(S0, K, r, T, sigma):\n        # Handle edge cases for sigma and T to avoid mathematical errors\n        if sigma = 0 or T = 0:\n            # If T > 0 and sigma = 0, the option price is deterministic.\n            # If T = 0, the option price is its intrinsic value.\n            return max(0.0, S0 - K * np.exp(-r * T))\n            \n        d1 = (np.log(S0 / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n        d2 = d1 - sigma * np.sqrt(T)\n        price = S0 * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n        return price\n\n    # Function to calculate analytical Vega\n    def analytical_vega(S0, K, r, T, sigma):\n        # Vega is zero if T=0 or sigma=0\n        if sigma = 0 or T = 0:\n            return 0.0\n        d1 = (np.log(S0 / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n        vega = S0 * np.sqrt(T) * norm.pdf(d1)\n        return vega\n\n    optimal_h_results = []\n\n    for case in test_cases:\n        S0, K, r, T, sigma = case\n\n        # Determine the admissible set of step sizes.\n        # For all test cases provided, sigma/2 is greater than the largest h_cand in H,\n        # so the admissible set is the full set H.\n        H_adm = [h for h in H if 0  h  sigma / 2.0]\n\n        # Calculate the true analytical Vega to use as a benchmark\n        true_vega = analytical_vega(S0, K, r, T, sigma)\n\n        errors_with_h = []\n        for h in H_adm:\n            # Calculate numerical Vega using symmetric difference quotient\n            C_plus = black_scholes_call(S0, K, r, T, sigma + h)\n            C_minus = black_scholes_call(S0, K, r, T, sigma - h)\n            numerical_vega = (C_plus - C_minus) / (2.0 * h)\n            \n            # Calculate the absolute error\n            error = abs(numerical_vega - true_vega)\n            errors_with_h.append((error, h))\n\n        # Find the minimum error among all step sizes\n        min_error = min(e for e, h in errors_with_h)\n\n        # Find all step sizes that achieve this minimum error\n        minimizing_hs = [h for e, h in errors_with_h if e == min_error]\n\n        # Select the largest h among the minimizers as per the tie-breaking rule\n        optimal_h = max(minimizing_hs)\n        optimal_h_results.append(optimal_h)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, optimal_h_results))}]\")\n\nsolve()\n```"
        }
    ]
}