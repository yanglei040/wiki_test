{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a direct application of Richardson extrapolation to a common scenario in computational science. By using two estimates from a simulation with a known first-order error, you will apply the standard extrapolation formula to compute a more accurate, second-order result. This practice will solidify your understanding of the basic mechanism for improving numerical approximations. ",
            "id": "2197905",
            "problem": "A team of aerospace engineers is simulating the atmospheric entry of a new space probe. They are particularly interested in estimating the maximum deceleration experienced by the probe, a critical parameter for structural design. The numerical algorithm they use to solve the equations of motion has a first-order global truncation error, meaning the error in the estimate is of order $O(h)$, where $h$ is the time step size used in the simulation.\n\nLet $D(h)$ be the estimated maximum deceleration when the simulation is run with a time step of $h$.\nThe team performs two simulations:\n1.  With a time step of $h_1 = 0.4$ seconds, they find the maximum deceleration to be $D(0.4) = 58.6$ $\\text{m/s}^2$.\n2.  With a reduced time step of $h_2 = 0.2$ seconds, they find the maximum deceleration to be $D(0.2) = 59.3$ $\\text{m/s}^2$.\n\nTo obtain a more accurate result without performing another lengthy simulation with an even smaller time step, the engineers decide to apply Richardson extrapolation to these two estimates. Calculate the improved estimate for the maximum deceleration. Express your answer in $\\text{m/s}^2$, rounded to three significant figures.",
            "solution": "Because the numerical method has first-order global truncation error, the estimated quantity satisfies the asymptotic error model\n$$\nD(h) = D^{\\ast} + C h + O(h^{2}),\n$$\nwhere $D^{\\ast}$ is the true maximum deceleration and $C$ is an $h$-independent constant.\n\nLet $h_{1} = 0.4$, $h_{2} = 0.2$, and define the refinement ratio $r = \\frac{h_{1}}{h_{2}} = 2$. Then\n$$\nD(h_{1}) = D^{\\ast} + C h_{1} + O(h_{1}^{2}), \\quad D(h_{2}) = D^{\\ast} + C \\frac{h_{1}}{r} + O\\left(\\frac{h_{1}^{2}}{r^{2}}\\right).\n$$\nEliminating the $O(h)$ term via Richardson extrapolation gives\n$$\nD_{\\text{RE}} = \\frac{r D(h_{2}) - D(h_{1})}{r - 1} = D(h_{2}) + \\frac{D(h_{2}) - D(h_{1})}{r - 1} = D^{\\ast} + O(h_{1}^{2}).\n$$\nSubstituting $r=2$, $D(0.4)=58.6$, and $D(0.2)=59.3$,\n$$\nD_{\\text{RE}} = \\frac{2 \\cdot 59.3 - 58.6}{2 - 1} = 118.6 - 58.6 = 60.0.\n$$\nRounded to three significant figures, the improved estimate is $60.0$ in $\\text{m/s}^{2}$.",
            "answer": "$$\\boxed{60.0}$$"
        },
        {
            "introduction": "Moving beyond simple formula application, this problem challenges you to derive an extrapolation formula from first principles. You will work with a numerical method that has a second-order error, $O(h^2)$, and a non-standard step size reduction, forcing you to construct the algebraic system that cancels the leading error term. This exercise is crucial for understanding that Richardson extrapolation is a general method, not just a single fixed formula. ",
            "id": "2197931",
            "problem": "A numerical simulation is used to estimate a physical quantity, whose true value is denoted by $V$. The approximation generated by the simulation, denoted $A(h)$, depends on a discretization parameter $h > 0$. The error of the approximation is known to follow the relationship:\n$$A(h) = V + C h^{2} + O(h^{4})$$\nwhere $C$ is a constant independent of $h$, and $O(h^{4})$ represents higher-order terms that are negligible for small $h$.\n\nAn analyst performs two simulations. The first simulation, using a step size of $h$, yields the result $A(h)$. The second simulation, using a refined step size of $h/3$, yields the result $A(h/3)$. To improve the estimate for $V$, the analyst wishes to combine these two results to cancel out the leading error term of order $O(h^2)$.\n\nDetermine the formula for this improved estimate of $V$ by finding the specific linear combination of $A(h)$ and $A(h/3)$ that eliminates the $O(h^2)$ error term.",
            "solution": "We are given the asymptotic expansions\n$$A(h) = V + C h^{2} + O(h^{4}),$$\nand, by substituting $h \\mapsto h/3$ and using $O((h/3)^4)=O(h^4)$,\n$$A(h/3) = V + C \\left(\\frac{h}{3}\\right)^{2} + O(h^{4}) = V + \\frac{C}{9} h^{2} + O(h^{4}).$$\nSeek a linear combination $\\alpha A(h/3) + \\beta A(h)$ that eliminates the $O(h^{2})$ term while leaving the leading term equal to $V$:\n$$(\\alpha + \\beta) V + C h^{2} \\left(\\frac{\\alpha}{9} + \\beta\\right) + O(h^{4}).$$\nImpose the two conditions\n$$(i)\\ \\ \\alpha + \\beta = 1,\\qquad (ii)\\ \\ \\frac{\\alpha}{9} + \\beta = 0,$$\nto ensure the coefficient of $V$ is $1$ and the $h^{2}$ term vanishes. Solving,\nfrom $(ii)$ we get $\\beta = -\\frac{\\alpha}{9}$; substituting into $(i)$ gives\n$\\alpha - \\frac{\\alpha}{9} = 1 \\;\\Rightarrow\\; \\frac{8}{9}\\alpha = 1 \\;\\Rightarrow\\; \\alpha = \\frac{9}{8}$, so $\\beta = -\\frac{1}{8}$.\nTherefore, the improved estimate that cancels the $O(h^{2})$ error is\n$$\\frac{9}{8} A(h/3) - \\frac{1}{8} A(h) = \\frac{9 A(h/3) - A(h)}{8},$$\nwhich has truncation error $O(h^{4})$.",
            "answer": "$$\\boxed{\\frac{9 A(h/3) - A(h)}{8}}$$"
        },
        {
            "introduction": "This final practice brings everything together by bridging theory with practical implementation. You will write code to solve a differential equation using the well-known forward Euler method and then apply Richardson extrapolation to improve its accuracy from first order to second order. This hands-on coding exercise demonstrates the tangible benefits of extrapolation in a real-world computational workflow and shows how it can be used to enhance existing numerical tools. ",
            "id": "3226253",
            "problem": "Consider the initial value problem (IVP) for an ordinary differential equation (ODE) given by $y'(t) = f(t, y(t))$ with initial condition $y(t_0) = y_0$. The forward Euler method arises from the fundamental definition of the derivative as the limit of a finite difference and the use of a first-order Taylor expansion. In practice, the forward Euler method updates an approximation $y_n$ to the exact solution $y(t_n)$ by advancing the solution in steps of size $h$ starting at $t_0$ and ending at a specified final time $T$, yielding an approximation at $T$ that we denote $y_h(T)$. The global discretization error of forward Euler is known to depend linearly on the step size $h$ under standard regularity assumptions on $f$ and $y(t)$.\n\nYour task is to:\n1. Implement a function that computes $y_h(T)$ using the forward Euler method for any given function $f(t,y)$, initial condition $y_0$, initial time $t_0$, final time $T$, and uniform step size $h$, assuming $T - t_0$ is an integer multiple of $h$.\n2. Assume the Euler methodâ€™s approximation at $T$ has an asymptotic error expansion of the form $y_h(T) = y(T) + C h + D h^2 + O(h^3)$ for constants $C$ and $D$ that depend on $f$ and the solution but not on $h$. Using the outputs $y_h(T)$ and $y_{h/2}(T)$, derive a linear combination of these two approximations, with constant weights independent of $h$, that cancels the leading $O(h)$ error term and yields an $O(h^2)$-accurate estimate of $y(T)$. Then implement this Richardson extrapolation estimator in code.\n3. For each test case below, compute the absolute error $\\lvert y_{\\text{extrap}}(T) - y(T) \\rvert$, where $y_{\\text{extrap}}(T)$ is your extrapolated estimate and $y(T)$ is the exact solution at time $T$.\n\nUse the following test suite. In all cases, take $t_0 = 0$ and use the provided $h$ such that $(T - t_0)/h$ is an integer:\n\n- Test 1 (happy path, linear homogeneous ODE): $f(t,y) = y$, $y_0 = 1$, $T = 1$, $h = 0.2$. The exact solution is $y(t) = e^{t}$ evaluated at $t = T$.\n- Test 2 (linear nonhomogeneous ODE): $f(t,y) = y + t$, $y_0 = 0$, $T = 2$, $h = 0.4$. The exact solution is $y(t) = e^{t} - t - 1$ evaluated at $t = T$.\n- Test 3 (nonlinear logistic growth): $f(t,y) = r y \\left(1 - \\frac{y}{K}\\right)$ with parameters $r = 1$ and $K = 10$, $y_0 = 1$, $T = 3$, $h = 0.5$. The exact solution is $y(t) = \\frac{K}{1 + A e^{-r t}}$ with $A = \\frac{K - y_0}{y_0}$, evaluated at $t = T$.\n- Test 4 (edge case, zero derivative): $f(t,y) = 0$, $y_0 = 3$, $T = 1$, $h = 0.5$. The exact solution is the constant function $y(t) = 3$ evaluated at $t = T$.\n\nYour program should:\n- Implement the forward Euler method to compute $y_h(T)$ and $y_{h/2}(T)$ for each test case.\n- Implement the derived Richardson extrapolation estimator using the two approximations $y_h(T)$ and $y_{h/2}(T)$ to obtain an $O(h^2)$ estimate at $T$.\n- Compute and record the absolute error for each test case, rounded to ten decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[0.0123456789,0.0000001234,0.0012340000,0.0000000000]\"), where the entries are the absolute errors for Test 1 through Test 4, each rounded to ten decimal places.",
            "solution": "The problem is assessed to be valid.\n\n### Step 1: Extract Givens\n- **Problem Type**: Initial Value Problem (IVP) for an Ordinary Differential Equation (ODE).\n- **ODE Form**: $y'(t) = f(t, y(t))$.\n- **Initial Condition**: $y(t_0) = y_0$.\n- **Numerical Method**: Forward Euler method, where $y_{n+1} = y_n + h f(t_n, y_n)$.\n- **Approximation at final time $T$**: $y_h(T)$.\n- **Asymptotic Error Expansion**: $y_h(T) = y(T) + C h + D h^2 + O(h^3)$.\n- **Constraint**: $T - t_0$ is an integer multiple of the step size $h$.\n- **Task 1**: Implement a function for the forward Euler method to compute $y_h(T)$.\n- **Task 2**: Derive and implement a Richardson extrapolation estimator for $y(T)$ that is $O(h^2)$-accurate, using $y_h(T)$ and $y_{h/2}(T)$.\n- **Task 3**: Compute the absolute error $\\lvert y_{\\text{extrap}}(T) - y(T) \\rvert$ for four test cases.\n- **Initial Time for all cases**: $t_0 = 0$.\n\n- **Test Case 1**:\n  - $f(t,y) = y$\n  - $y_0 = 1$\n  - $T = 1$\n  - $h = 0.2$\n  - Exact solution: $y(t) = e^{t}$\n\n- **Test Case 2**:\n  - $f(t,y) = y + t$\n  - $y_0 = 0$\n  - $T = 2$\n  - $h = 0.4$\n  - Exact solution: $y(t) = e^{t} - t - 1$\n\n- **Test Case 3**:\n  - $f(t,y) = r y \\left(1 - \\frac{y}{K}\\right)$ with $r = 1, K = 10$\n  - $y_0 = 1$\n  - $T = 3$\n  - $h = 0.5$\n  - Exact solution: $y(t) = \\frac{K}{1 + A e^{-r t}}$ with $A = \\frac{K - y_0}{y_0}$\n\n- **Test Case 4**:\n  - $f(t,y) = 0$\n  - $y_0 = 3$\n  - $T = 1$\n  - $h = 0.5$\n  - Exact solution: $y(t) = 3$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n1.  **Scientific or Factual Soundness**: The problem is built upon fundamental concepts in numerical analysis for ODEs. The forward Euler method, its error analysis, and Richardson extrapolation are standard and mathematically sound techniques. The ODEs provided are classic examples used in teaching and research. The problem is free of any scientific or factual errors.\n2.  **Well-Posed**: The problem is well-posed. For each test case, the function $f(t,y)$ is sufficiently smooth (Lipschitz continuous in $y$), which guarantees the existence and uniqueness of a solution to the IVP. The task is clearly defined, and all necessary data (initial conditions, parameters, time intervals) are provided.\n3.  **Objective**: The language is precise and unbiased. The tasks are quantitative and require specific calculations, leaving no room for subjective interpretation.\n4.  **Completeness**: The problem is self-contained. It specifies the ODEs, initial conditions, step sizes, final times, and the exact solutions for error comparison. The constraint that $(T - t_0)/h$ is an integer simplifies implementation and avoids ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution will be provided.\n\n### Principle-Based Solution\nThe solution is developed in three stages: first, the implementation of the forward Euler method; second, the derivation of the Richardson extrapolation formula; and third, the application of these methods to the specified test cases to compute the required errors.\n\n**1. The Forward Euler Method**\nThe forward Euler method is a first-order numerical procedure for solving an initial value problem of the form $y'(t) = f(t, y(t))$ with $y(t_0) = y_0$. It approximates the continuous solution $y(t)$ at discrete time points $t_n = t_0 + n h$, where $h$ is the step size. The method is derived from the first-order Taylor expansion of $y(t_{n+1})$ around $t_n$:\n$$y(t_{n+1}) = y(t_n) + h y'(t_n) + O(h^2) = y(t_n) + h f(t_n, y(t_n)) + O(h^2)$$\nBy ignoring the $O(h^2)$ term, we obtain the iterative formula for the approximation $y_n \\approx y(t_n)$:\n$$y_{n+1} = y_n + h f(t_n, y_n)$$\nStarting with the initial condition $y_0$, we can apply this formula iteratively for $n = 0, 1, 2, \\dots, N-1$ where $N = (T-t_0)/h$, to find an approximation $y_N \\approx y(T)$. This defines the function $y_h(T)$.\n\n**2. Richardson Extrapolation**\nRichardson extrapolation is a general technique for improving the accuracy of a numerical approximation. We are given that the approximation $y_h(T)$ from the forward Euler method has an asymptotic error expansion:\n$$y_h(T) = y(T) + C h + D h^2 + O(h^3)$$\nHere, $y(T)$ is the exact solution, and $C$ and $D$ are constants that depend on the function $f$ and its derivatives but not on the step size $h$.\n\nIf we compute the approximation again with a halved step size, $h/2$, the formula becomes:\n$$y_{h/2}(T) = y(T) + C \\left(\\frac{h}{2}\\right) + D \\left(\\frac{h}{2}\\right)^2 + O(h^3)$$\n$$y_{h/2}(T) = y(T) + \\frac{1}{2} C h + \\frac{1}{4} D h^2 + O(h^3)$$\nOur goal is to find a linear combination of $y_h(T)$ and $y_{h/2}(T)$, which we denote $y_{\\text{extrap}}(T)$, that provides a more accurate estimate of $y(T)$. Let $y_{\\text{extrap}}(T) = \\alpha y_h(T) + \\beta y_{h/2}(T)$. Substituting the error expansions:\n$$y_{\\text{extrap}}(T) = \\alpha \\left(y(T) + C h + D h^2\\right) + \\beta \\left(y(T) + \\frac{1}{2} C h + \\frac{1}{4} D h^2\\right) + O(h^3)$$\n$$y_{\\text{extrap}}(T) = (\\alpha + \\beta) y(T) + \\left(\\alpha + \\frac{\\beta}{2}\\right) C h + \\left(\\alpha + \\frac{\\beta}{4}\\right) D h^2 + O(h^3)$$\nTo obtain an $O(h^2)$-accurate estimate for $y(T)$, we require the coefficient of $y(T)$ to be $1$ and the coefficient of the leading error term, $Ch$, to be $0$. This gives a system of two linear equations for $\\alpha$ and $\\beta$:\n1. $\\alpha + \\beta = 1$\n2. $\\alpha + \\frac{\\beta}{2} = 0$\nFrom equation (2), we find $\\alpha = -\\beta/2$. Substituting this into equation (1) yields $-\\beta/2 + \\beta = 1$, which simplifies to $\\beta/2 = 1$, so $\\beta = 2$. Consequently, $\\alpha = -1$.\nThe extrapolated estimator is therefore:\n$$y_{\\text{extrap}}(T) = 2 y_{h/2}(T) - y_h(T)$$\nLet's verify the error of this new estimate:\n$$y_{\\text{extrap}}(T) - y(T) = (2 y_{h/2}(T) - y_h(T)) - y(T)$$\n$$= \\left(2\\left(y(T) + \\frac{1}{2}Ch + \\frac{1}{4}Dh^2\\right) - \\left(y(T) + Ch + Dh^2\\right)\\right) - y(T) + O(h^3)$$\n$$= (2y(T) + Ch + \\frac{1}{2}Dh^2) - y(T) - Ch - Dh^2 - y(T) + O(h^3)$$\n$$= (2-1-1)y(T) + (1-1)Ch + (\\frac{1}{2}-1)Dh^2 + O(h^3) = -\\frac{1}{2} D h^2 + O(h^3)$$\nThe error is indeed of order $h^2$, so the method successfully eliminates the leading error term.\n\n**3. Computational Procedure**\nFor each of the four test cases, the following algorithm is applied:\n1.  Define the function $f(t,y)$, initial conditions $y_0, t_0$, final time $T$, and step size $h$.\n2.  Implement a function `forward_euler(f, y0, t0, T, h)` that performs the iterative Euler updates and returns the final approximation at time $T$. The number of steps, $N$, is calculated as the integer `(T - t0) / h`.\n3.  Compute the approximation with the given step size $h$: $A_h = \\text{forward_euler}(f, y_0, t_0, T, h)$.\n4.  Compute the approximation with the halved step size $h/2$: $A_{h/2} = \\text{forward_euler}(f, y_0, t_0, T, h/2)$.\n5.  Calculate the Richardson extrapolated value: $y_{\\text{extrap}}(T) = 2 A_{h/2} - A_h$.\n6.  Calculate the exact solution $y(T)$ using the provided formula for the specific test case.\n7.  Compute the absolute error: $E = \\lvert y_{\\text{extrap}}(T) - y(T) \\rvert$.\n8.  The final result for the test case is this error, rounded to ten decimal places. This process is repeated for all test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes absolute errors for Richardson-extrapolated Euler method solutions\n    for a suite of ODE test cases.\n    \"\"\"\n\n    def forward_euler(f, y0, t0, T, h):\n        \"\"\"\n        Computes the solution of an IVP y'(t) = f(t, y) with y(t0) = y0 at time T\n        using the forward Euler method with step size h.\n        \"\"\"\n        t = t0\n        y = y0\n        \n        # The problem statement guarantees (T - t0) / h is an integer.\n        # Using int() directly is safe, but rounding is more robust for floats.\n        num_steps = int(round((T - t0) / h))\n\n        for _ in range(num_steps):\n            y = y + h * f(t, y)\n            t = t + h\n        \n        return y\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda t, y: y,\n            \"y0\": 1.0,\n            \"t0\": 0.0,\n            \"T\": 1.0,\n            \"h\": 0.2,\n            \"exact_sol\": lambda t: np.exp(t)\n        },\n        {\n            \"f\": lambda t, y: y + t,\n            \"y0\": 0.0,\n            \"t0\": 0.0,\n            \"T\": 2.0,\n            \"h\": 0.4,\n            \"exact_sol\": lambda t: np.exp(t) - t - 1.0\n        },\n        {\n            \"f\": lambda t, y: 1.0 * y * (1.0 - y / 10.0), # r=1, K=10\n            \"y0\": 1.0,\n            \"t0\": 0.0,\n            \"T\": 3.0,\n            \"h\": 0.5,\n            \"exact_sol\": lambda t: 10.0 / (1.0 + ((10.0 - 1.0) / 1.0) * np.exp(-1.0 * t))\n        },\n        {\n            \"f\": lambda t, y: 0.0,\n            \"y0\": 3.0,\n            \"t0\": 0.0,\n            \"T\": 1.0,\n            \"h\": 0.5,\n            \"exact_sol\": lambda t: 3.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        f = case[\"f\"]\n        y0 = case[\"y0\"]\n        t0 = case[\"t0\"]\n        T = case[\"T\"]\n        h = case[\"h\"]\n        exact_sol_func = case[\"exact_sol\"]\n\n        # 1. Compute approximations with step sizes h and h/2\n        y_h = forward_euler(f, y0, t0, T, h)\n        y_h_half = forward_euler(f, y0, t0, T, h / 2.0)\n\n        # 2. Apply Richardson extrapolation\n        y_extrap = 2.0 * y_h_half - y_h\n\n        # 3. Compute the exact solution\n        y_exact = exact_sol_func(T)\n        \n        # 4. Compute the absolute error\n        abs_error = abs(y_extrap - y_exact)\n        \n        results.append(abs_error)\n\n    # Format the results as strings rounded to 10 decimal places\n    # The f-string formatting ensures trailing zeros as in the example.\n    # The rounding prior to formatting correctly handles cases near the rounding boundary.\n    results_str = [f\"{round(res, 10):.10f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        }
    ]
}