{
    "hands_on_practices": [
        {
            "introduction": "The foundation of numerical differentiation lies in approximating derivatives using function values at discrete points. The primary tool for deriving these approximations and quantifying their accuracy is the Taylor series. This first exercise guides you through the fundamental process of deriving the forward, backward, and central difference formulas from first principles. By applying these formulas to the logistic function at its inflection point, you will gain a concrete understanding of truncation error and see how the local properties of a function can sometimes lead to surprisingly accurate results even with simple methods .",
            "id": "3132346",
            "problem": "You are given the scalar function $f(x) = \\dfrac{1}{1 + e^{-x}}$, which is smooth for all real $x$ and has an inflection point at $x = 0$. Your task is to estimate the slope $f'(0)$ using discrete difference quotients constructed on a uniform grid with spacing $h > 0$ and to quantify the bias of these estimates when the grid is coarse.\n\nUse the following as the fundamental base for your derivations and algorithm design:\n- The definition of the derivative of a smooth function, $f'(x) = \\lim_{h \\to 0} \\dfrac{f(x+h) - f(x)}{h}$.\n- The Taylor series expansion of a smooth function around a point, for example $f(x \\pm h)$ expanded about $x$.\n- Elementary rules of differentiation for the exponential function.\n\nDo not assume any difference formula as given. Instead, proceed from the principles above.\n\nTasks:\n1. Starting from Taylor series expansions about a point $x_0$, derive the leading-order truncation error for the three one-dimensional difference-quotient estimators of the derivative at $x_0$ commonly called the forward, backward, and central schemes. Express the truncation errors in terms of $f''(x_0)$, $f'''(x_0)$, and integer powers of $h$.\n2. Specialize your results to the logistic function $f(x) = \\dfrac{1}{1 + e^{-x}}$ at its inflection point $x_0 = 0$. Compute $f'(0)$ exactly, and also compute $f''(0)$ and $f'''(0)$ to determine the leading-order truncation error for the central scheme at $x_0 = 0$.\n3. Implement a program that, for each step size $h$ in the test suite $\\{2, 1, 0.5, 0.25\\}$, does all of the following at $x_0 = 0$:\n   - Computes the three numerical slope estimates using the forward, backward, and central schemes.\n   - Computes the empirical bias of each estimate as $\\text{bias} = \\text{estimate} - f'(0)$.\n   - Computes the predicted leading-order bias for the central scheme using your derived leading-order truncation term specialized to $x_0 = 0$.\n4. Final output format: Your program should produce a single line of output containing the results as a comma-separated list of lists, with no spaces, enclosed in square brackets. For each step size $h$, output the list $[h,\\text{bias}_{\\text{fwd}},\\text{bias}_{\\text{bwd}},\\text{bias}_{\\text{cen}},\\text{bias}_{\\text{cen,pred}}]$, where all entries are decimal numbers. The overall output is thus a single list of four such lists, in the order $h \\in \\{2,1,0.5,0.25\\}$.\n\nNotes:\n- There are no physical units in this problem; all quantities are dimensionless.\n- No angles are involved.\n- The final output must be exactly one line, formatted as a single list of lists as described above, for the specified test suite values only.",
            "solution": "The problem is valid as it is self-contained, scientifically grounded in the principles of calculus and numerical analysis, and unambiguously stated. We are asked to derive numerical differentiation formulas from first principles, analyze their truncation error, and apply this analysis to a specific function and point.\n\n### Part 1: Derivation of Difference Schemes and Truncation Errors\n\nWe begin with the Taylor series expansion of a smooth function $f(x)$ around a point $x_0$. For a step size $h > 0$, the expansions for $f(x_0+h)$ and $f(x_0-h)$ are:\n$$f(x_0+h) = f(x_0) + hf'(x_0) + \\frac{h^2}{2!}f''(x_0) + \\frac{h^3}{3!}f'''(x_0) + O(h^4)$$\n$$f(x_0-h) = f(x_0) - hf'(x_0) + \\frac{h^2}{2!}f''(x_0) - \\frac{h^3}{3!}f'''(x_0) + O(h^4)$$\n\nThe bias of an estimator is defined as $\\text{bias} = \\text{estimator} - f'(x_0)$. We seek the leading-order term of this bias, which is the leading-order truncation error.\n\n**Forward Difference Scheme**\nThe forward difference quotient is defined as $\\frac{f(x_0+h) - f(x_0)}{h}$. Rearranging the Taylor expansion for $f(x_0+h)$:\n$$f(x_0+h) - f(x_0) = hf'(x_0) + \\frac{h^2}{2}f''(x_0) + O(h^3)$$\nDividing by $h$:\n$$\\frac{f(x_0+h) - f(x_0)}{h} = f'(x_0) + \\frac{h}{2}f''(x_0) + O(h^2)$$\nThe bias is therefore:\n$$\\text{bias}_{\\text{fwd}} = \\left(\\frac{f(x_0+h) - f(x_0)}{h}\\right) - f'(x_0) = \\frac{h}{2}f''(x_0) + O(h^2)$$\nThe leading-order truncation error is $\\frac{h}{2}f''(x_0)$.\n\n**Backward Difference Scheme**\nThe backward difference quotient is $\\frac{f(x_0) - f(x_0-h)}{h}$. Rearranging the Taylor expansion for $f(x_0-h)$:\n$$f(x_0) - f(x_0-h) = hf'(x_0) - \\frac{h^2}{2}f''(x_0) + O(h^3)$$\nDividing by $h$:\n$$\\frac{f(x_0) - f(x_0-h)}{h} = f'(x_0) - \\frac{h}{2}f''(x_0) + O(h^2)$$\nThe bias is:\n$$\\text{bias}_{\\text{bwd}} = \\left(\\frac{f(x_0) - f(x_0-h)}{h}\\right) - f'(x_0) = -\\frac{h}{2}f''(x_0) + O(h^2)$$\nThe leading-order truncation error is $-\\frac{h}{2}f''(x_0)$.\n\n**Central Difference Scheme**\nTo derive the central difference scheme, we subtract the expansion for $f(x_0-h)$ from the expansion for $f(x_0+h)$:\n$$f(x_0+h) - f(x_0-h) = (f(x_0) + hf'(x_0) + \\frac{h^2}{2}f''(x_0) + \\frac{h^3}{6}f'''(x_0)) - (f(x_0) - hf'(x_0) + \\frac{h^2}{2}f''(x_0) - \\frac{h^3}{6}f'''(x_0)) + O(h^5)$$\nTerms with even powers of $h$ cancel:\n$$f(x_0+h) - f(x_0-h) = 2hf'(x_0) + \\frac{2h^3}{6}f'''(x_0) + O(h^5) = 2hf'(x_0) + \\frac{h^3}{3}f'''(x_0) + O(h^5)$$\nRearranging for the estimator $\\frac{f(x_0+h) - f(x_0-h)}{2h}$:\n$$\\frac{f(x_0+h) - f(x_0-h)}{2h} = f'(x_0) + \\frac{h^2}{6}f'''(x_0) + O(h^4)$$\nThe bias is:\n$$\\text{bias}_{\\text{cen}} = \\left(\\frac{f(x_0+h) - f(x_0-h)}{2h}\\right) - f'(x_0) = \\frac{h^2}{6}f'''(x_0) + O(h^4)$$\nThe leading-order truncation error is $\\frac{h^2}{6}f'''(x_0)$.\n\n### Part 2: Specialization to the Logistic Function\n\nWe are given the function $f(x) = \\frac{1}{1 + e^{-x}}$ and the point of interest $x_0 = 0$. We must compute the first three derivatives and evaluate them at $x_0 = 0$.\n\nA computationally efficient way to find the derivatives is to use the property $f'(x) = f(x)(1-f(x))$.\n$f'(x) = \\frac{d}{dx}(1+e^{-x})^{-1} = -(1+e^{-x})^{-2}(-e^{-x}) = \\frac{e^{-x}}{(1+e^{-x})^2}$.\n$f(x)(1-f(x)) = \\frac{1}{1+e^{-x}}\\left(1 - \\frac{1}{1+e^{-x}}\\right) = \\frac{1}{1+e^{-x}}\\frac{e^{-x}}{1+e^{-x}} = \\frac{e^{-x}}{(1+e^{-x})^2}$. The property holds.\n\nNow we compute higher derivatives:\n$f''(x) = \\frac{d}{dx}[f'(x)] = \\frac{d}{dx}[f(x)(1-f(x))] = f'(x)(1-f(x)) + f(x)(-f'(x)) = f'(x)(1-2f(x))$.\n$f'''(x) = \\frac{d}{dx}[f''(x)] = \\frac{d}{dx}[f'(x)(1-2f(x))] = f''(x)(1-2f(x)) + f'(x)(-2f'(x)) = f''(x)(1-2f(x)) - 2(f'(x))^2$.\n\nNext, we evaluate these at $x_0 = 0$:\nAt $x_0=0$, $e^{-0}=1$, so $f(0) = \\frac{1}{1+1} = \\frac{1}{2}$.\n$f'(0) = f(0)(1-f(0)) = \\frac{1}{2}(1-\\frac{1}{2}) = \\frac{1}{4}$. This is the exact value of the slope.\n$f''(0) = f'(0)(1-2f(0)) = \\frac{1}{4}(1-2(\\frac{1}{2})) = \\frac{1}{4}(0) = 0$. This confirms that $x_0 = 0$ is an inflection point, as stated.\n$f'''(0) = f''(0)(1-2f(0)) - 2(f'(0))^2 = (0)(1-2(\\frac{1}{2})) - 2(\\frac{1}{4})^2 = 0 - 2(\\frac{1}{16}) = -\\frac{1}{8}$.\n\nThe problem states that $x_0=0$ is an inflection point, which means the leading-order $O(h)$ error terms for the forward and backward schemes, which depend on $f''(x_0)$, vanish. Their accuracy becomes $O(h^2)$ at this specific point. The leading-order truncation error for the central scheme is determined by $f'''(0)$.\n\nThe predicted leading-order bias for the central scheme is given by the leading term in its error expansion:\n$$\\text{bias}_{\\text{cen,pred}} = \\frac{h^2}{6}f'''(0) = \\frac{h^2}{6}\\left(-\\frac{1}{8}\\right) = -\\frac{h^2}{48}$$\n\n### Part 3: Algorithmic Implementation\n\nThe program will implement the following logic:\n1. Define the logistic function $f(x)$.\n2. Set the exact derivative $f'(0) = 0.25$ and $f'''(0) = -0.125$.\n3. For each step size $h$ in the set $\\{2, 1, 0.5, 0.25\\}$:\n   a. Compute the three numerical estimates at $x_0=0$:\n      - Forward: $\\text{est}_{\\text{fwd}} = \\frac{f(h) - f(0)}{h}$\n      - Backward: $\\text{est}_{\\text{bwd}} = \\frac{f(0) - f(-h)}{h}$\n      - Central: $\\text{est}_{\\text{cen}} = \\frac{f(h) - f(-h)}{2h}$\n   b. Compute the empirical bias for each as $\\text{bias} = \\text{estimate} - 0.25$.\n   c. Compute the predicted leading-order bias for the central scheme using the derived formula: $\\text{bias}_{\\text{cen,pred}} = -\\frac{h^2}{48}$.\n   d. Store the results $[h, \\text{bias}_{\\text{fwd}}, \\text{bias}_{\\text{bwd}}, \\text{bias}_{\\text{cen}}, \\text{bias}_{\\text{cen,pred}}]$ for the current $h$.\n4. Format the collected results into a single string representing a list of lists and print it.\n\nIt is worth noting that for the function $f(x)=(1+e^{-x})^{-1}$, we have the symmetry property $f(x)+f(-x)=1$. At the evaluation point $x_0=0$, where $f(0)=0.5$, this leads to the forward, backward, and central difference quotients being numerically identical. Consequently, their empirical biases will also be identical.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical differentiation problem for the logistic function.\n\n    This function performs the following tasks:\n    1. Defines the logistic function and its exact derivative details at x=0.\n    2. Iterates through a set of step sizes h.\n    3. For each h, calculates the forward, backward, and central difference estimates\n       of the derivative at x=0.\n    4. Computes the empirical bias for each estimate.\n    5. Calculates the theoretically predicted leading-order bias for the central\n       difference scheme.\n    6. Formats and prints the results as a list of lists.\n    \"\"\"\n    \n    # Define the scalar function f(x)\n    def f(x: float) -> float:\n        return 1.0 / (1.0 + np.exp(-x))\n\n    # Test suite of step sizes\n    h_values = [2.0, 1.0, 0.5, 0.25]\n    \n    # Point of evaluation\n    x0 = 0.0\n    \n    # Analytically derived constants\n    f_prime_exact_at_0 = 0.25  # f'(0) = 1/4\n    f_triple_prime_at_0 = -0.125  # f'''(0) = -1/8\n    \n    all_results = []\n\n    for h in h_values:\n        # Evaluate the function at the required points on the grid\n        f_plus_h = f(x0 + h)\n        f_minus_h = f(x0 - h)\n        f_at_x0 = f(x0)\n\n        # Compute the three numerical slope estimates\n        est_fwd = (f_plus_h - f_at_x0) / h\n        est_bwd = (f_at_x0 - f_minus_h) / h\n        est_cen = (f_plus_h - f_minus_h) / (2.0 * h)\n        \n        # Compute the empirical bias for each estimate\n        bias_fwd = est_fwd - f_prime_exact_at_0\n        bias_bwd = est_bwd - f_prime_exact_at_0\n        bias_cen = est_cen - f_prime_exact_at_0\n        \n        # Compute the predicted leading-order bias for the central scheme\n        # The formula is (h^2 / 6) * f'''(0)\n        bias_cen_pred = (h**2 / 6.0) * f_triple_prime_at_0\n        \n        # Append the list of results for this h\n        all_results.append([h, bias_fwd, bias_bwd, bias_cen, bias_cen_pred])\n        \n    # Format the final output string as a list of lists without spaces\n    # Example: [[2.0,-0.0596...,-0.0596...,-0.0596...,-0.0833...],...]\n    output_str = f\"[{','.join([f'[{v[0]},{v[1]},{v[2]},{v[3]},{v[4]}]' for v in all_results])}]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond theoretical accuracy, a crucial aspect of any numerical algorithm is its robustness to imperfect data. Real-world measurements often contain noise or outliers, and our methods must be able to handle them gracefully. This practice shifts our focus from truncation error to the practical impact of data corruption on our derivative estimates. By analyzing how a single bad data point affects each of the three basic difference schemes, you will discover a critical relationship between a formula's stencil—the pattern of points it uses—and its sensitivity to localized errors . This reveals that the central difference formula possesses a remarkable robustness that is not apparent from its truncation error alone.",
            "id": "3132362",
            "problem": "You are given a smooth real-valued function $f$ sampled on a one-dimensional uniform grid $x_j = x_0 + j h$ with constant spacing $h > 0$. Consider three first-derivative finite difference schemes evaluated at an interior grid point $x_i$: the forward difference, the backward difference, and the central difference. Assume that, in the measured data, exactly one sample is corrupted by an additive outlier of magnitude $a$ at $x_i$, while all other samples remain accurate; that is, the measured value at $x_i$ equals the true value plus $a$, and the measured value at any $x_j$ with $j \\neq i$ equals the true value. Model this corruption as an additive perturbation using the Kronecker delta $\\delta_{ij}$, where $\\delta_{ij} = 1$ if $i=j$ and $\\delta_{ij}=0$ otherwise.\n\nUsing only the following foundational bases:\n- The definition of the derivative as a limit of a difference quotient, and\n- The idea that finite difference schemes approximate the derivative by replacing limits with finite differences of sampled data on a uniform grid,\n\nderive, for an interior grid point $x_i$, explicit expressions for how the single-point corruption of magnitude $a$ at $x_i$ changes each of the three derivative approximations at $x_i$ relative to the uncorrupted case. Then give the absolute magnitude of this change (the sensitivity) as a function of $a$ and $h$, assuming $h>0$ and that the central difference is defined (that is, $i$ is not a boundary index).\n\nImplement a program that computes, for each test case $(h, a)$ in the test suite below, a list of three real numbers containing the absolute changes (sensitivities) for the three schemes at $x_i$ in the order: forward, backward, central. Your program must not assume any specific formula beyond what is derivable from the above foundational bases, and it must produce the results for the test suite exactly as specified.\n\nTest suite (each case is $(h, a)$):\n- Case 1: $h = 0.1$, $a = 0.01$\n- Case 2: $h = 0.1$, $a = 0$\n- Case 3: $h = 10^{-6}$, $a = 10^{-3}$\n- Case 4: $h = 2.5$, $a = -0.5$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of sublists, each sublist corresponding to one test case and containing three floating-point numbers in the order [forward, backward, central]. The entire sequence must be enclosed in square brackets. For example, the output format must look like $[[v_{11},v_{12},v_{13}],[v_{21},v_{22},v_{23}],\\dots]$ with no additional text before or after. No physical units are involved, and all angles, if any, are in radians by default (though no angles are used directly in this task).",
            "solution": "The problem requires the derivation of the sensitivity of three finite difference approximations for the first derivative, $f'(x_i)$, to a single-point additive error at the point of evaluation, $x_i$. The derivation must be based on first principles: the limit definition of the derivative and its approximation by finite differences on a uniform grid.\n\nLet the true, smooth function be $f(x)$. Its values are sampled on a uniform grid $x_j = x_0 + j h$ for integer $j$ and constant spacing $h > 0$. The sampled values are denoted by $f_j = f(x_j)$. The measured data, $\\tilde{f}_j$, are corrupted by a single additive outlier of magnitude $a$ at the grid point $x_i$. This corruption is modeled as $\\tilde{f}_j = f_j + a \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta. This implies that $\\tilde{f}_i = f_i + a$, while for any other grid point $j \\neq i$, the measured value is uncorrupted, i.e., $\\tilde{f}_j = f_j$.\n\nWe aim to find the change in a derivative approximation, which we denote generically as $D$. This change, $\\Delta D$, is the difference between the approximation computed with the corrupted data and the one computed with the true data: $\\Delta D = D(\\{\\tilde{f}_k\\}) - D(\\{f_k\\})$. The sensitivity, $S$, is defined as the absolute magnitude of this change, $S = |\\Delta D|$.\n\nAll finite difference schemes for the first derivative $f'(x)$ are approximations of the limit definition $f'(x) = \\lim_{\\Delta x \\to 0} \\frac{f(x+\\Delta x) - f(x)}{\\Delta x}$. The schemes arise by replacing the limit with a finite difference over the grid spacing $h$. We will now analyze each of the three schemes at the interior grid point $x_i$.\n\n1.  **Forward Difference Scheme**\n\nThe forward difference approximation, $D_+f(x_i)$, is obtained by considering a forward step of size $h$. Its formula is:\n$$D_+f(x_i) = \\frac{f(x_i+h) - f(x_i)}{h} = \\frac{f_{i+1} - f_i}{h}$$\nUsing the true, uncorrupted function values, the approximation is $D_+f(x_i)_{\\text{true}} = \\frac{f_{i+1} - f_i}{h}$.\nUsing the measured, corrupted values, the approximation is $\\tilde{D}_+f(x_i) = \\frac{\\tilde{f}_{i+1} - \\tilde{f}_i}{h}$.\nFrom our error model, we substitute $\\tilde{f}_{i+1} = f_{i+1}$ (since $i+1 \\neq i$) and $\\tilde{f}_i = f_i + a$:\n$$\\tilde{D}_+f(x_i) = \\frac{f_{i+1} - (f_i + a)}{h} = \\frac{f_{i+1} - f_i}{h} - \\frac{a}{h} = D_+f(x_i)_{\\text{true}} - \\frac{a}{h}$$\nThe change in the forward difference approximation is $\\Delta D_+f(x_i) = \\tilde{D}_+f(x_i) - D_+f(x_i)_{\\text{true}} = -\\frac{a}{h}$.\nThe sensitivity is the absolute magnitude of this change:\n$$S_+ = \\left| -\\frac{a}{h} \\right| = \\frac{|a|}{h}$$\n\n2.  **Backward Difference Scheme**\n\nThe backward difference approximation, $D_-f(x_i)$, uses a backward step of size $h$. Its formula is derived from the alternative limit form $f'(x) = \\lim_{h \\to 0} \\frac{f(x) - f(x-h)}{h}$:\n$$D_-f(x_i) = \\frac{f(x_i) - f(x_i-h)}{h} = \\frac{f_i - f_{i-1}}{h}$$\nThe true approximation is $D_-f(x_i)_{\\text{true}} = \\frac{f_i - f_{i-1}}{h}$.\nThe approximation with corrupted data is $\\tilde{D}_-f(x_i) = \\frac{\\tilde{f}_i - \\tilde{f}_{i-1}}{h}$.\nSubstituting $\\tilde{f}_i = f_i + a$ and $\\tilde{f}_{i-1} = f_{i-1}$ (since $i-1 \\neq i$):\n$$\\tilde{D}_-f(x_i) = \\frac{(f_i + a) - f_{i-1}}{h} = \\frac{f_i - f_{i-1}}{h} + \\frac{a}{h} = D_-f(x_i)_{\\text{true}} + \\frac{a}{h}$$\nThe change is $\\Delta D_-f(x_i) = \\tilde{D}_-f(x_i) - D_-f(x_i)_{\\text{true}} = \\frac{a}{h}$.\nThe sensitivity is:\n$$S_- = \\left| \\frac{a}{h} \\right| = \\frac{|a|}{h}$$\n\n3.  **Central Difference Scheme**\nThe central difference approximation, $D_0f(x_i)$, uses a symmetric interval of width $2h$ around $x_i$, giving a more balanced approximation. The problem states $x_i$ is an interior point, so its neighbors $x_{i-1}$ and $x_{i+1}$ exist. The formula is:\n$$D_0f(x_i) = \\frac{f(x_i+h) - f(x_i-h)}{2h} = \\frac{f_{i+1} - f_{i-1}}{2h}$$\nThe true approximation is $D_0f(x_i)_{\\text{true}} = \\frac{f_{i+1} - f_{i-1}}{2h}$.\nFor the corrupted data, the approximation is $\\tilde{D}_0f(x_i) = \\frac{\\tilde{f}_{i+1} - \\tilde{f}_{i-1}}{2h}$.\nCritically, the formula for the derivative at $x_i$ uses values from $x_{i+1}$ and $x_{i-1}$. The corruption is localized at $x_i$. Therefore, the values used by the scheme are uncorrupted: $\\tilde{f}_{i+1} = f_{i+1}$ and $\\tilde{f}_{i-1} = f_{i-1}$. The perturbed value $\\tilde{f}_i$ does not appear in the stencil for the central difference evaluated at $x_i$.\nThus, the approximation is unchanged:\n$$\\tilde{D}_0f(x_i) = \\frac{f_{i+1} - f_{i-1}}{2h} = D_0f(x_i)_{\\text{true}}$$\nThe change is $\\Delta D_0f(x_i) = 0$.\nThe sensitivity is consequently zero:\n$$S_0 = |0| = 0$$\n\nTo summarize, for a single-point corruption of magnitude $a$ at grid point $x_i$, the sensitivities of the three derivative approximations evaluated at $x_i$ are:\n-   Forward Difference Sensitivity: $S_+ = \\frac{|a|}{h}$\n-   Backward Difference Sensitivity: $S_- = \\frac{|a|}{h}$\n-   Central Difference Sensitivity: $S_0 = 0$\n\nThese results will be implemented in the program to compute the numerical values for the given test cases. The forward and backward schemes exhibit sensitivity that is amplified as the grid spacing $h$ decreases, a hallmark of ill-conditioning for numerical differentiation. In contrast, the central difference scheme is perfectly robust to an outlier at the point of evaluation itself, due to the structure of its computational stencil.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the sensitivity of forward, backward, and central difference\n    schemes to a single-point outlier.\n    \"\"\"\n    \n    # Test suite of (h, a) pairs, where h is grid spacing and a is outlier magnitude.\n    test_cases = [\n        (0.1, 0.01),\n        (0.1, 0.0),\n        (1e-6, 1e-3),\n        (2.5, -0.5),\n    ]\n\n    all_results = []\n    for h, a in test_cases:\n        # Based on the derivation, the sensitivities (absolute changes) are:\n        # S_forward = |a| / h\n        # S_backward = |a| / h\n        # S_central = 0\n        \n        # The problem statement guarantees h > 0, so no division-by-zero check is needed.\n        \n        # Sensitivity of the forward difference scheme\n        sensitivity_forward = np.abs(a) / h\n        \n        # Sensitivity of the backward difference scheme\n        sensitivity_backward = np.abs(a) / h\n        \n        # Sensitivity of the central difference scheme\n        sensitivity_central = 0.0\n        \n        case_results = [sensitivity_forward, sensitivity_backward, sensitivity_central]\n        all_results.append(case_results)\n\n    # The required output is a single line string representation of a list of lists,\n    # with no spaces after commas. We construct this string manually to ensure\n    # exact formatting.\n    # Ex: [[v11,v12,v13],[v21,v22,v23]]\n    sublist_strs = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_output_str = f\"[{','.join(sublist_strs)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The convergence rates we derive, such as $\\mathcal{O}(h)$ or $\\mathcal{O}(h^2)$, are not universal laws; they depend on certain assumptions about the function being differentiated, namely its \"smoothness.\" This final practice challenges you to explore what happens when these assumptions break down. We will investigate the performance of our finite difference formulas when applied to a function, $f(x) = x^{\\alpha}$, that is not infinitely differentiable at the origin. By comparing the convergence rate at a smooth point to the rate near the singularity, you will see firsthand how the theoretical order of accuracy can degrade, emphasizing the importance of understanding the mathematical foundations upon which these numerical methods are built .",
            "id": "3132413",
            "problem": "Consider the function $f(x) = x^{\\alpha}$ defined for $x \\ge 0$ with parameter $0 < \\alpha < 1$. For a point $x > 0$ and a step size $h > 0$, define the forward, backward, and central difference formulas for estimating the derivative $f'(x)$ as follows:\n- Forward difference: $D^{+} f(x;h) = \\dfrac{f(x+h) - f(x)}{h}$.\n- Backward difference: $D^{-} f(x;h) = \\dfrac{f(x) - f(x-h)}{h}$, which requires $x - h \\ge 0$.\n- Central difference: $D^{0} f(x;h) = \\dfrac{f(x+h) - f(x-h)}{2h}$, which requires $x - h \\ge 0$.\n\nYour task is to:\n1. Starting only from the definition of the derivative and Taylor’s theorem with remainder for sufficiently smooth functions on $(0,\\infty)$, reason about the expected behavior of these formulas for $f(x) = x^{\\alpha}$ near the singular point $x=0$. In particular, address both the case of a fixed evaluation point $x_0 > 0$ and the case where the evaluation point approaches the singularity proportionally to the step size.\n2. Implement a program that, for prescribed parameter sets, computes the observed convergence rate by measuring the slope $p$ of $\\log(E(h))$ versus $\\log(h)$, where $E(h)$ is the absolute error of a given difference formula compared against the exact derivative $f'(x)$ at the evaluation point. Use the natural logarithm.\n\nUse the exact derivative $f'(x) = \\alpha x^{\\alpha - 1}$ for $x > 0$. All angle units are irrelevant for this task. No physical units are involved.\n\nTest suite specification:\n- Scenario $\\mathrm{A}$ (fixed point): Let $x_0 = 10^{-3}$ and $\\alpha \\in \\left\\{\\tfrac{1}{4}, \\tfrac{1}{2}, \\tfrac{3}{4}\\right\\}$. For each $\\alpha$, use step sizes $h_k = \\dfrac{x_0}{2^{k}}$ for $k \\in \\{3,4,5,6,7,8\\}$. For each $h_k$, compute the absolute errors $E^{+}(h_k) = \\left|D^{+} f(x_0;h_k) - f'(x_0)\\right|$, $E^{-}(h_k) = \\left|D^{-} f(x_0;h_k) - f'(x_0)\\right|$, and $E^{0}(h_k) = \\left|D^{0} f(x_0;h_k) - f'(x_0)\\right|$. Then estimate slopes $p_{\\mathrm{A}}^{+}$, $p_{\\mathrm{A}}^{-}$, and $p_{\\mathrm{A}}^{0}$ by linear least squares fitting of $\\log(E(h_k))$ versus $\\log(h_k)$.\n- Scenario $\\mathrm{B}$ (coalescing evaluation point): Let $c = 2$ and $\\alpha \\in \\left\\{\\tfrac{1}{4}, \\tfrac{1}{2}, \\tfrac{3}{4}\\right\\}$. For each $\\alpha$, use step sizes $h_k = \\dfrac{10^{-2}}{2^{k}}$ for $k \\in \\{0,1,2,3,4,5\\}$. At each $h_k$, set the evaluation point to $x(h_k) = c\\,h_k$. For each $h_k$, compute the absolute errors $E^{+}(h_k) = \\left|D^{+} f(x(h_k);h_k) - f'(x(h_k))\\right|$, $E^{-}(h_k) = \\left|D^{-} f(x(h_k);h_k) - f'(x(h_k))\\right|$, and $E^{0}(h_k) = \\left|D^{0} f(x(h_k);h_k) - f'(x(h_k))\\right|$. Then estimate slopes $p_{\\mathrm{B}}^{+}$, $p_{\\mathrm{B}}^{-}$, and $p_{\\mathrm{B}}^{0}$ by linear least squares fitting of $\\log(E(h_k))$ versus $\\log(h_k)$.\n\nFinal output format:\n- Your program must produce a single line containing a flat list of $18$ floating-point numbers, rounded to three decimal places, enclosed in square brackets and separated by commas. For each $\\alpha$ in ascending order $\\left\\{\\tfrac{1}{4}, \\tfrac{1}{2}, \\tfrac{3}{4}\\right\\}$, append the six values in the exact order\n$$\\left[p_{\\mathrm{A}}^{+},\\; p_{\\mathrm{A}}^{-},\\; p_{\\mathrm{A}}^{0},\\; p_{\\mathrm{B}}^{+},\\; p_{\\mathrm{B}}^{-},\\; p_{\\mathrm{B}}^{0}\\right],$$\nso that the final list contains all values for $\\alpha=\\tfrac{1}{4}$, then for $\\alpha=\\tfrac{1}{2}$, and finally for $\\alpha=\\tfrac{3}{4}$.",
            "solution": "The problem requires an analysis of the convergence properties of forward, backward, and central finite difference formulas for approximating the derivative of the function $f(x) = x^{\\alpha}$ where $0 < \\alpha < 1$. This function is smooth for $x > 0$ but has a singularity at $x=0$, as its derivatives are unbounded when $x \\to 0$. We will first derive the general error terms for these formulas using Taylor's theorem and then apply the analysis to the two specified scenarios.\n\nGeneral Truncation Error Analysis\nLet $f(x)$ be a function that is sufficiently smooth in a neighborhood of a point $x > 0$. We use Taylor's theorem with a Lagrange remainder to analyze the truncation error of each finite difference formula.\n\n1.  Forward Difference ($D^{+}$):\n    By Taylor's theorem, expanding $f(x+h)$ around $x$, we have:\n    $$f(x+h) = f(x) + f'(x)h + \\frac{f''(\\xi_{1})}{2!}h^2$$\n    for some $\\xi_{1} \\in (x, x+h)$. Rearranging this equation for the forward difference formula $D^{+}f(x;h) = \\frac{f(x+h) - f(x)}{h}$ gives:\n    $$D^{+}f(x;h) = f'(x) + \\frac{f''(\\xi_{1})}{2}h$$\n    The truncation error is $E^{+}(h) = |D^{+}f(x;h) - f'(x)| = \\left|\\frac{f''(\\xi_{1})}{2}\\right|h$. If $f''(x)$ is continuous, then as $h \\to 0$, $\\xi_{1} \\to x$, and the error is $E^{+}(h) \\approx \\left|\\frac{f''(x)}{2}\\right|h$. This shows that the forward difference formula is first-order accurate, i.e., $E^{+}(h) = O(h)$. The slope $p$ of $\\log(E)$ versus $\\log(h)$ is expected to be $1$.\n\n2.  Backward Difference ($D^{-}$):\n    Similarly, expanding $f(x-h)$ around $x$:\n    $$f(x-h) = f(x) - f'(x)h + \\frac{f''(\\xi_{2})}{2!}h^2$$\n    for some $\\xi_{2} \\in (x-h, x)$. Rearranging for the backward difference formula $D^{-}f(x;h) = \\frac{f(x) - f(x-h)}{h}$:\n    $$D^{-}f(x;h) = f'(x) - \\frac{f''(\\xi_{2})}{2}h$$\n    The truncation error is $E^{-}(h) = |D^{-}f(x;h) - f'(x)| = \\left|\\frac{f''(\\xi_{2})}{2}\\right|h$. Assuming continuity of $f''(x)$, the method is first-order accurate, $E^{-}(h) = O(h)$, and the slope $p$ is expected to be $1$.\n\n3.  Central Difference ($D^{0}$):\n    For the central difference, we need higher-order expansions:\n    $$f(x+h) = f(x) + f'(x)h + \\frac{f''(x)}{2}h^2 + \\frac{f'''(\\xi_{3})}{6}h^3, \\quad \\xi_{3} \\in (x, x+h)$$\n    $$f(x-h) = f(x) - f'(x)h + \\frac{f''(x)}{2}h^2 - \\frac{f'''(\\xi_{4})}{6}h^3, \\quad \\xi_{4} \\in (x-h, x)$$\n    Subtracting the second from the first gives:\n    $$f(x+h) - f(x-h) = 2f'(x)h + \\frac{h^3}{6}(f'''(\\xi_{3}) + f'''(\\xi_{4}))$$\n    Rearranging for the central difference formula $D^{0}f(x;h) = \\frac{f(x+h) - f(x-h)}{2h}$:\n    $$D^{0}f(x;h) = f'(x) + \\frac{h^2}{12}(f'''(\\xi_{3}) + f'''(\\xi_{4}))$$\n    The truncation error is $E^{0}(h) = |D^{0}f(x;h) - f'(x)| = \\left|\\frac{f'''(\\xi_{3}) + f'''(\\xi_{4})}{12}\\right|h^2$. If $f'''(x)$ is continuous, as $h \\to 0$, both $\\xi_{3}, \\xi_{4} \\to x$, and the error is $E^{0}(h) \\approx \\left|\\frac{f'''(x)}{6}\\right|h^2$. This shows the central difference is second-order accurate, $E^{0}(h) = O(h^2)$, and the slope $p$ is expected to be $2$.\n\nApplication to $f(x) = x^{\\alpha}$\nThe function is $f(x) = x^{\\alpha}$ with derivatives $f'(x) = \\alpha x^{\\alpha-1}$, $f''(x) = \\alpha(\\alpha-1)x^{\\alpha-2}$, and $f'''(x) = \\alpha(\\alpha-1)(\\alpha-2)x^{\\alpha-3}$. For $0 < \\alpha < 1$, these derivatives are unbounded as $x \\to 0^{+}$.\n\nScenario A: Fixed Point Evaluation ($x_0 = 10^{-3}$)\nIn this scenario, the derivative is evaluated at a fixed point $x_0 = 10^{-3} > 0$. For any such fixed point, the function $f(x)$ is $C^{\\infty}$ in a neighborhood around $x_0$. The step sizes $h_k = x_0/2^k$ for $k \\ge 3$ are small enough that the stencils $[x_0-h_k, x_0+h_k]$ are contained within $(0, \\infty)$. Therefore, the general analysis holds.\n- For $D^{+}$ and $D^{-}$, the error is $O(h)$, so we expect the measured slope to be $p_{\\mathrm{A}}^{+} \\approx 1$ and $p_{\\mathrm{A}}^{-} \\approx 1$.\n- For $D^{0}$, the error is $O(h^2)$, so we expect the measured slope to be $p_{\\mathrm{A}}^{0} \\approx 2$.\nThese predictions hold for all given values of $\\alpha \\in \\left\\{\\tfrac{1}{4}, \\tfrac{1}{2}, \\tfrac{3}{4}\\right\\}$.\n\nScenario B: Coalescing Evaluation Point ($x(h) = ch$)\nIn this scenario, the evaluation point $x(h) = ch$ (with $c=2$) approaches the singularity at $x=0$ as $h \\to 0$. The assumption from the general analysis that higher derivatives are approximately constant over the stencil $[x-h, x+h]$ is violated. For example, $f''(x)$ varies significantly over the interval $[(c-1)h, (c+1)h]$. We must perform a more direct analysis.\n\nLet $x = ch$. The error is the absolute difference between the numerical approximation and the exact derivative $f'(ch) = \\alpha(ch)^{\\alpha-1}$.\n1.  Forward Difference Error:\n    $$D^{+}f(ch;h) = \\frac{f((c+1)h) - f(ch)}{h} = \\frac{((c+1)h)^{\\alpha} - (ch)^{\\alpha}}{h} = ((c+1)^{\\alpha} - c^{\\alpha})h^{\\alpha-1}$$\n    The error is:\n    $$E^{+}(h) = |D^{+}f(ch;h) - f'(ch)| = |((c+1)^{\\alpha} - c^{\\alpha})h^{\\alpha-1} - \\alpha c^{\\alpha-1}h^{\\alpha-1}| = |(c+1)^{\\alpha} - c^{\\alpha} - \\alpha c^{\\alpha-1}| h^{\\alpha-1}$$\n    The error is of the form $Ch^{\\alpha-1}$. The slope of $\\log(E)$ vs $\\log(h)$ is $p_{\\mathrm{B}}^{+} = \\alpha-1$.\n\n2.  Backward Difference Error:\n    $$D^{-}f(ch;h) = \\frac{f(ch) - f((c-1)h)}{h} = \\frac{(ch)^{\\alpha} - ((c-1)h)^{\\alpha}}{h} = (c^{\\alpha} - (c-1)^{\\alpha})h^{\\alpha-1}$$\n    The error is:\n    $$E^{-}(h) = |D^{-}f(ch;h) - f'(ch)| = |(c^{\\alpha} - (c-1)^{\\alpha})h^{\\alpha-1} - \\alpha c^{\\alpha-1}h^{\\alpha-1}| = |c^{\\alpha} - (c-1)^{\\alpha} - \\alpha c^{\\alpha-1}| h^{\\alpha-1}$$\n    The slope is $p_{\\mathrm{B}}^{-} = \\alpha-1$.\n\n3.  Central Difference Error:\n    $$D^{0}f(ch;h) = \\frac{f((c+1)h) - f((c-1)h)}{2h} = \\frac{((c+1)h)^{\\alpha} - ((c-1)h)^{\\alpha}}{2h} = \\frac{(c+1)^{\\alpha} - (c-1)^{\\alpha}}{2}h^{\\alpha-1}$$\n    The error is:\n    $$E^{0}(h) = |D^{0}f(ch;h) - f'(ch)| = \\left|\\frac{(c+1)^{\\alpha} - (c-1)^{\\alpha}}{2}h^{\\alpha-1} - \\alpha c^{\\alpha-1}h^{\\alpha-1}\\right| = \\left|\\frac{(c+1)^{\\alpha} - (c-1)^{\\alpha}}{2} - \\alpha c^{\\alpha-1}\\right|h^{\\alpha-1}$$\n    The slope is $p_{\\mathrm{B}}^{0} = \\alpha-1$.\n\nFor Scenario B, all three methods exhibit the same order of accuracy, which is $\\alpha-1$. Since $0 < \\alpha < 1$, the slope $\\alpha-1$ is negative, indicating that the error increases as $h \\to 0$. The superior accuracy of the central difference is lost due to the strong variation of the higher derivatives near the singularity.\n\nSummary of Theoretical Predictions:\n-   For $\\alpha = 1/4 = 0.25$: $p_{\\mathrm{A}} \\approx [1, 1, 2]$, $p_{\\mathrm{B}} \\approx [-0.75, -0.75, -0.75]$.\n-   For $\\alpha = 1/2 = 0.5$: $p_{\\mathrm{A}} \\approx [1, 1, 2]$, $p_{\\mathrm{B}} \\approx [-0.5, -0.5, -0.5]$.\n-   For $\\alpha = 3/4 = 0.75$: $p_{\\mathrm{A}} \\approx [1, 1, 2]$, $p_{\\mathrm{B}} \\approx [-0.25, -0.25, -0.25]$.\n\nThe following program implements the numerical experiment to verify these theoretical predictions. It computes the absolute errors for the specified parameters and uses linear regression on the log-log data to estimate the convergence slopes.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef solve():\n    \"\"\"\n    Computes and prints the convergence rates for finite difference formulas\n    applied to f(x) = x^alpha, according to the problem specification.\n    \"\"\"\n\n    alphas = [0.25, 0.5, 0.75]\n    all_results = []\n\n    def f(x, alpha):\n        return x**alpha\n\n    def df_exact(x, alpha):\n        return alpha * x**(alpha - 1)\n\n    def estimate_slope(h_values, error_values):\n        \"\"\"\n        Estimates the slope of log(error) vs log(h) using linear regression.\n        \"\"\"\n        # Take the natural logarithm as specified\n        log_h = np.log(h_values)\n        log_error = np.log(error_values)\n        \n        # Use scipy.stats.linregress to perform linear regression\n        result = linregress(log_h, log_error)\n        return result.slope\n\n    # Loop through each value of alpha\n    for alpha in alphas:\n        # ---- SCENARIO A: Fixed Point Evaluation ----\n        x0 = 1e-3\n        k_A = np.arange(3, 9)  # k in {3, 4, 5, 6, 7, 8}\n        h_A = x0 / (2**k_A)\n        \n        exact_deriv_A = df_exact(x0, alpha)\n\n        # Forward difference error\n        approx_plus_A = (f(x0 + h_A, alpha) - f(x0, alpha)) / h_A\n        error_plus_A = np.abs(approx_plus_A - exact_deriv_A)\n\n        # Backward difference error\n        approx_minus_A = (f(x0, alpha) - f(x0 - h_A, alpha)) / h_A\n        error_minus_A = np.abs(approx_minus_A - exact_deriv_A)\n        \n        # Central difference error\n        approx_zero_A = (f(x0 + h_A, alpha) - f(x0 - h_A, alpha)) / (2 * h_A)\n        error_zero_A = np.abs(approx_zero_A - exact_deriv_A)\n\n        # Estimate slopes for Scenario A\n        p_A_plus = estimate_slope(h_A, error_plus_A)\n        p_A_minus = estimate_slope(h_A, error_minus_A)\n        p_A_zero = estimate_slope(h_A, error_zero_A)\n\n        # ---- SCENARIO B: Coalescing Evaluation Point ----\n        c = 2.0\n        k_B = np.arange(0, 6) # k in {0, 1, 2, 3, 4, 5}\n        h_B = 1e-2 / (2**k_B)\n        x_B = c * h_B\n\n        exact_deriv_B = df_exact(x_B, alpha)\n\n        # Forward difference error\n        # Note: x_B is an array, so f and df_exact are applied element-wise\n        approx_plus_B = (f(x_B + h_B, alpha) - f(x_B, alpha)) / h_B\n        error_plus_B = np.abs(approx_plus_B - exact_deriv_B)\n\n        # Backward difference error\n        approx_minus_B = (f(x_B, alpha) - f(x_B - h_B, alpha)) / h_B\n        error_minus_B = np.abs(approx_minus_B - exact_deriv_B)\n        \n        # Central difference error\n        approx_zero_B = (f(x_B + h_B, alpha) - f(x_B - h_B, alpha)) / (2 * h_B)\n        error_zero_B = np.abs(approx_zero_B - exact_deriv_B)\n\n        # Estimate slopes for Scenario B\n        p_B_plus = estimate_slope(h_B, error_plus_B)\n        p_B_minus = estimate_slope(h_B, error_minus_B)\n        p_B_zero = estimate_slope(h_B, error_zero_B)\n\n        # Append results for the current alpha in the specified order\n        all_results.extend([\n            p_A_plus, p_A_minus, p_A_zero,\n            p_B_plus, p_B_minus, p_B_zero\n        ])\n\n    # Format the final list as a string, with numbers rounded to three decimal places\n    formatted_results = [f\"{val:.3f}\" for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the main function\nsolve()\n```"
        }
    ]
}