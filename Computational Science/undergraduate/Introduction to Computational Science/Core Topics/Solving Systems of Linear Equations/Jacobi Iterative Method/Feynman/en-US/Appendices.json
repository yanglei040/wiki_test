{
    "hands_on_practices": [
        {
            "introduction": "The convergence of the Jacobi method is determined by the properties of its iteration matrix, $T_J$. The fundamental theorem of iterative methods states that the process converges for any initial guess if and only if the spectral radius of $T_J$, denoted $\\rho(T_J)$, is strictly less than 1. This first exercise provides foundational practice in the core mechanics of this analysis: for a given linear system, you will construct the Jacobi iteration matrix and compute its spectral radius to definitively determine convergence. ",
            "id": "2207653",
            "problem": "In a simplified economic model, the equilibrium prices, $p_1$ and $p_2$, of two competing but interdependent software products are determined by a system of linear equations. To find these prices, one might use an iterative numerical method. Consider the following system that models the price relationship:\n\n$$\n\\begin{align*}\n4p_1 - 2p_2 = 10 \\\\\np_1 + 5p_2 = 17\n\\end{align*}\n$$\n\nThis system can be written in the matrix form $A\\mathbf{p} = \\mathbf{b}$. The Jacobi method is an iterative algorithm for solving such a system. The convergence of the Jacobi method is determined by the spectral radius, $\\rho(T_J)$, of its iteration matrix $T_J$.\n\nCalculate the spectral radius of the Jacobi iteration matrix $T_J$ for the given system of equations. Express your final answer as a closed-form analytic expression.",
            "solution": "The linear system is written as $A\\mathbf{p}=\\mathbf{b}$ with\n$$\nA=\\begin{pmatrix}4  -2 \\\\ 1  5\\end{pmatrix},\\quad \\mathbf{p}=\\begin{pmatrix}p_{1} \\\\ p_{2}\\end{pmatrix},\\quad \\mathbf{b}=\\begin{pmatrix}10 \\\\ 17\\end{pmatrix}.\n$$\nFor the Jacobi method, decompose $A=D+R$ where $D$ is the diagonal of $A$ and $R=A-D$ is the strictly off-diagonal part. Thus,\n$$\nD=\\begin{pmatrix}4  0 \\\\ 0  5\\end{pmatrix},\\quad R=\\begin{pmatrix}0  -2 \\\\ 1  0\\end{pmatrix}.\n$$\nThe Jacobi iteration matrix is defined by\n$$\nT_{J}=-D^{-1}R=I-D^{-1}A.\n$$\nCompute $D^{-1}A$:\n$$\nD^{-1}=\\begin{pmatrix}\\frac{1}{4}  0 \\\\ 0  \\frac{1}{5}\\end{pmatrix},\\quad D^{-1}A=\\begin{pmatrix}1  -\\frac{1}{2} \\\\ \\frac{1}{5}  1\\end{pmatrix}.\n$$\nHence,\n$$\nT_{J}=I-D^{-1}A=\\begin{pmatrix}0  \\frac{1}{2} \\\\ -\\frac{1}{5}  0\\end{pmatrix}.\n$$\nTo find the spectral radius, compute the eigenvalues of $T_{J}$. For a matrix of the form $\\begin{pmatrix}0  a \\\\ b  0\\end{pmatrix}$, the characteristic polynomial is $\\lambda^{2}-ab=0$, so the eigenvalues are $\\lambda=\\pm\\sqrt{ab}$. Here $a=\\frac{1}{2}$ and $b=-\\frac{1}{5}$, giving\n$$\n\\lambda^{2}-\\left(\\frac{1}{2}\\right)\\left(-\\frac{1}{5}\\right)=\\lambda^{2}+\\frac{1}{10}=0,\n$$\nso\n$$\n\\lambda=\\pm i\\sqrt{\\frac{1}{10}}.\n$$\nThe spectral radius is the maximum modulus of the eigenvalues:\n$$\n\\rho(T_{J})=\\sqrt{\\frac{1}{10}}=\\frac{1}{\\sqrt{10}}.\n$$",
            "answer": "$$\\boxed{\\frac{1}{\\sqrt{10}}}$$"
        },
        {
            "introduction": "While the spectral radius provides a necessary and sufficient condition for convergence, calculating it can be complex. Simpler, sufficient conditions like strict diagonal dominance (SDD) offer a quick way to guarantee convergence. However, it is crucial to understand that these are not necessary conditions. This practice challenges you to explore this distinction by finding a matrix that is not strictly diagonally dominant but for which the Jacobi iteration still converges, reinforcing that the condition $\\rho(T_J) \\lt 1$ is the ultimate criterion. ",
            "id": "2442152",
            "problem": "Let $A(\\alpha)$ be the $3 \\times 3$ real matrix\n$$\nA(\\alpha)=\\begin{pmatrix}\n1  -\\alpha  0 \\\\\n-\\alpha  1  -\\alpha \\\\\n0  -\\alpha  1\n\\end{pmatrix},\n$$\nwhere $\\alpha \\in \\mathbb{R}$. A matrix is said to be strictly diagonally dominant if for every row index $i$ one has $|a_{ii}|  \\sum_{j \\neq i} |a_{ij}|$. Consider the Jacobi stationary iteration for solving the linear system $A(\\alpha)\\,x=b$, which is defined via the diagonal-offdiagonal splitting of $A(\\alpha)$. Say that the Jacobi method converges if, for this $A(\\alpha)$, the iteration converges for all right-hand sides $b \\in \\mathbb{R}^{3}$ and for any initial guess.\n\nDetermine the exact value of\n$$\nc=\\sup\\left\\{\\,|\\alpha| \\,:\\, A(\\alpha)\\ \\text{is not strictly diagonally dominant and the Jacobi iteration converges}\\,\\right\\}.\n$$\nGive your answer as a single closed-form expression. No rounding is required.",
            "solution": "The problem requires us to find the supremum of a set of values $|\\alpha|$ that satisfy two conditions simultaneously:\n1.  The matrix $A(\\alpha)$ is **not** strictly diagonally dominant.\n2.  The Jacobi iteration for the system $A(\\alpha)x=b$ converges.\n\nWe will analyze each condition separately to determine the constraints on $|\\alpha|$.\n\n**Condition 1: Strict Diagonal Dominance**\n\nA matrix is strictly diagonally dominant if for each row $i$, the absolute value of the diagonal element is greater than the sum of the absolute values of the off-diagonal elements in that row. For the given matrix $A(\\alpha)$:\n$$\nA(\\alpha)=\\begin{pmatrix}\n1  -\\alpha  0 \\\\\n-\\alpha  1  -\\alpha \\\\\n0  -\\alpha  1\n\\end{pmatrix}\n$$\nThe conditions for strict diagonal dominance are:\n-   Row 1: $|a_{11}|  |a_{12}| + |a_{13}| \\implies |1|  |-\\alpha| + |0| \\implies 1  |\\alpha|$.\n-   Row 2: $|a_{22}|  |a_{21}| + |a_{23}| \\implies |1|  |-\\alpha| + |-\\alpha| \\implies 1  2|\\alpha| \\implies |\\alpha|  \\frac{1}{2}$.\n-   Row 3: $|a_{33}|  |a_{31}| + |a_{32}| \\implies |1|  |0| + |-\\alpha| \\implies 1  |\\alpha|$.\n\nFor $A(\\alpha)$ to be strictly diagonally dominant, all three inequalities must be satisfied. The most restrictive condition is $|\\alpha|  \\frac{1}{2}$. Therefore, $A(\\alpha)$ is strictly diagonally dominant if and only if $|\\alpha|  \\frac{1}{2}$.\n\nThe problem specifies that $A(\\alpha)$ is **not** strictly diagonally dominant. This is the negation of the condition just derived. Thus, the first condition on $\\alpha$ is:\n$$ |\\alpha| \\ge \\frac{1}{2} $$\n\n**Condition 2: Convergence of the Jacobi Method**\n\nThe Jacobi iteration for a linear system $Ax=b$ is defined by splitting the matrix $A$ into its diagonal ($D$), strictly lower triangular ($L$), and strictly upper triangular ($U$) parts, such that $A = D + L + U$. The iteration is given by $x^{(k+1)} = T_J x^{(k)} + c_J$, where the iteration matrix is $T_J = -D^{-1}(L+U)$. The method converges for any initial guess if and only if the spectral radius of the iteration matrix, $\\rho(T_J)$, is strictly less than $1$.\n\nFor our matrix $A(\\alpha)$:\n$$ D = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} = I $$\n$$ L+U = \\begin{pmatrix} 0  -\\alpha  0 \\\\ -\\alpha  0  -\\alpha \\\\ 0  -\\alpha  0 \\end{pmatrix} $$\nThe inverse of the diagonal matrix is $D^{-1} = I^{-1} = I$. The Jacobi iteration matrix is therefore:\n$$ T_J = -I(L+U) = -(L+U) = \\begin{pmatrix} 0  \\alpha  0 \\\\ \\alpha  0  \\alpha \\\\ 0  \\alpha  0 \\end{pmatrix} $$\nTo find the spectral radius $\\rho(T_J)$, we must find the eigenvalues of $T_J$ by solving the characteristic equation $\\det(T_J - \\lambda I) = 0$.\n$$\n\\det\\begin{pmatrix}\n-\\lambda  \\alpha  0 \\\\\n\\alpha  -\\lambda  \\alpha \\\\\n0  \\alpha  -\\lambda\n\\end{pmatrix} = 0\n$$\nExpanding the determinant along the first row:\n$$\n-\\lambda \\det\\begin{pmatrix} -\\lambda  \\alpha \\\\ \\alpha  -\\lambda \\end{pmatrix} - \\alpha \\det\\begin{pmatrix} \\alpha  \\alpha \\\\ 0  -\\lambda \\end{pmatrix} = 0\n$$\n$$\n-\\lambda (\\lambda^2 - \\alpha^2) - \\alpha(-\\alpha\\lambda - 0) = 0\n$$\n$$\n-\\lambda^3 + \\alpha^2\\lambda + \\alpha^2\\lambda = 0\n$$\n$$\n-\\lambda^3 + 2\\alpha^2\\lambda = 0\n$$\n$$\n\\lambda(-\\lambda^2 + 2\\alpha^2) = 0\n$$\nThe eigenvalues $\\lambda$ are the roots of this equation: $\\lambda_1 = 0$ and $\\lambda^2 = 2\\alpha^2$, which gives $\\lambda_{2,3} = \\pm\\sqrt{2\\alpha^2} = \\pm\\sqrt{2}|\\alpha|$. For algebraic convenience, we can write $\\lambda_{2,3} = \\pm\\sqrt{2}\\alpha$.\nThe eigenvalues of $T_J$ are $\\{0, \\sqrt{2}\\alpha, -\\sqrt{2}\\alpha\\}$.\n\nThe spectral radius $\\rho(T_J)$ is the maximum of the absolute values of the eigenvalues:\n$$\n\\rho(T_J) = \\max\\left\\{|0|, |\\sqrt{2}\\alpha|, |-\\sqrt{2}\\alpha|\\right\\} = \\sqrt{2}|\\alpha|\n$$\nThe Jacobi method converges if and only if $\\rho(T_J)  1$. This provides our second condition on $\\alpha$:\n$$\n\\sqrt{2}|\\alpha|  1 \\implies |\\alpha|  \\frac{1}{\\sqrt{2}}\n$$\n\n**Combining Conditions to Find c**\n\nWe are asked to find $c = \\sup(S)$, where $S$ is the set of values of $|\\alpha|$ that satisfy both derived conditions:\n1.  $|\\alpha| \\ge \\frac{1}{2}$\n2.  $|\\alpha|  \\frac{1}{\\sqrt{2}}$\n\nCombining these inequalities, we define the interval for $|\\alpha|$:\n$$\n\\frac{1}{2} \\le |\\alpha|  \\frac{1}{\\sqrt{2}}\n$$\nThe set of possible values for $|\\alpha|$ is the interval $\\left[\\frac{1}{2}, \\frac{1}{\\sqrt{2}}\\right)$. The supremum of this set is the least upper bound, which is the right endpoint of the interval.\n\nTherefore, the exact value of $c$ is:\n$$\nc = \\sup\\left[\\frac{1}{2}, \\frac{1}{\\sqrt{2}}\\right) = \\frac{1}{\\sqrt{2}}\n$$\nThis value can also be written as $\\frac{\\sqrt{2}}{2}$. We will provide the former.",
            "answer": "$$\n\\boxed{\\frac{1}{\\sqrt{2}}}\n$$"
        },
        {
            "introduction": "Moving from theoretical principles to practical applications, the Jacobi method serves as a fundamental component in solvers for complex scientific problems, such as the discretization of partial differential equations. This advanced problem introduces the weighted Jacobi method, an enhancement where a relaxation parameter $\\omega$ is used to control the iteration's behavior. Your task is to apply Fourier analysis to determine the optimal value of $\\omega$ that maximizes the method's efficiency for a specific class of problems, illustrating how iterative methods are tuned for high-performance applications like multigrid solvers. ",
            "id": "3148682",
            "problem": "Consider the one-dimensional Poisson boundary value problem $-u''(x) = f(x)$ on the interval $\\left[0,1\\right]$ with homogeneous Dirichlet boundary conditions $u(0) = u(1) = 0$. Discretize the problem with $n$ interior grid points using the standard second-order centered finite difference scheme with uniform grid spacing $h = \\frac{1}{n+1}$. This yields the linear system $A \\, \\mathbf{u} = \\mathbf{f}$, where $A \\in \\mathbb{R}^{n \\times n}$ is the tridiagonal matrix with stencil $\\frac{1}{h^{2}}\\left(-1, 2, -1\\right)$ and diagonal part $D = \\operatorname{diag}(A) = \\frac{2}{h^{2}} I$.\n\nConsider the weighted Jacobi iteration for solving $A \\, \\mathbf{u} = \\mathbf{f}$,\n$$\n\\mathbf{u}^{(m+1)} = \\mathbf{u}^{(m)} + \\omega \\, D^{-1}\\left(\\mathbf{f} - A \\, \\mathbf{u}^{(m)}\\right),\n$$\nwhose error-propagation iteration matrix is $T_{\\omega J} = I - \\omega D^{-1} A$. Analyze the error amplification of $T_{\\omega J}$ on the discrete Fourier modes $v_{\\theta}$ with components $v_{\\theta}(i) = \\sin\\!\\left(i \\, \\theta\\right)$ for $i \\in \\{1,\\dots,n\\}$, where $\\theta \\in \\left(0,\\pi\\right)$ corresponds to the discrete sine spectrum induced by the Dirichlet boundary conditions.\n\nStarting only from the definition of the finite difference Laplacian and the discrete sine modes, derive the Fourier symbol of $D^{-1}A$, and then the corresponding scalar amplification factor of $T_{\\omega J}$ on a mode with frequency parameter $\\theta$. Using this, determine the value of the relaxation parameter $\\omega$ that minimizes the maximum error amplification factor of $T_{\\omega J}$ when restricted to the high-frequency range $\\theta \\in \\left[\\frac{\\pi}{2}, \\pi\\right]$, which is the relevant range for smoothing in multigrid methods. Express your final answer as a single exact real number without units. No rounding is required.",
            "solution": "The weighted Jacobi iteration for the linear system $A\\mathbf{u} = \\mathbf{f}$ is given by\n$$\n\\mathbf{u}^{(m+1)} = \\mathbf{u}^{(m)} + \\omega D^{-1}(\\mathbf{f} - A\\mathbf{u}^{(m)})\n$$\nwhere $D = \\operatorname{diag}(A)$. The error vector $\\mathbf{e}^{(m)} = \\mathbf{u}^{(m)} - \\mathbf{u}$, where $\\mathbf{u}$ is the exact solution to the linear system, evolves according to\n$$\n\\mathbf{e}^{(m+1)} = \\left(I - \\omega D^{-1}A\\right) \\mathbf{e}^{(m)}\n$$\nThe matrix $T_{\\omega J} = I - \\omega D^{-1}A$ is the error-propagation matrix. The effectiveness of the iteration as a smoother depends on its ability to damp high-frequency components of the error. The amplification factor for a given error mode is the corresponding eigenvalue of $T_{\\omega J}$.\n\nThe problem specifies that the discrete sine vectors $v_{\\theta}$ with components $v_{\\theta}(i) = \\sin(i\\theta)$ for $i \\in \\{1, \\dots, n\\}$ are the relevant modes to analyze. These are the eigenvectors of the discrete Laplacian matrix $A$. First, we find the eigenvalues of $A$ corresponding to these eigenvectors. The matrix $A$ arises from the centered finite difference stencil $\\frac{1}{h^2}(-1, 2, -1)$. The $i$-th component of the vector $A v_{\\theta}$ is\n$$\n(A v_{\\theta})_i = \\frac{1}{h^2} \\left[ -v_{\\theta}(i-1) + 2v_{\\theta}(i) - v_{\\theta}(i+1) \\right] = \\frac{1}{h^2} \\left[ -\\sin((i-1)\\theta) + 2\\sin(i\\theta) - \\sin((i+1)\\theta) \\right]\n$$\nUsing the trigonometric identity $\\sin(\\alpha - \\beta) + \\sin(\\alpha + \\beta) = 2\\sin(\\alpha)\\cos(\\beta)$, we can simplify the expression:\n$$\n\\sin((i-1)\\theta) + \\sin((i+1)\\theta) = 2\\sin(i\\theta)\\cos(\\theta)\n$$\nSubstituting this into the expression for $(A v_{\\theta})_i$:\n$$\n(A v_{\\theta})_i = \\frac{1}{h^2} \\left[ 2\\sin(i\\theta) - 2\\sin(i\\theta)\\cos(\\theta) \\right] = \\frac{2}{h^2}(1-\\cos(\\theta))\\sin(i\\theta) = \\frac{2}{h^2}(1-\\cos(\\theta)) v_{\\theta}(i)\n$$\nThis shows that $v_{\\theta}$ is an eigenvector of $A$ with eigenvalue $\\lambda_A(\\theta) = \\frac{2}{h^2}(1-\\cos(\\theta))$.\n\nThe matrix $D$ is the diagonal part of $A$, given as $D = \\frac{2}{h^2}I$, where $I$ is the identity matrix. Its inverse is $D^{-1} = \\frac{h^2}{2}I$. Now we can find the eigenvalues of the matrix $D^{-1}A$. Since $D^{-1}$ is a scalar multiple of the identity matrix, the eigenvectors of $D^{-1}A$ are the same as the eigenvectors of $A$. The eigenvalue of $D^{-1}A$ corresponding to the eigenvector $v_{\\theta}$, which is the requested Fourier symbol, is:\n$$\n\\widehat{A}(\\theta) = \\frac{h^2}{2} \\lambda_A(\\theta) = \\frac{h^2}{2} \\left( \\frac{2}{h^2}(1-\\cos(\\theta)) \\right) = 1-\\cos(\\theta)\n$$\n\nThe scalar amplification factor of the iteration, $\\mu(\\omega, \\theta)$, is the eigenvalue of the iteration matrix $T_{\\omega J} = I - \\omega D^{-1}A$ corresponding to the mode $v_{\\theta}$.\n$$\nT_{\\omega J} v_{\\theta} = (I - \\omega D^{-1}A)v_{\\theta} = I v_{\\theta} - \\omega (D^{-1}A v_{\\theta}) = v_{\\theta} - \\omega \\widehat{A}(\\theta) v_{\\theta} = (1 - \\omega \\widehat{A}(\\theta))v_{\\theta}\n$$\nSo, the amplification factor is:\n$$\n\\mu(\\omega, \\theta) = 1 - \\omega \\widehat{A}(\\theta) = 1 - \\omega(1-\\cos(\\theta))\n$$\nThis expression can also be written using the half-angle identity $1-\\cos(\\theta) = 2\\sin^2(\\frac{\\theta}{2})$ as $\\mu(\\omega, \\theta) = 1-2\\omega\\sin^2(\\frac{\\theta}{2})$.\n\nThe goal is to find the value of $\\omega$ that minimizes the maximum magnitude of this amplification factor over the high-frequency range $\\theta \\in [\\frac{\\pi}{2}, \\pi]$. Let us define the smoothing factor $S(\\omega)$ as:\n$$\nS(\\omega) = \\max_{\\theta \\in [\\frac{\\pi}{2}, \\pi]} |\\mu(\\omega, \\theta)| = \\max_{\\theta \\in [\\frac{\\pi}{2}, \\pi]} |1 - \\omega(1-\\cos(\\theta))|\n$$\nThe range of the frequency parameter is $\\theta \\in [\\frac{\\pi}{2}, \\pi]$. We analyze the range of the symbol $\\widehat{A}(\\theta) = 1-\\cos(\\theta)$ over this interval. The function $\\cos(\\theta)$ is monotonically decreasing on $[\\frac{\\pi}{2}, \\pi]$, from $\\cos(\\frac{\\pi}{2})=0$ to $\\cos(\\pi)=-1$. Therefore, $1-\\cos(\\theta)$ is monotonically increasing on this interval.\nThe minimum value of $\\widehat{A}(\\theta)$ occurs at $\\theta=\\frac{\\pi}{2}$:\n$$\n\\widehat{A}\\left(\\frac{\\pi}{2}\\right) = 1 - \\cos\\left(\\frac{\\pi}{2}\\right) = 1 - 0 = 1\n$$\nThe maximum value of $\\widehat{A}(\\theta)$ occurs at $\\theta=\\pi$:\n$$\n\\widehat{A}(\\pi) = 1 - \\cos(\\pi) = 1 - (-1) = 2\n$$\nSo, for $\\theta \\in [\\frac{\\pi}{2}, \\pi]$, the value of $\\lambda \\equiv \\widehat{A}(\\theta)$ lies in the interval $[1, 2]$. Our optimization problem becomes finding $\\omega$ that minimizes:\n$$\n\\min_{\\omega} \\left( \\max_{\\lambda \\in [1, 2]} |1 - \\omega\\lambda| \\right)\n$$\nLet $g(\\lambda) = 1-\\omega\\lambda$. Since this is a linear function of $\\lambda$, its absolute value $|g(\\lambda)|$ on a closed interval $[1, 2]$ must attain its maximum at one of the endpoints of the interval. Thus,\n$$\n\\max_{\\lambda \\in [1, 2]} |1 - \\omega\\lambda| = \\max \\left( |1 - \\omega \\cdot 1|, |1 - \\omega \\cdot 2| \\right) = \\max(|1-\\omega|, |1-2\\omega|)\n$$\nWe seek the value of $\\omega$ that solves the min-max problem:\n$$\n\\min_{\\omega} \\max(|1-\\omega|, |1-2\\omega|)\n$$\nThe minimum value of the maximum of two functions is typically found where the functions are equal. We set the magnitudes equal:\n$$\n|1-\\omega| = |1-2\\omega|\n$$\nThis equation implies one of two possibilities:\n1. $1-\\omega = 1-2\\omega \\implies \\omega = 0$. This corresponds to no iteration, and the amplification factor is $1$, which is not optimal.\n2. $1-\\omega = -(1-2\\omega) \\implies 1-\\omega = 2\\omega - 1$.\nSolving the second case for $\\omega$:\n$$\n2 = 3\\omega \\implies \\omega = \\frac{2}{3}\n$$\nFor this value of $\\omega$, the amplification at the boundaries of the spectral range $[1, 2]$ is:\n$$\n|1 - \\frac{2}{3}| = \\frac{1}{3}\n$$\n$$\n|1 - 2\\left(\\frac{2}{3}\\right)| = |1 - \\frac{4}{3}| = |-\\frac{1}{3}| = \\frac{1}{3}\n$$\nThe maximum amplification factor is $\\frac{1}{3}$. Any perturbation of $\\omega$ from $\\frac{2}{3}$ will increase one of these values, thus increasing the maximum. Therefore, the optimal value of the relaxation parameter is $\\frac{2}{3}$.",
            "answer": "$$\\boxed{\\frac{2}{3}}$$"
        }
    ]
}