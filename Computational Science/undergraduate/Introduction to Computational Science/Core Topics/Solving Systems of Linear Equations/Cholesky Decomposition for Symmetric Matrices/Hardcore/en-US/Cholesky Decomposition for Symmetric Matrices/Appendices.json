{
    "hands_on_practices": [
        {
            "introduction": "The most effective way to truly understand an algorithm is to build it from the ground up. This first practice challenges you to implement the Cholesky decomposition directly from its mathematical definition, $A = L L^\\top$. By translating this equation into a working program and validating it against a diverse test suite, you will gain a fundamental understanding of the factorization process and the critical importance of robust, test-driven development in computational science .",
            "id": "3106422",
            "problem": "You are to write a complete, runnable program that, given a real square matrix $A$, attempts to compute a lower-triangular matrix $L$ such that $A = L L^\\top$ and verifies two properties central to the Cholesky decomposition for symmetric positive definite (SPD) matrices: (i) $L$ has strictly positive diagonal entries, and (ii) $L L^\\top$ reconstructs $A$ within a specified tolerance. Your design must begin from fundamental definitions of symmetry and positive definiteness, and it must not rely on any pre-packaged factorization routine. Your program must integrate these checks into a test-driven workflow by evaluating a small test suite of matrices and reporting a boolean for each test that indicates whether all required checks for that test passed.\n\nFundamental definitions to use as the base:\n- A real square matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric if $A = A^\\top$.\n- A symmetric matrix $A$ is symmetric positive definite (SPD) if for all nonzero vectors $x \\in \\mathbb{R}^n$, $x^\\top A x > 0$.\n- The Cholesky decomposition asserts that for SPD $A$, there exists a unique lower-triangular $L$ with positive diagonal entries such that $A = L L^\\top$.\n\nYour program must:\n- Implement a function that attempts to compute a lower-triangular factor $L$ from an input $A$ using only elementary operations and linear algebra as needed, starting from the definitions above. The function must not call a built-in Cholesky factorization.\n- Implement error checking that ensures:\n  1) Symmetry within a tolerance $\\tau_{\\mathrm{sym}}$ using the maximum absolute entrywise deviation of $A - A^\\top$.\n  2) Strict positivity of the diagonal entries of $L$ relative to a positivity threshold $\\tau_{\\mathrm{pos}}$.\n  3) Reconstruction accuracy within a tolerance $\\tau_{\\mathrm{rec}}$ using the relative Frobenius norm error $\\lVert A - L L^\\top \\rVert_F / \\lVert A \\rVert_F$.\n- Return a boolean verdict per test case: return true if and only if all three checks pass and a valid $L$ is constructed; otherwise return false.\n\nAngle or physical units are not applicable here.\n\nTest Suite:\nFor each case below, the program must apply the specified tolerances and report a boolean. The test suite must be executed in the following order.\n\n- Test $1$ (happy path, small SPD): \n  - Matrix $$A_1 = \\begin{bmatrix} 4 & 2 & 2 \\\\ 2 & 3 & 1 \\\\ 2 & 1 & 3 \\end{bmatrix}$$.\n  - Tolerances: $\\tau_{\\mathrm{sym}} = 10^{-15}$, $\\tau_{\\mathrm{pos}} = 10^{-15}$, $\\tau_{\\mathrm{rec}} = 10^{-12}$.\n\n- Test $2$ (ill-conditioned but SPD, Hilbert matrix of size $3$):\n  - Matrix $A_2$ with entries $(A_2)_{ij} = \\dfrac{1}{i + j - 1}$ for $i,j \\in \\{1,2,3\\}$, i.e. $$A_2 = \\begin{bmatrix} 1 & \\tfrac{1}{2} & \\tfrac{1}{3} \\\\ \\tfrac{1}{2} & \\tfrac{1}{3} & \\tfrac{1}{4} \\\\ \\tfrac{1}{3} & \\tfrac{1}{4} & \\tfrac{1}{5} \\end{bmatrix}$$.\n  - Tolerances: $\\tau_{\\mathrm{sym}} = 10^{-15}$, $\\tau_{\\mathrm{pos}} = 10^{-15}$, $\\tau_{\\mathrm{rec}} = 10^{-12}$.\n\n- Test $3$ (symmetric but not positive definite):\n  - Matrix $$A_3 = \\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix}$$.\n  - Tolerances: $\\tau_{\\mathrm{sym}} = 10^{-15}$, $\\tau_{\\mathrm{pos}} = 10^{-15}$, $\\tau_{\\mathrm{rec}} = 10^{-12}$.\n\n- Test $4$ (non-symmetric input):\n  - Matrix $$A_4 = \\begin{bmatrix} 2 & 1 \\\\ 0 & 2 \\end{bmatrix}$$.\n  - Tolerances: $\\tau_{\\mathrm{sym}} = 10^{-15}$, $\\tau_{\\mathrm{pos}} = 10^{-15}$, $\\tau_{\\mathrm{rec}} = 10^{-12}$.\n\n- Test $5$ (random SPD of size $5$ with reproducible generation):\n  - Construct $A_5$ by the following algorithmic recipe: set an explicit random seed $s = 0$, draw a matrix $M \\in \\mathbb{R}^{5 \\times 5}$ with independent standard normal entries, then set $A_5 = M^\\top M + \\alpha I$ with $\\alpha = 10^{-6}$ and $I$ the $5 \\times 5$ identity.\n  - Tolerances: $\\tau_{\\mathrm{sym}} = 10^{-15}$, $\\tau_{\\mathrm{pos}} = 10^{-15}$, $\\tau_{\\mathrm{rec}} = 10^{-12}$.\n\n- Test $6$ (nearly symmetric SPD matrix, accept with symmetry tolerance):\n  - Construct a base SPD matrix $S$ as follows: let $$Q = \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 1 & 4 \\\\ 5 & 6 & 0 \\end{bmatrix}$$, set $S = Q^\\top Q + \\beta I$ with $\\beta = 10^{-3}$ and $I$ the $3 \\times 3$ identity. Then define $A_6$ by taking $S$ and adding a tiny perturbation only to entry $(1,2)$ (using $1$-based indexing): set $(A_6)_{1,2} \\leftarrow (A_6)_{1,2} + \\varepsilon$ with $\\varepsilon = 10^{-11}$, leaving all other entries identical to $S$.\n  - Tolerances: $\\tau_{\\mathrm{sym}} = 10^{-10}$, $\\tau_{\\mathrm{pos}} = 10^{-15}$, $\\tau_{\\mathrm{rec}} = 10^{-12}$.\n\nFinal output format:\nYour program should produce a single line of output containing the boolean results of the tests, in order from Test $1$ through Test $6$, as a comma-separated list enclosed in square brackets. For example, a valid output would look like $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True},\\mathrm{True},\\mathrm{False}]$ (this is only an example).",
            "solution": "The problem statement has been analyzed according to the specified validation protocol. All givens, definitions, constraints, and test cases have been extracted and evaluated. The problem is determined to be **valid**. It is scientifically grounded in the principles of numerical linear algebra, specifically the Cholesky decomposition. It is well-posed, objective, self-contained, and algorithmically formalizable. The test suite is well-defined and allows for a deterministic and verifiable solution.\n\nThe core task is to implement a function for Cholesky decomposition from first principles and use it to validate properties of several test matrices. The Cholesky decomposition states that any real, symmetric positive definite (SPD) matrix $A \\in \\mathbb{R}^{n \\times n}$ can be uniquely decomposed into the product $A = LL^\\top$, where $L$ is a lower-triangular matrix with strictly positive diagonal entries.\n\nThe algorithm for computing the entries of $L = (l_{ij})$ can be derived by equating the entries of $A = (a_{ij})$ with the entries of the product $LL^\\top$. The $(i, j)$-th entry of $LL^\\top$ is given by $(LL^\\top)_{ij} = \\sum_{k=1}^{j} l_{ik} l_{jk}$, assuming $j \\le i$.\n\nWe can compute the elements of $L$ column by column, for $j = 0, 1, \\dots, n-1$.\n\nFor the diagonal element $l_{jj}$:\nThe equation for the diagonal entry $a_{jj}$ is:\n$$a_{jj} = \\sum_{k=0}^{j} l_{jk} l_{jk} = \\left( \\sum_{k=0}^{j-1} l_{jk}^2 \\right) + l_{jj}^2$$\nSolving for $l_{jj}$, we get:\n$$l_{jj} = \\sqrt{a_{jj} - \\sum_{k=0}^{j-1} l_{jk}^2}$$\nFor $L$ to be a real matrix, the term under the square root must be non-negative. For it to have strictly positive diagonal entries, the term must be strictly positive. If at any step this term is less than or equal to $0$, the matrix $A$ is not symmetric positive definite, and the decomposition fails. This provides an algorithmic test for positive definiteness.\n\nFor the off-diagonal elements $l_{ij}$ where $i > j$:\nThe equation for the off-diagonal entry $a_{ij}$ is:\n$$a_{ij} = \\sum_{k=0}^{j} l_{ik} l_{jk} = \\left( \\sum_{k=0}^{j-1} l_{ik} l_{jk} \\right) + l_{ij} l_{jj}$$\nSolving for $l_{ij}$, given that $l_{jj} > 0$:\n$$l_{ij} = \\frac{1}{l_{jj}} \\left( a_{ij} - \\sum_{k=0}^{j-1} l_{ik} l_{jk} \\right)$$\nThese formulas define the Cholesky-Banachiewicz algorithm. It is important to note that these derivations assume $A$ is symmetric (i.e., $a_{ij} = a_{ji}$). The resulting algorithm only requires access to the lower triangle and diagonal of $A$ (entries $a_{ij}$ with $i \\ge j$). Our implementation will adhere to this standard convention. For an input matrix that is not perfectly symmetric, the algorithm will operate on its lower-triangular part.\n\nThe overall validation process for each test matrix $A$ involves three checks:\n\n$1$. **Symmetry Check**: Before attempting decomposition, we verify if $A$ is symmetric within a tolerance $\\tau_{\\mathrm{sym}}$. We compute the maximum absolute element-wise difference between $A$ and its transpose $A^\\top$. The check passes if $\\max_{i,j} |a_{ij} - a_{ji}| < \\tau_{\\mathrm{sym}}$. If this check fails, the matrix is deemed non-symmetric for the given tolerance, and the test verdict is `False`.\n\n$2$. **Diagonal Positivity Check**: After successfully computing a lower-triangular matrix $L$, we must verify that all its diagonal entries $l_{ii}$ are strictly positive. This check is performed relative to a positivity threshold $\\tau_{\\mathrm{pos}}$, meaning we require $l_{ii} > \\tau_{\\mathrm{pos}}$ for all $i=0, \\dots, n-1$. A value of $l_{ii}$ that is positive but does not exceed this threshold signifies a failure. This could occur for matrices that are nearly singular.\n\n$3$. **Reconstruction Accuracy Check**: To ensure the computed factor $L$ is correct, we reconstruct the matrix as $L L^\\top$ and compare it to the original matrix $A$. The error is quantified using the relative Frobenius norm:\n$$\\text{err}_{\\mathrm{rec}} = \\frac{\\lVert A - L L^\\top \\rVert_F}{\\lVert A \\rVert_F}$$\nwhere $\\lVert M \\rVert_F = \\sqrt{\\sum_{i,j} M_{ij}^2}$. The check passes if this relative error is less than a specified reconstruction tolerance $\\tau_{\\mathrm{rec}}$.\n\nA test case returns `True` if and only if the input matrix passes the symmetry check, the Cholesky decomposition algorithm successfully produces a matrix $L$, and this $L$ subsequently passes both the diagonal positivity and reconstruction accuracy checks. Otherwise, the test case returns `False`.\n\nThe logic is encapsulated in a program that evaluates the provided test suite. For Test $3$, the matrix is symmetric but not positive definite, so the decomposition algorithm is expected to fail. For Test $4$, the matrix is not symmetric, so it is expected to fail the initial symmetry check. For Test $6$, the matrix is constructed to be nearly symmetric, such that it passes the symmetry check with the given tolerance $\\tau_{\\mathrm{sym}} = 10^{-10}$. Since the implemented algorithm reads from the lower triangle of the input matrix, it will effectively operate on the underlying SPD matrix, successfully compute a factor, and pass all subsequent checks.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_cholesky(A):\n    \"\"\"\n    Attempts to compute the Cholesky decomposition of a matrix A.\n\n    This function implements the Cholesky-Banachiewicz algorithm, which computes\n    the lower-triangular matrix L column by column, such that A = L L^T.\n    The algorithm assumes A is symmetric and will only access the lower-triangular\n    part of A.\n\n    Args:\n        A (np.ndarray): A square numpy array.\n\n    Returns:\n        np.ndarray or None: The lower-triangular factor L if the decomposition is\n        successful, otherwise None (if the matrix is not positive definite).\n    \"\"\"\n    n = A.shape[0]\n    if A.shape[1] != n:\n        raise ValueError(\"Input matrix must be square.\")\n\n    L = np.zeros_like(A, dtype=float)\n\n    for j in range(n):\n        # Compute the diagonal element L[j, j]\n        # s = sum_{k=0}^{j-1} L[j, k]^2\n        s = L[j, :j] @ L[j, :j].T\n        \n        # Argument for square root\n        d = A[j, j] - s\n        \n        # Check for positive definiteness. If d <= 0, the matrix is not\n        # strictly positive definite.\n        if d <= 0:\n            return None\n        \n        L[j, j] = np.sqrt(d)\n        \n        # Compute the off-diagonal elements in column j\n        for i in range(j + 1, n):\n            # s_ij = sum_{k=0}^{j-1} L[i,k] * L[j,k]\n            s_ij = L[i, :j] @ L[j, :j].T\n            L[i, j] = (A[i, j] - s_ij) / L[j, j]\n            \n    return L\n\ndef test_cholesky(A, tau_sym, tau_pos, tau_rec):\n    \"\"\"\n    Performs Cholesky decomposition and verifies a set of properties.\n\n    Args:\n        A (np.ndarray): The input matrix for testing.\n        tau_sym (float): Tolerance for the symmetry check.\n        tau_pos (float): Threshold for diagonal entry positivity.\n        tau_rec (float): Tolerance for reconstruction accuracy.\n\n    Returns:\n        bool: True if A passes all checks, False otherwise.\n    \"\"\"\n    if A.ndim != 2 or A.shape[0] != A.shape[1]:\n        return False\n\n    # 1. Symmetry check\n    sym_err = np.max(np.abs(A - A.T))\n    if sym_err >= tau_sym:\n        return False\n\n    # Attempt to compute Cholesky factor\n    L = compute_cholesky(A)\n    \n    # If decomposition fails, L is None\n    if L is None:\n        return False\n        \n    # 2. Diagonal positivity check\n    if not np.all(np.diag(L) > tau_pos):\n        return False\n        \n    # 3. Reconstruction accuracy check\n    A_reconstructed = L @ L.T\n    norm_A = np.linalg.norm(A, 'fro')\n    \n    if norm_A == 0: # Handle zero matrix case\n        if np.linalg.norm(A - A_reconstructed, 'fro') == 0:\n            rec_err = 0.0\n        else:\n            return False # Non-zero error for a zero matrix\n    else:\n        rec_err = np.linalg.norm(A - A_reconstructed, 'fro') / norm_A\n\n    if rec_err >= tau_rec:\n        return False\n        \n    # All checks passed\n    return True\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and report the test suite results.\n    \"\"\"\n    # Test 1: Happy path, small SPD\n    A1 = np.array([[4, 2, 2], [2, 3, 1], [2, 1, 3]], dtype=float)\n    tols1 = (1e-15, 1e-15, 1e-12)\n\n    # Test 2: Ill-conditioned but SPD (Hilbert matrix)\n    A2 = np.array([[1, 1/2, 1/3], [1/2, 1/3, 1/4], [1/3, 1/4, 1/5]], dtype=float)\n    tols2 = (1e-15, 1e-15, 1e-12)\n\n    # Test 3: Symmetric but not positive definite\n    A3 = np.array([[1, 2], [2, 1]], dtype=float)\n    tols3 = (1e-15, 1e-15, 1e-12)\n\n    # Test 4: Non-symmetric input\n    A4 = np.array([[2, 1], [0, 2]], dtype=float)\n    tols4 = (1e-15, 1e-15, 1e-12)\n\n    # Test 5: Random SPD matrix\n    np.random.seed(0)\n    M = np.random.randn(5, 5)\n    A5 = M.T @ M + 1e-6 * np.eye(5)\n    tols5 = (1e-15, 1e-15, 1e-12)\n\n    # Test 6: Nearly symmetric SPD matrix\n    Q = np.array([[1, 2, 3], [0, 1, 4], [5, 6, 0]], dtype=float)\n    S = Q.T @ Q + 1e-3 * np.eye(3)\n    A6 = S.copy()\n    A6[0, 1] += 1e-11 # Perturb upper triangle (index (0,1) is entry (1,2))\n    tols6 = (1e-10, 1e-15, 1e-12)\n    \n    test_cases = [\n        (A1, *tols1),\n        (A2, *tols2),\n        (A3, *tols3),\n        (A4, *tols4),\n        (A5, *tols5),\n        (A6, *tols6),\n    ]\n\n    results = []\n    for case in test_cases:\n        A, ts, tp, tr = case\n        result = test_cholesky(A, ts, tp, tr)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having implemented the core algorithm, we can now probe its mathematical behavior more deeply. This exercise moves from implementation to analysis, asking how the Cholesky factor $L$ responds to small perturbations in the matrix $A$. Using the principles of first-order Taylor expansions, you will derive the sensitivity of the factorization, a key concept for understanding the stability and reliability of numerical methods in the face of real-world data and floating-point errors .",
            "id": "3106425",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be real symmetric positive definite (SPD), so it admits a Cholesky decomposition $A = L L^{\\top}$ with $L$ lower triangular and positive diagonal. Consider the parametric matrix $A(\\varepsilon) = I + \\varepsilon H$, where $I$ is the identity and $H \\in \\mathbb{R}^{3 \\times 3}$ is real symmetric. Assume $\\varepsilon$ is sufficiently small that $A(\\varepsilon)$ remains SPD. Work from the definition $A(\\varepsilon) = L(\\varepsilon) L(\\varepsilon)^{\\top}$ and basic differentiability to analyze the first-order effect of the perturbation.\n\n(a) Argue from first principles why $L(0) = I$.\n\n(b) Using only the core definition $A(\\varepsilon) = L(\\varepsilon) L(\\varepsilon)^{\\top}$, derive the first-order relation that determines the lower-triangular matrix $X \\in \\mathbb{R}^{3 \\times 3}$ such that $L(\\varepsilon) = I + \\varepsilon X + \\mathcal{O}(\\varepsilon^{2})$ as $\\varepsilon \\to 0$. Express the entries of $X$ in terms of the entries of $H$ and the lower-triangular structure with positive diagonal.\n\n(c) Let\n$$\nH = \\begin{pmatrix}\n2 & -1 & 4 \\\\\n-1 & 0 & 3 \\\\\n4 & 3 & 1\n\\end{pmatrix}.\n$$\nUsing your result from part (b), determine the exact value of $\\dfrac{d}{d\\varepsilon}\\big(L(\\varepsilon)\\big)_{31}\\Big|_{\\varepsilon=0}$.\n\n(d) Verify your first-order result for the $(3,1)$ entry by a finite-difference computation at $\\varepsilon = 10^{-4}$, using the central difference approximation $\\big(L(\\varepsilon)\\big)_{31}$ and $\\big(L(-\\varepsilon)\\big)_{31}$ obtained from the Cholesky factors of $A(\\varepsilon)$ and $A(-\\varepsilon)$. In your verification, you may round intermediate numerical values to six significant figures; the final answer for part (c) must be exact and expressed as a single real number with no units.",
            "solution": "The problem is subjected to validation and is found to be scientifically grounded, well-posed, and objective. All necessary information is provided, and the problem is a standard exercise in matrix perturbation theory.\n\n(a) We are given the parametric matrix $A(\\varepsilon) = I + \\varepsilon H$, where $I$ is the identity matrix. At $\\varepsilon=0$, we have $A(0) = I + 0 \\cdot H = I$. The Cholesky decomposition of $A(0)$ is defined as $A(0) = L(0) L(0)^{\\top}$, where $L(0)$ is a lower triangular matrix with strictly positive diagonal entries.\n\nSubstituting $A(0) = I$, we get the equation $I = L(0) L(0)^{\\top}$. Let $L \\equiv L(0)$. The matrix $L$ is, by definition, lower triangular with $L_{ii} > 0$ for all $i$. Let us determine the entries of $L$ component-wise.\nThe equation is $L L^{\\top} = I$.\nFor the $(1,1)$ entry, we have $(L L^{\\top})_{11} = \\sum_{k=1}^{n} L_{1k} (L^{\\top})_{k1} = \\sum_{k=1}^{n} L_{1k} L_{1k} = L_{11}^2 + L_{12}^2 + \\ldots + L_{1n}^2$.\nSince $L$ is lower triangular, $L_{1k} = 0$ for $k > 1$. Thus, the equation simplifies to $L_{11}^2 = I_{11} = 1$.\nAs the diagonal entries of $L$ must be positive, $L_{11} > 0$, we must have $L_{11} = 1$.\n\nFor the off-diagonal entries in the first row ($i=1, j>1$), we have $(L L^{\\top})_{1j} = \\sum_{k=1}^{n} L_{1k} L_{jk} = L_{11}L_{j1} = 0$. Since $L_{11}=1$, this implies $L_{j1}=0$ for $j>1$. This is consistent with $L$ being lower triangular.\n\nNow consider the $(2,2)$ entry: $(L L^{\\top})_{22} = \\sum_{k=1}^{n} L_{2k}L_{2k} = L_{21}^2 + L_{22}^2 + \\ldots + L_{2n}^2 = 1$.\nSince $L$ is lower triangular, $L_{2k}=0$ for $k>2$. Thus, $L_{21}^2 + L_{22}^2 = 1$.\nFrom the $(2,1)$ entry of $L L^{\\top}$: $(L L^{\\top})_{21} = \\sum_{k=1}^{n} L_{2k}L_{1k} = L_{21}L_{11} = I_{21} = 0$. Since $L_{11}=1$, we have $L_{21}=0$.\nSubstituting $L_{21}=0$ into the equation for the $(2,2)$ entry gives $L_{22}^2=1$. As $L_{22}>0$, we must have $L_{22}=1$.\n\nThis pattern continues. By induction, assume for all $k < i$, we have $L_{kk}=1$ and $L_{kj}=0$ for $k \\neq j$. We then examine the $i$-th row of $L L^{\\top}$.\nFor $j < i$, $(L L^{\\top})_{ij} = \\sum_{k=1}^{n} L_{ik}L_{jk} = \\sum_{k=1}^{j} L_{ik}L_{jk}$. By the inductive hypothesis, $L_{jk}=0$ unless $k=j$. So, the sum becomes $L_{ij}L_{jj} = L_{ij}(1) = L_{ij}$. Since $(L L^{\\top})_{ij} = I_{ij}=0$ for $i \\neq j$, we have $L_{ij}=0$ for $j < i$.\nFor the diagonal entry $(i,i)$, $(L L^{\\top})_{ii} = \\sum_{k=1}^{n} L_{ik}^2 = \\sum_{k=1}^{i} L_{ik}^2 = 1$. Since we just showed $L_{ik}=0$ for $k<i$, this simplifies to $L_{ii}^2=1$.\nWith the constraint $L_{ii} > 0$, we get $L_{ii}=1$.\nThis argument holds for all $i=1, \\ldots, n$. Therefore, $L=L(0)$ must be the identity matrix, $I$.\n\n(b) We start with the defining relation $A(\\varepsilon) = L(\\varepsilon) L(\\varepsilon)^{\\top}$.\nWe are given the expansions:\n$A(\\varepsilon) = I + \\varepsilon H$\n$L(\\varepsilon) = I + \\varepsilon X + \\mathcal{O}(\\varepsilon^2)$\nwhere $X = \\dfrac{d}{d\\varepsilon}L(\\varepsilon)\\Big|_{\\varepsilon=0}$. Since $L(\\varepsilon)$ is lower triangular for all $\\varepsilon$, its derivative $X$ must also be a lower triangular matrix.\n\nSubstitute these expansions into the defining relation:\n$I + \\varepsilon H = (I + \\varepsilon X + \\mathcal{O}(\\varepsilon^2))(I + \\varepsilon X + \\mathcal{O}(\\varepsilon^2))^{\\top}$\n$I + \\varepsilon H = (I + \\varepsilon X + \\mathcal{O}(\\varepsilon^2))(I + \\varepsilon X^{\\top} + \\mathcal{O}(\\varepsilon^2))$\nExpand the right-hand side and keep terms up to first order in $\\varepsilon$:\n$I + \\varepsilon H = I(I + \\varepsilon X^{\\top}) + \\varepsilon X(I + \\varepsilon X^{\\top}) + \\mathcal{O}(\\varepsilon^2)$\n$I + \\varepsilon H = I + \\varepsilon X^{\\top} + \\varepsilon X + \\varepsilon^2 X X^{\\top} + \\mathcal{O}(\\varepsilon^2)$\n$I + \\varepsilon H = I + \\varepsilon(X + X^{\\top}) + \\mathcal{O}(\\varepsilon^2)$\nBy equating the coefficients of the $\\varepsilon^1$ terms on both sides, we obtain the first-order relation:\n$H = X + X^{\\top}$\nThis is the equation that determines the matrix $X$. We now use the structure of $H$ and $X$. $H$ is a given real symmetric matrix, and $X$ is an unknown real lower triangular matrix. Let the entries be denoted by $H_{ij}$ and $X_{ij}$.\nThe equation in terms of entries is $H_{ij} = X_{ij} + (X^{\\top})_{ij} = X_{ij} + X_{ji}$.\n\nWe solve for $X_{ij}$ based on the indices $i$ and $j$:\n1. For diagonal entries ($i=j$):\n$H_{ii} = X_{ii} + X_{ii} = 2 X_{ii}$.\nThis gives $X_{ii} = \\dfrac{1}{2} H_{ii}$.\n\n2. For strict lower triangular entries ($i > j$):\n$H_{ij} = X_{ij} + X_{ji}$.\nSince $X$ is lower triangular, its entries above the main diagonal are zero, so $X_{ji} = 0$ for $j < i$.\nThus, for $i > j$, the equation simplifies to $H_{ij} = X_{ij}$.\n\n3. For strict upper triangular entries ($i < j$):\n$H_{ij} = X_{ij} + X_{ji}$.\nSince $X$ is lower triangular, $X_{ij} = 0$ for $i < j$.\nThe equation becomes $H_{ij} = X_{ji}$. Since $H$ is symmetric, $H_{ij} = H_{ji}$. So this yields $H_{ji} = X_{ji}$ for $j > i$, which is the same relationship as in case 2, just with swapped indices.\n\nIn summary, the entries of the lower triangular matrix $X$ are given by:\n$$\nX_{ij} =\n\\begin{cases}\n\\frac{1}{2} H_{ii} & \\text{if } i = j \\\\\nH_{ij} & \\text{if } i > j \\\\\n0 & \\text{if } i < j\n\\end{cases}\n$$\nThis result is consistent with the lower-triangular structure of $X$. The requirement that $L(\\varepsilon)$ has a positive diagonal is satisfied for small $\\varepsilon$ because $L_{ii}(\\varepsilon) = 1 + \\varepsilon X_{ii} + \\mathcal{O}(\\varepsilon^2) = 1 + \\frac{\\varepsilon}{2} H_{ii} + \\mathcal{O}(\\varepsilon^2)$, which is positive for sufficiently small $\\varepsilon$ since $L_{ii}(0) = 1 > 0$.\n\n(c) We are asked to determine the value of $\\dfrac{d}{d\\varepsilon}\\big(L(\\varepsilon)\\big)_{31}\\Big|_{\\varepsilon=0}$. From the Taylor expansion, this quantity is precisely the entry $X_{31}$ of the matrix $X$.\nThe given matrix is:\n$$\nH = \\begin{pmatrix}\n2 & -1 & 4 \\\\\n-1 & 0 & 3 \\\\\n4 & 3 & 1\n\\end{pmatrix}\n$$\nUsing the formula derived in part (b) for the strict lower triangular entries of $X$ ($i > j$), we have $X_{ij} = H_{ij}$.\nFor the specific entry $(3,1)$, we have $i=3$ and $j=1$, so $i > j$.\nTherefore, $X_{31} = H_{31}$.\nFrom the given matrix $H$, the entry $H_{31}$ is $4$.\nSo, $\\dfrac{d}{d\\varepsilon}\\big(L(\\varepsilon)\\big)_{31}\\Big|_{\\varepsilon=0} = 4$.\n\n(d) To verify this result, we use a central difference approximation for the derivative at $\\varepsilon=0$:\n$f'(0) \\approx \\dfrac{f(\\varepsilon) - f(-\\varepsilon)}{2\\varepsilon}$.\nHere, $f(\\varepsilon)$ is the function $\\big(L(\\varepsilon)\\big)_{31}$, and we will use $\\varepsilon = 10^{-4}$. The value to compute is $\\dfrac{\\big(L(10^{-4})\\big)_{31} - \\big(L(-10^{-4})\\big)_{31}}{2 \\times 10^{-4}}$.\n\nFirst, calculate $A(\\varepsilon)=I+\\varepsilon H$ for $\\varepsilon=10^{-4}$:\n$A(10^{-4}) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} + 10^{-4} \\begin{pmatrix} 2 & -1 & 4 \\\\ -1 & 0 & 3 \\\\ 4 & 3 & 1 \\end{pmatrix} = \\begin{pmatrix} 1.0002 & -0.0001 & 0.0004 \\\\ -0.0001 & 1.0000 & 0.0003 \\\\ 0.0004 & 0.0003 & 1.0001 \\end{pmatrix}$.\nLet $L(\\varepsilon)$ be the Cholesky factor of $A(\\varepsilon)$. The entry $\\big(L(\\varepsilon)\\big)_{31}$ is given by the formula $L_{31} = A_{31} / L_{11} = A_{31} / \\sqrt{A_{11}}$.\n$L_{11}(10^{-4}) = \\sqrt{1.0002} \\approx 1.00010$.\n$\\big(L(10^{-4})\\big)_{31} = \\dfrac{0.0004}{1.00010} \\approx 0.000399960$.\n\nNext, calculate $A(-\\varepsilon)=I-\\varepsilon H$ for $\\varepsilon=10^{-4}$:\n$A(-10^{-4}) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} - 10^{-4} \\begin{pmatrix} 2 & -1 & 4 \\\\ -1 & 0 & 3 \\\\ 4 & 3 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.9998 & 0.0001 & -0.0004 \\\\ 0.0001 & 1.0000 & -0.0003 \\\\ -0.0004 & -0.0003 & 0.9999 \\end{pmatrix}$.\nLet $L(-\\varepsilon)$ be the Cholesky factor of $A(-\\varepsilon)$.\n$L_{11}(-10^{-4}) = \\sqrt{0.9998} \\approx 0.999900$.\n$\\big(L(-10^{-4})\\big)_{31} = \\dfrac{-0.0004}{0.999900} \\approx -0.000400040$.\n\nFinally, compute the central difference:\n$\\dfrac{d}{d\\varepsilon}\\big(L(\\varepsilon)\\big)_{31}\\Big|_{\\varepsilon=0} \\approx \\dfrac{0.000399960 - (-0.000400040)}{2 \\times 10^{-4}} = \\dfrac{0.000800000}{0.0002} = 4.00000$.\nThe numerical result of $4.00000$ confirms the exact analytical result of $4$ derived in part (c).",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "Standard Cholesky decomposition requires a strictly positive definite matrix, but many practical applications involve matrices that are only positive semidefinite—or singular. This advanced practice introduces a powerful technique to handle such cases: the truncated Cholesky factorization. You will learn to implement an algorithm that detects numerical rank deficiency by monitoring pivot sizes, enabling you to compute stable low-rank approximations and apply targeted regularization, a skill essential in fields like statistics, machine learning, and optimization .",
            "id": "3106439",
            "problem": "You are tasked with designing and implementing a principled algorithm for numerical rank detection and regularization of symmetric matrices using Cholesky factorization within the context of introduction to computational science. Your solution must be a complete, runnable program. Work from first principles and core definitions without relying on any specialized shortcut formulas.\n\nAssumptions and core definitions:\n- A matrix $A \\in \\mathbb{R}^{n \\times n}$ is called symmetric positive semidefinite (SPSD) if $x^{\\top} A x \\ge 0$ for all nonzero $x \\in \\mathbb{R}^{n}$.\n- For a symmetric positive definite (SPD) matrix $A$, the Cholesky factorization is $A = L L^{\\top}$ where $L$ is lower triangular with strictly positive diagonal entries.\n- In floating-point arithmetic, tiny pivots during factorization indicate potential near-singular directions. The concept of numerical rank uses a tolerance to decide whether a direction is effectively zero.\n\nDesign an algorithm based on the following fundamental base:\n- The existence of $A = L L^{\\top}$ for SPD matrices and the definition of Schur complements.\n- The fact that, in a left-looking Cholesky process, the provisional squared pivot at step $k$ is $t_k = A_{kk} - \\sum_{i=0}^{k-1} L_{k i}^2$, which equals the $(k,k)$ entry of the current Schur complement.\n- A threshold test declares a pivot “numerically zero” when $t_k \\le \\tau \\cdot p_{\\max}$, where $\\tau \\in (0,1)$ is a relative tolerance and $p_{\\max} = \\max_j A_{jj}$ is a scale for the problem.\n\nYour tasks:\n1) Implement a truncated Cholesky factorization without pivoting for a given symmetric $A \\in \\mathbb{R}^{n \\times n}$ and tolerance $\\tau \\in (0,1)$:\n   - Initialize a lower-triangular matrix $L$ with zeros and a boolean drop mask $d \\in \\{ \\text{False}, \\text{True} \\}^n$.\n   - For $k = 0, 1, \\dots, n-1$ do:\n     - Compute $s_k = \\sum_{i=0}^{k-1} L_{k i}^2$ and $t_k = A_{k k} - s_k$.\n     - Let $p_{\\max} = \\max_j A_{j j}$. If $t_k \\le \\tau \\cdot p_{\\max}$, set $L_{k k} = 0$, mark $d_k = \\text{True}$, and skip computing $L_{j k}$ for $j > k$.\n     - Otherwise, set $L_{k k} = \\sqrt{\\max(t_k, 0)}$, and for each $j = k+1, \\dots, n-1$, compute\n       $$\n       u_{j k} = A_{j k} - \\sum_{i=0}^{k-1} L_{j i} L_{k i}, \\quad L_{j k} = \\frac{u_{j k}}{L_{k k}}.\n       $$\n   - Define the numerical rank estimate as $\\hat{r} = \\#\\{k : L_{k k} > 0\\}$.\n\n2) Define the truncated approximation $A_{\\text{trunc}} = L L^{\\top}$.\n\n3) Define a direction-aware regularization that only acts on dropped directions:\n   - Construct a diagonal projector $P \\in \\mathbb{R}^{n \\times n}$ with $P_{k k} = 1$ if $d_k = \\text{True}$ and $P_{k k} = 0$ otherwise.\n   - For a regularization strength $\\lambda > 0$, define the regularized matrix\n     $$\n     A_{\\text{reg}} = A_{\\text{trunc}} + \\lambda P.\n     $$\n\n4) For each test case below, compute and return the following four quantities:\n   - The estimated numerical rank $\\hat{r}$ (an integer).\n   - The Frobenius-norm reconstruction error $e_F = \\lVert A - A_{\\text{trunc}} \\rVert_F$ (a floating-point number).\n   - The smallest eigenvalue of $A_{\\text{reg}}$ (a floating-point number).\n   - The spectral condition number of $A_{\\text{reg}}$, defined as $\\kappa_2(A_{\\text{reg}}) = \\lambda_{\\max}(A_{\\text{reg}}) / \\lambda_{\\min}(A_{\\text{reg}})$ (a floating-point number).\n\nTest suite:\nProvide results for all of the following cases. All constants and numbers must be interpreted in standard real arithmetic.\n\n- Case $1$ (well-conditioned SPD): \n  $$\n  A_1 = \\begin{bmatrix}\n  5 & 1 & 0 & 0 \\\\\n  1 & 4 & 1 & 0 \\\\\n  0 & 1 & 3 & 1 \\\\\n  0 & 0 & 1 & 2\n  \\end{bmatrix}, \\quad \\tau = 10^{-12}, \\quad \\lambda = 10^{-6}.\n  $$\n\n- Case $2$ (diagonal with a very small direction):\n  $$\n  A_2 = \\operatorname{diag}(1, 10^{-8}, 10^{-12}), \\quad \\tau = 10^{-10}, \\quad \\lambda = 10^{-6}.\n  $$\n\n- Case $3$ (exactly rank-$1$):\n  $$\n  v = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad A_3 = v v^{\\top}, \\quad \\tau = 10^{-12}, \\quad \\lambda = 10^{-6}.\n  $$\n\n- Case $4$ (near-singular with off-diagonal structure):\n  Let $\\varepsilon = 10^{-8}$ and\n  $$\n  B = \\begin{bmatrix}\n  1 & 1 - \\varepsilon & 0 \\\\\n  0 & 1 & \\varepsilon \\\\\n  0 & 0 & \\varepsilon\n  \\end{bmatrix}, \\quad A_4 = B B^{\\top}, \\quad \\tau = 10^{-10}, \\quad \\lambda = 10^{-6}.\n  $$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain exactly four nested lists, one per test case, in the same order as above. Each nested list must be of the form [$ \\hat{r} $, $ e_F $, $ \\lambda_{\\min}(A_{\\text{reg}}) $, $ \\kappa_2(A_{\\text{reg}}) $]. For example:\n\"[ [r1,err1,min1,cond1],[r2,err2,min2,cond2],[r3,err3,min3,cond3],[r4,err4,min4,cond4] ]\"\nAll four entries in each nested list must be numerical scalars (an integer for $\\hat{r}$ and floats for the others). There are no physical units involved in this problem.",
            "solution": "The problem is valid. It is a well-defined exercise in numerical linear algebra, grounded in established principles of matrix factorization and regularization. The algorithm, parameters, and test cases are specified completely and consistently, allowing for a unique and verifiable solution.\n\n### Principles of Cholesky Factorization\n\nFor a symmetric positive definite (SPD) matrix $A \\in \\mathbb{R}^{n \\times n}$, the Cholesky factorization is given by $A = L L^{\\top}$, where $L \\in \\mathbb{R}^{n \\times n}$ is a lower triangular matrix with strictly positive diagonal entries. The elements of $L$ can be derived by equating the entries of $A$ with those of $L L^{\\top}$. The product $L L^{\\top}$ has entries $(L L^{\\top})_{jk} = \\sum_{i=0}^{n-1} L_{ji} L_{ki}^{\\top} = \\sum_{i=0}^{\\min(j,k)} L_{ji} L_{ki}$.\n\nLet's consider the computation of the $k$-th column of $L$, assuming columns $0, \\dots, k-1$ are already known. This is known as a left-looking or up-looking algorithm.\nFor the diagonal element $L_{kk}$, we have:\n$$\nA_{kk} = (L L^{\\top})_{kk} = \\sum_{i=0}^{k} L_{ki}^2 = \\sum_{i=0}^{k-1} L_{ki}^2 + L_{kk}^2\n$$\nRearranging for $L_{kk}$ gives:\n$$\nL_{kk} = \\sqrt{A_{kk} - \\sum_{i=0}^{k-1} L_{ki}^2}\n$$\nThe term under the square root, $t_k = A_{kk} - \\sum_{i=0}^{k-1} L_{ki}^2$, is the provisional pivot. For an SPD matrix, $t_k$ is guaranteed to be positive. This term is also the $(k,k)$-th entry of the Schur complement of the top-left $(k-1) \\times (k-1)$ block of $A$, which must be positive definite if $A$ is.\n\nFor the off-diagonal elements $L_{jk}$ where $j > k$, we have:\n$$\nA_{jk} = (L L^{\\top})_{jk} = \\sum_{i=0}^{k} L_{ji} L_{ki} = \\sum_{i=0}^{k-1} L_{ji} L_{ki} + L_{jk} L_{kk}\n$$\nRearranging for $L_{jk}$ yields:\n$$\nL_{jk} = \\frac{1}{L_{kk}} \\left( A_{jk} - \\sum_{i=0}^{k-1} L_{ji} L_{ki} \\right)\n$$\nThis requires $L_{kk} > 0$, which holds for SPD matrices.\n\n### Truncated Cholesky for Rank Detection\n\nIf $A$ is SPSD but not SPD (i.e., it is singular), at least one of the pivots $t_k$ will be zero. In floating-point arithmetic, for a near-singular matrix, a pivot $t_k$ can become a very small positive number. A standard Cholesky algorithm might proceed, but the resulting $L$ factor could be ill-conditioned, and subsequent divisions by a small $L_{kk}$ would introduce large numerical errors.\n\nThe specified algorithm introduces a threshold test to gracefully handle this situation. A pivot $t_k$ is considered numerically zero if $t_k \\le \\tau \\cdot p_{\\max}$, where $\\tau$ is a small relative tolerance and $p_{\\max} = \\max_j A_{jj}$ provides a problem-dependent scale. The use of a relative tolerance is crucial for the method to be robust to the scaling of the input matrix $A$.\n\nWhen a pivot $t_k$ is deemed numerically zero:\n1.  We set $L_{kk} = 0$. This signifies that the $k$-th direction is dependent on the previous directions $0, \\dots, k-1$.\n2.  We set a flag $d_k = \\text{True}$ to mark this column as \"dropped\".\n3.  We do not compute the rest of the column ($L_{jk}$ for $j > k$). Since $L$ is initialized to a zero matrix, these elements remain zero. This is mathematically consistent, as multiplying a column of $L$ by $0$ has no effect on the subsequent steps.\n\nThe numerical rank, $\\hat{r}$, is then the count of non-dropped columns, i.e., the number of columns for which we found a sufficiently large pivot, which is simply $\\hat{r} = \\#\\{k : L_{k k} > 0\\}$.\n\n### Matrix Approximation and Regularization\n\nFrom the computed truncated factor $L$, we can form an approximation to the original matrix, $A_{\\text{trunc}} = L L^{\\top}$. If $A$ was rank-deficient and the algorithm correctly identified the null space directions, $A_{\\text{trunc}}$ will be a low-rank approximation of $A$. The Frobenius norm of the difference, $e_F = \\lVert A - A_{\\text{trunc}} \\rVert_F$, quantifies the error of this approximation. For an exactly rank-deficient matrix (like in Case $3$), this error may be zero up to machine precision.\n\nThe matrix $A_{\\text{trunc}}$ is, by construction, SPSD but singular, as it has zero rows and columns corresponding to the dropped columns of $L$. For many applications, a strictly positive definite matrix is required. The proposed direction-aware regularization achieves this by \"lifting\" only the null directions. We construct a diagonal projector $P$ where $P_{kk}=1$ only for the dropped directions (where $d_k = \\text{True}$). The regularized matrix is:\n$$\nA_{\\text{reg}} = A_{\\text{trunc}} + \\lambda P\n$$\nwhere $\\lambda > 0$ is a small regularization parameter. This operation is equivalent to adding $\\lambda$ to the diagonal entries of $A_{\\text{trunc}}$ corresponding to the identified null-space directions. The result, $A_{\\text{reg}}$, is guaranteed to be SPD. Its smallest eigenvalue, $\\lambda_{\\min}(A_{\\text{reg}})$, will be strictly positive, and its spectral condition number, $\\kappa_2(A_{\\text{reg}}) = \\lambda_{\\max}(A_{\\text{reg}}) / \\lambda_{\\min}(A_{\\text{reg}})$, becomes well-defined and finite. This regularization is more targeted than adding $\\lambda I$ to the original matrix, as it does not perturb the well-defined part of the matrix.\n\n### Algorithmic Implementation\n\nWe shall now implement this procedure.\nThe main function will take $A$, $\\tau$, and $\\lambda$ as inputs.\n1.  Initialize an $n \\times n$ matrix $L$ to all zeros and a drop mask $d$ of length $n$ to all `False`.\n2.  Compute the scale factor $p_{\\max} = \\max_{j} A_{jj}$.\n3.  Iterate $k$ from $0$ to $n-1$:\n    a. Compute the dot product $s_k = \\sum_{i=0}^{k-1} L_{ki}^2 = L[k, :k] \\cdot L[k, :k]$.\n    b. Compute the provisional pivot $t_k = A_{kk} - s_k$.\n    c. If $t_k \\le \\tau \\cdot p_{\\max}$: set $L_{kk}=0$ and $d_k=\\text{True}$.\n    d. Else: set $L_{kk} = \\sqrt{t_k}$ (note `max(t_k, 0)` is used for robustness but $t_k$ is positive here), then compute the rest of the column: $u_{jk} = A_{jk} - \\sum_{i=0}^{k-1} L_{ji} L_{ki}$ can be vectorized as a matrix-vector product `A[j>k, k] - L[j>k, :k] @ L[k, :k]`, followed by division $L_{jk} = u_{jk} / L_{kk}$.\n4.  After the loop, compute the four required quantities:\n    a. Rank $\\hat{r}$: count of `True` values in `np.diag(L) > 0`.\n    b. Truncated matrix $A_{\\text{trunc}} = L L^{\\top}$.\n    c. Reconstruction error $e_F = \\lVert A - A_{\\text{trunc}} \\rVert_F$.\n    d. Projector $P$ from the mask $d$.\n    e. Regularized matrix $A_{\\text{reg}} = A_{\\text{trunc}} + \\lambda P$.\n    f. Eigenvalues of $A_{\\text{reg}}$ (using `numpy.linalg.eigvalsh` for symmetric matrices).\n    g. $\\lambda_{\\min}(A_{\\text{reg}})$ and the condition number $\\kappa_2(A_{\\text{reg}})$.\n\nThis procedure will be applied to each of the four test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef truncated_cholesky_regularization(A, tau, lambda_reg):\n    \"\"\"\n    Performs truncated Cholesky factorization, regularization, and analysis.\n\n    Args:\n        A (np.ndarray): The symmetric input matrix.\n        tau (float): The relative tolerance for rank detection.\n        lambda_reg (float): The regularization strength.\n\n    Returns:\n        tuple: A tuple containing (r_hat, e_F, min_eig_reg, cond_reg).\n    \"\"\"\n    n = A.shape[0]\n    L = np.zeros((n, n), dtype=float)\n    d = np.zeros(n, dtype=bool)\n\n    # The problem statement defines p_max based on the original matrix A.\n    # This ensures a consistent scale throughout the factorization.\n    diag_A = np.diag(A)\n    # Handle the edge case of a zero matrix, where max would fail.\n    p_max = np.max(diag_A) if diag_A.size > 0 else 1.0\n    if p_max <= 0: # Handle non-positive diagonal for robustness\n        p_max = 1.0\n\n    threshold = tau * p_max\n\n    for k in range(n):\n        # Compute s_k = sum_{i=0}^{k-1} L_ki^2\n        # This is the dot product of the k-th row of L up to column k-1\n        s_k = np.dot(L[k, :k], L[k, :k])\n        \n        # Compute the provisional pivot t_k\n        t_k = A[k, k] - s_k\n\n        if t_k <= threshold:\n            L[k, k] = 0.0\n            d[k] = True\n            # The rest of column k (L[j>k, k]) remains zero as initialized.\n        else:\n            # L_kk is guaranteed to be positive.\n            # max(t_k, 0) is a safeguard against minor fp errors pushing t_k negative\n            L[k, k] = np.sqrt(max(t_k, 0))\n            \n            # Compute remaining elements in column k\n            if k < n - 1:\n                # u_jk = A_jk - sum_{i=0}^{k-1} L_ji L_ki\n                # This is a vector operation for all j > k\n                u_col_k = A[k+1:n, k] - np.dot(L[k+1:n, :k], L[k, :k])\n                L[k+1:n, k] = u_col_k / L[k, k]\n\n    # 1) Estimated numerical rank\n    r_hat = np.sum(np.diag(L) > 0)\n\n    # 2) Truncated approximation and reconstruction error\n    A_trunc = np.dot(L, L.T)\n    e_F = np.linalg.norm(A - A_trunc, 'fro')\n\n    # 3) Direction-aware regularization\n    P = np.diag(d.astype(float))\n    A_reg = A_trunc + lambda_reg * P\n\n    # 4) Smallest eigenvalue and condition number of A_reg\n    # A_reg is symmetric by construction, use eigvalsh for efficiency and stability.\n    eigenvalues_reg = np.linalg.eigvalsh(A_reg)\n    \n    min_eig_reg = np.min(eigenvalues_reg)\n    max_eig_reg = np.max(eigenvalues_reg)\n\n    if min_eig_reg <= 0:\n        # This case should ideally not happen with lambda > 0,\n        # but as a safeguard against catastrophic cancellation or zero lambda.\n        cond_reg = np.inf\n    else:\n        cond_reg = max_eig_reg / min_eig_reg\n\n    return [int(r_hat), float(e_F), float(min_eig_reg), float(cond_reg)]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Case 1\n    A1 = np.array([\n        [5, 1, 0, 0],\n        [1, 4, 1, 0],\n        [0, 1, 3, 1],\n        [0, 0, 1, 2]\n    ], dtype=float)\n    tau1 = 1e-12\n    lambda1 = 1e-6\n    \n    # Case 2\n    A2 = np.diag([1.0, 1e-8, 1e-12])\n    tau2 = 1e-10\n    lambda2 = 1e-6\n    \n    # Case 3\n    v3 = np.array([1, 2, 3], dtype=float).reshape(-1, 1)\n    A3 = np.dot(v3, v3.T)\n    tau3 = 1e-12\n    lambda3 = 1e-6\n    \n    # Case 4\n    eps4 = 1e-8\n    B4 = np.array([\n        [1, 1 - eps4, 0],\n        [0, 1, eps4],\n        [0, 0, eps4]\n    ], dtype=float)\n    A4 = np.dot(B4, B4.T)\n    tau4 = 1e-10\n    lambda4 = 1e-6\n    \n    test_cases = [\n        (A1, tau1, lambda1),\n        (A2, tau2, lambda2),\n        (A3, tau3, lambda3),\n        (A4, tau4, lambda4),\n    ]\n\n    results = []\n    for A, tau, lambda_reg in test_cases:\n        result = truncated_cholesky_regularization(A, tau, lambda_reg)\n        results.append(result)\n\n    # Format the final output string\n    # Convert each inner list to a string representation\n    results_str = [f\"[{','.join(map(str, res))}]\" for res in results]\n    # Join the inner list strings into the final format\n    final_output = f\"[{','.join(results_str)}]\"\n\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}