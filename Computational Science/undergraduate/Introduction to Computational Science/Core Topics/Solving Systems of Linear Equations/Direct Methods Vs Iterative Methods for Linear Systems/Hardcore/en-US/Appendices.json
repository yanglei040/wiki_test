{
    "hands_on_practices": [
        {
            "introduction": "When faced with a large-scale linear system, the first practical question is often not about speed, but memory: \"Will the solver and its data structures fit into my computer's RAM?\". This exercise puts you in the role of a computational scientist making this critical decision. You will perform a \"back-of-the-envelope\" calculation to compare the memory footprint of a direct method ($LU$ factorization), which can suffer from unpredictable fill-in, against an iterative method (GMRES), which typically has more modest and predictable memory costs. This practice () is fundamental for understanding one of the primary motivations for using iterative solvers in large-scale applications.",
            "id": "3118498",
            "problem": "A sparse linear system arises from a uniform discretization of a partial differential equation on a structured grid, producing a matrix $A \\in \\mathbb{R}^{n \\times n}$ with average sparsity of $z$ nonzero entries per row. Assume the matrix is stored in Compressed Sparse Row (CSR) format, where each nonzero value is stored in double precision ($8$ bytes), each column index is stored as a $32$-bit integer ($4$ bytes), and the row pointer array has $n+1$ entries of $32$-bit integers ($4$ bytes each). Consider two solution strategies:\n- A direct method using Lower-Upper (LU) factorization, where the combined number of nonzeros in $L$ and $U$ is modeled by a fill factor $f$ times the number of nonzeros in $A$, and both $L$ and $U$ are stored in CSR with the same per-entry and per-pointer costs as $A$.\n- An iterative method using Generalized Minimal Residual (GMRES) with restart parameter $m$, denoted GMRES($m$), which stores the Arnoldi basis of $(m+1)$ vectors of length $n$ in double precision, an upper Hessenberg matrix of size $(m+1) \\times m$ in double precision, and three additional working vectors of length $n$ (the current iterate, the right-hand side, and the residual), all in double precision. Assume the matrix $A$ must be stored in CSR during GMRES.\n\nGiven the parameters $n = 10^{6}$, $z = 7$, $f = 5$, and a memory cap of $3.0 \\times 10^{8}$ bytes, use the CSR memory model to:\n1. Derive an expression for the CSR memory in bytes to store $A$ in terms of $n$ and $z$.\n2. Derive an expression for the CSR memory in bytes to store the LU factors in terms of $n$, $z$, and $f$.\n3. Derive an expression for the total memory in bytes required by GMRES($m$), including the storage for $A$, the Arnoldi basis, the Hessenberg matrix, and the three working vectors.\n\nThen determine the largest integer restart parameter $m$ such that GMRES($m$) is feasible under the memory cap while LU is not. Report only the value of $m$. No rounding is required; provide the exact integer value as your final answer.",
            "solution": "The problem will be validated and, if deemed valid, solved by following the specified derivation steps.\n\n**Problem Validation**\n\n*   **Givens Extraction:**\n    *   Matrix $A \\in \\mathbb{R}^{n \\times n}$.\n    *   Average sparsity: $z$ nonzeros per row.\n    *   Storage: Compressed Sparse Row (CSR).\n    *   Data types: double precision ($8$ bytes) for values, $32$-bit integer ($4$ bytes) for indices and pointers.\n    *   Row pointer array size: $n+1$ entries.\n    *   Direct method (LU): fill factor $f$, factors $L$ and $U$ stored in CSR.\n    *   Iterative method (GMRES($m$)): restart parameter $m$, stores Arnoldi basis of $(m+1)$ vectors (length $n$, double), Hessenberg matrix of size $(m+1) \\times m$ (double), and $3$ working vectors (length $n$, double). Matrix $A$ is also stored.\n    *   Parameters: $n = 10^{6}$, $z = 7$, $f = 5$.\n    *   Constraint: Memory cap $\\le 3.0 \\times 10^{8}$ bytes.\n\n*   **Validation Verdict:**\n    The problem is scientifically grounded, well-posed, and objective. It is based on standard, formalizable concepts in numerical linear algebra and computational science, namely the memory complexity of CSR storage, LU factorization with fill-in, and the GMRES algorithm. All parameters and models are explicitly defined, making the problem self-contained and free of ambiguity. The values provided are realistic for large-scale scientific computing. Therefore, the problem is deemed **valid**.\n\n**Solution Derivation**\n\nThe solution requires deriving memory expressions for three different scenarios and then using these expressions to find the maximum allowable restart parameter $m$ for GMRES.\n\n**1. Memory for Matrix $A$ in CSR Format**\nLet $M_A$ be the memory in bytes required to store matrix $A$. The CSR format consists of three arrays: one for nonzero values, one for their column indices, and one for row pointers.\n- The number of rows is $n$. With an average of $z$ nonzeros per row, the total number of nonzero entries is $nnz(A) = nz$.\n- Memory for nonzero values: Each of the $nz$ values is a double, requiring $8$ bytes. Total: $8nz$ bytes.\n- Memory for column indices: Each of the $nz$ nonzeros has a corresponding column index, stored as a $32$-bit integer ($4$ bytes). Total: $4nz$ bytes.\n- Memory for row pointers: This array has $n+1$ entries, each a $32$-bit integer ($4$ bytes). Total: $4(n+1)$ bytes.\nThe total memory for $A$ is the sum of these components:\n$$M_A(n, z) = 8nz + 4nz + 4(n+1)$$\n$$M_A(n, z) = 12nz + 4n + 4$$\n\n**2. Memory for LU Factors in CSR Format**\nLet $M_{LU}$ be the memory for the $L$ and $U$ factors. The problem states the combined number of nonzeros in $L$ and $U$ is $f$ times the number of nonzeros in $A$.\n- Total nonzeros in factors: $nnz(L) + nnz(U) = f \\times nnz(A) = fnz$.\n- We model the storage of the factors as a single logical entity in CSR format, with $fnz$ nonzero entries and $n$ rows (requiring $n+1$ row pointers), consistent with common practice for fill-in estimation.\n- Memory for nonzero values: $8(fnz)$ bytes.\n- Memory for column indices: $4(fnz)$ bytes.\n- Memory for row pointers: $4(n+1)$ bytes.\nThe total memory for the LU factors is:\n$$M_{LU}(n, z, f) = 8fnz + 4fnz + 4(n+1)$$\n$$M_{LU}(n, z, f) = 12fnz + 4n + 4$$\n\n**3. Total Memory for GMRES($m$)**\nLet $M_{GMRES}(m)$ be the total memory for the GMRES($m$) algorithm. This includes storing matrix $A$ and the data structures required by the algorithm.\n- Storage for matrix $A$: $M_A = 12nz + 4n + 4$.\n- Storage for the Arnoldi basis: This basis consists of $m+1$ vectors, each of length $n$ and stored in double precision. Memory: $8n(m+1)$ bytes.\n- Storage for the Hessenberg matrix: This is a dense $(m+1) \\times m$ matrix stored in double precision. Memory: $8m(m+1)$ bytes.\n- Storage for working vectors: Three vectors (current iterate, right-hand side, residual) of length $n$ in double precision. Memory: $3 \\times 8n = 24n$ bytes.\nThe total memory is the sum of these components:\n$$M_{GMRES}(n, z, m) = (12nz + 4n + 4) + 8n(m+1) + 8m(m+1) + 24n$$\nCombining terms:\n$$M_{GMRES}(n, z, m) = 12nz + (4n + 8n + 24n) + 8nm + 8m^2 + 8m + 4$$\n$$M_{GMRES}(n, z, m) = 12nz + 36n + 4 + 8nm + 8m^2 + 8m$$\n\n**Determining the Largest Integer Restart Parameter $m$**\n\nWe are given the parameters $n = 10^{6}$, $z = 7$, $f = 5$, and a memory cap of $M_{cap} = 3.0 \\times 10^{8}$ bytes. We need to find the largest integer $m$ such that $M_{LU} > M_{cap}$ and $M_{GMRES}(m) \\le M_{cap}$.\n\nFirst, verify that the LU method is not feasible:\n$$M_{LU} = 12(5)(10^{6})(7) + 4(10^{6}) + 4$$\n$$M_{LU} = 420 \\times 10^{6} + 4 \\times 10^{6} + 4$$\n$$M_{LU} = 424,000,004 \\text{ bytes}$$\nSince $424,000,004 > 3.0 \\times 10^{8}$, the LU factorization method exceeds the memory cap and is not feasible. This fulfills the first condition.\n\nNext, we establish the inequality for GMRES($m$):\n$$M_{GMRES}(m) \\le 3.0 \\times 10^{8}$$\n$$12(10^{6})(7) + 36(10^{6}) + 4 + 8(10^{6})m + 8m^2 + 8m \\le 300,000,000$$\n$$84,000,000 + 36,000,000 + 4 + 8,000,000m + 8m^2 + 8m \\le 300,000,000$$\n$$120,000,004 + 8,000,008m + 8m^2 \\le 300,000,000$$\nThis simplifies to a quadratic inequality in $m$:\n$$8m^2 + 8,000,008m - (300,000,000 - 120,000,004) \\le 0$$\n$$8m^2 + 8,000,008m - 179,999,996 \\le 0$$\nFor the expected range of $m$, the term $8m^2$ is much smaller than the term $8,000,008m$. We can approximate the solution by examining the linear part:\n$$8,000,008m \\approx 179,999,996$$\n$$m \\approx \\frac{179,999,996}{8,000,008} \\approx \\frac{1.8 \\times 10^{8}}{8.0 \\times 10^{6}} = \\frac{180}{8} = 22.5$$\nThis suggests the largest integer value for $m$ is $22$. We will verify this by testing $m = 22$ and $m = 23$ in the full quadratic inequality.\n\nFor $m = 22$:\n$$8(22)^2 + 8,000,008(22) - 179,999,996$$\n$$8(484) + 176,000,176 - 179,999,996$$\n$$3,872 + 176,000,176 - 179,999,996$$\n$$176,004,048 - 179,999,996 = -3,995,948$$\nSince $-3,995,948 \\le 0$, the condition is satisfied for $m=22$.\n\nFor $m = 23$:\n$$8(23)^2 + 8,000,008(23) - 179,999,996$$\n$$8(529) + 184,000,184 - 179,999,996$$\n$$4,232 + 184,000,184 - 179,999,996$$\n$$184,004,416 - 179,999,996 = 4,004,420$$\nSince $4,004,420 > 0$, the condition is not satisfied for $m=23$.\n\nThus, the largest integer restart parameter $m$ for which GMRES($m$) is feasible is $22$.",
            "answer": "$$\\boxed{22}$$"
        },
        {
            "introduction": "Beyond raw memory capacity, the *structure* of a matrix is a deciding factor in choosing a solver. This hands-on coding practice () explores how the symmetry of a sparse matrix's nonzero pattern influences both direct and iterative methods. You will see firsthand how a symmetric pattern can be exploited by fill-reducing orderings to make $LU$ factorization more efficient, and more importantly, how symmetry is a non-negotiable requirement for using powerful and fast iterative solvers like the Conjugate Gradient (CG) method. This exercise reveals that the choice of an algorithm is not made in a vacuum; it is deeply intertwined with the mathematical properties of the problem itself.",
            "id": "3118467",
            "problem": "You are given the task of constructing and analyzing sparse matrices to examine how the symmetry of the nonzero pattern influences factorization fill-in in lower-upper (LU) factorization and the applicability of the Conjugate Gradient (CG) iterative method. The mathematical basis is the linear system definition $A x = b$, the concept of Gaussian elimination leading to LU factorization, and the requirement of Symmetric Positive Definite (SPD) matrices for the Conjugate Gradient (CG) method.\n\nDefinitions and core facts to use:\n- A linear system is specified by a square matrix $A \\in \\mathbb{R}^{N \\times N}$ and a vector $b \\in \\mathbb{R}^{N}$, and seeks $x \\in \\mathbb{R}^{N}$ satisfying $A x = b$.\n- LU factorization represents $A$ (after possible row and column permutations) in the form $A = L U$, where $L$ is lower triangular and $U$ is upper triangular. Gaussian elimination introduces new nonzero entries, a phenomenon known as fill-in. The number of fill-in entries can be quantified by the difference between the total nonzeros in $L$ and $U$ and those originally in $A$.\n- The Conjugate Gradient (CG) method is defined for Symmetric Positive Definite (SPD) matrices. A matrix $A$ is SPD if $A^{\\mathsf{T}} = A$ and $x^{\\mathsf{T}} A x > 0$ for all nonzero vectors $x$.\n- The nonzero pattern symmetry refers to whether the support (set of indices of nonzero entries) of $A$ equals the support of $A^{\\mathsf{T}}$, regardless of numerical values.\n\nYour program must construct three test matrices and, for each, compute four quantities: whether the nonzero pattern is symmetric, the fill-in count under LU with natural column ordering, the fill-in count under LU with a fill-reducing column ordering, and whether CG is applicable (i.e., whether the matrix is SPD). All mathematical quantities must be handled as specified below, and the final output must be a single line in the required format.\n\nMatrix construction and test suite:\n- Test Case $1$ (symmetric positive definite baseline): Let $n = 6$, so $N = n^2 = 36$. Define the two-dimensional discrete Laplacian on a $n \\times n$ grid with Dirichlet boundary conditions as\n$$\nT_1 = \\mathrm{diags}\\left(\\{-\\mathbf{1}_{n-1},\\ 2\\mathbf{1}_{n},\\ -\\mathbf{1}_{n-1}\\},\\ \\{-1, 0, 1\\}\\right),\n$$\n$$\nI_n = \\mathrm{eye}(n),\n$$\n$$\nA_1 = \\mathrm{kron}(I_n, T_1) + \\mathrm{kron}(T_1, I_n).\n$$\nThis $A_1 \\in \\mathbb{R}^{N \\times N}$ is symmetric positive definite and has a symmetric nonzero pattern.\n- Test Case $2$ (nonsymmetric extreme pattern): Let $N = 36$. Define\n$$\nI_N = \\mathrm{eye}(N),\n$$\n$$\nS = \\mathrm{diags}\\left(\\{\\mathbf{1}_{N-1}\\},\\ \\{1\\}\\right),\n$$\n$$\nR = \\text{the sparse matrix whose first row is all ones},\n$$\n$$\nC = \\text{the sparse matrix whose last column is all ones}.\n$$\nSet\n$$\nA_2 = I_N + S + R + C.\n$$\nThis $A_2 \\in \\mathbb{R}^{N \\times N}$ is designed to be strongly nonsymmetric in its nonzero pattern and well-conditioned enough to admit LU factorization.\n- Test Case $3$ (symmetric indefinite): Using $A_1$ above, let\n$$\nA_3 = A_1 - 10 I_N,\n$$\nwhich preserves the symmetric nonzero pattern but breaks positive definiteness.\n\nFor each matrix $A_k$ for $k \\in \\{1,2,3\\}$, perform the following computations:\n- Nonzero pattern symmetry boolean: Determine whether the support of $A_k$ equals the support of $A_k^{\\mathsf{T}}$ (this is true if, for every pair $(i,j)$ such that $A_k(i,j) \\neq 0$, we also have $A_k(j,i) \\neq 0$).\n- Fill-in count under LU with natural column ordering: Compute LU factorization of $A_k$ using natural column ordering, then compute the integer\n$\\mathrm{fill\\_nat}(A_k) = \\mathrm{nnz}(L) + \\mathrm{nnz}(U) - \\mathrm{nnz}(A_k)$,\nwhere $\\mathrm{nnz}(\\cdot)$ denotes the number of nonzero entries. You should use a standard sparse LU from a numerical library with natural column ordering to obtain $L$ and $U$.\n- Fill-in count under LU with a fill-reducing column ordering: Repeat the computation using a fill-reducing column ordering such as Column Approximate Minimum Degree (COLAMD), to obtain\n$\\mathrm{fill\\_colamd}(A_k) = \\mathrm{nnz}(L) + \\mathrm{nnz}(U) - \\mathrm{nnz}(A_k)$.\n- CG applicability boolean: Determine whether $A_k$ is suitable for Conjugate Gradient by checking symmetry and attempting to verify positive definiteness via a Cholesky factorization test. Report true if and only if $A_k$ is symmetric and positive definite.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Aggregate the results for all three test cases in the following order:\n`[ pat_sym(A1), fill_nat(A1), fill_colamd(A1), cg_app(A1), pat_sym(A2), fill_nat(A2), fill_colamd(A2), cg_app(A2), pat_sym(A3), fill_nat(A3), fill_colamd(A3), cg_app(A3) ]`.\nHere `pat_sym(A_k)` and `cg_app(A_k)` are booleans, and `fill_nat(A_k)` and `fill_colamd(A_k)` are integers. No physical units apply in this problem. Your program must be self-contained, require no input, and produce exactly this single-line output.",
            "solution": "We start from the foundational definition of a linear system $A x = b$ with $A \\in \\mathbb{R}^{N \\times N}$. Direct methods, specifically Gaussian elimination, lead to a lower-upper (LU) factorization $A = L U$ (up to permutations), where $L$ is lower triangular and $U$ is upper triangular. During elimination, new nonzero entries can be created even when $A$ is sparse; this phenomenon is called fill-in. The amount of fill-in can be measured by counting the increase in nonzero entries in the factors compared to the original matrix. Formally, for a fixed ordering, the fill-in count is $\\mathrm{nnz}(L) + \\mathrm{nnz}(U) - \\mathrm{nnz}(A)$, where $\\mathrm{nnz}(\\cdot)$ denotes the number of nonzero entries.\n\nIterative methods approach the solution differently. The Conjugate Gradient (CG) method, in particular, requires that the coefficient matrix be Symmetric Positive Definite (SPD). The SPD conditions are $A^{\\mathsf{T}} = A$ (symmetry) and $x^{\\mathsf{T}} A x > 0$ for all nonzero $x \\in \\mathbb{R}^{N}$ (positive definiteness). An equivalent practical test for positive definiteness is the existence of a Cholesky factorization $A = R^{\\mathsf{T}} R$ for an upper triangular $R$, which succeeds if and only if $A$ is SPD.\n\nInfluence of nonzero pattern symmetry on fill-in can be reasoned via the elimination graph. The sparsity pattern of $A$ defines a graph on indices $\\{1,\\dots,N\\}$ with edges representing nonzero off-diagonal entries. Gaussian elimination corresponds to eliminating vertices, which introduces edges (fill-in) among their neighbors. Symmetric patterns often allow more balanced graph structures, and fill-reducing orderings (such as Column Approximate Minimum Degree (COLAMD)) leverage this structure to minimize the degree growth during elimination, thus reducing fill-in. In contrast, strongly nonsymmetric patterns, especially those with dense rows or columns forming star or arrowhead structures, can lead to more substantial fill-in because elimination connects many nodes densely.\n\nAlgorithmic design tied to principles:\n- Construct $A_1$ as the two-dimensional discrete Laplacian on a $n \\times n$ grid with Dirichlet boundaries, using Kronecker sums. Let $n = 6$, hence $N = n^2 = 36$. The one-dimensional Laplacian $T_1 = \\mathrm{diags}(\\{-\\mathbf{1}_{n-1}, 2\\mathbf{1}_{n}, -\\mathbf{1}_{n-1}\\}, \\{-1, 0, 1\\})$ is symmetric and strictly diagonally dominant, and the Kronecker sum $A_1 = \\mathrm{kron}(I_n, T_1) + \\mathrm{kron}(T_1, I_n)$ is Symmetric Positive Definite (SPD) and has a symmetric nonzero pattern. By the SPD property, $A_1$ admits a Cholesky factorization, and thus CG is applicable. For fill-in, we use sparse LU factorization with two column orderings: natural ordering and a fill-reducing ordering (COLAMD) to quantify differences.\n- Construct $A_2$ as a matrix with pronounced nonsymmetric pattern: $A_2 = I_N + S + R + C$ where $I_N$ is the identity, $S$ is the superdiagonal, $R$ is a dense first row, and $C$ is a dense last column. The identity ensures nonsingularity. The pattern symmetry check compares the supports of $A_2$ and $A_2^{\\mathsf{T}}$; dense first row and last column break symmetry of the support. CG is inapplicable because the pattern is not symmetric, and the matrix is not SPD. LU fill-in tends to be larger in nonsymmetric, star-like structures because elimination creates many new couplings; we again measure fill-in under natural and COLAMD orderings.\n- Construct $A_3 = A_1 - 10 I_N$. This preserves the symmetric nonzero pattern but destroys positive definiteness because subtracting a sufficiently large multiple of the identity shifts all eigenvalues left, making them negative (since $A_1$ has bounded positive eigenvalues for this size). The Cholesky test fails, so CG is not applicable. LU fill-in is driven by the pattern and ordering rather than the sign of values; thus $A_3$ exhibits fill-in similar to $A_1$ for a fixed ordering, with possible minor differences due to pivoting strategies.\n\nImplementation details aligned with the above:\n- Pattern symmetry is tested by comparing the set of nonzero indices (support) of $A$ to that of $A^{\\mathsf{T}}$; equality signifies symmetry.\n- Fill-in counts are computed using a sparse LU routine that provides $L$ and $U$. We compute $\\mathrm{nnz}(L) + \\mathrm{nnz}(U) - \\mathrm{nnz}(A)$ for both natural column ordering and COLAMD column ordering. Row pivoting for numerical stability is permitted internally and is part of realistic direct methods.\n- CG applicability is determined by two checks: (i) symmetry via $A \\stackrel{?}{=} A^{\\mathsf{T}}$ and (ii) positive definiteness via attempting a dense Cholesky factorization; success implies SPD.\n\nTest suite and outputs:\n- Test Case $1$ uses the SPD Laplacian $A_1$ with $N = 36$.\n- Test Case $2$ uses the nonsymmetric arrowhead/star $A_2$ with $N = 36$.\n- Test Case $3$ uses the symmetric indefinite $A_3 = A_1 - 10 I_N$ with $N = 36$.\nFor each $A_k$, the program outputs booleans `pat_sym(A_k)` and `cg_app(A_k)`, and integers `fill_nat(A_k)` and `fill_colamd(A_k)`. All twelve results are printed in a single comma-separated list within square brackets in the exact order specified in the problem statement.\n\nThis design directly reflects the interplay between nonzero pattern symmetry and direct versus iterative method behavior: symmetric patterns enable effective fill-reducing orderings and allow CG when SPD holds, whereas nonsymmetric patterns typically induce larger fill and invalidate CG.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import diags, eye, kron, csr_matrix, coo_matrix, csc_matrix\nfrom scipy.sparse.linalg import splu\n\ndef build_laplacian_2d(n: int) -> csr_matrix:\n    \"\"\"\n    Build the 2D discrete Laplacian (Dirichlet) on an n x n grid using Kronecker sums.\n    Resulting matrix is SPD and has symmetric sparsity pattern.\n    \"\"\"\n    # 1D Laplacian: tridiagonal with [ -1, 2, -1 ]\n    main = 2.0 * np.ones(n)\n    off = -1.0 * np.ones(n - 1)\n    T1 = diags([off, main, off], offsets=[-1, 0, 1], shape=(n, n), format=\"csr\")\n    In = eye(n, format=\"csr\")\n    A = kron(In, T1, format=\"csr\") + kron(T1, In, format=\"csr\")\n    return A\n\ndef build_nonsymmetric_arrow(N: int) -> csr_matrix:\n    \"\"\"\n    Build a sparse matrix with a strongly nonsymmetric pattern:\n    Identity + superdiagonal + dense first row + dense last column.\n    \"\"\"\n    A = eye(N, format=\"csr\")\n    # Superdiagonal ones\n    A += diags([np.ones(N - 1)], offsets=[1], shape=(N, N), format=\"csr\")\n    # Dense first row of ones\n    row0 = np.zeros(N, dtype=int)\n    cols = np.arange(N, dtype=int)\n    data = np.ones(N)\n    R = csr_matrix((data, (row0, cols)), shape=(N, N))\n    A += R\n    # Dense last column of ones\n    rows = np.arange(N, dtype=int)\n    col_last = np.full(N, N - 1, dtype=int)\n    C = csr_matrix((np.ones(N), (rows, col_last)), shape=(N, N))\n    A += C\n    return A\n\ndef is_pattern_symmetric(A: csr_matrix) -> bool:\n    \"\"\"\n    Check whether the sparsity pattern of A equals that of A^T.\n    \"\"\"\n    cooA = A.tocoo()\n    coordsA = set(zip(cooA.row.tolist(), cooA.col.tolist()))\n    cooAT = A.T.tocoo()\n    coordsAT = set(zip(cooAT.row.tolist(), cooAT.col.tolist()))\n    return coordsA == coordsAT\n\ndef fill_in_count(A: csr_matrix, permc_spec: str) -> int:\n    \"\"\"\n    Compute fill-in count for sparse LU: nnz(L) + nnz(U) - nnz(A),\n    using specified column permutation strategy (permc_spec).\n    \"\"\"\n    # Convert to CSC for splu\n    Acsc = A.tocsc()\n    lu = splu(Acsc, permc_spec=permc_spec)\n    L = lu.L\n    U = lu.U\n    return L.nnz + U.nnz - A.nnz\n\ndef cg_applicable(A: csr_matrix) -> bool:\n    \"\"\"\n    Determine CG applicability: True iff A is symmetric and positive definite.\n    We test symmetry and attempt dense Cholesky.\n    \"\"\"\n    dense = A.toarray()\n    if not np.allclose(dense, dense.T, atol=1e-12):\n        return False\n    try:\n        np.linalg.cholesky(dense)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\ndef solve():\n    # Define test cases: three matrices as specified in the problem statement.\n    n = 6\n    N = n * n\n\n    # Test Case 1: SPD Laplacian\n    A1 = build_laplacian_2d(n)\n\n    # Test Case 2: Nonsymmetric arrow/star\n    A2 = build_nonsymmetric_arrow(N)\n\n    # Test Case 3: Symmetric indefinite (shifted Laplacian)\n    A3 = A1 - 10.0 * eye(N, format=\"csr\")\n\n    test_cases = [A1, A2, A3]\n\n    results = []\n    for A in test_cases:\n        # Pattern symmetry boolean\n        pat_sym = is_pattern_symmetric(A)\n        # Fill-in with natural ordering\n        fill_nat = fill_in_count(A, permc_spec=\"NATURAL\")\n        # Fill-in with COLAMD ordering\n        fill_colamd = fill_in_count(A, permc_spec=\"COLAMD\")\n        # CG applicability boolean\n        cg_app = cg_applicable(A)\n        # Append in required order per test case\n        results.extend([pat_sym, fill_nat, fill_colamd, cg_app])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once we have established that an iterative method like Conjugate Gradient (CG) is a viable choice, the next question is \"How fast will it converge?\". This exercise () bridges the gap between abstract theory and practical performance. You will implement the CG method and compare its observed iteration count against a famous theoretical bound derived from the matrix's spectral condition number, $\\kappa(A)$. This comparison will give you a crucial insight: while theoretical bounds provide a valuable worst-case guarantee, the actual performance of iterative methods is often much better, highlighting the importance of empirical testing.",
            "id": "3118417",
            "problem": "You are given the task of comparing a predicted iteration bound derived from the spectral condition number to observed iteration counts for the Conjugate Gradient (CG) method when solving symmetric positive definite linear systems. Your methodology must be rooted in core definitions and well-tested facts: a symmetric positive definite matrix has real positive eigenvalues, the spectral condition number is defined as $\\kappa_2(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$, and the Conjugate Gradient method solves $A x = b$ for symmetric positive definite $A$. You must use the classical worst-case convergence theory for Conjugate Gradient, which bounds the energy norm of the error using a function of $\\sqrt{\\kappa_2(A)}$, to produce a conservative iteration count that guarantees a prescribed relative accuracy in the $A$-norm. Do not assume any shortcuts not implied by these fundamentals.\n\nYour program must implement the following methodology:\n\n1. For each test case, construct the symmetric positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$ and the right-hand side vector $b \\in \\mathbb{R}^n$ exactly as specified in the test suite below.\n\n2. Compute a reference solution $x_\\star$ using a direct method solver for $A x = b$. Use this $x_\\star$ to evaluate the relative error in the $A$-norm during Conjugate Gradient iterations. The $A$-norm of a vector $z$ is defined by $\\|z\\|_A = \\sqrt{z^\\top A z}$.\n\n3. Implement the Conjugate Gradient (CG) method starting from the zero initial guess $x_0 = 0$. After each iteration $k$, compute the relative $A$-norm of the error $\\|x_k - x_\\star\\|_A / \\|x_0 - x_\\star\\|_A$. Stop as soon as this relative error is less than or equal to the prescribed tolerance $\\varepsilon$, or after at most $n$ iterations. Record the observed iteration count $k_{\\text{obs}}$ required to meet the tolerance (or $n$ if the tolerance is not reached earlier).\n\n4. Compute the spectral condition number $\\kappa_2(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$ from the eigenvalues of $A$.\n\n5. Using the classical worst-case energy-norm convergence bound for the Conjugate Gradient method that depends on $\\sqrt{\\kappa_2(A)}$ and the tolerance $\\varepsilon$, produce a conservative integer prediction $k_{\\text{pred}}$ of the number of iterations needed to guarantee the relative $A$-norm error is at most $\\varepsilon$ starting from $x_0 = 0$. Handle the special case $\\kappa_2(A) = 1$ sensibly.\n\n6. For each test case, output the triple `[k_pred, k_obs, flag]`, where `flag` is a boolean that is true if and only if $k_{\\text{obs}} \\le k_{\\text{pred}}$.\n\nTest suite. Construct the following five symmetric positive definite systems and tolerances. In every case, the arithmetic is unitless.\n\n- Case 1 (Mildly conditioned diagonal): Let $n = 50$, $A = \\mathrm{diag}(d)$ with $d_i$ linearly spaced from $1$ to $2$ for $i = 1,\\dots,n$. Let $b_i = \\sin(i)$ for $i = 1,\\dots,n$. Let $\\varepsilon = 10^{-8}$.\n\n- Case 2 (One-dimensional Poisson tridiagonal): Let $n = 80$, $A$ be tridiagonal with $2$ on the diagonal and $-1$ on the first sub- and super-diagonals. Let $b_i = \\cos(i)$ for $i = 1,\\dots,n$. Let $\\varepsilon = 10^{-6}$.\n\n- Case 3 (Highly ill-conditioned diagonal): Let $n = 60$, $A = \\mathrm{diag}(d)$ with $d_i$ logarithmically spaced from $1$ to $10^6$ for $i = 1,\\dots,n$. Let $b_i = \\sin(0.5\\, i)$ for $i = 1,\\dots,n$. Let $\\varepsilon = 10^{-4}$.\n\n- Case 4 (Deterministic dense random-like SPD): Let $n = 40$. Construct a deterministic matrix $M \\in \\mathbb{R}^{n \\times n}$ with entries $M_{ij} = \\sin(i + j) + \\cos(i - 2 j)$ for $i,j = 1,\\dots,n$. Compute the reduced $QR$ factorization $M = Q R$ with $Q \\in \\mathbb{R}^{n \\times n}$ orthonormal. Let $D = \\mathrm{diag}(d)$ with $d_i$ linearly spaced from $1$ to $100$. Set $A = Q^\\top D Q$. Let $b_i = \\sin(i) + \\cos(2 i)$ for $i = 1,\\dots,n$. Let $\\varepsilon = 10^{-8}$.\n\n- Case 5 (Hilbert matrix): Let $n = 10$, $A_{ij} = 1/(i + j - 1)$ for $i,j = 1,\\dots,n$. Let $b_i = 1$ for $i = 1,\\dots,n$. Let $\\varepsilon = 10^{-6}$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a three-element list in the order `[k_pred, k_obs, flag]`. For example, the format must be exactly like\n\"[ [kpred1,kobs1,flag1], [kpred2,kobs2,flag2], ... ]\"\nbut without spaces between tokens, that is\n\"[[kpred1,kobs1,flag1],[kpred2,kobs2,flag2],...]\".\nEach $k_{\\text{pred}}$ and $k_{\\text{obs}}$ must be an integer, and each `flag` must be a boolean.",
            "solution": "The problem requires a comparison between the theoretically predicted worst-case number of iterations for the Conjugate Gradient (CG) method and the empirically observed number of iterations required to solve several specified linear systems $A x = b$ to a given tolerance. The problem is scientifically sound, well-posed, and all necessary parameters are provided.\n\n### Part 1: Theoretical Framework\n\nThe core of this problem lies in the convergence theory of the Conjugate Gradient method for solving linear systems $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite (SPD) matrix.\n\n**1. The Conjugate Gradient (CG) Algorithm**\n\nThe CG algorithm is an iterative method that, starting with an initial guess $x_0$, generates a sequence of approximations $x_k$ that converge to the true solution $x_\\star = A^{-1} b$. For an initial guess $x_0=0$, the standard algorithm is as follows:\n\nLet $r_0 = b - A x_0 = b$ and $p_0 = r_0$.\nFor $k = 0, 1, 2, \\dots$ until convergence:\n$$ \\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k} $$\n$$ x_{k+1} = x_k + \\alpha_k p_k $$\n$$ r_{k+1} = r_k - \\alpha_k A p_k $$\n$$ \\beta_k = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k} $$\n$$ p_{k+1} = r_{k+1} + \\beta_k p_k $$\n\n**2. Error Analysis and Stopping Criterion**\n\nThe error at iteration $k$ is the vector $e_k = x_k - x_\\star$. Convergence of the CG method is typically analyzed in the $A$-norm, also known as the energy norm, defined for any vector $z \\in \\mathbb{R}^n$ as:\n$$ \\|z\\|_A = \\sqrt{z^\\top A z} $$\nThe problem specifies a stopping criterion based on the relative error in this norm. We must stop at the first iteration $k$ for which:\n$$ \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\varepsilon $$\nwhere $\\varepsilon$ is a prescribed tolerance. Given the initial guess $x_0 = 0$, the initial error is $e_0 = x_0 - x_\\star = -x_\\star$. Thus, the denominator is $\\|e_0\\|_A = \\|-x_\\star\\|_A = \\|x_\\star\\|_A$. The number of iterations required to satisfy this condition is the observed iteration count, $k_{\\text{obs}}$.\n\n**3. Theoretical Convergence Bound**\n\nA classical result in numerical linear algebra provides a worst-case bound on the convergence of the CG method. This bound relates the relative error in the $A$-norm to the spectral condition number of the matrix $A$, denoted $\\kappa_2(A)$. For an SPD matrix $A$, all eigenvalues are real and positive. The condition number is the ratio of the largest to the smallest eigenvalue:\n$$ \\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)} $$\nThe error bound is given by:\n$$ \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le 2 \\left( \\frac{\\sqrt{\\kappa_2(A)} - 1}{\\sqrt{\\kappa_2(A)} + 1} \\right)^k $$\nThis inequality holds for any right-hand side vector $b$ and provides a conservative estimate of the convergence rate.\n\n**4. Predicted Iteration Count ($k_{\\text{pred}}$)**\n\nTo find the predicted number of iterations, $k_{\\text{pred}}$, we determine the smallest integer $k$ that guarantees the right-hand side of the bound is less than or equal to the tolerance $\\varepsilon$.\n$$ 2 \\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)^k \\le \\varepsilon $$\nLet $\\kappa = \\kappa_2(A)$. Rearranging the inequality to solve for $k$:\n$$ \\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)^k \\le \\frac{\\varepsilon}{2} $$\nTaking the natural logarithm of both sides and noting that $\\log\\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)$ is negative for $\\kappa > 1$, we must reverse the inequality sign:\n$$ k \\ge \\frac{\\log(\\varepsilon/2)}{\\log\\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)} $$\nTherefore, the predicted iteration count $k_{\\text{pred}}$ is the smallest integer satisfying this condition, which is obtained by taking the ceiling of the right-hand side:\n$$ k_{\\text{pred}} = \\left\\lceil \\frac{\\log(\\varepsilon/2)}{\\log\\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)} \\right\\rceil $$\nIn the special case where $\\kappa = 1$, the matrix $A$ is a scalar multiple of the identity matrix. The CG method converges in exactly one iteration. Our formula's denominator approaches $\\log(0)$, which is undefined. Thus, we handle this as a special case: if $\\kappa = 1$, then $k_{\\text{pred}} = 1$.\n\n### Part 2: Methodological Implementation\n\nThe procedure for each test case is as follows:\n\n1.  **System Construction**: For each test case, the matrix $A \\in \\mathbb{R}^{n \\times n}$ and vector $b \\in \\mathbb{R}^n$ are constructed according to the problem specification.\n\n2.  **Reference Solution**: A highly accurate reference solution $x_\\star$ is computed by solving $A x = b$ using a direct solver, specifically `numpy.linalg.solve`.\n\n3.  **Observed Iteration Count ($k_{\\text{obs}}$)**: The CG algorithm is implemented starting from $x_0=0$. After each iteration $k$ (from $k=1$ to $n$), the error vector $e_k = x_k - x_\\star$ is computed. The relative error in the $A$-norm, $\\sqrt{\\|e_k\\|_A^2 / \\|e_0\\|_A^2}$, is evaluated. Note that $\\|e_0\\|_A^2 = \\|x_\\star\\|_A^2 = x_\\star^\\top A x_\\star = x_\\star^\\top b$. The loop terminates as soon as this relative error is less than or equal to $\\varepsilon$, and the current iteration number is recorded as $k_{\\text{obs}}$. If the tolerance is not met within $n$ iterations, $k_{\\text{obs}}$ is set to $n$.\n\n4.  **Predicted Iteration Count ($k_{\\text{pred}}$)**: The eigenvalues of $A$ are computed using `numpy.linalg.eigvalsh` (which is specialized for symmetric matrices). The condition number $\\kappa_2(A)$ is calculated from the minimum and maximum eigenvalues. This value is then used in the derived formula for $k_{\\text{pred}}$.\n\n5.  **Result Assembly**: For each case, the final result is a triple $[k_{\\text{pred}}, k_{\\text{obs}}, \\text{flag}]$, where `flag` is the boolean value of the expression $k_{\\text{obs}} \\le k_{\\text{pred}}$. This flag verifies if the observed performance is indeed within the theoretical worst-case bound.\n\nThe entire process is repeated for all five test cases, and the results are collected into a list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import hilbert\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    def run_single_case(n, A_generator, b_generator, eps):\n        \"\"\"\n        Executes the required analysis for a single test case.\n\n        Args:\n            n (int): The dimension of the system.\n            A_generator (callable): A function that takes n and returns matrix A.\n            b_generator (callable): A function that takes n and returns vector b.\n            eps (float): The tolerance for the relative A-norm of the error.\n\n        Returns:\n            list: A list containing [k_pred, k_obs, flag].\n        \"\"\"\n        # 1. Construct the linear system A*x = b.\n        A = A_generator(n)\n        b = b_generator(n)\n\n        # 2. Compute the reference solution x_star using a direct method.\n        x_star = np.linalg.solve(A, b)\n\n        # 3. Run Conjugate Gradient to find the observed iteration count k_obs.\n        x_k = np.zeros(n)\n        r_k = b.copy()  # Since x_0 = 0, r_0 = b\n        p_k = r_k.copy()\n        \n        # Calculate the square of the initial A-norm of the error: ||x_0 - x_star||_A^2\n        # Since x_0 = 0, this is ||x_star||_A^2 = x_star.T @ A @ x_star = x_star.T @ b\n        norm_A_e0_sq = x_star.T @ b\n\n        k_obs = n  # Default if tolerance is not met within n iterations.\n\n        # Handle the trivial case where b=0, thus x=0 and converges instantly.\n        if np.isclose(norm_A_e0_sq, 0):\n            k_obs = 0\n        else:\n            for k in range(1, n + 1):\n                Apk = A @ p_k\n                r_k_dot_r_k = r_k.T @ r_k\n                alpha_k = r_k_dot_r_k / (p_k.T @ Apk)\n                \n                x_k += alpha_k * p_k\n                \n                # Check for convergence by calculating the relative A-norm of the error.\n                e_k = x_k - x_star\n                norm_A_ek_sq = e_k.T @ A @ e_k\n                relative_error = np.sqrt(norm_A_ek_sq / norm_A_e0_sq)\n\n                if relative_error <= eps:\n                    k_obs = k\n                    break\n\n                r_k_next = r_k - alpha_k * Apk\n                beta_k = (r_k_next.T @ r_k_next) / r_k_dot_r_k\n                p_k = r_k_next + beta_k * p_k\n                r_k = r_k_next\n\n        # 4. Compute the spectral condition number kappa and the predicted iteration count k_pred.\n        eigenvalues = np.linalg.eigvalsh(A)\n        lambda_min = eigenvalues[0]\n        lambda_max = eigenvalues[-1]\n        \n        if lambda_min <= 0:\n            # This should not happen for the given SPD problems.\n            # Assign a very large number to indicate failure.\n            kappa = np.inf\n        else:\n            kappa = lambda_max / lambda_min\n\n        k_pred = 0\n        if np.isclose(kappa, 1.0):\n            k_pred = 1\n        elif k_obs == 0:\n            k_pred = 1 # Consistent prediction for trivial case\n        else:\n            sqrt_kappa = np.sqrt(kappa)\n            rho = (sqrt_kappa - 1) / (sqrt_kappa + 1)\n            # The bound is ||e_k||/||e_0|| <= 2 * rho^k\n            # We solve 2 * rho^k <= eps for k.\n            k_pred = int(np.ceil((np.log(eps) - np.log(2.0)) / np.log(rho)))\n\n        # 5. Assemble the final triplet.\n        flag = k_obs <= k_pred\n        return [k_pred, k_obs, flag]\n\n    # Test suite definition\n    test_cases = [\n        {\n            \"n\": 50, \"eps\": 1e-8,\n            \"A_gen\": lambda n: np.diag(np.linspace(1, 2, n)),\n            \"b_gen\": lambda n: np.sin(np.arange(1, n + 1))\n        },\n        {\n            \"n\": 80, \"eps\": 1e-6,\n            \"A_gen\": lambda n: (np.diag(2 * np.ones(n)) + \n                              np.diag(-1 * np.ones(n - 1), k=1) + \n                              np.diag(-1 * np.ones(n - 1), k=-1)),\n            \"b_gen\": lambda n: np.cos(np.arange(1, n + 1))\n        },\n        {\n            \"n\": 60, \"eps\": 1e-4,\n            \"A_gen\": lambda n: np.diag(np.logspace(0, 6, n)),\n            \"b_gen\": lambda n: np.sin(0.5 * np.arange(1, n + 1))\n        },\n        {\n            \"n\": 40, \"eps\": 1e-8,\n            \"A_gen\": lambda n: (\n                lambda Q, D: Q.T @ D @ Q\n            )(\n                np.linalg.qr(\n                    np.sin(np.arange(1, n + 1) + np.arange(1, n + 1)[:, np.newaxis]) +\n                    np.cos(np.arange(1, n + 1) - 2 * np.arange(1, n + 1)[:, np.newaxis])\n                )[0],\n                np.diag(np.linspace(1, 100, n))\n            ),\n            \"b_gen\": lambda n: np.sin(np.arange(1, n + 1)) + np.cos(2 * np.arange(1, n + 1))\n        },\n        {\n            \"n\": 10, \"eps\": 1e-6,\n            \"A_gen\": lambda n: hilbert(n),\n            \"b_gen\": lambda n: np.ones(n)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_single_case(case[\"n\"], case[\"A_gen\"], case[\"b_gen\"], case[\"eps\"])\n        results.append(result)\n\n    # Format the output string exactly as required, removing all spaces.\n    # str(results) produces Python literal format, e.g., '[[1, 2, True], [3, 4, False]]'\n    # .replace(' ', '') removes spaces to match the specification.\n    # Python's str(True) is 'True', which is a valid boolean representation in many contexts.\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}