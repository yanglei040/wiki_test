{
    "hands_on_practices": [
        {
            "introduction": "Understanding an iterative method goes beyond just plugging numbers into a formula. This first practice invites you to visualize the Gauss-Seidel method in action on a simple $2 \\times 2$ system . By calculating the first few steps, you can trace the path of the approximate solution in the $x_1$-$x_2$ plane, revealing the geometric intuition behind how the algorithm \"zig-zags\" towards the true solution.",
            "id": "2214528",
            "problem": "Consider the following system of linear equations in the $x_1$-$x_2$ plane:\n$$5x_1 - 2x_2 = 3$$\n$$x_1 + 4x_2 = 10$$\nThe Gauss-Seidel method is an iterative algorithm used to approximate the solution to a system of linear equations. Starting with an initial guess $\\mathbf{x}^{(0)} = (x_1^{(0)}, x_2^{(0)})$, it generates a sequence of approximations $\\mathbf{x}^{(k)} = (x_1^{(k)}, x_2^{(k)})$ for $k = 1, 2, 3, \\ldots$. Geometrically, each step of this method can be interpreted as a movement on the $x_1$-$x_2$ plane.\n\nLet the sequence of points generated by the Gauss-Seidel method be denoted by $P_k = (x_1^{(k)}, x_2^{(k)})$. Using the initial guess $P_0 = (0, 0)$, determine the coordinates of the points $P_1 = (x_1^{(1)}, x_2^{(1)})$ and $P_2 = (x_1^{(2)}, x_2^{(2)})$, which result from the first two full iterations of the method.\n\nProvide your answer as the four coordinates $x_1^{(1)}, x_2^{(1)}, x_1^{(2)}, x_2^{(2)}$, in this specific order, within a single row matrix. Express all values as exact fractions.",
            "solution": "We rewrite each equation solved for one variable to obtain the Gauss-Seidel iteration form. From $5x_{1}-2x_{2}=3$, solve for $x_{1}$:\n$$\nx_{1}=\\frac{3+2x_{2}}{5}.\n$$\nFrom $x_{1}+4x_{2}=10$, solve for $x_{2}$:\n$$\nx_{2}=\\frac{10-x_{1}}{4}.\n$$\nIn Gauss-Seidel, at iteration $k$, we update sequentially using the newest values:\n$$\nx_{1}^{(k)}=\\frac{3+2x_{2}^{(k-1)}}{5},\\qquad x_{2}^{(k)}=\\frac{10-x_{1}^{(k)}}{4}.\n$$\nStarting from $P_{0}=(x_{1}^{(0)},x_{2}^{(0)})=(0,0)$:\n\nFirst iteration $k=1$:\n$$\nx_{1}^{(1)}=\\frac{3+2x_{2}^{(0)}}{5}=\\frac{3+2\\cdot 0}{5}=\\frac{3}{5},\n$$\n$$\nx_{2}^{(1)}=\\frac{10-x_{1}^{(1)}}{4}=\\frac{10-\\frac{3}{5}}{4}=\\frac{\\frac{50}{5}-\\frac{3}{5}}{4}=\\frac{\\frac{47}{5}}{4}=\\frac{47}{20}.\n$$\n\nSecond iteration $k=2$:\n$$\nx_{1}^{(2)}=\\frac{3+2x_{2}^{(1)}}{5}=\\frac{3+2\\cdot \\frac{47}{20}}{5}=\\frac{3+\\frac{47}{10}}{5}=\\frac{\\frac{30}{10}+\\frac{47}{10}}{5}=\\frac{\\frac{77}{10}}{5}=\\frac{77}{50},\n$$\n$$\nx_{2}^{(2)}=\\frac{10-x_{1}^{(2)}}{4}=\\frac{10-\\frac{77}{50}}{4}=\\frac{\\frac{500}{50}-\\frac{77}{50}}{4}=\\frac{\\frac{423}{50}}{4}=\\frac{423}{200}.\n$$\nThus the requested coordinates are $x_{1}^{(1)}=\\frac{3}{5}$, $x_{2}^{(1)}=\\frac{47}{20}$, $x_{1}^{(2)}=\\frac{77}{50}$, and $x_{2}^{(2)}=\\frac{423}{200}$.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{3}{5} & \\frac{47}{20} & \\frac{77}{50} & \\frac{423}{200}\\end{pmatrix}}$$"
        },
        {
            "introduction": "While the Gauss-Seidel method can be remarkably efficient, it is not a universal solution; this exercise explores a crucial aspect of numerical methods: their conditions for convergence . By applying the iteration to a system that is not diagonally dominant, you will directly observe how the process can diverge. This hands-on experience with a diverging system highlights why we must first analyze a system's properties before selecting an iterative solver.",
            "id": "1394885",
            "problem": "An engineer is tasked with solving a system of linear equations that models a simple static structural framework. The system is given by:\n$$\n\\begin{cases}\n    x_1 + 2x_2 = 5 \\\\\n    3x_1 + x_2 = 4\n\\end{cases}\n$$\nThe engineer decides to use the Gauss-Seidel method, an iterative technique, to approximate the solution. The method starts with an initial guess for the solution vector and refines it in each iteration.\n\nStarting with the initial guess $\\mathbf{x}^{(0)} = \\begin{pmatrix} x_1^{(0)} \\\\ x_2^{(0)} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, calculate the values for the solution vector $\\mathbf{x}^{(3)} = \\begin{pmatrix} x_1^{(3)} \\\\ x_2^{(3)} \\end{pmatrix}$ after three full iterations of the Gauss-Seidel method.",
            "solution": "The Gauss-Seidel method updates each variable sequentially using the most recently available values. For the system\n$$\n\\begin{cases}\nx_{1} + 2 x_{2} = 5 \\\\\n3 x_{1} + x_{2} = 4\n\\end{cases}\n$$\nsolve each equation for the corresponding variable to obtain the iteration formulas:\n$$\nx_{1}^{(k+1)} = 5 - 2 x_{2}^{(k)}, \\quad x_{2}^{(k+1)} = 4 - 3 x_{1}^{(k+1)}.\n$$\nStarting from $\\mathbf{x}^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, compute three full iterations.\n\nFirst iteration ($k=0 \\to 1$):\n$$\nx_{1}^{(1)} = 5 - 2 x_{2}^{(0)} = 5 - 2 \\cdot 0 = 5,\n$$\n$$\nx_{2}^{(1)} = 4 - 3 x_{1}^{(1)} = 4 - 3 \\cdot 5 = 4 - 15 = -11.\n$$\n\nSecond iteration ($k=1 \\to 2$):\n$$\nx_{1}^{(2)} = 5 - 2 x_{2}^{(1)} = 5 - 2 \\cdot (-11) = 5 + 22 = 27,\n$$\n$$\nx_{2}^{(2)} = 4 - 3 x_{1}^{(2)} = 4 - 3 \\cdot 27 = 4 - 81 = -77.\n$$\n\nThird iteration ($k=2 \\to 3$):\n$$\nx_{1}^{(3)} = 5 - 2 x_{2}^{(2)} = 5 - 2 \\cdot (-77) = 5 + 154 = 159,\n$$\n$$\nx_{2}^{(3)} = 4 - 3 x_{1}^{(3)} = 4 - 3 \\cdot 159 = 4 - 477 = -473.\n$$\n\nThus, after three full Gauss-Seidel iterations, the solution vector is $\\mathbf{x}^{(3)} = \\begin{pmatrix} 159 \\\\ -473 \\end{pmatrix}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 159 \\\\ -473 \\end{pmatrix}}$$"
        },
        {
            "introduction": "A theoretical algorithm is only as good as its practical implementation, and a key part of that is knowing when to stop. This advanced practice delves into the critical topic of stopping criteria for iterative methods . You will implement and compare two different rules for terminating the Gauss-Seidel process, discovering firsthand how a seemingly reasonable choice can sometimes provide a misleading sense of accuracy, a vital lesson for building robust and reliable numerical software.",
            "id": "3135103",
            "problem": "You are asked to design and analyze a terminating criterion for the Gauss-Seidel iterative method applied to solving a linear system. Consider a square matrix $A \\in \\mathbb{R}^{n \\times n}$ and a right-hand side vector $b \\in \\mathbb{R}^{n}$. The goal is to approximate the solution $x^{\\star}$ to the system $A x^{\\star} = b$ using the Gauss-Seidel iteration with an initial guess $x^{(0)}$. At each iteration $k$, define the residual $r^{(k)} = b - A x^{(k)}$. The task is to implement two stopping rules and to compare their behavior on carefully chosen test cases.\n\nFundamental base to be used:\n- The definition of a linear system $A x = b$ and the concept of iterative refinement via sequentially satisfying individual equations.\n- The residual definition $r(x) = b - A x$ and its role as a measure of how far $x$ is from being a solution.\n- The Euclidean norm (also called the $2$-norm) for vectors, denoted by $\\| \\cdot \\|$, and the induced operator norm for matrices $\\| A \\|$ (the largest singular value of $A$).\n\nRequirements:\n1. Implement the Gauss-Seidel iteration from first principles: starting from $x^{(0)}$, produce $x^{(k+1)}$ by sequentially updating the components so that, within each sweep, each scalar equation is satisfied using the latest available component values. Avoid any shortcut formulas; base your update on satisfying one equation at a time using the most recently updated components in the current sweep and the remaining components from the previous sweep.\n2. Implement and track the two stopping rules starting from iteration $k = 1$:\n   - Rule R1 (normalized residual by right-hand side): stop at the smallest $k$ such that $\\displaystyle \\frac{\\| r^{(k)} \\|}{\\| b \\|} \\leq \\tau$.\n   - Rule R2 (normalized residual by matrix and iterate): stop at the smallest $k$ such that $\\displaystyle \\frac{\\| r^{(k)} \\|}{\\| A \\| \\, \\| x^{(k)} \\|} \\leq \\tau$.\n3. Conventions for boundary cases:\n   - If $\\| r^{(k)} \\| = 0$, treat the ratio for either rule as $0$ regardless of the denominator and accept convergence.\n   - For Rule R1, if $\\| b \\| = 0$ and $\\| r^{(k)} \\| \\neq 0$, treat the ratio as $+\\infty$ (no convergence at this iteration).\n   - For Rule R2, if $\\| x^{(k)} \\| = 0$ and $\\| r^{(k)} \\| \\neq 0$, treat the ratio as $+\\infty$ at that iteration.\n4. For each test case, compute the exact solution $x^{\\star}$ using a direct solve (to enable evaluation of the forward error). Define the relative forward error at iteration $k$ as $\\displaystyle \\frac{\\| x^{(k)} - x^{\\star} \\|}{\\| x^{\\star} \\|}$, and use it only for analysis (do not use it as a stopping rule).\n5. Define the tolerance $\\tau = 10^{-4}$ and maximum number of iterations $K_{\\max} = 5000$. Use the Euclidean norm for vectors and the induced operator norm (largest singular value) for matrices. Angles are not involved; no physical units are involved.\n6. Your program must run the Gauss-Seidel iteration without early termination to collect the full sequence $\\{ x^{(k)} \\}_{k=1}^{K_{\\max}}$ and evaluate both stopping rules on that one sequence. Then, for each test case, return:\n   - $N_{\\mathrm{R1}}$: the smallest $k$ such that Rule R1 is satisfied, or $K_{\\max}$ if not satisfied by $K_{\\max}$,\n   - $N_{\\mathrm{R2}}$: the smallest $k$ such that Rule R2 is satisfied, or $K_{\\max}$ if not satisfied by $K_{\\max}$,\n   - $\\mathrm{inflated}$: a boolean that is $\\mathrm{True}$ if Rule R1 signals convergence earlier than Rule R2 (i.e., $N_{\\mathrm{R1}} < N_{\\mathrm{R2}}$) and, at iteration $N_{\\mathrm{R1}}$, the relative forward error $\\displaystyle \\frac{\\| x^{(N_{\\mathrm{R1}})} - x^{\\star} \\|}{\\| x^{\\star} \\|}$ is strictly greater than $\\tau$. Otherwise, $\\mathrm{inflated}$ is $\\mathrm{False}$.\n\nTest suite:\n- Case $1$ (well-scaled, symmetric positive definite, diagonally dominant):\n  - $A = \\begin{pmatrix} 4 & -1 & 0 \\\\ -1 & 4 & -1 \\\\ 0 & -1 & 3 \\end{pmatrix}$,\n  - $b = \\begin{pmatrix} 15 \\\\ 10 \\\\ 10 \\end{pmatrix}$,\n  - $x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n  This case is a general happy path where both rules are expected to behave similarly.\n- Case $2$ (constructed to inflate perceived progress under Rule R1 by scaling $b$ relative to the matrix norm):\n  - $A = \\begin{pmatrix} 10^{-2} & 2 \\cdot 10^{-3} & 0 \\\\ 10^{-3} & 2 \\cdot 10^{-2} & 3 \\cdot 10^{-3} \\\\ 0 & 10^{-3} & 3 \\cdot 10^{-2} \\end{pmatrix}$,\n  - $b = \\begin{pmatrix} 10^{8} \\\\ -10^{8} \\\\ 10^{8} \\end{pmatrix}$,\n  - $x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n  This case is designed so that dividing by the large $\\| b \\|$ can make $\\displaystyle \\frac{\\| r^{(k)} \\|}{\\| b \\|}$ appear small earlier than the alternative rule, potentially inflating perceived progress.\n- Case $3$ (boundary condition with zero right-hand side):\n  - $A = \\begin{pmatrix} 4 & -1 & 0 \\\\ -1 & 4 & -1 \\\\ 0 & -1 & 3 \\end{pmatrix}$,\n  - $b = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$,\n  - $x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n  This case verifies handling of $\\| b \\| = 0$ and the convention that a zero residual implies acceptance.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list of per-case triplets, with each triplet formatted as $[N_{\\mathrm{R1}},N_{\\mathrm{R2}},\\mathrm{inflated}]$, and the whole set enclosed in square brackets. For example, the output format must be like $[[N_{1,\\mathrm{R1}},N_{1,\\mathrm{R2}},\\mathrm{inflated}_1],[N_{2,\\mathrm{R1}},N_{2,\\mathrm{R2}},\\mathrm{inflated}_2],[N_{3,\\mathrm{R1}},N_{3,\\mathrm{R2}},\\mathrm{inflated}_3]]$ where each $N$ is an integer and each $\\mathrm{inflated}$ is either $\\mathrm{True}$ or $\\mathrm{False}$.",
            "solution": "The task is to solve the linear system $A x^{\\star} = b$ for three distinct cases using the Gauss-Seidel iteration. We will implement the method from first principles, apply two different stopping criteria, and analyze their behavior. The analysis involves comparing the iteration counts at which each rule is satisfied and evaluating a special condition, `inflated`, that diagnoses premature convergence signaled by one of the rules.\n\n**1. The Gauss-Seidel Method**\n\nThe Gauss-Seidel method is an iterative technique for approximating the solution to a linear system $A x = b$. Given a square matrix $A \\in \\mathbb{R}^{n \\times n}$ and a vector $b \\in \\mathbb{R}^{n}$, we seek the solution vector $x \\in \\mathbb{R}^{n}$. The system can be written component-wise as:\n$$ \\sum_{j=1}^{n} A_{ij} x_j = b_i \\quad \\text{for } i = 1, 2, \\ldots, n $$\nwhere $A_{ij}$ is the element in the $i$-th row and $j$-th column of $A$. To form an iterative scheme, we solve the $i$-th equation for the $i$-th component, $x_i$:\n$$ A_{ii} x_i = b_i - \\sum_{j \\neq i} A_{ij} x_j $$\nAssuming $A_{ii} \\neq 0$ for all $i$, we can write:\n$$ x_i = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j \\neq i} A_{ij} x_j \\right) $$\nThe Gauss-Seidel iteration constructs a sequence of approximate solutions $\\{x^{(k)}\\}_{k=0}^{\\infty}$ starting from an initial guess $x^{(0)}$. To compute the next iterate $x^{(k+1)}$ from the current one $x^{(k)}$, we update each component $x_i^{(k+1)}$ for $i=1, \\ldots, n$ sequentially. The core principle of the method is to use the most recently computed component values within the current iteration. When computing $x_i^{(k+1)}$, the components $x_1^{(k+1)}, \\ldots, x_{i-1}^{(k+1)}$ have already been determined. For the remaining components, we use the values from the previous iteration, $x_{i+1}^{(k)}, \\ldots, x_{n}^{(k)}$. This leads to the defining update rule for the Gauss-Seidel method:\n$$ x_i^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} A_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^{n} A_{ij} x_j^{(k)} \\right) \\quad \\text{for } i = 1, \\ldots, n $$\n\n**2. Stopping Criteria and Analysis**\n\nAn iterative method requires a rule to determine when to stop. A common measure of error is the residual, $r^{(k)} = b - A x^{(k)}$, which is zero only if $x^{(k)}$ is the exact solution. We are tasked with evaluating two stopping rules based on the Euclidean norm of the residual, denoted by $\\| \\cdot \\|$. The tolerance is given as $\\tau = 10^{-4}$.\n\n*   **Rule R1 (Normalized by right-hand side):** $\\displaystyle \\frac{\\| r^{(k)} \\|}{\\| b \\|} \\leq \\tau$\n    This criterion measures the norm of the residual relative to the norm of the right-hand side vector $b$. It is intuitive but can be misleading. If $\\|b\\|$ is very large, the ratio can become small even if $x^{(k)}$ is far from the true solution $x^{\\star}$, giving a false sense of convergence. Conversely, if $\\|b\\|$ is very small, the criterion may be too stringent. Test Case $2$ is designed to expose this vulnerability.\n\n*   **Rule R2 (Normalized by matrix and iterate):** $\\displaystyle \\frac{\\| r^{(k)} \\|}{\\| A \\| \\, \\| x^{(k)} \\|} \\leq \\tau$\n    This criterion is related to the concept of backward error. The value on the left can be interpreted as the smallest relative perturbation to the matrix $A$ such that $x^{(k)}$ becomes the exact solution of a perturbed system. It is generally more robust than Rule R1 because it normalizes the residual by a scale that is intrinsic to the current state of the system ($A$ and $x^{(k)}$), rather than just the input $b$. The matrix norm $\\|A\\|$ is the induced $2$-norm, calculated as the largest singular value of $A$.\n\nFor our analysis, we will calculate the exact solution $x^{\\star}$ using a direct solver. The true accuracy of an iterate $x^{(k)}$ is measured by the relative forward error: $\\frac{\\| x^{(k)} - x^{\\star} \\|}{\\| x^{\\star} \\|}$. We are asked to identify when Rule R1 signals convergence prematurely. This is captured by the `inflated` flag, which is set to `True` if Rule R1 stops earlier than Rule R2 ($N_{\\mathrm{R1}} < N_{\\mathrm{R2}}$) and, at that point, the solution is not yet accurate (relative forward error at iteration $N_{\\mathrm{R1}}$ is greater than $\\tau$).\n\n**3. Algorithmic Implementation**\n\nFor each test case, the algorithm proceeds as follows:\n\n1.  **Initialization**: Define the matrix $A$, vector $b$, and initial guess $x^{(0)}$. Set the tolerance $\\tau = 10^{-4}$ and maximum iterations $K_{\\max} = 5000$.\n2.  **Pre-computation**:\n    *   Calculate the exact solution $x^{\\star}$ by solving $A x = b$ directly (e.g., using LU decomposition).\n    *   Compute and store constant norms: $\\|b\\|$, $\\|A\\|$ (largest singular value), and $\\|x^{\\star}\\|$.\n3.  **Iteration Loop**: Run the Gauss-Seidel iteration for a fixed number of steps, from $k=0$ to $K_{\\max}-1$, to generate the sequence of iterates $\\{x^{(k)}\\}_{k=1}^{K_{\\max}}$.\n    *   In each step, to get $x^{(k+1)}$ from $x^{(k)}$, loop through the components $i = 1, \\ldots, n$ and apply the update formula.\n    *   After computing $x^{(k+1)}$, calculate the residual $r^{(k+1)} = b - A x^{(k+1)}$.\n    *   Compute and store the values for the left-hand side of both stopping criteria rules, being careful to handle the specified boundary conditions (e.g., if a denominator is zero).\n    *   Compute and store the relative forward error $\\frac{\\| x^{(k+1)} - x^{\\star} \\|}{\\| x^{\\star} \\|}$.\n4.  **Post-processing**: After the loop completes, analyze the stored sequences.\n    *   Find $N_{\\mathrm{R1}}$, the first iteration index $k \\ge 1$ where the condition for Rule R1 is met. If it is never met, set $N_{\\mathrm{R1}} = K_{\\max}$.\n    *   Similarly, find $N_{\\mathrm{R2}}$ for Rule R2.\n    *   Determine the value of the `inflated` boolean by checking if $N_{\\mathrm{R1}} < N_{\\mathrm{R2}}$ and if the relative forward error at iteration $N_{\\mathrm{R1}}$ is greater than $\\tau$.\n5.  **Output**: Collect the triplet $[N_{\\mathrm{R1}}, N_{\\mathrm{R2}}, \\mathrm{inflated}]$ for each test case and format the final output as specified.\n\nSpecial handling for Case $3$ where $b=0$: The exact solution is $x^{\\star}=0$. Starting with $x^{(0)}=0$, the Gauss-Seidel update produces $x^{(1)}=0$. The residual $r^{(1)}$ is then $0$. By convention, if $\\|r^{(k)}\\|=0$, the stopping criterion ratio is treated as $0$, which is less than or equal to $\\tau$. Thus, both rules are satisfied at $k=1$, leading to $N_{\\mathrm{R1}}=1$ and $N_{\\mathrm{R2}}=1$. In this case, the condition $N_{\\mathrm{R1}} < N_{\\mathrm{R2}}$ is false, so `inflated` is `False`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes Gauss-Seidel iteration with two stopping rules.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([[4, -1, 0], [-1, 4, -1], [0, -1, 3]], dtype=float),\n            \"b\": np.array([15, 10, 10], dtype=float),\n            \"x0\": np.array([0, 0, 0], dtype=float),\n        },\n        {\n            \"A\": np.array([[1e-2, 2e-3, 0], [1e-3, 2e-2, 3e-3], [0, 1e-3, 3e-2]], dtype=float),\n            \"b\": np.array([1e8, -1e8, 1e8], dtype=float),\n            \"x0\": np.array([0, 0, 0], dtype=float),\n        },\n        {\n            \"A\": np.array([[4, -1, 0], [-1, 4, -1], [0, -1, 3]], dtype=float),\n            \"b\": np.array([0, 0, 0], dtype=float),\n            \"x0\": np.array([0, 0, 0], dtype=float),\n        },\n    ]\n\n    tau = 1e-4\n    K_max = 5000\n    results = []\n\n    for case in test_cases:\n        A = case[\"A\"]\n        b = case[\"b\"]\n        x_k = case[\"x0\"].copy()\n        n = A.shape[0]\n\n        # Pre-computation\n        try:\n            x_star = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Should not happen for the given test cases as matrices are invertible.\n            # If it did, we couldn't compute forward error.\n            # Handle gracefully if needed for a general problem.\n            x_star = np.full_like(b, np.nan)\n\n        norm_b = np.linalg.norm(b)\n        norm_A = np.linalg.svd(A, compute_uv=False)[0]\n        norm_x_star = np.linalg.norm(x_star)\n\n        # Storage for analysis\n        ratios_r1 = []\n        ratios_r2 = []\n        fwd_errors = []\n\n        # Iteration loop\n        for _ in range(K_max):\n            x_next = x_k.copy()\n            for i in range(n):\n                sum1 = A[i, :i] @ x_next[:i]\n                sum2 = A[i, i+1:] @ x_k[i+1:]\n                if A[i, i] != 0:\n                    x_next[i] = (b[i] - sum1 - sum2) / A[i, i]\n            \n            x_k = x_next\n            \n            # Compute metrics for this iteration (k+1)\n            residual = b - A @ x_k\n            norm_r = np.linalg.norm(residual)\n            norm_x = np.linalg.norm(x_k)\n\n            # Rule R1 ratio\n            if norm_r == 0:\n                ratio_r1 = 0.0\n            elif norm_b == 0:\n                ratio_r1 = np.inf\n            else:\n                ratio_r1 = norm_r / norm_b\n            ratios_r1.append(ratio_r1)\n\n            # Rule R2 ratio\n            if norm_r == 0:\n                ratio_r2 = 0.0\n            elif norm_A * norm_x == 0:\n                ratio_r2 = np.inf\n            else:\n                ratio_r2 = norm_r / (norm_A * norm_x)\n            ratios_r2.append(ratio_r2)\n\n            # Forward error\n            if norm_x_star == 0:\n                # Use absolute error if x_star is the zero vector\n                fwd_error = np.linalg.norm(x_k - x_star)\n            else:\n                fwd_error = np.linalg.norm(x_k - x_star) / norm_x_star\n            fwd_errors.append(fwd_error)\n\n        # Post-processing to find N_R1, N_R2\n        N_R1 = K_max\n        for k, ratio in enumerate(ratios_r1):\n            if ratio <= tau:\n                N_R1 = k + 1\n                break\n        \n        N_R2 = K_max\n        for k, ratio in enumerate(ratios_r2):\n            if ratio <= tau:\n                N_R2 = k + 1\n                break\n\n        # Determine 'inflated' flag\n        inflated = False\n        if N_R1 < N_R2:\n            # fwd_errors is 0-indexed, N_R1 is 1-indexed\n            if fwd_errors[N_R1 - 1] > tau:\n                inflated = True\n\n        results.append([N_R1, N_R2, inflated])\n\n    # Final print statement in the exact required format.\n    # Convert boolean to string 'True' or 'False' as required.\n    formatted_results = [f\"[{r[0]},{r[1]},{str(r[2])}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}