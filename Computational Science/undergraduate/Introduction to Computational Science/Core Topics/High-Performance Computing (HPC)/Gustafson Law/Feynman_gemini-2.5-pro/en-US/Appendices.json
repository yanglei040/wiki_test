{
    "hands_on_practices": [
        {
            "introduction": "One of the most practical applications of performance models like Gustafson's Law is setting concrete engineering goals for software development. Before investing significant effort in code optimization, we can calculate the maximum acceptable serial overhead, represented by the fraction $\\alpha$, required to achieve a desired speedup on a given number of processors. This exercise  guides you through solving this \"inverse problem,\" starting from first principles to derive the governing relationship and then applying it to a realistic scenario in High-Performance Computing (HPC).",
            "id": "3139830",
            "problem": "A High-Performance Computing (HPC) team plans to scale a simulation so that the wall-clock time remains fixed while increasing the number of processors. Assume the computation consists of an inherently serial portion and a perfectly parallelizable portion with no additional overhead beyond the serial part. Let the fraction of the total run time on the parallel machine spent in the serial portion be denoted by $\\alpha$, and let the number of processors be $N$. The team uses the definition of scaled speedup: the ratio of the time a single processor would require to execute the same scaled workload to the time $N$ processors actually take when the wall-clock time is held fixed.\n\nUsing only the above definitions as your starting point, determine the maximum allowable serial fraction $\\alpha$ that will meet a design target scaled speedup of $S(N)=120$ on $N=128$ processors. Do not cite any memorized formulas; derive what you need from first principles. Report your final $\\alpha$ as a pure number rounded to four significant figures.",
            "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- The computation consists of an inherently serial portion and a perfectly parallelizable portion.\n- The wall-clock time is held fixed when scaling.\n- $\\alpha$: the fraction of the total run time on the parallel machine spent in the serial portion.\n- $N$: the number of processors.\n- Scaled speedup is defined as: the ratio of the time a single processor would require to execute the same scaled workload to the time $N$ processors actually take.\n- Design target scaled speedup: $S(N) = 120$.\n- Number of processors: $N = 128$.\n- Constraint: The derivation must be from first principles, without citing memorized formulas.\n- Required output: The value of $\\alpha$ as a pure number rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as it describes a standard model of parallel computation performance analysis known as scaled speedup, which is the basis for Gustafson's law. It is well-posed, providing a clear definition of scaled speedup and sufficient numerical data ($S(N)$ and $N$) to solve for the unknown variable $\\alpha$. The terminology is objective and precise. The problem does not violate any fundamental principles, is not incomplete or contradictory, and presents a solvable, non-trivial challenge within the field of computational science.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived from first principles as stipulated.\n\n### Derivation\nLet $T(N)$ be the total wall-clock time for a job running on $N$ processors. The problem states this time is held fixed. For convenience and without loss of generality, we can normalize this time to $T(N)=1$ unit.\n\nAccording to the problem definition, $\\alpha$ is the fraction of the total run time on the parallel machine ($N$ processors) that is spent in the serial portion.\nThe time spent on the serial portion on $N$ processors is:\n$$T_{\\text{serial}, N} = \\alpha T(N)$$\nThe remaining time is spent on the parallelizable portion:\n$$T_{\\text{parallel}, N} = (1 - \\alpha) T(N)$$\nThe sum is $T_{\\text{serial}, N} + T_{\\text{parallel}, N} = \\alpha T(N) + (1-\\alpha)T(N) = T(N)$, which is consistent.\n\nNext, we must determine the time it would take a single processor ($N=1$) to execute this same scaled workload. Let this time be $T(1)$. The workload consists of a serial part and a parallel part.\n\nThe serial part of the workload is, by definition, not parallelizable. Therefore, the time to execute it is independent of the number of processors. The time for the serial part on one processor is the same as the time for the serial part on $N$ processors.\n$$T_{\\text{serial}, 1} = T_{\\text{serial}, N} = \\alpha T(N)$$\n\nThe parallel part of the workload is perfectly parallelizable. When running on $N$ processors, the work is distributed among them, taking time $T_{\\text{parallel}, N}$. If this same amount of parallel work were to be executed on a single processor, it would take $N$ times as long.\n$$T_{\\text{parallel}, 1} = N \\cdot T_{\\text{parallel}, N} = N(1 - \\alpha) T(N)$$\n\nThe total time for the single processor, $T(1)$, is the sum of the times for its serial and parallel parts:\n$$T(1) = T_{\\text{serial}, 1} + T_{\\text{parallel}, 1}$$\n$$T(1) = \\alpha T(N) + N(1 - \\alpha) T(N)$$\nFactoring out $T(N)$, we get:\n$$T(1) = [\\alpha + N(1 - \\alpha)] T(N)$$\n\nThe problem defines the scaled speedup, $S(N)$, as the ratio of the time a single processor would require ($T(1)$) to the time $N$ processors actually take ($T(N)$).\n$$S(N) = \\frac{T(1)}{T(N)}$$\nSubstituting our expression for $T(1)$:\n$$S(N) = \\frac{[\\alpha + N(1 - \\alpha)] T(N)}{T(N)}$$\n$$S(N) = \\alpha + N(1 - \\alpha)$$\nThis expression, derived from first principles, relates the scaled speedup $S(N)$ to the serial fraction $\\alpha$ and the number of processors $N$.\n\nNow we must solve for $\\alpha$.\n$$S(N) = \\alpha + N - N\\alpha$$\n$$S(N) - N = \\alpha - N\\alpha$$\n$$S(N) - N = \\alpha(1 - N)$$\n$$\\alpha = \\frac{S(N) - N}{1 - N}$$\nTo avoid negative signs in the denominator, we can multiply the numerator and denominator by $-1$:\n$$\\alpha = \\frac{N - S(N)}{N - 1}$$\n\nThe problem provides the specific values $N = 128$ and a target speedup of $S(128) = 120$. We substitute these values into the derived equation for $\\alpha$.\n$$\\alpha = \\frac{128 - 120}{128 - 1}$$\n$$\\alpha = \\frac{8}{127}$$\n\nTo provide the final answer, we compute the numerical value and round to four significant figures.\n$$\\alpha \\approx 0.0629921259...$$\nThe first four significant figures are $6$, $2$, $9$, $9$. The fifth digit is $2$, which is less than $5$, so we do not round up the last significant figure.\n$$\\alpha \\approx 0.06299$$\nThis is the maximum allowable serial fraction that meets the specified design target.",
            "answer": "$$\\boxed{0.06299}$$"
        },
        {
            "introduction": "The serial fraction $\\alpha$ is a critical property of an algorithm that directly dictates its potential for scaling on parallel hardware. This practice  explores how Gustafson's Law can be used to perform a comparative analysis of the weak-scaling performance of two different computational kernels. By determining the conditions under which each kernel achieves a high percentage of ideal speedup, you will gain a deeper intuition for why minimizing the serial fraction is a primary goal in designing algorithms for large-scale systems.",
            "id": "3139820",
            "problem": "A High Performance Computing (HPC) application is evaluated under weak scaling on a parallel system. In weak scaling, the problem size grows proportionally with the number of processing elements, so that each processing element handles a fixed workload. Consider two computational kernels, each characterized by a measured single-processor fraction of time spent in inherently sequential work, denoted by $\\alpha$, at the baseline workload size. The two kernels have $\\alpha_{1} = 0.06$ and $\\alpha_{2} = 0.12$, respectively, with $\\alpha_{1} < \\alpha_{2}$. Assume that the parallelizable fraction behaves ideally under weak scaling: the parallel portion is perfectly distributed across $N$ processing elements, while the sequential portion remains serialized.\n\nStarting from the foundational definitions of parallel execution time decomposition and speedup, derive the weak-scaling speedup $S(N)$ as a function of $N$ and $\\alpha$, and use it to determine when the speedup achieves at least $95\\%$ of ideal linear scaling, meaning $S(N) \\geq 0.95\\,N$. For each kernel, analyze the inequality to determine the smallest integer $N \\geq 2$ for which the condition $S(N) \\geq 0.95\\,N$ is satisfied. Identify which kernel reaches this threshold first as $N$ grows and report the minimal integer $N$ at which the threshold is first achieved by any of the two kernels. Express your final answer as that single integer $N$ without units. No rounding is required.",
            "solution": "We begin from the fundamental decomposition of execution time. Let the total single-processor execution time at the baseline workload be decomposed as\n$$\nT_{1} = T_{s} + T_{p},\n$$\nwhere $T_{s}$ is the time spent in inherently sequential work and $T_{p}$ is the time spent in parallelizable work. Define the sequential fraction as\n$$\n\\alpha = \\frac{T_{s}}{T_{s} + T_{p}}.\n$$\nUnder weak scaling to $N$ processing elements, the problem size is increased so that each processing element receives the same amount of parallel work as in the single-processor baseline. With ideal distribution, the time to complete the parallel portion on $N$ processing elements remains equal to the single-processor parallel time for the baseline workload on each processing element, while the sequential portion remains serialized and must still be executed by one processing element. Using normalized units where the baseline total time $T_{1}$ is set to $1$, we have $T_{s} = \\alpha$ and $T_{p} = 1 - \\alpha$.\n\nFor the scaled problem on $N$ processing elements, the total scaled single-processor time (if one processor were to execute the scaled problem) would be\n$$\nT_{1}^{\\text{scaled}} = T_{s} + N T_{p} = \\alpha + N(1 - \\alpha).\n$$\nThe actual parallel execution time on $N$ processing elements remains\n$$\nT_{N} = T_{s} + T_{p} = \\alpha + (1 - \\alpha) = 1,\n$$\nbecause the ideal parallelization under weak scaling keeps the time per processing element constant for the parallel portion, and the serialized portion is unchanged.\n\nBy the definition of speedup,\n$$\nS(N) = \\frac{T_{1}^{\\text{scaled}}}{T_{N}} = \\frac{\\alpha + N(1 - \\alpha)}{1} = \\alpha + N(1 - \\alpha).\n$$\nThis is equivalent to the commonly cited Gustafson’s law form $S(N) = N - \\alpha(N - 1)$, but here we have derived it directly from time decomposition under weak scaling.\n\nWe are asked to determine for each kernel when $S(N)$ achieves at least $95\\%$ of the ideal linear speedup $N$, meaning\n$$\nS(N) \\geq 0.95\\,N.\n$$\nSubstitute $S(N) = \\alpha + N(1 - \\alpha)$:\n$$\n\\alpha + N(1 - \\alpha) \\geq 0.95\\,N.\n$$\nRearrange to isolate $N$:\n$$\nN(1 - \\alpha) - 0.95\\,N \\geq -\\alpha \\quad \\Longrightarrow \\quad N(0.05 - \\alpha) \\geq -\\alpha.\n$$\nThis inequality can be analyzed casewise depending on the sign of $(0.05 - \\alpha)$.\n\nFor $\\alpha_{1} = 0.06$, we have $(0.05 - \\alpha_{1}) = -0.01 < 0$. Dividing the inequality by a negative number flips the inequality direction:\n$$\nN \\leq \\frac{-\\alpha_{1}}{0.05 - \\alpha_{1}} = \\frac{-0.06}{-0.01} = 6.\n$$\nTherefore, for kernel $1$, the condition $S(N) \\geq 0.95\\,N$ holds for all integers $N$ with $N \\leq 6$. The smallest integer $N \\geq 2$ satisfying the condition for kernel $1$ is $N = 2$.\n\nFor $\\alpha_{2} = 0.12$, we have $(0.05 - \\alpha_{2}) = -0.07 < 0$, yielding\n$$\nN \\leq \\frac{-\\alpha_{2}}{0.05 - \\alpha_{2}} = \\frac{-0.12}{-0.07} = \\frac{12}{7} \\approx 1.7142857\\ldots.\n$$\nThus, for kernel $2$, the condition $S(N) \\geq 0.95\\,N$ holds only for integer $N \\leq 1$. There is no integer $N \\geq 2$ for which kernel $2$ satisfies the threshold.\n\nComparing the two kernels, kernel $1$ (with the smaller sequential fraction $\\alpha_{1}$) reaches the threshold $S(N) \\geq 0.95\\,N$ at $N = 2$, whereas kernel $2$ does not reach the threshold for any $N \\geq 2$. Therefore, as $N$ grows beyond the trivial single-processor case, the earliest $N$ at which either kernel meets the threshold is $N = 2$, achieved by kernel $1$.\n\nThe requested final numeric answer is this minimal integer $N$ at which the threshold is first achieved by any of the two kernels:\n$$\nN = 2.\n$$",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "The ultimate test of understanding a scientific model is to build and verify it from the ground up. This exercise  bridges the gap between abstract theory and computational practice by tasking you with both deriving Gustafson's Law and implementing a numerical verification. You will start from the foundational definitions of weak scaling to formulate the speedup equation and then write a simple program to confirm that the model behaves as expected across a range of test cases, reinforcing the connection between the mathematical formula and its practical implications.",
            "id": "3139767",
            "problem": "You are asked to formalize and verify a weak-scaling prediction attributed to Gustafson’s law in the context of High-Performance Computing (HPC). The goal is to build from first principles, without assuming any end formula, and to implement a program that verifies the resulting prediction numerically on a small test suite.\n\nStart from these foundational and widely accepted definitions:\n- Let $N$ denote the number of parallel workers (for example, Central Processing Unit (CPU) cores).\n- Under weak scaling, the total problem size grows in proportion to $N$ so that the per-worker wall time remains approximately constant.\n- Decompose the total work of a parallel program for a given workload into a non-parallelizable (sequential) fraction and a perfectly parallelizable fraction. Define a parameter $\\alpha \\in [0,1]$ to be the fraction of time spent in the non-parallelizable part when running a baseline workload on a single worker, and $1-\\alpha$ to be the fraction of time spent in the parallelizable part for that same baseline workload.\n- Define the speedup $S(N)$ for weak scaling as the ratio of the time a single worker would take to execute the scaled workload of size proportional to $N$ to the time $N$ workers actually take to execute that scaled workload.\n\nFrom these bases alone, derive an expression for $S(N)$ in terms of $N$ and $\\alpha$ by reasoning about how the sequential and parallelizable parts of the work transform under weak scaling when the total workload is scaled to keep the per-worker wall time constant.\n\nImplementation requirements:\n- Take the baseline per-worker wall time as $T_{\\text{base}} = 1$ in arbitrary but consistent time units; no physical unit conversion is required.\n- For each test case $(N,\\alpha)$, construct:\n  1. The modeled parallel execution time on $N$ workers for the scaled workload, $T_N$, using only the above definitions and the constraint of weak scaling (per-worker wall time constant at $T_{\\text{base}}=1$).\n  2. The modeled single-worker execution time for the same scaled workload, $T_1(N)$.\n  3. The measured speedup $S_{\\text{meas}}(N) = T_1(N)/T_N$.\n  4. The theoretically derived speedup $S_{\\text{theory}}(N)$ from your derivation.\n  5. A boolean verdict per test case that is true if $\\lvert S_{\\text{meas}}(N) - S_{\\text{theory}}(N)\\rvert \\le \\varepsilon$ with tolerance $\\varepsilon = 10^{-12}$, and false otherwise.\n\nTest suite:\n- Use the following five $(N,\\alpha)$ pairs, which together cover a general use case, boundary values, and edge cases:\n  - $(N,\\alpha) = (1, 0.3)$\n  - $(N,\\alpha) = (8, 0.1)$\n  - $(N,\\alpha) = (64, 0)$\n  - $(N,\\alpha) = (16, 1)$\n  - $(N,\\alpha) = (32, 0.25)$\n\nAnswer format:\n- Your program must produce a single line of output containing a list of booleans for the five test cases in the order given, formatted as a comma-separated Python-like list (for example, $[\\text{True},\\text{False},\\ldots]$). No other output is allowed.\n- All intermediate quantities are dimensionless real numbers and do not require units.",
            "solution": "The problem statement has been subjected to a rigorous validation process. All givens were extracted verbatim, and the problem was analyzed against the criteria of scientific soundness, well-posedness, and objectivity.\n\n**Givens:**\n1.  $N$: number of parallel workers.\n2.  Weak scaling: total problem size grows in proportion to $N$ so that the per-worker wall time remains approximately constant.\n3.  $\\alpha$: fraction of time spent in the non-parallelizable part when running a baseline workload on a single worker, with $\\alpha \\in [0,1]$.\n4.  $1-\\alpha$: fraction of time spent in the parallelizable part for the same baseline workload.\n5.  $S(N)$: weak scaling speedup, defined as the ratio of the time a single worker would take to execute the scaled workload to the time $N$ workers take.\n6.  $T_{\\text{base}} = 1$: baseline per-worker wall time.\n7.  Tolerance $\\varepsilon = 10^{-12}$.\n8.  Test cases: $(N,\\alpha) = (1, 0.3)$, $(8, 0.1)$, $(64, 0)$, $(16, 1)$, $(32, 0.25)$.\n\n**Verdict:**\nThe problem is **valid**. It is a scientifically grounded and well-posed exercise in deriving and verifying Gustafson's law from first principles, a fundamental concept in computational science. The definitions are clear, consistent, and sufficient for deriving a unique and meaningful solution.\n\nWe will now proceed with the derivation and subsequent numerical verification.\n\n### Derivation of Gustafson's Law from First Principles\n\nThe objective is to derive an expression for the weak-scaling speedup, $S(N)$, as a function of the number of workers, $N$, and the sequential fraction of the baseline program, $\\alpha$.\n\n**Step 1: Baseline Workload Analysis ($N=1$)**\n\nLet us consider the baseline workload executed on a single worker ($N=1$). The total execution time is given as $T_{\\text{base}}$, which we set to $1$ arbitrary time unit.\n$$\nT_1(\\text{text}) = T_{\\text{base}} = 1\n$$\nThis total time is composed of a sequential (non-parallelizable) part and a parallelizable part. According to the problem definition, $\\alpha$ is the fraction of time spent in the sequential part.\n-   Time for the sequential part: $T_{s,1} = \\alpha T_1(\\text{base}) = \\alpha \\cdot 1 = \\alpha$.\n-   Time for the parallelizable part: $T_{p,1} = (1-\\alpha) T_1(\\text{base}) = (1-\\alpha) \\cdot 1 = 1-\\alpha$.\n\nIt is useful to think in terms of the amount of computational work, which is proportional to execution time on a single worker. Assuming a single worker performs one unit of work per unit of time, the total work for the baseline case, $W_{\\text{base}}$, is:\n-   Sequential work: $W_s = T_{s,1} = \\alpha$.\n-   Parallelizable work: $W_p = T_{p,1} = 1-\\alpha$.\n\n**Step 2: Scaled Workload Analysis (for $N$ workers)**\n\nUnder weak scaling, the total problem size is increased to keep the execution time per worker constant. This is achieved by scaling the parallelizable portion of the work, while the sequential portion is assumed to remain fixed, as it often relates to overheads or problem setup/teardown that do not scale with data size.\n\n-   The sequential work for the scaled problem, $W_{s,N}$, remains constant: $W_{s,N} = W_s = \\alpha$.\n-   The parallelizable work, $W_{p,N}$, is scaled linearly with the number of workers: $W_{p,N} = N \\cdot W_p = N(1-\\alpha)$.\n\nThe total work for this new, scaled problem is the sum of its sequential and parallelizable components:\n$$\nW_{\\text{total}}(N) = W_{s,N} + W_{p,N} = \\alpha + N(1-\\alpha)\n$$\n\n**Step 3: Calculating Execution Times for the Scaled Workload**\n\nWe now calculate the execution time for this total work, $W_{\\text{total}}(N)$, on two different machine configurations: a single worker and $N$ workers.\n\n-   **Time on a single worker, $T_1(N)$**: A single worker must perform all the work, both sequential and parallelizable.\n    $$\n    T_1(N) = W_{\\text{total}}(N) = \\alpha + N(1-\\alpha)\n    $$\n-   **Time on $N$ workers, $T_N(N)$**: The $N$ workers execute the scaled workload in parallel.\n    -   The sequential work $W_{s,N}$ cannot be parallelized and must be executed by one worker, taking time $\\alpha$.\n    -   The parallelizable work $W_{p,N}$ is perfectly distributed among the $N$ workers. The time taken for this part is therefore $\\frac{W_{p,N}}{N} = \\frac{N(1-\\alpha)}{N} = 1-\\alpha$.\n    -   The total execution time on $N$ workers, $T_N(N)$, is the sum of the time for the sequential and parallel parts:\n        $$\n        T_N(N) = \\alpha + (1-\\alpha) = 1\n        $$\n    This result confirms the premise of weak scaling: the wall time for the scaled problem on $N$ workers is constant and equal to the baseline time, $T_{\\text{base}}=1$. For conciseness, we denote $T_N(N)$ as $T_N$.\n\n**Step 4: Deriving the Speedup Formula**\n\nThe problem defines the weak-scaling speedup, $S(N)$, as the ratio of the time a single worker would take to execute the scaled workload to the time $N$ workers actually take.\n$$\nS(N) = \\frac{T_1(N)}{T_N}\n$$\nSubstituting the expressions derived in Step 3:\n$$\nS(N) = \\frac{\\alpha + N(1-\\alpha)}{1}\n$$\nThis gives the theoretical formula for Gustafson's Law:\n$$\nS_{\\text{theory}}(N) = \\alpha + N(1-\\alpha)\n$$\n\n### Numerical Verification Plan\n\nThe implementation will test the consistency of this derivation. For each test case $(N, \\alpha)$:\n1.  The parallel execution time for the scaled workload is $T_N = 1$.\n2.  The single-worker execution time for the scaled workload is $T_1(N) = \\alpha + N(1-\\alpha)$.\n3.  The \"measured\" speedup is calculated as $S_{\\text{meas}}(N) = T_1(N) / T_N$.\n4.  The \"theoretical\" speedup is calculated using the derived formula, $S_{\\text{theory}}(N) = \\alpha + N(1-\\alpha)$.\n5.  By construction, $S_{\\textmeas}(N) = S_{\\text{theory}}(N)$. The verification check, $|\\text{S}_{\\text{meas}}(N) - S_{\\text{theory}}(N)| \\le \\varepsilon$, will therefore confirm the self-consistency of the implementation of the derived model. For all valid inputs, the result of this check must be true.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and numerically verifies Gustafson's law for weak scaling speedup.\n\n    For each test case (N, alpha), the program calculates the theoretical speedup\n    and a \"measured\" speedup based on a first-principles model of execution time,\n    then compares them.\n    \"\"\"\n    # Define the test cases from the problem statement: (N, alpha) pairs.\n    # N is the number of workers, alpha is the sequential fraction.\n    test_cases = [\n        (1, 0.3),    # Base case N=1\n        (8, 0.1),    # General case\n        (64, 0.0),   # Edge case: perfectly parallelizable\n        (16, 1.0),   # Edge case: purely sequential\n        (32, 0.25)   # General case\n    ]\n\n    # Tolerance for floating-point comparison\n    epsilon = 1e-12\n\n    results = []\n    for N, alpha in test_cases:\n        # Cast N to float to ensure floating-point arithmetic throughout\n        N = float(N)\n\n        # Step 1: Modeled parallel execution time on N workers, T_N.\n        # In weak scaling, problem size is increased to keep per-worker time\n        # constant. Starting with a baseline time T_base = 1, the time on\n        # N workers for the scaled problem, T_N, is also 1.\n        # T_N_sequential = alpha\n        # T_N_parallel = (N * (1 - alpha)) / N = 1 - alpha\n        # T_N = T_N_sequential + T_N_parallel = alpha + (1 - alpha) = 1.0\n        T_N = 1.0\n\n        # Step 2: Modeled single-worker execution time for the scaled workload, T_1(N).\n        # A single worker must perform all the work of the scaled problem.\n        # The sequential work is 'alpha'.\n        # The parallelizable work, scaled by N, is N * (1 - alpha).\n        T_1_N = alpha + N * (1.0 - alpha)\n\n        # Step 3: Measured speedup, S_meas(N), based on the definition.\n        # S_meas(N) = T_1(N) / T_N\n        S_meas_N = T_1_N / T_N\n\n        # Step 4: Theoretically derived speedup, S_theory(N) (Gustafson's Law).\n        # This is the formula derived from first principles in the solution text.\n        S_theory_N = alpha + N * (1.0 - alpha)\n\n        # Step 5: Boolean verdict.\n        # Check if the measured speedup matches the theoretical formula\n        # within the given tolerance. By construction, they should be identical.\n        verdict = abs(S_meas_N - S_theory_N) <= epsilon\n        results.append(verdict)\n\n    # Final print statement in the exact required format.\n    # The output is a list of booleans, formatted as a string.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}