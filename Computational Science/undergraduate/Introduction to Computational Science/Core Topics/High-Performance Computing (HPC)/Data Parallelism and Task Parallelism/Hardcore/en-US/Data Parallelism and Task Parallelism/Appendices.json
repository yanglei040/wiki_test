{
    "hands_on_practices": [
        {
            "introduction": "While we often assume mathematical operations are immutable, their implementation on computers can introduce subtle complexities. Floating-point arithmetic, for example, is not associative, meaning $(a+b)+c$ is not always equal to $a+(b+c)$. This exercise  explores how different parallel strategies, like data-parallel reduction and task-parallel aggregation, can produce different final results due to this property. By implementing and comparing these methods, you will gain crucial, practical experience with the numerical considerations inherent in parallel algorithm design.",
            "id": "3116514",
            "problem": "You are given the task of demonstrating how non-associativity of floating-point addition leads to different numeric results under two parallel aggregation strategies commonly used in computational science, and of quantifying the errors using a high-accuracy summation as a reference. The fundamental base for this exercise is the rounding model of floating-point addition under the Institute of Electrical and Electronics Engineers (IEEE) Standard $754$, which asserts that for each floating-point addition, the computed result $\\operatorname{fl}(a+b)$ equals $(a+b)(1+\\delta)$ for some $\\delta$ with $|\\delta| \\le \\epsilon$, where $\\epsilon$ is the machine epsilon for the corresponding floating-point format. This model implies that floating-point addition is not associative: for general $a$, $b$, and $c$, one can have $(a+b)+c \\ne a+(b+c)$.\n\nYour program must implement the following summation strategies over finite sequences of real numbers, all in double precision:\n\n- A left-to-right sequential summation producing $S_{\\text{naive}}$.\n- A data-parallel pairwise reduction modeled by a balanced binary tree that iteratively pairs adjacent elements and sums each pair, producing $S_{\\text{pair}}$.\n- A task-parallel chunked summation that first partitions the sequence into fixed-size chunks, computes a left-to-right sum within each chunk, and then sums the chunk totals left-to-right, producing $S_{\\text{chunk}}$.\n- A compensated summation using the Kahan algorithm producing $S_{\\text{kahan}}$, to act as a high-accuracy reference.\n\nDefine the machine epsilon $\\epsilon$ for double precision as the value reported by the environment for $\\text{float64}$. For each test case, compute the absolute errors $E_{\\text{naive}} = |S_{\\text{naive}} - S_{\\text{kahan}}|$, $E_{\\text{pair}} = |S_{\\text{pair}} - S_{\\text{kahan}}|$, and $E_{\\text{chunk}} = |S_{\\text{chunk}} - S_{\\text{kahan}}|$.\n\nImplement the data-parallel reduction by repeatedly replacing the sequence $\\{x_0, x_1, \\dots, x_{n-1}\\}$ with the sequence of pairwise sums $\\{x_0 + x_1, x_2 + x_3, \\dots\\}$, keeping a leftover element unchanged when the length is odd, until a single value remains. Implement the task-parallel chunked summation by partitioning the sequence into contiguous chunks of fixed size $m$, summing each chunk left-to-right, and then summing the resulting chunk totals left-to-right. Use chunk size $m = 64$ for all test cases.\n\nUse the following test suite of sequences, designed to probe happy-path behavior, extreme cancellation, and boundary conditions. Each sequence $\\mathcal{X}$ is specified by listing its elements in order:\n\n- Test case $1$ (large-positive, small positives, large-negative):\n  $$\\mathcal{X}_1 = \\{10^{16}, \\underbrace{1, 1, \\dots, 1}_{1000 \\text{ times}}, -10^{16}\\}.$$\n- Test case $2$ (large-negative, small positives, large-positive):\n  $$\\mathcal{X}_2 = \\{-10^{16}, \\underbrace{1, 1, \\dots, 1}_{1000 \\text{ times}}, 10^{16}\\}.$$\n- Test case $3$ (boundary: identical small integers):\n  $$\\mathcal{X}_3 = \\{\\underbrace{1, 1, \\dots, 1}_{1000 \\text{ times}}\\}.$$\n- Test case $4$ (alternating cancellation across extremes):\n  $$\\mathcal{X}_4 = \\{10^{16}, \\underbrace{-1, -1, \\dots, -1}_{1000 \\text{ times}}, -10^{16}, \\underbrace{1, 1, \\dots, 1}_{1000 \\text{ times}}\\}.$$\n\nFor each test case $\\mathcal{X}_i$, your program must compute and return the following $7$ floating-point values in order:\n$$[S_{\\text{kahan}}, S_{\\text{naive}}, S_{\\text{pair}}, S_{\\text{chunk}}, E_{\\text{naive}}, E_{\\text{pair}}, E_{\\text{chunk}}].$$\n\nFinal Output Format: Your program should produce a single line of output containing the concatenated results across all four test cases as a comma-separated list enclosed in square brackets. The list must contain the $7$ values for test case $1$ first, then the $7$ values for test case $2$, then test case $3$, and then test case $4$, yielding a total of $28$ numbers. For example, the output shape must be\n$$[S_{\\text{kahan}}^{(1)}, S_{\\text{naive}}^{(1)}, S_{\\text{pair}}^{(1)}, S_{\\text{chunk}}^{(1)}, E_{\\text{naive}}^{(1)}, E_{\\text{pair}}^{(1)}, E_{\\text{chunk}}^{(1)}, \\dots, S_{\\text{kahan}}^{(4)}, S_{\\text{naive}}^{(4)}, S_{\\text{pair}}^{(4)}, S_{\\text{chunk}}^{(4)}, E_{\\text{naive}}^{(4)}, E_{\\text{pair}}^{(4)}, E_{\\text{chunk}}^{(4)}].$$\n\nNo physical units or angle units are involved. All numeric answers must be reported as floating-point numbers in standard decimal notation.",
            "solution": "The problem requires the implementation and comparison of four distinct summation algorithms for sequences of double-precision floating-point numbers. This exercise is designed to demonstrate the practical consequences of the non-associativity of floating-point addition, a cornerstone concept in numerical analysis rooted in the finite precision representation of real numbers as specified by the IEEE 754 standard. The four algorithms—naive sequential, pairwise data-parallel, chunked task-parallel, and Kahan's compensated summation—will be evaluated on a suite of test cases designed to elicit different numerical error behaviors. Kahan's algorithm will serve as a high-accuracy baseline to quantify the errors of the other three methods.\n\nAll numerical operations will be performed using double-precision floating-point arithmetic, corresponding to the `numpy.float64` data type in the specified Python environment. The machine epsilon for this precision, denoted by $\\epsilon$, is the smallest number such that $1.0 + \\epsilon \\ne 1.0$.\n\nThe four summation algorithms are defined as follows:\n\n1.  **Naive Summation ($S_{\\text{naive}}$)**: This is the most straightforward approach, where the elements of the sequence $\\{x_0, x_1, \\dots, x_{n-1}\\}$ are accumulated one by one in a left-to-right order. The sum $S_k$ at step $k$ is $S_{k-1} + x_k$, with $S_{-1} = 0$. This method is susceptible to large errors, particularly when adding numbers of vastly different magnitudes (catastrophic cancellation) or when summing a long sequence of numbers (accumulation of round-off error).\n\n2.  **Kahan Summation ($S_{\\text{kahan}}$)**: This is a compensated summation algorithm that significantly reduces the numerical error in the total. It works by maintaining a running compensation variable, $c$, which accumulates the error (the low-order bits lost) from each addition. For each element $x_i$, the following steps are performed:\n    - $y \\leftarrow x_i - c$: Subtract the prior error from the current term.\n    - $t \\leftarrow \\text{sum} + y$: Add the corrected term to the running sum.\n    - $c \\leftarrow (t - \\text{sum}) - y$: The new error is the difference between the sum's increment and the corrected term. This captures the low-order bits lost in the addition `sum + y`.\n    - $\\text{sum} \\leftarrow t$: Update the sum.\n    The result, $S_{\\text{kahan}}$, is considered a high-fidelity reference for the true sum.\n\n3.  **Pairwise Summation ($S_{\\text{pair}}$)**: This algorithm models a data-parallel reduction using a balanced binary tree structure. The sequence is repeatedly processed by summing adjacent pairs of elements: $\\{x_0, x_1, x_2, \\dots\\} \\rightarrow \\{x_0+x_1, x_2+x_3, \\dots\\}$. If the sequence has an odd number of elements, the last element is carried over to the next iteration without being summed. This process is repeated until a single element remains. This approach tends to be more accurate than naive summation because it keeps the magnitudes of intermediate sums more balanced, reducing the growth of relative error. The error bound is proportional to $\\log_2(n)\\epsilon$, as opposed to $n\\epsilon$ for the naive sum.\n\n4.  **Chunked Summation ($S_{\\text{chunk}}$)**: This algorithm models a task-parallel approach. The input sequence is first partitioned into contiguous sub-sequences (chunks) of a fixed size, $m=64$. Each chunk is summed independently (and in parallel, conceptually) using the naive left-to-right method. The resulting list of partial sums (one from each chunk) is then summed, also using the naive left-to-right method, to produce the final result $S_{\\text{chunk}}$. The accuracy of this method depends on the data distribution within and across chunks.\n\nThe analysis proceeds by first generating the four test sequences as specified, ensuring all values are `numpy.float64`. For each sequence, we compute the sum using each of the four algorithms. Finally, we calculate the absolute errors $E_{\\text{naive}}$, $E_{\\text{pair}}$, and $E_{\\text{chunk}}$ by taking the absolute difference between their respective sums and the high-accuracy Kahan sum $S_{\\text{kahan}}$. The expected behavior is that for test cases involving catastrophic cancellation (cases 1, 2, 4), the naive, pairwise, and chunked methods will exhibit significant errors, while Kahan's method will yield a result close to the true mathematical sum. For the well-behaved case (case 3), all methods are expected to be accurate.\n\nThe implementation will consist of four functions, one for each summation algorithm, and a main `solve` function to orchestrate the testing, calculation, and formatting of the final output as a single, comma-separated string of $28$ floating-point numbers.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares four floating-point summation algorithms\n    to demonstrate the effects of non-associativity in numerical computation.\n    \"\"\"\n\n    def sum_kahan(arr: np.ndarray) -> np.float64:\n        \"\"\"Computes the sum of an array using Kahan's compensated summation algorithm.\"\"\"\n        s = np.float64(0.0)\n        c = np.float64(0.0)  # A running compensation for lost low-order bits.\n        for x in arr:\n            y = x - c\n            t = s + y\n            c = (t - s) - y\n            s = t\n        return s\n\n    def sum_naive(arr: np.ndarray) -> np.float64:\n        \"\"\"Computes the sum of an array using simple left-to-right accumulation.\"\"\"\n        s = np.float64(0.0)\n        for x in arr:\n            s += x\n        return s\n\n    def sum_pair(arr: np.ndarray) -> np.float64:\n        \"\"\"Computes the sum using a data-parallel pairwise reduction.\"\"\"\n        current_arr = arr.copy()\n        while len(current_arr) > 1:\n            n = len(current_arr)\n            new_len = (n + 1) // 2\n            next_arr = np.zeros(new_len, dtype=np.float64)\n            for i in range(n // 2):\n                next_arr[i] = current_arr[2 * i] + current_arr[2 * i + 1]\n            if n % 2 != 0:\n                next_arr[-1] = current_arr[-1]\n            current_arr = next_arr\n        return current_arr[0] if len(current_arr) > 0 else np.float64(0.0)\n        \n    def sum_chunk(arr: np.ndarray, chunk_size: int) -> np.float64:\n        \"\"\"Computes the sum using a task-parallel chunked approach.\"\"\"\n        n = len(arr)\n        if n == 0:\n            return np.float64(0.0)\n        \n        num_chunks = (n + chunk_size - 1) // chunk_size\n        chunk_sums = np.zeros(num_chunks, dtype=np.float64)\n\n        for i in range(num_chunks):\n            start = i * chunk_size\n            end = min(start + chunk_size, n)\n            chunk = arr[start:end]\n            # Naive sum within each chunk\n            chunk_sums[i] = sum_naive(chunk)\n        \n        # Naive sum of the chunk sums\n        return sum_naive(chunk_sums)\n\n    # Define the test cases from the problem statement.\n    test_cases_defs = [\n        # Case 1: Large positive, small positives, large negative\n        np.concatenate(([np.float64(1e16)], np.ones(1000, dtype=np.float64), [np.float64(-1e16)])),\n        # Case 2: Large negative, small positives, large positive\n        np.concatenate(([np.float64(-1e16)], np.ones(1000, dtype=np.float64), [np.float64(1e16)])),\n        # Case 3: Identical small integers\n        np.ones(1000, dtype=np.float64),\n        # Case 4: Alternating cancellation\n        np.concatenate(([np.float64(1e16)], np.full(1000, -1.0, dtype=np.float64), [np.float64(-1e16)], np.ones(1000, dtype=np.float64)))\n    ]\n    \n    chunk_size = 64\n    all_results = []\n\n    for case_arr in test_cases_defs:\n        s_kahan = sum_kahan(case_arr)\n        s_naive = sum_naive(case_arr)\n        s_pair = sum_pair(case_arr)\n        s_chunk = sum_chunk(case_arr, chunk_size)\n\n        e_naive = np.abs(s_naive - s_kahan)\n        e_pair = np.abs(s_pair - s_kahan)\n        e_chunk = np.abs(s_chunk - s_kahan)\n\n        all_results.extend([s_kahan, s_naive, s_pair, s_chunk, e_naive, e_pair, e_chunk])\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Achieving optimal performance in parallel computing requires not just dividing the work, but dividing it equitably. This practice  delves into the critical concept of load balancing by simulating how different scheduling strategies perform on non-uniform workloads. By comparing a simple static data-parallel partitioning with more adaptive dynamic and task-based approaches, you will develop an intuition for tuning task granularity to maximize computational throughput.",
            "id": "3116484",
            "problem": "You will implement a self-contained program to compare data parallelism and task parallelism under granularity tuning on non-uniform workloads. The foundational base consists of the definitions of makespan, throughput, and identical worker scheduling with independent tasks. For a set of independent tasks with costs, scheduled on identical Processing Elements (PEs), the makespan is the completion time of the slowest PE, and throughput is the total work divided by the makespan. Starting from these definitions, derive the scheduling behaviors and compute the throughput for the specified strategies.\n\nDefinitions and base principles:\n- Let there be $N$ tasks with non-negative costs $w_1, w_2, \\dots, w_N$. Each $w_i$ represents the compute cost of task $i$ measured in abstract \"work units\" where $1$ work unit equals $1$ time unit.\n- Let there be $P$ identical Processing Elements (PEs), $P \\ge 1$, each executing tasks at a rate of $1$ work unit per time unit.\n- The total work is $W = \\sum_{i=1}^{N} w_i$.\n- The load of a PE is the sum of the costs of tasks assigned to it. The system makespan is $M = \\max\\{\\text{load of each PE}\\}$.\n- The throughput is $T = W / M$, expressed in \"work units per time unit\". Output throughput values as floating-point numbers rounded to $6$ decimal places.\n\nYou will implement and compare the following scheduling strategies:\n1. Static data-parallel partitioning (contiguous): Partition the $N$ tasks into $P$ contiguous blocks whose sizes differ by at most $1$, in original index order. Formally, let $q = \\lfloor N / P \\rfloor$ and $r = N \\bmod P$. For PE $j$ where $0 \\le j < P$, assign a contiguous block of size $q + 1$ if $j < r$, else assign a block of size $q$. Compute the load of each PE as the sum of its assigned task costs, then compute $M$ and $T$.\n\n2. Guided self-scheduling (data-parallel dynamic): Maintain a queue of remaining tasks in original index order. Let $R$ be the number of remaining tasks at any step. When a PE becomes available, assign a chunk of size $k = \\max\\left(\\lceil \\alpha \\cdot R / P \\rceil, g_{\\min}\\right)$, truncated so that $k \\le R$, where $\\alpha \\in (0, 1]$ and $g_{\\min} \\in \\mathbb{N}$ are given tuning parameters. Each assigned chunk is the next $k$ consecutive tasks in the queue. Model the dynamic schedule by simulating PE finish times: initially, all $P$ PEs are idle at time $0$; whenever the earliest finishing PE becomes available, assign the next chunk and update its finish time by adding the chunk cost. Continue until all tasks are assigned. The makespan $M$ is the maximum final finish time among the $P$ PEs. Then compute $T$.\n\n3. Fork-join task parallelism (fixed coarse task size): First coarsen the original task list into consecutive groups (coarse tasks) of fixed size $g \\in \\mathbb{N}$, with the last group possibly smaller, preserving original order. Each coarse task has cost equal to the sum of its constituent task costs. Schedule these coarse tasks on $P$ PEs using greedy list scheduling: at each step, assign the next coarse task to the PE that becomes available earliest and update its finish time by the coarse task cost. The makespan $M$ is the maximum final finish time among the $P$ PEs. Then compute $T$.\n\nAssumptions:\n- All tasks are independent.\n- Scheduling overhead is negligible (zero).\n- Costs $w_i$ are deterministic and known.\n- Time and work share the same unit, so throughput $T$ is unit-consistent and dimensionally \"work units per time unit\".\n\nImplementers must strictly follow the above models to make the results comparable.\n\nTest suite:\nCompute, for each test case, the triple of throughputs $[T_{\\text{static}}, T_{\\text{guided}}, T_{\\text{fork\\_join}}]$ under the specified parameters. The program must produce a single line of output containing the results as a comma-separated list of these triples enclosed in square brackets, with each triple enclosed in square brackets. All floats must be rounded to $6$ decimal places.\n\n- Test case $1$ (happy path, non-uniform workload):\n  - $N = 20$, $P = 4$.\n  - Costs $w$ (in index order): $[5, 1, 1, 1, 10, 1, 1, 3, 2, 8, 1, 1, 1, 7, 1, 1, 4, 1, 1, 12]$.\n  - Guided parameters: $\\alpha = 1.0$, $g_{\\min} = 1$.\n  - Fork-join coarse task size: $g = 3$.\n\n- Test case $2$ (boundary: single PE):\n  - $N = 10$, $P = 1$.\n  - Costs $w$: $[2, 5, 1, 3, 4, 2, 6, 1, 1, 8]$.\n  - Guided parameters: $\\alpha = 0.5$, $g_{\\min} = 1$.\n  - Fork-join coarse task size: $g = 2$.\n\n- Test case $3$ (edge: tasks fewer than PEs):\n  - $N = 5$, $P = 8$.\n  - Costs $w$: $[7, 1, 3, 2, 9]$.\n  - Guided parameters: $\\alpha = 1.0$, $g_{\\min} = 1$.\n  - Fork-join coarse task size: $g = 1$.\n\n- Test case $4$ (skewed large workload with one heavy task):\n  - $N = 100$, $P = 8$.\n  - Costs $w$ defined as: $w_i = 1$ for all $i \\in \\{1, 2, \\dots, 100\\}$ except $w_{50} = 100$.\n  - Guided parameters: $\\alpha = 1.0$, $g_{\\min} = 1$.\n  - Fork-join coarse task size: $g = 10$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list of the per-test-case triples, enclosed in square brackets, with each triple enclosed in square brackets. For example: $[[T_1^{\\text{static}},T_1^{\\text{guided}},T_1^{\\text{fork\\_join}}],[T_2^{\\text{static}},T_2^{\\text{guided}},T_2^{\\text{fork\\_join}}],\\dots]$, with numeric values rounded to $6$ decimal places and no spaces.",
            "solution": "The user has provided a computational problem requiring the implementation and comparison of three parallel task scheduling strategies: static contiguous partitioning, guided self-scheduling, and a fork-join model based on task coarsening and list scheduling. The primary metric for comparison is throughput, defined as total work divided by the makespan.\n\nThe problem statement is rigorously validated and found to be valid. It is scientifically grounded in the principles of parallel computing and scheduling theory, well-posed with precise algorithmic definitions and complete input data, and objective in its formulation. All terms like makespan, throughput, and the specifics of each scheduling algorithm are clearly defined. The test cases provided cover a range of scenarios, including typical non-uniform workloads, boundary conditions ($P=1$), edge cases ($N<P$), and skewed distributions. Therefore, I will proceed with a full solution.\n\nThe solution will be implemented in Python, adhering to the specified environment. It will consist of three distinct functions, each responsible for calculating the throughput for one of the scheduling strategies. A main driver function will orchestrate the execution for each test case and format the output as required. The mathematical entities in this explanation are rendered in LaTeX, as per the rules.\n\n### Strategy 1: Static Data-Parallel Partitioning (Contiguous)\n\nThis strategy involves a fixed, predetermined assignment of tasks to Processing Elements (PEs). The list of $N$ tasks is divided into $P$ contiguous sublists (blocks). To ensure the workload is distributed as evenly as possible with this constraint, the sizes of these blocks differ by at most $1$.\n\nLet $N$ be the total number of tasks and $P$ be the number of PEs. We calculate $q = \\lfloor N / P \\rfloor$ and $r = N \\bmod P$. The first $r$ PEs are each assigned a block of $q+1$ tasks, and the remaining $P-r$ PEs are assigned a block of $q$ tasks.\n\nThe load for each PE is the sum of the costs, $w_i$, of the tasks in its assigned block. The system makespan, $M_{\\text{static}}$, is the maximum load found among all $P$ PEs:\n$$ M_{\\text{static}} = \\max_{j=0, \\dots, P-1} \\left( \\sum_{i \\in \\text{Tasks}(j)} w_i \\right) $$\nwhere $\\text{Tasks}(j)$ is the set of task indices assigned to PE $j$.\n\nThe total work is $W = \\sum_{i=1}^{N} w_i$. The throughput is then:\n$$ T_{\\text{static}} = \\frac{W}{M_{\\text{static}}} $$\n\nThis method is simple and has no scheduling overhead at runtime, but its effectiveness is highly dependent on the distribution of task costs $w_i$. A non-uniform distribution can lead to significant load imbalance and a large makespan.\n\n### Strategy 2: Guided Self-Scheduling (GSS)\n\nThis is a dynamic scheduling strategy that adapts the amount of work assigned to PEs over time. It aims to balance the load by assigning large chunks of tasks at the beginning when many tasks remain, and smaller chunks towards the end to fill in the gaps and reduce the likelihood of a PE being idle for long.\n\nThe simulation proceeds as follows:\n1. Initialize the finish time of all $P$ PEs to $0$. A central queue of tasks, ordered by their original index, is maintained.\n2. At each step, identify the PE with the earliest finish time. This is the next PE to become available.\n3. The number of tasks to assign, the chunk size $k$, is determined based on the number of remaining tasks $R$. The formula is $k = \\max\\left(\\lceil \\alpha \\cdot R / P \\rceil, g_{\\min}\\right)$, where $\\alpha \\in (0, 1]$ is a tuning parameter controlling the rate of chunk size decrease, and $g_{\\min}$ is a minimum chunk size. The calculated $k$ is truncated if it exceeds $R$.\n4. The next $k$ tasks from the queue are assigned to the selected PE. The cost of this chunk (sum of its task costs) is added to the PE's current finish time to determine its new finish time.\n5. This process repeats until all tasks have been assigned.\n\nThe makespan, $M_{\\text{guided}}$, is the maximum finish time among all PEs after all tasks are scheduled. The throughput is:\n$$ T_{\\text{guided}} = \\frac{W}{M_{\\text{guided}}} $$\n\n### Strategy 3: Fork-Join Task Parallelism (Coarse Graining + List Scheduling)\n\nThis strategy first transforms the granularity of the problem. The original list of $N$ fine-grained tasks is coarsened into a smaller number of coarse-grained tasks. This is done by grouping a fixed number, $g$, of consecutive original tasks. The cost of each coarse task is the sum of the costs of the constituent fine-grained tasks. The last coarse task may contain fewer than $g$ original tasks if $N$ is not a multiple of $g$.\n\nThese new coarse tasks are then scheduled on the $P$ PEs using a greedy list scheduling algorithm. The algorithm is similar to the GSS simulation:\n1. Initialize the finish time of all $P$ PEs to $0$.\n2. Iterate through the list of coarse tasks in order.\n3. For each coarse task, assign it to the PE with the earliest current finish time (i.e., the one that will become idle first).\n4. Update that PE's finish time by adding the cost of the coarse task.\n\nAfter all coarse tasks have been scheduled, the makespan, $M_{\\text{fork\\_join}}$, is the maximum finish time across all PEs. The throughput is calculated using the total work $W$ of the original tasks:\n$$ T_{\\text{fork\\_join}} = \\frac{W}{M_{\\text{fork\\_join}}} $$\n\nThis approach can reduce scheduling overhead in real systems (though overhead is assumed to be zero here) and can smooth out variations in the costs of fine-grained tasks.\n\nI will now proceed to implement these three strategies and apply them to the given test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to run the scheduling simulations and print results.\n    \"\"\"\n\n    def solve_static(w, P):\n        \"\"\"\n        Calculates throughput using static contiguous partitioning.\n        \"\"\"\n        N = len(w)\n        if N == 0:\n            return 0.0\n        \n        total_work = np.sum(w)\n        if total_work == 0:\n            return 0.0\n\n        pe_loads = np.zeros(P)\n        \n        q = N // P\n        r = N % P\n        \n        current_task_idx = 0\n        for j in range(P):\n            block_size = q + 1 if j < r else q\n            if block_size > 0:\n                end_idx = current_task_idx + block_size\n                block_cost = np.sum(w[current_task_idx:end_idx])\n                pe_loads[j] = block_cost\n                current_task_idx = end_idx\n        \n        makespan = np.max(pe_loads) if P > 0 else 0.0\n        if makespan == 0:\n            return 0.0\n        \n        throughput = total_work / makespan\n        return throughput\n\n    def solve_guided(w, P, alpha, gmin):\n        \"\"\"\n        Calculates throughput using guided self-scheduling.\n        \"\"\"\n        N = len(w)\n        if N == 0:\n            return 0.0\n        \n        total_work = np.sum(w)\n        if total_work == 0:\n            return 0.0\n\n        pe_finish_times = np.zeros(P)\n        task_idx = 0\n        \n        while task_idx < N:\n            pe_idx = np.argmin(pe_finish_times)\n            \n            R = N - task_idx\n            k = int(math.ceil(alpha * R / P))\n            k = max(k, gmin)\n            k = min(k, R)\n            \n            end_idx = task_idx + k\n            chunk_cost = np.sum(w[task_idx:end_idx])\n            pe_finish_times[pe_idx] += chunk_cost\n            task_idx = end_idx\n            \n        makespan = np.max(pe_finish_times) if P > 0 else 0.0\n        if makespan == 0:\n            return 0.0\n            \n        throughput = total_work / makespan\n        return throughput\n\n    def solve_fork_join(w, P, g):\n        \"\"\"\n        Calculates throughput using fork-join with coarse tasking and list scheduling.\n        \"\"\"\n        N = len(w)\n        if N == 0:\n            return 0.0\n            \n        total_work = np.sum(w)\n        if total_work == 0:\n            return 0.0\n            \n        coarse_tasks = []\n        for i in range(0, N, g):\n            chunk = w[i : i + g]\n            coarse_tasks.append(np.sum(chunk))\n            \n        pe_finish_times = np.zeros(P)\n        \n        for cost in coarse_tasks:\n            pe_idx = np.argmin(pe_finish_times)\n            pe_finish_times[pe_idx] += cost\n            \n        makespan = np.max(pe_finish_times) if P > 0 else 0.0\n        if makespan == 0:\n            return 0.0\n            \n        throughput = total_work / makespan\n        return throughput\n\n    # Test case 4 workload generation\n    w4 = np.ones(100)\n    w4[49] = 100.0  # w_50 in 1-based indexing\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'w': [5, 1, 1, 1, 10, 1, 1, 3, 2, 8, 1, 1, 1, 7, 1, 1, 4, 1, 1, 12], 'P': 4, 'guided_params': (1.0, 1), 'fork_join_g': 3},\n        {'w': [2, 5, 1, 3, 4, 2, 6, 1, 1, 8], 'P': 1, 'guided_params': (0.5, 1), 'fork_join_g': 2},\n        {'w': [7, 1, 3, 2, 9], 'P': 8, 'guided_params': (1.0, 1), 'fork_join_g': 1},\n        {'w': w4.tolist(), 'P': 8, 'guided_params': (1.0, 1), 'fork_join_g': 10},\n    ]\n\n    results = []\n    for case in test_cases:\n        w_case = case['w']\n        P_case = case['P']\n        alpha_case, gmin_case = case['guided_params']\n        g_case = case['fork_join_g']\n\n        t_static = solve_static(w_case, P_case)\n        t_guided = solve_guided(w_case, P_case, alpha_case, gmin_case)\n        t_fork_join = solve_fork_join(w_case, P_case, g_case)\n        \n        results.append([t_static, t_guided, t_fork_join])\n\n    # Final print statement in the exact required format.\n    formatted_triples = [\n        f\"[{','.join([f'{val:.6f}' for val in triple])}]\" \n        for triple in results\n    ]\n    print(f\"[{','.join(formatted_triples)}]\")\n    \nsolve()\n```"
        },
        {
            "introduction": "Many important algorithms, particularly in dynamic programming, contain inherent data dependencies that challenge simple parallelization schemes. This exercise  uses the classic Smith–Waterman algorithm to demonstrate the \"wavefront\" computational pattern, a powerful technique for parallelizing such problems. You will implement and contrast a fine-grained task-parallel schedule with a coarse-grained data-parallel blocked schedule, allowing you to directly analyze the fundamental trade-off between maximizing concurrency and minimizing synchronization overhead.",
            "id": "3116592",
            "problem": "You are to implement two algorithmic schedules for computing the Smith–Waterman local alignment dynamic programming (DP) matrix and analyze synchronization granularity in terms of barrier frequency and its impact on throughput under a simple, explicitly stated cost model. The two schedules are: a task-parallel wavefront schedule across DP anti-diagonals, and a data-parallel blocked schedule across tile anti-diagonals. The implementation must compute the same DP matrix in both schedules for given sequence pairs, and then compute barrier counts and throughputs using the cost model. Any acronym must be defined on first use; dynamic programming (DP) and Single Instruction Multiple Data (SIMD) will be spelled out first if used. All mathematical entities must be written in LaTeX.\n\nFundamental base. Use the core definition of dynamic programming for the Smith–Waterman local alignment. Let sequences be $A$ and $B$ of lengths $m$ and $n$. Construct a matrix $H$ of shape $(m+1)\\times(n+1)$ where the first row and first column are initialized to $0$. For $i\\in\\{1,\\dots,m\\}$ and $j\\in\\{1,\\dots,n\\}$, define\n$$\nH[i,j] = \\max\\Big(0,\\; H[i-1,j-1] + s(A[i],B[j]),\\; H[i-1,j] + g,\\; H[i,j-1] + g\\Big),\n$$\nwhere $s(\\cdot,\\cdot)$ is the substitution score that yields $+2$ for a match and $-1$ for a mismatch, and $g=-1$ is the gap penalty. This recurrence is the scientific basis of the computation.\n\nScheduling and synchronization. The dependencies of $H[i,j]$ are $H[i-1,j-1]$, $H[i-1,j]$, and $H[i,j-1]$, so cells with the same sum index $i+j=d$ can be processed after all cells with $i+j=d-1$ are completed. This induces a partial order that can be topologically scheduled by anti-diagonals of the DP matrix (a wavefront). In a task-parallel wavefront schedule, one barrier is inserted between each anti-diagonal to enforce global synchronization across tasks. In a data-parallel blocked schedule, the matrix is partitioned into rectangular tiles of sizes $b_i$ along the $i$-axis and $b_j$ along the $j$-axis, and tiles are scheduled by their tile anti-diagonals; one barrier is inserted between each tile anti-diagonal. Within a tile, computation proceeds sequentially in row-major order to respect intra-tile dependencies.\n\nCost model and throughput. Assume each DP cell evaluation costs $c_{\\text{cell}}$ time units, and each barrier costs $c_{\\text{barrier}}$ time units. The total number of DP cells is $N_{\\text{cells}}=m\\cdot n$. Let $B_{\\text{task}}$ denote the number of barriers in the task-parallel wavefront schedule, and $B_{\\text{block}}$ denote the number of barriers in the data-parallel blocked schedule. The total time for each schedule is\n$$\nT_{\\text{task}} = N_{\\text{cells}}\\cdot c_{\\text{cell}} + B_{\\text{task}}\\cdot c_{\\text{barrier}},\\quad\nT_{\\text{block}} = N_{\\text{cells}}\\cdot c_{\\text{cell}} + B_{\\text{block}}\\cdot c_{\\text{barrier}}.\n$$\nDefine throughput as\n$$\n\\Theta = \\frac{N_{\\text{cells}}}{T},\n$$\nexpressed in cells per abstract time unit. You must compute $B_{\\text{task}}$, $B_{\\text{block}}$, $\\Theta_{\\text{task}}$, and $\\Theta_{\\text{block}}$ for each test case.\n\nWhat to implement. Write a single, complete program that:\n- Implements the Smith–Waterman DP two ways: a task-parallel wavefront schedule over cell anti-diagonals and a data-parallel blocked schedule over tile anti-diagonals. Both implementations must compute the same $H$ for a given input.\n- Counts the number of barriers in each schedule by counting the number of scheduled anti-diagonals actually traversed that contain work.\n- Uses the cost model to compute throughput for each schedule.\n\nUnits. Throughput must be reported in cells per abstract time unit as a float rounded to six decimal places. No physical units are involved. Angles are not used. Percentages are not used.\n\nTest suite. Use the following fixed inputs. For each test case, you are given $A$, $B$, $b_i$, $b_j$, $c_{\\text{cell}}$, and $c_{\\text{barrier}}$.\n- Test $1$: $A=\\text{\"ACGTACGT\"}$ (length $8$), $B=\\text{\"ACGTACGA\"}$ (length $8$), $b_i=2$, $b_j=2$, $c_{\\text{cell}}=1.0$, $c_{\\text{barrier}}=10.0$.\n- Test $2$: $A=\\text{\"ATCGATCGATCGATCGATCGATCGATCGATCG\"}$ (length $32$), $B=\\text{\"ACGTACGT\"}$ (length $8$), $b_i=8$, $b_j=4$, $c_{\\text{cell}}=1.0$, $c_{\\text{barrier}}=5.0$.\n- Test $3$: $A=\\text{\"A\"}$ repeated $64$ times, $B=\\text{\"A\"}$ repeated $64$ times, $b_i=64$, $b_j=64$, $c_{\\text{cell}}=1.0$, $c_{\\text{barrier}}=50.0$.\n- Test $4$: $A=\\text{\"ACGTACGTACGTAC\"}$ (length $15$), $B=\\text{\"A\"}$ (length $1$), $b_i=3$, $b_j=1$, $c_{\\text{cell}}=1.0$, $c_{\\text{barrier}}=8.0$.\n- Test $5$: $A=\\text{\"ACGT\"}$ (length $4$), $B=\\text{\"TGCA\"}$ (length $4$), $b_i=1$, $b_j=1$, $c_{\\text{cell}}=1.0$, $c_{\\text{barrier}}=2.0$.\n\nRequired output. For each test case, output the list $[B_{\\text{task}},B_{\\text{block}},\\Theta_{\\text{task}},\\Theta_{\\text{block}}]$, where barrier counts are integers and throughputs are floats rounded to six decimals. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, for example $[[\\cdots],[\\cdots],[\\cdots]]$ for the five test cases above.",
            "solution": "The problem requires the implementation and analysis of two parallel scheduling strategies for the Smith-Waterman algorithm for local sequence alignment. The analysis focuses on synchronization granularity, quantified by the number of barriers required, and its effect on computational throughput under a specified cost model.\n\nThe core of the computation is a dynamic programming (DP) algorithm. Let the two sequences be $A$ and $B$, with respective lengths $m$ and $n$. A DP matrix, denoted as $H$, of size $(m+1) \\times (n+1)$ is constructed. The entries of this matrix are computed according to the following recurrence relation for $i \\in \\{1,\\dots,m\\}$ and $j \\in \\{1,\\dots,n\\}$:\n$$\nH[i,j] = \\max\\Big(0,\\; H[i-1,j-1] + s(A[i-1],B[j-1]),\\; H[i-1,j] + g,\\; H[i,j-1] + g\\Big)\n$$\nThe first row and first column of $H$ are initialized to $0$. The function $s(\\cdot,\\cdot)$ is a substitution score, defined as $+2$ for a character match and $-1$ for a mismatch. The parameter $g$ is the gap penalty, given as $g=-1$. Note that for an implementation using $0$-based indexing for sequences $A$ and $B$, the term $s(A[i],B[j])$ in the problem's mathematical notation corresponds to $s(A[i-1], B[j-1])$ in code.\n\nThe computation of an element $H[i,j]$ depends on its neighbors $H[i-1,j-1]$, $H[i-1,j]$, and $H[i,j-1]$. This dependency structure dictates the possible parallelization strategies. Specifically, all cells $(i,j)$ that lie on the same anti-diagonal, defined by a constant sum of indices $d = i+j$, can be computed concurrently, as their dependencies are all located on previous anti-diagonals ($d-1$ and $d-2$). This is often referred to as a wavefront computation.\n\n**Schedule 1: Task-Parallel Wavefront Schedule**\n\nIn this schedule, parallelism is expressed at the level of individual DP cells. Each anti-diagonal of the DP matrix forms a wavefront of computation. A synchronization barrier is inserted after the computation of all cells in a wavefront is complete, ensuring that all necessary values are available for the next wavefront.\n\nThe indices $i$ range from $1$ to $m$, and $j$ from $1$ to $n$. The sum $d=i+j$ thus ranges from the minimum value $1+1=2$ for cell $(1,1)$ to the maximum value $m+n$ for cell $(m,n)$. Each distinct value of $d$ in this range corresponds to a unique anti-diagonal that contains cells to be computed. The total number of such anti-diagonals, and therefore the number of required barriers, is the number of integer values from $2$ to $m+n$, inclusive.\n$$\nB_{\\text{task}} = (m+n) - 2 + 1 = m+n-1\n$$\n\n**Schedule 2: Data-Parallel Blocked Schedule**\n\nThis schedule introduces a coarser grain of parallelism by partitioning the DP matrix into rectangular tiles of size $b_i \\times b_j$. The computation proceeds tile by tile instead of cell by cell. The dependency structure between tiles mirrors the dependency structure between cells: a tile with tile-indices $(I,J)$ can only be computed after its neighboring tiles $(I-1,J-1)$, $(I-1,J)$, and $(I,J-1)$ are complete.\n\nLet the number of tiles along the $i$-axis (rows) and $j$-axis (columns) be $N_i$ and $N_j$, respectively. These are calculated by taking the ceiling of the dimension divided by the tile size:\n$$\nN_i = \\lceil m/b_i \\rceil, \\quad N_j = \\lceil n/b_j \\rceil\n$$\nIn integer arithmetic, these can be computed as $N_i = (m + b_i - 1) // b_i$ and $N_j = (n + b_j - 1) // b_j$.\n\nSimilar to the task-parallel case, a wavefront schedule is applied, but at the level of tiles. All tiles on a tile anti-diagonal, defined by $D = I+J$ (where $I \\in \\{1, \\dots, N_i\\}$ and $J \\in \\{1, \\dots, N_j\\}$), can be processed in parallel. A barrier is inserted after each tile anti-diagonal. The sum of tile indices $D$ ranges from $1+1=2$ to $N_i+N_j$. The total number of barriers is the number of such tile anti-diagonals:\n$$\nB_{\\text{block}} = (N_i+N_j) - 2 + 1 = N_i+N_j-1\n$$\nWithin each tile, the cells are computed sequentially, for example in row-major order, to respect the local cell-level dependencies.\n\n**Cost Model and Throughput Analysis**\n\nThe performance of each schedule is evaluated using a simple cost model. The total number of DP cells to be computed is $N_{\\text{cells}} = m \\times n$. Each cell computation is assumed to take $c_{\\text{cell}}$ time units, and each synchronization barrier costs $c_{\\text{barrier}}$ time units. The total time $T$ for a schedule is the sum of the time for all cell computations and the time for all barriers.\n\nFor the task-parallel schedule:\n$$\nT_{\\text{task}} = N_{\\text{cells}} \\cdot c_{\\text{cell}} + B_{\\text{task}} \\cdot c_{\\text{barrier}}\n$$\nFor the data-parallel blocked schedule:\n$$\nT_{\\text{block}} = N_{\\text{cells}} \\cdot c_{\\text{cell}} + B_{\\text{block}} \\cdot c_{\\text{barrier}}\n$$\nThroughput, $\\Theta$, is defined as the number of cells computed per unit of abstract time:\n$$\n\\Theta = \\frac{N_{\\text{cells}}}{T}\n$$\nThus, for the two schedules, we compute:\n$$\n\\Theta_{\\text{task}} = \\frac{N_{\\text{cells}}}{T_{\\text{task}}} \\quad \\text{and} \\quad \\Theta_{\\text{block}} = \\frac{N_{\\text{cells}}}{T_{\\text{block}}}\n$$\nThe implementation will carry out these calculations for each provided test case, including the full computation of the DP matrix $H$ for both scheduling methods to confirm they produce identical results, which they must if implemented correctly. The primary outputs are the barrier counts ($B_{\\text{task}}, B_{\\text{block}}$) and throughputs ($\\Theta_{\\text{task}}, \\Theta_{\\text{block}}$).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef smith_waterman_recurrence(H, s_val, i, j, g):\n    \"\"\"\n    Computes a single cell H[i, j] of the Smith-Waterman DP matrix.\n    H is the DP matrix.\n    s_val is the substitution score for the current characters.\n    i, j are the 1-based indices of the cell.\n    g is the gap penalty.\n    \"\"\"\n    match_score = H[i - 1, j - 1] + s_val\n    deletion_score = H[i - 1, j] + g\n    insertion_score = H[i, j - 1] + g\n    H[i, j] = max(0, match_score, deletion_score, insertion_score)\n\ndef compute_task_parallel(A, B, g, s):\n    \"\"\"\n    Computes the Smith-Waterman DP matrix using a task-parallel wavefront schedule.\n    Returns the computed matrix H and the number of barriers.\n    \"\"\"\n    m, n = len(A), len(B)\n    H = np.zeros((m + 1, n + 1), dtype=np.float64)\n    if m == 0 or n == 0:\n        return H, 0\n    num_barriers = m + n - 1\n\n    # Iterate through anti-diagonals d = i + j\n    for d in range(2, m + n + 1):\n        # In a parallel implementation, the inner loop would be parallelized.\n        # Here, we simulate the sequential dependency between wavefronts.\n        for i in range(1, m + 1):\n            j = d - i\n            if 1 <= j <= n:\n                s_val = s(A[i - 1], B[j - 1])\n                smith_waterman_recurrence(H, s_val, i, j, g)\n    \n    return H, num_barriers\n\ndef compute_data_parallel_blocked(A, B, bi, bj, g, s):\n    \"\"\"\n    Computes the Smith-Waterman DP matrix using a data-parallel blocked schedule.\n    Returns the computed matrix H and the number of barriers.\n    \"\"\"\n    m, n = len(A), len(B)\n    H = np.zeros((m + 1, n + 1), dtype=np.float64)\n    if m == 0 or n == 0:\n        return H, 0\n\n    # Calculate number of tiles\n    Ni = (m + bi - 1) // bi\n    Nj = (n + bj - 1) // bj\n    num_barriers = Ni + Nj - 1\n\n    # Iterate through tile anti-diagonals D = I + J\n    for D in range(2, Ni + Nj + 1):\n        # In a parallel implementation, the inner loop over tiles would be parallelized.\n        for I_tile in range(1, Ni + 1):\n            J_tile = D - I_tile\n            if 1 <= J_tile <= Nj:\n                # Process all cells within the tile (I_tile, J_tile) sequentially\n                i_start = (I_tile - 1) * bi + 1\n                i_end = min(I_tile * bi, m) + 1\n                j_start = (J_tile - 1) * bj + 1\n                j_end = min(J_tile * bj, n) + 1\n\n                for i in range(i_start, i_end):\n                    for j in range(j_start, j_end):\n                        s_val = s(A[i - 1], B[j - 1])\n                        smith_waterman_recurrence(H, s_val, i, j, g)\n                        \n    return H, num_barriers\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (A, B, bi, bj, c_cell, c_barrier)\n        (\"ACGTACGT\", \"ACGTACGA\", 2, 2, 1.0, 10.0),\n        (\"ATCGATCGATCGATCGATCGATCGATCGATCG\", \"ACGTACGT\", 8, 4, 1.0, 5.0),\n        (\"A\" * 64, \"A\" * 64, 64, 64, 1.0, 50.0),\n        (\"ACGTACGTACGTAC\", \"A\", 3, 1, 1.0, 8.0),\n        (\"ACGT\", \"TGCA\", 1, 1, 1.0, 2.0),\n    ]\n\n    # Smith-Waterman parameters\n    g = -1.0\n    s = lambda c1, c2: 2.0 if c1 == c2 else -1.0\n    \n    all_results_str = []\n\n    for A, B, bi, bj, c_cell, c_barrier in test_cases:\n        m, n = len(A), len(B)\n        N_cells = m * n\n\n        # Task-parallel schedule\n        _ , B_task = compute_task_parallel(A, B, g, s)\n        T_task = N_cells * c_cell + B_task * c_barrier\n        Theta_task = N_cells / T_task if T_task > 0 else 0.0\n\n        # Data-parallel blocked schedule\n        _ , B_block = compute_data_parallel_blocked(A, B, bi, bj, g, s)\n        T_block = N_cells * c_cell + B_block * c_barrier\n        Theta_block = N_cells / T_block if T_block > 0 else 0.0\n\n        case_result = [\n            str(B_task),\n            str(B_block),\n            f\"{Theta_task:.6f}\",\n            f\"{Theta_block:.6f}\"\n        ]\n        case_str = f\"[{','.join(case_result)}]\"\n        all_results_str.append(case_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        }
    ]
}