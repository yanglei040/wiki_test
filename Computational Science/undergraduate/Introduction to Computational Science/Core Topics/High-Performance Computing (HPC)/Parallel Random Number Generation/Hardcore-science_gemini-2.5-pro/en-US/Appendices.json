{
    "hands_on_practices": [
        {
            "introduction": "This first exercise is a practical demonstration of common pitfalls in parallel programming. By simulating incorrect approaches—like using the same seed for every thread or sharing a single generator without care—you will see firsthand how these errors lead to correlated and invalid random number streams. This practice  builds a crucial intuition for why robust parallel PRNG strategies are not just important, but essential.",
            "id": "3178991",
            "problem": "You are asked to write a complete, runnable program that demonstrates, quantitatively and deterministically, how accidental sharing of pseudo-random number generator state in multithreaded code can create dependent streams, and how to isolate streams per thread to prevent this. The context is pseudo-random number generation in introduction to computational science, and the task must be solved using basic principles about deterministic state machines and empirical statistical comparison. A pseudo-random number generator is a deterministic finite-state machine with state update function and output function. If two threads consume from the same state machine, their outputs belong to one underlying stream and are not independent. If two threads each create their own state machine from the same seed, they will produce identical streams. Proper isolation requires deriving per-thread initial states from a master seed without overlap.\n\nFundamental base to use:\n- A pseudo-random number generator (PRNG) is a deterministic finite-state machine: there exists a state update function $F$ and an output function $G$ such that, given an initial state $s_0$, the state evolves by $s_{t+1} = F(s_t)$ and produces outputs $x_t = G(s_t)$. Determinism implies that two consumers of the same state machine produce parts of one unique sequence $\\{x_t\\}$, hence their outputs are dependent through the shared state. Independence of streams requires distinct, non-overlapping initial states.\n- Empirical comparison of independence can be performed with the Pearson correlation coefficient $r$ between two sequences of real numbers, computed from the standard definition on finite samples. Independence of two independent, identically distributed $\\text{Uniform}(0,1)$ sequences implies that $r$ concentrates near $0$ for sufficiently large sample size $n$, while perfect identity of sequences yields $r = 1$.\n- Deterministic reproducibility in such experiments is obtained by fixing the seed and carefully controlling the schedule by which the shared state is consumed.\n\nYour program must implement three multithreaded experiments that each create two streams of pseudo-random numbers with $n$ draws per stream, using the Permuted Congruential Generator (PCG) engine (`PCG64`) available in standard numerical libraries. All numerical quantities and variables must be interpreted as pure numbers, with no physical units.\n\nExperiments to implement:\n1) Accidental “same-seed per thread” bug. Create $2$ worker threads. Each thread creates its own local PRNG instance but both threads use the same fixed seed $s$. Each thread produces $n$ real numbers in $[0,1)$. Compute the empirical Pearson correlation coefficient $r_1$ between the two length-$n$ sequences. Because both threads are producing the same deterministic sequence, this $r_1$ should equal $1$ when computed exactly on identical sequences.\n2) Shared global generator with enforced alternation. Create $2$ worker threads that share a single global PRNG instance seeded with $s$. Enforce a strict alternating schedule so that the threads draw in the exact order $A_0, B_0, A_1, B_1, \\dots, A_{n-1}, B_{n-1}$, where $A_i$ are the draws assigned to thread $A$ and $B_i$ to thread $B$. Separately, from the same seed $s$, generate a single-threaded reference sequence $Z$ of length $2n$. Construct the merged sequence $M$ by interleaving the two threaded outputs in the same alternation order. Output a boolean $b_2$ indicating whether $M$ is elementwise identical to $Z$. A value $b_2 = \\text{True}$ demonstrates that the two threaded streams are deterministically dependent subsequences of a single stream.\n3) Proper per-thread stream isolation via a master seed. Use a single master seed $s$ to initialize a master seed-sequence object, then derive two independent child seed-sequences for two per-thread PRNG instances (`PCG64`) so that the two streams are non-overlapping by construction. Run $2$ threads, each generating $n$ real numbers in $[0,1)$ from its own PRNG. Compute the empirical Pearson correlation coefficient $r_3$ between the two sequences. For sufficiently large $n$, this $r_3$ should be close to $0$ in magnitude.\n\nTest suite and parameters:\n- Use the following three test cases, with each case defined by a tuple `(scenario, n, s)`:\n  - `(\"same_seed_per_thread\", 4096, 123456789)`\n  - `(\"shared_generator_interleaved\", 4096, 123456789)`\n  - `(\"spawn_isolated_per_thread\", 4096, 123456789)`\n- For the correlation coefficients $r_1$ and $r_3$, report them as floating-point numbers rounded to $6$ decimal places.\n- For the equality check in experiment $2$, report the boolean $b_2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be ordered to correspond to the test suite above, i.e., $[r_1, b_2, r_3]$, where $r_1$ and $r_3$ are floats rounded to exactly $6$ decimals, and $b_2$ is a boolean literal. For example, a syntactically valid output would look like $[1.000000,True,0.001234]$.",
            "solution": "The problem requires a deterministic, quantitative demonstration of the effects of pseudo-random number generator (PRNG) state management in multithreaded programs. The core principle is that a PRNG is a deterministic finite-state machine. Its sequence of outputs is entirely determined by its initial state, or seed. Correct parallel PRNG usage demands that each thread of execution operates on an independent stream of random numbers, which requires that each thread's PRNG be initialized with a state that guarantees its sequence will not overlap with others.\n\nA PRNG can be modeled by a state space $S$, an initial state $s_0 \\in S$, a state transition function $F: S \\to S$, and an output function $G: S \\to \\mathbb{R}$. The sequence of states is given by $s_{t+1} = F(s_t)$, and the sequence of pseudo-random numbers is $x_t = G(s_t)$.\n\nTo empirically assess the independence of two random number streams, we use the Pearson correlation coefficient, $r$. For two sample sequences $X = \\{x_i\\}_{i=1}^n$ and $Y = \\{y_i\\}_{i=1}^n$ with sample means $\\bar{x}$ and $\\bar{y}$ respectively, the correlation is:\n$$\nr = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}}\n$$\nFor perfectly correlated sequences (i.e., identical up to a linear transformation), $|r|=1$. For statistically independent sequences, $r$ is expected to be close to $0$ for a large sample size $n$.\n\nWe will implement three experiments using the `PCG64` generator provided by the `numpy` library, with a sample size of $n=4096$ per thread and a seed of $s=123456789$.\n\n1.  **Experiment 1: Same Seed per Thread.**\n    This experiment simulates a common error where two threads, $T_A$ and $T_B$, each create a new PRNG instance but use the identical seed $s$. Because the PRNG is deterministic, both threads initialize an identical state machine: $(F, G, s_0=s)$. Consequently, both threads will generate the exact same sequence of $n$ numbers. The sequence from $T_A$, let us call it $X_A$, will be element-wise identical to the sequence $X_B$ from $T_B$. Calculating the Pearson correlation coefficient $r_1$ between $X_A$ and $X_B$ will therefore yield a value of exactly $r_1=1$, demonstrating a complete lack of independence.\n\n2.  **Experiment 2: Shared Global Generator with Enforced Alternation.**\n    This experiment demonstrates the consequence of two threads, $T_A$ and $T_B$, sharing a single PRNG instance. The state of the PRNG, $s_t$, becomes a shared resource. To make the outcome deterministic and verifiable, we enforce a strict alternating schedule for draws using synchronization primitives (semaphores). $T_A$ draws, then $T_B$, then $T_A$, and so on. The sequence of draws across both threads is $\\{A_0, B_0, A_1, B_1, \\dots, A_{n-1}, B_{n-1}\\}$. This interleaved process is equivalent to a single thread drawing $2n$ numbers. To prove this, we generate a reference sequence $Z$ of length $2n$ using a single thread initialized with the same seed $s$. We then construct a merged sequence $M$ by interleaving the outputs from $T_A$ and $T_B$. We expect the boolean comparison $b_2 = (M \\equiv Z)$ to be True, which confirms that the threads did not produce independent streams but rather disjoint subsequences of a single, underlying stream.\n\n3.  **Experiment 3: Proper Stream Isolation via Spawning.**\n    This experiment demonstrates the correct methodology for creating independent random number streams in parallel applications. Instead of using the same seed directly, we use a master seed $s$ to initialize a `SeedSequence` object. This object acts as a factory for generating cryptographically strong, independent seed data for multiple PRNGs. We use its `spawn(2)` method to create two child `SeedSequence` objects, $ss_A$ and $ss_B$. These children are guaranteed to initialize PRNGs that produce non-overlapping, statistically independent sequences. Thread $T_A$ uses a PRNG initialized from $ss_A$, and thread $T_B$ uses one initialized from $ss_B$. The resulting sequences, $X_A$ and $X_B$, will be statistically independent. The computed Pearson correlation coefficient $r_3$ between them will be close to zero, reflecting this independence. The magnitude of $r_3$ is expected to be on the order of $1/\\sqrt{n}$. For $n=4096$, we expect $|r_3| \\approx 1/\\sqrt{4096} \\approx 0.0156$.\n\nThe program below implements these three experiments, calculates the required metrics ($r_1$, $b_2$, $r_3$), and formats the output as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport threading\n\ndef solve():\n    \"\"\"\n    Runs three experiments to demonstrate PRNG state management in multithreading.\n    1. Same seed per thread -> correlated streams.\n    2. Shared generator -> dependent streams.\n    3. Spawning child seeds -> independent streams.\n    \"\"\"\n    \n    # Define the test parameters from the problem statement.\n    # n and s are the same for all three experiments.\n    n = 4096\n    s = 123456789\n\n    # --- Experiment 1: \"same_seed_per_thread\" ---\n    # Each thread creates its own PRNG from the same seed.\n    def worker_exp1(seed, size, out_array):\n        \"\"\"Worker for experiment 1: creates a local PRNG.\"\"\"\n        rng = np.random.Generator(np.random.PCG64(seed))\n        out_array[:] = rng.random(size)\n\n    stream1_exp1 = np.empty(n, dtype=np.float64)\n    stream2_exp1 = np.empty(n, dtype=np.float64)\n    \n    thread1 = threading.Thread(target=worker_exp1, args=(s, n, stream1_exp1))\n    thread2 = threading.Thread(target=worker_exp1, args=(s, n, stream2_exp1))\n    \n    thread1.start()\n    thread2.start()\n    thread1.join()\n    thread2.join()\n\n    # The streams should be identical, so correlation is 1.0.\n    r1 = np.corrcoef(stream1_exp1, stream2_exp1)[0, 1]\n\n    # --- Experiment 2: \"shared_generator_interleaved\" ---\n    # Threads share one PRNG, with access ordered by semaphores.\n    def worker_A_exp2(rng, size, sem_a, sem_b, out_array):\n        \"\"\"Worker A: acquires sem_a, releases sem_b.\"\"\"\n        for i in range(size):\n            sem_a.acquire()\n            out_array[i] = rng.random()\n            sem_b.release()\n\n    def worker_B_exp2(rng, size, sem_a, sem_b, out_array):\n        \"\"\"Worker B: acquires sem_b, releases sem_a.\"\"\"\n        for i in range(size):\n            sem_b.acquire()\n            out_array[i] = rng.random()\n            sem_a.release()\n\n    shared_rng = np.random.Generator(np.random.PCG64(s))\n    stream_a_exp2 = np.empty(n, dtype=np.float64)\n    stream_b_exp2 = np.empty(n, dtype=np.float64)\n\n    # Semaphores to enforce strict A, B, A, B... alternation.\n    sem_a = threading.Semaphore(1)\n    sem_b = threading.Semaphore(0)\n\n    thread_a = threading.Thread(target=worker_A_exp2, args=(shared_rng, n, sem_a, sem_b, stream_a_exp2))\n    thread_b = threading.Thread(target=worker_B_exp2, args=(shared_rng, n, sem_a, sem_b, stream_b_exp2))\n    \n    thread_a.start()\n    thread_b.start()\n    thread_a.join()\n    thread_b.join()\n\n    # Interleave the results from the two threads.\n    merged_sequence = np.empty(2 * n, dtype=np.float64)\n    merged_sequence[0::2] = stream_a_exp2\n    merged_sequence[1::2] = stream_b_exp2\n    \n    # Generate the single-threaded reference sequence.\n    ref_rng = np.random.Generator(np.random.PCG64(s))\n    reference_sequence = ref_rng.random(2 * n)\n\n    # The interleaved threaded sequence should be identical to the reference.\n    b2 = np.array_equal(merged_sequence, reference_sequence)\n\n    # --- Experiment 3: \"spawn_isolated_per_thread\" ---\n    # Each thread gets an independent PRNG from a spawned SeedSequence.\n    def worker_exp3(rng, size, out_array):\n        \"\"\"Worker for experiment 3: uses a pre-made PRNG.\"\"\"\n        out_array[:] = rng.random(size)\n\n    # Create a master SeedSequence and spawn two independent children.\n    ss = np.random.SeedSequence(s)\n    child_sequences = ss.spawn(2)\n    \n    rng1_exp3 = np.random.Generator(np.random.PCG64(child_sequences[0]))\n    rng2_exp3 = np.random.Generator(np.random.PCG64(child_sequences[1]))\n\n    stream1_exp3 = np.empty(n, dtype=np.float64)\n    stream2_exp3 = np.empty(n, dtype=np.float64)\n\n    thread1_exp3 = threading.Thread(target=worker_exp3, args=(rng1_exp3, n, stream1_exp3))\n    thread2_exp3 = threading.Thread(target=worker_exp3, args=(rng2_exp3, n, stream2_exp3))\n    \n    thread1_exp3.start()\n    thread2_exp3.start()\n    thread1_exp3.join()\n    thread2_exp3.join()\n    \n    # The streams should be independent, so correlation is near 0.\n    r3 = np.corrcoef(stream1_exp3, stream2_exp3)[0, 1]\n\n    # Final print statement in the exact required format.\n    print(f\"[{r1:.6f},{b2},{r3:.6f}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having seen what can go wrong, we now turn to a robust and widely used solution: assigning each parallel worker its own independent random number stream. This practice involves a \"balls-in-bins\" simulation, where you will use modern PRNG libraries to \"spawn\" distinct random sequences from a master seed. This approach  is fundamental for \"embarrassingly parallel\" statistical simulations where workers do not need to coordinate their random draws.",
            "id": "3170083",
            "problem": "You are asked to design and implement a complete, runnable program that performs parallel Random Balls-in-Bins experiments to empirically estimate the distribution of the maximum bin load, and to verify that it matches a theoretically justified approximation when each worker thread uses an independent random number generator.\n\nStart from the following fundamental base. In each experiment, $m$ balls are thrown independently and uniformly at random into $n$ bins, meaning each ball is placed into any one of the $n$ bins with probability $1/n$, independently of all other balls. Let $X_i$ be the number of balls in bin $i$, so the occupancy vector $(X_1,\\dots,X_n)$ follows a multinomial distribution with parameters $m$ and uniform probabilities $(1/n,\\dots,1/n)$. Define the maximum load random variable $L_{\\max} = \\max\\{X_1,\\dots,X_n\\}$. For large $n$ with $m$ of moderate order, a well-tested approximation known as Poissonization states that the joint distribution of occupancies can be accurately approximated as $n$ independent Poisson random variables each with mean $\\lambda = m/n$. Under this approximation, the cumulative distribution function of $L_{\\max}$ satisfies\n$$\n\\mathbb{P}(L_{\\max} \\le k) \\approx \\left(\\mathbb{P}\\left(\\mathrm{Pois}(\\lambda) \\le k\\right)\\right)^n,\n$$\nwhere $\\mathrm{Pois}(\\lambda)$ denotes a Poisson random variable with rate $\\lambda$. This approximation will be the theoretical reference for verification.\n\nDesign your program to implement $T$ parallel workers, each with an independent Random Number Generator (RNG). Independence is achieved by deterministically splitting a master seed into $T$ child seeds and instantiating one RNG per worker from its own child seed. Each worker should run a subset of the total experiments and report the histogram of observed values of $L_{\\max}$ for its assigned experiments. The master process aggregates histograms, forms the empirical cumulative distribution function (CDF), and compares it to the theoretical CDF derived from the Poisson approximation. Use the Kolmogorov–Smirnov (KS) distance,\n$$\nD_{\\mathrm{KS}} = \\sup_{k \\in \\{0,1,\\dots,m\\}} \\left| \\widehat{F}(k) - \\left(F_{\\mathrm{Pois}}(k;\\lambda)\\right)^n \\right|,\n$$\nwhere $\\widehat{F}(k)$ is the empirical CDF at $k$ and $F_{\\mathrm{Pois}}(k;\\lambda)$ is the Poisson CDF at $k$ with rate $\\lambda$, computed as\n$$\nF_{\\mathrm{Pois}}(k;\\lambda) = e^{-\\lambda} \\sum_{j=0}^{k} \\frac{\\lambda^j}{j!}.\n$$\n\nImplement the program so that it performs the following test suite and produces a single line of output containing a comma-separated list enclosed in square brackets. For each test case, compute the KS distance and return a boolean indicating whether the empirical distribution is within a specified tolerance of the theoretical approximation. The parallelism must be realized by $T$ workers with independent per-thread RNG streams.\n\nTest suite (four cases):\n- Case $1$ (happy path, moderate load): $n = 64$, $m = 512$, $T = 4$, number of experiments $E = 3000$, tolerance $\\tau = 0.10$.\n- Case $2$ (moderate regime): $n = 200$, $m = 400$, $T = 6$, number of experiments $E = 2000$, tolerance $\\tau = 0.10$.\n- Case $3$ (boundary near unit rate): $n = 100$, $m = 100$, $T = 8$, number of experiments $E = 3000$, tolerance $\\tau = 0.10$.\n- Case $4$ (sparse regime): $n = 500$, $m = 100$, $T = 5$, number of experiments $E = 3000$, tolerance $\\tau = 0.12$.\n\nYour program must:\n- Use a master seed to spawn $T$ independent per-thread RNGs.\n- For each case, run $E$ experiments in total, distributing them across the $T$ workers as evenly as possible.\n- In each experiment, simulate throwing $m$ balls into $n$ bins, compute $L_{\\max}$, and update a histogram of $L_{\\max}$ values over $\\{0,1,\\dots,m\\}$.\n- Aggregate the histograms from all workers to obtain the empirical CDF $\\widehat{F}$ and compute $D_{\\mathrm{KS}}$ by taking the supremum over $k \\in \\{0,1,\\dots,m\\}$.\n- Compare $D_{\\mathrm{KS}}$ to the given tolerance $\\tau$ and produce a boolean per case equal to $\\text{True}$ if $D_{\\mathrm{KS}} \\le \\tau$ and $\\text{False}$ otherwise.\n\nFinal output format specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the four cases in order. For example, the output must look like\n$[b_1,b_2,b_3,b_4]$\nwhere each $b_i$ is a boolean.\n\nNo physical units or angle units are involved in this problem. All random choices must be uniform over the $n$ bins, and all requested quantities must be computed exactly as specified without any shortcuts that bypass the balls-in-bins simulation. The program must not require any user input and must be fully self-contained.",
            "solution": "The problem is valid. It presents a well-defined task in computational science that is scientifically grounded, objective, and self-contained. The objective is to empirically validate the Poissonization approximation for the maximum load in a random balls-in-bins experiment using a parallel simulation.\n\nThe solution is designed around a master-worker parallel computing model. The core of the problem lies in correctly simulating the random process, managing parallel execution with statistically independent random number streams, and performing a rigorous comparison against a theoretical model.\n\n**1. Parallelism and Independent Random Number Generation**\n\nA fundamental principle in parallel stochastic simulations is that each parallel worker must use an independent stream of random numbers. If workers use correlated or identical streams, it introduces statistical biases that invalidate the results. To achieve this, we employ a master seed which is used to deterministically generate a set of independent child seeds, one for each of the $T$ workers.\n\nSpecifically, a `numpy.random.SeedSequence` object is initialized from a single master integer seed. This `SeedSequence` object's `.spawn(T)` method is then called to produce $T$ new, independent `SeedSequence` objects. Each of these is passed to one worker, which then uses it to initialize its own private `numpy.random.Generator` instance. This ensures that the random numbers generated by each worker are statistically independent of all others, fulfilling a critical requirement for a valid parallel simulation.\n\n**2. Simulation within a Worker Process**\n\nEach of the $T$ workers is assigned a fraction of the total $E$ experiments to perform. For each experiment, a worker simulates the process of throwing $m$ balls into $n$ bins. This is implemented efficiently by generating an array of $m$ random integers, where each integer is drawn uniformly from the set $\\{0, 1, \\dots, n-1\\}$. Each integer represents the bin into which a ball is thrown.\n\nThe number of balls in each bin, known as the bin occupancies $(X_1, \\dots, X_n)$, is then calculated. The `numpy.bincount` function is perfectly suited for this, as it counts the number of occurrences of each integer value in the generated array, directly yielding the bin counts. The maximum load for this single experiment, $L_{\\max} = \\max\\{X_1, \\dots, X_n\\}$, is then found by taking the maximum value of the resulting bin count array.\n\nEach worker maintains a local histogram, which is an array of size $m+1$ initialized to zeros. For each experiment it completes, the worker increments the histogram counter corresponding to the observed value of $L_{\\max}$. After running all its assigned experiments, the worker returns its local histogram to the master process.\n\n**3. Aggregation and Empirical Distribution**\n\nThe master process orchestrates the simulation. It first divides the total number of experiments $E$ as evenly as possible among the $T$ workers. It then launches the $T$ worker processes, providing each with its unique parameters (including its dedicated seed sequence).\n\nUpon completion, the master process collects the local histograms from all workers. It aggregates these into a single global histogram by performing an element-wise sum. This global histogram contains the total counts for each observed value of $L_{\\max}$ across all $E$ experiments.\n\nFrom this global histogram, the empirical probability mass function (PMF), $\\widehat{p}(k) = \\mathbb{P}(L_{\\max} = k)$, is estimated by dividing the count for each value $k$ by the total number of experiments $E$. The empirical cumulative distribution function (CDF), $\\widehat{F}(k) = \\mathbb{P}(L_{\\max} \\le k)$, is then computed by taking the cumulative sum of the empirical PMF: $\\widehat{F}(k) = \\sum_{j=0}^{k} \\widehat{p}(j)$.\n\n**4. Theoretical Distribution and Comparison**\n\nThe theoretical model for comparison is based on the Poissonization approximation. This approximation posits that for large $n$ and moderate load $\\lambda = m/n$, the distribution of the maximum load $L_{\\max}$ can be approximated by the maximum of $n$ independent and identically distributed Poisson random variables, each with mean $\\lambda$.\n\nThe CDF of a single Poisson variable with mean $\\lambda$ is given by $F_{\\mathrm{Pois}}(k;\\lambda) = \\mathbb{P}(\\mathrm{Pois}(\\lambda) \\le k) = e^{-\\lambda} \\sum_{j=0}^{k} \\frac{\\lambda^j}{j!}$. The `scipy.stats.poisson.cdf` function provides an accurate and efficient way to compute this.\n\nUnder the independence assumption of the approximation, the theoretical CDF of the maximum load is the CDF of a single variable raised to the power of $n$:\n$$\n\\mathbb{P}(L_{\\max} \\le k) \\approx \\left(F_{\\mathrm{Pois}}(k;\\lambda)\\right)^n\n$$\nThis theoretical CDF is computed for all relevant values of $k \\in \\{0, 1, \\dots, m\\}$.\n\n**5. Kolmogorov–Smirnov (KS) Distance**\n\nTo quantify the agreement between the empirical and theoretical distributions, the Kolmogorov–Smirnov (KS) distance is used. It is defined as the maximum absolute difference between the two CDFs over the entire range of possible outcomes:\n$$\nD_{\\mathrm{KS}} = \\sup_{k \\in \\{0,1,\\dots,m\\}} \\left| \\widehat{F}(k) - \\left(F_{\\mathrm{Pois}}(k;\\lambda)\\right)^n \\right|\n$$\nThis value represents the single greatest discrepancy between the observed data distribution and the theoretical prediction.\n\nFor each test case, the computed $D_{\\mathrm{KS}}$ value is compared against a specified tolerance $\\tau$. The result for the case is a boolean value, which is $\\text{True}$ if $D_{\\mathrm{KS}} \\le \\tau$ and $\\text{False}$ otherwise. This process is repeated for all four test cases provided in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import poisson\nimport multiprocessing\n\n# This function must be at the top level for multiprocessing to pickle it.\ndef worker_function(args):\n    \"\"\"\n    Performs a subset of balls-in-bins experiments for one worker.\n    \"\"\"\n    worker_id, num_experiments, n, m, seed_seq = args\n    \n    # Each worker gets its own independent Random Number Generator.\n    rng = np.random.default_rng(seed_seq)\n    \n    # Histogram to store counts of max_load values. Max load can be at most m.\n    max_load_hist = np.zeros(m + 1, dtype=np.int64)\n    \n    for _ in range(num_experiments):\n        # Simulate throwing m balls into n bins.\n        # This is equivalent to drawing m random integers from [0, n-1].\n        balls_in_bins = rng.integers(0, n, size=m)\n        \n        # Count the number of balls in each bin.\n        # minlength=n ensures bins with 0 balls are included.\n        bin_counts = np.bincount(balls_in_bins, minlength=n)\n        \n        # Find the maximum load for this experiment.\n        max_load = 0\n        if bin_counts.size > 0:\n            max_load = np.max(bin_counts)\n        \n        # Update the histogram. max_load is guaranteed to be <= m.\n        max_load_hist[max_load] += 1\n        \n    return max_load_hist\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    # test_cases: (n, m, T, E, tau)\n    test_cases = [\n        (64, 512, 4, 3000, 0.10),\n        (200, 400, 6, 2000, 0.10),\n        (100, 100, 8, 3000, 0.10),\n        (500, 100, 5, 3000, 0.12),\n    ]\n\n    results = []\n    # Use a fixed master seed for reproducibility of the entire process.\n    master_seed = 42\n\n    for n, m, T, E, tau in test_cases:\n        # 1. Setup for Parallel Execution\n        # Create a master SeedSequence to spawn independent seeds for workers.\n        ss = np.random.SeedSequence(master_seed)\n        child_s_seqs = ss.spawn(T)\n\n        # Distribute experiments as evenly as possible among workers.\n        experiments_per_worker = [E // T] * T\n        for i in range(E % T):\n            experiments_per_worker[i] += 1\n        \n        worker_args = [\n            (i, experiments_per_worker[i], n, m, child_s_seqs[i]) for i in range(T)\n        ]\n\n        # 2. Run Simulations in Parallel\n        # Using a context manager ensures the pool is properly shut down.\n        with multiprocessing.Pool(processes=T) as pool:\n            # pool.map is used as we have an iterable of single-argument tuples.\n            hist_results = pool.map(worker_function, worker_args)\n\n        # 3. Aggregate Results\n        # Sum the histograms from all workers.\n        total_hist = np.sum(hist_results, axis=0)\n\n        # 4. Compute Empirical CDF\n        # Normalize histogram to get the empirical probability mass function (PMF).\n        empirical_pmf = total_hist / E\n        # Compute the cumulative distribution function (CDF).\n        empirical_cdf = np.cumsum(empirical_pmf)\n\n        # 5. Compute Theoretical CDF\n        lambda_ = m / n\n        k_values = np.arange(m + 1)\n        \n        # CDF of a single Poisson(lambda) variable.\n        poisson_cdf_single = poisson.cdf(k_values, mu=lambda_)\n        \n        # CDF of the max of n independent Poisson(lambda) variables.\n        theoretical_cdf = poisson_cdf_single ** n\n        \n        # 6. Compute KS Distance and Final Boolean Result\n        # Calculate the maximum absolute difference between the CDFs.\n        ks_distance = np.max(np.abs(empirical_cdf - theoretical_cdf))\n        \n        # Compare with tolerance.\n        is_within_tolerance = ks_distance <= tau\n        results.append(is_within_tolerance)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Guard for multiprocessing on platforms that use 'spawn' or 'forkserver'.\n# The solve function is placed inside to ensure it runs only in the main process.\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "We explore a powerful alternative paradigm known as counter-based random number generation, which offers ultimate flexibility and guaranteed reproducibility. In this exercise, you will implement a \"stateless\" generator where the output is a pure function of a unique counter, completely decoupling it from the execution order. This method  is invaluable in modern parallel architectures, especially GPUs, where loop execution can be complex and non-sequential.",
            "id": "3170077",
            "problem": "You are given a nested parallel loop structure over indices $i$ and $j$ with $i \\in \\{0, 1, \\dots, N-1\\}$ and $j \\in \\{0, 1, \\dots, K-1\\}$. The task is to construct a deterministic stream labeling scheme for pseudorandom number generation that is invariant under changes to the loop execution schedule, such as loop tiling and loop fusion. The construction must use a counter-based mapping from the pair $(i, j)$ to a single integer counter $c$ defined by $c = i \\cdot K + j$. The pseudorandom number generator must be stateless and must derive the output exclusively from the seed $s$ and the counter $c$.\n\nFundamental base:\n- A Pseudorandom Number Generator (PRNG) is a deterministic algorithm that maps a seed $s$ and a state to an output. For counter-based PRNGs, the state is a counter $c$ and the output is $F(s, c)$ for a fixed function $F$.\n- Deterministic functions $F(s, c)$ yield the same output for the same inputs regardless of execution schedule or parallel ordering.\n- The mapping $c = i \\cdot K + j$ is a bijection between pairs $(i, j)$ with $i \\in \\{0, 1, \\dots, N-1\\}$ and $j \\in \\{0, 1, \\dots, K-1\\}$ and integers $c \\in \\{0, 1, \\dots, N \\cdot K - 1\\}$, with inverse $i = \\left\\lfloor \\frac{c}{K} \\right\\rfloor$ and $j = c \\bmod K$.\n\nYour program must:\n1. Implement a stateless, counter-based PRNG defined as $F(s, c)$ that produces a $64$-bit unsigned integer from $s$ and $c$. The function must depend only on $s$ and $c$, and the program must demonstrate reproducibility under different loop traversal orders.\n2. Use the mapping $c = i \\cdot K + j$ to label streams for all $(i, j)$ pairs. The PRNG output for a given $(i, j)$ must be identical across different schedules because the value depends solely on $c$ and $s$.\n3. Verify reproducibility under:\n   - Row-major nested loops.\n   - Loop tiling with tile sizes $T_i$ and $T_j$.\n   - Loop fusion into a single loop over $c$.\n   - Arbitrary reordering by permuting the sequence of counters $c$.\n4. Verify the effect of changing the seed $s$ by showing that outputs change when $s$ changes, while reproducibility holds when $s$ is fixed.\n\nTest suite:\n- The program must run the following five test cases and return a boolean for each case indicating whether the required property holds.\n  - Test $1$ (Row-major vs tiled):\n    - Parameters: $N = 4$, $K = 5$, $T_i = 2$, $T_j = 3$, $s = 0x0123456789ABCDEF$.\n    - Property: For all $(i, j)$, the outputs from row-major traversal and from tiled traversal are identical.\n  - Test $2$ (Nested vs fused):\n    - Parameters: $N = 5$, $K = 7$, $s = 0x0123456789ABCDEF$.\n    - Property: For all $(i, j)$, outputs from nested traversal and fused traversal over $c \\in \\{0, 1, \\dots, N \\cdot K - 1\\}$ are identical.\n  - Test $3$ (Boundary case $K = 1$):\n    - Parameters: $N = 8$, $K = 1$, $s = 0x0123456789ABCDEF$.\n    - Property: For all $i$, outputs from nested traversal and fused traversal over $c \\in \\{0, 1, \\dots, N \\cdot K - 1\\}$ are identical, where $c = i$ when $K = 1$.\n  - Test $4$ (Seed change vs reproducibility):\n    - Parameters: $N = 3$, $K = 4$, $s_1 = 0x0123456789ABCDEF$, $s_2 = 0xF0E1D2C3B4A59687$.\n    - Properties: \n      - With fixed $s_1$, outputs under row-major and tiled traversals are identical for all $(i, j)$.\n      - With $s_1 \\ne s_2$, there exists at least one $(i, j)$ for which $F(s_1, c) \\ne F(s_2, c)$.\n    - The test returns `True` only if both properties are satisfied.\n  - Test $5$ (Arbitrary permutation):\n    - Parameters: $N = 3$, $K = 7$, $s = 0x0123456789ABCDEF$.\n    - Property: For all $(i, j)$, outputs under an arbitrary permutation of counters $c \\in \\{0, 1, \\dots, N \\cdot K - 1\\}$ match the outputs under row-major traversal.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[result_1,result_2,result_3,result_4,result_5]$, where each $result_k$ is either $True$ or $False$. No additional text should be printed.\n- The program must be self-contained, require no input, and must be runnable as-is in a modern environment.\n\nThere are no physical units or angles involved in this problem, so no unit conversion is required.",
            "solution": "The supplied problem is deemed valid. It is a well-posed, scientifically grounded problem in computational science that requires the design and verification of a parallel-safe pseudorandom number generation scheme.\n\nThe solution is founded on two core principles: the creation of a stateless, deterministic pseudorandom number generator (PRNG) and the application of a bijective mapping from the multi-dimensional loop space to a one-dimensional counter space.\n\n**1. Principle of Stateless Determinism**\n\nA PRNG is typically a stateful function, where each call updates an internal state to produce the next number in a sequence. This approach is problematic in parallel settings, as concurrent updates to a shared state require synchronization, which introduces overhead and non-determinism in the output sequence depending on thread scheduling.\n\nThe solution is to use a stateless PRNG, which is a pure function $F(s, c)$ whose output depends exclusively on its inputs: a global seed $s$ and a counter $c$. It maintains no internal state between calls. Consequently, for a given seed $s$ and counter value $c$, the function $F(s, c)$ will always produce the exact same output, regardless of when or on which processor it is executed.\n\nFor this problem, we implement such a function $F(s, c)$ that generates a $64$-bit unsigned integer. A robust choice is a function based on the `splitmix64` algorithm. This function combines the seed $s$ and counter $c$ into an initial value and then applies a series of bitwise shifts, XOR operations, and multiplications with large, pre-determined prime constants. These operations effectively mix the bits of the input to produce an output that exhibits good statistical randomness. All arithmetic is performed using $64$-bit unsigned integer operations, ensuring wrap-around on overflow, which is critical for the algorithm's properties.\n\nLet $s$ and $c$ be $64$-bit unsigned integers. The function $F(s, c)$ is defined as follows, where all operations are performed modulo $2^{64}$:\n1.  Initialize state: $x = s + c$.\n2.  First mixing round: $x = (x \\oplus (x \\gg 30)) \\cdot 0xBF58476D1CE4E5B9$.\n3.  Second mixing round: $x = (x \\oplus (x \\gg 27)) \\cdot 0x94D049BB133111EB$.\n4.  Final mixing: $x = x \\oplus (x \\gg 31)$.\nThe result is the final value of $x$.\n\n**2. Principle of Bijective Mapping**\n\nTo make the stateless PRNG useful for a multi-dimensional problem space, such as a nested loop over indices $(i, j)$, we must uniquely label each point in that space. The problem specifies a bijective mapping from the pair of loop indices $(i, j)$ to a single integer counter $c$. The given mapping is a standard row-major linearization:\n$$c = i \\cdot K + j$$\nwhere $i \\in \\{0, 1, \\dots, N-1\\}$ and $j \\in \\{0, 1, \\dots, K-1\\}$. This function guarantees that every unique index pair $(i, j)$ maps to a unique counter $c \\in \\{0, 1, \\dots, N \\cdot K - 1\\}$. This uniqueness is the key to decoupling the random number generation from the loop execution order.\n\n**3. Invariance Under Schedule Changes**\n\nBy composing the bijective mapping and the stateless PRNG, we obtain a function $G(s, i, j) = F(s, i \\cdot K + j)$ that generates a pseudorandom number for each point $(i, j)$ in the loop's iteration space.\n\nDifferent loop execution schedules, such as:\n-   **Row-major traversal**: Iterating through $j$ for each $i$.\n-   **Loop tiling**: Breaking the iteration space into smaller rectangular tiles and iterating through those.\n-   **Loop fusion**: Collapsing the nested loop into a single loop over the counter $c$.\n-   **Arbitrary permutation**: Visiting the points $(i, j)$ in a shuffled, non-sequential order.\n\nare simply different orderings for evaluating the function $G(s, i, j)$ for all pairs $(i, j)$. Since the value of $G(s, i, j)$ for any specific pair is independent of the evaluation order of any other pair, the final set of generated numbers, when organized into an $N \\times K$ grid, will be identical across all schedules.\n\nThe provided test suite verifies this invariance.\n-   **Test 1 & 4**: Compare row-major traversal to tiled traversal. The generated values for each $(i, j)$ are based on the same $c = i \\cdot K + j$, so the resulting collections of numbers must be identical.\n-   **Test 2 & 3**: Compare nested row-major traversal to a fused loop over $c$. The fused loop directly uses $c$, while the nested loop first computes $c = i \\cdot K + j$. The inverse mapping $i = \\lfloor c/K \\rfloor, j = c \\pmod K$ ensures that the value computed for a given $c$ is placed in the correct $(i, j)$ location. The results must match.\n-   **Test 4**: Verifies that changing the seed $s$ alters the output, confirming the PRNG's sensitivity to the seed, which is a fundamental requirement.\n-   **Test 5**: Compares row-major traversal to an arbitrarily permuted order of execution. This is the strongest demonstration of schedule invariance, proving that as long as every counter value $c$ from $0$ to $N \\cdot K - 1$ is processed exactly once, the final result is deterministic.\n\nThe implementation will confirm that all these properties hold, resulting in a `True` value for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef F(s, c):\n    \"\"\"\n    A stateless, counter-based PRNG that produces a 64-bit unsigned integer.\n    The function is based on the splitmix64 algorithm.\n    It is a pure function of the seed 's' and the counter 'c'.\n    \"\"\"\n    # Ensure inputs and all intermediate calculations use 64-bit unsigned integers.\n    s_64 = np.uint64(s)\n    c_64 = np.uint64(c)\n\n    # Combine seed and counter to initialize the state.\n    state = s_64 + c_64\n\n    # The splitmix64 mixing function.\n    state = (state ^ (state >> np.uint64(30))) * np.uint64(0xbf58476d1ce4e5b9)\n    state = (state ^ (state >> np.uint64(27))) * np.uint64(0x94d049bb133111eb)\n    state = state ^ (state >> np.uint64(31))\n    \n    return state\n\ndef generate_row_major(N, K, s):\n    \"\"\"Generates PRNs in a row-major nested loop traversal.\"\"\"\n    results = np.zeros((N, K), dtype=np.uint64)\n    for i in range(N):\n        for j in range(K):\n            c = i * K + j\n            results[i, j] = F(s, c)\n    return results\n\ndef generate_tiled(N, K, Ti, Tj, s):\n    \"\"\"Generates PRNs in a tiled loop traversal.\"\"\"\n    results = np.zeros((N, K), dtype=np.uint64)\n    for i_tile in range(0, N, Ti):\n        for j_tile in range(0, K, Tj):\n            for i in range(i_tile, min(i_tile + Ti, N)):\n                for j in range(j_tile, min(j_tile + Tj, K)):\n                    c = i * K + j\n                    results[i, j] = F(s, c)\n    return results\n\ndef generate_fused(N, K, s):\n    \"\"\"Generates PRNs in a single, fused loop over the counter 'c'.\"\"\"\n    results = np.zeros((N, K), dtype=np.uint64)\n    for c in range(N * K):\n        i = c // K\n        j = c % K\n        results[i, j] = F(s, c)\n    return results\n\ndef generate_permuted(N, K, s):\n    \"\"\"Generates PRNs by processing counters in a permuted, arbitrary order.\"\"\"\n    results = np.zeros((N, K), dtype=np.uint64)\n    counters = np.arange(N * K, dtype=np.uint64)\n    \n    # Use a fixed seed for the permutation to ensure the test is deterministic.\n    rng = np.random.default_rng(seed=42)\n    rng.shuffle(counters)\n    \n    for c in counters:\n        c_int = int(c)\n        i = c_int // K\n        j = c_int % K\n        results[i, j] = F(s, c)\n    return results\n\ndef solve():\n    \"\"\"\n    Runs the test suite to verify the properties of the stateless,\n    counter-based PRNG scheme.\n    \"\"\"\n    test_cases = [\n        # Test 1: Row-major vs. Tiled\n        {'N': 4, 'K': 5, 'Ti': 2, 'Tj': 3, 's': 0x0123456789ABCDEF},\n        # Test 2: Nested vs. Fused\n        {'N': 5, 'K': 7, 's': 0x0123456789ABCDEF},\n        # Test 3: Boundary case K=1\n        {'N': 8, 'K': 1, 's': 0x0123456789ABCDEF},\n        # Test 4: Seed change vs. reproducibility\n        {'N': 3, 'K': 4, 's1': 0x0123456789ABCDEF, 's2': 0xF0E1D2C3B4A59687},\n        # Test 5: Arbitrary permutation\n        {'N': 3, 'K': 7, 's': 0x0123456789ABCDEF},\n    ]\n\n    results = []\n\n    # --- Test 1: Row-major vs. Tiled ---\n    p = test_cases[0]\n    res_row_major = generate_row_major(p['N'], p['K'], p['s'])\n    res_tiled = generate_tiled(p['N'], p['K'], p['Ti'], p['Tj'], p['s'])\n    results.append(np.array_equal(res_row_major, res_tiled))\n\n    # --- Test 2: Nested vs. Fused ---\n    p = test_cases[1]\n    res_nested = generate_row_major(p['N'], p['K'], p['s'])\n    res_fused = generate_fused(p['N'], p['K'], p['s'])\n    results.append(np.array_equal(res_nested, res_fused))\n\n    # --- Test 3: Boundary case K=1 ---\n    p = test_cases[2]\n    res_nested = generate_row_major(p['N'], p['K'], p['s'])\n    res_fused = generate_fused(p['N'], p['K'], p['s'])\n    results.append(np.array_equal(res_nested, res_fused))\n    \n    # --- Test 4: Seed change vs. reproducibility ---\n    p = test_cases[3]\n    # Tiling requires tile sizes. We use a reasonable choice for the test.\n    Ti_t4, Tj_t4 = 2, 2\n    # Property 1: Reproducibility with fixed seed s1\n    res_row_major_s1 = generate_row_major(p['N'], p['K'], p['s1'])\n    res_tiled_s1 = generate_tiled(p['N'], p['K'], Ti_t4, Tj_t4, p['s1'])\n    prop1 = np.array_equal(res_row_major_s1, res_tiled_s1)\n    # Property 2: Different outputs for different seeds\n    res_row_major_s2 = generate_row_major(p['N'], p['K'], p['s2'])\n    prop2 = not np.array_equal(res_row_major_s1, res_row_major_s2)\n    results.append(prop1 and prop2)\n\n    # --- Test 5: Arbitrary permutation ---\n    p = test_cases[4]\n    res_row_major = generate_row_major(p['N'], p['K'], p['s'])\n    res_permuted = generate_permuted(p['N'], p['K'], p['s'])\n    results.append(np.array_equal(res_row_major, res_permuted))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}