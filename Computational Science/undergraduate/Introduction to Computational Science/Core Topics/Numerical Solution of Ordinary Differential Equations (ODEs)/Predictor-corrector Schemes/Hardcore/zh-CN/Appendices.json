{
    "hands_on_practices": [
        {
            "introduction": "这项首个练习为你提供了一个关于预测-校正方法的直观几何视角。我们将探讨如何解决一个约束优化问题：首先朝着目标函数的最佳方向迈出一步（预测），然后通过投影将结果拉回到可行域（校正）。这个“预测-再投影”的思路是许多现代约束优化算法的基础。",
            "id": "3163808",
            "problem": "考虑一个等式约束优化问题，要求在光滑流形约束下最小化二次目标函数。设目标函数为 $f(x) = \\frac{1}{2}\\|x - c\\|^{2}$，其中 $x \\in \\mathbb{R}^{2}$ 且 $c = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，约束为 $h(x) = x_{1}^{2} + x_{2}^{2} - 1 = 0$，这定义了单位圆。假设您位于可行点 $x^{(0)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$。\n\n一个预测-校正方案的流程是：首先基于目标函数的梯度进行一个无约束的预测步，然后使用一阶局部近似投影到约束流形上进行校正以恢复可行性。具体来说：\n\n1. 使用无约束梯度步来形成预测值 $x^{\\mathrm{pred}} = x^{(0)} - \\alpha \\nabla f(x^{(0)})$，步长为 $\\alpha = \\frac{1}{4}$。\n2. 使用约束在 $x^{\\mathrm{pred}}$ 处的一阶泰勒展开 $h(x^{\\mathrm{pred}} + \\delta) \\approx h(x^{\\mathrm{pred}}) + J_{h}(x^{\\mathrm{pred}})\\,\\delta$，确定恢复可行性的最小范数校正量 $\\delta$，其中 $J_{h}(x)$ 表示 $h$ 在 $x$ 处的雅可比矩阵。然后构成校正后的迭代点 $x^{\\mathrm{corr}} = x^{\\mathrm{pred}} + \\delta$。\n3. 计算目标函数值 $f(x^{\\mathrm{corr}})$。\n\n将 $f(x^{\\mathrm{corr}})$ 的最终数值答案表示为精确的有理数。不要四舍五入。",
            "solution": "用户提供了一个良态的优化问题，需要应用单步预测-校正法来解决。该问题具有科学依据、自洽且客观。我们将按照指定步骤进行求解。\n\n目标函数为 $f(x) = \\frac{1}{2}\\|x - c\\|^{2}$，其中 $x = \\begin{pmatrix}x_1 \\\\ x_2\\end{pmatrix} \\in \\mathbb{R}^{2}$ 且 $c = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$。约束为 $h(x) = x_{1}^{2} + x_{2}^{2} - 1 = 0$。初始点为 $x^{(0)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$。\n\n**第 1 步：预测步**\n\n首先，我们计算目标函数的梯度 $\\nabla f(x)$。\n目标函数可以写为 $f(x) = \\frac{1}{2}((x_1 - 1)^2 + (x_2 - 0)^2)$。\n梯度由下式给出：\n$$ \\nabla f(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} x_1 - 1 \\\\ x_2 \\end{pmatrix} = x - c $$\n我们计算在初始点 $x^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ 处的梯度：\n$$ \\nabla f(x^{(0)}) = x^{(0)} - c = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} - \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}-1 \\\\ 1\\end{pmatrix} $$\n预测步是一个步长为 $\\alpha = \\frac{1}{4}$ 的无约束梯度步：\n$$ x^{\\mathrm{pred}} = x^{(0)} - \\alpha \\nabla f(x^{(0)}) $$\n代入数值：\n$$ x^{\\mathrm{pred}} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix}-1 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}0 + \\frac{1}{4} \\\\ 1 - \\frac{1}{4}\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{4} \\\\ \\frac{3}{4}\\end{pmatrix} $$\n\n**第 2 步：校正步**\n\n校正步旨在通过从 $x^{\\mathrm{pred}}$ 移动一个最小范数校正向量 $\\delta$ 来恢复可行性，使得约束的一阶近似得到满足。\n线性化的约束方程为 $h(x^{\\mathrm{pred}}) + J_{h}(x^{\\mathrm{pred}})\\,\\delta = 0$。\n\n首先，我们计算在 $x^{\\mathrm{pred}}$ 处的约束函数值：\n$$ h(x^{\\mathrm{pred}}) = \\left(\\frac{1}{4}\\right)^{2} + \\left(\\frac{3}{4}\\right)^{2} - 1 = \\frac{1}{16} + \\frac{9}{16} - 1 = \\frac{10}{16} - 1 = \\frac{5}{8} - \\frac{8}{8} = -\\frac{3}{8} $$\n接下来，我们计算约束函数 $h(x)$ 的雅可比矩阵。雅可比矩阵是一个 $1 \\times 2$ 矩阵：\n$$ J_h(x) = \\begin{pmatrix} \\frac{\\partial h}{\\partial x_1}  \\frac{\\partial h}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1  2x_2 \\end{pmatrix} $$\n我们计算在 $x^{\\mathrm{pred}}$ 处的雅可比矩阵：\n$$ J_h(x^{\\mathrm{pred}}) = \\begin{pmatrix} 2\\left(\\frac{1}{4}\\right)  2\\left(\\frac{3}{4}\\right) \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}  \\frac{3}{2} \\end{pmatrix} $$\n求解线性系统 $J_h(x^{\\mathrm{pred}})\\,\\delta = -h(x^{\\mathrm{pred}})$ 的最小范数校正量 $\\delta$ 由以下公式给出：\n$$ \\delta = J_h(x^{\\mathrm{pred}})^T \\left( J_h(x^{\\mathrm{pred}}) J_h(x^{\\mathrm{pred}})^T \\right)^{-1} (-h(x^{\\mathrm{pred}})) $$\n设 $J = J_h(x^{\\mathrm{pred}})$。我们计算 $JJ^T$ 项：\n$$ JJ^T = \\begin{pmatrix} \\frac{1}{2}  \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} = \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{3}{2}\\right)^2 = \\frac{1}{4} + \\frac{9}{4} = \\frac{10}{4} = \\frac{5}{2} $$\n其逆为 $(JJ^T)^{-1} = \\frac{2}{5}$。\n现在我们可以计算 $\\delta$：\n$$ \\delta = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\left( \\frac{2}{5} \\right) \\left( -(-\\frac{3}{8}) \\right) = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\left( \\frac{2}{5} \\cdot \\frac{3}{8} \\right) = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\left( \\frac{6}{40} \\right) = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\left( \\frac{3}{20} \\right) $$\n$$ \\delta = \\begin{pmatrix} \\frac{3}{40} \\\\ \\frac{9}{40} \\end{pmatrix} $$\n通过将 $\\delta$ 加到 $x^{\\mathrm{pred}}$ 上，得到校正后的迭代点 $x^{\\mathrm{corr}}$：\n$$ x^{\\mathrm{corr}} = x^{\\mathrm{pred}} + \\delta = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{3}{4} \\end{pmatrix} + \\begin{pmatrix} \\frac{3}{40} \\\\ \\frac{9}{40} \\end{pmatrix} $$\n为了进行加法，我们使用公分母 40：\n$$ x^{\\mathrm{corr}} = \\begin{pmatrix} \\frac{10}{40} + \\frac{3}{40} \\\\ \\frac{30}{40} + \\frac{9}{40} \\end{pmatrix} = \\begin{pmatrix} \\frac{13}{40} \\\\ \\frac{39}{40} \\end{pmatrix} $$\n\n**第 3 步：计算目标函数值**\n\n最后，我们计算校正点 $x^{\\mathrm{corr}}$ 处的目标函数值。\n$$ f(x^{\\mathrm{corr}}) = \\frac{1}{2} \\|x^{\\mathrm{corr}} - c\\|^2 $$\n首先，计算向量差 $x^{\\mathrm{corr}} - c$：\n$$ x^{\\mathrm{corr}} - c = \\begin{pmatrix} \\frac{13}{40} \\\\ \\frac{39}{40} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{13}{40} - \\frac{40}{40} \\\\ \\frac{39}{40} - 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{27}{40} \\\\ \\frac{39}{40} \\end{pmatrix} $$\n接下来，计算该向量的欧几里得范数的平方：\n$$ \\|x^{\\mathrm{corr}} - c\\|^2 = \\left(-\\frac{27}{40}\\right)^2 + \\left(\\frac{39}{40}\\right)^2 = \\frac{27^2}{40^2} + \\frac{39^2}{40^2} = \\frac{729}{1600} + \\frac{1521}{1600} = \\frac{729 + 1521}{1600} = \\frac{2250}{1600} $$\n目标函数值是该量的一半：\n$$ f(x^{\\mathrm{corr}}) = \\frac{1}{2} \\left( \\frac{2250}{1600} \\right) = \\frac{2250}{3200} $$\n我们通过将分子和分母同除以它们的最大公约数来化简分数。\n$$ f(x^{\\mathrm{corr}}) = \\frac{225}{320} = \\frac{45}{64} $$\n最终答案要求是精确的有理数。",
            "answer": "$$ \\boxed{\\frac{45}{64}} $$"
        },
        {
            "introduction": "在掌握了基本的几何概念后，这个练习将一个基于动量的方法重新表述为预测-校正方案。通过分析算法“过冲”最小值的条件，你将更深入地理解步长和动量等超参数如何影响优化过程的稳定性和收敛行为，这展示了预测-校正框架在分析算法动态特性时的威力。",
            "id": "3163746",
            "problem": "考虑用于最小化光滑函数 $f(x)$ 的基于动量的预测-校正方案，该方案由预测步 $v_{k+1}=\\beta v_{k}-\\alpha \\nabla f(x_{k})$ 和校正步 $x_{k+1}=x_{k}+v_{k+1}$ 定义，其中 $\\alpha>0$ 是步长，$\\beta \\geq 0$ 是动量系数。假设目标函数为严格凸的二次函数 $f(x)=\\frac{1}{2}\\lambda x^{2}$，其曲率参数为 $\\lambda>0$，因此其唯一最小化点位于 $x^{\\star}=0$，梯度为 $\\nabla f(x)=\\lambda x$。从 $x_{0}>0$ 和 $v_{0}=0$ 开始。定义校正步中的过冲为迭代点越过最小化点，即当 $x_{k}>0$ 时有 $x_{k+1} < 0$。仅使用二次目标函数的基本性质和给定的更新规则，推导最大的步长 $\\alpha^{\\star}(\\beta,\\lambda)$，使得第一次校正 $x_{1}$ 和第二次校正 $x_{2}$ 都不会发生过冲（即 $x_{1}\\geq 0$ 且 $x_{2}\\geq 0$）。将您的最终答案表示为关于 $\\beta$ 和 $\\lambda$ 的闭式解析表达式。无需四舍五入。",
            "solution": "首先对问题进行验证，以确保其科学上成立、良定且完整。\n\n### 步骤 1：提取已知条件\n-   **预测-校正方案**：一种用于最小化函数 $f(x)$ 的基于动量的方法。\n-   **预测步**：$v_{k+1}=\\beta v_{k}-\\alpha \\nabla f(x_{k})$。\n-   **校正步**：$x_{k+1}=x_{k}+v_{k+1}$。\n-   **步长**：$\\alpha>0$。\n-   **动量系数**：$\\beta \\geq 0$。\n-   **目标函数**：一个严格凸的二次函数，$f(x)=\\frac{1}{2}\\lambda x^{2}$。\n-   **曲率参数**：$\\lambda>0$。\n-   **唯一最小化点**：$x^{\\star}=0$。\n-   **梯度**：$\\nabla f(x)=\\lambda x$。\n-   **初始条件**：$x_{0}>0$ 和 $v_{0}=0$。\n-   **过冲定义**：迭代点越过最小化点，即当 $x_{k}>0$ 时有 $x_{k+1} < 0$。\n-   **约束条件**：找到最大的步长 $\\alpha^{\\star}(\\beta, \\lambda)$，使得第一次校正（$x_1$）和第二次校正（$x_2$）都不会发生过冲。这可以转化为条件 $x_{1}\\geq 0$ 和 $x_{2}\\geq 0$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题描述了一种基于动量的优化方法（重球法的一个变体）应用于一个简单的强凸二次函数。这是数值优化领域中一个标准且基础的分析。所有术语都有明确定义，初始条件和约束条件也清晰陈述。该问题是自洽的、数学上一致且客观的。它在科学上是合理的且是良定的，要求找到满足一组导出不等式的最大参数值。\n\n### 步骤 3：结论与行动\n问题有效。将推导完整的解答。\n\n### 解答推导\n\n给定预测-校正方案的更新规则为：\n$$v_{k+1} = \\beta v_{k} - \\alpha \\nabla f(x_{k})$$\n$$x_{k+1} = x_{k} + v_{k+1}$$\n\n对于指定的目标函数 $f(x)=\\frac{1}{2}\\lambda x^{2}$，其梯度为 $\\nabla f(x) = \\lambda x$。将此代入预测步，得到：\n$$v_{k+1} = \\beta v_{k} - \\alpha \\lambda x_{k}$$\n\n我们可以将状态更新表示为一个耦合的线性递推关系。我们将从初始条件 $x_0 > 0$ 和 $v_0 = 0$ 开始，计算前两次迭代的结果 $x_1$ 和 $x_2$。\n\n**第一次校正步 ($k=0$)：**\n我们计算 $v_1$，然后计算 $x_1$。\n$$v_1 = \\beta v_0 - \\alpha \\lambda x_0 = \\beta(0) - \\alpha \\lambda x_0 = -\\alpha \\lambda x_0$$\n$$x_1 = x_0 + v_1 = x_0 - \\alpha \\lambda x_0 = (1 - \\alpha \\lambda) x_0$$\n\n第一个不过冲条件是 $x_1 \\geq 0$。由于已知 $x_0 > 0$，这要求：\n$$1 - \\alpha \\lambda \\geq 0$$\n$$\\alpha \\lambda \\leq 1$$\n由于 $\\lambda > 0$，这给出了我们对步长 $\\alpha$ 的第一个约束：\n$$\\alpha \\leq \\frac{1}{\\lambda}$$\n\n**第二次校正步 ($k=1$)：**\n接下来，我们使用刚刚求得的 $x_1$ 和 $v_1$ 的值来计算 $v_2$ 和 $x_2$。\n$$v_2 = \\beta v_1 - \\alpha \\lambda x_1 = \\beta(-\\alpha \\lambda x_0) - \\alpha \\lambda ((1 - \\alpha \\lambda) x_0)$$\n$$v_2 = [-\\alpha \\lambda \\beta - \\alpha \\lambda (1 - \\alpha \\lambda)] x_0$$\n$$v_2 = [-\\alpha \\lambda \\beta - \\alpha \\lambda + (\\alpha \\lambda)^2] x_0$$\n$$v_2 = [(\\alpha \\lambda)^2 - \\alpha \\lambda (\\beta+1)] x_0$$\n\n现在，我们计算 $x_2$：\n$$x_2 = x_1 + v_2 = (1 - \\alpha \\lambda) x_0 + [(\\alpha \\lambda)^2 - \\alpha \\lambda (\\beta+1)] x_0$$\n$$x_2 = [1 - \\alpha \\lambda + (\\alpha \\lambda)^2 - \\alpha \\lambda \\beta - \\alpha \\lambda] x_0$$\n$$x_2 = [(\\alpha \\lambda)^2 - (2+\\beta) \\alpha \\lambda + 1] x_0$$\n\n第二个不过冲条件是 $x_2 \\geq 0$。由于 $x_0 > 0$，这要求关于 $\\alpha\\lambda$ 的多项式为非负：\n$$(\\alpha \\lambda)^2 - (2+\\beta) \\alpha \\lambda + 1 \\geq 0$$\n\n我们通过定义变量 $u = \\alpha \\lambda$ 来分析这个二次不等式。不等式变为：\n$$u^2 - (2+\\beta) u + 1 \\geq 0$$\n\n这是一个关于 $u$ 的二次式，代表一个开口向上的抛物线。当 $u$ 在对应方程 $u^2 - (2+\\beta) u + 1 = 0$ 的根之外时，该不等式成立。我们使用二次公式求根：\n$$u = \\frac{-(-(2+\\beta)) \\pm \\sqrt{(-(2+\\beta))^2 - 4(1)(1)}}{2(1)}$$\n$$u = \\frac{2+\\beta \\pm \\sqrt{(2+\\beta)^2 - 4}}{2}$$\n$$u = \\frac{2+\\beta \\pm \\sqrt{4 + 4\\beta + \\beta^2 - 4}}{2}$$\n$$u = \\frac{2+\\beta \\pm \\sqrt{\\beta^2 + 4\\beta}}{2}$$\n\n设两个根为 $u_1$ 和 $u_2$，且 $u_1 \\leq u_2$：\n$$u_1 = \\frac{2+\\beta - \\sqrt{\\beta^2 + 4\\beta}}{2}$$\n$$u_2 = \\frac{2+\\beta + \\sqrt{\\beta^2 + 4\\beta}}{2}$$\n不等式 $u^2 - (2+\\beta) u + 1 \\geq 0$ 在 $u \\leq u_1$ 或 $u \\geq u_2$ 时成立。\n\n**合并约束条件：**\n我们需要找到满足两个不过冲条件的最大 $\\alpha > 0$。用 $u = \\alpha \\lambda$ 表示：\n1.  由 $x_1 \\geq 0$ 得：$u \\leq 1$。\n2.  由 $x_2 \\geq 0$ 得：$u \\leq u_1$ 或 $u \\geq u_2$。\n\n我们需要同时满足这两个条件。让我们比较一下这些界限。\n首先，我们检查是否 $u_1 \\leq 1$：\n$$\\frac{2+\\beta - \\sqrt{\\beta^2 + 4\\beta}}{2} \\leq 1$$\n$$2+\\beta - \\sqrt{\\beta^2 + 4\\beta} \\leq 2$$\n$$\\beta \\leq \\sqrt{\\beta^2 + 4\\beta}$$\n由于 $\\beta \\geq 0$，两边都是非负的，因此我们可以对两边进行平方：\n$$\\beta^2 \\leq \\beta^2 + 4\\beta$$\n$$0 \\leq 4\\beta$$\n这对所有 $\\beta \\geq 0$ 都成立。因此，$u_1 \\leq 1$。\n\n其次，我们检查是否 $u_2 \\geq 1$：\n$$\\frac{2+\\beta + \\sqrt{\\beta^2 + 4\\beta}}{2} \\geq 1$$\n$$2+\\beta + \\sqrt{\\beta^2 + 4\\beta} \\geq 2$$\n$$\\beta + \\sqrt{\\beta^2 + 4\\beta} \\geq 0$$\n由于两项都是非负的，这对所有 $\\beta \\geq 0$ 也成立。因此，$u_2 \\geq 1$。\n\n我们在寻找最大的 $u$，使得 ($u \\leq 1$) 并且 ($u \\leq u_1$ 或 $u \\geq u_2$)。\n由于 $u_1 \\leq 1 \\leq u_2$，这些条件的交集是 $u \\leq u_1$。区域 $u \\geq u_2$ 与 $u \\leq 1$ 不兼容（除非在 $\\beta=0$ 时，$u_2=1$，此时交集为单点 $u=1$）。\n对于任何 $\\beta \\geq 0$，合并后的条件简化为：\n$$u \\leq u_1$$\n\n为了找到允许的最大步长 $\\alpha^{\\star}$，我们取 $u = \\alpha \\lambda$ 的最大可能值，即 $u_1$：\n$$\\alpha^{\\star} \\lambda = u_1 = \\frac{2+\\beta - \\sqrt{\\beta^2 + 4\\beta}}{2}$$\n\n求解 $\\alpha^{\\star}$ 得到最终表达式：\n$$\\alpha^{\\star}(\\beta, \\lambda) = \\frac{2+\\beta - \\sqrt{\\beta^2 + 4\\beta}}{2\\lambda}$$\n这可以写作：\n$$\\alpha^{\\star}(\\beta, \\lambda) = \\frac{2+\\beta - \\sqrt{\\beta(\\beta+4)}}{2\\lambda}$$\n这是保证第一次和第二次校正步都不会越过最小化点 $x^{\\star}=0$ 的最大步长。",
            "answer": "$$\n\\boxed{\\frac{2+\\beta - \\sqrt{\\beta(\\beta+4)}}{2\\lambda}}\n$$"
        },
        {
            "introduction": "最后的这项实践将挑战你在现代机器学习的背景下应用预测-校正思想。你将设计并实现一个算法，其中基础算法（ISTA）作为预测器，而一个加速步骤（受FISTA启发）作为校正器。通过这个过程，你将亲身体验如何构建更强大的方案来加速解决像Lasso这样的实际问题。",
            "id": "3163759",
            "problem": "考虑被称为最小绝对收缩和选择算子 (Lasso) 的凸优化问题，其目标函数定义为\n$$\nF(x) \\;=\\; \\phi(x) \\;+\\; \\lambda \\,\\|x\\|_1,\\qquad \\phi(x) \\;=\\; \\frac{1}{2}\\,\\|A x - b\\|_2^2,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，$\\lambda > 0$ 且 $x \\in \\mathbb{R}^n$。光滑部分 $\\phi(x)$ 是可微的，其梯度为\n$$\n\\nabla \\phi(x) \\;=\\; A^\\top (A x - b),\n$$\n其梯度在谱范数意义下是 Lipschitz 连续的，常数为 $L \\ge \\|A^\\top A\\|_2$。非光滑部分是缩放的 $\\ell_1$-范数，其邻近算子是软阈值（收缩）映射\n$$\n\\operatorname{shrink}(z, \\tau)_i \\;=\\; \\operatorname{sign}(z_i)\\,\\max\\!\\big(|z_i| - \\tau,\\, 0\\big), \\quad \\text{for each coordinate } i,\n$$\n它等于 $\\tau \\,\\|x\\|_1$ 在点 $z$ 处的邻近算子。\n\n您将设计一个预测-校正方案，用于从当前迭代点 $x_k$ 和前一个迭代点 $x_{k-1}$ 开始最小化 $F(x)$。预测器是一个迭代收缩阈值算法 (ISTA) 步骤，校正器是一个快速迭代收缩阈值算法 (FISTA) 步骤。您的任务如下：\n\n1) 预测器 (ISTA)：使用步长 $s \\in (0, 1/L]$，计算\n$$\nx_{\\mathrm{p}} \\;=\\; \\operatorname{shrink}\\!\\big(x_k - s\\,\\nabla \\phi(x_k),\\; s\\,\\lambda\\big).\n$$\n\n2) 校正器（一个 FISTA 步骤）：使用经典的 FISTA 动量参数，该参数由序列 $t_0 = 1$ 和以下公式定义\n$$\nt_{j} \\;=\\; \\frac{1 + \\sqrt{1 + 4\\,t_{j-1}^2}}{2},\\quad \\text{for } j \\ge 1,\n$$\n构建单步加速系数\n$$\n\\beta \\;=\\; \\frac{t_{1} - 1}{t_{2}},\n$$\n并计算外推点\n$$\ny \\;=\\; x_{\\mathrm{p}} + \\beta\\,(x_{\\mathrm{p}} - x_{k-1}),\n$$\n然后对点 $y$ 进行一次邻近梯度更新：\n$$\nx_{\\mathrm{c}} \\;=\\; \\operatorname{shrink}\\!\\big(y - s\\,\\nabla \\phi(y),\\; s\\,\\lambda\\big).\n$$\n\n3) 待测试条件：当以下两个条件都成立时，我们称校正器在没有过冲的情况下实现了加速：\n- 加速：$F(x_{\\mathrm{c}}) < F(x_{\\mathrm{p}})$。\n- 无过冲：$F(x_{\\mathrm{c}}) \\le F(x_k)$。\n由于有限精度，采用非负容差 $\\varepsilon = 10^{-10}$，并将 $a < b$ 解释为 $a \\le b - \\varepsilon$，将 $a \\le b$ 解释为 $a \\le b + \\varepsilon$。\n\n4) 初始化协议：对于给定的测试用例 $(A, b, \\lambda, \\alpha)$，使用 $A$ 的谱范数估计 $L = \\|A\\|_2^2$，并设置 $s = \\alpha/L$，其中 $\\alpha \\in (0, 1]$。从 $x_0 = 0$ 开始初始化。计算两个预热 ISTA 步骤\n$$\nx_1 \\;=\\; \\operatorname{shrink}\\!\\big(x_0 - s\\,\\nabla \\phi(x_0),\\; s\\,\\lambda\\big),\\qquad\nx_2 \\;=\\; \\operatorname{shrink}\\!\\big(x_1 - s\\,\\nabla \\phi(x_1),\\; s\\,\\lambda\\big).\n$$\n然后在 $k = 2$ 时，使用 $(x_{k-1}, x_k) = (x_1, x_2)$ 应用预测-校正方案，以获得 $x_{\\mathrm{p}}$ 和 $x_{\\mathrm{c}}$，并评估第 3 项中的两个不等式。\n\n5) 测试套件：使用以下四个测试用例。每个用例使用 $m = 2$ 和 $n = 2$，以及 $b = [1, 0]^\\top$。矩阵 $A$ 是对称的，其相关参数为 $\\rho$，通过以下方式给出\n$$\nA \\;=\\; \\begin{bmatrix} 1  \\rho \\\\ \\rho  1 \\end{bmatrix}.\n$$\n评估每个用例中表示“在没有过冲的情况下加速”的布尔值。\n- 用例 1（理想路径）：$\\rho = 0.3$, $\\lambda = 0.1$, $\\alpha = 1.0$。\n- 用例 2（病态挑战）：$\\rho = 0.99$, $\\lambda = 0.001$, $\\alpha = 1.0$。\n- 用例 3（强正则化）：$\\rho = 0.3$, $\\lambda = 0.6$, $\\alpha = 1.0$。\n- 用例 4（边界解耦情况）：$\\rho = 0$, $\\lambda = 0.1$, $\\alpha = 1.0$。\n\n6) 必需的最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。该列表必须包含与上述四个用例按顺序列出的相对应的四个布尔值。例如，输出格式必须如下所示\n$$\n[\\text{true}_1,\\text{true}_2,\\text{true}_3,\\text{true}_4]\n$$\n但使用 Python 的布尔值大写形式，即，完全像 $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True}]$（此示例仅为说明性的；您必须为指定的测试用例输出实际计算的结果）。",
            "solution": "用户在数值优化领域提供了一个定义明确的计算问题。我将首先验证问题陈述，然后提供一个全面的解决方案。\n\n### 问题验证\n\n1.  **提取已知条件**：\n    *   **目标函数**：$F(x) = \\phi(x) + \\lambda \\|x\\|_1$，其中 $\\phi(x) = \\frac{1}{2}\\|Ax - b\\|_2^2$。\n    *   **数据**：$A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，$\\lambda > 0$，$x \\in \\mathbb{R}^n$。\n    *   **梯度**：$\\nabla \\phi(x) = A^\\top (Ax - b)$。\n    *   **$\\nabla\\phi$ 的 Lipschitz 常数**：$L \\ge \\|A^\\top A\\|_2$。\n    *   **邻近算子**：$\\operatorname{shrink}(z, \\tau)_i = \\operatorname{sign}(z_i)\\max(|z_i| - \\tau, 0)$。\n    *   **预测器步骤 (ISTA)**：$x_{\\mathrm{p}} = \\operatorname{shrink}(x_k - s\\nabla \\phi(x_k), s\\lambda)$，步长 $s \\in (0, 1/L]$。\n    *   **校正器步骤 (类 FISTA)**：\n        *   动量参数：$t_0 = 1$，$t_{j} = (1 + \\sqrt{1 + 4t_{j-1}^2})/2$（当 $j \\ge 1$ 时）。\n        *   加速系数：$\\beta = (t_1 - 1)/t_2$。\n        *   外推：$y = x_{\\mathrm{p}} + \\beta(x_{\\mathrm{p}} - x_{k-1})$。\n        *   校正：$x_{\\mathrm{c}} = \\operatorname{shrink}(y - s\\nabla \\phi(y), s\\lambda)$。\n    *   **评估条件**：“在没有过冲的情况下加速”成立，需同时满足 $F(x_{\\mathrm{c}}) < F(x_{\\mathrm{p}})$ 和 $F(x_{\\mathrm{c}}) \\le F(x_k)$，使用容差 $\\varepsilon = 10^{-10}$ 进行解释，即 $a < b$ 为 $a \\le b - \\varepsilon$，$a \\le b$ 为 $a \\le b + \\varepsilon$。\n    *   **初始化**：对于一个用例 $(A, b, \\lambda, \\alpha)$，设置 $L = \\|A\\|_2^2$，$s = \\alpha/L$。从 $x_0 = 0$ 开始。计算 2 个预热 ISTA 步骤以得到 $x_1$ 和 $x_2$。\n    *   **测试执行**：在迭代 $k=2$ 时应用预测-校正方案，其中当前迭代点为 $x_k=x_2$，前一个迭代点为 $x_{k-1}=x_1$。\n    *   **测试套件**：$m=2$, $n=2$, $b = [1, 0]^\\top$, $A = \\begin{bmatrix} 1  \\rho \\\\ \\rho  1 \\end{bmatrix}$。指定了四个用例，分别具有不同的 $(\\rho, \\lambda, \\alpha)$ 值。\n\n2.  **使用提取的已知条件进行验证**：\n    *   **科学依据**：该问题在凸优化和一阶方法的理论中有充分的依据。Lasso 目标、ISTA 和 FISTA 都是标准课题。所给出的特定预测-校正方案是基于这些已建立算法的一个有效（尽管是自定义的）构造。选择 Lipschitz 常数 $L = \\|A\\|_2^2$ 是正确的，因为对于对称矩阵 $A$，$\\|A^\\top A\\|_2 = \\|A^2\\|_2 = \\|A\\|_2^2$。对于一般矩阵，$\\|A^\\top A\\|_2 = \\|A\\|_2^2$ 也成立。\n    *   **适定性和完整性**：该问题是确定性的，并提供了所有必要的信息：函数、算法步骤、初始条件、所有测试用例的参数值，以及包括数值容差在内的精确评估标准。它是明确且自洽的。\n    *   **客观性和可行性**：该问题使用客观的数学语言陈述。测试用例涉及小的（$2 \\times 2$）矩阵，使得计算完全可行。\n\n3.  **结论与行动**：问题陈述是有效的。它是一个清晰、一致且计算上合理的任务。我将继续进行求解。\n\n### 解法推导\n\n该问题要求实现并评估一个用于解决 Lasso 优化问题的特定预测-校正算法。将严格按照所提供的步骤构建解决方案。\n\n**1. Lasso 问题与邻近梯度法**\n\n目标是最小化函数 $F(x) = \\phi(x) + g(x)$，其中 $\\phi(x) = \\frac{1}{2}\\|Ax - b\\|_2^2$ 是一个光滑、凸、可微的函数，而 $g(x) = \\lambda \\|x\\|_1$ 是一个凸但不可微的正则化项。\n\n这类结构的问题可以通过邻近梯度法有效解决。其核心思想是迭代地对光滑部分 $\\phi(x)$ 执行梯度下降步骤，然后应用非光滑部分 $g(x)$ 的邻近算子来校正该步骤。通用的更新规则是：\n$$\nx_{k+1} = \\operatorname{prox}_{sg}(x_k - s\\nabla \\phi(x_k))\n$$\n其中 $s > 0$ 是步长。对于 $g(x) = \\lambda\\|x\\|_1$，邻近算子 $\\operatorname{prox}_{s\\lambda\\|\\cdot\\|_1}(z)$ 是软阈值函数 $\\operatorname{shrink}(z, s\\lambda)$，其按元素定义为：\n$$\n\\operatorname{shrink}(z, \\tau)_i = \\operatorname{sign}(z_i)\\max(|z_i| - \\tau, 0)\n$$\n这个迭代方案被称为迭代收缩阈值算法 (ISTA)。为保证收敛，步长 $s$ 必须在区间 $(0, 1/L]$ 内，其中 $L$ 是 $\\nabla\\phi(x)$ 的 Lipschitz 常数。\n\n**2. 指定的预测-校正方案**\n\n我们必须实现的算法是一个混合方案。它从一个当前迭代点 $x_k$ 和一个前一个迭代点 $x_{k-1}$ 开始。\n\n*   **初始化**：我们从 $x_0=0$ 开始。为了获得一个合理的初始点对 $(x_1, x_2)$，我们执行 2 个“预热”ISTA 步骤。步长固定为 $s = \\alpha/L$，其中 $L = \\|A\\|_2^2$ 是 $\\nabla\\phi$ 的 Lipschitz 常数。对于给定的测试用例，$\\alpha=1.0$，因此我们使用完整步长 $s=1/L$。\n    $$\n    x_1 = \\operatorname{shrink}(x_0 - s\\nabla \\phi(x_0), s\\lambda)\n    $$\n    $$\n    x_2 = \\operatorname{shrink}(x_1 - s\\nabla \\phi(x_1), s\\lambda)\n    $$\n    这些迭代点 $x_1$ 和 $x_2$ 将作为主算法的初始状态 $(x_{k-1}, x_k)$，此时 $k=2$。\n\n*   **预测器**：预测器步骤是对 $x_k = x_2$ 应用单次 ISTA：\n    $$\n    x_{\\mathrm{p}} = \\operatorname{shrink}(x_k - s\\nabla \\phi(x_k), s\\lambda)\n    $$\n    这将产生一个候选解 $x_{\\mathrm{p}}$，它在一个标准 ISTA 序列中将是下一个迭代点 $x_3$。根据构造，邻近梯度下降保证了目标函数值的下降，即 $F(x_{\\mathrm{p}}) \\le F(x_k)$。\n\n*   **校正器**：校正器步骤旨在通过引入动量来改进预测器，这借鉴了快速迭代收缩阈值算法 (FISTA) 的思想。FISTA 通过在一个外推点上进行邻近梯度步骤来加速收敛。在这里，外推定义为：\n    $$\n    y = x_{\\mathrm{p}} + \\beta(x_{\\mathrm{p}} - x_{k-1})\n    $$\n    其中 $x_{k-1} = x_1$。系数 $\\beta$ 是一个固定的、单步动量参数，源自 FISTA 的递推关系：\n    $$\n    t_0 = 1, \\quad t_1 = \\frac{1 + \\sqrt{1 + 4t_0^2}}{2} = \\frac{1 + \\sqrt{5}}{2}, \\quad t_2 = \\frac{1 + \\sqrt{1 + 4t_1^2}}{2}\n    $$\n    $$\n    \\beta = \\frac{t_1 - 1}{t_2}\n    $$\n    然后，通过在这个外推点 $y$ 上应用邻近梯度更新来获得校正器迭代点 $x_{\\mathrm{c}}$：\n    $$\n    x_{\\mathrm{c}} = \\operatorname{shrink}(y - s\\nabla \\phi(y), s\\lambda)\n    $$\n\n**3. 评估逻辑**\n\n校正器的性能由两个必须同时满足的条件来评判：\n1.  **加速**：$F(x_{\\mathrm{c}}) < F(x_{\\mathrm{p}})$。此条件检查校正器步骤是否确实比简单的预测器步骤有所改进。严格不等式很重要。\n2.  **无过冲**：$F(x_{\\mathrm{c}}) \\le F(x_k)$。加速方法中的动量项可能导致振荡，即迭代点可能会暂时增加目标函数值。此条件确保校正器步骤不会产生比起始迭代点 $x_k$ 更差的结果。\n\n数值实现将使用 $\\varepsilon = 10^{-10}$ 的容差，以使这些比较对浮点不精确性具有鲁棒性。\n\n**4. 在测试用例上执行**\n\n将该过程应用于 4 个指定的测试用例中的每一个。对于每个由参数 $(\\rho, \\lambda, \\alpha)$ 表征的用例：\n1.  构造矩阵 $A$ 和向量 $b$。\n2.  计算 Lipschitz 常数 $L = \\|A\\|_2^2$ 和步长 $s = \\alpha/L$。\n3.  从 $x_0=0$ 计算预热迭代点 $x_1$ 和 $x_2$。\n4.  从 $x_k=x_2$ 计算预测器 $x_{\\mathrm{p}}$。\n5.  计算动量系数 $\\beta$，并使用从 $(x_{\\mathrm{p}}, x_1)$ 的外推计算校正器 $x_{\\mathrm{c}}$。\n6.  计算目标函数值 $F(x_2)$、$F(x_{\\mathrm{p}})$ 和 $F(x_{\\mathrm{c}})$。\n7.  评估“在没有过冲的情况下加速”的两个条件，它们的逻辑与运算结果决定了该用例的最终布尔输出。\n\n这个完整、确定性的过程将被编码在提供的 Python 脚本中。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a predictor-corrector scheme for the Lasso problem\n    on a suite of four test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (rho, lambda, alpha)\n    test_cases = [\n        (0.3, 0.1, 1.0),    # Case 1: happy path\n        (0.99, 0.001, 1.0), # Case 2: ill-conditioned challenge\n        (0.3, 0.6, 1.0),    # Case 3: strong regularization\n        (0.0, 0.1, 1.0),    # Case 4: boundary decoupled case\n    ]\n\n    results = []\n    \n    # Define helper functions based on the problem statement\n    \n    def objective_F(x, A, b, lambda_val):\n        \"\"\"Computes the Lasso objective function F(x).\"\"\"\n        smooth_part = 0.5 * np.linalg.norm(A @ x - b)**2\n        nonsmooth_part = lambda_val * np.linalg.norm(x, 1)\n        return smooth_part + nonsmooth_part\n\n    def grad_phi(x, A, b):\n        \"\"\"Computes the gradient of the smooth part phi(x).\"\"\"\n        return A.T @ (A @ x - b)\n\n    def shrink(z, tau):\n        \"\"\"Applies the soft-thresholding operator element-wise.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n    for case_params in test_cases:\n        rho, lambda_val, alpha = case_params\n        \n        # Setup for the test case\n        m, n = 2, 2\n        b = np.array([1.0, 0.0])\n        A = np.array([[1.0, rho], [rho, 1.0]])\n        \n        # Initialization protocol\n        # L must be >= ||A^T A||_2. We use L = ||A||_2^2 which is equal to ||A^T A||_2\n        L = np.linalg.norm(A, 2)**2\n        s = alpha / L\n        epsilon = 1e-10\n\n        # Initial point\n        x0 = np.zeros(n)\n\n        # Two warm-up ISTA steps\n        x1 = shrink(x0 - s * grad_phi(x0, A, b), s * lambda_val)\n        x2 = shrink(x1 - s * grad_phi(x1, A, b), s * lambda_val)\n        \n        # Set iterates for the main step (k=2)\n        xk_minus_1 = x1\n        xk = x2\n\n        # 1) Predictor (ISTA step)\n        xp = shrink(xk - s * grad_phi(xk, A, b), s * lambda_val)\n\n        # 2) Corrector (one FISTA step)\n        # Compute momentum coefficient beta\n        t0 = 1.0\n        t1 = (1.0 + np.sqrt(1.0 + 4.0 * t0**2)) / 2.0\n        t2 = (1.0 + np.sqrt(1.0 + 4.0 * t1**2)) / 2.0\n        beta = (t1 - 1.0) / t2\n        \n        # Extrapolate\n        y = xp + beta * (xp - xk_minus_1)\n        \n        # Proximal-gradient update at y\n        xc = shrink(y - s * grad_phi(y, A, b), s * lambda_val)\n        \n        # 3) Evaluate conditions\n        F_xk = objective_F(xk, A, b, lambda_val)\n        F_xp = objective_F(xp, A, b, lambda_val)\n        F_xc = objective_F(xc, A, b, lambda_val)\n\n        # Condition: Acceleration (F_xc  F_xp)\n        accelerates = (F_xc = F_xp - epsilon)\n        \n        # Condition: No overshoot (F_xc = F_xk)\n        no_overshoot = (F_xc = F_xk + epsilon)\n\n        # Final boolean result for the case\n        case_result = accelerates and no_overshoot\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}