{
    "hands_on_practices": [
        {
            "introduction": "在处理带约束的优化问题时，一个直接的挑战是如何在向最优解迭代的同时确保所有点都满足约束条件。预测-校正方案为此提供了一个优雅的框架：首先，“预测”步骤朝着目标函数的下降方向迈出一步，暂时忽略约束；然后，“校正”步骤将得到的新点投影或拉回到可行域。这个练习将通过一个简单的二次规划问题，让你亲手计算一个预测步和随后的投影校正步，从而直观地理解如何在约束流形上保持可行性。",
            "id": "3163808",
            "problem": "考虑一个等式约束优化问题，其目标是在光滑流形约束下最小化二次目标函数。设目标函数为 $f(x) = \\frac{1}{2}\\|x - c\\|^{2}$，其中 $x \\in \\mathbb{R}^{2}$，$c = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，约束条件为 $h(x) = x_{1}^{2} + x_{2}^{2} - 1 = 0$，该约束定义了单位圆。假设您处于可行点 $x^{(0)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$。\n\n预测-校正方案的步骤是：首先基于目标函数的梯度进行无约束的预测步，然后通过使用一阶局部近似投影到约束流形上来校正以恢复可行性。具体步骤如下：\n\n1. 使用无约束梯度步长形成预测值 $x^{\\mathrm{pred}} = x^{(0)} - \\alpha \\nabla f(x^{(0)})$，步长为 $\\alpha = \\frac{1}{4}$。\n2. 使用约束在 $x^{\\mathrm{pred}}$ 点的一阶泰勒展开式 $h(x^{\\mathrm{pred}} + \\delta) \\approx h(x^{\\mathrm{pred}}) + J_{h}(x^{\\mathrm{pred}})\\,\\delta$，确定恢复可行性的最小范数校正量 $\\delta$，其中 $J_{h}(x)$ 表示 $h$ 在 $x$ 点的雅可比矩阵。然后形成校正后的迭代点 $x^{\\mathrm{corr}} = x^{\\mathrm{pred}} + \\delta$。\n3. 计算目标函数值 $f(x^{\\mathrm{corr}})$。\n\n将您计算出的 $f(x^{\\mathrm{corr}})$ 的最终数值答案精确地表示为一个有理数。不要四舍五入。",
            "solution": "用户提供了一个形式良好（well-posed）的优化问题，需要应用单步预测-校正方法。该问题有科学依据、自成体系且客观。我们将按照指定的步骤进行求解。\n\n目标函数为 $f(x) = \\frac{1}{2}\\|x - c\\|^{2}$，其中 $x = \\begin{pmatrix}x_1 \\\\ x_2\\end{pmatrix} \\in \\mathbb{R}^{2}$，$c = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$。约束条件为 $h(x) = x_{1}^{2} + x_{2}^{2} - 1 = 0$。初始点为 $x^{(0)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$。\n\n**第 1 步：预测步**\n\n首先，我们计算目标函数的梯度 $\\nabla f(x)$。\n目标函数可以写为 $f(x) = \\frac{1}{2}((x_1 - 1)^2 + (x_2 - 0)^2)$。\n梯度由下式给出：\n$$ \\nabla f(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} x_1 - 1 \\\\ x_2 \\end{pmatrix} = x - c $$\n我们在初始点 $x^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ 处计算梯度：\n$$ \\nabla f(x^{(0)}) = x^{(0)} - c = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} - \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}-1 \\\\ 1\\end{pmatrix} $$\n预测步是一个步长为 $\\alpha = \\frac{1}{4}$ 的无约束梯度步：\n$$ x^{\\mathrm{pred}} = x^{(0)} - \\alpha \\nabla f(x^{(0)}) $$\n代入数值：\n$$ x^{\\mathrm{pred}} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix}-1 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}0 + \\frac{1}{4} \\\\ 1 - \\frac{1}{4}\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{4} \\\\ \\frac{3}{4}\\end{pmatrix} $$\n\n**第 2 步：校正步**\n\n校正步旨在通过从 $x^{\\mathrm{pred}}$ 移动一个最小范数校正向量 $\\delta$ 来恢复可行性，使得约束的一阶近似得到满足。\n线性化的约束方程为 $h(x^{\\mathrm{pred}}) + J_{h}(x^{\\mathrm{pred}})\\,\\delta = 0$。\n\n首先，我们在 $x^{\\mathrm{pred}}$ 处计算约束函数的值：\n$$ h(x^{\\mathrm{pred}}) = \\left(\\frac{1}{4}\\right)^{2} + \\left(\\frac{3}{4}\\right)^{2} - 1 = \\frac{1}{16} + \\frac{9}{16} - 1 = \\frac{10}{16} - 1 = \\frac{5}{8} - \\frac{8}{8} = -\\frac{3}{8} $$\n接下来，我们计算约束函数 $h(x)$ 的雅可比矩阵。雅可比矩阵是一个 $1 \\times 2$ 的矩阵：\n$$ J_h(x) = \\begin{pmatrix} \\frac{\\partial h}{\\partial x_1}  \\frac{\\partial h}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1  2x_2 \\end{pmatrix} $$\n我们在 $x^{\\mathrm{pred}}$ 处计算雅可比矩阵：\n$$ J_h(x^{\\mathrm{pred}}) = \\begin{pmatrix} 2\\left(\\frac{1}{4}\\right)  2\\left(\\frac{3}{4}\\right) \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}  \\frac{3}{2} \\end{pmatrix} $$\n求解线性系统 $J_h(x^{\\mathrm{pred}})\\,\\delta = -h(x^{\\mathrm{pred}})$ 的最小范数校正量 $\\delta$ 由以下公式给出：\n$$ \\delta = J_h(x^{\\mathrm{pred}})^T \\left( J_h(x^{\\mathrm{pred}}) J_h(x^{\\mathrm{pred}})^T \\right)^{-1} (-h(x^{\\mathrm{pred}})) $$\n令 $J = J_h(x^{\\mathrm{pred}})$。我们计算 $JJ^T$ 项：\n$$ JJ^T = \\begin{pmatrix} \\frac{1}{2}  \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} = \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{3}{2}\\right)^2 = \\frac{1}{4} + \\frac{9}{4} = \\frac{10}{4} = \\frac{5}{2} $$\n其逆为 $(JJ^T)^{-1} = \\frac{2}{5}$。\n现在我们可以计算 $\\delta$：\n$$ \\delta = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\left( \\frac{2}{5} \\right) \\left( -(-\\frac{3}{8}) \\right) = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\left( \\frac{2}{5} \\cdot \\frac{3}{8} \\right) = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\left( \\frac{6}{40} \\right) = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\left( \\frac{3}{20} \\right) $$\n$$ \\delta = \\begin{pmatrix} \\frac{3}{40} \\\\ \\frac{9}{40} \\end{pmatrix} $$\n通过将 $\\delta$ 加到 $x^{\\mathrm{pred}}$ 上，可以得到校正后的迭代点 $x^{\\mathrm{corr}}$：\n$$ x^{\\mathrm{corr}} = x^{\\mathrm{pred}} + \\delta = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{3}{4} \\end{pmatrix} + \\begin{pmatrix} \\frac{3}{40} \\\\ \\frac{9}{40} \\end{pmatrix} $$\n为了进行加法运算，我们使用公分母 40：\n$$ x^{\\mathrm{corr}} = \\begin{pmatrix} \\frac{10}{40} + \\frac{3}{40} \\\\ \\frac{30}{40} + \\frac{9}{40} \\end{pmatrix} = \\begin{pmatrix} \\frac{13}{40} \\\\ \\frac{39}{40} \\end{pmatrix} $$\n\n**第 3 步：计算目标函数值**\n\n最后，我们计算校正点 $x^{\\mathrm{corr}}$ 处的目标函数值。\n$$ f(x^{\\mathrm{corr}}) = \\frac{1}{2} \\|x^{\\mathrm{corr}} - c\\|^2 $$\n首先，计算向量差 $x^{\\mathrm{corr}} - c$：\n$$ x^{\\mathrm{corr}} - c = \\begin{pmatrix} \\frac{13}{40} \\\\ \\frac{39}{40} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{13}{40} - \\frac{40}{40} \\\\ \\frac{39}{40} - 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{27}{40} \\\\ \\frac{39}{40} \\end{pmatrix} $$\n接下来，计算该向量的欧几里得范数的平方：\n$$ \\|x^{\\mathrm{corr}} - c\\|^2 = \\left(-\\frac{27}{40}\\right)^2 + \\left(\\frac{39}{40}\\right)^2 = \\frac{27^2}{40^2} + \\frac{39^2}{40^2} = \\frac{729}{1600} + \\frac{1521}{1600} = \\frac{729 + 1521}{1600} = \\frac{2250}{1600} $$\n目标函数值是该量的一半：\n$$ f(x^{\\mathrm{corr}}) = \\frac{1}{2} \\left( \\frac{2250}{1600} \\right) = \\frac{2250}{3200} $$\n我们通过将分子和分母除以它们的最大公约数来简化分数。\n$$ f(x^{\\mathrm{corr}}) = \\frac{225}{320} = \\frac{45}{64} $$\n最终答案要求是一个精确的有理数。",
            "answer": "$$ \\boxed{\\frac{45}{64}} $$"
        },
        {
            "introduction": "内点法是求解大规模约束优化问题的强大工具，其核心思想是在可行域的“内部”进行迭代。为了确保迭代点始终严格满足边界约束（例如 $x_{i} > 0$），预测-校正策略在这里扮演了“安全卫士”的角色。本练习模拟了内点法中的一个关键环节：计算一个既能保证迭代点严格远离边界，又能实现有效进展的步长。通过这个“边界分数”规则的计算，你将学会如何在预测步可能“越界”时进行校正，以保证算法的稳定性和有效性。",
            "id": "3163783",
            "problem": "考虑一个带简单箱式约束的凸优化问题，其中变量向量 $x \\in \\mathbb{R}^{n}$ 需要按分量满足 $0  x  u$，该方法使用形式为 $\\phi(x) = f(x) - \\mu \\sum_{i=1}^{n} \\left( \\ln(x_{i}) + \\ln(u_{i} - x_{i}) \\right)$ 的对数障碍，其中障碍参数 $\\mu > 0$。在内点法 (IPM) 中，一个 Mehrotra 风格的预测-校正步会从当前严格可行点 $x$ 提出一个试探位移 $d \\in \\mathbb{R}^{n}$，但如果采用单位步长，预测步可能会严重违反边界约束。为了在下一个迭代点 $x^{+} = x + \\alpha d$ 和 $s^{+} = s - \\alpha d$ 保持原始变量 $x$ 和相关的上界松弛变量 $s = u - x$ 的严格可行性，必须选择可接受的步长 $\\alpha$ 以保持 $x^{+} > 0$ 和 $s^{+} > 0$ 按分量成立。\n\n从对数障碍仅在内部（即对所有 $i$ 都有 $x_{i} > 0$ 和 $s_{i} = u_{i} - x_{i} > 0$）定义这一基本要求出发，推导沿给定方向 $d$ 为 $x^{+}$ 和 $s^{+}$ 保持严格正性的最大步长 $\\alpha_{\\max}$。然后，应用参数为 $\\tau \\in (0,1)$ 的到边界分数规则（fraction-to-the-boundary safeguard）来获得一个校正后的步长 $\\alpha_{\\text{safe}}$，该步长适用于预测-校正方案。\n\n对一个三维实例使用以下数据：\n- $x = (4, 1.5, 6)$,\n- $u = (5, 2, 7)$,\n- $d = (-3, -1, 2)$,\n- $\\tau = 0.8$。\n\n计算经过安全保护的步长 $\\alpha_{\\text{safe}}$。以单个实数形式提供你的最终答案。如果需要四舍五入，则保留四位有效数字；但是，如果得到精确值，请报告精确值。",
            "solution": "首先验证问题，以确保其科学上合理、适定且客观。\n\n### 步骤 1：提取已知条件\n- **问题背景**：带边界约束的凸优化问题。\n- **约束**: 对 $x \\in \\mathbb{R}^{n}$，按分量满足 $0  x  u$。\n- **障碍函数**: $\\phi(x) = f(x) - \\mu \\sum_{i=1}^{n} \\left( \\ln(x_{i}) + \\ln(u_{i} - x_{i}) \\right)$，其中 $\\mu > 0$。\n- **方法**：带有预测-校正步的内点法 (IPM)。\n- **当前原始迭代点**: 一个严格可行点 $x$。\n- **搜索方向**: $d \\in \\mathbb{R}^{n}$。\n- **上界松弛变量**: $s = u - x$。\n- **下一个迭代点**: $x^{+} = x + \\alpha d$ 和 $s^{+} = s - \\alpha d$。\n- **可行性要求**: 按分量满足 $x^{+} > 0$ 和 $s^{+} > 0$。\n- **步长**: $\\alpha_{\\max}$ 是保持严格正性的最大步长，$\\alpha_{\\text{safe}}$ 是经过安全保护的步长。\n- **安全保护规则**: $\\alpha_{\\text{safe}} = \\tau \\alpha_{\\max}$，其中 $\\tau \\in (0,1)$。\n- **实例数据 ($n=3$):**\n  - $x = (4, 1.5, 6)$\n  - $u = (5, 2, 7)$\n  - $d = (-3, -1, 2)$\n  - $\\tau = 0.8$\n\n### 步骤 2：使用提取的已知条件进行验证\n问题陈述具有科学依据，描述了优化内点法中使用的标准“到边界分数”规则。问题是适定的，提供了计算唯一解所需的所有数据。数据是一致的；初始点 $x$ 按要求是严格可行的，因为 $0  4  5$，$0  1.5  2$ 以及 $0  6  7$。不存在任何歧义、矛盾或违反科学原理之处。\n\n### 步骤 3：结论与行动\n该问题有效。将推导解答。\n\n### 解答推导\n\n目标是计算经过安全保护的步长 $\\alpha_{\\text{safe}}$。这首先需要确定 $\\alpha_{\\max}$，即为下一个迭代点保持严格可行性的最大步长 $\\alpha > 0$。严格可行性意味着对于每个分量 $i \\in \\{1, 2, \\dots, n\\}$，条件 $x_i^{+} > 0$ 和 $s_i^{+} > 0$ 都必须成立。\n\n下一个原始迭代点是 $x^{+} = x + \\alpha d$。条件 $x^{+} > 0$ 意味着对每个分量 $i$ 都有 $x_i + \\alpha d_i > 0$。\n- 如果 $d_i \\ge 0$，由于 $x_i > 0$（根据初始可行性）且我们寻求 $\\alpha > 0$，不等式 $x_i + \\alpha d_i > 0$ 总是满足的，不对 $\\alpha$ 施加任何上界。\n- 如果 $d_i  0$，不等式可以重排为 $\\alpha (-d_i)  x_i$。由于 $-d_i > 0$，我们可以通过除法得到步长的上界：$\\alpha  \\frac{x_i}{-d_i}$。\n\n下一个松弛变量迭代点是 $s^{+} = s - \\alpha d$，其中 $s = u - x$。条件 $s^{+} > 0$ 意味着对每个分量 $i$ 都有 $s_i - \\alpha d_i > 0$。\n- 如果 $d_i \\le 0$，由于 $s_i > 0$（根据初始可行性）且 $\\alpha > 0$，项 $-\\alpha d_i$ 是非负的，所以不等式 $s_i - \\alpha d_i > 0$ 总是满足的，不对 $\\alpha$ 施加任何上界。\n- 如果 $d_i > 0$，不等式可以重排为 $\\alpha d_i  s_i$。这给出了步长的上界：$\\alpha  \\frac{s_i}{d_i}$。\n\n为了对所有 $i$ 同时满足所有这些条件，$\\alpha$ 必须小于所有推导出的上界的最小值。在此背景下，$\\alpha_{\\max}$ 指的是使 $x^{+}$ 或 $s^{+}$ 的某个分量达到其边界值 $0$ 的步长。因此，$\\alpha_{\\max}$ 是所有极限步长比率的最小值。\n\n$$\n\\alpha_{\\max} = \\min \\left( \\min_{i: d_i  0} \\left\\{ \\frac{x_i}{-d_i} \\right\\}, \\min_{i: d_i > 0} \\left\\{ \\frac{s_i}{d_i} \\right\\} \\right)\n$$\n\n现在，我们将此公式应用于给定的三维数据：\n- $x = (4, 1.5, 6)$，所以 $x_1=4$, $x_2=1.5$, $x_3=6$。\n- $u = (5, 2, 7)$，所以 $u_1=5$, $u_2=2$, $u_3=7$。\n- $d = (-3, -1, 2)$，所以 $d_1=-3$, $d_2=-1$, $d_3=2$。\n\n首先，我们计算松弛变量 $s = u - x$：\n- $s_1 = u_1 - x_1 = 5 - 4 = 1$。\n- $s_2 = u_2 - x_2 = 2 - 1.5 = 0.5$。\n- $s_3 = u_3 - x_3 = 7 - 6 = 1$。\n\n接下来，我们为每个提供边界的分量计算步长限制。\n$d_i  0$ 的索引集合是 $\\{1, 2\\}$。\n- 对于 $i=1$：限制是 $\\frac{x_1}{-d_1} = \\frac{4}{-(-3)} = \\frac{4}{3}$。\n- 对于 $i=2$：限制是 $\\frac{x_2}{-d_2} = \\frac{1.5}{-(-1)} = 1.5 = \\frac{3}{2}$。\n\n$d_i > 0$ 的索引集合是 $\\{3\\}$。\n- 对于 $i=3$：限制是 $\\frac{s_3}{d_3} = \\frac{1}{2}$。\n\n现在，我们通过取这些计算出的限制值的最小值来找到 $\\alpha_{\\max}$：\n$$\n\\alpha_{\\max} = \\min\\left(\\frac{4}{3}, \\frac{3}{2}, \\frac{1}{2}\\right)\n$$\n比较这些值：$\\frac{4}{3} \\approx 1.333\\dots$，$\\frac{3}{2} = 1.5$，以及 $\\frac{1}{2} = 0.5$。最小值是 $\\frac{1}{2}$。\n$$\n\\alpha_{\\max} = 0.5\n$$\n\n最后，我们使用给定参数 $\\tau = 0.8$ 应用到边界分数规则：\n$$\n\\alpha_{\\text{safe}} = \\tau \\alpha_{\\max}\n$$\n代入数值：\n$$\n\\alpha_{\\text{safe}} = 0.8 \\times 0.5 = 0.4\n$$\n结果是一个精确的小数值。",
            "answer": "$$\\boxed{0.4}$$"
        },
        {
            "introduction": "在机器学习和信号处理中，迭代算法的收敛速度至关重要。预测-校正模式不仅能处理约束，还能用来“加速”收敛，著名的FISTA算法就是典型例子。这个练习要求你为一个经典的稀疏优化问题（Lasso）设计并实现一个结合了ISTA（作为预测器）和类FISTA步骤（作为校正器）的算法。通过编码和测试，你将探索校正步骤中的“动量”项如何帮助算法更快地收敛，并理解在何种条件下这种加速是有效且不会导致“过冲”的。",
            "id": "3163759",
            "problem": "考虑被称为最小绝对收缩和选择算子 (Lasso) 的凸优化问题，其目标函数定义为\n$$\nF(x) \\;=\\; \\phi(x) \\;+\\; \\lambda \\,\\|x\\|_1,\\qquad \\phi(x) \\;=\\; \\frac{1}{2}\\,\\|A x - b\\|_2^2,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，$\\lambda  0$ 且 $x \\in \\mathbb{R}^n$。光滑部分 $\\phi(x)$ 是可微的，其梯度为\n$$\n\\nabla \\phi(x) \\;=\\; A^\\top (A x - b),\n$$\n该梯度在谱范数下是利普希茨连续的，利普希茨常数 $L \\ge \\|A^\\top A\\|_2$。非光滑部分是按比例缩放的 $\\ell_1$ 范数，其近端算子是软阈值（收缩）映射\n$$\n\\operatorname{shrink}(z, \\tau)_i \\;=\\; \\operatorname{sign}(z_i)\\,\\max\\!\\big(|z_i| - \\tau,\\, 0\\big), \\quad \\text{对每个坐标 } i,\n$$\n它等于 $\\tau \\,\\|x\\|_1$ 在点 $z$ 处的近端算子。\n\n您将设计一个用于最小化 $F(x)$ 的预测-校正方案，该方案从当前迭代点 $x_k$ 和前一个迭代点 $x_{k-1}$ 开始。预测器是一个迭代收缩阈值算法 (ISTA) 步骤，而校正器是一个快速迭代收缩阈值算法 (FISTA) 步骤。您的任务：\n\n1) 预测器 (ISTA)：使用步长 $s \\in (0, 1/L]$，计算\n$$\nx_{\\mathrm{p}} \\;=\\; \\operatorname{shrink}\\!\\big(x_k - s\\,\\nabla \\phi(x_k),\\; s\\,\\lambda\\big).\n$$\n\n2) 校正器（一个 FISTA 步骤）：使用由序列 $t_0 = 1$ 和以下公式定义的经典 FISTA 动量参数\n$$\nt_{j} \\;=\\; \\frac{1 + \\sqrt{1 + 4\\,t_{j-1}^2}}{2},\\quad \\text{对 } j \\ge 1,\n$$\n构建单步加速系数\n$$\n\\beta \\;=\\; \\frac{t_{1} - 1}{t_{2}},\n$$\n并计算外推点\n$$\ny \\;=\\; x_{\\mathrm{p}} + \\beta\\,(x_{\\mathrm{p}} - x_{k-1}),\n$$\n随后在点 $y$ 处进行一次近端梯度更新：\n$$\nx_{\\mathrm{c}} \\;=\\; \\operatorname{shrink}\\!\\big(y - s\\,\\nabla \\phi(y),\\; s\\,\\lambda\\big).\n$$\n\n3) 测试条件：当以下两个条件都成立时，我们称校正器在没有过冲的情况下实现了加速：\n- 加速：$F(x_{\\mathrm{c}})  F(x_{\\mathrm{p}})$。\n- 无过冲：$F(x_{\\mathrm{c}}) \\le F(x_k)$。\n由于有限精度，请采用非负容差 $\\varepsilon = 10^{-10}$，并将 $a \\le b$ 解释为 $a \\le b + \\varepsilon$，将 $a  b$ 解释为 $a \\le b - \\varepsilon$。\n\n4) 初始化协议：对于给定的测试用例 $(A, b, \\lambda, \\alpha)$，使用 $A$ 的谱范数估计 $L$ 为 $L = \\|A\\|_2^2$，并设置 $s = \\alpha/L$，其中 $\\alpha \\in (0, 1]$。从 $x_0 = 0$ 开始初始化。计算两个预热 ISTA 步骤\n$$\nx_1 \\;=\\; \\operatorname{shrink}\\!\\big(x_0 - s\\,\\nabla \\phi(x_0),\\; s\\,\\lambda\\big),\\qquad\nx_2 \\;=\\; \\operatorname{shrink}\\!\\big(x_1 - s\\,\\nabla \\phi(x_1),\\; s\\,\\lambda\\big).\n$$\n然后在 $k = 2$ 处应用预测-校正方案，使用 $(x_{k-1}, x_k) = (x_1, x_2)$ 来获得 $x_{\\mathrm{p}}$ 和 $x_{\\mathrm{c}}$，并评估第 3 项中的两个不等式。\n\n5) 测试套件：使用以下四个测试用例。每个用例使用 $m = 2$ 和 $n = 2$，以及 $b = [1, 0]^\\top$。矩阵 $A$ 是对称的，通过相关系数 $\\rho$ 定义为\n$$\nA = \\begin{bmatrix} 1  \\rho \\\\ \\rho  1 \\end{bmatrix}.\n$$\n为每个用例评估指示“在没有过冲的情况下实现了加速”的布尔值。\n- 用例 1（理想路径）：$\\rho = 0.3$, $\\lambda = 0.1$, $\\alpha = 1.0$。\n- 用例 2（病态挑战）：$\\rho = 0.99$, $\\lambda = 0.001$, $\\alpha = 1.0$。\n- 用例 3（强正则化）：$\\rho = 0.3$, $\\lambda = 0.6$, $\\alpha = 1.0$。\n- 用例 4（边界解耦情况）：$\\rho = 0$, $\\lambda = 0.1$, $\\alpha = 1.0$。\n\n6) 要求的最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。该列表必须包含与上述四个用例按顺序列出的相对应的四个布尔值。例如，输出格式必须如下所示\n$$\n[\\text{true}_1,\\text{true}_2,\\text{true}_3,\\text{true}_4]\n$$\n但使用 Python 的布尔值大写形式，即与 $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True}]$ 完全一样（此示例仅为说明性的；您必须输出指定测试用例的实际计算结果）。",
            "solution": "用户提供了一个在数值优化领域中定义明确的计算问题。我将首先验证问题陈述，然后提供一个全面的解决方案。\n\n### 问题验证\n\n1.  **提取已知条件**：\n    *   **目标函数**：$F(x) = \\phi(x) + \\lambda \\|x\\|_1$，其中 $\\phi(x) = \\frac{1}{2}\\|Ax - b\\|_2^2$。\n    *   **数据**：$A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，$\\lambda  0$，$x \\in \\mathbb{R}^n$。\n    *   **梯度**：$\\nabla \\phi(x) = A^\\top (Ax - b)$。\n    *   **$\\nabla\\phi$ 的利普希茨常数**：$L \\ge \\|A^\\top A\\|_2$。\n    *   **近端算子**：$\\operatorname{shrink}(z, \\tau)_i = \\operatorname{sign}(z_i)\\max(|z_i| - \\tau, 0)$。\n    *   **预测器步骤 (ISTA)**：$x_{\\mathrm{p}} = \\operatorname{shrink}(x_k - s\\nabla \\phi(x_k), s\\lambda)$，步长 $s \\in (0, 1/L]$。\n    *   **校正器步骤 (类 FISTA)**：\n        *   动量参数：$t_0 = 1$，$t_{j} = (1 + \\sqrt{1 + 4t_{j-1}^2})/2$，对于 $j \\ge 1$。\n        *   加速系数：$\\beta = (t_1 - 1)/t_2$。\n        *   外推：$y = x_{\\mathrm{p}} + \\beta(x_{\\mathrm{p}} - x_{k-1})$。\n        *   校正：$x_{\\mathrm{c}} = \\operatorname{shrink}(y - s\\nabla \\phi(y), s\\lambda)$。\n    *   **评估条件**：“在没有过冲的情况下实现了加速”成立，如果 $F(x_{\\mathrm{c}})  F(x_{\\mathrm{p}})$ 和 $F(x_{\\mathrm{c}}) \\le F(x_k)$ 均为真，使用容差 $\\varepsilon = 10^{-10}$ 解释，即 $a  b$ 为 $a \\le b - \\varepsilon$，$a \\le b$ 为 $a \\le b + \\varepsilon$。\n    *   **初始化**：对于一个用例 $(A, b, \\lambda, \\alpha)$，设置 $L = \\|A\\|_2^2$，$s = \\alpha/L$。从 $x_0 = 0$ 开始。计算 2 个预热 ISTA 步骤以获得 $x_1$ 和 $x_2$。\n    *   **测试执行**：在迭代 $k=2$ 时应用预测-校正方案，其中当前迭代点 $x_k=x_2$，前一个迭代点 $x_{k-1}=x_1$。\n    *   **测试套件**：$m=2$，$n=2$，$b = [1, 0]^\\top$，$A = \\begin{bmatrix} 1  \\rho \\\\ \\rho  1 \\end{bmatrix}$。指定了四个具有不同 $(\\rho, \\lambda, \\alpha)$ 值的用例。\n\n2.  **使用提取的已知条件进行验证**：\n    *   **科学依据**：该问题在凸优化理论和一阶方法中有充分的理论基础。Lasso 目标、ISTA 和 FISTA 都是标准主题。这个特定的预测-校正方案是基于这些已建立算法的一个有效（尽管是自定义）的构造。选择利普希茨常数 $L = \\|A\\|_2^2$ 是正确的，因为对于对称矩阵 $A$，$\\|A^\\top A\\|_2 = \\|A^2\\|_2 = \\|A\\|_2^2$。对于一般矩阵，$\\|A^\\top A\\|_2 = \\|A\\|_2^2$ 也成立。\n    *   **适定性与完整性**：该问题是确定性的，并提供了所有必要的信息：函数、算法步骤、初始条件、所有测试用例的参数值，以及包括数值容差在内的精确评估标准。它没有歧义且内容自洽。\n    *   **客观性与可行性**：该问题使用客观的数学语言陈述。测试用例涉及小型（$2 \\times 2$）矩阵，使得计算完全可行。\n\n3.  **结论与行动**：问题陈述是有效的。这是一个清晰、一致且计算上合理的任务。我将继续进行求解。\n\n### 求解推导\n\n该问题要求实现并评估一个用于解决 Lasso 优化问题的特定预测-校正算法。解决方案将通过精确遵循所提供的步骤来构建。\n\n**1. Lasso 问题与近端梯度方法**\n\n目标是最小化函数 $F(x) = \\phi(x) + g(x)$，其中 $\\phi(x) = \\frac{1}{2}\\|Ax - b\\|_2^2$ 是一个光滑、凸、可微的函数，而 $g(x) = \\lambda \\|x\\|_1$ 是一个凸但不可微的正则化项。\n\n这种结构的问题可以通过近端梯度方法有效解决。其核心思想是迭代地对光滑部分 $\\phi(x)$ 执行梯度下降步骤，然后应用非光滑部分 $g(x)$ 的近端算子来校正该步骤。通用的更新规则是：\n$$\nx_{k+1} = \\operatorname{prox}_{sg}(x_k - s\\nabla \\phi(x_k))\n$$\n其中 $s  0$ 是步长。对于 $g(x) = \\lambda\\|x\\|_1$，近端算子 $\\operatorname{prox}_{s\\lambda\\|\\cdot\\|_1}(z)$ 是软阈值函数 $\\operatorname{shrink}(z, s\\lambda)$，其按元素定义为：\n$$\n\\operatorname{shrink}(z, \\tau)_i = \\operatorname{sign}(z_i)\\max(|z_i| - \\tau, 0)\n$$\n这种迭代方案被称为迭代收缩阈值算法 (ISTA)。为保证收敛，步长 $s$ 必须在区间 $(0, 1/L]$ 内，其中 $L$ 是 $\\nabla\\phi(x)$ 的利普希茨常数。\n\n**2. 指定的预测-校正方案**\n\n我们必须实现的算法是一个混合方案。它从一个当前迭代点 $x_k$ 和一个前一个迭代点 $x_{k-1}$ 开始。\n\n*   **初始化**：我们从 $x_0=0$ 开始。为了获得一个合理的起始点对 $(x_1, x_2)$，我们执行 2 个“预热”ISTA 步骤。步长 $s$ 固定为 $s = \\alpha/L$，其中 $L = \\|A\\|_2^2$ 是 $\\nabla\\phi$ 的利普希茨常数。对于给定的测试用例，$\\alpha=1.0$，因此我们使用完整步长 $s=1/L$。\n    $$\n    x_1 = \\operatorname{shrink}(x_0 - s\\nabla \\phi(x_0), s\\lambda)\n    $$\n    $$\n    x_2 = \\operatorname{shrink}(x_1 - s\\nabla \\phi(x_1), s\\lambda)\n    $$\n    这些迭代点 $x_1$ 和 $x_2$ 将作为主算法的初始状态 $(x_{k-1}, x_k)$，其中 $k=2$。\n\n*   **预测器**：预测器步骤是从 $x_k = x_2$ 开始的单次 ISTA 应用：\n    $$\n    x_{\\mathrm{p}} = \\operatorname{shrink}(x_k - s\\nabla \\phi(x_k), s\\lambda)\n    $$\n    这将产生一个候选解 $x_{\\mathrm{p}}$，它在标准的 ISTA 序列中将是下一个迭代点 $x_3$。根据构造，近端梯度下降保证了目标函数值的下降，即 $F(x_{\\mathrm{p}}) \\le F(x_k)$。\n\n*   **校正器**：校正器步骤旨在通过引入动量来改进预测器，这借鉴了快速迭代收缩阈值算法 (FISTA) 的思想。FISTA 通过在一个外推点上执行近端梯度步骤来加速收敛。在这里，外推定义为：\n    $$\n    y = x_{\\mathrm{p}} + \\beta(x_{\\mathrm{p}} - x_{k-1})\n    $$\n    其中 $x_{k-1} = x_1$。系数 $\\beta$ 是一个固定的单步动量参数，源自 FISTA 的递推关系：\n    $$\n    t_0 = 1, \\quad t_1 = \\frac{1 + \\sqrt{1 + 4t_0^2}}{2} = \\frac{1 + \\sqrt{5}}{2}, \\quad t_2 = \\frac{1 + \\sqrt{1 + 4t_1^2}}{2}\n    $$\n    $$\n    \\beta = \\frac{t_1 - 1}{t_2}\n    $$\n    然后，通过在此外推点 $y$ 上应用近端梯度更新来获得校正器迭代点 $x_{\\mathrm{c}}$：\n    $$\n    x_{\\mathrm{c}} = \\operatorname{shrink}(y - s\\nabla \\phi(y), s\\lambda)\n    $$\n\n**3. 评估逻辑**\n\n校正器的性能由两个条件来判断，这两个条件必须同时满足：\n1.  **加速**：$F(x_{\\mathrm{c}})  F(x_{\\mathrm{p}})$。此条件检查校正器步骤是否确实比简单的预测器步骤有所改进。严格不等式很重要。\n2.  **无过冲**：$F(x_{\\mathrm{c}}) \\le F(x_k)$。加速方法中的动量项可能导致振荡，即迭代点可能会暂时增加目标函数值。此条件确保校正器步骤产生的结果不会比起始迭代点 $x_k$ 的结果更差。\n\n数值实现将使用 $\\varepsilon = 10^{-10}$ 的容差，以使这些比较对浮点不精确性具有鲁棒性。\n\n**4. 在测试用例上执行**\n\n该过程应用于 4 个指定的测试用例中的每一个。对于每个由参数 $(\\rho, \\lambda, \\alpha)$ 表征的用例：\n1.  构建矩阵 $A$ 和向量 $b$。\n2.  计算利普希茨常数 $L = \\|A\\|_2^2$ 和步长 $s = \\alpha/L$。\n3.  从 $x_0=0$ 计算预热迭代点 $x_1$ 和 $x_2$。\n4.  从 $x_k=x_2$ 计算预测器 $x_{\\mathrm{p}}$。\n5.  计算动量系数 $\\beta$，并使用从 $(x_{\\mathrm{p}}, x_1)$ 的外推来计算校正器 $x_{\\mathrm{c}}$。\n6.  计算目标函数值 $F(x_2)$、$F(x_{\\mathrm{p}})$ 和 $F(x_{\\mathrm{c}})$。\n7.  评估“在没有过冲的情况下实现了加速”的两个条件，其结果的逻辑与运算决定了该用例的最终布尔输出。\n\n这个完整的、确定性的过程将在提供的 Python 脚本中编码。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a predictor-corrector scheme for the Lasso problem\n    on a suite of four test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (rho, lambda, alpha)\n    test_cases = [\n        (0.3, 0.1, 1.0),    # Case 1: happy path\n        (0.99, 0.001, 1.0), # Case 2: ill-conditioned challenge\n        (0.3, 0.6, 1.0),    # Case 3: strong regularization\n        (0.0, 0.1, 1.0),    # Case 4: boundary decoupled case\n    ]\n\n    results = []\n    \n    # Define helper functions based on the problem statement\n    \n    def objective_F(x, A, b, lambda_val):\n        \"\"\"Computes the Lasso objective function F(x).\"\"\"\n        smooth_part = 0.5 * np.linalg.norm(A @ x - b)**2\n        nonsmooth_part = lambda_val * np.linalg.norm(x, 1)\n        return smooth_part + nonsmooth_part\n\n    def grad_phi(x, A, b):\n        \"\"\"Computes the gradient of the smooth part phi(x).\"\"\"\n        return A.T @ (A @ x - b)\n\n    def shrink(z, tau):\n        \"\"\"Applies the soft-thresholding operator element-wise.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n    for case_params in test_cases:\n        rho, lambda_val, alpha = case_params\n        \n        # Setup for the test case\n        m, n = 2, 2\n        b = np.array([1.0, 0.0])\n        A = np.array([[1.0, rho], [rho, 1.0]])\n        \n        # Initialization protocol\n        # L must be >= ||A^T A||_2. We use L = ||A||_2^2 which is equal to ||A^T A||_2 for symmetric A.\n        L = np.linalg.norm(A, 2)**2\n        s = alpha / L\n        epsilon = 1e-10\n\n        # Initial point\n        x0 = np.zeros(n)\n\n        # Two warm-up ISTA steps\n        x1 = shrink(x0 - s * grad_phi(x0, A, b), s * lambda_val)\n        x2 = shrink(x1 - s * grad_phi(x1, A, b), s * lambda_val)\n        \n        # Set iterates for the main step (k=2)\n        xk_minus_1 = x1\n        xk = x2\n\n        # 1) Predictor (ISTA step)\n        xp = shrink(xk - s * grad_phi(xk, A, b), s * lambda_val)\n\n        # 2) Corrector (one FISTA step)\n        # Compute momentum coefficient beta\n        t0 = 1.0\n        t1 = (1.0 + np.sqrt(1.0 + 4.0 * t0**2)) / 2.0\n        t2 = (1.0 + np.sqrt(1.0 + 4.0 * t1**2)) / 2.0\n        beta = (t1 - 1.0) / t2\n        \n        # Extrapolate\n        y = xp + beta * (xp - xk_minus_1)\n        \n        # Proximal-gradient update at y\n        xc = shrink(y - s * grad_phi(y, A, b), s * lambda_val)\n        \n        # 3) Evaluate conditions\n        F_xk = objective_F(xk, A, b, lambda_val)\n        F_xp = objective_F(xp, A, b, lambda_val)\n        F_xc = objective_F(xc, A, b, lambda_val)\n\n        # Condition: Acceleration\n        accelerates = (F_xc = F_xp - epsilon)\n        \n        # Condition: No overshoot\n        no_overshoot = (F_xc = F_xk + epsilon)\n\n        # Final boolean result for the case\n        case_result = accelerates and no_overshoot\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}