{
    "hands_on_practices": [
        {
            "introduction": "The theoretical \"order\" of a numerical method, denoted by $p$, describes how its global error $E$ scales with the step size $h$, typically as $E \\propto h^p$. This practice provides a hands-on method to empirically verify this order by running a simulation at two different step sizes, $h$ and $2h$, and analyzing the ratio of the resulting errors. This technique, known as a step-doubling experiment, is a fundamental tool for validating the implementation of any numerical solver. ",
            "id": "3156057",
            "problem": "Consider the initial value problem (IVP) for an Ordinary Differential Equation (ODE) defined by $y^{\\prime}(t) = y(t)$ with $y(0) = 1$. The exact solution is $y(t) = e^{t}$. A one-step ODE solver advances the numerical state via a mapping $y_{n+1} = \\Phi(t_{n}, y_{n}, h)$ for a step size $h$, where $t_{n+1} = t_{n} + h$. A method has order $p$ if, for sufficiently small $h$, its global truncation error at a fixed final time $T$ scales as a power of $h$. The local truncation error is the discrepancy incurred in one step when injecting the exact solution into the numerical update; the global truncation error is the accumulated discrepancy at $t = T$.\n\nYour task is to construct a step-doubling experiment to estimate the order $p$ of a black-box ODE solver by measuring the global truncation errors $E(h)$ and $E(2h)$ when solving $y^{\\prime} = y$ with the exact solution $y = e^{t}$. The experiment must be grounded in first principles: use the definition of local and global truncation error and reason about how the global error changes as $h$ is scaled, without assuming any target formula in advance. Implement two black-box one-step solvers to serve as the unknown methods: a forward Euler solver and a Heun (explicit trapezoidal) solver. Treat each solver as a black-box function that, given $f$, $y_{0}$, $t_{0}$, $T$, and $h$, returns the numerical approximation $y_{N}$ at $t = T$ using fixed steps, where $N = T/h$ is an integer.\n\nFor each parameter set, compute the numerical solution at $t = T$ with step sizes $h$ and $2h$; measure the corresponding global truncation errors $E(h) = \\lvert y_{\\text{num}}(T; h) - e^{T} \\rvert$ and $E(2h) = \\lvert y_{\\text{num}}(T; 2h) - e^{T} \\rvert$; and, using the fundamental scaling relationship implied by the definition of method order, deduce an estimate of $p$. Do not use any physical units in this problem. Angles are not involved. Express each estimated $p$ as a floating-point number rounded to six decimal places.\n\nUse the following test suite of parameter values, each specified as a tuple $(\\text{solver}, T, h)$ with $T/h \\in \\mathbb{N}$ and $T/(2h) \\in \\mathbb{N}$:\n- $(\\text{Euler}, 1.0, 0.1)$ with $T = 1.0$ and $h = 0.1$,\n- $(\\text{Heun}, 1.0, 0.1)$ with $T = 1.0$ and $h = 0.1$,\n- $(\\text{Euler}, 2.0, 0.05)$ with $T = 2.0$ and $h = 0.05$,\n- $(\\text{Heun}, 2.0, 0.05)$ with $T = 2.0$ and $h = 0.05$,\n- $(\\text{Euler}, 1.0, 0.5)$ with $T = 1.0$ and $h = 0.5$,\n- $(\\text{Heun}, 1.0, 0.001)$ with $T = 1.0$ and $h = 0.001$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order given above; for example, output of the form [$p_{1},$ $p_{2},$ $p_{3},$ $p_{4},$ $p_{5},$ $p_{6}$], with each $p_{i}$ rounded to six decimal places.",
            "solution": "The user has provided a valid, well-posed problem statement from the field of computational science. The problem is scientifically grounded in the theory of numerical methods for ordinary differential equations (ODEs), is self-contained, and has a clear, verifiable objective.\n\nThe task is to estimate the order of convergence, $p$, for two one-step ODE solvers by performing a step-doubling experiment. This requires deriving an estimator for $p$ from first principles and then applying it to a specific initial value problem (IVP).\n\n### Principle and Derivation of the Order Estimator\n\nA numerical method for solving an ODE is said to have an order of convergence $p$ if its global truncation error, $E$, at a fixed final time $T$, scales with the step size $h$ according to the relation:\n$$\nE(h) = \\lvert y_{\\text{num}}(T; h) - y(T) \\rvert \\approx C h^p\n$$\nfor sufficiently small $h$. Here, $y_{\\text{num}}(T; h)$ is the numerical solution at time $T$ obtained using step size $h$, $y(T)$ is the exact solution, and $C$ is a constant that depends on the ODE, its solution, the final time $T$, and the specific method, but is assumed to be independent of $h$.\n\nTo estimate $p$ without knowing the constant $C$, we can compute the error using two different step sizes. The problem specifies a step-doubling experiment, so we use step sizes $h$ and $2h$. According to the scaling relationship, the errors are:\n$$\nE(h) \\approx C h^p \\quad (1)\n$$\n$$\nE(2h) \\approx C (2h)^p = C 2^p h^p \\quad (2)\n$$\n\nTo eliminate the unknown constant $C$, we can take the ratio of equation $(2)$ to equation $(1)$:\n$$\n\\frac{E(2h)}{E(h)} \\approx \\frac{C 2^p h^p}{C h^p} = 2^p\n$$\n\nThis relationship provides a direct way to estimate $p$. By taking the base-$2$ logarithm of both sides, we isolate $p$:\n$$\n\\log_2\\left(\\frac{E(2h)}{E(h)}\\right) \\approx \\log_2(2^p) = p\n$$\n\nThus, our estimator for the order of the method is:\n$$\np \\approx \\log_2\\left(\\frac{E(2h)}{E(h)}\\right)\n$$\nThis formula is derived directly from the definition of the method's order and forms the basis of our numerical experiment.\n\n### Implementation of the Numerical Experiment\n\nThe experiment will be conducted on the following IVP:\n- **ODE:** $y'(t) = y(t)$\n- **Initial Condition:** $y(0) = 1$\n- **Exact Solution:** $y(t) = e^t$\n\nThe right-hand side of the ODE is given by the function $f(t, y) = y$.\n\nWe will implement two \"black-box\" one-step solvers:\n\n1.  **Forward Euler Method:** This is an explicit first-order method. The update rule for advancing from step $n$ to $n+1$ is:\n    $$\n    y_{n+1} = y_n + h f(t_n, y_n)\n    $$\n    For our specific IVP, this becomes $y_{n+1} = y_n + h y_n = y_n(1+h)$.\n\n2.  **Heun's Method (Explicit Trapezoidal Rule):** This is an explicit second-order method that belongs to the family of predictor-corrector methods or Runge-Kutta methods. The update rule is:\n    $$\n    k_1 = f(t_n, y_n)\n    $$\n    $$\n    k_2 = f(t_n + h, y_n + h k_1)\n    $$\n    $$\n    y_{n+1} = y_n + \\frac{h}{2}(k_1 + k_2)\n    $$\n    This can be viewed as taking an initial Euler step (the predictor) and then averaging the slope at the beginning and the predicted end of the interval (the corrector).\n\nFor each test case $(\\text{solver}, T, h)$ specified in the problem statement, the following algorithm is executed:\n\n1.  Set the initial conditions $y_0 = 1$ and $t_0 = 0$.\n2.  Compute the numerical solution $y_{\\text{num}}(T; h)$ by applying the chosen solver for $N_1 = T/h$ steps of size $h$.\n3.  Compute the numerical solution $y_{\\text{num}}(T; 2h)$ by applying the same solver for $N_2 = T/(2h)$ steps of size $2h$.\n4.  Calculate the exact solution at the final time, $y(T) = e^T$.\n5.  Determine the global truncation errors:\n    $$\n    E(h) = \\lvert y_{\\text{num}}(T; h) - e^T \\rvert\n    $$\n    $$\n    E(2h) = \\lvert y_{\\text{num}}(T; 2h) - e^T \\rvert\n    $$\n6.  Finally, compute the estimated order $p$ using the derived formula:\n    $$\n    p = \\log_2\\left(\\frac{E(2h)}{E(h)}\\right)\n    $$\nThe resulting value of $p$ is rounded to six decimal places as required. This procedure is repeated for all test cases. The Forward Euler method is expected to yield $p \\approx 1$, and Heun's method should yield $p \\approx 2$, with the accuracy of the estimate depending on how well the asymptotic error behavior is established for the given step size $h$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the estimated order of convergence for Euler and Heun methods\n    using a step-doubling experiment based on a set of test cases.\n    \"\"\"\n\n    def ode_f(t, y):\n        \"\"\"The right-hand side of the ODE y' = y.\"\"\"\n        return y\n\n    def forward_euler_solver(f, y0, t0, T, h):\n        \"\"\"\n        Solves an IVP using the Forward Euler method.\n        \n        Args:\n            f: The function f(t, y) for the ODE y' = f(t, y).\n            y0: The initial value y(t0).\n            t0: The initial time.\n            T: The final time.\n            h: The step size.\n        \n        Returns:\n            The numerical solution at time T.\n        \"\"\"\n        y = y0\n        t = t0\n        num_steps = int(np.round((T - t0) / h))\n        \n        for _ in range(num_steps):\n            y = y + h * f(t, y)\n            t = t + h\n            \n        return y\n\n    def heun_solver(f, y0, t0, T, h):\n        \"\"\"\n        Solves an IVP using Heun's method (explicit trapezoidal rule).\n        \n        Args:\n            f: The function f(t, y) for the ODE y' = f(t, y).\n            y0: The initial value y(t0).\n            t0: The initial time.\n            T: The final time.\n            h: The step size.\n        \n        Returns:\n            The numerical solution at time T.\n        \"\"\"\n        y = y0\n        t = t0\n        num_steps = int(np.round((T - t0) / h))\n        \n        for _ in range(num_steps):\n            k1 = f(t, y)\n            k2 = f(t + h, y + h * k1)\n            y = y + (h / 2.0) * (k1 + k2)\n            t = t + h\n            \n        return y\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\"Euler\", 1.0, 0.1),\n        (\"Heun\", 1.0, 0.1),\n        (\"Euler\", 2.0, 0.05),\n        (\"Heun\", 2.0, 0.05),\n        (\"Euler\", 1.0, 0.5),\n        (\"Heun\", 1.0, 0.001),\n    ]\n\n    solvers = {\n        \"Euler\": forward_euler_solver,\n        \"Heun\": heun_solver,\n    }\n\n    results = []\n    \n    for solver_name, T, h in test_cases:\n        solver_func = solvers[solver_name]\n        \n        # Initial conditions for the IVP y'=y, y(0)=1\n        y0 = 1.0\n        t0 = 0.0\n        \n        # Define the two step sizes for the experiment\n        h1 = h\n        h2 = 2.0 * h\n        \n        # Compute numerical solutions for both step sizes\n        y_num_h1 = solver_func(ode_f, y0, t0, T, h1)\n        y_num_h2 = solver_func(ode_f, y0, t0, T, h2)\n        \n        # Compute the exact solution at T\n        y_exact = np.exp(T)\n        \n        # Compute the global truncation errors\n        error_h1 = np.abs(y_num_h1 - y_exact)\n        error_h2 = np.abs(y_num_h2 - y_exact)\n        \n        # Estimate the order p\n        # p is approximately log2(E(2h) / E(h))\n        # Handle cases where error might be zero to avoid division by zero\n        if error_h1 == 0.0:\n            # If the smaller step size gives zero error, the order cannot be determined\n            # by this method. This is unlikely for these methods and ODE.\n            # We can represent this as infinity or NaN, but for the purpose of this\n            # problem, we assume non-zero error.\n            # In a real-world scenario, a different approach would be needed.\n            # Given the problem context, this case is not expected to occur.\n            p = np.nan \n        else:\n            p = np.log2(error_h2 / error_h1)\n        \n        results.append(p)\n\n    # Format results to six decimal places and create the final output string\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Understanding the structure of the global truncation error allows us not only to analyze it, but also to actively cancel its dominant term. This practice introduces Richardson Extrapolation, a powerful technique that combines two lower-accuracy solutions to produce a single, higher-accuracy one. By applying this to the simple Euler method, you will see how a first-order method can be leveraged to achieve second-order accuracy, a foundational concept in constructing more sophisticated solvers. ",
            "id": "2409197",
            "problem": "Implement a complete program that, for a set of initial value problems of the form $\\dfrac{dy}{dt} = f(t,y)$ with $y(0) = y_0$, does the following: starting from the definitions of local truncation error and global truncation error, derive a Richardson extrapolation combination that cancels the dominant global error term of the explicit Euler method by combining two Euler solutions computed with step sizes $h$ and $h/2$. The combination must be derived from first principles by assuming an asymptotic expansion of the global error in powers of $h$, and it must yield a new approximation whose global truncation error is of order $\\mathcal{O}(h^2)$.\n\nYou must base your derivation on the following foundational facts:\n- The explicit Euler method is defined by the recurrence $y_{n+1} = y_n + h f(t_n, y_n)$ for $t_n = t_0 + n h$.\n- The local truncation error is the one-step defect obtained by inserting the exact solution into a single method step, and for smooth $f$ it scales as $\\mathcal{O}(h^2)$.\n- The global truncation error at a fixed final time $T$ is the accumulated difference between the exact solution and the numerical solution after $N = T/h$ steps; for the explicit Euler method it scales as $\\mathcal{O}(h)$.\n\nYour program must implement:\n1. A function to compute the explicit Euler solution $y_h(T)$ at the final time $T$ with uniform step size $h$.\n2. A Richardson extrapolation combination that, using only the two Euler solutions computed with step sizes $h$ and $h/2$, produces an improved approximation $y_{\\mathrm{RE}}(T)$ whose global truncation error scales as $\\mathcal{O}(h^2)$.\n3. An empirical verification routine that also computes solutions with step $h/4$ so that you can estimate observed orders of convergence:\n   - For the explicit Euler method, estimate $p_{\\mathrm{E}} \\approx \\log_2\\!\\left(\\dfrac{|y_h(T) - y(T)|}{|y_{h/2}(T) - y(T)|}\\right)$.\n   - For the Richardson-extrapolated approximation, first form $y_{\\mathrm{RE}}(h) = \\mathrm{RE}(y_h, y_{h/2})$ and $y_{\\mathrm{RE}}(h/2) = \\mathrm{RE}(y_{h/2}, y_{h/4})$, then estimate $p_{\\mathrm{RE}} \\approx \\log_2\\!\\left(\\dfrac{|y_{\\mathrm{RE}}(h) - y(T)|}{|y_{\\mathrm{RE}}(h/2) - y(T)|}\\right)$.\n\nAngles must be interpreted in radians.\n\nTest Suite:\nFor each case below, compute $y_h(T)$, $y_{h/2}(T)$, $y_{h/4}(T)$, form $y_{\\mathrm{RE}}(h)$ and $y_{\\mathrm{RE}}(h/2)$, and then compute the absolute errors with respect to the exact solution $y(T)$.\n- Case A (exponential decay): $\\dfrac{dy}{dt} = -3 y$, $y(0) = 1$, $T = 1$, base step $h = 0.2$. Exact solution: $y(t) = e^{-3 t}$.\n- Case B (linear non-homogeneous): $\\dfrac{dy}{dt} = t + y$, $y(0) = 0$, $T = 1$, base step $h = 0.2$. Exact solution: $y(t) = e^{t} - (t + 1)$.\n- Case C (variable-coefficient growth): $\\dfrac{dy}{dt} = \\sin(t)\\, y$, $y(0) = 1$, $T = 1$, base step $h = 0.2$. Exact solution: $y(t) = \\exp\\!\\big(1 - \\cos(t)\\big)$.\n\nFor each case, your program must produce a list of five real numbers:\n- The absolute global error of Euler with step $h$, namely $E_h = |y_h(T) - y(T)|$.\n- The absolute global error of Euler with step $h/2$, namely $E_{h/2} = |y_{h/2}(T) - y(T)|$.\n- The absolute global error of the Richardson-extrapolated approximation built from $h$ and $h/2$, namely $E_{\\mathrm{RE}}(h) = |y_{\\mathrm{RE}}(h) - y(T)|$.\n- The observed order $p_{\\mathrm{E}}$ defined above.\n- The observed order $p_{\\mathrm{RE}}$ defined above.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the three cases, in the order Case A, Case B, Case C, as a comma-separated list of three lists, enclosed in a single pair of square brackets, with no spaces anywhere. Each real number must be printed in scientific notation with ten significant digits. For example, the line must look like:\n[[E_h_A,E_h2_A,E_RE_A,pE_A,pRE_A],[E_h_B,E_h2_B,E_RE_B,pE_B,pRE_B],[E_h_C,E_h2_C,E_RE_C,pE_C,pRE_C]]\nMake sure all trigonometric function arguments are in radians. No units are required since all quantities are dimensionless in this problem.",
            "solution": "The problem statement is subjected to validation.\n\nGivens are extracted verbatim:\n1.  Initial value problem: $\\dfrac{dy}{dt} = f(t,y)$ with $y(0) = y_0$.\n2.  Explicit Euler method recurrence: $y_{n+1} = y_n + h f(t_n, y_n)$ for $t_n = t_0 + n h$.\n3.  Local truncation error for explicit Euler scales as $\\mathcal{O}(h^2)$.\n4.  Global truncation error for explicit Euler at a fixed time $T$ scales as $\\mathcal{O}(h)$.\n5.  Task: Derive a Richardson extrapolation combination for two Euler solutions with step sizes $h$ and $h/2$ to cancel the dominant global error term, assuming an asymptotic expansion for the global error. The resulting approximation must have a global error of order $\\mathcal{O}(h^2)$.\n6.  Task: Implement an empirical verification using step sizes $h$, $h/2$, and $h/4$.\n7.  Observed order for Euler: $p_{\\mathrm{E}} \\approx \\log_2\\!\\left(\\dfrac{|y_h(T) - y(T)|}{|y_{h/2}(T) - y(T)|}\\right)$.\n8.  Observed order for Richardson extrapolation: $p_{\\mathrm{RE}} \\approx \\log_2\\!\\left(\\dfrac{|y_{\\mathrm{RE}}(h) - y(T)|}{|y_{\\mathrm{RE}}(h/2) - y(T)|}\\right)$, where $y_{\\mathrm{RE}}(h) = \\mathrm{RE}(y_h, y_{h/2})$ and $y_{\\mathrm{RE}}(h/2) = \\mathrm{RE}(y_{h/2}, y_{h/4})$.\n9.  Test Case A: $\\dfrac{dy}{dt} = -3 y$, $y(0) = 1$, $T = 1$, $h = 0.2$. Exact solution: $y(t) = e^{-3 t}$.\n10. Test Case B: $\\dfrac{dy}{dt} = t + y$, $y(0) = 0$, $T = 1$, $h = 0.2$. Exact solution: $y(t) = e^{t} - (t + 1)$.\n11. Test Case C: $\\dfrac{dy}{dt} = \\sin(t)\\, y$, $y(0) = 1$, $T = 1$, $h = 0.2$. Exact solution: $y(t) = \\exp\\!\\big(1 - \\cos(t)\\big)$.\n12. Required output for each case: A list of five numbers: $|y_h(T) - y(T)|$, $|y_{h/2}(T) - y(T)|$, $|y_{\\mathrm{RE}}(h) - y(T)|$, $p_{\\mathrm{E}}$, and $p_{\\mathrm{RE}}$.\n\nValidation using extracted givens:\nThe problem is scientifically grounded. It addresses Richardson extrapolation, a standard and fundamental technique in numerical analysis for improving the accuracy of numerical methods. The premises—the definition of the explicit Euler method and the orders of its local and global truncation errors—are standard results in the study of numerical solutions to ordinary differential equations. The problem is well-posed; it asks for a specific derivation and an implementation for well-defined test cases with unique analytical solutions, ensuring that a unique and meaningful result can be obtained and verified. The language is objective and precise. The problem does not violate any of the invalidity criteria. It is a formalizable problem directly relevant to computational physics and numerical methods.\n\nVerdict: The problem is valid. A solution will be provided.\n\nThe derivation of the Richardson extrapolation formula must be constructed from first principles, as stipulated. The foundation of this method is the existence of an asymptotic expansion for the global error of the numerical scheme. For the explicit Euler method, which has a global error of order $\\mathcal{O}(h)$, this expansion takes the form:\n$$\ny_h(T) = y(T) + C_1 h + C_2 h^2 + C_3 h^3 + \\dots\n$$\nHere, $y(T)$ is the exact solution at the final time $T$, $y_h(T)$ is the numerical approximation obtained with a step size $h$, and the coefficients $C_k$ are independent of $h$ but depend on the function $f(t,y)$ and its derivatives at various points. The dominant error term is $C_1 h$.\n\nOur objective is to eliminate this dominant term by combining two separate computations. Let us perform the numerical integration with step size $h$ and again with step size $h/2$. The asymptotic expansions for the respective numerical solutions are:\n$$\n(1) \\quad y_h(T) = y(T) + C_1 h + C_2 h^2 + \\mathcal{O}(h^3)\n$$\n$$\n(2) \\quad y_{h/2}(T) = y(T) + C_1 \\frac{h}{2} + C_2 \\left(\\frac{h}{2}\\right)^2 + \\mathcal{O}(h^3) = y(T) + \\frac{1}{2} C_1 h + \\frac{1}{4} C_2 h^2 + \\mathcal{O}(h^3)\n$$\nWe seek a linear combination of $y_h(T)$ and $y_{h/2}(T)$ that cancels the term proportional to $C_1 h$. Let the extrapolated approximation be $y_{\\mathrm{RE}}(T)$. To eliminate $C_1 h$, we can multiply equation $(2)$ by $2$ and subtract equation $(1)$:\n$$\n2 y_{h/2}(T) - y_h(T) = \\left(2y(T) + C_1 h + \\frac{1}{2} C_2 h^2 + \\dots\\right) - \\left(y(T) + C_1 h + C_2 h^2 + \\dots\\right)\n$$\n$$\n2 y_{h/2}(T) - y_h(T) = (2-1)y(T) + (1-1)C_1 h + \\left(\\frac{1}{2}-1\\right)C_2 h^2 + \\mathcal{O}(h^3)\n$$\n$$\n2 y_{h/2}(T) - y_h(T) = y(T) - \\frac{1}{2} C_2 h^2 + \\mathcal{O}(h^3)\n$$\nFrom this, we define the Richardson-extrapolated solution $y_{\\mathrm{RE}}(T)$ as:\n$$\ny_{\\mathrm{RE}}(T) = 2 y_{h/2}(T) - y_h(T)\n$$\nThe global truncation error of this new approximation is the difference $y_{\\mathrm{RE}}(T) - y(T)$. Based on our derivation:\n$$\ny_{\\mathrm{RE}}(T) - y(T) = -\\frac{1}{2} C_2 h^2 + \\mathcal{O}(h^3)\n$$\nThis confirms that the global truncation error is now of order $\\mathcal{O}(h^2)$, as required. The dominant error term of the original method has been successfully eliminated.\n\nThe computational implementation will consist of three main parts. First, a function `explicit_euler` will solve the given ODE from an initial time $t_0$ to a final time $T$ using a constant step size $h$. This function will implement the recurrence $y_{k+1} = y_k + h f(t_k, y_k)$, which is iterated $N = \\text{round}((T-t_0)/h)$ times.\n\nSecond, a main routine will execute the logic for each test case. For a base step size $h$, it will call the `explicit_euler` function three times with step sizes $h$, $h/2$, and $h/4$ to obtain the numerical solutions $y_h(T)$, $y_{h/2}(T)$, and $y_{h/4}(T)$.\n\nThird, this routine will use these numerical results to perform the empirical analysis. It will compute the Richardson-extrapolated approximations:\n$$\ny_{\\mathrm{RE}}(h) = 2 y_{h/2}(T) - y_h(T)\n$$\n$$\ny_{\\mathrm{RE}}(h/2) = 2 y_{h/4}(T) - y_{h/2}(T)\n$$\nThe absolute global errors are then calculated with respect to the known exact solution $y(T)$. Finally, the observed orders of convergence, $p_{\\mathrm{E}}$ and $p_{\\mathrm{RE}}$, are computed using the specified logarithmic formulas. These empirical orders should be approximately $1$ for the Euler method and $2$ for the Richardson-extrapolated method, thus verifying the theoretical improvement in accuracy. The final results for all test cases will be formatted and printed according to the specified format. Trigonometric function inputs will be processed in radians as is standard in scientific computation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Richardson extrapolation for the explicit Euler method to solve\n    several initial value problems and empirically verifies the order of convergence.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda t, y: -3.0 * y,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"h\": 0.2,\n            \"exact_y\": lambda t: np.exp(-3.0 * t),\n        },\n        {\n            \"f\": lambda t, y: t + y,\n            \"y0\": 0.0,\n            \"T\": 1.0,\n            \"h\": 0.2,\n            \"exact_y\": lambda t: np.exp(t) - (t + 1.0),\n        },\n        {\n            \"f\": lambda t, y: np.sin(t) * y,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"h\": 0.2,\n            \"exact_y\": lambda t: np.exp(1.0 - np.cos(t)),\n        },\n    ]\n\n    def explicit_euler(f_ode, y_start, t_start, t_end, step_size):\n        \"\"\"\n        Computes the solution of an ODE using the explicit Euler method.\n        \"\"\"\n        t = t_start\n        y = y_start\n        # Use round() to avoid floating-point inaccuracies in step counting\n        num_steps = int(round((t_end - t_start) / step_size))\n        \n        for _ in range(num_steps):\n            y = y + step_size * f_ode(t, y)\n            t = t + step_size\n        return y\n\n    all_results = []\n    for case in test_cases:\n        f = case[\"f\"]\n        y0 = case[\"y0\"]\n        T = case[\"T\"]\n        h_base = case[\"h\"]\n        exact_y_func = case[\"exact_y\"]\n\n        # Define the three step sizes\n        h = h_base\n        h_half = h_base / 2.0\n        h_quarter = h_base / 4.0\n\n        # Compute Euler solutions for each step size\n        y_h = explicit_euler(f, y0, 0.0, T, h)\n        y_h_half = explicit_euler(f, y0, 0.0, T, h_half)\n        y_h_quarter = explicit_euler(f, y0, 0.0, T, h_quarter)\n\n        # Compute the exact solution at the final time T\n        y_exact = exact_y_func(T)\n\n        # Compute Richardson-extrapolated approximations\n        y_re_h = 2.0 * y_h_half - y_h\n        y_re_h_half = 2.0 * y_h_quarter - y_h_half\n\n        # Compute absolute errors\n        E_h = np.abs(y_h - y_exact)\n        E_h_half = np.abs(y_h_half - y_exact)\n        E_re_h = np.abs(y_re_h - y_exact)\n        E_re_h_half = np.abs(y_re_h_half - y_exact)\n\n        # Compute observed orders of convergence\n        # Handle cases where error is zero to avoid division by zero or log(0)\n        p_E = np.log2(E_h / E_h_half) if E_h_half  0 else 0.0\n        p_RE = np.log2(E_re_h / E_re_h_half) if E_re_h_half  0 else 0.0\n\n        results = [E_h, E_h_half, E_re_h, p_E, p_RE]\n        all_results.append(results)\n\n    # Format the final output string as specified\n    formatted_cases = []\n    for case_res in all_results:\n        formatted_nums = [f\"{num:.10e}\" for num in case_res]\n        formatted_cases.append(f\"[{','.join(formatted_nums)}]\")\n    \n    final_output = f\"[{','.join(formatted_cases)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "A higher-order method is not always superior, especially when the step size is not sufficiently small, a key insight this exercise reveals. You will investigate a \"malicious\" but smooth ordinary differential equation where the error constant hidden within the $\\mathcal{O}(h^p)$ notation becomes critically large due to high-frequency oscillations. By deriving the local truncation error for the fourth-order Runge-Kutta method, you will discover why problems with large higher-order derivatives can challenge even sophisticated solvers, highlighting that the order of a method alone does not guarantee accuracy. ",
            "id": "2409196",
            "problem": "Consider the initial value problem for an ordinary differential equation (ODE): $y^{\\prime}(t) = f(t)$, with initial condition $y(0) = 0$. The goal is to analyze how local truncation error and global truncation error behave for a high-order explicit one-step method when $f(t)$ is smooth but has large higher derivatives. Use the classical fourth-order Runge–Kutta method (also called the $4$-stage, fourth-order explicit Runge–Kutta method) with step size $h$.\n\nFundamental bases to use:\n- Definition of local truncation error: for a one-step method advancing from time $t_0$ to $t_0 + h$, the local truncation error is $e_{\\mathrm{loc}}(t_0,h) = y(t_0 + h) - \\Phi(t_0,y(t_0),h)$, where $\\Phi$ denotes one numerical step produced by the method when initialized at the exact state $y(t_0)$.\n- Definition of global truncation error: for a fixed step size $h$ over a finite interval $[0,T]$ with $N = T/h$ steps, starting at the exact initial condition $y(0)$, the global truncation error at time $T$ is $e_{\\mathrm{glob}}(T,h) = y(T) - y_N$, where $y_N$ is the numerical approximation after $N$ steps.\n- Taylor expansion around $t_0$ for a smooth function $f(t)$.\n\nYour tasks:\n$1.$ Derive, from the Taylor expansion of $f(t)$ and the construction of the classical $4$-stage Runge–Kutta method, the leading-order expression for the local truncation error of a single step when the right-hand side depends on time only, i.e., $y^{\\prime}(t) = f(t)$ with $f \\in C^4$. Show that, in this case, a single Runge–Kutta step reduces to a quadrature of $f(t)$ over $[t_0,t_0+h]$, and obtain the leading-order term of $e_{\\mathrm{loc}}(t_0,h)$ in terms of derivatives of $f(t)$ at $t_0$.\n$2.$ Construct a smooth but “malicious” right-hand side $f(t)$ whose higher derivatives are large: let $f(t) = \\sin(\\Omega t)$, where $\\Omega$ is an angular frequency. Angles must be treated in radians. Explain, using your derivation, why large values of $\\Omega$ can lead to unexpectedly large local truncation error for fixed $h$, even with a high-order method.\n$3.$ Implement, for $y^{\\prime}(t) = \\sin(\\Omega t)$, the exact solution $y(t) = \\int_0^t \\sin(\\Omega s)\\,ds = \\frac{1 - \\cos(\\Omega t)}{\\Omega}$ for $\\Omega \\neq 0$, and $y(t) = 0$ for $\\Omega = 0$.\n$4.$ Implement one step of the classical $4$-stage Runge–Kutta method with step size $h$ for this ODE, and compute the local truncation error $e_{\\mathrm{loc}}(0,h)$ at $t_0 = 0$ by comparing a single numerical step to the exact $y(h)$, starting from $y(0)=0$.\n$5.$ Implement the numerical solution from $t = 0$ to $t = T$ using the same method and fixed step size $h$, and compute the global truncation error $e_{\\mathrm{glob}}(T,h)$ by comparing the final numerical value to the exact $y(T)$.\n\nAngle unit requirement:\n- All trigonometric arguments must be in radians.\n\nTest suite and parameter coverage:\nUse $T = 1$ and the following test cases, which vary frequency $\\Omega$ and step size $h$ to expose different regimes (happy path, increasing difficulty, and edge case):\n- Case $1$: $(\\Omega, h) = (0, 0.1)$.\n- Case $2$: $(\\Omega, h) = (50, 0.02)$.\n- Case $3$: $(\\Omega, h) = (200, 0.01)$.\n- Case $4$: $(\\Omega, h) = (1000, 0.005)$.\n\nFor each case, compute two floats:\n- The magnitude of the local truncation error for one step from $t_0 = 0$ to $t_0 + h$: $|e_{\\mathrm{loc}}(0,h)|$.\n- The magnitude of the global truncation error at $T = 1$: $|e_{\\mathrm{glob}}(1,h)|$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$[|e_{\\mathrm{loc}}|_{\\mathrm{case}\\,1},|e_{\\mathrm{glob}}|_{\\mathrm{case}\\,1},|e_{\\mathrm{loc}}|_{\\mathrm{case}\\,2},|e_{\\mathrm{glob}}|_{\\mathrm{case}\\,2},|e_{\\mathrm{loc}}|_{\\mathrm{case}\\,3},|e_{\\mathrm{glob}}|_{\\mathrm{case}\\,3},|e_{\\mathrm{loc}}|_{\\mathrm{case}\\,4},|e_{\\mathrm{glob}}|_{\\mathrm{case}\\,4}]$.\n\nAll numbers must be printed as decimal floats. No other text should be printed.",
            "solution": "The problem requires an analysis of the local and global truncation errors for the classical fourth-order Runge-Kutta method applied to the simple ordinary differential equation (ODE) $y'(t) = f(t)$ with the initial condition $y(0) = 0$. The particular focus is on how a smooth but highly oscillatory right-hand side, $f(t) = \\sin(\\Omega t)$, affects these errors.\n\nFirst, we must derive the expression for the local truncation error. The classical fourth-order Runge-Kutta (RK4) method for a general ODE $y'(t) = F(t, y(t))$ advances the solution from $t_n$ to $t_{n+1} = t_n + h$ using the following steps:\n$$ k_1 = F(t_n, y_n) $$\n$$ k_2 = F\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_1\\right) $$\n$$ k_3 = F\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_2\\right) $$\n$$ k_4 = F(t_n + h, y_n + h k_3) $$\n$$ y_{n+1} = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4) $$\n\nFor the specific ODE $y'(t) = f(t)$, the function $F(t, y)$ is independent of $y$, i.e., $F(t,y) = f(t)$. The RK4 stages simplify considerably:\n$$ k_1 = f(t_n) $$\n$$ k_2 = f\\left(t_n + \\frac{h}{2}\\right) $$\n$$ k_3 = f\\left(t_n + \\frac{h}{2}\\right) $$\n$$ k_4 = f(t_n + h) $$\nThe one-step update rule becomes:\n$$ y_{n+1} = y_n + \\frac{h}{6}\\left[f(t_n) + 2f\\left(t_n + \\frac{h}{2}\\right) + 2f\\left(t_n + \\frac{h}{2}\\right) + f(t_n + h)\\right] $$\n$$ y_{n+1} = y_n + h\\left[\\frac{1}{6}f(t_n) + \\frac{4}{6}f\\left(t_n + \\frac{h}{2}\\right) + \\frac{1}{6}f(t_n + h)\\right] $$\nThis formula is precisely Simpson's rule for the numerical integration of $f(t)$ over the interval $[t_n, t_{n+1}]$. This is logical, as the exact solution to $y'(t) = f(t)$ is obtained by integration: $y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(s) ds$.\n\nThe local truncation error $e_{\\mathrm{loc}}(t_n, h)$ is defined as the error incurred in a single step, assuming the method starts from the exact solution $y(t_n)$.\n$$ e_{\\mathrm{loc}}(t_n, h) = y(t_{n+1}) - \\left(y(t_n) + h\\left[\\frac{1}{6}f(t_n) + \\frac{4}{6}f\\left(t_n + \\frac{h}{2}\\right) + \\frac{1}{6}f(t_n + h)\\right]\\right) $$\n$$ e_{\\mathrm{loc}}(t_n, h) = \\int_{t_n}^{t_{n+1}} f(s) ds - h\\left[\\frac{1}{6}f(t_n) + \\frac{4}{6}f\\left(t_n + \\frac{h}{2}\\right) + \\frac{1}{6}f(t_n + h)\\right] $$\nThis is the error of Simpson's rule. To find its leading-order term, we employ Taylor series expansion for $f(s)$ around $t_n$. Let $t_n = t_0$. The exact integral is:\n$$ \\int_{t_0}^{t_0+h} f(s) ds = \\int_{t_0}^{t_0+h} \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(t_0)}{k!}(s-t_0)^k ds = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(t_0)}{(k+1)!}h^{k+1} $$\n$$ \\int_{t_0}^{t_0+h} f(s) ds = hf(t_0) + \\frac{h^2}{2}f'(t_0) + \\frac{h^3}{6}f''(t_0) + \\frac{h^4}{24}f'''(t_0) + \\frac{h^5}{120}f^{(4)}(t_0) + \\mathcal{O}(h^6) $$\nThe numerical approximation is expanded similarly by using Taylor series for $f(t_0+h/2)$ and $f(t_0+h)$:\n$$ \\frac{h}{6}\\left[f(t_0) + 4f\\left(t_0 + \\frac{h}{2}\\right) + f(t_0 + h)\\right] $$\nAfter combining terms, this expansion yields:\n$$ hf(t_0) + \\frac{h^2}{2}f'(t_0) + \\frac{h^3}{6}f''(t_0) + \\frac{h^4}{24}f'''(t_0) + \\frac{5h^5}{576}f^{(4)}(t_0) + \\mathcal{O}(h^6) $$\nThe local truncation error is the difference between the exact integral and the numerical approximation:\n$$ e_{\\mathrm{loc}}(t_0, h) = \\left(\\frac{1}{120} - \\frac{5}{576}\\right)h^5 f^{(4)}(t_0) + \\mathcal{O}(h^6) = \\left(\\frac{24 - 25}{2880}\\right)h^5 f^{(4)}(t_0) + \\mathcal{O}(h^6) $$\n$$ e_{\\mathrm{loc}}(t_0, h) = -\\frac{1}{2880}h^5 f^{(4)}(t_0) + \\mathcal{O}(h^6) $$\nThis confirms that the local truncation error is of order $\\mathcal{O}(h^5)$, as expected for a fourth-order method.\n\nNow, we analyze the specific case $f(t) = \\sin(\\Omega t)$, where $\\Omega$ is a potentially large angular frequency. We must compute the fourth derivative of $f(t)$:\n$$ f(t) = \\sin(\\Omega t) $$\n$$ f'(t) = \\Omega \\cos(\\Omega t) $$\n$$ f''(t) = -\\Omega^2 \\sin(\\Omega t) $$\n$$ f'''(t) = -\\Omega^3 \\cos(\\Omega t) $$\n$$ f^{(4)}(t) = \\Omega^4 \\sin(\\Omega t) $$\nSubstituting this into the leading-order error term gives:\n$$ e_{\\mathrm{loc}}(t_0, h) \\approx -\\frac{h^5}{2880} \\Omega^4 \\sin(\\Omega t_0) $$\nThe magnitude of the error is therefore proportional to $\\Omega^4$. This is the key observation. Even for small step size $h$, if the frequency $\\Omega$ is sufficiently large, the factor $\\Omega^4$ can cause the local truncation error to become unacceptably large. The condition for maintaining accuracy is not merely that $h$ is small, but that the dimensionless quantity $(\\Omega h)$ is small enough to control the term $h^5 \\Omega^4 = h (\\Omega h)^4$. A fourth-order method is highly accurate for smooth functions with bounded derivatives, but for functions whose derivatives grow rapidly (as with increasing $\\Omega$), the constant in the $\\mathcal{O}(h^5)$ term can be enormous, compromising the method's effectiveness unless $h$ is made prohibitively small.\n\nTo verify this behavior, we will perform a numerical experiment. The initial value problem is $y'(t) = \\sin(\\Omega t)$ with $y(0)=0$.\nThe exact solution is found by direct integration:\nFor $\\Omega \\neq 0$, $y(t) = \\int_0^t \\sin(\\Omega s) ds = \\left[-\\frac{\\cos(\\Omega s)}{\\Omega}\\right]_0^t = \\frac{1 - \\cos(\\Omega t)}{\\Omega}$.\nFor $\\Omega = 0$, $f(t)=0$, so $y(t)=0$.\n\nWe will compute:\n$1.$ The local truncation error for one step from $t_0=0$ with step size $h$: $e_{\\mathrm{loc}}(0,h) = y(h) - y_1$, where $y_1$ is the result of one RK4 step starting from $y_0 = y(0)=0$.\n$$ y_1 = \\frac{h}{6}\\left[f(0) + 4f\\left(\\frac{h}{2}\\right) + f(h)\\right] = \\frac{h}{6}\\left[0 + 4\\sin\\left(\\frac{\\Omega h}{2}\\right) + \\sin(\\Omega h)\\right] $$\n$2.$ The global truncation error at $T=1$: $e_{\\mathrm{glob}}(1,h) = y(1) - y_N$, where $y_N$ is the numerical solution at $t=1$ after $N=T/h$ steps.\nThe numerical solution $y_N$ is computed by recursively applying the RK4 step, which in this case amounts to summing the integral contributions over each subinterval:\n$$ y_N = \\sum_{n=0}^{N-1} \\frac{h}{6}\\left[f(t_n) + 4f\\left(t_n + \\frac{h}{2}\\right) + f(t_n + h)\\right] $$\nwhere $t_n = n \\cdot h$.\n\nThe provided test cases will explore the effect of increasing $\\Omega$ while decreasing $h$ such that the product $\\Omega h$ remains constant or increases. For Case $1$ $(\\Omega=0, h=0.1)$, the error is analytically zero. For Cases $2, 3, 4$, we expect to see the error increase dramatically with $\\Omega$ despite the corresponding decrease in $h$. The implementation will calculate $|e_{\\mathrm{loc}}(0,h)|$ and $|e_{\\mathrm{glob}}(1,h)|$ for each parameter set.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes local and global truncation errors for the classical a\n    fourth-order Runge-Kutta method on a test ODE.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 0.1),    # Case 1: (Omega, h)\n        (50.0, 0.02),   # Case 2\n        (200.0, 0.01),  # Case 3\n        (1000.0, 0.005) # Case 4\n    ]\n    \n    T = 1.0  # Final time for global error calculation.\n    \n    results = []\n    \n    for case in test_cases:\n        Omega, h = case\n        \n        # --- Define necessary functions ---\n        \n        def f(t, Omega_val):\n            \"\"\"The right-hand side of the ODE y'(t) = f(t).\"\"\"\n            return np.sin(Omega_val * t)\n            \n        def y_exact(t, Omega_val):\n            \"\"\"The exact solution of the IVP y'(t) = f(t), y(0)=0.\"\"\"\n            if Omega_val == 0.0:\n                return 0.0\n            return (1.0 - np.cos(Omega_val * t)) / Omega_val\n\n        # --- Task 4: Compute Local Truncation Error at t=0 ---\n        \n        # Exact solution at t=h\n        y_h_exact = y_exact(h, Omega)\n        \n        # One step of RK4 for y'(t)=f(t) from y(0)=0.\n        # This is equivalent to applying Simpson's rule to integrate f(t) from 0 to h.\n        # y_1 = y_0 + integral_approx\n        # y_0 = 0\n        t0 = 0.0\n        k1 = f(t0, Omega)\n        k2 = f(t0 + h / 2.0, Omega)\n        # k3 is the same as k2 since f is independent of y\n        k4 = f(t0 + h, Omega)\n        \n        y1_numerical = (h / 6.0) * (k1 + 4.0 * k2 + k4) # Using simplified 2*k2+2*k3 = 4*k2\n        \n        # Local truncation error at t=0\n        e_loc_mag = np.abs(y_h_exact - y1_numerical)\n        results.append(e_loc_mag)\n        \n        # --- Task 5: Compute Global Truncation Error at T=1 ---\n        \n        # Exact solution at t=T\n        y_T_exact = y_exact(T, Omega)\n\n        # Number of steps\n        # Given T/h is an integer for all test cases\n        N = int(T / h)\n        \n        y_numerical = 0.0\n        \n        # In this special case y'(t) = f(t), the global solution is the sum of\n        # the increments from each step.\n        for i in range(N):\n            ti = i * h\n            \n            # Since F(t,y) = f(t), the RK4 update is simply adding the\n            # Simpson's rule approximation of the integral over [ti, ti+h].\n            k1_global = f(ti, Omega)\n            k2_global = f(ti + h / 2.0, Omega)\n            k4_global = f(ti + h, Omega)\n            \n            increment = (h / 6.0) * (k1_global + 4.0 * k2_global + k4_global)\n            y_numerical += increment\n\n        # Global truncation error at T=1\n        e_glob_mag = np.abs(y_T_exact - y_numerical)\n        results.append(e_glob_mag)\n        \n    # Final print statement in the exact required format.\n    # Printing with many digits to avoid precision issues in evaluation.\n    print(f\"[{','.join(f'{r:.17f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}