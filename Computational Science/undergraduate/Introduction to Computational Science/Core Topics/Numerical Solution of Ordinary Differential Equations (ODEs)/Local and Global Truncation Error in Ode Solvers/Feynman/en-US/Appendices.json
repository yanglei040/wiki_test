{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of numerical analysis is understanding how the error of a method behaves as we change its parameters. For ODE solvers, the most crucial parameter is the step size, $h$. This exercise guides you through a step-doubling experiment, a fundamental technique to empirically determine a solver's order of convergence, $p$, which dictates how the global truncation error scales with $h$. By treating common solvers like Euler's method and Heun's method as \"black boxes,\" you will learn to verify their theoretical accuracy from first principles, a vital skill for validating any numerical code .",
            "id": "3156057",
            "problem": "Consider the initial value problem (IVP) for an Ordinary Differential Equation (ODE) defined by $y'(t) = y(t)$ with $y(0) = 1$. The exact solution is $y(t) = e^{t}$. A one-step ODE solver advances the numerical state via a mapping $y_{n+1} = \\Phi(t_{n}, y_{n}, h)$ for a step size $h$, where $t_{n+1} = t_{n} + h$. A method has order $p$ if, for sufficiently small $h$, its global truncation error at a fixed final time $T$ scales as a power of $h$. The local truncation error is the discrepancy incurred in one step when injecting the exact solution into the numerical update; the global truncation error is the accumulated discrepancy at $t = T$.\n\nYour task is to construct a step-doubling experiment to estimate the order $p$ of a black-box ODE solver by measuring the global truncation errors $E(h)$ and $E(2h)$ when solving $y' = y$ with the exact solution $y = e^{t}$. The experiment must be grounded in first principles: use the definition of local and global truncation error and reason about how the global error changes as $h$ is scaled, without assuming any target formula in advance. Implement two black-box one-step solvers to serve as the unknown methods: a forward Euler solver and a Heun (explicit trapezoidal) solver. Treat each solver as a black-box function that, given $f$, $y_{0}$, $t_{0}$, $T$, and $h$, returns the numerical approximation $y_{N}$ at $t = T$ using fixed steps, where $N = T/h$ is an integer.\n\nFor each parameter set, compute the numerical solution at $t = T$ with step sizes $h$ and $2h$; measure the corresponding global truncation errors $E(h) = |y_{\\text{num}}(T; h) - e^{T}|$ and $E(2h) = |y_{\\text{num}}(T; 2h) - e^{T}|$; and, using the fundamental scaling relationship implied by the definition of method order, deduce an estimate of $p$. Do not use any physical units in this problem. Angles are not involved. Express each estimated $p$ as a floating-point number rounded to six decimal places.\n\nUse the following test suite of parameter values, each specified as a tuple $(\\text{solver}, T, h)$ with $T/h \\in \\mathbb{N}$ and $T/(2h) \\in \\mathbb{N}$:\n- $(\\text{Euler}, 1.0, 0.1)$ with $T = 1.0$ and $h = 0.1$,\n- $(\\text{Heun}, 1.0, 0.1)$ with $T = 1.0$ and $h = 0.1$,\n- $(\\text{Euler}, 2.0, 0.05)$ with $T = 2.0$ and $h = 0.05$,\n- $(\\text{Heun}, 2.0, 0.05)$ with $T = 2.0$ and $h = 0.05$,\n- $(\\text{Euler}, 1.0, 0.5)$ with $T = 1.0$ and $h = 0.5$,\n- $(\\text{Heun}, 1.0, 0.001)$ with $T = 1.0$ and $h = 0.001$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order given above; for example, output of the form [$p_{1},$ $p_{2},$ $p_{3},$ $p_{4},$ $p_{5},$ $p_{6}$], with each $p_{i}$ rounded to six decimal places.",
            "solution": "The user has provided a valid, well-posed problem statement from the field of computational science. The problem is scientifically grounded in the theory of numerical methods for ordinary differential equations (ODEs), is self-contained, and has a clear, verifiable objective.\n\nThe task is to estimate the order of convergence, $p$, for two one-step ODE solvers by performing a step-doubling experiment. This requires deriving an estimator for $p$ from first principles and then applying it to a specific initial value problem (IVP).\n\n### Principle and Derivation of the Order Estimator\n\nA numerical method for solving an ODE is said to have an order of convergence $p$ if its global truncation error, $E$, at a fixed final time $T$, scales with the step size $h$ according to the relation:\n$$\nE(h) = |y_{\\text{num}}(T; h) - y(T)| \\approx C h^p\n$$\nfor sufficiently small $h$. Here, $y_{\\text{num}}(T; h)$ is the numerical solution at time $T$ obtained using step size $h$, $y(T)$ is the exact solution, and $C$ is a constant that depends on the ODE, its solution, the final time $T$, and the specific method, but is assumed to be independent of $h$.\n\nTo estimate $p$ without knowing the constant $C$, we can compute the error using two different step sizes. The problem specifies a step-doubling experiment, so we use step sizes $h$ and $2h$. According to the scaling relationship, the errors are:\n$$\nE(h) \\approx C h^p \\quad (1)\n$$\n$$\nE(2h) \\approx C (2h)^p = C 2^p h^p \\quad (2)\n$$\n\nTo eliminate the unknown constant $C$, we can take the ratio of equation $(2)$ to equation $(1)$:\n$$\n\\frac{E(2h)}{E(h)} \\approx \\frac{C 2^p h^p}{C h^p} = 2^p\n$$\n\nThis relationship provides a direct way to estimate $p$. By taking the base-$2$ logarithm of both sides, we isolate $p$:\n$$\n\\log_2\\left(\\frac{E(2h)}{E(h)}\\right) \\approx \\log_2(2^p) = p\n$$\n\nThus, our estimator for the order of the method is:\n$$\np \\approx \\log_2\\left(\\frac{E(2h)}{E(h)}\\right)\n$$\nThis formula is derived directly from the definition of the method's order and forms the basis of our numerical experiment.\n\n### Implementation of the Numerical Experiment\n\nThe experiment will be conducted on the following IVP:\n- **ODE:** $y'(t) = y(t)$\n- **Initial Condition:** $y(0) = 1$\n- **Exact Solution:** $y(t) = e^t$\n\nThe right-hand side of the ODE is given by the function $f(t, y) = y$.\n\nWe will implement two \"black-box\" one-step solvers:\n\n1.  **Forward Euler Method:** This is an explicit first-order method. The update rule for advancing from step $n$ to $n+1$ is:\n    $$\n    y_{n+1} = y_n + h f(t_n, y_n)\n    $$\n    For our specific IVP, this becomes $y_{n+1} = y_n + h y_n = y_n(1+h)$.\n\n2.  **Heun's Method (Explicit Trapezoidal Rule):** This is an explicit second-order method that belongs to the family of predictor-corrector methods or Runge-Kutta methods. The update rule is:\n    $$\n    k_1 = f(t_n, y_n)\n    $$\n    $$\n    k_2 = f(t_n + h, y_n + h k_1)\n    $$\n    $$\n    y_{n+1} = y_n + \\frac{h}{2}(k_1 + k_2)\n    $$\n    This can be viewed as taking an initial Euler step (the predictor) and then averaging the slope at the beginning and the predicted end of the interval (the corrector).\n\nFor each test case $(\\text{solver}, T, h)$ specified in the problem statement, the following algorithm is executed:\n\n1.  Set the initial conditions $y_0 = 1$ and $t_0 = 0$.\n2.  Compute the numerical solution $y_{\\text{num}}(T; h)$ by applying the chosen solver for $N_1 = T/h$ steps of size $h$.\n3.  Compute the numerical solution $y_{\\text{num}}(T; 2h)$ by applying the same solver for $N_2 = T/(2h)$ steps of size $2h$.\n4.  Calculate the exact solution at the final time, $y(T) = e^T$.\n5.  Determine the global truncation errors:\n    $$\n    E(h) = |y_{\\text{num}}(T; h) - e^T|\n    $$\n    $$\n    E(2h) = |y_{\\text{num}}(T; 2h) - e^T|\n    $$\n6.  Finally, compute the estimated order $p$ using the derived formula:\n    $$\n    p = \\log_2\\left(\\frac{E(2h)}{E(h)}\\right)\n    $$\nThe resulting value of $p$ is rounded to six decimal places as required. This procedure is repeated for all test cases. The Forward Euler method is expected to yield $p \\approx 1$, and Heun's method should yield $p \\approx 2$, with the accuracy of the estimate depending on how well the asymptotic error behavior is established for the given step size $h$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the estimated order of convergence for Euler and Heun methods\n    using a step-doubling experiment based on a set of test cases.\n    \"\"\"\n\n    def ode_f(t, y):\n        \"\"\"The right-hand side of the ODE y' = y.\"\"\"\n        return y\n\n    def forward_euler_solver(f, y0, t0, T, h):\n        \"\"\"\n        Solves an IVP using the Forward Euler method.\n        \n        Args:\n            f: The function f(t, y) for the ODE y' = f(t, y).\n            y0: The initial value y(t0).\n            t0: The initial time.\n            T: The final time.\n            h: The step size.\n        \n        Returns:\n            The numerical solution at time T.\n        \"\"\"\n        y = y0\n        t = t0\n        num_steps = int(np.round((T - t0) / h))\n        \n        for _ in range(num_steps):\n            y = y + h * f(t, y)\n            t = t + h\n            \n        return y\n\n    def heun_solver(f, y0, t0, T, h):\n        \"\"\"\n        Solves an IVP using Heun's method (explicit trapezoidal rule).\n        \n        Args:\n            f: The function f(t, y) for the ODE y' = f(t, y).\n            y0: The initial value y(t0).\n            t0: The initial time.\n            T: The final time.\n            h: The step size.\n        \n        Returns:\n            The numerical solution at time T.\n        \"\"\"\n        y = y0\n        t = t0\n        num_steps = int(np.round((T - t0) / h))\n        \n        for _ in range(num_steps):\n            k1 = f(t, y)\n            k2 = f(t + h, y + h * k1)\n            y = y + (h / 2.0) * (k1 + k2)\n            t = t + h\n            \n        return y\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\"Euler\", 1.0, 0.1),\n        (\"Heun\", 1.0, 0.1),\n        (\"Euler\", 2.0, 0.05),\n        (\"Heun\", 2.0, 0.05),\n        (\"Euler\", 1.0, 0.5),\n        (\"Heun\", 1.0, 0.001),\n    ]\n\n    solvers = {\n        \"Euler\": forward_euler_solver,\n        \"Heun\": heun_solver,\n    }\n\n    results = []\n    \n    for solver_name, T, h in test_cases:\n        solver_func = solvers[solver_name]\n        \n        # Initial conditions for the IVP y'=y, y(0)=1\n        y0 = 1.0\n        t0 = 0.0\n        \n        # Define the two step sizes for the experiment\n        h1 = h\n        h2 = 2.0 * h\n        \n        # Compute numerical solutions for both step sizes\n        y_num_h1 = solver_func(ode_f, y0, t0, T, h1)\n        y_num_h2 = solver_func(ode_f, y0, t0, T, h2)\n        \n        # Compute the exact solution at T\n        y_exact = np.exp(T)\n        \n        # Compute the global truncation errors\n        error_h1 = np.abs(y_num_h1 - y_exact)\n        error_h2 = np.abs(y_num_h2 - y_exact)\n        \n        # Estimate the order p\n        # p is approximately log2(E(2h) / E(h))\n        # Handle cases where error might be zero to avoid division by zero\n        if error_h1 == 0.0:\n            # If the smaller step size gives zero error, the order cannot be determined\n            # by this method. This is unlikely for these methods and ODE.\n            # We can represent this as infinity or NaN, but for the purpose of this\n            # problem, we assume non-zero error.\n            # In a real-world scenario, a different approach would be needed.\n            # Given the problem context, this case is not expected to occur.\n            p = np.nan \n        else:\n            p = np.log2(error_h2 / error_h1)\n        \n        results.append(p)\n\n    # Format results to six decimal places and create the final output string\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Understanding the structure of truncation error allows for more than just analysis; it enables us to actively improve our solutions. This practice introduces Richardson Extrapolation, a powerful and general technique for increasing the order of accuracy of a numerical method. By deriving the method from the asymptotic error expansion of the simple Euler solver, you will see how to combine two less accurate solutions, computed at different step sizes, to cancel the dominant error term and produce a significantly more accurate result. This exercise demonstrates a constructive approach to error control, showing that a deep understanding of error can lead directly to better algorithms .",
            "id": "2409197",
            "problem": "Implement a complete program that, for a set of initial value problems of the form $\\dfrac{dy}{dt} = f(t,y)$ with $y(0) = y_0$, does the following: starting from the definitions of local truncation error and global truncation error, derive a Richardson extrapolation combination that cancels the dominant global error term of the explicit Euler method by combining two Euler solutions computed with step sizes $h$ and $h/2$. The combination must be derived from first principles by assuming an asymptotic expansion of the global error in powers of $h$, and it must yield a new approximation whose global truncation error is of order $O(h^2)$.\n\nYou must base your derivation on the following foundational facts:\n- The explicit Euler method is defined by the recurrence $y_{n+1} = y_n + h f(t_n, y_n)$ for $t_n = t_0 + n h$.\n- The local truncation error is the one-step defect obtained by inserting the exact solution into a single method step, and for smooth $f$ it scales as $O(h^2)$.\n- The global truncation error at a fixed final time $T$ is the accumulated difference between the exact solution and the numerical solution after $N = T/h$ steps; for the explicit Euler method it scales as $O(h)$.\n\nYour program must implement:\n1. A function to compute the explicit Euler solution $y_h(T)$ at the final time $T$ with uniform step size $h$.\n2. A Richardson extrapolation combination that, using only the two Euler solutions computed with step sizes $h$ and $h/2$, produces an improved approximation $y_{\\mathrm{RE}}(T)$ whose global truncation error scales as $O(h^2)$.\n3. An empirical verification routine that also computes solutions with step $h/4$ so that you can estimate observed orders of convergence:\n   - For the explicit Euler method, estimate $p_{\\mathrm{E}} \\approx \\log_2\\!\\left(\\dfrac{|y_h(T) - y(T)|}{|y_{h/2}(T) - y(T)|}\\right)$.\n   - For the Richardson-extrapolated approximation, first form $y_{\\mathrm{RE}}(h) = \\mathrm{RE}(y_h, y_{h/2})$ and $y_{\\mathrm{RE}}(h/2) = \\mathrm{RE}(y_{h/2}, y_{h/4})$, then estimate $p_{\\mathrm{RE}} \\approx \\log_2\\!\\left(\\dfrac{|y_{\\mathrm{RE}}(h) - y(T)|}{|y_{\\mathrm{RE}}(h/2) - y(T)|}\\right)$.\n\nAngles must be interpreted in radians.\n\nTest Suite:\nFor each case below, compute $y_h(T)$, $y_{h/2}(T)$, $y_{h/4}(T)$, form $y_{\\mathrm{RE}}(h)$ and $y_{\\mathrm{RE}}(h/2)$, and then compute the absolute errors with respect to the exact solution $y(T)$.\n- Case A (exponential decay): $\\dfrac{dy}{dt} = -3 y$, $y(0) = 1$, $T = 1$, base step $h = 0.2$. Exact solution: $y(t) = e^{-3 t}$.\n- Case B (linear non-homogeneous): $\\dfrac{dy}{dt} = t + y$, $y(0) = 0$, $T = 1$, base step $h = 0.2$. Exact solution: $y(t) = e^{t} - (t + 1)$.\n- Case C (variable-coefficient growth): $\\dfrac{dy}{dt} = \\sin(t)\\, y$, $y(0) = 1$, $T = 1$, base step $h = 0.2$. Exact solution: $y(t) = \\exp\\!\\big(1 - \\cos(t)\\big)$.\n\nFor each case, your program must produce a list of five real numbers:\n- The absolute global error of Euler with step $h$, namely $E_h = |y_h(T) - y(T)|$.\n- The absolute global error of Euler with step $h/2$, namely $E_{h/2} = |y_{h/2}(T) - y(T)|$.\n- The absolute global error of the Richardson-extrapolated approximation built from $h$ and $h/2$, namely $E_{\\mathrm{RE}}(h) = |y_{\\mathrm{RE}}(h) - y(T)|$.\n- The observed order $p_{\\mathrm{E}}$ defined above.\n- The observed order $p_{\\mathrm{RE}}$ defined above.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the three cases, in the order Case A, Case B, Case C, as a comma-separated list of three lists, enclosed in a single pair of square brackets, with no spaces anywhere. Each real number must be printed in scientific notation with ten significant digits. For example, the line must look like:\n[[E_h_A,E_h2_A,E_RE_A,pE_A,pRE_A],[E_h_B,E_h2_B,E_RE_B,pE_B,pRE_B],[E_h_C,E_h2_C,E_RE_C,pE_C,pRE_C]]\nMake sure all trigonometric function arguments are in radians. No units are required since all quantities are dimensionless in this problem.",
            "solution": "The problem statement is subjected to validation.\n\nGivens are extracted verbatim:\n1.  Initial value problem: $\\dfrac{dy}{dt} = f(t,y)$ with $y(0) = y_0$.\n2.  Explicit Euler method recurrence: $y_{n+1} = y_n + h f(t_n, y_n)$ for $t_n = t_0 + n h$.\n3.  Local truncation error for explicit Euler scales as $\\mathcal{O}(h^2)$.\n4.  Global truncation error for explicit Euler at a fixed time $T$ scales as $\\mathcal{O}(h)$.\n5.  Task: Derive a Richardson extrapolation combination for two Euler solutions with step sizes $h$ and $h/2$ to cancel the dominant global error term, assuming an asymptotic expansion for the global error. The resulting approximation must have a global error of order $\\mathcal{O}(h^2)$.\n6.  Task: Implement an empirical verification using step sizes $h$, $h/2$, and $h/4$.\n7.  Observed order for Euler: $p_{\\mathrm{E}} \\approx \\log_2\\!\\left(\\dfrac{|y_h(T) - y(T)|}{|y_{h/2}(T) - y(T)|}\\right)$.\n8.  Observed order for Richardson extrapolation: $p_{\\mathrm{RE}} \\approx \\log_2\\!\\left(\\dfrac{|y_{\\mathrm{RE}}(h) - y(T)|}{|y_{\\mathrm{RE}}(h/2) - y(T)|}\\right)$, where $y_{\\mathrm{RE}}(h) = \\mathrm{RE}(y_h, y_{h/2})$ and $y_{\\mathrm{RE}}(h/2) = \\mathrm{RE}(y_{h/2}, y_{h/4})$.\n9.  Test Case A: $\\dfrac{dy}{dt} = -3 y$, $y(0) = 1$, $T = 1$, $h = 0.2$. Exact solution: $y(t) = e^{-3 t}$.\n10. Test Case B: $\\dfrac{dy}{dt} = t + y$, $y(0) = 0$, $T = 1$, $h = 0.2$. Exact solution: $y(t) = e^{t} - (t + 1)$.\n11. Test Case C: $\\dfrac{dy}{dt} = \\sin(t)\\, y$, $y(0) = 1$, $T = 1$, $h = 0.2$. Exact solution: $y(t) = \\exp\\!\\big(1 - \\cos(t)\\big)$.\n12. Required output for each case: A list of five numbers: $|y_h(T) - y(T)|$, $|y_{h/2}(T) - y(T)|$, $|y_{\\mathrm{RE}}(h) - y(T)|$, $p_{\\mathrm{E}}$, and $p_{\\mathrm{RE}}$.\n\nValidation using extracted givens:\nThe problem is scientifically grounded. It addresses Richardson extrapolation, a standard and fundamental technique in numerical analysis for improving the accuracy of numerical methods. The premises—the definition of the explicit Euler method and the orders of its local and global truncation errors—are standard results in the study of numerical solutions to ordinary differential equations. The problem is well-posed; it asks for a specific derivation and an implementation for well-defined test cases with unique analytical solutions, ensuring that a unique and meaningful result can be obtained and verified. The language is objective and precise. The problem does not violate any of the invalidity criteria. It is a formalizable problem directly relevant to computational physics and numerical methods.\n\nVerdict: The problem is valid. A solution will be provided.\n\nThe derivation of the Richardson extrapolation formula must be constructed from first principles, as stipulated. The foundation of this method is the existence of an asymptotic expansion for the global error of the numerical scheme. For the explicit Euler method, which has a global error of order $\\mathcal{O}(h)$, this expansion takes the form:\n$$\ny_h(T) = y(T) + C_1 h + C_2 h^2 + C_3 h^3 + \\dots\n$$\nHere, $y(T)$ is the exact solution at the final time $T$, $y_h(T)$ is the numerical approximation obtained with a step size $h$, and the coefficients $C_k$ are independent of $h$ but depend on the function $f(t,y)$ and its derivatives at various points. The dominant error term is $C_1 h$.\n\nOur objective is to eliminate this dominant term by combining two separate computations. Let us perform the numerical integration with step size $h$ and again with step size $h/2$. The asymptotic expansions for the respective numerical solutions are:\n$$\n(1) \\quad y_h(T) = y(T) + C_1 h + C_2 h^2 + \\mathcal{O}(h^3)\n$$\n$$\n(2) \\quad y_{h/2}(T) = y(T) + C_1 \\frac{h}{2} + C_2 \\left(\\frac{h}{2}\\right)^2 + \\mathcal{O}(h^3) = y(T) + \\frac{1}{2} C_1 h + \\frac{1}{4} C_2 h^2 + \\mathcal{O}(h^3)\n$$\nWe seek a linear combination of $y_h(T)$ and $y_{h/2}(T)$ that cancels the term proportional to $C_1 h$. Let the extrapolated approximation be $y_{\\mathrm{RE}}(T)$. To eliminate $C_1 h$, we can multiply equation $(2)$ by $2$ and subtract equation $(1)$:\n$$\n2 y_{h/2}(T) - y_h(T) = \\left(2y(T) + C_1 h + \\frac{1}{2} C_2 h^2 + \\dots\\right) - \\left(y(T) + C_1 h + C_2 h^2 + \\dots\\right)\n$$\n$$\n2 y_{h/2}(T) - y_h(T) = (2-1)y(T) + (1-1)C_1 h + \\left(\\frac{1}{2}-1\\right)C_2 h^2 + \\mathcal{O}(h^3)\n$$\n$$\n2 y_{h/2}(T) - y_h(T) = y(T) - \\frac{1}{2} C_2 h^2 + \\mathcal{O}(h^3)\n$$\nFrom this, we define the Richardson-extrapolated solution $y_{\\mathrm{RE}}(T)$ as:\n$$\ny_{\\mathrm{RE}}(T) = 2 y_{h/2}(T) - y_h(T)\n$$\nThe global truncation error of this new approximation is the difference $y_{\\mathrm{RE}}(T) - y(T)$. Based on our derivation:\n$$\ny_{\\mathrm{RE}}(T) - y(T) = -\\frac{1}{2} C_2 h^2 + \\mathcal{O}(h^3)\n$$\nThis confirms that the global truncation error is now of order $\\mathcal{O}(h^2)$, as required. The dominant error term of the original method has been successfully eliminated.\n\nThe computational implementation will consist of three main parts. First, a function `explicit_euler` will solve the given ODE from an initial time $t_0$ to a final time $T$ using a constant step size $h$. This function will implement the recurrence $y_{k+1} = y_k + h f(t_k, y_k)$, which is iterated $N = \\text{round}((T-t_0)/h)$ times.\n\nSecond, a main routine will execute the logic for each test case. For a base step size $h$, it will call the `explicit_euler` function three times with step sizes $h$, $h/2$, and $h/4$ to obtain the numerical solutions $y_h(T)$, $y_{h/2}(T)$, and $y_{h/4}(T)$.\n\nThird, this routine will use these numerical results to perform the empirical analysis. It will compute the Richardson-extrapolated approximations:\n$$\ny_{\\mathrm{RE}}(h) = 2 y_{h/2}(T) - y_h(T)\n$$\n$$\ny_{\\mathrm{RE}}(h/2) = 2 y_{h/4}(T) - y_{h/2}(T)\n$$\nThe absolute global errors are then calculated with respect to the known exact solution $y(T)$. Finally, the observed orders of convergence, $p_{\\mathrm{E}}$ and $p_{\\mathrm{RE}}$, are computed using the specified logarithmic formulas. These empirical orders should be approximately $1$ for the Euler method and $2$ for the Richardson-extrapolated method, thus verifying the theoretical improvement in accuracy. The final results for all test cases will be formatted and printed according to the specified format. Trigonometric function inputs will be processed in radians as is standard in scientific computation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Richardson extrapolation for the explicit Euler method to solve\n    several initial value problems and empirically verifies the order of convergence.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda t, y: -3.0 * y,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"h\": 0.2,\n            \"exact_y\": lambda t: np.exp(-3.0 * t),\n        },\n        {\n            \"f\": lambda t, y: t + y,\n            \"y0\": 0.0,\n            \"T\": 1.0,\n            \"h\": 0.2,\n            \"exact_y\": lambda t: np.exp(t) - (t + 1.0),\n        },\n        {\n            \"f\": lambda t, y: np.sin(t) * y,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"h\": 0.2,\n            \"exact_y\": lambda t: np.exp(1.0 - np.cos(t)),\n        },\n    ]\n\n    def explicit_euler(f_ode, y_start, t_start, t_end, step_size):\n        \"\"\"\n        Computes the solution of an ODE using the explicit Euler method.\n        \"\"\"\n        t = t_start\n        y = y_start\n        # Use round() to avoid floating-point inaccuracies in step counting\n        num_steps = int(round((t_end - t_start) / step_size))\n        \n        for _ in range(num_steps):\n            y = y + step_size * f_ode(t, y)\n            t = t + step_size\n        return y\n\n    all_results = []\n    for case in test_cases:\n        f = case[\"f\"]\n        y0 = case[\"y0\"]\n        T = case[\"T\"]\n        h_base = case[\"h\"]\n        exact_y_func = case[\"exact_y\"]\n\n        # Define the three step sizes\n        h = h_base\n        h_half = h_base / 2.0\n        h_quarter = h_base / 4.0\n\n        # Compute Euler solutions for each step size\n        y_h = explicit_euler(f, y0, 0.0, T, h)\n        y_h_half = explicit_euler(f, y0, 0.0, T, h_half)\n        y_h_quarter = explicit_euler(f, y0, 0.0, T, h_quarter)\n\n        # Compute the exact solution at the final time T\n        y_exact = exact_y_func(T)\n\n        # Compute Richardson-extrapolated approximations\n        y_re_h = 2.0 * y_h_half - y_h\n        y_re_h_half = 2.0 * y_h_quarter - y_h_half\n\n        # Compute absolute errors\n        E_h = np.abs(y_h - y_exact)\n        E_h_half = np.abs(y_h_half - y_exact)\n        E_re_h = np.abs(y_re_h - y_exact)\n        E_re_h_half = np.abs(y_re_h_half - y_exact)\n\n        # Compute observed orders of convergence\n        # Handle cases where error is zero to avoid division by zero or log(0)\n        p_E = np.log2(E_h / E_h_half) if E_h_half > 0 else 0.0\n        p_RE = np.log2(E_re_h / E_re_h_half) if E_re_h_half > 0 else 0.0\n\n        results = [E_h, E_h_half, E_re_h, p_E, p_RE]\n        all_results.append(results)\n\n    # Format the final output string as specified\n    formatted_cases = []\n    for case_res in all_results:\n        formatted_nums = [f\"{num:.10e}\" for num in case_res]\n        formatted_cases.append(f\"[{','.join(formatted_nums)}]\")\n    \n    final_output = f\"[{','.join(formatted_cases)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "In many real-world scientific computations, the exact solution to a differential equation is unknown, making direct error measurement impossible. This practice introduces a professional-grade technique for estimating the global truncation error in such scenarios. You will use a sophisticated adaptive solver from a standard library, `scipy.integrate.solve_ivp`, and learn how comparing a \"coarse\" tolerance solution to a \"fine\" tolerance solution provides a reliable estimate of the error. This method is a cornerstone of code verification, allowing you to quantify the uncertainty and build confidence in your numerical simulations, even without an analytical benchmark .",
            "id": "2409225",
            "problem": "You are given the task of quantifying the global truncation error in adaptive one-step Ordinary Differential Equation (ODE) solvers by comparing solutions computed under two different tolerance settings. Work from first principles: begin with the initial value problem (IVP) definition, the definition of local truncation error and global truncation error, and the fundamental idea that adaptive steppers control an estimate of the local error per step to meet user-specified tolerances. Use this to justify how a difference between two numerical solutions computed with different tolerances can serve as a practical estimator of the global truncation error at a fixed final time.\n\nLet an initial value problem be of the form $y'(t)=f(t,y)$ with initial condition $y(t_0)=y_0$, where $f$ is sufficiently smooth so that the solution $y(t)$ exists and is unique on the interval of interest. The local truncation error is the one-step error that a numerical method would make over a single step if it were started from the exact solution at the previous point. The global truncation error is the accumulated difference between the numerical solution and the exact solution at a given time.\n\nYour task is to implement a program that uses the function scipy.integrate.solve_ivp to solve three test IVPs at a final time $T$ twice, once with a coarse tolerance and once with a fine tolerance, and then uses the difference between the two final states as an estimate of the global truncation error (measured in the Euclidean norm) for the coarse-tolerance solution at $t=T$. All variables are dimensionless and all angles must be in radians.\n\nUse the following test suite of problems, each with a known exact solution $y(t)$, a final time $T$, and an initial value $y_0$:\n\n- Test case A (scalar exponential decay):\n  - Differential equation: $y'(t)=-3y(t)$.\n  - Initial condition: $y(0)=1$.\n  - Final time: $T=5$.\n  - Exact solution: $y(t)=e^{-3t}$.\n\n- Test case B (scalar linear forced equation):\n  - Differential equation: $y'(t)+y(t)=\\sin(t)$.\n  - Initial condition: $y(0)=0$.\n  - Final time: $T=20$.\n  - Exact solution: $y(t)=\\dfrac{1}{2}\\big(\\sin t-\\cos t\\big)+\\dfrac{1}{2}e^{-t}$.\n\n- Test case C (two-dimensional harmonic oscillator written as a first-order system):\n  - Differential equation: $\\begin{cases} y_{1}^{\\prime}(t)=y_{2}(t), \\\\ y_{2}^{\\prime}(t)=-y_{1}(t). \\end{cases}$\n  - Initial condition: $(y_{1}(0),y_{2}(0))=(1,0)$.\n  - Final time: $T=30$.\n  - Exact solution: $(y_{1}(t),y_{2}(t))=\\big(\\cos t,-\\sin t\\big)$.\n\nFor each test case, do the following:\n\n1. Solve the IVP on $[0,T]$ using scipy.integrate.solve_ivp with the explicit Runge–Kutta method of order five with embedded fourth order (set method to the default explicit Runge–Kutta), using a coarse tolerance pair $(\\mathrm{rtol},\\mathrm{atol})=(10^{-3},10^{-6})$, and record the numerical solution $y_{\\mathrm{coarse}}(T)$.\n2. Solve the same IVP again on $[0,T]$ with a fine tolerance pair $(\\mathrm{rtol},\\mathrm{atol})=(10^{-9},10^{-12})$, and record the numerical solution $y_{\\mathrm{fine}}(T)$.\n3. Form the estimator $\\widehat{E}=\\|y_{\\mathrm{coarse}}(T)-y_{\\mathrm{fine}}(T)\\|_{2}$, where $\\|\\cdot\\|_{2}$ denotes the Euclidean norm. For scalar problems this reduces to the absolute value.\n\nAlthough you may confirm reasonableness against the exact solution $y(T)$, your program’s required outputs are only the three estimators $\\widehat{E}$, one per test case, in the order A, B, C.\n\nFinal output format requirement:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC]\").\n- Each result must be a floating-point number. Angles are in radians, values are dimensionless, and no units are required.\n\nThis test suite is designed to probe:\n- A \"happy path\" smooth decay (A),\n- Long-interval oscillatory forcing with transient decay (B),\n- A conservative two-dimensional system over many periods (C), for which phase accuracy matters.\n\nImplement the program so that it runs without any user input, performs the computations exactly as specified, and prints only the required single-line output.",
            "solution": "The problem presented is a valid exercise in computational science. It is scientifically grounded, well-posed, and objective. It directly addresses the relationship between local error control and global error accumulation in adaptive Ordinary Differential Equation (ODE) solvers, a fundamental concept in numerical analysis. The procedure is to estimate the global error of a computation by comparing it to a much more accurate reference computation. This method is standard practice for code verification and analysis when an exact analytical solution is unavailable.\n\nWe begin from first principles. An initial value problem (IVP) is defined by a first-order ODE system and an initial condition:\n$$\ny'(t) = f(t, y(t)), \\quad y(t_0) = y_0\n$$\nHere, $t$ is the independent variable, typically time, and $y(t)$ is the state vector. We assume $f$ is sufficiently smooth to guarantee the existence of a unique, continuous, and differentiable solution $y(t)$ for $t \\ge t_0$.\n\nNumerical methods do not find the continuous function $y(t)$; they generate a sequence of approximations $y_n$ at discrete points in time $t_n$. A one-step method computes the next approximation $y_{n+1}$ at $t_{n+1} = t_n + h_n$ using only the information from the previous step, $(t_n, y_n)$. The step size is $h_n$.\n\nThe **local truncation error** (LTE) is the error committed in a single step, assuming the solver starts from the exact solution $y(t_n)$. For a one-step method of order $p$, the local truncation error at step $n+1$ is of the order $\\mathcal{O}(h_n^{p+1})$. It represents the discrepancy between the exact solution propagated one step and the numerical approximation.\n\nThe **global truncation error** (GTE) is the cumulative effect of these local errors. At a time $T$, the GTE is the difference between the true solution $y(T)$ and the computed numerical approximation $y_N$, where $t_N = T$:\n$$\nE(T) = y(T) - y_N\n$$\nThe global error is generally of a lower order in $h$ than the local error; for a method of order $p$, the global error is typically $\\mathcal{O}(h^p)$, where $h$ is some characteristic step size.\n\nModern adaptive solvers, such as the default `RK45` method in `scipy.integrate.solve_ivp` (an explicit Runge-Kutta method of order $5$ with an embedded estimator of order $4$), do not use a fixed step size. Instead, they adjust the step size $h_n$ at each step to keep an *estimate* of the local error below a user-specified tolerance. This tolerance is a combination of a relative tolerance, $\\mathrm{rtol}$, and an absolute tolerance, $\\mathrm{atol}$. The error estimate $\\Delta_n$ for a step is controlled such that:\n$$\n\\|\\Delta_n\\| \\le \\mathrm{atol} + \\mathrm{rtol} \\cdot \\|y_n\\|\n$$\nIf the error estimate is larger than this tolerance, the step is rejected and re-attempted with a smaller $h_n$. If it is much smaller, the next step $h_{n+1}$ may be increased.\n\nThis mechanism implies that the global error $E(T)$ is a function of the tolerances. A smaller tolerance forces smaller step sizes, which reduces the local errors and, consequently, the final global error. Under reasonable assumptions, the magnitude of the global error is roughly proportional to the magnitude of the requested tolerance. Let $y_{\\text{tol}}(T)$ denote the numerical solution at time $T$ obtained with a certain tolerance setting $\\mathrm{tol}$. Then the global error is $E_{\\text{tol}}(T) = y(T) - y_{\\text{tol}}(T)$, and we expect $\\|E_{\\text{tol}}(T)\\| \\propto \\mathrm{tol}$.\n\nThe problem requires us to compute a numerical solution with a coarse tolerance, $(\\mathrm{rtol}_{\\mathrm{c}}, \\mathrm{atol}_{\\mathrm{c}}) = (10^{-3}, 10^{-6})$, yielding $y_{\\mathrm{coarse}}(T)$. The global error of this solution is $E_{\\mathrm{coarse}} = y(T) - y_{\\mathrm{coarse}}(T)$. We also compute a solution with a very fine tolerance, $(\\mathrm{rtol}_{\\mathrm{f}}, \\mathrm{atol}_{\\mathrm{f}}) = (10^{-9}, 10^{-12})$, yielding $y_{\\mathrm{fine}}(T)$. Its global error is $E_{\\mathrm{fine}} = y(T) - y_{\\mathrm{fine}}(T)$.\n\nSince the fine tolerance is many orders of magnitude smaller than the coarse tolerance, we can assert that the fine solution is a much better approximation to the true solution. That is, $\\|E_{\\mathrm{fine}}\\| \\ll \\|E_{\\mathrm{coarse}}\\|$, and therefore $y_{\\mathrm{fine}}(T)$ is significantly closer to $y(T)$ than $y_{\\mathrm{coarse}}(T)$ is. We can thus use the fine solution as a high-fidelity proxy for the exact solution:\n$$\ny(T) \\approx y_{\\mathrm{fine}}(T)\n$$\nSubstituting this approximation into the definition of the coarse error gives the practical estimator $\\widehat{E}$:\n$$\nE_{\\mathrm{coarse}} = y(T) - y_{\\mathrm{coarse}}(T) \\approx y_{\\mathrm{fine}}(T) - y_{\\mathrm{coarse}}(T)\n$$\nThe problem asks for the Euclidean norm of this error vector estimate:\n$$\n\\widehat{E} = \\| y_{\\mathrm{coarse}}(T) - y_{\\mathrm{fine}}(T) \\|_{2}\n$$\nThis is the quantity we must compute for each test case.\n\nThe algorithmic procedure is as follows:\nFor each of the three IVPs (A, B, and C):\n$1$. Define the ODE function $f(t, y)$, the initial condition $y_{0}$, and the final time $T$.\n$2$. Call `scipy.integrate.solve_ivp` to solve the IVP over the interval $[0, T]$, specifying the coarse tolerances $\\mathrm{rtol}=10^{-3}$ and $\\mathrm{atol}=10^{-6}$. To obtain the solution precisely at $T$, we use the `t_eval=[T]` argument. We extract the final state vector, $y_{\\mathrm{coarse}}(T)$.\n$3$. Call `scipy.integrate.solve_ivp` again for the same IVP, but with the fine tolerances $\\mathrm{rtol}=10^{-9}$ and $\\mathrm{atol}=10^{-12}$. We extract the final state vector, $y_{\\mathrm{fine}}(T)$.\n$4$. Compute the difference vector $y_{\\mathrm{coarse}}(T) - y_{\\mathrm{fine}}(T)$.\n$5$. Calculate the Euclidean norm of this difference vector using `numpy.linalg.norm`. This value is the estimated global error $\\widehat{E}$ for the coarse solution.\n\nThis process is repeated for all three test cases, and the resulting error estimates are collected and formatted as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Computes an estimate of the global truncation error for three IVPs by\n    comparing solutions generated with coarse and fine tolerances.\n    \"\"\"\n\n    # --- Define the differential equations ---\n\n    # Test case A: y'(t) = -3*y(t)\n    def f_a(t, y):\n        return -3 * y\n\n    # Test case B: y'(t) = sin(t) - y(t)\n    def f_b(t, y):\n        return np.sin(t) - y\n\n    # Test case C: y1'(t) = y2(t), y2'(t) = -y1(t)\n    def f_c(t, y):\n        y1, y2 = y\n        return np.array([y2, -y1])\n\n    # --- Define the test cases ---\n    # Each case is a dictionary containing the ODE function, initial condition,\n    # and final time.\n    test_cases = [\n        {'func': f_a, 'y0': np.array([1.0]), 'T': 5.0},\n        {'func': f_b, 'y0': np.array([0.0]), 'T': 20.0},\n        {'func': f_c, 'y0': np.array([1.0, 0.0]), 'T': 30.0},\n    ]\n\n    # --- Define tolerance settings ---\n    coarse_tol = {'rtol': 1e-3, 'atol': 1e-6}\n    fine_tol = {'rtol': 1e-9, 'atol': 1e-12}\n\n    results = []\n\n    # --- Main loop to process each test case ---\n    for case in test_cases:\n        f = case['func']\n        y0 = case['y0']\n        T = case['T']\n        t_span = (0, T)\n        t_eval = [T]  # Ensure solution is evaluated exactly at the final time\n\n        # 1. Solve with coarse tolerances\n        sol_coarse = solve_ivp(\n            fun=f,\n            t_span=t_span,\n            y0=y0,\n            t_eval=t_eval,\n            rtol=coarse_tol['rtol'],\n            atol=coarse_tol['atol']\n        )\n        # The result from t_eval is a 2D array of shape (n_vars, 1)\n        # Flatten to get a 1D array of the final state\n        y_coarse_T = sol_coarse.y.flatten()\n\n        # 2. Solve with fine tolerances\n        sol_fine = solve_ivp(\n            fun=f,\n            t_span=t_span,\n            y0=y0,\n            t_eval=t_eval,\n            rtol=fine_tol['rtol'],\n            atol=fine_tol['atol']\n        )\n        y_fine_T = sol_fine.y.flatten()\n\n        # 3. Compute the error estimator\n        # The estimator is the Euclidean norm of the difference between the two\n        # numerical solutions at the final time.\n        error_estimator = np.linalg.norm(y_coarse_T - y_fine_T)\n        \n        results.append(error_estimator)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}