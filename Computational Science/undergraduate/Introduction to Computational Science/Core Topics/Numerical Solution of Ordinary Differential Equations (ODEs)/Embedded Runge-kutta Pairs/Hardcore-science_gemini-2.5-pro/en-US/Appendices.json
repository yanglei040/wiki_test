{
    "hands_on_practices": [
        {
            "introduction": "To build a reliable adaptive solver, we need a way to estimate the error at each step. Embedded Runge-Kutta pairs achieve this by computing two solutions of different orders, but why is this difference in order crucial? This practice guides you through the symbolic derivation of order conditions to reveal the answer, demonstrating why a pair of methods with the same order would fail to produce a useful error estimate for a common class of problems . Mastering this concept is key to understanding how adaptive integrators are constructed from first principles.",
            "id": "3123519",
            "problem": "Consider a general two-stage explicit Runge-Kutta (RK) method with one embedded estimator. The stages are defined by\n$$\nk_{1} \\;=\\; f(y_{n}), \\qquad k_{2} \\;=\\; f\\!\\big(y_{n} + h\\,a_{21}\\,k_{1}\\big),\n$$\nand the main update and an embedded update are\n$$\ny_{n+1}^{(p)} \\;=\\; y_{n} + h\\big(b_{1}\\,k_{1} + b_{2}\\,k_{2}\\big), \\qquad y_{n+1}^{(\\hat{p})} \\;=\\; y_{n} + h\\big(\\hat{b}_{1}\\,k_{1} + \\hat{b}_{2}\\,k_{2}\\big),\n$$\nrespectively, where $h$ is the time step, $f$ is a sufficiently smooth right-hand side, and $a_{21}$, $b_{1}$, $b_{2}$, $\\hat{b}_{1}$, $\\hat{b}_{2}$ are method coefficients. The goal is to construct an embedded pair with main order $p=2$ and embedded order $\\hat{p}=1$, denoted $p=2(1)$.\n\nStarting from the Taylor expansion of the exact solution to the initial value problem $y'(t) = f(y(t))$ with $y(t_{n}) = y_{n}$, and the definitions of the method above, derive the order conditions imposed by matching the series coefficients up to the appropriate order for the main method and the embedded estimator. Solve the order conditions symbolically for the two-stage explicit case in terms of a free stage parameter $a_{21} = \\alpha \\neq 0$, and produce the one-parameter family of $p=2(1)$ embedded pairs $\\{b_{1}(\\alpha), b_{2}(\\alpha); \\hat{b}_{1}, \\hat{b}_{2}\\}$.\n\nThen, construct a pathological system to demonstrate why the embedded error must be of different order than the main method for reliable step-size control. Consider the linear autonomous test equation $y'(t) = \\lambda\\,y(t)$ with constant $\\lambda \\in \\mathbb{R}$ and show, by direct computation of the one-step updates, that if the embedded estimator were chosen to have the same order $p=2$ as the main method while sharing the same stage parameter $a_{21}=\\alpha$, then the embedded error estimate \n$$\nE \\;=\\; y_{n+1}^{(p)} - y_{n+1}^{(\\hat{p})}\n$$\nwould vanish for all $h$, $\\lambda$, and $y_{n}$, even though the true local truncation error of either method is nonzero for this test equation. Conclude with the explicit expression for $E$ for this pathological case.\n\nYour final answer must be a single row matrix containing the five coefficients $\\big(b_{1}(\\alpha), b_{2}(\\alpha), a_{21}(\\alpha), \\hat{b}_{1}, \\hat{b}_{2}\\big)$ for the $p=2(1)$ pair you derived, followed by the expression for $E$ in the pathological case described above. No numerical rounding is required. Express the final answer using exact symbolic forms.",
            "solution": "The user has provided a valid problem statement from the field of computational science, specifically concerning the derivation and analysis of embedded Runge-Kutta methods. The problem is scientifically grounded, well-posed, and objective. I will proceed with a full solution.\n\nThe problem is divided into two parts. First, we derive the order conditions for a two-stage explicit Runge-Kutta (RK) method to form a $p=2(1)$ embedded pair. Second, we analyze a pathological case where both the main method and the embedded estimator are of order $p=2$.\n\n**Part 1: Derivation of the $p=2(1)$ RK Pair**\n\nWe start by considering the Taylor series expansion of the exact solution to the initial value problem $y'(t) = f(y(t))$ with $y(t_n) = y_n$. The solution at time $t_{n+1} = t_n + h$ is given by:\n$$\ny(t_n + h) = y(t_n) + h y'(t_n) + \\frac{h^2}{2} y''(t_n) + O(h^3)\n$$\nUsing the chain rule and the definition of the ODE, we can express the derivatives of $y$ in terms of $f$ and its partial derivatives with respect to $y$, which we denote as $f_y$, $f_{yy}$, etc.\n$y'(t_n) = f(y(t_n)) = f(y_n)$\n$y''(t_n) = \\frac{d}{dt} f(y(t)) \\Big|_{t=t_n} = f_y(y(t_n)) \\cdot y'(t_n) = f_y f$\nSubstituting these into the Taylor expansion, we get:\n$$\ny(t_n + h) = y_n + h f + \\frac{h^2}{2} f_y f + O(h^3)\n$$\nNext, we expand the numerical solution provided by the RK method. The method is defined by:\n$$\nk_{1} = f(y_{n}) \\\\\nk_{2} = f(y_{n} + h a_{21} k_{1}) \\\\\ny_{n+1} = y_{n} + h (b_{1} k_{1} + b_{2} k_{2})\n$$\nWe expand the stage $k_2$ as a Taylor series around $y_n$:\n$k_1 = f(y_n) = f$\n$$\nk_2 = f(y_n + h a_{21} f) = f(y_n) + (h a_{21} f) f_y(y_n) + O(h^2) = f + h a_{21} f_y f + O(h^2)\n$$\nNow, we substitute the expansions of $k_1$ and $k_2$ into the update formula for the main method, $y_{n+1}^{(p)}$:\n$$\ny_{n+1}^{(p)} = y_n + h \\big( b_1 f + b_2 (f + h a_{21} f_y f + O(h^2)) \\big)\n$$\n$$\ny_{n+1}^{(p)} = y_n + h (b_1 + b_2) f + h^2 (b_2 a_{21}) f_y f + O(h^3)\n$$\nFor the main method to have order $p=2$, its expansion must match the exact solution's expansion up to the $O(h^2)$ term. By comparing the coefficients of $f$ and $f_y f$, we obtain the order conditions:\n\\begin{align*}\n\\text{Order } p=2 \\text{ conditions:} \\\\\nO(h):  \\quad b_1 + b_2 = 1 \\\\\nO(h^2):  \\quad b_2 a_{21} = \\frac{1}{2}\n\\end{align*}\nSimilarly, for the embedded method $y_{n+1}^{(\\hat{p})} = y_{n} + h(\\hat{b}_{1}k_{1} + \\hat{b}_{2}k_{2})$, we find its expansion:\n$$\ny_{n+1}^{(\\hat{p})} = y_n + h (\\hat{b}_1 + \\hat{b}_2) f + h^2 (\\hat{b}_2 a_{21}) f_y f + O(h^3)\n$$\nFor the embedded method to have order $\\hat{p}=1$, its expansion must match the exact solution's expansion up to the $O(h)$ term. This gives a single order condition:\n\\begin{align*}\n\\text{Order } \\hat{p}=1 \\text{ condition:} \\\\\nO(h):  \\quad \\hat{b}_1 + \\hat{b}_2 = 1\n\\end{align*}\nNow, we solve these systems of equations. The problem specifies a free parameter $a_{21} = \\alpha \\neq 0$.\nFrom the $p=2$ conditions:\n$b_2 \\alpha = \\frac{1}{2} \\implies b_2 = \\frac{1}{2\\alpha}$\n$b_1 = 1 - b_2 = 1 - \\frac{1}{2\\alpha}$\nFor the $\\hat{p}=1$ condition, there are infinite solutions. A common and simple choice is to set $\\hat{b}_2=0$, which implies $\\hat{b}_1=1$. This choice makes the embedded method equivalent to the Forward Euler method, $y_{n+1}^{(\\hat{p})} = y_n + h k_1 = y_n + h f(y_n)$, which is indeed a first-order method.\n\nThe resulting one-parameter family of coefficients for the $p=2(1)$ pair is:\n$b_1(\\alpha) = 1 - \\frac{1}{2\\alpha}$\n$b_2(\\alpha) = \\frac{1}{2\\alpha}$\n$a_{21}(\\alpha) = \\alpha$\n$\\hat{b}_1 = 1$\n$\\hat{b}_2 = 0$\n\n**Part 2: Analysis of the Pathological Case**\n\nWe now investigate the scenario where the embedded method is chosen to have the same order as the main method, i.e., $p=\\hat{p}=2$, while sharing the same stage values (same $a_{21}$). The test equation is the linear autonomous system $y'(t) = \\lambda y(t)$, for which $f(y) = \\lambda y$.\n\nThe order conditions for both methods are now identical:\nFor the main method ($p=2$): $b_1 + b_2 = 1$ and $b_2 a_{21} = \\frac{1}{2}$.\nFor the embedded method ($\\hat{p}=2$): $\\hat{b}_1 + \\hat{b}_2 = 1$ and $\\hat{b}_2 a_{21} = \\frac{1}{2}$.\n\nLet's compute the stages for this test problem:\n$k_1 = f(y_n) = \\lambda y_n$\n$k_2 = f(y_n + h a_{21} k_1) = \\lambda(y_n + h a_{21} (\\lambda y_n)) = \\lambda y_n (1 + h a_{21} \\lambda)$\n\nThe update for the main method is:\n$$\ny_{n+1}^{(p)} = y_n + h \\left( b_1(\\lambda y_n) + b_2(\\lambda y_n (1 + h a_{21} \\lambda)) \\right)\n$$\n$$\ny_{n+1}^{(p)} = y_n \\left( 1 + h\\lambda(b_1 + b_2) + (h\\lambda)^2 (b_2 a_{21}) \\right)\n$$\nSubstituting the order $p=2$ conditions, $b_1+b_2=1$ and $b_2 a_{21}=1/2$:\n$$\ny_{n+1}^{(p)} = y_n \\left( 1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2 \\right)\n$$\nThe update for the embedded method is:\n$$\ny_{n+1}^{(\\hat{p})} = y_n + h \\left( \\hat{b}_1(\\lambda y_n) + \\hat{b}_2(\\lambda y_n (1 + h a_{21} \\lambda)) \\right)\n$$\n$$\ny_{n+1}^{(\\hat{p})} = y_n \\left( 1 + h\\lambda(\\hat{b}_1 + \\hat{b}_2) + (h\\lambda)^2 (\\hat{b}_2 a_{21}) \\right)\n$$\nSubstituting the (hypothetical) order $\\hat{p}=2$ conditions, $\\hat{b}_1+\\hat{b}_2=1$ and $\\hat{b}_2 a_{21}=1/2$:\n$$\ny_{n+1}^{(\\hat{p})} = y_n \\left( 1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2 \\right)\n$$\nThe embedded error estimate is defined as $E = y_{n+1}^{(p)} - y_{n+1}^{(\\hat{p})}$. Computing this difference:\n$$\nE = y_n \\left( 1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2 \\right) - y_n \\left( 1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2 \\right) = 0\n$$\nThe error estimate is identically zero for any choice of $h$, $\\lambda$, and $y_n$. However, the true local truncation error (LTE) of either method is non-zero. For the exact solution $y(t_{n+1}) = y_n \\exp(h\\lambda)$, the LTE is:\n$$\n\\text{LTE} = y_n \\exp(h\\lambda) - y_n \\left( 1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2 \\right) = y_n \\left( \\frac{(h\\lambda)^3}{6} + O(h^4) \\right) \\neq 0\n$$\nThis demonstrates that if the embedded method has the same order as the main-method and shares its stage computations in a two-stage scheme, the error estimator fails for linear problems, as it incorrectly reports zero error. This is why embedded pairs are constructed with methods of different orders, such as $p(\\hat{p})$ with $\\hat{p}=p-1$.\n\nThe expression for $E$ in this pathological case is $E=0$. The five coefficients for the $p=2(1)$ pair are $b_{1}(\\alpha) = 1 - \\frac{1}{2\\alpha}$, $b_{2}(\\alpha) = \\frac{1}{2\\alpha}$, $a_{21}(\\alpha) = \\alpha$, $\\hat{b}_{1}=1$, and $\\hat{b}_{2}=0$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 - \\frac{1}{2\\alpha}  \\frac{1}{2\\alpha}  \\alpha  1  0  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Once we have an embedded pair, how do we use it to control the step size? This exercise provides a hands-on look at the two primary constraints an adaptive solver must respect: the stability of the numerical method and the user-specified accuracy tolerance . By constructing a simple pedagogical method and analyzing it on a test equation, you will derive the maximum step size, $h_{\\max}$, that satisfies both conditions, revealing the delicate balance at the heart of adaptive control.",
            "id": "3123496",
            "problem": "An explicit embedded Runge-Kutta $2(1)$ pair advances the initial value problem $y' = f(t,y)$ from $t_n$ to $t_{n+1} = t_n + h$ by computing stage derivatives and combining them with two different sets of weights: one set for a second-order approximation and another set for a first-order embedded approximation. Starting from the definition of explicit Runge-Kutta methods and the order conditions for second order, construct a pedagogical Butcher tableau with simple rational coefficients that defines such a pair with two stages. Then, analyze its behavior on the linear test equation $y' = \\lambda y$ with constant $\\lambda \\in \\mathbb{R}$ and step size $h  0$, where $z = h \\lambda$.\n\nUsing the linear test equation and the constructed tableau, derive the one-step stability function $R(z)$ for the second-order method and the one-step function $\\widehat{R}(z)$ for the embedded first-order method. Define the embedded one-step error estimator by $E(z) = R(z) - \\widehat{R}(z)$ so that the per-step difference between the two approximations satisfies $y_{n+1} - \\widehat{y}_{n+1} = E(z)\\, y_n$ on the test equation. Calibrate the absolute stability region of the second-order method on the negative real axis by imposing two simultaneous conditions for real $z$: the usual absolute stability condition $\\lvert R(z) \\rvert \\leq 1$ and an embedded calibration bound $\\lvert E(z) \\rvert \\leq \\theta$, where $\\theta \\ge 0$ is a user-specified tolerance. For $\\lambda  0$, this results in a maximum admissible step size $h_{\\max}$ expressed in terms of $\\lambda$ and $\\theta$. For $\\lambda  0$, explain the consequence for admissible $h$.\n\nYour program must implement the constructed tableau and the derived conditions to compute $h_{\\max}$ for each case in the test suite below. The program should not perform any numerical integration; it should use the analytically derived expressions specialized to the test equation $y'=\\lambda y$.\n\nTest suite (each case is a pair $(\\lambda,\\theta)$):\n- Case $1$: $\\lambda = -1$, $\\theta = \\frac{1}{8}$.\n- Case $2$: $\\lambda = -\\frac{1}{2}$, $\\theta = 10$.\n- Case $3$: $\\lambda = -10$, $\\theta = 100$.\n- Case $4$: $\\lambda = -10$, $\\theta = 0.005$.\n- Case $5$: $\\lambda = 3$, $\\theta = 1$.\n- Case $6$: $\\lambda = -2$, $\\theta = 0$.\n\nFor each case, output the value of $h_{\\max}$ as a floating-point number. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[h_1,h_2,\\dots,h_6]$). No physical units are involved; all values are dimensionless real numbers.",
            "solution": "The problem statement is critically validated and found to be valid. It is scientifically grounded in the theory of numerical methods for ordinary differential equations, well-posed with a clear objective and sufficient data, and free from any scientific unsoundness, ambiguity, or contradiction.\n\nThe solution proceeds in several steps:\n1.  Construction of a suitable Butcher tableau for a $2(1)$ embedded Runge-Kutta pair.\n2.  Derivation of the stability functions for the higher-order and embedded methods for the linear test equation $y'=\\lambda y$.\n3.  Analysis of the stability and calibration conditions to derive an expression for the maximum step size $h_{\\max}$.\n4.  Evaluation of $h_{\\max}$ for the provided test cases.\n\n**Step 1: Construction of the Butcher Tableau**\nAn $s$-stage explicit Runge-Kutta method is defined by a Butcher tableau of the form:\n$$\n\\begin{array}{c|c}\n\\mathbf{c}  \\mathbf{A} \\\\\n\\hline\n \\mathbf{b}^T \\\\\n \\widehat{\\mathbf{b}}^T\n\\end{array}\n=\n\\begin{array}{c|cccc}\nc_1  a_{11}  a_{12}  \\dots  a_{1s} \\\\\nc_2  a_{21}  a_{22}  \\dots  a_{2s} \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\nc_s  a_{s1}  a_{s2}  \\dots  a_{ss} \\\\\n\\hline\n b_1  b_2  \\dots  b_s \\\\\n \\hat{b}_1  \\hat{b}_2  \\dots  \\hat{b}_s\n\\end{array}\n$$\nFor a two-stage ($s=2$) explicit method, the matrix $\\mathbf{A}$ is strictly lower triangular, so $a_{11}=a_{12}=a_{22}=0$. The consistency condition $c_i = \\sum_{j=1}^{s} a_{ij}$ requires $c_1 = 0$ and $c_2 = a_{21}$. The tableau simplifies to:\n$$\n\\begin{array}{c|cc}\n0  0  0 \\\\\nc_2  c_2  0 \\\\\n\\hline\n b_1  b_2 \\\\\n \\hat{b}_1  \\hat{b}_2\n\\end{array}\n$$\nThe method with weights $b_i$ must be second-order accurate. The order conditions for a second-order method are:\n1.  Order 1: $\\sum_{i=1}^2 b_i = b_1 + b_2 = 1$\n2.  Order 2: $\\sum_{i=1}^2 b_i c_i = b_1 c_1 + b_2 c_2 = 1/2$\n\nThe embedded method with weights $\\hat{b}_i$ must be first-order accurate. The order condition for a first-order method is:\n1.  Order 1: $\\sum_{i=1}^2 \\hat{b}_i = \\hat{b}_1 + \\hat{b}_2 = 1$\n\nSubstituting $c_1=0$ into the second-order condition gives $b_2 c_2 = 1/2$. To construct a tableau with simple rational coefficients, we can choose $c_2=1$. This implies $a_{21}=1$ and $b_2=1/2$. From the first-order condition on $b_i$, we find $b_1 = 1 - b_2 = 1 - 1/2 = 1/2$.\n\nFor the embedded first-order method, we need to satisfy $\\hat{b}_1 + \\hat{b}_2 = 1$, while ensuring it is not second-order, i.e., $\\sum \\hat{b}_i c_i = \\hat{b}_2 c_2 = \\hat{b}_2 \\neq 1/2$. A simple choice is to select the first stage result, which corresponds to the Forward Euler method. This is achieved by setting $\\hat{b}_1=1$ and $\\hat{b}_2=0$. These weights satisfy $\\hat{b}_1+\\hat{b}_2=1$, and the second-order condition is not met since $\\hat{b}_2(1) = 0 \\neq 1/2$.\n\nThe resulting Butcher tableau for our $2(1)$ pair is:\n$$\n\\begin{array}{c|cc}\n0  0  0 \\\\\n1  1  0 \\\\\n\\hline\n 1/2  1/2 \\\\\n 1  0\n\\end{array}\n$$\n\n**Step 2: Derivation of the Stability Functions**\nWe analyze the method on the linear test equation $y' = \\lambda y$, where $\\lambda \\in \\mathbb{R}$. The stages are:\n$$ k_1 = f(t_n, y_n) = \\lambda y_n $$\n$$ k_2 = f(t_n+c_2h, y_n+ha_{21}k_1) = \\lambda(y_n + h(1)k_1) = \\lambda(y_n + h\\lambda y_n) = \\lambda y_n (1+h\\lambda) $$\nLet $z=h\\lambda$. Then $k_1 = \\lambda y_n$ and $k_2 = \\lambda y_n (1+z)$.\n\nThe second-order approximation $y_{n+1}$ is:\n$$ y_{n+1} = y_n + h(b_1 k_1 + b_2 k_2) = y_n + h\\left(\\frac{1}{2}\\lambda y_n + \\frac{1}{2}\\lambda y_n(1+z)\\right) $$\n$$ y_{n+1} = y_n \\left(1 + \\frac{h\\lambda}{2}(1 + 1+z)\\right) = y_n \\left(1 + \\frac{z}{2}(2+z)\\right) = y_n \\left(1 + z + \\frac{z^2}{2}\\right) $$\nThe stability function for the second-order method is $R(z)$, defined by $y_{n+1} = R(z)y_n$. Thus:\n$$ R(z) = 1 + z + \\frac{z^2}{2} $$\n\nThe first-order embedded approximation $\\widehat{y}_{n+1}$ is:\n$$ \\widehat{y}_{n+1} = y_n + h(\\hat{b}_1 k_1 + \\hat{b}_2 k_2) = y_n + h(1 \\cdot \\lambda y_n + 0 \\cdot k_2) = y_n (1+h\\lambda) $$\nThe stability function for the embedded method is $\\widehat{R}(z)$, defined by $\\widehat{y}_{n+1} = \\widehat{R}(z)y_n$. Thus:\n$$ \\widehat{R}(z) = 1+z $$\n\nThe one-step error estimator function $E(z)$ is:\n$$ E(z) = R(z) - \\widehat{R}(z) = \\left(1 + z + \\frac{z^2}{2}\\right) - (1+z) = \\frac{z^2}{2} $$\n\n**Step 3: Analysis of Stability and Calibration Conditions**\nWe need to find the maximum step size $h_{\\max}$ that simultaneously satisfies two conditions for a given $\\lambda \\in \\mathbb{R}$ and $\\theta \\ge 0$:\n1.  Absolute stability: $|R(z)| \\le 1$\n2.  Embedded calibration: $|E(z)| \\le \\theta$\n\nCase $\\lambda  0$: In this case, $z = h\\lambda  0$.\nThe absolute stability condition $|1 + z + z^2/2| \\le 1$ is equivalent to $-1 \\le 1 + z + z^2/2 \\le 1$.\nThe right inequality, $1 + z + z^2/2 \\le 1$, simplifies to $z + z^2/2 \\le 0$, or $z(1+z/2) \\le 0$. Since $z0$, this requires $1+z/2 \\ge 0$, which implies $z \\ge -2$.\nThe left inequality, $-1 \\le 1 + z + z^2/2$, simplifies to $z^2/2 + z + 2 \\ge 0$, or $z^2+2z+4 \\ge 0$. The discriminant of this quadratic is $2^2 - 4(1)(4) = -12  0$, and the leading coefficient is positive, so the quadratic is always positive for all real $z$.\nThus, absolute stability for $z0$ requires $z \\in [-2, 0)$.\n\nThe embedded calibration condition is $|z^2/2| \\le \\theta$. Since $z^2/2$ is non-negative, this is $z^2/2 \\le \\theta$, which gives $z^2 \\le 2\\theta$, or $|z| \\le \\sqrt{2\\theta}$. Since $z0$, this means $-z \\le \\sqrt{2\\theta}$, or $z \\ge -\\sqrt{2\\theta}$.\n\nTo satisfy both conditions, $z$ must satisfy $z \\ge -2$ and $z \\ge -\\sqrt{2\\theta}$. This is equivalent to $z \\ge \\max(-2, -\\sqrt{2\\theta}) = -\\min(2, \\sqrt{2\\theta})$.\nSubstituting $z=h\\lambda$:\n$$ h\\lambda \\ge -\\min(2, \\sqrt{2\\theta}) $$\nSince $\\lambda  0$, dividing by $\\lambda$ reverses the inequality:\n$$ h \\le \\frac{-\\min(2, \\sqrt{2\\theta})}{\\lambda} = \\frac{\\min(2, \\sqrt{2\\theta})}{-\\lambda} = \\frac{\\min(2, \\sqrt{2\\theta})}{|\\lambda|} $$\nThe maximum admissible step size is therefore:\n$$ h_{\\max} = \\frac{\\min(2, \\sqrt{2\\theta})}{|\\lambda|} \\quad (\\text{for } \\lambda  0) $$\n\nCase $\\lambda  0$: In this case, $z = h\\lambda  0$.\nThe stability function is $R(z) = 1 + z + z^2/2$. For any $z0$, every term is positive, so $R(z)  1$. The absolute stability condition $|R(z)| \\le 1$ cannot be satisfied for any $h0$. The only admissible non-negative step size is $h=0$. Therefore, for $\\lambda  0$, the method is unconditionally unstable.\nThe maximum admissible step size is $h_{\\max} = 0$.\n\n**Summary for Implementation:**\nThe value of $h_{\\max}$ for a given pair $(\\lambda, \\theta)$ is computed as follows:\n- If $\\lambda  0$, $h_{\\max} = 0$.\n- If $\\lambda = 0$, $z=0$, $R(0)=1$, $E(0)=0$. The conditions are satisfied for any $h0$, so $h_{\\max} \\to \\infty$. This case is not in the test suite.\n- If $\\lambda  0$, $h_{\\max} = \\frac{1}{-\\lambda} \\min(2, \\sqrt{2\\theta})$.\n\nThis derived formula will be implemented to compute the results for the test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the maximum admissible step size h_max for an embedded RK2(1)\n    method based on stability and error tolerance criteria for the linear\n    test equation y' = lambda*y.\n    \"\"\"\n    # Define the test cases from the problem statement as pairs of (lambda, theta).\n    test_cases = [\n        (-1.0, 1.0/8.0),   # Case 1\n        (-0.5, 10.0),      # Case 2\n        (-10.0, 100.0),    # Case 3\n        (-10.0, 0.005),    # Case 4\n        (3.0, 1.0),        # Case 5\n        (-2.0, 0.0),       # Case 6\n    ]\n\n    results = []\n    for case in test_cases:\n        lam, theta = case\n        h_max = 0.0\n\n        # For lambda  0, the method is unconditionally unstable for h  0.\n        # R(z) = 1 + z + z^2/2  1 for z = h*lambda  0.\n        # Thus, |R(z)| = 1 is only satisfied for h=0.\n        if lam > 0:\n            h_max = 0.0\n        # For lambda  0, we must satisfy two conditions:\n        # 1. Absolute stability |R(z)| = 1, which for z0 implies z = -2.\n        #    This translates to h = -2/lambda.\n        # 2. Error calibration |E(z)| = theta, where E(z) = z^2/2.\n        #    This implies z^2 = 2*theta, or z = -sqrt(2*theta).\n        #    This translates to h = -sqrt(2*theta)/lambda.\n        # The maximum step size h_max is the minimum of these two bounds.\n        elif lam  0:\n            abs_lam = -lam\n            # Calculate the two upper bounds for h\n            bound_stability = 2.0 / abs_lam\n            bound_calibration = np.sqrt(2.0 * theta) / abs_lam\n            h_max = min(bound_stability, bound_calibration)\n\n        # The case lambda = 0 is not in the test suite. If it were,\n        # z=0 for all h, R(0)=1, E(0)=0, so any h is admissible,\n        # and h_max would be infinity.\n\n        results.append(h_max)\n\n    # Format the final output as a comma-separated list in brackets.\n    # The map(str, ...) ensures float representation for all numbers.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The principles of adaptive step-size control extend from single equations to systems of ODEs, but this introduces a new challenge: what if the different variables in the system have very different magnitudes? This practice tackles this crucial real-world issue by comparing a simple \"scalar\" tolerance with a more sophisticated \"vector\" tolerance tailored to the unique scales of the predator-prey model . You will investigate how this choice affects solver efficiency and the permissible step size, gaining insight into the practical art of configuring a robust solver for multi-component systems.",
            "id": "3123484",
            "problem": "You are to design and implement a complete, runnable program that compares scalar and vector absolute tolerances in an adaptive Ordinary Differential Equation (ODE) solver based on an embedded Runge-Kutta (RK) pair, and analyzes single-step acceptance maps. The problem uses the classical predator–prey (Lotka–Volterra) model, and your program must produce a single-line output that aggregates quantitative results from a small, specified test suite.\n\nThe starting point is the fundamental model given by the autonomous system\n$$\n\\frac{d\\mathbf{y}}{dt}=\\mathbf{f}(t,\\mathbf{y}), \\quad \\mathbf{y}(t)=\\begin{bmatrix}y_1(t)\\\\y_2(t)\\end{bmatrix},\n$$\nwhere, for the predator–prey model,\n$$\n\\mathbf{f}(t,\\mathbf{y})=\\begin{bmatrix}\n\\alpha y_1 - \\beta y_1 y_2\\\\\n\\delta y_1 y_2 - \\gamma y_2\n\\end{bmatrix},\n$$\nwith parameters $\\alpha0$, $\\beta0$, $\\gamma0$, and $\\delta0$. The steady state $\\mathbf{s}=\\begin{bmatrix}s_1\\\\s_2\\end{bmatrix}$ satisfies $\\mathbf{f}(t,\\mathbf{s})=\\mathbf{0}$, which for this system yields\n$$\ns_1=\\frac{\\gamma}{\\delta},\\quad s_2=\\frac{\\alpha}{\\beta}.\n$$\n\nAn embedded Runge-Kutta (ERK) pair of orders $(p,p-1)$ produces two approximations $\\mathbf{y}^{[p]}$ and $\\mathbf{y}^{[p-1]}$ at the end of a step of size $h$, and their difference estimates the local truncation error. You must use an ERK pair with $p=3$ and $p-1=2$ to drive adaptive step size control. The step accept/reject decision must be made by comparing a Root Mean Square (RMS) norm of the component-wise scaled error to the threshold value $1$ as follows. If $\\mathbf{e}$ is the error estimate vector for one step, then define per-component scaling\n$$\n\\tau_i = a_i + r\\cdot \\max\\!\\big(|y_i|,|y_i^{\\text{new}}|\\big),\n$$\nwhere $a_i$ are absolute tolerances and $r$ is a relative tolerance, and accept the step if\n$$\n\\left(\\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{e_i}{\\tau_i}\\right)^2\\right)^{1/2}\\le 1,\n$$\nwith $n=2$ for this system.\n\nYou must implement and compare two absolute tolerance schemes:\n- Scalar absolute tolerance: $a_i=a_{\\text{sc}}$ for all components, with\n$$\na_{\\text{sc}}=\\eta\\cdot \\frac{s_1+s_2}{2},\n$$\nwhere $\\eta0$ is a user-specified scale.\n- Vector absolute tolerance: $a_i=\\eta\\cdot s_i$ for each component $i\\in\\{1,2\\}$.\n\nUse a standard adaptive step size update rule of the form\n$$\nh_{\\text{new}}=h\\cdot \\theta\\cdot \\left(\\max\\!\\big(\\varepsilon, \\min(M,\\rho)\\big)\\right),\n$$\nwhere $\\rho$ is a function of the error measure that reduces $h$ when the error is too large and increases $h$ when the error is small, $\\theta\\in(0,1)$ is a safety factor, and $\\varepsilon$ and $M$ are minimum and maximum growth limiters. You must choose parameters so that $h_{\\text{new}}$ appropriately responds to the measured error from the chosen ERK pair with $p=3$.\n\nDefine the “embedded acceptance map” at a given state $(t,\\mathbf{y})$ as the mapping $h\\mapsto \\{\\text{accept},\\text{reject}\\}$ that results when a single ERK step of size $h$ is tested with the above criterion, without actually advancing the state. This map can be summarized by the largest tested $h$ that would be accepted.\n\nImplementation requirements:\n- Use the predator–prey vector field $\\mathbf{f}$ defined above.\n- Use an embedded Runge-Kutta pair of orders $(3,2)$ for local error estimation and adaptive step size control.\n- Implement both absolute tolerance schemes defined above with the same relative tolerance $r$.\n- For each simulation, initialize the first step size to $h_0=(t_1-t_0)/2$.\n- For the acceptance map at the initial state $(t_0,\\mathbf{y}_0)$, evaluate a logarithmically spaced grid of candidate step sizes $h$ over $[h_{\\min},h_{\\max}]$ with $h_{\\min}=10^{-4}(t_1-t_0)$ and $h_{\\max}=0.9(t_1-t_0)$, and summarize by the maximum $h$ that is accepted.\n\nTest suite:\nFor each of the following three test cases, conduct two tasks: an adaptive integration over $[t_0,t_1]$ with both tolerance schemes and the acceptance map analysis at $(t_0,\\mathbf{y}_0)$ with both schemes. For the integration, record the number of rejected steps. For the acceptance map, report the ratio $h_{\\max}^{\\text{vec}}/h_{\\max}^{\\text{sc}}$, where $h_{\\max}^{\\text{vec}}$ is the maximum accepted step size under the vector absolute tolerance and $h_{\\max}^{\\text{sc}}$ is the maximum accepted step size under the scalar absolute tolerance. Use the following parameter sets:\n- Case A (baseline): $\\alpha=1.0$, $\\beta=0.1$, $\\gamma=1.5$, $\\delta=0.075$, $\\mathbf{y}_0=\\begin{bmatrix}10.0\\\\5.0\\end{bmatrix}$, $[t_0,t_1]=[0.0,20.0]$, $r=10^{-3}$, $\\eta=10^{-6}$.\n- Case B (unbalanced initial condition): $\\alpha=1.0$, $\\beta=0.1$, $\\gamma=1.5$, $\\delta=0.075$, $\\mathbf{y}_0=\\begin{bmatrix}1000.0\\\\0.1\\end{bmatrix}$, $[t_0,t_1]=[0.0,5.0]$, $r=10^{-4}$, $\\eta=10^{-8}$.\n- Case C (near steady state): $\\alpha=1.0$, $\\beta=0.1$, $\\gamma=1.5$, $\\delta=0.075$, $\\mathbf{y}_0=\\begin{bmatrix}20.0(1+10^{-4})\\\\10.0(1-10^{-4})\\end{bmatrix}$, $[t_0,t_1]=[0.0,10.0]$, $r=10^{-6}$, $\\eta=10^{-10}$.\n\nFor each case, output a triple of values in order:\n- The integer number of rejected steps using the scalar absolute tolerance.\n- The integer number of rejected steps using the vector absolute tolerance.\n- The floating-point ratio $h_{\\max}^{\\text{vec}}/h_{\\max}^{\\text{sc}}$ rounded to six decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of cases A, B, C, each contributing its triple, flattened into a single list. For example, the output must be of the form\n$$\n[\\text{A\\_rej\\_sc},\\text{A\\_rej\\_vec},\\text{A\\_ratio},\\text{B\\_rej\\_sc},\\text{B\\_rej\\_vec},\\text{B\\_ratio},\\text{C\\_rej\\_sc},\\text{C\\_rej\\_vec},\\text{C\\_ratio}],\n$$\nwhere the first two entries of each triple are integers and the third is a decimal rounded to six places. No additional text may be printed.",
            "solution": "The user wants to solve a numerical analysis problem involving Ordinary Differential Equations (ODEs).The problem requires the design and implementation of an adaptive ODE solver to analyze the behavior of the Lotka-Volterra predator-prey model. The analysis focuses on comparing two different absolute tolerance schemes—scalar versus vector—within the framework of an embedded Runge-Kutta method.\n\n**1. The Predator-Prey Model**\n\nThe system is described by the autonomous system of first-order ODEs:\n$$\n\\frac{d\\mathbf{y}}{dt} = \\mathbf{f}(t, \\mathbf{y}), \\quad \\text{with } \\mathbf{y}(t) = \\begin{bmatrix} y_1(t) \\\\ y_2(t) \\end{bmatrix}\n$$\nHere, $y_1(t)$ represents the prey population and $y_2(t)$ represents the predator population. The vector field $\\mathbf{f}$ for the Lotka-Volterra model is given by:\n$$\n\\mathbf{f}(t, \\mathbf{y}) = \\begin{bmatrix}\n\\alpha y_1 - \\beta y_1 y_2 \\\\\n\\delta y_1 y_2 - \\gamma y_2\n\\end{bmatrix}\n$$\nwhere $\\alpha, \\beta, \\gamma, \\delta$ are positive parameters governing the population dynamics. The non-trivial steady state (or equilibrium point) $\\mathbf{s} = \\begin{bmatrix} s_1 \\\\ s_2 \\end{bmatrix}$ is found by setting $\\mathbf{f}(t, \\mathbf{s}) = \\mathbf{0}$, which yields:\n$$\ns_1 = \\frac{\\gamma}{\\delta}, \\quad s_2 = \\frac{\\alpha}{\\beta}\n$$\nThis steady state is a key reference point for defining the absolute tolerance schemes.\n\n**2. The Embedded Runge-Kutta (3,2) Method**\n\nTo solve the ODE system numerically, we employ an adaptive step-size strategy driven by an embedded Runge-Kutta (ERK) pair of orders $p=3$ and $p-1=2$. Such a pair provides two approximations at each step, a higher-order one $\\mathbf{y}^{[3]}$ and a lower-order one $\\mathbf{y}^{[2]}$. Their difference serves as an estimate of the local truncation error.\n\nWe will use the Bogacki-Shampine 3(2) pair, which is a popular choice for its efficiency, including the First Same As Last (FSAL) property. For a step from $t_n$ to $t_{n+1} = t_n + h$, the stages are computed as follows:\n$$\n\\begin{aligned}\n\\mathbf{k}_1 = \\mathbf{f}(t_n, \\mathbf{y}_n) \\\\\n\\mathbf{k}_2 = \\mathbf{f}(t_n + \\frac{1}{2}h, \\mathbf{y}_n + \\frac{1}{2}h\\mathbf{k}_1) \\\\\n\\mathbf{k}_3 = \\mathbf{f}(t_n + \\frac{3}{4}h, \\mathbf{y}_n + \\frac{3}{4}h\\mathbf{k}_2)\n\\end{aligned}\n$$\nThe third-order approximation, which is used to advance the solution, is:\n$$\n\\mathbf{y}_{n+1}^{[3]} = \\mathbf{y}_n + h \\left( \\frac{2}{9}\\mathbf{k}_1 + \\frac{1}{3}\\mathbf{k}_2 + \\frac{4}{9}\\mathbf{k}_3 \\right)\n$$\nTo estimate the error, a fourth stage is computed. Due to the FSAL property, this stage is an evaluation at the new point $(t_{n+1}, \\mathbf{y}_{n+1}^{[3]})$, meaning it can be reused as the first stage ($\\mathbf{k}_1$) of the subsequent step if the current step is accepted.\n$$\n\\mathbf{k}_4 = \\mathbf{f}(t_{n+1}, \\mathbf{y}_{n+1}^{[3]})\n$$\nThe local error estimate vector $\\mathbf{e}_{n+1}$ is the difference between the third-order solution and an embedded second-order solution. It can be computed directly using the stages:\n$$\n\\mathbf{e}_{n+1} = \\mathbf{y}_{n+1}^{[3]} - \\mathbf{y}_{n+1}^{[2]} = h \\left( -\\frac{5}{72}\\mathbf{k}_1 + \\frac{1}{12}\\mathbf{k}_2 + \\frac{1}{9}\\mathbf{k}_3 - \\frac{1}{8}\\mathbf{k}_4 \\right)\n$$\n\n**3. Adaptive Step-Size Control**\n\nThe decision to accept or reject a step is based on a scaled root-mean-square (RMS) norm of the error estimate. First, a per-component tolerance scale $\\tau_i$ is defined:\n$$\n\\tau_i = a_i + r \\cdot \\max(|y_{n,i}|, |y_{n+1,i}^{[3]}|)\n$$\nwhere $a_i$ is the absolute tolerance for component $i$, $r$ is the relative tolerance, $y_{n,i}$ is the $i$-th component of the solution at the start of the step, and $y_{n+1,i}^{[3]}$ is the proposed value at the end of the step.\n\nThe error norm $E$ is then calculated as:\n$$\nE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{e_i}{\\tau_i}\\right)^2}\n$$\nFor our 2D system, $n=2$. The step is accepted if $E \\le 1$.\n\nIf the step is accepted, the new step size is calculated. If it is rejected, the current step is re-tried with a smaller step size. The formula for the new step size $h_{\\text{new}}$ is:\n$$\nh_{\\text{new}} = h \\cdot \\theta \\cdot \\max(\\varepsilon, \\min(M, E^{-1/p}))\n$$\n- $h$ is the current step size.\n- $p=3$ is the order of the method. The factor $E^{-1/p}$ is the ideal scaling based on local error theory.\n- $\\theta \\in (0,1)$ is a safety factor to prevent overly optimistic step size increases. We choose $\\theta=0.9$.\n- $\\varepsilon$ and $M$ are factors to limit the minimum and maximum change in step size. We choose $\\varepsilon=0.2$ and $M=5.0$.\n\n**4. Absolute Tolerance Schemes**\n\nThe problem requires a comparison of two schemes for setting the absolute tolerances $a_i$:\n- **Scalar Tolerance:** All components share the same absolute tolerance, $a_i = a_{\\text{sc}}$, which is based on the average of the steady-state values:\n  $$\n  a_{\\text{sc}} = \\eta \\cdot \\frac{s_1 + s_2}{2}\n  $$\n- **Vector Tolerance:** Each component has its own absolute tolerance, scaled by its corresponding steady-state value:\n  $$\n  a_i = \\eta \\cdot s_i \\quad \\text{for } i \\in \\{1, 2\\}\n  $$\nIn both cases, $\\eta$ is a small, user-specified parameter. This comparison highlights how tailoring tolerances to the characteristic scales of each variable can affect solver performance.\n\n**5. Analysis Tasks and Implementation**\n\nFor each test case, two analyses are performed:\n1.  **Adaptive Integration:** The ODE is integrated over the specified interval $[t_0, t_1]$. The total number of rejected steps is counted for both the scalar and vector tolerance schemes. This metric provides insight into the efficiency of each scheme.\n2.  **Acceptance Map Analysis:** At the initial state $(t_0, \\mathbf{y}_0)$, we determine the maximum step size $h$ that would be accepted by the error control criterion. This is done by testing a logarithmically spaced grid of candidate step sizes $h$ from $h_{\\min}=10^{-4}(t_1-t_0)$ to $h_{\\max}=0.9(t_1-t_0)$. The ratio of the maximum accepted step for the vector tolerance ($h_{\\max}^{\\text{vec}}$) to that for the scalar tolerance ($h_{\\max}^{\\text{sc}}$) is computed. This ratio quantifies the immediate impact of the tolerance choice on the permissible step size.\n\nThe implementation will consist of functions to perform a single ERK step, run the full adaptive integration, and analyze the acceptance map. These functions will be called for each test case specified in the problem statement to generate the required output values.",
            "answer": "```python\nimport numpy as np\nfrom functools import partial\n\ndef solve():\n    \"\"\"\n    Main solver function to orchestrate the test suite and generate the final output.\n    \"\"\"\n    \n    # ERK 3(2) Bogacki-Shampine coefficients (FSAL variant)\n    # The method advances with a 3rd-order solution. The error is estimated\n    # by taking the difference with an embedded 2nd-order solution.\n    # c: nodes, A: matrix, b: 3rd-order weights, e: error weights (b_3 - b_2)\n    C = np.array([0, 1/2, 3/4, 1])\n    A = np.array([\n        [0, 0, 0, 0],\n        [1/2, 0, 0, 0],\n        [0, 3/4, 0, 0],\n        [2/9, 1/3, 4/9, 0]\n    ])\n    B3 = np.array([2/9, 1/3, 4/9, 0])\n    B2 = np.array([7/24, 1/4, 1/3, 1/8])\n    E = B3 - B2  # [-5/72, 1/12, 1/9, -1/8]\n\n    # Adaptive step-size controller parameters\n    SAFETY_FACTOR = 0.9\n    MIN_GROWTH = 0.2\n    MAX_GROWTH = 5.0\n    METHOD_ORDER = 3\n\n    def lotka_volterra_rhs(t, y, alpha, beta, gamma, delta):\n        \"\"\"Lotka-Volterra predator-prey model RHS function.\"\"\"\n        y1, y2 = y\n        dy1_dt = alpha * y1 - beta * y1 * y2\n        dy2_dt = delta * y1 * y2 - gamma * y2\n        return np.array([dy1_dt, dy2_dt])\n\n    def perform_single_step(rhs, t, y, h, k1_in=None):\n        \"\"\"\n        Performs a single step of the Bogacki-Shampine 3(2) method.\n        Implements the FSAL (First Same As Last) property by accepting k1.\n        \"\"\"\n        k = np.zeros((4, len(y)))\n        \n        if k1_in is None:\n            k[0] = rhs(t, y)\n        else:\n            k[0] = k1_in\n        \n        k[1] = rhs(t + C[1] * h, y + h * A[1,0] * k[0])\n        k[2] = rhs(t + C[2] * h, y + h * (A[2,0] * k[0] + A[2,1] * k[1]))\n\n        y_new = y + h * (B3[0]*k[0] + B3[1]*k[1] + B3[2]*k[2])\n        \n        # This k4 evaluation can be used as k1 of the next step (FSAL).\n        k[3] = rhs(t + h, y_new)\n\n        error_estimate = h * (E[0]*k[0] + E[1]*k[1] + E[2]*k[2] + E[3]*k[3])\n        \n        return y_new, error_estimate, k[3]\n\n    def run_integration(rhs, y0, t_span, r, a):\n        \"\"\"Runs the adaptive integration and returns the number of rejected steps.\"\"\"\n        t0, t1 = t_span\n        t = t0\n        y = np.copy(y0)\n        h = (t1 - t0) / 2.0  # Initial step size per problem spec\n        rejected_steps = 0\n        k1_next = None\n\n        while t  t1:\n            if t + h > t1:\n                h = t1 - t\n\n            y_new, err_est, k4 = perform_single_step(rhs, t, y, h, k1_in=k1_next)\n\n            # Calculate error norm E\n            y_max_abs = np.maximum(np.abs(y), np.abs(y_new))\n            tol_scale = a + r * y_max_abs\n            \n            # Guard against division by zero if tolerance is zero\n            err_ratio = np.divide(err_est, tol_scale, out=np.zeros_like(err_est), where=tol_scale!=0)\n            \n            error_norm = np.sqrt(np.mean(err_ratio**2))\n\n            if error_norm = 1.0: # Step accepted\n                t += h\n                y = y_new\n                k1_next = k4 # FSAL property\n            else: # Step rejected\n                rejected_steps += 1\n                k1_next = None # Must recompute k1 for the new, smaller step\n\n            # Update step size\n            if error_norm == 0:\n                scale_factor = MAX_GROWTH\n            else:\n                scale_factor = (error_norm)**(-1.0 / METHOD_ORDER)\n\n            h_new_unlimited = h * SAFETY_FACTOR * scale_factor\n            h = np.clip(h_new_unlimited, h * MIN_GROWTH, h * MAX_GROWTH)\n            \n        return rejected_steps\n\n    def analyze_acceptance_map(rhs, y0, t_span, r, a):\n        \"\"\"Finds the maximum accepted step size h at the initial point.\"\"\"\n        t0, t1 = t_span\n        h_min = 1e-4 * (t1 - t0)\n        h_max = 0.9 * (t1 - t0)\n        h_candidates = np.logspace(np.log10(h_min), np.log10(h_max), 500)\n        \n        max_h_accepted = 0.0\n\n        for h in h_candidates:\n            y_new, err_est, _ = perform_single_step(rhs, t0, y0, h)\n            \n            y_max_abs = np.maximum(np.abs(y0), np.abs(y_new))\n            tol_scale = a + r * y_max_abs\n            \n            err_ratio = np.divide(err_est, tol_scale, out=np.full_like(err_est, np.inf), where=tol_scale!=0)\n            \n            error_norm = np.sqrt(np.mean(err_ratio**2))\n            \n            if error_norm = 1.0:\n                max_h_accepted = h\n        \n        return max_h_accepted\n\n    test_cases = [\n        # Case A (baseline)\n        {'alpha': 1.0, 'beta': 0.1, 'gamma': 1.5, 'delta': 0.075, 'y0': np.array([10.0, 5.0]), 't_span': [0.0, 20.0], 'r': 1e-3, 'eta': 1e-6},\n        # Case B (unbalanced initial condition)\n        {'alpha': 1.0, 'beta': 0.1, 'gamma': 1.5, 'delta': 0.075, 'y0': np.array([1000.0, 0.1]), 't_span': [0.0, 5.0], 'r': 1e-4, 'eta': 1e-8},\n        # Case C (near steady state)\n        {'alpha': 1.0, 'beta': 0.1, 'gamma': 1.5, 'delta': 0.075, 'y0': np.array([20.0*(1+1e-4), 10.0*(1-1e-4)]), 't_span': [0.0, 10.0], 'r': 1e-6, 'eta': 1e-10},\n    ]\n    \n    results = []\n\n    for case in test_cases:\n        p = {k: case[k] for k in ('alpha', 'beta', 'gamma', 'delta')}\n        y0, t_span, r, eta = case['y0'], case['t_span'], case['r'], case['eta']\n        \n        rhs = partial(lotka_volterra_rhs, **p)\n\n        # Steady state\n        s1 = p['gamma'] / p['delta']\n        s2 = p['alpha'] / p['beta']\n\n        # Scalar absolute tolerance\n        a_sc_val = eta * (s1 + s2) / 2\n        a_sc = np.array([a_sc_val, a_sc_val])\n        \n        # Vector absolute tolerance\n        a_vec = eta * np.array([s1, s2])\n\n        # Task 1: Adaptive integration\n        rej_sc = run_integration(rhs, y0, t_span, r, a_sc)\n        rej_vec = run_integration(rhs, y0, t_span, r, a_vec)\n\n        # Task 2: Acceptance map analysis\n        h_max_sc = analyze_acceptance_map(rhs, y0, t_span, r, a_sc)\n        h_max_vec = analyze_acceptance_map(rhs, y0, t_span, r, a_vec)\n        \n        ratio = h_max_vec / h_max_sc if h_max_sc > 0 else 0.0\n\n        results.extend([rej_sc, rej_vec, ratio])\n\n    # Format the results for the final output string\n    formatted_results = []\n    for i, res in enumerate(results):\n        if (i + 1) % 3 == 0: # It's a ratio\n            formatted_results.append(f\"{res:.6f}\")\n        else: # It's a rejection count\n            formatted_results.append(str(res))\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}